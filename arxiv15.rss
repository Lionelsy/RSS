<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 31 Oct 2025 14:58:35 +0800</lastBuildDate>
    <item>
      <title>UniField: Joint Multi-Domain Training for Universal Surface Pressure Modeling</title>
      <link>http://arxiv.org/abs/2510.24106v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UniField的方法，通过整合多个子领域的空气动力学数据进行联合训练，解决了数据稀缺性问题，实现了更通用的流场表示。&lt;h4&gt;背景&lt;/h4&gt;表面压力场的空气动力学模拟对许多工程问题至关重要。深度神经网络已成为传统计算流体力学(CFD)模拟的高效替代方案，但数据稀缺性限制了神经网络的应用。&lt;h4&gt;目的&lt;/h4&gt;整合多个子领域的空气动力学数据进行联合训练，以学习更通用的流场表示，解决数据稀缺性问题。&lt;h4&gt;方法&lt;/h4&gt;整合五个涵盖汽车、火车、飞机和一般形状等不同领域的数据集。提出UniField方法，采用领域无关的Transformer模块提取通用点云特征，并定制领域特定的流条件适配器以适应不同子领域的流信息。&lt;h4&gt;主要发现&lt;/h4&gt;尽管不同子领域的空气动力学数据遵循不同方程，但联合训练的模型比单独训练的模型表现更好，表明这些数据相互补充，帮助模型学习更好的流场表示。&lt;h4&gt;结论&lt;/h4&gt;UniField作为通用流场表示模型具有潜力，为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;物体表面压力场的空气动力学模拟对许多工程问题至关重要。近年来，深度神经网络已成为传统计算成本高昂的CFD模拟的高效替代方案，用于建模表面压力场。然而，数据稀缺性仍然是一个基本挑战，限制了神经网络的应用。为了解决这一限制，我们提出整合多个子领域的空气动力学数据进行联合训练，以学习更通用的流场表示。我们整合了五个涵盖不同领域的数据集，包括汽车、火车、飞机和一般形状。面对不同领域间的显著数据差异，我们提出了UniField，它采用领域无关的Transformer模块提取通用点云特征，并定制领域特定的流条件适配器以适应不同子领域的流信息。尽管不同子领域的空气动力学数据通常遵循不同的方程，但我们比较了在所有数据上联合训练的模型与在单个数据集上分别训练的模型，发现联合训练的模型通常表现更好。这表明这些数据相互补充，帮助模型学习更好的流场表示。这些结果突显了UniField作为通用流场表示模型的潜力，为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aerodynamic simulation of the surface pressure field around objects iscrucial for many engineering problems. In recent years, deep neural networkshave emerged as an efficient alternative to traditional, computationallyexpensive CFD simulations for modeling surface pressure fields. However, datascarcity remains a fundamental challenge, limiting the application of neuralnetworks. To address this limitation, we propose to integrate aerodynamic datafrom multiple subfields and conduct joint training to learn more general fieldrepresentations. We consolidate five different datasets covering variousfields, including automobiles, trains, aircraft, and general shapes. Facingsignificant data differences across different domains, we propose UniField,which employs a domain-agnostic Transformer module to extract general pointcloud features and customizes domain-specific flow-conditioned adapters toadapt to the flow information in different subfields. Despite the fact thataerodynamic data from different subfields are typically governed by differentequations, we compare models trained jointly on all data with those trainedseparately on individual datasets and find that the jointly-trained modelcommonly demonstrates better performance. This indicates that these datacomplement each other to help the model learn better flow fieldrepresentations. These results highlight the potential of UniField as auniversal flow field representation model and lay the foundation for broaderapplications of neural networks in aerodynamic analysis.</description>
      <author>example@mail.com (Junhong Zou, Zhenxu Sun, Yueqing Wang, Wei Qiu, Zhaoxiang Zhang, Zhen Lei, Xiangyu Zhu)</author>
      <guid isPermaLink="false">2510.24106v2</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
  <item>
      <title>Linearized Optimal Transport for Analysis of High-Dimensional Point-Cloud and Single-Cell Data</title>
      <link>http://arxiv.org/abs/2510.22033v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种线性最优传输(LOT)框架，用于处理单细胞技术生成的高维点云数据，实现了预测准确性、可解释性和生成建模的统一。&lt;h4&gt;背景&lt;/h4&gt;单细胞技术生成高维点云数据，能详细表征复杂患者状态和治疗反应。每个患者由不规则点云而非简单向量表示，难以直接量化和比较个体间生物学差异。现有非线性方法虽准确但如同黑箱，生物学可解释性差。&lt;h4&gt;目的&lt;/h4&gt;开发一种既能保持预测准确性又具有生物学可解释性的方法，解决单细胞点云数据分析中的挑战。&lt;h4&gt;方法&lt;/h4&gt;适配线性最优传输(LOT)框架，将不规则点云嵌入固定维度欧几里得空间，同时保留分布结构。这种嵌入提供有原则的线性表示，形成患者间的配准，支持直接比较细胞分布。&lt;h4&gt;主要发现&lt;/h4&gt;LOT实现了：(i) COVID-19患者状态的准确且可解释的分类，分类器权重映射回特定标志物和空间区域；(ii) 患者来源类器官的合成数据生成。LOT形心产生平均细胞谱，支持药物相互作用测试。&lt;h4&gt;结论&lt;/h4&gt;LOT作为统一框架连接了预测性能、可解释性和生成建模，通过将异质点云转换为结构化嵌入，为理解高维生物系统中的免疫变异和治疗效应开辟新机会。&lt;h4&gt;翻译&lt;/h4&gt;单细胞技术生成细胞的高维点云，能够详细表征复杂的患者状态和治疗反应。然而每个患者由不规则点云而非简单向量表示，使得难以直接量化和比较个体间的生物学差异。诸如核方法和神经网络等非线性方法能达到预测准确性，但如同黑箱，提供很少的生物学可解释性。为解决这些局限性，我们将线性最优传输(LOT)框架适配到这一场景，将不规则点云嵌入到固定维度的欧几里得空间，同时保留分布结构。这种嵌入提供了有原则的线性表示，保留了最优传输几何，同时支持下游分析。它还形成了任意两个患者之间的配准，使其能够直接比较细胞分布。在此空间中，LOT实现了：(i) COVID-19患者状态的准确且可解释的分类，其中分类器权重映射回驱动预测的特定标志物和空间区域；(ii) 患者来源类器官的合成数据生成，利用LOT嵌入的线性性。LOT形心产生平均细胞谱，代表组合条件或样本，支持药物相互作用测试。这些结果共同确立了LOT作为连接预测性能、可解释性和生成建模的统一框架。通过将异质点云转换为可直接追溯到原始数据的结构化嵌入，LOT为理解高维生物系统中的免疫变异和治疗效应开辟了新机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell technologies generate high-dimensional point clouds of cells,enabling detailed characterization of complex patient states and treatmentresponses. Yet each patient is represented by an irregular point cloud ratherthan a simple vector, making it difficult to directly quantify and comparebiological differences between individuals. Nonlinear methods such as kernelsand neural networks achieve predictive accuracy but act as black boxes,offering little biological interpretability.  To address these limitations, we adapt the Linear Optimal Transport (LOT)framework to this setting, embedding irregular point clouds into afixed-dimensional Euclidean space while preserving distributional structure.This embedding provides a principled linear representation that preservesoptimal transport geometry while enabling downstream analysis. It also forms aregistration between any two patients, enabling direct comparison of theircellular distributions. Within this space, LOT enables: (i) \textbf{accurateand interpretable classification} of COVID-19 patient states, where classifierweights map back to specific markers and spatial regions driving predictions;and (ii) \textbf{synthetic data generation} for patient-derived organoids,exploiting the linearity of the LOT embedding. LOT barycenters yield averagedcellular profiles representing combined conditions or samples, supporting druginteraction testing.  Together, these results establish LOT as a unified framework that bridgespredictive performance, interpretability, and generative modeling. Bytransforming heterogeneous point clouds into structured embeddings directlytraceable to the original data, LOT opens new opportunities for understandingimmune variation and treatment effects in high-dimensional biological systems.</description>
      <author>example@mail.com (Tianxiang Wang, Yingtong Ke, Dhananjay Bhaskar, Smita Krishnaswamy, Alexander Cloninger)</author>
      <guid isPermaLink="false">2510.22033v2</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Clone Deterministic 3D Worlds with Geometrically-Regularized World Models</title>
      <link>http://arxiv.org/abs/2510.26782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种几何正则化世界模型(GRWM)，通过改进表示学习来提高世界模型的性能，特别是在长期预测任务中表现出更好的保真度和稳定性。&lt;h4&gt;背景&lt;/h4&gt;世界模型是一种内部模型，用于模拟世界的发展，基于过去的观察和行动预测智能体及其环境的未来。准确的世界模型对智能体在复杂环境中有效思考、规划和推理至关重要。然而，当前世界模型在长期范围内表现脆弱且会退化。&lt;h4&gt;目的&lt;/h4&gt;研究仅通过改进表示学习是否能显著提高世界模型的性能，并构建一个能够完全克隆并拟合确定性3D世界的模型。&lt;h4&gt;方法&lt;/h4&gt;提出几何正则化世界模型(GRWM)，强制自然感觉轨迹上的连续点在潜在表示空间中保持接近，从而学习与环境真实拓扑紧密对齐的潜在表示。&lt;h4&gt;主要发现&lt;/h4&gt;GRWM显著提高了确定性3D环境和长期预测任务中的滚动保真度和稳定性，其优势源于学习具有优越几何结构的潜在流形。GRWM是即插即用的，只需最小架构修改，可随轨迹长度扩展，且兼容各种潜在生成主干网络。&lt;h4&gt;结论&lt;/h4&gt;改进表示学习是构建健壮世界模型的直接且有用的途径，无需扩大动态模块即可提供可靠的长期预测。&lt;h4&gt;翻译&lt;/h4&gt;世界模型是一种内部模型，用于模拟世界的发展。基于过去的观察和行动，它预测智能体及其环境的未来。准确的世界模型对于智能体在复杂动态环境中有效思考、规划和推理至关重要。尽管进展迅速，但当前世界模型仍然脆弱，且在长期范围内会退化。我们认为，一个主要原因是表示质量：外部输入（如图像）是高维度的，且有损或纠缠的潜在表示使动态学习变得不必要地困难。因此，我们研究仅通过改进表示学习是否能显著提高世界模型的性能。在本文中，我们通过解决一个基本但尚未解决的问题，朝着构建真正准确的世界模型迈出了一步：构建一个能够完全克隆并拟合确定性3D世界的模型。我们提出了几何正则化世界模型(GRWM)，强制自然感觉轨迹上的连续点在潜在表示空间中保持接近。这种方法产生了显著改进的潜在表示，与环境真实拓扑紧密对齐。GRWM是即插即用的，只需要最小的架构修改，可随轨迹长度扩展，并且兼容各种潜在生成主干网络。在确定性3D环境和长期预测任务中，GRWM显著提高了滚动保真度和稳定性。分析表明，其优势源于学习具有优越几何结构的潜在流形。这些发现支持一个明确的结论：改进表示学习是构建健壮世界模型的直接且有用的途径，无需扩大动态模块即可提供可靠的长期预测。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是如何构建能够准确克隆确定性3D世界的世界模型，特别是解决长期预测中误差累积导致轨迹偏离现实的问题。这个问题在现实中非常重要，因为世界模型是强化学习、机器人规划和游戏内容生成等应用的核心工具，而当前世界模型在长期预测方面表现脆弱，无法满足需要精确模拟环境的实际应用需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过实验发现，当世界模型直接使用真实底层状态时预测效果极佳，但使用标准VAE的潜在空间时性能急剧下降，这使他们认识到表示质量是主要瓶颈。他们借鉴了对比学习(特别是时序对比学习)和几何正则化在3D物体表示学习中的应用，将其扩展到3D环境建模领域。具体设计上，他们提出了结合时序上下文架构和时序对比正则化的GRWM方法，确保潜在空间结构与环境的真实状态流形一致。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过改进表示学习来提升世界模型的性能，使潜在空间结构与环境的真实状态流形保持一致。整体实现流程包括：1)使用因果编码器处理连续观察序列生成潜在表示；2)结合重构损失、KL散度项、时序缓慢损失(确保连续状态在潜在空间中接近)和潜在均匀性损失(防止特征坍塌)进行训练；3)将训练好的潜在表示作为输入提供给各种动力学模型进行状态转换学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)重新定义问题，从开放世界生成转向确定性环境的精确复制；2)提出'表示质量优先'的观点，与当前侧重改进动力学模型的主流思路不同；3)引入几何正则化方法，确保潜在空间结构与环境的真实拓扑一致；4)设计插件式组件，可无缝集成到现有模型中；5)显著提高长期轨迹预测的稳定性和准确性。相比之前工作，GRWM更注重表示质量而非复杂动力学模型，结合了时序上下文和几何正则化，且完全无监督，不需要真实状态标签。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过几何正则化改进潜在表示学习，GRWM显著提高了确定性3D世界模型的长期预测准确性和稳定性，证明了表示质量对构建可靠世界模型的关键作用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A world model is an internal model that simulates how the world evolves.Given past observations and actions, it predicts the future of both theembodied agent and its environment. Accurate world models are essential forenabling agents to think, plan, and reason effectively in complex, dynamicsettings. Despite rapid progress, current world models remain brittle anddegrade over long horizons. We argue that a central cause is representationquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy orentangled latents make dynamics learning unnecessarily hard. We therefore askwhether improving representation learning alone can substantially improveworld-model performance. In this work, we take a step toward building a trulyaccurate world model by addressing a fundamental yet open problem: constructinga model that can fully clone and overfit to a deterministic 3D world. Wepropose Geometrically-Regularized World Models (GRWM), which enforces thatconsecutive points along a natural sensory trajectory remain close in latentrepresentation space. This approach yields significantly improved latentrepresentations that align closely with the true topology of the environment.GRWM is plug-and-play, requires only minimal architectural modification, scaleswith trajectory length, and is compatible with diverse latent generativebackbones. Across deterministic 3D settings and long-horizon prediction tasks,GRWM significantly increases rollout fidelity and stability. Analyses show thatits benefits stem from learning a latent manifold with superior geometricstructure. These findings support a clear takeaway: improving representationlearning is a direct and useful path to robust world models, deliveringreliable long-horizon predictions without enlarging the dynamics module.</description>
      <author>example@mail.com (Zaishuo Xia, Yukuan Lu, Xinyi Li, Yifan Xu, Yubei Chen)</author>
      <guid isPermaLink="false">2510.26782v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens</title>
      <link>http://arxiv.org/abs/2510.26372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UniTok-Audio框架，解决了音频生成模型在质量和泛化能力方面的挑战，实现了统一的音频生成任务处理。&lt;h4&gt;背景&lt;/h4&gt;生成式建模在文本、图像和音频领域取得显著成功，但音频生成模型仍面临音频质量和跨任务泛化能力的挑战，导致开发冗余、性能不一致和扩展性有限。&lt;h4&gt;目的&lt;/h4&gt;提出UniTok-Audio，一个可扩展且可扩展的统一音频生成任务框架，解决现有音频生成模型的碎片化问题。&lt;h4&gt;方法&lt;/h4&gt;1) 提取条件的连续特征，以自回归方式生成目标音频的离散令牌；2) 使用特殊任务标识符令牌统一不同任务的学习模式；3) 开发包含声学和语义分支的双流音频编解码器实现高保真波形重构。&lt;h4&gt;主要发现&lt;/h4&gt;UniTok-Audio在五个时间对齐任务（语音恢复、目标说话人提取、语音分离、语音转换和语言查询音频源分离）上与最先进的特定任务或多任务系统相比具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;UniTok-Audio提供了一个统一的音频生成框架，解决了现有模型的碎片化问题，作者将开源代码库以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;生成式建模最近在文本、图像和音频领域取得了显著成功，展示了统一表征学习的强大能力。然而，音频生成模型在音频质量和跨任务泛化能力方面仍面临挑战。这种碎片化导致了冗余的开发工作、不一致的性能和有限的扩展性。为了解决这些问题，我们提出了UniTok-Audio，一个可扩展且可扩展的统一音频生成任务框架。具体而言，1) UniTok-Audio以自回归方式提取条件的连续特征，生成目标音频的离散令牌；2) 特殊的任务标识符令牌在单一框架中统一了多种任务的学习模式；3) 开发了包含声学和语义分支的双流音频编解码器，用于高保真波形重构。实验结果表明，UniTok-Audio在五个时间对齐任务（语音恢复、目标说话人提取、语音分离、语音转换和语言查询音频源分离）上与最先进的特定任务或多任务系统相比具有竞争力的性能。为了促进未来的研究，我们将开源我们的代码库。我们工作的演示页面可以在以下网址找到：https://alibaba.github.io/unified-audio。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling has recently achieved remarkable success across text,image, and audio domains, demonstrating powerful capabilities for unifiedrepresentation learning. However, audio generation models still face challengesin terms of audio quality and generalization ability across tasks. Thisfragmentation results in redundant development efforts, inconsistentperformance, and limited extensibility. To address these issues, we propose\textbf{UniTok-Audio}, a scalable and extensible framework for unified audiogeneration tasks. Specifically, 1) UniTok-Audio extracts continuous feature ofconditions to generates discrete tokens of target audio in an autoregressivemanner; 2) a special task identifier token unifies different learning patternsof multiple tasks in a single framework; 3) a dual-stream audio codec involvingacoustic and semantic branch is developed for high-fidelity waveformreconstruction. Experimental results demonstrate that UniTok-Audio achievescompetitive performance in comparation with state-of-the-art task-specific ormulti-task systems across five time-aligned tasks: speech restoration, targetspeaker extraction, speech separation, voice conversion, and language-queriedaudio source separation. To foster future research, we will open-source ourcodebase. The demo page of our work can be found here:https://alibaba.github.io/unified-audio.</description>
      <author>example@mail.com (Chengwei Liu, Haoyin Yan, Shaofei Xue, Xiaotao Liang, Yinghao Liu, Zheng Xue, Gang Song, Boyang Zhou)</author>
      <guid isPermaLink="false">2510.26372v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens</title>
      <link>http://arxiv.org/abs/2510.26302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对CLIP模型在组合推理方面的局限性提出了一种基于标记的因果表示学习框架，揭示了CLIP在处理对象、属性和关系组合时的脆弱性根源，并为改进模型提供了理论指导。&lt;h4&gt;背景&lt;/h4&gt;CLIP模型通过在共享嵌入空间中对齐图像和文本实现了强大的跨模态泛化能力，但在处理对象、属性和关系的组合推理方面持续失败，其行为类似于词袋匹配器。先前的因果解释通常将文本建模为单个向量，掩盖了标记级别的结构，无法解释提示敏感性和对困难样本失败等核心现象。&lt;h4&gt;目的&lt;/h4&gt;解决现有CLIP模型在组合推理方面的局限性，提供对标记级别结构的更好理解，并解释模型在提示敏感性和困难样本处理上的失败原因。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于标记的因果表示学习（CRL）框架，该框架基于顺序的语言标记结构因果模型（SCM）。将块可识别性理论扩展到标记化文本，证明CLIP的对比目标可以在句子级和标记级SCM下恢复模态不变潜在变量。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了CLIP的组合脆弱性源于'组合不可识别性'现象。存在伪最优文本编码器，它们可以实现完美的模态不变对齐，但对SWAP、REPLACE和ADD操作在原子概念上明显不敏感，因此无法区分正确的标题和困难样本。分析还表明语言侧的不可识别性与视觉侧的失败通过模态差距相联系，迭代组合运算会增加问题难度。&lt;h4&gt;结论&lt;/h4&gt;标记级别的因果表示学习框架能够解释CLIP在组合推理方面的失败，这些发现为改进负样本挖掘策略和提高模型组合推理能力提供了理论依据。&lt;h4&gt;翻译&lt;/h4&gt;对比语言-图像预训练（CLIP）通过在共享嵌入空间中对齐图像和文本，提供了强大的跨模态泛化能力，但在处理对象、属性和关系的组合推理方面持续失败，其行为常常类似于词袋匹配器。先前的因果解释通常将文本建模为单个向量，掩盖了标记级别的结构，无法解释提示敏感性和对困难样本失败等核心现象。我们通过基于标记的因果表示学习（CRL）框架解决了这一差距，该框架基于顺序的语言标记结构因果模型（SCM）。我们的理论将块可识别性扩展到标记化文本，证明CLIP的对比目标可以在句子级和标记级SCM下恢复模态不变的潜在变量。关键是，标记粒度为CLIP的组合脆弱性提供了第一个原则性解释：组合不可识别性。我们展示了伪最优文本编码器的存在，这些编码器可以实现完美的模态不变对齐，但对SWAP、REPLACE和ADD操作在原子概念上明显不敏感，因此尽管与真正最优编码器优化相同的训练目标，却无法区分正确的标题和困难样本。分析进一步将语言侧的不可识别性与视觉侧的失败通过模态差距联系起来，并展示了迭代组合运算如何增加难度，从而改进了负样本挖掘策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Language-Image Pre-training (CLIP) delivers strong cross modalgeneralization by aligning images and texts in a shared embedding space, yet itpersistently fails at compositional reasoning over objects, attributes, andrelations often behaving like a bag-of-words matcher. Prior causal accountstypically model text as a single vector, obscuring token-level structure andleaving core phenomena-such as prompt sensitivity and failures on hardnegatives unexplained. We address this gap with a token-aware causalrepresentation learning (CRL) framework grounded in a sequential,language-token SCM. Our theory extends block identifiability to tokenized text,proving that CLIP's contrastive objective can recover the modal-invariantlatent variable under both sentence-level and token-level SCMs. Crucially,token granularity yields the first principled explanation of CLIP'scompositional brittleness: composition nonidentifiability. We show theexistence of pseudo-optimal text encoders that achieve perfect modal-invariantalignment yet are provably insensitive to SWAP, REPLACE, and ADD operationsover atomic concepts, thereby failing to distinguish correct captions from hardnegatives despite optimizing the same training objective as true-optimalencoders. The analysis further links language-side nonidentifiability tovisual-side failures via the modality gap and shows how iterated compositionoperators compound hardness, motivating improved negative mining strategies.</description>
      <author>example@mail.com (Ziliang Chen, Tianang Xiao, Jusheng Zhang, Yongsen Zheng, Xipeng Chen)</author>
      <guid isPermaLink="false">2510.26302v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations with LLMs</title>
      <link>http://arxiv.org/abs/2510.26178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了ReaKase-8B框架，通过整合法律事实、法律问题、法律关系三元组和法律推理，显著提高了法律案例检索的性能。&lt;h4&gt;背景&lt;/h4&gt;法律案例检索(LCR)是现实世界法律决策的基石，现有方法主要依赖传统的词汇模型和预训练语言模型编码法律文本，但忽略了法律实体间的关系以及法律事实和法律问题如何导致司法决策的推理过程。&lt;h4&gt;目的&lt;/h4&gt;将法律关系信息和关键推理过程整合到精确的案例嵌入中，以提高案例检索的准确性，并提出ReaKase-8B框架有效利用这些信息进行法律案例检索。&lt;h4&gt;方法&lt;/h4&gt;ReaKase-8B设计了上下文法律案例表示学习范式，使用微调的大语言模型，整合提取的法律事实、法律问题、法律关系三元组和法律推理信息。&lt;h4&gt;主要发现&lt;/h4&gt;在COLIEE 2022和COLIEE 2023两个基准数据集上的实验表明，知识和推理增强的嵌入显著提高了检索性能，超越了基线模型。&lt;h4&gt;结论&lt;/h4&gt;集成法律推理到法律案例检索系统中具有巨大潜力，ReaKase-8B框架展示了这种整合的有效性，代码已在GitHub发布。&lt;h4&gt;翻译&lt;/h4&gt;该摘要已按要求翻译为中文，提取了论文的核心要素，包括背景、目的、方法、发现和结论，并以JSON格式组织。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Legal case retrieval (LCR) is a cornerstone of real-world legal decisionmaking, as it enables practitioners to identify precedents for a given querycase. Existing approaches mainly rely on traditional lexical models andpretrained language models to encode the texts of legal cases. Yet there arerich information in the relations among different legal entities as well as thecrucial reasoning process that uncovers how legal facts and legal issues canlead to judicial decisions. Such relational reasoning process reflects thedistinctive characteristics of each case that can distinguish one from another,mirroring the real-world judicial process. Naturally, incorporating suchinformation into the precise case embedding could further enhance the accuracyof case retrieval. In this paper, a novel ReaKase-8B framework is proposed toleverage extracted legal facts, legal issues, legal relation triplets and legalreasoning for effective legal case retrieval. ReaKase-8B designs an in-contextlegal case representation learning paradigm with a fine-tuned large languagemodel. Extensive experiments on two benchmark datasets from COLIEE 2022 andCOLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddingssubstantially improve retrieval performance over baseline models, highlightingthe potential of integrating legal reasoning into legal case retrieval systems.The code has been released on https://github.com/yanran-tang/ReaKase-8B.</description>
      <author>example@mail.com (Yanran Tang, Ruihong Qiu, Xue Li, Zi Huang)</author>
      <guid isPermaLink="false">2510.26178v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment</title>
      <link>http://arxiv.org/abs/2510.26157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025 (main)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MolBridge，一种基于亚结构感知对齐的新型分子-文本学习框架，通过增强分子亚结构和化学短语之间的细粒度对齐，有效提升了分子表示学习的性能。&lt;h4&gt;背景&lt;/h4&gt;分子和文本表示学习越来越受到关注，因为它有潜力增强对化学信息的理解。然而，现有模型往往难以捕捉分子及其描述之间的细微差异。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型缺乏学习分子亚结构和化学短语之间细粒度对齐能力的问题，提高分子表示学习的准确性。&lt;h4&gt;方法&lt;/h4&gt;通过从分子亚结构和化学短语中衍生的额外对齐信号来增强原始分子-描述对，采用亚结构感知对比学习，并结合自我完善机制来过滤有噪声的对齐信号。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MolBridge能够有效捕获细粒度对应关系，并在广泛的分子基准测试中优于最先进的基线模型。&lt;h4&gt;结论&lt;/h4&gt;亚结构感知对齐在分子-文本学习中具有重要意义，MolBridge框架为分子表示学习提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;分子和文本表示学习因其增强化学信息理解的潜力而日益受到关注。然而，现有模型往往难以捕捉分子及其描述之间的细微差异，因为它们缺乏学习分子亚结构和化学短语之间细粒度对齐的能力。为解决这一局限性，我们引入了MolBridge，一种基于亚结构感知对齐的新型分子-文本学习框架。具体而言，我们通过从分子亚结构和化学短语中衍生的额外对齐信号来增强原始分子-描述对。为了有效学习这些丰富的对齐信息，MolBridge采用亚结构感知对比学习，并结合一种自我完善机制来过滤有噪声的对齐信号。实验结果表明，MolBridge能够有效捕获细粒度对应关系，并在广泛的分子基准测试中优于最先进的基线模型，突显了亚结构感知对齐在分子-文本学习中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecule and text representation learning has gained increasing interest dueto its potential for enhancing the understanding of chemical information.However, existing models often struggle to capture subtle differences betweenmolecules and their descriptions, as they lack the ability to learnfine-grained alignments between molecular substructures and chemical phrases.To address this limitation, we introduce MolBridge, a novel molecule-textlearning framework based on substructure-aware alignments. Specifically, weaugment the original molecule-description pairs with additional alignmentsignals derived from molecular substructures and chemical phrases. Toeffectively learn from these enriched alignments, MolBridge employssubstructure-aware contrastive learning, coupled with a self-refinementmechanism that filters out noisy alignment signals. Experimental results showthat MolBridge effectively captures fine-grained correspondences andoutperforms state-of-the-art baselines on a wide range of molecular benchmarks,highlighting the significance of substructure-aware alignment in molecule-textlearning.</description>
      <author>example@mail.com (Hyuntae Park, Yeachan Kim, SangKeun Lee)</author>
      <guid isPermaLink="false">2510.26157v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization</title>
      <link>http://arxiv.org/abs/2510.26068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新颖的机器学习范式，超越了传统的参数优化方法。它将模型本身视为可变形的几何实体，通过优化流形上的度量张量场来动态塑造模型空间的几何结构。&lt;h4&gt;背景&lt;/h4&gt;传统的机器学习方法在固定的几何空间内搜索最优参数，而本文提出了一种不同的思路。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的机器学习范式，通过优化度量张量场来动态调整模型的几何结构，从而提高模型的表示能力和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;构建了一个变分框架，其损失函数平衡了数据保真度和流形的内在几何复杂度。为解决这个无限维优化问题的计算挑战，引入了一种基于离散微分几何的实用方法，将连续流形离散化为三角形网格，并通过边长参数化度量张量。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析揭示了该框架与广义相对论中的爱因斯坦-希尔伯特作用之间的深刻类比，为'数据驱动几何'概念提供了优雅的物理解释。即使拓扑结构固定，度量优化也比固定几何的模型具有更强的表达能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作为构建能够自主进化其几何和拓扑结构的完全动态'元学习器'奠定了坚实基础，并在科学模型发现和鲁棒表示学习等领域具有广阔的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种超越传统参数优化的机器学习新范式。与在固定几何空间内搜索最优参数的传统方法不同，我们的核心思想是将模型本身视为可变形的几何实体。具体而言，我们在具有预定义拓扑的流形上优化度量张量场，从而动态塑造模型空间的几何结构。为此，我们构建了一个变分框架，其损失函数仔细平衡了数据保真度与流形的内在几何复杂性。前者确保模型能有效解释观测数据，而后者则作为正则化项，对过度弯曲或不规则的几何结构进行惩罚，以鼓励更简单的模型并防止过拟合。为解决这个无限维优化问题的计算挑战，我们引入了一种基于离散微分几何的实用方法：将连续流形离散化为三角形网格，并通过边长参数化度量张量，从而能够使用自动微分工具进行高效优化。理论分析揭示了我们的框架与广义相对论中爱因斯坦-希尔伯特作用之间的深刻类比，为'数据驱动几何'概念提供了优雅的物理解释。我们进一步认为，即使拓扑结构固定，度量优化也比具有固定几何的模型具有更强的表达能力。这项工作为构建能够自主进化其几何和拓扑结构的完全动态'元学习器'奠定了坚实基础，并指向了科学模型发现和鲁棒表示学习等领域的广泛应用前景。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决传统机器学习方法的局限性：模型在固定的几何空间中寻找最优参数，而非优化模型空间本身的几何结构。当数据的本质几何结构是非欧几里得、弯曲或具有复杂拓扑时，传统方法可能导致效率低下、泛化能力弱或难以解释。这个问题的重要性在于，它限制了模型对数据内在结构的捕捉能力，阻碍了机器学习在处理复杂数据结构时的表现，同时也限制了我们对学习过程本质的理解。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者的思考始于对传统机器学习局限性的反思，提出不仅应优化参数点，还应优化塑造空间的几何结构本身。他借鉴了信息几何的启发，将参数化的概率分布视为微分流形，但超越了传统信息几何中静态度量的观念。作者设计了在固定拓扑流形上优化度量张量场的理论框架，构建了平衡数据保真度和几何复杂性的变分框架，并引入基于离散微分几何的实用计算方法。这项工作借鉴了信息几何、几何深度学习、生成模型和离散微分几何等领域的研究成果，但提出了全新的整合视角。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将机器学习模型视为可塑的几何实体，其学习过程表现为流形自身度量结构的自我优化，通过优化流形上的度量张量场来动态塑造模型空间的几何结构。实现流程包括：1)问题形式化，给定固定拓扑流形和观测数据集；2)设计变分框架，包含数据保真度项和几何复杂性项；3)将连续流形离散化为三角形网格，通过边长参数化度量张量；4)使用基于自动微分的优化算法进行迭代优化，通过投影梯度更新确保满足三角形不等式约束。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)从参数学习到结构学习的范式转变；2)将流形上的度量张量场本身作为优化目标；3)构建平衡数据保真度和几何复杂性的变分框架；4)基于离散微分几何的实用计算方法；5)揭示框架与广义相对论中爱因斯坦-希尔伯特作用的深刻联系。相比之前的工作，本文超越了传统信息几何中静态度量的观念，不同于几何深度学习中的固定几何假设，区别于传统生成模型中预设的简单潜在空间，也不同于传统非线性降维中固定的描述性流形，使几何结构成为学习过程的核心部分。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种机器学习新范式，通过优化流形上的度量张量场使模型能够动态塑造自身的几何结构，为构建完全自适应的'元学习器'奠定了理论基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a novel paradigm for machine learning that moves beyondtraditional parameter optimization. Unlike conventional approaches that searchfor optimal parameters within a fixed geometric space, our core idea is totreat the model itself as a malleable geometric entity. Specifically, weoptimize the metric tensor field on a manifold with a predefined topology,thereby dynamically shaping the geometric structure of the model space. Toachieve this, we construct a variational framework whose loss functioncarefully balances data fidelity against the intrinsic geometric complexity ofthe manifold. The former ensures the model effectively explains observed data,while the latter acts as a regularizer, penalizing overly curved or irregulargeometries to encourage simpler models and prevent overfitting. To address thecomputational challenges of this infinite-dimensional optimization problem, weintroduce a practical method based on discrete differential geometry: thecontinuous manifold is discretized into a triangular mesh, and the metrictensor is parameterized by edge lengths, enabling efficient optimization usingautomatic differentiation tools. Theoretical analysis reveals a profoundanalogy between our framework and the Einstein-Hilbert action in generalrelativity, providing an elegant physical interpretation for the concept of"data-driven geometry". We further argue that even with fixed topology, metricoptimization offers significantly greater expressive power than models withfixed geometry. This work lays a solid foundation for constructing fullydynamic "meta-learners" capable of autonomously evolving their geometry andtopology, and it points to broad application prospects in areas such asscientific model discovery and robust representation learning.</description>
      <author>example@mail.com (Di Zhang)</author>
      <guid isPermaLink="false">2510.26068v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis</title>
      <link>http://arxiv.org/abs/2510.26014v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025 workshop Learning from Time Series for  Health (TS4H)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于离散时间生存分析的双重专家混合框架，有效解决了患者异质性建模和风险预测适应性的挑战，在乳腺癌数据集上表现出色。&lt;h4&gt;背景&lt;/h4&gt;生存分析是建模直到感兴趣事件发生的时间的任务，广泛应用于临床和生物医学研究。主要挑战是如何建模患者异质性，同时使风险预测适应个体特征和时间动态性。&lt;h4&gt;目的&lt;/h4&gt;开发一个双重专家混合框架，用于离散时间生存分析，以有效处理患者异质性和时间动态性。&lt;h4&gt;方法&lt;/h4&gt;提出双重专家混合框架，结合特征编码器专家混合用于亚组感知的表示学习，以及风险专家混合利用患者特征和时间嵌入来捕获时间动态性。该设计可灵活集成到现有的基于深度学习的生存分析管道中。&lt;h4&gt;主要发现&lt;/h4&gt;在METABRIC和GBSG乳腺癌数据集上，该方法持续提高了性能，将时间依赖的C-index提升了最多0.04（在测试集上），当整合到Consurv框架中时获得了进一步的提升。&lt;h4&gt;结论&lt;/h4&gt;双重MoE框架能够有效处理患者异质性并适应时间动态性，在乳腺癌数据集上表现优异，且可以与现有框架结合使用。&lt;h4&gt;翻译&lt;/h4&gt;生存分析是一项建模直到感兴趣事件发生的时间的任务，广泛应用于临床和生物医学研究。一个关键挑战是建模患者异质性，同时使风险预测适应个体特征和时间动态性。我们提出了一个用于离散时间生存分析的双重专家混合框架。我们的方法结合了一个特征编码器专家混合，用于亚组感知的表示学习，以及一个风险专家混合，利用患者特征和时间嵌入来捕获时间动态性。这种双重MoE设计可以灵活地与现有的基于深度学习的生存分析管道集成。在METABRIC和GBSG乳腺癌数据集上，我们的方法持续提高了性能，将时间依赖的C-index提升了最多0.04（在测试集上），当整合到Consurv框架中时获得了进一步的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Survival analysis is a task to model the time until an event of interestoccurs, widely used in clinical and biomedical research. A key challenge is tomodel patient heterogeneity while also adapting risk predictions to bothindividual characteristics and temporal dynamics. We propose a dualmixture-of-experts (MoE) framework for discrete-time survival analysis. Ourapproach combines a feature-encoder MoE for subgroup-aware representationlearning with a hazard MoE that leverages patient features and time embeddingsto capture temporal dynamics. This dual-MoE design flexibly integrates withexisting deep learning based survival pipelines. On METABRIC and GBSG breastcancer datasets, our method consistently improves performance, boosting thetime-dependent C-index up to 0.04 on the test sets, and yields further gainswhen incorporated into the Consurv framework.</description>
      <author>example@mail.com (Hyeonjun Lee, Hyungseob Shin, Gunhee Nam, Hyeonsoo Lee)</author>
      <guid isPermaLink="false">2510.26014v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Predictive Coding Done Right for Mutual Information Estimation</title>
      <link>http://arxiv.org/abs/2510.25983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文指出InfoNCE作为互信息估计器的局限性，并提出改进方法InfoNCE-anchor，通过引入辅助锚点类实现更准确的互信息估计。研究还揭示了对比表示学习的真正价值在于学习结构化密度比，而非准确估计互信息。&lt;h4&gt;背景&lt;/h4&gt;InfoNCE目标最初用于对比表示学习，已成为互信息(MI)估计的流行选择，尽管它与MI的联系是间接的。&lt;h4&gt;目的&lt;/h4&gt;证明为什么InfoNCE不应被视为有效的MI估计器，并引入一个称为InfoNCE-anchor的简单修改，用于准确的MI估计。&lt;h4&gt;方法&lt;/h4&gt;通过引入一个辅助锚点类来修改InfoNCE，实现了一致的密度比估计，并产生了一个偏差显著减少的即插即用MI估计器。此外，使用适当的评分规则将框架推广，当使用对数评分时，InfoNCE-anchor作为特例被恢复。这个公式将一系列对比目标（包括NCE、InfoNCE和f-散度变体）统一在一个单一的原则性框架下。&lt;h4&gt;主要发现&lt;/h4&gt;使用对数评分的InfoNCE-anchor能实现最准确的MI估计；然而，在自监督表示学习实验中，锚点并未提高下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;对比表示学习受益的不是准确的MI估计本身，而是结构化密度比的学习。&lt;h4&gt;翻译&lt;/h4&gt;InfoNCE目标最初引入用于对比表示学习，尽管它与互信息(MI)的联系是间接的，但已成为MI估计的流行选择。在本文中，我们证明了为什么不应将InfoNCE视为有效的MI估计器，并介绍了一个简单的修改，我们称之为InfoNCE-anchor，用于准确的MI估计。我们的修改引入了一个辅助锚点类，实现了一致的密度比估计，并产生了一个偏差显著减少的即插即用MI估计器。除此之外，我们使用适当的评分规则推广了我们的框架，当使用对数评分时，InfoNCE-anchor作为特例被恢复。这个公式在单一原则性框架下统一了广泛的对比目标，包括NCE、InfoNCE和f-散度变体。从经验上看，我们发现使用对数评分的InfoNCE-anchor实现了最准确的MI估计；然而，在自监督表示学习实验中，我们发现锚点并未提高下游任务性能。这些发现证实了对比表示学习受益的不是准确的MI估计本身，而是结构化密度比的学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The InfoNCE objective, originally introduced for contrastive representationlearning, has become a popular choice for mutual information (MI) estimation,despite its indirect connection to MI. In this paper, we demonstrate whyInfoNCE should not be regarded as a valid MI estimator, and we introduce asimple modification, which we refer to as InfoNCE-anchor, for accurate MIestimation. Our modification introduces an auxiliary anchor class, enablingconsistent density ratio estimation and yielding a plug-in MI estimator withsignificantly reduced bias. Beyond this, we generalize our framework usingproper scoring rules, which recover InfoNCE-anchor as a special case when thelog score is employed. This formulation unifies a broad spectrum of contrastiveobjectives, including NCE, InfoNCE, and $f$-divergence variants, under a singleprincipled framework. Empirically, we find that InfoNCE-anchor with the logscore achieves the most accurate MI estimates; however, in self-supervisedrepresentation learning experiments, we find that the anchor does not improvethe downstream task performance. These findings corroborate that contrastiverepresentation learning benefits not from accurate MI estimation per se, butfrom the learning of structured density ratios.</description>
      <author>example@mail.com (J. Jon Ryu, Pavan Yeddanapudi, Xiangxiang Xu, Gregory W. Wornell)</author>
      <guid isPermaLink="false">2510.25983v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series</title>
      <link>http://arxiv.org/abs/2510.25785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为HiMAE的自监督学习方法，用于分析可穿戴传感器产生的生理时间序列数据，探索时间分辨率对预测效用的影响。&lt;h4&gt;背景&lt;/h4&gt;可穿戴传感器提供了丰富的生理时间序列数据，但支配这些数据预测效用的基本原理尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;测试时间分辨率作为表征学习基本轴的假设，即不同的临床和行为结果依赖于不同尺度的结构。&lt;h4&gt;方法&lt;/h4&gt;引入HiMAE（分层掩码自编码器），一种结合掩码自编码和分层卷积编码器-解码器的自监督框架。&lt;h4&gt;主要发现&lt;/h4&gt;HiMAE能产生多分辨率嵌入，系统评估哪些时间尺度携带预测信号；在分类、回归和生成基准测试中优于最先进模型；体积小几个数量级；可在智能手表上实现亚毫秒级推理，支持真正的边缘计算。&lt;h4&gt;结论&lt;/h4&gt;HiMAE既是高效的自监督学习方法，也是发现可穿戴健康数据中尺度敏感结构的有效工具。&lt;h4&gt;翻译&lt;/h4&gt;可穿戴传感器提供了丰富的生理时间序列，但支配其预测效用的原理仍然不清楚。我们假设时间分辨率是表征学习的基本轴，不同的临床和行为结果依赖于不同尺度的结构。为了测试这一分辨率假设，我们引入了HiMAE（分层掩码自编码器），这是一种结合掩码自编码和分层卷积编码器-解码器的自监督框架。HiMAE产生多分辨率嵌入，能够系统评估哪些时间尺度携带预测信号，将分辨率从超参数转变为可解释性的探针。在分类、回归和生成基准测试中，HiMAE始终优于将尺度压缩的最先进基础模型，同时体积小几个数量级。HiMAE是一种高效的表征学习器，足够紧凑，可以在手表上完全运行，在智能手表类CPU上实现亚毫秒级推理，实现真正的边缘推理。总之，这些贡献使HiMAE既成为高效的自监督学习方法，也成为发现可穿戴健康中尺度敏感结构的发现工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wearable sensors provide abundant physiological time series, yet theprinciples governing their predictive utility remain unclear. We hypothesizethat temporal resolution is a fundamental axis of representation learning, withdifferent clinical and behavioral outcomes relying on structure at distinctscales. To test this resolution hypothesis, we introduce HiMAE (HierarchicalMasked Autoencoder), a self supervised framework that combines maskedautoencoding with a hierarchical convolutional encoder decoder. HiMAE producesmulti resolution embeddings that enable systematic evaluation of which temporalscales carry predictive signal, transforming resolution from a hyperparameterinto a probe for interpretability. Across classification, regression, andgenerative benchmarks, HiMAE consistently outperforms state of the artfoundation models that collapse scale, while being orders of magnitude smaller.HiMAE is an efficient representation learner compact enough to run entirely onwatch, achieving sub millisecond inference on smartwatch class CPUs for trueedge inference. Together, these contributions position HiMAE as both anefficient self supervised learning method and a discovery tool for scalesensitive structure in wearable health.</description>
      <author>example@mail.com (Simon A. Lee, Cyrus Tanade, Hao Zhou, Juhyeon Lee, Megha Thukral, Minji Han, Rachel Choi, Md Sazzad Hissain Khan, Baiying Lu, Migyeong Gwak, Mehrab Bin Morshed, Viswam Nathan, Md Mahbubur Rahman, Li Zhu, Subramaniam Venkatraman, Sharanya Arcot Desai)</author>
      <guid isPermaLink="false">2510.25785v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education</title>
      <link>http://arxiv.org/abs/2510.26402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Autograder+是一个创新的自动评分系统，通过AI驱动的反馈、语义聚类和交互式可视化，将自动评分从纯总结性过程转变为形成性学习体验，减轻教师工作量的同时支持有针对性的教学并促进更强的学习成果。&lt;h4&gt;背景&lt;/h4&gt;编程教育的快速增长已经超过了传统评估工具的发展，使教师难以提供有意义、可扩展的反馈。传统的自动评分器虽然高效，但作为黑盒系统仅返回通过/失败结果，很少提供关于学生思维或学习需求的见解。&lt;h4&gt;目的&lt;/h4&gt;将自动评分从纯总结性过程转变为形成性学习体验，为教师提供更有效的评估工具，同时为学生提供更有价值的反馈。&lt;h4&gt;方法&lt;/h4&gt;引入两个关键功能：1) 使用微调的大语言模型自动生成反馈；2) 可视化学生代码提交以发现学习模式。模型经过精心筛选的学生代码和专家反馈进行微调，确保教育对齐和上下文感知的指导。系统支持提示池，允许教师通过选择的提示模板指导反馈风格。&lt;h4&gt;主要发现&lt;/h4&gt;在来自多个编程任务的600多个学生提交的评估中，系统生成的反馈与教师评论具有很强的语义一致性。基于1000个带注释的提交训练的对比学习代码嵌入能够基于功能和方法的相似性将解决方案分组为有意义的集群。&lt;h4&gt;结论&lt;/h4&gt;通过整合AI驱动的反馈、语义聚类和交互式可视化，Autograder+减轻了教师的工作量，同时支持有针对性的教学并促进更强的学习成果。&lt;h4&gt;翻译&lt;/h4&gt;编程教育的快速增长已经超过了传统评估工具的发展，使教师难以提供有意义、可扩展的反馈。传统的自动评分器虽然高效，但作为黑盒系统仅返回通过/失败结果，很少提供关于学生思维或学习需求的见解。Autograder+旨在将自动评分从纯总结性过程转变为形成性学习体验。它引入了两个关键功能：使用微调的大语言模型自动生成反馈，以及可视化学生代码提交以发现学习模式。该模型经过精心筛选的学生代码和专家反馈进行微调，确保教育对齐、上下文感知的指导。在来自多个编程任务的600多个学生提交的评估中，系统生成的反馈与教师评论具有很强的语义一致性。对于可视化功能，基于1000个带注释的提交训练的对比学习代码嵌入能够基于功能和方法的相似性将解决方案分组为有意义的集群。该系统还支持提示池，允许教师通过选择的提示模板指导反馈风格。通过整合AI驱动的反馈、语义聚类和交互式可视化，Autograder+减轻了教师的工作量，同时支持有针对性的教学并促进更强的学习成果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of programming education has outpaced traditional assessmenttools, leaving faculty with limited means to provide meaningful, scalablefeedback. Conventional autograders, while efficient, act as black-box systemsthat simply return pass/fail results, offering little insight into studentthinking or learning needs.  Autograder+ is designed to shift autograding from a purely summative processto a formative learning experience. It introduces two key capabilities:automated feedback generation using a fine-tuned Large Language Model, andvisualization of student code submissions to uncover learning patterns. Themodel is fine-tuned on curated student code and expert feedback to ensurepedagogically aligned, context-aware guidance.  In evaluation across 600 student submissions from multiple programming tasks,the system produced feedback with strong semantic alignment to instructorcomments. For visualization, contrastively learned code embeddings trained on1,000 annotated submissions enable grouping solutions into meaningful clustersbased on functionality and approach. The system also supports prompt-pooling,allowing instructors to guide feedback style through selected prompt templates.  By integrating AI-driven feedback, semantic clustering, and interactivevisualization, Autograder+ reduces instructor workload while supportingtargeted instruction and promoting stronger learning outcomes.</description>
      <author>example@mail.com (Vikrant Sahu, Gagan Raj Gupta, Raghav Borikar, Nitin Mane)</author>
      <guid isPermaLink="false">2510.26402v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.26241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究揭示了当前视觉语言模型在视频时间信息理解上的显著缺陷，提出了一个简单但有效的评估方法——时间方向判断(AoT)。&lt;h4&gt;背景&lt;/h4&gt;现代视觉语言模型在许多多模态任务中表现出色，但它们对视频中时间信息的理解仍然薄弱，并且这一点尚未得到充分评估。&lt;h4&gt;目的&lt;/h4&gt;探究视觉语言模型在理解视频时间信息方面的差距，通过判断视频播放方向(正向或反向)的挑战来评估其时间推理能力。&lt;h4&gt;方法&lt;/h4&gt;引入AoT-PsyPhyBENCH基准测试，使用经过心理物理学验证的刺激和行为基线，测试VLMs能否推断自然视频中的时间方向，并对开源和专有模型进行全面评估。&lt;h4&gt;主要发现&lt;/h4&gt;大多数模型表现接近随机水平，即使在物理不可逆过程和因果手动操作上表现最好的模型也远低于人类准确度，而人类几乎能立即识别这些内容。&lt;h4&gt;结论&lt;/h4&gt;当前多模态系统存在基本差距：虽然捕捉了丰富的视觉语义相关性，但缺乏时间连续性和因果理解所需的归纳偏置。&lt;h4&gt;翻译&lt;/h4&gt;现代视觉语言模型在许多多模态任务中表现出色，但它们对视频中时间信息的理解仍然薄弱，且这一点尚未得到充分评估。我们通过一个看似简单但具有揭示性的挑战来探究这一差距：判断时间方向(AoT)，即判断短视频片段是正向播放还是反向播放。我们引入了AoT-PsyPhyBENCH，这是一个经过心理物理学验证的基准测试，用于测试VLMs能否使用与人类相同的刺激和行为基线来推断自然视频中的时间方向。我们对开源和专有的、推理和非推理的VLMs进行了全面评估，发现大多数模型表现接近随机水平，即使在物理不可逆过程(如自由落体、扩散/爆炸)和因果手动操作(分割/添加)上表现最好的模型也远低于人类的准确度，而人类几乎能立即识别这些内容。这些结果突显了当前多模态系统的一个基本差距：虽然它们捕捉了丰富的视觉语义相关性，但缺乏时间连续性和因果理解所需的归纳偏置。我们发布了AoT-PsyPhyBENCH的代码和数据，以鼓励VLMs在物理和时间推理能力方面的进一步发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern vision-language models (VLMs) excel at many multimodal tasks, yettheir grasp of temporal information in video remains weak and, crucially,under-evaluated. We probe this gap with a deceptively simple but revealingchallenge: judging the arrow of time (AoT)-whether a short clip is playedforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validatedbenchmark that tests whether VLMs can infer temporal direction in naturalvideos using the same stimuli and behavioral baselines established for humans.Our comprehensive evaluation of open-weight and proprietary, reasoning andnon-reasoning VLMs reveals that most models perform near chance, and even thebest lag far behind human accuracy on physically irreversible processes (e.g.,free fall, diffusion/explosion) and causal manual actions (division/addition)that humans recognize almost instantly. These results highlight a fundamentalgap in current multimodal systems: while they capture rich visual-semanticcorrelations, they lack the inductive biases required for temporal continuityand causal understanding. We release the code and data for AoT-PsyPhyBENCH toencourage further progress in the physical and temporal reasoning capabilitiesof VLMs.</description>
      <author>example@mail.com (Shiho Matta, Lis Kanashiro Pereira, Peitao Han, Fei Cheng, Shigeru Kitazawa)</author>
      <guid isPermaLink="false">2510.26241v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing Environments</title>
      <link>http://arxiv.org/abs/2510.26148v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为STAR的边缘AI优化框架，用于在低功耗嵌入式设备上实现实时、节能的人类活动识别(HAR)系统。该系统通过简化的神经网络、自适应信号处理和硬件感知优化，在保持高准确率的同时显著提高了计算效率。&lt;h4&gt;背景&lt;/h4&gt;人类活动识别(HAR)通过Wi-Fi信道状态信息(CSI)提供了一种保护隐私、无需接触的感知方法，适用于智能家居、健康监测和移动物联网系统。然而，现有方法存在计算效率低下、高延迟和资源受限环境中可行性有限的问题。&lt;h4&gt;目的&lt;/h4&gt;开发STAR(Sensing Technology for Activity Recognition)框架，在低功耗嵌入式设备上实现实时、节能的HAR，解决现有方法在资源受限环境中的局限性。&lt;h4&gt;方法&lt;/h4&gt;集成轻量级神经网络架构、自适应信号处理和硬件感知联合优化；使用简化的基于门控循环单元(GRU)的循环神经网络，比传统LSTM减少33%参数；采用多阶段预处理管道(中值滤波、8阶巴特沃斯低通滤波和经验模态分解)；在配备NPU的Rockchip RV1126处理器上实现，并与ESP32-S3 CSI采集模块接口。&lt;h4&gt;主要发现&lt;/h4&gt;在七类活动上的平均识别准确率达93.52%，人体存在检测准确率达99.11%；使用仅97.6k参数的紧凑模型；INT8量化推理以33 MHz速度运行，仅占用8% CPU利用率，比CPU执行快六倍；系统具有亚秒级响应延迟和低功耗。&lt;h4&gt;结论&lt;/h4&gt;STAR系统确保了实时、隐私保护的HAR，为移动和普适计算环境提供了实用、可扩展的解决方案，有效解决了传统方法在资源受限嵌入式环境中的局限性。&lt;h4&gt;翻译&lt;/h4&gt;人类活动识别(HAR)通过Wi-Fi信道状态信息(CSI)提供了一种保护隐私、无需接触的感知方法，适用于智能家居、健康监测和移动物联网系统。然而，现有方法常遇到计算效率低下、高延迟以及在资源受限的嵌入式移动边缘环境中可行性有限的问题。本文提出了STAR(Sensing Technology for Activity Recognition)，这是一个边缘AI优化的框架，集成了轻量级神经网络架构、自适应信号处理和硬件感知的联合优化，以在低功耗嵌入式设备上实现实时、节能的HAR。STAR采用简化的基于门控循环单元(GRU)的循环神经网络，比传统LSTM模型减少33%的模型参数，同时保持有效的时间建模能力。采用结合中值滤波、8阶巴特沃斯低通滤波和经验模态分解(EMD)的多阶段预处理管道，用于去噪CSI振幅数据并提取时空特征。对于设备上部署，STAR在配备嵌入式神经处理单元(NPU)的Rockchip RV1126处理器上实现，与基于ESP32-S3的CSI采集模块接口。实验结果显示，在七类活动上的平均识别准确率为93.52%，人体存在检测为99.11%，使用紧凑的97.6k参数模型。INT8量化推理以33 MHz的处理速度运行，仅占用8%的CPU利用率，比基于CPU的执行速度快六倍。凭借亚秒级响应延迟和低功耗，该系统确保了实时、隐私保护的HAR，为移动和普适计算环境提供了实用、可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)presents a privacy-preserving, contactless sensing approach suitable for smarthomes, healthcare monitoring, and mobile IoT systems. However, existing methodsoften encounter computational inefficiency, high latency, and limitedfeasibility within resource-constrained, embedded mobile edge environments.This paper proposes STAR (Sensing Technology for Activity Recognition), anedge-AI-optimized framework that integrates a lightweight neural architecture,adaptive signal processing, and hardware-aware co-optimization to enablereal-time, energy-efficient HAR on low-power embedded devices. STARincorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neuralnetwork, reducing model parameters by 33% compared to conventional LSTM modelswhile maintaining effective temporal modeling capability. A multi-stagepre-processing pipeline combining median filtering, 8th-order Butterworthlow-pass filtering, and Empirical Mode Decomposition (EMD) is employed todenoise CSI amplitude data and extract spatial-temporal features. For on-devicedeployment, STAR is implemented on a Rockchip RV1126 processor equipped with anembedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSIacquisition module. Experimental results demonstrate a mean recognitionaccuracy of 93.52% across seven activity classes and 99.11% for human presencedetection, utilizing a compact 97.6k-parameter model. INT8 quantized inferenceachieves a processing speed of 33 MHz with just 8% CPU utilization, deliveringsixfold speed improvements over CPU-based execution. With sub-second responselatency and low power consumption, the system ensures real-time,privacy-preserving HAR, offering a practical, scalable solution for mobile andpervasive computing environments.</description>
      <author>example@mail.com (Kexing Liu)</author>
      <guid isPermaLink="false">2510.26148v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>EgoExo-Con: Exploring View-Invariant Video Temporal Understanding</title>
      <link>http://arxiv.org/abs/2510.26113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  project page:  \url{https://minjoong507.github.io/projects/EgoExo-Con/}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视频大语言模型在不同视角下捕捉同一事件时的时间理解一致性问题，提出了EgoExo-Con基准测试和View-GRPO强化学习框架来解决现有模型的局限性。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型在不同视角下捕捉同一事件时可能存在时间理解不一致的问题。&lt;h4&gt;目的&lt;/h4&gt;研究视频大语言模型在不同视角下捕捉同一事件时是否能保持一致的时间理解能力，并提出改进方法。&lt;h4&gt;方法&lt;/h4&gt;引入EgoExo-Con基准测试，包含全面同步的第一人称和第三人称视角视频对及人类优化的自然语言查询，强调时间验证和时间定位两个任务，并提出View-GRPO强化学习框架来增强特定视角的时间推理并促进跨视角一致性理解。&lt;h4&gt;主要发现&lt;/h4&gt;现有Video-LLMs存在两个关键限制：(1)模型通常无法保持一致性，结果远差于单视角表现；(2)当使用双视角同步视频微调时，模型显示出一致性改进，但表现往往不如单视角训练的模型。&lt;h4&gt;结论&lt;/h4&gt;View-GRPO方法在提高跨视角一致性方面优于简单的SFT和GRPO，所有研究资源将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;当视频从不同视角捕捉同一事件时，视频大语言模型能否实现一致的时间理解？为研究此问题，我们引入了EgoExo-Con(一致性)基准，该基准包含全面同步的第一人称和第三人称视频对以及人类优化的自然语言查询。EgoExo-Con强调两个时间理解任务：时间验证和时间定位。它不仅评估正确性，还评估跨视角的一致性。我们的分析揭示了现有Video-LLMs的两个关键局限：(1)模型通常无法保持一致性，结果远差于其单视角表现。(2)当使用双视角同步视频进行微调时，模型显示出一致性改进，但往往表现不如单视角训练的模型。为改进，我们提出了View-GRPO，一种新的强化学习框架，能有效增强特定视角的时间推理，同时鼓励跨视角的一致性理解。我们的方法在提高跨视角一致性方面优于简单的SFT和GRPO。所有资源将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Can Video-LLMs achieve consistent temporal understanding when videos capturethe same event from different viewpoints? To study this, we introduceEgoExo-Con (Consistency), a benchmark of comprehensively synchronizedegocentric and exocentric video pairs with human-refined queries in naturallanguage. EgoExo-Con emphasizes two temporal understanding tasks: TemporalVerification and Temporal Grounding. It evaluates not only correctness butconsistency across viewpoints. Our analysis reveals two critical limitations ofexisting Video-LLMs: (1) models often fail to maintain consistency, withresults far worse than their single-view performances. (2) When naivelyfinetuned with synchronized videos of both viewpoints, the models show improvedconsistency but often underperform those trained on a single view. Forimprovements, we propose View-GRPO, a novel reinforcement learning frameworkthat effectively strengthens view-specific temporal reasoning while encouragingconsistent comprehension across viewpoints. Our method demonstrates itssuperiority over naive SFT and GRPO, especially for improving cross-viewconsistency. All resources will be made publicly available.</description>
      <author>example@mail.com (Minjoon Jung, Junbin Xiao, Junghyun Kim, Byoung-Tak Zhang, Angela Yao)</author>
      <guid isPermaLink="false">2510.26113v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders</title>
      <link>http://arxiv.org/abs/2510.26027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的视频大语言模型架构，通过在视觉编码器中引入堆叠时序注意力模块，解决了当前模型在理解视频时序动态方面的局限性，显著提升了时序推理能力和动作识别性能。&lt;h4&gt;背景&lt;/h4&gt;尽管多模态大语言模型(MLLMs)已取得显著进展，但理解视频中复杂的时序动态仍然是一个主要挑战。当前视频大语言模型(Video-LLM)架构在时序理解方面存在关键局限性，难以处理需要详细理解动作序列和时间进展的任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的Video-LLM架构，增强模型对视频时序动态的理解能力，特别是在动作序列和时间进展方面的理解。&lt;h4&gt;方法&lt;/h4&gt;在视觉编码器中直接引入堆叠的时序注意力模块，在视觉编码器中融入时序注意力，使模型能够更好地捕捉动作进展和帧之间的关系，在将视觉令牌传递给LLM之前增强时序理解。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著提高了时序推理能力，在视频问答任务中优于现有模型，特别是在动作识别方面。在VITATECS、MVBench和Video-MME等基准测试中，性能提升了高达5.5%。&lt;h4&gt;结论&lt;/h4&gt;通过增强视觉编码器的时序结构，解决了Video-LLMs在视频理解方面的关键差距，为视频时序理解提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;尽管多模态大语言模型(MLLMs)取得了显著进展，但理解视频中复杂的时序动态仍然是一个主要挑战。我们的实验表明，当前视频大语言模型(Video-LLM)架构在时序理解方面存在关键局限性，难以处理需要详细理解动作序列和时间进展的任务。在这项工作中，我们提出了一种Video-LLM架构，在视觉编码器中直接引入堆叠的时序注意力模块。这种设计在视觉编码器中融入了时序注意力，使模型能够在将视觉令牌传递给LLM之前更好地捕捉动作进展和帧之间的关系。我们的结果表明，这种方法显著提高了时序推理能力，并在视频问答任务中优于现有模型，特别是在动作识别方面。我们在VITATECS、MVBench和Video-MME等基准测试中提高了高达5.5%。通过增强视觉编码器的时序结构，我们解决了Video-LLMs在视频理解方面的关键差距。项目页面和代码可在以下网址获取：https://alirasekh.github.io/STAVEQ2/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant advances in Multimodal Large Language Models (MLLMs),understanding complex temporal dynamics in videos remains a major challenge.Our experiments show that current Video Large Language Model (Video-LLM)architectures have critical limitations in temporal understanding, strugglingwith tasks that require detailed comprehension of action sequences and temporalprogression. In this work, we propose a Video-LLM architecture that introducesstacked temporal attention modules directly within the vision encoder. Thisdesign incorporates a temporal attention in vision encoder, enabling the modelto better capture the progression of actions and the relationships betweenframes before passing visual tokens to the LLM. Our results show that thisapproach significantly improves temporal reasoning and outperforms existingmodels in video question answering tasks, specifically in action recognition.We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to+5.5%. By enhancing the vision encoder with temporal structure, we address acritical gap in video understanding for Video-LLMs. Project page and code areavailable at: https://alirasekh.github.io/STAVEQ2/.</description>
      <author>example@mail.com (Ali Rasekh, Erfan Bagheri Soula, Omid Daliran, Simon Gottschalk, Mohsen Fayyaz)</author>
      <guid isPermaLink="false">2510.26027v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL</title>
      <link>http://arxiv.org/abs/2510.25997v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, GeoGenAgent'25 - ACM SIGSPATIAL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于代理的管道系统，用于处理复杂的空间和时间自然语言查询，显著提高了查询准确性和用户友好性。&lt;h4&gt;背景&lt;/h4&gt;现有的自然语言到SQL系统在处理真实空间和时间查询时存在困难，需要将模糊的用户表述与特定模式类别匹配、处理时间推理并选择适当输出。&lt;h4&gt;目的&lt;/h4&gt;开发能够处理复杂空间和时间查询的系统，支持缺乏SQL专业知识、详细模式知识或提示技能的用户。&lt;h4&gt;方法&lt;/h4&gt;构建了一个基于代理的管道，通过基于Mistral的ReAct代理对基础文本到SQL模型(llama-3-sqlcoder-8b)进行编排，使代理能够通过模式检查、SQL生成、执行和可视化工具来规划、分解和调整查询。&lt;h4&gt;主要发现&lt;/h4&gt;在纽约和东京签到数据集的35个自然语言查询评估中，代理系统准确率达到91.4%，而基础模型仅为28.6%，并通过地图、图表和结构化的自然语言摘要显著增强了可用性。&lt;h4&gt;结论&lt;/h4&gt;代理编排而非更强的SQL生成器本身是构建交互式地理空间助手的有前途的基础。&lt;h4&gt;翻译&lt;/h4&gt;自然语言到SQL系统有望使结构化数据访问民主化，使用户无需学习SQL即可查询数据库。然而，现有系统在处理现实空间时间查询方面存在困难，成功需要将模糊的用户表述与特定模式类别对齐、处理时间推理并选择适当输出。我们提出了一种基于代理的管道，通过基于Mistral的ReAct代理的编排，扩展了一个基础文本到SQL模型(llama-3-sqlcoder-8b)。该代理可以通过模式检查、SQL生成、执行和可视化工具来规划、分解和调整查询。我们在纽约和东京签到数据集上的35个自然语言查询进行了评估，涵盖了空间、时间和多数据集推理。代理的准确率显著高于基础模型，达到91.4%对28.6%，并通过地图、图表和结构化的自然语言摘要增强了可用性。关键的是，我们的设计支持了更自然的人机数据库交互，支持缺乏SQL专业知识、详细模式知识或提示技能的用户。我们得出结论，代理编排而非更强的SQL生成器本身，是交互式地理空间助手的有前途的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3764915.3770724&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizingaccess to structured data, allowing users to query databases without learningSQL. Yet existing systems struggle with realistic spatio-temporal queries,where success requires aligning vague user phrasing with schema-specificcategories, handling temporal reasoning, and choosing appropriate outputs. Wepresent an agentic pipeline that extends a naive text-to-SQL baseline(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. Theagent can plan, decompose, and adapt queries through schema inspection, SQLgeneration, execution, and visualization tools. We evaluate on 35natural-language queries over the NYC and Tokyo check-in dataset, coveringspatial, temporal, and multi-dataset reasoning. The agent achievessubstantially higher accuracy than the naive baseline 91.4% vs. 28.6% andenhances usability through maps, plots, and structured natural-languagesummaries. Crucially, our design enables more natural human-databaseinteraction, supporting users who lack SQL expertise, detailed schemaknowledge, or prompting skill. We conclude that agentic orchestration, ratherthan stronger SQL generators alone, is a promising foundation for interactivegeospatial assistants.</description>
      <author>example@mail.com (Manu Redd, Tao Zhe, Dongjie Wang)</author>
      <guid isPermaLink="false">2510.25997v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks</title>
      <link>http://arxiv.org/abs/2510.25797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了时空建模和空间注意力机制在水下物体检测中的有效性，比较了标准YOLOv5、T-YOLOv5及其与CBAM结合的变体性能。&lt;h4&gt;背景&lt;/h4&gt;水下物体检测在动态海洋环境中面临挑战，如突然运动、部分遮挡和逐渐运动等。&lt;h4&gt;目的&lt;/h4&gt;研究时空建模和空间注意力机制如何提高水下物体检测的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;分两个阶段进行，第一阶段评估T-YOLOv5与标准YOLOv5的性能比较；第二阶段开发添加了卷积块注意力模块(CBAM)的T-YOLOv5增强版本。&lt;h4&gt;主要发现&lt;/h4&gt;T-YOLOv5和T-YOLOv5与CBAM的变体在mAP@50-95指标上分别达到0.813和0.811，显著优于标准YOLOv5的0.563。&lt;h4&gt;结论&lt;/h4&gt;T-YOLOv5相比标准模型显著提高了检测可靠性，而T-YOLOv5与CBAM在具有挑战性的场景中进一步提高了性能，但在简单场景中会损失一些准确性。&lt;h4&gt;翻译&lt;/h4&gt;该研究检验了时空建模和空间注意力机制在深度学习模型中用于水下物体检测的有效性。具体而言，在第一阶段，评估了增强时序的YOLOv5变体T-YOLOv5与标准YOLOv5的性能比较。在第二阶段，通过添加卷积块注意力模块(CBAM)开发了T-YOLOv5的增强版本。研究表明，CBAM如何通过时序建模提高了在动态海洋环境中的检测准确性，特别是在突然运动、部分遮挡和逐渐运动的条件下。测试结果显示，YOLOv5达到了0.563的mAP@50-95，而T-YOLOv5和带有CBAM的T-YOLOv5分别以0.813和0.811的mAP@50-95分数表现更优，突显了它们在检测复杂物体方面的卓越准确性和泛化能力。研究结果表明，与标准模型相比，T-YOLOv5显著提高了检测可靠性，而带有CBAM的T-YOLOv5在具有挑战性的场景中进一步提高了性能，尽管在简单场景中会损失一些准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study examines the effectiveness of spatio-temporal modeling and theintegration of spatial attention mechanisms in deep learning models forunderwater object detection. Specifically, in the first phase, the performanceof temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison withthe standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 isdeveloped, through the addition of a Convolutional Block Attention Module(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 andT-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, theresearch highlights how temporal modeling improves detection accuracy indynamic marine environments, particularly under conditions of sudden movements,partial occlusions, and gradual motion. The testing results showed that YOLOv5achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAMoutperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,highlighting their superior accuracy and generalization in detecting complexobjects. The findings demonstrate that T-YOLOv5 significantly enhancesdetection reliability compared to the standard model, while T-YOLOv5 with CBAMfurther improves performance in challenging scenarios, although there is a lossof accuracy when it comes to simpler scenarios.</description>
      <author>example@mail.com (Sai Likhith Karri, Ansh Saxena)</author>
      <guid isPermaLink="false">2510.25797v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios</title>
      <link>http://arxiv.org/abs/2510.26580v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint under review at IEEE Transactions on Pattern Analysis and  Machine Intelligence (TPAMI), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种动态上下文感知场景推理框架，利用视觉语言对齐解决零样本现实世界场景问题，通过结合视觉Transformer和大语言模型显著提高了复杂环境中的场景理解准确性。&lt;h4&gt;背景&lt;/h4&gt;在现实世界环境中，AI系统经常面临没有标记数据的陌生场景，这给传统场景理解模型带来重大挑战。无法在未见过的上下文中进行泛化限制了基于视觉的应用程序在动态、非结构化环境中的部署。&lt;h4&gt;目的&lt;/h4&gt;使智能系统能够在没有特定任务先验训练的情况下推断并适应新环境。&lt;h4&gt;方法&lt;/h4&gt;提出的方法集成了预训练的视觉Transformer和大语言模型，将视觉语义与自然语言描述对齐增强上下文理解能力。动态推理模块通过结合全局场景线索和由语言先验引导的对象级交互来优化预测。&lt;h4&gt;主要发现&lt;/h4&gt;在COCO、Visual Genome和Open Images等零样本基准上的实验表明，在复杂且未见过的环境中，场景理解准确性比基线模型提高了高达18%。由于视觉和语言的协同融合，系统在模糊或杂乱的场景中表现出强大的性能。&lt;h4&gt;结论&lt;/h4&gt;该框架为上下文感知推理提供了一种可扩展和可解释的方法，推动了动态现实世界环境中的零样本泛化。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界环境中，AI系统经常面临没有标记数据的陌生场景，这给传统的场景理解模型带来了重大挑战。无法在未见过的上下文中进行泛化限制了基于视觉的应用程序在动态、非结构化环境中的部署。这项工作引入了一种动态上下文感知场景推理框架，利用视觉语言对齐来解决零样本现实世界场景问题。目标是使智能系统能够在没有特定任务先验训练的情况下推断并适应新环境。提出的方法集成了预训练的视觉Transformer和大语言模型，将视觉语义与自然语言描述对齐，增强上下文理解能力。动态推理模块通过结合全局场景线索和由语言先验引导的对象级交互来优化预测。在COCO、Visual Genome和Open Images等零样本基准上的广泛实验表明，在复杂且未见过的环境中，场景理解准确性比基线模型提高了高达18%。结果还显示，由于视觉和语言的协同融合，在模糊或杂乱的场景中表现出强大的性能。该框架为上下文感知推理提供了一种可扩展和可解释的方法，推动了动态现实世界环境中的零样本泛化。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决AI系统在零样本(real-world)场景下进行动态上下文感知场景推理的问题。传统场景理解模型在面对没有标记数据的新环境时无法有效泛化，这限制了视觉应用在自动驾驶、机器人导航、监控等动态、非结构化环境中的部署。这个问题在现实中非常重要，因为真实世界环境往往是复杂、多变且缺乏标记数据的，AI系统需要能够理解和适应从未见过的新场景而不需要针对每个特定任务重新训练。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过整合预训练的视觉转换器(visual transformers)和大语言模型(large language models)来设计这个方法。他们借鉴了多项现有工作，包括Context VLM用于自动驾驶安全、结合开放世界检测器和大视觉语言模型的零样本目标识别、基于图的视觉语言模型调整方法、利用CLIP模型的动态场景恢复框架等。作者在这些工作的基础上，提出了一个动态推理模块，该模块利用全局场景线索和对象级交互，由语言先验指导，从而实现更有效的场景理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过视觉-语言对齐来实现动态上下文感知的场景推理，使AI系统能够在没有任务特定监督的情况下理解和适应新环境。整体实现流程包括：1)使用视觉编码器(如CLIP-ViT)提取丰富的视觉特征；2)使用语言编码器(如GPT或BERT变体)建模语义先验；3)采用基于注意力的跨模态融合机制对齐视觉和语言；4)引入上下文精炼单元，建模对象级交互和全局场景语义；5)在零样本场景下评估模型性能，通过计算视觉和文本嵌入的余弦相似度来进行场景理解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出动态上下文感知场景推理框架，整合预训练视觉转换器和大语言模型；2)引入动态推理模块，利用全局场景线索和对象级交互；3)在多个零样本基准数据集上证明模型泛化和适应性；4)在模糊或杂乱场景中表现出强大性能；5)提供可扩展和可解释的上下文感知推理方法。相比之前的工作，这个方法的主要不同在于：结合了动态推理能力和视觉-语言对齐；不依赖任务特定监督；能处理复杂、模糊场景；具有更好的泛化能力和可解释性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的动态上下文感知场景推理框架，通过视觉-语言对齐实现了在没有任务特定监督的情况下理解和适应新环境的能力，显著提升了AI系统在复杂、模糊和杂乱场景中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world environments, AI systems often face unfamiliar scenarioswithout labeled data, creating a major challenge for conventional sceneunderstanding models. The inability to generalize across unseen contexts limitsthe deployment of vision-based applications in dynamic, unstructured settings.This work introduces a Dynamic Context-Aware Scene Reasoning framework thatleverages Vision-Language Alignment to address zero-shot real-world scenarios.The goal is to enable intelligent systems to infer and adapt to newenvironments without prior task-specific training. The proposed approachintegrates pre-trained vision transformers and large language models to alignvisual semantics with natural language descriptions, enhancing contextualcomprehension. A dynamic reasoning module refines predictions by combiningglobal scene cues and object-level interactions guided by linguistic priors.Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, andOpen Images demonstrate up to 18% improvement in scene understanding accuracyover baseline models in complex and unseen environments. Results also showrobust performance in ambiguous or cluttered scenes due to the synergisticfusion of vision and language. This framework offers a scalable andinterpretable approach for context-aware reasoning, advancing zero-shotgeneralization in dynamic real-world settings.</description>
      <author>example@mail.com (Manjunath Prasad Holenarasipura Rajiv, B. M. Vidyavathi)</author>
      <guid isPermaLink="false">2510.26580v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM</title>
      <link>http://arxiv.org/abs/2510.26358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AgriGS-SLAM是一种结合视觉和激光雷达的SLAM框架，利用多相机3D高斯溅射技术实现果园环境的实时3D场景理解，克服了重复几何结构、季节变化和风吹 foliage 运动的挑战。&lt;h4&gt;背景&lt;/h4&gt;果园中的自主机器人需要实时3D场景理解，尽管存在重复的行列几何结构、季节性外观变化和风吹 foliage 运动。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理果园环境特殊挑战的实时3D场景理解系统。&lt;h4&gt;方法&lt;/h4&gt;结合直接激光雷达里程计和闭环检测与多相机3D高斯溅射渲染；通过互补视角的批量栅格化恢复遮挡下的果园结构；在关键帧之间执行统一的梯度驱动地图生命周期；基于概率激光雷达深度一致性项进行姿态优化并加强几何-外观耦合。&lt;h4&gt;主要发现&lt;/h4&gt;在苹果和梨果园的休眠期、开花期和收获期测试；跨季节和站点提供比先进3DGS-SLAM基线更清晰、更稳定的重建和轨迹；在拖拉机上保持实时性能。&lt;h4&gt;结论&lt;/h4&gt;虽然演示于果园监测，但该方法可应用于需要鲁棒多模态感知的其他户外领域。&lt;h4&gt;翻译&lt;/h4&gt;果园中的自主机器人需要实时3D场景理解，尽管存在重复的行列几何结构、季节性外观变化和风吹 foliage 运动。我们提出了AgriGS-SLAM，这是一种结合直接激光雷达里程计和闭环检测与多相机3D高斯溅射渲染的视觉-激光雷达SLAM框架。通过互补视角的批量栅格化恢复遮挡下的果园结构，同时在关键帧之间执行统一的梯度驱动地图生命周期以保留精细细节并限制内存使用。姿态优化由基于概率激光雷达的深度一致性项引导，通过相机投影反向传播以加强几何-外观耦合。我们在苹果和梨果园的休眠期、开花期和收获期使用标准化轨迹协议部署了该系统，评估训练视图和新颖视图合成以减少3DGS评估中的过拟合。跨季节和站点，AgriGS-SLAM比最近的先进3DGS-SLAM基线提供更清晰、更稳定的重建和更稳定的轨迹，同时在拖拉机上保持实时性能。虽然演示于果园监测，但该方法可应用于需要鲁棒多模态感知的其他户外领域。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决果园环境中自主机器人的实时3D场景理解问题，特别是在面对季节性外观变化、重复的行几何结构和风吹引起的叶片运动等挑战时的SLAM系统适应性。这个问题很重要，因为全球人口增长和劳动力短缺增加了对自主农业技术的需求，果园机器人需要准确的3D重建能力来执行导航、收获、喷洒和修剪等任务，同时农民需要即时反馈来调整田间操作，农业数字孪子也需要物理世界与数字表示之间的持续同步。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有SLAM系统在农业环境中的局限性：视觉-only方法在重复作物模式和植被运动下会失败，而激光雷达-only系统在几何稀疏性方面受限。他们借鉴了神经渲染的最新进展，特别是3D高斯泼溅(3DGS)的显式点表示和高效光栅化特性，适合大型场景SLAM的增量特性。方法设计上结合了直接激光雷达里程计(DLO)作为前端，因子图作为后端，并扩展了3DGS到多视图设置以处理果园遮挡，同时参考了现有的多视图优化和闭环检测技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合视觉和激光雷达两种传感器的优势，使用多摄像头系统提供互补视角解决遮挡问题，利用3D高斯泼溅进行实时场景表示，设计梯度驱动的地图生命周期管理，以及使用多模态损失函数联合优化场景表示和机器人定位。整体流程包括：SLAM前端使用直接激光雷达里程计估计运动；SLAM后端维护关键帧姿势的因子图；多视图3D高斯泼溅部分进行增量映射、内存管理和泼溅生命周期操作；优化循环调度各种操作；最后通过多模态损失函数结合光度一致性和几何一致性进行优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 实时视觉-激光雷达3DGS-SLAM系统；2) 跨季节适用性基准评估方法；3) 统一的梯度驱动的3DGS-SLAM地图和定位优化；4) 支持多摄像头设置的户外3DGS-SLAM框架。相比之前工作，该方法专门针对果园环境设计，使用多摄像头系统而非单摄像头，结合激光雷达里程计和闭环检测而非仅依赖视觉或激光雷达，设计了特定的地图生命周期管理策略，使用多模态损失函数结合光度和几何一致性，并在真实果园的不同生长阶段进行了广泛测试。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AgriGS-SLAM通过结合多视图3D高斯泼溅与激光雷达里程计和闭环检测，实现了果园环境中的实时、跨季节精确地图构建和机器人定位，解决了季节性变化、重复结构和遮挡带来的挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous robots in orchards require real-time 3D scene understandingdespite repetitive row geometry, seasonal appearance changes, and wind-drivenfoliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework thatcouples direct LiDAR odometry and loop closures with multi-camera 3D GaussianSplatting (3DGS) rendering. Batch rasterization across complementary viewpointsrecovers orchard structure under occlusions, while a unified gradient-drivenmap lifecycle executed between keyframes preserves fine details and boundsmemory. Pose refinement is guided by a probabilistic LiDAR-based depthconsistency term, back-propagated through the camera projection to tightengeometry-appearance coupling. We deploy the system on a field platform in appleand pear orchards across dormancy, flowering, and harvesting, using astandardized trajectory protocol that evaluates both training-view andnovel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasonsand sites, AgriGS-SLAM delivers sharper, more stable reconstructions andsteadier trajectories than recent state-of-the-art 3DGS-SLAM baselines whilemaintaining real-time performance on-tractor. While demonstrated in orchardmonitoring, the approach can be applied to other outdoor domains requiringrobust multimodal perception.</description>
      <author>example@mail.com (Mirko Usuelli, David Rapado-Rincon, Gert Kootstra, Matteo Matteucci)</author>
      <guid isPermaLink="false">2510.26358v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>A Three-Stage Bayesian Transfer Learning Framework to Improve Predictions in Data-Scarce Domains</title>
      <link>http://arxiv.org/abs/2510.26541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Engineering Applications of Artificial Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种全监督的三阶段框架（staged B-DANN），结合参数迁移和共享潜在空间适应，用于解决数据稀缺领域的机器学习问题。该方法通过确定性特征提取、对抗性优化和贝叶斯微调三个阶段，提高了预测准确性和泛化能力，同时提供不确定性估计。&lt;h4&gt;背景&lt;/h4&gt;机器学习在工程领域的应用持续增长，深度神经网络因其性能和可访问性被广泛采用，但需要大量高质量数据集。实验数据通常稀疏、嘈杂或不足以构建稳健的数据驱动模型。迁移学习利用数据丰富的源领域来辅助数据稀缺目标领域的学习，但传统参数迁移在领域差异较大时性能下降，领域对抗神经网络(DANNs)在半监督设置下可以处理更大的领域偏移，但训练不稳定且缺乏不确定性量化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理领域差异、提供不确定性估计并提高数据稀缺领域预测准确性和泛化能力的机器学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种全监督的三阶段框架：阶段性贝叶斯领域对抗神经网络(staged B-DANN)。第一阶段在源领域训练确定性特征提取器；第二阶段使用DANN对抗性地优化特征提取器；第三阶段在适应的特征提取器上构建贝叶斯神经网络，用于在目标领域微调，处理条件偏移并提供校准的不确定性估计。&lt;h4&gt;主要发现&lt;/h4&gt;在合成基准测试中，该方法显著优于标准迁移技术；应用于预测矩形通道中的临界热通量时，利用管状实验数据作为源领域，结果显示staged B-DANN方法可以提高预测准确性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;staged B-DANN方法可以改进预测准确性和泛化能力，并提供不确定性估计，可能有助于核工程等其他领域的应用。&lt;h4&gt;翻译&lt;/h4&gt;机器学习在工程领域的应用持续增长，以支持广泛的应用。在这些方法中，深度神经网络因其性能和可访问性被广泛采用，但它们需要大量高质量的数据集。实验数据通常稀疏、嘈杂或不足以构建稳健的数据驱动模型。迁移学习利用数据丰富的相关源领域来辅助数据稀缺目标领域的学习，已显示出有效性。参数迁移（重用预训练权重）很常见，但在大的领域偏移下性能会下降。领域对抗神经网络(DANNs)通过学习领域不变的表示来解决这个问题，从而在半监督设置下提高较大领域偏移的迁移效果。然而，DANNs在训练过程中可能不稳定，且缺乏原生的不确定性量化手段。本研究引入了一种全监督的三阶段框架——阶段性贝叶斯领域对抗神经网络(staged B-DANN)，它结合了参数迁移和共享潜在空间适应。在第一阶段，在源领域训练确定性特征提取器。然后在第二阶段使用DANN对抗性地优化该特征提取器。在第三阶段，在适应的特征提取器上构建贝叶斯神经网络，用于在目标领域进行微调，以处理条件偏移并产生校准的不确定性估计。首先在合成基准测试中验证了这种阶段性B-DANN方法，结果表明它显著优于标准迁移技术。然后将其应用于预测矩形通道中的临界热通量任务，利用管状实验数据作为源领域。本研究结果表明，阶段性B-DANN方法可以提高预测准确性和泛化能力，可能有助于核工程的其他领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of ML in engineering has grown steadily to support a wide array ofapplications. Among these methods, deep neural networks have been widelyadopted due to their performance and accessibility, but they require large,high-quality datasets. Experimental data are often sparse, noisy, orinsufficient to build resilient data-driven models. Transfer learning, whichleverages relevant data-abundant source domains to assist learning indata-scarce target domains, has shown efficacy. Parameter transfer, wherepretrained weights are reused, is common but degrades under large domainshifts. Domain-adversarial neural networks (DANNs) help address this issue bylearning domain-invariant representations, thereby improving transfer undergreater domain shifts in a semi-supervised setting. However, DANNs can beunstable during training and lack a native means for uncertaintyquantification. This study introduces a fully-supervised three-stage framework,the staged Bayesian domain-adversarial neural network (staged B-DANN), thatcombines parameter transfer and shared latent space adaptation. In Stage 1, adeterministic feature extractor is trained on the source domain. This featureextractor is then adversarially refined using a DANN in Stage 2. In Stage 3, aBayesian neural network is built on the adapted feature extractor forfine-tuning on the target domain to handle conditional shifts and yieldcalibrated uncertainty estimates. This staged B-DANN approach was firstvalidated on a synthetic benchmark, where it was shown to significantlyoutperform standard transfer techniques. It was then applied to the task ofpredicting critical heat flux in rectangular channels, leveraging data fromtube experiments as the source domain. The results of this study show that thestaged B-DANN method can improve predictive accuracy and generalization,potentially assisting other domains in nuclear engineering.</description>
      <author>example@mail.com (Aidan Furlong, Robert Salko, Xingang Zhao, Xu Wu)</author>
      <guid isPermaLink="false">2510.26541v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm</title>
      <link>http://arxiv.org/abs/2510.26509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于二维元胞自动机的可适应边缘检测器，通过元启发式方法和迁移学习技术进行优化。研究分析了优化阶段搜索空间扩展的影响以及检测器在不同图像集上的适应性。&lt;h4&gt;背景&lt;/h4&gt;边缘检测是图像处理中提取相关信息的重要任务，但现有检测器存在难以检测松散边缘和缺乏上下文信息等问题。&lt;h4&gt;目的&lt;/h4&gt;分析扩大优化阶段搜索空间的影响，以及检测器在识别自然图像集及其专门子集边缘时的适应性稳健性。&lt;h4&gt;方法&lt;/h4&gt;开发了一种由二维元胞自动机描述并通过元启发式方法结合迁移学习技术优化的可适应检测器。&lt;h4&gt;主要发现&lt;/h4&gt;扩大优化阶段的搜索空间对所选图像集并不有效；模型能够适应不同输入，但迁移学习技术未带来显著改进。&lt;h4&gt;结论&lt;/h4&gt;所提出的检测器具有良好的适应性，但扩大搜索空间和迁移学习技术未能显著提高性能。&lt;h4&gt;翻译&lt;/h4&gt;边缘检测任务在图像处理中至关重要，旨在从图像中提取相关信息。此任务中存在一个反复出现的问题，即某些检测器的弱点，例如难以检测松散边缘以及缺乏从特定问题中提取相关信息的上下文。为解决这些弱点并使检测器适应图像特性，研究人员开发了一种由二维元胞自动机描述并通过元启发式方法结合迁移学习技术优化的可适应检测器。本研究旨在分析扩大优化阶段搜索空间的影响，以及检测器在识别自然图像集及其从同一图像集中提取的专门子集边缘时的适应性稳健性。获得的结果证明，对于所选图像集，扩大优化阶段的搜索空间并不有效。研究还通过一系列实验和验证技术分析了模型的适应性，发现无论采用何种验证方法，模型都能适应输入，并且应用于模型的迁移学习技术未显示出显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The edge detection task is essential in image processing aiming to extractrelevant information from an image. One recurring problem in this task is theweaknesses found in some detectors, such as the difficulty in detecting looseedges and the lack of context to extract relevant information from specificproblems. To address these weaknesses and adapt the detector to the propertiesof an image, an adaptable detector described by two-dimensional cellularautomaton and optimized by meta-heuristic combined with transfer learningtechniques was developed. This study aims to analyze the impact of expandingthe search space of the optimization phase and the robustness of theadaptability of the detector in identifying edges of a set of natural imagesand specialized subsets extracted from the same image set. The results obtainedprove that expanding the search space of the optimization phase was noteffective for the chosen image set. The study also analyzed the adaptability ofthe model through a series of experiments and validation techniques and foundthat, regardless of the validation, the model was able to adapt to the inputand the transfer learning techniques applied to the model showed no significantimprovements.</description>
      <author>example@mail.com (Vinícius Ferraria, Eurico Ruivo)</author>
      <guid isPermaLink="false">2510.26509v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Applications of Machine Learning in Polymer Materials: Property Prediction, Material Design, and Systematic Processes</title>
      <link>http://arxiv.org/abs/2510.26100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  55 pages, 6 tables, 9 figures, a systematic review on the research  progress and application prospects of machine learning in polymer materials&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文系统综述了机器学习技术在聚合物材料领域的研究进展和应用前景，介绍了基本技术、应用方法、当前挑战和未来趋势。&lt;h4&gt;背景&lt;/h4&gt;机器学习技术在聚合物材料领域发展迅速，显著加速了材料预测和设计，但其复杂性也给传统领域研究者带来理解和应用困难。聚合物材料研发面临结构复杂性和传统试错方法局限性等挑战。&lt;h4&gt;目的&lt;/h4&gt;应对聚合物材料研发中的挑战，解决机器学习方法的复杂性给传统领域研究者带来的理解和应用困难，促进机器学习技术在聚合物材料领域的有效应用。&lt;h4&gt;方法&lt;/h4&gt;分析聚合物材料研发中的固有挑战；介绍分子描述符、特征表示、数据标准化和清洗等关键技术；记录高质量聚合物数据库；构建高可靠性机器学习模型；实施实验验证、模型评估和优化方法。&lt;h4&gt;主要发现&lt;/h4&gt;机器学习在聚合物性能预测和材料设计中发挥关键作用；传统机器学习、深度学习和迁移学习等算法有具体应用；数据驱动设计策略包括反向设计、高通量虚拟筛选和多目标优化。&lt;h4&gt;结论&lt;/h4&gt;当前研究面临数据质量和模型泛化能力等技术挑战；未来发展趋势包括多尺度建模、物理信息机器学习、标准化数据共享和可解释机器学习。&lt;h4&gt;翻译&lt;/h4&gt;本文系统综述了机器学习技术在聚合物材料领域的研究进展和应用前景。目前，机器学习方法在聚合物材料研究中发展迅速；尽管它们显著加速了材料预测和设计，但其复杂性也给传统领域研究者的理解和应用带来了困难。针对上述问题，本文首先分析了聚合物材料研发中的固有挑战，包括结构复杂性和传统试错方法的局限性。为解决这些问题，它重点介绍了分子描述符和特征表示、数据标准化和清洗等关键基础技术，并记录了多个高质量的聚合物数据库。随后，它详细阐述了机器学习在聚合物性能预测和材料设计中的关键作用，涵盖了传统机器学习、深度学习和迁移学习等算法的具体应用；进一步深入阐述了数据驱动设计策略，如反向设计、高通量虚拟筛选和多目标优化。本文还系统介绍了构建高可靠性机器学习模型的完整过程，总结了有效的实验验证、模型评估和优化方法。最后，它总结了当前研究中的技术挑战，如数据质量和模型泛化能力，并展望了包括多尺度建模、物理信息机器学习、标准化数据共享和可解释机器学习在内的未来发展趋势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper systematically reviews the research progress and applicationprospects of machine learning technologies in the field of polymer materials.Currently, machine learning methods are developing rapidly in polymer materialresearch; although they have significantly accelerated material prediction anddesign, their complexity has also caused difficulties in understanding andapplication for researchers in traditional fields. In response to the aboveissues, this paper first analyzes the inherent challenges in the research anddevelopment of polymer materials, including structural complexity and thelimitations of traditional trial-and-error methods. To address these problems,it focuses on introducing key basic technologies such as molecular descriptorsand feature representation, data standardization and cleaning, and records anumber of high-quality polymer databases. Subsequently, it elaborates on thekey role of machine learning in polymer property prediction and materialdesign, covering the specific applications of algorithms such as traditionalmachine learning, deep learning, and transfer learning; further, it deeplyexpounds on data-driven design strategies, such as reverse design,high-throughput virtual screening, and multi-objective optimization. The paperalso systematically introduces the complete process of constructinghigh-reliability machine learning models and summarizes effective experimentalverification, model evaluation, and optimization methods. Finally, itsummarizes the current technical challenges in research, such as data qualityand model generalization ability, and looks forward to future developmenttrends including multi-scale modeling, physics-informed machine learning,standardized data sharing, and interpretable machine learning.</description>
      <author>example@mail.com (Hongtao Guo Shuai Li Shu Li)</author>
      <guid isPermaLink="false">2510.26100v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry</title>
      <link>http://arxiv.org/abs/2510.26008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures, submitted to nsdi 26&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为System-X的系统级优化方法，它采用以硬件为中心的思路，仅依赖硬件信号而非工作负载知识进行优化，成功识别了网络和系统配置问题，加速了DeepSeek模型5.97%。&lt;h4&gt;背景&lt;/h4&gt;现代机器学习已成为一个紧密结合的全栈生态系统，许多用户依赖云提供商提供弹性、隔离和成本高效的资源。然而，这些平台即服务使用虚拟化，导致运营商对用户的工作负载了解有限，阻碍了资源优化。&lt;h4&gt;目的&lt;/h4&gt;论证工作负载知识对系统级优化不是必需的，并提出一种仅依赖硬件信号的优化方法。&lt;h4&gt;方法&lt;/h4&gt;提出System-X系统，采用以硬件为中心的方法，仅依赖运营商完全可访问的硬件信号。通过从系统收集低级信号，使用无监督学习管道检测异常。该管道通过分析各种硬件平台上30多种流行的ML模型开发，确保能够适应新兴工作负载和未知部署模式。&lt;h4&gt;主要发现&lt;/h4&gt;使用System-X成功识别了网络和系统配置问题，加速了DeepSeek模型5.97%。&lt;h4&gt;结论&lt;/h4&gt;系统级优化可以通过仅依赖硬件信号来实现，无需了解具体的工作负载细节。&lt;h4&gt;翻译&lt;/h4&gt;现代机器学习已发展成为一个紧密结合的全栈生态系统，结合了硬件、软件、网络和应用。许多用户依赖云提供商提供弹性、隔离和成本高效的资源。不幸的是，这些平台即服务使用虚拟化，这意味着运营商对用户的工作负载了解有限。这阻碍了运营商的资源优化，而这对确保成本效率和最小化执行时间至关重要。在本文中，我们认为工作负载知识对系统级优化不是必需的。我们提出了System-X，它采用以硬件为中心的方法，仅依赖硬件信号——这些信号完全可被运营商访问。使用从系统收集的低级信号，System-X通过无监督学习管道检测异常。该管道是通过分析各种硬件平台上30多种流行的ML模型开发的，确保了对新兴工作负载和未知部署模式的适应性。使用System-X，我们成功识别了网络和系统配置问题，将DeepSeek模型加速了5.97%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern machine learning (ML) has grown into a tightly coupled, full-stackecosystem that combines hardware, software, network, and applications. Manyusers rely on cloud providers for elastic, isolated, and cost-efficientresources. Unfortunately, these platforms as a service use virtualization,which means operators have little insight into the users' workloads. Thishinders resource optimizations by the operator, which is essential to ensurecost efficiency and minimize execution time. In this paper, we argue thatworkload knowledge is unnecessary for system-level optimization. We proposeSystem-X, which takes a \emph{hardware-centric} approach, relying only onhardware signals -- fully accessible by operators. Using low-level signalscollected from the system, System-X detects anomalies through an unsupervisedlearning pipeline. The pipeline is developed by analyzing over 30 popular MLmodels on various hardware platforms, ensuring adaptability to emergingworkloads and unknown deployment patterns. Using System-X, we successfullyidentified both network and system configuration issues, accelerating theDeepSeek model by 5.97%.</description>
      <author>example@mail.com (Ziji Chen, Steven Chien, Peng Qian, Noa Zilberman)</author>
      <guid isPermaLink="false">2510.26008v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised local learning based on voltage-dependent synaptic plasticity for resistive and ferroelectric synapses</title>
      <link>http://arxiv.org/abs/2510.25787v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于忆阻器件的电压依赖性突触可塑性(VDSP)学习方法，用于解决边缘计算设备上AI部署的能耗问题。该方法实现了低功耗的无监督学习，在MNIST模式识别任务上取得了超过83%的准确率，并针对不同类型的忆阻器件进行了适应性调整和鲁棒性优化。&lt;h4&gt;背景&lt;/h4&gt;边缘计算设备上部署人工智能面临显著的能耗和功能性挑战。这些设备需要低功耗且能够实时适应的学习机制。基于纳米尺度电阻存储器的内存计算技术可能在这些边缘设备上执行AI工作负载方面发挥关键作用。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一种高效的无监督和局部学习方法，基于赫布原理在忆阻突触中实现，使AI能够在边缘设备上低功耗运行，同时保持高性能。&lt;h4&gt;方法&lt;/h4&gt;研究引入了电压依赖性突触可塑性(VDSP)作为基于赫布原理的忆阻突触无监督和局部学习方法。这种方法无需复杂脉冲整形电路即可实现在线学习，展示了如何将VDSP适应到三种具有不同开关特性的忆阻器件：TiO₂、基于HfO₂的金属氧化物丝状突触和基于HfZrO₄的铁电隧道结(FTJ)。通过系统级模拟验证了这些器件在脉冲神经网络中的无监督学习能力。&lt;h4&gt;主要发现&lt;/h4&gt;1. 所有测试的忆阻器件在使用200个神经元的情况下，在基于MNIST的模式识别任务上实现了超过83%的准确率，达到最先进性能。2. VDSP方法成功适应了三种不同类型的忆阻器件，证明了其通用性。3. 研究评估了器件变异性(如开关阈值和高低电阻状态水平比率)对性能的影响，并提出了增强系统鲁棒性的缓解策略。&lt;h4&gt;结论&lt;/h4&gt;VDSP方法为边缘计算设备上的AI部署提供了一种高效、低功耗的学习解决方案，无需复杂电路即可实现无监督学习。该方法在不同类型的忆阻器件上表现出色，并通过针对器件变异性的缓解策略增强了系统鲁棒性，为边缘AI应用提供了实用可行的技术路径。&lt;h4&gt;翻译&lt;/h4&gt;在边缘计算设备上部署人工智能面临与能耗和功能相关的重大挑战。这些设备可以从大脑启发的学习机制中极大受益，允许在低功耗条件下进行实时适应。使用纳米尺度电阻存储器的内存计算可能在使这些边缘设备上执行AI工作负载方面发挥关键作用。在本研究中，我们引入了电压依赖性突触可塑性(VDSP)作为一种基于赫布原理的忆阻突触中无监督和局部学习的高效方法。这种方法无需脉冲时间依赖可塑性(STDP)通常需要的复杂脉冲整形电路即可实现在线学习。我们展示了如何将VDSP advantageous地适应到三种具有不同开关特性的忆阻器件：TiO₂、基于HfO₂的金属氧化物丝状突触和基于HfZrO₄的铁电隧道结(FTJ)。进行了包含这些器件的脉冲神经网络系统级模拟，以验证基于MNIST的模式识别任务上的无监督学习，达到了最先进的性能。结果表明所有设备在使用200个神经元的情况下都实现了超过83%的准确率。此外，我们评估了器件变异性的影响，如开关阈值和高低电阻状态水平比率，并提出了增强鲁棒性的缓解策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of AI on edge computing devices faces significant challengesrelated to energy consumption and functionality. These devices could greatlybenefit from brain-inspired learning mechanisms, allowing for real-timeadaptation while using low-power. In-memory computing with nanoscale resistivememories may play a crucial role in enabling the execution of AI workloads onthese edge devices. In this study, we introduce voltage-dependent synapticplasticity (VDSP) as an efficient approach for unsupervised and local learningin memristive synapses based on Hebbian principles. This method enables onlinelearning without requiring complex pulse-shaping circuits typically necessaryfor spike-timing-dependent plasticity (STDP). We show how VDSP can beadvantageously adapted to three types of memristive devices (TiO$_2$,HfO$_2$-based metal-oxide filamentary synapses, and HfZrO$_4$-basedferroelectric tunnel junctions (FTJ)) with disctinctive switchingcharacteristics. System-level simulations of spiking neural networksincorporating these devices were conducted to validate unsupervised learning onMNIST-based pattern recognition tasks, achieving state-of-the-art performance.The results demonstrated over 83% accuracy across all devices using 200neurons. Additionally, we assessed the impact of device variability, such asswitching thresholds and ratios between high and low resistance state levels,and proposed mitigation strategies to enhance robustness.</description>
      <author>example@mail.com (Nikhil Garg, Ismael Balafrej, Joao Henrique Quintino Palhares, Laura Bégon-Lours, Davide Florini, Donato Francesco Falcone, Tommaso Stecconi, Valeria Bragaglia, Bert Jan Offrein, Jean-Michel Portal, Damien Querlioz, Yann Beilliard, Dominique Drouin, Fabien Alibart)</author>
      <guid isPermaLink="false">2510.25787v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>HEIR: Learning Graph-Based Motion Hierarchies</title>
      <link>http://arxiv.org/abs/2510.26786v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code link: https://github.com/princeton-computational-imaging/HEIR&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通用的层次运动建模方法，通过图神经网络直接从数据中学习结构化的、可解释的运动关系，克服了传统方法依赖手动定义层次结构和固定运动基元的局限性，在多种运动类型上展现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;运动的层次结构存在于计算机视觉、图形学和机器人学等多个研究领域，复杂动力学通常源于简单运动组件之间的协调相互作用。现有方法通常依赖于手动定义的或启发式的层次结构，具有固定的运动基元，限制了它们在不同任务间的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种通用的层次运动建模方法，直接从数据中学习结构化的、可解释的运动关系，适用于广泛的以运动为中心的任务。&lt;h4&gt;方法&lt;/h4&gt;使用基于图的层次结构表示观察到的运动，将全局绝对运动分解为父继承模式和局部运动残差；将层次推断制定为可微的图学习问题，其中顶点表示基本运动，有向边通过图神经网络捕获学习的父子依赖关系；在一维平移运动、二维旋转运动和动态三维场景变形三个示例上评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在一维和二维情况下成功重建了内在的运动层次结构；与基线相比，在动态3D高斯飞溅场景中产生了更真实、可解释的变形效果。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一种适应性强、数据驱动的层次建模范式，适用于广泛的以运动为中心的任务，通过学习而非预设层次结构实现了更好的泛化能力和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;运动层次结构存在于包括计算机视觉、图形学和机器人学在内的研究领域，其中复杂动力学通常源于简单运动组件之间的协调相互作用。现有方法对这类动力学进行建模通常依赖于手动定义的或启发式的层次结构，具有固定的运动基元，限制了它们在不同任务间的泛化能力。在本工作中，我们提出了一种通用的层次运动建模方法，直接从数据中学习结构化的、可解释的运动关系。我们的方法使用基于图的层次结构来表示观察到的运动，明确地将全局绝对运动分解为父继承模式和局部运动残差。我们将层次推断制定为可微的图学习问题，其中顶点表示基本运动，有向边通过图神经网络捕获学习的父子依赖关系。我们在三个示例上评估了我们的层次重建方法：一维平移运动、二维旋转运动和通过高斯飞溅的动态三维场景变形。实验结果表明，我们的方法在一维和二维情况下重建了内在的运动层次结构，与基线相比，在动态3D高斯飞溅场景中产生了更真实、可解释的变形。通过提供一种适应性强、数据驱动的层次建模范式，我们的方法适用于广泛的以运动为中心的任务。项目页面：https://light.princeton.edu/HEIR/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决运动层次结构的自动建模问题，现有方法通常依赖手动定义的层次结构或固定的运动基元，限制了跨任务泛化能力。这个问题在计算机视觉、图形学和机器人学等多个领域都很重要，因为复杂运动往往源于简单运动组件间的协调，层次结构能帮助理解、生成、预测和控制运动，解决多尺度依赖关系和组合爆炸问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法（手动定义模板或非可解释神经模块）的局限性，注意到不同研究领域面临共同挑战，需要自适应选择合适抽象层次的方法。他们借鉴了图神经网络思想用于学习边权重和父子关系，使用Gumbel-Softmax技巧处理离散层次结构的可微分采样，还参考了层次运动表示（如骨骼定义关系）和3D场景变形（如NeMF、MovingParts）等领域的现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于图的层次结构表示运动，将全局绝对运动分解为父继承模式和局部运动残差，将层次推断转化为可微分图学习问题。流程包括：1)构建邻近有向图，顶点表示运动元素；2)通过图注意力层计算边权重；3)使用Gumbel-Softmax采样层次结构；4)沿层次结构累积相对速度重建绝对运动；5)通过重建损失和正则化项训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：直接从数据学习可解释运动关系；使用图结构显式分解运动为父继承和局部残差；将层次推断转化为可微分图学习问题；适用于多种运动类型。相比之前工作，不依赖手动定义层次或固定基元，提供可解释结构，不假设特定领域或维度，在3D场景变形中产生更真实结果，具有更好泛化能力和可解释性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于图的层次运动学习方法HEIR，能够直接从数据中学习可解释的运动层次结构，有效分解复杂运动为父继承模式和局部残差，并在多种运动建模任务中展现出优越的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hierarchical structures of motion exist across research fields, includingcomputer vision, graphics, and robotics, where complex dynamics typically arisefrom coordinated interactions among simpler motion components. Existing methodsto model such dynamics typically rely on manually-defined or heuristichierarchies with fixed motion primitives, limiting their generalizabilityacross different tasks. In this work, we propose a general hierarchical motionmodeling method that learns structured, interpretable motion relationshipsdirectly from data. Our method represents observed motions using graph-basedhierarchies, explicitly decomposing global absolute motions intoparent-inherited patterns and local motion residuals. We formulate hierarchyinference as a differentiable graph learning problem, where vertices representelemental motions and directed edges capture learned parent-child dependenciesthrough graph neural networks. We evaluate our hierarchical reconstructionapproach on three examples: 1D translational motion, 2D rotational motion, anddynamic 3D scene deformation via Gaussian splatting. Experimental results showthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,and produces more realistic and interpretable deformations compared to thebaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,data-driven hierarchical modeling paradigm, our method offers a formulationapplicable to a broad range of motion-centric tasks. Project Page:https://light.princeton.edu/HEIR/</description>
      <author>example@mail.com (Cheng Zheng, William Koch, Baiang Li, Felix Heide)</author>
      <guid isPermaLink="false">2510.26786v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Graph Guided Modulo Recovery of EEG Signals</title>
      <link>http://arxiv.org/abs/2510.26756v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure, and 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络的GraphUnwrapNet方法，用于解决脑电图(EEG)信号模数采样恢复问题。通过将EEG信号表示为有组织的图结构，并引入预估计引导的特征注入模块，有效提升了在信号折叠边界处的恢复稳定性。实验结果表明，该方法优于传统优化技术，并与当前深度学习模型具有竞争性。&lt;h4&gt;背景&lt;/h4&gt;脑电图(EEG)在不同人之间常表现出显著变异性，这种波动会干扰可靠信号采集并可能导致信号失真或削波。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的方法从模数采样的折叠观测中恢复原始EEG信号，解决这一高度不适定问题。&lt;h4&gt;方法&lt;/h4&gt;提出GraphUnwrapNet，一种基于图神经网络的方法，将EEG信号表示为有组织的图结构，并引入预估计引导的特征注入模块，提供粗略的折叠指示器以增强恢复稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;在STEW数据集上的实验表明，与传统优化技术相比有持续提升，与当前深度学习模型相比具有竞争性的准确性。&lt;h4&gt;结论&lt;/h4&gt;基于图的方法在鲁棒模数EEG恢复方面具有显著潜力。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)通常表现出显著的个体间变异性。这种波动会干扰可靠的信号采集并可能导致失真或削波。模数采样现在是解决这个问题的有前途的方法，通过折叠信号而不是使它们饱和。从折叠观测中恢复原始波形是一个高度不适定的问题。在本工作中，我们提出了一种基于图神经网络的方法，称为GraphUnwrapNet，用于EEG信号的模数恢复。我们的核心思想是将EEG信号表示为一个有组织的图，其通道和时间连接建立了潜在的相互依赖关系。我们的一个关键贡献是引入了一个预估计引导的特征注入模块，提供粗略的折叠指示器，增强在折叠边界处的恢复稳定性。这种设计将结构信息与折叠先验集成到一个统一的框架中。我们在同时任务脑电图工作负荷(STEW)数据集上进行了全面的实验。结果表明与传统优化技术相比有持续的提升，与当前深度学习模型相比具有竞争性的准确性。我们的发现强调了基于图的方法在鲁棒模数EEG恢复方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) often shows significant variability amongpeople. This fluctuation disrupts reliable acquisition and may result indistortion or clipping. Modulo sampling is now a promising solution to thisproblem, by folding signals instead of saturating them. Recovery of theoriginal waveform from folded observations is a highly ill-posed problem. Inthis work, we propose a method based on a graph neural network, referred to asGraphUnwrapNet, for the modulo recovery of EEG signals. Our core idea is torepresent an EEG signal as an organized graph whose channels and temporalconnections establish underlying interdependence. One of our key contributionsis in introducing a pre-estimation guided feature injection module to providecoarse folding indicators that enhance stability during recovery at wrapboundaries. This design integrates structural information with folding priorsinto an integrated framework. We performed comprehensive experiments on theSimultaneous Task EEG Workload (STEW) dataset. The results demonstrateconsistent enhancements over traditional optimization techniques andcompetitive accuracy relative to current deep learning models. Our findingsemphasize the potential of graph-based methodology for robust modulo EEGrecovery.</description>
      <author>example@mail.com (Soujanya Hazra, Sanjay Ghosh)</author>
      <guid isPermaLink="false">2510.26756v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras</title>
      <link>http://arxiv.org/abs/2510.26614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对事件相机的tokenization方法，称为Spiking Patches，能够保留事件流的异步性和空间稀疏性特性，同时保持高准确性，并且推理速度比传统方法更快。&lt;h4&gt;背景&lt;/h4&gt;现有的事件表示方法（如帧或体素）虽然是同步的且降低了空间稀疏性，但能产生高准确性。&lt;h4&gt;目的&lt;/h4&gt;发现一种能够保留事件相机独特属性的事件表示方法。&lt;h4&gt;方法&lt;/h4&gt;提出Spiking Patches tokenizer，专门为事件相机设计，能够保留事件流的异步性和空间稀疏性。&lt;h4&gt;主要发现&lt;/h4&gt;使用GNN、PCN和Transformer在手势识别和物体检测任务上评估，Spiking Patches的token比基于体素的token推理速度快3.4倍，比基于帧的token推理速度快10.4倍，在保持相同准确性的同时，在某些情况下甚至超越它们，手势识别绝对改进最高达3.8，物体检测绝对改进最高达1.4。&lt;h4&gt;结论&lt;/h4&gt;tokenization是事件视觉领域的新方向，标志着保留事件相机属性方法的发展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出事件的tokenization并展示了一个tokenizer，Spiking Patches，专门为事件相机设计。给定异步和空间稀疏的事件流，我们的目标是发现保留这些属性的事件表示。先前的工作将事件表示为帧或体素。然而，虽然这些表示能产生高准确性，但帧和体素都是同步的，降低了空间稀疏性。Spiking Patches提供了保留事件相机独特属性的方法，我们在实验中证明这不会牺牲准确性。我们使用GNN、PCN和Transformer在手势识别和物体检测任务上评估我们的tokenizer。来自Spiking Patches的token比基于体素的token推理速度快3.4倍，比基于帧的token推理速度快10.4倍。我们在保持相同准确性的同时实现了这一点，在某些情况下甚至超越它们，手势识别绝对改进最高达3.8，物体检测绝对改进最高达1.4。因此，tokenization构成了事件视觉领域的一个新方向，标志着保留事件相机属性方法的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose tokenization of events and present a tokenizer, Spiking Patches,specifically designed for event cameras. Given a stream of asynchronous andspatially sparse events, our goal is to discover an event representation thatpreserves these properties. Prior works have represented events as frames or asvoxels. However, while these representations yield high accuracy, both framesand voxels are synchronous and decrease the spatial sparsity. Spiking Patchesgives the means to preserve the unique properties of event cameras and we showin our experiments that this comes without sacrificing accuracy. We evaluateour tokenizer using a GNN, PCN, and a Transformer on gesture recognition andobject detection. Tokens from Spiking Patches yield inference times that are upto 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. Weachieve this while matching their accuracy and even surpassing in some caseswith absolute improvements up to 3.8 for gesture recognition and up to 1.4 forobject detection. Thus, tokenization constitutes a novel direction inevent-based vision and marks a step towards methods that preserve theproperties of event cameras.</description>
      <author>example@mail.com (Christoffer Koo Øhrstrøm, Ronja Güldenring, Lazaros Nalpantidis)</author>
      <guid isPermaLink="false">2510.26614v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation</title>
      <link>http://arxiv.org/abs/2510.26350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出UnifiedFL，一种动态联邦学习框架，用于处理具有不同神经网络架构和非相同分布数据的客户端之间的协作训练，通过图神经网络优化异构本地网络，实验证明在多个基准测试中表现优越。&lt;h4&gt;背景&lt;/h4&gt;联邦学习作为关键范式，允许多个客户端在不共享原始数据的情况下协作训练模型，支持隐私保护应用。然而，关于具有不同神经网络架构和非相同分布数据集的客户端之间的协作训练研究仍然很少。&lt;h4&gt;目的&lt;/h4&gt;解决现有联邦学习框架在支持根本不同架构客户端、处理数据统计异质性和领域断裂问题上的局限性，提高模型在不同测试域上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出UnifiedFL框架，将异构本地网络表示为有向模型图中的节点和边，通过共享图神经网络优化。引入通用GNN参数化所有架构、基于客户端参数之间欧几里得距离的距离驱动聚类，以及平衡收敛性和多样性的两层聚合策略。&lt;h4&gt;主要发现&lt;/h4&gt;现有联邦学习方法只能支持单一模型家族内的变体，假设共享全局架构，无法适应不同网络类型；现有方法通常只处理统计异质性，忽略领域断裂问题；当客户端使用不同架构、具有非相同分布数据并遇到不同测试域时，当前方法表现不佳。&lt;h4&gt;结论&lt;/h4&gt;UnifiedFL在MedMNIST分类和海马体分割基准测试中表现出优越性能，代码和数据可在https://github.com/basiralab/UnifiedFL获取。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习（FL）已成为一种关键范式，使多个客户端能够在不共享原始数据的情况下协作训练模型，在放射学和病理学等领域支持隐私保护应用。然而，关于具有根本不同神经网络架构和非相同分布数据集的客户端之间的协作训练研究仍然很少。现有的联邦学习框架面临几个局限性。尽管声称支持架构异构性，但大多数联邦学习方法只容忍单一模型家族内的变体（例如，更浅、更深或更宽的CNN），仍然假设共享全局架构，无法适应客户端部署不同网络类型（例如，CNN、GNN、MLP）的联邦。此外，现有方法通常只处理统计异质性，而忽略了领域断裂问题，即每个客户端的数据分布与测试时面临的数据分布明显不同，从而削弱了模型的泛化能力。当客户端使用不同架构、具有非相同分布数据并遇到不同的测试域时，当前方法表现不佳。为解决这些挑战，我们提出UnifiedFL，一种动态联邦学习框架，将异构本地网络表示为有向模型图中的节点和边，并通过共享的图神经网络（GNN）进行优化。UnifiedFL引入了（i）通用GNN参数化所有架构，（ii）基于客户端参数之间欧几里得距离的距离驱动聚类，以及（iii）平衡收敛性和多样性的两层聚合策略。在MedMNIST分类和海马体分割基准测试中进行的实验证明了UnifiedFL的优越性能。代码和数据：https://github.com/basiralab/UnifiedFL&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated learning (FL) has emerged as a key paradigm for collaborative modeltraining across multiple clients without sharing raw data, enablingprivacy-preserving applications in areas such as radiology and pathology.However, works on collaborative training across clients with fundamentallydifferent neural architectures and non-identically distributed datasets remainscarce. Existing FL frameworks face several limitations. Despite claiming tosupport architectural heterogeneity, most recent FL methods only toleratevariants within a single model family (e.g., shallower, deeper, or wider CNNs),still presuming a shared global architecture and failing to accommodatefederations where clients deploy fundamentally different network types (e.g.,CNNs, GNNs, MLPs). Moreover, existing approaches often address only statisticalheterogeneity while overlooking the domain-fracture problem, where eachclient's data distribution differs markedly from that faced at testing time,undermining model generalizability. When clients use different architectures,have non-identically distributed data, and encounter distinct test domains,current methods perform poorly. To address these challenges, we proposeUnifiedFL, a dynamic federated learning framework that represents heterogeneouslocal networks as nodes and edges in a directed model graph optimized by ashared graph neural network (GNN). UnifiedFL introduces (i) a common GNN toparameterize all architectures, (ii) distance-driven clustering via Euclideandistances between clients' parameters, and (iii) a two-tier aggregation policybalancing convergence and diversity. Experiments on MedMNIST classification andhippocampus segmentation benchmarks demonstrate UnifiedFL's superiorperformance. Code and data: https://github.com/basiralab/UnifiedFL</description>
      <author>example@mail.com (Furkan Pala, Islem Rekik)</author>
      <guid isPermaLink="false">2510.26350v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>From Embedding to Control: Representations for Stochastic Multi-Object Systems</title>
      <link>http://arxiv.org/abs/2510.26344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出图可控嵌入（GCE）框架，用于学习线性控制下的随机多对象动力学系统。该框架基于希尔伯特空间嵌入，将受控随机动力学的概率分布嵌入到再生核希尔伯特空间中，保留非线性表达能力的同时支持线性操作。GCE采用平均场近似技术捕获对象间依赖关系，并通过图神经网络构建适应动态交互模式的核特征，能够推广到未见过的拓扑结构。&lt;h4&gt;背景&lt;/h4&gt;具有多个相互作用的随机非线性动力学系统的精确建模和控制是一个具有挑战性的问题。非均匀交互和随机拓扑结构使得这一任务更加困难，需要开发新的方法来处理这些复杂情况。&lt;h4&gt;目的&lt;/h4&gt;研究如何实现具有多个相互作用的随机非线性动力学系统的精确建模和有效控制，特别是应对非均匀交互和随机拓扑带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了图可控嵌入（GCE）框架，这是一种基于希尔伯特空间嵌入的通用方法。GCE将受控随机动力学的概率分布直接嵌入到再生核希尔伯特空间（RKHS）中，允许在保留非线性表达能力的同时进行线性操作。该方法采用平均场近似技术来捕获对象间依赖关系，并通过整合图神经网络构建数据相关的核特征，使其能够适应动态交互模式并推广到未见过的拓扑结构。&lt;h4&gt;主要发现&lt;/h4&gt;1. GCE提供了关于存在性、收敛性和适用性的理论保证；2. 平均场近似技术能够有效捕获对象间依赖关系，实现低样本复杂度；3. 构建的核特征能够适应动态交互模式，仅用有限训练实例即可推广到未见拓扑；4. GCE可无缝扩展到不同大小和拓扑的多对象系统；5. 希尔伯特空间的线性支持简单而有效的控制算法来合成最优序列。&lt;h4&gt;结论&lt;/h4&gt;图可控嵌入（GCE）框架为随机多对象动力学系统的建模和控制提供了一种有效方法。通过结合希尔伯特空间嵌入、平均场近似和图神经网络，GCE能够处理非均匀交互和随机拓扑带来的挑战，并在物理系统、机器人和电力系统实验中展现出优越性能，特别是在分布内和少样本测试中优于其他嵌入方法。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了如何在具有多个相互作用的随机非线性动力学系统中实现精确建模和有效控制。然而，非均匀交互和随机拓扑使这一任务具有挑战性。我们通过提出图可控嵌入（GCE）来解决这些挑战，这是一个用于学习线性控制下随机多对象动力学的通用框架。具体来说，GCE建立在希尔伯特空间嵌入的基础上，允许将受控随机动力学的概率分布直接嵌入到再生核希尔伯特空间（RKHS）中，这使其RKHS中的线性操作能够保留非线性表达能力。我们提供了关于GCE的存在性、收敛性和适用性的理论保证。值得注意的是，采用平均场近似技术来有效捕获对象间依赖关系，并实现可证明的低样本复杂度。通过整合图神经网络，我们构建了能够适应动态交互模式的数据相关核特征，并且仅用有限的训练实例就能推广到未见过的拓扑结构。GCE可以无缝扩展到不同大小和拓扑的多对象系统。利用希尔伯特空间的线性，GCE还支持简单而有效的控制算法来合成最优序列。在物理系统、机器人和电力系统上的实验验证了GCE的有效性，并在分布内和少样本测试中，相比各种有竞争力的嵌入方法都展现出一致的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper studies how to achieve accurate modeling and effective control instochastic nonlinear dynamics with multiple interacting objects. However,non-uniform interactions and random topologies make this task challenging. Weaddress these challenges by proposing \textit{Graph Controllable Embeddings}(GCE), a general framework to learn stochastic multi-object dynamics for linearcontrol. Specifically, GCE is built on Hilbert space embeddings, allowingdirect embedding of probability distributions of controlled stochastic dynamicsinto a reproducing kernel Hilbert space (RKHS), which enables linear operationsin its RKHS while retaining nonlinear expressiveness. We provide theoreticalguarantees on the existence, convergence, and applicability of GCE. Notably, amean field approximation technique is adopted to efficiently captureinter-object dependencies and achieve provably low sample complexity. Byintegrating graph neural networks, we construct data-dependent kernel featuresthat are capable of adapting to dynamic interaction patterns and generalizingto even unseen topologies with only limited training instances. GCE scalesseamlessly to multi-object systems of varying sizes and topologies. Leveragingthe linearity of Hilbert spaces, GCE also supports simple yet effective controlalgorithms for synthesizing optimal sequences. Experiments on physical systems,robotics, and power grids validate GCE and demonstrate consistent performanceimprovement over various competitive embedding methods in both in-distributionand few-shot tests</description>
      <author>example@mail.com (Xiaoyuan Cheng, Yiming Yang, Wei Jiang, Chenyang Yuan, Zhuo Sun, Yukun Hu)</author>
      <guid isPermaLink="false">2510.26344v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Heterogeneous Graph Neural Networks for Cybersecurity Anomaly Detection</title>
      <link>http://arxiv.org/abs/2510.26307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 4 figures, 86 references. Submitted to Journal of Computer  Security (under review)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提供了对网络安全中基于异构图神经网络(HGNN)的异常检测方法的全面综述，建立了分类法，分析了代表性模型，回顾了基准数据集和评估指标，并确定了未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;异常检测在网络安全中是关键任务，需要识别内部威胁、访问违规和协调攻击。基于图的方法在建模实体交互方面变得越来越重要，但大多数依赖于同质和静态结构，这限制了它们捕捉现实环境中异构性和时间演化的能力。&lt;h4&gt;目的&lt;/h4&gt;解决基于HGNN的异常检测研究分散、缺乏比较评估和标准化基准的问题，为该领域建立结构化的基础。&lt;h4&gt;方法&lt;/h4&gt;引入按异常类型和图动力学对方法进行分类的分类法，分析代表性模型并将其映射到关键网络安全应用，回顾常用基准数据集和评估指标，强调其优缺点。&lt;h4&gt;主要发现&lt;/h4&gt;确定了与建模、数据和部署相关的主要开放挑战，概述了未来研究的有希望的方向。&lt;h4&gt;结论&lt;/h4&gt;该综述旨在为推进基于HGNN的异常检测建立结构化的基础，使其可扩展、可解释且实际可部署。&lt;h4&gt;翻译&lt;/h4&gt;异常检测是网络安全中的关键任务，其中识别内部威胁、访问违规和协调攻击对于确保系统弹性至关重要。基于图的方法在建模实体交互方面变得越来越重要，但大多数依赖于同质和静态结构，这限制了它们捕捉现实环境中异构性和时间演化的能力。异构图神经网络已成为一种有前景的异常检测范式，通过整合类型感知变换和关系敏感聚合，能够对复杂的网络数据进行更具表现力的建模。然而，当前关于基于HGNN的异常检测研究仍然分散，建模策略多样，比较评估有限，且缺乏标准化基准。为了解决这一差距，我们对网络安全中基于HGNN的异常检测方法进行了全面综述。我们引入了一个按异常类型和图动力学对方法进行分类的分类法，分析了代表性模型，并将它们映射到关键的网络安全应用。我们还回顾了常用的基准数据集和评估指标，强调了它们的优缺点。最后，我们确定了与建模、数据和部署相关的主要开放挑战，并概述了未来研究的有希望的方向。本综述旨在为推进基于HGNN的异常检测建立结构化的基础，朝着可扩展、可解释和实际可部署的解决方案发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection is a critical task in cybersecurity, where identifyinginsider threats, access violations, and coordinated attacks is essential forensuring system resilience. Graph-based approaches have become increasinglyimportant for modeling entity interactions, yet most rely on homogeneous andstatic structures, which limits their ability to capture the heterogeneity andtemporal evolution of real-world environments. Heterogeneous Graph NeuralNetworks (HGNNs) have emerged as a promising paradigm for anomaly detection byincorporating type-aware transformations and relation-sensitive aggregation,enabling more expressive modeling of complex cyber data. However, currentresearch on HGNN-based anomaly detection remains fragmented, with diversemodeling strategies, limited comparative evaluation, and an absence ofstandardized benchmarks. To address this gap, we provide a comprehensive surveyof HGNN-based anomaly detection methods in cybersecurity. We introduce ataxonomy that classifies approaches by anomaly type and graph dynamics, analyzerepresentative models, and map them to key cybersecurity applications. We alsoreview commonly used benchmark datasets and evaluation metrics, highlightingtheir strengths and limitations. Finally, we identify key open challengesrelated to modeling, data, and deployment, and outline promising directions forfuture research. This survey aims to establish a structured foundation foradvancing HGNN-based anomaly detection toward scalable, interpretable, andpractically deployable solutions.</description>
      <author>example@mail.com (Laura Jiang, Reza Ryan, Qian Li, Nasim Ferdosian)</author>
      <guid isPermaLink="false">2510.26307v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Morphology-Aware Graph Reinforcement Learning for Tensegrity Robot Locomotion</title>
      <link>http://arxiv.org/abs/2510.26067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种形态感知的强化学习框架，通过将图神经网络集成到Soft Actor-Critic算法中，解决了张力完整性机器人的运动控制问题。&lt;h4&gt;背景&lt;/h4&gt;张力完整性机器人结合刚性杆和弹性缆索，具有高弹性和可部署性，但因其欠驱动和高度耦合的动力学特性，在运动控制方面面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种强化学习方法，利用机器人的结构先验知识，提高张力完整性机器人的运动控制性能。&lt;h4&gt;方法&lt;/h4&gt;将机器人的物理拓扑表示为图，使用基于图神经网络(GNN)的策略捕捉组件间的耦合关系，并将其集成到Soft Actor-Critic (SAC)算法中。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在物理三杆张力完整性机器人上得到验证，在直线跟踪和双向转向等运动任务中表现出优异的样本效率、对噪声和刚度变化的鲁棒性以及改进的轨迹精度；学习到的策略可以直接从模拟转移到硬件，无需微调。&lt;h4&gt;结论&lt;/h4&gt;将结构先验知识整合到强化学习中对于张力完整性机器人控制具有显著优势，能够实现更高效、更稳定的控制策略。&lt;h4&gt;翻译&lt;/h4&gt;张力完整性机器人结合刚性杆和弹性缆索，提供高弹性和可部署性，但由于其欠驱动和高度耦合的动力学特性，给运动控制带来了重大挑战。本文引入了一种形态感知的强化学习框架，将图神经网络(GNN)集成到Soft Actor-Critic (SAC)算法中。通过将机器人的物理拓扑表示为图，所提出的基于GNN的策略捕捉了组件之间的耦合关系，实现了比传统多层感知器(MLP)策略更快且更稳定的学习。该方法在物理三杆张力完整性机器人上得到了验证，包括直线跟踪和双向转向在内的三种运动原语。它显示出优异的样本效率、对噪声和刚度变化的鲁棒性，以及改进的轨迹精度。值得注意的是，学习到的策略可以直接从模拟转移到硬件而无需微调，实现了稳定的真实世界运动。这些结果表明，将结构先验知识整合到强化学习中对于张力完整性机器人控制具有优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tensegrity robots combine rigid rods and elastic cables, offering highresilience and deployability but posing major challenges for locomotion controldue to their underactuated and highly coupled dynamics. This paper introduces amorphology-aware reinforcement learning framework that integrates a graphneural network (GNN) into the Soft Actor-Critic (SAC) algorithm. Byrepresenting the robot's physical topology as a graph, the proposed GNN-basedpolicy captures coupling among components, enabling faster and more stablelearning than conventional multilayer perceptron (MLP) policies. The method isvalidated on a physical 3-bar tensegrity robot across three locomotionprimitives, including straight-line tracking and bidirectional turning. Itshows superior sample efficiency, robustness to noise and stiffness variations,and improved trajectory accuracy. Notably, the learned policies transferdirectly from simulation to hardware without fine-tuning, achieving stablereal-world locomotion. These results demonstrate the advantages ofincorporating structural priors into reinforcement learning for tensegrityrobot control.</description>
      <author>example@mail.com (Chi Zhang, Mingrui Li, Wenzhe Tong, Xiaonan Huang)</author>
      <guid isPermaLink="false">2510.26067v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Data-driven Projection Generation for Efficiently Solving Heterogeneous Quadratic Programming Problems</title>
      <link>http://arxiv.org/abs/2510.26061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出一种数据驱动的框架，通过针对特定实例的投影减少高维二次规划问题的变量数量，使用图神经网络生成定制化投影，高效解决二次规划问题。&lt;h4&gt;背景&lt;/h4&gt;二次规划问题在高维情况下求解复杂度高，传统方法面临计算挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效解决高维二次规划问题的方法，通过减少变量数量降低计算复杂度，同时保证解的质量。&lt;h4&gt;方法&lt;/h4&gt;设计基于图神经网络的模型生成定制化投影；使用双层优化训练模型，内层优化在给定投影下解决QP问题，外层优化更新模型参数；开发高效算法计算参数梯度，无需通过求解器反向传播；提供神经网络生成投影矩阵解决QP问题的泛化能力理论分析。&lt;h4&gt;主要发现&lt;/h4&gt;方法能产生高质量可行解并减少计算时间；即使对未见过的QP问题也能生成高质量解决方案；实验结果优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的数据驱动框架通过针对特定实例的投影和图神经网络模型，能高效解决高维二次规划问题，在保证解质量的同时显著减少计算时间。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种数据驱动的框架，通过使用针对特定实例的投影来减少高维二次规划问题中的变量数量，从而高效解决二次规划问题。我们设计了一个基于图神经网络的模型，为每个二次规划实例生成定制化投影，使我们能够即使对于未见过的也能产生高质量解。该模型在异构QP上进行训练，以最小化在投影解上评估的期望目标值。这被表述为一个双层优化问题；内层优化在给定投影下使用QP求解器解决QP问题，而外层优化更新模型参数。我们开发了一种高效算法来解决这个双层优化问题，计算参数梯度时无需通过求解器进行反向传播。我们提供了使用神经网络生成的投影矩阵解决QP问题的泛化能力理论分析。实验结果表明，我们的方法产生了高质量可行解并减少了计算时间，优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a data-driven framework for efficiently solving quadraticprogramming (QP) problems by reducing the number of variables inhigh-dimensional QPs using instance-specific projection. A graph neuralnetwork-based model is designed to generate projections tailored to each QPinstance, enabling us to produce high-quality solutions even for previouslyunseen problems. The model is trained on heterogeneous QPs to minimize theexpected objective value evaluated on the projected solutions. This isformulated as a bilevel optimization problem; the inner optimization solves theQP under a given projection using a QP solver, while the outer optimizationupdates the model parameters. We develop an efficient algorithm to solve thisbilevel optimization problem, which computes parameter gradients withoutbackpropagating through the solver. We provide a theoretical analysis of thegeneralization ability of solving QPs with projection matrices generated byneural networks. Experimental results demonstrate that our method produceshigh-quality feasible solutions with reduced computation time, outperformingexisting methods.</description>
      <author>example@mail.com (Tomoharu Iwata, Futoshi Futami)</author>
      <guid isPermaLink="false">2510.26061v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Robust GNN Watermarking via Implicit Perception of Topological Invariants</title>
      <link>http://arxiv.org/abs/2510.25934v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为InvGNN-WM的图神经网络水印技术，它不依赖后门触发器，而是将所有权与模型对图不变性的隐式感知联系起来，实现了黑盒验证且对任务影响微小的水印方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是有价值的知识产权，但现有水印技术大多依赖后门触发器，这些触发器在常见模型编辑下会被破坏，导致所有权模糊。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需触发器、支持黑盒验证且对任务影响微小的图神经网络水印技术，以解决现有水印技术的局限性。&lt;h4&gt;方法&lt;/h4&gt;使用轻量级头在所有者私有的载体集上预测归一化代数连通性；使用敏感解码器输出比特；使用校准阈值控制误报率。&lt;h4&gt;主要发现&lt;/h4&gt;在多种节点和图分类数据集和主干网络上，InvGNN-WM保持了清洁准确率，同时比基线方法产生更高的水印准确率；该方法在非结构化剪枝、微调和后训练量化条件下保持强健性；纯知识蒸馏会削弱水印，而带有水印损失的知识蒸馏可以恢复水印。&lt;h4&gt;结论&lt;/h4&gt;InvGNN-WM提供了不可感知性和鲁棒性的保证，精确移除该水印被证明是NP完全问题。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)是有价值的知识产权，但许多水印依赖于后门触发器，这些触发器在常见的模型编辑下会被破坏并导致所有权模糊。我们提出了InvGNN-WM，它将所有权与模型对图不变性的隐式感知联系起来，实现了无需触发器、黑盒验证且对任务影响微小的水印。轻量级头在所有者私有的载体集上预测归一化代数连通性；敏感解码器输出比特，校准阈值控制误报率。在多样化的节点和图分类数据集及主干网络上，InvGNN-WM匹配清洁准确率，同时比基于触发器和压缩的基线产生更高的水印准确率。它在非结构化剪枝、微调和后训练量化下保持强健性；普通知识蒸馏(KD)会削弱水印，而带有水印损失的KD(KD+WM)可恢复它。我们提供了不可感知性和鲁棒性的保证，并证明精确移除是NP完全的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are valuable intellectual property, yet manywatermarks rely on backdoor triggers that break under common model edits andcreate ownership ambiguity. We present InvGNN-WM, which ties ownership to amodel's implicit perception of a graph invariant, enabling trigger-free,black-box verification with negligible task impact. A lightweight head predictsnormalized algebraic connectivity on an owner-private carrier set; asign-sensitive decoder outputs bits, and a calibrated threshold controls thefalse-positive rate. Across diverse node and graph classification datasets andbackbones, InvGNN-WM matches clean accuracy while yielding higher watermarkaccuracy than trigger- and compression-based baselines. It remains strong underunstructured pruning, fine-tuning, and post-training quantization; plainknowledge distillation (KD) weakens the mark, while KD with a watermark loss(KD+WM) restores it. We provide guarantees for imperceptibility and robustness,and we prove that exact removal is NP-complete.</description>
      <author>example@mail.com (Jipeng Li, Yannning Shen)</author>
      <guid isPermaLink="false">2510.25934v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Attention Augmented GNN RNN-Attention Models for Advanced Cybersecurity Intrusion Detection</title>
      <link>http://arxiv.org/abs/2510.25802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型混合深度学习架构，结合图神经网络、循环神经网络和多头注意力机制，显著提升了网络安全入侵检测能力。&lt;h4&gt;背景&lt;/h4&gt;现代网络安全环境需要实时入侵检测系统，且需要将计算资源集中在高影响安全事件上。UNSW-NB15数据集包含多样的网络流量模式，为研究提供了基础。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕获网络流量中的空间依赖性和时间动态性的入侵检测系统，提高检测复杂攻击模式的能力。&lt;h4&gt;方法&lt;/h4&gt;提出混合深度学习架构，结合图神经网络(GNNs)捕获空间依赖性、循环神经网络(RNNs)进行序列分析，以及多头注意力机制提高模型可解释性和特征选择。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，与传统机器学习方法和独立深度学习模型相比，该混合模型在准确率、精确率、召回率和F1分数等多个评估指标上表现更优。特别是在检测高级持续性威胁(APTs)、分布式拒绝服务(DDoS)攻击和零日漏洞等复杂攻击模式方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;该混合模型是复杂网络环境中下一代网络安全应用的有前景解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种新型混合深度学习架构，协同结合图神经网络(GNNs)、循环神经网络(RNNs)和多头注意力机制，显著提升了网络安全入侵检测能力。通过利用包含多样化网络流量模式的UNSW-NB15综合数据集，我们的方法有效地通过图结构关系捕获空间依赖性，并通过网络事件的序列分析捕获时间动态性。集成的注意力机制提供了提高模型可解释性和增强特征选择的双重好处，使网络安全分析师能够将计算资源集中在高影响安全事件上——这是现代实时入侵检测系统的关键要求。我们广泛的实验评估表明，与传统机器学习方法和独立的深度学习模型相比，所提出的混合模型在多个评估指标上实现了优越的性能，包括准确率、精确率、召回率和F1分数。该模型在检测高级持续性威胁(APTs)、分布式拒绝服务(DDoS)攻击和零日漏洞等复杂攻击模式方面表现出特别强的性能，使其成为复杂网络环境中下一代网络安全应用的有前景解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a novel hybrid deep learning architecture thatsynergistically combines Graph Neural Networks (GNNs), Recurrent NeuralNetworks (RNNs), and multi-head attention mechanisms to significantly enhancecybersecurity intrusion detection capabilities. By leveraging the comprehensiveUNSW-NB15 dataset containing diverse network traffic patterns, our approacheffectively captures both spatial dependencies through graph structuralrelationships and temporal dynamics through sequential analysis of networkevents. The integrated attention mechanism provides dual benefits of improvedmodel interpretability and enhanced feature selection, enabling cybersecurityanalysts to focus computational resources on high-impact security events -- acritical requirement in modern real-time intrusion detection systems. Ourextensive experimental evaluation demonstrates that the proposed hybrid modelachieves superior performance compared to traditional machine learningapproaches and standalone deep learning models across multiple evaluationmetrics, including accuracy, precision, recall, and F1-score. The modelachieves particularly strong performance in detecting sophisticated attackpatterns such as Advanced Persistent Threats (APTs), Distributed Denial ofService (DDoS) attacks, and zero-day exploits, making it a promising solutionfor next-generation cybersecurity applications in complex network environments.</description>
      <author>example@mail.com (Jayant Biradar, Smit Shah, Tanmay Naik)</author>
      <guid isPermaLink="false">2510.25802v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>SHA-256 Infused Embedding-Driven Generative Modeling of High-Energy Molecules in Low-Data Regimes</title>
      <link>http://arxiv.org/abs/2510.25788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合LSTM网络和注意力GNN的新方法用于高能分子生成和属性预测，通过创新的嵌入空间构建策略实现了67.5%的有效性和37.5%的新颖性，成功识别出37种新型超爆炸物。&lt;h4&gt;背景&lt;/h4&gt;高能材料在推进和防御领域至关重要，但其发现受限于实验数据和测试设施的有限获取。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法来发现高能分子，特别是高能爆炸材料。&lt;h4&gt;方法&lt;/h4&gt;结合长短期记忆网络(LSTM)进行分子生成，使用注意力图神经网络(GNN)进行属性预测，提出了一种创新的嵌入空间构建策略，整合固定的SHA-256嵌入和部分可训练表示，在学习开始前重塑分子输入空间，不依赖预训练。&lt;h4&gt;主要发现&lt;/h4&gt;生成器达到67.5%的有效性和37.5%的新颖性；生成的库相对于训练集的平均Tanimoto系数为0.214，表明框架能够生成多样化的化学空间；识别出37种新型超爆炸物，预测爆速超过9公里/秒。&lt;h4&gt;结论&lt;/h4&gt;这种新方法能够有效发现新型高能材料，特别是超爆炸物，为高能材料的研究提供了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;高能材料(HEMs)对于推进和防御领域至关重要，但其发现受限于实验数据和测试设施的有限获取。这项工作通过结合用于分子生成的长短期记忆网络(LSTM)和用于属性预测的注意力图神经网络(GNN)，提出了一种针对高能分子的新方法。我们提出了一种变革性的嵌入空间构建策略，整合固定的SHA-256嵌入和部分可训练表示。与传统的正则化技术不同，这改变了表示基础本身，在学习开始前重塑了分子输入空间。无需依赖预训练，生成器实现了67.5%的有效性和37.5%的新颖性。生成的库相对于训练集的平均Tanimoto系数为0.214，表明该框架能够生成多样化的化学空间。我们识别出37种新型超爆炸物，预测爆速超过9公里/秒。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-energy materials (HEMs) are critical for propulsion and defense domains,yet their discovery remains constrained by experimental data and restrictedaccess to testing facilities. This work presents a novel approach towardhigh-energy molecules by combining Long Short-Term Memory (LSTM) networks formolecular generation and Attentive Graph Neural Networks (GNN) for propertypredictions. We propose a transformative embedding space construction strategythat integrates fixed SHA-256 embeddings with partially trainablerepresentations. Unlike conventional regularization techniques, this changesthe representational basis itself, reshaping the molecular input space beforelearning begins. Without recourse to pretraining, the generator achieves 67.5%validity and 37.5% novelty. The generated library exhibits a mean Tanimotocoefficient of 0.214 relative to training set signifying the ability offramework to generate a diverse chemical space. We identified 37 new superexplosives higher than 9 km/s predicted detonation velocity.</description>
      <author>example@mail.com (Siddharth Verma, Alankar Alankar)</author>
      <guid isPermaLink="false">2510.25788v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>4-Doodle: Text to 3D Sketches that Move!</title>
      <link>http://arxiv.org/abs/2510.25319v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了4-Doodle框架，一个从文本描述生成动态3D草图动画的无需训练方法，通过双空间蒸馏方案解决文本到3D草图动画任务中的关键挑战。&lt;h4&gt;背景&lt;/h4&gt;现有3D内容生成方法主要关注逼真内容，忽视了稀疏、风格化的3D矢量草图这一轻量级媒介。该任务面临三大挑战：缺乏配对数据集、结构抽象难以建模、动画需要时间一致性和多视角一致性。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需训练的框架，从文本生成动态、时间一致且多视角一致的3D矢量草图动画，实现结构稳定的动画效果，包括翻转、旋转和关节运动等。&lt;h4&gt;方法&lt;/h4&gt;4-Doodle采用双空间蒸馏方案：一个空间使用可微贝塞尔曲线捕获多视角一致的几何形状；另一个通过时间感知先验编码运动动态。采用多视图优化确保结构对齐，并引入结构感知的运动模块将保持形状的轨迹与变形感知的变化分开。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，4-Doodle能生成时间逼真且结构稳定的3D草图动画，在保真度和可控性方面优于现有基线。多视图优化确保结构对齐，结构感知运动模块实现富有表现力的动画效果。&lt;h4&gt;结论&lt;/h4&gt;4-Doodle为文本到动态3D草图动画提供了有效解决方案，使4D内容创作更加直观和易于访问，填补了相关研究空白，为视觉交流和原型设计提供新可能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一个新任务：文本到3D草图动画，旨在让自由形式的草图在动态3D空间中'活起来'。与专注于生成逼真内容的前期工作不同，我们目标是稀疏的、风格化的和视角一致的3D矢量草图，这是一种轻量级且可解释的媒介，非常适合视觉交流和原型设计。然而，这项任务非常具有挑战性：(i) 没有文本和3D（或4D）草图的配对数据集；(ii) 草图需要结构抽象，难以用传统的3D表示建模；(iii) 为这样的草图添加动画需要时间一致性和多视角一致性，而当前的处理流程无法解决这个问题。因此，我们提出了4-Doodle，这是第一个从文本生成动态3D草图的无需训练的框架。它通过双空间蒸馏方案利用预训练的图像和视频扩散模型：一个空间使用可微的贝塞尔曲线捕获多视角一致的几何形状，而另一个通过时间感知先验编码运动动态。与之前的工作不同，后者每步从单一视图进行优化，而我们的多视图优化确保了结构对齐并避免了视图模糊性，这对稀疏草图至关重要。此外，我们引入了一个结构感知的运动模块，该模块将保持形状的轨迹与变形感知的变化分开，实现翻转、旋转和关节运动等富有表现力的动作。大量实验表明，我们的方法能够生成时间上逼真且结构稳定的3D草图动画，在保真度和可控性方面都优于现有的基线。我们希望这项工作能推动更加直观和易于访问的4D内容创作发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从文本描述生成动态3D矢量草图的问题。这个问题很重要，因为随着空间计算平台（如Apple Vision Pro和Meta Quest）的兴起，创建和动画3D草图成为沉浸式内容创建的基础。传统的3D内容生成方法主要关注照片真实感内容，而3D草图作为一种轻量级、可解释的媒介，非常适合设计原型、视觉叙事和空间用户界面等应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将问题分解为两个互补阶段：构建一致的3D草图结构和添加动画。他们借鉴了现有工作，如DreamFusion中的Score Distillation Sampling技术、3Doodle中的贝塞尔曲线表示、LiveSketch中的运动先验蒸馏，以及MVDream等多视角扩散模型。作者的核心创新在于设计了一个双空间知识蒸馏框架，利用预训练的图像和视频扩散模型，通过多视角优化和基于贝塞尔曲线的表示来生成动态3D草图，避免了需要成对的文本-4D草图训练数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是双空间知识蒸馏框架，利用预训练的图像和视频扩散模型将3D结构和运动动力学的知识转移过来，无需特定训练数据。整体流程分为两个阶段：第一阶段是多视角3D草图生成，通过随机初始化贝塞尔曲线，从多个视角（前、后、左、右）渲染并使用Score Distillation Sampling优化曲线参数；第二阶段是运动场学习，将3D场景投影到前视图和侧视图，利用视频扩散模型预测位移序列，然后重建3D位移向量，最后添加时间平滑确保动画流畅。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）首个文本到3D草图动画框架；2）基于可微分贝塞尔曲线的双空间架构；3）多视角优化策略减少歧义并确保结构对齐；4）结构感知的运动生成模块；5）投影-重建策略使视频扩散模型能在3D空间中合成运动。相比之前工作，4-Doodle不仅处理静态3D草图（如SketchDream、Sketch2NeRF），还能生成动态内容；不需要手动绘制运动轨迹（如Sketch2Anim）；专注于结构抽象和草图感知（如Animate3D、3DTopia）；支持动态草图抽象和跨视图时间一致性（如CLAY）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 4-Doodle首次实现了从文本描述直接生成动态、空间一致且富有表现力的3D矢量草图动画，通过双空间知识蒸馏框架和基于贝塞尔曲线的可微表示，解决了草图动画中的结构一致性和时间连贯性挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel task: text-to-3D sketch animation, which aims to bringfreeform sketches to life in dynamic 3D space. Unlike prior works focused onphotorealistic content generation, we target sparse, stylized, andview-consistent 3D vector sketches, a lightweight and interpretable mediumwell-suited for visual communication and prototyping. However, this task isvery challenging: (i) no paired dataset exists for text and 3D (or 4D)sketches; (ii) sketches require structural abstraction that is difficult tomodel with conventional 3D representations like NeRFs or point clouds; and(iii) animating such sketches demands temporal coherence and multi-viewconsistency, which current pipelines do not address. Therefore, we propose4-Doodle, the first training-free framework for generating dynamic 3D sketchesfrom text. It leverages pretrained image and video diffusion models through adual-space distillation scheme: one space captures multi-view-consistentgeometry using differentiable B\'ezier curves, while the other encodes motiondynamics via temporally-aware priors. Unlike prior work (e.g., DreamFusion),which optimizes from a single view per step, our multi-view optimizationensures structural alignment and avoids view ambiguity, critical for sparsesketches. Furthermore, we introduce a structure-aware motion module thatseparates shape-preserving trajectories from deformation-aware changes,enabling expressive motion such as flipping, rotation, and articulatedmovement. Extensive experiments show that our method produces temporallyrealistic and structurally stable 3D sketch animations, outperforming existingbaselines in both fidelity and controllability. We hope this work serves as astep toward more intuitive and accessible 4D content creation.</description>
      <author>example@mail.com (Hao Chen, Jiaqi Wang, Yonggang Qi, Ke Li, Kaiyue Pang, Yi-Zhe Song)</author>
      <guid isPermaLink="false">2510.25319v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
  <item>
      <title>SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation</title>
      <link>http://arxiv.org/abs/2510.25268v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SynHLMA框架，用于生成关节物体的手部语言操作序列，实现了HAOI生成、预测和插值三种任务，在HAOI-lang数据集上展示了优越性能，并可用于机器人抓取应用。&lt;h4&gt;背景&lt;/h4&gt;生成手部抓取动作是具身AI和VR/AR应用中的广泛研究课题。当涉及关节物体交互时，手部抓取合成需要同时考虑物体功能性和物体变形过程中的长期操作序列。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的HAOI序列生成框架SynHLMA，用于合成关节物体的手部语言操作。&lt;h4&gt;方法&lt;/h4&gt;给定关节物体的完整点云，使用离散的HAOI表示建模每个手部物体交互帧；结合自然语言嵌入，通过HAOI操作语言模型训练这些表示，在共享表示空间中对齐抓取过程与语言描述；采用关节感知损失确保手部抓取遵循关节物体的动态变化。&lt;h4&gt;主要发现&lt;/h4&gt;SynHLMA实现了关节物体的三种典型手部操作任务：HAOI生成、HAOI预测和HAOI插值；在HAOI-lang数据集上的评估结果显示，与最先进方法相比具有优越的手部抓取序列生成性能；通过使用SynHLMA提供的操作序列，机器人可以实现灵巧抓取的模仿学习。&lt;h4&gt;结论&lt;/h4&gt;SynHLMA框架在关节物体的手部抓取序列生成方面表现优越，代码和数据集将公开可用，为具身AI和VR/AR应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;通过语言指令生成手部抓取是一个广泛研究的课题，受益于具身AI和VR/AR应用。当转化为关节物体交互时，手部抓取合成不仅需要物体功能性，还需要考虑物体变形过程中的长期操作序列。本文提出了一个新的HAOI序列生成框架SynHLMA，用于合成关节物体的手部语言操作。给定关节物体的完整点云，我们使用离散的HAOI表示来建模每个手部物体交互帧。结合自然语言嵌入，通过HAOI操作语言模型训练这些表示，在共享表示空间中对齐抓取过程与语言描述。采用关节感知损失来确保手部抓取遵循关节物体的动态变化。通过这种方式，我们的SynHLMA实现了关节物体的三种典型手部操作任务：HAOI生成、HAOI预测和HAOI插值。我们在构建的HAOI-lang数据集上评估SynHLMA，实验结果展示了与最先进方法相比的优越手部抓取序列生成性能。我们还展示了机器抓取应用，通过使用SynHLMA提供的操作序列，使模仿学习能够执行灵巧抓取。我们的代码和数据集将公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating hand grasps with language instructions is a widely studied topicthat benefits from embodied AI and VR/AR applications. While transferring intohand articulatied object interaction (HAOI), the hand grasps synthesis requiresnot only object functionality but also long-term manipulation sequence alongthe object deformation. This paper proposes a novel HAOI sequence generationframework SynHLMA, to synthesize hand language manipulation for articulatedobjects. Given a complete point cloud of an articulated object, we utilize adiscrete HAOI representation to model each hand object interaction frame. Alongwith the natural language embeddings, the representations are trained by anHAOI manipulation language model to align the grasping process with itslanguage description in a shared representation space. A joint-aware loss isemployed to ensure hand grasps follow the dynamic variations of articulatedobject joints. In this way, our SynHLMA achieves three typical handmanipulation tasks for articulated objects of HAOI generation, HAOI predictionand HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset andexperimental results demonstrate the superior hand grasp sequence generationperformance comparing with state-of-the-art. We also show a robotics graspapplication that enables dexterous grasps execution from imitation learningusing the manipulation sequence provided by our SynHLMA. Our codes and datasetswill be made publicly available.</description>
      <author>example@mail.com (Wang zhi, Yuyan Liu, Liu Liu, Li Zhang, Ruixuan Lu, Dan Guo)</author>
      <guid isPermaLink="false">2510.25268v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>U-CAN: Unsupervised Point Cloud Denoising with Consistency-Aware Noise2Noise Matching</title>
      <link>http://arxiv.org/abs/2510.25210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025. Project page:  https://gloriasze.github.io/U-CAN/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为U-CAN的无监督点云去噪框架，采用一致性感知的Noise2Noise匹配方法，通过神经网络推断多步去噪路径，并引入几何一致性约束，无需大量人工标注数据即可达到与监督方法相当的去噪效果。&lt;h4&gt;背景&lt;/h4&gt;扫描传感器捕获的点云数据通常受到噪声干扰，这对下游任务（如表面重建和形状理解）有负面影响。先前的工作大多使用含噪-清洁点云对训练神经网络来学习去噪先验，这需要大量的人工努力。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需人工标注数据的无监督点云去噪方法，以减少对大量含噪-清洁点云对的依赖。&lt;h4&gt;方法&lt;/h4&gt;U-CAN框架利用神经网络推断每个点或场景的多步去噪路径，通过噪声到噪声匹配方案实现。通过一种新的损失函数，使模型能够对多个含噪点云观测进行统计推理。引入一种去噪后几何一致性约束，以学习一致性感知的去噪模式。该约束不仅限于3D领域，还可以贡献于2D图像去噪领域。&lt;h4&gt;主要发现&lt;/h4&gt;在点云去噪、上采样和图像去噪的广泛基准测试中，U-CAN比最先进的无监督方法有显著改进，并且产生的结果与监督方法相当。&lt;h4&gt;结论&lt;/h4&gt;U-CAN是一种有效的无监督点云去噪方法，不需要大量人工标注的数据，同时能够达到与监督方法相当的性能，为点云去噪领域提供了一种新的无监督解决方案。&lt;h4&gt;翻译&lt;/h4&gt;扫描传感器捕获的点云数据常常受到噪声干扰，这对下游任务（例如表面重建和形状理解）有严重的负面影响。先前的工作主要集中在使用含噪-清洁点云对训练神经网络来学习去噪先验，这需要大量的人工努力。在本工作中，我们引入了U-CAN，一种基于一致性感知的Noise2Noise匹配的无监督点云去噪框架。具体来说，我们利用神经网络推断形状或场景中每个点的多步去噪路径，采用噪声到噪声匹配方案。我们通过一种新的损失函数实现这一点，该损失函数能够在多个含噪点云观测上进行统计推理。我们进一步引入了一种对去噪后几何一致性的新约束，以学习一致性感知的去噪模式。我们证明所提出的约束是一个通用术语，不仅限于3D领域，还可以贡献于2D图像去噪领域。在点云去噪、上采样和图像去噪的广泛基准测试中，我们的评估显示比最先进的无监督方法有显著改进，其中U-CAN也产生了与监督方法相当的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无监督点云去噪问题。点云数据（如激光雷达扫描获取的三维点数据）通常包含噪声，影响下游任务如表面重建和形状理解。现有方法需要成对的'带噪-干净'点云数据训练，需要大量人工标注，成本高。在现实中，自动驾驶汽车、手机等设备每天都在产生大量带噪点云，而干净数据获取困难。解决这个问题能减少对人工标注的依赖，使去噪技术更容易应用于实际场景，提升自动驾驶、增强现实和机器人等领域的性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到尽管干净点云有限，但带噪点云数据每天都在快速增长。借鉴了2D图像的Noise2Noise方法，但发现点云无序不规则，没有像素间的一对一对应关系，不能直接应用。现有无监督方法如TotalDenoising只使用全局约束，难以保持局部几何结构。因此，作者设计多步去噪框架，通过神经网络为每个点推断去噪路径；提出点对点噪声到噪声匹配，使用地球移动距离建立点间对应关系；引入一致性感知约束确保不同噪声观测的去噪预测保持一致。借鉴了PointNet、PointNet++等点云处理架构和TotalDenoising的全局约束思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是噪声到噪声匹配和一致性感知约束。通过学习从一个带噪点云到另一个带噪点云的映射，利用统计推理从多个带噪观测中揭示干净结构；同时确保不同噪声观测的去噪预测间保持几何一致性，解决缺乏真实表面位置信息导致的收敛不稳定问题。整体流程：输入带噪点云→多步去噪框架（每步包含特征提取和路径预测）→噪声到噪声匹配（使用地球移动距离建立点间对应）→一致性感知约束（最小化不同去噪预测间的几何差异）→输出去噪后点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) U-CAN无监督框架，利用噪声到噪声匹配和一致性感知约束；2) 点对点噪声匹配方案，使用地球移动距离建立点云对应关系；3) 去噪几何一致性约束，确保不同噪声观测的去噪预测一致；4) 证明该约束不仅限于3D领域，也可用于2D图像去噪；5) 可用于无监督点云上采样任务。不同之处：相比监督方法，无需干净数据；相比TotalDenoising等，不仅用全局约束，还引入局部约束；相比直接应用Noise2Noise，解决了点云对应关系缺失问题；相比其他无监督方法，引入一致性约束解决收敛不稳定问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; U-CAN通过创新的噪声到噪声匹配和一致性感知约束，实现了仅使用带噪点云数据就能达到与监督方法相当的去噪效果，无需人工标注的干净数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds captured by scanning sensors are often perturbed by noise, whichhave a highly negative impact on downstream tasks (e.g. surface reconstructionand shape understanding). Previous works mostly focus on training neuralnetworks with noisy-clean point cloud pairs for learning denoising priors,which requires extensively manual efforts. In this work, we introduce U-CAN, anUnsupervised framework for point cloud denoising with Consistency-AwareNoise2Noise matching. Specifically, we leverage a neural network to infer amulti-step denoising path for each point of a shape or scene with a noise tonoise matching scheme. We achieve this by a novel loss which enablesstatistical reasoning on multiple noisy point cloud observations. We furtherintroduce a novel constraint on the denoised geometry consistency for learningconsistency-aware denoising patterns. We justify that the proposed constraintis a general term which is not limited to 3D domain and can also contribute tothe area of 2D image denoising. Our evaluations under the widely usedbenchmarks in point cloud denoising, upsampling and image denoising showsignificant improvement over the state-of-the-art unsupervised methods, whereU-CAN also produces comparable results with the supervised methods.</description>
      <author>example@mail.com (Junsheng Zhou, Xingyu Shi, Haichuan Song, Yi Fang, Yu-Shen Liu, Zhizhong Han)</author>
      <guid isPermaLink="false">2510.25210v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>$D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction</title>
      <link>http://arxiv.org/abs/2510.25173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;D²GS是一种无LiDAR的城市场景重建框架，通过多视图深度预测和优化技术，实现了比使用LiDAR的方法更准确的几何重建。&lt;h4&gt;背景&lt;/h4&gt;高斯溅射(GS)在自动驾驶城市场景重建中显示出潜力，但现有方法依赖多模态传感器(如LiDAR和图像)，而LiDAR数据获取存在时空校准困难和空间不对齐问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需LiDAR数据的城市场景重建方法，避免获取准确LiDAR深度的困难，同时保持或提高重建质量。&lt;h4&gt;方法&lt;/h4&gt;1) 通过反向投影多视图度量深度预测初始化密集点云，并用渐进修剪策略优化；2) 利用深度基础模型的扩散先验增强高斯渲染的深度图，提供更强几何约束；3) 约束道路区域内高斯的形状和法线属性提高地面几何准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在Waymo数据集上的实验表明，D²GS方法始终优于最先进方法，即使与使用真实LiDAR数据的方法相比，也能产生更准确的几何。&lt;h4&gt;结论&lt;/h4&gt;D²GS框架成功实现了无LiDAR的城市场景重建，获得了比LiDAR方法更密集、更准确的几何先验，证明了深度先验和优化策略的有效性。&lt;h4&gt;翻译&lt;/h4&gt;最近，高斯溅射(GS)在自动驾驶领域的城市场景重建中显示出巨大潜力。然而，当前的城市场景重建方法通常依赖于多模态传感器作为输入，即LiDAR和图像。虽然LiDAR点云提供的几何先验可以大大减轻重建中的不适定性，但在实践中获取准确的LiDAR数据仍然具有挑战性：i)需要LiDAR和其他传感器之间精确的时空校准，因为它们可能不会同时捕获数据；ii)当LiDAR和相机安装在不同位置时，空间不对齐会导致重投影误差。为了避免获取准确LiDAR深度的困难，我们提出了D²GS，一个无LiDAR的城市场景重建框架。在这项工作中，我们获得了与LiDAR一样有效但更密集、更准确的几何先验。首先，我们通过反向投影多视图度量深度预测来初始化密集点云。然后通过渐进修剪策略优化该点云以提高全局一致性。其次，我们通过深度增强器联合优化高斯几何和预测的密集度量深度。具体来说，我们利用来自深度基础模型的扩散先验来增强由高斯渲染的深度图。反过来，增强的深度在高斯训练期间提供更强的几何约束。最后，我们通过约束道路区域内高斯的形状和法线属性来提高地面几何的准确性。在Waymo数据集上的大量实验表明，我们的方法始终优于最先进的方法，即使与使用真实LiDAR数据的方法相比，也能产生更准确的几何。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶领域城市场景重建中对LiDAR（激光雷达）数据的依赖问题。这个问题很重要，因为获取准确的LiDAR数据在实际应用中面临诸多挑战：需要专业设备和车辆进行数据收集、传感器间需要精确的时空校准、LiDAR与相机安装在不同位置会导致重投影误差，此外LiDAR数据成本高昂且难以扩展，限制了大规模应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法对LiDAR依赖的问题，探索了替代方案。他们借鉴了多视图深度估计网络、3D Gaussian Splatting框架、扩散先验模型（如Marigold）、场景图表示方法以及道路几何先验知识。在此基础上，设计了三个关键组件：利用渐进式剪枝策略管理密集点云、通过深度增强模块迭代优化深度和高斯表示、在场景图中引入专门的道路节点约束。这些设计既吸收了现有工作的优点，又针对LiDAR-free场景进行了创新改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过有效利用从图像中推导的几何先验，消除对LiDAR数据的依赖，创建一个仅使用相机输入的动态城市街道场景重建框架。整体流程分为：1)初始化阶段：使用多视图深度估计预测深度图，反投影得到点云，通过渐进式剪枝获得代表性点集；2)优化阶段：创建道路节点约束，实施联合优化策略，使用深度增强模块利用扩散先验细化深度；3)训练阶段：迭代更新高斯参数和深度估计，利用置信度图指导深度增强；4)评估阶段：在Waymo数据集上评估性能，与使用LiDAR的方法进行比较。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)LiDAR-free框架，消除对LiDAR数据的需求和校准误差；2)渐进式剪枝策略，有效管理密集点云；3)基于扩散的深度增强联合优化策略，提供密集度量深度监督；4)道路节点约束，利用地面平面先验提高道路重建精度。相比之前工作，不同之处在于：不需要LiDAR数据避免校准问题；不依赖单目深度估计避免尺度模糊；能处理动态场景而多视图深度估计不能；将生成深度先验直接集成到3DGS优化循环中；提供比LiDAR更密集、更准确的几何先验。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; D²GS提出了一种无需LiDAR的城市场景重建框架，通过渐进式剪枝、深度增强和道路节点约束，仅使用相机输入就能实现比使用LiDAR数据更准确的动态城市街道场景重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Gaussian Splatting (GS) has shown great potential for urban scenereconstruction in the field of autonomous driving. However, current urban scenereconstruction methods often depend on multimodal sensors as inputs,\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDARpoint clouds can largely mitigate ill-posedness in reconstruction, acquiringsuch accurate LiDAR data is still challenging in practice: i) precisespatiotemporal calibration between LiDAR and other sensors is required, as theymay not capture data simultaneously; ii) reprojection errors arise from spatialmisalignment when LiDAR and cameras are mounted at different locations. Toavoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, aLiDAR-free urban scene reconstruction framework. In this work, we obtaingeometry priors that are as effective as LiDAR while being denser and moreaccurate. $\textbf{First}$, we initialize a dense point cloud byback-projecting multi-view metric depth predictions. This point cloud is thenoptimized by a Progressive Pruning strategy to improve the global consistency.$\textbf{Second}$, we jointly refine Gaussian geometry and predicted densemetric depth via a Depth Enhancer. Specifically, we leverage diffusion priorsfrom a depth foundation model to enhance the depth maps rendered by Gaussians.In turn, the enhanced depths provide stronger geometric constraints duringGaussian training. $\textbf{Finally}$, we improve the accuracy of groundgeometry by constraining the shape and normal attributes of Gaussians withinroad regions. Extensive experiments on the Waymo dataset demonstrate that ourmethod consistently outperforms state-of-the-art methods, producing moreaccurate geometry even when compared with those using ground-truth LiDAR data.</description>
      <author>example@mail.com (Kejing Xia, Jidong Jia, Ke Jin, Yucai Bai, Li Sun, Dacheng Tao, Youjian Zhang)</author>
      <guid isPermaLink="false">2510.25173v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Point-level Uncertainty Evaluation of Mobile Laser Scanning Point Clouds</title>
      <link>http://arxiv.org/abs/2510.24773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于机器学习的框架，用于评估移动激光扫描点云的点级别不确定性，无需依赖高精度参考数据。&lt;h4&gt;背景&lt;/h4&gt;移动激光扫描点云中的不确定性可靠量化对3D制图、建模和变化分析等下游应用的准确性和可信度至关重要，而传统方法高度依赖难以获取的高精度参考数据。&lt;h4&gt;目的&lt;/h4&gt;解决传统不确定性建模方法依赖高精度参考数据的问题，开发一种不依赖此类数据的点级别不确定性评估方法。&lt;h4&gt;方法&lt;/h4&gt;提出基于机器学习的框架，学习局部几何特征与点级别误差之间的关系，使用随机森林和XGBoost两种集成学习模型，并在空间分区化的真实世界数据集上训练验证以避免数据泄露。&lt;h4&gt;主要发现&lt;/h4&gt;两种模型能有效捕捉几何特征与不确定性间的非线性关系，平均ROC-AUC值超过0.87；描述高程变化、点密度和局部结构复杂性的几何特征在预测不确定性中起主导作用。&lt;h4&gt;结论&lt;/h4&gt;该框架为不确定性评估提供了数据驱动的方法，为大规模点云的质量控制和误差分析提供了可扩展且适应性强的基础。&lt;h4&gt;翻译&lt;/h4&gt;移动激光扫描点云中不确定性的可靠量化对于确保3D制图、建模和变化分析等下游应用的准确性和可信度至关重要。传统的不确定性建模方法高度依赖于高精度参考数据，而这些数据在大规模情况下通常成本高昂或难以获取。为解决这一问题，本研究提出了一种基于机器学习的点级别不确定性评估框架，学习局部几何特征与点级别误差之间的关系。该框架使用随机森林和XGBoost两种集成学习模型实现，在空间分区化的真实世界数据集上进行训练和验证以避免数据泄露。实验结果表明，两种模型都能有效捕捉几何特征与不确定性之间的非线性关系，平均ROC-AUC值超过0.87。分析进一步表明，描述高程变化、点密度和局部结构复杂性的几何特征在预测不确定性中起主导作用。所提出的框架为不确定性评估提供了数据驱动的方法，为未来大规模点云的质量控制和误差分析提供了可扩展且适应性强的基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决移动激光扫描点云的点级不确定性评估问题，特别是减少对高精度参考数据的依赖。这个问题很重要，因为可靠的不确定性量化对3D建模、变化分析等下游应用的准确性和可信度至关重要，而不充分评估点云质量会影响高精度应用如导航和变化分析，不仅降低可靠性，还会浪费时间和资源。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了MLS系统中的不确定性来源，评估了现有方法（前向建模和后向建模）的局限性，特别是后向建模对参考数据的依赖和高成本问题。然后提出用机器学习替代方案，建立点云特征与不确定性关系。该方法借鉴了现有工作，如使用C2C距离作为不确定性度量、基于KNN的邻域定义策略，以及采用随机森林和XGBoost等集成学习方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过机器学习学习局部几何特征与点级误差之间的关系，将不确定性评估转化为二分类问题。整体流程包括：1)使用C2C距离定义不确定性度量；2)基于KNN提取每个点的局部几何特征；3)采用随机森林和XGBoost模型进行二分类训练；4)使用多种指标和空间分区5折交叉验证评估模型性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出基于机器学习的框架减少对参考数据依赖；将不确定性评估转化为二分类问题；使用局部几何特征预测不确定性；采用互补的集成学习方法验证；通过特征重要性分析提供误差源新见解。相比传统方法，本研究采用数据驱动方式，训练后不再需要参考数据，提供了更可扩展和适应性强的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本研究提出了一种基于机器学习的框架，能够通过学习点云的局部几何特征与点级误差之间的关系，实现对移动激光扫描点云的点级不确定性预测，减少了对高精度参考数据的依赖，为大规模点云质量评估提供了新的数据驱动方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable quantification of uncertainty in Mobile Laser Scanning (MLS) pointclouds is essential for ensuring the accuracy and credibility of downstreamapplications such as 3D mapping, modeling, and change analysis. Traditionalbackward uncertainty modeling heavily rely on high-precision reference data,which are often costly or infeasible to obtain at large scales. To address thisissue, this study proposes a machine learning-based framework for point-leveluncertainty evaluation that learns the relationship between local geometricfeatures and point-level errors. The framework is implemented using twoensemble learning models, Random Forest (RF) and XGBoost, which are trained andvalidated on a spatially partitioned real-world dataset to avoid data leakage.Experimental results demonstrate that both models can effectively capture thenonlinear relationships between geometric characteristics and uncertainty,achieving mean ROC-AUC values above 0.87. The analysis further reveals thatgeometric features describing elevation variation, point density, and localstructural complexity play a dominant role in predicting uncertainty. Theproposed framework offers a data-driven perspective of uncertainty evaluation,providing a scalable and adaptable foundation for future quality control anderror analysis of large-scale point clouds.</description>
      <author>example@mail.com (Ziyang Xu, Olaf Wysocki, Christoph Holst)</author>
      <guid isPermaLink="false">2510.24773v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Controlling Contrastive Self-Supervised Learning with Knowledge-Driven Multiple Hypothesis: Application to Beat Tracking</title>
      <link>http://arxiv.org/abs/2510.25560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种对比自监督预训练方法，利用多种可能的正样本假设来解决数据模糊性问题，在音乐节拍跟踪任务上取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;数据中的模糊性和问题约束的多样性会导致机器学习任务产生多种同样合理的不同结果。例如在节拍和强拍跟踪中，不同听众可能采用各种节奏解释，这些解释都不一定是错误的。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理数据模糊性的方法，通过利用多种可能的正样本假设来提高机器学习模型的性能，特别是在音乐表示学习领域。&lt;h4&gt;方法&lt;/h4&gt;提出一种对比自监督预训练方法，模型被训练为学习与不同假设兼容的表示，这些假设通过基于知识的评分函数选择，以保留最合理的假设。&lt;h4&gt;主要发现&lt;/h4&gt;在有标签数据上进行微调时，该方法在标准基准测试上优于现有方法，证明了将领域知识与多假设选择相结合的有效性。&lt;h4&gt;结论&lt;/h4&gt;将领域知识与多假设选择相结合在音乐表示学习中具有显著优势，能够有效处理数据中的模糊性问题并提高模型性能。&lt;h4&gt;翻译&lt;/h4&gt;数据中的模糊性和问题约束可能导致机器学习任务产生多种同样合理的不同结果。例如在节拍和强拍跟踪中，不同听众可能采用各种节奏解释，这些解释都不一定是错误的。为此，我们提出了一种对比自监督预训练方法，利用数据中可能的正样本的多种假设。我们的模型被训练为学习与不同假设兼容的表示，这些假设通过基于知识的评分函数选择，以保留最合理的假设。在有标签数据上进行微调时，我们的模型在标准基准测试上优于现有方法，展示了将领域知识与多假设选择相结合在音乐表示学习中的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ambiguities in data and problem constraints can lead to diverse, equallyplausible outcomes for a machine learning task. In beat and downbeat tracking,for instance, different listeners may adopt various rhythmic interpretations,none of which would necessarily be incorrect. To address this, we propose acontrastive self-supervised pre-training approach that leverages multiplehypotheses about possible positive samples in the data. Our model is trained tolearn representations compatible with different such hypotheses, which areselected with a knowledge-based scoring function to retain the most plausibleones. When fine-tuned on labeled data, our model outperforms existing methodson standard benchmarks, showcasing the advantages of integrating domainknowledge with multi-hypothesis selection in music representation learning inparticular.</description>
      <author>example@mail.com (Antonin Gagnere, Slim Essid, Geoffroy Peeters)</author>
      <guid isPermaLink="false">2510.25560v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning</title>
      <link>http://arxiv.org/abs/2510.25262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于信息瓶颈原理的新归一化方法IBNorm，通过有界压缩操作鼓励嵌入保留预测信息同时抑制无用变异性，在大规模语言模型和视觉模型上均优于传统归一化方法。&lt;h4&gt;背景&lt;/h4&gt;归一化是深度学习的基础，但现有方法如BatchNorm、LayerNorm和RMSNorm都是方差中心的，通过强制零均值和单位方差稳定训练，但没有控制表示如何捕获任务相关信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的归一化方法，能够鼓励表示保留预测信息同时抑制无用变异性，从而产生更具信息量的表示。&lt;h4&gt;方法&lt;/h4&gt;提出IB-Inspired Normalization (IBNorm)，一种基于信息瓶颈原理的简单而强大的方法系列，引入有界压缩操作来优化信息表示。&lt;h4&gt;主要发现&lt;/h4&gt;理论上证明IBNorm比方差中心方法获得更高的IB值和更紧的泛化边界；实验上在大型语言模型和视觉模型上一致优于传统归一化方法，互信息分析证实了其优越的信息瓶颈行为。&lt;h4&gt;结论&lt;/h4&gt;IBNorm能够产生更具信息量的表示，同时保持标准归一化的稳定性和兼容性，是一种优于传统归一化方法的新方法。&lt;h4&gt;翻译&lt;/h4&gt;归一化是深度学习的基础，但现有的方法如BatchNorm、LayerNorm和RMSNorm都是方差中心的，通过强制零均值和单位方差来稳定训练，而没有控制表示如何捕获任务相关信息。我们提出了受信息瓶颈原理启发的归一化方法（IBNorm），这是一种简单而强大的方法系列。IBNorm引入了有界压缩操作，鼓励嵌入保留预测信息同时抑制无用变异性，从而产生更具信息量的表示，同时保持标准归一化的稳定性和兼容性。理论上，我们证明IBNorm比方差中心方法获得更高的IB值和更紧的泛化边界。实验上，IBNorm在大型语言模型（LLaMA、GPT-2）和视觉模型（ResNet、ViT）上一致优于BatchNorm、LayerNorm和RMSNorm，互信息分析证实了其优越的信息瓶颈行为。代码将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Normalization is fundamental to deep learning, but existing approaches suchas BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zeromean and unit variance, stabilizing training without controlling howrepresentations capture task-relevant information. We propose IB-InspiredNormalization (IBNorm), a simple yet powerful family of methods grounded in theInformation Bottleneck principle. IBNorm introduces bounded compressionoperations that encourage embeddings to preserve predictive information whilesuppressing nuisance variability, yielding more informative representationswhile retaining the stability and compatibility of standard normalization.Theoretically, we prove that IBNorm achieves a higher IB value and tightergeneralization bounds than variance-centric methods. Empirically, IBNormconsistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scalelanguage models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutualinformation analysis confirming superior information bottleneck behavior. Codewill be released publicly.</description>
      <author>example@mail.com (Xiandong Zou, Pan Zhou)</author>
      <guid isPermaLink="false">2510.25262v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Improving time series estimation and prediction via transfer learning</title>
      <link>http://arxiv.org/abs/2510.25236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于表示的迁移学习框架，用于解决高维度但样本量有限的时间序列数据集的估计和预测问题，通过利用相关源数据集的丰富观测信息提高估计效率。&lt;h4&gt;背景&lt;/h4&gt;许多时间序列数据集（如宏观经济变量）具有高维度但样本量有限，仅使用这些数据集本身几乎无法获得有效的估计和准确的预测。&lt;h4&gt;目的&lt;/h4&gt;引入一种基于表示的迁移学习框架用于向量自回归模型，利用相关源数据集的丰富观测信息，通过表示学习提高估计效率。&lt;h4&gt;方法&lt;/h4&gt;提出一种具有良好非渐近性质的两阶段正则化估计程序，并建议使用交替更新算法来寻找估计值。该框架能够处理具有不同样本量和异步开始/结束时间点的时间序列，灵活整合来自不同数据集的信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟实验评估了所提出方法的有限样本性能，并通过对日本和其他20个宏观经济变量的实证分析证明了该方法的有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;该迁移学习框架解决了高维度但样本量有限的时间序列分析问题，通过利用相关源数据集的信息提高了估计效率和预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;现有文献中存在许多高维度但样本量有限的时间序列，如宏观经济变量，仅使用相应的数据集本身几乎不可能获得有效的估计和准确的预测。本文通过引入一种基于表示的迁移学习框架来填补这一空白，该框架用于向量自回归模型，可以通过表示学习利用来自相关源数据集的丰富观测信息来提高估计效率。提出了一种具有良好建立的非渐近性质的两阶段正则化估计程序，并建议使用交替更新算法来寻找估计值。我们的迁移学习框架可以处理具有不同样本量和异步开始/结束时间点的时间序列，从而在整合来自不同数据集的信息方面提供了显著的灵活性。进行了模拟实验来评估所提出方法的有限样本性能，并通过分析日本和其他20个宏观经济变量的实证分析证明了其有用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There are many time series in the literature with high dimension yet limitedsample sizes, such as macroeconomic variables, and it is almost impossible toobtain efficient estimation and accurate prediction by using the correspondingdatasets themselves. This paper fills the gap by introducing a novelrepresentation-based transfer learning framework for vector autoregressivemodels, and information from related source datasets with rich observations canbe leveraged to enhance estimation efficiency through representation learning.A two-stage regularized estimation procedure is proposed with well establishednon-asymptotic properties, and algorithms with alternating updates aresuggested to search for the estimates. Our transfer learning framework canhandle time series with varying sample sizes and asynchronous starting and/orending time points, thereby offering remarkable flexibility in integratinginformation from diverse datasets. Simulation experiments are conducted toevaluate the finite-sample performance of the proposed methodology, and itsusefulness is demonstrated by an empirical analysis on 20 macroeconomicvariables from Japan and another nine countries.</description>
      <author>example@mail.com (Yuchang Lin, Qianqian Zhu, Guodong Li)</author>
      <guid isPermaLink="false">2510.25236v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Learning Fair Graph Representations with Multi-view Information Bottleneck</title>
      <link>http://arxiv.org/abs/2510.25096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FairMIB是一种多视图信息瓶颈框架，通过分解图为特征、结构和扩散视图，结合对比学习和逆概率加权邻域校正，有效减轻图神经网络中的偏见传播，在保持高任务效用的同时提高公平性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在处理关系数据时表现优秀，但会放大训练数据中的偏见，将歧视性属性和结构不平衡传播到不公平的结果中。现有公平性方法将偏见视为单一来源，忽略了不同的属性和结构效应，导致公平性和实用性之间的次优权衡。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时考虑属性和结构效应的框架，以减轻图神经网络中的偏见传播，实现更好的公平性和实用性权衡。&lt;h4&gt;方法&lt;/h4&gt;FairMIB是一种多视图信息瓶颈框架，将图分解为特征、结构和扩散三个视图。它使用对比学习最大化跨视图互信息实现无偏见表示学习，整合多视角条件信息瓶颈目标平衡任务效用和公平性，并在扩散视图中引入逆概率加权邻域校正减少偏见传播。&lt;h4&gt;主要发现&lt;/h4&gt;FairMIB能够有效分解并处理不同类型的偏见，通过多视图方法实现了比现有方法更好的公平性和实用性权衡。实验表明它在五个真实世界基准数据集上达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;FairMIB通过多视图信息瓶颈框架和创新的偏见缓解技术，成功解决了图神经网络中的偏见问题，在不牺牲任务效用的前提下显著提高了公平性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络通过在节点特征和结构上传递消息，在关系数据上表现出色，但它们会放大训练数据中的偏见，将歧视性属性和结构不平衡传播到不公平的结果中。许多公平性方法将偏见视为单一来源，忽略了不同的属性和结构效应，导致公平性和实用性之间的次优权衡。为了克服这一挑战，我们提出了FairMIB，一种多视图信息瓶颈框架，旨在将图分解为特征、结构和扩散视图，以减轻图神经网络中的复杂度偏见。特别是，所提出的FairMIB采用对比学习来最大化跨视图互信息，实现无偏见的表示学习。它进一步整合多视角条件信息瓶颈目标，通过最小化与敏感属性的互信息来平衡任务效用和公平性。此外，FairMIB在扩散视图中引入了逆概率加权邻域校正，减少了消息传递过程中偏见的传播。在五个真实世界基准数据集上的实验表明，FairMIB在效用和公平性指标上都达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) excel on relational data by passing messagesover node features and structure, but they can amplify training data biases,propagating discriminatory attributes and structural imbalances into unfairoutcomes. Many fairness methods treat bias as a single source, ignoringdistinct attribute and structure effects and leading to suboptimal fairness andutility trade-offs. To overcome this challenge, we propose FairMIB, amulti-view information bottleneck framework designed to decompose graphs intofeature, structural, and diffusion views for mitigating complexity biases inGNNs. Especially, the proposed FairMIB employs contrastive learning to maximizecross-view mutual information for bias-free representation learning. It furtherintegrates multi-perspective conditional information bottleneck objectives tobalance task utility and fairness by minimizing mutual information withsensitive attributes. Additionally, FairMIB introduces an inverseprobability-weighted (IPW) adjacency correction in the diffusion view, whichreduces the spread of bias propagation during message passing. Experiments onfive real-world benchmark datasets demonstrate that FairMIB achievesstate-of-the-art performance across both utility and fairness metrics.</description>
      <author>example@mail.com (Chuxun Liu, Debo Cheng, Qingfeng Chen, Jiangzhang Gan, Jiuyong Li, Lin Liu)</author>
      <guid isPermaLink="false">2510.25096v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Topic Analysis with Side Information: A Neural-Augmented LDA Approach</title>
      <link>http://arxiv.org/abs/2510.24918v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;nnLDA是一种创新的神经增强概率主题模型，通过神经先验机制动态整合辅助信息，解决了传统主题模型难以融入元数据、用户属性或文档标签等辅助信息的局限性，在多个基准数据集上表现出色。&lt;h4&gt;背景&lt;/h4&gt;传统主题模型如LDA被广泛用于揭示文本语料库中的潜在结构，但这些模型往往难以整合辅助信息如元数据、用户属性或文档标签，限制了它们的表现力、个性化和可解释性。&lt;h4&gt;目的&lt;/h4&gt;提出nnLDA，一种神经增强的概率主题模型，通过神经先验机制动态整合辅助信息，以克服传统主题模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;nnLDA将每个文档建模为潜在主题的混合，其中主题比例的先验由基于辅助特征的神经网络生成。这种设计使模型能够捕获辅助信息和主题分布之间复杂的非线性交互，并开发了随机变分期望最大化算法来联合优化神经和概率组件。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上，nnLDA在主题一致性、困惑度和下游分类方面持续优于LDA和Dirichlet-Multinomial Regression。&lt;h4&gt;结论&lt;/h4&gt;当辅助信息可用时，结合神经表示学习和概率主题建模能够带来显著优势，nnLDA证明了这种混合方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;传统的主题模型如潜在狄利克雷分配（LDA）已被广泛用于揭示文本语料库中的潜在结构，但它们往往难以整合辅助信息，如元数据、用户属性或文档标签。这些局限性限制了它们的表现力、个性化和可解释性。为此，我们提出了nnLDA，一种神经增强的概率主题模型，通过神经先验机制动态整合辅助信息。nnLDA将每个文档建模为潜在主题的混合，其中主题比例的先验由基于辅助特征的神经网络生成。这种设计使模型能够捕获辅助信息和主题分布之间复杂的非线性交互，这是静态狄利克雷先验无法表示的。我们开发了一种随机变分期望最大化算法来联合优化神经和概率组件。在多个基准数据集上，nnLDA在主题一致性、困惑度和下游分类方面持续优于LDA和狄利克雷-多项式回归。这些结果强调了在辅助信息可用的情况下，结合神经表示学习和概率主题建模的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional topic models such as Latent Dirichlet Allocation (LDA) have beenwidely used to uncover latent structures in text corpora, but they oftenstruggle to integrate auxiliary information such as metadata, user attributes,or document labels. These limitations restrict their expressiveness,personalization, and interpretability. To address this, we propose nnLDA, aneural-augmented probabilistic topic model that dynamically incorporates sideinformation through a neural prior mechanism. nnLDA models each document as amixture of latent topics, where the prior over topic proportions is generatedby a neural network conditioned on auxiliary features. This design allows themodel to capture complex nonlinear interactions between side information andtopic distributions that static Dirichlet priors cannot represent. We develop astochastic variational Expectation-Maximization algorithm to jointly optimizethe neural and probabilistic components. Across multiple benchmark datasets,nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression intopic coherence, perplexity, and downstream classification. These resultshighlight the benefits of combining neural representation learning withprobabilistic topic modeling in settings where side information is available.</description>
      <author>example@mail.com (Biyi Fang, Kripa Rajshekhar, Truong Vo, Diego Klabjan)</author>
      <guid isPermaLink="false">2510.24918v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Transformers from Compressed Representations</title>
      <link>http://arxiv.org/abs/2510.23665v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TEMPEST是一种利用压缩文件字节流结构进行表示学习的方法，通过紧凑编码实现高效语义表示，同时保持与最先进方法相当的准确性。&lt;h4&gt;背景&lt;/h4&gt;压缩文件格式是高效数据存储和传输的基石，但其在表示学习方面的潜力尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;引入一种能够利用压缩文件固有字节流结构进行有效标记化和编码策略的方法，直接从压缩数据流中学习语义表示。&lt;h4&gt;方法&lt;/h4&gt;TEMPEST（TransformErs froM comPressed rEpreSenTations）利用压缩文件的固有字节流结构设计标记化和编码策略，使标准transformer可以直接从压缩数据流中学习语义表示，绕过原始字节级处理或完整媒体解码的需要。&lt;h4&gt;主要发现&lt;/h4&gt;TEMPEST显著减少了语义分类所需的标记数量，降低了计算复杂性和内存使用；在多个数据集、编码方案和模态的实验中，实现了与最先进方法相当的准确性，同时在内存和计算方面提高了效率。&lt;h4&gt;结论&lt;/h4&gt;TEMPEST是一种有效的方法，可以从压缩数据中学习语义表示，在保持准确性的同时提高了效率。&lt;h4&gt;翻译&lt;/h4&gt;压缩文件格式是高效数据存储和传输的基石，但其在表示学习方面的潜力仍未被充分探索。我们引入了TEMPEST（一种基于压缩表示的transformer方法），它利用压缩文件的固有字节流结构来设计有效的标记化和编码策略。通过利用这种紧凑编码，标准的transformer可以直接从压缩数据流中学习语义表示，绕过了原始字节级处理或完整媒体解码的需要。我们的提议显著减少了语义分类所需的标记数量，从而降低了计算复杂性和内存使用。通过在不同数据集、编码方案和模态上的广泛实验，我们表明TEMPEST实现了与最先进方法相当的准确性，同时在内存和计算方面带来了效率提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compressed file formats are the corner stone of efficient data storage andtransmission, yet their potential for representation learning remains largelyunderexplored. We introduce TEMPEST (TransformErs froM comPressedrEpreSenTations), a method that exploits the inherent byte-stream structure ofcompressed files to design an effective tokenization and encoding strategy. Byleveraging this compact encoding, a standard transformer can directly learnsemantic representations from compressed data streams, bypassing the need forraw byte-level processing or full media decoding. Our proposal substantiallyreduces the number of tokens required for semantic classification, therebylowering both computational complexity and memory usage. Through extensiveexperiments across diverse datasets, coding schemes, and modalities, we showthat TEMPEST achieves accuracy competitive wit the state-of-the-art whiledelivering efficiency gains in memory and compute.</description>
      <author>example@mail.com (Juan C. Leon Alcazar, Mattia Soldan, Mohammad Saatialsoruji, Alejandro Pardo, Hani Itani, Juan Camilo Perez, Bernard Ghanem)</author>
      <guid isPermaLink="false">2510.23665v2</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis</title>
      <link>http://arxiv.org/abs/2510.24777v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 8 figures, and 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种多模态交叉增强融合框架，通过整合眼动追踪和面部特征进行阿尔茨海默病诊断，并在包含25名AD患者和25名健康对照者的数据集上实现了95.11%的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;准确诊断阿尔茨海默病对及时干预和减缓疾病进展至关重要。多模态诊断方法通过整合行为和感知领域的互补信息显示出巨大潜力，而眼动追踪和面部特征是认知功能的重要指标，但很少有研究探索它们的联合集成用于辅助AD诊断。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够协同利用眼动追踪和面部特征进行AD检测的多模态交叉增强融合框架，提高诊断性能的鲁棒性和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含两个关键模块的多模态框架：(a)交叉增强融合注意力模块(CEFAM)，通过交叉注意力和全局增强建模模态间交互；(b)方向感知卷积模块(DACM)，通过水平-垂直感受野捕获细粒度方向性面部特征。同时构建了一个同步多模态数据集，包括AD患者和健康对照者在视觉记忆搜索范式中的面部视频和眼动追踪序列。&lt;h4&gt;主要发现&lt;/h4&gt;在构建的数据集上，该框架优于传统的后期融合和特征连接方法，在区分AD和健康对照者方面实现了95.11%的分类准确率，通过明确建模模态间依赖性和模态特定贡献，显示出卓越的鲁棒性和诊断性能。&lt;h4&gt;结论&lt;/h4&gt;多模态交叉增强融合框架通过协同整合眼动追踪和面部特征，能够有效提高阿尔茨海默病的诊断准确性和鲁棒性，为AD的辅助诊断提供了新的方法。&lt;h4&gt;翻译&lt;/h4&gt;阿尔茨海默病(AD)的准确诊断对于实现及时干预和减缓疾病进展至关重要。多模态诊断方法通过整合行为和感知领域的互补信息显示出巨大潜力。特别是，眼动追踪和面部特征是认知功能的重要指标，反映了注意力分布和神经认知状态。然而，很少有研究探索它们的联合集成用于辅助AD诊断。在本研究中，我们提出了一种多模态交叉增强融合框架，通过协同利用眼动追踪和面部特征进行AD检测。该框架包含两个关键模块：(a)交叉增强融合注意力模块(CEFAM)，通过交叉注意力和全局增强建模模态间交互；(b)方向感知卷积模块(DACM)，通过水平-垂直感受野捕获细粒度方向性面部特征。这些模块共同实现了自适应和判别性多模态表示学习。为支持这项工作，我们构建了一个同步多模态数据集，包括25名AD患者和25名健康对照者(HC)，通过在视觉记忆搜索范式期间记录对齐的面部视频和眼动追踪序列，为评估集成策略提供了生态有效资源。在该数据集上的大量实验表明，我们的框架优于传统的后期融合和特征连接方法，在区分AD和HC方面实现了95.11%的分类准确率，通过明确建模模态间依赖性和模态特定贡献，突显了卓越的鲁棒性和诊断性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate diagnosis of Alzheimer's disease (AD) is essential for enablingtimely intervention and slowing disease progression. Multimodal diagnosticapproaches offer considerable promise by integrating complementary informationacross behavioral and perceptual domains. Eye-tracking and facial features, inparticular, are important indicators of cognitive function, reflectingattentional distribution and neurocognitive state. However, few studies haveexplored their joint integration for auxiliary AD diagnosis. In this study, wepropose a multimodal cross-enhanced fusion framework that synergisticallyleverages eye-tracking and facial features for AD detection. The frameworkincorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module(CEFAM), which models inter-modal interactions through cross-attention andglobal enhancement, and (b) a Direction-Aware Convolution Module (DACM), whichcaptures fine-grained directional facial features via horizontal-verticalreceptive fields. Together, these modules enable adaptive and discriminativemultimodal representation learning. To support this work, we constructed asynchronized multimodal dataset, including 25 patients with AD and 25 healthycontrols (HC), by recording aligned facial video and eye-tracking sequencesduring a visual memory-search paradigm, providing an ecologically validresource for evaluating integration strategies. Extensive experiments on thisdataset demonstrate that our framework outperforms traditional late fusion andfeature concatenation methods, achieving a classification accuracy of 95.11% indistinguishing AD from HC, highlighting superior robustness and diagnosticperformance by explicitly modeling inter-modal dependencies andmodality-specific contributions.</description>
      <author>example@mail.com (Yujie Nie, Jianzhang Ni, Yonglong Ye, Yuan-Ting Zhang, Yun Kwok Wing, Xiangqing Xu, Xin Ma, Lizhou Fan)</author>
      <guid isPermaLink="false">2510.24777v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning</title>
      <link>http://arxiv.org/abs/2510.24927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, submitted to the 12th International Conference on Soft  Computing and Machine Intelligence (ISCMI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了WBT-BGRL框架，一种用于二分图链接预测的加权非对比学习方法，通过三元损失中的加权机制增强自举学习，在真实数据集上展示了有竞争力的性能。&lt;h4&gt;背景&lt;/h4&gt;二分图链接预测对推荐系统和故障检测等应用至关重要，但研究较少；对比方法在负采样上效率低且偏差大，非对比方法仅依赖正样本；现有模型在直推式设置中表现良好，但在归纳式、加权和二分场景中效果未验证。&lt;h4&gt;目的&lt;/h4&gt;解决现有二分图链接预测方法的局限性，特别是在归纳、加权和二分场景中的有效性问题。&lt;h4&gt;方法&lt;/h4&gt;提出加权二分图三元自举图潜在表示(WBT-BGRL)，采用非对比框架，通过三元损失中的新加权机制增强自举学习；使用具有双GCN编码器的二分架构；与适配的最先进模型(T-BGRL, BGRL, GBT, CCA-SSG)进行比较评估。&lt;h4&gt;主要发现&lt;/h4&gt;在工业和电子商务真实数据集上，WBT-BGRL展现出有竞争力的性能，特别是在预训练过程中应用加权时效果更佳，突显了加权的非对比学习在二分图归纳链接预测中的价值。&lt;h4&gt;结论&lt;/h4&gt;加权的非对比学习对于二分图中的归纳链接预测具有重要价值，WBT-BGRL框架为此提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;二分图中的链接预测对于推荐系统和故障检测等应用至关重要，但相比单分图的研究较少。对比方法在负采样方面效率低下且存在偏差，而非对比方法仅依赖正样本。现有模型在直推式设置中表现良好，但在归纳式、加权和二分场景中的有效性尚未得到验证。为解决这一问题，我们提出了加权二分图三元自举图潜在表示(WBT-BGRL)，这是一种非对比框架，通过三元损失中的新加权机制增强自举学习。使用具有双GCN编码器的二分架构，将WBT-BGRL与适配的最先进模型(T-BGRL, BGRL, GBT, CCA-SSG)进行比较评估。在工业和电子商务真实世界数据集上的结果显示了具有竞争力的性能，特别是在预训练过程中应用加权时，突显了加权的非对比学习在二分图归纳链接预测中的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction in bipartite graphs is crucial for applications likerecommendation systems and failure detection, yet it is less studied than inmonopartite graphs. Contrastive methods struggle with inefficient and biasednegative sampling, while non-contrastive approaches rely solely on positivesamples. Existing models perform well in transductive settings, but theireffectiveness in inductive, weighted, and bipartite scenarios remains untested.To address this, we propose Weighted Bipartite Triplet-Bootstrapped GraphLatents (WBT-BGRL), a non-contrastive framework that enhances bootstrappedlearning with a novel weighting mechanism in the triplet loss. Using abipartite architecture with dual GCN encoders, WBT-BGRL is evaluated againstadapted state-of-the-art models (T-BGRL, BGRL, GBT, CCA-SSG). Results onreal-world datasets (Industry and E-commerce) show competitive performance,especially when weighting is applied during pretraining-highlighting the valueof weighted, non-contrastive learning for inductive link prediction inbipartite graphs.</description>
      <author>example@mail.com (Joel Frank Huarayo Quispe, Lilian Berton, Didier Vega-Oliveros)</author>
      <guid isPermaLink="false">2510.24927v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Improving Temporal Consistency and Fidelity at Inference-time in Perceptual Video Restoration by Zero-shot Image-based Diffusion Models</title>
      <link>http://arxiv.org/abs/2510.25420v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出两种无需训练的推理时策略，以改善基于零样本图像扩散模型的时间一致性视频修复，通过感知直线引导(PSG)和多路径集成采样(MPES)技术，实现时间稳定的高保真感知视频修复。&lt;h4&gt;背景&lt;/h4&gt;扩散模型已成为单图像修复的强大先验，但由于采样的随机性和整合显式时间建模的复杂性，将其应用于零样本视频修复时存在时间一致性问题。&lt;h4&gt;目的&lt;/h4&gt;在不重新训练或修改预训练扩散模型架构的情况下，提高视频修复中的时间一致性，实现时间稳定的高保真感知视频修复。&lt;h4&gt;方法&lt;/h4&gt;提出两种互补的推理时策略：(1)感知直线引导(PSG)：基于神经科学启发的感知直线假设，通过在感知空间中引入曲率惩罚，引导扩散去噪过程向更平滑的时间演化发展；(2)多路径集成采样(MPES)：通过集成多个扩散轨迹来减少随机变化，提高保真度分数而不牺牲清晰度。&lt;h4&gt;主要发现&lt;/h4&gt;PSG增强了时间自然性，特别是在时间模糊的情况下；MPES在所有任务中一致提高了保真度和时空感知-失真权衡。这两种策略无需重新训练或修改模型架构即可实现显著改进。&lt;h4&gt;结论&lt;/h4&gt;这些无需训练的技术为使用大型预训练扩散模型实现时间稳定的高保真感知视频修复提供了实用路径，通过结合PSG和MPES可以同时改善时间自然性和保真度。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型已成为单图像修复的强大先验，但将其应用于零样本视频修复时，由于采样的随机性和整合显式时间建模的复杂性，存在时间一致性问题。在本工作中，我们解决了在不重新训练或修改其架构的情况下，使用零样本基于图像的扩散模型提高视频修复时间一致性的挑战。我们提出了两种互补的推理时策略：(1)基于神经科学启发的感知直线假设的感知直线引导(PSG)，通过在感知空间中引入曲率惩罚来引导扩散去噪过程向更平滑的时间演化发展，以改善时间感知分数，如Fréchet视频距离(FVD)和感知直线度；(2)多路径集成采样(MPES)，旨在通过集成多个扩散轨迹来减少随机变化，提高保真度(失真)分数，如PSNR和SSIM，而不牺牲清晰度。这些无需训练的技术共同为使用大型预训练扩散模型实现时间稳定的高保真感知视频修复提供了实用路径。我们在多个数据集和退化类型上进行了广泛实验，系统评估了每种策略以了解其优势和局限性。我们的结果表明，虽然PSG增强了时间自然性，特别是在时间模糊的情况下，但MPES在所有任务中一致提高了保真度和时空感知-失真权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models have emerged as powerful priors for single-imagerestoration, but their application to zero-shot video restoration suffers fromtemporal inconsistencies due to the stochastic nature of sampling andcomplexity of incorporating explicit temporal modeling. In this work, weaddress the challenge of improving temporal coherence in video restorationusing zero-shot image-based diffusion models without retraining or modifyingtheir architecture. We propose two complementary inference-time strategies: (1)Perceptual Straightening Guidance (PSG) based on the neuroscience-inspiredperceptual straightening hypothesis, which steers the diffusion denoisingprocess towards smoother temporal evolution by incorporating a curvaturepenalty in a perceptual space to improve temporal perceptual scores, such asFr\'echet Video Distance (FVD) and perceptual straightness; and (2) Multi-PathEnsemble Sampling (MPES), which aims at reducing stochastic variation byensembling multiple diffusion trajectories to improve fidelity (distortion)scores, such as PSNR and SSIM, without sacrificing sharpness. Together, thesetraining-free techniques provide a practical path toward temporally stablehigh-fidelity perceptual video restoration using large pretrained diffusionmodels. We performed extensive experiments over multiple datasets anddegradation types, systematically evaluating each strategy to understand theirstrengths and limitations. Our results show that while PSG enhances temporalnaturalness, particularly in case of temporal blur, MPES consistently improvesfidelity and spatio-temporal perception--distortion trade-off across all tasks.</description>
      <author>example@mail.com (Nasrin Rahimi, A. Murat Tekalp)</author>
      <guid isPermaLink="false">2510.25420v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA</title>
      <link>http://arxiv.org/abs/2510.25332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StreamingCoT是首个专为流媒体视频问答中的时间演化和多模态思维链任务设计的数据集，解决了现有VideoQA数据集无法捕捉时间动态和缺少明确推理过程标注的问题。&lt;h4&gt;背景&lt;/h4&gt;流媒体视频应用的快速增长需要具有增强时间动态理解和复杂推理能力多模态模型，但当前VideoQA数据集存在静态标注机制无法捕捉视频流中答案的演变性质，以及缺少明确推理过程标注两大关键限制。&lt;h4&gt;目的&lt;/h4&gt;解决现有VideoQA数据集的两个关键限制：1)静态标注机制无法捕捉时间视频流中答案的演变性质；2)缺少明确的推理过程标注，限制了模型的可解释性和逻辑推理能力。&lt;h4&gt;方法&lt;/h4&gt;建立动态分层标注架构，生成每秒密集描述并通过相似性融合构建时间依赖的语义段，加入受时间演化模式约束的问题-答案集；提出明确推理链生成范式，通过关键帧语义提取时空对象，使用大语言模型基于对象状态转换推导推理路径，并通过人工验证确保逻辑一致性。&lt;h4&gt;主要发现&lt;/h4&gt;通过StreamingCoT数据集的构建，为推进流媒体视频理解、复杂时间推理和多模态推理研究奠定了基础。&lt;h4&gt;结论&lt;/h4&gt;StreamingCoT数据集及其构建工具包为解决流媒体视频理解中的时间动态和复杂推理问题提供了有效支持，相关资源已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;流媒体视频应用的快速增长需要具有增强时间动态理解和复杂推理能力多模态模型。然而，当前视频问答(VideoQA)数据集存在两个关键限制：1)静态标注机制无法捕捉时间视频流中答案的演变性质；2)缺少明确的推理过程标注，限制了模型的可解释性和逻辑推理能力。为解决这些挑战，我们引入了StreamingCoT，这是首个专为流媒体视频问答中的时间演化推理和多模态思维链(CoT)任务设计的数据集。我们的框架首先建立了动态分层标注架构，生成每秒密集描述并通过相似性融合构建时间依赖的语义段，同时加入受时间演化模式约束的问题-答案集。我们进一步提出了明确的推理链生成范式，通过关键帧语义对齐提取时空对象，使用大语言模型基于对象状态转换推导推理路径，并通过人工验证确保逻辑一致性。该数据集为推进流媒体视频理解、复杂时间推理和多模态推理研究奠定了基础。我们的StreamingCoT及其构建工具包可在https://github.com/Fleeting-hyh/StreamingCoT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758311&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of streaming video applications demands multimodal modelswith enhanced capabilities for temporal dynamics understanding and complexreasoning. However, current Video Question Answering (VideoQA) datasets sufferfrom two critical limitations: 1) Static annotation mechanisms fail to capturethe evolving nature of answers in temporal video streams, and 2) The absence ofexplicit reasoning process annotations restricts model interpretability andlogical deduction capabilities. To address these challenges, We introduceStreamingCoT, the first dataset explicitly designed for temporally evolvingreasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Ourframework first establishes a dynamic hierarchical annotation architecture thatgenerates per-second dense descriptions and constructs temporally-dependentsemantic segments through similarity fusion, paired with question-answer setsconstrained by temporal evolution patterns. We further propose an explicitreasoning chain generation paradigm that extracts spatiotemporal objects viakeyframe semantic alignment, derives object state transition-based reasoningpaths using large language models, and ensures logical coherence throughhuman-verified validation. This dataset establishes a foundation for advancingresearch in streaming video understanding, complex temporal reasoning, andmultimodal inference. Our StreamingCoT and its construction toolkit can beaccessed at https://github.com/Fleeting-hyh/StreamingCoT.</description>
      <author>example@mail.com (Yuhang Hu, Zhenyu Yang, Shihan Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Changsheng Xu)</author>
      <guid isPermaLink="false">2510.25332v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</title>
      <link>http://arxiv.org/abs/2510.25760v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述文章全面回顾了大型多模态空间推理模型在各类任务中的进展，分类了多模态大语言模型的最新研究，并引入了开放基准进行评估。&lt;h4&gt;背景&lt;/h4&gt;人类具有空间推理能力，能够通过视觉和声音等多模态观察来理解空间。大型多模态推理模型扩展了这些能力，在各种空间任务中展现出有希望的性能，但系统性综述和公开可用的基准仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提供对大型多模态空间推理任务的全面回顾，分类多模态大语言模型的进展，并引入用于评估的开放基准。&lt;h4&gt;方法&lt;/h4&gt;文章首先概述通用空间推理，重点关注训练后技术、可解释性和架构。研究内容包括空间关系推理、场景和布局理解、3D空间中的视觉问答和定位，以及具身AI（如视觉语言导航和动作模型）。此外还探讨了音频和第一人称视频等新兴模态对空间理解的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;多模态空间推理模型在2D和3D空间理解、视觉问答、具身AI等任务中展现出有希望的性能。音频和第一人称视频等新兴模态通过新传感器为空间理解提供了新的视角。&lt;h4&gt;结论&lt;/h4&gt;这篇综述为多模态空间推理这一不断发展的领域奠定了坚实的基础，并提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;人类拥有空间推理能力，使他们能够通过多模态观察（如视觉和声音）来理解空间。大型多模态推理模型通过学习感知和推理扩展了这些能力，在各种空间任务中展现出有希望的性能。然而，对这些模型的系统性综述和公开可用的基准仍然有限。在本综述中，我们对大型多模态空间推理任务进行了全面回顾，分类了多模态大语言模型的最新进展，并引入了用于评估的开放基准。我们首先概述了通用的空间推理，重点关注训练后技术、可解释性和架构。除了传统的2D任务外，我们还研究了空间关系推理、场景和布局理解，以及3D空间中的视觉问答和定位。我们还回顾了具身AI的进展，包括视觉语言导航和动作模型。此外，我们还考虑了音频和第一人称视频等新兴模态，这些模态通过新传感器为空间理解做出贡献。我们相信本综述为多模态空间推理这一不断发展的领域奠定了坚实的基础，并提供了有价值的见解。关于本综述的更新信息、开放基准的代码和实现可以在https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态空间推理在大型模型时代缺乏系统综述和公开基准测试的问题。这个问题很重要，因为人类通过视觉、声音等多模态输入理解空间的能力是基础性的，而大型语言模型虽然文本处理能力强，但空间推理能力有限。整合多模态信息增强空间推理对机器人导航、增强现实、自动驾驶等现实应用至关重要，同时缺乏系统评估阻碍了该领域的标准化发展和比较研究。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性回顾和分析现有文献构建了这篇综述。他们首先定义多模态空间推理，然后分类各类空间任务（从2D到3D，从静态到动态），分析技术进展（测试时扩展、后训练方法、架构修改等），最后引入评估基准。作者确实借鉴了大量现有工作，如Wang等人的小型推理模型研究、Ke等人的推理扩展分析、Zha等人的3D能力研究等，但指出这些工作要么未深入多模态空间推理，要么缺乏系统评估框架，因此他们的综述填补了这一空白。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提供首个多模态空间推理在大型模型时代的全面综述，建立系统分类框架，并引入开放基准。整体流程：1)定义多模态空间推理并概述评估维度；2)分析一般多模态空间推理技术（测试时扩展、后训练、架构修改等）；3)探讨3D空间中的核心任务（视觉定位、场景推理、3D生成）；4)讨论具身AI中的空间推理；5)考虑音频和第一人称视频等新兴模态；6)提供开放基准和评估框架。作者还通过GitHub仓库提供代码和最新信息，方便研究实践。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)构建首个专门针对多模态空间推理的系统综述框架；2)建立详细任务分类体系，涵盖从2D到3D、静态到动态、视觉到其他模态的广泛任务；3)引入开放基准标准化评估；4)整合音频和第一人称视频等新兴模态；5)提供跨领域视角连接传统2D理解与3D推理、具身AI等。相比之前工作，本文专注多模态空间推理而非一般推理，提供系统性评估框架而非单一任务分析，引入开放基准而非仅文献回顾，并通过GitHub提供实用资源，更全面且实用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文为多模态空间推理在大型模型时代提供了首个全面的综述框架，系统性地分类了各类空间任务，引入了开放评估基准，并通过整合新兴模态为该领域的研究和实践奠定了坚实基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans possess spatial reasoning abilities that enable them to understandspaces through multimodal observations, such as vision and sound. Largemultimodal reasoning models extend these abilities by learning to perceive andreason, showing promising performance across diverse spatial tasks. However,systematic reviews and publicly available benchmarks for these models remainlimited. In this survey, we provide a comprehensive review of multimodalspatial reasoning tasks with large models, categorizing recent progress inmultimodal large language models (MLLMs) and introducing open benchmarks forevaluation. We begin by outlining general spatial reasoning, focusing onpost-training techniques, explainability, and architecture. Beyond classical 2Dtasks, we examine spatial relationship reasoning, scene and layoutunderstanding, as well as visual question answering and grounding in 3D space.We also review advances in embodied AI, including vision-language navigationand action models. Additionally, we consider emerging modalities such as audioand egocentric video, which contribute to novel spatial understanding throughnew sensors. We believe this survey establishes a solid foundation and offersinsights into the growing field of multimodal spatial reasoning. Updatedinformation about this survey, codes and implementation of the open benchmarkscan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.</description>
      <author>example@mail.com (Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu)</author>
      <guid isPermaLink="false">2510.25760v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>EA3D: Online Open-World 3D Object Extraction from Streaming Videos</title>
      <link>http://arxiv.org/abs/2510.25146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The Thirty-Ninth Annual Conference on Neural Information Processing  Systems(NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ExtractAnything3D (EA3D)，一个统一的在线开放世界3D物体提取框架，能够同时实现几何重建和整体场景理解。&lt;h4&gt;背景&lt;/h4&gt;当前3D场景理解方法受限于离线收集的多视图数据或预构建的3D几何形状。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的在线框架，用于开放世界的3D物体提取，同时实现几何重建和整体场景理解。&lt;h4&gt;方法&lt;/h4&gt;EA3D通过视觉语言和2D视觉基础编码器动态解释每个视频帧提取物体级知识，使用前馈在线更新策略将知识集成到高斯特征图中，从历史帧迭代估计视觉里程计并增量更新高斯特征，通过循环联合优化模块引导模型关注感兴趣区域。&lt;h4&gt;主要发现&lt;/h4&gt;EA3D在多样化基准测试和任务中表现出色，包括照片级真实感渲染、语义和实例分割、3D边界框和语义占用估计以及3D网格生成。&lt;h4&gt;结论&lt;/h4&gt;EA3D建立了统一的、高效的框架，用于联合在线3D重建和整体场景理解，能够支持广泛的下游任务。&lt;h4&gt;翻译&lt;/h4&gt;当前的3D场景理解方法受限于离线收集的多视图数据或预构建的3D几何形状。在本文中，我们提出了ExtractAnything3D (EA3D)，一个统一的在线开放世界3D物体提取框架，能够同时实现几何重建和整体场景理解。给定流式视频，EA3D使用视觉语言和2D视觉基础编码器动态解释每个帧，提取物体级知识。这些知识通过前馈在线更新策略被集成并嵌入到高斯特征图中。然后我们从历史帧迭代估计视觉里程计，并用新观察增量更新在线高斯特征。循环联合优化模块引导模型关注感兴趣区域，同时增强几何重建和语义理解。在多样化的基准测试和任务中的大量实验，包括照片级真实感渲染、语义和实例分割、3D边界框和语义占用估计以及3D网格生成，证明了EA3D的有效性。我们的方法为联合在线3D重建和整体场景理解建立了统一且高效的框架，能够支持广泛的下游任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从流式视频中实时提取和理解开放世界中的3D物体问题。这个问题在现实中非常重要，因为自主智能体（如机器人）需要在陌生环境中实时理解和重建周围环境，而现实世界中的场景是开放的，物体种类和数量未知，需要同时处理流式视频输入并理解物体的几何结构和语义信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有的视觉语言模型在2D开放世界理解上表现出色，但在3D领域存在视角不一致和几何错位问题。他们发现直接将2D模型提升到3D的方法需要预构建的几何和标注数据，而现有的可微分渲染框架又需要完整的多视图图像。受人类感知启发，作者设计了EA3D，使其能像人类一样进入环境时立即开始处理视觉输入。该方法借鉴了视觉语言模型进行开放世界解释，利用视觉基础模型提取特征，基于高斯泼溅构建在线表示，并参考了在线视觉里程计和高斯更新的方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是建立一个统一的在线开放世界3D物体提取框架，能同时进行几何重建和场景理解，无需几何或姿态先验。整体流程包括：1)知识提取与集成：使用VLMs识别物体，维护语义缓存，利用VFMs提取特征并嵌入高斯表示；2)在线3D物体提取：通过在线视觉里程计估计相机姿态，利用在线高斯更新重建几何和理解语义；3)循环联合优化：设计语义感知正则化，联合优化高斯特征和相机姿态，结合多种损失函数提升重建和理解质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的在线开放世界3D物体提取框架，能同时进行重建和理解；2)在线特征高斯表示，结合在线视觉里程计和高斯更新；3)循环联合优化策略，动态引导模型注意力；4)支持多种下游任务。相比之前的工作，EA3D的不同之处在于：它能在线处理流式视频而非依赖完整多视图；能处理开放世界中的未知物体类别；无需预构建几何或姿态先验；提供支持多种任务的统一框架而非专注于单一任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; EA3D提出了一种统一的在线框架，能够从流式视频中实时提取开放世界中的3D物体，同时进行几何重建和语义理解，无需任何几何或姿态先验，支持多种下游3D感知任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current 3D scene understanding methods are limited by offline-collectedmulti-view data or pre-constructed 3D geometry. In this paper, we presentExtractAnything3D (EA3D), a unified online framework for open-world 3D objectextraction that enables simultaneous geometric reconstruction and holisticscene understanding. Given a streaming video, EA3D dynamically interprets eachframe using vision-language and 2D vision foundation encoders to extractobject-level knowledge. This knowledge is integrated and embedded into aGaussian feature map via a feed-forward online update strategy. We theniteratively estimate visual odometry from historical frames and incrementallyupdate online Gaussian features with new observations. A recurrent jointoptimization module directs the model's attention to regions of interest,simultaneously enhancing both geometric reconstruction and semanticunderstanding. Extensive experiments across diverse benchmarks and tasks,including photo-realistic rendering, semantic and instance segmentation, 3Dbounding box and semantic occupancy estimation, and 3D mesh generation,demonstrate the effectiveness of EA3D. Our method establishes a unified andefficient framework for joint online 3D reconstruction and holistic sceneunderstanding, enabling a broad range of downstream tasks.</description>
      <author>example@mail.com (Xiaoyu Zhou, Jingqi Wang, Yuang Jia, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang)</author>
      <guid isPermaLink="false">2510.25146v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Vision-Language Integration for Zero-Shot Scene Understanding in Real-World Environments</title>
      <link>http://arxiv.org/abs/2510.25070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint under review at IEEE Transactions on Pattern Analysis and  Machine Intelligence (TPAMI), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种视觉-语言集成框架，通过统一预训练视觉编码器和大语言模型，实现零样本场景理解，在多个数据集上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;真实世界环境中的零样本场景理解面临重大挑战，由于自然场景的复杂性和可变性，模型必须在没有先前标记示例的情况下识别新对象、动作和上下文。&lt;h4&gt;目的&lt;/h4&gt;实现稳健的零样本场景理解，利用自然语言作为桥梁，推广到未见过的类别和上下文。&lt;h4&gt;方法&lt;/h4&gt;提出视觉-语言集成框架，统一预训练视觉编码器（如CLIP、ViT）和大语言模型（如基于GPT的架构），开发将视觉输入和文本提示嵌入共享空间的统一模型，并使用多模态融合和推理层进行上下文解释。&lt;h4&gt;主要发现&lt;/h4&gt;在Visual Genome、COCO、ADE20K和自定义真实世界数据集上的实验表明，与最先进的零样本模型相比，在对象识别、活动检测和场景字幕生成方面有显著提升，top-1准确率提高高达18%，语义一致性指标也有显著提升。&lt;h4&gt;结论&lt;/h4&gt;跨模态对齐和语言锚定在增强真实世界场景理解的泛化能力方面非常有效。&lt;h4&gt;翻译&lt;/h4&gt;真实世界环境中的零样本场景理解由于自然场景的复杂性和可变性而面临重大挑战，模型必须在没有先前标记示例的情况下识别新对象、动作和上下文。这项工作提出了一种视觉-语言集成框架，统一了预训练的视觉编码器（如CLIP、ViT）和大语言模型（如基于GPT的架构），以实现视觉和文本模态之间的语义对齐。目标是利用自然语言作为桥梁，推广到未见过的类别和上下文，实现稳健的零样本场景理解。我们的方法开发了一个统一模型，将视觉输入和文本提示嵌入到共享空间，然后使用多模态融合和推理层进行上下文解释。在Visual Genome、COCO、ADE20K和自定义真实世界数据集上的实验表明，与最先进的零样本模型相比，在对象识别、活动检测和场景字幕生成方面有显著提升。提出的系统在top-1准确率上提高了高达18%，在语义一致性指标方面也有显著提升，突显了跨模态对齐和语言锚定在增强真实世界场景理解泛化能力方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-shot scene understanding in real-world settings presents majorchallenges due to the complexity and variability of natural scenes, wheremodels must recognize new objects, actions, and contexts without prior labeledexamples. This work proposes a vision-language integration framework thatunifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models(e.g., GPT-based architectures) to achieve semantic alignment between visualand textual modalities. The goal is to enable robust zero-shot comprehension ofscenes by leveraging natural language as a bridge to generalize over unseencategories and contexts. Our approach develops a unified model that embedsvisual inputs and textual prompts into a shared space, followed by multimodalfusion and reasoning layers for contextual interpretation. Experiments onVisual Genome, COCO, ADE20K, and custom real-world datasets demonstratesignificant gains over state-of-the-art zero-shot models in object recognition,activity detection, and scene captioning. The proposed system achieves up to18% improvement in top-1 accuracy and notable gains in semantic coherencemetrics, highlighting the effectiveness of cross-modal alignment and languagegrounding in enhancing generalization for real-world scene understanding.</description>
      <author>example@mail.com (Manjunath Prasad Holenarasipura Rajiv, B. M. Vidyavathi)</author>
      <guid isPermaLink="false">2510.25070v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.24792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 11 tables and figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PISA-Bench是一个基于PISA测试的多语言多模态推理基准，包含六种语言的平行数据集，用于评估视觉语言模型在不同语言和推理任务上的表现。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在多模态推理方面取得显著进展，但现有基准测试在高质量、人工验证的例子方面有限，且大多数数据集仅限于英语，翻译样本的质量保证既耗时又昂贵。&lt;h4&gt;目的&lt;/h4&gt;填补高质量多语言多模态推理基准的空白，提供一个包含多种语言的人工验证数据集。&lt;h4&gt;方法&lt;/h4&gt;从专家创建的PISA测试英语例子中衍生出PISA-Bench，包含人工提取的指令、问题、答案选项和图像，并增加问题类型分类；将这些内容从英语翻译成西班牙语、德语、中文、法语和意大利语，形成完全平行的六语言语料库。&lt;h4&gt;主要发现&lt;/h4&gt;小型视觉语言模型(&lt;20B参数)在PISA-Bench上无法获得高分；模型在非英语部分的性能显著下降；当处理空间和几何推理任务时，模型错误率高。&lt;h4&gt;结论&lt;/h4&gt;通过发布PISA-Bench数据集和评估框架，为推进多模态多语言推理研究提供了重要资源。&lt;h4&gt;翻译&lt;/h4&gt;原文摘要为英文，上述内容已将其核心信息翻译并结构化为中文。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) have demonstrated remarkable progress inmultimodal reasoning. However, existing benchmarks remain limited in terms ofhigh-quality, human-verified examples. Many current datasets rely onsynthetically generated content by large language models (LLMs). Furthermore,most datasets are limited to English, as manual quality assurance of translatedsamples is time-consuming and costly. To fill this gap, we introducePISA-Bench, a multilingual benchmark derived from English examples of theexpert-created PISA tests, a unified framework for the assessment of studentcompetencies in over eighty countries. Each example consists of human-extractedinstructions, questions, answer options, and images, enriched with questiontype categories, and has been translated from English into five additionallanguages (Spanish, German, Chinese, French, and Italian), resulting in a fullyparallel corpus covering six languages. We evaluate state-of-the-artvision-language models on PISA-Bench and find that especially small models(&lt;20B parameters) fail to achieve high test scores. We further find substantialperformance degradation on non-English splits as well as high error-rates whenmodels are tasked with spatial and geometric reasoning. By releasing thedataset and evaluation framework, we provide a resource for advancing researchon multilingual multimodal reasoning.</description>
      <author>example@mail.com (Patrick Haller, Fabio Barth, Jonas Golde, Georg Rehm, Alan Akbik)</author>
      <guid isPermaLink="false">2510.24792v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs</title>
      <link>http://arxiv.org/abs/2510.25753v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025, 24 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了预训练Transformer模型中的上下文学习（ICL）能力，特别是在具有非线性MLP头部的模型上，从多个异构数据源学习的非线性任务。作者通过理论分析和实验验证，证明了非线性MLP相比线性基线能显著提升ICL性能，特别是在非线性任务上，并确定了高质量数据源的关键特性和特征学习的条件。&lt;h4&gt;背景&lt;/h4&gt;预训练Transformer模型展现出显著的上下文学习能力，使其能够在无需参数更新的情况下从演示中适应新任务。然而，理论研究通常依赖于简化的架构（如省略MLP）、数据模型（如具有各向同性输入的线性回归）和单源训练，这限制了它们与现实设置的相关性。&lt;h4&gt;目的&lt;/h4&gt;研究具有非线性MLP头部的预训练Transformer在从多个具有异构输入、任务和噪声分布的数据源获取的非线性任务上的ICL能力，分析数据混合效应，并提供关于架构和数据在ICL中作用的可操作见解。&lt;h4&gt;方法&lt;/h4&gt;分析一个包含两层的MLP模型，其中第一层通过单次梯度步骤训练，第二层完全优化。在高维渐近条件下，利用高斯普适性和正交多项式理论，证明这类模型的ICL误差等价于结构化多项式预测器。在各种激活函数、模型大小和数据分布上进行经验验证，并在多语言情感分析的真实场景中进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;非线性MLP能显著提升ICL性能，特别是在非线性任务上；高质量数据源具有低噪声和结构化协方差的关键特性；只有当任务协方差具有足够结构时，特征学习才会出现；这些发现在各种激活函数、模型大小和数据分布上得到了经验验证；多语言情感分析实验表明这些发现可以扩展到真实世界案例。&lt;h4&gt;结论&lt;/h4&gt;这项工作推进了Transformer中ICL的理论基础，并提供了关于架构和数据在ICL中作用的可操作见解，特别是在非线性任务和异构数据源设置下。&lt;h4&gt;翻译&lt;/h4&gt;预训练的Transformer模型展现出显著的上下文学习（ICL）能力，使它们能够在无需参数更新的情况下从演示中适应新任务。然而，理论研究通常依赖于简化的架构（例如，省略MLP）、数据模型（例如，具有各向同性输入的线性回归）和单源训练，限制了它们与现实设置的相关性。在这项工作中，我们研究了具有非线性MLP头部的预训练Transformer的ICL能力，这些模型在从多个具有异构输入、任务和噪声分布的数据源中获取的非线性任务上表现。我们分析了一个模型，其中MLP包含两层，第一层通过单次梯度步骤训练，第二层完全优化。在高维渐近条件下，我们证明这类模型的ICL误差等价于结构化多项式预测器，利用了高斯普适性和正交多项式理论的结果。这种等价性表明非线性MLP相比线性基线能显著提升ICL性能，特别是在非线性任务上。它还使数据分析混合效应的精确分析成为可能：我们确定了高质量数据源的关键特性（低噪声、结构化协方差），并表明只有当任务协方差具有足够结构时，特征学习才会出现。这些发现在各种激活函数、模型大小和数据分布上得到了经验验证。最后，我们在一个涉及多语言情感分析的真实场景中进行了实验，每种语言被视为不同的数据源。这个案例的实验结果说明了我们的发现如何扩展到真实世界案例。总体而言，我们的工作推进了Transformer中ICL的理论基础，并提供了关于架构和数据在ICL中作用的可操作见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pretrained Transformers demonstrate remarkable in-context learning (ICL)capabilities, enabling them to adapt to new tasks from demonstrations withoutparameter updates. However, theoretical studies often rely on simplifiedarchitectures (e.g., omitting MLPs), data models (e.g., linear regression withisotropic inputs), and single-source training, limiting their relevance torealistic settings. In this work, we study ICL in pretrained Transformers withnonlinear MLP heads on nonlinear tasks drawn from multiple data sources withheterogeneous input, task, and noise distributions. We analyze a model wherethe MLP comprises two layers, with the first layer trained via a singlegradient step and the second layer fully optimized. Under high-dimensionalasymptotics, we prove that such models are equivalent in ICL error tostructured polynomial predictors, leveraging results from the theory ofGaussian universality and orthogonal polynomials. This equivalence reveals thatnonlinear MLPs meaningfully enhance ICL performance, particularly on nonlineartasks, compared to linear baselines. It also enables a precise analysis of datamixing effects: we identify key properties of high-quality data sources (lownoise, structured covariances) and show that feature learning emerges only whenthe task covariance exhibits sufficient structure. These results are validatedempirically across various activation functions, model sizes, and datadistributions. Finally, we experiment with a real-world scenario involvingmultilingual sentiment analysis where each language is treated as a differentsource. Our experimental results for this case exemplify how our findingsextend to real-world cases. Overall, our work advances the theoreticalfoundations of ICL in Transformers and provides actionable insight into therole of architecture and data in ICL.</description>
      <author>example@mail.com (Samet Demir, Zafer Dogan)</author>
      <guid isPermaLink="false">2510.25753v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models</title>
      <link>http://arxiv.org/abs/2510.25577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures, 4 tables, submitted to LREC 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了语音基础模型(SFMs)如何处理语音中的副语言变化，特别是音质(如嘶哑和气声)对模型行为的影响。作者通过开放式生成任务和语音情感识别评估模型对不同音质输入的一致性反应，并引入了新的平行数据集来评估SFMs对音质的敏感性。&lt;h4&gt;背景&lt;/h4&gt;语音基础模型(SFMs)可直接处理原始音频中的口语，绕过文本表示，因此能接触到语音信号中的副语言变化。音质是副语言变化中未被充分探索的维度，包括嘶哑和气声等发声类型，这些类型影响听众对情感状态、立场和社会意义的推断。现有语音理解基准主要依赖多项选择题格式，容易失败，难以捕捉副语言特征对模型行为的微妙影响。&lt;h4&gt;目的&lt;/h4&gt;通过开放式生成任务和语音情感识别探测SFMs，评估模型行为在不同音质输入下是否一致；引入包含音质合成修改的平行数据集，评估SFMs对嘶哑和气声的反应；提供对SFMs对这些特定语音感知非词汇方面敏感性的首次检验。&lt;h4&gt;方法&lt;/h4&gt;使用开放式生成任务探测SFMs；通过语音情感识别评估模型反应；引入包含音质合成修改的平行数据集；评估SFMs对嘶哑和气声的反应。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体实验结果，主要介绍了研究方法和数据集的构建。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及具体结论，主要介绍了研究的创新点和贡献，即首次检验了SFMs对语音中特定非词汇方面的敏感性。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型(SFMs)的最新进展使得可以直接处理原始音频中的口语，绕过中间的文本表示。这种能力使SFMs能够接触到输入语音信号中嵌入的丰富副语言变化，并可能对这些变化做出响应。副语言变化的一个未被充分探索的维度是音质，包括嘶哑和气声等发声类型。这些发声类型已知会影响听众如何推断语音中的情感状态、立场和社会意义。现有的语音理解基准测试主要依赖多项选择题问答(MCQA)格式，这些格式容易失败，因此在捕捉副语言特征如何微妙影响模型行为方面并不可靠。在本文中，我们通过开放式生成任务和语音情感识别来探测SFMs，评估模型行为在不同音质输入下是否一致。我们引入了一个新的平行数据集，其中包含对音质的合成修改，旨在评估SFMs对嘶哑和气声的反应。我们的工作首次检验了SFMs对这些特定语音感知非词汇方面的敏感性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in speech foundation models (SFMs) have enabled the directprocessing of spoken language from raw audio, bypassing intermediate textualrepresentations. This capability allows SFMs to be exposed to, and potentiallyrespond to, rich paralinguistic variations embedded in the input speech signal.One under-explored dimension of paralinguistic variation is voice quality,encompassing phonation types such as creaky and breathy voice. These phonationtypes are known to influence how listeners infer affective state, stance andsocial meaning in speech. Existing benchmarks for speech understanding largelyrely on multiple-choice question answering (MCQA) formats, which are prone tofailure and therefore unreliable in capturing the nuanced ways paralinguisticfeatures influence model behaviour. In this paper, we probe SFMs throughopen-ended generation tasks and speech emotion recognition, evaluating whethermodel behaviours are consistent across different phonation inputs. We introducea new parallel dataset featuring synthesized modifications to voice quality,designed to evaluate SFM responses to creaky and breathy voice. Our workprovides the first examination of SFM sensitivity to these particularnon-lexical aspects of speech perception.</description>
      <author>example@mail.com (Harm Lameris, Shree Harsha Bokkahalli Satish, Joakim Gustafson, Éva Székely)</author>
      <guid isPermaLink="false">2510.25577v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting</title>
      <link>http://arxiv.org/abs/2510.25563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究将大气预报深度学习模型Aurora适应化用于海洋预测，通过微调实现了高精度的海表温度预测，同时降低了计算成本，为数据驱动的海洋预报提供了新方法。&lt;h4&gt;背景&lt;/h4&gt;准确的海洋变量预测对理解气候变化、管理海洋资源和优化海洋活动至关重要。传统海洋预报依赖数值模型，但面临计算成本高和可扩展性有限的问题。&lt;h4&gt;目的&lt;/h4&gt;将Aurora深度学习模型（原为大气预报设计）适应化，用于预测加那利上升流系统的海表温度(SST)，探索深度学习在海洋预报中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;使用高分辨率海洋再分析数据对模型进行分阶段微调，结合纬度加权误差指标，优化超参数以实现高效学习，减少计算需求。&lt;h4&gt;主要发现&lt;/h4&gt;模型实现了0.119K的低均方根误差，异常相关系数高达0.997，成功重现大尺度SST结构，但在捕捉沿海地区精细细节方面存在挑战。&lt;h4&gt;结论&lt;/h4&gt;研究证明使用在不同领域预训练的深度学习模型进行海洋应用具有可行性，未来改进方向包括整合更多海洋变量、提高空间分辨率和探索物理信息神经网络。&lt;h4&gt;翻译&lt;/h4&gt;准确的海洋变量预测对于理解气候变化、管理海洋资源和优化海洋活动至关重要。传统的海洋预报依赖于数值模型；然而，这些方法在计算成本和可扩展性方面存在局限性。在本研究中，我们将Aurora（一种最初为大气预报设计的基础深度学习模型）适应化，用于预测加那利上升流系统的海表温度(SST)。通过使用高分辨率的海洋再分析数据对模型进行微调，我们展示了其捕捉复杂时空模式的能力，同时减少了计算需求。我们的方法包括分阶段微调过程，结合纬度加权误差指标，并优化超参数以实现高效学习。实验结果显示，模型实现了0.119K的低均方根误差，并保持高的异常相关系数(ACC≈0.997)。模型成功重现了大尺度SST结构，但在捕捉沿海地区更精细的细节方面面临挑战。这项工作通过证明使用在不同领域预训练的深度学习模型进行海洋应用的可行性，为数据驱动的海洋预报领域做出了贡献。未来改进包括整合额外的海洋变量、提高空间分辨率，以及探索物理信息神经网络以增强可解释性和理解。这些进步可以改善气候建模和海洋预测精度，支持环境和经济部门的决策制定。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The accurate prediction of oceanographic variables is crucial forunderstanding climate change, managing marine resources, and optimizingmaritime activities. Traditional ocean forecasting relies on numerical models;however, these approaches face limitations in terms of computational cost andscalability. In this study, we adapt Aurora, a foundational deep learning modeloriginally designed for atmospheric forecasting, to predict sea surfacetemperature (SST) in the Canary Upwelling System. By fine-tuning this modelwith high-resolution oceanographic reanalysis data, we demonstrate its abilityto capture complex spatiotemporal patterns while reducing computationaldemands. Our methodology involves a staged fine-tuning process, incorporatinglatitude-weighted error metrics and optimizing hyperparameters for efficientlearning. The experimental results show that the model achieves a low RMSE of0.119K, maintaining high anomaly correlation coefficients (ACC $\approx0.997$). The model successfully reproduces large-scale SST structures but faceschallenges in capturing finer details in coastal regions. This work contributesto the field of data-driven ocean forecasting by demonstrating the feasibilityof using deep learning models pre-trained in different domains for oceanicapplications. Future improvements include integrating additional oceanographicvariables, increasing spatial resolution, and exploring physics-informed neuralnetworks to enhance interpretability and understanding. These advancements canimprove climate modeling and ocean prediction accuracy, supportingdecision-making in environmental and economic sectors.</description>
      <author>example@mail.com (Víctor Medina, Giovanny A. Cuervo-Londoño, Javier Sánchez)</author>
      <guid isPermaLink="false">2510.25563v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>FaCT: Faithful Concept Traces for Explaining Neural Network Decisions</title>
      <link>http://arxiv.org/abs/2510.25512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025; Code is available at  https://github.com/m-parchami/FaCT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种具有模型内在机制概念解释的新模型，强调概念化解释的忠实性，并引入了概念一致性度量C²-Score来评估概念化方法。&lt;h4&gt;背景&lt;/h4&gt;深度网络在各种任务上表现出色，但要全面理解其工作机制仍然是一个挑战。现有的后验概念化方法在解释模型时并不总是忠实于模型本身，且对模型学习的概念做出了严格的假设。&lt;h4&gt;目的&lt;/h4&gt;强调概念化解释的忠实性，提出一种具有模型内在机制概念解释的新模型，并开发一种新的概念一致性度量标准来评估概念化方法。&lt;h4&gt;方法&lt;/h4&gt;提出的新模型的概念跨类别共享，可以从任何层追踪其对logit的贡献和输入可视化。利用基础模型提出了一种新的概念一致性度量标准C²-Score，用于评估概念化方法。&lt;h4&gt;主要发现&lt;/h4&gt;与先前的工作相比，提出的模型在定量上更加一致，用户发现其概念更具可解释性，同时保持了有竞争力的ImageNet性能。&lt;h4&gt;结论&lt;/h4&gt;通过强调概念化解释的忠实性和提出新的度量标准，该研究为理解深度网络的工作机制提供了更有效的方法。&lt;h4&gt;翻译&lt;/h4&gt;深度网络在广泛任务中表现出色，但要全面理解其工作机制仍然是一个关键挑战。许多后验概念化方法被引入以解释其工作原理，但它们并不总是忠实于模型。此外，它们对模型学习的概念做出了严格的假设，如类别特异性、小的空间范围或符合人类预期。在本工作中，我们强调此类概念化解释的忠实性，并提出了一种具有模型内在机制概念解释的新模型。我们的概念跨类别共享，并且可以从任何层追踪其对logit的贡献及其输入可视化。我们还利用基础模型提出了一个新的概念一致性度量标准C²-Score，可用于评估概念化方法。我们表明，与先前的工作相比，我们的概念在定量上更加一致，用户发现我们的概念更具可解释性，同时保持了有竞争力的ImageNet性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep networks have shown remarkable performance across a wide range of tasks,yet getting a global concept-level understanding of how they function remains akey challenge. Many post-hoc concept-based approaches have been introduced tounderstand their workings, yet they are not always faithful to the model.Further, they make restrictive assumptions on the concepts a model learns, suchas class-specificity, small spatial extent, or alignment to human expectations.In this work, we put emphasis on the faithfulness of such concept-basedexplanations and propose a new model with model-inherent mechanisticconcept-explanations. Our concepts are shared across classes and, from anylayer, their contribution to the logit and their input-visualization can befaithfully traced. We also leverage foundation models to propose a newconcept-consistency metric, C$^2$-Score, that can be used to evaluateconcept-based methods. We show that, compared to prior work, our concepts arequantitatively more consistent and users find our concepts to be moreinterpretable, all while retaining competitive ImageNet performance.</description>
      <author>example@mail.com (Amin Parchami-Araghi, Sukrut Rao, Jonas Fischer, Bernt Schiele)</author>
      <guid isPermaLink="false">2510.25512v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.25502v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 18 figures, 13 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TempoPFN，一种基于线性循环神经网络的单变量时间序列基础模型，仅在合成数据上预训练，解决了零样本时间序列预测中的长期预测效率和可重现性问题。&lt;h4&gt;背景&lt;/h4&gt;零样本时间序列预测的基础模型面临长期预测效率低和可重现性差的挑战，现有的仅使用合成数据的方法在具有挑战性的基准测试中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且可重现的时间序列基础模型，用于零样本预测，超越现有仅使用合成数据的方法的性能。&lt;h4&gt;方法&lt;/h4&gt;TempoPFN采用GatedDeltaProduct架构和状态编织技术，实现跨序列长度的完全并行化训练，消除对窗口化或摘要技术的需求。综合合成数据管道统一了随机微分方程、高斯过程和音频合成等多种生成器，并引入新颖的数据增强技术。&lt;h4&gt;主要发现&lt;/h4&gt;在Gift-Eval基准测试中，TempoPFN达到顶尖竞争性能，超越所有现有的仅使用合成数据的方法，并超过绝大多数在真实数据上训练的模型。同时，通过完全并行化的训练和推理，比现有基线更高效。&lt;h4&gt;结论&lt;/h4&gt;开源完整的数据生成管道和训练代码，为未来研究提供可重现的基础，推动零样本时间序列预测领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;零样本时间序列预测的基础模型面临长期预测效率低和可重现性差的挑战，现有的仅使用合成数据的方法在具有挑战性的基准测试中表现不佳。本文提出了TempoPFN，一种基于线性循环神经网络的单变量时间序列基础模型，该模型仅在合成数据上进行预训练。该模型采用GatedDeltaProduct架构和状态编织技术，实现跨序列长度的完全并行化训练，消除对窗口化或摘要技术的需求，同时保持强大的时间状态跟踪能力。我们的综合合成数据管道统一了多种生成器，包括随机微分方程、高斯过程和音频合成，并引入了新颖的数据增强技术。在Gift-Eval基准的零样本评估中，TempoPFN达到了顶尖的竞争性能，超越了所有现有的仅使用合成数据的方法，并超过了绝大多数在真实数据上训练的模型，同时通过利用完全并行化的训练和推理，比现有基线更高效。我们开源了完整的数据生成管道和训练代码，为未来研究提供可重现的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for zero-shot time series forecasting face challenges inefficient long-horizon prediction and reproducibility, with existingsynthetic-only approaches underperforming on challenging benchmarks. This paperpresents TempoPFN, a univariate time series foundation model based on linearRecurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. Themodel uses a GatedDeltaProduct architecture with state-weaving for fullyparallelizable training across sequence lengths, eliminating the need forwindowing or summarization techniques while maintaining robust temporalstate-tracking. Our comprehensive synthetic data pipeline unifies diversegenerators, including stochastic differential equations, Gaussian processes,and audio synthesis, with novel augmentations. In zero-shot evaluations on theGift-Eval benchmark, TempoPFN achieves top-tier competitive performance,outperforming all existing synthetic-only approaches and surpassing the vastmajority of models trained on real-world data, while being more efficient thanexisting baselines by leveraging fully parallelizable training and inference.We open-source our complete data generation pipeline and training code,providing a reproducible foundation for future research.</description>
      <author>example@mail.com (Vladyslav Moroshan, Julien Siems, Arber Zela, Timur Carstensen, Frank Hutter)</author>
      <guid isPermaLink="false">2510.25502v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Position: Biology is the Challenge Physics-Informed ML Needs to Evolve</title>
      <link>http://arxiv.org/abs/2510.25368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出将物理信息机器学习(PIML)扩展到生物学领域，称为生物学信息机器学习(BIML)，以应对生物建模的独特挑战。&lt;h4&gt;背景&lt;/h4&gt;物理信息机器学习已成功将机理理解整合到机器学习中，特别是在受已知物理定律支配的领域，这一成功促使人们尝试将其应用于生物学领域。&lt;h4&gt;目的&lt;/h4&gt;将PIML的原则性方法扩展到生物学，创建BIML框架，使其能够适应生物学的实际现实，而非视为障碍。&lt;h4&gt;方法&lt;/h4&gt;重新调整PIML的方法，使其能够在更软性、概率形式的先验知识下运行，提出四大基础支柱：不确定性量化、上下文化、受限潜在结构推断和可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;生物建模面临的挑战（多方面且不确定的先验知识、异构且嘈杂的数据、部分可观察性以及复杂的高维网络）不应被视为PIML的障碍，而应被视为其进化的催化剂。&lt;h4&gt;结论&lt;/h4&gt;基础模型和大语言模型将成为关键推动因素，将人类专业知识与计算建模结合，构建BIML生态系统，并将PIML启发的创新引向具有高度科学和社会相关性的挑战。&lt;h4&gt;翻译&lt;/h4&gt;物理信息机器学习(PIML)已成功将机理理解整合到机器学习中，特别是在受已知物理定律支配的领域。这一成功促使人们尝试将PIML应用于生物学，这是一个充满动态系统但受不同约束塑造的领域。然而，生物建模面临独特挑战：多方面且不确定的先验知识、异构且嘈杂的数据、部分可观察性以及复杂的高维网络。在这篇立场论文中，我们认为这些挑战不应被视为PIML的障碍，而应是其进化的催化剂。我们提出了生物学信息机器学习(BIML)：PIML的原则性扩展，保留了其结构基础，同时适应生物学的实际现实。BIML不是取代PIML，而是重新调整其方法，使其能够在更软性、概率形式的先验知识下运行。我们概述了四个基础支柱作为这一转变的路线图：不确定性量化、上下文化、受限潜在结构推断和可扩展性。基础模型和大语言模型将成为关键推动因素，将人类专业知识与计算建模结合起来。最后，我们提出具体建议，以构建BIML生态系统，并将PIML启发的创新引向具有高度科学和社会相关性的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physics-Informed Machine Learning (PIML) has successfully integratedmechanistic understanding into machine learning, particularly in domainsgoverned by well-known physical laws. This success has motivated efforts toapply PIML to biology, a field rich in dynamical systems but shaped bydifferent constraints. Biological modeling, however, presents uniquechallenges: multi-faceted and uncertain prior knowledge, heterogeneous andnoisy data, partial observability, and complex, high-dimensional networks. Inthis position paper, we argue that these challenges should not be seen asobstacles to PIML, but as catalysts for its evolution. We proposeBiology-Informed Machine Learning (BIML): a principled extension of PIML thatretains its structural grounding while adapting to the practical realities ofbiology. Rather than replacing PIML, BIML retools its methods to operate undersofter, probabilistic forms of prior knowledge. We outline four foundationalpillars as a roadmap for this transition: uncertainty quantification,contextualization, constrained latent structure inference, and scalability.Foundation Models and Large Language Models will be key enablers, bridginghuman expertise with computational modeling. We conclude with concreterecommendations to build the BIML ecosystem and channel PIML-inspiredinnovation toward challenges of high scientific and societal relevance.</description>
      <author>example@mail.com (Julien Martinelli)</author>
      <guid isPermaLink="false">2510.25368v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine Learning Framework</title>
      <link>http://arxiv.org/abs/2510.25347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 Figures, MICCAI AMAI 2025 workshop, to be published in  Volume 16206 of the Lecture Notes in Computer Science series&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于放射组学的流程，利用伪标记生成训练标签，解决了非对比冠状动脉计算机断层血管造影(CCTA)扫描中冠状动脉钙化(CAC)评分标记数据有限的问题。&lt;h4&gt;背景&lt;/h4&gt;冠状动脉钙化(CAC)评分在冠状动脉疾病(CAD)的早期检测和风险分层中起着关键作用。非对比冠状动脉计算机断层血管造影(CCTA)扫描在临床中常用于早期钙化检测。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于放射组学的流程，利用伪标记生成训练标签，避免需要专家定义的分割，并探索预训练基础模型在特征提取中的应用。&lt;h4&gt;方法&lt;/h4&gt;提出基于放射组学的流程，利用伪标记生成训练标签；探索使用预训练基础模型(CT-FM和RadImageNet)提取图像特征并与传统分类器结合；比较深度学习特征与放射组学特征性能；在包含182名患者的临床CCTA数据集上评估，将患者分为零钙化评分组和非零钙化评分组；研究在非对比数据集与对比+非对比数据集上训练的影响。&lt;h4&gt;主要发现&lt;/h4&gt;基于放射组学的模型显著优于来自基础模型的CNN嵌入，达到84%的准确率(p&lt;0.05)，尽管没有专家标注可用。&lt;h4&gt;结论&lt;/h4&gt;基于放射组学的方法在冠状动脉钙化检测中表现出色，即使在没有专家标注的情况下也能达到高准确率。&lt;h4&gt;翻译&lt;/h4&gt;冠状动脉钙化(CAC)评分在冠状动脉疾病(CAD)的早期检测和风险分层中起着关键作用。在本研究中，我们关注非对比冠状动脉计算机断层血管造影(CCTA)扫描，这些扫描在临床中常用于早期钙化检测。为解决标记数据有限这一挑战，我们提出了一种基于放射组学的流程，利用伪标记生成训练标签，从而消除对专家定义分割的需求。此外，我们探索了使用预训练基础模型(特别是CT-FM和RadImageNet)提取图像特征，然后与传统分类器一起使用。我们将这些深度学习特征与放射组学特征的性能进行了比较。评估在包含182名患者的临床CCTA数据集上进行，个体被分为两组：零钙化评分组与非零钙化评分组。我们进一步研究了在非对比数据集与对比+非对比数据集上训练的影响，测试仅在非对比扫描上进行。结果表明，尽管没有专家标注可用，但基于放射组学的模型显著优于来自基础模型的CNN嵌入(达到84%的准确率和p&lt;0.05)。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Coronary artery calcium (CAC) scoring plays a crucial role in the earlydetection and risk stratification of coronary artery disease (CAD). In thisstudy, we focus on non-contrast coronary computed tomography angiography (CCTA)scans, which are commonly used for early calcification detection in clinicalsettings. To address the challenge of limited annotated data, we propose aradiomics-based pipeline that leverages pseudo-labeling to generate traininglabels, thereby eliminating the need for expert-defined segmentations.Additionally, we explore the use of pretrained foundation models, specificallyCT-FM and RadImageNet, to extract image features, which are then used withtraditional classifiers. We compare the performance of these deep learningfeatures with that of radiomics features. Evaluation is conducted on a clinicalCCTA dataset comprising 182 patients, where individuals are classified into twogroups: zero versus non-zero calcium scores. We further investigate the impactof training on non-contrast datasets versus combined contrast and non-contrastdatasets, with testing performed only on non contrast scans. Results show thatradiomics-based models significantly outperform CNN-derived embeddings fromfoundation models (achieving 84% accuracy and p&lt;0.05), despite theunavailability of expert annotations.</description>
      <author>example@mail.com (Ayman Abaid, Gianpiero Guidone, Sara Alsubai, Foziyah Alquahtani, Talha Iqbal, Ruth Sharif, Hesham Elzomor, Emiliano Bianchini, Naeif Almagal, Michael G. Madden, Faisal Sharif, Ihsan Ullah)</author>
      <guid isPermaLink="false">2510.25347v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.25320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了基于图的代理规划(GAP)框架，通过建模任务间依赖关系实现工具的并行和顺序执行，解决了现有顺序推理范式的效率问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型驱动的自主代理在工具操作方面显示出强大能力，但现有范式(如ReAct)依赖顺序推理和执行，无法利用独立子任务之间的内在并行性。&lt;h4&gt;目的&lt;/h4&gt;解决顺序推理瓶颈导致的工具利用效率低下和多步推理场景中表现不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;提出基于图的代理规划(GAP)框架，训练基础模型将复杂任务分解为依赖感知的子任务图，自主确定工具的并行或顺序执行方式；采用两阶段训练策略：监督微调(SFT)和强化学习(RL)。&lt;h4&gt;主要发现&lt;/h4&gt;GAP在MHQA数据集上显著优于传统ReAct基线，特别是在多步检索任务上，同时通过智能并行化实现了工具调用效率的显著提升。&lt;h4&gt;结论&lt;/h4&gt;依赖感知的任务编排在执行效率和任务准确性方面都取得了实质性改进。&lt;h4&gt;翻译&lt;/h4&gt;由大型语言模型(LLM)驱动的自主代理在工具操作方面展现出解决复杂任务的强大能力。然而，现有的ReAct等范式依赖顺序推理和执行，无法利用独立子任务之间的内在并行性。这种顺序瓶颈导致工具利用效率低下，以及在多步推理场景中表现不佳。我们引入了基于图的代理规划(GAP)，这是一个新框架，通过基于图的规划明确建模任务间依赖关系，实现自适应并行和顺序工具执行。我们的方法训练基础模型将复杂任务分解为依赖感知的子任务图，自主确定哪些工具可以并行执行，哪些必须遵循顺序依赖。这种依赖感知的编排在执行效率和任务准确性方面都取得了实质性改进。为了训练GAP，我们从多跳问答(MHQA)基准构建了高质量的基于图的规划轨迹数据集。我们采用两阶段训练策略：首先在整理的数据集上进行监督微调(SFT)，然后在基于正确性奖励函数的强化学习(RL)阶段，在战略采样的查询上进行训练。在MHQA数据集上的实验结果表明，GAP显著优于传统的ReAct基线，特别是在多步检索任务上，同时通过智能并行化实现了工具调用效率的显著提升。项目页面可在以下网址访问：https://github.com/WJQ7777/Graph-Agent-Planning。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous agents powered by large language models (LLMs) have shownimpressive capabilities in tool manipulation for complex task-solving. However,existing paradigms such as ReAct rely on sequential reasoning and execution,failing to exploit the inherent parallelism among independent sub-tasks. Thissequential bottleneck leads to inefficient tool utilization and suboptimalperformance in multi-step reasoning scenarios. We introduce Graph-based AgentPlanning (GAP), a novel framework that explicitly models inter-taskdependencies through graph-based planning to enable adaptive parallel andserial tool execution. Our approach trains agent foundation models to decomposecomplex tasks into dependency-aware sub-task graphs, autonomously determiningwhich tools can be executed in parallel and which must follow sequentialdependencies. This dependency-aware orchestration achieves substantialimprovements in both execution efficiency and task accuracy. To train GAP, weconstruct a high-quality dataset of graph-based planning traces derived fromthe Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stagetraining strategy: supervised fine-tuning (SFT) on the curated dataset,followed by reinforcement learning (RL) with a correctness-based rewardfunction on strategically sampled queries where tool-based reasoning providesmaximum value. Experimental results on MHQA datasets demonstrate that GAPsignificantly outperforms traditional ReAct baselines, particularly onmulti-step retrieval tasks, while achieving dramatic improvements in toolinvocation efficiency through intelligent parallelization. The project page isavailable at: https://github.com/WJQ7777/Graph-Agent-Planning.</description>
      <author>example@mail.com (Jiaqi Wu, Qinlao Zhao, Zefeng Chen, Kai Qin, Yifei Zhao, Xueqian Wang, Yuhang Yao)</author>
      <guid isPermaLink="false">2510.25320v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2510.25257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种成本效益高且适应性强的蒸馏框架，利用视觉基础模型(VFMs)增强轻量级目标检测器，解决了高速度推理与特征表示能力之间的矛盾。&lt;h4&gt;背景&lt;/h4&gt;实时目标检测通过精心设计的架构和优化策略取得了显著进展，但轻量级网络设计追求高速推理往往导致特征表示能力下降，阻碍了性能提升和实际设备部署。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用视觉基础模型(VFMs)能力增强轻量级目标检测器的成本效益高且适应性强的蒸馏框架，解决VFM与资源受限检测器之间架构和学习目标差异导致的语义传输挑战。&lt;h4&gt;方法&lt;/h4&gt;引入深度语义注入器(DSI)模块促进VFM高级表示与检测器深层集成；设计基于梯度的自适应调制(GAM)策略，根据梯度范数比率动态调整语义传输强度。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在不增加部署和推理开销的情况下，为各种基于DETR的模型带来显著且一致的性能提升；新模型RT-DETRv4在COCO上取得最先进结果，在273/169/124/78 FPS速度下分别达到49.7/53.5/55.4/57.0的AP分数。&lt;h4&gt;结论&lt;/h4&gt;该方法强调了其在实时检测中的实际效用，为实时目标检测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;实时目标检测通过精心设计的架构和优化策略取得了实质性进展。然而，通过轻量级网络设计追求高速推理通常会导致特征表示能力下降，这阻碍了性能的进一步改进和实际设备部署。在本文中，我们提出了一种具有成本效益且高度适应性的蒸馏框架，利用视觉基础模型(VFMs)的快速发展能力来增强轻量级目标检测器。鉴于VFM与资源受限检测器之间存在显著的架构和学习目标差异，实现稳定且任务对齐的语义传输具有挑战性。为此，一方面，我们引入了深度语义注入器(DSI)模块，促进VFM的高级表示与检测器深层层的集成；另一方面，我们设计了基于梯度的自适应调制(GAM)策略，根据梯度范数比率动态调整语义传输强度。在不增加部署和推理开销的情况下，我们的方法在各种基于DETR的模型上轻松实现了显著且一致的性能提升，凸显了其在实时检测中的实际效用。我们的新模型系列RT-DETRv4在COCO上取得了最先进的结果，在相应速度为273/169/124/78 FPS时分别达到49.7/53.5/55.4/57.0的AP分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time object detection has achieved substantial progress throughmeticulously designed architectures and optimization strategies. However, thepursuit of high-speed inference via lightweight network designs often leads todegraded feature representation, which hinders further performance improvementsand practical on-device deployment. In this paper, we propose a cost-effectiveand highly adaptable distillation framework that harnesses the rapidly evolvingcapabilities of Vision Foundation Models (VFMs) to enhance lightweight objectdetectors. Given the significant architectural and learning objectivedisparities between VFMs and resource-constrained detectors, achieving stableand task-aligned semantic transfer is challenging. To address this, on onehand, we introduce a Deep Semantic Injector (DSI) module that facilitates theintegration of high-level representations from VFMs into the deep layers of thedetector. On the other hand, we devise a Gradient-guided Adaptive Modulation(GAM) strategy, which dynamically adjusts the intensity of semantic transferbased on gradient norm ratios. Without increasing deployment and inferenceoverhead, our approach painlessly delivers striking and consistent performancegains across diverse DETR-based models, underscoring its practical utility forreal-time detection. Our new model family, RT-DETRv4, achieves state-of-the-artresults on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at correspondingspeeds of 273/169/124/78 FPS.</description>
      <author>example@mail.com (Zijun Liao, Yian Zhao, Xin Shan, Yu Yan, Chang Liu, Lei Lu, Xiangyang Ji, Jie Chen)</author>
      <guid isPermaLink="false">2510.25257v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Test-Time Adaptive Object Detection with Foundation Model</title>
      <link>http://arxiv.org/abs/2510.25175v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于基础模型的测试时自适应目标检测方法，消除了对源数据的完全需求，克服了传统封闭集限制，实现了高效的跨域和跨类别适应。&lt;h4&gt;背景&lt;/h4&gt;测试时自适应目标检测近年来受到越来越多的关注，因为它在在线领域适应方面具有独特优势，更接近实际应用场景。然而，现有方法严重依赖于源域统计特征，并假设源域和目标域共享相同的类别空间。&lt;h4&gt;目的&lt;/h4&gt;提出第一个基于基础模型的测试时自适应目标检测方法，消除对源数据的完全需求，克服传统封闭集限制，实现任意跨域和跨类别的目标数据适应。&lt;h4&gt;方法&lt;/h4&gt;设计了一个多模态提示的Mean-Teacher框架，结合文本和视觉提示调整，以参数高效的方式适应语言和视觉表示空间；提出了针对视觉提示的测试时预热策略；维护实例动态内存模块存储高质量伪标签；并提出了内存增强和内存幻觉两种新策略。&lt;h4&gt;主要发现&lt;/h4&gt;在跨损坏和跨数据集基准上的广泛实验表明，该方法持续优于之前的最先进方法，能够适应任意跨域和跨类别的目标数据。&lt;h4&gt;结论&lt;/h4&gt;基于基础模型的方法在测试时自适应目标检测方面表现出色，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;近年来，测试时自适应目标检测因其在线领域适应中的独特优势而受到越来越多的关注，这更接近实际应用场景。然而，现有方法严重依赖于源域统计特征，同时做出源域和目标域共享相同类别空间的强假设。本文提出了第一个基于基础模型的测试时自适应目标检测方法，完全消除了对源数据的需求并克服了传统封闭集限制。具体而言，我们设计了一个多模态提示的Mean-Teacher框架，用于视觉-语言检测器驱动的测试时适应，结合文本和视觉提示调整，以参数高效的方式在测试数据上适应语言和视觉表示空间。相应地，我们提出了针对视觉提示定制的测试时预热策略，以有效保持视觉分支的表示能力。此外，为保证每个测试批次的高质量伪标签，我们维护了一个存储来自先前测试样本的高质量伪标签的实例动态内存模块，并提出了两种新策略——内存增强和内存幻觉，分别利用IDM的高质量实例来增强原始预测和对没有可用伪标签的图像进行幻觉处理。在跨损坏和跨数据集基准上的广泛实验表明，我们的方法持续优于之前的最先进方法，并能适应任意跨域和跨类别的目标数据。代码可在https://github.com/gaoyingjay/ttaod_foundation获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, test-time adaptive object detection has attracted increasingattention due to its unique advantages in online domain adaptation, whichaligns more closely with real-world application scenarios. However, existingapproaches heavily rely on source-derived statistical characteristics whilemaking the strong assumption that the source and target domains share anidentical category space. In this paper, we propose the first foundationmodel-powered test-time adaptive object detection method that eliminates theneed for source data entirely and overcomes traditional closed-set limitations.Specifically, we design a Multi-modal Prompt-based Mean-Teacher framework forvision-language detector-driven test-time adaptation, which incorporates textand visual prompt tuning to adapt both language and vision representationspaces on the test data in a parameter-efficient manner. Correspondingly, wepropose a Test-time Warm-start strategy tailored for the visual prompts toeffectively preserve the representation capability of the vision branch.Furthermore, to guarantee high-quality pseudo-labels in every test batch, wemaintain an Instance Dynamic Memory (IDM) module that stores high-qualitypseudo-labels from previous test samples, and propose two novelstrategies-Memory Enhancement and Memory Hallucination-to leverage IDM'shigh-quality instances for enhancing original predictions and hallucinatingimages without available pseudo-labels, respectively. Extensive experiments oncross-corruption and cross-dataset benchmarks demonstrate that our methodconsistently outperforms previous state-of-the-art methods, and can adapt toarbitrary cross-domain and cross-category target data. Code is available athttps://github.com/gaoyingjay/ttaod_foundation.</description>
      <author>example@mail.com (Yingjie Gao, Yanan Zhang, Zhi Cai, Di Huang)</author>
      <guid isPermaLink="false">2510.25175v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>TabMGP: Martingale Posterior with TabPFN</title>
      <link>http://arxiv.org/abs/2510.25154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages (+3 reference, +22 appendix). Extra plots in  https://drive.google.com/drive/folders/1ct_effOoTEGpiWUf0_1xI3VqLWHtJY16&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于基础转换器的鞅后验方法（TabMGP），用于解决贝叶斯推断中的不确定性量化问题。&lt;h4&gt;背景&lt;/h4&gt;贝叶斯推断在不确定性量化方面具有优势，但面临先验设定、似然误设和计算负担等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的预测规则，用于构建高质量的鞅后验方法，提高不确定性量化的准确性。&lt;h4&gt;方法&lt;/h4&gt;利用基础转换器（特别是TabPFN，一种表格数据领域的最先进模型）构建鞅后验（TabMGP），通过自回归生成模拟前向数据生成过程。&lt;h4&gt;主要发现&lt;/h4&gt;TabMGP产生的可信集具有接近名义覆盖范围的性能，并且通常优于现有的MGP结构和标准贝叶斯方法。&lt;h4&gt;结论&lt;/h4&gt;基于基础转换器的鞅后验方法（TabMGP）在表格数据的不确定性量化方面表现优异，为贝叶斯推断提供了有效替代方案。&lt;h4&gt;翻译&lt;/h4&gt;贝叶斯推断提供了有原则的不确定性量化，但常常受到先验设定、似然误设和计算负担的限制。鞅后验（MGP，Fong等人，2023年）提供了一种替代方案，用预测规则（即一步前向预测分布序列）替代先验-似然设定，用于前向数据生成。MGP的有效性取决于预测规则的选择，但文献中很少有令人信服的例子。基础转换器在这里非常适合，因为它们的自回归生成模拟了这种前向模拟，并且它们的通用设计能够实现丰富的预测建模。我们介绍了TabMGP，这是一种基于TabPFN构建的MGP，TabPFN是表格数据当前最先进的基础模型。TabMGP产生的可信集具有接近名义覆盖范围的性能，并且通常优于现有的MGP结构和标准贝叶斯方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bayesian inference provides principled uncertainty quantification but isoften limited by challenges of prior elicitation, likelihood misspecification,and computational burden. The martingale posterior (MGP, Fong et al., 2023)offers an alternative, replacing prior-likelihood elicitation with a predictiverule - namely, a sequence of one-step-ahead predictive distributions - forforward data generation. The utility of MGPs depends on the choice ofpredictive rule, yet the literature has offered few compelling examples.Foundation transformers are well-suited here, as their autoregressivegeneration mirrors this forward simulation and their general-purpose designenables rich predictive modeling. We introduce TabMGP, an MGP built on TabPFN,a transformer foundation model that is currently state-of-the-art for tabulardata. TabMGP produces credible sets with near-nominal coverage and oftenoutperforms both existing MGP constructions and standard Bayes.</description>
      <author>example@mail.com (Kenyon Ng, Edwin Fong, David T. Frazier, Jeremias Knoblauch, Susan Wei)</author>
      <guid isPermaLink="false">2510.25154v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>POWSM: A Phonetic Open Whisper-Style Speech Foundation Model</title>
      <link>http://arxiv.org/abs/2510.24992v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;POWSM是一个统一的语音处理框架，能够同时执行多个音位相关任务，性能与专业模型相当，且支持多种转换功能。&lt;h4&gt;背景&lt;/h4&gt;语音处理领域在自动语音识别(ASR)、音位识别(PR)、字形到音位转换(G2P)和音位到字形转换(P2G)等音位任务上取得了显著进展，但这些任务大多是孤立研究的，每个任务都依赖于特定的架构和数据集。&lt;h4&gt;目的&lt;/h4&gt;引入POWSM（Phonetic Open Whisper-style Speech Model），创建第一个能够同时执行多个与音位相关任务的统一框架。&lt;h4&gt;方法&lt;/h4&gt;POWSM模型实现了音频、文本（字形）和音位之间的无缝转换，为通用和低资源语音处理开辟了新的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;POWSM在性能上与类似大小的专业PR模型（Wav2Vec2Phoneme和ZIPA）相当或更优，同时支持G2P、P2G和ASR。&lt;h4&gt;结论&lt;/h4&gt;训练数据、代码和模型已公开发布，以促进开放科学。&lt;h4&gt;翻译&lt;/h4&gt;最近语音处理方面的进展已在语音识别、音位识别、字形到音位转换和音位到字形转换等音位任务中取得实质性进展。尽管这些任务在概念上相似，但它们大多被孤立研究，每个任务都依赖于特定的架构和数据集。在本文中，我们引入了POWSM（Phonetic Open Whisper-style Speech Model），这是第一个能够同时执行多个音位相关任务的统一框架。POWSM实现了音频、文本（字形）和音位之间的无缝转换，为通用和低资源语音处理开辟了新的可能性。我们的模型在性能上与类似大小的专业PR模型（Wav2Vec2Phoneme和ZIPA）相当或更优，同时支持G2P、P2G和ASR。我们的训练数据、代码和模型已公开发布，以促进开放科学。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in spoken language processing have led to substantialprogress in phonetic tasks such as automatic speech recognition (ASR), phonerecognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-graphemeconversion (P2G). Despite their conceptual similarity, these tasks have largelybeen studied in isolation, each relying on task-specific architectures anddatasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style SpeechModel), the first unified framework capable of jointly performing multiplephone-related tasks. POWSM enables seamless conversion between audio, text(graphemes), and phones, opening up new possibilities for universal andlow-resource speech processing. Our model outperforms or matches specialized PRmodels of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P,P2G, and ASR. Our training data, code and models are released to foster openscience.</description>
      <author>example@mail.com (Chin-Jou Li, Kalvin Chang, Shikhar Bharadwaj, Eunjung Yeo, Kwanghee Choi, Jian Zhu, David Mortensen, Shinji Watanabe)</author>
      <guid isPermaLink="false">2510.24992v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Pearl: A Foundation Model for Placing Every Atom in the Right Location</title>
      <link>http://arxiv.org/abs/2510.24670v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Pearl是一个用于蛋白质-配体共同折叠的基础模型，通过创新的训练方法、架构设计和推理控制，显著提高了蛋白质-配体复合物结构预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确预测蛋白质-配体复合物的三维结构是计算药物发现中的基本挑战，限制了治疗设计的速度和成功。深度学习方法虽有潜力，但受限于实验数据稀缺、架构效率低下、物理无效构象以及辅助信息利用能力有限等因素。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够克服当前蛋白质-配体结构预测局限性的基础模型，提高预测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;Pearl通过三个关键创新解决挑战：(1)包含大规模合成数据的训练配方，克服数据稀缺；(2)融合SO(3)-等变扩散模块的架构，尊重3D旋转对称性，提高泛化能力；(3)可控推理系统，支持蛋白质和非聚合物组分以及无条件/条件模式。&lt;h4&gt;主要发现&lt;/h4&gt;Pearl在蛋白质-配体共同折叠方面建立了新性能标准，在公共基准测试中超越AlphaFold3和其他开源模型14%以上；在口袋条件共同折叠模式下，对真实世界药物目标实现了3.6倍的改进；模型性能与训练中使用的合成数据集大小直接相关。&lt;h4&gt;结论&lt;/h4&gt;Pearl通过创新的训练方法、架构设计和推理控制，显著提高了蛋白质-配体复合物结构预测的准确性，为药物设计提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;准确预测蛋白质-配体复合物的三维结构仍然是计算药物发现中的一个基本挑战，它限制了治疗设计的速度和成功率。深度学习方法最近作为结构预测工具显示出强大的潜力，在各种生物分子系统中取得了有希望的准确性。然而，它们的性能和实用性受到实验数据稀缺、架构效率低下、物理无效构象以及在推理过程中利用辅助信息能力有限等因素的制约。为解决这些问题，我们引入了Pearl（Placing Every Atom in the Right Location），一个用于大规模蛋白质-配体共同折叠的基础模型。Pearl通过三个关键创新来解决这些挑战：(1)包含大规模合成数据的训练配方，以克服数据稀缺；(2)融合SO(3)-等变扩散模块的架构， inherently尊重3D旋转对称性，提高泛化能力和样本效率；(3)可控推理，包括支持蛋白质和非聚合物组分以及无条件/条件模式的双链通用模板系统。Pearl在蛋白质-配体共同折叠方面建立了新的最先进性能。在生成准确（RMSD &lt; 2Å）和物理有效构象的关键指标上，Pearl在公共Runs N' Poses和PoseBusters基准测试中超越了AlphaFold3和其他开源基线，比次优模型分别提高了14.5%和14.2%。在口袋条件共同折叠模式下，Pearl在一个具有挑战性的真实世界药物目标专有集上，在更严格的RMSD &lt; 1Å阈值下实现了3.6倍的改进。最后，我们证明了模型性能与训练中使用的合成数据集大小直接相关。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting the three-dimensional structures of protein-ligandcomplexes remains a fundamental challenge in computational drug discovery thatlimits the pace and success of therapeutic design. Deep learning methods haverecently shown strong potential as structural prediction tools, achievingpromising accuracy across diverse biomolecular systems. However, theirperformance and utility are constrained by scarce experimental data,inefficient architectures, physically invalid poses, and the limited ability toexploit auxiliary information available at inference. To address these issues,we introduce Pearl (Placing Every Atom in the Right Location), a foundationmodel for protein-ligand cofolding at scale. Pearl addresses these challengeswith three key innovations: (1) training recipes that include large-scalesynthetic data to overcome data scarcity; (2) architectures that incorporate anSO(3)-equivariant diffusion module to inherently respect 3D rotationalsymmetries, improving generalization and sample efficiency, and (3)controllable inference, including a generalized multi-chain templating systemsupporting both protein and non-polymeric components as well as dualunconditional/conditional modes. Pearl establishes a new state-of-the-artperformance in protein-ligand cofolding. On the key metric of generatingaccurate (RMSD &lt; 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold3 and other open source baselines on the public Runs N' Poses and PoseBustersbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over thenext best model. In the pocket-conditional cofolding regime, Pearl delivers$3.6\times$ improvement on a proprietary set of challenging, real-world drugtargets at the more rigorous RMSD &lt; 1 \r{A} threshold. Finally, we demonstratethat model performance correlates directly with synthetic dataset size used intraining.</description>
      <author>example@mail.com (Genesis Research Team, Alejandro Dobles, Nina Jovic, Kenneth Leidal, Pranav Murugan, David C. Williams, Drausin Wulsin, Nate Gruver, Christina X. Ji, Korrawat Pruegsanusak, Gianluca Scarpellini, Ansh Sharma, Wojciech Swiderski, Andrea Bootsma, Richard Strong Bowen, Charlotte Chen, Jamin Chen, Marc André Dämgen, Benjamin DiFrancesco, J. D. Fishman, Alla Ivanova, Zach Kagin, David Li-Bland, Zuli Liu, Igor Morozov, Jeffrey Ouyang-Zhang, Frank C. Pickard IV, Kushal S. Shah, Ben Shor, Gabriel Monteiro da Silva, Roy Tal, Maxx Tessmer, Carl Tilbury, Cyr Vetcher, Daniel Zeng, Maruan Al-Shedivat, Aleksandra Faust, Evan N. Feinberg, Michael V. LeVine, Matteus Pan)</author>
      <guid isPermaLink="false">2510.24670v2</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering</title>
      <link>http://arxiv.org/abs/2510.24799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 5 figures, submitted to ACM Transactions on Software  Engineering and Methodology&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Compiler.next，一种基于搜索的新型编译器，旨在解决AI辅助软件工程中的认知过载、工具集成效率低和AI副驾驶功能有限等问题，实现AI原生软件系统的无缝演进。&lt;h4&gt;背景&lt;/h4&gt;AI辅助软件工程快速发展，但现有工具和范式受认知过载、工具集成效率低下和AI副驾驶功能有限等因素的限制。&lt;h4&gt;目的&lt;/h4&gt;提出Compiler.next作为软件工程3.0时代的一部分，实现AI原生软件系统的无缝演进，降低非专家技术门槛，实现可扩展、适应性强和可靠的AI驱动软件。&lt;h4&gt;方法&lt;/h4&gt;Compiler.next接受人类编写的意图，通过搜索最优解决方案自动生成工作软件，涉及认知架构及其组成部分的动态优化，在准确性、成本和延迟等多个目标间找到最佳平衡。&lt;h4&gt;主要发现&lt;/h4&gt;Compiler.next的架构设计使其能够作为降低非专家技术门槛、实现可扩展、适应性强和可靠的AI驱动软件的基石。&lt;h4&gt;结论&lt;/h4&gt;Compiler.next为完全自动化、搜索驱动的软件开发奠定了基础，促进了更快创新和更高效的AI驱动系统，解决了意图编译的核心挑战。&lt;h4&gt;翻译&lt;/h4&gt;AI辅助软件工程的快速发展为软件工程领域带来了变革潜力，但现有工具和范式仍然受到认知过载、工具集成效率低下以及AI副驾驶功能有限等因素的限制。为此，我们提出了Compiler.next，一种基于搜索的新型编译器，作为新兴软件工程3.0时代的一部分，旨在实现AI原生软件系统的无缝演进。与传统的静态编译器不同，Compiler.next接受人类编写的意图，并通过搜索最优解决方案来自动生成工作软件。这一过程涉及认知架构及其组成部分（如提示、基础模型配置和系统参数）的动态优化，同时找到准确性、成本和延迟等多个目标之间的最佳权衡。本文概述了Compiler.next的架构，并将其定位为降低非专家技术门槛、实现可扩展、适应性强和可靠的AI驱动软件的基石。我们提出了一个路线图来解决意图编译中的核心挑战，包括开发高质量编程构造、有效搜索启发式方法、可复现性以及编译器之间的互操作性。我们的愿景为完全自动化、搜索驱动的软件开发奠定了基础，促进了更快创新和更高效的AI驱动系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement of AI-assisted software engineering has broughttransformative potential to the field of software engineering, but existingtools and paradigms remain limited by cognitive overload, inefficient toolintegration, and the narrow capabilities of AI copilots. In response, wepropose Compiler.next, a novel search-based compiler designed to enable theseamless evolution of AI-native software systems as part of the emergingSoftware Engineering 3.0 era. Unlike traditional static compilers,Compiler.next takes human-written intents and automatically generates workingsoftware by searching for an optimal solution. This process involves dynamicoptimization of cognitive architectures and their constituents (e.g., prompts,foundation model configurations, and system parameters) while finding theoptimal trade-off between several objectives, such as accuracy, cost, andlatency. This paper outlines the architecture of Compiler.next and positions itas a cornerstone in democratizing software development by lowering thetechnical barrier for non-experts, enabling scalable, adaptable, and reliableAI-powered software. We present a roadmap to address the core challenges inintent compilation, including developing quality programming constructs,effective search heuristics, reproducibility, and interoperability betweencompilers. Our vision lays the groundwork for fully automated, search-drivensoftware development, fostering faster innovation and more efficient AI-drivensystems.</description>
      <author>example@mail.com (Filipe R. Cogo, Gustavo A. Oliva, Ahmed E. Hassan)</author>
      <guid isPermaLink="false">2510.24799v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Why Foundation Models in Pathology Are Failing</title>
      <link>http://arxiv.org/abs/2510.23807v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基础模型在计算病理学应用中的不足，指出其存在诊断准确率低、鲁棒性差等问题，并分析了这些问题背后的七个相互关联原因，认为当前病理学基础模型在概念上与组织形态学本质不匹配，需要范式重构。&lt;h4&gt;背景&lt;/h4&gt;在非医疗领域，基础模型通过大规模自监督和多模态学习彻底改变了计算机视觉和语言处理，人们预期其在计算病理学领域也能取得类似突破。&lt;h4&gt;目的&lt;/h4&gt;检查基础模型在计算病理学应用中的不足，分析这些缺点背后的根本原因。&lt;h4&gt;方法&lt;/h4&gt;通过系统评估方法检查基础模型的缺点，并分析这些缺点背后的原因。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型存在诊断准确率低、鲁棒性差、几何不稳定性、计算需求量大以及安全漏洞等问题；这些问题源于七个相互关联的原因：生物复杂性、无效的自监督、过度泛化、过度的架构复杂性、缺乏领域特定创新、数据不足以及与组织块大小相关的基本设计缺陷。&lt;h4&gt;结论&lt;/h4&gt;当前病理学基础模型在概念上与组织形态学本质不匹配，需要对范式本身进行根本性的重新思考。&lt;h4&gt;翻译&lt;/h4&gt;在非医疗领域，基础模型通过大规模自监督和多模态学习彻底改变了计算机视觉和语言处理。因此，计算病理学领域对这类模型的快速应用预期能在癌症诊断、预后判断和多模态检索方面带来类似的突破。然而，最近的系统评估揭示了根本性弱点：低诊断准确率、差鲁棒性、几何不稳定性、高计算需求以及令人担忧的安全漏洞。这篇短文检查了这些不足，并认为它们源于主流人工智能中通用基础建模的基本假设与人体组织内在复杂性之间的更深层次概念不匹配。确定了七个相互关联的原因：生物复杂性、无效的自监督、过度泛化、过度的架构复杂性、缺乏领域特定创新、数据不足以及与组织块大小相关的基本设计缺陷。这些发现表明，当前的病理学基础模型在概念上仍与组织形态学的本质不匹配，需要对范式本身进行根本性的重新思考。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In non-medical domains, foundation models (FMs) have revolutionized computervision and language processing through large-scale self-supervised andmultimodal learning. Consequently, their rapid adoption in computationalpathology was expected to deliver comparable breakthroughs in cancer diagnosis,prognostication, and multimodal retrieval. However, recent systematicevaluations reveal fundamental weaknesses: low diagnostic accuracy, poorrobustness, geometric instability, heavy computational demands, and concerningsafety vulnerabilities. This short paper examines these shortcomings and arguesthat they stem from deeper conceptual mismatches between the assumptionsunderlying generic foundation modeling in mainstream AI and the intrinsiccomplexity of human tissue. Seven interrelated causes are identified:biological complexity, ineffective self-supervision, overgeneralization,excessive architectural complexity, lack of domain-specific innovation,insufficient data, and a fundamental design flaw related to tissue patch size.These findings suggest that current pathology foundation models remainconceptually misaligned with the nature of tissue morphology and call for afundamental rethinking of the paradigm itself.</description>
      <author>example@mail.com (Hamid R. Tizhoosh)</author>
      <guid isPermaLink="false">2510.23807v2</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans</title>
      <link>http://arxiv.org/abs/2510.14205v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出动态人格完善框架(DPRF)，用于提高大型语言模型角色扮演代理的行为与目标个体的一致性，通过迭代识别认知分歧并优化个人资料实现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型角色扮演代理旨在模拟个体人类行为，但手动创建的个人资料(如精心挑选的信息和个性特征)未经验证与目标个体的对齐度，导致人格保真度受损。&lt;h4&gt;目的&lt;/h4&gt;优化LLM RPAs的行为与目标个体行为的一致性，提高角色扮演的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;DPRF通过迭代识别生成行为与人类真实认知之间的分歧(无论是自由形式还是基于理论的结构化分析)，并完善个人资料以减轻这些分歧。&lt;h4&gt;主要发现&lt;/h4&gt;在五个大型语言模型和四种多样的行为预测场景(正式辩论、涉及心理健康问题的社交媒体帖子、公开采访和电影评论)中，DPRF能够显著提高行为一致性，并且跨模型和场景具有通用性。&lt;h4&gt;结论&lt;/h4&gt;该工作为创建高保真度个人资料和增强下游应用(如用户模拟、社会研究和个性化AI)的有效性提供了稳健的方法论。&lt;h4&gt;翻译&lt;/h4&gt;新兴的大型语言模型角色扮演代理旨在模拟个体人类行为，但人格保真度常因手动创建的个人资料(如精心挑选的信息和个性特征)未经验证与目标个体的对齐度而受损。为解决这一局限，我们的工作引入了动态人格完善框架(DPRF)。DPRF旨在通过迭代识别生成行为与人类真实认知之间的分歧(无论是自由形式还是基于理论的结构化分析)，并完善个人资料以减轻这些分歧，从而优化LLM RPAs的行为与目标个体行为的一致性。我们在五个大型语言模型和四种多样的行为预测场景(正式辩论、涉及心理健康问题的社交媒体帖子、公开采访和电影评论)中评估了DPRF。DPRF能够显著提高行为一致性，并且跨模型和场景具有通用性。我们的工作为创建高保真度个人资料和增强下游应用(如用户模拟、社会研究和个性化AI)的有效性提供了稳健的方法论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emerging large language model role-playing agents (LLM RPAs) aim tosimulate individual human behaviors, but the persona fidelity is oftenundermined by manually-created profiles (e.g., cherry-picked information andpersonality characteristics) without validating the alignment with the targetindividuals. To address this limitation, our work introduces the DynamicPersona Refinement Framework (DPRF). DPRF aims to optimize the alignment of LLMRPAs' behaviors with those of target individuals by iteratively identifying thecognitive divergence, either through free-form or theory-grounded, structuredanalysis, between generated behaviors and human ground truth, and refining thepersona profile to mitigate these divergences. We evaluate DPRF with five LLMson four diverse behavior-prediction scenarios: formal debates, social mediaposts with mental health issues, public interviews, and movie reviews. DPRF canconsistently improve behavioral alignment considerably over baseline personasand generalizes across models and scenarios. Our work provides a robustmethodology for creating high-fidelity persona profiles and enhancing thevalidity of downstream applications, such as user simulation, social studies,and personalized AI.</description>
      <author>example@mail.com (Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang)</author>
      <guid isPermaLink="false">2510.14205v3</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics</title>
      <link>http://arxiv.org/abs/2510.25683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为GNSS的图神经网络框架，用于动态结构问题的代理建模，通过三个关键特征解决了现有方法的局限性，在案例研究中表现出色，实现了比传统方法更快的推理速度。&lt;h4&gt;背景&lt;/h4&gt;图神经网络作为数值模拟的代理模型已在计算流体动力学领域有所研究，但在结构问题特别是动态情况中的应用相对较少，存在研究空白。&lt;h4&gt;目的&lt;/h4&gt;为了填补动态结构问题中图神经网络应用的空白，作者开发了GNSS框架，专门用于动态结构问题的代理建模。&lt;h4&gt;方法&lt;/h4&gt;GNSS遵循编码-处理-解码范式，具有三个关键特征：在节点固定的局部框架中表达节点运动学；采用符号感知回归损失减少相位误差；使用波长感知的连接半径优化图结构构建。&lt;h4&gt;主要发现&lt;/h4&gt;GNSS在50kHz汉宁调制脉冲激励梁的案例研究中，能够准确复现物理特性并泛化到未见过的加载条件，而现有GNN方法无法收敛。与有限元方法相比，GNSS实现了显著的推理加速同时保持空间和时间保真度。&lt;h4&gt;结论&lt;/h4&gt;具有物理一致性更新规则且保持局部性的图神经网络是动态、波主导结构模拟的有竞争力的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络最近被探索作为数值模拟的代理模型。虽然它们在计算流体动力学中的应用已被研究，但很少被应用于结构问题，特别是动态情况。为了解决这一研究空白，我们引入了基于图网络的结构模拟器，这是一个用于动态结构问题代理建模的图神经网络框架。GNSS遵循基于GNN的机器学习模型的典型编码-处理-解码范式，其设计使其特别适合动态模拟，这得益于三个关键特征：在节点固定的局部框架中表达节点运动学，避免有限差分速度中的灾难性取消；采用符号感知回归损失，减少长期rollout中的相位误差；使用波长感知的连接半径，优化图结构构建。我们在一个涉及由50kHz汉宁调制脉冲激励的梁的案例研究中评估了GNSS。结果表明GNSS能够在数百个时间步长内准确复现问题的物理特性，并能泛化到未见过的加载条件，而现有的GNN方法无法收敛或提供有意义的预测。与显式有限元基线方法相比，GNSS在保持空间和时间保真度的同时实现了显著的推理加速。这些发现表明，具有物理一致性更新规则且保持局部性的GNN是动态、波主导结构模拟的有竞争力的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have recently been explored as surrogate modelsfor numerical simulations. While their applications in computational fluiddynamics have been investigated, little attention has been given to structuralproblems, especially for dynamic cases. To address this gap, we introduce theGraph Network-based Structural Simulator (GNSS), a GNN framework for surrogatemodeling of dynamic structural problems.  GNSS follows the encode-process-decode paradigm typical of GNN-based machinelearning models, and its design makes it particularly suited for dynamicsimulations thanks to three key features: (i) expressing node kinematics innode-fixed local frames, which avoids catastrophic cancellation infinite-difference velocities; (ii) employing a sign-aware regression loss,which reduces phase errors in long rollouts; and (iii) using awavelength-informed connectivity radius, which optimizes graph construction.  We evaluate GNSS on a case study involving a beam excited by a 50kHzHanning-modulated pulse. The results show that GNSS accurately reproduces thephysics of the problem over hundreds of timesteps and generalizes to unseenloading conditions, where existing GNNs fail to converge or deliver meaningfulpredictions.  Compared with explicit finite element baselines, GNSS achieves substantialinference speedups while preserving spatial and temporal fidelity. Thesefindings demonstrate that locality-preserving GNNs with physics-consistentupdate rules are a competitive alternative for dynamic, wave-dominatedstructural simulations.</description>
      <author>example@mail.com (Alessandro Lucchetti, Francesco Cadini, Marco Giglio, Luca Lomazzi)</author>
      <guid isPermaLink="false">2510.25683v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Bridging the Divide: End-to-End Sequence-Graph Learning</title>
      <link>http://arxiv.org/abs/2510.25126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BRIDGE是一种统一的端到端架构，能够联合学习序列和图信息，在友谊预测和欺诈检测任务上表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;现实世界的数据集通常是序列性和关系性的，每个节点携带事件序列，边编码交互。现有方法往往只考虑一种模态而忽略另一种。&lt;h4&gt;目的&lt;/h4&gt;作者认为序列和图是同一数据集的互补方面，应该联合学习，而不是作为独立问题处理。&lt;h4&gt;方法&lt;/h4&gt;BRIDGE将序列编码器与图神经网络(GNN)耦合在单一目标下，允许梯度在两个模块间流动。添加了TOKENXATTN标记级交叉注意力层，实现邻居间细粒度的标记级消息传递。&lt;h4&gt;主要发现&lt;/h4&gt;在友谊预测(Brightkite)和欺诈检测(Amazon)两种场景下，BRIDGE在排序和分类指标上始终优于静态GNN、时图方法和仅基于序列的基线。&lt;h4&gt;结论&lt;/h4&gt;BRIDGE通过联合学习序列和图信息，能够学习任务对齐的表示，在各种任务上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;许多现实世界的数据集既是序列性的又是关系性的：每个节点携带事件序列，而边则编码交互。现有的序列建模和图建模方法通常忽略了一种或另一种模态。我们认为序列和图不是独立的问题，而是同一数据集的互补方面，应该联合学习。我们引入了BRIDGE，一个统一的端到端架构，将序列编码器与GNN在单一目标下耦合，允许梯度在两个模块间流动，并学习任务对齐的表示。为了实现邻居间细粒度的标记级消息传递，我们添加了TOKENXATTN，一个标记级交叉注意力层，用于在相邻序列的事件之间传递消息。在友谊预测(Brightkite)和欺诈检测(Amazon)两种设置下，BRIDGE在排序和分类指标上始终优于静态GNN、时图方法和仅基于序列的基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many real-world datasets are both sequential and relational: each nodecarries an event sequence while edges encode interactions. Existing methods insequence modeling and graph modeling often neglect one modality or the other.We argue that sequences and graphs are not separate problems but complementaryfacets of the same dataset, and should be learned jointly. We introduce BRIDGE,a unified end-to-end architecture that couples a sequence encoder with a GNNunder a single objective, allowing gradients to flow across both modules andlearning task-aligned representations. To enable fine-grained token-levelmessage passing among neighbors, we add TOKENXATTN, a token-levelcross-attention layer that passes messages between events in neighboringsequences. Across two settings, friendship prediction (Brightkite) and frauddetection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graphmethods, and sequence-only baselines on ranking and classification metrics.</description>
      <author>example@mail.com (Yuen Chen, Yulun Wu, Samuel Sharpe, Igor Melnyk, Nam H. Nguyen, Furong Huang, C. Bayan Bruss, Rizal Fathony)</author>
      <guid isPermaLink="false">2510.25126v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>The Underappreciated Power of Vision Models for Graph Structural Understanding</title>
      <link>http://arxiv.org/abs/2510.24788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了图神经网络与人类视觉感知的差异，发现视觉模型在图理解任务中具有与GNNs相当的性能但展现出不同的学习模式。作者提出了新的基准测试GraphAbstract，评估模型对全局图属性的理解能力，结果表明视觉模型在需要整体结构理解的任务上优于GNNs。&lt;h4&gt;背景&lt;/h4&gt;图神经网络通过自下而上的消息传递机制工作，这与人类视觉感知先捕捉全局结构的直觉方式有根本不同。现有基准测试将领域特征与拓扑理解混为一谈，无法有效评估模型对图全局结构的理解能力。&lt;h4&gt;目的&lt;/h4&gt;研究视觉模型在图理解方面的潜力，评估它们与GNNs在性能和学习模式上的差异，并开发新的基准测试来评估模型对全局图属性的理解能力。&lt;h4&gt;方法&lt;/h4&gt;提出了名为GraphAbstract的新基准测试，评估模型识别组织原型、检测对称性、感知连接强度和识别关键元素的能力，这些能力与人类对图的全局理解方式相似。&lt;h4&gt;主要发现&lt;/h4&gt;视觉模型在需要整体结构理解的任务上显著优于GNNs；视觉模型在不同图规模上保持泛化能力；GNNs在全局模式抽象方面存在困难，且随着图规模增大性能下降；视觉模型具有显著的但未被充分利用的图结构理解能力。&lt;h4&gt;结论&lt;/h4&gt;视觉模型在需要全局拓扑意识和尺度不变推理的问题上具有显著的能力，这些发现为开发更有效的图基础模型开辟了新途径，特别是对于那些由整体模式识别主导的任务。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络通过自下而上的消息传递运行，这与人类视觉感知有根本不同，人类视觉直觉上首先捕捉全局结构。我们研究了视觉模型在图理解方面的未被充分认识到的潜力，发现它们在既定基准上实现了与GNNs相当的性能，同时表现出明显不同的学习模式。这些不同的行为，加上现有基准将领域特征与拓扑理解混为一谈的限制，促使我们引入GraphAbstract。这个基准评估模型像人类一样感知全局图属性的能力：识别组织原型、检测对称性、感知连接强度和识别关键元素。我们的结果显示，在需要整体结构理解的任务上，视觉模型显著优于GNNs，并且在不同图规模上保持泛化能力，而GNNs在全局模式抽象方面存在困难，且随着图规模增大性能下降。这项工作表明，视觉模型具有显著的但未被充分利用的图结构理解能力，特别是对于需要全局拓扑意识和尺度不变推理的问题。这些发现为利用这种未被充分认识到的潜力开发更有效的图基础模型开辟了新途径，这些任务主要由整体模式识别主导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks operate through bottom-up message-passing,fundamentally differing from human visual perception, which intuitivelycaptures global structures first. We investigate the underappreciated potentialof vision models for graph understanding, finding they achieve performancecomparable to GNNs on established benchmarks while exhibiting distinctlydifferent learning patterns. These divergent behaviors, combined withlimitations of existing benchmarks that conflate domain features withtopological understanding, motivate our introduction of GraphAbstract. Thisbenchmark evaluates models' ability to perceive global graph properties ashumans do: recognizing organizational archetypes, detecting symmetry, sensingconnectivity strength, and identifying critical elements. Our results revealthat vision models significantly outperform GNNs on tasks requiring holisticstructural understanding and maintain generalizability across varying graphscales, while GNNs struggle with global pattern abstraction and degrade withincreasing graph size. This work demonstrates that vision models possessremarkable yet underutilized capabilities for graph structural understanding,particularly for problems requiring global topological awareness andscale-invariant reasoning. These findings open new avenues to leverage thisunderappreciated potential for developing more effective graph foundationmodels for tasks dominated by holistic pattern recognition.</description>
      <author>example@mail.com (Xinjian Zhao, Wei Pang, Zhongkai Xue, Xiangru Jian, Lei Zhang, Yaoyao Xu, Xiaozhuang Song, Shu Wu, Tianshu Yu)</author>
      <guid isPermaLink="false">2510.24788v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>FastJAM: a Fast Joint Alignment Model for Images</title>
      <link>http://arxiv.org/abs/2510.22842v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025. Pages 1-10 are the Main Paper. Pages 23-31  are Supplemental Material. FastJAM website -  https://bgu-cs-vil.github.io/FastJAM/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为FastJAM的快速图像联合对齐方法，显著降低计算复杂度，实现高质量对齐效果。&lt;h4&gt;背景&lt;/h4&gt;现有图像联合对齐方法通常需要长时间训练、大容量模型和大量超参数调整，计算效率低下。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、高效的图像联合对齐方法，减少计算时间并提高对齐质量。&lt;h4&gt;方法&lt;/h4&gt;FastJAM基于图方法，利用现成图像匹配器计算pairwise匹配，结合快速非参数聚类构建表示图像内和图像间关键点关系的图。通过图神经网络传播和聚合对应关系，利用图像级池化预测单应性参数。采用逆组合损失消除正则化项需求，避免超参数调整。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，FastJAM在多个基准测试上实现了优于现有现代JA方法的对齐质量，同时将计算时间从小时或分钟级减少到几秒钟。&lt;h4&gt;结论&lt;/h4&gt;FastJAM通过创新的图神经网络和逆组合损失方法，实现了快速、高效的图像联合对齐，为图像处理领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图像联合对齐（JA）旨在将一组图像对齐到统一的坐标系中，使语义相似的特征出现在对应的空间位置。大多数现有方法通常需要长时间训练、大容量模型和大量超参数调整。我们引入了FastJAM，一种快速的基于图的方法，显著降低了联合对齐任务的计算复杂度。FastJAM利用现成的图像匹配器计算的pairwise匹配，结合快速非参数聚类，构建表示图像内和图像间关键点关系的图。图神经网络传播和聚合这些对应关系，通过图像级池化有效预测每个图像的单应性参数。利用逆组合损失，消除了对预测变换的正则化项的需求（因此也避免了与这些项相关的超参数调整），FastJAM能够快速有效地执行图像JA。在几个基准测试上的实验结果表明，FastJAM在对齐质量方面优于现有的现代JA方法，同时将计算时间从小时或分钟减少到几秒钟。我们的代码可在项目网页获取：https://bgu-cs-vil.github.io/FastJAM/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Joint Alignment (JA) of images aims to align a collection of images into aunified coordinate frame, such that semantically-similar features appear atcorresponding spatial locations. Most existing approaches often require longtraining times, large-capacity models, and extensive hyperparameter tuning. Weintroduce FastJAM, a rapid, graph-based method that drastically reduces thecomputational complexity of joint alignment tasks. FastJAM leverages pairwisematches computed by an off-the-shelf image matcher, together with a rapidnonparametric clustering, to construct a graph representing intra- andinter-image keypoint relations. A graph neural network propagates andaggregates these correspondences, efficiently predicting per-image homographyparameters via image-level pooling. Utilizing an inverse-compositional loss,that eliminates the need for a regularization term over the predictedtransformations (and thus also obviates the hyperparameter tuning associatedwith such terms), FastJAM performs image JA quickly and effectively.Experimental results on several benchmarks demonstrate that FastJAM achievesresults better than existing modern JA methods in terms of alignment quality,while reducing computation time from hours or minutes to mere seconds. Our codeis available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/</description>
      <author>example@mail.com (Omri Hirsch, Ron Shapira Weber, Shira Ifergane, Oren Freifeld)</author>
      <guid isPermaLink="false">2510.22842v2</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>JiuTian Chuanliu: A Large Spatiotemporal Model for General-purpose Dynamic Urban Sensing</title>
      <link>http://arxiv.org/abs/2510.23662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为通用动态人类移动性嵌入(GDHME)的框架，用于处理大规模人类移动性数据，发现移动行为背后的潜在语义，并支持各种城市感知任务。&lt;h4&gt;背景&lt;/h4&gt;人类移动性作为城市感知的窗口，包含丰富的时空信息，反映了居民行为偏好和城市区域功能。现有方法通常从特定角度处理特定任务，导致对人类移动性建模不足，所学知识在下游应用中适用性有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，通过时空模型处理大量人类移动性数据，发现潜在语义，支持多种城市感知任务。&lt;h4&gt;方法&lt;/h4&gt;GDHME框架遵循自监督学习思想，包含两个阶段：第一阶段将人和区域视为动态图中的节点，统一为人-区域-时间交互，使用连续时间编码器计算演化节点表示，并设计自回归自监督任务引导学习；第二阶段利用这些表示支持各种任务。通过基站系统收集大规模移动数据，并构建多任务城市感知基准进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;离线实验证明GDHME能从大量数据中自动学习有价值的节点特征。该框架已成功部署九天川流大模型，该系统在2023年中国移动全球合作伙伴大会上展示。&lt;h4&gt;结论&lt;/h4&gt;GDHME框架能有效处理人类移动性数据，提取有价值特征，支持多种城市感知任务，具有广泛的适用性和实用价值。&lt;h4&gt;翻译&lt;/h4&gt;作为城市感知的窗口，人类移动性包含丰富的时空信息，反映了居民的行为偏好和城市区域的功能。人类移动性分析吸引了众多研究者的关注。然而，现有方法通常从特定角度处理特定任务，导致对人类移动性建模不足，所学知识在各种下游应用中适用性有限。为解决这些挑战，本文提出将大量人类移动性数据输入时空模型，发现移动行为背后的潜在语义，并支持各种城市感知任务。具体来说，通过无处不在的基站系统收集大规模、广泛覆盖的人类移动性数据，并引入了一个名为通用动态人类移动性嵌入(GDHME)的城市感知框架。该框架遵循自监督学习思想，包含两个主要阶段。第一阶段，GDHME将人和区域视为动态图中的节点，将人类移动性数据统一为人-区域-时间交互。在连续时间运行的编码器动态计算演化的节点表示，捕捉人和区域的动态状态。此外，专门设计了自回归自监督任务来引导通用节点嵌入的学习。第二阶段，利用这些表示来支持各种任务。为评估GDHME框架的有效性，作者构建了一个多任务城市感知基准。离线实验证明了GDHME能够从大量数据中自动学习有价值的节点特征。此外，该框架被用于部署九天川流大模型，该系统已在2023年中国移动全球合作伙伴大会上展示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a window for urban sensing, human mobility contains rich spatiotemporalinformation that reflects both residents' behavior preferences and thefunctions of urban areas. The analysis of human mobility has attracted theattention of many researchers. However, existing methods often address specifictasks from a particular perspective, leading to insufficient modeling of humanmobility and limited applicability of the learned knowledge in variousdownstream applications. To address these challenges, this paper proposes topush massive amounts of human mobility data into a spatiotemporal model,discover latent semantics behind mobility behavior and support various urbansensing tasks. Specifically, a large-scale and widely covering human mobilitydata is collected through the ubiquitous base station system and a frameworknamed General-purpose and Dynamic Human Mobility Embedding (GDHME) for urbansensing is introduced. The framework follows the self-supervised learning ideaand contains two major stages. In stage 1, GDHME treats people and regions asnodes within a dynamic graph, unifying human mobility data aspeople-region-time interactions. An encoder operating in continuous-timedynamically computes evolving node representations, capturing dynamic statesfor both people and regions. Moreover, an autoregressive self-supervised taskis specially designed to guide the learning of the general-purpose nodeembeddings. In stage 2, these representations are utilized to support varioustasks. To evaluate the effectiveness of our GDHME framework, we furtherconstruct a multi-task urban sensing benchmark. Offline experiments demonstrateGDHME's ability to automatically learn valuable node features from vast amountsof data. Furthermore, our framework is used to deploy the JiuTian ChuanLiu BigModel, a system that has been presented at the 2023 China Mobile WorldwidePartner Conference.</description>
      <author>example@mail.com (Liangzhe Han, Leilei Sun, Tongyu Zhu, Tao Tao, Jibin Wang, Weifeng Lv)</author>
      <guid isPermaLink="false">2510.23662v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
  <item>
      <title>Pearl: A Foundation Model for Placing Every Atom in the Right Location</title>
      <link>http://arxiv.org/abs/2510.24670v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了Pearl，一种用于蛋白质-配体协同折叠的基础模型，通过三个关键创新解决了深度学习方法在结构预测中的局限性，实现了最先进的性能表现。&lt;h4&gt;背景&lt;/h4&gt;准确预测蛋白质-配体复合物的三维结构是计算药物发现中的基本挑战，限制了治疗设计的速度和成功率。虽然深度学习方法显示出潜力，但其性能受限于实验数据稀少、架构效率低下、物理无效构象以及无法充分利用可用辅助信息等因素。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服数据稀缺、提高架构效率、确保物理有效性并充分利用辅助信息的蛋白质-配体结构预测模型。&lt;h4&gt;方法&lt;/h4&gt;作者提出了Pearl（Placing Every Atom in the Right Location）模型，包含三个关键创新：(1) 使用大规模合成数据的训练方法以克服数据稀缺；(2) 融入SO(3)-等变扩散模块的架构，尊重3D旋转对称性；(3) 支持蛋白质和非聚合物组分的通用多链模板系统，以及无条件/条件双模式的可控推理。&lt;h4&gt;主要发现&lt;/h4&gt;Pearl在蛋白质-配体协同折叠方面建立了新的性能标准，在公共基准测试中超越了AlphaFold3和其他开源基线模型，准确构象生成比次优模型分别提高了14.5%和14.2%。在口袋条件协同折叠模式下，Pearl在严格标准下实现了3.6倍的改进，且模型性能与训练中使用的合成数据集大小直接相关。&lt;h4&gt;结论&lt;/h4&gt;Pearl通过创新的训练方法、架构设计和推理机制，显著提高了蛋白质-配体复合物结构预测的准确性，合成数据的使用对模型性能有直接积极影响。&lt;h4&gt;翻译&lt;/h4&gt;准确预测蛋白质-配体复合物的三维结构仍然是计算药物发现中的一个基本挑战，它限制了治疗设计的速度和成功率。深度学习方法最近显示出作为结构预测工具的强大潜力，在多样化的生物分子系统中取得了有希望的准确性。然而，它们的性能和效用受到实验数据稀少、架构效率低下、物理无效构象以及在推理阶段利用可用辅助信息的能力有限等因素的制约。为了解决这些问题，我们引入了Pearl（Placing Every Atom in the Right Location），一种用于大规模蛋白质-配体协同折叠的基础模型。Pearl通过三个关键创新解决了这些挑战：(1) 包括大规模合成数据的训练方法，以克服数据稀缺；(2) 融入SO(3)-等变扩散模块的架构，本质上尊重3D旋转对称性，提高泛化能力和样本效率；(3) 可控推理，包括支持蛋白质和非聚合物组分的通用多链模板系统，以及无条件/条件双模式。Pearl在蛋白质-配体协同折叠方面建立了新的最先进性能。在生成准确和物理有效构象的关键指标上，Pearl在公共基准测试中超越了AlphaFold3和其他开源基线模型，比次优模型分别提高了14.5%和14.2%。在口袋条件协同折叠模式下，Pearl在更严格的标准下实现了3.6倍的改进。最后，我们研究表明模型性能与训练中使用的合成数据集大小直接相关。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting the three-dimensional structures of protein-ligandcomplexes remains a fundamental challenge in computational drug discovery thatlimits the pace and success of therapeutic design. Deep learning methods haverecently shown strong potential as structural prediction tools, achievingpromising accuracy across diverse biomolecular systems. However, theirperformance and utility are constrained by scarce experimental data,inefficient architectures, physically invalid poses, and the limited ability toexploit auxiliary information available at inference. To address these issues,we introduce Pearl (Placing Every Atom in the Right Location), a foundationmodel for protein-ligand cofolding at scale. Pearl addresses these challengeswith three key innovations: (1) training recipes that include large-scalesynthetic data to overcome data scarcity; (2) architectures that incorporate anSO(3)-equivariant diffusion module to inherently respect 3D rotationalsymmetries, improving generalization and sample efficiency, and (3)controllable inference, including a generalized multi-chain templating systemsupporting both protein and non-polymeric components as well as dualunconditional/conditional modes. Pearl establishes a new state-of-the-artperformance in protein-ligand cofolding. On the key metric of generatingaccurate (RMSD &lt; 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold3 and other open source baselines on the public Runs N' Poses and PoseBustersbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over thenext best model. In the pocket-conditional cofolding regime, Pearl delivers$3.6\times$ improvement on a proprietary set of challenging, real-world drugtargets at the more rigorous RMSD &lt; 1 \r{A} threshold. Finally, we demonstratethat model performance correlates directly with synthetic dataset size used intraining.</description>
      <author>example@mail.com (Genesis Research Team, Alejandro Dobles, Nina Jovic, Kenneth Leidal, Pranav Murugan, David C. Williams, Drausin Wulsin, Nate Gruver, Christina X. Ji, Korrawat Pruegsanusak, Gianluca Scarpellini, Ansh Sharma, Wojciech Swiderski, Andrea Bootsma, Richard Strong Bowen, Charlotte Chen, Jamin Chen, Marc André Dämgen, Roy Tal Dew, Benjamin DiFrancesco, J. D. Fishman, Alla Ivanova, Zach Kagin, David Li-Bland, Zuli Liu, Igor Morozov, Jeffrey Ouyang-Zhang, Frank C. Pickard IV, Kushal S. Shah, Ben Shor, Gabriel Monteiro da Silva, Maxx Tessmer, Carl Tilbury, Cyr Vetcher, Daniel Zeng, Maruan Al-Shedivat, Aleksandra Faust, Evan N. Feinberg, Michael V. LeVine, Matteus Pan)</author>
      <guid isPermaLink="false">2510.24670v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning</title>
      <link>http://arxiv.org/abs/2510.24650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 8 figures, and 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该综述探讨了基础模型(FMs)在农业特定地点疾病管理(SSDM)中的应用，重点关注大型语言模型(LLMs)和视觉语言模型(VLMs)的发展及其在自适应学习、强化学习和数字孪生框架中的作用。&lt;h4&gt;背景&lt;/h4&gt;农业特定地点疾病管理通过机器学习和深度学习在实时计算机视觉方面取得快速进展，研究从手工特征提取发展到大规模自动化特征学习，基础模型为作物疾病数据处理带来全新方式。&lt;h4&gt;目的&lt;/h4&gt;筛选约40篇关于基础模型在SSDM中应用的论文，讨论其在自适应学习、强化学习和数字孪生框架中的作用，并分析当前发展状况和挑战。&lt;h4&gt;方法&lt;/h4&gt;分析基础模型如何整合视觉和文本数据，解释症状文本，推理症状-管理关系，支持交互式问答，以及机器人和自适应学习如何支持基于现场的疾病管理。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在2023-24年文献激增；视觉语言模型发表数量比大型语言模型多5-10倍；强化学习和自适应学习在智能喷洒方面仍处于起步阶段；数字孪生可虚拟模拟目标喷洒；解决模拟到现实的差距对实际部署至关重要；人机协作仍有限；具有实时反馈的多模态基础模型将推动下一代SSDM。&lt;h4&gt;结论&lt;/h4&gt;基础模型特别是视觉语言模型在农业特定地点疾病管理中展现出巨大潜力，但仍需解决模拟到现实的差距和人机协作的局限性等挑战。&lt;h4&gt;翻译&lt;/h4&gt;作物特定地点疾病管理通过机器学习和深度学习在实时计算机视觉方面取得了快速进展。研究从手工特征提取发展到大规模自动化特征学习。随着基础模型的出现，作物疾病数据现在以全新的方式被处理。与传统神经网络不同，基础模型整合视觉和文本数据，解释文本中的症状，推理症状-管理关系，并为种植者和教育者支持交互式问答。机器人和自适应学习进一步支持基于现场的疾病管理。本综述筛选了约40篇关于基础模型在特定地点疾病管理中应用的论文，重点关注大型语言模型和视觉语言模型，并讨论它们在自适应学习、强化学习和用于目标喷洒的数字孪生框架中的作用。主要发现：基础模型在2023-24年文献激增中越来越受欢迎；视觉语言模型的发表数量比大型语言模型多5-10倍；强化学习和自适应学习在智能喷洒方面仍处于起步阶段；带有强化学习的数字孪生可以虚拟模拟目标喷洒；解决模拟到现实的差距对实际部署至关重要；人机协作仍然有限，特别是在人机交互方法中，机器人检测早期症状，人类验证不确定情况；具有实时反馈的多模态基础模型将推动下一代特定地点疾病管理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Site-specific disease management (SSDM) in crops has advanced rapidly throughmachine and deep learning (ML and DL) for real-time computer vision. Researchevolved from handcrafted feature extraction to large-scale automated featurelearning. With foundation models (FMs), crop disease datasets are now processedin fundamentally new ways. Unlike traditional neural networks, FMs integratevisual and textual data, interpret symptoms in text, reason aboutsymptom-management relationships, and support interactive QA for growers andeducators. Adaptive and imitation learning in robotics further enablesfield-based disease management. This review screened approx. 40 articles on FMapplications for SSDM, focusing on large-language models (LLMs) andvision-language models (VLMs), and discussing their role in adaptive learning(AL), reinforcement learning (RL), and digital twin frameworks for targetedspraying. Key findings: (a) FMs are gaining traction with surging literature in2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RLand AL are still nascent for smart spraying; (d) digital twins with RL cansimulate targeted spraying virtually; (e) addressing the sim-to-real gap iscritical for real-world deployment; (f) human-robot collaboration remainslimited, especially in human-in-the-loop approaches where robots detect earlysymptoms and humans validate uncertain cases; (g) multi-modal FMs withreal-time feedback will drive next-gen SSDM. For updates, resources, andcontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, tosubmit papers, code, or datasets.</description>
      <author>example@mail.com (Nitin Rai, Daeun, Choi, Nathan S. Boyd, Arnold W. Schumann)</author>
      <guid isPermaLink="false">2510.24650v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives</title>
      <link>http://arxiv.org/abs/2510.24551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种以数据为中心的医疗保健生成式人工智能系统设计范式，通过构建医疗数据生态系统作为基础支撑，实现高质量、有效的医疗保健服务。&lt;h4&gt;背景&lt;/h4&gt;生成式人工智能(GenAI)正在全球范围内迅速发展，为医疗保健领域带来变革性机会。从大型语言模型用于临床笔记合成和对话式辅助，到整合医学影像、电子健康记录和基因组数据的多模态系统用于决策支持，GenAI正在改变医学实践和医疗保健提供方式。&lt;h4&gt;目的&lt;/h4&gt;提出一种数据中心的范式，用于医疗保健领域生成式人工智能系统的设计和部署，解决GenAI在医疗保健应用中的挑战。&lt;h4&gt;方法&lt;/h4&gt;重新定位数据生命周期，将医疗数据生态系统作为生成式医疗保健系统的基础支撑。该生态系统支持多样化医疗数据和知识的集成、表示和检索，通过语义向量搜索和上下文查询等数据处理管道，为上游模型组件和下游临床应用提供支持。&lt;h4&gt;主要发现&lt;/h4&gt;生成式人工智能在医疗保健领域部署需要深入了解医疗保健任务以及可以实现和不能实现的目标，医疗数据生态系统是解决这一挑战的关键。&lt;h4&gt;结论&lt;/h4&gt;通过构建可持续的医疗数据生态系统，不仅能为基础模型提供高质量、多模态数据用于大规模预训练和领域特定微调，还能作为知识检索后端支持特定任务推理，使GenAI能够高质量、有效地部署医疗保健服务。&lt;h4&gt;翻译&lt;/h4&gt;生成式人工智能(GenAI)正在席卷全球。它承诺为推进和颠覆现有实践（包括医疗保健）带来变革性机会。从用于临床笔记合成和对话式辅助的大型语言模型(LLMs)，到整合医学影像、电子健康记录和基因组数据用于决策支持的多模态系统，GenAI正在改变医学实践和医疗保健的提供方式，如诊断和个性化治疗，有潜力减轻临床医生的认知负担，从而改善整体医疗保健服务。然而，GenAI在医疗保健领域的部署需要对医疗保健任务以及可实现和不可实现的目标有深入理解。在本文中，我们提出了一个以数据为中心的范式，用于医疗保健领域生成式人工智能系统的设计和部署。具体而言，我们通过将医疗数据生态系统作为生成式医疗保健系统的基础支撑物，重新定位了数据生命周期。该生态系统旨在可持续地支持多样化医疗数据和知识的集成、表示和检索。通过有效和高效的数据处理管道，如语义向量搜索和上下文查询，它使生成式人工智能能够为上游模型组件和下游临床应用提供支持。最终，它不仅为基础模型提供高质量、多模态数据用于大规模预训练和领域特定微调，还充当知识检索后端，通过智能层支持特定任务的推理。该生态系统使生成式人工智能能够高质量、有效地部署医疗保健服务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative Artificial Intelligence (GenAI) is taking the world by storm. Itpromises transformative opportunities for advancing and disrupting existingpractices, including healthcare. From large language models (LLMs) for clinicalnote synthesis and conversational assistance to multimodal systems thatintegrate medical imaging, electronic health records, and genomic data fordecision support, GenAI is transforming the practice of medicine and thedelivery of healthcare, such as diagnosis and personalized treatments, withgreat potential in reducing the cognitive burden on clinicians, therebyimproving overall healthcare delivery. However, GenAI deployment in healthcarerequires an in-depth understanding of healthcare tasks and what can and cannotbe achieved. In this paper, we propose a data-centric paradigm in the designand deployment of GenAI systems for healthcare. Specifically, we reposition thedata life cycle by making the medical data ecosystem as the foundationalsubstrate for generative healthcare systems. This ecosystem is designed tosustainably support the integration, representation, and retrieval of diversemedical data and knowledge. With effective and efficient data processingpipelines, such as semantic vector search and contextual querying, it enablesGenAI-powered operations for upstream model components and downstream clinicalapplications. Ultimately, it not only supplies foundation models withhigh-quality, multimodal data for large-scale pretraining and domain-specificfine-tuning, but also serves as a knowledge retrieval backend to supporttask-specific inference via the agentic layer. The ecosystem enables thedeployment of GenAI for high-quality and effective healthcare delivery.</description>
      <author>example@mail.com (Gang Chen, Changshuo Liu, Gene Anne Ooi, Marcus Tan, Zhongle Xie, Jianwei Yin, James Wei Luen Yip, Wenqiao Zhang, Jiaqi Zhu, Beng Chin Ooi)</author>
      <guid isPermaLink="false">2510.24551v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Affordance Representation and Recognition for Autonomous Agents</title>
      <link>http://arxiv.org/abs/2510.24459v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种从结构化数据构建世界模型的新方法，解决了软件代理在适应不断变化的Web环境时面临的两个关键挑战。&lt;h4&gt;背景&lt;/h4&gt;软件代理的自主性依赖于其从结构化数据（如网页DOM和Web服务语义描述）构建内部世界模型的能力。然而，原始HTML的冗长性和硬编码API集成的静态性构成了重大障碍。&lt;h4&gt;目的&lt;/h4&gt;开发一种模式语言，使软件代理能够高效构建和维护准确的世界模型，从而实现跨Web及其扩展资源的可扩展、自适应和互操作性自动化。&lt;h4&gt;方法&lt;/h4&gt;提出两种互补的架构模式：1) DOM转换模式，将冗长的原始DOM提炼为紧凑的、任务相关的表示；2) 超媒体功能识别模式，通过解析语义描述动态发现和集成未知Web服务的能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过结合这两种模式，软件代理能够克服数据冗余和服务动态变化的挑战，构建和维护准确的世界模型。&lt;h4&gt;结论&lt;/h4&gt;所提出的模式语言为构建能够高效适应不断演化的数字环境的智能代理提供了强大框架，增强了自动化系统的可扩展性、适应性和互操作性。&lt;h4&gt;翻译&lt;/h4&gt;软件代理的自主性根本上取决于其从定义其数字环境的结构化数据（如网页的文档对象模型和Web服务的语义描述）构建可行的内部世界模型的能力。然而，从原始结构化数据构建此世界模型存在两个关键挑战：原始HTML的冗长性使其在计算上难以被基础模型直接使用，而硬编码API集成的静态性质阻止了代理适应不断发展的服务。本文介绍了一种从结构化数据进行世界建模的模式语言，提出了两种互补的架构模式。DOM转换模式通过将冗长的原始DOM提炼为紧凑的、任务相关的表示或为代理推理核心优化的世界模型，解决了网页复杂性的挑战。同时，超媒体功能识别模式使代理能够通过解析标准化的语义描述来动态丰富其世界模型，从而在运行时发现和集成未知Web服务的能力。这些模式共同提供了一个强大的框架，用于构建能够高效构建和维护准确世界模型的代理，从而实现Web及其扩展资源的可扩展、自适应和互操作性自动化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The autonomy of software agents is fundamentally dependent on their abilityto construct an actionable internal world model from the structured data thatdefines their digital environment, such as the Document Object Model (DOM) ofweb pages and the semantic descriptions of web services. However, constructingthis world model from raw structured data presents two critical challenges: theverbosity of raw HTML makes it computationally intractable for direct use byfoundation models, while the static nature of hardcoded API integrationsprevents agents from adapting to evolving services.  This paper introduces a pattern language for world modeling from structureddata, presenting two complementary architectural patterns. The DOM TransductionPattern addresses the challenge of web page complexity by distilling} averbose, raw DOM into a compact, task-relevant representation or world modeloptimized for an agent's reasoning core. Concurrently, the HypermediaAffordances Recognition Pattern enables the agent to dynamically enrich itsworld model by parsing standardized semantic descriptions to discover andintegrate the capabilities of unknown web services at runtime. Together, thesepatterns provide a robust framework for engineering agents that can efficientlyconstruct and maintain an accurate world model, enabling scalable, adaptive,and interoperable automation across the web and its extended resources.</description>
      <author>example@mail.com (Habtom Kahsay Gidey, Niklas Huber, Alexander Lenz, Alois Knoll)</author>
      <guid isPermaLink="false">2510.24459v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Visual Intelligence: Insights from Video Pretraining</title>
      <link>http://arxiv.org/abs/2510.24448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on  visual intelligence. This work can be considered as v2&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究表明视频扩散模型在视觉任务上比语言模型更高效，视频预训练提供的归纳偏置有助于构建视觉基础模型。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在大规模预训练后能够在语言领域快速适应新问题，但这种成功在视觉领域并未同样有效，包括LLMs在内的模型在组合理解、样本效率和通用问题解决方面仍然存在困难。&lt;h4&gt;目的&lt;/h4&gt;研究视频扩散模型作为弥合语言模型与视觉模型差距的有前途方向，测试视频预训练是否能提供支持广泛任务适应性的归纳偏置。&lt;h4&gt;方法&lt;/h4&gt;在时空数据上预训练视频扩散模型，使其具有结构和动态性的强归纳偏置；设计对照评估，让预训练的LLM和VDM都配备轻量级适配器，并以自然方式呈现任务；在多个基准测试中评估，包括ARC-AGI、ConceptARC、视觉游戏、路线规划和细胞自动机。&lt;h4&gt;主要发现&lt;/h4&gt;VDMs比语言对应模型具有更高的数据效率；视频预训练提供的归纳偏置支持视觉基础模型的进展。&lt;h4&gt;结论&lt;/h4&gt;视频预训练提供了归纳偏置，支持向视觉基础模型进展。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型已经证明，大规模预训练使系统能够在语言领域以少量监督快速适应新问题。然而，这种成功并没有同样有效地转化为视觉领域，包括LLMs在内的模型在组合理解、样本效率和通用问题解决方面仍然存在困难。我们研究视频扩散模型作为弥合这一差距的有前途方向。在时空数据上的预训练赋予这些模型结构和动态性的强归纳偏置，我们假设这可以支持广泛的任务适应性。为了验证这一点，我们设计了一个对照评估，让预训练的LLM和预训练的VDM都配备轻量级适配器，并以它们自然的方式呈现任务。在包括ARC-AGI、ConceptARC、视觉游戏、路线规划和细胞自动机在内的基准测试中，VDMs比其语言对应模型表现出更高的数据效率。综上所述，我们的结果表明视频预训练提供了归纳偏置，支持向视觉基础模型的进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated that large-scale pretrainingenables systems to adapt rapidly to new problems with little supervision in thelanguage domain. This success, however, has not translated as effectively tothe visual domain, where models, including LLMs, continue to struggle withcompositional understanding, sample efficiency, and general-purposeproblem-solving. We investigate Video Diffusion Models (VDMs) as a promisingdirection for bridging this gap. Pretraining on spatiotemporal data endowsthese models with strong inductive biases for structure and dynamics, which wehypothesize can support broad task adaptability. To test this, we design acontrolled evaluation in which both a pretrained LLM and a pretrained VDM areequipped with lightweight adapters and presented with tasks in their naturalmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,route planning, and cellular automata, VDMs demonstrate higher data efficiencythan their language counterparts. Taken together, our results indicate thatvideo pretraining offers inductive biases that support progress toward visualfoundation models.</description>
      <author>example@mail.com (Pablo Acuaviva, Aram Davtyan, Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Alexandre Alahi, Paolo Favaro)</author>
      <guid isPermaLink="false">2510.24448v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>A Unified Geometric Space Bridging AI Models and the Human Brain</title>
      <link>http://arxiv.org/abs/2510.24342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了'类脑空间'概念，通过映射AI模型的内在空间注意力拓扑结构到人类功能性脑网络，实现了跨模态AI模型的统一比较框架，揭示了机器与大脑之间的深层组织原则。&lt;h4&gt;背景&lt;/h4&gt;几十年来，神经科学家和计算机科学家一直致力于理解并构建智能。现代人工神经网络在语言、感知和推理方面已可与人类媲美，但这些系统是否像大脑一样组织信息仍然在很大程度上未知。现有的大脑-AI对齐研究虽展示了两个系统间的对应关系，但比较局限于特定输入和任务，缺乏跨模态AI模型内在组织的共同比较基础。&lt;h4&gt;目的&lt;/h4&gt;引入'类脑空间'概念，这是一个统一的几何空间，无论输入模态、任务或感觉域如何，都能通过将AI模型内在的空间注意力拓扑结构映射到典型人类功能性脑网络上，实现对AI模型的精确定位和比较。&lt;h4&gt;方法&lt;/h4&gt;对151个基于Transformer的模型进行广泛分析，涵盖最先进的大型视觉模型、大型语言模型和大型多模态模型。&lt;h4&gt;主要发现&lt;/h4&gt;在类脑空间中存在连续的弧形几何结构，反映类脑性的逐渐增加；不同模型表现出不同的分布模式，与不同程度的类脑性相关，这种模式不仅受模态影响，还受预训练范式是否强调全局语义抽象以及位置编码方案是否促进不同模态间深度融合的影响；此外，模型的类脑程度和其下游任务表现并非完全相同。&lt;h4&gt;结论&lt;/h4&gt;类脑空间提供了第一个跨领域的定位、量化和比较智能的统一框架，揭示了连接机器和大脑的深层组织原则。&lt;h4&gt;翻译&lt;/h4&gt;几十年来，神经科学家和计算机科学家一直怀有共同的志向：理解智能并构建它。现代人工神经网络在语言、感知和推理方面现在可以与人类相媲美，但这些人工系统是否像大脑一样组织信息仍然在很大程度上未知。现有的大脑-AI对齐研究已经展示了两个系统之间的惊人对应关系，但这样的比较仍然局限于特定的输入和任务，没有提供共同的基础来比较不同模态（视觉、语言或多模态）的AI模型是如何内在组织的。在这里，我们引入了一个突破性的概念：类脑空间：一个统一的几何空间，通过将AI模型内在的空间注意力拓扑组织映射到典型的人类功能性脑网络上，无论输入模态、任务或感觉域如何，每个AI模型都可以在这个空间中被精确定位和比较。我们对151个基于Transformer的模型进行了广泛分析，这些模型涵盖了最先进的大型视觉模型、大型语言模型和大型多模态模型，在这个空间中发现了一个连续的弧形几何结构，反映了类脑性的逐渐增加；不同的模型在这个几何结构中表现出不同的分布模式，与不同程度的类脑性相关，这种模式不仅受模态影响，还受预训练范式是否强调全局语义抽象以及位置编码方案是否促进不同模态间的深度融合的影响。此外，模型的类脑程度和其下游任务表现并非'完全相同'。类脑空间提供了第一个跨领域的定位、量化和比较智能的统一框架，揭示了连接机器和大脑的深层组织原则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For decades, neuroscientists and computer scientists have pursued a sharedambition: to understand intelligence and build it. Modern artificial neuralnetworks now rival humans in language, perception, and reasoning, yet it isstill largely unknown whether these artificial systems organize information asthe brain does. Existing brain-AI alignment studies have shown the strikingcorrespondence between the two systems, but such comparisons remain bound tospecific inputs and tasks, offering no common ground for comparing how AImodels with different kinds of modalities-vision, language, or multimodal-areintrinsically organized. Here we introduce a groundbreaking concept ofBrain-like Space: a unified geometric space in which every AI model can beprecisely situated and compared by mapping its intrinsic spatial attentiontopological organization onto canonical human functional brain networks,regardless of input modality, task, or sensory domain. Our extensive analysisof 151 Transformer-based models spanning state-of-the-art large vision models,large language models, and large multimodal models uncovers a continuousarc-shaped geometry within this space, reflecting a gradual increase ofbrain-likeness; different models exhibit distinct distribution patterns withinthis geometry associated with different degrees of brain-likeness, shaped notmerely by their modality but by whether the pretraining paradigm emphasizesglobal semantic abstraction and whether the positional encoding schemefacilitates deep fusion across different modalities. Moreover, the degree ofbrain-likeness for a model and its downstream task performance are not"identical twins". The Brain-like Space provides the first unified frameworkfor situating, quantifying, and comparing intelligence across domains,revealing the deep organizational principles that bridge machines and thebrain.</description>
      <author>example@mail.com (Silin Chen, Yuzhong Chen, Zifan Wang, Junhao Wang, Zifeng Jia, Keith M Kendrick, Tuo Zhang, Lin Zhao, Dezhong Yao, Tianming Liu, Xi Jiang)</author>
      <guid isPermaLink="false">2510.24342v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2</title>
      <link>http://arxiv.org/abs/2510.24195v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UAP-SAM2，第一个针对SAM2的跨提示通用对抗攻击，通过双语义偏差驱动，有效解决了SAM2架构差异带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;图像分割基础模型SAM对对抗样本存在脆弱性，其后续模型SAM2在视频分割方面表现出强大的泛化能力，但其鲁棒性尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;分析现有攻击在SAM和SAM2之间的性能差距，并提出一种有效的对抗攻击方法针对SAM2。&lt;h4&gt;方法&lt;/h4&gt;提出UAP-SAM2方法，包括：1) 设计目标扫描策略，将每帧划分为k个区域，每个区域随机分配提示，减少优化过程中的提示依赖性；2) 设计双语义偏差框架，通过扭曲当前帧内语义和破坏连续帧间语义一致性来优化UAP。&lt;h4&gt;主要发现&lt;/h4&gt;现有攻击在SAM和SAM2之间存在性能差距，主要由于SAM2架构差异带来的两个关键挑战：来自提示的方向性指导和连续帧之间的语义纠缠。&lt;h4&gt;结论&lt;/h4&gt;UAP-SAM2在两个分割任务上的六个数据集实验中表现出有效性，以较大优势显著优于最先进的攻击方法。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究揭示了图像分割基础模型SAM对对抗样本的脆弱性。其后续模型SAM2因其强大的视频分割泛化能力而受到广泛关注。然而，其鲁棒性尚未被探索，目前还不清楚现有的针对SAM的攻击是否可以直接转移到SAM2上。在本文中，我们首先分析了现有攻击在SAM和SAM2之间的性能差距，并指出了它们架构差异带来的两个关键挑战：来自提示的方向性指导和连续帧之间的语义纠缠。为解决这些问题，我们提出了UAP-SAM2，这是第一个由双语义偏差驱动的针对SAM2的跨提示通用对抗攻击。为实现跨提示可转移性，我们首先设计了一个目标扫描策略，将每帧划分为k个区域，每个区域随机分配一个提示，以减少优化过程中的提示依赖性。为提高有效性，我们设计了一个双语义偏差框架，通过扭曲当前帧内的语义和破坏连续帧之间的语义一致性来优化UAP。在两个分割任务上的六个数据集进行的广泛实验证明了所提方法对SAM2的有效性。比较结果显示，UAP-SAM2以较大优势显著优于最先进的(SOTA)攻击。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies reveal the vulnerability of the image segmentation foundationmodel SAM to adversarial examples. Its successor, SAM2, has attractedsignificant attention due to its strong generalization capability in videosegmentation. However, its robustness remains unexplored, and it is unclearwhether existing attacks on SAM can be directly transferred to SAM2. In thispaper, we first analyze the performance gap of existing attacks between SAM andSAM2 and highlight two key challenges arising from their architecturaldifferences: directional guidance from the prompt and semantic entanglementacross consecutive frames. To address these issues, we propose UAP-SAM2, thefirst cross-prompt universal adversarial attack against SAM2 driven by dualsemantic deviation. For cross-prompt transferability, we begin by designing atarget-scanning strategy that divides each frame into k regions, each randomlyassigned a prompt, to reduce prompt dependency during optimization. Foreffectiveness, we design a dual semantic deviation framework that optimizes aUAP by distorting the semantics within the current frame and disrupting thesemantic consistency across consecutive frames. Extensive experiments on sixdatasets across two segmentation tasks demonstrate the effectiveness of theproposed method for SAM2. The comparative results show that UAP-SAM2significantly outperforms state-of-the-art (SOTA) attacks by a large margin.</description>
      <author>example@mail.com (Ziqi Zhou, Yifan Hu, Yufei Song, Zijing Li, Shengshan Hu, Leo Yu Zhang, Dezhong Yao, Long Zheng, Hai Jin)</author>
      <guid isPermaLink="false">2510.24195v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames</title>
      <link>http://arxiv.org/abs/2510.24194v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;行为克隆是一种从演示中学习序列决策的简单而有效技术，本文提出让演示者在部分信息不足的情况下进行演示，发现这种'蒙眼'专家的克隆方法在泛化到未见任务时表现更好。&lt;h4&gt;背景&lt;/h4&gt;行为克隆已成为物理世界基础模型的核心，实现泛化需要无数任务的演示。通常，具有完整任务信息的人类专家会演示最优行为。&lt;h4&gt;目的&lt;/h4&gt;研究向演示者隐藏部分任务信息是否能提高行为克隆的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出'蒙眼专家'方法，即向任务演示者隐藏部分信息，迫使其使用非平凡的探索策略来解决任务。在真实世界机器人插销任务和Procgen基准视频游戏上进行实验，并进行理论分析。&lt;h4&gt;主要发现&lt;/h4&gt;克隆'蒙眼专家'的行为比克隆完全信息专家的行为在未见任务上泛化能力更好。理论分析表明泛化误差与演示者可用的任务信息量和演示任务数量有关，使用更少演示任务时克隆蒙眼专家能实现更好泛化。&lt;h4&gt;结论&lt;/h4&gt;理论和实践均表明，使用更少的演示任务，克隆蒙眼专家能够实现更好的泛化效果。&lt;h4&gt;翻译&lt;/h4&gt;行为克隆是一种简单而有效的技术，用于从演示中学习序列决策。最近，它已成为物理世界基础模型的核心，其中实现泛化需要无数任务的演示。通常，具有任务完整信息的人类专家会演示（几乎）最优的行为。在本文中，我们提出向演示者隐藏任务的部分信息。这种'蒙眼'专家被迫采用非平凡的探索来解决任务。我们证明，克隆蒙眼专家比完全信息的专家在未见任务上泛化能力更好。我们在有限人类演示下的真实世界机器人插销任务以及Procgen基准的视频游戏上进行了实验。此外，我们通过理论分析支持了这一发现，理论和实践都表明，用更少的演示任务克隆蒙眼专家能更好地泛化。项目页面包含视频和代码：https://sites.google.com/view/blindfoldedexperts/home&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Behavioral cloning is a simple yet effective technique for learningsequential decision-making from demonstrations. Recently, it has gainedprominence as the core of foundation models for the physical world, whereachieving generalization requires countless demonstrations of a multitude oftasks. Typically, a human expert with full information on the task demonstratesa (nearly) optimal behavior. In this paper, we propose to hide some of thetask's information from the demonstrator. This ``blindfolded'' expert iscompelled to employ non-trivial exploration to solve the task. We show thatcloning the blindfolded expert generalizes better to unseen tasks than itsfully-informed counterpart. We conduct experiments of real-world robot peginsertion tasks with (limited) human demonstrations, alongside videogames fromthe Procgen benchmark. Additionally, we support our findings with theoreticalanalysis, which confirms that the generalization error scales with$\sqrt{I/m}$, where $I$ measures the amount of task information available tothe demonstrator, and $m$ is the number of demonstrated tasks. Both theory andpractice indicate that cloning blindfolded experts generalizes better withfewer demonstrated tasks. Project page with videos and code:https://sites.google.com/view/blindfoldedexperts/home</description>
      <author>example@mail.com (Ev Zisselman, Mirco Mutti, Shelly Francis-Meretzki, Elisei Shafer, Aviv Tamar)</author>
      <guid isPermaLink="false">2510.24194v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning</title>
      <link>http://arxiv.org/abs/2510.24161v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Boundless Large Model (BLM₁)，一个多模态空间基础模型，能够在数字和物理空间无缝操作，实现跨具身和任务泛化。通过两阶段训练范式，BLM₁整合了跨空间迁移、跨任务学习和跨具身泛化能力，在数字和物理基准测试中超越了多种模型家族。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在视觉-语言推理方面取得进展并应用于具身智能体，但仍存在显著局限性：MLLMs在数字-物理空间和具身形式间泛化能力差；视觉-语言-动作模型(VLAs)产生低级行动但缺乏稳健的高层次具身推理；大多数具身大语言模型(ELLMs)局限于数字空间，对物理世界泛化能力差。缺乏能够在数字和物理空间无缝操作、跨具身和任务泛化的统一模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在数字和物理空间无缝操作、跨具身和任务泛化的统一模型。&lt;h4&gt;方法&lt;/h4&gt;提出Boundless Large Model (BLM₁)，一个多模态空间基础模型，保留指令跟随和推理能力，整合具身知识，支持稳健的跨具身控制。通过两阶段训练范式整合三种关键能力：跨空间迁移、跨任务学习和跨具身泛化。第一阶段通过精心挑选的数字语料库将具身知识注入MLLM，同时保持语言能力；第二阶段通过意图桥接接口训练策略模块，从MLLM中提取高层语义来指导控制，无需微调MLLM主干。使用自收集的跨具身演示套件，涵盖四种机器人具身和六种渐进式挑战任务。&lt;h4&gt;主要发现&lt;/h4&gt;在数字和物理基准测试中评估显示，单个BLM₁实例优于四种模型家族（MLLMs、ELLMs、VLAs和GMLMs），在数字任务中实现约6%的提升，在物理任务中实现约3%的提升。&lt;h4&gt;结论&lt;/h4&gt;BLM₁是一个有效的多模态空间基础模型，能够整合具身知识并实现跨空间、跨任务和跨具身的泛化能力，为具身智能体提供了新的发展方向。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)已推进视觉-语言推理，并越来越多地部署在具身智能体中。然而，仍存在显著局限性：MLLMs在数字-物理空间和具身形式间泛化能力差；视觉-语言-动作模型(VLAs)产生低级行动但缺乏稳健的高层次具身推理；大多数具身大语言模型(ELLMs)局限于数字空间，对物理世界泛化能力差。因此，能够在数字和物理空间无缝操作、跨具身和任务泛化的统一模型仍然缺失。我们引入了Boundless Large Model (BLM₁)，一个多模态空间基础模型，保留了指令跟随和推理能力，整合了具身知识，并支持稳健的跨具身控制。BLM₁通过两阶段训练范式整合了三种关键能力——跨空间迁移、跨任务学习和跨具身泛化。第一阶段通过精心挑选的数字语料库将具身知识注入MLLM，同时保持语言能力。第二阶段通过意图桥接接口训练策略模块，从MLLM中提取高层语义来指导控制，无需微调MLLM主干。这一过程得到了一个自收集的跨具身演示套件的支持，涵盖四种机器人具身和六种渐进式挑战任务。在数字和物理基准测试中的评估显示，单个BLM₁实例优于四种模型家族——MLLMs、ELLMs、VLAs和GMLMs，在数字任务中实现约6%的提升，在物理任务中实现约3%的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have advanced vision-languagereasoning and are increasingly deployed in embodied agents. However,significant limitations remain: MLLMs generalize poorly across digital-physicalspaces and embodiments; vision-language-action models (VLAs) produce low-levelactions yet lack robust high-level embodied reasoning; and most embodied largelanguage models (ELLMs) are constrained to digital-space with poorgeneralization to the physical world. Thus, unified models that operateseamlessly across digital and physical spaces while generalizing acrossembodiments and tasks remain absent. We introduce the \textbf{Boundless LargeModel (BLM$_1$)}, a multimodal spatial foundation model that preservesinstruction following and reasoning, incorporates embodied knowledge, andsupports robust cross-embodiment control. BLM$_1$ integrates three keycapabilities -- \textit{cross-space transfer, cross-task learning, andcross-embodiment generalization} -- via a two-stage training paradigm. Stage Iinjects embodied knowledge into the MLLM through curated digital corpora whilemaintaining language competence. Stage II trains a policy module through anintent-bridging interface that extracts high-level semantics from the MLLM toguide control, without fine-tuning the MLLM backbone. This process is supportedby a self-collected cross-embodiment demonstration suite spanning four robotembodiments and six progressively challenging tasks. Evaluations across digitaland physical benchmarks show that a single BLM$_1$ instance outperforms fourmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving$\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physicaltasks.</description>
      <author>example@mail.com (Wentao Tan, Bowen Wang, Heng Zhi, Chenyu Liu, Zhe Li, Jian Liu, Zengrong Lin, Yukun Dai, Yipeng Chen, Wenjie Yang, Enci Xie, Hao Xue, Baixu Ji, Chen Xu, Zhibin Wang, Tianshi Wang, Lei Zhu, Heng Tao Shen)</author>
      <guid isPermaLink="false">2510.24161v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Global Chlorophyll-\textit{a} Retrieval algorithm from Sentinel 2 Using Residual Deep Learning and Novel Machine Learning Water Classification</title>
      <link>http://arxiv.org/abs/2510.24124v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为全球水分类器(GWC)的监督式机器学习分类器，用于全球范围内水体识别和叶绿素-a浓度反演，并通过残差CNN校正提高了反演精度。&lt;h4&gt;背景&lt;/h4&gt;传统的水体识别和叶绿素-a浓度反演面临多种干扰因素，如云、太阳耀斑、雪、冰、水生植被、陆地和沉积物等，需要一种能够全球应用且稳健的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够全球范围内准确识别水体并反演叶绿素-a浓度的方法，克服地理和气候条件的限制。&lt;h4&gt;方法&lt;/h4&gt;1. 使用Sen2Cor校正的Sentinel-2地表反射率数据训练全球水分类器(GWC)2. 基于近100个全球分布的内陆水体样本进行训练3. 使用XGBoost回归器进行叶绿素-a浓度反演4. 添加残差CNN(RCNN)校正阶段提高反演精度5. 在867个水体上进行测试验证&lt;h4&gt;主要发现&lt;/h4&gt;1. GWC能够有效区分不同叶绿素-a水平的水体与非水体光谱2. GWC表现出地理位置稳定的性能3. GWC正标记的场景产生的叶绿素-a反演值更准确4. 残差CNN校正阶段显著提高了反演精度5. 最终算法在测试中表现出稳健性、可扩展性和全球可转移性&lt;h4&gt;结论&lt;/h4&gt;全球水分类器结合残差CNN校正的方法能够准确、稳健地进行全球水体识别和叶绿素-a浓度反演，无需针对不同地区进行额外调优，具有很高的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了全球水分类器(GWC)，一种监督式、地理范围广泛的机器学习分类器，基于Sen2Cor校正的Sentinel-2地表反射率数据训练。使用近100个全球分布的内陆水体，GWC能够区分不同叶绿素-a水平的水体光谱与非水体光谱(云、太阳耀斑、雪、冰、水生植被、陆地和沉积物)，并表现出地理位置稳定的性能。在此基础模型上，我们使用匹配的Sentinel-2反射率数据与美国地质调查局(USGS)AquaMatch现场数据集进行叶绿素-a反演，覆盖了多样的地理和水文条件。我们在13626个匹配点上训练了一个XGBoost回归器。GWC正标记的场景持续优于负标记场景，并产生更准确的叶绿素-a反演值，这证实了分类器在减少各种干扰方面的优势。接下来，回归预测的残差分析揭示了结构化误差，促使我们添加了残差CNN(RCNN)校正阶段。我们添加了一个基于归一化残差训练的CNN残差阶段，取得了显著改进。我们的算法在867个水体上进行了测试，超过2000个预测，叶绿素-a值高达1000毫克每立方米，实现了R² = 0.79，平均绝对误差 = 13.52毫克每立方米，斜率 = 0.91，证明了其稳健、可扩展且全球可转移的性能，无需额外调优。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the Global Water Classifier (GWC), a supervised, geospatiallyextensive Machine Learning (ML) classifier trained on Sen2Cor correctedSentinel-2 surface reflectance data. Using nearly 100 globally distributedinland water bodies, GWC distinguishes water across Chlorophyll-a (Chla) levelsfrom non-water spectra (clouds, sun glint, snow, ice, aquatic vegetation, landand sediments) and shows geographically stable performance.  Building on this foundation model, we perform Chla retrieval based on amatchup Sentinel-2 reflectance data with the United States Geological Survey(USGS) AquaMatch in-situ dataset, covering diverse geographical andhydrological conditions.  We train an XGBoost regressor on 13626 matchup points. The positive labeledscenes by the GWC consistently outperform the negatives and produce moreaccurate Chla retrieval values, which confirms the classifiers advantage inreducing various interferences.  Next, residual analysis of the regression predictions revealed structurederrors, motivating a residual CNN (RCNN) correction stage. We add a CNNresidual stage trained on normalized residuals, which yield substantialimprovement. Our algorithm was tested on 867 water bodies with over 2,000predictions and Chla values up to 1000~mg$/m^{3}$, achieving $R^2$ = 0.79, MAE= 13.52~mg$/m^{3}$, and slope = 0.91, demonstrating robust, scalable, andglobally transferable performance without additional tuning.</description>
      <author>example@mail.com (Yotam Sherf, Bar Efrati, Gabriel Rozman, Moshe Harel)</author>
      <guid isPermaLink="false">2510.24124v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>OmniLearned: A Foundation Model Framework for All Tasks Involving Jet Physics</title>
      <link>http://arxiv.org/abs/2510.24066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了OmniLearn基础模型的重大升级，形成了OmniLearned框架。该框架包含模型架构和训练的更新、使用超过十亿个喷注进行训练，以及提供完善的软件访问工具。通过三个代表性任务展示了该框架的性能，结果表明它在所有任务中都是最先进的，显著增强了粒子物理实验的发现潜力。&lt;h4&gt;背景&lt;/h4&gt;基础模型利用大型数据集构建有效的数据表示，可应用于多样化的下游任务。之前开发的OmniLearn基础模型针对粒子物理喷注，利用了粒子物理的独特性质，能够显著增强对撞机实验的发现潜力。&lt;h4&gt;目的&lt;/h4&gt;对现有的OmniLearn基础模型进行重大升级，创建OmniLearned框架，进一步提升其在粒子物理实验中的性能和可用性，扩展过去、当前和未来对撞机实验的发现潜力。&lt;h4&gt;方法&lt;/h4&gt;开发OmniLearned框架，包含三个新元素：更新模型架构和训练方法、使用超过十亿个喷注进行训练、提供完善的软件用于访问所有数据集和模型。通过三个代表性任务进行验证：top夸克喷注标记、b标记和异常检测。&lt;h4&gt;主要发现&lt;/h4&gt;在三个代表性任务（top夸克喷注标记、b标记和异常检测）中，OmniLearned均达到了最先进的性能水平。该框架能够显著增强对撞机实验的发现潜力，包括过去、当前和未来的实验。&lt;h4&gt;结论&lt;/h4&gt;OmniLearned框架代表了基础模型在粒子物理领域的重要进展，通过架构更新、大规模训练和完善的软件工具，显著提升了模型性能，为粒子物理研究提供了更强大的分析工具。&lt;h4&gt;翻译&lt;/h4&gt;基础模型使用大型数据集构建有效的数据表示，可部署在多样化的下游任务中。先前的研究开发了用于粒子物理喷注的OmniLearn基础模型，利用了粒子物理的独特性质，并表明它可以显著增强对撞机实验的发现潜力。本文介绍了一个重大升级，结果是OmniLearned框架。该框架有三个新元素：(1)对模型架构和训练的更新，(2)使用超过十亿个喷注进行训练，(3)提供完善的软件用于访问所有数据集和模型。我们通过三个代表性任务展示了OmniLearned：使用社区Delphes基准数据集进行top夸克喷注标记，使用ATLAS全模拟进行b标记，以及使用CMS实验数据进行异常检测。在每种情况下，OmniLearned都是最先进的，进一步扩展了过去、当前和未来对撞机实验的发现潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models use large datasets to build an effective representation ofdata that can be deployed on diverse downstream tasks. Previous researchdeveloped the OmniLearn foundation model for jet physics, using uniqueproperties of particle physics, and showed that it could significantly advancediscovery potential across collider experiments. This paper introduces a majorupgrade, resulting in the OmniLearned framework. This framework has three newelements: (1) updates to the model architecture and training, (2) using overone billion jets used for training, and (3) providing well-documented softwarefor accessing all datasets and models. We demonstrate OmniLearned with threerepresentative tasks: top-quark jet tagging with the community Delphes-basedbenchmark dataset, b-tagging with ATLAS full simulation, and anomaly detectionwith CMS experimental data. In each case, OmniLearned is the state of the art,further expanding the discovery potential of past, current, and future colliderexperiments.</description>
      <author>example@mail.com (Wahid Bhimji, Chris Harris, Vinicius Mikuni, Benjamin Nachman)</author>
      <guid isPermaLink="false">2510.24066v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Human Machine Social Hybrid Intelligence:A Collaborative Decision Making Framework for Large Model Agent Groups and Human Experts</title>
      <link>http://arxiv.org/abs/2510.24030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新型的人机社会混合智能(HMS-HI)框架，通过共享认知空间、动态角色任务分配和跨物种信任校准三个核心组件，实现了人类专家和AI代理之间的深度协作决策，在应急响应模拟中显著降低了伤亡和认知负荷。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型和多智能体系统快速发展提供了前所未有的能力，但当前人机协同(HiTL)范式未能充分整合人类专业知识，在复杂、高风险环境中常导致认知过载和决策瓶颈。&lt;h4&gt;目的&lt;/h4&gt;设计一种新型架构，用于人类专家群体和基于大语言模型的AI代理之间的深度协作决策，解决现有人机协同方法的不足。&lt;h4&gt;方法&lt;/h4&gt;HMS-HI建立在三个核心支柱上：(1)共享认知空间(SCS)用于统一的多模态态势感知和结构化世界建模；(2)动态角色和任务分配(DRTA)模块根据能力和工作负载将任务自适应分配给最适合的代理；(3)跨物种信任校准(CSTC)协议通过可解释声明和结构化反馈促进透明度、责任和相互适应。&lt;h4&gt;主要发现&lt;/h4&gt;在高保真城市应急响应模拟中，HMS-HI相比传统HiTL方法将平民伤亡减少72%，认知负荷减少70%，证明了卓越的决策质量、效率和人类-AI信任。消融研究确认了每个模块的关键贡献，表明工程化的信任和共享背景是可扩展的人机协作基础。&lt;h4&gt;结论&lt;/h4&gt;HMS-HI框架通过三个核心组件的整合，在复杂、高风险环境中实现了更有效的人机协作决策，显著提高了决策质量和效率，同时减轻了人类认知负担。&lt;h4&gt;翻译&lt;/h4&gt;大型基础模型和多智能体系统的快速发展提供了前所未有的能力，但当前人机协同(HiTL)范式未能充分整合人类专业知识，在复杂、高风险环境中常常导致认知过载和决策瓶颈。我们提出了'人机社会混合智能'(HMS-HI)框架，这是一种专为人类专家群体和由大语言模型驱动的AI代理之间的深度协作决策而设计的新型架构。HMS-HI建立在三个核心支柱上：(1)共享认知空间(SCS)用于统一、多模态的态势感知和结构化世界建模；(2)动态角色和任务分配(DRTA)模块，根据能力和工作负载将任务自适应地分配给最适合的代理(人类或AI)；(3)跨物种信任校准(CSTC)协议，通过可解释声明和结构化反馈促进透明度、责任和相互适应。在高保真的城市应急响应模拟中验证，HMS-HI与传统HiTL方法相比，平民伤亡减少了72%，认知负荷减少了70%，证明了卓越的决策质量、效率和人类-AI信任。消融研究确认了每个模块的关键贡献，强调工程化的信任和共享背景是可扩展的、协同的人机协作的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancements in large foundation models and multi-agent systemsoffer unprecedented capabilities, yet current Human-in-the-Loop (HiTL)paradigms inadequately integrate human expertise, often leading to cognitiveoverload and decision-making bottlenecks in complex, high-stakes environments.We propose the "Human-Machine Social Hybrid Intelligence" (HMS-HI) framework, anovel architecture designed for deep, collaborative decision-making betweengroups of human experts and LLM-powered AI agents. HMS-HI is built upon threecore pillars: (1) a \textbf{Shared Cognitive Space (SCS)} for unified,multi-modal situational awareness and structured world modeling; (2) a\textbf{Dynamic Role and Task Allocation (DRTA)} module that adaptively assignstasks to the most suitable agent (human or AI) based on capabilities andworkload; and (3) a \textbf{Cross-Species Trust Calibration (CSTC)} protocolthat fosters transparency, accountability, and mutual adaptation throughexplainable declarations and structured feedback. Validated in a high-fidelityurban emergency response simulation, HMS-HI significantly reduced civiliancasualties by 72\% and cognitive load by 70\% compared to traditional HiTLapproaches, demonstrating superior decision quality, efficiency, and human-AItrust. An ablation study confirms the critical contribution of each module,highlighting that engineered trust and shared context are foundational forscalable, synergistic human-AI collaboration.</description>
      <author>example@mail.com (Ahmet Akkaya Melih, Yamuna Singh, Kunal L. Agarwal, Priya Mukherjee, Kiran Pattnaik, Hanuman Bhatia)</author>
      <guid isPermaLink="false">2510.24030v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks</title>
      <link>http://arxiv.org/abs/2510.24010v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Mars-Bench，这是第一个用于系统评估火星相关任务的基准，包含20个数据集，涵盖分类、分割和目标检测，专注于关键地质特征。研究结果表明火星特定的基础模型可能比通用领域模型具有优势，为火星科学领域的机器学习模型开发提供了标准化基础。&lt;h4&gt;背景&lt;/h4&gt;基础模型通过大规模无标签数据预训练在许多专业领域取得了快速进展，表现出强大的泛化能力。然而，这类模型在火星科学领域的应用仍然有限，主要原因是火星科学缺乏标准化基准和评估框架，限制了火星任务基础模型的发展。&lt;h4&gt;目的&lt;/h4&gt;引入Mars-Bench，第一个基准，旨在系统评估使用轨道和表面图像的广泛火星相关任务模型，为火星科学领域的机器学习模型开发提供标准化基础。&lt;h4&gt;方法&lt;/h4&gt;Mars-Bench包含20个数据集，涵盖分类、分割和目标检测，专注于陨石坑、锥体、巨石和霜等关键地质特征。提供标准化、即用型数据集和基线评估，使用在自然图像、地球卫星数据和最先进的视觉语言模型上预训练的模型进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;所有分析结果表明，火星特定的基础模型可能比通用领域模型具有优势，这激励了对领域自适应预训练的进一步探索。&lt;h4&gt;结论&lt;/h4&gt;Mars-Bench旨在为开发和比较火星科学的机器学习模型建立标准化基础，其数据、模型和代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;基础模型通过大规模无标签数据预训练在许多专业领域取得了快速进展，显示出对各种下游任务的强大泛化能力。尽管这类模型在地球观测等领域受到广泛关注，但在火星科学领域的应用仍然有限。其他领域取得进展的一个关键因素是标准化基准的可用性，这些基准支持系统评估。相比之下，火星科学缺乏此类基准和标准化评估框架，这限制了火星任务基础模型的发展。为解决这一差距，我们引入了Mars-Bench，这是第一个基准，旨在使用轨道和表面图像系统评估广泛火星相关任务的模型。Mars-Bench包含20个数据集，涵盖分类、分割和目标检测，专注于陨石坑、锥体、巨石和霜等关键地质特征。我们提供了标准化、即用型数据集和基线评估，使用在自然图像、地球卫星数据和最先进的视觉语言模型上预训练的模型。所有分析的结果表明，火星特定的基础模型可能比通用领域对应模型具有优势，这激励了对领域自适应预训练的进一步探索。Mars-Bench旨在为开发和比较火星科学的机器学习模型建立标准化基础。我们的数据、模型和代码可在 https://mars-bench.github.io/ 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have enabled rapid progress across many specialized domainsby leveraging large-scale pre-training on unlabeled data, demonstrating stronggeneralization to a variety of downstream tasks. While such models have gainedsignificant attention in fields like Earth Observation, their application toMars science remains limited. A key enabler of progress in other domains hasbeen the availability of standardized benchmarks that support systematicevaluation. In contrast, Mars science lacks such benchmarks and standardizedevaluation frameworks, which have limited progress toward developing foundationmodels for Martian tasks. To address this gap, we introduce Mars-Bench, thefirst benchmark designed to systematically evaluate models across a broad rangeof Mars-related tasks using both orbital and surface imagery. Mars-Benchcomprises 20 datasets spanning classification, segmentation, and objectdetection, focused on key geologic features such as craters, cones, boulders,and frost. We provide standardized, ready-to-use datasets and baselineevaluations using models pre-trained on natural images, Earth satellite data,and state-of-the-art vision-language models. Results from all analyses suggestthat Mars-specific foundation models may offer advantages over general-domaincounterparts, motivating further exploration of domain-adapted pre-training.Mars-Bench aims to establish a standardized foundation for developing andcomparing machine learning models for Mars science. Our data, models, and codeare available at: https://mars-bench.github.io/.</description>
      <author>example@mail.com (Mirali Purohit, Bimal Gajera, Vatsal Malaviya, Irish Mehta, Kunal Kasodekar, Jacob Adler, Steven Lu, Umaa Rebbapragada, Hannah Kerner)</author>
      <guid isPermaLink="false">2510.24010v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Why Foundation Models in Pathology Are Failing</title>
      <link>http://arxiv.org/abs/2510.23807v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基础模型在非医学领域取得成功，但在计算病理学应用中存在根本性概念不匹配，需要重新思考建模范式&lt;h4&gt;背景&lt;/h4&gt;在非医学领域，基础模型通过大规模自监督和多模态学习彻底改变了计算机视觉和语言处理，预期计算病理学中也会迅速采用这些模型&lt;h4&gt;目的&lt;/h4&gt;检查基础模型在计算病理学中的缺点，论证这些缺点源于通用基础建模假设与人体组织复杂性之间的概念性不匹配&lt;h4&gt;方法&lt;/h4&gt;进行系统评估，识别导致基础模型在计算病理学中失效的七个相互关联原因&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在计算病理学中存在低诊断准确性、鲁棒性差、几何不稳定性、计算需求量大以及安全漏洞等基本弱点&lt;h4&gt;结论&lt;/h4&gt;当前病理学基础模型在概念上与组织形态学性质不一致，需要对范式本身进行根本性重新思考&lt;h4&gt;翻译&lt;/h4&gt;在非医学领域，基础模型(FMs)通过大规模自监督和多模态学习彻底改变了计算机视觉和语言处理。因此，预期计算病理学中会迅速采用这些模型，并在癌症诊断、预后和多模态检索方面取得类似突破。然而，最近的系统评估揭示了基本弱点：低诊断准确性、鲁棒性差、几何不稳定性、计算需求量大，以及令人担忧的安全漏洞。这篇简短论文检查了这些缺点，并论证它们源于主流人工智能中通用基础建模的假设与人体组织内在复杂性之间的更深层次的概念性不匹配。确定了七个相互关联的原因：生物复杂性、无效的自监督、过度概括、过度的架构复杂性、缺乏领域特定创新、数据不足，以及与组织块大小相关的基本设计缺陷。这些发现表明，当前病理学基础模型在概念上仍然与组织形态学的性质不一致，需要对这一范式本身进行根本性的重新思考。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In non-medical domains, foundation models (FMs) have revolutionized computervision and language processing through large-scale self-supervised andmultimodal learning. Consequently, their rapid adoption in computationalpathology was expected to deliver comparable breakthroughs in cancer diagnosis,prognostication, and multimodal retrieval. However, recent systematicevaluations reveal fundamental weaknesses: low diagnostic accuracy, poorrobustness, geometric instability, heavy computational demands, and concerningsafety vulnerabilities. This short paper examines these shortcomings and arguesthat they stem from deeper conceptual mismatches between the assumptionsunderlying generic foundation modeling in mainstream AI and the intrinsiccomplexity of human tissue. Seven interrelated causes are identified:biological complexity, ineffective self-supervision, overgeneralization,excessive architectural complexity, lack of domain-specific innovation,insufficient data, and a fundamental design flaw related to tissue patch size.These findings suggest that current pathology foundation models remainconceptually misaligned with the nature of tissue morphology and call for afundamental rethinking of the paradigm itself.</description>
      <author>example@mail.com (Hamid R. Tizhoosh)</author>
      <guid isPermaLink="false">2510.23807v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting</title>
      <link>http://arxiv.org/abs/2510.23785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了CountFormer，一个基于transformer的框架，用于学习识别重复和结构一致性，实现类别无关的物体计数。该模型使用DINOv2作为视觉编码器，并融入位置嵌入融合，在FSC-147数据集上实现了与当前最先进方法相当的性能，同时在结构复杂或密集场景中表现更优。&lt;h4&gt;背景&lt;/h4&gt;人类能够通过感知视觉重复和结构关系而非依赖类别身份来计数多样化物体，但大多数现有计数模型在物体具有复杂形状、内部对称性或重叠组件时经常计数错误。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够像人类一样通过感知视觉重复和结构关系来进行计数的模型，实现类别无关的物体计数。&lt;h4&gt;方法&lt;/h4&gt;基于CounTR架构，用自监督基础模型DINOv2替换视觉编码器以获得更丰富的特征表示，融入位置嵌入融合保留几何关系，并通过轻量级卷积解码器将特征解码为密度图。&lt;h4&gt;主要发现&lt;/h4&gt;在FSC-147数据集上，CountFormer实现了与当前最先进方法相当的性能，同时在结构复杂或密集堆积的场景中表现出更高的准确性。&lt;h4&gt;结论&lt;/h4&gt;集成基础模型如DINOv2可以使计数系统接近人类的结构感知能力，朝着真正通用和无样本范例的计数范式迈进。&lt;h4&gt;翻译&lt;/h4&gt;人类可以通过感知视觉重复和结构关系而不是依赖类别身份来轻松计数多样化的物体。然而，大多数现有的计数模型无法复制这种能力；当物体表现出复杂形状、内部对称性或重叠组件时，它们经常计数错误。在这项工作中，我们引入了CountFormer，一个基于transformer的框架，用于学习识别重复和结构一致性，实现类别无关的物体计数。基于CounTR架构，我们的模型用自监督基础模型DINOv2替换了其视觉编码器，DINOv2产生更丰富和空间一致的特征表示。我们进一步融合位置嵌入，在通过轻量级卷积解码器将这些特征解码为密度图之前保留几何关系。在FSC-147数据集上评估，我们的模型实现了与当前最先进方法相当的性能，同时在结构复杂或密集堆积的场景中表现出更高的准确性。我们的研究结果表明，集成基础模型如DINOv2可以使计数系统接近人类的结构感知能力，朝着真正通用和无样本范例的计数范式迈进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans can effortlessly count diverse objects by perceiving visual repetitionand structural relationships rather than relying on class identity. However,most existing counting models fail to replicate this ability; they oftenmiscount when objects exhibit complex shapes, internal symmetry, or overlappingcomponents. In this work, we introduce CountFormer, a transformer-basedframework that learns to recognize repetition and structural coherence forclass-agnostic object counting. Built upon the CounTR architecture, our modelreplaces its visual encoder with the self-supervised foundation model DINOv2,which produces richer and spatially consistent feature representations. Wefurther incorporate positional embedding fusion to preserve geometricrelationships before decoding these features into density maps through alightweight convolutional decoder. Evaluated on the FSC-147 dataset, our modelachieves performance comparable to current state-of-the-art methods whiledemonstrating superior accuracy on structurally intricate or densely packedscenes. Our findings indicate that integrating foundation models such as DINOv2enables counting systems to approach human-like structural perception,advancing toward a truly general and exemplar-free counting paradigm.</description>
      <author>example@mail.com (Md Tanvir Hossain, Akif Islam, Mohd Ruhul Ameen)</author>
      <guid isPermaLink="false">2510.23785v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Long-Term Memory for Long-Context Question Answering</title>
      <link>http://arxiv.org/abs/2510.23730v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages including appendix, 3 figures. Submitted to October ARR and  to Metacognition in Generative AI EurIPS workshop (under review for both)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究系统评估了不同类型的记忆增强方法，发现记忆架构复杂度应与模型能力相匹配，不同类型模型适合不同记忆方法，情景记忆可帮助大语言模型识别自身知识局限性。&lt;h4&gt;背景&lt;/h4&gt;大语言模型需要记忆功能实现真正的对话连续性和经验学习，但研究虽已聚焦复杂记忆系统开发，仍不清楚哪种记忆类型对长上下文对话任务最有效。&lt;h4&gt;目的&lt;/h4&gt;使用LoCoMo基准（一个需要多种推理策略的问答任务合成长上下文对话基准）系统评估增强记忆的方法。&lt;h4&gt;方法&lt;/h4&gt;分析四种记忆增强方法：全上下文提示、通过检索增强生成和智能体记忆实现的语义记忆、通过上下文学习实现的情景记忆、通过提示优化的程序记忆。&lt;h4&gt;主要发现&lt;/h4&gt;增强记忆方法可减少90%以上的token使用量同时保持有竞争力准确性；小型基础模型从RAG中获益最多；强大指令微调推理模型通过反思获得情景学习好处并受益于更复杂智能体语义记忆。&lt;h4&gt;结论&lt;/h4&gt;记忆架构复杂度应与模型能力相匹配，情景记忆可以帮助大语言模型识别自身知识的局限性。&lt;h4&gt;翻译&lt;/h4&gt;为了让大语言模型实现真正的对话连续性和从经验学习中受益，它们需要记忆功能。虽然研究已集中在复杂记忆系统的开发上，但目前尚不清楚哪种类型的记忆对长上下文对话任务最有效。我们使用LoCoMo（一个为需要多种推理策略的问答任务标注的合成长上下文对话基准）对增强记忆的方法进行了系统评估。我们分析了全上下文提示、通过检索增强生成和智能体记忆实现的语义记忆、通过上下文学习实现的情景记忆，以及通过提示优化的程序记忆。我们的研究结果表明，增强记忆的方法在保持有竞争力的准确性的同时，可减少90%以上的token使用量。记忆架构的复杂度应与模型能力相匹配，小型基础模型从RAG中获益最多，而强大的指令微调推理模型通过反思获得情景学习的好处，并受益于更复杂的智能体语义记忆。特别是，情景记忆可以帮助大语言模型识别自身知识的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In order for large language models to achieve true conversational continuityand benefit from experiential learning, they need memory. While research hasfocused on the development of complex memory systems, it remains unclear whichtypes of memory are most effective for long-context conversational tasks. Wepresent a systematic evaluation of memory-augmented methods using LoCoMo, abenchmark of synthetic long-context dialogues annotated for question-answeringtasks that require diverse reasoning strategies. We analyse full-contextprompting, semantic memory through retrieval-augmented generation and agenticmemory, episodic memory through in-context learning, and procedural memorythrough prompt optimization. Our findings show that memory-augmented approachesreduce token usage by over 90% while maintaining competitive accuracy. Memoryarchitecture complexity should scale with model capability, with smallfoundation models benefitting most from RAG, and strong instruction-tunedreasoning model gaining from episodic learning through reflections and morecomplex agentic semantic memory. In particular, episodic memory can help LLMsrecognise the limits of their own knowledge.</description>
      <author>example@mail.com (Alessandra Terranova, Björn Ross, Alexandra Birch)</author>
      <guid isPermaLink="false">2510.23730v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents</title>
      <link>http://arxiv.org/abs/2510.23691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Game-TARS是一种基于人类对齐的原生键盘鼠标输入的通用游戏代理，使用统一的可扩展动作空间进行训练，在多种游戏任务上表现出色&lt;h4&gt;背景&lt;/h4&gt;与API或GUI方法不同，需要能够在异构领域(如OS、网络和模拟游戏)进行大规模持续预训练的游戏代理&lt;h4&gt;目的&lt;/h4&gt;开发一种通用游戏代理，通过简单的可扩展动作表示与大规模预训练相结合，实现广泛的计算机使用能力&lt;h4&gt;方法&lt;/h4&gt;Game-TARS在超过500B tokens的多样化轨迹和多模态数据上进行预训练，采用衰减持续损失减少因果混淆，以及高效的稀疏思考策略平衡推理深度和推理成本&lt;h4&gt;主要发现&lt;/h4&gt;在开放世界Minecraft任务上成功率比前SOTA模型高约2倍；在未见过的网络3D游戏中通用性接近新鲜人类；在FPS基准测试中优于GPT-5、Gemini-2.5-Pro和Claude-4-Sonnet；统一动作空间在扩展到跨游戏和多模态数据时能持续改进&lt;h4&gt;结论&lt;/h4&gt;简单的可扩展动作表示与大规模预训练相结合，为具有广泛计算机使用能力的通用代理提供了有前途的发展路径&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Game-TARS，一种通用游戏代理，通过统一的、可扩展的动作空间进行训练，该动作空间锚定在人类对齐的原生键盘鼠标输入上。与基于API或GUI的方法不同，这种范式能够在包括操作系统、网络和模拟游戏在内的异构领域进行大规模持续预训练。Game-TARS在超过500B tokens的多样化轨迹和多模态数据上进行预训练。关键技术包括减少因果混淆的衰减持续损失，以及平衡推理深度和推理成本的高效稀疏思考策略。实验表明，Game-TARS在开放世界Minecraft任务上的成功率比之前的SOTA模型高出约2倍，在未见过的网络3D游戏中接近新鲜人类的通用性，并在FPS基准测试中优于GPT-5、Gemini-2.5-Pro和Claude-4-Sonnet。训练时间和测试时间的扩展结果证实，统一动作空间在扩展到跨游戏和多模态数据时能够持续改进。我们的结果表明，简单的可扩展动作表示与大规模预训练相结合，为具有广泛计算机使用能力的通用代理提供了一条有前途的道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何创建真正可扩展且具有广泛泛化能力的游戏智能体问题。这个问题很重要，因为构建能够与复杂动态数字环境无缝交互的通用人工智能体是实现AGI的关键路径，而视频游戏因其多样化的任务目标和丰富的视觉信息，成为训练和评估此类智能体的理想平台。现有方法在创建具有真正泛化能力的智能体方面仍面临重大挑战，限制了AI系统在开放世界环境中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统智能体的局限性，认识到动作空间与特定环境紧密耦合限制了泛化能力。他们提出将动作空间抽象到更低层次，直接锚定到人类交互的基本输入设备——键盘和鼠标。设计过程借鉴了ReAct范式将推理和动作统一输出，采用Deitke等人的在线思考协议(think-aloud protocol)收集高质量轨迹，使用视觉锚点方法解决多模态数据对齐问题，并在后训练阶段借鉴拒绝采样优化推理-动作链。这些方法基于对现有工作的理解，但进行了创新性整合和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用统一锚定于键盘-鼠标输入的动作空间，结合稀疏思考策略和大规模持续预训练，实现跨领域的泛化能力。整体流程分为两个主要阶段：1)持续预训练阶段：使用统一动作空间收集游戏轨迹，通过在线思考协议收集稀疏ReAct轨迹，使用视觉锚点对齐多模态数据，采用衰减损失函数处理动作分布不平衡，在500B+ token上预训练；2)后训练阶段：通过指令遵循、多模态提示、稀疏思考优化、双层记忆架构和跨域数据整合，提升智能体的核心能力。最终在Minecraft、FPS游戏等未见环境中进行评估验证。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一动作空间：锚定到底层键盘-鼠标输入而非高层API，实现跨环境泛化；2)稀疏思考策略：只在关键决策点推理，通过在线思考协议收集高质量轨迹；3)衰减持续损失：解决动作分布不平衡导致的因果混淆；4)双层记忆架构：结合短期上下文和长期摘要记忆；5)跨域数据整合：将游戏数据与其他领域智能体轨迹结合。相比之前工作，传统API/GUI方法使用定制化动作集与环境紧密耦合，而Game-TARS的统一动作空间具有更好的泛化性；现有游戏智能体通常专注于特定环境，而Game-TARS通过大规模预训练实现了真正的泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Game-TARS通过统一锚定于键盘-鼠标输入的动作空间和大规模持续预训练，实现了在多样化游戏和环境中表现卓越的通用游戏智能体，相比之前的方法展现出显著的泛化能力和性能提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Game-TARS, a generalist game agent trained with a unified,scalable action space anchored to human-aligned native keyboard-mouse inputs.Unlike API- or GUI-based approaches, this paradigm enables large-scalecontinual pre-training across heterogeneous domains, including OS, web, andsimulation games. Game-TARS is pre-trained on over 500B tokens with diversetrajectories and multimodal data. Key techniques include a decaying continualloss to reduce causal confusion and an efficient Sparse-Thinking strategy thatbalances reasoning depth and inference cost. Experiments show that Game-TARSachieves about 2 times the success rate over the previous sota model onopen-world Minecraft tasks, is close to the generality of fresh humans inunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnetin FPS benchmarks. Scaling results on training-time and test-time confirm thatthe unified action space sustains improvements when scaled to cross-game andmultimodal data. Our results demonstrate that simple, scalable actionrepresentations combined with large-scale pre-training provide a promising pathtoward generalist agents with broad computer-use abilities.</description>
      <author>example@mail.com (Zihao Wang, Xujing Li, Yining Ye, Junjie Fang, Haoming Wang, Longxiang Liu, Shihao Liang, Junting Lu, Zhiyong Wu, Jiazhan Feng, Wanjun Zhong, Zili Li, Yu Wang, Yu Miao, Bo Zhou, Yuanfan Li, Hao Wang, Zhongkai Zhao, Faming Wu, Zhengxuan Jiang, Weihao Tan, Heyuan Yao, Shi Yan, Xiangyang Li, Yitao Liang, Yujia Qin, Guang Shi)</author>
      <guid isPermaLink="false">2510.23691v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Explicit Memory through Online 3D Gaussian Splatting Improves Class-Agnostic Video Segmentation</title>
      <link>http://arxiv.org/abs/2510.23521v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE Robotics and Automation Letters September 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用显式3D记忆增强视频分割模型的方法，通过在线3D高斯溅射技术存储预测的对象级片段，显著提高了预测的准确性和一致性。&lt;h4&gt;背景&lt;/h4&gt;现有视频分割算法通常不使用对象级记忆（如FastSAM）或仅使用循环神经网络特征的隐式记忆（如SAM2），而记住过去预测的对象片段位置对提高分割质量至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种显式3D记忆方法来增强现有分割模型，使增强后的模型具有更准确和一致的预测能力。&lt;h4&gt;方法&lt;/h4&gt;开发在线3D高斯溅射（3DGS）技术存储视频过程中生成的预测对象级片段，并基于此开发融合技术FastSAM-Splat和SAM2-Splat，利用显式3DGS记忆改进各自基础模型预测。&lt;h4&gt;主要发现&lt;/h4&gt;消融实验验证了所提技术和超参数设置的有效性；真实世界和模拟基准实验结果表明，使用显式3D记忆的模型比无记忆或仅使用隐式神经网络记忆的模型产生更准确和一致的预测。&lt;h4&gt;结论&lt;/h4&gt;显式3D记忆技术可以显著改善视频分割模型的性能，提高预测的准确性和一致性。&lt;h4&gt;翻译&lt;/h4&gt;记住过去预测的对象片段位置对提高无类别视频分割算法的准确性和一致性是有用的。现有的视频分割算法通常使用不使用对象级记忆（例如FastSAM）或使用循环神经网络特征的隐式记忆（例如SAM2）。在本文中，我们使用显式3D记忆增强这两种分割模型，并证明 resulting 模型具有更准确和一致的预测。为此，我们开发了一种在线3D高斯溅射（3DGS）技术来存储在整个视频持续时间内生成的预测对象级片段。基于这种3DGS表示，开发了一系列融合技术，分别命名为FastSAM-Splat和SAM2-Splat，它们使用显式3DGS记忆来改进各自基础模型的预测。使用消融实验来验证所提技术的设计和超参数设置。来自真实世界和模拟基准实验的结果表明，使用显式3D记忆的模型比不使用记忆或仅使用隐式神经网络记忆的模型产生更准确和一致的预测。项目页面：https://topipari.com/projects/FastSAM-Splat/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决类无关视频分割(class-agnostic video segmentation)的准确性和一致性问题。这个问题在机器人应用中非常重要，因为机器人需要在人类家庭环境中构建有用的语义地图，必须能够检测和分割任何类别的物体（包括部署前未知的物体）。现实环境中的遮挡、低光照、重复和动态物体等因素使得这一挑战更加复杂，而现有的视频分割算法要么不使用物体级记忆，要么使用隐式记忆，导致分割结果在时间维度上不一致，影响机器人对环境的理解和交互。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到记住物体分割在过去的预测位置对提高视频分割一致性很有用，并假设模型如果能访问过去预测的密集3D记忆将会受益。他们借鉴了3D高斯溅射(3DGS)技术，这是一种用于密集3D场景重建的强大表示方法。作者还受到在线3DGS技术的启发，这些技术可以从视频输入中实时构建环境3D地图。他们扩展了3DGS表示，将每个高斯与一个段ID特征向量关联，用于存储语义记忆，并基于对比学习优化段ID码本，确保不同物体段的ID向量之间有足够距离。这种方法结合了现有的视频分割模型(如FastSAM和SAM2)和3D重建技术，创造性地将显式3D记忆引入视频分割任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用显式的3D记忆来增强视频分割模型，以提高分割的一致性和准确性。通过在线3D高斯溅射(3DGS)技术存储物体分割的历史信息，形成一个3D记忆系统，利用这个记忆来指导当前帧的分割预测。整体实现流程包括：1)构建3DGS表示，每个高斯参数包括位置、方向、缩放、不透明度、颜色和段ID特征；2)创建段ID码本，使用对比损失优化确保不同段ID间有足够距离；3)对于FastSAM-Splat，将渲染的3DGS段与FastSAM预测的图像段匹配并融合；4)对于SAM2-Splat，使用SAM2预测的跟踪ID与3DGS段关联，识别不一致并通过重新提示SAM2来纠正错误；5)更新3DGS记忆以对齐当前帧的预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)FastSAM-Splat模型，扩展了原本没有时间记忆的FastSAM，通过集成3DGS记忆提高分割一致性；2)SAM2-Splat模型，开发了基于3DGS的重新提示策略，通过整合显式3D记忆减少SAM2的不一致预测；3)通过实验验证显式3D记忆的优势。相比之前的工作，这种方法使用显式的3D记忆而非无物体级记忆或隐式记忆(如循环神经网络特征)；使用3D高斯溅射存储和表示物体分割历史，这是一种更密集和结构化的表示；专注于机器人应用场景，利用深度和相机姿态信息构建3D记忆；针对不同类型的分割模型(FastSAM和SAM2)设计了不同的融合策略；实验表明在处理遮挡和物体重新出现等挑战性场景时性能更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过引入基于在线3D高斯溅射的显式记忆机制，显著提升了类无关视频分割的准确性和时间一致性，为机器人感知任务提供了更可靠的分割解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3619783&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remembering where object segments were predicted in the past is useful forimproving the accuracy and consistency of class-agnostic video segmentationalgorithms. Existing video segmentation algorithms typically use either noobject-level memory (e.g. FastSAM) or they use implicit memories in the form ofrecurrent neural network features (e.g. SAM2). In this paper, we augment bothtypes of segmentation models using an explicit 3D memory and show that theresulting models have more accurate and consistent predictions. For this, wedevelop an online 3D Gaussian Splatting (3DGS) technique to store predictedobject-level segments generated throughout the duration of a video. Based onthis 3DGS representation, a set of fusion techniques are developed, namedFastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improvetheir respective foundation models' predictions. Ablation experiments are usedto validate the proposed techniques' design and hyperparameter settings.Results from both real-world and simulated benchmarking experiments show thatmodels which use explicit 3D memories result in more accurate and consistentpredictions than those which use no memory or only implicit neural networkmemories. Project Page: https://topipari.com/projects/FastSAM-Splat/</description>
      <author>example@mail.com (Anthony Opipari, Aravindhan K Krishnan, Shreekant Gayaka, Min Sun, Cheng-Hao Kuo, Arnie Sen, Odest Chadwicke Jenkins)</author>
      <guid isPermaLink="false">2510.23521v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Dexbotic: Open-Source Vision-Language-Action Toolbox</title>
      <link>http://arxiv.org/abs/2510.23511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Authors are listed in alphabetical order. The official website is  located at https://dexbotic.com/. Code is available at  https://github.com/Dexmal/dexbotic&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Dexbotic，一个基于PyTorch的开源视觉-语言-行动模型工具箱，为具身智能研究提供一站式服务。&lt;h4&gt;背景&lt;/h4&gt;具身智能领域需要有效的工具来支持视觉-语言-行动模型的研究和开发，现有工具可能缺乏统一性和易用性。&lt;h4&gt;目的&lt;/h4&gt;提供一个开源的、统一的VLA模型工具箱，使研究人员能够轻松复现各种VLA方法，快速开发新实验，并利用更强大的预训练模型提升性能。&lt;h4&gt;方法&lt;/h4&gt;开发了一个基于PyTorch的Dexbotic工具箱，支持多种主流VLA策略，提供实验为中心的开发环境，并开发更强大的预训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;该工具箱能够支持多种VLA策略的统一实现，通过简单的环境设置即可复现各种方法；通过修改Exp脚本可以快速开发新实验；使用更强大的预训练模型可以显著提升最先进VLA策略的性能。&lt;h4&gt;结论&lt;/h4&gt;Dexbotic作为一个开源工具箱，有效简化了VLA模型的研究流程，提高了研究效率，并将持续更新以包含最新的预训练模型和前沿VLA模型。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了Dexbotic，一个基于PyTorch的开源视觉-语言-行动模型工具箱。它旨在为具身智能领域的专业人士提供一站式VLA研究服务。它提供了一个代码库，同时支持多种主流VLA策略，使用户只需通过单一环境设置即可重现各种VLA方法。该工具箱以实验为中心，用户只需修改Exp脚本即可快速开发新的VLA实验。此外，我们提供了更强大的预训练模型，以实现最先进的VLA策略的性能提升。Dexbotic将不断更新，以包含更多最新的预训练基础模型和行业前沿的VLA模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present Dexbotic, an open-source Vision-Language-Action(VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLAresearch service for professionals in the field of embodied intelligence. Itoffers a codebase that supports multiple mainstream VLA policiessimultaneously, allowing users to reproduce various VLA methods with just asingle environment setup. The toolbox is experiment-centric, where the userscan quickly develop new VLA experiments by simply modifying the Exp script.Moreover, we provide much stronger pretrained models to achieve greatperformance improvements for state-of-the-art VLA policies. Dexbotic willcontinuously update to include more of the latest pre-trained foundation modelsand cutting-edge VLA models in the industry.</description>
      <author>example@mail.com (Bin Xie, Erjin Zhou, Fan Jia, Hao Shi, Haoqiang Fan, Haowei Zhang, Hebei Li, Jianjian Sun, Jie Bin, Junwen Huang, Kai Liu, Kaixin Liu, Kefan Gu, Lin Sun, Meng Zhang, Peilong Han, Ruitao Hao, Ruitao Zhang, Saike Huang, Songhan Xie, Tiancai Wang, Tianle Liu, Wenbin Tang, Wenqi Zhu, Yang Chen, Yingfei Liu, Yizhuang Zhou, Yu Liu, Yucheng Zhao, Yunchao Ma, Yunfei Wei, Yuxiang Chen, Ze Chen, Zeming Li, Zhao Wu, Ziheng Zhang, Ziming Liu, Ziwei Yan, Ziyu Zhang)</author>
      <guid isPermaLink="false">2510.23511v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Vulnerability in AI Industry</title>
      <link>http://arxiv.org/abs/2510.23421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preliminary Draft&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种合成AI脆弱性指数(AIVI)，评估Foundation Models(FMs)生产上游价值链的脆弱性，重点关注计算、数据、人才、资本和能源五个关键输入因素。&lt;h4&gt;背景&lt;/h4&gt;Foundation Models(FMs)因Transformer架构的快速发展而推动当前AI生态系统。这些大型模型通过大规模训练和下游适应性获得广泛采用，形成由平台经济学和激烈投资塑造的动荡市场。&lt;h4&gt;目的&lt;/h4&gt;由于数据限制，评估快速发展的AI行业脆弱性具有挑战性。本研究旨在提出一种专注于FM生产上游价值链的合成AI脆弱性指数(AIVI)，优先考虑公开可用数据。&lt;h4&gt;方法&lt;/h4&gt;将FM输出建模为五个输入的函数：计算(Compute)、数据(Data)、人才(Talent)、资本(Capital)和能源(Energy)，假设任何输入的供应脆弱性都会威胁整个行业。使用加权几何平均数聚合子指数，并使用理论或经验基准进行归一化。&lt;h4&gt;主要发现&lt;/h4&gt;关键脆弱性包括：计算集中、数据稀缺和法律风险、人才瓶颈、资本密集度和战略依赖性，以及不断增长的能源需求。&lt;h4&gt;结论&lt;/h4&gt;尽管存在局限性和改进空间，但这一初步指数旨在量化AI核心生产引擎中的系统性风险，并间接揭示下游价值链的风险。&lt;h4&gt;翻译&lt;/h4&gt;Foundation Models(FMs)的快速发展，得益于Transformer架构，推动了当前的AI生态系统。这些大型模型以大规模训练和下游适应性为特征（如GPT系列），已获得广泛采用，促成了由平台经济学和激烈投资塑造的动荡市场。由于数据限制，评估这个快速发展的行业的脆弱性至关重要且具有挑战性。本文提出了一种专注于FM生产上游价值链的合成AI脆弱性指数(AIVI)，优先考虑公开可用数据。我们将FM输出建模为五个输入的函数：计算、数据、人才、资本和能源，并假设任何输入的供应脆弱性都会威胁整个行业。主要脆弱性包括计算集中、数据稀缺和法律风险、人才瓶颈、资本密集度和战略依赖性，以及不断增长的能源需求。考虑到输入的不完全可替代性，我们提出使用加权几何平均数来聚合子指数，并使用理论或经验基准进行归一化。尽管存在局限性和改进空间，但这一初步指数旨在量化AI核心生产引擎中的系统性风险，并间接揭示了下游价值链的风险。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid ascent of Foundation Models (FMs), enabled by the Transformerarchitecture, drives the current AI ecosystem. Characterized by large-scaletraining and downstream adaptability, FMs (as GPT family) have achieved massivepublic adoption, fueling a turbulent market shaped by platform economics andintense investment. Assessing the vulnerability of this fast-evolving industryis critical yet challenging due to data limitations. This paper proposes asynthetic AI Vulnerability Index (AIVI) focusing on the upstream value chainfor FM production, prioritizing publicly available data. We model FM output asa function of five inputs: Compute, Data, Talent, Capital, and Energy,hypothesizing that supply vulnerability in any input threatens the industry.Key vulnerabilities include compute concentration, data scarcity and legalrisks, talent bottlenecks, capital intensity and strategic dependencies, aswell as escalating energy demands. Acknowledging imperfect inputsubstitutability, we propose a weighted geometrical average of aggregatesubindexes, normalized using theoretical or empirical benchmarks. Despitelimitations and room for improvement, this preliminary index aims to quantifysystemic risks in AI's core production engine, and implicitly shed a light onthe risks for downstream value chain.</description>
      <author>example@mail.com (Claudio Pirrone, Stefano Fricano, Gioacchino Fazio)</author>
      <guid isPermaLink="false">2510.23421v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Towards Generalisable Foundation Models for 3D Brain MRI</title>
      <link>http://arxiv.org/abs/2510.23415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BrainFound是一种自监督基础模型，通过扩展DINO-v2视觉转换器构建，专为脑部MRI设计，能够处理3D脑部解剖信息，支持单模态和多模态输入，在标签稀缺和多对比度环境下表现优异，提高了诊断准确性并减少了对专家标注的依赖。&lt;h4&gt;背景&lt;/h4&gt;人工智能基础模型通过大规模无标签数据集实现通用特征学习，正在改变医学影像领域。&lt;h4&gt;目的&lt;/h4&gt;开发一个针对脑部MRI的自监督基础模型，能够处理3D脑部解剖信息，支持多种下游任务。&lt;h4&gt;方法&lt;/h4&gt;通过扩展DINO-v2视觉转换器构建BrainFound，整合连续MRI切片的体积信息来建模完整3D脑部解剖结构，支持单模态和多模态输入，适用于多种MRI模态（如T1、T2、FLAIR）。&lt;h4&gt;主要发现&lt;/h4&gt;BrainFound在性能上始终优于现有的自监督预训练策略和监督基线，特别是在标签稀缺和多对比度设置下；通过整合多种3D MRI模态信息，提高了诊断准确性，减少了对大量专家标注的依赖。&lt;h4&gt;结论&lt;/h4&gt;BrainFound的灵活性使其成为3D神经影像流程的可扩展且实用的解决方案，具有在临床部署和研究创新方面的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;人工智能（AI）中的基础模型正在通过大规模无标签数据集实现通用特征学习，从而改变医学影像。在这项工作中，我们介绍了BrainFound，这是一个用于脑部MRI的自监督基础模型，通过扩展DINO-v2（一种最初为2D自然图像设计的视觉转换器）构建。BrainFound通过整合连续MRI切片的体积信息来适应DINO-v2，以建模完整的3D脑部解剖结构，超越了传统的单切片范式。它支持单模态和多模态输入，能够实现广泛的下游任务，包括疾病检测和图像分割，同时能够在不同的成像协议和临床场景中泛化。我们证明BrainFound在性能上始终优于现有的自监督预训练策略和监督基线，特别是在标签稀缺和多对比度设置下。通过整合多种3D MRI模态（如T1、T2、FLAIR）的信息，它提高了诊断准确性，减少了对大量专家标注的依赖。这种灵活性使BrainFound成为3D神经影像流程的可扩展且实用的解决方案，在临床部署和研究创新方面具有巨大潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何构建通用的基础模型用于3D脑部MRI分析的问题。这个问题在现实中非常重要，因为放射科医生面临巨大工作压力（平均每3-4秒需解读一张图像），导致诊断延迟和错误；深度学习在放射学领域虽潜力巨大，但成功依赖于大量昂贵耗时的标记数据；现有监督模型难以跨领域泛化；而大多数基础模型是为2D自然图像设计，无法有效处理3D医学影像数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了DINO-v2框架（一种为2D自然图像设计的视觉变换器）和自监督学习方法，特别是对比学习和知识蒸馏。设计思路是将DINO-v2从2D扩展到3D，通过处理3D扫描作为2D切片序列；设计支持单模态和多模态MRI输入的架构；将T1、T2和FLAIR扫描堆叠为通道输入（类似RGB图像）；采用多尺度裁剪策略捕获全局和局部脑结构；使用双域预训练（先在自然图像上预训练，再在脑部MRI上微调）。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D脑部MRI作为2D切片序列处理，利用自监督学习在大规模未标记数据上学习通用特征表示，结合自然图像和脑部MRI的双域预训练，并支持多模态输入整合不同MRI对比度的互补信息。整体流程包括：1)收集10,000个体积脑部MRI图像并进行标准化预处理；2)基于DINO-v2构建支持多模态输入的Vision Transformer架构；3)使用多尺度裁剪和自监督知识蒸馏方法进行预训练；4)在下游任务（疾病检测和图像分割）上进行微调应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将DINO-v2从2D扩展到3D脑部MRI处理；2)支持可变体积深度，提高对不同MRI任务的适应性；3)设计能处理单模态和多模态MRI输入的架构，将不同MRI模态堆叠为通道输入；4)采用双域预训练策略，结合自然图像和脑部MRI的优势；5)统一处理疾病检测和图像分割任务。相比之前的工作，BrainFound在多种任务上表现优于仅使用自然图像预训的模型、仅在脑部图像上从头训练的自监督模型以及其他自监督方法，其多模态设计也提供了比单模态模型更强的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BrainFound通过将DINO-v2框架扩展到3D脑部MRI并采用双域预训练策略，创建了一个强大的自监督基础模型，能够有效处理多模态输入并在疾病检测和图像分割任务上实现卓越的泛化性能，减少了对大量标记数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models in artificial intelligence (AI) are transforming medicalimaging by enabling general-purpose feature learning from large-scale,unlabeled datasets. In this work, we introduce BrainFound, a self-supervisedfoundation model for brain MRI, built by extending DINO-v2, a visiontransformer originally designed for 2D natural images. BrainFound adaptsDINO-v2 to model full 3D brain anatomy by incorporating volumetric informationfrom sequential MRI slices, moving beyond conventional single-slice paradigms.It supports both single- and multimodal inputs, enabling a broad range ofdownstream tasks, including disease detection and image segmentation, whilegeneralising across varied imaging protocols and clinical scenarios. We showthat BrainFound consistently outperforms existing self-supervised pretrainingstrategies and supervised baselines, particularly in label-scarce andmulti-contrast settings. By integrating information from diverse 3D MRImodalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reducesdependency on extensive expert annotations. This flexibility makes BrainFound ascalable and practical solution for 3D neuroimaging pipelines, with significantpotential for clinical deployment and research innovation.</description>
      <author>example@mail.com (Moona Mazher, Geoff J. M. Parker, Daniel C. Alexander)</author>
      <guid isPermaLink="false">2510.23415v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Bid2X: Revealing Dynamics of Bidding Environment in Online Advertising from A Foundation Model Lens</title>
      <link>http://arxiv.org/abs/2510.23410v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Bid2X出价基础模型，通过统一的函数估计特定出价下的效果，解决了传统出价模型在不同场景间泛化能力有限的问题。该模型结合了序列嵌入、双注意力机制和零膨胀投影模块，在淘宝广告平台部署后显著提升了广告效果。&lt;h4&gt;背景&lt;/h4&gt;自动出价对在线广告至关重要，但现有出价模型通常针对特定场景设计，在不同环境间的泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;探索场景无关的出价原则，提出一个统一的函数来估计特定出价下的效果，如预算消耗、商品交易总额(GMV)和页面浏览量等。&lt;h4&gt;方法&lt;/h4&gt;提出Bid2X出价基础模型，构建在统一序列嵌入之上，通过定制嵌入方法编码异构数据；提出两种注意力机制分别处理不同变量和不同时间的嵌入；使用变量感知融合模块进行自适应出价结果预测；设计零膨胀投影模块将估计的非零概率纳入值预测，形成包含分类和回归的联合优化目标。&lt;h4&gt;主要发现&lt;/h4&gt;模型已在淘宝广告平台部署；在八个数据集上的离线评估显示Bid2X优于各种基线模型且在不同场景中具有通用性；在线A/B测试中GMV增加4.65%，ROI增加2.44%。&lt;h4&gt;结论&lt;/h4&gt;Bid2X为计算广告中的出价基础模型铺平了道路，展示了基础模型在广告出价领域的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;自动出价对于通过为广告商自动提供出价来促进在线广告至关重要。虽然之前的工作在建模出价环境以获得更好的广告效果方面做出了巨大努力，但这些模型通常针对特定的出价场景定制，在不同环境间的泛化能力存在局限性。为此，我们通过一个统一的函数来探索场景无关的原则，该函数估计特定出价下的效果，如预算消耗、商品交易总额(GMV)、页面浏览量等。然后，我们提出了Bid2X出价基础模型，从各种场景的数据中学习这个基本函数。我们的Bid2X构建在统一序列嵌入之上，通过定制的嵌入方法编码异构数据。为了捕捉出价数据中复杂的变量间动态和时间依赖关系，我们提出了两种注意力机制，分别将不同变量和不同时间的嵌入作为注意力令牌进行表示学习。在学习到的变量和时间表示之上，使用变量感知融合模块进行自适应出价结果预测。为了建模独特的出价数据分布，我们设计了一个零膨胀投影模块，将估计的非零概率纳入其值预测，这构成了一个包含分类和回归的联合优化目标。该目标被证明可以收敛到零膨胀分布。我们的模型已部署在淘宝广告平台上，这是世界上最大的电子商务平台之一。在八个数据集上的离线评估显示，与各种基线相比，Bid2X具有优越性，并且在不同场景中具有通用性。Bid2X在线A/B测试中使GMV增加了4.65%，ROI增加了2.44%，为计算广告中的出价基础模型铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Auto-bidding is crucial in facilitating online advertising by automaticallyproviding bids for advertisers. While previous work has made great efforts tomodel bidding environments for better ad performance, it has limitations ingeneralizability across environments since these models are typically tailoredfor specific bidding scenarios. To this end, we approach thescenario-independent principles through a unified function that estimates theachieved effect under specific bids, such as budget consumption, grossmerchandise volume (GMV), page views, etc. Then, we propose a biddingfoundation model Bid2X to learn this fundamental function from data in variousscenarios. Our Bid2X is built over uniform series embeddings that encodeheterogeneous data through tailored embedding methods. To capture complexinter-variable and dynamic temporal dependencies in bidding data, we proposetwo attention mechanisms separately treating embeddings of different variablesand embeddings at different times as attention tokens for representationlearning. On top of the learned variable and temporal representations, avariable-aware fusion module is used to perform adaptive bidding outcomeprediction. To model the unique bidding data distribution, we devise azero-inflated projection module to incorporate the estimated non-zeroprobability into its value prediction, which makes up a joint optimizationobjective containing classification and regression. The objective is proven toconverge to the zero-inflated distribution. Our model has been deployed on thead platform in Taobao, one of the world's largest e-commerce platforms. Offlineevaluation on eight datasets exhibits Bid2X's superiority compared to variousbaselines and its generality across different scenarios. Bid2X increased GMV by4.65% and ROI by 2.44% in online A/B tests, paving the way for biddingfoundation model in computational advertising.</description>
      <author>example@mail.com (Jiahao Ji, Tianyu Wang, Yeshu Li, Yushen Huo, Zhilin Zhang, Chuan Yu, Jian Xu, Bo Zheng)</author>
      <guid isPermaLink="false">2510.23410v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Solar flare forecasting with foundational transformer models across image, video, and time-series modalities</title>
      <link>http://arxiv.org/abs/2510.23400v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了基于Transformer的架构在利用异构数据模态（包括图像、视频序列和时间序列观测）进行太阳耀斑预测方面的性能&lt;h4&gt;背景&lt;/h4&gt;太阳活动预测对于空间天气预警至关重要，但需要处理多种类型的数据&lt;h4&gt;目的&lt;/h4&gt;评估Transformer主干架构在太阳活动的空间和时间表示方面的优势和局限性&lt;h4&gt;方法&lt;/h4&gt;使用三种预训练模型（SigLIP2用于图像编码，VideoMAE用于时空视频表示，Moirai2用于多元时间序列预测）处理来自SDO/HMI任务的太阳磁图和GOES卫星的软X射线通量数据，并采用多种损失函数和训练平衡策略来处理类别不平衡问题&lt;h4&gt;主要发现&lt;/h4&gt;虽然SigLIP2和VideoMAE在图像和视频数据上达到典型性能（真实技能统计约0.60-0.65），但仅基于辐照度时间演化的Moirai2时间序列模型达到了优越的预测技能（真实技能统计约0.74）&lt;h4&gt;结论&lt;/h4&gt;预训练Transformer架构和跨模态学习对推进业务空间天气预报具有潜力，为整合视觉和时间信息的统一多模态模型铺平了道路&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一项基于Transformer架构的太阳耀斑预测的比较研究，该研究使用异构数据模态，包括图像、视频序列和时间序列观测。我们的分析评估了三个最近的基础模型 - 用于图像编码的SigLIP2，用于时空视频表示的VideoMAE，以及用于多元时间序列预测的Moirai2 - 应用于来自SDO/HMI任务的太阳磁图公共数据集和GOES卫星获取的软X射线通量。所有模型在一致的数据分割和评估标准下进行训练和验证，旨在评估Transformer主干架构在太阳活动的空间和时间表示方面的优势和局限性。我们研究了多种损失公式（加权BCE、focal和分数导向的）和训练平衡策略，以减轻耀斑数据集中典型的类别不平衡。结果表明，虽然SigLIP2和VideoMAE在图像和视频数据上达到典型性能（真实技能统计约0.60-0.65），但时间序列模型Moirai2仅基于辐照度时间演化就达到了优越的预测技能（真实技能统计约0.74）。这些发现突显了预训练Transformer架构和跨模态学习在推进业务空间天气预报方面的潜力，为整合视觉和时间信息的统一多模态模型铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a comparative study of transformer-based architectures for solarflare forecasting using heterogeneous data modalities, including images, videosequences, and time-series observations. Our analysis evaluates three recentfoundational models - SigLIP2 for image encoding, VideoMAE for spatio-temporalvideo representation, and Moirai2 for multivariate time-series forecasting -applied to publicly available datasets of solar magnetograms from the SDO/HMImission and soft X-ray fluxes acquired by GOES satellites. All models aretrained and validated under consistent data splits and evaluation criteria,with the goal of assessing the strengths and limitations of transformerbackbones across spatial and temporal representations of solar activity. Weinvestigate multiple loss formulations (weighted BCE, focal, andscore-oriented) and training balance strategies to mitigate class imbalancetypical of flare datasets. Results show that while both SigLIP2 and VideoMAEachieve typical performance on image and video data (True Skill StatisticTSS~0.60-0.65), the time-series model Moirai2 reaches superior forecastingskill (TSS~0.74) using irradiance-based temporal evolution alone. Thesefindings highlight the potential of pretrained transformer architectures andcross-modal learning for advancing operational space weather forecasting,paving the way toward unified multimodal models that integrate visual andtemporal information.</description>
      <author>example@mail.com (S. Riggi, P. Romano, A. Pilzer, U. Becciani)</author>
      <guid isPermaLink="false">2510.23400v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility Mapping</title>
      <link>http://arxiv.org/abs/2510.23364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint submitted to EUSAR 2026 (under review)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ZeroFlood是一种地理空间基础模型框架，通过思维模态推理实现数据高效的洪水易感性映射，能够在数据稀缺地区从基本地球观测数据进行洪水预测。&lt;h4&gt;背景&lt;/h4&gt;洪水易感性映射对于灾害预防至关重要，但在数据稀缺地区面临挑战，因为传统水动力模型需要密集的地球物理输入数据。&lt;h4&gt;目的&lt;/h4&gt;开发ZeroFlood框架，解决数据稀缺地区洪水易感性映射的问题，提供一种数据高效的解决方案。&lt;h4&gt;方法&lt;/h4&gt;使用思维模态(TiM)推理微调地理空间基础模型(GFMs)，从Sentinel-1或Sentinel-2等基本地球观测数据进行洪水预测；利用数据丰富地区的成对地球观测和模拟洪水地图，通过跨模态表示学习弥合数据差距；使用TerraMind和Prithvi GFMs进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;TiM推理增强了模型的鲁棒性，TerraMind-Large配置实现了67.21的F1分数。&lt;h4&gt;结论&lt;/h4&gt;基于基础模型的FSM是一种可扩展且数据高效的洪水风险管理解决方案，适用于数据稀缺地区。&lt;h4&gt;翻译&lt;/h4&gt;洪水易感性映射(FSM)对灾害预防至关重要，但在需要密集地球物理输入的水动力模型难以应用的稀缺数据地区仍然具有挑战性。本文介绍了ZeroFlood，一种用于数据高效FSM的地理空间基础模型框架。该方法通过思维模态(TiM)推理微调地理空间基础模型(GFMs)，能够从Sentinel-1或Sentinel-2等基本地球观测数据进行洪水预测。利用数据丰富地区的成对地球观测和模拟洪水地图，ZeroFlood通过跨模态表示学习弥合了数据可用性差距。使用TerraMind和Prithvi GFMs的实验表明，TiM增强了模型鲁棒性，其中TerraMind-Large配置实现了67.21的F1分数。结果证明了基于基础模型的FSM作为可扩展和数据高效的洪水风险管理解决方案的可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Flood susceptibility mapping (FSM) is vital for disaster prevention butremains challenging in data-scarce regions where hydrodynamic models requiredense geophysical inputs. This work introduces ZeroFlood, a geospatialfoundation model framework for data-efficient FSM. The approach fine-tunesGeospatial Foundation Models (GFMs) with Thinking-in-Modality (TiM) reasoning,enabling flood prediction from basic Earth observation data such as Sentinel-1or Sentinel-2 imagery. Using paired EO and simulated flood maps from data-richregions, ZeroFlood bridges data availability gaps through cross-modalrepresentation learning. Experiments with TerraMind and Prithvi GFMs show thatTiM enhances model robustness, with the TerraMind-Large configuration achievingan F1 score of 67.21. The results demonstrate the feasibility offoundation-model-based FSM as a scalable and data-efficient solution for floodrisk management.</description>
      <author>example@mail.com (Hyeongkyun Kim, Orestis Oikonomou)</author>
      <guid isPermaLink="false">2510.23364v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Provable test-time adaptivity and distributional robustness of in-context learning</title>
      <link>http://arxiv.org/abs/2510.23254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  44 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了预训练Transformer在不同难度任务上的性能表现，证明其能够达到与任务难度相对应的最优收敛速率，并且对分布偏移具有鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;研究上下文学习问题，其中Transformer在从混合分布中抽取的任务上进行预训练，该混合分布由不同难度级别的任务分布组成。&lt;h4&gt;目的&lt;/h4&gt;理解预训练Transformer在不同于预训练分布的测试分布上的性能，特别是当测试分布与预训练分布中对应难度级别的分布存在卡方散度限制的偏移时。&lt;h4&gt;方法&lt;/h4&gt;考虑非参数回归问题和多指标模型，分析大型预训练Transformer在这些模型上的收敛性能。&lt;h4&gt;主要发现&lt;/h4&gt;预训练Transformer能够达到与任务难度级别相对应的最优收敛速率，并且在卡方散度球内的测试分布上是一致的；Transformer在较容易任务上收敛更快，且对分布偏移具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;预训练Transformer即使面对分布偏移也能保持最优性能，其性能优于理论上能够访问测试分布的估计器，提供了比最小最大下界更合适的最优性保证。&lt;h4&gt;翻译&lt;/h4&gt;我们研究上下文学习问题，其中Transformer在从混合分布中抽取的任务上进行预训练，称为预训练先验，其中每个混合分量是针对特定难度级别的任务分布。我们的目标是理解预训练Transformer在不同于测试分布上的性能表现，该测试分布由固定难度的任务组成，并且相对于对应难度级别的分布可能存在分布偏移，但卡方散度至多为某个常数。特别是，我们考虑具有随机光滑性的非参数回归问题，以及具有随机光滑性和随机有效维度的多指标模型。我们证明，在足够数据上预训练的大型Transformer能够达到与难度级别相对应的最优收敛速率，并且在卡方散度球内的测试分布上是一致的。因此，预训练的Transformer能够在较容易的任务上实现更快的收敛速率，并且对测试时的分布偏移具有鲁棒性。最后，我们证明即使估计器能够访问测试分布，其在测试分布上的期望风险的收敛速率也不会比预训练的Transformer更快，从而提供了比最小最大下界更合适的最优性保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study in-context learning problems where a Transformer is pretrained ontasks drawn from a mixture distribution $\pi=\sum_{\alpha\in\mathcal{A}}\lambda_{\alpha} \pi_{\alpha}$, called the pretraining prior, in which eachmixture component $\pi_{\alpha}$ is a distribution on tasks of a specificdifficulty level indexed by $\alpha$. Our goal is to understand the performanceof the pretrained Transformer when evaluated on a different test distribution$\mu$, consisting of tasks of fixed difficulty $\beta\in\mathcal{A}$, and withpotential distribution shift relative to $\pi_\beta$, subject to thechi-squared divergence $\chi^2(\mu,\pi_{\beta})$ being at most $\kappa$. Inparticular, we consider nonparametric regression problems with randomsmoothness, and multi-index models with random smoothness as well as randomeffective dimension. We prove that a large Transformer pretrained on sufficientdata achieves the optimal rate of convergence corresponding to the difficultylevel $\beta$, uniformly over test distributions $\mu$ in the chi-squareddivergence ball. Thus, the pretrained Transformer is able to achieve fasterrates of convergence on easier tasks and is robust to distribution shift attest time. Finally, we prove that even if an estimator had access to the testdistribution $\mu$, the convergence rate of its expected risk over $\mu$ couldnot be faster than that of our pretrained Transformers, thereby providing amore appropriate optimality guarantee than minimax lower bounds.</description>
      <author>example@mail.com (Tianyi Ma, Tengyao Wang, Richard J. Samworth)</author>
      <guid isPermaLink="false">2510.23254v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?</title>
      <link>http://arxiv.org/abs/2510.23252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This manuscript contains 11 pages, 5 tables and 16 figures This was  accepted at International Joint Conference on Natural Language Processing &amp;  Asia-Pacific Chapter of the Association for Computational Linguistics  (IJCNLP-AACL) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了名为Ben-10的孟加拉语音转文本语料库，研究了方言变化对自动语音识别(ASR)的影响，发现语音基础模型在区域方言ASR中表现不佳，但方言特定模型训练可缓解此问题。&lt;h4&gt;背景&lt;/h4&gt;传统语音识别研究大多使用标准形式处理低资源语言，而区域方言的自动语音识别(ASR)被视为微调任务。&lt;h4&gt;目的&lt;/h4&gt;研究方言变化对自动语音识别(ASR)的影响。&lt;h4&gt;方法&lt;/h4&gt;开发了一个78小时标注的孟加拉语音转文本(STT)语料库，命名为Ben-10，并从语言学和数据驱动角度进行研究。&lt;h4&gt;主要发现&lt;/h4&gt;语音基础模型在区域方言ASR中表现不佳，无论是零样本还是微调设置；所有深度学习方法都难以在方言变化条件下建模语音数据，但方言特定的模型训练可以缓解这一问题。&lt;h4&gt;结论&lt;/h4&gt;该数据集可作为ASR算法在资源受限条件下建模的分布外(OOD)资源。&lt;h4&gt;翻译&lt;/h4&gt;传统语音识别建模研究大多依赖标准形式处理大多数低资源语言，而区域方言的自动语音识别(ASR)被视为微调任务。为研究对方言变化对ASR的影响，我们开发了一个名为Ben-10的78小时标注的孟加拉语音转文本(STT)语料库。从语言学和数据驱动角度的研究表明，语音基础模型在区域方言ASR中表现严重不佳，无论是在零样本还是微调设置下。我们观察到所有深度学习方法都难以在方言变化条件下建模语音数据，但方言特定的模型训练可以缓解这一问题。我们的数据集也可作为ASR算法在资源受限条件下建模的分布外(OOD)资源。该项目开发的数据集和代码已公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional research on speech recognition modeling relies on the canonicalform for most low-resource languages while automatic speech recognition (ASR)for regional dialects is treated as a fine-tuning task. To investigate theeffects of dialectal variations on ASR we develop a 78-hour annotated BengaliSpeech-to-Text (STT) corpus named Ben-10. Investigation from linguistic anddata-driven perspectives shows that speech foundation models struggle heavilyin regional dialect ASR, both in zero-shot and fine-tuned settings. We observethat all deep learning methods struggle to model speech data under dialectalvariations but dialect specific model training alleviates the issue. Ourdataset also serves as a out of-distribution (OOD) resource for ASR modelingunder constrained resources in ASR algorithms. The dataset and code developedfor this project are publicly available</description>
      <author>example@mail.com (Tawsif Tashwar Dipto, Azmol Hossain, Rubayet Sabbir Faruque, Md. Rezuwan Hassan, Kanij Fatema, Tanmoy Shome, Ruwad Naswan, Md. Foriduzzaman Zihad, Mohaymen Ul Anam, Nazia Tasnim, Hasan Mahmud, Md Kamrul Hasan, Md. Mehedi Hasan Shawon, Farig Sadeque, Tahsin Reasat)</author>
      <guid isPermaLink="false">2510.23252v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Finding 3D Scene Analogies with Multimodal Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to FM4RoboPlan workshop at RSS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出使用多模态基础模型在零样本、开放词汇设置下寻找3D场景类比，通过混合神经表示实现复杂场景间的准确对应关系，支持轨迹和航路点转移。&lt;h4&gt;背景&lt;/h4&gt;将当前观察与先验经验连接有助于机器人在新3D环境中适应和规划。3D场景类比可作为平滑映射对齐具有共同空间关系的场景区域，支持轨迹或航路点转移，可用于模仿学习示范转移或跨场景任务规划。&lt;h4&gt;目的&lt;/h4&gt;提出使用多模态基础模型在零样本、开放词汇设置下寻找3D场景类比，避免现有方法需要的额外训练和固定物体词汇表限制。&lt;h4&gt;方法&lt;/h4&gt;采用混合神经表示场景，包括基于视觉语言模型特征的稀疏图和来自3D形状基础模型的特征场。通过粗到细方式寻找3D场景类比，首先对齐图，然后用特征场细化对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够建立复杂场景之间的准确对应关系，并成功应用于轨迹和航路点转移。&lt;h4&gt;结论&lt;/h4&gt;使用多模态基础模型可以在无需额外训练和固定词汇表的情况下实现3D场景类比，为机器人在新环境中的适应和规划提供了有效方法。&lt;h4&gt;翻译&lt;/h4&gt;将当前观察与先验经验连接起来有助于机器人在新的、未见过的3D环境中进行适应和规划。最近，3D场景类比被提出用于连接两个3D场景，这些是平滑的映射，能够对齐具有共同空间关系的场景区域。这些映射可以支持轨迹或航路点的详细转移，可能支持模仿学习的示范转移或跨场景的任务规划转移。然而，现有方法需要额外的训练和固定的物体词汇表。在本文中，我们提出使用多模态基础模型在零样本、开放词汇设置下寻找3D场景类比。我们方法的核心是场景的混合神经表示，包括基于视觉语言模型特征的稀疏图和来自3D形状基础模型的特征场。3D场景类比通过粗到细的方式找到，首先对齐图，然后使用特征场细化对应关系。我们的方法能够建立复杂场景之间的准确对应关系，我们展示了在轨迹和航路点转移中的应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的是如何在没有额外训练和固定词汇表限制的情况下，找到3D场景之间的类比关系。这个问题很重要，因为它能帮助机器人将新环境与已知经验联系起来，从而在未知环境中更好地进行规划和行动。3D场景类比可以创建场景间的平滑映射，支持轨迹或路径点的转移，可用于模仿学习或跨场景的任务规划。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到现有方法要么需要特定领域训练（神经描述符场），要么依赖语义标签（场景图匹配），限制了泛化能力。因此，作者转向利用已在大量多模态数据上训练的基础模型。方法借鉴了视觉语言模型（CLIP）提取对象特征、3D形状模型（PartField）构建特征场、图匹配技术、DBSCAN聚类和薄板样条拟合等现有技术，但将它们创新地组合成一个新的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用多模态基础模型在零样本、开放词汇表设置下寻找3D场景类比，通过混合神经表示（稀疏图+密集特征场）实现从粗到细的场景类比估计。流程包括：1)构建场景图（对象节点+CLIP特征）和特征场（PartField特征）；2)图匹配获得粗粒度对象关联；3)DBSCAN聚类并拟合仿射映射；4)基于特征场优化局部位移；5)用薄板样条拟合得到最终映射。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用多模态基础模型实现零样本3D场景类比；2)提出混合神经表示方法结合稀疏图和密集场；3)采用从粗到细的估计策略；4)支持开放词汇表场景。相比之前工作，不同之处在于：无需特定领域训练（优于神经场景图方法）、不需要预知语义标签（优于场景图匹配方法）、能处理复杂场景且映射准确性更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于多模态基础模型的3D场景类比方法，通过结合视觉语言和3D形状特征的混合表示，实现了零样本、开放词汇表场景下的高精度场景映射，为机器人规划和模仿学习提供了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Connecting current observations with prior experiences helps robots adapt andplan in new, unseen 3D environments. Recently, 3D scene analogies have beenproposed to connect two 3D scenes, which are smooth maps that align sceneregions with common spatial relationships. These maps enable detailed transferof trajectories or waypoints, potentially supporting demonstration transfer forimitation learning or task plan transfer across scenes. However, existingmethods for the task require additional training and fixed object vocabularies.In this work, we propose to use multimodal foundation models for finding 3Dscene analogies in a zero-shot, open-vocabulary setting. Central to ourapproach is a hybrid neural representation of scenes that consists of a sparsegraph based on vision-language model features and a feature field derived from3D shape foundation models. 3D scene analogies are then found in acoarse-to-fine manner, by first aligning the graph and refining thecorrespondence with feature fields. Our method can establish accuratecorrespondences between complex scenes, and we showcase applications intrajectory and waypoint transfer.</description>
      <author>example@mail.com (Junho Kim, Young Min Kim)</author>
      <guid isPermaLink="false">2510.23184v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Guiding Skill Discovery with Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23167v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Foundation model Guided (FoG)的技能发现方法，通过基础模型将人类意图融入技能发现过程，解决了现有方法只关注技能多样性而忽略人类偏好导致不理想行为的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的技能发现方法仅专注于最大化技能多样性，不考虑人类偏好，这会导致不理想甚至危险的行为。例如，使用先前方法训练的猎豹机器人学会向各个方向翻滚以最大化技能多样性，而非我们期望的奔跑行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将人类意图融入技能发现过程的方法，从而学习符合人类偏好的多样化技能，避免不理想和危险行为。&lt;h4&gt;方法&lt;/h4&gt;FoG方法从基础模型中提取评分函数，根据人类意图评估状态，为理想状态赋予更高值，不理想状态赋予更低值。然后使用这些分数重新加权技能发现算法的奖励，通过优化重新加权的奖励来指导技能发现过程。&lt;h4&gt;主要发现&lt;/h4&gt;FoG成功消除了不理想行为，如翻转或翻滚，并在基于状态和基于像素的任务中有效避免了危险区域。此外，该方法能够发现涉及难以定义的行为的技能。&lt;h4&gt;结论&lt;/h4&gt;FoG方法通过将人类意图融入技能发现过程，解决了现有方法只关注多样性而忽略人类偏好的问题，使强化学习能够学习更符合人类期望的技能，从而加速下游任务的强化学习过程。&lt;h4&gt;翻译&lt;/h4&gt;无需手工设计的奖励函数即可学习多样化技能，可以加速下游任务中的强化学习。然而，现有的技能发现方法只专注于最大化技能多样性，而没有考虑人类偏好，这会导致不理想的行为甚至危险技能。例如，使用先前方法训练的猎豹机器人学会向各个方向翻滚以最大化技能多样性，而我们更希望它能够奔跑而不翻转或进入危险区域。在这项工作中，我们提出了一种基础模型引导(FoG)的技能发现方法，通过基础模型将人类意图融入技能发现。具体来说，FoG从基础模型中提取评分函数，根据人类意图评估状态，为理想状态赋予更高值，为不理想状态赋予更低值。然后使用这些分数重新加权技能发现算法的奖励。通过优化重新加权的技能发现奖励，FoG成功消除了不理想行为，如翻转或翻滚，并在基于状态和基于像素的任务中避免了危险区域。有趣的是，我们表明FoG可以发现涉及难以定义的行为的技能。交互式可视化可通过https://sites.google.com/view/submission-fog获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning diverse skills without hand-crafted reward functions couldaccelerate reinforcement learning in downstream tasks. However, existing skilldiscovery methods focus solely on maximizing the diversity of skills withoutconsidering human preferences, which leads to undesirable behaviors andpossibly dangerous skills. For instance, a cheetah robot trained using previousmethods learns to roll in all directions to maximize skill diversity, whereaswe would prefer it to run without flipping or entering hazardous areas. In thiswork, we propose a Foundation model Guided (FoG) skill discovery method, whichincorporates human intentions into skill discovery through foundation models.Specifically, FoG extracts a score function from foundation models to evaluatestates based on human intentions, assigning higher values to desirable statesand lower to undesirable ones. These scores are then used to re-weight therewards of skill discovery algorithms. By optimizing the re-weighted skilldiscovery rewards, FoG successfully learns to eliminate undesirable behaviors,such as flipping or rolling, and to avoid hazardous areas in both state-basedand pixel-based tasks. Interestingly, we show that FoG can discover skillsinvolving behaviors that are difficult to define. Interactive visualisationsare available from https://sites.google.com/view/submission-fog.</description>
      <author>example@mail.com (Zhao Yang, Thomas M. Moerland, Mike Preuss, Aske Plaat, Vincent François-Lavet, Edward S. Hu)</author>
      <guid isPermaLink="false">2510.23167v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Implicit Modeling for Transferability Estimation of Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为隐式可迁移性建模(ITM)的新框架，用于评估预训练模型对下游任务的适用性，无需进行完整的微调过程。&lt;h4&gt;背景&lt;/h4&gt;可迁移性估计能够识别最适合下游任务的预训练模型，避免完整微调的高计算成本，促进模型部署和预训练-微调范式发展。然而，现有方法在评估具有多样化架构、训练策略和任务对齐的新兴预训练模型时，准确性有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确评估各种类型预训练模型可迁移性的新方法，使其能够在更广泛的模型和下游任务上实现泛化。&lt;h4&gt;方法&lt;/h4&gt;提出隐式可迁移性建模(ITM)框架，隐式建模每个模型的内在可迁移性，并结合分治变分近似(DVA)策略来有效近似嵌入空间的演化过程。&lt;h4&gt;主要发现&lt;/h4&gt;在涵盖多种训练策略和模型类型的综合基准测试中，ITM在稳定性、有效性和效率方面持续优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;ITM框架为评估新兴预训练模型的可迁移性提供了更准确、更高效的解决方案，有助于推动预训练和微调范式的发展。&lt;h4&gt;翻译&lt;/h4&gt;可迁移性估计能够识别最适合下游任务的预训练模型，而无需承担完整微调的高计算成本。这种能力促进了部署并推动了预训练和微调范式的发展。然而，现有方法在评估具有多样化架构、训练策略和任务对齐的新兴预训练模型的可迁移性时，往往难以准确评估。在这项工作中，我们提出了隐式可迁移性建模(ITM)，这是一个新颖的框架，它隐式地建模每个模型的内在可迁移性，并结合分治变分近似(DVA)策略来有效近似嵌入空间的演化。这种设计使模型能够在更广泛的模型和下游任务上实现泛化。在涵盖广泛训练策略和更多样化模型类型的综合基准上的大量实验表明，ITM在稳定性、有效性和效率方面持续优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transferability estimation identifies the best pre-trained models fordownstream tasks without incurring the high computational cost of fullfine-tuning. This capability facilitates deployment and advances thepre-training and fine-tuning paradigm. However, existing methods often struggleto accurately assess transferability for emerging pre-trained models withdiverse architectures, training strategies, and task alignments. In this work,we propose Implicit Transferability Modeling (ITM), a novel framework thatimplicitly models each model's intrinsic transferability, coupled with aDivide-and-Conquer Variational Approximation (DVA) strategy to efficientlyapproximate embedding space evolution. This design enables generalizationacross a broader range of models and downstream tasks. Extensive experiments ona comprehensive benchmark--spanning extensive training regimes and a widervariety of model types--demonstrate that ITM consistently outperforms existingmethods in terms of stability, effectiveness, and efficiency.</description>
      <author>example@mail.com (Yaoyan Zheng, Huiqun Wang, Nan Zhou, Di Huang)</author>
      <guid isPermaLink="false">2510.23145v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and Force Feedback</title>
      <link>http://arxiv.org/abs/2510.23119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://isee-laboratory.github.io/OmniDexGrasp/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了OmniDexGrasp框架，结合基础模型与转移控制策略，实现了机器人根据人类命令灵巧抓取和操作物体的通用能力。&lt;h4&gt;背景&lt;/h4&gt;让机器人根据人类命令灵巧地抓取和操作物体是机器人学的一个有前景的方向，但现有方法由于语义灵巧抓取数据集规模有限，难以在不同物体或任务上泛化。&lt;h4&gt;目的&lt;/h4&gt;解决基础模型与物理机器人执行之间的差距问题，开发一个能在用户提示、灵巧抓取和抓取任务方面实现全能力的通用框架。&lt;h4&gt;方法&lt;/h4&gt;OmniDexGrasp集成了三个关键模块：使用基础模型生成人类抓取图像增强泛化能力；人类图像到机器人行动的转移策略实现全灵巧抓取；力感知自适应抓取策略确保稳健稳定的抓取执行。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实机器人上的实验验证了OmniDexGrasp在不同用户提示、抓取任务和灵巧手方面的有效性，且可扩展到灵巧操作任务。&lt;h4&gt;结论&lt;/h4&gt;OmniDexGrasp通过结合基础模型与转移控制策略，显著提升了机器人灵巧抓取和操作能力，具有广泛的适用性和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;使机器人能够根据人类命令灵巧地抓取和操作物体是机器人学中的一个有前景的方向。然而，由于语义灵巧抓取数据集的规模有限，现有方法难以在多样化的物体或任务上泛化。基础模型提供了一种增强泛化的新方法，但由于抽象模型知识与物理机器人执行之间的差距，直接利用它们生成可行的机器人行动仍然具有挑战性。为了解决这些挑战，我们提出了OmniDexGrasp，一个通用框架，通过结合基础模型与转移和控制策略，在用户提示、灵巧抓取和抓取任务方面实现全能力。OmniDexGrasp集成了三个关键模块：(i) 使用基础模型生成人类抓取图像，增强泛化能力，支持用户提示和任务的全能力；(ii) 人类图像到机器人行动的转移策略将人类演示转化为可执行的机器人行动，实现全灵巧抓取；(iii) 力感知自适应抓取策略确保稳健和稳定的抓取执行。在模拟和真实机器人上的实验验证了OmniDexGrasp在不同用户提示、抓取任务和灵巧手方面的有效性，进一步的结果显示其可扩展到灵巧操作任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enabling robots to dexterously grasp and manipulate objects based on humancommands is a promising direction in robotics. However, existing approaches arechallenging to generalize across diverse objects or tasks due to the limitedscale of semantic dexterous grasp datasets. Foundation models offer a new wayto enhance generalization, yet directly leveraging them to generate feasiblerobotic actions remains challenging due to the gap between abstract modelknowledge and physical robot execution. To address these challenges, we proposeOmniDexGrasp, a generalizable framework that achieves omni-capabilities in userprompting, dexterous embodiment, and grasping tasks by combining foundationmodels with the transfer and control strategies. OmniDexGrasp integrates threekey modules: (i) foundation models are used to enhance generalization bygenerating human grasp images supporting omni-capability of user prompt andtask; (ii) a human-image-to-robot-action transfer strategy converts humandemonstrations into executable robot actions, enabling omni dexterousembodiment; (iii) force-aware adaptive grasp strategy ensures robust and stablegrasp execution. Experiments in simulation and on real robots validate theeffectiveness of OmniDexGrasp on diverse user prompts, grasp task and dexteroushands, and further results show its extensibility to dexterous manipulationtasks.</description>
      <author>example@mail.com (Yi-Lin Wei, Zhexi Luo, Yuhao Lin, Mu Lin, Zhizhao Liang, Shuoyu Chen, Wei-Shi Zheng)</author>
      <guid isPermaLink="false">2510.23119v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Eigenfunction Extraction for Ordered Representation Learning</title>
      <link>http://arxiv.org/abs/2510.24672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种通用框架，用于提取有序且可识别的特征函数，解决了现有表示学习方法只能恢复核的前几个特征函数线性张成空间的问题。&lt;h4&gt;背景&lt;/h4&gt;表示学习的最新进展表明，广泛使用的目标（如对比和非对比方法）隐式地对由输入与其上下文之间的关系诱导的上下文核执行谱分解。&lt;h4&gt;目的&lt;/h4&gt;开发一个能提取有序且可识别特征函数的通用框架，满足与上下文核兼容和可扩展到现代设置的需求。&lt;h4&gt;方法&lt;/h4&gt;展示低秩近似和瑞利商优化两种主要方法论范式如何与特征函数提取框架对齐，基于模块化构建块设计。&lt;h4&gt;主要发现&lt;/h4&gt;恢复的特征值可作为特征选择的有效重要性分数，通过自适应维度表示实现原则性的效率-准确性权衡。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在合成核和真实图像数据集上均得到验证，能够提供更精确的特征提取和理解，有助于特征选择和模型效率与准确性的平衡。&lt;h4&gt;翻译&lt;/h4&gt;表示学习的最新进展表明，广泛使用的目标（如对比和非对比方法）隐式地对由输入与其上下文之间的关系诱导的上下文核执行谱分解。然而，这些方法只能恢复核的前几个特征函数的线性张成空间，而精确的谱分解对于理解特征排序和重要性至关重要。在本研究中，我们提出一个通用框架来提取有序且可识别的特征函数，基于满足关键需求的模块化构建块设计，包括与上下文核的兼容性和可扩展到现代设置的能力。然后，我们展示了两种主要方法论范式（低秩近似和瑞利商优化）如何与这一特征函数提取框架对齐。最后，我们在合成核上验证了我们的方法，并在真实图像数据集上证明恢复的特征值可作为特征选择的有效重要性分数，通过自适应维度表示实现原则性的效率-准确性权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in representation learning reveal that widely usedobjectives, such as contrastive and non-contrastive, implicitly performspectral decomposition of a contextual kernel, induced by the relationshipbetween inputs and their contexts. Yet, these methods recover only the linearspan of top eigenfunctions of the kernel, whereas exact spectral decompositionis essential for understanding feature ordering and importance. In this work,we propose a general framework to extract ordered and identifiableeigenfunctions, based on modular building blocks designed to satisfy keydesiderata, including compatibility with the contextual kernel and scalabilityto modern settings. We then show how two main methodological paradigms,low-rank approximation and Rayleigh quotient optimization, align with thisframework for eigenfunction extraction. Finally, we validate our approach onsynthetic kernels and demonstrate on real-world image datasets that therecovered eigenvalues act as effective importance scores for feature selection,enabling principled efficiency-accuracy tradeoffs via adaptive-dimensionalrepresentations.</description>
      <author>example@mail.com (Burak Varıcı, Che-Ping Tsai, Ritabrata Ray, Nicholas M. Boffi, Pradeep Ravikumar)</author>
      <guid isPermaLink="false">2510.24672v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning</title>
      <link>http://arxiv.org/abs/2510.24356v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出感知学习(PeL)范式，使用任务无关信号优化智能体的感官接口，与下游决策学习解耦。PeL直接针对无标签的感知属性，通过表示不变的指标评估，形式化了感知与决策的分离，并证明PeL更新与贝叶斯任务风险梯度正交。&lt;h4&gt;背景&lt;/h4&gt;传统学习中感知和决策通常紧密耦合，限制了智能体发展通用感知能力，因为学习过于关注特定任务表现而忽略基本感知属性。&lt;h4&gt;目的&lt;/h4&gt;提出PeL范式解耦感知与决策；优化智能体感官接口；定义独立于任务目标的感知属性；提供评估感知质量的指标。&lt;h4&gt;方法&lt;/h4&gt;使用任务无关信号优化感官接口；将感知学习与下游决策学习解耦；针对稳定性、信息量、几何结构等感知属性优化；使用表示不变的指标评估；形式化感知与决策分离；证明PeL更新与贝叶斯任务风险梯度正交；提供任务无关评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;感知与决策可成功分离；可定义独立于目标或重新参数化的感知属性；保持不变量的PeL更新与贝叶斯任务风险梯度正交；提供有效评估指标认证感知质量。&lt;h4&gt;结论&lt;/h4&gt;PeL范式为智能体提供新感知学习框架，通过解耦感知与决策，发展更通用、鲁棒的感知能力，提高任务表现并增强环境理解和适应能力。&lt;h4&gt;翻译&lt;/h4&gt;我们引入感知学习(PeL)，一种使用任务无关信号优化智能体感官接口的范式，与下游决策学习解耦。PeL直接针对无标签的感知属性，如对扰动的稳定性、信息量而不崩溃、受控几何结构等，通过表示不变的指标进行评估。我们形式化了感知与决策的分离，定义了独立于目标或重新参数化的感知属性，并证明了保持足够不变量的PeL更新与贝叶斯任务风险梯度正交。此外，我们还提供了一套任务无关的评估指标来认证感知质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Perception Learning (PeL), a paradigm that optimizes an agent'ssensory interface $f_\phi:\mathcal{X}\to\mathcal{Z}$ using task-agnosticsignals, decoupled from downstream decision learning$g_\theta:\mathcal{Z}\to\mathcal{Y}$. PeL directly targets label-freeperceptual properties, such as stability to nuisances, informativeness withoutcollapse, and controlled geometry, assessed via objectiverepresentation-invariant metrics. We formalize the separation of perception anddecision, define perceptual properties independent of objectives orreparameterizations, and prove that PeL updates preserving sufficientinvariants are orthogonal to Bayes task-risk gradients. Additionally, weprovide a suite of task-agnostic evaluation metrics to certify perceptualquality.</description>
      <author>example@mail.com (Suman Sanyal)</author>
      <guid isPermaLink="false">2510.24356v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.24261v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出DynaRend框架，通过可微分体积渲染学习3D感知和动态信息的三平面特征，统一捕获空间几何、未来动态和任务语义，有效提升机器人操作任务的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;由于缺乏多样化的真实世界训练数据，学习可泛化的机器人操作策略仍面临挑战。现有方法要么依赖2D视觉预训练范式关注静态语义或几何，要么利用视频预测模型强调2D动态，无法同时学习操作所需的几何、语义和动态信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够联合学习几何、语义和动态的表示学习框架，解决机器人操作任务中数据稀缺和泛化能力不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出DynaRend表示学习框架，通过掩码重建和未来预测学习三平面特征，在多视图RGB-D视频数据上进行预训练，并通过动作价值图预测将学习到的表示转移到下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;在RLBench和Colosseum基准测试及真实世界实验中，DynaRend在策略成功率、环境扰动泛化能力和多样化任务实际适用性方面均有显著提升。&lt;h4&gt;结论&lt;/h4&gt;DynaRend成功解决了现有方法的局限性，能够有效联合学习几何、语义和动态信息，大幅提高机器人操作策略的泛化能力和实际应用效果。&lt;h4&gt;翻译&lt;/h4&gt;由于缺乏多样化的真实世界训练数据，学习可泛化的机器人操作策略仍然是一个关键挑战。尽管最近的方法尝试通过自监督表示学习来缓解这一问题，但大多数方法要么依赖于2D视觉预训练范式如掩码图像建模，主要关注静态语义或场景几何，要么利用大规模视频预测模型强调2D动态，因此无法有效操作所需的几何、语义和动态的联合学习。在本文中，我们提出DynaRend，一个表示学习框架，通过可微分体积渲染进行掩码重建和未来预测，学习3D感知和动态信息的三平面特征。通过在多视图RGB-D视频数据上预训练，DynaRend能够在统一的三平面表示中同时捕获空间几何、未来动态和任务语义。学习到的表示可以通过动作价值图预测有效地转移到下游机器人操作任务中。我们在两个具有挑战性的基准测试RLBench和Colosseum以及真实世界机器人实验中评估DynaRend，证明了其在策略成功率、对环境扰动的泛化能力以及在不同操作任务中的实际适用性方面都有显著改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人操作策略难以泛化的问题，原因是缺乏多样化的真实世界训练数据。现有方法要么依赖2D视觉预训练（关注静态语义或几何），要么使用视频预测模型（强调2D动态），无法同时学习有效操作所需的几何、语义和动态信息。这个问题很重要，因为机器人操作需要理解3D环境变化，真实世界数据收集成本高，且有效的操作需要综合理解场景结构、物体语义和动态变化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：2D预训练缺乏3D感知，3D方法结构复杂难以扩展，渲染方法需要密集相机设置不实用。作者设计了一个统一框架，通过掩码重建和未来预测学习3D感知和动态信息的三平面特征。借鉴了掩码图像建模、神经渲染、视频预测和三平面表示等技术，但创新性地结合了这些方法，并引入目标视图增强技术提高真实世界适用性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'掩码未来渲染'学习3D感知和动态信息的三平面特征，同时捕获场景几何、语义信息和未来动态变化。流程包括：1)从多视图RGB-D重建点云并投影为三平面特征；2)随机掩码部分特征，通过重建网络恢复当前场景，通过预测网络生成未来场景；3)对重建和预测结果进行体积渲染，用RGB、语义和深度损失进行监督；4)利用预训练模型合成新视图增强监督；5)预训练后添加动作解码器，在下游任务上微调预测动作值图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的3D表示学习框架，联合学习几何、动态和语义；2)掩码未来渲染技术，结合重建和预测两个互补目标；3)目标视图增强技术，减少对密集相机设置的依赖；4)多任务渲染损失，同时优化RGB、语义和深度。相比之前工作，DynaRend具有明确的3D空间感知能力，能捕获3D动态而非2D，使用更高效简洁的三平面表示，且通过目标视图增强提高了真实世界适用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DynaRend通过掩码未来渲染技术，首次在统一的三平面表示中联合学习空间几何、未来动态和任务语义，显著提高了机器人操作策略的泛化能力和环境适应性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning generalizable robotic manipulation policies remains a key challengedue to the scarcity of diverse real-world training data. While recentapproaches have attempted to mitigate this through self-supervisedrepresentation learning, most either rely on 2D vision pretraining paradigmssuch as masked image modeling, which primarily focus on static semantics orscene geometry, or utilize large-scale video prediction models that emphasize2D dynamics, thus failing to jointly learn the geometry, semantics, anddynamics required for effective manipulation. In this paper, we presentDynaRend, a representation learning framework that learns 3D-aware anddynamics-informed triplane features via masked reconstruction and futureprediction using differentiable volumetric rendering. By pretraining onmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, futuredynamics, and task semantics in a unified triplane representation. The learnedrepresentations can be effectively transferred to downstream roboticmanipulation tasks via action value map prediction. We evaluate DynaRend on twochallenging benchmarks, RLBench and Colosseum, as well as in real-world roboticexperiments, demonstrating substantial improvements in policy success rate,generalization to environmental perturbations, and real-world applicabilityacross diverse manipulation tasks.</description>
      <author>example@mail.com (Jingyi Tian, Le Wang, Sanping Zhou, Sen Wang, Jiayi Li, Gang Hua)</author>
      <guid isPermaLink="false">2510.24261v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Debiasing Reward Models by Representation Learning with Guarantees</title>
      <link>http://arxiv.org/abs/2510.23751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种减轻大型语言模型对齐过程中奖励模型偏见的新框架，通过识别和利用非虚假潜在变量来提高模型的稳健性。&lt;h4&gt;背景&lt;/h4&gt;近期基于人类反馈的强化学习等对齐技术被广泛采用，但这些模型常常利用虚假相关性，如响应长度、歧视性、奉承和概念偏见等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个有原则的框架，减轻奖励模型中的偏见，同时保留反映预期潜在偏好的因素。&lt;h4&gt;方法&lt;/h4&gt;提供数据生成过程的公式化，假设观测数据由虚假和非虚假潜在变量生成；使用变分推断来恢复这些变量并利用它们训练奖励模型。&lt;h4&gt;主要发现&lt;/h4&gt;非虚假潜在变量理论上可以从数据中识别出来，无论虚假潜在变量的代理变量是否可用。&lt;h4&gt;结论&lt;/h4&gt;在合成和真实世界数据集上的实验表明，该方法有效减轻了虚假相关性问题，并产生了更稳健的奖励模型。&lt;h4&gt;翻译&lt;/h4&gt;近期的对齐技术，如基于人类反馈的强化学习，已被广泛采用，通过学习和利用奖励模型使大型语言模型与人类偏好保持一致。在实践中，这些模型常常利用虚假相关性，例如响应长度、歧视性、奉承和概念偏见，这个问题已受到越来越多的关注。在这项工作中，我们提出了一个有原则的框架，在减轻奖励模型中这些偏见的同时，保留反映预期潜在偏好的因素。我们首先提供了数据生成过程的公式化，假设观测数据是由虚假和非虚假的潜在变量生成的。我们有趣地表明，这些非虚假的潜在变量理论上可以从数据中识别出来，无论虚假潜在变量的代理变量是否可用。这进一步启发了一种使用变分推断来恢复这些变量并利用它们训练奖励模型的实用方法。在合成和真实世界数据集上的实验表明，我们的方法有效减轻了虚假相关性问题，并产生了更稳健的奖励模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent alignment techniques, such as reinforcement learning from humanfeedback, have been widely adopted to align large language models with humanpreferences by learning and leveraging reward models. In practice, these modelsoften exploit spurious correlations, involving, e.g., response length,discrimination, sycophancy, and conceptual bias, which is a problem that hasreceived increasing attention. In this work, we propose a principled frameworkthat mitigates these biases in reward models while preserving the underlyingfactors that reflect intended preferences. We first provide a formulation ofthe data-generating process, assuming that the observed data (e.g., text) isgenerated from both spurious and non-spurious latent variables. We show that,interestingly, these non-spurious latent variables can be theoreticallyidentified from data, regardless of whether a surrogate for the spurious latentvariables is available. This further inspires a practical method that usesvariational inference to recover these variables and leverages them to trainreward models. Experiments on synthetic and real-world datasets demonstratethat our method effectively mitigates spurious correlation issues and yieldsmore robust reward models.</description>
      <author>example@mail.com (Ignavier Ng, Patrick Blöbaum, Siddharth Bhandari, Kun Zhang, Shiva Kasiviswanathan)</author>
      <guid isPermaLink="false">2510.23751v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Manifold Approximation leads to Robust Kernel Alignment</title>
      <link>http://arxiv.org/abs/2510.22953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures + supplementary&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的流形近似核对齐(MKA)方法，用于改进现有的中心化核对齐(CKA)度量方法，使其能够考虑底层流形几何，从而提供更稳健的表征测量基础。&lt;h4&gt;背景&lt;/h4&gt;中心化核对齐(CKA)是一种流行的度量方法，广泛应用于比较表征、确定网络等价性和神经科学研究。然而，CKA存在局限性，它没有考虑底层流形，并且依赖于许多启发式方法，导致在不同数据尺度下表现不一致。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够考虑流形几何的核对齐方法，以克服CKA的局限性，提供更稳健的表征测量基础。&lt;h4&gt;方法&lt;/h4&gt;提出流形近似核对齐(MKA)方法，将流形几何整合到对齐任务中，并推导了MKA的理论框架。在合成数据集和真实世界示例上进行经验评估，以表征和比较MKA及其当代方法。&lt;h4&gt;主要发现&lt;/h4&gt;考虑流形的核对齐为测量表征提供了更稳健的基础，在表征学习中有潜在应用价值。&lt;h4&gt;结论&lt;/h4&gt;MKA方法通过考虑底层流形，克服了CKA的局限性，为表征比较提供了更可靠的理论和实践基础。&lt;h4&gt;翻译&lt;/h4&gt;中心化核对齐(CKA)是一种流行的度量方法，用于比较表征、确定网络等价性和神经科学研究。然而，CKA没有考虑底层流形，并且依赖于许多启发式方法，导致它在不同数据尺度下表现不同。在这项工作中，我们提出了流形近似核对齐(MKA)，将流形几何整合到对齐任务中。我们推导了MKA的理论框架。我们在合成数据集和真实世界示例上进行经验评估，以表征和比较MKA及其当代方法。我们的研究结果表明，考虑流形的核对齐为测量表征提供了更稳健的基础，在表征学习中有潜在应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Centered kernel alignment (CKA) is a popular metric for comparingrepresentations, determining equivalence of networks, and neuroscienceresearch. However, CKA does not account for the underlying manifold and relieson numerous heuristics that cause it to behave differently at different scalesof data. In this work, we propose Manifold approximated Kernel Alignment (MKA),which incorporates manifold geometry into the alignment task. We derive atheoretical framework for MKA. We perform empirical evaluations on syntheticdatasets and real-world examples to characterize and compare MKA to itscontemporaries. Our findings suggest that manifold-aware kernel alignmentprovides a more robust foundation for measuring representations, with potentialapplications in representation learning.</description>
      <author>example@mail.com (Mohammad Tariqul Islam, Du Liu, Deblina Sarkar)</author>
      <guid isPermaLink="false">2510.22953v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Switchable Token-Specific Codebook Quantization For Face Image Compression</title>
      <link>http://arxiv.org/abs/2510.22943v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种可切换的特定token码本量化方法，用于面部图像压缩，通过为不同类别图像学习不同码本组并为每个token分配独立码本，提高了低比特率下的重建性能。&lt;h4&gt;背景&lt;/h4&gt;随着视觉数据量不断增加，高效无损的传输及其解释理解成为现代信息系统的关键瓶颈。现有的基于码本的解决方案使用全局共享码本，忽视了面部图像内部的类别相关性和token间的语义差异，导致低比特率下性能不佳。&lt;h4&gt;目的&lt;/h4&gt;解决全局码本策略在面部图像压缩中的局限性，特别是在低比特率(bpp)情况下的性能问题，提高重建图像的质量和识别准确率。&lt;h4&gt;方法&lt;/h4&gt;提出可切换的特定token码本量化方法，为不同图像类别学习不同的码本组，为每个token分配独立码本，并用少量比特记录每个token所属的码本组，以减少码本组减小时造成的损失。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用少量比特记录token所属的码本组，可以在较低总bpp下拥有更多码本总数，增强表达能力并提高重建性能。该方法在面部识别数据集上表现出色，0.05 bpp下重建图像的平均准确率达到93.51%。&lt;h4&gt;结论&lt;/h4&gt;所提出的可切换特定token码本量化方法有效解决了全局码本策略在面部图像压缩中的局限性，特别是在低比特率情况下，且具有良好的通用性，可集成到现有基于码本的表示学习方法中。&lt;h4&gt;翻译&lt;/h4&gt;随着视觉数据量的不断增加，高效无损的传输及其随后的解释理解已成为现代信息系统的关键瓶颈。新兴的基于码本的解决方案利用全局共享码本对每个token进行量化和反量化，通过调整token数量或码本大小来控制bpp。然而，对于富含属性的面部图像，这种全局码本策略忽视了图像内部的类别特定相关性以及token之间的语义差异，导致性能不佳，特别是在低bpp情况下。受这些观察的启发，我们提出了一种用于面部图像压缩的可切换特定token码本量化方法，该方法为不同图像类别学习不同的码本组，并为每个token分配独立的码本。通过用少量比特记录每个token所属的码本组，我们的方法可以减少减小每个码本组大小时造成的损失。这使得在较低的总bpp下可以拥有更多的码本总数，从而增强表达能力并提高重建性能。由于其通用设计，我们的方法可以集成到任何现有的基于码本的表示学习方法中，并在面部识别数据集上证明了其有效性，在0.05 bpp下重建图像的平均准确率达到93.51%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the ever-increasing volume of visual data, the efficient and losslesstransmission, along with its subsequent interpretation and understanding, hasbecome a critical bottleneck in modern information systems. The emergedcodebook-based solution utilize a globally shared codebook to quantize anddequantize each token, controlling the bpp by adjusting the number of tokens orthe codebook size. However, for facial images, which are rich in attributes,such global codebook strategies overlook both the category-specificcorrelations within images and the semantic differences among tokens, resultingin suboptimal performance, especially at low bpp. Motivated by theseobservations, we propose a Switchable Token-Specific Codebook Quantization forface image compression, which learns distinct codebook groups for differentimage categories and assigns an independent codebook to each token. Byrecording the codebook group to which each token belongs with a small number ofbits, our method can reduce the loss incurred when decreasing the size of eachcodebook group. This enables a larger total number of codebooks under a loweroverall bpp, thereby enhancing the expressive capability and improvingreconstruction performance. Owing to its generalizable design, our method canbe integrated into any existing codebook-based representation learning approachand has demonstrated its effectiveness on face recognition datasets, achievingan average accuracy of 93.51% for reconstructed images at 0.05 bpp.</description>
      <author>example@mail.com (Yongbo Wang, Haonan Wang, Guodong Mu, Ruixin Zhang, Jiaqi Chen, Jingyun Zhang, Jun Wang, Yuan Xie, Zhizhong Zhang, Shouhong Ding)</author>
      <guid isPermaLink="false">2510.22943v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Transformers from Compressed Representations</title>
      <link>http://arxiv.org/abs/2510.23665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了TEMPEST方法，它利用压缩文件的字节流结构设计了一种有效的标记化和编码策略，使标准变换器可以直接从压缩数据流中学习语义表示，绕过原始字节级处理或完全媒体解码的需求。&lt;h4&gt;背景&lt;/h4&gt;压缩文件格式是高效数据存储和传输的基石，但它们在表示学习方面的潜力在很大程度上尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，能够直接从压缩数据中学习语义表示，同时减少计算复杂性和内存使用。&lt;h4&gt;方法&lt;/h4&gt;TEMPEST（TransformErs froM comPressed rEpreSenTations）利用压缩文件固有的字节流结构设计有效的标记化和编码策略，使标准变换器可以直接从压缩数据流中学习语义表示。&lt;h4&gt;主要发现&lt;/h4&gt;TEMPEST显著减少了语义分类所需的标记数量，从而降低了计算复杂性和内存使用。通过在不同数据集、编码方案和模态上的大量实验，TEMPEST实现了与最先进技术相当的准确性，同时在内存和计算方面提供了效率提升。&lt;h4&gt;结论&lt;/h4&gt;TEMPEST是一种有效的方法，可以直接从压缩数据中学习语义表示，同时保持高准确性和提高效率。&lt;h4&gt;翻译&lt;/h4&gt;压缩文件格式是高效数据存储和传输的基石，但它们在表示学习方面的潜力在很大程度上尚未被探索。我们介绍了TEMPEST（来自压缩表示的变换器），这是一种利用压缩文件固有字节流结构设计有效标记化和编码策略的方法。通过利用这种紧凑编码，标准变换器可以直接从压缩数据流中学习语义表示，绕过原始字节级处理或完全媒体解码的需求。我们的提议显著减少了语义分类所需的标记数量，从而降低了计算复杂性和内存使用。通过在不同数据集、编码方案和模态上的大量实验，我们表明TEMPEST实现了与最先进技术相当的准确性，同时在内存和计算方面提供了效率提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compressed file formats are the corner stone of efficient data storage andtransmission, yet their potential for representation learning remains largelyunderexplored. We introduce TEMPEST (TransformErs froM comPressedrEpreSenTations), a method that exploits the inherent byte-stream structure ofcompressed files to design an effective tokenization and encoding strategy. Byleveraging this compact encoding, a standard transformer can directly learnsemantic representations from compressed data streams, bypassing the need forraw byte-level processing or full media decoding. Our proposal substantiallyreduces the number of tokens required for semantic classification, therebylowering both computational complexity and memory usage. Through extensiveexperiments across diverse datasets, coding schemes, and modalities, we showthat TEMPEST achieves accuracy competitive wit the state-of-the-art whiledelivering efficiency gains in memory and compute.</description>
      <author>example@mail.com (Juan C. Leon Alcazar, Mattia Soldan, Mohammad Saatialsoruji, Alejandro Pardo, Hani Itani, Juan Camilo Perez, Bernard Ghanem)</author>
      <guid isPermaLink="false">2510.23665v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Learning Without Augmenting: Unsupervised Time Series Representation Learning via Frame Projections</title>
      <link>http://arxiv.org/abs/2510.22655v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at the Conference on Neural Information Processing Systems  (NeurIPS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用正交基和过完备帧替代传统数据增强的自监督学习方法，通过联合利用不同流形的互补几何特性，在多个时间序列任务上实现了高达15-20%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;自监督学习(SSL)是一种无需标记数据即可学习表征的强大范式，但大多数SSL方法依赖于强大、成熟、手工制作的数据增强技术，这需要领域特定知识并可能限制模型的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种无监督表征学习方法，用正交基和过完备帧生成视图来替代传统的数据增强技术，避免其带来的限制。&lt;h4&gt;方法&lt;/h4&gt;使用正交基和过完备帧生成视图替代传统数据增强，联合利用从正交和过完备空间学习到的不同流形上的互补几何特性。&lt;h4&gt;主要发现&lt;/h4&gt;从正交和过完备空间学习到的嵌入位于由不同空间中表示样本所引入的几何偏差形成的不同流形上，通过联合利用这些不同流形的互补几何特性，可以在不通过强增强人为增加数据多样性的情况下实现优越性能。&lt;h4&gt;结论&lt;/h4&gt;在五个时间序列任务上的九个数据集上证明了该方法的有效性，在这些信号特性使得数据增强具有挑战性的任务上，不依赖于增强引起的多样性，实现了高达15-20%的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习(SSL)已成为一种无需标记数据即可学习表征的强大范式。大多数SSL方法依赖于强大、成熟、手工制作的数据增强来为表征学习生成多样化视图。然而，设计此类增强需要领域特定知识，并隐式地对模型施加表征不变性，这可能限制泛化能力。在这项工作中，我们提出了一种无监督表征学习方法，使用正交基和过完备帧生成视图来替代数据增强。我们表明，从正交和过完备空间学习到的嵌入位于不同的流形上，这些流形由在不同空间中表示样本所引入的几何偏差形成。通过联合利用这些不同流形的互补几何，我们的方法在不通过强增强人为增加数据多样性的情况下实现了优越性能。我们在五个时间序列任务上的九个数据集上证明了该方法的有效性，在这些任务中，信号特定特性使得数据增强特别具有挑战性。在不依赖于增强引起的多样性的情况下，我们的方法相比现有自监督方法实现了高达15-20%的性能提升。源代码：https://github.com/eth-siplab/Learning-with-FrameProjections&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has emerged as a powerful paradigm forlearning representations without labeled data. Most SSL approaches rely onstrong, well-established, handcrafted data augmentations to generate diverseviews for representation learning. However, designing such augmentationsrequires domain-specific knowledge and implicitly imposes representationalinvariances on the model, which can limit generalization. In this work, wepropose an unsupervised representation learning method that replacesaugmentations by generating views using orthonormal bases and overcompleteframes. We show that embeddings learned from orthonormal and overcompletespaces reside on distinct manifolds, shaped by the geometric biases introducedby representing samples in different spaces. By jointly leveraging thecomplementary geometry of these distinct manifolds, our approach achievessuperior performance without artificially increasing data diversity throughstrong augmentations. We demonstrate the effectiveness of our method on ninedatasets across five temporal sequence tasks, where signal-specificcharacteristics make data augmentations particularly challenging. Withoutrelying on augmentation-induced diversity, our method achieves performancegains of up to 15--20\% over existing self-supervised approaches. Source code:https://github.com/eth-siplab/Learning-with-FrameProjections</description>
      <author>example@mail.com (Berken Utku Demirel, Christian Holz)</author>
      <guid isPermaLink="false">2510.22655v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DynaCausal: Dynamic Causality-Aware Root Cause Analysis for Distributed Microservices</title>
      <link>http://arxiv.org/abs/2510.22613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DynaCausal是一种动态因果感知框架，用于分布式微服务系统的根本原因分析，通过统一多模态动态信号和动态对比机制，有效解决了级联故障传播建模不足、噪声干扰和概念漂移以及过度依赖服务偏离强度等挑战。&lt;h4&gt;背景&lt;/h4&gt;云原生微服务支持快速迭代和可扩展部署，但创建了复杂且快速演化的依赖关系，对可靠诊断构成挑战。现有的根本原因分析方法在捕捉动态行为和变化的服务关系方面有限。&lt;h4&gt;目的&lt;/h4&gt;解决三个关键挑战：级联故障传播建模不足、噪声干扰和概念漂移影响、以及过度依赖服务偏离强度掩盖真正根本原因的问题。&lt;h4&gt;方法&lt;/h4&gt;提出DynaCausal框架，统一多模态动态信号通过交互感知表征学习捕捉时空依赖关系，引入动态对比机制分离故障指标与噪声，采用因果优先的成对排序目标优化因果归因。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准上的评估显示，DynaCausal持续超越最先进方法，平均AC@1达到0.63，绝对增益从0.25到0.46，在高度动态的微服务环境中提供准确且可解释的诊断。&lt;h4&gt;结论&lt;/h4&gt;DynaCausal通过动态因果感知的方法有效解决了微服务系统故障诊断中的关键挑战，实现了更准确和可解释的根本原因分析。&lt;h4&gt;翻译&lt;/h4&gt;云原生微服务支持快速迭代和可扩展部署，但也创建了复杂且快速演化的依赖关系，对可靠诊断构成挑战。现有的根本原因分析方法，即使融合了日志、追踪和指标等多模态数据，在捕捉动态行为和变化的服务关系方面仍然有限。三个关键挑战仍然存在：(i)级联故障传播建模不足，(ii)容易受到正常服务行为中噪声干扰和概念漂移的影响，(iii)过度依赖服务偏离强度掩盖了真正的根本原因。为解决这些挑战，我们提出了DynaCausal，一种用于分布式微服务系统RCA的动态因果感知框架。DynaCausal统一多模态动态信号，通过交互感知的表征学习捕捉时空依赖关系。它进一步引入了动态对比机制，将真正的故障指标与上下文噪声分离，并采用因果优先的成对排序目标，明确优化因果归因。在公共基准上的全面评估表明，DynaCausal持续超越最先进的方法，平均AC@1达到0.63，绝对增益从0.25到0.46，在高度动态的微服务环境中提供准确且可解释的诊断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cloud-native microservices enable rapid iteration and scalable deployment butalso create complex, fast-evolving dependencies that challenge reliablediagnosis. Existing root cause analysis (RCA) approaches, even with multi-modalfusion of logs, traces, and metrics, remain limited in capturing dynamicbehaviors and shifting service relationships. Three critical challengespersist: (i) inadequate modeling of cascading fault propagation, (ii)vulnerability to noise interference and concept drift in normal servicebehavior, and (iii) over-reliance on service deviation intensity that obscurestrue root causes. To address these challenges, we propose DynaCausal, a dynamiccausality-aware framework for RCA in distributed microservice systems.DynaCausal unifies multi-modal dynamic signals to capture time-varyingspatio-temporal dependencies through interaction-aware representation learning.It further introduces a dynamic contrastive mechanism to disentangle true faultindicators from contextual noise and adopts a causal-prioritized pairwiseranking objective to explicitly optimize causal attribution. Comprehensiveevaluations on public benchmarks demonstrate that DynaCausal consistentlysurpasses state-of-the-art methods, attaining an average AC@1 of 0.63 withabsolute gains from 0.25 to 0.46, and delivering both accurate andinterpretable diagnoses in highly dynamic microservice environments.</description>
      <author>example@mail.com (Songhan Zhang, Aoyang Fang, Yifan Yang, Ruiyi Cheng, Xiaoying Tang, Pinjia He)</author>
      <guid isPermaLink="false">2510.22613v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Random Search Neural Networks for Efficient and Expressive Graph Learning</title>
      <link>http://arxiv.org/abs/2510.22520v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NEURIPS 2025; version with full appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的随机搜索神经网络(RSNNs)方法，用于解决随机游走神经网络(RWNNs)在图表示学习中的局限性。RSNNs通过保证完全节点覆盖的随机搜索，显著降低了采样复杂度，并在理论和实验上都表现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;随机游走神经网络(RWNNs)已成为图表示学习的一种有前景的方法，利用序列模型处理随机游走。然而，在现实采样约束下，RWNNs往往无法捕捉全局结构，即使在小型图中也是如此，这是因为节点和边覆盖不完整，限制了其表达能力。&lt;h4&gt;目的&lt;/h4&gt;解决RWNNs在捕捉全局结构方面的局限性，提高图表示学习的效率和表达能力，特别是在稀疏图上的表现。&lt;h4&gt;方法&lt;/h4&gt;提出随机搜索神经网络(RSNNs)，在保证完全节点覆盖的随机搜索上运行。理论上证明在稀疏图中，只需O(log |V|)次搜索即可实现完全边覆盖，显著低于RWNNs所需的O(|V|)次游走。RSNNs与通用序列模型配对时是通用近似器，且对图同构具有概率不变性。&lt;h4&gt;主要发现&lt;/h4&gt;在稀疏图中，RSNNs只需O(log |V|)次搜索就能实现完全边覆盖，比RWNNs的O(|V|)次游走大幅降低采样复杂度。RSNNs是通用近似器，且对图同构具有概率不变性。实验表明，RSNNs在分子和蛋白质基准测试中持续优于RWNNs，使用最多减少16倍的采样序列就能达到相当或更好的性能。&lt;h4&gt;结论&lt;/h4&gt;RSNNs弥合了基于随机游走方法在理论和实践上的差距，为稀疏图上的学习提供了一种高效且具有表达力的框架，在保持高性能的同时显著降低了计算复杂度。&lt;h4&gt;翻译&lt;/h4&gt;随机游走神经网络(RWNNs)已成为图表示学习的一种有前景的方法，利用序列模型的最新进展来处理随机游走。然而，在现实采样约束下，RWNNs往往无法捕捉全局结构，即使在小型图中也是如此，这是由于节点和边覆盖不完整，限制了其表达能力。为解决这一问题，我们提出了随机搜索神经网络(RSNNs)，它在随机搜索上运行，每个搜索都能保证完全的节点覆盖。理论上，我们证明了在稀疏图中，只需O(log |V|)次搜索就能实现完全边覆盖，与RWNNs所需的O(|V|)次游走相比，显著降低了采样复杂度（假设游走长度随图大小缩放）。此外，当与通用序列模型配对时，RSNNs是通用近似器。最后，我们证明了RSNNs对图同构具有概率不变性，确保其期望是同构不变的图函数。实验上，RSNNs在分子和蛋白质基准测试中持续优于RWNNs，使用最多减少16倍的采样序列就能实现相当或更好的性能。我们的工作弥合了基于随机游走方法在理论和实践上的进展，为在稀疏图上的学习提供了一种高效且具有表达力的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Random walk neural networks (RWNNs) have emerged as a promising approach forgraph representation learning, leveraging recent advances in sequence models toprocess random walks. However, under realistic sampling constraints, RWNNsoften fail to capture global structure even in small graphs due to incompletenode and edge coverage, limiting their expressivity. To address this, wepropose \textit{random search neural networks} (RSNNs), which operate on randomsearches, each of which guarantees full node coverage. Theoretically, wedemonstrate that in sparse graphs, only $O(\log |V|)$ searches are needed toachieve full edge coverage, substantially reducing sampling complexity comparedto the $O(|V|)$ walks required by RWNNs (assuming walk lengths scale with graphsize). Furthermore, when paired with universal sequence models, RSNNs areuniversal approximators. We lastly show RSNNs are probabilistically invariantto graph isomorphisms, ensuring their expectation is an isomorphism-invariantgraph function. Empirically, RSNNs consistently outperform RWNNs on molecularand protein benchmarks, achieving comparable or superior performance with up to16$\times$ fewer sampled sequences. Our work bridges theoretical and practicaladvances in random walk based approaches, offering an efficient and expressiveframework for learning on sparse graphs.</description>
      <author>example@mail.com (Michael Ito, Danai Koutra, Jenna Wiens)</author>
      <guid isPermaLink="false">2510.22520v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MAGIC-Flow: Multiscale Adaptive Conditional Flows for Generation and Interpretable Classification</title>
      <link>http://arxiv.org/abs/2510.22070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MAGIC-Flow，一种条件多尺度归一化流架构，在单一模块化框架内同时执行生成和分类任务，解决了生成式建模在医学影像等困难领域应用时缺乏任务对齐的问题。&lt;h4&gt;背景&lt;/h4&gt;生成式建模已成为表示学习的强大范式，但其在医学影像等困难领域的直接应用仍然有限，仅进行生成而不考虑任务对齐，无法为临床应用提供稳健的基础。&lt;h4&gt;目的&lt;/h4&gt;提出MAGIC-Flow，一种条件多尺度归一化流架构，在单一模块化框架内执行生成和分类，以解决生成式建模在临床应用中的局限性。&lt;h4&gt;方法&lt;/h4&gt;MAGIC-Flow构建为可逆和可微双射的层次结构，其中雅可比行列式在子变换中因子化，确保了似然的精确计算和稳定的优化。通过基于类标签的条件化，支持可控样本合成和原则性类概率估计，同时可逆性使得样本似然的显式可视化成为可能，为模型推理提供了可解释的视角。&lt;h4&gt;主要发现&lt;/h4&gt;MAGIC-Flow在相似性、保真度和多样性指标上与顶级基线相当，在多个数据集上解决了扫描噪声下的生成和分类问题，以及模态特定的合成和识别。结果显示MAGIC-Flow创建了真实、多样的样本并改进了分类性能。&lt;h4&gt;结论&lt;/h4&gt;MAGIC-Flow是数据有限领域中生成和分类的有效策略，对隐私保护增强、鲁棒泛化和可信医疗AI有直接益处。&lt;h4&gt;翻译&lt;/h4&gt;生成式建模已成为表示学习的强大范式，但其在医学影像等困难领域的直接应用仍然有限：仅进行生成而不考虑任务对齐，无法为临床应用提供稳健的基础。我们提出MAGIC-Flow，一种条件多尺度归一化流架构，在单一模块化框架内执行生成和分类。该模型构建为可逆和可微双射的层次结构，其中雅可比行列式在子变换中因子化。我们展示了这如何确保似然的精确计算和稳定的优化，同时可逆性使得样本似然的显式可视化成为可能，为模型推理提供了可解释的视角。通过基于类标签的条件化，MAGIC-Flow支持可控样本合成和原则性类概率估计，有效地辅助生成性和判别性目标。我们使用相似性、保真度和多样性指标将MAGIC-Flow与顶级基线进行比较。在多个数据集上，它解决了扫描噪声下的生成和分类，以及模态特定的合成和识别。结果表明MAGIC-Flow创建了真实、多样的样本并改进了分类。MAGIC-Flow是数据有限领域中生成和分类的有效策略，对隐私保护增强、鲁棒泛化和可信医疗AI有直接益处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling has emerged as a powerful paradigm for representationlearning, but its direct applicability to challenging fields like medicalimaging remains limited: mere generation, without task alignment, fails toprovide a robust foundation for clinical use. We propose MAGIC-Flow, aconditional multiscale normalizing flow architecture that performs generationand classification within a single modular framework. The model is built as ahierarchy of invertible and differentiable bijections, where the Jacobiandeterminant factorizes across sub-transformations. We show how this ensuresexact likelihood computation and stable optimization, while invertibilityenables explicit visualization of sample likelihoods, providing aninterpretable lens into the model's reasoning. By conditioning on class labels,MAGIC-Flow supports controllable sample synthesis and principledclass-probability estimation, effectively aiding both generative anddiscriminative objectives. We evaluate MAGIC-Flow against top baselines usingmetrics for similarity, fidelity, and diversity. Across multiple datasets, itaddresses generation and classification under scanner noise, andmodality-specific synthesis and identification. Results show MAGIC-Flow createsrealistic, diverse samples and improves classification. MAGIC-Flow is aneffective strategy for generation and classification in data-limited domains,with direct benefits for privacy-preserving augmentation, robustgeneralization, and trustworthy medical AI.</description>
      <author>example@mail.com (Luca Caldera, Giacomo Bottacini, Lara Cavinato)</author>
      <guid isPermaLink="false">2510.22070v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability</title>
      <link>http://arxiv.org/abs/2510.22039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to Annual Conference on Neural Information Processing  Systems (NeurIPS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了在部分可观测环境中，通过整合预测编码模块到元强化学习中，以学习更紧凑、可解释的贝叶斯最优表示，从而提高代理的适应性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;在部分可观测环境中，学习历史信息的紧凑表示对规划和泛化至关重要。现有元强化学习代理虽能接近贝叶斯最优策略，但往往无法学习到紧凑且可解释的贝叶斯最优信念状态，这种表示效率低下限制了代理的适应性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;研究将自监督预测编码模块整合到元强化学习中，是否能够促进贝叶斯最优表示的学习，从而提高代理在部分可观测环境中的表现。&lt;h4&gt;方法&lt;/h4&gt;受神经科学中预测编码和深度强化学习中辅助预测目标的启发，作者将预测编码模块整合到元强化学习框架中，并通过状态机模拟进行了实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;带有预测模块的元强化学习能够生成更可解释的表示，更好地近似贝叶斯最优信念状态；在需要主动信息获取的挑战性任务中，只有带有预测模块的元强化学习成功学习了最优表示和策略；更好的表示学习能够提高泛化能力。&lt;h4&gt;结论&lt;/h4&gt;预测学习作为代理在部分可观测环境中有效表示学习的指导原则具有重要价值，能够显著提升代理的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;学习历史信息的紧凑表示对于部分可观测环境中的规划和泛化至关重要。虽然元强化学习代理能够获得接近贝叶斯最优的策略，但它们往往无法学习到紧凑、可解释的贝叶斯最优信念状态。这种表示效率低下可能限制了代理的适应性和泛化能力。受神经科学中预测编码的启发——它表明大脑预测感觉输入是贝叶斯推断的神经实现——以及深度强化学习中的辅助预测目标，我们研究了将自监督预测编码模块整合到元强化学习中是否有助于学习贝叶斯最优表示。通过状态机模拟，我们表明带有预测模块的元强化学习能够生成更可解释的表示，更好地近似贝叶斯最优信念状态，与传统的元强化学习相比，在多种任务中表现一致，即使两者都实现了最优策略。在需要主动信息获取的挑战性任务中，只有带有预测模块的元强化学习成功学习了最优表示和策略，而传统元强化学习在表示学习方面表现不足。最后，我们证明了更好的表示学习能够提高泛化能力。我们的结果强烈表明，预测学习作为代理在部分可观测环境中有效表示学习的指导原则具有重要价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning a compact representation of history is critical for planning andgeneralization in partially observable environments. While meta-reinforcementlearning (RL) agents can attain near Bayes-optimal policies, they often fail tolearn the compact, interpretable Bayes-optimal belief states. Thisrepresentational inefficiency potentially limits the agent's adaptability andgeneralization capacity. Inspired by predictive coding in neuroscience--whichsuggests that the brain predicts sensory inputs as a neural implementation ofBayesian inference--and by auxiliary predictive objectives in deep RL, weinvestigate whether integrating self-supervised predictive coding modules intometa-RL can facilitate learning of Bayes-optimal representations. Through statemachine simulation, we show that meta-RL with predictive modules consistentlygenerates more interpretable representations that better approximateBayes-optimal belief states compared to conventional meta-RL across a widevariety of tasks, even when both achieve optimal policies. In challenging tasksrequiring active information seeking, only meta-RL with predictive modulessuccessfully learns optimal representations and policies, whereas conventionalmeta-RL struggles with inadequate representation learning. Finally, wedemonstrate that better representation learning leads to improvedgeneralization. Our results strongly suggest the role of predictive learning asa guiding principle for effective representation learning in agents navigatingpartial observability.</description>
      <author>example@mail.com (Po-Chen Kuo, Han Hou, Will Dabney, Edgar Y. Walker)</author>
      <guid isPermaLink="false">2510.22039v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Orbital Minimization Method for Neural Operator Decomposition</title>
      <link>http://arxiv.org/abs/2510.21952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 8 figures. To appear at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究重新审视了轨道最小化方法(OMM)，这是一种来自计算物理学的经典优化框架，用于训练神经网络分解线性算子，展示了其在现代机器学习中的实用性和优势。&lt;h4&gt;背景&lt;/h4&gt;谱分解线性算子在机器学习和科学计算中扮演核心角色。近期工作探索了训练神经网络来近似这些算子的特征函数，为表示学习、动力系统和偏微分方程提供了可扩展的方法。&lt;h4&gt;目的&lt;/h4&gt;证明轨道最小化方法(OMM)在现代学习流程中的更广泛应用性，并将其调整为训练神经网络分解正半定算子的框架。&lt;h4&gt;方法&lt;/h4&gt;重新审视轨道最小化方法(OMM)，提供OMM目标一致性的简单线性代数证明，揭示此方法与其他领域独立出现的思想之间的联系，并调整该框架用于训练神经网络。&lt;h4&gt;主要发现&lt;/h4&gt;轨道最小化方法在一系列基准任务中展示了实际优势，通过现代理论和计算的角度重新审视经典数值方法，可以为数值模拟中部署神经网络提供原则性方法，同时为机器学习提供有效且可扩展的工具。&lt;h4&gt;结论&lt;/h4&gt;重新审视经典数值方法可以为机器学习和科学计算提供新的视角和实用工具，扩展了神经网络在数值模拟和机器学习中的应用范围。&lt;h4&gt;翻译&lt;/h4&gt;线性算子的谱分解在机器学习和科学计算的许多领域中扮演着核心角色。近期工作探索了训练神经网络来近似这些算子的特征函数，使表示学习、动力系统和偏微分方程(PDEs)的方法具有可扩展性。在本文中，我们重新审视了来自计算物理学文献的一个经典优化框架，称为轨道最小化方法(OMM)，最初在1990年代提出用于计算化学中的特征值问题。我们提供了OMM目标一致性的简单线性代数证明，并揭示了此方法与不同领域独立出现的几种思想之间的联系。我们的主要目标是证明它在现代学习流程中的更广泛应用性。我们将这一框架调整为训练神经网络来分解正半定算子，并在一系列基准任务中展示了其实际优势。我们的结果强调了如何通过现代理论和计算的角度重新审视经典数值方法，不仅可以为在数值模拟中部署神经网络提供原则性方法，还可以为机器学习提供有效且可扩展的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spectral decomposition of linear operators plays a central role in many areasof machine learning and scientific computing. Recent work has explored trainingneural networks to approximate eigenfunctions of such operators, enablingscalable approaches to representation learning, dynamical systems, and partialdifferential equations (PDEs). In this paper, we revisit a classicaloptimization framework from the computational physics literature known as the\emph{orbital minimization method} (OMM), originally proposed in the 1990s forsolving eigenvalue problems in computational chemistry. We provide a simplelinear-algebraic proof of the consistency of the OMM objective, and revealconnections between this method and several ideas that have appearedindependently across different domains. Our primary goal is to justify itsbroader applicability in modern learning pipelines. We adapt this framework totrain neural networks to decompose positive semidefinite operators, anddemonstrate its practical advantages across a range of benchmark tasks. Ourresults highlight how revisiting classical numerical methods through the lensof modern theory and computation can provide not only a principled approach fordeploying neural networks in numerical simulation, but also effective andscalable tools for machine learning.</description>
      <author>example@mail.com (J. Jon Ryu, Samuel Zhou, Gregory W. Wornell)</author>
      <guid isPermaLink="false">2510.21952v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning</title>
      <link>http://arxiv.org/abs/2510.23640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了MuMo，一种结构化多模态融合框架，解决了分子表示中的3D构象不稳定性和模态崩溃问题，提高了模型的鲁棒性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;多模态分子模型通常受到3D构象不可靠性和模态崩溃的限制，这影响了它们的鲁棒性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;设计MuMo框架，解决分子表示中的3D构象不稳定性和模态崩溃问题，提高分子表示的质量。&lt;h4&gt;方法&lt;/h4&gt;1) 设计结构化融合管道（SFP），将2D拓扑和3D几何结合成统一且稳定的结构先验；2) 引入渐进注入（PI）机制，非对称地将先验整合到序列流中，保留模态特定建模同时实现跨模态增强；3) 基于状态空间主干构建，支持长程依赖建模和鲁棒信息传播。&lt;h4&gt;主要发现&lt;/h4&gt;在TDC和MoleculeNet的29个基准任务上，MuMo平均比最佳基线提高2.7%，在22个任务中排名第一，包括在LD50任务上提高27%，验证了模型对3D构象噪声的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;MuMo框架通过结构化多模态融合有效解决了分子表示中的3D构象不稳定性和模态崩溃问题，在各种任务上表现出色。&lt;h4&gt;翻译&lt;/h4&gt;多模态分子模型通常受到3D构象不可靠性和模态崩溃的限制，限制了它们的鲁棒性和泛化能力。我们提出了MuMo，一种结构化多模态融合框架，通过两个关键策略解决了分子表示中的这些挑战。为了减少构象依赖融合的不稳定性，我们设计了一个结构化融合管道（SFP），将2D拓扑和3D几何结合成统一且稳定的结构先验。为了缓解朴素融合引起的模态崩溃，我们引入了渐进注入（PI）机制，非对称地将此先验整合到序列流中，同时保留模态特定建模并实现跨模态增强。基于状态空间主干构建，MuMo支持长程依赖建模和鲁棒信息传播。在来自Therapeutics Data Commons（TDC）和MoleculeNet的29个基准任务上，MuMo在每个任务上平均比最佳基线提高2.7%，在22个任务中排名第一，包括在LD50任务上提高27%。这些结果验证了它对3D构象噪声的鲁棒性以及多模态融合在分子表示中的有效性。代码可在github.com/selmiss/MuMo获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal molecular models often suffer from 3D conformer unreliability andmodality collapse, limiting their robustness and generalization. We proposeMuMo, a structured multimodal fusion framework that addresses these challengesin molecular representation through two key strategies. To reduce theinstability of conformer-dependent fusion, we design a Structured FusionPipeline (SFP) that combines 2D topology and 3D geometry into a unified andstable structural prior. To mitigate modality collapse caused by naive fusion,we introduce a Progressive Injection (PI) mechanism that asymmetricallyintegrates this prior into the sequence stream, preserving modality-specificmodeling while enabling cross-modal enrichment. Built on a state spacebackbone, MuMo supports long-range dependency modeling and robust informationpropagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) andMoleculeNet, MuMo achieves an average improvement of 2.7% over thebest-performing baseline on each task, ranking first on 22 of them, including a27% improvement on the LD50 task. These results validate its robustness to 3Dconformer noise and the effectiveness of multimodal fusion in molecularrepresentation. The code is available at: github.com/selmiss/MuMo.</description>
      <author>example@mail.com (Zihao Jing, Yan Sun, Yan Yi Li, Sugitha Janarthanan, Alana Deng, Pingzhao Hu)</author>
      <guid isPermaLink="false">2510.23640v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models in Dermatopathology: Skin Tissue Classification</title>
      <link>http://arxiv.org/abs/2510.21664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究比较了两种基础模型(UNI和Virchow2)在皮肤病理学全切片图像分类中的表现，发现Virchow2特征提取器表现略优，使用逻辑回归分类器可达到90%的准确率。研究还探索了数据增强和图像归一化技术，并证实平均聚合方法能有效生成切片级特征表示。&lt;h4&gt;背景&lt;/h4&gt;皮肤病理学中全切片图像(WSI)的快速生成需要自动化方法进行高效处理和准确分类。&lt;h4&gt;目的&lt;/h4&gt;评估两种基础模型(UNI和Virchow2)作为特征提取器，用于将WSI分类为三种诊断类别：黑素细胞性、基底样性和鳞状病变。&lt;h4&gt;方法&lt;/h4&gt;使用平均聚合策略将块级嵌入聚合成切片级特征；训练多种机器学习分类器，包括逻辑回归、梯度提升树和随机森林模型；使用精确度、召回率、真正例率、假正例率和AUROC评估性能；探索数据增强技术和图像归一化；使用WandB.ai跟踪和可视化实验结果。&lt;h4&gt;主要发现&lt;/h4&gt;使用Virchow2提取的块级特征在大多数切片级分类器上优于通过UNI提取的特征；使用Virchow2的逻辑回归模型达到了最高的准确率(90%)，但差异不具有统计学意义；平均聚合方法提供了可靠的切片级特征表示。&lt;h4&gt;结论&lt;/h4&gt;这项研究强调了基础模型在自动化WSI分类中的潜力；为皮肤病理学诊断提供了一种可扩展且有效的方法；为切片级表示学习的未来进展铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;皮肤病理学中全切片图像(WSI)的快速生成需要自动化方法进行高效处理和准确分类。本研究评估了两种基础模型UNI和Virchow2作为特征提取器的性能，用于将WSI分类为三种诊断类别：黑素细胞性、基底样性和鳞状病变。使用平均聚合策略将块级嵌入聚合成切片级特征，并随后用于训练多种机器学习分类器，包括逻辑回归、梯度提升树和随机森林模型。使用精确度、召回率、真正例率、假正例率和接收者操作特征曲线下面积(AUROC)在测试集上评估性能。结果表明，使用Virchow2提取的块级特征在大多数切片级分类器上优于通过UNI提取的特征，其中使用Virchow2的逻辑回归达到了最高的准确率(90%)，尽管差异不具有统计学意义。该研究还探索了数据增强技术和图像归一化，以提高模型的鲁棒性和泛化能力。平均聚合方法提供了可靠的切片级特征表示。所有实验结果和指标都使用WandB.ai进行跟踪和可视化，促进了可重复性和可解释性。这项研究强调了基础模型在自动化WSI分类中的潜力，为皮肤病理学诊断提供了一种可扩展且有效的方法，同时为切片级表示学习的未来进展铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid generation of whole-slide images (WSIs) in dermatopathologynecessitates automated methods for efficient processing and accurateclassification. This study evaluates the performance of two foundation models,UNI and Virchow2, as feature extractors for classifying WSIs into threediagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-levelembeddings were aggregated into slide-level features using a mean-aggregationstrategy and subsequently used to train multiple machine learning classifiers,including logistic regression, gradient-boosted trees, and random forestmodels. Performance was assessed using precision, recall, true positive rate,false positive rate, and the area under the receiver operating characteristiccurve (AUROC) on the test set. Results demonstrate that patch-level featuresextracted using Virchow2 outperformed those extracted via UNI across mostslide-level classifiers, with logistic regression achieving the highestaccuracy (90%) for Virchow2, though the difference was not statisticallysignificant. The study also explored data augmentation techniques and imagenormalization to enhance model robustness and generalizability. Themean-aggregation approach provided reliable slide-level featurerepresentations. All experimental results and metrics were tracked andvisualized using WandB.ai, facilitating reproducibility and interpretability.This research highlights the potential of foundation models for automated WSIclassification, providing a scalable and effective approach fordermatopathological diagnosis while paving the way for future advancements inslide-level representation learning.</description>
      <author>example@mail.com (Riya Gupta, Yiwei Zong, Dennis H. Murphree)</author>
      <guid isPermaLink="false">2510.21664v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Causality Meets Locality: Provably Generalizable and Scalable Policy Learning for Networked Systems</title>
      <link>http://arxiv.org/abs/2510.21427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 (Spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出GSAC框架，结合因果表示学习和元actor-critic学习，解决了大规模网络系统（如交通、电力和无线网格）中强化学习的规模和环境变化挑战。&lt;h4&gt;背景&lt;/h4&gt;大规模网络系统（如交通、电力和无线网格）对强化学习代理提出了规模和环境变化的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够同时实现可扩展性和领域泛化的框架，解决大规模网络系统中的强化学习挑战。&lt;h4&gt;方法&lt;/h4&gt;提出GSAC（Generalizable and Scalable Actor-Critic）框架，每个代理学习稀疏局部因果掩码识别关键变量，生成状态和领域因素的紧凑表示（ACRs），元actor-critic训练跨多个源领域的共享策略，测试时通过少量轨迹估计新领域因素并部署自适应策略。&lt;h4&gt;主要发现&lt;/h4&gt;建立了因果恢复、actor-critic收敛和自适应差距的有限样本保证，GSAC能够快速适应并显著优于传统方法。&lt;h4&gt;结论&lt;/h4&gt;GSAC框架有效解决了大规模网络系统中的强化学习挑战，实现了可扩展性和领域泛化。&lt;h4&gt;翻译&lt;/h4&gt;大规模网络系统，如交通、电力和无线网格，对强化学习代理提出了规模和环境变化的挑战。为应对这些挑战，我们提出了GSAC（Generalizable and Scalable Actor-Critic）框架，该框架结合因果表示学习和元actor-critic学习，以实现可扩展性和领域泛化。每个代理首先学习一个稀疏的局部因果掩码，可识别影响其动态的最小邻域变量，生成状态和领域因素的指数紧致近似紧凑表示（ACRs）。这些ACRs限制了将值函数截断到κ-跳邻域的误差，实现在图上的高效学习。元actor-critic则在多个源领域上训练共享策略，同时基于紧凑的领域因素进行条件化；在测试时，只需少量轨迹即可估计新的领域因素并部署自适应策略。我们建立了因果恢复、actor-critic收敛和自适应差距的有限样本保证，并表明GSAC能够快速适应且显著优于从头学习和传统自适应基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale networked systems, such as traffic, power, and wireless grids,challenge reinforcement-learning agents with both scale and environment shifts.To address these challenges, we propose GSAC (Generalizable and ScalableActor-Critic), a framework that couples causal representation learning withmeta actor-critic learning to achieve both scalability and domaingeneralization. Each agent first learns a sparse local causal mask thatprovably identifies the minimal neighborhood variables influencing itsdynamics, yielding exponentially tight approximately compact representations(ACRs) of state and domain factors. These ACRs bound the error of truncatingvalue functions to $\kappa$-hop neighborhoods, enabling efficient learning ongraphs. A meta actor-critic then trains a shared policy across multiple sourcedomains while conditioning on the compact domain factors; at test time, a fewtrajectories suffice to estimate the new domain factor and deploy the adaptedpolicy. We establish finite-sample guarantees on causal recovery, actor-criticconvergence, and adaptation gap, and show that GSAC adapts rapidly andsignificantly outperforms learning-from-scratch and conventional adaptationbaselines.</description>
      <author>example@mail.com (Hao Liang, Shuqing Shi, Yudi Zhang, Biwei Huang, Yali Du)</author>
      <guid isPermaLink="false">2510.21427v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Disentangled Representation Learning via Modular Compositional Bias</title>
      <link>http://arxiv.org/abs/2510.21402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种组合偏置方法，用于解决解耦表示学习中的归纳偏置问题。通过随机混合潜在变量并根据特定规则重组，该方法能够在不修改架构或目标的情况下实现属性、对象甚至两者的解耦。&lt;h4&gt;背景&lt;/h4&gt;当前的解耦表示学习方法主要依赖于特定策略的归纳偏置，包括为属性学习特定目标或为对象设计特定架构。这种依赖性在新的变化因素与先验假设不匹配或多个因素共存时会导致显著开销。&lt;h4&gt;目的&lt;/h4&gt;提出一种组合偏置，一种与目标和架构都解耦的模块化归纳偏置，解决当多个因素共存时需要重新设计架构或目标的问题。&lt;h4&gt;方法&lt;/h4&gt;根据不同因素在数据分布中的不同重组规则（全局属性互斥，对象共享公共支持），随机混合潜在变量。通过两个互补目标强制编码器发现混合策略反映的因素结构：(i)先验损失确保每个混合解码为真实图像；(ii)组合一致性损失将复合图像与其对应的复合潜在变量对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在通用框架下，只需调整混合策略即可实现属性、对象甚至两者的解耦，无需修改目标或架构。实验表明该方法在属性和对象解耦方面具有竞争性性能，且唯一实现了全局风格和对象的联合解耦。&lt;h4&gt;结论&lt;/h4&gt;提出的组合偏置框架提供了一种灵活的方法来处理不同类型的解耦表示学习任务，只需调整混合策略而无需修改架构或目标。&lt;h4&gt;翻译&lt;/h4&gt;最近的解耦表示学习方法严重依赖于特定因素策略-无论是为属性学习目标还是为对象设计架构-以嵌入归纳偏置。这种不同的方法在新的变化因素与先验假设（如统计独立性或空间排他性）不匹配或多个因素共存时会导致显著开销，因为从业者必须重新设计架构或目标。为此，我们提出了一种组合偏置，一种与目标和架构都解耦的模块化归纳偏置。我们的关键见解是，不同因素在数据分布中遵循不同的重组规则：全局属性是互斥的，例如一张脸只有一个鼻子，而对象共享公共支持（任何对象子集都可以共存）。因此，我们根据特定规则随机混合潜在变量，即混合策略，并通过两个互补目标强制编码器发现混合策略反映的任何因素结构：(i)确保每个混合解码为真实图像的先验损失；(ii)Wiedemer等人(arXiv:2310.05327)引入的组合一致性损失，它将每个复合图像与其对应的复合潜在变量对齐。在这一通用框架下，只需调整混合策略即可实现属性、对象甚至两者的解耦，而无需修改目标或架构。大量实验表明，我们的方法在属性和对象解耦方面都表现出竞争性性能，并且唯一实现了全局风格和对象的联合解耦。代码可在https://github.com/whieya/Compositional-DRL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent disentangled representation learning (DRL) methods heavily rely onfactor specific strategies-either learning objectives for attributes or modelarchitectures for objects-to embed inductive biases. Such divergent approachesresult in significant overhead when novel factors of variation do not alignwith prior assumptions, such as statistical independence or spatialexclusivity, or when multiple factors coexist, as practitioners must redesignarchitectures or objectives. To address this, we propose a compositional bias,a modular inductive bias decoupled from both objectives and architectures. Ourkey insight is that different factors obey distinct recombination rules in thedata distribution: global attributes are mutually exclusive, e.g., a face hasone nose, while objects share a common support (any subset of objects canco-exist). We therefore randomly remix latents according to factor-specificrules, i.e., a mixing strategy, and force the encoder to discover whicheverfactor structure the mixing strategy reflects through two complementaryobjectives: (i) a prior loss that ensures every remix decodes into a realisticimage, and (ii) the compositional consistency loss introduced by Wiedemer etal. (arXiv:2310.05327), which aligns each composite image with itscorresponding composite latent. Under this general framework, simply adjustingthe mixing strategy enables disentanglement of attributes, objects, and evenboth, without modifying the objectives or architectures. Extensive experimentsdemonstrate that our method shows competitive performance in both attribute andobject disentanglement, and uniquely achieves joint disentanglement of globalstyle and objects. Code is available athttps://github.com/whieya/Compositional-DRL.</description>
      <author>example@mail.com (Whie Jung, Dong Hoon Lee, Seunghoon Hong)</author>
      <guid isPermaLink="false">2510.21402v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Models Meet Text-Attributed Graphs: A Survey of Integration Frameworks and Applications</title>
      <link>http://arxiv.org/abs/2510.21131v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Surveys and overviews; Natural language processing; Knowledge  representation and reasoning; Graph algorithms&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述从编排的角度系统地回顾了大型语言模型和文本属性图的集成研究，展示了两者结合带来的互补优势。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在自然语言处理方面取得了显著成功，但它们的黑盒性质限制了结构化和多跳推理能力；文本属性图提供了明确的文本关系结构，但往往缺乏语义深度。&lt;h4&gt;目的&lt;/h4&gt;首次从编排的角度系统地回顾LLM和TAG的集成，总结现有方法，并指出未来在语言和图学习交叉领域的研究方向。&lt;h4&gt;方法&lt;/h4&gt;引入新的分类法涵盖两个基本方向：LLM用于TAG（丰富基于图的任务）和TAG用于LLM（改进LLM推理）；将编排策略分为顺序、并行和多模块框架；讨论TAG特定的预训练、提示和参数高效微调的进展。&lt;h4&gt;主要发现&lt;/h4&gt;结合LLMs和TAGs可以产生互补的好处：增强TAG表示学习和提高LLMs的推理能力和可解释性。&lt;h4&gt;结论&lt;/h4&gt;总结了经验性见解，整理了可用数据集，强调了在推荐系统、生物医学分析和知识密集型问答等领域的应用，并指出了开放的挑战和有希望的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型通过强大的语义理解和生成在自然语言处理方面取得了显著成功。然而，它们的黑盒性质限制了结构化和多跳推理能力。相比之下，文本属性图提供了丰富的文本关系结构，但往往缺乏语义深度。最近的研究表明，结合LLMs和TAGs可以带来互补的好处：增强TAG表示学习并提高LLMs的推理能力和可解释性。这篇综述从编排的角度首次系统地回顾了LLM和TAG的集成。我们介绍了一种新的分类法，涵盖两个基本方向：LLM用于TAG，即LLMs丰富基于图的任务；以及TAG用于LLM，即结构化图改进LLM推理。我们将编排策略分为顺序、并行和多模块框架，并讨论了TAG特定的预训练、提示和参数高效微调的进展。除了方法论，我们还总结了经验性见解，整理了可用数据集，并强调了在推荐系统、生物医学分析和知识密集型问答等领域的多样化应用。最后，我们指出了开放的挑战和有希望的研究方向，旨在指导未来在语言和图学习交叉领域的工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have achieved remarkable success in naturallanguage processing through strong semantic understanding and generation.However, their black-box nature limits structured and multi-hop reasoning. Incontrast, Text-Attributed Graphs (TAGs) provide explicit relational structuresenriched with textual context, yet often lack semantic depth. Recent researchshows that combining LLMs and TAGs yields complementary benefits: enhancing TAGrepresentation learning and improving the reasoning and interpretability ofLLMs. This survey provides the first systematic review of LLM--TAG integrationfrom an orchestration perspective. We introduce a novel taxonomy covering twofundamental directions: LLM for TAG, where LLMs enrich graph-based tasks, andTAG for LLM, where structured graphs improve LLM reasoning. We categorizeorchestration strategies into sequential, parallel, and multi-moduleframeworks, and discuss advances in TAG-specific pretraining, prompting, andparameter-efficient fine-tuning. Beyond methodology, we summarize empiricalinsights, curate available datasets, and highlight diverse applications acrossrecommendation systems, biomedical analysis, and knowledge-intensive questionanswering. Finally, we outline open challenges and promising researchdirections, aiming to guide future work at the intersection of language andgraph learning.</description>
      <author>example@mail.com (Guangxin Su, Hanchen Wang, Jianwei Wang, Wenjie Zhang, Ying Zhang, Jian Pei)</author>
      <guid isPermaLink="false">2510.21131v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging semantic similarity for experimentation with AI-generated treatments</title>
      <link>http://arxiv.org/abs/2510.21119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种双核表示学习方法，用于处理大型语言模型生成的高维数字实验处理，并通过学习低维表示捕捉处理的基本结构，应用于指导生成模型产生有意义的处理变体和促进在线实验的自适应分配。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）使数字实验能够以新的形式进行，其中处理方式结合了人类和模型生成的内容，且方式日益复杂。在这种环境下，主要的方法论挑战是如何表示这些高维处理而不丢失其语义含义或使分析变得不可行。&lt;h4&gt;目的&lt;/h4&gt;解决高维处理的表示问题，学习能够捕捉此类处理基本结构的低维表示，并应用于下游任务如指导生成模型和促进在线实验的自适应分配。&lt;h4&gt;方法&lt;/h4&gt;提出双核表示学习方法，通过处理和用户协变量的基于核的表示的内积来建模因果效应。开发了一种交替最小化算法，能够从数据中高效学习这些表示，并在低秩因子模型下提供收敛保证。引入了一种在线实验的自适应设计策略作为该框架的应用。&lt;h4&gt;主要发现&lt;/h4&gt;双核表示学习方法能够有效地从数据中学习处理的高维表示，并在低秩因子模型下保证收敛。该方法在在线实验的自适应设计中表现出有效性。&lt;h4&gt;结论&lt;/h4&gt;通过学习低维表示来捕捉高维处理的基本结构，可以有效地解决大型语言模型数字实验中的表示挑战，并应用于多种下游任务。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）使数字实验能够以新形式进行，其中处理方式结合了人类和模型生成的内容，且方式日益复杂。这种环境下的主要方法论挑战是如何表示这些高维处理而不丢失其语义含义或使分析变得不可行。在此，我们通过专注于学习能够捕捉此类处理基本结构的低维表示来解决这一问题。这些表示使下游应用成为可能，例如指导生成模型产生有意义的处理变体和促进在线实验中的自适应分配。我们提出了双核表示学习方法，通过处理和用户协变量的基于核的表示的内积来建模因果效应。我们开发了一种交替最小化算法，能够从数据中高效学习这些表示，并在低秩因子模型下提供收敛保证。作为该框架的应用，我们引入了一种在线实验的自适应设计策略，并通过数值实验证明了该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) enable a new form of digital experimentationwhere treatments combine human and model-generated content in increasinglysophisticated ways. The main methodological challenge in this setting isrepresenting these high-dimensional treatments without losing their semanticmeaning or rendering analysis intractable. Here, we address this problem byfocusing on learning low-dimensional representations that capture theunderlying structure of such treatments. These representations enabledownstream applications such as guiding generative models to produce meaningfultreatment variants and facilitating adaptive assignment in online experiments.We propose double kernel representation learning, which models the causaleffect through the inner product of kernel-based representations of treatmentsand user covariates. We develop an alternating-minimization algorithm thatlearns these representations efficiently from data and provides convergenceguarantees under a low-rank factor model. As an application of this framework,we introduce an adaptive design strategy for online experimentation anddemonstrate the method's effectiveness through numerical experiments.</description>
      <author>example@mail.com (Lei Shi, David Arbour, Raghavendra Addanki, Ritwik Sinha, Avi Feller)</author>
      <guid isPermaLink="false">2510.21119v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Fair Representation Learning with Controllable High Confidence Guarantees via Adversarial Inference</title>
      <link>http://arxiv.org/abs/2510.21017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了FRG框架，用于学习具有高置信度公平性保证的表示，确保在下游任务中人口统计差异被限制在用户定义的阈值内。&lt;h4&gt;背景&lt;/h4&gt;表示学习被广泛应用于生成能在多个下游任务中泛化的表示，但确保其公平性对于防止对特定人口统计群体产生不公平至关重要。&lt;h4&gt;目的&lt;/h4&gt;正式引入学习高置信度公平性表示的任务，保证每个下游预测中的人口统计差异被限制在用户定义的错误阈值内，并以可控的高概率实现。&lt;h4&gt;方法&lt;/h4&gt;提出FRG(Fair Representation learning with high-confidence Guarantees)框架，通过利用优化的对抗模型提供高置信度公平性保证。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界数据集上的实证评估表明，FRG相比六种最先进的公平表示学习方法，能够在一系列下游模型和任务中持续限制不公平性。&lt;h4&gt;结论&lt;/h4&gt;FRG框架能有效提供高置信度的公平性保证，确保在多种下游任务中公平性得到控制。&lt;h4&gt;翻译&lt;/h4&gt;表示学习越来越多地被应用于生成能够在多个下游任务中泛化的表示。确保表示学习中的公平性保证对于防止在下游任务中对特定人口统计群体产生不公平至关重要。在这项工作中，我们正式引入了学习能够实现高置信度公平性表示的任务。我们旨在保证每个下游预测中的人口统计差异被限制在用户定义的错误阈值内，并以可控的高概率实现。为此，我们提出了FRG框架，该框架通过利用优化的对抗模型提供这些高置信度公平性保证。我们在三个真实世界数据集上对FRG进行了实证评估，将其性能与六种最先进的公平表示学习方法进行比较。我们的结果表明FRG能够在一系列下游模型和任务中持续限制不公平性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning is increasingly applied to generate representationsthat generalize well across multiple downstream tasks. Ensuring fairnessguarantees in representation learning is crucial to prevent unfairness towardspecific demographic groups in downstream tasks. In this work, we formallyintroduce the task of learning representations that achieve high-confidencefairness. We aim to guarantee that demographic disparity in every downstreamprediction remains bounded by a *user-defined* error threshold $\epsilon$, with*controllable* high probability. To this end, we propose the ***F**air**R**epresentation learning with high-confidence **G**uarantees (FRG)*framework, which provides these high-confidence fairness guarantees byleveraging an optimized adversarial model. We empirically evaluate FRG on threereal-world datasets, comparing its performance to six state-of-the-art fairrepresentation learning methods. Our results demonstrate that FRG consistentlybounds unfairness across a range of downstream models and tasks.</description>
      <author>example@mail.com (Yuhong Luo, Austin Hoag, Xintong Wang, Philip S. Thomas, Przemyslaw A. Grabowicz)</author>
      <guid isPermaLink="false">2510.21017v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks</title>
      <link>http://arxiv.org/abs/2510.20976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了L2M3OF，首个专门用于金属有机框架设计的多模态大语言模型，通过整合晶体表示学习与语言理解能力，在材料设计领域取得了突破性进展。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在自然语言任务中展现出强大推理能力，但在科学发现方面的突破有限。理解复杂物理现象需要超越语言的多方面表示，MOFs设计面临巨大三维原子排列空间和严格网状规则的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理MOFs设计的多模态大语言模型，克服仅基于文本表示的局限性，减少对人类专家经验的依赖。&lt;h4&gt;方法&lt;/h4&gt;L2M3OF整合晶体表示学习与语言理解，联合处理结构、文本和知识模态；使用预训练的晶体编码器和轻量级投影层压缩结构信息；构建晶体材料的结构-性质-知识数据库；与GPT-5、Gemini-2.5-Pro和DeepSeek-R1等闭源LLM进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;L2M3OF在性质预测和知识生成任务上优于领先的基于文本的闭源LLM，尽管使用的参数少得多。&lt;h4&gt;结论&lt;/h4&gt;多模态方法对于多孔材料理解的重要性，L2M3OF成为材料发现领域下一代AI系统的基础。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型已在各种自然语言任务中展现出卓越的推理能力。然而，在科学发现方面的可比突破更为有限，因为理解复杂的物理现象需要超越语言本身的多方面表示。一个有力的例子是功能材料如金属有机框架的设计，这些材料对于碳捕获和氢储存等一系列重要应用至关重要。由于其可能存在的三维原子排列数量众多以及配位几何和拓扑的严格网状规则，在基于语言的、可被大语言模型解释的表示中导航其巨大而复杂的设计空间具有挑战性。尽管在更简单的材料系统中，大语言模型辅助的发现已显示出有希望的结果，但MOF设计仍然严重依赖于很少仅以文本信息编码的隐性人类专业知识。为了克服这一障碍，我们引入了L2M3OF，这是首个用于MOFs的多模态大语言模型。L2M3OF整合了晶体表示学习与语言理解，以联合处理结构、文本和知识模态。L2M3OF采用预训练的晶体编码器和轻量级投影层将结构信息压缩到标记空间，从而实现与语言指令的有效对齐。为了促进训练和评估，我们整理了一个包含晶体材料的结构-性质-知识数据库，并将L2M3OF与最先进的闭源大语言模型（如GPT-5、Gemini-2.5-Pro和DeepSeek-R1）进行基准测试。实验表明，尽管使用的参数少得多，L2M3OF在性质预测和知识生成任务上优于领先的基于文本的闭源大语言模型。这些结果突显了多模态方法对于多孔材料理解的重要性，并将L2M3OF确立为材料发现领域下一代AI系统的基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用大型语言模型有效理解和设计金属有机框架（MOFs）材料的问题。这个问题很重要，因为MOFs是一类多孔晶体材料，在碳捕获、氢储存、水收集和药物输送等领域有广泛应用潜力。然而，MOFs的三维复杂结构难以用纯文本表示，现有方法无法有效捕捉其三维对称性、周期性和长程结构相关性，限制了材料设计的效率和准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有LLMs在材料科学中的局限性，特别是它们难以处理MOFs的三维结构信息。他们借鉴了晶体表示学习（如PMTransformer）和指令微调范式，设计了L2M3OF模型。该方法结合了预训练的晶体编码器与轻量级投影层，将结构信息压缩到标记空间，实现与语言指令的高效对齐。同时，作者还采用了分组训练策略增强上下文多样性。这些设计基于对现有工作的深入分析，并针对MOFs的特殊性进行了创新改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态学习，将MOFs的三维结构信息与文本知识相结合，使模型能够同时理解和处理材料的结构、属性和应用知识。整体流程包括：1) 构建MOF-SPK数据库，包含超过10万种MOFs的结构、属性和知识；2) 使用PMTransformer作为晶体结构编码器，将三维结构转换为潜在表示；3) 通过多模态投影桥将结构信息压缩并投影到语言模型空间；4) 使用指令微调范式训练模型，采用分组训练策略增强上下文多样性；5) 在属性预测、结构提取、描述生成和问答等任务上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个专门为MOFs设计的多模态大型语言模型；2) 构建了首个包含MOFs结构、属性和领域知识的综合数据库MOF-SPK；3) 设计了高效的多模态对齐方法，使用轻量级投影层实现结构信息与语言指令的融合；4) 提出了分组训练策略增强模型性能。相比之前的工作，L2M3OF能够处理复杂的三维MOF结构，超越了纯文本表示的局限性；同时能够同时处理多种任务（属性预测、结构提取、描述生成和问答），在性能上超越了参数更多的商业LLMs，尽管使用了更少的参数。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; L2M3OF通过整合三维晶体结构表示与领域知识，开创了多模态大型语言模型在金属有机框架材料设计中的应用，实现了超越纯文本模型的性能，为材料科学发现提供了新的AI辅助工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models have demonstrated remarkable reasoning capabilitiesacross diverse natural language tasks. However, comparable breakthroughs inscientific discovery are more limited, because understanding complex physicalphenomena demands multifaceted representations far beyond language alone. Acompelling example is the design of functional materials such as MOFs-criticalfor a range of impactful applications like carbon capture and hydrogen storage.Navigating their vast and intricate design space in language-basedrepresentations interpretable by LLMs is challenging due to the numerouspossible three-dimensional atomic arrangements and strict reticular rules ofcoordination geometry and topology. Despite promising early results inLLM-assisted discovery for simpler materials systems, MOF design remainsheavily reliant on tacit human expertise rarely codified in textual informationalone. To overcome this barrier, we introduce L2M3OF, the first multimodal LLMfor MOFs. L2M3OF integrates crystal representation learning with languageunderstanding to process structural, textual, and knowledge modalities jointly.L2M3OF employs a pre-trained crystal encoder with a lightweight projectionlayer to compress structural information into a token space, enabling efficientalignment with language instructions. To facilitate training and evaluation, wecurate a structure-property-knowledge database of crystalline materials andbenchmark L2M3OF against state-of-the-art closed-source LLMs such as GPT-5,Gemini-2.5-Pro and DeepSeek-R1. Experiments show that L2M3OF outperformsleading text-based closed-source LLMs in property prediction and knowledgegeneration tasks, despite using far fewer parameters. These results highlightthe importance of multimodal approaches for porous material understanding andestablish L2M3OF as a foundation for next-generation AI systems in materialsdiscovery.</description>
      <author>example@mail.com (Jiyu Cui, Fang Wu, Haokai Zhao, Minggao Feng, Xenophon Evangelopoulos, Andrew I. Cooper, Yejin Choi)</author>
      <guid isPermaLink="false">2510.20976v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Irish-BLiMP: A Linguistic Benchmark for Evaluating Human and Language Model Performance in a Low-Resource Setting</title>
      <link>http://arxiv.org/abs/2510.20957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Irish-BLiMP是首个专门为评估爱尔兰语语言能力而设计的数据集和框架，研究对比了人类和大型语言模型在爱尔兰语语法知识上的表现。&lt;h4&gt;背景&lt;/h4&gt;爱尔兰语是一种濒危语言，缺乏专门的语言能力评估工具和框架。&lt;h4&gt;目的&lt;/h4&gt;评估现有大型语言模型和流利人类参与者在爱尔兰语语法知识方面的表现，并分析两者之间的差异。&lt;h4&gt;方法&lt;/h4&gt;基于多种语言学文献和参考资料，由流利的爱尔兰语使用者团队手动构建和审查了1020个最小对比对，涵盖11个语言学特征的分类法。&lt;h4&gt;主要发现&lt;/h4&gt;人类在所有语言学特征上的表现都优于所有模型，平均准确率高出16.6%；开源和闭源大型语言模型之间存在18.1%的性能差距；最强模型(gpt-5)准确率为73.5%，人类为90.1%；人类和模型在不同语法方面存在不同困难。&lt;h4&gt;结论&lt;/h4&gt;Irish-BLiMP为评估LLMs在爱尔兰语中的语法能力提供了首个系统性框架，为低资源语言理解研究提供了有价值的基准。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Irish-BLiMP(爱尔兰语语言能力最小对比数据集)，这是第一个专门为评估爱尔兰语(一种濒危语言)语言能力而设计和构建的数据集和框架。借鉴多种语言学文献和语法参考资料，我们由流利的爱尔兰语使用者团队手动构建并审查了涵盖11个语言学特征分类法的1020个最小对比对。我们评估了现有大型语言模型和流利人类参与者在爱尔兰语语法知识方面的表现。我们的发现显示，人类在所有语言学特征上的表现都优于所有模型，平均准确率高出16.6%。此外，开源和闭源大型语言模型之间存在18.1%的显著性能差距，即使是最强的模型也仅达到73.5%的准确率，而人类达到90.1%。有趣的是，人类参与者和模型在爱尔兰语语法的不同方面存在困难，这突显了模型学习表征的差异。总体而言，Irish-BLiMP为评估大型语言模型在爱尔兰语中的语法能力提供了首个系统性框架，并为推进低资源语言理解研究提供了有价值的基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Irish-BLiMP (Irish Benchmark of Linguistic Minimal Pairs), thefirst dataset and framework designed for fine-grained evaluation of linguisticcompetence in the Irish language, an endangered language. Drawing on a varietyof linguistic literature and grammar reference works, we manually constructedand reviewed 1020 minimal pairs across a taxonomy of 11 linguistic features,through a team of fluent Irish speakers. We evaluate both existing LargeLanguage Models (LLMs) and fluent human participants on their syntacticknowledge of Irish. Our findings show that humans outperform all models acrossall linguistic features, achieving 16.6% higher accuracy on average. Moreover,a substantial performance gap of 18.1% persists between open- and closed-sourceLLMs, with even the strongest model (gpt-5) reaching only 73.5% accuracycompared to 90.1% by human. Interestingly, human participants and modelsstruggle on different aspects of Irish grammar, thus highlighting a differencein representation learned by the models. Overall, Irish-BLiMP provides thefirst systematic framework for evaluating the grammatical competence of LLMs inIrish and offers a valuable benchmark for advancing research on linguisticunderstanding in low-resource languages.</description>
      <author>example@mail.com (Josh McGiff, Khanh-Tung Tran, William Mulcahy, Dáibhidh Ó Luinín, Jake Dalzell, Róisín Ní Bhroin, Adam Burke, Barry O'Sullivan, Hoang D. Nguyen, Nikola S. Nikolov)</author>
      <guid isPermaLink="false">2510.20957v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>ROPES: Robotic Pose Estimation via Score-Based Causal Representation Learning</title>
      <link>http://arxiv.org/abs/2510.20884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A preliminary version of this paper appeared at NeurIPS 2025 Workshop  on Embodied World Models for Decision Making&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出ROPES方法，将因果表征学习应用于机器人姿态估计，这是一个无监督框架，能够解离潜在生成因素并识别可通过致动直接控制的变量。&lt;h4&gt;背景&lt;/h4&gt;因果表征学习(CRL)是一种强大的无监督框架，能够解离高维数据下的潜在生成因素，并学习解离变量之间的因果关系。尽管在可识别性和实际应用方面取得了进展，但理论与实践之间仍存在显著差距。&lt;h4&gt;目的&lt;/h4&gt;将CRL引入机器人领域，以缩小理论与实践之间的差距。具体解决机器人姿态估计问题，即从原始图像中恢复位置和方向。&lt;h4&gt;方法&lt;/h4&gt;提出ROPES（基于分数的CRL的机器人姿态估计）框架。这是一个无监督框架，通过识别被致动的生成因素来体现干预性CRL的本质。图像由内在和外在潜在因素生成（如关节角度、手臂/肢体几何形状、光照、背景和相机配置），目标是解离和恢复可控制的潜在变量。&lt;h4&gt;主要发现&lt;/h4&gt;在半合成机械臂实验中的经验评估表明，ROPES能够高保真度地解离潜在生成因素。这是仅通过利用分布变化实现的，而没有使用任何标记数据。论文还包括了与基于半监督框架的基线方法的比较。&lt;h4&gt;结论&lt;/h4&gt;将机器人姿态定位为CRL的近实用测试平台。&lt;h4&gt;翻译&lt;/h4&gt;因果表征学习（CRL）已成为一种强大的无监督框架，它（i）解离高维数据下的潜在生成因素，以及（ii）学习解离变量之间的因果关系。尽管最近在可识别性方面取得了广泛进展并有一些实践进展，但理论与实践之间仍然存在巨大差距。本文通过将CRL引入机器人领域（这一领域推动了CRL的发展）来缩小这一差距。具体而言，本文通过引入基于分数的CRL的机器人姿态估计（ROPES）来解决明确的机器人姿态估计问题——从原始图像中恢复位置和方向。作为一个无监督框架，ROPES通过识别被致动的生成因素体现了干预性CRL的本质：图像由内在和外在潜在因素（例如，关节角度、手臂/肢体几何形状、光照、背景和相机配置）生成，目标是解离和恢复可控制的潜在变量，即那些可以通过致动直接操作（干预）的变量。干预性CRL理论表明，通过干预经历变化的变量可以被识别。在机器人领域，这种干预通过命令各种关节的致动器并在不同控制下记录图像自然产生。在半合成机械臂实验中的经验评估表明，ROPES能够高保真度地解离潜在生成因素。关键的是，这是仅通过利用分布变化实现的，而没有使用任何标记数据。本文还包括了与最近提出的半监督框架的基线方法的比较。本文最后将机器人姿态定位为CRL的近实用测试平台。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal representation learning (CRL) has emerged as a powerful unsupervisedframework that (i) disentangles the latent generative factors underlyinghigh-dimensional data, and (ii) learns the cause-and-effect interactions amongthe disentangled variables. Despite extensive recent advances inidentifiability and some practical progress, a substantial gap remains betweentheory and real-world practice. This paper takes a step toward closing that gapby bringing CRL to robotics, a domain that has motivated CRL. Specifically,this paper addresses the well-defined robot pose estimation -- the recovery ofposition and orientation from raw images -- by introducing Robotic PoseEstimation via Score-Based CRL (ROPES). Being an unsupervised framework, ROPESembodies the essence of interventional CRL by identifying those generativefactors that are actuated: images are generated by intrinsic and extrinsiclatent factors (e.g., joint angles, arm/limb geometry, lighting, background,and camera configuration) and the objective is to disentangle and recover thecontrollable latent variables, i.e., those that can be directly manipulated(intervened upon) through actuation. Interventional CRL theory shows thatvariables that undergo variations via interventions can be identified. Inrobotics, such interventions arise naturally by commanding actuators of variousjoints and recording images under varied controls. Empirical evaluations insemi-synthetic manipulator experiments demonstrate that ROPES successfullydisentangles latent generative factors with high fidelity with respect to theground truth. Crucially, this is achieved by leveraging only distributionalchanges, without using any labeled data. The paper also includes a comparisonwith a baseline based on a recently proposed semi-supervised framework. Thispaper concludes by positioning robot pose estimation as a near-practicaltestbed for CRL.</description>
      <author>example@mail.com (Pranamya Kulkarni, Puranjay Datta, Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Ali Tajer)</author>
      <guid isPermaLink="false">2510.20884v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Machine-Learning-Assisted Comparison of Regression Functions</title>
      <link>http://arxiv.org/abs/2510.24714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文重新审视了比较回归函数的经典问题，提出了一种基于核的条件均值依赖性的广义概念，并开发了两种新的检验方法，利用现代机器学习方法进行灵活估计。&lt;h4&gt;背景&lt;/h4&gt;比较回归函数是统计推断中的一个基本问题，与数据集成、迁移学习和因果推断等现代应用密切相关。现有方法通常依赖于平滑技术，因此受到维度诅咒的限制。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来比较回归函数，克服现有方法在维度诅咒下的局限性，并减少对分布假设的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出基于核的条件均值依赖性的广义概念，为回归函数相等的零假设提供新表征，并基于此重新表述开发两种新的检验方法，利用现代机器学习方法进行灵活估计。&lt;h4&gt;主要发现&lt;/h4&gt;建立了检验统计量的渐近性质，这些性质在固定维度和高维度情况下都成立；与需要严格分布假设的现有方法不同，该框架仅施加温和的矩条件。&lt;h4&gt;结论&lt;/h4&gt;所提出的检验方法在广泛的数值研究中证明了其有效性，为比较回归函数提供了更灵活和实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们重新审视了比较回归函数的经典问题，这是统计推断中的一个基本问题，与数据集成、迁移学习和因果推断等现代应用密切相关。现有方法通常依赖于平滑技术，因此受到维度诅咒的限制。我们提出了一种基于核的条件均值依赖性的广义概念，为回归函数相等的零假设提供了新的表征。基于这一重新表述，我们开发了两种新的检验方法，利用现代机器学习方法进行灵活估计。我们建立了检验统计量的渐近性质，这些性质在固定维度和高维度情况下都成立。与通常需要严格分布假设的现有方法不同，我们的框架仅施加温和的矩条件。所提出检验方法的有效性通过大量的数值研究得到了证明。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We revisit the classical problem of comparing regression functions, afundamental question in statistical inference with broad relevance to modernapplications such as data integration, transfer learning, and causal inference.Existing approaches typically rely on smoothing techniques and are thushindered by the curse of dimensionality. We propose a generalized notion ofkernel-based conditional mean dependence that provides a new characterizationof the null hypothesis of equal regression functions. Building on thisreformulation, we develop two novel tests that leverage modern machine learningmethods for flexible estimation. We establish the asymptotic properties of thetest statistics, which hold under both fixed- and high-dimensional regimes.Unlike existing methods that often require restrictive distributionalassumptions, our framework only imposes mild moment conditions. The efficacy ofthe proposed tests is demonstrated through extensive numerical studies.</description>
      <author>example@mail.com (Jian Yan, Zhuoxi Li, Yang Ning, Yong Chen)</author>
      <guid isPermaLink="false">2510.24714v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Cluster Dose Prediction in Carbon Ion Therapy: Using Transfer Learning from a Pretrained Dose Prediction U-Net</title>
      <link>http://arxiv.org/abs/2510.24703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究使用神经网络预测簇剂量分布，以替代计算密集型模拟，并通过迁移学习技术优化了U-Net架构，实现了快速且准确的簇剂量估计。&lt;h4&gt;背景&lt;/h4&gt;簇剂量概念为基于放射生物学效应(RBE)的模型提供了替代方案，用于描述辐射诱导的生物效应。&lt;h4&gt;目的&lt;/h4&gt;研究应用神经网络预测簇剂量分布，以替代当前需要的计算密集型模拟。&lt;h4&gt;方法&lt;/h4&gt;使用U-Net架构预测簇剂量分布，该网络首先在常规剂量分布上预训练，然后通过迁移学习技术对解码器路径进行适应；训练和预训练数据集包括来自多个患者头颈区域的不同能量和位置的碳离子束；使用蒙特卡洛模拟生成真实簇剂量分布作为基准。&lt;h4&gt;主要发现&lt;/h4&gt;U-Net能够在使用图形处理单元(GPU)的情况下，在几毫秒内完成单笔束的簇剂量估计；预测的簇剂量分布与真实值的偏差小于0.35%。&lt;h4&gt;结论&lt;/h4&gt;该原理验证研究证明了使用机器学习在临床可接受的计算时间内准确估计簇剂量的可行性；通过利用预训练神经网络和应用迁移学习技术，显著减少了对大规模、计算成本高昂的训练数据的需求。&lt;h4&gt;翻译&lt;/h4&gt;簇剂量概念为基于放射生物学效应(RBE)的模型提供了替代方案，用于描述辐射诱导的生物效应。本研究探讨了应用神经网络预测簇剂量分布的可能性，旨在替代当前需要的计算密集型模拟。使用最初在常规剂量分布上预训练的U-Net来预测簇剂量分布，并通过迁移学习技术对解码器路径进行适应以用于簇剂量估计。训练和预训练数据集包括来自多个患者头颈区域的不同能量和位置的碳离子束。使用蒙特卡洛(MC)模拟生成真实簇剂量分布。U-Net能够在使用图形处理单元(GPU)的情况下，在几毫秒内完成单笔束的簇剂量估计。预测的簇剂量分布与真实值的偏差小于0.35%。这项原理验证研究证明了使用机器学习(ML)在临床可接受的计算时间内准确估计簇剂量的可行性。通过利用预训练神经网络和应用迁移学习技术，该方法显著减少了对大规模、计算成本高昂的训练数据的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The cluster dose concept offers an alternative to the radiobiologicaleffectiveness (RBE)-based model for describing radiation-induced biologicaleffects. This study examines the application of a neural network to predictcluster dose distributions, with the goal of replacing the computationallyintensive simulations currently required. Cluster dose distributions arepredicted using a U-Net that was initially pretrained on conventional dosedistributions. Using transfer learning techniques, the decoder path is adaptedfor cluster dose estimation. Both the training and pretraining datasets includehead and neck regions from multiple patients and carbon ion beams of varyingenergies and positions. Monte Carlo (MC) simulations were used to generate theground truth cluster dose distributions. The U-Net enables cluster doseestimation for a single pencil beam within milliseconds using a graphicsprocessing unit (GPU). The predicted cluster dose distributions deviate fromthe ground truth by less than 0.35%. This proof-of-principle study demonstratesthe feasibility of accurately estimating cluster doses within clinicallyacceptable computation times using machine learning (ML). By leveraging apretrained neural network and applying transfer learning techniques, theapproach significantly reduces the need for large-scale, computationallyexpensive training data.</description>
      <author>example@mail.com (Miriam Schwarze, Hui Khee Looe, Björn Poppe, Leo Thomas, Hans Rabus)</author>
      <guid isPermaLink="false">2510.24703v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Semi-supervised and unsupervised learning for health indicator extraction from guided waves in aerospace composite structures</title>
      <link>http://arxiv.org/abs/2510.24614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种综合数据驱动框架，通过结合两种学习方法与多域信号处理来提取航空航天复合材料结构的健康指标，解决了健康指标提取中的挑战。&lt;h4&gt;背景&lt;/h4&gt;健康指标对于诊断和预测航空航天复合材料结构的状况至关重要，有助于高效维护和操作安全。然而，由于材料特性的变异性、损伤演变的随机性和多样化的损伤模式，提取可靠的健康指标具有挑战性。制造缺陷和服役期间的事故进一步增加了复杂性。&lt;h4&gt;目的&lt;/h4&gt;开发一种综合数据驱动框架，通过两种学习方法与多域信号处理相结合来学习健康指标，由于缺乏真实健康指标，提出半监督和无监督方法来解决这一问题。&lt;h4&gt;方法&lt;/h4&gt;提出两种学习方法：多样性深度半监督异常检测(Diversity-DeepSAD)方法，使用连续辅助标签作为假设损伤代理；退化趋势约束变分自编码器(DTC-VAE)，通过显式趋势约束嵌入单调性准则。使用多种激励频率的导波监测单加筋复合材料结构，探索时间、频率和时间频域表示，并通过无监督集成融合各频率健康指标。&lt;h4&gt;主要发现&lt;/h4&gt;使用快速傅里叶变换特征，增强的Diversity-DeepSAD模型达到81.6%的性能，DTC-VAE提供最一致的健康指标，达到92.3%的性能，优于现有基线方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的数据驱动框架，特别是DTC-VAE方法，能够有效提取航空航天复合材料结构的健康指标，为结构健康监测提供了可靠解决方案。&lt;h4&gt;翻译&lt;/h4&gt;健康指标对于诊断和预测航空航天复合材料结构的状况至关重要，能够实现高效维护和操作安全。然而，由于材料特性的变异性、损伤演变的随机性和多样化的损伤模式，提取可靠的健康指标仍然具有挑战性。制造缺陷（如脱粘）和服役期间的事故（如鸟撞）进一步使这一过程复杂化。本研究提出了一种综合数据驱动框架，通过结合两种学习方法与多域信号处理来学习健康指标。由于缺乏真实健康指标，提出了半监督和无监督方法：(i)多样性深度半监督异常检测方法，使用连续辅助标签作为假设损伤代理，克服了仅区分健康和故障状态的二元标签的局限性；(ii)退化趋势约束变分自编码器，其中单调性准则通过显式趋势约束嵌入。使用多种激励频率的导波来监测在疲劳载荷下的单加筋复合材料结构，并通过无监督集成融合各频率的健康指标，以减少频率依赖性和方差。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Health indicators (HIs) are central to diagnosing and prognosing thecondition of aerospace composite structures, enabling efficient maintenance andoperational safety. However, extracting reliable HIs remains challenging due tovariability in material properties, stochastic damage evolution, and diversedamage modes. Manufacturing defects (e.g., disbonds) and in-service incidents(e.g., bird strikes) further complicate this process. This study presents acomprehensive data-driven framework that learns HIs via two learning approachesintegrated with multi-domain signal processing. Because ground-truth HIs areunavailable, a semi-supervised and an unsupervised approach are proposed: (i) adiversity deep semi-supervised anomaly detection (Diversity-DeepSAD) approachaugmented with continuous auxiliary labels used as hypothetical damage proxies,which overcomes the limitation of prior binary labels that only distinguishhealthy and failed states while neglecting intermediate degradation, and (ii) adegradation-trend-constrained variational autoencoder (DTC-VAE), in which themonotonicity criterion is embedded via an explicit trend constraint. Guidedwaves with multiple excitation frequencies are used to monitor single-stiffenercomposite structures under fatigue loading. Time, frequency, and time-frequencyrepresentations are explored, and per-frequency HIs are fused via unsupervisedensemble learning to mitigate frequency dependence and reduce variance. Usingfast Fourier transform features, the augmented Diversity-DeepSAD model achieved81.6% performance, while DTC-VAE delivered the most consistent HIs with 92.3%performance, outperforming existing baselines.</description>
      <author>example@mail.com (James Josep Perry, Pablo Garcia-Conde Ortiz, George Konstantinou, Cornelie Vergouwen, Edlyn Santha Kumaran, Morteza Moradi)</author>
      <guid isPermaLink="false">2510.24614v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised learning for variability detection with Gaia DR3 photometry. The main sequence-white dwarf valley</title>
      <link>http://arxiv.org/abs/2510.23776v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in Astronomy &amp; Astrophysics (A&amp;A); 10 pages,  9 figures, 1 appendix (7 additional figures, 2 tables)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用无监督学习方法从Gaia DR3数据中识别变星和特殊系统，成功发现了包括热亚矮星、激变变星、食双星等多种天体类型，并证实该方法在大规模恒星群体分析中的有效性。&lt;h4&gt;背景&lt;/h4&gt;来自空间和地面望远镜的空前数量和质量的数据为机器学习提供了机会，使其能够识别传统方法可能忽略的新变星类别和特殊系统。之前已有相关方法学工作。&lt;h4&gt;目的&lt;/h4&gt;研究无监督学习方法在大恒星群体（包括拥挤场中的天体）上的扩展潜力，无需预先选择的目录，专注于从Gaia DR3中选出的13405个源。&lt;h4&gt;方法&lt;/h4&gt;使用基于从Gaia DR3时代测光中提取的统计特征的无监督聚类技术，采用t-SNE算法识别变星类别、子类型和仪器效应引起的虚假变异性。&lt;h4&gt;主要发现&lt;/h4&gt;聚类结果显示了不同组别，包括热亚矮星、激变变星、食双星和仙女座场中的拥挤场天体；发现了潜在的恒星子类型；被标记为RR Lyrae的天体出现在CMD的意外区域，可能由于不可靠的天体测量或替代演化途径。&lt;h4&gt;结论&lt;/h4&gt;所提出方法在寻找Gaia CMD大区域中可变天体（包括可变热亚矮星和激变变星）具有稳健性，展示了检测扩展恒星群体中变异性的效率，该无监督学习框架可扩展到大型数据集并在识别恒星子类方面有前景。&lt;h4&gt;翻译&lt;/h4&gt;来自空间和地面望远镜的空前数量和质量的数据为机器学习提供了识别新类别变星和可能被传统方法忽视的特殊系统的机会。在先前方法学研究的基础上，本研究探讨了无监督学习方法在大恒星群体（包括拥挤场中的天体）上的扩展潜力，无需预先选择的目录，特别专注于从Gaia DR3中选出的13405个源，位于选定CMD区域。我们的方法主要基于从Gaia DR3时代测光中提取的统计特征，采用无监督聚类技术。我们使用t-SNE算法来识别变星类别、其子类型以及由仪器效应引起的虚假变异性。聚类结果显示了不同的组别，包括热亚矮星、激变变星、食双星以及拥挤场中的天体，如仙女座(M31)场中的天体。在这些集群中还出现了几种潜在的恒星子类型。值得注意的是，先前被标记为RR Lyrae的天体在CMD的意外区域被发现，可能是由于不可靠的天体测量（如双星性）或替代的演化途径。本研究强调了所提出方法在寻找Gaia CMD大区域中可变天体的稳健性，包括可变热亚矮星和激变变星，同时展示了其在检测扩展恒星群体中变异性的效率。所提出的无监督学习框架可扩展到大型数据集，并在识别恒星子类方面有前景的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The unprecedented volume and quality of data from space- and ground-basedtelescopes present an opportunity for machine learning to identify new classesof variable stars and peculiar systems that may have been overlooked bytraditional methods. Extending prior methodological work, this studyinvestigates the potential of an unsupervised learning approach to scaleeffectively to larger stellar populations, including objects in crowded fields,and without the need for pre-selected catalogues, specifically focusing on 13405 sources selected from Gaia DR3 and lying in the selected region of the CMD.Our methodology incorporates unsupervised clustering techniques based primarilyon statistical features extracted from Gaia DR3 epoch photometry. We used thet-distributed stochastic neighbour embedding (t-SNE) algorithm to identifyvariability classes, their subtypes, and spurious variability induced byinstrumental effects. The clustering results revealed distinct groups,including hot subdwarfs, cataclysmic variables (CVs), eclipsing binaries, andobjects in crowded fields, such as those in the Andromeda (M31) field. Severalpotential stellar subtypes also emerged within these clusters. Notably, objectspreviously labelled as RR Lyrae were found in an unexpected region of the CMD,potentially due to either unreliable astrometric measurements (e.g., due tobinarity) or alternative evolutionary pathways. This study emphasises therobustness of the proposed method in finding variable objects in a large regionof the Gaia CMD, including variable hot subdwarfs and CVs, while demonstratingits efficiency in detecting variability in extended stellar populations. Theproposed unsupervised learning framework demonstrates scalability to largedatasets and yields promising results in identifying stellar subclasses.</description>
      <author>example@mail.com (P. Ranaivomanana, C. Johnston, G. Iorio, P. J. Groot, M. Uzundag, T. Kupfer, C. Aerts)</author>
      <guid isPermaLink="false">2510.23776v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Genomics into Multimodal EHR Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种创新的电子健康记录(EHR)基础模型，整合多基因风险评分(PRS)作为基础数据模态，超越传统EHR-only方法，构建更全面的健康档案。&lt;h4&gt;背景&lt;/h4&gt;传统EHR模型仅使用临床数据，忽略了遗传因素对健康的影响。All of Us (AoU)研究项目提供了广泛而多样的数据资源。&lt;h4&gt;目的&lt;/h4&gt;开发一个多模态框架，学习临床数据和遗传倾向之间的复杂关系，增强预测能力和可解释性。&lt;h4&gt;方法&lt;/h4&gt;将生成式AI的进步扩展到EHR基础模型空间，利用AoU研究项目的数据进行训练，并探索迁移学习用于定制分类任务。&lt;h4&gt;主要发现&lt;/h4&gt;在AoU数据上的评估表明，该模型对多种疾病发作(特别是2型糖尿病)具有预测价值，并展示了PRS和EHR数据之间的相互作用。&lt;h4&gt;结论&lt;/h4&gt;这种方法对于解锁疾病预测、主动健康管理、风险分层和个性化治疗策略的新见解至关重要，为医疗保健中更个性化、公平和可行的真实世界证据生成奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种创新的电子健康记录(EHR)基础模型，将多基因风险评分(PRS)作为基础数据模态整合其中，超越了传统的仅使用EHR的方法，以构建更全面的健康档案。利用All of Us (AoU)研究项目的广泛而多样的数据，这个多模态框架旨在学习临床数据和遗传倾向之间的复杂关系。该方法将生成式AI的进步扩展到EHR基础模型空间，增强了预测能力和可解释性。在AoU数据上的评估证明了该模型对多种疾病发作(特别是2型糖尿病)的预测价值，并说明了PRS和EHR数据之间的相互作用。该研究还探索了迁移学习用于定制分类任务，展示了架构的多功能性和效率。这种方法对于解锁疾病预测、主动健康管理、风险分层和个性化治疗策略的新见解至关重要，为医疗保健中更个性化、公平和可行的真实世界证据生成奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces an innovative Electronic Health Record (EHR) foundationmodel that integrates Polygenic Risk Scores (PRS) as a foundational datamodality, moving beyond traditional EHR-only approaches to build more holistichealth profiles. Leveraging the extensive and diverse data from the All of Us(AoU) Research Program, this multimodal framework aims to learn complexrelationships between clinical data and genetic predispositions. Themethodology extends advancements in generative AI to the EHR foundation modelspace, enhancing predictive capabilities and interpretability. Evaluation onAoU data demonstrates the model's predictive value for the onset of variousconditions, particularly Type 2 Diabetes (T2D), and illustrates the interplaybetween PRS and EHR data. The work also explores transfer learning for customclassification tasks, showcasing the architecture's versatility and efficiency.This approach is pivotal for unlocking new insights into disease prediction,proactive health management, risk stratification, and personalized treatmentstrategies, laying the groundwork for more personalized, equitable, andactionable real-world evidence generation in healthcare.</description>
      <author>example@mail.com (Jonathan Amar, Edward Liu, Alessandra Breschi, Liangliang Zhang, Pouya Kheradpour, Sylvia Li, Lisa Soleymani Lehmann, Alessandro Giulianelli, Matt Edwards, Yugang Jia, David Nola, Raghav Mani, Pankaj Vats, Jesse Tetreault, T. J. Chen, Cory Y. McLean)</author>
      <guid isPermaLink="false">2510.23639v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>An unsupervised tour through the hidden pathways of deep neural networks</title>
      <link>http://arxiv.org/abs/2510.21582v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文旨在提高对深度人工神经网络创建有意义表示并能泛化的内部机制的理解，专注于使用无监督学习工具表征隐藏表示的语义内容。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络创建有意义表示和泛化的内部机制尚不完全清楚，需要工具来表征隐藏表示的语义内容并利用数据的低维结构。&lt;h4&gt;目的&lt;/h4&gt;改进对深度神经网络如何创建有意义表示并能泛化的内部机制的理解，开发无监督学习工具来表征隐藏表示的语义内容。&lt;h4&gt;方法&lt;/h4&gt;开发无监督学习工具利用数据的低维结构；介绍Gride方法估计数据内在维度作为尺度的显式函数；研究深度神经网络隐藏层概率密度的演变；分析深度神经网络中的泛化问题。&lt;h4&gt;主要发现&lt;/h4&gt;初始层产生单模态概率密度，消除与分类无关的结构；后续层中密度峰以分层方式出现，反映概念语义层次；输出层概率密度的峰地形可重建类别语义关系；宽神经网络学习冗余表示而非对虚假相关性过拟合；冗余神经元只在网络被正则化且训练误差为零时出现。&lt;h4&gt;结论&lt;/h4&gt;深度神经网络通过分层方式构建语义层次结构；增加参数到插值训练数据的网络会改善泛化性能，与经典偏差-方差权衡相悖；宽神经网络学习冗余表示而非过拟合。&lt;h4&gt;翻译&lt;/h4&gt;本论文的目标是提高我们对深度人工神经网络创建有意义表示并能泛化的内部机制的理解。我们专注于使用无监督学习工具表征隐藏表示的语义内容，这些工具由我们部分开发并在本论文中描述，它们能够利用数据的低维结构。第二章介绍了Gride，一种方法，允许将数据的内在维度估计为尺度的显式函数，而无需对数据集进行任何降采样。我们的方法基于严格的分布结果，能够量化估计的不确定性。此外，我们的方法简单且计算高效，因为它仅依赖于最近数据点之间的距离。在第三章中，我们研究了最先进深度神经网络中隐藏层概率密度的演变。我们发现初始层产生单模态概率密度，消除任何与分类无关的结构。在后续层中，密度峰以分层方式出现，反映了概念的语义层次结构。这个过程在输出层的概率密度中留下了痕迹，其中峰的地形允许重建类别的语义关系。在第四章中，我们研究了深度神经网络中的泛化问题：向插值训练数据的网络添加参数通常会改善其泛化性能，这与经典的偏差-方差权衡相悖。我们证明宽神经网络学习冗余表示而不是对虚假相关性过拟合，并且只有当网络被正则化且训练误差为零时，冗余神经元才会出现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of this thesis is to improve our understanding of the internalmechanisms by which deep artificial neural networks create meaningfulrepresentations and are able to generalize. We focus on the challenge ofcharacterizing the semantic content of the hidden representations withunsupervised learning tools, partially developed by us and described in thisthesis, which allow harnessing the low-dimensional structure of the data.Chapter 2. introduces Gride, a method that allows estimating the intrinsicdimension of the data as an explicit function of the scale without performingany decimation of the data set. Our approach is based on rigorousdistributional results that enable the quantification of uncertainty of theestimates. Moreover, our method is simple and computationally efficient sinceit relies only on the distances among nearest data points. In Chapter 3, westudy the evolution of the probability density across the hidden layers in somestate-of-the-art deep neural networks. We find that the initial layers generatea unimodal probability density getting rid of any structure irrelevant toclassification. In subsequent layers, density peaks arise in a hierarchicalfashion that mirrors the semantic hierarchy of the concepts. This processleaves a footprint in the probability density of the output layer, where thetopography of the peaks allows reconstructing the semantic relationships of thecategories. In Chapter 4, we study the problem of generalization in deep neuralnetworks: adding parameters to a network that interpolates its training datawill typically improve its generalization performance, at odds with theclassical bias-variance trade-off. We show that wide neural networks learnredundant representations instead of overfitting to spurious correlation andthat redundant neurons appear only if the network is regularized and thetraining error is zero.</description>
      <author>example@mail.com (Diego Doimo)</author>
      <guid isPermaLink="false">2510.21582v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MFiSP: A Multimodal Fire Spread Prediction Framework</title>
      <link>http://arxiv.org/abs/2510.23934v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个多模态火灾蔓延预测框架(MFiSP)，整合社交媒体数据和遥感观测以提高预测准确性，评估结果显示该方法优于传统火灾预测方法。&lt;h4&gt;背景&lt;/h4&gt;2019-2020年澳大利亚黑色夏季山火摧毁了1900万公顷土地，3000栋房屋，持续七个月，显示了野火威胁的规模和紧迫性，需要更好的预测来有效应对。&lt;h4&gt;目的&lt;/h4&gt;开发一种更准确的火灾蔓延预测方法，以应对日益严重的野火威胁，提高应急响应效率。&lt;h4&gt;方法&lt;/h4&gt;提出多模态火灾蔓延预测框架(MFiSP)，整合社交媒体数据和遥感观测，通过调整燃料图操纵策略动态调整火灾行为预测，使其与观察到的蔓延速率保持一致。&lt;h4&gt;主要发现&lt;/h4&gt;整合多模态数据的MFiSP可以提高火灾蔓延预测的准确性，超越依赖消防行为分析师专业知识和静态输入的传统方法。&lt;h4&gt;结论&lt;/h4&gt;新兴数据源如NASA的FIRMS卫星图像和自愿地理信息，结合多模态数据整合方法，能够显著改善火灾蔓延预测，为应对日益严重的野火威胁提供有效工具。&lt;h4&gt;翻译&lt;/h4&gt;2019-2020年澳大利亚黑色夏季山火摧毁了1900万公顷土地，3000栋房屋，持续七个月，显示了野火威胁规模和紧迫性的升级，需要更好的预测以实现有效应对。传统火灾建模依赖于消防行为分析师(FBAns)的手动解读和静态环境数据，常常导致不准确和操作限制。新兴数据源，如NASA的FIRMS卫星图像和自愿地理信息，通过实现动态火灾蔓延预测，提供了改进的可能性。本研究提出了一个多模态火灾蔓延预测框架(MFiSP)，整合社交媒体数据和遥感观测以提高预测准确性。通过在同化周期之间调整燃料图操纵策略，该框架动态调整火灾行为预测，以与观察到的蔓延速率保持一致。我们使用在不同场景中合成的火灾事件多边形评估MFiSP的有效性，分析个体和组合对预测边界的影响。结果表明，整合多模态数据的MFiSP可以提高火灾蔓延预测，超越依赖FBAn专业知识和静态输入的传统方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 2019-2020 Black Summer bushfires in Australia devastated 19 millionhectares, destroyed 3,000 homes, and lasted seven months, demonstrating theescalating scale and urgency of wildfire threats requiring better forecastingfor effective response. Traditional fire modeling relies on manualinterpretation by Fire Behaviour Analysts (FBAns) and static environmentaldata, often leading to inaccuracies and operational limitations. Emerging datasources, such as NASA's FIRMS satellite imagery and Volunteered GeographicInformation, offer potential improvements by enabling dynamic fire spreadprediction. This study proposes a Multimodal Fire Spread Prediction Framework(MFiSP) that integrates social media data and remote sensing observations toenhance forecast accuracy. By adapting fuel map manipulation strategies betweenassimilation cycles, the framework dynamically adjusts fire behaviorpredictions to align with the observed rate of spread. We evaluate the efficacyof MFiSP using synthetically generated fire event polygons across multiplescenarios, analyzing individual and combined impacts on forecast perimeters.Results suggest that our MFiSP integrating multimodal data can improve firespread prediction beyond conventional methods reliant on FBAn expertise andstatic inputs.</description>
      <author>example@mail.com (Alec Sathiyamoorthy, Wenhao Zhou, Xiangmin Zhou, Xiaodong Li, Iqbal Gondal)</author>
      <guid isPermaLink="false">2510.23934v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans</title>
      <link>http://arxiv.org/abs/2510.14205v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;动态人格完善框架（DPRF）通过迭代识别和解决认知差异，提高了LLM RPAs与目标个体行为的一致性&lt;h4&gt;背景&lt;/h4&gt;大语言模型角色扮演代理（LLM RPAs）旨在模拟个人人类行为，但人格保真度常因手动创建的档案（如精心挑选的信息和人格特征）而受损，这些档案未经与目标个体一致性的验证&lt;h4&gt;目的&lt;/h4&gt;解决上述限制，引入动态人格完善框架（DPRF），优化LLM RPAs行为与目标个体行为的一致性&lt;h4&gt;方法&lt;/h4&gt;DPRF通过迭代识别认知差异来优化LLM RPAs与目标个体行为的对齐，这些认知差异可以通过自由形式或基于理论的、结构化的分析来识别生成行为与人类真实情况之间的差异，并完善人格档案以减轻这些差异&lt;h4&gt;主要发现&lt;/h4&gt;在五个大语言模型和四种多样的行为预测场景上评估了DPRF，这些场景包括正式辩论、涉及心理健康问题的社交媒体帖子、公开访谈和电影评论，DPRF能够持续显著提高行为对齐度，优于基线人格，且能够跨模型和场景泛化&lt;h4&gt;结论&lt;/h4&gt;提供了一种创建高保真人格档案的稳健方法，提高了下游应用的有效性，如用户模拟、社会研究和个性化AI&lt;h4&gt;翻译&lt;/h4&gt;新兴的大语言模型角色扮演代理（LLM RPAs）旨在模拟个人人类行为，但人格保真度常因手动创建的档案（如精心挑选的信息和人格特征）而受损，这些档案未经与目标个体一致性的验证。为解决这一限制，我们的工作引入了动态人格完善框架（DPRF）。DPRF旨在通过迭代识别生成行为与人类真实情况之间的认知差异（无论是通过自由形式还是基于理论的、结构化的分析）来优化LLM RPAs行为与目标个体行为的一致性，并完善人格档案以减轻这些差异。我们在四个多样的行为预测场景中用五个大语言模型评估了DPRF：正式辩论、涉及心理健康问题的社交媒体帖子、公开访谈和电影评论。DPRF能够持续显著提高行为对齐度，优于基线人格，并且能够跨模型和场景泛化。我们的工作为创建高保真人格档案提供了一种稳健方法，并增强了下游应用的有效性，如用户模拟、社会研究和个性化AI。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emerging large language model role-playing agents (LLM RPAs) aim tosimulate individual human behaviors, but the persona fidelity is oftenundermined by manually-created profiles (e.g., cherry-picked information andpersonality characteristics) without validating the alignment with the targetindividuals. To address this limitation, our work introduces the DynamicPersona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLMRPAs' behaviors with those of target individuals by iteratively identifying thecognitive divergence, either through free-form or theory-grounded, structuredanalysis, between generated behaviors and human ground truth, and refining thepersona profile to mitigate these divergences.We evaluate DPRF with five LLMson four diverse behavior-prediction scenarios: formal debates, social mediaposts with mental health issues, public interviews, and movie reviews.DPRF canconsistently improve behavioral alignment considerably over baseline personasand generalizes across models and scenarios.Our work provides a robustmethodology for creating high-fidelity persona profiles and enhancing thevalidity of downstream applications, such as user simulation, social studies,and personalized AI.</description>
      <author>example@mail.com (Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang)</author>
      <guid isPermaLink="false">2510.14205v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?</title>
      <link>http://arxiv.org/abs/2510.24706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文引入了ComboBench基准测试，评估大型语言模型将语义动作转换为VR设备操作序列的能力，发现即使是顶级模型在程序推理和空间理解方面仍与人类存在差距。&lt;h4&gt;背景&lt;/h4&gt;虚拟现实游戏需要玩家将高级语义动作转换为使用控制器和头戴显示器的精确设备操作，而人类基于常识和具身理解直观地执行这种转换，但大型语言模型是否能有效复制这种能力尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;引入一个名为ComboBench的基准测试，评估大型语言模型将语义动作转换为VR设备操作序列的能力。&lt;h4&gt;方法&lt;/h4&gt;从四个流行的VR游戏(《半衰期：爱莉克斯》、《Into the Radius》、《Moss：Book II》和《Vivecraft》)的262个场景中评估七个大型语言模型(GPT-3.5、GPT-4、GPT-4o、Gemini-1.5-Pro、LLaMA-3-8B、Mixtral-8x7B和GLM-4-Flash)，并与标注的真实基线和人类表现进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;表现最佳的模型(如Gemini-1.5-Pro)展示了强大的任务分解能力，但在程序推理和空间理解方面仍与人类存在差距；不同游戏之间的性能差异显著，表明对交互复杂性的敏感性；少样本示例显著提高了性能，表明有潜力针对性地增强大型语言模型的VR操作能力。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型在VR设备操作序列生成方面仍有改进空间，特别是在程序推理和空间理解方面。&lt;h4&gt;翻译&lt;/h4&gt;虚拟现实游戏要求玩家使用控制器和头戴显示器将高级语义动作转换为精确的设备操作。虽然人类基于常识和具身理解直观地执行这种转换，但大型语言模型是否能有效复制这种能力尚未得到充分探索。本文引入了ComboBench基准测试，评估大型语言模型将语义动作转换为VR设备操作序列的能力，涵盖来自四个流行VR游戏(半衰期：爱莉克斯、Into the Radius、Moss：Book II和Vivecraft)的262个场景。我们评估了七个大型语言模型，包括GPT-3.5、GPT-4、GPT-4o、Gemini-1.5-Pro、LLaMA-3-8B、Mixtral-8x7B和GLM-4-Flash，并与标注的真实基线和人类表现进行比较。结果表明，尽管表现最佳的模型(如Gemini-1.5-Pro)展示了强大的任务分解能力，但在程序推理和空间理解方面仍与人类存在差距。不同游戏之间的性能差异显著，表明对交互复杂性的敏感性。少样本示例显著提高了性能，表明有潜力针对性地增强大型语言模型的VR操作能力。我们在https://sites.google.com/view/combobench发布了所有材料。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文研究大型语言模型（LLMs）是否能够有效地将高级语义动作（如'投降'、'驯服马匹'）转化为精确的VR设备操作序列（如'按X键'、'将头显朝向苦力怕'）。这个问题重要是因为VR游戏需要玩家将抽象意图转化为具体物理操作，这种能力是人类智能的关键组成部分，但目前尚不清楚LLMs是否具备这种具身认知能力，这对开发更智能的虚拟代理和游戏AI具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过认知科学专家访谈确定了VR交互所需的六种核心认知能力（任务分解、程序推理等），然后系统性地选择了四款代表性VR游戏，提取了262个语义动作场景，并由经验VR玩家进行精细的设备操作序列标注。他们借鉴了机器人系统（如SayCan）、虚拟环境智能体（如Voyager）和多步骤规划策略（如Chain-of-Thought）的工作，但专注于VR设备操作的精确映射，而非代码生成或机器人控制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建全面基准测试ComboBench评估LLMs将语义动作转化为VR设备操作的能力，并采用多维度认知框架分析其表现。整体流程包括：1)构建基准：确定认知能力、选择游戏、提取场景、标注操作序列和认知能力；2)模型评估：在多个模型和少样本设置下测试；3)性能分析：比较LLMs与人类表现、分析认知能力差异、研究少样本影响和游戏复杂度关系。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门评估LLMs将语义动作转化为VR设备操作能力的基准测试；2)建立六种核心认知能力框架并实现步骤级别标注；3)设计多维度评估指标全面分析性能；4)收集四款不同类型VR游戏的262个场景提供多样化测试环境；5)通过与人类对比揭示LLMs在具身认知方面的优势和不足。相比之前工作，ComboBench专注于VR环境中的物理设备操作而非代码生成、机器人控制或平面界面操作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了ComboBench基准测试，首次系统评估了大型语言模型将高级语义动作转化为VR设备操作的能力，揭示了当前LLMs在具身认知方面的优势与局限，为开发更智能的VR交互AI提供了重要指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Virtual Reality (VR) games require players to translate high-level semanticactions into precise device manipulations using controllers and head-mounteddisplays (HMDs). While humans intuitively perform this translation based oncommon sense and embodied understanding, whether Large Language Models (LLMs)can effectively replicate this ability remains underexplored. This paperintroduces a benchmark, ComboBench, evaluating LLMs' capability to translatesemantic actions into VR device manipulation sequences across 262 scenariosfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared againstannotated ground truth and human performance. Our results reveal that whiletop-performing models like Gemini-1.5-Pro demonstrate strong task decompositioncapabilities, they still struggle with procedural reasoning and spatialunderstanding compared to humans. Performance varies significantly acrossgames, suggesting sensitivity to interaction complexity. Few-shot examplessubstantially improve performance, indicating potential for targetedenhancement of LLMs' VR manipulation capabilities. We release all materials athttps://sites.google.com/view/combobench.</description>
      <author>example@mail.com (Shuqing Li, Jiayi Yan, Chenyu Niu, Jen-tse Huang, Yun Peng, Wenxuan Wang, Yepang Liu, Michael R. Lyu)</author>
      <guid isPermaLink="false">2510.24706v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Sound Source Localization for Spatial Mapping of Surgical Actions in Dynamic Scenes</title>
      <link>http://arxiv.org/abs/2510.24332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种创新方法，通过整合3D声学信息和视觉数据，增强了手术场景的理解。该方法能够将声学事件在3D空间中定位并与视觉元素关联，实验证明在真实手术室环境中表现良好。&lt;h4&gt;背景&lt;/h4&gt;手术场景理解对于推进计算机辅助和智能手术系统至关重要。当前方法主要依赖视觉数据或端到端学习，这限制了细粒度上下文建模。&lt;h4&gt;目的&lt;/h4&gt;通过整合3D声学信息来增强手术场景表示，实现对手术环境在时间和空间上的多模态理解。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的框架，用于生成手术场景的4D视听表示。通过将相控麦克风阵列的声学定位信息投影到RGB-D相机的动态点云上，并使用基于Transformer的声学事件检测模块识别包含工具-组织相互作用的相关时间片段。在专家执行的模拟手术程序期间，在真实的手术室设置中进行了实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法成功地将手术声学事件在3D空间中定位，并与视觉场景元素关联。实验评估证明了精确的空间声音定位和多模态数据的稳健融合，提供了手术活动的全面、动态表示。&lt;h4&gt;结论&lt;/h4&gt;这项工作首次引入了动态手术场景中的空间声音定位方法，向多模态手术场景表示迈出了重要一步。通过整合声学和视觉数据，所提出的框架实现了更丰富的上下文理解，为未来的智能和自主手术系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;目的：手术场景理解对于推进计算机辅助和智能手术系统至关重要。当前方法主要依赖视觉数据或端到端学习，这限制了细粒度上下文建模。这项工作旨在通过整合3D声学信息来增强手术场景表示，实现对手术环境在时间和空间上的多模态理解。方法：我们提出了一种新颖的框架，通过将相控麦克风阵列的声学定位信息投影到RGB-D相机的动态点云上，生成手术场景的4D视听表示。基于Transformer的声学事件检测模块识别包含工具-组织相互作用的相关时间片段，这些片段在视听场景表示中进行空间定位。系统在专家执行的模拟手术程序期间，在真实的手术室设置中进行了实验评估。结果：所提出的方法成功地将手术声学事件在3D空间中定位，并将它们与视觉场景元素关联。实验评估证明了精确的空间声音定位和多模态数据的稳健融合，提供了手术活动的全面、动态表示。结论：这项工作首次引入了动态手术场景中的空间声音定位方法，向多模态手术场景表示迈出了重要一步。通过整合声学和视觉数据，所提出的框架实现了更丰富的上下文理解，为未来的智能和自主手术系统奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决手术场景中的声学事件定位问题，目的是通过整合声学信息来增强手术场景的数字表示。这个问题很重要，因为当前手术场景理解主要依赖视觉数据，无法捕捉工具-组织相互作用的细粒度信息，而声学信息可以提供视觉无法获取的关键细节，对于开发智能手术系统和提高手术安全性与效率至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有手术场景理解方法的局限性，然后提出多模态融合的思路，认为结合声学和视觉信息可以提供更全面的手术场景理解。他们借鉴了AudioSpectrogramTransformer模型进行声学事件检测，利用现有的声学波束形成技术生成2D声学热图，并参考了点云处理和3D定位技术。整个系统设计围绕如何有效融合声学和视觉信息，创建时空一致的4D手术场景表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过融合声学和视觉信息创建更全面的4D（3D空间+时间）手术场景表示，利用声学事件补充视觉信息，提供工具-组织相互作用的上下文。整体流程包括：1)使用相控麦克风阵列和RGB-D相机采集多模态数据；2)通过波束形成生成2D声学热图并投影到3D点云上；3)使用transformer模型检测手术声学事件；4)通过聚类算法定位声源并生成3D边界框；5)评估系统性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次在动态手术场景中进行空间声音定位；2)提出4D音频-视觉手术场景表示的新概念；3)基于transformer的声学事件检测方法；4)有效的多模态融合方法。相比之前工作，本文不仅整合了声学和视觉两种模态信息，还创建了时空一致的4D表示，专注于细粒度的声学事件检测和空间定位，而非高层概念预测，提供了更好的可解释性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次通过融合声学和视觉信息创建了4D动态手术场景表示，实现了手术声学事件的空间定位，为开发更智能、更全面的手术理解系统奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Surgical scene understanding is key to advancing computer-aided andintelligent surgical systems. Current approaches predominantly rely on visualdata or end-to-end learning, which limits fine-grained contextual modeling.This work aims to enhance surgical scene representations by integrating 3Dacoustic information, enabling temporally and spatially aware multimodalunderstanding of surgical environments.  Methods: We propose a novel framework for generating 4D audio-visualrepresentations of surgical scenes by projecting acoustic localizationinformation from a phased microphone array onto dynamic point clouds from anRGB-D camera. A transformer-based acoustic event detection module identifiesrelevant temporal segments containing tool-tissue interactions which arespatially localized in the audio-visual scene representation. The system wasexperimentally evaluated in a realistic operating room setup during simulatedsurgical procedures performed by experts.  Results: The proposed method successfully localizes surgical acoustic eventsin 3D space and associates them with visual scene elements. Experimentalevaluation demonstrates accurate spatial sound localization and robust fusionof multimodal data, providing a comprehensive, dynamic representation ofsurgical activity.  Conclusion: This work introduces the first approach for spatial soundlocalization in dynamic surgical scenes, marking a significant advancementtoward multimodal surgical scene representations. By integrating acoustic andvisual data, the proposed framework enables richer contextual understanding andprovides a foundation for future intelligent and autonomous surgical systems.</description>
      <author>example@mail.com (Jonas Hein, Lazaros Vlachopoulos, Maurits Geert Laurent Olthof, Bastian Sigrist, Philipp Fürnstahl, Matthias Seibold)</author>
      <guid isPermaLink="false">2510.24332v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning</title>
      <link>http://arxiv.org/abs/2510.24152v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  RoboSense Challenge with IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个系统性框架，用于提高视觉语言模型在自动驾驶场景理解任务中的性能，通过四个核心组件实现问题分类、任务特定提示设计、视觉信息组装和模型参数优化。&lt;h4&gt;背景&lt;/h4&gt;IROS 2025 RoboSense Challenge评估视觉语言模型在自动驾驶场景理解方面的能力，涵盖感知、预测、规划和损坏检测四个任务领域。&lt;h4&gt;目的&lt;/h4&gt;开发一个有效的框架，提升视觉语言模型在安全关键型自动驾驶任务中的表现，特别是在处理干净数据和损坏数据时的准确率。&lt;h4&gt;方法&lt;/h4&gt;构建了一个四组件框架：1)混合提示路由器分类并分派问题；2)特定任务提示嵌入坐标系、空间推理规则等；3)视觉组装模块组合多视图图像；4)按任务配置模型推理参数。&lt;h4&gt;主要发现&lt;/h4&gt;在Qwen2.5-VL-72B模型上实现，该方法在第一阶段(干净数据)达到70.87%平均准确率，在第二阶段(损坏数据)达到72.85%准确率。&lt;h4&gt;结论&lt;/h4&gt;结构化提示和空间接地能显著提升视觉语言模型在安全关键型自动驾驶任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;本技术报告介绍了我们在IROS 2025 RoboSense Challenge上的解决方案，该方案评估视觉语言模型在自动驾驶场景理解方面的能力，涵盖感知、预测、规划和损坏检测任务。我们提出了一个基于四个核心组件构建的系统性框架。首先，混合提示路由器对问题进行分类并将其分派给特定任务的专家提示，消除了不同问题类型之间的干扰。其次，特定任务提示嵌入明确的坐标系、空间推理规则、角色扮演、思维链/思维树推理以及为每个任务定制的小样本示例。第三，视觉组装模块根据问题要求组合多视图图像、对象裁剪、洋红色标记和自适应历史帧。第四，我们按任务配置模型推理参数(温度、top-p、消息角色)以优化输出质量。在Qwen2.5-VL-72B上实现，我们的方法在第一阶段(干净数据)上平均准确率达到70.87%，在第二阶段(损坏数据)上达到72.85%，证明结构化提示和空间接地显著提高了VLM在安全关键型自动驾驶任务上的性能。代码和提示可在https://github.com/wuaodi/UCAS-CSU-phase2获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉-语言模型(VLMs)在自动驾驶场景理解中面临的三个关键挑战：多视角场景中的空间推理困难(如混淆左右方向、错误判断物体位置)、不同任务类型之间的提示干扰(单一通用提示难以同时优化感知、预测、规划等不同任务)、以及时间上下文集成不当(添加历史帧可能引入噪声而非有用信息)。这些问题在现实中非常重要，因为自动驾驶系统需要准确的场景理解才能做出安全决策，解决这些问题能显著提高VLMs在安全关键自动驾驶任务中的可靠性和性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过三个关键设计原则来解决问题：消除提示干扰(使用专家提示而非通用提示)、增强空间推理(明确定义坐标系统和约束)、以及自适应时间上下文(根据问题类型选择适当历史帧)。作者借鉴了多项现有工作，包括Mixture-of-Prompts(使用多个专家提示)、Role-playing(为模型分配特定角色)、Chain-of-Thought/Tree-of-Thought推理(逐步推理和探索多种可能性)、In-context learning(通过示例引导模型)以及Visual prompting(视觉注意力引导)。作者将这些现有技术组合并调整，以适应自动驾驶场景的特殊需求，特别是空间推理和时间上下文处理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过系统化的提示工程增强VLM在自动驾驶场景理解中的性能，特别关注空间推理和任务特定提示设计。整体实现流程包含四个核心组件：1)路由器：分类测试查询并分配到适当的任务专家提示；2)任务特定提示：包含坐标系统、空间规则、角色扮演、链式/树式思维推理和少样本示例；3)视觉组装模块：根据问题需求组合多视角图像、物体裁剪、标记和自适应历史帧；4)模型选择和推理参数：使用Qwen2.5-VL-72B并根据任务类型调整推理参数。流程是：路由器分类问题→选择任务特定提示→组合视觉输入→使用特定推理参数调用VLM生成答案。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Mixture-of-Prompts路由器，消除不同任务类型间的提示干扰；2)明确的坐标系统和空间规则，增强多视角空间定位能力；3)自适应视觉组装，根据问题类型组合视觉输入；4)任务特定的推理参数，优化不同任务类型的输出质量；5)结合多种推理技术，为不同任务定制推理策略。相比之前工作，本文的主要不同在于不是通过微调模型来增强性能，而是通过提示级别的设计(明确空间定位、自适应时间证据和任务特定路由)来提高可靠性，这种方法在保持基础模型不变的情况下显著提升了性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于任务特定提示和空间推理的系统化框架，显著提升了视觉-语言模型在自动驾驶场景理解任务中的性能，特别是在多视角空间定位和不同任务类型处理方面。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This technical report presents our solution for the RoboSense Challenge atIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous drivingscene understanding across perception, prediction, planning, and corruptiondetection tasks. We propose a systematic framework built on four corecomponents. First, a Mixture-of-Prompts router classifies questions anddispatches them to task-specific expert prompts, eliminating interferenceacross diverse question types. Second, task-specific prompts embed explicitcoordinate systems, spatial reasoning rules, role-playing,Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored toeach task. Third, a visual assembly module composes multi-view images withobject crops, magenta markers, and adaptive historical frames based on questionrequirements. Fourth, we configure model inference parameters (temperature,top-p, message roles) per task to optimize output quality. Implemented onQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (cleandata) and 72.85% on Phase-2 (corrupted data), demonstrating that structuredprompting and spatial grounding substantially enhance VLM performance onsafety-critical autonomous driving tasks. Code and prompt are available athttps://github.com/wuaodi/UCAS-CSU-phase2.</description>
      <author>example@mail.com (Aodi Wu, Xubo Luo)</author>
      <guid isPermaLink="false">2510.24152v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations</title>
      <link>http://arxiv.org/abs/2510.23607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025, produced by Pointcept, project page:  https://pointcept.github.io/Concerto&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Concerto是一个模拟人类概念学习的模型，通过结合3D模态内自蒸馏和2D-3D跨模态联合嵌入，学习空间认知中的抽象概念，表现出优越的性能。&lt;h4&gt;背景&lt;/h4&gt;人类通过多感官协同学习抽象概念，一旦形成，可以从单一感官回忆这些表示。&lt;h4&gt;目的&lt;/h4&gt;受人类学习原理启发，开发一个用于空间认知的概念学习模型。&lt;h4&gt;方法&lt;/h4&gt;Concerto结合了3D模态内自蒸馏和2D-3D跨模态联合嵌入的方法。&lt;h4&gt;主要发现&lt;/h4&gt;Concerto学习到更连贯和信息丰富的空间特征；在零样本可视化中表现出色；在线性探测中分别比最先进的2D和3D自监督模型高出14.2%和4.8%；完全微调后在多个场景理解基准测试中设置了新的最先进结果；提出了针对视频提升点云空间理解的变体；开发了将表示投影到CLIP语言空间的翻译器，实现开放世界感知。&lt;h4&gt;结论&lt;/h4&gt;Concerto产生了具有优越细粒度几何和语义一致性的空间表示。&lt;h4&gt;翻译&lt;/h4&gt;人类通过多感官协同学习抽象概念，一旦形成，这样的表示通常可以从单一感官回忆。受这一原理启发，我们引入了Concerto，这是一个用于空间认知的人类概念学习的极简模拟，结合了3D模态内自蒸馏和2D-3D跨模态联合嵌入。尽管简单，但Concerto学习到更连贯和信息丰富的空间特征，如零样本可视化所示。在3D场景感知的线性探测中，它分别比独立的最新2D和3D自监督模型高出14.2%和4.8%，也优于它们的特征连接。通过完全微调，Concerto在多个场景理解基准测试中设置了新的最新结果（例如在ScanNet上达到80.7% mIoU）。我们进一步提出了一个针对视频提升点云空间理解的Concerto变体，以及一个将Concerto表示线性投影到CLIP语言空间的翻译器，实现开放世界感知。这些结果表明，Concerto产生了具有优越细粒度几何和语义一致性的空间表示。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何通过联合2D图像和3D点云的自监督学习，学习更丰富、更一致的空间表示问题。这个问题在现实中很重要，因为空间认知是自动驾驶、混合现实和机器人等应用的基础，而多模态学习可以提供更全面的空间理解，减少对标注数据的依赖，使模型能够从大量无标签数据中学习更强大的表示。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受人类多感官协同学习抽象过程的启发，认识到人类可以通过不同感官（如视觉和触觉）形成统一概念，并能从单一感官唤起完整体验。他们首先进行了初步研究，验证了简单拼接2D和3D特征优于单一模态，进而设计了更复杂的框架。该方法借鉴了Sonata框架用于3D点云表示学习的单模态自蒸馏技术，以及基于LeCun的联合嵌入预测架构(JEPA)的跨模态对齐方法，将两者结合形成Concerto框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过模拟人类多感官协同学习的方式，结合2D图像和3D点云的自监督学习，学习更丰富、更一致的空间表示，使得模型能够从单一模态中唤出完整的空间概念。整体实现流程包括：1) 单模态自蒸馏：使用Point Transformer V3作为点云编码器，通过教师-学生范式训练，使用在线聚类目标函数增强一致性；2) 跨模态联合嵌入预测：使用预训练图像编码器提取特征，建立点云点和图像像素对应关系，预测点云特征以匹配图像特征，使用余弦相似度作为损失；3) 协同训练：结合两个目标函数，适当平衡权重，训练出能从单一模态唤出丰富空间表示的模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 多模态协同学习框架，通过跨模态预测而非简单特征融合学习统一表示；2) 自监督点云Transformer，结合单模态自蒸馏和跨模态联合嵌入；3) 视频感知变体，利用前馈重建从视频中生成点云数据；4) 语言桥接，将表示映射到CLIP语言空间实现开放词汇感知。相比之前的工作，Concerto不仅整合了2D图像和3D点云信息，还通过联合学习产生了比单一模态或简单特征拼接更丰富、更一致的表示空间，在多个场景理解任务上取得了最先进的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Concerto通过模拟人类多感官协同学习的方式，联合2D图像和3D点云的自监督学习，学习到了比单一模态更丰富、更一致的空间表示，并在多个场景理解任务上取得了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans learn abstract concepts through multisensory synergy, and once formed,such representations can often be recalled from a single modality. Inspired bythis principle, we introduce Concerto, a minimalist simulation of human conceptlearning for spatial cognition, combining 3D intra-modal self-distillation with2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns morecoherent and informative spatial features, as demonstrated by zero-shotvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervisedmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,in linear probing for 3D scene perception. With full fine-tuning, Concerto setsnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%mIoU on ScanNet). We further present a variant of Concerto tailored forvideo-lifted point cloud spatial understanding, and a translator that linearlyprojects Concerto representations into CLIP's language space, enablingopen-world perception. These results highlight that Concerto emerges spatialrepresentations with superior fine-grained geometric and semantic consistency.</description>
      <author>example@mail.com (Yujia Zhang, Xiaoyang Wu, Yixing Lao, Chengyao Wang, Zhuotao Tian, Naiyan Wang, Hengshuang Zhao)</author>
      <guid isPermaLink="false">2510.23607v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Localising under the drape: proprioception in the era of distributed surgical robotic system</title>
      <link>http://arxiv.org/abs/2510.23512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种无需标记的手术机器人本体感觉方法，通过轻量级立体RGB摄像头和基于Transformer的深度学习模型，实现了在无菌遮挡情况下的精确定位，提高了手术场景的可见性和追踪能力。&lt;h4&gt;背景&lt;/h4&gt;手术机器人虽然机械精密，但对周围环境缺乏感知能力，导致碰撞、系统恢复和工作流程中断等问题。现有的追踪系统依赖笨重的红外摄像头和反射标记，只能提供有限视角并增加手术室硬件负担。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需标记的本体感觉方法，使手术机器人在无菌遮挡情况下能够精确定位，提高手术场景的可见性和追踪能力，减少硬件负担，提高手术安全性。&lt;h4&gt;方法&lt;/h4&gt;使用轻量级立体RGB摄像头和基于Transformer的新型深度学习模型。基于最大的多中心空间机器人手术数据集（140万张来自人体尸体和临床前体内研究的自注释图像），通过跟踪整个机器人和手术场景而非单个标记来实现定位。&lt;h4&gt;主要发现&lt;/h4&gt;该方法提供对遮挡具有鲁棒性的整体视图，支持手术场景理解和上下文感知控制。在体内呼吸补偿中展示了临床应用潜力，可获取组织动力学；在多机器人系统中实现精确定位。与现有系统相比，消除了标记并将追踪可见性提高了25%。&lt;h4&gt;结论&lt;/h4&gt;这是首次展示完全覆盖的手术机器人的无标记本体感觉，降低了设置复杂性，提高了安全性，并为模块化和自主机器人手术铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;尽管手术机器人具有机械精密性，但它们仍然无法感知周围环境。这种空间意识的缺乏导致碰撞、系统恢复和工作流程中断等问题，随着具有独立交互臂的分布式机器人的引入，这些问题将加剧。现有的追踪系统依赖笨重的红外摄像头和反射标记，仅提供手术场景的有限视角，并在拥挤的手术室中增加硬件负担。我们提出了一种无需标记的本体感觉方法，使手术机器人在无菌遮挡的情况下能够精确定位，尽管视觉线索被遮挡。我们的方法仅依靠轻量级立体RGB摄像头和基于Transformer的新型深度学习模型。它基于迄今为止最大的多中心空间机器人手术数据集（来自人体尸体和临床前体内研究的140万张自注释图像）。通过跟踪整个机器人和手术场景，而不是单个标记，我们的方法提供了对遮挡具有鲁棒性的整体视图，支持手术场景理解和上下文感知控制。我们展示了体内呼吸补偿的潜在临床应用示例，可以获取最先进追踪技术无法观察到的组织动力学，并在多机器人系统中精确定位以支持未来的智能交互。此外，与现有系统相比，我们的方法消除了标记并将追踪可见性提高了25%。据我们所知，这是首次展示完全覆盖的手术机器人的无标记本体感觉，降低了设置复杂性，提高了安全性，并为模块化和自主机器人手术铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决手术机器人在被无菌布覆盖后无法精确定位的问题。这个问题很重要，因为当前手术机器人缺乏环境感知能力，会导致碰撞、系统恢复和工作流程中断，随着分布式多臂机器人系统的普及，这些问题会更加严重。现有的红外跟踪系统笨重、容易被遮挡、需要严格校准，且难以扩展到多机器人环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有红外跟踪系统的局限性，然后设计了基于轻量级立体RGB相机和Transformer深度学习模型的解决方案。他们借鉴了工业机器人中的无标记定位方法，但进行了修改以适应外科环境中的无菌布遮挡问题。方法核心是立体可微分渲染技术，结合了粒子群优化算法进行初始估计，并通过'上下文先验'方法迭代改进分割结果。作者还构建了140万张图像的大型多中心数据集来训练模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用轻量级立体RGB相机和深度学习模型，通过立体可微分渲染技术实现对被覆盖手术机器人的精确定位，无需传统反射标记。整体流程包括：1)收集和预处理多中心数据集；2)训练能够处理遮挡的分割模型；3)使用相机群优化进行初始估计；4)应用立体可微分渲染优化姿态估计；5)使用'上下文先验'方法迭代改进结果；6)在临床前和临床环境中验证方法。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次实现完全覆盖的无标记手术机器人定位；2)开发立体可微分渲染技术处理遮挡；3)构建最大规模的多中心手术机器人数据集；4)开发遮挡不变的分割方法；5)提出'上下文先验'方法改进分割；6)支持多机器人设置。相比之前工作，本文方法无需标记、硬件更轻量(轻13倍、体积小3倍)、提供更完整的场景理解、能捕获传统系统不可见的组织动态，并在真实临床条件下验证了亚毫米级定位精度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于轻量级立体RGB相机和深度学习的无标记定位方法，实现了对被无菌布覆盖的手术机器人的精确定位，提高了手术场景可见性并揭示了传统系统无法观察到的组织动态，为模块化和自主机器人手术铺平了道路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite their mechanical sophistication, surgical robots remain blind totheir surroundings. This lack of spatial awareness causes collisions, systemrecoveries, and workflow disruptions, issues that will intensify with theintroduction of distributed robots with independent interacting arms. Existingtracking systems rely on bulky infrared cameras and reflective markers,providing only limited views of the surgical scene and adding hardware burdenin crowded operating rooms. We present a marker-free proprioception method thatenables precise localisation of surgical robots under their sterile drapingdespite associated obstruction of visual cues. Our method solely relies onlightweight stereo-RGB cameras and novel transformer-based deep learningmodels. It builds on the largest multi-centre spatial robotic surgery datasetto date (1.4M self-annotated images from human cadaveric and preclinical invivo studies). By tracking the entire robot and surgical scene, rather thanindividual markers, our approach provides a holistic view robust to occlusions,supporting surgical scene understanding and context-aware control. Wedemonstrate an example of potential clinical benefits during in vivo breathingcompensation with access to tissue dynamics, unobservable under state of theart tracking, and accurately locate in multi-robot systems for futureintelligent interaction. In addition, and compared with existing systems, ourmethod eliminates markers and improves tracking visibility by 25%. To ourknowledge, this is the first demonstration of marker-free proprioception forfully draped surgical robots, reducing setup complexity, enhancing safety, andpaving the way toward modular and autonomous robotic surgery.</description>
      <author>example@mail.com (Martin Huber, Nicola A. Cavalcanti, Ayoob Davoodi, Ruixuan Li, Christopher E. Mower, Fabio Carrillo, Christoph J. Laux, Francois Teyssere, Thibault Chandanson, Antoine Harlé, Elie Saghbiny, Mazda Farshad, Guillaume Morel, Emmanuel Vander Poorten, Philipp Fürnstahl, Sébastien Ourselin, Christos Bergeles, Tom Vercauteren)</author>
      <guid isPermaLink="false">2510.23512v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception</title>
      <link>http://arxiv.org/abs/2510.23478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025. Including supplemental material. For code  and dataset, see https://github.com/thi-ad/UrbanIng-V2X&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UrbanIng-V2X是首个大规模、多模态数据集，支持德国Ingolstadt三个城市交叉路口的车辆和基础设施传感器之间的合作感知，包含34个20秒的传感器序列，提供多种传感器数据，并以10Hz频率标注3D边界框。&lt;h4&gt;背景&lt;/h4&gt;现有合作感知数据集在智能移动应用中起关键作用，但真实世界数据集通常仅限于单个交叉路口或单辆车，缺乏多连接车辆和基础设施传感器跨越多个交叉路口的全面感知数据集，限制了算法在多样化交通环境中的基准测试。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集的局限性，引入首个大规模、多模态合作感知数据集UrbanIng-V2X，支持车辆和基础设施传感器之间的合作感知。&lt;h4&gt;方法&lt;/h4&gt;在德国Ingolstadt的三个城市交叉路口部署传感器，收集34个时间对齐和空间校准的传感器序列(每个20秒)，涉及两辆车和最多三个基础设施传感器杆，使用12个车载RGB摄像头、2个车载LiDAR、17个基础设施热成像摄像头和12个基础设施LiDAR，以10Hz频率标注13个类别的3D边界框。&lt;h4&gt;主要发现&lt;/h4&gt;提供使用最先进的合作感知方法的全面评估，验证了数据集的有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;公开提供代码库、数据集、高清地图和完整数据收集环境的数字孪生，促进合作感知领域的研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;近期的合作感知数据集通过促进智能体之间的信息交换，在推进智能移动应用方面发挥了关键作用，帮助克服遮挡等挑战，并提高整体场景理解能力。虽然一些现有的真实世界数据集同时包含车辆对车辆和车辆对基础设施的交互，但它们通常仅限于单个交叉路口或单辆车。一个包含多个连接车辆和基础设施传感器跨越多个交叉路口的全面感知数据集仍然不可用，限制了算法在多样化交通环境中的基准测试。因此，可能会发生过拟合，模型可能由于相似的交叉路口布局和交通参与者行为而表现出误导性的高性能。为解决这一差距，我们引入了UrbanIng-V2X，这是首个大规模、多模态数据集，支持在德国Ingolstadt三个城市交叉路口部署的车辆和基础设施传感器之间的合作感知。UrbanIng-V2X包含34个时间对齐和空间校准的传感器序列，每个持续20秒。所有序列包含三个交叉路口中一个的记录，涉及两辆车和最多三个基础设施安装的传感器杆，在协调场景中运行。总的来说，UrbanIng-V2X提供来自12个车载RGB摄像头、2个车载LiDAR、17个基础设施热成像摄像头和12个基础设施LiDAR的数据。所有序列以10Hz的频率标注3D边界框，涵盖13个对象类别，导致整个数据集大约有712k个标注实例。我们使用最先进的合作感知方法提供了全面评估，并公开提供了代码库、数据集、高清地图和完整数据收集环境的数字孪生。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是缺乏一个大规模、多车辆、多基础设施、多交叉路口的合作感知数据集。这个问题在现实中很重要，因为城市交叉路口是自动驾驶中最复杂的场景之一，单一智能体的感知系统常因视野受限和遮挡而无法检测关键物体，而合作感知可以克服这些限制。缺乏多样性的数据集会导致算法过拟合，模型可能因相似场景而表现出误导性的高性能，限制了真实世界应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有数据集的不足（如只包含单个交叉路口或车辆、缺乏多种传感器类型）来设计方法。他们精心设计了传感器部署（两辆车各配备6个RGB摄像头和1个激光雷达，三个交叉路口配备热成像摄像头和激光雷达系统），实现了精确的传感器同步和校准，并从8小时数据中挑选34个代表性场景进行标注。作者借鉴了现有工作如V2V4Real、DAIR-V2X-C等数据集的经验，同时采用了类似nuScenes的标注方法和OpenCOOD的格式转换工具。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模、多模态、多交叉路口的合作感知数据集，使研究人员能够开发和评估在复杂城市环境中能有效协作的感知算法。整体流程包括：1)传感器部署（车载和基础设施）；2)数据采集（三个交叉路口34个20秒序列）；3)传感器同步与校准（UTC时钟同步、精确校准）；4)场景选择与标注（多样化光照条件、10Hz频率标注13个物体类别）；5)数据发布与工具提供（代码库、高清地图、数字孪生）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个多车辆、多基础设施、多交叉路口的合作感知数据集；2)引入最多的合作传感器和热成像相机等多模态传感器；3)支持多种合作感知基准任务；4)提供综合基准评估；5)提供开发者工具包和数字孪生。相比之前工作，UrbanIng-V2X同时支持多车辆和多基础设施，覆盖多个交叉路口，提供更丰富的传感器组合和更全面的标注（13个类别、712k实例），还提供了数字孪生环境和新的数据集分割策略以评估泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UrbanIng-V2X数据集通过提供首个大规模、多车辆、多基础设施、多交叉路口的合作感知数据集，克服了现有数据集的局限性，为开发和评估在复杂城市环境中能有效协作的感知算法提供了坚实基础，同时揭示了模型在未见环境中的泛化挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent cooperative perception datasets have played a crucial role inadvancing smart mobility applications by enabling information exchange betweenintelligent agents, helping to overcome challenges such as occlusions andimproving overall scene understanding. While some existing real-world datasetsincorporate both vehicle-to-vehicle and vehicle-to-infrastructure interactions,they are typically limited to a single intersection or a single vehicle. Acomprehensive perception dataset featuring multiple connected vehicles andinfrastructure sensors across several intersections remains unavailable,limiting the benchmarking of algorithms in diverse traffic environments.Consequently, overfitting can occur, and models may demonstrate misleadinglyhigh performance due to similar intersection layouts and traffic participantbehavior. To address this gap, we introduce UrbanIng-V2X, the firstlarge-scale, multi-modal dataset supporting cooperative perception involvingvehicles and infrastructure sensors deployed across three urban intersectionsin Ingolstadt, Germany. UrbanIng-V2X consists of 34 temporally aligned andspatially calibrated sensor sequences, each lasting 20 seconds. All sequencescontain recordings from one of three intersections, involving two vehicles andup to three infrastructure-mounted sensor poles operating in coordinatedscenarios. In total, UrbanIng-V2X provides data from 12 vehicle-mounted RGBcameras, 2 vehicle LiDARs, 17 infrastructure thermal cameras, and 12infrastructure LiDARs. All sequences are annotated at a frequency of 10 Hz with3D bounding boxes spanning 13 object classes, resulting in approximately 712kannotated instances across the dataset. We provide comprehensive evaluationsusing state-of-the-art cooperative perception methods and publicly release thecodebase, dataset, HD map, and a digital twin of the complete data collectionenvironment.</description>
      <author>example@mail.com (Karthikeyan Chandra Sekaran, Markus Geisler, Dominik Rößle, Adithya Mohan, Daniel Cremers, Wolfgang Utschick, Michael Botsch, Werner Huber, Torsten Schön)</author>
      <guid isPermaLink="false">2510.23478v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions</title>
      <link>http://arxiv.org/abs/2510.22798v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025. Project Website: https://vehme.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍VEHME，一种用于评估手写数学表达式的视觉语言模型，能够以高准确性和可解释的推理痕迹评估开放形式的手写数学答案。&lt;h4&gt;背景&lt;/h4&gt;自动评估手写数学解题是教育技术中的重要问题，具有实际应用，但由于学生作业的多样格式、非结构化布局和符号复杂性，这仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发VEHME模型，以高准确性和可解释的推理痕迹评估开放形式的手写数学答案。&lt;h4&gt;方法&lt;/h4&gt;VEHME采用两阶段训练管道：使用结构化推理数据进行监督微调；通过强化学习使模型输出与多维度评分目标（正确性、推理深度和错误定位）保持一致；提出表达式感知的视觉提示模块，在合成的多行数学表达式数据集上训练，以在视觉异构输入中稳健地引导注意力。&lt;h4&gt;主要发现&lt;/h4&gt;在AIHub和FERMAT数据集上评估，VEHME在开源模型中取得了最先进的性能，并接近专有系统的准确性。&lt;h4&gt;结论&lt;/h4&gt;VEHME展示了其作为可扩展且可访问的自动数学评估工具的潜力，训练和实验代码已在GitHub公开存储库中可用。&lt;h4&gt;翻译&lt;/h4&gt;自动评估手写数学解题是教育技术中的一个重要问题，具有实际应用，但由于学生作业的多样格式、非结构化布局和符号复杂性，这仍然是一个重大挑战。为应对这一挑战，我们介绍了VEHME——一种用于评估手写数学表达式的视觉语言模型——旨在以高准确性和可解释的推理痕迹评估开放形式的手写数学答案。VEHME集成了一个两阶段训练管道：(i) 使用结构化推理数据进行监督微调，(ii) 强化学习使模型输出与多维度评分目标（包括正确性、推理深度和错误定位）保持一致。为增强空间理解，我们提出了一个表达式感知的视觉提示模块，在我们合成的多行数学表达式数据集上训练，以在视觉异构输入中稳健地引导注意力。在AIHub和FERMAT数据集上评估，VEHME在开源模型中取得了最先进的性能，并接近专有系统的准确性，展示了其作为可扩展且可访问的自动数学评估工具的潜力。我们的训练和实验代码已在GitHub公开存储库中提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatically assessing handwritten mathematical solutions is an importantproblem in educational technology with practical applications, but it remains asignificant challenge due to the diverse formats, unstructured layouts, andsymbolic complexity of student work. To address this challenge, we introduceVEHME-a Vision-Language Model for Evaluating Handwritten MathematicsExpressions-designed to assess open-form handwritten math responses with highaccuracy and interpretable reasoning traces. VEHME integrates a two-phasetraining pipeline: (i) supervised fine-tuning using structured reasoning data,and (ii) reinforcement learning that aligns model outputs withmulti-dimensional grading objectives, including correctness, reasoning depth,and error localization. To enhance spatial understanding, we propose anExpression-Aware Visual Prompting Module, trained on our synthesized multi-linemath expressions dataset to robustly guide attention in visually heterogeneousinputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-artperformance among open-source models and approaches the accuracy of proprietarysystems, demonstrating its potential as a scalable and accessible tool forautomated math assessment. Our training and experiment code is publiclyavailable at our GitHub repository.</description>
      <author>example@mail.com (Thu Phuong Nguyen, Duc M. Nguyen, Hyotaek Jeon, Hyunwook Lee, Hyunmin Song, Sungahn Ko, Taehwan Kim)</author>
      <guid isPermaLink="false">2510.22798v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2510.22706v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/lifuguan/IGGT_official&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了InstanceGrounded Geometry Transformer (IGGT)，一种端到端的大型统一transformer，用于统一3D场景的空间重建和实例级上下文理解。&lt;h4&gt;背景&lt;/h4&gt;人类自然将3D世界的几何结构和语义内容作为相互交织的维度感知，但先前方法优先训练几何模型进行低级3D重建，将高级空间理解孤立处理，忽视了两者间的相互作用，限制了泛化能力和下游任务表现。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一框架，同时处理3D场景的几何结构和语义理解，提高3D场景分析的准确性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出IGGT模型，设计3D一致性对比学习策略，通过仅2D视觉输入编码具有几何结构和实例聚类的统一表示，并构建InsScene-15K数据集，包含高质量RGB图像、姿态、深度图和3D一致的实例级掩码注释。&lt;h4&gt;主要发现&lt;/h4&gt;通过统一几何结构和语义理解的方法，可以改善3D场景的理解和重建效果，实现从2D输入到连贯3D场景的有效转换。&lt;h4&gt;结论&lt;/h4&gt;所提出的IGGT方法和3D一致性对比学习策略能够有效地将2D视觉输入转换为连贯的3D场景，并明确区分对象实例，为3D场景分析提供了新的统一框架。&lt;h4&gt;翻译&lt;/h4&gt;人类自然地将3D世界的几何结构和语义内容作为相互交织的维度来感知，从而能够连贯准确地理解复杂场景。然而，先前的方法优先训练大型几何模型进行低级3D重建，将高级空间理解孤立处理，忽视了这两个基本方面之间的相互作用，从而限制了泛化能力并在下游3D理解任务中表现不佳。最近的尝试通过简单地将3D模型与特定语言模型对齐来缓解这一问题，从而限制了感知能力，并限制了下游任务的适应性。在本文中，我们提出了InstanceGrounded Geometry Transformer (IGGT)，一个端到端的大型统一transformer，用于统一空间重建和实例级上下文理解的知识。具体来说，我们设计了一种3D一致性对比学习策略，指导IGGT仅通过2D视觉输入来编码具有几何结构和实例聚类的统一表示。该表示支持将2D视觉输入一致提升为具有明确不同对象实例的连贯3D场景。为了促进这一任务，我们进一步构建了InsScene-15K，这是一个大规模数据集，包含高质量的RGB图像、姿态、深度图和3D一致的实例级掩码注释，采用新颖的数据整理流程。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D场景几何重建与高层次语义理解相分离的问题。人类自然地将3D世界的几何结构和语义内容视为交织在一起的维度，而现有方法通常将这两个方面视为独立任务，导致它们无法相互增强，限制了模型在下游任务中的泛化能力和性能。这个问题很重要，因为统一的几何-语义表示对于机器人操作、增强现实/虚拟现实和空间规划等应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法的局限性：几何重建和语义理解被分离处理，或简单地将3D模型与特定语言模型对齐，导致感知能力受限和适应性差。他们设计了一个端到端的统一框架，通过联合训练耦合几何和语义特征，让模型自主学习3D实例级语义与几何结构的关系。作者借鉴了VGGT的结构，使用DINOv2进行特征提取，采用DPT架构进行密集预测，并利用SAM2进行数据标注。他们还创新性地设计了3D一致的对比学习策略来增强模型性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合训练将几何和实例级语义特征耦合，实现上下文理解和几何重建的相互改进。整体流程包括：1)接收多视图图像输入；2)使用大型统一变换器提取统一表示；3)通过几何头和实例头分别预测几何结构和实例特征；4)应用跨模态融合块增强实例特征的细粒度空间感知；5)使用3D一致的对比监督确保多视图一致性；6)通过聚类生成实例掩码，用于下游任务如空间跟踪、开放词汇分割和场景理解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的3D重建与理解框架IGGT；2)3D一致的对比学习策略；3)InsScene-15K大规模数据集；4)实例级场景理解范式。相比之前的工作，不同之处在于：不是简单对齐几何与语言特征，而是通过联合训练实现相互增强；不与特定视觉语言模型紧密耦合，可以集成更强大的基础模型；能够区分同一语义类别内的不同对象，支持更复杂的下游应用；实现了即插即用的方式与各种视觉语言模型和大型多模态模型集成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; IGGT通过提出统一的实例级几何变换器框架和3D一致的对比学习策略，实现了几何重建与语义理解的深度融合，显著提升了3D场景重建与理解的质量和一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans naturally perceive the geometric structure and semantic content of a3D world as intertwined dimensions, enabling coherent and accurateunderstanding of complex scenes. However, most prior approaches prioritizetraining large geometry models for low-level 3D reconstruction and treathigh-level spatial understanding in isolation, overlooking the crucialinterplay between these two fundamental aspects of 3D-scene analysis, therebylimiting generalization and leading to poor performance in downstream 3Dunderstanding tasks. Recent attempts have mitigated this issue by simplyaligning 3D models with specific language models, thus restricting perceptionto the aligned model's capacity and limiting adaptability to downstream tasks.In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), anend-to-end large unified transformer to unify the knowledge for both spatialreconstruction and instance-level contextual understanding. Specifically, wedesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encodea unified representation with geometric structures and instance-groundedclustering through only 2D visual inputs. This representation supportsconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitlydistinct object instances. To facilitate this task, we further constructInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depthmaps, and 3D-consistent instance-level mask annotations with a novel datacuration pipeline.</description>
      <author>example@mail.com (Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, Manyuan Zhang, Gang Yu, Dingwen Zhang, Ziwei Liu)</author>
      <guid isPermaLink="false">2510.22706v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework for Lane Keeping in Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2510.22370v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/Amin-A96/BLIP-FusePPO-A-Vision-Language-Deep-Reinforcement-Learning-Framework-for-Lane-Keeping-in-Autonomous.git&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了BLIP-FusePPO，一种用于自动驾驶车道保持的新型多模态强化学习框架，将视觉语言模型生成的语义嵌入与几何状态、LiDAR观测和PID控制反馈相融合。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶中的车道保持需要结合高级语义理解和低级控制信号，而现有方法可能仅使用语义模型来塑造奖励或未充分利用多模态信息。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够学习具有环境意识且易于理解的驾驶规则的多模态强化学习框架，结合视觉语言模型的高级场景理解与低级控制和空间信号。&lt;h4&gt;方法&lt;/h4&gt;提出BLIP-FusePPO框架，将视觉语言模型生成的语义嵌入直接融合到代理观测空间中的几何状态、LiDAR观测和PID控制反馈，结合语义、几何和控制感知表示，并使用包含语义对齐、车道保持准确性、障碍物避让和速度调节的混合奖励函数。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在车道保持的稳定性和适应性方面优于最佳视觉和多模态强化学习基线，在各种困难的驾驶情况下表现良好，且直接嵌入语义特征减少了昂贵的运行时推理，确保语义指导始终可用。&lt;h4&gt;结论&lt;/h4&gt;BLIP-FusePPO是一个有效的多模态强化学习框架，能够提高自动驾驶车道保持任务的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了基于引导语言-图像预训练的融合状态表示近端策略优化（BLIP-FusePPO），这是一种用于自动驾驶车道保持的新型多模态强化学习框架，其中视觉语言模型生成的语义嵌入直接与几何状态、LiDAR观测和基于比例-积分-微分的控制反馈在代理观测空间内融合。所提出的方法通过结合视觉语言模型的高级场景理解与低级控制和空间信号，使代理能够学习具有环境意识且易于理解的驾驶规则。我们的架构将语义、几何和控制感知表示结合在一起，使策略学习更加稳健。包含语义对齐、车道保持准确性、障碍物避让和速度调节的混合奖励函数有助于学习更加高效和可泛化。我们的方法不同于仅使用语义模型来塑造奖励的方法，而是直接将语义特征嵌入到状态表示中。这减少了昂贵的运行时推理，并确保语义指导始终可用。仿真结果表明，在广泛的困难驾驶情况下，所提出的模型在车道保持的稳定性和适应性方面优于最佳视觉和多模态强化学习基线。我们公开提供代码。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶车辆车道保持任务中语义感知与控制感知融合不足的问题。这一问题在现实中很重要，因为现有系统在复杂环境（如车道标记磨损、不同光照条件或被遮挡车道）中表现有限，而缺乏对场景语义理解的系统难以适应多变路况，影响自动驾驶的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统视觉方法缺乏语义理解、多模态RL方法仅用VLM塑造奖励而非融入状态、传统控制器缺乏可解释性。作者借鉴了BLIP视觉语言模型用于语义提取、PPO算法用于稳定策略学习、PID控制器提供控制反馈等现有工作。创新点在于将语义特征与几何状态、LiDAR观测和PID控制反馈直接融合到状态表示中，而非仅用于奖励塑造。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将语义感知（通过BLIP提取）与控制感知（通过PID获取）直接融合到强化学习智能体的状态表示中，使智能体同时理解场景语义上下文和执行精确控制。整体流程包括：1)混合状态表示（RGB视觉、LiDAR数据、PID反馈、语义嵌入）；2)预处理和特征提取；3)数据增强（水平镜像）；4)连续动作空间设计（转向和速度控制）；5)混合奖励函数（车道保持、障碍物避免、速度匹配等）；6)使用PPO算法训练策略网络。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)新颖架构将BLIP语义嵌入和PID信号直接注入状态表示；2)基于PID控制的状态增强技术提高学习稳定性；3)新型混合奖励函数整合语义对齐和几何遵循；4)直接语义特征嵌入而非仅用于奖励塑造。相比之前工作，不同之处在于：传统方法缺乏语义理解、现有多模态RL方法仅用VLM塑造奖励、传统控制器缺乏可解释性、基于VLM的RL系统计算开销大。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了BLIP-FusePPO框架，通过融合语义感知与控制感知到状态表示中，显著提高了自动驾驶车道保持的稳定性和适应性，同时降低了计算开销，为更安全鲁棒的自动驾驶系统提供了新思路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose Bootstrapped Language-Image Pretraining-drivenFused State Representation in Proximal Policy Optimization (BLIP-FusePPO), anovel multimodal reinforcement learning (RL) framework for autonomouslane-keeping (LK), in which semantic embeddings generated by a vision-languagemodel (VLM) are directly fused with geometric states, LiDAR observations, andProportional-Integral-Derivative-based (PID) control feedback within the agentobservation space. The proposed method lets the agent learn driving rules thatare aware of their surroundings and easy to understand by combining high-levelscene understanding from the VLM with low-level control and spatial signals.Our architecture brings together semantic, geometric, and control-awarerepresentations to make policy learning more robust. A hybrid reward functionthat includes semantic alignment, LK accuracy, obstacle avoidance, and speedregulation helps learning to be more efficient and generalizable. Our method isdifferent from the approaches that only use semantic models to shape rewards.Instead, it directly embeds semantic features into the state representation.This cuts down on expensive runtime inference and makes sure that semanticguidance is always available. The simulation results show that the proposedmodel is better at LK stability and adaptability than the best vision-based andmultimodal RL baselines in a wide range of difficult driving situations. Wemake our code publicly available.</description>
      <author>example@mail.com (Seyed Ahmad Hosseini Miangoleh, Amin Jalal Aghdasian, Farzaneh Abdollahi)</author>
      <guid isPermaLink="false">2510.22370v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MOGRAS: Human Motion with Grasping in 3D Scenes</title>
      <link>http://arxiv.org/abs/2510.22199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  British Machine Vision Conference Workshop - From Scene Understanding  to Human Modeling&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了MOGRAS数据集和一种简单有效的方法，用于解决在3D场景中生成物理合理的全身抓取运动的挑战，通过定量和定性实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;生成与物体交互的真实全身运动对机器人技术、虚拟现实和人机交互应用至关重要，但现有方法要么缺乏精细任务的保真度，要么忽略了周围3D场景。&lt;h4&gt;目的&lt;/h4&gt;弥合现有方法在生成全身抓取运动方面的局限性，提供能够在3D场景中生成物理合理全身抓取运动的解决方案。&lt;h4&gt;方法&lt;/h4&gt;引入MOGRAS（3D场景中的人体抓取运动）数据集，提供预抓取全身行走运动和最终抓取姿态；利用该数据集基准测试现有方法；提出一种简单有效的方法使现有方法能在3D场景中无缝工作。&lt;h4&gt;主要发现&lt;/h4&gt;现有全身抓握方法在场景感知生成方面存在局限性；所提出的方法在全身抓取运动生成方面取得了显著改进；通过大量定量和定性实验验证了数据集的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究为更真实的人体-场景交互铺平了道路，展示了在3D场景中生成物理合理全身抓取运动的可行性。&lt;h4&gt;翻译&lt;/h4&gt;生成与物体交互的真实全身运动对机器人技术、虚拟现实和人机交互应用至关重要。虽然现有方法可以生成3D场景中的全身运动，但它们通常缺乏精细任务（如物体抓取）的保真度。相反，生成精确抓取运动的方法通常忽略了周围的3D场景。在3D场景中生成物理上合理的全身抓取运动仍然是一个重大挑战。为解决这一问题，我们引入了MOGRAS（3D场景中的人体抓取运动），这是一个弥合这一差距的大规模数据集。MOGRAS在丰富的3D室内场景标注中提供了预抓取的全身行走运动和最终抓取姿态。我们利用MOGRAS对现有的全身抓取方法进行基准测试，展示了它们在场景感知生成方面的局限性。此外，我们提出了一种简单而有效的方法，使现有方法能够在3D场景中无缝工作。通过大量的定量和定性实验，我们验证了数据集的有效性，并突显了我们提出方法所取得的显著改进，为更真实的人体-场景交互铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在3D场景中生成物理合理的全身抓取运动的问题。现有方法要么能生成全身运动但缺乏精细抓保真度，要么能生成精确抓取但忽略周围3D场景。这个问题对机器人、虚拟现实和人机交互等领域至关重要，因为准确建模人-物体交互能支持行为分析、智能机器人系统开发和逼真虚拟环境创建。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别现有研究的差距：全身运动方法缺乏精细抓取能力，抓取方法忽略场景上下文。考虑到手动捕获此类数据成本高昂，他们设计了自动化数据生成框架。作者借鉴了HUMANISE的运动对齐方法、AMASS的行走序列、ScanNetv2的3D环境、BABEL的运动标签、GRAB的抓取物体，并改进了FLEX的抓取姿势生成和PriorMDM的运动填充技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建MOGRAS数据集，提供3D室内场景中的全身抓取运动序列，包括预抓取行走和最终抓取姿势。实现流程分五步：1)行走运动对齐和物体放置；2)改进ScanNet场景的地板对齐；3)使用改进的FLEX生成抓取姿势；4)用PriorMDM生成从行走到抓取的平滑过渡；5)确保数据集规模和质量，通过自动过滤和人类评估保证物理合理性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)MOGRAS数据集：首个结合全身运动、精细抓取和3D场景的大规模合成数据集；2)GNet++方法：通过场景编码和穿透损失实现场景感知抓取；3)场景处理改进：解决地板不对齐问题。相比之前工作，MOGRAS是首个同时包含三种元素(3D场景、精细抓取、全身运动)的数据集，而GNet++显式考虑环境约束，而之前方法如GOAL和SAGA忽略了场景上下文。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过引入首个大规模场景感知全身抓取数据集MOGRAS和相应生成方法GNet++，论文弥合了3D场景中全身运动生成与精细物体抓取之间的差距，为更真实的人-场景交互研究奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating realistic full-body motion interacting with objects is criticalfor applications in robotics, virtual reality, and human-computer interaction.While existing methods can generate full-body motion within 3D scenes, theyoften lack the fidelity for fine-grained tasks like object grasping.Conversely, methods that generate precise grasping motions typically ignore thesurrounding 3D scene. This gap, generating full-body grasping motions that arephysically plausible within a 3D scene, remains a significant challenge. Toaddress this, we introduce MOGRAS (Human MOtion with GRAsping in 3D Scenes), alarge-scale dataset that bridges this gap. MOGRAS provides pre-graspingfull-body walking motions and final grasping poses within richly annotated 3Dindoor scenes. We leverage MOGRAS to benchmark existing full-body graspingmethods and demonstrate their limitations in scene-aware generation.Furthermore, we propose a simple yet effective method to adapt existingapproaches to work seamlessly within 3D scenes. Through extensive quantitativeand qualitative experiments, we validate the effectiveness of our dataset andhighlight the significant improvements our proposed method achieves, paving theway for more realistic human-scene interactions.</description>
      <author>example@mail.com (Kunal Bhosikar, Siddharth Katageri, Vivek Madhavaram, Kai Han, Charu Sharma)</author>
      <guid isPermaLink="false">2510.22199v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>CogStereo: Neural Stereo Matching with Implicit Spatial Cognition Embedding</title>
      <link>http://arxiv.org/abs/2510.22119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为CogStereo的新型立体匹配框架，通过引入空间认知机制来改进立体匹配性能，特别是在处理遮挡或弱纹理等挑战性区域时表现出色，同时提高了跨域泛化能力。&lt;h4&gt;背景&lt;/h4&gt;深度立体匹配通过微调在基准数据集上取得了显著进展，但在零样本泛化方面不如其他视觉任务中的基础模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖数据集特定先验的框架，解决立体匹配中的挑战性问题，提高跨域泛化能力，并将立体视觉转向认知驱动的方法。&lt;h4&gt;方法&lt;/h4&gt;CogStereo通过使用单目深度特征作为先验，将隐式空间认知嵌入到细化过程中，捕获超越局部对应的全场景理解。该方法采用双条件细化机制，结合逐像素不确定性和认知引导特征，实现对不匹配的一致性全局校正。&lt;h4&gt;主要发现&lt;/h4&gt;CogStereo在Scene Flow、KITTI、Middlebury、ETH3D、EuRoc和真实世界等多个数据集上实现了最先进的结果，并在跨域泛化方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;CogStereo成功解决了传统立体匹配方法在处理挑战性区域时的局限性，通过引入空间认知机制提高了立体视觉系统的泛化能力，推动了立体视觉向认知驱动方向发展。&lt;h4&gt;翻译&lt;/h4&gt;深度立体匹配通过微调在基准数据集上取得了显著进展，但在零样本泛化方面不如其他视觉任务中的基础模型。我们引入了CogStereo，一种新颖的框架，它解决了遮挡或弱纹理等挑战性区域，而不依赖于数据集特定的先验。CogStereo通过使用单目深度特征作为先验，将隐式空间认知嵌入到细化过程中，捕获超越局部对应的全局场景理解。这种方法确保了结构一致性的视差估计，即使在仅靠几何不足的区域。CogStereo采用双条件细化机制，结合逐像素不确定性和认知引导特征，实现对不匹配的一致性全局校正。在Scene Flow、KITTI、Middlebury、ETH3D、EuRoc和真实世界上的大量实验表明，CogStereo不仅取得了最先进的结果，还在跨域泛化方面表现出色，将立体视觉转向认知驱动的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决深度立体匹配方法在零样本泛化能力上的不足问题。当前方法虽然在基准数据集上表现良好，但在遮挡区域、弱纹理等困难区域表现不佳，且缺乏强大的零样本泛化能力。这个问题在自动驾驶、机器人等应用中至关重要，因为这些应用需要在各种不同环境下保持一致的深度估计性能，而现有方法过度依赖局部几何对应，缺乏全局场景理解能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察立体匹配与其他视觉任务基础模型的差距，引入了'空间认知'概念，借鉴单目深度估计的成功经验，特别是Depth Anything v2捕获的物体级几何和全局场景理解能力。作者还借鉴了条件控制思想，设计了双条件修正机制。创新之处在于将不确定性估计提前到成本体积阶段，而非视差回归之后，并设计了不确定性引导的空间认知注意力机制、低不确定性区域的KNN对齐策略和突然深度差异感知梯度损失等组件。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将隐式空间认知嵌入到立体匹配过程中，利用单目深度特征作为先验，超越局部对应关系，捕获整体场景理解。整体流程分为两阶段：第一阶段是成本体积不确定性估计预训练，使用标准立体匹配骨干网络提取特征，构建三维成本体积，并预测每个像素的不确定性；第二阶段是通过空间认知的双条件修正，整合不确定性先验与空间认知特征，使用注意力机制进行视差修正，并通过KNN对齐防止度量漂移，最后应用特殊损失函数确保视差图的平滑性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将隐式空间认知概念引入立体匹配；2)设计双条件修正机制结合不确定性和认知特征；3)在成本体积阶段而非视差回归后进行不确定性估计；4)引入低不确定性区域的KNN对齐策略防止度量漂移；5)设计突然深度差异感知梯度损失。相比之前工作，CogStereo超越了纯几何匹配，实现了强大的零样本泛化，改变了不确定性处理方式，创新性地利用深度基础模型的中间特征而非原始深度预测，并针对遮挡、弱纹理等困难区域提供了更鲁棒的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CogStereo通过引入隐式空间认知嵌入到立体匹配过程中，结合像素级不确定性与认知引导特征，显著提升了在遮挡、弱纹理等困难区域的零样本泛化能力和视差估计准确性，实现了从纯几何匹配向认知驱动立体视觉的转变。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep stereo matching has advanced significantly on benchmark datasets throughfine-tuning but falls short of the zero-shot generalization seen in foundationmodels in other vision tasks. We introduce CogStereo, a novel framework thataddresses challenging regions, such as occlusions or weak textures, withoutrelying on dataset-specific priors. CogStereo embeds implicit spatial cognitioninto the refinement process by using monocular depth features as priors,capturing holistic scene understanding beyond local correspondences. Thisapproach ensures structurally coherent disparity estimation, even in areaswhere geometry alone is inadequate. CogStereo employs a dual-conditionalrefinement mechanism that combines pixel-wise uncertainty with cognition-guidedfeatures for consistent global correction of mismatches. Extensive experimentson Scene Flow, KITTI, Middlebury, ETH3D, EuRoc, and real-world demonstrate thatCogStereo not only achieves state-of-the-art results but also excels incross-domain generalization, shifting stereo vision towards a cognition-drivenapproach.</description>
      <author>example@mail.com (Lihuang Fang, Xiao Hu, Yuchen Zou, Hong Zhang)</author>
      <guid isPermaLink="false">2510.22119v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.21881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对大型语言模型在几何视觉推理任务中的性能下降问题，开发了一个名为GeoThoughts的几何推理数据集和一个名为GeoThought-MLLM的多模态数学推理模型，通过链式思维训练显著提升了模型在几何问题上的表现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在文本数学问题解决中表现出强大的推理能力，但在视觉推理任务，特别是几何问题解决中，其性能显著下降。这是因为几何问题具有独特挑战：一是几何本身的复杂性需要详细的图像理解和多步推理；二是现有数据集规模不足、多样性有限且缺乏明确的推理痕迹，阻碍了有效模型训练。&lt;h4&gt;目的&lt;/h4&gt;开发一个全面的几何推理数据集和一个多模态数学推理模型，以解决大型语言模型在几何问题解决中的性能下降问题。&lt;h4&gt;方法&lt;/h4&gt;1. 创建了GeoThoughts数据集，包含两个子集：Geo-Thought-6K（6,243个样本）和Geo-Thought-Augmented-10K（10,834个样本）。2. 每个数据条目包括视觉描述、分步解决方案、明确的推理链、反思步骤和最终答案。3. 基于此数据集开发了GeoThought-MLLM，一个在问题解决过程中生成详细思考过程的多模态数学推理模型。&lt;h4&gt;主要发现&lt;/h4&gt;1. 使用链式思维数据集训练的GeoThought-MLLM在几何任务上优于现有基准。2. 训练显著提升了模型在领域内和领域外几何推理能力。3. 错误主要源于数学概念错误解释或空间判断失误。4. 通过调用链式思维纠正这些错误，模型能够产生正确答案。&lt;h4&gt;结论&lt;/h4&gt;GeoThoughts数据集和GeoThought-MLLM模型有效解决了大型语言模型在几何问题解决中的性能下降问题，为几何视觉推理任务提供了新的解决方案和见解。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在基于文本的数学问题解决中表现出强大的推理能力；然而，当适应到视觉推理任务，特别是几何问题解决时，它们的性能大幅下降，因为几何问题带来了独特的挑战。具体来说，这些挑战源于两个关键因素：首先，几何本身的复杂性需要详细的图像理解和多步推理；其次，现有数据集的规模、多样性和明确的推理痕迹不足，从而阻碍了有效的模型训练。为应对这些挑战，我们开发了GeoThoughts数据集，这是一个全面的几何推理语料库，包含两个子集：包含6,243个样本的Geo-Thought-6K及其增强版本Geo-Thought-Augmented-10K，包含10,834个样本。每个条目包括视觉描述、分步解决方案、明确的推理链、反思步骤和最终答案。使用此数据集，我们开发了GeoThought-MLLM，一个在问题解决过程中生成详细思考过程的数学推理多模态模型。我们的模型在几何任务上优于现有基准，证明使用我们的链式思维数据集进行训练可以提升领域内和领域外设置的几何推理能力。最后，我们分析了失败案例，发现错误主要源于数学概念错误解释或空间判断失误。通过调用链式思维纠正这些错误，模型产生了正确答案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated strong reasoning capabilitiesin text-based mathematical problem solving; however, when adapted to visualreasoning tasks, particularly geometric problem solving, their performancesubstantially declines because geometric problems present unique challenges.Specifically, these challenges stem from two key factors: first, the intrinsiccomplexity of geometry requiring detailed image comprehension and multi-stepreasoning, and second, the limitations of existing datasets which lacksufficient scale, diversity, and explicit reasoning traces, consequentlyhindering effective model training. To address these challenges, we developedthe GeoThoughts dataset, a comprehensive geometric reasoning corpus with twosubsets: Geo-Thought-6K with 6,243 samples and its augmented versionGeo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visualdescriptions, step-by-step solutions, explicit reasoning chains, reflectionsteps, and final answers. Using this dataset, we developed GeoThought-MLLM, amathematical reasoning multimodal model that generates detailed thinkingprocesses during problem-solving. Our model outperforms existing benchmarks ingeometric tasks, demonstrating that training with our Chain-of-Thought datasetimproves geometric reasoning capabilities across both in-domain andout-of-domain settings. Finally, we analyze failure cases and observe thaterrors primarily arise from incorrect interpretation of mathematical conceptsor spatial misjudgment. By invoking CoT to correct these mistakes, the modelproduces correct answers.</description>
      <author>example@mail.com (Nannan Shi, Chuanyu Qin, Shipeng Song, Man Luo)</author>
      <guid isPermaLink="false">2510.21881v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>GRAPHIA: Harnessing Social Graph Data to Enhance LLM-Based Social Simulation</title>
      <link>http://arxiv.org/abs/2510.24251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Graphia是一种基于大型语言模型的社会图模拟框架，利用图数据作为监督信号通过强化学习对LLM进行后训练，能够预测互动对象和互动方式，在微观和宏观层面都显示出显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在模拟类人社会行为方面展现出潜力，但社会图作为包含局部交互和全局网络结构的高质量监督信号，在LLM训练中仍未得到充分利用。&lt;h4&gt;目的&lt;/h4&gt;提出Graphia框架，利用图数据作为监督信号，通过强化学习对LLM进行后训练，以缩小代理行为与基于LLM的模拟中网络动力学之间的差距。&lt;h4&gt;方法&lt;/h4&gt;Graphia训练专门的代理来预测与谁互动（目标选择）和如何互动（边生成），使用基于GNN的结构性奖励，并设计了图生成流程。在归因动态图生成（TDGG）和归纳动态图生成（IDGG）两种设置下进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界网络上，Graphia相比最强基线模型，微观对齐方面：综合目标选择分数提高6.1%，边分类准确率提高12%，边内容BERTScore提高27.9%；宏观对齐方面：结构相似性提高41.11%，社会现象（如幂律和回音室）复制能力提高32.98%。Graphia还支持反事实模拟，能在平台激励下生成合理的行为转变。&lt;h4&gt;结论&lt;/h4&gt;社会图可以作为LLM后训练的高质量监督信号，有效缩小基于LLM的模拟中代理行为与网络动力学之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在模拟类人社会行为方面展现出潜力。社会图提供了高质量监督信号，编码了局部交互和全局网络结构，但这些信号在LLM训练中仍未得到充分利用。为解决这一差距，我们提出了Graphia，这是第一个基于LLM的社会图模拟通用框架，它利用图数据作为监督信号，通过强化学习对LLM进行后训练。基于GNN的结构性奖励，Graphia训练专门的代理来预测与谁互动（目标选择）和如何互动（边生成），然后使用设计的图生成流程。我们在两种设置下评估Graphia：归因动态图生成（TDGG），这是使用我们提出的节点级交互对齐指标的微观任务；以及归纳动态图生成（IDGG），这是使用我们提出的指标对齐涌现网络属性的宏观任务。在三个真实世界网络上，Graphia相比最强基线模型，在微观对齐方面提高了6.1%的综合目标选择分数，12%的边分类准确率和27.9%的边内容BERTScore。对于宏观对齐，它实现了41.11%更高的结构相似性和32.98%更好的社会现象（如幂律和回音室）复制能力。Graphia还支持反事实模拟，在平台激励下生成合理的行为转变。我们的结果表明，社会图可以作为LLM后训练的高质量监督信号，缩小基于LLM的模拟中代理行为与网络动力学之间的差距。代码可在https://github.com/Ji-Cather/Graphia.git获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have shown promise in simulating human-likesocial behaviors. Social graphs provide high-quality supervision signals thatencode both local interactions and global network structure, yet they remainunderutilized for LLM training. To address this gap, we propose Graphia, thefirst general LLM-based social graph simulation framework that leverages graphdata as supervision for LLM post-training via reinforcement learning. WithGNN-based structural rewards, Graphia trains specialized agents to predict whomto interact with (destination selection) and how to interact (edge generation),followed by designed graph generation pipelines. We evaluate Graphia under twosettings: Transductive Dynamic Graph Generation (TDGG), a micro-level task withour proposed node-wise interaction alignment metrics; and Inductive DynamicGraph Generation (IDGG), a macro-level task with our proposed metrics foraligning emergent network properties. On three real-world networks, Graphiaimproves micro-level alignment by 6.1% in the composite destination selectionscore, 12% in edge classification accuracy, and 27.9% in edge content BERTScoreover the strongest baseline. For macro-level alignment, it achieves 41.11%higher structural similarity and 32.98% better replication of social phenomenasuch as power laws and echo chambers. Graphia also supports counterfactualsimulation, generating plausible behavioral shifts under platform incentives.Our results show that social graphs can serve as high-quality supervisionsignals for LLM post-training, closing the gap between agent behaviors andnetwork dynamics for LLM-based simulation. Code is available athttps://github.com/Ji-Cather/Graphia.git.</description>
      <author>example@mail.com (Jiarui Ji, Zehua Zhang, Zhewei Wei, Bin Tong, Guan Wang, Bo Zheng)</author>
      <guid isPermaLink="false">2510.24251v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MAGNET: A Multi-Graph Attentional Network for Code Clone Detection</title>
      <link>http://arxiv.org/abs/2510.24241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MAGNET是一种多图注意力框架，通过联合利用AST、CFG和DFG表示来捕获源代码的语法和语义特征，实现了代码克隆检测的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;代码克隆检测是软件工程中的基础任务，支持重构、调试、剽窃检测和漏洞分析。现有方法通常依赖单一表示（如AST、CFG、DFG），只能捕捉代码语义的部分方面，而混合方法的融合策略通常是手工设计的且效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出MAGNET框架，一种多图注意力框架，联合利用AST、CFG和DFG表示来捕获源代码的语法和语义特征。&lt;h4&gt;方法&lt;/h4&gt;MAGNET结合残差图神经网络与节点级自注意力学习局部和长距离依赖关系，引入门控交叉注意力机制用于细粒度的图间交互，采用Set2Set池化将多图嵌入融合为统一的程序级表示。&lt;h4&gt;主要发现&lt;/h4&gt;在BigCloneBench和Google Code Jam上的实验表明，MAGNET分别达到96.5%和99.2%的总体F1分数，实现了最先进的性能。消融研究证实了多图融合和每个注意力组件的关键贡献。&lt;h4&gt;结论&lt;/h4&gt;MAGNET通过多图表示和注意力机制实现了高效的代码克隆检测，代码已开源于https://github.com/ZixianReid/Multigraph_match。&lt;h4&gt;翻译&lt;/h4&gt;代码克隆检测是软件工程中的一个基础任务，它支持重构、调试、剽窃检测和漏洞分析。现有方法通常依赖于单一表示，如抽象语法树（AST）、控制流图（CFG）和数据流图（DFG），这些表示只能捕捉代码语义的部分方面。混合方法已经出现，但它们的融合策略通常是手工设计的且效果不佳。在本研究中，我们提出了MAGNET，一种多图注意力框架，它联合利用AST、CFG和DFG表示来捕获源代码的语法和语义特征。MAGNET将残差图神经网络与节点级自注意力相结合，学习局部和长距离依赖关系，引入门控交叉注意力机制用于细粒度的图间交互，并采用Set2Set池化将多图嵌入融合为统一的程序级表示。在BigCloneBench和Google Code Jam上的大量实验表明，MAGNET在两个数据集上分别实现了96.5%和99.2%的总体F1分数，达到了最先进的性能。消融研究证实了多图融合和每个注意力组件的关键贡献。我们的代码可在https://github.com/ZixianReid/Multigraph_match获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Code clone detection is a fundamental task in software engineering thatunderpins refactoring, debugging, plagiarism detection, and vulnerabilityanalysis. Existing methods often rely on singular representations such asabstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs(DFGs), which capture only partial aspects of code semantics. Hybrid approacheshave emerged, but their fusion strategies are typically handcrafted andineffective. In this study, we propose MAGNET, a multi-graph attentionalframework that jointly leverages AST, CFG, and DFG representations to capturesyntactic and semantic features of source code. MAGNET integrates residualgraph neural networks with node-level self-attention to learn both local andlong-range dependencies, introduces a gated cross-attention mechanism forfine-grained inter-graph interactions, and employs Set2Set pooling to fusemulti-graph embeddings into unified program-level representations. Extensiveexperiments on BigCloneBench and Google Code Jam demonstrate that MAGNETachieves state-of-the-art performance with an overall F1 score of 96.5\% and99.2\% on the two datasets, respectively. Ablation studies confirm the criticalcontributions of multi-graph fusion and each attentional component. Our code isavailable at https://github.com/ZixianReid/Multigraph_match</description>
      <author>example@mail.com (Zixian Zhang, Takfarinas Saber)</author>
      <guid isPermaLink="false">2510.24241v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>HyperGraphX: Graph Transductive Learning with Hyperdimensional Computing and Message Passing</title>
      <link>http://arxiv.org/abs/2510.23980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为hdgc的新型算法，该算法结合了图卷积与高维计算中的绑定和捆绑操作，用于归纳图学习。&lt;h4&gt;背景&lt;/h4&gt;在图学习领域，图神经网络和高维计算是两种重要的方法，各有优势和局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时利用图神经网络和高维计算优势的新算法，提高图学习的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;hdgc算法将图卷积操作与高维计算中的绑定和捆绑操作相结合，主要在二进制向量上进行学习操作。&lt;h4&gt;主要发现&lt;/h4&gt;hdgc在预测准确性上优于主流图神经网络实现和最先进的高维计算实现，适用于同质图和异质图；在相同GPU平台上，hdgc比gcnii图神经网络实现平均快9561倍，比hdgl高维计算实现平均快144.5倍。&lt;h4&gt;结论&lt;/h4&gt;hdgc算法在多种图类型上表现出色，由于主要操作在二进制向量上进行，预期在神经形态和新兴存储器处理设备上具有出色的能源性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为hdgc的新颖算法，该算法将图卷积与高维计算中的绑定和捆绑操作相结合，用于归纳图学习。在预测准确性方面，hdgc优于主要的和流行的图神经网络实现以及最先进的高维计算实现，适用于一系列同质图和异质图。与我们测试的最准确的学习方法相比，在相同的目标GPU平台上，hdgc比图神经网络实现gcnii平均快9561.0倍，比高维计算实现hdgl平均快144.5倍。由于大部分学习操作在二进制向量上进行，我们期望hdgc在神经形态和新兴存储器处理设备上具有出色的能源性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel algorithm, \hdgc, that marries graph convolution withbinding and bundling operations in hyperdimensional computing for transductivegraph learning. For prediction accuracy \hdgc outperforms major and populargraph neural network implementations as well as state-of-the-arthyperdimensional computing implementations for a collection of homophilicgraphs and heterophilic graphs. Compared with the most accurate learningmethodologies we have tested, on the same target GPU platform, \hdgc is onaverage 9561.0 and 144.5 times faster than \gcnii, a graph neural networkimplementation and HDGL, a hyperdimensional computing implementation,respectively. As the majority of the learning operates on binary vectors, weexpect outstanding energy performance of \hdgc on neuromorphic and emergingprocess-in-memory devices.</description>
      <author>example@mail.com (Guojing Cong, Tom Potok, Hamed Poursiami, Maryam Parsa)</author>
      <guid isPermaLink="false">2510.23980v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Exploring an image-based $b$-jet tagging method using convolution neural networks</title>
      <link>http://arxiv.org/abs/2510.23962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于图像的喷流味道识别方法，利用主顶点周围的图像和喷流锥内的带电粒子，通过卷积神经网络进行分析，在b-喷流识别中实现了80-90%的效率，有望提高高能核物理实验的准确性。&lt;h4&gt;背景&lt;/h4&gt;喷流味道识别（识别起源于c夸克、b夸克和其他夸克（轻夸克和胶子）的喷流）在高能重离子物理中至关重要，因为它能够研究重离子碰撞产生的热密核介质中的味道依赖响应。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的喷流味道识别方法，提高在高能核物理实验中的准确性。&lt;h4&gt;方法&lt;/h4&gt;基于主顶点周围的图像，利用喷流锥内的带电粒子（可通过硅跟踪系统测量），使用卷积神经网络进行分析。研究假设跟踪系统具有理想性能。&lt;h4&gt;主要发现&lt;/h4&gt;基于图像的味道识别方法在横向动量范围从20到100 GeV/c的喷流中，实现了80-90%的b-喷流识别效率。&lt;h4&gt;结论&lt;/h4&gt;这种基于图像的喷流味道识别方法有潜力显著提高高能核物理实验中喷流味道识别的准确性。&lt;h4&gt;翻译&lt;/h4&gt;喷流味道识别，即识别起源于c夸克、b夸克和其他夸克（轻夸克和胶子）的喷流，是高能重离子物理中的关键任务，因为它能够研究重离子碰撞产生的热密核介质中的味道依赖响应。最近，基于深度学习技术（如深度神经网络和图神经网络）的几种方法已被开发。这些基于深度学习的方法相比依赖轨迹影响参数和次级顶点的传统方法表现出显著改进的性能。在识别算法中，使用了喷流和组成带电粒子的各种属性作为输入参数。我们探索了一种基于主顶点周围图像的新方法，利用喷流锥内的带电粒子，这些粒子可以通过硅跟踪系统测量。对于这项初步实验研究，我们假设跟踪系统具有理想性能。为了分析这些图像，我们采用了卷积神经网络。基于图像的味道识别方法在横向动量范围从20到100 GeV/c的喷流中显示了80-90%的b-喷流识别效率。这种方法有潜力显著提高高能核物理实验中喷流味道识别的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/s40042-025-01506-3&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Jet flavor tagging, the identification of jets originating from $c$-quarks,$b$-quarks, and other quarks (light quarks and gluons), is a crucial task inhigh-energy heavy-ion physics, as it enables the investigation offlavor-dependent responses within the hot and dense nuclear medium produced inheavy-ion collisions. Recently, several methods based on deep learningtechniques, such as deep neural networks and graph neural networks, have beendeveloped. These deep-learning-based methods demonstrate significantly improvedperformance compared to traditional methods that rely on track impactparameters and secondary vertices. In the tagging algorithms, variousproperties of jets and constituent charged particles are used as inputparameters. We explore a new method based on images surrounding the primaryvertex, utilizing charged particles within the jet cone, which can be measuredusing a silicon tracking system. For this initial experimental study, we assumethe ideal performance of the tracking system. To analyze these images, weemployed convolutional neural networks. The image-based flavor tagging methodshows an 80-90% $b$-jet tagging efficiency for jets in the transverse momentumrange from 20 to 100 GeV/$c$. This approach has the potential to significantlyimprove the accuracy of jet flavor tagging in high-energy nuclear physicsexperiments.</description>
      <author>example@mail.com (Hangil Jang, Sanghoon Lim)</author>
      <guid isPermaLink="false">2510.23962v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network Assisted Genetic Algorithm for Structural Dynamic Response and Parameter Optimization</title>
      <link>http://arxiv.org/abs/2510.22839v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器，用于结构参数优化，以克服传统数值方法计算成本高的问题。&lt;h4&gt;背景&lt;/h4&gt;优化结构参数(如质量m、刚度k和阻尼系数c)对于设计高效、有韧性和稳定的结构至关重要。传统的数值方法，如有限元法(FEM)和计算流体动力学(CFD)模拟，虽然能提供高精度结果，但在迭代优化任务中计算成本高昂，因为每次评估都需要为每个参数组合求解控制方程。&lt;h4&gt;目的&lt;/h4&gt;开发一种计算效率更高的方法来优化结构参数，避免传统数值方法的计算负担，实现自动化和智能化的结构设计。&lt;h4&gt;方法&lt;/h4&gt;提出混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器；使用GNN学习结构参数与动态位移响应之间的非线性映射；使用Newmark Beta方法生成单自由度(SDOF)系统响应数据集；GA通过最小化预测位移和提高动态稳定性来搜索全局最优参数集。&lt;h4&gt;主要发现&lt;/h4&gt;GNN和GA框架实现了强收敛性和鲁棒泛化能力；与传统模拟相比，显著降低了计算成本；结合机器学习代理和进化优化的方法对于自动化和智能结构设计是有效的。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法克服了传统数值优化方法的计算效率问题，为结构参数优化提供了一种有效途径，展示了机器学习代理与进化优化结合在自动化和智能结构设计中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;结构参数(如质量m、刚度k和阻尼系数c)的优化对于设计高效、有韧性和稳定的结构至关重要。传统的数值方法，包括有限元法(FEM)和计算流体动力学(CFD)模拟，虽然能提供高精度结果，但在迭代优化任务中计算成本高昂，因为每次评估都需要为每个参数组合求解控制方程。本研究提出了一种混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器来克服这些挑战。GNN被训练以准确学习结构参数与动态位移响应之间的非线性映射，实现无需重复求解系统方程的快速预测。使用Newmark Beta方法生成单自由度(SDOF)系统响应数据集，涵盖多种质量、刚度和阻尼配置。然后，GA通过最小化预测位移和提高动态稳定性来搜索全局最优参数集。结果表明，与传统模拟相比，GNN和GA框架实现了强收敛性、鲁棒泛化能力和显著降低的计算成本。这种方法突显了将机器学习代理与进化优化相结合用于自动化和智能结构设计的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The optimization of structural parameters, such as mass(m), stiffness(k), anddamping coefficient(c), is critical for designing efficient, resilient, andstable structures. Conventional numerical approaches, including Finite ElementMethod (FEM) and Computational Fluid Dynamics (CFD) simulations, providehigh-fidelity results but are computationally expensive for iterativeoptimization tasks, as each evaluation requires solving the governing equationsfor every parameter combination. This study proposes a hybrid data-drivenframework that integrates a Graph Neural Network (GNN) surrogate model with aGenetic Algorithm (GA) optimizer to overcome these challenges. The GNN istrained to accurately learn the nonlinear mapping between structural parametersand dynamic displacement responses, enabling rapid predictions withoutrepeatedly solving the system equations. A dataset of single-degree-of-freedom(SDOF) system responses is generated using the Newmark Beta method acrossdiverse mass, stiffness, and damping configurations. The GA then searches forglobally optimal parameter sets by minimizing predicted displacements andenhancing dynamic stability. Results demonstrate that the GNN and GA frameworkachieves strong convergence, robust generalization, and significantly reducedcomputational cost compared to conventional simulations. This approachhighlights the effectiveness of combining machine learning surrogates withevolutionary optimization for automated and intelligent structural design.</description>
      <author>example@mail.com (Sagnik Mukherjee)</author>
      <guid isPermaLink="false">2510.22839v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2510.24688v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;基于基础设施的感知在智能交通系统中起着关键作用，提供全局态势感知并支持协作自动驾驶。然而，现有基于摄像头的检测模型在多视图基础设施设置、多样化摄像头配置、降级视觉输入和各种道路布局等场景下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出MIC-BEV，一种基于Transformer的鸟瞰图(BEV)感知框架，用于基于基础设施的多摄像头3D物体检测。&lt;h4&gt;方法&lt;/h4&gt;MIC-BEV灵活支持具有异构内参和外参的变量摄像头数量，在传感器降级情况下表现出强大的鲁棒性。提出的图增强融合模块通过利用摄像头和BEV单元之间的几何关系以及潜在视觉线索，将多视图图像特征集成到BEV空间。同时引入M2I数据集，用于基于基础设施的物体检测，具有多样化的摄像头配置、道路布局和环境条件。&lt;h4&gt;主要发现&lt;/h4&gt;在M2I和真实世界数据集RoScenes上的大量实验表明，MIC-BEV在3D物体检测方面实现了最先进的性能，在极端天气和传感器降级等具有挑战性的条件下保持鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;MIC-BEV的结果突显了其在现实世界部署的潜力，数据集和源代码可在GitHub链接获取。&lt;h4&gt;翻译&lt;/h4&gt;基于基础设施的感知在智能交通系统中起着关键作用，提供全局态势感知并支持协作自动驾驶。然而，现有基于摄像头的检测模型在多视图基础设施设置、多样化摄像头配置、降级视觉输入和各种道路布局等场景下表现不佳。我们提出MIC-BEV，一种基于Transformer的鸟瞰图(BEV)感知框架，用于基于基础设施的多摄像头3D物体检测。MIC-BEV灵活支持具有异构内参和外参的变量摄像头数量，在传感器降级情况下表现出强大的鲁棒性。MIC-BEV中提出的图增强融合模块通过利用摄像头和BEV单元之间的几何关系以及潜在视觉线索，将多视图图像特征集成到BEV空间。为支持训练和评估，我们引入M2I数据集，用于基于基础设施的物体检测，具有多样化的摄像头配置、道路布局和环境条件。在M2I和真实世界数据集RoScenes上的大量实验表明，MIC-BEV在3D物体检测方面实现了最先进的性能，在极端天气和传感器降级等具有挑战性的条件下保持鲁棒性。这些结果突显了MIC-BEV在现实世界部署的潜力。数据集和源代码可在以下链接获取：https://github.com/HandsomeYun/MIC-BEV。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于基础设施的多摄像头3D目标检测问题，特别是在多视图基础设施设置、多样化摄像头配置、退化视觉输入和复杂道路布局等挑战场景下的性能提升。这个问题在现实中很重要，因为基于基础设施的感知是智能交通系统的关键组成部分，能提供全局态势感知和协同自主能力；同时，相比昂贵的激光雷达方案，摄像头更实惠、可扩展且提供丰富的语义信息，但现有摄像头检测模型在这些复杂场景中表现不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了基础设施感知与车载感知的区别，指出基础设施摄像头空间分布广泛，具有异构参数，这给现有BEV方法带来挑战。他们设计MIC-BEV框架时借鉴了现有BEV感知方法（如BEVFormer）的Transformer架构，但针对基础设施场景进行了创新。具体来说，他们引入了图增强融合模块，利用摄像头和BEV单元间的几何关系进行特征融合；借鉴了可变形注意力机制进行特征提取；采用DETR风格解码器进行目标检测；并使用图注意力网络(GAT)来学习融合权重，使模型能够自适应地处理不同摄像头配置。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过图神经网络建模摄像头和BEV单元之间的几何关系，实现关系感知的多视图特征融合，使模型能够根据每个摄像头与BEV单元的几何相关性动态分配权重。整体流程包括：1)处理可变数量的摄像头输入；2)使用ResNet-101和FPN提取多尺度特征；3)初始化可学习的BEV查询；4)通过Transformer编码进行特征处理，包括时间自注意力和关系增强空间交叉注意力(ReSCA)；5)使用任务特定解码器进行3D目标检测和BEV语义分割。其中ReSCA模块是关键，它为每个BEV查询生成3D参考点，投影到摄像头视图，使用可变形注意力提取特征，并通过GAT学习融合权重。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)MIC-BEV框架，专门设计用于处理基础设施多摄像头系统的异构配置；2)关系增强的空间交叉注意力(ReSCA)，利用图神经网络建模几何关系；3)M2I数据集，提供多样化的摄像头配置、道路布局和环境条件；4)摄像头掩码策略，提高对传感器降级的鲁棒性。相比之前工作，MIC-BEV能适应不同数量、方向、高度和视场的摄像头；使用显式的关系建模而非隐式融合；考虑摄像头和BEV单元间的几何关系；通过多任务学习（3D检测和BEV分割）增强空间理解；并在训练中模拟传感器故障以提高鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MIC-BEV通过引入关系感知的图增强融合机制和M2I多样化数据集，显著提升了基础设施多摄像头系统在复杂场景下的3D目标检测性能和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infrastructure-based perception plays a crucial role in intelligenttransportation systems, offering global situational awareness and enablingcooperative autonomy. However, existing camera-based detection models oftenunderperform in such scenarios due to challenges such as multi-viewinfrastructure setup, diverse camera configurations, degraded visual inputs,and various road layouts. We introduce MIC-BEV, a Transformer-basedbird's-eye-view (BEV) perception framework for infrastructure-basedmulti-camera 3D object detection. MIC-BEV flexibly supports a variable numberof cameras with heterogeneous intrinsic and extrinsic parameters anddemonstrates strong robustness under sensor degradation. The proposedgraph-enhanced fusion module in MIC-BEV integrates multi-view image featuresinto the BEV space by exploiting geometric relationships between cameras andBEV cells alongside latent visual cues. To support training and evaluation, weintroduce M2I, a synthetic dataset for infrastructure-based object detection,featuring diverse camera configurations, road layouts, and environmentalconditions. Extensive experiments on both M2I and the real-world datasetRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3Dobject detection. It also remains robust under challenging conditions,including extreme weather and sensor degradation. These results highlight thepotential of MIC-BEV for real-world deployment. The dataset and source code areavailable at: https://github.com/HandsomeYun/MIC-BEV.</description>
      <author>example@mail.com (Yun Zhang, Zhaoliang Zheng, Johnson Liu, Zhiyu Huang, Zewei Zhou, Zonglin Meng, Tianhui Cai, Jiaqi Ma)</author>
      <guid isPermaLink="false">2510.24688v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Optimizing Retrieval for RAG via Reinforced Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.24652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;R3是一种通过试验和反馈强化对比学习为RAG优化的检索框架，能够在RAG环境中动态探索和优化相关性，实验表明其性能优于原始检索器和最先进检索器，且高效实用。&lt;h4&gt;背景&lt;/h4&gt;随着检索增强生成(RAG)的普及，信息检索(IR)的角色正从为人类用户检索信息转变为为AI系统检索上下文知识，这使得相关性难以预先定义或标注。&lt;h4&gt;目的&lt;/h4&gt;提出一种解决方案，使检索器能够在RAG环境中动态探索和优化相关性，无需依赖预先标注的数据。&lt;h4&gt;方法&lt;/h4&gt;R3框架通过检索结果与环境交互产生对比信号，自动引导检索器的自我改进，不同于依赖标注或合成数据进行监督微调的先前方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明R3比原始检索器提高RAG性能5.2%，超越最先进检索器4.9%，同时与LLM增强检索和基于后训练或指令调整LLM的RAG系统性能相当。&lt;h4&gt;结论&lt;/h4&gt;R3是一种高效实用的解决方案，解决了在RAG环境中定义和优化相关性的挑战，只需4个GPU一天内完成训练。&lt;h4&gt;翻译&lt;/h4&gt;随着检索增强生成(RAG)变得越来越普遍，信息检索(IR)的角色正从为人类用户检索信息转变为为人工智能(AI)系统检索上下文知识，这使得相关性变得难以预先定义或标注。为应对这一挑战，我们提出了R3，一种通过试验和反馈强化对比学习为RAG优化的检索框架。与依赖标注或合成数据进行监督微调的先前方法不同，R3使检索器能够在RAG环境中动态探索和优化相关性。在训练过程中，检索结果与环境交互以产生对比信号，自动引导检索器的自我改进。在各种不同任务上的大量实验表明，R3比原始检索器提高RAG性能5.2%，超越最先进检索器4.9%，同时与LLM增强检索和基于后训练或指令调整LLM构建的RAG系统相当。R3既高效又实用，只需4个GPU，并在一天内完成训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As retrieval-augmented generation (RAG) becomes increasingly widespread, therole of information retrieval (IR) is shifting from retrieving information forhuman users to retrieving contextual knowledge for artificial intelligence (AI)systems, where relevance becomes difficult to define or annotate beforehand. Toaddress this challenge, we propose R3, a Retrieval framework optimized for RAGthrough trialand-feedback Reinforced contrastive learning. Unlike priorapproaches that rely on annotated or synthetic data for supervised fine-tuning,R3 enables the retriever to dynamically explore and optimize relevance withinthe RAG environment. During training, the retrieved results interact with theenvironment to produce contrastive signals that automatically guide theretriever's self-improvement. Extensive experiments across diverse tasksdemonstrate that R3 improves RAG performance by 5.2% over the originalretriever and surpasses state-of-the-art retrievers by 4.9%, while achievingcomparable results to LLM-augmented retrieval and RAG systems built onpost-trained or instruction-tuned LLMs. It is both efficient and practical,requiring only 4 GPUs and completing training within a single day.</description>
      <author>example@mail.com (Jiawei Zhou, Lei Chen)</author>
      <guid isPermaLink="false">2510.24652v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DeshadowMamba: Deshadowing as 1D Sequential Similarity</title>
      <link>http://arxiv.org/abs/2510.24260v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于Mamba模型的图像阴影去除方法，通过引入CrossGate方向调制机制和ColorShift正则化技术，有效解决了现有方法中阴影结构扭曲和颜色不一致的问题，实现了高质量的阴影去除效果。&lt;h4&gt;背景&lt;/h4&gt;当前基于深度学习的图像阴影去除方法通常依赖于基于注意力的架构来捕获长距离依赖关系，但这些固定的注意模式往往会混合来自不相关区域的照明线索，导致结构扭曲和颜色不一致。&lt;h4&gt;目的&lt;/h4&gt;重新审视阴影去除问题，从序列建模的角度探索更有效的解决方案，解决现有方法中阴影结构扭曲和颜色不一致的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 从序列建模角度探索使用Mamba（选择性状态空间模型）来传播全局上下文；2. 提出CrossGate方向调制机制，将阴影感知相似性注入Mamba的输入门；3. 引入ColorShift正则化，通过合成结构化的信息负样本引导模型抑制颜色污染并实现稳健的颜色恢复。&lt;h4&gt;主要发现&lt;/h4&gt;1. Mamba模型能通过方向状态转换实现有效的全局感受野同时保持位置连续性；2. 直接将Mamba应用于图像数据缺乏阴影-非阴影语义意识且易受颜色干扰；3. 所提出的CrossGate和ColorShift正则化技术能有效解决这些问题；4. DeshadowMamba在公共基准测试上实现了最先进的视觉质量和强大的定量性能。&lt;h4&gt;结论&lt;/h4&gt;通过将序列建模适应于阴影去除所需的完整结构和色度一致性，所提出的方法DeshadowMamba在图像阴影去除任务中取得了显著效果，为解决阴影去除中的结构完整性和色度一致性问题提供了有效途径。&lt;h4&gt;翻译&lt;/h4&gt;最近的图像阴影去除深度模型通常依赖于基于注意力的架构来捕获长距离依赖关系。然而，它们的固定注意模式往往会混合来自不相关区域的照明线索，导致结构扭曲和颜色不一致。在这项工作中，我们从序列建模的角度重新审视阴影去除问题，并探索使用Mamba（一种选择性状态空间模型）来通过方向状态转换传播全局上下文。这些转换产生有效的全局感受野，同时保持位置连续性。尽管有潜力，但直接将Mamba应用于图像数据并非最佳选择，因为它缺乏阴影-非阴影语义意识，并且仍然容易受到附近区域颜色干扰的干扰。为了解决这些局限性，我们提出了CrossGate，一种方向调制机制，将阴影感知相似性注入Mamba的输入门，允许沿过渡轴选择性地集成相关上下文。为了进一步确保外观保真度，我们引入了ColorShift正则化，这是一种由全局颜色统计驱动的对比学习目标。通过合成结构化的信息负样本，它引导模型抑制颜色污染并实现稳健的颜色恢复。这些组件共同将序列建模适应于阴影去除所需的完整结构和色度一致性。在公共基准测试上的大量实验表明，DeshadowMamba实现了最先进的视觉质量和强大的定量性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent deep models for image shadow removal often rely on attention-basedarchitectures to capture long-range dependencies. However, their fixedattention patterns tend to mix illumination cues from irrelevant regions,leading to distorted structures and inconsistent colors. In this work, werevisit shadow removal from a sequence modeling perspective and explore the useof Mamba, a selective state space model that propagates global context throughdirectional state transitions. These transitions yield an efficient globalreceptive field while preserving positional continuity. Despite its potential,directly applying Mamba to image data is suboptimal, since it lacks awarenessof shadow-non-shadow semantics and remains susceptible to color interferencefrom nearby regions. To address these limitations, we propose CrossGate, adirectional modulation mechanism that injects shadow-aware similarity intoMamba's input gate, allowing selective integration of relevant context alongtransition axes. To further ensure appearance fidelity, we introduce ColorShiftregularization, a contrastive learning objective driven by global colorstatistics. By synthesizing structured informative negatives, it guides themodel to suppress color contamination and achieve robust color restoration.Together, these components adapt sequence modeling to the structural integrityand chromatic consistency required for shadow removal. Extensive experiments onpublic benchmarks demonstrate that DeshadowMamba achieves state-of-the-artvisual quality and strong quantitative performance.</description>
      <author>example@mail.com (Zhaotong Yang, Yi Chen, Yanying Li, Shengfeng He, Yangyang Xu, Junyu Dong, Jian Yang, Yong Du)</author>
      <guid isPermaLink="false">2510.24260v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MATCH: Task-Driven Code Evaluation through Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.23169v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MATCH，一种新颖的无参考代码评估指标，用于解决AI生成代码与开发者意图匹配度评估的挑战。&lt;h4&gt;背景&lt;/h4&gt;AI代码生成越来越普遍，GitHub Copilot估计生成了GitHub上46%的代码。准确评估生成代码与开发者意图的匹配程度仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决无参考代码评估的空白问题，除了ICE-Score等少数替代方案外，引入MATCH作为一种新颖的无参考代码指标。&lt;h4&gt;方法&lt;/h4&gt;MATCH使用对比学习为代码和自然语言任务描述生成有意义的嵌入，实现反映生成代码执行任务程度的相似性评分。&lt;h4&gt;主要发现&lt;/h4&gt;MATCH在多种编程语言上，与功能正确性和人类偏好相比，比现有指标实现了更强的相关性。&lt;h4&gt;结论&lt;/h4&gt;MATCH是一种有效的无参考代码评估指标，能够更好地评估生成代码与开发者意图的匹配程度。&lt;h4&gt;翻译&lt;/h4&gt;基于AI的代码生成日益普及，GitHub Copilot估计生成了GitHub上46%的代码。准确评估生成代码与开发者意图的匹配程度仍然是一个重大挑战。传统评估方法，如单元测试，通常难以扩展且成本高昂。语法相似性指标（如BLEU、ROUGE）无法捕捉代码功能，而像CodeBERTScore这样的指标需要参考代码，但参考代码并不总是可用的。为了解决无参考代码评估的空白问题，除了ICE-Score等少数替代方案外，本文引入了MATCH，一种新颖的无参考代码指标。MATCH使用对比学习为代码和自然语言任务描述生成有意义的嵌入，实现反映生成代码执行任务程度的相似性评分。我们表明，MATCH在多种编程语言上，与功能正确性和人类偏好相比，比现有指标实现了更强的相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-based code generation is increasingly prevalent, with GitHub Copilotestimated to generate 46% of the code on GitHub. Accurately evaluating how wellgenerated code aligns with developer intent remains a critical challenge.Traditional evaluation methods, such as unit tests, are often unscalable andcostly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture codefunctionality, and metrics like CodeBERTScore require reference code, which isnot always available. To address the gap in reference-free evaluation, with fewalternatives such as ICE-Score, this paper introduces MATCH, a novelreference-free metric. MATCH uses Contrastive Learning to generate meaningfulembeddings for code and natural language task descriptions, enabling similarityscoring that reflects how well generated code implements the task. We show thatMATCH achieves stronger correlations with functional correctness and humanpreference than existing metrics across multiple programming languages.</description>
      <author>example@mail.com (Marah Ghoummaid, Vladimir Tchuiev, Ofek Glick, Michal Moshkovitz, Dotan Di Castro)</author>
      <guid isPermaLink="false">2510.23169v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>UniField: Joint Multi-Domain Training for Universal Surface Pressure Modeling</title>
      <link>http://arxiv.org/abs/2510.24106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为UniField的方法，通过整合多个子领域的空气动力学数据进行联合训练，解决了数据稀缺问题，实现了更好的流场表示。&lt;h4&gt;背景&lt;/h4&gt;物体表面压力场的空气动力学模拟对许多工程问题至关重要。深度神经网络已成为传统计算流体力学(CFD)模拟的高效替代方案，但数据稀缺限制了其应用。&lt;h4&gt;目的&lt;/h4&gt;解决空气动力学数据稀缺问题，通过整合多个子领域的数据进行联合训练，学习更通用的流场表示。&lt;h4&gt;方法&lt;/h4&gt;提出UniField方法，整合五个不同数据集（涵盖汽车、火车、飞机和一般形状）。该方法采用领域无关的Transformer模块提取通用点云特征，并定制领域特定的流动条件适配器来适应不同子领域的流动信息。&lt;h4&gt;主要发现&lt;/h4&gt;尽管不同子领域的空气动力学数据通常遵循不同方程，但联合训练的模型通常比单独训练的模型表现更好，表明这些数据相互补充，帮助模型学习更好的流场表示。&lt;h4&gt;结论&lt;/h4&gt;UniField作为通用流场表示模型具有潜力，为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;物体表面压力场的空气动力学模拟对许多工程问题至关重要。近年来，深度神经网络已成为传统计算成本高昂的CFD模拟的高效替代方案，用于建模表面压力场。然而，数据稀缺仍然是一个基本挑战，限制了神经网络的应用。为了解决这一限制，我们提出整合多个子领域的空气动力学数据进行联合训练，以学习更通用的流场表示。我们整合了涵盖不同领域的五个不同数据集，包括汽车、火车、飞机和一般形状。面对不同领域间的显著数据差异，我们提出了UniField，它采用领域无关的Transformer模块提取通用点云特征，并定制领域特定的流动条件适配器来适应不同子领域的流动信息。尽管不同子领域的空气动力学数据通常遵循不同的方程，但我们比较了在所有数据上联合训练的模型与在单个数据集上单独训练的模型，发现联合训练的模型通常表现出更好的性能。这表明这些数据相互补充，帮助模型学习更好的流场表示。这些结果突显了UniField作为通用流场表示模型的潜力，为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aerodynamic simulation of the surface pressure field around objects iscrucial for many engineering problems. In recent years, deep neural networkshave emerged as an efficient alternative to traditional, computationallyexpensive CFD simulations for modeling surface pressure fields. However, datascarcity remains a fundamental challenge, limiting the application of neuralnetworks. To address this limitation, we propose to integrate aerodynamic datafrom multiple subfields and conduct joint training to learn more general fieldrepresentations. We consolidate five different datasets covering variousfields, including automobiles, trains, aircraft, and general shapes. Facingsignificant data differences across different domains, we propose UniField,which employs a domain-agnostic Transformer module to extract general pointcloud features and customizes domain-specific flow-conditioned adapters toadapt to the flow information in different subfields. Despite the fact thataerodynamic data from different subfields are typically governed by differentequations, we compare models trained jointly on all data with those trainedseparately on individual datasets and find that the jointly-trained modelcommonly demonstrates better performance. This indicates that these datacomplement each other to help the model learn better flow fieldrepresentations. These results highlight the potential of UniField as auniversal flow field representation model and lay the foundation for broaderapplications of neural networks in aerodynamic analysis.</description>
      <author>example@mail.com (Junhong Zou, Zhenxu Sun, Yueqing Wang, Wei Qiu, Zhaoxiang Zhang, Zhen Lei, Xiangyu Zhu)</author>
      <guid isPermaLink="false">2510.24106v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised Domain Adaptation in 3D LiDAR Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2510.23525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for publication at the 2025 IEEE/RSJ  International Conference on Intelligent Robots and Systems (IROS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种动态伪标签过滤(DPLF)方案和先导引导的数据增强流水线(PG-DAP)，用于增强点云无监督域适应语义分割中真实数据的利用，提高合成到真实点云语义分割的性能。&lt;h4&gt;背景&lt;/h4&gt;为智能自主系统标注真实世界的LiDAR点云成本很高，现有基于自训练的无监督域适应方法在利用合成点云数据时未能有效利用未标记数据。&lt;h4&gt;目的&lt;/h4&gt;提高点云语义分割性能，更有效地利用未标记数据，克服现有方法依赖预定义或固定置信度阈值导致的性能限制。&lt;h4&gt;方法&lt;/h4&gt;提出动态伪标签过滤(DPLF)方案增强真实数据利用，设计先导引导的数据增强流水线(PG-DAP)减轻域偏移，并使用数据混合一致性损失推动模型学习上下文无关表示。&lt;h4&gt;主要发现&lt;/h4&gt;在两个具有挑战性的合成到真实点云语义分割任务上，该方法取得了优越的性能，消融研究证实了DPLF和PG-DAP模块的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效解决现有点云UDA语义分割中未标记数据利用不足的问题，显著提高了合成到真实点云语义分割的性能。&lt;h4&gt;翻译&lt;/h4&gt;为智能自主系统使用而标注真实世界的LiDAR点云成本很高。为克服这一限制，基于自训练的无监督域适应(UDA)已被广泛用于利用合成点云数据提高点云语义分割。然而，我们认为现有方法没有有效利用未标记数据，因为它们要么依赖于预定义或固定的置信度阈值，导致性能不佳。在本文中，我们提出了一种动态伪标签过滤(DPLF)方案，以增强点云UDA语义分割中真实数据的利用。此外，我们设计了一个简单高效的先导引导的数据增强流水线(PG-DAP)，以减轻合成和真实世界点云之间的域偏移。最后，我们使用数据混合一致性损失来推动模型学习上下文无关的表示。我们通过最先进方法的广泛比较实施并彻底评估了我们的方法。在两个具有挑战性的合成到真实点云语义分割任务上的实验表明，我们的方法取得了优越的性能。消融研究证实了DPLF和PG-DAP模块的有效性。我们在本文中发布了我们方法的代码。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D激光雷达点云语义分割中的无监督域适应问题，具体是如何有效利用带有自动标签的合成数据来改善对真实世界点云的语义分割，而无需对真实世界数据进行昂贵的标注。这个问题很重要，因为对真实世界的激光雷达点云进行密集标注成本高昂且耗时，而智能自主系统（如自动驾驶汽车）需要在动态真实环境中工作。虽然合成数据可以自动生成标签，但合成环境和真实世界之间存在域分布差异，导致在合成数据上训练的模型在真实数据上性能下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，特别是发现固定置信度阈值导致真实数据利用效率低下和类别不平衡问题。然后设计了解决方案：1) 动态伪标签过滤(DPLF)方案，自适应调整置信度阈值；2) 先验引导的数据增强管道(PG-DAP)，缓解域差异；3) 数据混合一致性损失，学习上下文无关表示。作者借鉴了Mean Teacher模型架构、LaserMix数据混合框架、局部和全局仿射变换以及指数移动平均(EMA)更新机制，但进行了创新性改进以解决特定问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过动态调整伪标签过滤策略和利用先验知识进行高效数据增强，解决合成到真实数据的域适应问题。整体流程：1) 使用Mean Teacher架构，预训练教师网络；2) 教师网络生成目标域伪标签；3) 通过DPLF进行伪标签过滤(距离加权、分层过滤、动态阈值更新)；4) 使用PG-DAP进行数据增强(DAS、DAJ、HAJ)；5) 应用LaserMix进行数据混合；6) 计算分割损失和数据混合一致性损失；7) 更新学生网络参数，使用EMA更新教师网络参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 动态伪标签过滤(DPLF)：自适应调整全局和类别特定置信度阈值，使用距离权重和EMA动态更新，解决固定阈值导致的类别不平衡；2) 先验引导数据增强(PG-DAP)：包含DAS(密度感知采样)、DAJ(距离感知抖动)和HAJ(高度感知抖动)，基于先验知识无需额外学习；3) 数据混合一致性损失：推动模型学习上下文无关表示。相比之前工作，不同之处在于：不使用固定置信度阈值，避免类别不平衡；不依赖计算资源昂贵的GAN进行域转换；结合了动态伪标签过滤和高效数据增强，通过一致性损失进一步改善特征表示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DPGLA通过动态伪标签过滤和先验引导数据增强，有效解决了3D激光雷达点云语义分割中合成到真实数据的无监督域适应问题，显著提升了模型在真实场景中的分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Annotating real-world LiDAR point clouds for use in intelligent autonomoussystems is costly. To overcome this limitation, self-training-basedUnsupervised Domain Adaptation (UDA) has been widely used to improve pointcloud semantic segmentation by leveraging synthetic point cloud data. However,we argue that existing methods do not effectively utilize unlabeled data, asthey either rely on predefined or fixed confidence thresholds, resulting insuboptimal performance. In this paper, we propose a Dynamic Pseudo-LabelFiltering (DPLF) scheme to enhance real data utilization in point cloud UDAsemantic segmentation. Additionally, we design a simple and efficientPrior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shiftbetween synthetic and real-world point clouds. Finally, we utilize data mixingconsistency loss to push the model to learn context-free representations. Weimplement and thoroughly evaluate our approach through extensive comparisonswith state-of-the-art methods. Experiments on two challenging synthetic-to-realpoint cloud semantic segmentation tasks demonstrate that our approach achievessuperior performance. Ablation studies confirm the effectiveness of the DPLFand PG-DAP modules. We release the code of our method in this paper.</description>
      <author>example@mail.com (Wanmeng Li, Simone Mosco, Daniel Fusaro, Alberto Pretto)</author>
      <guid isPermaLink="false">2510.23525v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Quality-controlled registration of urban MLS point clouds reducing drift effects by adaptive fragmentation</title>
      <link>http://arxiv.org/abs/2510.23416v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures. This manuscript is currently under review at the  International Journal of Applied Earth Observation and Geoinformation  (Elsevier). A preprint version will also be available on SSRN (Elsevier  Preprints) with a DOI once processed. This is the original preprint version  submitted for peer review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型工作流程，用于高效准确地在大规模城市街道场景中将移动激光扫描(MLS)点云配准到目标模型点云。&lt;h4&gt;背景&lt;/h4&gt;城市环境中的点云配准面临复杂挑战，包括点云密度差异、噪声特性和遮挡场景，这些在城市中心尤为常见。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够应对城市环境复杂性的工作流程，实现大规模MLS点云与目标模型点云的高效准确配准。&lt;h4&gt;方法&lt;/h4&gt;提出两种方法创新：1) 半球检查(SSC)预处理技术，通过识别相互正交的平面表面分割MLS轨迹数据，减少MLS漂移影响；2) 平面体素广义最近点迭代算法(PV-GICP)，在体素分区中选择性使用平面表面进行精细配准。&lt;h4&gt;主要发现&lt;/h4&gt;慕尼黑市中心真实数据集实验表明，该工作流程实现了平均亚0.01米的配准精度，同时比传统点对平面ICP方法减少50%以上的计算时间。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够推动自动化三维城市建模和更新，在城市规划、基础设施管理和动态城市监测中有直接应用。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种新型工作流程，旨在高效准确地在大规模移动激光扫描(MLS)点云与城市街道场景中的目标模型点云之间进行配准。该工作流程专门针对城市环境中固有的复杂性，巧妙地解决了点云密度、噪声特性和遮挡场景差异带来的挑战，这些挑战在城市中心中普遍存在。研究引入了两种方法创新。首先，提出的半球检查(SSC)预处理技术通过识别相互正交的平面表面，最优地分割MLS轨迹数据。这一步骤减少了MLS漂移对整个点云配准精度的影响，同时确保每个片段内有足够的几何特征以避免局部最小值。其次，我们提出了平面体素广义最近点迭代算法(PV-GICP)，一种在体素分区中选择性使用平面表面的精细配准方法。这种预处理策略不仅提高了配准精度，而且比传统的点对平面ICP方法减少了50%以上的计算时间。在慕尼黑市中心的真实数据集实验中，我们的工作流程实现了亚0.01米的平均配准精度，同时显著缩短了处理时间。研究结果强调了所提出方法在推进自动化三维城市建模和更新方面的潜力，可直接应用于城市规划、基础设施管理和动态城市监测。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决城市环境中大规模移动激光扫描(MLS)点云与目标模型点云的高效准确配准问题，特别是处理城市环境中的复杂性和挑战，包括不同密度、噪声特性和遮挡场景的点云集成，以及MLS长时间扫描过程中产生的漂移效应。这个问题在现实中非常重要，因为城市环境变化迅速，需要频繁更新3D城市模型用于城市规划、基础设施管理和城市发展监测等应用，而传统更新方法劳动密集且容易出错。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有方法的局限性，包括传统点云配准技术在处理大规模城市数据时效率低下，单个变换矩阵可能不足以实现精确对齐，以及固定分割方法在某些缺乏特征的片段中可能失败。在此基础上，作者借鉴了现有的点云分割技术(如等时间间隔分割)、特征匹配方法(如RANSAC)和ICP算法，但进行了创新改进，提出了自适应分割策略确保每个片段包含足够几何特征，以及专门针对城市环境的配准流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自适应分割将大型MLS点云分成包含足够几何特征的小片段减少漂移影响，只在稳定的平面区域进行精细配提高效率和准确性，并利用变换参数分析MLS漂移效应。整体流程包括：1)数据预处理(重采样、去噪、语义分类)；2)数据分割(初始分割后通过半球检查验证)；3)粗配准(特征检测、匹配和异常值去除)；4)精细配准(识别平面区域并执行GICP)；5)漂移分析(评估变换参数)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)半球检查(SSC)自适应分割技术，确保每个片段包含足够的相互正交平面表面；2)基于平面体素的广义ICP(PV-GICP)，选择性使用平面区域提高配准精度并减少50%以上的计算时间；3)漂移分析策略，通过分割过程识别和减少漂移误差。相比之前的工作，传统分割方法使用固定时间间隔可能导致某些片段缺乏特征，传统ICP方法在整个点云上运行计算量大且对非刚性区域敏感，而本文方法通过自适应分割和选择性平面配准解决了这些问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一种基于自适应分割和选择性平面配准的城市MLS点云高质量配准方法，显著提高了配准精度并减少了计算时间，同时有效量化了MLS漂移效应，为自动化3D城市模型更新提供了实用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents a novel workflow designed to efficiently and accuratelyregister large-scale mobile laser scanning (MLS) point clouds to a target modelpoint cloud in urban street scenarios. This workflow specifically targets thecomplexities inherent in urban environments and adeptly addresses thechallenges of integrating point clouds that vary in density, noisecharacteristics, and occlusion scenarios, which are common in bustling citycenters. Two methodological advancements are introduced. First, the proposedSemi-sphere Check (SSC) preprocessing technique optimally fragments MLStrajectory data by identifying mutually orthogonal planar surfaces. This stepreduces the impact of MLS drift on the accuracy of the entire point cloudregistration, while ensuring sufficient geometric features within each fragmentto avoid local minima. Second, we propose Planar Voxel-based GeneralizedIterative Closest Point (PV-GICP), a fine registration method that selectivelyutilizes planar surfaces within voxel partitions. This pre-process strategy notonly improves registration accuracy but also reduces computation time by morethan 50% compared to conventional point-to-plane ICP methods. Experiments onreal-world datasets from Munich's inner city demonstrate that our workflowachieves sub-0.01 m average registration accuracy while significantlyshortening processing times. The results underscore the potential of theproposed methods to advance automated 3D urban modeling and updating, withdirect applications in urban planning, infrastructure management, and dynamiccity monitoring.</description>
      <author>example@mail.com (Marco Antonio Ortiz Rincon, Yihui Yang, Christoph Holst)</author>
      <guid isPermaLink="false">2510.23416v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Symmetria: A Synthetic Dataset for Learning in Point Clouds</title>
      <link>http://arxiv.org/abs/2510.23414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  40 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Symmetria是一种公式驱动的点云数据集，通过利用对称性概念克服了点云学习中数据稀缺的问题，能够按任意规模生成，提供精确的地面真值，促进数据高效实验，并支持广泛的泛化和扩展。&lt;h4&gt;背景&lt;/h4&gt;与图像或文本领域受益于大量大型数据集不同，点云学习技术经常由于缺乏大规模数据集而遇到限制，这成为研究中的一个主要挑战。&lt;h4&gt;目的&lt;/h4&gt;克服点云数据集稀缺的限制，提出一种可按任意规模生成的公式驱动数据集，为点云学习提供充足且高质量的数据支持。&lt;h4&gt;方法&lt;/h4&gt;利用对称性概念创建具有已知结构和高度可变性的形状，确保精确地面真值的绝对可用性，设计数据集以促进数据高效的实验，实现跨不同几何设置的广泛泛化，并为新任务和模态提供易于扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;该数据集对点云自监督预训练非常有效，训练的模型在分类和分割等下游任务中表现出强大的性能，同时也显示出良好的少样本学习能力；该数据集还可用于真实世界物体的分类，展示了方法的实用价值；作者还引入了一个具有挑战性的对称检测任务，并为基线比较提供了基准。&lt;h4&gt;结论&lt;/h4&gt;Symmetria数据集和相关代码的公开可用性，以及能够生成非常大的数据集集合的能力，为点云学习领域的进一步研究和创新提供了重要基础。&lt;h4&gt;翻译&lt;/h4&gt;与图像或文本领域受益于大量大型数据集不同，点云学习技术经常由于缺乏大规模数据集而遇到限制。为了克服这一限制，我们提出了Symmetria，一种可按任意规模生成的公式驱动数据集。通过构造，它确保精确地面真值的绝对可用性，通过需要更少的样本促进数据高效的实验，实现跨不同几何设置的广泛泛化，并为新任务和模态提供易于扩展性。利用对称性的概念，我们创建了具有已知结构和高度可变性的形状，使神经网络能够有效地学习点云特征。我们的结果表明，该数据集对于点云自监督预训练非常有效，产生的模型在分类和分割等下游任务中表现出强大的性能，同时也显示出良好的少样本学习能力。此外，我们的数据集可以支持将模型微调以分类真实世界物体，突显了我们方法的实用性和应用价值。我们还引入了一个具有挑战性的对称检测任务，并为基线比较提供了基准。我们方法的一个显著优势是数据集、配套代码的公开可用性，以及生成非常大集合的能力，促进了点云学习的进一步研究和创新。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云学习领域中的数据稀缺问题。与图像或文本领域有大量大规模数据集不同，点云学习技术经常因缺乏大规模数据集而受到限制。这个问题很重要，因为点云是3D视觉的重要表示形式，广泛应用于机器人、自动驾驶等领域；缺乏大规模数据集限制了点云深度学习模型的发展；现有的3D数据集存在版权、隐私和标注成本等问题；真实世界的3D数据获取成本高、耗时长，难以满足机器学习所需的规模。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到图像领域有公式驱动的数据集表现出了有趣性能，甚至在某些任务上超过了ImageNet，这启发了他们在3D点云领域采用类似方法。他们借鉴了SHREC23数据集的工作，但进行了扩展和改进。作者基于对称性概念设计数据集，因为几乎现实世界中的每个物体都表现出某种对称性。他们设计数据集时遵循了几个原则：大规模探索、数据效率、可用的真实标签、隐私保护和通用可扩展性。从平面参数曲线开始生成3D形状，这些曲线具有已知的几何特性，然后通过挤压或旋转操作将它们转换为3D表面。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用对称性作为生成合成点云数据集的基础，通过参数化平面曲线生成具有已知对称结构的形状，然后通过几何变换转换为3D表面，并添加各种扰动增加数据集的多样性和挑战性。整体实现流程包括：1)从具有对称性的参数化平面曲线开始；2)通过挤压（大多数曲线）或旋转（贝塞尔曲线）将曲线转换为3D表面；3)确保3D形状保持原始平面曲线的对称性；4)应用各种变换增加数据集多样性；5)以一定概率应用随机平移和/或旋转；6)将生成的点云组织成不同复杂度的子数据集；7)为每个点云提供其对称性的真实标签信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于对称性的合成数据集；2)公式驱动的生成方法；3)精确的真实标签；4)数据效率；5)多样化的几何变换；6)可扩展性；7)多任务支持。相比之前的工作（如ShapeNet、ModelNet、SHREC23等）的不同之处：1)数据来源不同，Symmetria是完全合成的，避免了版权和隐私问题；2)生成方式不同，使用了更丰富的参数化平面曲线库；3)数据规模可按需生成任意规模；4)提供了更全面的真实标签注释；5)不仅验证了在传统任务上的有效性，还专门验证了对称性检测这一特定任务上的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了Symmetria，一个基于对称性的大规模合成点云数据集，通过程序化生成方法解决了3D点云学习中的数据稀缺问题，同时提供了精确的真实标签和多样化的几何变换，有效支持了自监督预训练和对称性检测等任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unlike image or text domains that benefit from an abundance of large-scaledatasets, point cloud learning techniques frequently encounter limitations dueto the scarcity of extensive datasets. To overcome this limitation, we presentSymmetria, a formula-driven dataset that can be generated at any arbitraryscale. By construction, it ensures the absolute availability of precise groundtruth, promotes data-efficient experimentation by requiring fewer samples,enables broad generalization across diverse geometric settings, and offers easyextensibility to new tasks and modalities. Using the concept of symmetry, wecreate shapes with known structure and high variability, enabling neuralnetworks to learn point cloud features effectively. Our results demonstratethat this dataset is highly effective for point cloud self-supervisedpre-training, yielding models with strong performance in downstream tasks suchas classification and segmentation, which also show good few-shot learningcapabilities. Additionally, our dataset can support fine-tuning models toclassify real-world objects, highlighting our approach's practical utility andapplication. We also introduce a challenging task for symmetry detection andprovide a benchmark for baseline comparisons. A significant advantage of ourapproach is the public availability of the dataset, the accompanying code, andthe ability to generate very large collections, promoting further research andinnovation in point cloud learning.</description>
      <author>example@mail.com (Ivan Sipiran, Gustavo Santelices, Lucas Oyarzún, Andrea Ranieri, Chiara Romanengo, Silvia Biasotti, Bianca Falcidieno)</author>
      <guid isPermaLink="false">2510.23414v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Workspace Registration and Collision Detection for Industrial Robotics Applications</title>
      <link>http://arxiv.org/abs/2510.23227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文主要研究机器人运动规划中的环境建模和碰撞检测方法&lt;h4&gt;背景&lt;/h4&gt;机器人运动规划依赖于对环境的精确知识，需要定义受限区域并考虑碰撞物体&lt;h4&gt;目的&lt;/h4&gt;比较不同传感器，说明从检测到完成碰撞环境的过程，以及检测机器人与环境的碰撞&lt;h4&gt;方法&lt;/h4&gt;使用各种传感器获取环境点云，通过区域增长分割和VCCS算法识别碰撞物体，并对点簇进行近似处理&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体发现&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及结论&lt;h4&gt;翻译&lt;/h4&gt;机器人运动规划依赖于对环境的精确知识，以便能够定义受限区域并考虑碰撞物体。为了捕获工作空间，使用各种传感器获取环境的点云。碰撞物体通过区域增长分割和VCCS算法进行识别。随后对点簇进行近似处理。本文的目的是比较不同的传感器，说明从检测到完成碰撞环境的过程，并检测机器人与该环境之间的碰撞。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决工业机器人在复杂生产环境中进行精确环境感知和碰撞检测的问题。这个问题在现实中非常重要，因为它关系到机器人能否安全高效地工作，避免与周围环境发生碰撞，同时最大化利用可用工作空间，确保生产过程的顺利进行和人员设备的安全。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先考虑了机器人操作环境需要精确建模的需求，然后设计了一套从环境感知到碰撞检测的完整流程。他们借鉴了多项现有工作：使用了Point Cloud Library (PCL)中的方法和算法；应用了区域增长分割算法来识别物体；采用了Voxel Cloud Connectivity Segmentation (VCCS)算法进行更精细的分割；并使用了分离轴定理和基于Minkowski差的碰撞检测方法。作者的主要贡献在于将这些技术整合成一个完整的工业机器人应用系统，并进行了系统性的比较和优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过传感器获取环境点云数据，经过处理和分割后将环境近似为简单的几何形状，然后应用碰撞检测算法确保机器人安全运行。整体流程包括：1)环境描述：将机器人操作环境表示为点云数据；2)检测和特征提取：使用3D传感器获取环境数据，进行预处理和去噪，通过分割算法识别和聚类物体；3)边界框生成：将聚类近似为立方体边界框，并与物体协方差矩阵主轴对齐以减少空间损失；4)碰撞检测：使用分离轴定理或基于Minkowski差的方法检测机器人与环境的碰撞。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)系统比较了不同传感器(TOF、主动立体视觉)在工业机器人环境感知中的性能差异；2)提出了一套从环境检测到完成碰撞环境的完整流程；3)研究了不同的物体近似方法及其对可用空间的影响；4)比较了不同碰撞检测方法在可用空间和约束数量方面的表现。相比之前的工作，这篇论文不仅关注单一算法，而是关注整个系统流程；不仅评估算法性能，还考虑了实际可用工作空间；通过物体对齐方法和基于Minkowski差的碰撞检测，显著减少了约束数量，提高了路径规划效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一套完整的工业机器人环境感知与碰撞检测方法，通过系统比较不同传感器和碰撞检测算法，优化了机器人工作空间利用率和路径规划效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion planning for robotic manipulators relies on precise knowledge of theenvironment in order to be able to define restricted areas and to takecollision objects into account. To capture the workspace, point clouds of theenvironment are acquired using various sensors. The collision objects areidentified by region growing segmentation and VCCS algorithm. Subsequently thepoint clusters are approximated. The aim of the present paper is to comparedifferent sensors, to illustrate the process from detection to the finishedcollision environment and to detect collisions between the robot and thisenvironment.</description>
      <author>example@mail.com (Klaus Zauner, Josef El Dib, Hubert Gattringer, Andreas Mueller)</author>
      <guid isPermaLink="false">2510.23227v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>UGAE: Unified Geometry and Attribute Enhancement for G-PCC Compressed Point Clouds</title>
      <link>http://arxiv.org/abs/2510.23009v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的几何和属性增强(UGAE)框架，通过三个核心组件(PoGE、PAE和PoAE)有效解决了点云有损压缩导致的几何结构和属性信息失真问题，在多个基准数据集上显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;点云的有损压缩可以减少存储和传输成本，但不可避免地导致几何结构和属性信息中的不可逆失真。&lt;h4&gt;目的&lt;/h4&gt;解决有损压缩导致的几何结构和属性信息失真问题，提高压缩后的点云质量。&lt;h4&gt;方法&lt;/h4&gt;提出统一的几何和属性增强(UGAE)框架，包含三个核心组件：1)后几何增强(PoGE)使用基于Transformer的稀疏卷积U-Net重建几何结构；2)预属性增强(PAE)引入增强几何引导的重新着色策略，使用DA-KNN方法保留高频细节；3)后属性增强(PoAE)使用带W-MSE损失的属性残差预测网络增强高频区域质量。&lt;h4&gt;主要发现&lt;/h4&gt;UGAE在8iVFB、Owlii和MVUB三个基准数据集上显著优于现有方法；与G-PCC测试模型相比，几何部分平均BD-PSNR增益9.98 dB，BD-比特率节省90.98%；属性部分BD-PSNR提高3.67 dB，BD-比特率节省56.88%；显著改善了感知质量。&lt;h4&gt;结论&lt;/h4&gt;UGAE框架能有效解决点云有损压缩导致的失真问题，在多个指标上表现优异，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;有损压缩点云减少了存储和传输成本；然而，它不可避免地导致几何结构和属性信息中的不可逆失真。为解决这些问题，我们提出了统一的几何和属性增强(UGAE)框架，包含三个核心组件：后几何增强(PoGE)、预属性增强(PAE)和后属性增强(PoAE)。在PoGE中，使用基于Transformer的稀疏卷积U-Net通过预测体素占用概率高精度重建几何结构。基于改进的几何结构，PAE引入创新的增强几何引导重新着色策略，使用细节感知的K-近邻(DA-KNN)方法实现精确重新着色，并在属性压缩前有效保留高频细节。最后，在解码器端，PoAE使用带加权均方误差(W-MSE)损失的属性残差预测网络，增强高频区域质量，同时保持低频区域的保真度。UGAE在三个基准数据集上显著优于现有方法：8iVFB、Owlii和MVUB。与最新的G-PCC测试模型(TMC13v29)相比，UGAE在D1指标下几何部分平均BD-PSNR增益9.98 dB，BD-比特率节省90.98%，属性部分在Y分量上BD-PSNR提高3.67 dB，BD-比特率节省56.88%。此外，它显著改善了感知质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云压缩过程中几何结构和属性信息不可避免产生的不可逆失真问题。这个问题在现实中非常重要，因为点云数据广泛应用于自动驾驶、文化遗产保护和虚拟现实等领域，高精度点云数据量大，存储和传输成本高，而现有压缩方法在减少数据量的同时会引入失真，影响3D应用的效率和用户体验。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云压缩方法的局限性，发现它们主要分别优化几何或属性，忽略了两者间的耦合关系。现有联合增强方法仅在解码器端进行增强，无法充分利用几何增强对属性压缩的好处。作者借鉴了点云上采样方法（如PU-Net、PUFA-GAN）、稀疏卷积方法（如PU-Dense、GRNet）以及网络架构（Transformer和U-Net）和损失函数（BCE和MSE），设计了包含PoGE、PAE和PoAE三个核心组件的UGAE框架，在整个压缩过程中协同优化几何和属性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是联合优化几何和属性，通过编码器-解码器协同增强来保留高频细节，分阶段解决压缩失真问题。整体流程是：编码器端，PoGE接收有损几何并生成增强几何结构；PAE使用增强几何和原始属性通过DA-KNN重新着色生成中间属性。传输有损几何比特流和重新着色后的属性比特流。解码器端，PoGE重建相同的增强几何；PoAE使用W-MSE损失函数专注于重建属性残差，特别是在高频区域。最终输出联合增强的点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一的几何和属性增强框架(UGAE)；2) 后几何增强(PoGE)结合Transformer和U-Net架构，使用密集连接并解决GPU随机性问题；3) 前属性增强(PAE)引入DA-KNN算法保留高频细节；4) 后属性增强(PoAE)使用W-MSE损失函数专注于高频区域。相比之前工作，UGAE同时处理几何和属性失真，考虑两者耦合关系，在编码器和解码器端都进行增强，而G-PCC++等现有方法仅在解码器端增强，且基于有损几何进行重新着色。UGAE在三个基准数据集上性能显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了UGAE框架，通过在编码器和解码器端协同优化几何和属性增强，显著提高了点云压缩质量，特别是在保留高频细节方面表现优异。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lossy compression of point clouds reduces storage and transmission costs;however, it inevitably leads to irreversible distortion in geometry structureand attribute information. To address these issues, we propose a unifiedgeometry and attribute enhancement (UGAE) framework, which consists of threecore components: post-geometry enhancement (PoGE), pre-attribute enhancement(PAE), and post-attribute enhancement (PoAE). In PoGE, a Transformer-basedsparse convolutional U-Net is used to reconstruct the geometry structure withhigh precision by predicting voxel occupancy probabilities. Building on therefined geometry structure, PAE introduces an innovative enhancedgeometry-guided recoloring strategy, which uses a detail-aware K-NearestNeighbors (DA-KNN) method to achieve accurate recoloring and effectivelypreserve high-frequency details before attribute compression. Finally, at thedecoder side, PoAE uses an attribute residual prediction network with aweighted mean squared error (W-MSE) loss to enhance the quality ofhigh-frequency regions while maintaining the fidelity of low-frequency regions.UGAE significantly outperformed existing methods on three benchmark datasets:8iVFB, Owlii, and MVUB. Compared to the latest G-PCC test model (TMC13v29),UGAE achieved an average BD-PSNR gain of 9.98 dB and 90.98% BD-bitrate savingsfor geometry under the D1 metric, as well as a 3.67 dB BD-PSNR improvement with56.88% BD-bitrate savings for attributes on the Y component. Additionally, itimproved perceptual quality significantly.</description>
      <author>example@mail.com (Pan Zhao, Hui Yuan, Chongzhen Tian, Tian Guo, Raouf Hamzaoui, Zhigeng Pan)</author>
      <guid isPermaLink="false">2510.23009v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method</title>
      <link>http://arxiv.org/abs/2510.22973v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的占据中心驾驶场景生成方法，通过创建大规模语义占据数据集Nuplan-Occ和开发统一框架，解决了占据中心方法对标注数据依赖的问题，实现了高质量语义占据、多视图视频和LiDAR点云的联合生成。&lt;h4&gt;背景&lt;/h4&gt;场景生成是自动驾驶的关键领域，占据中心方法最近取得了最先进的结果，但这些方法严重依赖于标注的占据数据，而这类数据仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;克服占据中心方法对标注数据的依赖限制，创建大规模语义占据数据集，并开发统一框架实现多模态场景生成。&lt;h4&gt;方法&lt;/h4&gt;创建Nuplan-Occ数据集，开发统一框架联合生成语义占据、多视图视频和LiDAR点云，采用时空解耦架构支持4D动态占据的扩展和预测，提出基于高斯飞溅的稀疏点图渲染策略和传感器感知嵌入策略。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在生成保真度和可扩展性方面优于现有方法，在下游任务中验证了其实用价值。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的统一占据中心驾驶场景生成方法在自动驾驶场景生成方面表现出色，有助于感知和规划评估等下游应用。&lt;h4&gt;翻译&lt;/h4&gt;场景生成是自动驾驶的关键领域，使包括感知和规划评估在内的下游应用成为可能。占据中心方法最近通过提供跨帧和模态的一致条件取得了最先进的结果；然而，它们的性能严重依赖于标注的占据数据，而这种数据仍然稀缺。为了克服这一限制，我们整理了Nuplan-Occ，这是迄今为止最大的语义占据数据集，由广泛使用的Nuplan基准构建而成。其规模和多样性不仅促进了大规模生成建模，也促进了自动驾驶的下游应用。基于此数据集，我们开发了一个统一框架，联合合成高质量语义占据、多视图视频和LiDAR点云。我们的方法采用时空解耦架构，支持4D动态占据的高保真空间扩展和时间预测。为了弥合模态差距，我们进一步提出了两种新颖技术：一种基于高斯飞溅的稀疏点图渲染策略，增强多视图视频生成；一种传感器感知嵌入策略，明确建模LiDAR传感器特性，以实现真实的多LiDAR模拟。大量实验表明，与现有方法相比，我们的方法在生成保真度和可扩展性方面取得了优越的性能，并在下游任务中验证了其实际价值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶场景生成中的规模限制问题。现有占用中心方法虽先进，但受限于标注数据稀缺，无法实现大规模训练；同时，多模态生成（语义占用、视频、LiDAR）存在模态差距，导致生成质量受限。这一问题对自动驾驶领域至关重要，因为高质量场景生成能支持感知和规划评估，降低开发成本，提高系统鲁棒性和安全性，同时支持算法在多样化环境中训练，提升泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法受限于数据稀缺性，无法实现大规模训练；然后意识到需要构建更大规模数据集；观察到多模态生成存在模态差距，需要新技术弥合；提出时空解耦架构分解4D占用生成；为解决视频生成中的传感器校准问题，引入高斯飞溅稀疏点图渲染；为实现真实LiDAR模拟，提出传感器感知嵌入策略。该方法借鉴了UniScene的前期工作，利用了扩散模型、3D高斯表示、CogVideoX的3D因果VAE和体积渲染技术等现有成果。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是以语义占用为中心构建统一框架，通过时空解耦架构将4D动态占用生成分解为空间扩展和时间预测，利用大规模数据集支持训练，并通过专门技术弥合模态差距。整体流程：1)构建Nuplan-Occ数据集，使用前景-背景分离聚合策略；2)4D占用生成，使用VAE和DiT编码解码，时空解耦处理；3)视频生成，将占用转为3D高斯基元渲染成稀疏点图，用视频扩散Transformer生成；4)LiDAR生成，使用传感器感知嵌入和稀疏UNet，应用射线平滑正则化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)可扩展的统一4D动态场景生成框架，联合扩展模型架构和数据；2)4D占用的时空解耦建模，分离空间扩展和时间预测；3)多传感器真实性的模态桥接策略，包括稀疏点图渲染和传感器感知嵌入；4)构建Nuplan-Occ最大语义占用数据集。相比之前工作，本文数据规模更大（比Nuscenes-Occupancy大19倍），采用时空解耦架构而非混合处理，使用分层生成策略和稀疏渲染技术，在多个任务上取得了最先进性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了UniScenev2，一个基于大规模Nuplan-Occ数据集的统一占用中心框架，通过时空解耦架构和模态桥接技术，实现了高质量语义占用、多视角视频和LiDAR点云的联合生成，显著提升了自动驾驶场景生成的规模和质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driving scene generation is a critical domain for autonomous driving,enabling downstream applications, including perception and planning evaluation.Occupancy-centric methods have recently achieved state-of-the-art results byoffering consistent conditioning across frames and modalities; however, theirperformance heavily depends on annotated occupancy data, which still remainsscarce. To overcome this limitation, we curate Nuplan-Occ, the largest semanticoccupancy dataset to date, constructed from the widely used Nuplan benchmark.Its scale and diversity facilitate not only large-scale generative modeling butalso autonomous driving downstream applications. Based on this dataset, wedevelop a unified framework that jointly synthesizes high-quality semanticoccupancy, multi-view videos, and LiDAR point clouds. Our approach incorporatesa spatio-temporal disentangled architecture to support high-fidelity spatialexpansion and temporal forecasting of 4D dynamic occupancy. To bridge modalgaps, we further propose two novel techniques: a Gaussian splatting-basedsparse point map rendering strategy that enhances multi-view video generation,and a sensor-aware embedding strategy that explicitly models LiDAR sensorproperties for realistic multi-LiDAR simulation. Extensive experimentsdemonstrate that our method achieves superior generation fidelity andscalability compared to existing approaches, and validates its practical valuein downstream tasks. Repo:https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2</description>
      <author>example@mail.com (Bohan Li, Xin Jin, Hu Zhu, Hongsi Liu, Ruikai Li, Jiazhe Guo, Kaiwen Cai, Chao Ma, Yueming Jin, Hao Zhao, Xiaokang Yang, Wenjun Zeng)</author>
      <guid isPermaLink="false">2510.22973v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments</title>
      <link>http://arxiv.org/abs/2510.22754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the IEEE/RSJ International Conference on Intelligent  Robots and Systems (IROS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了TWC-SLAM，一种多智能体协作SLAM框架，通过整合文本语义和WiFi信号特征来增强位置识别和回环检测，以提高在具有重复结构的相似室内环境中的协作SLAM性能。&lt;h4&gt;背景&lt;/h4&gt;多智能体协作SLAM在具有重复结构的相似室内环境中（如走廊和房间）常面临挑战。当使用基于点云的技术时，这些挑战会导致共享位置识别出现显著不准确。&lt;h4&gt;目的&lt;/h4&gt;减轻多智能体协作SLAM在相似室内环境中的挑战，提高共享位置识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;TWC-SLAM框架包括基于FAST-LIO2的单智能体前端里程计模块、利用文本语义和WiFi特征的位置识别和回环检测模块以及全局映射模块。智能体配备了能够捕获文本信息和检测WiFi信号的传感器。通过关联这些数据源，TWC-SLAM建立共同位置，促进不同智能体地图之间的点云对齐，并采用回环检测和优化模块实现全局优化和一致性映射。&lt;h4&gt;主要发现&lt;/h4&gt;使用具有相似走廊、房间和文本标志的室内数据集评估的结果表明，TWC-SLAM显著提高了在具有重复建筑特征的复杂环境中协作SLAM系统的性能。&lt;h4&gt;结论&lt;/h4&gt;整合文本语义和WiFi信号特征可以有效提高多智能体协作SLAM在具有重复结构的相似室内环境中的性能，特别是在位置识别和回环检测方面。&lt;h4&gt;翻译&lt;/h4&gt;多智能体协作SLAM常在具有重复结构的相似室内环境（如走廊和房间）中遇到挑战。当采用基于点云的技术时，这些挑战可能导致共享位置识别出现显著不准确。为缓解这些问题，我们引入了TWC-SLAM，一种多智能体协作SLAM框架，它整合文本语义和WiFi信号特征以增强位置识别和回环检测。TWC-SLAM包括基于FAST-LIO2的单智能体前端里程计模块、利用文本语义和WiFi特征的位置识别和回环检测模块以及全局映射模块。智能体配备了能够捕获文本信息和检测WiFi信号的传感器。通过关联这些数据源，TWC-SLAM建立共同位置，促进不同智能体地图之间的点云对齐。此外，系统采用回环检测和优化模块来实现全局优化和一致性映射。我们使用具有相似走廊、房间和文本标志的室内数据集评估了我们的方法。结果表明，TWC-SLAM显著提高了在具有重复建筑特征的复杂环境中协作SLAM系统的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多智能体协同SLAM在具有重复结构的相似室内环境（如走廊和相似房间）中面临的位置识别不准确问题。这个问题在现实中很重要，因为许多室内环境（如办公楼、医院、学校）都有相似结构，传统基于点云的技术容易在这些环境中产生错误匹配，导致地图不一致和定位错误，影响多智能体系统在检查、救援、物流等领域的实际应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法在相似环境中的局限性，然后借鉴了现有工作如FAST-LIO2作为前端里程计模块，参考了TextSLAM等文本识别方法和SpotFi等WiFi定位技术。作者设计了多模态融合思路，结合文本语义（提供明确标识但可能重复）和WiFi特征（提供环境特定信号但区分度有限）两种互补信息，设计了四个主要组件：多智能体前端里程计、文本语义匹配、WiFi特征匹配和全局映射模块，通过算法1实现多模态位置识别，确保位置识别的准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过整合文本语义和WiFi特征这两种互补的模态信息，解决相似室内环境中多智能体协同SLAM的位置识别挑战。整体流程包括：1)多智能体使用FAST-LIO2计算里程和生成点云地图；2)通过OCR提取文本语义，使用Levenshtein距离计算文本相似度进行匹配；3)收集WiFi数据，计算MAC地址相似度和RSS值相似度进行验证；4)基于匹配结果识别相同位置，使用点云匹配方法计算坐标变换，执行回环检测和全局优化，生成一致的全局地图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首创性地整合文本语义与WiFi特征进行多智能体协同SLAM；2)提出新颖的回环检测和位置识别方法，协同利用两种模态信息；3)构建专门的多智能体数据集。相比之前工作，TWC-SLAM比传统点云方法精度高88%，比纯文本方法高82%，比纯WiFi方法高92%。它解决了单一模态方法在相似环境中的局限性，通过多模态融合提高了位置识别的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TWC-SLAM通过创新性地整合文本语义与WiFi特征，显著提高了多智能体协同SLAM系统在具有重复结构的相似室内环境中的定位精度和地图一致性，解决了传统方法在复杂环境中易出现错误匹配的关键挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent cooperative SLAM often encounters challenges in similar indoorenvironments characterized by repetitive structures, such as corridors androoms. These challenges can lead to significant inaccuracies in shared locationidentification when employing point cloud-based techniques. To mitigate theseissues, we introduce TWC-SLAM, a multi-agent cooperative SLAM framework thatintegrates text semantics and WiFi signal features to enhance locationidentification and loop closure detection. TWC-SLAM comprises a single-agentfront-end odometry module based on FAST-LIO2, a location identification andloop closure detection module that leverages text semantics and WiFi features,and a global mapping module. The agents are equipped with sensors capable ofcapturing textual information and detecting WiFi signals. By correlating thesedata sources, TWC-SLAM establishes a common location, facilitating point cloudalignment across different agents' maps. Furthermore, the system employs loopclosure detection and optimization modules to achieve global optimization andcohesive mapping. We evaluated our approach using an indoor dataset featuringsimilar corridors, rooms, and text signs. The results demonstrate that TWC-SLAMsignificantly improves the performance of cooperative SLAM systems in complexenvironments with repetitive architectural features.</description>
      <author>example@mail.com (Chunyu Li, Shoubin Chen, Dong Li, Weixing Xue, Qingquan Li)</author>
      <guid isPermaLink="false">2510.22754v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Estimating Continuum Robot Shape under External Loading using Spatiotemporal Neural Networks</title>
      <link>http://arxiv.org/abs/2510.22339v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2025 IEEE/RSJ International Conference on Intelligent Robots and  Systems (IROS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于学习的方法，用于准确估计受外部负载的柔性连续体机器人的3D形状。该方法通过时空神经网络架构融合多模态输入，生成点云表示机器人变形配置，并通过拟合贝塞尔曲线实现连续3D形状重建。实验验证显示该方法具有高精度，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;柔性连续体机器人在外部负载下的3D形状估计是一个挑战性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于学习的方法来准确估计柔性连续体机器人在外部负载下的3D形状。&lt;h4&gt;方法&lt;/h4&gt;提出时空神经网络架构，融合多模态输入（当前和历史肌腱位移数据以及RGB图像），生成点云表示机器人变形配置。网络集成了循环神经模块进行时间特征提取，编码模块进行空间特征提取，以及多模态融合模块结合视觉数据的空间特征和历史执行器输入的时间依赖性。通过将贝塞尔曲线拟合到预测点云上实现连续3D形状重建。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证显示该方法具有高精度，无负载时平均形状估计误差为0.08毫米，负载时为0.22毫米，优于最先进的TDCRs形状传感方法。&lt;h4&gt;结论&lt;/h4&gt;基于深度学习的时空数据融合在负载条件下能有效实现精确的形状估计。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于学习的方法，用于准确估计受外部负载的柔性连续体机器人的3D形状。提出的方法引入了一种时空神经网络架构，融合多模态输入，包括当前和历史肌腱位移数据以及RGB图像，生成代表机器人变形配置的点云。网络集成了循环神经模块进行时间特征提取，编码模块进行空间特征提取，以及多模态融合模块来结合从视觉数据中提取的空间特征和来自历史执行器输入的时间依赖性。通过将贝塞尔曲线拟合到预测的点云上实现连续3D形状重建。实验验证表明，我们的方法实现了高精度，无负载时平均形状估计误差为0.08毫米，负载时为0.22毫米，优于TDCRs形状传感的最先进方法。结果证明了基于深度学习的时空数据融合在负载条件下精确形状估计的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何准确估计在外部负载作用下的柔性连续机器人的3D形状问题。这个问题很重要，因为连续机器人在医疗手术、工业检测等领域有广泛应用，而外部负载会改变它们的变形行为，使得准确预测形状变得复杂。精确的形状估计对机器人控制至关重要，能帮助它们动态调整配置以优化任务执行和环境交互。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括传感器方法的精度限制和易损坏问题，模型方法难以处理材料非线性和未知外部负载，以及数据驱动方法在负载下鲁棒性有限。他们设计了一种时空神经网络架构，融合多模态输入数据。该方法借鉴了U-Net架构用于视觉特征提取，LSTM网络捕捉时间依赖性，以及空间注意力机制进行特征融合。同时创新性地将这些技术组合，形成了一个能够处理外部负载条件下的形状估计系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用时空神经网络融合视觉数据（RGB图像）和肌腱位移数据，建立这些多模态输入与连续机器人3D形状之间的非线性映射关系。整体流程包括：1)输入当前和历史肌腱位移数据及RGB图像；2)通过空间特征提取器从图像中提取空间特征；3)利用时间特征提取器处理肌腱位移序列；4)通过注意力机制融合时空特征；5)预测代表机器人形状的3D点云；6)使用贝塞尔曲线拟合点云生成连续3D形状。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多模态数据融合，同时利用视觉和肌腱位移数据；2)时空神经网络架构，结合卷积和循环神经网络；3)在外部负载下实现高精度形状估计；4)端到端学习框架。相比之前工作，该方法结合了视觉和本体感觉数据，提高了精度和鲁棒性；使用更先进的神经网络架构；在各种负载条件下表现出更好的一致性；无需显式机械建模，能处理材料非线性和未知外部负载。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于时空神经网络的创新方法，通过融合视觉和肌腱位移数据，实现了在外部负载条件下高精度估计连续机器人3D形状的目标，相比现有方法在精度和鲁棒性上均有显著提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a learning-based approach for accurately estimating the3D shape of flexible continuum robots subjected to external loads. The proposedmethod introduces a spatiotemporal neural network architecture that fusesmulti-modal inputs, including current and historical tendon displacement dataand RGB images, to generate point clouds representing the robot's deformedconfiguration. The network integrates a recurrent neural module for temporalfeature extraction, an encoding module for spatial feature extraction, and amulti-modal fusion module to combine spatial features extracted from visualdata with temporal dependencies from historical actuator inputs. Continuous 3Dshape reconstruction is achieved by fitting B\'ezier curves to the predictedpoint clouds. Experimental validation demonstrates that our approach achieveshigh precision, with mean shape estimation errors of 0.08 mm (unloaded) and0.22 mm (loaded), outperforming state-of-the-art methods in shape sensing forTDCRs. The results validate the efficacy of deep learning-based spatiotemporaldata fusion for precise shape estimation under loading conditions.</description>
      <author>example@mail.com (Enyi Wang, Zhen Deng, Chuanchuan Pan, Bingwei He, Jianwei Zhang)</author>
      <guid isPermaLink="false">2510.22339v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Breaking the Static Assumption: A Dynamic-Aware LIO Framework Via Spatio-Temporal Normal Analysis</title>
      <link>http://arxiv.org/abs/2510.22313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, Accepted to IEEE Robotics and Automation Letters  (RA-L)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种解决动态环境中激光雷达-惯性里程计(LIO)挑战的新方法，通过将动态感知直接集成到点云配准过程中，打破了静态特征识别和姿态估计之间的循环依赖关系。&lt;h4&gt;背景&lt;/h4&gt;传统LIO算法基于静态世界假设，在动态环境中表现不佳，特别是在动态物体主导场景和几何稀疏环境中。当前动态LIO方法面临根本性挑战：准确的定位需要可靠识别静态特征，而区分动态物体又需要精确的姿态估计。&lt;h4&gt;目的&lt;/h4&gt;解决动态环境中的LIO挑战，打破静态特征识别和姿态估计之间的循环依赖关系。&lt;h4&gt;方法&lt;/h4&gt;引入了一种新颖的动态感知迭代最近点算法，利用时空法线分析，并配以高效的空间一致性验证方法来增强静态地图构建。&lt;h4&gt;主要发现&lt;/h4&gt;在具有有限几何结构的挑战性动态环境中，与最先进的LIO系统相比，性能有显著提升。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效解决了动态环境中的LIO问题，代码和数据集已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了动态环境中激光雷达-惯性里程计(LIO)的挑战，传统方法由于其静态世界假设经常失败。当动态物体主导场景，特别是在几何稀疏环境中时，传统LIO算法表现不佳。当前动态LIO方法面临一个基本挑战：准确的定位需要可靠识别静态特征，而区分动态物体又需要精确的姿态估计。我们的解决方案通过将动态感知直接集成到点云配准过程中，打破了这种循环依赖。我们引入了一种新颖的动态感知迭代最近点算法，利用时空法线分析，并辅以高效的空间一致性验证方法来增强静态地图构建。实验评估表明，在具有有限几何结构的挑战性动态环境中，与最先进的LIO系统相比，性能有显著提升。代码和数据集可在https://github.com/thisparticle/btsa获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决激光雷达-惯性里程计（LIO）在动态环境中的定位和地图构建问题。传统LIO系统假设环境是静态的，当场景中存在大量移动物体（如行人、车辆）时会导致严重的定位误差。这个问题在现实中非常重要，因为真实世界环境通常是动态的，特别是在几何特征稀疏的环境中，动态物体可能主导场景，使传统系统完全失效。此外，现有方法存在循环依赖问题：准确的定位需要可靠的静态特征识别，而有效的动态物体检测又需要精确的位姿估计。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统方法将动态物体检测作为预处理步骤而非解决配准算法的根本问题；学习技术只能检测预定义物体类别且需要大量训练数据；几何方法依赖于特定假设且在复杂场景中表现不佳。作者借鉴了时空法线分析的概念，但创新性地将其直接集成到点云配准过程中，而不是作为后处理。同时，作者采用了双地图架构（时间滑动窗口地图用于时空法线计算，长期体素地图提供全局一致性），并扩展了点对点ICP算法的异常值剔除步骤。通过这种方式，作者打破了状态估计和动态物体检测之间的循环依赖。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过时空法线分析直接将动态感知集成到点云配准过程中，同时解决状态估计和动态点分类问题，打破两者之间的循环依赖。整体流程包括：1）输入数据预处理（IMU预积分和点云畸变校正）；2）点云下采样；3）动态感知点云配准（计算时空法线、分类稳定/不稳定点、迭代优化位姿）；4）静态地图构建（不稳定点上采样、DBSCAN聚类、空间一致性检查）。这一过程在每个迭代中同时评估点的动态性和优化位姿估计，而不是分步处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）动态感知ICP算法，通过时空法线分析直接集成动态感知到配准过程；2）双地图架构平衡时空计算需求；3）高效的空间一致性验证方法改进静态地图构建；4）在真正具有挑战性的动态环境中进行验证。相比之前工作的不同：传统方法将动态检测作为预处理步骤，而本文直接集成到配准中；现有方法要么依赖学习技术（需大量训练数据且泛化有限），要么依赖几何假设（在复杂场景中表现不佳）；大多数方法假设已知精确位姿，而本文同时优化位姿和动态点分类；评估不仅限于几何丰富的城市环境，还包括几何退化和动态物体主导的环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通过时空法线分析直接集成动态感知到激光雷达-惯性里程计配准过程中的新框架，打破了状态估计和动态物体检测之间的循环依赖，在具有挑战性的动态环境中实现了更准确和鲁棒的定位与地图构建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenge of Lidar-Inertial Odometry (LIO) indynamic environments, where conventional methods often fail due to theirstatic-world assumptions. Traditional LIO algorithms perform poorly whendynamic objects dominate the scenes, particularly in geometrically sparseenvironments. Current approaches to dynamic LIO face a fundamental challenge:accurate localization requires a reliable identification of static features,yet distinguishing dynamic objects necessitates precise pose estimation. Oursolution breaks this circular dependency by integrating dynamic awarenessdirectly into the point cloud registration process. We introduce a noveldynamic-aware iterative closest point algorithm that leverages spatio-temporalnormal analysis, complemented by an efficient spatial consistency verificationmethod to enhance static map construction. Experimental evaluations demonstratesignificant performance improvements over state-of-the-art LIO systems inchallenging dynamic environments with limited geometric structure. The code anddataset are available at https://github.com/thisparticle/btsa.</description>
      <author>example@mail.com (Chen Zhiqiang, Le Gentil Cedric, Lin Fuling, Lu Minghao, Qiao Qiyuan, Xu Bowen, Qi Yuhua, Lu Peng)</author>
      <guid isPermaLink="false">2510.22313v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Linearized Optimal Transport for Analysis of High-Dimensional Point-Cloud and Single-Cell Data</title>
      <link>http://arxiv.org/abs/2510.22033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于线性最优传输(LOT)的框架，用于处理单细胞技术生成的高维点云数据，解决了不规则点云难以直接量化和比较的问题，同时兼顾了预测准确性和生物学可解释性。&lt;h4&gt;背景&lt;/h4&gt;单细胞技术生成高维点云数据，能够详细表征复杂的患者状态和治疗反应，但每个患者由不规则点云表示，难以直接量化和比较个体间的生物学差异。非线性方法虽能达到预测准确性，但作为黑盒模型，生物学可解释性差。&lt;h4&gt;目的&lt;/h4&gt;将不规则点云嵌入到固定维度的欧几里得空间中，同时保留分布结构，提供一种有原则性的线性表示，保留最优传输几何结构，同时支持下游分析。&lt;h4&gt;方法&lt;/h4&gt;适应线性最优传输(LOT)框架到单细胞数据分析场景，将不规则点云嵌入到固定维度的欧几里得空间中，保留分布结构，形成任意两个患者之间的配准，使其能够直接比较细胞分布。&lt;h4&gt;主要发现&lt;/h4&gt;LOT实现了COVID-19患者状态的准确且可解释的分类，分类器权重映射回驱动预测的特定标记物和空间区域；同时实现了患者来源类器官的合成数据生成，利用LOT嵌入的线性特性；LOT重心产生表示组合条件或样本的平均细胞谱，支持药物相互作用测试。&lt;h4&gt;结论&lt;/h4&gt;LOT作为一个统一框架，连接了预测性能、可解释性和生成建模，通过将异质点云转换为可直接追踪到原始数据的结构化嵌入，为理解高维生物系统中的免疫变异和治疗效应开辟了新机会。&lt;h4&gt;翻译&lt;/h4&gt;单细胞技术生成细胞的高维点云，能够详细表征复杂的患者状态和治疗反应。然而每个患者由不规则点云而非简单向量表示，使得难以直接量化和比较个体间的生物学差异。非线性方法如核方法和神经网络能实现预测准确性，但作为黑箱模型，提供的生物学可解释性有限。为解决这些限制，我们将线性最优传输(LOT)框架适应到这一场景，将不规则点云嵌入到固定维度的欧几里得空间中，同时保留分布结构。这种嵌入提供了有原则性的线性表示，保留最优传输几何结构，同时支持下游分析。它还形成了任意两个患者之间的配准，使其能够直接比较细胞分布。在此空间中，LOT实现了：(i) COVID-19患者状态的准确且可解释的分类，其中分类器权重映射回驱动预测的特定标记物和空间区域；(ii) 患者来源类器官的合成数据生成，利用LOT嵌入的线性特性。LOT重心产生表示组合条件或样本的平均细胞谱，支持药物相互作用测试。这些结果共同确立了LOT作为连接预测性能、可解释性和生成建模的统一框架。通过将异质点云转换为可直接追踪到原始数据的结构化嵌入，LOT为理解高维生物系统中的免疫变异和治疗效应开辟了新机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell technologies generate high-dimensional point clouds of cells,enabling detailed characterization of complex patient states and treatmentresponses. Yet each patient is represented by an irregular point cloud ratherthan a simple vector, making it difficult to directly quantify and comparebiological differences between individuals. Nonlinear methods such as kernelsand neural networks achieve predictive accuracy but act as black boxes,offering little biological interpretability.  To address these limitations, we adapt the Linear Optimal Transport (LOT)framework to this setting, embedding irregular point clouds into afixed-dimensional Euclidean space while preserving distributional structure.This embedding provides a principled linear representation that preservesoptimal transport geometry while enabling downstream analysis. It also forms aregistration between any two patients, enabling direct comparison of theircellular distributions. Within this space, LOT enables: (i) \textbf{accurateand interpretable classification} of COVID-19 patient states, where classifierweights map back to specific markers and spatial regions driving predictions;and (ii) \textbf{synthetic data generation} for patient-derived organoids,exploiting the linearity of the LOT embedding. LOT barycenters yield averagedcellular profiles representing combined conditions or samples, supporting druginteraction testing.  Together, these results establish LOT as a unified framework that bridgespredictive performance, interpretability, and generative modeling. Bytransforming heterogeneous point clouds into structured embeddings directlytraceable to the original data, LOT opens new opportunities for understandingimmune variation and treatment effects in high-dimensional biological systems.</description>
      <author>example@mail.com (Tianxiang Wang, Yingtong Ke, Dhananjay Bhaskar, Smita Krishnaswamy, Alexander Cloninger)</author>
      <guid isPermaLink="false">2510.22033v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging</title>
      <link>http://arxiv.org/abs/2510.23641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为空间感知线性变换器(SAL-T)的新型架构，解决了在高能物理高数据吞吐量环境中部署Transformer模型时的资源消耗和延迟问题。&lt;h4&gt;背景&lt;/h4&gt;Transformers在捕获高能粒子碰撞中的全局和局部相关性方面非常有效，但在高数据吞吐量环境(如CERN LHC)中部署面临挑战，其二次复杂度需要大量资源并增加推理延迟。&lt;h4&gt;目的&lt;/h4&gt;开发一种资源高效且低延迟的Transformer变体，能够在保持高性能的同时适应高能物理等高数据吞吐量环境。&lt;h4&gt;方法&lt;/h4&gt;SAL-T基于linformer架构，结合了空间感知粒子分区(基于运动学特征)和卷积层(捕获局部相关性)，实现了物理意义显著的区域间注意力计算。&lt;h4&gt;主要发现&lt;/h4&gt;SAL-T在喷流分类任务中优于标准linformer，实现了与全注意力Transformer相当的结果，同时显著减少了资源使用和推理延迟；在ModelNet10点云分类数据集上验证了这一优势。&lt;h4&gt;结论&lt;/h4&gt;SAL-T是一种高效解决方案，能够在保持高性能的同时显著降低计算需求和延迟，特别适合高能物理等资源受限环境。&lt;h4&gt;翻译&lt;/h4&gt;Transformers在捕获高能粒子碰撞中的全局和局部相关性方面非常有效，但在高数据吞吐量环境中部署时面临挑战，例如CERN LHC。Transformer模型的二次复杂度需要大量资源并增加推理时的延迟。为了解决这些问题，我们引入了空间感知线性变换器(SAL-T)，这是对linformer架构的物理启发式增强，保持了线性注意力。我们的方法基于运动学特征对粒子进行空间感知分区，从而计算物理意义显著的区域之间的注意力。此外，我们使用卷积层来捕获局部相关性，这些见解来自喷流物理学。除了在喷流分类任务中优于标准linformer外，SAL-T还实现了与全注意力Transformer相当的分类结果，同时在推理过程中使用更少的资源和更低的延迟。在通用点云分类数据集(ModelNet10)上的实验进一步证实了这一趋势。我们的代码可在https://github.com/aaronw5/SAL-T4HEP获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决传统Transformer模型在粒子物理应用中的计算复杂度问题。传统Transformer具有二次方复杂度(O(n²))，导致在处理高能物理数据时资源需求大、推理延迟高。这个问题在现实中非常重要，因为CERN大型强子对撞机每秒产生4000万次碰撞事件，需要实时过滤系统(触发系统)来筛选数据，而传统Transformer无法满足这种低延迟、高吞吐量的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有高效Transformer变体(如Longformer、Linformer)在粒子物理中等长度输入(约100个token)上的局限性，然后结合粒子物理专业知识设计了SAL-T。方法借鉴了Linformer的线性注意力机制作为基础，并融入了粒子物理中的kT排序概念(用于粒子聚类算法)和卷积层设计来捕获局部相关性。作者通过三种主要修改来增强Linformer：基于kT指标的空间感知排序、分区注意力机制和卷积增强注意力，创造出一种既高效又能捕捉物理相关空间信息的新方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; SAL-T的核心思想是通过空间感知的分区和卷积增强来改进Linformer架构，使其能够更有效地处理粒子喷射分类任务，同时保持线性计算复杂度。具体实现流程包括：1)使用kT=pTΔR指标对粒子进行排序，确保物理相关的邻近粒子在序列中彼此接近；2)将排序后的键和值向量分区，使每个投影头只关注自己的粒子子集；3)在每个头的原始注意力分数上应用小型深度2D卷积，以融入局部邻居交互；4)通过线性分区粒子多头注意力(LPP-MHA)计算注意力并聚合值表示；5)对注意力输出进行最大聚合并传递到分类层。整个流程保持了线性计算复杂度(O(np))，同时捕获了局部喷射子结构。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)空间感知分区：基于kT指标对粒子排序并分区，使每个投影头关注物理相关的粒子子集；2)卷积增强注意力：在注意力分数上应用2D卷积，捕获局部邻居交互；3)物理启发的特征排序：使用kT而非传统pT排序，更好地保留空间结构；4)在保持线性复杂度的同时实现与全注意力Transformer相当的性能。相比之前工作，SAL-T克服了标准Linformer不编码空间信息的局限，实现了与全注意力Transformer相当的性能但计算复杂度显著降低，同时比特定于粒子物理的Transformer变体更适合资源受限的触发系统。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAL-T通过空间感知的分区和卷积增强，在保持线性计算复杂度的同时，实现了与全注意力Transformer相当的粒子喷射分类性能，使其成为资源受限的粒子物理触发系统的可行选择。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers are very effective in capturing both global and localcorrelations within high-energy particle collisions, but they presentdeployment challenges in high-data-throughput environments, such as the CERNLHC. The quadratic complexity of transformer models demands substantialresources and increases latency during inference. In order to address theseissues, we introduce the Spatially Aware Linear Transformer (SAL-T), aphysics-inspired enhancement of the linformer architecture that maintainslinear attention. Our method incorporates spatially aware partitioning ofparticles based on kinematic features, thereby computing attention betweenregions of physical significance. Additionally, we employ convolutional layersto capture local correlations, informed by insights from jet physics. Inaddition to outperforming the standard linformer in jet classification tasks,SAL-T also achieves classification results comparable to full-attentiontransformers, while using considerably fewer resources with lower latencyduring inference. Experiments on a generic point cloud classification dataset(ModelNet10) further confirm this trend. Our code is available athttps://github.com/aaronw5/SAL-T4HEP.</description>
      <author>example@mail.com (Aaron Wang, Zihan Zhao, Subash Katel, Vivekanand Gyanchand Sahu, Elham E Khoda, Abhijith Gandrakota, Jennifer Ngadiuba, Richard Cavanaugh, Javier Duarte)</author>
      <guid isPermaLink="false">2510.23641v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization</title>
      <link>http://arxiv.org/abs/2510.20974v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PCA点云(PPC)的标准化框架，用于解决点云强化学习中对相机姿态不匹配的敏感性问题，提高了算法在现实环境中的鲁棒性和可靠性。&lt;h4&gt;背景&lt;/h4&gt;强化学习从原始视觉输入中取得了显著成功，但对分布外变化(如光照、颜色和视点变化)仍然很脆弱。点云强化学习减轻了基于外观的脆弱性，但仍然受相机姿态不匹配的影响。&lt;h4&gt;目的&lt;/h4&gt;解决点云强化学习对相机姿态不匹配的敏感性，提高在现实环境中的可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出PCA点云(PPC)框架，将任意刚体变换下的点云映射到唯一的规范姿态，使观测与一致框架对齐，减少视点引起的不一致性。&lt;h4&gt;主要发现&lt;/h4&gt;PPC提高了对具有挑战性的机器人任务中未见过的相机姿态的鲁棒性，为领域随机化提供了有原则的替代方案。&lt;h4&gt;结论&lt;/h4&gt;PPC框架有效解决了点云强化学习中的视点不一致性问题，提高了算法在现实环境中的鲁棒性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;从原始视觉输入的强化学习(RL)近年来取得了显著的成功，但它对分布外变化(如光照、颜色和视点变化)仍然很脆弱。点云强化学习(PC-RL)通过减轻基于外观的脆弱性提供了一种有前途的替代方案，但它对相机姿态不匹配的敏感性继续削弱了在现实环境中的可靠性。为了解决这一挑战，我们提出了PCA点云(PPC)，这是一种专门为下游机器人控制设计的标准化框架。PPC将任意刚体变换下的点云映射到唯一的规范姿态，使观测与一致框架对齐，从而显著减少视点引起的不一致性。在我们的实验中，我们表明PPC提高了对具有挑战性的机器人任务中未见过的相机姿态的鲁棒性，为领域随机化提供了有原则的替代方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云强化学习（PC-RL）中的视角敏感性问题。点云通常在相机局部坐标系中表示，即使微小的视角变化也会导致场景表示显著改变，影响算法在实际环境中的可靠性。这个问题很重要，因为视角变化是现实世界中的常见现象，而现有方法如域随机化往往导致样本效率低下且理论保证较弱。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出PC-RL虽然减轻了外观相关的脆弱性，但仍存在视角敏感性问题。然后分析了域随机化方法的局限性，借鉴了旋转不变点云分析领域的进展，特别是基于PCA的标准化方法。作者识别出PCA方法存在符号歧义问题，设计了新颖的几何驱动消歧步骤。该方法借鉴了PointNet、PointNet++等点云处理方法，以及PointPatch RL作为基础强化学习框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过基于PCA的标准化方法，将点云映射到唯一的规范姿态，实现对刚体变换（平移和旋转）的不变性。具体流程包括：1)点云下采样（体素下采样+最远点采样）；2)中心化处理（减去质心）；3)PCA对齐（与特征向量对齐）；4)符号消歧（使用几何驱动分数函数解决PCA符号歧义）；5)生成规范表示（在消歧后的坐标系中重新表达点云）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出PCA点云（PPC）标准化框架；2)设计几何驱动的消歧方法解决PCA符号歧义；3)提供理论保证证明对刚体变换的不变性；4)模块化设计可集成到任何PC-RL算法中。相比之前工作，PPC比域随机化更高效且有理论保证；比现有旋转不变分析方法解决符号歧义问题；比传统PCA方法确保确定性输出；比其他点云方法更适合强化学习任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于PCA的点云标准化方法（PPC），通过解决PCA的符号歧义问题，实现了点云表示对相机视角变化的鲁棒性，显著提高了点云强化学习在未见视角下的零样本泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) from raw visual input has achieved impressivesuccesses in recent years, yet it remains fragile to out-of-distributionvariations such as changes in lighting, color, and viewpoint. Point CloudReinforcement Learning (PC-RL) offers a promising alternative by mitigatingappearance-based brittleness, but its sensitivity to camera pose mismatchescontinues to undermine reliability in realistic settings. To address thischallenge, we propose PCA Point Cloud (PPC), a canonicalization frameworkspecifically tailored for downstream robotic control. PPC maps point cloudsunder arbitrary rigid-body transformations to a unique canonical pose, aligningobservations to a consistent frame, thereby substantially decreasingviewpoint-induced inconsistencies. In our experiments, we show that PPCimproves robustness to unseen camera poses across challenging robotic tasks,providing a principled alternative to domain randomization.</description>
      <author>example@mail.com (Michael Bezick, Vittorio Giammarino, Ahmed H. Qureshi)</author>
      <guid isPermaLink="false">2510.20974v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence</title>
      <link>http://arxiv.org/abs/2510.24693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Homepage: https://internlm.github.io/StarBench/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了STAR-Bench，一个新的音频基准测试，用于评估模型对声音在时间和三维空间中的动态推理能力。与现有基准测试不同，STAR-Bench专注于文本描述难以捕捉的精细感知推理。&lt;h4&gt;背景&lt;/h4&gt;尽管多模态大语言模型和大型音频语言模型发展迅速，但现有的音频基准测试主要测试可以从文本标题中恢复的语义，掩盖了在精细感知推理方面的不足。&lt;h4&gt;目的&lt;/h4&gt;正式定义音频4D智能（即对时间和三维空间中声音动态的推理），并引入STAR-Bench基准测试来衡量这种能力，揭示当前模型在理解物理世界方面的不足。&lt;h4&gt;方法&lt;/h4&gt;STAR-Bench结合基础声学感知设置（六个属性）和整体时空推理设置（包括段重排序和空间任务）。数据收集使用程序合成和物理模拟的音频，以及包括人工注释的四阶段流程确保高质量样本。&lt;h4&gt;主要发现&lt;/h4&gt;对19个模型的评估显示与人类存在显著差距，闭源模型受限于精细感知，而开源模型在感知、知识和推理方面都落后。STAR-Bench导致的准确性下降远大于先前基准测试（时间维度-31.5%，空间维度-35.2%）。&lt;h4&gt;结论&lt;/h4&gt;STAR-Bench为开发具有更强大物理世界理解能力的未来模型提供了关键见解和明确的发展路径。&lt;h4&gt;翻译&lt;/h4&gt;尽管多模态大语言模型和大型音频语言模型取得了快速进展，但现有的音频基准测试主要测试可以从文本标题中恢复的语义，掩盖了在精细感知推理方面的不足。我们将音频4D智能正式定义为对时间和三维空间中声音动态的推理，并引入STAR-Bench来衡量它。STAR-Bench结合了基础声学感知设置（绝对和相对条件下的六个属性）和整体时空推理设置，包括连续和离散过程的段重排序以及跨越静态定位、多源关系和动态轨迹的空间任务。我们的数据收集流程使用两种方法确保高质量样本。对于基础任务，我们使用程序合成和物理模拟的音频。对于整体数据，我们遵循包括人工注释和基于人类表现的最终选择在内的四阶段流程。与先前仅通过标题回答略微降低准确性的基准测试不同，STAR-Bench导致更大的下降（时间维度-31.5%，空间维度-35.2%），证明了其对语言难以描述线索的关注。对19个模型的评估显示与人类存在显著差距，并揭示了能力层次结构：闭源模型受限于精细感知，而开源模型在感知、知识和推理方面都落后。我们的STAR-Bench为开发具有更强大物理世界理解能力的未来模型提供了关键见解和明确的发展路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite rapid progress in Multi-modal Large Language Models and LargeAudio-Language Models, existing audio benchmarks largely test semantics thatcan be recovered from text captions, masking deficits in fine-grainedperceptual reasoning. We formalize audio 4D intelligence that is defined asreasoning over sound dynamics in time and 3D space, and introduce STAR-Bench tomeasure it. STAR-Bench combines a Foundational Acoustic Perception setting (sixattributes under absolute and relative regimes) with a Holistic Spatio-TemporalReasoning setting that includes segment reordering for continuous and discreteprocesses and spatial tasks spanning static localization, multi-sourcerelations, and dynamic trajectories. Our data curation pipeline uses twomethods to ensure high-quality samples. For foundational tasks, we useprocedurally synthesized and physics-simulated audio. For holistic data, wefollow a four-stage process that includes human annotation and final selectionbased on human performance. Unlike prior benchmarks where caption-onlyanswering reduces accuracy slightly, STAR-Bench induces far larger drops(-31.5\% temporal, -35.2\% spatial), evidencing its focus on linguisticallyhard-to-describe cues. Evaluating 19 models reveals substantial gaps comparedwith humans and a capability hierarchy: closed-source models are bottleneckedby fine-grained perception, while open-source models lag across perception,knowledge, and reasoning. Our STAR-Bench provides critical insights and a clearpath forward for developing future models with a more robust understanding ofthe physical world.</description>
      <author>example@mail.com (Zihan Liu, Zhikang Niu, Qiuyang Xiao, Zhisheng Zheng, Ruoqi Yuan, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Jianze Liang, Xie Chen, Leilei Sun, Dahua Lin, Jiaqi Wang)</author>
      <guid isPermaLink="false">2510.24693v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning</title>
      <link>http://arxiv.org/abs/2510.23907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为DynaStride的管道方法，用于生成教学视频中场景级别的连贯字幕，无需手动场景分割。该方法通过自适应帧采样和多模态窗口捕获关键转换，采用多模态思维链过程生成动作-对象对，并使用动态步长窗口选择算法进行融合，最终生成整合视觉语义和时间推理的场景级字幕。&lt;h4&gt;背景&lt;/h4&gt;教学视频中的场景级字幕通过理解视觉线索和时间结构来增强学习。将视觉线索与文本指导相结合支持程序学习和多模态推理，为技能获取提供丰富上下文。然而，未能捕捉这种结构的字幕可能缺乏连贯性和质量，造成混淆并破坏视频的教育意图。&lt;h4&gt;目的&lt;/h4&gt;解决现有字幕生成方法无法有效捕捉教学视频中时间结构和视觉语义的问题，开发一种能够生成连贯、高质量场景级字幕的方法，而无需手动场景分割。&lt;h4&gt;方法&lt;/h4&gt;作者提出了DynaStride管道，使用YouCookII数据集的场景注释，执行自适应帧采样和多模态窗口化来捕获每个场景内的关键转换。然后采用多模态思维链过程产生多个动作-对象对，并使用动态步长窗口选择算法进行精炼和融合，该算法自适应地平衡时间上下文和冗余。最终的场景级字幕将视觉语义和时间推理整合在一个教学字幕中。&lt;h4&gt;主要发现&lt;/h4&gt;与包括VLLaMA3和GPT-4o在内的强大基线相比，DynaStride在基于N-gram的指标(BLEU, METEOR)和语义相似性度量(BERTScore, CLIPScore)上均表现出一致的性能提升。定性分析进一步表明，DynaStride生成的字幕在时间连贯性和信息性方面更优。&lt;h4&gt;结论&lt;/h4&gt;DynaStride为改进AI驱动的教学内容生成提供了有希望的方向，能够生成更连贯、信息更丰富的场景级字幕，有助于提高教学视频的学习效果。&lt;h4&gt;翻译&lt;/h4&gt;教学视频中的场景级字幕可以通过要求理解视觉线索和时间结构来增强学习。通过将视觉线索与文本指导相一致，这种理解支持程序学习和多模态推理，为技能获取提供更丰富的上下文。然而，未能捕捉这种结构的字幕可能缺乏连贯性和质量，这可能造成混淆并破坏视频的教育意图。为了解决这一差距，我们引入了DynaStride，一个无需手动场景分割即可生成连贯场景级字幕的管道。使用YouCookII数据集的场景注释，DynaStride执行自适应帧采样和多模态窗口化来捕获每个场景内的关键转换。然后，它采用多模态思维链过程产生多个动作-对象对，这些对使用动态步长窗口选择算法进行精炼和融合，该算法自适应地平衡时间上下文和冗余。最终的场景级字幕将视觉语义和时间推理整合在一个教学字幕中。与包括VLLaMA3和GPT-4o在内的强大基线的经验评估表明，在基于N-gram的指标(BLEU, METEOR)和语义相似性度量(BERTScore, CLIPScore)上均表现出一致的提升。定性分析进一步表明，DynaStride生成的字幕在时间连贯性和信息性方面更优，这表明改进AI驱动的教学内容生成是一个有希望的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene-level captioning in instructional videos can enhance learning byrequiring an understanding of both visual cues and temporal structure. Byaligning visual cues with textual guidance, this understanding supportsprocedural learning and multimodal reasoning, providing a richer context forskill acquisition. However, captions that fail to capture this structure maylack coherence and quality, which can create confusion and undermine thevideo's educational intent. To address this gap, we introduce DynaStride, apipeline to generate coherent, scene-level captions without requiring manualscene segmentation. Using the YouCookII dataset's scene annotations, DynaStrideperforms adaptive frame sampling and multimodal windowing to capture keytransitions within each scene. It then employs a multimodal chain-of-thoughtprocess to produce multiple action-object pairs, which are refined and fusedusing a dynamic stride window selection algorithm that adaptively balancestemporal context and redundancy. The final scene-level caption integratesvisual semantics and temporal reasoning in a single instructional caption.Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) andsemantic similarity measures (BERTScore, CLIPScore). Qualitative analysesfurther show that DynaStride produces captions that are more temporallycoherent and informative, suggesting a promising direction for improvingAI-powered instructional content generation.</description>
      <author>example@mail.com (Eddison Pham, Prisha Priyadarshini, Adrian Maliackel, Kanishk Bandi, Cristian Meo, Kevin Zhu)</author>
      <guid isPermaLink="false">2510.23907v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations</title>
      <link>http://arxiv.org/abs/2510.23397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VideoTG-R1是一种新型课程强化学习框架，通过反射边界标注解决视频时间定位中的训练样本质量和难度问题，实现了数据高效训练。&lt;h4&gt;背景&lt;/h4&gt;视频时间定位(VTG)是根据语言查询在视频中定位精确片段的基础挑战。多模态大型语言模型(MLLMs)通过强化学习(RL)在VTG方面显示出潜力，但忽视了训练样本质量和难度带来的挑战。&lt;h4&gt;目的&lt;/h4&gt;解决VTG训练中部分标注样本和难以定位样本带来的问题，提高训练效率。&lt;h4&gt;方法&lt;/h4&gt;提出VideoTG-R1框架，包含边界反射代理(识别并过滤部分标注样本)和难度估计代理(评估样本难度并设计课程RL策略)，实现数据高效训练。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用10%的训练样本和21%的计算预算，VideoTG-R1在组相对策略优化(GRPO)和监督微调(SFT)下都优于全数据对应方法。&lt;h4&gt;结论&lt;/h4&gt;VideoTG-R1通过解决训练样本质量和难度问题，实现了在VTG和基于视频的问答任务上的有效性能提升。&lt;h4&gt;翻译&lt;/h4&gt;视频时间定位(VTG)旨在根据语言查询在视频中定位精确片段，这是视频理解中的一个基础挑战。虽然最近的多模态大型语言模型(MLLMs)通过强化学习(RL)在解决VTG方面显示出潜力，但它们忽视了训练样本质量和难度带来的挑战。(1)部分标注样本：许多样本包含超出标注区间的相关片段，引入了模糊监督。(2)难以定位的样本：零样本性能差的样本在RL训练中产生持续低且不可区分的奖励，在多个输出中没有明显偏好，从而阻碍学习效率。为解决这些挑战，我们提出VideoTG-R1，一个具有反射边界标注的新型课程RL框架，实现数据高效训练。具体来说，我们提出边界反射代理，利用MLLMs预测标注区间外的查询相关时间戳，使我们能够识别并过滤部分标注样本，从而减少模糊性。此外，我们引入难度估计代理来评估每个样本的训练难度，并设计课程RL策略，根据训练步骤动态掩盖难以定位样本的视频，降低训练难度并提供更清晰的偏好。在VTG和基于视频的问答任务上的实验证明了我们方法的有效性。值得注意的是，仅使用10%的训练样本和21%的计算预算，VideoTG-R1在组相对策略优化(GRPO)和监督微调(SFT)下都优于全数据对应方法。代码可在https://github.com/ldong1111/VideoTG-R1获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video temporal grounding (VTG) aims to locate precise segments in videosbased on language queries, which is a fundamental challenge in videounderstanding. While recent Multimodal Large Language Models (MLLMs) have shownpromise in tackling VTG through reinforcement learning (RL), they overlook thechallenges arising from both the quality and difficulty of training samples.(1) Partially annotated samples. Many samples contain relevant segments beyondthe annotated interval, introducing ambiguous supervision. (2) Hard-to-groundsamples. Samples with poor zero-shot performance produce consistently low andindistinguishable rewards during RL training, exhibiting no clear preferenceamong multiple outputs and thus hindering learning efficiency. To address thesechallenges, we propose VideoTG-R1, a novel curriculum RL framework withreflected boundary annotations, enabling data-efficient training. Specifically,we propose a Boundary Reflection Agent that utilizes MLLMs to predictquery-relevant timestamps outside the annotated intervals, allowing us toidentify and filter out partially annotated samples, thereby reducingambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assessthe training difficulty of each sample and design a curriculum RL strategy thatdynamically masks the videos of hard-to-ground samples according to thetraining steps, easing the training difficulty and providing clearerpreference. Experiments on the VTG and grounded VideoQA tasks demonstrate theeffectiveness of our method. Remarkably, with only 10% of the training samplesand 21% of the computational budget, VideoTG-R1 outperforms full-datacounterparts under both group relative policy optimization (GRPO) andsupervised fine-tuning (SFT). The code is available athttps://github.com/ldong1111/VideoTG-R1.</description>
      <author>example@mail.com (Lu Dong, Haiyu Zhang, Han Lin, Ziang Yan, Xiangyu Zeng, Hongjie Zhang, Yifei Huang, Yi Wang, Zhen-Hua Ling, Limin Wang, Yali Wang)</author>
      <guid isPermaLink="false">2510.23397v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Evaluation of Vision-LLMs in Surveillance Video</title>
      <link>http://arxiv.org/abs/2510.23190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as poster in the NeurIPS 2025 Workshop on Space in Vision,  Language, and Embodied AI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视觉语言模型在异常动作识别中的空间推理能力，将其作为零样本、语言基础的任务，解决了从稀疏2D视频中解释动态3D场景的具身感知挑战。&lt;h4&gt;背景&lt;/h4&gt;社会中摄像机的广泛应用产生了大量视频数据，远远超出人工监控能力，这对公共安全构成重大挑战，因为及时检测异常或犯罪事件对有效预防和响应至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究视觉语言模型（VLMs）的空间推理能力，探索小型预训练视觉-语言模型作为空间基础的零样本异常检测器的可行性，并评估其在提示和隐私保护条件下的表现。&lt;h4&gt;方法&lt;/h4&gt;将视频转换为文本描述并通过文本蕴含对标签进行评分，在UCF-Crime和RWF-2000数据集上评估四个开放模型，研究少样本示例和隐私过滤器对模型性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;少样本示例可以提高某些模型的准确性但可能增加误报；隐私过滤器（尤其是全身GAN变换）会引入不一致性降低准确性；当前视觉-语言模型在简单、空间显著事件上表现良好，但在处理嘈杂空间线索和身份模糊时表现不佳。&lt;h4&gt;结论&lt;/h4&gt;提出了加强空间基础的具体路径，包括结构感知提示、跨片段轻量级空间记忆、描述过程中的场景图或3D姿态先验，以及保留动作相关几何形状的隐私方法，使零样本、语言基础的管道成为具身、真实世界视频理解的适应性构建块。&lt;h4&gt;翻译&lt;/h4&gt;我们社会中摄像机的广泛应用产生了大量视频数据，远远超出了人工监控的能力。这对公共安全和安全构成了关键挑战，因为及时检测异常或犯罪事件对于有效预防和应对至关重要。具身代理识别意外事件的能力根本上与其空间推理能力相关。本文通过将异常动作识别构架为零样本、语言基础的任务，研究了视觉语言模型（VLMs）的空间推理，解决了从稀疏2D视频中解释动态3D场景的具身感知挑战。具体来说，我们调查了小型预训练视觉-语言模型是否可以通过将视频转换为文本描述并通过文本蕴含对标签进行评分，作为空间基础的零样本异常检测器。我们在提示和隐私保护条件下，在UCF-Crime和RWF-2000数据集上评估了四个开放模型。少样本示例可以提高某些模型的准确性，但可能增加误报，而隐私过滤器——尤其是全身GAN变换——会引入不一致性，降低准确性。这些结果展示了当前视觉-语言模型在哪些方面成功（简单、空间显著事件）和哪些方面失败（嘈杂的空间线索、身份模糊）。展望未来，我们概述了加强空间基础的具体路径，无需任务特定训练：结构感知提示、跨片段轻量级空间记忆、描述过程中的场景图或3D姿态先验，以及保留动作相关几何形状的隐私方法。这使零样本、语言基础的管道成为具身、真实世界视频理解的适应性构建块。我们用于评估VLMs的实现已在以下公开可用：https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何评估视觉-语言模型在监控视频中进行零样本异常行为识别的问题。这个问题很重要，因为社会上有大量摄像头产生的视频数据远远超过人类监控能力，及时检测异常或犯罪事件对公共安全和有效预防至关重要。此外，现有的公共异常行为识别数据集有限，仅在这些数据集上训练的模型可能无法很好地泛化到新的异常类型。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考过程是认识到传统视频异常检测依赖监督学习，需要大量带注释的数据集，成本高且难以识别新异常。因此他们设计了一个零样本框架，将异常分类重新构建为语言基础推理任务而非像素到标签映射。该方法借鉴了现有VLM的语义推理和世界知识，但不同于之前的AnomalyCLIP、LAVAD等方法，它不需要任务特定微调，而是专注于纯零样本异常检测。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将异常分类转化为语言基础推理任务，利用大型预训练视觉-语言模型的语义推理能力。整体流程分为两步：1)文本描述生成：视觉-LLM处理视频输入，基于视觉和提示生成描述性文本；2)零样本分类：使用预训练的NLI分类器评估文本与候选异常类别的逻辑蕴含程度，选择最高分数的类别作为结果。整个过程无需对模型进行梯度更新，实现了真正的零样本学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：系统评估小型视觉-LLMs在零样本异常识别中的能力；设计多种提示策略实验；研究隐私保护变换对模型性能的影响；提出改进空间推理的具体方法。与之前工作不同，该方法不需要大量标注数据，专注于纯零样本检测，首次系统评估了隐私保护变换对模型的影响，并揭示了提示技术与隐私过滤器之间的关键权衡。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统评估视觉-LLMs在零样本异常检测中的能力，揭示了提示技术和隐私过滤器之间的关键权衡，为设计更安全、更有效的视频理解系统提供了实用建议。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The widespread use of cameras in our society has created an overwhelmingamount of video data, far exceeding the capacity for human monitoring. Thispresents a critical challenge for public safety and security, as the timelydetection of anomalous or criminal events is crucial for effective response andprevention. The ability for an embodied agent to recognize unexpected events isfundamentally tied to its capacity for spatial reasoning. This paperinvestigates the spatial reasoning of vision-language models (VLMs) by framinganomalous action recognition as a zero-shot, language-grounded task, addressingthe embodied perception challenge of interpreting dynamic 3D scenes from sparse2D video. Specifically, we investigate whether small, pre-trained vision--LLMscan act as spatially-grounded, zero-shot anomaly detectors by converting videointo text descriptions and scoring labels via textual entailment. We evaluatefour open models on UCF-Crime and RWF-2000 under prompting andprivacy-preserving conditions. Few-shot exemplars can improve accuracy for somemodels, but may increase false positives, and privacy filters -- especiallyfull-body GAN transforms -- introduce inconsistencies that degrade accuracy.These results chart where current vision--LLMs succeed (simple, spatiallysalient events) and where they falter (noisy spatial cues, identityobfuscation). Looking forward, we outline concrete paths to strengthen spatialgrounding without task-specific training: structure-aware prompts, lightweightspatial memory across clips, scene-graph or 3D-pose priors during description,and privacy methods that preserve action-relevant geometry. This positionszero-shot, language-grounded pipelines as adaptable building blocks forembodied, real-world video understanding. Our implementation for evaluatingVLMs is publicly available at:https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition</description>
      <author>example@mail.com (Pascal Benschop, Cristian Meo, Justin Dauwels, Jelte P. Mense)</author>
      <guid isPermaLink="false">2510.23190v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Large-Model AI for Near Field Beam Prediction: A CNN-GPT2 Framework for 6G XL-MIMO</title>
      <link>http://arxiv.org/abs/2510.22557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于CNN-GPT2的新型近场波束预测框架，用于解决毫米波通信中极大规模天线阵列在高移动性场景下的近场波束预测挑战。&lt;h4&gt;背景&lt;/h4&gt;毫米波通信中极大规模天线阵列在高移动性场景下的应用凸显了近场波束预测的重要性。与传统远场假设不同，近场波束预测需要在角度和距离域联合采样码本，导致导频开销大幅增加。此外，最优近场波束指数表现出突发的非线性动态特性，对时间建模构成挑战。&lt;h4&gt;目的&lt;/h4&gt;解决近场波束预测中的挑战，设计一个有效的近场波束预测框架，以应对导频开销增加和波束指数非线性动态特性问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于CNN-GPT2的新型近场波束预测框架。具体包括：设计上行链路导频传输策略，通过宽波束模拟预编码和频率变化数字预编码实现高效信道探测；接收的导频信号经过预处理后，通过基于CNN的特征提取器；然后通过GPT-2模型捕获多个帧之间的时间依赖性，以端到端方式直接预测近场波束指数。&lt;h4&gt;主要发现&lt;/h4&gt;CNN-GPT2框架能够有效处理近场波束预测的挑战，所提出的导频传输策略实现了高效信道探测，该方法能够捕获时间依赖性并直接预测近场波束指数。&lt;h4&gt;结论&lt;/h4&gt;基于CNN-GPT2的近场波束预测框架为解决毫米波通信中极大规模天线阵列在高移动性场景下的近场波束预测问题提供了有效方案。&lt;h4&gt;翻译&lt;/h4&gt;毫米波通信中极大规模天线阵列的出现，尤其是在高移动性场景下，凸显了近场波束预测的重要性。与传统远场假设不同，近场波束预测需要同时在角度和距离域采样的码本，这导致导频开销大幅增加。此外，与远场情况下最优波束演化的时间平滑性不同，最优近场波束指数由于其同时依赖于用户角度和距离而表现出突发和非线性动态特性，给时间建模带来了重大挑战。为应对这些挑战，我们提出了一种新颖的基于卷积神经网络-生成预训练Transformer 2（CNN-GPT2）的近场波束预测框架。具体而言，设计了一种上行链路导频传输策略，通过宽波束模拟预编码和频率变化数字预编码实现高效信道探测。接收的导频信号经过预处理后，通过基于CNN的特征提取器，然后由GPT-2模型捕获多个帧之间的时间依赖性，并以端到端方式直接预测近场波束指数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of extremely large-scale antenna arrays (ELAA) inmillimeter-wave (mmWave) communications, particularly in high-mobilityscenarios, highlights the importance of near-field beam prediction. Unlike theconventional far-field assumption, near-field beam prediction requirescodebooks that jointly sample the angular and distance domains, which leads toa dramatic increase in pilot overhead. Moreover, unlike the far-field casewhere the optimal beam evolution is temporally smooth, the optimal near-fieldbeam index exhibits abrupt and nonlinear dynamics due to its joint dependenceon user angle and distance, posing significant challenges for temporalmodeling. To address these challenges, we propose a novel Convolutional NeuralNetwork-Generative Pre-trained Transformer 2 (CNN-GPT2) based near-field beamprediction framework. Specifically, an uplink pilot transmission strategy isdesigned to enable efficient channel probing through widebeam analog precodingand frequency-varying digital precoding. The received pilot signals arepreprocessed and passed through a CNN-based feature extractor, followed by aGPT-2 model that captures temporal dependencies across multiple frames anddirectly predicts the near-field beam index in an end-to-end manner.</description>
      <author>example@mail.com (Wang Liu, Cunhua Pan, Hong Ren, Wei Zhang, Cheng-Xiang Wang, Jiangzhou Wang)</author>
      <guid isPermaLink="false">2510.22557v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>TERRA: A Transformer-Enabled Recursive R-learner for Longitudinal Heterogeneous Treatment Effect Estimation</title>
      <link>http://arxiv.org/abs/2510.22407v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为TERRA的新方法，用于解决纵向数据中异质性处理效应估计的挑战，特别是在时变干预情况下。&lt;h4&gt;背景&lt;/h4&gt;在纵向数据中准确估计异质性处理效应对医疗保健、公共政策、教育和数字营销等领域的个性化决策至关重要。然而，时变干预带来了延续效应、时变异质性和处理后偏差等独特挑战，这些挑战无法通过标准HTE方法解决。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理时变干预带来的独特挑战（延续效应、时变异质性和处理后偏差）的方法，以准确估计纵向数据中的异质性处理效应。&lt;h4&gt;方法&lt;/h4&gt;引入TERRA（Transformer-Enabled Recursive R-learner），包含两个主要组件：1）使用Transformer架构编码完整的处理特征历史，捕捉长期时间依赖性和延续效应；2）开发递归残差学习公式，将经典结构嵌套均值模型推广到参数规范之外，解决处理后偏差问题。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和数据应用中，TERRA在HTE估计的准确性和稳定性方面始终优于强大的基线方法。&lt;h4&gt;结论&lt;/h4&gt;将有原则的因果结构与高容量的序列模型相结合，对于纵向异质性处理效应估计具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;在纵向环境中准确估计异质性处理效应(HTE)对于医疗保健、公共政策、教育和数字营销等领域的个性化决策至关重要。然而，时变干预带来了许多独特挑战，如延续效应、时变异质性和处理后偏差，这些问题标准HTE方法无法解决。为应对这些挑战，我们引入了TERRA（Transformer-Enabled Recursive R-learner），它促进具有灵活时间建模和学习的纵向HTE估计。TERRA有两个组件。首先，我们使用Transformer架构编码完整的处理特征历史，使表示长期时间依赖性和延续效应成为可能，从而更全面地捕捉个体和特定时间的处理效应变化。其次，我们开发了一种递归残差学习公式，将经典结构嵌套均值模型(SNMMs)推广到参数规范之外，解决了处理后偏差问题，同时减少了对功能假设的依赖。在模拟和数据应用中，TERRA在HTE估计的准确性和稳定性方面始终优于强大的基线方法，突显了将原则性因果结构与高容量序列模型相结合对纵向HTE的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately estimating heterogeneous treatment effects (HTE) in longitudinalsettings is essential for personalized decision-making across healthcare,public policy, education, and digital marketing. However, time-varyinginterventions introduce many unique challenges, such as carryover effects,time-varying heterogeneity, and post-treatment bias, which are not addressed bystandard HTE methods. To address these challenges, we introduce TERRA(Transformer-Enabled Recursive R-learner), which facilitates longitudinal HTEestimation with flexible temporal modeling and learning. TERRA has twocomponents. First, we use a Transformer architecture to encode fulltreatment-feature histories, enabling the representation of long-range temporaldependencies and carryover effects, hence capturing individual- andtime-specific treatment effect variation more comprehensively. Second, wedevelop a recursive residual-learning formulation that generalizes theclassical structural nested mean models (SNMMs) beyond parametricspecifications, addressing post-treatment bias while reducing reliance onfunctional assumptions. In simulations and data applications, TERRAconsistently outperforms strong baselines in HTE estimation in both accuracyand stability, highlighting the value of combining principled causal structurewith high-capacity sequence models for longitudinal HTE.</description>
      <author>example@mail.com (Lei Shi, Sizhu Lu, Qiuran Lyu, Peng Ding, Nikos Vlassis)</author>
      <guid isPermaLink="false">2510.22407v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World and Spatio-Temporal Deep Learning</title>
      <link>http://arxiv.org/abs/2510.22056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种稳健的深度学习框架，通过结合以人为中心的预处理和时空建模来解决监控视频异常检测中的挑战，在UCF-Crime数据集上实现了92.41%的平均测试准确率。&lt;h4&gt;背景&lt;/h4&gt;监控视频中的异常检测面临异常事件多样性、类别不平衡和场景依赖的视觉混乱等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个稳健的深度学习框架，整合以人为中心的预处理与时空建模，用于多类异常分类。&lt;h4&gt;方法&lt;/h4&gt;使用YOLO-World识别人体实例，ByteTrack进行身份感知跟踪，通过高斯模糊抑制背景区域，使用InceptionV3进行空间特征提取，并用双向LSTM捕获时间动态进行序列级分类。&lt;h4&gt;主要发现&lt;/h4&gt;在UCF-Crime五类子集上评估，三次独立试验中平均测试准确率达92.41%，每类F1分数均超过0.85，展示了对类别不平衡的强鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;前景聚焦的预处理显著增强了现实监控场景中的异常辨别能力。&lt;h4&gt;翻译&lt;/h4&gt;监控视频中的异常检测由于异常事件的多样性、类别不平衡和场景依赖的视觉混乱而仍然是一项具有挑战性的任务。为了解决这些问题，我们提出了一个稳健的深度学习框架，该框架整合了以人为中心的预处理与时空建模，用于多类异常分类。我们的流程首先应用YOLO-World（一种开放词汇的视觉语言检测器）来识别原始视频片段中的人体实例，然后使用ByteTrack进行一致的身份感知跟踪。通过高斯模糊抑制检测边界框外的背景区域，有效减少场景特定的干扰，使模型专注于行为相关的前景内容。然后，经过精炼的帧由在ImageNet上预训练的InceptionV3网络处理进行空间特征提取，并使用双向LSTM（BiLSTM）捕获时间动态，进行序列级分类。在UCF-Crime数据集的五类子集（正常、入室盗窃、打架、纵火、爆炸）上评估，我们的方法在三次独立试验中平均测试准确率达到92.41%，每个类别的F1分数均超过0.85。全面的评估指标，包括混淆矩阵、ROC曲线和宏/加权平均值，展示了强大的泛化能力和对类别不平衡的鲁棒性。结果证实，前景聚焦的预处理显著增强了现实监控场景中的异常辨别能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection in surveillance videos remains a challenging task due tothe diversity of abnormal events, class imbalance, and scene-dependent visualclutter. To address these issues, we propose a robust deep learning frameworkthat integrates human-centric preprocessing with spatio-temporal modeling formulti-class anomaly classification. Our pipeline begins by applying YOLO-World- an open-vocabulary vision-language detector - to identify human instances inraw video clips, followed by ByteTrack for consistent identity-aware tracking.Background regions outside detected bounding boxes are suppressed via Gaussianblurring, effectively reducing scene-specific distractions and focusing themodel on behaviorally relevant foreground content. The refined frames are thenprocessed by an ImageNet-pretrained InceptionV3 network for spatial featureextraction, and temporal dynamics are captured using a bidirectional LSTM(BiLSTM) for sequence-level classification. Evaluated on a five-class subset ofthe UCF-Crime dataset (Normal, Burglary, Fighting, Arson, Explosion), ourmethod achieves a mean test accuracy of 92.41% across three independent trials,with per-class F1-scores consistently exceeding 0.85. Comprehensive evaluationmetrics - including confusion matrices, ROC curves, and macro/weighted averages- demonstrate strong generalization and resilience to class imbalance. Theresults confirm that foreground-focused preprocessing significantly enhancesanomaly discrimination in real-world surveillance scenarios.</description>
      <author>example@mail.com (Mohammad Ali Etemadi Naeen, Hoda Mohammadzade, Saeed Bagheri Shouraki)</author>
      <guid isPermaLink="false">2510.22056v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues</title>
      <link>http://arxiv.org/abs/2510.18016v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为ViBED-Net的新型深度学习框架，用于从视频数据中检测学生在在线学习环境中的参与度。该模型通过结合面部表情和全场景上下文信息，利用双流架构和EfficientNetV2进行特征提取，并使用LSTM或Transformer进行时间建模。&lt;h4&gt;背景&lt;/h4&gt;在线学习环境中的参与度检测对于提高学生成果和个性化教学至关重要。存在一个名为DAiSEE的大规模基准数据集，用于电子学习中情感状态识别。&lt;h4&gt;目的&lt;/h4&gt;提出ViBED-Net（基于视频的参与度检测网络），一种新的深度学习框架，设计用于从视频数据评估学生参与度。&lt;h4&gt;方法&lt;/h4&gt;采用双流架构，使用EfficientNetV2进行空间特征提取，处理面部裁剪和完整视频帧来捕捉面部表情和全场景上下文。使用两种时间建模策略分析特征：长短期记忆（LSTM）网络和Transformer编码器。在DAiSEE数据集上评估模型，并应用有针对性的数据增强技术提高在代表性不足的参与度类别上的性能。&lt;h4&gt;主要发现&lt;/h4&gt;ViBED-Net与LSTM的组合达到73.43%的准确率，优于现有的最先进方法。结合面部感知和场景感知的时空线索显著提高了参与度检测的准确性。&lt;h4&gt;结论&lt;/h4&gt;ViBED-Net的模块化设计使其具有灵活性，可应用于教育、用户体验研究和内容个性化。该工作通过为现实世界的参与度分析提供可扩展的高性能解决方案，推动了基于视频的情感计算发展。项目源代码可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;在线学习环境中的参与度检测对于提高学生成果和个性化教学至关重要。我们提出了ViBED-Net（基于视频的参与度检测网络），一种新的深度学习框架，旨在通过双流架构从视频数据评估学生参与度。ViBED-Net通过EfficientNetV2处理面部裁剪和完整视频帧来捕捉面部表情和全场景上下文，用于空间特征提取。然后使用两种时间建模策略分析这些特征：长短期记忆（LSTM）网络和Transformer编码器。我们的模型在DAiSEE数据集上进行了评估，这是一个电子学习中情感状态识别的大规模基准。为了提高在代表性不足的参与度类别上的性能，我们应用了有针对性的数据增强技术。在测试的变体中，ViBED-Net与LSTM结合实现了73.43%的准确率，优于现有的最先进方法。ViBED-Net证明，结合面部感知和场景感知的时空线索显著提高了参与度检测的准确性。其模块化设计使其具有灵活性，可应用于教育、用户体验研究和内容个性化。这项工作通过为现实世界的参与度分析提供可扩展的高性能解决方案，推动了基于视频的情感计算发展。该项目的源代码可在https://github.com/prateek-gothwal/ViBED-Net获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Engagement detection in online learning environments is vital for improvingstudent outcomes and personalizing instruction. We present ViBED-Net(Video-Based Engagement Detection Network), a novel deep learning frameworkdesigned to assess student engagement from video data using a dual-streamarchitecture. ViBED-Net captures both facial expressions and full-scene contextby processing facial crops and entire video frames through EfficientNetV2 forspatial feature extraction. These features are then analyzed over time usingtwo temporal modeling strategies: Long Short-Term Memory (LSTM) networks andTransformer encoders. Our model is evaluated on the DAiSEE dataset, alarge-scale benchmark for affective state recognition in e-learning. To enhanceperformance on underrepresented engagement classes, we apply targeted dataaugmentation techniques. Among the tested variants, ViBED-Net with LSTMachieves 73.43\% accuracy, outperforming existing state-of-the-art approaches.ViBED-Net demonstrates that combining face-aware and scene-aware spatiotemporalcues significantly improves engagement detection accuracy. Its modular designallows flexibility for application across education, user experience research,and content personalization. This work advances video-based affective computingby offering a scalable, high-performing solution for real-world engagementanalysis. The source code for this project is available onhttps://github.com/prateek-gothwal/ViBED-Net .</description>
      <author>example@mail.com (Prateek Gothwal, Deeptimaan Banerjee, Ashis Kumer Biswas)</author>
      <guid isPermaLink="false">2510.18016v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment</title>
      <link>http://arxiv.org/abs/2510.23224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PathSearch是一个结合细粒度注意力马赛克表示与全局幻灯片嵌入的检索框架，通过视觉语言对比学习对齐，在数字病理学中实现了准确和灵活的幻灯片检索，提高了诊断准确性和观察者一致性。&lt;h4&gt;背景&lt;/h4&gt;病理切片的快速数字化为临床和研究工作流程中的计算工具开辟了新可能。基于内容的幻灯片检索使病理学家能够识别形态学和语义上相似的病例，支持精确诊断、增强观察者间一致性并协助基于案例的教育。然而，全幻灯片图像的有效检索仍具挑战性，因其具有千兆像素规模且在大量无关内容中捕捉细微语义差异困难。&lt;h4&gt;目的&lt;/h4&gt;开发一个检索框架来克服全幻灯片图像检索的挑战，实现准确、高效的幻灯片检索功能。&lt;h4&gt;方法&lt;/h4&gt;提出PathSearch检索框架，统一细粒度注意力马赛克表示与全局幻灯片嵌入，通过视觉语言对比学习对齐。在6,926个幻灯片-报告对语料库上训练，支持两种关键功能：(1)基于马赛克的图像到图像检索；(2)多模态检索，文本查询可直接检索相关幻灯片。&lt;h4&gt;主要发现&lt;/h4&gt;在四个公共病理数据集和三个内部队列上进行了严格评估，涵盖解剖部位检索、肿瘤亚型分类、肿瘤与非肿瘤鉴别及跨多个器官的分级任务。外部结果显示PathSearch优于传统图像到图像检索框架。多中心读者研究证明在真实临床场景中提高了诊断准确性，增强了信心，并提高了观察者间一致性。&lt;h4&gt;结论&lt;/h4&gt;PathSearch被确立为数字病理学中可扩展和通用的检索解决方案。&lt;h4&gt;翻译&lt;/h4&gt;病理切片的快速数字化为临床和研究工作流程中的计算工具开辟了新的可能性。在这些工具中，基于内容的幻灯片检索脱颖而出，使病理学家能够识别形态学和语义上相似的病例，从而支持精确诊断，增强观察者间的一致性，并协助基于案例的教育。然而，由于全幻灯片图像的千兆像素规模以及在大量无关内容中捕捉细微语义差异的困难，全幻灯片图像的有效检索仍然具有挑战性。为了克服这些挑战，我们提出了PathSearch，这是一个检索框架，统一了细粒度注意力马赛克表示与全局幻灯片嵌入，通过视觉语言对比学习对齐。在包含6,926个幻灯片-报告对的语料库上训练，PathSearch捕捉了细粒度形态学线索和高级语义模式，以实现准确和灵活的检索。该框架支持两个关键功能：(1)基于马赛克的图像到图像检索，确保准确和高效的幻灯片研究；(2)多模态检索，文本查询可以直接检索相关幻灯片。PathSearch在四个公共病理数据集和三个内部队列上进行了严格评估，涵盖了解剖部位检索、肿瘤亚型分类、肿瘤与非肿瘤鉴别以及跨乳腺、肺、肾、肝和胃等多种器官的分级任务。外部结果显示，PathSearch优于传统的图像到图像检索框架。多中心读者研究进一步证明，在真实临床场景中，PathSearch提高了病理学家的诊断准确性，增强了信心，并提高了观察者间的一致性。这些结果确立了PathSearch作为数字病理学中可扩展和通用的检索解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid digitization of histopathology slides has opened up newpossibilities for computational tools in clinical and research workflows. Amongthese, content-based slide retrieval stands out, enabling pathologists toidentify morphologically and semantically similar cases, thereby supportingprecise diagnoses, enhancing consistency across observers, and assistingexample-based education. However, effective retrieval of whole slide images(WSIs) remains challenging due to their gigapixel scale and the difficulty ofcapturing subtle semantic differences amid abundant irrelevant content. Toovercome these challenges, we present PathSearch, a retrieval framework thatunifies fine-grained attentive mosaic representations with global-wise slideembeddings aligned through vision-language contrastive learning. Trained on acorpus of 6,926 slide-report pairs, PathSearch captures both fine-grainedmorphological cues and high-level semantic patterns to enable accurate andflexible retrieval. The framework supports two key functionalities: (1)mosaic-based image-to-image retrieval, ensuring accurate and efficient slideresearch; and (2) multi-modal retrieval, where text queries can directlyretrieve relevant slides. PathSearch was rigorously evaluated on four publicpathology datasets and three in-house cohorts, covering tasks includinganatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination,and grading across diverse organs such as breast, lung, kidney, liver, andstomach. External results show that PathSearch outperforms traditionalimage-to-image retrieval frameworks. A multi-center reader study furtherdemonstrates that PathSearch improves diagnostic accuracy, boosts confidence,and enhances inter-observer agreement among pathologists in real clinicalscenarios. These results establish PathSearch as a scalable and generalizableretrieval solution for digital pathology.</description>
      <author>example@mail.com (Hongyi Wang, Zhengjie Zhu, Jiabo Ma, Fang Wang, Yue Shi, Bo Luo, Jili Wang, Qiuyu Cai, Xiuming Zhang, Yen-Wei Chen, Lanfen Lin, Hao Chen)</author>
      <guid isPermaLink="false">2510.23224v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
  <item>
      <title>MATCH: Task-Driven Code Evaluation through Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.23169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MATCH的新型无参考代码评估指标，使用对比学习为代码和自然语言任务描述生成有意义的嵌入，实现相似性评分。&lt;h4&gt;背景&lt;/h4&gt;AI代码生成日益普及，GitHub Copilot估计生成GitHub上46%的代码。准确评估生成代码与开发者意图的匹配度是重大挑战，传统方法如单元测试难以扩展且成本高，语法相似性指标无法捕捉代码功能，而需要参考代码的指标如CodeBERTScore并不总是可用。&lt;h4&gt;目的&lt;/h4&gt;解决无参考代码评估的空白，提供一种不依赖参考代码的代码生成质量评估方法。&lt;h4&gt;方法&lt;/h4&gt;引入MATCH指标，使用对比学习技术为代码和自然语言任务描述生成有意义的嵌入，实现反映生成代码实现任务程度的相似性评分。&lt;h4&gt;主要发现&lt;/h4&gt;MATCH在多种编程语言上实现了比现有指标与功能正确性和人类偏好更强的相关性。&lt;h4&gt;结论&lt;/h4&gt;MATCH是一种有效的无参考代码评估指标，能够更好地评估生成代码与任务意图的匹配度。&lt;h4&gt;翻译&lt;/h4&gt;基于AI的代码生成日益普及，GitHub Copilot估计生成了GitHub上46%的代码。准确评估生成代码与开发者意图的匹配度仍然是一个重大挑战。传统的评估方法，如单元测试，通常难以扩展且成本高昂。语法相似性指标（如BLEU、ROUGE）无法捕捉代码功能，而像CodeBERTScore这样的指标需要参考代码，但参考代码并不总是可用的。为了解决无参考评估的空白，除了ICE-Score等少数替代方案外，本文引入了MATCH，一种新型的无参考指标。MATCH使用对比学习为代码和自然语言任务描述生成有意义的嵌入，实现反映生成代码实现任务程度的相似性评分。我们证明，在多种编程语言上，MATCH比现有指标实现了与功能正确性和人类偏好更强的相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-based code generation is increasingly prevalent, with GitHub Copilotestimated to generate 46% of the code on GitHub. Accurately evaluating how wellgenerated code aligns with developer intent remains a critical challenge.Traditional evaluation methods, such as unit tests, are often unscalable andcostly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture codefunctionality, and metrics like CodeBERTScore require reference code, which isnot always available. To address the gap in reference-free evaluation, with fewalternatives such as ICE-Score, this paper introduces MATCH, a novelreference-free metric. MATCH uses Contrastive Learning to generate meaningfulembeddings for code and natural language task descriptions, enabling similarityscoring that reflects how well generated code implements the task. We show thatMATCH achieves stronger correlations with functional correctness and humanpreference than existing metrics across multiple programming languages.</description>
      <author>example@mail.com (Marah Ghoummaid, Vladimir Tchuiev, Ofek Glick, Michal Moschkovitz, Dotan Di Castro)</author>
      <guid isPermaLink="false">2510.23169v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>VALA: Learning Latent Anchors for Training-Free and Temporally Consistent</title>
      <link>http://arxiv.org/abs/2510.22970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VALA（Variational Alignment for Latent Anchors）变分对齐模块，用于解决现有无需训练的视频编辑方法中帧选择和时间一致性的问题。&lt;h4&gt;背景&lt;/h4&gt;无需训练的视频编辑技术最近取得了进展，利用预训练的文本到图像扩散模型实现了轻量级和精确的跨帧生成。然而，现有方法通常依赖启发式帧选择来维持DDIM反演过程中的时间一致性，这引入了人工偏差并降低了端到端推理的可扩展性。&lt;h4&gt;目的&lt;/h4&gt;开发一种自适应选择关键帧并将它们的潜在特征压缩为语义锚点的方法，以实现一致的视频编辑。&lt;h4&gt;方法&lt;/h4&gt;VALA采用具有对比学习目标的变分框架，将跨帧潜在表示转换为保留内容和时间一致性的压缩潜在锚点。该方法可以完全集成到无需训练的基于文本到图像的视频编辑模型中。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界视频编辑基准上的大量实验表明，VALA在反演保真度、编辑质量和时间一致性方面达到了最先进的性能，同时相比之前的方法提供了更高的效率。&lt;h4&gt;结论&lt;/h4&gt;VALA是一种有效的变分对齐模块，能够解决现有视频编辑方法中帧选择和时间一致性的挑战，提高了视频编辑的质量和效率。&lt;h4&gt;翻译&lt;/h4&gt;最近无需训练的视频编辑技术的进步使得利用预训练的文本到图像扩散模型实现了轻量级和精确的跨帧生成。然而，现有方法通常依赖启发式帧选择来维持DDIM反演过程中的时间一致性，这引入了人工偏差并降低了端到端推理的可扩展性。在本文中，我们提出了VALA（变分锚点对齐），这是一种变分对齐模块，可以自适应选择关键帧并将它们的潜在特征压缩为语义锚点，以实现一致的视频编辑。为了学习有意义的分配，VALA提出了一个具有对比学习目标的变分框架。因此，它可以将跨帧潜在表示转换为保留内容和时间一致性的压缩潜在锚点。我们的方法可以完全集成到无需训练的基于文本到图像的视频编辑模型中。在真实世界视频编辑基准上的大量实验表明，VALA在反演保真度、编辑质量和时间一致性方面达到了最先进的性能，同时相比之前的方法提供了更高的效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in training-free video editing have enabled lightweight andprecise cross-frame generation by leveraging pre-trained text-to-imagediffusion models. However, existing methods often rely on heuristic frameselection to maintain temporal consistency during DDIM inversion, whichintroduces manual bias and reduces the scalability of end-to-end inference. Inthis paper, we propose~\textbf{VALA} (\textbf{V}ariational \textbf{A}lignmentfor \textbf{L}atent \textbf{A}nchors), a variational alignment module thatadaptively selects key frames and compresses their latent features intosemantic anchors for consistent video editing. To learn meaningful assignments,VALA propose a variational framework with a contrastive learning objective.Therefore, it can transform cross-frame latent representations into compressedlatent anchors that preserve both content and temporal coherence. Our methodcan be fully integrated into training-free text-to-image based video editingmodels. Extensive experiments on real-world video editing benchmarks show thatVALA achieves state-of-the-art performance in inversion fidelity, editingquality, and temporal consistency, while offering improved efficiency overprior methods.</description>
      <author>example@mail.com (Zhangkai Wu, Xuhui Fan, Zhongyuan Xie, Kaize Shi, Longbing Cao)</author>
      <guid isPermaLink="false">2510.22970v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Lingual Sponsored Search via Dual-Encoder and Graph Neural Networks for Context-Aware Query Translation in Advertising Platforms</title>
      <link>http://arxiv.org/abs/2510.22957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AdGraphTrans是一种结合图神经网络的双编码器框架，用于广告中的上下文感知查询翻译，显著提高了跨语言搜索广告的效果。&lt;h4&gt;背景&lt;/h4&gt;跨语言搜索广告对全球广告平台至关重要，但传统机器翻译方法无法捕捉查询特定的上下文线索，导致语义歧义，影响点击率和转化率。&lt;h4&gt;目的&lt;/h4&gt;解决传统翻译方法在广告搜索中的局限性，提高跨语言搜索广告的效果。&lt;h4&gt;方法&lt;/h4&gt;提出AdGraphTrans框架，使用多语言Transformer编码器独立编码用户查询和广告内容，将上下文关系建模为异构图，应用图注意力网络改进嵌入，并通过对比学习对齐嵌入以减少翻译歧义。&lt;h4&gt;主要发现&lt;/h4&gt;AdGraphTrans在EN-ZH、EN-ES、EN-FR语言对上实现BLEU得分38.9和语义相似度0.83，优于mBERT和M2M-100基线方法；在下游广告检索任务中提高4.67%点击率和1.72%转化率。&lt;h4&gt;结论&lt;/h4&gt;将基于图的上下文信号与双编码器翻译相结合，为增强广告平台中的跨语言搜索广告提供了强大的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;跨语言搜索广告对全球广告平台至关重要，用户来自不同语言背景并与多语言广告互动。传统机器翻译方法往往无法捕捉查询特定的上下文线索，导致语义歧义，对点击率和转化率产生负面影响。为应对这一挑战，我们提出了AdGraphTrans，一种结合图神经网络的双编码器新框架，用于广告中的上下文感知查询翻译。具体而言，使用多语言Transformer编码器独立编码用户查询和广告内容，并将上下文关系（如共同点击的广告、用户搜索会话和查询-广告共现）建模为异构图。然后应用图注意力网络利用语义和行为上下文改进嵌入。这些嵌入通过对比学习对齐，以减少翻译歧义。在从Google Ads和Amazon Ads收集的跨语言搜索广告数据集（EN-ZH、EN-ES、EN-FR对）上进行的实验表明，AdGraphTrans显著提高了查询翻译质量，实现BLEU得分38.9和语义相似度0.83，优于mBERT和M2M-100等强基线方法。此外，在下游广告检索任务中，AdGraphTrans比基线方法提高了4.67%的点击率和1.72%的转化率。这些结果证实，将基于图的上下文信号与双编码器翻译相结合，为增强广告平台中的跨语言搜索广告提供了强大的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-lingual sponsored search is crucial for global advertising platforms,where users from different language backgrounds interact with multilingual ads.Traditional machine translation methods often fail to capture query-specificcontextual cues, leading to semantic ambiguities that negatively impactclick-through rates (CTR) and conversion rates (CVR). To address thischallenge, we propose AdGraphTrans, a novel dual-encoder framework enhancedwith graph neural networks (GNNs) for context-aware query translation inadvertising. Specifically, user queries and ad contents are independentlyencoded using multilingual Transformer-based encoders (mBERT/XLM-R), andcontextual relations-such as co-clicked ads, user search sessions, and query-adco-occurrence-are modeled as a heterogeneous graph. A graph attention network(GAT) is then applied to refine embeddings by leveraging semantic andbehavioral context. These embeddings are aligned via contrastive learning toreduce translation ambiguity. Experiments conducted on a cross-lingualsponsored search dataset collected from Google Ads and Amazon Ads (EN-ZH,EN-ES, EN-FR pairs) demonstrate that AdGraphTrans significantly improves querytranslation quality, achieving a BLEU score of 38.9 and semantic similarity(cosine score) of 0.83, outperforming strong baselines such as mBERT andM2M-100. Moreover, in downstream ad retrieval tasks, AdGraphTrans yields +4.67%CTR and +1.72% CVR improvements over baseline methods. These results confirmthat incorporating graph-based contextual signals with dual-encoder translationprovides a robust solution for enhancing cross-lingual sponsored search inadvertising platforms.</description>
      <author>example@mail.com (Ziyang Gao, Yuanliang Qu, Yi Han)</author>
      <guid isPermaLink="false">2510.22957v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics</title>
      <link>http://arxiv.org/abs/2510.22937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究挑战了生物特征统计学上不相关的传统假设，证明同一个体的生物特征（特别是虹膜）实际上存在相关性，并使用双编码器网络和深度学习模型验证了这一发现。&lt;h4&gt;背景&lt;/h4&gt;历史上一直假设个体的生物特征在统计学上是不相关的，这一假设需要被检验。&lt;h4&gt;目的&lt;/h4&gt;测试个体生物特征统计学上不相关的假设，通过训练双编码器网络进行三种生物特征验证任务。&lt;h4&gt;方法&lt;/h4&gt;在274名受试者上训练双编码器网络（约10万张指纹图像和7千张虹膜图像），进行三种匹配任务：指纹到指纹匹配、虹膜到虹膜匹配、跨模态指纹到虹膜匹配。使用ResNet-50和Vision Transformer骨干网络构建双编码器架构，最小化来自同一个体的图像之间的对比损失。&lt;h4&gt;主要发现&lt;/h4&gt;虹膜ResNet架构在虹膜到虹膜匹配中达到91的ROC AUC分数，证明个体的左右虹膜是相关的；指纹模型重现了先前工作提出的正样本内相关性；这是首次尝试使用Vision Transformer进行此类匹配；跨模态匹配仅略高于随机水平。&lt;h4&gt;结论&lt;/h4&gt;这些发现继续挑战生物特征的独立性假设，研究计划将这项工作扩展到其他生物特征。&lt;h4&gt;翻译&lt;/h4&gt;历史上一直假设个体的生物特征在统计学上是不相关的。我们通过在三种验证任务上训练双编码器网络来测试这一假设，包括指纹到指纹匹配、虹膜到虹膜匹配，以及使用274名受试者（约10万张指纹和7千张虹膜图像）进行的跨模态指纹到虹膜匹配。我们在双编码器架构中训练了ResNet-50和Vision Transformer骨干网络，以最小化来自同一个体的图像之间的对比损失。虹膜ResNet架构在虹膜到虹膜匹配中达到91的ROC AUC分数，提供了个体左右虹膜相关的明确证据。指纹模型重现了该领域先前工作所提出的正样本内相关性。这是首次尝试使用Vision Transformer进行此类匹配。跨模态匹配仅略高于随机水平，这表明需要更多数据和更复杂的管道才能获得令人信服的结果。这些发现继续挑战生物特征的独立性假设，我们计划将这项工作扩展到其他生物特征。代码可用：https://github.com/MatthewSo/bio_fingerprints_iris。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There has been a historic assumption that the biometrics of an individual arestatistically uncorrelated. We test this assumption by training Bi-Encodernetworks on three verification tasks, including fingerprint-to-fingerprintmatching, iris-to-iris matching, and cross-modal fingerprint-to-iris matchingusing 274 subjects with $\sim$100k fingerprints and 7k iris images. We trainedResNet-50 and Vision Transformer backbones in Bi-Encoder architectures suchthat the contrastive loss between images sampled from the same individual isminimized. The iris ResNet architecture reaches 91 ROC AUC score foriris-to-iris matching, providing clear evidence that the left and right irisesof an individual are correlated. Fingerprint models reproduce the positiveintra-subject suggested by prior work in this space. This is the first workattempting to use Vision Transformers for this matching. Cross-modal matchingrises only slightly above chance, which suggests that more data and a moresophisticated pipeline is needed to obtain compelling results. These findingscontinue challenge independence assumptions of biometrics and we plan to extendthis work to other biometrics in the future. Code available:https://github.com/MatthewSo/bio_fingerprints_iris.</description>
      <author>example@mail.com (Matthew So, Judah Goldfeder, Mark Lis, Hod Lipson)</author>
      <guid isPermaLink="false">2510.22937v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-Preserving Cross-Style Visual Reasoning for Robust Multi-Modal Understanding in Large Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.22838v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SP-CSVR的新型框架，用于解决大型视觉-语言模型中的'风格陷阱'问题，实现稳定语义理解和跨风格视觉推理。&lt;h4&gt;背景&lt;/h4&gt;'风格陷阱'阻碍了大型视觉-语言模型在不同视觉风格下的稳健语义理解，特别是在上下文学习中。现有方法难以有效解耦风格与内容，限制了模型的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实现稳定语义理解和自适应跨风格视觉推理的框架，以克服风格陷阱问题。&lt;h4&gt;方法&lt;/h4&gt;提出语义保持跨风格视觉推理器(SP-CSVR)，包含三个核心组件：跨风格特征编码器(CSFE)用于风格-内容解耦；语义对齐的上下文解码器(SAICD)用于高效的少样本风格适应；自适应语义一致性模块(ASCM)采用多任务对比学习强制跨风格语义不变性。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的多风格数据集上，SP-CSVR在视觉描述、视觉问答和上下文风格适应方面达到了最先进的性能。消融研究和泛化分析证实了该方法在增强稳健性、泛化能力和效率方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;SP-CSVR成功解决了大型视觉-语言模型中的风格陷阱问题，实现了跨风格的稳定语义理解和高效推理。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为'语义保持跨风格视觉推理器'(SP-CSVR)的新型框架，旨在解决大型视觉-语言模型(LVLMs)中的'风格陷阱'问题，从而实现稳定的语义理解和自适应的跨风格视觉推理。SP-CSVR集成了跨风格特征编码器(CSFE)用于风格-内容解耦，语义对齐的上下文解码器(SAICD)用于高效的少样本风格适应，以及采用多任务对比学习的自适应语义一致性模块(ASCM)来强制跨风格语义不变性。在具有挑战性的多风格数据集上的广泛实验表明，SP-CSVR在视觉描述、视觉问答和上下文风格适应方面达到了最先进的性能。包括消融研究和泛化分析在内的全面评估证实了SP-CSVR在增强稳健性、泛化能力和效率方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The "style trap" poses a significant challenge for Large Vision-LanguageModels (LVLMs), hindering robust semantic understanding across diverse visualstyles, especially in in-context learning (ICL). Existing methods often fail toeffectively decouple style from content, hindering generalization. To addressthis, we propose the Semantic-Preserving Cross-Style Visual Reasoner (SP-CSVR),a novel framework for stable semantic understanding and adaptive cross-stylevisual reasoning. SP-CSVR integrates a Cross-Style Feature Encoder (CSFE) forstyle-content disentanglement, a Semantic-Aligned In-Context Decoder (SAICD)for efficient few-shot style adaptation, and an Adaptive Semantic ConsistencyModule (ASCM) employing multi-task contrastive learning to enforce cross-stylesemantic invariance. Extensive experiments on a challenging multi-style datasetdemonstrate SP-CSVR's state-of-the-art performance across visual captioning,visual question answering, and in-context style adaptation. Comprehensiveevaluations, including ablation studies and generalization analysis, confirmSP-CSVR's efficacy in enhancing robustness, generalization, and efficiencyacross diverse visual styles.</description>
      <author>example@mail.com (Aya Nakayama, Brian Wong, Yuji Nishimura, Kaito Tanaka)</author>
      <guid isPermaLink="false">2510.22838v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2510.22706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/lifuguan/IGGT_official&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了InstanceGrounded Geometry Transformer (IGGT)，一个端到端的大型统一transformer，用于统一空间重建和实例级上下文理解的知识，并通过3D一致的对比学习策略仅使用2D视觉输入编码具有几何结构和基于实例聚类的统一表示。&lt;h4&gt;背景&lt;/h4&gt;人类自然地将3D世界的几何结构和语义内容视为相互交织的维度，但大多数先前方法优先训练大型几何模型进行低级3D重建，将高级空间理解孤立处理，忽视了这两个基本方面间的关键相互作用，导致下游3D理解任务表现不佳。最近的尝试通过简单对齐3D模型与特定语言模型来缓解问题，但限制了感知能力和下游任务适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够统一几何结构和语义理解的模型，改善3D场景的理解和重建能力，提高在下游任务中的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;设计了IGGT模型和3D一致的对比学习策略，指导模型仅通过2D视觉输入编码具有几何结构和基于实例聚类的统一表示。同时构建了InsScene-15K数据集，包含高质量的RGB图像、姿态、深度图和3D一致的实例级掩码注释。&lt;h4&gt;主要发现&lt;/h4&gt;通过统一几何结构和语义理解，可以有效地将2D视觉输入一致提升到具有明显不同对象实例的连贯3D场景，改善3D场景的理解和重建。&lt;h4&gt;结论&lt;/h4&gt;IGGT模型和InsScene-15K数据集能够有效解决3D场景分析中几何结构和语义理解分离的问题，提高下游3D理解任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;人类自然地将3D世界的几何结构和语义内容视为交织的维度，使能够连贯准确地理解复杂场景。然而，大多数先前方法优先训练大型几何模型进行低级3D重建，并将高级空间理解孤立处理，忽视了这两个3D场景分析基本方面之间的关键相互作用，从而限制了泛化能力，导致在下游3D理解任务中表现不佳。最近的尝试通过简单地将3D模型与特定语言模型对齐来缓解这一问题，但这限制了感知能力，并限制了下游任务的适应性。在本文中，我们提出了InstanceGrounded Geometry Transformer (IGGT)，一个端到端的大型统一transformer，用于统一空间重建和实例级上下文理解的知识。具体来说，我们设计了一种3D一致的对比学习策略，指导IGGT仅通过2D视觉输入编码具有几何结构和基于实例聚类的统一表示。这种表示支持将2D视觉输入一致提升到具有明显不同对象实例的连贯3D场景。为了促进这一任务，我们进一步构建了InsScene-15K，一个包含高质量RGB图像、姿态、深度图和3D一致的实例级掩码注释的大规模数据集，采用了新颖的数据整理流程。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D几何重建和高级语义理解被分离处理的问题。人类自然将几何结构和语义内容作为交织维度理解3D世界，但当前方法要么优先处理低级几何重建而忽视高级语义理解，要么简单将几何与特定语言模型对齐。这种分离限制了模型泛化能力，导致在下游3D理解任务中表现不佳。解决这个问题对实现接近人类的空间智能理解至关重要，对机器人操作、AR/VR和空间规划等应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到人类自然地将几何结构和语义内容作为交织维度理解3D世界，然后指出当前方法的局限性：将几何重建和语义理解分离，或者简单地将几何与语言模型对齐。他们设计了一种新思路：通过联合训练将几何和实例级语义特征耦合，让模型自主学习3D实例级语义与其几何结构之间的关系。该方法借鉴了VGGT的Transformer架构、DINOv2的图像特征提取、DPT的密集预测架构以及SAM2的视频对象分割技术，并在实例空间跟踪中受到SAMPart3D的启发。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合训练将几何结构和实例级语义特征耦合，使模型能够自主学习3D实例级语义与其几何结构之间的关系，并使用实例掩码作为桥梁连接统一表示与各种视觉语言模型。整体流程为：1)输入多张RGB图像；2)使用大型统一Transformer编码为统一标记表示；3)通过几何头部和实例头部解码生成几何点图和实例聚类场；4)使用跨模态融合块增强实例特征的空间感知；5)应用3D一致的对比学习策略确保跨视图一致性；6)使用无监督聚类生成实例掩码；7)这些掩码引导视觉语言模型执行下游任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出IGGT框架，统一几何重建和语义理解；2)设计3D一致的对比学习策略；3)构建InsScene-15K数据集；4)提出实例级场景理解范式。相比之前工作，不同之处在于：不再将几何和语义分离处理，而是通过联合训练实现相互增强；不再绑定特定语言模型，而是通过实例掩码灵活集成各种视觉语言模型；能区分同一类别内的不同对象实例；实现更好的多视图一致性，特别是在大视角变化和复杂场景中。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; IGGT通过统一几何重建和语义理解，并引入实例级场景理解范式，实现了高质量、一致的语义3D重建，并能灵活支持多种下游应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans naturally perceive the geometric structure and semantic content of a3D world as intertwined dimensions, enabling coherent and accurateunderstanding of complex scenes. However, most prior approaches prioritizetraining large geometry models for low-level 3D reconstruction and treathigh-level spatial understanding in isolation, overlooking the crucialinterplay between these two fundamental aspects of 3D-scene analysis, therebylimiting generalization and leading to poor performance in downstream 3Dunderstanding tasks. Recent attempts have mitigated this issue by simplyaligning 3D models with specific language models, thus restricting perceptionto the aligned model's capacity and limiting adaptability to downstream tasks.In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), anend-to-end large unified transformer to unify the knowledge for both spatialreconstruction and instance-level contextual understanding. Specifically, wedesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encodea unified representation with geometric structures and instance-groundedclustering through only 2D visual inputs. This representation supportsconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitlydistinct object instances. To facilitate this task, we further constructInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depthmaps, and 3D-consistent instance-level mask annotations with a novel datacuration pipeline.</description>
      <author>example@mail.com (Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, Manyuan Zhang, Gang Yu, Dingwen Zhang, Ziwei Liu)</author>
      <guid isPermaLink="false">2510.22706v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>CLEANet: Robust and Efficient Anomaly Detection in Contaminated Multivariate Time Series</title>
      <link>http://arxiv.org/abs/2510.22619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为CLEANet的稳健高效的多变量时间序列异常检测框架，用于解决训练数据污染和模型推理效率低下的问题。CLEANet通过抗污染训练框架和轻量级共轭MLP设计，显著提高了检测性能并降低了计算成本。&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列异常检测对于维护工业系统可靠性至关重要，但现实部署面临两大挑战：训练数据污染（噪声和隐藏异常）和低效的模型推理。现有无监督方法假设训练数据干净，但污染会扭曲学习模式并降低检测准确性；同时，复杂深度模型容易过度拟合到污染数据上且延迟高，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种稳健且高效的多变量时间序列异常检测框架，能够有效处理训练数据污染问题，避免模型过度拟合，并提高计算效率，从而提升异常检测的准确性和实用性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了CLEANet框架，包含两个核心组件：1) 抗污染训练框架(CRTF)，通过自适应重建权重策略结合聚类引导的对比学习减轻污染样本影响；2) 轻量级共轭MLP，用于分离时间跨特征依赖关系，避免过度拟合并提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;在五个公共数据集上，CLEANet比十个最先进的基线方法实现了高达73.04%的F1提升和81.28%的运行时间减少。此外，将CRTF集成到三个先进模型中平均获得5.35%的F1提升，证明了其良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CLEANet框架有效解决了多变量时间序列异常检测中的训练数据污染和模型推理效率问题，通过抗污染训练策略和轻量级模型设计显著提升了检测性能和计算效率，具有良好的实用价值和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;多变量时间序列异常检测对于维护工业系统可靠性至关重要，但现实部署受到两个关键挑战的阻碍：训练数据污染（噪声和隐藏异常）和低效的模型推理。现有无监督方法假设训练数据干净，但污染会扭曲学习模式并降低检测准确性。同时，复杂深度模型往往过度拟合到污染数据上并遭受高延迟，限制了实际应用。为解决这些挑战，我们提出了CLEANet，一个在受污染多变量时间序列中稳健且高效的异常检测框架。CLEANet引入了抗污染训练框架(CRTF)，通过自适应重建权重策略结合聚类引导的对比学习减轻污染样本的影响，从而增强稳健性。为进一步避免在污染数据上过度拟合并提高计算效率，我们设计了一个轻量级共轭MLP，用于分离时间跨特征依赖关系。在五个公共数据集上，CLEANet比十个最先进的基线方法实现了高达73.04%的F1提升和81.28%的运行时间减少。此外，将CRTF集成到三个先进模型中平均获得5.35%的F1提升，证实了其强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series (MTS) anomaly detection is essential for maintainingthe reliability of industrial systems, yet real-world deployment is hindered bytwo critical challenges: training data contamination (noises and hiddenanomalies) and inefficient model inference. Existing unsupervised methodsassume clean training data, but contamination distorts learned patterns anddegrades detection accuracy. Meanwhile, complex deep models often overfit tocontamination and suffer from high latency, limiting practical use. To addressthese challenges, we propose CLEANet, a robust and efficient anomaly detectionframework in contaminated multivariate time series. CLEANet introduces aContamination-Resilient Training Framework (CRTF) that mitigates the impact ofcorrupted samples through an adaptive reconstruction weighting strategycombined with clustering-guided contrastive learning, thereby enhancingrobustness. To further avoid overfitting on contaminated data and improvecomputational efficiency, we design a lightweight conjugate MLP thatdisentangles temporal and cross-feature dependencies. Across five publicdatasets, CLEANet achieves up to 73.04% higher F1 and 81.28% lower runtimecompared with ten state-of-the-art baselines. Furthermore, integrating CRTFinto three advanced models yields an average 5.35% F1 gain, confirming itsstrong generalizability.</description>
      <author>example@mail.com (Songhan Zhang, Yuanhao Lai, Pengfei Zheng, Boxi Yu, Xiaoying Tang, Qiuai Fu, Pinjia He)</author>
      <guid isPermaLink="false">2510.22619v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers</title>
      <link>http://arxiv.org/abs/2510.22555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CP-GBA(Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers)，一种新的可转移图后门攻击方法，通过图提示学习训练通用子图触发器，实现了跨学习范式的有效攻击。&lt;h4&gt;背景&lt;/h4&gt;图神经网络易受后门攻击，现有触发器生成器结构简单，过度依赖特定特征，局限于单一图学习范式（如图监督学习、图对比学习或图提示学习），导致跨范式转移性差，无法充分利用图数据的复杂结构信息和节点多样性。&lt;h4&gt;目的&lt;/h4&gt;解决现有触发器生成器的局限性，提高攻击成功率，开发一种能在多种学习范式中有效工作的可转移图后门攻击方法。&lt;h4&gt;方法&lt;/h4&gt;提出CP-GBA方法，首先从目标图中提炼出紧凑且具有表达力的触发器集合，通过联合强制类感知性、特征丰富度和结构保真度来实现；其次探索了GPL在基于提示的目标下训练这些触发器的理论可转移性，使其能泛化到多样化和未见过的测试时范式。&lt;h4&gt;主要发现&lt;/h4&gt;CP-GBA在多个真实数据集和防御场景中实现了最先进的攻击成功率，证明了其在不同学习范式间的有效泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CP-GBA通过利用图提示学习训练通用子图触发器，解决了现有触发器生成器的局限性，提高了攻击成功率，为图神经网络的后门攻击研究提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)容易受到后门攻击，攻击者植入恶意触发器来操纵模型预测。现有的触发器生成器通常结构简单，过度依赖特定特征，将其限制在单一图学习范式中。这种专门化设计导致触发器在应用于其他学习范式时转移性差。此外，这些简单生成器通常无法利用图数据中的复杂结构信息或节点多样性，限制了攻击成功率。因此，我们提出了CP-GBA，采用图提示学习训练一组通用子图触发器，通过提炼紧凑且具有表达力的触发器集合和探索理论可转移性，实现了在多种学习范式中的有效攻击，并在多个数据集和防御场景中取得了最先进的攻击成功率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks(GNNs) are vulnerable to backdoor attacks, whereadversaries implant malicious triggers to manipulate model predictions.  Existing trigger generators are often simplistic in structure and overlyreliant on specific features, confining them to a single graph learningparadigm, such as graph supervised learning, graph contrastive learning, orgraph prompt learning.  This specialized design, which aligns the trigger with one learningobjective, results in poor transferability when applied to other learningparadigms.  For instance, triggers generated for the graph supervised learning paradigmperform poorly when tested within graph contrastive learning or graph promptlearning environments.  Furthermore, these simple generators often fail to utilize complex structuralinformation or node diversity within the graph data.  These constraints limit the attack success rates of such methods in generaltesting scenarios.  Therefore, to address these limitations, we propose Cross-Paradigm GraphBackdoor Attacks with Promptable Subgraph Triggers(CP-GBA), a new transferablegraph backdoor attack that employs graph prompt learning(GPL) to train a set ofuniversal subgraph triggers.  First, we distill a compact yet expressive trigger set from target graphs,which is structured as a queryable repository, by jointly enforcingclass-awareness, feature richness, and structural fidelity.  Second, we conduct the first exploration of the theoretical transferabilityof GPL to train these triggers under prompt-based objectives, enablingeffective generalization to diverse and unseen test-time paradigms.  Extensive experiments across multiple real-world datasets and defensescenarios show that CP-GBA achieves state-of-the-art attack success rates.</description>
      <author>example@mail.com (Dongyi Liu, Jiangtong Li, Dawei Cheng, Changjun Jiang)</author>
      <guid isPermaLink="false">2510.22555v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding</title>
      <link>http://arxiv.org/abs/2510.22264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了PatenTEB，一个全面的专利文本嵌入基准测试，包含15个任务，跨越检索、分类、释义和聚类领域，共206万个示例。同时开发了patembed模型家族，通过多任务训练，参数量从67M到344M不等，上下文长度最高可达4096个token。外部验证显示patembed-base在MTEB BigPatentClustering.v2上达到最先进水平（0.494 V-measure vs. 之前的0.445最佳），而patembed-large在DAPFAM上达到0.377 NDCG@100。&lt;h4&gt;背景&lt;/h4&gt;专利文本嵌入能够实现现有技术搜索、技术景观分析和专利分析，但现有的基准测试无法充分捕捉专利特有的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够更好地捕捉专利特定挑战的基准测试和模型，以提高专利文本嵌入的性能和适用性。&lt;h4&gt;方法&lt;/h4&gt;1. 创建PatenTEB基准测试，包含15个任务，跨越检索、分类、释义和聚类领域；2. 使用领域分层分割、领域特定硬负挖掘和系统覆盖不对称片段到文档匹配场景；3. 开发patembed模型家族，通过多任务训练，参数量从67M到344M；4. 使用领域预训练初始化。&lt;h4&gt;主要发现&lt;/h4&gt;1. 多任务训练提高了外部泛化能力，尽管对基准测试有轻微影响；2. 领域预训练初始化在任务家族中提供了持续的优势；3. patembed-base在MTEB BigPatentClustering.v2上达到最先进水平（0.494 V-measure）；4. patembed-large在DAPFAM上达到0.377 NDCG@100。&lt;h4&gt;结论&lt;/h4&gt;PatenTEB基准测试和patembed模型家族能够有效解决专利文本嵌入中的特定挑战，并通过多任务训练和领域预训练初始化实现了更好的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;专利检索、句子嵌入、多任务学习、不对称检索、基准测试评估、对比学习&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patent text embeddings enable prior art search, technology landscaping, andpatent analysis, yet existing benchmarks inadequately capture patent-specificchallenges. We introduce PatenTEB, a comprehensive benchmark comprising 15tasks across retrieval, classification, paraphrase, and clustering, with 2.06million examples. PatenTEB employs domain-stratified splits, domain specifichard negative mining, and systematic coverage of asymmetricfragment-to-document matching scenarios absent from general embeddingbenchmarks. We develop the patembed model family through multi-task training,spanning 67M to 344M parameters with context lengths up to 4096 tokens.External validation shows strong generalization: patembed-base achievesstate-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.Systematic ablations reveal that multi-task training improves externalgeneralization despite minor benchmark costs, and that domain-pretrainedinitialization provides consistent advantages across task families. Allresources will be made available at https://github.com/iliass-y/patenteb.Keywords: patent retrieval, sentence embeddings, multi-task learning,asymmetric retrieval, benchmark evaluation, contrastive learning.</description>
      <author>example@mail.com (Iliass Ayaou, Denis Cavallucci)</author>
      <guid isPermaLink="false">2510.22264v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Attention Residual Fusion Network with Contrast for Source-free Domain Adaptation</title>
      <link>http://arxiv.org/abs/2510.22142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于对比学习的注意力残差融合网络(ARFNet)框架，用于解决源域无适应(SFDA)中的负迁移和域偏移问题。&lt;h4&gt;背景&lt;/h4&gt;源域无适应(SFDA)是在源域训练模型后应用于相关目标域，但适应过程中无法访问源数据和标签的任务。场景信息复杂和缺乏源域数据使SFDA成为一项困难任务。现有研究虽取得一定成果，但许多方法只关注域偏移而忽略负迁移的影响。&lt;h4&gt;目的&lt;/h4&gt;解决SFDA中的负迁移和域偏移问题，提高模型在适应过程中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出ARFNet框架，利用三种技术：1)注意力机制捕获目标对象的判别区域；2)将注意力特征分解为空间和通道注意力，实现跨层注意力残差融合和自蒸馏；3)对比全局和局部表示，提高类别感知能力；4)动态质心评估策略评估可信质心和标签，用于自监督自蒸馏，减轻域偏移。&lt;h4&gt;主要发现&lt;/h4&gt;在五个不同规模的基准测试上进行的综合实验表明，该方法优于其他技术，在SFDA基准测试中取得了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;ARFNet框架有效解决了SFDA中的负迁移和域偏移问题，在多个基准测试上证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;源域无适应(SFDA)涉及在源域训练模型，然后将其应用于相关目标域，但在适应过程中无法访问源数据和标签。场景信息复杂和缺乏源域数据使SFDA成为一项困难任务。最近研究显示出有希望的结果，但许多域适应方法集中在域偏移上，而忽略了负迁移的影响，这可能阻碍模型在适应过程中的性能提升。在本文中，针对这一问题，我们提出了一个基于对比学习的注意力残差融合网络(ARFNet)框架，用于SFDA，以减轻适应过程中的负迁移和域偏移，其中利用了注意力残差融合、全局-局部注意力对比和动态质心评估。具体来说，首先利用注意力机制捕获目标对象的判别区域。然后，在每个块中，注意力特征被分解为空间注意力和通道注意力，以逐步实现跨层注意力残差融合和自蒸馏。在适应过程中，我们对比全局和局部表示，以提高不同类别的感知能力，使模型能够区分类内和类间变化。最后，利用动态质心评估策略评估可信质心和标签，用于自监督自蒸馏，旨在准确近似源域中心和伪标签，以减轻域偏移。为了验证有效性，我们在五个不同规模的基准上进行了综合实验。实验结果表明，我们的方法优于其他技术，在SFDA基准测试中取得了优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TCSVT.2025.3626247&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Source-free domain adaptation (SFDA) involves training a model on sourcedomain and then applying it to a related target domain without access to thesource data and labels during adaptation. The complexity of scene informationand lack of the source domain make SFDA a difficult task. Recent studies haveshown promising results, but many approaches to domain adaptation concentrateon domain shift and neglect the effects of negative transfer, which may impedeenhancements of model performance during adaptation. n this paper, addressingthis issue, we propose a novel framework of Attention Residual Fusion Network(ARFNet) based on contrast learning for SFDA to alleviate negative transfer anddomain shift during the progress of adaptation, in which attention residualfusion, global-local attention contrast, and dynamic centroid evaluation areexploited. Concretely, the attention mechanism is first exploited to capturethe discriminative region of the target object. Then, in each block, attentionfeatures are decomposed into spatial-wise and channel-wise attentions toachieve the cross-layer attention residual fusion progressively andself-distillation. During adaptation progress, we contrast global and localrepresentations to improve the perceptual capabilities of different categories,which enables the model to discriminate variations between inner-class andintra-class. Finally, a dynamic centroid evaluation strategy is exploited toevaluate the trustworthy centroids and labels for self-supervisedself-distillation, which aims to accurately approximate the center of thesource domain and pseudo-labels to mitigate domain shift. To validate theefficacy, we execute comprehensive experiments on five benchmarks of varyingscales. Experimental outcomes indicate that our method surpasses othertechniques, attaining superior performance across SFDA benchmarks.</description>
      <author>example@mail.com (Renrong Shao, Wei Zhang, Jun Wang)</author>
      <guid isPermaLink="false">2510.22142v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction</title>
      <link>http://arxiv.org/abs/2510.22141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了LOC框架，一种用于3D场景理解的视觉语言模型方法，通过密集对比学习增强开放集识别能力。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在开放集挑战中取得了显著进展，但3D数据集的有限可用性限制了它们在3D场景理解中的有效应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的语言引导框架，适应各种占据网络，支持监督和自监督学习范式，以改善VLMs在3D场景理解中的应用。&lt;h4&gt;方法&lt;/h4&gt;提出LOC框架，融合多帧LiDAR点，使用泊松重建填补空洞，通过KNN分配体素语义，引入DCL缓解特征过度同质化，预测嵌入CLIP特征空间的密集体素特征。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验表明，该方法对已知类别实现了高精度预测，能够区分未知类别而无需额外训练数据。&lt;h4&gt;结论&lt;/h4&gt;LOC框架有效解决了VLMs在3D场景理解中的应用限制，通过密集对比学习增强了开放集识别能力，同时支持监督和自监督学习。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在开放集挑战中已显示出显著进展。然而，3D数据集的有限可用性阻碍了它们在3D场景理解中的有效应用。我们提出了LOC，一个通用的语言引导框架，可适应各种占据网络，支持监督和自监督学习范式。对于自监督任务，我们采用了一种融合多帧LiDAR点以处理动态/静态场景的策略，使用泊松重建填补空洞，并通过K近邻为体素分配语义，以获得全面的体素表示。为了缓解直接高维特征蒸馏导致的特征过度同质化问题，我们引入了密集对比学习。DCL利用密集体素语义信息和预定义的文本提示，有效增强了开放集识别能力，无需密集像素级监督，我们的框架还可以利用现有真实数据进一步改善性能。我们的模型预测嵌入在CLIP特征空间中的密集体素特征，整合文本和图像像素信息，并基于文本和语义相似性进行分类。在nuScenes数据集上的实验证明了该方法的优越性能，对已知类别实现了高精度预测，并能区分未知类别而无需额外训练数据。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决开放集3D占用预测问题，即让模型能够识别训练数据中未包含的新物体类别。这个问题在自动驾驶等领域非常重要，因为现实世界中物体种类繁多，训练数据无法覆盖所有可能的物体类别，系统需要能够识别未知物体以确保安全。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D占用预测方法的局限性，即它们只能识别已知类别。然后借鉴了视觉语言模型(如CLIP)的知识，利用它们在大量图像-文本对上训练的优势。同时采用了多帧LiDAR点云融合、Poisson重建和KNN等技术来处理3D数据的稀疏性问题。最终设计了LOC框架，结合了监督学习和自监督学习，并通过密集对比学习解决了特征过度同质化的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练的视觉语言模型的丰富语义知识，通过密集对比学习将2D空间的知识转移到3D空间，增强模型对未知类别的识别能力。整体流程包括：1)将2D图像特征投影到3D体素空间；2)使用占用头预测体素状态；3)通过语言头将体素特征映射到文本嵌入空间；4)应用鲁棒密集化策略生成密集3D表示；5)使用密集对比学习对齐体素特征与文本提示；6)结合两个头的输出实现开放集预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)LOC框架，首个用于开放集3D占用预测的语言引导框架；2)密集对比学习(DCL)，有效增强开放集识别能力并避免特征过度同质化；3)鲁棒密集化策略，生成高质量密集3D占用表示。相比之前的工作，LOC能够同时处理已知和未知类别，而传统方法只能识别已知类别；LOC避免了直接高维特征蒸馏的问题；LOC支持监督和自监督学习，能更好地利用有限标注数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LOC是一个通用的语言引导框架，通过密集对比学习将2D视觉语言模型的知识有效转移到3D空间，实现了对已知类别的高精度预测和对未知类别的有效区分，无需额外训练数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have shown significant progress in open-setchallenges. However, the limited availability of 3D datasets hinders theireffective application in 3D scene understanding. We propose LOC, a generallanguage-guided framework adaptable to various occupancy networks, supportingboth supervised and self-supervised learning paradigms. For self-supervisedtasks, we employ a strategy that fuses multi-frame LiDAR points fordynamic/static scenes, using Poisson reconstruction to fill voids, andassigning semantics to voxels via K-Nearest Neighbor (KNN) to obtaincomprehensive voxel representations. To mitigate feature over-homogenizationcaused by direct high-dimensional feature distillation, we introduce DenselyContrastive Learning (DCL). DCL leverages dense voxel semantic information andpredefined textual prompts. This efficiently enhances open-set recognitionwithout dense pixel-level supervision, and our framework can also leverageexisting ground truth to further improve performance. Our model predicts densevoxel features embedded in the CLIP feature space, integrating textual andimage pixel information, and classifies based on text and semantic similarity.Experiments on the nuScenes dataset demonstrate the method's superiorperformance, achieving high-precision predictions for known classes anddistinguishing unknown classes without additional training data.</description>
      <author>example@mail.com (Yuhang Gao, Xiang Xiang, Sheng Zhong, Guoyou Wang)</author>
      <guid isPermaLink="false">2510.22141v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.21957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper was accepted in the 2025 IEEE International Conference on  Computer Design (ICCD)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合自监督对比学习和神经架构搜索的框架，用于解决勒索软件检测中的三大局限性，实现了更高的检测准确性和更快的响应时间。&lt;h4&gt;背景&lt;/h4&gt;勒索软件已成为网络安全的严重威胁，因其快速演变、需要早期检测且多样性增加，对传统检测方法构成重大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有AI勒索软件检测方法的三大局限性：特征依赖性、响应延迟和对未知变种的适应性有限。&lt;h4&gt;方法&lt;/h4&gt;提出一个结合自监督对比学习和神经架构搜索的框架，具体包括：(1)设计结合硬件性能计数器的对比学习框架分析勒索软件运行时行为；(2)引入自定义损失函数实现早期检测减少延迟；(3)部署神经架构搜索框架自动构建自适应模型架构。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与现有方法相比，提出的方法在检测准确性上提升高达16.1%，响应时间改善高达6倍，并在规避攻击下保持鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在检测准确性、响应时间和鲁棒性方面均优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;勒索软件由于其快速演变、早期检测的必要性和日益增长的多样性，已成为网络安全的关键威胁，对传统检测方法构成了重大挑战。虽然先前的研究提出了基于人工智能的方法来辅助勒索软件检测，但现有方法存在三个主要局限性：特定的特征依赖性、响应延迟以及对未见变种的适应性有限。在本文中，我们提出了一种结合自监督对比学习和神经架构搜索(NAS)的框架来解决这些挑战。具体来说，本文提供了三个重要贡献：(1)我们设计了一个结合硬件性能计数器(HPC)的对比学习框架，用于分析目标勒索软件的运行时行为。(2)我们引入了一个自定义的损失函数，鼓励对恶意活动的早期检测，并显著减少了检测延迟。(3)我们部署了一个神经架构搜索(NAS)框架，自动构建自适应的模型架构，使检测器能够灵活地与未见的勒索软件变种保持一致。实验结果表明，与现有方法相比，我们提出的方法在检测准确性(高达16.1%)和响应时间(高达6倍)方面都有显著提高，同时在规避攻击下保持鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ransomware has become a critical threat to cybersecurity due to its rapidevolution, the necessity for early detection, and growing diversity, posingsignificant challenges to traditional detection methods. While AI-basedapproaches had been proposed by prior works to assist ransomware detection,existing methods suffer from three major limitations, ad-hoc featuredependencies, delayed response, and limited adaptability to unseen variants. Inthis paper, we propose a framework that integrates self-supervised contrastivelearning with neural architecture search (NAS) to address these challenges.Specifically, this paper offers three important contributions. (1) We design acontrastive learning framework that incorporates hardware performance counters(HPC) to analyze the runtime behavior of target ransomware. (2) We introduce acustomized loss function that encourages early-stage detection of maliciousactivity, and significantly reduces the detection latency. (3) We deploy aneural architecture search (NAS) framework to automatically construct adaptivemodel architectures, allowing the detector to flexibly align with unseenransomware variants. Experimental results show that our proposed methodachieves significant improvements in both detection accuracy (up to 16.1%) andresponse time (up to 6x) compared to existing approaches while maintainingrobustness under evasive attacks.</description>
      <author>example@mail.com (Zhixin Pan, Ziyu Shu, Amberbir Alemayoh)</author>
      <guid isPermaLink="false">2510.21957v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Learning Neural Observer-Predictor Models for Limb-level Sampling-based Locomotion Planning</title>
      <link>http://arxiv.org/abs/2510.22789v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于学习的观察器-预测器框架，用于准确预测足式机器人的全身运动，解决了简化运动学模型无法捕捉复杂闭环动力学的问题。该系统通过神经观察器提供可靠状态估计，并使用高效预测器评估潜在轨迹，在四足机器人上成功实现了肢体感知的运动规划。&lt;h4&gt;背景&lt;/h4&gt;准确的全身运动预测对足式机器人的安全自主导航至关重要，特别是在复杂环境中进行肢体级碰撞检查。简化的运动学模型往往无法捕捉机器人和其底层控制器的复杂闭环动力学，导致预测仅限于简单的平面运动。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测足式机器人复杂全身运动的框架，克服传统简化模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出一个基于学习的观察器-预测器框架，包含：1)具有可证明UUB保证的神经观察器，从本体感觉测量历史中提供可靠的潜在状态估计；2)计算高效的预测器，能够快速并行评估数千条潜在轨迹；3)将系统集成到基于MPPI的规划器中，并在Vision 60四足机器人上进行了硬件实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;硬件实验成功展示了系统在具有挑战性的狭窄通道和小物体上的有效肢体感知运动规划能力，证明该系统为动态机器人平台上的高性能、碰撞感知规划提供了稳健基础。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于学习的观察器-预测器框架能够准确预测足式机器人的全身运动，为复杂环境中的安全自主导航和碰撞感知规划提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的全身运动预测对足式机器人的安全自主导航至关重要，能够实现如杂乱环境中肢体级碰撞检查等关键功能。简化的运动学模型往往无法捕捉机器人和其底层控制器的复杂闭环动力学，限制了它们仅能预测简单的平面运动。为此，我们提出了一种基于学习的观察器-预测器框架，能够准确预测这种运动。我们的方法特点是一个具有可证明UUB保证的神经观察器，它从本体感觉测量历史中提供可靠的潜在状态估计。这个稳定的估计初始化了一个计算高效的预测器，专为现代采样规划器所需的大量潜在轨迹的快速并行评估而设计。我们通过将神经预测器集成到Vision 60四足机器人的基于MPPI的规划器中验证了该系统。硬件实验成功展示了在具有挑战性的狭窄通道和小物体上的有效肢体感知运动规划，突显了我们系统为动态机器人平台上的高性能、碰撞感知规划提供稳健基础的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate full-body motion prediction is essential for the safe, autonomousnavigation of legged robots, enabling critical capabilities like limb-levelcollision checking in cluttered environments. Simplified kinematic models oftenfail to capture the complex, closed-loop dynamics of the robot and itslow-level controller, limiting their predictions to simple planar motion. Toaddress this, we present a learning-based observer-predictor framework thataccurately predicts this motion. Our method features a neural observer withprovable UUB guarantees that provides a reliable latent state estimate from ahistory of proprioceptive measurements. This stable estimate initializes acomputationally efficient predictor, designed for the rapid, parallelevaluation of thousands of potential trajectories required by modernsampling-based planners. We validated the system by integrating our neuralpredictor into an MPPI-based planner on a Vision 60 quadruped. Hardwareexperiments successfully demonstrated effective, limb-aware motion planning ina challenging, narrow passage and over small objects, highlighting our system'sability to provide a robust foundation for high-performance, collision-awareplanning on dynamic robotic platforms.</description>
      <author>example@mail.com (Abhijeet M. Kulkarni, Ioannis Poulakakis, Guoquan Huang)</author>
      <guid isPermaLink="false">2510.22789v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing</title>
      <link>http://arxiv.org/abs/2510.19661v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AgentSense是一种混合的、无需训练的框架，将大型语言模型集成到参与式城市感知中，通过多智能体进化系统适应动态城市条件，提供自然语言解释以提高透明度。&lt;h4&gt;背景&lt;/h4&gt;基于网络的参与式城市感知已成为现代城市管理的重要方法，利用移动个体作为分布式传感器。然而，现有系统在跨不同城市场景的泛化能力有限，且在决策过程中可解释性差。&lt;h4&gt;目的&lt;/h4&gt;解决现有城市感知系统的局限性，提高系统的泛化能力和可解释性。&lt;h4&gt;方法&lt;/h4&gt;AgentSense框架将大型语言模型集成到参与式城市感知中，通过多智能体进化系统实现。它首先使用经典规划器生成基线解决方案，然后迭代改进这些解决方案以适应动态城市条件和异构工人偏好，同时生成自然语言解释以提高透明度和信任度。&lt;h4&gt;主要发现&lt;/h4&gt;在两个大规模移动数据集和七种动态干扰的广泛实验中，AgentSense在适应性和可解释性方面明显优于传统方法。与单一智能体LLM基线相比，该方法在性能和鲁棒性方面表现更好，并提供更合理和透明的解释。&lt;h4&gt;结论&lt;/h4&gt;AgentSense是部署自适应和可解释的网络城市感知系统的重要进展。&lt;h4&gt;翻译&lt;/h4&gt;基于网络的参与式城市感知已通过利用移动个体作为分布式传感器，成为现代城市管理的重要方法。然而，现有的城市感知系统在跨不同城市场景的泛化能力有限，且在决策过程中的可解释性较差。在这项工作中，我们介绍了AgentSense，这是一种混合的、无需训练的框架，通过多智能体进化系统将大型语言模型（LLMs）集成到参与式城市感知中。AgentSense首先使用经典规划器生成基线解决方案，然后迭代改进这些解决方案，以使感知任务分配适应动态城市条件和异构工人偏好，同时生成自然语言解释以提高透明度和信任度。在两个大规模移动数据集和七种动态干扰的广泛实验中证明，与传统方法相比，AgentSense在适应性和可解释性方面具有明显优势。此外，与单一智能体LLM基线相比，我们的方法在性能和鲁棒性方面都更优，并提供更合理和透明的解释。这些结果表明，AgentSense是向网络部署自适应和可解释的城市感知系统迈进的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Web-based participatory urban sensing has emerged as a vital approach formodern urban management by leveraging mobile individuals as distributedsensors. However, existing urban sensing systems struggle with limitedgeneralization across diverse urban scenarios and poor interpretability indecision-making. In this work, we introduce AgentSense, a hybrid, training-freeframework that integrates large language models (LLMs) into participatory urbansensing through a multi-agent evolution system. AgentSense initially employsclassical planner to generate baseline solutions and then iteratively refinesthem to adapt sensing task assignments to dynamic urban conditions andheterogeneous worker preferences, while producing natural language explanationsthat enhance transparency and trust. Extensive experiments across twolarge-scale mobility datasets and seven types of dynamic disturbancesdemonstrate that AgentSense offers distinct advantages in adaptivity andexplainability over traditional methods. Furthermore, compared to single-agentLLM baselines, our approach outperforms in both performance and robustness,while delivering more reasonable and transparent explanations. These resultsposition AgentSense as a significant advancement towards deploying adaptive andexplainable urban sensing systems on the web.</description>
      <author>example@mail.com (Xusen Guo, Mingxing Peng, Xixuan Hao, Xingchen Zou, Qiongyan Wang, Sijie Ruan, Yuxuan Liang)</author>
      <guid isPermaLink="false">2510.19661v2</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>CURVETE: Curriculum Learning and Progressive Self-supervised Training for Medical Image Classification</title>
      <link>http://arxiv.org/abs/2510.23442v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in the proceedings of ICONIP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CURVETE是一种创新的深度卷积神经网络，通过课程学习和类别分解方法解决了医学图像分析中的样本有限和类别分布不规则的挑战，在各种医学图像数据集上表现出优越的分类性能。&lt;h4&gt;背景&lt;/h4&gt;在医学图像分析中，识别高质量且易于获取的标注样本是一个显著挑战。迁移学习技术利用预训练数据为这一问题提供了灵活的解决方案。然而，当数据集在类别间呈现不规则分布时，微调的效果会减弱。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为课程学习和渐进式自监督训练(CURVETE)的新型深度卷积神经网络，解决与样本有限相关的挑战，增强模型泛化能力，并提高整体分类性能。&lt;h4&gt;方法&lt;/h4&gt;CURVETE采用基于样本分解粒度的课程学习策略，在训练通用未标记样本时使用；在下游任务中整合类别分解方法，解决类别分布不规则的挑战；在脑肿瘤、数字膝盖X光和Mini-DDSM三个医学图像数据集上进行评估，研究了使用通用自监督样本分解方法的分类性能，包括和不包括课程学习组件。&lt;h4&gt;主要发现&lt;/h4&gt;CURVETE模型在测试集上实现了优越的性能，使用基线ResNet-50在脑肿瘤数据集上达到96.60%的准确率，在数字膝盖X光数据集上达到75.60%，在Mini-DDSM数据集上达到93.35%；使用基线DenseNet-121，在三个数据集上分别达到95.77%、80.36%和93.22%的准确率，优于其他训练策略。&lt;h4&gt;结论&lt;/h4&gt;CURVETE模型能够有效解决医学图像分析中的样本有限和类别分布不规则的挑战，通过课程学习和渐进式自监督训练，显著提高了分类性能。&lt;h4&gt;翻译&lt;/h4&gt;在医学图像分析中，识别高质量且易于获取的标注样本是一个显著挑战。迁移学习技术利用预训练数据为这一问题提供了灵活的解决方案。然而，当数据集在类别间呈现不规则分布时，微调的效果会减弱。本文提出了一种名为课程学习和渐进式自监督训练(CURVETE)的新型深度卷积神经网络。CURVETE通过在训练通用未标记样本时采用基于样本分解粒度的课程学习策略，解决了与样本有限相关的挑战，增强了模型泛化能力，并提高了整体分类性能。此外，CURVETE通过在下游任务中整合类别分解方法，解决了类别分布不规则的挑战。该方法在三个不同的医学图像数据集上进行了评估：脑肿瘤、数字膝盖X光和Mini-DDSM数据集。我们研究了使用通用自监督样本分解方法进行分类性能，包括和不包括在训练预任务中使用课程学习组件。实验结果表明，CURVETE模型在测试集上实现了优越的性能，使用基线ResNet-50在脑肿瘤数据集上达到96.60%的准确率，在数字膝盖X光数据集上达到75.60%，在Mini-DDSM数据集上达到93.35%。此外，使用基线DenseNet-121，在脑肿瘤、数字膝盖X光和Mini-DDSM数据集上分别达到95.77%、80.36%和93.22%的准确率，优于其他训练策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying high-quality and easily accessible annotated samples poses anotable challenge in medical image analysis. Transfer learning techniques,leveraging pre-training data, offer a flexible solution to this issue. However,the impact of fine-tuning diminishes when the dataset exhibits an irregulardistribution between classes. This paper introduces a novel deep convolutionalneural network, named Curriculum Learning and Progressive Self-supervisedTraining (CURVETE). CURVETE addresses challenges related to limited samples,enhances model generalisability, and improves overall classificationperformance. It achieves this by employing a curriculum learning strategy basedon the granularity of sample decomposition during the training of genericunlabelled samples. Moreover, CURVETE address the challenge of irregular classdistribution by incorporating a class decomposition approach in the downstreamtask. The proposed method undergoes evaluation on three distinct medical imagedatasets: brain tumour, digital knee x-ray, and Mini-DDSM datasets. Weinvestigate the classification performance using a generic self-supervisedsample decomposition approach with and without the curriculum learningcomponent in training the pretext task. Experimental results demonstrate thatthe CURVETE model achieves superior performance on test sets with an accuracyof 96.60% on the brain tumour dataset, 75.60% on the digital knee x-raydataset, and 93.35% on the Mini-DDSM dataset using the baseline ResNet-50.Furthermore, with the baseline DenseNet-121, it achieved accuracies of 95.77%,80.36%, and 93.22% on the brain tumour, digital knee x-ray, and Mini-DDSMdatasets, respectively, outperforming other training strategies.</description>
      <author>example@mail.com (Asmaa Abbas, Mohamed Gaber, Mohammed M. Abdelsamea)</author>
      <guid isPermaLink="false">2510.23442v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>DREaM: Drug-Drug Relation Extraction via Transfer Learning Method</title>
      <link>http://arxiv.org/abs/2510.23189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为DREAM的方法，用于药物关系抽取，通过结合关系抽取模型和大型语言模型构建药物关系本体并验证结果。&lt;h4&gt;背景&lt;/h4&gt;药物关系抽取对识别药物相互作用和预测副作用至关重要。机器学习方法和大型医学文本数据库的发展降低了关系抽取成本，但目前缺乏专门针对药物关系抽取的数据集。&lt;h4&gt;目的&lt;/h4&gt;由于缺乏专业数据集，需要采用迁移学习方法来应用机器学习技术进行药物关系抽取，并构建药物关系本体。&lt;h4&gt;方法&lt;/h4&gt;DREAM方法首先使用训练好的关系抽取模型发现实体间关系，然后将模型应用于医学文本语料库构建药物关系本体，最后使用大型语言模型验证抽取的关系。&lt;h4&gt;主要发现&lt;/h4&gt;定量结果显示，大型语言模型同意从PubMed摘要子集中提取的71个关系。定性分析表明该方法能揭示医学领域的模糊性，突显了关系抽取的挑战。&lt;h4&gt;结论&lt;/h4&gt;通过迁移学习和大型语言模型验证，DREAM方法能有效提取药物关系并构建药物关系本体，同时揭示了医学领域中关系抽取的固有挑战。&lt;h4&gt;翻译&lt;/h4&gt;药物之间的关系抽取在识别药物-药物相互作用和预测副作用方面起着至关重要的作用。机器学习方法在关系抽取方面的进步，以及大型医学文本数据库的发展，使得与其他通常需要专业知识的方法相比，这种关系的提取成本更低。然而，据我们所知，目前专门用于药物关系抽取的数据集非常有限。因此，采用迁移学习成为在该领域应用机器学习方法的必要手段。在本研究中，我们提出了DREAM方法，该方法首先使用训练好的关系抽取模型发现实体间的关系，然后将该模型应用于医学文本语料库以构建药物关系本体。随后使用大型语言模型验证抽取的关系。定量结果表明，大型语言模型同意从PubMed摘要子集中提取的71个关系。此外，我们的定性分析表明，这种方法可以揭示医学领域中的模糊性，突显了该领域关系抽取的固有挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relation extraction between drugs plays a crucial role in identifying drugdrug interactions and predicting side effects. The advancement of machinelearning methods in relation extraction, along with the development of largemedical text databases, has enabled the low cost extraction of such relationscompared to other approaches that typically require expert knowledge. However,to the best of our knowledge, there are limited datasets specifically designedfor drug drug relation extraction currently available. Therefore, employingtransfer learning becomes necessary to apply machine learning methods in thisdomain. In this study, we propose DREAM, a method that first employs a trainedrelation extraction model to discover relations between entities and thenapplies this model to a corpus of medical texts to construct an ontology ofdrug relationships. The extracted relations are subsequently validated using alarge language model. Quantitative results indicate that the LLM agreed with 71of the relations extracted from a subset of PubMed abstracts. Furthermore, ourqualitative analysis indicates that this approach can uncover ambiguities inthe medical domain, highlighting the challenges inherent in relation extractionin this field.</description>
      <author>example@mail.com (Ali Fata, Hossein Rahmani, Parinaz Soltanzadeh, Amirhossein Derakhshan, Behrouz Minaei Bidgoli)</author>
      <guid isPermaLink="false">2510.23189v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>LightPFP: A Lightweight Route to Ab Initio Accuracy at Scale</title>
      <link>http://arxiv.org/abs/2510.23064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LightPFP是一种数据高效的知识蒸馏框架，利用通用机器学习原子势(u-MLIP)生成针对特定材料的高质量训练数据，结合预训练轻量级MLIP提高效率，实现了比传统DFT方法快三个数量级的模型开发速度，同时保持与第一性原理相当的准确性，且生成的特定任务MLIP(ts-MLIP)在保持高精度的同时实现了1-2个数量级的推理速度提升。&lt;h4&gt;背景&lt;/h4&gt;原子模拟方法已从量子力学发展到密度泛函理论(DFT)，再到机器学习原子势(MLIPs)。通用MLIPs(u-MLIPs)具有良好的可转移性但计算开销大，限制了大规模应用；特定任务MLIPs(ts-MLIPs)效率更高但为每个材料系统生成DFT数据的成本极高。&lt;h4&gt;目的&lt;/h4&gt;提出一种数据高效的知识蒸馏框架LightPFP，解决传统方法中DFT计算成本高的问题，实现快速开发高精度、高效的特定任务MLIPs。&lt;h4&gt;方法&lt;/h4&gt;LightPFP框架利用u-MLIP生成针对特定材料的高质量训练数据，并使用预训练的轻量级MLIP进一步提高数据效率，通过知识蒸馏技术生成ts-MLIP，同时支持高效的精度迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;LightPFP比传统基于DFT的方法快三个数量级的模型开发速度，同时保持与第一性原理预测相当的准确性；蒸馏出的ts-MLIP比u-MLIP快1-2个数量级的推理速度；仅需10个高精度DFT数据点即可校正u-MLIP的系统误差。&lt;h4&gt;结论&lt;/h4&gt;这种u-MLIP驱动的蒸馏方法能够为材料科学应用快速开发高保真度、高效的MLIPs。&lt;h4&gt;翻译&lt;/h4&gt;原子模拟方法已经通过连续的计算层级逐步发展，每个层级都建立在更基础的方法之上：从量子力学到密度泛函理论(DFT)，随后发展到机器学习原子势(MLIPs)。虽然通用MLIPs(u-MLIPs)具有广泛的可转移性，但其计算开销限制了大规模应用。特定任务MLIPs(ts-MLIPs)实现了更高的效率，但为每个材料系统生成DFT数据的成本高得令人望而却步。在本文中，我们提出了LightPFP，一种数据高效的知识蒸馏框架。LightPFP不使用昂贵的DFT计算，而是利用u-MLIP生成针对特定材料定制的高质量训练数据，并使用预训练的轻量级MLIP进一步提高数据效率，从而生成蒸馏的ts-MLIP。在包括固态电解质、高熵合金和反应离子系统在内的广泛材料范围内，LightPFP比传统基于DFT的方法快三个数量级的模型开发速度，同时保持与第一性原理预测相当的准确性。此外，蒸馏出的ts-MLs进一步维持了大规模分子动力学计算所必需的计算效率，比u-MLIPs快1-2个数量级的推理速度。该框架还支持高效的精度迁移学习，其中可以使用少至10个高精度DFT数据点来校正u-MLIP的系统误差，如在MgO熔点预测中所演示的。这种u-MLIP驱动的蒸馏方法能够为材料科学应用快速开发高保真度、高效的MLIPs。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Atomistic simulation methods have evolved through successive computationallevels, each building upon more fundamental approaches: from quantum mechanicsto density functional theory (DFT), and subsequently, to machine learninginteratomic potentials (MLIPs). While universal MLIPs (u-MLIPs) offer broadtransferability, their computational overhead limits large-scale applications.Task-specific MLIPs (ts-MLIPs) achieve superior efficiency but requireprohibitively expensive DFT data generation for each material system. In thispaper, we propose LightPFP, a data-efficient knowledge distillation framework.Instead of using costly DFT calculations, LightPFP generates a distilledts-MLIP by leveraging u-MLIP to generate high-quality training data tailoredfor specific materials and utilizing a pre-trained light-weight MLIP to furtherenhance data efficiency. Across a broad spectrum of materials, includingsolid-state electrolytes, high-entropy alloys, and reactive ionic systems,LightPFP delivers three orders of magnitude faster model development thanconventional DFT-based methods, while maintaining accuracy on par withfirst-principles predictions. Moreover, the distilled ts-MLIPs further sustainthe computational efficiency essential for large-scale molecular dynamics,achieving 1-2 orders of magnitude faster inference than u-MLIPs. The frameworkfurther enables efficient precision transfer learning, where systematic errorsfrom the u-MLIP can be corrected using as few as 10 high-accuracy DFT datapoints, as demonstrated for MgO melting point prediction. This u-MLIP-drivendistillation approach enables rapid development of high-fidelity, efficientMLIPs for materials science applications.</description>
      <author>example@mail.com (Wenwen Li, Nontawat Charoenphakdee, Yong-Bin Zhuang, Ryuhei Okuno, Yuta Tsuboi, So Takamoto, Junichi Ishida, Ju Li)</author>
      <guid isPermaLink="false">2510.23064v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>TLCD: A Deep Transfer Learning Framework for Cross-Disciplinary Cognitive Diagnosis</title>
      <link>http://arxiv.org/abs/2510.23062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种创新的跨学科认知诊断方法(TLCD)，结合深度学习和迁移学习策略，解决了跨学科领域中认知诊断面临的挑战，提高了对学生学习情况评估的准确性。&lt;h4&gt;背景&lt;/h4&gt;在线教育模式已成为教育产业的重要组成部分。认知诊断技术可利用学生学习数据评估其能力水平，但跨学科领域存在特征提取复杂性和学科数据稀缺性问题，传统认知诊断方法面临挑战。&lt;h4&gt;目的&lt;/h4&gt;针对不同学科间知识系统、认知结构和数据特征的差异，研究神经网络认知诊断和知识关联神经网络认知诊断，提出创新的跨学科认知诊断方法。&lt;h4&gt;方法&lt;/h4&gt;提出跨学科认知诊断方法(TLCD)，结合深度学习技术和迁移学习策略，通过利用主学科的共同特征来提高模型在目标学科中的性能。&lt;h4&gt;主要发现&lt;/h4&gt;基于深度学习的跨学科认知诊断模型在跨学科认知诊断任务中表现优于基础模型，能够更准确地评估学生的学习情况。&lt;h4&gt;结论&lt;/h4&gt;跨学科认知诊断方法(TLCD)有效解决了跨学科认知诊断中的挑战，提高了诊断的准确性和性能，对智能教育领域具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;受智能教育和人工智能技术的双重驱动，在线教育模式已迅速成为教育产业的重要组成部分。认知诊断技术可以利用教育评估中学生学习的数据和反馈信息，准确评估他们在知识层面的能力水平。然而，大量信息虽然提供了丰富的数据资源，但也带来了特征提取的复杂性和学科数据的稀缺性。在跨学科领域，传统的认知诊断方法仍面临许多挑战。鉴于不同学科之间知识系统、认知结构和数据特征的差异，本文对神经网络认知诊断和知识关联神经网络认知诊断进行了深入研究，并提出了一种创新的跨学科认知诊断方法(TLCD)。该方法结合了深度学习技术和迁移学习策略，通过利用主学科的共同特征来提高模型在目标学科中的性能。实验结果表明，基于深度学习的跨学科认知诊断模型在跨学科认知诊断任务中表现优于基础模型，能够更准确地评估学生的学习情况。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driven by the dual principles of smart education and artificial intelligencetechnology, the online education model has rapidly emerged as an importantcomponent of the education industry. Cognitive diagnostic technology canutilize students' learning data and feedback information in educationalevaluation to accurately assess their ability level at the knowledge level.However, while massive amounts of information provide abundant data resources,they also bring about complexity in feature extraction and scarcity ofdisciplinary data. In cross-disciplinary fields, traditional cognitivediagnostic methods still face many challenges. Given the differences inknowledge systems, cognitive structures, and data characteristics betweendifferent disciplines, this paper conducts in-depth research on neural networkcognitive diagnosis and knowledge association neural network cognitivediagnosis, and proposes an innovative cross-disciplinary cognitive diagnosismethod (TLCD). This method combines deep learning techniques and transferlearning strategies to enhance the performance of the model in the targetdiscipline by utilizing the common features of the main discipline. Theexperimental results show that the cross-disciplinary cognitive diagnosis modelbased on deep learning performs better than the basic model incross-disciplinary cognitive diagnosis tasks, and can more accurately evaluatestudents' learning situation.</description>
      <author>example@mail.com (Zhifeng Wang, Meixin Su, Yang Yang, Chunyan Zeng, Lizhi Ye)</author>
      <guid isPermaLink="false">2510.23062v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges</title>
      <link>http://arxiv.org/abs/2510.22964v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述从模态驱动视角对多模态地理空间基础模型(GFMs)进行全面回顾，涵盖五种核心视觉和视觉-语言模态，分析其在遥感图像分析中的应用、挑战和未来发展方向。&lt;h4&gt;背景&lt;/h4&gt;基础模型已改变自然语言处理和计算机视觉领域，其影响正在重塑遥感图像分析。基础模型强大的泛化和迁移学习能力与遥感数据的多模态、多分辨率和多时态特性自然契合。&lt;h4&gt;目的&lt;/h4&gt;解决遥感领域的独特挑战，通过多模态地理空间基础模型(GFMs)这一专门研究前沿，提供从模态驱动视角的全面回顾，并分析关键技术、评估模型性能和应用场景。&lt;h4&gt;方法&lt;/h4&gt;涵盖五种核心视觉和视觉-语言模态，检查成像物理和数据表示差异如何塑造交互设计，分析对齐、集成和知识转移的关键技术，系统评估训练范式、架构和适应策略进展，在十个下游任务上评估代表性模型，并通过真实案例研究展示应用潜力。&lt;h4&gt;主要发现&lt;/h4&gt;多模态GFMs在土地覆盖制图、农业监测、灾害响应、气候研究和地理空间情报等领域展现实际应用潜力，不同模型在架构、性能和应用场景上存在差异，需要针对模态异构性、分布偏移和语义差距进行优化。&lt;h4&gt;结论&lt;/h4&gt;领域泛化、可解释性、效率和隐私是GFMs发展面临的紧迫挑战，未来研究需要在这些方面探索有前途的方向，进一步提升模型性能和应用范围。&lt;h4&gt;翻译&lt;/h4&gt;基础模型已经改变了自然语言处理和计算机视觉，它们的影响现在正在重塑遥感图像分析。凭借强大的泛化和迁移学习能力，它们与遥感数据的多模态、多分辨率和多时态特性自然契合。为解决该领域的独特挑战，多模态地理空间基础模型(GFMs)已成为专门的研究前沿。这篇综述从模态驱动视角对多模态GFMs进行全面回顾，涵盖五种核心视觉和视觉-语言模态。我们检查成像物理和数据表示差异如何塑造交互设计，并分析对齐、集成和知识转移的关键技术，以处理模态异构性、分布偏移和语义差距。系统评估了训练范式、架构和任务特定适应策略的进展，以及大量新兴基准。在十个下游任务上评估了代表性的多模态视觉和视觉-语言GFMs，深入了解它们的架构、性能和应用场景。真实案例研究，涵盖土地覆盖制图、农业监测、灾害响应、气候研究和地理空间情报，展示了GFMs的实际潜力。最后，我们概述了领域泛化、可解释性、效率和隐私方面的紧迫挑战，并为未来研究规划了有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have transformed natural language processing and computervision, and their impact is now reshaping remote sensing image analysis. Withpowerful generalization and transfer learning capabilities, they alignnaturally with the multimodal, multi-resolution, and multi-temporalcharacteristics of remote sensing data. To address unique challenges in thefield, multimodal geospatial foundation models (GFMs) have emerged as adedicated research frontier. This survey delivers a comprehensive review ofmultimodal GFMs from a modality-driven perspective, covering five core visualand vision-language modalities. We examine how differences in imaging physicsand data representation shape interaction design, and we analyze key techniquesfor alignment, integration, and knowledge transfer to tackle modalityheterogeneity, distribution shifts, and semantic gaps. Advances in trainingparadigms, architectures, and task-specific adaptation strategies aresystematically assessed alongside a wealth of emerging benchmarks.Representative multimodal visual and vision-language GFMs are evaluated acrossten downstream tasks, with insights into their architectures, performance, andapplication scenarios. Real-world case studies, spanning land cover mapping,agricultural monitoring, disaster response, climate studies, and geospatialintelligence, demonstrate the practical potential of GFMs. Finally, we outlinepressing challenges in domain generalization, interpretability, efficiency, andprivacy, and chart promising avenues for future research.</description>
      <author>example@mail.com (Liling Yang, Ning Chen, Jun Yue, Yidan Liu, Jiayi Ma, Pedram Ghamisi, Antonio Plaza, Leyuan Fang)</author>
      <guid isPermaLink="false">2510.22964v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Inductive Transfer Learning for Graph-Based Recommenders</title>
      <link>http://arxiv.org/abs/2510.22799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the New Perspectives in Graph Machine Learning Workshop  at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了NBF-Rec，一种支持跨不同数据集进行归纳迁移学习的图推荐模型，能够在不重新训练的情况下处理新用户、新项目或新数据集。&lt;h4&gt;背景&lt;/h4&gt;图推荐系统通常在归纳设置下训练，这限制了它们对新用户、新项目或新数据集的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种支持在不同数据集上进行归纳迁移学习的图推荐模型，解决传统方法需要为每个领域重新训练的问题。&lt;h4&gt;方法&lt;/h4&gt;提出NBF-Rec模型，一种基于图的推荐模型，可以在用户和项目集合不相交的数据集之间进行归纳迁移学习。与传统基于嵌入的方法不同，NBF-Rec在推理时动态计算节点嵌入，无需为每个领域重新训练。&lt;h4&gt;主要发现&lt;/h4&gt;NBF-Rec在七个真实世界数据集（涵盖电影、音乐、电子商务和地点签到等领域）上进行了评估，在零样本设置下（不使用目标域数据进行训练）取得了具有竞争力的性能，并通过轻量级微调进一步提高了性能。&lt;h4&gt;结论&lt;/h4&gt;归纳迁移在图推荐中是可行的，交互级别的消息传递支持跨数据集的泛化，而无需对齐用户或项目。&lt;h4&gt;翻译&lt;/h4&gt;基于图的推荐系统通常在归纳设置下进行训练，这限制了它们对新用户、新项目或新数据集的适用性。我们提出了NBF-Rec，一种基于图的推荐模型，支持在不同用户和项目集合不相交的数据集上进行归纳迁移学习。与需要为每个领域重新训练的传统基于嵌入的方法不同，NBF-Rec在推理时动态计算节点嵌入。我们在七个涵盖电影、音乐、电子商务和地点签到的真实世界数据集上评估了该方法。NBF-Rec在零样本设置下（不使用目标域数据进行训练）取得了具有竞争力的性能，并通过轻量级微调展示了进一步的改进。这些结果表明，归纳迁移在图推荐中是可行的，并且交互级别的消息传递支持跨数据集的泛化，而无需对齐用户或项目。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-based recommender systems are commonly trained in transductivesettings, which limits their applicability to new users, items, or datasets. Wepropose NBF-Rec, a graph-based recommendation model that supports inductivetransfer learning across datasets with disjoint user and item sets. Unlikeconventional embedding-based methods that require retraining for each domain,NBF-Rec computes node embeddings dynamically at inference time. We evaluate themethod on seven real-world datasets spanning movies, music, e-commerce, andlocation check-ins. NBF-Rec achieves competitive performance in zero-shotsettings, where no target domain data is used for training, and demonstratesfurther improvements through lightweight fine-tuning. These results show thatinductive transfer is feasible in graph-based recommendation and thatinteraction-level message passing supports generalization across datasetswithout requiring aligned users or items.</description>
      <author>example@mail.com (Florian Grötschla, Elia Trachsel, Luca A. Lanzendörfer, Roger Wattenhofer)</author>
      <guid isPermaLink="false">2510.22799v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Qlustering: Harnessing Network-Based Quantum Transport for Data Clustering</title>
      <link>http://arxiv.org/abs/2510.22727v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Qlustering，一种受量子启发的无监督学习算法，利用基于网络的量子传输进行数据聚类，在多种数据集上表现出与经典方法相当或更优的性能，特别是在处理非凸或高维数据时。&lt;h4&gt;背景&lt;/h4&gt;传统聚类方法主要基于距离度量，而量子计算提供了新的计算范式，可以解决传统方法难以处理的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的量子启发式聚类算法，能够有效处理非凸或高维数据，并具有计算效率和物理可实现性。&lt;h4&gt;方法&lt;/h4&gt;Qlustering将数据编码为紧束缚哈密顿量框架中的输入状态，通过量子粒子在网络中的传播动力学进行计算，聚类分配从终端节点的稳态输出电流中产生，算法通过迭代优化网络哈密顿量和随机更新实现收敛。&lt;h4&gt;主要发现&lt;/h4&gt;在合成数据集、定位问题、QM9分子数据库和Iris数据集上，Qlustering与k-means等经典方法相比具有竞争力或更优的性能，特别是在处理非凸或高维数据时表现出色。&lt;h4&gt;结论&lt;/h4&gt;Qlustering具有内在的鲁棒性、低计算复杂性和与光子实现的兼容性，为构建物理可实现的、量子原生的聚类架构提供了有前途的途径。&lt;h4&gt;翻译&lt;/h4&gt;我们引入Qlustering，一种用于无监督学习的受量子启发的算法，它利用基于网络的量子传输来执行数据聚类。与传统的基于距离的方法不同，Qlustering将量子粒子通过网络传播的稳态动力学视为计算资源。数据被编码为由Lindblad主方程控制的紧束缚哈密顿量框架中的输入状态，聚类分配从终端节点的稳态输出电流中产生。该算法迭代地优化网络的哈密顿量以最小化物理动机的成本函数，通过随机更新实现收敛。我们在合成数据集、定位问题以及真实的化学和生物数据（即QM9分子数据库和Iris数据集的子集）上对Qlustering进行了基准测试。在这些多样化的任务中，Qlustering展示了与k-means等经典方法相比具有竞争力或更优的性能，特别是对于非凸或高维数据。其内在的鲁棒性、低计算复杂性和与光子实现的兼容性表明，其有望实现物理可实现的、量子原生的聚类架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Qlustering, a quantum-inspired algorithm for unsupervisedlearning that leverages network-based quantum transport to perform dataclustering. In contrast to traditional distance-based methods, Qlusteringtreats the steady-state dynamics of quantum particles propagating through anetwork as a computational resource. Data are encoded as input states in atight-binding Hamiltonian framework governed by the Lindblad master equation,and cluster assignments emerge from steady-state output currents at terminalnodes. The algorithm iteratively optimizes the network's Hamiltonian tominimize a physically motivated cost function, achieving convergence throughstochastic updates. We benchmark Qlustering on synthetic datasets, alocalization problem, and real-world chemical and biological data, namelysubsets of the QM9 molecular database and the Iris dataset. Across thesediverse tasks, Qlustering demonstrates competitive or superior performancecompared with classical methods such as k-means, particularly for non-convex orhigh-dimensional data. Its intrinsic robustness, low computational complexity,and compatibility with photonic implementations suggest a promising routetoward physically realizable, quantum-native clustering architectures.</description>
      <author>example@mail.com (Shmuel Lorber, Yonatan Dubi)</author>
      <guid isPermaLink="false">2510.22727v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Species Transfer Learning in Agricultural AI: Evaluating ZebraPose Adaptation for Dairy Cattle Pose Estimation</title>
      <link>http://arxiv.org/abs/2510.22618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 11 figures, 6 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了使用在斑马图像上训练的ZebraPose模型进行奶牛姿态估计的跨物种迁移学习潜力，发现在不同环境间存在显著泛化挑战。&lt;h4&gt;背景&lt;/h4&gt;姿态估计是计算机视觉的核心技术，用于理解动物姿态、行为和福利，但农业应用受限于缺乏大型标注的牲畜数据集，特别是奶牛数据集。&lt;h4&gt;目的&lt;/h4&gt;评估跨物种迁移学习的潜力和局限性，通过将ZebraPose模型适应于谷仓条件下奶牛的27个关键点检测。&lt;h4&gt;方法&lt;/h4&gt;使用三种配置评估模型：自定义农场数据集（375张图像）、APT-36K基准数据集的子集以及它们的组合，系统评估了模型在不同环境中的准确性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;组合模型在分布内数据上表现良好，但在未见过的谷仓和奶牛群体上出现显著泛化失败，表明合成到真实域差距是农业AI部署的主要障碍，物种形态相似性不足以实现跨域迁移。&lt;h4&gt;结论&lt;/h4&gt;研究强调了数据集多样性、环境变化性和计算约束对现实世界部署的影响，呼吁以农业为先的AI设计，优先考虑农场级真实性、跨环境鲁棒性和开放基准数据集。&lt;h4&gt;翻译&lt;/h4&gt;姿态估计作为计算机视觉的基石，用于理解动物姿态、行为和福利。然而，农业应用仍然受限于大型标注牲畜数据集的稀缺，特别是奶牛。本研究通过将ZebraPose（一种基于视觉变换器的模型，在合成斑马图像上训练）适应于谷仓条件下奶牛的27个关键点检测，评估了跨物种迁移学习的潜力和局限性。使用三种配置——自定义农场数据集（375张图像，加拿大新不伦瑞克州苏塞克斯）、APT-36K基准数据集的子集以及它们的组合，我们系统评估了模型在不同环境中的准确性和泛化能力。虽然组合模型在分布内数据上取得了有希望的性能，但当应用于未见过的谷仓和奶牛群体时，出现了显著的泛化失败。这些发现揭示了合成到真实域差距是农业AI部署的主要障碍，并强调物种间的形态相似性不足以进行跨域迁移。研究提供了关于数据集多样性、环境变化性和计算约束影响现实世界部署的实践见解。我们呼吁以农业为先的AI设计，优先考虑农场级真实性、跨环境鲁棒性和开放基准数据集，以推进可信和可扩展的以动物为中心的技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pose estimation serves as a cornerstone of computer vision for understandinganimal posture, behavior, and welfare. Yet, agricultural applications remainconstrained by the scarcity of large, annotated datasets for livestock,especially dairy cattle. This study evaluates the potential and limitations ofcross-species transfer learning by adapting ZebraPose - a visiontransformer-based model trained on synthetic zebra imagery - for 27-keypointdetection in dairy cows under real barn conditions. Using three configurations- a custom on-farm dataset (375 images, Sussex, New Brunswick, Canada), asubset of the APT-36K benchmark dataset, and their combination, wesystematically assessed model accuracy and generalization across environments.While the combined model achieved promising performance (AP = 0.86, AR = 0.87,PCK 0.5 = 0.869) on in-distribution data, substantial generalization failuresoccurred when applied to unseen barns and cow populations. These findingsexpose the synthetic-to-real domain gap as a major obstacle to agricultural AIdeployment and emphasize that morphological similarity between species isinsufficient for cross-domain transfer. The study provides practical insightsinto dataset diversity, environmental variability, and computationalconstraints that influence real-world deployment of livestock monitoringsystems. We conclude with a call for agriculture-first AI design, prioritizingfarm-level realism, cross-environment robustness, and open benchmark datasetsto advance trustworthy and scalable animal-centric technologies.</description>
      <author>example@mail.com (Mackenzie Tapp, Sibi Chakravarthy Parivendan, Kashfia Sailunaz, Suresh Neethirajan)</author>
      <guid isPermaLink="false">2510.22618v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>A roadmap for curvature-based geometric data analysis and learning</title>
      <link>http://arxiv.org/abs/2510.22599v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提供了对离散曲率模型的首次全面综述，涵盖了数学基础、计算公式以及在数据分析和学习中的实际应用。文章从黎曼几何和度量几何角度讨论离散曲率，并提出了一种曲率驱动的数据分析系统流程。&lt;h4&gt;背景&lt;/h4&gt;几何数据分析和学习已成为一个独特且快速发展的研究领域，因其跨领域的有效性而日益受到认可。曲率是该领域的核心概念，它能够捕捉内在几何结构并支持从社区检测到几何深度学习的众多任务。针对图、单纯复形、立方体复形和流形采样点云等多种数据表示，已经提出了广泛的离散曲率模型。&lt;h4&gt;目的&lt;/h4&gt;这篇论文旨在对现有的离散曲率模型进行首次全面综述，涵盖其数学基础、计算公式以及在数据分析和学习中的实际应用。&lt;h4&gt;方法&lt;/h4&gt;作者从黎曼几何和度量几何两个角度讨论离散曲率，并提出了一种曲率驱动的数据分析系统流程。他们还检查了不同数据表示下的相应计算算法，提供了详细的比较和见解。&lt;h4&gt;主要发现&lt;/h4&gt;离散曲率模型不仅为数据几何提供了有效的表征，而且构成了几何学习框架的基本组成部分。这些模型在各种数据表示上都有应用，并在监督和无监督学习中取得了最先进的应用效果。&lt;h4&gt;结论&lt;/h4&gt;这篇综述为研究人员提供了一个概念性和实践性的路线图，帮助他们更好地理解离散曲率作为几何理解和学习的基本工具。&lt;h4&gt;翻译&lt;/h4&gt;几何数据分析和学习已成为一个独特且快速发展的研究领域，其有效性在多样化的应用中日益得到认可。该领域的核心是曲率，一个强大且可解释的概念，它捕捉内在的几何结构并支撑着从社区检测到几何深度学习的众多任务。针对图、单纯复形、立方体复形和从流形采样的点云等多种数据表示，已经提出了广泛的离散曲率模型。这些模型不仅为数据几何提供了有效的表征，而且构成了几何学习框架的基本组成部分。在本文中，我们首次对现有的离散曲率模型进行了全面综述，涵盖了它们的数学基础、计算公式以及数据分析和学习中的实际应用。特别是，我们从黎曼几何和度量几何的角度讨论了离散曲率，并提出了一个曲率驱动的数据分析系统流程。我们进一步检查了不同数据表示下的相应计算算法，提供了详细的比较和见解。最后，我们回顾了曲率在监督和无监督学习中的最先进应用。本综述为研究人员提供了一个概念性和实践性的路线图，使他们能够更好地理解离散曲率作为几何理解和学习的基本工具。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是离散曲率模型的系统综述和整合。目前存在多种离散曲率模型（如Forman-Ricci、Ollivier-Ricci等），它们基于不同数学原理，应用于不同数据表示，但缺乏统一框架和比较。这个问题的重要性在于几何数据分析和学习已成为快速发展的研究领域，曲率是理解数据内在几何结构的关键概念，从社区检测到几何深度学习都有重要应用，而离散曲率模型为数据几何提供了高效表征，并构成几何学习框架的基本组成部分。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性地回顾现有文献来构建他们的方法。首先介绍几何数据分析和学习的背景，强调曲率的重要性；然后回顾黎曼几何和度量几何中曲率的数学基础；接着介绍多种离散曲率模型的定义和计算方法；最后提出一个三步流程用于基于曲率的数据分析。作者借鉴了大量现有工作，包括Forman基于Bochner-Weitzenböck公式的组合曲率方法、Ollivier基于最优输运的粗糙Ricci曲率、Bakry-Émery的Ricci曲率下界、Joharinad和Jost的sectional曲率，以及Menger曲率、Haantjes曲率和电阻曲率等网络分析中的定义。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是曲率作为理解数据内在几何结构的基本工具，可以用于分析和学习各种非欧几里得数据。整体实现流程是三步流程：1）数据表示：根据应用领域提取合适的拓扑表示（如图、单纯复形、立方体复形或超图）；2）离散曲率计算：在提取的拓扑表示上应用适当的曲率定义（如Forman-Ricci、Ollivier-Ricci等）；3）特征提取：从计算出的曲率中提取有意义的几何特征，如边基曲率特征化边或连接，顶点基曲率特征化顶点或数据点。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次全面综述离散曲率模型；系统化分类曲率模型；跨领域整合不同领域的曲率概念；提出实用三步流程；提供各种曲率模型在不同数据表示上的具体计算方法。相比之前的工作，这篇论文的不同之处在于：之前的文献通常专注于单一曲率模型或特定应用，而本文提供了全面视角，比较了不同模型的优缺点；不仅关注理论，还关注实际计算和应用；强调了曲率在不同数据表示上的通用性，展示了其在几何深度学习中的广泛应用潜力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提供了离散曲率模型的全面综述，建立了从理论到实践的系统性框架，使研究者能够理解和应用曲率作为几何数据分析和学习的基本工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometric data analysis and learning has emerged as a distinct and rapidlydeveloping research area, increasingly recognized for its effectiveness acrossdiverse applications. At the heart of this field lies curvature, a powerful andinterpretable concept that captures intrinsic geometric structure and underpinsnumerous tasks, from community detection to geometric deep learning. A widerange of discrete curvature models have been proposed for various datarepresentations, including graphs, simplicial complexes, cubical complexes, andpoint clouds sampled from manifolds. These models not only provide efficientcharacterizations of data geometry but also constitute essential components ingeometric learning frameworks. In this paper, we present the firstcomprehensive review of existing discrete curvature models, covering theirmathematical foundations, computational formulations, and practicalapplications in data analysis and learning. In particular, we discuss discretecurvature from both Riemannian and metric geometry perspectives and propose asystematic pipeline for curvature-driven data analysis. We further examine thecorresponding computational algorithms across different data representations,offering detailed comparisons and insights. Finally, we review state-of-the-artapplications of curvature in both supervised and unsupervised learning. Thissurvey provides a conceptual and practical roadmap for researchers to gain abetter understanding of discrete curvature as a fundamental tool for geometricunderstanding and learning.</description>
      <author>example@mail.com (Yasharth Yadav, Kelin Xia)</author>
      <guid isPermaLink="false">2510.22599v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>AnyECG-Lab: An Exploration Study of Fine-tuning an ECG Foundation Model to Estimate Laboratory Values from Single-Lead ECG Signals</title>
      <link>http://arxiv.org/abs/2510.22301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了利用心电图(ECG)通过深度学习技术估计实验室值的可行性，提供了一种非侵入性、快速的临床决策支持方法。&lt;h4&gt;背景&lt;/h4&gt;当前实验室检测依赖于侵入性静脉采样，存在延迟问题。心电图作为无创且广泛可用的信号，为快速估计实验室值提供了有前景的途径，但现有模型受限于低信噪比、个体间变异性大、数据多样性有限以及泛化能力不足。&lt;h4&gt;目的&lt;/h4&gt;探索使用迁移学习微调大规模预训练的ECG基础模型(ECGFounder)，实现从ECG信号中估计实验室值，并建立实时、无创估计实验室值的可行性范围。&lt;h4&gt;方法&lt;/h4&gt;利用斯坦福大学的多模式临床监测急诊数据集(MC-MED)进行探索性研究，使用迁移学习技术对ECGFounder大型预训练模型进行微调，并生成了超过2000万个标准化的10秒ECG片段以增强对细微生化相关性的敏感性。&lt;h4&gt;主要发现&lt;/h4&gt;在内部验证中，模型对33项实验室指标表现出强的预测性能(曲线下面积高于0.65)，对59项指标表现出中等性能(曲线下面积在0.55到0.65之间)，对16项指标表现有限(曲线下面积低于0.55)。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种高效的人工智能驱动解决方案，并建立了实时、无创估计实验室值的可行性范围。&lt;h4&gt;翻译&lt;/h4&gt;及时获取实验室值对临床决策至关重要，但当前方法依赖于侵入性静脉采样且本质上存在延迟。心电图作为一种无创且广泛可用的信号，为快速估计实验室值提供了有前景的方式。深度学习的最新进展使得从心电图中提取潜在的血液学特征成为可能。然而，现有模型受限于低信噪比、显著的个体间变异性、有限的数据多样性和次优的泛化能力，特别是在适配到导联数较少的可穿戴设备时。在本工作中，我们进行了一项探索性研究，利用迁移学习技术在斯坦福大学的多模式临床监测急诊数据集(MC-MED)上微调ECGFounder——一个大规模预训练的心电基础模型。我们生成了超过2000万个标准化的十秒心电片段，以增强对细微生化相关性的敏感性。在内部验证中，该模型对三十三项实验室指标表现出强的预测性能(曲线下面积高于0.65)，对五十九项指标表现出中等性能(在0.55和0.65之间)，对十六项指标表现有限(低于0.55)。该研究提供了一种高效的人工智能驱动解决方案，并建立了实时、无创估计实验室值的可行性范围。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely access to laboratory values is critical for clinical decision-making,yet current approaches rely on invasive venous sampling and are intrinsicallydelayed. Electrocardiography (ECG), as a non-invasive and widely availablesignal, offers a promising modality for rapid laboratory estimation. Recentprogress in deep learning has enabled the extraction of latent hematologicalsignatures from ECGs. However, existing models are constrained by lowsignal-to-noise ratios, substantial inter-individual variability, limited datadiversity, and suboptimal generalization, especially when adapted to low-leadwearable devices. In this work, we conduct an exploratory study leveragingtransfer learning to fine-tune ECGFounder, a large-scale pre-trained ECGfoundation model, on the Multimodal Clinical Monitoring in the EmergencyDepartment (MC-MED) dataset from Stanford. We generated a corpus of more than20 million standardized ten-second ECG segments to enhance sensitivity tosubtle biochemical correlates. On internal validation, the model demonstratedstrong predictive performance (area under the curve above 0.65) forthirty-three laboratory indicators, moderate performance (between 0.55 and0.65) for fifty-nine indicators, and limited performance (below 0.55) forsixteen indicators. This study provides an efficient artificial-intelligencedriven solution and establishes the feasibility scope for real-time,non-invasive estimation of laboratory values.</description>
      <author>example@mail.com (Yujie Xiao, Gongzhen Tang, Wenhui Liu, Jun Li, Guangkun Nie, Zhuoran Kan, Deyun Zhang, Qinghao Zhao, Shenda Hong)</author>
      <guid isPermaLink="false">2510.22301v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Synthetic-to-Real Transfer Learning for Chromatin-Sensitive PWS Microscopy</title>
      <link>http://arxiv.org/abs/2510.22239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 5 figures and 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出CFU Net，一种分层分割架构，使用三阶段课程在合成多模态数据上训练，实现了近乎完美的细胞核分割性能，应用于超过一万个细胞核的自动分析，提取了区分正常与癌前组织的染色质生物标志物，为专业显微镜中的合成到真实迁移学习提供了通用框架。&lt;h4&gt;背景&lt;/h4&gt;染色质敏感部分波谱(csPWS)显微镜技术能够无标记检测发生在可见细胞转化之前的纳米级染色质包装变化，但手动细胞核分割限制了群体规模分析，且缺乏注释的csPWS成像数据阻碍了标准深度学习方法的使用。&lt;h4&gt;目的&lt;/h4&gt;解决手动细胞核分割限制群体规模分析的问题，克服缺乏注释csPWS成像数据的挑战，开发能够自动分析染色质包装变化的方法，用于早期癌症检测中的生物标志物发现。&lt;h4&gt;方法&lt;/h4&gt;提出CFU Net分层分割架构，使用三阶段课程在合成多模态数据上训练；采用基于物理的渲染，结合染色质包装统计、Mie散射模型和模态特定噪声；整合五种架构元素：ConvNeXt骨干网络、特征金字塔网络、UNet++密集连接、双注意力和深度监督；实现INT8量化以提高效率。&lt;h4&gt;主要发现&lt;/h4&gt;在合成测试数据上实现近乎完美性能（Dice 0.9879，IoU 0.9895）；与基础UNet相比Dice提高8.3%；通过量化实现240倍吞吐量增益；提取的染色质生物标志物区分正常与癌前组织效应量显著（Cohen's d在1.31到2.98之间）；分类准确率达94%。&lt;h4&gt;结论&lt;/h4&gt;该工作为专业显微镜中的合成到真实迁移学习提供了通用框架，并提供了社区在临床标本上进行验证的开放资源，有效应用于早期癌症检测中的生物标志物发现。&lt;h4&gt;翻译&lt;/h4&gt;染色质敏感部分波谱(csPWS)显微镜技术能够无标记检测发生在可见细胞转化之前的纳米级染色质包装变化。然而，手动细胞核分割限制了早期癌症检测中生物标志物发现所需的群体规模分析。缺乏注释的csPWS成像数据阻碍了标准深度学习方法的直接使用。我们提出了CFU Net，一种使用三阶段课程在合成多模态数据上训练的分层分割架构。CFU Net在代表多样化光谱成像条件的保留合成测试数据上实现了近乎完美的性能，无需手动注释（Dice 0.9879，IoU 0.9895）。我们的方法使用基于物理的渲染，结合经验支持的染色质包装统计、Mie散射模型和模态特定噪声，并结合一个从对抗性RGB预训练进展到光谱微调和组织学验证的课程。CFU Net整合了五种架构元素（ConvNeXt骨干网络、特征金字塔网络、UNet++密集连接、双注意力和深度监督），这些元素共同使Dice比基础UNet提高了8.3%。我们展示了可部署的INT8量化，压缩率为74.9%，推理时间为0.15秒，比手动分析提高了240倍的吞吐量。应用于来自合成测试数据的超过一万个自动分割的细胞核，该流程提取了区分正常与癌前组织的染色质生物标志物，具有大的效应量（Cohen's d在1.31到2.98之间），达到94%的分类准确率。这项工作为专业显微镜中的合成到真实迁移学习提供了通用框架，并为社区在临床标本上进行验证提供了开放资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chromatin sensitive partial wave spectroscopic (csPWS) microscopy enableslabel free detection of nanoscale chromatin packing alterations that occurbefore visible cellular transformation. However, manual nuclear segmentationlimits population scale analysis needed for biomarker discovery in early cancerdetection. The lack of annotated csPWS imaging data prevents direct use ofstandard deep learning methods. We present CFU Net, a hierarchical segmentationarchitecture trained with a three stage curriculum on synthetic multimodaldata. CFU Net achieves near perfect performance on held out synthetic test datathat represent diverse spectroscopic imaging conditions without manualannotations (Dice 0.9879, IoU 0.9895). Our approach uses physics basedrendering that incorporates empirically supported chromatin packing statistics,Mie scattering models, and modality specific noise, combined with a curriculumthat progresses from adversarial RGB pretraining to spectroscopic fine tuningand histology validation. CFU Net integrates five architectural elements(ConvNeXt backbone, Feature Pyramid Network, UNet plus plus dense connections,dual attention, and deep supervision) that together improve Dice over abaseline UNet by 8.3 percent. We demonstrate deployment ready INT8 quantizationwith 74.9 percent compression and 0.15 second inference, giving a 240 timesthroughput gain over manual analysis. Applied to more than ten thousandautomatically segmented nuclei from synthetic test data, the pipeline extractschromatin biomarkers that distinguish normal from pre cancerous tissue withlarge effect sizes (Cohens d between 1.31 and 2.98), reaching 94 percentclassification accuracy. This work provides a general framework for syntheticto real transfer learning in specialized microscopy and open resources forcommunity validation on clinical specimens.</description>
      <author>example@mail.com (Jahidul Arafat, Sanjaya Poudel)</author>
      <guid isPermaLink="false">2510.22239v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model</title>
      <link>http://arxiv.org/abs/2510.22057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 12 figures, and 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种自动化系统来检测在线学习期间的学生参与度，同时确保模型不依赖性别等敏感特征进行预测，提高了模型的公平性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;随着在线和虚拟学习的兴起，监控和提升学生参与度已成为有效教育的重要方面，但传统评估方法可能不直接适用于虚拟环境。&lt;h4&gt;目的&lt;/h4&gt;开发一个自动化系统来检测在线学习期间学生的参与度水平，解决传统方法在虚拟环境中不适用的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的训练方法，应用属性正则正交化技术到分割模型分类器中，并使用多种迁移学习策略，以阻止模型利用敏感特征如性别进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法不仅有助于执行道德标准，还能增强模型预测的可解释性；通过该方法，预测敏感群体的分布差异从未缓解模型的皮尔逊相关系数0.897降低到缓解模型的0.999。&lt;h4&gt;结论&lt;/h4&gt;成功开发了一个能够检测在线学习中学生参与度的自动化系统，同时确保了模型的公平性和可解释性，源代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;随着在线和虚拟学习的兴起，监控和提升学生参与度已成为有效教育的重要方面。评估学生参与度的传统方法可能不直接适用于虚拟环境。在本研究中，我们关注这一问题，致力于开发一个自动化系统来检测在线学习期间学生的参与度水平。我们提出了一种新的训练方法，可以阻止模型利用性别等敏感特征进行预测。所提出的方法不仅在执行道德标准方面有益，还能增强模型预测的可解释性。我们将属性正则正交化技术应用于分割模型分类器，该分类器使用多种迁移学习策略，在减少预测敏感群体的分布差异方面取得了有效成果，从未缓解模型的皮尔逊相关系数0.897降低到缓解模型的0.999。该项目的源代码可在https://github.com/ashiskb/elearning-engagement-study获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rise of online and virtual learning, monitoring and enhancingstudent engagement have become an important aspect of effective education.Traditional methods of assessing a student's involvement might not beapplicable directly to virtual environments. In this study, we focused on thisproblem and addressed the need to develop an automated system to detect studentengagement levels during online learning. We proposed a novel training methodwhich can discourage a model from leveraging sensitive features like gender forits predictions. The proposed method offers benefits not only in theenforcement of ethical standards, but also to enhance interpretability of themodel predictions. We applied an attribute-orthogonal regularization techniqueto a split-model classifier, which uses multiple transfer learning strategiesto achieve effective results in reducing disparity in the distribution ofprediction for sensitivity groups from a Pearson correlation coefficient of0.897 for the unmitigated model, to 0.999 for the mitigated model. The sourcecode for this project is available onhttps://github.com/ashiskb/elearning-engagement-study .</description>
      <author>example@mail.com (James Thiering, Tarun Sethupat Radha Krishna, Dylan Zelkin, Ashis Kumer Biswas)</author>
      <guid isPermaLink="false">2510.22057v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>LiteDiff</title>
      <link>http://arxiv.org/abs/2510.22004v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Lite-Diff，一种轻量级扩散模型适应方法，通过将轻量适应层集成到冻结的扩散U-Net中，结合潜在形态自编码器和像素级判别器，显著降低了计算成本并减少了过拟合，即使在数据有限的情况下也能高效工作。&lt;h4&gt;背景&lt;/h4&gt;扩散模型在高保真图像合成方面取得了显著成功，但在特定领域（如医学成像）微调这些模型仍然具有挑战性，原因是领域特定数据有限和完整模型适应的高计算成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效微调方法，使扩散模型能够在特定领域（如医学成像）有效适应，同时降低计算成本并减少过拟合风险。&lt;h4&gt;方法&lt;/h4&gt;Lite-Diff将轻量级适应层集成到冻结的扩散U-Net中，同时使用潜在形态自编码器（用于领域特定潜在一致性）和像素级判别器（用于对抗对齐）来增强训练。通过冻结基础模型权重并仅优化小型残差适配器模块实现轻量化。&lt;h4&gt;主要发现&lt;/h4&gt;选择性在不同U-Net块中集成适应层可以找到效率与性能的最佳平衡。在三个胸部X光数据集（Kaggle Chest X-Ray Pneumonia、NIH Chest X-ray14和VinBigData Chest X_ray）上的实验表明，Lite-Diff相比传统完整微调实现了更好的适应效率。&lt;h4&gt;结论&lt;/h4&gt;Lite-Diff框架为扩散模型的迁移学习提供了有希望的方向，促进了它们在多样化低数据领域中的部署。&lt;h4&gt;翻译&lt;/h4&gt;近年来，扩散模型在高保真图像合成方面表现出色。然而，由于领域特定数据有限和完整模型适应的高计算成本，将这些模型微调到专业领域（如医学成像）仍然具有挑战性。在本文中，我们引入了Lite-Diff（轻量级扩散模型适应），一种新的微调方法，它将轻量级适应层集成到冻结的扩散U-Net中，同时使用潜在形态自编码器（用于领域特定潜在一致性）和像素级判别器（用于对抗对齐）来增强训练。通过冻结基础模型的权重并仅优化小型残差适配器模块，Lite-Diff显著降低了计算开销并减轻了过拟合，即使在数据有限的情况下也是如此。此外，我们进行了消融研究，分析了在不同U-Net块中选择性集成适应层的效果，揭示了效率与性能之间的最佳平衡。在三个胸部X光数据集 - (1) Kaggle胸部X光肺炎、(2) NIH胸部X光14和(3) VinBigData胸部X光上的实验表明，Lite-Diff相比传统完整微调实现了更好的适应效率。我们的框架为扩散模型的迁移学习提供了有希望的方向，促进了它们在多样化低数据领域中的部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, diffusion models have demonstrated remarkable success inhigh-fidelity image synthesis. However, fine-tuning these models forspecialized domains, such as medical imaging, remains challenging due tolimited domain-specific data and the high computational cost of full modeladaptation. In this paper, we introduce Lite-Diff (Lightweight Diffusion ModelAdaptation), a novel finetuning approach that integrates lightweight adaptationlayers into a frozen diffusion U-Net while enhancing training with a latentmorphological autoencoder (for domain-specific latent consistency) and a pixellevel discriminator(for adversarial alignment). By freezing weights of the basemodel and optimizing only small residual adapter modules, LiteDiffsignificantly reduces the computational overhead and mitigates overfitting,even in minimal-data settings. Additionally, we conduct ablation studies toanalyze the effects of selectively integrating adaptation layers in differentU-Net blocks, revealing an optimal balance between efficiency and performance.Experiments on three chest X-ray datasets - (1) Kaggle Chest X-Ray Pneumonia,(2) NIH Chest X-ray14 and (3) VinBigData Chest X_ray demonstrate that LiteDiffachieves superior adaptation efficiency compared to naive full fine-tuning. Ourframework provides a promising direction for transfer learning in diffusionmodels, facilitating their deployment in diverse low data domains.</description>
      <author>example@mail.com (Ruchir Namjoshi, Nagasai Thadishetty, Vignesh Kumar, Hemanth Venkateshwara)</author>
      <guid isPermaLink="false">2510.22004v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Split-MMD Training for Small-Sample Cross-Dataset P300 EEG Classification</title>
      <link>http://arxiv.org/abs/2510.21969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures. Submitted to IEEE BIBM 2025 Workshop on Machine  Learning for EEG Signal Processing (MLESP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种自适应分割最大均值差异训练(AS-MMD)方法，用于解决从脑电图(EEG)中检测单次试验P300时数据量有限的问题，特别是在跨数据集迁移学习中的分布偏移挑战。&lt;h4&gt;背景&lt;/h4&gt;当只有少量标记试验可用时，从脑电图(EEG)中检测单次试验P300是困难的。当尝试通过迁移学习用大型源数据集增强小型目标集时，会出现跨数据集偏移问题。&lt;h4&gt;目的&lt;/h4&gt;研究两个公共视觉oddball ERP数据集之间的迁移学习，解决在小样本设置下(目标:每个受试者10次试验；源:每个受试者80次试验)的跨数据集分布不一致问题。&lt;h4&gt;方法&lt;/h4&gt;提出自适应分割最大均值差异训练(AS-MMD)，结合了三种技术：(1)与源/目标大小比值相关的目标加权损失和预热；(2)具有共享参数和每域统计的分割批量归一化；(3)使用中带带宽启发式的无参数对数级RBF核最大均值差异项。该方法在EEG Conformer上实现，与主干网络无关且保持推理模型不变。&lt;h4&gt;主要发现&lt;/h4&gt;在两种迁移方向上，AS-MMD均优于仅目标训练和联合训练(Active Visual Oddball: 准确率/AUC为0.66/0.74；ERP CORE P3: 0.61/0.65)，与联合训练相比的增益在统计上显著。消融研究表明所有三个组件都对性能提升有贡献。&lt;h4&gt;结论&lt;/h4&gt;AS-MMD方法有效解决了小样本条件下EEG信号P300检测中的跨数据集迁移学习挑战，通过结合三种创新技术显著提高了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;当只有少量标记试验可用时，从脑电图(EEG)中检测单次试验P300是困难的。当尝试通过迁移学习用大型源数据集增强小型目标集时，会出现跨数据集偏移。为应对这一挑战，我们在严格的小样本设置下(目标:每个受试者10次试验；源:每个受试者80次试验)，研究了使用五个共享电极(Fz, Pz, P3, P4, Oz)在两个公共视觉oddball ERP数据集之间的迁移学习。我们引入了自适应分割最大均值差异训练(AS-MMD)，它结合了(i)与源/目标大小比值的平方根相关的目标加权损失和预热，(ii)具有共享仿射参数和每域运行统计的分割批量归一化(Split-BN)，以及(iii)使用中带带宽启发式的无参数对数级径向基函数核最大均值差异(RBF-MMD)项。在EEG Conformer上实现后，AS-MMD与主干网络无关且保持推理时模型不变。在两种迁移方向上，它都优于仅目标训练和联合训练(Active Visual Oddball: 准确率/AUC为0.66/0.74；ERP CORE P3: 0.61/0.65)，与联合训练相比的增益在校正后的配对t检验下显著。消融研究将改进归因于所有三个组件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting single-trial P300 from EEG is difficult when only a few labeledtrials are available. When attempting to boost a small target set with a largesource dataset through transfer learning, cross-dataset shift arises. Toaddress this challenge, we study transfer between two public visual-oddball ERPdatasets using five shared electrodes (Fz, Pz, P3, P4, Oz) under a strictsmall-sample regime (target: 10 trials/subject; source: 80 trials/subject). Weintroduce Adaptive Split Maximum Mean Discrepancy Training (AS-MMD), whichcombines (i) a target-weighted loss with warm-up tied to the square root of thesource/target size ratio, (ii) Split Batch Normalization (Split-BN) with sharedaffine parameters and per-domain running statistics, and (iii) a parameter-freelogit-level Radial Basis Function kernel Maximum Mean Discrepancy (RBF-MMD)term using the median-bandwidth heuristic. Implemented on an EEG Conformer,AS-MMD is backbone-agnostic and leaves the inference-time model unchanged.Across both transfer directions, it outperforms target-only and pooled training(Active Visual Oddball: accuracy/AUC 0.66/0.74; ERP CORE P3: 0.61/0.65), withgains over pooling significant under corrected paired t-tests. Ablationsattribute improvements to all three components.</description>
      <author>example@mail.com (Weiyu Chen, Arnaud Delorme)</author>
      <guid isPermaLink="false">2510.21969v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>An unsupervised tour through the hidden pathways of deep neural networks</title>
      <link>http://arxiv.org/abs/2510.21582v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究旨在深入理解深度人工神经网络创建有意义表示并实现泛化的内部机制。研究重点关注使用无监督学习工具描述隐藏表示的语义内容，并利用数据的低维结构。论文介绍了Gride方法用于估计数据内在维度，研究了深度神经网络中隐藏层概率密度的演变，以及探讨了深度神经网络中的泛化问题。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络虽然取得了显著成功，但其内部工作机制和泛化能力仍不完全清楚。理解神经网络如何创建有意义的表示以及它们如何能够泛化到未见数据是深度学习领域的关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提高对深度人工神经网络创建有意义表示和泛化能力的内部机制的理解。重点在于使用无监督学习工具描述隐藏表示的语义内容，并利用数据的低维结构。&lt;h4&gt;方法&lt;/h4&gt;1. 开发了Gride方法，用于估计数据内在维度作为尺度的显式函数，无需降采样数据集。2. 研究了最先进深度神经网络中隐藏层概率密度的演变。3. 研究了深度神经网络中的泛化问题，特别是添加参数如何提高泛化性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. Gride方法基于严格的分布结果，能够量化估计的不确定性，且计算效率高。2. 深度神经网络的初始层生成单模态概率密度，消除与分类无关的结构；后续层中密度峰以分层方式出现，反映概念的语义层次。3. 宽神经网络学习冗余表示而非对虚假相关性过拟合，冗余神经元仅在网络被正则化且训练误差为零时出现。&lt;h4&gt;结论&lt;/h4&gt;深度神经网络通过分层结构创建有意义的表示，初始层消除无关结构，后续层建立语义层次。网络的泛化能力与冗余表示学习相关，而非传统的偏差-方差权衡。Gride方法为分析数据内在结构提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文的目的是提高我们对深度人工神经网络创建有意义表示并能够泛化的内部机制的理解。我们专注于使用无监督学习工具描述隐藏表示的语义内容的挑战，这些工具部分由我们开发并在本论文中描述，它们允许利用数据的低维结构。第2章介绍了Gride，一种允许将数据的内在维度估计为尺度的显式函数的方法，而无需对数据集进行任何降采样。我们的方法基于严格的分布结果，能够量化估计的不确定性。此外，我们的方法简单且计算高效，因为它仅依赖于最近数据点之间的距离。在第3章中，我们研究了一些最先进的深度神经网络中隐藏层概率密度的演变。我们发现初始层生成单模态概率密度，消除任何与分类无关的结构。在后续层中，密度峰以分层方式出现，反映概念的语义层次结构。这个过程在输出层的概率密度中留下了足迹，其中峰的地形可以重建类别的语义关系。在第4章中，我们研究了深度神经网络中的泛化问题：向插值其训练数据的网络添加参数通常会提高其泛化性能，这与经典的偏差-方差权衡相悖。我们证明宽神经网络学习冗余表示，而不是对虚假相关性过拟合，并且只有当网络被正则化且训练误差为零时，冗余神经元才会出现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of this thesis is to improve our understanding of the internalmechanisms by which deep artificial neural networks create meaningfulrepresentations and are able to generalize. We focus on the challenge ofcharacterizing the semantic content of the hidden representations withunsupervised learning tools, partially developed by us and described in thisthesis, which allow harnessing the low-dimensional structure of the data.Chapter 2. introduces Gride, a method that allows estimating the intrinsicdimension of the data as an explicit function of the scale without performingany decimation of the data set. Our approach is based on rigorousdistributional results that enable the quantification of uncertainty of theestimates. Moreover, our method is simple and computationally efficient sinceit relies only on the distances among nearest data points. In Chapter 3, westudy the evolution of the probability density across the hidden layers in somestate-of-the-art deep neural networks. We find that the initial layers generatea unimodal probability density getting rid of any structure irrelevant toclassification. In subsequent layers, density peaks arise in a hierarchicalfashion that mirrors the semantic hierarchy of the concepts. This processleaves a footprint in the probability density of the output layer, where thetopography of the peaks allows reconstructing the semantic relationships of thecategories. In Chapter 4, we study the problem of generalization in deep neuralnetworks: adding parameters to a network that interpolates its training datawill typically improve its generalization performance, at odds with theclassical bias-variance trade-off. We show that wide neural networks learnredundant representations instead of overfitting to spurious correlation andthat redundant neurons appear only if the network is regularized and thetraining error is zero.</description>
      <author>example@mail.com (Diego Doimo)</author>
      <guid isPermaLink="false">2510.21582v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Cost-Sensitive Freeze-thaw Bayesian Optimization for Efficient Hyperparameter Tuning</title>
      <link>http://arxiv.org/abs/2510.21379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于冻结-解冻贝叶斯优化的成本敏感超参数优化方法，通过引入效用函数、新的获取函数和停止准则，实现了在成本和性能之间的动态权衡，并通过迁移学习提高了样本效率。&lt;h4&gt;背景&lt;/h4&gt;研究基于冻结-解冻贝叶斯优化的成本敏感超参数优化问题，关注用户在预期性能改进相对于额外计算成本不够满意时提前停止HPO过程的场景。&lt;h4&gt;目的&lt;/h4&gt;引入描述成本与性能之间权衡的效用函数，结合新的获取函数和停止准则，动态选择最优配置并自动停止HPO过程，同时通过迁移学习提高样本效率。&lt;h4&gt;方法&lt;/h4&gt;提出成本敏感HPO方法，引入效用函数，设计新的获取函数和停止准则，使用迁移学习开发专门的代理模型，提高冻结-解冻方法的样本效率。&lt;h4&gt;主要发现&lt;/h4&gt;在多保真度HPO基准测试上验证了算法性能，优于所有考虑的冻结-解冻BO和迁移-BO基线方法，实现了成本和性能之间显著更好的权衡。&lt;h4&gt;结论&lt;/h4&gt;所提方法在成本敏感HPO问题上表现出色，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们解决了基于冻结-解冻贝叶斯优化（BO）的成本敏感超参数优化（HPO）问题。具体而言，我们假设一种场景，即当预期性能改进相对于额外计算成本不够令人满意时，用户希望提前停止HPO过程。受此场景启发，我们在冻结-解冻框架中引入了'效用'，这是一个描述成本与性能之间权衡的函数，可以从用户偏好数据中估计。这个效用函数结合我们新的获取函数和停止准则，使我们能够动态继续训练我们预期未来效用最大化的配置，并在效用最大值附近自动停止HPO过程。此外，我们通过迁移学习改进了现有冻结-解冻方法的样本效率，为成本敏感HPO问题开发了专门的代理模型。我们在既定的多保真度HPO基准上验证了我们的算法，并表明它优于我们考虑的所有先前冻结-解冻BO和迁移-BO基线方法，同时实现了成本和性能之间显著更好的权衡。我们的代码已在https://github.com/db-Lee/CFBO公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we address the problem of \emph{cost-sensitive} hyperparameteroptimization (HPO) built upon freeze-thaw Bayesian optimization (BO).Specifically, we assume a scenario where users want to early-stop the HPOprocess when the expected performance improvement is not satisfactory withrespect to the additional computational cost. Motivated by this scenario, weintroduce \emph{utility} in the freeze-thaw framework, a function describingthe trade-off between the cost and performance that can be estimated from theuser's preference data. This utility function, combined with our novelacquisition function and stopping criterion, allows us to dynamically continuetraining the configuration that we expect to maximally improve the utility inthe future, and also automatically stop the HPO process around the maximumutility. Further, we improve the sample efficiency of existing freeze-thawmethods with transfer learning to develop a specialized surrogate model for thecost-sensitive HPO problem. We validate our algorithm on establishedmulti-fidelity HPO benchmarks and show that it outperforms all the previousfreeze-thaw BO and transfer-BO baselines we consider, while achieving asignificantly better trade-off between the cost and performance. Our code ispublicly available at https://github.com/db-Lee/CFBO.</description>
      <author>example@mail.com (Dong Bok Lee, Aoxuan Silvia Zhang, Byungjoo Kim, Junhyeon Park, Steven Adriaensen, Juho Lee, Sung Ju Hwang, Hae Beom Lee)</author>
      <guid isPermaLink="false">2510.21379v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>$α$-LoRA: Effective Fine-Tuning via Base Model Rescaling</title>
      <link>http://arxiv.org/abs/2510.21345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一类新的用于迁移学习的重参数化方法，旨在提高微调模型的泛化能力，并通过理论分析和实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;微调被证明是使预训练模型在少量数据样本上在新任务上表现更好的有效方法，其中重参数化方法是最广泛使用的方法之一。&lt;h4&gt;目的&lt;/h4&gt;设计一类新的重参数化方法，以增强微调模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出一类新的重参数化方法，在高维二分类设置中使用随机矩阵理论工具建立其有效性，并通过微调大型语言模型等实验进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的重参数化方法在高维二分类任务中表现有效，且通过微调LLMs的实验进一步验证了理论发现。&lt;h4&gt;结论&lt;/h4&gt;新提出的重参数化方法能够有效提高微调模型的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;微调已被证明是使预训练模型在少量数据样本上在新任务上表现更好的有效方法。其中最广泛使用的方法是重参数化方法，它们通过添加一个额外的可训练权重矩阵来更新目标模块的冻结权重矩阵。最突出的例子是低秩适应(LoRA)，近年来受到了广泛关注。在本文中，我们介绍了一类用于迁移学习的新型重参数化方法，旨在提高微调模型的泛化能力。我们使用随机矩阵理论工具在高维二分类设置中建立了该方法的有效性，并通过更真实的实验（如微调大型语言模型）进一步验证了我们的理论发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning has proven to be highly effective in adapting pre-trained modelsto perform better on new desired tasks with minimal data samples. Among themost widely used approaches are reparameterization methods, which update atarget module by augmenting its frozen weight matrix with an additionaltrainable weight matrix. The most prominent example is Low Rank Adaption(LoRA), which gained significant attention in recent years. In this paper, weintroduce a new class of reparameterization methods for transfer learning,designed to enhance the generalization ability of fine-tuned models. Weestablish the effectiveness of our approach in a high-dimensional binaryclassification setting using tools from Random Matrix Theory, and furthervalidate our theoretical findings through more realistic experiments, such asfine-tuning LLMs.</description>
      <author>example@mail.com (Aymane El Firdoussi, El Mahdi Chayti, Mohamed El Amine Seddik, Martin Jaggi)</author>
      <guid isPermaLink="false">2510.21345v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Graph Mixture of Residual Experts: Unsupervised Learning on Diverse Graphs with Heterogeneous Specialization</title>
      <link>http://arxiv.org/abs/2510.21207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出ADaMoRE框架，解决了图神经网络在适应多样化图结构方面的挑战，通过无监督训练实现了异构专家的有效组合，在各种任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;图神经网络面临基本适应性挑战：固定的消息传递架构难以应对现实世界图的巨大多样性，最优计算策略因局部结构和任务而异。现有图专家混合方法依赖监督信号且训练异构专家时存在不稳定性。&lt;h4&gt;目的&lt;/h4&gt;引入ADaMoRE框架，实现在图上进行异构专家混合的稳健、完全无监督训练。&lt;h4&gt;方法&lt;/h4&gt;ADaMoRE采用骨干-残差专家架构，基础编码器提供稳定性，残差专家捕获不同计算模式；结构感知门控网络执行细粒度节点路由；通过统一无监督目标进行端到端训练，结合重建任务和信息论多样性正则化器强制专家功能专业化。&lt;h4&gt;主要发现&lt;/h4&gt;在16个基准测试上验证了ADaMoRE在无监督节点分类和少样本学习方面的最先进性能，以及优越的泛化能力、训练效率和更快收敛速度。&lt;h4&gt;结论&lt;/h4&gt;ADaMoRE框架通过无监督训练有效解决了图神经网络适应性问题，在多样化图和任务上展现出卓越性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)面临一个基本的适应性挑战：它们固定的消息传递架构难以应对现实世界图的巨大多样性，而最优的计算策略因局部结构和任务的不同而异。尽管专家混合(MoE)为适应性提供了一条有前景的路径，但现有的图MoE方法仍然依赖于监督信号，并且在训练异构专家时存在不稳定性。我们引入ADaMoRE(Adaptive Mixture of Residual Experts)，这是一个原则性框架，能够在图上实现异构MoE的稳健、完全无监督训练。ADaMoRE采用骨干-残差专家架构，其中基础编码器提供稳定性，而专门的残差专家捕获不同的计算模式。一个结构感知的门控网络执行细粒度的节点路由。整个架构通过统一的无监督目标进行端到端训练，该目标结合了主要的重建任务和信息论多样性正则化器，以明确强制专家之间的功能专业化。理论分析证实了他们的设计提高了数据效率和训练稳定性。在16个基准测试上的广泛评估验证了ADaMoRE在无监督节点分类和少样本学习方面的最先进性能，以及在多样化图和任务上的优越泛化能力、训练效率和更快收敛速度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) face a fundamental adaptability challenge: theirfixed message-passing architectures struggle with the immense diversity ofreal-world graphs, where optimal computational strategies vary by localstructure and task. While Mixture-of-Experts (MoE) offers a promising pathwayto adaptability, existing graph MoE methods remain constrained by theirreliance on supervised signals and instability when training heterogeneousexperts. We introduce ADaMoRE (Adaptive Mixture of Residual Experts), aprincipled framework that enables robust, fully unsupervised training ofheterogeneous MoE on graphs. ADaMoRE employs a backbone-residual expertarchitecture where foundational encoders provide stability while specializedresidual experts capture diverse computational patterns. A structurally-awaregating network performs fine-grained node routing. The entire architecture istrained end-to-end using a unified unsupervised objective, which integrates aprimary reconstruction task with an information-theoretic diversity regularizerto explicitly enforce functional specialization among the experts. Theoreticalanalysis confirms our design improves data efficiency and training stability.Extensive evaluation across 16 benchmarks validates ADaMoRE's state-of-the-artperformance in unsupervised node classification and few-shot learning,alongside superior generalization, training efficiency, and faster convergenceon diverse graphs and tasks.</description>
      <author>example@mail.com (Yunlong Chu, Minglai Shao, Zengyi Wo, Bing Hao, Yuhang Liu, Ruijie Wang, Jianxin Li)</author>
      <guid isPermaLink="false">2510.21207v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>CIPHER: Scalable Time Series Analysis for Physical Sciences with Application to Solar Wind Phenomena</title>
      <link>http://arxiv.org/abs/2510.21022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures, Machine Learning and the Physical Sciences  Workshop @ NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为CIPHER的框架，用于加速物理学中复杂时间序列的大规模标注。该框架结合了可索引符号聚合近似、基于密度的聚类和人类专家验证，解决了物理科学中时间序列标注稀缺、成本高且不一致的问题。&lt;h4&gt;背景&lt;/h4&gt;在物理科学中，时间序列的标注或分类是一个持续的挑战。专家标注稀缺、成本高且往往不一致，但稳健的标注对于启用机器学习模型进行理解、预测和预测至关重要。&lt;h4&gt;目的&lt;/h4&gt;设计一个框架来加速物理学中复杂时间序列的大规模标注，解决专家标注稀缺的问题。&lt;h4&gt;方法&lt;/h4&gt;CIPHER框架集成了以下组件：1. 可索引符号聚合近似用于可解释的压缩和索引；2. 基于密度的聚类来分组重复出现的现象；3. 人类在环中的步骤用于高效的专家验证。领域科学家对代表性样本进行标注，然后将这些标注传播到整个集群中，产生系统化的、可扩展的分类。&lt;h4&gt;主要发现&lt;/h4&gt;作者在OMNI数据中分类太阳风现象的任务上评估了CIPHER，这是空间天气研究中的一个核心挑战。结果表明，该框架能够识别有意义的现象，如日冕物质抛射和流相互作用区域。&lt;h4&gt;结论&lt;/h4&gt;CIPHER展示了一种结合符号表示、无监督学习和专业知识的通用策略，以解决物理科学中时间序列标注稀缺的问题。研究所用的代码和配置文件是公开的，以支持可重复性。&lt;h4&gt;翻译&lt;/h4&gt;时间序列的标注或分类在物理科学中是一个持续的挑战，其中专家标注稀缺、成本高昂且往往不一致。然而，稳健的标注对于启用机器学习模型进行理解、预测和预测至关重要。我们提出了'聚类与索引管道及人类评估用于识别'，这是一个旨在加速物理学中复杂时间序列大规模标注的框架。CIPHER集成了可索引符号聚合近似用于可解释的压缩和索引，基于密度的聚类来分组重复出现的现象，以及一个人机交互的步骤用于高效的专家验证。代表性样本由领域科学家标注，这些标注被传播到整个集群中，产生系统化、可扩展的分类。我们在OMNI数据中分类太阳风现象的任务上评估了CIPHER，这是空间天气研究中的一个核心挑战，结果表明该框架能够识别有意义的现象，如日冕物质抛射和流相互作用区域。除了这个案例研究，CIPHER强调了一种结合符号表示、无监督学习和专业知识的通用策略，以解决物理科学中时间序列的标注稀缺问题。本研究使用的代码和配置文件是公开的，以支持可重复性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Labeling or classifying time series is a persistent challenge in the physicalsciences, where expert annotations are scarce, costly, and often inconsistent.Yet robust labeling is essential to enable machine learning models forunderstanding, prediction, and forecasting. We present the \textit{Clusteringand Indexation Pipeline with Human Evaluation for Recognition} (CIPHER), aframework designed to accelerate large-scale labeling of complex time series inphysics. CIPHER integrates \textit{indexable Symbolic Aggregate approXimation}(iSAX) for interpretable compression and indexing, density-based clustering(HDBSCAN) to group recurring phenomena, and a human-in-the-loop step forefficient expert validation. Representative samples are labeled by domainscientists, and these annotations are propagated across clusters to yieldsystematic, scalable classifications. We evaluate CIPHER on the task ofclassifying solar wind phenomena in OMNI data, a central challenge in spaceweather research, showing that the framework recovers meaningful phenomena suchas coronal mass ejections and stream interaction regions. Beyond this casestudy, CIPHER highlights a general strategy for combining symbolicrepresentations, unsupervised learning, and expert knowledge to address labelscarcity in time series across the physical sciences. The code andconfiguration files used in this study are publicly available to supportreproducibility.</description>
      <author>example@mail.com (Jasmine R. Kobayashi, Daniela Martin, Valmir P Moraes Filho, Connor O'Brien, Jinsu Hong, Sudeshna Boro Saikia, Hala Lamdouar, Nathan D. Miles, Marcella Scoczynski, Mavis Stone, Sairam Sundaresan, Anna Jungbluth, Andrés Muñoz-Jaramillo, Evangelia Samara, Joseph Gallego)</author>
      <guid isPermaLink="false">2510.21022v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Memory Constrained Dynamic Subnetwork Update for Transfer Learning</title>
      <link>http://arxiv.org/abs/2510.20979v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MeDyate的框架，用于解决在设备上神经网络训练面临的内存限制问题，通过动态子网络适应方法实现了在严格内存预算下的有效微调。&lt;h4&gt;背景&lt;/h4&gt;在设备上的神经网络训练面临严重的内存限制，这些限制阻碍了预训练模型对下游任务的适应。&lt;h4&gt;目的&lt;/h4&gt;提出一个有理论依据的框架，用于内存受限的动态子网络适应，实现在严格内存预算下的有效微调。&lt;h4&gt;方法&lt;/h4&gt;MeDyate框架包含两个关键创新：LaRa（Layer Ranking）作为改进的层重要性度量实现有原则的层预选择，以及动态通道采样策略利用微调过程中通道重要性分布的时间稳定性；根据重要性加权概率在周期之间动态重新采样通道，确保在尊重内存预算的同时全面探索参数空间。&lt;h4&gt;主要发现&lt;/h4&gt;在广泛的任务和架构上进行的大量评估表明，MeDyate在极端内存限制下实现了最先进的性能，一致优于现有的静态和动态方法，同时保持高计算效率。&lt;h4&gt;结论&lt;/h4&gt;该方法代表了推动设备上高效学习的重要一步，证明了在内存预算低至几百KB RAM的情况下进行有效微调的可能性。&lt;h4&gt;翻译&lt;/h4&gt;设备上的神经网络训练面临关键的内存限制，这些限制阻碍了预训练模型对下游任务的适应。我们提出了MeDyate，一个有理论依据的框架，用于内存受限的动态子网络适应。我们的方法引入了两个关键创新：LaRa（Layer Ranking），一种改进的层重要性度量，能够实现有原则的层预选择，以及动态通道采样策略，利用微调过程中通道重要性分布的时间稳定性。MeDyate根据重要性加权概率在周期之间动态重新采样通道，确保在尊重严格内存预算的同时全面探索参数空间。在广泛的任务和架构上进行的大量评估表明，MeDyate在极端内存限制下实现了最先进的性能，一致优于现有的静态和动态方法，同时保持高计算效率。我们的方法代表了推动设备上高效学习的重要一步，证明了在内存预算低至几百KB RAM的情况下进行有效微调的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; On-device neural network training faces critical memory constraints thatlimit the adaptation of pre-trained models to downstream tasks. We presentMeDyate, a theoretically-grounded framework for memory-constrained dynamicsubnetwork adaptation. Our approach introduces two key innovations: LaRa (LayerRanking), an improved layer importance metric that enables principled layerpre-selection, and a dynamic channel sampling strategy that exploits thetemporal stability of channel importance distributions during fine-tuning.MeDyate dynamically resamples channels between epochs according toimportance-weighted probabilities, ensuring comprehensive parameter spaceexploration while respecting strict memory budgets. Extensive evaluation acrossa large panel of tasks and architectures demonstrates that MeDyate achievesstate-of-the-art performance under extreme memory constraints, consistentlyoutperforming existing static and dynamic approaches while maintaining highcomputational efficiency. Our method represents a significant step towardsenabling efficient on-device learning by demonstrating effective fine-tuningwith memory budgets as low as a few hundred kB of RAM.</description>
      <author>example@mail.com (Aël Quélennec, Pavlo Mozharovskyi, Van-Tam Nguyen, Enzo Tartaglione)</author>
      <guid isPermaLink="false">2510.20979v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes</title>
      <link>http://arxiv.org/abs/2510.23151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种自适应门控融合方法，通过选择性整合跨模态知识，在复杂场景中实现了更鲁棒的3D目标检测。&lt;h4&gt;背景&lt;/h4&gt;多模态相机-激光雷达融合技术在3D目标检测中应用广泛，但在传感器退化或环境干扰等具有挑战性的场景中，现有方法性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自适应门控融合方法，通过识别可靠的模式来选择性地整合跨模态知识，以实现复杂场景中的鲁棒检测。&lt;h4&gt;方法&lt;/h4&gt;将每个模态的特征投影到统一的BEV空间并使用基于窗口的注意力机制增强特征，然后设计基于跨模态注意力的自适应门控融合模块来整合这些特征，同时构建了一个名为Excavator3D（E3D）的新数据集，专注于具有挑战性的挖掘机操作场景。&lt;h4&gt;主要发现&lt;/h4&gt;在标准的KITTI数据集上达到93.92%的准确率，在具有挑战性的E3D数据集上比基线方法高出24.88%，证明了在复杂工业场景中对不可靠模态信息具有优越的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;提出的AG-Fusion方法在复杂场景中表现优异，特别是在处理不可靠模态信息时具有更强的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;多模态相机-激光雷达融合技术在3D目标检测中已得到广泛应用，并显示出令人鼓舞的性能表现。然而，在传感器退化或环境干扰等具有挑战性的场景中，现有方法表现出显著的性能下降。我们提出了一种新颖的自适应门控融合方法，通过识别可靠的模式来选择性地整合跨模态知识，从而在复杂场景中实现鲁棒检测。具体而言，我们首先将每个模态的特征投影到统一的BEV空间，并使用基于窗口的注意力机制增强这些特征。随后，我们设计了一个基于跨模态注意力的自适应门控融合模块，将这些特征整合为可靠的BEV表示，以应对具有挑战性的环境。此外，我们构建了一个名为Excavator3D（E3D）的新数据集，专注于具有挑战性的挖掘机操作场景，以在复杂条件下评估性能。我们的方法不仅在标准的KITTI数据集上实现了93.92%的准确率，具有竞争力的性能，而且在具有挑战性的E3D数据集上比基线方法高出24.88%，证明了在复杂工业场景中对不可靠模态信息具有优越的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态相机-激光雷达融合技术在复杂场景（如挖掘机操作环境）中性能显著下降的问题。这些问题在现实中很重要，因为灰尘、光照变化导致图像退化，机械部件遮挡和金属表面反射干扰点云数据，这些挑战限制了自动驾驶和工业自动化技术在真实世界中的应用，现有方法在标准数据集上表现良好但在复杂场景中性能大幅下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有BEV融合技术的局限性，指出它们主要依赖卷积操作，无法自适应建模跨模态交互。针对工业场景中的挑战，作者在BEVFusion基础上进行改进，借鉴了Swin Transformer的窗口自注意力机制设计SA-E模块增强特征，并引入双向交叉注意力和自适应门控机制实现更智能的融合。整体设计思路是根据场景特点动态调整不同模态的贡献权重。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自适应选择和融合来自相机和激光雷达的可靠信息，提高在复杂场景中的3D目标检测性能。方法首先使用窗口自注意力增强每个模态的特征；然后通过双向交叉注意力和自适应门控机制融合这些特征，根据场景特点动态调整不同模态的贡献权重；最后将所有特征流集成到统一的BEV表示中进行3D检测。整体流程包括特征提取、增强特征提取、跨模态门控融合和多级特征聚合四个主要步骤。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 自适应门控多模态融合(AG-Fusion)框架，结合双向交叉注意力和空间自适应门控机制；2) 基于窗口的自注意力增强(SA-E)模块，有效降低计算复杂度；3) 构建了专门的挖掘机3D检测数据集(E3D)。相比之前的工作，该方法不再依赖静态或局部约束的特征聚合，能够处理遮挡和传感器噪声，在复杂工业场景中表现显著优于现有方法，在E3D数据集上比基线方法提升24.88%的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种自适应门控多模态融合方法，通过动态整合相机和激光雷达的可靠信息，显著提升了在复杂工业场景中的3D目标检测性能，并构建了专门的挖掘机3D检测数据集验证方法的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal camera-LiDAR fusion technology has found extensive application in3D object detection, demonstrating encouraging performance. However, existingmethods exhibit significant performance degradation in challenging scenarioscharacterized by sensor degradation or environmental disturbances. We propose anovel Adaptive Gated Fusion (AG-Fusion) approach that selectively integratescross-modal knowledge by identifying reliable patterns for robust detection incomplex scenes. Specifically, we first project features from each modality intoa unified BEV space and enhance them using a window-based attention mechanism.Subsequently, an adaptive gated fusion module based on cross-modal attention isdesigned to integrate these features into reliable BEV representations robustto challenging environments. Furthermore, we construct a new dataset namedExcavator3D (E3D) focusing on challenging excavator operation scenarios tobenchmark performance in complex conditions. Our method not only achievescompetitive performance on the standard KITTI dataset with 93.92% accuracy, butalso significantly outperforms the baseline by 24.88% on the challenging E3Ddataset, demonstrating superior robustness to unreliable modal information incomplex industrial scenes.</description>
      <author>example@mail.com (Sixian Liu, Chen Xu, Qiang Wang, Donghai Shi, Yiwen Li)</author>
      <guid isPermaLink="false">2510.23151v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in Traffic Scenarios</title>
      <link>http://arxiv.org/abs/2510.23144v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个名为DQ3D的深度引导查询生成器，用于解决3D目标检测中的参考点采样问题，并通过混合注意力机制处理部分遮挡目标，在nuScenes数据集上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;多视角图像中的3D目标检测在交通场景中近年来受到广泛关注。现有方法依赖于从3D参考点生成的目标查询来定位物体。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中参考点可能远离目标物体导致误检的问题，并处理当前帧中部分遮挡的目标物体。&lt;h4&gt;方法&lt;/h4&gt;提出了深度引导查询生成器(DQ3D)，利用深度信息和2D检测确保参考点从物体表面或内部采样；引入混合注意力机制，将历史检测结果与深度引导查询融合，形成混合查询。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的评估表明，该方法在平均精度(mAP)上比基线提高了6.3%，在NuScenes检测分数(NDS)上提高了4.3%。&lt;h4&gt;结论&lt;/h4&gt;深度引导查询生成器和混合注意力机制能有效提高3D目标检测的性能，特别是在处理参考点采样和遮挡物体方面。&lt;h4&gt;翻译&lt;/h4&gt;近年来，交通场景中基于多视角图像的3D目标检测受到了广泛关注。许多现有方法依赖于从3D参考点生成的目标查询来定位物体。然而，这些方法的一个局限性是，一些参考点通常远离目标物体，这可能导致误检。在本文中，我们提出了一个用于3D目标检测的深度引导查询生成器(DQ3D)，它利用深度信息和2D检测确保参考点从物体表面或内部采样。此外，为了解决当前帧中部分遮挡的物体，我们引入了一种混合注意力机制，将历史检测结果与深度引导查询融合，从而形成混合查询。在nuScenes数据集上的评估表明，我们的方法在平均精度(mAP)上比基线提高了6.3%，在NuScenes检测分数(NDS)上提高了4.3%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection from multi-view images in traffic scenarios has garneredsignificant attention in recent years. Many existing approaches rely on objectqueries that are generated from 3D reference points to localize objects.However, a limitation of these methods is that some reference points are oftenfar from the target object, which can lead to false positive detections. Inthis paper, we propose a depth-guided query generator for 3D object detection(DQ3D) that leverages depth information and 2D detections to ensure thatreference points are sampled from the surface or interior of the object.Furthermore, to address partially occluded objects in current frame, weintroduce a hybrid attention mechanism that fuses historical detection resultswith depth-guided queries, thereby forming hybrid queries. Evaluation on thenuScenes dataset demonstrates that our method outperforms the baseline by 6.3\%in terms of mean Average Precision (mAP) and 4.3\% in the NuScenes DetectionScore (NDS).</description>
      <author>example@mail.com (Ziyu Wang, Wenhao Li, Ji Wu)</author>
      <guid isPermaLink="false">2510.23144v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation</title>
      <link>http://arxiv.org/abs/2509.16500v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了自动驾驶系统中合成视频数据的几何失真问题，提出了强化学习与几何反馈(RLGF)方法，显著提高了合成数据的几何准确性和3D目标检测性能。&lt;h4&gt;背景&lt;/h4&gt;合成数据对推进自动驾驶系统至关重要，但当前最先进的视频生成模型尽管视觉上逼真，却存在微妙的几何失真，限制了其在下游感知任务中的应用。研究显示，使用合成数据与真实数据进行3D目标检测时存在显著性能差距。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来减少合成视频数据中的几何失真，提高其在自动驾驶感知任务中的效用，缩小合成数据与真实数据之间的性能差距。&lt;h4&gt;方法&lt;/h4&gt;研究引入了'带有几何反馈的强化学习'(RLGF)，该方法通过整合来自专用潜在空间自动驾驶感知模型的奖励来优化视频扩散模型。其核心组件包括：1) 潜在空间窗口优化技术，用于在扩散过程中提供针对性反馈；2) 分层几何奖励(HGR)系统，为点线面对齐和场景占用一致性提供多级奖励。研究还提出了GeoScores来量化几何失真。&lt;h4&gt;主要发现&lt;/h4&gt;应用RLGF到DiVE模型上，在nuScenes数据集上显著减少了几何误差（例如：消失点误差降低21%，深度误差降低57%），并大幅提高了3D目标检测mAP达12.7%，缩小了与真实数据性能的差距。&lt;h4&gt;结论&lt;/h4&gt;RLGF为自动驾驶开发提供了一种即插即用的解决方案，能够生成几何准确可靠的合成视频，有助于推进自动驾驶系统的训练和测试。&lt;h4&gt;翻译&lt;/h4&gt;合成数据对推进自动驾驶系统至关重要，但当前最先进的视频生成模型尽管视觉上逼真，却存在微妙的几何失真，限制了其在下游感知任务中的应用。研究确定了并量化了这一关键问题，展示了使用合成数据与真实数据进行3D目标检测时的显著性能差距。为此，研究引入了'带有几何反馈的强化学习'(RLGF)。RLGF通过整合来自专用潜在空间自动驾驶感知模型的奖励，独特地优化了视频扩散模型。其核心组件包括：1) 用于在扩散过程中提供针对性反馈的高效潜在空间窗口优化技术；2) 提供点线面对齐和场景占用一致性多级奖励的分层几何奖励(HGR)系统。为了量化这些失真，研究提出了GeoScores。将RLGF应用于nuScenes上的DiVE等模型，显著减少了几何误差（例如：消失点误差降低21%，深度误差降低57%），并大幅提高了3D目标检测mAP达12.7%，缩小了与真实数据性能的差距。RLGF为生成几何准确可靠的合成视频用于自动驾驶开发提供了一种即插即用的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶视频生成中存在的几何失真问题。虽然当前视频生成模型在视觉上看起来很真实，但它们生成的视频中存在微妙的几何扭曲，这些扭曲限制了它们在下游感知任务中的实用性。这个问题很重要，因为自动驾驶系统需要大量高质量的合成数据进行训练和验证，而这些几何扭曲会导致基于合成数据训练的3D目标检测等下游任务性能显著下降，影响自动驾驶系统的可靠性和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别并量化了当前视频生成模型中的几何失真问题，提出了GeoScores指标来评估这些失真。通过实验发现当前模型虽然保留了2D外观但无法捕捉准确的3D场景结构，主要原因是潜在的几何不一致性。作者借鉴了强化学习从人类反馈的成功经验（如LLMs中的PPO或DPO）、视频扩散模型的研究以及自动驾驶感知模型的设计。但作者指出现有方法主要依赖像素级对齐，无法显式强制遵守复杂的底层几何原理，因此设计了RLGF框架，利用专门的预训练自动驾驶感知模型作为奖励提供者，确保几何保真度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入基于感知模型的几何反馈来增强视频扩散模型的几何完整性，使生成的自动驾驶场景视频不仅在视觉上逼真，而且在几何结构上准确可靠。整体流程包括：1)预训练两个专门的感知模型（潜在几何感知模型Pgeo和潜在占用预测模型Pocc）；2)设计分层几何奖励（HGR）系统，提供点线面几何反馈和场景级占用反馈；3)实现潜在空间窗口化优化，在扩散过程的中间步骤提供反馈；4)使用强化学习微调预训练的视频扩散模型，将感知模型作为奖励提供者，通过LoRA高效更新模型参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次系统量化了自动驾驶视频生成中的几何失真问题，提出GeoScores评估指标；2)引入强化学习与几何反馈（RLGF）框架，将感知模型驱动的几何空间约束直接注入视频生成过程；3)提出潜在空间窗口化优化技术，在扩散过程的中间步骤而非仅最终输出提供反馈；4)设计分层几何奖励（HGR）系统，结合点线面几何反馈和场景级占用反馈。相比之前的工作，RLGF专注于几何完整性而非仅像素级视觉保真度，使用具体的、可解释的几何约束而非人类偏好或高层次奖励信号，并且是即插即用的解决方案，可与现有视频扩散模型集成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了RLGF框架，通过引入基于感知模型的几何反馈强化学习，有效解决了自动驾驶视频生成中的几何失真问题，显著提升了合成数据的3D感知任务性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic data is crucial for advancing autonomous driving (AD) systems, yetcurrent state-of-the-art video generation models, despite their visual realism,suffer from subtle geometric distortions that limit their utility fordownstream perception tasks. We identify and quantify this critical issue,demonstrating a significant performance gap in 3D object detection when usingsynthetic versus real data. To address this, we introduce ReinforcementLearning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusionmodels by incorporating rewards from specialized latent-space AD perceptionmodels. Its core components include an efficient Latent-Space WindowingOptimization technique for targeted feedback during diffusion, and aHierarchical Geometric Reward (HGR) system providing multi-level rewards forpoint-line-plane alignment, and scene occupancy coherence. To quantify thesedistortions, we propose GeoScores. Applied to models like DiVE on nuScenes,RLGF substantially reduces geometric errors (e.g., VP error by 21\%, Deptherror by 57\%) and dramatically improves 3D object detection mAP by 12.7\%,narrowing the gap to real-data performance. RLGF offers a plug-and-playsolution for generating geometrically sound and reliable synthetic videos forAD development.</description>
      <author>example@mail.com (Tianyi Yan, Wencheng Han, Xia Zhou, Xueyang Zhang, Kun Zhan, Cheng-zhong Xu, Jianbing Shen)</author>
      <guid isPermaLink="false">2509.16500v2</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning</title>
      <link>http://arxiv.org/abs/2510.23532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted at NeurIPS 2025 D&amp;B track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了NoRA新基准测试，用于评估系统关系推理模型的泛化能力，突破了现有基于路径组合的简化假设。&lt;h4&gt;背景&lt;/h4&gt;设计能够系统化学习的模型是重要挑战，近年已提出多种解决方案，包括神经符号方法、Transformer变体和图神经网络，但现有基准测试过于简化。&lt;h4&gt;目的&lt;/h4&gt;支持神经网络在系统关系推理领域的进一步发展，引入更全面的评估基准。&lt;h4&gt;方法&lt;/h4&gt;开发NoRA基准测试，增加多个难度级别，要求模型超越简单的路径组合推理。&lt;h4&gt;主要发现&lt;/h4&gt;现有基准测试基于推理可简化为关系路径组合的假设，导致模型在这些基准上表现良好但难以泛化到其他场景。&lt;h4&gt;结论&lt;/h4&gt;需要NoRA这样的新基准来更全面地评估模型的真实系统推理能力，推动领域发展。&lt;h4&gt;翻译&lt;/h4&gt;设计能够以系统化方式学习的模型是一个重要且长期存在的挑战。近年来，针对系统关系推理的特定案例提出了多种解决方案，包括神经符号方法、Transformer架构的变体和专门的图神经网络。然而，现有的系统关系推理基准测试基于过于简化的设置，基于推理可以简化为组合关系路径的假设。事实上，这个假设被嵌入到几个最新模型的架构中，导致这些方法在现有基准上表现良好但难以推广到其他设置。为了支持神经网络在系统关系推理领域的进一步发展，我们引入NoRA，一个新的基准测试，它增加了多个难度级别，要求模型超越基于路径的推理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing models that can learn to reason in a systematic way is an importantand long-standing challenge. In recent years, a wide range of solutions havebeen proposed for the specific case of systematic relational reasoning,including Neuro-Symbolic approaches, variants of the Transformer architecture,and specialised Graph Neural Networks. However, existing benchmarks forsystematic relational reasoning focus on an overly simplified setting, based onthe assumption that reasoning can be reduced to composing relational paths. Infact, this assumption is hard-baked into the architecture of several recentmodels, leading to approaches that can perform well on existing benchmarks butare difficult to generalise to other settings. To support further progress inthe field of systematic relational reasoning with neural networks, we introduceNoRA, a new benchmark which adds several levels of difficulty and requiresmodels to go beyond path-based reasoning.</description>
      <author>example@mail.com (Anirban Das, Irtaza Khalid, Rafael Peñaloza, Steven Schockaert)</author>
      <guid isPermaLink="false">2510.23532v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>iPac: Incorporating Intra-image Patch Context into Graph Neural Networks for Medical Image Classification</title>
      <link>http://arxiv.org/abs/2510.23504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in the proceedings of ICONIP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;iPac是一种创新的图神经网络方法，通过引入图像的新型图表示来改进图像分类性能，特别在医学图像分类中表现出色，比基线方法平均提高5%的准确率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为图像处理的一种有前景的范式，但在图像分类任务中的表现受到限制，因为它们对视觉实体之间的底层结构和关系考虑不足。&lt;h4&gt;目的&lt;/h4&gt;提出iPac方法，引入图像的新型图表示，增强图神经网络在医学图像分类中的性能，通过认识到底层结构和关系在医学图像分类中的重要性。&lt;h4&gt;方法&lt;/h4&gt;iPac集成了多个阶段，包括patch分区、特征提取、聚类、图构建和基于图的学习，将这些阶段整合到一个统一的网络中，通过捕获相关特征并将它们组织成簇，构建有意义的图表示，有效封装图像的语义。&lt;h4&gt;主要发现&lt;/h4&gt;在多种医学图像数据集上的实验评估证明了iPac的有效性，与基线方法相比，平均准确率提高了高达5%。&lt;h4&gt;结论&lt;/h4&gt;该方法为图像分类提供了一种通用且灵活的解决方案，特别是在医学图像领域，通过利用图表示并考虑视觉实体之间的固有结构和关系。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为图像处理的一种有前景的范式，但它们在图像分类任务中的表现受到对视觉实体之间底层结构和关系考虑不足的限制。本文提出了iPac，一种通过引入图像的新型图表示来增强图神经网络图像分类的新方法，通过认识到底层结构和关系在医学图像分类中的重要性。iPac将多个阶段（包括patch分区、特征提取、聚类、图构建和基于图的学习）整合到一个统一的网络中，以推进图神经网络图像分类。通过捕获相关特征并将它们组织成簇，我们构建了一个有意义的图表示，有效地封装了图像的语义。在多种医学图像数据集上的实验评估证明了iPac的有效性，与基线方法相比，平均准确率提高了高达5%。我们的方法通过利用图表示并考虑视觉实体之间的固有结构和关系，为图像分类提供了一种通用且灵活的解决方案，特别是在医学图像领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have emerged as a promising paradigm for imageprocessing, yet their performance in image classification tasks is hindered bya limited consideration of the underlying structure and relationships amongvisual entities. This work presents iPac, a novel approach to introduce a newgraph representation of images to enhance graph neural network imageclassification by recognizing the importance of underlying structure andrelationships in medical image classification. iPac integrates various stages,including patch partitioning, feature extraction, clustering, graphconstruction, and graph-based learning, into a unified network to advance graphneural network image classification. By capturing relevant features andorganising them into clusters, we construct a meaningful graph representationthat effectively encapsulates the semantics of the image. Experimentalevaluation on diverse medical image datasets demonstrates the efficacy of iPac,exhibiting an average accuracy improvement of up to 5% over baseline methods.Our approach offers a versatile and generic solution for image classification,particularly in the realm of medical images, by leveraging the graphrepresentation and accounting for the inherent structure and relationshipsamong visual entities.</description>
      <author>example@mail.com (Usama Zidan, Mohamed Gaber, Mohammed M. Abdelsamea)</author>
      <guid isPermaLink="false">2510.23504v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Dual Prompting: Hierarchical Debiasing for Fairness-aware Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.23469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;近年来，通过在无标签图数据上进行自监督学习来预训练图神经网络已成为图学习中的广泛采用范式。虽然这种范式对预训练强大的GNN模型有效，但预训练和下游任务之间通常存在目标差距。图提示通过额外的可学习提示来调整预训练的GNN模型以适应特定下游任务，同时保持预训练的GNN模型冻结。&lt;h4&gt;目的&lt;/h4&gt;解决现有图提示方法在设计提示时往往忽视公平性的问题。预训练的GNN模型会在不同人口统计子群中产生有区别性的节点表示，因为下游图数据在节点属性和图结构中固有地包含偏见。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自适应双提示(ADPrompt)框架，用于增强预训练GNN模型适应下游任务的公平性。设计了自适应特征校正模块，学习自定义的属性提示，在输入层抑制敏感信息，从源头减少偏见。提出了自适应消息校准模块，在每一层生成结构提示，调整来自邻居节点的消息，实现信息流的动态和软校准。联合优化两个提示模块，以适应预训练的GNN同时增强公平性。&lt;h4&gt;主要发现&lt;/h4&gt;在四个数据集上使用四种预训练策略进行了广泛的实验来评估ADPrompt的性能。结果表明，ADPrompt在节点分类任务上优于七种基线方法。&lt;h4&gt;结论&lt;/h4&gt;ADPrompt框架能够有效提升预训练GNN模型在下游任务中的公平性和性能。&lt;h4&gt;翻译&lt;/h4&gt;近年来，通过在无标签图数据上进行自监督学习来预训练图神经网络已成为图学习中的广泛采用范式。虽然这种范式对预训练强大的GNN模型有效，但预训练和下游任务之间通常存在目标差距。为了弥合这一差距，图提示通过额外的可学习提示来调整预训练的GNN模型以适应特定的下游任务，同时保持预训练的GNN模型冻结。由于最近的图提示方法主要关注增强模型在下游任务上的效用，它们在设计提示进行适应时往往忽视了公平性问题。实际上，预训练的GNN模型会在不同人口统计子群中产生有区别性的节点表示，因为下游图数据在节点属性和图结构中固有地包含偏见。为了解决这个问题，我们提出了一个自适应双提示(ADPrompt)框架，用于增强预训练GNN模型适应下游任务的公平性。为了减轻属性偏见，我们设计了一个自适应特征校正模块，学习自定义的属性提示，在输入层抑制敏感信息，从源头减少偏见。之后，我们提出了一个自适应消息校准模块，在每一层生成结构提示，调整来自邻居节点的消息，实现信息流的动态和软校准。最后，ADPrompt联合优化两个提示模块，以适应预训练的GNN同时增强公平性。我们在四个数据集上使用四种预训练策略进行了广泛的实验来评估ADPrompt的性能。结果表明，我们提出的ADPrompt在节点分类任务上优于七种基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, pre-training Graph Neural Networks (GNNs) throughself-supervised learning on unlabeled graph data has emerged as a widelyadopted paradigm in graph learning. Although the paradigm is effective forpre-training powerful GNN models, the objective gap often exists betweenpre-training and downstream tasks. To bridge this gap, graph prompting adaptspre-trained GNN models to specific downstream tasks with extra learnableprompts while keeping the pre-trained GNN models frozen. As recent graphprompting methods largely focus on enhancing model utility on downstream tasks,they often overlook fairness concerns when designing prompts for adaptation. Infact, pre-trained GNN models will produce discriminative node representationsacross demographic subgroups, as downstream graph data inherently containsbiases in both node attributes and graph structures. To address this issue, wepropose an Adaptive Dual Prompting (ADPrompt) framework that enhances fairnessfor adapting pre-trained GNN models to downstream tasks. To mitigate attributebias, we design an Adaptive Feature Rectification module that learns customizedattribute prompts to suppress sensitive information at the input layer,reducing bias at the source. Afterward, we propose an Adaptive MessageCalibration module that generates structure prompts at each layer, which adjustthe message from neighboring nodes to enable dynamic and soft calibration ofthe information flow. Finally, ADPrompt jointly optimizes the two promptingmodules to adapt the pre-trained GNN while enhancing fairness. We conductextensive experiments on four datasets with four pre-training strategies toevaluate the performance of ADPrompt. The results demonstrate that our proposedADPrompt outperforms seven baseline methods on node classification tasks.</description>
      <author>example@mail.com (Yuhan Yang, Xingbo Fu, Jundong Li)</author>
      <guid isPermaLink="false">2510.23469v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Improving Predictions of Molecular Properties with Graph Featurisation and Heterogeneous Ensemble Models</title>
      <link>http://arxiv.org/abs/2510.23428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合图神经网络学习描述符与传统分子描述符的混合方法，通过MetaModel框架整合多种机器学习模型，以提高分子性质预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;分子性质预测是化学信息学和药物发现中的关键任务，现有的方法通常专注于单一类型的特征或模型架构，可能无法充分利用不同方法的优势。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够结合多种描述符和机器学习模型的'最佳结合'方法，以提高分子性质预测的准确性，特别是在广泛的回归和分类任务中。&lt;h4&gt;方法&lt;/h4&gt;1. 引入MetaModel框架聚合来自多样化领先ML模型的预测；2. 设计特征化方案结合任务特定的GNN衍生特征与传统分子描述符；3. 使用图神经网络(GNN)学习分子描述符；4. 结合通用描述符和混合机器学习模型集成。&lt;h4&gt;主要发现&lt;/h4&gt;在所有测试的回归数据集上优于最先进的ChemProp模型；在9个分类数据集中的6个上优于ChemProp模型；包含从ChemProp衍生的GNN特征可以提升集成模型在多个数据集上的性能；该方法在多种类型的分子性质预测任务中表现优异。&lt;h4&gt;结论&lt;/h4&gt;为了在广泛问题上实现最佳性能，结合通用描述符与任务特定的学习特征，并使用多样化的机器学习模型进行预测至关重要。&lt;h4&gt;翻译&lt;/h4&gt;我们通过结合图神经网络(GNN)学习到的分子描述符与通用描述符以及混合的机器学习模型集成，探索了一种用于建模分子性质的'最佳结合'方法。我们引入了一个MetaModel框架来聚合来自多样化领先ML模型的预测。我们提出了一种特征化方案，用于结合任务特定的GNN衍生特征与传统分子描述符。我们证明，在所有测试的回归数据集上以及在9个分类数据集中的6个上，我们的框架优于最先进的ChemProp模型。我们进一步表明，包含从ChemProp衍生的GNN特征可以提升集成模型在多个数据集上的性能，否则这些数据集上的性能会较差。我们得出结论，为了在广泛问题上实现最佳性能，结合通用描述符与任务特定的学习特征，并使用多样化的ML模型进行预测至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1021/acs.jcim.5c01844&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We explore a "best-of-both" approach to modelling molecular properties bycombining learned molecular descriptors from a graph neural network (GNN) withgeneral-purpose descriptors and a mixed ensemble of machine learning (ML)models. We introduce a MetaModel framework to aggregate predictions from adiverse set of leading ML models. We present a featurisation scheme forcombining task-specific GNN-derived features with conventional moleculardescriptors.  We demonstrate that our framework outperforms the cutting-edge ChemProp modelon all regression datasets tested and 6 of 9 classification datasets. Wefurther show that including the GNN features derived from ChemProp boosts theensemble model's performance on several datasets where it otherwise would haveunderperformed. We conclude that to achieve optimal performance across a wideset of problems, it is vital to combine general-purpose descriptors withtask-specific learned features and use a diverse set of ML models to make thepredictions.</description>
      <author>example@mail.com (Michael L. Parker, Samar Mahmoud, Bailey Montefiore, Mario Öeren, Himani Tandon, Charlotte Wharrick, Matthew D. Segall)</author>
      <guid isPermaLink="false">2510.23428v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Framework for Multi-Modal Protein Representation Learning</title>
      <link>http://arxiv.org/abs/2510.23273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DAMPE是一个统一框架，通过最优传输(OT)表示对齐和条件图生成(CGG)信息融合解决了蛋白质功能预测中的跨模态异质性和嘈杂数据问题，在标准基准上取得了优于或匹配最先进方法的性能。&lt;h4&gt;背景&lt;/h4&gt;准确的蛋白质功能预测需要整合异构的内在信号(如序列和结构)与嘈杂的外部上下文(如蛋白质相互作用和GO术语注释)。然而，两个关键挑战阻碍了有效融合：(i)预训练的内在编码器产生的嵌入之间的跨模态分布不匹配；(ii)外部数据的嘈杂关系图降低了基于GNN的信息聚合效果。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一框架来解决蛋白质功能预测中的跨模态分布不匹配和嘈杂数据关系图问题，提高蛋白质功能预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了DAMPE(Diffused and Aligned Multi-modal Protein Embedding)框架，包含两个核心机制：(1)基于最优传输(OT)的表示对齐，建立不同模态内在嵌入空间之间的对应关系，缓解跨模态异质性；(2)基于条件图生成(CGG)的信息融合方法，条件编码器融合对齐的内在嵌入为图重建提供信息提示。理论分析表明CGG目标驱动条件编码器将图感知知识吸收到蛋白质表示中。&lt;h4&gt;主要发现&lt;/h4&gt;DAMPE在标准GO基准测试上优于或匹配了DPFunc等最先进方法，实现了0.002-0.013 pp的AUPR增益和0.004-0.007 pp的Fmax增益。消融研究表明基于OT的对齐贡献了0.043-0.064 pp的AUPR，基于CGG的融合增加了0.005-0.111 pp的Fmax。&lt;h4&gt;结论&lt;/h4&gt;DAMPE为稳健的多模态蛋白质表示学习提供了一种可扩展且理论上有依据的方法，显著提高了蛋白质功能预测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;准确的蛋白质功能预测需要整合异构的内在信号(如序列和结构)与嘈杂的外部上下文(如蛋白质相互作用和GO术语注释)。然而，两个关键挑战阻碍了有效融合：(i)由预训练的内在编码器产生的嵌入之间的跨模态分布不匹配，以及(ii)外部数据的嘈杂关系图降低了基于GNN的信息聚合效果。我们提出了DAMPE(扩散和对齐的多模态蛋白质嵌入)，一个通过两个核心机制解决这些问题的统一框架。首先，我们提出了基于最优传输(OT)的表示对齐，建立了不同模态内在嵌入空间之间的对应关系，有效缓解了跨模态异质性。其次，我们开发了基于条件图生成(CGG)的信息融合方法，其中条件编码器融合对齐的内在嵌入，为图重建提供信息提示。同时，我们的理论分析表明CGG目标驱动条件编码器将其产生的蛋白质表示吸收图感知知识。经验上，DAMPE在标准GO基准测试上优于或匹配了DPFunc等最先进方法，实现了0.002-0.013 pp的AUPR增益和0.004-0.007 pp的Fmax增益。消融研究进一步表明，基于OT的对齐贡献了0.043-0.064 pp的AUPR，而基于CGG的融合增加了0.005-0.111 pp的Fmax。总体而言，DAMPE为稳健的多模态蛋白质表示学习提供了一种可扩展且理论上有依据的方法，显著提高了蛋白质功能预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate protein function prediction requires integrating heterogeneousintrinsic signals (e.g., sequence and structure) with noisy extrinsic contexts(e.g., protein-protein interactions and GO term annotations). However, two keychallenges hinder effective fusion: (i) cross-modal distributional mismatchamong embeddings produced by pre-trained intrinsic encoders, and (ii) noisyrelational graphs of extrinsic data that degrade GNN-based informationaggregation. We propose Diffused and Aligned Multi-modal Protein Embedding(DAMPE), a unified framework that addresses these through two core mechanisms.First, we propose Optimal Transport (OT)-based representation alignment thatestablishes correspondence between intrinsic embedding spaces of differentmodalities, effectively mitigating cross-modal heterogeneity. Second, wedevelop a Conditional Graph Generation (CGG)-based information fusion method,where a condition encoder fuses the aligned intrinsic embeddings to provideinformative cues for graph reconstruction. Meanwhile, our theoretical analysisimplies that the CGG objective drives this condition encoder to absorbgraph-aware knowledge into its produced protein representations. Empirically,DAMPE outperforms or matches state-of-the-art methods such as DPFunc onstandard GO benchmarks, achieving AUPR gains of 0.002-0.013 pp and Fmax gains0.004-0.007 pp. Ablation studies further show that OT-based alignmentcontributes 0.043-0.064 pp AUPR, while CGG-based fusion adds 0.005-0.111 ppFmax. Overall, DAMPE offers a scalable and theoretically grounded approach forrobust multi-modal protein representation learning, substantially enhancingprotein function prediction.</description>
      <author>example@mail.com (Runjie Zheng, Zhen Wang, Anjie Qiao, Jiancong Xie, Jiahua Rao, Yuedong Yang)</author>
      <guid isPermaLink="false">2510.23273v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation</title>
      <link>http://arxiv.org/abs/2510.22942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures, 4 tables, submitted to ICDE 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GTR-Mamba是一种创新的下一个兴趣点推荐框架，通过结合双曲几何和欧几里得切线空间的优势，解决了现有模型难以同时捕捉空间层次结构和时间动态性的问题。&lt;h4&gt;背景&lt;/h4&gt;下一个兴趣点推荐是现代位置社交网络中的关键任务，现有模型主要基于图神经网络和序列模型，但存在基本局限性。&lt;h4&gt;目的&lt;/h4&gt;克服现有模型的限制，能够同时捕捉空间选择的内在层次结构和用户特定时间上下文的动态变化，提供更精准的个性化推荐。&lt;h4&gt;方法&lt;/h4&gt;提出GTR-Mamba框架，利用双曲几何建模静态树状偏好层次结构，在欧几里得切线空间中通过Mamba层处理动态序列更新，并通过跨流形通道融合时空信息引导状态空间模型。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实数据集上的实验表明，GTR-Mamba在下一个POI推荐任务上持续优于最先进的基线模型。&lt;h4&gt;结论&lt;/h4&gt;GTR-Mamba框架有效解决了现有模型在捕捉空间层次结构和时间动态性方面的局限性，提升了推荐的准确性。&lt;h4&gt;翻译&lt;/h4&gt;下一个兴趣点推荐是现代位置社交网络中的关键任务，旨在对人类移动的复杂决策过程进行建模，为用户的下一个签到位置提供个性化推荐。现有的POI推荐模型主要基于图神经网络和序列模型，已得到广泛研究。然而，这些模型面临一个基本限制：它们难以同时捕捉空间选择的内在层次结构和用户特定时间上下文的动态及不规则变化。为克服这一限制，我们提出了GTR-Mamba，一个用于跨流形条件和路由的新框架。GTR-Mamba利用不同数学空间的不同优势处理不同任务：它在双曲几何中建模静态的树状偏好层次结构，同时将动态序列更新路由到计算稳定且高效的欧几里得切线空间中的新型Mamba层。这一过程由跨流形通道协调，该通道融合时空信息以明确引导状态空间模型，实现对上下文变化的灵活适应。在三个真实数据集上的大量实验表明，GTR-Mamba在下一个POI推荐方面持续优于最先进的基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next Point-of-Interest (POI) recommendation is a critical task in modernLocation-Based Social Networks (LBSNs), aiming to model the complexdecision-making process of human mobility to provide personalizedrecommendations for a user's next check-in location. Existing POIrecommendation models, predominantly based on Graph Neural Networks andsequential models, have been extensively studied. However, these models face afundamental limitation: they struggle to simultaneously capture the inherenthierarchical structure of spatial choices and the dynamics and irregular shiftsof user-specific temporal contexts. To overcome this limitation, we proposeGTR-Mamba, a novel framework for cross-manifold conditioning and routing.GTR-Mamba leverages the distinct advantages of different mathematical spacesfor different tasks: it models the static, tree-like preference hierarchies inhyperbolic geometry, while routing the dynamic sequence updates to a novelMamba layer in the computationally stable and efficient Euclidean tangentspace. This process is coordinated by a cross-manifold channel that fusesspatio-temporal information to explicitly steer the State Space Model (SSM),enabling flexible adaptation to contextual changes. Extensive experiments onthree real-world datasets demonstrate that GTR-Mamba consistently outperformsstate-of-the-art baseline models in next POI recommendation.</description>
      <author>example@mail.com (Zhuoxuan Li, Jieyuan Pei, Tangwei Ye, Zhongyuan Lai, Zihan Liu, Fengyuan Xu, Qi Zhang, Liang Hu)</author>
      <guid isPermaLink="false">2510.22942v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Diffuse to Detect: A Generalizable Framework for Anomaly Detection with Diffusion Models Applications to UAVs and Beyond</title>
      <link>http://arxiv.org/abs/2510.22928v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DTD是一种创新的异常检测框架，通过适应扩散模型并采用单步过程实现快速精确的异常识别，结合图神经网络捕捉时空异常，并通过双分支架构平衡可扩展性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;复杂、高维数据（如无人机传感器读数）中的异常检测对于操作安全至关重要，但现有方法在敏感性、可扩展性和捕捉复杂依赖关系方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种创新的异常检测方法，解决现有方法的局限性，实现快速、精确的异常识别。&lt;h4&gt;方法&lt;/h4&gt;提出Diffuse to Detect (DTD)框架，采用单步扩散过程预测噪声模式；集成图神经网络建模传感器关系为动态图；使用双分支架构（参数化神经网络能量评分和非参数统计方法）平衡可扩展性和可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;DTD能够在不产生重构错误的情况下快速精确识别异常；在无人机传感器数据、多元时间序列和图像上的评估表明DTD优于现有方法；DTD具有跨不同数据模态的通用性。&lt;h4&gt;结论&lt;/h4&gt;DTD因其多功能性和适应性，成为安全关键应用（包括工业监测等）的变革性解决方案。&lt;h4&gt;翻译&lt;/h4&gt;复杂、高维数据（如无人机传感器读数）中的异常检测对于操作安全至关重要，但由于现有方法的敏感性、可扩展性有限且无法捕捉复杂依赖关系，这仍然是一个挑战。我们提出了Diffuse to Detect (DTD)框架，这是一种创新的方法，将扩散模型适应于异常检测，不同于其在具有高推理时间的生成任务中的常规使用。相比之下，DTD采用单步扩散过程来预测噪声模式，能够快速精确地识别异常而不会产生重构错误。这种方法基于稳健的理论基础，将噪声预测与数据分布的得分函数联系起来，确保可靠的偏差检测。通过集成图神经网络将传感器关系建模为动态图，DTD有效捕捉了空间（传感器间）和时间异常。其双分支架构采用基于参数化神经网络的能量评分实现可扩展性，非参数统计方法提供可解释性，在计算效率和透明度之间提供了灵活的权衡。在无人机传感器数据、多元时间序列和图像上的广泛评估证明了DTD优于现有方法的性能，强调了其在不同数据模态上的通用性。这种多功能性及其适应性使DTD成为安全关键应用的变革性解决方案，包括工业监测等。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection in complex, high-dimensional data, such as UAV sensorreadings, is essential for operational safety but challenging for existingmethods due to their limited sensitivity, scalability, and inability to captureintricate dependencies. We propose the Diffuse to Detect (DTD) framework, anovel approach that innovatively adapts diffusion models for anomaly detection,diverging from their conventional use in generative tasks with high inferencetime. By comparison, DTD employs a single-step diffusion process to predictnoise patterns, enabling rapid and precise identification of anomalies withoutreconstruction errors. This approach is grounded in robust theoreticalfoundations that link noise prediction to the data distribution's scorefunction, ensuring reliable deviation detection. By integrating Graph NeuralNetworks to model sensor relationships as dynamic graphs, DTD effectivelycaptures spatial (inter-sensor) and temporal anomalies. Its two-brancharchitecture, with parametric neural network-based energy scoring forscalability and nonparametric statistical methods for interpretability,provides flexible trade-offs between computational efficiency and transparency.Extensive evaluations on UAV sensor data, multivariate time series, and imagesdemonstrate DTD's superior performance over existing methods, underscoring itsgenerality across diverse data modalities. This versatility, combined with itsadaptability, positions DTD as a transformative solution for safety-criticalapplications, including industrial monitoring and beyond.</description>
      <author>example@mail.com (Mingze Gong, Juan Du, Jianbang You)</author>
      <guid isPermaLink="false">2510.22928v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>FastJAM: a Fast Joint Alignment Model for Images</title>
      <link>http://arxiv.org/abs/2510.22842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025. Pages 1-10 are the Main Paper. Pages 23-31  are Supplemental Material. FastJAM website -  https://bgu-cs-vil.github.io/FastJAM/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FastJAM是一种快速的基于图的图像联合对齐方法，能够显著降低计算复杂度，实现从小时或分钟到秒级的速度提升，同时保持或提高对齐质量。&lt;h4&gt;背景&lt;/h4&gt;图像联合对齐(JA)旨在将一组图像对齐到统一坐标系，使语义相似特征出现在对应空间位置。现有方法通常需要长时间训练、大容量模型和大量超参数调整。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、高效的图像联合对齐方法，减少计算时间和资源需求，同时保持或提高对齐质量。&lt;h4&gt;方法&lt;/h4&gt;FastJAM利用现成的图像匹配器计算的对和快速非参数聚类构建图，表示图像内和图像间关键点关系。通过图神经网络传播和聚合这些对应关系，使用图像级池化预测单应性参数。采用逆组合损失消除正则化项需求，避免相关超参数调整。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，FastJAM在对齐质量方面优于现有现代JA方法，同时将计算时间从小时或分钟减少到几秒钟。&lt;h4&gt;结论&lt;/h4&gt;FastJAM是一种高效、快速的图像联合对齐方法，能够在保持高质量对齐的同时显著减少计算时间。&lt;h4&gt;翻译&lt;/h4&gt;图像联合对齐(JA)旨在将一组图像对齐到统一的坐标系中，使语义相似的特征出现在对应的空间位置。大多数现有方法通常需要长时间的训练、大容量模型和大量的超参数调整。我们引入了FastJAM，一种快速的基于图的方法，显著降低了联合对齐任务的计算复杂度。FastJAM利用现成的图像匹配器计算的对，以及快速的非参数聚类，来构建表示图像内和图像间关键点关系的图。图神经网络传播和聚合这些对应关系，通过图像级池化有效地预测每个图像的单应性参数。利用逆组合损失消除了对预测变换的正则化项的需求（因此也避免了与这些项相关的超参数调整），FastJAM能够快速有效地执行图像联合对齐。在几个基准测试上的实验结果表明，FastJAM在对齐质量方面优于现有的现代JA方法，同时将计算时间从小时或分钟减少到几秒钟。我们的代码可在项目网页获取，https://bgu-cs-vil.github.io/FastJAM/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Joint Alignment (JA) of images aims to align a collection of images into aunified coordinate frame, such that semantically-similar features appear atcorresponding spatial locations. Most existing approaches often require longtraining times, large-capacity models, and extensive hyperparameter tuning. Weintroduce FastJAM, a rapid, graph-based method that drastically reduces thecomputational complexity of joint alignment tasks. FastJAM leverages pairwisematches computed by an off-the-shelf image matcher, together with a rapidnonparametric clustering, to construct a graph representing intra- andinter-image keypoint relations. A graph neural network propagates andaggregates these correspondences, efficiently predicting per-image homographyparameters via image-level pooling. Utilizing an inverse-compositional loss,that eliminates the need for a regularization term over the predictedtransformations (and thus also obviates the hyperparameter tuning associatedwith such terms), FastJAM performs image JA quickly and effectively.Experimental results on several benchmarks demonstrate that FastJAM achievesresults better than existing modern JA methods in terms of alignment quality,while reducing computation time from hours or minutes to mere seconds. Our codeis available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/</description>
      <author>example@mail.com (Omri Hirsch, Ron Shapira Weber, Shira Ifergane, Oren Freifeld)</author>
      <guid isPermaLink="false">2510.22842v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network Assisted Genetic Algorithm for Structural Dynamic Response and Parameter Optimization</title>
      <link>http://arxiv.org/abs/2510.22839v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合图神经网络(GNN)代理模型和遗传算法(GA)优化器的混合数据驱动框架，用于优化结构参数（质量、刚度和阻尼系数）。该方法克服了传统数值方法在迭代优化任务中计算成本高的问题，实现了强收敛性、良好泛化能力和显著降低的计算成本，为自动化和智能结构设计提供了有效途径。&lt;h4&gt;背景&lt;/h4&gt;结构参数（质量m、刚度k和阻尼系数c）的优化对设计高效、有韧性和稳定的结构至关重要。传统的数值方法如有限元法(FEM)和计算流体动力学(CFD)模拟虽能提供高精度结果，但在迭代优化任务中计算成本高昂，因为每次评估都需要为每个参数组合求解控制方程。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器，以克服传统数值方法在结构参数优化中的计算成本高问题，实现更高效、准确的参数优化。&lt;h4&gt;方法&lt;/h4&gt;研究采用了一种混合数据驱动框架，包括：1) 使用图神经网络(GNN)作为代理模型，学习结构参数与动态位移响应之间的非线性映射；2) 使用Newmark Beta方法生成具有不同质量、刚度和阻尼配置的单自由度(SDOF)系统响应数据集；3) 应用遗传算法(GA)通过最小化预测位移和提高动态稳定性来搜索全局最优参数集。&lt;h4&gt;主要发现&lt;/h4&gt;GNN和GA框架实现了强收敛性、良好的泛化能力，并显著降低了计算成本，相比传统模拟方法具有明显优势。该方法能够准确学习结构参数与动态响应之间的复杂关系，实现快速预测和优化。&lt;h4&gt;结论&lt;/h4&gt;结合机器学习代理与进化优化的方法在自动化和智能结构设计中具有显著有效性。该框架为结构参数优化提供了一种高效、准确的解决方案，克服了传统数值方法的计算瓶颈。&lt;h4&gt;翻译&lt;/h4&gt;结构参数（如质量m、刚度k和阻尼系数c）的优化对设计高效、有韧性和稳定的结构至关重要。传统的数值方法，包括有限元法(FEM)和计算流体动力学(CFD)模拟，能提供高精度结果，但在迭代优化任务中计算成本高，因为每次评估都需要为每个参数组合求解控制方程。本研究提出了一种混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器来克服这些挑战。GNN被训练来准确学习结构参数和动态位移响应之间的非线性映射，从而能够快速预测而无需重复求解系统方程。使用Newmark Beta方法生成了具有不同质量、刚度和阻尼配置的单自由度(SDOF)系统响应数据集。然后，GA通过最小化预测位移和提高动态稳定性来搜索全局最优参数集。结果表明，与传统模拟相比，GNN和GA框架实现了强收敛性、良好的泛化能力和显著降低的计算成本。这种方法强调了将机器学习代理与进化优化相结合在自动化和智能结构设计中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The optimization of structural parameters, such as mass(m), stiffness(k), anddamping coefficient(c), is critical for designing efficient, resilient, andstable structures. Conventional numerical approaches, including Finite ElementMethod (FEM) and Computational Fluid Dynamics (CFD) simulations, providehigh-fidelity results but are computationally expensive for iterativeoptimization tasks, as each evaluation requires solving the governing equationsfor every parameter combination. This study proposes a hybrid data-drivenframework that integrates a Graph Neural Network (GNN) surrogate model with aGenetic Algorithm (GA) optimizer to overcome these challenges. The GNN istrained to accurately learn the nonlinear mapping between structural parametersand dynamic displacement responses, enabling rapid predictions withoutrepeatedly solving the system equations. A dataset of single-degree-of-freedom(SDOF) system responses is generated using the Newmark Beta method acrossdiverse mass, stiffness, and damping configurations. The GA then searches forglobally optimal parameter sets by minimizing predicted displacements andenhancing dynamic stability. Results demonstrate that the GNN and GA frameworkachieves strong convergence, robust generalization, and significantly reducedcomputational cost compared to conventional simulations. This approachhighlights the effectiveness of combining machine learning surrogates withevolutionary optimization for automated and intelligent structural design.</description>
      <author>example@mail.com (Sagnik Mukherjee)</author>
      <guid isPermaLink="false">2510.22839v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM</title>
      <link>http://arxiv.org/abs/2510.22740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE International Symposium on Multi-Robot &amp; Multi-Agent Systems  (MRS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于多智能体强化学习的分布式姿态图优化框架，解决了传统方法收敛到局部最小值的问题，显著提高了轨迹估计的准确性和计算效率。&lt;h4&gt;背景&lt;/h4&gt;分布式姿态图优化（PGO）是多机器人同时定位与地图构建（SLAM）中精确轨迹估计的基础。传统迭代方法将高度非凸优化目标线性化，需要重复求解正规方程，通常收敛到局部最小值，从而产生次优估计。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展、抗异常值的分布式平面PGO框架，利用多智能体强化学习（MARL）来提高轨迹估计的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;将分布式PGO建模为部分可观察马尔可夫博弈，每个机器人运行带有自适应边缘门控的循环边缘条件图神经网络（GNN）编码器来去噪，通过混合策略利用先验动作记忆和图嵌入优化姿态，最后使用一致性方案解决机器人间的不一致。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实世界数据集上的评估表明，该方法比最先进的分布式PGO框架平均减少37.5%的全局目标，同时将推理效率提高至少6倍；单个学习策略无需重新训练即可扩展到更大的机器人团队。&lt;h4&gt;结论&lt;/h4&gt;基于MARL的分布式PGO框架在提高轨迹估计精度的同时显著增强了计算效率，具有良好的可扩展性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;我们考虑分布式姿态图优化（PGO）问题，这是多机器人同时定位与地图构建（SLAM）中精确轨迹估计的基础。传统迭代方法将高度非凸优化目标线性化，需要重复求解正规方程，通常收敛到局部最小值，从而产生次优估计。我们提出了一种使用多智能体强化学习（MARL）的可扩展、抗异常值的分布式平面PGO框架。我们将分布式PGO构建为定义在局部姿态图上的部分可观察马尔可夫博弈，其中每个动作优化单个边的姿态估计。图分区器分解全局姿态图，每个机器人运行带有自适应边缘门控的循环边缘条件图神经网络（GNN）编码器来去噪噪声边缘。机器人通过利用先验动作记忆和图嵌入的混合策略顺序优化姿态。在局部图校正后，使用一致性方案解决机器人间的不一致，产生全局一致的估计。我们在综合的合成和真实世界数据集上的广泛评估表明，我们学习的基于MARL的智能体比最先进的分布式PGO框架平均减少37.5%的全局目标，同时将推理效率提高至少6倍。我们还证明了智能体复制允许单个学习策略无需重新训练即可轻松扩展到更大的机器人团队。代码可在https://github.com/herolab-uga/policies-over-poses公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider the distributed pose-graph optimization (PGO) problem, which isfundamental in accurate trajectory estimation in multi-robot simultaneouslocalization and mapping (SLAM). Conventional iterative approaches linearize ahighly non-convex optimization objective, requiring repeated solving of normalequations, which often converge to local minima and thus produce suboptimalestimates. We propose a scalable, outlier-robust distributed planar PGOframework using Multi-Agent Reinforcement Learning (MARL). We cast distributedPGO as a partially observable Markov game defined on local pose-graphs, whereeach action refines a single edge's pose estimate. A graph partitionerdecomposes the global pose graph, and each robot runs a recurrentedge-conditioned Graph Neural Network (GNN) encoder with adaptive edge-gatingto denoise noisy edges. Robots sequentially refine poses through a hybridpolicy that utilizes prior action memory and graph embeddings. After localgraph correction, a consensus scheme reconciles inter-robot disagreements toproduce a globally consistent estimate. Our extensive evaluations on acomprehensive suite of synthetic and real-world datasets demonstrate that ourlearned MARL-based actors reduce the global objective by an average of 37.5%more than the state-of-the-art distributed PGO framework, while enhancinginference efficiency by at least 6X. We also demonstrate that actor replicationallows a single learned policy to scale effortlessly to substantially largerrobot teams without any retraining. Code is publicly available athttps://github.com/herolab-uga/policies-over-poses.</description>
      <author>example@mail.com (Sai Krishna Ghanta, Ramviyas Parasuraman)</author>
      <guid isPermaLink="false">2510.22740v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking</title>
      <link>http://arxiv.org/abs/2510.22726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpoofTrackBench是一个可重现、模块化的基准测试，用于评估雷达欺骗下的实时定位和跟踪系统的对抗鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;雷达欺骗攻击对实时定位和跟踪系统构成威胁，需要有效的评估方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个基准测试框架，评估不同跟踪架构在雷达欺骗攻击下的性能和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用Hampton University Skyler Radar Sensor数据集，模拟漂移、幽灵和镜像类型的欺骗攻击，并使用联合概率数据关联(JPDA)和全局最近邻(GNN)架构评估跟踪器性能。框架分离干净和欺骗的检测流，可视化轨迹偏离，并量化分配错误。&lt;h4&gt;主要发现&lt;/h4&gt;通过聚类叠加、注入感知时间线和场景自适应可视化，实现了不同欺骗类型和配置下的结果可解释性。评估图表和日志自动导出，确保可重现性。&lt;h4&gt;结论&lt;/h4&gt;SpoofTrackBench为开放、合乎道德的欺骗感知跟踪管道基准测试设定了新标准，实现了严格的跨架构分析和社区验证。&lt;h4&gt;翻译&lt;/h4&gt;SpoofTrackBench是一个可重现、模块化的基准测试，用于评估雷达欺骗下的实时定位和跟踪(RTLS)系统的对抗鲁棒性。利用Hampton University Skyler雷达传感器数据集，我们模拟了漂移、幽灵和镜像类型的欺骗攻击，并使用联合概率数据关联(JPDA)和全局最近邻(GNN)架构评估跟踪器性能。我们的框架分离干净和欺骗的检测流，可视化欺骗引起的轨迹偏离，并通过直接偏离真相的指标量化分配错误。聚类叠加、注入感知时间线和场景自适应可视化使不同欺骗类型和配置下的结果可解释。评估图表和日志自动导出，用于可重现的比较。SpoofTrackBench为开放、合乎道德的欺骗感知跟踪管道基准测试设定了新标准，实现了严格的跨架构分析和社区验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; SpoofTrackBench is a reproducible, modular benchmark for evaluatingadversarial robustness in real-time localization and tracking (RTLS) systemsunder radar spoofing. Leveraging the Hampton University Skyler Radar Sensordataset, we simulate drift, ghost, and mirror-type spoofing attacks andevaluate tracker performance using both Joint Probabilistic Data Association(JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separatesclean and spoofed detection streams, visualizes spoof-induced trajectorydivergence, and quantifies assignment errors via direct drift-from-truthmetrics. Clustering overlays, injection-aware timelines, and scenario-adaptivevisualizations enable interpretability across spoof types and configurations.Evaluation figures and logs are auto-exported for reproducible comparison.SpoofTrackBench sets a new standard for open, ethical benchmarking ofspoof-aware tracking pipelines, enabling rigorous cross-architecture analysisand community validation.</description>
      <author>example@mail.com (Van Le, Tan Le)</author>
      <guid isPermaLink="false">2510.22726v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>If You Want to Be Robust, Be Wary of Initialization</title>
      <link>http://arxiv.org/abs/2510.22652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络(GNNs)的权重初始化和超参数对对抗性鲁棒性的影响，发现适当的初始化方法可显著提升模型防御能力。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在各种图相关任务中表现出色，但容易受到对抗性扰动的攻击。现有防御策略主要关注预处理技术和自适应消息传递方案，而忽略了权重初始化的影响。&lt;h4&gt;目的&lt;/h4&gt;探索权重初始化和相关超参数(如训练周期)对模型鲁棒性的影响，建立初始化策略与网络抗扰动能力之间的理论联系。&lt;h4&gt;方法&lt;/h4&gt;引入连接初始化策略和网络鲁棒性的理论框架，分析初始权重、训练周期与模型脆弱性的关系，并将框架扩展到深度神经网络。通过多种模型和真实数据集的对抗性攻击实验验证理论发现。&lt;h4&gt;主要发现&lt;/h4&gt;适当的权重初始化不仅能保证模型在干净数据集上的性能，还能显著提升对抗性防御能力，与其他初始化方法相比性能差距可达50%。&lt;h4&gt;结论&lt;/h4&gt;权重初始化是提升图神经网络对抗鲁棒性的重要因素，为防御对抗性攻击提供了超越传统机制的新视角。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在各种图相关任务中表现出色，然而人们对它们容易受到对抗性扰动的担忧依然存在。虽然主流防御策略主要关注预处理技术和自适应消息传递方案，但本研究探讨了一个尚未充分研究的维度：权重初始化及相关超参数(如训练周期)对模型鲁棒性的影响。我们引入了一个理论框架，连接初始化策略与网络对抗扰动的恢复能力。我们的分析揭示了初始权重、训练周期数量与模型脆弱性之间的直接关系，为对抗性鲁棒性提供了超越传统防御机制的新见解。虽然我们的主要关注点是图神经网络，但我们扩展了理论框架，提供了一个适用于深度神经网络的通用上界。跨越多种模型和真实世界数据集的广泛实验( subjected to various adversarial attacks)验证了我们的发现。我们说明，选择适当的初始化不仅能确保在干净数据集上的性能，还能增强模型对抗扰动的鲁棒性，与其他初始化方法相比观察到高达50%的性能差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable performance acrossa spectrum of graph-related tasks, however concerns persist regarding theirvulnerability to adversarial perturbations. While prevailing defense strategiesfocus primarily on pre-processing techniques and adaptive message-passingschemes, this study delves into an under-explored dimension: the impact ofweight initialization and associated hyper-parameters, such as training epochs,on a model's robustness. We introduce a theoretical framework bridging theconnection between initialization strategies and a network's resilience toadversarial perturbations. Our analysis reveals a direct relationship betweeninitial weights, number of training epochs and the model's vulnerability,offering new insights into adversarial robustness beyond conventional defensemechanisms. While our primary focus is on GNNs, we extend our theoreticalframework, providing a general upper-bound applicable to Deep Neural Networks.Extensive experiments, spanning diverse models and real-world datasetssubjected to various adversarial attacks, validate our findings. We illustratethat selecting appropriate initialization not only ensures performance on cleandatasets but also enhances model robustness against adversarial perturbations,with observed gaps of up to 50\% compared to alternative initializationapproaches.</description>
      <author>example@mail.com (Sofiane Ennadir, Johannes F. Lutzeyer, Michalis Vazirgiannis, El Houcine Bergou)</author>
      <guid isPermaLink="false">2510.22652v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Graph Classification Robustness with Singular Pooling</title>
      <link>http://arxiv.org/abs/2510.22643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at Neurips 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络在图分类任务中的对抗鲁棒性问题，特别关注了池化操作在塑造鲁棒性方面的作用，并提出了一种名为鲁棒奇异池化(RS-Pool)的新策略。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在图表示学习任务中表现出色，但其在图分类任务中的对抗鲁棒性研究相对较少，尤其是在与节点分类相比时。大多数现有防御方法集中在消息传递组件上，而忽略了池化操作的作用。&lt;h4&gt;目的&lt;/h4&gt;研究池化操作在图神经网络对抗鲁棒性中的角色，并开发一种能够提高图分类任务鲁棒性的新池化策略。&lt;h4&gt;方法&lt;/h4&gt;对标准扁平池化方法(求和、平均和最大值)进行理论分析，推导对抗风险上限，确定不同攻击场景和图结构下的脆弱性。基于这些见解，提出利用节点嵌入矩阵主奇异向量构建鲁棒图级表示的RS-Pool策略。&lt;h4&gt;主要发现&lt;/h4&gt;RS-Pool在遭受最先进对抗攻击时，比其他池化方法提供更好的鲁棒性，同时保持有竞争力的干净准确率。该策略与模型无关，可通过幂迭代有效实现，理论分析表明其具有良好的鲁棒性特性。&lt;h4&gt;结论&lt;/h4&gt;池化操作在图神经网络的对抗鲁棒性中扮演着重要角色，所提出的RS-Pool方法能够有效提高图分类任务的鲁棒性，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在一系列图表示学习任务中已取得强大性能，然而与节点分类相比，其在图分类中的对抗鲁棒性研究仍然不足。虽然大多数现有防御方法集中在消息传递组件上，但本研究调查了池化操作在塑造鲁棒性中被忽视的作用。我们对标准扁平池化方法(求和、平均和最大值)进行了理论分析，推导了它们对抗风险的上限，并确定了它们在不同攻击场景和图结构下的脆弱性。受这些见解启发，我们提出了鲁棒奇异池化(RS-Pool)，这是一种新颖的池化策略，利用节点嵌入矩阵的主奇异向量构建鲁棒的图级表示。我们从理论上研究了RS-Pool的鲁棒性，并解释了所得界限，从而加深对我们提出的池化算子的理解。虽然我们的分析集中在图卷积网络(GCNs)上，但RS-Pool是与模型无关的，可以通过幂迭代有效实现。真实世界基准测试的实证结果表明，当遭受最先进的对抗攻击时，RS-Pool比考虑的池化方法提供更好的鲁棒性，同时保持有竞争力的干净准确率。我们的代码已在GitHub公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved strong performance across a rangeof graph representation learning tasks, yet their adversarial robustness ingraph classification remains underexplored compared to node classification.While most existing defenses focus on the message-passing component, this workinvestigates the overlooked role of pooling operations in shaping robustness.We present a theoretical analysis of standard flat pooling methods (sum,average and max), deriving upper bounds on their adversarial risk andidentifying their vulnerabilities under different attack scenarios and graphstructures. Motivated by these insights, we propose \textit{Robust SingularPooling (RS-Pool)}, a novel pooling strategy that leverages the dominantsingular vector of the node embedding matrix to construct a robust graph-levelrepresentation. We theoretically investigate the robustness of RS-Pool andinterpret the resulting bound leading to improved understanding of our proposedpooling operator. While our analysis centers on Graph Convolutional Networks(GCNs), RS-Pool is model-agnostic and can be implemented efficiently via poweriteration. Empirical results on real-world benchmarks show that RS-Poolprovides better robustness than the considered pooling methods when subject tostate-of-the-art adversarial attacks while maintaining competitive cleanaccuracy. Our code is publicly availableat:\href{https://github.com/king/rs-pool}{https://github.com/king/rs-pool}.</description>
      <author>example@mail.com (Sofiane Ennadir, Oleg Smirnov, Yassine Abbahaddou, Lele Cao, Johannes F. Lutzeyer)</author>
      <guid isPermaLink="false">2510.22643v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval</title>
      <link>http://arxiv.org/abs/2510.22538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了IsoNet++，一种基于子图同构的改进图检索方法，通过技术创新显著提升了检索性能。&lt;h4&gt;背景&lt;/h4&gt;基于子图同构的图检索在场景图检索、分子指纹检测和电路设计等领域有广泛应用。Roy等人提出的IsoNet是一种后期交互模型，用于子图匹配，它先独立计算每对图的节点和边嵌入，再计算可训练的对齐映射。&lt;h4&gt;目的&lt;/h4&gt;开发IsoNet++，一种早期交互图神经网络，通过技术创新改进现有的子图匹配方法，提高图检索的准确性。&lt;h4&gt;方法&lt;/h4&gt;IsoNet++包含三个技术创新：1)通过在两个输入图内部和之间传递消息计算所有节点嵌入，由节点间的单射对齐引导；2)以惰性方式在多轮中更新对齐，每轮基于当前对齐状态从头运行逐层GNN；3)引入节点对伙伴交互概念，将节点对而非单个节点视为潜在伙伴。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，随着轮次增加，对齐 progressively 得到细化，检索性能显著优于现有方法，且所有三个创新都对提升准确性有贡献。&lt;h4&gt;结论&lt;/h4&gt;IsoNet++通过三个技术创新显著提高了图检索性能，代码和数据集已公开在https://github.com/structlearning/isonetpp。&lt;h4&gt;翻译&lt;/h4&gt;基于子图同构的图检索在场景图检索、分子指纹检测和电路设计等现实世界应用中有多种应用。Roy等人提出了IsoNet，一种用于子图匹配的后期交互模型，它首先独立计算每对图中节点和边的嵌入，然后计算可训练的对齐映射。本文提出了IsoNet++，一种早期交互图神经网络(GNN)，基于几项技术创新。首先，我们通过在两个输入图内部和之间传递消息来计算所有节点的嵌入，这些消息由节点之间的单射对齐引导。其次，我们在多轮中以惰性方式更新对齐。每轮中，我们基于当前对齐状态从头开始运行逐层GNN。一轮GNN完成后，我们使用最后一层嵌入更新对齐，然后进入下一轮。第三，IsoNet++引入了节点对伙伴交互的新概念。传统早期交互计算节点与另一图中潜在伙伴之间的注意力，注意力控制跨图传递的消息。相比之下，我们将节点对（而非单个节点）视为潜在伙伴。一个图中节点间存在边而另一图中不存在，提供了细化对齐的重要信号。我们在多个数据集上的实验表明，对齐随着轮次的推进逐步细化，检索性能显著优于现有方法。我们证明了所有三个创新都对提高准确性做出了贡献。我们的代码和数据集已在https://github.com/structlearning/isonetpp公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph retrieval based on subgraph isomorphism has several real-worldapplications such as scene graph retrieval, molecular fingerprint detection andcircuit design. Roy et al. [35] proposed IsoNet, a late interaction model forsubgraph matching, which first computes the node and edge embeddings of eachgraph independently of paired graph and then computes a trainable alignmentmap. Here, we present IsoNet++, an early interaction graph neural network(GNN), based on several technical innovations. First, we compute embeddings ofall nodes by passing messages within and across the two input graphs, guided byan injective alignment between their nodes. Second, we update this alignment ina lazy fashion over multiple rounds. Within each round, we run a layerwise GNNfrom scratch, based on the current state of the alignment. After the completionof one round of GNN, we use the last-layer embeddings to update the alignments,and proceed to the next round. Third, IsoNet++ incorporates a novel notion ofnode-pair partner interaction. Traditional early interaction computes attentionbetween a node and its potential partners in the other graph, the attentionthen controlling messages passed across graphs. In contrast, we consider nodepairs (not single nodes) as potential partners. Existence of an edge betweenthe nodes in one graph and non-existence in the other provide vital signals forrefining the alignment. Our experiments on several datasets show that thealignments get progressively refined with successive rounds, resulting insignificantly better retrieval performance than existing methods. Wedemonstrate that all three innovations contribute to the enhanced accuracy. Ourcode and datasets are publicly available athttps://github.com/structlearning/isonetpp.</description>
      <author>example@mail.com (Ashwin Ramachandran, Vaibhav Raj, Indrayumna Roy, Soumen Chakrabarti, Abir De)</author>
      <guid isPermaLink="false">2510.22538v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Toward Robust Signed Graph Learning through Joint Input-Target Denoising</title>
      <link>http://arxiv.org/abs/2510.22513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了RIDGE框架，一种通过联合去噪图输入和监督目标来实现的鲁棒符号图学习方法，有效提高了SGNN在噪声环境下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;符号图神经网络（SGNNs）被广泛用于分析包含正负链接的符号图中的复杂模式。鉴于现实世界连接的噪声特性，SGNN的鲁棒性也已成为一个关键研究领域。在经验属性监督下，图结构学习已在符号图表示学习中显示出其鲁棒性，然而，缺乏理论指导的鲁棒SGNN研究仍然较少。&lt;h4&gt;目的&lt;/h4&gt;受图信息瓶颈（GIB）在信息提取中的成功启发，提出一种有理论指导的鲁棒SGNN框架，通过联合去噪图输入和监督目标来提高鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了RIDGE框架，扩展了GIB理论以支持目标空间去噪，因为输入和目标空间都存在噪声。通过重参数化机制和变分近似产生的可处理目标函数，RIDGE有效清理输入数据和监督目标。&lt;h4&gt;主要发现&lt;/h4&gt;在四个常用的符号图数据集上的广泛验证表明，RIDGE在各种噪声水平下显著提高了流行SGNN模型的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;RIDGE框架能够有效提高SGNN在噪声环境下的鲁棒性，为符号图分析提供了新的理论指导和实践方法。&lt;h4&gt;翻译&lt;/h4&gt;符号图神经网络（SGNNs）被广泛用于分析包含正负链接的符号图中的复杂模式。鉴于现实世界连接的噪声特性，SGNN的鲁棒性也已成为一个关键研究领域。在经验属性监督下，图结构学习已在符号图表示学习中显示出其鲁棒性，然而，缺乏理论指导的鲁棒SGNN研究仍然较少。受图信息瓶颈（GIB）在信息提取中的成功启发，我们提出了RIDGE，一种通过联合去噪图输入和监督目标来实现鲁棒符号图学习的新框架。与基本GIB不同，我们扩展了GIB理论，使其能够对目标空间进行去噪，因为输入和目标空间都存在噪声。在实例化中，RIDGE通过重参数化机制和变分近似产生的可处理目标函数有效清理输入数据和监督目标。我们在四个常用的符号图数据集上广泛验证了我们的方法，结果表明，在各种噪声水平下，RIDGE显著提高了流行SGNN模型的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Signed Graph Neural Networks (SGNNs) are widely adopted to analyze complexpatterns in signed graphs with both positive and negative links. Given thenoisy nature of real-world connections, the robustness of SGNN has also emergedas a pivotal research area. Under the supervision of empirical properties,graph structure learning has shown its robustness on signed graphrepresentation learning, however, there remains a paucity of researchinvestigating a robust SGNN with theoretical guidance. Inspired by the successof graph information bottleneck (GIB) in information extraction, we proposeRIDGE, a novel framework for Robust sI gned graph learning through jointDenoising of Graph inputs and supervision targEts. Different from the basicGIB, we extend the GIB theory with the capability of target space denoising asthe co-existence of noise in both input and target spaces. In instantiation,RIDGE effectively cleanses input data and supervision targets via a tractableobjective function produced by reparameterization mechanism and variationalapproximation. We extensively validate our method on four prevalent signedgraph datasets, and the results show that RIDGE clearly improves the robustnessof popular SGNN models under various levels of noise.</description>
      <author>example@mail.com (Junran Wu, Beng Chin Ooi, Ke Xu)</author>
      <guid isPermaLink="false">2510.22513v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>GraphTOP: Graph Topology-Oriented Prompting for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.22451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the 39 Annual Conference on Neural Information Processing  Systems (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了GraphTOP框架，开创性地探索了面向图拓扑的提示方法，通过修改图拓扑而非仅修改节点特征来适配预训练的图神经网络模型。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)通过大规模图数据学习强大的图表示，'预训练、适配'方案是训练强大GNN的常见模式。在适配阶段，图提示是一种有效策略，可修改输入图数据同时保持预训练GNN模型冻结。&lt;h4&gt;目的&lt;/h4&gt;进行图提示在图拓扑方面的开创性研究，提出第一个图拓扑导向提示(GraphTOP)框架，有效适配预训练GNN模型用于下游任务。&lt;h4&gt;方法&lt;/h4&gt;将拓扑导向提示表述为多跳局部子图中的边重连问题，通过重参数化将其松弛到连续概率空间，同时确保紧密松弛并保持图的稀疏性。&lt;h4&gt;主要发现&lt;/h4&gt;在四种预训练策略下的五个图数据集上进行大量实验，GraphTOP在多个节点分类数据集上优于六个基线方法。&lt;h4&gt;结论&lt;/h4&gt;GraphTOP框架在图提示领域，特别是面向拓扑的提示方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)通过大规模图数据学习强大的图表示，革新了图学习领域。作为训练强大GNN的常见模式，'预训练、适配'方案首先在无标签图数据上预训练GNN，然后将其适配到特定下游任务。在适配阶段，图提示是一种有效策略，即可学习提示修改输入图数据，同时保持预训练GNN模型冻结。通常，现有图提示研究主要关注面向特征的方法，将图提示应用于节点特征或隐藏表示。然而，这些研究表现次优，因为它们持续忽视了面向拓扑提示的潜力，后者通过修改图拓扑来适配预训练GNN。在本研究中，我们从图拓扑角度对图提示进行了开创性研究。我们提出了第一个图拓扑导向提示(GraphTOP)框架，有效适配预训练GNN模型用于下游任务。更具体地说，我们将拓扑导向提示表述为多跳局部子图中的边重连问题，并通过重参数化将其松弛到连续概率空间，同时确保紧密松弛并保持图的稀疏性。在四种预训练策略下的五个图数据集上进行的大量实验表明，我们提出的GraphTOP在多个节点分类数据集上优于六个基线方法。我们的代码可在https://github.com/xbfu/GraphTOP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have revolutionized the field of graph learningby learning expressive graph representations from massive graph data. As acommon pattern to train powerful GNNs, the "pre-training, adaptation" schemefirst pre-trains GNNs over unlabeled graph data and subsequently adapts them tospecific downstream tasks. In the adaptation phase, graph prompting is aneffective strategy that modifies input graph data with learnable prompts whilekeeping pre-trained GNN models frozen. Typically, existing graph promptingstudies mainly focus on *feature-oriented* methods that apply graph prompts tonode features or hidden representations. However, these studies often achievesuboptimal performance, as they consistently overlook the potential of*topology-oriented* prompting, which adapts pre-trained GNNs by modifying thegraph topology. In this study, we conduct a pioneering investigation of graphprompting in terms of graph topology. We propose the first **Graph****T**opology-**O**riented **P**rompting (GraphTOP) framework to effectivelyadapt pre-trained GNN models for downstream tasks. More specifically, wereformulate topology-oriented prompting as an edge rewiring problem withinmulti-hop local subgraphs and relax it into the continuous probability spacethrough reparameterization while ensuring tight relaxation and preserving graphsparsity. Extensive experiments on five graph datasets under four pre-trainingstrategies demonstrate that our proposed GraphTOP outshines six baselines onmultiple node classification datasets. Our code is available athttps://github.com/xbfu/GraphTOP.</description>
      <author>example@mail.com (Xingbo Fu, Zhenyu Lei, Zihan Chen, Binchi Zhang, Chuxu Zhang, Jundong Li)</author>
      <guid isPermaLink="false">2510.22451v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Augmentation: Leveraging Inter-Instance Relation in Self-Supervised Representation Learning</title>
      <link>http://arxiv.org/abs/2510.22322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE Signal Processing Letters, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将图论与自监督表示学习相结合的新方法，通过构建k近邻图并利用图神经网络进行表示学习，显著提升了模型性能。&lt;h4&gt;背景&lt;/h4&gt;传统自监督表示学习方法主要关注通过数据增强技术生成的实例内变化，但往往忽略了实例间的重要关系信息。&lt;h4&gt;目的&lt;/h4&gt;在保留实例内属性的同时，有效捕获实例间关系，并通过图神经网络实现更广泛的上下文集成，提升表示学习效果。&lt;h4&gt;方法&lt;/h4&gt;在预训练阶段为教师和学生流构建k近邻(KNN)图，其中节点表示样本及其潜在表示，边编码实例间的相似性；在表示细化阶段，使用图神经网络在多个跃点间传播消息，实现更广泛的上下文整合。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10、ImageNet-100和ImageNet-1K三个数据集上，分别实现了7.3%、3.2%和1.0%的准确率提升，显著优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;基于图的机制在自监督表示学习中具有显著优势，能够有效提升模型性能，代码已公开可获取。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了一种将图论整合到自监督表示学习中的新方法。传统方法专注于应用增强技术生成的实例内变化。然而，它们常常忽略了重要的实例间关系。虽然我们的方法保留了实例内属性，但通过在预训练期间为教师和学生流构建k近邻(KNN)图，进一步捕获了实例间关系。在这些图中，节点表示样本及其潜在表示，边编码实例之间的相似性。预训练后，执行表示细化阶段。在此阶段，图神经网络不仅可以在直接邻居之间传播消息，还可以跨越多个跃点，从而实现更广泛的上下文集成。在CIFAR-10、ImageNet-100和ImageNet-1K上的实验结果分别比最先进的方法提高了7.3%、3.2%和1.0%的准确率。这些结果突显了所提出的基于图机制的有效性。代码可在https://github.com/alijavidani/SSL-GraphNNCLR公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LSP.2025.3610549&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel approach that integrates graph theory intoself-supervised representation learning. Traditional methods focus onintra-instance variations generated by applying augmentations. However, theyoften overlook important inter-instance relationships. While our method retainsthe intra-instance property, it further captures inter-instance relationshipsby constructing k-nearest neighbor (KNN) graphs for both teacher and studentstreams during pretraining. In these graphs, nodes represent samples along withtheir latent representations. Edges encode the similarity between instances.Following pretraining, a representation refinement phase is performed. In thisphase, Graph Neural Networks (GNNs) propagate messages not only among immediateneighbors but also across multiple hops, thereby enabling broader contextualintegration. Experimental results on CIFAR-10, ImageNet-100, and ImageNet-1Kdemonstrate accuracy improvements of 7.3%, 3.2%, and 1.0%, respectively, overstate-of-the-art methods. These results highlight the effectiveness of theproposed graph based mechanism. The code is publicly available athttps://github.com/alijavidani/SSL-GraphNNCLR.</description>
      <author>example@mail.com (Ali Javidani, Babak Nadjar Araabi, Mohammad Amin Sadeghi)</author>
      <guid isPermaLink="false">2510.22322v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Does Homophily Help in Robust Test-time Node Classification?</title>
      <link>http://arxiv.org/abs/2510.22289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为GrapHoST的测试时图结构转换方法，通过调整测试图中的同质性来提高预训练图神经网络在节点分类任务上的鲁棒性和性能，无需重新训练模型。&lt;h4&gt;背景&lt;/h4&gt;同质性是现实世界图的基本属性，但现有方法主要关注训练图的学习。然而，测试图常面临数据质量问题和分布偏移，如社交网络中不同地区用户的领域偏移和引文网络中的时间演化偏移，这些因素会降低预训练模型的鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;提高预训练GNN模型在面对测试时数据质量问题和分布偏移情况下的鲁棒性和性能。&lt;h4&gt;方法&lt;/h4&gt;提出GrapHoST方法，开发同质性预测器来区分测试边，通过预测同质性得分的置信度实现自适应的测试时图结构转换。&lt;h4&gt;主要发现&lt;/h4&gt;通过增加同质性图中的同质性或减少异质性图中的同质性来转换测试图结构，可以显著提高预训练GNN在节点分类任务上的鲁棒性和性能，无需模型训练或更新。&lt;h4&gt;结论&lt;/h4&gt;在九个基准数据集上的实验表明，GrapHoST在各种测试时数据质量问题下始终实现了最先进的性能，最高提升达10.92%，代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;同质性，即同一类别节点倾向于连接的特性，是现实世界图的基本属性，支撑着引文网络和社会网络等领域中的结构和语义模式。现有方法通过设计同质性感知的GNN架构或图结构学习策略来利用同质性，但它们主要关注训练图的GNN学习。然而，在现实场景中，测试图常常面临数据质量问题和分布偏移，如社交网络中不同地区用户之间的领域偏移，以及在不同时间段收集的引文网络图中的时间演化偏移。这些因素显著降低了预训练模型的鲁棒性，导致测试时性能下降。通过实证观察和理论分析，我们揭示出通过转换测试图结构——在同质性图中增加同质性或在异质性图中减少同质性——可以显著提高预训练GNN在节点分类任务上的鲁棒性和性能，无需模型训练或更新。基于这些见解，我们提出了一种基于同质性的新颖测试时图结构转换方法，名为GrapHoST。具体来说，开发了一个同质性预测器来区分测试边，通过预测同质性得分的置信度实现自适应的测试时图结构转换。在九个基准数据集上针对多种测试时数据质量问题的广泛实验表明，GrapHoST始终实现了最先进的性能，最高提升达10.92%。我们的代码已在https://github.com/YanJiangJerry/GrapHoST发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Homophily, the tendency of nodes from the same class to connect, is afundamental property of real-world graphs, underpinning structural and semanticpatterns in domains such as citation networks and social networks. Existingmethods exploit homophily through designing homophily-aware GNN architecturesor graph structure learning strategies, yet they primarily focus on GNNlearning with training graphs. However, in real-world scenarios, test graphsoften suffer from data quality issues and distribution shifts, such as domainshifts across users from different regions in social networks and temporalevolution shifts in citation network graphs collected over varying timeperiods. These factors significantly compromise the pre-trained model'srobustness, resulting in degraded test-time performance. With empiricalobservations and theoretical analysis, we reveal that transforming the testgraph structure by increasing homophily in homophilic graphs or decreasing itin heterophilic graphs can significantly improve the robustness and performanceof pre-trained GNNs on node classifications, without requiring model trainingor update. Motivated by these insights, a novel test-time graph structuraltransformation method grounded in homophily, named GrapHoST, is proposed.Specifically, a homophily predictor is developed to discriminate test edges,facilitating adaptive test-time graph structural transformation by theconfidence of predicted homophily scores. Extensive experiments on ninebenchmark datasets under a range of test-time data quality issues demonstratethat GrapHoST consistently achieves state-of-the-art performance, withimprovements of up to 10.92%. Our code has been released athttps://github.com/YanJiangJerry/GrapHoST.</description>
      <author>example@mail.com (Yan Jiang, Ruihong Qiu, Zi Huang)</author>
      <guid isPermaLink="false">2510.22289v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Graph Neural Network for Data-Driven Physiologically Based Pharmacokinetic Modeling</title>
      <link>http://arxiv.org/abs/2510.22096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了使用深度学习替代传统生理药代动力学建模方法，提出了一种动态图神经网络(Dynamic GNN)来模拟器官间的相互作用，实现了更高的预测性能。&lt;h4&gt;背景&lt;/h4&gt;生理药代动力学(PBPK)建模在药物开发中通过预测器官间药物浓度动态发挥关键作用。传统方法依赖常微分方程和强简化假设，限制了其对非线性生理相互作用的适应性。&lt;h4&gt;目的&lt;/h4&gt;探索使用深度学习进行PBPK预测的数据驱动替代方法，以提高预测准确性和适应性。&lt;h4&gt;方法&lt;/h4&gt;实现两种基线架构：多层感知器(MLP)和长短期记忆(LSTM)网络，分别用于捕捉分子和时间依赖性。提出动态图神经网络(Dynamic GNN)将生理连接建模为器官间的递归消息传递过程。&lt;h4&gt;主要发现&lt;/h4&gt;动态GNN在所有模型中表现最佳，预测性能最高，R平方值为0.9342，均方根误差为0.0159，平均绝对误差为0.0116。相比之下，MLP基线获得R平方值0.8705，LSTM获得0.8059。明确建模器官相互作用的时空依赖性可实现更准确和可推广的药物浓度预测。&lt;h4&gt;结论&lt;/h4&gt;动态GNN为传统PBPK公式提供了可扩展的、无方程的替代方案，在临床前和临床研究中，数据驱动的药代动力学建模展现出巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;生理药代动力学(PBPK)建模通过预测器官间药物浓度动态在药物开发中发挥关键作用。传统方法依赖常微分方程和强简化假设，限制了其对非线性生理相互作用的适应性。本研究探索了使用深度学习进行PBPK预测的数据驱动替代方法。实现了两种基线架构：多层感知器(MLP)和长短期记忆(LSTM)网络，分别用于捕捉分子和时间依赖性。为了整合器官间相互作用，我们提出了一种动态图神经网络(Dynamic GNN)，将生理连接建模为器官间的递归消息传递过程。实验结果表明，所提出的动态GNN在所有模型中实现了最高的预测性能，R平方值为0.9342，均方根误差为0.0159，平均绝对误差为0.0116。相比之下，MLP基线获得R平方值0.8705，LSTM获得0.8059。这些结果表明，明确建模器官相互作用的时空依赖性可实现更准确和可推广的药物浓度预测。动态GNN为传统PBPK公式提供了可扩展的、无方程的替代方案，并在临床前和临床研究中展现出数据驱动药代动力学建模的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physiologically Based Pharmacokinetic (PBPK) modeling plays a critical rolein drug development by predicting drug concentration dynamics across organs.Traditional approaches rely on ordinary differential equations with strongsimplifying assumptions, which limit their adaptability to nonlinearphysiological interactions. In this study, we explore data-driven alternativesfor PBPK prediction using deep learning. Two baseline architectures - amultilayer perceptron (MLP) and a long short-term memory (LSTM) network - areimplemented to capture molecular and temporal dependencies, respectively. Toincorporate inter-organ interactions, we propose a Dynamic Graph Neural Network(Dynamic GNN) that models physiological connections as recurrentmessage-passing processes between organs. Experimental results demonstrate thatthe proposed Dynamic GNN achieves the highest predictive performance among allmodels, with an R^2 of 0.9342, an RMSE of 0.0159, and an MAE of 0.0116. Incomparison, the MLP baseline obtains an R^2 of 0.8705 and the LSTM achieves0.8059. These results highlight that explicitly modeling the spatial andtemporal dependencies of organ interactions enables more accurate andgeneralizable drug concentration prediction. The Dynamic GNN provides ascalable, equation-free alternative to traditional PBPK formulations anddemonstrates strong potential for data-driven pharmacokinetic modeling inpreclinical and clinical research.</description>
      <author>example@mail.com (Su Liu, Xin Hu, Shurong Wen, Jiaqi Liu, Jiexi Xu, Lanruo Wang)</author>
      <guid isPermaLink="false">2510.22096v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Graph Networks for Accurate Weather Forecasting via Lightweight Training</title>
      <link>http://arxiv.org/abs/2510.22094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了HiFlowCast和HiAntFlow两种层级图神经网络模型，通过创新机制提高气候事件预测准确性，同时降低训练成本和环境影响。&lt;h4&gt;背景&lt;/h4&gt;气候事件由复杂的全球尺度驱动因素导致的多元动态过程产生，对食物、能源和基础设施有深远影响。然而，由于物理过程跨越不同的时空尺度，固定分辨率方法无法捕捉这些过程，导致准确天气预测仍然困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉多尺度物理过程的气候预测方法，提高预测准确性，特别是对于极端事件。&lt;h4&gt;方法&lt;/h4&gt;提出HiFlowCast和其集合变体HiAntFlow，这是一种层级图神经网络，将物理学嵌入多尺度预测框架。创新点包括：1)潜在记忆保留机制，在向下遍历过程中保持全局趋势；2)从潜在到物理的分支，整合不同尺度上的偏微分方程解场。&lt;h4&gt;主要发现&lt;/h4&gt;在13天提前期的预测中，模型将误差减少了5%以上；在第一和第九十九百分位极端情况下，误差减少了5-8%，提高了罕见事件的可靠性。利用预训练模型权重，模型在一个周期内收敛，显著降低了训练成本和碳足迹。&lt;h4&gt;结论&lt;/h4&gt;提高预测效率对于应对机器学习规模增长带来的可持续性挑战和研究可及性限制至关重要，代码和模型权重见补充材料。&lt;h4&gt;翻译&lt;/h4&gt;气候事件源于由全球尺度驱动的复杂多变量动态过程，深刻影响食物、能源和基础设施。然而，由于物理过程跨越多样的时空尺度展开，固定分辨率方法无法捕捉，准确的天气预测仍然难以实现。层级图神经网络提供多尺度表示，但非线性向下映射通常会抹去全局趋势，削弱物理学与预测的整合。我们引入HiFlowCast及其集合变体HiAntFlow，这些图神经网络将物理学嵌入多尺度预测框架。两个创新支撑了它们的设计：潜在记忆保留机制在向下遍历过程中保持全局趋势，以及从潜在到物理的分支整合不同尺度上的偏微分方程解场。我们的模型在13天提前期将误差减少5%以上，在第一和第九十九百分位极端情况下减少5-8%，提高了罕见事件的可靠性。利用预训练模型权重，它们在一个周期内收敛，降低了训练成本和碳足迹。这种效率至关重要，因为机器学习规模的不断增长挑战可持续性并限制研究可及性。代码和模型权重见补充材料。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Climate events arise from intricate, multivariate dynamics governed byglobal-scale drivers, profoundly impacting food, energy, and infrastructure.Yet, accurate weather prediction remains elusive due to physical processesunfolding across diverse spatio-temporal scales, which fixed-resolution methodscannot capture. Hierarchical Graph Neural Networks (HGNNs) offer a multiscalerepresentation, but nonlinear downward mappings often erase global trends,weakening the integration of physics into forecasts. We introduce HiFlowCastand its ensemble variant HiAntFlow, HGNNs that embed physics within amultiscale prediction framework. Two innovations underpin their design: aLatent-Memory-Retention mechanism that preserves global trends during downwardtraversal, and a Latent-to-Physics branch that integrates PDE solution fieldsacross diverse scales. Our Flow models cut errors by over 5% at 13-day leadtimes and by 5-8% under 1st and 99th quantile extremes, improving reliabilityfor rare events. Leveraging pretrained model weights, they converge within asingle epoch, reducing training cost and their carbon footprint. Suchefficiency is vital as the growing scale of machine learning challengessustainability and limits research accessibility. Code and model weights are inthe supplementary materials.</description>
      <author>example@mail.com (Thomas Bailie, S. Karthik Mukkavilli, Varvara Vetrova, Yun Sing Koh)</author>
      <guid isPermaLink="false">2510.22094v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Pruning and Quantization Impact on Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.22058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了神经网络压缩方法(剪枝和量化)对图神经网络(GNNs)的影响，发现非结构化细粒度和全局剪枝可显著减小模型大小(50%)同时保持或提高精度，而不同量化方法在不同数据集上有不同影响。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在图结构数据学习上具有高准确性，但面临高计算和资源成本问题。&lt;h4&gt;目的&lt;/h4&gt;研究神经网络压缩方法(剪枝和量化)如何减小GNN模型大小同时保持合理准确性。&lt;h4&gt;方法&lt;/h4&gt;在三个图数据集(Cora, Proteins, BBBBP)上评估三种剪枝方法和三种量化方法对三种GNN任务(图分类、节点分类和链接预测)的影响。&lt;h4&gt;主要发现&lt;/h4&gt;非结构化细粒度和全局剪枝可显著减小模型大小(50%)，同时在微调后保持甚至提高精度；不同量化方法对GNN的准确性、推理时间和模型大小在不同数据集上有不同影响。&lt;h4&gt;结论&lt;/h4&gt;神经网络压缩技术(特别是剪枝)可以有效减少GNN模型大小而不牺牲性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在从图结构数据学习方面以高精度著称，但它们面临高计算和资源成本的问题。神经网络压缩方法用于减小模型大小同时保持合理准确性。两种常见的神经网络压缩技术包括剪枝和量化。在这项研究中，我们实证检验了三种剪枝方法和三种量化方法对不同GNN模型的影响，包括图分类任务、节点分类任务和链接预测。我们在三个图数据集上进行了所有实验，包括Cora、Proteins和BBBP。我们的研究结果表明，非结构化细粒度和全局剪枝可以显著减小模型大小(50%)，同时在微调剪枝后的模型后保持甚至提高精度。对GNN上不同量化方法的评估显示，在不同数据集上，这些方法对准确性、推理时间和模型大小有不同影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are known to operate with high accuracy onlearning from graph-structured data, but they suffer from high computationaland resource costs. Neural network compression methods are used to reduce themodel size while maintaining reasonable accuracy. Two of the common neuralnetwork compression techniques include pruning and quantization. In thisresearch, we empirically examine the effects of three pruning methods and threequantization methods on different GNN models, including graph classificationtasks, node classification tasks, and link prediction. We conducted allexperiments on three graph datasets, including Cora, Proteins, and BBBP. Ourfindings demonstrate that unstructured fine-grained and global pruning cansignificantly reduce the model's size(50\%) while maintaining or even improvingprecision after fine-tuning the pruned model. The evaluation of differentquantization methods on GNN shows diverse impacts on accuracy, inference time,and model size across different datasets.</description>
      <author>example@mail.com (Khatoon Khedri, Reza Rawassizadeh, Qifu Wen, Mehdi Hosseinzadeh)</author>
      <guid isPermaLink="false">2510.22058v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>PF$Δ$: A Benchmark Dataset for Power Flow under Load, Generation, and Topology Variations</title>
      <link>http://arxiv.org/abs/2510.22048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 14 figures. Accepted at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PFΔ，一个用于潮流计算的基准数据集，旨在解决电力系统操作中的计算瓶颈和不确定性挑战。&lt;h4&gt;背景&lt;/h4&gt;潮流计算是实时电网操作的基础，但存在计算瓶颈问题。可再生能源整合和气候引起的极端天气增加了电力系统操作的不确定性，需要能够准确高效模拟各种场景的工具。&lt;h4&gt;目的&lt;/h4&gt;引入一个能够捕捉负荷、发电和拓扑多样变化的潮流计算基准数据集PFΔ，以评估现有方法并确定未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;构建包含859,800个已解决潮流计算实例的数据集，涵盖六种不同总线系统规模，包含三种应急场景（N、N-1和N-2），以及接近稳态电压稳定性极限的案例。&lt;h4&gt;主要发现&lt;/h4&gt;评估了传统求解器和基于GNN的方法，突出了现有方法面临的挑战领域，并确定了未来研究的开放性问题。&lt;h4&gt;结论&lt;/h4&gt;PFΔ数据集和相关代码已公开发布，为电力系统潮流计算研究提供了新资源。&lt;h4&gt;翻译&lt;/h4&gt;潮流计算是实时电网操作的基础，贯穿于工作流程中，如contingency analysis（重复PF评估评估停电情况下的电网安全）和拓扑优化（涉及基于PF的组合式大动作空间搜索）。在操作时间尺度上运行这些计算或在大评估空间中运行仍然是主要的计算瓶颈。此外，可再生能源整合和气候引起的极端天气导致的电力系统操作中不断增加的不确定性，也需要能够准确高效地模拟各种场景和操作条件的工具。机器学习方法相比传统求解器提供了潜在的速度提升，但它们在捕捉现实世界变异性的基准上尚未得到系统性评估。本文引入了PFΔ，一个潮流计算的基准数据集，捕捉了负荷、发电和拓扑的多样变化。PFΔ包含859,800个已解决的潮流计算实例，涵盖六种不同总线系统规模，捕获三种类型的应急场景（N、N-1和N-2），并包括接近稳态电压稳定性极限的接近不可行案例。我们评估了传统求解器和基于GNN的方法，突出了现有方法遇到困难的领域，并确定了未来研究的开放性问题。我们的数据集可在https://huggingface.co/datasets/pfdelta/pfdelta/tree/main获取，我们的代码包含数据生成脚本和模型实现，位于https://github.com/MOSSLab-MIT/pfdelta。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Power flow (PF) calculations are the backbone of real-time grid operations,across workflows such as contingency analysis (where repeated PF evaluationsassess grid security under outages) and topology optimization (which involvesPF-based searches over combinatorially large action spaces). Running thesecalculations at operational timescales or across large evaluation spacesremains a major computational bottleneck. Additionally, growing uncertainty inpower system operations from the integration of renewables and climate-inducedextreme weather also calls for tools that can accurately and efficientlysimulate a wide range of scenarios and operating conditions. Machine learningmethods offer a potential speedup over traditional solvers, but theirperformance has not been systematically assessed on benchmarks that capturereal-world variability. This paper introduces PF$\Delta$, a benchmark datasetfor power flow that captures diverse variations in load, generation, andtopology. PF$\Delta$ contains 859,800 solved power flow instances spanning sixdifferent bus system sizes, capturing three types of contingency scenarios (N ,N -1, and N -2), and including close-to-infeasible cases near steady-statevoltage stability limits. We evaluate traditional solvers and GNN-basedmethods, highlighting key areas where existing approaches struggle, andidentifying open problems for future research. Our dataset is available athttps://huggingface.co/datasets/pfdelta/pfdelta/tree/main and our code withdata generation scripts and model implementations is athttps://github.com/MOSSLab-MIT/pfdelta.</description>
      <author>example@mail.com (Ana K. Rivera, Anvita Bhagavathula, Alvaro Carbonero, Priya Donti)</author>
      <guid isPermaLink="false">2510.22048v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>A Hybrid GNN-LSE Method for Fast, Robust, and Physically-Consistent AC Power Flow</title>
      <link>http://arxiv.org/abs/2510.22020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合物理信息图神经网络(GNN)和线性状态估计(LSE)的两阶段混合方法，用于解决传统交流潮流求解器在大规模电力系统中的计算和收敛挑战。&lt;h4&gt;背景&lt;/h4&gt;传统的交流潮流(ACPF)求解器如牛顿-拉夫森法(NR)在现代大规模电力系统中面临显著的计算和收敛挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速且物理一致的电力系统求解方法，适用于实时操作和分析。&lt;h4&gt;方法&lt;/h4&gt;结合物理信息图神经网络与迭代线性状态估计：使用物理信息损失函数训练GNN预测高质量初始系统状态，然后通过LSE细化步骤解决线性方程以强制执行物理定律，绕过传统求解器的非线性和收敛问题。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在IEEE 33节点、69节点和118节点系统上得到验证；GNN变体比牛顿-拉夫森法快高达8400倍；LSE细化能快速获得物理一致解；重载压力测试和N-1 contingencies证明了方法的可靠性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该框架成功连接了快速数据驱动模型与电力系统物理约束，为实时电力系统操作和分析提供了实用工具。&lt;h4&gt;翻译&lt;/h4&gt;传统的交流潮流(ACPF)求解器如牛顿-拉夫森法(NR)在现代大规模电力系统中面临显著的计算和收敛挑战。本文提出了一种新颖的两阶段混合方法，结合物理信息图神经网络(GNN)和稳健的迭代线性状态估计(LSE)细化步骤，以产生快速且物理一致的解。使用具有高效动态加权方案的物理信息损失函数训练的GNN可快速预测高质量的初始系统状态。然后使用受状态估计技术启发的迭代直接线性求解器进行细化。LSE细化步骤解决一系列线性方程以强制执行物理定律，有效绕过传统求解器的非线性和收敛问题。所提出的GNN-LSE框架在从小的辐射状配电网(IEEE 33节点、69节点)到大型网状输电系统(IEEE 118节点)的各种系统上得到了全面验证。结果表明，我们的GNN变体比NR快高达8400倍。LSE细化提供了一条快速获得物理一致解的途径，而重载压力测试(标称值的120%-150%)和N-1 contingencies展示了该方法的可靠性和泛化能力。这项工作提出了一个强大而灵活的框架，用于连接快速的数据驱动模型与电力系统物理的严格约束，为实时操作和分析提供了实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional AC Power Flow (ACPF) solvers like Newton-Raphson (NR) facesignificant computational and convergence challenges in modern, large-scalepower systems. This paper proposes a novel, two-stage hybrid method thatintegrates a Physics-Informed Graph Neural Network (GNN) with a robust,iterative Linear State Estimation (LSE) refinement step to produce fast andphysically-consistent solutions. The GNN, trained with a physics-informed lossfunction featuring an efficient dynamic weighting scheme, rapidly predicts ahigh-quality initial system state. This prediction is then refined using aniterative, direct linear solver inspired by state estimation techniques. ThisLSE refinement step solves a series of linear equations to enforce physicallaws, effectively bypassing the non-linearities and convergence issues oftraditional solvers. The proposed GNN-LSE framework is comprehensivelyvalidated on systems ranging from small radial distribution networks (IEEE33-bus, 69-bus) to a large, meshed transmission system (IEEE 118-bus). Resultsshow that our GNN variants are up to $8.4 \times 10^3$ times faster than NR.The LSE refinement provides a fast route to a physically-consistent solution,while heavy-loading stress tests (120%-150% of nominal) and N-1 contingenciesdemonstrate the method's reliability and generalization. This work presents apowerful and flexible framework for bridging fast, data-driven models with therigorous constraints of power system physics, offering a practical tool forreal-time operations and analysis.</description>
      <author>example@mail.com (Mohamed Shamseldein)</author>
      <guid isPermaLink="false">2510.22020v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Human Protein Embeddings Database: DeepDrug Protein Embeddings Bank (DPEB)</title>
      <link>http://arxiv.org/abs/2510.22008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DPEB是一个整合了四种蛋白质嵌入类型的数据集，用于提高蛋白质-蛋白质相互作用(PPI)预测的准确性，并在多种蛋白质分类任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;计算预测蛋白质-蛋白质相互作用(PPI)具有挑战性，主要原因是缺乏整合的多模态蛋白质表示方法。&lt;h4&gt;目的&lt;/h4&gt;创建一个整合多种蛋白质嵌入类型的数据集，填补AlphaFold2内部神经网络嵌入不可用的空白，为计算建模提供支持。&lt;h4&gt;方法&lt;/h4&gt;DPEB是一个包含22,043个人类蛋白质的精选集合，整合了四种嵌入类型：结构嵌入(AlphaFold2)、基于transformer的序列嵌入(BioEmbeddings)、上下文氨基酸模式(ESM-2)和基于序列的n-gram统计(ProtVec)。&lt;h4&gt;主要发现&lt;/h4&gt;GraphSAGE与BioEmbedding结合实现了最高的PPI预测性能(87.37% AUROC, 79.16%准确率)；该框架在酶分类任务上达到77.42%的准确率；在蛋白质家族分类任务上达到86.04%的准确率。&lt;h4&gt;结论&lt;/h4&gt;DPEB支持多种图神经网络方法进行PPI预测，可应用于系统生物学、药物靶点识别、通路分析和疾病机制研究。&lt;h4&gt;翻译&lt;/h4&gt;计算预测蛋白质-蛋白质相互作用(PPI)具有挑战性，由于缺乏整合的多模态蛋白质表示。DPEB是一个包含22,043个人类蛋白质的精选集合，整合了四种嵌入类型：结构(AlphaFold2)、基于transformer的序列(BioEmbeddings)、上下文氨基酸模式(ESM-2: Evolutionary Scale Modeling)和基于序列的n-gram统计(ProtVec)。AlphaFold2蛋白质结构可通过公共数据库(如AlphaFold2蛋白质结构数据库)获取，但内部神经网络嵌入不可用。DPEB通过提供AlphaFold2衍生的嵌入用于计算建模来填补这一空白。我们的基准评估显示，GraphSAGE与BioEmbedding结合实现了最高的PPI预测性能(87.37% AUROC, 79.16%准确率)。该框架在酶分类上实现了77.42%的准确率，在蛋白质家族分类上实现了86.04%的准确率。DPEB支持多种图神经网络方法进行PPI预测，能够在系统生物学、药物靶点识别、通路分析和疾病机制研究中应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computationally predicting protein-protein interactions (PPIs) is challengingdue to the lack of integrated, multimodal protein representations. DPEB is acurated collection of 22,043 human proteins that integrates four embeddingtypes: structural (AlphaFold2), transformer-based sequence (BioEmbeddings),contextual amino acid patterns (ESM-2: Evolutionary Scale Modeling), andsequence-based n-gram statistics (ProtVec]). AlphaFold2 protein structures areavailable through public databases (e.g., AlphaFold2 Protein StructureDatabase), but the internal neural network embeddings are not. DPEB addressesthis gap by providing AlphaFold2-derived embeddings for computational modeling.Our benchmark evaluations show GraphSAGE with BioEmbedding achieved the highestPPI prediction performance (87.37% AUROC, 79.16% accuracy). The framework alsoachieved 77.42% accuracy for enzyme classification and 86.04% accuracy forprotein family classification. DPEB supports multiple graph neural networkmethods for PPI prediction, enabling applications in systems biology, drugtarget identification, pathway analysis, and disease mechanism studies.</description>
      <author>example@mail.com (Md Saiful Islam Sajol, Magesh Rajasekaran, Hayden Gemeinhardt, Adam Bess, Chris Alvin, Supratik Mukhopadhyay)</author>
      <guid isPermaLink="false">2510.22008v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning on Real-World Graphs</title>
      <link>http://arxiv.org/abs/2510.21994v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The thesis was submitted for the degree of Doctor of Philosophy in  Computing at Imperial College London (February 2024), under the supervision  of Prof. Michael M. Bronstein. It includes work published at ICML, ICLR,  NeurIPS, and the Learning on Graphs Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一系列图神经网络模型，解决了GNNs在实际应用中的关键挑战，包括可扩展性、时间性、方向性、数据不完整性和结构不确定性等问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为学习图结构数据的核心工具，但在实际系统中的应用受到可扩展性、时间性、方向性、数据不完整性和结构不确定性等关键挑战的限制。&lt;h4&gt;目的&lt;/h4&gt;解决GNNs在实际应用中的限制，使其能够应用于工业规模的图数据。&lt;h4&gt;方法&lt;/h4&gt;作者提出了五个模型：SIGN用于可扩展图学习，TGN用于时间图，Dir-GNN用于有向和异质网络，Feature Propagation用于处理缺失节点特征，NuGget用于博弈论结构推断。&lt;h4&gt;主要发现&lt;/h4&gt;这些模型共同弥合了学术基准和工业规模图之间的差距，使GNNs能够在社交系统和推荐系统等领域使用。&lt;h4&gt;结论&lt;/h4&gt;通过这些创新模型，GNNs的实际应用限制得到了解决，使其能够在真实世界系统中有效应用。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为学习图结构数据的核心工具，但它们在实际系统中的应用受到可扩展性、时间性、方向性、数据不完整性和结构不确定性等关键挑战的限制。本论文引入了一系列解决这些限制的模型：SIGN用于可扩展图学习，TGN用于时间图，Dir-GNN用于有向和异质网络，Feature Propagation (FP)用于学习缺失节点特征，NuGget用于博弈论结构推断。这些贡献共同弥合了学术基准和工业规模图之间的差距，使GNNs能够在社交和推荐系统等领域使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.25560/112863&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become a central tool for learning ongraph-structured data, yet their applicability to real-world systems remainslimited by key challenges such as scalability, temporality, directionality,data incompleteness, and structural uncertainty. This thesis introduces aseries of models addressing these limitations: SIGN for scalable graphlearning, TGN for temporal graphs, Dir-GNN for directed and heterophilicnetworks, Feature Propagation (FP) for learning with missing node features, andNuGget for game-theoretic structural inference. Together, these contributionsbridge the gap between academic benchmarks and industrial-scale graphs,enabling the use of GNNs in domains such as social and recommender systems.</description>
      <author>example@mail.com (Emanuele Rossi)</author>
      <guid isPermaLink="false">2510.21994v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Classical Algorithms for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.21574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了通过在经典算法上预训练图神经网络(GNNs)来提升其在分子属性预测任务上的性能。研究证明将经典算法的先验知识嵌入到GNNs中能够提供有用的归纳偏置，从而提升在复杂真实世界图数据上的表现。&lt;h4&gt;背景&lt;/h4&gt;神经网络在处理非结构化数据方面表现出色，但通常无法分布外泛化；而经典算法虽然保证正确性但缺乏灵活性。&lt;h4&gt;目的&lt;/h4&gt;探索通过在经典算法上预训练图神经网络(GNNs)来改善其在Open Graph Benchmark上的分子属性预测任务中的性能，包括ogbg-molhiv(HIV抑制)和ogbg-molclintox(临床毒性)任务。&lt;h4&gt;方法&lt;/h4&gt;使用从CLRS算法推理基准中的24个经典算法训练的GNNs，来初始化并冻结第二GNN的选定层，用于分子预测任务。&lt;h4&gt;主要发现&lt;/h4&gt;与随机初始化的基线相比，预训练模型取得了一致的胜利或平局。其中，基于Segments Intersect算法的预训练在ogbg-molhiv上取得了6%的绝对增益，基于Dijkstra的预训练在ogbg-molclintox上取得了3%的增益。&lt;h4&gt;结论&lt;/h4&gt;将经典算法的先验知识嵌入到GNNs中可以提供有用的归纳偏置，提高在复杂、真实世界图数据上的性能。&lt;h4&gt;翻译&lt;/h4&gt;神经网络在处理非结构化数据方面表现出色，但通常无法分布外泛化，而经典算法虽然保证正确性但缺乏灵活性。我们探索了通过在经典算法上预训练图神经网络(GNNs)来改善其在Open Graph Benchmark上的分子属性预测任务中的性能，包括ogbg-molhiv(HIV抑制)和ogbg-molclintox(临床毒性)任务。使用从CLRS算法推理基准中的24个经典算法训练的GNNs，来初始化并冻结第二GNN的选定层用于分子预测。与随机初始化的基线相比，预训练模型取得了一致的胜利或平局，其中基于Segments Intersect算法的预训练在ogbg-molhiv上取得了6%的绝对增益，基于Dijkstra的预训练在ogbg-molclintox上取得了3%的增益。这些结果表明将经典算法的先验知识嵌入到GNNs中可以提供有用的归纳偏置，提高在复杂、真实世界图数据上的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural networks excel at processing unstructured data but often fail togeneralise out-of-distribution, whereas classical algorithms guaranteecorrectness but lack flexibility. We explore whether pretraining Graph NeuralNetworks (GNNs) on classical algorithms can improve their performance onmolecular property prediction tasks from the Open Graph Benchmark: ogbg-molhiv(HIV inhibition) and ogbg-molclintox (clinical toxicity). GNNs trained on 24classical algorithms from the CLRS Algorithmic Reasoning Benchmark are used toinitialise and freeze selected layers of a second GNN for molecular prediction.Compared to a randomly initialised baseline, the pretrained models achieveconsistent wins or ties, with the Segments Intersect algorithm pretrainingyielding a 6% absolute gain on ogbg-molhiv and Dijkstra pretraining achieving a3% gain on ogbg-molclintox. These results demonstrate embedding classicalalgorithmic priors into GNNs provides useful inductive biases, boostingperformance on complex, real-world graph data.</description>
      <author>example@mail.com (Jason Wu, Petar Veličković)</author>
      <guid isPermaLink="false">2510.21574v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>HollowFlow: Efficient Sample Likelihood Evaluation using Hollow Message Passing</title>
      <link>http://arxiv.org/abs/2510.21542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为HollowFlow的流模型，利用非回溯图神经网络解决大规模系统中的计算瓶颈问题，实现了高达O(n²)的加速，使基于流的生成模型能够应用于更大规模的科学问题。&lt;h4&gt;背景&lt;/h4&gt;流和扩散模型已成为科学应用的强大工具，特别适用于采样非归一化概率分布，如玻尔兹曼生成器(BGs)。然而，这些模型在实际部署时面临关键挑战：它们依赖于样本似然计算，而这种计算的系统规模n呈指数级增长，使得大规模问题难以处理。&lt;h4&gt;目的&lt;/h4&gt;为了解决流模型在大规模系统中的计算效率问题，作者引入了HollowFlow，旨在显著提高似然评估速度，使BGs能够扩展到更大的系统。&lt;h4&gt;方法&lt;/h4&gt;作者提出了HollowFlow，一种利用新型非回溯图神经网络(NoBGNN)的基于流的生成模型。通过强制块对角雅可比结构，HollowFlow的似然评估可以在n中用常数次反向传播完成。该框架具有普适性，任何等变GNN或基于注意力的架构都可以被适配为NoBGNN。&lt;h4&gt;主要发现&lt;/h4&gt;作者通过在两个不同规模的系统上训练BGs验证了HollowFlow。对于这两个系统，采样和似然评估时间都显著减少，遵循了理论上的缩放规律。对于较大的系统，作者获得了100倍的加速，展示了基于HollowFlow的方法在高维科学问题上的潜力。&lt;h4&gt;结论&lt;/h4&gt;HollowFlow为基于流的生成模型在大规模科学问题中的应用提供了有效解决方案，通过创新的图神经网络架构显著提高了计算效率，使得以前因计算限制而无法处理的高维问题现在变得可行。&lt;h4&gt;翻译&lt;/h4&gt;流和扩散模型已成为科学应用的强大工具，特别适用于采样非归一化概率分布，如玻尔兹曼生成器(BGs)。部署这些模型的一个关键挑战是它们依赖于样本似然计算，而这种计算的系统规模n呈指数级增长，通常使得大规模问题变得不可行。为了解决这个问题，我们引入了HollowFlow，这是一种基于流的生成模型，利用了一种新型的非回溯图神经网络(NoBGNN)。通过强制块对角雅可比结构，HollowFlow的似然评估可以在n中用常数次反向传播完成，实现高达O(n²)的加速：这是将BGs扩展到更大系统的重要一步。重要的是，我们的框架具有普适性：任何等变GNN或基于注意力的架构都可以被适配为NoBGNN。我们通过在两个不同规模的系统上训练BGs来验证HollowFlow。对于这两个系统，采样和似然评估时间都显著减少，遵循了理论上的缩放规律。对于较大的系统，我们获得了100倍的加速，清楚地展示了基于HollowFlow的方法在高维科学问题上的潜力，这些问题以前因计算瓶颈而受到阻碍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Flow and diffusion-based models have emerged as powerful tools for scientificapplications, particularly for sampling non-normalized probabilitydistributions, as exemplified by Boltzmann Generators (BGs). A criticalchallenge in deploying these models is their reliance on sample likelihoodcomputations, which scale prohibitively with system size $n$, often renderingthem infeasible for large-scale problems. To address this, we introduce$\textit{HollowFlow}$, a flow-based generative model leveraging a novelnon-backtracking graph neural network (NoBGNN). By enforcing a block-diagonalJacobian structure, HollowFlow likelihoods are evaluated with a constant numberof backward passes in $n$, yielding speed-ups of up to $\mathcal{O}(n^2)$: asignificant step towards scaling BGs to larger systems. Crucially, ourframework generalizes: $\textbf{any equivariant GNN or attention-basedarchitecture}$ can be adapted into a NoBGNN. We validate HollowFlow by trainingBGs on two different systems of increasing size. For both systems, the samplingand likelihood evaluation time decreases dramatically, following ourtheoretical scaling laws. For the larger system we obtain a $10^2\times$speed-up, clearly illustrating the potential of HollowFlow-based approaches forhigh-dimensional scientific problems previously hindered by computationalbottlenecks.</description>
      <author>example@mail.com (Johann Flemming Gloy, Simon Olsson)</author>
      <guid isPermaLink="false">2510.21542v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Estimating Treatment Effects in Networks using Domain Adversarial Training</title>
      <link>http://arxiv.org/abs/2510.21457v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了HINet方法，通过结合图神经网络和领域对抗训练，解决了网络环境中估计异质治疗效应时面临的干扰、未知暴露映射和网络层面协变量偏移等问题。&lt;h4&gt;背景&lt;/h4&gt;在网络环境中估计异质治疗效应受到干扰困扰，一个实例的结果可能受到他人治疗状态的影响。现有方法通常假设已知的暴露映射，这往往不现实。同质性与治疗分配机制的相互作用可能导致网络层面的协变量偏移，进而导致治疗效应估计不准确，这种现象尚未被明确研究。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够在未知暴露映射下估计治疗效应的方法，同时减轻网络层面协变量偏移的影响。&lt;h4&gt;方法&lt;/h4&gt;提出了HINet，一种新颖的方法，结合了图神经网络和领域对抗训练。这种组合允许在未知暴露映射下估计治疗效应，同时减轻网络层面协变量偏移的影响。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和半合成网络数据集上的广泛实证评估证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;HINet方法成功解决了网络环境中估计异质治疗效应的挑战。&lt;h4&gt;翻译&lt;/h4&gt;在网络环境中估计异质治疗效应因干扰而复杂化，这意味着一个实例的结果可能受到他人治疗状态的影响。现有的因果机器学习方法通常假设已知的暴露映射，该映射总结了给定实例的结果如何受他人治疗的影响，这是一种简化的假设，通常不切实际。此外，同质性——相似实例倾向于连接——与治疗分配机制之间的相互作用可能引发网络层面的协变量偏移，可能导致不准确的治疗效应估计，这种现象尚未被明确研究。为了应对这些挑战，我们提出了HINet，一种将图神经网络与领域对抗训练相结合的新颖方法。这种组合允许在未知暴露映射的情况下估计治疗效应，同时减轻（网络层面）协变量偏移的影响。在合成和半合成网络数据集上的广泛实证评估证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating heterogeneous treatment effects in network settings is complicatedby interference, meaning that the outcome of an instance can be influenced bythe treatment status of others. Existing causal machine learning approachesusually assume a known exposure mapping that summarizes how the outcome of agiven instance is influenced by others' treatment, a simplification that isoften unrealistic. Furthermore, the interaction between homophily -- thetendency of similar instances to connect -- and the treatment assignmentmechanism can induce a network-level covariate shift that may lead toinaccurate treatment effect estimates, a phenomenon that has not yet beenexplicitly studied. To address these challenges, we propose HINet, a novelmethod that integrates graph neural networks with domain adversarial training.This combination allows estimating treatment effects under unknown exposuremappings while mitigating the impact of (network-level) covariate shift. Anextensive empirical evaluation on synthetic and semi-synthetic network datasetsdemonstrates the effectiveness of our approach.</description>
      <author>example@mail.com (Daan Caljon, Jente Van Belle, Wouter Verbeke)</author>
      <guid isPermaLink="false">2510.21457v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Modest-Align: Data-Efficient Alignment for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.21606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Modest-Align是一个轻量级跨模态对齐框架，通过随机扰动和嵌入平滑两种策略解决资源受限场景下的过度自信问题，在保持高性能的同时大幅减少训练数据和计算资源需求。&lt;h4&gt;背景&lt;/h4&gt;跨模态对齐旨在将异构模态映射到共享潜在空间，CLIP等模型通过大规模预训练获得强大识别能力，但在资源受限、数据有限或质量低的情况下，这些模型常因模糊或弱相关的图像-文本对而出现过度自信和性能下降。&lt;h4&gt;目的&lt;/h4&gt;设计一个轻量级的对齐框架，提高在资源受限场景下的鲁棒性和效率，解决模型的过度自信问题。&lt;h4&gt;方法&lt;/h4&gt;提出Modest-Align框架，采用两种互补策略：随机扰动引入受控噪声模拟不确定性，嵌入平滑校准嵌入空间中的相似度分布，共同减少过度自信并提高对噪声或弱对齐样本的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的实验表明，Modest-Align在检索任务中优于最先进方法，使用超过100倍少的训练数据和600倍少的GPU时间达到与CLIP竞争的结果。&lt;h4&gt;结论&lt;/h4&gt;Modest-Align为现实世界中资源受限的跨模态对齐问题提供了实用且可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;跨模态对齐旨在将异构模态映射到共享的潜在空间，如CLIP等模型所示，这些模型通过大规模图像-文本预训练获得强大的识别能力。然而，在资源受限、数据有限或质量低的环境中，由于模糊或弱相关的图像-文本对普遍存在，这些模型常常过度自信且性能下降。当前依赖单一正样本对的对比学习方法进一步加剧了这一问题，通过强化对不确定样本的过度自信。为应对这些挑战，我们提出了Modest-Align，一个为鲁棒性和效率而设计的轻量级对齐框架。我们的方法采用两种互补策略——随机扰动，引入受控噪声来模拟不确定性；以及嵌入平滑，校准嵌入空间中的相似度分布。这些机制共同减少过度自信并提高对噪声或弱对齐样本的性能。在多个基准数据集上的广泛实验表明，Modest-Align在检索任务中优于最先进方法，使用超过100倍少的训练数据和600倍少的GPU时间达到与CLIP竞争的结果。我们的方法为现实世界中资源受限的跨模态对齐问题提供了实用且可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-modal alignment aims to map heterogeneous modalities into a sharedlatent space, as exemplified by models like CLIP, which benefit fromlarge-scale image-text pretraining for strong recognition capabilities.However, when operating in resource-constrained settings with limited orlow-quality data, these models often suffer from overconfidence and degradedperformance due to the prevalence of ambiguous or weakly correlated image-textpairs. Current contrastive learning approaches, which rely on single positivepairs, further exacerbate this issue by reinforcing overconfidence on uncertainsamples. To address these challenges, we propose Modest-Align, a lightweightalignment framework designed for robustness and efficiency. Our approachleverages two complementary strategies -- Random Perturbation, which introducescontrolled noise to simulate uncertainty, and Embedding Smoothing, whichcalibrates similarity distributions in the embedding space. These mechanismscollectively reduce overconfidence and improve performance on noisy or weaklyaligned samples. Extensive experiments across multiple benchmark datasetsdemonstrate that Modest-Align outperforms state-of-the-art methods in retrievaltasks, achieving competitive results with over 100x less training data and 600xless GPU time than CLIP. Our method offers a practical and scalable solutionfor cross-modal alignment in real-world, low-resource scenarios.</description>
      <author>example@mail.com (Jiaxiang Liu, Yuan Wang, Jiawei Du, Joey Tianyi Zhou, Mingkun Xu, Zuozhu Liu)</author>
      <guid isPermaLink="false">2510.21606v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
  <item>
      <title>Visual Diffusion Models are Geometric Solvers</title>
      <link>http://arxiv.org/abs/2510.21697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://kariander1.github.io/visual-geo-solver/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了视觉扩散模型可以作为有效的几何求解器，通过在像素空间工作直接解决几何问题。研究者将这种方法应用于三个著名的几何难题：内接正方形问题、斯坦纳树问题和简单多边形问题。&lt;h4&gt;背景&lt;/h4&gt;几何问题求解一直是挑战性问题，尤其是内接正方形问题（询问每个约旦曲线是否包含四个点形成正方形）等长期未解决的难题。前期工作需要专门的架构和领域特定的适应来将扩散应用于参数化几何表示。&lt;h4&gt;目的&lt;/h4&gt;展示视觉扩散模型作为几何求解器的有效性，探索在像素空间直接推理几何问题的可能性，并开发一种通用框架来近似解决著名的几何难题。&lt;h4&gt;方法&lt;/h4&gt;将每个问题实例视为图像，训练标准视觉扩散模型将高斯噪声转换为代表有效近似解的图像。模型学习将嘈杂的几何结构转换为正确配置，将几何推理重新表述为图像生成过程。&lt;h4&gt;主要发现&lt;/h4&gt;视觉扩散模型能够有效解决内接正方形问题、斯坦纳树问题和简单多边形问题；模型能够将嘈杂的几何结构转换为正确配置；生成模型与几何问题解决之间存在联系；在图像空间操作提供了一种通用框架来近似解决著名难题。&lt;h4&gt;结论&lt;/h4&gt;视觉扩散模型可以作为有效的几何求解器，在图像空间操作为近似解决著名难题提供了通用且实用的框架，为解决更广泛的挑战性几何任务开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们展示了视觉扩散模型可以作为有效的几何求解器：它们通过在像素空间工作，能够直接推理几何问题。我们首先在内接正方形问题上证明了这一点，这是几何学中的一个长期未解决的问题，询问每个约旦曲线是否包含四个点形成正方形。然后我们将这种方法扩展到其他两个著名的难解几何问题：斯坦纳树问题和简单多边形问题。我们的方法将每个问题实例视为图像，并训练一个标准的视觉扩散模型，该模型将高斯噪声转换为代表有效近似解的图像，该解与精确解非常匹配。模型学习将嘈杂的几何结构转换为正确配置，有效地将几何推理重新表述为图像生成。与之前需要专门架构和领域特定适应的工作不同，我们使用在问题视觉表示上操作的标准视觉扩散模型。这种简单性突显了生成模型与几何问题解决之间令人惊讶的联系。除了这里研究的具体问题外，我们的结果指向一个更广泛的范式：在图像空间操作为近似解决著名难题提供了通用且实用的框架，并为解决更广泛的挑战性几何任务开辟了新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we show that visual diffusion models can serve as effectivegeometric solvers: they can directly reason about geometric problems by workingin pixel space. We first demonstrate this on the Inscribed Square Problem, along-standing problem in geometry that asks whether every Jordan curve containsfour points forming a square. We then extend the approach to two otherwell-known hard geometric problems: the Steiner Tree Problem and the SimplePolygon Problem.  Our method treats each problem instance as an image and trains a standardvisual diffusion model that transforms Gaussian noise into an imagerepresenting a valid approximate solution that closely matches the exact one.The model learns to transform noisy geometric structures into correctconfigurations, effectively recasting geometric reasoning as image generation.  Unlike prior work that necessitates specialized architectures anddomain-specific adaptations when applying diffusion to parametric geometricrepresentations, we employ a standard visual diffusion model that operates onthe visual representation of the problem. This simplicity highlights asurprising bridge between generative modeling and geometric problem solving.Beyond the specific problems studied here, our results point toward a broaderparadigm: operating in image space provides a general and practical frameworkfor approximating notoriously hard problems, and opens the door to tackling afar wider class of challenging geometric tasks.</description>
      <author>example@mail.com (Nir Goren, Shai Yehezkel, Omer Dahary, Andrey Voynov, Or Patashnik, Daniel Cohen-Or)</author>
      <guid isPermaLink="false">2510.21697v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary Radiance Fields</title>
      <link>http://arxiv.org/abs/2510.21441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出OpenHype方法，使用连续双曲潜在空间表示3D场景层次结构，实现了更高效的3D场景理解，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;建模3D对象和3D场景的内在层次结构对自主代理理解环境至关重要，但使用隐式表示如神经辐射场实现这一目标仍面临挑战。现有显式建模层次结构的方法存在局限性：要么需要多次渲染增加推理时间，要么依赖预定义的封闭集离散层次结构，难以泛化到真实世界的多样化结构。&lt;h4&gt;目的&lt;/h4&gt;开发一种能有效表示3D场景层次结构的方法，解决现有方法在推理效率和泛化能力方面的局限性，实现对3D场景更全面、高效的理解。&lt;h4&gt;方法&lt;/h4&gt;提出OpenHype，使用连续双曲潜在空间表示场景层次结构。利用双曲几何特性自然编码多尺度关系，通过潜在空间中的测地线路径实现层次结构的平滑遍历。&lt;h4&gt;主要发现&lt;/h4&gt;OpenHype在标准基准测试中优于最先进的方法，展示了在3D场景理解方面卓越的效率和适应性。&lt;h4&gt;结论&lt;/h4&gt;通过利用双曲几何性质，OpenHype提供了表示和探索3D场景层次结构的有效方式，解决了现有方法的效率和泛化局限性，为自主代理的环境理解提供了更强大工具。&lt;h4&gt;翻译&lt;/h4&gt;建模3D对象和3D场景的内在层次结构是非常可取的，因为它能够使自主代理更全面地理解环境。使用隐式表示（如神经辐射场）来实现这一点仍然是一个未被探索的挑战。明确建模层次结构的现有方法通常面临显著限制：它们要么需要多次渲染传递来捕获不同粒度级别的嵌入，显著增加了推理时间；要么依赖于预定义的封闭集离散层次结构，难以泛化到代理在真实世界中遇到的多样化且细微的结构。为解决这些挑战，我们提出了OpenHype，一种使用连续双曲潜在空间表示场景层次结构的新方法。通过利用双曲几何的特性，OpenHype自然编码了多尺度关系，并能够通过潜在空间中的测地线路径实现层次结构的平滑遍历。我们的方法在标准基准测试中优于最先进的方法，展示了在3D场景理解方面卓越的效率和适应性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何有效表示3D场景中的层次结构问题，特别是在使用神经辐射场(NeRF)等隐式表示时。这个问题很重要，因为理解3D场景的层次结构对自主代理全面理解环境至关重要，例如物体由多个部分组成，也可以在更高层次上语义分组，这种层次组织对于语义分割、场景重建和物体检测等应用非常关键。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行思考，发现现有方法要么需要多次渲染增加推理时间，要么依赖预定义的离散层次结构泛化能力差。他们借鉴了双曲几何的思想，因为双曲空间的指数扩展特性能够自然编码多尺度关系。方法借鉴了现有工作如使用CLIP提取语言特征、使用中性词减少噪声等，但创新性地将双曲几何应用于3D场景层次表示，实现了连续层次遍历。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用双曲空间的几何特性自然编码3D场景的层次结构，实现连续的多尺度关系表示。整体流程包括：1)双曲自编码器训练：将语言对齐特征转换为双曲空间表示，高层对象靠近原点，低层对象靠近边界；2)NeRF训练：监督模型预测双曲特征，使用双曲距离作为损失函数；3)层次遍历：通过沿测地线路径连续遍历层次，解码特征并计算与文本提示的相似度，使用softmax加权聚合结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次在3D场景理解中应用双曲空间自然编码层次结构；2)实现连续层次遍历，只需一次渲染而非多次；3)提出特征外推技术解决多视图一致性问题；4)改进的softmax加权聚合方法处理复杂查询。相比之前工作，OpenHype无需预定义离散层次或多次渲染，能连续遍历层次结构，在处理组合查询时表现更好，解决了现有方法的'词袋效应'问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenHype通过双曲空间几何特性实现了在神经辐射场中连续、高效地表示和遍历3D场景层次结构，显著提升了开放词汇3D场景理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling the inherent hierarchical structure of 3D objects and 3D scenes ishighly desirable, as it enables a more holistic understanding of environmentsfor autonomous agents. Accomplishing this with implicit representations, suchas Neural Radiance Fields, remains an unexplored challenge. Existing methodsthat explicitly model hierarchical structures often face significantlimitations: they either require multiple rendering passes to captureembeddings at different levels of granularity, significantly increasinginference time, or rely on predefined, closed-set discrete hierarchies thatgeneralize poorly to the diverse and nuanced structures encountered by agentsin the real world. To address these challenges, we propose OpenHype, a novelapproach that represents scene hierarchies using a continuous hyperbolic latentspace. By leveraging the properties of hyperbolic geometry, OpenHype naturallyencodes multi-scale relationships and enables smooth traversal of hierarchiesthrough geodesic paths in latent space. Our method outperforms state-of-the-artapproaches on standard benchmarks, demonstrating superior efficiency andadaptability in 3D scene understanding.</description>
      <author>example@mail.com (Lisa Weijler, Sebastian Koch, Fabio Poiesi, Timo Ropinski, Pedro Hermosilla)</author>
      <guid isPermaLink="false">2510.21441v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.21069v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ZING-3D是一个创新的框架，能够生成丰富语义表示的3D场景图，支持增量更新和几何基础，适用于机器人应用。&lt;h4&gt;背景&lt;/h4&gt;理解和推理复杂的3D环境需要结构化的场景表示，捕获物体及其语义和空间关系。现有3D场景图生成工作利用预训练VLMs但存在局限：局限于单视图设置、不支持增量更新、缺乏3D空间几何基础。&lt;h4&gt;目的&lt;/h4&gt;提出ZING-3D框架，利用预训练基础模型实现开放词汇识别，零样本生成丰富场景语义表示，支持增量更新和3D空间几何基础，适用于下游机器人应用。&lt;h4&gt;方法&lt;/h4&gt;利用VLM推理生成丰富2D场景图，使用深度信息与3D空间关联；节点表示开放词汇对象(含特征、3D位置、语义上下文)，边捕获空间和语义关系(含对象间距离)。&lt;h4&gt;主要发现&lt;/h4&gt;在Replica和HM3D数据集上的实验表明，ZING-3D能有效捕获空间和关系知识，无需特定任务训练。&lt;h4&gt;结论&lt;/h4&gt;ZING-3D解决了现有方法的局限性，适用于下游机器人应用。&lt;h4&gt;翻译&lt;/h4&gt;理解和推理复杂的3D环境需要结构化的场景表示，这些表示不仅要捕获物体，还要捕获它们的语义和空间关系。虽然最近关于3D场景图生成的工作利用了没有针对特定任务微调的预训练VLMs，但它们主要局限于单视图设置，无法支持随着新观察到来时的增量更新，并且缺乏在3D空间中的明确几何基础，所有这些对于具身场景都是必不可少的。在本文中，我们提出了ZING-3D框架，它利用预训练基础模型的丰富知识，实现开放词汇识别，并以零样本方式生成丰富的场景语义表示，同时支持在3D空间中进行增量更新和几何基础，使其适用于下游机器人应用。我们的方法利用VLM推理生成丰富的2D场景图，并使用深度信息将其与3D关联。节点表示具有特征、3D位置和语义上下文的开放词汇对象，而边捕获具有对象间距离的空间和语义关系。我们在来自Replica和HM3D数据集的场景上的实验表明，ZING-3D能够在无需特定任务训练的情况下有效地捕获空间和关系知识。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有3D场景图生成方法的三个限制：依赖特定词汇表、只能在单张2D图像上操作、无法增量更新。这个问题在现实中很重要，因为机器人需要在动态环境中在线构建和更新对3D环境的理解，而现有方法无法处理现实世界中的新物体或关系，也无法捕捉不同视角间的空间一致性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从嵌入式AI代理在环境中探索的角度设计系统，考虑如何随着新观测的加入逐步构建场景理解。他们借鉴了Vision-Language Models(VLMs)的进步，特别是像Open-World SGG和Pixels-to-Graphs等利用预训练模型进行零样本关系推理的工作。同时，他们结合了深度信息实现3D几何定位，并使用Grounded-SAM2提供精确的物体分割掩码，将2D场景表示提升到3D空间。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练视觉-语言模型的广泛知识，实现零样本开放词汇表识别，同时支持场景图的增量更新和3D空间中的几何定位。整体流程包括：1)使用VLM进行开放词汇表物体检测；2)构建2D场景图，捕获物体间的空间和语义关系；3)使用Grounded-SAM2获取精确分割掩码，结合深度信息将物体投影到3D空间；4)随着机器人探索，增量更新全局3D场景图；5)根据导航任务需求进行场景图剪枝。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)零样本嵌入式场景图生成，无需任务特定训练；2)丰富的语义信息，节点包含物体特征、3D位置和房间类型，边表示精确的空间关系；3)支持增量更新，场景图随探索过程动态演进。相比之前工作，ZING-3D的独特之处在于结合了2D视觉推理与3D几何信息，支持增量更新，实现了真正的开放词汇表识别，并提供了任务导向的场景图剪枝功能，更适合机器人实际应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ZING-3D通过结合视觉-语言模型与3D几何信息，实现了零样本增量式3D场景图生成，为机器人在复杂环境中的导航和交互提供了结构化的语义-空间表示。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and reasoning about complex 3D environments requires structuredscene representations that capture not only objects but also their semantic andspatial relationships. While recent works on 3D scene graph generation haveleveraged pretrained VLMs without task-specific fine-tuning, they are largelyconfined to single-view settings, fail to support incremental updates as newobservations arrive and lack explicit geometric grounding in 3D space, all ofwhich are essential for embodied scenarios. In this paper, we propose, ZING-3D,a framework that leverages the vast knowledge of pretrained foundation modelsto enable open-vocabulary recognition and generate a rich semanticrepresentation of the scene in a zero-shot manner while also enablingincremental updates and geometric grounding in 3D space, making it suitable fordownstream robotics applications. Our approach leverages VLM reasoning togenerate a rich 2D scene graph, which is grounded in 3D using depthinformation. Nodes represent open-vocabulary objects with features, 3Dlocations, and semantic context, while edges capture spatial and semanticrelations with inter-object distances. Our experiments on scenes from theReplica and HM3D dataset show that ZING-3D is effective at capturing spatialand relational knowledge without the need of task-specific training.</description>
      <author>example@mail.com (Pranav Saxena, Jimmy Chiun)</author>
      <guid isPermaLink="false">2510.21069v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models</title>
      <link>http://arxiv.org/abs/2510.20198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 24 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过五项任务测试了大型语言模型在文本输入上的空间推理能力，发现模型在简单任务中表现中等，但随着复杂性和规模增加，性能显著下降，平均准确率损失42.7%，最高达84%，表明LLMs在空间推理方面存在明显局限性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在自然语言处理方面表现出色，但其空间推理能力尚未被充分研究。&lt;h4&gt;目的&lt;/h4&gt;探究大型语言模型在文本输入上的空间理解和计算能力，识别其在空间推理方面的优势和局限。&lt;h4&gt;方法&lt;/h4&gt;设计并实施了五项空间推理任务：象限识别、几何变换、距离评估、单词搜索和滑块拼图。这些任务在结构化网格环境中进行，通过增加网格尺寸来提高复杂性，要求模型从简单模式识别扩展到抽象空间推理。&lt;h4&gt;主要发现&lt;/h4&gt;1) 模型在复杂性和规模较小的任务中表现中等；2) 随着规模增加，性能迅速下降，准确率平均损失42.7%，最高达84%；3) 所有初始准确率超过50%的测试显示至少48%的性能损失；4) 模型在扩展复杂性方面的挣扎暗示其底层架构中缺乏强大的空间表示。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型在语言推理和空间推理之间存在明显差距，本研究揭示了其当前局限性，并为未来在语言和几何交叉领域的集成基准研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;本文通过一套五项任务，探究了大型语言模型对文本输入的空间推理能力，旨在测试它们的空间理解和计算能力。模型在基于结构化网格环境中的基本空间推理和多步问题解决方面接受了测试，使用了象限识别、几何变换、距离评估、单词搜索和滑块拼图等任务。每个任务通过增加网格尺寸来提高复杂性，要求模型超越简单的模式识别，进入抽象空间推理。我们的结果显示，虽然大型语言模型在复杂性和规模较小的所有任务中表现出中等成功，但随着规模增加，性能迅速下降，准确率平均损失42.7%，最高达到84%。所有初始准确率超过50%的测试都显示出至少48%的损失，说明了性能下降的一致性。此外，模型在扩展复杂性方面的挣扎暗示其底层架构中缺乏强大的空间表示。本文强调了大型语言模型中语言推理和空间推理之间的差距，提供了对其当前局限性的见解，并为未来在语言和几何交叉领域的集成基准研究奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper explores the spatial reasoning capability of large language models(LLMs) over textual input through a suite of five tasks aimed at probing theirspatial understanding and computational abilities. The models were tested onboth fundamental spatial reasoning and multi-step problem-solving withinstructured grid-based environments using tasks such as quadrant identification,geometric transformations, distance evaluation, word searches, and tilesliding. Each task was scaled in complexity through increasing grid dimensions,requiring models to extend beyond simple pattern recognition into abstractspatial reasoning. Our results reveal that while LLMs demonstrate moderatesuccess in all tasks with small complexity and size, performance drops offrapidly as scale increases, with an average loss in accuracy of 42.7%, andreaching as high as 84%. Every test that began with over 50% accuracy showed aloss of at least 48%, illustrating the consistent nature of the deterioration.Furthermore, their struggles with scaling complexity hint at a lack of robustspatial representations in their underlying architectures. This paperunderscores the gap between linguistic and spatial reasoning in LLMs, offeringinsights into their current limitations, and laying the groundwork for futureintegrative benchmarks at the intersection of language and geometry.</description>
      <author>example@mail.com (Maggie Bai, Ava Kim Cohen, Eleanor Koss, Charlie Lichtenbaum)</author>
      <guid isPermaLink="false">2510.20198v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty evaluation of segmentation models for Earth observation</title>
      <link>http://arxiv.org/abs/2510.19586v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了从卫星影像中估计语义分割预测不确定性的方法，针对遥感地球观测应用对现有方法进行了基准测试，评估了不确定性度量的实际效用，并提出了实用建议。&lt;h4&gt;背景&lt;/h4&gt;与标准图像分类相比，分割中的不确定性估计面临独特挑战，需要可扩展的方法来生成逐像素估计。大多数相关研究集中在场景理解或医学影像领域。&lt;h4&gt;目的&lt;/h4&gt;专门针对遥感地球观测应用对不确定性估计方法进行基准测试，评估不确定性度量的实际效用，测试它们识别预测错误和噪声损坏的输入图像区域的能力。&lt;h4&gt;方法&lt;/h4&gt;在两个遥感数据集PASTIS和ForTy上进行实验，这些数据集在规模、地理覆盖范围和标签置信度方面存在差异。评估包括多种模型（如随机分割网络和集成方法）与多种神经网络架构和不确定性度量相结合的广泛评估。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验评估了不同不确定性估计方法在遥感应用中的表现，确定了哪些方法更适合识别预测错误和噪声损坏区域。&lt;h4&gt;结论&lt;/h4&gt;基于研究结果提出了若干实用建议，为遥感影像语义分割中的不确定性估计提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了从卫星影像中估计语义分割预测不确定性的方法。与标准图像分类相比，分割中的不确定性估计面临独特挑战，需要可扩展的方法来生成逐像素估计。虽然大多数关于此主题的研究集中在场景理解或医学影像上，但这项工作专门针对遥感地球观测应用对现有方法进行了基准测试。我们的评估侧重于不确定性度量的实际效用，测试它们识别预测错误和噪声损坏的输入图像区域的能力。实验在两个遥感数据集PASTIS和ForTy上进行，这两个数据集在规模、地理覆盖范围和标签置信度方面存在差异。我们进行了广泛的评估，结合了多种模型（如随机分割网络和集成方法）与多种神经网络架构和不确定性度量。根据我们的发现，我们提出了若干实用建议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates methods for estimating uncertainty in semanticsegmentation predictions derived from satellite imagery. Estimating uncertaintyfor segmentation presents unique challenges compared to standard imageclassification, requiring scalable methods producing per-pixel estimates. Whilemost research on this topic has focused on scene understanding or medicalimaging, this work benchmarks existing methods specifically for remote sensingand Earth observation applications. Our evaluation focuses on the practicalutility of uncertainty measures, testing their ability to identify predictionerrors and noise-corrupted input image regions. Experiments are conducted ontwo remote sensing datasets, PASTIS and ForTy, selected for their differencesin scale, geographic coverage, and label confidence. We perform an extensiveevaluation featuring several models, such as Stochastic Segmentation Networksand ensembles, in combination with a number of neural architectures anduncertainty metrics. We make a number of practical recommendations based on ourfindings.</description>
      <author>example@mail.com (Melanie Rey, Andriy Mnih, Maxim Neumann, Matt Overlan, Drew Purves)</author>
      <guid isPermaLink="false">2510.19586v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</title>
      <link>http://arxiv.org/abs/2510.19400v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The project and benchmark are publicly available at  https://github.com/microsoft/MV-RoboBench&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MV-RoboBench基准测试，用于评估视觉语言模型在机器人操作中的多视图空间推理能力。研究显示当前最先进模型表现远低于人类水平，并发现空间智能与机器人任务执行呈正相关，但单视图基准表现不能可靠预测多视图机器人任务表现。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型对具身人工智能至关重要，是视觉语言动作模型的基础。然而大多数VLM评估集中在单视图设置，对多视图信息整合能力的探索不足。多摄像头设置在机器人平台上越来越标准，能提供互补视角以缓解遮挡和深度模糊问题。&lt;h4&gt;目的&lt;/h4&gt;填补VLMs多视图空间推理能力评估的空白，专门设计一个基准测试来评估VLMs在机器人操作中的多视图空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;创建MV-RoboBench基准测试，包含8个子任务中的1.7k个手动筛选的问答项目，分为空间理解和机器人执行两个主要类别。评估多种现有VLMs（包括开源和闭源模型）以及采用CoT启发技术的增强版本。&lt;h4&gt;主要发现&lt;/h4&gt;(i)在多视图机器人场景中，空间智能和机器人任务执行呈正相关；(ii)在现有通用单视图空间理解基准上的良好表现并不能可靠地转化为在机器人空间任务中的成功。&lt;h4&gt;结论&lt;/h4&gt;当前最先进的VLMs在多视图机器人感知方面仍面临重大挑战。作者发布MV-RoboBench作为开放资源，旨在促进空间感知VLMs和VLAs的进步，提供数据和多视图具身推理的标准化评估协议。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型对具身人工智能至关重要，使机器人能够感知、推理并在复杂环境中行动。它们也是最近视觉语言动作模型的基础。然而，大多数VLM评估集中在单视图设置，对其整合多视图信息的能力探索不足。与此同时，多摄像头设置在机器人平台上越来越标准，因为它们提供互补视角以缓解遮挡和深度模糊问题。因此，VLMs是否能有效利用此类多视图输入进行机器人推理仍然是一个开放问题。为填补这一空白，我们引入MV-RoboBench，一个专门设计用于评估VLMs在机器人操作中多视图空间推理能力的基准测试。MV-RoboBench包含8个子任务中的1.7k个手动筛选的问答项目，分为两个主要类别：空间理解和机器人执行。我们评估了多种现有的VLMs，包括开源和闭源模型，以及采用CoT启发技术的增强版本。结果显示，最先进的模型表现远低于人类水平，突显了VLMs在多视图机器人感知方面面临的重大挑战。此外，我们的分析揭示了两个关键发现：(i)在多视图机器人场景中，空间智能和机器人任务执行呈正相关；(ii)在现有通用单视图空间理解基准上的良好表现并不能可靠地转化为在我们基准评估的机器人空间任务中的成功。我们发布MV-RoboBench作为开放资源，旨在促进空间感知VLMs和VLAs的进步，不仅提供数据，还提供多视图具身推理的标准化评估协议。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的问题是评估视觉语言模型（VLMs）在机器人场景中的多视图空间推理能力。这个问题很重要，因为现有的VLM评估大多集中在单视图设置，而机器人平台越来越多地采用多摄像头系统来提供互补视角以克服遮挡和深度模糊问题。理解VLM能否有效整合这些多视图信息对提升机器人在复杂环境中的感知和决策能力至关重要，也是实现先进视觉语言动作（VLA）模型的基础。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有空间推理基准的局限性，发现它们大多专注于单视图数据或非具身任务，而机器人操作场景需要多视图感知能力。他们借鉴了ShareRobot（具身机器人任务但无多视图）、All-Angles Bench和Ego3D-Bench（多视图但仅限导航或照片对齐）等工作，设计了MV-RoboBench，一个专门针对机器人操作场景中多视图空间推理的基准。作者构建了多阶段管道：数据收集（从AgiWorld和BridgeV2数据集筛选）、问答生成（为八个子任务设计模板）和人工质保审查，确保基准质量和多样性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是评估VLMs能否有效整合多个摄像头视图的互补信息，支持机器人在现实世界中的决策。基准包含1700多个人工策划的问答项目，分为空间理解（跨视图匹配、距离判断、视角识别、3D空间一致性）和机器人执行（动作规划、步骤执行、轨迹选择、功能识别）两大类。实现流程包括：1)数据收集（规则过滤+GPT-4.1辅助筛选+人工验证）；2)问答生成（任务特定模板+五选一问答对构建）；3)人工质保审查（迭代审查+内容修正+答案分布平衡）；4)模型评估（统一零样本提示+准确率作为指标）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个整合空间和机器人推理与多视图输入的机器人操作基准；2)系统性评估VLMs整合多视图信息的能力；3)发现空间智能与机器人执行在多视图场景中正相关；4)揭示单视图基准性能不能可靠转移到多视图机器人任务。相比之前工作，MV-RoboBench专注于具身多视图推理而非抽象任务；使用真实机器人演示而非模板生成；同时评估空间理解和机器人执行；强调多视图互补信息整合而非单一视角分析。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MV-RoboBench是首个专门针对机器人操作场景中多视图空间推理能力的基准测试，通过系统评估现有视觉语言模型的表现，揭示了它们在整合多视图信息进行机器人决策方面的显著不足，并为未来具身人工智能和多视图感知研究提供了新的评估标准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) are essential to Embodied AI, enabling robotsto perceive, reason, and act in complex environments. They also serve as thefoundation for the recent Vision-Language-Action (VLA) models. Yet mostevaluations of VLMs focus on single-view settings, leaving their ability tointegrate multi-view information underexplored. At the same time, multi-camerasetups are increasingly standard in robotic platforms, as they providecomplementary perspectives to mitigate occlusion and depth ambiguity. WhetherVLMs can effectively leverage such multi-view inputs for robotic reasoningtherefore remains an open question. To bridge this gap, we introduceMV-RoboBench, a benchmark specifically designed to evaluate the multi-viewspatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBenchconsists of 1.7k manually curated QA items across eight subtasks, divided intotwo primary categories: spatial understanding and robotic execution. Weevaluate a diverse set of existing VLMs, including both open-source andclosed-source models, along with enhanced versions incorporating CoT-inspiredtechniques. The results show that state-of-the-art models remain far belowhuman performance, underscoring the substantial challenges VLMs face inmulti-view robotic perception. Additionally, our analysis uncovers two keyfindings: (i) spatial intelligence and robotic task execution are positivelycorrelated in multi-view robotic scenarios; and (ii) strong performance onexisting general-purpose single-view spatial understanding benchmarks does notreliably translate to success in the robotic spatial tasks assessed by ourbenchmark. We release MV-RoboBench as an open resource to foster progress inspatially grounded VLMs and VLAs, providing not only data but also astandardized evaluation protocol for multi-view embodied reasoning.</description>
      <author>example@mail.com (Zhiyuan Feng, Zhaolu Kang, Qijie Wang, Zhiying Du, Jiongrui Yan, Shubin Shi, Chengbo Yuan, Huizhi Liang, Yu Deng, Qixiu Li, Rushuai Yang, Arctanx An, Leqi Zheng, Weijie Wang, Shawn Chen, Sicheng Xu, Yaobo Liang, Jiaolong Yang, Baining Guo)</author>
      <guid isPermaLink="false">2510.19400v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Scale Shift in Crowd Localization under the Context of Domain Generalization</title>
      <link>http://arxiv.org/abs/2510.19330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了人群定位中的尺度偏移问题及其在域泛化场景下的影响，提出了Catto算法来减轻尺度偏移的影响，并建立了ScaleBench基准测试。&lt;h4&gt;背景&lt;/h4&gt;人群定位在视觉场景理解中扮演关键角色，但现有方法因训练和测试数据之间头部尺度分布差异（尺度偏移）导致性能显著下降，这一问题被称为域泛化挑战。&lt;h4&gt;目的&lt;/h4&gt;理解人群定位模型在域泛化背景下尺度偏移的本质，解决四个关键问题：尺度偏移如何影响人群定位、如何量化这种影响、产生原因以及如何减轻影响。&lt;h4&gt;方法&lt;/h4&gt;系统检查不同尺度偏移水平下人群定位性能变化；建立ScaleBench基准测试，重现20种先进域泛化算法；提供尺度偏移的严格理论分析；提出因果特征分解和各向异性处理（Catto）算法。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验展示了现有算法的局限性；强调了尺度偏移的重要性和复杂性；提供了四个对未来研究有重要意义的见解。&lt;h4&gt;结论&lt;/h4&gt;强调了'尺度偏移域泛化'这一新颖且适用的研究方向的重要性。&lt;h4&gt;翻译&lt;/h4&gt;人群定位在视觉场景理解中扮演关键角色，用于预测人群中每个行人的位置，因此适用于各种下游任务。然而，由于训练和测试数据之间头部尺度分布的差异（尺度偏移），现有方法性能显著下降，这一挑战被称为域泛化（DG）。本文旨在理解在人群定位模型的域泛化背景下尺度偏移的本质。为此，我们解决了四个关键问题：(i) 尺度偏移如何在域泛化场景中影响人群定位？(ii) 如何量化这种影响？(iii) 产生这种影响的原因是什么？(iv) 如何减轻这种影响？首先，我们系统地检查了人群定位性能如何随不同水平的尺度偏移而变化。然后，我们建立了一个基准ScaleBench，重现了20种先进的域泛化算法来量化这种影响。通过大量实验，我们展示了现有算法的局限性，并强调了尺度偏移的重要性和复杂性，这是一个尚未充分探索的主题。为了加深理解，我们对尺度偏移提供了严格的理论分析。基于这些见解，我们进一步提出了一种名为因果特征分解和各向异性处理（Catto）的有效算法，以减轻域泛化设置中尺度偏移的影响。随后，我们还提供了大量的分析实验，揭示了四个对未来研究有重要意义的见解。我们的结果强调了这一新颖且适用的研究方向的重要性，我们称之为尺度偏移域泛化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crowd localization plays a crucial role in visual scene understanding towardspredicting each pedestrian location in a crowd, thus being applicable tovarious downstream tasks. However, existing approaches suffer from significantperformance degradation due to discrepancies in head scale distributions (scaleshift) between training and testing data, a challenge known as domaingeneralization (DG). This paper aims to comprehend the nature of scale shiftwithin the context of domain generalization for crowd localization models. Tothis end, we address four critical questions: (i) How does scale shiftinfluence crowd localization in a DG scenario? (ii) How can we quantify thisinfluence? (iii) What causes this influence? (iv) How to mitigate theinfluence? Initially, we conduct a systematic examination of how crowdlocalization performance varies with different levels of scale shift. Then, weestablish a benchmark, ScaleBench, and reproduce 20 advanced DG algorithms toquantify the influence. Through extensive experiments, we demonstrate thelimitations of existing algorithms and underscore the importance and complexityof scale shift, a topic that remains insufficiently explored. To deepen ourunderstanding, we provide a rigorous theoretical analysis on scale shift.Building on these insights, we further propose an effective algorithm calledCausal Feature Decomposition and Anisotropic Processing (Catto) to mitigate theinfluence of scale shift in DG settings. Later, we also provide extensiveanalytical experiments, revealing four significant insights for futureresearch. Our results emphasize the importance of this novel and applicableresearch direction, which we term Scale Shift Domain Generalization.</description>
      <author>example@mail.com (Juncheng Wang, Lei Shang, Ziqi Liu, Wang Lu, Xixu Hu, Zhe Hu, Jindong Wang, Shujun Wang)</author>
      <guid isPermaLink="false">2510.19330v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning</title>
      <link>http://arxiv.org/abs/2510.18337v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MoTVLA是一种基于混合变压器的视觉-语言-动作模型，结合了快速-慢速统一推理与行为策略学习，解决了现有方法中语言控制能力有限和推理延迟显著的问题。&lt;h4&gt;背景&lt;/h4&gt;将视觉语言指令整合到视觉运动策略中是增强机器人开放世界泛化能力的热门研究方向。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够平衡语言控制能力和执行效率的模型，解决现有方法中的两个主要挑战。&lt;h4&gt;方法&lt;/h4&gt;MoTVLA模型保留了预训练视觉语言模型的通用智能，同时引入一个与预训练模型共享知识的领域专家transformer，生成领域特定的快速推理，并将动作专家基于分解的运动指令进行条件化。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛评估，MoTVLA在快速-慢速推理和操作任务性能方面表现出优越性，能够学习多样化行为并显著提高语言控制能力。&lt;h4&gt;结论&lt;/h4&gt;MoTVLA成功整合了快速-慢速统一推理与行为策略学习，有效解决了现有方法中的局限性，为机器人学习提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;将视觉语言指令整合到视觉运动策略中正在增强机器人学习的开放世界泛化能力方面获得动力。尽管有 promising 的进展，现有方法面临两个挑战：在不使用生成推理作为条件时，语言控制能力有限，或者在整合推理时推理延迟显著。在这项工作中，我们引入了MoTVLA，一种基于混合变压器(MoT)的视觉-语言-动作(VLA)模型，它整合了快速-慢速统一推理与行为策略学习。MoTVLA保留了预训练VLMs的通用智能（作为通用者）用于感知、场景理解和语义规划等任务，同时整合了一个领域专家（第二个transformer），它与预训练VLM共享知识，以生成领域特定的快速推理（例如机器人运动分解），从而提高策略执行效率。通过将动作专家基于分解的运动指令进行条件化，MoTVLA能够学习多样化行为并显著提高语言控制能力。在自然语言处理基准、机器人仿真环境和真实世界实验中的广泛评估证实了MoTVLA在快速-慢速推理和操作任务性能方面的优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在机器人学习中整合视觉语言指令时面临的两个挑战：一是当不使用生成的推理作为条件时语言控制能力有限，二是当整合推理时推理延迟显著。这个问题很重要，因为它限制了机器人在开放世界中的泛化能力和实时应用，影响了机器人在需要快速响应和精确控制的环境中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性：传统视觉语言动作模型在连续动作表示上存在问题，扩散策略虽然适合连续动作空间但语言控制能力有限。他们提出通过'分解-组合-再分解'的混合变换器架构统一快速和慢速推理。该方法借鉴了混合变换器架构、预训练视觉语言模型、扩散策略等现有工作，并参考了BAGEL模型中的Vision Transformer和Qwen2.5 LLM的文本分词器。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过混合变换器架构统一快速和慢速推理，在一个模型中同时保留通用智能和领域特定知识，使用'分解-组合-再分解'流程实现知识共享。整体流程包括：输入空间设计(处理语言、RGB图像和可学习查询)；推理骨干设计(通用专家负责慢速推理，领域专家负责快速推理)；推理输出设计(统一在文本空间但分为两种功能)；动作专家设计(使用扩散变换器生成动作)；训练流程(领域专家微调和动作专家扩散策略训练)；推理流程(支持对话模式和动作模式两种交互方式)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一快速-慢速推理的MoT架构；2)基于分解运动条件的策略学习；3)支持对话和动作的双模式操作。相比之前的工作，MoTVLA解决了连续动作表示的精度问题，显式生成推理内容提高语言控制能力，显著降低推理延迟，并通过知识共享避免了灾难性遗忘，实现了更好的知识保留和执行效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MoTVLA通过混合变换器架构统一了快速和慢速推理，在保持通用视觉语言模型智能的同时，实现了高效、可解释的机器人操作策略学习，显著提升了语言控制能力和任务执行效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating visual-language instructions into visuomotor policies is gainingmomentum in robot learning for enhancing open-world generalization. Despitepromising advances, existing approaches face two challenges: limited languagesteerability when no generated reasoning is used as a condition, or significantinference latency when reasoning is incorporated. In this work, we introduceMoTVLA, a mixture-of-transformers (MoT)-based vision-language-action (VLA)model that integrates fast-slow unified reasoning with behavior policylearning. MoTVLA preserves the general intelligence of pre-trained VLMs(serving as the generalist) for tasks such as perception, scene understanding,and semantic planning, while incorporating a domain expert, a secondtransformer that shares knowledge with the pretrained VLM, to generatedomain-specific fast reasoning (e.g., robot motion decomposition), therebyimproving policy execution efficiency. By conditioning the action expert ondecomposed motion instructions, MoTVLA can learn diverse behaviors andsubstantially improve language steerability. Extensive evaluations acrossnatural language processing benchmarks, robotic simulation environments, andreal-world experiments confirm the superiority of MoTVLA in both fast-slowreasoning and manipulation task performance.</description>
      <author>example@mail.com (Wenhui Huang, Changhe Chen, Han Qi, Chen Lv, Yilun Du, Heng Yang)</author>
      <guid isPermaLink="false">2510.18337v3</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.14828v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RoboGPT-R1的两阶段微调框架，用于提高具身智能体在长期操作任务中的推理能力，显著提升了模型在EmbodiedBench基准测试上的表现。&lt;h4&gt;背景&lt;/h4&gt;提高具身智能体的推理能力对机器人在长期操作任务中成功完成复杂人类指令至关重要。尽管基于监督微调的大语言模型和视觉语言模型在规划任务中取得成功，但在复杂真实环境中的长期操作任务仍面临挑战，这是由于它们有限的常识和推理能力。&lt;h4&gt;目的&lt;/h4&gt;解决通用视觉语言模型通过监督微调对机器人规划任务的泛化能力差和物理理解不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出RoboGPT-R1框架，第一阶段通过监督训练使用专家序列获取基础知识，第二阶段使用强化学习解决模型在视觉空间理解和推理方面的不足。设计基于规则的奖励函数，同时考虑长期性能和环境中的动作约束，以实现多步推理任务中的物理理解和动作序列一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在Qwen2.5-VL-3B上训练的推理模型在EmbodiedBench基准测试上显著优于更大规模的GPT-4o-mini模型，提高了21.33%，超越了在Qwen2.5-VL-7B上训练的其他工作，提高了20.33%。&lt;h4&gt;结论&lt;/h4&gt;RoboGPT-R1框架有效提高了具身智能体的推理能力和长期操作任务表现。&lt;h4&gt;翻译&lt;/h4&gt;提高具身智能体的推理能力对于机器人在长期操作任务中成功完成复杂的人类指令至关重要。尽管基于监督微调的大语言模型和视觉语言模型在规划任务中取得了成功，但由于其有限的常识和推理能力，它们在复杂真实环境中执行长期操作任务时仍面临挑战。考虑到通过监督微调将通用视觉语言模型与机器人规划任务对齐存在泛化能力差和物理理解不足的问题，我们提出了RoboGPT-R1，一个用于具身规划的两阶段微调框架。在该框架中，监督训练通过专家序列获取基础知识，随后使用强化学习来解决模型在视觉空间理解和推理方面的不足。为了在多步推理任务中实现物理理解和动作序列一致性，我们设计了一个基于规则的奖励函数，同时考虑长期性能和环境中的动作约束。在Qwen2.5-VL-3B上训练的推理模型在EmbodiedBench基准测试上显著优于更大规模的GPT-4o-mini模型，提高了21.33%，并超越了在Qwen2.5-VL-7B上训练的其他工作，提高了20.33%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Improving the reasoning capabilities of embodied agents is crucial for robotsto complete complex human instructions in long-view manipulation taskssuccessfully. Despite the success of large language models and vision languagemodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continuefacing challenges in performing long-horizon manipulation tasks in complexreal-world environments, owing to their restricted common sense and reasoningcapabilities. Considering that aligning general-purpose vision language modelsto robotic planning tasks via supervised fine-tuning suffers from poorgeneralization and insufficient physical understanding, we propose RoboGPT-R1,a two-stage fine-tuning framework for embodied planning. In this framework,supervised training acquires foundational knowledge through expert sequences,followed by RL to address the model's shortcomings in visual-spatialunderstanding and reasoning. To achieve physical understanding and actionsequence consistency in multi-step reasoning tasks, we design a rule-basedreward function that simultaneously considers long-horizon performance andaction constraint in the environment. The reasoning model, trained onQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on theEmbodiedBench benchmark.</description>
      <author>example@mail.com (Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li)</author>
      <guid isPermaLink="false">2510.14828v2</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning</title>
      <link>http://arxiv.org/abs/2510.21635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DAP-MAE的领域自适应点云掩码自编码器方法，用于解决跨领域点云数据整合问题，提高下游3D点云分析任务性能。&lt;h4&gt;背景&lt;/h4&gt;与2D数据相比，可用于训练的点云数据在不同领域中规模有限，研究人员尝试结合不同领域数据进行MAE预训练以缓解数据稀缺问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自适应整合跨领域数据集知识的方法，以改善通用点云分析任务性能。&lt;h4&gt;方法&lt;/h4&gt;设计了异构领域适配器，在预训练阶段使用适配模式学习跨领域点云信息，在微调阶段采用融合模式增强特征；同时引入领域特征生成器指导点云特征适应下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;仅通过一次预训练，DAP-MAE在四种点云分析任务上表现优异，在ScanObjectNN上的目标分类达到95.18%，在Bosphorus上的面部表情识别达到88.45%。&lt;h4&gt;结论&lt;/h4&gt;DAP-MAE有效解决了跨领域点云数据整合问题，提高了下游任务性能，为点云分析提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;与2D数据相比，可用于训练的点云数据在不同领域的规模相当有限。研究人员一直在尝试结合这些不同领域的数据进行掩码自编码器预训练，以利用这种数据稀缺问题。然而，从混合领域学到的先验知识可能与下游3D点云分析任务不太匹配，导致性能下降。为解决这一问题，我们提出了领域自适应点云掩码自编码器，这是一种MAE预训练方法，可以自适应地整合跨领域数据集的知识，用于通用点云分析。在DAP-MAE中，我们设计了一个异构领域适配器，在预训练期间使用适配模式，使模型能够全面学习来自不同领域点云的信息，同时在微调阶段采用融合模式以增强点云特征。同时，DAP-MAE集成了一个领域特征生成器，指导点云特征适应各种下游任务。仅通过一次预训练，DAP-MAE在四种不同的点云分析任务上取得了优异的性能，在ScanObjectNN上的目标分类达到95.18%，在Bosphorus上的面部表情识别达到88.45%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云数据在不同领域（物体、人脸、场景）之间的迁移学习问题。在现实中，3D点云数据的收集和标注需要大量资源，导致各领域可用数据有限。现有方法通常只在单一领域内进行预训练，当应用于不同领域任务时性能显著下降。解决这个问题对于实现通用3D点云分析至关重要，可应用于自动驾驶、机器人、增强/虚拟现实等领域，有效利用有限的数据资源并提高模型泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云MAE方法的局限性：单一领域预训练导致跨领域性能下降，简单组合多领域数据也会因信息误解为噪声而降低性能。基于此，他们设计了DAP-MAE框架，包含异构领域适配器(HDA)和领域特征生成器(DFG)两个核心组件。该方法借鉴了掩码自编码器(MAE)的自监督学习框架、Transformer架构、PointNet的点云处理方法以及对比学习技术，但创新性地将其应用于跨领域点云学习场景，实现了单模态一次预训练适应多任务的目标。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过异构领域适配器和领域特征生成器，使模型能够协作学习来自不同领域的点云数据，实现一次预训练适应多种下游任务。整体流程分为两阶段：1)预训练阶段：使用来自物体、人脸、场景三个领域的数据，通过HDA的适应模式分别处理各领域数据，使用Transformer编码器-解码器架构进行掩码重建，同时DFG提取领域特征并通过对比损失训练；2)微调阶段：针对特定下游任务，使用HDA的融合模式整合多领域信息，DFG提取领域和类别特征，输入任务头进行训练。这种方法既保留了各领域的特性，又实现了跨领域知识的有效迁移。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出DAP-MAE框架，实现单模态一次预训练适应多任务；2)设计异构领域适配器(HDA)，预训练时使用适应模式分别处理不同领域，微调时使用融合模式整合信息；3)引入领域特征生成器(DFG)提取多样化领域特征指导下游任务。相比之前工作：与传统MAE不同，DAP-MAE能跨领域学习；与简单组合多领域数据的方法不同，它避免将跨域信息误解为噪声；与跨模态方法不同，它专注于单模态点云数据降低训练成本；在多个下游任务上实现了优于其他自监督方法的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DAP-MAE通过异构领域适配器和领域特征生成器实现了跨领域点云数据的有效协作学习，仅需一次预训练就能在多种3D点云分析任务上达到顶尖性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compared to 2D data, the scale of point cloud data in different domainsavailable for training, is quite limited. Researchers have been trying tocombine these data of different domains for masked autoencoder (MAE)pre-training to leverage such a data scarcity issue. However, the priorknowledge learned from mixed domains may not align well with the downstream 3Dpoint cloud analysis tasks, leading to degraded performance. To address such anissue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE),an MAE pre-training method, to adaptively integrate the knowledge ofcross-domain datasets for general point cloud analysis. In DAP-MAE, we design aheterogeneous domain adapter that utilizes an adaptation mode duringpre-training, enabling the model to comprehensively learn information frompoint clouds across different domains, while employing a fusion mode in thefine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates adomain feature generator to guide the adaptation of point cloud features tovarious downstream tasks. With only one pre-training, DAP-MAE achievesexcellent performance across four different point cloud analysis tasks,reaching 95.18% in object classification on ScanObjectNN and 88.45% in facialexpression recognition on Bosphorus.</description>
      <author>example@mail.com (Ziqi Gao, Qiufu Li, Linlin Shen)</author>
      <guid isPermaLink="false">2510.21635v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization</title>
      <link>http://arxiv.org/abs/2510.20974v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PCA点云(PPC)框架，用于解决点云强化学习中的相机姿态不匹配问题，通过将点云映射到规范姿态，显著提高了对视角变化的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;从原始视觉输入的强化学习近年来取得了显著成功，但它对分布外变化(如光照、颜色和视角变化)仍然很脆弱。点云强化学习提供了一种有前景的替代方案，减轻了基于外观的脆弱性，但其对相机姿态不匹配的敏感性继续削弱了在现实环境中的可靠性。&lt;h4&gt;目的&lt;/h4&gt;解决点云强化学习对相机姿态不匹配敏感的挑战，提高在现实场景中的可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出PCA点云(PPC)框架，这是一个专门为下游机器人控制设计的规范化框架，它将任意刚体变换下的点云映射到唯一的规范姿态，将观测结果对齐到一致的坐标系。&lt;h4&gt;主要发现&lt;/h4&gt;PPC显著减少了视角引起的不一致性，在实验中提高了在具有挑战性的机器人任务中对未见过的相机姿态的鲁棒性，为域随机化提供了有原则的替代方案。&lt;h4&gt;结论&lt;/h4&gt;PPC框架有效地解决了点云强化学习中的相机姿态不匹配问题，提高了在现实场景中的鲁棒性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;从原始视觉输入的强化学习近年来取得了显著成功，但它对分布外变化(如光照、颜色和视角变化)仍然很脆弱。点云强化学习提供了一种有前景的替代方案，减轻了基于外观的脆弱性，但其对相机姿态不匹配的敏感性继续削弱了在现实环境中的可靠性。为应对这一挑战，我们提出了PCA点云(PPC)，这是一个专门为下游机器人控制设计的规范化框架。PPC将任意刚体变换下的点云映射到唯一的规范姿态，将观测结果对齐到一致的坐标系，从而显著减少了视角引起的不一致性。在我们的实验中，我们证明了PPC提高了在具有挑战性的机器人任务中对未见过的相机姿态的鲁棒性，为域随机化提供了有原则的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) from raw visual input has achieved impressivesuccesses in recent years, yet it remains fragile to out-of-distributionvariations such as changes in lighting, color, and viewpoint. Point CloudReinforcement Learning (PC-RL) offers a promising alternative by mitigatingappearance-based brittleness, but its sensitivity to camera pose mismatchescontinues to undermine reliability in realistic settings. To address thischallenge, we propose PCA Point Cloud (PPC), a canonicalization frameworkspecifically tailored for downstream robotic control. PPC maps point cloudsunder arbitrary rigid-body transformations to a unique canonical pose, aligningobservations to a consistent frame, thereby substantially decreasingviewpoint-induced inconsistencies. In our experiments, we show that PPCimproves robustness to unseen camera poses across challenging robotic tasks,providing a principled alternative to domain randomization.</description>
      <author>example@mail.com (Michael Bezick, Vittorio Giammarino, Ahmed H. Qureshi)</author>
      <guid isPermaLink="false">2510.20974v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Fractional harmonic transform on point cloud manifolds</title>
      <link>http://arxiv.org/abs/2510.20842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种点云流形分数阶谐波变换(PMFHT)，通过引入分数阶参数扩展了传统的点云流形谐波变换(PMHT)，构建了空间域和频率域之间可连续调节的中间分数阶谱域，实现了更灵活的变换和滤波操作。&lt;h4&gt;背景&lt;/h4&gt;点云可被视为光滑流形的离散样本，可使用拉普拉斯-贝尔特拉米算子进行谱分析。然而，传统的PMHT受限于固定基函数和单一谱表示，难以捕获复杂几何特征。&lt;h4&gt;目的&lt;/h4&gt;提出PMFHT来克服传统PMHT的局限性，通过引入分数阶参数构建连续可调的中间谱域。&lt;h4&gt;方法&lt;/h4&gt;引入分数阶参数，构建空间域和频率域之间的中间分数阶谱域，支持更灵活的变换和滤波操作。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，选择不同的变换顺序可以丰富点云的谱表示，在滤波和特征增强等任务中取得优异结果。&lt;h4&gt;结论&lt;/h4&gt;PMFHT扩展了点云谱分析的理论框架，为流形几何处理提供了强大的新工具。&lt;h4&gt;翻译&lt;/h4&gt;三维点云可以被视为光滑流形的离散样本，允许使用拉普拉斯-贝尔特拉米算子进行谱分析。然而，传统的点云流形谐波变换(PMHT)受其固定基函数和单一谱表示的限制，限制了其捕获复杂几何特征的能力。本文提出了一种点云流形分数阶谐波变换(PMFHT)，通过引入分数阶参数推广了PMHT，并构建了空间域和频率域之间可连续调节的中间分数阶谱域。这种分数阶框架支持更灵活的变换和滤波操作。实验表明，选择不同的变换顺序可以丰富点云的谱表示，并在滤波和特征增强等任务中取得优异结果。因此，PMFHT不仅扩展了点云谱分析的理论框架，还为流形几何处理提供了强大的新工具。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决传统点云流形谐波变换(PMHT)的局限性，即其固定的基函数和单一频谱表示无法充分捕捉复杂几何特征。这一问题很重要，因为三维点云是3D场景最常见的数据表示形式之一，广泛应用于LiDAR、结构光扫描和立体重建等领域，有效的几何特征提取对点云处理至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统PMHT的局限性，然后从信号处理中的分数傅里叶变换(FRFT)获得启发，后者通过引入分数阶参数解决了类似限制，提供了时域和频域之间的连续中间表示。作者借鉴了PMHT的基础框架、FRFT的分数阶参数思想以及LBO在点云上的离散化方法，将流形谐波扩展为分数阶形式，通过非线性缩放LBO特征值创建连续可调的中间频谱域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入分数阶参数α，将传统PMHT扩展为分数阶形式(PMFHT)，创建空间域和频率域之间可连续调整的中间分数阶频谱域。实现流程包括：1)构建离散拉普拉斯-贝尔特拉米算子；2)求解广义特征值问题获得点云流形谐波基；3)定义分数阶傅里叶矩阵和点云流形分数阶谐波变换；4)应用变换进行不同类型的滤波操作，如特征增强或平滑处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)构建点云流形分数阶谐波变换的统一框架；2)提供简单高效的PMFHT分数幂公式；3)证明PMFHT能提供更丰富的频谱表示并在点云处理任务中表现出色。相比传统PMHT，PMFHT引入分数阶参数提供连续可调的中间频谱域，能捕捉多尺度几何特征，通过选择不同变换阶数丰富频谱表示，为流形几何处理提供新工具。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了点云流形分数阶谐波变换(PMFHT)，通过引入分数阶参数扩展了传统点云流形谐波变换，提供了空间域和频率域之间的连续可调中间表示，丰富了点云的频谱分析能力，并在点云处理任务中展现出优越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional point clouds can be viewed as discrete samples of smoothmanifolds, allowing spectral analysis using the Laplace-Beltrami operator(LBO). However, the traditional point cloud manifold harmonic transform (PMHT)is limited by its fixed basis functions and single spectral representation,which restricts its ability to capture complex geometric features. This paperproposes a point cloud manifold fractional harmonic transform (PMFHT), whichgeneralizes PMHT by introducing fractional-order parameters and constructs acontinuously adjustable intermediate fractional-order spectral domain betweenthe spatial domain and the frequency domain. This fractional-order frameworksupports more flexible transformation and filtering operations. Experimentsshow that choosing different transformation orders can enrich the spectralrepresentation of point clouds and achieve excellent results in tasks such asfiltering and feature enhancement. Therefore, PMFHT not only expands thetheoretical framework of point cloud spectral analysis, but also provides apowerful new tool for manifold geometry processing.</description>
      <author>example@mail.com (Jiamian Li, Bing-Zhao Li)</author>
      <guid isPermaLink="false">2510.20842v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>REVE: A Foundation Model for EEG -- Adapting to Any Setup with Large-Scale Pretraining on 25,000 Subjects</title>
      <link>http://arxiv.org/abs/2510.21585v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at: https://brain-bzh.github.io/reve/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了REVE模型，一个专为EEG信号设计的基础模型，能够处理不同长度和电极排列的EEG信号，并在多种下游任务中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;基础模型通过大规模预训练减少了任务特定数据的依赖，在语言和视觉领域取得成功，但在EEG领域的应用滞后。公共EEG数据集的异质性（不同协议、设备和电极配置）导致现有EEG基础模型难以泛化，现有模型通常限制在单一设置下预训练，导致次优性能，特别是在线性探测下。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够跨多样化EEG信号泛化的预训练模型REVE（Representation for EEG with Versatile Embeddings）。&lt;h4&gt;方法&lt;/h4&gt;引入了一种新颖的4D位置编码方案，使其能够处理任意长度和电极排列的信号；使用掩码自编码目标函数进行预训练；在来自92个数据集、25,000名受试者的超过60,000小时EEG数据上预训练REVE，这是迄今为止最大的EEG预训练工作。&lt;h4&gt;主要发现&lt;/h4&gt;REVE在10个下游EEG任务上取得了最先进的结果，包括运动想象分类、癫痫检测、睡眠分期、认知负荷估计和情绪识别；几乎不需要微调的情况下，REVE展示了强大的泛化能力和细致的时空建模能力。&lt;h4&gt;结论&lt;/h4&gt;REVE为EEG信号处理提供了新的基础模型，能够处理多样化的EEG数据；研究团队发布了代码、预训练权重和教程，以支持标准化的EEG研究并加速临床神经科学的进展。&lt;h4&gt;翻译&lt;/h4&gt;基础模型通过大规模预训练减少了对任务特定数据的依赖，从而改变了人工智能领域。尽管在语言和视觉领域取得了成功，但由于公共数据集的异质性（收集于不同的协议、设备和电极配置下），它们在EEG中的应用一直滞后。现有的EEG基础模型难以在这些变化中泛化，通常将预训练限制在单一设置中，导致次优性能，特别是在线性探测下。我们提出了REVE（Representation for EEG with Versatile Embeddings），一个明确设计为能够泛化到多样化EEG信号的预训练模型。REVE引入了一种新颖的4D位置编码方案，使其能够处理任意长度和电极排列的信号。使用掩码自编码目标函数，我们在来自92个数据集、25,000名受试者的超过60,000小时EEG数据上预训练了REVE，这是迄今为止最大的EEG预训练工作。REVE在10个下游EEG任务上取得了最先进的结果，包括运动想象分类、癫痫检测、睡眠分期、认知负荷估计和情绪识别。几乎不需要微调的情况下，它展示了强大的泛化能力和细致的时空建模能力。我们发布了代码、预训练权重和教程，以支持标准化的EEG研究并加速临床神经科学的进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have transformed AI by reducing reliance on task-specificdata through large-scale pretraining. While successful in language and vision,their adoption in EEG has lagged due to the heterogeneity of public datasets,which are collected under varying protocols, devices, and electrodeconfigurations. Existing EEG foundation models struggle to generalize acrossthese variations, often restricting pretraining to a single setup, resulting insuboptimal performance, in particular under linear probing. We present REVE(Representation for EEG with Versatile Embeddings), a pretrained modelexplicitly designed to generalize across diverse EEG signals. REVE introduces anovel 4D positional encoding scheme that enables it to process signals ofarbitrary length and electrode arrangement. Using a masked autoencodingobjective, we pretrain REVE on over 60,000 hours of EEG data from 92 datasetsspanning 25,000 subjects, representing the largest EEG pretraining effort todate. REVE achieves state-of-the-art results on 10 downstream EEG tasks,including motor imagery classification, seizure detection, sleep staging,cognitive load estimation, and emotion recognition. With little to nofine-tuning, it demonstrates strong generalization, and nuanced spatio-temporalmodeling. We release code, pretrained weights, and tutorials to supportstandardized EEG research and accelerate progress in clinical neuroscience.</description>
      <author>example@mail.com (Yassine El Ouahidi, Jonathan Lys, Philipp Thölke, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon, Karim Jerbi, Giulia Lioi)</author>
      <guid isPermaLink="false">2510.21585v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>MUVR: A Multi-Modal Untrimmed Video Retrieval Benchmark with Multi-Level Visual Correspondence</title>
      <link>http://arxiv.org/abs/2510.21406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025 D&amp;B Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了多模态未修剪视频检索(MUVR)任务及相应基准数据集，旨在推进长视频平台上的视频检索技术。该研究构建了实用的检索范式、多层次视觉对应和全面的评估标准，并对多种先进模型进行了评估，揭示了当前方法在处理未修剪视频和多模态查询方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;随着长视频平台的普及，视频检索技术面临新的挑战。现有的视频检索方法主要针对修剪过的短视频，而长视频平台上的视频通常包含多个相关片段，需要更灵活的检索方式来满足用户需求。&lt;h4&gt;目的&lt;/h4&gt;提出并构建一个专门针对长视频平台的多模态未修剪视频检索任务和基准数据集，以促进该领域的研究和发展，并评估现有方法在这一新任务上的表现。&lt;h4&gt;方法&lt;/h4&gt;设计了MUVR基准数据集，包含53K个来自Bilibili的未修剪视频、1,050个多模态查询和84K个匹配。构建了以视频为中心的多模态查询支持长文本描述、视频标签提示和掩码提示。建立了六个级别的多层次视觉对应标准（副本、事件、场景、实例、动作和其他）。开发了三个版本的评估基准（基础版、过滤版、问答版），并提出了重新排序分数评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果显示，当前的视频检索模型在处理未修剪视频和多模态查询方面存在明显局限性；MLLMs在多视频理解和重新排序能力上也表现出不足，这为未来研究指明了方向。&lt;h4&gt;结论&lt;/h4&gt;MUVR基准为长视频平台上的视频检索研究提供了新的评估框架，揭示了现有方法的不足，并为未来改进提供了方向。该研究有助于推动多模态未修剪视频检索领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了多模态未修剪视频检索任务，并创建了一个新的基准(MUVR)以推进长视频平台的视频检索。MUVR旨在使用多模态查询检索包含相关片段的未修剪视频。它具有以下特点：1)实用的检索范式：MUVR支持以视频为中心的多模态查询，通过长文本描述、视频标签提示和掩码提示表达细粒度检索需求。它采用一对多检索范式，专注于未修剪视频，专为长视频平台应用定制。2)多层次视觉对应：为了涵盖常见的视频类别（如新闻、旅行、舞蹈）并精确定义检索匹配标准，我们基于用户感兴趣且想要检索的核心视频内容（如新闻事件、旅行地点、舞蹈动作）构建了多层次视觉对应。它涵盖六个级别：副本、事件、场景、实例、动作和其他。3)全面的评估标准：我们开发了3个版本的MUVR（即基础版、过滤版、问答版）。MUVR-Base/Filter评估检索模型，而MUVR-QA以问答格式评估MLLMs。我们还提出了重新排序分数来评估MLLMs的重新排序能力。MUVR包含来自Bilibili视频平台的53K个未修剪视频，有1,050个多模态查询和84K个匹配。我们对3个最先进的视频检索模型、6个基于图像的VLMs和10个MLLMs进行了广泛评估。MUVR揭示了检索方法在处理未修剪视频和多模态查询方面的局限性，以及MLLMs在多视频理解和重新排序方面的局限性。我们的代码和基准可在https://github.com/debby-0527/MUVR获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the Multi-modal Untrimmed Video Retrieval task, along with a newbenchmark (MUVR) to advance video retrieval for long-video platforms. MUVR aimsto retrieve untrimmed videos containing relevant segments using multi-modalqueries. It has the following features: 1) Practical retrieval paradigm: MUVRsupports video-centric multi-modal queries, expressing fine-grained retrievalneeds through long text descriptions, video tag prompts, and mask prompts. Itadopts a one-to-many retrieval paradigm and focuses on untrimmed videos,tailored for long-video platform applications. 2) Multi-level visualcorrespondence: To cover common video categories (e.g., news, travel, dance)and precisely define retrieval matching criteria, we construct multi-levelvisual correspondence based on core video content (e.g., news events, travellocations, dance moves) which users are interested in and want to retrieve. Itcovers six levels: copy, event, scene, instance, action, and others. 3)Comprehensive evaluation criteria: We develop 3 versions of MUVR (i.e., Base,Filter, QA). MUVR-Base/Filter evaluates retrieval models, while MUVR-QAassesses MLLMs in a question-answering format. We also propose a RerankingScore to evaluate the reranking ability of MLLMs. MUVR consists of 53Kuntrimmed videos from the video platform Bilibili, with 1,050 multi-modalqueries and 84K matches. Extensive evaluations of 3 state-of-the-art videoretrieval models, 6 image-based VLMs, and 10 MLLMs are conducted. MUVR revealsthe limitations of retrieval methods in processing untrimmed videos andmulti-modal queries, as well as MLLMs in multi-video understanding andreranking. Our code and benchmark is available athttps://github.com/debby-0527/MUVR.</description>
      <author>example@mail.com (Yue Feng, Jinwei Hu, Qijia Lu, Jiawei Niu, Li Tan, Shuo Yuan, Ziyi Yan, Yizhen Jia, Qingzhi He, Shiping Ge, Ethan Q. Chen, Wentong Li, Limin Wang, Jie Qin)</author>
      <guid isPermaLink="false">2510.21406v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>HRT1: One-Shot Human-to-Robot Trajectory Transfer for Mobile Manipulation</title>
      <link>http://arxiv.org/abs/2510.21026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 11 figures and 3 tables. Project page is available at  \url{https://irvlutd.github.io/HRT1/}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一个新颖的人机轨迹传递系统，使机器人能够通过学习人类示范视频来操作物体，系统包含四个模块，实现了机器人观看一次人类示范后就能在不同环境中重复相同操作任务的能力。&lt;h4&gt;背景&lt;/h4&gt;机器人操作任务通常需要精确的编程和大量调整，而人类能够直观地通过示范学习操作任务。如何让机器人从人类示范中学习操作技能是一个重要研究方向。&lt;h4&gt;目的&lt;/h4&gt;开发一个系统，使机器人能够通过观看人类示范视频来学习操作任务，并能在不同环境中重复这些任务，即使物体放置方式与示范不同。&lt;h4&gt;方法&lt;/h4&gt;系统包含四个模块：1)使用AR头戴设备从机器人视角收集人类示范视频的数据收集模块；2)从示范视频中检测物体并提取3D人类手部轨迹的视频理解模块；3)将人类手部轨迹转换为机器人末端执行器参考轨迹的轨迹传递模块；4)利用轨迹优化算法解决机器人配置空间中轨迹问题的轨迹优化模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明该系统能够使移动机械臂观看一次人类示范视频后，就能在不同的环境中重复相同的移动操作任务，即使物体放置方式与示范不同。&lt;h4&gt;结论&lt;/h4&gt;该系统有效地实现了从人类示范到机器人操作的技能传递，为机器人学习人类操作任务提供了一种直观、灵活的方法。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种新颖的人机轨迹传递系统，使机器人能够通过学习人类示范视频来操作物体。该系统由四个模块组成。第一个模块是数据收集模块，旨在使用AR头戴设备从机器人视角收集人类示范视频。第二个模块是视频理解模块，从示范视频中检测物体并提取3D人类手部轨迹。第三个模块将人类手部轨迹转换为3D空间中机器人末端执行器的参考轨迹。最后一个模块利用轨迹优化算法解决机器人配置空间中的轨迹问题，使其能够遵循从人类示范传递而来的末端执行器轨迹。因此，这些模块使机器人能够观看一次人类示范视频，然后在不同环境中重复相同的移动操作任务，即使物体放置方式与示范不同。我们在移动机械臂上进行了不同操作任务的实验，以验证我们系统的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何让机器人通过观看一次人类演示视频，就能在不同环境中重复执行相同的移动操作任务。这个问题在现实中很重要，因为传统的机器人操作需要大量编程和调参，而现有的基于学习的方法通常需要大量机器人遥操作数据或多次演示，收集成本高。此外，现有方法在物体被手部遮挡时表现不佳，且大多不支持移动操作，限制了机器人在日常环境中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于人类演示的机器人操作方法，发现模仿学习方法需要多次演示，强化学习方法需要构建任务空间的数字孪生，而训练免费方法在物体遮挡或噪声处理方面存在局限。作者借鉴了多个现有工作：使用AR头显收集数据类似iTeach框架；使用GroundingDINO和SAMv2进行物体检测；使用HaMeR进行手部姿态估计；使用统一夹持器坐标系空间(UGCS)进行抓取转移；使用BundleSDF进行物体姿态估计。基于这些分析，作者设计了HRT1系统，专注于手部轨迹转移而非物体轨迹，并加入了轨迹优化算法以提高鲁棒性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过分析人类演示视频中的手部动作，将其转换为机器人的执行轨迹，使机器人能够一次性学习并执行相同的操作任务，即使在不同环境和物体摆放情况下也能成功。整体流程分为四个模块：1)数据收集模块：使用HoloLens 2从机器人视角收集人类演示视频；2)视频理解模块：检测物体并提取3D人类手部轨迹；3)人类到机器人抓取转移模块：使用UGCS表示将人类手部轨迹转换为机器人夹持器轨迹；4)任务执行的轨迹对齐模块：使用BundleSDF估计物体姿态变换，并通过两阶段轨迹优化算法(先优化机器人基座位置，再优化关节空间轨迹)使机器人精确执行任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于手部轨迹而非物体轨迹的转移，在物体被遮挡时更可靠；2)使用统一夹持器坐标系空间(UGCS)进行抓取转移，支持不同类型夹持器；3)两阶段轨迹优化算法，能处理转移轨迹中的噪声；4)支持移动操作，是首个支持移动的训练免费方法；5)改进的3D手部姿态估计，提高深度准确性。相比之前工作，HRT1不依赖物体姿态估计，使用轨迹优化而非简单逆运动学，支持移动操作，且只需一次演示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HRT1通过分析人类演示视频中的手部动作并转换为机器人执行轨迹，实现了机器人仅需观看一次演示就能在不同环境中成功执行移动操作任务的能力，显著提高了轨迹转移的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel system for human-to-robot trajectory transfer thatenables robots to manipulate objects by learning from human demonstrationvideos. The system consists of four modules. The first module is a datacollection module that is designed to collect human demonstration videos fromthe point of view of a robot using an AR headset. The second module is a videounderstanding module that detects objects and extracts 3D human-handtrajectories from demonstration videos. The third module transfers a human-handtrajectory into a reference trajectory of a robot end-effector in 3D space. Thelast module utilizes a trajectory optimization algorithm to solve a trajectoryin the robot configuration space that can follow the end-effector trajectorytransferred from the human demonstration. Consequently, these modules enable arobot to watch a human demonstration video once and then repeat the same mobilemanipulation task in different environments, even when objects are placeddifferently from the demonstrations. Experiments of different manipulationtasks are conducted on a mobile manipulator to verify the effectiveness of oursystem</description>
      <author>example@mail.com (Sai Haneesh Allu, Jishnu Jaykumar P, Ninad Khargonkar, Tyler Summers, Jian Yao, Yu Xiang)</author>
      <guid isPermaLink="false">2510.21026v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>LLM-Integrated Bayesian State Space Models for Multimodal Time-Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.20952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了LLM集成的贝叶斯状态空间模型(LBS)，一种用于多模态时间预测的新概率框架，解决了现有方法在架构上的限制，实现了灵活的时间窗口、不确定性量化和改进的时间泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现实世界预测需要整合结构化时间序列数据与非结构化文本信息，但现有方法受固定输入/输出时间跨度限制，无法建模或量化不确定性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时处理数值和文本信息的时间预测方法，提供灵活的时间窗口、不确定性量化和更好的时间泛化能力。&lt;h4&gt;方法&lt;/h4&gt;LBS包含两个主要组件：(1)状态空间模型(SSM)骨干，捕获生成数值和文本观测的潜在状态的时间动态；(2)预训练大型语言模型(LLM)，用于编码文本输入进行后验状态估计和解码文本预测。&lt;h4&gt;主要发现&lt;/h4&gt;LBS在TextTimeCorpus基准测试上比之前的最先进方法提高13.20%，同时为每个预测提供可读的人类摘要，实现了灵活的回看和预测窗口、原则性不确定性量化和改进的时间泛化。&lt;h4&gt;结论&lt;/h4&gt;该研究首次统一LLM和SSM进行数值和文本联合预测，为多模态时间推理提供了新的基础框架。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的预测需要整合结构化的时间序列数据和非结构化的文本信息，但现有方法在架构上受到固定输入/输出时间跨度的限制，无法建模或量化不确定性。我们通过引入LLM集成的贝叶斯状态空间模型(LBS)来解决这一挑战，这是一种用于多模态时间预测的新概率框架。总体而言，LBS包含两个组件：(1)状态空间模型(SSM)骨干，捕获生成数值和文本观测的潜在状态的时间动态；(2)预训练的大型语言模型(LLM)，调整后用于编码文本输入进行后验状态估计和解码与潜在轨迹一致的文本预测。这种设计能够提供灵活的回看和预测窗口，原则性的不确定性量化，并改善时间泛化能力。在TextTimeCorpus基准测试上的实验表明，LBS比之前的最先进方法提高13.20%，同时为每个预测提供可读的人类摘要。我们的工作是首次统一LLM和SSM进行数值和文本联合预测，为多模态时间推理提供了新的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forecasting in the real world requires integrating structured time-seriesdata with unstructured textual information, but existing methods arearchitecturally limited by fixed input/output horizons and are unable to modelor quantify uncertainty. We address this challenge by introducingLLM-integrated Bayesian State space models (LBS), a novel probabilisticframework for multimodal temporal forecasting. At a high level, LBS consists oftwo components: (1) a state space model (SSM) backbone that captures thetemporal dynamics of latent states from which both numerical and textualobservations are generated and (2) a pretrained large language model (LLM) thatis adapted to encode textual inputs for posterior state estimation and decodetextual forecasts consistent with the latent trajectory. This design enablesflexible lookback and forecast windows, principled uncertainty quantification,and improved temporal generalization thanks to the well-suited inductive biasof SSMs toward modeling dynamical systems. Experiments on the TextTimeCorpusbenchmark demonstrate that LBS improves the previous state-of-the-art by 13.20%while providing human-readable summaries of each forecast. Our work is thefirst to unify LLMs and SSMs for joint numerical and textual prediction,offering a novel foundation for multimodal temporal reasoning.</description>
      <author>example@mail.com (Sungjun Cho, Changho Shin, Suenggwan Jo, Xinya Yan, Shourjo Aditya Chaudhuri, Frederic Sala)</author>
      <guid isPermaLink="false">2510.20952v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2510.20622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SeViCES框架，通过语义-视觉共识证据选择方法解决长视频理解中的挑战，在准确性和鲁棒性方面超越了现有方法。&lt;h4&gt;背景&lt;/h4&gt;长视频理解因其复杂、多样且时间分散的内容而具有挑战性。现有的Video-LLMs可处理数十分钟的视频，但应用于真正长序列时计算成本高，且推理往往不聚焦或不一致。&lt;h4&gt;目的&lt;/h4&gt;开发一个有效可靠的长视频理解框架，解决现有方法中忽略时间依赖性和依赖单模态证据的局限性。&lt;h4&gt;方法&lt;/h4&gt;SeViCES是一个无需训练且与模型无关的框架，包含两个关键组件：(1)语义-视觉共识帧选择(SVCFS)模块，通过时间感知语义分支和聚类引导视觉分支选择信息量最大的帧；(2)答案共识精炼(ACR)模块，通过融合证据和约束答案空间解决语义和视觉预测间的不一致。&lt;h4&gt;主要发现&lt;/h4&gt;在长视频理解基准上的大量实验表明，SeViCES在准确性和鲁棒性方面均持续优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;共识驱动的证据选择对Video-LLMs的长视频理解能力至关重要。&lt;h4&gt;翻译&lt;/h4&gt;长视频理解由于其复杂、多样且时间分散的内容而仍然具有挑战性。尽管视频大语言模型(Video-LLMs)可以处理长达数十分钟的视频，但将它们应用于真正长序列在计算上是禁止的，并且往往导致不聚焦或不一致的推理。一个有希望的解决方案是只选择信息量最大的帧，然而现有方法通常忽略时间依赖性或依赖单模态证据，限制了它们提供完整且与查询相关上下文的能力。我们提出了一个用于有效可靠长视频理解的语义-视觉共识证据选择(SeViCES)框架。SeViCES无需训练且与模型无关，并引入了两个关键组件。语义-视觉共识帧选择(SVCFS)模块通过(1)利用LLM对字幕进行推理的时间感知语义分支，和(2)通过互信息将嵌入与语义分数对齐的聚类引导视觉分支来选择帧。答案共识精炼(ACR)模块通过融合证据和约束答案空间，进一步解决基于语义和视觉的预测之间的一致性问题。在长视频理解基准上的大量实验表明，SeViCES在准确性和鲁棒性方面均持续优于最先进的方法，证明了共识驱动的证据选择对Video-LLMs的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long video understanding remains challenging due to its complex, diverse, andtemporally scattered content. Although video large language models (Video-LLMs)can process videos lasting tens of minutes, applying them to truly longsequences is computationally prohibitive and often leads to unfocused orinconsistent reasoning. A promising solution is to select only the mostinformative frames, yet existing approaches typically ignore temporaldependencies or rely on unimodal evidence, limiting their ability to providecomplete and query-relevant context. We propose a Semantic-Visual ConsensusEvidence Selection (SeViCES) framework for effective and reliable long videounderstanding. SeViCES is training-free and model-agnostic, and introduces twokey components. The Semantic-Visual Consensus Frame Selection (SVCFS) moduleselects frames through (1) a temporal-aware semantic branch that leverages LLMreasoning over captions, and (2) a cluster-guided visual branch that alignsembeddings with semantic scores via mutual information. The Answer ConsensusRefinement (ACR) module further resolves inconsistencies between semantic- andvisual-based predictions by fusing evidence and constraining the answer space.Extensive experiments on long video understanding benchmarks show that SeViCESconsistently outperforms state-of-the-art methods in both accuracy androbustness, demonstrating the importance of consensus-driven evidence selectionfor Video-LLMs.</description>
      <author>example@mail.com (Yuan Sheng, Yanbin Hao, Chenxu Li, Shuo Wang, Xiangnan He)</author>
      <guid isPermaLink="false">2510.20622v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</title>
      <link>http://arxiv.org/abs/2510.20579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Open-o3 Video是一个非智能体框架，将显式时空证据整合到视频推理中，通过专门的数据集和训练策略实现了在多个视频理解基准测试上的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;大多数视频推理模型只生成文本推理轨迹而不指示关键证据出现的时间和位置。将图像证据中心推理扩展到视频更具挑战性，因为它需要在动态场景中联合时空跟踪和定位。&lt;h4&gt;目的&lt;/h4&gt;引入Open-o3 Video框架，解决视频推理中时空证据整合的挑战，通过收集训练数据和设计训练策略来提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;创建了两个高质量数据集STGR-CoT-30k用于SFT和STGR-RL-36k用于RL，包含精心构建的时空注释；采用冷启动强化学习策略，使用多种专门设计的奖励来鼓励答案准确性、时间对齐和空间精度。&lt;h4&gt;主要发现&lt;/h4&gt;在V-STAR基准测试上，Open-o3 Video实现了最先进性能，相比Qwen2.5-VL基线，mAM提高14.4%，mLGM提高24.2%；在VideoMME、WorldSense、VideoMMMU和TVGBench等多个视频理解基准测试上观察到一致改进。&lt;h4&gt;结论&lt;/h4&gt;Open-o3 Video的推理轨迹为测试时扩展提供了有价值的信号，支持置信感知的验证，提高答案可靠性。&lt;h4&gt;翻译&lt;/h4&gt;大多数视频推理模型只生成文本推理轨迹而不指示关键证据出现的时间和位置。最近的模型如OpenAI-o3在图像证据中心推理方面引起了广泛兴趣，但将这种能力扩展到视频更具挑战性，因为它需要在动态场景中联合时空跟踪和定位。我们引入了Open-o3 Video，一个将显式时空证据整合到视频推理中的非智能体框架，并仔细收集训练数据和设计训练策略来解决上述挑战。模型在答案旁边突出显示关键时间戳、对象和边界框，使推理能够基于具体的视觉观察。为实现这一功能，我们首先策划并构建了两个高质量数据集：用于SFT的STGR-CoT-30k和用于RL的STGR-RL-36k，包含精心构建的时间和空间注释，因为大多数现有数据集只提供视频的时间跨度或图像的空间框，缺乏统一的时空监督和推理轨迹。然后，我们采用冷启动强化学习策略，使用多种专门设计的奖励，共同鼓励答案准确性、时间对齐和空间精度。在V-STAR基准测试上，Open-o3 Video实现了最先进的性能，相比Qwen2.5-VL基线，mAM提高了14.4%，mLGM提高了24.2%。在广泛的视频理解基准测试上，包括VideoMME、WorldSense、VideoMMMU和TVGBench，也观察到一致改进。除了准确性，Open-o3 Video产生的推理轨迹还为测试时扩展提供了有价值的信号，使置信感知的验证成为可能，并提高答案可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most video reasoning models only generate textual reasoning traces withoutindicating when and where key evidence appears. Recent models such as OpenAI-o3have sparked wide interest in evidence-centered reasoning for images, yetextending this ability to videos is more challenging, as it requires jointtemporal tracking and spatial localization across dynamic scenes. We introduceOpen-o3 Video, a non-agent framework that integrates explicit spatio-temporalevidence into video reasoning, and carefully collect training data and designtraining strategies to address the aforementioned challenges. The modelhighlights key timestamps, objects, and bounding boxes alongside its answers,allowing reasoning to be grounded in concrete visual observations. To enablethis functionality, we first curate and build two high-quality datasets,STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructedtemporal and spatial annotations, since most existing datasets offer eithertemporal spans for videos or spatial boxes on images, lacking unifiedspatio-temporal supervision and reasoning traces. Then, we adopt a cold-startreinforcement learning strategy with multiple specially designed rewards thatjointly encourage answer accuracy, temporal alignment, and spatial precision.On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistentimprovements are also observed on a broad range of video understandingbenchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyondaccuracy, the reasoning traces produced by Open-o3 Video also provide valuablesignals for test-time scaling, enabling confidence-aware verification andimproving answer reliability.</description>
      <author>example@mail.com (Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, Zhuochen Wang)</author>
      <guid isPermaLink="false">2510.20579v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence</title>
      <link>http://arxiv.org/abs/2510.20470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Conan，一个基于证据的多步视频推理框架，通过识别上下文和证据帧、跨帧线索推理以及自适应决策机制，解决了现有视频推理方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;视频推理需要跨帧多步推理，对多模态大语言模型(MLLMs)仍是主要挑战。基于强化学习的方法依赖文本链导致结论缺乏基础，帧检索方法则难以准确进行证据定位。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效进行多步视频推理的框架，解决现有方法在推理准确性和证据定位方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;构建Conan-91K数据集，包含自动生成的推理轨迹；设计多阶段渐进式冷启动策略和识别-推理-行动(AIR)RLVR训练框架，共同增强多步视觉推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;在六个多步推理基准测试上，Conan的准确性平均超过基线Qwen2.5-VL-7B-Instruct模型10%以上，达到最先进性能；且能有效泛化到长视频理解任务，展示出良好的可扩展性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;Conan框架通过结合证据基础和多步推理，有效解决了视频推理中的挑战，实现了更准确可靠的视频理解。&lt;h4&gt;翻译&lt;/h4&gt;视频推理需要跨帧多步推理，这对多模态大语言模型(MLLMs)仍然是一个主要挑战。虽然基于强化学习(RL)的方法增强了推理能力，但它们通常只依赖文本链，导致结论缺乏基础或产生幻觉。相反，帧检索方法引入了视觉基础，但仍然难以准确进行证据定位。为了解决这些挑战，我们提出了Conan，一个用于基于证据的多步视频推理框架。Conan能够识别上下文和证据帧，跨帧线索进行推理，并自适应地决定何时得出结论或进一步探索。为此，我们(1)构建了Conan-91K，这是一个大规模的自动生成推理轨迹数据集，包括帧识别、证据推理和行动决策，以及(2)设计了一个多阶段渐进式冷启动策略，结合识别-推理-行动(AIR)RLVR训练框架，共同增强多步视觉推理。在六个多步推理基准测试上的广泛实验表明，Conan在准确性上平均超过基线Qwen2.5-VL-7B-Instruct模型10%以上，达到了最先进的性能。此外，Conan能有效地泛化到长视频理解任务，验证了其强大的可扩展性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video reasoning, which requires multi-step deduction across frames, remains amajor challenge for multimodal large language models (MLLMs). Whilereinforcement learning (RL)-based methods enhance reasoning capabilities, theyoften rely on text-only chains that yield ungrounded or hallucinatedconclusions. Conversely, frame-retrieval approaches introduce visual groundingbut still struggle with inaccurate evidence localization. To address thesechallenges, we present Conan, a framework for evidence-grounded multi-stepvideo reasoning. Conan identifies contextual and evidence frames, reasons overcross-frame clues, and adaptively decides when to conclude or explore further.To achieve this, we (1) construct Conan-91K, a large-scale dataset ofautomatically generated reasoning traces that includes frame identification,evidence reasoning, and action decision, and (2) design a multi-stageprogressive cold-start strategy combined with anIdentification-Reasoning-Action (AIR) RLVR training framework to jointlyenhance multi-step visual reasoning. Extensive experiments on six multi-stepreasoning benchmarks demonstrate that Conan surpasses the baselineQwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achievingstate-of-the-art performance. Furthermore, Conan generalizes effectively tolong-video understanding tasks, validating its strong scalability androbustness.</description>
      <author>example@mail.com (Kun Ouyang, Yuanxin Liu, Linli Yao, Yishuo Cai, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun)</author>
      <guid isPermaLink="false">2510.20470v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling</title>
      <link>http://arxiv.org/abs/2510.20302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为InvDec的混合架构，用于多元时间序列预测，有效结合了时间建模和跨变量依赖关系建模，特别在高维数据集上表现优异。&lt;h4&gt;背景&lt;/h4&gt;多元时间序列预测需要同时建模时间模式和跨变量依赖关系。现有方法存在局限性：通道独立方法如PatchTST擅长时间建模但忽略变量相关性，而纯变量注意力方法如iTransformer牺牲了时间编码。&lt;h4&gt;目的&lt;/h4&gt;提出一种混合架构，实现时间编码和变量级解码的原则性分离，以同时捕捉时间模式和跨变量依赖关系。&lt;h4&gt;方法&lt;/h4&gt;提出InvDec架构，结合基于补丁的时间编码器和通过变量级自注意力操作的倒置解码器；引入延迟变量嵌入，在时间编码后才丰富变量特定表示；采用自适应残差融合机制动态平衡时间信息和变量信息；将InvDec与PatchTST结合形成InvDec-PatchTST。&lt;h4&gt;主要发现&lt;/h4&gt;在七个基准测试上，InvDec-PatchTST在高维数据集上表现显著：Electricity数据集（321个变量）MSE降低20.9%，Weather数据集提升4.3%，Traffic数据集提升2.7%；在低维ETT数据集上保持竞争力；消融研究验证了各组件有效性；InvDec的优势随数据集维度增长而增长。&lt;h4&gt;结论&lt;/h4&gt;InvDec有效地结合了时间建模和跨变量相关性建模，特别适合高维数据集，随着变量数量增加，跨变量建模变得至关重要。&lt;h4&gt;翻译&lt;/h4&gt;多元时间序列预测需要同时建模时间模式和跨变量依赖关系。通道独立方法如PatchTST擅长时间建模但忽略了变量相关性，而纯变量注意力方法如iTransformer牺牲了时间编码。我们提出了InvDec（倒置解码器），一种混合架构，实现了时间编码和变量级解码的原则性分离。InvDec结合了基于补丁的时间编码器和一个通过变量级自注意力操作在变量维度上运行的倒置解码器。我们引入了延迟变量嵌入，仅在时间编码后丰富变量特定表示，保持时间特征完整性。自适应残差融合机制动态平衡不同维度数据集的时间信息和变量信息。将InvDec与PatchTST实例化得到InvDec-PatchTST。在七个基准测试上的广泛实验表明，在高维数据集上取得了显著提升：Electricity（321个变量）上MSE降低20.9%，Weather上提升4.3%，Traffic上提升2.7%，同时在低维ETT数据集上保持竞争力。消融研究验证了每个组件，分析显示InvDec的优势随数据集维度增长而增长，证实了随着变量数量增加，跨变量建模变得关键。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series forecasting requires simultaneously modelingtemporal patterns and cross-variate dependencies. Channel-independent methodssuch as PatchTST excel at temporal modeling but ignore variable correlations,while pure variate-attention approaches such as iTransformer sacrifice temporalencoding. We proposeInvDec (Inverted Decoder), a hybrid architecture thatachieves principled separation between temporal encoding and variate-leveldecoding. InvDec combines a patch-based temporal encoder with an inverteddecoder operating on the variate dimension through variate-wise self-attention.We introduce delayed variate embeddings that enrich variable-specificrepresentations only after temporal encoding, preserving temporal featureintegrity. An adaptive residual fusion mechanism dynamically balances temporaland variate information across datasets of varying dimensions. InstantiatingInvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on sevenbenchmarks demonstrate significant gains on high-dimensional datasets: 20.9%MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and2.7% gain on Traffic compared to PatchTST, while maintaining competitiveperformance on low-dimensional ETT datasets. Ablation studies validate eachcomponent, and analysis reveals that InvDec's advantage grows with datasetdimensionality, confirming that cross-variate modeling becomes critical as thenumber of variables increases.</description>
      <author>example@mail.com (Yuhang Wang)</author>
      <guid isPermaLink="false">2510.20302v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering</title>
      <link>http://arxiv.org/abs/2510.20285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种双模态反事实对比构建（DMC³）框架，用于解决以第一人称视角视频为基础的问题回答任务中的独特挑战，如理解多个事件和识别手部物体交互。&lt;h4&gt;背景&lt;/h4&gt;以第一人称视角视频为基础的问题回答（Egocentric VideoQA）在以第一人称视频理解中扮演着重要角色。现有的预训练和微调方法忽略了第一人称视角带来的独特挑战，如理解多个事件和识别手部物体交互。&lt;h4&gt;目的&lt;/h4&gt;为了解决第一人称视角视频理解中的独特挑战，特别是理解多个事件和识别手部物体交互，作者提出了一种新的框架DMC³。&lt;h4&gt;方法&lt;/h4&gt;DMC³框架包含三个主要部分：开发一个反事实样本构建模块，通过事件描述重述和核心交互挖掘分别为文本和视觉模态生成正负样本；将这些样本与原始样本一起输入基线模型；在反事实样本参与的对比优化模块中应用对比损失，最小化原始样本特征与正样本特征之间的距离，同时最大化与负样本的距离。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在EgoTaskQA数据集的normal和indirect分割上分别达到了52.51%和46.04%的性能，在QAEGO4D上达到了13.2%的性能，均达到了最先进的水平。&lt;h4&gt;结论&lt;/h4&gt;通过提出DMC³框架，有效解决了第一人称视角视频理解中的独特挑战，特别是在理解多个事件和识别手部物体交互方面取得了显著进展，并在多个基准测试中达到了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;以第一人称视频问答（Egocentric VideoQA）在以第一人称视频理解中发挥着重要作用，它指的是基于第一人称视频回答问题。尽管现有方法通过预训练和微调的范式已经取得了进展，但它们忽略了第一人称视角带来的独特挑战，如理解多个事件和识别手部物体交互。为了应对这些挑战，我们提出了一个双模态反事实对比构建（DMC³）框架，该框架包含一个以第一人称视频问答的基线模型、一个反事实样本构建模块和一个反事实样本参与的对比优化。具体来说，我们首先开发了一个反事实样本构建模块，通过事件描述重述和核心交互挖掘分别为文本和视觉模态生成正负样本。然后，我们将这些样本与原始样本一起输入基线模型。最后，在反事实样本参与的对比优化模块中，我们应用对比损失来最小化原始样本特征与正样本特征之间的距离，同时最大化与负样本的距离。实验表明，我们的方法在EgoTaskQA的normal和indirect分割上分别达到了52.51%和46.04%，在QAEGO4D上达到了13.2%，均达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3755085&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Egocentric Video Question Answering (Egocentric VideoQA) plays an importantrole in egocentric video understanding, which refers to answering questionsbased on first-person videos. Although existing methods have made progressthrough the paradigm of pre-training and fine-tuning, they ignore the uniquechallenges posed by the first-person perspective, such as understandingmultiple events and recognizing hand-object interactions. To deal with thesechallenges, we propose a Dual-Modal Counterfactual Contrastive Construction(DMC$^3$) framework, which contains an egocentric videoqa baseline, acounterfactual sample construction module and a counterfactual sample-involvedcontrastive optimization. Specifically, We first develop a counterfactualsample construction module to generate positive and negative samples fortextual and visual modalities through event description paraphrasing and coreinteraction mining, respectively. Then, We feed these samples together with theoriginal samples into the baseline. Finally, in the counterfactualsample-involved contrastive optimization module, we apply contrastive loss tominimize the distance between the original sample features and the positivesample features, while maximizing the distance from the negative samples.Experiments show that our method achieve 52.51\% and 46.04\% on the\textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2\% onQAEGO4D, both reaching the state-of-the-art performance.</description>
      <author>example@mail.com (Jiayi Zou, Chaofan Chen, Bing-Kun Bao, Changsheng Xu)</author>
      <guid isPermaLink="false">2510.20285v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching</title>
      <link>http://arxiv.org/abs/2510.20178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为PPMStereo的新方法，通过引入内存缓冲区和两阶段决策过程（选择和播放），实现了从立体视频中估计时间上一致的深度信息，在保持计算效率的同时提高了时空一致性。&lt;h4&gt;背景&lt;/h4&gt;从立体视频中估计时间上一致的深度信息对增强现实等实际应用至关重要，因为深度估计的不一致会破坏用户沉浸感。然而，这项任务具有挑战性，因为很难以计算高效的方式建模长期的时间一致性。之前的方法在时空建模的广度和计算效率之间存在权衡。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决立体视频中时间一致深度估计的挑战，特别是如何在保持计算效率的同时建模长程时空一致性，开发一种能够实现高效信息聚合且保持深度估计时间一致性的方法。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种名为PPMStereo的方法，引入了内存缓冲区用于建模长程时空一致性。受人类两阶段决策过程的启发，PPMStereo包含一个'选择'过程（识别最相关的帧）和一个'播放'过程（为时空聚合自适应地加权所选帧），形成一种两阶段协作过程。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验验证了PPMStereo的有效性，表明其在准确性和时间一致性方面达到了最先进的性能。在Sintel clean/final数据集上，PPMStereo实现了0.62/1.11 TEPE的性能，比BiDAStereo分别提高了17.3%和9.02%，且计算成本更低。&lt;h4&gt;结论&lt;/h4&gt;PPMStereo通过创新的内存缓冲区和两阶段决策过程，成功解决了立体视频中时间一致深度估计的挑战，在保持计算效率的同时提高了时空一致性，为增强现实等实际应用提供了更可靠的深度估计技术。&lt;h4&gt;翻译&lt;/h4&gt;从立体视频中估计时间上一致的深度信息对于增强现实等实际应用至关重要，因为不一致的深度估计会破坏用户的沉浸感。尽管如此，由于难以以计算高效的方式建模长期时间一致性，这项任务仍然具有挑战性。先前的方法试图通过聚合时空信息来解决这一问题，但面临一个基本的权衡：有限的时空建模只能带来适度的提升，而捕捉长程依赖关系则会显著增加计算成本。为了解决这一限制，我们引入了一个内存缓冲区，用于建模长程时空一致性，同时实现高效的动态立体匹配。受人类两阶段决策过程的启发，我们提出了一种用于动态立体匹配的选择并播放记忆(PPM)构建模块，称为PPMStereo。PPM包括一个'选择'过程，用于识别最相关的帧，以及一个'播放'过程，用于为时空聚合自适应地加权所选帧。这种两阶段协作过程保持了一个紧凑但信息丰富的内存缓冲区，同时实现了时间上一致的信息聚合。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决动态场景中的立体匹配问题，即在视频序列中保持深度估计的时间一致性，避免出现闪烁和模糊等不一致现象。这个问题在现实中非常重要，因为像增强现实、自动驾驶和机器人等应用需要时间一致的深度估计来提供稳定可靠的用户体验，而不一致的深度估计会严重影响用户体验和系统性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：一些方法使用小时间窗口限制了信息传播，而扩大窗口则计算成本过高且不考虑帧可靠性差异。作者借鉴了人类决策的两阶段过程（'选择'和'播放'），并参考了视频任务中的记忆模型（如XMem和RMem），但针对立体匹配任务进行了专门改进。作者设计了一个质量评估模块来评估帧的价值，并采用动态选择机制来构建高效的记忆缓冲区。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入记忆缓冲区来建模长期时空一致性，同时保持计算效率。方法包含两个关键过程：1）'选择'过程：使用质量评估模块识别最相关的K帧，评估标准包括置信度、冗余性和相似性；2）'播放'过程：通过动态记忆调制机制自适应加权选定帧的特征，并使用注意力机制读取记忆缓冲区。整体流程包括特征提取、成本体积构建、上下文编码、记忆缓冲区更新和迭代细化等步骤，通过GRU模块逐步优化视差估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）首次将记忆缓冲区引入动态立体匹配任务，实现高效长期建模；2）提出'选择和播放'记忆构建方法，动态选择并加权关键帧；3）引入质量评估模块联合评估帧的置信度、冗余性和相似性；4）设计动态记忆调制机制自适应调整特征权重。相比之前工作，PPMStereo不再使用固定窗口或平等对待所有帧，而是根据帧质量和相关性动态选择和加权，在保持计算效率的同时显著提高了时间一致性和准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PPMStereo通过创新的'选择和播放'记忆构建机制，实现了在计算高效的同时保持时间一致性的动态立体匹配，显著提高了深度估计的准确性和一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporally consistent depth estimation from stereo video is critical forreal-world applications such as augmented reality, where inconsistent depthestimation disrupts the immersion of users. Despite its importance, this taskremains challenging due to the difficulty in modeling long-term temporalconsistency in a computationally efficient manner. Previous methods attempt toaddress this by aggregating spatio-temporal information but face a fundamentaltrade-off: limited temporal modeling provides only modest gains, whereascapturing long-range dependencies significantly increases computational cost.To address this limitation, we introduce a memory buffer for modelinglong-range spatio-temporal consistency while achieving efficient dynamic stereomatching. Inspired by the two-stage decision-making process in humans, wepropose a \textbf{P}ick-and-\textbf{P}lay \textbf{M}emory (PPM) constructionmodule for dynamic \textbf{Stereo} matching, dubbed as \textbf{PPMStereo}. PPMconsists of a `pick' process that identifies the most relevant frames and a`play' process that weights the selected frames adaptively for spatio-temporalaggregation. This two-stage collaborative process maintains a compact yethighly informative memory buffer while achieving temporally consistentinformation aggregation. Extensive experiments validate the effectiveness ofPPMStereo, demonstrating state-of-the-art performance in both accuracy andtemporal consistency. % Notably, PPMStereo achieves 0.62/1.11 TEPE on theSintel clean/final (17.3\% \&amp; 9.02\% improvements over BiDAStereo) with fewercomputational costs. Codes are available at\textcolor{blue}{https://github.com/cocowy1/PPMStereo}.</description>
      <author>example@mail.com (Yun Wang, Junjie Hu, Qiaole Dong, Yongjian Zhang, Yanwei Fu, Tin Lun Lam, Dapeng Wu)</author>
      <guid isPermaLink="false">2510.20178v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency</title>
      <link>http://arxiv.org/abs/2510.19980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 4 figures. Accepted as Spotlight poster in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对时间序列预测中的冗余特征学习问题，提出了一种名为AMRC的创新解决方案，通过动态掩码损失和表示一致性约束提高了预测性能，挑战了传统的时间序列建模假设。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测在能源管理和金融市场等关键领域发挥着重要作用。尽管基于深度学习的方法（如MLP、RNN、Transformer）已取得显著进展，但现有的'长序列信息增益假设'存在固有局限性。&lt;h4&gt;目的&lt;/h4&gt;研究旨在解决现有模型在训练过程中学习大量冗余特征（如噪声或不相关波动）的问题，从而影响有效信号提取，提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为'具有表示一致性的自适应掩码损失'（AMRC）的创新解决方案，包含两个核心组件：1) 动态掩码损失，自适应识别高判别性时间段以指导梯度下降；2) 表示一致性约束，稳定输入、标签和预测之间的映射关系。&lt;h4&gt;主要发现&lt;/h4&gt;通过系统实验发现了一个反直觉现象：适当截断历史数据可以 paradoxically 提高预测准确性，表明现有模型在训练过程中学习了大量冗余特征，损害了有效信号的提取。&lt;h4&gt;结论&lt;/h4&gt;AMRC能有效抑制冗余特征学习，同时显著提高模型性能。这项工作不仅挑战了时间建模中的传统假设，还为开发高效和稳健的预测模型提供了新的理论见解和方法突破。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测在能源管理和金融市场等关键领域发挥着关键作用。尽管基于深度学习的方法（如MLP、RNN、Transformer）已取得显著进展，但现有的'长序列信息增益假设'存在固有局限性。通过系统实验，本研究揭示了一个反直觉现象：适当截断历史数据可以 paradoxically 提高预测准确性，表明现有模型在训练过程中学习了大量冗余特征（如噪声或不相关波动），从而损害了有效信号的提取。基于信息瓶颈理论，我们提出了一种名为'具有表示一致性的自适应掩码损失'（AMRC）的创新解决方案，包含两个核心组件：1) 动态掩码损失，在模型训练过程中自适应识别高判别性时间段以指导梯度下降；2) 表示一致性约束，稳定输入、标签和预测之间的映射关系。实验结果表明，AMRC能有效抑制冗余特征学习，同时显著提高模型性能。这项工作不仅挑战了时间建模中的传统假设，还为开发高效和稳健的预测模型提供了新的理论见解和方法突破。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting plays a pivotal role in critical domains such asenergy management and financial markets. Although deep learning-basedapproaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, theprevailing "long-sequence information gain hypothesis" exhibits inherentlimitations. Through systematic experimentation, this study reveals acounterintuitive phenomenon: appropriately truncating historical data canparadoxically enhance prediction accuracy, indicating that existing modelslearn substantial redundant features (e.g., noise or irrelevant fluctuations)during training, thereby compromising effective signal extraction. Buildingupon information bottleneck theory, we propose an innovative solution termedAdaptive Masking Loss with Representation Consistency (AMRC), which featurestwo core components: 1) Dynamic masking loss, which adaptively identifiedhighly discriminative temporal segments to guide gradient descent during modeltraining; 2) Representation consistency constraint, which stabilized themapping relationships among inputs, labels, and predictions. Experimentalresults demonstrate that AMRC effectively suppresses redundant feature learningwhile significantly improving model performance. This work not only challengesconventional assumptions in temporal modeling but also provides noveltheoretical insights and methodological breakthroughs for developing efficientand robust forecasting models.</description>
      <author>example@mail.com (Renzhao Liang, Sizhe Xu, Chenggang Xie, Jingru Chen, Feiyang Ren, Shu Yang, Takahiro Yabe)</author>
      <guid isPermaLink="false">2510.19980v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks</title>
      <link>http://arxiv.org/abs/2510.20683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Spikachu是一种基于脉冲神经网络的脑机接口解码框架，具有可扩展性、因果性和高能效性，解决了现有方法在实时应用和能源效率方面的限制。&lt;h4&gt;背景&lt;/h4&gt;脑机接口对神经运动障碍人群具有重要意义，但现有解码方法要么简单但缺乏泛化能力，要么复杂但难以实时应用，且都依赖于高能耗的人工神经网络，难以集成到资源有限的现实系统中。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于脉冲神经网络的、可扩展、因果且节能的神经解码框架，以克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;Spikachu直接处理分箱脉冲，将其投影到共享潜在空间，利用适应输入时序的脉冲模块提取特征，然后整合这些潜在表示并解码生成行为预测。研究在6只非人灵长类动物的113个记录会话（总计43小时）上评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;与因果基线相比，Spikachu在使用少2.26到418.81倍能源的情况下表现更好；将训练扩展到多个会话和受试者可提高性能，并实现向未见过的会话、受试者和任务的少样本迁移。&lt;h4&gt;结论&lt;/h4&gt;Spikachu引入了一种基于SNN的可扩展、在线兼容的神经解码框架，其性能与最先进模型相当，同时能耗低几个数量级。&lt;h4&gt;翻译&lt;/h4&gt;脑机接口(BCIs)有望为神经运动障碍个体实现言语和假肢控制等关键功能。其成功的关键是神经解码器，即将神经活动映射到预期行为的模型。当前基于学习的解码方法分为两类：简单但缺乏泛化能力的因果模型，或复杂但难以实时应用的非因果模型。两者都面临一个共同挑战，它们依赖于能耗高的人工神经网络骨干，这使得集成到现实世界的资源有限系统中变得困难。脉冲神经网络(SNNs)提供了一种有前景的替代方案。由于它们以因果方式运行，这些模型适合实时使用，并且它们的低能耗需求使其成为电池受限环境的理想选择。为此，我们引入了Spikachu：一种基于SNN的可扩展、因果且节能的神经解码框架。我们的方法通过将分箱脉冲直接投影到共享潜在空间来处理它们，在该空间中，适应输入时序的脉冲模块提取相关特征；然后将这些潜在表示整合和解码以生成行为预测。我们在6只非人灵长类动物的113个记录会话上评估了我们的方法，总计43小时的记录。与因果基线相比，我们的方法在使用少2.26到418.81倍能源的情况下表现更好。此外，我们证明将训练扩展到多个会话和受试者可以提高性能，并实现向未见过的会话、受试者和任务的少样本迁移。总体而言，Spikachu引入了一种基于SNN的可扩展、在线兼容的神经解码框架，其性能与最先进模型相当，同时能耗低几个数量级。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain-computer interfaces (BCIs) promise to enable vital functions, such asspeech and prosthetic control, for individuals with neuromotor impairments.Central to their success are neural decoders, models that map neural activityto intended behavior. Current learning-based decoding approaches fall into twoclasses: simple, causal models that lack generalization, or complex, non-causalmodels that generalize and scale offline but struggle in real-time settings.Both face a common challenge, their reliance on power-hungry artificial neuralnetwork backbones, which makes integration into real-world, resource-limitedsystems difficult. Spiking neural networks (SNNs) offer a promisingalternative. Because they operate causally these models are suitable forreal-time use, and their low energy demands make them ideal forbattery-constrained environments. To this end, we introduce Spikachu: ascalable, causal, and energy-efficient neural decoding framework based on SNNs.Our approach processes binned spikes directly by projecting them into a sharedlatent space, where spiking modules, adapted to the timing of the input,extract relevant features; these latent representations are then integrated anddecoded to generate behavioral predictions. We evaluate our approach on 113recording sessions from 6 non-human primates, totaling 43 hours of recordings.Our method outperforms causal baselines when trained on single sessions usingbetween 2.26 and 418.81 times less energy. Furthermore, we demonstrate thatscaling up training to multiple sessions and subjects improves performance andenables few-shot transfer to unseen sessions, subjects, and tasks. Overall,Spikachu introduces a scalable, online-compatible neural decoding frameworkbased on SNNs, whose performance is competitive relative to state-of-the-artmodels while consuming orders of magnitude less energy.</description>
      <author>example@mail.com (Georgios Mentzelopoulos, Ioannis Asmanis, Konrad P. Kording, Eva L. Dyer, Kostas Daniilidis, Flavia Vitale)</author>
      <guid isPermaLink="false">2510.20683v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
  <item>
      <title>Monocular Visual 8D Pose Estimation for Articulated Bicycles and Cyclists</title>
      <link>http://arxiv.org/abs/2510.20158v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对关节式自行车和骑行者的类别级8D姿态估计方法，能够从单张RGB图像估计自行车的3D平移、旋转以及转向手柄和踏板相对于车身的旋转，从而提供更细粒度的自行车姿态状态和行驶方向估计。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶中，骑行者属于安全关键类弱势道路使用者(VRU)，准确估计其姿态对过马路意图分类、行为预测和碰撞避免至关重要。与刚性物体不同，关节式自行车由通过关节连接的可移动刚性部件组成，6D姿态方法在自行车转向/踏板角度变化时变得不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够估计自行车完整关节状态的方法，包括3D位置、旋转以及转向手柄和踏板的相对旋转，以提供更准确的骑行者行为预测和碰撞避免能力。&lt;h4&gt;方法&lt;/h4&gt;提出联合估计关节式自行车8D姿态和3D关键点的模型，使用合成和真实图像数据的混合进行训练，以在真实图像上实现良好的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的8D姿态估计方法能够提供更细粒度的自行车姿态状态和行驶方向估计，与使用刚性规范对象模板的最先进6D姿态估计器相比具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;该方法在处理关节式自行车姿态变化方面表现出色，能够更好地理解骑行者的实际行驶意图，为自动驾驶系统提供更可靠的骑行者行为预测能力。&lt;h4&gt;翻译&lt;/h4&gt;在自动驾驶中，骑行者属于安全关键类弱势道路使用者(VRU)，准确估计他们的姿态对于骑行者过马路意图分类、行为预测和碰撞避免至关重要。与刚性物体不同，关节式自行车由通过关节连接的可移动刚性部件组成，并受运动学结构约束。6D姿态方法可以估计刚性自行车的3D旋转和平移，但当自行车的转向/踏板角度变化时，6D方法变得不足。这是因为：1)自行车关节姿态的变化会导致其3D边界框也发生变化，2)3D框的方向不一定与决定实际预期行驶方向的转向方向对齐。在这项工作中，我们介绍了一种针对关节式自行车和骑行者的类别级8D姿态估计方法，可以从单张RGB图像进行估计。除了能够从单张图像估计自行车的3D平移和旋转外，我们的方法还估计其转向手柄和踏板相对于自行车车身的旋转。这两个新参数能够估计更细粒度的自行车姿态状态和行驶方向。我们提出的模型联合估计关节式自行车的8D姿态和3D关键点，并使用合成和真实图像数据的混合进行训练，以在真实图像上泛化。我们包含了一个评估部分，评估了估计的8D姿态参数的准确性，与使用刚性规范对象模板进行匹配的最先进类别级6D姿态估计器相比，我们的方法取得了具有竞争力的分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In Autonomous Driving, cyclists belong to the safety-critical class ofVulnerable Road Users (VRU), and accurate estimation of their pose is criticalfor cyclist crossing intention classification, behavior prediction, andcollision avoidance. Unlike rigid objects, articulated bicycles are composed ofmovable rigid parts linked by joints and constrained by a kinematic structure.6D pose methods can estimate the 3D rotation and translation of rigid bicycles,but 6D becomes insufficient when the steering/pedals angles of the bicyclevary. That is because: 1) varying the articulated pose of the bicycle causesits 3D bounding box to vary as well, and 2) the 3D box orientation is notnecessarily aligned to the orientation of the steering which determines theactual intended travel direction. In this work, we introduce a method forcategory-level 8D pose estimation for articulated bicycles and cyclists from asingle RGB image. Besides being able to estimate the 3D translation androtation of a bicycle from a single image, our method also estimates therotations of its steering handles and pedals with respect to the bicycle bodyframe. These two new parameters enable the estimation of a more fine-grainedbicycle pose state and travel direction. Our proposed model jointly estimatesthe 8D pose and the 3D Keypoints of articulated bicycles, and trains with a mixof synthetic and real image data to generalize on real images. We include anevaluation section where we evaluate the accuracy of our estimated 8D poseparameters, and our method shows promising results by achieving competitivescores when compared against state-of-the-art category-level 6D pose estimatorsthat use rigid canonical object templates for matching.</description>
      <author>example@mail.com (Eduardo R. Corral-Soto, Yang Liu, Yuan Ren, Bai Dongfeng, Liu Bingbing)</author>
      <guid isPermaLink="false">2510.20158v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</title>
      <link>http://arxiv.org/abs/2510.19789v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OmniMotion-X，一个用于全身人体运动生成的多模态框架，采用自回归扩散变换器以统一的序列到序列方式工作。该框架支持多种多模态任务，包括文本到运动、音乐到舞蹈、语音到手势以及全局时空控制场景，并能灵活组合这些任务。&lt;h4&gt;背景&lt;/h4&gt;人体运动生成领域需要能够处理多种模态输入并生成连贯、可控运动的系统。现有方法在处理多模态任务组合和保持生成内容一致性方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的多模态框架，能够处理多种运动生成任务，提高生成内容的一致性、风格和时序动态，并实现长时间的真实、连贯、可控运动生成。&lt;h4&gt;方法&lt;/h4&gt;使用自回归扩散变换器作为核心架构，引入参考运动作为条件信号增强一致性，采用渐进式弱到强混合条件训练策略处理多模态冲突，并构建了OmniMoCap-X数据集，整合28个公开MoCap来源，使用GPT-4o自动生成结构化字幕。&lt;h4&gt;主要发现&lt;/h4&gt;OmniMotion-X在多个多模态任务上显著超越现有方法，实现了最先进的性能，能够生成交互式的真实、连贯、可控的长时间运动。&lt;h4&gt;结论&lt;/h4&gt;OmniMotion-X通过统一的多模态框架和创新的条件训练策略，解决了人体运动生成中的多模态整合问题，为动画制作和人机交互提供了有力工具。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了OmniMotion-X，一个用于全身人体运动生成的多模态框架，利用自回归扩散变换器以统一的序列到序列方式工作。OmniMotion-X有效支持多种多模态任务，包括文本到运动、音乐到舞蹈、语音到手势和全局时空控制场景（如运动预测、中间帧生成、补全和关节/轨迹引导合成），以及这些任务的灵活组合。具体而言，我们提出使用参考运动作为新的条件信号，显著提高了生成内容、风格和时序动态的一致性，这对真实动画至关重要。为处理多模态冲突，我们引入了渐进式弱到强混合条件训练策略。为支持高质量多模态训练，我们构建了OmniMoCap-X，这是迄今为止最大的统一多模态运动数据集，整合了10个不同任务中的28个公开MoCap来源，标准化为30fps的SMPL-X格式。为确保详细且一致的标注，我们将序列渲染为视频并使用GPT-4o自动生成结构化和层次化字幕，捕捉低级行动和高层语义。大量实验评估证实，OmniMotion-X显著超越现有方法，在多个多模态任务上展示了最先进的性能，并能实现真实、连贯、可控的长时间运动的交互式生成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces OmniMotion-X, a versatile multimodal framework forwhole-body human motion generation, leveraging an autoregressive diffusiontransformer in a unified sequence-to-sequence manner. OmniMotion-X efficientlysupports diverse multimodal tasks, including text-to-motion, music-to-dance,speech-to-gesture, and global spatial-temporal control scenarios (e.g., motionprediction, in-betweening, completion, and joint/trajectory-guided synthesis),as well as flexible combinations of these tasks. Specifically, we propose theuse of reference motion as a novel conditioning signal, substantially enhancingthe consistency of generated content, style, and temporal dynamics crucial forrealistic animations. To handle multimodal conflicts, we introduce aprogressive weak-to-strong mixed-condition training strategy. To enablehigh-quality multimodal training, we construct OmniMoCap-X, the largest unifiedmultimodal motion dataset to date, integrating 28 publicly available MoCapsources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps.To ensure detailed and consistent annotations, we render sequences into videosand use GPT-4o to automatically generate structured and hierarchical captions,capturing both low-level actions and high-level semantics. Extensiveexperimental evaluations confirm that OmniMotion-X significantly surpassesexisting methods, demonstrating state-of-the-art performance across multiplemultimodal tasks and enabling the interactive generation of realistic,coherent, and controllable long-duration motions.</description>
      <author>example@mail.com (Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu)</author>
      <guid isPermaLink="false">2510.19789v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>ProTerrain: Probabilistic Physics-Informed Rough Terrain World Modeling</title>
      <link>http://arxiv.org/abs/2510.19364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is submitted to IEEE International Conference on Robotics  and Automation (ICRA) 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种有效的概率框架，用于处理非结构化越野环境中机器人运动预测的不确定性，通过建模地形参数的空间相关偶然不确定性并传播到轨迹预测中，显著提高了预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;不确定性感知的机器人运动预测对于非结构化越野环境中的下游可通行性估计和自主导航至关重要，因为在这种环境中地形是异构的且感知不确定性很高。&lt;h4&gt;目的&lt;/h4&gt;引入一个有效的概率框架，明确地对地形参数的空间相关偶然不确定性建模为概率世界模型，并通过可微分物理引擎传播这种不确定性，实现概率轨迹预测。&lt;h4&gt;方法&lt;/h4&gt;利用结构化卷积算子，提供高分辨率多变量预测，同时保持可管理的计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;在公开数据集上的实验评估显示，与偶然不确定性估计基线相比，该方法的不确定性估计和轨迹预测准确性显著提高。&lt;h4&gt;结论&lt;/h4&gt;通过明确建模和传播空间相关的偶然不确定性，该方法能够提供更可靠的机器人轨迹预测，适用于复杂的越野环境。&lt;h4&gt;翻译&lt;/h4&gt;不确定性感知的机器人运动预测对于非结构化越野环境中的下游可通行性估计和安全自主导航至关重要，在这种环境中地形是异构的且感知不确定性很高。大多数现有方法假设确定性或空间独立的地面不确定性，忽略了3D空间数据的固有局部相关性，并且通常产生不可靠的预测。在这项工作中，我们引入了一个有效的概率框架，明确地将地形参数的空间相关偶然不确定性建模为概率世界模型，并通过可微分物理引擎传播这种不确定性以实现概率轨迹预测。通过利用结构化卷积算子，我们的方法在可管理的计算成本下提供了高分辨率的多变量预测。在公开可用数据集上的实验评估表明，与偶然不确定性估计基线相比，不确定性估计和轨迹预测准确性显著提高。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人越野导航中不确定性感知的运动预测问题。具体来说，现有方法大多假设地形是确定性的或空间独立的不确定性，忽略了3D空间数据的固有局部相关性，导致预测不可靠。这个问题在现实中非常重要，因为越野环境地形异构、感知不确定性高，不确定性感知的运动预测对可通行性评估和安全的自主导航至关重要，而传统方法忽略了关键的物理地形参数、空间相关性和环境不确定性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括确定性方法、空间独立的不确定性估计方法和物理信息方法。他们借鉴了MonoForce的物理信息世界建模框架，但扩展了它以包含空间相关的不确定性。方法设计考虑了计算效率，通过结构化卷积算子避免显式构造高维协方差矩阵，结合深度学习与可微分物理引擎，实现了从感知到轨迹预测的端到端不确定性传播。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个端到端的概率框架，显式建模地形参数上的空间相关偶然不确定性，并通过可微分物理引擎传播这种不确定性用于概率轨迹预测，利用结构化卷积算子以可管理的计算成本提供高分辨率多变量预测。整体流程：1)输入车载摄像头图像；2)使用Lift-Splat-Shoot将特征投影到鸟瞰图网格；3)通过卷积层预测地形参数的平均图和方差图；4)将方差图与固定高斯核卷积形成结构化多元高斯分布；5)从概率世界模型中采样；6)通过可微分物理引擎进行轨迹预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)具有空间相关偶然不确定性的地形端到端概率世界建模；2)基于卷积的结构化协方差估计的可扩展损失公式；3)通过可微分物理引擎进行不确定性感知的轨迹预测；4)在真实世界数据集上的广泛验证。相比之前工作，不同之处在于：与确定性方法相比显式建模了不确定性；与空间独立方法相比考虑了空间相关性；与现有物理信息方法相比在模型中明确包含空间相关不确定性；与高斯过程等方法相比通过结构化卷积提高了计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ProTerrain通过引入基于结构化卷积的高效概率框架，解决了越野导航中地形参数空间相关不确定性建模的挑战，实现了从感知到轨迹预测的端到端不确定性传播，显著提高了机器人在复杂环境中的导航安全性和可靠性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Uncertainty-aware robot motion prediction is crucial for downstreamtraversability estimation and safe autonomous navigation in unstructured,off-road environments, where terrain is heterogeneous and perceptualuncertainty is high. Most existing methods assume deterministic or spatiallyindependent terrain uncertainties, ignoring the inherent local correlations of3D spatial data and often producing unreliable predictions. In this work, weintroduce an efficient probabilistic framework that explicitly models spatiallycorrelated aleatoric uncertainty over terrain parameters as a probabilisticworld model and propagates this uncertainty through a differentiable physicsengine for probabilistic trajectory forecasting. By leveraging structuredconvolutional operators, our approach provides high-resolution multivariatepredictions at manageable computational cost. Experimental evaluation on apublicly available dataset shows significantly improved uncertainty estimationand trajectory prediction accuracy over aleatoric uncertainty estimationbaselines.</description>
      <author>example@mail.com (Golnaz Raja, Ruslan Agishev, Miloš Prágr, Joni Pajarinen, Karel Zimmermann, Arun Kumar Singh, Reza Ghabcheloo)</author>
      <guid isPermaLink="false">2510.19364v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>HumanCM: One Step Human Motion Prediction</title>
      <link>http://arxiv.org/abs/2510.16709v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HumanCM是一种基于一致性模型的人体运动预测框架，能够实现高效的单步生成，无需依赖多步去噪过程。&lt;h4&gt;背景&lt;/h4&gt;现有基于扩散模型的人体运动预测方法通常需要多步去噪过程，计算效率较低。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的单步人体运动预测框架，在减少计算步骤的同时保持或提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;HumanCM学习嘈杂和清洁运动状态之间的自一致映射，采用基于Transformer的时空架构，使用时间嵌入来建模长程依赖并保持运动一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在Human3.6M和HumanEva-I数据集上的实验表明，HumanCM能够达到与最先进的扩散模型相当或更好的准确性，同时将推理步骤减少最多两个数量级。&lt;h4&gt;结论&lt;/h4&gt;HumanCM是一种高效的人体运动预测方法，能够在单步生成中实现高准确性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了HumanCM，一种基于一致性模型构建的单步人体运动预测框架。HumanCM不依赖于扩散模型中的多步去噪，而是通过学习嘈杂和清洁运动状态之间的自一致映射，执行高效的单步生成。该框架采用基于Transformer的时空架构，使用时间嵌入来建模长程依赖并保持运动一致性。在Human3.6M和HumanEva-I上的实验表明，HumanCM能够达到与最先进的扩散模型相当或更好的准确性，同时将推理步骤减少最多两个数量级。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present HumanCM, a one-step human motion prediction framework built uponconsistency models. Instead of relying on multi-step denoising as indiffusion-based methods, HumanCM performs efficient single-step generation bylearning a self-consistent mapping between noisy and clean motion states. Theframework adopts a Transformer-based spatiotemporal architecture with temporalembeddings to model long-range dependencies and preserve motion coherence.Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achievescomparable or superior accuracy to state-of-the-art diffusion models whilereducing inference steps by up to two orders of magnitude.</description>
      <author>example@mail.com (Liu Haojie, Gao Suixiang)</author>
      <guid isPermaLink="false">2510.16709v2</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Bayesian Inference of Primordial Magnetic Field Parameters from CMB with Spherical Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.20795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 6 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于贝叶斯图深度学习的框架，用于从宇宙微波背景图中估算原始磁场宇宙学参数，并结合贝叶斯神经网络实现不确定性量化。&lt;h4&gt;背景&lt;/h4&gt;深度学习已成为现代宇宙学中的变革性方法，为从复杂数据集中提取物理信息提供了强大工具。&lt;h4&gt;目的&lt;/h4&gt;实现一种新颖的贝叶斯图深度学习框架，用于从模拟的宇宙微波背景(CMB)图中直接估算原始磁场(PMF)宇宙学中的关键宇宙学参数。&lt;h4&gt;方法&lt;/h4&gt;使用DeepSphere球形卷积神经网络架构，通过HEALPix像素化尊重CMB数据的球形几何特性，并集成贝叶斯神经网络(BNNs)来捕获偶然性和认知性不确定性，实现稳健的不确定性量化。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法表现出卓越的性能，在磁场参数估计中实现了超过0.89的R²分数，并通过方差缩放和GPNormal后训练技术获得了校准良好的不确定性估计。&lt;h4&gt;结论&lt;/h4&gt;集成的DeepSphere-BNNs框架不仅提供了来自带有PMF贡献的CMB图的准确参数估计，还提供了可靠的不确定性量化，为精确宇宙学时代的稳健宇宙学推理提供了必要工具。&lt;h4&gt;翻译&lt;/h4&gt;深度学习已成为现代宇宙学中的变革性方法，为从复杂数据集中提取有意义的物理信息提供了强大工具。本文实现了一种新颖的贝叶斯图深度学习框架，用于从模拟的宇宙微波背景(CMB)图中直接估算原始磁场(PMF)宇宙学中的关键宇宙学参数。我们的方法利用了DeepSphere，这是一种专门设计用于通过HEALPix像素化尊重CMB数据球形几何形状的球形卷积神经网络架构。为了超越确定性点估计并实现稳健的不确定性量化，我们将贝叶斯神经网络(BNNs)集成到框架中，捕获反映模型对其预测置信度的偶然性和认知性不确定性。所提出的方法表现出卓越的性能，在磁场参数估计中实现了超过0.89的R²分数。我们通过方差缩放和GPNormal等后训练技术获得了校准良好的不确定性估计。这种集成的DeepSphere-BNNs框架不仅提供了来自带有PMF贡献的CMB图的准确参数估计，还提供了可靠的不确定性量化，为精确宇宙学时代的稳健宇宙学推理提供了必要工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has emerged as a transformative methodology in moderncosmology, providing powerful tools to extract meaningful physical informationfrom complex astronomical datasets. This paper implements a novel Bayesiangraph deep learning framework for estimating key cosmological parameters in aprimordial magnetic field (PMF) cosmology directly from simulated CosmicMicrowave Background (CMB) maps. Our methodology utilizes DeepSphere, aspherical convolutional neural network architecture specifically designed torespect the spherical geometry of CMB data through HEALPix pixelization. Toadvance beyond deterministic point estimates and enable robust uncertaintyquantification, we integrate Bayesian Neural Networks (BNNs) into theframework, capturing aleatoric and epistemic uncertainties that reflect themodel confidence in its predictions. The proposed approach demonstratesexceptional performance, achieving $R^{2}$ scores exceeding 0.89 for themagnetic parameter estimation. We further obtain well-calibrated uncertaintyestimates through post-hoc training techniques including Variance Scaling andGPNormal. This integrated DeepSphere-BNNs framework not only delivers accurateparameter estimation from CMB maps with PMF contributions but also providesreliable uncertainty quantification, providing the necessary tools for robustcosmological inference in the era of precision cosmology.</description>
      <author>example@mail.com (Juan Alejandro Pinto Castro, Héctor J. Hortúa, Jorge Enrique García-Farieta, Roger Anderson Hurtado)</author>
      <guid isPermaLink="false">2510.20795v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages</title>
      <link>http://arxiv.org/abs/2510.20739v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了如何使用机器学习技术优先处理程序分析工具报告的漏洞，通过在Node.js包上的实验表明，机器学习模型能有效减少人工审查工作量，同时保持高准确率。&lt;h4&gt;背景&lt;/h4&gt;程序分析工具会产生大量候选漏洞报告，需要昂贵的人工审查，这给安全分析师带来了如何优先处理最可能是真实漏洞的报告的实际挑战。&lt;h4&gt;目的&lt;/h4&gt;研究机器学习是否可以应用于优先处理程序分析工具报告的漏洞，以减轻安全分析师的工作负担。&lt;h4&gt;方法&lt;/h4&gt;收集了1,883个Node.js包的基准测试数据集，每个包包含一个报告的ACE或ACI漏洞；评估了多种机器学习方法，包括经典模型、图神经网络、大型语言模型以及混合模型；所有模型都基于动态程序分析工具的输出数据进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;顶级大型语言模型达到F1分数0.915，最佳图神经网络和经典机器学习模型达到F1分数0.904；在低于7%的假阴性率下，领先模型可消除66.9%的良性包无需人工审查，每个包处理时间约60毫秒；当模型调整为0.8精度水平时，可检测99.2%的可利用污染流，仅遗漏0.8%。&lt;h4&gt;结论&lt;/h4&gt;该机器学习方法在现实世界漏洞分类中显示出强大的潜力，能显著减少人工审查工作量，同时保持高检测率。&lt;h4&gt;翻译&lt;/h4&gt;程序分析工具经常产生大量候选漏洞报告，需要昂贵的人工审查，这带来了一个实际挑战：安全分析师如何优先处理最可能是真实漏洞的报告？本文研究了机器学习是否可以应用于优先处理程序分析工具报告的漏洞。我们专注于Node.js包，收集了1,883个Node.js包的基准测试，每个包包含一个报告的ACE或ACI漏洞。我们评估了多种机器学习方法，包括经典模型、图神经网络、大型语言模型以及结合GNN和LLMs的混合模型，这些模型都基于动态程序分析工具的输出数据进行训练。顶级LLM达到F1分数0.915，而最佳GNN和经典ML模型达到F1分数0.904。在低于7%的假阴性率下，领先模型消除了66.9%需要人工审查的良性包，每个包处理时间约60毫秒。如果将最佳模型调整为在0.8的精度水平下运行（即在所有警告中允许20%的假阳性），我们的方法可以检测99.2%的可利用污染流，仅遗漏0.8%，这表明在现实世界漏洞分类方面具有强大的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Program analysis tools often produce large volumes of candidate vulnerabilityreports that require costly manual review, creating a practical challenge: howcan security analysts prioritize the reports most likely to be truevulnerabilities?  This paper investigates whether machine learning can be applied toprioritizing vulnerabilities reported by program analysis tools. We focus onNode.js packages and collect a benchmark of 1,883 Node.js packages, eachcontaining one reported ACE or ACI vulnerability. We evaluate a variety ofmachine learning approaches, including classical models, graph neural networks(GNNs), large language models (LLMs), and hybrid models that combine GNN andLLMs, trained on data based on a dynamic program analysis tool's output. Thetop LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML modelsreaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leadingmodel eliminates 66.9% of benign packages from manual review, taking around 60ms per package. If the best model is tuned to operate at a precision level of0.8 (i.e., allowing 20% false positives amongst all warnings), our approach candetect 99.2% of exploitable taint flows while missing only 0.8%, demonstratingstrong potential for real-world vulnerability triage.</description>
      <author>example@mail.com (Ronghao Ni, Aidan Z. H. Yang, Min-Chien Hsu, Nuno Sabino, Limin Jia, Ruben Martins, Darion Cassel, Kevin Cheang)</author>
      <guid isPermaLink="false">2510.20739v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series</title>
      <link>http://arxiv.org/abs/2510.20718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 27 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了半导体制造中的异常预测问题，提出了两种新方法：基于N-BEATS的单变量预测模型和基于图神经网络(GNN)的多变量关系模型。研究显示GNN在性能和效率上均优于N-BEATS模型，为制造环境中的在线异常预测提供了有前景的解决方案。&lt;h4&gt;背景&lt;/h4&gt;半导体制造是极其复杂且精度要求高的过程，涉及数千个相互依赖的参数。多变量时间序列分析在此类环境中至关重要，但异常预测面临传感器数据高维度、真实故障稀有导致的类别不平衡，以及变量间复杂相互依赖关系等挑战。&lt;h4&gt;目的&lt;/h4&gt;推动半导体制造领域从异常检测向异常预测发展，为实现实时工艺校正和主动故障预防提供技术支持。&lt;h4&gt;方法&lt;/h4&gt;提出包含两个阶段的异常预测框架：首先在假设无异常的数据集上训练预测模型，然后对未见时间序列数据进行预测。两种方法差异在于：第一种使用N-BEATS模型进行单变量时间序列预测，假设变量间独立；第二种使用图神经网络捕捉变量间关系，取消独立性假设。&lt;h4&gt;主要发现&lt;/h4&gt;两种模型在20个时间点的预测范围内表现出色，并在50个时间点范围内保持稳定的异常预测能力。GNN持续优于N-BEATS模型，同时需要更少的可训练参数和更低的计算成本。&lt;h4&gt;结论&lt;/h4&gt;图神经网络(GNN)作为在线异常预测解决方案具有显著优势，适合在制造环境中部署，为半导体制造业的实时监控和故障预防提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;半导体制造是一个极其复杂且精度要求高的过程，其特点是在各种工具和工艺步骤中收集数千个相互依赖的参数。多变量时间序列分析已成为这类环境中实时监控和故障检测的关键领域。然而，半导体制造中的异常预测面临几个关键挑战，包括传感器数据的高维度和由于真实故障的稀有性导致的严重类别不平衡。此外，变量之间复杂的相互依赖关系使异常预测和根本原因分析都变得复杂。本文提出了两种新颖的方法，推动该领域从异常检测向异常预测发展，这是实现实时工艺校正和主动故障预防的关键步骤。提出的异常预测框架包含两个主要阶段：在假设不含异常的数据集上训练预测模型，然后对未见的时间序列数据进行预测。将预测结果与训练信号的预测进行比较，超出预定阈值的偏差被标记为异常。这两种方法在采用的预测模型上有所不同。第一种利用N-BEATS模型进行单变量时间序列预测，假设变量间相互独立。第二种利用图神经网络捕捉变量间关系，取消了这一假设。两种模型在长达20个时间点的预测范围内都表现出强大的预测性能，并在长达50个时间点的范围内保持稳定的异常预测能力。图神经网络始终优于N-BEATS模型，同时需要显著更少的可训练参数和更低的计算成本。这些结果将图神经网络定位为一种有前景的在线异常预测解决方案，适合在制造环境中部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semiconductor manufacturing is an extremely complex and precision-drivenprocess, characterized by thousands of interdependent parameters collectedacross diverse tools and process steps. Multi-variate time-series analysis hasemerged as a critical field for real-time monitoring and fault detection insuch environments. However, anomaly prediction in semiconductor fabricationpresents several critical challenges, including high dimensionality of sensordata and severe class imbalance due to the rarity of true faults. Furthermore,the complex interdependencies between variables complicate both anomalyprediction and root-cause-analysis. This paper proposes two novel approaches toadvance the field from anomaly detection to anomaly prediction, an essentialstep toward enabling real-time process correction and proactive faultprevention. The proposed anomaly prediction framework contains two main stages:(a) training a forecasting model on a dataset assumed to contain no anomalies,and (b) performing forecast on unseen time series data. The forecast iscompared with the forecast of the trained signal. Deviations beyond apredefined threshold are flagged as anomalies. The two approaches differ in theforecasting model employed. The first assumes independence between variables byutilizing the N-BEATS model for univariate time series forecasting. The secondlifts this assumption by utilizing a Graph Neural Network (GNN) to captureinter-variable relationships. Both models demonstrate strong forecastingperformance up to a horizon of 20 time points and maintain stable anomalyprediction up to 50 time points. The GNN consistently outperforms the N-BEATSmodel while requiring significantly fewer trainable parameters and lowercomputational cost. These results position the GNN as promising solution foronline anomaly forecasting to be deployed in manufacturing environments.</description>
      <author>example@mail.com (Daniel Sorensen, Bappaditya Dey, Minjin Hwang, Sandip Halder)</author>
      <guid isPermaLink="false">2510.20718v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>GRACE: GRaph-based Addiction Care prEdiction</title>
      <link>http://arxiv.org/abs/2510.20671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为GRACE的图神经网络框架，用于自动确定成瘾患者的适当护理场所，解决了传统方法中存在的类别不平衡问题，并在真实数据中取得显著改进。&lt;h4&gt;背景&lt;/h4&gt;确定成瘾患者的适当护理场所是影响治疗效果和资源利用的关键临床决策。由于专科治疗资源不足，且现有决策方法在成瘾数据集中存在严重类别不平衡问题，需要开发自动化框架。&lt;h4&gt;目的&lt;/h4&gt;开发一个自动化框架来确定成瘾患者的适当护理场所，并解决成瘾数据集中的类别不平衡问题。&lt;h4&gt;方法&lt;/h4&gt;提出新的图神经网络框架GRACE，将护理场所预测形式化为结构化学习问题；进行广泛特征工程；提出获取无偏元图的新方法训练图神经网络以克服类别不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据上的实验结果表明，与竞争基线相比，少数类别的F1分数提高了11-35%。&lt;h4&gt;结论&lt;/h4&gt;GRACE框架在解决成瘾患者护理场所预测问题上有效，特别是在处理类别不平衡问题时表现优异。&lt;h4&gt;翻译&lt;/h4&gt;确定成瘾患者的适当护理场所是影响患者治疗效果和资源有效利用的最关键临床决策之一。由于缺乏足够的专科治疗资源，如住院床位或人员，开发自动化框架的需求尚未得到满足。当前的决策方法在成瘾数据集中存在严重的类别不平衡问题。为解决这一限制，我们提出了一个新的图神经网络框架，将护理场所预测形式化为一个结构化学习问题。此外，我们进行了广泛的特征工程，并提出了一种获得无偏元图的新方法来训练图神经网络，以克服类别不平衡问题。真实世界数据的实验结果表明，与竞争基线相比，少数类别的F1分数提高了11-35%。代码和注释嵌入可在https://anonymous.4open.science/r/GRACE-F8E1/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Determining the appropriate locus of care for addiction patients is one ofthe most critical clinical decisions that affects patient treatment outcomesand effective use of resources. With a lack of sufficient specialized treatmentresources, such as inpatient beds or staff, there is an unmet need to developan automated framework for the same. Current decision-making approaches sufferfrom severe class imbalances in addiction datasets. To address this limitation,we propose a novel graph neural network (GRACE) framework that formalizes locusof care prediction as a structured learning problem. Further, we performextensive feature engineering and propose a new approach of obtaining anunbiased meta-graph to train a GNN to overcome the class imbalance problem.Experimental results in real-world data show an improvement of 11-35% in termsof the F1 score of the minority class over competitive baselines. The codes andnote embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.</description>
      <author>example@mail.com (Subham Kumar, Prakrithi Shivaprakash, Koustav Rudra, Lekhansh Shukla, Animesh Mukherjee)</author>
      <guid isPermaLink="false">2510.20671v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting</title>
      <link>http://arxiv.org/abs/2510.20591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络(GNN)的电网拓扑优化(NTO)加速方法，用于解决输电网拥堵管理问题。该方法通过母线分裂技术，能够实现大规模系统的近实时优化，并具有良好的泛化能力和跨系统可转移性。&lt;h4&gt;背景&lt;/h4&gt;电网拓扑优化(NTO)通过母线分裂可以缓解输电网拥堵并减少重新调度成本。然而，对于大规模系统，使用现有求解器解决这种混合整数非线性问题在近实时情况下目前是不可行的。虽然机器学习方法是一种有前景的替代方案，但它们对未见的拓扑、变化的运行条件和不同系统的泛化能力有限，限制了其实际应用。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在开发一种能够实现大规模系统近实时电网拓扑优化的方法，同时解决现有方法在泛化能力和跨系统可转移性方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;本文考虑线性化交流潮流(AC PF)，将NTO公式化为拥堵管理问题，并提出了一种图神经网络(GNN)加速方法。研究人员开发了一种异构边缘感知消息传递神经网络，以预测有效的母线分裂行为作为候选NTO解决方案。该方法能够捕获局部流模式，实现对未见的拓扑变化的泛化，并提高跨系统的可转移性。&lt;h4&gt;主要发现&lt;/h4&gt;案例研究显示，所提出的GNN方法在速度上提高了4个数量级，能够在GOC 2000母线系统上一分钟内提供交流可行解，优化间隙为2.3%。这表明该方法在近实时优化方面取得了显著进展。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明，所提出的GNN方法在具有拓扑和跨系统泛化能力的大规模系统近实时NTO方面取得了重要进展，为解决电网拥堵管理问题提供了新的有效途径。&lt;h4&gt;翻译&lt;/h4&gt;电网拓扑优化(NTO)通过母线分裂可以缓解输电网拥堵并减少重新调度成本。然而，使用现有求解器解决大规模系统的这种混合整数非线性问题在近实时情况下目前是不可行的。机器学习方法已成为一种有前景的替代方案，但它们对未见的拓扑、变化的运行条件和不同系统的泛化能力有限，这限制了它们的实际应用。本文考虑线性化交流潮流(AC PF)，将NTO公式化为拥堵管理问题，并提出了一种图神经网络(GNN)加速方法。我们开发了一种异构边缘感知消息传递神经网络，以预测有效的母线分裂行为作为候选NTO解决方案。所提出的GNN捕获局部流模式，实现对未见的拓扑变化的泛化，并提高了跨系统的可转移性。案例研究显示速度提高了4个数量级，在GOC 2000母线系统上在一分钟内提供交流可行解，优化间隙为2.3%。这些结果表明，在具有拓扑和跨系统泛化能力的大规模系统近实时NTO方面取得了重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network topology optimization (NTO) via busbar splitting can mitigatetransmission grid congestion and reduce redispatch costs. However, solving thismixed-integer non-linear problem for large-scale systems in near-real-time iscurrently intractable with existing solvers. Machine learning (ML) approacheshave emerged as a promising alternative, but they have limited generalizationto unseen topologies, varying operating conditions, and different systems,which limits their practical applicability. This paper formulates NTO forcongestion management problem considering linearized AC PF, and proposes agraph neural network (GNN)-accelerated approach. We develop a heterogeneousedge-aware message passing NN to predict effective busbar splitting actions ascandidate NTO solutions. The proposed GNN captures local flow patterns,achieves generalization to unseen topology changes, and improvestransferability across systems. Case studies show up to 4 orders-of-magnitudespeed-up, delivering AC-feasible solutions within one minute and a 2.3%optimality gap on the GOC 2000-bus system. These results demonstrate asignificant step toward near-real-time NTO for large-scale systems withtopology and cross-system generalization.</description>
      <author>example@mail.com (Ali Rajaei, Peter Palensky, Jochen L. Cremer)</author>
      <guid isPermaLink="false">2510.20591v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Structural Invariance Matters: Rethinking Graph Rewiring through Graph Metrics</title>
      <link>http://arxiv.org/abs/2510.20556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 5 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图重连是减轻图神经网络和图变换器中过度压缩的关键技术，通过修改图拓扑改善信息流动，但会改变图结构，可能扭曲重要拓扑信号。本研究首次系统分析了重连对图结构指标的影响及其与下游任务性能的关系。&lt;h4&gt;背景&lt;/h4&gt;图重连已成为减轻图神经网络（GNNs）和图变换器（Graph Transformers）中过度压缩的关键技术，通过修改图拓扑来改善信息流动。然而，重连会改变图的结构，有扭曲重要拓扑相关信号的风险。&lt;h4&gt;目的&lt;/h4&gt;提供关于重连如何影响各种图结构指标的系统性分析，以及这些变化如何与下游任务性能相关联，明确需要保留哪些结构属性以确保性能提升和结构保真度。&lt;h4&gt;方法&lt;/h4&gt;研究七种不同的重连策略，并将局部和全局图属性的变化与节点分类准确性进行关联分析。&lt;h4&gt;主要发现&lt;/h4&gt;成功的重连方法倾向于保留局部结构，同时在全局连接方面保持灵活性。这一模式在研究中呈现出一致性。&lt;h4&gt;结论&lt;/h4&gt;这些发现为设计有效的重连策略提供了新的见解，弥合了图理论与实际GNN优化之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;图重连已成为减轻图神经网络（GNNs）和图变换器（Graph Transformers）中过度压缩的关键技术，通过修改图拓扑来改善信息流动。虽然有效，但重连本质上改变了图的结构，带来了扭曲重要拓扑相关信号的风险。然而，尽管重连的使用日益增多，人们尚不清楚需要保留哪些结构属性以确保性能提升和结构保真度。在本工作中，我们首次提供了关于重连如何影响各种图结构指标的系统性分析，以及这些变化如何与下游任务性能相关联。我们研究了七种不同的重连策略，并将局部和全局图属性的变化与节点分类准确性进行关联。我们的结果揭示了一个一致的规律：成功的重连方法倾向于保留局部结构，同时在全局连接方面保持灵活性。这些发现为设计有效的重连策略提供了新的见解，弥合了图理论与实际GNN优化之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph rewiring has emerged as a key technique to alleviate over-squashing inGraph Neural Networks (GNNs) and Graph Transformers by modifying the graphtopology to improve information flow. While effective, rewiring inherentlyalters the graph's structure, raising the risk of distorting importanttopology-dependent signals. Yet, despite the growing use of rewiring, little isknown about which structural properties must be preserved to ensure bothperformance gains and structural fidelity. In this work, we provide the firstsystematic analysis of how rewiring affects a range of graph structuralmetrics, and how these changes relate to downstream task performance. We studyseven diverse rewiring strategies and correlate changes in local and globalgraph properties with node classification accuracy. Our results reveal aconsistent pattern: successful rewiring methods tend to preserve localstructure while allowing for flexibility in global connectivity. These findingsoffer new insights into the design of effective rewiring strategies, bridgingthe gap between graph theory and practical GNN optimization.</description>
      <author>example@mail.com (Alexandre Benoit, Catherine Aitken, Yu He)</author>
      <guid isPermaLink="false">2510.20556v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Intransitive Player Dominance and Market Inefficiency in Tennis Forecasting: A Graph Neural Network Approach</title>
      <link>http://arxiv.org/abs/2510.20454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络的网球比赛预测方法，专门处理非传递性玩家优势现象，并发现博彩市场在此类比赛中存在效率低下的问题。&lt;h4&gt;背景&lt;/h4&gt;非传递性玩家优势（即A击败B，B击败C，但C击败A）在网球比赛中很常见，但很少有预测方法能处理这种复杂关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够建模和利用非传递性关系的预测方法，以识别和利用博彩市场中的效率低下点。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络方法，通过时序有向图建模玩家关系，其中玩家作为节点，历史比赛结果作为有向边。&lt;h4&gt;主要发现&lt;/h4&gt;博彩公司Pinnacle Sports在处理高非传递性复杂度的比赛时表现不佳；基于图的方法能有效捕捉这些场景中的关系动态；在1903次投注中，使用Kelly下注策略获得了3.26%的显著正回报率。&lt;h4&gt;结论&lt;/h4&gt;博彩市场在处理非传递性比赛时存在效率低下的问题，而所提出的图神经网络方法能够成功利用这种市场效率低下问题。&lt;h4&gt;翻译&lt;/h4&gt;非传递性玩家优势，即玩家A击败B，B击败C，但C击败A，在竞争性网球比赛中很常见。然而，很少有已知的方法尝试将其纳入预测方法中。我们通过图神经网络方法解决了这个问题，该方法通过时序有向图明确建模这些非传递性关系，玩家作为节点，他们的历史比赛结果作为有向边。我们发现博彩公司Pinnacle Sports在处理高非传递性复杂度的比赛时表现不佳，并认为我们的基于图的方法能够捕捉这些场景中的关系动态。当有选择地使用我们的模型对高非传递性比赛进行下注时（准确率65.7%，0.215 Brier分数），在1903次投注中，使用Kelly下注策略获得了3.26%的显著正回报率，表明市场在处理非传递性比赛时存在效率低下的问题，而我们的方法成功地利用了这一点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intransitive player dominance, where player A beats B, B beats C, but C beatsA, is common in competitive tennis. Yet, there are few known attempts toincorporate it within forecasting methods. We address this problem with a graphneural network approach that explicitly models these intransitive relationshipsthrough temporal directed graphs, with players as nodes and their historicalmatch outcomes as directed edges. We find the bookmaker Pinnacle Sports poorlyhandles matches with high intransitive complexity and posit that ourgraph-based approach is uniquely positioned to capture relational dynamics inthese scenarios. When selectively betting on higher intransitivity matchupswith our model (65.7% accuracy, 0.215 Brier Score), we achieve significantpositive returns of 3.26% ROI with Kelly staking over 1903 bets, suggesting amarket inefficiency in handling intransitive matchups that our approachsuccessfully exploits.</description>
      <author>example@mail.com (Lawrence Clegg, John Cartlidge)</author>
      <guid isPermaLink="false">2510.20454v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Quantifying Distributional Invariance in Causal Subgraph for IRM-Free Graph Generalization</title>
      <link>http://arxiv.org/abs/2510.20295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种不需要不变风险最小化框架的方法，用于捕获因果子图，解决分布偏移下图神经网络的分布外泛化挑战。通过识别因果子图在不同环境中具有较小分布变化的特性，建立了不变分布标准，并基于此提出了范数引导的不变分布目标方法，实验证明该方法优于现有最先进方法。&lt;h4&gt;背景&lt;/h4&gt;分布偏移下图神经网络的分布外泛化仍然是一个关键挑战。现有方法通常采用不变风险最小化框架，需要昂贵的环境注释或启发式生成的合成分割。&lt;h4&gt;目的&lt;/h4&gt;开发一种不需要不变风险最小化框架的方法，用于捕获因果子图，以克服现有方法的限制。&lt;h4&gt;方法&lt;/h4&gt;基于不变分布标准，系统地揭示分布偏移与表示范数之间的定量关系，用于识别因果子图；提出一种范数引导的不变分布目标方法，用于因果子图发现和预测。&lt;h4&gt;主要发现&lt;/h4&gt;因果子图在不同环境中表现出比非因果成分小得多的分布变化，这被形式化为不变分布标准，并从理论上得到了证明。&lt;h4&gt;结论&lt;/h4&gt;在两个广泛使用的基准测试上，所提出的方法在图泛化任务中始终优于现有的最先进方法，证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在分布偏移下的分布外泛化对于图神经网络仍然是一个关键挑战。现有方法通常采用不变风险最小化框架，需要昂贵的环境注释或启发式生成的合成分割。为避免这些限制，本文旨在开发一种不需要不变风险最小化的方法来捕获因果子图。我们首先确定因果子图在不同环境中表现出比非因果成分小得多的分布变化，这被形式化为不变分布标准，并在本文中从理论上进行了证明。基于这一标准，我们系统地揭示了分布偏移与表示范数之间的定量关系，用于识别因果子图，并深入研究了其潜在机制。最后，我们通过引入一个基于范数的不变分布目标，提出了一种不需要不变风险最小化的方法，用于因果子图发现和预测。在两个广泛使用的基准测试上的大量实验表明，我们的方法在图泛化方面始终优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Out-of-distribution generalization under distributional shifts remains acritical challenge for graph neural networks. Existing methods generally adoptthe Invariant Risk Minimization (IRM) framework, requiring costly environmentannotations or heuristically generated synthetic splits. To circumvent theselimitations, in this work, we aim to develop an IRM-free method for capturingcausal subgraphs. We first identify that causal subgraphs exhibit substantiallysmaller distributional variations than non-causal components across diverseenvironments, which we formalize as the Invariant Distribution Criterion andtheoretically prove in this paper. Building on this criterion, wesystematically uncover the quantitative relationship between distributionalshift and representation norm for identifying the causal subgraph, andinvestigate its underlying mechanisms in depth. Finally, we propose an IRM-freemethod by introducing a norm-guided invariant distribution objective for causalsubgraph discovery and prediction. Extensive experiments on two widely usedbenchmarks demonstrate that our method consistently outperformsstate-of-the-art methods in graph generalization.</description>
      <author>example@mail.com (Yang Qiu, Yixiong Zou, Jun Wang, Wei Liu, Xiangyu Fu, Ruixuan Li)</author>
      <guid isPermaLink="false">2510.20295v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Layer-to-Layer Knowledge Mixing in Graph Neural Network for Chemical Property Prediction</title>
      <link>http://arxiv.org/abs/2510.20236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为层到层知识混合(LKM)的自知识蒸馏方法，能够在不显著增加计算复杂性的情况下提高图神经网络(GNNs)预测分子性质的准确性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)是目前预测分子性质最有效的方法，但仍需要更准确的模型。增加模型复杂度可以提高准确性，但也会增加训练和推理过程中的计算成本和内存需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种提高GNN准确性的方法，同时不显著增加计算复杂性和内存需求。&lt;h4&gt;方法&lt;/h4&gt;提出层到层知识混合(LKM)方法，通过最小化GNN层现有隐藏嵌入之间的平均绝对距离，有效聚合多跳和多尺度信息，改进局部和全局分子特征的表示。&lt;h4&gt;主要发现&lt;/h4&gt;LKM方法在不显著增加训练和推理复杂性的情况下，提高了最先进GNN的准确性。使用三种不同的GNN架构(DimeNet++、MXMNet和PAMNet)和三个数据集(QM9、MD17和Chignolin)进行评估，LKM将量子化学和生物物理性质预测的平均绝对误差分别降低了最高9.8%(QM9)、45.3%(MD17能量)和22.9%(Chignolin)。&lt;h4&gt;结论&lt;/h4&gt;LKM有潜力显著提高GNN预测化学性质的准确性，而不会显著增加训练和推理成本。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)是目前预测分子性质最有效的方法，但仍需要更准确的模型。通过增加模型复杂度可以提高GNN的准确性，但这也会增加训练和推理过程中的计算成本和内存需求。在本研究中，我们开发了层到层知识混合(LKM)，一种新颖的自知识蒸馏方法，它在提高最先进GNN准确性的同时，在训练和推理过程中只增加了微不足道的计算复杂度。通过最小化GNN层现有隐藏嵌入之间的平均绝对距离，LKM有效地聚合了多跳和多尺度信息，实现了对局部和全局分子特征的改进表示。我们使用三种不同的GNN架构(DimeNet++、MXMNet和PAMNet)和量子化学性质数据集(QM9、MD17和Chignolin)评估了LKM。我们发现LKM方法将量子化学和生物物理性质预测的平均绝对误差分别降低了高达9.8%(QM9)、45.3%(MD17能量)和22.9%(Chignolin)。这项工作证明了LKM在不显著增加训练和推理成本的情况下，显著提高GNN化学性质预测准确性的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are the currently most effective methods forpredicting molecular properties but there remains a need for more accuratemodels. GNN accuracy can be improved by increasing the model complexity butthis also increases the computational cost and memory requirement duringtraining and inference. In this study, we develop Layer-to-Layer KnowledgeMixing (LKM), a novel self-knowledge distillation method that increases theaccuracy of state-of-the-art GNNs while adding negligible computationalcomplexity during training and inference. By minimizing the mean absolutedistance between pre-existing hidden embeddings of GNN layers, LKM efficientlyaggregates multi-hop and multi-scale information, enabling improvedrepresentation of both local and global molecular features. We evaluated LKMusing three diverse GNN architectures (DimeNet++, MXMNet, and PAMNet) usingdatasets of quantum chemical properties (QM9, MD17 and Chignolin). We foundthat the LKM method effectively reduces the mean absolute error of quantumchemical and biophysical property predictions by up to 9.8% (QM9), 45.3% (MD17Energy), and 22.9% (Chignolin). This work demonstrates the potential of LKM tosignificantly improve the accuracy of GNNs for chemical property predictionwithout any substantial increase in training and inference cost.</description>
      <author>example@mail.com (Teng Jiek See, Daokun Zhang, Mario Boley, David K. Chalmers)</author>
      <guid isPermaLink="false">2510.20236v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Extending machine learning model for implicit solvation to free energy calculations</title>
      <link>http://arxiv.org/abs/2510.20103v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络的隐式溶剂模型LSNN，通过结合力匹配和化学变量导数匹配，实现了与显式溶剂模型相当的自由能预测准确性，同时提高了计算效率，为药物发现应用奠定了基础。&lt;h4&gt;背景&lt;/h4&gt;隐式溶剂方法在分子模拟中计算效率高，但与显式溶剂模型相比准确性不足，限制了其在精确热力学计算中的应用。基于机器学习的方法目前主要依靠力匹配，可能导致能量预测相差任意常数，不适合绝对自由能比较。&lt;h4&gt;目的&lt;/h4&gt;开发更精确的隐式溶剂势能，解决当前基于机器学习的方法仅依靠力匹配的缺点，确保不同化学物种的溶剂化自由能可以进行有意义的比较。&lt;h4&gt;方法&lt;/h4&gt;引入基于图神经网络(GNN)的隐式溶剂模型Lambda Solvation Neural Network (LSNN)，除了力匹配外，还训练网络匹配化学变量的导数。在约30万个小分子的数据集上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;LSNN实现了与显式溶剂化学模拟相当的自由能预测准确性，同时提供了计算速度提升，为药物发现中的未来应用建立了基础框架。&lt;h4&gt;结论&lt;/h4&gt;LSNN克服了传统隐式溶剂方法的局限性，通过结合力匹配和化学变量导数匹配，确保了不同化学物种的溶剂化自由能可以进行有意义的比较。&lt;h4&gt;翻译&lt;/h4&gt;隐式溶剂方法为分子模拟中的溶剂化效应建模提供了计算效率高的框架。然而，与显式溶剂模型相比，其准确性往往不足，限制了其在精确热力学计算中的应用。机器学习(ML)的最新进展提供了一种克服这些局限性的机会，通过利用神经网络为各种应用开发更精确的隐式溶剂势能。当前基于ML方法的一个主要缺点是其仅依靠力匹配，这可能导致能量预测相差任意常数，因此不适合绝对自由能比较。在此，我们介绍了一种新颖的方法，即基于图神经网络(GNN)的隐式溶剂模型，称为Lambda Solvation Neural Network (LSNN)。除了力匹配外，该网络还被训练以匹配化学变量的导数，确保不同化学物种的溶剂化自由能可以进行有意义的比较。在约30万个小分子的数据集上训练后，LSNN实现了与显式溶剂化学模拟相当的自由能预测准确性，同时提供了计算加速，并为药物发现中的未来应用建立了基础框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The implicit solvent approach offers a computationally efficient framework tomodel solvation effects in molecular simulations. However, its accuracy oftenfalls short compared to explicit solvent models, limiting its use in precisethermodynamic calculations. Recent advancements in machine learning (ML)present an opportunity to overcome these limitations by leveraging neuralnetworks to develop more precise implicit solvent potentials for diverseapplications. A major drawback of current ML-based methods is their reliance onforce-matching alone, which can lead to energy predictions that differ by anarbitrary constant and are therefore unsuitable for absolute free energycomparisons. Here, we introduce a novel methodology with a graph neural network(GNN)-based implicit solvent model, dubbed Lambda Solvation Neural Network(LSNN). In addition to force-matching, this network was trained to match thederivatives of alchemical variables, ensuring that solvation free energies canbe meaningfully compared across chemical species.. Trained on a dataset ofapproximately 300,000 small molecules, LSNN achieves free energy predictionswith accuracy comparable to explicit-solvent alchemical simulations, whileoffering a computational speedup and establishing a foundational framework forfuture applications in drug discovery.</description>
      <author>example@mail.com (Rishabh Dey, Michael Brocidiacono, Kushal Koirala, Alexander Tropsha, Konstantin I. Popov)</author>
      <guid isPermaLink="false">2510.20103v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs</title>
      <link>http://arxiv.org/abs/2510.19954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了RELATE，一种与模式无关的特征编码器，用于处理关系型多表数据，能与任何通用图神经网络一起使用，在保持性能的同时大幅减少参数数量。&lt;h4&gt;背景&lt;/h4&gt;关系型多表数据在电子商务、医疗健康和科学研究等领域很常见，可表示为具有多模态节点属性的异质时间图。现有图神经网络依赖特定模式的特征编码器，需为每种节点类型和特征列设计单独模块，限制了可扩展性和参数共享。&lt;h4&gt;目的&lt;/h4&gt;开发一种与模式无关、即插即用的特征编码器，能与任何通用图神经网络配合使用，解决现有方法的可扩展性和参数共享问题。&lt;h4&gt;方法&lt;/h4&gt;RELATE采用针对分类、数值、文本和时间属性的共享模态特定编码器，后接Perceiver风格的交叉注意力模块，将特征聚合成固定大小、排列不变的节点表示。&lt;h4&gt;主要发现&lt;/h4&gt;在RelBench基准测试中，RELATE实现了与特定模式编码器相当的性能（差距在3%以内），同时将参数数量减少了高达5倍。&lt;h4&gt;结论&lt;/h4&gt;RELATE设计支持变化的数据模式，并支持通用图神经网络的多数据集预训练，为关系图数据的基础模型铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;关系型多表数据在电子商务、医疗健康和科学研究等领域很常见，可以自然地表示为具有多模态节点属性的异质时间图。现有的图神经网络依赖于特定模式的特征编码器，需要为每种节点类型和特征列设计单独的模块，这阻碍了可扩展性和参数共享。我们引入了RELATE（关系型实体潜在聚合编码器），这是一种与模式无关的、即插即用的特征编码器，可以与任何通用图神经网络一起使用。RELATE采用针对分类、数值、文本和时间属性的共享模态特定编码器，然后是一个Perceiver风格的交叉注意力模块，将特征聚合成固定大小、排列不变的节点表示。我们在RelBench基准测试的ReLGNN和HGT上评估了RELATE，结果显示它实现了与特定模式编码器相当的性能（差距在3%以内），同时将参数数量减少了高达5倍。这种设计支持变化的数据模式，并支持通用图神经网络的多数据集预训练，为关系图数据的基础模型铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relational multi-table data is common in domains such as e-commerce,healthcare, and scientific research, and can be naturally represented asheterogeneous temporal graphs with multi-modal node attributes. Existing graphneural networks (GNNs) rely on schema-specific feature encoders, requiringseparate modules for each node type and feature column, which hindersscalability and parameter sharing. We introduce RELATE (Relational Encoder forLatent Aggregation of Typed Entities), a schema-agnostic, plug-and-play featureencoder that can be used with any general purpose GNN. RELATE employs sharedmodality-specific encoders for categorical, numerical, textual, and temporalattributes, followed by a Perceiver-style cross-attention module thataggregates features into a fixed-size, permutation-invariant noderepresentation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,where it achieves performance within 3% of schema-specific encoders whilereducing parameter counts by up to 5x. This design supports varying schemas andenables multi-dataset pretraining for general-purpose GNNs, paving the waytoward foundation models for relational graph data.</description>
      <author>example@mail.com (Joseph Meyer, Divyansha Lachi, Reza Mohammadi, Roshan Reddy Upendra, Eva L. Dyer, Mark Li, Tom Palczewski)</author>
      <guid isPermaLink="false">2510.19954v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>FnRGNN: Distribution-aware Fairness in Graph Neural Network</title>
      <link>http://arxiv.org/abs/2510.19257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FnRGNN，一种用于图神经网络节点回归的公平性感知处理框架，通过在结构、表示和预测三个层面进行干预，有效减少了组间差异而不牺牲性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在结构化数据学习中表现出色，但在回归任务中的公平性研究仍然不足。&lt;h4&gt;目的&lt;/h4&gt;解决图神经网络在节点级回归任务中的公平性问题，特别是处理现有方法无法解决的连续特性挑战。&lt;h4&gt;方法&lt;/h4&gt;FnRGNN框架采用三级干预策略：结构层面的边重加权、表示层面的MMD对齐、以及预测层面的Sinkhorn分布匹配归一化。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实世界数据集上的实验表明，FnRGNN能够有效减少组间差异，同时保持模型性能。&lt;h4&gt;结论&lt;/h4&gt;多层级干预策略确保了FnRGNN在复杂图拓扑结构下的鲁棒公平性，为图神经网络回归任务中的公平性问题提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在结构化数据学习中表现出色，但在回归任务中的公平性研究仍然不足。现有方法主要针对分类任务和表示层面的去偏见，无法完全处理节点级回归的连续特性。我们提出了FnRGNN，一个基于GNN的节点回归的公平性感知处理框架，在三个层面进行干预：(i)结构层面的边重加权，(ii)表示层面的MMD对齐，(iii)预测层面的Sinkhorn分布匹配归一化。这种多层级策略确保了在复杂图拓扑结构下的鲁棒公平性。在四个真实世界数据集上的实验表明，FnRGNN减少了组间差异而不牺牲性能。代码可在https://github.com/sybeam27/FnRGNN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760796&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) excel at learning from structured data, yetfairness in regression tasks remains underexplored. Existing approaches mainlytarget classification and representation-level debiasing, which cannot fullyaddress the continuous nature of node-level regression. We propose FnRGNN, afairness-aware in-processing framework for GNN-based node regression thatapplies interventions at three levels: (i) structure-level edge reweighting,(ii) representation-level alignment via MMD, and (iii) prediction-levelnormalization through Sinkhorn-based distribution matching. This multi-levelstrategy ensures robust fairness under complex graph topologies. Experiments onfour real-world datasets demonstrate that FnRGNN reduces group disparitieswithout sacrificing performance. Code is available athttps://github.com/sybeam27/FnRGNN.</description>
      <author>example@mail.com (Soyoung Park, Sungsu Lim)</author>
      <guid isPermaLink="false">2510.19257v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Graph Neural Networks: A Mutual Learning Approach</title>
      <link>http://arxiv.org/abs/2510.19223v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种图神经网络之间的协作学习框架，在没有预训练教师模型的情况下，让简单浅层的GNN架构相互教学，从而提升模型在推理时的性能，特别是在处理多个任务时。&lt;h4&gt;背景&lt;/h4&gt;知识蒸馏技术是将复杂教师模型专业知识转移到轻量级学生模型的有效工具，特别适合在资源受限设备上部署高性能模型。该方法已成功应用于图神经网络，利用其表达能力生成捕获结构和特征相关信息的节点嵌入。&lt;h4&gt;目的&lt;/h4&gt;探索GNNs之间协作学习的潜力，使相对简单和浅层的GNN架构能够协同学习高效模型，在推理时表现更好，特别是在处理多个任务时。&lt;h4&gt;方法&lt;/h4&gt;提出一个协作学习框架，其中学生GNN集合在整个训练过程中相互教学；引入自适应logit加权单元促进模型间的高效知识交换；采用熵增强技术改进相互学习；这些组件动态赋能模型在训练过程中调整学习策略，优化下游任务性能。&lt;h4&gt;主要发现&lt;/h4&gt;简单浅层的GNN架构能够协同学习高效模型；这些模型在推理时表现更好，特别是在处理多个任务时；提出的自适应logit加权单元和熵增强技术有效促进了模型间的知识交换和相互学习。&lt;h4&gt;结论&lt;/h4&gt;通过协作学习框架，学生GNN能够在没有预训练教师模型的情况下相互教学；提出的方法在节点分类和图分类任务上表现出色；为资源受限环境中的高效模型部署提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;知识蒸馏技术已成为一种强大的工具，用于将复杂教师模型的专业知识转移到轻量级学生模型中，特别适合在资源受限设备上部署高性能模型。这种方法已成功应用于图神经网络，利用其表达能力生成捕获结构和特征相关信息的节点嵌入。在本研究中，我们通过探索GNNs之间协作学习的潜力，偏离了传统的KD方法。在没有预训练教师模型的情况下，我们证明相对简单和浅层的GNN架构能够协同学习高效模型，在推理时表现更好，特别是在处理多个任务时。我们提出了一个协作学习框架，其中学生GNN集合在整个训练过程中相互教学。我们引入了自适应logit加权单元以促进模型间的高效知识交换，以及熵增强技术以改进相互学习。这些组件动态赋能模型在训练过程中调整学习策略，优化下游任务性能。在三个节点分类和图分类数据集上进行的广泛实验证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge distillation (KD) techniques have emerged as a powerful tool fortransferring expertise from complex teacher models to lightweight studentmodels, particularly beneficial for deploying high-performance models inresource-constrained devices. This approach has been successfully applied tograph neural networks (GNNs), harnessing their expressive capabilities togenerate node embeddings that capture structural and feature-relatedinformation. In this study, we depart from the conventional KD approach byexploring the potential of collaborative learning among GNNs. In the absence ofa pre-trained teacher model, we show that relatively simple and shallow GNNarchitectures can synergetically learn efficient models capable of performingbetter during inference, particularly in tackling multiple tasks. We propose acollaborative learning framework where ensembles of student GNNs mutually teacheach other throughout the training process. We introduce an adaptive logitweighting unit to facilitate efficient knowledge exchange among models and anentropy enhancement technique to improve mutual learning. These componentsdynamically empower the models to adapt their learning strategies duringtraining, optimizing their performance for downstream tasks. Extensiveexperiments conducted on three datasets each for node and graph classificationdemonstrate the effectiveness of our approach.</description>
      <author>example@mail.com (Paul Agbaje, Akajyoti Mitra, Afia Anjum, Pranali Khose, Ebelechukwu Nwafor, Habeeb Olufowobi)</author>
      <guid isPermaLink="false">2510.19223v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>An Active Diffusion Neural Network for Graphs</title>
      <link>http://arxiv.org/abs/2510.19202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ADGNN的新型图神经网络，通过整合多个外部信息源实现主动扩散，解决了传统扩散GNN的过平滑问题，使节点能保留独特特征同时获取全局图结构信息。&lt;h4&gt;背景&lt;/h4&gt;热扩散类比促进了图信息流理解和图神经网络发展，但大多数扩散GNN模拟被动热扩散，存在过平滑问题，限制了捕获全局图信息的能力。这类似于宇宙热寂理论，即封闭系统中能量分布随时间变得均匀，导致节点表示收敛到相同特征向量。&lt;h4&gt;目的&lt;/h4&gt;解决传统扩散GNN中的过平滑问题，使节点能保留独特特征同时有效获取图全局结构信息。&lt;h4&gt;方法&lt;/h4&gt;提出ADGNN（主动扩散图神经网络），通过整合多个外部信息源实现主动扩散，动态影响扩散过程克服过平滑问题。通过直接计算主动扩散迭代公式的闭式解，实现真正的无限扩散。&lt;h4&gt;主要发现&lt;/h4&gt;ADGNN在多种图任务上与最先进GNN模型相比，显著提高了准确性和效率，有效捕获全局图信息并保持节点独特性。&lt;h4&gt;结论&lt;/h4&gt;ADGNN通过主动扩散机制解决了传统扩散GNN的过平滑问题，使节点既能保持独特特征又能获取全局图结构信息，在多种图任务上表现出色。&lt;h4&gt;翻译&lt;/h4&gt;热扩散的类比增强了对图中信息流的理解，并启发了图神经网络(GNNs)的发展。然而，大多数基于扩散的GNN模拟被动热扩散，仍然存在过平滑问题，限制了它们捕获全局图信息的能力。受宇宙热寂理论的启发，该理论认为在封闭系统中能量分布随时间变得均匀，我们认识到，在没有外部输入的情况下，节点表示会随着扩散过程收敛到相同的特征向量。为解决这个问题，我们提出了主动扩散图神经网络(ADGNN)。ADGNN通过整合多个外部信息源实现主动扩散，这些信息源动态影响扩散过程，有效克服了过平滑问题。此外，我们的方法通过直接计算主动扩散迭代公式的闭式解，实现了真正的无限扩散。这使得节点能够保留其独特特征，同时有效地获取对图全局结构的全面理解。我们在各种图任务上将ADGNN与几个最先进的GNN模型进行了评估。结果表明，ADGNN显著提高了准确性和效率，突显了其在捕获全局图信息和保持节点独特性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The analogy to heat diffusion has enhanced our understanding of informationflow in graphs and inspired the development of Graph Neural Networks (GNNs).However, most diffusion-based GNNs emulate passive heat diffusion, which stillsuffers from over-smoothing and limits their ability to capture global graphinformation. Inspired by the heat death of the universe, which posits thatenergy distribution becomes uniform over time in a closed system, we recognizethat, without external input, node representations in a graph converge toidentical feature vectors as diffusion progresses. To address this issue, wepropose the Active Diffusion-based Graph Neural Network (ADGNN). ADGNN achievesactive diffusion by integrating multiple external information sources thatdynamically influence the diffusion process, effectively overcoming theover-smoothing problem. Furthermore, our approach realizes true infinitediffusion by directly calculating the closed-form solution of the activediffusion iterative formula. This allows nodes to preserve their uniquecharacteristics while efficiently gaining comprehensive insights into thegraph's global structure. We evaluate ADGNN against several state-of-the-artGNN models across various graph tasks. The results demonstrate that ADGNNsignificantly improves both accuracy and efficiency, highlighting itseffectiveness in capturing global graph information and maintaining nodedistinctiveness.</description>
      <author>example@mail.com (Mengying Jiang)</author>
      <guid isPermaLink="false">2510.19202v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning noisy tissue dynamics across time scales</title>
      <link>http://arxiv.org/abs/2510.19090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了一种仿生机器学习框架，能够直接从实验电影中推断噪声多细胞动力学，成功应用于上皮组织、果蝇翅膀发育和ERK波介导的细胞信号过程。&lt;h4&gt;背景&lt;/h4&gt;组织动力学在从伤口愈合到形态发生的生物过程中起着关键作用，但这些噪声多细胞动力学难以预测。&lt;h4&gt;目的&lt;/h4&gt;引入一个能够直接从实验电影中推断噪声多细胞动力学的仿生机器学习框架。&lt;h4&gt;方法&lt;/h4&gt;该生成模型结合了图神经网络、归一化流和WaveNet算法，将组织表示为神经随机微分方程，其中细胞是 evolving graph 的边。&lt;h4&gt;主要发现&lt;/h4&gt;该机器学习架构反映了底层生物组织的架构，大大减少了训练所需的数据量；该模型能捕捉随机细胞运动并预测细胞在分裂周期中状态的演变；可以准确生成发育系统和细胞信号过程的实验动力学。&lt;h4&gt;结论&lt;/h4&gt;该方法为在生物工程和临床环境中作为数字孪生使用铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;组织动力学在从伤口愈合到形态发生的生物过程中起着关键作用。然而，这些噪声多细胞动力学难以预测。在此，我们引入了一种仿生机器学习框架，能够直接从实验电影中推断噪声多细胞动力学。该生成模型结合了图神经网络、归一化流和WaveNet算法，将组织表示为神经随机微分方程，其中细胞是 evolving graph 的边。与卷积或全连接神经网络相比，该机器学习架构反映了底层生物组织的架构，大大减少了训练所需的数据量。以上皮组织实验为例，我们表明该模型不仅能捕捉随机细胞运动，还能预测细胞在分裂周期中状态的演变。最后，我们证明了该方法可以准确生成发育系统（如果蝇翅膀）和由随机ERK波介导的细胞信号过程的实验动力学，为在生物工程和临床环境中作为数字孪生使用铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tissue dynamics play a crucial role in biological processes ranging fromwound healing to morphogenesis. However, these noisy multicellular dynamics arenotoriously hard to predict. Here, we introduce a biomimetic machine learningframework capable of inferring noisy multicellular dynamics directly fromexperimental movies. This generative model combines graph neural networks,normalizing flows and WaveNet algorithms to represent tissues as neuralstochastic differential equations where cells are edges of an evolving graph.This machine learning architecture reflects the architecture of the underlyingbiological tissues, substantially reducing the amount of data needed to trainit compared to convolutional or fully-connected neural networks. Takingepithelial tissue experiments as a case study, we show that our model not onlycaptures stochastic cell motion but also predicts the evolution of cell statesin their division cycle. Finally, we demonstrate that our method can accuratelygenerate the experimental dynamics of developmental systems, such as the flywing, and cell signaling processes mediated by stochastic ERK waves, paving theway for its use as a digital twin in bioengineering and clinical contexts.</description>
      <author>example@mail.com (Ming Han, John Devany, Michel Fruchart, Margaret L. Gardel, Vincenzo Vitelli)</author>
      <guid isPermaLink="false">2510.19090v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Committors without Descriptors</title>
      <link>http://arxiv.org/abs/2510.18018v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于committor的增强采样方法，用于解决原子模拟中的稀有事件研究挑战。通过结合图神经网络技术，实现了对系统亚稳态之间频繁转换的促进，并对过程过渡态集合进行了广泛采样。&lt;h4&gt;背景&lt;/h4&gt;稀有事件研究是原子模拟中的主要挑战之一，已经提出了几种增强采样方法来解决这一问题。最近有研究建议使用committor（提供对稀有事件的精确形式化描述）来解决这一挑战。&lt;h4&gt;目的&lt;/h4&gt;进一步自动化基于committor的方法，通过将其与图神经网络的强大表达能力相结合，使方法能够直接处理原子坐标而非描述符，并展示基于图的方法在描述溶剂分子在离子对解离或配体结合等系统中作用方面的优势。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于committor的方法，该方法促进系统亚稳态之间的频繁转换，并允许对过程过渡态集合进行广泛采样。该方法的特点是自洽和半自动，利用变分准则迭代优化基于神经网络的committor参数化，使用一组物理描述符作为输入。进一步通过将先前方法与图神经网络的强大表达能力相结合，直接处理原子坐标而非描述符。&lt;h4&gt;主要发现&lt;/h4&gt;基于committor的方法能够促进系统亚稳态之间的频繁转换；该方法允许对过程过渡态集合进行广泛采样；结合图神经网络使方法更加自动化；基于图的方法在描述溶剂分子在离子对解离或配体结合等系统中作用方面具有优势。&lt;h4&gt;结论&lt;/h4&gt;通过将基于committor的方法与图神经网络相结合，研究成功实现了方法的进一步自动化，并展示了基于图的方法在描述溶剂分子在特定系统中的角色方面的优势，为原子模拟中稀有事件的研究提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;稀有事件研究是原子模拟中的主要挑战之一，已经提出了几种解决这一问题的增强采样方法。最近有研究建议使用committor（提供对稀有事件的精确形式化描述）来解决这一挑战。我们最近跟进这一建议，提出了一种基于committor的方法，该方法促进系统亚稳态之间的频繁转换，并允许对过程过渡态集合进行广泛采样。我们方法的优势之一是自洽和半自动，利用变分准则迭代优化基于神经网络的committor参数化，使用一组物理描述符作为输入。在这里，我们通过将先前方法与图神经网络的强大表达能力相结合，进一步自动化了这一过程，图神经网络可以直接处理原子坐标而非描述符。除了在基准系统上的应用外，我们强调了基于图的方法在描述离子对解离或配体结合等系统中溶剂分子作用方面的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The study of rare events is one of the major challenges in atomisticsimulations, and several enhanced sampling methods towards its solution havebeen proposed. Recently, it has been suggested that the use of the committor,which provides a precise formal description of rare events, could be of use inthis context. We have recently followed up on this suggestion and proposed acommittor-based method that promotes frequent transitions between themetastable states of the system and allows extensive sampling of the processtransition state ensemble. One of the strengths of our approach is beingself-consistent and semi-automatic, exploiting a variational criterion toiteratively optimize a neural-network-based parametrization of the committor,which uses a set of physical descriptors as input. Here, we further automatethis procedure by combining our previous method with the expressive power ofgraph neural networks, which can directly process atomic coordinates ratherthan descriptors. Besides applications on benchmark systems, we highlight theadvantages of a graph-based approach in describing the role of solventmolecules in systems, such as ion pair dissociation or ligand binding.</description>
      <author>example@mail.com (Peilin Kang, Jintu Zhang, Enrico Trizio, TingJun Hou, Michele Parrinello)</author>
      <guid isPermaLink="false">2510.18018v2</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>AutoScape: Geometry-Consistent Long-Horizon Scene Generation</title>
      <link>http://arxiv.org/abs/2510.20726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025. Project page: https://auto-scape.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AutoScape是一个长时程驾驶场景生成框架，通过创新的RGB-D扩散模型生成几何一致的关键帧，并使用视频扩散模型生成连贯的驾驶视频。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶和场景生成领域，需要生成长时间、几何一致的驾驶场景，这是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成长时间（超过20秒）、真实且几何一致的驾驶视频的框架，解决现有方法在长时程场景生成中的局限性。&lt;h4&gt;方法&lt;/h4&gt;AutoScape框架包含一个RGB-D扩散模型，该模型在共享潜在空间中联合处理图像和深度，基于先前生成的关键帧场景几何条件，并使用一致的引导来引导采样过程。给定高质量RGB-D关键帧后，视频扩散模型在关键帧之间进行插值。&lt;h4&gt;主要发现&lt;/h4&gt;AutoScape能够生成超过20秒的真实且几何一致的驾驶视频，相比之前的最先进方法，在长时程FID和FVD评分上分别提高了48.6%和43.0%。&lt;h4&gt;结论&lt;/h4&gt;AutoScape通过创新的RGB-D扩散模型和视频插值方法，显著提高了长时程驾驶场景生成的质量和一致性，为自动驾驶模拟和训练提供了更真实的数据来源。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了AutoScape，一个长时程驾驶场景生成框架。其核心是一个创新的RGB-D扩散模型，迭代生成稀疏的、几何一致的关键帧，作为场景外观和几何的可靠锚点。为了保持长距离几何一致性，模型1)在共享潜在空间中联合处理图像和深度，2)显式地基于先前生成的关键帧的场景几何（即渲染的点云）进行条件化，3)使用一致的引导来引导采样过程。给定高质量的RGB-D关键帧后，视频扩散模型在它们之间进行插值，生成密集且连贯的视频帧。AutoScape生成超过20秒的真实且几何一致的驾驶视频，相比之前的最先进方法，长时程FID和FVD评分分别提高了48.6%和43.0%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成长时间（20秒以上）且保持3D几何一致性的驾驶场景视频问题。这个问题在自动驾驶领域至关重要，因为自动驾驶系统需要大量高质量、长时间一致的场景数据进行安全可靠的测试和验证，而当前方法在长时间生成时难以保持几何一致性，限制了实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将长时间场景生成问题分解为稀疏关键帧生成和密集帧插值两个子问题，采用层次化方法解决。核心洞察是几何一致性退化是长时间生成的关键瓶颈，因此需要显式建模几何信息。方法借鉴了扩散模型在图像生成中的成功应用、RGB-D联合建模、ControlNet的条件控制机制以及视频扩散模型，但创新性地设计了RGB-D扩散模型、几何条件机制和Warp Consistent Guidance来提高几何一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过层次化方法解决长时间场景生成：先生成稀疏但几何一致的关键帧作为场景锚点，然后在关键帧间插值生成密集视频帧。整体流程分为两个阶段：1)关键帧生成阶段：从输入图像反投影为3D点云，投影到下一视角生成渲染点和掩码，使用RGB-D扩散模型生成新关键帧，迭代添加到点云集合，并用Warp Consistent Guidance提高一致性；2)插值阶段：使用视频扩散模型在关键帧间生成密集视频帧，条件于从关键帧渲染的3D点云确保几何一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)层次化框架，分解为关键帧生成和插值；2)RGB-D扩散模型，联合建模颜色和深度；3)显式几何条件，将渲染点云作为条件输入；4)Warp Consistent Guidance，减少误差累积；5)两阶段训练策略。相比WonderJourney和Vista等之前工作，AutoScape专门设计的RGB-D扩散模型具有更好的几何感知能力，通过关键帧锚点确保长期一致性，并显式利用几何信息而非仅依赖时间一致性模块。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AutoScape提出了一种创新的层次化框架，通过RGB-D扩散模型生成几何一致的关键帧，并结合视频插值实现了长达20秒的高质量、3D一致的驾驶场景视频生成，显著提升了长时间场景生成的质量和一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes AutoScape, a long-horizon driving scene generationframework. At its core is a novel RGB-D diffusion model that iterativelygenerates sparse, geometrically consistent keyframes, serving as reliableanchors for the scene's appearance and geometry. To maintain long-rangegeometric consistency, the model 1) jointly handles image and depth in a sharedlatent space, 2) explicitly conditions on the existing scene geometry (i.e.,rendered point clouds) from previously generated keyframes, and 3) steers thesampling process with a warp-consistent guidance. Given high-quality RGB-Dkeyframes, a video diffusion model then interpolates between them to producedense and coherent video frames. AutoScape generates realistic andgeometrically consistent driving videos of over 20 seconds, improving thelong-horizon FID and FVD scores over the prior state-of-the-art by 48.6\% and43.0\%, respectively.</description>
      <author>example@mail.com (Jiacheng Chen, Ziyu Jiang, Mingfu Liang, Bingbing Zhuang, Jong-Chyi Su, Sparsh Garg, Ying Wu, Manmohan Chandraker)</author>
      <guid isPermaLink="false">2510.20726v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata</title>
      <link>http://arxiv.org/abs/2510.20708v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为ALICE-LRI的新型方法，实现了从旋转LiDAR点云生成无损距离图像，无需制造商元数据或校准文件。该方法通过自动推断LiDAR传感器的内几何参数，实现了零点损失的点云投影和重建。&lt;h4&gt;背景&lt;/h4&gt;3D LiDAR传感器在自主导航、环境监测和遥感应用中至关重要。为了高效处理这些传感器产生的大量点云数据，LiDAR数据通常被投影到2D距离图像中，这些图像根据点的角度位置和距离来组织点。然而，传统的投影方法存在基本的几何不一致性，导致不可逆的信息丢失，影响高保真应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用的、与传感器无关的方法，能够从旋转LiDAR点云生成无损距离图像，无需制造商元数据或校准文件，并保持几何精度在传感器精度范围内。&lt;h4&gt;方法&lt;/h4&gt;ALICE-LRI是一种自动LiDAR内标定估计方法，能够自动逆向工程任何旋转LiDAR传感器的内几何。该方法通过推断关键参数，包括激光束配置、角度分布和每束校准校正，实现无损投影和完整的点云重建，零点损失。&lt;h4&gt;主要发现&lt;/h4&gt;在完整的KITTI和DurLAR数据集上的全面评估表明，ALICE-LRI实现了完美的点保留，所有点云中都没有点丢失。几何精度保持在传感器精度范围内，建立了具有实时性能的几何无损性。压缩案例研究验证了显著的下游效益，展示了实际应用中的显著质量改进。&lt;h4&gt;结论&lt;/h4&gt;从近似到无损LiDAR投影的范式转变，为需要完整几何保存的高精度遥感应用开辟了新的可能性。ALICE-LRI方法代表了LiDAR数据处理领域的重要进展，能够在不损失信息的情况下实现高效处理。&lt;h4&gt;翻译&lt;/h4&gt;3D LiDAR传感器对于自主导航、环境监测和遥感应用中的精密制图至关重要。为了高效处理这些传感器产生的大量点云数据，LiDAR数据通常被投影到2D距离图像中，这些图像根据点的角度位置和距离来组织点。虽然这些距离图像表示能够实现高效处理，但传统的投影方法存在基本的几何不一致性，导致不可逆的信息丢失，影响高保真应用。我们提出了ALICE-LRI（无损距离图像的自动LiDAR内标定估计），这是第一个通用的、与传感器无关的方法，能够从旋转LiDAR点云生成无损距离图像，无需制造商元数据或校准文件。我们的算法通过推断关键参数（包括激光束配置、角度分布和每束校准校正）来自动逆向工程任何旋转LiDAR传感器的内几何，实现无损投影和零点损失的完整点云重建。在完整的KITTI和DurLAR数据集上的全面评估表明，ALICE-LRI实现了完美的点保留，所有点云中都没有点丢失。几何精度保持在传感器精度范围内，建立了具有实时性能的几何无损性。我们还介绍了压缩案例研究，验证了显著的下游效益，展示了实际应用中的显著质量改进。从近似到无损LiDAR投影的范式转变，为需要完整几何保存的高精度遥感应用开辟了新的可能性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决的是如何在不依赖制造商提供的校准元数据的情况下，从旋转式激光雷达(LiDAR)点云生成无损的2D距离图像。这个问题很重要，因为LiDAR传感器在自动驾驶、环境监测和遥感等领域至关重要，而传统投影方法存在几何不一致性导致信息损失，会影响高精度应用的质量。许多实际场景中我们只有已校准的点云数据，而没有原始传感器数据或校准文件，限制了现有方法的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了理想传感器模型和实际传感器模型之间的差异，认识到需要推断出厂校准的几何参数。他们设计了ALICE-LRI方法，包含参数估计和无损投影两个主要阶段。参数估计又分为垂直和水平参数估计，使用Hough变换识别候选扫描线，加权最小二乘法进行拟合，以及冲突解决机制确保一致性。作者借鉴了Hough变换用于特征提取的技术和加权最小二乘法处理异方差噪声，但解决了现有校准研究的逆问题——从已校准点云推断参数而非从原始数据估计参数。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是自动反向推断LiDAR传感器的内在几何结构，无需制造商元数据，直接从点云数据推断关键参数，生成与传感器几何完全一致的距离图像，实现点云完全重建。整体流程分为：1)垂直参数估计：使用Hough变换识别候选扫描线参数，选择一致点，加权最小二乘拟合，解决冲突；2)水平参数估计：对每束进行分辨率穷举搜索，计算水平和方位角偏移，对点数不足的扫描线使用启发式方法；3)距离图像生成：使用估计参数将点云投影到2D图像，确保双射性和完全重建。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个通用方法实现从已校准点云生成无损距离图像，无需元数据；2)自动推断传感器内在几何参数；3)实现完全零点损失和几何无损；4)具有实时性能；5)提供开源实现。相比之前工作，传统方法依赖制造商提供的查找表或数据包信息，现有校准研究处理正向问题而非逆问题，大多数方法接受轻微几何失真，而ALICE-LRI实现了完全无损且通用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ALICE-LRI首次实现了无需传感器元数据即可从旋转式LiDAR点云生成完全无损的距离图像，通过自动推断传感器内在几何参数解决了传统投影方法中的信息损失问题，为高精度遥感应用提供了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D LiDAR sensors are essential for autonomous navigation, environmentalmonitoring, and precision mapping in remote sensing applications. Toefficiently process the massive point clouds generated by these sensors, LiDARdata is often projected into 2D range images that organize points by theirangular positions and distances. While these range image representations enableefficient processing, conventional projection methods suffer from fundamentalgeometric inconsistencies that cause irreversible information loss,compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDARIntrinsic Calibration Estimation for Lossless Range Images), the first general,sensor-agnostic method that achieves lossless range image generation fromspinning LiDAR point clouds without requiring manufacturer metadata orcalibration files. Our algorithm automatically reverse-engineers the intrinsicgeometry of any spinning LiDAR sensor by inferring critical parametersincluding laser beam configuration, angular distributions, and per-beamcalibration corrections, enabling lossless projection and complete point cloudreconstruction with zero point loss. Comprehensive evaluation across thecomplete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfectpoint preservation, with zero points lost across all point clouds. Geometricaccuracy is maintained well within sensor precision limits, establishinggeometric losslessness with real-time performance. We also present acompression case study that validates substantial downstream benefits,demonstrating significant quality improvements in practical applications. Thisparadigm shift from approximate to lossless LiDAR projections opens newpossibilities for high-precision remote sensing applications requiring completegeometric preservation.</description>
      <author>example@mail.com (Samuel Soutullo, Miguel Yermo, David L. Vilariño, Óscar G. Lorenzo, José C. Cabaleiro, Francisco F. Rivera)</author>
      <guid isPermaLink="false">2510.20708v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning</title>
      <link>http://arxiv.org/abs/2510.20406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了PointMapPolicy，一种新颖的机器人操作方法，通过将点云组织成结构化网格并结合RGB数据，实现了高效的多模态感知，在多种操作任务中达到了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;机器人操作系统受益于互补的传感模态，其中每种模态提供独特的环境信息。点云能捕获详细的几何结构，而RGB图像提供丰富的语义上下文。然而，当前点云方法难以捕获细粒度细节，特别是对于复杂任务；而RGB方法缺乏几何意识，限制了其精度和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型方法，结合点云和RGB图像的优势，解决现有方法在几何细节和语义理解方面的局限性，提高机器人操作系统的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出了PointMapPolicy，一种将扩散策略基于未下采样的结构化点网格的新方法。这种方法创建的数据类型更容易从观测中提取形状和空间关系，并且可以在参考帧之间转换。由于点网格的结构规整，可以直接使用成熟的计算机视觉技术处理3D数据。模型使用xLSTM作为骨干网络，高效融合点图与RGB数据以增强多模态感知。&lt;h4&gt;主要发现&lt;/h4&gt;在RoboCasa和CALVIN基准测试以及真实机器人评估上的大量实验表明，该方法在各种操作任务中实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;PointMapPolicy有效结合了点云和RGB数据的优势，通过结构化点网格和多模态融合，显著提升了机器人操作系统的性能，特别是在需要精细几何理解和语义上下文的复杂任务中。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作系统受益于互补的传感模态，每种模态提供独特的环境信息。点云捕获详细的几何结构，而RGB图像提供丰富的语义上下文。当前点云方法难以捕获细粒度细节，特别是对于复杂任务，而RGB方法缺乏几何意识，这限制了它们的精度和泛化能力。我们引入了PointMapPolicy，一种新颖的方法，将扩散策略基于未下采样的结构化点网格。产生的数据类型更容易从观测中提取形状和空间关系，并且可以在参考帧之间转换。但由于它们在规则网格中的结构，我们能够直接使用成熟的计算机视觉技术处理3D数据。使用xLSTM作为骨干网络，我们的模型高效地将点图与RGB数据融合以增强多模态感知。在RoboCasa和CALVIN基准测试以及真实机器人评估的大量实验中，我们证明我们的方法在各种操作任务中实现了最先进的性能。概述和演示可在我们的项目页面查看：https://point-map.github.io/Point-Map/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人系统中多模态感知的挑战，即如何同时利用点云提供的几何信息和RGB图像提供的语义信息。这个问题很重要，因为机器人执行复杂任务时需要同时理解场景的几何结构和语义内容，而现有的方法要么缺乏几何细节（仅用RGB图像），要么难以处理精细细节（仅用点云），限制了机器人在复杂环境中的精确操作能力和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到计算机视觉社区在立体重建方面的最新进展启发，提出将点云转换为结构化的点图表示。他们借鉴了立体重建技术中的点图方法，将其应用于机器人模仿学习领域。设计过程中，他们考虑了点云和RGB图像的优缺点，创建了可以与标准视觉架构兼容的点图表示，并探索了多种融合策略来结合几何和语义信息。同时，他们借鉴了xLSTM架构作为骨干网络，平衡了时间建模能力和计算效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将无结构的点云转换为结构化的点图表示，使其能够与标准视觉架构兼容，同时融合点图的几何信息和RGB图像的语义信息。整体实现流程包括：1)将深度图转换为结构化的点图表示，编码为2D网格中的XYZ坐标；2)使用预训练的视觉编码器处理RGB图像和点图；3)采用晚期融合策略(如拼接)来结合多模态信息；4)使用xLSTM作为骨干网络处理多模态输入；5)基于EDM框架应用扩散策略生成动作序列；6)通过少量去噪步骤(4步)生成最终动作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出点图(point maps)这一新的观察模态，首次在基于扩散的模仿学习中使用；2)将点云结构化为规则的2D网格，可直接应用标准视觉架构，无需KNN和FPS等昂贵操作；3)设计高效的多模态融合策略，平衡几何精度和语义理解；4)使用xLSTM作为骨干网络，相比Transformer具有更高的计算效率。相比之前的工作，PointMapPolicy不需要复杂的点云处理步骤，能同时利用几何和语义信息，且计算效率更高，在多个基准测试中取得了最先进性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointMapPolicy通过结构化点云处理和多模态融合，在机器人模仿学习中实现了几何精度与语义理解的平衡，并在多个基准测试中取得了最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic manipulation systems benefit from complementary sensing modalities,where each provides unique environmental information. Point clouds capturedetailed geometric structure, while RGB images provide rich semantic context.Current point cloud methods struggle to capture fine-grained detail, especiallyfor complex tasks, which RGB methods lack geometric awareness, which hinderstheir precision and generalization. We introduce PointMapPolicy, a novelapproach that conditions diffusion policies on structured grids of pointswithout downsampling. The resulting data type makes it easier to extract shapeand spatial relationships from observations, and can be transformed betweenreference frames. Yet due to their structure in a regular grid, we enable theuse of established computer vision techniques directly to 3D data. Using xLSTMas a backbone, our model efficiently fuses the point maps with RGB data forenhanced multi-modal perception. Through extensive experiments on the RoboCasaand CALVIN benchmarks and real robot evaluations, we demonstrate that ourmethod achieves state-of-the-art performance across diverse manipulation tasks.The overview and demos are available on our project page:https://point-map.github.io/Point-Map/</description>
      <author>example@mail.com (Xiaogang Jia, Qian Wang, Anrui Wang, Han A. Wang, Balázs Gyenes, Emiliyan Gospodinov, Xinkai Jiang, Ge Li, Hongyi Zhou, Weiran Liao, Xi Huang, Maximilian Beck, Moritz Reuss, Rudolf Lioutikov, Gerhard Neumann)</author>
      <guid isPermaLink="false">2510.20406v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>NeuralTouch: Neural Descriptors for Precise Sim-to-Real Tactile Robot Control</title>
      <link>http://arxiv.org/abs/2510.20390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了NeuralTouch，一个集成神经描述场（NDF）和触觉传感的多模态框架，通过轻柔的物理交互实现准确、可推广的机器人抓取。&lt;h4&gt;背景&lt;/h4&gt;抓取精度对精确物体操作至关重要，通常需要机器人手与物体仔细对齐。NDF是一种基于视觉的生成抓取姿势的方法，但单独使用时可能因相机校准不完美、点云不完整和物体变化而产生不准确姿势。触觉传感虽能实现更精确接触，但现有方法通常仅限于简单、预定义的接触几何形状。&lt;h4&gt;目的&lt;/h4&gt;开发一个集成NDF和触觉传感的框架，通过轻柔的物理交互实现准确、可推广的抓取，解决视觉方法在精确抓取方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;利用NDF隐式表示目标接触几何形状，训练深度强化学习策略使用触觉反馈来优化抓取。该策略以神经描述符为条件，无需明确指定接触类型。通过模拟中的消融研究和零样本迁移到现实世界任务（如销钉插入孔和瓶盖开启）进行验证，无需额外微调。&lt;h4&gt;主要发现&lt;/h4&gt;NeuralTouch显著提高了抓取精度和鲁棒性，优于基线方法，为精确、接触丰富的机器人操作提供了一个通用框架。&lt;h4&gt;结论&lt;/h4&gt;通过结合视觉（NDF）和触觉传感，NeuralTouch实现了准确、可推广的抓取，为需要精确接触的机器人操作任务提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;抓取精度是精确物体操作的关键前提，通常需要机器人手与物体之间的仔细对齐。神经描述场（NDF）提供了一种有前景的基于视觉的方法，可生成跨物体类别的抓取姿势。然而，仅靠NDF可能因不完美的相机校准、不完整的点云和物体变化而产生不准确姿势。同时，触觉传感能实现更精确接触，但现有方法通常学习限于简单、预定义接触几何形状的策略。在这项工作中，我们介绍了NeuralTouch，一个集成NDF和触觉传感的多模态框架，通过轻柔的物理交互实现准确、可推广的抓取。我们的方法利用NDF隐式表示目标接触几何形状，基于此训练深度强化学习策略，使用触觉反馈来优化抓取。该策略以神经描述符为条件，不需要明确指定接触类型。我们通过模拟中的消融研究和零样本迁移到现实世界操作任务（如销钉插入孔和瓶盖开启）来验证NeuralTouch，无需额外微调。结果表明，NeuralTouch显著提高了抓取精度和鲁棒性，优于基线方法，为精确、接触丰富的机器人操作提供了一个通用框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grasping accuracy is a critical prerequisite for precise object manipulation,often requiring careful alignment between the robot hand and object. NeuralDescriptor Fields (NDF) offer a promising vision-based method to generategrasping poses that generalize across object categories. However, NDF alone canproduce inaccurate poses due to imperfect camera calibration, incomplete pointclouds, and object variability. Meanwhile, tactile sensing enables more precisecontact, but existing approaches typically learn policies limited to simple,predefined contact geometries. In this work, we introduce NeuralTouch, amultimodal framework that integrates NDF and tactile sensing to enableaccurate, generalizable grasping through gentle physical interaction. Ourapproach leverages NDF to implicitly represent the target contact geometry,from which a deep reinforcement learning (RL) policy is trained to refine thegrasp using tactile feedback. This policy is conditioned on the neuraldescriptors and does not require explicit specification of contact types. Wevalidate NeuralTouch through ablation studies in simulation and zero-shottransfer to real-world manipulation tasks--such as peg-out-in-hole and bottlelid opening--without additional fine-tuning. Results show that NeuralTouchsignificantly improves grasping accuracy and robustness over baseline methods,offering a general framework for precise, contact-rich robotic manipulation.</description>
      <author>example@mail.com (Yijiong Lin, Bowen Deng, Chenghua Lu, Max Yang, Efi Psomopoulou, Nathan F. Lepora)</author>
      <guid isPermaLink="false">2510.20390v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>AnyPcc: Compressing Any Point Cloud with a Single Universal Model</title>
      <link>http://arxiv.org/abs/2510.20331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为AnyPcc的通用点云压缩框架，解决了深度学习点云几何压缩中的泛化性问题。该框架通过通用上下文模型和实例自适应微调策略，有效处理了分布外数据，并在15个数据集上实现了最先进的压缩性能。&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的点云几何压缩面临泛化性挑战，主要原因是缺乏鲁棒的上下文模型和对分布外数据的低效处理。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用点云压缩框架AnyPcc，以解决点云几何压缩中的泛化性问题，特别是上下文建模和分布外数据处理方面的挑战。&lt;h4&gt;方法&lt;/h4&gt;AnyPcc包含两个主要组件：1) 通用上下文模型，利用空间和通道分组的先验信息捕获鲁棒的上下文依赖关系；2) 实例自适应微调策略，通过协同显式和隐式压缩范式处理分布外数据，为每个实例微调一小部分网络权重并整合到位流中。&lt;h4&gt;主要发现&lt;/h4&gt;在15个不同数据集上的广泛实验表明，AnyPcc在点云压缩方面设立了新的最先进水平，证明了其有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;AnyPcc成功解决了点云几何压缩中的泛化性问题，通过创新的通用上下文模型和实例自适应微调策略，实现了更好的压缩性能，为点云压缩领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;泛化性是基于深度学习的点云几何压缩的一个关键挑战。我们认为这源于两个关键限制：缺乏鲁棒的上下文模型和对分布外数据的低效处理。为解决这两个问题，我们引入了AnyPcc，一个通用的点云压缩框架。AnyPcc首先采用通用上下文模型，利用空间和通道分组的先验信息来捕获鲁棒的上下文依赖关系。其次，我们新颖的实例自适应微调策略通过协同显式和隐式压缩范式来处理分布外数据。它为每个实例微调一小部分网络权重，并将它们整合到位流中，其中权重的边际位成本远小于几何压缩带来的节省。在15个不同数据集组成的基准测试上的大量实验证实，AnyPcc在点云压缩方面设立了新的最先进水平。我们的代码和数据集将被发布以鼓励可重复研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云压缩中的泛化挑战。现有方法通常针对特定点云密度设计，无法在真实世界中各种密度的点云上保持稳定性能，并且在遇到分布外数据时压缩效率急剧下降。这个问题很重要，因为随着自动驾驶和虚拟现实等应用中3D内容的普及，点云已成为主要数据表示形式，对高效压缩算法有迫切需求，而真实世界的点云数据具有广泛的多样性，许多关键类型（如医学扫描、3D高斯溅射）通常没有专用的训练数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的局限性：空间上下文模型在密集数据上表现好但在稀疏场景中性能差，而通道级模型虽能处理稀疏输入但牺牲了空间信息。为了解决这一权衡，他们设计了通用上下文模型(UCM)协同整合空间和通道先验。对于分布外数据问题，他们借鉴了参数高效微调技术，结合了隐式神经表示的优点和预训练模型的效率。作者借鉴了图像压缩中的上下文建模技术，但将其应用于几何占用码，还参考了参数高效微调方法，但这些技术在点云压缩领域尚未被探索。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合通用强大的预训练模型与快速的实例特定适应。整体流程包括：1)UCM采用从粗到细的层次结构，通过空间-通道上下文分解处理点云数据；2)使用3D棋盘模式将体素分为两组(G1和G2)，并在每组内将8位占用码分解为两个4位子符号；3)通过协同特征聚合增强上下文；4)IAFT策略只微调网络的一小部分参数(最终预测头)，实现快速实例适应；5)最终压缩位流包含微调后的权重和几何组件两部分；6)解码时先重建模型参数，再对几何数据进行解码。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)AnyPcc框架是首个使用单一统一模型实现高压缩率和跨多样化点云类型鲁棒性能的方法；2)UCM通过创新的棋盘分组策略协同整合空间和通道先验，建立跨越所有点云密度的鲁棒上下文建模；3)IAFT策略通过只微调预训练模型的一小部分参数，实现快速实例特定适应；4)实现了统一的无损和有损压缩。相比之前的工作，AnyPcc解决了类别特定方法的局限性，避免了Unicorn等泛化尝试的非统一架构问题，克服了隐式压缩的高计算成本，并解决了现有上下文模型在密集和稀疏数据之间的权衡问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AnyPcc通过引入通用上下文模型和实例自适应微调策略，首次实现了使用单一统一模型在各种点云类型上达到最先进的压缩性能，解决了点云压缩中长期存在的泛化挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalization remains a critical challenge for deep learning-based pointcloud geometry compression. We argue this stems from two key limitations: thelack of robust context models and the inefficient handling ofout-of-distribution (OOD) data. To address both, we introduce AnyPcc, auniversal point cloud compression framework. AnyPcc first employs a UniversalContext Model that leverages priors from both spatial and channel-wise groupingto capture robust contextual dependencies. Second, our novel Instance-AdaptiveFine-Tuning (IAFT) strategy tackles OOD data by synergizing explicit andimplicit compression paradigms. It fine-tunes a small subset of network weightsfor each instance and incorporates them into the bitstream, where the marginalbit cost of the weights is dwarfed by the resulting savings in geometrycompression. Extensive experiments on a benchmark of 15 diverse datasetsconfirm that AnyPcc sets a new state-of-the-art in point cloud compression. Ourcode and datasets will be released to encourage reproducible research.</description>
      <author>example@mail.com (Kangli Wang, Qianxi Yi, Yuqi Ye, Shihao Li, Wei Gao)</author>
      <guid isPermaLink="false">2510.20331v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.15530v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种纯视觉单视图扩散策略学习方法(VO-DP)，通过融合预训练视觉基础模型的语义和几何特征，在机器人操作任务中取得了优异性能，特别是在真实世界任务中明显优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;在模仿学习领域，基于视觉运动的扩散策略学习是机器人操作的主要方向。现有方法多依赖点云作为输入，通过点云特征学习构建场景表示，但缺乏对纯视觉解决方案的深入探索，而纯视觉方案具有显著潜力。&lt;h4&gt;目的&lt;/h4&gt;提出一种纯视觉且单视图的扩散策略学习方法(VO-DP)，利用预训练的视觉基础模型实现语义和几何特征的有效融合，以提高机器人操作的性能。&lt;h4&gt;方法&lt;/h4&gt;利用VGGT的中间特征，结合来自DINOv2的语义特征和来自交替注意力块的几何特征，通过交叉注意力融合特征，并使用CNN进行空间压缩，形成策略头的输入。&lt;h4&gt;主要发现&lt;/h4&gt;VO-DP在仿真任务中达到64.6%的平均成功率，与基于点云的方法DP3(64.0%)相当，远高于纯视觉基线DP(34.8%)；在真实世界任务中达到87.9%的成功率，显著优于DP3(67.5%)和DP(11.2%)。鲁棒性评估表明VO-DP在各种变化条件下保持高度稳定。&lt;h4&gt;结论&lt;/h4&gt;提出的VO-DP方法在机器人操作任务中表现出色，特别是在真实世界任务中。研究团队还开源了一个支持多机器和多GPU并行训练的机器人操作训练库，兼容多种视觉运动策略。&lt;h4&gt;翻译&lt;/h4&gt;在模仿学习的背景下，基于视觉运动的扩散策略学习是机器人操作的主要方向之一。大多数方法依赖点云作为观察输入，通过点云特征学习构建场景表示，从而实现显著的准确性。然而，现有文献缺乏对具有显著潜力的纯视觉解决方案的深入探索。在本文中，我们提出了一种纯视觉且单视图的扩散策略学习方法(VO-DP)，利用预训练的视觉基础模型实现语义和几何特征的有效融合。我们利用VGGT的中间特征，结合来自DINOv2的语义特征和来自交替注意力块的几何特征。特征通过交叉注意力融合，并使用CNN进行空间压缩，形成策略头的输入。大量实验证明，VO-DP不仅显著优于纯视觉基线DP，而且与基于点云的方法DP3相比表现出明显的性能趋势：在仿真任务中，VO-DP的平均成功率为64.6%，与DP3的64.0%相当，远高于DP的34.8%；而在真实世界任务中，它达到87.9%，明显优于DP3的67.5%和DP的11.2%。进一步的鲁棒性评估证实，VO-DP在颜色、大小、背景和光照等变化条件下保持高度稳定。最后，我们开源了一个机器人操作训练库。该库基于Accelerate构建，支持多机器和多GPU并行训练以及混合精度训练。它与DP、DP3和VO-DP等视觉运动策略兼容，并支持RoboTwin模拟器。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的是如何仅使用RGB图像（vision-only）作为输入，实现与基于点云的方法相媲美的机器人操作性能。这个问题重要是因为当前主流机器人操作方法依赖昂贵的深度传感器（如深度相机或LiDAR），导致硬件成本高、系统复杂（需要多传感器校准），且在复杂场景中表现受限。相比之下，仅使用RGB图像的方法成本低得多、实用性高，但现有研究显示其性能通常不如基于点云的方法。提升vision-only方法的性能具有显著的实际应用价值，可以大幅降低机器人系统的部署成本和复杂度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前机器人操作领域的研究现状，指出vision-only方法成本低但性能不足，而基于点云的方法性能好但成本高。他们评估了现有方法，发现vision-only方法的瓶颈主要在于表示学习模块不够发达。作者认识到视觉基础模型（如VGGT、DINOv2）的潜力，这些模型可以直接从RGB图像中提取几何信息。因此，他们提出将语义特征和几何特征进行有效融合的思路，并在不依赖3D传感器的情况下获得丰富的场景理解。该方法借鉴了多个现有工作：使用预训练的VGGT模型作为视觉编码器，利用DINOv2进行语义特征提取，采用Alternating Attention网络进行几何特征提取，并借鉴了扩散模型（Diffusion Policy）的思想进行动作生成。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过预训练的视觉基础模型，从单视角RGB图像中同时提取语义和几何特征，并将这些特征自适应融合，从而在不依赖3D传感器的情况下实现高性能的机器人操作。整体实现流程包括四个主要步骤：1) 视觉编码：使用DINOv2提取语义特征，通过VGGT模型的Alternating Attention网络提取几何特征；2) 特征融合：使用残差交叉注意力机制将语义和几何特征融合，并通过前馈网络进一步处理；3) 场景表示压缩：使用轻量级ResNet对融合特征进行下采样和空间压缩，然后将压缩后的空间特征与本体感受信息连接，形成紧凑的场景表示；4) 动作生成：使用基于去噪扩散概率模型（DDPM）的策略头，以压缩后的场景表示为条件，通过迭代去噪过程预测动作轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次证明了仅使用RGB图像的vision-only方法可以达到与基于点云方法相媲美的性能水平；2) 设计了基于交叉注意力的特征融合模块，能够根据任务需求自适应地融合语义和几何特征；3) 通过空间特征压缩模块，从高维特征中提取关键信息，实现高效的单视角场景表示；4) 开源了DRRM训练库，支持多机多GPU并行训练和混合精度训练。相比之前的工作，VO-DP不再依赖点云或RGB-D等3D输入，仅使用RGB图像；利用预训练的视觉基础模型提取更丰富的特征；设计了专门的特征融合机制；在保持高性能的同时，显著降低了硬件成本和系统复杂度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VO-DP通过创新性地融合预训练视觉模型的语义和几何特征，首次实现了仅使用低成本RGB图像输入的机器人操作方法达到与基于昂贵3D传感器方法相媲美的性能，同时显著提升了在真实世界环境中的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the context of imitation learning, visuomotor-based diffusion policylearning is one of the main directions in robotic manipulation. Most of theseapproaches rely on point clouds as observation inputs and construct scenerepresentations through point clouds feature learning, which enables them toachieve remarkable accuracy. However, the existing literature lacks an in-depthexploration of vision-only solutions that have significant potential. In thispaper, we propose a Vision-Only and single-view Diffusion Policy learningmethod (VO-DP) that leverages pretrained visual foundation models to achieveeffective fusion of semantic and geometric features. We utilize intermediatefeatures from VGGT incorporating semantic features from DINOv2 and geometricfeatures from Alternating Attention blocks. Features are fused viacross-attention and spatially compressed with a CNN to form the input to thepolicy head. Extensive experiments demonstrate that VO-DP not only outperformsthe vision-only baseline DP significantly but also exhibits distinctperformance trends against the point cloud-based method DP3: in simulationtasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,outperforming both DP3 67.5% and DP 11.2% by a notable margin. Furtherrobustness evaluations confirm that VO-DP remains highly stable under varyingconditions including color, size, background, and lighting. Lastly, weopen-source a training library for robotic manipulation. Built on Accelerate,this library supports multi-machine and multi-GPU parallel training, as well asmixed precision training. It is compatible with visuomotor policies such as DP,DP3 and VO-DP, and also supports the RoboTwin simulator.</description>
      <author>example@mail.com (Zehao Ni, Yonghao He, Lingfeng Qian, Jilei Mao, Fa Fu, Wei Sui, Hu Su, Junran Peng, Zhipeng Wang, Bin He)</author>
      <guid isPermaLink="false">2510.15530v3</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>On Optimal Hyperparameters for Differentially Private Deep Transfer Learning</title>
      <link>http://arxiv.org/abs/2510.20616v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 30 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了差分隐私迁移学习中的关键超参数选择问题，特别关注裁剪边界C和批量大小B，揭示了理论理解与实际结果之间的不匹配，并提出改进方法。&lt;h4&gt;背景&lt;/h4&gt;差分隐私迁移学习（即在私有数据上微调预训练模型）是当前在隐私约束下训练大型模型的最先进方法。然而，在关键超参数选择方面存在理论与实践的脱节。&lt;h4&gt;目的&lt;/h4&gt;研究差分隐私迁移学习中两个关键超参数（裁剪边界C和批量大小B）的最优选择方法，解决当前理论理解与实证结果之间的不匹配问题。&lt;h4&gt;方法&lt;/h4&gt;分析裁剪边界C和批量大小B对模型性能的影响，考察梯度分布的变化，在固定计算预算（固定周期）下评估现有启发式方法，分析累积DP噪声对批量大小选择的影响，研究跨任务使用单一(C,B)设置的效果，并分析裁剪作为梯度重加权形式和累积DP噪声的作用。&lt;h4&gt;主要发现&lt;/h4&gt;1. 当前关于如何选择最优C的理论理解（更强的隐私需要更小的C）与实证结果（更强的隐私下更大的C表现更好）之间存在明显不匹配，这是由梯度分布变化引起的。2. 在计算预算有限的情况下，现有的调整批量大小B的启发式方法不适用，而累积DP噪声能更好地解释较小或较大批量的表现差异。3. 跨任务使用单一(C,B)设置会导致次优性能，特别是在从宽松隐私转向严格隐私以及从充足计算转向有限计算的情况下。&lt;h4&gt;结论&lt;/h4&gt;差分隐私迁移学习中的超参数选择需要考虑梯度分布变化和累积DP噪声的影响，不应简单地跨任务应用相同的超参数设置，而应根据隐私需求和计算资源进行针对性调整。&lt;h4&gt;翻译&lt;/h4&gt;差分隐私（DP）迁移学习，即在私有数据上微调预训练模型，是当前在隐私约束下训练大型模型的最先进方法。我们关注此设置中的两个关键超参数：裁剪边界C和批量大小B。我们展示了当前关于如何选择最优C的理论理解（更强的隐私需要更小的C）与实证结果（更强的隐私下更大的C表现更好）之间的明显不匹配，这是由梯度分布变化引起的。假设计算预算有限（固定周期），我们证明了现有的调整B的启发式方法不适用，而累积DP噪声能更好地解释较小或较大批量的表现差异。我们还强调了跨任务使用单一(C,B)设置的常见做法可能导致次优性能。我们发现，当在宽松与严格隐私之间转换以及在充足与有限计算之间转换时，性能下降特别明显，我们通过分析裁剪作为梯度重加权形式和检查累积DP噪声来解释这一现象。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Differentially private (DP) transfer learning, i.e., fine-tuning a pretrainedmodel on private data, is the current state-of-the-art approach for traininglarge models under privacy constraints. We focus on two key hyperparameters inthis setting: the clipping bound $C$ and batch size $B$. We show a clearmismatch between the current theoretical understanding of how to choose anoptimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes(larger $C$ performs better under strong privacy), caused by changes in thegradient distributions. Assuming a limited compute budget (fixed epochs), wedemonstrate that the existing heuristics for tuning $B$ do not work, whilecumulative DP noise better explains whether smaller or larger batches performbetter. We also highlight how the common practice of using a single $(C,B)$setting across tasks can lead to suboptimal performance. We find thatperformance drops especially when moving between loose and tight privacy andbetween plentiful and limited compute, which we explain by analyzing clippingas a form of gradient re-weighting and examining cumulative DP noise.</description>
      <author>example@mail.com (Aki Rehn, Linzh Zhao, Mikko A. Heikkilä, Antti Honkela)</author>
      <guid isPermaLink="false">2510.20616v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Reliable and Reproducible Demographic Inference for Fairness in Face Analysis</title>
      <link>http://arxiv.org/abs/2510.20482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个模块化迁移学习方法的人口统计属性推断流水线，以提高面部分析系统公平性评估的可靠性。该方法通过整合预训练的人脸识别编码器与非线性分类头，在性别和种族推断任务上超越了基线方法，特别是在更具挑战性的种族属性上表现优异。研究还引入了通过身份一致性定义的鲁棒性指标，适用于任何人口统计分割方案。&lt;h4&gt;背景&lt;/h4&gt;面部分析系统中的公平性评估通常依赖于自动人口统计属性推断，而人口统计属性推断又依赖于预定义的人口统计分割。公平性审计的有效性取决于DAI过程的可靠性，但这一问题在以往研究中未得到充分重视。&lt;h4&gt;目的&lt;/h4&gt;提高DAI的可靠性，从而获得更少偏差和更低方差的面部分析系统公平性估计；提出一个完全可复现的DAI流水线；为公平审计中的人口统计推断提供可靠基础。&lt;h4&gt;方法&lt;/h4&gt;用模块化迁移学习方法替代传统的端到端训练；整合预训练的人脸识别编码器与非线性分类头；从准确性、公平性和新引入的鲁棒性（通过身份一致性定义）三个维度评估流水线；在多个数据集和训练设置上对性别和种族推断进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;提出的模块化迁移学习方法在性别和种族推断上优于强大的基线方法；在更具挑战性的种族属性上表现尤其出色；新引入的鲁棒性指标适用于任何人口统计分割方案。&lt;h4&gt;结论&lt;/h4&gt;该工作为公平审计中的人口统计推断提供了可靠的基础；通过公开训练数据集元数据、完整代码库、预训练模型和评估工具包，促进了研究的透明度和可复现性。&lt;h4&gt;翻译&lt;/h4&gt;面部分析系统中的公平性评估通常依赖于自动人口统计属性推断，而人口统计属性推断本身又依赖于预定义的人口统计分割。然而，公平性审计的有效性取决于DAI过程的可靠性。我们首先提供了这种依赖关系的理论动机，表明提高DAI可靠性可以减少偏差并降低面部分析系统公平性估计的方差。为此，我们提出了一个完全可复现的DAI流水线，用模块化迁移学习方法替代传统的端到端训练。我们的设计整合了预训练的人脸识别编码器与非线性分类头。我们从三个维度评估了这个流水线：准确性、公平性，以及通过身份一致性定义的新引入的鲁棒性概念。所提出的鲁棒性指标适用于任何人口统计分割方案。我们在多个数据集和训练设置上对性别和种族推断进行了基准测试。我们的结果表明，所提出的方法优于强大的基线方法，特别是在更具挑战性的种族属性上。为了促进透明度和可复现性，我们将公开训练数据集元数据、完整代码库、预训练模型和评估工具包。这项工作为公平审计中的人口统计推断提供了可靠的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fairness evaluation in face analysis systems (FAS) typically depends onautomatic demographic attribute inference (DAI), which itself relies onpredefined demographic segmentation. However, the validity of fairness auditinghinges on the reliability of the DAI process. We begin by providing atheoretical motivation for this dependency, showing that improved DAIreliability leads to less biased and lower-variance estimates of FAS fairness.To address this, we propose a fully reproducible DAI pipeline that replacesconventional end-to-end training with a modular transfer learning approach. Ourdesign integrates pretrained face recognition encoders with non-linearclassification heads. We audit this pipeline across three dimensions: accuracy,fairness, and a newly introduced notion of robustness, defined viaintra-identity consistency. The proposed robustness metric is applicable to anydemographic segmentation scheme. We benchmark the pipeline on gender andethnicity inference across multiple datasets and training setups. Our resultsshow that the proposed method outperforms strong baselines, particularly onethnicity, which is the more challenging attribute. To promote transparency andreproducibility, we will publicly release the training dataset metadata, fullcodebase, pretrained models, and evaluation toolkit. This work contributes areliable foundation for demographic inference in fairness auditing.</description>
      <author>example@mail.com (Alexandre Fournier-Montgieux, Hervé Le Borgne, Adrian Popescu, Bertrand Luvison)</author>
      <guid isPermaLink="false">2510.20482v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Machine learning identification of fractional-order vortex beam diffraction process</title>
      <link>http://arxiv.org/abs/2510.20245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于ResNet的深度学习方法，用于在衍射条件下准确识别分数阶涡旋光束的传播距离和拓扑荷，考虑了大气湍流的影响，实现了高精度的识别。&lt;h4&gt;背景&lt;/h4&gt;分数阶涡旋光束具有分数阶轨道角动量(FOAM)模式，理论上可以无限增加传输容量，在测量、光通信和微粒操纵等领域有重要应用前景。然而，当分数阶涡旋光束在自由空间传播时，其螺旋相位的连续性使其在实际应用中容易受到衍射的影响，从而影响OAM模式识别的准确性，严重限制了基于FOAM的光通信的使用。&lt;h4&gt;目的&lt;/h4&gt;实现衍射条件下分数阶涡旋光束的机器学习识别，解决目前尚未报道的紧急问题。&lt;h4&gt;方法&lt;/h4&gt;基于ResNet，提出了一种深度学习方法。利用实验测量和数值模拟的强度分布，创建了大气湍流环境中涡旋光束衍射强度模式的数据集。采用基于迁移学习的改进101层ResNet结构，实现不同传播距离下FOAM模型的准确高效识别。&lt;h4&gt;主要发现&lt;/h4&gt;该方法可以在湍流条件下准确识别传播距离为100厘米、间距为5厘米、模式间距为0.1的FOAM模式，准确率达到99.69%。该方法考虑了空间传输过程中大气湍流的影响，使得识别方案即使在特殊环境中也能实现高精度。它具有区分超细FOAM模式和传播距离的能力，这是传统方法无法实现的。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法解决了分数阶涡旋光束在衍射条件下识别的难题，特别是在考虑大气湍流影响的情况下，实现了高精度的FOAM模式识别，为实际应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;分数阶涡旋光束具有分数阶轨道角动量(FOAM)模式，理论上具有无限增加传输容量的潜力。因此，它们在测量、光通信和微粒操纵领域具有重要的应用前景。然而，当分数阶涡旋光束在自由空间传播时，螺旋相位的连续性使它们在实际应用中容易受到衍射的影响，从而影响OAM模式识别的准确性，严重限制了基于FOAM的光通信的使用。实现衍射条件下分数阶涡旋光束的机器学习识别目前是一个紧迫且尚未报道的问题。在本工作中，基于ResNet，提出了一种深度学习(DL)方法，用于准确识别分数阶涡旋光束衍射过程中的传播距离和拓扑荷。利用实验测量和数值模拟的强度分布，创建了大气湍流环境中涡旋光束衍射强度模式的数据集。采用基于迁移学习的改进101层ResNet结构，实现不同传播距离下FOAM模型的准确高效识别。实验结果表明，所提出的方法可以在湍流条件下准确识别传播距离为100厘米、间距为5厘米、模式间距为0.1的FOAM模式，准确率为99.69%。该方法考虑了空间传输过程中大气湍流的影响，使得识别方案即使在特殊环境中也能实现高精度。它具有区分超细FOAM模式和传播距离的能力，这是传统方法无法实现的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.7498/aps.74.20241458&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fractional-order vortex beams possess fractional orbital angular momentum(FOAM) modes, which theoretically have the potential to increase transmissioncapacity infinitely. Therefore, they have significant application prospects inthe fields of measurement, optical communication and micro-particlemanipulation. However, when fractional-order vortex beams propagate in freespace, the discontinuity of the helical phase makes them susceptible todiffraction in practical applications, thereby affecting the accuracy of OAMmode recognition and severely limiting the use of FOAM-based opticalcommunication. Achieving machine learning recognition of fractional-ordervortex beams under diffraction conditions is currently an urgent and unreportedissue. Based on ResNet, a deep learning (DL) method of accurately recognizingthe propagation distance and topological charge of fractional-order vortex beamdiffraction process is proposed in this work. Utilizing both experimentallymeasured and numerically simulated intensity distributions, a dataset of vortexbeam diffraction intensity patterns in atmospheric turbulence environments iscreated. An improved 101-layer ResNet structure based on transfer learning isemployed to achieve accurate and efficient recognition of the FOAM model atdifferent propagation distances. Experimental results show that the proposedmethod can accurately recognize FOAM modes with a propagation distance of 100cm, a spacing of 5 cm, and a mode spacing of 0.1 under turbulent conditions,with an accuracy of 99.69%. This method considers the effect of atmosphericturbulence during spatial transmission, allowing the recognition scheme toachieve high accuracy even in special environments. It has the ability todistinguish ultra-fine FOAM modes and propagation distances, which cannot beachieved by traditional methods.</description>
      <author>example@mail.com (Yan Guo, Heng Lyu, Chunling Ding, Chenzhi Yuan, Ruibo Jin)</author>
      <guid isPermaLink="false">2510.20245v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models</title>
      <link>http://arxiv.org/abs/2510.20033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇博士论文提出了三种改进序列标注任务迁移学习的方法，通过调整预训练神经语言模型来提高性能&lt;h4&gt;背景&lt;/h4&gt;序列标注任务的迁移学习需要更有效的预训练语言模型适应方法&lt;h4&gt;目的&lt;/h4&gt;改进序列标注任务的迁移学习方法，使预训练神经语言模型能够更好地适应特定任务&lt;h4&gt;方法&lt;/h4&gt;提出了三种改进方法：1) 引入额外信号的多任务模型；2) 修改自回归大语言模型架构以实现层间双向信息流；3) 利用监督上下文微调结合响应导向适应策略的序列标注框架&lt;h4&gt;主要发现&lt;/h4&gt;预训练神经语言模型在有针对性的迁移学习范式下，在序列标注任务上能够达到最佳性能&lt;h4&gt;结论&lt;/h4&gt;通过有针对性的迁移学习范式调整预训练神经语言模型，可以显著提高其在序列标注任务上的性能&lt;h4&gt;翻译&lt;/h4&gt;这篇博士论文通过调整预训练的神经语言模型，改进了序列标注任务的迁移学习。所提出的迁移学习改进包括引入一个额外信号的多任务模型、基于自回归大语言模型架构修改的方法，以及利用监督上下文微调结合响应导向适应策略的自回归大语言模型序列标注框架。第一个改进是在事件触发检测任务的领域迁移背景下提出的，通过将独立于领域的文本处理系统获得的额外信号整合到多任务模型中来改进领域迁移。第二个改进涉及修改模型架构，为此提出了一个方法，使自回归大语言模型的层之间能够实现双向信息流。第三个改进利用自回归大语言模型作为文本生成器，通过生成式监督上下文微调框架实现。所提出的模型、方法和框架表明，当通过有针对性的迁移学习范式进行调整时，预训练的神经语言模型在序列标注任务上能够达到最佳性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This doctoral thesis improves the transfer learning for sequence labelingtasks by adapting pre-trained neural language models. The proposed improvementsin transfer learning involve introducing a multi-task model that incorporatesan additional signal, a method based on architectural modifications inautoregressive large language models, and a sequence labeling framework forautoregressive large language models utilizing supervised in-contextfine-tuning combined with response-oriented adaptation strategies. The firstimprovement is given in the context of domain transfer for the event triggerdetection task. The domain transfer of the event trigger detection task can beimproved by incorporating an additional signal obtained from adomain-independent text processing system into a multi-task model. The secondimprovement involves modifying the model's architecture. For that purpose, amethod is proposed to enable bidirectional information flow across layers ofautoregressive large language models. The third improvement utilizesautoregressive large language models as text generators through a generativesupervised in-context fine-tuning framework. The proposed model, method, andframework demonstrate that pre-trained neural language models achieve theirbest performance on sequence labeling tasks when adapted through targetedtransfer learning paradigms.</description>
      <author>example@mail.com (David Dukić)</author>
      <guid isPermaLink="false">2510.20033v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning</title>
      <link>http://arxiv.org/abs/2510.13307v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于因果表示和推理的联合学习方法，用于解决点云分割中的新类别发现问题&lt;h4&gt;背景&lt;/h4&gt;在点云分割任务中，需要学习一个模型，仅使用已标记的基础类别监督信息，来分割未标记的新类别点云&lt;h4&gt;目的&lt;/h4&gt;建立点表示与基础类别标签之间的精确相关性，以及基础类别与新类别点之间的表示相关性&lt;h4&gt;方法&lt;/h4&gt;引入结构因果模型(SCM)重新形式化3D-NCD问题，分析基础类别表示中的隐藏混杂因素，设计消除混杂因素的因果表示原型，并使用图结构建模基础类别与新类别之间的因果关系&lt;h4&gt;主要发现&lt;/h4&gt;粗略或统计相关性学习可能导致新类别推理的混淆，而施加因果关系作为强相关约束可以准确揭示对应类别的本质点云表示&lt;h4&gt;结论&lt;/h4&gt;在3D和2D NCD语义分割任务上的大量实验和可视化结果证明了该方法的优势&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们专注于点云分割的新类别发现(3D-NCD)，旨在学习一个模型，仅使用已标记的基础3D类别的监督信息，来分割未标记的新3D类别。此任务的关键在于建立点表示与其基础类别标签之间的精确相关性，以及基础类别与新类别点之间的表示相关性。粗略或统计相关性学习可能导致新类别推理的混淆。如果我们将因果关系作为强相关约束强加于学习过程，应该能够准确揭示对应于类别的本质点云表示。为此，我们引入结构因果模型(SCM)重新形式化3D-NCD问题，并提出一种新方法，即因果表示和推理的联合学习。具体而言，我们首先通过SCM分析基础类别表示中的隐藏混杂因素以及基础类别与新类别之间的因果关系。我们设计了一个消除混杂因素的因果表示原型，以捕获基础类别的因果表示。然后使用图结构建模基础类别的因果表示原型与新类别原型之间的因果关系，实现从基础类别到新类别的因果推理。在3D和2D NCD语义分割上的大量实验和可视化结果证明了我们方法的优势&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决点云分割中的新类别发现问题，即如何仅使用已标记的基础类别数据来分割未标记的新类别。这个问题在自动驾驶、机器人感知等实际应用中非常重要，因为这些场景中环境是动态开放的，可能会突然出现新的物体类别，而传统方法无法处理这些未预先定义的类别，限制了系统在真实世界中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法的局限性：它们本质上是统计模型，倾向于学习'捷径特征'而非本质特征，且难以处理新类别。因此，作者引入结构因果模型(SCM)重新形式化问题，识别出混杂因素对基础类别学习的干扰，以及基础到新类别的因果路径。方法设计借鉴了点云分割领域常用的MinkowskiUNet架构、因果学习理论中的独立因果机制原则、对抗训练思想以及图神经网络技术，将它们创新性地结合来解决3D-NCD问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过因果表示学习识别和去除点云中的非因果特征(混杂因素)，学习到能准确对应类别的本质表示，并利用基础类别的因果表示通过图结构建模基础到新类别的因果关系，实现因果推理。整体流程包括：1)因果表示原型学习，通过对抗训练去除混杂因素，提取基础类别的因果表示并生成原型；2)构建因果推理图，使用自注意力机制调整边权重，应用因果剪枝和推理方向一致性约束；3)基于图卷积网络生成高质量伪标签，实现新类别的分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三：1)首次将因果学习引入3D-NCD领域；2)提出因果表示原型学习方法，通过对抗机制消除混杂因素；3)设计基于图的因果推理框架，显式建模类别间因果关系。相比之前工作，不同之处在于：传统方法依赖统计相似性且易受捷径特征干扰，而本文方法通过因果学习提取本质特征；传统方法直接测量类别相似性，而本文使用图结构建模复杂的高阶依赖关系；实验表明本文方法在多个数据集上显著优于现有方法，特别是在新类别分割任务上。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种结合因果表示学习和推理的新方法，通过识别和去除点云中的非因果特征并建模基础到新类别的因果关系，显著提升了点云分割中新类别的发现和分割能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation(3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classesusing only the supervision from labeled (base) 3D classes. The key to this taskis to setup the exact correlations between the point representations and theirbase class labels, as well as the representation correlations between thepoints from base and novel classes. A coarse or statistical correlationlearning may lead to the confusion in novel class inference. lf we impose acausal relationship as a strong correlated constraint upon the learningprocess, the essential point cloud representations that accurately correspondto the classes should be uncovered. To this end, we introduce a structuralcausal model (SCM) to re-formalize the 3D-NCD problem and propose a new method,i.e., Joint Learning of Causal Representation and Reasoning. Specifically, wefirst analyze hidden confounders in the base class representations and thecausal relationships between the base and novel classes through SCM. We devisea causal representation prototype that eliminates confounders to capture thecausal representations of base classes. A graph structure is then used to modelthe causal relationships between the base classes' causal representationprototypes and the novel class prototypes, enabling causal reasoning from baseto novel classes. Extensive experiments and visualization results on 3D and 2DNCD semantic segmentation demonstrate the superiorities of our method.</description>
      <author>example@mail.com (Yang Li, Aming Wu, Zihao Zhang, Yahong Han)</author>
      <guid isPermaLink="false">2510.13307v2</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process</title>
      <link>http://arxiv.org/abs/2510.20736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeruIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于狄利克雷过程(DP)的多模态学习框架，通过DP的'富者愈富'特性自动实现显著的模态内表示学习和跨模态对齐之间的最佳平衡，有效解决了多模态融合中保持特征表达能力和学习跨模态交互的挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态融合在医疗保健和金融等现实世界场景中变得越来越重要，关键挑战是如何在保持每个模态特征表达能力的同时学习跨模态交互。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的多模态学习方法，避免过度强调模态边缘分布对齐带来的问题，同时保持每个模态内的有意义表示。&lt;h4&gt;方法&lt;/h4&gt;提出DP驱动的多模态学习框架，假设每个模态遵循多元高斯分布的混合，并采用狄利克雷过程计算所有组件的混合权重，利用其'富者愈富'特性动态分配特征贡献并选择最突出的特征。&lt;h4&gt;主要发现&lt;/h4&gt;在多个多模态数据集上的实验表明，该模型优于其他竞争方法；消融分析验证了DP在模态分布对齐中的有效性及其对关键超参数变化的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;DP驱动的多模态学习框架能够自动实现显著的模态内表示学习和跨模态对齐之间的最佳平衡，有效解决了多模态融合中的关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;开发有效的多模态融合方法在医疗保健和金融等现实世界场景中变得越来越重要。关键挑战是如何在保持每个模态特征表达能力的同时学习跨模态交互。先前的方法主要关注跨模态对齐，但过度强调模态边缘分布的对齐可能会施加过度的正则化，并阻碍每个模态内的有意义表示。狄利克雷过程(DP)混合模型是一种强大的贝叶斯非参数方法，可以通过其'富者愈富'特性放大最突出的特征，为它们分配不断增加的权重。受DP这一独特特性的启发，我们提出了一种新的DP驱动的多模态学习框架，自动实现显著的模态内表示学习和跨模态对齐之间的最佳平衡。具体而言，我们假设每个模态遵循多元高斯分布的混合，并进一步采用DP计算所有组件的混合权重。这种范式允许DP动态分配特征的贡献并选择最突出的特征，利用其'富者愈富'特性，从而促进多模态特征融合。在多个多模态数据集上的广泛实验证明了我们的模型优于其他竞争模型。消融分析进一步验证了DP在模态分布对齐中的有效性及其对关键超参数变化的鲁棒性。代码已在https://github.com/HKU-MedAI/DPMM.git匿名提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing effective multimodal fusion approaches has become increasinglyessential in many real-world scenarios, such as health care and finance. Thekey challenge is how to preserve the feature expressiveness in each modalitywhile learning cross-modal interactions. Previous approaches primarily focus onthe cross-modal alignment, while over-emphasis on the alignment of marginaldistributions of modalities may impose excess regularization and obstructmeaningful representations within each modality. The Dirichlet process (DP)mixture model is a powerful Bayesian non-parametric method that can amplify themost prominent features by its richer-gets-richer property, which allocatesincreasing weights to them. Inspired by this unique characteristic of DP, wepropose a new DP-driven multimodal learning framework that automaticallyachieves an optimal balance between prominent intra-modal representationlearning and cross-modal alignment. Specifically, we assume that each modalityfollows a mixture of multivariate Gaussian distributions and further adopt DPto calculate the mixture weights for all the components. This paradigm allowsDP to dynamically allocate the contributions of features and select the mostprominent ones, leveraging its richer-gets-richer property, thus facilitatingmultimodal feature fusion. Extensive experiments on several multimodal datasetsdemonstrate the superior performance of our model over other competitors.Ablation analysis further validates the effectiveness of DP in aligningmodality distributions and its robustness to changes in key hyperparameters.Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git</description>
      <author>example@mail.com (Tsai Hor Chan, Feng Wu, Yihang Chen, Guosheng Yin, Lequan Yu)</author>
      <guid isPermaLink="false">2510.20736v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>From Masks to Worlds: A Hitchhiker's Guide to World Models</title>
      <link>http://arxiv.org/abs/2510.20668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Github: https://github.com/M-E-AGI-Lab/Awesome-World-Models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文不是典型的世界模型综述，而是面向世界构建者的指南，聚焦于从早期掩码模型到记忆增强系统的世界模型发展路径&lt;h4&gt;背景&lt;/h4&gt;现有关于世界模型的文献分散且缺乏系统性，许多论文仅提及'世界模型'概念但未深入探讨&lt;h4&gt;目的&lt;/h4&gt;提供一条清晰的世界模型发展路径，专注于核心组件而非列举所有相关研究&lt;h4&gt;方法&lt;/h4&gt;遵循从跨模态表示学习的掩码模型，到统一架构，再到交互式生成模型，最后到记忆增强系统的发展脉络&lt;h4&gt;主要发现&lt;/h4&gt;世界模型的核心在于三个关键组件：生成核心、交互循环和记忆系统&lt;h4&gt;结论&lt;/h4&gt;通过关注这三个核心组件构建的系统是实现真正世界模型的最有前途路径&lt;h4&gt;翻译&lt;/h4&gt;这不是一篇典型的世界模型综述；这是一份面向那些想要构建世界的人的指南。我们的目标不是罗列所有曾经提及'世界模型'的论文。相反，我们遵循一条清晰的道路：从早期跨模态统一表示学习的掩码模型，到共享单一范式的统一架构，再到闭合动作-感知循环的交互式生成模型，最后到随时间保持一致世界的记忆增强系统。我们绕过了松散相关的分支，专注于核心：生成核心、交互循环和记忆系统。我们表明这是实现真正世界模型的最有前途的路径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何构建真正的世界模型的问题。尽管有数百篇相关论文，但对于如何实际构建一个真正的世界模型还没有明确共识。这个问题很重要，因为真正的世界模型可以模拟整个环境，用于强化学习、智能体规划、大型语言模型模拟社会等多个领域。它能从预测引擎转变为'活的世界'，具有持久性、代理性和涌现性，对理解复杂系统、进行科学实验和创造交互体验具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析世界模型的历史发展，提出了一条'狭窄的道路'，将世界模型发展分为五个阶段：基于掩码的模型、统一模型、交互式生成模型、记忆与一致性系统，以及真正的世界模型。论文大量借鉴了现有工作，每个阶段都列举了代表性模型和方法，如BERT、MAE等（第一阶段），EMU3、Chameleon等（第二阶段），Genie系列等（第三阶段），RETRO、MemGPT等（第四阶段）。作者通过分析这些工作的演进，提出了构建真正世界模型的路径。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：真正的世界模型不是单一实体，而是由三个核心子系统合成的系统：生成核心（产生世界状态）、交互循环（实时关闭行动-感知循环）和持久记忆系统（随时间维持一致的世界）。整体实现流程遵循五个阶段：首先建立基于掩码的模型，为跨模态表示学习提供通用范式；然后统一架构，使单一架构能处理和生成多种模态；接着引入交互循环，将静态生成器转变为实时模拟器；然后添加记忆系统，使模拟能随时间持续；最后将这些组件合成为自主整体，实现真正的世界模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）提出了清晰的世界模型五阶段发展路线图；2）定义了真正世界模型的三个核心子系统；3）指出了构建真正世界模型的三个基本挑战：一致性问题、压缩问题和对齐问题；4）提出了真正世界模型的三个定义属性：持久性、代理性和涌现性。与之前工作相比，不同之处在于：它不是简单罗列论文，而是提供清晰发展路径；不仅关注技术细节，还关注哲学意义和潜在影响；将世界模型视为综合系统而非孤立组件；提出了构建真正世界模型的具体挑战和未来方向。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提供了一个从基于掩码的模型到真正世界模型的五阶段发展路线图，定义了真正世界模型的三个核心子系统和三个关键属性，并指出了构建真正世界模型的三个基本挑战，为构建能够持久、交互和涌现的'活的世界'提供了清晰的指南。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This is not a typical survey of world models; it is a guide for those whowant to build worlds. We do not aim to catalog every paper that has evermentioned a ``world model". Instead, we follow one clear road: from earlymasked models that unified representation learning across modalities, tounified architectures that share a single paradigm, then to interactivegenerative models that close the action-perception loop, and finally tomemory-augmented systems that sustain consistent worlds over time. We bypassloosely related branches to focus on the core: the generative heart, theinteractive loop, and the memory system. We show that this is the mostpromising path towards true world models.</description>
      <author>example@mail.com (Jinbin Bai, Yu Lei, Hecong Wu, Yuchen Zhu, Shufan Li, Yi Xin, Xiangtai Li, Molei Tao, Aditya Grover, Ming-Hsuan Yang)</author>
      <guid isPermaLink="false">2510.20668v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Connecting Jensen-Shannon and Kullback-Leibler Divergences: A New Bound for Representation Learning</title>
      <link>http://arxiv.org/abs/2510.20644v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025. Code available at  https://github.com/ReubenDo/JSDlowerbound/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了互信息(MI)与Jensen-Shannon散度(JSD)之间的关系，通过推导一个新的紧密且可处理的Kullback-Leibler散度(KLD)下界作为JSD的函数，建立了两者之间的理论联系。研究证明最大化基于JSD的信息会增加对互信息的保证下界，并通过实验验证了该方法的有效性和实用性，为在基于MI的表示学习中使用判别学习提供了理论依据和实证支持。&lt;h4&gt;背景&lt;/h4&gt;互信息(MI)是表示学习中广泛使用的基本统计依赖性度量。然而，直接通过其定义为Kullback-Leibler散度(KLD)来优化MI通常是不可行的。因此，最近的方法转而最大化替代的依赖性度量，特别是Jensen-Shannon散度(JSD)作为判别损失。但这些替代目标与MI之间的联系尚未被充分理解。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在填补替代目标(特别是基于JSD的目标)与互信息之间理论理解的空白，通过建立它们之间的严格数学关系，为在表示学习中使用判别学习提供理论依据。&lt;h4&gt;方法&lt;/h4&gt;研究通过推导一个新的、紧密且可处理的KLD下界作为JSD的函数来建立MI与JSD之间的理论联系。通过将这一边界专门应用于联合分布和边缘分布，证明了最大化基于JSD的信息会增加对互信息的保证下界。此外，研究重新审视了基于JSD目标的实际实现，并观察到最小化二元分类器的交叉熵损失可以恢复JSD的已知变分下界。&lt;h4&gt;主要发现&lt;/h4&gt;1. 推导出了一个新的、紧密且可处理的KLD下界作为JSD的函数；2. 证明了最大化基于JSD的信息会增加对互信息的保证下界；3. 最小化区分联合分布与边缘分布对的二元分类器的交叉熵损失可以恢复JSD的已知变分下界；4. 实验表明该下界应用于MI估计时是紧密的；5. 与最先进的神经变分下界估计器相比，该下界估计器提供了稳定的低方差估计；6. 在信息瓶颈框架中展示了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;研究的结果为在基于互信息的表示学习中使用判别学习提供了新的理论依据和强有力的实证证据。所提出的下界估计器能够提供对互信息的稳定、低方差估计，并且在信息瓶颈框架中具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;互信息(MI)是表示学习中广泛使用的基本统计依赖性度量。虽然直接通过其定义为Kullback-Leibler散度(KLD)来优化MI通常是不可行的，但最近的方法转而最大化替代的依赖性度量，特别是通过判别损失来最大化联合分布与边缘分布乘积之间的Jensen-Shannon散度(JSD)。然而，这些替代目标与MI之间的联系尚未被充分理解。在本工作中，我们通过推导一个新的、紧密且可处理的KLD下界作为JSD的函数来填补这一空白。通过将这一边界专门应用于联合分布和边缘分布，我们证明了最大化基于JSD的信息会增加对互信息的保证下界。此外，我们重新审视了基于JSD目标的实际实现，并观察到最小化训练以区分联合分布与边缘分布对的二元分类器的交叉熵损失可以恢复JSD的已知变分下界。广泛的实验表明该下界应用于MI估计时是紧密的。我们将该下界与一系列既定参考场景中最先进的神经变分下界估计器进行了比较。我们的下界估计器一致地提供了对互信息紧密下界的稳定、低方差估计。我们还展示了其在信息瓶颈框架中的实际实用性。综上所述，我们的结果为在基于MI的表示学习中使用判别学习提供了新的理论依据和强有力的实证证据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mutual Information (MI) is a fundamental measure of statistical dependencewidely used in representation learning. While direct optimization of MI via itsdefinition as a Kullback-Leibler divergence (KLD) is often intractable, manyrecent methods have instead maximized alternative dependence measures, mostnotably, the Jensen-Shannon divergence (JSD) between joint and product ofmarginal distributions via discriminative losses. However, the connectionbetween these surrogate objectives and MI remains poorly understood. In thiswork, we bridge this gap by deriving a new, tight, and tractable lower bound onKLD as a function of JSD in the general case. By specializing this bound tojoint and marginal distributions, we demonstrate that maximizing the JSD-basedinformation increases a guaranteed lower bound on mutual information.Furthermore, we revisit the practical implementation of JSD-based objectivesand observe that minimizing the cross-entropy loss of a binary classifiertrained to distinguish joint from marginal pairs recovers a known variationallower bound on the JSD. Extensive experiments demonstrate that our lower boundis tight when applied to MI estimation. We compared our lower bound tostate-of-the-art neural estimators of variational lower bound across a range ofestablished reference scenarios. Our lower bound estimator consistentlyprovides a stable, low-variance estimate of a tight lower bound on MI. We alsodemonstrate its practical usefulness in the context of the InformationBottleneck framework. Taken together, our results provide new theoreticaljustifications and strong empirical evidence for using discriminative learningin MI-based representation learning.</description>
      <author>example@mail.com (Reuben Dorent, Polina Golland, William Wells III)</author>
      <guid isPermaLink="false">2510.20644v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion Autoencoders with Perceivers for Long, Irregular and Multimodal Astronomical Sequences</title>
      <link>http://arxiv.org/abs/2510.20595v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为Diffusion Autoencoder with Perceivers (daep)的新架构，用于处理科学领域中不规则、多模态序列数据。该架构通过将异构测量值标记化，使用Perceiver编码器压缩，并使用Perceiver-IO扩散解码器重建，实现了在不同数据设置中的可扩展学习。研究还建立了maep作为基线模型，实验表明daep在重建误差、判别性潜在空间保存和精细结构保留方面均优于VAE和maep基线模型。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已成为表征学习的中心策略，但大多数用于编码数据的架构仅在规则采样的输入（如图像、音频和视频）上得到验证。然而，在许多科学领域中，数据是以长序列、不规则和多模态的形式出现的。&lt;h4&gt;目的&lt;/h4&gt;为了从这些不规则、多模态序列数据中提取语义信息，作者提出了daep架构，并建立了一个强基线模型maep，以评估daep的性能。&lt;h4&gt;方法&lt;/h4&gt;daep架构通过以下步骤工作：将异构测量值标记化，使用Perceiver编码器进行压缩，使用Perceiver-IO扩散解码器进行重建。为了评估daep，作者将掩码自编码器调整为Perceiver编码器/解码器设计，建立了maep基线模型。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的光谱和光度天文数据集上，daep实现了比VAE和maep基线模型更低的重建误差，产生了更具判别性的潜在空间，并更好地保留了精细结构。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明daep是处理科学领域中不规则、异构序列数据的有效框架。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习已成为表征学习的中心策略，但大多数用于编码数据的架构仅在规则采样的输入（如图像、音频和视频）上得到验证。在许多科学领域中，数据实际上是以长序列、不规则和多模态的形式出现的。为了从这些数据中提取语义信息，我们引入了带有Perceiver的扩散自编码器（daep）。daep将异构测量值标记化，使用Perceiver编码器压缩它们，并使用Perceiver-IO扩散解码器重建它们，从而能够在各种数据设置中实现可扩展学习。为了对daep架构进行基准测试，我们将掩码自编码器调整为Perceiver编码器/解码器设计，并在与daep相同的架构家族中建立了强基线（maep）。在多样化的光谱和光度天文数据集上，daep实现了比VAE和maep基线模型更低的重建误差，产生更具判别性的潜在空间，并更好地保留了精细结构。这些结果确立了daep作为科学领域中数据以不规则、异构序列形式出现的有效框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning has become a central strategy for representationlearning, but the majority of architectures used for encoding data have onlybeen validated on regularly-sampled inputs such as images, audios. and videos.In many scientific domains, data instead arrive as long, irregular, andmultimodal sequences. To extract semantic information from these data, weintroduce the Diffusion Autoencoder with Perceivers (daep). daep tokenizesheterogeneous measurements, compresses them with a Perceiver encoder, andreconstructs them with a Perceiver-IO diffusion decoder, enabling scalablelearning in diverse data settings. To benchmark the daep architecture, we adaptthe masked autoencoder to a Perceiver encoder/decoder design, and establish astrong baseline (maep) in the same architectural family as daep. Across diversespectroscopic and photometric astronomical datasets, daep achieves lowerreconstruction errors, produces more discriminative latent spaces, and betterpreserves fine-scale structure than both VAE and maep baselines. These resultsestablish daep as an effective framework for scientific domains where dataarrives as irregular, heterogeneous sequences.</description>
      <author>example@mail.com (Yunyi Shen, Alexander Gagliano)</author>
      <guid isPermaLink="false">2510.20595v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval</title>
      <link>http://arxiv.org/abs/2510.20393v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM Multimedia 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的因果方法，用于解决图像到食谱检索中的表示偏差问题，通过预测并注入图像中可能被忽视的烹饪元素，提高了检索性能。&lt;h4&gt;背景&lt;/h4&gt;现有图像到食谱检索方法假设食物图像能完全捕捉食谱细节，但实际上图像只反映烹饪结果而非过程。这导致跨模态表示学习忽略视觉上不明显但对检索关键的细节，使表示偏向主要视觉元素，难以区分相似食谱。当训练数据包含不同菜系时，这种偏差更严重。&lt;h4&gt;目的&lt;/h4&gt;提出一种因果方法，预测图像中可能被忽视的烹饪元素，并将这些元素明确注入跨模态表示学习中，以减轻偏差。&lt;h4&gt;方法&lt;/h4&gt;采用因果方法预测图像中可能被忽视的烹饪元素，并将这些元素注入跨模态表示学习过程。在标准单语Recipe1M数据集和新策划的多语言多文化菜系数据集上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;提出的因果表示学习能够揭示细微的成分和烹饪动作，在单语和多语言多文化数据集上都取得了令人印象深刻的检索性能。&lt;h4&gt;结论&lt;/h4&gt;通过因果方法预测并注入图像中可能被忽视的烹饪元素，可以有效减轻跨模态表示学习中的偏差，特别是在处理不同菜系的图像和食谱时效果显著。&lt;h4&gt;翻译&lt;/h4&gt;现有的图像到食谱检索方法隐含假设食物图像可以完全捕捉其食谱中文本记录的细节。然而，食物图像只反映了烹饪菜肴的视觉结果，而不是底层的烹饪过程。因此，学习跨模态表示来弥合图像和食谱之间的模态差距时，往往会忽略那些在视觉上不明显但对食谱检索至关重要的细微、特定于食谱的细节。具体来说，这些表示偏向于捕捉主要的视觉元素，导致难以对在使用成分和烹饪方法上有细微差异的相似食谱进行排序。当训练数据混合来自不同菜系的图像和食谱时，表示学习中的偏差预计会更严重。本文提出了一种新的因果方法，预测图像中可能被忽视的烹饪元素，同时明确地将这些元素注入跨模态表示学习中以减轻偏差。实验在标准的单语Recipe1M数据集和一个新策划的多语言多文化菜系数据集上进行。结果表明，提出的因果表示学习能够揭示细微的成分和烹饪动作，并在单语和多语言多文化数据集上都取得了令人印象深刻的检索性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing approaches for image-to-recipe retrieval have the implicitassumption that a food image can fully capture the details textually documentedin its recipe. However, a food image only reflects the visual outcome of acooked dish and not the underlying cooking process. Consequently, learningcross-modal representations to bridge the modality gap between images andrecipes tends to ignore subtle, recipe-specific details that are not visuallyapparent but are crucial for recipe retrieval. Specifically, therepresentations are biased to capture the dominant visual elements, resultingin difficulty in ranking similar recipes with subtle differences in use ofingredients and cooking methods. The bias in representation learning isexpected to be more severe when the training data is mixed of images andrecipes sourced from different cuisines. This paper proposes a novel causalapproach that predicts the culinary elements potentially overlooked in images,while explicitly injecting these elements into cross-modal representationlearning to mitigate biases. Experiments are conducted on the standardmonolingual Recipe1M dataset and a newly curated multilingual multiculturalcuisine dataset. The results indicate that the proposed causal representationlearning is capable of uncovering subtle ingredients and cooking actions andachieves impressive retrieval performance on both monolingual and multilingualmulticultural datasets.</description>
      <author>example@mail.com (Qing Wang, Chong-Wah Ngo, Yu Cao, Ee-Peng Lim)</author>
      <guid isPermaLink="false">2510.20393v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>GUSL-Dehaze: A Green U-Shaped Learning Approach to Image Dehazing</title>
      <link>http://arxiv.org/abs/2510.20266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GUSL-Dehaze是一种绿色U型学习方法的图像去雾技术，结合了基于物理的模型与绿色学习框架，避免了深度学习的高计算成本和大参数量问题。&lt;h4&gt;背景&lt;/h4&gt;图像去雾是恢复清晰图像的任务，传统方法依赖统计先验和物理模型，而最先进的方法主要基于深度学习，但这些方法计算成本高且参数量大，不适合资源受限设备。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级、透明的图像去雾方法，避免深度学习的高计算成本，同时保持与最先进方法相当的性能。&lt;h4&gt;方法&lt;/h4&gt;GUSL-Dehaze采用改进的暗通道先验进行初始去雾，然后通过U型架构实现绿色学习流程，使用无监督表示学习进行特征提取，并结合相关特征测试和最小二乘归一化变换等特征工程技术，最后通过透明的监督学习策略获得去雾图像。&lt;h4&gt;主要发现&lt;/h4&gt;GUSL-Dehaze显著减少了参数数量，同时确保了数学可解释性，并取得了与最先进深度学习模型相当的性能。&lt;h4&gt;结论&lt;/h4&gt;GUSL-Dehaze为图像去雾提供了一种轻量级、透明的替代方案，避免了深度学习的计算负担，同时保持了高性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;图像去雾是一项恢复任务，旨在从单幅有雾输入中恢复清晰图像。传统方法依赖于统计先验和基于物理的大气散射模型来重建无雾图像。虽然最近最先进的方法主要基于深度学习架构，但这些模型通常涉及高计算成本和大参数量，使其不适合资源受限设备。在本工作中，我们提出了GUSL-Dehaze，一种绿色U型学习方法的图像去雾技术。我们的方法将基于物理的模型与绿色学习框架相结合，提供了比传统深度学习技术更轻量、更透明的替代方案。与基于神经网络的解决方案不同，GUSL-Dehaze完全避免了深度学习。相反，我们首先使用改进的暗通道先验进行初始去雾步骤，然后通过U型架构实现绿色学习流程。该架构采用无监督表示学习进行有效特征提取，并结合相关特征测试和最小二乘归一化变换等特征工程技术来保持紧凑的模型大小。最后，通过透明的监督学习策略获得去雾图像。GUSL-Dehaze显著减少了参数数量，同时确保了数学可解释性，并取得了与最先进深度学习模型相当的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image dehazing is a restoration task that aims to recover a clear image froma single hazy input. Traditional approaches rely on statistical priors and thephysics-based atmospheric scattering model to reconstruct the haze-free image.While recent state-of-the-art methods are predominantly based on deep learningarchitectures, these models often involve high computational costs and largeparameter sizes, making them unsuitable for resource-constrained devices. Inthis work, we propose GUSL-Dehaze, a Green U-Shaped Learning approach to imagedehazing. Our method integrates a physics-based model with a green learning(GL) framework, offering a lightweight, transparent alternative to conventionaldeep learning techniques. Unlike neural network-based solutions, GUSL-Dehazecompletely avoids deep learning. Instead, we begin with an initial dehazingstep using a modified Dark Channel Prior (DCP), which is followed by a greenlearning pipeline implemented through a U-shaped architecture. Thisarchitecture employs unsupervised representation learning for effective featureextraction, together with feature-engineering techniques such as the RelevantFeature Test (RFT) and the Least-Squares Normal Transform (LNT) to maintain acompact model size. Finally, the dehazed image is obtained via a transparentsupervised learning strategy. GUSL-Dehaze significantly reduces parameter countwhile ensuring mathematical interpretability and achieving performance on parwith state-of-the-art deep learning models.</description>
      <author>example@mail.com (Mahtab Movaheddrad, Laurence Palmer, C. -C. Jay Kuo)</author>
      <guid isPermaLink="false">2510.20266v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection</title>
      <link>http://arxiv.org/abs/2510.20214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the preprint version of the manuscript submitted to IEEE  Journal of Biomedical and Health Informatics (JBHI) for review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CURL的新型自监督学习框架，用于从胎儿超声视频中准确检测胎儿运动，解决了传统方法的主观性和准确性有限的问题。&lt;h4&gt;背景&lt;/h4&gt;准确的胎儿运动检测对于评估产前健康至关重要，异常的运动模式可能表明存在潜在的并发症，如胎盘功能障碍或胎儿窘迫。传统方法包括母亲感知和胎心宫缩监护图，但这些方法存在主观性和准确性有限的问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决传统胎儿运动检测方法的挑战，研究人员提出了一种新型自监督学习框架CURL，用于从延长的胎儿超声视频记录中检测胎儿运动。&lt;h4&gt;方法&lt;/h4&gt;CURL方法利用双重对比损失，结合空间和时间对比学习，来学习鲁棒的运动表示。此外，研究还引入了一种特定任务的采样策略，确保在自监督训练过程中有效分离运动和非运动段，同时通过概率微调方法实现对任意长度的超声记录的灵活推断。&lt;h4&gt;主要发现&lt;/h4&gt;在包含92名受试者（每人进行30分钟超声检查）的内部数据集上评估，CURL达到了78.01%的敏感性和81.60%的AUROC，证明了其在胎儿运动分析方面的可靠性和客观性。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了自监督对比学习在胎儿运动分析中的潜力，为改进产前监测和临床决策铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;准确的胎儿运动检测对于评估产前健康至关重要，因为异常的运动模式可能表明存在潜在的并发症，如胎盘功能障碍或胎儿窘迫。传统方法，包括母亲感知和胎心宫缩监护图，存在主观性和准确性有限的问题。为了解决这些挑战，我们提出了对比超声视频表示学习，这是一种新颖的自监督学习框架，用于从延长的胎儿超声视频记录中检测胎儿运动。我们的方法利用双重对比损失，结合空间和时间对比学习，来学习鲁棒的运动表示。此外，我们引入了一种特定任务的采样策略，确保在自监督训练过程中有效分离运动和非运动段，同时通过概率微调方法实现对任意长度的超声记录的灵活推断。在包含92名受试者（每人进行30分钟超声检查）的内部数据集上评估，CURL达到了78.01%的敏感性和81.60%的AUROC，证明了其在胎儿运动分析方面的可靠性和客观性。这些结果突显了自监督对比学习在胎儿运动分析中的潜力，为改进产前监测和临床决策铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate fetal movement (FM) detection is essential for assessing prenatalhealth, as abnormal movement patterns can indicate underlying complicationssuch as placental dysfunction or fetal distress. Traditional methods, includingmaternal perception and cardiotocography (CTG), suffer from subjectivity andlimited accuracy. To address these challenges, we propose ContrastiveUltrasound Video Representation Learning (CURL), a novel self-supervisedlearning framework for FM detection from extended fetal ultrasound videorecordings. Our approach leverages a dual-contrastive loss, incorporating bothspatial and temporal contrastive learning, to learn robust motionrepresentations. Additionally, we introduce a task-specific sampling strategy,ensuring the effective separation of movement and non-movement segments duringself-supervised training, while enabling flexible inference on arbitrarily longultrasound recordings through a probabilistic fine-tuning approach. Evaluatedon an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions,CURL achieves a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating itspotential for reliable and objective FM analysis. These results highlight thepotential of self-supervised contrastive learning for fetal movement analysis,paving the way for improved prenatal monitoring and clinical decision-making.</description>
      <author>example@mail.com (Talha Ilyas, Duong Nhu, Allison Thomas, Arie Levin, Lim Wei Yap, Shu Gong, David Vera Anaya, Yiwen Jiang, Deval Mehta, Ritesh Warty, Vinayak Smith, Maya Reddy, Euan Wallace, Wenlong Cheng, Zongyuan Ge, Faezeh Marzbanrad)</author>
      <guid isPermaLink="false">2510.20214v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>A Structured Review and Quantitative Profiling of Public Brain MRI Datasets for Foundation Model Development</title>
      <link>http://arxiv.org/abs/2510.20196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究分析了54个公开可用的脑部MRI数据集，评估了数据规模、多样性和一致性对基础模型开发的影响，发现数据集间存在显著不平衡和异质性，预处理无法完全消除数据集间的偏差，强调了设计脑部MRI基础模型时需要考虑预处理感知和领域自适应策略的必要性。&lt;h4&gt;背景&lt;/h4&gt;脑部MRI基础模型的发展依赖于可用数据的规模、多样性和一致性，然而对这些因素的系统评估仍然很少见。&lt;h4&gt;目的&lt;/h4&gt;分析54个公开可用的脑部MRI数据集（包含超过538,031个样本），为脑部MRI基础模型开发提供结构化、多层次的概述。&lt;h4&gt;方法&lt;/h4&gt;在数据集层面分析模态组成、疾病覆盖范围和数据集规模；在图像层面量化15个代表性数据集中的体素间距、方向和强度分布；评估预处理步骤（强度归一化、偏置场校正、颅骨剥离、空间配准和插值）对体素统计和几何形状的影响；使用3D DenseNet121进行特征空间案例研究，评估标准化预处理后的协变量偏移。&lt;h4&gt;主要发现&lt;/h4&gt;数据集层面存在大型健康队列与较小临床人群之间的严重不平衡；图像层面存在显著的异质性，可能影响表示学习；预处理步骤虽提高了数据集内部一致性，但数据集间的残余差异仍然存在；标准化预处理后仍可测量到残余协变量偏移，确认仅靠调和无法消除数据集间的偏差。&lt;h4&gt;结论&lt;/h4&gt;这些分析提供了公共脑部MRI资源中变异性的统一表征，强调在设计可推广的脑部MRI基础模型时需要考虑预处理感知和领域自适应策略。&lt;h4&gt;翻译&lt;/h4&gt;脑部MRI基础模型的发展在很大程度上取决于可用数据的规模、多样性和一致性，然而对这些因素的系统评估仍然很少。在本研究中，我们分析了54个公开可用的脑部MRI数据集，包含超过538,031个样本，为基础模型开发提供了结构化、多层次的概述。在数据集层面，我们表征了模态组成、疾病覆盖范围和数据集规模，揭示了大型健康队列与较小临床人群之间的严重不平衡。在图像层面，我们量化了15个代表性数据集中的体素间距、方向和强度分布，证明了可能影响表示学习的显著异质性。然后，我们对预处理变异性进行了定量评估，检查了强度归一化、偏置场校正、颅骨剥离、空间配准和插值如何改变体素统计和几何形状。虽然这些步骤提高了数据集内部的一致性，但数据集之间的残余差异仍然存在。最后，使用3D DenseNet121进行的特征空间案例研究显示，在标准化预处理后仍可测量到残余协变量偏移，确认仅靠调和无法消除数据集间的偏差。总之，这些分析提供了公共脑部MRI资源中变异性的统一表征，并强调了在设计可推广的脑部MRI基础模型时需要考虑预处理感知和领域自适应策略的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of foundation models for brain MRI depends critically on thescale, diversity, and consistency of available data, yet systematic assessmentsof these factors remain scarce. In this study, we analyze 54 publiclyaccessible brain MRI datasets encompassing over 538,031 to provide astructured, multi-level overview tailored to foundation model development. Atthe dataset level, we characterize modality composition, disease coverage, anddataset scale, revealing strong imbalances between large healthy cohorts andsmaller clinical populations. At the image level, we quantify voxel spacing,orientation, and intensity distributions across 15 representative datasets,demonstrating substantial heterogeneity that can influence representationlearning. We then perform a quantitative evaluation of preprocessingvariability, examining how intensity normalization, bias field correction,skull stripping, spatial registration, and interpolation alter voxel statisticsand geometry. While these steps improve within-dataset consistency, residualdifferences persist between datasets. Finally, feature-space case study using a3D DenseNet121 shows measurable residual covariate shift after standardizedpreprocessing, confirming that harmonization alone cannot eliminateinter-dataset bias. Together, these analyses provide a unified characterizationof variability in public brain MRI resources and emphasize the need forpreprocessing-aware and domain-adaptive strategies in the design ofgeneralizable brain MRI foundation models.</description>
      <author>example@mail.com (Minh Sao Khue Luu, Margaret V. Benedichuk, Ekaterina I. Roppert, Roman M. Kenzhin, Bair N. Tuchinov)</author>
      <guid isPermaLink="false">2510.20196v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks</title>
      <link>http://arxiv.org/abs/2510.20165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the Proceedings of the Thirty Fifth AAAI Conference on  Artificial Intelligence (AAAI 2021), paper number 7926&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于GAN的无监督解纠缠表征学习模型IB-GAN，利用信息瓶颈框架优化GAN，通过生成器的中间层约束输入与生成输出之间的互信息，实现了对潜在空间的解纠缠和可解释性利用。&lt;h4&gt;背景&lt;/h4&gt;解纠缠表征学习是机器学习领域的重要研究方向，现有的方法如InfoGAN和β-VAEs存在一定局限性，需要改进模型架构以获得更好的解纠缠能力和样本质量。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于GAN的无监督解纠缠表征学习模型IB-GAN，利用信息瓶颈框架优化GAN，实现更好的解纠缠能力和样本质量。&lt;h4&gt;方法&lt;/h4&gt;IB-GAN架构与InfoGAN部分相似但有关键差异：利用生成器的中间层约束输入与生成输出之间的互信息；中间随机层可作为可学习的潜在分布，与生成器端到端联合训练；使生成器能够以解纠缠和可解释的方式利用潜在空间。&lt;h4&gt;主要发现&lt;/h4&gt;在dSprites和Color-dSprites数据集上的实验表明，IB-GAN实现了与最先进的β-VAEs相当的解纠缠分数，并优于InfoGAN；在CelebA和3D Chairs数据集上，IB-GAN在FID分数方面通常比β-VAEs和Info-GAN生成的样本具有更好的视觉质量和多样性。&lt;h4&gt;结论&lt;/h4&gt;IB-GAN通过利用信息瓶颈框架优化GAN，有效提升了模型的解纠缠能力和样本生成质量，是一种有效的无监督解纠缠表征学习方法。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的基于GAN的无监督解纠缠表征学习模型。这一新模型是在尝试将信息瓶颈框架应用于GAN优化的过程中发现的，因此命名为IB-GAN。IB-GAN的架构与InfoGAN部分相似，但有一个关键区别：利用生成器的中间层来约束输入与生成输出之间的互信息。中间随机层可以作为可学习的潜在分布，与生成器以端到端的方式联合训练。因此，IB-GAN的生成器能够以解纠缠和可解释的方式利用潜在空间。在dSprites和Color-dSprites数据集上的实验表明，IB-GAN实现了与最先进的β-VAEs相当的解纠缠分数，并优于InfoGAN。此外，在CelebA和3D Chairs数据集上，IB-GAN生成的样本在FID分数方面通常比β-VAEs和Info-GAN具有更好的视觉质量和多样性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1609/aaai.v35i9.16967&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a new GAN-based unsupervised model for disentangled representationlearning. The new model is discovered in an attempt to utilize the InformationBottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. Thearchitecture of IB-GAN is partially similar to that of InfoGAN but has acritical difference; an intermediate layer of the generator is leveraged toconstrain the mutual information between the input and the generated output.The intermediate stochastic layer can serve as a learnable latent distributionthat is trained with the generator jointly in an end-to-end fashion. As aresult, the generator of IB-GAN can harness the latent space in a disentangledand interpretable manner. With the experiments on dSprites and Color-dSpritesdataset, we demonstrate that IB-GAN achieves competitive disentanglement scoresto those of state-of-the-art \b{eta}-VAEs and outperforms InfoGAN. Moreover,the visual quality and the diversity of samples generated by IB-GAN are oftenbetter than those by \b{eta}-VAEs and Info-GAN in terms of FID score on CelebAand 3D Chairs dataset.</description>
      <author>example@mail.com (Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, Gunhee Kim)</author>
      <guid isPermaLink="false">2510.20165v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning</title>
      <link>http://arxiv.org/abs/2510.20162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的组合零样本学习方法，通过无监督数据积累多模态知识并更新原型，解决了测试时分布偏移的问题，在多个基准数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;组合零样本学习旨在基于已学习知识识别新的属性-对象组合。现有方法在测试时由于标签空间分布偏移导致性能下降，这种偏移源于包含了从未见过的属性和对象重新组合的样本。&lt;h4&gt;目的&lt;/h4&gt;克服测试时标签空间分布偏移带来的挑战，提出一种方法来更新多模态原型，使模型能够灵活适应测试时的分布偏移。&lt;h4&gt;方法&lt;/h4&gt;提出一种新方法，通过无监督数据积累文本和视觉模态的综合知识；设计自适应更新权重控制原型调整程度；引入动态优先队列存储高置信度图像，从历史图像获取视觉知识；通过多模态协同表示学习对齐文本和视觉原型。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准数据集上，无论是在封闭世界还是开放世界设置下，该方法都达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法通过更新多模态原型和自适应权重，有效解决了组合零样本学习中的分布偏移问题，代码将在https://github.com/xud-yan/TOMCAT上提供。&lt;h4&gt;翻译&lt;/h4&gt;组合零样本学习旨在基于已学习知识识别新的属性-对象组合。现有方法在测试时由于标签空间分布偏移导致性能下降，这源于包含了从未见过的属性和对象重新组合的样本。为克服这一挑战，我们提出了一种新方法，通过无监督数据在文本和视觉模态中积累综合知识，以在测试时更新多模态原型。基于此，我们进一步设计了自适应更新权重来控制原型调整程度，使模型能够在测试过程中灵活适应分布偏移。此外，我们引入了动态优先队列，存储高置信度图像，以便从历史图像获取视觉知识进行推理。考虑到多模态知识的语义一致性，我们通过多模态协同表示学习对齐文本和视觉原型。大量实验表明，我们的方法在四个基准数据集上，无论是在封闭世界还是开放世界设置下，都达到了最先进的性能。代码将在https://github.com/xud-yan/TOMCAT上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compositional Zero-Shot Learning (CZSL) aims to recognize novelattribute-object compositions based on the knowledge learned from seen ones.Existing methods suffer from performance degradation caused by the distributionshift of label space at test time, which stems from the inclusion of unseencompositions recombined from attributes and objects. To overcome the challenge,we propose a novel approach that accumulates comprehensive knowledge in bothtextual and visual modalities from unsupervised data to update multimodalprototypes at test time. Building on this, we further design an adaptive updateweight to control the degree of prototype adjustment, enabling the model toflexibly adapt to distribution shift during testing. Moreover, a dynamicpriority queue is introduced that stores high-confidence images to acquirevisual knowledge from historical images for inference. Considering the semanticconsistency of multimodal knowledge, we align textual and visual prototypes bymultimodal collaborative representation learning. Extensive experimentsindicate that our approach achieves state-of-the-art performance on fourbenchmark datasets under both closed-world and open-world settings. Code willbe available at https://github.com/xud-yan/TOMCAT .</description>
      <author>example@mail.com (Xudong Yan, Songhe Feng)</author>
      <guid isPermaLink="false">2510.20162v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Improving Predictive Confidence in Medical Imaging via Online Label Smoothing</title>
      <link>http://arxiv.org/abs/2510.20011v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and presented in International Conference on Advancing  Science and Technologies in Health Science&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了在线标签平滑(OLS)在医学图像分类中的应用，结果显示OLS能提高分类准确率并改善模型校准性。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型，特别是卷积神经网络，在医学图像分类中取得了显著成果，但这些模型经常产生过度自信的预测，影响在关键医疗环境中的可靠性。&lt;h4&gt;目的&lt;/h4&gt;研究使用在线标签平滑(OLS)，一种基于模型自身预测模式动态调整软标签的方法，以提高医学图像分类模型的性能和可靠性。&lt;h4&gt;方法&lt;/h4&gt;在大型RadImageNet数据集上使用三种架构评估OLS：ResNet-50、MobileNetV2和VGG-19，并与标准训练方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;OLS相比标准训练方法持续提高了Top-1和Top-5分类准确率，并产生更紧凑和良好分离的特征嵌入，表明表示学习得到改善。&lt;h4&gt;结论&lt;/h4&gt;OLS不仅增强了预测性能，还提高了校准性，使其成为医学成像领域开发可信AI系统的实用有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型，特别是卷积神经网络，在医学图像分类中已取得了令人印象深刻的结果。然而，这些模型通常会产生过度自信的预测，这可能削弱它们在关键医疗环境中的可靠性。虽然传统的标签平滑提供了一种减少这种过度自信的简单方法，但它未能考虑类别之间的关系，将所有非目标类别同等对待。在本研究中，我们探索了在线标签平滑(OLS)的使用，这是一种动态方法，基于模型自身的预测模式在训练过程中调整软标签。我们在大型RadImageNet数据集上使用三种广泛使用的架构评估了OLS：ResNet-50、MobileNetV2和VGG-19。我们的结果表明，与标准训练方法（包括硬标签、传统标签平滑和无教师知识蒸馏）相比，OLS持续提高了Top-1和Top-5分类准确率。除了准确率的提升外，OLS还导致更紧凑和良好分离的特征嵌入，表明表示学习得到改善。这些发现表明，OLS不仅增强了预测性能，还提高了校准性，使其成为医学成像领域开发可信AI系统的实用有效解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models, especially convolutional neural networks, have achievedimpressive results in medical image classification. However, these models oftenproduce overconfident predictions, which can undermine their reliability incritical healthcare settings. While traditional label smoothing offers a simpleway to reduce such overconfidence, it fails to consider relationships betweenclasses by treating all non-target classes equally. In this study, we explorethe use of Online Label Smoothing (OLS), a dynamic approach that adjusts softlabels throughout training based on the model's own prediction patterns. Weevaluate OLS on the large-scale RadImageNet dataset using three widely usedarchitectures: ResNet-50, MobileNetV2, and VGG-19. Our results show that OLSconsistently improves both Top-1 and Top-5 classification accuracy compared tostandard training methods, including hard labels, conventional label smoothing,and teacher-free knowledge distillation. In addition to accuracy gains, OLSleads to more compact and well-separated feature embeddings, indicatingimproved representation learning. These findings suggest that OLS not onlystrengthens predictive performance but also enhances calibration, making it apractical and effective solution for developing trustworthy AI systems in themedical imaging domain.</description>
      <author>example@mail.com (Kushan Choudhury, Shubhrodeep Roy, Ankur Chanda, Shubhajit Biswas, Somenath Kuiry)</author>
      <guid isPermaLink="false">2510.20011v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Transformed Multi-view 3D Shape Features with Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.19955v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了3D形状特征表示学习的挑战，通过将Vision Transformers架构与对比学习目标相结合，在3D形状理解方面取得了良好效果&lt;h4&gt;背景&lt;/h4&gt;计算机视觉方法在从2D图像识别3D物体方面存在困难，通常需要大量标记数据，且依赖的卷积神经网络可能会忽略关键的形状关系&lt;h4&gt;目的&lt;/h4&gt;解决3D形状特征表示学习中的挑战，探索减少对大量标记数据依赖的方法，克服CNNs在捕获关键形状关系方面的局限性&lt;h4&gt;方法&lt;/h4&gt;使用Vision Transformers (ViTs)架构与现代对比目标相结合，进行多视图3D分析，结合ViTs理解整体形状的能力和对比学习的有效性&lt;h4&gt;主要发现&lt;/h4&gt;监督对比损失在ModelNet10上达到了约90.6%的准确率；ViTs能够捕获全局形状语义，而对比优化能够完善局部判别特征&lt;h4&gt;结论&lt;/h4&gt;通过结合ViTs与对比目标，成功实现了3D表示学习，这种方法基于大量实验评估，证明了其有效性&lt;h4&gt;翻译&lt;/h4&gt;这篇论文通过研究最先进的骨干网络与对比监督和自监督学习目标的组合，解决了3D形状特征表示学习中的挑战。计算机视觉方法在从2D图像识别3D物体方面存在困难，通常需要大量标记数据，并依赖于卷积神经网络(CNNs)，而这些网络可能会忽略关键的形状关系。我们的研究表明，当Vision Transformers (ViTs)架构与现代对比目标配对时，在我们的下游任务中多视图3D分析取得了有希望的结果，统一了对比学习和3D形状理解的流程。例如，监督对比损失在ModelNet10上达到了约90.6%的准确率。ViTs和对比学习的应用，利用了ViTs理解整体形状的能力和对比学习的有效性，克服了对大量标记数据的需求以及CNNs在捕获关键形状关系方面的局限性。成功的原因在于通过ViTs捕获全局形状语义，并通过对比优化完善局部判别特征。重要的是，我们的方法是经验性的，因为它基于大量的实验评估来验证将ViTs与对比目标相结合用于3D表示学习的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D形状特征表示学习中的挑战，特别是计算机视觉方法从2D图像识别3D物体的困难。这个问题很重要，因为3D形状理解对机器人、虚拟现实等应用至关重要，而当前方法需要大量标记数据且依赖CNN，这些网络可能忽略关键的形状关系，限制了模型在真实世界应用中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到CNN在3D形状理解中的局限性，注意到ViT架构在视觉识别任务中表现优异，并观察到对比学习在利用未标记数据方面的潜力。他们借鉴了MVCNN(首次使用CNN进行3D形状理解)、ViT架构以及多种对比学习方法(如InfoNCE、SimCLR、SupCon)，创新性地将这些技术结合应用于3D多视图形状理解，这是之前研究较少探索的组合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用Vision Transformer的注意力机制捕获全局形状语义，通过对比学习优化细化局部判别特征，统一对比学习和3D形状理解流程。整体流程分为两个阶段：第一阶段是多视图渲染和对比学习，从3D网格生成12个视图图像，使用多种对比损失函数训练ViT和CNN骨干；第二阶段是下游任务评估，包括分类(线性评估、k-NN分类、t-SNE可视化)和检索任务(基于余弦相似度排序，使用mAP评估)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将ViT与对比学习结合用于3D多视图形状理解；系统评估四种ViT骨干和五种对比损失函数；统一对比学习和3D形状理解流程；在多个下游任务进行全面评估。相比之前工作，本文主要使用ViT而非CNN骨干；将对比学习从2D扩展到3D多视图数据；提供更全面的评估；在ModelNet10上达到90.6%分类准确率和95.5%的mAP，超越之前方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过将Vision Transformer与先进的对比学习目标相结合，显著提升了3D多视图形状理解的性能，同时减少了对大量标记数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenges in representation learning of 3D shapefeatures by investigating state-of-the-art backbones paired with bothcontrastive supervised and self-supervised learning objectives. Computer visionmethods struggle with recognizing 3D objects from 2D images, often requiringextensive labeled data and relying on Convolutional Neural Networks (CNNs) thatmay overlook crucial shape relationships. Our work demonstrates that VisionTransformers (ViTs) based architectures, when paired with modern contrastiveobjectives, achieve promising results in multi-view 3D analysis on ourdownstream tasks, unifying contrastive and 3D shape understanding pipelines.For example, supervised contrastive losses reached about 90.6% accuracy onModelNet10. The use of ViTs and contrastive learning, leveraging ViTs' abilityto understand overall shapes and contrastive learning's effectiveness,overcomes the need for extensive labeled data and the limitations of CNNs incapturing crucial shape relationships. The success stems from capturing globalshape semantics via ViTs and refining local discriminative features throughcontrastive optimization. Importantly, our approach is empirical, as it isgrounded on extensive experimental evaluation to validate the effectiveness ofcombining ViTs with contrastive objectives for 3D representation learning.</description>
      <author>example@mail.com (Márcus Vinícius Lobo Costa, Sherlon Almeida da Silva, Bárbara Caroline Benato, Leo Sampaio Ferraz Ribeiro, Moacir Antonelli Ponti)</author>
      <guid isPermaLink="false">2510.19955v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation</title>
      <link>http://arxiv.org/abs/2510.20596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI 2021&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于相似性原型的跨模态分割新框架，通过在嵌入空间中学习类别原型并引入相似性约束，以及使用字典存储不同图像中提取的原型，解决了深度学习模型在未见数据上性能下降的问题，实验证明该方法优于其他最先进方法。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型在各种视觉挑战中取得了巨大成功，但训练好的模型在应用于未见过的数据时性能会急剧下降，模型对域偏移敏感。&lt;h4&gt;目的&lt;/h4&gt;减少域差距，避免对未见域的昂贵标注，提高模型在跨模态分割任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于相似性原型的跨模态分割框架，在嵌入空间中学习类别的代表性原型，引入相似性约束使原型对每个语义类别具有代表性且不同类别间可分离，使用字典存储从不同图像中提取的原型防止类别缺失问题，实现原型的对比学习以提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过原型学习和对比学习的方法可以有效解决域偏移问题，提高跨模态分割性能。&lt;h4&gt;结论&lt;/h4&gt;本文提出的基于相似性原型的跨模态分割框架比其他最先进方法取得了更好的结果。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型在各种视觉挑战中取得了巨大成功，但训练好的模型在应用于未见过的数据时性能会急剧下降。由于模型对域偏移敏感，无监督域适应尝试减少域差距并避免对未见域的昂贵标注。本文提出了一种基于相似性原型的跨模态分割新框架。具体来说，我们在嵌入空间中学习类别的原型，然后引入相似性约束，使这些原型对每个语义类别具有代表性，同时不同类别之间可分离。此外，我们使用字典存储从不同图像中提取的原型，这可以防止类别缺失问题，并实现原型的对比学习，进一步提高性能。大量实验表明，我们的方法比其他最先进方法取得了更好的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models have achieved great success on various visionchallenges, but a well-trained model would face drastic performance degradationwhen applied to unseen data. Since the model is sensitive to domain shift,unsupervised domain adaptation attempts to reduce the domain gap and avoidcostly annotation of unseen domains. This paper proposes a novel framework forcross-modality segmentation via similarity-based prototypes. In specific, welearn class-wise prototypes within an embedding space, then introduce asimilarity constraint to make these prototypes representative for each semanticclass while separable from different classes. Moreover, we use dictionaries tostore prototypes extracted from different images, which prevents theclass-missing problem and enables the contrastive learning of prototypes, andfurther improves performance. Extensive experiments show that our methodachieves better results than other state-of-the-art methods.</description>
      <author>example@mail.com (Ziyu Ye, Chen Ju, Chaofan Ma, Xiaoyun Zhang)</author>
      <guid isPermaLink="false">2510.20596v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment</title>
      <link>http://arxiv.org/abs/2510.20540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SheafAlign是一种基于层理论的多模态对齐框架，适用于分布式场景，不要求所有模态相互冗余，能有效保留共享和独特信息，并在多个方面表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;传统多模态对齐方法假设所有模态之间存在相互冗余，这一假设在现实世界的分布式场景中并不成立。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为SheafAlign的框架，用于去中心化的多模态对齐，用多个比较空间替代单一空间对齐。&lt;h4&gt;方法&lt;/h4&gt;使用层理论框架，通过层结构建模成对模态关系，并利用基于去中心化对比学习的目标进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;Sheaf克服了先前方法的局限性，不需要所有模态之间存在相互冗余；在多模态传感数据集上表现出优越的零样本泛化能力、优秀的跨模态对齐能力、对缺失模态具有鲁棒性，与最先进的基线相比，通信成本降低50%。&lt;h4&gt;结论&lt;/h4&gt;SheafAlign是一种有效的去中心化多模态对齐方法，能够在分布式场景中更好地处理模态间关系。&lt;h4&gt;翻译&lt;/h4&gt;传统多模态对齐方法假设所有模态之间存在相互冗余，这一假设在现实世界的分布式场景中并不成立。我们提出了SheafAlign，一种基于层理论的去中心化多模态对齐框架，它用多个比较空间替代了单一空间对齐。这种方法通过层结构建模成对模态关系，并利用基于去中心化对比学习的目标进行训练。SheafAlign通过不要求所有模态之间存在相互冗余，克服了先前方法的局限性，同时保留了共享信息和独特信息。在多模态传感数据集上的实验显示，它在零样本泛化、跨模态对齐和对缺失模态的鲁棒性方面具有优越性，并且与最先进的基线相比，通信成本降低了50%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional multimodal alignment methods assume mutual redundancy across allmodalities, an assumption that fails in real-world distributed scenarios. Wepropose SheafAlign, a sheaf-theoretic framework for decentralized multimodalalignment that replaces single-space alignment with multiple comparison spaces.This approach models pairwise modality relations through sheaf structures andleverages decentralized contrastive learning-based objectives for training.SheafAlign overcomes the limitations of prior methods by not requiring mutualredundancy among all modalities, preserving both shared and unique information.Experiments on multimodal sensing datasets show superior zero-shotgeneralization, cross-modal alignment, and robustness to missing modalities,with 50\% lower communication cost than state-of-the-art baselines.</description>
      <author>example@mail.com (Abdulmomen Ghalkha, Zhuojun Tian, Chaouki Ben Issaid, Mehdi Bennis)</author>
      <guid isPermaLink="false">2510.20540v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>ViTacGen: Robotic Pushing with Vision-to-Touch Generation</title>
      <link>http://arxiv.org/abs/2510.14117v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ViTacGen是一种新颖的机器人操作框架，通过视觉到触觉生成在强化学习中消除对高分辨率真实触觉传感器的依赖，实现仅视觉机器人系统上的有效零样本部署，在模拟和真实世界实验中展现出高达86%的成功率。&lt;h4&gt;背景&lt;/h4&gt;机器人推动是一种需要触觉反馈来捕捉末端执行器与物体间细微接触力和动力学的基本操作任务。真实触觉传感器面临高成本、脆弱性、校准和传感器差异等挑战，而仅依赖视觉的策略性能有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从视觉推断触觉状态的机器人操作框架，减少对昂贵且脆弱的真实触觉传感器的依赖，实现仅视觉机器人系统上的有效零样本部署。&lt;h4&gt;方法&lt;/h4&gt;ViTacGen包含一个编码器-解码器视觉到触觉生成网络，直接从视觉图像序列生成接触深度图像（标准化触觉表示），以及一个使用对比学习融合视觉-触觉数据的强化学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界实验中验证了ViTacGen的有效性，其性能优于传统方法，成功率达到86%。&lt;h4&gt;结论&lt;/h4&gt;ViTacGen成功实现了从视觉到触觉的生成，使仅视觉的机器人系统能够在没有真实触觉传感器的情况下执行有效的机器人推动任务。&lt;h4&gt;翻译&lt;/h4&gt;机器人推动是一种基本操作任务，需要触觉反馈来捕捉末端执行器与物体之间的细微接触力和动力学特性。然而，真实触觉传感器通常面临高成本和脆弱性等硬件限制，以及涉及校准和不同传感器间差异的部署挑战，而仅视觉的策略难以获得令人满意的性能。受人类从视觉推断触觉状态能力的启发，我们提出了ViTacGen，一种专为视觉机器人推动设计的新颖机器人操作框架，在强化学习中使用视觉到触觉生成，消除对高分辨率真实触觉传感器的依赖，实现仅视觉机器人系统上的有效零样本部署。具体来说，ViTacGen包含一个编码器-解码器视觉到触觉生成网络，直接从视觉图像序列生成接触深度图像（标准化的触觉表示），随后是一个基于视觉和生成触觉观察使用对比学习融合视觉-触觉数据的强化学习策略。我们在模拟和真实世界实验中都验证了我们方法的有效性，展示了其优越的性能，成功率高达86%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic pushing is a fundamental manipulation task that requires tactilefeedback to capture subtle contact forces and dynamics between the end-effectorand the object. However, real tactile sensors often face hardware limitationssuch as high costs and fragility, and deployment challenges involvingcalibration and variations between different sensors, while vision-onlypolicies struggle with satisfactory performance. Inspired by humans' ability toinfer tactile states from vision, we propose ViTacGen, a novel robotmanipulation framework designed for visual robotic pushing with vision-to-touchgeneration in reinforcement learning to eliminate the reliance onhigh-resolution real tactile sensors, enabling effective zero-shot deploymenton visual-only robotic systems. Specifically, ViTacGen consists of anencoder-decoder vision-to-touch generation network that generates contact depthimages, a standardized tactile representation, directly from visual imagesequence, followed by a reinforcement learning policy that fuses visual-tactiledata with contrastive learning based on visual and generated tactileobservations. We validate the effectiveness of our approach in both simulationand real world experiments, demonstrating its superior performance andachieving a success rate of up to 86\%.</description>
      <author>example@mail.com (Zhiyuan Wu, Yijiong Lin, Yongqiang Zhao, Xuyang Zhang, Zhuo Chen, Nathan Lepora, Shan Luo)</author>
      <guid isPermaLink="false">2510.14117v2</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Real Deep Research for AI, Robotics and Beyond</title>
      <link>http://arxiv.org/abs/2510.20809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  website: https://realdeepresearch.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Real Deep Research (RDR)的全面框架，用于系统分析AI和机器人研究领域，帮助研究人员识别新兴趋势、发现跨领域机会并为新研究提供起点。&lt;h4&gt;背景&lt;/h4&gt;AI和机器人研究快速增长，每年发表超过10,000篇论文，使得研究人员难以跟上最新发展。快速发展的趋势、跨学科工作的兴起以及需要探索专业领域之外的知识都构成了这一挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个通用流程，能够系统分析任何研究领域，识别新兴趋势，发现跨领域机会，并为新研究提供具体起点。&lt;h4&gt;方法&lt;/h4&gt;开发了Real Deep Research (RDR)全面框架，应用于AI和机器人领域，特别关注基础模型和机器人进展，同时简要扩展到其他科学领域。主论文详细介绍了RDR流程的构建，附录提供了各分析主题的广泛结果。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体的研究发现。&lt;h4&gt;结论&lt;/h4&gt;希望这项工作能为AI领域及更广泛领域的研究人员提供启示，帮助他们应对信息过载的挑战。&lt;h4&gt;翻译&lt;/h4&gt;随着AI和机器人研究的快速增长，现在每年产生超过10,000篇论文，研究人员越来越难以跟上最新发展。快速发展的趋势、跨学科工作的兴起以及探索专业领域之外知识的需求都构成了这一挑战。为解决这些问题，我们提出了一种能够系统分析任何研究领域的通用流程：识别新兴趋势，发现跨领域机会，并为新研究提供具体起点。在本工作中，我们介绍了Real Deep Research (RDR)，这是一个应用于AI和机器人领域的全面框架，特别关注基础模型和机器人进展。我们还简要扩展了对其他科学领域的分析。主论文详细介绍了RDR流程的构建，附录提供了每个分析主题的广泛结果。我们希望这项工作能为AI领域及其他领域的研究人员提供启示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid growth of research in AI and robotics now producing over10,000 papers annually it has become increasingly difficult for researchers tostay up to date. Fast evolving trends, the rise of interdisciplinary work, andthe need to explore domains beyond one's expertise all contribute to thischallenge. To address these issues, we propose a generalizable pipeline capableof systematically analyzing any research area: identifying emerging trends,uncovering cross domain opportunities, and offering concrete starting pointsfor new inquiry. In this work, we present Real Deep Research (RDR) acomprehensive framework applied to the domains of AI and robotics, with aparticular focus on foundation models and robotics advancements. We alsobriefly extend our analysis to other areas of science. The main paper detailsthe construction of the RDR pipeline, while the appendix provides extensiveresults across each analyzed topic. We hope this work sheds light forresearchers working in the field of AI and beyond.</description>
      <author>example@mail.com (Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, Xiaolong Wang)</author>
      <guid isPermaLink="false">2510.20809v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence</title>
      <link>http://arxiv.org/abs/2510.20578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了EmbodiedBrain，一种新型的视觉语言基础模型，解决了当前具身AI模型在模型设计、实时性能评估和离线指标方面的局限性，实现了在所有评估指标上的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;实现通用人工智能(AGI)需要能够在物理环境中进行稳健的空间感知、有效任务规划和自适应执行的具身AI智能体。然而，当前用于具身任务的大型语言模型和多模态大型语言模型存在关键局限性。&lt;h4&gt;目的&lt;/h4&gt;解决当前具身AI模型的局限性，提出一种新的视觉语言基础模型，提高具身智能体在物理环境中的感知、规划和执行能力。&lt;h4&gt;方法&lt;/h4&gt;开发了EmbodiedBrain模型(7B和32B参数规模)，采用与智能体对齐的数据结构，结合大规模监督微调(SFT)和步骤增强组相对策略优化(Step-GRPO)训练方法，引入包含生成奖励模型(GRM)的全面奖励系统，并建立三部分评估体系(通用、规划和端到端模拟基准测试)。&lt;h4&gt;主要发现&lt;/h4&gt;EmbodiedBrain在所有评估指标上实现了卓越性能，为具身基础模型建立了新的最先进水平，有效解决了模型设计与智能体需求之间的差距、实时延迟与性能的权衡问题，以及不真实的离线评估指标问题。&lt;h4&gt;结论&lt;/h4&gt;EmbodiedBrain为下一代通用具身智能体的发展铺平了道路，所有数据、模型权重和评估方法均已开源，可供研究社区使用。&lt;h4&gt;翻译&lt;/h4&gt;实现通用人工智能(AGI)需要能够在物理环境中进行稳健的空间感知、有效任务规划和自适应执行的具身AI智能体。然而，当前用于具身任务的大型语言模型(LLMs)和多模态大型语言模型(MLLMs)存在关键局限性，包括模型设计与智能体需求之间的显著差距、实时延迟与性能之间的不可避免权衡，以及使用不真实的离线评估指标。为解决这些挑战，我们提出了EmbodiedBrain，一种有7B和32B两种参数规模的新型视觉语言基础模型。我们的框架具有与智能体对齐的数据结构，采用结合大规模监督微调(SFT)和步骤增强组相对策略优化(Step-GRPO)的强大训练方法，通过将前序步骤整合为引导前体来提高长距离任务成功率。此外，我们引入了包含在基础设施层面加速的生成奖励模型(GRM)的全面奖励系统，以提高训练效率。为进行彻底验证，我们建立了包含通用、规划和端到端模拟基准测试的三部分评估体系，并提出了一个具有挑战性的新模拟环境并开源。实验结果表明，EmbodiedBrain在所有指标上都实现了卓越性能，为具身基础模型建立了新的最先进水平。为铺平下一代通用具身智能体的发展道路，我们开源了所有数据、模型权重和评估方法，可在https://zterobot.github.io/EmbodiedBrain.github.io获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The realization of Artificial General Intelligence (AGI) necessitatesEmbodied AI agents capable of robust spatial perception, effective taskplanning, and adaptive execution in physical environments. However, currentlarge language models (LLMs) and multimodal LLMs (MLLMs) for embodied taskssuffer from key limitations, including a significant gap between model designand agent requirements, an unavoidable trade-off between real-time latency andperformance, and the use of unauthentic, offline evaluation metrics. To addressthese challenges, we propose EmbodiedBrain, a novel vision-language foundationmodel available in both 7B and 32B parameter sizes. Our framework features anagent-aligned data structure and employs a powerful training methodology thatintegrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented GroupRelative Policy Optimization (Step-GRPO), which boosts long-horizon tasksuccess by integrating preceding steps as Guided Precursors. Furthermore, weincorporate a comprehensive reward system, including a Generative Reward Model(GRM) accelerated at the infrastructure level, to improve training efficiency.For enable thorough validation, we establish a three-part evaluation systemencompassing General, Planning, and End-to-End Simulation Benchmarks,highlighted by the proposal and open-sourcing of a novel, challengingsimulation environment. Experimental results demonstrate that EmbodiedBrainachieves superior performance across all metrics, establishing a newstate-of-the-art for embodied foundation models. Towards paving the way for thenext generation of generalist embodied agents, we open-source all of our data,model weight, and evaluating methods, which are available athttps://zterobot.github.io/EmbodiedBrain.github.io.</description>
      <author>example@mail.com (Ding Zou, Feifan Wang, Mengyu Ge, Siyuan Fan, Zongbing Zhang, Wei Chen, Lingfeng Wang, Zhongyou Hu, Wenrui Yan, Zhengwei Gao, Hao Wang, Weizhao Jin, Yu Zhang, Hainan Zhao, Mingliang Zhang, Xianxian Xi, Yaru Zhang, Wenyuan Li, Zhengguang Gao, Yurui Zhu)</author>
      <guid isPermaLink="false">2510.20578v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>A Unified Framework for Zero-Shot Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.20542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了零样本强化学习的第一个统一框架，引入了一致的符号和分类法，将现有方法组织为直接表示和组合表示两大类，为该领域提供了原则性基础和研究方向。&lt;h4&gt;背景&lt;/h4&gt;零样本强化学习允许代理在不监督的情况下开发通用能力，无需额外训练或规划即可解决下游任务。与传统RL优化固定奖励不同，零样本RL需要代理编码足够丰富的表示以立即适应任何目标，类似于视觉和语言基础模型。尽管该领域兴趣增长，但缺乏共同的分析视角。&lt;h4&gt;目的&lt;/h4&gt;提出零样本强化学习的第一个统一框架，引入一致的符号和分类法，组织现有方法并允许直接比较不同方法，为该领域提供原则性基础。&lt;h4&gt;方法&lt;/h4&gt;框架将算法分为两类：直接表示（学习从奖励到策略的端到端映射）和组合表示（利用值函数的子结构分解表示）。在此框架内，突出方法的共同原则和关键差异，为后续特征方法推导扩展界限，提供其在零样本环境中的新视角。&lt;h4&gt;主要发现&lt;/h4&gt;通过共同视角整合了现有工作，揭示了不同方法间的共享原则和关键差异，为后续特征方法提供了新的理论视角，表明组合表示可能更适合零样本场景。&lt;h4&gt;结论&lt;/h4&gt;该框架为零样本强化学习的未来研究提供了原则性基础，并指明了开发更通用代理的明确路径，有助于推动通用人工智能代理的发展。&lt;h4&gt;翻译&lt;/h4&gt;零样本强化学习(RL)已成为一种在不监督情况下开发通用代理的设置，能够在测试时无需额外训练或规划的情况下解决下游任务。与传统优化固定奖励的RL不同，零样本RL需要代理编码足够丰富的表示以支持立即适应任何目标，这与视觉和语言基础模型相类似。尽管兴趣日益增长，该领域仍缺乏共同的分析视角。我们提出了零样本RL的第一个统一框架，我们的引入了一致的符号和分类法，组织了现有方法并允许直接比较它们。我们框架的核心是将算法分为两个家族：直接表示，学习从奖励到策略的端到端映射；以及组合表示，利用值函数的子结构分解表示。在此框架内，我们突出了跨方法的共同原则和关键差异，并为后续特征方法推导了扩展界限，提供了它们在零样本环境中性能的新视角。通过在共同视角下整合现有工作，我们的框架为未来零样本RL研究提供了原则性基础，并概述了开发更通用代理的明确路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-shot reinforcement learning (RL) has emerged as a setting for developinggeneral agents in an unsupervised manner, capable of solving downstream taskswithout additional training or planning at test-time. Unlike conventional RL,which optimizes policies for a fixed reward, zero-shot RL requires agents toencode representations rich enough to support immediate adaptation to anyobjective, drawing parallels to vision and language foundation models. Despitegrowing interest, the field lacks a common analytical lens.  We present the first unified framework for zero-shot RL. Our formulationintroduces a consistent notation and taxonomy that organizes existingapproaches and allows direct comparison between them. Central to our frameworkis the classification of algorithms into two families: direct representations,which learn end-to-end mappings from rewards to policies, and compositionalrepresentations, which decompose the representation leveraging the substructureof the value function. Within this framework, we highlight shared principlesand key differences across methods, and we derive an extended bound forsuccessor-feature methods, offering a new perspective on their performance inthe zero-shot regime. By consolidating existing work under a common lens, ourframework provides a principled foundation for future research in zero-shot RLand outlines a clear path toward developing more general agents.</description>
      <author>example@mail.com (Jacopo Di Ventura, Jan Felix Kleuker, Aske Plaat, Thomas Moerland)</author>
      <guid isPermaLink="false">2510.20542v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking</title>
      <link>http://arxiv.org/abs/2510.20335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code is at  https://github.com/ChampagneAndfragrance/Dino_Diffusion_Parking_Official&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Dino-Diffusion Parking (DDP)，一种结合视觉基础模型与基于扩散规划的自动停车流水线，解决了在不同环境条件下停车的鲁棒性问题。&lt;h4&gt;背景&lt;/h4&gt;停车是驾驶安全的关键支柱，尽管端到端方法在领域内取得了良好结果，但在天气和光照变化等条件下的鲁棒性仍是主要挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种领域无关的自动停车流水线，实现分布变化下的通用感知和鲁棒运动规划。&lt;h4&gt;方法&lt;/h4&gt;提出Dino-Diffusion Parking (DDP)，将视觉基础模型与基于扩散的规划相结合，在CARLA常规设置中训练，然后以零样本方式转移到更具挑战性的环境。&lt;h4&gt;主要发现&lt;/h4&gt;模型在所有测试的分布外场景中停车成功率 consistently保持在90%以上，消融研究证实网络架构和算法设计显著提高了跨域性能，在3D高斯飞溅环境中的测试显示了有希望的模拟到现实迁移能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的DDP方法在不同环境条件下都能实现高成功率的自动停车，并且具有从模拟到现实的迁移能力。&lt;h4&gt;翻译&lt;/h4&gt;停车是驾驶安全的关键支柱。尽管最近的端到端方法在领域内取得了有希望的结果，但在领域变化（如天气和光照变化）下的鲁棒性仍然是一个关键挑战。我们提出Dino-Diffusion Parking (DDP)，一个领域无关的自动停车流水线，它将视觉基础模型与基于扩散的规划相结合，以实现分布变化下的通用感知和鲁棒运动规划。我们在CARLA的常规设置中训练我们的流水线，并以零样本方式将其转移到更具挑战性的设置中。我们的模型在所有测试的分布外场景中停车成功率始终保持在90%以上，消融研究证实，网络架构和算法设计都显著提高了跨域性能，优于现有基线。此外，在从真实停车场重建的3D高斯飞溅环境中的测试显示了有希望的模拟到现实迁移。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶停车系统在不同环境条件下的跨域适应问题。当系统从训练环境（如晴天）转移到部署环境（如雨天、雾天或不同光照条件）时，性能会显著下降。这个问题很重要，因为停车占美国车辆事故的20%，且91%与倒车操作相关，准确的感知、规划和控制对安全至关重要。此外，传统解决方案需要大量收集不同条件下的数据，成本高昂，而本文方法无需额外数据就能适应各种环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将自动驾驶停车问题分解为感知、规划和控制三个模块，而非使用单一端到端模型。他们借鉴了多个现有工作：使用DINOv2视觉基础模型实现鲁棒感知，参考'Lift, Splat, Shoot'和'BEVFormer'进行BEV转换，借鉴机器人领域的扩散模型进行运动规划，并采用经典的Stanley控制器进行轨迹跟踪。作者特别设计了'后视目标重标记'的数据增强策略，通过人工扰动目标位置增强数据多样性，提高目标识别的鲁棒性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用模块化设计解耦感知和规划，利用视觉基础模型实现跨环境鲁棒感知，通过扩散模型减少运动规划中的累积误差，并结合经典控制器实现精确跟踪。整体流程：1)使用DINOv2处理摄像头图像生成鲁棒特征；2)将特征转换为鸟瞰图(BEV)表示；3)通过交叉注意力和FiLM结构将目标位置信息融合到BEV特征；4)扩散模型基于融合特征和目标位置预测轨迹；5)Stanley控制器根据预测轨迹生成控制命令执行停车。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次研究跨域自动驾驶停车问题，实现零样本迁移；2)模块化设计解耦感知和规划，避免过拟合；3)目标重标记数据增强技术提高目标识别鲁棒性；4)在SE(2)空间进行扩散运动规划，包含位置和方向信息；5)在3D高斯飞溅环境中验证模拟到真实世界的迁移能力。相比之前工作，本文方法在跨域场景中表现更好，不需要额外收集不同条件的数据，结合了模块化设计和扩散模型优势，并针对停车任务进行了专门优化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于视觉基础模型和扩散模型的模块化自动驾驶停车框架，实现了在无需额外数据的情况下跨环境零样本迁移的能力，显著提升了自动驾驶系统在不同天气、光照条件下的停车鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parking is a critical pillar of driving safety. While recent end-to-end (E2E)approaches have achieved promising in-domain results, robustness under domainshifts (e.g., weather and lighting changes) remains a key challenge. Ratherthan relying on additional data, in this paper, we propose Dino-DiffusionParking (DDP), a domain-agnostic autonomous parking pipeline that integratesvisual foundation models with diffusion-based planning to enable generalizedperception and robust motion planning under distribution shifts. We train ourpipeline in CARLA at regular setting and transfer it to more adversarialsettings in a zero-shot fashion. Our model consistently achieves a parkingsuccess rate above 90% across all tested out-of-distribution (OOD) scenarios,with ablation studies confirming that both the network architecture andalgorithmic design significantly enhance cross-domain performance over existingbaselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environmentreconstructed from a real-world parking lot demonstrates promising sim-to-realtransfer.</description>
      <author>example@mail.com (Zixuan Wu, Hengyuan Zhang, Ting-Hsuan Chen, Yuliang Guo, David Paz, Xinyu Huang, Liu Ren)</author>
      <guid isPermaLink="false">2510.20335v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Breakdance Video classification in the age of Generative AI</title>
      <link>http://arxiv.org/abs/2510.20287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究分析了现代视频基础模型在霹雳舞这一小众但流行的舞蹈体育中的应用性，发现视频编码器模型在预测任务上表现优于最先进的视频语言模型。&lt;h4&gt;背景&lt;/h4&gt;大型视觉语言模型已在多个体育用例中得到广泛应用，但大多数研究仅针对足球、板球、篮球等流行体育项目，专注于视觉问答和精彩片段生成等生成任务。&lt;h4&gt;目的&lt;/h4&gt;分析现代视频基础模型（包括编码器和解码器）在霹雳舞这一非常小众但极受欢迎的舞蹈体育中的应用性。&lt;h4&gt;方法&lt;/h4&gt;评估视频编码器模型和视频语言模型在霹雳舞视频分类任务上的表现，并提供编码器模型选择和微调解码器模型分析的见解。&lt;h4&gt;主要发现&lt;/h4&gt;视频编码器模型在预测任务上继续优于最先进的视频语言模型，研究提供了如何选择编码器模型的见解，并对微调后的解码器模型在霹雳舞视频分类中的工作机制进行了详细分析。&lt;h4&gt;结论&lt;/h4&gt;视频编码器模型在特定体育应用（如霹雳舞）中可能比视频语言模型更有效。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉语言模型最近在多个体育用例中得到了广泛应用。这些工作大多针对足球、板球、篮球等流行体育项目的一个有限子集，专注于视觉问答、精彩片段生成等生成任务。这项工作分析了现代视频基础模型（包括编码器和解码器）在霹雳舞这种非常小众但极受欢迎的舞蹈体育中的应用性。我们的结果表明，视频编码器模型在预测任务上继续优于最先进的视频语言模型。我们提供了如何选择编码器模型的见解，并对微调后的解码器模型在霹雳舞视频分类中的工作机制进行了详细分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Vision Language models have seen huge application in several sportsuse-cases recently. Most of these works have been targeted towards a limitedsubset of popular sports like soccer, cricket, basketball etc; focusing ongenerative tasks like visual question answering, highlight generation. Thiswork analyzes the applicability of the modern video foundation models (bothencoder and decoder) for a very niche but hugely popular dance sports -breakdance. Our results show that Video Encoder models continue to outperformstate-of-the-art Video Language Models for prediction tasks. We provideinsights on how to choose the encoder model and provide a thorough analysisinto the workings of a finetuned decoder model for breakdance videoclassification.</description>
      <author>example@mail.com (Sauptik Dhar, Naveen Ramakrishnan, Michelle Munson)</author>
      <guid isPermaLink="false">2510.20287v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Optimistic Task Inference for Behavior Foundation Models</title>
      <link>http://arxiv.org/abs/2510.20264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OpTI-BFM是一种改进的行为基础模型方法，通过直接对奖励函数不确定性建模和乐观决策，实现了在测试时仅通过环境交互来高效推断和优化奖励函数，显著减少了数据需求。&lt;h4&gt;背景&lt;/h4&gt;行为基础模型(BFMs)能够在测试时直接检索针对任何指定奖励函数的高性能策略，实现零样本强化学习。尽管这种方法在计算上高效，但在数据方面效率较低，因为它通常需要在非平凡的推断数据集上计算奖励，假设可以访问奖励的功能形式或需要大量标注工作。&lt;h4&gt;目的&lt;/h4&gt;解决BFMs在数据效率方面的问题，使模型能够通过在测试时仅与环境交互来进行任务推断，避免对奖励函数功能形式的依赖或大量标注工作。&lt;h4&gt;方法&lt;/h4&gt;提出OpTI-BFM，一种乐观决策标准，直接对奖励函数的不确定性进行建模，并指导BFMs进行任务推断的数据收集。通过与线性bandit的上置信度算法的直接连接，为训练良好的BFMs提供了遗憾界限。&lt;h4&gt;主要发现&lt;/h4&gt;在既定的零样本基准上评估OpTI-BFM后，观察到它使基于后继特征的BFMs能够在少量回合中识别和优化未见过的奖励函数，且计算开销最小。&lt;h4&gt;结论&lt;/h4&gt;OpTI-BFM解决了传统BFMs在数据效率方面的限制，使其能够在测试时仅通过环境交互来推断和优化任务，显著减少了数据需求。&lt;h4&gt;翻译&lt;/h4&gt;行为基础模型(BFMs)能够检索针对任何在测试时直接指定的奖励函数的高性能策略，通常被称为零样本强化学习(RL)。虽然这在计算方面是一个非常高效的过程，但在数据方面可能效率较低：作为标准假设，BFMs需要在非平凡的推断数据集上计算奖励，假设可以访问奖励的功能形式或需要大量的标注工作。为了减轻这些限制，我们解决了在测试时仅通过与环境交互来进行任务推断的问题。我们提出了OpTI-BFM，一种乐观决策标准，直接对奖励函数的不确定性进行建模，并指导BFMs进行任务推断的数据收集。形式上，我们通过与线性bandit的上置信度算法的直接连接，为训练良好的BFMs提供了遗憾界限。经验上，我们在既定的零样本基准上评估了OpTI-BFM，并观察到它使基于后继特征的BFMs能够在少量回合中识别和优化未见过的奖励函数，且计算开销最小。代码可在https://github.com/ThomasRupf/opti-bfm获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Behavior Foundation Models (BFMs) are capable of retrieving high-performingpolicy for any reward function specified directly at test-time, commonlyreferred to as zero-shot reinforcement learning (RL). While this is a veryefficient process in terms of compute, it can be less so in terms of data: as astandard assumption, BFMs require computing rewards over a non-negligibleinference dataset, assuming either access to a functional form of rewards, orsignificant labeling efforts. To alleviate these limitations, we tackle theproblem of task inference purely through interaction with the environment attest-time. We propose OpTI-BFM, an optimistic decision criterion that directlymodels uncertainty over reward functions and guides BFMs in data collection fortask inference. Formally, we provide a regret bound for well-trained BFMsthrough a direct connection to upper-confidence algorithms for linear bandits.Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, andobserve that it enables successor-features-based BFMs to identify and optimizean unseen reward function in a handful of episodes with minimal computeoverhead. Code is available at https://github.com/ThomasRupf/opti-bfm.</description>
      <author>example@mail.com (Thomas Rupf, Marco Bagatella, Marin Vlastelica, Andreas Krause)</author>
      <guid isPermaLink="false">2510.20264v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance</title>
      <link>http://arxiv.org/abs/2510.20119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列基础模型(TSFMs)与轻量级监督基线模型和经典模型性能相当，差距源于简单导入NLP或CV流程。时间序列数据不像图像和文本那样直接捕捉人类概念，因此'在线抓取一切'的范式不适用。进步需要从机会性聚合转向原则性设计，构建系统跨越保持时间语义不变性空间的数据集，并基于第一原理构建时间序列不变性本体论，以确保表示完整性，使TSFMs实现泛化、推理和真正涌现行为所需的对齐结构。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)数量不断增加，但轻量级监督基线模型甚至经典模型通常与它们表现相当。这种差距源于简单导入NLP或CV的流程。&lt;h4&gt;目的&lt;/h4&gt;提出需要从机会性聚合转向原则性设计：构建数据集，系统性地跨越保持时间语义的不变性空间。&lt;h4&gt;方法&lt;/h4&gt;建议基于第一原理构建时间序列不变性的本体论，通过不变性覆盖确保表示的完整性。&lt;h4&gt;主要发现&lt;/h4&gt;在语言和视觉领域，大规模网络语料库密集捕捉人类概念，但时间序列数据没有直接对应的概念，因此'在线抓取一切'的范式对时间序列不适用。&lt;h4&gt;结论&lt;/h4&gt;只有通过不变性覆盖确保表示的完整性，TSFMs才能实现泛化、推理和真正涌现行为所需的对齐结构。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)不断增加，然而轻量级监督基线和甚至经典模型常常与它们匹敌。我们认为这种差距源于简单导入NLP或CV流程。在语言和视觉中，大规模网络语料库密集捕捉人类概念，即有无数的苹果图像和文本。相比之下，时间序列数据设计用来补充图像和文本模态。没有包含'苹果'概念的时间序列数据集。因此，'在线抓取一切'的范式对时间序列不适用。我们认为进步需要从机会性转向原则性设计：构建数据集，系统性地跨越保持时间语义的不变性空间。为此，我们建议时间序列不变性的本体论应基于第一原理构建。只有通过不变性覆盖确保表示的完整性，TSFMs才能实现泛化、推理和真正涌现行为所需的对齐结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timeseries foundation models (TSFMs) have multiplied, yet lightweightsupervised baselines and even classical models often match them. We argue thisgap stems from the naive importation of NLP or CV pipelines. In language andvision, large web-scale corpora densely capture human concepts i.e. there arecountless images and text of apples. In contrast, timeseries data is built tocomplement the image and text modalities. There are no timeseries dataset thatcontains the concept apple. As a result, the scrape-everything-online paradigmfails for TS. We posit that progress demands a shift from opportunisticaggregation to principled design: constructing datasets that systematicallyspan the space of invariance that preserve temporal semantics. To this end, wesuggest that the ontology of timeseries invariances should be built based onfirst principles. Only by ensuring representational completeness throughinvariance coverage can TSFMs achieve the aligned structure necessary forgeneralisation, reasoning, and truly emergent behaviour.</description>
      <author>example@mail.com (Arian Prabowo, Flora D. Salim)</author>
      <guid isPermaLink="false">2510.20119v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models</title>
      <link>http://arxiv.org/abs/2510.20095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://imageomics.github.io/biocap/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了描述性字幕作为生物多模态基础模型的额外监督来源，通过使用多模态大语言模型生成合成字幕，训练出BIOCAP模型，在物种分类和文本图像检索方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;图像和字幕可以被视为物种潜在形态空间中的互补样本，各自捕捉特定的生物学特征。在训练中加入字幕可以促进与共享潜在结构的对齐，强调可能有诊断价值的特征，同时抑制虚假相关性。&lt;h4&gt;目的&lt;/h4&gt;解决生物有机体生物学中大规模获取忠实、实例特定字幕的挑战，以充分利用自然语言监督在生物多模态基础模型中的应用。&lt;h4&gt;方法&lt;/h4&gt;使用多模态大语言模型(MLLMs)生成合成字幕，这些字幕由维基百科衍生的视觉信息和特定于分类群的格式示例指导，以减少幻觉并产生准确的描述性字幕。然后使用这些字幕训练BIOCAP(即带有字幕的BIOCLIP)模型。&lt;h4&gt;主要发现&lt;/h4&gt;BIOCAP模型能够捕捉丰富的语义，并在物种分类和文本图像检索方面取得强大的性能。&lt;h4&gt;结论&lt;/h4&gt;描述性字幕在连接生物图像与多模态基础模型方面具有超越标签的价值。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了描述性字幕作为生物多模态基础模型的额外监督来源。图像和字幕可以被视为物种潜在形态空间中的互补样本，每种都捕捉了特定的生物学特征。在训练中加入字幕可以鼓励与这种共享潜在结构的对齐，强调可能有诊断价值的特征，同时抑制虚假相关性。然而，主要挑战在于大规模获取忠实、实例特定的字幕。这一要求限制了自然语言监督在生物有机体生物学中的应用，与其他许多科学领域相比。我们通过使用多模态大语言模型(MLLMs)生成合成字幕来弥补这一差距，这些字幕由维基百科衍生的视觉信息和特定于分类群的格式示例指导。这些特定领域的上下文有助于减少幻觉，并产生准确、基于实例的描述性字幕。使用这些字幕，我们训练了BIOCAP(即带有字幕的BIOCLIP)，这是一个能够捕捉丰富语义并在物种分类和文本图像检索方面取得强大性能的生物基础模型。这些结果证明了描述性字幕在连接生物图像与多模态基础模型方面超越标签的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work investigates descriptive captions as an additional source ofsupervision for biological multimodal foundation models. Images and captionscan be viewed as complementary samples from the latent morphospace of aspecies, each capturing certain biological traits. Incorporating captionsduring training encourages alignment with this shared latent structure,emphasizing potentially diagnostic characters while suppressing spuriouscorrelations. The main challenge, however, lies in obtaining faithful,instance-specific captions at scale. This requirement has limited theutilization of natural language supervision in organismal biology compared withmany other scientific domains. We complement this gap by generating syntheticcaptions with multimodal large language models (MLLMs), guided byWikipedia-derived visual information and taxon-tailored format examples. Thesedomain-specific contexts help reduce hallucination and yield accurate,instance-based descriptive captions. Using these captions, we train BIOCAP(i.e., BIOCLIP with Captions), a biological foundation model that captures richsemantics and achieves strong performance in species classification andtext-image retrieval. These results demonstrate the value of descriptivecaptions beyond labels in bridging biological images with multimodal foundationmodels.</description>
      <author>example@mail.com (Ziheng Zhang, Xinyue Ma, Arpita Chowdhury, Elizabeth G. Campolongo, Matthew J. Thompson, Net Zhang, Samuel Stevens, Hilmar Lapp, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao, Jianyang Gu)</author>
      <guid isPermaLink="false">2510.20095v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Surfer 2: The Next Generation of Cross-Platform Computer Use Agents</title>
      <link>http://arxiv.org/abs/2510.19949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 9 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Surfer 2是一个统一的架构，仅从视觉观察操作，在Web、桌面和移动三种环境中实现了最先进的性能，无需任务特定微调即可超越人类表现。&lt;h4&gt;背景&lt;/h4&gt;构建能够在网络、桌面和移动环境中通用的代理仍然是一个开放的挑战，因为之前的系统依赖于特定环境的接口，限制了跨平台部署。&lt;h4&gt;目的&lt;/h4&gt;介绍Surfer 2，一个统一的架构，仅从视觉观察操作，在所有三种环境中实现最先进的性能。&lt;h4&gt;方法&lt;/h4&gt;Surfer 2集成了分层上下文管理、解耦的规划和执行，以及自适应恢复的自验证， enabling可靠操作在长任务范围内。&lt;h4&gt;主要发现&lt;/h4&gt;在WebVoyager上达到97.1%的准确率，在WebArena上达到69.6%的准确率，在OSWorld上达到60.1%的准确率，在AndroidWorld上达到87.1%的准确率，超越了所有之前的系统，无需任务特定的微调，多次尝试后，Surfer 2在所有基准测试中超过了人类性能。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明，系统编排增强了基础模型的能力，仅通过视觉交互实现了通用计算机控制，同时呼吁新一代视觉语言模型以实现帕累托最优的成本效益。&lt;h4&gt;翻译&lt;/h4&gt;构建能够在网络、桌面和移动环境中通用的代理仍然是一个开放的挑战，因为之前的系统依赖于特定环境的接口，这限制了跨平台部署。我们介绍了Surfer 2，一个统一的架构，仅从视觉观察操作，在所有三种环境中实现最先进的性能。Surfer 2集成了分层上下文管理、解耦的规划和执行，以及自适应恢复的自验证， enabling可靠操作在长任务范围内。我们的系统在WebVoyager上达到97.1%的准确率，在WebArena上达到69.6%，在OSWorld上达到60.1%，在AndroidWorld上达到87.1%，超越了所有之前的系统，无需任务特定的微调。多次尝试后，Surfer 2在所有基准测试中超过了人类性能。这些结果表明，系统编排增强了基础模型的能力，仅通过视觉交互实现了通用计算机控制，同时呼吁新一代视觉语言模型以实现帕累托最优的成本效益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building agents that generalize across web, desktop, and mobile environmentsremains an open challenge, as prior systems rely on environment-specificinterfaces that limit cross-platform deployment. We introduce Surfer 2, aunified architecture operating purely from visual observations that achievesstate-of-the-art performance across all three environments. Surfer 2 integrateshierarchical context management, decoupled planning and execution, andself-verification with adaptive recovery, enabling reliable operation over longtask horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% onWebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all priorsystems without task-specific fine-tuning. With multiple attempts, Surfer 2exceeds human performance on all benchmarks. These results demonstrate thatsystematic orchestration amplifies foundation model capabilities and enablesgeneral-purpose computer control through visual interaction alone, whilecalling for a next-generation vision language model to achieve Pareto-optimalcost-efficiency.</description>
      <author>example@mail.com (Mathieu Andreux, Märt Bakler, Yanael Barbier, Hamza Ben Chekroun, Emilien Biré, Antoine Bonnet, Riaz Bordie, Nathan Bout, Matthias Brunel, Aleix Cambray, Pierre-Louis Cedoz, Antoine Chassang, Gautier Cloix, Ethan Connelly, Alexandra Constantinou, Ramzi De Coster, Hubert de la Jonquiere, Aurélien Delfosse, Maxime Delpit, Alexis Deprez, Augustin Derupti, Mathieu Diaz, Shannon D'Souza, Julie Dujardin, Abai Edmund, Michael Eickenberg, Armand Fatalot, Wissem Felissi, Isaac Herring, Xavier Koegler, Erwan Le Jumeau de Kergaradec, Aurélien Lac, Maxime Langevin, Corentin Lauverjat, Antonio Loison, Avshalom Manevich, Axel Moyal, Axel Nguyen Kerbel, Marinela Parovic, Julien Revelle, Guillaume Richard, Mats Richter, Ronan Riochet, María Santos, Romain Savidan, Laurent Sifre, Maxime Theillard, Marc Thibault, Ivan Valentini, Tony Wu, Laura Yie, Kai Yuan, Jevgenij Zubovskij)</author>
      <guid isPermaLink="false">2510.19949v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets</title>
      <link>http://arxiv.org/abs/2510.19944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Seed3D 1.0 Technical Report; Official Page on  https://seed.bytedance.com/seed3d&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Seed3D 1.0是一个基础模型，可以从单张图像生成可用于仿真的3D资产，解决了具身AI代理训练环境中的可扩展性问题，同时保持物理准确性。&lt;h4&gt;背景&lt;/h4&gt;开发具身AI代理需要平衡内容多样性和物理准确性的可扩展训练环境。现有世界模拟器存在局限：基于视频的方法内容多样但缺乏实时物理反馈，基于物理的引擎物理准确但受限于昂贵的手动资产创建。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从单张图像生成仿真就绪3D资产的基础模型，解决可扩展性挑战，同时保持物理严谨性。&lt;h4&gt;方法&lt;/h4&gt;提出Seed3D 1.0基础模型，生成具有准确几何、良好对齐纹理和真实物理材质的3D资产，可直接集成到物理引擎中，支持机器人操作和仿真训练，并能扩展到完整场景生成。&lt;h4&gt;主要发现&lt;/h4&gt;Seed3D 1.0生成的3D资产具有准确的几何结构、对齐良好的纹理和真实的物理材质，可直接用于物理引擎，支持机器人操作和仿真训练，并能扩展到完整场景生成。&lt;h4&gt;结论&lt;/h4&gt;Seed3D 1.0通过实现可扩展的仿真就绪内容创建，为推进基于物理的世界模拟器提供了基础，现已可在指定网址获取。&lt;h4&gt;翻译&lt;/h4&gt;开发具身AI代理需要可扩展的训练环境，这些环境需要在内容多样性和物理准确性之间取得平衡。世界模拟器提供了这样的环境，但面临不同的限制：基于视频的方法可以生成多样化的内容，但缺乏实时物理反馈以支持交互式学习；而基于物理的引擎能提供准确的动力学，但由于昂贵的手动资产创建而面临可扩展性限制。我们提出了Seed3D 1.0，这是一个基础模型，可以从单张图像生成仿真就绪的3D资产，解决了可扩展性挑战，同时保持物理严谨性。与现有的3D生成模型不同，我们的系统生成具有准确几何、对齐良好的纹理和真实物理材质的资产。这些资产可以直接集成到物理引擎中，只需最少的配置，支持在机器人操作和仿真训练中部署。除了单个对象外，系统还能通过将对象组装成连贯的环境来扩展到完整场景生成。通过实现可扩展的仿真就绪内容创建，Seed3D 1.0为推进基于物理的世界模拟器提供了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从单张图像生成高质量的、可直接用于物理仿真的3D资产的问题。这个问题重要是因为开发具身AI代理需要可扩展的训练环境，平衡内容多样性和物理准确性，而现有世界模拟器面临根本性权衡：基于视频的方法缺乏实时物理反馈，基于物理的引擎则受限于手动资产创建的可扩展性，制约了训练环境的多样性和规模。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有世界模拟器的局限性，认识到需要结合生成多样性和物理严谨性，设计了Seed3D 1.0基础模型。系统借鉴了现有工作：几何生成部分采用VAE和基于修正流的扩散Transformer架构；使用DINOv2和RADIO作为图像编码器；纹理生成部分借鉴多模态扩散Transformer；数据预处理参考3DShape2VecSet设计；训练基础设施采用FlashAttention和混合分片数据并行等技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合生成多样性和物理严谨性，解决3D资产创建的可扩展性问题，生成可直接用于物理仿真的高质量3D资产。整体流程包括：1)几何生成：使用Seed3D-VAE学习紧凑潜在表示，Seed3D-DiT合成3D形状；2)纹理生成：Seed3D-MV生成多视图图像，Seed3D-PBR分解为PBR材质图，Seed3D-UV补全UV纹理；3)数据处理：自动化预处理管道、格式标准化和质量过滤；4)训练和推理：采用渐进式策略训练模型，通过多阶段处理生成最终3D资产。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)高质量资产生成：生成具有精确几何、高分辨率纹理和真实物理材质的3D资产；2)物理引擎兼容性：资产可直接集成到物理引擎中；3)可扩展场景合成：从室内到城市环境实现连贯场景；4)技术创新：开发了Seed3D-VAE、Seed3D-DiT、Seed3D-MV、Seed3D-PBR和Seed3D-UV五个核心组件。相比之前工作，解决了几何伪影和纹理错位问题，通过UV纹理补全解决自遮挡，采用混合架构平衡跨模态学习和模态特定处理，使用长度感知时间步长维持生成质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Seed3D 1.0通过从单张图像生成高质量的、物理兼容的3D资产，解决了具身AI训练环境中内容多样性和物理准确性之间的权衡问题，为物理驱动的世界模拟器提供了可扩展的基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing embodied AI agents requires scalable training environments thatbalance content diversity with physics accuracy. World simulators provide suchenvironments but face distinct limitations: video-based methods generatediverse content but lack real-time physics feedback for interactive learning,while physics-based engines provide accurate dynamics but face scalabilitylimitations from costly manual asset creation. We present Seed3D 1.0, afoundation model that generates simulation-ready 3D assets from single images,addressing the scalability challenge while maintaining physics rigor. Unlikeexisting 3D generation models, our system produces assets with accurategeometry, well-aligned textures, and realistic physically-based materials.These assets can be directly integrated into physics engines with minimalconfiguration, enabling deployment in robotic manipulation and simulationtraining. Beyond individual objects, the system scales to complete scenegeneration through assembling objects into coherent environments. By enablingscalable simulation-ready content creation, Seed3D 1.0 provides a foundationfor advancing physics-based world simulators. Seed3D 1.0 is now available onhttps://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?modelId=doubao-seed3d-1-0-250928&amp;tab=Gen3D</description>
      <author>example@mail.com (Jiashi Feng, Xiu Li, Jing Lin, Jiahang Liu, Gaohong Liu, Weiqiang Lou, Su Ma, Guang Shi, Qinlong Wang, Jun Wang, Zhongcong Xu, Xuanyu Yi, Zihao Yu, Jianfeng Zhang, Yifan Zhu, Rui Chen, Jinxin Chi, Zixian Du, Li Han, Lixin Huang, Kaihua Jiang, Yuhan Li, Guan Luo, Shuguang Wang, Qianyi Wu, Fan Yang, Junyang Zhang, Xuanmeng Zhang)</author>
      <guid isPermaLink="false">2510.19944v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning</title>
      <link>http://arxiv.org/abs/2510.19893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as Oral on NeurIPS 2025 GenAI4Health Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为FairGRPO的分层强化学习方法，用于提高医学人工智能系统在不同人口统计群体中的诊断公平性，通过自适应重要性加权和无监督聚类处理缺失标签问题，实验表明该方法显著提高了预测平等性和F1分数。&lt;h4&gt;背景&lt;/h4&gt;医学人工智能系统在诊断方面取得显著成就，但在不同人口统计群体中表现出明显的性能差异，对代表性不足人群造成实际伤害。多模态推理基础模型推动了临床诊断，但通过强化学习进行的推理训练继承了并放大了训练数据集中的偏见。&lt;h4&gt;目的&lt;/h4&gt;提出一种促进跨异质临床人群公平学习的方法，解决临床领域常见的缺乏人口统计标签的问题。&lt;h4&gt;方法&lt;/h4&gt;引入Fairness-aware Group Relative Policy Optimization (FairGRPO)，一种分层强化学习方法，采用基于代表性、任务难度和数据源的自适应优势重要性加权。采用无监督聚类来处理缺失的人口统计标签，当标签不可用时自动发现潜在的人口统计群体。&lt;h4&gt;主要发现&lt;/h4&gt;在7个涵盖5种临床模态的临床诊断数据集上，FairGRPO与所有普通和偏见缓解的强化学习基线相比，将预测平等性提高了27.2%，同时F1分数提高了12.49%。训练动态分析显示，FairGRPO在整个优化过程中逐步改善公平性，而基线强化学习方法在训练过程中表现出公平性恶化。基于FairGRPO，发布了FairMedGemma-4B，一个公平感知的临床VLLM，在实现最先进性能的同时显著减少了不同人口统计群体之间的差异。&lt;h4&gt;结论&lt;/h4&gt;FairGRPO是一种有效的医学人工智能系统公平性提升方法，能够在不牺牲性能的情况下提高跨人群的诊断公平性，解决了临床数据中缺乏人口统计标签的常见问题。&lt;h4&gt;翻译&lt;/h4&gt;医学人工智能系统已取得显著的诊断能力，然而它们在不同人口统计群体中持续表现出性能差异，对代表性不足的人群造成实际伤害。虽然最近的多模态推理基础模型通过整合分析多样化的医疗数据推动了临床诊断的发展，但通过强化学习进行的推理训练继承了主导多数人群的训练数据集中存在的偏见，并往往放大这些偏见。我们引入了公平感知的群体相对策略优化（FairGRPO），这是一种分层强化学习方法，促进跨异质临床人群的公平学习。FairGRPO基于代表性、任务难度和数据源采用自适应的优势重要性加权。为解决临床领域中常见的人口统计标签缺失问题，我们进一步采用无监督聚类，当标签不可用时自动发现潜在的人口统计群体。在跨越X光、CT扫描、皮肤镜检查、乳腺X光检查和超声波5种临床模态的7个临床诊断数据集上进行综合实验，我们证明FairGRPO与所有普通和偏见缓解的强化学习基线相比，将预测平等性提高了27.2%，同时F1分数提高了12.49%。此外，训练动态分析显示，FairGRPO在整个优化过程中逐步改善公平性，而基线强化学习方法在训练过程中表现出公平性恶化。基于FairGRPO，我们发布了FairMedGemma-4B，一个公平感知的临床VLLM，在实现最先进性能的同时，显著减少了不同人口统计群体之间的差异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical artificial intelligence systems have achieved remarkable diagnosticcapabilities, yet they consistently exhibit performance disparities acrossdemographic groups, causing real-world harm to underrepresented populations.While recent multimodal reasoning foundation models have advanced clinicaldiagnosis through integrated analysis of diverse medical data, reasoningtrainings via reinforcement learning inherit and often amplify biases presentin training datasets dominated by majority populations. We introduceFairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchicalreinforcement learning approach that promotes equitable learning acrossheterogeneous clinical populations. FairGRPO employs adaptive importanceweighting of advantages based on representation, task difficulty, and datasource. To address the common issue of missing demographic labels in theclinical domain, we further employ unsupervised clustering, which automaticallydiscovers latent demographic groups when labels are unavailable. Throughcomprehensive experiments across 7 clinical diagnostic datasets spanning 5clinical modalities across X-ray, CT scan, dermoscropy, mammography andultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%against all vanilla and bias mitigated RL baselines, while improving F1 scoreby 12.49%. Furthermore, training dynamics analysis reveals that FairGRPOprogressively improves fairness throughout optimization, while baseline RLmethods exhibit deteriorating fairness as training progresses. Based onFairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM thatachieves state-of-the-art performance while demonstrating significantly reduceddisparities across demographic groups.</description>
      <author>example@mail.com (Shiqi Dai, Wei Dai, Jiaee Cheong, Paul Pu Liang)</author>
      <guid isPermaLink="false">2510.19893v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>SEMPO: Lightweight Foundation Models for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.19710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SEMPO是一种新型轻量级时间序列预测基础模型，通过两个创新模块在减少预训练数据规模和模型大小的同时实现强大的预测性能。&lt;h4&gt;背景&lt;/h4&gt;现有的时间序列基础模型虽然性能出色，但网络架构庞大，需要大规模数据集进行预训练，难以在资源受限环境中部署。&lt;h4&gt;目的&lt;/h4&gt;开发一种多功能且经济实惠的时间序列基础模型，解决现有模型在多功能性和可负担性之间的矛盾。&lt;h4&gt;方法&lt;/h4&gt;SEMPO包含两个关键模块：(1)能量感知的频谱分解模块，同时建模高能量和低能量但信息丰富的频率信号；(2)基于提示混合的Transformer，通过小型数据集特定提示学习异构时间模式，实现参数高效模型适应。&lt;h4&gt;主要发现&lt;/h4&gt;SEMPO在两个大规模基准测试(包含16个数据集)上的实验表明，与最先进方法相比，它在零样本和少样本预测场景中表现出优越性能，同时显著减少了预训练数据规模和模型大小。&lt;h4&gt;结论&lt;/h4&gt;SEMPO成功实现了时间序列预测领域多功能性和可负担性的平衡，为资源受限环境中的时间序列预测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近大型预训练模型的兴起见证了在时间序列预测领域开发基础模型的显著成功。尽管在各种下游预测任务中表现出令人印象深刻的性能，但现有时间序列基础模型拥有庞大的网络架构，需要在大规模数据集上进行大量预训练，这严重阻碍了它们在资源受限环境中的部署。为了应对多功能性和可负担性之间日益加剧的矛盾，我们提出了SEMPO，一种新型轻量级基础模型，它只需要在相对小规模的数据上进行预训练，却表现出强大的通用时间序列预测能力。具体而言，SEMPO包含两个关键模块：1)能量感知的频谱分解模块，通过不仅建模高能量频率信号，还建模当前方法中被忽略的低能量但信息丰富的频率信号，显著提高了预训练数据的利用率；以及2)基于提示混合的Transformer，通过小型数据集特定的提示学习异构时间模式，并将时间序列标记自适应路由到基于提示的专家，实现跨不同数据集和领域的参数高效模型适应。配备这些模块后，SEMPO显著减少了预训练数据规模和模型大小，同时实现了强大的泛化能力。在覆盖16个数据集的两个大规模基准测试上进行的广泛实验表明，与最先进的方法相比，SEMPO在零样本和少样本预测场景中表现出优越性能。代码和数据可在https://github.com/mala-lab/SEMPO获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent boom of large pre-trained models witnesses remarkable success indeveloping foundation models (FMs) for time series forecasting. Despiteimpressive performance across diverse downstream forecasting tasks, existingtime series FMs possess massive network architectures and require substantialpre-training on large-scale datasets, which significantly hinders theirdeployment in resource-constrained environments. In response to this growingtension between versatility and affordability, we propose SEMPO, a novellightweight foundation model that requires pretraining on relativelysmall-scale data, yet exhibits strong general time series forecasting.Concretely, SEMPO comprises two key modules: 1) energy-aware SpEctraldecomposition module, that substantially improves the utilization ofpre-training data by modeling not only the high-energy frequency signals butalso the low-energy yet informative frequency signals that are ignored incurrent methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learnsheterogeneous temporal patterns through small dataset-specific prompts andadaptively routes time series tokens to prompt-based experts forparameter-efficient model adaptation across different datasets and domains.Equipped with these modules, SEMPO significantly reduces both pre-training datascale and model size, while achieving strong generalization. Extensiveexperiments on two large-scale benchmarks covering 16 datasets demonstrate thesuperior performance of SEMPO in both zero-shot and few-shot forecastingscenarios compared with state-of-the-art methods. Code and data are availableat https://github.com/mala-lab/SEMPO.</description>
      <author>example@mail.com (Hui He, Kun Yi, Yuanchi Ma, Qi Zhang, Zhendong Niu, Guansong Pang)</author>
      <guid isPermaLink="false">2510.19710v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark</title>
      <link>http://arxiv.org/abs/2510.19585v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review. Both the dataset and code will be published&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一项从混合语言历史文档中提取拉丁语片段的新任务，并评估了大型基础模型在此任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;历史文档通常包含多种语言和不同的布局，从中提取特定语言片段具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;评估大型基础模型在从混合语言历史文档中提取拉丁语片段任务上的能力和局限性。&lt;h4&gt;方法&lt;/h4&gt;使用包含724个标注页面的多模态数据集，对大型基础模型进行了基准测试和性能评估。&lt;h4&gt;主要发现&lt;/h4&gt;当代模型能够可靠地检测和提取拉丁语片段。&lt;h4&gt;结论&lt;/h4&gt;该研究首次全面分析了大型基础模型在从混合语言历史文档中提取拉丁语片段任务上的能力和局限性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一项从混合语言历史文档中提取拉丁语片段的新任务，这些文档具有不同的布局。我们使用一个包含724个标注页面的多模态数据集，对大型基础模型进行了基准测试和性能评估。结果表明，使用当代模型进行可靠的拉丁语检测是可行的。我们的研究首次对这些模型在此任务上的能力和局限性进行了全面分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel task of extracting Latin fragments frommixed-language historical documents with varied layouts. We benchmark andevaluate the performance of large foundation models against a multimodaldataset of 724 annotated pages. The results demonstrate that reliable Latindetection with contemporary models is achievable. Our study provides the firstcomprehensive analysis of these models' capabilities and limits for this task.</description>
      <author>example@mail.com (Yu Wu, Ke Shu, Jonas Fischer, Lidia Pivovarova, David Rosson, Eetu Mäkelä, Mikko Tolonen)</author>
      <guid isPermaLink="false">2510.19585v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>GigaBrain-0: A World Model-Powered Vision-Language-Action Model</title>
      <link>http://arxiv.org/abs/2510.19430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://gigabrain0.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了GigaBrain-0，一个利用世界模型生成数据的新型视觉-语言-动作(VLA)基础模型，减少了对真实机器人数据的依赖，提高了跨任务泛化能力和策略鲁棒性，在灵巧操作、长视野和移动操作任务中实现了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;为通用机器人训练视觉-语言-动作(VLA)模型通常需要大规模的真实世界机器人数据，这些数据的收集既昂贵又耗时。物理数据收集的低效严重限制了当前VLA系统的可扩展性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决物理数据收集低效的问题，减少对真实机器人数据的依赖，同时提高VLA系统的跨任务泛化能力和策略鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1. 引入GigaBrain-0，由世界模型生成数据(如视频生成、真实到真实转移、人类转移、视角转移、仿真到真实转移数据)赋能的新型VLA基础模型；2. 利用世界模型大规模生成多样化数据；3. 通过RGBD输入建模和具身思维链(CoT)监督提高策略鲁棒性；4. 开发了GigaBrain-0-Small，一个优化的轻量级变体，可在NVIDIA Jetson AGX Orin等设备上高效运行。&lt;h4&gt;主要发现&lt;/h4&gt;1. GigaBrain-0在灵巧操作、长视野和移动操作任务中实现了显著的性能提升；2. 广泛的实验证明GigaBrain-0在外观(如纹理、颜色)、物体放置和摄像机视点变化方面具有优越的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过利用世界模型生成数据，GigaBrain-0显著减少了对真实机器人数据的依赖，同时提高了跨任务泛化能力和策略鲁棒性，为通用机器人提供了一个更高效、更可扩展的VLA解决方案。&lt;h4&gt;翻译&lt;/h4&gt;为通用机器人训练视觉-语言-动作(VLA)模型通常需要大规模的真实世界机器人数据，这些数据的收集既昂贵又耗时。物理数据收集的低效严重限制了当前VLA系统的可扩展性和泛化能力。为应对这一挑战，我们引入了GigaBrain-0，一个由世界模型生成数据(如视频生成、真实到真实转移、人类转移、视角转移、仿真到真实转移数据)赋能的新型VLA基础模型。通过利用世界模型大规模生成多样化数据，GigaBrain-0显著减少了对真实机器人数据的依赖，同时提高了跨任务泛化能力。我们的方法通过RGBD输入建模和具身思维链(CoT)监督进一步提高了策略鲁棒性，使模型能够在任务执行过程中推理空间几何、物体状态和长视野依赖关系。这导致在灵巧操作、长视野和移动操作任务中的实际性能显著提升。广泛的实验证明，GigaBrain-0在外观(如纹理、颜色)、物体放置和摄像机视点变化方面实现了优越的泛化能力。此外，我们提出了GigaBrain-0-Small，一个优化的轻量级变体，设计用于在NVIDIA Jetson AGX Orin等设备上高效运行。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决训练视觉-语言-行动（VLA）模型需要大规模真实世界机器人数据的问题，而收集这些数据既昂贵又耗时。这个问题很重要，因为它严重限制了当前VLA系统的可扩展性和泛化能力，阻碍了通用机器人在多样化环境中的应用。缺乏足够多样性的训练数据导致模型在现实世界中表现不佳，限制了机器人技术的实际部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到真实世界机器人数据收集的局限性，然后提出利用世界模型生成多样化训练数据的解决方案。他们设计了混合transformer架构，结合预训练视觉语言模型和动作扩散变换器，并引入RGB-D输入和具身思维链机制。作者借鉴了多项现有工作，包括π0等VLA模型架构、世界模型作为数据生成器、视觉语言模型如PaliGemma2、扩散模型用于视频生成，以及思维链推理和知识隔离技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用世界模型生成多样化、逼真的合成训练数据，减少对真实世界机器人数据的依赖，并通过RGB-D输入和具身思维链增强模型的感知和推理能力。整体流程包括：1）收集真实世界数据并利用GigaWorld生成多种合成数据（Real2Real转移、视图转移等）；2）采用混合transformer架构处理RGB-D输入和语言指令；3）训练模型生成具身思维链作为中间表示；4）基于思维链输出连续动作序列；5）提供轻量级版本适配边缘设备。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）整合多种世界模型生成的数据源，增强数据多样性；2）RGB-D输入建模提升3D空间理解；3）具身思维链监督机制改善推理能力；4）混合架构与知识隔离技术提高训练效率；5）高效数据生成与质量评估。相比之前工作，GigaBrain-0利用了更多样化的数据源（包括视图转移和Real2Real转移），具有更强的3D空间理解能力，能显式生成中间推理步骤，训练效率更高，在变化条件下泛化能力更强。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GigaBrain-0通过创新性地整合世界模型生成的多样化训练数据和具身思维链推理机制，显著提升了视觉-语言-行动模型在真实世界任务中的泛化能力和执行效率，同时大幅减少了对昂贵真实世界机器人数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training Vision-Language-Action (VLA) models for generalist robots typicallyrequires large-scale real-world robot data, which is expensive andtime-consuming to collect. The inefficiency of physical data collectionseverely limits the scalability, and generalization capacity of current VLAsystems. To address this challenge, we introduce GigaBrain-0, a novel VLAfoundation model empowered by world model-generated data (e.g., videogeneration, real2real transfer, human transfer, view transfer, sim2realtransfer data). By leveraging world models to generate diverse data at scale,GigaBrain-0 significantly reduces reliance on real robot data while improvingcross-task generalization. Our approach further improves policy robustnessthrough RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,enabling the model to reason about spatial geometry, object states, andlong-horizon dependencies during task execution. This leads to substantialgains in real-world performance on dexterous, long-horizon, and mobilemanipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achievessuperior generalization across variations in appearances (e.g., textures,colors), object placements, and camera viewpoints. Additionally, we presentGigaBrain-0-Small, an optimized lightweight variant designed to run efficientlyon devices such as the NVIDIA Jetson AGX Orin.</description>
      <author>example@mail.com (GigaBrain Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jie Li, Jiagang Zhu, Lv Feng, Peng Li, Qiuping Deng, Runqi Ouyang, Wenkang Qin, Xinze Chen, Xiaofeng Wang, Yang Wang, Yifan Li, Yilong Li, Yiran Ding, Yuan Xu, Yun Ye, Yukun Zhou, Zhehao Dong, Zhenan Wang, Zhichao Liu, Zheng Zhu)</author>
      <guid isPermaLink="false">2510.19430v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Using Temperature Sampling to Effectively Train Robot Learning Policies on Imbalanced Datasets</title>
      <link>http://arxiv.org/abs/2510.19373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种简单的策略训练采样方法，用于缓解机器人任务数据集中物理动作序列的不平衡问题。&lt;h4&gt;背景&lt;/h4&gt;随着越来越多的机器人动作和感官观测数据集被收集用于训练大型神经网络，发现许多基于不同描述的任务实际上涉及非常相似的身体动作序列，导致数据集在物理机器人动作方面存在严重不平衡。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单的采样策略来缓解机器人任务数据集中的动作不平衡问题，提高模型在多任务场景下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出一种简单的策略训练采样方法，只需几行代码即可集成到现有代码库中，并在预训练小型模型和微调大型基础模型上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;与之前最先进的方法相比，该方法在低资源任务上取得了显著改进，同时没有降低高资源任务上的性能，使得模型容量能够更有效地用于多任务策略。&lt;h4&gt;结论&lt;/h4&gt;在Franka Panda机械臂上的多样化任务中进一步验证了该方法的有效性，证明了其在实际应用中的可行性。&lt;h4&gt;翻译&lt;/h4&gt;越来越多的机器人动作和感官观测数据集被收集起来，用于训练日益庞大的神经网络。这些数据集是基于任务收集的，尽管这些任务在描述上可能不同，但许多任务涉及非常相似的身体动作序列（例如，'拿起苹果'与'拿起橙子'）。因此，许多机器人任务数据集在所代表的物理机器人动作方面存在严重不平衡。在这项工作中，我们提出了一种简单的策略训练采样方法来缓解这种不平衡。我们的方法只需要几行代码就可以集成到现有代码库中，并提高了泛化能力。我们在预训练小型模型和微调大型基础模型上都评估了我们的方法。结果表明，与之前最先进的方法相比，在低资源任务上取得了显著改进，同时没有降低高资源任务上的性能。这使得模型容量能够更有效地用于多任务策略。我们还进一步在Franka Panda机械臂上的多样化任务设置中验证了我们的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Increasingly large datasets of robot actions and sensory observations arebeing collected to train ever-larger neural networks. These datasets arecollected based on tasks and while these tasks may be distinct in theirdescriptions, many involve very similar physical action sequences (e.g., 'pickup an apple' versus 'pick up an orange'). As a result, many datasets of robotictasks are substantially imbalanced in terms of the physical robotic actionsthey represent. In this work, we propose a simple sampling strategy for policytraining that mitigates this imbalance. Our method requires only a few lines ofcode to integrate into existing codebases and improves generalization. Weevaluate our method in both pre-training small models and fine-tuning largefoundational models. Our results show substantial improvements on low-resourcetasks compared to prior state-of-the-art methods, without degrading performanceon high-resource tasks. This enables more effective use of model capacity formulti-task policies. We also further validate our approach in a real-worldsetup on a Franka Panda robot arm across a diverse set of tasks.</description>
      <author>example@mail.com (Basavasagar Patil, Sydney Belt, Jayjun Lee, Nima Fazeli, Bernadette Bucher)</author>
      <guid isPermaLink="false">2510.19373v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>AMAuT: A Flexible and Efficient Multiview Audio Transformer Framework Trained from Scratch</title>
      <link>http://arxiv.org/abs/2510.19368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AMAuT框架，一个无需预训练权重且支持任意采样率和音频长度的音频模型，在多个基准测试中达到高准确度同时大幅减少计算资源消耗。&lt;h4&gt;背景&lt;/h4&gt;最近的SSAST、EAT、HuBERT、Qwen-Audio和AudioFlamingo等基础模型在标准音频基准测试中表现优异，但受限于固定的输入速率和持续时间，影响了它们的重用性。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需依赖预训练权重、支持任意采样率和音频长度的音频分类框架，提高模型的灵活性和效率。&lt;h4&gt;方法&lt;/h4&gt;AMAuT集成了四个关键组件：增强驱动的多视图学习提高鲁棒性；conv1+conv7+conv1一维CNN瓶颈实现稳定的时间编码；双CLS+TAL令牌进行双向上下文表示；测试时自适应/增强(TTA²)提高推理可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;在AudioMNIST、SpeechCommands V1&amp;V2、VocalSound和CochlScene五个公共基准测试上，AMAuT准确度高达99.8%，同时消耗的GPU小时数不到可比预训练模型的3%。&lt;h4&gt;结论&lt;/h4&gt;AMAuT为大型预训练模型提供了一个高效且灵活的替代方案，使最先进的音频分类在计算受限环境中变得可行。&lt;h4&gt;翻译&lt;/h4&gt;最近的SSAST、EAT、HuBERT、Qwen-Audio和AudioFlamingo等基础模型在标准音频基准测试中取得了顶尖结果，但受限于固定的输入速率和持续时间，阻碍了它们的重用性。本文引入了增强驱动多视图音频变换器(AMAuT)，这是一个从头开始训练的框架，消除对预训练权重的依赖，同时支持任意采样率和音频长度。AMAuT集成了四个关键组件：(1)增强驱动的多视图学习，提高鲁棒性；(2)conv1+conv7+conv1一维CNN瓶颈，用于稳定的时间编码；(3)双CLS+TAL令牌，用于双向上下文表示；(4)测试时自适应/增强(TTA²)，提高推理可靠性。在AudioMNIST、SpeechCommands V1&amp;V2、VocalSound和CochlScene五个公共基准测试上的实验表明，AMAuT准确度高达99.8%，同时消耗的GPU小时数不到可比预训练模型的3%。因此，AMAuT为大型预训练模型提供了一个高效且灵活的替代方案，使最先进的音频分类在计算受限环境中变得可行。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent foundational models, SSAST, EAT, HuBERT, Qwen-Audio, and AudioFlamingo, achieve top-tier results across standard audio benchmarks but arelimited by fixed input rates and durations, hindering their reusability. Thispaper introduces the Augmentation-driven Multiview Audio Transformer (AMAuT), atraining-from-scratch framework that eliminates the dependency on pre-trainedweights while supporting arbitrary sample rates and audio lengths. AMAuTintegrates four key components: (1) augmentation-driven multiview learning forrobustness, (2) a conv1 + conv7 + conv1 one-dimensional CNN bottleneck forstable temporal encoding, (3) dual CLS + TAL tokens for bidirectional contextrepresentation, and (4) test-time adaptation/augmentation (TTA^2) to improveinference reliability. Experiments on five public benchmarks, AudioMNIST,SpeechCommands V1 &amp; V2, VocalSound, and CochlScene, show that AMAuT achievesaccuracies up to 99.8% while consuming less than 3% of the GPU hours requiredby comparable pre-trained models. Thus, AMAuT presents a highly efficient andflexible alternative to large pre-trained models, making state-of-the-art audioclassification accessible in computationally constrained settings.</description>
      <author>example@mail.com (Weichuang Shao, Iman Yi Liao, Tomas Henrique Bode Maul, Tissa Chandesa)</author>
      <guid isPermaLink="false">2510.19368v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Model Forecasts: Form and Function</title>
      <link>http://arxiv.org/abs/2510.19345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列基础模型(TSFMs)虽然预测准确性高，但预测形式（点预测、分位数预测、参数化预测或轨迹集合）决定了其实际应用价值。研究发现大多数TSFMs只能提供点或参数化预测，而实际操作任务常需要保留时间依赖性的轨迹集合。研究确定了预测类型间的转换条件，证明边际分布无法确定路径相关事件概率，并将六个基本预测任务映射到最小充分预测类型，表明预测类型而非准确性才是区分模型实用价值的关键。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)在预测准确性方面表现出色，但准确性并不完全决定其实际价值。&lt;h4&gt;目的&lt;/h4&gt;研究不同预测形式对实际操作任务的支持能力，确定预测类型间的转换条件，并提供任务对齐的评估框架。&lt;h4&gt;方法&lt;/h4&gt;分析现有TSFMs的预测类型，研究预测类型之间的转换条件，证明边际分布与联合分布的关系，并将基本预测任务映射到最小充分预测类型。&lt;h4&gt;主要发现&lt;/h4&gt;1. 三分之二的TSFMs只产生点或参数化预测，而许多操作任务需要保留时间依赖性的轨迹集合；2. 轨迹集合可通过边际化转换为简单形式，但反向转换需要额外方法；3. 边际分布无法确定路径相关事件概率，无限多联合分布可具有相同边际分布但给出不同操作答案；4. 六个基本预测任务可映射到最小充分预测类型。&lt;h4&gt;结论&lt;/h4&gt;在实际应用中，预测类型而非准确性是区分模型实用价值的关键因素。选择适当的预测形式对于支持特定操作任务至关重要。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)实现了强大的预测准确性，然而准确性本身并不决定实际价值。预测的形式——点预测、分位数预测、参数化预测或轨迹集合——从根本上限制了它能够支持的操作任务。我们调查了最近的TSFMs，发现三分之二只产生点预测或参数化预测，而许多操作任务需要保留时间依赖性的轨迹集合。我们确定了预测类型何时可以转换、何时不可以转换：轨迹集合可以通过边际化转换为更简单的形式而无需额外假设，但反向转换则需要通过copulas或conformal方法施加时间依赖性。我们证明了边际分布无法确定路径相关事件概率——无限多的联合分布具有相同的边际分布，但对操作问题给出不同的答案。我们将六个基本预测任务映射到最小充分预测类型，并提供了任务对齐的评估框架。我们的分析阐明了当预测类型而非准确性区分实用价值时的情况。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time-series foundation models (TSFMs) achieve strong forecast accuracy, yetaccuracy alone does not determine practical value. The form of a forecast --point, quantile, parametric, or trajectory ensemble -- fundamentally constrainswhich operational tasks it can support. We survey recent TSFMs and find thattwo-thirds produce only point or parametric forecasts, while many operationaltasks require trajectory ensembles that preserve temporal dependence. Weestablish when forecast types can be converted and when they cannot: trajectoryensembles convert to simpler forms via marginalization without additionalassumptions, but the reverse requires imposing temporal dependence throughcopulas or conformal methods. We prove that marginals cannot determinepath-dependent event probabilities -- infinitely many joint distributions shareidentical marginals but yield different answers to operational questions. Wemap six fundamental forecasting tasks to minimal sufficient forecast types andprovide a task-aligned evaluation framework. Our analysis clarifies whenforecast type, not accuracy, differentiates practical utility.</description>
      <author>example@mail.com (Alvaro Perez-Diaz, James C. Loach, Danielle E. Toutoungi, Lee Middleton)</author>
      <guid isPermaLink="false">2510.19345v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Slot Filling as a Reasoning Task for SpeechLLMs</title>
      <link>http://arxiv.org/abs/2510.19326v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出将推理能力整合到语音大语言模型中用于端到端槽填充任务，通过链式思维框架分解任务并创建推理数据集，实验表明混合语音LLM结合直接和推理模式表现最佳。&lt;h4&gt;背景&lt;/h4&gt;受到最近推理大语言模型发展的启发，研究者尝试将推理能力引入语音大语言模型。&lt;h4&gt;目的&lt;/h4&gt;通过链式思维框架将槽填充任务分解为多个推理步骤，创建推理数据集，并应用监督微调策略到语音大语言模型中。&lt;h4&gt;方法&lt;/h4&gt;区分常规和推理语音大语言模型，实验不同类型和大小的LLM作为文本基础模型，通过引入推理步骤展示性能改进。&lt;h4&gt;主要发现&lt;/h4&gt;引入推理步骤可提升性能；主要为数学、逻辑和编码领域开发的推理文本LLM作为基础模型时表现不佳；混合语音LLM结合直接和推理操作模式比单一模式微调的模型性能更好。&lt;h4&gt;结论&lt;/h4&gt;混合语音LLM（结合直接和推理模式）在性能上优于仅使用一种模式的模型，是更优的选择。&lt;h4&gt;翻译&lt;/h4&gt;我们提出将推理整合到语音大语言模型中用于端到端槽填充任务。受推理大语言模型最近发展的启发，我们使用链式思维框架将槽填充任务分解为多个推理步骤，创建推理数据集，并应用监督微调策略到语音大语言模型中。我们区分常规和推理语音大语言模型，并实验不同类型和大小的LLM作为它们的文本基础模型。我们通过引入推理（中间）步骤展示了性能改进。然而，我们表明主要为数学、逻辑和编码领域开发的推理文本LLM作为推理语音LLM的基础模型时可能表现不佳。我们进一步表明，构建在混合文本基础LLM上并微调以保留直接和推理操作模式的混合语音LLM，比仅使用一种操作模式微调的模型有更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose integration of reasoning into speech large language models(speechLLMs) for the end-to-end slot-filling task. Inspired by the recentdevelopment of reasoning LLMs, we use a chain-of-thought framework to decomposethe slot-filling task into multiple reasoning steps, create a reasoning datasetand apply the supervised fine-tuning strategy to a speechLLM. We distinguishbetween regular and reasoning speechLLMs and experiment with different typesand sizes of LLMs as their text foundation models. We demonstrate performanceimprovements by introducing reasoning (intermediate) steps. However, we showthat a reasoning textual LLM developed mainly for math, logic and codingdomains might be inferior as a foundation model for a reasoning speechLLM. Wefurther show that hybrid speechLLMs, built on a hybrid text foundation LLM andfine-tuned to preserve both direct and reasoning modes of operation, havebetter performance than those fine-tuned employing only one mode of operation.</description>
      <author>example@mail.com (Kadri Hacioglu, Manjunath K E, Andreas Stolcke)</author>
      <guid isPermaLink="false">2510.19326v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization</title>
      <link>http://arxiv.org/abs/2510.19325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为超体积优化(HVO)的新策略，用于解决大型语言模型在文本摘要任务中的多目标优化问题，通过动态调整奖励过程中的分数，使模型逐步逼近帕累托前沿，生成在多个维度上平衡的摘要。&lt;h4&gt;背景&lt;/h4&gt;文本摘要需要同时优化一致性、连贯性、相关性和流畅性等多个目标，这带来了很大挑战。虽然大型语言模型通过强化学习已展现出卓越性能，但很少有研究关注基于LLMs通过RL优化摘要的多目标问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的优化策略，用于解决基于大型语言模型的文本摘要任务中的多目标优化问题，生成在多个维度上更加平衡的摘要。&lt;h4&gt;方法&lt;/h4&gt;提出超体积优化(HVO)方法，在强化学习的奖励过程中使用超体积方法动态调整组之间的分数，引导模型优化逐步逼近帕累托前沿，从而在多个目标上生成平衡的摘要。&lt;h4&gt;主要发现&lt;/h4&gt;在多个代表性摘要数据集上的实验表明，HVO在总体得分上优于组相对策略优化(GRPO)，且在不同维度上表现更平衡。通过HVO增强的7B基础模型在摘要任务中表现与GPT-4相当，同时保持更短的生成长度。&lt;h4&gt;结论&lt;/h4&gt;HVO是一种有效的多目标优化方法，能够生成在多个维度上平衡的摘要，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;文本摘要是一项关键任务，需要同时优化一致性、连贯性、相关性和流畅性等多个目标，这带来了相当大的挑战。尽管大型语言模型已经展示了卓越的性能，并通过强化学习得到了增强，但很少有研究关注基于LLMs通过RL优化摘要的多目标问题。在本文中，我们引入了超体积优化(HVO)，一种新颖的优化策略，通过使用超体积方法在强化学习的奖励过程中动态调整组之间的分数。这种方法引导模型的优化逐步逼近帕累托前沿，从而在多个目标上生成平衡的摘要。在几个代表性摘要数据集上的实验结果表明，我们的方法在总体得分上优于组相对策略优化(GRPO)，并在不同维度上表现出更平衡的性能。此外，通过HVO增强的7B基础模型在摘要任务中表现与GPT-4相当，同时保持更短的生成长度。我们的代码已在https://github.com/ai4business-LiAuto/HVO.git公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text summarization is a crucial task that requires the simultaneousoptimization of multiple objectives, including consistency, coherence,relevance, and fluency, which presents considerable challenges. Although largelanguage models (LLMs) have demonstrated remarkable performance, enhanced byreinforcement learning (RL), few studies have focused on optimizing themulti-objective problem of summarization through RL based on LLMs. In thispaper, we introduce hypervolume optimization (HVO), a novel optimizationstrategy that dynamically adjusts the scores between groups during the rewardprocess in RL by using the hypervolume method. This method guides the model'soptimization to progressively approximate the pareto front, thereby generatingbalanced summaries across multiple objectives. Experimental results on severalrepresentative summarization datasets demonstrate that our method outperformsgroup relative policy optimization (GRPO) in overall scores and shows morebalanced performance across different dimensions. Moreover, a 7B foundationmodel enhanced by HVO performs comparably to GPT-4 in the summarization task,while maintaining a shorter generation length. Our code is publicly availableat https://github.com/ai4business-LiAuto/HVO.git</description>
      <author>example@mail.com (Junjie Song, Yiwen Liu, Dapeng Li, Yin Sun, Shukun Fu, Siqi Chen, Yuji Cao)</author>
      <guid isPermaLink="false">2510.19325v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Advances in 4D Representation: Geometry, Motion, and Interaction</title>
      <link>http://arxiv.org/abs/2510.19255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages. Project Page: https://mingrui-zhao.github.io/4DRep-GMI/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于4D生成和重建的调查论文，从4D表示的独特视角出发，帮助读者了解如何选择和定制适合自己任务的4D表示方法。&lt;h4&gt;背景&lt;/h4&gt;4D生成和重建是计算机图形学中一个快速发展的子领域，其发展受到神经场、几何和深度学习以及3D生成人工智能(GenAI)最近进展的推动。&lt;h4&gt;目的&lt;/h4&gt;帮助读者了解如何选择和定制适合自己任务的4D表示方法，以建模随时间演变的3D几何并展示运动和交互。&lt;h4&gt;方法&lt;/h4&gt;采用选择性方法，重点关注代表性工作，以突出不同计算、应用和数据场景下每种表示的理想特性和随之而来的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;将4D表示基于几何、运动和交互三个关键支柱进行分类；涵盖当前流行的表示方法如NeRFs和3DGS，以及相对未被充分探索的表示；讨论大型语言模型和视频基础模型在4D应用中的作用及其局限性；分析当前可用的4D数据集及推动领域发展所需的更多数据集。&lt;h4&gt;结论&lt;/h4&gt;选择和定制适当的4D表示对于完成特定任务至关重要。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了关于4D生成和重建的调查，这是计算机图形学中一个快速发展的子领域，其发展受到神经场、几何和深度学习以及3D生成人工智能(GenAI)最近进展的推动。虽然我们的调查不是首创，但我们从4D表示的独特视角构建了该领域的覆盖范围，用于建模随时间演变的3D几何并展示运动和交互。具体而言，我们没有提供大量工作的详尽列举，而是采用更选择性的方法，重点关注代表性工作，以突出不同计算、应用和数据场景下每种表示的理想特性和随之而来的挑战。我们旨在传达给读者的主要信息是如何为他们的任务选择和定制适当的4D表示。在组织上，我们基于三个关键支柱来区分4D表示：几何、运动和交互。我们的讨论不仅将涵盖当今最受欢迎的表示，如神经辐射场(NeRFs)和3D高斯溅射(3DGS)，还将引起对4D背景下相对未被充分探索的表示的关注，如结构化模型和长程运动。在整个调查中，我们将回顾大型语言模型(LLMs)和视频基础模型(VFMs)在多种4D应用中的作用，同时引导讨论它们当前的局限性以及如何解决这些局限性。我们还专门介绍了当前可用的4D数据集，以及推动该子领域发展所缺乏的数据集。项目页面：https://mingrui-zhao.github.io/4DRep-GMI/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决4D表示（随时间变化的3D几何形状）的系统分类和分析问题。这个问题在现实中非常重要，因为随着计算机图形学应用扩展到电影视觉效果、虚拟现实、自主机器人、医学成像和电子商务等领域，能够捕获、表示和操作4D内容已成为连接图形学、视觉和机器学习的基本挑战。4D表示技术能够帮助我们理解和建模动态世界，为各种应用提供基础支撑。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用以表示为中心的独特视角，而非像之前综述那样按应用或方法分类。他们借鉴了Cao等人、Fan等人、Miao等人的工作，但认为这些综述未能充分涵盖所有相关表示方法，特别是结构化表示、运动和交互方面。作者通过三个关键支柱（几何、运动和交互）构建分析框架，并在几何部分进一步区分结构化和非结构化表示，从而提供了一个更全面、更有条理的分析视角。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 作为一篇综述论文，其核心思想是从表示角度系统分类和分析4D表示方法，帮助研究人员理解不同表示的特性、优势和局限性。整体流程分为六个部分：1）引言介绍背景和问题；2）几何建模分析非结构化表示（网格、点云、NeRF、3D高斯飞溅）和结构化表示（模板、部件、图）；3）运动建模分析不同运动类型与表示的交互；4）交互建模讨论多实体交互表示；5）数据集、评估指标和基准测试；6）整体分析和未来方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）采用表示中心视角而非应用或方法分类；2）提出几何、运动、交互三支柱框架；3）明确区分结构化与非结构化表示；4）全面分析不同运动类型与表示选择的相互作用；5）专门讨论交互表示问题；6）探讨大型语言模型和视频基础模型在4D应用中的作用。相比之前工作，这篇论文提供了更全面的表示分类，更深入分析表示方法的优缺点和适用场景，并提供了如何为特定任务选择表示的实用指导。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过从几何、运动和交互三个关键支柱对4D表示方法进行系统分类和分析，为研究人员提供了如何为特定4D任务选择和定制适当表示的全面指导框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a survey on 4D generation and reconstruction, a fast-evolvingsubfield of computer graphics whose developments have been propelled by recentadvances in neural fields, geometric and motion deep learning, as well 3Dgenerative artificial intelligence (GenAI). While our survey is not the firstof its kind, we build our coverage of the domain from a unique and distinctiveperspective of 4D representations\/}, to model 3D geometry evolving over timewhile exhibiting motion and interaction. Specifically, instead of offering anexhaustive enumeration of many works, we take a more selective approach byfocusing on representative works to highlight both the desirable properties andensuing challenges of each representation under different computation,application, and data scenarios. The main take-away message we aim to convey tothe readers is on how to select and then customize the appropriate 4Drepresentations for their tasks. Organizationally, we separate the 4Drepresentations based on three key pillars: geometry, motion, and interaction.Our discourse will not only encompass the most popular representations oftoday, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS),but also bring attention to relatively under-explored representations in the 4Dcontext, such as structured models and long-range motions. Throughout oursurvey, we will reprise the role of large language models (LLMs) and videofoundational models (VFMs) in a variety of 4D applications, while steering ourdiscussion towards their current limitations and how they can be addressed. Wealso provide a dedicated coverage on what 4D datasets are currently available,as well as what is lacking, in driving the subfield forward. Projectpage:https://mingrui-zhao.github.io/4DRep-GMI/</description>
      <author>example@mail.com (Mingrui Zhao, Sauradip Nag, Kai Wang, Aditya Vora, Guangda Ji, Peter Chun, Ali Mahdavi-Amiri, Hao Zhang)</author>
      <guid isPermaLink="false">2510.19255v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>TinyUSFM: Towards Compact and Efficient Ultrasound Foundation Models</title>
      <link>http://arxiv.org/abs/2510.19239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submit to JBHI, 14 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TinyUSLM是一种通过知识蒸馏技术开发的轻量级超声基础模型，能够在保持优异性能的同时显著减少计算资源需求，使其适用于资源有限的临床环境。&lt;h4&gt;背景&lt;/h4&gt;医学成像的基础模型在多样化的解剖结构和临床应用中表现出优越的泛化能力，但其出色的性能依赖于大量计算资源，限制了在资源有限的临床环境中的部署。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级超声基础模型，能够在保持大规模超声基础模型(USFM)的优异器官多样性和任务适应性的同时，实现显著的计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出特征梯度驱动的核心集选择策略筛选高质量训练数据；开发域分离的掩码图像建模辅助一致性驱动的动态蒸馏保留空间和频域特性；建立包含8个分类和10个分割数据集的UniUS-Bench超声基准。&lt;h4&gt;主要发现&lt;/h4&gt;TinyUSLM仅使用20万张图像进行蒸馏，就能以仅6.36%的参数和6.40%的GFLOPs达到与USFM相当的性能；在分类和分割任务上分别比普通模型高出9.45%和7.72%；超越了所有最先进的轻量级模型；实现了84.91%的平均分类准确率和85.78%的平均分割Dice分数。&lt;h4&gt;结论&lt;/h4&gt;TinyUSLM成功实现了轻量级超声基础模型的开发，在保持优异性能的同时显著降低了计算资源需求，使其适用于资源有限的临床环境，为医学成像领域提供了实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;医学成像基础模型在多样化的解剖结构和临床应用中表现出优越的泛化能力。它们的出色性能依赖于大量计算资源，限制了在资源有限的临床环境中的部署。本文提出了TinyUSLM，这是第一个轻量级超声基础模型，通过使用精心筛选的小型数据集进行知识蒸馏，保持了我们的大规模超声基础模型(USFM)的卓越器官多样性和任务适应性，在不牺牲性能的情况下提供了显著的计算效率。考虑到轻量级模型的有限容量和表示能力，我们提出了一个特征梯度驱动的核心集选择策略，用于筛选高质量的紧凑训练数据，避免因低质量冗余图像导致的训练退化。为了在知识转移过程中保留基本的空间和频域特性，我们开发了域分离的掩码图像建模辅助一致性驱动的动态蒸馏。这个新颖的框架通过利用教师模型在不同域掩码上的一致性，自适应地从大型基础模型转移知识，专门针对超声解释进行定制。为了评估，我们建立了UniUS-Bench，这是最大的公开可用超声基准，包含跨15个器官的8个分类和10个分割数据集。仅使用20万张图像进行蒸馏，TinyUSLM就能以仅6.36%的参数和6.40%的GFLOPs达到USLM的性能。TinyUSLM在分类和分割任务上分别比普通模型高出9.45%和7.72%，超越了所有最先进的轻量级模型，并在各种医疗设备和中心实现了84.91%的平均分类准确率和85.78%的平均分割Dice分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for medical imaging demonstrate superior generalizationcapabilities across diverse anatomical structures and clinical applications.Their outstanding performance relies on substantial computational resources,limiting deployment in resource-constrained clinical environments. This paperpresents TinyUSFM, the first lightweight ultrasound foundation model thatmaintains superior organ versatility and task adaptability of our large-scaleUltrasound Foundation Model (USFM) through knowledge distillation withstrategically curated small datasets, delivering significant computationalefficiency without sacrificing performance. Considering the limited capacityand representation ability of lightweight models, we propose a feature-gradientdriven coreset selection strategy to curate high-quality compact training data,avoiding training degradation from low-quality redundant images. To preservethe essential spatial and frequency domain characteristics during knowledgetransfer, we develop domain-separated masked image modeling assistedconsistency-driven dynamic distillation. This novel framework adaptivelytransfers knowledge from large foundation models by leveraging teacher modelconsistency across different domain masks, specifically tailored for ultrasoundinterpretation. For evaluation, we establish the UniUS-Bench, the largestpublicly available ultrasound benchmark comprising 8 classification and 10segmentation datasets across 15 organs. Using only 200K images in distillation,TinyUSFM matches USFM's performance with just 6.36% of parameters and 6.40% ofGFLOPs. TinyUSFM significantly outperforms the vanilla model by 9.45% inclassification and 7.72% in segmentation, surpassing all state-of-the-artlightweight models, and achieving 84.91% average classification accuracy and85.78% average segmentation Dice score across diverse medical devices andcenters.</description>
      <author>example@mail.com (Chen Ma, Jing Jiao, Shuyu Liang, Junhu Fu, Qin Wang, Zeju Li, Yuanyuan Wang, Yi Guo)</author>
      <guid isPermaLink="false">2510.19239v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Implicit Biases of Design Choices for Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2510.19236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过理论和实证方法分析了时间序列基础模型(TSFMs)中设计选择的影响，揭示了不同设计如何导致模型中的隐式偏置，以及这些偏置如何影响模型行为，研究结果对于理解和改进未来TSFMs的发展具有重要意义。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)是一类强大的通用工具，用于时间序列预测和相关时间任务，但这些模型的行为受到其设计中微妙归纳偏置的强烈影响。&lt;h4&gt;目的&lt;/h4&gt;理解训练过程中的各种'旋钮'如何影响模型质量，而非开发一个声称比现有TSFMs更好的新模型；探讨设计选择如何导致模型基本属性中的隐式偏置。&lt;h4&gt;方法&lt;/h4&gt;使用理论和受控经验评估相结合的方法；识别几种设计选择(如patch大小、嵌入选择、训练目标等)；研究这些设计选择如何影响模型的基本属性。&lt;h4&gt;主要发现&lt;/h4&gt;不同的设计选择会导致模型基本属性中的隐式偏置；这些偏置可能是直观的或非常违反直觉的，取决于模型和数据的特性；在异常值处理的案例研究中，展示了多种偏置如何以复杂方式相互作用；讨论了研究结果对学习'苦涩教训'和构建TSFMs的启示。&lt;h4&gt;结论&lt;/h4&gt;理解设计选择对模型行为的影响对于构建有效的TSFMs至关重要；模型中的隐式偏置可以是有益的，但也可能导致意想不到的行为。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)是一类潜在的强大通用工具，用于时间序列预测和相关时间任务，但它们的行为受到其设计中微妙归纳偏置的强烈影响。我们不是开发一个新模型并声称它比现有的TSFMs更好，例如通过在现有成熟的基准测试中获胜，我们的目标是理解训练过程中的各种'旋钮'如何影响模型质量。结合理论和受控经验评估，我们确定了几个设计选择(补丁大小、嵌入选择、训练目标等)，并展示了它们如何导致模型基本属性中的隐式偏置(时间行为、几何结构、模型回归到均值的激进程度等)；我们展示了这些偏置如何可能是直观的或非常违反直觉的，这取决于模型和数据的特性。我们还在异常值处理的案例研究中说明了多种偏置如何以复杂方式相互作用；我们讨论了我们的结果对学习苦涩教训和构建TSFMs的启示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series foundation models (TSFMs) are a class of potentially powerful,general-purpose tools for time series forecasting and related temporal tasks,but their behavior is strongly shaped by subtle inductive biases in theirdesign. Rather than developing a new model and claiming that it is better thanexisting TSFMs, e.g., by winning on existing well-established benchmarks, ourobjective is to understand how the various ``knobs'' of the training processaffect model quality. Using a mix of theory and controlled empiricalevaluation, we identify several design choices (patch size, embedding choice,training objective, etc.) and show how they lead to implicit biases infundamental model properties (temporal behavior, geometric structure, howaggressively or not the model regresses to the mean, etc.); and we show howthese biases can be intuitive or very counterintuitive, depending on propertiesof the model and data. We also illustrate in a case study on outlier handlinghow multiple biases can interact in complex ways; and we discuss implicationsof our results for learning the bitter lesson and building TSFMs.</description>
      <author>example@mail.com (Annan Yu, Danielle C. Maddix, Boran Han, Xiyuan Zhang, Abdul Fatir Ansari, Oleksandr Shchur, Christos Faloutsos, Andrew Gordon Wilson, Michael W. Mahoney, Yuyang Wang)</author>
      <guid isPermaLink="false">2510.19236v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions</title>
      <link>http://arxiv.org/abs/2510.19060v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 9 figures. Metric/benchmark available at  https://github.com/amith-ananthram/posh&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PoSh的新型评估指标，用于评估视觉语言模型生成的详细图像描述，并引入了DOCENT数据集作为基准测试。PoSh使用场景图作为结构化评分指南，能够更好地模拟人类评分行为，并且在多个方面优于现有评估方法。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)已发展到能够生成详细的图像描述，但评估这些描述仍然面临挑战。现有标准评估指标(如CIDEr、SPICE)是为短文本设计的，主要针对现在已不常见的错误类型(如对象识别错误)进行调整，无法有效评估长文本中属性和关系的连接性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够评估详细图像描述的新指标，特别关注长文本中属性和关系的连接性，并能将错误定位到特定文本跨度。同时创建一个具有挑战性的新数据集来验证该指标的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出PoSh评估指标，利用场景图作为结构化评分指南指导大语言模型作为评判者，产生基于细粒度错误的聚合分数。同时创建DOCENT数据集，包含艺术品、专家参考描述和模型生成描述，配有艺术史学生的质量评估。通过PoSh评估开放和封闭模型在描述绘画、素描和雕像方面的性能。&lt;h4&gt;主要发现&lt;/h4&gt;PoSh在DOCENT上与人类判断的相关性比最佳开源替代方案更强，对图像类型具有鲁棒性，且作为奖励函数优于标准监督微调。研究发现基础模型难以实现对具有丰富场景动态的图像的完整、无错误覆盖，确立了评估VLM进展的新任务标准。&lt;h4&gt;结论&lt;/h4&gt;PoSh和DOCENT为评估详细图像描述提供了新工具，有望促进辅助文本生成等重要领域的进步，为视觉语言模型的发展提供更准确的评估方法。&lt;h4&gt;翻译&lt;/h4&gt;虽然视觉语言模型已经发展到能够进行详细的图像描述，但评估仍然是一个挑战。标准指标是为短文本设计的，并且调整为识别现在不常见的错误，如对象识别错误。相比之下，长文本需要对属性和关系连接的敏感性，以及将错误定位到特定文本跨度的评分。在这项工作中，我们介绍了PoSh，一种用于详细图像描述的指标，它使用场景图作为结构化评分指南来指导大语言模型作为评判者，产生基于细粒度错误的聚合分数。PoSh是可复制的、可解释的，并且比现有指标更好地模拟人类评分者的行为。为了验证PoSh，我们引入了一个具有挑战性的新数据集DOCENT。这个新的基准包含艺术品，配以专家撰写的参考文本和模型生成的描述，并附有艺术史学生对它们质量的细致和粗略判断。因此，DOCENT能够在一个具有挑战性的新领域评估详细的图像描述指标和详细的图像描述本身。我们表明，PoSh在DOCENT上与人类判断的相关性比最佳开源替代方案更强，对图像类型具有鲁棒性，并且是一个有效的奖励函数，优于标准的监督微调。然后，使用PoSh，我们描述了开放和封闭模型在描述绘画、素描和雕像方面的性能，发现基础模型难以实现对具有丰富场景动态的图像的完整、无错误的覆盖，从而确立了一个具有挑战性的新任务来衡量VLM的进展。通过PoSh和DOCENT，我们希望能够在辅助文本生成等重要领域取得进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While vision-language models (VLMs) have advanced into detailed imagedescription, evaluation remains a challenge. Standard metrics (e.g. CIDEr,SPICE) were designed for short texts and tuned to recognize errors that are nowuncommon, such as object misidentification. In contrast, long texts requiresensitivity to attribute and relation attachments and scores that localizeerrors to particular text spans. In this work, we introduce PoSh, a metric fordetailed image description that uses scene graphs as structured rubrics toguide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grainederrors (e.g. mistakes in compositional understanding). PoSh is replicable,interpretable and a better proxy for human raters than existing metrics(including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging newdataset, DOCENT. This novel benchmark contains artwork, paired withexpert-written references, and model-generated descriptions, augmented withgranular and coarse judgments of their quality from art history students. Thus,DOCENT enables evaluating both detailed image description metrics and detailedimage description itself in a challenging new domain. We show that PoShachieves stronger correlations (+0.05 Spearman $\rho$) with the human judgmentsin DOCENT than the best open-weight alternatives, is robust to image type(using CapArena, an existing dataset of web imagery) and is a capable rewardfunction, outperforming standard supervised fine-tuning. Then, using PoSh, wecharacterize the performance of open and closed models in describing thepaintings, sketches and statues in DOCENT and find that foundation modelsstruggle to achieve full, error-free coverage of images with rich scenedynamics, establishing a demanding new task to gauge VLM progress. Through bothPoSh and DOCENT, we hope to enable advances in important areas such asassistive text generation.</description>
      <author>example@mail.com (Amith Ananthram, Elias Stengel-Eskin, Lorena A. Bradford, Julia Demarest, Adam Purvis, Keith Krut, Robert Stein, Rina Elster Pantalony, Mohit Bansal, Kathleen McKeown)</author>
      <guid isPermaLink="false">2510.19060v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models</title>
      <link>http://arxiv.org/abs/2510.20222v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为QKCV的注意力机制，通过融入静态类别嵌入来增强传统QKV框架，提高时间序列预测准确性。&lt;h4&gt;背景&lt;/h4&gt;在现实世界的时间序列预测任务中，类别信息在捕捉固有数据模式方面起着关键作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用类别信息提高预测准确性的注意力机制。&lt;h4&gt;方法&lt;/h4&gt;引入QKCV（Query-Key-Category-Value）注意力机制，作为传统QKV框架的扩展，融入静态类别嵌入C来强调特定类别的信息。&lt;h4&gt;主要发现&lt;/h4&gt;QKCV作为即插即用模块能增强多种基于注意力的模型在现实数据集上的预测准确性；在微调单变量时间序列基础模型时，只需更新静态嵌入C，保留预训练权重，减少计算开销并提高微调性能。&lt;h4&gt;结论&lt;/h4&gt;QKCV注意力机制能有效利用类别信息提高时间序列预测的准确性，具有良好的适应性和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界的时间序列预测任务中，类别信息在捕捉固有数据模式方面起着关键作用。本文引入了QKCV（查询-键-类别-值）注意力，这是传统QKV框架的扩展，融入了静态类别嵌入C来强调特定类别的信息。作为一个通用的即插即用模块，QKCV增强了基于注意力的模型（如普通Transformer、Informer、PatchTST、TFT）在各种现实世界数据集上的预测准确性。此外，QKCV在通过仅更新静态嵌入C同时保留预训练权重来微调单变量时间序列基础模型时表现出显著的适应性，从而减少了计算开销并实现了更好的微调性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world time series forecasting tasks, category information plays apivotal role in capturing inherent data patterns. This paper introduces QKCV(Query-Key-Category-Value) attention, an extension of the traditional QKVframework that incorporates a static categorical embedding C to emphasizecategory-specific information. As a versatile plug-in module, QKCV enhances theforecasting accuracy of attention-based models (e.g., Vanilla Transformer,Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCVdemonstrates remarkable adaptability in fine-tuning univariate time seriesfoundation model by solely updating the static embedding C while preservingpretrained weights, thereby reducing computational overhead and achievingsuperior fine-tuning performance.</description>
      <author>example@mail.com (Hao Wang, Baojun Ma)</author>
      <guid isPermaLink="false">2510.20222v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>MobiAct: Efficient MAV Action Recognition Using MobileNetV4 with Contrastive Learning and Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2510.19273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个轻量级的MAV动作识别框架MobiAct，实现了高精度与低计算成本的平衡，在保持92.12%平均识别准确率的同时，仅消耗136.16 pJ能量并以每秒8.84个动作的速度处理，解码速度比领先方法快2倍。&lt;h4&gt;背景&lt;/h4&gt;微型飞行器(MAV)的精确高效运动识别对于自主空中群体的实时感知和协调至关重要。然而，现有方法大多依赖于大型、计算密集型模型，不适合资源有限的MAV平台，导致识别精度和推理速度之间的权衡。&lt;h4&gt;目的&lt;/h4&gt;提出一种轻量级的MAV动作识别框架MobiAct，旨在以低计算成本实现高精度的MAV动作识别。&lt;h4&gt;方法&lt;/h4&gt;采用MobileNetV4作为骨干网络；引入分阶段正交知识蒸馏(SOKD)策略将教师网络(ResNet18)的MAV运动特征有效转移到学生网络；集成无参数注意力机制提高识别精度而不增加模型复杂度；开发混合损失训练策略结合多个损失目标确保训练过程的稳定和鲁棒优化。&lt;h4&gt;主要发现&lt;/h4&gt;MobiAct实现了低能耗、低计算的MAV动作识别；在所有三个自收集数据集上，平均识别准确率达到92.12%；仅消耗136.16 pJ能量，处理速度为每秒8.84个动作；动作解码速度比领先方法快2倍，同时保持高度相当的识别精度。&lt;h4&gt;结论&lt;/h4&gt;MobiAct在MAV动作识别方面展现出卓越的效率，成功解决了识别精度与计算资源消耗之间的权衡问题。&lt;h4&gt;翻译&lt;/h4&gt;微型飞行器(MAV)运动的精确高效识别对于自主空中群体的实时感知和协调至关重要。然而，大多数现有方法依赖于大型、计算密集型模型，不适合资源有限的MAV平台，这导致了识别精度和推理速度之间的权衡。为解决这些挑战，本文提出了一个轻量级的MAV动作识别框架MobiAct，旨在以低计算成本实现高精度。具体而言，MobiAct采用MobileNetV4作为骨干网络，并引入分阶段正交知识蒸馏(SOKD)策略，将MAV运动特征从教师网络(ResNet18)有效转移到学生网络，从而提高知识转移效率。此外，架构中集成了无参数注意力机制，在不增加模型复杂度的情况下提高识别精度。此外，还开发了混合损失训练策略，结合多个损失目标，确保训练过程中的稳定和鲁棒优化。实验结果表明，所提出的MobiAct实现了低能耗、低计算的MAV动作识别，同时在比较的方法中保持最快的动作解码速度。在所有三个自收集数据集上，MobiAct平均识别准确率达到92.12%，而仅消耗136.16 pJ的能量，并以每秒8.84个动作的速度进行识别。值得注意的是，MobiAct的动作解码速度比领先方法快2倍，同时具有高度相当的识别精度，突显了其在MAV动作识别方面的卓越效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and efficient recognition of Micro Air Vehicle (MAV) motion isessential for enabling real-time perception and coordination in autonomousaerial swarm. However, most existing approaches rely on large, computationallyintensive models that are unsuitable for resource-limited MAV platforms, whichresults in a trade-off between recognition accuracy and inference speed. Toaddress these challenges, this paper proposes a lightweight MAV actionrecognition framework, MobiAct, designed to achieve high accuracy with lowcomputational cost. Specifically, MobiAct adopts MobileNetV4 as the backbonenetwork and introduces a Stage-wise Orthogonal Knowledge Distillation (SOKD)strategy to effectively transfer MAV motion features from a teacher network(ResNet18) to a student network, thereby enhancing knowledge transferefficiency. Furthermore, a parameter-free attention mechanism is integratedinto the architecture to improve recognition accuracy without increasing modelcomplexity. In addition, a hybrid loss training strategy is developed tocombine multiple loss objectives, which ensures stable and robust optimizationduring training. Experimental results demonstrate that the proposed MobiActachieves low-energy and low-computation MAV action recognition, whilemaintaining the fastest action decoding speed among compared methods. Acrossall three self-collected datasets, MobiAct achieves an average recognitionaccuracy of 92.12%, while consuming only 136.16 pJ of energy and processingrecognition at a rate of 8.84 actions per second. Notably, MobiAct decodesactions up to 2 times faster than the leading method, with highly comparablerecognition accuracy, highlighting its superior efficiency in MAV actionrecognition.</description>
      <author>example@mail.com (Zhang Nengbo, Ho Hann Woei)</author>
      <guid isPermaLink="false">2510.19273v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
  <item>
      <title>X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning</title>
      <link>http://arxiv.org/abs/2510.19150v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了X-Ego-CS基准数据集和交叉自我中心对比学习(CECL)方法，用于研究电竞游戏中的多智能体决策和团队战术学习。&lt;h4&gt;背景&lt;/h4&gt;人类团队战术源于个人视角及其预测、解释和适应队友意图的能力。现有视频理解研究虽改善了体育中团队互动建模，但大多依赖第三方广播视角，忽视了多智能体学习的同步、自我中心特性。&lt;h4&gt;目的&lt;/h4&gt;引入X-Ego-CS基准数据集，促进复杂3D环境中多智能体决策的研究，并提供交叉自我中心视角来捕捉团队互动。&lt;h4&gt;方法&lt;/h4&gt;X-Ego-CS数据集包含45场专业级《反恐精英2》比赛的124小时游戏录像，提供所有玩家的同步第一人称视角和状态-行动轨迹。提出CECL方法，对齐队友的自我中心视觉流，培养团队战术情境意识。&lt;h4&gt;主要发现&lt;/h4&gt;CECL能有效增强智能体从单一第一人称视图推断队友和对手位置的能力，使用最先进的视频编码器实现了有效性能。&lt;h4&gt;结论&lt;/h4&gt;X-Ego-CS和CECL为电竞中的交叉自我中心多智能体基准测试奠定基础，将游戏理解定位为多智能体建模和战术学习的测试平台，对虚拟和现实领域中的时空推理和人类-AI团队协作具有启示意义。&lt;h4&gt;翻译&lt;/h4&gt;人类团队战术源于每个球员的个人视角及其预测、解释和适应队友意图的能力。尽管视频理解方面的进展已改善了体育中团队互动的建模，但大多数现有工作依赖第三方广播视角，并忽视了多智能体学习的同步、自我中心特性。我们引入X-Ego-CS基准数据集，包含来自45场专业级流行电竞游戏《反恐精英2》的124小时游戏录像，旨在促进复杂3D环境中多智能体决策的研究。X-Ego-CS提供交叉自我中心视频流，同步捕捉所有玩家的第一人称视角以及状态-行动轨迹。基于此资源，我们提出交叉自我中心对比学习(CECL)，对齐队友的自我中心视觉流，从个人视角培养团队层面的战术情境意识。我们在队友-对手位置预测任务上评估CECL，证明了其有效性，能够增强智能体使用最先进的视频编码器从单一第一人称视图推断队友和对手位置的能力。X-Ego-CS和CECL共同为电竞中的交叉自我中心多智能体基准测试奠定基础。更广泛地说，我们的工作将游戏理解定位为多智能体建模和战术学习的测试平台，对虚拟和现实领域中的时空推理以及人类-AI团队协作具有启示意义。代码和数据集可在https://github.com/HATS-ICT/x-ego获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何让AI系统从团队成员的第一人称视角中获得团队层面的战术态势感知能力。这个问题在现实中很重要，因为真实世界的团队协作（如体育竞技、军事行动、应急响应等）需要参与者能够根据队友和对手的意图来协调行动，而现有方法大多依赖第三人称视角，无法捕捉个体感知和协调的第一人称特性，限制了智能体在部分可观测环境中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有团队行为建模方法的局限性，特别是在处理部分可观测环境中的团队协调时。他们借鉴了体育理解中的第三人称分析方法、游戏理解中的人-AI协作研究以及对比学习在计算机视觉和多智能体学习中的应用。作者选择使用第一人称射击游戏（反恐精英）作为研究平台，因为它提供了丰富的游戏状态和决策复杂性。基于此，他们创建了X-Ego-CS数据集并设计了跨自我中心对比学习（CECL）方法，通过对比学习对齐队友的第一人称视觉表征。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过对比学习对齐队友的第一人称视觉表征，使模型能够从有限的第一人称视角中推断团队层面的战术态势。整体流程包括：1) 收集和处理专业级反恐精英比赛数据，提取第一人称视频流和状态-动作轨迹；2) 使用时空视频编码器处理每个玩家的视角；3) 应用对比学习目标函数，使同一时间点的队友视角产生相似表征；4) 设计下游任务（队友和对手位置预测）来评估模型性能；5) 结合对比损失和分类损失进行训练，使模型能够从单个视角推断团队态势。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) X-Ego-CS数据集：首个包含同步第一人称视频流和结构化状态-动作轨迹的专业电子竞技数据集；2) 跨自我中心对比学习（CECL）方法：通过对比学习对齐队友视角，实现团队态势感知；3) 队友-对手位置预测任务：为评估团队理解能力提供标准化基准。相比之前工作，本文方法使用同步第一人称视角而非第三人称视角，提供完整的第一人称视频流和精确轨迹数据，并通过对比学习模拟人类心智理论能力，在复杂3D环境中验证而非简化环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文贡献了一个包含同步第一人称视频流的专业电子竞技数据集和一个通过对比学习对齐队友视角的方法，使AI系统能够从有限的第一人称视角中获取团队层面的战术态势感知能力，为多智能体系统中的团队协作研究建立了新的基准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human team tactics emerge from each player's individual perspective and theirability to anticipate, interpret, and adapt to teammates' intentions. Whileadvances in video understanding have improved the modeling of team interactionsin sports, most existing work relies on third-person broadcast views andoverlooks the synchronous, egocentric nature of multi-agent learning. Weintroduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplayfootage from 45 professional-level matches of the popular e-sports gameCounter-Strike 2, designed to facilitate research on multi-agentdecision-making in complex 3D environments. X-Ego-CS provides cross-egocentricvideo streams that synchronously capture all players' first-person perspectivesalong with state-action trajectories. Building on this resource, we proposeCross-Ego Contrastive Learning (CECL), which aligns teammates' egocentricvisual streams to foster team-level tactical situational awareness from anindividual's perspective. We evaluate CECL on a teammate-opponent locationprediction task, demonstrating its effectiveness in enhancing an agent'sability to infer both teammate and opponent positions from a singlefirst-person view using state-of-the-art video encoders. Together, X-Ego-CS andCECL establish a foundation for cross-egocentric multi-agent benchmarking inesports. More broadly, our work positions gameplay understanding as a testbedfor multi-agent modeling and tactical learning, with implications forspatiotemporal reasoning and human-AI teaming in both virtual and real-worlddomains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.</description>
      <author>example@mail.com (Yunzhe Wang, Soham Hans, Volkan Ustun)</author>
      <guid isPermaLink="false">2510.19150v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>UniHPR: Unified Human Pose Representation via Singular Value Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.19078v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UniHPR的统一人体姿态表示学习管道，通过创新的基于奇异值的对比学习损失函数，实现了图像、2D和3D人体姿态表示的有效对齐，并在人体姿态估计和检索任务中取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，开发有效的对齐管道以从不同模态生成统一表示受到越来越多的关注。人体姿态表示作为以人为中心应用的关键组成部分，在人体姿态估计、动作识别、人机交互、目标跟踪等下游任务中至关重要。然而，目前很少有研究使用对比范式清晰研究多种人体姿态表示之间的相关性。&lt;h4&gt;目的&lt;/h4&gt;提出UniHPR，一个统一的人体姿态表示学习管道，用于对齐来自图像、2D和3D人体姿态的人体姿态嵌入。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的基于奇异值的对比学习损失函数，用于同时对齐超过两种数据表示，更好地对齐不同模态并进一步提高性能。选择2D和3D人体姿态估计作为评估任务，以验证对齐表示的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;使用简单的3D人体姿态解码器，UniHPR在Human3.6M数据集上实现了49.9mm的MPJPE性能指标，在3DPW数据集上实现了跨域评估的51.6mm PA-MPJPE性能指标。此外，在Human3.6M数据集上，使用统一的人体姿态表示实现了2D和3D姿态检索，检索误差为9.24mm的MPJPE。&lt;h4&gt;结论&lt;/h4&gt;UniHPR能够有效对齐不同模态的人体姿态表示，并在多种下游任务中展现出优异的性能，为多模态人体姿态表示的学习提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;近年来，人们越来越关注开发有效的对齐管道，从不同模态生成统一表示，用于多模态融合和生成。作为以人为中心应用的重要组成部分，人体姿态表示在许多下游任务中至关重要，如人体姿态估计、动作识别、人机交互、目标跟踪等。人体姿态表示或嵌入可以从图像、2D关键点、3D骨架、网格模型等多种模态中提取。然而，使用对比范式清晰研究所有这些表示之间相关性的实例有限。在本文中，我们提出UniHPR，一个统一的人体姿态表示学习管道，用于对齐来自图像、2D和3D人体姿态的人体姿态嵌入。为了同时对齐超过两种数据表示，我们提出了一种新颖的基于奇异值的对比学习损失函数，更好地对齐不同模态并进一步提高性能。为了评估对齐表示的有效性，我们选择2D和3D人体姿态估计(HPE)作为评估任务。在我们的评估中，使用简单的3D人体姿态解码器，UniHPR在Human3.6M数据集上实现了49.9mm的MPJPE性能指标，在3DPW数据集上实现了跨域评估的51.6mm的PA-MPJPE性能指标。同时，我们能够在Human3.6M数据集上使用统一的人体姿态表示实现2D和3D姿态检索，检索误差为9.24mm的MPJPE。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, there has been a growing interest in developing effectivealignment pipelines to generate unified representations from differentmodalities for multi-modal fusion and generation. As an important component ofHuman-Centric applications, Human Pose representations are critical in manydownstream tasks, such as Human Pose Estimation, Action Recognition,Human-Computer Interaction, Object tracking, etc. Human Pose representations orembeddings can be extracted from images, 2D keypoints, 3D skeletons, meshmodels, and lots of other modalities. Yet, there are limited instances wherethe correlation among all of those representations has been clearly researchedusing a contrastive paradigm. In this paper, we propose UniHPR, a unified HumanPose Representation learning pipeline, which aligns Human Pose embeddings fromimages, 2D and 3D human poses. To align more than two data representations atthe same time, we propose a novel singular value-based contrastive learningloss, which better aligns different modalities and further boosts performance.To evaluate the effectiveness of the aligned representation, we choose 2D and3D Human Pose Estimation (HPE) as our evaluation tasks. In our evaluation, witha simple 3D human pose decoder, UniHPR achieves remarkable performance metrics:MPJPE 49.9mm on the Human3.6M dataset and PA-MPJPE 51.6mm on the 3DPW datasetwith cross-domain evaluation. Meanwhile, we are able to achieve 2D and 3D poseretrieval with our unified human pose representations in Human3.6M dataset,where the retrieval error is 9.24mm in MPJPE.</description>
      <author>example@mail.com (Zhongyu Jiang, Wenhao Chai, Lei Li, Zhuoran Zhou, Cheng-Yen Yang, Jenq-Neng Hwang)</author>
      <guid isPermaLink="false">2510.19078v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</title>
      <link>http://arxiv.org/abs/2510.18795v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 5 fiugres&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出ProCLIP框架，解决CLIP文本编码器在处理长文本和多语言输入方面的局限性，通过课程学习实现CLIP图像编码器与LLM嵌入器的有效对齐。&lt;h4&gt;背景&lt;/h4&gt;原始CLIP文本编码器受限于77个token的最大输入长度，不支持多语言输入，这些限制显著阻碍了其在更广泛任务中的应用。虽然近期研究尝试用基于LLM的嵌入器替代CLIP文本编码器，但由于LLM和CLIP的表示空间独立预训练且缺乏先验对齐，直接使用对比学习会破坏CLIP图像编码器中固有的视觉-语言对齐。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来有效对齐CLIP图像编码器与基于LLM的嵌入器，同时保留CLIP的预训练知识，从而增强模型在处理长文本、多语言理解和细粒度语义理解方面的能力。&lt;h4&gt;方法&lt;/h4&gt;ProCLIP采用课程学习的渐进式视觉-语言对齐框架：首先从CLIP文本编码器中蒸馏知识到LLM嵌入器建立初始对齐；然后通过图像-文本对比微调进一步对齐，并使用自蒸馏正则化避免过拟合；在表示继承和对比微调过程中采用实例语义对齐损失和嵌入结构对齐损失以实现更有效的对齐。&lt;h4&gt;主要发现&lt;/h4&gt;直接对齐LLM和CLIP的表示空间会破坏CLIP图像编码器中固有的视觉-语言对齐，导致预训练知识利用不足；而ProCLIP框架能够有效对齐两者并保留CLIP的预训练知识。&lt;h4&gt;结论&lt;/h4&gt;ProCLIP通过课程学习的渐进式对齐方法，解决了LLM嵌入器和CLIP图像编码器之间的对齐问题，同时保留了CLIP的预训练知识，显著提升了模型在长文本处理、多语言理解和细粒度语义理解方面的能力。&lt;h4&gt;翻译&lt;/h4&gt;原始的CLIP文本编码器受限于最大77个token的输入长度，这妨碍了它有效处理长文本和进行细粒度语义理解的能力。此外，CLIP文本编码器不支持多语言输入。所有这些限制显著限制了它在更广泛任务中的应用性。最近的研究尝试用基于LLM的嵌入器替换CLIP文本编码器，以增强其处理长文本、多语言理解和细粒度语义理解的能力。然而，由于LLM的表示空间和CLIP的视觉-语言空间是独立预训练且没有对齐先验，直接使用对比学习对齐会破坏CLIP图像编码器中固有的视觉-语言对齐，导致预训练知识利用不足。为解决这一挑战，我们提出ProCLIP，一种基于课程学习的渐进式视觉-语言对齐框架，以有效对齐CLIP图像编码器和基于LLM的嵌入器。具体而言，ProCLIP首先从CLIP的文本编码器中蒸馏知识到基于LLM的嵌入器，利用CLIP丰富的预训练知识，同时建立LLM嵌入器和CLIP图像编码器之间的初始对齐。随后，ProCLIP通过图像-文本对比微调进一步对齐CLIP图像编码器和基于LLM的嵌入器，采用自蒸馏正则化来避免过拟合。为了实现更有效的对齐，在表示继承和对比微调过程中采用了实例语义对齐损失和嵌入结构对齐损失。代码可在https://github.com/VisionXLab/ProCLIP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The original CLIP text encoder is limited by a maximum input length of 77tokens, which hampers its ability to effectively process long texts and performfine-grained semantic understanding. In addition, the CLIP text encoder lackssupport for multilingual inputs. All these limitations significantly restrictits applicability across a broader range of tasks. Recent studies haveattempted to replace the CLIP text encoder with an LLM-based embedder toenhance its ability in processing long texts, multilingual understanding, andfine-grained semantic comprehension. However, because the representation spacesof LLMs and the vision-language space of CLIP are pretrained independentlywithout alignment priors, direct alignment using contrastive learning candisrupt the intrinsic vision-language alignment in the CLIP image encoder,leading to an underutilization of the knowledge acquired during pre-training.To address this challenge, we propose ProCLIP, a curriculum learning-basedprogressive vision-language alignment framework to effectively align the CLIPimage encoder with an LLM-based embedder. Specifically, ProCLIP first distillsknowledge from CLIP's text encoder into the LLM-based embedder to leverageCLIP's rich pretrained knowledge while establishing initial alignment betweenthe LLM embedder and CLIP image encoder. Subsequently, ProCLIP further alignsthe CLIP image encoder with the LLM-based embedder through image-textcontrastive tuning, employing self-distillation regularization to avoidoverfitting. To achieve a more effective alignment, instance semantic alignmentloss and embedding structure alignment loss are employed during representationinheritance and contrastive tuning. The Code is available athttps://github.com/VisionXLab/ProCLIP.</description>
      <author>example@mail.com (Xiaoxing Hu, Kaicheng Yang, Ziyang Gong, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang)</author>
      <guid isPermaLink="false">2510.18795v2</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation</title>
      <link>http://arxiv.org/abs/2510.19592v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://www.jshyun.me/projects/decaf&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DecAF的分解注意力融合方法，用于在无需重新训练多模态大语言模型(MLLMs)的情况下实现视频理解与定位，通过改进注意力图实现了与需要训练的方法相当的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)能够通过关注与文本查询相关的视觉标记来展示强大的视频理解能力，但直接将其应用于定位任务存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需训练的方法，将MLLMs的视频理解能力直接适应于视频推理分割任务。&lt;h4&gt;方法&lt;/h4&gt;将视频推理分割视为视频问答任务并通过展开机制提取注意力图；提出DecAF方法，通过对比对象-背景融合和互补视频帧融合两种机制改进原始注意力图；引入注意力引导的SAM2提示获取精细掩码。&lt;h4&gt;主要发现&lt;/h4&gt;DecAF能够抑制不相关的激活并增强对象聚焦的线索，使注意力图可以直接转换为粗略分割掩码；无需训练的方法实现了与需要训练的方法相当的性能。&lt;h4&gt;结论&lt;/h4&gt;DecAF优于现有的无需训练的方法，并在指代和推理VOS基准测试上达到了与基于训练方法相当的性能；与现有的将MLLMs与SAM联合训练的方法不同，DecAF完全无需重新训练。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)通过关注与文本查询相关的视觉标记展示了强大的视频理解能力。为了直接以无需训练的方式将其适应于定位任务，我们将视频推理分割视为视频问答任务，并通过展开机制提取注意力图。然而，原始注意力图嘈杂且与对象区域对齐不良。我们提出了分解注意力融合(DecAF)，通过两种机制改进这些图：(1)对比对象-背景融合和(2)互补视频帧融合。此方法抑制了不相关的激活并增强了对象聚焦的线索，使注意力图可以直接转换为粗略分割掩码。此外，我们引入了注意力引导的SAM2提示来获取精细掩码。与现有的将MLLMs与SAM联合训练的方法不同，我们的方法完全无需重新训练。DecAF优于无需训练的方法，并在指代和推理VOS基准测试上实现了与基于训练方法相当的性能。代码将在https://github.com/HYUNJS/DecAF上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) demonstrate strong videounderstanding by attending to visual tokens relevant to textual queries. Todirectly adapt this for localization in a training-free manner, we cast videoreasoning segmentation as a video QA task and extract attention maps viarollout mechanism. However, raw attention maps are noisy and poorly alignedwith object regions. We propose Decomposed Attention Fusion (DecAF), whichrefines these maps through two mechanisms: (1) contrastive object-backgroundfusion and (2) complementary video-frame fusion. This method suppressesirrelevant activations and enhances object-focused cues, enabling directconversion of attention maps into coarse segmentation masks. In addition, weintroduce attention-guided SAM2 prompting for obtaining fine-grained masks.Unlike existing methods that jointly train MLLMs with SAM, our method operatesentirely without retraining. DecAF outperforms training-free methods andachieves performance comparable to training-based methods on both referring andreasoning VOS benchmarks. The code will be available athttps://github.com/HYUNJS/DecAF.</description>
      <author>example@mail.com (Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim)</author>
      <guid isPermaLink="false">2510.19592v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>A Matter of Time: Revealing the Structure of Time in Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.19559v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模视觉语言模型的时间感知能力，提出了TIME10k基准数据集，并发现时间信息在VLM嵌入空间中沿低维非线性流形结构化，基于此提出了时间线表示方法，该方法在计算效率高的同时实现了优异的时间推理性能。&lt;h4&gt;背景&lt;/h4&gt;大规模视觉语言模型如CLIP因其可泛化和表达性的多模态表示而受到欢迎。这些模型通过利用具有多样化文本元数据的大规模训练数据，获得了开放词汇能力，能够解决超出其训练范围的任务。&lt;h4&gt;目的&lt;/h4&gt;研究视觉语言模型的时间感知能力，评估它们将视觉内容定位在时间中的能力。&lt;h4&gt;方法&lt;/h4&gt;引入TIME10k基准数据集（包含超过10,000张图像的时间基准数据），通过一种新方法评估37个VLMs的时间感知能力，并基于发现提出从嵌入空间推导显式'时间线'表示的方法。&lt;h4&gt;主要发现&lt;/h4&gt;时间信息在VLM嵌入空间中沿着低维、非线性的流形结构化，基于此可以推导出显式的'时间线'表示。&lt;h4&gt;结论&lt;/h4&gt;提出的时间线表示方法能够模拟时间及其时间进展，促进时间推理任务，在计算效率高的同时，实现了与基于提示的基线相当或更优的准确性。&lt;h4&gt;翻译&lt;/h4&gt;大规模视觉语言模型如CLIP因其可泛化和表达性的多模态表示而受到欢迎。通过利用具有多样化文本元数据的大规模训练数据，VLMs获得了开放词汇能力，能够解决超出其训练范围的任务。本文研究了VLMs的时间感知能力，评估它们将视觉内容定位在时间中的能力。我们引入了TIME10k，一个包含超过10,000张图像的时间基准数据集，并通过一种新方法评估了37个VLMs的时间感知能力。我们的研究揭示，时间信息在VLM嵌入空间中沿着低维、非线性的流形结构化。基于这一见解，我们提出了从嵌入空间推导显式'时间线'表示的方法。这些表示模拟时间及其时间进展，从而促进时间推理任务。我们的时间线方法在计算效率高的同时，实现了与基于提示的基线相当或更优的准确性。所有代码和数据都在https://tekayanidham.github.io/timeline-page/上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758163&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale vision-language models (VLMs) such as CLIP have gained popularityfor their generalizable and expressive multimodal representations. Byleveraging large-scale training data with diverse textual metadata, VLMsacquire open-vocabulary capabilities, solving tasks beyond their trainingscope. This paper investigates the temporal awareness of VLMs, assessing theirability to position visual content in time. We introduce TIME10k, a benchmarkdataset of over 10,000 images with temporal ground truth, and evaluate thetime-awareness of 37 VLMs by a novel methodology. Our investigation revealsthat temporal information is structured along a low-dimensional, non-linearmanifold in the VLM embedding space. Based on this insight, we propose methodsto derive an explicit ``timeline'' representation from the embedding space.These representations model time and its chronological progression and therebyfacilitate temporal reasoning tasks. Our timeline approaches achievecompetitive to superior accuracy compared to a prompt-based baseline whilebeing computationally efficient. All code and data are available athttps://tekayanidham.github.io/timeline-page/.</description>
      <author>example@mail.com (Nidham Tekaya, Manuela Waldner, Matthias Zeppelzauer)</author>
      <guid isPermaLink="false">2510.19559v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation</title>
      <link>http://arxiv.org/abs/2510.19475v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 6 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PRGCN的新型框架，通过跨序列模式检索和适应来解决单目3D人体姿态估计中的深度模糊性问题。该方法利用图记忆库存储姿态原型，并通过注意力机制动态检索，结合双流混合架构实现了最先进的性能和跨域泛化能力。&lt;h4&gt;背景&lt;/h4&gt;单目3D人体姿态估计是一个不适定的逆问题，因为从2D到3D的提升中存在固有的深度模糊性。现有基于视频的方法虽然利用时间上下文增强空间推理，但独立处理每个序列，未能充分利用跨序列中人类运动的强结构规律性和重复运动模式。&lt;h4&gt;目的&lt;/h4&gt;解决单目3D人体姿态估计中的深度模糊性问题，突破现有方法仅独立处理每个序列的局限，通过跨序列模式重用机制提升姿态估计的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出PRGCN框架，将姿态估计形式化为模式检索和适应问题；引入图记忆库学习和存储姿态原型；通过注意力机制动态检索提供结构化先验；通过内存驱动的图卷积将先验与解剖约束融合；设计双流混合架构，结合Mamba的局部时间建模和自注意力的全局关系能力。&lt;h4&gt;主要发现&lt;/h4&gt;在Human3.6M和MPI-INF-3DHP基准测试上，PRGCN实现了37.1mm和13.4mm的MPJPE，建立了新的最先进水平，同时表现出增强的跨域泛化能力。&lt;h4&gt;结论&lt;/h4&gt;跨序列模式重用机制对推进人体姿态估计领域至关重要，将研究范式从每序列优化转向累积知识学习。&lt;h4&gt;翻译&lt;/h4&gt;单目3D人体姿态估计由于2D到3D提升中的固有深度模糊性，仍然是一个根本性的不适定逆问题。虽然当代基于视频的方法利用时间上下文来增强空间推理，但它们在关键范式限制下运行：独立处理每个序列，因此未能充分利用跨序列中普遍存在的强结构规律性和重复运动模式。这项工作引入了模式重用图卷积网络，一个将姿态估计形式化为模式检索和适应问题的新型框架。其核心是，PRGCN具有一个图记忆库，学习和存储一组紧凑的姿态原型，编码为关系图，这些原型通过注意力机制动态检索以提供结构化先验。这些先验通过内存驱动的图卷积与硬编码的解剖约束自适应融合，确保几何合理性。为了用鲁棒的空间-时间特征支持这一检索过程，我们设计了一个双流混合架构，协同结合了基于Mamba的状态空间模型的线性复杂度局部时间建模与自注意力的全局关系能力。在Human3.6M和MPI-INF-3DHP基准测试上的广泛评估表明，PRGCN建立了新的最先进水平，分别实现了37.1mm和13.4mm的MPJPE，同时表现出增强的跨域泛化能力。我们的研究认为，长期以来被忽视的跨序列模式重用机制对推进该领域至关重要，将范式从每序列优化转向累积知识学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D human pose estimation remains a fundamentally ill-posed inverseproblem due to the inherent depth ambiguity in 2D-to-3D lifting. Whilecontemporary video-based methods leverage temporal context to enhance spatialreasoning, they operate under a critical paradigm limitation: processing eachsequence in isolation, thereby failing to exploit the strong structuralregularities and repetitive motion patterns that pervade human movement acrosssequences. This work introduces the Pattern Reuse Graph Convolutional Network(PRGCN), a novel framework that formalizes pose estimation as a problem ofpattern retrieval and adaptation. At its core, PRGCN features a graph memorybank that learns and stores a compact set of pose prototypes, encoded asrelational graphs, which are dynamically retrieved via an attention mechanismto provide structured priors. These priors are adaptively fused with hard-codedanatomical constraints through a memory-driven graph convolution, ensuringgeometrical plausibility. To underpin this retrieval process with robustspatiotemporal features, we design a dual-stream hybrid architecture thatsynergistically combines the linear-complexity, local temporal modeling ofMamba-based state-space models with the global relational capacity ofself-attention. Extensive evaluations on Human3.6M and MPI-INF-3DHP benchmarksdemonstrate that PRGCN establishes a new state-of-the-art, achieving an MPJPEof 37.1mm and 13.4mm, respectively, while exhibiting enhanced cross-domaingeneralization capability. Our work posits that the long-overlooked mechanismof cross-sequence pattern reuse is pivotal to advancing the field, shifting theparadigm from per-sequence optimization towards cumulative knowledge learning.</description>
      <author>example@mail.com (Zhuoyang Xie, Yibo Zhao, Hui Huang, Riwei Wang, Zan Gao)</author>
      <guid isPermaLink="false">2510.19475v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>$Δ$t-Mamba3D: A Time-Aware Spatio-Temporal State-Space Model for Breast Cancer Risk Prediction</title>
      <link>http://arxiv.org/abs/2510.19003v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了Time-Aware Δt-Mamba3D，一种新型的状态空间架构，专门用于纵向医学图像分析。该模型能够有效编码不规则访问间隔和丰富的时空上下文，同时保持计算效率，在乳腺癌风险预测任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;纵向放射图像分析面临一个基本数据挑战：如何有效建模在非规则时间间隔采集的高分辨率图像序列。这种数据结构包含重要的空间和时间线索，但当前方法无法充分利用。现有方法通常要么将空间信息压缩为向量，要么使用计算效率低下且与非均匀时间步不兼容的时空模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理不规则时间间隔采集的图像序列的模型，同时充分利用空间和时间信息，并保持计算效率，应用于纵向医学图像分析，特别是乳腺癌风险预测。&lt;h4&gt;方法&lt;/h4&gt;研究者提出了Time-Aware Δt-Mamba3D，一种专为纵向医学成像设计的新的状态空间架构。该模型的核心创新是一个连续时间选择性扫描机制，明确地将检查之间的真实时间差异整合到状态转换中。此外，还采用了多尺度3D邻域融合模块，稳健地捕获时空关系。&lt;h4&gt;主要发现&lt;/h4&gt;在乳腺癌风险预测基准测试中，该模型表现出色，验证c-index提高了2-5个百分点，相比现有的循环、变压器和状态空间模型的变体，实现了更高的1-5年AUC分数。由于具有线性复杂度，该模型能够高效处理长期复杂的患者筛查历史。&lt;h4&gt;结论&lt;/h4&gt;Time-Aware Δt-Mamba3D为纵向图像分析形成了一个新框架，能够有效处理不规则时间间隔采集的图像序列，充分利用时空信息，同时保持计算效率，在医学图像分析任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;纵向连续放射图像分析受到一个基本数据挑战的阻碍：如何有效建模在非规则时间间隔采集的高分辨率图像序列。这种数据结构包含了当前方法无法充分利用的必不可少的空间和时间线索。模型通常要么将空间信息压缩为向量，要么使用计算效率低下且与非均匀时间步不兼容的时空模型。我们通过Time-Aware Δt-Mamba3D解决了这一挑战，这是一种专为纵向医学成像设计的新型状态空间架构。我们的模型同时编码不规则访问间隔和丰富的时空上下文，同时保持计算效率。其核心创新是一个连续时间选择性扫描机制，明确地将检查之间的真实时间差异整合到其状态转换中。这辅以一个多尺度3D邻域融合模块，稳健地捕获时空关系。在使用连续筛查乳腺X光检查的乳腺癌风险预测综合基准中，我们的模型表现出卓越性能，相比现有的循环、变压器和状态空间模型的变体，将验证c-index提高了2-5个百分点，并实现了更高的1-5年AUC分数。由于其线性复杂度，该模型能够高效处理长期复杂的患者筛查历史，为纵向图像分析形成了一个新框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何有效建模在不同时间间隔捕获的高分辨率图像序列的问题，特别是在乳腺癌筛查中不规则时间间隔的纵向放射学图像分析。这个问题很重要，因为乳腺癌是全球女性最常见的癌症之一，早期风险预测可以提高筛查效率；现有方法未能充分利用不规则时间间隔这一重要预测因素；医生评估风险时会考虑多次检查的比较，而大多数深度学习系统仍只处理单次检查；开发能够处理不规则时间间隔的高效模型对临床应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到乳腺癌筛查是纵向的，患者会定期返回检查，乳房随年龄变化，病变可能逐渐显现。他们指出大多数深度学习系统忽略了时序背景，现有方法要么将空间信息压缩为向量，要么使用计算效率低下的模型，或者无法处理非均匀时间步长。作者分析了各种处理不规则时间序列的方法，发现它们都有局限性，而状态空间模型如Mamba虽能捕获长期依赖关系，但尚未显式编码不规则时间间隔。作者借鉴了Mamba的状态空间架构、3D卷积网络处理空间信息的思想、时间感知模型处理时间间隔的方法，以及视频视觉 transformers处理时空信息的思路，但都进行了改进以适应不规则时间间隔的医学成像数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1) 时间感知选择性扫描：将真实时间差Δt直接注入选择性扫描中，使模型能根据实际时间间隔调整状态更新；2) 多尺度3D邻域融合：使用深度3D卷积捕获空间和时间依赖关系，同时保持计算效率；3) 线性复杂度：能高效处理长期复杂的患者筛查历史。整体流程：1) 输入处理：每个患者的纵向成像序列，使用Swin-V2处理每个图像并融合特征；2) Δt-Mamba3D块处理：将特征展平为标记序列，运行Mamba选择性扫描，状态更新由真实Δt调制，然后重塑回3D格式并应用3D邻域融合；3) 患者嵌入和风险模块：跨空间和时间聚合特征获得患者嵌入，使用加性危害模型估计未来乳腺癌风险；4) 处理可变长度序列：左填充序列到固定长度，使用掩码处理填充标记。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 时间感知选择性扫描机制：将真实时间差Δt注入选择性扫描，实现连续时间记忆衰减或累积；2) 多尺度深度3D融合模块：使用深度3D卷积捕获空间和时间依赖关系；3) 线性复杂度设计：能高效处理长期复杂历史。相比之前工作的不同：1) 与时间感知模型(如GRU-D)相比，使用连续时间状态空间模型，能更好地建模观测间的演变风险；2) 与连续时间模型(如Neural ODEs)相比，专为高维医学成像数据设计，处理长时间间隔；3) 与视频视觉transformers相比，明确编码不规则时间间隔，计算效率更高；4) 与视觉状态空间模型相比，引入真实时间间隔信息，结合3D邻域融合；5) 与标准Mamba相比，显式编码真实时间间隔，添加3D邻域融合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Time-Aware Δt-Mamba3D通过将真实时间间隔和多尺度3D空间-时间信息整合到高效的状态空间模型中，显著提高了乳腺癌风险预测的准确性，同时保持了线性计算复杂度，为纵向医学图像分析提供了新框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Longitudinal analysis of sequential radiological images is hampered by afundamental data challenge: how to effectively model a sequence ofhigh-resolution images captured at irregular time intervals. This datastructure contains indispensable spatial and temporal cues that current methodsfail to fully exploit. Models often compromise by either collapsing spatialinformation into vectors or applying spatio-temporal models that arecomputationally inefficient and incompatible with non-uniform time steps. Weaddress this challenge with Time-Aware $\Delta$t-Mamba3D, a novel state-spacearchitecture adapted for longitudinal medical imaging. Our model simultaneouslyencodes irregular inter-visit intervals and rich spatio-temporal context whileremaining computationally efficient. Its core innovation is a continuous-timeselective scanning mechanism that explicitly integrates the true timedifference between exams into its state transitions. This is complemented by amulti-scale 3D neighborhood fusion module that robustly capturesspatio-temporal relationships. In a comprehensive breast cancer risk predictionbenchmark using sequential screening mammogram exams, our model shows superiorperformance, improving the validation c-index by 2-5 percentage points andachieving higher 1-5 year AUC scores compared to established variants ofrecurrent, transformer, and state-space models. Thanks to its linearcomplexity, the model can efficiently process long and complex patientscreening histories of mammograms, forming a new framework for longitudinalimage analysis.</description>
      <author>example@mail.com (Zhengbo Zhou, Dooman Arefan, Margarita Zuley, Shandong Wu)</author>
      <guid isPermaLink="false">2510.19003v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>SFGFusion: Surface Fitting Guided 3D Object Detection with 4D Radar and Camera Fusion</title>
      <link>http://arxiv.org/abs/2510.19215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Pattern Recognition&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SFGFusion的新型相机-4D成像雷达检测网络，通过表面拟合引导来解决3D物体检测中的多模态融合问题。&lt;h4&gt;背景&lt;/h4&gt;3D物体检测对自动驾驶至关重要，4D成像雷达作为一种新兴传感器具有低成本、长距离检测和精确速度测量的优势，但其稀疏点云和低分辨率限制了物体的几何表示和跨模态融合。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效融合相机和4D成像雷达数据的方法，提高3D物体检测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;SFGFusion通过估计物体的二次曲面参数增强空间表示和跨模态交互，预测细粒度密集深度用于图像特征转换和伪点云生成，采用基于支柱的方法处理雷达点云，并在BEV空间中进行特征融合和检测。&lt;h4&gt;主要发现&lt;/h4&gt;SFGFusion有效融合了相机和4D雷达特征，在TJ4DRadSet和view-of-delft(VoD)物体检测基准上取得了优越性能。&lt;h4&gt;结论&lt;/h4&gt;基于表面拟合的SFGFusion网络能够有效解决4D成像雷达的稀疏性问题，提升多模态融合效果，提高3D物体检测性能。&lt;h4&gt;翻译&lt;/h4&gt;3D物体检测对自动驾驶至关重要。作为一种新兴传感器，4D成像雷达具有低成本、长距离检测和精确速度测量的优势，使其非常适合物体检测。然而，其稀疏点云和低分辨率限制了物体的几何表示并阻碍了多模态融合。在本研究中，我们引入了SFGFusion，一种基于表面拟合引导的新型相机-4D成像雷达检测网络。通过从图像和雷达数据估计物体的二次曲面参数，显式表面拟合模型增强了空间表示和跨模态交互，实现了对细粒度密集深度更可靠的预测。预测的深度有两个用途：1)在图像分支中引导图像特征从透视视图(PV)转换为统一的鸟瞰图(BEV)用于多模态融合，提高空间映射准确性；2)在表面伪点分支中生成密集伪点云，减轻雷达点稀疏性。原始雷达点云也在单独的雷达分支中编码。这两个点云分支采用基于支柱的方法，然后将特征转换为BEV空间。最后，使用标准的2D主干和检测头从BEV特征预测物体标签和边界框。实验结果表明，SFGFusion有效融合了相机和4D雷达特征，在TJ4DRadSet和view-of-delft(VoD)物体检测基准上取得了优越性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决4D成像雷达点云稀疏性和低分辨率导致的物体几何表示不足，以及由此带来的多模态融合困难问题。这个问题在现实中非常重要，因为3D物体检测是自动驾驶的核心技术，而4D成像雷达具有成本低、远距离检测和精确测速的优势，能有效弥补相机在深度信息上的不足。解决这一问题可以提升自动驾驶系统的环境感知能力，增强在复杂场景下的检测精度和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有多模态3D检测框架的局限性，特别是图像特征从2D到3D转换过程中因雷达点云稀疏导致的几何约束不足问题，提出了表面拟合模型作为解决方案。作者借鉴了基于图像的3D检测中的特征投影方法、基于点云的3D检测中的柱状处理方法(PointPillars)，以及多模态融合中的BEV特征融合技术，但针对4D成像雷达的特点进行了专门优化和创新设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用表面拟合模型估计物体表面深度，通过结合图像语义和雷达几何信息来增强深度预测精度，然后用这些深度信息指导图像特征转换和生成密集伪点云。整体流程包括：1)表面拟合模型融合图像和雷达信息预测物体深度；2)图像分支在深度指导下将特征从透视视图转换为鸟瞰视图；3)雷达分支处理原始4D雷达点云；4)表面伪点分支利用预测深度生成密集伪点云；5)多分支特征融合后通过检测头输出3D物体检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出表面拟合模型增强跨模态交互和深度估计；2)利用拟合深度指导图像特征视图变换提高空间映射精度；3)生成密集伪点云缓解雷达点云稀疏问题；4)设计针对4D雷达特点的多维特征提取方法。相比之前工作，本文专门针对4D成像雷达而非LiDAR进行优化，解决了雷达点云稀疏不规则带来的挑战，同时结合了图像语义和雷达几何信息的优势，实现了更准确的特征对齐和物体表示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SFGFusion通过表面拟合模型有效融合相机和4D成像雷达数据，解决了雷达点云稀疏性导致的几何表示不足问题，显著提升了3D物体检测的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection is essential for autonomous driving. As an emergingsensor, 4D imaging radar offers advantages as low cost, long-range detection,and accurate velocity measurement, making it highly suitable for objectdetection. However, its sparse point clouds and low resolution limit objectgeometric representation and hinder multi-modal fusion. In this study, weintroduce SFGFusion, a novel camera-4D imaging radar detection network guidedby surface fitting. By estimating quadratic surface parameters of objects fromimage and radar data, the explicit surface fitting model enhances spatialrepresentation and cross-modal interaction, enabling more reliable predictionof fine-grained dense depth. The predicted depth serves two purposes: 1) in animage branch to guide the transformation of image features from perspectiveview (PV) to a unified bird's-eye view (BEV) for multi-modal fusion, improvingspatial mapping accuracy; and 2) in a surface pseudo-point branch to generatedense pseudo-point cloud, mitigating the radar point sparsity. The originalradar point cloud is also encoded in a separate radar branch. These two pointcloud branches adopt a pillar-based method and subsequently transform thefeatures into the BEV space. Finally, a standard 2D backbone and detection headare used to predict object labels and bounding boxes from BEV features.Experimental results show that SFGFusion effectively fuses camera and 4D radarfeatures, achieving superior performance on the TJ4DRadSet and view-of-delft(VoD) object detection benchmarks.</description>
      <author>example@mail.com (Xiaozhi Li, Huijun Di, Jian Li, Feng Liu, Wei Liang)</author>
      <guid isPermaLink="false">2510.19215v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing</title>
      <link>http://arxiv.org/abs/2510.19661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AgentSense是一种混合的、无需训练的框架，通过多智能体进化系统将大型语言模型集成到参与式城市感知中，解决了现有系统在多样化城市场景中泛化能力差和决策解释性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;基于网络的参与式城市感知已成为现代城市管理的重要方法，通过利用移动个体作为分布式传感器收集城市数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适应动态城市条件和异构工作者偏好的城市感知系统，同时提供自然语言解释以提高透明度和信任度。&lt;h4&gt;方法&lt;/h4&gt;AgentSense首先使用经典规划器生成基线解决方案，然后通过多智能体进化系统迭代优化这些解决方案，使感知任务分配适应动态变化，并生成自然语言解释。&lt;h4&gt;主要发现&lt;/h4&gt;在两个大规模移动数据集和七种动态干扰上的实验表明，AgentSense在适应性和可解释性方面明显优于传统方法，且比单智能体LLM基线在性能和鲁棒性方面表现更好。&lt;h4&gt;结论&lt;/h4&gt;AgentSense代表了在网络上部署自适应和可解释的城市感知系统的重要进展，为现代城市管理提供了更有效的工具。&lt;h4&gt;翻译&lt;/h4&gt;基于网络的参与式城市感知已通过利用移动个体作为分布式传感器成为现代城市管理的重要方法。然而，现有的城市感知系统难以在多样化的城市场景中泛化，并且在决策过程中解释性差。在这项工作中，我们介绍了AgentSense，一个混合的、无需训练的框架，通过多智能体进化系统将大型语言模型集成到参与式城市感知中。AgentSense最初使用经典规划器生成基线解决方案，然后迭代优化它们，使感知任务分配适应动态城市条件和异构工作者偏好，同时产生自然语言解释以提高透明度和信任度。在两个大规模移动数据集和七种动态干扰上的大量实验表明，AgentSense在适应性和可解释性方面比传统方法具有明显优势。此外，与单智能体LLM基线相比，我们的方法在性能和鲁棒性方面表现更好，并提供更合理和透明的解释。这些结果表明AgentSense是在网络上部署自适应和可解释的城市感知系统的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Web-based participatory urban sensing has emerged as a vital approach formodern urban management by leveraging mobile individuals as distributedsensors. However, existing urban sensing systems struggle with limitedgeneralization across diverse urban scenarios and poor interpretability indecision-making. In this work, we introduce AgentSense, a hybrid, training-freeframework that integrates large language models (LLMs) into participatory urbansensing through a multi-agent evolution system. AgentSense initially employsclassical planner to generate baseline solutions and then iteratively refinesthem to adapt sensing task assignments to dynamic urban conditions andheterogeneous worker preferences, while producing natural language explanationsthat enhance transparency and trust. Extensive experiments across twolarge-scale mobility datasets and seven types of dynamic disturbancesdemonstrate that AgentSense offers distinct advantages in adaptivity andexplainability over traditional methods. Furthermore, compared to single-agentLLM baselines, our approach outperforms in both performance and robustness,while delivering more reasonable and transparent explanations. These resultsposition AgentSense as a significant advancement towards deploying adaptive andexplainable urban sensing systems on the web.</description>
      <author>example@mail.com (Xusen Guo, Mingxing Peng, Xixuan Hao, Xingchen Zou, Qiongyan Wang, Sijie Ruan, Yuxuan Liang)</author>
      <guid isPermaLink="false">2510.19661v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Conditions for Catastrophic Forgetting in Multilingual Translation</title>
      <link>http://arxiv.org/abs/2510.19546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Multilingual Representation Learning (MRL) Workshop 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过系统实证研究，探讨了多语言模型微调中的灾难性遗忘现象，发现模型与数据规模比例是遗忘的主要决定因素，指令遵循能力比架构更重要，参数高效微调无明显优势，而跨语言对齐可有效减轻遗忘并促进知识迁移。&lt;h4&gt;背景&lt;/h4&gt;在多语言基础模型上针对特定语言进行微调通常会导致灾难性遗忘，降低在微调中未见语言的性能。虽然这种现象有广泛记录，但文献中关于何时发生遗忘的结果是零散的。&lt;h4&gt;目的&lt;/h4&gt;解决关于灾难性遗忘发生条件的模糊性，进行系统的实证研究，使用机器翻译作为测试平台，以识别多语言微调中触发灾难性遗忘的条件。&lt;h4&gt;方法&lt;/h4&gt;使用机器翻译作为测试平台，进行受控实验，跨越不同的模型架构、数据规模和微调方法。&lt;h4&gt;主要发现&lt;/h4&gt;模型和数据规模之间的相对规模是遗忘的主要决定因素；模型的指令遵循能力对于保留多语言知识比其架构更重要；与假设相反，参数高效微调在减轻遗忘方面没有明显优于完全微调；跨语言对齐可以减轻遗忘，同时促进对未见目标语言的积极迁移。&lt;h4&gt;结论&lt;/h4&gt;跨语言对齐是一种有效的策略，可以减轻灾难性遗忘，并促进知识迁移到未见语言。&lt;h4&gt;翻译&lt;/h4&gt;在多语言基础模型上针对特定语言进行微调通常会导致灾难性遗忘，降低在微调中未见语言的性能。虽然这种现象有广泛记录，但文献中关于何时发生遗忘的结果是零散的。为解决这一模糊性，我们使用机器翻译作为测试平台进行系统实证研究，以识别多语言微调中触发灾难性遗忘的条件。通过跨越不同模型架构、数据规模和微调方法的受控实验，我们揭示了模型与数据规模之间的相对比例是遗忘的主要决定因素。此外，我们证明模型的指令遵循能力对于保留多语言知识比其架构更为关键。与假设相反，参数高效微调在减轻遗忘方面并未显示出比完全微调的明显优势。最后，我们表明跨语言对齐可以减轻遗忘，同时促进对未见目标语言的积极迁移。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning multilingual foundation models on specific languages ofteninduces catastrophic forgetting, degrading performance on languages unseen infine-tuning. While this phenomenon is widely-documented, the literaturepresents fragmented results about when forgetting occurs. To address thisambiguity, we conduct a systematic empirical study using machine translation asa testbed to identify the conditions that trigger catastrophic forgetting inmultilingual fine-tuning. Through controlled experiments across different modelarchitectures, data scales, and fine-tuning approaches, we reveal that therelative scale between model and data size is a primary determinant offorgetting. Moreover, we demonstrate that a model's instruction-followingability is more critical for retaining multilingual knowledge than itsarchitecture. Contrary to assumptions, parameter-efficient fine-tuning offersno clear advantage over full fine-tuning in mitigating forgetting. Lastly, weshow that cross-lingual alignment can mitigate forgetting while alsofacilitating positive transfer to unseen target languages.</description>
      <author>example@mail.com (Danni Liu, Jan Niehues)</author>
      <guid isPermaLink="false">2510.19546v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Which Evaluation for Which Model? A Taxonomy for Speech Model Assessment</title>
      <link>http://arxiv.org/abs/2510.19509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  57 pages (26 main, 25 appendix, 6 references)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的分类法，用于解决语音基础模型评估的匹配问题，通过三个正交轴对现有评估方法进行系统分类，为模型与合适评估方法的匹配提供了原则性框架，并揭示了未来基准设计的优先事项。&lt;h4&gt;背景&lt;/h4&gt;语音基础模型最近在广泛任务中取得了显著能力，但其评估在不同任务和模型类型之间仍然分散，不同模型在语音处理的不同方面表现出色，因此需要不同的评估协议。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的分类法，解决'哪种评估适合哪种模型'的问题，为选择、解释和扩展语音模型评估提供概念基础和实践指南。&lt;h4&gt;方法&lt;/h4&gt;定义三个正交轴：测量的评估方面、尝试任务所需的模型能力、执行任务所需的任务或协议要求，沿着这些轴对现有评估和基准进行分类，涵盖表示学习、语音生成和交互式对话等领域。&lt;h4&gt;主要发现&lt;/h4&gt;通过将每个评估映射到模型展示的能力和方法论需求，该分类法揭示了系统性的差距，如韵律、交互或推理覆盖有限，突显了未来基准设计的优先事项。&lt;h4&gt;结论&lt;/h4&gt;该统一的分类法为模型与合适评估方法的匹配提供了原则性框架，为语音模型评估领域提供了概念基础和实践指导。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型最近在广泛任务中取得了显著能力。然而，它们的评估在不同任务和模型类型之间仍然分散。不同的模型在语音处理的不同方面表现出色，因此需要不同的评估协议。本文提出了一种统一的分类法，解决'哪种评估适合哪种模型'的问题。该分类法定义了三个正交轴：测量的评估方面、尝试任务所需的模型能力、执行任务所需的任务或协议要求。我们沿着这些轴对广泛的现有评估和基准进行分类，涵盖表示学习、语音生成和交互式对话等领域。通过将每个评估映射到模型展示的能力（如语音生成、实时处理）及其方法论需求（如微调数据、人工判断），该分类法为模型与合适评估方法的匹配提供了原则性框架。它还揭示了系统性的差距，如韵律、交互或推理覆盖有限，突显了未来基准设计的优先事项。总体而言，这项工作为选择、解释和扩展语音模型评估提供了概念基础和实践指南。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech foundation models have recently achieved remarkable capabilitiesacross a wide range of tasks. However, their evaluation remains disjointedacross tasks and model types. Different models excel at distinct aspects ofspeech processing and thus require different evaluation protocols. This paperproposes a unified taxonomy that addresses the question: Which evaluation isappropriate for which model? The taxonomy defines three orthogonal axes: the\textbf{evaluation aspect} being measured, the model capabilities required toattempt the task, and the task or protocol requirements needed to perform it.We classify a broad set of existing evaluations and benchmarks along theseaxes, spanning areas such as representation learning, speech generation, andinteractive dialogue. By mapping each evaluation to the capabilities a modelexposes (e.g., speech generation, real-time processing) and to itsmethodological demands (e.g., fine-tuning data, human judgment), the taxonomyprovides a principled framework for aligning models with suitable evaluationmethods. It also reveals systematic gaps, such as limited coverage of prosody,interaction, or reasoning, that highlight priorities for future benchmarkdesign. Overall, this work offers a conceptual foundation and practical guidefor selecting, interpreting, and extending evaluations of speech models.</description>
      <author>example@mail.com (Maureen de Seyssel, Eeshan Gunesh Dhekane)</author>
      <guid isPermaLink="false">2510.19509v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Universal Quantitative Abstraction: Categorical Duality and Logical Completeness for Probabilistic Systems</title>
      <link>http://arxiv.org/abs/2510.19444v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种概率系统的定量抽象统一理论，结合了范畴论、最优传输和定量模态逻辑。核心是一个具有普遍性质的规范ε-商，在所有ε-抽象中最为信息丰富且满足值损失上限。该理论建立了抽象与实现函子之间的伴随关系，揭示了度量结构与逻辑语义的范畴对偶。研究还引入了定量模态μ演算，证明了其在逻辑可表示系统中的表达完整性，并分析了接口细化下的组合性。通过在有限马尔可夫决策过程上的验证，证实了理论的收缩性、值损失界限等性质，为状态聚合和表示学习提供了数学精确的保证。&lt;h4&gt;背景&lt;/h4&gt;概率系统的抽象和近似是计算机科学和人工智能中的重要问题，特别是在处理复杂系统时。现有的抽象方法往往缺乏统一的数学框架，难以保证近似的质量和性质。范畴论提供了描述系统结构和关系的强大工具，最优传输提供了度量概率空间之间距离的方法，而定量模态逻辑则允许对系统的行为进行精确描述。这些理论领域的结合为概率系统的定量抽象提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在构建一个统一的概率系统定量抽象理论，该理论能够：1) 提供一个信息丰富且满足值损失上限的抽象方法；2) 建立抽象与实现之间的数学关系；3) 刻画行为伪度量的性质；4) 开发表达完整的定量模态μ演算；5) 分析系统组合性；6) 为状态聚合和表示学习提供数学保证。&lt;h4&gt;方法&lt;/h4&gt;本研究采用了以下方法：1) 构建具有普遍性质的规范ε-商作为核心抽象机制；2) 应用范畴论建立抽象与实现函子之间的伴随关系；3) 使用贝尔曼风格算子刻画行为伪度量并证明其不动点性质；4) 在余代数框架中证明收缩性和利普希茨性质；5) 引入定量模态μ演算并证明其表达完整性；6) 分析接口细化下的组合性质；7) 在有限马尔可夫决策过程上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;研究的主要发现包括：1) 规范ε-商在所有ε-抽象中是最能保留信息且满足值损失上限的；2) 抽象与实现函子之间存在伴随关系，揭示了度量结构与逻辑语义的范畴对偶；3) 行为伪度量是贝尔曼风格算子的唯一不动点，具有收缩性和利普希茨性质；4) 定量模态μ演算在逻辑可表示系统中具有表达完整性，行为距离与最大逻辑偏差一致；5) 在接口细化下，抽象具有组合性质，能够清晰描述系统边界处的交互；6) 通过实验验证，理论具有收缩性、值损失界限、扰动稳定性、对抗区分性和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;本研究提出的概率系统定量抽象统一理论为复杂系统的抽象和近似提供了坚实的数学基础。该理论通过结合范畴论、最优传输和定量模态逻辑，建立了抽象与实现之间的严格关系，并确保了抽象的质量和性质。定量模态μ演算的表达完整性以及行为距离与逻辑偏差的一致性，为系统分析和验证提供了有力工具。实验验证证明了理论的鲁棒性和计算可行性。该框架不仅为状态聚合和表示学习提供了有原则的目标，还在随机域中为值函数近似提供了数学上精确的保证，对概率系统的建模、分析和应用具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种概率系统的定量抽象统一理论，该理论将范畴论、最优传输和定量模态逻辑联系起来。其核心是一个具有普遍性质的规范ε-商：在所有ε-抽象中，它是最能保留信息且满足规定值损失上限的。这种构造通过特殊伴随函子定理在抽象函子和实现函子之间诱导了一个伴随关系，揭示了度量结构和逻辑语义之间的范畴对偶。行为伪度量被刻画为贝尔曼风格算子的唯一不动点，并在余代数框架中证明了其收缩性和利普希茨性质。引入了定量模态μ演算并证明其在逻辑可表示系统中具有表达完整性，使得行为距离与最大逻辑偏差一致。分析了在接口细化下的组合性，阐明了抽象如何在系统边界处交互。在有限马尔可夫决策过程上的精确验证套件证实了收缩性、值损失界限、扰动下的稳定性、对抗区分性和可扩展性，展示了鲁棒性和计算可行性。由此产生的框架为状态聚合和表示学习提供了有原则的目标，并在随机域中为值函数近似提供了数学上精确的保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A unified theory of quantitative abstraction is presented for probabilisticsystems that links category theory, optimal transport, and quantitative modallogic. At its core is a canonical $ \varepsilon $-quotient endowed with auniversal property: among all $ \varepsilon $-abstractions, it is the mostinformative one that respects a prescribed bound on value loss. Thisconstruction induces an adjunction between abstraction and realization functors$ (Q_{\varepsilon} \dashv R_{\varepsilon}) $, established via the SpecialAdjoint Functor Theorem, revealing a categorical duality between metricstructure and logical semantics. A behavioral pseudometric is characterized asthe unique fixed point of a Bellman-style operator, with contraction andLipschitz properties proved in a coalgebraic setting. A quantitative modal $\mu $-calculus is introduced and shown to be expressively complete forlogically representable systems, so that behavioral distance coincides withmaximal logical deviation. Compositionality under interface refinement isanalyzed, clarifying how abstractions interact across system boundaries. Anexact validation suite on finite Markov decision processes corroborates thecontraction property, value-loss bounds, stability under perturbation,adversarial distinguishability, and scalability, demonstrating both robustnessand computational feasibility. The resulting framework provides principledtargets for state aggregation and representation learning, with mathematicallyprecise guarantees for value-function approximation in stochastic domains.</description>
      <author>example@mail.com (Nivar Anwer)</author>
      <guid isPermaLink="false">2510.19444v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Learning Noise-Resilient and Transferable Graph-Text Alignment via Dynamic Quality Assessment</title>
      <link>http://arxiv.org/abs/2510.19384v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ADAligner，一个动态、质量感知的图文本对齐框架，解决了现有CLIP风格图文本对齐器在处理多对多关系和适应不同数据质量方面的局限性。ADAligner能够根据监督质量在表达性的多对多和保守的一对一目标之间动态调整，在多个任务上表现出色，并具有更强的鲁棒性和更快的预训练速度。&lt;h4&gt;背景&lt;/h4&gt;在文本属性图上预训练图基础模型对于搜索、推荐和知识发现等网络规模应用至关重要。然而，现有的CLIP风格图文本对齐器面临两个关键限制：假设节点和文本之间存在严格的一对一对应关系，忽略了现实世界图中的固有多对多关系；以及依赖于静态对齐目标，无法适应不同的数据质量，在有噪声监督下变得脆弱。&lt;h4&gt;目的&lt;/h4&gt;解决现有图文本对齐器的局限性，提出一个动态、质量感知的图文本对齐框架，能够根据监督质量在表达性的多对多和保守的一对一目标之间动态调整。&lt;h4&gt;方法&lt;/h4&gt;提出了ADAligner框架，实时估计批次级别的对齐可靠性，并相应地调整优化过程：当监督干净时，促进软的、子图级别的多对多对齐；在噪声下，通过动态过滤低置信度配对来强调可靠的一对一对齐。理论上证明了这种动态机制形成一个稳定的负反馈过程，确保收敛性和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在九个不同的TAG数据集上的实验表明，ADAligner在零样本/少样本节点分类、链接预测和跨模态检索任务上一致优于先前的图文本对齐器。在有噪声监督下保持强大的鲁棒性，与多模态基线相比，预训练速度加快约2到3倍。&lt;h4&gt;结论&lt;/h4&gt;ADAligner为现实网络环境中的图文本表示学习建立了一个可扩展和可靠的基础。&lt;h4&gt;翻译&lt;/h4&gt;在文本属性图上预训练图基础模型对于搜索、推荐和知识发现等网络规模应用至关重要。然而，现有的CLIP风格图文本对齐器面临两个关键限制：它们假设节点和文本之间存在严格的一对一对应关系，忽略了现实世界图中的固有多对多关系；并且它们依赖于静态对齐目标，无法适应不同的数据质量，在有噪声监督下变得脆弱。总之，这些限制暴露了一个核心困境：拥抱表达性的多对多对齐会放大噪声，而恢复到严格的一对一策略则会牺牲语义多样性，无法处理本质上不匹配的配对。为了应对这些挑战，我们提出了ADAligner，一个动态、质量感知的图文本对齐框架，根据监督质量在表达性的多对多和保守的一对一目标之间动态调整。ADAligner实时估计批次级别的对齐可靠性，并相应地调整其优化过程，在监督干净时促进软的、子图级别的多对多对齐，同时在噪声下通过动态过滤低置信度配对来强调可靠的一对一对齐。理论上，我们证明这种动态机制形成一个稳定的负反馈过程，确保收敛性和鲁棒性。在九个不同的TAG数据集上的综合实验表明，ADAligner在零样本/少样本节点分类、链接预测和跨模态检索任务上一致地优于先前的图文本对齐器。它在有噪声监督下保持强大的鲁棒性，与多模态基线相比，预训练速度加快约2到3倍，为现实网络环境中的图文本表示学习建立了一个可扩展和可靠的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training Graph Foundation Models (GFMs) on text-attributed graphs (TAGs)is central to web-scale applications such as search, recommendation, andknowledge discovery. However, existing CLIP-style graph-text aligners face twokey limitations: they assume strict one-to-one correspondences between nodesand texts, overlooking the inherent many-to-many relations in real-worldgraphs; and they rely on static alignment objectives that cannot adapt tovarying data quality, making them brittle under noisy supervision. Together,these limitations expose a core dilemma: embracing expressive many-to-manyalignment amplifies noise, while reverting to strict one-to-one strategiessacrifices semantic diversity and fails to handle inherently mismatched pairs.To address these challenges, we propose ADAligner, a dynamic, quality-awaregraph-text alignment framework that dynamically adjusts between expressivemany-to-many and conservative one-to-one objectives according to supervisionquality. ADAligner estimates batch-level alignment reliability in real time andadapts its optimization accordingly, promoting soft, subgraph-levelmany-to-many alignment when supervision is clean, while emphasizing reliableone-to-one alignment by dynamically filtering low-confidence pairs under noise.Theoretically, we prove that this dynamic mechanism forms a stable negativefeedback process, ensuring convergence and robustness. Comprehensiveexperiments on nine diverse TAG datasets demonstrate that ADAlignerconsistently outperforms prior graph-text aligners on zero-/few-shot nodeclassification, link prediction and cross-modal retrieval tasks. It maintainsstrong robustness under noisy supervision and accelerates pre-training byapproximately 2 to 3 times compared to multimodal baselines, establishing ascalable and reliable foundation for graph-text representation learning inreal-world web environments.</description>
      <author>example@mail.com (Yuhang Liu, Minglai Shao, Zengyi Wo, Yunlong Chu, Bing Hao, Shengzhong Liu, Ruijie Wang, Jianxin Li)</author>
      <guid isPermaLink="false">2510.19384v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>From Newborn to Impact: Bias-Aware Citation Prediction</title>
      <link>http://arxiv.org/abs/2510.19246v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种偏差感知引用预测框架，通过多智能体特征提取和鲁棒图表示学习，解决了新生论文引用预测中的两个关键研究空白，实验证明其有效性。&lt;h4&gt;背景&lt;/h4&gt;引用动态是获取研究影响的关键，支撑研究评估、学术推荐和知识扩散研究。引用预测对新生论文尤为重要，因为在没有引用信号和高度长尾分布的情况下，必须进行早期评估。&lt;h4&gt;目的&lt;/h4&gt;解决两个关键研究空白：一是对科学影响的隐含因素建模不足，导致依赖粗略代理指标；二是缺乏偏差感知学习，无法在低引用论文上提供稳定预测。&lt;h4&gt;方法&lt;/h4&gt;提出偏差感知引用预测框架，结合多智能体特征提取和鲁棒图表示学习。多智能体图共学习模块从元数据和外部资源中提取细粒度可解释信号，并与异构网络嵌入融合；同时采用鲁棒机制，包括两阶段前向过程、GroupDRO优化和正则化头。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实世界数据集上的综合实验证明了所提模型的有效性。模型实现了约百分之十三的错误指标降低和百分之五点五的排名指标显著改善。&lt;h4&gt;结论&lt;/h4&gt;提出的偏差感知引用预测框架能够有效解决现有研究空白，提高引用预测的准确性和稳定性。&lt;h4&gt;翻译&lt;/h4&gt;作为获取研究影响的关键，引用动态支撑着研究评估、学术推荐和知识扩散研究。引用预测对新生论文尤为重要，因为在没有引用信号和高度长尾分布的情况下，必须进行早期评估。我们确定了两个关键研究空白：一是对科学影响的隐含因素建模不足，导致依赖粗略代理指标；二是缺乏偏差感知学习，无法在低引用论文上提供稳定预测。我们通过提出偏差感知引用预测框架来解决这些空白，该框架结合了多智能体特征提取和鲁棒图表示学习。首先，多智能体图共学习模块从元数据和外部资源中推导出细粒度、可解释的信号，如可重复性、协作网络和文本质量，并将它们与异构网络嵌入融合，即使在缺乏早期引用信号的情况下也能提供丰富的监督。其次，我们加入了一套鲁棒机制：一个将显性因素通过中间曝光估计路由的两阶段前向过程，用于优化跨环境最坏情况组风险的GroupDRO，以及在单调性和平滑性约束下对可控因素执行假设分析的正则化头。在两个真实世界数据集上的综合实验证明了我们提出的模型的有效性。具体而言，我们的模型在错误指标上实现了约百分之十三的降低，在排名指标上比基线方法有显著的百分之五点五的改善。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a key to accessing research impact, citation dynamics underpins researchevaluation, scholarly recommendation, and the study of knowledge diffusion.Citation prediction is particularly critical for newborn papers, where earlyassessment must be performed without citation signals and under highlylong-tailed distributions. We identify two key research gaps: (i) insufficientmodeling of implicit factors of scientific impact, leading to reliance oncoarse proxies; and (ii) a lack of bias-aware learning that can deliver stablepredictions on lowly cited papers. We address these gaps by proposing aBias-Aware Citation Prediction Framework, which combines multi-agent featureextraction with robust graph representation learning. First, a multi-agent xgraph co-learning module derives fine-grained, interpretable signals, such asreproducibility, collaboration network, and text quality, from metadata andexternal resources, and fuses them with heterogeneous-network embeddings toprovide rich supervision even in the absence of early citation signals. Second,we incorporate a set of robust mechanisms: a two-stage forward process thatroutes explicit factors through an intermediate exposure estimate, GroupDRO tooptimize worst-case group risk across environments, and a regularization headthat performs what-if analyses on controllable factors under monotonicity andsmoothness constraints. Comprehensive experiments on two real-world datasetsdemonstrate the effectiveness of our proposed model. Specifically, our modelachieves around a 13% reduction in error metrics (MALE and RMSLE) and a notable5.5% improvement in the ranking metric (NDCG) over the baseline methods.</description>
      <author>example@mail.com (Mingfei Lu, Mengjia Wu, Jiawei Xu, Weikai Li, Feng Liu, Ying Ding, Yizhou Sun, Jie Lu, Yi Zhang)</author>
      <guid isPermaLink="false">2510.19246v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence</title>
      <link>http://arxiv.org/abs/2510.19212v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;人工智能的理论和方法基础实际上是统计学，而非仅仅来自计算机科学。统计学为机器学习和现代AI提供了不可或缺的基础。&lt;h4&gt;背景&lt;/h4&gt;人工智能的快速发展通常被描述为来自计算机科学和工程的革命，但这种描述掩盖了一个基本事实：AI的理论和方法核心一直是统计学。&lt;h4&gt;目的&lt;/h4&gt;系统性地论证统计学为机器学习和现代AI提供了不可或缺的基础，并呼吁教育、研究和实践重新拥抱这一统计学基础。&lt;h4&gt;方法&lt;/h4&gt;将AI分解为九个基础支柱（推断、密度估计、序列学习、泛化、表示学习、可解释性、因果性、优化和统一），展示每个支柱都建立在百年统计原理之上。&lt;h4&gt;主要发现&lt;/h4&gt;AI的九个基础支柱都建立在统计原理之上；从假设检验和估计的推断框架到聚类和生成式AI的密度估计根源；从启发循环网络的时间序列分析到提供真正理解的因果模型；统计学提供了理论框架、不确定性量化等'大脑'功能，而计算机科学提供了可扩展算法和硬件等'肌肉'功能。&lt;h4&gt;结论&lt;/h4&gt;承认统计学的基础对于开发更强大、可解释和值得信赖的智能系统是必要的步骤。没有统计学习就没有机器学习；没有统计思维就没有人工智能。&lt;h4&gt;翻译&lt;/h4&gt;人工智能的迅速崛起通常被描述为一场源于计算机科学和工程学的革命。然而，这种叙事掩盖了一个基本事实：AI的理论和方法核心，并且一直是，统计学的。本文系统性地论证统计学领域为机器学习和现代AI提供了不可或缺的基础。我们将AI分解为九个基础支柱——推断、密度估计、序列学习、泛化、表示学习、可解释性、因果性、优化和统一——证明每一个都建立在百年统计原理之上。从支撑模型评估的假设检验和估计推断框架，到聚类和生成式AI的密度估计根源；从启发循环网络的时间序列分析到提供真正理解的因果模型，我们追溯了一条不间断的统计谱系。在庆祝推动现代AI的计算引擎的同时，我们认为统计学提供了'大脑'——理论框架、不确定性量化和推断目标——而计算机科学提供了'肌肉'——可扩展算法和硬件。认识到这一统计基础不仅仅是一个学术练习，而是开发更强大、可解释和值得信赖的智能系统的必要步骤。我们呼吁教育、研究和实践重新拥抱这一统计基础。忽视这些根基可能会构建一个脆弱的未来；拥抱它们才是通向真正智能机器的道路。没有统计学习就没有机器学习；没有统计思维就没有人工智能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid ascent of artificial intelligence (AI) is often portrayed as arevolution born from computer science and engineering. This narrative, however,obscures a fundamental truth: the theoretical and methodological core of AI is,and has always been, statistical. This paper systematically argues that thefield of statistics provides the indispensable foundation for machine learningand modern AI. We deconstruct AI into nine foundational pillars-Inference,Density Estimation, Sequential Learning, Generalization, RepresentationLearning, Interpretability, Causality, Optimization, andUnification-demonstrating that each is built upon century-old statisticalprinciples. From the inferential frameworks of hypothesis testing andestimation that underpin model evaluation, to the density estimation roots ofclustering and generative AI; from the time-series analysis inspiring recurrentnetworks to the causal models that promise true understanding, we trace anunbroken statistical lineage. While celebrating the computational engines thatpower modern AI, we contend that statistics provides the brain-the theoreticalframeworks, uncertainty quantification, and inferential goals-while computerscience provides the brawn-the scalable algorithms and hardware. Recognizingthis statistical backbone is not merely an academic exercise, but a necessarystep for developing more robust, interpretable, and trustworthy intelligentsystems. We issue a call to action for education, research, and practice tore-embrace this statistical foundation. Ignoring these roots risks building afragile future; embracing them is the path to truly intelligent machines. Thereis no machine learning without statistical learning; no artificial intelligencewithout statistical thought.</description>
      <author>example@mail.com (Ernest Fokoué)</author>
      <guid isPermaLink="false">2510.19212v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection on Contaminated Training Data--Extended Version</title>
      <link>http://arxiv.org/abs/2510.18998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages. An extended version of "An Encode-then-Decompose Approach  to Unsupervised Time Series Anomaly Detection on Contaminated Training Data"  accepted at ICDE 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的编码-分解范式和基于互信息的度量方法，用于时间序列异常检测，提高了对污染时间序列的鲁棒性，并在多个基准测试上取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;时间序列异常检测在现代大规模系统中至关重要，应用于多个领域分析和监控系统运行。无监督方法因不需要异常标签而受到广泛关注，避免了高成本并具有更广泛的应用。&lt;h4&gt;目的&lt;/h4&gt;解决自动编码器学习到的表示对训练时间序列中的异常敏感导致准确性降低的问题，提高方法在污染数据上的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出编码-分解范式，将编码表示分解为稳定表示和辅助表示；同时提出基于互信息的新度量方法替代重构误差来识别异常。&lt;h4&gt;主要发现&lt;/h4&gt;在八个常用的多变量和单变量时间序列基准测试上展示了具有竞争力或最先进的性能，对不同污染比例的时间序列表现出鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;新方法通过分解编码表示和使用互信息度量，有效提高了时间序列异常检测的准确性和鲁棒性，特别是在训练数据存在异常污染的情况下。&lt;h4&gt;翻译&lt;/h4&gt;时间序列异常检测在现代大规模系统中很重要，并应用于各种领域以分析和监控不同系统的运行。无监督方法引起了广泛关注，因为它们在训练期间不需要异常标签，从而避免了潜在的高成本并具有更广泛的应用。其中，自动编码器受到了广泛关注。它们使用来自压缩表示的重构误差来定义异常分数。然而，自动编码器学习到的表示对训练时间序列中的异常敏感，导致准确性降低。我们提出了一种新颖的编码-分解范式，将编码表示分解为稳定表示和辅助表示，从而在使用污染时间序列进行训练时增强鲁棒性。此外，我们提出了一种基于互信息的新指标来替代重构误差以识别异常。我们的提案在八个常用的多变量和单变量时间序列基准测试上展示了具有竞争力或最先进的性能，并对具有不同污染比例的时间序列表现出鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series anomaly detection is important in modern large-scale systems andis applied in a variety of domains to analyze and monitor the operation ofdiverse systems. Unsupervised approaches have received widespread interest, asthey do not require anomaly labels during training, thus avoiding potentiallyhigh costs and having wider applications. Among these, autoencoders havereceived extensive attention. They use reconstruction errors from compressedrepresentations to define anomaly scores. However, representations learned byautoencoders are sensitive to anomalies in training time series, causingreduced accuracy. We propose a novel encode-then-decompose paradigm, where wedecompose the encoded representation into stable and auxiliary representations,thereby enhancing the robustness when training with contaminated time series.In addition, we propose a novel mutual information based metric to replace thereconstruction errors for identifying anomalies. Our proposal demonstratescompetitive or state-of-the-art performance on eight commonly used multi- andunivariate time series benchmarks and exhibits robustness to time series withdifferent contamination ratios.</description>
      <author>example@mail.com (Buang Zhang, Tung Kieu, Xiangfei Qiu, Chenjuan Guo, Jilin Hu, Aoying Zhou, Christian S. Jensen, Bin Yang)</author>
      <guid isPermaLink="false">2510.18998v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>SBAN: A Framework \&amp; Multi-Dimensional Dataset for Large Language Model Pre-Training and Software Code Mining</title>
      <link>http://arxiv.org/abs/2510.18936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为SBAN的大规模多维度数据集，用于推进大型语言模型在软件代码分析方面的预训练和评估。&lt;h4&gt;背景&lt;/h4&gt;软件代码分析领域需要大规模、多模态的数据集来支持大型语言模型的训练和评估，特别是在安全分析和软件理解方面。&lt;h4&gt;目的&lt;/h4&gt;创建一个包含源代码、二进制代码、汇编指令和自然语言描述的多维度数据集，以支持跨表示学习、软件语义理解和自动化恶意软件检测等研究。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含超过300万个样本的数据集，其中包括290万个良性样本和672,000个恶意软件样本，每个样本都通过四个互补层表示：二进制代码、汇编指令、自然语言描述和源代码。&lt;h4&gt;主要发现&lt;/h4&gt;这种独特的多模态结构支持跨表示学习研究，并且可以应用于安全分析、代码翻译、代码解释和其他涉及异构数据的软件挖掘任务。&lt;h4&gt;结论&lt;/h4&gt;SBAN数据集通过桥接低级机器表示和高级人类语义，为构建能够推理代码的智能系统提供了坚实的基础，为挖掘软件行为、改进安全分析和增强大型语言模型在软件代码挖掘方面的能力开辟了新的机会。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了SBAN（源代码、二进制代码、汇编指令和自然语言描述），这是一个大规模、多维度数据集，旨在推进大型语言模型在软件代码分析方面的预训练和评估。SBAN包含超过300万个样本，其中包括290万个良性样本和672,000个恶意软件样本，每个样本都通过四个互补层表示：二进制代码、汇编指令、自然语言描述和源代码。这种独特的多模态结构支持跨表示学习研究、软件语义理解和自动化恶意软件检测。除了安全应用外，SBAN还支持更广泛的任务，如代码翻译、代码解释和其他涉及异构数据的软件挖掘任务。它特别适合深度模型的可扩展训练，包括变压器和其他大型语言模型架构。通过桥接低级机器表示和高级人类语义，SBAN为构建能够推理代码的智能系统提供了坚实的基础。我们相信，这个数据集为挖掘软件行为、改进安全分析和增强大型语言模型在软件代码挖掘的预训练和微调任务方面的能力开辟了新的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces SBAN (Source code, Binary, Assembly, and NaturalLanguage Description), a large-scale, multi-dimensional dataset designed toadvance the pre-training and evaluation of large language models (LLMs) forsoftware code analysis. SBAN comprises more than 3 million samples, including2.9 million benign and 672,000 malware respectively, each represented acrossfour complementary layers: binary code, assembly instructions, natural languagedescriptions, and source code. This unique multimodal structure enablesresearch on cross-representation learning, semantic understanding of software,and automated malware detection. Beyond security applications, SBAN supportsbroader tasks such as code translation, code explanation, and other softwaremining tasks involving heterogeneous data. It is particularly suited forscalable training of deep models, including transformers and other LLMarchitectures. By bridging low-level machine representations and high-levelhuman semantics, SBAN provides a robust foundation for building intelligentsystems that reason about code. We believe that this dataset opens newopportunities for mining software behavior, improving security analytics, andenhancing LLM capabilities in pre-training and fine-tuning tasks for softwarecode mining.</description>
      <author>example@mail.com (Hamed Jelodar, Mohammad Meymani, Samita Bai, Roozbeh Razavi-Far, Ali A. Ghorbani)</author>
      <guid isPermaLink="false">2510.18936v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>A flexible framework for structural plasticity in GPU-accelerated sparse spiking neural networks</title>
      <link>http://arxiv.org/abs/2510.19764v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 9 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的灵活框架，用于实现GPU加速的结构可塑性规则，展示了如何训练高效稀疏的脉冲神经网络分类器并学习拓扑图，稀疏模型可比密集模型训练速度快10倍。&lt;h4&gt;背景&lt;/h4&gt;大多数人工神经网络和生物大脑学习研究集中在突触可塑性上，而生物大脑中的结构可塑性（创建和移除连接）对有效学习、损伤恢复和资源优化同样重要。尽管受此启发，机器学习中常使用剪枝移除弱连接，但现有框架针对密集连接优化，无法降低大型模型的训练成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种支持结构可塑性规则的GPU加速框架，用于训练高效稀疏的SNN分类器，并在无监督学习背景下实现拓扑图形成，探索稀疏性的计算优势。&lt;h4&gt;方法&lt;/h4&gt;基于GeNN模拟器，使用e-prop监督学习规则和DEEP R训练稀疏SNN分类器，然后在无监督学习场景中应用该框架学习拓扑图。&lt;h4&gt;主要发现&lt;/h4&gt;稀疏分类器比基准密集模型训练时间减少高达10倍，同时通过DEEP R重连线保持与原始模型相当的性能；在比实时更快的模拟中成功展示了拓扑图形成，提供了连接演变的见解，并测量了模拟速度与网络规模的关系。&lt;h4&gt;结论&lt;/h4&gt;该框架使研究人员能够探索网络结构和神经通信中的稀疏性维持，以及在各种神经形态应用中稀疏性的计算优势。&lt;h4&gt;翻译&lt;/h4&gt;关于人工神经网络训练和生物大脑学习建模的大多数研究都集中在突触可塑性上，其中学习等同于改变现有连接的强度。然而，在生物大脑中，结构可塑性——创建新连接和移除其他连接——同样重要，不仅对有效学习至关重要，还有助于从损伤中恢复和优化资源使用。受结构可塑性启发，剪枝常用于机器学习以从训练好的模型中移除弱连接，从而降低推理的计算需求。然而，通常用于基于反向传播训练ANN和SNN的机器学习框架针对密集连接进行了优化，这意味着剪枝无法帮助降低不断增长的模型的训练成本。GeNN模拟器已经支持稀疏SNN的高效GPU加速模拟，用于计算神经科学和机器学习。在这里，我们提出了一个新的灵活框架，用于实现GPU加速的结构可塑性规则，并首先使用e-prop监督学习规则和DEEP R训练高效稀疏的SNN分类器，然后在无监督学习背景下学习拓扑图。与基准密集模型相比，我们的稀疏分类器将训练时间减少了高达10倍，而DEEP R重连线使它们能够与原始模型一样好地执行。我们在比实时更快的模拟中展示了拓扑图的形成，提供了连接演变的见解，并测量了模拟速度与网络规模的关系。所提出的框架将使进一步研究能够在网络结构和神经通信中实现和保持稀疏性，以及探索稀疏性在各种神经形态应用中的计算优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The majority of research in both training Artificial Neural Networks (ANNs)and modeling learning in biological brains focuses on synaptic plasticity,where learning equates to changing the strength of existing connections.However, in biological brains, structural plasticity - where new connectionsare created and others removed - is also vital, not only for effective learningbut also for recovery from damage and optimal resource usage. Inspired bystructural plasticity, pruning is often used in machine learning to remove weakconnections from trained models to reduce the computational requirements ofinference. However, the machine learning frameworks typically used forbackpropagation-based training of both ANNs and Spiking Neural Networks (SNNs)are optimized for dense connectivity, meaning that pruning does not help reducethe training costs of ever-larger models. The GeNN simulator already supportsefficient GPU-accelerated simulation of sparse SNNs for computationalneuroscience and machine learning. Here, we present a new flexible frameworkfor implementing GPU-accelerated structural plasticity rules and demonstratethis first using the e-prop supervised learning rule and DEEP R to trainefficient, sparse SNN classifiers and then, in an unsupervised learningcontext, to learn topographic maps. Compared to baseline dense models, oursparse classifiers reduce training time by up to 10x while the DEEP R rewiringenables them to perform as well as the original models. We demonstratetopographic map formation in faster-than-realtime simulations, provide insightsinto the connectivity evolution, and measure simulation speed versus networksize. The proposed framework will enable further research into achieving andmaintaining sparsity in network structure and neural communication, as well asexploring the computational benefits of sparsity in a range of neuromorphicapplications.</description>
      <author>example@mail.com (James C. Knight, Johanna Senk, Thomas Nowotny)</author>
      <guid isPermaLink="false">2510.19764v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Study of Training Dynamics for Memory-Constrained Fine-Tuning</title>
      <link>http://arxiv.org/abs/2510.19675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了TraDy，一种内存高效的深度神经网络迁移学习方案，通过动态通道选择和层重要性预判实现严格内存约束下的高性能训练。&lt;h4&gt;背景&lt;/h4&gt;随着深度神经网络模型规模不断增大，而部署环境对资源有严格限制，内存高效训练变得越来越重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在严格内存约束下实现高效训练的深度神经网络训练方法。&lt;h4&gt;方法&lt;/h4&gt;TraDy利用两个关键洞察：更新的层重要性依赖于架构且可预先确定；动态随机通道选择相比静态方法能提供更好的梯度近似。引入动态通道选择方法，在预选层内周期性地随机重新采样通道。&lt;h4&gt;主要发现&lt;/h4&gt;TraDy在各种下游任务和架构上取得最先进性能；实现高达99%的激活稀疏性；实现95%的权重导数稀疏性；权重导数计算的计算量减少97%。&lt;h4&gt;结论&lt;/h4&gt;TraDy是一种有效的内存高效训练方法，能够在保持性能的同时显著减少内存使用和计算需求。&lt;h4&gt;翻译&lt;/h4&gt;随着模型规模扩大而部署环境施加严格的资源限制，深度神经网络的内存高效训练变得越来越重要。我们提出了TraDy，一种利用两个关键洞察的新型迁移学习方案：更新的层重要性依赖于架构且可预先确定，而动态随机通道选择相比静态方法能提供更好的梯度近似。我们引入了一种动态通道选择方法，在预选层内周期性地随机重新采样通道。大量实验表明，TraDy在各种下游任务和架构上取得了最先进性能，同时保持严格的内存约束，实现了高达99%的激活稀疏性、95%的权重导数稀疏性，以及97%的权重导数计算FLOPs减少。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Memory-efficient training of deep neural networks has become increasinglyimportant as models grow larger while deployment environments impose strictresource constraints. We propose TraDy, a novel transfer learning schemeleveraging two key insights: layer importance for updates isarchitecture-dependent and determinable a priori, while dynamic stochasticchannel selection provides superior gradient approximation compared to staticapproaches. We introduce a dynamic channel selection approach thatstochastically resamples channels between epochs within preselected layers.Extensive experiments demonstrate TraDy achieves state-of-the-art performanceacross various downstream tasks and architectures while maintaining strictmemory constraints, achieving up to 99% activation sparsity, 95% weightderivative sparsity, and 97% reduction in FLOPs for weight derivativecomputation.</description>
      <author>example@mail.com (Aël Quélennec, Nour Hezbri, Pavlo Mozharovskyi, Van-Tam Nguyen, Enzo Tartaglione)</author>
      <guid isPermaLink="false">2510.19675v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning Beyond the Standard Model</title>
      <link>http://arxiv.org/abs/2510.19168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4+8 pages, 7 figures. Accepted at NeurIPS 2025 Workshop: Machine  Learning and the Physical Sciences&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究展示了如何通过迁移学习减少宇宙学模拟成本，使用标准ΛCDM模型预训练，然后在各种超越ΛCDM场景上微调，发现预训练可以显著减少所需模拟数量，但也存在负迁移风险，特别是当参数间存在强物理简并时。&lt;h4&gt;背景&lt;/h4&gt;机器学习能够实现强大的宇宙学推断，但通常需要大量高保真模拟来覆盖多种宇宙学模型，这带来了高昂的计算成本。&lt;h4&gt;目的&lt;/h4&gt;探索通过迁移学习重用不同宇宙学模型间的知识，以减少模拟成本并提高推断效率。&lt;h4&gt;方法&lt;/h4&gt;在标准ΛCDM宇宙学模型上进行预训练，然后在各种超越ΛCDM场景(包括大质量中微子、修正引力、原始非高斯性)上进行微调，并测试包含瓶颈结构的不同迁移架构。&lt;h4&gt;主要发现&lt;/h4&gt;预训练可以在使用显著更少的超越ΛCDM模拟的情况下实现推断；当ΛCDM和超越ΛCDM参数之间存在强物理简并时，可能会发生负迁移；包含瓶颈结构的迁移架构提供了最佳性能。&lt;h4&gt;结论&lt;/h4&gt;预训练可以加速宇宙学推断过程，但也可能阻碍对新物理的学习，基础模型方法在物理学应用中既带来机会也存在潜在陷阱。&lt;h4&gt;翻译&lt;/h4&gt;机器学习能够实现强大的宇宙学推断，但通常需要覆盖多种宇宙学模型的大量高保真模拟。迁移学习提供了一种通过跨模型重用知识来减少模拟成本的方法。我们展示了在标准宇宙学模型ΛCDM上进行预训练，并在各种超越ΛCDM场景(包括大质量中微子、修正引力和原始非高斯性)上进行微调，可以使用显著更少的超越ΛCDM模拟实现推断。然而，我们也表明当ΛCDM和超越ΛCDM参数之间存在强物理简并时，可能会发生负迁移。我们考虑了各种迁移架构，发现包含瓶颈结构的架构提供最佳性能。我们的研究结果阐明了基础模型方法在物理学中的机会和陷阱：预训练可以加速推断，但也可能阻碍对新物理的学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning enables powerful cosmological inference but typicallyrequires many high-fidelity simulations covering many cosmological models.Transfer learning offers a way to reduce the simulation cost by reusingknowledge across models. We show that pre-training on the standard model ofcosmology, $\Lambda$CDM, and fine-tuning on various beyond-$\Lambda$CDMscenarios -- including massive neutrinos, modified gravity, and primordialnon-Gaussianities -- can enable inference with significantly fewerbeyond-$\Lambda$CDM simulations. However, we also show that negative transfercan occur when strong physical degeneracies exist between $\Lambda$CDM andbeyond-$\Lambda$CDM parameters. We consider various transfer architectures,finding that including bottleneck structures provides the best performance. Ourfindings illustrate the opportunities and pitfalls of foundation-modelapproaches in physics: pre-training can accelerate inference, but may alsohinder learning new physics.</description>
      <author>example@mail.com (Veena Krishnaraj, Adrian E. Bayer, Christian Kragh Jespersen, Peter Melchior)</author>
      <guid isPermaLink="false">2510.19168v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2510.14810v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为结构投影Hebbian表示（SPHeRe）的新型无监督学习方法，通过整合正交性和结构信息保留，解决了传统Hebbian学习在机器学习中的局限性。实验表明，该方法在图像分类、持续学习、迁移学习和图像重建任务中均表现出色，证明了Hebbian无监督学习在现代深度学习框架中的竞争力和潜力。&lt;h4&gt;背景&lt;/h4&gt;Hebbian学习是一种生物学原理，描述了神经元如何通过重复刺激调整其连接。然而，在机器学习中应用时，由于连接更新不受约束且缺乏反馈介导，它存在严重问题，限制了其在复杂网络架构和任务中的有效扩展。&lt;h4&gt;目的&lt;/h4&gt;解决Hebbian学习在机器学习中的局限性，提出一种能够有效扩展到复杂网络架构和任务的无监督学习方法。&lt;h4&gt;方法&lt;/h4&gt;引入结构投影Hebbian表示（SPHeRe），一种新的无监督学习方法，通过局部的辅助非线性块整合正交性和结构信息保留。结构信息保留的损失通过辅助的轻量级投影反向传播到输入，充当反馈介导，正交性约束则考虑了更新幅度的有界性。&lt;h4&gt;主要发现&lt;/h4&gt;SPHeRe在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准图像分类基准测试上，在无监督突触可塑性方法中达到了最先进的性能。该方法在持续学习和迁移学习场景中表现出强大的有效性，图像重建任务显示了所提取特征的鲁棒性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Hebbian无监督学习规则在现代深度学习框架中具有竞争力和潜力，展示了不依赖于严格反向传播的高效和生物启发式学习算法的可能性。&lt;h4&gt;翻译&lt;/h4&gt;Hebbian学习是一种生物学原理，直观地描述了神经元如何通过重复刺激来调整其连接。然而，当应用于机器学习时，由于连接更新不受约束且缺乏反馈介导，它存在严重问题。这些缺点限制了其有效扩展到复杂的网络架构和任务。为此，我们在这里引入了结构投影Hebbian表示（SPHeRe），一种新的无监督学习方法，它通过一个局部的辅助非线性块整合了正交性和结构信息保留。结构信息保留的损失通过一个辅助的轻量级投影反向传播到输入，该投影在概念上充当反馈介导，而正交性约束则考虑了更新幅度的有界性。大量的实验结果表明，SPHeRe在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准图像分类基准测试中，在无监督突触可塑性方法中达到了最先进的性能。此外，该方法在持续学习和迁移学习场景中表现出强大的有效性，图像重建任务显示了所提取特征的鲁棒性和泛化能力。这项工作证明了Hebbian无监督学习规则在现代深度学习框架中的竞争力和潜力，展示了不依赖于严格反向传播的高效和生物启发式学习算法的可能性。我们的代码可在https://github.com/brain-intelligence-lab/SPHeRe获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hebbian learning is a biological principle that intuitively describes howneurons adapt their connections through repeated stimuli. However, when appliedto machine learning, it suffers serious issues due to the unconstrained updatesof the connections and the lack of accounting for feedback mediation. Suchshortcomings limit its effective scaling to complex network architectures andtasks. To this end, here we introduce the Structural Projection HebbianRepresentation (SPHeRe), a novel unsupervised learning method that integratesorthogonality and structural information preservation through a local auxiliarynonlinear block. The loss for structural information preservationbackpropagates to the input through an auxiliary lightweight projection thatconceptually serves as feedback mediation while the orthogonality constraintsaccount for the boundedness of updating magnitude. Extensive experimentalresults show that SPHeRe achieves SOTA performance among unsupervised synapticplasticity approaches on standard image classification benchmarks, includingCIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strongeffectiveness in continual learning and transfer learning scenarios, and imagereconstruction tasks show the robustness and generalizability of the extractedfeatures. This work demonstrates the competitiveness and potential of Hebbianunsupervised learning rules within modern deep learning frameworks,demonstrating the possibility of efficient and biologically inspired learningalgorithms without the strong dependence on strict backpropagation. Our code isavailable at https://github.com/brain-intelligence-lab/SPHeRe.</description>
      <author>example@mail.com (Shikuang Deng, Jiayuan Zhang, Yuhang Wu, Ting Chen, Shi Gu)</author>
      <guid isPermaLink="false">2510.14810v2</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>$\nabla$-SDF: Learning Euclidean Signed Distance Functions Online with Gradient-Augmented Octree Interpolation and Neural Residual</title>
      <link>http://arxiv.org/abs/2510.18999v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为∇-SDF的混合方法，用于从点云数据估计符号距离函数(SDF)，结合了显式先验和隐式神经残差，实现了高效率、高准确性和可微性的SDF重建。&lt;h4&gt;背景&lt;/h4&gt;从点云数据估计符号距离函数(SDF)对机器人自主能力(如定位、建图、运动规划和控制)有很多好处。现有方法中，支持在线和大规模SDF重建的体积方法会影响SDF估计的连续性和可微性；而神经网络方法虽然能提供高保真度和可微的SDF重建，但效率较低，在大环境中可能面临灾难性遗忘和内存限制，且通常仅限于截断的SDF。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实现非截断(欧几里得)SDF重建的方法，同时具有体积方法的计算和内存效率以及神经网络方法的可微性和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出∇-SDF，一种混合方法，结合了从梯度增强八叉树插值获得的显式先验和隐式神经残差。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明，∇-SDF在准确性和效率方面优于现有最先进的技术，为机器人技术和计算机视觉中的下游任务提供了可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;∇-SDF为机器人自主能力中的SDF估计提供了一个高效、准确且可微的解决方案，克服了现有方法的局限性，为下游任务提供了可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;从点云数据估计符号距离函数(SDF)已被证明有益于许多机器人自主能力，包括定位、建图、运动规划和控制。支持在线和大规模SDF重建的方法往往依赖于离散的体积数据结构，这会影响SDF估计的连续性和可微性。最近，使用隐式特征的神经网络方法展示了高保真度和可微的SDF重建，但它们往往效率较低，在大环境中可能会经历灾难性遗忘和内存限制，并且通常仅限于截断的SDF。本文提出了∇-SDF，一种混合方法，结合了从梯度增强八叉树插值获得的显式先验和隐式神经残差。我们的方法实现了非截断(欧几里得)的SDF重建，计算和内存效率与体积方法相当，可微性和准确性可与神经网络方法相媲美。大量实验证明，∇-SDF在准确性和效率方面优于最先进的技术，为机器人技术和计算机视觉中的下游任务提供了可扩展的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从点云数据在线学习欧几里得符号距离函数（SDF）的问题。这个问题在机器人自主和计算机视觉领域非常重要，因为准确且可微分的几何环境表示对机器人定位、建图、运动规划和控制等关键功能至关重要。快速更新环境模型和获取梯度信息能让机器人更安全、精确地导航和交互环境，而小内存占用对表示在大场景中的可扩展性很重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有SDF重建方法的优缺点：体积方法实时性好但不可微；神经网络方法可微分但效率低且易遗忘；高斯过程方法连续但计算复杂。作者借鉴了H2-Mapping的八叉树先验和神经网络残差思想，以及HIO-SDF的全局SDF表示方法。作者设计了一种混合方法，结合显式八叉树先验和隐式神经残差，使用半稀疏八叉树结构和梯度增强插值提高精度，并设计了三种损失函数加速训练，从而克服了现有方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合显式八叉树先验和隐式神经残差的混合模型，实现高效、可微分且全局准确的SDF重建。整体流程包括：1)使用半稀疏八叉树存储SDF值和梯度，通过梯度增强插值获得SDF先验；2)使用多分辨率哈希网格编码器和MLP解码器预测SDF残差修正；3)选择关键帧保持训练数据代表性；4)生成表面点、扰动点和自由空间点三种训练样本；5)使用重建损失、Eikonal损失和投影损失训练模型；6)最终SDF预测为八叉树先验与神经网络残差之和。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)梯度增强的八叉树插值方法，在顶点存储SDF值和梯度，提高先验精度；2)半稀疏八叉树结构，平衡内存和精度；3)混合显式-隐式模型，实现全空间而非仅近表面的SDF学习；4)三种精心设计的损失函数加速收敛。相比H2-Mapping，∇-SDF实现非截断SDF重建；相比HIO-SDF，直接优化八叉参数学习更准确先验；相比体积方法，提供可微SDF；相比纯神经网络方法，解决了大环境中的遗忘问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ∇-SDF通过梯度增强的八叉树插值与神经残差相结合，实现了高效、可微分且全局准确的在线符号距离函数重建，结合了体积方法和神经网络方法的优点，同时克服了它们的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimation of signed distance functions (SDFs) from point cloud data has beenshown to benefit many robot autonomy capabilities, including localization,mapping, motion planning, and control. Methods that support online andlarge-scale SDF reconstruction tend to rely on discrete volumetric datastructures, which affect the continuity and differentiability of the SDFestimates. Recently, using implicit features, neural network methods havedemonstrated high-fidelity and differentiable SDF reconstruction but they tendto be less efficient, can experience catastrophic forgetting and memorylimitations in large environments, and are often restricted to truncated SDFs.This work proposes $\nabla$-SDF, a hybrid method that combines an explicitprior obtained from gradient-augmented octree interpolation with an implicitneural residual. Our method achieves non-truncated (Euclidean) SDFreconstruction with computational and memory efficiency comparable tovolumetric methods and differentiability and accuracy comparable to neuralnetwork methods. Extensive experiments demonstrate that \methodname{}outperforms the state of the art in terms of accuracy and efficiency, providinga scalable solution for downstream tasks in robotics and computer vision.</description>
      <author>example@mail.com (Zhirui Dai, Qihao Qian, Tianxing Fan, Nikolay Atanasov)</author>
      <guid isPermaLink="false">2510.18999v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception</title>
      <link>http://arxiv.org/abs/2510.17568v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PAGE-4D是一个扩展到动态场景的前馈模型，解决了现有3D前馈模型在处理动态元素时的局限性，通过动态感知聚合器实现了无需后处理的相机姿态估计、深度预测和点云重建。&lt;h4&gt;背景&lt;/h4&gt;最新的3D前馈模型（如VGGT）在推断静态场景的3D属性方面表现出色，但这些模型通常在静态数据集上训练，因此在涉及移动人类或可变形物体等复杂动态元素的真实场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;引入PAGE-4D模型，将VGGT扩展到动态场景，实现相机姿态估计、深度预测和点云重建，无需后处理。&lt;h4&gt;方法&lt;/h4&gt;提出一种动态感知聚合器，通过预测动态感知掩码来解耦静态和动态信息，对于姿态估计抑制运动线索，对于几何重建则增强这些线索，从而解决多任务4D重建中任务间的固有冲突。&lt;h4&gt;主要发现&lt;/h4&gt;PAGE-4D在动态场景中始终优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点图重建方面取得了优越的结果。&lt;h4&gt;结论&lt;/h4&gt;PAGE-4D成功解决了多任务4D重建中任务之间的固有冲突，通过动态感知聚合器有效分离了静态和动态信息，在动态场景中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;最近的3D前馈模型，如视觉几何基础变换器（VGGT），在推断静态场景的3D属性方面表现出强大能力。然而，由于它们通常在静态数据集上训练，这些模型在涉及复杂动态元素的真实场景中往往表现不佳，例如移动的人或像伞这样的可变形物体。为解决这一局限性，我们引入了PAGE-4D，一种将VGGT扩展到动态场景的前馈模型，能够实现相机姿态估计、深度预测和点云重建，且无需后处理。多任务4D重建的一个核心挑战是任务之间的内在冲突：准确的相机姿态估计需要抑制动态区域，而几何重建则需要建模这些区域。为解决这一矛盾，我们提出了一种动态感知聚合器，通过预测动态感知掩码来解耦静态和动态信息——在姿态估计中抑制运动线索，而在几何重建中增强它们。大量实验表明，PAGE-4D在动态场景中始终优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点图重建方面取得了优越结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何在动态场景（包含移动人或可变形物体如伞的场景）中进行准确的3D重建问题。这个问题在现实中非常重要，因为我们的世界本质上是动态的，包含大量移动的物体和人。能够在动态场景中进行准确的3D重建对于机器人导航、增强现实、自动驾驶、视频编辑等多个应用领域至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到现有的静态3D重建模型在动态场景中表现不佳，尤其是在相机姿态估计和几何重建之间存在冲突：姿态估计需要抑制动态区域，而几何重建则需要建模这些区域。他们借鉴了VGGT作为基础模型，但针对动态场景进行了改进。通过分析VGGT在动态条件下的行为，发现它会忽略动态内容。基于这些观察，作者设计了一个动态感知聚合器，通过预测掩码来分离静态和动态信息，并采用针对性的微调策略，只更新对动态最敏感的中间层。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; PAGE-4D的核心思想是解耦动态区域在不同任务中的作用：在相机姿态估计时抑制动态区域，而在几何重建时利用这些区域的动态信息。整体实现流程包括：1) 使用预训练编码器提取图像特征；2) 通过动态感知聚合器整合空间和时间线索，包括帧间注意、帧内注意和动态感知全局注意；3) 使用轻量级解码器进行深度和点图重建；4) 专门的相机姿态估计解码器。特别的是，PAGE-4D预测一个动态掩码，通过交叉注意机制应用：过滤相机姿态令牌的动态内容，同时为几何令牌强调这些内容。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; PAGE-4D的关键创新点包括：1) 动态感知聚合器，通过预测掩码分离静态和动态信息；2) 针对性的微调策略，只更新对动态最敏感的中间层；3) 任务特定的动态处理，在不同任务中不同方式处理动态区域；4) 统一高效的框架，能在单一前向传递中同时完成多个任务。相比之前的工作，PAGE-4D不需要后处理，运行速度快，在动态场景中表现更好，能产生更密集和准确的点云重建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PAGE-4D通过解耦姿态和几何估计中的动态信息处理，首次实现了在单一前向模型中对动态场景的高效准确4D感知，显著超越了之前静态模型在动态环境中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent 3D feed-forward models, such as the Visual Geometry GroundedTransformer (VGGT), have shown strong capability in inferring 3D attributes ofstatic scenes. However, since they are typically trained on static datasets,these models often struggle in real-world scenarios involving complex dynamicelements, such as moving humans or deformable objects like umbrellas. Toaddress this limitation, we introduce PAGE-4D, a feedforward model that extendsVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, andpoint cloud reconstruction -- all without post-processing. A central challengein multi-task 4D reconstruction is the inherent conflict between tasks:accurate camera pose estimation requires suppressing dynamic regions, whilegeometry reconstruction requires modeling them. To resolve this tension, wepropose a dynamics-aware aggregator that disentangles static and dynamicinformation by predicting a dynamics-aware mask -- suppressing motion cues forpose estimation while amplifying them for geometry reconstruction. Extensiveexperiments show that PAGE-4D consistently outperforms the original VGGT indynamic scenarios, achieving superior results in camera pose estimation,monocular and video depth estimation, and dense point map reconstruction.</description>
      <author>example@mail.com (Kaichen Zhou, Yuhan Wang, Grace Chen, Xinhai Chang, Gaspard Beaudouin, Fangneng Zhan, Paul Pu Liang, Mengyu Wang)</author>
      <guid isPermaLink="false">2510.17568v2</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.15530v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为VO-DP的纯视觉单视角扩散策略学习方法，利用预训练视觉基础模型融合语义和几何特征，在模拟和真实世界任务中均表现出色，并开源了机器人操作训练库。&lt;h4&gt;背景&lt;/h4&gt;在模仿学习中，基于视觉运动的扩散策略学习是机器人操作的主要方向。现有方法大多依赖点云作为输入并通过点云特征学习构建场景表示，但对纯视觉解决方案的探索不足。&lt;h4&gt;目的&lt;/h4&gt;探索一种纯视觉且单视角的扩散策略学习方法，以克服对点云输入的依赖，并发挥视觉基础模型在机器人操作中的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出VO-DP方法，利用VGGT的中间特征结合DINOv2的语义特征和交替注意力块的几何特征，通过交叉注意力融合特征，并用CNN空间压缩后输入策略头。同时开源基于Accelerate的机器人操作训练库，支持多GPU并行训练和混合精度训练。&lt;h4&gt;主要发现&lt;/h4&gt;模拟任务中VO-DP成功率达64.6%，与点云方法DP3(64.0%)相当，远高于基线DP(34.8%)；真实世界任务中达到87.9%，显著优于DP3(67.5%)和DP(11.2%)。VO-DP在颜色、尺寸、背景和光照变化条件下保持高度稳定。&lt;h4&gt;结论&lt;/h4&gt;VO-DP证明了纯视觉解决方案在机器人操作中的巨大潜力，特别是在真实世界任务中表现出色。开源的训练库为机器人操作研究提供了有价值的资源。&lt;h4&gt;翻译&lt;/h4&gt;在模仿学习背景下，基于视觉运动的扩散策略学习是机器人操作的主要方向之一。大多数方法依赖点云作为观察输入，通过点云特征学习构建场景表示，实现显著准确性。然而，现有文献缺乏对具有巨大潜力的纯视觉解决方案的深入探索。本文提出纯视觉和单视角扩散策略学习方法(VO-DP)，利用预训练视觉基础模型实现语义和几何特征有效融合。使用VGGT中间特征，结合DINOv2语义特征和交替注意力块几何特征。特征通过交叉注意力融合，用CNN空间压缩后输入策略头。大量实验表明，VO-DP不仅显著优于纯视觉基线DP，且与点云方法DP3表现不同：模拟任务中VO-DP平均成功率达64.6%，与DP3的64.0%相当，远高于DP的34.8%；真实世界任务中达87.9%，显著优于DP3的67.5%和DP的11.2%。进一步鲁棒性评估证实VO-DP在颜色、尺寸、背景和光照变化条件下保持高度稳定。最后开源机器人操作训练库，基于Accelerate构建，支持多机器多GPU并行训练和混合精度训练，兼容DP、DP3和VO-DP等视觉运动策略，支持RoboTwin模拟器。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人操作领域中纯视觉（仅RGB图像）模仿学习方法性能不足的问题。这个问题很重要，因为现有基于点云或RGB-D图像的方法虽然精度高，但依赖昂贵的深度传感器，而纯视觉方法成本低、实用性强，但性能通常不如基于点云的方法。探索纯视觉方法的潜力可以显著降低机器人系统的硬件成本和复杂度，避免多传感器校准问题，并更接近生物感知-行动系统，具有广泛的应用前景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的优缺点，指出纯视觉方法的性能瓶颈主要在于表示学习模块不完善。他们借鉴了多项现有工作：利用预训练的VGGT模型提取几何信息，使用DINOv2提取语义特征，采用DP中的扩散策略框架，以及Transformer中的cross-attention机制进行特征融合。在此基础上，他们创新设计了语义-几何自适应融合模块和空间特征压缩模块，实现了从单目RGB图像中同时提取和融合语义与几何信息，为下游策略学习提供高质量输入。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练的视觉基础模型，从单目RGB图像中同时提取语义和几何特征，并通过自适应融合机制将这些特征有效结合，为下游策略学习提供高质量的输入。整体流程包括：1) 输入处理：接收单视图RGB图像序列；2) 特征提取：使用DINOv2提取语义特征，用VGGT的Alternating Attention网络提取几何特征；3) 特征融合：通过残差交叉注意力机制自适应融合语义和几何特征；4) 场景表示压缩：使用轻量级ResNet压缩融合后的特征，并与机器人关节状态连接形成场景表示；5) 动作生成：基于DDPM的策略头根据场景表示生成动作轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次实现纯视觉方法达到点云级别的性能；2) 创新设计语义-几何自适应融合机制；3) 高效的单视图表示学习方法；4) 开源DRRM训练框架。相比之前的工作：1) 相比传统纯视觉方法（如DP），性能显著提升；2) 相比基于点云的方法（如DP3），不需要昂贵深度传感器，在真实世界任务中表现更好；3) 相比其他纯视觉方法，更注重语义和几何特征的融合，在复杂场景中表现更好，对环境变化具有更强鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VO-DP通过创新性地融合预训练视觉模型的语义和几何特征，首次实现了仅使用RGB图像的机器人操作方法达到与基于点云方法相当的精度，同时大幅降低了硬件成本和系统复杂度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the context of imitation learning, visuomotor-based diffusion policylearning is one of the main directions in robotic manipulation. Most of theseapproaches rely on point clouds as observation inputs and construct scenerepresentations through point clouds feature learning, which enables them toachieve remarkable accuracy. However, the existing literature lacks an in-depthexploration of vision-only solutions that have significant potential. In thispaper, we propose a Vision-Only and single-view Diffusion Policy learningmethod (VO-DP) that leverages pretrained visual foundation models to achieveeffective fusion of semantic and geometric features. We utilize intermediatefeatures from VGGT incorporating semantic features from DINOv2 and geometricfeatures from Alternating Attention blocks. Features are fused viacross-attention and spatially compressed with a CNN to form the input to thepolicy head. Extensive experiments demonstrate that VO-DP not only outperformsthe vision-only baseline DP significantly but also exhibits distinctperformance trends against the point cloud-based method DP3: in simulationtasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,outperforming both DP3 67.5% and DP 11.2% by a notable margin. Furtherrobustness evaluations confirm that VO-DP remains highly stable under varyingconditions including color, size, background, and lighting. Lastly, weopen-source a training library for robotic manipulation. Built on Accelerate,this library supports multi-machine and multi-GPU parallel training, as well asmixed precision training. It is compatible with visuomotor policies such as DP,DP3 and VO-DP, and also supports the RoboTwin simulator.</description>
      <author>example@mail.com (Zehao Ni, Yonghao He, Lingfeng Qian, Jilei Mao, Fa Fu, Wei Sui, Hu Su, Junran Peng, Zhipeng Wang, Bin He)</author>
      <guid isPermaLink="false">2510.15530v2</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning</title>
      <link>http://arxiv.org/abs/2510.17772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种基于图册的流形学习方法，克服了传统方法在低维嵌入时丢失关键特征的局限性，实现了在潜在流形上的直接机器学习。&lt;h4&gt;背景&lt;/h4&gt;尽管流形假设很流行，但当前流形学习方法主要将数据降维到R^D空间，当嵌入维度D接近流形真实维度d时会丢失关键特征。直接将潜在流形学习为可微图册的方法相对未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;旨在证明基于图册方法的有效性和潜力，为流形学习提供新思路。&lt;h4&gt;方法&lt;/h4&gt;实现了一个通用数据结构来维护可微图册，支持在流形上进行黎曼优化；补充了从点云数据学习可微图册的无监督启发式方法。&lt;h4&gt;主要发现&lt;/h4&gt;在选定的设置中，该方法在效率和准确性方面具有优势；在克莱因瓶上的监督分类任务和造血数据的RNA速度分析中，展示了改进的可解释性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;基于图册的流形学习方法为直接在潜在流形上进行机器学习提供了有效途径，提高了模型的可解释性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;尽管流形假设很受欢迎，但当前的流形学习方法不支持直接在潜在d维数据流形上进行机器学习，因为它们主要旨在将数据降维到R^D空间，当嵌入维度D接近d时会丢失关键流形特征。另一方面，直接将潜在流形学习为可微图册的方法相对未被充分探索。在本文中，我们旨在证明基于图册方法的有效性和潜力。为此，我们实现了一个通用的数据结构来维护可微图册，使得能够在流形上进行黎曼优化。我们补充了一种无监督启发式方法，从点云数据中学习可微图册。我们在选定的设置中实验证明了这种方法在效率和准确性方面的优势。此外，在克莱因瓶上的监督分类任务和造血数据的RNA速度分析中，我们展示了我们方法在可解释性和鲁棒性方面的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the popularity of the manifold hypothesis, current manifold-learningmethods do not support machine learning directly on the latent $d$-dimensionaldata manifold, as they primarily aim to perform dimensionality reduction into$\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$approaches $d$.  On the other hand, methods that directly learn the latent manifold as adifferentiable atlas have been relatively underexplored.  In this paper, we aim to give a proof of concept of the effectiveness andpotential of atlas-based methods. To this end, we implement a generic datastructure to maintain a differentiable atlas that enables Riemannianoptimization over the manifold. We complement this with an unsupervisedheuristic that learns a differentiable atlas from point cloud data. Weexperimentally demonstrate that this approach has advantages in terms ofefficiency and accuracy in selected settings. Moreover, in a supervisedclassification task over the Klein bottle and in RNA velocity analysis ofhematopoietic data, we showcase the improved interpretability and robustness ofour approach.</description>
      <author>example@mail.com (Ryan A. Robinett, Sophia A. Madejski, Kyle Ruark, Samantha J. Riesenfeld, Lorenzo Orecchia)</author>
      <guid isPermaLink="false">2510.17772v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
  <item>
      <title>Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance</title>
      <link>http://arxiv.org/abs/2510.17237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'Pole-Image'的新型规范表示方法，利用电线杆作为高精度锚点来生成周围环境的签名，通过对比学习实现稳健的自定位和可靠的地图维护。&lt;h4&gt;背景&lt;/h4&gt;移动机器人的长期自主性需要稳健的自定位和可靠的地图维护，而传统的基于地标的方法面临高可检测性但低独特性与高独特性但难以稳定检测之间的权衡。&lt;h4&gt;目的&lt;/h4&gt;解决如何描述性地识别独特的'签名'（局部点云）的挑战，通过利用可检测的高精度'锚点'（如电线杆）。&lt;h4&gt;方法&lt;/h4&gt;提出'Pole-Image'作为混合方法，将检测到的类似电线杆的地标及其周围环境表示为以电线杆本身为原点的2D极坐标图像，利用电线杆易于检测的特点实现多样观测数据的自动大规模收集，并应用对比学习学习视角不变且高度判别性的描述符。&lt;h4&gt;主要发现&lt;/h4&gt;电线杆地标'检测'极其容易，使得机器人可以轻松跟踪同一电线杆，实现多样观测数据的自动大规模收集；通过对比学习获得的描述符克服了感知别名问题，实现了稳健的自定位；高精度编码实现了高灵敏度变化检测，有助于地图维护。&lt;h4&gt;结论&lt;/h4&gt;所提出的'Pole-Image'方法和对比学习应用能够有效解决传统地标方法面临的权衡问题，为移动机器人的长期自主性提供了稳健的自定位和可靠的地图维护方案。&lt;h4&gt;翻译&lt;/h4&gt;移动机器人的长期自主性需要稳健的自定位和可靠的地图维护。传统的基于地标的方法面临一个基本权衡：高可检测性但低独特性的地标（如电线杆）与高独特性但难以稳定检测的地标（如局部点云结构）之间的权衡。本文通过利用可检测的高精度'锚点'（如电线杆）来解决描述性识别独特'签名'（局部点云）的挑战。为此，我们提出了一种名为'Pole-Image'的新型规范表示方法，作为混合方法，使用电线杆作为锚点来生成周围3D结构的签名。Pole-Image将检测到的类似电线杆的地标及其周围环境表示为以电线杆本身为原点的2D极坐标图像。这种表示利用了电线杆作为高精度参考点的特性，明确编码了稳定电线杆与可变周围点云之间的'相对几何关系'。电线杆地标的主要优势是'检测'极其容易。这种检测的简便性使得机器人可以轻松跟踪同一电线杆，从而实现多样观测数据（正样本对）的自动大规模收集。这种数据获取可行性使得'对比学习（CL）'得以应用。通过应用CL，模型学会了视角不变且高度判别性的描述符。贡献有两方面：1）描述符克服了感知别名问题，实现了稳健的自定位。2）高精度编码实现了高灵敏度变化检测，有助于地图维护。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决移动机器人在长期自主运行中的鲁棒自定位和可靠地图维护问题。传统地标方法面临一个根本权衡：电线杆等地标容易检测但缺乏独特性，而高区分度的局部点云结构又难以稳定检测。这个问题很重要，因为它关系到机器人在动态环境中能否长期稳定工作，特别是在城市网格状环境中，地标排列高度重复会导致定位模糊，环境变化也会影响系统稳定性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统地标方法的局限性，特别是感知别名问题。他们注意到电线杆等垂直结构有独特性质：检测容易精确但个体识别困难。作者思考如何利用电线杆作为'锚点'来描述周围环境的独特'签名'。他们发现电线杆允许检测和识别任务解耦，使跟踪同一电线杆变得容易，从而能自动收集多样化观测数据，使对比学习可行。作者借鉴了LiDAR-Iris的2D表示方法、对比学习与监督学习范式、点云描述符从手工设计到深度学习的演进思路，以及视觉位置识别网络中的特征聚合技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用电线杆作为高精度锚点，将电线杆及其周围环境表示为以电线杆为中心的2D极坐标图像（Pole-Image），然后训练编码器生成具有视角不变性和高区分度的描述符，实现鲁棒自定位和高灵敏度变化检测。整体流程：1）检测系统从LiDAR点云提取电线杆；2）将电线杆及其周围环境转换为Pole-Image表示；3）使用轻量级编码器将2D图像转换为紧凑向量描述符；4）通过对比学习或监督学习训练编码器，区分同一地标的图像和不同地标的图像。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1）提出Pole-Image表示法，使用电线杆作为锚点生成签名；2）利用电线杆易检测性收集多样化数据，使对比学习可行；3）通过高精度编码实现高灵敏度变化检测；4）设计轻量级编码器生成紧凑描述符。不同之处：传统方法使用几何配置或手工设计描述符，本文学习数据驱动描述符；传统方法将地标视为'匿名点'，本文强调个体识别；传统方法难以处理感知别名，本文通过区分性描述符解决；传统方法只关注定位，本文同时解决定位和地图维护；对比学习方法比监督学习表现更好，泛化能力更强。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于电线杆锚点的自监督描述符Pole-Image，通过对比学习生成具有高区分度和视角不变性的特征，同时解决了移动机器人在长期自主运行中的鲁棒自定位和高灵敏度地图维护问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-term autonomy for mobile robots requires both robust self-localizationand reliable map maintenance. Conventional landmark-based methods face afundamental trade-off between landmarks with high detectability but lowdistinctiveness (e.g., poles) and those with high distinctiveness but difficultstable detection (e.g., local point cloud structures). This work addresses thechallenge of descriptively identifying a unique "signature" (local point cloud)by leveraging a detectable, high-precision "anchor" (like a pole). To solvethis, we propose a novel canonical representation, "Pole-Image," as a hybridmethod that uses poles as anchors to generate signatures from the surrounding3D structure. Pole-Image represents a pole-like landmark and its surroundingenvironment, detected from a LiDAR point cloud, as a 2D polar coordinate imagewith the pole itself as the origin. This representation leverages the pole'snature as a high-precision reference point, explicitly encoding the "relativegeometry" between the stable pole and the variable surrounding point cloud. Thekey advantage of pole landmarks is that "detection" is extremely easy. Thisease of detection allows the robot to easily track the same pole, enabling theautomatic and large-scale collection of diverse observational data (positivepairs). This data acquisition feasibility makes "Contrastive Learning (CL)"applicable. By applying CL, the model learns a viewpoint-invariant and highlydiscriminative descriptor. The contributions are twofold: 1) The descriptorovercomes perceptual aliasing, enabling robust self-localization. 2) Thehigh-precision encoding enables high-sensitivity change detection, contributingto map maintenance.</description>
      <author>example@mail.com (Wuhao Xie, Kanji Tanaka)</author>
      <guid isPermaLink="false">2510.17237v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression</title>
      <link>http://arxiv.org/abs/2510.15337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 4 figures, 2 tables, 1 algorithm; camera-ready version  accepted at NeurIPS 2025 (Spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了迁移学习与最小二范数插值器(MNI)的结合，提出了新颖的两步式Transfer MNI方法，分析了其性能优势和适用条件，并通过实验验证了其鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;迁移学习是现代机器学习的关键组成部分，通过利用多样化数据源增强目标任务性能。同时，过参数化模型如高维线性回归中的MNI因其卓越的泛化能力（良性过拟合）而受到关注。然而，迁移学习与MNI的交集尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;填补迁移学习与MNI研究空白，提出Transfer MNI方法，分析其性能特征，开发数据驱动程序检测信息源，并通过实验验证方法的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出新颖的两步式Transfer MNI方法，表征其非渐近超额风险，开发数据驱动程序检测信息源，引入整合多个信息性Transfer MNIs的集成方法。&lt;h4&gt;主要发现&lt;/h4&gt;确定了Transfer MNI优于仅目标MNI的条件；揭示了自由协变量转移机制，利用异构数据可在有限成本下获得知识转移收益；有限样本实验证明了方法对模型和数据异质性的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;Transfer MNI方法在适当条件下能有效结合迁移学习和MNI的优势，为利用异构数据源提供了一种有效途径，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习是现代机器学习的关键组成部分，通过利用多样化数据源增强目标任务性能。同时，高维线性回归中的最小二范数插值器等过参数化模型因其卓越的泛化能力而受到关注，这种现象被称为良性过拟合。尽管迁移学习和MNI各自都很重要，但它们的交集尚未被充分探索。我们的研究通过提出新颖的两步式Transfer MNI方法并分析其权衡关系来填补这一空白。我们表征了其非渐近超额风险，并确定了它优于仅目标MNI的条件。我们的分析揭示了自由协变量转移机制，在这种机制下，利用异构数据可以在有限成本下获得知识转移的收益。为了实现研究发现，我们开发了数据驱动程序来检测信息源，并引入了整合多个信息性Transfer MNIs的集成方法。有限样本实验证明了我们的方法对模型和数据异质性的鲁棒性，确认了它们的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning is a key component of modern machine learning, enhancingthe performance of target tasks by leveraging diverse data sources.Simultaneously, overparameterized models such as the minimum-$\ell_2$-norminterpolator (MNI) in high-dimensional linear regression have garneredsignificant attention for their remarkable generalization capabilities, aproperty known as benign overfitting. Despite their individual importance, theintersection of transfer learning and MNI remains largely unexplored. Ourresearch bridges this gap by proposing a novel two-step Transfer MNI approachand analyzing its trade-offs. We characterize its non-asymptotic excess riskand identify conditions under which it outperforms the target-only MNI. Ouranalysis reveals free-lunch covariate shift regimes, where leveragingheterogeneous data yields the benefit of knowledge transfer at limited cost. Tooperationalize our findings, we develop a data-driven procedure to detectinformative sources and introduce an ensemble method incorporating multipleinformative Transfer MNIs. Finite-sample experiments demonstrate the robustnessof our methods to model and data heterogeneity, confirming their advantage.</description>
      <author>example@mail.com (Yeichan Kim, Ilmun Kim, Seyoung Park)</author>
      <guid isPermaLink="false">2510.15337v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients</title>
      <link>http://arxiv.org/abs/2510.17172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种混合预测框架，整合心电图基础模型与XGBoost分类器，提高了急性心肌梗死后恶性心律失常的预测准确性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;急性心肌梗死后发生的恶性室性心律失常是住院死亡的主要原因，但早期识别仍是临床挑战。传统风险评分表现有限，而端到端深度学习模型缺乏临床所需的可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合预测框架，整合大规模心电图基础模型（ECGFounder）和可解释的XGBoost分类器，以提高预测准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;分析6,634例AMI患者的心电图记录（其中175例发生住院VT/VF），使用ECGFounder模型提取150维诊断概率特征，通过特征选择优化后训练XGBoost分类器，使用AUC和F1-score评估性能，并应用SHAP方法进行可解释性分析。&lt;h4&gt;主要发现&lt;/h4&gt;ECGFounder+XGBoost混合模型AUC达到0.801，优于KNN（0.677）、RNN（0.676）和端到端1D-CNN（0.720）。SHAP分析显示模型识别的关键特征（如'室性早搏'作为风险预测因子，'正常窦性心律'作为保护因素）与临床知识高度一致。&lt;h4&gt;结论&lt;/h4&gt;该混合框架为VT/VF风险预测提供了新范式，验证了基础模型输出作为有效自动化特征工程的用途，为构建可信、可解释的AI临床决策支持系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;急性心肌梗死后发生的恶性室性心律失常是住院死亡的主要原因，但早期识别仍是临床挑战。传统风险评分表现有限，而端到端深度学习模型缺乏临床所需的可解释性。本研究旨在开发一种混合预测框架，整合大规模心电图基础模型与可解释的XGBoost分类器，以提高预测准确性和可解释性。我们分析了6,634例AMI患者的心电图记录，其中175例发生住院VT/VF。使用ECGFounder模型提取150维诊断概率特征，通过特征选择优化后训练XGBoost分类器。模型性能通过AUC和F1-score评估，并使用SHAP方法进行可解释性分析。ECGFounder+XGBoost混合模型AUC达到0.801，优于其他模型。SHAP分析显示模型识别的关键特征与临床知识高度一致。我们得出结论，该混合框架为VT/VF风险预测提供了新范式，验证了基础模型输出作为有效自动化特征工程的用途，为构建可信、可解释的AI临床决策支持系统奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Malignant ventricular arrhythmias (VT/VF) following acute myocardialinfarction (AMI) are a major cause of in-hospital death, yet earlyidentification remains a clinical challenge. While traditional risk scores havelimited performance, end-to-end deep learning models often lack theinterpretability needed for clinical trust. This study aimed to develop ahybrid predictive framework that integrates a large-scale electrocardiogram(ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier toimprove both accuracy and interpretability. We analyzed 6,634 ECG recordingsfrom AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFoundermodel was used to extract 150-dimensional diagnostic probability features ,which were then refined through feature selection to train the XGBoostclassifier. Model performance was evaluated using AUC and F1-score , and theSHAP method was used for interpretability. The ECGFounder + XGBoost hybridmodel achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed thatmodel-identified key features, such as "premature ventricular complexes" (riskpredictor) and "normal sinus rhythm" (protective factor), were highlyconsistent with clinical knowledge. We conclude that this hybrid frameworkprovides a novel paradigm for VT/VF risk prediction by validating the use offoundation model outputs as effective, automated feature engineering forbuilding trustworthy, explainable AI-based clinical decision support systems.</description>
      <author>example@mail.com (Shun Huang, Wenlu Xing, Shijia Geng, Hailong Wang, Guangkun Nie, Gongzheng Tang, Chenyang He, Shenda Hong)</author>
      <guid isPermaLink="false">2510.17172v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning</title>
      <link>http://arxiv.org/abs/2510.17289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究社交媒体多方对话环境中的反社会行为(ASB)检测与分析，使用多模态融合方法提升检测效果&lt;h4&gt;背景&lt;/h4&gt;社交媒体上的反社会行为(包括仇恨言论、骚扰和网络欺凌)对平台安全和社会福祉构成风险，而多方对话环境因数据有限研究不足&lt;h4&gt;目的&lt;/h4&gt;填补多方对话环境中反社会行为研究的空白，使用专门数据集评估相关任务的性能&lt;h4&gt;方法&lt;/h4&gt;使用法语开放数据集CyberAgressionAdo-Large，评估虐待检测、欺凌行为分析和欺凌同伴群体识别三个任务，对比六种基于文本和八种基于图的表示学习方法，分析词汇线索、互动动态及其多模态融合&lt;h4&gt;主要发现&lt;/h4&gt;多模态模型优于单模态基线；晚期融合模型mBERT + WD-SGCN表现最佳，在虐待检测(0.718)、同伴群体识别(0.286)和欺凌分析(0.606)方面均有良好成绩；能有效处理隐含攻击、角色转换和上下文相关的敌意等细微反社会行为现象&lt;h4&gt;结论&lt;/h4&gt;多模态融合方法在检测和分析多方对话中的反社会行为方面表现优异，特别是在处理复杂和微妙的反社会行为现象时具有优势&lt;h4&gt;翻译&lt;/h4&gt;社交媒体上的反社会行为(ASB)包括仇恨言论、骚扰和网络欺凌，对平台安全和社会福祉构成日益增长的风险。先前研究主要集中在X和Reddit等网络平台，而多方对话环境由于数据有限而研究不足。为解决这一差距，我们使用CyberAgressionAdo-Large(法语开放访问数据集)模拟多方对话中的ASB，并评估三个任务：虐待检测、欺凌行为分析和欺凌同伴群体识别。我们对比了六种基于文本和八种基于图的表示学习方法，分析词汇线索、互动动态及其多模态融合。结果显示多模态模型优于单模态基线模型。晚期融合模型mBERT + WD-SGCN获得最佳整体结果，在虐待检测方面表现最佳(0.718)，在同伴群体识别(0.286)和欺凌分析(0.606)方面具有竞争力。错误分析显示其在处理细微ASB现象方面的有效性，如隐含攻击、角色转换和上下文相关的敌意。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Antisocial behavior (ASB) on social media -- including hate speech,harassment, and cyberbullying -- poses growing risks to platform safety andsocietal well-being. Prior research has focused largely on networks such as Xand Reddit, while \textit{multi-party conversational settings} remainunderexplored due to limited data. To address this gap, we use\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASBin multi-party conversations, and evaluate three tasks: \textit{abusedetection}, \textit{bullying behavior analysis}, and \textit{bullyingpeer-group identification}. We benchmark six text-based and eight graph-based\textit{representation-learning methods}, analyzing lexical cues, interactionaldynamics, and their multimodal fusion. Results show that multimodal modelsoutperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}achieves the best overall results, with top performance on abuse detection(0.718) and competitive scores on peer-group identification (0.286) andbullying analysis (0.606). Error analysis highlights its effectiveness inhandling nuanced ASB phenomena such as implicit aggression, role transitions,and context-dependent hostility.</description>
      <author>example@mail.com (Hajar Bakarou, Mohamed Sinane El Messoussi, Anaïs Ollagnier)</author>
      <guid isPermaLink="false">2510.17289v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding</title>
      <link>http://arxiv.org/abs/2510.17034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为W2R2的新型训练框架，用于解决多模态3D定位中的2D语义偏差问题，通过解耦表示学习和目标性快捷方式抑制，显著提升了定位准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;多模态3D定位在视觉语言模型(VLMs)中受到关注，用于推进复杂环境中的空间推理能力，但现有模型存在严重的'2D语义偏差'问题。&lt;h4&gt;目的&lt;/h4&gt;解决模型过度依赖2D图像特征进行粗略定位而忽略3D几何输入的问题，提高多模态融合性能。&lt;h4&gt;方法&lt;/h4&gt;提出What-Where Representation Re-Forming (W2R2)训练框架，将2D特征作为'What'识别的语义信标，3D特征作为'Where'定位的空间锚点，包含双目标损失函数：对齐损失监督融合预测，伪标签损失惩罚过度有效的2D主导伪输出。&lt;h4&gt;主要发现&lt;/h4&gt;W2R2方法在ScanRefer和ScanQA数据集上表现出色，在定位准确性和鲁棒性方面有显著提升，尤其在杂乱的室外场景中表现突出。&lt;h4&gt;结论&lt;/h4&gt;W2R2能够在不修改推理架构的情况下，通过重新塑造模型内部空间，实现精确的3D定位。&lt;h4&gt;翻译&lt;/h4&gt;多模态3D定位在视觉语言模型(VLMs)中引起了相当大的关注，用于推进复杂环境中的空间推理。然而，这些模型存在严重的'2D语义偏差'问题，这源于过度依赖2D图像特征进行粗略定位， largely忽视了3D几何输入，导致融合性能不佳。在本文中，我们提出了一种名为What-Where Representation Re-Forming (W2R2)的新型训练框架，通过解耦表示学习和目标性快捷方式抑制来解决此问题。我们的方法通过将2D特征指定为'What'识别的语义信标，将3D特征指定为'Where'定位的空间锚点，从根本上重塑了模型的内部空间，从而能够在不修改推理架构的情况下实现精确的3D定位。关键组件包括一个双目标损失函数，其中对齐损失使用自适应交叉熵监督融合预测以实现多模态协同，伪标签损失通过基于边界的机制惩罚过度有效的2D主导伪输出。在ScanRefer和ScanQA上进行的实验证明了W2R2的有效性，在定位准确性和鲁棒性方面有显著提升，尤其是在杂乱的室外场景中。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态3D定位任务中的'2D语义偏差'问题，即模型过度依赖2D图像特征进行粗略定位而忽略3D几何输入的问题。这个问题很重要，因为人类生活在3D世界中并使用自然语言与之交互，准确的3D定位对于空间推理在复杂环境(如自动驾驶、机器人导航、增强现实等)中至关重要，而2D偏差限制了模型在这些场景中的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过诊断实验验证了2D语义偏差的存在：行为诊断显示移除3D输入后模型仍能保持较高准确率；表示空间分析(t-SNE可视化)显示融合特征更接近2D特征而非3D特征。作者借鉴了现有工作如VG-LLM(将2D与3D编码器配对)和ULIP(通过共享嵌入空间对齐模态)等思想，但发现它们仍存在2D偏差。基于这些观察，作者设计了W2R2框架，通过'拉-推'训练策略重构表示空间，将2D特征用于语义识别('what')，3D特征用于空间定位('where')。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'What-Where表示重塑'(W2R2)训练框架解耦2D语义和3D几何的表示学习，不修改推理架构只改变训练方式。整体流程包括：1)基线设置，使用多视图图像生成2D语义特征和3D几何特征并融合；2)正式化2D捷径，定义仅使用2D特征的预测路径；3)实施'拉-推'训练，'拉'目标对齐融合输出与真实标签，'推'目标惩罚过好的2D仅解决方案；4)通过总损失函数Ltotal = Lalign + λ Ldeterrence优化模型，使2D特征负责语义识别，3D特征负责空间定位。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出W2R2训练框架，通过解耦表示学习解决2D语义偏差；2)设计双目标损失函数，包括对齐损失和伪标签损失；3)采用'拉-推'训练策略重构表示空间；4)保持2D语义能力的同时增强3D几何利用。相比之前工作，不同之处在于：不优化特征融合而是重构表示空间；不消除2D语义而是明确分离其职责；通过训练策略而非架构修改解决问题；将评估层面的2D/3D表示解耦延伸到训练过程中。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 论文提出了W2R2训练框架，通过解耦2D语义和3D几何的表示学习，有效解决了多模态3D定位中的2D语义偏差问题，显著提升了定位准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal 3D grounding has garnered considerable interest in Vision-LanguageModels (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complexenvironments. However, these models suffer from a severe "2D semantic bias"that arises from over-reliance on 2D image features for coarse localization,largely disregarding 3D geometric inputs and resulting in suboptimal fusionperformance. In this paper, we propose a novel training framework calledWhat-Where Representation Re-Forming (W2R2) to tackle this issue viadisentangled representation learning and targeted shortcut suppression. Ourapproach fundamentally reshapes the model's internal space by designating 2Dfeatures as semantic beacons for "What" identification and 3D features asspatial anchors for "Where" localization, enabling precise 3D grounding withoutmodifying inference architecture. Key components include a dual-objective lossfunction with an Alignment Loss that supervises fused predictions using adaptedcross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizesoverly effective 2D-dominant pseudo-outputs via a margin-based mechanism.Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness ofW2R2, with significant gains in localization accuracy and robustness,particularly in cluttered outdoor scenes.</description>
      <author>example@mail.com (Yutong Zhong)</author>
      <guid isPermaLink="false">2510.17034v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>GBlobs: Local LiDAR Geometry for Improved Sensor Placement Generalization</title>
      <link>http://arxiv.org/abs/2510.18539v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  1st place at the IROS'25 RoboSense Challenge, Track #3: Cross-Sensor  Placement 3D Object Detection&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇技术报告概述了RoboSense 2025:Track 3的顶级解决方案，在各种传感器配置下实现了3D目标检测的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;基于LiDAR的3D检测器在使用传统全局特征（即绝对笛卡尔坐标）进行训练时，常常受到'几何捷径'的影响，导致模型主要依赖物体绝对位置而非形状和外观特征，限制了在不同传感器配置下的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来克服3D目标检测中的几何捷径问题，提高模型在不同传感器配置下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用GBlobs（一种局部点云特征描述符）作为网络输入特征，有效绕过几何捷径，迫使网络学习强大的、以物体为中心的表示。&lt;h4&gt;主要发现&lt;/h4&gt;通过GBlobs方法，模型能够学习更加鲁棒的物体中心表示，显著提高了模型泛化能力，在本挑战中展示了卓越性能。&lt;h4&gt;结论&lt;/h4&gt;GBlobs方法成功解决了3D目标检测中的几何捷径问题，使模型能够更好地适应不同的传感器配置，在各种条件下实现优异的3D目标检测性能。&lt;h4&gt;翻译&lt;/h4&gt;这篇技术报告概述了RoboSense 2025:Track 3的顶级解决方案，在各种传感器配置下实现了3D目标检测的最先进性能。我们的提交使用了GBlobs，这是一种局部点云特征描述符，专门设计用于增强模型在不同LiDAR配置上的泛化能力。当前的基于LiDAR的3D检测器在使用传统全局特征（即绝对笛卡尔坐标）进行训练时，常常受到'几何捷径'的影响。这引入了位置偏差，导致模型主要依赖物体的绝对位置，而不是区分形状和外观特征。虽然对领域内数据有效，但这种捷径在遇到不同点分布时（例如由不同传感器放置引起的）会严重限制泛化能力。通过使用GBlobs作为网络输入特征，我们有效地绕过了这种几何捷径，迫使网络学习强大的、以物体为中心的表示。这种方法显著提高了模型的泛化能力，从而在本挑战中展示了卓越的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决激光雷达3D物体检测模型在不同传感器位置下泛化能力不足的问题。现有模型过度依赖物体的绝对位置信息，当传感器位置改变时，模型性能会大幅下降。这个问题在现实中很重要，因为自动驾驶车辆可能需要使用不同位置配置的激光雷达传感器，如果模型无法适应这种变化，每次更换传感器位置都需要重新训练，限制了系统的灵活性和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有检测器的局限性（即'几何捷径'问题）来设计方法。他们发现使用全局坐标导致模型只关注物体位置而非形状特征，因此决定采用局部几何信息。作者借鉴了已有的GBlobs表示方法，将点云局部区域表示为高斯斑点，通过均值和协方差描述局部结构。同时，考虑到激光雷达数据在远距离的稀疏性，设计了双模型方法：一个基于GBlobs处理近距离，一个基于全局坐标处理远距离。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用局部几何特征替代全局坐标，迫使模型学习物体的形状和外观特征而非绝对位置，从而提高泛化能力。实现流程：1）将点云局部区域表示为GBlobs；2）分别训练基于GBlobs的主模型和基于全局坐标的辅助模型；3）推理时对输入应用测试时增强（随机旋转、翻转和缩放）；4）两个模型分别进行推理，然后反转增强并应用非极大值抑制；5）基于30米距离阈值融合结果：近距离用GBlobs模型，远距离用全局坐标模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1）将GBlobs作为输入特征解决几何捷径问题；2）设计双模型融合策略处理数据稀疏性；3）应用测试时增强技术提高鲁棒性；4）基于距离的预测融合方法。相比之前工作：传统检测器依赖全局坐标，而本文使用局部几何特征减轻位置偏见；之前工作未专门针对传感器位置变化优化；本文采用双模型而非单一模型适应不同距离的检测需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于局部几何特征GBlobs的3D物体检测方法，通过减轻几何捷径问题和采用双模型融合策略，显著提高了模型在不同传感器位置下的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This technical report outlines the top-ranking solution for RoboSense 2025:Track 3, achieving state-of-the-art performance on 3D object detection undervarious sensor placements. Our submission utilizes GBlobs, a local point cloudfeature descriptor specifically designed to enhance model generalization acrossdiverse LiDAR configurations. Current LiDAR-based 3D detectors often sufferfrom a \enquote{geometric shortcut} when trained on conventional globalfeatures (\ie, absolute Cartesian coordinates). This introduces a position biasthat causes models to primarily rely on absolute object position rather thandistinguishing shape and appearance characteristics. Although effective forin-domain data, this shortcut severely limits generalization when encounteringdifferent point distributions, such as those resulting from varying sensorplacements. By using GBlobs as network input features, we effectivelycircumvent this geometric shortcut, compelling the network to learn robust,object-centric representations. This approach significantly enhances themodel's ability to generalize, resulting in the exceptional performancedemonstrated in this challenge.</description>
      <author>example@mail.com (Dušan Malić, Christian Fruhwirth-Reisinger, Alexander Prutsch, Wei Lin, Samuel Schulter, Horst Possegger)</author>
      <guid isPermaLink="false">2510.18539v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation</title>
      <link>http://arxiv.org/abs/2510.18341v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种在自动驾驶闭环仿真中进行真实视角外推的方法，在 ICCV 2025 RealADSim Workshop NVS 赛道中获得第一名。方法采用四阶段流水线，包括数据驱动初始化、几何先验注入、生成先验利用和数据驱动自适应，解决了街道视角外推的核心挑战，在 RealADSim-NVS 基准测试上获得 0.441 的最高分。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶闭环仿真中的真实视角外推是一个重要挑战，当前的 NovelView Synthesis (NVS) 方法在原始轨迹之外常常产生扭曲和不一致的图像。&lt;h4&gt;目的&lt;/h4&gt;提出一个解决方案，解决街道视角外推的核心挑战，并在 RealADSim Workshop NVS 赛道中取得领先成绩。&lt;h4&gt;方法&lt;/h4&gt;引入了一个全面的四阶段流水线：1) 使用数据驱动初始化策略生成稳健的伪激光雷达点云，避免局部最小值；2) 通过建模道路表面引入强几何先验，使用称为 2D-SDF 的新型降维 SDF；3) 利用生成先验为外推视点创建伪真实值，提供辅助监督；4) 使用数据驱动自适应网络移除时间特定伪影。&lt;h4&gt;主要发现&lt;/h4&gt;在 RealADSim-NVS 基准测试上，该方法获得了 0.441 的最终得分，在所有参与者中排名第一。&lt;h4&gt;结论&lt;/h4&gt;该方法成功解决了自动驾驶中视角外推的挑战，通过综合的四阶段流水线实现了高质量的视角外推，在基准测试中取得了最佳成绩。&lt;h4&gt;翻译&lt;/h4&gt;真实的视角外推对自动驾驶的闭环仿真至关重要，然而对于当前的 NovelView Synthesis (NVS) 方法来说，这仍然是一个重大挑战，这些方法通常在原始轨迹之外产生扭曲和不一致的图像。本报告提出了我们的获奖解决方案，在 ICCV 2025 RealADSim Workshop NVS 赛道中荣获第一名。为解决街道视角外推的核心挑战，我们引入了一个全面的四阶段流水线。首先，我们采用数据驱动初始化策略生成稳健的伪激光雷达点云，避免局部最小值。其次，我们通过使用称为 2D-SDF 的新型降维 SDF 对道路表面建模，注入强几何先验。第三，我们利用生成先验为外推视点创建伪真实值，提供辅助监督。最后，数据驱动自适应网络移除时间特定伪影。在 RealADSim-NVS 基准测试上，我们的方法获得了 0.441 的最终得分，在所有参与者中排名第一。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶场景中的街景视角外推问题，即从原始轨迹之外生成新视角的图像。这个问题在现实中很重要，因为高保真的模拟是验证自动驾驶算法的关键技术，而传统模拟器存在领域差距，真实日志回放又无法支持交互式闭环评估。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性分析街景外推的核心挑战，设计了一个四阶段流程。他们借鉴了3D高斯泼溅(3DGS)和神经辐射场(NeRF)等体积基元方法，但发现这些方法在外推视角下存在几何扭曲问题。同时参考了结构化运动(SfM)、视觉几何变换(VGGT)技术，并引入了生成模型如Difix3D+作为伪真实数据生成器，创新性地提出了2D-SDF来专门处理道路表面。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过系统化的四阶段流程实现鲁棒且几何一致的街景外推。整体流程包括：(1)使用VGGT生成视觉伪激光雷达点云进行3D场景初始化；(2)使用2D-SDF表示道路表面，结合3D高斯泼溅表示地面以上物体；(3)利用扩散模型为外推视角生成伪真实数据，提供对未观察区域的监督；(4)训练时不变自适应网络去除时间特定特征，确保在不同条件下保持一致的渲染结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：(1)鲁棒的无需激光雷达的初始化策略，避免局部最小值问题；(2)创新的2D-SDF表示，强制道路表面为局部平面先验；(3)迭代式伪真实数据框架，利用生成先验为未观察区域提供监督；(4)时不变自适应网络，去除时间特定特征。相比之前的工作，这个方法特别关注外推而非插值，同时结合了几何约束和生成模型的优势，既保证了几何一致性，又提供了视觉真实感。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种系统化的四阶段视觉街景外推方法，通过结合几何先验和生成模型，实现了在自动驾驶场景中从原始轨迹外推到新视角的鲁棒且几何一致的图像合成。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Realistic view extrapolation is critical for closed-loop simulation inautonomous driving, yet it remains a significant challenge for current NovelView Synthesis (NVS) methods, which often produce distorted and inconsistentimages beyond the original trajectory. This report presents our winningsolution which ctook first place in the RealADSim Workshop NVS track at ICCV2025. To address the core challenges of street view extrapolation, we introducea comprehensive four-stage pipeline. First, we employ a data-driveninitialization strategy to generate a robust pseudo-LiDAR point cloud, avoidinglocal minima. Second, we inject strong geometric priors by modeling the roadsurface with a novel dimension-reduced SDF termed 2D-SDF. Third, we leverage agenerative prior to create pseudo ground truth for extrapolated viewpoints,providing auxilary supervision. Finally, a data-driven adaptation networkremoves time-specific artifacts. On the RealADSim-NVS benchmark, our methodachieves a final score of 0.441, ranking first among all participants.</description>
      <author>example@mail.com (Kaiyuan Tan, Yingying Shen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye)</author>
      <guid isPermaLink="false">2510.18341v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>BlendCLIP: Bridging Synthetic and Real Domains for Zero-Shot 3D Object Classification with Multimodal Pretraining</title>
      <link>http://arxiv.org/abs/2510.18244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BlendCLIP的多模态预训练框架，用于解决零样本3D物体分类中合成数据与真实数据之间的领域差距问题，实现了在自动驾驶等实际应用中的高效3D物体识别。&lt;h4&gt;背景&lt;/h4&gt;零样本3D物体分类对自动驾驶等实际应用至关重要，但面临合成训练数据与真实世界稀疏、嘈杂的LiDAR扫描数据之间的显著领域差距。仅使用合成数据训练的方法无法泛化到户外场景，而仅使用真实数据训练的方法缺乏语义多样性，难以识别罕见或未见过的物体。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够弥合合成数据与真实数据之间领域差距的方法，实现高效准确的零样本3D物体分类，特别是在自动驾驶等实际应用场景中。&lt;h4&gt;方法&lt;/h4&gt;提出BlendCLIP多模态预训练框架，通过以下步骤实现：1) 从真实世界驾驶数据和人工标注的3D框中生成大规模物体级三元组数据集（点云、图像和文本描述）；2) 采用基于课程的数据混合策略，首先将模型建立在语义丰富的合成CAD数据基础上，然后逐步适应真实世界扫描的特性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该方法具有很高的标签效率：每批次仅引入1.5%的真实世界样本就能将nuScenes基准上的零样本准确率提高27%。最终模型在nuScenes和TruckScenes等户外数据集上实现了最先进的性能，比最佳先前方法提高了19.3%，同时在多样化合成基准上保持强大泛化能力。&lt;h4&gt;结论&lt;/h4&gt;有效的领域适应而非大规模真实世界标注是解锁强大开放词汇3D感知的关键。该研究为解决3D物体识别中的领域差距提供了新思路，代码和数据集将在论文被接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;零样本3D物体分类对自动驾驶等实际应用至关重要，但常常受到用于训练的合成数据与真实世界中遇到的稀疏、嘈杂的LiDAR扫描之间的显著领域差距的阻碍。仅使用合成数据训练的方法无法泛化到户外场景，而仅使用真实数据训练的方法缺乏语义多样性，无法识别罕见或未见过的物体。我们引入了BlendCLIP，一个多模态预训练框架，通过战略性地结合两个领域的优势来弥合这一合成到真实的差距。我们首先提出了一种管道，从真实世界驾驶数据和人工标注的3D框中直接生成大规模物体级三元组数据集——包括点云、图像和文本描述。我们的核心贡献是基于课程的数据混合策略，首先将模型建立在语义丰富的合成CAD数据基础上，然后逐步使其适应真实世界扫描的特定特性。实验表明我们的方法具有很高的标签效率：每批次引入仅1.5%的真实世界样本就能将nuScenes基准上的零样本准确率提高27%。因此，我们的最终模型在nuScenes和TruckScenes等具有挑战性的户外数据集上实现了最先进的性能，在nuScenes上比最佳先前方法提高了19.3%，同时在多样化的合成基准上保持强大的泛化能力。我们的研究结果表明，有效的领域适应而非大规模真实世界标注是解锁强大开放词汇3D感知的关键。我们的代码和数据集将在论文被接受后发布在https://github.com/kesu1/BlendCLIP。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决零样本3D物体分类中的合成数据（如CAD模型）和真实世界数据（如LiDAR扫描）之间的域差距问题。这个问题在自动驾驶等现实应用中非常重要，因为系统需要识别各种可能遇到的物体，包括训练中未见过的类别。现有方法要么完全依赖合成数据但无法很好地泛化到真实世界的稀疏、嘈杂环境，要么完全依赖真实数据但缺乏语义多样性来识别罕见物体。同时，真实世界数据标注成本高昂，大规模标注不现实。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法的两条路径各有局限：纯合成数据无法泛化到真实世界，而纯真实数据缺乏语义多样性。因此他们思考如何结合两种数据的优势。设计方法时借鉴了ULIP-2的预训练策略，学习3D编码器并与CLIP嵌入对齐；采用了课程学习思想，从简单到复杂逐步引入数据；参考了多模态表示学习，使用点云-图像-文本三元组进行训练。核心思路是先让模型在语义丰富的合成数据上建立基础，然后逐步适应真实世界数据特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用课程学习策略，先在语义丰富的合成CAD数据上建立基础，然后逐步适应真实世界LiDAR数据的特性，通过战略性地结合两个领域优势来弥合域差距。整体流程包括：1)数据准备，使用合成数据集和从真实数据集中构建的三元组；2)三元组生成，通过多扫描融合和运动补偿提取密集点云，投影边界框获取图像，用视觉-语言模型生成描述；3)课程训练，先仅用合成数据训练，再逐步引入真实世界数据；4)评估，在合成和真实数据集上评估零样本分类性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新的课程学习数据混合策略；2)构建大规模户外三元组数据集；3)展示只需少量真实世界样本就能显著提高性能；4)在多个数据集上实现最先进的零样本分类。相比之前工作的不同：与纯合成数据方法相比能更好泛化到真实世界；与纯真实数据方法相比保留更好的语义多样性；相比简单数据混合策略避免了模型过度拟合到域标识符；相比需要大量标注的方法标注效率高，只需少量真实样本。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BlendCLIP通过课程学习策略战略性地结合合成数据和真实世界数据，以最小的标注成本实现了强大的零样本3D物体分类，显著缩小了合成到真实世界的域差距。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-shot 3D object classification is crucial for real-world applicationslike autonomous driving, however it is often hindered by a significant domaingap between the synthetic data used for training and the sparse, noisy LiDARscans encountered in the real-world. Current methods trained solely onsynthetic data fail to generalize to outdoor scenes, while those trained onlyon real data lack the semantic diversity to recognize rare or unseen objects.  We introduce BlendCLIP, a multimodal pretraining framework that bridges thissynthetic-to-real gap by strategically combining the strengths of both domains.We first propose a pipeline to generate a large-scale dataset of object-leveltriplets -- consisting of a point cloud, image, and text description -- mineddirectly from real-world driving data and human annotated 3D boxes. Our corecontribution is a curriculum-based data mixing strategy that first grounds themodel in the semantically rich synthetic CAD data before progressively adaptingit to the specific characteristics of real-world scans.  Our experiments show that our approach is highly label-efficient: introducingas few as 1.5\% real-world samples per batch into training boosts zero-shotaccuracy on the nuScenes benchmark by 27\%. Consequently, our final modelachieves state-of-the-art performance on challenging outdoor datasets likenuScenes and TruckScenes, improving over the best prior method by 19.3\% onnuScenes, while maintaining strong generalization on diverse syntheticbenchmarks. Our findings demonstrate that effective domain adaptation, notfull-scale real-world annotation, is the key to unlocking robustopen-vocabulary 3D perception. Our code and dataset will be released uponacceptance on https://github.com/kesu1/BlendCLIP.</description>
      <author>example@mail.com (Ajinkya Khoche, Gergő László Nagy, Maciej Wozniak, Thomas Gustafsson, Patric Jensfelt)</author>
      <guid isPermaLink="false">2510.18244v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>A Generalizable Light Transport 3D Embedding for Global Illumination</title>
      <link>http://arxiv.org/abs/2510.18189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种可泛化的3D光传输嵌入方法，直接从3D场景配置近似全局光照，无需使用光栅化或路径追踪的线索。&lt;h4&gt;背景&lt;/h4&gt;全局光照对真实感渲染至关重要，但由于模拟间接光传输的复杂性，计算成本很高。现有神经方法主要依赖场景优化，跨场景泛化努力停留在2D屏幕空间，存在视图不一致性和有限空间理解问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个可泛化的3D光传输嵌入方法，直接从3D场景配置近似全局光照，不使用光栅化或路径追踪的线索。&lt;h4&gt;方法&lt;/h4&gt;将场景表示为具有几何和材质特征点云；使用可扩展transformer模型对全局点与点交互建模，编码为神经基元；渲染时通过最近邻搜索检索附近基元，通过交叉注意力聚合潜在特征预测渲染量。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化室内场景中实现了漫反射全局光照预测；训练的嵌入可通过有限微调快速适应新渲染任务；展示了有光泽材质的空间-方向辐射场估计初步结果；归一化场可加速无偏路径引导。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了将学习先验整合到渲染管道中的路径，无需显式光线追踪光照线索。&lt;h4&gt;翻译&lt;/h4&gt;全局光照(GI)对于真实感渲染至关重要，但由于模拟间接光传输的复杂性，计算成本仍然很高。最近的神经方法主要依赖于场景优化，有时扩展到处理相机或几何变化。跨场景泛化的努力主要停留在2D屏幕空间，如神经去噪或基于G缓冲区的GI预测，这些方法常常存在视图不一致性和有限的空间理解问题。我们提出了一种可泛化的3D光传输嵌入，直接从3D场景配置近似全局光照，不使用光栅化或路径追踪的线索。每个场景表示为具有几何和材质特征点云。可扩展的transformer模型对全局点与点之间的交互进行建模，将这些特征编码为神经基元。渲染时，每个查询点通过最近邻搜索检索附近基元，并通过交叉注意力聚合它们的潜在特征，以预测所需的渲染量。我们在不同布局、几何形状和材质的多样化室内场景中展示了漫反射全局光照预测的结果。为辐照度估计训练的嵌入可以通过有限的微调快速适应新的渲染任务。我们还展示了针对有光泽材质的空间-方向辐射场估计的初步结果，并展示了归一化场如何加速无偏路径引导。这种方法展示了将学习先验整合到渲染管道中的路径，而无需显式的光线追踪光照线索。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决全局照明的通用化问题。当前的全局照明方法计算成本高昂，而现有神经方法通常针对单个场景优化，缺乏跨场景泛化能力。这个问题在游戏开发、电影制作、虚拟现实和建筑设计等领域至关重要，因为这些领域需要高质量的全局照明效果，但又面临实时渲染和资源限制的挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到光传输算子和注意力机制之间的相似性，启发他们使用transformer架构来模拟光传输。他们意识到传统方法在处理复杂场景时的局限性，特别是视角一致性和泛化能力问题。他们借鉴了transformer架构在处理长距离依赖关系上的成功、PointTransformerV3作为点云编码的基础架构、传统的光传输方程和渲染方程，以及irradiance caching等传统技术的思想，但用神经网络方法进行了改进和创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用3D点云作为场景的中间表示，利用transformer架构编码场景点之间的长距离交互来模拟光传输，并设计可学习的局部查询解码器通过注意力机制聚合邻近点特征。整体流程包括：将3D场景转换为点云表示；使用transformer编码器处理场景点生成光传输嵌入；对于渲染时的查询点，通过k近邻搜索获取邻近场景点嵌入；使用基于交叉注意力的解码器聚合特征并预测渲染量；使用路径追踪的地面真实值进行端到端训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出通用的3D光传输嵌入方法直接从3D场景配置近似全局照明；使用点云作为中间表示避免屏幕空间限制；设计可扩展的transformer编码器捕获长距离交互；提出基于交叉注意力的局部查询解码器实现自适应特征聚合；展示如何重用预训练编码器适应不同渲染任务。与之前工作不同：相比单场景优化的神经方法，实现了跨场景泛化；相比2D屏幕空间方法，保持了多视角一致性；相比传统预计算方法，学习通用嵌入可跨场景重用；针对光传输模拟专门优化了点云处理方法；在3D空间直接预测入射辐射场，而非在2D渲染图像上训练。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于transformer的通用3D光传输嵌入方法，可以直接从场景几何、材质和光源配置中学习并泛化到复杂室内场景的全局照明效果，无需针对每个场景进行优化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Global illumination (GI) is essential for realistic rendering but remainscomputationally expensive due to the complexity of simulating indirect lighttransport. Recent neural methods have mainly relied on per-scene optimization,sometimes extended to handle changes in camera or geometry. Efforts towardcross-scene generalization have largely stayed in 2D screen space, such asneural denoising or G-buffer based GI prediction, which often suffer from viewinconsistency and limited spatial understanding. We propose a generalizable 3Dlight transport embedding that approximates global illumination directly from3D scene configurations, without using rasterized or path-traced cues. Eachscene is represented as a point cloud with geometric and material features. Ascalable transformer models global point-to-point interactions to encode thesefeatures into neural primitives. At render time, each query point retrievesnearby primitives via nearest-neighbor search and aggregates their latentfeatures through cross-attention to predict the desired rendering quantity. Wedemonstrate results on diffuse global illumination prediction across diverseindoor scenes with varying layouts, geometry, and materials. The embeddingtrained for irradiance estimation can be quickly adapted to new rendering taskswith limited fine-tuning. We also present preliminary results forspatial-directional radiance field estimation for glossy materials and show howthe normalized field can accelerate unbiased path guiding. This approachhighlights a path toward integrating learned priors into rendering pipelineswithout explicit ray-traced illumination cues.</description>
      <author>example@mail.com (Bing Xu, Mukund Varma T, Cheng Wang, Tzumao Li, Lifan Wu, Bartlomiej Wronski, Ravi Ramamoorthi, Marco Salvi)</author>
      <guid isPermaLink="false">2510.18189v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>HyperDiffusionFields (HyDiF): Diffusion-Guided Hypernetworks for Learning Implicit Molecular Neural Fields</title>
      <link>http://arxiv.org/abs/2510.18122v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为HyperDiffusionFields (HyDiF)的框架，将3D分子构象建模为连续场而非离散原子坐标或图，使用分子方向场和分子神经场表示，并通过超网络和去噪扩散模型实现生成能力。&lt;h4&gt;背景&lt;/h4&gt;传统分子建模方法使用离散原子坐标或图表示，而本文提出了一种基于连续场的建模方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将3D分子构象表示为连续场的框架，支持分子生成和性质预测任务。&lt;h4&gt;方法&lt;/h4&gt;使用分子方向场(MDF)将空间中的任何点映射到特定类型最近原子的方向；通过分子特定的神经隐式场(MNF)表示MDF；采用超网络条件化为分子生成MNF权重；将超网络训练为去噪扩散模型以实现生成能力；扩展到掩码扩散机制支持结构条件生成任务。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够支持空间细粒度特征提取，这是基于图或点云的方法难以实现的；该方法可扩展到更大的生物分子。&lt;h4&gt;结论&lt;/h4&gt;基于场的分子建模是一个有前途的方向，能够有效处理分子生成和性质预测任务。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了HyperDiffusionFields (HyDiF)框架，将3D分子构象建模为连续场而非离散原子坐标或图。我们方法的核心是分子方向场(MDF)，它将空间中的任何点映射到特定类型最近原子的方向。我们使用分子特定的神经隐式场表示MDF，称为分子神经场(MNF)。为了实现跨分子学习和促进泛化，我们采用了一种方法，其中共享的超网络条件化为分子，生成给定分子MNF的权重。为了赋予模型生成能力，我们将超网络训练为去噪扩散模型，能够在分子场的函数空间中进行采样。我们的设计自然扩展到掩码扩散机制，通过选择性对场区域加噪来支持结构条件生成任务，如分子修复。除了生成任务外，MDF的局部和连续特性使得能够进行空间细粒度的特征提取用于分子性质预测，这是基于图或点云的方法难以实现的。此外，我们证明了我们的方法可以扩展到更大的生物分子，展示了基于场的分子建模的一个有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce HyperDiffusionFields (HyDiF), a framework that models 3Dmolecular conformers as continuous fields rather than discrete atomiccoordinates or graphs. At the core of our approach is the Molecular DirectionalField (MDF), a vector field that maps any point in space to the direction ofthe nearest atom of a particular type. We represent MDFs usingmolecule-specific neural implicit fields, which we call Molecular Neural Fields(MNFs). To enable learning across molecules and facilitate generalization, weadopt an approach where a shared hypernetwork, conditioned on a molecule,generates the weights of the given molecule's MNF. To endow the model withgenerative capabilities, we train the hypernetwork as a denoising diffusionmodel, enabling sampling in the function space of molecular fields. Our designnaturally extends to a masked diffusion mechanism to supportstructure-conditioned generation tasks, such as molecular inpainting, byselectively noising regions of the field. Beyond generation, the localized andcontinuous nature of MDFs enables spatially fine-grained feature extraction formolecular property prediction, something not easily achievable with graph orpoint cloud based methods. Furthermore, we demonstrate that our approach scalesto larger biomolecules, illustrating a promising direction for field-basedmolecular modeling.</description>
      <author>example@mail.com (Sudarshan Babu, Phillip Lo, Xiao Zhang, Aadi Srivastava, Ali Davariashtiyani, Jason Perera, Michael Maire, Aly A. Khan)</author>
      <guid isPermaLink="false">2510.18122v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Unifying and Enhancing Graph Transformers via a Hierarchical Mask Framework</title>
      <link>http://arxiv.org/abs/2510.18825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025 (Poster)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的分层掩码框架，揭示了模型架构和注意力掩码构建之间的等价性，并基于此设计了M3Dphormer模型，该模型结合了多级掩码和双注意力计算，在多个基准测试上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;图变换器(Graph Transformers)在图表示学习中表现出色，但现有方法往往依赖针对特定交互的复杂架构设计，缺乏灵活性。&lt;h4&gt;目的&lt;/h4&gt;解决现有图变换器架构灵活性不足的问题，提出一个统一的框架来建模多样化的节点交互。&lt;h4&gt;方法&lt;/h4&gt;提出统一的分层掩码框架，揭示模型架构与注意力掩码构建的等价性；设计M3Dphormer模型，包含三种理论支持的分层掩码和双层专家路由机制；引入双注意力计算方案，根据局部掩码稀疏性动态切换模式。&lt;h4&gt;主要发现&lt;/h4&gt;正确分类的概率与感受野大小和标签一致性呈正相关；有效的注意力掩码应确保足够大的感受野和高水平的标签一致性；分层掩码在不同场景下具有互补优势。&lt;h4&gt;结论&lt;/h4&gt;M3Dphormer通过统一框架和多级掩码设计有效解决了图变换器的灵活性限制，在多个基准测试上实现了最先进的性能，验证了所提方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;图变换器(Graph Transformers)因其能够建模多样化的节点交互，已成为图表示学习的强大范式。然而，现有的图变换器通常依赖于针对特定交互的复杂架构设计，限制了其灵活性。为此，我们提出了一个统一的分层掩码框架，揭示了模型架构与注意力掩码构建之间的基本等价性。该框架通过精心设计的注意力掩码捕获多样化交互，实现了一致性的建模范式。在该框架下的理论分析表明，正确分类的概率与感受野大小和标签正相关，导致了一个基本设计原则：有效的注意力掩码应确保足够大的感受野和高水平的标签一致性。虽然没有单一现有的掩码能在所有场景下满足这一原则，但我们的分析显示分层掩码提供了互补优势，促使它们的有效集成。随后，我们引入了M3Dphormer，这是一种基于专家混合的图变换器，具有多级掩码和双注意力计算。M3Dphormer包含三种理论支持的分层掩码，并采用双层专家路由机制来自适应地集成多级交互信息。为确保可扩展性，我们进一步引入了双注意力计算方案，根据局部掩码稀疏性在密集和稀疏模式之间动态切换。在多个基准测试上的广泛实验证明，M3Dphormer达到了最先进的性能，验证了我们的统一框架和模型设计的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) have emerged as a powerful paradigm for graphrepresentation learning due to their ability to model diverse nodeinteractions. However, existing GTs often rely on intricate architecturaldesigns tailored to specific interactions, limiting their flexibility. Toaddress this, we propose a unified hierarchical mask framework that reveals anunderlying equivalence between model architecture and attention maskconstruction. This framework enables a consistent modeling paradigm bycapturing diverse interactions through carefully designed attention masks.Theoretical analysis under this framework demonstrates that the probability ofcorrect classification positively correlates with the receptive field size andlabel consistency, leading to a fundamental design principle: an effectiveattention mask should ensure both a sufficiently large receptive field and ahigh level of label consistency. While no single existing mask satisfies thisprinciple across all scenarios, our analysis reveals that hierarchical masksoffer complementary strengths, motivating their effective integration. Then, weintroduce M3Dphormer, a Mixture-of-Experts-based Graph Transformer withMulti-Level Masking and Dual Attention Computation. M3Dphormer incorporatesthree theoretically grounded hierarchical masks and employs a bi-level expertrouting mechanism to adaptively integrate multi-level interaction information.To ensure scalability, we further introduce a dual attention computation schemethat dynamically switches between dense and sparse modes based on local masksparsity. Extensive experiments across multiple benchmarks demonstrate thatM3Dphormer achieves state-of-the-art performance, validating the effectivenessof our unified framework and model design.</description>
      <author>example@mail.com (Yujie Xing, Xiao Wang, Bin Wu, Hai Huang, Chuan Shi)</author>
      <guid isPermaLink="false">2510.18825v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents</title>
      <link>http://arxiv.org/abs/2510.18703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: this https://linyq17.github.io/VC2L/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VC2L是一种统一的以视觉为中心的对比学习框架，通过在像素空间操作处理文本、图像及其组合，解决了传统对比视觉语言模型处理复杂网页文档的局限性，并在多个基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;对比视觉语言模型如CLIP通过学习对齐的图像-文本对在各种多模态任务中表现出色，但它们处理复杂的真实世界网页文档的能力仍然有限，特别是在文本和图像交错、松散对齐或嵌入视觉形式的情况下。&lt;h4&gt;目的&lt;/h4&gt;解决对比视觉语言模型处理复杂网页文档的局限性，提出一种统一的框架来有效处理文本、图像及其组合。&lt;h4&gt;方法&lt;/h4&gt;提出以视觉为中心的对比学习（VC2L）框架，使用单一的视觉变换器建模文本、图像及其组合；完全在像素空间操作，将所有输入渲染为图像，消除OCR、文本分词或模态融合的需要；采用片段级对比学习目标，对齐连续的多模态片段，利用文档的固有连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;引入了三个检索基准：AnyCIR（跨模态检索）、SeqCIR（细粒度顺序理解）和CSR（泛化到未见数据）；实验结果表明，VC2L在提出的基准和已建立的datasets（如M-BEIR和MTEB）上与CLIP风格模型相比具有竞争性或更优的性能。&lt;h4&gt;结论&lt;/h4&gt;多模态网页数据作为对比学习宝贵训练资源具有潜力；统一的、以视觉为中心的方法在多模态表示学习中具有良好的可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;对比视觉语言模型（如CLIP）通过学习对齐的图像-文本对在各种多模态任务中表现出色。然而，它们处理复杂的真实世界网页文档的能力仍然有限，特别是在文本和图像交错、松散对齐或嵌入视觉形式的情况下。为解决这些挑战，我们提出了以视觉为中心的对比学习（VC2L），这是一个统一框架，使用单一的视觉变换器对文本、图像及其组合进行建模。VC2L完全在像素空间操作，通过将所有输入（文本、视觉或组合）渲染为图像，从而消除了OCR、文本分词或模态融合策略的需要。为捕捉多模态网页文档中的复杂跨模态关系，VC2L采用片段级对比学习目标，对齐连续的多模态片段，利用文档的固有连贯性，不需要明确配对的图像-文本数据。为评估这种方法的有效性，我们引入了三个检索基准：AnyCIR、SeqCIR和CSR，分别用于评估跨模态检索、细粒度顺序理解和泛化到未见数据的能力。实验结果表明，与CLIP风格模型相比，VC2L在提出的基准和已建立的datasets（如M-BEIR和MTEB）上取得了竞争性或更优的性能。这些发现强调了多模态网页数据作为对比学习宝贵训练资源的潜力，并说明了统一的、以视觉为中心的方法在多模态表示学习中的可扩展性。代码和模型可在https://github.com/showlab/VC2L获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive vision-language models such as CLIP have demonstrated strongperformance across a wide range of multimodal tasks by learning from alignedimage-text pairs. However, their ability to handle complex, real-world webdocuments remains limited, particularly in scenarios where text and images areinterleaved, loosely aligned, or embedded in visual form. To address thesechallenges, we propose Vision-Centric Contrastive Learning (VC2L), a unifiedframework that models text, images, and their combinations using a singlevision transformer. VC2L operates entirely in pixel space by rendering allinputs, whether textual, visual, or combined, as images, thus eliminating theneed for OCR, text tokenization, or modality fusion strategy. To capturecomplex cross-modal relationships in multimodal web documents, VC2L employs asnippet-level contrastive learning objective that aligns consecutive multimodalsegments, leveraging the inherent coherence of documents without requiringexplicitly paired image-text data. To assess the effectiveness of thisapproach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR,designed to evaluate cross-modal retrieval, fine-grained sequentialunderstanding, and generalization to unseen data, respectively. Empiricalresults show that VC2L achieves competitive or superior performance compared toCLIP-style models on both the proposed benchmarks and established datasets suchas M-BEIR and MTEB. These findings underscore the potential of multimodal webdata as a valuable training resource for contrastive learning and illustratethe scalability of a unified, vision-centric approach for multimodalrepresentation learning. Code and models are available at:https://github.com/showlab/VC2L.</description>
      <author>example@mail.com (Yiqi Lin, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Mike Zheng Shou)</author>
      <guid isPermaLink="false">2510.18703v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>A Stage-Wise Learning Strategy with Fixed Anchors for Robust Speaker Verification</title>
      <link>http://arxiv.org/abs/2510.18530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于锚点的分阶段学习策略，用于在嘈杂条件下学习稳健的说话人表示。&lt;h4&gt;背景&lt;/h4&gt;在嘈杂条件下学习稳健的说话人表示面临重大挑战，需要谨慎处理区分性和噪声不变性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在嘈杂环境中保持说话人识别准确性的方法。&lt;h4&gt;方法&lt;/h4&gt;采用基于锚点的分阶段学习策略，首先训练基础模型建立区分性的说话人边界，然后从模型中提取锚嵌入作为稳定参考，最后在嘈杂输入上对基础模型的副本进行微调，通过强制接近固定的锚嵌入来保持失真情况下的说话人身份。&lt;h4&gt;主要发现&lt;/h4&gt;这种策略相比传统的联合优化方法有优势，特别是在保持区分性的同时提高噪声鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;提出的方法在各种噪声条件下都表现出一致的改进，这可能是由于其能够分别处理边界稳定性和变化抑制。&lt;h4&gt;翻译&lt;/h4&gt;在嘈杂条件下学习稳健的说话人表示面临重大挑战，这需要谨慎处理区分性和噪声不变性。在这项工作中，我们提出了一种基于锚点的分阶段学习策略用于稳健的说话人表示学习。具体来说，我们的方法首先训练一个基础模型来建立区分性的说话人边界，然后从该模型中提取锚嵌入作为稳定参考。最后，在嘈杂输入上对基础模型的副本进行微调，通过强制接近其对应的固定锚嵌入来保持失真情况下的说话人身份。实验结果表明，这种策略相比传统的联合优化方法具有优势，特别是在保持区分性的同时提高噪声鲁棒性。提出的方法在各种噪声条件下都表现出一致的改进，这可能是由于它能够分别处理边界稳定性和变化抑制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning robust speaker representations under noisy conditions presentssignificant challenges, which requires careful handling of both discriminativeand noise-invariant properties. In this work, we proposed an anchor-basedstage-wise learning strategy for robust speaker representation learning.Specifically, our approach begins by training a base model to establishdiscriminative speaker boundaries, and then extract anchor embeddings from thismodel as stable references. Finally, a copy of the base model is fine-tuned onnoisy inputs, regularized by enforcing proximity to their corresponding fixedanchor embeddings to preserve speaker identity under distortion. Experimentalresults suggest that this strategy offers advantages over conventional jointoptimization, particularly in maintaining discrimination while improving noiserobustness. The proposed method demonstrates consistent improvements acrossvarious noise conditions, potentially due to its ability to handle boundarystabilization and variation suppression separately.</description>
      <author>example@mail.com (Bin Gu, Lipeng Dai, Huipeng Du, Haitao Zhao, Jibo Wei)</author>
      <guid isPermaLink="false">2510.18530v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Simple and Efficient Heterogeneous Temporal Graph Neural Network</title>
      <link>http://arxiv.org/abs/2510.18467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Neurips 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种简单高效的异质时序图神经网络(SE-HTGNN)，通过动态注意力机制将时间建模集成到空间学习中，并利用大型语言模型增强模型理解能力。&lt;h4&gt;背景&lt;/h4&gt;异质时序图(HTGs)是现实世界中普遍存在的数据结构，现有基于注意力机制的神经网络方法采用解耦的时空学习范式，削弱了时空信息交互并导致模型复杂度高。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的HTGs学习范式，解决现有方法的时空信息交互弱和模型复杂度高的问题。&lt;h4&gt;方法&lt;/h4&gt;通过创新的动态注意力机制将时间建模集成到空间学习中，保留历史快图的注意力信息指导后续计算；同时利用大型语言模型提示SE-HTGNN，捕获节点类型的隐含属性作为先验知识。&lt;h4&gt;主要发现&lt;/h4&gt;SE-HTGNN比最先进和最新的基线方法快达10倍，同时保持最佳的预测准确性。&lt;h4&gt;结论&lt;/h4&gt;SE-HTGNN在提高计算效率的同时保持了准确性，是一种有效的异质时序图表示学习方法。&lt;h4&gt;翻译&lt;/h4&gt;异质时序图(HTGs)是现实世界中普遍存在的数据结构。最近，为了增强HTGs的表示学习，已提出许多基于注意力机制的神经网络。尽管取得了这些成功，现有方法依赖于解耦的时空学习范式，这削弱了时空信息的交互作用并导致模型复杂度高。为了弥合这一差距，我们提出了一种名为简单高效的异质时序图神经网络(SE-HTGNN)的新型HTGs学习范式。具体来说，我们通过创新的动态注意力机制将时间建模集成到空间学习中，该机制保留来自历史图快图的注意力信息以指导后续注意力计算，从而提高HTGs的整体判别性表示学习能力。此外，为了全面且自适应地理解HTGs，我们利用大型语言模型提示SE-HTGNN，使模型能够捕获节点类型的隐含属性作为先验知识。大量实验证明，SE-HTGNN比最先进和最新的基线方法快达10倍，同时保持最佳的预测准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous temporal graphs (HTGs) are ubiquitous data structures in thereal world. Recently, to enhance representation learning on HTGs, numerousattention-based neural networks have been proposed. Despite these successes,existing methods rely on a decoupled temporal and spatial learning paradigm,which weakens interactions of spatio-temporal information and leads to a highmodel complexity. To bridge this gap, we propose a novel learning paradigm forHTGs called Simple and Efficient Heterogeneous Temporal Graph N}eural Network(SE-HTGNN). Specifically, we innovatively integrate temporal modeling intospatial learning via a novel dynamic attention mechanism, which retainsattention information from historical graph snapshots to guide subsequentattention computation, thereby improving the overall discriminativerepresentations learning of HTGs. Additionally, to comprehensively andadaptively understand HTGs, we leverage large language models to promptSE-HTGNN, enabling the model to capture the implicit properties of node typesas prior knowledge. Extensive experiments demonstrate that SE-HTGNN achieves upto 10x speed-up over the state-of-the-art and latest baseline while maintainingthe best forecasting accuracy.</description>
      <author>example@mail.com (Yili Wang, Tairan Huang, Changlong He, Qiutong Li, Jianliang Gao)</author>
      <guid isPermaLink="false">2510.18467v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>ProLAP: Probabilistic Language-Audio Pre-Training</title>
      <link>http://arxiv.org/abs/2510.18423v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了概率语言-音频预训练模型(ProLAP)，用于解决语言-音频关系中的多对多对应问题，通过概率分布扩散建模多样性，并引入层次包含损失和掩码排斥损失来提高学习效率。&lt;h4&gt;背景&lt;/h4&gt;现有的语言-音频联合表征学习框架通常依赖确定性嵌入，假设音频和文本之间存在一一对应关系。然而在现实世界中，语言-音频关系本质上是多对多的：一个音频片段可以用多个字幕描述，反之亦然。&lt;h4&gt;目的&lt;/h4&gt;解决语言-音频关系中的多对多对应问题，提出能够从小数据集中学习数据固有层次结构的模型。&lt;h4&gt;方法&lt;/h4&gt;提出概率语言-音频预训练模型(ProLAP)，将多样性建模为联合语言-音频嵌入空间中概率分布的扩散。同时引入两个训练目标：层次包含损失促进对输入的语义层次理解，掩码排斥损失提高学习效率。&lt;h4&gt;主要发现&lt;/h4&gt;ProLAP在音频-文本检索任务上优于现有的确定性方法。通过音频遍历任务实验，证明ProLAP能够捕捉合理的语义层次结构，即使从小数据集也能有效学习。&lt;h4&gt;结论&lt;/h4&gt;ProLAP成功解决了语言-音频关系中的多对多对应问题，能够从小数据集中学习数据固有的层次结构，在检索任务和语义层次捕捉方面表现优异。&lt;h4&gt;翻译&lt;/h4&gt;语言-音频联合表征学习框架通常依赖于确定性嵌入，假设音频和文本之间存在一一对应关系。然而在现实世界中，语言-音频关系本质上是多对多的：一个音频片段可以用多个字幕描述，反之亦然。为解决这一问题，我们提出了概率语言-音频预训练(ProLAP)，将多样性建模为联合语言-音频嵌入空间中概率分布的扩散。为有效训练模态内的层次关系，我们还引入了两个目标：(i)层次包含损失促进对输入的语义层次理解，(ii)掩码排斥损失优化层次包含损失时提高学习效率。通过这种训练策略，我们的模型能够从小数据集中学习数据固有的层次结构，这与依赖大规模数据集的先前概率方法形成对比。在我们的实验中，ProLAP在音频-文本检索任务上优于现有的确定性方法。此外，通过本文介绍的音频遍历任务实验，我们证明了ProLAP捕捉了合理的语义层次。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Language-audio joint representation learning frameworks typically depend ondeterministic embeddings, assuming a one-to-one correspondence between audioand text. In real-world settings, however, the language-audio relationship isinherently many-to-many: one audio segment can be described by multiplecaptions and vice versa. To address this, we propose ProbabilisticLanguage-Audio Pre-training (ProLAP), which models multiplicity as the spreadof probability distributions in a joint language-audio embedding space. Totrain the intra-modal hierarchical relationship effectively, we also introducetwo objectives: (i) hierarchical inclusion loss to promote semantichierarchical understanding of inputs and (ii) mask repulsive loss to improvethe efficiency of learning when optimizing the hierarchical inclusion loss.With this training strategy, our model can learn the hierarchical structureinherent in the data even from small datasets, in contrast to priorprobabilistic approaches that rely on large-scale datasets. In our experiments,ProLAP outperforms existing deterministic approaches on audio-text retrievaltasks. Moreover, through experiments on the audio traversal task introduced inthis paper, we demonstrate that ProLAP captures the plausible semantichierarchy.</description>
      <author>example@mail.com (Toranosuke Manabe, Yuchi Ishikawa, Hokuto Munakata, Tatsuya Komatsu)</author>
      <guid isPermaLink="false">2510.18423v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Towards Identifiability of Hierarchical Temporal Causal Representation Learning</title>
      <link>http://arxiv.org/abs/2510.18310v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CHiLD的因果层次化潜在动态识别框架，能够有效捕捉时间序列数据中的多层次时间依赖关系，解决了现有方法无法从单时间步观测变量中恢复层次化潜在变量联合分布的问题。&lt;h4&gt;背景&lt;/h4&gt;对时间序列数据中的层次化潜在动态进行建模对于捕捉现实世界任务中多层次的抽象时间依赖关系至关重要，但现有方法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉时间序列数据中层次化潜在动态的框架，解决现有时间序列因果表征学习方法的不足。&lt;h4&gt;方法&lt;/h4&gt;提出CHiLD识别框架，首先利用时间上下文观测变量识别多层潜在变量的联合分布，然后利用层次结构的自然稀疏性识别每层内的潜在变量，并基于变分推断开发时间序列生成模型，包含上下文编码器和归一化流层次化先验网络。&lt;h4&gt;主要发现&lt;/h4&gt;使用三个条件独立的观测可以唯一确定层次化潜在变量的联合分布，这一发现为构建新方法提供了理论基础。&lt;h4&gt;结论&lt;/h4&gt;在合成和真实世界数据集上的经验评估验证了理论主张，证明了CHiLD在建模层次化潜在动态方面的有效性。&lt;h4&gt;翻译&lt;/h4&gt;对时间序列数据背后的层次化潜在动态进行建模，对于捕捉现实世界任务中多层次抽象的时间依赖关系至关重要。然而，现有的时间序列因果表征学习方法无法捕捉此类动态，因为它们无法从单时间步观测变量中恢复层次化潜在变量的联合分布。有趣的是，我们发现使用三个条件独立的观测可以唯一确定层次化潜在变量的联合分布。基于这一见解，我们提出了一个因果层次化潜在动态（CHiLD）识别框架。我们的方法首先利用时间上下文观测变量来识别多层潜在变量的联合分布。随后，我们利用层次结构中潜在变量的自然稀疏性来识别每层内的潜在变量。在理论结果的指导下，我们开发了一个基于变分推断的时间序列生成模型。该模型包含一个上下文编码器来重建多层潜在变量，以及基于归一化流的层次化先验网络，以施加层次化潜在动态的独立噪声条件。在合成和真实世界数据集上的经验评估验证了我们的理论主张，并证明了CHiLD在建模层次化潜在动态方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling hierarchical latent dynamics behind time series data is critical forcapturing temporal dependencies across multiple levels of abstraction inreal-world tasks. However, existing temporal causal representation learningmethods fail to capture such dynamics, as they fail to recover the jointdistribution of hierarchical latent variables from \textit{single-timestepobserved variables}. Interestingly, we find that the joint distribution ofhierarchical latent variables can be uniquely determined using threeconditionally independent observations. Building on this insight, we propose aCausally Hierarchical Latent Dynamic (CHiLD) identification framework. Ourapproach first employs temporal contextual observed variables to identify thejoint distribution of multi-layer latent variables. Sequentially, we exploitthe natural sparsity of the hierarchical structure among latent variables toidentify latent variables within each layer. Guided by the theoretical results,we develop a time series generative model grounded in variational inference.This model incorporates a contextual encoder to reconstruct multi-layer latentvariables and normalize flow-based hierarchical prior networks to impose theindependent noise condition of hierarchical latent dynamics. Empiricalevaluations on both synthetic and real-world datasets validate our theoreticalclaims and demonstrate the effectiveness of CHiLD in modeling hierarchicallatent dynamics.</description>
      <author>example@mail.com (Zijian Li, Minghao Fu, Junxian Huang, Yifan Shen, Ruichu Cai, Yuewen Sun, Guangyi Chen, Kun Zhang)</author>
      <guid isPermaLink="false">2510.18310v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Universal Spectral Tokenization via Self-Supervised Panchromatic Representation Learning</title>
      <link>http://arxiv.org/abs/2510.17959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025 Machine Learning and the Physical Sciences  Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种深度学习模型，能够以自监督方式学习异构天文光谱数据，生成统一且物理上有意义的表示，可作为天文领域基础模型的强大构建块，并可能扩展到其他科学领域。&lt;h4&gt;背景&lt;/h4&gt;连续科学数据跨越多个分辨率和领域，将其统一为共同表示是开发科学基础模型的关键步骤。天文光谱体现了这一挑战：大规模调查已收集数百万个跨越广泛波长和分辨率的光谱，但分析仍分散在光谱域（如光学与红外）和天体类型（如恒星与星系）中，限制了跨数据集信息整合的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够统一不同分辨率和领域光谱数据的模型，创建物理上有意义的表示，并适应各种下游任务。&lt;h4&gt;方法&lt;/h4&gt;提出了一种深度学习模型，以自监督方式联合学习异构光谱数据。该通用光谱标记器直接在原生波长网格上处理多种天体类型和分辨率的光谱，产生内在对齐、同质且物理上有意义的表示。&lt;h4&gt;主要发现&lt;/h4&gt;首次证明单个模型可以统一不同分辨率和领域的光谱数据。该模型能够高效适应，在各种下游任务中实现具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;该模型可作为天文领域基础模型的强大构建块，并可能扩展到其他具有异构连续数据的科学领域，如气候和医疗保健。&lt;h4&gt;翻译&lt;/h4&gt;连续科学数据跨越多个分辨率和领域，将其统一为共同表示是开发科学基础模型的关键步骤。天文光谱体现了这一挑战：大规模调查已收集了数百万个跨越广泛波长和分辨率的光谱，但分析仍分散在光谱域（如光学与红外）和天体类型（如恒星与星系）中，限制了跨数据集信息整合的能力。我们提出了一种深度学习模型，能够以自监督方式联合学习异构光谱数据。我们的通用光谱标记器直接在原生波长网格上处理多种天体类型和分辨率的光谱，产生内在对齐、同质且物理上有意义的表示，这些表示可以高效适应，在各种下游任务中实现具有竞争力的性能。我们首次证明，单个模型可以统一不同分辨率和领域的光谱数据，这表明我们的模型可以作为天文领域基础模型的强大构建块——并可能扩展到其他具有异构连续数据的科学领域，如气候和医疗保健。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential scientific data span many resolutions and domains, and unifyingthem into a common representation is a key step toward developing foundationmodels for the sciences. Astronomical spectra exemplify this challenge: massivesurveys have collected millions of spectra across a wide range of wavelengthsand resolutions, yet analyses remain fragmented across spectral domains (e.g.,optical vs. infrared) and object types (e.g., stars vs. galaxies), limiting theability to pool information across datasets. We present a deep learning modelthat jointly learns from heterogeneous spectra in a self-supervised manner. Ouruniversal spectral tokenizer processes spectra from a variety of object typesand resolutions directly on their native wavelength grids, producingintrinsically aligned, homogeneous, and physically meaningful representationsthat can be efficiently adapted to achieve competitive performance across arange of downstream tasks. For the first time, we demonstrate that a singlemodel can unify spectral data across resolutions and domains, suggesting thatour model can serve as a powerful building block for foundation models inastronomy -- and potentially extend to other scientific domains withheterogeneous sequential data, such as climate and healthcare.</description>
      <author>example@mail.com (Jeff Shen, Francois Lanusse, Liam Holden Parker, Ollie Liu, Tom Hehir, Leopoldo Sarra, Lucas Meyer, Micah Bowles, Sebastian Wagner-Carena, Sebastian Wagner-Carena, Helen Qu, Siavash Golkar, Alberto Bietti, Hatim Bourfoune, Nathan Cassereau, Pierre Cornette, Keiya Hirashima, Geraud Krawezik, Ruben Ohana, Nicholas Lourie, Michael McCabe, Rudy Morel, Payel Mukhopadhyay, Mariel Pettee, Bruno Régaldo-Saint Blancard, Kyunghyun Cho, Miles Cranmer, Shirley Ho)</author>
      <guid isPermaLink="false">2510.17959v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation</title>
      <link>http://arxiv.org/abs/2510.17914v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了NeuCo-Bench，一个用于评估地球观测背景下神经压缩和表示学习的新型基准框架。&lt;h4&gt;背景&lt;/h4&gt;地球观测领域需要有效的神经压缩和表示学习方法，但目前缺乏标准化的评估框架。&lt;h4&gt;目的&lt;/h4&gt;开发一个社区驱动、标准化的评估框架，用于评估地球观测领域的神经嵌入，平衡准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;基于固定大小的嵌入作为紧凑、任务无关的表示，包含三个核心组件：可重用嵌入的评估管道、隐藏任务排行榜的挑战模式、平衡准确性和稳定性的评分系统，并提供多光谱多时相的地球观测数据集。&lt;h4&gt;主要发现&lt;/h4&gt;通过公开挑战和与最先进基础模型的消融研究，验证了NeuCo-Bench框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;NeuCo-Bench为地球观测及更广泛领域的神经嵌入的社区驱动、标准化评估提供了重要基础。&lt;h4&gt;翻译&lt;/h4&gt;我们引入NeuCo-Bench，一个用于评估地球观测背景下（有损）神经压缩和表示学习的新型基准框架。我们的方法基于固定大小的嵌入，这些嵌入作为紧凑的、与任务无关的表示，适用于广泛的下游任务。NeuCo-Bench包含三个核心组件：(i)围绕可重用嵌入构建的评估管道，(ii)具有隐藏任务排行榜的新挑战模式，旨在减轻预训练偏差，(iii)平衡准确性和稳定性的评分系统。为了支持可重现性，我们发布了SSL4EO-S12-downstream，这是一个精心策划的多光谱、多时相的地球观测数据集。我们展示了在2025年CVPR EARTHVISION研讨会上公开挑战的初步结果，并使用最先进的基础模型进行了消融研究。NeuCo-Bench为地球观测及更广泛领域的神经嵌入的社区驱动、标准化评估迈出了第一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce NeuCo-Bench, a novel benchmark framework for evaluating (lossy)neural compression and representation learning in the context of EarthObservation (EO). Our approach builds on fixed-size embeddings that act ascompact, task-agnostic representations applicable to a broad range ofdownstream tasks. NeuCo-Bench comprises three core components: (i) anevaluation pipeline built around reusable embeddings, (ii) a new challenge modewith a hidden-task leaderboard designed to mitigate pretraining bias, and (iii)a scoring system that balances accuracy and stability. To supportreproducibility, we release SSL4EO-S12-downstream, a curated multispectral,multitemporal EO dataset. We present initial results from a public challenge atthe 2025 CVPR EARTHVISION workshop and conduct ablations with state-of-the-artfoundation models. NeuCo-Bench provides a first step towards community-driven,standardized evaluation of neural embeddings for EO and beyond.</description>
      <author>example@mail.com (Rikard Vinge, Isabelle Wittmann, Jannik Schneider, Michael Marszalek, Luis Gilch, Thomas Brunschwiler, Conrad M Albrecht)</author>
      <guid isPermaLink="false">2510.17914v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability</title>
      <link>http://arxiv.org/abs/2510.17040v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为多样化影响成分分析(DICA)的新框架，用于从未知非线性混合中识别潜在成分，无需依赖辅助信息、潜在成分独立性或雅可比矩阵稀疏性假设。&lt;h4&gt;背景&lt;/h4&gt;从未知非线性混合中识别潜在成分是机器学习中的一个基础性挑战，应用于解缠表示学习和因果推断等任务。先前在线性独立成分分析方面的工作表明辅助信号可以支持潜在成分的可识别性，而较新方法则通过结构假设（如混合函数雅可比矩阵稀疏性）来放宽要求。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从未知非线性混合中识别潜在成分的方法，同时减少对辅助信息、潜在成分独立性或雅可比矩阵稀疏性假设的依赖。&lt;h4&gt;方法&lt;/h4&gt;研究提出了多样化影响成分分析(DICA)框架，利用混合函数雅可比矩阵的凸几何特性，并引入雅可比体积最大化(J-VolMax)准则，通过鼓励潜在成分对观测变量的影响多样化来实现潜在成分的识别。&lt;h4&gt;主要发现&lt;/h4&gt;在合理条件下，所提出的方法无需依赖辅助信息、潜在成分独立性或雅可比矩阵稀疏性假设即可实现潜在成分的可识别性。&lt;h4&gt;结论&lt;/h4&gt;研究结果扩展了可识别性分析的范围，为现有方法提供了互补的视角，为从非线性混合中识别潜在成分提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;从未知非线性混合中识别潜在成分是机器学习中的一个基础性挑战，应用于解缠表示学习和因果推断等任务。先前在线性独立成分分析(nICA)方面的工作表明，辅助信号（如弱监督）可以支持条件独立潜在成分的可识别性。较新的方法则探索结构假设，例如混合函数的雅可比矩阵中的稀疏性，以放宽这些要求。在这项工作中，我们引入了多样化影响成分分析(DICA)框架，该框架利用混合函数雅可比矩阵的凸几何特性。我们提出了雅可比体积最大化(J-VolMax)准则，通过鼓励潜在成分对观测变量的影响多样化，实现潜在成分的识别。在合理条件下，这种方法无需依赖辅助信息、潜在成分独立性或雅可比矩阵稀疏性假设即可实现可识别性。这些结果扩展了可识别性分析的范围，并为现有方法提供了互补的视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Latent component identification from unknown nonlinear mixtures is afoundational challenge in machine learning, with applications in tasks such asdisentangled representation learning and causal inference. Prior work innonlinear independent component analysis (nICA) has shown that auxiliarysignals -- such as weak supervision -- can support identifiability ofconditionally independent latent components. More recent approaches explorestructural assumptions, e.g., sparsity in the Jacobian of the mixing function,to relax such requirements. In this work, we introduce Diverse InfluenceComponent Analysis (DICA), a framework that exploits the convex geometry of themixing function's Jacobian. We propose a Jacobian Volume Maximization(J-VolMax) criterion, which enables latent component identification byencouraging diversity in their influence on the observed variables. Underreasonable conditions, this approach achieves identifiability without relyingon auxiliary information, latent component independence, or Jacobian sparsityassumptions. These results extend the scope of identifiability analysis andoffer a complementary perspective to existing methods.</description>
      <author>example@mail.com (Hoang-Son Nguyen, Xiao Fu)</author>
      <guid isPermaLink="false">2510.17040v2</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Triangle Multiplication Is All You Need For Biomolecular Structure Representations</title>
      <link>http://arxiv.org/abs/2510.18870v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Pairmixer是一种替代AlphaFold3中Pairformer主干网络的高效解决方案，消除了计算密集型的三角形注意力机制，同时保留了高阶几何推理能力，显著提高了计算效率。&lt;h4&gt;背景&lt;/h4&gt;AlphaFold已变革蛋白质结构预测，但虚拟配体筛选、全蛋白质组折叠和从头结合剂设计等新兴应用需要大规模预测，面临运行时间和内存成本过高的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种替代方案，消除三角形注意力机制，同时保留对结构预测至关重要的高阶几何推理能力，提高计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出Pairmixer，一种简化的替代方案，消除三角形注意力机制，同时保留高阶几何推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;Pairmixer在折叠和对接基准测试中匹配最先进结构预测器性能，实现长序列上高达4倍的更快推理速度，训练成本降低34%；在BoltzDesign中提供超过2倍的更快采样速度，可扩展到比Pairformer内存限制长约30%的序列。&lt;h4&gt;结论&lt;/h4&gt;Pairmixer解决了大规模蛋白质结构预测的计算瓶颈问题，为建模大型蛋白质复合物、高通量配体和结合剂筛选等下游应用提供了高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;AlphaFold已经改变了蛋白质结构预测，但新兴应用如虚拟配体筛选、全蛋白质组折叠和从头结合剂设计需要大规模预测，此时运行时间和内存成本变得过高。主要瓶颈在于AlphaFold3类模型的Pairformer主干网络，它依赖于计算密集型的三角形原语——特别是三角形注意力机制——用于成对推理。我们引入Pairmixer，一种简化的替代方案，消除了三角形注意力同时保留了对结构预测至关重要的高阶几何推理能力。Pairmixer显著提高了计算效率，在折叠和对接基准测试中匹配最先进的结构预测器，在长序列上实现高达4倍的更快推理速度，同时将训练成本降低34%。其效率减轻了下游应用的计算负担，如建模大型蛋白质复合物、高通量配体和结合剂筛选以及基于幻觉的设计。例如，在BoltzDesign中，Pairmixer提供超过2倍的更快采样速度，并可扩展到比Pairformer内存限制长约30%的序列。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AlphaFold has transformed protein structure prediction, but emergingapplications such as virtual ligand screening, proteome-wide folding, and denovo binder design demand predictions at a massive scale, where runtime andmemory costs become prohibitive. A major bottleneck lies in the Pairformerbackbone of AlphaFold3-style models, which relies on computationally expensivetriangular primitives-especially triangle attention-for pairwise reasoning. Weintroduce Pairmixer, a streamlined alternative that eliminates triangleattention while preserving higher-order geometric reasoning capabilities thatare critical for structure prediction. Pairmixer substantially improvescomputational efficiency, matching state-of-the-art structure predictors acrossfolding and docking benchmarks, delivering up to 4x faster inference on longsequences while reducing training cost by 34%. Its efficiency alleviates thecomputational burden of downstream applications such as modeling large proteincomplexes, high-throughput ligand and binder screening, and hallucination-baseddesign. Within BoltzDesign, for example, Pairmixer delivers over 2x fastersampling and scales to sequences ~30% longer than the memory limits ofPairformer.</description>
      <author>example@mail.com (Jeffrey Ouyang-Zhang, Pranav Murugan, Daniel J. Diaz, Gianluca Scarpellini, Richard Strong Bowen, Nate Gruver, Adam Klivans, Philipp Krähenbühl, Aleksandra Faust, Maruan Al-Shedivat)</author>
      <guid isPermaLink="false">2510.18870v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Event-Grounding Graph: Unified Spatio-Temporal Scene Graph from Robotic Observations</title>
      <link>http://arxiv.org/abs/2510.18697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to RA-L&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了事件基础图（EGG）框架，通过将事件交互与场景空间特征关联，增强了机器人对环境的理解和交互能力，使机器人能够感知、推理和响应复杂的时空查询。&lt;h4&gt;背景&lt;/h4&gt;构建能够协助人类日常生活的智能自主机器人需要丰富的环境表示。虽然语义场景表示的进步已丰富了机器人的场景理解，但当前方法缺乏空间特征与动态事件之间的联系，例如无法将蓝色杯子与'洗杯子'事件联系起来。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将事件交互与场景空间特征联系起来的框架，使机器人能够更好地理解和响应环境中的动态事件。&lt;h4&gt;方法&lt;/h4&gt;引入事件基础图（EGG）框架，这是一种将事件交互与场景空间特征关联的环境表示方法，允许机器人感知、推理和响应复杂的时空查询。&lt;h4&gt;主要发现&lt;/h4&gt;使用真实机器人数据的实验证明EGG能够检索相关信息并准确响应对环境和事件的询问，展示了其在实际应用中的有效性。&lt;h4&gt;结论&lt;/h4&gt;EGG框架通过开源源代码和评估数据集（https://github.com/aalto-intelligent-robotics/EGG）为机器人环境理解和交互提供了新的解决方案，促进了该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;构建能够协助人类日常生活的智能自主机器人的一个基本方面是构建丰富的环境表示。尽管语义场景表示的进步已经丰富了机器人的场景理解，但当前方法缺乏空间特征与动态事件之间的联系；例如，无法将蓝色杯子与'洗杯子'事件联系起来。在这项工作中，我们引入了事件基础图（EGG），这是一个将事件交互与场景空间特征关联的框架。这种表示允许机器人感知、推理和响应复杂的时空查询。使用真实机器人数据的实验证明了EGG能够检索相关信息并准确响应对环境和事件的询问。此外，EGG框架的源代码和评估数据集已在以下地址开源：https://github.com/aalto-intelligent-robotics/EGG。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何构建一个能够将空间特征与动态事件连接起来的统一场景表示方法。这个问题很重要，因为当前的场景表示方法要么只关注静态空间元素(如3D场景图)，要么只记录事件描述(如视频字幕)，但无法将两者有效连接。这种连接缺失使得机器人无法理解空间变化与导致这些变化的交互之间的关系，限制了它们回答复杂时空查询的能力，如'你最后一次看到我的杯子是什么时候？'或'找到一台咖啡机'。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在设计方法时借鉴了现有工作的优点并试图解决其局限性。他们参考了3D场景图(3DSGs)的层次结构来表示空间元素，但增加了动态事件的表示；借鉴了记忆表示方法(如ReMEmbR和Embodied-RAG)记录事件交互的思想，但解决了它们缺乏空间特征连接的问题；还参考了利用大型语言模型进行信息检索的方法(如SayPlan和H-EMV)，但专注于改进基础表示。作者将这些方法的优势整合，创建了一个统一的图结构，通过边将事件与参与的空间元素连接起来。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过一个统一的图结构(EGG)同时表示场景中的空间元素和动态事件，并通过边将事件与参与事件的物体连接起来。整体流程包括：1) 构建EGG图，包含空间组件(类似3D场景图)和事件组件(描述观察到的活动)，以及连接事件与空间元素的事件边；2) 当接收到查询时，根据查询的四个维度(时间、位置、空间元素和事件)修剪图，提取相关子图；3) 将修剪后的子图序列化为JSON格式，输入到大型语言模型中生成回答。这种方法使机器人能够同时理解场景的空间布局和其中发生的事件。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 创建了统一的时空场景表示，同时包含空间组件和事件组件；2) 通过事件边将动态事件与参与的空间元素连接起来，解决了现有方法中空间特征与动态事件之间的连接缺失问题；3) 开发了基于查询的图修剪策略，减少计算负担并提高回答质量；4) 使用真实机器人数据进行了全面的实验验证。相比之前的工作，EGG的主要不同在于：与传统3D场景图相比，增加了动态事件的表示；与纯事件表示方法相比，保留了空间特征的精确表示；与现有记忆表示方法相比，通过事件边明确地连接事件与空间元素，解决了多视角一致性和冗余信息问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了EGG框架，通过将动态事件与空间特征连接起来，创建了一个统一的时空场景图，使机器人能够感知、推理和解释场景中发生的事件，并准确回答复杂的时空查询。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A fundamental aspect for building intelligent autonomous robots that canassist humans in their daily lives is the construction of rich environmentalrepresentations. While advances in semantic scene representations have enrichedrobotic scene understanding, current approaches lack a connection betweenspatial features and dynamic events; e.g., connecting the blue mug to the eventwashing a mug. In this work, we introduce the event-grounding graph (EGG), aframework grounding event interactions to spatial features of a scene. Thisrepresentation allows robots to perceive, reason, and respond to complexspatio-temporal queries. Experiments using real robotic data demonstrate EGG'scapability to retrieve relevant information and respond accurately to humaninquiries concerning the environment and events within. Furthermore, the EGGframework's source code and evaluation dataset are released as open-source at:https://github.com/aalto-intelligent-robotics/EGG.</description>
      <author>example@mail.com (Phuoc Nguyen, Francesco Verdoja, Ville Kyrki)</author>
      <guid isPermaLink="false">2510.18697v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning</title>
      <link>http://arxiv.org/abs/2510.18337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出MoTVLA模型，一种基于混合变换器的视觉语言动作模型，整合快速-慢速统一推理与行为策略学习，解决机器人学习中语言可控性和推理效率的平衡问题。&lt;h4&gt;背景&lt;/h4&gt;将视觉语言指令整合到视觉运动策略中是增强机器人学习在开放世界中泛化能力的重要方向，但现有方法面临语言可控性有限或推理延迟显著的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种既能保持预训练视觉语言模型的通用智能，又能提高策略执行效率和语言可控性的机器人学习模型。&lt;h4&gt;方法&lt;/h4&gt;提出MoTVLA模型，结合预训练视觉语言模型作为通用专家和领域特定专家，生成快速推理并基于分解的运动指令学习多样化行为。&lt;h4&gt;主要发现&lt;/h4&gt;MoTVLA在保持通用智能的同时，通过领域专家生成快速推理显著提高了策略执行效率，并通过条件化动作专家提升了语言可控性。&lt;h4&gt;结论&lt;/h4&gt;MoTVLA在自然语言处理基准、机器人仿真环境和真实世界实验中表现出优越的快速-慢速推理能力和操作任务性能。&lt;h4&gt;翻译&lt;/h4&gt;将视觉语言指令整合到视觉运动策略中正在增强机器人学习在开放世界中的泛化能力。尽管取得了 promising 的进展，但现有方法面临两个挑战：在不使用生成推理作为条件时，语言可控性有限；当加入推理时，推理延迟显著。在这项工作中，我们引入了MoTVLA，一种基于混合变换器的视觉语言动作模型，整合了快速-慢速统一推理与行为策略学习。MoTVLA保留了预训练视觉语言模型的通用智能，用于感知、场景理解和语义规划等任务，同时引入领域专家（第二个与预训练VLM共享知识的Transformer）生成领域特定的快速推理（如机器人运动分解），从而提高策略执行效率。通过将动作专家基于分解的运动指令进行条件化，MoTVLA可以学习多样化行为并显著提高语言可控性。在自然语言处理基准、机器人仿真环境和真实世界实验中的广泛评估证实了MoTVLA在快速-慢速推理和操作任务性能方面的优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉-语言-行动(VLA)模型中的两个关键挑战：当不使用生成的推理作为条件时，语言可控性有限；当融入推理过程时，推理延迟显著。这个问题在机器人学习领域非常重要，因为机器人需要既能理解复杂语言指令，又能快速执行任务的能力。特别是在现实世界的应用中，机器人需要实时响应用户指令并准确执行任务，现有方法要么牺牲语言引导能力，要么牺牲推理速度，这限制了机器人在时间关键型应用中的实用性和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法要么缺乏语言可控性，要么推理速度慢，因此需要一种能够同时兼顾两者的方法。他们思考如何在一个统一架构中结合快速推理（用于高效执行）和慢速推理（用于复杂理解），同时保留预训练VLM的通用智能并添加领域特定知识以提高执行效率。作者借鉴了多种现有工作，包括：视觉-语言模型(VLMs)作为预训练基础，扩散策略(DPs)用于建模连续动作空间，混合变换器(MoT)架构用于整合不同功能组件，以及现有的VLA模型如RT-2、OpenVLA、π0.5等。在这些工作基础上，作者创新性地提出了统一的快速-慢速推理架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; MoTVLA的核心思想是在单一架构中统一快速和慢速推理，通过在输入端分解模态，在输出端分解功能，同时在中间保持共享的全局知识库。模型包含三个关键组件：通才(generalist)负责视觉-文本理解和慢速推理；领域专家(domain expert)专注于机器人任务的快速推理；行动专家(action expert)负责多任务策略学习。整体实现流程：1)输入处理：将输入分解为语言提示、RGB图像和可学习查询；2)推理主干：遵循'分解-组合-分解'范式，先独立处理多模态输入，再通过全局自注意力机制整合，最后在输出端解耦执行不同类型推理；3)推理输出：慢速推理用下一个token预测，快速推理用token-wise预测；4)行动专家：用扩散Transformer(DiT)在动作扩散框架内学习策略；5)训练：先进行领域专家监督微调，再训练行动专家；6)推理：支持对话模式(慢速推理)和行动模式(快速推理)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)统一的快速-慢速推理架构：首次在单一模型中统一两种推理，保留通用智能同时高效学习领域知识；2)基于分解运动的策略学习：通过快速推理生成的分解运动来条件化策略学习，加快任务执行速度；3)分解-组合-分解设计：输入端分解模态，中间通过共享注意力整合，输出端分解功能；4)双模式推理：支持对话模式和行动模式，确保响应与语言提示一致；5)知识共享机制：通才和领域专家通过全局自注意力共享知识。相比之前工作的不同：1)与现有VLA模型相比：同时实现高语言可控性和低推理延迟；2)与基于扩散的策略相比：显式生成推理来条件化策略，提高语言可控性；3)与π0.5等模型相比：通过token-wise预测实现快速推理，大幅提高推理效率；4)架构设计不同：采用创新的MoT架构，明确分离功能但允许知识共享；5)训练策略不同：采用两阶段课程学习，保留预训练VLM的通用智能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了MoTVLA，一种基于混合变换器架构的视觉-语言-行动模型，通过统一的快速-慢速推理机制，在保留预训练视觉语言模型通用智能的同时，显著提高了语言可控性和推理效率，为机器人学习中的开放世界语言指导任务提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating visual-language instructions into visuomotor policies is gainingmomentum in robot learning for enhancing open-world generalization. Despitepromising advances, existing approaches face two challenges: limited languagesteerability when no generated reasoning is used as a condition, or significantinference latency when reasoning is incorporated.In this work, we introduceMoTVLA, a mixture-of-transformers (MoT)-based vision-language-action (VLA)model that integrates fast-slow unified reasoning with behavior policylearning. MoTVLA preserves the general intelligence of pre-trained VLMs(serving as the generalist) for tasks such as perception, scene understanding,and semantic planning, while incorporating a domain expert, a secondtransformer that shares knowledge with the pretrained VLM, to generatedomain-specific fast reasoning (e.g., robot motion decomposition), therebyimproving policy execution efficiency. By conditioning the action expert ondecomposed motion instructions, MoTVLA can learn diverse behaviors andsubstantially improve language steerability. Extensive evaluations acrossnatural language processing benchmarks, robotic simulation environments, andreal-world experiments confirm the superiority of MoTVLA in both fast-slowreasoning and manipulation task performance.</description>
      <author>example@mail.com (Wenhui Huang, Changhe Chen, Han Qi, Chen Lv, Yilun Du, Heng Yang)</author>
      <guid isPermaLink="false">2510.18337v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding</title>
      <link>http://arxiv.org/abs/2510.18262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  We have released V1, which only reports the test results. Our work is  still ongoing, and the next version will be coming soon&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型视觉语言模型在自然场景理解方面取得成功，但在水下环境应用尚未充分探索。研究人员开发了UWBench基准数据集，包含15003张高分辨率水下图像及丰富标注，建立了三个水下视觉语言理解基准任务，实验表明水下理解仍具挑战性，该基准为水下环境视觉语言研究提供重要资源。&lt;h4&gt;背景&lt;/h4&gt;大型视觉语言模型在自然场景理解方面取得显著成功，但在水下环境中的应用仍被忽视。水下图像面临光衰减、色彩失真和悬浮颗粒散射等独特挑战，同时需要海洋生态系统和生物分类学专业知识。&lt;h4&gt;目的&lt;/h4&gt;填补大型视觉语言模型在水下环境应用的空白，引入专为水下视觉语言理解设计的综合基准UWBench，为水下环境中的视觉语言研究提供必要资源，支持海洋科学、生态监测和自主水下探索等应用。&lt;h4&gt;方法&lt;/h4&gt;构建UWBench基准，包含15003张不同水生环境的高分辨率水下图像，每张图像配有15281个对象指代表达式和124983个问答对。基于此建立三个综合基准：详细图像字幕生成、视觉定位和视觉问答，用于评估模型在水下环境中的表现。&lt;h4&gt;主要发现&lt;/h4&gt;在最先进视觉语言模型上的实验表明，水下理解仍然具有挑战性，存在大量改进空间。数据集捕捉了可见度、光照条件和水浊度的丰富变化，为模型评估提供了现实的测试平台。&lt;h4&gt;结论&lt;/h4&gt;UWBench基准为推进水下环境中的视觉语言研究提供了必要资源，支持海洋科学、生态监测和自主水下探索等应用。研究代码和基准将公开可用，促进相关领域发展。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉语言模型在自然场景理解方面取得了显著成功，但在水下环境中的应用仍然很大程度上未被探索。水下图像呈现独特的挑战，包括严重的光衰减、色彩失真和悬浮颗粒散射，同时需要海洋生态系统和生物分类学的专业知识。为了填补这一空白，我们引入UWBench，一个专门为水下视觉语言理解设计的综合基准。UWBench包含15003张在不同水生环境中捕获的高分辨率水下图像，涵盖海洋、珊瑚礁和深海栖息地。每张图像都经过人工验证的注释丰富，包括15281个精确描述海洋生物和水下结构的对象指代表达式，以及124983个涵盖从对象识别到生态关系理解的多样化推理能力的问答对。数据集捕捉到了可见度、光照条件和水浊度的丰富变化，为模型评估提供了现实的测试平台。基于UWBench，我们建立了三个综合基准：用于生成生态信息场景描述的详细图像字幕生成，用于精确定位海洋生物的视觉定位，以及用于水下环境多模态推理的视觉问答。在最先进的视觉语言模型上的广泛实验表明，水下理解仍然具有挑战性，有大量改进空间。我们的基准为推进水下环境中的视觉语言研究提供了必要的资源，支持海洋科学、生态监测和自主水下探索等应用。我们的代码和基准将公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large vision-language models (VLMs) have achieved remarkable success innatural scene understanding, yet their application to underwater environmentsremains largely unexplored. Underwater imagery presents unique challengesincluding severe light attenuation, color distortion, and suspended particlescattering, while requiring specialized knowledge of marine ecosystems andorganism taxonomy. To bridge this gap, we introduce UWBench, a comprehensivebenchmark specifically designed for underwater vision-language understanding.UWBench comprises 15,003 high-resolution underwater images captured acrossdiverse aquatic environments, encompassing oceans, coral reefs, and deep-seahabitats. Each image is enriched with human-verified annotations including15,281 object referring expressions that precisely describe marine organismsand underwater structures, and 124,983 question-answer pairs covering diversereasoning capabilities from object recognition to ecological relationshipunderstanding. The dataset captures rich variations in visibility, lightingconditions, and water turbidity, providing a realistic testbed for modelevaluation. Based on UWBench, we establish three comprehensive benchmarks:detailed image captioning for generating ecologically informed scenedescriptions, visual grounding for precise localization of marine organisms,and visual question answering for multimodal reasoning about underwaterenvironments. Extensive experiments on state-of-the-art VLMs demonstrate thatunderwater understanding remains challenging, with substantial room forimprovement. Our benchmark provides essential resources for advancingvision-language research in underwater contexts and supporting applications inmarine science, ecological monitoring, and autonomous underwater exploration.Our code and benchmark will be available.</description>
      <author>example@mail.com (Da Zhang, Chenggang Rong, Bingyu Li, Feiyu Wang, Zhiyuan Zhao, Junyu Gao, Xuelong Li)</author>
      <guid isPermaLink="false">2510.18262v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>OpenInsGaussian: Open-vocabulary Instance Gaussian Segmentation with Context-aware Cross-view Fusion</title>
      <link>http://arxiv.org/abs/2510.18253v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OpenInsGaussian是一种开放词汇实例高斯分割框架，通过上下文感知特征提取和注意力驱动的特征聚合两个模块解决了现有3D场景理解方法中的两个主要局限：预处理过程中单个掩码的上下文线索不足，以及融合多视图特征时存在的不一致性和缺失细节。&lt;h4&gt;背景&lt;/h4&gt;理解3D场景对自动驾驶、机器人和增强现实至关重要。现有的语义高斯飞溅方法利用大规模2D视觉模型将2D语义特征投影到3D场景上。&lt;h4&gt;目的&lt;/h4&gt;解决现有语义高斯飞溅方法的两个主要局限：预处理过程中单个掩码的上下文线索不足，以及融合多视图特征时存在的不一致性和缺失细节。&lt;h4&gt;方法&lt;/h4&gt;提出OpenInsGaussian框架，包含两个模块：1) 上下文感知特征提取，为每个掩码添加丰富的语义上下文；2) 注意力驱动的特征聚合，选择性融合多视图特征以减轻对齐错误和不完整性。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上进行的大量实验表明，OpenInsGaussian在开放词汇3D高斯分割方面取得了最先进的结果，大幅优于现有基线。&lt;h4&gt;结论&lt;/h4&gt;OpenInsGaussian方法具有鲁棒性和通用性，标志着3D场景理解及其在各种实际场景中实际部署的重要一步。&lt;h4&gt;翻译&lt;/h4&gt;理解3D场景对自动驾驶、机器人和增强现实至关重要。最近的语义高斯飞溅方法利用大规模2D视觉模型将2D语义特征投影到3D场景上。然而，它们存在两个主要局限：(1) 预处理过程中单个掩码的上下文线索不足；(2) 融合这些2D模型的多视图特征时存在不一致性和缺失细节。在本文中，我们介绍了OpenInsGaussian，一个具有上下文感知跨视图融合的开放词汇实例高斯分割框架。我们的方法包含两个模块：上下文感知特征提取，为每个掩码添加丰富的语义上下文；以及注意力驱动的特征聚合，选择性融合多视图特征以减轻对齐错误和不完整性。在基准数据集上的大量实验表明，OpenInsGaussian在开放词汇3D高斯分割方面取得了最先进的结果，大幅优于现有基线。这些发现强调了所提出方法的鲁棒性和通用性，标志着3D场景理解及其在各种实际场景中实际部署的重要一步。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D场景理解中的两个关键问题：1) 在预处理阶段从掩码提取特征时丢失上下文信息，特别是对小型或部分遮挡对象的识别；2) 多视图特征融合时由于光照、遮挡和视角变化导致的不一致问题。这些问题在自动驾驶、机器人和增强现实等应用中至关重要，因为它们直接影响3D场景语义理解的准确性和可靠性，而现有的语义高斯飞溅方法在这两方面存在明显局限。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性，借鉴了3D高斯飞溅(3DGS)技术用于场景建模，利用CLIP等视觉语言模型提取语义特征，并参考了OpenGaussian的对比学习框架和SAM的分割能力。在此基础上，作者设计了两个关键模块：上下文感知特征提取模块，直接从CLIP中间特征图提取保留上下文的信息；以及注意力驱动的特征聚合策略，根据语义一致性选择性融合多视图特征。这种方法既利用了现有技术的优势，又针对性地解决了它们忽视的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过保留上下文信息和智能融合多视图特征来提升3D场景语义理解的准确性。整体流程分为四个阶段：1) 预处理：使用SAM生成对象掩码，同时提取局部特征和上下文感知特征并融合；2) 实例特征学习：通过对比学习训练3D高斯的类无关实例特征；3) 离散化：使用分层聚类策略将3D高斯分割为类无关簇；4) 语言特征聚合：通过注意力机制将语言特征与分割实例关联，根据视图间语义一致性加权融合特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 上下文感知特征提取，从CLIP中间特征图而非裁剪图像提取特征，保留空间上下文；2) 注意力驱动的特征聚合，基于余弦相似度而非复杂自注意力机制，高效融合多视图特征；3) 几何集成策略融合局部和上下文特征。相比之前工作，OpenInsGaussian解决了LangSplat和LEGaussians的模糊特征问题，也克服了OpenGaussian在处理运动模糊图像时的局限性，同时计算效率更高，无需额外训练即可直接应用于CLIP特征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenInsGaussian通过创新的上下文感知特征提取和注意力驱动的多视图融合机制，显著提升了3D高斯飞溅在开放词汇场景理解中的性能，解决了现有方法中上下文丢失和多视图不一致的关键挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding 3D scenes is pivotal for autonomous driving, robotics, andaugmented reality. Recent semantic Gaussian Splatting approaches leveragelarge-scale 2D vision models to project 2D semantic features onto 3D scenes.However, they suffer from two major limitations: (1) insufficient contextualcues for individual masks during preprocessing and (2) inconsistencies andmissing details when fusing multi-view features from these 2D models. In thispaper, we introduce \textbf{OpenInsGaussian}, an \textbf{Open}-vocabulary\textbf{Ins}tance \textbf{Gaussian} segmentation framework with Context-awareCross-view Fusion. Our method consists of two modules: Context-Aware FeatureExtraction, which augments each mask with rich semantic context, andAttention-Driven Feature Aggregation, which selectively fuses multi-viewfeatures to mitigate alignment errors and incompleteness. Through extensiveexperiments on benchmark datasets, OpenInsGaussian achieves state-of-the-artresults in open-vocabulary 3D Gaussian segmentation, outperforming existingbaselines by a large margin. These findings underscore the robustness andgenerality of our proposed approach, marking a significant step forward in 3Dscene understanding and its practical deployment across diverse real-worldscenarios.</description>
      <author>example@mail.com (Tianyu Huang, Runnan Chen, Dongting Hu, Fengming Huang, Mingming Gong, Tongliang Liu)</author>
      <guid isPermaLink="false">2510.18253v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>HouseTour: A Virtual Real Estate A(I)gent</title>
      <link>http://arxiv.org/abs/2510.18054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published on ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HouseTour，一种从描绘现有3D空间的图像集合中生成具有空间感知的3D相机轨迹和自然语言摘要的方法。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉语言模型在几何推理方面存在困难，缺乏能够同时处理3D相机轨迹生成和文本描述的集成方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够生成平滑视频轨迹并提供3D基础描述的方法，实现自动化、专业质量的视频创作，应用于房地产和旅游领域，且不需要专业知识或设备。&lt;h4&gt;方法&lt;/h4&gt;通过受已知相机位姿约束的扩散过程生成平滑视频轨迹；将这些信息整合到视觉语言模型中以生成基于3D的描述；使用3D高斯溅射技术合成最终视频，沿轨迹渲染新视图；提出包含1200多个房屋导览视频的HouseTour数据集，包括相机位姿、3D重建和房地产描述。&lt;h4&gt;主要发现&lt;/h4&gt;将3D相机轨迹整合到文本生成过程中，比独立处理每个任务的方法性能更好；研究评估了单独和端到端的性能，并引入了一种新的联合度量标准。&lt;h4&gt;结论&lt;/h4&gt;HouseTour方法实现了自动化、专业质量的视频创作，可应用于房地产和旅游领域，且不需要专业知识和设备。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了HouseTour，一种从描绘现有3D空间的图像集合中生成具有空间感知的3D相机轨迹和自然语言摘要的方法。与现有的视觉语言模型不同，后者在几何推理方面存在困难，我们的方法通过受已知相机位姿约束的扩散过程生成平滑的视频轨迹，并将这些信息整合到视觉语言模型中以生成基于3D的描述。我们使用3D高斯溅射技术合成最终视频，沿轨迹渲染新视图。为了支持这项任务，我们提出了HouseTour数据集，其中包含超过1200个带有相机位姿、3D重建和房地产描述的房屋导览视频。实验表明，将3D相机轨迹整合到文本生成过程中，比独立处理每个任务的方法性能更好。我们评估了单独和端到端的性能，并引入了一种新的联合度量标准。我们的工作实现了自动化、专业质量的视频创作，可用于房地产和旅游应用，无需专业知识或设备。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从一组带有已知相机位置的图像中自动生成类似专业房地产视频游览的平滑3D相机轨迹和描述性文本摘要的问题。这个问题在现实中很重要，因为房地产视频游览在YouTube上有超过6.24亿个视频，是美国价值3.43万亿美元房地产市场的关键工具，但目前制作这类视频需要专业人员和高昂设备，劳动密集且成本高。在研究上，这个问题也很重要，因为现有的视觉语言模型在几何推理方面存在困难，生成基于3D空间的视频并用结构化语言描述空间特性仍然是一个挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将问题分解为两个主要任务：3D相机轨迹生成和文本摘要生成。在轨迹生成方面，作者借鉴了Diffuser的扩散过程，但提出了残差扩散器(Residual Diffuser)，将轨迹规划表示为样条插值的残差，这种方法更适合不同房地产布局的交互空间。在文本生成方面，作者基于Qwen2-VL模型构建了Qwen2-VL-3D，使用LoRA微调方法，并将3D空间信息作为第三种模态整合到视觉语言模型中。作者还构建了HouseTour数据集来支持这一任务。整体流程是从输入图像生成轨迹，然后结合轨迹信息和图像生成文本摘要，最后使用3D高斯飞溅技术渲染视频。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过空间感知的3D相机轨迹生成和多模态文本生成来创建类似专业房地产视频游览的系统。整体流程是：1)输入一组带有已知相机位置的图像；2)使用残差扩散器生成平滑的3D相机轨迹；3)使用Qwen2-VL-3D模型结合轨迹信息和图像生成描述性文本摘要；4)使用3D高斯飞溅技术沿着生成的轨迹渲染新视图，合成最终视频。这种方法确保了文本与空间路径对齐，并能生成描述空间布局、功能性和建筑特征的文本，而不仅仅是列出物体。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'空间感知的3D相机轨迹和文本摘要生成'这一新任务；2)开发残差扩散器，将轨迹规划表示为样条插值的残差；3)构建Qwen2-VL-3D模型，整合3D空间信息作为第三种模态；4)创建HouseTour数据集，包含专业房地产描述和真实世界视频轨迹。相比之前的工作，HouseTour的方法在轨迹生成上明确基于已知3D场景几何条件，整体性而非顺序地制定轨迹规划；在文本生成上能处理大量稀疏多图像数据，捕捉完整布局；在数据集上关注空间布局和建筑特征，而非仅关注家具和物体关系。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HouseTour提出了一种结合扩散过程生成的3D相机轨迹和整合空间信息的视觉语言模型，能够自动生成专业质量的房地产视频游览，无需专业知识或设备，并为此任务构建了一个新的数据集。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce HouseTour, a method for spatially-aware 3D camera trajectory andnatural language summary generation from a collection of images depicting anexisting 3D space. Unlike existing vision-language models (VLMs), whichstruggle with geometric reasoning, our approach generates smooth videotrajectories via a diffusion process constrained by known camera poses andintegrates this information into the VLM for 3D-grounded descriptions. Wesynthesize the final video using 3D Gaussian splatting to render novel viewsalong the trajectory. To support this task, we present the HouseTour dataset,which includes over 1,200 house-tour videos with camera poses, 3Dreconstructions, and real estate descriptions. Experiments demonstrate thatincorporating 3D camera trajectories into the text generation process improvesperformance over methods handling each task independently. We evaluate bothindividual and end-to-end performance, introducing a new joint metric. Our workenables automated, professional-quality video creation for real estate andtouristic applications without requiring specialized expertise or equipment.</description>
      <author>example@mail.com (Ata Çelen, Marc Pollefeys, Daniel Barath, Iro Armeni)</author>
      <guid isPermaLink="false">2510.18054v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>SceneCOT: Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes</title>
      <link>http://arxiv.org/abs/2510.16714v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://scenecot.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SCENECOT的新框架和SCENECOT-185K数据集，通过将复杂推理任务分解并结合多模态专家模块，实现了3D场景中的类人推理，在多个基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;现有关于3D大型语言模型的研究仍然难以实现基于场景的问答，主要原因是人类场景-对象基础推理机制的探索不足。&lt;h4&gt;目的&lt;/h4&gt;填补3D场景基础推理研究的空白，通过提出新颖框架实现类人场景-对象推理。&lt;h4&gt;方法&lt;/h4&gt;引入SCENECOT方法将复杂推理任务分解为更简单的问题，基于多模态专家模块构建视觉线索，并开发包含185K个高质量实例的SCENECOT-185K数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在各种复杂的3D场景推理基准测试中，新框架在保持高基础问答连贯性的同时实现了强大的性能。&lt;h4&gt;结论&lt;/h4&gt;首次成功将思维链推理应用于3D场景理解，实现了逐步类人推理，并显示出扩展到更广泛3D场景理解场景的潜力。&lt;h4&gt;翻译&lt;/h4&gt;现有关于3D大型语言模型的研究仍然难以实现基于场景的问答，这主要是由于对类人场景-对象基础推理机制的探索不足。本文通过提出一个新颖框架来弥合这一差距。我们首先在3D场景中引入了一种基础的思维链推理方法，将复杂的推理任务解耦为更简单和可管理的问题，并基于多模态专家模块构建相应的视觉线索。为实现这种方法，我们开发了SCENECOT-185K，这是第一个大规模的基础CoT推理数据集，包含185K个高质量实例。在各种复杂的3D场景推理基准上进行的广泛实验表明，我们的新框架在保持高基础问答连贯性的同时实现了强大的性能。据我们所知，这是首次成功将CoT推理应用于3D场景理解，实现了逐步类人推理，并显示出扩展到更广泛的3D场景理解场景的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D大语言模型在实现基于场景的问答（grounded question-answering）方面的困难，特别是缺乏类人场景-对象推理机制的问题。这个问题在现实中很重要，因为3D场景理解是构建类人智能体的基础能力，而现有模型虽然能生成看似合理的答案，但无法将中间推理步骤与最终结果联系起来，导致grounding-QA一致性差，影响模型在复杂3D环境中的可靠性和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了语言领域中的思维链（Chain-of-Thought, CoT）推理方法，观察到CoT通过将复杂问题分解为可管理的子问题，使语言模型在多种任务中表现出色。作者认为这种逐步推理的方式与人类认知过程相似，也符合3D场景中需要的多跳推理。然而，直接将CoT转移到3D场景具有挑战性，因为需要将基于语言的推理与多模态3D场景表示对齐。因此，作者设计了SCENECOT框架，将3D推理分解为四个阶段：任务识别、区域定位、实体定位和基于场景的推理。该方法借鉴了语言领域的CoT、2D视觉语言推理、多模态大型语言模型和专门的视觉定位模型等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将复杂的3D场景推理任务分解为逐步的、基于场景的思维链推理过程，确保每个答案都有明确的基于场景的步骤支持，从而增强grounding-QA一致性。整体实现流程分为四个阶段：1）任务识别和分析，确定回答问题所需的底层任务和初始分析；2）任务相关区域定位，通过方向线索或基于时钟的参考系缩小推理空间；3）实体定位，使用多模态专家模块定位与问题相关的目标对象；4）基于场景的推理，获取候选对象信息并整合为最终答案。技术实现上，SCENECOT建立在多模态大型语言模型基础上，融入专门的3D-VL和2D-VL模型以及符号引擎来支持这种逐步推理结构。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）提出SCENECOT框架，首次将思维链推理应用于3D场景理解；2）构建SCENECOT-185K数据集，第一个大规模3D场景思维链推理数据集；3）显著提高grounding-QA一致性，在Beacon3D基准上达到34.7的良好一致性。相比之前工作，SCENECOT采用显式的逐步推理机制而非端到端训练，通过显式强制推理前的定位实现准确答案和高grounding-QA一致性，将3D问答视为多阶段任务而非单步任务，并利用大规模标注的推理轨迹数据集进行训练，使决策过程更透明、更接近人类推理方式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCENECOT通过引入首个大规模3D场景思维链推理数据集和框架，实现了类人、可解释且基于场景的3D推理，显著提高了复杂3D场景问答中的grounding-QA一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing research on 3D Large Language Models (LLMs) still struggles toachieve grounded question-answering, primarily due to the under-exploration ofthe mechanism of human-like scene-object grounded reasoning. This paper bridgesthe gap by presenting a novel framework. We first introduce a groundedChain-of-Thought reasoning method in 3D scenes (SCENECOT), decoupling a complexreasoning task into simpler and manageable problems, and building correspondingvisual clues based on multimodal expert modules. To enable such a method, wedevelop SCENECOT-185K, the first large-scale grounded CoT reasoning dataset,consisting of 185K high-quality instances. Extensive experiments across variouscomplex 3D scene reasoning benchmarks demonstrate that our new frameworkachieves strong performance with high grounding-QA coherence. To the best ofour knowledge, this is the first successful application of CoT reasoning to 3Dscene understanding, enabling step-by-step human-like reasoning and showingpotential for extension to broader 3D scene understanding scenarios.</description>
      <author>example@mail.com (Xiongkun Linghu, Jiangyong Huang, Ziyu Zhu, Baoxiong Jia, Siyuan Huang)</author>
      <guid isPermaLink="false">2510.16714v2</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>StreamingTOM: Streaming Token Compression for Efficient Video Understanding</title>
      <link>http://arxiv.org/abs/2510.18269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StreamingTOM是一个解决流式视频视觉语言模型因果性和累积性约束的两阶段框架，通过因果时间减少和在线量化存储技术，实现了高效的流式视频理解。&lt;h4&gt;背景&lt;/h4&gt;与离线处理不同，流式视频视觉语言模型面临两个基本约束：因果性（无法访问未来帧）和累积性（令牌无限增长）。现有方法仅调节后大语言模型的kv-cache，而未解决成本高昂的前大语言模型prefill问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需训练、即插即用的两阶段框架，解决流式视频视觉语言模型的前后大语言模型瓶颈，实现可预测延迟的高效处理。&lt;h4&gt;方法&lt;/h4&gt;StreamingTOM采用两阶段框架：1）因果时间减少：为每帧设定固定预算，基于相邻帧变化和令牌显著性选择令牌，只处理紧凑的视觉子集；2）在线量化存储：以4位格式存储令牌，按需检索相关组并解量化，保持活跃kv-cache有界。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，StreamingTOM实现了15.7倍的kv-cache压缩，1.2倍的更低峰值内存和2倍的更快TTFT。在无需训练的方法中保持最先进精度，在离线基准测试上平均达到63.8%，在RVS上达到55.8%/3.7。&lt;h4&gt;结论&lt;/h4&gt;两阶段方法在具有有界增长的流式视频理解中具有实际优势，有效解决了因果性和累积性约束带来的效率瓶颈。&lt;h4&gt;翻译&lt;/h4&gt;与离线处理不同，流式视频视觉语言模型面临两个基本约束：因果性和累积性。因果性阻止了对离线方法利用的未来帧的访问，而累积性导致令牌无限增长，造成效率瓶颈。然而，现有方法只调节后大语言模型的kv-cache，而保持成本高昂的前大语言模型prefill不变。我们引入了StreamingTOM，这是一个无需训练、即插即用的两阶段框架，通过可预测延迟解决了前大语言模型和后大语言模型的瓶颈。因果时间减少设定了固定的每帧预算，并根据相邻帧变化和令牌显著性选择令牌，通过每帧只处理紧凑的视觉令牌子集而非所有视觉令牌，显著降低了每帧prefill成本。在线量化存储以4位格式存储令牌，按需检索相关组并解量化，无论流长度如何保持活跃kv-cache有界。实验证明，我们的方法相比先前最先进技术实现了15.7倍的kv-cache压缩，1.2倍的更低峰值内存和2倍的更快TTFT。在无需训练的方法中，StreamingTOM在离线基准测试上平均保持63.8%的精度，在RVS上保持55.8%/3.7的精度。这些结果突显了我们的两阶段方法在具有有界增长的流式视频理解中的实际优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unlike offline processing, streaming video vision-language models face twofundamental constraints: causality and accumulation. Causality prevents accessto future frames that offline methods exploit, while accumulation causes tokensto grow unbounded, creating efficiency bottlenecks. However, existingapproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefillunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stageframework that addresses both pre-LLM and post-LLM bottlenecks with predictablelatency. Causal Temporal Reduction imposes a fixed per-frame budget and selectstokens based on adjacent-frame changes and token saliency, drastically reducingper-frame prefill cost by processing only a compact subset of visual tokens perframe instead of all visual tokens. Online Quantized Memory stores tokens in4-bit format, retrieves relevant groups on demand, and dequantizes them,keeping the active kv-cache bounded regardless of stream length. Experimentsdemonstrate our method achieves $15.7\times$ kv-cache compression, $1.2\times$lower peak memory and $2\times$ faster TTFT compared to prior SOTA.StreamingTOM maintains state-of-the-art accuracy among training-free methodswith an average of $63.8\%$ on offline benchmarks and $55.8\%/3.7$ on RVS.These results highlight the practical benefits of our two-stage approach forefficient streaming video understanding with bounded growth.</description>
      <author>example@mail.com (Xueyi Chen, Keda Tao, Kele Shao, Huan Wang)</author>
      <guid isPermaLink="false">2510.18269v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues</title>
      <link>http://arxiv.org/abs/2510.18016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为ViBED-Net的新型深度学习框架，用于从视频数据中检测学生在在线学习环境中的参与度，通过结合面部表情和场景上下文信息，实现了73.43%的准确率，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;在线学习环境中的参与度检测对于提高学生成果和个性化教学至关重要，但现有的检测方法仍有改进空间。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从视频数据中准确评估学生参与度的深度学习框架，通过结合面部表情和全场景上下文信息提高检测准确性。&lt;h4&gt;方法&lt;/h4&gt;设计了ViBED-Net双流架构，使用EfficientNetV2进行空间特征提取，分别处理面部区域和整个视频帧；采用LSTM和Transformer编码器进行时间建模；在DAiSEE数据集上评估；应用有针对性的数据增强技术提高代表性不足类别的性能。&lt;h4&gt;主要发现&lt;/h4&gt;ViBED-Net与LSTM结合的变体达到73.43%的准确率，优于现有最先进方法；结合面部感知和场景感知的时空线索显著提高了参与度检测准确性；模块化设计使其可灵活应用于教育、用户体验研究和内容个性化。&lt;h4&gt;结论&lt;/h4&gt;ViBED-Net为视频情感计算提供了可扩展的高性能解决方案，可用于现实世界的参与度分析，推进了视频情感计算领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;在线学习环境中的参与度检测对于提高学生成果和个性化教学至关重要。我们提出了ViBED-Net（基于视频的参与度检测网络），这是一种新颖的深度学习框架，采用双流架构设计，用于从视频数据中评估学生参与度。ViBED-Net通过EfficientNetV2处理面部裁剪和整个视频帧来捕获面部表情和全场景上下文，进行空间特征提取。然后，使用两种时间建模策略分析这些特征：长短期记忆网络和Transformer编码器。我们的模型在DAiSEE数据集上进行了评估，这是一个大规模的电子学习情感状态识别基准。为了提高代表性不足的参与度类别的性能，我们应用了有针对性的数据增强技术。在测试的变体中，带有LSTM的ViBED-Net实现了73.43%的准确率，优于现有的最先进方法。ViBED-Net证明，结合面部感知和场景感知的时空线索显著提高了参与度检测的准确性。其模块化设计使其具有灵活性，可应用于教育、用户体验研究和内容个性化。这项工作通过为现实世界的参与度分析提供可扩展的高性能解决方案，推动了视频情感计算的发展。该项目的源代码可在https://github.com/prateek-gothwal/ViBED-Net获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Engagement detection in online learning environments is vital for improvingstudent outcomes and personalizing instruction. We present ViBED-Net(Video-Based Engagement Detection Network), a novel deep learning frameworkdesigned to assess student engagement from video data using a dual-streamarchitecture. ViBED-Net captures both facial expressions and full-scene contextby processing facial crops and entire video frames through EfficientNetV2 forspatial feature extraction. These features are then analyzed over time usingtwo temporal modeling strategies: Long Short-Term Memory (LSTM) networks andTransformer encoders. Our model is evaluated on the DAiSEE dataset, alarge-scale benchmark for affective state recognition in e-learning. To enhanceperformance on underrepresented engagement classes, we apply targeted dataaugmentation techniques. Among the tested variants, ViBED-Net with LSTMachieves 73.43\% accuracy, outperforming existing state-of-the-art approaches.ViBED-Net demonstrates that combining face-aware and scene-aware spatiotemporalcues significantly improves engagement detection accuracy. Its modular designallows flexibility for application across education, user experience research,and content personalization. This work advances video-based affective computingby offering a scalable, high-performing solution for real-world engagementanalysis. The source code for this project is available onhttps://github.com/prateek-gothwal/ViBED-Net .</description>
      <author>example@mail.com (Prateek Gothwal, Deeptimaan Banerjee, Ashis Kumer Biswas)</author>
      <guid isPermaLink="false">2510.18016v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding</title>
      <link>http://arxiv.org/abs/2510.17305v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ARR Rolling Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LongInsightBench是首个专门评估模型理解长视频能力的基准测试，整合视觉、音频和文本多模态信息，包含长时长信息密集视频、多样任务场景和严格质量保证流程，实验显示全模态模型在精确时间定位和长距离因果推断任务中仍有挑战。&lt;h4&gt;背景&lt;/h4&gt;现有模型在理解长视频方面的能力缺乏系统评估基准，特别是对于包含丰富人类语言、视角、动作和其他上下文元素的长视频内容。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门的基准测试来评估模型在理解长视频时整合视觉、音频和文本多模态信息的能力，重点关注人类语言、视角、动作等上下文元素。&lt;h4&gt;方法&lt;/h4&gt;1) 从开源数据集FineVideo精心筛选约1000个长时长、信息密集的视频，重点关注讲座、访谈和vlog等包含丰富语言元素的内容；2) 设计六种具有挑战性的任务场景，包括事件内任务和事件间任务；3) 开发三步半自动数据质量保证流程，确保合成问题和答案选项的难度与有效性。&lt;h4&gt;主要发现&lt;/h4&gt;全模态模型(OLMs)在需要精确时间定位和长距离因果推断的任务中面临挑战；扩展实验揭示了OLMs多模态融合中存在信息损失和处理偏差问题。&lt;h4&gt;结论&lt;/h4&gt;当前模型在长视频理解方面仍有明显局限性，特别是在需要精确时间定位和长距离因果推断的任务中，多模态融合过程中存在信息损失和处理偏差。&lt;h4&gt;翻译&lt;/h4&gt;我们引入LongInsightBench，这是首个专门用于评估模型理解长视频能力的基准测试，重点关注人类语言、视角、动作和其他上下文元素，同时整合视觉、音频和文本多模态信息。我们的基准在三个关键方面表现出色：a) 长时长、信息密集的视频：我们基于时长限制和视觉与音频模态的信息密度，从开源数据集FineVideo中精心选择了约1000个视频，重点关注包含丰富语言元素的内容，如讲座、访谈和vlog；b) 多样且有挑战性的任务场景：我们设计了六种具有挑战性的任务场景，包括事件内任务和事件间任务；c) 严格且全面的质量保证流程：我们开发了一个三步半自动数据质量保证流程，以确保合成问题和答案选项的难度和有效性。基于LongInsightBench，我们设计了一系列实验。实验结果表明，全模态模型(OLMs)在需要精确时间定位(T-Loc)和长距离因果推断(CE-Caus)的任务中仍面临挑战。扩展实验揭示了OLMs多模态融合中的信息损失和处理偏差。我们的数据集和代码可在https://anonymous.4open.science/r/LongInsightBench-910F/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce \textbf{LongInsightBench}, the first benchmark designed toassess models' ability to understand long videos, with a focus on humanlanguage, viewpoints, actions, and other contextual elements, while integrating\textbf{visual, audio, and text} modalities. Our benchmark excels in three keyareas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully selectapproximately 1,000 videos from open-source datasets FineVideo based onduration limit and the information density of both visual and audio modalities,focusing on content like lectures, interviews, and vlogs, which contain richlanguage elements. \textbf{b) Diverse and Challenging Task Scenarios:} We havedesigned six challenging task scenarios, including both Intra-Event andInter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality AssurancePipelines:} We have developed a three-step, semi-automated data qualityassurance pipeline to ensure the difficulty and validity of the synthesizedquestions and answer options. Based on LongInsightBench, we designed a seriesof experiments. Experimental results shows that Omni-modal models(OLMs) stillface challenge in tasks requiring precise temporal localization (T-Loc) andlong-range causal inference (CE-Caus). Extended experiments reveal theinformation loss and processing bias in multi-modal fusion of OLMs. Our datasetand code is available athttps://anonymous.4open.science/r/LongInsightBench-910F/.</description>
      <author>example@mail.com (ZhaoYang Han, Qihan Lin, Hao Liang, Bowen Chen, Zhou Liu, Wentao Zhang)</author>
      <guid isPermaLink="false">2510.17305v2</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</title>
      <link>http://arxiv.org/abs/2510.18795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 5 fiugres&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ProCLIP是一种基于课程学习的渐进式视觉-语言对齐框架，旨在解决原始CLIP文本编码器在处理长文本、多语言理解和细粒度语义理解方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;原始CLIP文本编码器受限于77个token的最大输入长度，不支持多语言输入，限制了其在广泛任务中的应用。最近研究尝试用基于LLM的嵌入器替换CLIP文本编码器，但LLM和CLIP的表示空间独立预训练，直接对比学习会破坏CLIP图像编码器的视觉-语言对齐。&lt;h4&gt;目的&lt;/h4&gt;提出ProCLIP框架，有效对齐CLIP图像编码器和基于LLM的嵌入器，解决CLIP文本编码器的局限性。&lt;h4&gt;方法&lt;/h4&gt;ProCLIP首先从CLIP文本编码器向LLM嵌入器蒸馏知识，建立初始对齐；然后通过图像-文本对比调进一步对齐，使用自蒸馏正则化避免过拟合；采用实例语义对齐损失和嵌入结构对齐损失提高对齐效果。&lt;h4&gt;主要发现&lt;/h4&gt;结合知识蒸馏和对比学习的渐进式对齐方法能够有效利用CLIP的预训练知识，同时保持视觉-语言对齐质量。&lt;h4&gt;结论&lt;/h4&gt;ProCLIP框架能够解决CLIP文本编码器的局限性，实现更好的长文本处理、多语言理解和细粒度语义理解能力。&lt;h4&gt;翻译&lt;/h4&gt;原始CLIP文本编码器受限于77个token的最大输入长度，这阻碍了它有效处理长文本和执行细粒度语义理解的能力。此外，CLIP文本编码器不支持多语言输入。所有这些限制显著限制了它在更广泛任务中的适用性。最近的研究试图用基于LLM的嵌入器替换CLIP文本编码器，以增强其在处理长文本、多语言理解和细粒度语义理解方面的能力。然而，由于LLM和CLIP的表示空间是独立预训练的，没有先验对齐，使用对比学习直接对齐会破坏CLIP图像编码器中的内在视觉-语言对齐，导致预训练期间获取的知识利用不足。为应对这一挑战，我们提出了ProCLIP，一种基于课程学习的渐进式视觉-语言对齐框架，有效对齐CLIP图像编码器和基于LLM的嵌入器。具体来说，ProCLIP首先从CLIP文本编码器向基于LLM的嵌入器蒸馏知识，利用CLIP丰富的预训练知识，同时建立LLM嵌入器和CLIP图像编码器之间的初始对齐。随后，ProCLIP通过图像-文本对比调进一步对齐CLIP图像编码器和基于LLM的嵌入器，采用自蒸馏正则化避免过拟合。为了实现更有效的对齐，在表示继承和对比调优过程中使用了实例语义对齐损失和嵌入结构对齐损失。代码可在https://github.com/VisionXLab/ProCLIP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The original CLIP text encoder is limited by a maximum input length of 77tokens, which hampers its ability to effectively process long texts and performfine-grained semantic understanding. In addition, the CLIP text encoder lackssupport for multilingual inputs. All these limitations significantly restrictits applicability across a broader range of tasks. Recent studies haveattempted to replace the CLIP text encoder with an LLM-based embedder toenhance its ability in processing long texts, multilingual understanding, andfine-grained semantic comprehension. However, because the representation spacesof LLMs and the vision-language space of CLIP are pretrained independentlywithout alignment priors, direct alignment using contrastive learning candisrupt the intrinsic vision-language alignment in the CLIP image encoder,leading to an underutilization of the knowledge acquired during pre-training.To address this challenge, we propose ProCLIP, a curriculum learning-basedprogressive vision-language alignment framework to effectively align the CLIPimage encoder with an LLM-based embedder. Specifically, ProCLIP first distillsknowledge from CLIP's text encoder into the LLM-based embedder to leverageCLIP's rich pretrained knowledge while establishing initial alignment betweenthe LLM embedder and CLIP image encoder. Subsequently, ProCLIP further alignsthe CLIP image encoder with the LLM-based embedder through image-textcontrastive tuning, employing self-distillation regularization to avoidoverfitting. To achieve a more effective alignment, instance semantic alignmentloss and embedding structure alignment loss are employed during representationinheritance and contrastive tuning. The Code is available athttps://github.com/VisionXLab/ProCLIP</description>
      <author>example@mail.com (Xiaoxing Hu, Kaicheng Yang, Ziyong Feng, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang)</author>
      <guid isPermaLink="false">2510.18795v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>SEAL: Semantic-Aware Hierarchical Learning for Generalized Category Discovery</title>
      <link>http://arxiv.org/abs/2510.18740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了SEAL框架，通过利用自然层次结构和创新的对比学习方法解决了广义类别发现中的挑战，在多个基准测试上实现了最先进的性能，并展示了良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;广义类别发现在部分标记数据集上的目标是分类所有未标记图像，无论它们属于已知还是未知类别。现有方法通常依赖于单层语义或手动设计的抽象层次结构，限制了泛化能力和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法的局限性，引入一个由自然存在且易于访问的层次结构指导的语义感知层次学习框架(SEAL)。&lt;h4&gt;方法&lt;/h4&gt;在SEAL框架中，提出层次语义引导的软对比学习方法，利用层次相似性生成信息丰富的软负样本；同时设计跨粒度一致性(CGC)模块，对齐不同粒度级别的预测。&lt;h4&gt;主要发现&lt;/h4&gt;SEAL在细粒度基准测试上持续实现了最先进的性能，包括SSB基准、Oxford-Pet和Herbarium19数据集，并在粗粒度数据集上进一步展示了泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SEAL框架通过利用自然层次结构和创新的对比学习方法，有效解决了广义类别发现中的挑战，提高了模型的泛化能力和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文研究了广义类别发现(GCD)的问题。给定一个部分标记的数据集，GCD旨在对所有未标记图像进行分类，无论它们属于已知类别还是未知类别。现有方法通常依赖于单层语义或手动设计的抽象层次结构，这限制了它们的泛化能力和可扩展性。为了解决这些局限性，我们引入了一个由自然存在且易于访问的层次结构指导的语义感知层次学习框架(SEAL)。在SEAL中，我们提出了一种层次语义引导的软对比学习方法，利用层次相似性生成信息丰富的软负样本，解决了传统对比损失将所有负样本同等对待的局限性。此外，还设计了一个跨粒度一致性(CGC)模块，用于对齐不同粒度级别的预测。SEAL在细粒度基准测试上持续实现了最先进的性能，包括SSB基准、Oxford-Pet和Herbarium19数据集，并在粗粒度数据集上进一步展示了泛化能力。项目页面：https://visual-ai.github.io/seal/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates the problem of Generalized Category Discovery (GCD).Given a partially labelled dataset, GCD aims to categorize all unlabelledimages, regardless of whether they belong to known or unknown classes. Existingapproaches typically depend on either single-level semantics or manuallydesigned abstract hierarchies, which limit their generalizability andscalability. To address these limitations, we introduce a SEmantic-awarehierArchical Learning framework (SEAL), guided by naturally occurring andeasily accessible hierarchical structures. Within SEAL, we propose aHierarchical Semantic-Guided Soft Contrastive Learning approach that exploitshierarchical similarity to generate informative soft negatives, addressing thelimitations of conventional contrastive losses that treat all negativesequally. Furthermore, a Cross-Granularity Consistency (CGC) module is designedto align the predictions from different levels of granularity. SEALconsistently achieves state-of-the-art performance on fine-grained benchmarks,including the SSB benchmark, Oxford-Pet, and the Herbarium19 dataset, andfurther demonstrates generalization on coarse-grained datasets. Project page:https://visual-ai.github.io/seal/</description>
      <author>example@mail.com (Zhenqi He, Yuanpei Liu, Kai Han)</author>
      <guid isPermaLink="false">2510.18740v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>ε-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data</title>
      <link>http://arxiv.org/abs/2510.18637v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages main text, 17 pages total&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ε-Seg是一种创新的分层变分自编码器方法，结合了中心区域掩蔽、稀疏标签对比学习、高斯混合模型先验和无聚类标签预测等技术，用于解决电子显微镜图像的生物样本语义分割挑战，特别是在标签稀疏的情况下表现优异。&lt;h4&gt;背景&lt;/h4&gt;电子显微镜(EM)图像的生物样本语义分割在生命科学中仍然是一个挑战。EM数据捕获生物结构的细节，有时复杂到即使是人类观察者也会感到难以处理。&lt;h4&gt;目的&lt;/h4&gt;引入一种名为ε-Seg的方法，用于解决EM图像的语义分割问题，特别是在标签稀疏的情况下(总图像数据的0.05%或更少)。&lt;h4&gt;方法&lt;/h4&gt;基于分层变分自编码器(HVAEs)的方法，采用中心区域掩蔽和修复损失来学习鲁棒和代表性的嵌入，使用对比学习和高斯混合模型先验来塑造潜在空间，并通过MLP语义分割头直接从潜在嵌入预测类标签，而不是聚类潜在嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;ε-Seg在两个密集的生物组织EM数据集上展示了经验结果，该方法也适用于荧光显微镜数据，能够在复杂的生物图像数据上实现具有竞争力的稀疏监督分割结果，即使只有有限的训练标签可用。&lt;h4&gt;结论&lt;/h4&gt;ε-Seg是一种有效的方法，可以在标签稀疏的情况下进行生物图像的语义分割，为生命科学领域的图像分析提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;生物样本的电子显微镜(EM)图像语义分割在生命科学中仍然是一个挑战。EM数据捕获生物结构的细节，有时复杂到即使是人类观察者也会感到难以处理。我们引入ε-Seg，一种基于分层变分自编码器(HVAEs)的方法，采用中心区域掩蔽、稀疏标签对比学习(CL)、高斯混合模型(GMM)先验和无聚类标签预测。中心区域掩蔽和修复损失鼓励模型学习鲁棒和代表性的嵌入来区分所需的类别，即使训练标签稀疏(占总图像数据的0.05%或更少)。为了获得最佳性能，我们采用CL和GMM先验来塑造HVAE的潜在空间，使得编码的输入斑块倾向于关于我们希望区分的语义类别进行聚类。最后，我们不是对潜在嵌入进行聚类以进行语义分割，而是提出一个MLP语义分割头来直接从潜在嵌入预测类标签。我们在两个密集的生物组织EM数据集上展示了ε-Seg和基线方法的经验结果，并证明了我们的方法在荧光显微镜数据上的适用性。我们的结果表明，即使在只有有限训练标签可用的情况下，ε-Seg也能够在复杂的生物图像数据上实现具有竞争力的稀疏监督分割结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation of electron microscopy (EM) images of biologicalsamples remains a challenge in the life sciences. EM data captures details ofbiological structures, sometimes with such complexity that even human observerscan find it overwhelming. We introduce {\epsilon}-Seg, a method based onhierarchical variational autoencoders (HVAEs), employing center-region masking,sparse label contrastive learning (CL), a Gaussian mixture model (GMM) prior,and clustering-free label prediction. Center-region masking and the inpaintingloss encourage the model to learn robust and representative embeddings todistinguish the desired classes, even if training labels are sparse (0.05% ofthe total image data or less). For optimal performance, we employ CL and a GMMprior to shape the latent space of the HVAE such that encoded input patchestend to cluster wrt. the semantic classes we wish to distinguish. Finally,instead of clustering latent embeddings for semantic segmentation, we propose aMLP semantic segmentation head to directly predict class labels from latentembeddings. We show empirical results of {\epsilon}-Seg and baseline methods on2 dense EM datasets of biological tissues and demonstrate the applicability ofour method also on fluorescence microscopy data. Our results show that{\epsilon}-Seg is capable of achieving competitive sparsely-supervisedsegmentation results on complex biological image data, even if only limitedamounts of training labels are available.</description>
      <author>example@mail.com (Sheida Rahnamai Kordasiabi, Damian Dalle Nogare, Florian Jug)</author>
      <guid isPermaLink="false">2510.18637v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with Trainable Text Encoder</title>
      <link>http://arxiv.org/abs/2510.18583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CovMatch是一种可扩展的多模态数据集蒸馏框架，通过联合优化两个编码器实现更强的跨模态对齐和改进的性能，在Flickr30K和COCO数据集上优于最先进方法，仅使用500个合成对即可实现高达6.8%的检索精度提升。&lt;h4&gt;背景&lt;/h4&gt;多模态数据集蒸馏旨在合成少量图像-文本对以高效训练大规模视觉-语言模型。虽然数据集蒸馏在单模态任务中显示出前景，但扩展到多模态对比学习面临关键挑战：学习跨模态对齐和管理大型编码器的高计算成本。&lt;h4&gt;目的&lt;/h4&gt;提出CovMatch框架，解决先前方法中语义对齐受限和性能扩展瓶颈的问题，实现更有效的多模态数据集蒸馏。&lt;h4&gt;方法&lt;/h4&gt;CovMatch通过使真实和合成特征的跨协方差对齐，同时正则化每个模态内的特征分布。与先前方法不同，CovMatch能够联合优化两个编码器，而非仅更新图像编码器和文本投影层。&lt;h4&gt;主要发现&lt;/h4&gt;先前方法通过冻结文本编码器来解决可扩展性问题，但这种方法严重限制了语义对齐，成为性能扩展的瓶颈。&lt;h4&gt;结论&lt;/h4&gt;在Flickr30K和COCO上评估，CovMatch优于最先进的多模态蒸馏方法，仅使用500个合成对即可实现高达6.8%的检索精度绝对提升。&lt;h4&gt;翻译&lt;/h4&gt;多模态数据集蒸馏旨在合成一组小的图像-文本对，以实现大规模视觉-语言模型的高效训练。虽然数据集蒸馏在单模态任务中显示出前景，但将其扩展到多模态对比学习存在关键挑战：学习跨模态对齐和管理大型编码器的高计算成本。先前的方法通过冻结文本编码器并仅更新图像编码器和文本投影层来解决可扩展性问题。然而，我们发现这严重限制了语义对齐，成为性能扩展的瓶颈。我们提出CovMatch，一种可扩展的数据集蒸馏框架，使真实和合成特征的跨协方差对齐，同时正则化每个模态内的特征分布。与先前的方法不同，CovMatch能够联合优化两个编码器，从而实现更强的跨模态对齐和改进的性能。在Flickr30K和COCO上评估，CovMatch优于最先进的多模态蒸馏方法，仅使用500个合成对即可实现高达6.8%的检索精度绝对提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal dataset distillation aims to synthesize a small set of image-textpairs that enables efficient training of large-scale vision-language models.While dataset distillation has shown promise in unimodal tasks, extending it tomultimodal contrastive learning presents key challenges: learning cross-modalalignment and managing the high computational cost of large encoders. Priorapproaches address scalability by freezing the text encoder and update only theimage encoder and text projection layer. However, we find this severely limitssemantic alignment and becomes a bottleneck for performance scaling. We proposeCovMatch, a scalable dataset distillation framework that aligns thecross-covariance of real and synthetic features while regularizing featuredistributions within each modality. Unlike prior approaches, CovMatch enablesjoint optimization of both encoders, leading to stronger cross-modal alignmentand improved performance. Evaluated on Flickr30K and COCO, CovMatch outperformsstate-of-the-art multimodal distillation methods and achieves up to 6.8%absolute gains in retrieval accuracy using only 500 synthetic pairs.</description>
      <author>example@mail.com (Yongmin Lee, Hye Won Chung)</author>
      <guid isPermaLink="false">2510.18583v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>AWSPNet: Attention-based Dual-Tree Wavelet Scattering Prototypical Network for MIMO Radar Target Recognition and Jamming Suppression</title>
      <link>http://arxiv.org/abs/2510.18422v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 10 figures, The code is available in  https://github.com/jiaxuanzhi/AwspNet&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AWSPNet的新型深度学习框架，用于雷达目标识别和干扰抑制。该方法结合了双树复小波变换、注意力机制、预训练网络和监督对比学习，在低信噪比环境下表现优异，具有良好的特征可分离性和泛化能力。通过与滑动窗口方法的集成，形成完整且实用的干扰识别与抑制系统。&lt;h4&gt;背景&lt;/h4&gt;基于数字射频存储器的电子对抗措施日益增多，对雷达系统的生存能力和有效性构成重大威胁。这些干扰器能生成大量欺骗性虚假目标，淹没雷达的处理能力并掩盖真实目标。因此，稳健地区分真实目标和复杂干扰信号的能力，特别是在低信噪比环境下，显得尤为重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够稳健区分真实目标和复杂干扰信号的框架，特别关注低信噪比环境下的性能，实现雷达目标识别和干扰抑制的同时处理。&lt;h4&gt;方法&lt;/h4&gt;提出了基于注意力的双树小波散射原型网络(AWSPNet)，利用双树复小波变换提取对噪声和信号平移具有内在鲁棒性的特征，通过注意力机制和预训练骨干网络进一步优化这些特征。采用监督对比学习策略解决标记数据有限的问题并增强泛化能力，使用原型网络进行分类。通过t-SNE可视化提供网络内部工作的物理解释，并将AWSPNet与时域滑动窗口方法集成形成完整算法。&lt;h4&gt;主要发现&lt;/h4&gt;在-6 dB信噪比条件下，AWSPNet达到了90.45%的准确率。t-SNE可视化显示模型不同阶段的特征可分离性良好，集成算法不仅能识别还能有效抑制各种类型的干扰。&lt;h4&gt;结论&lt;/h4&gt;AWSPNet在复杂电磁环境中具有实际应用潜力，能够有效处理低信噪比环境下的目标识别和干扰抑制问题。&lt;h4&gt;翻译&lt;/h4&gt;基于数字射频存储器的电子对抗措施日益增多，对雷达系统的生存能力和有效性构成重大威胁。这些干扰器能生成大量欺骗性虚假目标，淹没雷达的处理能力并掩盖真实目标。因此，稳健地区分真实目标和复杂干扰信号的能力，特别是在低信噪比环境下，显得尤为重要。本文介绍了基于注意力的双树小波散射原型网络(AWSPNet)，一种为同时进行雷达目标识别和干扰抑制而设计的深度学习框架。AWSPNet的核心是一个编码器，它利用双树复小波变换提取对噪声和信号平移具有内在鲁棒性的特征。这些特征通过注意力机制和预训练骨干网络得到进一步优化。为了解决标记数据有限的问题并增强泛化能力，我们在训练阶段采用了监督对比学习策略。分类由原型网络执行，该网络在少样本学习场景中特别有效，能够快速适应新的信号类型。我们通过大量实验证明了该方法的有效性。结果显示，AWSPNet在-6 dB信噪比下达到90.45%的准确率。此外，我们通过t-SNE可视化提供了网络内部工作的物理解释，分析了模型不同阶段特征的可分离性。最后，通过将AWSPNet与时域滑动窗口方法集成，我们提出了一个不仅能识别还能有效抑制各种类型干扰的完整算法，从而验证了其在复杂电磁环境中实际应用的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing of digital radio frequency memory based electroniccountermeasures poses a significant threat to the survivability andeffectiveness of radar systems. These jammers can generate a multitude ofdeceptive false targets, overwhelming the radar's processing capabilities andmasking targets. Consequently, the ability to robustly discriminate betweentrue targets and complex jamming signals, especially in low signal-to-noiseratio (SNR) environments, is of importance. This paper introduces theattention-based dual-tree wavelet scattering prototypical network (AWSPNet), adeep learning framework designed for simultaneous radar target recognition andjamming suppression. The core of AWSPNet is the encoder that leverages thedual-tree complex wavelet transform to extract features that are inherentlyrobust to noise and signal translations. These features are further refined byan attention mechanism and a pre-trained backbone network. To address thechallenge of limited labeled data and enhance generalization, we employ asupervised contrastive learning strategy during the training phase. Theclassification is performed by a prototypical network, which is particularlyeffective in few-shot learning scenarios, enabling rapid adaptation to newsignal types. We demonstrate the efficacy of our approach through extensiveexperiments. The results show that AWSPNet achieves 90.45\% accuracy at -6 dBSNR. Furthermore, we provide a physical interpretation of the network's innerworkings through t-SNE visualizations, which analyze the feature separabilityat different stages of the model. Finally, by integrating AWSPNet with atime-domain sliding window approach, we present a complete algorithm capable ofnot only identifying but also effectively suppressing various types of jamming,thereby validating its potential for practical application in complexelectromagnetic environments.</description>
      <author>example@mail.com (Yizhen Jia, Siyao Xiao, Wenkai Jia, Hui Chen, Wen-Qin Wang)</author>
      <guid isPermaLink="false">2510.18422v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Few-Shot Classification of Benchmark and Disaster Imagery with ATTBHFA-Net</title>
      <link>http://arxiv.org/abs/2510.18326v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to a SN journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于注意力的Bhattacharyya-Hellinger特征聚合网络(ATTBHFA-Net)，用于解决灾害图像分类中的少样本学习问题。该方法通过线性组合Bhattacharyya系数和Hellinger距离来比较和聚合特征概率分布，形成鲁棒的原型，并提出了基于Bhattacharyya-Hellinger距离的对比损失。实验表明，该方法在四个FSL基准和两个灾害图像数据集上表现出优越的有效性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;自然和人为灾害的频率增加需要先进的视觉识别技术分析关键摄影数据。人工智能和弹性计算系统的进步使快速准确的灾害分类对有效救援行动变得至关重要。然而，灾害背景下的视觉识别面临数据有限且多样的挑战，难以收集和整理全面的高质量灾害图像。&lt;h4&gt;目的&lt;/h4&gt;解决灾害图像分类中的数据稀缺问题，提高少样本学习在灾害图像分类中的性能，克服灾害图像高类内变异和类间相似性的挑战。&lt;h4&gt;方法&lt;/h4&gt;引入了基于注意力的Bhattacharyya-Hellinger特征聚合网络(ATTBHFA-Net)，线性组合Bhattacharyya系数和Hellinger距离来比较和聚合特征概率分布形成鲁棒原型。Bhattacharyya系数作为对比边界增强类间可分性，Hellinger距离对同类对齐进行正则化。提出基于Bhattacharyya-Hellinger距离的对比损失作为余弦相似度损失的分布对应物，与分类交叉熵结合使用提高FSL性能。&lt;h4&gt;主要发现&lt;/h4&gt;在四个FSL基准和两个灾害图像数据集上的实验表明，ATTBHFA-Net与现有方法相比具有优越的有效性和泛化能力。该方法能够有效处理灾害图像的高类内变异和类间相似性问题，提高少样本学习在灾害图像分类中的性能。&lt;h4&gt;结论&lt;/h4&gt;ATTBHFA-Net为灾害图像分类中的少样本学习提供了有效解决方案。通过结合Bhattacharyya系数和Hellinger距离，该方法能够形成更鲁棒的原型，有效处理灾害图像的复杂特征分布，显著提高分类性能。&lt;h4&gt;翻译&lt;/h4&gt;自然和人为灾害频率的增加需要能够分析关键摄影数据的先进视觉识别技术。随着人工智能和弹性计算系统的进步，快速准确的灾害分类对高效救援行动变得至关重要。然而，由于数据有限且多样，难以收集和整理全面的高质量灾害图像，灾害背景下的视觉识别面临重大挑战。少样本学习为数据稀缺问题提供了有前景的方法，但现有的FSL研究主要依赖于缺乏遥感灾害图像的通用基准数据集，限制了其实际有效性。此外，灾害图像表现出高的类内变异和类间相似性，阻碍了基于度量的传统FSL方法的性能。为解决这些问题，本文引入了基于注意力的Bhattacharyya-Hellinger特征聚合网络(ATTBHFA-Net)，该网络线性组合Bhattacharyya系数和Hellinger距离来比较和聚合特征概率分布，形成鲁棒的原型。Bhattacharyya系数作为对比边界增强类间可分性，而Hellinger距离对同类对齐进行正则化。该框架类似于对比学习，但在概率分布上运行，而不是嵌入特征点。此外，提出了基于Bhattacharyya-Hellinger距离的对比损失，作为余弦相似度损失的分布对应物，与分类交叉熵结合使用，显著提高FSL性能。在四个FSL基准和两个灾害图像数据集上的实验表明，与现有方法相比，ATTBHFA-Net具有优越的有效性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing frequency of natural and human-induced disasters necessitatesadvanced visual recognition techniques capable of analyzing criticalphotographic data. With progress in artificial intelligence and resilientcomputational systems, rapid and accurate disaster classification has becomecrucial for efficient rescue operations. However, visual recognition indisaster contexts faces significant challenges due to limited and diverse datafrom the difficulties in collecting and curating comprehensive, high-qualitydisaster imagery. Few-Shot Learning (FSL) provides a promising approach to datascarcity, yet current FSL research mainly relies on generic benchmark datasetslacking remote-sensing disaster imagery, limiting its practical effectiveness.Moreover, disaster images exhibit high intra-class variation and inter-classsimilarity, hindering the performance of conventional metric-based FSL methods.To address these issues, this paper introduces the Attention-basedBhattacharyya-Hellinger Feature Aggregation Network (ATTBHFA-Net), whichlinearly combines the Bhattacharyya coefficient and Hellinger distances tocompare and aggregate feature probability distributions for robust prototypeformation. The Bhattacharyya coefficient serves as a contrastive margin thatenhances inter-class separability, while the Hellinger distance regularizessame-class alignment. This framework parallels contrastive learning butoperates over probability distributions rather than embedded feature points.Furthermore, a Bhattacharyya-Hellinger distance-based contrastive loss isproposed as a distributional counterpart to cosine similarity loss, usedjointly with categorical cross-entropy to significantly improve FSLperformance. Experiments on four FSL benchmarks and two disaster image datasetsdemonstrate the superior effectiveness and generalization of ATTBHFA-Netcompared to existing approaches.</description>
      <author>example@mail.com (Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, Vu Duong)</author>
      <guid isPermaLink="false">2510.18326v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection</title>
      <link>http://arxiv.org/abs/2510.16219v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SentinelNet，一个用于多智能体系统中主动检测和缓解恶意行为的去中心化框架，通过基于信用的检测器和对比学习，实现了高精度的恶意智能体检测和系统准确性恢复。&lt;h4&gt;背景&lt;/h4&gt;恶意智能体对基于大型语言模型的多智能体系统构成重大威胁，影响其可靠性和决策能力。现有防御措施通常采用被动设计或集中式架构，存在单点故障风险。&lt;h4&gt;目的&lt;/h4&gt;解决现有防御措施的不足，提出一个去中心化的框架，用于主动检测和缓解多智能体协作中的恶意行为。&lt;h4&gt;方法&lt;/h4&gt;提出SentinelNet框架，为每个智能体配备基于信用的检测器，通过对比学习在增强的对抗性辩论轨迹上训练，实现自主评估消息可信度，并通过bottom-k消除进行动态邻居排名，抑制恶意通信。同时生成模拟多样威胁的对抗性轨迹，以克服攻击数据稀缺问题。&lt;h4&gt;主要发现&lt;/h4&gt;在多智能体系统基准测试中，SentinelNet实现了近乎完美的恶意智能体检测，两轮辩论内检测率接近100%，从被破坏的基线恢复了95%的系统准确性，并在不同领域和攻击模式中表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SentinelNet为保护协作式多智能体系统建立了一种新的范式，通过去中心化设计解决了单点故障问题，提高了系统的安全性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;恶意智能体对基于大型语言模型的多智能体系统的可靠性和决策能力构成重大威胁。现有防御措施通常因被动设计或引入单点故障风险的集中式架构而不足。为应对这些挑战，我们提出了SentinelNet，这是第一个用于主动检测和缓解多智能体协作中恶意行为的去中心化框架。SentinelNet为每个智能体配备基于信用的检测器，通过在增强的对抗性辩论轨迹上进行对比学习训练，实现自主评估消息可信度，并通过bottom-k消除进行动态邻居排名以抑制恶意通信。为克服攻击数据稀缺问题，它生成模拟多样威胁的对抗性轨迹，确保训练的鲁棒性。在多智能体系统基准测试中，SentinelNet实现了近乎完美的恶意智能体检测，两轮辩论内接近100%，并从被破坏的基线恢复了95%的系统准确性。通过在不同领域和攻击模式中表现出强大的泛化能力，SentinelNet为保护协作式多智能体系统建立了一种新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Malicious agents pose significant threats to the reliability anddecision-making capabilities of Multi-Agent Systems (MAS) powered by LargeLanguage Models (LLMs). Existing defenses often fall short due to reactivedesigns or centralized architectures which may introduce single points offailure. To address these challenges, we propose SentinelNet, the firstdecentralized framework for proactively detecting and mitigating maliciousbehaviors in multi-agent collaboration. SentinelNet equips each agent with acredit-based detector trained via contrastive learning on augmented adversarialdebate trajectories, enabling autonomous evaluation of message credibility anddynamic neighbor ranking via bottom-k elimination to suppress maliciouscommunications. To overcome the scarcity of attack data, it generatesadversarial trajectories simulating diverse threats, ensuring robust training.Experiments on MAS benchmarks show SentinelNet achieves near-perfect detectionof malicious agents, close to 100% within two debate rounds, and recovers 95%of system accuracy from compromised baselines. By exhibiting stronggeneralizability across domains and attack patterns, SentinelNet establishes anovel paradigm for safeguarding collaborative MAS.</description>
      <author>example@mail.com (Yang Feng, Xudong Pan)</author>
      <guid isPermaLink="false">2510.16219v2</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model for Microclimate Impact Prediction</title>
      <link>http://arxiv.org/abs/2510.18773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 9 figures. Accepted at the NeurIPS 2025 Workshop on  Tackling Climate Change with Machine Learning&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了如何利用地理空间基础模型来预测城市热岛效应，并通过微调模型来评估缓解策略的有效性。&lt;h4&gt;背景&lt;/h4&gt;随着城市化和气候变化的发展，城市热岛效应变得越来越频繁和严重。传统机器学习模型因数据有限，尤其是在服务不足的地区，往往产生不准确的预测。&lt;h4&gt;目的&lt;/h4&gt;为了制定有效的城市热岛缓解计划，需要详细的气温数据，而地理空间基础模型提供了一种有希望的替代方案，表现出强大的泛化能力且只需少量微调。&lt;h4&gt;方法&lt;/h4&gt;研究通过量化绿色空间的冷却效果建立城市热模式的实证真实基准，并将其与模型预测比较以评估模型准确性。随后对基础模型进行微调，预测未来气候情景下的地表温度，并通过模拟修复演示其实际应用价值。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型为评估数据稀缺地区的城市热岛缓解策略提供了一种强大的方式。&lt;h4&gt;结论&lt;/h4&gt;基础模型可以帮助支持更具气候适应能力的城市建设。&lt;h4&gt;翻译&lt;/h4&gt;随着城市化和气候变化的推进，城市热岛效应正变得更加频繁和严重。为了制定有效的缓解计划，城市需要详细的气温数据，然而传统的机器学习模型因数据有限，尤其是在服务不足的地区，往往产生不准确的预测。基于全球非结构化数据训练的地理空间基础模型提供了一种有希望的替代方案，表现出强大的泛化能力且只需少量微调。在本研究中，通过量化绿色空间的冷却效果并建立城市热模式的实证真实基准，将其与模型预测进行比较以评估模型准确性。随后对基础模型进行微调以预测未来气候情景下的地表温度，并通过模拟修复演示其实际价值。结果表明，基础模型为评估数据稀缺地区的城市热岛缓解策略提供了一种强大方式，以支持更具气候适应能力的城市建设。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As urbanization and climate change progress, urban heat island effects arebecoming more frequent and severe. To formulate effective mitigation plans,cities require detailed air temperature data, yet conventional machine learningmodels with limited data often produce inaccurate predictions, particularly inunderserved areas. Geospatial foundation models trained on global unstructureddata offer a promising alternative by demonstrating strong generalization andrequiring only minimal fine-tuning. In this study, an empirical ground truth ofurban heat patterns is established by quantifying cooling effects from greenspaces and benchmarking them against model predictions to evaluate the model'saccuracy. The foundation model is subsequently fine-tuned to predict landsurface temperatures under future climate scenarios, and its practical value isdemonstrated through a simulated inpainting that highlights its role formitigation support. The results indicate that foundation models offer apowerful way for evaluating urban heat island mitigation strategies indata-scarce regions to support more climate-resilient cities.</description>
      <author>example@mail.com (Jannis Fleckenstein, David Kreismann, Tamara Rosemary Govindasamy, Thomas Brunschwiler, Etienne Vos, Mattia Rigotti)</author>
      <guid isPermaLink="false">2510.18773v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Adapting Language Balance in Code-Switching Speech</title>
      <link>http://arxiv.org/abs/2510.18724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型基础模型在代码转换测试案例中表现不佳，原因可能是代码转换时刻不频繁且第二语言嵌入微妙。研究提出通过为训练过程提供标签并利用语言差异突出代码转换点，减轻生成过程中的上下文偏差，提高模型鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型在标准基准测试中取得了令人印象深刻的结果，但在代码转换测试案例中仍然表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提高大型基础模型在代码转换任务中的表现，解决模型在识别和预测代码转换点时的困难。&lt;h4&gt;方法&lt;/h4&gt;利用嵌入语言和主要语言之间的差异来突出代码转换点，为训练过程提供标签，采用简单有效的可微分替代方法减轻生成过程中的上下文偏差。&lt;h4&gt;主要发现&lt;/h4&gt;大型基础模型在代码转换测试案例中表现不佳的原因是代码转换时刻不频繁且第二语言嵌入微妙；通过提供标签和突出转换点，模型能够更正确地预测转换位置，替换错误减少。&lt;h4&gt;结论&lt;/h4&gt;通过为训练过程提供标签并利用语言差异突出代码转换点，可以有效提高大型基础模型在代码转换任务中的鲁棒性，减轻生成过程中的上下文偏差这一核心挑战。&lt;h4&gt;翻译&lt;/h4&gt;尽管在标准基准测试上取得了令人印象深刻的结果，大型基础模型仍然难以应对代码转换测试案例。当数据稀缺不能作为性能不佳的通常理由时，原因可能在于代码转换时刻的不频繁出现，其中第二语言的嵌入显得微妙。与其期望模型自己学习这种不频繁性，不如为训练过程提供标签。评估模型在代码转换数据上的性能需要仔细定位代码转换点，在这些点上识别错误最为关键，以便分析强调在这些时刻发生的错误。基于这一观察，我们利用嵌入语言和主要语言之间的差异来突出这些代码转换点，从而强调在这些位置的学习。这种简单而有效的可微分替代方法减轻了生成过程中的上下文偏差——代码转换中的核心挑战，从而提高了模型的鲁棒性。我们在阿拉伯语和中文-英语方面的实验表明，模型能够更正确地预测转换位置，表现为替换错误的减少。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite achieving impressive results on standard benchmarks, largefoundational models still struggle against code-switching test cases. When datascarcity cannot be used as the usual justification for poor performance, thereason may lie in the infrequent occurrence of code-switched moments, where theembedding of the second language appears subtly. Instead of expecting themodels to learn this infrequency on their own, it might be beneficial toprovide the training process with labels. Evaluating model performance oncode-switching data requires careful localization of code-switching pointswhere recognition errors are most consequential, so that the analysisemphasizes mistakes occurring at those moments. Building on this observation,we leverage the difference between the embedded and the main language tohighlight those code-switching points and thereby emphasize learning at thoselocations. This simple yet effective differentiable surrogate mitigates contextbias during generation -- the central challenge in code-switching -- therebyimproving the model's robustness. Our experiments with Arabic andChinese-English showed that the models are able to predict the switching placesmore correctly, reflected by the reduced substitution error.</description>
      <author>example@mail.com (Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel)</author>
      <guid isPermaLink="false">2510.18724v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Bayesian Low-Rank Factorization for Robust Model Adaptation</title>
      <link>http://arxiv.org/abs/2510.18723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了使用贝叶斯因式分解适配器来适应语音基础模型，以处理代码切换场景，实现在特定领域适应的同时保留基础模型的通用性能。&lt;h4&gt;背景&lt;/h4&gt;大型语音基础模型在许多领域表现出色，但需要适应本地需求如代码切换(说话者在同一话语中混合使用多种语言)。直接微调这些模型存在过拟合风险，可能会覆盖基础模型的广泛能力。&lt;h4&gt;目的&lt;/h4&gt;解决语音基础模型在适应特定领域时保留通用性能的挑战，避免过拟合和灾难性遗忘问题。&lt;h4&gt;方法&lt;/h4&gt;探索贝叶斯因式分解适配器用于语音基础模型，通过将先验设置接近零来实现更稀疏的适应矩阵，从而在适应特定领域的同时保留通用性能。将这种方法应用于Whisper模型，并在不同的多语言代码切换场景中进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;结果显示方法仅产生最小的适应损失，同时显著减少了基础模型的灾难性遗忘。与LoRA相比，该方法在新领域上仅下降4%，同时实现了54%的向后增益。&lt;h4&gt;结论&lt;/h4&gt;贝叶斯适应方法在微调语音基础模型时非常有效，可以在不牺牲泛化能力的情况下实现特定领域的适应。&lt;h4&gt;翻译&lt;/h4&gt;大型语音基础模型在许多领域实现了强大的性能，但它们通常需要适应处理本地需求，如代码切换，即说话者在同一话语中混合语言。直接微调这些模型存在对目标领域过拟合的风险，并可能覆盖基础模型的广泛能力。为解决这一挑战，我们探索了用于语音基础模型的贝叶斯因式分解适配器，它们将先验设置接近零，以实现更稀疏的适应矩阵，从而在适应特定领域的同时保留通用性能。我们将这种方法应用于Whisper模型，并在不同的多语言代码切换场景中进行评估。我们的结果显示，仅产生最小的适应损失，同时显著减少了基础模型的灾难性遗忘。与LoRA相比，我们的方法在新领域上仅下降4%，同时实现了54%的向后增益。这些发现强调了贝叶斯适应在微调语音基础模型而不牺牲泛化能力方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large speech foundation models achieve strong performance across manydomains, but they often require adaptation to handle local needs such ascode-switching, where speakers mix languages within the same utterance. Directfine-tuning of these models risks overfitting to the target domain andoverwriting the broad capabilities of the base model. To address thischallenge, we explore Bayesian factorized adapters for speech foundationmodels, which place priors near zero to achieve sparser adaptation matrices andthereby retain general performance while adapting to specific domains. We applyour approach to the Whisper model and evaluate on different multilingualcode-switching scenarios. Our results show only minimal adaptation loss whilesignificantly reducing catastrophic forgetting of the base model. Compared toLoRA, our method achieves a backward gain of 54% with only a 4% drop on the newdomain. These findings highlight the effectiveness of Bayesian adaptation forfine-tuning speech foundation models without sacrificing generalization.</description>
      <author>example@mail.com (Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel)</author>
      <guid isPermaLink="false">2510.18723v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</title>
      <link>http://arxiv.org/abs/2510.18632v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了3DThinker框架，解决了视觉语言模型从有限视角理解3D空间关系的挑战，通过两阶段训练实现3D思维能力，无需3D先验输入或显式标记的3D数据，在多个基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在多模态任务中取得显著进展，但从有限视角理解3D空间关系仍是重大挑战。先前方法依赖纯文本或2D视觉线索，其有限表示能力阻碍了需要3D空间想象力的任务性能。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在3D空间关系理解上的局限性，开发一个能像人类一样在推理过程中利用图像中丰富几何信息的框架。&lt;h4&gt;方法&lt;/h4&gt;提出3DThinker框架，首个在推理过程中实现3D思维而无需任何3D先验输入的框架。训练分两阶段：首先通过监督训练将VLM生成的3D潜变量与3D基础模型对齐；然后仅基于结果信号优化整个推理轨迹，优化底层3D思维能力。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上的广泛实验表明，3DThinker始终优于强大的基线模型，为将3D表示统一到多模态推理中提供了新视角。&lt;h4&gt;结论&lt;/h4&gt;3DThinker框架成功解决了从有限视角理解3D空间关系的挑战，不依赖3D先验输入或显式标记的3D数据，通过两阶段训练实现了有效的3D思维能力。&lt;h4&gt;翻译&lt;/h4&gt;尽管视觉语言模型的最新进展在广泛的多模态任务中取得了显著进步，但从有限视角理解3D空间关系仍然是一个重大挑战。先前的推理方法通常依赖纯文本（如拓扑认知图）或2D视觉线索。然而，它们有限的表示能力阻碍了需要3D空间想象力的特定任务性能。为解决这一限制，我们提出了3DThinker，一个能像人类一样在推理过程中有效利用图像中丰富几何信息的框架。我们的框架是首个在推理过程中实现3D思维而无需任何3D先验输入的框架，并且不依赖显式标记的3D数据进行训练。具体来说，我们的训练包括两个阶段。首先，我们进行监督训练，将VLM推理时生成的3D潜变量与3D基础模型（如VGGT）的3D潜变量对齐。然后，我们仅基于结果信号优化整个推理轨迹，从而优化底层的3D思维能力。在多个基准测试上的广泛实验表明，3DThinker始终优于强大的基线模型，并为将3D表示统一到多模态推理中提供了新视角。我们的代码将在https://github.com/zhangquanchen/3DThinker上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉语言模型(VLMs)从有限视角理解3D空间关系的挑战。这个问题很重要，因为空间理解是机器与真实3D世界交互(如具身AI、自动驾驶)的关键能力，这些系统通常依赖于多视角观察，需要从有限视角想象完整场景并进行空间推理。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到人类认知机制的启发，特别是心理意象的认知机制。他们借鉴了Mirage框架利用图像嵌入进行监督训练的思想，以及现有的认知地图构建方法如MindCube和Ego3D。作者发现现有方法要么依赖纯文本或2D视觉线索，要么需要辅助模态或外部工具，因此设计3DThinker框架让VLMs能像人类一样在推理过程中进行3D空间想象。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让VLMs在推理过程中生成紧凑的3D潜在嵌入作为3D令牌，模拟人类在空间推理中想象的3D场景。整体实现流程分为两阶段：1)监督训练阶段，将VLM生成的3D潜在与3D基础模型对齐，使用3D对齐损失和交叉熵损失优化；2)强化训练阶段，使用基于结果的信号优化整个采样轨迹，保持3D潜在对齐的同时优化推理过程中的3D视觉令牌。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次引入'3D思维'框架，无需密集标注数据；2)提出两阶段训练框架，从特征对齐到基于结果信号学习几何感知；3)增强模型可解释性，能从潜在空间恢复3D表示；4)在多个基准测试中表现优异。相比之前工作，3DThinker不依赖纯文本或2D视觉线索，不需要辅助模态或外部工具，不依赖真实图像监督，且在推理时无需外部3D先验。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 3DThinker首次让视觉语言模型能够在推理过程中进行3D空间想象，通过两阶段训练框架实现了从有限视角图像理解3D几何关系的能力，无需依赖外部3D先验或密集标注数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Though recent advances in vision-language models (VLMs) have achievedremarkable progress across a wide range of multimodal tasks, understanding 3Dspatial relationships from limited views remains a significant challenge.Previous reasoning methods typically rely on pure text (e.g., topologicalcognitive maps) or on 2D visual cues. However, their limited representationalcapacity hinders performance in specific tasks that require 3D spatialimagination. To address this limitation, we propose 3DThinker, a framework thatcan effectively exploits the rich geometric information embedded within imageswhile reasoning, like humans do. Our framework is the first to enable 3Dmentaling during reasoning without any 3D prior input, and it does not rely onexplicitly labeled 3D data for training. Specifically, our training consists oftwo stages. First, we perform supervised training to align the 3D latentgenerated by VLM while reasoning with that of a 3D foundation model (e.g.,VGGT). Then, we optimize the entire reasoning trajectory solely based onoutcome signals, thereby refining the underlying 3D mentaling. Extensiveexperiments across multiple benchmarks show that 3DThinker consistentlyoutperforms strong baselines and offers a new perspective toward unifying 3Drepresentations into multimodal reasoning. Our code will be available athttps://github.com/zhangquanchen/3DThinker.</description>
      <author>example@mail.com (Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, Ruqi Huang)</author>
      <guid isPermaLink="false">2510.18632v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>A Compositional Paradigm for Foundation Models: Towards Smarter Robotic Agents</title>
      <link>http://arxiv.org/abs/2510.18608v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出应用持续学习和组合性原则，以促进开发更灵活、高效和智能的AI解决方案，解决基础模型在适应动态现实世界场景时需要重新训练整个模型的问题。&lt;h4&gt;背景&lt;/h4&gt;基础模型在语言、视觉、机器人控制等多种任务中带来了前所未有的结果。这些模型能够处理大量数据，提取和开发丰富的表示，这些表示可以跨不同领域和模态使用。&lt;h4&gt;目的&lt;/h4&gt;提出应用持续学习和组合性原则，以促进开发更灵活、高效和智能的AI解决方案。&lt;h4&gt;方法&lt;/h4&gt;应用持续学习和组合性原则。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体发现。&lt;h4&gt;结论&lt;/h4&gt;通过应用持续学习和组合性原则，可以开发更灵活、高效和智能的AI解决方案，解决基础模型在适应动态场景时需要重新训练的问题。&lt;h4&gt;翻译&lt;/h4&gt;基础模型的诞生在从语言到视觉，再到机器人控制的广泛任务中带来了前所未有的结果。这些模型能够处理大量数据，并提取和开发丰富的表示，这些表示可以跨不同领域和模态使用。然而，它们在适应动态、现实世界场景方面仍然存在问题，需要从头开始重新训练整个模型。在这项工作中，我们提出应用持续学习和组合性原则，以促进开发更灵活、高效和智能的AI解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The birth of Foundation Models brought unprecedented results in a wide rangeof tasks, from language to vision, to robotic control. These models are able toprocess huge quantities of data, and can extract and develop richrepresentations, which can be employed across different domains and modalities.However, they still have issues in adapting to dynamic, real-world scenarioswithout retraining the entire model from scratch. In this work, we propose theapplication of Continual Learning and Compositionality principles to foster thedevelopment of more flexible, efficient and smart AI solutions.</description>
      <author>example@mail.com (Luigi Quarantiello, Elia Piccoli, Jack Bell, Malio Li, Giacomo Carfì, Eric Nuertey Coleman, Gerlando Gramaglia, Lanpei Li, Mauro Madeddu, Irene Testa, Vincenzo Lomonaco)</author>
      <guid isPermaLink="false">2510.18608v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Decoding Dynamic Visual Experience from Calcium Imaging via Cell-Pattern-Aware SSL</title>
      <link>http://arxiv.org/abs/2510.18516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;POYO-SSL是一种创新的神经科学自监督学习方法，通过专注于可预测神经元并利用数据异质性，实现了比传统方法更好的性能和可扩展性。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在神经科学领域有很大潜力，因为缺乏大规模、标签一致的神经数据集。大多数神经数据集包含异构群体，混合了稳定、可预测的细胞和高度随机、刺激依赖的细胞，这使得在自监督学习中识别一致的活动模式变得困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自监督预训练方法，利用神经数据的异质性来改进预训练并实现规模效益。&lt;h4&gt;方法&lt;/h4&gt;POYO-SSL仅在可预测的(统计规律性的)神经元上进行预训练，这些神经元通过简单的更高阶统计量(偏度和峰度)在预训练分割中识别，然后在不可预测的群体上进行微调，用于下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;在Allen Brain Observatory数据集上，POYO-SSL比从头开始训练获得了约12-13%的相对提升，显示出与模型大小相关的平滑、单调的扩展性。相比之下，现有最先进的基线在模型大小增加时趋于平稳或不稳定。&lt;h4&gt;结论&lt;/h4&gt;通过将可预测性作为构建数据饮食的明确指标，POYO-SSL将异质性从负债转变为资产，为可扩展的神经解码提供了一种稳健的、基于生物学的方法，为神经动力学的基础模型铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习在神经科学应用中具有巨大潜力，这归因于缺乏大规模、标签一致的神经数据集。然而，大多数神经数据集包含异构群体，混合了稳定、可预测的细胞和高度随机、刺激依赖的细胞，这使得在自监督学习中识别一致的活动模式变得困难。因此，自监督预训练尚未在神经数据上显示出明显的规模效益。在这里，我们提出了一种自监督预训练的新方法POYO-SSL，它利用神经数据的异质性来改进预训练并实现规模效益。具体而言，在POYO-SSL中，我们仅在可预测的(统计规律性)神经元上进行预训练——这些神经元通过简单的更高阶统计量(偏度和峰度)在预训练分割中识别，然后在不可预测的群体上进行微调，用于下游任务。在Allen Brain Observatory数据集上，这种策略比从头开始训练获得了约12-13%的相对提升，并显示出与模型大小相关的平滑、单调的扩展性。相比之下，现有最先进的基线在模型大小增加时趋于平稳或不稳定。通过将可预测性作为构建数据饮食的明确指标，POYO-SSL将异质性从负债转变为资产，为可扩展的神经解码提供了一种稳健的、基于生物学的方法，并为神经动力学的基础模型铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) holds a great deal of promise for applicationsin neuroscience, due to the lack of large-scale, consistently labeled neuraldatasets. However, most neural datasets contain heterogeneous populations thatmix stable, predictable cells with highly stochastic, stimulus-contingent ones,which has made it hard to identify consistent activity patterns during SSL. Asa result, self-supervised pretraining has yet to show clear signs of benefitsfrom scale on neural data. Here, we present a novel approach to self-supervisedpretraining, POYO-SSL that exploits the heterogeneity of neural data to improvepre-training and achieve benefits of scale. Specifically, in POYO-SSL wepretrain only on predictable (statistically regular) neurons-identified on thepretraining split via simple higher-order statistics (skewness andkurtosis)-then we fine-tune on the unpredictable population for downstreamtasks. On the Allen Brain Observatory dataset, this strategy yieldsapproximately 12-13% relative gains over from-scratch training and exhibitssmooth, monotonic scaling with model size. In contrast, existingstate-of-the-art baselines plateau or destabilize as model size increases. Bymaking predictability an explicit metric for crafting the data diet, POYO-SSLturns heterogeneity from a liability into an asset, providing a robust,biologically grounded recipe for scalable neural decoding and a path towardfoundation models of neural dynamics.</description>
      <author>example@mail.com (Sangyoon Bae, Mehdi Azabou, Jiook Cha, Blake Richards)</author>
      <guid isPermaLink="false">2510.18516v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models</title>
      <link>http://arxiv.org/abs/2510.18457v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code and models available at: https://github.com/tianciB/VFM-VAE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种视觉基础模型变分自编码器(VFM-VAE)方法，解决了潜在扩散模型中视觉tokenizer与视觉基础模型对齐的鲁棒性问题，通过多尺度潜在融合和渐进分辨率重建技术，实现了高质量图像重建，并在较少训练epochs内取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;潜在扩散模型(LDMs)的性能严重依赖于其视觉tokenizer的质量。近期研究尝试通过蒸馏方法整合视觉基础模型(VFMs)，但这种方法会削弱与原始VFM的鲁棒性对齐，导致潜在表示在分布偏移下语义偏离。&lt;h4&gt;目的&lt;/h4&gt;开发一种直接整合视觉基础模型到潜在扩散模型中的方法，避免蒸馏方法带来的对齐鲁棒性问题，同时实现高质量图像重建和高效训练。&lt;h4&gt;方法&lt;/h4&gt;提出视觉基础模型变分自编码器(VFM-VAE)，重新设计解码器结构，采用多尺度潜在融合和渐进分辨率重建模块，从空间上粗糙的VFM特征实现高质量重建。同时提供扩散训练期间表示动力学的综合分析，引入SE-CKNNA指标作为诊断工具，并开发联合tokenizer-扩散对齐策略。&lt;h4&gt;主要发现&lt;/h4&gt;通过重新设计VFM-VAE解码器结构和联合对齐策略，系统在仅80个epoch内达到2.20的gFID(无CFG)，比之前的tokenizer快10倍；继续训练至640个epoch后，进一步达到1.62的gFID(无CFG)。直接VFM整合被证明是LDMs的优越范式。&lt;h4&gt;结论&lt;/h4&gt;直接整合视觉基础模型到潜在扩散模型中的方法比传统的蒸馏方法更有效，能够保持与原始VFM的鲁棒性对齐，实现高质量图像重建，并显著加速训练过程。&lt;h4&gt;翻译&lt;/h4&gt;潜在扩散模型(LDMs)的性能严重依赖于其视觉tokenizer的质量。虽然近期工作已经探索通过蒸馏整合视觉基础模型(VFMs)，但我们发现这种方法存在一个根本缺陷：它不可避免地会削弱与原始VFM的对齐鲁棒性，导致对齐的潜在表示在分布偏移下发生语义偏离。在本文中，我们通过提出一种更直接的方法来绕过蒸馏：视觉基础模型变分自编码器(VFM-VAE)。为了解决VFM的语义焦点与像素级保真度需求之间的内在张力，我们使用多尺度潜在融合和渐进分辨率重建块重新设计了VFM-VAE解码器，使得从空间上粗糙的VFM特征中实现高质量重建成为可能。此外，我们提供了扩散训练期间表示动力学的全面分析，引入了所提出的SE-CKNNA指标作为这一诊断的更精确工具。这一分析使我们能够开发联合tokenizer-扩散对齐策略，显著加速了收敛。我们在tokenizer设计和训练策略方面的创新带来了卓越的性能和效率：我们的系统在仅80个epoch内达到2.20的gFID(无CFG)(比之前的tokenizer快10倍)。继续训练至640个epoch后，它进一步获得了1.62的gFID(无CFG)，确立了直接VFM整合作为LDMs的优越范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The performance of Latent Diffusion Models (LDMs) is critically dependent onthe quality of their visual tokenizer. While recent works have exploredincorporating Vision Foundation Models (VFMs) via distillation, we identify afundamental flaw in this approach: it inevitably weakens the robustness ofalignment with the original VFM, causing the aligned latents to deviatesemantically under distribution shifts. In this paper, we bypass distillationby proposing a more direct approach: Vision Foundation Model VariationalAutoencoder (VFM-VAE). To resolve the inherent tension between the VFM'ssemantic focus and the need for pixel-level fidelity, we redesign the VFM-VAEdecoder with Multi-Scale Latent Fusion and Progressive ResolutionReconstruction blocks, enabling high-quality reconstruction from spatiallycoarse VFM features. Furthermore, we provide a comprehensive analysis ofrepresentation dynamics during diffusion training, introducing the proposedSE-CKNNA metric as a more precise tool for this diagnosis. This analysis allowsus to develop a joint tokenizer-diffusion alignment strategy that dramaticallyaccelerates convergence. Our innovations in tokenizer design and trainingstrategy lead to superior performance and efficiency: our system reaches a gFID(w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers).With continued training to 640 epochs, it further attains a gFID (w/o CFG) of1.62, establishing direct VFM integration as a superior paradigm for LDMs.</description>
      <author>example@mail.com (Tianci Bi, Xiaoyi Zhang, Yan Lu, Nanning Zheng)</author>
      <guid isPermaLink="false">2510.18457v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Automated urban waterlogging assessment and early warning through a mixture of foundation models</title>
      <link>http://arxiv.org/abs/2510.18425v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Nature&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了UrbanWaterlogging Assessment (UWAssess)框架，利用基础模型自动识别监控图像中的内涝区域并生成结构化评估报告，通过半监督微调和思维链提示策略解决数据稀缺问题，显著提高了感知性能，并能够生成可靠的文本报告，支持城市管理和灾害应对。&lt;h4&gt;背景&lt;/h4&gt;气候变化加剧，城市内涝对全球公共安全和基础设施构成越来越严重的威胁。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自动识别内涝区域并生成结构化评估报告的框架，替代依赖人工报告的传统监测方法。&lt;h4&gt;方法&lt;/h4&gt;设计UrbanWaterlogging Assessment (UWAssess)框架，采用半监督微调策略和思维链(CoT)提示策略，以解决标记数据稀缺问题，释放基础模型在数据稀缺下游任务中的潜力。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的视觉基准测试中评估显示感知性能有显著提高；基于GPT的评估确认了UWAssess能够生成可靠的文本报告，准确描述内涝范围、深度、风险和影响。&lt;h4&gt;结论&lt;/h4&gt;UWAssess的双重能力使内涝监测从感知转变为生成，多个基础模型的协作框架为智能和可扩展系统奠定了基础，支持城市管理、灾害应对和气候韧性。&lt;h4&gt;翻译&lt;/h4&gt;随着气候变化加剧，城市内涝对全球公共安全和基础设施构成越来越严重的威胁。然而，现有的监测方法主要依赖人工报告，无法提供及时和全面的评估。在本研究中，我们提出了UrbanWaterlogging Assessment (UWAssess)，这是一个基础模型驱动的框架，可以自动识别监控图像中的内涝区域并生成结构化评估报告。为解决标记数据稀缺的问题，我们设计了一种半监督微调策略和思维链(CoT)提示策略，以释放基础模型在数据稀缺下游任务中的潜力。在具有挑战性的视觉基准测试中的评估表明感知性能有显著提高。基于GPT的评估确认了UWAssess能够生成可靠的文本报告，准确描述内涝范围、深度、风险和影响。这种双重能力使内涝监测从感知转变为生成，而多个基础模型的协作框架为智能和可扩展系统奠定了基础，支持城市管理、灾害应对和气候韧性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With climate change intensifying, urban waterlogging poses an increasinglysevere threat to global public safety and infrastructure. However, existingmonitoring approaches rely heavily on manual reporting and fail to providetimely and comprehensive assessments. In this study, we present UrbanWaterlogging Assessment (UWAssess), a foundation model-driven framework thatautomatically identifies waterlogged areas in surveillance images and generatesstructured assessment reports. To address the scarcity of labeled data, wedesign a semi-supervised fine-tuning strategy and a chain-of-thought (CoT)prompting strategy to unleash the potential of the foundation model fordata-scarce downstream tasks. Evaluations on challenging visual benchmarksdemonstrate substantial improvements in perception performance. GPT-basedevaluations confirm the ability of UWAssess to generate reliable textualreports that accurately describe waterlogging extent, depth, risk and impact.This dual capability enables a shift of waterlogging monitoring from perceptionto generation, while the collaborative framework of multiple foundation modelslays the groundwork for intelligent and scalable systems, supporting urbanmanagement, disaster response and climate resilience.</description>
      <author>example@mail.com (Chenxu Zhang, Fuxiang Huang, Lei Zhang)</author>
      <guid isPermaLink="false">2510.18425v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning</title>
      <link>http://arxiv.org/abs/2510.18318v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Earth AI是一种创新的地理空间AI系统，结合了三个关键领域的基础模型和Gemini驱动的推理引擎，能够有效处理大量多样的地理空间数据，提供深入的见解和预测能力，特别是在危机情境中表现突出。&lt;h4&gt;背景&lt;/h4&gt;地理空间数据为理解我们的星球提供了巨大潜力。然而，这些数据的巨大规模和多样性，以及不同的分辨率、时间尺度和稀疏性，给彻底分析和解释带来了显著挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍Earth AI，这是一种地理空间AI模型和智能代理推理系统，旨在显著提高我们从数据中解锁关于地球的新见解的能力。&lt;h4&gt;方法&lt;/h4&gt;基于三个关键领域的基础模型——全球规模图像、人口和环境，以及一个由Gemini驱动的智能推理引擎。还开发了一个由Gemini驱动的代理，能够对多个基础模型以及大型地理空间数据源和工具进行联合推理，以处理复杂的多步骤查询。&lt;h4&gt;主要发现&lt;/h4&gt;严格基准测试证明了基础模型的强大功能和新型能力；当这些模型一起使用时，它们为地理空间推理提供了互补价值，协同作用能够解锁卓越的预测能力；在真实世界危机场景基准测试中，代理展示了提供关键及时见解的能力，有效弥合了原始地理空间数据和可操作理解之间的差距。&lt;h4&gt;结论&lt;/h4&gt;Earth AI通过结合基础模型和智能代理推理，能够克服地理空间数据分析中的挑战，提供更深入的见解，特别是在危机情境中，能够将原始数据转化为可操作的理解。&lt;h4&gt;翻译&lt;/h4&gt;地理空间数据为理解我们的星球提供了巨大潜力。然而，这些数据的巨大规模和多样性，以及不同的分辨率、时间尺度和稀疏性，给彻底分析和解释带来了显著挑战。本文介绍了Earth AI，这是一种地理空间AI模型家族和智能代理推理系统，使我们在解锁关于地球的新颖而深刻见解的能力上取得了显著进步。该方法建立在三个关键领域的基础模型之上——全球规模图像、人口和环境，以及一个由Gemini驱动的智能推理引擎。我们展示了严格的基准测试，证明了我们基础模型的强大功能和新型能力，并验证了当一起使用时，它们为地理空间推理提供了互补价值，它们的协同作用能够解锁卓越的预测能力。为了处理复杂的多步骤查询，我们开发了一个由Gemini驱动的代理，能够对多个基础模型以及大型地理空间数据源和工具进行联合推理。在一个新的真实世界危机场景基准测试中，我们的代理展示了提供关键及时见解的能力，有效地弥合了原始地理空间数据和可操作理解之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geospatial data offers immense potential for understanding our planet.However, the sheer volume and diversity of this data along with its variedresolutions, timescales, and sparsity pose significant challenges for thoroughanalysis and interpretation. This paper introduces Earth AI, a family ofgeospatial AI models and agentic reasoning that enables significant advances inour ability to unlock novel and profound insights into our planet. Thisapproach is built upon foundation models across three key domains--Planet-scaleImagery, Population, and Environment--and an intelligent Gemini-poweredreasoning engine. We present rigorous benchmarks showcasing the power and novelcapabilities of our foundation models and validate that when used together,they provide complementary value for geospatial inference and their synergiesunlock superior predictive capabilities. To handle complex, multi-step queries,we developed a Gemini-powered agent that jointly reasons over our multiplefoundation models along with large geospatial data sources and tools. On a newbenchmark of real-world crisis scenarios, our agent demonstrates the ability todeliver critical and timely insights, effectively bridging the gap between rawgeospatial data and actionable understanding.</description>
      <author>example@mail.com (Aaron Bell, Amit Aides, Amr Helmy, Arbaaz Muslim, Aviad Barzilai, Aviv Slobodkin, Bolous Jaber, David Schottlander, George Leifman, Joydeep Paul, Mimi Sun, Nadav Sherman, Natalie Williams, Per Bjornsson, Roy Lee, Ruth Alcantara, Thomas Turnbull, Tomer Shekel, Vered Silverman, Yotam Gigi, Adam Boulanger, Alex Ottenwess, Ali Ahmadalipour, Anna Carter, Charles Elliott, David Andre, Elad Aharoni, Gia Jung, Hassler Thurston, Jacob Bien, Jamie McPike, Juliet Rothenberg, Kartik Hegde, Kel Markert, Kim Philipp Jablonski, Luc Houriez, Monica Bharel, Phing VanLee, Reuven Sayag, Sebastian Pilarski, Shelley Cazares, Shlomi Pasternak, Siduo Jiang, Stone Jiang, Thomas Colthurst, Yang Chen, Yehonathan Refael, Yochai Blau, Yuval Carny, Yael Maguire, Avinatan Hassidim, James Manyika, Tim Thelin, Genady Beryozkin, Gautam Prasad, Luke Barrington, Yossi Matias, Niv Efron, Shravya Shetty)</author>
      <guid isPermaLink="false">2510.18318v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>From Agent Simulation to Social Simulator: A Comprehensive Review (Part 1)</title>
      <link>http://arxiv.org/abs/2510.18271v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是基于主体的建模(Agent-Based Modeling, ABM)综合评论的第一部分，重点介绍了ABM的历史发展和经典案例，包括基础模型和社会模拟案例的分类。&lt;h4&gt;背景&lt;/h4&gt;传统物理模拟方法在社会领域面临重大挑战，这促使了ABM的发展。ABM有着自己的发展历史和设计原则。&lt;h4&gt;目的&lt;/h4&gt;帮助读者理解传统物理模拟方法在社会领域面临的挑战，并介绍ABM的基础模型和经典案例。&lt;h4&gt;方法&lt;/h4&gt;详细介绍了模拟社会系统的基础模型，包括个体模型、环境模型和基于规则的模型。&lt;h4&gt;主要发现&lt;/h4&gt;社会模拟的经典案例可分为三类：思想实验、机制探索和平行优化。&lt;h4&gt;结论&lt;/h4&gt;ABM作为一种模拟社会系统的方法，有着自己的历史发展、设计原则、基础模型和经典应用案例。&lt;h4&gt;翻译&lt;/h4&gt;这是基于主体的建模(Agent-Based Modeling, ABM)综合评论的第一部分，重点介绍了ABM的历史发展和经典案例。它首先讨论了ABM的发展历史和设计原则，帮助读者理解传统物理模拟方法在社会领域面临的重大挑战。然后，它详细介绍了模拟社会系统的基础模型，包括个体模型、环境模型和基于规则的模型。最后，它介绍了社会模拟的经典案例，涵盖三种类型：思想实验、机制探索和平行优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This is the first part of the comprehensive review, focusing on thehistorical development of Agent-Based Modeling (ABM) and its classic cases. Itbegins by discussing the development history and design principles ofAgent-Based Modeling (ABM), helping readers understand the significantchallenges that traditional physical simulation methods face in the socialdomain. Then, it provides a detailed introduction to foundational models forsimulating social systems, including individual models, environmental models,and rule-based models. Finally, it presents classic cases of social simulation,covering three types: thought experiments, mechanism exploration, and paralleloptimization.</description>
      <author>example@mail.com (Xiao Xue, Deyu Zhou, Ming Zhang, Fei-Yue Wang)</author>
      <guid isPermaLink="false">2510.18271v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title>
      <link>http://arxiv.org/abs/2510.18214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, 4 tables. Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Vision Language Safety Understanding (VLSU)框架，通过细化的严重性分类和组合分析系统评估多模态安全性，揭示了当前模型在联合图像-文本理解方面的弱点。&lt;h4&gt;背景&lt;/h4&gt;当前多模态基础模型的安全评估通常将视觉和语言输入分开处理，忽略了良性内容组合后可能产生的风险；现有方法无法明确区分不安全内容和边缘情况，导致过度屏蔽或对真正有害内容拒绝不足。&lt;h4&gt;目的&lt;/h4&gt;提出一个综合框架来系统评估多模态安全性，通过细化的严重性分类和组合分析识别模型在联合理解方面的缺陷。&lt;h4&gt;方法&lt;/h4&gt;使用多阶段流程结合真实世界图像和人工注释，构建包含8,187个样本、涵盖15个危害类别的大规模基准数据集，评估11个最先进的模型。&lt;h4&gt;主要发现&lt;/h4&gt;模型在清晰的单模态安全信号上准确率达90%以上，但在需要联合图像-文本推理时性能显著下降至20-55%；34%的联合分类错误发生在正确分类单个模态的情况下；模型难以平衡拒绝不安全内容与回应边缘情况；指令框架可减少过度屏蔽但代价是降低对不安全内容的拒绝率。&lt;h4&gt;结论&lt;/h4&gt;该框架暴露了当前模型在联合图像-文本理解方面的弱点和对齐差距，为研究稳健的视觉-语言安全提供了关键的测试平台。&lt;h4&gt;翻译&lt;/h4&gt;多模态基础模型的安全评估通常将视觉和语言输入分开处理，忽略了良性内容组合后可能产生的风险。现有方法也无法明确区分不安全内容和边缘情况，导致对真正有害内容过度屏蔽或拒绝不足。我们提出了Vision Language Safety Understanding (VLSU)，一个通过细化的严重性分类和17种不同安全模式的组合分析来系统评估多模态安全的综合框架。使用多阶段流程结合真实世界图像和人工注释，我们构建了一个包含8,187个样本、跨越15个危害类别的大规模基准。我们对11个最先进模型的评估揭示了系统性的联合理解失败：尽管模型在清晰的单模态安全信号上达到90%以上的准确率，但当需要联合图像-文本推理来确定安全标签时，性能显著下降到20-55%。最关键的是，34%的联合图像-文本安全分类错误发生在正确分类单个模态的情况下，进一步证明了缺乏组合推理能力。此外，我们发现模型难以平衡拒绝不安全内容同时回应值得关注的边缘情况。例如，我们发现指令框架可以将Gemini-1.5对边缘内容的过度屏蔽率从62.4%降低到10.4%，但代价是对不安全内容的拒绝率从90.8%下降到53.9%。总体而言，我们的框架暴露了当前模型在联合图像-文本理解方面的弱点和对齐差距，并为研究稳健的视觉-语言安全的下一个里程碑提供了关键的测试平台。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safety evaluation of multimodal foundation models often treats vision andlanguage inputs separately, missing risks from joint interpretation wherebenign content becomes harmful in combination. Existing approaches also fail todistinguish clearly unsafe content from borderline cases, leading toproblematic over-blocking or under-refusal of genuinely harmful content. Wepresent Vision Language Safety Understanding (VLSU), a comprehensive frameworkto systematically evaluate multimodal safety through fine-grained severityclassification and combinatorial analysis across 17 distinct safety patterns.Using a multi-stage pipeline with real-world images and human annotation, weconstruct a large-scale benchmark of 8,187 samples spanning 15 harm categories.Our evaluation of eleven state-of-the-art models reveals systematic jointunderstanding failures: while models achieve 90%-plus accuracy on clearunimodal safety signals, performance degrades substantially to 20-55% whenjoint image-text reasoning is required to determine the safety label. Mostcritically, 34% of errors in joint image-text safety classification occurdespite correct classification of the individual modalities, furtherdemonstrating absent compositional reasoning capabilities. Additionally, wefind that models struggle to balance refusing unsafe content while stillresponding to borderline cases that deserve engagement. For example, we findthat instruction framing can reduce the over-blocking rate on borderlinecontent from 62.4% to 10.4% in Gemini-1.5, but only at the cost ofunder-refusing on unsafe content with refusal rate dropping from 90.8% to53.9%. Overall, our framework exposes weaknesses in joint image-textunderstanding and alignment gaps in current models, and provides a criticaltest bed to enable the next milestones in research on robust vision-languagesafety.</description>
      <author>example@mail.com (Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng)</author>
      <guid isPermaLink="false">2510.18214v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>EMA-SAM: Exponential Moving-average for SAM-based PTMC Segmentation</title>
      <link>http://arxiv.org/abs/2510.18213v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为EMA-SAM的新型模型，用于在甲状腺乳头状微小癌射频消融超声视频中实现更稳定的肿瘤分割。该模型通过在SAM-2基础上添加基于置信度的指数移动平均指针，解决了传统模型在介入超声应用中的不稳定预测和时间漂移问题。&lt;h4&gt;背景&lt;/h4&gt;甲状腺乳头状微小癌(PTMC)越来越多地使用射频消融(RFA)进行治疗，但在超声视频中准确分割病灶面临挑战，主要由于低对比度、探头引起的运动和热相关伪影等问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有Segment Anything Model 2 (SAM-2)在介入超声视频中的不稳定预测和时间漂移问题，实现更稳定和准确的肿瘤分割。&lt;h4&gt;方法&lt;/h4&gt;开发了EMA-SAM，这是SAM-2的轻量级扩展，在记忆库中融入了基于置信度的指数移动平均指针，提供跨帧的稳定肿瘤潜在原型，保持时间一致性的同时能快速适应新情况。&lt;h4&gt;主要发现&lt;/h4&gt;在PTMC-RFA数据集上，EMA-SAM将maxDice从0.82提高到0.86，maxIoU从0.72提高到0.76，同时减少29%的假阳性；在外部基准测试中比SAM-2提高2-5个Dice点；计算开销增加不到0.1%，保持约30FPS的实时性能。&lt;h4&gt;结论&lt;/h4&gt;EMA-SAM是一个稳健高效的框架，能够实现稳定的肿瘤跟踪，弥合了基础模型和介入超声严格需求之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;甲状腺乳头状微小癌(PTMC)越来越多地采用射频消融(RFA)治疗，然而由于低对比度、探头引起的运动和热相关伪影，在超声视频中准确分割病灶仍然困难。最近的Segment Anything Model 2 (SAM-2)在静态图像上表现良好，但其帧独立设计在介入超声中会导致不稳定预测和时间漂移。我们引入了EMA-SAM，这是SAM-2的一个轻量级扩展，在记忆库中融入了基于置信度的指数移动平均指针，提供跨帧的稳定肿瘤潜在原型。这种设计能够在探头压力和气泡遮挡时保持时间一致性，并在清晰证据重新出现时快速适应。在我们精心准备的PTMC-RFA数据集(124分钟，13名患者)上，EMA-SAM将maxDice从0.82(SAM-2)提高到0.86，maxIoU从0.72提高到0.76，同时减少29%的假阳性。在外部基准测试中，包括VTUS和结肠镜视频息肉数据集，EMA-SAM比SAM-2一致提高了2-5个Dice点。重要的是，EMA指针增加了不到0.1%的FLOPs，在单个A100 GPU上保持约30FPS的实时吞吐量。这些结果确立了EMA-SAM作为稳定肿瘤跟踪的稳健高效框架，弥合了基础模型和介入超声严格需求之间的差距。代码可在以下网址获取：https://github.com/mdialameh/EMA-SAM&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Papillary thyroid microcarcinoma (PTMC) is increasingly managed withradio-frequency ablation (RFA), yet accurate lesion segmentation in ultrasoundvideos remains difficult due to low contrast, probe-induced motion, andheat-related artifacts. The recent Segment Anything Model 2 (SAM-2) generalizeswell to static images, but its frame-independent design yields unstablepredictions and temporal drift in interventional ultrasound. We introduce\textbf{EMA-SAM}, a lightweight extension of SAM-2 that incorporates aconfidence-weighted exponential moving average pointer into the memory bank,providing a stable latent prototype of the tumour across frames. This designpreserves temporal coherence through probe pressure and bubble occlusion whilerapidly adapting once clear evidence reappears. On our curated PTMC-RFA dataset(124 minutes, 13 patients), EMA-SAM improves \emph{maxDice} from 0.82 (SAM-2)to 0.86 and \emph{maxIoU} from 0.72 to 0.76, while reducing false positives by29\%. On external benchmarks, including VTUS and colonoscopy video polypdatasets, EMA-SAM achieves consistent gains of 2--5 Dice points over SAM-2.Importantly, the EMA pointer adds \textless0.1\% FLOPs, preserving real-timethroughput of $\sim$30\,FPS on a single A100 GPU. These results establishEMA-SAM as a robust and efficient framework for stable tumour tracking,bridging the gap between foundation models and the stringent demands ofinterventional ultrasound. Codes are available here \hyperref[code{https://github.com/mdialameh/EMA-SAM}.</description>
      <author>example@mail.com (Maryam Dialameh, Hossein Rajabzadeh, Jung Suk Sim, Hyock Ju Kwon)</author>
      <guid isPermaLink="false">2510.18213v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>MACE Foundation Models for Lattice Dynamics: A Benchmark Study on Double Halide Perovskites</title>
      <link>http://arxiv.org/abs/2510.18178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;材料信息学和人工智能的发展催生了MACE基础模型，为无机固体带来通用势能突破。研究使用DFT数据库对四种MACE变体进行基准测试，发现模型准确性随训练数据增加而提高，能更好地预测弱非谐性材料的动态稳定性，主要误差来源是原子力预测中的误差放大。&lt;h4&gt;背景&lt;/h4&gt;材料信息学和人工智能的最新发展催生了材料化学的基础能量模型，以MACE系列基础模型为代表，为无机固体带来了通用势能的重大突破。&lt;h4&gt;目的&lt;/h4&gt;对计算材料科学中的方法开发进行性能基准测试，理解模型局限性，促进模型改进，推动材料理论发展。&lt;h4&gt;方法&lt;/h4&gt;使用作者发表的DFT数据库，包含约2000种立方卤化物双钙钛矿的室温动态稳定性和振动非谐性，对四种MACE基础模型变体进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;模型准确性随训练数据增加而提高；基础模型能更准确地再现弱非谐性材料的动态稳定性；预测动态稳定性的主要误差来自原子力预测误差的放大，而非构型空间采样的差异。&lt;h4&gt;结论&lt;/h4&gt;希望研究结果能激励未来工作朝着更多物理启发的方向发展，以评估基础模型在原子建模中的准确性。&lt;h4&gt;翻译&lt;/h4&gt;材料信息学和人工智能的最新发展催生了材料化学的基础能量模型，如MACE基础模型系列，为无机固体带来了通用势能的重大突破。对于计算材料科学中的所有方法开发，都需要针对特定应用与现有高级数据进行性能基准测试，以理解模型的局限性，从而促进模型开发过程的持续改进，有时会导致材料理论的重大概念飞跃。在此，我们使用自己发表的DFT（密度泛函理论）数据库，包含约2000种立方卤化物双钙钛矿的室温动态稳定性和振动非谐性，对四种不同变体的MACE基础模型进行了基准测试，评估其筛选无机固体动态稳定性的性能。我们的分析表明，正如预期，模型准确性随着更多训练数据的加入而提高。基础模型能更准确地再现弱非谐性材料的动态稳定性（由DFT预测），而非高度非谐性和动力学不稳定的材料。预测动态稳定性的主要误差来源在于预测谐波声子特性时通过计算Hessian矩阵放大原子力误差，而非DFT和基础模型在分子动力学中采样构型空间的差异。我们希望当前的研究发现将激励未来工作朝着更多物理启发的方向发展，以评估基础模型在原子建模中的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent developments in materials informatics and artificial intelligence hasled to the emergence of foundational energy models for material chemistry, asrepresented by the suite of MACE-based foundation models, bringing asignificant breakthrough in universal potentials for inorganic solids. As toall method developments in computational materials science, performancebenchmarking against existing high-level data with focusing on specificapplications, is critically needed to understand the limitations in the models,thus facilitating the ongoing improvements in the model development process,and occasionally, leading to significant conceptual leaps in materials theory.Here, using our own published DFT (Density Functional Theory) database ofroom-temperature dynamic stability and vibrational anharmonicity for $\sim2000$cubic halide double perovskites, we benchmarked the performances of fourdifferent variants of the MACE foundation models for screening the dynamicstabilities of inorganic solids. Our analysis shows that, as anticipated, themodel accuracy improves with more training data. The dynamic stabilities ofweakly anharmonic materials (as predicted by DFT) are more accuratelyreproduced by the foundation model, than those highly anharmonic anddynamically unstable ones. The predominant source of error in predicting thedynamic stability arises predominantly from the amplification of errors inatomic forces when predicting the harmonic phonon properties through thecomputation of the Hessian matrix, less so is the contribution from possibledifferences in the range of the configurational spaces that are sampled by DFTand the foundation model in molecular dynamics. We hope that our presentfindings will stimulate future works towards more physics-inspired approachesin assessing the accuracy of foundation models for atomistic modelling.</description>
      <author>example@mail.com (Jack Yang, Ziqi Yin, Lei Ao, Sean Li)</author>
      <guid isPermaLink="false">2510.18178v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>MEG-GPT: A transformer-based foundation model for magnetoencephalography data</title>
      <link>http://arxiv.org/abs/2510.18080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了MEG-GPT，一种基于transformer的基础模型，用于处理脑磁图数据。通过引入新颖的数据驱动标记化器和在大规模数据集上训练，该模型能生成具有真实脑电特性的数据，并在下游解码任务中表现出色，特别是在跨会话和跨受试者场景中。&lt;h4&gt;背景&lt;/h4&gt;神经科学领域需要建模大规模脑动力学的复杂时空模式，但传统方法无法捕捉脑磁图等模态中的丰富结构。同时，深度学习通过大规模基础模型在语言和视觉等领域已取得显著进展。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于transformer的基础模型MEG-GPT，用于处理脑磁图数据，解决传统方法在处理脑电生理数据方面的局限性，并探索其在计算神经科学和神经解码中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;提出MEG-GPT模型，使用时间注意力和下一时间点预测；引入数据驱动的标记化器处理连续MEG数据，保持高时间分辨率；在大规模MEG数据集上训练模型，包含612名闭眼休息状态的受试者数据；使用标记化的大脑区域时间序列进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;模型能生成具有真实时空频谱特性的数据，包括瞬态事件和群体变异性；在下游解码任务中表现良好，改善了监督预测任务；在跨会话准确率从0.54提高到0.59，跨受试者准确率从0.41提高到0.49；模型可在小标记数据集上高效微调，提升跨受试者解码性能。&lt;h4&gt;结论&lt;/h4&gt;该研究为电生理数据建立了一个强大的基础模型，为计算神经科学和神经解码应用铺平了道路，展示了基础模型在神经科学领域的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;建模大规模脑动力学的复杂时空模式对神经科学至关重要，但传统方法无法捕捉脑磁图等模态中的丰富结构。深度学习的最新进展通过大规模基础模型在语言和视觉等领域实现了显著进步。在此，我们介绍了MEG-GPT，一个基于transformer的基础模型，使用时间注意力和下一时间点预测。为此，我们还引入了一种新颖的数据驱动标记化器用于连续MEG数据，它保留了连续MEG信号的高时间分辨率而无需有损变换。我们在从大规模MEG数据集提取的标记化大脑区域时间序列上训练MEG-GPT，并表明学习到的模型能够生成具有真实时空频谱特性的数据，包括瞬态事件和群体变异性。关键的是，它在下游解码任务中表现良好，改善了下游监督预测任务，在跨会话和跨受试者方面显示出改进的零样本泛化能力。此外，我们表明模型可以在较小的标记数据集上高效微调，以提高跨受试者解码场景中的性能。这项工作为电生理数据建立了一个强大的基础模型，为计算神经科学和神经解码应用铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modelling the complex spatiotemporal patterns of large-scale brain dynamicsis crucial for neuroscience, but traditional methods fail to capture the richstructure in modalities such as magnetoencephalography (MEG). Recent advancesin deep learning have enabled significant progress in other domains, such aslanguage and vision, by using foundation models at scale. Here, we introduceMEG-GPT, a transformer based foundation model that uses time-attention and nexttime-point prediction. To facilitate this, we also introduce a noveldata-driven tokeniser for continuous MEG data, which preserves the hightemporal resolution of continuous MEG signals without lossy transformations. Wetrained MEG-GPT on tokenised brain region time-courses extracted from alarge-scale MEG dataset (N=612, eyes-closed rest, Cam-CAN data), and show thatthe learnt model can generate data with realistic spatio-spectral properties,including transient events and population variability. Critically, it performswell in downstream decoding tasks, improving downstream supervised predictiontask, showing improved zero-shot generalisation across sessions (improvingaccuracy from 0.54 to 0.59) and subjects (improving accuracy from 0.41 to 0.49)compared to a baseline methods. Furthermore, we show the model can beefficiently fine-tuned on a smaller labelled dataset to boost performance incross-subject decoding scenarios. This work establishes a powerful foundationmodel for electrophysiological data, paving the way for applications incomputational neuroscience and neural decoding.</description>
      <author>example@mail.com (Rukuang Huang, Sungjun Cho, Chetan Gohil, Oiwi Parker Jones, Mark Woolrich)</author>
      <guid isPermaLink="false">2510.18080v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity</title>
      <link>http://arxiv.org/abs/2510.18037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 39th Conference on Neural Information Processing  Systems (NeurIPS 2025) Workshop: Data on the Brain &amp; Mind&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究系统评估了深度学习模型在神经活动预测中的性能，发现深度学习模型优于传统方法，最佳模型可预测未来1.5秒的神经活动，为神经系统的理解和控制提供了新途径。&lt;h4&gt;背景&lt;/h4&gt;神经活动预测对于理解神经系统和实现闭环控制至关重要。虽然深度学习最近在时间序列预测领域取得了最先进进展，但其在神经活动预测中的应用仍然有限。&lt;h4&gt;目的&lt;/h4&gt;弥合深度学习在神经活动预测中应用的差距，系统评估多种深度学习模型在神经活动预测中的性能。&lt;h4&gt;方法&lt;/h4&gt;系统评估了8种概率深度学习模型（包括2种基础模型），将其与4种经典统计模型和2种基线方法进行比较，使用宽场成像技术记录的小鼠皮层自发性神经活动作为数据，在不同的预测时间范围内进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;在各种预测时间范围内，几种深度学习模型持续优于经典方法，最佳模型能够提供未来1.5秒内有信息量的预测。&lt;h4&gt;结论&lt;/h4&gt;研究结果指向未来的控制应用，为探索神经活动的内在时间结构开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;神经活动预测对于理解神经系统和实现闭环控制至关重要。虽然深度学习最近在时间序列预测文献中取得了最先进进展，但其在神经活动预测中的应用仍然有限。为了弥合这一差距，我们系统评估了八种概率深度学习模型（包括两种基础模型），这些模型在通用预测基准测试中表现出色。我们在通过宽场成像记录的小鼠皮层自发性神经活动上，将这些模型与四种经典统计模型和两种基线方法进行了比较。在各种预测时间范围内，几种深度学习模型持续优于经典方法，其中最佳模型能够提供未来1.5秒内有信息量的预测。我们的研究结果指向未来的控制应用，并为探索神经活动的内在时间结构开辟了新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural activity forecasting is central to understanding neural systems andenabling closed-loop control. While deep learning has recently advanced thestate-of-the-art in the time series forecasting literature, its application toneural activity forecasting remains limited. To bridge this gap, wesystematically evaluated eight probabilistic deep learning models, includingtwo foundation models, that have demonstrated strong performance on generalforecasting benchmarks. We compared them against four classical statisticalmodels and two baseline methods on spontaneous neural activity recorded frommouse cortex via widefield imaging. Across prediction horizons, several deeplearning models consistently outperformed classical approaches, with the bestmodel producing informative forecasts up to 1.5 seconds into the future. Ourfindings point toward future control applications and open new avenues forprobing the intrinsic temporal structure of neural activity.</description>
      <author>example@mail.com (Ziyu Lu, Anna J. Li, Alexander E. Ladd, Pascha Matveev, Aditya Deole, Eric Shea-Brown, J. Nathan Kutz, Nicholas A. Steinmetz)</author>
      <guid isPermaLink="false">2510.18037v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>AION-1: Omnimodal Foundation Model for Astronomical Sciences</title>
      <link>http://arxiv.org/abs/2510.17960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at Neural Information Processing Systems (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AION-1，一个天文领域的大规模多模态基础模型家族，能够整合异构的成像、光谱和标量数据，并在多种天文任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;基础模型已在多个领域展现出潜力，但天文学仍缺乏一个统一框架来对其多样化的数据模态进行联合建模。&lt;h4&gt;目的&lt;/h4&gt;开发一个天文领域的大规模多模态基础模型，能够处理天文学中各种不同的数据类型。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段架构：首先进行模态特定的标记化，然后使用基于transformer的跨模态标记序列掩码建模。模型在五个大规模调查数据集上进行预训练，包括Legacy Survey、HSC、SDSS、DESI和Gaia，覆盖超过2亿个天体观测。&lt;h4&gt;主要发现&lt;/h4&gt;使用单个冻结编码器，AION-1在多种下游任务上取得优异表现，包括星系和恒星属性估计、星系形态分类、基于相似性的检索、星系图像分割和光谱超分辨率。&lt;h4&gt;结论&lt;/h4&gt;AION-1提供了构建多模态科学基础模型的可扩展蓝图，能够无缝集成嘈杂的、仪器特定的观测数据。研究团队发布了参数量从3亿到31亿不等的模型变体，并开源了所有代码、标记器、预训练权重和评估套件。&lt;h4&gt;翻译&lt;/h4&gt;尽管基础模型已在各种领域展现出前景，天文学仍然缺乏一个统一的框架来对其高度多样化的数据模态进行联合建模。在本文中，我们提出了AION-1，一个用于天文的大规模多模态基础模型家族。AION-1使用两阶段架构整合异构的成像、光谱和标量数据：模态特定的标记化，随后是基于transformer的跨模态标记序列掩码建模。该模型在五个大规模调查数据集上进行预训练：Legacy Survey、Hyper Suprime-Cam (HSC)、Sloan Digital Sky Survey (SDSS)、Dark Energy Spectroscopic Instrument (DESI)和Gaia。这些数据集涵盖了超过2亿颗恒星、星系和类星体的观测。使用单个冻结编码器，AION-1在广泛的下游任务上取得了强劲结果，包括星系和恒星属性估计、星系形态分类、基于相似性的检索、星系图像分割和光谱超分辨率。我们发布了参数量从3亿到31亿不等的AION-1模型变体。除天文学外，AION-1为多模态科学基础模型提供了可扩展的蓝图，能够无缝集成嘈杂的、仪器特定的观测。所有代码、标记器、预训练权重和轻量级评估套件均在开源许可下发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While foundation models have shown promise across a variety of fields,astronomy still lacks a unified framework for joint modeling across its highlydiverse data modalities. In this paper, we present AION-1, a family oflarge-scale multimodal foundation models for astronomy. AION-1 integratesheterogeneous imaging, spectroscopic, and scalar data using a two-stagearchitecture: modality-specific tokenization followed by transformer-basedmasked modeling of cross-modal token sequences. The model is pretrained on fivelarge-scale surveys: Legacy Survey, Hyper Suprime-Cam (HSC), Sloan Digital SkySurvey (SDSS), Dark Energy Spectroscopic Instrument (DESI), and Gaia. Thesespan more than 200 million observations of stars, galaxies, and quasars. With asingle frozen encoder, AION-1 achieves strong results on a broad suite ofdownstream tasks, including galaxy and stellar property estimation, galaxymorphology classification, similarity-based retrieval, galaxy imagesegmentation, and spectral super-resolution. We release AION-1 model variantsranging from 300 M to 3.1 B parameters. Beyond astronomy, AION-1 provides ascalable blueprint for multimodal scientific foundation models that canseamlessly integrate noisy, instrument-specific observations. All code,tokenizers, pretrained weights, and a lightweight evaluation suite are releasedunder an open-source license.</description>
      <author>example@mail.com (Liam Parker, Francois Lanusse, Jeff Shen, Ollie Liu, Tom Hehir, Leopoldo Sarra, Lucas Meyer, Micah Bowles, Sebastian Wagner-Carena, Helen Qu, Siavash Golkar, Alberto Bietti, Hatim Bourfoune, Nathan Casserau, Pierre Cornette, Keiya Hirashima, Geraud Krawezik, Ruben Ohana, Nicholas Lourie, Michael McCabe, Rudy Morel, Payel Mukhopadhyay, Mariel Pettee, Bruno Regaldo-Saint Blancard, Kyunghyun Cho, Miles Cranmer, Shirley Ho)</author>
      <guid isPermaLink="false">2510.17960v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Trust in foundation models and GenAI: A geographic perspective</title>
      <link>http://arxiv.org/abs/2510.17942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基础模型（特别是地理学领域）中的信任概念，将信任分为三类：对训练数据的认知信任、对模型功能的操作信任以及对模型开发者的人际信任。论文讨论了信任在地理应用中的含义、挑战、偏见问题、透明度和可解释性的重要性，以及地理信息科学家的独特视角。&lt;h4&gt;背景&lt;/h4&gt;大型预训练机器学习模型已经改变了多个领域对人工智能的理解，包括地理学领域。随着这些模型被越来越多地依赖并用于关键决策，信任已成为讨论中的重要议题，但同时也变得复杂且多方面。&lt;h4&gt;目的&lt;/h4&gt;论文旨在提供一个概念起点，帮助研究人员、从业者和政策制定者更好地理解（生成性）地理人工智能中的信任问题。&lt;h4&gt;方法&lt;/h4&gt;作者将信任概念分为三个类型进行分析：认知信任、操作信任和人际信任，并探讨这些信任类型在地理应用中的独特含义。&lt;h4&gt;主要发现&lt;/h4&gt;信任在基础模型中是一个多方面的概念；信任可分为三种类型：对训练数据的认知信任、对模型功能的操作信任以及对模型开发者的人际信任；文化背景、数据异质性和空间关系等主题对空间科学至关重要，并在发展信任中起重要作用；不同形式的偏见带来了挑战；透明度和可解释性很重要；模型开发中存在伦理责任；地理信息科学家提供了新的视角，呼吁进一步提高透明度、减少偏见并制定区域知情政策。&lt;h4&gt;结论&lt;/h4&gt;随着对基础模型依赖的增加，信任已成为一个复杂但至关重要的概念。通过将信任分类并考虑地理学特有的因素，论文为理解和建立对地理人工智能的信任提供了概念框架。&lt;h4&gt;翻译&lt;/h4&gt;大型预训练机器学习模型已经重塑了我们对多个领域人工智能的理解，包括我们自己的地理学领域。与任何新技术一样，信任在这一讨论中扮演着重要角色。在本章中，我们探讨了基础模型中信任的多方面概念，特别是在地理背景下。随着对这些模型的依赖增加并用于关键决策，信任虽然必不可少，但已成为一个分裂的概念。在这里，我们将信任分为三类：对训练数据的认知信任、对模型功能的操作信任以及对模型开发者的人际信任。每种信任类型都为地理应用带来了独特的含义。文化背景、数据异质性和空间关系等主题是空间科学的基础，并在发展信任中发挥重要作用。本章继续讨论了不同形式偏见带来的挑战、透明度和可解释性的重要性以及模型开发中的伦理责任。最后，强调了地理信息科学家的新颖视角，呼吁进一步提高透明度、减少偏见并制定区域知情政策。简而言之，本章旨在为研究人员、从业者和政策制定者提供一个概念起点，以更好地理解（生成性）地理人工智能中的信任。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale pre-trained machine learning models have reshaped ourunderstanding of artificial intelligence across numerous domains, including ourown field of geography. As with any new technology, trust has taken on animportant role in this discussion. In this chapter, we examine the multifacetedconcept of trust in foundation models, particularly within a geographiccontext. As reliance on these models increases and they become relied upon forcritical decision-making, trust, while essential, has become a fracturedconcept. Here we categorize trust into three types: epistemic trust in thetraining data, operational trust in the model's functionality, andinterpersonal trust in the model developers. Each type of trust brings with itunique implications for geographic applications. Topics such as culturalcontext, data heterogeneity, and spatial relationships are fundamental to thespatial sciences and play an important role in developing trust. The chaptercontinues with a discussion of the challenges posed by different forms ofbiases, the importance of transparency and explainability, and ethicalresponsibilities in model development. Finally, the novel perspective ofgeographic information scientists is emphasized with a call for furthertransparency, bias mitigation, and regionally-informed policies. Simply put,this chapter aims to provide a conceptual starting point for researchers,practitioners, and policy-makers to better understand trust in (generative)GeoAI.</description>
      <author>example@mail.com (Grant McKenzie, Krzysztof Janowicz, Carsten Kessler)</author>
      <guid isPermaLink="false">2510.17942v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Robustness Verification of Graph Neural Networks Via Lightweight Satisfiability Testing</title>
      <link>http://arxiv.org/abs/2510.18591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图神经网络（GNNs）是学习图结构的主导架构，对抗攻击检测是一个重要问题。作者提出使用高效部分求解器替代传统强大求解器的方法，以提高结构鲁棒性，并在多种GNN变体和数据集上评估了其工具RobLight。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）是学习图结构的主导架构。与任何机器学习模型一样，检测对抗性攻击（对手通过小幅扰动输入来改变输出）是一个重要问题。解决对抗鲁棒性问题（确定是否存在此类攻击）的技术最初是为图像分类开发的，但也有适用于其他机器学习架构的变体。&lt;h4&gt;目的&lt;/h4&gt;提高图神经网络结构鲁棒性的最先进水平，通过替代传统方法中使用强大求解器的做法。&lt;h4&gt;方法&lt;/h4&gt;用高效的部分求解器（运行时间为多项式时间但不一定完整）替换强大求解器的使用，开发工具RobLight，并在多种GNN变体和数据集上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;可以通过使用高效的部分求解器替代强大求解器，来改进结构鲁棒性的最先进技术。&lt;h4&gt;结论&lt;/h4&gt;作者的工具RobLight在多种GNN变体和数据集上进行了评估，表明该方法在对抗攻击检测方面具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNNs）是学习图结构的主导架构。与任何机器学习模型一样，一个重要问题是检测对抗性攻击，即对手可以通过对输入的小幅扰动来改变输出。解决对抗鲁棒性问题（确定是否存在此类攻击）的技术最初是为图像分类开发的，但也有适用于许多其他机器学习架构的变体。在图学习的情况下，攻击模型通常考虑对图结构的更改，而不仅仅是或代替输入的数值特征，该领域最先进的技术通过简化为约束求解来实现，基于强大的求解器（例如用于混合整数编程的求解器）。我们展示了可以通过用高效的部分求解器（运行时间为多项式时间但不一定完整）替换强大求解器的使用，来提高结构鲁棒性的最先进水平。我们在多种GNN变体和数据集上评估了我们的工具RobLight。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are the predominant architecture for learningover graphs. As with any machine learning model, and important issue is thedetection of adversarial attacks, where an adversary can change the output witha small perturbation of the input. Techniques for solving the adversarialrobustness problem - determining whether such an attack exists - wereoriginally developed for image classification, but there are variants for manyother machine learning architectures. In the case of graph learning, the attackmodel usually considers changes to the graph structure in addition to orinstead of the numerical features of the input, and the state of the arttechniques in the area proceed via reduction to constraint solving, working ontop of powerful solvers, e.g. for mixed integer programming. We show that it ispossible to improve on the state of the art in structural robustness byreplacing the use of powerful solvers by calls to efficient partial solvers,which run in polynomial time but may be incomplete. We evaluate our toolRobLight on a diverse set of GNN variants and datasets.</description>
      <author>example@mail.com (Chia-Hsuan Lu, Tony Tan, Michael Benedikt)</author>
      <guid isPermaLink="false">2510.18591v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Fairness-aware Graph Neural Networks in Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2510.18473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了知识图谱上的公平感知图神经网络(GNNs)基准研究，从YAGO、DBpedia和Wikidata生成更大规模的数据集，评估不同GNN主干和早期停止条件下的预处理和内处理方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是学习图结构数据的强大工具，但通常对敏感属性产生有偏见的预测。尽管公平感知的GNNs已被研究用于减轻这种偏见，但之前的研究未在知识图谱这一重要应用领域进行评估。&lt;h4&gt;目的&lt;/h4&gt;评估公平感知的GNNs在知识图谱上的表现，并建立相关基准。&lt;h4&gt;方法&lt;/h4&gt;从三个知识图谱(YAGO、DBpedia和Wikidata)生成新的更大规模图数据集，在不同GNN主干和早期停止条件下对预处理和内处理方法进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;(i)知识图谱与现有数据集表现出不同趋势，在公平性与准确性间有更清晰的权衡；(ii)性能不仅受公平感知GNN方法影响，还受GNN主干和早期停止条件显著影响；(iii)预处理方法改善公平性指标，内处理方法提高预测准确性。&lt;h4&gt;结论&lt;/h4&gt;知识图谱上的公平性研究需要综合考虑数据集特性、模型架构和训练方法的选择，这些因素共同影响公平性与准确性的权衡。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)是学习图结构数据的强大工具，但通常对敏感属性产生有偏见的预测。公平感知的GNNs已被积极研究用于减轻有偏见的预测。然而，之前的研究没有在知识图谱上评估公平感知的GNNs，而知识图谱是许多应用（如推荐系统）中最重要的图之一。因此，我们引入一个关于知识图谱的基准研究。我们从三个知识图谱（YAGO、DBpedia和Wikidata）生成新的图，这些图比公平性研究中使用的现有图数据集大得多。我们在不同的GNN主干和早期停止条件下对预处理和内处理方法进行基准测试。我们发现几个关键见解：(i)知识图谱显示出与现有数据集不同的趋势；在公平感知的GNNs中，与其他图相比，预测准确性和公平性指标之间有更清晰的权衡，(ii)性能不仅受到公平感知的GNN方法的影响，还受到GNN主干和早期停止条件的显著影响，以及(iii)预处理方法通常改善公平性指标，而内处理方法提高预测准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are powerful tools for learning fromgraph-structured data but often produce biased predictions with respect tosensitive attributes. Fairness-aware GNNs have been actively studied formitigating biased predictions. However, no prior studies have evaluatedfairness-aware GNNs on knowledge graphs, which are one of the most importantgraphs in many applications, such as recommender systems. Therefore, weintroduce a benchmarking study on knowledge graphs. We generate new graphs fromthree knowledge graphs, YAGO, DBpedia, and Wikidata, that are significantlylarger than the existing graph datasets used in fairness studies. We benchmarkinprocessing and preprocessing methods in different GNN backbones and earlystopping conditions. We find several key insights: (i) knowledge graphs showdifferent trends from existing datasets; clearer trade-offs between predictionaccuracy and fairness metrics than other graphs in fairness-aware GNNs, (ii)the performance is largely affected by not only fairness-aware GNN methods butalso GNN backbones and early stopping conditions, and (iii) preprocessingmethods often improve fairness metrics, while inprocessing methods improveprediction accuracy.</description>
      <author>example@mail.com (Yuya Sasaki)</author>
      <guid isPermaLink="false">2510.18473v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Training Diverse Graph Experts for Ensembles: A Systematic Empirical Study</title>
      <link>http://arxiv.org/abs/2510.18370v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究首次对图神经网络集成的专家级多样化技术进行了系统性实证研究，评估了20种多样化策略在14个节点分类基准上的表现，构建并分析了200多个集成变体，为专家训练和图数据上有效混合专家框架的设计提供了指导。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为学习关系数据的重要工具，但单一图神经网络在处理现实世界图中存在的异质性时性能受限。近期混合专家框架的进展表明，组合多个具有明显不同泛化模式的多样化图神经网络可以显著提高性能。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在对图神经网络集成的专家级多样化技术进行首个系统性实证研究，评估不同多样化策略的效果，并提供训练最大化多样化专家的机制见解。&lt;h4&gt;方法&lt;/h4&gt;研究评估了20种多样化策略，包括随机重新初始化、超参数调整、架构变化、方向性建模和训练数据分区等，在14个节点分类基准上构建并分析了200多个集成变体，从专家多样性、互补性和集成性能等方面全面评估了每种技术。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了训练最大化多样化专家的机制见解，发现不同多样化策略在产生专家多样性、互补性和提升集成性能方面有不同效果，为专家训练和有效混合专家框架设计提供了可操作的指导。&lt;h4&gt;结论&lt;/h4&gt;该研究通过大规模实证分析，为图神经网络集成的专家级多样化技术提供了系统性理解和实用指导，有助于开发更高效的混合专家框架来处理图数据中的异质性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为学习关系数据的重要工具，然而单一图神经网络的性能往往受到现实世界图中存在的异质性的限制。混合专家框架的最新进展表明，组合多个具有明显不同泛化模式的多样化图神经网络可以显著提高性能。在这项工作中，我们首次对图神经网络集成的专家级多样化技术进行了系统性实证研究。通过在14个节点分类基准上评估20种多样化策略——包括随机重新初始化、超参数调整、架构变化、方向性建模和训练数据分区等，我们构建并分析了200多个集成变体。我们的全面评估从专家多样性、互补性和集成性能等方面检验了每种技术。我们还揭示了训练最大化多样化专家的机制见解。这些发现为图数据上的专家训练和有效混合专家框架的设计提供了可操作的指导。我们的代码可在指定链接获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become essential tools for learning onrelational data, yet the performance of a single GNN is often limited by theheterogeneity present in real-world graphs. Recent advances inMixture-of-Experts (MoE) frameworks demonstrate that assembling multiple,explicitly diverse GNNs with distinct generalization patterns can significantlyimprove performance. In this work, we present the first systematic empiricalstudy of expert-level diversification techniques for GNN ensembles. Evaluating20 diversification strategies -- including random re-initialization,hyperparameter tuning, architectural variation, directionality modeling, andtraining data partitioning -- across 14 node classification benchmarks, weconstruct and analyze over 200 ensemble variants. Our comprehensive evaluationexamines each technique in terms of expert diversity, complementarity, andensemble performance. We also uncovers mechanistic insights into trainingmaximally diverse experts. These findings provide actionable guidance forexpert training and the design of effective MoE frameworks on graph data. Ourcode is available at https://github.com/Hydrapse/bench-gnn-diversification.</description>
      <author>example@mail.com (Gangda Deng, Yuxin Yang, Ömer Faruk Akgül, Hanqing Zeng, Yinglong Xia, Rajgopal Kannan, Viktor Prasanna)</author>
      <guid isPermaLink="false">2510.18370v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Committors without Descriptors</title>
      <link>http://arxiv.org/abs/2510.18018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合图神经网络的改进版基于committor的方法，用于原子模拟中的稀有事件研究。&lt;h4&gt;背景&lt;/h4&gt;稀有事件的研究是原子模拟中的主要挑战之一，已提出几种增强采样方法。最近建议使用committor来提供稀有事件的精确形式描述。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于committor的方法，促进系统亚稳态之间的频繁转换，并允许对过程过渡态集合进行广泛采样。&lt;h4&gt;方法&lt;/h4&gt;利用变分标准迭代优化基于神经网络的committor参数化，使用一组物理描述符作为输入，方法具有自洽和半自动的优势。&lt;h4&gt;主要发现&lt;/h4&gt;将之前的方法与图神经网络结合，可以直接处理原子坐标而不是描述符，进一步自动化该过程。&lt;h4&gt;结论&lt;/h4&gt;结合图神经网络增强了方法的能力，特别是在处理原子坐标和描述溶剂分子的作用方面，如离子对解离或配体结合。&lt;h4&gt;翻译&lt;/h4&gt;The study of rare events is one of the major challenges in atomistic simulations, and several enhanced sampling methods towards its solution have been proposed. Recently, it has been suggested that the use of the committor, which provides a precise formal description of rare events, could be of use in this context. We have recently followed up on this suggestion and proposed a committor-based method that promotes frequent transitions between the metastable states of the system and allows extensive sampling of the process transition state ensemble. One of the strengths of our approach is being self-consistent and semi-automatic, exploiting a variational criterion to iteratively optimize a neural-network-based parametrization of the committor, which uses a set of physical descriptors as input. Here, we further automate this procedure by combining our previous method with the expressive power of graph neural networks, which can directly process atomic coordinates rather than descriptors. Besides applications on benchmark systems, we highlight the advantages of a graph-based approach in describing the role of solvent molecules in systems, such as ion pair dissociation or ligand binding.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The study of rare events is one of the major challenges in atomisticsimulations, and several enhanced sampling methods towards its solution havebeen proposed. Recently, it has been suggested that the use of the committor,which provides a precise formal description of rare events, could be of use inthis context. We have recently followed up on this suggestion and proposed acommittor-based method that promotes frequent transitions between themetastable states of the system and allows extensive sampling of the processtransition state ensemble. One of the strengths of our approach is beingself-consistent and semi-automatic, exploiting a variational criterion toiteratively optimize a neural-network-based parametrization of the committor,which uses a set of physical descriptors as input. Here, we further automatethis procedure by combining our previous method with the expressive power ofgraph neural networks, which can directly process atomic coordinates ratherthan descriptors. Besides applications on benchmark systems, we highlight theadvantages of a graph-based approach in describing the role of solventmolecules in systems, such as ion pair dissociation or ligand binding.</description>
      <author>example@mail.com (Peilin Kang, Jintu Zhang, Enrico Trizio, TingJun Hou, Michele Parrinello)</author>
      <guid isPermaLink="false">2510.18018v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>QINNs: Quantum-Informed Neural Networks</title>
      <link>http://arxiv.org/abs/2510.17984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出量子信息神经网络(QINNs)框架，将量子信息概念引入经典模型，通过量子费舍尔信息矩阵(QFIM)作为粒子关联的紧凑表示，在喷注标记任务中提高模型性能，使粒子碰撞分析更加实用、可解释和可扩展。&lt;h4&gt;背景&lt;/h4&gt;经典深度神经网络能够学习强子对撞机数据中的丰富多粒子关联，但其归纳偏差很少基于物理结构。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用框架，将量子信息概念和量子可观测量引入纯经典模型中，增强粒子碰撞分析的量子信息处理能力。&lt;h4&gt;方法&lt;/h4&gt;研究QINNs的一个具体实现，将每个粒子编码为量子比特，使用量子费舍尔信息矩阵(QFIM)作为粒子关联的紧凑、基无关摘要，在图神经网络中将QFIM用作轻量级嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;QFIM能够区分QCD和强子顶喷注，显示出符合物理预期的不同模式，表明QINNs能够捕捉有意义的物理特征。&lt;h4&gt;结论&lt;/h4&gt;QINNs为粒子碰撞的量子信息分析(断层扫描)提供了一种实用、可解释和可扩展的途径，特别是通过增强现有的深度学习方法。&lt;h4&gt;翻译&lt;/h4&gt;经典深度神经网络可以学习强子对撞机数据中的丰富多粒子关联，但它们的归纳偏差很少锚定在物理结构上。我们提出了量子信息神经网络(QINNs)，这是一个通用框架，将量子信息概念和量子可观测量引入纯经典模型。虽然该框架很广泛，但在本文中，我们研究了一个具体实现，将每个粒子编码为量子比特，并使用量子费舍尔信息矩阵(QFIM)作为粒子关联的紧凑、基无关摘要。以喷注标记为案例研究，QFIM在图神经网络中充当轻量级嵌入，提高了模型的表型和可塑性。QFIM揭示了QCD和强子顶喷注的不同模式，这些模式符合物理预期。因此，QINNs为粒子碰撞的量子信息分析(即断层扫描)提供了一条实用、可解释和可扩展的途径，特别是通过增强成熟的深度学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classical deep neural networks can learn rich multi-particle correlations incollider data, but their inductive biases are rarely anchored in physicsstructure. We propose quantum-informed neural networks (QINNs), a generalframework that brings quantum information concepts and quantum observables intopurely classical models. While the framework is broad, in this paper, we studyone concrete realisation that encodes each particle as a qubit and uses theQuantum Fisher Information Matrix (QFIM) as a compact, basis-independentsummary of particle correlations. Using jet tagging as a case study, QFIMs actas lightweight embeddings in graph neural networks, increasing modelexpressivity and plasticity. The QFIM reveals distinct patterns for QCD andhadronic top jets that align with physical expectations. Thus, QINNs offer apractical, interpretable, and scalable route to quantum-informed analyses, thatis, tomography, of particle collisions, particularly by enhancingwell-established deep learning approaches.</description>
      <author>example@mail.com (Aritra Bal, Markus Klute, Benedikt Maier, Melik Oughton, Eric Pezone, Michael Spannowsky)</author>
      <guid isPermaLink="false">2510.17984v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>SemiAdapt and SemiLoRA: Efficient Domain Adaptation for Transformer-based Low-Resource Language Translation with a Case Study on Irish</title>
      <link>http://arxiv.org/abs/2510.18725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了SemiAdapt和SemiLoRA两种半监督推理高效方法，用于加强神经机器翻译中的领域适应，提高整体性能，特别是在低资源语言如爱尔兰语翻译方面。&lt;h4&gt;背景&lt;/h4&gt;微调被广泛用于定制大语言模型执行特定任务，但大型多语言模型的微调计算成本高，为研究低资源领域的研究人员设置了障碍。&lt;h4&gt;目的&lt;/h4&gt;解决低资源语言领域中高质量领域适应和微调的可访问性问题，使研究人员更容易进行这些工作。&lt;h4&gt;方法&lt;/h4&gt;介绍SemiAdapt和SemiLoRA作为半监督推理高效方法；利用参数高效微调和低秩适应技术；评估按数据集进行领域微调；开发基于嵌入的推理方法。&lt;h4&gt;主要发现&lt;/h4&gt;SemiAdapt可以优于全领域微调；SemiLoRA可以使参数高效微调方法匹配甚至超过全模型微调的性能；基于嵌入的推理方法在更大和更嘈杂的语料库上表现特别好。&lt;h4&gt;结论&lt;/h4&gt;这些方法使高质量领域适应和微调更容易被低资源语言研究人员获取，所有爱尔兰语翻译模型都作为开放资源发布。&lt;h4&gt;翻译&lt;/h4&gt;本研究特别关注爱尔兰语翻译，开发的爱尔兰语翻译模型已作为开放资源发布，旨在促进低资源语言的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning is widely used to tailor large language models for specific taskssuch as neural machine translation (NMT). However, leveraging transfer learningis computationally expensive when fine-tuning large multilingual models withbillions of parameters, thus creating a barrier to entry for researchersworking on low-resource domains such as Irish translation. Parameter-efficientfine-tuning (PEFT) bridges this gap by training on a fraction of the originalmodel parameters, with the Low-Rank Adaptation (LoRA) approach introducingsmall, trainable adapter layers. We introduce SemiAdapt and SemiLoRA assemi-supervised inference-efficient approaches that strengthen domainadaptation and lead to improved overall performance in NMT. We demonstrate thatSemiAdapt can outperform full-domain fine-tuning, while most notably, SemiLoRAcan propel PEFT methods to match or even outperform full-model fine-tuning. Wefurther evaluate domain-by-dataset fine-tuning and demonstrate that ourembedding-based inference methods perform especially well on larger and noisiercorpora. All Irish translation models developed in this work are released asopen resources. These methods aim to make high-quality domain adaptation andfine-tuning more accessible to researchers working with low-resource languages.</description>
      <author>example@mail.com (Josh McGiff, Nikola S. Nikolov)</author>
      <guid isPermaLink="false">2510.18725v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Automated Wicket-Taking Delivery Segmentation and Weakness Detection in Cricket Videos Using OCR-Guided YOLOv8 and Trajectory Modeling</title>
      <link>http://arxiv.org/abs/2510.18405v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 figures, 5 tables, submitted to the 11th IEEE International Women  in Engineering (WIE) Conference on Electrical and Computer Engineering 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种用于板球视频分析的自动化系统，该系统利用深度学习技术来提取导致球门被取的投球、检测板球并建立球轨迹模型。系统使用YOLOv8架构进行场地和球检测，结合光学字符识别技术提取记分卡信息来识别球门被取的时刻。通过全面的图像预处理，系统实现了从视频帧中稳健的文本提取。场地检测模型达到高精确度，球检测模型使用迁移学习也表现出色。该系统可以在检测到的场地上进行轨迹建模，为识别击球弱点提供数据驱动的洞察。&lt;h4&gt;背景&lt;/h4&gt;板球比赛产生大量视频数据，人工分析这些数据以提取战术信息是一个耗时且复杂的过程。随着深度学习技术的发展，自动分析板球视频以提取关键战术信息成为可能，这可以为教练团队和战略决策提供数据支持。&lt;h4&gt;目的&lt;/h4&gt;开发一个自动化系统，用于分析板球视频，提取关键信息（如导致球门被取的投球、球的检测和轨迹建模），从而为教练团队提供数据驱动的战术洞察，帮助识别击球弱点和改进比赛策略。&lt;h4&gt;方法&lt;/h4&gt;使用YOLOv8架构进行场地和球检测；结合光学字符识别技术提取记分卡信息；应用图像预处理技术，包括灰度转换、幂变换和形态学操作；使用迁移学习技术改进球检测模型；在检测到的场地上进行轨迹建模；在多个板球比赛视频上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;场地检测模型达到99.5%的平均精度均值和0.999的精确度；球检测模型使用迁移学习达到99.18%的平均精度均值、0.968的精确度和0.978的召回率；系统能够有效识别导致球门被取的时刻；轨迹建模能够提供识别击球弱点的数据驱动洞察；该系统在多个板球比赛视频上表现出有效性。&lt;h4&gt;结论&lt;/h4&gt;该自动化板球视频分析系统利用深度学习技术实现了高精度的场地和球检测，能够有效提取关键战术信息。该系统不仅能够识别导致球门被取的时刻，还能通过轨迹建模提供数据驱动的战术洞察，为教练团队和战略决策提供重要支持，具有广阔的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种用于板球视频分析的自动化系统，该系统利用深度学习技术来提取导致球门被取的投球、检测板球并建立球轨迹模型。系统采用YOLOv8架构进行场地和球检测，结合光学字符识别技术提取记分卡信息以识别球门被取的时刻。通过全面的图像预处理，包括灰度转换、幂变换和形态学操作，系统实现了从视频帧中稳健的文本提取。场地检测模型达到99.5%的平均精度均值，精确度为0.999；而使用迁移学习的球检测模型达到99.18%的平均精度均值，精确度为0.968，召回率为0.978。该系统可在检测到的场地上进行轨迹建模，为识别击球弱点提供数据驱动的洞察。在多个板球比赛视频上的实验结果证明了这种方法在自动化板球分析中的有效性，为教练和战略决策提供了巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents an automated system for cricket video analysis thatleverages deep learning techniques to extract wicket-taking deliveries, detectcricket balls, and model ball trajectories. The system employs the YOLOv8architecture for pitch and ball detection, combined with optical characterrecognition (OCR) for scorecard extraction to identify wicket-taking moments.Through comprehensive image preprocessing, including grayscale transformation,power transformation, and morphological operations, the system achieves robusttext extraction from video frames. The pitch detection model achieved 99.5%mean Average Precision at 50% IoU (mAP50) with a precision of 0.999, while theball detection model using transfer learning attained 99.18% mAP50 with 0.968precision and 0.978 recall. The system enables trajectory modeling on detectedpitches, providing data-driven insights for identifying batting weaknesses.Experimental results on multiple cricket match videos demonstrate theeffectiveness of this approach for automated cricket analytics, offeringsignificant potential for coaching and strategic decision-making.</description>
      <author>example@mail.com (Mst Jannatun Ferdous, Masum Billah, Joy Karmoker, Mohd Ruhul Ameen, Akif Islam, Md. Omar Faruqe)</author>
      <guid isPermaLink="false">2510.18405v1</guid>
      <pubDate>Wed, 22 Oct 2025 15:12:04 +0800</pubDate>
    </item>
    <item>
      <title>Flow-Aware Ellipsoidal Filtration for Persistent Homology of Recurrent Signals</title>
      <link>http://arxiv.org/abs/2510.17735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 13 figures. Extended version of the paper presented at  NOLTA 2025; prepared for journal submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为椭圆滤波的新型滤波方法，用于从动态光滑流中采样的点云分析，改进了循环信号的去噪和循环时间的估计。&lt;h4&gt;背景&lt;/h4&gt;持久同调通常用于探索点云的形状，这些点云假设是从几何对象中采样得到的。传统方法使用各向同性球体在不断增加的尺度上创建拓扑结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种假设点云从动态光滑流中采样的新型滤波方法，以改进信号处理和特征估计。&lt;h4&gt;方法&lt;/h4&gt;椭圆滤波基于局部流方差在点周围创建椭圆体，随着尺度增加来近似流的流形，而非使用传统的各向同性球体方法。&lt;h4&gt;主要发现&lt;/h4&gt;构建椭圆邻域能够改进循环信号的去噪和循环时间的估计，特别是在数据包含瓶颈的情况下效果更佳。&lt;h4&gt;结论&lt;/h4&gt;根据H1类的最大持久性选择椭圆体，可以为去噪和循环时间估计提供数据驱动的阈值。&lt;h4&gt;翻译&lt;/h4&gt;持久同调的一个常见用途是探索点云的形状，其中点假设是从几何对象中采样的。我们提出了一种新型滤波，称为椭圆滤波，它假设点云是从动态光滑流中采样的。椭圆滤波不是使用各向同性球体（例如Vietoris-Rips滤波）在不断增加的尺度上从点云创建拓扑结构，而是基于局部流方差在点周围创建椭圆体，随着尺度的增加来近似流的流形。我们表明，构建椭圆邻域可以改进循环信号的去噪和循环时间的估计，特别是当数据包含瓶颈时。根据H1类的最大持久性选择椭圆体，可以为去噪和循环时间估计提供数据驱动的阈值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决传统持久同调方法（如Vietoris-Rips过滤）在处理重复信号时的局限性，特别是在信号含有瓶颈结构和不同采样密度的情况下表现不佳的问题。这个问题在现实中很重要，因为重复信号在自然系统中很常见（如生理信号、气候数据、机械振动等），准确分析其拓扑结构对于理解系统动态特性、去噪和估计返回时间等任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到传统拓扑数据分析方法在处理时间序列数据时的局限性，特别是在处理瓶颈结构和不同采样密度时表现不佳。他们借鉴了持久同调的基本概念，特别是H1类的持久性分析；参考了Fernández等人的Fermat距离方法，该方法考虑了采样密度；受Kališnik等人的椭球过滤工作启发，但他们的工作针对的是静态点云而非时间序列数据。作者结合时空邻域概念，通过局部协方差估计和主成分分析构建自适应椭球体，使其能够根据局部流动方向和速度调整形状，从而更好地捕捉信号的动态特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云视为从动态光滑流中采样的样本，而非静态几何对象，通过使用自适应的椭球体邻域替代传统的各向同性球体邻域来更好地捕捉信号的局部流动特性。整体实现流程包括：1)局部协方差估计，结合时间邻域和空间邻域；2)基于PCA构建自适应椭球体；3)检测椭球体相交情况；4)构建椭球复形；5)进行持久同调分析；6)应用在信号去噪和返回时间估计上。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)时空邻域设计，同时考虑短期时间演化和空间重复性；2)自适应椭球体过滤，根据局部流动方向和速度调整形状；3)基于H1类最大持久性的数据驱动尺度选择方法；4)将拓扑分析方法应用于返回时间估计。相比之前的工作，本文方法超越了传统Vietoris-Rips过滤的各向同性限制，解决了Fermat距离方法未整合时间信息的问题，并改进了Kališnik等人针对静态点云的椭球过滤方法，使其能够处理时间序列数据的时空特性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于流动感知的自适应椭球过滤方法，通过结合时空邻域信息和局部流动几何，显著提高了持久同调在处理重复信号（特别是含有瓶颈结构和不同采样密度的信号）时的拓扑分析、去噪和返回时间估计性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One common use of persistent homology is to explore the shape of pointclouds, where points are assumed to be sampled from a geometric object. Wepropose a novel filtration, called ellipsoidal filtration, which assumes thatpoint clouds are sampled from a dynamic smooth flow. Instead of creatingtopologies from point clouds at increasing scales using isotropic balls (forexample, Vietoris-Rips filtration), ellipsoidal filtration creates ellipsoidsaround points based on local flow variances, approximating the flow's manifoldas the scale increases. We show that constructing ellipsoidal neighbourhoodsimproves the denoising of recurrent signals and the estimation of recurrencetimes, especially when the data contain bottlenecks. Choosing ellipsoidsaccording to the maximum persistence of the H1 class provides a data-driventhreshold for both denoising and recurrence-time estimation.</description>
      <author>example@mail.com (Omer Bahadir Eryilmaz, Cihan Katar, Max A. Little)</author>
      <guid isPermaLink="false">2510.17735v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
  <item>
      <title>Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions</title>
      <link>http://arxiv.org/abs/2510.17719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对雨滴条件下3D高斯散射(3DGS)的重建质量问题，提出了一个名为RaindropGS的综合基准测试，用于评估从受雨滴影响的非约束图像到清晰3D重建的完整流程。&lt;h4&gt;背景&lt;/h4&gt;3DGS在雨滴条件下因镜头上的雨滴污染而面临严重的遮挡和光学失真，导致重建质量显著下降。现有基准测试通常使用已知相机姿态的合成雨滴图像评估3DGS，假设理想条件。然而，在真实场景中，雨滴干扰相机姿态估计和点云初始化，且合成与真实雨滴间的域差距损害了泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决雨滴条件下3DGS评估的局限性，提出一个全面基准测试，评估从受雨滴影响的非约束图像到清晰3DGS重建的完整流程。&lt;h4&gt;方法&lt;/h4&gt;RaindropGS基准测试包含三部分：数据准备、数据处理和雨滴感知的3DGS评估。研究收集了真实世界雨滴重建数据集，每个场景包含三个对齐图像集：雨滴聚焦、背景聚焦和无雨滴真实地面，用于全面评估不同聚焦条件下的重建质量。&lt;h4&gt;主要发现&lt;/h4&gt;现有3DGS方法在非约束雨滴图像上存在性能限制；相机聚焦位置显著影响3DGS重建性能；不准确的姿态估计和点云初始化对重建造成干扰。&lt;h4&gt;结论&lt;/h4&gt;研究为开发雨滴条件下更强大的3DGS方法提供了明确方向，通过全面评估揭示了不同流水线组件的影响和局限性。&lt;h4&gt;翻译&lt;/h4&gt;在雨滴条件下的3D高斯散射(3DGS)因相机镜头上雨滴污染造成的严重遮挡和光学失真而遭受显著重建质量下降。现有基准测试通常使用具有已知相机姿态的合成雨滴图像来评估3DGS，假设理想条件。然而，在真实场景中，雨滴常常干扰准确的相机姿态估计和点云初始化。此外，合成雨滴和真实雨滴之间的显著域差距进一步损害了泛化能力。为了解决这些问题，我们引入了RaindropGS，这是一个全面的基准测试，旨在评估从受雨滴影响的非约束图像到清晰的3DGS重建的完整3DGS流程。具体而言，整个基准测试流程包括三个部分：数据准备、数据处理和雨滴感知的3DGS评估，包括雨滴干扰类型、相机姿态估计和点云初始化、单图像雨滴去除比较以及3D高斯训练比较。首先，我们收集了一个真实世界的雨滴重建数据集，其中每个场景包含三个对齐的图像集：雨滴聚焦、背景聚焦和无雨滴真实地面，从而能够全面评估不同聚焦条件下的重建质量。通过全面的实验和分析，我们揭示了现有3DGS方法在非约束雨滴图像上的性能限制以及不同流水线组件的 varying 影响：相机聚焦位置对3DGS重建性能的影响，以及不准确姿态和点云初始化对重建造成的干扰。这些见解为在雨滴条件下开发更强大的3DGS方法指明了明确的方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在雨滴条件下进行3D高斯泼溅(3DGS)重建时，因相机镜头上雨滴造成的遮挡和光学失真导致的重建质量下降问题。这个问题在现实中很重要，因为雨滴是常见的环境干扰因素，会影响自动驾驶、增强现实等户外视觉应用；在研究中，现有方法在合成数据上表现良好但在真实场景中效果不佳，且缺乏针对真实雨滴条件的全面评估基准，无法准确衡量方法在实际应用中的性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有研究的局限性：主要在合成数据上评估，假设了理想条件；认识到雨滴影响3DGS流程的多个阶段；注意到真实雨滴和合成雨滴间的差异；强调相机聚焦位置的重要性。作者设计了RaindropGS基准，包括数据准备(收集真实世界数据集)、数据处理(评估相机姿态估计、点云初始化和雨滴去除)和雨滴感知的3DGS评估。该方法借鉴了现有3DGS方法、COLMAP和VGGT进行姿态估计、以及Uformer、Restormer和IDT等雨滴去除模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个全面基准，评估从受雨滴影响的未约束图像到清晰3DGS重建的完整流程，关注每个关键步骤(相机姿态估计、点云初始化、雨滴去除)对最终质量的影响。整体流程：1)数据收集-11个真实场景，每场景有雨滴聚焦、背景聚焦和无雨滴真实图像；2)数据处理-使用COLMAP和VGGT进行姿态估计和点云初始化，使用Uformer、Restormer和IDT进行雨滴去除；3)3DGS重建-使用原始3DGS、WeatherGS、GS-W和3DGS-MCMC四种方法；4)评估-使用PSNR、SSIM等指标比较不同聚焦条件和预处理策略的影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个全面的3DGS雨滴重建基准，覆盖完整流程；2)首个真实世界3DGS雨滴重建数据集，包含三种对齐图像集和随机变化的雨滴；3)对现有方法的全面评估和见解。相比之前工作：1)评估从合成数据转向真实世界数据；2)关注点从仅关注3D高斯拟合扩展到整个流程；3)数据集从合成转向真实，考虑了雨滴在不同视角的变化；4)明确考虑了雨滴聚焦和背景聚焦两种条件，使评估更全面。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RaindropGS提供了一个全面的基准和真实世界数据集，用于评估在真实雨滴条件下3D高斯泼溅技术的完整重建流程，揭示了现有方法的局限性并指明了未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severeocclusions and optical distortions caused by raindrop contamination on thecamera lens, substantially degrading reconstruction quality. Existingbenchmarks typically evaluate 3DGS using synthetic raindrop images with knowncamera poses (constrained images), assuming ideal conditions. However, inreal-world scenarios, raindrops often interfere with accurate camera poseestimation and point cloud initialization. Moreover, a significant domain gapbetween synthetic and real raindrops further impairs generalization. To tacklethese issues, we introduce RaindropGS, a comprehensive benchmark designed toevaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted imagesto clear 3DGS reconstructions. Specifically, the whole benchmark pipelineconsists of three parts: data preparation, data processing, and raindrop-aware3DGS evaluation, including types of raindrop interference, camera poseestimation and point cloud initialization, single image rain removalcomparison, and 3D Gaussian training comparison. First, we collect a real-worldraindrop reconstruction dataset, in which each scene contains three alignedimage sets: raindrop-focused, background-focused, and rain-free ground truth,enabling a comprehensive evaluation of reconstruction quality under differentfocus conditions. Through comprehensive experiments and analyses, we revealcritical insights into the performance limitations of existing 3DGS methods onunconstrained raindrop images and the varying impact of different pipelinecomponents: the impact of camera focus position on 3DGS reconstructionperformance, and the interference caused by inaccurate pose and point cloudinitialization on reconstruction. These insights establish clear directions fordeveloping more robust 3DGS methods under raindrop conditions.</description>
      <author>example@mail.com (Zhiqiang Teng, Beibei Lin, Tingting Chen, Zifeng Yuan, Xuanyi Li, Xuanyu Zhang, Shunli Zhang)</author>
      <guid isPermaLink="false">2510.17719v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Towards 3D Objectness Learning in an Open World</title>
      <link>http://arxiv.org/abs/2510.17686v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出OP3Det，一种无类别开放世界无提示3D检测器，能够检测3D场景中的任何物体，包括训练中未见过的物体，显著提升了开放世界3D物体检测性能。&lt;h4&gt;背景&lt;/h4&gt;3D物体检测和新类别检测虽有进展，但学习泛化3D物体性的研究仍不足；传统封闭集3D检测器难以泛化到开放世界场景，而直接引入3D开放词汇模型面临词汇扩展和语义重叠的挑战。&lt;h4&gt;目的&lt;/h4&gt;研究开放世界3D物体性，实现能够检测3D场景中所有物体（包括训练中未见过的物体）的泛化3D物体发现。&lt;h4&gt;方法&lt;/h4&gt;提出OP3Det检测器，引入2D基础模型的强泛化和零样本能力，利用2D语义先验和3D几何先验进行无类别提案，通过跨模态专家混合集成点云和RGB图像的互补信息，动态路由单模态和多模态特征以学习泛化的3D物体性。&lt;h4&gt;主要发现&lt;/h4&gt;OP3Det表现卓越，与现有开放世界3D检测器相比在AR指标上显著提高最多16.0%，与封闭世界3D检测器相比实现了13.5%的改进。&lt;h4&gt;结论&lt;/h4&gt;OP3Det成功实现了开放世界3D物体检测的目标，能够检测3D场景中的任何物体而不依赖手工制作的文本提示。&lt;h4&gt;翻译&lt;/h4&gt;最近的3D物体检测和新类别检测研究取得了显著进展，然而关于学习泛化3D物体性的研究仍然不足。在本文中，我们深入研究开放世界3D物体性学习，专注于检测3D场景中的所有物体，包括训练中未见过的新物体。传统的封闭集3D检测器难以泛化到开放世界场景，而直接将3D开放词汇模型整合以获得开放世界能力又面临词汇扩展和语义重叠的挑战。为实现泛化的3D物体发现，我们提出了OP3Det，一种无类别开放世界无提示3D检测器，可以检测3D场景中的任何物体，而不依赖手工制作的文本提示。我们引入了2D基础模型的强泛化和零样本能力，利用2D语义先验和3D几何先验进行无类别提案，以扩展3D物体发现。然后，通过在跨模态专家混合中集成点云和RGB图像的互补信息，OP3Det动态路由单模态和多模态特征以学习泛化的3D物体性。大量实验证明了OP3Det的卓越性能，在AR指标上显著超越现有开放世界3D检测器最多16.0%，相比封闭世界3D检测器实现了13.5%的改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决开放世界中的3D物体检测问题，特别是学习通用的3D物体性，使模型能够检测3D场景中的所有物体，包括训练过程中未见过的物体类别。这个问题在现实世界中非常重要，因为自动驾驶、机器人等应用场景中物体类别可能会动态变化，传统封闭集3D检测器无法泛化到这些开放世界场景，而直接使用开放词汇模型又面临词汇扩展和语义重叠的问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先注意到现实世界环境中物体类别可能动态变化，导致需要开放世界3D检测能力。观察到2D领域在未知物体识别方面已有较多探索，但3D领域有限。意识到3D点云数据规模和标注类别有限，而2D领域有丰富的基础模型和训练数据。因此，作者将2D预训练模型的强大零样本能力转移到3D领域。借鉴了SAM模型用于提取类别无关物体掩码，以及多模态融合方法，但提出了创新的跨模态专家混合模块来解决现有方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D基础模型（特别是SAM）的强大泛化能力帮助3D物体发现，并通过多尺度点采样策略优化SAM产生的掩码，再通过跨模态专家混合模块动态融合单模态和多模态特征学习通用3D物体性。整体流程分为：1)3D物体发现阶段，使用SAM和多尺度点采样策略处理RGB图像，投影到3D空间；2)训练阶段，提取点云和图像特征，使用跨模态MoE融合特征；3)推理阶段，直接在点云-图像对上进行检测，完全无提示地进行类别无关的3D物体性推理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次定义并解决3D领域的类别无关开放世界物体检测问题；2)设计多尺度点采样策略增强2D-3D关联，解决SAM碎片化掩码问题；3)提出跨模态专家混合模块动态选择单模态和多模态路径。相比传统封闭集3D检测器，它能检测未见过的物体类别；相比开放词汇3D检测器，它不需要预定义词汇表；相比现有多模态融合方法，它保留模态特定信息并动态选择最相关特征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了OP3Det，首个类别无关的开放世界3D检测器，通过利用2D语义知识和创新的跨模态专家混合机制，实现了在开放世界中检测所有物体的能力，包括训练过程中未见过的物体类别。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in 3D object detection and novel category detection havemade significant progress, yet research on learning generalized 3D objectnessremains insufficient. In this paper, we delve into learning open-world 3Dobjectness, which focuses on detecting all objects in a 3D scene, includingnovel objects unseen during training. Traditional closed-set 3D detectorsstruggle to generalize to open-world scenarios, while directly incorporating 3Dopen-vocabulary models for open-world ability struggles with vocabularyexpansion and semantic overlap. To achieve generalized 3D object discovery, Wepropose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detectany objects within 3D scenes without relying on hand-crafted text prompts. Weintroduce the strong generalization and zero-shot capabilities of 2D foundationmodels, utilizing both 2D semantic priors and 3D geometric priors forclass-agnostic proposals to broaden 3D object discovery. Then, by integratingcomplementary information from point cloud and RGB image in the cross-modalmixture of experts, OP3Det dynamically routes uni-modal and multi-modalfeatures to learn generalized 3D objectness. Extensive experiments demonstratethe extraordinary performance of OP3Det, which significantly surpasses existingopen-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvementcompared to closed-world 3D detectors.</description>
      <author>example@mail.com (Taichi Liu, Zhenyu Wang, Ruofeng Liu, Guang Wang, Desheng Zhang)</author>
      <guid isPermaLink="false">2510.17686v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation</title>
      <link>http://arxiv.org/abs/2510.17609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于机器学习的框架，用于自动分割3D点云，特别是在基础设施健康监测中的应用，结合了无人机扫描的真实数据和从BIM生成的合成数据。&lt;h4&gt;背景&lt;/h4&gt;无人机技术进步实现了高效、非接触式的结构健康监测，结合摄影测量可捕获高分辨率扫描并重建基础设施的详细3D模型，但从这些模型中分割特定结构组件仍面临挑战，传统方法依赖耗时且容易出错的手动标注。&lt;h4&gt;目的&lt;/h4&gt;解决从3D模型中分割特定结构组件的挑战，开发一种自动化分割方法替代耗时且易错的手动标注流程。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于机器学习的框架用于3D点云自动分割，利用真实无人机扫描点云和从建筑信息建模(BIM)生成的合成数据的互补优势，结合BIM数据克服手动标注的局限性。&lt;h4&gt;主要发现&lt;/h4&gt;在铁路轨道数据集验证中，该方法在识别和分割主要组件如铁轨和轨枕方面表现高准确性；通过使用较小的数据集并补充BIM数据，框架显著减少了训练时间同时保持了合理的分割准确性。&lt;h4&gt;结论&lt;/h4&gt;这种自动化方法提高了3D基础设施模型分割的精度和效率，推进了无人机和BIM技术在结构健康监测和基础设施管理中的集成应用。&lt;h4&gt;翻译&lt;/h4&gt;无人机技术的进步实现了高效、非接触式的结构健康监测。结合摄影测量技术，无人机可以捕获高分辨率扫描并重建基础设施的详细3D模型。然而，从这些模型中分割特定结构组件仍然是一个关键挑战，这一过程传统上依赖于耗时且容易出错的手动标注。为解决这一问题，我们提出了一种基于机器学习的框架，用于3D点云的自动分割。我们的方法利用真实无人机扫描点云和从建筑信息建模(BIM)生成的合成数据的互补优势，克服了与手动标注相关的局限性。在铁路轨道数据集上的验证显示，在识别和分割铁轨和轨枕等主要组件方面具有高准确性。此外，通过使用较小的数据集并补充BIM数据，该框架显著减少了训练时间，同时保持了合理的分割准确性。这种自动化方法提高了3D基础设施模型分割的精度和效率，并推进了无人机和BIM技术在结构健康监测和基础设施管理中的集成。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从3D点云中自动分割铁路基础设施的关键结构组件（如钢轨和轨枕）的问题。传统方法依赖耗时且易错的手动标注，这在大型数据集上尤其不切实际。这个问题很重要，因为铁路是社会经济支柱，组件损坏可能导致严重事故；人工检查既耗时又危险；而无人机虽高效安全，但点云分割仍需手动标注，限制了自动化监测的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到铁路基础设施健康监测的重要性及传统人工检查的局限性。他们借鉴了无人机技术用于基础设施检查的研究、摄影测量3D重建方法、以及PointNet++等深度学习网络用于点云分割的工作。在此基础上，他们创新性地提出结合真实世界无人机扫描点云和从建筑信息模型生成的合成数据，设计了一个机器学习框架，通过创建不同规模的训练数据集并测试不同旋转策略，实现了高效准确的自动分割。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合真实世界无人机扫描点云和从BIM生成的合成数据，减少对手动标注数据的依赖，实现铁路基础设施3D点云的自动分割。实现流程包括：1)使用无人机沿规划路径采集铁路图像并重建3D点云；2)对真实点云进行手动标注，同时创建BIM模型并自动生成标注点云；3)对数据进行下采样并创建不同规模的数据集；4)使用PointNet++架构训练分割模型；5)评估模型性能并测试在不同材料轨枕上的泛化能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创新性地融合真实点云与BIM合成数据；2)使用较小规模训练数据集结合BIM数据实现高效分割；3)系统研究数据增强策略对模型性能的影响；4)展示模型在不同材料轨枕上的泛化能力。相比之前工作，本文不仅减少了手动标注需求，还显著缩短了训练时间，实现了小规模数据集的高效利用，并证明了方法在不同材料铁路结构上的适用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的机器学习框架，通过结合无人机摄影测量的真实点云和BIM生成的合成数据，实现了铁路基础设施3D点云的高效自动分割，为铁路健康监测提供了一种可扩展且实用的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advancement of UAV technology has enabled efficient, non-contactstructural health monitoring. Combined with photogrammetry, UAVs can capturehigh-resolution scans and reconstruct detailed 3D models of infrastructure.However, a key challenge remains in segmenting specific structural componentsfrom these models-a process traditionally reliant on time-consuming anderror-prone manual labeling. To address this issue, we propose a machinelearning-based framework for automated segmentation of 3D point clouds. Ourapproach uses the complementary strengths of real-world UAV-scanned pointclouds and synthetic data generated from Building Information Modeling (BIM) toovercome the limitations associated with manual labeling. Validation on arailroad track dataset demonstrated high accuracy in identifying and segmentingmajor components such as rails and crossties. Moreover, by using smaller-scaledatasets supplemented with BIM data, the framework significantly reducedtraining time while maintaining reasonable segmentation accuracy. Thisautomated approach improves the precision and efficiency of 3D infrastructuremodel segmentation and advances the integration of UAV and BIM technologies instructural health monitoring and infrastructure management.</description>
      <author>example@mail.com (Siqi Chen, Shanyue Guan)</author>
      <guid isPermaLink="false">2510.17609v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception</title>
      <link>http://arxiv.org/abs/2510.17568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PAGE-4D的前馈模型，能够处理包含动态元素的3D场景，实现相机姿态估计、深度预测和点云重建，无需后处理处理。&lt;h4&gt;背景&lt;/h4&gt;现有的3D前馈模型（如VGGT）在静态场景的3D属性推断上表现良好，但由于主要在静态数据集上训练，在处理包含移动人类或可变形物体等复杂动态元素的真实世界场景时效果不佳。&lt;h4&gt;目的&lt;/h4&gt;引入PAGE-4D模型，将VGGT扩展到动态场景，实现相机姿态估计、深度预测和点云重建的多功能4D重建系统。&lt;h4&gt;方法&lt;/h4&gt;提出了一种动态感知聚合器，通过预测动态感知掩码来解耦静态和动态信息，在相机姿态估计时抑制运动线索，在几何重建时增强这些线索，解决了多任务4D重建中任务间的固有冲突。&lt;h4&gt;主要发现&lt;/h4&gt;PAGE-4D在动态场景中始终优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点图重建方面取得了更好的性能表现。&lt;h4&gt;结论&lt;/h4&gt;PAGE-4D成功解决了多任务4D重建中任务间的固有冲突，通过动态感知聚合器有效处理了静态和动态场景中的各种任务，无需后处理即可实现高质量的4D重建。&lt;h4&gt;翻译&lt;/h4&gt;最近的3D前馈模型，如视觉几何基础变换器（VGGT），在推断静态场景的3D属性方面表现出强大能力。然而，由于它们通常在静态数据集上训练，这些模型在涉及复杂动态元素的真实世界场景中往往表现不佳，例如移动的人类或像雨伞这样的可变形物体。为了解决这一局限性，我们引入了PAGE-4D，一种将VGGT扩展到动态场景的前馈模型，能够实现相机姿态估计、深度预测和点云重建——所有这些都无需后处理。多任务4D重建的一个核心挑战是任务之间的固有冲突：准确的相机姿态估计需要抑制动态区域，而几何重建则需要建模这些区域。为了解决这种张力，我们提出了一种动态感知聚合器，通过预测动态感知掩码来解耦静态和动态信息——为姿态估计抑制运动线索，同时为几何重建增强这些线索。大量实验表明，PAGE-4D在动态场景中始终优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点图重建方面取得了优越的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决将静态3D场景理解模型扩展到动态场景的问题。这个问题在现实中非常重要，因为动态场景（如包含移动人或物体的环境）在自动驾驶、机器人导航、增强现实等应用中非常普遍，准确理解和重建这些场景对现实应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到VGGT在静态场景中表现良好但在动态场景中性能下降，然后分析发现核心冲突：相机姿态估计需要抑制动态区域，而几何重建则需要利用动态信息。他们借鉴了VGGT的基础架构，引入了动态感知聚合器和注意力机制，并采用了针对性的微调策略，只调整对动态最敏感的中间层，而不是重新设计整个网络。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是解耦动态信息在不同任务中的作用：对相机姿态估计抑制动态区域，对几何重建则利用动态信息。整体流程包括：输入RGB图像序列；使用预训练编码器提取特征；通过三阶段动态感知聚合器处理特征；最后通过解码器输出深度图、3D点云和相机姿态估计结果。其中动态感知聚合器预测动态掩码，并根据任务类型有选择地应用这个掩码。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 动态感知聚合器，解耦动态和静态信息；2) 针对性微调策略，只调整关键层；3) 动态掩码预测，自适应学习动态区域；4) 任务特定的注意力应用。相比之前的工作，PAGE-4D不需要大的架构改变，而是通过微调和动态感知注意力机制实现，在保持高效的同时提高了动态场景的性能，超越了VGGT和其他动态场景模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PAGE-4D通过解耦动态信息在不同任务中的作用，有效地将静态3D场景理解模型扩展到动态场景，实现了高效的相机姿态估计和几何重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent 3D feed-forward models, such as the Visual Geometry GroundedTransformer (VGGT), have shown strong capability in inferring 3D attributes ofstatic scenes. However, since they are typically trained on static datasets,these models often struggle in real-world scenarios involving complex dynamicelements, such as moving humans or deformable objects like umbrellas. Toaddress this limitation, we introduce PAGE-4D, a feedforward model that extendsVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, andpoint cloud reconstruction -- all without post-processing. A central challengein multi-task 4D reconstruction is the inherent conflict between tasks:accurate camera pose estimation requires suppressing dynamic regions, whilegeometry reconstruction requires modeling them. To resolve this tension, wepropose a dynamics-aware aggregator that disentangles static and dynamicinformation by predicting a dynamics-aware mask -- suppressing motion cues forpose estimation while amplifying them for geometry reconstruction. Extensiveexperiments show that PAGE-4D consistently outperforms the original VGGT indynamic scenarios, achieving superior results in camera pose estimation,monocular and video depth estimation, and dense point map reconstruction.</description>
      <author>example@mail.com (Kaichen Zhou, Yuhan Wang, Grace Chen, Xinhai Chang, Gaspard Beaudouin, Fangneng Zhan, Paul Pu Liang, Mengyu Wang)</author>
      <guid isPermaLink="false">2510.17568v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS</title>
      <link>http://arxiv.org/abs/2510.17479v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A preprint paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究针对稀疏视图3D高斯溅射(3DGS)对训练视图过拟合导致的新视图渲染伪影问题，提出了一种基于改进初始化的解决方案。研究发现初始化是决定性能的关键因素，而非训练时约束。研究团队设计了频率感知SfM、3DGS自初始化和点云正则化三种方法，显著提升了稀疏视图设置下的渲染质量，并在LLFF和Mip-NeRF360数据集上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;稀疏视图3D高斯溅射(3DGS)技术通常对训练视图过拟合，导致在新视图渲染时出现模糊等伪影。先前的研究主要通过增强初始化（即改进来自运动结构SfM的点云）或添加训练时约束（正则化）来解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决稀疏视图3DGS的过拟合问题，通过改进初始化策略而非依赖训练时约束，提升新视图渲染质量，消除伪影。&lt;h4&gt;方法&lt;/h4&gt;研究团队设计了三种方法来改进初始化：(i) 频率感知SfM：通过低频视图增强和放宽的多视图对应关系提高低纹理区域的覆盖率；(ii) 3DGS自初始化：将光度监督提升为附加点，用学习的高斯中心补偿SfM未能充分覆盖的区域；(iii) 点云正则化：通过简单的几何/可见性先验强制执行多视图一致性和均匀空间覆盖，产生干净可靠的点云。&lt;h4&gt;主要发现&lt;/h4&gt;通过对照实验发现，初始化是决定稀疏视图3DGS性能的关键因素，它决定了可达到的性能范围，而训练时约束只能在该范围内带来适度改进，且需要额外计算成本。&lt;h4&gt;结论&lt;/h4&gt;本研究提出的三种初始化改进方法在稀疏视图设置下展示了一致的性能提升，确立了一种更强初始化策略的有效性。代码已在GitHub开源，可供进一步研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;稀疏视图3D高斯溅射(3DGS)通常对训练视图过拟合，导致新视图渲染时出现模糊等伪影。先前的研究通过增强初始化（即来自运动结构SfM的点云）或添加训练时约束（正则化）来解决3DGS优化问题。然而，我们的对照实验揭示初始化是决定性因素：它决定了稀疏视图3DGS可达到的性能范围，而训练时约束只能在该范围内带来适度改进且需要额外成本。鉴于初始化的首要地位，我们将设计重点放在那里。尽管SfM由于其依赖特征匹配在稀疏视图下表现不佳，但它仍能提供可靠的种子点。因此，我们在SfM基础上努力尽可能全面地补充它未能覆盖的区域。具体而言，我们设计了：(i)频率感知SfM，通过低频视图增强和放宽的多视图对应关系提高低纹理覆盖率；(ii)3DGS自初始化，将光度监督提升为附加点，用学习的高斯中心补偿SfM稀疏区域；(iii)点云正则化，通过简单的几何/可见性先验强制执行多视图一致性和均匀空间覆盖，产生干净可靠的点云。我们在LLFF和Mip-NeRF360上的实验展示了在稀疏视图设置中的一致性改进，确立了我们的方法作为一种更强的初始化策略。代码可在https://github.com/zss171999645/ItG-GS获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决稀疏视图3D高斯泼溅（3DGS）对训练视图过拟合的问题，导致在渲染新视图时出现模糊等伪影。这个问题在现实中非常重要，因为虚拟现实、增强现实、自由视点视频和数字内容创作等应用都需要从有限视点生成逼真新视图的能力。在实际场景中，由于硬件限制、成本或捕获条件，我们通常只能获取有限数量的视点图像，因此解决稀疏视图下的过拟合问题能显著提升这些应用的质量和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过受控实验发现初始化质量是决定稀疏视图3DGS性能的关键因素，而训练时正则化只能提供有限的改进。基于这一发现，他们将设计重点放在初始化阶段。他们借鉴了现有的SfM算法，尽管它在稀疏视图下表现不佳但仍能提供可靠的种子点。具体来说，他们参考了EAP-GS的工作，通过将最小轨道匹配要求从三个视图降低到两个视图来增加初始点集密度。此外，他们还利用了3DGS自身的学习信号，设计了自初始化方法，将像素级的光度约束转化为额外的3D点。最后，他们引入了点云正则化技术，结合了几何和可见性先验的概念来优化点云质量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过增强初始化点云的质量和覆盖范围来解决稀疏视图3DGS的过拟合问题。整体实现流程分为三个阶段：1）低频感知SfM：通过掩码高频图像区域，在增强的双图像集上执行SfM，从而改善低纹理区域的覆盖；2）3DGS自初始化：在输入视图上训练一个轻量级3DGS模型，并将所有原始高斯中心作为新的点云重用，这能补偿图像特征不足的区域；3）点云正则化：通过单视图点过滤（去除深度模糊的点）、聚类去噪（减少不稳定点和重复点）以及基于法线的一致性过滤（去除几何不一致的点），最终产生一个干净可靠的点云作为3DGS的初始化输入。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三个：1）频率感知SfM：通过掩码高频区域和放松多视图对应要求，显著改善低纹理区域的覆盖；2）3DGS自初始化：创新性地利用3DGS的学习信号，将像素级光度约束提升为额外的3D点，弥补了传统SfM在弱纹理区域的不足；3）点云正则化：结合单视图过滤、聚类去噪和基于法线的一致性过滤，产生干净可靠的点云。相比之前的工作，本文的不同之处在于：首先，它首次系统性地证明了初始化质量比训练时正则化对稀疏视图3DGS的性能影响更大；其次，它不是简单地改进单一组件，而是设计了一个完整的三阶段初始化管道；最后，它能够与现有的正则化方法（如DropGS）结合使用，进一步提升性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种三阶段初始化管道，通过显著增强稀疏视图3DGS的初始点云质量和覆盖范围，有效解决了过拟合问题，大幅提升了新视图合成的质量和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the trainingviews, leading to artifacts like blurring in novel view rendering. Prior workaddresses it either by enhancing the initialization (\emph{i.e.}, the pointcloud from Structure-from-Motion (SfM)) or by adding training-time constraints(regularization) to the 3DGS optimization. Yet our controlled ablations revealthat initialization is the decisive factor: it determines the attainableperformance band in sparse-view 3DGS, while training-time constraints yieldonly modest within-band improvements at extra cost. Given initialization'sprimacy, we focus our design there. Although SfM performs poorly under sparseviews due to its reliance on feature matching, it still provides reliable seedpoints. Thus, building on SfM, our effort aims to supplement the regions itfails to cover as comprehensively as possible. Specifically, we design: (i)frequency-aware SfM that improves low-texture coverage via low-frequency viewaugmentation and relaxed multi-view correspondences; (ii) 3DGSself-initialization that lifts photometric supervision into additional points,compensating SfM-sparse regions with learned Gaussian centers; and (iii)point-cloud regularization that enforces multi-view consistency and uniformspatial coverage through simple geometric/visibility priors, yielding a cleanand reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrateconsistent gains in sparse-view settings, establishing our approach as astronger initialization strategy. Code is available athttps://github.com/zss171999645/ItG-GS.</description>
      <author>example@mail.com (Feng Zhou, Wenkai Guo, Pu Cao, Zhicheng Zhang, Jianqin Yin)</author>
      <guid isPermaLink="false">2510.17479v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding</title>
      <link>http://arxiv.org/abs/2510.17068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ProDAT的新型渐进式点云编码方法，通过密度感知尾部丢弃机制实现多比特率下的渐进式解码，并在编码效率上显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;三维点云在自动驾驶、增强现实和沉浸式通信等应用中日益重要，需要实时处理和低延迟。但大数据量和带宽限制在资源有限环境中阻碍了高质量服务的部署。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于学习的点云几何编码方法固定潜在表示不支持渐进式解码的问题，实现高效的渐进式点云编码。&lt;h4&gt;方法&lt;/h4&gt;提出ProDAT，一种密度感知尾部丢弃机制，通过利用密度信息作为指导信号，使潜在特征和坐标根据其重要性进行自适应解码，从而使用单个模型在多个比特率下实现渐进式解码。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的实验结果表明，ProDAT不仅能够实现渐进式编码，而且与最先进的基于学习的编码技术相比，实现了更高的编码效率。在SemanticKITTI上，PSNR-D2的BD-rate改进超过28.6%，在ShapeNet上超过18.15%。&lt;h4&gt;结论&lt;/h4&gt;ProDAT成功填补了现有方法不支持渐进式解码的空白，同时显著提高了点云编码的效率。&lt;h4&gt;翻译&lt;/h4&gt;三维（3D）点云在自动驾驶、增强现实和沉浸式通信等应用中变得越来越重要，要求实时处理和低延迟。然而，其大数据量和带宽限制阻碍了在资源有限环境中部署高质量服务。渐进式编码允许在不同细节级别解码，通过允许初始部分解码随后进行细化提供了一种替代方案。尽管最近基于学习的点云几何编码方法取得了显著成功，但其固定的潜在表示不支持渐进式解码。为了填补这一空白，我们提出了ProDAT，一种用于渐进式点云编码的新型密度感知尾部丢弃机制。通过利用密度信息作为指导信号，潜在特征和坐标根据其重要性进行自适应解码，从而使用单个模型在多个比特率下实现渐进式解码。在基准数据集上的实验结果表明，所提出的ProDAT不仅能够实现渐进式编码，而且与最先进的基于学习的编码技术相比，实现了更高的编码效率，在SemanticKITTI上PSNR-D2的BD-rate改进超过28.6%，在ShapeNet上超过18.15%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云编码中不支持渐进式解码的问题。这个问题在现实中很重要，因为点云数据在自动驾驶、增强现实和沉浸式通信等应用中需求日益增长，但这些应用需要实时处理和低延迟。现有的学习点云编码方法产生单一比特流，必须完全解码才能重建，无法满足带宽受限环境下的渐进式质量提升需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了点云编码的现状，发现传统方法在大规模数据集上效率低，而学习-based方法虽性能好但不支持渐进式解码。他们借鉴了2D图像和视频领域的渐进式编码技术（如JPEG、JPEG2000和H.264 SVC），并参考了密度保留点云压缩方法和图像编码中的尾部丢弃技术。通过结合这些思想，作者设计了密度感知的尾部丢弃机制，利用点云密度信息指导特征选择，实现渐进式解码。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用密度信息作为指导信号，高密度区域通常表示复杂几何结构，稀疏区域对应简单结构，通过密度感知的尾部丢弃机制优先保留重要特征。整体流程包括：1)编码阶段提取特征并应用密度感知的尾部丢弃；2)根据渐进比例选择保留特征；3)解码器重建点云；4)训练时使用随机丢弃比例使模型适应不同级别的特征完整性，损失函数结合几何质量、密度保持和比特率约束。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)单一训练阶段的渐进式点云编码；2)密度感知的尾部丢弃机制；3)联合特征和坐标丢弃策略；4)动态密度归一化方法；5)结合全局方差和局部梯度的通道重要性计算。相比之前的工作，ProDAT不仅支持渐进式解码，还通过密度信息优化特征选择，在保持高质量重建的同时提高了编码效率，特别适合大规模点云数据，如SemanticKITTI和ShapeNet数据集。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ProDAT通过引入密度感知的尾部丢弃机制，实现了单一训练阶段的渐进式点云编码，在保持高质量重建的同时显著提高了编码效率和灵活性，特别适合资源受限的实时应用场景。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional (3D) point clouds are becoming increasingly vital inapplications such as autonomous driving, augmented reality, and immersivecommunication, demanding real-time processing and low latency. However, theirlarge data volumes and bandwidth constraints hinder the deployment ofhigh-quality services in resource-limited environments. Progres- sive coding,which allows for decoding at varying levels of detail, provides an alternativeby allowing initial partial decoding with subsequent refinement. Althoughrecent learning-based point cloud geometry coding methods have achieved notablesuccess, their fixed latent representation does not support progressivedecoding. To bridge this gap, we propose ProDAT, a novel density-awaretail-drop mechanism for progressive point cloud coding. By leveraging densityinformation as a guidance signal, latent features and coordinates are decodedadaptively based on their significance, therefore achieving progressivedecoding at multiple bitrates using one single model. Experimental results onbenchmark datasets show that the proposed ProDAT not only enables progressivecoding but also achieves superior coding efficiency compared tostate-of-the-art learning-based coding techniques, with over 28.6% BD-rateimprovement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet</description>
      <author>example@mail.com (Zhe Luo, Wenjing Jia, Stuart Perry)</author>
      <guid isPermaLink="false">2510.17068v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection</title>
      <link>http://arxiv.org/abs/2510.16865v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于配准的旋转不变性特征提取框架，用于点云数据中的3D异常检测，解决了现有方法在特征转换一致性和判别能力方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;3D点云异常检测对工业质量控制至关重要，但当前基于内存库的方法常面临特征转换不一致和判别能力有限的问题，特别是在捕捉局部几何细节和实现旋转不变性方面。当配准失败时，这些问题更加突出，导致不可靠的检测结果。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时解决点云配准和异常检测问题的框架，通过整合这两个任务的目标，实现旋转不变且局部判别性强的特征提取，提高异常检测的可靠性和有效性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种配准诱导的旋转不变性特征提取框架，将点云配准和基于内存的异常检测目标整合在一起。通过将特征提取嵌入到配准学习过程中，框架联合优化对齐和表示学习，使网络获得对旋转鲁棒且对异常检测高效的特征。&lt;h4&gt;主要发现&lt;/h4&gt;点云配准不仅在几何结构对齐中起关键作用，还引导特征提取实现旋转不变性和局部判别性表示。两个任务都依赖于建模局部几何结构和利用样本间的特征相似性。在Anomaly-ShapeNet和Real3D-AD数据集上的实验表明，该方法在有效性和泛化能力上一致优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;通过将特征提取与配准学习过程相结合，所提出的框架能够获得既对旋转具有鲁棒性又对异常检测高效的特征，显著提高了点云异常检测的性能和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;点云数据中的3D异常检测对工业质量控制至关重要，旨在以高可靠性识别结构缺陷。然而，当前基于内存库的方法常常遭受特征转换不一致和判别能力有限的困扰，特别是在捕捉局部几何细节和实现旋转不变性方面。当配准失败时，这些局限性变得更加明显，导致不可靠的检测结果。我们认为点云配准不仅在几何结构对齐中起着关键作用，还引导特征提取朝向旋转不变和局部判别性表示方向发展。为此，我们提出了一种基于配准的旋转不变性特征提取框架，整合了点云配准和基于内存的异常检测目标。我们的关键见解是，这两个任务都依赖于建模局部几何结构和利用样本间的特征相似性。通过将特征提取嵌入到配准学习过程中，我们的框架联合优化了对齐和表示学习。这种整合使网络能够获得对旋转具有鲁棒性且对异常检测非常有效的特征。在Anomaly-ShapeNet和Real3D-AD数据集上的大量实验表明，我们的方法在有效性和泛化能力上一致优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点云异常检测中的两个关键问题：现有基于内存库的方法在特征变换方面存在不一致性，以及在捕捉局部几何细节和实现旋转不变性方面能力有限。这个问题在现实中非常重要，因为3D异常检测对工业质量控制至关重要，可以可靠地识别结构缺陷。当物体以不同方向呈现时，旋转不变性变得尤为重要，而当前方法在注册失败时会产生不可靠的检测结果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析点云注册和基于内存库的异常检测之间的内在联系，发现这两个任务都依赖于对局部几何结构的建模和跨样本的特征相似性利用。他们提出将注册作为特征学习过程的一部分，而不是独立的预处理模块。作者借鉴了RIConv++、KPConv-FPN和Geometric Transformer等现有组件，应用了粗到细的点云注册策略、最优传输算法、RANSAC技术和内存库采样技术，但将它们整合到一个新的统一框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云注册过程整合到异常检测的特征学习中，通过注册任务强制源点云和目标点云之间的几何对齐和多尺度特征一致性，使网络能够获得对旋转具有鲁棒性且对异常检测有效的特征。整体流程分为两个阶段：1)注册诱导的特征学习阶段：生成变换点云对、多尺度下采样、构建补丁匹配、提取多尺度特征，并通过三个损失函数联合优化；2)注册诱导的异常检测阶段：点对齐、特征归一化和内存库构建、特征过滤和异常分数计算。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：将点云注册作为特征学习的一部分而非独立预处理步骤；提出统一的Reg2Inv框架用于注册诱导的旋转不变特征提取；联合优化原型-样本对齐和基于鲁棒局部几何特征的异常评分；通过注册任务同时学习旋转不变和局部判别性表示。相比之前工作，本文方法将注册整合到特征学习中而非作为预处理；保留了细粒度局部几何而非仅关注全局语义；在注册不完美时仍保持鲁棒性；同时优化空间对齐和特征学习而非分开处理。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种将点云注册整合到特征学习过程中的统一框架Reg2Inv，通过联合优化几何对齐和多尺度特征一致性，实现了对旋转具有鲁棒性且对异常检测高度有效的3D点云特征提取，显著提升了工业质量控制中的异常检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D anomaly detection in point-cloud data is critical for industrial qualitycontrol, aiming to identify structural defects with high reliability. However,current memory bank-based methods often suffer from inconsistent featuretransformations and limited discriminative capacity, particularly in capturinglocal geometric details and achieving rotation invariance. These limitationsbecome more pronounced when registration fails, leading to unreliable detectionresults. We argue that point-cloud registration plays an essential role notonly in aligning geometric structures but also in guiding feature extractiontoward rotation-invariant and locally discriminative representations. To thisend, we propose a registration-induced, rotation-invariant feature extractionframework that integrates the objectives of point-cloud registration andmemory-based anomaly detection. Our key insight is that both tasks rely onmodeling local geometric structures and leveraging feature similarity acrosssamples. By embedding feature extraction into the registration learningprocess, our framework jointly optimizes alignment and representation learning.This integration enables the network to acquire features that are both robustto rotations and highly effective for anomaly detection. Extensive experimentson the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our methodconsistently outperforms existing approaches in effectiveness andgeneralizability.</description>
      <author>example@mail.com (Yuyang Yu, Zhengwei Chen, Xuemiao Xu, Lei Zhang, Haoxin Yang, Yongwei Nie, Shengfeng He)</author>
      <guid isPermaLink="false">2510.16865v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Rotation, Scale, and Translation Resilient Black-box Fingerprinting for Intellectual Property Protection of EaaS Models</title>
      <link>http://arxiv.org/abs/2510.16706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种用于EaaS（嵌入即服务）模型的指纹框架，通过分析嵌入空间拓扑结构的几何特性来验证模型所有权，而非传统水印技术。该方法能有效抵抗旋转、缩放和平移（RST）攻击，并在视觉和文本嵌入任务中验证了其优越性。&lt;h4&gt;背景&lt;/h4&gt;特征嵌入已成为处理高维复杂数据的核心技术，导致EaaS模型在云环境中广泛部署。现有水印技术通过修改训练样本或网络参数注入后门触发器来保护知识产权，但这些方法易受语义分析和几何变换攻击的影响。&lt;h4&gt;目的&lt;/h4&gt;解决现有EaaS模型水印技术易受语义分析和几何变换攻击的问题，开发一种更鲁棒的模型所有权验证方法。&lt;h4&gt;方法&lt;/h4&gt;提出一种指纹框架，将受害者模型和可疑模型的嵌入建模为点云，执行鲁棒的空间对齐和相似度测量，通过分析嵌入空间拓扑结构的几何特性建立EaaS模型所有权，而非依赖修改的训练样本或触发器。&lt;h4&gt;主要发现&lt;/h4&gt;将嵌入建模为点云的方法实现了鲁棒的空间对齐和相似度测量；该方法能有效抵抗RST攻击；在视觉和文本嵌入任务中验证了该方法的优越性和适用性。&lt;h4&gt;结论&lt;/h4&gt;该研究揭示了EaaS模型的固有特性，并为黑盒场景下EaaS模型的所有权验证提供了一种有前景的解决方案，通过几何分析而非传统水印技术提供更鲁棒的验证方法。&lt;h4&gt;翻译&lt;/h4&gt;特征嵌入已成为处理高维和复杂数据的核心技术，这使得嵌入即服务（EaaS）模型已在云环境中广泛部署。为了保护EaaS模型的知识产权，现有方法应用数字水印技术，通过修改训练样本或网络参数向EaaS模型注入特定的后门触发器。然而，这些方法不可避免地会产生可通过语义分析检测到的模式，并且容易受到几何变换（包括旋转、缩放和平移，RST）的影响。为了解决这个问题，我们提出了一种用于EaaS模型的指纹框架，而非仅仅改进现有的水印技术。与水印技术不同，所提出的方法通过分析嵌入空间拓扑结构的几何特性来建立EaaS模型所有权，而不是依赖修改的训练样本或触发器。关键创新在于将受害者模型和可疑模型的嵌入建模为点云，使我们能够执行鲁棒的空间对齐和相似度测量，这 inherently 抵抗RST攻击。在视觉和文本嵌入任务上评估的实验结果验证了其优越性和适用性。这项研究揭示了EaaS模型的固有特性，并为黑盒场景下EaaS模型的所有权验证提供了一种有前景的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feature embedding has become a cornerstone technology for processinghigh-dimensional and complex data, which results in that Embedding as a Service(EaaS) models have been widely deployed in the cloud. To protect theintellectual property of EaaS models, existing methods apply digitalwatermarking to inject specific backdoor triggers into EaaS models by modifyingtraining samples or network parameters. However, these methods inevitablyproduce detectable patterns through semantic analysis and exhibitsusceptibility to geometric transformations including rotation, scaling, andtranslation (RST). To address this problem, we propose a fingerprintingframework for EaaS models, rather than merely refining existing watermarkingtechniques. Different from watermarking techniques, the proposed methodestablishes EaaS model ownership through geometric analysis of embeddingspace's topological structure, rather than relying on the modified trainingsamples or triggers. The key innovation lies in modeling the victim andsuspicious embeddings as point clouds, allowing us to perform robust spatialalignment and similarity measurement, which inherently resists RST attacks.Experimental results evaluated on visual and textual embedding tasks verify thesuperiority and applicability. This research reveals inherent characteristicsof EaaS models and provides a promising solution for ownership verification ofEaaS models under the black-box scenario.</description>
      <author>example@mail.com (Hongjie Zhang, Zhiqi Zhao, Hanzhou Wu, Zhihua Xia, Athanasios V. Vasilakos)</author>
      <guid isPermaLink="false">2510.16706v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>A Bayesian Framework for Symmetry Inference in Chaotic Attractors</title>
      <link>http://arxiv.org/abs/2510.16509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种贝叶斯框架，用于从动力系统轨迹数据中检测对称性，解决了现有方法缺乏不确定性量化的问题，提高了对噪声的鲁棒性，并能够处理层次对称结构。&lt;h4&gt;背景&lt;/h4&gt;对称性检测是信号分析中的基本问题，能揭示底层结构和约束。当数据表现为动力系统轨迹时，对称性编码了动力系统的结构特性，实现模型简化、条件间比较和状态变化检测。现有最优传输方法依赖确定性阈值且缺乏不确定性量化，限制了鲁棒性和层次对称结构的解决能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个贝叶斯框架，将对称性检测构建为在候选子群格上的概率模型选择，使用基于Wasserstein距离的Gibbs后验，以解决不确定性量化问题，提高对噪声的鲁棒性，并处理层次对称结构。&lt;h4&gt;方法&lt;/h4&gt;提出贝叶斯框架，将对称性检测表述为候选子群格上的概率模型选择，使用基于Wasserstein距离的Gibbs后验。通过Metropolis-Hastings采样进行后验推断，建立在三个理论保证上：贝叶斯奥卡姆剃刀原理、共轭等变性和扰动下的稳定性界限。&lt;h4&gt;主要发现&lt;/h4&gt;建立了三个理论保证：(i)贝叶斯奥卡姆剃刀原理，倾向于与数据一致的最小对称性；(ii)共轭等变性确保框架独立性；(iii)扰动下的稳定性界限确保对噪声的鲁棒性。数值实验展示了高噪声和小样本量下准确的对称性恢复，应用人类步行动力学揭示了机械约束引起的对称性变化。&lt;h4&gt;结论&lt;/h4&gt;该贝叶斯框架有效解决了对称性检测中的不确定性量化问题，提高了对噪声的鲁棒性，并能够处理层次对称结构，在生物力学和动力系统的统计推断中具有实用价值。&lt;h4&gt;翻译&lt;/h4&gt;从数据中检测对称性是信号分析中的一个基本问题，它为底层结构和约束提供了见解。当数据表现为动力系统的轨迹时，对称性编码了动力系统的结构特性，这些特性能够实现模型简化、条件间的有原则比较以及检测状态变化。虽然最近的最优传输方法为这种情况下的数据驱动对称性检测提供了实用工具，但它们依赖于确定性阈值且缺乏不确定性量化，限制了它们对噪声的鲁棒性以及解决层次对称结构的能力。我们提出了一个贝叶斯框架，将对称性检测构建为在候选子群格上的概率模型选择，使用基于观测数据与群变换副本之间Wasserstein距离构建的Gibbs后验。我们建立了三个理论保证：(i)贝叶斯奥卡姆剃刀原理，倾向于与数据一致的最小对称性；(ii)共轭等变性确保了框架独立性；(iii)扰动下的稳定性界限确保了对噪声的鲁棒性。后验推断通过Metropolis-Hastings采样进行，在等变动力系统和合成点云上的数值实验展示了在高噪声和小样本量下准确的对称性恢复。在人类步行动力学的应用中揭示了机械约束引起的对称性变化，证明了该框架在生物力学和动力系统统计推断中的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting symmetry from data is a fundamental problem in signal analysis,providing insight into underlying structure and constraints. When data emergeas trajectories of dynamical systems, symmetries encode structural propertiesof the dynamics that enable model reduction, principled comparison acrossconditions, and detection of regime changes. While recent optimal transportmethods provide practical tools for data-driven symmetry detection in thissetting, they rely on deterministic thresholds and lack uncertaintyquantification, limiting robustness to noise and ability to resolvehierarchical symmetry structures. We present a Bayesian framework thatformulates symmetry detection as probabilistic model selection over a latticeof candidate subgroups, using a Gibbs posterior constructed from Wassersteindistances between observed data and group-transformed copies. We establishthree theoretical guarantees: $(i)$ a Bayesian Occam's razor favoring minimalsymmetry consistent with data, $(ii)$ conjugation equivariance ensuringframe-independence, and $(iii)$ stability bounds under perturbations forrobustness to noise. Posterior inference is performed via Metropolis-Hastingssampling and numerical experiments on equivariant dynamical systems andsynthetic point clouds demonstrate accurate symmetry recovery under high noiseand small sample sizes. An application to human gait dynamics reveals symmetrychanges induced by mechanical constraints, demonstrating the framework'sutility for statistical inference in biomechanical and dynamical systems.</description>
      <author>example@mail.com (Ziad Ghanem, Chang Hyunwoong, Preskella Mrad)</author>
      <guid isPermaLink="false">2510.16509v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data</title>
      <link>http://arxiv.org/abs/2510.16071v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多尺度神经网络算子(MNO)架构，用于三维非结构化点云上的计算流体动力学，通过分解三个尺度的信息来提高精度和可扩展性，在多个基准测试中表现优异。&lt;h4&gt;背景&lt;/h4&gt;现有神经网络算子在求解偏微分方程时虽然比传统求解器快几个数量级，但在精度和可扩展性方面仍存在局限，特别是在不规则域上具有丰富多尺度结构的流体流动问题中。&lt;h4&gt;目的&lt;/h4&gt;引入多尺度神经网络算子(MNO)架构，解决三维非结构化点云上的计算流体动力学问题，提高预测精度和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;MNO明确分解三个尺度的信息：全局维度缩减注意力模块处理长程依赖关系，局部图注意力模块处理邻域级相互作用，微观逐点注意力模块处理细粒度细节，这种设计保留了多尺度归纳偏置同时保持计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;在四个涵盖稳态和非稳态流动场景的基准测试上(最多30万个点)，MNO始终优于最先进的基线方法，减少5%到40%的预测误差，并在具有挑战性的三维CFD问题中表现出更强的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;明确的多尺度设计对神经网络算子至关重要，MNO可作为不规则域上学习复杂流体动力学的可扩展框架。&lt;h4&gt;翻译&lt;/h4&gt;神经网络算子已成为求解偏微分方程的强大数据驱动范式，比传统求解器快几个数量级。然而，现有方法在精度和可扩展性方面仍然有限，特别是在流体流动表现出丰富多尺度结构的不规则域上。在这项工作中，我们引入了多尺度神经网络算子(MNO)，这是一种用于三维非结构化点云上计算流体动力学(CFD)的新架构。MNO明确分解三个尺度的信息：全局维度缩减注意力模块用于长程依赖关系，局部图注意力模块用于邻域级相互作用，微观逐点注意力模块用于细粒度细节。这种设计保留了多尺度归纳偏置，同时保持计算效率。我们在四个不同的基准测试上评估了MNO，涵盖了最多30万个点的稳态和非稳态流动场景。在所有任务中，MNO始终优于最先进的基线方法，减少5%到40%的预测误差，并在具有挑战性的三维CFD问题中表现出改进的鲁棒性。我们的结果强调了明确的多尺度设计对神经网络算子的重要性，并将MNO确立为不规则域上学习复杂流体动力学的可扩展框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有神经算子在计算流体动力学(CFD)中处理不规则域时的局限，特别是对流体流动中丰富多尺度结构的建模不足问题。这个问题很重要，因为CFD在工程设计、气象预测等领域有广泛应用，而传统CFD计算成本高，难以实现实时计算；神经算子虽能提供更快速度，但在精度上仍落后于传统方法，特别是在处理复杂几何形状和动态域时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到物理量在流场中表现出强烈的多尺度效应（大尺度全局趋势、局部相互作用和细粒度点变化），而现有方法要么过于依赖全局建模牺牲局部细节，要么细粒度注意力机制计算成本过高。因此设计了MNO，包含三个互补的并行模块分别处理不同尺度信息。该方法借鉴了Transolver和LNO的低秩投影策略（全局模块）、Point Transformer的思想（局部模块），以及Encoder-MNO-Decoder的常见架构，但针对点云数据和多尺度特性进行了专门设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是显式地将信息分解为全局、局部和微观三个尺度，通过三个并行注意力模块分别捕获不同尺度的特征，并在每个块后融合这些特征。整体流程包括：1)输入处理接收3D点云数据；2)编码器将输入嵌入到潜在标记空间；3)MNO块处理（包含全局维度收缩注意力、局部图注意力和微观点级注意力三个并行模块）；4)特征融合三个模块的输出；5)解码器将处理后的特征映射回目标物理量；6)输出预测流场中的关键物理量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个针对3D无结构点云的多尺度神经算子；2)显式的多尺度分解设计，包含三个互补的注意力模块；3)直接在点云上处理，避免网格约束；4)统一框架同时提取全局、局部和细粒度流场表示。相比之前工作的不同：与规则域方法相比，MNO可直接处理不规则域；与不规则域方法相比，MNO明确考虑多尺度特性并提供更平衡的表示能力；与多尺度方法相比，MNO直接在点云上工作且不使用重复下采样/上采样，避免了信息丢失。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MNO通过显式的多尺度注意力机制设计，首次实现了在3D点云数据上高效准确地捕获流体流动的全局趋势、局部相互作用和细粒度细节，显著提高了不规则域上计算流体动力学任务的预测精度和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural operators have emerged as a powerful data-driven paradigm for solvingPartial Differential Equations (PDEs), offering orders-of-magnitudeacceleration over traditional solvers. However, existing approaches stillsuffer from limited accuracy and scalability, particularly on irregular domainswhere fluid flows exhibit rich multiscale structures. In this work, weintroduce the Multiscale Neural Operator (MNO), a new architecture forComputational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured pointclouds. MNO explicitly decomposes information across three scales: a globaldimension-shrinkage attention module for long-range dependencies, a local graphattention module for neighborhood-level interactions, and a micro point-wiseattention module for fine-grained details. This design preserves multiscaleinductive biases while remaining computationally efficient. We evaluate MNO onfour diverse benchmarks, covering both steady-state and unsteady flow scenarioswith up to 300K points. Across all tasks, MNO consistently outperformsstate-of-the-art baselines, reducing prediction errors by 5% to 40% anddemonstrating improved robustness in challenging 3D CFD problems. Our resultshighlight the importance of explicit multiscale design for neural operators andestablish MNO as a scalable framework for learning complex fluid dynamics onirregular domains.</description>
      <author>example@mail.com (Qinxuan Wang, Chuang Wang, Mingyu Zhang, Jingwei Sun, Peipei Yang, Shuo Tang, Shiming Xiang)</author>
      <guid isPermaLink="false">2510.16071v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception</title>
      <link>http://arxiv.org/abs/2510.17363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the IEEE/RSJ International Conference on Intelligent  Robots and Systems (IROS 2025). 8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为Multi-Mono-Hydra (M2H)的新型多任务学习框架，用于从单目图像中进行语义分割和深度、边缘及表面法线估计，在边缘设备上实现高效实时空间感知。&lt;h4&gt;背景&lt;/h4&gt;在边缘设备上部署实时空间感知需要高效的多任务模型，这些模型需要能够利用互补任务信息同时最小化计算开销。&lt;h4&gt;目的&lt;/h4&gt;开发一种优化的多任务学习框架，支持从单目图像中同时进行多种空间感知任务，并能在边缘设备上实时运行，为动态环境中的3D场景图构建提供基础。&lt;h4&gt;方法&lt;/h4&gt;M2H框架采用基于窗口的跨任务注意模块实现结构化特征交换，同时保留任务特定细节，提高预测一致性。框架基于轻量级ViT-based DINOv2主干网络构建，优化了实时部署性能。&lt;h4&gt;主要发现&lt;/h4&gt;M2H在NYUDv2数据集上超越最先进的多任务模型；在Hypersim上超越单任务深度和语义基线；在Cityscapes数据集上表现优异；同时保持笔记本电脑硬件上的计算效率；在真实世界数据验证中展现出实用性。&lt;h4&gt;结论&lt;/h4&gt;M2H通过创新的跨任务注意机制有效利用了任务间的互补信息，在多个基准测试和实际应用中表现出色，是一种适用于边缘设备的高效多任务空间感知解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在边缘设备上部署实时空间感知需要高效的多任务模型，这些模型能够利用互补任务信息同时最小化计算开销。本文介绍了Multi-Mono-Hydra (M2H)，一种新颖的多任务学习框架，专为从单目图像进行语义分割和深度、边缘及表面法线估计而设计。与传统依赖独立单任务模型或共享编码器-解码器架构的方法不同，M2H引入了基于窗口的跨任务注意模块，能够在保留任务特定细节的同时实现结构化特征交换，提高跨任务预测一致性。基于轻量级ViT-based DINOv2主干网络构建，M2H针对实时部署进行优化，并作为支持动态环境中3D场景图构建的单目空间感知系统的基础。全面评估显示，M2H在NYUDv2上超越最先进的多任务模型，在Hypersim上超越单任务深度和语义基线，在Cityscapes数据集上实现卓越性能，同时在笔记本电脑硬件上保持计算效率。除了基准测试外，M2H还在真实世界数据上得到验证，展示了其在空间感知任务中的实用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何在边缘设备上实现高效的多任务空间感知问题，特别是从单目图像同时进行语义分割和深度、边缘、表面法线估计。这个问题在现实中非常重要，因为自动驾驶系统、增强现实和机器人感知等应用需要实时理解环境，而边缘设备资源有限，需要高效的多任务模型来利用任务间的互补信息，同时最小化计算开销。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于Taskonomy的研究，认识到任务间关系可以减少监督需求并提高泛化能力。他们发现现有方法存在局限：局部交互保留了效率但限制了协同效应，全局注意力提供了丰富上下文但计算开销高。因此设计了一种平衡效率与协同效应的框架。作者借鉴了DINOv2的轻量级ViT主干网络、Swin Transformer的窗口注意力机制，并参考了注意力机制在其他多任务学习方法中的应用，如PAD-Net和MTI-Net。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入基于窗口的跨任务注意力模块实现结构化特征交换，同时保留任务特定细节，并使用轻量级ViT主干网络优化实时部署。整体流程：1) DINOv2编码器提取多尺度令牌表示；2) 通过MSTR块重组为空间特征图；3) MSF块生成任务特定特征；4) 双路径细化：WMCA捕获局部跨任务交互，GGFM聚合全局上下文；5) 融合局部和全局特征；6) 专用解码器头生成最终预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 窗口多任务交叉注意力(WMCA)模块，在局部窗口内高效交换信息；2) 全局门控特征合并(GGFM)模块，使用门控机制聚合全局上下文；3) 双路径细化策略，平衡特征交换深度与计算效率；4) 使用动态权重平均(DWA)平衡跨任务学习。相比之前工作，M2H在保持计算效率的同时实现了更好的性能，超越了多种最先进方法，特别适合边缘设备上的实时应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; M2H通过创新的窗口化跨任务注意力和双路径特征融合机制，实现了在边缘设备上高效运行的多任务空间感知系统，在保持计算效率的同时超越了现有最先进方法在多个任务和数据集上的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deploying real-time spatial perception on edge devices requires efficientmulti-task models that leverage complementary task information while minimizingcomputational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novelmulti-task learning framework designed for semantic segmentation and depth,edge, and surface normal estimation from a single monocular image. Unlikeconventional approaches that rely on independent single-task models or sharedencoder-decoder architectures, M2H introduces a Window-Based Cross-TaskAttention Module that enables structured feature exchange while preservingtask-specific details, improving prediction consistency across tasks. Built ona lightweight ViT-based DINOv2 backbone, M2H is optimized for real-timedeployment and serves as the foundation for monocular spatial perceptionsystems supporting 3D scene graph construction in dynamic environments.Comprehensive evaluations show that M2H outperforms state-of-the-art multi-taskmodels on NYUDv2, surpasses single-task depth and semantic baselines onHypersim, and achieves superior performance on the Cityscapes dataset, allwhile maintaining computational efficiency on laptop hardware. Beyondbenchmarks, M2H is validated on real-world data, demonstrating its practicalityin spatial perception tasks.</description>
      <author>example@mail.com (U. V. B. L Udugama, George Vosselman, Francesco Nex)</author>
      <guid isPermaLink="false">2510.17363v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2510.17274v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In proceedings of IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'即插即预测'(PnF)的即插即用方法，通过多模态大语言模型增强现有运动预测模型，提高自动驾驶系统在多样化实际场景中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;当前自动驾驶系统依赖专用模型进行感知和运动预测，在标准条件下表现可靠，但在多样化实际场景中泛化且经济高效地适应仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新方法解决自动驾驶系统在多样化实际场景中的泛化问题。&lt;h4&gt;方法&lt;/h4&gt;PnF方法利用自然语言描述复杂场景的优势，设计提示从多模态大语言模型中提取结构化场景理解，并将这些信息蒸馏为可学习的嵌入，增强现有行为预测模型，无需微调即可实现性能提升。&lt;h4&gt;主要发现&lt;/h4&gt;该方法利用多模态大语言模型的零样本推理能力，显著提高了运动预测性能，同时不需要微调，使其易于实际采用。&lt;h4&gt;结论&lt;/h4&gt;在Waymo OpenMotion数据集和nuScenes数据集上使用两种最先进的运动预测模型验证了该方法，证明了在两个基准测试中都有一致的性能改进。&lt;h4&gt;翻译&lt;/h4&gt;当前的自动驾驶系统依赖于用于感知和预测运动的专用模型，这些模型在标准条件下表现出可靠的性能。然而，泛化到多样化的实际场景并保持成本效益仍然是一个重大挑战。为此，我们提出了即插即预测(PnF)，一种即插即用的方法，通过多模态大语言模型(MLLMs)增强现有的运动预测模型。PnF基于自然语言能更有效地描述和处理复杂场景的见解，实现了针对特定行为的快速适应。我们设计了提示从MLLMs中提取结构化的场景理解，并将这些信息蒸馏为可学习的嵌入，以增强现有的行为预测模型。我们的方法利用MLLMs的零样本推理能力，显著提高了运动预测性能，同时不需要微调，使其易于实际采用。我们在Waymo OpenMotion数据集和nuScenes数据集上使用两种最先进的运动预测模型验证了我们的方法，证明了在两个基准测试中都有一致的性能改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶系统在多样化现实场景中运动预测模型的泛化能力问题。这个问题很重要，因为自动驾驶车辆不可避免会遇到训练数据中未涵盖的罕见情况(长尾案例)，而持续收集大量数据和重新训练系统的成本过高。提高系统在复杂场景下的泛化能力对确保安全性和实用性至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从多模态大语言模型(MLLM)的进展获得灵感，认识到语言能更好地描述复杂场景。他们设计提示从MLLM中提取结构化场景理解，并将其转化为可学习的嵌入来增强现有预测模型。借鉴了现有运动预测模型(如Wayformer和MotionLM)的架构，以及MLLM在其他自动驾驶应用中的使用，但创新点在于采用零样本推理而非微调方式，保留了MLLM的通用能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用MLLM的零样本推理能力增强现有运动预测模型，通过自然语言描述更有效地处理复杂场景。整体流程包括：1)视觉语义分析器(VSA)提取代理特定语义；2)场景分类器(SC)提供场景级理解；3)通过学习的信息增益机制将MLLM提取的特征选择性整合到预测模型中。整个过程作为即插即用组件，无需对MLLM进行微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)即插即用框架PnF；2)利用MLLM零样本推理能力；3)多模态提示设计；4)选择性信息整合机制；5)双重分析组件(代理级和场景级)。相比之前工作，PnF不需要对MLLM进行微调，保留了其通用能力；不同于直接映射原始数据的EMMA模型，PnF专注于增强运动预测任务；相比其他需要修改架构的方法，PnF可作为轻量级组件集成到现有系统。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的Plug-and-Forecast方法通过即插即用多模态大语言模型的零样本推理能力，显著提升了自动驾驶系统中运动预测模型的性能，特别是在处理复杂和罕见场景时无需对语言模型进行微调。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current autonomous driving systems rely on specialized models for perceivingand predicting motion, which demonstrate reliable performance in standardconditions. However, generalizing cost-effectively to diverse real-worldscenarios remains a significant challenge. To address this, we proposePlug-and-Forecast (PnF), a plug-and-play approach that augments existing motionforecasting models with multimodal large language models (MLLMs). PnF builds onthe insight that natural language provides a more effective way to describe andhandle complex scenarios, enabling quick adaptation to targeted behaviors. Wedesign prompts to extract structured scene understanding from MLLMs and distillthis information into learnable embeddings to augment existing behaviorprediction models. Our method leverages the zero-shot reasoning capabilities ofMLLMs to achieve significant improvements in motion prediction performance,while requiring no fine-tuning -- making it practical to adopt. We validate ourapproach on two state-of-the-art motion forecasting models using the Waymo OpenMotion Dataset and the nuScenes Dataset, demonstrating consistent performanceimprovements across both benchmarks.</description>
      <author>example@mail.com (Katie Luo, Jingwei Ji, Tong He, Runsheng Xu, Yichen Xie, Dragomir Anguelov, Mingxing Tan)</author>
      <guid isPermaLink="false">2510.17274v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes</title>
      <link>http://arxiv.org/abs/2510.16714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://scenecot.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的SCENECOT框架，首次将思维链推理应用于3D场景理解，解决了现有3D大型语言模型在实现基于场景的问答方面的困难。&lt;h4&gt;背景&lt;/h4&gt;现有关于3D大型语言模型的研究在实现基于场景的问答方面仍然存在困难，主要原因是对类人场景-对象基础推理机制的探索不足。&lt;h4&gt;目的&lt;/h4&gt;通过提出一个新颖的框架来填补3D场景理解中类人推理机制的空白。&lt;h4&gt;方法&lt;/h4&gt;作者在3D场景中引入了基础的思维链推理方法（SCENECOT），将复杂推理任务解耦为更简单的问题，并基于多模态专家模块构建视觉线索。同时开发了SCENECOT-185K数据集，包含185K个高质量实例。&lt;h4&gt;主要发现&lt;/h4&gt;在各种复杂的3D场景推理基准上的广泛实验表明，新框架在保持高基础问答连贯性的同时实现了强大的性能。&lt;h4&gt;结论&lt;/h4&gt;思维链推理首次成功应用于3D场景理解，实现了类人的逐步推理，并显示出扩展到更广泛的3D场景理解场景的潜力。&lt;h4&gt;翻译&lt;/h4&gt;现有关于3D大型语言模型的研究在实现基于场景的问答方面仍然存在困难，主要原因是对类人场景-对象基础推理机制的探索不足。本文通过提出一个新颖的框架来填补这一空白。我们首先在3D场景中引入了一种基础的思维链推理方法（SCENECOT），将复杂的推理任务解耦为更简单、更易管理的问题，并基于多模态专家模块构建相应的视觉线索。为此，我们开发了SCENECOT-185K，这是第一个大规模的基础思维链推理数据集，包含185K个高质量实例。在各种复杂的3D场景推理基准上的广泛实验表明，我们的新框架在保持高基础问答连贯性的同时实现了强大的性能。据我们所知，这是思维链推理首次成功应用于3D场景理解，实现了类人的逐步推理，并显示出扩展到更广泛的3D场景理解场景的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D大语言模型难以实现接地式问答的问题，即模型生成的回答与3D场景中的实际对象和空间关系缺乏明确联系。这个问题很重要，因为3D场景理解是构建人类级别具身智能体的基础能力，而现有模型往往产生看似合理但未与场景关联的答案，导致接地-问答连贯性差，阻碍了AI系统在需要精确空间理解的应用中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过识别3D推理的挑战（导航大空间、解释复杂空间关系、处理部分可观察性），借鉴了语言领域的思维链（CoT）推理方法，将复杂问题分解为可管理的子问题。他们还参考了2D视觉-语言模型中的CoT应用，将其扩展到3D场景理解。此外，作者结合了多模态大语言模型、专门的3D-VL和2D-VL模型以及符号引擎等现有技术，构建了SCENECOT框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将复杂的3D场景推理任务分解为四个明确的阶段，每个阶段都引入明确的接地信号，确保逐步推理并提高接地-问答连贯性。整体流程包括：1）任务识别和分析，识别问题类型和初始分析；2）任务相关区域定位，缩小推理空间；3）实体和属性接地，使用多模态专家模块关联目标对象；4）接地推理，集成中间结果生成最终答案。整个过程使用特殊标记表示不同推理阶段，结合多种模型共同完成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）SCENECOT框架，首次将思维链推理应用于3D场景理解；2）SCENECOT-185K数据集，首个大规模接地CoT推理数据集；3）四阶段推理流程，明确分解复杂任务；4）显著提高接地-问答连贯性。相比之前工作，SCENECOT采用逐步推理而非单步任务，在每个阶段都引入明确的接地信号，专门设计了针对3D场景的推理数据集，并在接地-问答连贯性方面表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCENECOT通过引入首个专门为3D场景设计的接地式思维链推理框架和数据集，显著提升了AI系统在复杂3D环境中的类人推理能力和答案与场景的连贯性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing research on 3D Large Language Models (LLMs) still struggles toachieve grounded question-answering, primarily due to the under-exploration ofthe mech- anism of human-like scene-object grounded reasoning. This paperbridges the gap by presenting a novel framework. We first introduce a groundedChain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling acomplex reasoning task into simpler and manageable problems, and buildingcorresponding visual clues based on multimodal expert modules. To enable such amethod, we develop SCENECOT-185K, the first large-scale grounded CoT reasoningdataset, consisting of 185K high-quality instances. Extensive experimentsacross various complex 3D scene reasoning benchmarks demonstrate that our newframework achieves strong performance with high grounding-QA coherence. To thebest of our knowledge, this is the first successful application of CoTreasoning to 3D scene understanding, enabling step-by-step human-like reasoningand showing potential for extension to broader 3D scene understandingscenarios.</description>
      <author>example@mail.com (Xiongkun Linghu, Jiangyong Huang, Ziyu Zhu, Baoxiong Jia, Siyuan Huang)</author>
      <guid isPermaLink="false">2510.16714v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Structured Interfaces for Automated Reasoning with 3D Scene Graphs</title>
      <link>http://arxiv.org/abs/2510.16643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合大型语言模型与三维场景图的新方法，通过检索增强生成技术选择相关场景图子集，并使用图数据库和Cypher查询语言接口作为工具，提高自然语言与机器人世界表示之间的连接效率。&lt;h4&gt;背景&lt;/h4&gt;为了使机器人能够理解和响应用户自然语言输入，需要将自然语言与机器人对世界的基础表示连接起来。目前，大型语言模型和三维场景图已成为热门选择，但现有方法将场景图编码为LLM上下文窗口内的序列化文本，无法有效处理大型或丰富的场景图。&lt;h4&gt;目的&lt;/h4&gt;解决使用大型语言模型与三维场景图连接自然语言时的可扩展性问题，特别是处理大型或丰富场景图的挑战。&lt;h4&gt;方法&lt;/h4&gt;采用检索增强生成技术选择与任务相关的三维场景图子集，将场景图编码到图数据库中，并提供Cypher查询语言接口作为大型语言模型的工具，使其能够检索与语言连接相关的数据。&lt;h4&gt;主要发现&lt;/h4&gt;在指令跟随和场景问答任务上的评估表明，使用Cypher作为三维场景图的接口能更好地扩展到大型、丰富的图形，显著提高语言连接任务性能，同时大幅减少场景图内容的token数量。&lt;h4&gt;结论&lt;/h4&gt;通过将三维场景图存储在图数据库并提供Cypher接口作为工具，可有效解决大型场景图的表示问题，提高自然语言与机器人世界表示之间的连接效率，减少计算资源消耗。&lt;h4&gt;翻译&lt;/h4&gt;为了使机器人具备理解和响应用户自然语言输入的能力，自然语言必须与机器人对世界的基础表示相连接。最近，大型语言模型和三维场景图已成为连接自然语言和表示世界的流行选择。在这项工作中，我们解决了使用大型语言模型与三维场景图连接自然语言的挑战。现有方法将场景图编码为大型语言模型上下文窗口内的序列化文本，但这种编码无法扩展到大型或丰富的三维场景图。相反，我们提议使用一种检索增强生成形式来选择与任务相关的三维场景图子集。我们将三维场景图编码在图数据库中，并提供Cypher查询语言接口作为大型语言模型的工具，使其能够检索与语言连接相关的数据。我们在指令跟随和场景问答任务上评估了我们的方法，并与基线上下文窗口和代码生成方法进行了比较。我们的结果表明，使用Cypher作为三维场景图的接口，在本地和基于云的模型上都能显著更好地扩展到大型、丰富的图形。这大大提高了语言连接任务的性能，同时大幅减少了场景图内容的token数量。视频补充材料可在https://www.youtube.com/watch?v=zY_YI9giZSA获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何让大型语言模型（LLMs）有效地利用3D场景图（3DSGs）来理解并处理自然语言指令的问题。这个问题非常重要，因为它关系到机器人能否理解并执行人类的自然语言指令，对于人机交互至关重要。传统方法将3D场景图序列化为文本放入LLMs上下文窗口，但这种方法无法扩展到大型场景图、难以处理空间关系推理、且依赖LLMs进行定量推理（而这并非其强项）。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，特别是序列化方法无法处理大型场景图的问题。他们考虑了两种替代方法：使用LLMs过滤节点和基于向量的RAG，但发现这些方法各有不足。作者借鉴了现有的检索增强生成（RAG）技术和图数据库技术，结合'代理AI'概念，设计出使用Cypher查询语言作为LLMs和3D场景图之间接口的新方法。他们特别受到GraphRAG技术的启发，将其专门应用于3D场景图和LLMs的结合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图检索增强生成（GraphRAG）方法，通过Cypher查询语言作为大型语言模型（LLMs）和3D场景图（3DSGs）之间的接口。整体流程包括：1)将3D场景图编码到图数据库中；2)向LLMs提供Cypher查询作为工具；3)当用户输入自然语言时，LLMs决定是否需要查询场景图；4)如果需要，LLMs生成一个或多个Cypher查询；5)执行查询并获取结果；6)使用查询结果作为上下文生成最终响应（对于指令跟随任务转换为PDDL目标，对于问答任务直接回答问题）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用Cypher查询语言作为结构化接口；2)将RAG技术扩展到图数据结构（GraphRAG）；3)采用代理AI方法允许LLMs主动决定如何与场景图交互；4)利用图数据库的几何空间索引功能处理定量推理；5)有效处理大型、丰富的3D场景图。相比之前的工作，本文不再将整个场景图序列化为文本，而是按需查询；不依赖向量检索，而是使用结构化的图查询语言；不依赖LLMs进行定量推理，而是利用图数据库的专门功能；并采用更灵活的代理交互方式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种使用Cypher查询语言作为大型语言模型和3D场景图之间结构化接口的新方法，通过图检索增强生成技术，使机器人系统能够有效地理解和执行自然语言指令，同时解决了传统方法在处理大型场景图和定量推理方面的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In order to provide a robot with the ability to understand and react to auser's natural language inputs, the natural language must be connected to therobot's underlying representations of the world. Recently, large languagemodels (LLMs) and 3D scene graphs (3DSGs) have become a popular choice forgrounding natural language and representing the world. In this work, we addressthe challenge of using LLMs with 3DSGs to ground natural language. Existingmethods encode the scene graph as serialized text within the LLM's contextwindow, but this encoding does not scale to large or rich 3DSGs. Instead, wepropose to use a form of Retrieval Augmented Generation to select a subset ofthe 3DSG relevant to the task. We encode a 3DSG in a graph database and providea query language interface (Cypher) as a tool to the LLM with which it canretrieve relevant data for language grounding. We evaluate our approach oninstruction following and scene question-answering tasks and compare againstbaseline context window and code generation methods. Our results show thatusing Cypher as an interface to 3D scene graphs scales significantly better tolarge, rich graphs on both local and cloud-based models. This leads to largeperformance improvements in grounded language tasks while also substantiallyreducing the token count of the scene graph content. A video supplement isavailable at https://www.youtube.com/watch?v=zY_YI9giZSA.</description>
      <author>example@mail.com (Aaron Ray, Jacob Arkin, Harel Biggie, Chuchu Fan, Luca Carlone, Nicholas Roy)</author>
      <guid isPermaLink="false">2510.16643v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2510.16410v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了REALM，一个创新的MLLM-agent框架，实现了开放世界的基于推理的3D物体分割，无需大量3D特定后训练。&lt;h4&gt;背景&lt;/h4&gt;在视觉和机器人领域，弥合复杂人类指令与精确3D物体定位之间的差距仍然是一个重大挑战。现有的3D分割方法难以解释模糊的、基于推理的指令，而擅长此类推理的2D视觉-语言模型则缺乏内在的3D空间理解。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够理解复杂人类指令并进行精确3D物体分割的方法，无需大量3D特定后训练。&lt;h4&gt;方法&lt;/h4&gt;直接在3D高斯溅射表示上进行分割，利用其渲染逼真新视图的能力。提出全局到局部空间定位策略：先并行输入多个全局视图到MLLM代理进行粗略定位，聚合响应识别目标物体；然后合成物体的特写新视图进行细粒度局部分割，获得准确且一致的3D掩码。&lt;h4&gt;主要发现&lt;/h4&gt;REALM在解释LERF、3D-OVS和REALM3D基准测试中的显式和隐式指令方面取得了显著性能，并能支持一系列3D交互任务。&lt;h4&gt;结论&lt;/h4&gt;REALM代理框架具有实用性和多功能性，能够无缝支持物体移除、替换和风格转换等多种3D交互任务。&lt;h4&gt;翻译&lt;/h4&gt;弥合复杂人类指令与精确3D物体定位之间的差距在视觉和机器人领域仍然是一个重大挑战。现有的3D分割方法往往难以解释模糊的、基于推理的指令，而擅长此类推理的2D视觉-语言模型则缺乏内在的3D空间理解。在本文中，我们介绍了REALM，一个创新的MLLM-agent框架，能够实现开放世界的基于推理的分割，而无需大量的3D特定后训练。我们直接在3D高斯溅射表示上进行分割，利用其能够渲染高度适合MLLM理解的逼真新视图的能力。由于直接将一个或多个渲染视图输入到MLLM可能导致对视角选择的高度敏感性，我们提出了一种新颖的全局到局部空间定位策略。具体来说，多个全局视图首先并行输入到MLLM代理中进行粗略定位，聚合响应以稳健地识别目标物体。然后，合成物体的几个特写新视图以执行细粒度的局部分割，从而获得准确且一致的3D掩码。大量实验表明，REALM在解释LERF、3D-OVS和我们新引入的REALM3D基准测试中的显式和隐式指令方面取得了显著性能。此外，我们的代理框架无缝支持一系列3D交互任务，包括物体移除、替换和风格转换，展示了其实用性和多功能性。项目页面：https://ChangyueShi.github.io/REALM。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何弥合复杂人类指令和精确3D物体定位之间的差距。这个问题重要是因为AI系统需要理解并能够通过自然语言与3D世界交互，这对未来机器人和人机协作至关重要。目前，3D分割方法难以解释模糊的推理指令，而2D视觉语言模型又缺乏3D空间理解能力，这限制了AI在现实场景中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有方法各有局限：3D分割方法擅长直接查询但缺乏推理能力，而2D视觉语言模型能推理但缺乏3D空间意识。作者借鉴了3D高斯溅射(3DGS)作为3D世界的高保真表示，结合SAM进行实例分割，并利用多模态大语言模型(MLLM)进行推理。为解决视角选择敏感性问题，作者设计了全局到局部空间定位策略，通过多视角聚合提高分割准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用MLLM的推理能力和3DGS的高保真表示，实现开放世界的3D推理分割和编辑。整体流程包括：1)构建3D特征场为每个高斯基元分配身份特征；2)使用MLLM基础视觉分割器(LMSeg)进行图像级推理；3)通过全局到局部空间定位(GLSpaG)策略聚合多视图结果，先从全局视角粗略定位目标，再从局部视角细粒度分割；4)基于分割结果执行各种3D编辑任务如移除、替换和风格转换。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)REALM框架实现无需大量3D特定后训练的开放世界3D推理分割；2)MLLM基础实例分割器结合MLLM和SAM的能力；3)全局到局部空间定位策略提高分割准确性；4)REALM3D基准数据集促进研究。相比之前工作，REALM能处理需要推理空间关系、语义属性或常识的模糊指令，将2D推理能力提升到3D领域，支持多种3D交互任务，解决了视角选择敏感性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; REALM通过将多模态大语言模型的推理能力与3D高斯溅射的高保真表示相结合，实现了开放世界中的3D推理分割和编辑，解决了现有方法在处理模糊、基于推理的3D指令时的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bridging the gap between complex human instructions and precise 3D objectgrounding remains a significant challenge in vision and robotics. Existing 3Dsegmentation methods often struggle to interpret ambiguous, reasoning-basedinstructions, while 2D vision-language models that excel at such reasoning lackintrinsic 3D spatial understanding. In this paper, we introduce REALM, aninnovative MLLM-agent framework that enables open-world reasoning-basedsegmentation without requiring extensive 3D-specific post-training. We performsegmentation directly on 3D Gaussian Splatting representations, capitalizing ontheir ability to render photorealistic novel views that are highly suitable forMLLM comprehension. As directly feeding one or more rendered views to the MLLMcan lead to high sensitivity to viewpoint selection, we propose a novelGlobal-to-Local Spatial Grounding strategy. Specifically, multiple global viewsare first fed into the MLLM agent in parallel for coarse-level localization,aggregating responses to robustly identify the target object. Then, severalclose-up novel views of the object are synthesized to perform fine-grainedlocal segmentation, yielding accurate and consistent 3D masks. Extensiveexperiments show that REALM achieves remarkable performance in interpretingboth explicit and implicit instructions across LERF, 3D-OVS, and our newlyintroduced REALM3D benchmarks. Furthermore, our agent framework seamlesslysupports a range of 3D interaction tasks, including object removal,replacement, and style transfer, demonstrating its practical utility andversatility. Project page: https://ChangyueShi.github.io/REALM.</description>
      <author>example@mail.com (Changyue Shi, Minghao Chen, Yiping Mao, Chuxiao Yang, Xinyuan Hu, Jiajun Ding, Zhou Yu)</author>
      <guid isPermaLink="false">2510.16410v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>SWIR-LightFusion: Multi-spectral Semantic Fusion of Synthetic SWIR with Thermal IR (LWIR/MWIR) and RGB</title>
      <link>http://arxiv.org/abs/2510.13404v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种通过合成短波红外（SWIR）图像来增强多模态融合框架的方法，以改善在能见度不良条件下的场景理解，同时保持实时性能。&lt;h4&gt;背景&lt;/h4&gt;在能见度不良条件下进行场景理解是监控和自主导航系统面临的重大挑战。传统成像模式（如RGB和热红外）在融合时往往无法提供全面的场景信息，特别是在大气干扰或照明不足的条件下。SWIR成像虽然能够穿透大气干扰并提供更清晰的材料区分能力，但其广泛应用面临主要障碍是缺乏公开可用的SWIR数据集。&lt;h4&gt;目的&lt;/h4&gt;解决SWIR数据集稀缺的问题，通过从现有LWIR数据合成生成类似SWIR的结构/对比度提示图像，并开发一种多模态融合框架，整合合成的SWIR、LWIR和RGB模式，以提高在不良能见度条件下的场景理解能力。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种从现有LWIR数据合成生成类似SWIR的结构/对比度提示图像的方法，然后提出了一种多模态融合框架，整合合成的SWIR、LWIR和RGB模式。该框架采用优化的编码器-解码器神经网络架构，具有模态特定的编码器和softmax门控融合头。&lt;h4&gt;主要发现&lt;/h4&gt;在公共RGB-LWIR基准测试集和额外的私有真实RGB-MWIR-SWIR数据集上的综合实验表明，合成SWIR增强融合框架提高了融合图像质量（对比度、边缘定义、结构保真度），同时保持实时性能。研究还添加了公平的三模态基线和级联三模态变体。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的合成SWIR增强融合框架在监控和自主系统等实际应用中具有巨大潜力，能够有效改善在能见度不良条件下的场景理解能力。&lt;h4&gt;翻译&lt;/h4&gt;在能见度不良条件下增强场景理解仍然是监控和自主导航系统的关键挑战。传统成像模式，如RGB和热红外（MWIR/LWIR），在融合时往往无法提供全面的场景信息，特别是在大气干扰或照明不足的条件下。为了解决这些局限性，短波红外（SWIR）成像已成为一种有前景的模式，因为它能够穿透大气干扰并提供更清晰的材料区分能力。然而，基于SWIR系统的进步和广泛应用面临重大障碍，主要是由于公开可用的SWIR数据集稀缺。为应对这一挑战，我们的研究提出了一种使用先进的对比度增强技术从现有LWIR数据合成生成类似SWIR的结构/对比度提示（不声称光谱再现）图像的方法。随后，我们提出了一种多模态融合框架，整合合成的SWIR、LWIR和RGB模式，采用优化的编码器-解码器神经网络架构，具有模态特定的编码器和softmax门控融合头。在公共RGB-LWIR基准（M3FD、TNO、CAMEL、MSRS、RoadScene）和额外的私有真实RGB-MWIR-SWIR数据集上的综合实验表明，我们的合成SWIR增强融合框架提高了融合图像质量（对比度、边缘定义、结构保真度），同时保持实时性能。我们还添加了公平的三模态基线（LP、LatLRR、GFF）和U2Fusion/SwinFusion的级联三模态变体，采用统一协议。结果突显了在监控和自主系统中实际应用的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enhancing scene understanding in adverse visibility conditions remains acritical challenge for surveillance and autonomous navigation systems.Conventional imaging modalities, such as RGB and thermal infrared (MWIR /LWIR), when fused, often struggle to deliver comprehensive scene information,particularly under conditions of atmospheric interference or inadequateillumination. To address these limitations, Short-Wave Infrared (SWIR) imaginghas emerged as a promising modality due to its ability to penetrate atmosphericdisturbances and differentiate materials with improved clarity. However, theadvancement and widespread implementation of SWIR-based systems facesignificant hurdles, primarily due to the scarcity of publicly accessible SWIRdatasets. In response to this challenge, our research introduces an approach tosynthetically generate SWIR-like structural/contrast cues (without claimingspectral reproduction) images from existing LWIR data using advanced contrastenhancement techniques. We then propose a multimodal fusion frameworkintegrating synthetic SWIR, LWIR, and RGB modalities, employing an optimizedencoder-decoder neural network architecture with modality-specific encoders anda softmax-gated fusion head. Comprehensive experiments on public RGB-LWIRbenchmarks (M3FD, TNO, CAMEL, MSRS, RoadScene) and an additional private realRGB-MWIR-SWIR dataset demonstrate that our synthetic-SWIR-enhanced fusionframework improves fused-image quality (contrast, edge definition, structuralfidelity) while maintaining real-time performance. We also add fair trimodalbaselines (LP, LatLRR, GFF) and cascaded trimodal variants ofU2Fusion/SwinFusion under a unified protocol. The outcomes highlightsubstantial potential for real-world applications in surveillance andautonomous systems.</description>
      <author>example@mail.com (Muhammad Ishfaq Hussain, Ma Van Linh, Zubia Naz, Unse Fatima, Yeongmin Ko, Moongu Jeon)</author>
      <guid isPermaLink="false">2510.13404v2</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence</title>
      <link>http://arxiv.org/abs/2510.16555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Urban-R1框架，一种基于强化学习的后训练方法，用于解决城市基础模型中的地域偏见问题，有效提升了跨区域泛化能力。&lt;h4&gt;背景&lt;/h4&gt;快速城市化加剧了对城市通用智能(UGI)的需求，即能够理解和推理复杂城市环境的AI系统。现有使用监督微调(SFT)构建的城市基础模型存在持续的地域偏见，产生区域倾斜的预测和有限的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出Urban-R1框架，使多模态大型语言模型(MLLMs)与UGI目标保持一致，缓解地域偏见并提高跨区域泛化能力。&lt;h4&gt;方法&lt;/h4&gt;Urban-R1采用组相对策略优化(GRPO)来优化跨地理群体的推理，并使用城市区域画像作为代理任务，从多模态城市数据提供可衡量的奖励。&lt;h4&gt;主要发现&lt;/h4&gt;在不同地区和任务的广泛实验中，Urban-R1有效缓解了地域偏见，改善了跨区域泛化能力，性能优于监督微调训练和闭源模型。&lt;h4&gt;结论&lt;/h4&gt;强化学习对齐是迈向公平和可信城市智能的有前景的途径。&lt;h4&gt;翻译&lt;/h4&gt;快速城市化加剧了对城市通用智能(UGI)的需求，UGI指的是能够理解和推理复杂城市环境的AI系统。最近的研究使用监督微调(SFT)构建了城市基础模型，但这些模型存在持续的地域偏见，产生区域倾斜的预测和有限的泛化能力。为此，我们提出Urban-R1，一个基于强化学习的后训练框架，使MLLMs与UGI目标保持一致。Urban-R1采用组相对策略优化(GRPO)来优化跨地理群体的推理，并使用城市区域画像作为代理任务，从多模态城市数据提供可衡量的奖励。在不同地区和任务的广泛实验表明，Urban-R1有效缓解了地域偏见并改善了跨区域泛化能力，性能优于监督微调训练和闭源模型。我们的结果强调了强化学习对齐作为迈向公平和可信城市智能的有前景途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid urbanization intensifies the demand for Urban General Intelligence(UGI), referring to AI systems that can understand and reason about complexurban environments. Recent studies have built urban foundation models usingsupervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibitpersistent geospatial bias, producing regionally skewed predictions and limitedgeneralization. To this end, we propose Urban-R1, a reinforcementlearning-based post-training framework that aligns MLLMs with the objectives ofUGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimizereasoning across geographic groups and employs urban region profiling as aproxy task to provide measurable rewards from multimodal urban data. Extensiveexperiments across diverse regions and tasks show that Urban-R1 effectivelymitigates geo-bias and improves cross-region generalization, outperforming bothSFT-trained and closed-source models. Our results highlight reinforcementlearning alignment as a promising pathway toward equitable and trustworthyurban intelligence.</description>
      <author>example@mail.com (Qiongyan Wang, Xingchen Zou, Yutian Jiang, Haomin Wen, Jiaheng Wei, Qingsong Wen, Yuxuan Liang)</author>
      <guid isPermaLink="false">2510.16555v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Lingua Custodi's participation at the WMT 2025 Terminology shared task</title>
      <link>http://arxiv.org/abs/2510.17504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究BERT-based跨语言句子嵌入方法，结合单语和跨语言表征学习技术，显著减少所需训练数据量，并在多语言任务上取得优异性能。&lt;h4&gt;背景&lt;/h4&gt;BERT在单语句子嵌入学习中表现有效，但BERT-based跨语言句子嵌入尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;系统研究学习多语言句子嵌入的方法，结合单语和跨语言表征的最佳方法。&lt;h4&gt;方法&lt;/h4&gt;结合掩码语言建模(MLM)、翻译语言建模(TLM)、双编码器翻译排序和加性边际softmax等方法。&lt;h4&gt;主要发现&lt;/h4&gt;引入预训练多语言语言模型可将实现良好性能所需的并行训练数据量减少80%；组合方法产生的模型在112种语言上达到83.7%的双语检索准确率，高于LASER的65.5%；在单语迁移学习基准测试中仍具竞争力；使用该方法挖掘的并行数据可训练出具有竞争力的NMT模型。&lt;h4&gt;结论&lt;/h4&gt;公开发布了109+语言的最佳多语言句子嵌入模型。&lt;h4&gt;翻译&lt;/h4&gt;虽然BERT是学习单语句子嵌入用于语义相似性和基于嵌入的迁移学习的有效方法，但基于BERT的跨语言句子嵌入尚未被探索。我们通过结合学习单语和跨语言表征的最佳方法，系统研究了学习多语言句子嵌入的方法，包括：掩码语言建模(MLM)、翻译语言建模(TLM)、双编码器翻译排序和加性边际softmax。我们证明，引入预训练多语言语言模型可将实现良好性能所需的并行训练数据量减少80%。组合这些最佳方法产生的模型在Tatoeba的112种语言上达到83.7%的双语检索准确率，远高于LASER的65.5%，同时在单语迁移学习基准测试中仍保持竞争力。使用我们的最佳模型从CommonCrawl挖掘的并行数据被证明可以训练出具有竞争力的NMT模型用于en-zh和en-de。我们在https://tfhub.dev/google/LaBSE上公开了我们的最佳多语言句子嵌入模型，适用于109+种语言。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While BERT is an effective method for learning monolingual sentenceembeddings for semantic similarity and embedding based transfer learning BERTbased cross-lingual sentence embeddings have yet to be explored. Wesystematically investigate methods for learning multilingual sentenceembeddings by combining the best methods for learning monolingual andcross-lingual representations including: masked language modeling (MLM),translation language modeling (TLM), dual encoder translation ranking, andadditive margin softmax. We show that introducing a pre-trained multilinguallanguage model dramatically reduces the amount of parallel training datarequired to achieve good performance by 80%. Composing the best of thesemethods produces a model that achieves 83.7% bi-text retrieval accuracy over112 languages on Tatoeba, well above the 65.5 achieved by LASER, while stillperforming competitively on monolingual transfer learning benchmarks. Paralleldata mined from CommonCrawl using our best model is shown to train competitiveNMT models for en-zh and en-de. We publicly release our best multilingualsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.</description>
      <author>example@mail.com (Jingshu Liu, Raheel Qader, Gaëtan Caillaut, Mariam Nakhlé)</author>
      <guid isPermaLink="false">2510.17504v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting</title>
      <link>http://arxiv.org/abs/2510.17408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的方法，将可信人工智能与节能机械臂相结合，用于智能垃圾分类和分拣。该系统通过使用MobileNetV2迁移学习增强的卷积神经网络，准确地将废物分为六类：塑料、玻璃、金属、纸张、纸板和垃圾。&lt;h4&gt;背景&lt;/h4&gt;城市废物管理需要智能化的解决方案来提高分类效率和可持续性。&lt;h4&gt;目的&lt;/h4&gt;开发一个结合可信人工智能和节能机械臂的系统，用于智能垃圾分类和分拣，提高废物管理的效率和可靠性。&lt;h4&gt;方法&lt;/h4&gt;使用MobileNetV2迁移学习增强的卷积神经网络进行废物分类；实现机械臂模拟器进行虚拟分拣；使用欧几里得距离计算每个动作的能耗；融入可信人工智能的要素：透明度、鲁棒性、公平性和安全性。&lt;h4&gt;主要发现&lt;/h4&gt;模型实现了99.8%的训练准确率和80.5%的验证准确率；系统能够准确将废物分为六类；通过能耗计算优化了机械臂的移动路径，提高了能源效率。&lt;h4&gt;结论&lt;/h4&gt;该框架是一个可靠且可扩展的解决方案，适用于城市环境中的智能废物管理系统，结合了可信人工智能和节能机械臂的优势。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新颖的方法，将可信人工智能与节能机械臂相结合，用于智能垃圾分类和分拣。通过利用通过MobileNetV2迁移学习增强的卷积神经网络，系统准确地将废物分为六类：塑料、玻璃、金属、纸张、纸板和垃圾。该模型实现了99.8%的高训练准确率和80.5%的验证准确率，展示了强大的学习和泛化能力。实现了机械臂模拟器进行虚拟分拣，使用欧几里得距离计算每个动作的能耗，确保最佳和高效的移动。该框架融入了可信人工智能的关键要素，如透明度、鲁棒性、公平性和安全性，使其成为城市环境中智能废物管理系统的可靠且可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel methodology that integrates trustworthyartificial intelligence (AI) with an energy-efficient robotic arm forintelligent waste classification and sorting. By utilizing a convolutionalneural network (CNN) enhanced through transfer learning with MobileNetV2, thesystem accurately classifies waste into six categories: plastic, glass, metal,paper, cardboard, and trash. The model achieved a high training accuracy of99.8% and a validation accuracy of 80.5%, demonstrating strong learning andgeneralization. A robotic arm simulator is implemented to perform virtualsorting, calculating the energy cost for each action using Euclidean distanceto ensure optimal and efficient movement. The framework incorporates keyelements of trustworthy AI, such as transparency, robustness, fairness, andsafety, making it a reliable and scalable solution for smart waste managementsystems in urban settings.</description>
      <author>example@mail.com (Halima I. Kure, Jishna Retnakumari, Augustine O. Nwajana, Umar M. Ismail, Bilyaminu A. Romo, Ehigiator Egho-Promise)</author>
      <guid isPermaLink="false">2510.17408v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Dictionary-Based Deblurring for Unpaired Data</title>
      <link>http://arxiv.org/abs/2510.16428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于字典学习的图像去模糊方法，能够在不同数据监督条件下实现鲁棒的去模糊效果，解决了传统方法对大量配对数据的依赖问题。&lt;h4&gt;背景&lt;/h4&gt;有效的图像去模糊通常依赖于大量完全配对的模糊和清晰图像数据集，但在现实世界中获取这种准确对齐的数据存在困难，限制了现有方法的有效性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决数据稀缺依赖问题，提出一种基于字典学习的去模糊方法，用于联合估计结构化的模糊矩阵和高分辨率图像字典。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的基于字典学习的去模糊方法，能够在不同程度的监督下实现鲁棒的图像去模糊，并在三种不同实验设置下进行了评估：完全监督、部分监督和无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;在CMU-Cornell iCoseg数据集和FocusPath数据集的合成模糊子集上的实验表明，所提出的框架相比传统耦合字典学习方法具有优越的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法通过准确的模糊建模和自适应字典表示，能够使用更少的训练样本，为数据受限场景下的图像去模糊提供了一种高效且鲁棒的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;有效的图像去模糊通常依赖于大量完全配对的模糊和对应清晰图像数据集。然而，在现实世界中获取这种准确对齐的数据存在许多困难，限制了现有去模糊方法的有效性和泛化能力。为解决这种数据稀缺依赖问题，我们提出了一种新颖的基于字典学习的去模糊方法，用于联合估计结构化的模糊矩阵和高分辨率图像字典。该框架能够在不同程度的监督下实现鲁棒的图像去模糊。我们在三种不同的实验设置下对方法进行了全面评估：(i) 涉及具有明确对应关系的配对数据的完全监督；(ii) 使用具有隐含关系的未配对数据的部分监督；(iii) 使用不存在直接配对的非对应数据的无监督学习。在CMU-Cornell iCoseg数据集和真实世界FocusPath数据集的合成模糊子集上进行的大量实验验证一致表明，与传统的耦合字典学习方法相比，所提出的框架具有优越的性能。结果验证了我们的方法通过准确的模糊建模和自适应字典表示，能够使用显著更少的训练样本，为数据受限场景下的图像去模糊提供了一种高效且鲁棒的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective image deblurring typically relies on large and fully paireddatasets of blurred and corresponding sharp images. However, obtaining suchaccurately aligned data in the real world poses a number of difficulties,limiting the effectiveness and generalizability of existing deblurring methods.To address this scarcity of data dependency, we present a novel dictionarylearning based deblurring approach for jointly estimating a structured blurmatrix and a high resolution image dictionary. This framework enables robustimage deblurring across different degrees of data supervision. Our method isthoroughly evaluated across three distinct experimental settings: (i) fullsupervision involving paired data with explicit correspondence, (ii) partialsupervision employing unpaired data with implicit relationships, and (iii)unsupervised learning using non-correspondence data where direct pairings areabsent. Extensive experimental validation, performed on synthetically blurredsubsets of the CMU-Cornell iCoseg dataset and the real-world FocusPath dataset,consistently shows that the proposed framework has superior performancecompared to conventional coupled dictionary learning approaches. The resultsvalidate that our approach provides an efficient and robust solution for imagedeblurring in data-constrained scenarios by enabling accurate blur modeling andadaptive dictionary representation with a notably smaller number of trainingsamples.</description>
      <author>example@mail.com (Alok Panigrahi, Jayaprakash Katual, Satish Mulleti)</author>
      <guid isPermaLink="false">2510.16428v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>A Semiparametric Gaussian Mixture Model with Spatial Dependence and Its Application to Whole-Slide Image Clustering Analysis</title>
      <link>http://arxiv.org/abs/2510.16421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种半参数高斯混合模型(SGMM)，用于考虑空间信息进行无监督学习，该模型比传统GMM更灵活，能够实现同类实例的空间聚类。&lt;h4&gt;背景&lt;/h4&gt;无监督学习中通常需要考虑空间信息，但传统高斯混合模型在这方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够考虑空间信息的半参数高斯混合模型，提高无监督学习的聚类性能。&lt;h4&gt;方法&lt;/h4&gt;提出半参数高斯混合模型(SGMM)，为每个实例假设随机位置，并基于此假设特征向量服从标准GMM；开发新的EM算法估计SGMM并建立渐近理论；进行数值模拟验证性能；将方法应用于CAMELYON16数据集进行乳腺癌检测。&lt;h4&gt;主要发现&lt;/h4&gt;SGMM比传统GMM更灵活，能使同类实例在空间上聚集；在数值模拟和实际应用中表现出色，尤其在乳腺癌检测任务中展现了卓越的聚类性能。&lt;h4&gt;结论&lt;/h4&gt;SGMM是一种有效的无监督学习方法，通过考虑空间信息提高了聚类性能，在理论和实际应用中都表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们在这里开发了一种半参数高斯混合模型(SGMM)，用于考虑有价值空间信息的无监督学习。具体来说，我们为每个实例假设一个随机位置。然后，基于这个随机位置，我们为特征向量假设一个标准高斯混合模型(GMM)。所提出的SGMM允许混合概率与空间位置非参数相关。与传统GMM相比，SGMM更加灵活，并允许同一类的实例在空间上聚集。为了估计SGMM，开发了新的EM算法并建立了严格的渐近理论。进行了大量的数值模拟来证明我们的有限样本性能。对于实际应用，我们将SGMM方法应用于CAMELYON16数据集的全幻灯片图像(WSIs)进行乳腺癌检测。SGMM方法表现出卓越的聚类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We develop here a semiparametric Gaussian mixture model (SGMM) forunsupervised learning with valuable spatial information taken intoconsideration. Specifically, we assume for each instance a random location.Then, conditional on this random location, we assume for the feature vector astandard Gaussian mixture model (GMM). The proposed SGMM allows the mixingprobability to be nonparametrically related to the spatial location. Comparedwith a classical GMM, SGMM is considerably more flexible and allows theinstances from the same class to be spatially clustered. To estimate the SGMM,novel EM algorithms are developed and rigorous asymptotic theories areestablished. Extensive numerical simulations are conducted to demonstrate ourfinite sample performance. For a real application, we apply our SGMM method tothe CAMELYON16 dataset of whole-slide images (WSIs) for breast cancerdetection. The SGMM method demonstrates outstanding clustering performance.</description>
      <author>example@mail.com (Baichen Yu, Jin Liu, Hansheng Wang)</author>
      <guid isPermaLink="false">2510.16421v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Adversarially Robust Quantum Transfer Learning</title>
      <link>http://arxiv.org/abs/2510.16301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This Book Chapter will publish in "Quantum Robustness in Artificial  Intelligence" Book by Springer and is currently in production. More  information about the Book is at:  https://link.springer.com/book/9783032111524?srsltid=AfmBOood7vZYc5xJYtLrQWND4pjedgfWAfAFFocjvnNS1lrNpVBwvJcO#accessibility-information&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种量子迁移学习（QTL）模型，结合量子计算与迁移学习技术，用于高分辨率图像分类，在多个数据集上显示出优于传统和量子模型的性能，并通过对抗训练提高了模型鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;量子机器学习（QML）作为利用量子计算原理增强经典机器学习系统性能的有前景领域，受限于当前硬件约束（如量子比特数量有限和量子噪声），其实际部署仍然有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合量子-经典架构，结合量子计算优势与迁移学习技术，解决高分辨率图像分类问题，并提高模型在实际应用中的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出量子迁移学习（QTL）模型，集成经典卷积特征提取与量子变分电路，并在Ants &amp; Bees、CIFAR-10和道路标志检测等数据集上进行模拟实验，同时研究模型对抗攻击的脆弱性并加入对抗训练提高鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;QTL在多个数据集上实现了比传统模型和未使用迁移学习的量子模型更优的分类性能；对抗训练显著增强了QTL的鲁棒性，提高了其在安全敏感应用中部署的潜力。&lt;h4&gt;结论&lt;/h4&gt;量子迁移学习模型结合了量子计算与迁移学习的优势，有效解决了量子机器学习在实际部署中的局限性，为高分辨率图像分类提供了一种有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;量子机器学习（QML）已成为一个有前景的研究领域，通过利用量子计算原理来增强经典机器学习系统的性能。然而，由于当前硬件限制（如量子比特数量有限和量子噪声），QML的实际部署仍然有限。本章介绍了一种混合量子-经典架构，结合量子计算的优势和迁移学习技术来解决高分辨率图像分类问题。具体而言，我们提出了一种量子迁移学习（QTL）模型，集成了经典卷积特征提取和量子变分电路。通过在Ants &amp; Bees、CIFAR-10和道路标志检测等多样化数据集上进行广泛模拟，我们证明QTL比传统模型和未使用迁移学习的量子模型实现了更好的分类性能。此外，我们还研究了模型对抗攻击的脆弱性，并证明加入对抗训练显著提高了QTL的鲁棒性，增强了其在安全敏感应用中部署的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum machine learning (QML) has emerged as a promising area of researchfor enhancing the performance of classical machine learning systems byleveraging quantum computational principles. However, practical deployment ofQML remains limited due to current hardware constraints such as limited numberof qubits and quantum noise. This chapter introduces a hybrid quantum-classicalarchitecture that combines the advantages of quantum computing with transferlearning techniques to address high-resolution image classification.Specifically, we propose a Quantum Transfer Learning (QTL) model thatintegrates classical convolutional feature extraction with quantum variationalcircuits. Through extensive simulations on diverse datasets including Ants \&amp;Bees, CIFAR-10, and Road Sign Detection, we demonstrate that QTL achievessuperior classification performance compared to both conventional and quantummodels trained without transfer learning. Additionally, we also investigate themodel's vulnerability to adversarial attacks and demonstrate that incorporatingadversarial training significantly boosts the robustness of QTL, enhancing itspotential for deployment in security sensitive applications.</description>
      <author>example@mail.com (Amena Khatun, Muhammad Usman)</author>
      <guid isPermaLink="false">2510.16301v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Orthology Networks</title>
      <link>http://arxiv.org/abs/2510.15837v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了TRON（Transfer Orthology Networks），一种用于跨物种迁移学习的新型神经网络架构，利用直系同源关系来指导知识迁移，实现了从源物种到目标物种的高效知识传递。&lt;h4&gt;背景&lt;/h4&gt;跨物种迁移学习在生物信息学领域具有重要意义，特别是如何有效利用一个物种的知识来帮助理解另一个物种。现有的方法可能缺乏生物学解释性，且难以充分利用物种间的进化关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用物种间直系同源关系进行知识迁移的神经网络架构，提高跨物种预测的准确性和可解释性，并更有效地利用现有的转录组数据。&lt;h4&gt;方法&lt;/h4&gt;设计了TRON架构，通过在预训练的前馈神经网络前添加一个学习的物种转换层来实现知识迁移。该转换层的权重被物种间二分图的二元邻接矩阵掩码，学习一个线性变换将源物种的基因表达映射到目标物种的基因空间。&lt;h4&gt;主要发现&lt;/h4&gt;转换层的学习权重可以解释功能直系同源，提供不同物种基因如何对特定表型做出贡献的见解。这种方法为跨物种迁移学习提供了生物学基础和可解释的途径。&lt;h4&gt;结论&lt;/h4&gt;TRON为跨物种迁移学习提供了一种生物学基础扎实且可解释的方法，为更有效地利用现有转录组数据铺平了道路。研究团队正在收集跨物种转录组/表型数据，以获得对TRON架构的实验验证。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了TRON（Transfer Orthology Networks），一种用于跨物种迁移学习的新型神经网络架构。TRON利用直系同源关系（以物种之间的二分图表示）来指导知识迁移。具体来说，我们在一个预训练的前馈神经网络前添加一个学习的物种转换层，该层的权重被这个二分图的二元邻接矩阵掩码，该神经网络用于从源物种的基因表达数据预测表型。这通过学习一个线性变换来实现知识向目标物种的高效迁移，该变换将源物种的基因表达映射到目标物种的基因空间。这个转换层的学习权重为解释功能直系同源提供了潜在途径，提供了关于不同物种基因如何对感兴趣的表型做出贡献的见解。TRON为跨物种迁移学习提供了一种生物学基础扎实且可解释的方法，为更有效地利用现有转录组数据铺平了道路。我们正在收集跨物种转录组/表型数据，以获得对TRON架构的实验验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Transfer Orthology Networks (TRON), a novel neural networkarchitecture designed for cross-species transfer learning. TRON leveragesorthologous relationships, represented as a bipartite graph between species, toguide knowledge transfer. Specifically, we prepend a learned species conversionlayer, whose weights are masked by the biadjacency matrix of this bipartitegraph, to a pre-trained feedforward neural network that predicts a phenotypefrom gene expression data in a source species. This allows for efficienttransfer of knowledge to a target species by learning a linear transformationthat maps gene expression from the source to the target species' gene space.The learned weights of this conversion layer offer a potential avenue forinterpreting functional orthology, providing insights into how genes acrossspecies contribute to the phenotype of interest. TRON offers a biologicallygrounded and interpretable approach to cross-species transfer learning, pavingthe way for more effective utilization of available transcriptomic data. We arein the process of collecting cross-species transcriptomic/phenotypic data togain experimental validation of the TRON architecture.</description>
      <author>example@mail.com (Vikash Singh)</author>
      <guid isPermaLink="false">2510.15837v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI</title>
      <link>http://arxiv.org/abs/2510.15684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, BraTS GoAT 2025 challenge&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型的多模态视觉变换器自编码器(MViT-AE)，用于在磁共振成像中进行脑肿瘤分割的无监督异常检测，无需依赖手动标注数据。&lt;h4&gt;背景&lt;/h4&gt;无监督异常检测(UAD)是脑肿瘤分割的一种替代方法，特别在标注数据有限、昂贵或不一致的情况下，可解决神经影像工作流程中的可扩展性瓶颈。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖手动标注的脑肿瘤分割方法，解决传统监督学习方法在数据获取方面的限制。&lt;h4&gt;方法&lt;/h4&gt;提出MViT-AE模型，仅在健康脑部MRI图像上训练，通过基于重建的误差图检测和定位肿瘤；采用多模态早期-晚期融合策略整合多种MRI序列信息；引入后处理流程整合分割任何模型(SAM)优化肿瘤轮廓预测。&lt;h4&gt;主要发现&lt;/h4&gt;在BraTS-GoAT 2025 Lighthouse数据集(包含胶质瘤、脑膜瘤和儿童脑肿瘤等)上评估，测试集上病变级别Dice相似系数：全肿瘤0.437，肿瘤核心0.316，增强肿瘤0.350；验证集上异常检测率为89.4%。&lt;h4&gt;结论&lt;/h4&gt;基于变换器的无监督模型有潜力成为神经肿瘤成像中可扩展、标签高效的工具，尽管在检测小或非增强病变方面仍存在挑战。&lt;h4&gt;翻译&lt;/h4&gt;无监督异常检测(UAD)为磁共振成像(MRI)中的脑肿瘤分割提供了监督学习的替代方案，特别是在标注数据有限、昂贵或不一致的情况下。本研究提出了一种新型的多模态视觉变换器自编码器(MViT-AE)，仅在健康脑部MRI上进行训练，通过基于重建的误差图检测和定位肿瘤。这种无监督范式实现了不依赖手动标签的分割，解决了神经影像工作流程中的一个关键可扩展性瓶颈。我们的方法在BraTS-GoAT 2025 Lighthouse数据集上进行了评估，该数据集包含各种类型的肿瘤，如胶质瘤、脑膜瘤和儿童脑肿瘤。为提高性能，我们引入了多模态早期-晚期融合策略，利用多种MRI序列的互补信息，以及一个整合分割任何模型(SAM)的后处理流程，以优化预测的肿瘤轮廓。尽管UAD存在已知挑战，特别是在检测小或非增强病变方面，我们的方法仍实现了具有临床意义的肿瘤定位，测试集上的病变级别Dice相似系数为0.437(全肿瘤)、0.316(肿瘤核心)和0.350(增强肿瘤)，验证集上的异常检测率为89.4%。这些发现强调了基于变换器的无监督模型作为神经肿瘤成像中可扩展、标签高效工具的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised anomaly detection (UAD) presents a complementary alternative tosupervised learning for brain tumor segmentation in magnetic resonance imaging(MRI), particularly when annotated datasets are limited, costly, orinconsistent. In this work, we propose a novel Multimodal Vision TransformerAutoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect andlocalize tumors via reconstruction-based error maps. This unsupervised paradigmenables segmentation without reliance on manual labels, addressing a keyscalability bottleneck in neuroimaging workflows. Our method is evaluated inthe BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumorssuch as gliomas, meningiomas, and pediatric brain tumors. To enhanceperformance, we introduce a multimodal early-late fusion strategy thatleverages complementary information across multiple MRI sequences, and apost-processing pipeline that integrates the Segment Anything Model (SAM) torefine predicted tumor contours. Despite the known challenges of UAD,particularly in detecting small or non-enhancing lesions, our method achievesclinically meaningful tumor localization, with lesion-wise Dice SimilarityCoefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (EnhancingTumor) on the test set, and an anomaly Detection Rate of 89.4% on thevalidation set. These findings highlight the potential of transformer-basedunsupervised models to serve as scalable, label-efficient tools forneuro-oncological imaging.</description>
      <author>example@mail.com (Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier, Noel E. O'Connor, Ferran Marques)</author>
      <guid isPermaLink="false">2510.15684v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning</title>
      <link>http://arxiv.org/abs/2510.15372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分阶段自适应微调方法，用于解决微创手术中自动化工具检测面临的标注数据有限问题。该方法通过线性探测和渐进式冻结两个阶段，有效提高了手术工具检测的性能，在胆囊切除和眼科手术数据集上均表现出色。&lt;h4&gt;背景&lt;/h4&gt;微创手术可以从自动化手术工具检测中获益，但手术环境中标注数据的有限性对训练强健的深度学习模型构成了挑战。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的分阶段自适应微调方法，提高手术工具检测的性能和效率。&lt;h4&gt;方法&lt;/h4&gt;提出两步法微调策略：1)线性探测阶段，在预训练CNN架构上添加额外分类层；2)渐进式冻结阶段，动态减少可微调层数。该方法降低了网络复杂性，提高效率，只需单次训练循环。使用在ImageNet上预训练的ResNet-50和DenseNet-121架构，在Cholec80数据集上检测胆囊切除内窥镜视频中的手术工具。&lt;h4&gt;主要发现&lt;/h4&gt;该方法比现有方法和既定微调技术提高了检测性能，平均精度均值(mAP)达到96.4%。在CATARACTS数据集(眼科手术)上验证了该方法的可推广性。&lt;h4&gt;结论&lt;/h4&gt;渐进式冻结微调是提高不同手术过程中工具存在检测的一种有前途的技术，可能在一般图像分类任务中有更广泛的应用。&lt;h4&gt;翻译&lt;/h4&gt;微创手术可以从自动化手术工具检测中显著获益，实现高级分析和辅助。然而，手术环境中标注数据的有限性对训练强健的深度学习模型构成了挑战。本文引入了一种新颖的分阶段自适应微调方法，包含两个步骤：线性探测阶段，在预训练的基于CNN的架构上添加额外的分类层；渐进式冻结阶段，动态减少可微调层数，旨在调节对手术领域的适应。这种策略降低了网络复杂性并提高了效率，只需要单个训练循环，消除了多次迭代的必要性。我们在Cholec80数据集上验证了我们的方法，使用在ImageNet上预训练的CNN架构(ResNet-50和DenseNet-121)来检测胆囊切除内窥镜视频中的手术工具。我们的结果表明，与现有方法和既定的微调技术相比，我们的方法提高了检测性能，平均精度均值(mAP)达到96.4%。为了评估其更广泛的适用性，在CATARACTS数据集(一个不同的微创眼科手术领域)上进一步确认了微调策略的可推广性。这些发现表明，渐进式冻结微调是提高不同手术过程中工具存在检测的一种有前途的技术，可能在一般的图像分类任务中有更广泛的应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1002/ima.70218&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Minimally invasive surgery can benefit significantly from automated surgicaltool detection, enabling advanced analysis and assistance. However, the limitedavailability of annotated data in surgical settings poses a challenge fortraining robust deep learning models. This paper introduces a novel stagedadaptive fine-tuning approach consisting of two steps: a linear probing stageto condition additional classification layers on a pre-trained CNN-basedarchitecture and a gradual freezing stage to dynamically reduce thefine-tunable layers, aiming to regulate adaptation to the surgical domain. Thisstrategy reduces network complexity and improves efficiency, requiring only asingle training loop and eliminating the need for multiple iterations. Wevalidated our method on the Cholec80 dataset, employing CNN architectures(ResNet-50 and DenseNet-121) pre-trained on ImageNet for detecting surgicaltools in cholecystectomy endoscopic videos. Our results demonstrate that ourmethod improves detection performance compared to existing approaches andestablished fine-tuning techniques, achieving a mean average precision (mAP) of96.4%. To assess its broader applicability, the generalizability of thefine-tuning strategy was further confirmed on the CATARACTS dataset, a distinctdomain of minimally invasive ophthalmic surgery. These findings suggest thatgradual freezing fine-tuning is a promising technique for improving toolpresence detection in diverse surgical procedures and may have broaderapplications in general image classification tasks.</description>
      <author>example@mail.com (Ana Davila, Jacinto Colan, Yasuhisa Hasegawa)</author>
      <guid isPermaLink="false">2510.15372v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization</title>
      <link>http://arxiv.org/abs/2510.15165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了在连续时间线性二次调节器(LQRs)中使用策略迁移方法来提高强化学习效率，提供了连续时间RL策略迁移的第一个理论证明，并提出了新型策略学习算法。&lt;h4&gt;背景&lt;/h4&gt;强化学习使智能体能够通过与环境的交互学习最优决策策略，但在复杂任务上从零开始训练效率低下。迁移学习在大语言模型中非常成功，为提高强化学习效率提供了有前景的方向。&lt;h4&gt;目的&lt;/h4&gt;研究策略迁移方法，即在带熵正则化的连续时间线性二次调节器(LQRs)背景下，使用相关源任务的策略初始化目标RL任务的学习，并提供连续时间RL策略迁移的理论证明。&lt;h4&gt;方法&lt;/h4&gt;采用策略迁移方法，使用相关源任务的策略初始化目标RL任务的学习；提出针对连续时间LQRs的新型策略学习算法；分析连续时间LQRs与基于分数的扩散模型之间的联系。&lt;h4&gt;主要发现&lt;/h4&gt;证明了一个最优于一个LQR的策略可以作为紧密相关LQRs的近似最优初始化，同时保持原始算法的收敛速率；提出的策略学习算法实现了全局线性和局部超线性收敛；通过分析推导出一类基于分数的连续时间扩散模型的稳定性。&lt;h4&gt;结论&lt;/h4&gt;展示了迁移学习在连续时间RL中的理论保证和算法优势，弥补了现有文献中的空白，将先前的工作从离散时间扩展到连续时间设置。&lt;h4&gt;翻译&lt;/h4&gt;强化学习使智能体能够通过与环境的交互学习最优决策策略，但在复杂任务上从零开始训练可能效率极低。在大语言模型中广泛成功的迁移学习为利用预训练模型提高强化学习效率提供了有前景的方向。本文研究了策略迁移，这是一种迁移学习方法，在带熵正则化的连续时间线性二次调节器(LQRs)背景下，使用相关源任务的策略初始化目标RL任务的学习。我们首次提供了连续时间RL策略迁移的理论证明，证明了一个最优于一个LQR的策略可以作为紧密相关LQRs的近似最优初始化，同时保持原始算法的收敛速率。此外，我们提出了针对连续时间LQRs的新型策略学习算法，实现了全局线性和局部超线性收敛。我们的结果展示了连续时间RL中迁移学习的理论保证和算法优势，解决了现有文献中的空白，并将先前的工作从离散时间扩展到连续时间设置。作为我们分析的副产品，我们通过LQRs与基于分数的连续时间扩散模型之间的联系，推导出一类连续时间扩散模型的稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) enables agents to learn optimal decision-makingstrategies through interaction with an environment, yet training from scratchon complex tasks can be highly inefficient. Transfer learning (TL), widelysuccessful in large language models (LLMs), offers a promising direction forenhancing RL efficiency by leveraging pre-trained models.  This paper investigates policy transfer, a TL approach that initializeslearning in a target RL task using a policy from a related source task, in thecontext of continuous-time linear quadratic regulators (LQRs) with entropyregularization. We provide the first theoretical proof of policy transfer forcontinuous-time RL, proving that a policy optimal for one LQR serves as anear-optimal initialization for closely related LQRs, while preserving theoriginal algorithm's convergence rate. Furthermore, we introduce a novel policylearning algorithm for continuous-time LQRs that achieves global linear andlocal super-linear convergence. Our results demonstrate both theoreticalguarantees and algorithmic benefits of transfer learning in continuous-time RL,addressing a gap in existing literature and extending prior work from discreteto continuous time settings.  As a byproduct of our analysis, we derive the stability of a class ofcontinuous-time score-based diffusion models via their connection with LQRs.</description>
      <author>example@mail.com (Xin Guo, Zijiu Lyu)</author>
      <guid isPermaLink="false">2510.15165v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Transfer learning strategies for accelerating reinforcement-learning-based flow control</title>
      <link>http://arxiv.org/abs/2510.16016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了迁移学习策略以加速深度强化学习在多保真度混沌流体流动控制中的应用，首次将渐进神经网络应用于基于DRL的流动控制，并评估了传统微调策略的性能。&lt;h4&gt;背景&lt;/h4&gt;在深度强化学习应用于混沌流体流动控制时，如何有效利用低保真度环境训练的知识到高保真度环境是一个挑战，传统微调方法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;研究迁移学习策略，特别是渐进神经网络，来加速深度强化学习在多保真度混沌流体流动控制中的应用，并评估这些策略的性能、收敛行为和保留已转移知识的能力。&lt;h4&gt;方法&lt;/h4&gt;1)首次将渐进神经网络应用于基于DRL的流动控制；2)对传统微调策略进行全面基准测试；3)使用Kuramoto-Sivashinsky系统作为基准，研究知识转移；4)进行逐层敏感性分析，研究PNNs如何重用中间表示。&lt;h4&gt;主要发现&lt;/h4&gt;1)微调虽可加速收敛但对预训练时长敏感且易发生灾难性遗忘；2)PNNs通过保留先验知识实现稳定高效迁移；3)PNNs能动态重用源策略中间表示并逐步适应新任务；4)即使环境差异大，PNNs仍有效，而微调策略往往失败。&lt;h4&gt;结论&lt;/h4&gt;新型迁移学习框架在稳健、可扩展和计算高效的流动控制方面具有潜力，可应用于更复杂的流动配置。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了迁移学习策略以加速深度强化学习在多保真度混沌流体流动控制中的应用。渐进神经网络是一种模块化架构，旨在跨任务保留和重用知识，首次被应用于基于DRL的流动控制背景下。此外，还对传统的微调策略进行了全面的基准测试，评估了它们的性能、收敛行为以及保留已转移知识的能力。Kuramoto-Sivashinsky系统被用作基准，研究如何在低保真度环境中训练的控制策略知识有效地转移到高保真度设置中。系统评估表明，虽然微调可以加速收敛，但它对预训练时长非常敏感，且容易发生灾难性遗忘。相比之下，渐进神经网络通过保留先验知识实现稳定高效的迁移，提供一致的性能提升，并且在预训练阶段对过拟合具有显著的鲁棒性。逐层敏感性分析进一步揭示了渐进神经网络如何动态重用来自源策略的中间表示，同时逐步使更深层次层适应目标任务。此外，即使在源环境和目标环境差异很大的情况下，如物理机制不匹配或控制目标不同的情况下，渐进神经网络仍然有效，而微调策略往往导致次优适应或知识转移完全失败。这些结果突显了新型迁移学习框架在稳健、可扩展和计算高效的流动控制方面的潜力，可以应用于更复杂的流动配置。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work investigates transfer learning strategies to accelerate deepreinforcement learning (DRL) for multifidelity control of chaotic fluid flows.Progressive neural networks (PNNs), a modular architecture designed to preserveand reuse knowledge across tasks, are employed for the first time in thecontext of DRL-based flow control. In addition, a comprehensive benchmarking ofconventional fine-tuning strategies is conducted, evaluating their performance,convergence behavior, and ability to retain transferred knowledge. TheKuramoto-Sivashinsky (KS) system is employed as a benchmark to examine howknowledge encoded in control policies, trained in low-fidelity environments,can be effectively transferred to high-fidelity settings. Systematicevaluations show that while fine-tuning can accelerate convergence, it ishighly sensitive to pretraining duration and prone to catastrophic forgetting.In contrast, PNNs enable stable and efficient transfer by preserving priorknowledge and providing consistent performance gains, and are notably robust tooverfitting during the pretraining phase. Layer-wise sensitivity analysisfurther reveals how PNNs dynamically reuse intermediate representations fromthe source policy while progressively adapting deeper layers to the targettask. Moreover, PNNs remain effective even when the source and targetenvironments differ substantially, such as in cases with mismatched physicalregimes or control objectives, where fine-tuning strategies often result insuboptimal adaptation or complete failure of knowledge transfer. The resultshighlight the potential of novel transfer learning frameworks for robust,scalable, and computationally efficient flow control that can potentially beapplied to more complex flow configurations.</description>
      <author>example@mail.com (Saeed Salehi)</author>
      <guid isPermaLink="false">2510.16016v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Machine Learning-Based Ultrasonic Weld Characterization Using Hierarchical Wave Modeling and Diffusion-Driven Distribution Alignment</title>
      <link>http://arxiv.org/abs/2510.13023v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 6 page appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种端到端的机器学习工作流程，用于解决工业环境中超声波焊接检测面临的训练数据有限和环境波动导致的信号损坏问题。&lt;h4&gt;背景&lt;/h4&gt;自动化超声波焊接检测在无损评估领域仍然是一个重大挑战，主要由于训练数据有限（实验标本整理或高保真模拟的复杂性）和工业环境的环境波动性导致的实时测量数据损坏。&lt;h4&gt;目的&lt;/h4&gt;开发一种端到端的机器学习工作流程，用于真实工业环境中的声学焊接检测，解决数据整理和信号损坏的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出的工作流程包括降阶建模方案、基于扩散的分布对齐和基于U-Net的分割与反演；使用基于Lamb波理论的降阶Helmholtz模型生成数据集；通过迁移学习使用有限的全3D弹性动力学模拟完善模型；使用引导扩散处理分布外真实世界测量数据。&lt;h4&gt;主要发现&lt;/h4&gt;低阶解决方案为反演模型提供了强大的训练数据集；引导扩散能有效处理具有不同且不可预测噪声分布的真实世界测量数据。&lt;h4&gt;结论&lt;/h4&gt;这种集成框架为真实数据上的自动化焊接检测提供了端到端解决方案。&lt;h4&gt;翻译&lt;/h4&gt;自动化超声波焊接检测由于训练数据有限（由于实验标本整理或高保真模拟的复杂性）和许多工业环境的环境波动性（导致实时测量数据损坏）等因素，在无损评估社区仍然是一个重大挑战。因此，用于真实（即工业）环境中声学焊接检测的端到端机器学习工作流程一直是一个难以实现的目标。这项工作通过提出包括降阶建模方案、基于扩散的分布对齐以及基于U-Net的分割和反演的工作流程，解决了数据整理和信号损坏的挑战。使用基于Lamb波理论的降阶Helmholtz模型生成涵盖不同焊接异质性和裂纹缺陷的综合数据集。相对廉价低阶解决方案为反演模型提供了强大的训练数据集，并通过使用有限的全3D弹性动力学模拟的迁移学习阶段进行完善。为了处理具有不同且不可预测噪声分布的分布外真实世界测量，即激光多普勒测振扫描，引导扩散生成OOD实验LDV扫描的分布内表示，随后由反演模型处理。这种集成框架为真实数据上的自动化焊接检测提供了端到端解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated ultrasonic weld inspection remains a significant challenge in thenondestructive evaluation (NDE) community to factors such as limited trainingdata (due to the complexity of curating experimental specimens or high-fidelitysimulations) and environmental volatility of many industrial settings(resulting in the corruption of on-the-fly measurements). Thus, an end-to-endmachine learning (ML) workflow for acoustic weld inspection in realistic (i.e.,industrial) settings has remained an elusive goal. This work addresses thechallenges of data curation and signal corruption by proposing workflowconsisting of a reduced-order modeling scheme, diffusion based distributionalignment, and U-Net-based segmentation and inversion. A reduced-orderHelmholtz model based on Lamb wave theory is used to generate a comprehensivedataset over varying weld heterogeneity and crack defects. The relativelyinexpensive low-order solutions provide a robust training dateset for inversionmodels which are refined through a transfer learning stage using a limited setof full 3D elastodynamic simulations. To handle out-of-distribution (OOD)real-world measurements with varying and unpredictable noise distributions,i.e., Laser Doppler Vibrometry scans, guided diffusion produces in-distributionrepresentations of OOD experimental LDV scans which are subsequently processedby the inversion models. This integrated framework provides an end-to-endsolution for automated weld inspection on real data.</description>
      <author>example@mail.com (Joshua R. Tempelman, Adam J. Wachtor, Eric B. Flynn)</author>
      <guid isPermaLink="false">2510.13023v2</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues</title>
      <link>http://arxiv.org/abs/2510.17722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Website: https://github.com/NJU-LINK/MT-Video-Bench&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MT-Video-Bench，一个专门用于评估多模态大语言模型在多轮对话中视频理解能力的基准测试。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)的近期发展显著提升了AI理解视觉模态的能力，但现有评估基准仅限于单轮问答，忽视了现实场景中多轮对话的复杂性。&lt;h4&gt;目的&lt;/h4&gt;弥补现有评估基准的不足，引入一个专门用于评估MLLMs在多轮对话中表现的视频理解基准。&lt;h4&gt;方法&lt;/h4&gt;MT-Video-Bench主要评估六种关注感知性和互动性的核心能力，包含来自不同领域的987个精心策划的多轮对话，这些能力与现实应用如交互式体育分析和多轮视频智能辅导紧密对齐。&lt;h4&gt;主要发现&lt;/h4&gt;通过评估各种最先进的开源和闭源MLLMs，揭示了它们在处理多轮视频对话时的显著性能差异和局限性。&lt;h4&gt;结论&lt;/h4&gt;该基准将公开可用，以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)的近期发展显著提升了AI理解视觉模态的能力。然而，现有评估基准仍仅限于单轮问答，忽视了现实场景中多轮对话的复杂性。为弥补这一差距，我们引入了MT-Video-Bench，一个用于评估MLLMs在多轮对话中表现的整体视频理解基准。具体而言，我们的MT-Video-Bench主要评估六种关注感知性和互动性的核心能力，包含来自不同领域的987个精心策划的多轮对话。这些能力与现实应用（如交互式体育分析和多轮视频智能辅导）紧密对齐。通过MT-Video-Bench，我们广泛评估了各种最先进的开源和闭源MLLMs，揭示了它们在处理多轮视频对话时的显著性能差异和局限性。该基准将公开可用以促进未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent development of Multimodal Large Language Models (MLLMs) hassignificantly advanced AI's ability to understand visual modalities. However,existing evaluation benchmarks remain limited to single-turn questionanswering, overlooking the complexity of multi-turn dialogues in real-worldscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic videounderstanding benchmark for evaluating MLLMs in multi-turn dialogues.Specifically, our MT-Video-Bench mainly assesses six core competencies thatfocus on perceptivity and interactivity, encompassing 987 meticulously curatedmulti-turn dialogues from diverse domains. These capabilities are rigorouslyaligned with real-world applications, such as interactive sports analysis andmulti-turn video-based intelligent tutoring. With MT-Video-Bench, weextensively evaluate various state-of-the-art open-source and closed-sourceMLLMs, revealing their significant performance discrepancies and limitations inhandling multi-turn video dialogues. The benchmark will be publicly availableto foster future research.</description>
      <author>example@mail.com (Yaning Pan, Zekun Wang, Qianqian Xie, Yongqian Wen, Yuanxing Zhang, Guohui Zhang, Haoxuan Hu, Zhiyu Pan, Yibing Huang, Zhidong Gan, Yonghong Lin, An Ping, Tianhao Peng, Jiaheng Liu)</author>
      <guid isPermaLink="false">2510.17722v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>A Mimamsa Inspired Framework For Instruction Sequencing In AI Agents</title>
      <link>http://arxiv.org/abs/2510.17691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于印度Mimamsa哲学体系的AI智能体指令排序正式框架&lt;h4&gt;背景&lt;/h4&gt;灵感来源于印度哲学体系Mimamsa&lt;h4&gt;目的&lt;/h4&gt;建立可靠的指令排序机制，影响AI应用如任务规划和机器人技术&lt;h4&gt;方法&lt;/h4&gt;通过动作-对象对以三种方式形式化排序机制：直接断言(Srutikrama)用于时间先后顺序，目的驱动排序(Arthakrama)用于功能依赖关系，迭代过程(Pravrittikrama)用于区分重复任务中的并行和顺序执行。引入动作对象命令式逻辑的语法和语义，扩展MIRA形式化体系&lt;h4&gt;主要发现&lt;/h4&gt;建立了指令排序的正确性定理，基于连续指令间对象依赖关系，并证明了可靠性和完备性&lt;h4&gt;结论&lt;/h4&gt;形式化验证实现可靠的指令排序，解决时间推理和依赖建模问题&lt;h4&gt;翻译&lt;/h4&gt;这篇论文提出了一个用于AI智能体指令排序的正式框架，灵感来源于印度哲学体系Mimamsa。该框架通过动作-对象对以三种不同方式形式化排序机制：直接断言(Srutikrama)用于时间先后顺序，目的驱动排序(Arthakrama)用于功能依赖关系，以及迭代过程(Pravrittikrama)用于区分重复任务中的并行和顺序执行。它引入了动作对象命令式逻辑的语法和语义，扩展了MIRA形式化体系，并添加了明确的排序演绎规则。指令排序的正确性通过一个验证定理建立，该定理基于连续指令间的对象依赖关系。这得到了可靠性和完备性的证明支持。这种形式化验证实现了可靠的指令排序，通过解决时间推理和依赖建模问题，影响了任务规划和机器人等AI应用领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a formal framework for sequencing instructions in AIagents, inspired by the Indian philosophical system of Mimamsa. The frameworkformalizes sequencing mechanisms through action object pairs in three distinctways: direct assertion (Srutikrama) for temporal precedence, purpose drivensequencing (Arthakrama) for functional dependencies, and iterative procedures(Pravrittikrama) for distinguishing between parallel and sequential executionin repetitive tasks. It introduces the syntax and semantics of an action objectimperative logic, extending the MIRA formalism (Srinivasan and Parthasarathi,2021) with explicit deduction rules for sequencing. The correctness ofinstruction sequencing is established through a validated theorem, which isbased on object dependencies across successive instructions. This is furthersupported by proofs of soundness and completeness. This formal verificationenables reliable instruction sequencing, impacting AI applications across areaslike task planning and robotics by addressing temporal reasoning and dependencymodeling.</description>
      <author>example@mail.com (Bama Srinivasan)</author>
      <guid isPermaLink="false">2510.17691v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding</title>
      <link>http://arxiv.org/abs/2510.17305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ARR Rolling Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LongInsightBench是首个专门评估模型理解长视频能力的基准测试，整合视觉、音频和文本模态，包含长信息密集视频、多样化任务场景和严格质量保证流程。&lt;h4&gt;背景&lt;/h4&gt;目前缺乏专门评估模型理解长视频能力的基准测试，尤其关注人类语言、视角、动作等上下文元素。&lt;h4&gt;目的&lt;/h4&gt;开发一个基准测试来评估模型对长视频的理解能力，特别关注多模态整合和复杂推理任务。&lt;h4&gt;方法&lt;/h4&gt;构建包含约1000个长信息密集视频的基准测试，设计六种挑战性任务场景，开发三步半自动数据质量保证流程，并进行一系列实验评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;全模态模型在需要精确时间定位和长距离因果推理的任务中面临挑战，多模态融合中存在信息损失和处理偏差。&lt;h4&gt;结论&lt;/h4&gt;LongInsightBench为评估长视频理解能力提供了有效工具，揭示了当前模型在特定任务上的局限性。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了LongInsightBench，这是第一个专门评估模型理解长视频能力的基准测试，重点关注人类语言、视角、动作和其他上下文元素，同时整合视觉、音频和文本模态。我们的基准测试在三个关键方面表现出色：a) 长时长、信息密集的视频：我们从开源数据集FineVideo中根据时长限制和视觉及音频模态的信息密度精心选择了约1000个视频，重点关注包含丰富语言元素的内容，如讲座、访谈和vlog。b) 多样且具有挑战性的任务场景：我们设计了六种具有挑战性的任务场景，包括事件内部和事件之间的任务。c) 严格且全面的质量保证流程：我们开发了一个三步半自动数据质量保证流程，以确保合成问题和答案选项的难度和有效性。基于LongInsightBench，我们设计了一系列实验。实验结果表明，全模态模型在需要精确时间定位和长距离因果推理的任务中仍然面临挑战。扩展实验揭示了全模态模型多模态融合中的信息损失和处理偏差。我们的数据集和代码可在提供的链接获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce \textbf{LongInsightBench}, the first benchmark designed toassess models' ability to understand long videos, with a focus on humanlanguage, viewpoints, actions, and other contextual elements, while integrating\textbf{visual, audio, and text} modalities. Our benchmark excels in three keyareas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully selectapproximately 1,000 videos from open-source datasets FineVideo based onduration limit and the information density of both visual and audio modalities,focusing on content like lectures, interviews, and vlogs, which contain richlanguage elements. \textbf{b) Diverse and Challenging Task Scenarios:} We havedesigned six challenging task scenarios, including both Intra-Event andInter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality AssurancePipelines:} We have developed a three-step, semi-automated data qualityassurance pipeline to ensure the difficulty and validity of the synthesizedquestions and answer options. Based on LongInsightBench, we designed a seriesof experiments. Experimental results shows that Omni-modal models(OLMs) stillface challenge in tasks requiring precise temporal localization (T-Loc) andlong-range causal inference (CE-Caus). Extended experiments reveal theinformation loss and processing bias in multi-modal fusion of OLMs. Our datasetand code is available athttps://anonymous.4open.science/r/LongInsightBench-910F/.</description>
      <author>example@mail.com (ZhaoYang Han, Qihan Lin, Hao Liang, Bowen Chen, Zhou Liu, Wentao Zhang)</author>
      <guid isPermaLink="false">2510.17305v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Fair and Interpretable Deepfake Detection in Videos</title>
      <link>http://arxiv.org/abs/2510.17264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages (including References)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种公平感知的深度伪造检测框架，整合时间特征学习和人口感知数据增强，提高检测的公平性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;现有深度伪造检测方法存在偏见、缺乏透明度，无法捕捉时间信息，导致对不同人口统计群体做出有偏见的决策和不可靠结果。&lt;h4&gt;目的&lt;/h4&gt;开发一个公平感知的深度伪造检测框架，整合时间特征学习和人口感知数据增强，以提高公平性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;使用基于序列的聚类进行深度伪造视频的时间建模，利用概念提取提高检测可靠性并为非专业用户提供可解释决策；引入人口感知的数据增强方法，平衡代表性不足的群体，应用频域变换保留深度伪造伪影，减轻偏见并提高泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在FaceForensics++、DFD、Celeb-DF和DFDC数据集上的实验表明，所提出的方法在公平性和准确性之间取得了最佳平衡，优于现有最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的公平感知深度伪造检测框架通过整合时间特征学习和人口感知数据增强，有效提高了检测的公平性和可解释性，同时保持了高准确性。&lt;h4&gt;翻译&lt;/h4&gt;现有的深度伪造检测方法往往存在偏见、缺乏透明度，并且无法捕捉时间信息，导致对不同人口群体做出有偏见的决策和不可靠的结果。在本文中，我们提出了一个公平感知的深度伪造检测框架，整合时间特征学习和人口感知的数据增强，以提高公平性和可解释性。我们的方法利用基于序列的聚类对深度伪造视频进行时间建模，并通过概念提取提高检测可靠性，同时也为非专业用户提供可解释的决策。此外，我们引入了一种人口感知的数据增强方法，平衡代表性不足的群体，并应用频域变换来保留深度伪造伪影，从而减轻偏见并提高泛化能力。在FaceForensics++、DFD、Celeb-DF和DFDC数据集上使用最先进的架构（Xception、ResNet）进行的广泛实验证明了所提出方法在获得公平性和准确性之间最佳平衡方面的有效性，优于现有最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing deepfake detection methods often exhibit bias, lack transparency,and fail to capture temporal information, leading to biased decisions andunreliable results across different demographic groups. In this paper, wepropose a fairness-aware deepfake detection framework that integrates temporalfeature learning and demographic-aware data augmentation to enhance fairnessand interpretability. Our method leverages sequence-based clustering fortemporal modeling of deepfake videos and concept extraction to improvedetection reliability while also facilitating interpretable decisions fornon-expert users. Additionally, we introduce a demography-aware dataaugmentation method that balances underrepresented groups and appliesfrequency-domain transformations to preserve deepfake artifacts, therebymitigating bias and improving generalization. Extensive experiments onFaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)architectures (Xception, ResNet) demonstrate the efficacy of the proposedmethod in obtaining the best tradeoff between fairness and accuracy whencompared to SoTA.</description>
      <author>example@mail.com (Akihito Yoshii, Ryosuke Sonoda, Ramya Srinivasan)</author>
      <guid isPermaLink="false">2510.17264v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>An empirical study of the effect of video encoders on Temporal Video Grounding</title>
      <link>http://arxiv.org/abs/2510.17007v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究调查了不同视频特征对时序视频定位任务中经典架构性能的影响，发现仅通过更改视频编码器就能显著改变模型性能，并揭示了特征间可能存在的互补性。&lt;h4&gt;背景&lt;/h4&gt;时序视频定位是计算机视觉的基础任务，旨在长视频中定位自然语言查询。由于每天产生大量视频，该任务在科学界具有重要地位。&lt;h4&gt;目的&lt;/h4&gt;解决当前研究仅集中在少数视频表示上可能导致架构过拟合的问题，通过实证研究调查不同视频特征对经典架构的影响。&lt;h4&gt;方法&lt;/h4&gt;在三个基准测试（Charades-STA、ActivityNet-Captions和YouCookII）上提取特征，使用基于CNN、时序推理和transformers的视频编码器进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;仅通过更改视频编码器，模型的性能就显示出显著差异，同时揭示了使用某些特征产生的明显模式和错误。&lt;h4&gt;结论&lt;/h4&gt;不同视频特征对模型性能有显著影响，特征之间可能存在互补性，这为未来研究提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;时序视频定位是计算机视觉中的一个基础任务，旨在定位长视频中未经修剪的自然语言查询。由于每天产生大量视频，它在科学界起着关键作用。尽管我们发现该领域有大量工作，但我们注意到研究仍然集中在少数几种视频表示上，长期来看可能导致架构过拟合。为了解决这个问题，我们提出了一项实证研究来调查不同视频特征对经典架构的影响。我们使用基于CNN、时序推理和transformers的视频编码器，为三个知名基准测试Charades-STA、ActivityNet-Captions和YouCookII提取特征。我们的结果表明，仅通过更改视频编码器，我们模型的性能就显示出显著差异，同时揭示了使用某些特征产生的明显模式和错误，最终表明特征间可能存在互补性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal video grounding is a fundamental task in computer vision, aiming tolocalize a natural language query in a long, untrimmed video. It has a key rolein the scientific community, in part due to the large amount of video generatedevery day. Although we find extensive work in this task, we note that researchremains focused on a small selection of video representations, which may leadto architectural overfitting in the long run. To address this issue, we proposean empirical study to investigate the impact of different video features on aclassical architecture. We extract features for three well-known benchmarks,Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based onCNNs, temporal reasoning and transformers. Our results show significantdifferences in the performance of our model by simply changing the videoencoder, while also revealing clear patterns and errors derived from the use ofcertain features, ultimately indicating potential feature complementarity.</description>
      <author>example@mail.com (Ignacio M. De la Jara, Cristian Rodriguez-Opazo, Edison Marrese-Taylor, Felipe Bravo-Marquez)</author>
      <guid isPermaLink="false">2510.17007v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display</title>
      <link>http://arxiv.org/abs/2510.16833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M2HVideo的视频生成框架，能够从人体模特影像生成身份可控、照片级真实的人体视频，解决了头部与身体运动不匹配以及时间建模导致的身份漂移问题。&lt;h4&gt;背景&lt;/h4&gt;人体模特展示是线上服装展示的经济有效替代方案，但缺乏真实感和表现细节，限制了其在时尚展示中的应用效果。&lt;h4&gt;目的&lt;/h4&gt;引入人体模特到人体(M2H)视频生成任务，旨在从人体模特影像中合成身份可控、照片级真实的人体视频，提高线上服装展示的真实感和表现力。&lt;h4&gt;方法&lt;/h4&gt;提出M2HVideo框架，包含动态姿态感知头部编码器融合面部语义和身体姿态，通过基于DDIM的一步去噪在像素空间应用镜像损失解决面部细节丢失问题，以及设计分布感知适配器对齐身份和服装特征统计分布增强时间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在UBC时尚数据集、自建的ASOS数据集和新收集的MannequinVideos数据集上的实验表明，M2HVideo在服装一致性、身份保持和视频保真度方面优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;M2HVideo框架有效解决了人体模特到人体视频生成的关键挑战，能够生成高质量、高保真度的服装展示视频，为线上时尚展示提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;人体模特展示为线上服装展示提供了比真人展示更具成本效益的替代方案，但缺乏真实感和表现细节。为克服这一局限，我们引入了一种称为人体模特到人体(M2H)视频生成的新任务，旨在从人体模特影像中合成身份可控、照片级真实的人体视频。我们提出M2HVideo，一个姿态感知和身份保持的视频生成框架，解决了两个关键挑战：头部和身体运动不匹配，以及时间建模导致的身份漂移。特别是，M2HVideo集成了动态姿态感知头部编码器，融合面部语义和身体姿态，产生跨帧一致的身份嵌入。为解决潜在空间压缩导致的面部细节丢失问题，我们引入了通过基于DDIM的一步去噪在像素空间应用的镜像损失。此外，我们设计了一个分布感知适配器，对齐身份和服装特征的统计分布，以增强时间一致性。在UBC时尚数据集、我们自建的ASOS数据集以及在现场收集的新MannequinVideos数据集上的大量实验表明，M2HVideo在服装一致性、身份保持和视频保真度方面优于最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mannequin-based clothing displays offer a cost-effective alternative toreal-model showcases for online fashion presentation, but lack realism andexpressive detail. To overcome this limitation, we introduce a new task calledmannequin-to-human (M2H) video generation, which aims to synthesizeidentity-controllable, photorealistic human videos from footage of mannequins.We propose M2HVideo, a pose-aware and identity-preserving video generationframework that addresses two key challenges: the misalignment between head andbody motion, and identity drift caused by temporal modeling. In particular,M2HVideo incorporates a dynamic pose-aware head encoder that fuses facialsemantics with body pose to produce consistent identity embeddings acrossframes. To address the loss of fine facial details due to latent spacecompression, we introduce a mirror loss applied in pixel space through adenoising diffusion implicit model (DDIM)-based one-step denoising.Additionally, we design a distribution-aware adapter that aligns statisticaldistributions of identity and clothing features to enhance temporal coherence.Extensive experiments on the UBC fashion dataset, our self-constructed ASOSdataset, and the newly collected MannequinVideos dataset captured on-sitedemonstrate that M2HVideo achieves superior performance in terms of clothingconsistency, identity preservation, and video fidelity in comparison tostate-of-the-art methods.</description>
      <author>example@mail.com (Xiangyu Mu, Dongliang Zhou, Jie Hou, Haijun Zhang, Weili Guan)</author>
      <guid isPermaLink="false">2510.16833v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features</title>
      <link>http://arxiv.org/abs/2510.16781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种无需训练的视频理解框架，通过结合预训练视觉语言模型(VLMs)和经典机器学习算法，实现了视频内容的零样本、自动化结构分析。&lt;h4&gt;背景&lt;/h4&gt;大规模视觉语言模型在静态图像上表现出显著的零样本推理能力，但这种能力尚未完全转移到视频领域。传统视频理解模型依赖于大量标注数据集的特定任务训练，过程昂贵且难以扩展。&lt;h4&gt;目的&lt;/h4&gt;开发一种新颖的、无需训练的视频理解框架，避免端到端训练，协同结合预训练VLM的语义先验与经典机器学习算法的模式发现能力。&lt;h4&gt;方法&lt;/h4&gt;将视频理解重构为高维语义特征空间中的自监督时空聚类问题：使用预训练VLM的冻结视觉编码器将视频转换为语义特征轨迹；应用核时间分割(KTS)技术将特征流分割为语义连贯事件段；通过无监督密度聚类识别重复出现的宏观场景和主题；从每个聚类中选择代表性关键帧并利用VLM生成文本描述，最终形成结构化多模态摘要。&lt;h4&gt;主要发现&lt;/h4&gt;该方法为视频内容的零样本、自动化结构分析提供了有效、可解释且与模型无关的途径。&lt;h4&gt;结论&lt;/h4&gt;该框架无需训练即可实现视频理解，结合了预训练VLM和经典机器学习算法的优势，能够生成结构化的视频摘要。&lt;h4&gt;翻译&lt;/h4&gt;大规模视觉语言模型(VLMs)在静态图像上显著的零样本推理能力尚未完全转移到视频领域。传统视频理解模型通常依赖于在标注数据集上进行大量特定任务的训练，这一过程既昂贵又难以扩展。本文提出了一种新颖的、无需训练的视频理解框架，通过协同结合预训练VLM的丰富语义先验与经典机器学习算法的模式发现能力，避免了端到端训练。我们的核心思想是将视频理解重新构建为高维语义特征空间中的自监督时空聚类问题。所提出的管道首先使用预训练VLM的冻结视觉编码器将视频流转换为语义特征轨迹。随后，我们采用核时间分割(KTS)这一稳健的机器学习技术，将连续特征流分割为离散的语义连贯事件段。这些段随后接受无监督密度聚类，以识别视频中重复出现的宏观场景和主题。通过从每个发现的聚类中选择代表性关键帧，并利用VLM的生成能力进行文本描述，我们的框架自动生成视频内容的结构化、多模态摘要。这种方法为视频内容的零样本、自动化结构分析提供了有效、可解释且与模型无关的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The remarkable zero-shot reasoning capabilities of large-scale VisualLanguage Models (VLMs) on static images have yet to be fully translated to thevideo domain. Conventional video understanding models often rely on extensive,task-specific training on annotated datasets, a process that is both costly andlimited in scalability. This paper introduces a novel, training-free frameworkfor video understanding that circumvents end-to-end training by synergisticallycombining the rich semantic priors of pre-trained VLMs with classic machinelearning algorithms for pattern discovery. Our core idea is to reframe videounderstanding as a self-supervised spatio-temporal clustering problem within ahigh-dimensional semantic feature space. The proposed pipeline first transformsa video stream into a semantic feature trajectory using the frozen visualencoder of a pre-trained VLM. Subsequently, we employ Kernel TemporalSegmentation (KTS), a robust machine learning technique, to partition thecontinuous feature stream into discrete, semantically coherent event segments.These segments are then subjected to unsupervised density-based clustering toidentify recurring macroscopic scenes and themes throughout the video. Byselecting representative keyframes from each discovered cluster and leveragingthe VLM's generative capabilities for textual description, our frameworkautomatically produces a structured, multi-modal summary of the video content.This approach provides an effective, interpretable, and model-agnostic pathwayfor zero-shot, automated structural analysis of video content.</description>
      <author>example@mail.com (Shihao Ji, Zihui Song)</author>
      <guid isPermaLink="false">2510.16781v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Survey on World Models for Embodied AI</title>
      <link>http://arxiv.org/abs/2510.16732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/Li-Zn-H/AwesomeWorldModels&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文综述了具身AI中的世界模型，提出了一个统一的框架，包括三轴分类法、系统化的数据资源和评估指标，并对最先进模型进行了定量比较。&lt;h4&gt;背景&lt;/h4&gt;具身AI需要能够感知、行动并预测行动如何重塑未来世界状态的智能体。世界模型作为内部模拟器捕捉环境动态，支持感知、预测和决策。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的具身AI中世界模型的框架，正式化问题设定和学习目标，并系统化相关资源和评估方法。&lt;h4&gt;方法&lt;/h4&gt;提出一个三轴分类法：(1)功能性：决策耦合型vs通用型；(2)时间建模：顺序模拟与推理vs全局差异预测；(3)空间表示：全局潜在向量、标记特征序列、空间潜在网格和分解渲染表示。系统化机器人、自动驾驶和一般视频环境的数据资源和评估指标，对最先进模型进行定量比较，并总结关键挑战。&lt;h4&gt;主要发现&lt;/h4&gt;世界模型在具身AI中具有统一框架，可通过三轴分类法进行系统化分类；当前研究面临统一数据集稀缺、评估指标需要更多关注物理一致性而非像素保真度、模型性能与计算效率之间的权衡，以及实现长时间一致性同时减轻错误累积等挑战。&lt;h4&gt;结论&lt;/h4&gt;世界模型研究仍面临多个开放挑战，包括需要统一的评估指标、平衡性能与计算效率、实现长时间一致性等。论文提供了一个精选的参考文献库供进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;具身AI需要能够感知、行动并预测行动如何重塑未来世界状态的智能体。世界模型作为内部模拟器捕捉环境动态，支持前向和反事实展开以支持感知、预测和决策。本综述提出了具身AI中世界模型的统一框架。具体而言，我们正式化了问题设定和学习目标，并提出了一个三轴分类法，包括：(1)功能性：决策耦合型vs通用型；(2)时间建模：顺序模拟与推理vs全局差异预测；(3)空间表示：全局潜在向量、标记特征序列、空间潜在网格和分解渲染表示。我们系统化了机器人、自动驾驶和一般视频环境的数据资源和评估指标，涵盖了像素预测质量、状态级理解和任务性能。此外，我们对最先进模型进行了定量比较，并总结了关键开放挑战，包括统一数据集的稀缺性、需要评估物理一致性而非像素保真度的评估指标、模型性能与实时控制所需计算效率之间的权衡，以及实现长时间一致性同时减轻错误累积的核心建模难度。最后，我们在https://github.com/Li-Zn-H/AwesomeWorldModels上维护了一个精选的参考文献库。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决具身AI领域中世界模型缺乏统一分类框架的问题。这个问题很重要，因为具身AI需要智能体能够感知环境、行动并预测行动如何改变未来状态，而世界模型作为内部模拟器是支持这些能力的关键组件。缺乏统一分类导致研究分散、术语不一致，难以进行有效比较和知识整合，阻碍了领域的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到具身AI中世界模型的重要性及当前研究的分类混乱问题。他们从认知科学中人类构建内部世界模型的方式获得启发，分析了世界模型的核心概念（模拟与规划、时间演化、空间表示）。作者借鉴了早期基于模型强化学习的研究、Ha和Schmidhuber的开创性工作、Dreamer系列模型以及Sora和V-JEPA 2等大型生成模型的研究成果，但发现现有综述采用功能导向或应用驱动的方法缺乏统一框架，因此提出了自己的三轴分类系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个统一的三轴分类框架来系统化组织具身AI中的世界模型研究。这三个轴分别是：功能（决策耦合vs通用目的）、时间建模（顺序模拟与推理vs全局差异预测）和空间表示（全局潜在向量、标记特征序列、空间潜在网格和分解渲染表示）。整体流程包括：介绍核心概念和理论基础、提出分类框架并映射方法、调查数据资源和评估指标、提供模型定量比较、讨论挑战和趋势、总结综述。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出统一的三轴分类框架；区分决策耦合和通用目的两种功能类型；区分顺序模拟与推理和全局差异预测两种时间建模策略；涵盖多种空间表示方法；系统整理跨领域数据资源和评估指标；提供最先进模型的定量比较；确定关键开放挑战。相比之前工作，本文提供了一个更全面、系统的分类框架，超越了之前功能导向或应用驱动的局限，覆盖更广泛的应用场景，并提供更全面的数据资源、评估指标和未来研究方向。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出一个统一的三轴分类框架，系统化地组织和分析了具身AI中的世界模型研究，为该领域提供了全面的知识图谱和未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied AI requires agents that perceive, act, and anticipate how actionsreshape future world states. World models serve as internal simulators thatcapture environment dynamics, enabling forward and counterfactual rollouts tosupport perception, prediction, and decision making. This survey presents aunified framework for world models in embodied AI. Specifically, we formalizethe problem setting and learning objectives, and propose a three-axis taxonomyencompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)Temporal Modeling, Sequential Simulation and Inference vs. Global DifferencePrediction; (3) Spatial Representation, Global Latent Vector, Token FeatureSequence, Spatial Latent Grid, and Decomposed Rendering Representation. Wesystematize data resources and metrics across robotics, autonomous driving, andgeneral video settings, covering pixel prediction quality, state-levelunderstanding, and task performance. Furthermore, we offer a quantitativecomparison of state-of-the-art models and distill key open challenges,including the scarcity of unified datasets and the need for evaluation metricsthat assess physical consistency over pixel fidelity, the trade-off betweenmodel performance and the computational efficiency required for real-timecontrol, and the core modeling difficulty of achieving long-horizon temporalconsistency while mitigating error accumulation. Finally, we maintain a curatedbibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.</description>
      <author>example@mail.com (Xinqing Li, Xin He, Le Zhang, Yun Liu)</author>
      <guid isPermaLink="false">2510.16732v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Understanding under Deictic Frame of Reference</title>
      <link>http://arxiv.org/abs/2510.16685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了TUuD框架，评估大型语言模型(LLMs)在时间参考框架(t-FoR)下如何解释时间关系，发现LLMs表现出部分类人时间认知，但推理能力仍受参考框架转换和时间距离影响。&lt;h4&gt;背景&lt;/h4&gt;理解时间是人类认知的基础，时间经验常通过空间隐喻概念化。人类依赖参考框架(FoR)解释意义，而时间参考框架(t-FoR)定义了时间关系如何相对于'现在'被感知。尽管LLMs在自然语言理解上进展显著，但其时间解释和推理能力有限。&lt;h4&gt;目的&lt;/h4&gt;引入TUuD(Deictic t-FoR下的时间理解)框架，评估当'现在'的参考点沿时间线动态移动时，LLMs如何解释时间-事件和事件-事件关系。&lt;h4&gt;方法&lt;/h4&gt;提示LLMs对当前时刻和目标事件之间的相似性进行评分(0.00-1.00)，其中相似性量化了两个点之间的感知时间对齐。&lt;h4&gt;主要发现&lt;/h4&gt;四个评估的LLMs表现出对指示性t-FoR的可测量适应，相似性评分在当前时刻达到峰值并向过去和未来事件递减。然而，这种适应在近期语境之外会减弱。&lt;h4&gt;结论&lt;/h4&gt;虽然LLMs显示出部分类人时间认知，但它们的时间推理仍然对参考框架的转换和时间距离敏感。&lt;h4&gt;翻译&lt;/h4&gt;理解时间是人类认知的基础，其中时间经验通常通过基于感官-运动经验的空间隐喻来概念化。例如，'夏天即将来临'与'我们正在接近夏天'是平行的表达。在这些表达中，人类依赖参考框架(FoR)来解释相对于特定视点的意义。将这一概念扩展到时间，时间参考框架(t-FoR)定义了时间关系如何相对于体验者的'现在'时刻被感知。虽然大型语言模型(LLMs)在自然语言理解方面显示出显著进展，但它们解释和推理时间的能力仍然有限。在这项工作中，我们引入了TUuD(Deictic t-FoR下的时间理解)框架，评估当'现在'的参考点沿时间线动态移动时，LLMs如何解释时间-事件和事件-事件关系。遵循最近关于时间认知的工作，提示LLMs对当前时刻和目标事件之间的相似性进行评分，从0.00(完全不同)到1.00(高度相似)，其中相似性量化了两个点之间的感知时间对齐。我们的结果表明，四个评估的LLMs表现出对指示性t-FoR的可测量适应，相似性评分在当前时刻达到峰值并向过去和未来事件递减。然而，这种适应在近期语境之外会减弱，这表明虽然LLMs显示出部分类人时间认知，但它们的时间推理仍然对参考框架的转换和时间距离敏感。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding time is fundamental to human cognition, where temporalexperience is often conceptualized through spatial metaphors grounded insensory-motor experience. For example, "summer is approaching" parallels "Weare approaching the summer". In such expressions, humans rely on a frame ofreference (FoR) to interpret meaning relative to a particular viewpoint.Extending this concept to time, a temporal frame of reference (t-FoR) defineshow temporal relations are perceived relative to an experiencer's moment of"now". While Large Language Models (LLMs) have shown remarkable advances innatural language understanding, their ability to interpret and reason abouttime remains limited. In this work, we introduce TUuD (Temporal Understandingunder Deictic t-FoR), a framework that evaluates how LLMs interpret time-eventand event-event relations when the reference point of "now" dynamically shiftsalong a timeline. Following recent work on temporal cognition\cite{li2025other}, LLMs are prompted to rate the similarity between thecurrent moment and a target event from 0.00 (completely dissimilar) to 1.00(highly similar), where similarity quantifies perceived temporal alignmentbetween the two points. Our results show that four evaluated LLMs exhibitmeasurable adaptation to a deictic t-FoR, with similarity ratings peakingaround the present and decreasing toward past and future events. Theadaptation, however, weakens beyond near-term contexts, suggesting that whileLLMs display partial human-like temporal cognition, their temporal reasoningremains sensitive to reference-frame shifts and temporal distance.</description>
      <author>example@mail.com (Damin Zhang, Julia Rayz)</author>
      <guid isPermaLink="false">2510.16685v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition</title>
      <link>http://arxiv.org/abs/2510.16541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GaitRDAE的区域感知动态聚合和激励框架，用于解决步态识别中动态建模运动区域的问题&lt;h4&gt;背景&lt;/h4&gt;深度学习步态识别在各种应用中取得了巨大成功，准确步态识别的关键在于考虑不同运动区域的独特和多样化的行为模式，特别是当协变量影响视觉外观时&lt;h4&gt;目的&lt;/h4&gt;解决现有方法使用预定义区域进行时间建模，为不同类型区域分配固定或等效时间尺度，难以处理随时间动态变化的运动区域的问题&lt;h4&gt;方法&lt;/h4&gt;提出GaitRDAE框架，包括两个核心模块：区域感知动态聚合（RDA）模块，为每个区域动态搜索最佳时间感受野；区域感知动态激励（RDE）模块，强调学习稳定行为模式的运动区域，抑制对易受协变量影响的静态区域的注意力&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，GaitRDAE在几个基准数据集上取得了最先进的性能&lt;h4&gt;结论&lt;/h4&gt;GaitRDAE框架能够有效处理步态识别中动态变化的运动区域，提高了识别准确率&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的步态识别在各种应用中取得了巨大成功。准确步态识别的关键在于考虑不同运动区域中独特且多样化的行为模式，特别是当协变量影响视觉外观时。然而，现有方法通常使用预定义区域进行时间建模，为不同类型的区域分配固定或等效的时间尺度，这使得难以建模随时间动态变化的运动区域并适应其特定模式。为解决这个问题，我们引入了一个区域感知动态聚合和激励框架（GaitRDAE），该框架自动搜索运动区域，分配自适应时间尺度并应用相应的注意力机制。具体而言，该框架包括两个核心模块：区域感知动态聚合（RDA）模块，为每个区域动态搜索最佳时间感受野；区域感知动态激励（RDE）模块，强调学习包含更稳定行为模式的运动区域，同时抑制对更易受协变量影响的静态区域的注意力。实验结果表明，GaitRDAE在几个基准数据集上取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TMM.2025.3613158&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based gait recognition has achieved great success in variousapplications. The key to accurate gait recognition lies in considering theunique and diverse behavior patterns in different motion regions, especiallywhen covariates affect visual appearance. However, existing methods typicallyuse predefined regions for temporal modeling, with fixed or equivalent temporalscales assigned to different types of regions, which makes it difficult tomodel motion regions that change dynamically over time and adapt to theirspecific patterns. To tackle this problem, we introduce a Region-aware DynamicAggregation and Excitation framework (GaitRDAE) that automatically searches formotion regions, assigns adaptive temporal scales and applies correspondingattention. Specifically, the framework includes two core modules: theRegion-aware Dynamic Aggregation (RDA) module, which dynamically searches theoptimal temporal receptive field for each region, and the Region-aware DynamicExcitation (RDE) module, which emphasizes the learning of motion regionscontaining more stable behavior patterns while suppressing attention to staticregions that are more susceptible to covariates. Experimental results show thatGaitRDAE achieves state-of-the-art performance on several benchmark datasets.</description>
      <author>example@mail.com (Binyuan Huang, Yongdong Luo, Xianda Guo, Xiawu Zheng, Zheng Zhu, Jiahui Pan, Chengju Zhou)</author>
      <guid isPermaLink="false">2510.16541v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales</title>
      <link>http://arxiv.org/abs/2510.16209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为StretchySnake的灵活状态空间模型训练方法，解决了视频理解中时空不灵活性问题，使模型能够更好地处理不同分辨率和长度的视频，在多种动作识别基准测试中表现优异。&lt;h4&gt;背景&lt;/h4&gt;状态空间模型(SSMs)已成为transformers的有力替代方案，具有线性计算复杂度和隐藏状态递归特性，特别适合建模长序列。然而，当前视频理解训练方法针对transformers设计，未能充分利用SSMs的独特属性，导致模型在训练中未见过的空间和时间分辨率视频上性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种灵活的训练方法，利用并改进SSMs的固有适应性，使模型能够无缝处理从短时精细片段到长时间复杂活动的各种视频，解决时空不灵活性限制模型在短视频和长视频间保持性能的问题。&lt;h4&gt;方法&lt;/h4&gt;在训练过程中对视频进行不同时间和空间分辨率的采样，并动态插值模型权重以适应任何时空尺度。研究提出了五种不同的灵活训练变体，并确定了视频SSMs最有效的策略，创建了名为StretchySnake的模型。&lt;h4&gt;主要发现&lt;/h4&gt;StretchySnake在短动作(UCF-101, HMDB-51)和长动作(COIN, Breakfast)基准测试中，超越了transformer和SSM基线，性能提升高达28%，同时展现出对精细动作(SSV2, Diving-48)的强大适应能力。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一种简单的即插即用训练方案，使视频SSMs在各种动作识别场景中更加健壮、分辨率无关且高效，解决了视频模型在时空变化下的性能退化问题。&lt;h4&gt;翻译&lt;/h4&gt;状态空间模型(SSMs)已成为各种任务中transformers的有竞争力的替代方案。它们的线性复杂度和隐藏状态递归特性使它们特别适合建模长序列，而注意力机制则变得二次方昂贵。然而，当前视频理解的训练方法是为transformers量身定制的，未能充分利用SSMs的独特属性。例如，视频模型通常在固定分辨率和视频长度下训练，以平衡注意力成本的二次方扩展与性能。因此，当在训练中未见过的空间和时间分辨率的视频上评估时，这些模型性能会下降；我们称这种特性为时空不灵活性。在动作识别的背景下，这严重限制了模型在短视频和长视频之间保持性能的能力。因此，我们提出了一种灵活的训练方法，利用并改进SSMs的固有适应性。我们的方法在训练过程中对视频进行不同时间和空间分辨率的采样，并动态插值模型权重以适应任何时空尺度。这使我们的SSM（我们称之为StretchySnake）具有时空灵活性，能够无缝处理从短时精细片段到长时间复杂活动的各种视频。我们介绍并比较了五种不同的灵活训练变体，确定了视频SSMs最有效的策略。在短动作(UCF-101, HMDB-51)和长动作(COIN, Breakfast)基准测试中，StretchySnake超越了transformer和SSM基线，性能提升高达28%，对精细动作(SSV2, Diving-48)具有强大的适应能力。因此，我们的方法提供了一种简单的即插即用训练方案，使视频SSMs在各种动作识别场景中更加健壮、分辨率无关且高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State space models (SSMs) have emerged as a competitive alternative totransformers in various tasks. Their linear complexity and hidden-staterecurrence make them particularly attractive for modeling long sequences,whereas attention becomes quadratically expensive. However, current trainingmethods for video understanding are tailored towards transformers and fail tofully leverage the unique attributes of SSMs. For example, video models areoften trained at a fixed resolution and video length to balance the quadraticscaling of attention cost against performance. Consequently, these modelssuffer from degraded performance when evaluated on videos with spatial andtemporal resolutions unseen during training; a property we call spatio-temporalinflexibility. In the context of action recognition, this severely limits amodel's ability to retain performance across both short- and long-form videos.Therefore, we propose a flexible training method that leverages and improvesthe inherent adaptability of SSMs. Our method samples videos at varyingtemporal and spatial resolutions during training and dynamically interpolatesmodel weights to accommodate any spatio-temporal scale. This instills our SSM,which we call StretchySnake, with spatio-temporal flexibility and enables it toseamlessly handle videos ranging from short, fine-grained clips to long,complex activities. We introduce and compare five different variants offlexible training, and identify the most effective strategy for video SSMs. Onshort-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,StretchySnake outperforms transformer and SSM baselines alike by up to 28%,with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,our method provides a simple drop-in training recipe that makes video SSMs morerobust, resolution-agnostic, and efficient across diverse action recognitionscenarios.</description>
      <author>example@mail.com (Nyle Siddiqui, Rohit Gupta, Sirnam Swetha, Mubarak Shah)</author>
      <guid isPermaLink="false">2510.16209v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Referential Consistency: Do LLMs Favor Sequences Over Absolute Time References?</title>
      <link>http://arxiv.org/abs/2510.15513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP Main Long Paper 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文针对大型语言模型(LLMs)在时间参考一致性方面的不足，提出了一个新的基准测试TEMP-ReCon和一种基于推理路径对齐的模型UnTRaP，以增强LLMs在时间敏感领域的时间推理能力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)正越来越多地被作为知识来源的替代品，在法律、医疗保健和金融等时间敏感领域尤为显著。LLMs需要具备事实准确性和时间维度上的一致性，但目前确保LLMs时间一致性的努力仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;引入一个名为'temporal referential consistency'的新基准，并开发资源TEMP-ReCon，用于评估各种开源和闭源LLMs在不同语言环境(包括英语、法语和罗马尼亚语)中的时间参考一致性。&lt;h4&gt;方法&lt;/h4&gt;提出UnTRaP模型，这是一种基于推理路径对齐的模型，旨在提高LLMs的时间参考一致性。通过实验验证其与基线模型相比的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现大型语言模型确实表现出不足的时间参考一致性。通过引入新基准和资源，以及提出UnTRaP模型，可以有效解决这一问题。&lt;h4&gt;结论&lt;/h4&gt;UnTRaP模型相比几个基线模型更为有效，能够增强LLMs的时间参考一致性，为LLMs在时间敏感领域的应用提供了更好的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)作为知识来源替代品的日益普及标志着各领域的重要范式转变，包括法律、医疗保健和金融等时间敏感领域。为了满足这一扩展角色，LLMs不仅需要事实准确，还需要在时间维度上表现出一致性，这需要强大的时间推理能力。尽管有这一关键需求，确保LLMs时间一致性的努力仍然很少，包括在时间敏感查询中评估或增强LLMs时间参考方面的明显缺失。在本文中，我们通过引入一个名为'temporal referential consistency'的新基准以及资源TEMP-ReCon来填补这一空白，该资源用于评估各种开源和闭源LLMs在不同资源丰富度的语言环境(包括英语、法语和罗马尼亚语)中的表现。研究结果强调LLMs确实表现出不足的时间参考一致性。为此，我们提出了UnTRaP，一种基于推理路径对齐的模型，旨在提高LLMs的时间参考一致性。我们的实证实验证明了UnTRaP相比几个基线模型的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing acceptance of large language models (LLMs) as an alternativeto knowledge sources marks a significant paradigm shift across various domains,including time-sensitive fields such as law, healthcare, and finance. Tofulfill this expanded role, LLMs must not only be factually accurate but alsodemonstrate consistency across temporal dimensions, necessitating robusttemporal reasoning capabilities. Despite this critical requirement, efforts toensure temporal consistency in LLMs remain scarce including noticeable absenceof endeavors aimed at evaluating or augmenting LLMs across temporal referencesin time-sensitive inquiries. In this paper, we seek to address this gap byintroducing a novel benchmark entitled temporal referential consistency,accompanied by a resource TEMP-ReCon designed to benchmark a wide range of bothopen-source and closed-source LLMs with various linguistic contextscharacterized by differing resource richness (including English, French, andRomanian). The findings emphasis that LLMs do exhibit insufficient temporalreferent consistency. To address this, we propose \newmodel, a reasoning pathalignment-based model that aims to enhance the temporal referential consistencyof LLMs. Our empirical experiments substantiate the efficacy of UnTRaP comparedto several baseline models.</description>
      <author>example@mail.com (Ashutosh Bajpai, Tanmoy Chakraborty)</author>
      <guid isPermaLink="false">2510.15513v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models</title>
      <link>http://arxiv.org/abs/2510.15148v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了XModBench，一个用于评估全模态大语言模型(OLLMs)跨模态一致性的基准测试。研究发现，即使是目前最强的模型如Gemini 2.5 Pro，在空间和时间推理方面表现不佳，存在模态差异和方向性不平衡问题，表明当前OLLMs距离真正的模态不变推理还有很大差距。&lt;h4&gt;背景&lt;/h4&gt;全模态大语言模型(OLLMs)旨在统一音频、视觉和文本理解于单一框架。现有基准主要评估通用跨模态问答能力，但不清楚OLLMs是否实现了模态不变推理或存在模态特定偏差。&lt;h4&gt;目的&lt;/h4&gt;引入XModBench，一个大规模的三模态基准，专门用于测量跨模态一致性，评估OLLMs的模态不变推理能力、模态差异和方向性不平衡。&lt;h4&gt;方法&lt;/h4&gt;XModBench包含60,828个多选题，涵盖五个任务家族，系统覆盖了问答对中的所有六种模态组合，能够对OLLM的模态不变推理、模态差异和方向性不平衡进行细粒度诊断。&lt;h4&gt;主要发现&lt;/h4&gt;即使是目前最强的模型Gemini 2.5 Pro，(i)在空间和时间推理方面表现不佳，准确率低于60%，(ii)存在持续的模态差异，当相同语义内容通过音频而非文本传达时性能显著下降，(iii)表现出系统性的方向性不平衡，当视觉作为上下文时一致性低于文本。&lt;h4&gt;结论&lt;/h4&gt;当前OLLMs距离真正的模态不变推理还有很长的路要走，XModBench可作为评估和改进跨模态能力的基本诊断工具。&lt;h4&gt;翻译&lt;/h4&gt;全模态大语言模型(OLLMs)旨在在单一框架内统一音频、视觉和文本理解。虽然现有基准主要评估通用的跨模态问答能力，但尚不清楚OLLMs是否实现了模态不变的推理或表现出模态特定的偏差。我们引入了XModBench，一个大规模的三模态基准，专门设计用于测量跨模态一致性。XModBench包含60,828个多选题，涵盖五个任务家族，并系统性地覆盖了问答对中的所有六种模态组合，能够对OLLM的模态不变推理、模态差异和方向性不平衡进行细粒度诊断。实验表明，即使是最强的模型Gemini 2.5 Pro，(i)在空间和时间推理方面表现不佳，准确率低于60%，(ii)表现出持续的模态差异，当相同语义内容通过音频而非文本传达时，性能显著下降，(iii)显示出系统性的方向性不平衡，当视觉作为上下文时，一致性低于文本。这些发现表明，当前的OLLMs距离真正的模态不变推理还有很长的路要走，并将XModBench定位为评估和改进跨模态能力的基本诊断工具。所有数据和评估工具将在https://xingruiwang.github.io/projects/XModBench/提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Omni-modal large language models (OLLMs) aim to unify audio, vision, and textunderstanding within a single framework. While existing benchmarks primarilyevaluate general cross-modal question-answering ability, it remains unclearwhether OLLMs achieve modality-invariant reasoning or exhibit modality-specificbiases. We introduce XModBench, a large-scale tri-modal benchmark explicitlydesigned to measure cross-modal consistency. XModBench comprises 60,828multiple-choice questions spanning five task families and systematically coversall six modality compositions in question-answer pairs, enabling fine-graineddiagnosis of an OLLM's modality-invariant reasoning, modality disparity, anddirectional imbalance. Experiments show that even the strongest model, Gemini2.5 Pro, (i) struggles with spatial and temporal reasoning, achieving less than60% accuracy, (ii) reveals persistent modality disparities, with performancedropping substantially when the same semantic content is conveyed through audiorather than text, and (iii) shows systematic directional imbalance, exhibitinglower consistency when vision serves as context compared to text. Thesefindings indicate that current OLLMs remain far from truly modality-invariantreasoning and position XModBench as a fundamental diagnostic tool forevaluating and improving cross-modal competence. All data and evaluation toolswill be available at https://xingruiwang.github.io/projects/XModBench/.</description>
      <author>example@mail.com (Xingrui Wang, Jiang Liu, Chao Huang, Xiaodong Yu, Ze Wang, Ximeng Sun, Jialian Wu, Alan Yuille, Emad Barsoum, Zicheng Liu)</author>
      <guid isPermaLink="false">2510.15148v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting</title>
      <link>http://arxiv.org/abs/2510.16053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;交通预测是智能交通系统的核心技术，图神经网络已成为该领域的主流方法，但在处理事件信息方面仍面临挑战。&lt;h4&gt;背景&lt;/h4&gt;随着城市化进程加快，交通拥堵问题加剧，需要可靠且响应迅速的预测模型来改善城市资源分配和出行体验。&lt;h4&gt;目的&lt;/h4&gt;开发能够有效捕捉交通网络空间依赖关系和时间演化模式的预测模型，提高对复杂交通状况的响应能力。&lt;h4&gt;方法&lt;/h4&gt;采用图神经网络(GNNs)作为主要技术路线，结合图卷积结构和时间建模机制，包括STGCN、GraphWaveNet、STWave和D2STGNN等模型，并探索融入事件信息的方法。&lt;h4&gt;主要发现&lt;/h4&gt;GNNs在捕捉周期性交通模式方面特别有效；早期基于人工特征的方法虽能提升对特定事件的响应，但严重依赖领域专家先验知识，难以泛化到复杂未知事件，且低维人工特征会导致语义细节丢失。&lt;h4&gt;结论&lt;/h4&gt;需要减少对人工特征的依赖，开发更有效的方法来处理交通预测中的事件信息，提高模型对未知事件的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;准确的交通预测是构建智能交通系统的核心技术，能够更好地进行城市资源分配和改善出行体验。随着城市化的发展，交通拥堵加剧，凸显了对可靠且响应迅速的预测模型的需求。近年来，深度学习，特别是图神经网络(GNNs)，已成为交通预测的主流范式。GNNs能够有效捕捉道路网络拓扑中的复杂空间依赖关系和交通流量数据中的动态时间演化模式。诸如STGCN和GraphWaveNet等基础模型，以及包括STWave和D2STGNN在内的最新发展，在标准交通数据集上取得了令人印象深刻的性能。这些方法结合了复杂的图卷积结构和时间建模机制，在捕捉和预测具有周期性规律的交通模式方面表现出特别的有效性。为了应对这一挑战，研究人员探索了多种融入事件信息的方式。早期尝试主要依赖人工设计的特征。例如，一些方法引入了人工定义的事件影响分数，或为不同事件引起的交通状况构建特定的子图。虽然这些方法在某种程度上增强了对特定事件的响应能力，但其主要缺点在于严重依赖领域专家的先验知识，使得对多样且复杂的未知事件的泛化变得困难，而低维人工特征往往导致丰富语义细节的丢失。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3748636.3762776&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate traffic forecasting is a core technology for building IntelligentTransportation Systems (ITS), enabling better urban resource allocation andimproved travel experiences. With growing urbanization, traffic congestion hasintensified, highlighting the need for reliable and responsive forecastingmodels. In recent years, deep learning, particularly Graph Neural Networks(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs caneffectively capture complex spatial dependencies in road network topology anddynamic temporal evolution patterns in traffic flow data. Foundational modelssuch as STGCN and GraphWaveNet, along with more recent developments includingSTWave and D2STGNN, have achieved impressive performance on standard trafficdatasets. These approaches incorporate sophisticated graph convolutionalstructures and temporal modeling mechanisms, demonstrating particulareffectiveness in capturing and forecasting traffic patterns characterized byperiodic regularities. To address this challenge, researchers have exploredvarious ways to incorporate event information. Early attempts primarily reliedon manually engineered event features. For instance, some approaches introducedmanually defined incident effect scores or constructed specific subgraphs fordifferent event-induced traffic conditions. While these methods somewhatenhance responsiveness to specific events, their core drawback lies in a heavyreliance on domain experts' prior knowledge, making generalization to diverseand complex unknown events difficult, and low-dimensional manual features oftenlead to the loss of rich semantic details.</description>
      <author>example@mail.com (Chenyang Yu, Xinpeng Xie, Yan Huang, Chenxi Qiu)</author>
      <guid isPermaLink="false">2510.16053v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model</title>
      <link>http://arxiv.org/abs/2510.17684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了IC-MoE模型，一种智能通信混合专家增强的医学图像分割基础模型，解决了现有微调方法中高级特征表示不足和预训练权重结构完整性受损的问题。&lt;h4&gt;背景&lt;/h4&gt;基础模型在医学图像分割领域已取得显著性能，自适应微调自然图像分割基础模型对医学图像分割任务至关重要。然而，现有微调方法存在两个局限性：高级特征表示不足和微调过程破坏预训练权重的结构完整性。&lt;h4&gt;目的&lt;/h4&gt;解决现有微调方法的局限性，提出一个能够增强高级特征表示同时保持预训练权重结构完整性的医学图像分割基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出IC-MoE模型，包含两个核心创新：1) 构建基础专家、语义专家和自适应专家，实现像素概率自适应投票策略，通过标签一致性和负载平衡进行专家选择和融合；2) 提出语义引导的对比学习方法，解决对比学习中的弱监督问题。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公共医学图像分割数据集上的大量实验表明，IC-MoE优于其他最先进模型。IC-MoE有效地为基础医学图像分割模型补充了高级特征和预训练结构完整性，并在多样化医学图像分割场景中展现出优越的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;IC-MoE模型成功解决了现有微调方法的局限性，能够在增强高级特征表示的同时保持预训练权重的结构完整性，为医学图像分割任务提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;医学图像分割的基础模型已取得显著性能。自然图像分割基础模型的自适应微调对医学图像分割任务至关重要。然而，现有微调方法存在一些局限性：1) 高级特征表示不足；2) 微调过程破坏了预训练权重的结构完整性。受这些关键问题的启发，我们提出了一个智能通信混合专家增强的医学图像分割基础模型，名为IC-MoE，包含两个核心想法：1) 我们构建基础专家、语义专家和自适应专家。此外，我们实现了像素概率自适应投票策略，通过标签一致性和负载平衡实现专家选择和融合。这种方法初步增强了高级特征的表示能力，同时保留了预训练权重的结构完整性。2) 我们提出了一种语义引导的对比学习方法，解决了对比学习中弱监督的问题。这种方法进一步增强了高级特征的表示能力，同时保留了预训练权重的结构完整性。在三个公共医学图像分割数据集上的大量实验表明，IC-MoE优于其他最先进的模型。因此，所提出的IC-MoE有效地为基础医学图像分割模型补充了高级特征和预训练结构完整性。我们还验证了IC-MoE在多样化医学图像分割场景中的优越泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for medical image segmentation have achieved remarkableperformance. Adaptive fine-tuning of natural image segmentation foundationmodels is crucial for medical image segmentation tasks. However, somelimitations exist in existing fine-tuning methods: 1) insufficientrepresentation of high-level features and 2) the fine-tuning process disruptsthe structural integrity of pretrained weights. Inspired by these criticalproblems, we propose an intelligent communication mixture-of-expertsboosted-medical image segmentation foundation model, named IC-MoE, with twofoldideas: 1) We construct basic experts, semantic experts, and adaptive experts.Moreover, we implement a pixel probability adaptive voting strategy, whichenables expert selection and fusion through label consistency and loadbalancing. This approach preliminarily enhances the representation capabilityof high-level features while preserving the structural integrity of pretrainedweights. 2) We propose a semantic-guided contrastive learning method to addressthe issue of weak supervision in contrastive learning. This method furtherenhances the representation capability of high-level features while preservingthe structural integrity of pretrained weights. Extensive experiments acrossthree public medical image segmentation datasets demonstrate that the IC-MoEoutperforms other SOTA models. Consequently, the proposed IC-MoE effectivelysupplements foundational medical image segmentation models with high-levelfeatures and pretrained structural integrity. We also validate the superiorgeneralizability of the IC-MoE across diverse medical image segmentationscenarios.</description>
      <author>example@mail.com (Xinwei Zhang, Hu Chen, Zhe Yuan, Sukun Tian, Peng Feng)</author>
      <guid isPermaLink="false">2510.17684v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Curiosity-driven RL for symbolic equation solving</title>
      <link>http://arxiv.org/abs/2510.17022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the NeurIPS 2025 MATH-AI Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究展示了增强的PPO算法在解决符号数学问题上的有效性，特别是能够处理涉及根式、指数和三角函数的非线性方程。&lt;h4&gt;背景&lt;/h4&gt;先前的研究表明对比学习可以解决单变量线性方程，但强化学习在符号数学领域的应用尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;探索强化学习是否可以有效地应用于符号数学问题。&lt;h4&gt;方法&lt;/h4&gt;使用无模型PPO算法，并加入基于好奇心的探索机制和基于图的动作表示。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够解决涉及根式、指数和三角函数的非线性方程，而不仅仅是简单的线性方程。&lt;h4&gt;结论&lt;/h4&gt;基于好奇心的探索机制可能对解决一般符号推理任务具有实用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们探索强化学习是否可以用于符号数学。先前的工作表明对比学习可以解决单变量线性方程。我们展示了无模型PPO结合基于好奇心的探索和基于图的动作可以解决非线性方程，如涉及根式、指数和三角函数的方程。我们的研究表明基于好奇心的探索可能对一般符号推理任务有用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We explore if RL can be useful for symbolic mathematics. Previous work showedcontrastive learning can solve linear equations in one variable. We showmodel-free PPO \cite{schulman2017proximal} augmented with curiosity-basedexploration and graph-based actions can solve nonlinear equations such as thoseinvolving radicals, exponentials, and trig functions. Our work suggestscuriosity-based exploration may be useful for general symbolic reasoning tasks.</description>
      <author>example@mail.com (Kevin P. O Keeffe)</author>
      <guid isPermaLink="false">2510.17022v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.16797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MOSAIC是一种多阶段框架，用于句子嵌入模型的领域自适应，结合了领域特定的掩码监督。&lt;h4&gt;背景&lt;/h4&gt;大规模通用领域句子嵌入模型适应到专业领域面临的挑战。&lt;h4&gt;目的&lt;/h4&gt;有效学习领域相关表示，同时保持原始模型的强语义区分特性。&lt;h4&gt;方法&lt;/h4&gt;通过在统一训练流程中联合优化掩码语言模型(MLM)和对比目标。&lt;h4&gt;主要发现&lt;/h4&gt;在高资源和低资源领域都取得了显著提升，NDCG@10指标比基线提高最多13.4%。&lt;h4&gt;结论&lt;/h4&gt;平衡的联合监督和阶段适应对有效领域自适应至关重要。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了MOSAIC（具有选择性适应的掩码目标用于领域内对比学习），这是一种用于句子嵌入模型领域自适应的多阶段框架，它结合了联合领域特定的掩码监督。我们的方法解决了将大规模通用领域句子嵌入模型适应到专业领域的挑战。通过在统一的训练流程中联合优化掩码语言模型(MLM)和对比目标，我们的方法能够有效地学习领域相关表示，同时保持原始模型的强语义区分特性。我们在高资源和低资源领域上都经验性地验证了我们的方法，在NDCG@10（标准化折损累积增益）上比强大的通用领域基线提高了最多13.4%。全面的消融研究进一步证明了每个组件的有效性，强调了平衡联合监督和阶段适应的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domainContrastive learning), a multi-stage framework for domain adaptation ofsentence embedding models that incorporates joint domain-specific maskedsupervision. Our approach addresses the challenges of adapting large-scalegeneral-domain sentence embedding models to specialized domains. By jointlyoptimizing masked language modeling (MLM) and contrastive objectives within aunified training pipeline, our method enables effective learning ofdomain-relevant representations while preserving the robust semanticdiscrimination properties of the original model. We empirically validate ourapproach on both high-resource and low-resource domains, achieving improvementsup to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over stronggeneral-domain baselines. Comprehensive ablation studies further demonstratethe effectiveness of each component, highlighting the importance of balancedjoint supervision and staged adaptation.</description>
      <author>example@mail.com (Vera Pavlova, Mohammed Makhlouf)</author>
      <guid isPermaLink="false">2510.16797v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization</title>
      <link>http://arxiv.org/abs/2510.16704v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的领域连接对比学习(DCCL)方法，用于解决领域泛化(DG)中的问题。研究发现直接应用对比学习(CL)会降低DG性能，原因是缺乏类内连接。DCCL通过改进数据增强和跨域正样本，以及提出模型锚定和生成变换损失来增强类内连接，从而提升DG性能。实验表明，该方法在五个标准DG基准上优于最先进的基线方法，且不需要领域监督。&lt;h4&gt;背景&lt;/h4&gt;训练和测试样本之间的分布偏移在实践中经常发生，阻碍了模型的泛化性能。这促使了对领域泛化(DG)的研究，DG旨在仅使用源域数据来预测未见过的目标域数据的标签。&lt;h4&gt;目的&lt;/h4&gt;解决直接应用对比学习(CL)降低领域泛化(DG)性能的问题，增强跨领域的概念连接，获得适用于DG的可泛化表示。&lt;h4&gt;方法&lt;/h4&gt;提出领域连接对比学习(DCCL)范式。在数据方面，引入更积极的数据增强和跨域正样本以改善类内连接；在模型方面，提出模型锚定来利用预训练表示中的类内连接，并通过生成变换损失补充锚定。&lt;h4&gt;主要发现&lt;/h4&gt;在DG设置中缺乏类内连接是导致直接应用CL降低性能的原因；提出的DCCL方法通过增强类内连接，在五个标准DG基准上优于最先进的基线方法，且不需要领域监督。&lt;h4&gt;结论&lt;/h4&gt;领域连接对比学习(DCCL)是解决DG中分布偏移问题的有效方法，通过增强类内连接和跨领域概念连接，能够获得更好的可泛化表示。&lt;h4&gt;翻译&lt;/h4&gt;训练和测试样本之间的分布偏移在实践中经常发生，并阻碍了模型的泛化性能。这一关键挑战促使了对领域泛化(DG)的研究，DG旨在仅使用源域数据来预测未见过的目标域数据的标签。直观上，对比学习(CL)中学到的类分离表示应该能够改善DG，但实际情况恰恰相反：直接应用CL会降低性能。作者通过CL理论的见解分析了这一现象，发现在DG设置中缺乏类内连接导致了这一缺陷。因此，作者提出了一个新的范式——领域连接对比学习(DCCL)，以增强跨领域的概念连接并获得适用于DG的可泛化表示。在数据方面，引入更积极的数据增强和跨域正样本以改善类内连接；在模型方面，为更好地嵌入未见过的测试域，作者提出模型锚定来利用预训练表示中的类内连接，并通过生成变换损失补充锚定。在五个标准的DG基准上进行了大量实验，结果验证了DCCL优于最先进的基线方法，甚至不需要领域监督。详细的模型实现和代码可通过https://github.com/weitianxin/DCCL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3690624.3709280&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distribution shifts between training and testing samples frequently occur inpractice and impede model generalization performance. This crucial challengethereby motivates studies on domain generalization (DG), which aim to predictthe label on unseen target domain data by solely using data from sourcedomains. It is intuitive to conceive the class-separated representationslearned in contrastive learning (CL) are able to improve DG, while the realityis quite the opposite: users observe directly applying CL deteriorates theperformance. We analyze the phenomenon with the insights from CL theory anddiscover lack of intra-class connectivity in the DG setting causes thedeficiency. We thus propose a new paradigm, domain-connecting contrastivelearning (DCCL), to enhance the conceptual connectivity across domains andobtain generalizable representations for DG. On the data side, more aggressivedata augmentation and cross-domain positive samples are introduced to improveintra-class connectivity. On the model side, to better embed the unseen testdomains, we propose model anchoring to exploit the intra-class connectivity inpre-trained representations and complement the anchoring with generativetransformation loss. Extensive experiments on five standard DG benchmarks areperformed. The results verify that DCCL outperforms state-of-the-art baselineseven without domain supervision. The detailed model implementation and the codeare provided through https://github.com/weitianxin/DCCL</description>
      <author>example@mail.com (Tianxin Wei, Yifan Chen, Xinrui He, Wenxuan Bao, Jingrui He)</author>
      <guid isPermaLink="false">2510.16704v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions</title>
      <link>http://arxiv.org/abs/2510.16540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025 (poster). This is the camera-ready version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出READ方法，通过添加两个辅助目标到对比学习中，增强视觉-语言模型的组合推理能力。READ-CLIP在五个主要组合推理基准测试中取得最先进性能，比传统微调基线高4.1%。&lt;h4&gt;背景&lt;/h4&gt;尽管近期有所进展，但使用标准对比目标训练的视觉-语言模型在组合推理（理解视觉和语言元素间结构关系）方面仍存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种微调方法，增强视觉-语言模型的组合推理能力，解决文本编码器关注单词而非单词间关系的问题。&lt;h4&gt;方法&lt;/h4&gt;引入READ方法，添加两个辅助目标到对比学习中：(1)令牌级重建目标，使用冻结预训练解码器重建替代标题；(2)句子级对齐目标，在嵌入空间中对齐释义句子。&lt;h4&gt;主要发现&lt;/h4&gt;READ-CLIP在五个组合推理基准测试中达到最先进性能，比最强基线高4.1%；READ应用于其他CLIP变体也提高了性能；重建和对齐目标提供互补好处。&lt;h4&gt;结论&lt;/h4&gt;READ方法有效增强了视觉-语言模型的组合推理能力，重建目标促进编码器捕获单词间关系，对齐目标确保不同措辞表达的释义具有一致表示。&lt;h4&gt;翻译&lt;/h4&gt;尽管近期有所进展，但使用标准对比目标训练的视觉-语言模型仍然在组合推理——即理解视觉和语言元素之间结构关系的能力——方面存在困难。这一缺点主要是由于文本编码器倾向于关注单个单词而非它们之间的关系，这种局限性通过主要将单词与视觉对象对齐的对比训练得到了强化。在本文中，我们引入了READ（文本描述的重建和对齐），这是一种微调方法，通过向对比学习中添加两个辅助目标来增强组合推理能力：(1)令牌级重建目标，其中冻结的预训练解码器基于原始标题的嵌入重建替代标题；(2)句子级对齐目标，在嵌入空间中明确对齐释义句子。我们表明，通过将READ方法应用于预训练的CLIP模型得到的READ-CLIP模型在五个主要的组合推理基准测试中取得了最先进的性能，比最强的传统微调基线高出最多4.1%。此外，将READ应用于现有的CLIP变体（包括NegCLIP和FSC-CLIP）也提高了这些基准测试的性能。定量和定性分析表明，我们提出的目标——重建和对齐——提供了互补的好处：前者鼓励编码器捕获标题中单词之间的关系，而后者确保用不同措辞表达的释义具有一致的表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite recent advances, vision-language models trained with standardcontrastive objectives still struggle with compositional reasoning -- theability to understand structured relationships between visual and linguisticelements. This shortcoming is largely due to the tendency of the text encoderto focus on individual words rather than their relations, a limitationreinforced by contrastive training that primarily aligns words with visualobjects. In this paper, we introduce REconstruction and Alignment of textDescriptions (READ), a fine-tuning method designed to enhance compositionalreasoning by adding two auxiliary objectives to the contrastive learning: (1) atoken-level reconstruction objective, where a frozen pre-trained decoderreconstructs alternative captions based on the embedding of the originalcaption; and (2) a sentence-level alignment objective, which explicitly alignsparaphrased sentences in the embedding space. We show that READ-CLIP, a modelderived by applying the READ method to the pre-trained CLIP model, achieves thestate-of-the-art performance across five major compositional reasoningbenchmarks, outperforming the strongest conventional fine-tuning baseline by upto 4.1%. Furthermore, applying the READ to existing CLIP variants (includingNegCLIP and FSC-CLIP) also improves performance on these benchmarks.Quantitative and qualitative analyses reveal that our proposed objectives --reconstruction and alignment -- offer complementary benefits: the formerencourages the encoder to capture relationships between words within a caption,while the latter ensures consistent representations for paraphrases expressedwith different wording.</description>
      <author>example@mail.com (Jihoon Kwon, Kyle Min, Jy-yong Sohn)</author>
      <guid isPermaLink="false">2510.16540v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy</title>
      <link>http://arxiv.org/abs/2510.16450v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种弱监督域适应方法，用于电子显微镜图像中线粒体的高效分割，通过多任务学习框架和实例感知的伪标签选择策略，显著提高了分割性能。&lt;h4&gt;背景&lt;/h4&gt;从各种电子显微镜图像中分割大量线粒体实例对生物和神经科学研究具有重要价值。无监督域适应方法虽然可以缓解域偏移并降低标注成本，但在实际应用中性能较低。&lt;h4&gt;目的&lt;/h4&gt;研究弱监督域适应(WDA)方法，利用目标域上的稀疏点标签，这些标签需要最少的标注工作和专业知识，以实现高效准确的线粒体分割。&lt;h4&gt;方法&lt;/h4&gt;引入一个多任务学习框架，同时进行分割和中心检测，采用新颖的交叉教学机制和面向类的跨域对比学习。提出分割自训练，使用实例感知的伪标签(IPL)选择策略，帮助选择语义上可靠和多样的伪标签。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的数据集上验证，该方法优于现有的UDA和WDA方法，显著缩小了与监督上限的性能差距。在UDA设置下，也显著优于其他UDA技术。&lt;h4&gt;结论&lt;/h4&gt;所提出的弱监督域适应方法通过有效利用稀疏点标注和实例感知的伪标签策略，实现了电子显微镜图像中线粒体的高效分割，为生物和神经科学研究提供了有力工具。&lt;h4&gt;翻译&lt;/h4&gt;从各种电子显微镜图像中高效分割大量线粒体实例对生物和神经科学研究非常有价值。尽管无监督域适应方法可以帮助缓解域偏移并降低每个域的标注成本，但它们在实际应用中通常具有相对较低的性能。因此，我们研究了弱监督域适应(WDA)，它利用目标域上的额外稀疏点标签，这些标签需要最少的标注工作和最少的专家知识。为了充分利用不完整和不精确的点标注，我们引入了一个多任务学习框架，通过新颖的交叉教学机制和面向类的跨域对比学习共同进行分割和中心检测。虽然利用未标记的图像区域至关重要，我们引入了分割自训练，采用新颖的实例感知的伪标签(IPL)选择策略。与通常依赖像素级伪标签过滤的现有方法不同，IPL在检测任务的帮助下，在语义上选择可靠和多样的伪标签。在具有挑战性的数据集上进行的全面验证和比较表明，我们的方法优于现有的UDA和WDA方法，显著缩小了与监督上限的性能差距。此外，在UDA设置下，我们的方法也实现了对其他UDA技术的显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Annotation-efficient segmentation of the numerous mitochondria instances fromvarious electron microscopy (EM) images is highly valuable for biological andneuroscience research. Although unsupervised domain adaptation (UDA) methodscan help mitigate domain shifts and reduce the high costs of annotating eachdomain, they typically have relatively low performance in practicalapplications. Thus, we investigate weakly supervised domain adaptation (WDA)that utilizes additional sparse point labels on the target domain, whichrequire minimal annotation effort and minimal expert knowledge. To take fulluse of the incomplete and imprecise point annotations, we introduce a multitasklearning framework that jointly conducts segmentation and center detection witha novel cross-teaching mechanism and class-focused cross-domain contrastivelearning. While leveraging unlabeled image regions is essential, we introducesegmentation self-training with a novel instance-aware pseudo-label (IPL)selection strategy. Unlike existing methods that typically rely on pixel-wisepseudo-label filtering, the IPL semantically selects reliable and diversepseudo-labels with the help of the detection task. Comprehensive validationsand comparisons on challenging datasets demonstrate that our method outperformsexisting UDA and WDA methods, significantly narrowing the performance gap withthe supervised upper bound. Furthermore, under the UDA setting, our method alsoachieves substantial improvements over other UDA techniques.</description>
      <author>example@mail.com (Shan Xiong, Jiabao Chen, Ye Wang, Jialin Peng)</author>
      <guid isPermaLink="false">2510.16450v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Toward General Digraph Contrastive Learning: A Dual Spatial Perspective</title>
      <link>http://arxiv.org/abs/2510.16311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;S2-DiGCL是一种针对有向图对比学习的新型框架，通过结合复域和实域的空间视角，构建高质量的正负样本，实现更通用和鲁棒的有向图对比学习。&lt;h4&gt;背景&lt;/h4&gt;现有图对比学习方法主要关注无向图，忽略了现实网络(如社交网络和推荐系统)中基本且不可或缺的方向信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从复杂和真实领域角度强调空间洞察力的有向图对比学习框架，以捕获方向信息。&lt;h4&gt;方法&lt;/h4&gt;S2-DiGCL从复域角度将个性化扰动引入磁拉普拉斯矩阵以自适应调整边相位和方向语义；从实域角度采用基于路径的子图增强策略来捕获细粒度的局部非对称性和拓扑依赖。&lt;h4&gt;主要发现&lt;/h4&gt;在7个真实有向图数据集上的广泛实验表明，S2-DiGCL方法具有优越性，在监督和非监督设置下，节点分类和链路预测任务均达到了SOTA性能，分别提高了4.41%和4.34%。&lt;h4&gt;结论&lt;/h4&gt;通过联合利用互补的空间视角，S2-DiGCL能够构建高质量的正负样本，实现更通用和鲁棒的有向图对比学习。&lt;h4&gt;翻译&lt;/h4&gt;图对比学习(GCL)已成为从图中提取一致表示的强大工具，独立于标记信息。然而，现有方法主要关注无向图，忽略了现实网络(如社交网络和推荐系统)中基本且不可或缺的方向信息。本文介绍了S2-DiGCL，一种新型框架，从复杂和真实领域角度强调有向图(有向图)对比学习的空间洞察力。从复域角度，S2-DiGCL将个性化扰动引入磁拉普拉斯矩阵，以自适应调整边相位和方向语义。从实域角度，它采用基于路径的子图增强策略来捕获细粒度的局部非对称性和拓扑依赖。通过联合利用这两个互补的空间视角，S2-DiGCL构建高质量的正负样本，实现更通用和鲁棒的有向图对比学习。在7个真实有向图数据集上的广泛实验证明了我们方法的优越性，在监督和非监督设置下，节点分类和链路预测任务均达到了SOTA性能，分别提高了4.41%和4.34%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Contrastive Learning (GCL) has emerged as a powerful tool forextracting consistent representations from graphs, independent of labeledinformation. However, existing methods predominantly focus on undirectedgraphs, disregarding the pivotal directional information that is fundamentaland indispensable in real-world networks (e.g., social networks andrecommendations).In this paper, we introduce S2-DiGCL, a novel framework thatemphasizes spatial insights from complex and real domain perspectives fordirected graph (digraph) contrastive learning. From the complex-domainperspective, S2-DiGCL introduces personalized perturbations into the magneticLaplacian to adaptively modulate edge phases and directional semantics. Fromthe real-domain perspective, it employs a path-based subgraph augmentationstrategy to capture fine-grained local asymmetries and topologicaldependencies. By jointly leveraging these two complementary spatial views,S2-DiGCL constructs high-quality positive and negative samples, leading to moregeneral and robust digraph contrastive learning. Extensive experiments on 7real-world digraph datasets demonstrate the superiority of our approach,achieving SOTA performance with 4.41% improvement in node classification and4.34% in link prediction under both supervised and unsupervised settings.</description>
      <author>example@mail.com (Daohan Su, Yang Zhang, Xunkai Li, Rong-Hua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2510.16311v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection</title>
      <link>http://arxiv.org/abs/2510.16219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出SentinelNet，首个用于多智能体系统中主动检测和减轻恶意行为的去中心化框架，通过基于信誉的检测器和动态邻居排名实现高效防御。&lt;h4&gt;背景&lt;/h4&gt;恶意智能体对基于大型语言模型的多智能体系统的可靠性和决策能力构成重大威胁，现有防御措施因反应式设计或集中式架构存在单点故障问题而效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一个去中心化框架，能够主动检测并减轻多智能体协作中的恶意行为，提高系统安全性。&lt;h4&gt;方法&lt;/h4&gt;为每个智能体配备基于信誉的检测器，通过对比学习在增强的对抗辩论轨迹上进行训练，实现消息可信度自主评估和动态邻居排名，并通过生成对抗轨迹解决攻击数据稀缺问题。&lt;h4&gt;主要发现&lt;/h4&gt;SentinelNet在多智能体系统基准测试中实现了接近100%的恶意智能体检测率（两轮内），能从受损系统中恢复95%的准确性，并展现出跨领域和攻击模式的强泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SentinelNet为保护协作多智能体系统建立了新的防御范式，有效解决了现有防御机制的局限性。&lt;h4&gt;翻译&lt;/h4&gt;恶意智能体对由大型语言模型驱动的多智能体系统的可靠性和决策能力构成重大威胁。现有防御往往因反应式设计或集中式架构而不足，这些架构可能引入单点故障。为解决这些挑战，我们提出SentinelNet，首个用于主动检测和减轻多智能体协作中恶意行为的去中心化框架。SentinelNet为每个智能体配备基于信誉的检测器，通过在增强的对抗辩论轨迹上进行对比学习训练，使智能体能够自主评估消息可信度并通过bottom-k消除进行动态邻居排名，以抑制恶意通信。为克服攻击数据稀缺问题，它生成模拟各种威胁的对抗轨迹，确保稳健训练。在多智能体系统基准测试中，SentinelNet实现了对恶意智能体的近乎完美检测，在两轮辩论内接近100%，并从受损的基线系统中恢复95%的准确性。通过在不同领域和攻击模式中展现强大的泛化能力，SentinelNet为保护协作多智能体系统建立了新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Malicious agents pose significant threats to the reliability anddecision-making capabilities of Multi-Agent Systems (MAS) powered by LargeLanguage Models (LLMs). Existing defenses often fall short due to reactivedesigns or centralized architectures which may introduce single points offailure. To address these challenges, we propose SentinelNet, the firstdecentralized framework for proactively detecting and mitigating maliciousbehaviors in multi-agent collaboration. SentinelNet equips each agent with acredit-based detector trained via contrastive learning on augmented adversarialdebate trajectories, enabling autonomous evaluation of message credibility anddynamic neighbor ranking via bottom-k elimination to suppress maliciouscommunications. To overcome the scarcity of attack data, it generatesadversarial trajectories simulating diverse threats, ensuring robust training.Experiments on MAS benchmarks show SentinelNet achieves near-perfect detectionof malicious agents, close to 100% within two debate rounds, and recovers 95%of system accuracy from compromised baselines. By exhibiting stronggeneralizability across domains and attack patterns, SentinelNet establishes anovel paradigm for safeguarding collaborative MAS.</description>
      <author>example@mail.com (Yang Feng, Xudong Pan)</author>
      <guid isPermaLink="false">2510.16219v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</title>
      <link>http://arxiv.org/abs/2510.17790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了UltraCUA，一种基础模型，通过混合动作机制无缝集成GUI基本操作与高级程序化工具调用，解决了传统计算机使用代理(CUAs)仅依赖基本操作导致的级联故障和性能瓶颈问题。研究包含四个关键组件：自动化工具扩展管道、合成数据引擎、混合动作轨迹收集和两阶段训练流程。实验证明UltraCUA在多个基准测试上显著优于现有代理。&lt;h4&gt;背景&lt;/h4&gt;当前计算机使用多模态代理(CUAs)完全依赖基本操作(点击、输入、滚动)，这些操作需要准确的视觉定位和冗长的执行链，导致级联故障和性能瓶颈。与其他利用丰富程序化接口(API、MCP服务器、工具)的代理不同，CUAs仍然与这些能力隔离。&lt;h4&gt;目的&lt;/h4&gt;开发一种基础模型，弥合CUAs与其他代理之间的差距，通过混合动作无缝集成GUI基本操作与高级程序化工具调用，提高CUAs的性能和效率。&lt;h4&gt;方法&lt;/h4&gt;研究方法包括四个关键组件：(1)自动化管道，从软件文档、开源代码库和代码生成扩展程序化工具；(2)合成数据引擎，生成超过17,000个可验证的任务，涵盖真实世界计算机使用场景；(3)大规模高质量混合动作轨迹收集，同时包含低级GUI动作和高级程序化工具调用；(4)两阶段训练流程，结合监督微调与在线强化学习，实现低级和高级动作之间的战略性交替。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在OSWorld基准测试中，UltraCUA模型比基础模型平均实现22%的相对改进，并且在步骤上快11%；2) 在WindowsAgentArena上的跨域评估显示，模型达到21.7%的成功率，优于在Windows数据上训练的基线模型；3) 混合动作机制被证明是关键的，它减少了错误传播，同时保持了执行效率。&lt;h4&gt;结论&lt;/h4&gt;UltraCUA成功解决了传统CUAs的局限性，通过混合动作机制将GUI基本操作与高级程序化工具调用相结合，显著提高了性能和效率。这种创新方法不仅减少了错误传播，还保持了执行效率，为计算机使用代理领域带来了重大进步。&lt;h4&gt;翻译&lt;/h4&gt;多模态计算机使用代理完全依赖基本操作(点击、输入、滚动)，这些操作需要准确的视觉定位和冗长的执行链，导致级联故障和性能瓶颈。而其他代理则利用丰富的程序化接口(API、MCP服务器、工具)，计算机使用代理(CUAs)仍然与这些能力隔离。我们提出了UltraCUA，一种基础模型，通过混合动作弥合这一差距——无缝集成GUI基本操作与高级程序化工具调用。为实现这一目标，我们的方法包含四个关键组件：(1)自动化管道，从软件文档、开源代码库和代码生成扩展程序化工具；(2)合成数据引擎，生成超过17,000个可验证的任务，涵盖真实世界计算机使用场景；(3)大规模高质量混合动作轨迹收集，同时包含低级GUI动作和高级程序化工具调用；(4)两阶段训练流程，结合监督微调与在线强化学习，实现低级和高级动作之间的战略性交替。我们的7B和32B模型的实验表明，比最先进的代理有显著改进。在OSWorld上，UltraCUA模型比基础模型平均实现22%的相对改进，并且在步骤上快11%。在WindowsAgentArena上的跨域评估显示，我们的模型达到21.7%的成功率，优于在Windows数据上训练的基线模型。混合动作机制被证明是关键的，它减少了错误传播，同时保持了执行效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal agents for computer use rely exclusively on primitive actions(click, type, scroll) that require accurate visual grounding and lengthyexecution chains, leading to cascading failures and performance bottlenecks.While other agents leverage rich programmatic interfaces (APIs, MCP servers,tools), computer-use agents (CUAs) remain isolated from these capabilities. Wepresent UltraCUA, a foundation model that bridges this gap through hybridaction -- seamlessly integrating GUI primitives with high-level programmatictool calls. To achieve this, our approach comprises four key components: (1) anautomated pipeline that scales programmatic tools from software documentation,open-source repositories, and code generation; (2) a synthetic data engineproducing over 17,000 verifiable tasks spanning real-world computer-usescenarios; (3) a large-scale high-quality hybrid action trajectory collectionwith both low-level GUI actions and high-level programmatic tool calls; and (4)a two-stage training pipeline combining supervised fine-tuning with onlinereinforcement learning, enabling strategic alternation between low-level andhigh-level actions. Experiments with our 7B and 32B models demonstratesubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUAmodels achieve an average 22% relative improvement over base models, whilebeing 11% faster in terms of steps. Out-of-domain evaluation onWindowsAgentArena shows our model reaches 21.7% success rate, outperformingbaselines trained on Windows data. The hybrid action mechanism proves critical,reducing error propagation while maintaining execution efficiency.</description>
      <author>example@mail.com (Yuhao Yang, Zhen Yang, Zi-Yi Dou, Anh Nguyen, Keen You, Omar Attia, Andrew Szot, Michael Feng, Ram Ramrakhya, Alexander Toshev, Chao Huang, Yinfei Yang, Zhe Gan)</author>
      <guid isPermaLink="false">2510.17790v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Elastic ViTs from Pretrained Models without Retraining</title>
      <link>http://arxiv.org/abs/2510.17700v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出SnapViT，一种用于Vision Transformers的单次网络近似方法，通过结构化剪枝实现弹性推理，无需重新训练或标签数据，可适应各种计算预算。&lt;h4&gt;背景&lt;/h4&gt;现有视觉基础模型仅在有限的预定义尺寸中可用，导致在现实约束下无法做出最优部署选择。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的预训练后结构化剪枝方法，使模型能够在连续的计算预算范围内进行弹性推理。&lt;h4&gt;方法&lt;/h4&gt;SnapViT结合梯度信息和跨网络结构相关性，通过进化算法近似，无需标记数据，适用于无分类头的模型，且无需重新训练。&lt;h4&gt;主要发现&lt;/h4&gt;在DINO、SigLIPv2、DeIT和AugReg模型上的实验表明，该方法在各种稀疏度下优于最先进方法，在单个A100 GPU上仅需不到五分钟即可生成可调整到任何计算预算的弹性模型。&lt;h4&gt;结论&lt;/h4&gt;SnapViT贡献包括：预训练Vision Transformers的有效剪枝策略、Hessian非对角结构的新进化近似方法，以及自监督重要性评分机制，无需重新训练或标签即可保持强性能。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型取得了显著的性能，但仅在有限的预定义尺寸中可用，这迫使在现实约束下做出次优的部署选择。我们介绍了SnapViT：用于剪枝Vision Transformers的单次网络近似，这是一种新的预训练后结构化剪枝方法，能够在连续的计算预算范围内实现弹性推理。我们的方法高效地结合了梯度信息和跨网络结构相关性，通过进化算法近似，不需要标记数据，适用于没有分类头的模型，且无需重新训练。在DINO、SigLIPv2、DeIT和AugReg模型上的实验表明，在各种稀疏度下优于最先进的方法，在单个A100 GPU上需要不到五分钟生成可调整到任何计算预算的弹性模型。我们的主要贡献包括：预训练Vision Transformers的有效剪策策略，Hessian非对角结构的新进化近似方法，以及无需重新训练或标签的自监督重要性评分机制。代码和剪枝模型可在以下网址获取：https://elastic.ashita.nl/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models achieve remarkable performance but are onlyavailable in a limited set of pre-determined sizes, forcing sub-optimaldeployment choices under real-world constraints. We introduce SnapViT:Single-shot network approximation for pruned Vision Transformers, a newpost-pretraining structured pruning method that enables elastic inferenceacross a continuum of compute budgets. Our approach efficiently combinesgradient information with cross-network structure correlations, approximatedvia an evolutionary algorithm, does not require labeled data, generalizes tomodels without a classification head, and is retraining-free. Experiments onDINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance overstate-of-the-art methods across various sparsities, requiring less than fiveminutes on a single A100 GPU to generate elastic models that can be adjusted toany computational budget. Our key contributions include an efficient pruningstrategy for pretrained Vision Transformers, a novel evolutionary approximationof Hessian off-diagonal structures, and a self-supervised importance scoringmechanism that maintains strong performance without requiring retraining orlabels. Code and pruned models are available at: https://elastic.ashita.nl/</description>
      <author>example@mail.com (Walter Simoncini, Michael Dorkenwald, Tijmen Blankevoort, Cees G. M. Snoek, Yuki M. Asano)</author>
      <guid isPermaLink="false">2510.17700v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration</title>
      <link>http://arxiv.org/abs/2510.17670v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种级联方法，结合大型预训练开放词汇目标检测模型与轻量级少样本分类器，解决了遥感等专业领域中开放词汇目标检测模型的零样本性能问题，显著提高了对细粒度类别的区分能力，并大幅降低了遥感图像标注成本。&lt;h4&gt;背景&lt;/h4&gt;开放词汇目标检测(OVD)模型能够通过任意文本查询检测物体，具有显著灵活性，但在遥感等专业领域的零样本性能常受自然语言固有歧义的影响，限制了关键下游应用，例如难以区分'渔船'和'游艇'等细粒度类别。&lt;h4&gt;目的&lt;/h4&gt;解决OVD模型在专业领域如遥感中的零样本性能问题，提高模型对细粒度类别的区分能力，降低遥感图像标注的高成本，实现即时适应特定用户需求。&lt;h4&gt;方法&lt;/h4&gt;提出一种级联方法，首先使用零样本模型生成高召回率的目标提案，然后通过仅用少量用户标注示例实时训练的紧凑分类器进行高精度精炼；引入FLAME作为框架核心，这是一种一步主动学习策略，使用密度识别决策边界附近的不确定边际候选样本，并通过聚类确保样本多样性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法无需昂贵的全模型微调即可实现高精度；能在不到一分钟内实现即时适应，比最先进的替代方案快得多；在遥感基准测试中一致超越最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;建立了一个实用且资源高效的框架，使基础模型能够适应特定用户需求，为开放词汇目标检测在专业领域的应用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;开放词汇目标检测(OVD)模型能够通过任意文本查询检测物体，提供显著灵活性。然而，它们在遥感等专业领域的零样本性能常因自然语言的固有歧义而受到影响，限制了关键的下游应用。例如，OVD模型可能难以区分'渔船'和'游艇'等细粒度类别，因为它们的嵌入相似且常常不可分割。这可能会通过产生不相关的检测来阻碍特定的用户目标，如监控非法捕鱼。为解决此问题，我们提出了一种级联方法，将大型预训练OVD模型的广泛泛化能力与轻量级少样本分类器相结合。我们的方法首先使用零样本模型生成高召回率的目标提案。然后，这些提案通过仅用少量用户标注示例实时训练的紧凑分类器进行高精度精炼，大幅降低了遥感图像标注的高成本。我们框架的核心是FLAME，一种一步主动学习策略，用于选择信息量最大的样本进行训练。FLAME实时识别决策边界附近的不确定边际候选样本，然后进行聚类以确保样本多样性。这种高效的采样技术无需昂贵的全模型微调即可实现高精度，并能在不到一分钟内实现即时适应，比最先进的替代方案快得多。我们的方法在遥感基准测试中一致超越最先进的性能，为将基础模型适应特定用户需求建立了实用且资源高效的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary object detection (OVD) models offer remarkable flexibility bydetecting objects from arbitrary text queries. However, their zero-shotperformance in specialized domains like Remote Sensing (RS) is oftencompromised by the inherent ambiguity of natural language, limiting criticaldownstream applications. For instance, an OVD model may struggle to distinguishbetween fine-grained classes such as "fishing boat" and "yacht" since theirembeddings are similar and often inseparable. This can hamper specific usergoals, such as monitoring illegal fishing, by producing irrelevant detections.To address this, we propose a cascaded approach that couples the broadgeneralization of a large pre-trained OVD model with a lightweight few-shotclassifier. Our method first employs the zero-shot model to generatehigh-recall object proposals. These proposals are then refined for highprecision by a compact classifier trained in real-time on only a handful ofuser-annotated examples - drastically reducing the high costs of RS imageryannotation.The core of our framework is FLAME, a one-step active learningstrategy that selects the most informative samples for training. FLAMEidentifies, on the fly, uncertain marginal candidates near the decisionboundary using density estimation, followed by clustering to ensure samplediversity. This efficient sampling technique achieves high accuracy withoutcostly full-model fine-tuning and enables instant adaptation, within less thena minute, which is significantly faster than state-of-the-art alternatives.Ourmethod consistently surpasses state-of-the-art performance on RS benchmarks,establishing a practical and resource-efficient framework for adaptingfoundation models to specific user needs.</description>
      <author>example@mail.com (Yehonathan Refael, Amit Aides, Aviad Barzilai, George Leifman, Genady Beryozkin, Vered Silverman, Bolous Jaber, Tomer Shekel)</author>
      <guid isPermaLink="false">2510.17670v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>DELULU: Discriminative Embedding Learning Using Latent Units for Speaker-Aware Self-Supervised Speech Foundational Model</title>
      <link>http://arxiv.org/abs/2510.17662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DELULU是一个说话人感知的自监督语音基础模型，通过在外部监督集成到伪标签生成过程中，显著提升了在说话人相关任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;自监督语音模型在内容驱动任务上表现优异，但在捕捉说话人区分性特征方面有限，这对验证、说话人分割和档案应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够捕捉说话人区分性特征的自监督语音模型，解决现有模型在说话人相关任务上的局限性。&lt;h4&gt;方法&lt;/h4&gt;DELULU利用ReDimNet的帧级嵌入指导k-means聚类，引入说话人区分性归纳偏差；使用掩码预测和去噪的双重目标进行训练，增强鲁棒性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;DELULU在说话人验证任务上实现高达62%的相对EER改进，在零样本档案任务（性别、年龄、口音、说话人计数）上取得一致提升。&lt;h4&gt;结论&lt;/h4&gt;DELULU是说话人感知语音处理的强大通用编码器，无需任务特定微调即可实现卓越性能。&lt;h4&gt;翻译&lt;/h4&gt;自监督语音模型在内容驱动任务上取得了显著成功，但在捕捉对验证、说话人分割和档案应用至关重要的说话人区分性特征方面仍然有限。我们引入了DELULU，一个说话人感知的自监督基础模型，通过在外部监督集成到伪标签生成过程中来解决这一局限性。DELULU利用来自ReDimNet（最先进的说话人验证模型）的帧级嵌入来指导预训练期间的k-means聚类步骤，引入了强大的说话人区分性归纳偏差，使表示学习与说话人身份保持一致。该模型使用结合掩码预测和去噪的双重目标进行训练，进一步增强了鲁棒性和泛化能力。DELULU在一系列以说话人为中心的任务上显著优于先前的自监督学习模型，在说话人验证的等错误率上实现了高达62%的相对改进，并在零样本档案任务（如性别、年龄、口音和说话人计数）上取得了一致的提升。我们的研究结果表明，DELULU是说话人感知语音处理的强大通用编码器，即使在没有任务特定微调的情况下也能实现卓越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised speech models have achieved remarkable success oncontent-driven tasks, yet they remain limited in capturingspeaker-discriminative features critical for verification, diarization, andprofiling applications. We introduce DELULU, a speaker-aware self-supervisedfoundational model that addresses this limitation by integrating externalsupervision into the pseudo-label generation process. DELULU leveragesframe-level embeddings from ReDimNet, a state-of-the-art speaker verificationmodel, to guide the k-means clustering step during pre-training, introducing astrong speaker-discriminative inductive bias that aligns representationlearning with speaker identity. The model is trained using a dual objectivethat combines masked prediction and denoising, further enhancing robustness andgeneralization. DELULU significantly outperforms prior self-supervised learning(SSL) models across a range of speaker-centric tasks, achieving up to 62%relative improvement in equal error rate (EER) for speaker verification andconsistent gains on zero-shot profiling tasks such as gender, age, accent, andspeaker counting. Our findings demonstrate that DELULU is a strong universalencoder for speaker-aware speech processing, enabling superior performance evenwithout task-specific fine-tuning.</description>
      <author>example@mail.com (Massa Baali, Rita Singh, Bhiksha Raj)</author>
      <guid isPermaLink="false">2510.17662v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2510.17457v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accept by NeurIPS 25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GBN的局部方法，通过自适应调整基于局部结构的消息传递来解决MPNNs的过平滑和过挤压问题。&lt;h4&gt;背景&lt;/h4&gt;MPNNs是图基础模型的构建块，但存在过平滑和过挤压问题。现有解决方案主要采用全局方法，导致表达能力不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种局部方法，能够自适应地调整消息传递，同时解决过平滑和过挤压问题，提高MPNNs的表达能力。&lt;h4&gt;方法&lt;/h4&gt;作者将局部黎曼几何与MPNNs连接，建立了新的非齐次边界条件，并设计了具有局部瓶颈调整的GBN网络，基于Robin条件构建。&lt;h4&gt;主要发现&lt;/h4&gt;谱间隙的增加会导致梯度消失，削弱消息传递效果；GBN在同类同质和异类异质图上表现出强大的表达能力，且在网络深度超过256层时仍保持性能。&lt;h4&gt;结论&lt;/h4&gt;局部方法比全局方法更有效地解决MPNNs的过平滑和过挤压问题，GBN网络提供了理论保证和优异的实验性能。&lt;h4&gt;翻译&lt;/h4&gt;消息传递神经网络是图基础模型的构建块，但 fundamentally suffer from 过平滑和过挤压问题。最近有很多研究试图解决这两个问题。现有工作主要采用全局方法，在某些区域可能有益，但在其他区域可能有害，最终导致表达能力不足。本文通过全局度量谱间隙重新审视过挤压问题，并证明谱间隙的增加会导致相对于输入特征的梯度消失，从而削弱消息传递的有效性。基于这些理论见解，我们提出了一种局部方法，根据局部结构自适应调整消息传递。为此，我们将局部黎曼几何与MPNNs连接，并建立了新的非齐次边界条件来解决过挤压和过平滑问题。基于Robin条件，我们设计了具有局部瓶颈调整的GBN网络，并提供了理论保证。在同类同质和异类异质图上的广泛实验表明GBN的表达能力。此外，即使网络深度超过256层，GBN也不会表现出性能下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Message Passing Neural Networks (MPNNs) is the building block of graphfoundation models, but fundamentally suffer from oversmoothing andoversquashing. There has recently been a surge of interest in fixing bothissues. Existing efforts primarily adopt global approaches, which may bebeneficial in some regions but detrimental in others, ultimately leading to thesuboptimal expressiveness. In this paper, we begin by revisiting oversquashingthrough a global measure -- spectral gap $\lambda$ -- and prove that theincrease of $\lambda$ leads to gradient vanishing with respect to the inputfeatures, thereby undermining the effectiveness of message passing. Motivatedby such theoretical insights, we propose a \textbf{local} approach thatadaptively adjusts message passing based on local structures. To achieve this,we connect local Riemannian geometry with MPNNs, and establish a novelnonhomogeneous boundary condition to address both oversquashing andoversmoothing. Building on the Robin condition, we design a GBN network withlocal bottleneck adjustment, coupled with theoretical guarantees. Extensiveexperiments on homophilic and heterophilic graphs show the expressiveness ofGBN. Furthermore, GBN does not exhibit performance degradation even when thenetwork depth exceeds $256$ layers.</description>
      <author>example@mail.com (Li Sun, Zhenhao Huang, Ming Zhang, Philip S. Yu)</author>
      <guid isPermaLink="false">2510.17457v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</title>
      <link>http://arxiv.org/abs/2510.17439v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://falcon-vla.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FALCON的新型视觉-语言-行动模型，通过将3D空间令牌注入行动头，解决了现有VLA模型的空间推理差距问题，实现了在模拟和现实场景中的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉-语言-行动模型在3D真实世界中运行，但通常基于2D编码器构建，导致空间推理差距，限制了泛化能力和适应性。近期VLA的3D集成技术要么需要专用传感器且跨模态迁移性差，要么注入缺乏几何信息的弱提示，导致视觉-语言对齐质量下降。&lt;h4&gt;目的&lt;/h4&gt;解决现有VLA模型在空间表示、模态迁移性和对齐方面的局限性，提升模型在复杂环境中的表现和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;引入FALCON（From Spatial to Action）范式，将丰富的3D空间令牌注入到行动头中；利用空间基础模型仅从RGB提供强大的几何先验；包含一个可选择性融合深度或姿态的具身空间模型，无需重新训练或架构改变；为保留语言推理能力，空间令牌被空间增强行动头处理，而非连接到视觉-语言主干。&lt;h4&gt;主要发现&lt;/h4&gt;FALCON在三个模拟基准测试和十一个现实世界任务的综合评估中实现了最先进性能，一致超越竞争基线，并且在杂乱环境、空间提示条件和物体高度变化等情况下保持鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;FALCON通过创新的空间令牌注入机制和灵活的多模态融合能力，有效解决了VLA模型在空间表示、模态迁移性和对齐方面的局限性，为3D环境中的智能行动提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;现有的视觉-语言-行动模型在3D真实世界中运行，但通常基于2D编码器构建，留下了限制泛化和适应性的空间推理差距。近期VLA的3D集成技术要么需要专用传感器且跨模态迁移性差，要么注入缺乏几何信息的弱提示，导致视觉-语言对齐质量下降。在这项工作中，我们引入FALCON（From Spatial to Action），一种将丰富的3D空间令牌注入行动头的新颖范式。FALCON利用空间基础模型仅从RGB提供强大的几何先验，并包含一个可选择性融合深度或姿态的具身空间模型，当可用时提供更高保真度，无需重新训练或架构改变。为保留语言推理能力，空间令牌被空间增强行动头消耗，而非连接到视觉-语言主干。这些设计使FALCON能够解决空间表示、模态迁移性和对齐方面的局限性。在三个模拟基准测试和十一个现实世界任务的综合评估中，我们提出的FALCON实现了最先进性能，一致超越竞争基线，并且在杂乱环境、空间提示条件和物体高度变化等情况下保持鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有视觉-语言-动作（VLA）模型的空间推理差距问题。这些模型虽然能在3D真实世界运行，但通常基于2D编码器构建，导致缺乏可靠的3D空间理解，限制了机器人在新场景、背景变化或物体变化时的泛化能力和适应性。这个问题非常重要，因为机器人需要与3D物理世界交互，而缺乏明确的3D意识使它们难以处理需要几何推理、深度感知或空间关系理解的任务，这已成为开发可靠通用机器人政策的主要瓶颈。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先深入分析了现有VLA模型的局限性，注意到它们基于2D编码器但需要在3D世界运行，存在空间推理差距。作者借鉴了空间基础模型（如VGGT、DUSt3R）的思路，这些模型能将场景编码为令牌序列进行3D重建；同时受到大脑分工的启发，将VLM比作处理高级推理的大脑，动作头比作管理精细运动的小脑。作者还参考了现有VLA架构（如RT-2、OpenVLA），但改进了空间信息集成方式，并利用了现有的深度估计和相机姿态编码技术。最终设计了具身空间模型(ESM)和空间增强动作头，实现了空间与语义的有效融合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过将丰富的3D空间令牌注入到VLA模型的动作头中来增强空间理解能力，同时保持语言推理能力。整体流程包括：1)双路径处理 - VLM路径处理视觉和语言输入提取语义表示，ESM路径处理图像和可选几何输入提取空间令牌；2)ESM通过令牌化、空间编码和可选的深度/姿态注入来生成空间令牌；3)通过最大池化和MLP适配器将空间特征投影到VLM特征空间；4)使用元素级加法融合空间特征与语义动作令牌；5)融合后的特征输入动作预测器（MLP或LSTM）生成机器人动作序列；6)采用两阶段后训练方法确保训练稳定性和特征对齐。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)空间令牌注入新范式 - 将3D空间令牌注入动作头而非连接文本令牌；2)具身空间模型(ESM) - 可选择性整合深度和姿态等3D模态；3)空间增强动作头 - 直接将空间令牌整合到动作决策中；4)随机条件策略 - 确保模型在不同输入条件下都能有效工作。相比之前工作，FALCON不依赖特定3D传感器（区别于PointVLA、GeoVLA），不会破坏预训练的视觉-语言对齐（区别于3D-VLA、SpatialVLA），提供了显式3D理解（区别于传统2D VLA），并采用高效的两阶段训练方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FALCON通过将空间基础模型提供的丰富3D空间令牌注入到专门设计的空间增强动作头中，解决了现有视觉-语言-动作模型在3D空间理解上的局限，实现了强大的模态转移能力和在复杂空间任务中的最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing vision-language-action (VLA) models act in 3D real-world but aretypically built on 2D encoders, leaving a spatial reasoning gap that limitsgeneralization and adaptability. Recent 3D integration techniques for VLAseither require specialized sensors and transfer poorly across modalities, orinject weak cues that lack geometry and degrade vision-language alignment. Inthis work, we introduce FALCON (From Spatial to Action), a novel paradigm thatinjects rich 3D spatial tokens into the action head. FALCON leverages spatialfoundation models to deliver strong geometric priors from RGB alone, andincludes an Embodied Spatial Model that can optionally fuse depth, or pose forhigher fidelity when available, without retraining or architectural changes. Topreserve language reasoning, spatial tokens are consumed by a Spatial-EnhancedAction Head rather than being concatenated into the vision-language backbone.These designs enable FALCON to address limitations in spatial representation,modality transferability, and alignment. In comprehensive evaluations acrossthree simulation benchmarks and eleven real-world tasks, our proposed FALCONachieves state-of-the-art performance, consistently surpasses competitivebaselines, and remains robust under clutter, spatial-prompt conditioning, andvariations in object scale and height.</description>
      <author>example@mail.com (Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou)</author>
      <guid isPermaLink="false">2510.17439v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion Models as Dataset Distillation Priors</title>
      <link>http://arxiv.org/abs/2510.17421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Diffusion As Priors (DAP)方法，通过利用扩散模型中的代表性先验，解决了数据集蒸馏中同时实现多样性、泛化能力和代表性的挑战。DAP在特征空间中使用Mercer核量化合成数据与真实数据的相似性，并将此先验作为指导引导反向扩散过程，无需重新训练即可提高蒸馏数据集质量。实验证明，DAP在ImageNet-1K等大型数据集上优于现有方法，实现了更好的跨架构泛化能力。&lt;h4&gt;背景&lt;/h4&gt;数据集蒸馏旨在从大型数据集中合成紧凑而信息丰富的数据集。该领域的一个重大挑战是在单个蒸馏数据集中同时实现多样性、泛化能力和代表性。虽然最近的生成式数据集蒸馏方法采用了强大的扩散模型作为基础模型，但这些方法忽略了扩散模型中固有的代表性先验，因此需要集成外部约束来提高数据质量。&lt;h4&gt;目的&lt;/h4&gt;解决数据集蒸馏中同时实现多样性、泛化能力和代表性的挑战，通过利用扩散模型中固有的代表性先验，提出一种无需重新训练即可提高蒸馏数据集质量的方法。&lt;h4&gt;方法&lt;/h4&gt;作者提出了Diffusion As Priors (DAP)方法，该方法通过以下步骤实现：在特征空间中使用Mercer核量化合成数据与真实数据之间的相似性，将代表性形式化；将此先验作为指导来引导反向扩散过程；增强蒸馏样本的代表性，无需任何重新训练。&lt;h4&gt;主要发现&lt;/h4&gt;在ImageNet-1K及其子集等大规模数据集上的大量实验表明，DAP在生成高保真度数据集方面优于最先进的方法；DAP实现了更好的跨架构泛化能力；该研究在扩散先验与数据集蒸馏目标之间建立了理论联系。&lt;h4&gt;结论&lt;/h4&gt;Diffusion As Priors (DAP)方法为提高数据集蒸馏质量提供了一个实用的、无需训练的框架。该研究不仅在扩散先验与数据集蒸馏目标之间建立了理论联系，还为解决数据集蒸馏中的代表性挑战提供了有效的方法，同时实现了多样性和泛化能力的平衡。&lt;h4&gt;翻译&lt;/h4&gt;数据集蒸馏旨在从大型数据集中合成紧凑而信息丰富的数据集。该领域的一个重大挑战是在单个蒸馏数据集中同时实现多样性、泛化能力和代表性。虽然最近的生成式数据集蒸馏方法采用了强大的扩散模型作为基础模型，但这些方法忽略了扩散模型中固有的代表性先验，因此需要集成外部约束来提高数据质量。为此，我们提出了Diffusion As Priors (DAP)，该方法通过在特征空间中使用Mercer核量化合成数据与真实数据之间的相似性，将代表性形式化。然后，我们将此先验作为指导来引导反向扩散过程，无需任何重新训练即可增强蒸馏样本的代表性。在ImageNet-1K及其子集等大规模数据集上的大量实验表明，DAP在生成高保真度数据集方面优于最先进的方法，同时实现了更好的跨架构泛化能力。我们的研究不仅在扩散先验与数据集蒸馏目标之间建立了理论联系，还为提高蒸馏数据集质量提供了一个实用的、无需训练的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dataset distillation aims to synthesize compact yet informative datasets fromlarge ones. A significant challenge in this field is achieving a trifecta ofdiversity, generalization, and representativeness in a single distilleddataset. Although recent generative dataset distillation methods adopt powerfuldiffusion models as their foundation models, the inherent representativenessprior in diffusion models is overlooked. Consequently, these approaches oftennecessitate the integration of external constraints to enhance data quality. Toaddress this, we propose Diffusion As Priors (DAP), which formalizesrepresentativeness by quantifying the similarity between synthetic and realdata in feature space using a Mercer kernel. We then introduce this prior asguidance to steer the reverse diffusion process, enhancing therepresentativeness of distilled samples without any retraining. Extensiveexperiments on large-scale datasets, such as ImageNet-1K and its subsets,demonstrate that DAP outperforms state-of-the-art methods in generatinghigh-fidelity datasets while achieving superior cross-architecturegeneralization. Our work not only establishes a theoretical connection betweendiffusion priors and the objectives of dataset distillation but also provides apractical, training-free framework for improving the quality of the distilleddataset.</description>
      <author>example@mail.com (Duo Su, Huyu Wu, Huanran Chen, Yiming Shi, Yuzhu Wang, Xi Ye, Jun Zhu)</author>
      <guid isPermaLink="false">2510.17421v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Monitoring Horses in Stalls: From Object to Event Detection</title>
      <link>http://arxiv.org/abs/2510.17409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于视觉的监控系统，可自动化检测和跟踪马厩中的马匹和人，用于早期发现健康和福利问题，减少人工监控的劳动强度。&lt;h4&gt;背景&lt;/h4&gt;监控拴马的行为对于早期发现健康和福利问题至关重要，但目前的监控方法仍然劳动密集且耗时。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于视觉的原型监控系统，自动化检测和跟踪马厩内的马匹和人，实现实时行为监控。&lt;h4&gt;方法&lt;/h4&gt;使用目标检测和多目标跟踪技术，系统利用YOLOv11和BoT-SORT进行检测和跟踪，基于物体轨迹和空间关系推断事件状态，构建了使用CLIP和GroundingDINO标注的自定义数据集，系统能区分五种事件类型并考虑相机盲点。&lt;h4&gt;主要发现&lt;/h4&gt;定性评估表明系统在马匹相关事件检测方面表现可靠，但由于数据稀缺，在检测人方面存在局限性。&lt;h4&gt;结论&lt;/h4&gt;这项工作为马匹设施的实时行为监控提供了基础，对动物福利和马厩管理有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;监控拴马的行为对于早期发现健康和福利问题至关重要，但仍然劳动密集且耗时。在本研究中，我们提出了一个基于视觉的原型监控系统，使用目标检测和多目标跟踪技术自动化检测和跟踪马厩内的马匹和人。系统利用YOLOv11和BoT-SORT进行检测和跟踪，同时基于马厩内物体的轨迹和空间关系推断事件状态。为支持开发，我们构建了一个使用基础模型CLIP和GroundingDINO协助标注的自定义数据集。系统能区分五种事件类型并考虑相机的盲点。定性评估表明系统在马匹相关事件检测方面表现可靠，同时指出由于数据稀缺，在检测人方面存在局限性。这项工作为马匹设施的实时行为监控提供了基础，对动物福利和马厩管理有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monitoring the behavior of stalled horses is essential for early detection ofhealth and welfare issues but remains labor-intensive and time-consuming. Inthis study, we present a prototype vision-based monitoring system thatautomates the detection and tracking of horses and people inside stables usingobject detection and multi-object tracking techniques. The system leveragesYOLOv11 and BoT-SORT for detection and tracking, while event states areinferred based on object trajectories and spatial relations within the stall.To support development, we constructed a custom dataset annotated withassistance from foundation models CLIP and GroundingDINO. The systemdistinguishes between five event types and accounts for the camera's blindspots. Qualitative evaluation demonstrated reliable performance forhorse-related events, while highlighting limitations in detecting people due todata scarcity. This work provides a foundation for real-time behavioralmonitoring in equine facilities, with implications for animal welfare andstable management.</description>
      <author>example@mail.com (Dmitrii Galimzianov, Viacheslav Vyshegorodtsev, Ivan Nezhivykh)</author>
      <guid isPermaLink="false">2510.17409v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine</title>
      <link>http://arxiv.org/abs/2510.17402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了Ladder-base，首个使用组相对策略优化(GRPO)训练的中医领域大语言模型，在多个推理指标上表现优于通用大语言模型和特定中医模型。&lt;h4&gt;背景&lt;/h4&gt;中医拥有丰富且结构独特的知识体系，这对常规大语言模型的应用提出了挑战。虽然之前的中医特定LLM通过监督微调取得进展，但它们在对齐、数据质量和评估一致性方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一个针对中医领域的大语言模型，解决现有模型在一致性、数据质量和评估一致性方面的局限性，提高模型在中医领域的推理能力和事实一致性。&lt;h4&gt;方法&lt;/h4&gt;使用组相对策略优化(GRPO)强化学习方法训练Ladder-base模型，该方法通过基于组内比较优化响应选择来提高推理和事实一致性。模型基于Qwen2.5-7B-Instruct构建，在中医阶梯基准的文本子集上训练，使用80%数据训练，剩余20%平均分为验证和测试集。&lt;h4&gt;主要发现&lt;/h4&gt;通过标准化评估，Ladder-base在多个推理指标上表现优于GPT-4、Gemini 2.5、Claude 3、Qwen3等通用大语言模型，以及BenTsao、HuatuoGPT2、Zhongjing等特定中医模型。&lt;h4&gt;结论&lt;/h4&gt;GRPO为将大语言模型与中医领域专家级推理对齐提供了有效且高效的策略，支持开发可信且临床基础的中医人工智能系统。&lt;h4&gt;翻译&lt;/h4&gt;传统中医呈现了一个丰富且结构独特的知识体系，这对常规大语言模型的应用提出了挑战。尽管之前的中医特定LLM通过监督微调已经显示出进展，但它们常常在对齐、数据质量和评估一致性方面面临局限性。在本研究中，我们引入了Ladder-base，这是第一个使用组相对策略优化训练的中医领域LLM，这是一种通过基于组内比较优化响应选择来提高推理和事实一致性的强化学习方法。Ladder-base基于Qwen2.5-7B-Instruct基础模型构建，并仅在中医阶梯基准的文本子集上进行训练，使用80%的数据进行训练，剩余的20%平均分为验证集和测试集。通过标准化评估，Ladder-base在多个推理指标上表现出优于最先进的通用LLM和特定中医模型的性能。这些发现表明，GRPO为将LLM与中医领域专家级推理对齐提供了一种有效且高效的策略，支持开发可信且临床基础的中医人工智能系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional Chinese Medicine (TCM) presents a rich and structurally uniqueknowledge system that challenges conventional applications of large languagemodels (LLMs). Although previous TCM-specific LLMs have shown progress throughsupervised fine-tuning, they often face limitations in alignment, data quality,and evaluation consistency. In this study, we introduce Ladder-base, the firstTCM-focused LLM trained with Group Relative Policy Optimization (GRPO), areinforcement learning method that improves reasoning and factual consistencyby optimizing response selection based on intra-group comparisons. Ladder-baseis built upon the Qwen2.5-7B-Instruct foundation model and trained exclusivelyon the textual subset of the TCM-Ladder benchmark, using 80 percent of the datafor training and the remaining 20 percent split evenly between validation andtest sets. Through standardized evaluation, Ladder-base demonstrates superiorperformance across multiple reasoning metrics when compared to bothstate-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, andQwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, andZhongjing. These findings suggest that GRPO provides an effective and efficientstrategy for aligning LLMs with expert-level reasoning in traditional medicaldomains and supports the development of trustworthy and clinically grounded TCMartificial intelligence systems.</description>
      <author>example@mail.com (Jiacheng Xie, Shuai Zeng, Yang Yu, Xiaoting Tang, Guanghui An, Dong Xu)</author>
      <guid isPermaLink="false">2510.17402v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Exploring The Missing Semantics In Event Modality</title>
      <link>http://arxiv.org/abs/2510.17347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Semantic-E2VID框架，通过探索事件模态中缺失的视觉语义知识并利用其增强事件到视频重建，解决了事件相机无法捕捉静态物体和背景导致的语义信息缺失问题。&lt;h4&gt;背景&lt;/h4&gt;事件相机具有低延迟、高动态范围和高效运动捕捉等优势，但事件到视频重建任务面临重建和恢复语义信息的挑战。事件相机只捕捉强度变化，忽略静态物体和背景，导致捕获的事件模态中缺乏语义信息。&lt;h4&gt;目的&lt;/h4&gt;提出Semantic-E2VID框架，探索事件模态中缺失的视觉语义知识并利用其增强事件到视频重建。&lt;h4&gt;方法&lt;/h4&gt;引入跨模态特征对齐(CFA)模块将SAM模型的鲁棒视觉语义传输到事件编码器；提出语义感知特征融合(SFF)块整合学习到的语义信息；提出语义感知E2V监督利用SAM生成的类别标签帮助重建语义细节。&lt;h4&gt;主要发现&lt;/h4&gt;Semantic-E2VID显著提高了帧质量，在多个基准测试中优于最先进的E2V方法。&lt;h4&gt;结论&lt;/h4&gt;Semantic-E2VID有效解决了事件模态中语义信息缺失的问题，通过跨模态特征对齐和语义感知特征融合提升了事件到视频重建的质量。&lt;h4&gt;翻译&lt;/h4&gt;事件相机提供低延迟、高动态范围和高效运动捕捉等独特优势。然而，作为基础事件视觉任务的事件到视频重建(E2V)仍然具有挑战性，特别是在重建和恢复语义信息方面。这主要源于事件相机的本质，因为它只捕捉强度变化，忽略静态物体和背景，导致捕获的事件模态中缺乏语义信息。此外，语义信息在视频和帧重建中起着关键作用，但现有的E2V方法常常忽略了这一点。为了弥合这一差距，我们提出了Semantic-E2VID，这是一个E2V框架，探索事件模态中缺失的视觉语义知识，并利用它来增强事件到视频的重建。具体来说，Semantic-E2VID引入了跨模态特征对齐(CFA)模块，将基于帧的视觉基础模型(Segment Anything Model, SAM)的鲁棒视觉语义传输到事件编码器，同时对齐来自不同模态的高级特征。为了更好地利用学习到的语义特征，我们进一步提出了一个语义感知特征融合(SFF)块，将学习到的帧模态语义整合到事件表示中，形成具有丰富语义的事件表示，可被事件解码器解码。此外，为了促进语义信息的重建，我们提出了一种新颖的语义感知E2V监督，它通过利用SAM生成的类别标签帮助模型重建语义细节。大量实验证明，Semantic-E2VID显著提高了帧质量，在多个基准测试中优于最先进的E2V方法。示例代码包含在补充材料中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras offer distinct advantages such as low latency, high dynamicrange, and efficient motion capture. However, event-to-video reconstruction(E2V), a fundamental event-based vision task, remains challenging, particularlyfor reconstructing and recovering semantic information. This is primarily dueto the nature of the event camera, as it only captures intensity changes,ignoring static objects and backgrounds, resulting in a lack of semanticinformation in captured event modality. Further, semantic information plays acrucial role in video and frame reconstruction, yet is often overlooked byexisting E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2Vframework that explores the missing visual semantic knowledge in event modalityand leverages it to enhance event-to-video reconstruction. Specifically,Semantic-E2VID introduces a cross-modal feature alignment (CFA) module totransfer the robust visual semantics from a frame-based vision foundationmodel, the Segment Anything Model (SAM), to the event encoder, while aligningthe high-level features from distinct modalities. To better utilize the learnedsemantic feature, we further propose a semantic-aware feature fusion (SFF)block to integrate learned semantics in frame modality to form eventrepresentations with rich semantics that can be decoded by the event decoder.Further, to facilitate the reconstruction of semantic information, we propose anovel Semantic Perceptual E2V Supervision that helps the model to reconstructsemantic details by leveraging SAM-generated categorical labels. Extensiveexperiments demonstrate that Semantic-E2VID significantly enhances framequality, outperforming state-of-the-art E2V methods across multiple benchmarks.The sample code is included in the supplementary material.</description>
      <author>example@mail.com (Jingqian Wu, Shengpeng Xu, Yunbo Jia, Edmund Y. Lam)</author>
      <guid isPermaLink="false">2510.17347v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Trading with the Devil: Risk and Return in Foundation Model Strategies</title>
      <link>http://arxiv.org/abs/2510.17165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种扩展的资本资产定价模型(CAPM)，用于分离基础模型引入的系统风险和特定微调带来的特定风险，帮助金融从业者更好地理解和评估基于基础模型的交易策略风险状况。&lt;h4&gt;背景&lt;/h4&gt;基础模型已在自然语言处理等领域产生变革性影响，现开始应用于金融时间序列任务。这些预训练架构虽能提供多样化预测信号，但如何影响交易策略风险状况尚不明确，导致实践者不愿投入大量资本。&lt;h4&gt;目的&lt;/h4&gt;扩展资本资产定价模型，分离基础模型引入的系统风险（可能产生alpha）与特定微调带来的特定风险（通常不积累系统性溢价），并开发实用方法估计这些风险。&lt;h4&gt;方法&lt;/h4&gt;将风险分解与不确定性解耦概念对齐，将系统性风险视为本体不确定性，特定风险视为偶然不确定性。在偶然崩溃假设下，使用蒙特卡洛dropout等方法直接测量本体风险，将交易策略映射到更透明的风险-回报平面。&lt;h4&gt;主要发现&lt;/h4&gt;分离不同风险因素可更深入了解基于基础模型策略的性能限制、模型随时间的退化情况，以及有针对性的改进途径。&lt;h4&gt;结论&lt;/h4&gt;研究结果突出了在竞争性金融市场部署大型预训练模型的希望和陷阱，为金融从业者提供了更全面的风险评估框架。&lt;h4&gt;翻译&lt;/h4&gt;基础模型-已在自然语言处理等领域产生变革性影响-现正开始出现在金融时间序列任务中。虽然这些预训练架构承诺提供多样化的预测信号，但人们对其如何塑造构建于其上的交易策略的风险状况知之甚少，导致实践者不愿投入大量资本。在本文中，我们提出对资本资产定价模型(CAPM)的扩展，该模型分离了共享基础模型引入的系统风险-如果底层模型真正具有预测能力，则可能产生alpha-以及归因于自定义微调的特定风险，后者通常不积累系统性溢价。为了能够实际估计这些独立风险，我们将这种分解与不确定性解耦的概念对齐，将系统性风险视为本体不确定性（源于预训练模型），将特定风险视为偶然不确定性（在自定义适应过程中引入）。在偶然崩溃假设下，我们说明了如何使用蒙特卡洛dropout-以及其他不确定性量化工具包中的方法-直接测量本体风险，从而将交易策略映射到更透明的风险-回报平面。我们的实验表明，分离这些不同的风险因素可以更深入地了解基于基础模型的策略的性能限制、其随时间的模型退化情况，以及有针对性的改进途径。总的来说，我们的结果突出了在竞争性金融市场部署大型预训练模型的希望和陷阱。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models - already transformative in domains such as naturallanguage processing - are now starting to emerge for time-series tasks infinance. While these pretrained architectures promise versatile predictivesignals, little is known about how they shape the risk profiles of the tradingstrategies built atop them, leaving practitioners reluctant to commit seriouscapital. In this paper, we propose an extension to the Capital Asset PricingModel (CAPM) that disentangles the systematic risk introduced by a sharedfoundation model - potentially capable of generating alpha if the underlyingmodel is genuinely predictive - from the idiosyncratic risk attributable tocustom fine-tuning, which typically accrues no systematic premium. To enable apractical estimation of these separate risks, we align this decomposition withthe concepts of uncertainty disentanglement, casting systematic risk asepistemic uncertainty (rooted in the pretrained model) and idiosyncratic riskas aleatory uncertainty (introduced during custom adaptations). Under theAleatory Collapse Assumption, we illustrate how Monte Carlo dropout - amongother methods in the uncertainty-quantization toolkit - can directly measurethe epistemic risk, thereby mapping trading strategies to a more transparentrisk-return plane. Our experiments show that isolating these distinct riskfactors yields deeper insights into the performance limits offoundation-model-based strategies, their model degradation over time, andpotential avenues for targeted refinements. Taken together, our resultshighlight both the promise and the pitfalls of deploying large pretrainedmodels in competitive financial markets.</description>
      <author>example@mail.com (Jinrui Zhang)</author>
      <guid isPermaLink="false">2510.17165v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework</title>
      <link>http://arxiv.org/abs/2510.17163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型基础模型正在改变软件工程领域，但缺乏全面的可信度评估方法。研究团队提出了TREAT评估框架，通过多任务、多语言多模态、鲁棒性和严格评估方法四个改进点，对26个先进模型进行了评估，发现了模型在编程任务上的性能差异和多模态模型在UI代码生成方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型正在从根本上改变软件工程领域，在代码生成、调试和测试等任务上表现出色。然而，如何全面评估这些模型在真实软件工程场景中的可信度仍存在显著差距。现有基准测试存在任务范围有限、未包含模型鲁棒性和可靠性等关键评估方面的问题。&lt;h4&gt;目的&lt;/h4&gt;填补现有评估方法的不足，提供一个全面的模型性能评估框架，以评估大型基础模型在软件工程任务中的可信度和可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出名为TREAT（Code LLMs Trustworthiness/Reliability Evaluation And Testing）的评估框架，包含四个主要改进：1) 多任务全面评估，涵盖多样化的软件工程活动；2) 多语言和多模态评估，包含多模态编码任务；3) 鲁棒性评估，评估模型在语义保持代码转换下的可靠性；4) 严格的评估方法，通过多样化的评估提示和自适应解决方案提取提高评估结果的可信度。基于此框架评估了26个最先进的模型。&lt;h4&gt;主要发现&lt;/h4&gt;1) 当前模型在编程任务上表现出显著的性能差异；2) 多模态语言模型在UI代码生成和编辑方面表现出特定的性能局限性。&lt;h4&gt;结论&lt;/h4&gt;TREAT框架为评估大型基础模型在软件工程任务中的可信度和可靠性提供了更全面的方法，有助于识别模型的优势和局限性，指导未来的模型改进方向。&lt;h4&gt;翻译&lt;/h4&gt;大型基础模型正在从根本上改变软件工程领域，在代码生成、调试和测试等多样化任务上表现出色。尽管进展迅速，但在如何全面评估这些模型在真实软件工程场景中的可信度方面仍存在显著差距。现有基准测试存在任务范围有限，未能纳入模型的鲁棒性和可靠性等关键评估方面的问题。为填补这一差距，我们提出了一个名为TREAT（Code LLMs Trustworthiness/Reliability Evaluation And Testing）的评估框架，该框架提供对模型在代码智能任务中性能的全面评估。我们的评估框架通过四个主要改进解决了现有方法的关键局限性：(1) 多任务全面评估，涵盖多样化的软件工程活动，而非有限的编码任务；(2) 多语言和多模态评估，超越传统的单语言、纯文本基准，包含多模态编码任务；(3) 鲁棒性评估，评估模型在语义保持代码转换下的可靠性；(4) 严格的评估方法，通过多样化的评估提示和自适应解决方案提取提高评估结果的可信度。基于此评估框架，我们评估了26个最先进的模型，发现了它们的优势和局限性，得出了几个关键见解：(1) 当前模型在编程任务上表现出显著的性能差异；(2) 多模态语言模型在UI代码生成和编辑方面表现出特定的性能局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large foundation models are fundamentally transforming the softwareengineering landscape, demonstrating exceptional capabilities across diversetasks such as code generation, debugging, and testing. Despite this rapidprogress, a significant gap remains in how to comprehensively evaluate thesemodels' trustworthiness in real-world software engineering scenarios. Existingbenchmarks suffer from limited task scope and fail to incorporate criticalevaluation aspects such as the robustness and reliability of models. To bridgethis gap, we present an evaluation framework called TREAT (Code LLMsTrustworthiness / Reliability Evaluation And Testing) that provides a holisticassessment of model performance in code intelligence tasks. Our evaluationframework addresses key limitations in existing approaches with four mainimprovements: (1) Multi-Task Holistic Evaluation that spans diverse softwareengineering activities rather than limited coding tasks; (2) Multi-Language andMulti-Modality Assessment that extends beyond traditional single-language,text-only benchmarks to include multi-modality coding tasks; (3) RobustnessAssessment that evaluates model reliability under semantically-preserving codetransformations; and (4) Rigorous Evaluation Methodology that enhances thetrustworthiness of evaluation results through diverse evaluation prompts andadaptive solution extraction. Based on this evaluation framework, we assess 26state-of-the-art models and uncover both their strengths and limitations,yielding several key insights:(1) Current models show substantial performancevariation across programming tasks; (2) Multi-modal language models demonstratespecific performance limitations in UI code generation and edit;</description>
      <author>example@mail.com (Shuzheng Gao, Eric John Li, Man Ho Lam, Jingyu Xiao, Yuxuan Wan, Chaozheng Wang, Ng Man Tik, Michael R. Lyu)</author>
      <guid isPermaLink="false">2510.17163v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Do Satellite Tasks Need Special Pretraining?</title>
      <link>http://arxiv.org/abs/2510.17014v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究挑战了特定遥感基础模型比通用视觉基础模型更有用的观点，特别是在小规模应用中。作者设计了一个评估模型对低分辨率图像泛化能力的基准，并在卫星图像数据集上训练了iBOT模型，但发现没有预训练模型能比通用基线带来一致改进。&lt;h4&gt;背景&lt;/h4&gt;基础模型已在多种模态中推动了机器学习的发展，最近多个团队训练了专门用于遥感应用的基础模型。这一研究方向受到遥感图像的独特特性、特定应用以及对卫星图像分析有用的鲁棒性类型的驱动。&lt;h4&gt;目的&lt;/h4&gt;系统性地挑战特定基础模型比通用视觉基础模型更有用的观点，至少在小规模情况下。&lt;h4&gt;方法&lt;/h4&gt;设计了一个简单的基准来衡量遥感模型对较低分辨率图像的泛化能力；在MillionAID（ImageNet规模的卫星图像数据集）上训练了iBOT（自监督视觉编码器），并进行了针对遥感的若干修改。&lt;h4&gt;主要发现&lt;/h4&gt;在ViT-B规模下，没有一个预训练模型能比通用基线带来一致的改进。&lt;h4&gt;结论&lt;/h4&gt;特定基础模型在小规模应用中并不比通用视觉基础模型更有优势。&lt;h4&gt;翻译&lt;/h4&gt;基础模型已在各种模态中推动了机器学习的发展，包括图像。最近，多个团队训练了专门用于遥感应用的基础模型。这一研究方向受到遥感图像的独特特性、特定应用以及对卫星图像分析有用的鲁棒性类型的驱动。在这项工作中，我们系统地挑战了特定基础模型比通用视觉基础模型更有用的观点，至少在小规模情况下。首先，我们设计了一个简单的基准，用于衡量遥感模型在两个下游任务中对较低分辨率图像的泛化能力。其次，我们在MillionAID（一个ImageNet规模的卫星图像数据集）上训练了iBOT（一种自监督视觉编码器），并进行了针对遥感的若干修改。我们表明，在ViT-B规模下，这些预训练模型中没有哪一个比通用基线带来一致的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have advanced machine learning across various modalities,including images. Recently multiple teams trained foundation models specializedfor remote sensing applications. This line of research is motivated by thedistinct characteristics of remote sensing imagery, specific applications andtypes of robustness useful for satellite image analysis. In this work wesystematically challenge the idea that specific foundation models are moreuseful than general-purpose vision foundation models, at least in the smallscale. First, we design a simple benchmark that measures generalization ofremote sensing models towards images with lower resolution for two downstreamtasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,an ImageNet-scale satellite imagery dataset, with several modificationsspecific to remote sensing. We show that none of those pretrained models bringconsistent improvements upon general-purpose baselines at the ViT-B scale.</description>
      <author>example@mail.com (Ani Vanyan, Alvard Barseghyan, Hakob Tamazyan, Tigran Galstyan, Vahan Huroyan, Naira Hovakimyan, Hrant Khachatrian)</author>
      <guid isPermaLink="false">2510.17014v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Graph4MM: Weaving Multimodal Learning with Structural Information</title>
      <link>http://arxiv.org/abs/2510.16990v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Graph4MM，一个基于图的多模态学习框架，通过Hop-Diffused Attention和MM-QFormer解决了多模态学习中的两个关键挑战：整合多跳邻居结构信息和融合模态特定信息。实验表明该方法显著优于现有模型。&lt;h4&gt;背景&lt;/h4&gt;现实世界多模态数据具有复杂结构关系，跨模态实体通过上下文依赖和共指关系形成多样连接。图为建模模态内和模态间关系提供强大结构信息，但先前工作未能区分多跳邻居并将图视为独立模态，导致理解碎片化。&lt;h4&gt;目的&lt;/h4&gt;解决多模态学习中的两个关键挑战：(1)将多跳邻居的结构信息整合到基础模型中，(2)以原则性的方式融合模态特定信息。重新审视图在基础模型时代多模态学习中的作用。&lt;h4&gt;方法&lt;/h4&gt;提出Graph4MM框架，包含Hop-Diffused Attention（通过因果掩蔽和跳扩散将多跳结构信息整合到自注意力中）和MM-QFormer（用于跨模态融合的多映射查询transformer）。&lt;h4&gt;主要发现&lt;/h4&gt;利用结构整合模态内和模态间交互比将它们视为独立模态能更好地提升多模态理解。在生成性和判别性任务上，Graph4MM优于更大的VLMs、LLMs和多模态图基线，平均实现6.93%的改进。&lt;h4&gt;结论&lt;/h4&gt;Graph4MM框架有效解决了多模态学习中的关键挑战，通过整合多跳结构信息和跨模态融合，显著提升了多模态理解能力。&lt;h4&gt;翻译&lt;/h4&gt;现实世界多模态数据通常表现出超越传统图像-标题对等一对一映射的复杂结构关系。跨模态的实体以复杂的方式交互，图像和文本通过上下文依赖和共指关系形成多样的相互连接。图为建模模态内和模态间关系提供了强大的结构信息。然而，先前的工作未能区分多跳邻居，而是将图视为独立模态，这碎片化了整体理解。这一局限性给多模态学习带来了两个关键挑战：(1)将多跳邻居的结构信息整合到基础模型中，(2)以原则性的方式融合模态特定信息。为应对这些挑战，我们重新审视了基础模型时代图在多模态学习中的作用，并提出了Graph4MM，一个基于图的多模态学习框架。具体而言，我们引入了Hop-Diffused Attention，通过因果掩蔽和跳扩散将多跳结构信息整合到自注意力中。此外，我们设计了MM-QFormer，一个用于跨模态融合的多映射查询transformer。通过理论和经验分析，我们表明利用结构整合模态内和模态间交互，比将它们视为独立模态能更好地提升多模态理解。在生成性和判别性任务上的实验表明，Graph4MM优于更大的VLMs、LLMs和多模态图基线，实现了6.93%的平均改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world multimodal data usually exhibit complex structural relationshipsbeyond traditional one-to-one mappings like image-caption pairs. Entitiesacross modalities interact in intricate ways, with images and text formingdiverse interconnections through contextual dependencies and co-references.Graphs provide powerful structural information for modeling intra-modal andinter-modal relationships. However, previous works fail to distinguishmulti-hop neighbors and treat the graph as a standalone modality, whichfragments the overall understanding. This limitation presents two keychallenges in multimodal learning: (1) integrating structural information frommulti-hop neighbors into foundational models, and (2) fusing modality-specificinformation in a principled manner. To address these challenges, we revisit therole of graphs in multimodal learning within the era of foundation models andpropose Graph4MM, a graph-based multimodal learning framework. To be specific,we introduce Hop-Diffused Attention, which integrates multi-hop structuralinformation into self-attention through causal masking and hop diffusion.Furthermore, we design MM-QFormer, a multi-mapping querying transformer forcross-modal fusion. Through theoretical and empirical analysis, we show thatleveraging structures to integrate both intra- and inter-modal interactionsimproves multimodal understanding beyond treating them as a standalonemodality. Experiments on both generative and discriminative tasks show thatGraph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,achieving a 6.93% average improvement.</description>
      <author>example@mail.com (Xuying Ning, Dongqi Fu, Tianxin Wei, Wujiang Xu, Jingrui He)</author>
      <guid isPermaLink="false">2510.16990v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis</title>
      <link>http://arxiv.org/abs/2510.16973v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述文章对医学图像分析中的基础模型(FMs)进行了全面和结构化的分析，系统性地分类了研究进展并评估了其临床应用价值。&lt;h4&gt;背景&lt;/h4&gt;人工智能特别是基础模型的最新进展彻底改变了医学图像分析，在多种医学影像任务中表现出强大的零样本和少样本性能。与传统特定任务AI模型不同，基础模型利用大量标记和非标记的多模态数据集学习通用表示，可通过微调适应各种下游临床应用。&lt;h4&gt;目的&lt;/h4&gt;弥补医学影像领域基础模型研究的碎片化现状，提供一个统一的综合分析，系统性地映射不同模态下架构、训练范式和临床应用的演变。&lt;h4&gt;方法&lt;/h4&gt;将研究按架构基础、训练策略和下游临床任务分为纯视觉基础模型和视觉语言基础模型；进行定量元分析，描述数据集利用和应用领域的时间趋势；批判性地讨论持续存在的挑战和新出现的解决方案。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型可通过微调适应各种临床应用；持续存在的挑战包括领域适应、高效微调、计算限制和可解释性；新兴解决方案包括联邦学习、知识蒸馏和高级提示技术。&lt;h4&gt;结论&lt;/h4&gt;需要加强基础模型的鲁棒性、可解释性和临床集成研究，以加速这些模型转化为实际医疗实践。&lt;h4&gt;翻译&lt;/h4&gt;人工智能(AI)特别是基础模型(FMs)的最新进展彻底改变了医学图像分析，在从分割到报告生成的多种医学影像任务中表现出强大的零样本和少样本性能。与传统的特定任务AI模型不同，基础模型利用大量标记和非标记的多模态数据集学习通用表示，这些通用表示可以通过微调适应各种下游临床应用。然而，尽管医学影像中基础模型研究迅速增长，该领域仍然碎片化，缺乏一个统一的综合分析来系统性地映射不同模态下架构、训练范式和临床应用的演变。为解决这一差距，这篇综述文章对医学图像分析中的基础模型提供了全面和结构化的分析。我们根据架构基础、训练策略和下游临床任务将研究系统性地分为纯视觉基础模型和视觉语言基础模型。此外，还对研究进行了定量元分析，以描述数据集利用和应用领域的时间趋势。我们还批判性地讨论了持续存在的挑战，包括领域适应、高效微调、计算限制和可解释性，以及新兴的解决方案，如联邦学习、知识蒸馏和高级提示技术。最后，我们确定了旨在增强基础模型的鲁棒性、可解释性和临床集成的关键未来研究方向，从而加速它们转化为实际医疗实践。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in artificial intelligence (AI), particularly foundationmodels (FMs), have revolutionized medical image analysis, demonstrating strongzero- and few-shot performance across diverse medical imaging tasks, fromsegmentation to report generation. Unlike traditional task-specific AI models,FMs leverage large corpora of labeled and unlabeled multimodal datasets tolearn generalized representations that can be adapted to various downstreamclinical applications with minimal fine-tuning. However, despite the rapidproliferation of FM research in medical imaging, the field remains fragmented,lacking a unified synthesis that systematically maps the evolution ofarchitectures, training paradigms, and clinical applications across modalities.To address this gap, this review article provides a comprehensive andstructured analysis of FMs in medical image analysis. We systematicallycategorize studies into vision-only and vision-language FMs based on theirarchitectural foundations, training strategies, and downstream clinical tasks.Additionally, a quantitative meta-analysis of the studies was conducted tocharacterize temporal trends in dataset utilization and application domains. Wealso critically discuss persistent challenges, including domain adaptation,efficient fine-tuning, computational constraints, and interpretability alongwith emerging solutions such as federated learning, knowledge distillation, andadvanced prompting. Finally, we identify key future research directions aimedat enhancing the robustness, explainability, and clinical integration of FMs,thereby accelerating their translation into real-world medical practice.</description>
      <author>example@mail.com (Praveenbalaji Rajendran, Mojtaba Safari, Wenfeng He, Mingzhe Hu, Shansong Wang, Jun Zhou, Xiaofeng Yang)</author>
      <guid isPermaLink="false">2510.16973v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Chem-R: Learning to Reason as a Chemist</title>
      <link>http://arxiv.org/abs/2510.16880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, 14 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Chem-R是一个通用的化学推理模型，通过三阶段训练框架实现先进化学推理能力，在综合基准测试上取得最先进性能，超越现有模型。&lt;h4&gt;背景&lt;/h4&gt;当前大语言模型在化学发现方面缺乏核心化学知识，推理轨迹不可靠，且在各类化学任务中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决现有大语言模型在化学领域的局限性，开发一个能模拟化学家深思熟虑过程的通用化学推理模型。&lt;h4&gt;方法&lt;/h4&gt;通过三阶段框架训练：1)化学基础训练建立核心知识；2)化学推理协议蒸馏融入结构化专家推理轨迹；3)多任务组相对策略优化模型在分子和反应级任务上的平衡性能。&lt;h4&gt;主要发现&lt;/h4&gt;Chem-R在综合基准测试上取得最先进性能，超越Gemini-2.5-Pro和DeepSeek-R1等领先模型，在分子任务上领先最多46%，在反应任务上领先最多66%，且在分子和反应级任务上都优于现有化学基础模型。&lt;h4&gt;结论&lt;/h4&gt;Chem-R具有强大的泛化能力和可解释性，有望成为下一代AI驱动化学发现的基础。&lt;h4&gt;翻译&lt;/h4&gt;尽管大型语言模型在化学发现方面具有巨大潜力，但当前模型缺乏核心化学知识，产生不可靠的推理轨迹，并在各种化学任务中表现不佳。为解决这些挑战，我们提出了Chem-R，一个通用的化学推理模型，旨在模拟化学家的深思熟虑过程。Chem-R通过三阶段框架进行训练，逐步构建高级推理能力：1)化学基础训练，建立核心化学知识；2)化学推理协议蒸馏，融入结构化、专家式的推理轨迹，引导系统化和可靠的问题解决；3)多任务组相对策略优化，优化模型在多样化的分子级和反应级任务上的平衡性能。这种结构化管道使Chem-R能够在综合基准测试上实现最先进的性能，超越包括Gemini-2.5-Pro和DeepSeek-R1在内的领先大型语言模型，在分子任务上领先高达46%，在反应任务上领先高达66%。同时，Chem-R在分子和反应级任务上也 consistently 优于现有的化学基础模型。这些结果突显了Chem-R的强大泛化能力、可解释性以及作为下一代AI驱动化学发现基础的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although large language models (LLMs) have significant potential to advancechemical discovery, current LLMs lack core chemical knowledge, produceunreliable reasoning trajectories, and exhibit suboptimal performance acrossdiverse chemical tasks. To address these challenges, we propose Chem-R, ageneralizable Chemical Reasoning model designed to emulate the deliberativeprocesses of chemists. Chem-R is trained through a three-phase framework thatprogressively builds advanced reasoning capabilities, including: 1) ChemicalFoundation Training, which establishes core chemical knowledge. 2) ChemicalReasoning Protocol Distillation, incorporating structured, expert-likereasoning traces to guide systematic and reliable problem solving. 3)Multi-task Group Relative Policy Optimization that optimizes the model forbalanced performance across diverse molecular- and reaction-level tasks. Thisstructured pipeline enables Chem-R to achieve state-of-the-art performance oncomprehensive benchmarks, surpassing leading large language models, includingGemini-2.5-Pro and DeepSeek-R1, by up to 46% on molecular tasks and 66% onreaction tasks. Meanwhile, Chem-R also consistently outperforms the existingchemical foundation models across both molecular and reaction level tasks.These results highlight Chem-R's robust generalization, interpretability, andpotential as a foundation for next-generation AI-driven chemical discovery.</description>
      <author>example@mail.com (Weida Wang, Benteng Chen, Di Zhang, Wanhao Liu, Shuchen Pu, Ben Gao, Jin Zeng, Lei Bai, Wanli Ouyang, Xiaoyong Wei, Tianshu Yu, Tianfan Fu, Shuzhou Sun, Jiatong Li, Zifu Wang, Yuqiang Li, Shufei Zhang)</author>
      <guid isPermaLink="false">2510.16880v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation</title>
      <link>http://arxiv.org/abs/2510.16776v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EMRRG的新型X光报告生成框架，该框架使用参数高效方法微调预训练的Mamba网络，结合具有混合解码器的LLM生成医学报告，在基准数据集上取得了良好效果。&lt;h4&gt;背景&lt;/h4&gt;X光图像医学报告生成(MRG)是人工智能的重要领域，可以显著减轻临床医生的诊断负担和患者等待时间。现有MRG模型主要依赖大型语言模型，对预训练视觉基础模型或高级微调技术的探索有限，且非Transformer架构在医学报告生成中研究不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的X光报告生成框架EMRRG，使用参数高效方法微调预训练的Mamba网络，探索非Transformer架构在医学报告生成中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;将X光图像分割成块并进行标记化处理，通过基于SSM的视觉主干网络进行特征提取，使用部分LoRA技术获得最佳性能。采用具有混合解码器的LLM生成医学报告，实现端到端训练。&lt;h4&gt;主要发现&lt;/h4&gt;在三个广泛使用的基准数据集上的大量实验验证了所提出策略在X光医学报告生成中的有效性。&lt;h4&gt;结论&lt;/h4&gt;EMRRG框架是一种有效的X光医学报告生成方法，结合了Mamba网络和参数高效微调技术，为医学报告生成领域提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence that can significantly reduce diagnostic burdens for clinicians and patient wait times. Existing MRG models predominantly rely on Large Language Models (LLMs) to improve report generation, with limited exploration of pre-trained vision foundation models or advanced fine-tuning techniques. Mainstream frameworks either avoid fine-tuning or utilize simplistic methods like LoRA, often neglecting the potential of enhancing cross-attention mechanisms. Additionally, while Transformer-based models dominate vision-language tasks, non-Transformer architectures, such as the Mamba network, remain underexplored for medical report generation, presenting a promising avenue for future research. In this paper, we propose EMRRG, a novel X-ray report generation framework that fine-tunes pre-trained Mamba networks using parameter-efficient methods. Specifically, X-ray images are divided into patches, tokenized, and processed by an SSM-based vision backbone for feature extraction, with Partial LoRA yielding optimal performance. An LLM with a hybrid decoder generates the medical report, enabling end-to-end training and achieving strong results on benchmark datasets. Extensive experiments on three widely used benchmark datasets fully validated the effectiveness of our proposed strategies for the X-ray MRG. The source code of this paper will be released on https://github.com/Event-AHU/Medical_Image_Analysis.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; X-ray image-based medical report generation (MRG) is a pivotal area inartificial intelligence that can significantly reduce diagnostic burdens forclinicians and patient wait times. Existing MRG models predominantly rely onLarge Language Models (LLMs) to improve report generation, with limitedexploration of pre-trained vision foundation models or advanced fine-tuningtechniques. Mainstream frameworks either avoid fine-tuning or utilizesimplistic methods like LoRA, often neglecting the potential of enhancingcross-attention mechanisms. Additionally, while Transformer-based modelsdominate vision-language tasks, non-Transformer architectures, such as theMamba network, remain underexplored for medical report generation, presenting apromising avenue for future research. In this paper, we propose EMRRG, a novelX-ray report generation framework that fine-tunes pre-trained Mamba networksusing parameter-efficient methods. Specifically, X-ray images are divided intopatches, tokenized, and processed by an SSM-based vision backbone for featureextraction, with Partial LoRA yielding optimal performance. An LLM with ahybrid decoder generates the medical report, enabling end-to-end training andachieving strong results on benchmark datasets. Extensive experiments on threewidely used benchmark datasets fully validated the effectiveness of ourproposed strategies for the X-ray MRG. The source code of this paper will bereleased on https://github.com/Event-AHU/Medical_Image_Analysis.</description>
      <author>example@mail.com (Mingzheng Zhang, Jinfeng Gao, Dan Xu, Jiangrui Yu, Yuhan Qiao, Lan Chen, Jin Tang, Xiao Wang)</author>
      <guid isPermaLink="false">2510.16776v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge</title>
      <link>http://arxiv.org/abs/2510.16716v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DistilLock是一个TEE辅助的微调框架，能够在边缘设备上实现隐私保护的知识蒸馏，同时保护数据隐私和模型知识产权。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在各种任务上表现出色，但微调通常依赖于基于云的集中式基础设施，需要数据所有者上传敏感数据，引发隐私问题；而在边缘设备上直接微调则存在模型知识产权泄露风险。&lt;h4&gt;目的&lt;/h4&gt;解决在保护数据隐私和模型知识产权的同时，在边缘设备上高效微调大型语言模型的困境。&lt;h4&gt;方法&lt;/h4&gt;提出DistilLock框架，在TEE中执行专有基础模型作为安全黑盒教师，并采用模型模糊化机制将模糊化权重卸载到不可信加速器上进行知识蒸馏。&lt;h4&gt;主要发现&lt;/h4&gt;DistilLock能够防止未授权的知识蒸馏过程和模型窃取攻击，同时保持高计算效率。&lt;h4&gt;结论&lt;/h4&gt;DistilLock为基于边缘的LLM个性化提供了一种安全且实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型已在各种任务上展现出强大的性能，但对其进行微调通常依赖于基于云的集中式基础设施。这需要数据所有者将可能敏感的数据上传到外部服务器，引发严重的隐私问题。另一种替代方法是在边缘设备上使用本地数据直接微调LLMs；然而，这带来了新的挑战：模型所有者必须将专有模型传输到边缘设备，这存在知识产权泄露的风险。为了解决这一困境，我们提出了DistilLock，一个TEE辅助的微调框架，能够在边缘上实现隐私保护的知识蒸馏。在DistilLock中，专有基础模型在数据所有者设备上的可信执行环境enclave中执行，充当安全的黑盒教师。这种设置通过防止直接访问模型内部，既保护了数据隐私又保护了模型IP。此外，DistilLock采用模型模糊化机制，将模糊化的权重卸载到不可信的加速器上，以实现高效的知识蒸馏而不损害安全性。我们证明，DistilLock能够防止未授权的知识蒸馏过程和模型窃取攻击，同时保持高计算效率，为基于边缘的LLM个性化提供了一种安全且实用的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have demonstrated strong performance acrossdiverse tasks, but fine-tuning them typically relies on cloud-based,centralized infrastructures. This requires data owners to upload potentiallysensitive data to external servers, raising serious privacy concerns. Analternative approach is to fine-tune LLMs directly on edge devices using localdata; however, this introduces a new challenge: the model owner must transferproprietary models to the edge, which risks intellectual property (IP) leakage.To address this dilemma, we propose DistilLock, a TEE-assisted fine-tuningframework that enables privacy-preserving knowledge distillation on the edge.In DistilLock, a proprietary foundation model is executed within a trustedexecution environment (TEE) enclave on the data owner's device, acting as asecure black-box teacher. This setup preserves both data privacy and model IPby preventing direct access to model internals. Furthermore, DistilLock employsa model obfuscation mechanism to offload obfuscated weights to untrustedaccelerators for efficient knowledge distillation without compromisingsecurity. We demonstrate that DistilLock prevents unauthorized knowledgedistillation processes and model-stealing attacks while maintaining highcomputational efficiency, but offering a secure and practical solution foredge-based LLM personalization.</description>
      <author>example@mail.com (Asmita Mohanty, Gezheng Kang, Lei Gao, Murali Annavaram)</author>
      <guid isPermaLink="false">2510.16716v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Universal and Transferable Attacks on Pathology Foundation Models</title>
      <link>http://arxiv.org/abs/2510.16660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  38 Pages, 8 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队提出了UTAP（通用可迁移对抗扰动）方法，揭示病理学基础模型的关键漏洞，该扰动能系统性地破坏多个模型的特征表示能力，导致下游任务性能下降。&lt;h4&gt;背景&lt;/h4&gt;病理学基础模型在医学诊断中应用广泛，但存在安全漏洞和鲁棒性问题，需要评估和防御机制。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用的对抗扰动方法，评估病理学基础模型的鲁棒性，并推动防御机制的发展。&lt;h4&gt;方法&lt;/h4&gt;使用深度学习优化UTAP，创建一种固定且微弱的噪声模式，添加到病理图像中以破坏基础模型的特征表示能力。&lt;h4&gt;主要发现&lt;/h4&gt;UTAP导致下游任务性能下降，具有通用性（可应用于不同视野范围，与开发数据集无关）和可迁移性（能降低各种黑盒病理学基础模型的性能），构成对多种病理学基础模型的广泛威胁。&lt;h4&gt;结论&lt;/h4&gt;UTAP为模型鲁棒性评估设定了高标准基准，突显了推进防御机制的必要性，可能为对抗训练提供资源，确保AI在病理学中的安全可靠部署。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了针对病理学基础模型的通用可迁移对抗扰动（UTAP），揭示了其能力中的关键漏洞。使用深度学习优化，UTAP由固定且微弱的噪声模式组成，当添加到病理图像中时，会系统性地破坏多个病理学基础模型的特征表示能力。因此，UTAP会导致利用基础模型的下游任务性能下降，包括在广泛未见过的数据分布上的错误分类。除了损害模型性能外，我们还证明了UTAP的两个关键特性：(1) 通用性：其扰动可以应用于不同的视野范围，且与开发UTAP的数据集无关；(2) 可迁移性：其扰动可以成功降低各种外部、黑盒病理学基础模型的性能——这些模型以前从未见过。这两个特性表明，UTAP不是与特定基础模型或图像数据集相关的专门攻击，而是对各种新兴病理学基础模型及其应用构成广泛威胁。我们在多个数据集上的各种最先进病理学基础模型上系统评估了UTAP，使用固定噪声模式对输入图像进行几乎不可见的修改，导致了其性能显著下降。这些强大攻击的建立为模型鲁棒性评估设定了关键的高标准基准，突显了推进防御机制的必要性，并可能为对抗训练提供必要资源，以确保AI在病理学中的安全可靠部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Universal and Transferable Adversarial Perturbations (UTAP) forpathology foundation models that reveal critical vulnerabilities in theircapabilities. Optimized using deep learning, UTAP comprises a fixed and weaknoise pattern that, when added to a pathology image, systematically disruptsthe feature representation capabilities of multiple pathology foundationmodels. Therefore, UTAP induces performance drops in downstream tasks thatutilize foundation models, including misclassification across a wide range ofunseen data distributions. In addition to compromising the model performance,we demonstrate two key features of UTAP: (1) universality: its perturbation canbe applied across diverse field-of-views independent of the dataset that UTAPwas developed on, and (2) transferability: its perturbation can successfullydegrade the performance of various external, black-box pathology foundationmodels - never seen before. These two features indicate that UTAP is not adedicated attack associated with a specific foundation model or image dataset,but rather constitutes a broad threat to various emerging pathology foundationmodels and their applications. We systematically evaluated UTAP across variousstate-of-the-art pathology foundation models on multiple datasets, causing asignificant drop in their performance with visually imperceptible modificationsto the input images using a fixed noise pattern. The development of thesepotent attacks establishes a critical, high-standard benchmark for modelrobustness evaluation, highlighting a need for advancing defense mechanisms andpotentially providing the necessary assets for adversarial training to ensurethe safe and reliable deployment of AI in pathology.</description>
      <author>example@mail.com (Yuntian Wang, Xilin Yang, Che-Yung Shen, Nir Pillar, Aydogan Ozcan)</author>
      <guid isPermaLink="false">2510.16660v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Hallucination Benchmark for Speech Foundation Models</title>
      <link>http://arxiv.org/abs/2510.16567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SHALLOW，第一个系统分类和量化语音识别系统幻觉现象的基准框架，解决了传统评估指标无法区分幻觉与其他类型错误的问题。&lt;h4&gt;背景&lt;/h4&gt;自动语音识别系统中的幻觉现象指的是神经模型产生的流畅连贯的转录，但这些转录与底层声学输入完全无关。这些幻觉虽然类似于传统解码错误，但由于保留了句法和语义上合理的结构，可能更具危害性，特别是在医疗和法律等关键领域。传统评估指标主要基于错误指标，无法区分语音不准确和幻觉。&lt;h4&gt;目的&lt;/h4&gt;开发新的评估框架，能够有效识别和评估产生幻觉内容倾向更高的模型，并提供更细粒度的错误分析。&lt;h4&gt;方法&lt;/h4&gt;提出SHALLOW框架，系统性地沿着四个互补轴对ASR中的幻觉现象进行分类和量化：词汇、语音、形态和语义。在每个类别中定义有针对性的指标，以产生可解释的模型行为特征。&lt;h4&gt;主要发现&lt;/h4&gt;通过评估各种架构和语音领域，发现当识别质量高（即低词错误率WER）时，SHALLOW指标与WER高度相关；但随着WER的增加，这种相关性显著减弱。SHALLOW能够捕获在降级和挑战性条件下WER无法区分的细粒度错误模式。&lt;h4&gt;结论&lt;/h4&gt;SHALLOW框架支持对模型弱点的具体诊断，并提供比总体错误率所能提供的更详细的模型改进反馈，有助于提高语音识别系统在关键领域的可靠性。&lt;h4&gt;翻译&lt;/h4&gt;自动语音识别系统中的幻觉现象指的是神经ASR模型产生的流畅连贯的转录，这些转录与底层声学输入（即语音信号）完全无关。虽然幻觉可能类似于传统解码错误，在可能降低转录对下游应用的可用性方面，但幻觉由于其保留了句法和语义上合理的结构，可能更具危害性。这种明显的连贯性可能误导后续处理阶段并引入严重风险，特别是在医疗和法律等关键领域。传统评估指标主要基于错误指标，无法区分语音不准确和幻觉。因此，迫切需要新的评估框架，能够有效识别和评估产生幻觉内容倾向更高的模型。为此，我们引入了SHALLOW，这是第一个基准框架，系统性地沿着四个互补轴对ASR中的幻觉现象进行分类和量化：词汇、语音、形态和语义。我们在每个类别中定义有针对性的指标，以产生可解释的模型行为特征。通过评估各种架构和语音领域，我们发现当识别质量高（即低WER）时，SHALLOW指标与词错误率（WER）高度相关。然而，随着WER的增加，这种相关性显著减弱。因此，SHALLOW捕获了在降级和挑战性条件下WER无法区分的细粒度错误模式。我们的框架支持对模型弱点的具体诊断，并提供比总体错误率所能提供的更详细的模型改进反馈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hallucinations in automatic speech recognition (ASR) systems refer to fluentand coherent transcriptions produced by neural ASR models that are completelyunrelated to the underlying acoustic input (i.e., the speech signal). Whilesimilar to conventional decoding errors in potentially compromising theusability of transcriptions for downstream applications, hallucinations can bemore detrimental due to their preservation of syntactically and semanticallyplausible structure. This apparent coherence can mislead subsequent processingstages and introduce serious risks, particularly in critical domains such ashealthcare and law. Conventional evaluation metrics are primarily centered onerror-based metrics and fail to distinguish between phonetic inaccuracies andhallucinations. Consequently, there is a critical need for new evaluationframeworks that can effectively identify and assess models with a heightenedpropensity for generating hallucinated content. To this end, we introduceSHALLOW, the first benchmark framework that systematically categorizes andquantifies hallucination phenomena in ASR along four complementary axes:lexical, phonetic, morphological, and semantic. We define targeted metricswithin each category to produce interpretable profiles of model behavior.Through evaluation across various architectures and speech domains, we havefound that SHALLOW metrics correlate strongly with word error rate (WER) whenrecognition quality is high (i.e., low WER). Still, this correlation weakenssubstantially as WER increases. SHALLOW, therefore, captures fine-grained errorpatterns that WER fails to distinguish under degraded and challengingconditions. Our framework supports specific diagnosis of model weaknesses andprovides feedback for model improvement beyond what aggregate error rates canoffer.</description>
      <author>example@mail.com (Alkis Koudounas, Moreno La Quatra, Manuel Giollo, Sabato Marco Siniscalchi, Elena Baralis)</author>
      <guid isPermaLink="false">2510.16567v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>NeurIPT: Foundation Model for Neural Interfaces</title>
      <link>http://arxiv.org/abs/2510.16548v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by The Thirty-Ninth Annual Conference on Neural Information  Processing Systems (NeurIPS 2025). Project Page:  https://ZzzitaoFang.github.io/projects/NeurIPT/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了NeurIPT，一种专为多样化EEG神经接口设计的基础模型，通过结合基于幅度的掩码预训练和渐进式专家混合架构，有效捕捉EEG信号中的时空特征，在多个BCI数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;脑电图(EEG)在临床诊断和脑机接口中有广泛应用，随着EEG数据量和多样性的增加，建立基础模型来扩展和泛化神经解码成为研究热点。然而，将基础模型应用于EEG仍面临受试者间、任务间和条件间变异性，以及不同电极配置带来的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理EEG数据多样性和变异性的基础模型，以提高神经解码的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出NeurIPT基础模型，采用预训练Transformer架构；时间维度引入基于信号幅度的掩码预训练(AAMP)和渐进式专家混合(PMoE)架构；空间维度利用电极的三维物理坐标实现跨设置的嵌入迁移，并开发脑叶内-间池化(IILP)以利用区域脑特征。&lt;h4&gt;主要发现&lt;/h4&gt;在八个下游BCI数据集上的实证评估表明，NeurIPT通过微调始终取得了最先进的性能，展示了其广泛的适用性和鲁棒的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作推动了EEG基础模型的前沿发展，并为可扩展和可泛化的神经信息处理系统提供了见解。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)有广泛的应用，从临床诊断到脑机接口(BCIs)。随着EEG数据的数量和多样性的增加，人们越来越有兴趣建立基础模型(FMs)来扩展和泛化神经解码。尽管显示出早期潜力，但由于显著的受试者间、任务间和条件间变异性，以及不同记录设置中的多样化电极配置，将基础模型应用于EEG仍然具有挑战性。为了解决这些开放挑战，我们提出了NeurIPT，这是一种为多样化EEG神经接口开发的基础模型，通过捕捉EEG信号中固有的同质和异质时空特征，采用预训练Transformer架构。在时间维度上，我们引入了基于幅度的掩码预训练(AAMP)，基于信号幅度而非随机间隔进行掩码，以学习跨越不同信号强度的鲁棒表示，而不仅仅是局部插值。此外，这种时间表示通过渐进式专家混合(PMoE)架构得到增强，在更深层次逐步引入专门的专家子网络，有效适应EEG信号的多样化时间特征。在空间维度上，NeurIPT利用电极的三维物理坐标，实现不同EEG设置间的嵌入有效迁移，并在微调过程中开发脑叶内-间池化(IILP)，以高效利用区域脑特征。在八个下游BCI数据集上的实证评估，通过微调，证明了NeurIPT始终取得了最先进的性能，突显了其广泛的适用性和鲁棒的泛化能力。我们的工作推动了EEG基础模型的前沿发展，并为可扩展和可泛化的神经信息处理系统提供了见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) has wide-ranging applications, from clinicaldiagnosis to brain-computer interfaces (BCIs). With the increasing volume andvariety of EEG data, there has been growing interest in establishing foundationmodels (FMs) to scale up and generalize neural decoding. Despite showing earlypotential, applying FMs to EEG remains challenging due to substantialinter-subject, inter-task, and inter-condition variability, as well as diverseelectrode configurations across recording setups. To tackle these openchallenges, we propose NeurIPT, a foundation model developed for diverseEEG-based Neural Interfaces with a Pre-trained Transformer by capturing bothhomogeneous and heterogeneous spatio-temporal characteristics inherent in EEGsignals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),masking based on signal amplitude rather than random intervals, to learn robustrepresentations across varying signal intensities beyond local interpolation.Moreover, this temporal representation is enhanced by a ProgressiveMixture-of-Experts (PMoE) architecture, where specialized expert subnetworksare progressively introduced at deeper layers, adapting effectively to thediverse temporal characteristics of EEG signals. Spatially, NeurIPT leveragesthe 3D physical coordinates of electrodes, enabling effective transfer ofembedding across varying EEG settings, and develops Intra-Inter Lobe Pooling(IILP) during fine-tuning to efficiently exploit regional brain features.Empirical evaluations across eight downstream BCI datasets, via fine-tuning,demonstrated NeurIPT consistently achieved state-of-the-art performance,highlighting its broad applicability and robust generalization. Our work pushesforward the state of FMs in EEG and offers insights into scalable andgeneralizable neural information processing systems.</description>
      <author>example@mail.com (Zitao Fang, Chenxuan Li, Hongting Zhou, Shuyang Yu, Guodong Du, Ashwaq Qasem, Yang Lu, Jing Li, Junsong Zhang, Sim Kuan Goh)</author>
      <guid isPermaLink="false">2510.16548v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion</title>
      <link>http://arxiv.org/abs/2510.16446v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VIPAMIN是一种视觉提示初始化策略，通过将提示与嵌入空间中的语义信息丰富区域对齐，并向预训练子空间注入新的表示方向，增强自监督模型的适应性，在各种任务和数据集大小上一致提高性能。&lt;h4&gt;背景&lt;/h4&gt;在大规模基础模型时代，为每个下游任务完全微调预训练网络通常需要大量资源。&lt;h4&gt;目的&lt;/h4&gt;解决现有视觉提示调整方法在专门化提示或丰富表示空间方面的局限性，特别是在自监督主干网络应用于具有挑战性任务和数据稀缺环境时。&lt;h4&gt;方法&lt;/h4&gt;提出VIPAMIN，一种视觉提示初始化策略，通过两种方式增强自监督模型的适应性：(1)将提示与嵌入空间中的语义信息丰富的区域对齐，(2)向预训练子空间注入新的表示方向。该方法仅需一次前向传播和轻量级操作。&lt;h4&gt;主要发现&lt;/h4&gt;VIPAMIN在各种任务和数据集大小上一致提高了性能，在视觉提示调整方面树立了新的最先进水平，特别是在具有挑战性的任务和数据稀缺环境中表现突出。&lt;h4&gt;结论&lt;/h4&gt;VIPAMIN是一种简单而有效的视觉提示初始化策略，能够显著提升自监督模型在下游任务中的适应性和性能。&lt;h4&gt;翻译&lt;/h4&gt;在大规模基础模型时代，为每个下游任务完全微调预训练网络通常需要大量资源。提示调整通过引入可调整提示同时保持主干网络冻结，提供了一种轻量级替代方案。然而，现有的视觉提示调整方法通常无法专门化提示或丰富表示空间——特别是当应用于自监督主干网络时。我们表明，这些限制在具有挑战性的任务和数据稀缺环境中变得尤为明显，而有效适应在这些环境中最为关键。在这项工作中，我们介绍了VIPAMIN，一种视觉提示初始化策略，通过(1)将提示与嵌入空间中的语义信息丰富区域对齐，和(2)向预训练子空间注入新的表示方向，来增强自监督模型的适应性。尽管简单——只需要一次前向传播和轻量级操作——VIPAMIN在不同任务和数据集大小上一致提高了性能，在视觉提示调整方面树立了新的最先进水平。我们的代码可在https://github.com/iamjaekyun/vipamin获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of large-scale foundation models, fully fine-tuning pretrainednetworks for each downstream task is often prohibitively resource-intensive.Prompt tuning offers a lightweight alternative by introducing tunable promptswhile keeping the backbone frozen. However, existing visual prompt tuningmethods often fail to specialize the prompts or enrich the representationspace--especially when applied to self-supervised backbones. We show that theselimitations become especially pronounced in challenging tasks and data-scarcesettings, where effective adaptation is most critical. In this work, weintroduce VIPAMIN, a visual prompt initialization strategy that enhancesadaptation of self-supervised models by (1) aligning prompts with semanticallyinformative regions in the embedding space, and (2) injecting novelrepresentational directions beyond the pretrained subspace. Despite itssimplicity--requiring only a single forward pass and lightweightoperations--VIPAMIN consistently improves performance across diverse tasks anddataset sizes, setting a new state of the art in visual prompt tuning. Our codeis available at https://github.com/iamjaekyun/vipamin.</description>
      <author>example@mail.com (Jaekyun Park, Hye Won Chung)</author>
      <guid isPermaLink="false">2510.16446v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment</title>
      <link>http://arxiv.org/abs/2510.16387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了Whisper模型在第二语言口语评估中的应用潜力，通过提取声学和语言特征，实现了超越现有方法的性能，并揭示了模型内在编码语言能力的特点。&lt;h4&gt;背景&lt;/h4&gt;Whisper是一个成熟的自动语音识别基础模型，先前研究主要分析其生成的转录文本，而对其潜在能力的探索不足。&lt;h4&gt;目的&lt;/h4&gt;探索Whisper模型在第二语言口语评估任务中的潜在能力，分析其内在编码的语言能力特点。&lt;h4&gt;方法&lt;/h4&gt;从Whisper的隐藏表示中提取声学和语言特征，在Whisper的中间和最终输出之上训练轻量级分类器，并融入图像和文本提示信息作为辅助线索。&lt;h4&gt;主要发现&lt;/h4&gt;Whisper模型在GEPT图片描述数据集上超越了现有最先进基线；融入图像和文本提示信息可进一步提升性能；即使没有任务特定微调，Whisper也能内在编码语言熟练程度的顺序模式和语音的语义方面。&lt;h4&gt;结论&lt;/h4&gt;Whisper模型具有作为第二语言口语评估和其他口语理解任务强大基础的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们探讨了Whisper这一成熟的自动语音识别基础模型在第二语言口语评估背景下的未开发潜力。与先前研究仅外在分析Whisper生成的转录文本不同，我们的方法更进一步，通过从隐藏表示中提取声学和语言特征来探测其潜在能力。仅通过在Whisper的中间和最终输出之上训练一个轻量级分类器，我们的方法在GEPT图片描述数据集上实现了强大的性能，超越了现有的最先进基线，包括一种多模态方法。此外，通过将图像和文本提示信息作为辅助相关性线索纳入，我们展示了额外的性能提升。最后，我们对Whisper的嵌入进行了深入分析，揭示出即使没有任务特定的微调，该模型也内在地编码了熟练程度的顺序模式和语音的语义方面，突显了其作为第二语言口语评估和其他口语理解任务强大基础的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we explore the untapped potential of Whisper, awell-established automatic speech recognition (ASR) foundation model, in thecontext of L2 spoken language assessment (SLA). Unlike prior studies thatextrinsically analyze transcriptions produced by Whisper, our approach goes astep further to probe its latent capabilities by extracting acoustic andlinguistic features from hidden representations. With only a lightweightclassifier being trained on top of Whisper's intermediate and final outputs,our method achieves strong performance on the GEPT picture-description dataset,outperforming existing cutting-edge baselines, including a multimodal approach.Furthermore, by incorporating image and text-prompt information as auxiliaryrelevance cues, we demonstrate additional performance gains. Finally, weconduct an in-depth analysis of Whisper's embeddings, which reveals that, evenwithout task-specific fine-tuning, the model intrinsically encodes both ordinalproficiency patterns and semantic aspects of speech, highlighting its potentialas a powerful foundation for SLA and other spoken language understanding tasks.</description>
      <author>example@mail.com (Fu-An Chao, Bi-Cheng Yan, Berlin Chen)</author>
      <guid isPermaLink="false">2510.16387v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning</title>
      <link>http://arxiv.org/abs/2510.16240v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了Cosmos-Surg-dVRK，一种基于Cosmos世界基础模型的外科微调版本，结合视频分类器，实现了手术策略的全自动在线评估和基准测试，解决了在真实机器人平台上评估的高成本、时间和可重复性问题。&lt;h4&gt;背景&lt;/h4&gt;手术机器人和视觉-语言-动作模型的兴起推动了自主手术策略的发展，但在物理机器人平台（如dVRK）上直接评估这些策略面临高成本、时间需求、可重复性挑战和执行变异性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种高保真度的模拟方法，用于评估复杂的现实世界手术任务，并提供全自动化的在线评估和基准测试平台。&lt;h4&gt;方法&lt;/h4&gt;引入Cosmos-Surg-dVRK（Cosmos世界基础模型的外科微调版本），结合训练好的视频分类器，使用两个不同的外科数据集进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;在桌面缝合垫任务上，Cosmos-Surg-dVRK中的在线运行与真实dVRK平台上的策略结果具有强相关性；人类标注者与V-JEPA2派生的视频分类器达成良好一致；离体猪胆囊切除术任务的初步实验显示与现实世界评估的良好一致性。&lt;h4&gt;结论&lt;/h4&gt;Cosmos-Surg-dVRK平台在更复杂的手术程序评估方面具有潜力，为手术策略的自动化评估提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;手术机器人和视觉-语言-动作模型的兴起加速了自主手术策略和高效评估策略的发展。然而，在da Vinci研究套件(dVRK)等物理机器人平台上直接评估这些策略仍然受到高成本、时间需求、可重复性挑战和执行变异性的阻碍。物理AI的世界基础模型(WFM)为模拟复杂的现实世界手术任务（如软组织变形）提供了具有高保真度的变革性方法。这项工作介绍了Cosmos-Surg-dVRK，这是Cosmos WFM的外科微调版本，它与训练好的视频分类器一起，实现了手术策略的完全自动化在线评估和基准测试。我们使用两个不同的外科数据集评估了Cosmos-Surg-dVRK。在桌面缝合垫任务上，自动化流程在Cosmos-Surg-dVRK中的在线运行与真实dVRK Si平台上的策略结果之间实现了强相关性，并且人类标注者与V-JEPA2派生的视频分类器之间达成良好一致。此外，在Cosmos-Surg-dVRK中使用离体猪胆囊切除术任务的初步实验显示出与现实世界评估的良好一致性，突显了该平台在更复杂手术程序方面的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决外科机器人策略评估的问题。传统上，直接在物理机器人平台（如dVRK）上评估策略存在成本高、耗时长、可重复性差和执行变异大等挑战。此外，现有模拟器难以准确模拟外科手术中的软组织变形等复杂物理现象。这个问题在现实中很重要，因为随着外科机器人和视觉-语言-动作模型的发展，自主外科策略的开发日益增多，但缺乏高效、可靠的评估方法严重制约了这一领域的进步。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了世界基础模型（WFM）的概念，特别是Cosmos WFM这一视频生成模型，将其作为可扩展的通用学习模拟器。他们针对外科手术领域对Cosmos WFM进行了微调，创建了Cosmos-Surg-dVRK。此外，他们还使用了V-JEPA 2视频分类器来自动评估策略执行的成功率。作者参考了早期世界模型工作（如Ha &amp; Schmidhuber, 2018）和基于扩散过程的大规模多模态世界基础模型，以及现有的视觉-语言-动作模型在外科机器人中的应用，但针对外科手术的特殊需求进行了创新性改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用微调后的Cosmos世界基础模型（Cosmos-Surg-dVRK）作为专门针对外科手术的学习模拟器，结合视频分类器实现完全自动化的策略评估。整体流程包括：1) 使用真实机器人记录的初始状态初始化策略；2) 策略和Cosmos-Surg-dVRK自回归地生成未来帧；3) 将生成的帧组合成视频；4) 使用V-JEPA 2视频分类器自动评估视频中的任务成功或失败；5) 根据评估结果选择最有前途的策略进行真实机器人测试。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 针对外科手术领域微调的Cosmos-Surg-dVRK学习模拟器；2) 使用V-JEPA 2视频分类器实现完全自动化的策略评估；3) 在真实外科任务（包括桌面缝合垫任务和离体猪胆囊切除术）上验证了方法有效性；4) 通过消融研究强调了失败数据在训练中的重要性。相比之前的工作，Cosmos-Surg-dVRK直接从外科数据中学习软组织动力学，不需要显式指定材料属性；使用标准的相对笛卡尔动作空间，实现即插即用的接口；专注于外科手术这一特定领域，而大多数现有方法专注于其他领域如视频游戏或通用机器人。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了Cosmos-Surg-dVRK，一个基于世界基础模型的自动化外科机器人策略评估系统，通过在模拟环境中生成逼真的外科手术视频并自动评估策略性能，有效解决了传统评估中成本高、耗时长和难以模拟软组织变形等问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of surgical robots and vision-language-action models has acceleratedthe development of autonomous surgical policies and efficient assessmentstrategies. However, evaluating these policies directly on physical roboticplatforms such as the da Vinci Research Kit (dVRK) remains hindered by highcosts, time demands, reproducibility challenges, and variability in execution.World foundation models (WFM) for physical AI offer a transformative approachto simulate complex real-world surgical tasks, such as soft tissue deformation,with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetuneof the Cosmos WFM, which, together with a trained video classifier, enablesfully automated online evaluation and benchmarking of surgical policies. Weevaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletopsuture pad tasks, the automated pipeline achieves strong correlation betweenonline rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Siplatform, as well as good agreement between human labelers and the V-JEPA2-derived video classifier. Additionally, preliminary experiments with ex-vivoporcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promisingalignment with real-world evaluations, highlighting the platform's potentialfor more complex surgical procedures.</description>
      <author>example@mail.com (Lukas Zbinden, Nigel Nelson, Juo-Tung Chen, Xinhao Chen, Ji Woong, Kim, Mahdi Azizian, Axel Krieger, Sean Huver)</author>
      <guid isPermaLink="false">2510.16240v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Probing the Higgs Portal to a Strongly-Interacting Dark Sector at the FCC-ee</title>
      <link>http://arxiv.org/abs/2510.17675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 figure, to be submitted to EPJC&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了在未来的圆形对撞机e+e-碰撞模式下可能出现的来自禁闭暗区的奇异信号，研究希格斯玻色子作为媒介产生的暗夸克及其导致的半可见喷注终态，并提出使用图神经网络喷注标记器提高信号探测灵敏度。&lt;h4&gt;背景&lt;/h4&gt;在未来的圆形对撞机中，e+e-碰撞模式下可能产生来自禁闭暗区的奇异信号，希格斯玻色子可能作为标准模型和暗区之间相互作用的媒介。&lt;h4&gt;目的&lt;/h4&gt;研究希格斯玻色子诱导的半可见喷注的探测方法，提高信号与背景的区分度，增强对希格斯玻色子到暗夸克稀有分支比的探测能力。&lt;h4&gt;方法&lt;/h4&gt;研究不同不可见状态比例的半可见喷注特性；当不可见成分较大时，基于运动学特征（如缺失能量）进行选择；当不可见成分较小时，采用图神经网络喷注标记器利用喷注亚结构差异进行信号识别。&lt;h4&gt;主要发现&lt;/h4&gt;对于不可见成分较大的情况，基于缺失能量的选择能提供良好的信背比；对于不可见成分较小的情况，图神经网络喷注标记器能有效提高信号探测灵敏度；所提策略可探测广泛参数空间，将希格斯玻色子到暗夸克的稀有分支比限制在千分之一水平。&lt;h4&gt;结论&lt;/h4&gt;提出的机器学习策略能有效探测希格斯玻色子诱导的半可见喷注，增强在未来的圆形对撞机上的发现前景，并限制希格斯玻色子的稀有分支比。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了在未来的圆形对撞机e+e-碰撞模式下可能出现的来自禁闭暗区的奇异信号。假设希格斯玻色子介导标准模型与暗区之间的相互作用，暗夸克可以在e+e-碰撞中产生。随后的强动力学可能导致包含可见和不可见粒子的半可见喷注终态。我们研究了不同不可见状态比例的半可见喷注，以及富含轻子和光子的喷注。当不可见成分较大时，基于运动学特征（如事件中的缺失能量）的选择已能提供良好的信背比。对于较小的不可见比例，缺失能量的减少使这些信号更类似于标准模型事件，因此我们采用利用喷注亚结构差异的图神经网络喷注标记器。这种机器学习策略提高了灵敏度，增强了在未来的圆形对撞机上发现希格斯玻色子诱导的半可见喷注的前景。我们的结果表明，所提出的策略可以有效探测所考虑模型的广泛参数空间和各种信号，将希格斯玻色子到暗夸克的稀有分支比限制在千分之一水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work explores exotic signatures from confining dark sectors that mayarise in the e+e- collision mode at the Future Circular Collider. Assuming theHiggs boson mediates the interaction between the Standard Model and the darksector, dark quarks can be produced in e+e- collisions. The ensuing strongdynamics may lead to semi-visible jet final states, containing both visible andinvisible particles. We investigate semi-visible jets with different fractionsof invisible states, and enriched in leptons and photons. When the invisiblecomponent is large, selections based on kinematic features, such as the missingenergy in the event, already provide good signal-to-background discrimination.For smaller invisible fractions, the reduced missing energy makes these signalsmore similar to Standard Model events, and we therefore employ a graph neuralnetwork jet tagger exploiting differences in jet substructure. This machinelearning strategy improves sensitivity and enhances the discovery prospects ofHiggs boson-induced semi-visible jets at the Future Circular Collider. Ourresults show that the proposed strategy can effectively probe a wide parameterspace for the models considered, and a variety of signatures, constraining theHiggs boson exotic branching ratios into dark quarks at the permille-level.</description>
      <author>example@mail.com (Cesare Cazzaniga, Annapaola de Cosa, Felix Kahlhoefer, Andrea S. Maria, Roberto Seidita, Emre Sitti)</author>
      <guid isPermaLink="false">2510.17675v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Model Metamers Reveal Invariances in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.17378v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过元形态生成技术揭示了图神经网络(GNNs)中的过度不变性问题，并提出了改进方向和评估基准。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络在感知系统中被广泛使用以学习具有不变性的表示，模仿人脑机制。然而，视觉和听觉领域的研究证实，人工神经网络的不变性属性与人脑之间仍存在显著差距。&lt;h4&gt;目的&lt;/h4&gt;研究图神经网络(GNNs)中的不变性行为，探索其与人脑不变性机制的差异。&lt;h4&gt;方法&lt;/h4&gt;引入元形态生成技术，通过优化输入图使内部节点激活与参考图匹配，获得在表示空间中等效但在结构和特征上显著不同的图。理论研究聚焦于单个节点的局部元形态维度和元形态流形的激活诱导体积变化。&lt;h4&gt;主要发现&lt;/h4&gt;多种经典GNN架构表现出极端水平的表示不变性。虽然修改模型架构和训练策略可部分减轻这种过度不变性，但无法从根本上达到人脑水平的不变性。&lt;h4&gt;结论&lt;/h4&gt;量化元形态图与原始图之间的偏差，揭示了当前GNNs的独特失效模式，为模型评估提供了补充基准。&lt;h4&gt;翻译&lt;/h4&gt;近年来，深度神经网络已被广泛应用于感知系统，以学习具有不变性的表示，旨在模仿人脑中观察到的不变性机制。然而，视觉和听觉领域的研究证实，人工神经网络的不变性与人类之间仍存在显著差距。为了研究图神经网络(GNNs)中的不变性行为，我们引入了一种模型'元形态'生成技术。通过优化输入图使其内部节点激活与参考图相匹配，我们获得了在模型表示空间中等效但在结构和节点特征上显著不同的图。我们的理论分析聚焦于两个方面：单个节点的局部元形态维度和元形态流形的激活诱导体积变化。利用这种方法，我们在几种经典的GNN架构中发现了极端水平的表示不变性。虽然对模型架构和训练策略的有针对性的修改可以部分减轻这种过度不变性，但它们无法从根本上弥合与人脑相似的不变性之间的差距。最后，我们量化了元形态图与其原始对应物之间的偏差，揭示了当前GNNs的独特失效模式，并为模型评估提供了补充基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, deep neural networks have been extensively employed inperceptual systems to learn representations endowed with invariances, aiming toemulate the invariance mechanisms observed in the human brain. However, studiesin the visual and auditory domains have confirmed that significant gaps remainbetween the invariance properties of artificial neural networks and those ofhumans. To investigate the invariance behavior within graph neural networks(GNNs), we introduce a model ``metamers'' generation technique. By optimizinginput graphs such that their internal node activations match those of areference graph, we obtain graphs that are equivalent in the model'srepresentation space, yet differ significantly in both structure and nodefeatures. Our theoretical analysis focuses on two aspects: the local metamerdimension for a single node and the activation-induced volume change of themetamer manifold. Utilizing this approach, we uncover extreme levels ofrepresentational invariance across several classic GNN architectures. Althoughtargeted modifications to model architecture and training strategies canpartially mitigate this excessive invariance, they fail to fundamentally bridgethe gap to human-like invariance. Finally, we quantify the deviation betweenmetamer graphs and their original counterparts, revealing unique failure modesof current GNNs and providing a complementary benchmark for model evaluation.</description>
      <author>example@mail.com (Wei Xu, Xiaoyi Jiang, Lixiang Xu, Dechao Tang)</author>
      <guid isPermaLink="false">2510.17378v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses</title>
      <link>http://arxiv.org/abs/2510.17185v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种统一的框架来评估图神经网络和大语言模型在文本属性图学习中的鲁棒性，发现了模型在文本和结构之间存在固有的鲁棒性权衡，并提出了SFT-auto框架来克服这些权衡。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)和大语言模型(LLMs)是学习文本属性图(TAGs)的强大方法，但目前对其鲁棒性的理解还不全面。现有的评估是零散的，未能系统地研究不同模型和攻击场景下文本和结构扰动的不同影响。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些局限性，作者引入了一个统一的框架来评估TAG学习的鲁棒性，旨在系统地研究不同类型的扰动对各种模型的影响，并提出解决方案来克服发现的权衡。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一个统一的框架，评估了经典GNNs、鲁棒GNNs(RGNNs)和GraphLLMs在四个领域的十个数据集上的性能，测试了基于文本、基于结构和混合扰动在投毒和规避场景下的影响。为了克服发现的权衡，他们引入了SFT-auto框架。&lt;h4&gt;主要发现&lt;/h4&gt;1) 模型在文本和结构之间存在固有的鲁棒性权衡；2) GNNs和RGNNs的性能在很大程度上取决于文本编码器和攻击类型；3) GraphLLMs特别容易受到训练数据损坏的影响。&lt;h4&gt;结论&lt;/h4&gt;该研究为未来的TAG安全研究奠定了基础，并为对抗环境中的鲁棒TAG学习提供了实用的解决方案。作者公开了他们的代码。&lt;h4&gt;翻译&lt;/h4&gt;尽管图神经网络(GNNs)和大语言模型(LLMs)是学习文本属性图(TAGs)的强大方法，但对其鲁棒性的全面理解仍然模糊。目前的评估是零散的，未能系统地研究不同模型和攻击场景下文本和结构扰动的不同影响。为了解决这些局限性，我们引入了一个统一的综合框架来评估TAG学习中的鲁棒性。我们的框架在四个领域的十个数据集上评估了经典GNNs、鲁棒GNNs(RGNNs)和GraphLLMs，在投毒和规避场景下，应对了多种基于文本、基于结构和混合的扰动。我们的广泛分析揭示了多个发现，其中三个特别值得注意：1) 模型在文本和结构之间存在固有的鲁棒性权衡；2) GNNs和RGNNs的性能在很大程度上取决于文本编码器和攻击类型；3) GraphLLMs特别容易受到训练数据损坏的影响。为了克服识别出的权衡，我们引入了SFT-auto，这是一个新颖的框架，在单一模型内提供针对文本和结构攻击的优越且平衡的鲁棒性。我们的研究为未来的TAG安全研究奠定了基础，并为对抗环境中的鲁棒TAG学习提供了实用的解决方案。我们的代码可在以下网址获取：https://github.com/Leirunlin/TGRB。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Graph Neural Networks (GNNs) and Large Language Models (LLMs) arepowerful approaches for learning on Text-Attributed Graphs (TAGs), acomprehensive understanding of their robustness remains elusive. Currentevaluations are fragmented, failing to systematically investigate the distincteffects of textual and structural perturbations across diverse models andattack scenarios. To address these limitations, we introduce a unified andcomprehensive framework to evaluate robustness in TAG learning. Our frameworkevaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across tendatasets from four domains, under diverse text-based, structure-based, andhybrid perturbations in both poisoning and evasion scenarios. Our extensiveanalysis reveals multiple findings, among which three are particularlynoteworthy: 1) models have inherent robustness trade-offs between text andstructure, 2) the performance of GNNs and RGNNs depends heavily on the textencoder and attack type, and 3) GraphLLMs are particularly vulnerable totraining data corruption. To overcome the identified trade-offs, we introduceSFT-auto, a novel framework that delivers superior and balanced robustnessagainst both textual and structural attacks within a single model. Our workestablishes a foundation for future research on TAG security and offerspractical solutions for robust TAG learning in adversarial environments. Ourcode is available at: https://github.com/Leirunlin/TGRB.</description>
      <author>example@mail.com (Runlin Lei, Lu Yi, Mingguo He, Pengyu Qiu, Zhewei Wei, Yongchao Liu, Chuntao Hong)</author>
      <guid isPermaLink="false">2510.17185v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning-Based Extraction of Promising Material Groups and Common Features from High-Dimensional Data: A Case of Optical Spectra of Inorganic Crystals</title>
      <link>http://arxiv.org/abs/2510.17123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种深度学习模型的解释方法，用于处理材料科学中的高维光谱数据，并通过特征提取和聚类分析对材料进行分类，最终成功应用于光学吸收光谱预测模型的解释。&lt;h4&gt;背景&lt;/h4&gt;材料科学研究中需要处理高维光谱数据，而深度学习模型在处理这类数据时存在解释性挑战。传统方法难以同时考虑光谱数据和化学特性（如元素组成和原子排列）的相似性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理高维光谱数据的深度学习模型解释方法，并根据光谱数据和化学特性的相似性对材料进行分类。&lt;h4&gt;方法&lt;/h4&gt;使用特征提取和聚类分析技术，结合光谱数据和化学特性（元素组成和原子排列）对材料进行分类。将该方法应用于原子线图神经网络(ALIGNN)模型，该模型使用2,681种金属氧化物、硫属化物及相关化合物的第一性原理计算数据进行训练，用于预测光学吸收光谱。&lt;h4&gt;主要发现&lt;/h4&gt;分析揭示了影响光学吸收起始特性的关键元素种类及其配位环境，这些因素对材料的光学特性有重要影响。&lt;h4&gt;结论&lt;/h4&gt;提出的方法适用于各种光谱数据的分类和解释，不仅限于无机晶体的光学吸收光谱，为材料科学研究提供了一种新的分析工具。&lt;h4&gt;翻译&lt;/h4&gt;我们报道了一种深度学习模型的解释方法，使我们能够处理材料科学中的高维光谱数据。所提出的方法使用特征提取和聚类分析，根据光谱数据以及化学特性（如元素组成和原子排列）的相似性将材料分类。作为演示，我们将此方法应用于原子线图神经网络(ALIGNN)模型，该模型使用2,681种金属氧化物、硫属化物及相关化合物的第一性原理计算数据进行训练，用于预测光学吸收光谱。我们的分析揭示了影响光学吸收起始特性的关键元素种类及其配位环境。本文提出的方法适用于各种光谱数据的分类和解释，超越了无机晶体的光学吸收光谱范围。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We report an interpretation method for deep learning models that allows us tohandle high-dimensional spectral data in materials science. The proposed methoduses feature extraction and clustering analysis to categorize materials intoclasses based on similarities in both spectral data and chemicalcharacteristics such as elemental composition and atomic arrangement. As ademonstration, we apply this method to an atomistic line graph neural network(ALIGNN) model trained on first-principles calculation data of 2,681 metaloxides, chalcogenides, and related compounds for optical absorption spectrumprediction. Our analysis reveals key elemental species and their coordinationenvironments that influence optical absorption onset characteristics. Themethod proposed herein is broadly applicable to the classification andinterpretation of diverse spectral data, extending beyond the opticalabsorption spectra of inorganic crystals.</description>
      <author>example@mail.com (Akira Takahashi, Yu Kumagai, Arata Takamatsu, Fumiyasu Oba)</author>
      <guid isPermaLink="false">2510.17123v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains</title>
      <link>http://arxiv.org/abs/2510.16885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UniGTE是一种指令调优的编码器-解码器框架，通过整合图结构与大型语言模型语义，实现了无需任务特定监督的通用图推理能力，在多种图任务上达到最先进的零样本性能。&lt;h4&gt;背景&lt;/h4&gt;传统图神经网络通常与固定标签空间绑定，而大型语言模型难以捕捉图结构，使得在没有任务特定监督的情况下推广到未见过的图任务具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的框架，整合结构和语义推理能力，以解决图神经网络与大型语言模型各自的局限性，实现跨任务和跨域的通用图推理。&lt;h4&gt;方法&lt;/h4&gt;UniGTE采用增强预训练自回归大型语言模型的编码器，通过可学习对齐令牌和结构感知的图-文本注意力机制，使模型能同时处理标记化的图和自然语言任务提示，同时保持节点排列不变性；编码器生成紧凑的任务感知图表示，基于这些表示，冻结的大型语言模型解码器预测任务答案并重新表述输入图，重建目标正则化编码器保留结构线索。&lt;h4&gt;主要发现&lt;/h4&gt;UniGTE在五个涵盖不同领域节点级、边级和图级任务的数据集上进行指令调优后，推理时无需微调，在跨任务和跨域设置下的节点分类、链接预测、图分类和图回归任务上实现了新的最先进零样本结果。&lt;h4&gt;结论&lt;/h4&gt;图结构与大型语言模型语义的紧密集成能够实现强大且可迁移的图推理能力，为无需任务特定监督的通用图学习提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;在没有任务特定监督的情况下推广到未见过的图任务具有挑战性：传统图神经网络通常与固定标签空间绑定，而大型语言模型难以捕捉图结构。我们引入UniGTE，一种统一的指令调优编码器-解码器框架，整合了结构和语义推理能力。编码器通过可学习对齐令牌和结构感知的图-文本注意力机制增强预训练的自回归大型语言模型，使其能够同时处理标记化的图和自然语言任务提示，同时保持对节点排列的不变性。这产生了紧凑的、任务感知的图表示。仅基于这些表示，冻结的大型语言模型解码器进行预测和重建：输出任务答案并同时用自然语言重新表述输入图。重建目标正则化编码器以保留结构线索。UniGTE在五个涵盖不同领域节点级、边级和图级任务的数据集上进行指令调优，但推理时不需要微调。在跨任务和跨域设置下的节点分类、链接预测、图分类和图回归任务上，它实现了新的最先进零样本结果，表明图结构与大型语言模型语义的紧密集成能够实现强大且可迁移的图推理能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalizing to unseen graph tasks without task-specific supervision ischallenging: conventional graph neural networks are typically tied to a fixedlabel space, while large language models (LLMs) struggle to capture graphstructure. We introduce UniGTE, an instruction-tuned encoder-decoder frameworkthat unifies structural and semantic reasoning. The encoder augments apretrained autoregressive LLM with learnable alignment tokens and astructure-aware graph-text attention mechanism, enabling it to attend jointlyto a tokenized graph and a natural-language task prompt while remainingpermutation-invariant to node order. This yields compact, task-aware graphrepresentations. Conditioned solely on these representations, a frozen LLMdecoder predicts and reconstructs: it outputs the task answer andsimultaneously paraphrases the input graph in natural language. Thereconstruction objective regularizes the encoder to preserve structural cues.UniGTE is instruction-tuned on five datasets spanning node-level, edge-level,and graph-level tasks across diverse domains, yet requires no fine-tuning atinference. It achieves new state-of-the-art zero-shot results on nodeclassification, link prediction, graph classification, and graph regressionunder cross-task and cross-domain settings, demonstrating that tightintegration of graph structure with LLM semantics enables robust, transferablegraph reasoning.</description>
      <author>example@mail.com (Duo Wang, Yuan Zuo, Guangyue Lu, Junjie Wu)</author>
      <guid isPermaLink="false">2510.16885v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning Accelerated First-Principles Quantum Transport Simulations at Nonequilibrium State</title>
      <link>http://arxiv.org/abs/2510.16878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeepQT是一种深度学习框架，结合图神经网络和transformer架构，实现了电子结构和输运的多属性预测，无需手动特征工程，同时保持了第一性原理精度并大幅降低计算成本。&lt;h4&gt;背景&lt;/h4&gt;非平衡格林函数方法结合密度泛函理论(NEGF-DFT)为纳米尺度电子输运模拟提供了严格框架，但其计算成本随系统规模急剧增加。现有的人工智能方法在加速此类模拟时存在局限性，包括缺乏原子分辨率、难以外推到更大系统以及无法同时预测多个属性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够加速纳米尺度电子输运模拟的深度学习方法，解决现有AI方法的局限性，实现多属性预测，并能够从小系统外推到更大系统。&lt;h4&gt;方法&lt;/h4&gt;引入DeepQT框架，结合图神经网络和transformer架构，通过学习NEGF-DFT的关键中间量(平衡哈密顿量和非平衡总势差)来重建哈密顿量，利用电子近视原理实现从小训练系统到更大系统的推广。&lt;h4&gt;主要发现&lt;/h4&gt;在石墨烯、MoS2和硅二极管(具有不同缺陷和掺杂剂)的基准测试中，DeepQT达到了第一性原理精度，同时将计算成本降低了几个数量级。&lt;h4&gt;结论&lt;/h4&gt;DeepQT是一个可扩展、可转移的框架，推进了AI辅助的量子输运研究，为下一代纳米电子器件设计提供了强大工具。&lt;h4&gt;翻译&lt;/h4&gt;非平衡格林函数方法结合密度泛函理论(NEGF-DFT)为纳米尺度电子输运模拟提供了严格的框架，但其计算成本随系统规模急剧增加。最近的人工智能方法试图加速此类模拟，但大多数依赖传统机器学习，缺乏原子分辨率，难以外推到更大系统，并且无法同时预测多个属性。在此我们引入DeepQT，一个深度学习框架，集成了图神经网络和transformer架构，实现了电子结构和输运的多属性预测，无需手动特征工程。通过学习NEGF-DFT的关键中间量——平衡哈密顿量和非平衡总势差，DeepQT重建了平衡和偏置条件下的哈密顿量，从而获得准确的输运预测。利用电子近视原理，DeepQT能够从小训练系统高保真地推广到更大系统。在石墨烯、MoS2和硅二极管(具有不同缺陷和掺杂剂)上的基准测试表明，DeepQT实现了第一性原理精度，同时将计算成本降低了几个数量级。这种可扩展、可转移的框架推动了AI辅助的量子输运发展，为下一代纳米电子器件设计提供了强大工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The non-equilibrium Green's function method combined with density functionaltheory (NEGF-DFT) provides a rigorous framework for simulating nanoscaleelectronic transport, but its computational cost scales steeply with systemsize. Recent artificial intelligence (AI) approaches have sought to acceleratesuch simulations, yet most rely on conventional machine learning, lack atomicresolution, struggle to extrapolate to larger systems, and cannot predictmultiple properties simultaneously. Here we introduce DeepQT, a deep-learningframework that integrates graph neural networks with transformer architecturesto enable multi-property predictions of electronic structure and transportwithout manual feature engineering. By learning key intermediate quantities ofNEGF-DFT, the equilibrium Hamiltonian and the non-equilibrium total potentialdifference, DeepQT reconstructs Hamiltonians under both equilibrium and biasconditions, yielding accurate transport predictions. Leveraging the principleof electronic nearsightedness, DeepQT generalizes from small training systemsto much larger ones with high fidelity. Benchmarks on graphene, MoS2, andsilicon diodes with varied defects and dopants show that DeepQT achievesfirst-principles accuracy while reducing computational cost by orders ofmagnitude. This scalable, transferable framework advances AI-assisted quantumtransport, offering a powerful tool for next-generation nanoelectronic devicedesign.</description>
      <author>example@mail.com (Zili Tang, Xiaoxin Xie, Guanwen Yao, Ligong Zhang, Xiaoyan Liu, Xing Zhang, Liu Fei)</author>
      <guid isPermaLink="false">2510.16878v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning</title>
      <link>http://arxiv.org/abs/2510.16824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ProtoMol是一种原型引导的多模态框架，通过双分支层次编码器和层次双向跨模态注意力机制，实现分子图和文本描述之间的细粒度集成和一致语义对齐，解决了现有方法在跨模态交互和原型空间方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;多模态分子表示学习通过联合建模分子图和文本描述，整合结构和语义信息，提高药物毒性、生物活性和理化性质的预测准确性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态方法的两个关键局限性：层次语义依赖被忽略和缺乏统一的原型空间，实现更稳健的跨模态对齐。&lt;h4&gt;方法&lt;/h4&gt;ProtoMol采用双分支层次编码器（图神经网络处理分子图，Transformer编码文本），引入层次双向跨模态注意力机制逐层对齐语义特征，并构建共享原型空间引导模态向一致且具有区分性的表示发展。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上，ProtoMol在各种分子性质预测任务中持续优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;ProtoMol通过细粒度的跨模态集成和一致的语义对齐，有效提升了分子性质预测的准确性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;多模态分子表示学习通过联合建模分子图和它们的文本描述，通过整合结构和语义信息，能够更稳健可靠地预测药物毒性、生物活性和理化性质，从而提高预测准确性和可解释性。然而，现有的多模态方法存在两个关键局限性：(1)它们通常只在最终编码器层执行跨模态交互，从而忽略了层次语义依赖；(2)它们缺乏统一的原型空间来实现模态间的稳健对齐。为了解决这些局限性，我们提出了ProtoMol，一种原型引导的多模态框架，能够实现分子图和文本描述之间的细粒度集成和一致语义对齐。ProtoMol采用双分支层次编码器，利用图神经网络处理结构化分子图，使用Transformer编码非结构化文本，从而生成全面的层次化表示。然后，ProtoMol引入了一种层次双向跨模态注意力机制，逐层对齐跨层的语义特征。此外，还构建了一个共享原型空间，包含可学习的、类别特定的锚点，引导两种模态向连贯且具有区分性的表示发展。在多个基准数据集上的广泛实验表明，在各种分子性质预测任务中，ProtoMol持续优于最先进的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal molecular representation learning, which jointly models moleculargraphs and their textual descriptions, enhances predictive accuracy andinterpretability by enabling more robust and reliable predictions of drugtoxicity, bioactivity, and physicochemical properties through the integrationof structural and semantic information. However, existing multimodal methodssuffer from two key limitations: (1) they typically perform cross-modalinteraction only at the final encoder layer, thus overlooking hierarchicalsemantic dependencies; (2) they lack a unified prototype space for robustalignment between modalities. To address these limitations, we proposeProtoMol, a prototype-guided multimodal framework that enables fine-grainedintegration and consistent semantic alignment between molecular graphs andtextual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,utilizing Graph Neural Networks to process structured molecular graphs andTransformers to encode unstructured texts, resulting in comprehensivelayer-wise representations. Then, ProtoMol introduces a layer-wisebidirectional cross-modal attention mechanism that progressively alignssemantic features across layers. Furthermore, a shared prototype space withlearnable, class-specific anchors is constructed to guide both modalitiestoward coherent and discriminative representations. Extensive experiments onmultiple benchmark datasets demonstrate that ProtoMol consistently outperformsstate-of-the-art baselines across a variety of molecular property predictiontasks.</description>
      <author>example@mail.com (Yingxu Wang, Kunyu Zhang, Jiaxin Huang, Nan Yin, Siwei Liu, Eran Segal)</author>
      <guid isPermaLink="false">2510.16824v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network for Unified Electronic and Interatomic Potentials: Strain-tunable Electronic Structures in 2D Materials</title>
      <link>http://arxiv.org/abs/2510.16605v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了UEIPNet，一种等变图神经网络，用于预测原子结构的原子间势和紧束缚哈密顿量，实现物理上一致的机械-电子响应耦合建模，具有接近DFT的精度。&lt;h4&gt;背景&lt;/h4&gt;在原子结构模拟中，需要能够同时准确描述机械和电子响应的方法，传统DFT计算准确但计算成本高，而经典力场方法无法准确描述电子效应。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时预测原子间势和紧束缚哈密顿量的神经网络模型，实现物理一致的机械-电子响应耦合建模，并具有接近DFT的精度。&lt;h4&gt;方法&lt;/h4&gt;UEIPNet是一种等变图神经网络，使用密度泛函理论计算并结合Wannier投影进行训练，预测节点级别的能量和力作为目标，以及Wannier投影的TB矩阵作为边级别目标，在双层石墨烯和单层MoS2的DFT数据上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;在扭曲双层石墨烯中，UEIPNet揭示了层间间距、面内应变和平面外起伏如何驱动孤立平带的形成，并显示调制基底相互作用强度可以在非魔角处产生平带；对于单层MoS2，UEIPNet准确重现了声子色散、应变相关的带隙演化以及非均匀应变下的局部态密度调制。&lt;h4&gt;结论&lt;/h4&gt;UEIPNet提供了一个通用、高效且可扩展的框架，用于研究大规模原子系统中的变形-电子耦合，桥接了经典原子模拟和电子结构计算。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了UEIPNet，一种等变图神经网络，专为预测原子结构的原子间势和紧束缚哈密顿量而设计。UEIPNet使用密度泛函理论计算结合Wannier投影进行训练，预测能量和力作为节点级别目标，Wannier投影的TB矩阵作为边级别目标。这实现了物理上一致的机械-电子响应耦合建模，具有接近DFT的精度。在双层石墨烯和单层MoS2的DFT数据上训练后，UEIPNet捕捉了关键的变形-电子效应：在扭曲双层石墨烯中，它揭示了层间间距、面内应变和平面外起伏如何驱动孤立平带的形成，并进一步表明调制基底相互作用强度可以在非魔角处产生平带。对于单层MoS2，UEIPNet准确重现了声子色散、应变相关的带隙演化以及非均匀应变下的局部态密度调制。UEIPNet为研究大规模原子系统中的变形-电子耦合提供了一个通用、高效且可扩展的框架，桥接了经典原子模拟和电子结构计算。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce UEIPNet, an equivariant graph neural network designed to predictboth interatomic potentials and tight-binding (TB) Hamiltonians for an atomicstructure. The UEIPNet is trained using density functional theory calculationsfollowed by Wannier projection to predict energies and forces as node-leveltargets and Wannier-projected TB matrices as edge-level targets. This enablesphysically consistent modeling of coupled mechanical electronic responses withnear-DFT accuracy. Trained on bilayer graphene and monolayer MoS2 DFT data,UEIPNet captures key deformation-electronic effects: in twisted bilayergraphene, it reveals how interlayer spacing, in-plane strain, and out-of-planecorrugation drive isolated flat-band formation, and further shows thatmodulating substrate interaction strength can generate flat bands even awayfrom the magic angle. For monolayer MoS2, the UEIPNet accurately reproducesphonon dispersions, strain-dependent band-gap evolution, and local density ofstates modulations under non-uniform strain. The UEIPNet offers a generalized,efficient, and scalable framework for studying deformation-electronic couplingin large-scale atomistic systems, bridging classical atomistic simulations andelectronic-structure calculations.</description>
      <author>example@mail.com (Moon-ki Choi, Daniel Palmer, Harley T. Johnson)</author>
      <guid isPermaLink="false">2510.16605v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations</title>
      <link>http://arxiv.org/abs/2510.16591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了深度学习模型中参数对称性和网络表达能力对泛化行为的影响，特别是在学习实空间重正化群变换时。研究发现对称性约束和表达能力之间存在竞争，过度复杂或受限的模型泛化能力较差。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型通过多层表示学习结构化数据特征非常成功。将物理对称性编码到模型中可提高困难任务性能，且参数对称性破坏和恢复原则被视为其分层学习动力学的统一机制。&lt;h4&gt;目的&lt;/h4&gt;评估参数对称性和网络表达能力在神经网络学习实空间重正化群(RG)变换时对泛化行为的作用，使用中心极限定理(CLT)作为测试案例映射。&lt;h4&gt;方法&lt;/h4&gt;研究简单多层感知器(MLPs)和图神经网络(GNNs)，在不同架构中改变权重对称性和激活函数。通过将CLT重新表述为累积量递归关系并利用既定框架分析MLPs的泛化行为，并验证该框架从MLPs到GNNs的扩展。&lt;h4&gt;主要发现&lt;/h4&gt;对称性约束和表达能力之间存在竞争，过于复杂或过度受限的模型泛化能力较差。分析揭示了这些复杂模型执行的信息处理过程。&lt;h4&gt;结论&lt;/h4&gt;这些发现为对称网络的学习动态提供了新见解，揭示了它们在建模结构化物理转换方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型已被证明在使用多层表示学习结构化数据的相关特征方面极为成功。将这些模型中的物理对称性编码可以提高困难任务上的性能，最近的工作提出了参数对称性破坏和恢复的原则，作为其分层学习动力学的统一机制。我们评估了参数对称性和网络表达能力在神经网络学习实空间重正化群(RG)变换时的泛化行为中的作用，使用中心极限定理(CLT)作为测试案例映射。我们考虑了简单的多层感知器(MLPs)和图神经网络(GNNs)，并在不同架构中改变权重对称性和激活函数。我们的结果表明对称性约束和表达能力之间存在竞争，过于复杂或过度受限的模型泛化能力较差。我们通过将CLT重新表述为累积量递归关系，并利用既定框架通过MLPs传播累积量，分析性地证明了某些受限MLP架构的这种 poor generalisation 行为。我们还经验性地验证了该框架从MLPs到GNNs的扩展，阐明了这些更复杂模型执行的信息处理过程。这些发现为对称网络的学习动态提供了新的见解，以及它们在建模结构化物理转换方面的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models have proven enormously successful at using multiplelayers of representation to learn relevant features of structured data.Encoding physical symmetries into these models can improve performance ondifficult tasks, and recent work has motivated the principle of parametersymmetry breaking and restoration as a unifying mechanism underlying theirhierarchical learning dynamics. We evaluate the role of parameter symmetry andnetwork expressivity in the generalisation behaviour of neural networks whenlearning a real-space renormalisation group (RG) transformation, using thecentral limit theorem (CLT) as a test case map. We consider simple multilayerperceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetriesand activation functions across architectures. Our results reveal a competitionbetween symmetry constraints and expressivity, with overly complex oroverconstrained models generalising poorly. We analytically demonstrate thispoor generalisation behaviour for certain constrained MLP architectures byrecasting the CLT as a cumulant recursion relation and making use of anestablished framework to propagate cumulants through MLPs. We also empiricallyvalidate an extension of this framework from MLPs to GNNs, elucidating theinternal information processing performed by these more complex models. Thesefindings offer new insight into the learning dynamics of symmetric networks andtheir limitations in modelling structured physical transformations.</description>
      <author>example@mail.com (Cassidy Ashworth, Pietro Liò, Francesco Caso)</author>
      <guid isPermaLink="false">2510.16591v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching</title>
      <link>http://arxiv.org/abs/2510.16438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCVW 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种轻量级的点和线段匹配器LightGlueStick，通过注意力线条消息传递(ALMP)实现高效通信，在基准测试中达到最先进水平。&lt;h4&gt;背景&lt;/h4&gt;线条和点是互补的局部特征，在SLAM和运动恢复结构等应用中有效。传统方法将点和线匹配视为独立任务，而GlueStick虽通过联合匹配降低计算复杂度，但架构过于复杂难以实时应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级的点和线段匹配器，适用于实时应用和边缘设备部署。&lt;h4&gt;方法&lt;/h4&gt;提出LightGlueStick，引入注意力线条消息传递(ALMP)组件，明确向网络暴露线条连接性，实现节点间高效通信。&lt;h4&gt;主要发现&lt;/h4&gt;LightGlueStick在不同基准测试中建立了新的最先进水平，同时保持了轻量级架构。&lt;h4&gt;结论&lt;/h4&gt;LightGlueStick实现了高效且轻量的点和线段匹配，适合实时应用和边缘设备部署。&lt;h4&gt;翻译&lt;/h4&gt;线条和点是互补的局部特征，它们的组合已被证明在SLAM和运动恢复结构等应用中有效。这些流程的核心是局部特征匹配器，用于在图像之间建立对应关系。传统上，点和线匹配被视为独立任务。最近，GlueStick提出了一种基于GNN的网络，同时处理点和线以建立匹配。虽然单一联合运行降低了整体计算复杂度，但复杂的架构阻碍了实时应用或边缘设备的部署。受点匹配最新进展的启发，我们提出了LightGlueStick，一种用于点和线段的轻量级匹配器。我们架构中的关键新颖组件是注意力线条消息传递(ALMP)，它明确地向网络暴露线条的连接性，允许节点之间进行高效通信。在彻底的实验中，我们表明LightGlueStick在不同基准测试中建立了新的最先进水平。代码可在https://github.com/aubingazhib/LightGlueStick获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lines and points are complementary local features, whose combination hasproven effective for applications such as SLAM and Structure-from-Motion. Thebackbone of these pipelines are the local feature matchers, establishingcorrespondences across images. Traditionally, point and line matching have beentreated as independent tasks. Recently, GlueStick proposed a GNN-based networkthat simultaneously operates on points and lines to establish matches. Whilerunning a single joint matching reduced the overall computational complexity,the heavy architecture prevented real-time applications or deployment to edgedevices.  Inspired by recent progress in point matching, we propose LightGlueStick, alightweight matcher for points and line segments. The key novel component inour architecture is the Attentional Line Message Passing (ALMP), whichexplicitly exposes the connectivity of the lines to the network, allowing forefficient communication between nodes. In thorough experiments we show thatLightGlueStick establishes a new state-of-the-art across different benchmarks.The code is available at https://github.com/aubingazhib/LightGlueStick.</description>
      <author>example@mail.com (Aidyn Ubingazhibov, Rémi Pautrat, Iago Suárez, Shaohui Liu, Marc Pollefeys, Viktor Larsson)</author>
      <guid isPermaLink="false">2510.16438v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites</title>
      <link>http://arxiv.org/abs/2510.16083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by Elsevier Expert Systems with Applications&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PassREfinder-FL的新框架，用于预测跨网站的凭证填充风险，解决了现有方法在可用性和实际部署方面的问题。&lt;h4&gt;背景&lt;/h4&gt;凭证填充攻击对经常在多个网站重复使用密码的在线用户造成了重大伤害。先前的研究方法虽然试图检测重复使用密码的用户或识别恶意登录尝试，但通常通过限制密码创建或网站访问影响可用性，且依赖复杂的账户共享机制，阻碍了实际部署。&lt;h4&gt;目的&lt;/h4&gt;提出PassREfinder-FL框架，预测跨网站的凭证填充风险，解决现有方法的局限性，同时保护用户隐私并提高可用性。&lt;h4&gt;方法&lt;/h4&gt;引入密码重用关系的概念，将其表示为网站图中的边；使用图神经网络(GNNs)执行链接预测任务，评估网站间的凭证重用风险；整合公共网站信息使方法可扩展到大量网站；采用联邦学习(FL)方法保护用户隐私，避免共享敏感信息。&lt;h4&gt;主要发现&lt;/h4&gt;在包含22,378个网站的3.6亿个泄露账户的真实数据集上评估，PassREfinder-FL在FL设置中实现了0.9153的F1分数；基于FL的GNN比其他最先进的GNN模型性能提升4-11%；预测结果可用于量化密码重用可能性，作为可操作的风险分数。&lt;h4&gt;结论&lt;/h4&gt;PassREfinder-FL是一个有效的框架，能够在保护用户隐私的同时准确预测跨网站的凭证填充风险，其预测结果可作为风险分数使用，帮助量化密码重用风险。&lt;h4&gt;翻译&lt;/h4&gt;凭证填充攻击对经常在多个网站重复使用密码的在线用户造成了重大伤害。虽然先前的研究试图检测有重复使用密码的用户或识别恶意登录尝试，但现有方法通常通过限制密码创建或网站访问来影响可用性，且它们依赖复杂的账户共享机制，阻碍了实际部署。为解决这些局限性，我们提出了PassREfinder-FL，一个预测跨网站凭证填充风险的新框架。我们引入了密码重用关系的概念——定义为用户在网站间重用密码的可能性——并将其表示为网站图中的边。使用图神经网络(GNNs)，我们执行链接预测任务来评估网站之间的凭证重用风险。我们的方法通过整合公共网站信息并将新观察到的网站作为图中的节点链接，可扩展到大量任意网站。为了保护用户隐私，我们使用联邦学习(FL)方法扩展了PassREfinder-FL，消除了跨管理员共享用户敏感信息的需求。在包含22,378个网站的3.6亿个泄露账户的真实世界数据集上的评估显示，PassREfinder-FL在FL设置中实现了0.9153的F1分数。我们进一步通过消融研究验证，基于FL的GNN比其他最先进的GNN模型实现了4-11%的性能提升。最后，我们证明预测结果可用于量化密码重用可能性，作为可操作的风险分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Credential stuffing attacks have caused significant harm to online users whofrequently reuse passwords across multiple websites. While prior research hasattempted to detect users with reused passwords or identify malicious loginattempts, existing methods often compromise usability by restricting passwordcreation or website access, and their reliance on complex account-sharingmechanisms hinders real-world deployment. To address these limitations, wepropose PassREfinder-FL, a novel framework that predicts credential stuffingrisks across websites. We introduce the concept of password reuse relations --defined as the likelihood of users reusing passwords between websites -- andrepresent them as edges in a website graph. Using graph neural networks (GNNs),we perform a link prediction task to assess credential reuse risk betweensites. Our approach scales to a large number of arbitrary websites byincorporating public website information and linking newly observed websites asnodes in the graph. To preserve user privacy, we extend PassREfinder-FL with afederated learning (FL) approach that eliminates the need to share usersensitive information across administrators. Evaluation on a real-world datasetof 360 million breached accounts from 22,378 websites shows thatPassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We furthervalidate that our FL-based GNN achieves a 4-11% performance improvement overother state-of-the-art GNN models through an ablation study. Finally, wedemonstrate that the predicted results can be used to quantify password reuselikelihood as actionable risk scores.</description>
      <author>example@mail.com (Jaehan Kim, Minkyoo Song, Minjae Seo, Youngjin Jin, Seungwon Shin, Jinwoo Kim)</author>
      <guid isPermaLink="false">2510.16083v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions</title>
      <link>http://arxiv.org/abs/2510.16064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于残差学习的电力系统优化方法，通过结合快速直流最优潮流解和图神经网络，解决了交流最优潮流计算效率低下的问题，实现了比传统方法更快的计算速度和更高的准确性。&lt;h4&gt;背景&lt;/h4&gt;解决非线性交流最优潮流(AC OPF)问题是实时电网运行中的主要计算瓶颈，传统AC OPF求解器计算复杂度高，难以满足实时决策需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且可扩展的方法，能够快速提供交流可行的最优潮流解，以支持近实时电网运行决策。&lt;h4&gt;方法&lt;/h4&gt;提出残差学习范式，使用直流最优潮流解作为基线，通过拓扑感知图神经网络结合局部注意力和两级直流特征集成，学习非线性修正项，并采用物理信息损失函数强制执行交流潮流可行性和运行限制。&lt;h4&gt;主要发现&lt;/h4&gt;在57、118和2000总线系统上的评估表明，与传统AC OPF求解器相比，均方误差降低约25%，可行性误差减少高达3倍，运行时间加速高达13倍。模型在N-1 contingencies情况下保持准确性，并能高效扩展到大型网络。&lt;h4&gt;结论&lt;/h4&gt;残差学习是线性近似和交流可行最优潮流之间的实用且可扩展的桥梁，能够实现近实时运行决策。&lt;h4&gt;翻译&lt;/h4&gt;解决非线性交流最优潮流(AC OPF)问题仍然是实时电网运行的主要计算瓶颈。在本文中，我们提出了一种残差学习范式，使用快速的直流最优潮流(DC OPF)解作为基线，并学习仅提供完整AC-OPF解决方案所需的非线性修正。该方法利用了具有局部注意力和两级直流特征集成的拓扑感知图神经网络，使用强制执行交流潮流可行性和运行限制的物理信息损失函数进行训练。在57、118和2000总线系统上的OPFData评估显示，与传统AC OPF求解器相比，均方误差(MSE)降低约25%，可行性误差减少高达3倍，运行时间加速高达13倍。该模型在N-1 contingencies情况下保持准确性，并能高效扩展到大型网络。这些结果表明，残差学习是线性近似和交流可行最优潮流之间实用且可扩展的桥梁，能够实现近实时运行决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solving the nonlinear AC optimal power flow (AC OPF) problem remains a majorcomputational bottleneck for real-time grid operations. In this paper, wepropose a residual learning paradigm that uses fast DC optimal power flow (DCOPF) solutions as a baseline, and learns only the nonlinear correctionsrequired to provide the full AC-OPF solution. The method utilizes atopology-aware Graph Neural Network with local attention and two-level DCfeature integration, trained using a physics-informed loss that enforces ACpower-flow feasibility and operational limits. Evaluations on OPFData for 57-,118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction infeasibility error, and up to 13X runtime speedup compared to conventional ACOPF solvers. The model maintains accuracy under N-1 contingencies and scalesefficiently to large networks. These results demonstrate that residual learningis a practical and scalable bridge between linear approximations andAC-feasible OPF, enabling near real-time operational decision making.</description>
      <author>example@mail.com (Muhy Eddin Za'ter, Bri-Mathias Hodge, Kyri Baker)</author>
      <guid isPermaLink="false">2510.16064v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks</title>
      <link>http://arxiv.org/abs/2510.16063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于变电站级电压估计的分层图神经网络方法，能够处理配电网中常见的低可观测性水平，并在实验中展现出比其他数据驱动模型更优的性能。&lt;h4&gt;背景&lt;/h4&gt;配电网中准确的电压估计对实时监控和提高电网可靠性至关重要。随着分布式能源渗透率和配电层电压变化性的增加，稳健的配电网状态估计(DSSE)对保持安全高效运行更加重要。然而，传统DSSE技术在处理稀疏测量和现代馈线规模方面存在困难，限制了它们在大规模网络中的可扩展性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用电气拓扑和物理特征的分层图神经网络，用于变电站级电压估计，同时对现实配电网中常见的低可观测性水平保持稳健。&lt;h4&gt;方法&lt;/h4&gt;利用公开的SMART-DS数据集，在多个变电站和DER渗透场景的数千个总线上的模型进行训练和评估。提出了一种分层图神经网络方法，该方法能够有效处理配电网的拓扑结构和物理特性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法比其他数据驱动模型的RMSE低达2倍，即使在只有1%的测量覆盖率的情况下，也能保持高精度。&lt;h4&gt;结论&lt;/h4&gt;研究结果突出了图神经网络在实现可扩展、可重现和数据驱动的配电系统电压监控方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;配电网中的准确电压估计对实时监控和提高电网可靠性至关重要。随着分布式能源渗透率和配电层电压变化性的增加，稳健的配电网状态估计(DSSE)对保持安全高效运行变得更加必要。然而，传统的DSSE技术在处理稀疏测量和现代馈线规模方面存在困难，限制了它们在大规模网络中的可扩展性。本文提出了一种用于变电站级电压估计的分层图神经网络，它利用电气拓扑和物理特征，同时对现实配电网中常见的低可观测性水平保持稳健。利用公开的SMART-DS数据集，该模型在多个变电站和DER渗透场景的数千个总线上进行了训练和评估。全面的实验表明，所提出的方法比其他数据驱动模型的RMSE低达2倍，并且在只有1%的测量覆盖率的情况下仍能保持高精度。这些结果突出了图神经网络在实现可扩展、可重现和数据驱动的配电系统电压监控方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate voltage estimation in distribution networks is critical forreal-time monitoring and increasing the reliability of the grid. As DERpenetration and distribution level voltage variability increase, robustdistribution system state estimation (DSSE) has become more essential tomaintain safe and efficient operations. Traditional DSSE techniques, however,struggle with sparse measurements and the scale of modern feeders, limitingtheir scalability to large networks. This paper presents a hierarchical graphneural network for substation-level voltage estimation that exploits bothelectrical topology and physical features, while remaining robust to the lowobservability levels common to real-world distribution networks. Leveraging thepublic SMART-DS datasets, the model is trained and evaluated on thousands ofbuses across multiple substations and DER penetration scenarios. Comprehensiveexperiments demonstrate that the proposed method achieves up to 2 times lowerRMSE than alternative data-driven models, and maintains high accuracy with aslittle as 1\% measurement coverage. The results highlight the potential of GNNsto enable scalable, reproducible, and data-driven voltage monitoring fordistribution systems.</description>
      <author>example@mail.com (Muhy Eddin Za'ter, Bri-Mathias Hodge)</author>
      <guid isPermaLink="false">2510.16063v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph Anomaly Detection and LLMs</title>
      <link>http://arxiv.org/abs/2510.15188v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the authors' extended version of the paper accepted for  publication at the ACM SIGSAC Conference on Computer and Communications  Security (CCS 2025). The final published version is available at  https://doi.org/10.1145/3719027.3765219&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了OCR-APT系统，用于高级持续性威胁(APTs)的检测和攻击故事重建，通过结合图神经网络和大型语言模型，实现了更准确的异常检测和更具可解释性的攻击报告。&lt;h4&gt;背景&lt;/h4&gt;高级持续性威胁(APTs)是一种隐蔽的网络攻击，通常能逃避系统级审计日志中的检测。现有的图异常检测系统存在高误报率和粗粒度警报问题，且依赖于文件路径或IP地址等节点属性，导致虚假关联，降低了检测的稳健性和可靠性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成准确、类人叙述整个攻击的系统，帮助安全分析师完全理解攻击的进展和影响，提供更可靠、更具可解释性的APT检测方案。&lt;h4&gt;方法&lt;/h4&gt;OCR-APT系统采用图神经网络(GNNs)进行子图异常检测，学习节点周围的行为模式而非脆弱的属性。然后使用大型语言模型(LLMs)迭代检测到的子图，重建多阶段攻击故事，并在每个阶段进行验证以减少幻觉并确保可解释的最终报告。&lt;h4&gt;主要发现&lt;/h4&gt;在DARPA TC3、OpTC和NODLINK数据集上的评估表明，OCR-APT在检测准确性和警报可解释性方面优于最先进的系统。OCR-APT能够重建类人报告，全面捕获攻击故事。&lt;h4&gt;结论&lt;/h4&gt;OCR-APT通过结合图神经网络和大型语言模型，解决了现有APT检测系统的局限性，提供了更准确、更可靠且更具可解释性的攻击检测和报告方案。&lt;h4&gt;翻译&lt;/h4&gt;高级持续性威胁(APTs)是一种隐蔽的网络攻击，通常能逃避系统级审计日志中的检测。来源图将这些日志建模为连接的实体和事件，揭示了线性日志表示中遗漏的关系。现有系统对这些图应用异常检测，但常常存在高误报率和粗粒度警报的问题。它们对文件路径或IP等节点属性的依赖导致虚假关联，降低了检测的稳健性和可靠性。为了完全理解攻击的进展和影响，安全分析师需要能够生成整个攻击的准确、类人叙述的系统。为了解决这些挑战，我们介绍了OCR-APT，一个用于APT检测和重建类人攻击故事的系统。OCR-APT使用图神经网络(GNNs)进行子图异常检测，学习节点周围的行为模式而非文件路径或IP等脆弱属性。这种方法带来了更稳健的异常检测。然后使用大型语言模型(LLMs)迭代检测到的子图，重建多阶段攻击故事。每个阶段在继续之前都经过验证，减少了幻觉并确保了可解释的最终报告。我们在DARPA TC3、OpTC和NODLINK数据集上的评估表明，OCR-APT在检测准确性和警报可解释性方面优于最先进的系统。此外，OCR-APT重建的类人报告能够全面捕获攻击故事。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advanced Persistent Threats (APTs) are stealthy cyberattacks that often evadedetection in system-level audit logs. Provenance graphs model these logs asconnected entities and events, revealing relationships that are missed bylinear log representations. Existing systems apply anomaly detection to thesegraphs but often suffer from high false positive rates and coarse-grainedalerts. Their reliance on node attributes like file paths or IPs leads tospurious correlations, reducing detection robustness and reliability. To fullyunderstand an attack's progression and impact, security analysts need systemsthat can generate accurate, human-like narratives of the entire attack. Toaddress these challenges, we introduce OCR-APT, a system for APT detection andreconstruction of human-like attack stories. OCR-APT uses Graph Neural Networks(GNNs) for subgraph anomaly detection, learning behavior patterns around nodesrather than fragile attributes such as file paths or IPs. This approach leadsto a more robust anomaly detection. It then iterates over detected subgraphsusing Large Language Models (LLMs) to reconstruct multi-stage attack stories.Each stage is validated before proceeding, reducing hallucinations and ensuringan interpretable final report. Our evaluations on the DARPA TC3, OpTC, andNODLINK datasets show that OCR-APT outperforms state-of-the-art systems in bothdetection accuracy and alert interpretability. Moreover, OCR-APT reconstructshuman-like reports that comprehensively capture the attack story.</description>
      <author>example@mail.com (Ahmed Aly, Essam Mansour, Amr Youssef)</author>
      <guid isPermaLink="false">2510.15188v2</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction</title>
      <link>http://arxiv.org/abs/2510.16035v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了首个针对基于图神经网络(GNN)的社交机器人检测器的对抗性多智能体强化学习框架(RoBCtrl)，通过扩散模型生成高保真机器人账户和多智能体强化学习优化攻击策略，实验证明该框架能有效削弱基于GNN的检测器性能。&lt;h4&gt;背景&lt;/h4&gt;社交网络已成为个人获取实时信息的关键来源。社交机器人在这些平台上的影响引起了研究者的广泛关注，导致了许多检测技术的发展。然而，这些检测方法的脆弱性和鲁棒性仍未得到充分探索。由于对社交代理的控制有限、机器人检测器的黑盒特性以及机器人的异构性问题，现有的基于图神经网络(GNN)的方法无法直接应用。&lt;h4&gt;目的&lt;/h4&gt;为应对现有GNN-based社交机器人检测方法面临的挑战，提出首个针对基于GNN的社交机器人检测器的对抗性多智能体强化学习框架，用于社交机器人控制攻击(RoBCtrl)。&lt;h4&gt;方法&lt;/h4&gt;使用扩散模型通过微小修改重构现有账户数据来生成高保真机器人账户；采用多智能体强化学习(MARL)方法模拟机器人的对抗行为；根据影响力和预算对社交账户进行分类；使用不同的智能体控制各类别的机器人账户，通过强化学习优化附着策略；设计基于结构熵的分层状态抽象以加速强化学习过程。&lt;h4&gt;主要发现&lt;/h4&gt;据我们所知，这是首次应用扩散模型有效模拟 evolving social bots 的行为；在社交机器人检测数据集上的大量实验表明，该框架可以有效削弱基于GNN的检测器的性能。&lt;h4&gt;结论&lt;/h4&gt;提出的RoBCtrl框架能够有效对抗基于GNN的社交机器人检测器，通过生成高保真机器人账户和智能化的攻击策略实现了这一目标。&lt;h4&gt;翻译&lt;/h4&gt;社交网络已成为个人获取实时信息的关键来源。社交机器人在这些平台上的影响引起了研究者的广泛关注，导致了许多检测技术的发展。然而，这些检测方法的脆弱性和鲁棒性仍未得到充分探索。由于对社交代理的控制有限、机器人检测器的黑盒特性以及机器人的异构性问题，现有的基于图神经网络(GNN)的方法无法直接应用。为应对这些挑战，本文提出了首个针对基于GNN的社交机器人检测器的对抗性多智能体强化学习框架，用于社交机器人控制攻击(RoBCtrl)。具体而言，我们使用扩散模型通过微小修改重构现有账户数据来生成高保真机器人账户，从而逃避社交平台的检测。据我们所知，这是首次应用扩散模型有效模拟 evolving social bots 的行为。随后，我们采用多智能体强化学习(MARL)方法模拟机器人的对抗行为。我们根据影响力和预算对社交账户进行分类，然后使用不同的智能体控制各类别的机器人账户，通过强化学习优化附着策略。此外，我们还设计了一种基于结构熵的分层状态抽象以加速强化学习。在社交机器人检测数据集上的大量实验表明，我们的框架可以有效削弱基于GNN的检测器的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social networks have become a crucial source of real-time information forindividuals. The influence of social bots within these platforms has garneredconsiderable attention from researchers, leading to the development of numerousdetection technologies. However, the vulnerability and robustness of thesedetection methods is still underexplored. Existing Graph Neural Network(GNN)-based methods cannot be directly applied due to the issues of limitedcontrol over social agents, the black-box nature of bot detectors, and theheterogeneity of bots. To address these challenges, this paper proposes thefirst adversarial multi-agent Reinforcement learning framework for social Botcontrol attacks (RoBCtrl) targeting GNN-based social bot detectors.Specifically, we use a diffusion model to generate high-fidelity bot accountsby reconstructing existing account data with minor modifications, therebyevading detection on social platforms. To the best of our knowledge, this isthe first application of diffusion models to mimic the behavior of evolvingsocial bots effectively. We then employ a Multi-Agent Reinforcement Learning(MARL) method to simulate bots adversarial behavior. We categorize socialaccounts based on their influence and budget. Different agents are thenemployed to control bot accounts across various categories, optimizing theattachment strategy through reinforcement learning. Additionally, ahierarchical state abstraction based on structural entropy is designed toaccelerate the reinforcement learning. Extensive experiments on social botdetection datasets demonstrate that our framework can effectively undermine theperformance of GNN-based detectors.</description>
      <author>example@mail.com (Yingguang Yang, Xianghua Zeng, Qi Wu, Hao Peng, Yutong Xia, Hao Liu, Bin Chong, Philip S. Yu)</author>
      <guid isPermaLink="false">2510.16035v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Plasma Shape Control via Zero-shot Generative Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.17531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合生成对抗模仿学习与希尔伯特空间表征学习的新框架，从历史PID控制数据中开发通用的零样本控制策略，用于等离子体形状控制，无需任务特定微调即可在各种场景下精确稳定地跟踪参考轨迹。&lt;h4&gt;背景&lt;/h4&gt;传统PID控制器在等离子体形状控制方面的适应性有限，而特定任务的强化学习方法存在泛化能力不足和需要重复重新训练的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个新框架，从大规模历史PID控制放电数据中开发通用的零样本控制策略，克服传统方法和特定任务强化学习方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;结合生成对抗模仿学习(GAIL)与希尔伯特空间表征学习，实现双重目标：模仿PID数据的稳定操作风格，构建几何结构的潜在空间以实现高效的目标导向控制。&lt;h4&gt;主要发现&lt;/h4&gt;基础策略可以零样本方式部署，无需任务特定的微调；在HL-3托卡马克模拟器上的评估表明，该策略能够精确且稳定地跟踪各种等离子体场景下关键形状参数的参考轨迹。&lt;h4&gt;结论&lt;/h4&gt;这项工作为未来聚变反应堆开发高度灵活和数据高效的智能控制系统提供了可行途径。&lt;h4&gt;翻译&lt;/h4&gt;传统PID控制器在等离子体形状控制方面的适应性有限，而特定任务的强化学习方法存在泛化能力有限和需要重复重新训练的问题。为克服这些挑战，本文提出了一种新框架，用于从大规模历史PID控制放电数据中开发通用的零样本控制策略。我们的方法将生成对抗模仿学习(GAIL)与希尔伯特空间表征学习相结合，以实现双重目标：模仿PID数据的稳定操作风格，构建几何结构的潜在空间以实现高效的目标导向控制。所得基础策略可以零样本方式部署，用于各种轨迹跟踪任务，无需任务特定的微调。在HL-3托卡马克模拟器上的评估表明，该策略在多种等离子体场景下能够精确且稳定地跟踪关键形状参数的参考轨迹。这项工作为未来聚变反应堆开发高度灵活和数据高效的智能控制系统提供了可行途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional PID controllers have limited adaptability for plasma shapecontrol, and task-specific reinforcement learning (RL) methods suffer fromlimited generalization and the need for repetitive retraining. To overcomethese challenges, this paper proposes a novel framework for developing aversatile, zero-shot control policy from a large-scale offline dataset ofhistorical PID-controlled discharges. Our approach synergistically combinesGenerative Adversarial Imitation Learning (GAIL) with Hilbert spacerepresentation learning to achieve dual objectives: mimicking the stableoperational style of the PID data and constructing a geometrically structuredlatent space for efficient, goal-directed control. The resulting foundationpolicy can be deployed for diverse trajectory tracking tasks in a zero-shotmanner without any task-specific fine-tuning. Evaluations on the HL-3 tokamaksimulator demonstrate that the policy excels at precisely and stably trackingreference trajectories for key shape parameters across a range of plasmascenarios. This work presents a viable pathway toward developing highlyflexible and data-efficient intelligent control systems for future fusionreactors.</description>
      <author>example@mail.com (Niannian Wu, Rongpeng Li, Zongyu Yang, Yong Xiao, Ning Wei, Yihang Chen, Bo Li, Zhifeng Zhao, Wulyu Zhong)</author>
      <guid isPermaLink="false">2510.17531v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning</title>
      <link>http://arxiv.org/abs/2510.17489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DETree是一种新型AI参与文本检测方法，通过层次亲和树结构建模不同文本生成过程间的关系，并引入专门损失函数对齐文本表示。研究团队开发了RealBench基准数据集，显著提升了混合文本检测性能和分布外场景的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;检测AI参与的文本对打击错误信息、剽窃和学术不端行为至关重要。AI文本生成涉及多种协作过程（如AI生成文本由人类编辑、人类文本由AI编辑、AI文本由其他AI优化），不同过程生成的文本具有复杂特征，给检测带来巨大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发更有效的AI参与文本检测方法，准确识别不同人-AI协作过程生成的文本，提高检测性能和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出DETree方法，将不同文本生成过程间的关系建模为层次亲和树结构，并引入专门损失函数使文本表示与该树结构对齐。同时开发了RealBench基准数据集，自动整合各种人-AI协作过程产生的混合文本。&lt;h4&gt;主要发现&lt;/h4&gt;不同过程生成的文本表示表现出内在聚类关系；DETree方法在混合文本检测任务中提高了性能；显著增强了在分布外场景中的鲁棒性和泛化能力，特别是在少样本学习条件下。&lt;h4&gt;结论&lt;/h4&gt;基于训练的方法在分布外设置中具有潜力，DETree为AI参与文本检测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;检测AI参与的文本对于打击错误信息、剽窃和学术不端行为至关重要。然而，AI文本生成包括多样的协作过程（AI生成文本由人类编辑、人类编写文本由AI编辑、AI生成文本由其他AI优化），其中可能涉及各种甚至新的LLM。这些不同过程生成的文本表现出复杂特征，给检测带来巨大挑战。当前方法对这些过程建模过于简单，主要采用二元分类（纯人类vs AI参与）或多分类（将人-AI协作视为新类别）。我们观察到，通过不同过程生成的文本表示表现出内在的聚类关系。因此，我们提出了DETree，一种新方法，将不同过程之间的关系建模为层次亲和树结构，并引入专门的损失函数使文本表示与此树对齐。为此，我们开发了RealBench，一个全面的基准数据集，自动整合通过各种人-AI协作过程产生的混合文本。我们的方法在混合文本检测任务中提高了性能，显著增强了在分布外场景中的鲁棒性和泛化能力，特别是在少样本学习条件下，进一步证明了基于训练的方法在OOD设置中的潜力。我们的代码和数据集可在https://github.com/heyongxin233/DETree获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting AI-involved text is essential for combating misinformation,plagiarism, and academic misconduct. However, AI text generation includesdiverse collaborative processes (AI-written text edited by humans,human-written text edited by AI, and AI-generated text refined by other AI),where various or even new LLMs could be involved. Texts generated through thesevaried processes exhibit complex characteristics, presenting significantchallenges for detection. Current methods model these processes rather crudely,primarily employing binary classification (purely human vs. AI-involved) ormulti-classification (treating human-AI collaboration as a new class). Weobserve that representations of texts generated through different processesexhibit inherent clustering relationships. Therefore, we propose DETree, anovel approach that models the relationships among different processes as aHierarchical Affinity Tree structure, and introduces a specialized lossfunction that aligns text representations with this tree. To facilitate thislearning, we developed RealBench, a comprehensive benchmark dataset thatautomatically incorporates a wide spectrum of hybrid texts produced throughvarious human-AI collaboration processes. Our method improves performance inhybrid text detection tasks and significantly enhances robustness andgeneralization in out-of-distribution scenarios, particularly in few-shotlearning conditions, further demonstrating the promise of training-basedapproaches in OOD settings. Our code and dataset are available athttps://github.com/heyongxin233/DETree.</description>
      <author>example@mail.com (Yongxin He, Shan Zhang, Yixuan Cao, Lei Ma, Ping Luo)</author>
      <guid isPermaLink="false">2510.17489v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition</title>
      <link>http://arxiv.org/abs/2510.17338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种后处理开放集识别方法，通过测量模型特征与预测logit之间的一致性来识别未知类别，无需重新训练预训练模型，在两个数据集上都取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;当前最先进的野生动物分类模型在封闭世界设置下训练，遇到未知类别时仍过于自信预测。&lt;h4&gt;目的&lt;/h4&gt;开发一种开放集识别方法，能够分类已知类别同时拒绝未知样本，无需重新训练预训练模型。&lt;h4&gt;方法&lt;/h4&gt;提出基于输入到最近类别均值(NCM)距离的概率分布，并与logit空间的softmax概率比较，测量NCM与分类头之间的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在两个评估数据集上排名前三，性能一致；在非洲和瑞典动物数据集上分别实现了93.41和95.35的AUROC。&lt;h4&gt;结论&lt;/h4&gt;该方法作为后处理技术应用，无需重新训练预训练模型，在开放集识别任务中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;当前最先进的野生动物分类模型是在封闭世界设置下训练的。当遇到未知类别时，它们对自己的预测仍然过于自信。开放集识别(OSR)旨在分类已知类别同时拒绝未知样本。本研究提出了一种后处理OSR方法，用于测量模型特征和预测logit之间的一致性。我们提出了一种基于输入到其最近类别均值(NCM)距离的概率分布。然后将基于NCM的分布与logit空间中的softmax概率进行比较，以测量NCM和分类头之间的一致性。所提出的策略在两个评估数据集中排名前三，且在两个数据集上表现一致。相比之下，当前最先进的方法在单个数据集上表现出色。我们在非洲和瑞典动物数据集上分别实现了93.41和95.35的AUROC。代码可在https://github.com/Applied-Representation-Learning-Lab/OSR找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current state-of-the-art Wildlife classification models are trained under theclosed world setting. When exposed to unknown classes, they remainoverconfident in their predictions. Open-set Recognition (OSR) aims to classifyknown classes while rejecting unknown samples. Several OSR methods have beenproposed to model the closed-set distribution by observing the feature, logit,or softmax probability space. A significant drawback of many existingapproaches is the requirement to retrain the pre-trained classification modelwith the OSR-specific strategy. This study contributes a post-processing OSRmethod that measures the agreement between the models' features and predictedlogits. We propose a probability distribution based on an input's distance toits Nearest Class Mean (NCM). The NCM-based distribution is then compared withthe softmax probabilities from the logit space to measure agreement between theNCM and the classification head. Our proposed strategy ranks within the topthree on two evaluated datasets, showing consistent performance across the twodatasets. In contrast, current state-of-the-art methods excel on a singledataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedishanimals. The code can be foundhttps://github.com/Applied-Representation-Learning-Lab/OSR.</description>
      <author>example@mail.com (Jiahao Huo, Mufhumudzi Muthivhi, Terence L. van Zyl, Fredrik Gustafsson)</author>
      <guid isPermaLink="false">2510.17338v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery</title>
      <link>http://arxiv.org/abs/2510.17188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accpeted at NeurIPS (2025) Main Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HIDISC是一种双曲表示学习框架，用于解决领域泛化广义类别发现问题，无需片段模拟即可实现领域和类别级别的泛化。&lt;h4&gt;背景&lt;/h4&gt;广义类别发现(GCD)旨在对测试样本进行分类，分为训练期间可见的类别或新类别，无需标签监督。现有GCD方法假设训练期间可以同时访问有标签和无标签数据，且来自同一领域，限制了其在涉及分布偏移的开放世界场景中的应用。DG-GCD要求模型泛化到包含新类别的未见领域，且在训练期间不访问目标领域数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的方法，使模型能够泛化到未见领域中的新类别，同时避免现有方法的高计算成本和错误累积问题。&lt;h4&gt;方法&lt;/h4&gt;HIDISC框架使用GPT引导的扩散增强源领域，引入Tangent CutMix在切线空间中合成伪新样本，采用统一损失函数(惩罚Busemann对齐、混合双曲对比正则化和自适应异常值排斥)，并使用可学习的曲率参数使几何结构适应数据集复杂性。&lt;h4&gt;主要发现&lt;/h4&gt;HIDISC在PACS、Office-Home和DomainNet数据集上实现了最先进的结果，一致性地优于现有的欧几里得和双曲(DG)-GCD基线。&lt;h4&gt;结论&lt;/h4&gt;HIDISC通过双曲表示学习框架有效解决了领域泛化广义类别发现问题，无需片段模拟即可实现领域和类别级别的泛化，同时保持了高效率。&lt;h4&gt;翻译&lt;/h4&gt;广义类别发现(GCD)旨在对测试样本进行分类，分为训练期间可见的类别或新类别，无需依赖标签监督。大多数现有GCD方法假设在训练期间可以同时访问有标签和无标签数据，并且来自同一领域，这限制了其在涉及分布偏移的开放世界场景中的应用。带有GCD的领域泛化(DG-GCD)通过要求模型泛化到包含新类别的未见领域，且在训练期间不访问目标领域数据，从而消除了这一限制。唯一的现有DG-GCD方法DG2CD-Net依赖于多合成领域的片段训练和任务向量聚合，导致高计算成本和错误累积。我们提出了HIDISC，一种双曲表示学习框架，无需片段模拟即可实现领域和类别级别的泛化。为了使模型接触到最小但多样化的领域变化，我们使用GPT引导的扩散增强源领域，避免过拟合并保持效率。为了构建表示空间，我们引入了Tangent CutMix，这是一种曲率感知的插值方法，在切线空间中合成伪新样本，保持流形一致性。统一的损失函数(结合惩罚Busemann对齐、混合双曲对比正则化和自适应异常值排斥)促进了紧凑、语义结构化的嵌入。可学习的曲率参数进一步使几何结构适应数据集的复杂性。HIDISC在PACS、Office-Home和DomainNet上实现了最先进的结果，一致性地优于现有的欧几里得和双曲(DG)-GCD基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized Category Discovery (GCD) aims to classify test-time samples intoeither seen categories** -- available during training -- or novel ones, withoutrelying on label supervision. Most existing GCD methods assume simultaneousaccess to labeled and unlabeled data during training and arising from the samedomain, limiting applicability in open-world scenarios involving distributionshifts. Domain Generalization with GCD (DG-GCD) lifts this constraint byrequiring models to generalize to unseen domains containing novel categories,without accessing targetdomain data during training. The only prior DG-GCDmethod, DG2CD-Net, relies on episodic training with multiple synthetic domainsand task vector aggregation, incurring high computational cost and erroraccumulation. We propose HIDISC, a hyperbolic representation learning frameworkthat achieves domain and category-level generalization without episodicsimulation. To expose the model to minimal but diverse domain variations, weaugment the source domain using GPT-guided diffusion, avoiding overfittingwhile maintaining efficiency. To structure the representation space, weintroduce Tangent CutMix, a curvature-aware interpolation that synthesizespseudo-novel samples in tangent space, preserving manifold consistency. Aunified loss -- combining penalized Busemann alignment, hybrid hyperboliccontrastive regularization, and adaptive outlier repulsion -- **facilitatescompact, semantically structured embeddings. A learnable curvature parameterfurther adapts the geometry to dataset complexity. HIDISC achievesstate-of-the-art results on PACS , Office-Home , and DomainNet, consistentlyoutperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.</description>
      <author>example@mail.com (Vaibhav Rathore, Divyam Gupta, Biplab Banerjee)</author>
      <guid isPermaLink="false">2510.17188v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>DFNN: A Deep Fréchet Neural Network Framework for Learning Metric-Space-Valued Responses</title>
      <link>http://arxiv.org/abs/2510.17072v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了深度Fréchet神经网络（DFNNs）框架，用于从欧几里得预测变量预测非欧几里得响应变量，并通过理论证明和实证研究展示了其优越性。&lt;h4&gt;背景&lt;/h4&gt;非欧几里得响应变量（如概率分布、网络、对称正定矩阵和组合）的回归在现代应用中变得越来越重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种端到端的深度学习框架，用于预测被视为度量空间中随机对象的非欧几里得响应变量。&lt;h4&gt;方法&lt;/h4&gt;深度Fréchet神经网络（DFNNs），利用深度神经网络的表示学习能力来近似给定预测变量的响应的条件Fréchet均值，通过最小化Fréchet风险实现。&lt;h4&gt;主要发现&lt;/h4&gt;建立了DFNNs的通用近似定理，将神经网络近似理论推进到一般度量空间值响应，无需模型假设或局部平滑；在多种应用场景中，DFNNs始终优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;DFNNs是一种高度灵活的框架，能够适应不同的度量和高维预测变量，为非欧几里得响应变量的回归提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;回归非欧几里得响应变量——例如概率分布、网络、对称正定矩阵和组合——在现代应用中已变得越来越重要。在本文中，我们提出了深度Fréchet神经网络（DFNNs），一个用于从欧几里得预测变量预测非欧几里得响应变量的端到端深度学习框架——这些响应被视为度量空间中的随机对象。我们的方法利用深度神经网络的表示学习能力，通过最小化Fréchet风险来近似给定预测变量的响应的条件Fréchet均值——这是条件期望的度量空间类比。该框架非常灵活，能够适应不同的度量和高维预测变量。我们建立了DFNNs的通用近似定理，将神经网络近似理论的最先进水平推进到一般度量空间值响应，无需做出模型假设或依赖局部平滑。在合成分布和网络值响应以及预测就业职业构成的真实世界应用中的实证研究表明，DFNNs始终优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Regression with non-Euclidean responses -- e.g., probability distributions,networks, symmetric positive-definite matrices, and compositions -- has becomeincreasingly important in modern applications. In this paper, we propose deepFr\'echet neural networks (DFNNs), an end-to-end deep learning framework forpredicting non-Euclidean responses -- which are considered as random objects ina metric space -- from Euclidean predictors. Our method leverages therepresentation-learning power of deep neural networks (DNNs) to the task ofapproximating conditional Fr\'echet means of the response given the predictors,the metric-space analogue of conditional expectations, by minimizing aFr\'echet risk. The framework is highly flexible, accommodating diverse metricsand high-dimensional predictors. We establish a universal approximation theoremfor DFNNs, advancing the state-of-the-art of neural network approximationtheory to general metric-space-valued responses without making modelassumptions or relying on local smoothing. Empirical studies on syntheticdistributional and network-valued responses, as well as a real-worldapplication to predicting employment occupational compositions, demonstratethat DFNNs consistently outperform existing methods.</description>
      <author>example@mail.com (Kyum Kim, Yaqing Chen, Paromita Dubey)</author>
      <guid isPermaLink="false">2510.17072v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability</title>
      <link>http://arxiv.org/abs/2510.17040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了Diverse Influence Component Analysis (DICA)框架，利用混合函数Jacobian的凸几何特性，通过Jacobian Volume Maximization (J-VolMax)准则实现潜在成分识别，无需依赖辅助信息、潜在成分独立性或Jacobian稀疏假设。&lt;h4&gt;背景&lt;/h4&gt;从未知非线性混合中识别潜在成分是机器学习的基础挑战，应用于解纠缠表示学习和因果推断等领域。先前工作表明辅助信号可支持条件独立潜在成分的可识别性，而更新的方法则通过结构假设（如混合函数Jacobian的稀疏性）来放宽要求。&lt;h4&gt;目的&lt;/h4&gt;引入DICA框架，利用混合函数Jacobian的凸几何特性，开发一种新的潜在成分识别方法。&lt;h4&gt;方法&lt;/h4&gt;提出Jacobian Volume Maximization (J-VolMax)准则，通过鼓励潜在成分对观察变量的影响多样性来实现潜在成分识别。&lt;h4&gt;主要发现&lt;/h4&gt;在合理条件下，DICA方法无需依赖辅助信息、潜在成分独立性或Jacobian稀疏假设即可实现潜在成分的可识别性。&lt;h4&gt;结论&lt;/h4&gt;这些结果扩展了可识别性分析的范围，为现有方法提供了互补的视角。&lt;h4&gt;翻译&lt;/h4&gt;从未知非线性混合中识别潜在成分是机器学习中的一个基础性挑战，应用于解纠缠表示学习和因果推断等任务。先前在非线性独立成分分析方面的工作表明，辅助信号（如弱监督）可以支持条件独立潜在成分的可识别性。更新的方法探索结构假设，例如混合函数的Jacobian稀疏性，以放宽这些要求。在这项工作中，我们引入了Diverse Influence Component Analysis (DICA)框架，利用混合函数Jacobian的凸几何特性。我们提出了Jacobian Volume Maximization (J-VolMax)准则，通过鼓励潜在成分对观察变量的影响多样性来实现潜在成分识别。在合理条件下，这种方法无需依赖辅助信息、潜在成分独立性或Jacobian稀疏假设即可实现可识别性。这些结果扩展了可识别性分析的范围，为现有方法提供了互补的视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Latent component identification from unknown nonlinear mixtures is afoundational challenge in machine learning, with applications in tasks such asdisentangled representation learning and causal inference. Prior work innonlinear independent component analysis (nICA) has shown that auxiliarysignals -- such as weak supervision -- can support identifiability ofconditionally independent latent components. More recent approaches explorestructural assumptions, e.g., sparsity in the Jacobian of the mixing function,to relax such requirements. In this work, we introduce Diverse InfluenceComponent Analysis (DICA), a framework that exploits the convex geometry of themixing function's Jacobian. We propose a Jacobian Volume Maximization(J-VolMax) criterion, which enables latent component identification byencouraging diversity in their influence on the observed variables. Underreasonable conditions, this approach achieves identifiability without relyingon auxiliary information, latent component independence, or Jacobian sparsityassumptions. These results extend the scope of identifiability analysis andoffer a complementary perspective to existing methods.</description>
      <author>example@mail.com (Hoang-Son Nguyen, Xiao Fu)</author>
      <guid isPermaLink="false">2510.17040v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams</title>
      <link>http://arxiv.org/abs/2510.16988v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CARE框架，通过序列-图像对比对齐方法解决了日常生活活动识别中的表征局限性，实现了高性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;从事件触发式环境传感器识别日常生活活动(ADLs)是环境辅助生活的关键任务，但现有方法存在表征层面的局限性。基于序列的方法对噪声敏感且缺乏空间感知，而基于图像的方法压缩了时间动态并扭曲了传感器布局。简单融合无法充分利用这两种方法的互补优势。&lt;h4&gt;目的&lt;/h4&gt;开发一个端到端框架，通过联合优化表征学习和分类，确保跨表征对齐和任务特定判别性，从而提高ADL识别的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出CARE(从事件触发式传感器流进行ADL识别的对比对齐)框架，集成时间感知、噪声鲁棒的序列编码与空间感知、频率敏感的图像表征，并采用联合对比-分类目标进行端到端学习。&lt;h4&gt;主要发现&lt;/h4&gt;在三个CASAS数据集上评估，CARE实现了最先进的性能：Milan上89.8%，Cairo上88.9%，Kyoto7上73.3%。同时，该方法展示了对传感器故障和布局变化的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;CARE框架在智能家居环境中可靠的ADL识别具有显著潜力，其性能和鲁棒性证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;从事件触发式环境传感器识别日常生活活动(ADLs)是环境辅助生活(AAL)中的关键任务，然而现有方法仍受表征层面限制。基于序列的方法保留了传感器激活的时间顺序，但对噪声敏感且缺乏空间感知，而基于图像的方法捕捉全局模式和隐含的空间相关性，但压缩了细粒度时间动态并扭曲了传感器布局。简单融合(如特征连接)无法强制序列和图像表征视图之间的对齐，未能充分利用它们的互补优势。我们提出了CARE(从事件触发式传感器流进行ADL识别的对比对齐)，一个通过序列-图像对比对齐(SICA)和交叉熵联合优化表征学习的端到端框架，确保跨表征对齐和任务特定判别性。CARE集成(i)时间感知、噪声鲁棒的序列编码，(ii)空间感知和频率敏感的图像表征，并采用(iii)联合对比-分类目标进行对齐且具有判别性的嵌入的端到端学习。在三个CASAS数据集上评估，CARE实现了最先进的性能(Milan上89.8%，Cairo上88.9%，Kyoto7上73.3%)，并展示了对传感器故障和布局变化的鲁棒性，突显了其在智能家居中可靠ADL识别的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recognition of Activities of Daily Living (ADLs) from event-triggeredambient sensors is an essential task in Ambient Assisted Living, yet existingmethods remain constrained by representation-level limitations. Sequence-basedapproaches preserve temporal order of sensor activations but are sensitive tonoise and lack spatial awareness, while image-based approaches capture globalpatterns and implicit spatial correlations but compress fine-grained temporaldynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)fail to enforce alignment between sequence- and image-based representationviews, underutilizing their complementary strengths. We propose ContrastiveAlignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), anend-to-end framework that jointly optimizes representation learning viaSequence-Image Contrastive Alignment (SICA) and classification viacross-entropy, ensuring both cross-representation alignment and task-specificdiscriminability. CARE integrates (i) time-aware, noise-resilient sequenceencoding with (ii) spatially-informed and frequency-sensitive imagerepresentations, and employs (iii) a joint contrastive-classification objectivefor end-to-end learning of aligned and discriminative embeddings. Evaluated onthree CASAS datasets, CARE achieves state-of-the-art performance (89.8% onMilan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness tosensor malfunctions and layout variability, highlighting its potential forreliable ADL recognition in smart homes.</description>
      <author>example@mail.com (Junhao Zhao, Zishuai Liu, Ruili Fang, Jin Lu, Linghan Zhang, Fei Dou)</author>
      <guid isPermaLink="false">2510.16988v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Domain Generalizable Continual Learning</title>
      <link>http://arxiv.org/abs/2510.16914v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为域可推广持续学习(DGCL)的新设置，以及自适应域变换(DoT)方法来解决智能系统在动态环境中学习新技能并推广到多样化场景的挑战。&lt;h4&gt;背景&lt;/h4&gt;智能系统需要不断获取新技能并将其推广到多样化、未见过的场景。现有持续学习方法假设每个任务的训练和测试域相同，在域变化场景下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出DGCL设置，使模型能够学习序列任务，每个任务涉及单一域，目标是模型在所有遇到的任务和域中表现良好。解决获取、保留和利用语义及域相关信息的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出自适应域变换(DoT)方法，基于预训练模型，受人类大脑分布式加枢纽理论启发，在表示学习中解耦语义和域相关信息，自适应转换跨域任务表示以实现输出对齐，确保平衡和泛化的预测。&lt;h4&gt;主要发现&lt;/h4&gt;DoT作为即插即用策略显著提升了最先进CL基线在DGCL下的性能，能够积累域可推广知识，具有轻量级实现确保资源效率，在全参数调整和参数高效调整范式下均有效。&lt;h4&gt;结论&lt;/h4&gt;DoT解决了DGCL中的独特挑战，通过解耦语义和域相关信息实现更好的泛化能力，使智能系统能够有效适应动态现实环境。&lt;h4&gt;翻译&lt;/h4&gt;为了有效适应动态现实环境，智能系统必须不断获取新技能，同时将其推广到多样化、未见过的场景。在此，我们引入一种名为域可推广持续学习(DGCL)的新颖且现实的设置：模型学习序列任务，每个任务涉及单一域，旨在在所有遇到的任务和域中表现良好。这种设置在获取、保留和利用语义和域相关信息以实现稳健泛化方面提出了独特挑战。尽管最先进的持续学习方法采用预训练模型来增强任务特定泛化，但它们通常假设每个任务的训练和测试域相同，因此在DGCL中表现不佳。为此，我们提出了自适应域变换(DoT)，这是一种专为DGCL设计的创新预训练模型方法。受人类大脑分布式加枢纽理论的启发，DoT在表示学习中解耦语义和域相关信息，并自适应地跨不同域转换任务表示以实现输出对齐，确保平衡和泛化的预测。DoT作为一种即插即用策略，在DGCL下极大地促进了最先进CL基线在全参数调整和参数高效调整范式中的性能，并通过大量实验得到验证。此外，DoT被证明能够从DGCL中积累域可推广知识，并通过轻量级实现确保资源效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To adapt effectively to dynamic real-world environments, intelligent systemsmust continually acquire new skills while generalizing them to diverse, unseenscenarios. Here, we introduce a novel and realistic setting named domaingeneralizable continual learning (DGCL): a model learns sequential tasks witheach involving a single domain, aiming to perform well across all encounteredtasks and domains. This setting poses unique challenges in acquiring,retaining, and leveraging both semantic- and domain-relevant information forrobust generalization. Although state-of-the-art continual learning (CL)methods have employed pre-trained models (PTMs) to enhance task-specificgeneralization, they typically assume identical training and testing domainsfor each task and therefore perform poorly in DGCL. To this end, we proposeadaptive Domain Transformation (DoT), an innovative PTMs-based approachtailored to DGCL. Inspired by the distributed-plus-hub theory of the humanbrain, DoT disentangles semantic- and domain-relevant information inrepresentation learning, and adaptively transforms task representations acrossvarious domains for output alignment, ensuring balanced and generalizedpredictions. DoT serves as a plug-in strategy that greatly facilitatesstate-of-the-art CL baselines under both full parameter tuning andparameter-efficient tuning paradigms in DGCL, validated by extensiveexperiments. Also, DoT is shown to accumulate domain-generalizable knowledgefrom DGCL, and ensure resource efficiency with a lightweight implementation.</description>
      <author>example@mail.com (Hongwei Yan, Guanglong Sun, Zhiqi Kang, Yi Zhong, Liyuan Wang)</author>
      <guid isPermaLink="false">2510.16914v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning</title>
      <link>http://arxiv.org/abs/2510.16877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Fly-CL是一种受果蝇嗅觉回路启发的生物启发框架，用于持续表征学习，解决了直接利用预训练特征时的多重共线性问题，同时显著减少了训练时间，性能达到或超过当前最先进方法。&lt;h4&gt;背景&lt;/h4&gt;持续表征学习范式将参数更新重新构建为相似度匹配问题以减轻灾难性遗忘，但直接利用预训练特征进行下游任务通常在相似度匹配阶段存在多重共线性问题，且更高级的方法可能对实时、低延迟应用计算成本过高。&lt;h4&gt;目的&lt;/h4&gt;解决直接利用预训练特征进行下游任务时存在的多重共线性问题，并提出一种计算效率高的方法，适用于实时、低延迟应用。&lt;h4&gt;方法&lt;/h4&gt;受果蝇嗅觉回路的启发，提出了Fly-CL框架，与各种预训练骨干网络兼容。从理论上展示了Fly-CL如何逐步解决多重共线性问题，实现更有效的相似度匹配，同时具有低时间复杂度。&lt;h4&gt;主要发现&lt;/h4&gt;Fly-CL显著减少了训练时间，同时实现了与当前最先进方法相当或更好的性能。通过生物启发设计有效解决了多重共线性挑战。&lt;h4&gt;结论&lt;/h4&gt;Fly-CL是一种生物启发框架，与多种预训练骨干网络兼容。在不同网络架构和数据集上的广泛模拟实验验证了其有效性。代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;使用几乎冻结的预训练模型，持续表征学习范式将参数更新重新构建为相似度匹配问题，以减轻灾难性遗忘。然而，直接利用预训练特征进行下游任务通常在相似度匹配阶段存在多重共线性问题，更高级的方法可能对实时、低延迟应用来说计算成本过高。受果蝇嗅觉回路的启发，我们提出了Fly-CL，这是一种与多种预训练骨干网络兼容的生物启发框架。Fly-CL显著减少了训练时间，同时实现了与当前最先进方法相当或更好的性能。我们从理论上展示了Fly-CL如何逐步解决多重共线性问题，实现更有效的相似度匹配，同时具有低时间复杂度。在各种网络架构和数据集上的广泛模拟实验验证了Fly-CL通过生物启发设计解决这一挑战的有效性。代码可在https://github.com/gfyddha/Fly-CL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Using a nearly-frozen pretrained model, the continual representation learningparadigm reframes parameter updates as a similarity-matching problem tomitigate catastrophic forgetting. However, directly leveraging pretrainedfeatures for downstream tasks often suffers from multicollinearity in thesimilarity-matching stage, and more advanced methods can be computationallyprohibitive for real-time, low-latency applications. Inspired by the flyolfactory circuit, we propose Fly-CL, a bio-inspired framework compatible witha wide range of pretrained backbones. Fly-CL substantially reduces trainingtime while achieving performance comparable to or exceeding that of currentstate-of-the-art methods. We theoretically show how Fly-CL progressivelyresolves multicollinearity, enabling more effective similarity matching withlow time complexity. Extensive simulation experiments across diverse networkarchitectures and data regimes validate Fly-CL's effectiveness in addressingthis challenge through a biologically inspired design. Code is available athttps://github.com/gfyddha/Fly-CL.</description>
      <author>example@mail.com (Heming Zou, Yunliang Zang, Wutong Xu, Xiangyang Ji)</author>
      <guid isPermaLink="false">2510.16877v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding</title>
      <link>http://arxiv.org/abs/2510.16780v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为3D-GSRD的新型3D分子图自编码器，通过选择性重新掩码解码技术解决了将2D MGM成功扩展到3D MGM时面临的两个相互冲突的挑战。&lt;h4&gt;背景&lt;/h4&gt;掩码图模型(MGM)是分子表示学习(MRL)的一种有前景的方法，但将2D重新掩码解码的成功经验扩展到3D MGM面临两个相互冲突的挑战：避免2D结构信息泄漏到解码器，同时为重新掩码的原子重构提供足够的2D上下文。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理3D分子数据并解决2D结构信息泄漏与上下文提供之间矛盾的分子表示学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出3D-GSRD，其核心创新是选择性重新掩码解码(SRD)，该技术仅从编码器表示中重新掩码3D相关信息，同时保留2D图结构。SRD与3D关系转换器(3D-ReTrans)编码器和结构无关的解码器协同集成。&lt;h4&gt;主要发现&lt;/h4&gt;SRD与结构无关的解码器增强了编码器在分子表示学习中的作用。在MD17分子性质预测基准测试中，3D-GSRD在8个目标中的7个上达到了最新的最优性能。&lt;h4&gt;结论&lt;/h4&gt;3D-GSRD成功解决了将2D MGM扩展到3D MGM时面临的挑战，为分子表示学习提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;掩码图建模(MGM)是分子表示学习(MRL)的一种有前景的方法。然而，将重新掩码解码的成功从2D扩展到3D MGM并非易事，主要由于两个相互冲突的挑战：避免将2D结构信息泄漏到解码器，同时仍为重新掩码的原子重构提供足够的2D上下文。为解决这些挑战，我们提出了3D-GSRD：一种具有选择性重新掩码解码的3D分子图自编码器。3D-GSRD的核心创新在于其选择性重新掩码解码(SRD)，它仅从编码器表示中重新掩码3D相关信息，同时保留2D图结构。SRD与3D关系转换器(3D-ReTrans)编码器和结构无关的解码器协同集成。我们分析指出，SRD结合结构无关的解码器增强了编码器在MRL中的作用。大量实验表明，3D-GSRD实现了强大的下游性能，在广泛使用的MD17分子性质预测基准的8个目标中，有7个达到了最新的最优状态。代码已发布在https://github.com/WuChang0124/3D-GSRD。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked graph modeling (MGM) is a promising approach for molecularrepresentation learning (MRL).However, extending the success of re-maskdecoding from 2D to 3D MGM is non-trivial, primarily due to two conflictingchallenges: avoiding 2D structure leakage to the decoder, while still providingsufficient 2D context for reconstructing re-masked atoms.To address thesechallenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder withSelective Re-mask Decoding. The core innovation of 3D-GSRD lies in itsSelective Re-mask Decoding(SRD), which re-masks only 3D-relevant informationfrom encoder representations while preserving the 2D graph structures.This SRDis synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)encoder alongside a structure-independent decoder. We analyze that SRD,combined with the structure-independent decoder, enhances the encoder's role inMRL. Extensive experiments show that 3D-GSRD achieves strong downstreamperformance, setting a new state-of-the-art on 7 out of 8 targets in the widelyused MD17 molecular property prediction benchmark. The code is released athttps://github.com/WuChang0124/3D-GSRD.</description>
      <author>example@mail.com (Chang Wu, Zhiyuan Liu, Wen Shu, Liang Wang, Yanchen Luo, Wenqiang Lei, Yatao Bian, Junfeng Fang, Xiang Wang)</author>
      <guid isPermaLink="false">2510.16780v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning</title>
      <link>http://arxiv.org/abs/2510.16474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自适应核注意力机制的新预测建模方法，解决了传统方法在高维、异构数据处理中的局限性，实验证明该方法优于现有技术。&lt;h4&gt;背景&lt;/h4&gt;高维、异构数据及其复杂特征交互对传统预测建模方法构成挑战。传统方法如投影到潜在结构(PLS)难以建模复杂非线性关系，特别是在具有高维相关结构的多变量系统中。多尺度上的同时交互使局部处理无法捕获跨组依赖，而静态特征加权限制了模型对上下文变化的适应性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新方法，通过新颖的架构创新来增强预测性能，解决传统方法在高维、异构数据处理中的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的架构，引入基于自适应核的注意力机制。该机制分别处理不同的特征组，然后在集成之前进行整合，从而能够捕获局部模式同时保留全局关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与最先进的方法相比，作者提出的方法在各种数据集上的性能指标都有显著改进。&lt;h4&gt;结论&lt;/h4&gt;基于自适应核的注意力机制架构能够有效处理高维、异构数据中的复杂特征交互和多尺度交互问题，提高了预测性能。&lt;h4&gt;翻译&lt;/h4&gt;高维、异构数据与复杂特征交互对传统预测建模方法构成了重大挑战。虽然投影到潜在结构(PLS)仍然是一种流行技术，但它难以建模复杂的非线性关系，特别是在具有高维相关结构的多变量系统中。多尺度上的同时交互进一步加剧了这一挑战，使得局部处理无法捕获跨组依赖关系。此外，静态特征加权限制了适应上下文变化的能力，因为它忽略了样本特定的相关性。为解决这些局限性，我们提出了一种通过新颖架构创新来增强预测性能的新方法。我们的架构引入了一个基于自适应核的注意力机制，它分别处理不同的特征组，然后在集成之前进行整合，从而能够捕获局部模式同时保留全局关系。实验结果表明，与最先进的方法相比，在各种数据集上的性能指标都有显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-dimensional, heterogeneous data with complex feature interactions posesignificant challenges for traditional predictive modeling approaches. WhileProjection to Latent Structures (PLS) remains a popular technique, it strugglesto model complex non-linear relationships, especially in multivariate systemswith high-dimensional correlation structures. This challenge is furthercompounded by simultaneous interactions across multiple scales, where localprocessing fails to capture crossgroup dependencies. Additionally, staticfeature weighting limits adaptability to contextual variations, as it ignoressample-specific relevance. To address these limitations, we propose a novelmethod that enhances predictive performance through novel architecturalinnovations. Our architecture introduces an adaptive kernel-based attentionmechanism that processes distinct feature groups separately before integration,enabling capture of local patterns while preserving global relationships.Experimental results show substantial improvements in performance metrics,compared to the state-of-the-art methods across diverse datasets.</description>
      <author>example@mail.com (Farwa Abbas, Hussain Ahmad, Claudia Szabo)</author>
      <guid isPermaLink="false">2510.16474v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Humanoid-inspired Causal Representation Learning for Domain Generalization</title>
      <link>http://arxiv.org/abs/2510.16382v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种受人类智能启发的结构化因果模型HSCM，通过模仿人类视觉系统的分层处理和多级学习机制，专注于建模细粒度因果关系，从而提升模型在不同领域间的泛化能力和稳健性。&lt;h4&gt;背景&lt;/h4&gt;传统领域泛化模型存在局限性，它们通常依赖统计数据来捕获数据标签依赖和学习扭曲不变表示，无法充分捕捉人类视觉系统的层次化处理机制。&lt;h4&gt;目的&lt;/h4&gt;克服传统领域泛化模型的局限性，开发一种受人类智能启发的因果框架，提升模型在不同领域间的泛化能力，确保模型的稳健性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出Humanoid-inspired Structural Causal Model (HSCM)，模仿人类视觉系统的分层处理和多级学习，通过解耦和重新加权关键图像属性（如颜色、纹理和形状）来建模细粒度因果机制。&lt;h4&gt;主要发现&lt;/h4&gt;HSCM通过理论和实证评估证明优于现有领域泛化模型，提供了更规范的方法来捕获因果关系并提高模型稳健性，在动态复杂环境中实现更有效的迁移学习。&lt;h4&gt;结论&lt;/h4&gt;HSCM作为一种受人类智能启发的因果框架，能够有效提升模型跨领域的泛化能力，确保稳健性和可解释性，为领域泛化问题提供了新的解决思路。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种受人类智能启发的结构化因果模型（HSCM），这是一种新颖的因果框架，旨在克服传统领域泛化模型的局限性。与依赖统计数据捕获数据标签依赖和学习扭曲不变表示的方法不同，HSCM模仿人类视觉系统的分层处理和多级学习，专注于建模细粒度因果机制。通过解耦和重新加权关键图像属性（如颜色、纹理和形状），HSCM增强了跨不同领域的泛化能力，确保了模型的稳健性和可解释性。利用人类智能的灵活性和适应性，我们的方法使模型在动态复杂环境中能够实现更有效的迁移和学习。通过理论和实证评估，我们证明了HSCM优于现有领域泛化模型，为捕获因果关系和提高模型稳健性提供了更规范的方法。代码可在https://github.com/lambett/HSCM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes the Humanoid-inspired Structural Causal Model (HSCM), anovel causal framework inspired by human intelligence, designed to overcome thelimitations of conventional domain generalization models. Unlike approachesthat rely on statistics to capture data-label dependencies and learndistortion-invariant representations, HSCM replicates the hierarchicalprocessing and multi-level learning of human vision systems, focusing onmodeling fine-grained causal mechanisms. By disentangling and reweighting keyimage attributes such as color, texture, and shape, HSCM enhancesgeneralization across diverse domains, ensuring robust performance andinterpretability. Leveraging the flexibility and adaptability of humanintelligence, our approach enables more effective transfer and learning indynamic, complex environments. Through both theoretical and empiricalevaluations, we demonstrate that HSCM outperforms existing domaingeneralization models, providing a more principled method for capturing causalrelationships and improving model robustness. The code is available athttps://github.com/lambett/HSCM.</description>
      <author>example@mail.com (Ze Tao, Jian Zhang, Haowei Li, Xianshuai Li, Yifei Peng, Xiyao Liu, Senzhang Wang, Chao Liu, Sheng Ren, Shichao Zhang)</author>
      <guid isPermaLink="false">2510.16382v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema</title>
      <link>http://arxiv.org/abs/2510.16357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 7 figures, 4 tables, 2 algorithms, and 34 references.  HuggingFace:  https://huggingface.co/datasets/jugalgajjar/MultiLang-Code-Parser-Dataset  GitHub: https://github.com/JugalGajjar/MultiLang-Code-Parser-Dataset&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了多语言代码解析器数据集(MLCPD)，一个统一十种主要编程语言语法结构的大规模、语言无关数据集，包含超过七百万个标准化解析源文件，支持跨语言推理、结构学习和多语言软件分析。&lt;h4&gt;背景&lt;/h4&gt;现有代码语料库主要关注标记级代码或孤立解析器，缺乏跨语言的统一表示，需要一种能够统一不同编程语言语法结构的数据集。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模、语言无关的数据集，统一十种主要编程语言的语法和结构表示，为跨语言表示学习和程序分析提供开放、可重现的基础。&lt;h4&gt;方法&lt;/h4&gt;提出通用抽象语法树(AST)模式标准化解析源文件，为每个文件提供分层树表示和丰富元数据，以Parquet格式存储，进行跨语言结构分析，并开发数据集复现、语法编译和可视化工具。&lt;h4&gt;主要发现&lt;/h4&gt;跨语言代码结构存在强大的规律性，差异很大的编程语言(如Python、Java和Go)的语法图可以在共享模式下对齐，提出的统一AST模式能够无损地表示不同语言的语法结构。&lt;h4&gt;结论&lt;/h4&gt;MLCPD为跨语言表示学习和程序分析提供了开放、可重现的基础，通过统一的数据表示和丰富的工具支持，促进了多语言软件分析和跨语言推理研究。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了多语言代码解析器数据集(MLCPD)，这是一个大规模、语言无关的数据集，统一了十种主要编程语言的代码语法和结构表示。MLCPD包含超过七百万个在我们提出的通用抽象语法树(AST)模式下标准化的解析源文件，实现了跨语言推理、结构学习和多语言软件分析的一致性。与仅关注标记级代码或孤立解析器的现有语料库不同，MLCPD为每个文件提供了分层树表示和丰富的元数据，确保无损的语法覆盖和结构一致性。每个条目包括标准化的模式、语言级元数据和抽象节点语义，以Parquet格式存储以便可扩展检索。经验分析揭示了强大的跨语言结构规律性，证明像Python、Java和Go这样差异很大的语言的语法图可以在共享模式下对齐。我们在Hugging Face上公开发布了该数据集，并在GitHub上提供了配套代码库，包括数据集复现、语法编译和探索跨语言统一AST的可视化工具。这些资源共同确立了MLCPD作为跨语言表示学习和程序分析未来研究的开放、可重现基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,language-agnostic dataset unifying syntactic and structural representations ofcode across ten major programming languages. MLCPD contains over seven millionparsed source files normalized under our proposed universal Abstract SyntaxTree (AST) schema, enabling consistent cross-language reasoning, structurallearning, and multilingual software analysis. Unlike existing corpora thatfocus purely on token-level code or isolated parsers, MLCPD provides bothhierarchical tree representations and rich metadata for every file, ensuringlossless syntactic coverage and structural uniformity. Each entry includes anormalized schema, language-level metadata, and abstracted node semanticsstored in Parquet format for scalable retrieval. Empirical analyses revealstrong cross-language structural regularities-demonstrating that syntacticgraphs from languages as diverse as Python, Java, and Go can be aligned under ashared schema. We release the dataset publicly on Hugging Face and theaccompanying codebase on GitHub, which includes complete pipelines for datasetreproduction, grammar compilation, and a visualization tool for exploring theunified AST across languages. Together, these resources establish MLCPD as anopen, reproducible foundation for future research in cross-languagerepresentation learning and program analysis.</description>
      <author>example@mail.com (Jugal Gajjar, Kamalasankari Subramaniakuppusamy)</author>
      <guid isPermaLink="false">2510.16357v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Disentangling Hyperedges through the Lens of Category Theory</title>
      <link>http://arxiv.org/abs/2510.16289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了超图结构数据中的超边解缠问题，从范畴论角度提出了一种新的解缠准则，并通过基因功能关系分析验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;尽管解缠表示学习在图结构数据分析中取得了进展，但针对超图结构数据的解缠研究较少，存在研究空白。&lt;h4&gt;目的&lt;/h4&gt;将超边解缠整合到超图神经网络中，使模型能够利用与标签相关的隐藏超边语义，如节点间未注释的关系。&lt;h4&gt;方法&lt;/h4&gt;从范畴论角度分析超边解缠，提出基于自然性条件的新解缠准则，并构建概念验证模型进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;概念验证模型成功捕获了基因通路中基因的功能关系，证明了所提准则的潜力。&lt;h4&gt;结论&lt;/h4&gt;基于自然性条件的解缠准则在超图结构数据中有效，特别是在分析基因功能关系方面表现出应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;尽管解缠表示学习在发现图结构数据中的潜在模式方面取得了有希望的结果，但很少有研究探索超图结构数据的解缠。将超边解缠整合到超图神经网络中，使模型能够利用与标签相关的隐藏超边语义，例如节点之间未注释的关系。本文从范畴论的角度对超边解缠进行了分析，并提出了一种从自然性条件推导出的新解缠准则。我们的概念验证模型通过成功捕获基因通路（超边）中基因（节点）的功能关系，实验性地展示了所提出准则的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the promising results of disentangled representation learning indiscovering latent patterns in graph-structured data, few studies have exploreddisentanglement for hypergraph-structured data. Integrating hyperedgedisentanglement into hypergraph neural networks enables models to leveragehidden hyperedge semantics, such as unannotated relations between nodes, thatare associated with labels. This paper presents an analysis of hyperedgedisentanglement from a category-theoretical perspective and proposes a novelcriterion for disentanglement derived from the naturality condition. Ourproof-of-concept model experimentally showed the potential of the proposedcriterion by successfully capturing functional relations of genes (nodes) ingenetic pathways (hyperedges).</description>
      <author>example@mail.com (Yoonho Lee, Junseok Lee, Sangwoo Seo, Sungwon Kim, Yeongmin Kim, Chanyoung Park)</author>
      <guid isPermaLink="false">2510.16289v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>MuseTok: Symbolic Music Tokenization for Generation and Semantic Understanding</title>
      <link>http://arxiv.org/abs/2510.16273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MuseTok是一种创新的离散表示学习方法，专门针对符号音乐设计，结合了RQ-VAE和Transformer架构，在音乐生成和语义理解任务中均取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;离散表示学习在图像、语音和语言的生成和理解领域已显示出有前景的结果，这些进展启发了作者对音乐符号表示的研究。&lt;h4&gt;目的&lt;/h4&gt;提出MuseTok，一种用于符号音乐的标记化方法，研究其在音乐生成和理解任务中的有效性。&lt;h4&gt;方法&lt;/h4&gt;MuseTok采用基于残差向量量化-变分自编码器（RQ-VAE）的方法，在基于Transformer的编码器-解码器框架中，对小节音乐段进行处理，生成能够实现高保真音乐重建和准确音乐理论理解的音乐代码。&lt;h4&gt;主要发现&lt;/h4&gt;在音乐生成和语义理解任务的综合评估中，使用MuseTok的模型在语义理解方面优于先前的表示学习基线，在内容生成方面保持可比的性能；对MuseTok代码的定性分析表明，它能够从大型音乐集中有效捕捉潜在的音乐概念。&lt;h4&gt;结论&lt;/h4&gt;MuseTok是一种有效的符号音乐标记化方法，在音乐生成和理解任务中表现良好。&lt;h4&gt;翻译&lt;/h4&gt;离散表示学习在图像、语音和语言的生成和理解等多个领域已显示出有前景的结果。受这些进展的启发，我们提出了MuseTok，一种用于符号音乐的标记化方法，并研究了其在音乐生成和理解任务中的有效性。MuseTok在基于Transformer的编码器-解码器框架中，对小节音乐段应用残差向量量化-变分自编码器（RQ-VAE），生成能够实现高保真音乐重建和准确音乐理论理解的音乐代码。为了进行全面评估，我们将MuseTok应用于音乐生成和语义理解任务，包括旋律提取、和弦识别和情感识别。采用MuseTok的模型在语义理解方面优于先前的表示学习基线，同时在内容生成方面保持可比的性能。此外，使用真实类别和合成数据集对MuseTok代码进行的定性分析表明，MuseTok能够有效从大型音乐集中捕捉潜在的音乐概念。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Discrete representation learning has shown promising results across variousdomains, including generation and understanding in image, speech and language.Inspired by these advances, we propose MuseTok, a tokenization method forsymbolic music, and investigate its effectiveness in both music generation andunderstanding tasks. MuseTok employs the residual vector quantized-variationalautoencoder (RQ-VAE) on bar-wise music segments within a Transformer-basedencoder-decoder framework, producing music codes that achieve high-fidelitymusic reconstruction and accurate understanding of music theory. Forcomprehensive evaluation, we apply MuseTok to music generation and semanticunderstanding tasks, including melody extraction, chord recognition, andemotion recognition. Models incorporating MuseTok outperform previousrepresentation learning baselines in semantic understanding while maintainingcomparable performance in content generation. Furthermore, qualitative analyseson MuseTok codes, using ground-truth categories and synthetic datasets, revealthat MuseTok effectively captures underlying musical concepts from large musiccollections.</description>
      <author>example@mail.com (Jingyue Huang, Zachary Novack, Phillip Long, Yupeng Hou, Ke Chen, Taylor Berg-Kirkpatrick, Julian McAuley)</author>
      <guid isPermaLink="false">2510.16273v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis</title>
      <link>http://arxiv.org/abs/2510.16086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages,3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种因子引导的语义恢复框架(FSRF)，用于解决多模态情感分析中的模态缺失问题，通过去冗余的同质异质因子分解模块和分布对齐的自蒸馏模块，有效恢复了缺失模态的语义信息，实验证明该方法在不确定缺失模态的情况下具有显著的性能优势。&lt;h4&gt;背景&lt;/h4&gt;多模态情感分析(MSA)已成为研究热点，旨在利用多模态数据进行人类情感理解。以往研究主要关注完整多模态数据的交互和融合，忽略了实际应用中因遮挡、个人隐私限制和设备故障导致的模态缺失问题，导致泛化能力低。&lt;h4&gt;目的&lt;/h4&gt;提出一个因子引导的语义恢复框架(FSRF)，以缓解MSA任务中的模态缺失问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种去冗余的同质异质因子分解模块，将模态分解为模态同质、模态异质和噪声表示，并设计了表示学习的精细约束范式；设计了一种分布对齐的自蒸馏模块，通过利用双向知识转移完全恢复缺失的语义。&lt;h4&gt;主要发现&lt;/h4&gt;在两个数据集上的综合实验表明，与之前的方法相比，FSRF在不确定缺失模态的情况下具有显著的性能优势。&lt;h4&gt;结论&lt;/h4&gt;FSRF框架有效解决了多模态情感分析中的模态缺失问题，提高了模型在真实应用场景中的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;近年来，多模态情感分析(MSA)已成为一个研究热点，旨在利用多模态数据进行人类情感理解。以往的MSA研究主要集中在完整多模态数据的交互和融合上，忽略了实际应用中因遮挡、个人隐私限制和设备故障导致的模态缺失问题，导致泛化能力低。为此，我们提出了一种因子引导的语义恢复框架(FSRF)，以缓解MSA任务中的模态缺失问题。具体而言，我们提出了一种去冗余的同质异质因子分解模块，将模态分解为模态同质、模态异质和噪声表示，并设计了表示学习的精细约束范式。此外，我们设计了一种分布对齐的自蒸馏模块，通过利用双向知识转移完全恢复缺失的语义。在两个数据集上的综合实验表明，与之前的方法相比，FSRF在不确定缺失模态的情况下具有显著的性能优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, Multimodal Sentiment Analysis (MSA) has become a researchhotspot that aims to utilize multimodal data for human sentiment understanding.Previous MSA studies have mainly focused on performing interaction and fusionon complete multimodal data, ignoring the problem of missing modalities inreal-world applications due to occlusion, personal privacy constraints, anddevice malfunctions, resulting in low generalizability.  To this end, we propose a Factorization-guided Semantic Recovery Framework(FSRF) to mitigate the modality missing problem in the MSA task.  Specifically, we propose a de-redundant homo-heterogeneous factorizationmodule that factorizes modality into modality-homogeneous,modality-heterogeneous, and noisy representations and design elaborateconstraint paradigms for representation learning.  Furthermore, we design a distribution-aligned self-distillation module thatfully recovers the missing semantics by utilizing bidirectional knowledgetransfer.  Comprehensive experiments on two datasets indicate that FSRF has asignificant performance advantage over previous methods with uncertain missingmodalities.</description>
      <author>example@mail.com (Ziyang Liu, Pengjunfei Chu, Shuming Dong, Chen Zhang, Mingcheng Li, Jin Wang)</author>
      <guid isPermaLink="false">2510.16086v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.15430v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Withdrawn due to an accidental duplicate submission. This paper  (arXiv:2510.15430) was unintentionally submitted as a new entry instead of a  new version of our previous work (arXiv:2508.09201)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Learning to Detect (LoD)的通用框架，用于检测大视觉语言模型中的未知越狱攻击。&lt;h4&gt;背景&lt;/h4&gt;尽管进行了广泛的对齐努力，大视觉语言模型(LVLMs)仍然容易受到越狱攻击，带来严重的安全风险。&lt;h4&gt;目的&lt;/h4&gt;解决现有检测方法的局限性，这些方法要么学习特定攻击参数（难以泛化到新攻击），要么依赖启发式原理（限制准确性和效率）。&lt;h4&gt;方法&lt;/h4&gt;提出Learning to Detect (LoD)框架，通过从攻击特定学习转向任务特定学习来检测未知越狱攻击。框架包括：1)多模态安全概念激活向量模块，用于安全导向的表征学习；2)安全模式自动编码器模块，用于无监督攻击分类。&lt;h4&gt;主要发现&lt;/h4&gt;广泛实验表明，该方法在各种未知攻击上实现了更高的一致性检测AUROC，同时提高了效率。&lt;h4&gt;结论&lt;/h4&gt;Learning to Detect框架有效解决了现有检测方法的局限性，能够准确检测未知越狱攻击并提高效率。&lt;h4&gt;翻译&lt;/h4&gt;尽管进行了广泛的对齐努力，大视觉语言模型(LVLMs)仍然容易受到越狱攻击，带来了严重的安全风险。为了解决这个问题，现有的检测方法要么学习特定攻击的参数，这阻碍了对未见过攻击的泛化能力，要么依赖启发式原理，这限制了准确性和效率。为了克服这些限制，我们提出了Learning to Detect (LoD)框架，通过将重点从攻击特定学习转向任务特定学习，准确检测未知的越狱攻击。该框架包括一个用于安全导向表征学习的多模态安全概念激活向量模块和一个用于无监督攻击分类的安全模式自动编码器模块。广泛的实验表明，我们的方法在各种未知攻击上实现了更高的一致性检测AUROC，同时提高了效率。代码可在https://anonymous.4open.science/r/Learning-to-Detect-51CB获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)remain vulnerable to jailbreak attacks, posing serious safety risks. To addressthis, existing detection methods either learn attack-specific parameters, whichhinders generalization to unseen attacks, or rely on heuristically soundprinciples, which limit accuracy and efficiency. To overcome these limitations,we propose Learning to Detect (LoD), a general framework that accuratelydetects unknown jailbreak attacks by shifting the focus from attack-specificlearning to task-specific learning. This framework includes a Multi-modalSafety Concept Activation Vector module for safety-oriented representationlearning and a Safety Pattern Auto-Encoder module for unsupervised attackclassification. Extensive experiments show that our method achievesconsistently higher detection AUROC on diverse unknown attacks while improvingefficiency. The code is available athttps://anonymous.4open.science/r/Learning-to-Detect-51CB.</description>
      <author>example@mail.com (Shuang Liang, Zhihao Xu, Jialing Tao, Hui Xue, Xiting Wang)</author>
      <guid isPermaLink="false">2510.15430v2</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Large-scale User Game Lifecycle Representation Learning</title>
      <link>http://arxiv.org/abs/2510.15412v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对在线游戏平台广告和推荐系统的需求，提出了一种称为用户游戏生命周期(UGL)的表示学习方法，以解决游戏数据稀疏和游戏不平衡问题，并通过实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;随着视频游戏的快速扩张，在线游戏平台需要有效的广告和推荐系统。现有的表示学习方法是为处理推荐系统中的数十亿个项目而设计的，但不适用于游戏广告和推荐，主要原因是游戏稀疏性（仅有数百个游戏不足以进行大规模用户表示学习）和游戏不平衡性（用户行为被少数热门游戏主导）。&lt;h4&gt;目的&lt;/h4&gt;解决游戏稀疏性和游戏不平衡性对游戏广告和推荐系统的影响，提高用户兴趣捕捉的准确性。&lt;h4&gt;方法&lt;/h4&gt;1. 引入用户游戏生命周期(UGL)来丰富用户在游戏中的行为，解决稀疏性问题；2. 提出两种创新策略来操纵用户行为，更有效地提取短期和长期兴趣；3. 提出逆概率掩码策略用于UGL表示学习，解决游戏不平衡挑战。&lt;h4&gt;主要发现&lt;/h4&gt;UGL表示显著提升了模型性能：对于游戏广告，平均AUC离线增加1.83%，CVR在线平均增加21.67%；对于游戏内物品推荐，平均AUC离线增加0.5%，ARPU在线平均增加0.82%。&lt;h4&gt;结论&lt;/h4&gt;UGL表示学习方法能够有效解决游戏稀疏性和不平衡性问题，显著提升游戏广告和推荐系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;随着视频游戏生产的快速扩张，有必要为在线游戏平台开发有效的广告和推荐系统。向用户推荐和宣传游戏取决于捕捉他们对游戏的兴趣。然而，为处理推荐系统中的数十亿个项目而设计的现有表示学习方法不适用于游戏广告和推荐。这主要是由于游戏稀疏性，其中仅有的数百个游戏不足以进行大规模用户表示学习，以及游戏不平衡性，其中用户行为被少数热门游戏主导。为了解决稀疏性问题，我们引入了用户游戏生命周期(UGL)，旨在丰富用户在游戏中的行为。此外，我们提出了两种创新策略，旨在操纵用户行为以更有效地提取短期和长期兴趣。为了应对游戏不平衡挑战，我们提出了用于UGL表示学习的逆概率掩码策略。离线和在线实验结果表明，UGL表示显著增强了模型性能，在游戏广告方面平均实现1.83%的AUC离线增长和21.67%的CVR在线增长，在游戏内物品推荐方面平均实现0.5%的AUC离线增长和0.82%的ARPU在线增长。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid expansion of video game production necessitates the development ofeffective advertising and recommendation systems for online game platforms.Recommending and advertising games to users hinges on capturing their interestin games. However, existing representation learning methods crafted forhandling billions of items in recommendation systems are unsuitable for gameadvertising and recommendation. This is primarily due to game sparsity, wherethe mere hundreds of games fall short for large-scale user representationlearning, and game imbalance, where user behaviors are overwhelmingly dominatedby a handful of popular games. To address the sparsity issue, we introduce theUser Game Lifecycle (UGL), designed to enrich user behaviors in games.Additionally, we propose two innovative strategies aimed at manipulating userbehaviors to more effectively extract both short and long-term interests. Totackle the game imbalance challenge, we present an Inverse Probability Maskingstrategy for UGL representation learning. The offline and online experimentalresults demonstrate that the UGL representations significantly enhance model byachieving a 1.83% AUC offline increase on average and a 21.67% CVR onlineincrease on average for game advertising and a 0.5% AUC offline increase and a0.82% ARPU online increase for in-game item recommendation.</description>
      <author>example@mail.com (Yanjie Gou, Jiangming Liu, Kouying Xue, Yi Hu)</author>
      <guid isPermaLink="false">2510.15412v2</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>HumanCM: One Step Human Motion Prediction</title>
      <link>http://arxiv.org/abs/2510.16709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了HumanCM，一个基于一致性模型的一步式人体运动预测框架，能够高效地单步生成人体运动。&lt;h4&gt;背景&lt;/h4&gt;现有的基于扩散模型的人体运动预测方法依赖多步去噪过程，计算效率较低。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效、准确地预测人体运动的单步生成方法，减少计算负担。&lt;h4&gt;方法&lt;/h4&gt;HumanCM采用基于Transformer的时空架构，通过学习嘈杂和清洁运动状态之间的自一致映射来实现单步生成，并使用时间嵌入来建模长程依赖关系和保持运动连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;在Human3.6M和HumanEva-I数据集上的实验表明，HumanCM实现了与最先进的扩散模型相当或更好的准确性，同时将推理步骤减少了高达两个数量级。&lt;h4&gt;结论&lt;/h4&gt;HumanCM是一种高效的人体运动预测方法，能够在保持高准确性的同时显著减少计算负担。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了HumanCM，这是一个基于一致性模型的一步式人体运动预测框架。与依赖多步去噪的基于扩散的方法不同，HumanCM通过学习嘈杂和清洁运动状态之间的自一致映射来执行高效的单步生成。该框架采用基于Transformer的时空架构，并使用时间嵌入来建模长程依赖关系并保持运动连贯性。在Human3.6M和HumanEva-I上的实验表明，HumanCM实现了与最先进的扩散模型相当或更好的准确性，同时将推理步骤减少了高达两个数量级。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present HumanCM, a one-step human motion prediction framework built uponconsistency models. Instead of relying on multi-step denoising as indiffusion-based methods, HumanCM performs efficient single-step generation bylearning a self-consistent mapping between noisy and clean motion states. Theframework adopts a Transformer-based spatiotemporal architecture with temporalembeddings to model long-range dependencies and preserve motion coherence.Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achievescomparable or superior accuracy to state-of-the-art diffusion models whilereducing inference steps by up to two orders of magnitude.</description>
      <author>example@mail.com (Liu Haojie, Gao Suixiang)</author>
      <guid isPermaLink="false">2510.16709v1</guid>
      <pubDate>Tue, 21 Oct 2025 17:07:44 +0800</pubDate>
    </item>
    <item>
      <title>Semantic representations emerge in biologically inspired ensembles of cross-supervising neural networks</title>
      <link>http://arxiv.org/abs/2510.14486v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 8 figures, 2 supplementary figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种通过并行神经网络集合的交叉监督学习来实现表示学习的模型，每个网络通过与其他网络的交互来编码刺激到抽象表示空间。该模型在生物 plausible 性方面表现出色，学习到的表示易于解码，解码准确性与监督网络相当。研究发现小感受野性能最优，稀疏连接与全连接效果相近但计算量更少。&lt;h4&gt;背景&lt;/h4&gt;大脑通常通过弱监督从大量刺激中学习表示信息。无监督学习是探索生物神经网络设计和计算的天然方法。冗余减少已被建议作为神经编码的突出设计原则，但其机制性生物实现尚不清楚。人工神经网络的无监督训练产生的内部表示允许准确分类，但通常依赖生物上不合理的实现。&lt;h4&gt;目的&lt;/h4&gt;探索大脑中并行子网络间的相互作用如何支持学习，提出一个由神经网络集合组成的表示学习模型，其中每个网络通过与其他网络的交叉监督来学习将刺激编码到抽象表示空间。&lt;h4&gt;方法&lt;/h4&gt;提出一个模型，每个网络具有小的感受野，接收外部输入的固定部分，且网络间不共享权重。测试了不同类型的网络架构以及视觉或神经元刺激下的表现。&lt;h4&gt;主要发现&lt;/h4&gt;1. 交叉监督的网络学习到的语义表示易于解码；2. 单个网络和集合层面的解码准确性与监督网络相当；3. 小感受野性能最优；4. 网络间的稀疏连接几乎与全连接同样准确，但计算量更少。&lt;h4&gt;结论&lt;/h4&gt;稀疏交互的交叉监督网络集合可作为大脑中表示学习和集体计算的算法框架。&lt;h4&gt;翻译&lt;/h4&gt;大脑通过弱监督通常从大量刺激中学习表示信息。因此，无监督学习是探索生物神经网络设计和计算的天然方法。相应地，冗余减少已被建议作为神经编码的突出设计原则，但其机制性生物实现尚不清楚。类似地，人工神经网络的无监督训练产生的内部表示允许准确的刺激分类或解码，但通常依赖于生物上不合理的实现。作者认为大脑中并行子网络之间的相互作用可能支持这种学习：作者提出了一个由神经网络集合组成的表示学习模型，其中每个网络通过与其他网络的交叉监督来学习将刺激编码到抽象表示空间，这些网络接收同时或时间上接近的输入。为了生物 plausible 性，每个网络具有小的感受野，因此接收外部输入的固定部分，且网络间不共享权重。作者发现，对于不同类型的网络架构，以及视觉或神经元刺激，这些交叉监督的网络学习到的语义表示易于解码，且解码准确性与监督网络相当——无论是在单个网络层面还是集合层面。作者进一步表明，小感受野性能最优，且网络间的稀疏连接几乎与全连接同样准确，但计算量少得多。因此，作者提出一个稀疏交互的交叉监督网络集合作为大脑中表示学习和集体计算的算法框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brains learn to represent information from a large set of stimuli, typicallyby weak supervision. Unsupervised learning is therefore a natural approach forexploring the design of biological neural networks and their computations.Accordingly, redundancy reduction has been suggested as a prominent designprinciple of neural encoding, but its ``mechanistic'' biological implementationis unclear. Analogously, unsupervised training of artificial neural networksyields internal representations that allow for accurate stimulus classificationor decoding, but typically rely on biologically-implausible implementations. Wesuggest that interactions between parallel subnetworks in the brain mayunderlie such learning: we present a model of representation learning byensembles of neural networks, where each network learns to encode stimuli intoan abstract representation space by cross-supervising interactions with othernetworks, for inputs they receive simultaneously or in close temporalproximity. Aiming for biological plausibility, each network has a small``receptive field'', thus receiving a fixed part of the external input, and thenetworks do not share weights. We find that for different types of networkarchitectures, and for both visual or neuronal stimuli, these cross-supervisingnetworks learn semantic representations that are easily decodable and thatdecoding accuracy is comparable to supervised networks -- both at the level ofsingle networks and the ensemble. We further show that performance is optimalfor small receptive fields, and that sparse connectivity between networks isnearly as accurate as all-to-all interactions, with far fewer computations. Wethus suggest a sparsely interacting collective of cross-supervising networks asan algorithmic framework for representational learning and collectivecomputation in the brain.</description>
      <author>example@mail.com (Roy Urbach, Elad Schneidman)</author>
      <guid isPermaLink="false">2510.14486v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
  <item>
      <title>Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents</title>
      <link>http://arxiv.org/abs/2510.14438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出'探索到进化'范式构建可验证训练数据，增强网络代理信息聚合能力，创建了WebAggregatorQA数据集和WebAggregator系列模型，在信息聚合任务上表现优异。&lt;h4&gt;背景&lt;/h4&gt;现有开源深度研究网络代理主要关注信息获取能力，忽视信息聚合需求，限制了支持深入研究的能力。&lt;h4&gt;目的&lt;/h4&gt;解决网络代理在信息聚合方面的不足，开发有效聚合信息的基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出'探索到进化'范式：1)主动在线探索从真实网络获取信息；2)自我进化聚合程序，通过选择、组合和细化12种高级逻辑类型操作合成可验证问答对。基于此构建WebAggregatorQA数据集，并使用SmolAgents框架开发WebAggregator模型。&lt;h4&gt;主要发现&lt;/h4&gt;WebAggregator-8B性能与GPT-4.1相当；32B变体在GAIA-text上比GPT-4.1高10%以上，接近Claude-3.7-sonnet；在信息聚合评估基准上，Claude-3.7-sonnet仅达28%，GPT-4.1为25.8%；即使能检索所有参考资料，代理仍表现不佳。&lt;h4&gt;结论&lt;/h4&gt;网络代理基础模型需要加强信息聚合能力，而不仅是信息获取能力。'探索到进化'范式和WebAggregator模型为解决这一问题提供了有效方法。&lt;h4&gt;翻译&lt;/h4&gt;深度研究网络代理不仅从多种来源检索信息，更重要的是需要严格分析和聚合知识进行深入研究。现有开源代理主要关注信息获取而忽视信息聚合，限制了深入研究能力。我们提出'探索到进化'范式可扩展构建可验证训练数据。从主动在线探索开始，代理从真实网络获取信息；然后自我进化聚合程序，通过选择、组合和细化操作合成可验证问答对。这种进化使我们能生产包含50K网站和11个领域10K样本的WebAggregatorQA数据集。基于SmolAgents框架，我们开发了WebAggregator系列模型。WebAggregator-8B与GPT-4.1性能相当，32B变体在GAIA-text上比GPT-4.1高10%以上，接近Claude-3.7-sonnet。我们构建了WebAggregatorQA的人工注释评估分割作为测试集，Claude-3.7-sonnet仅达28%，GPT-4.1为25.8%，突显了加强网络代理信息聚合能力的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep research web agents not only retrieve information from diverse sourcessuch as web environments, files, and multimodal inputs, but more importantly,they need to rigorously analyze and aggregate knowledge for insightfulresearch. However, existing open-source deep research agents predominantlyfocus on enhancing information-seeking capabilities of web agents to locatespecific information, while overlooking the essential need for informationaggregation, which would limit their ability to support in-depth research. Wepropose an Explore to Evolve paradigm to scalably construct verifiable trainingdata for web agents. Begins with proactive online exploration, an agent sourcesgrounded information by exploring the real web. Using the collected evidence,the agent then self-evolves an aggregation program by selecting, composing, andrefining operations from 12 high-level logical types to synthesize a verifiableQA pair. This evolution from high-level guidance to concrete operations allowedus to scalably produce WebAggregatorQA, a dataset of 10K samples across 50Kwebsites and 11 domains. Based on an open-source agent framework, SmolAgents,we collect supervised fine-tuning trajectories to develop a series offoundation models, WebAggregator. WebAggregator-8B matches the performance ofGPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-textand closely approaches Claude-3.7-sonnet. Moreover, given the limitedavailability of benchmarks that evaluate web agents' information aggregationabilities, we construct a human-annotated evaluation split of WebAggregatorQAas a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve allreferences, they still struggle on WebAggregatorQA, highlighting the need tostrengthen the information aggregation capabilities of web agent foundations.</description>
      <author>example@mail.com (Rui Wang, Ce Zhang, Jun-Yu Ma, Jianshu Zhang, Hongru Wang, Yi Chen, Boyang Xue, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu, Kam-Fai Wong)</author>
      <guid isPermaLink="false">2510.14438v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking</title>
      <link>http://arxiv.org/abs/2510.14824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究对比分析了大型语言模型(LLMs)在重排序任务中对比学习(CL)与监督微调(SFT)两种训练目标的差异，发现SFT在LLM重排序中表现优于CL，主要原因是SFT提供了更强的权重方案。&lt;h4&gt;背景&lt;/h4&gt;在信息检索中，重排序模型训练主要关注两类目标：度量学习和分类。对于BERT编码器，对比学习更有效；而对于大型语言模型，通过监督微调进行分类似乎更有前景，因为它与LLMs的生成性质良好对齐。这种分歧引出了核心问题：哪个目标本质上更适合基于LLM的重排序，以及差异背后的机制是什么？&lt;h4&gt;目的&lt;/h4&gt;全面对比分析CL和SFT在重排序任务中的表现，并探究两者之间的差异机制，确定哪种目标更适合基于LLM的重排序模型。&lt;h4&gt;方法&lt;/h4&gt;以通用多模态检索(UMR)为实验平台，将训练目标分解为权重和方向两个组成部分，提出统一框架理解它们的相互作用，通过探测实验进行分析，进行大规模SFT训练并在MRB基准上评估，同时对SFT设置进行消融研究。&lt;h4&gt;主要发现&lt;/h4&gt;SFT提供了比CL明显更强的权重方案，而优选的评分方向没有明显差异；综合结果表明SFT在LLM重排序方面一致优于CL。&lt;h4&gt;结论&lt;/h4&gt;SFT比CL更适合LLM重排序任务，这一发现源于SFT的权重优势而非评分方向差异，研究结果有望促进该领域未来的研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;在信息检索中，训练重排序模型主要关注两类目标：度量学习（如对比损失，提高相关查询-文档对的预测分数）和分类（相关性与不相关性的二元标签预测）。对于BERT风格的编码器，各种研究表明对比学习(CL)可以比判别性（分类）学习更有效。然而，对于大型语言模型(LLMs)，通过监督微调(SFT)进行分类（预测相关对为'是'，不相关对为'否'）似乎更有前景，因为它与LLMs的生成性质良好对齐。这种分歧引出了核心问题：哪个目标本质上更适合基于LLM的重排序，以及这种差异背后的机制是什么？在这项工作中，我们在重排序任务中对CL和SFT进行了全面的比较和分析，以通用多模态检索(UMR)为实验平台。我们首先将目标分解为两个组成部分：权重（控制更新的幅度）和方向（指导模型更新），然后提出了一个统一框架来理解它们的相互作用。通过探测实验，我们发现SFT提供了比CL明显更强的权重方案，而优选的评分方向没有明显胜者。综合这些结果，一致表明SFT在LLM重排序方面优于CL。为了进一步验证我们的发现，我们进行了大规模SFT训练，并在MRB基准上展示了新的最先进重排序器。我们还对SFT设置进行了消融研究，并期望我们的发现能够促进该领域未来的研究和应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In information retrieval, training reranking models mainly focuses on twotypes of objectives: metric learning (e.g. contrastive loss to increase thepredicted scores on relevant query-document pairs) and classification (binarylabel prediction of relevance vs. irrelevance). For BERT-style encoders,various studies have shown that contrastive learning (CL) can be more effectivethan discriminative (classification) learning. However, for large languagemodels (LLMs), classification via supervised fine-tuning (SFT), which predicts''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appearsmore promising as it aligns well with the generative nature of LLMs. Thisdivergence raises a central question: which objective is intrinsically bettersuited to LLM-based reranking, and what mechanism underlies the difference? Inthis work, we conduct a comprehensive comparison and analysis between CL andSFT for reranking, taking the universal multimodal retrieval (UMR) as theexperimental playground. We first decompose the objectives into two components:weight, which controls the magnitude of those updates, and direction, whichguides the model updates, then present a unified framework for understandingtheir interactions. Through probing experiments, we find that SFT provides asubstantially stronger weighting scheme than CL, whereas the preferred scoringdirection shows no clear winner. Taken together, these results point to aconsistent advantage of SFT over CL for LLM reranking. To further validate ourfindings, we conduct large-scale training with SFT and present newstate-of-the-art rerankers on the MRB benchmark. We also provide ablations onSFT settings and expect our findings to benefit future research andapplications in this area.</description>
      <author>example@mail.com (Ziqi Dai, Xin Zhang, Mingxin Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, Min Zhang)</author>
      <guid isPermaLink="false">2510.14824v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Hypergraph Contrastive Sensor Fusion for Multimodal Fault Diagnosis in Induction Motors</title>
      <link>http://arxiv.org/abs/2510.15547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE Sensors Journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多模态超图对比注意力网络(MM-HCAN)，用于感应电机的鲁棒故障诊断，实现了高达99.82%的准确率，具有强大的跨域泛化能力和噪声鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;可靠的感应电机故障诊断对工业安全和运营连续性至关重要，但传统方法难以捕捉复杂的多模态信号关系，局限于单模态数据或单一故障类型，在嘈杂或跨域条件下性能下降。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的故障诊断框架，解决多模态传感器融合问题，实现轴承、定子和转子故障的同时诊断，提高诊断系统的泛化能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出多模态超图对比注意力网络(MM-HCAN)，首次将对比学习整合到专为多模态传感器融合设计的超图拓扑中，实现模态内和模态间依赖关系的联合建模，增强超越欧几里得嵌入空间的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界基准测试中，MM-HCAN实现了高达99.82%的准确率，具有强大的跨域泛化能力和对噪声的鲁棒性，消融研究验证了每个组件的有效贡献。&lt;h4&gt;结论&lt;/h4&gt;MM-HCAN为全面的多故障诊断提供了可扩展和鲁棒的解决方案，支持工业环境中的预测维护和资产寿命延长。&lt;h4&gt;翻译&lt;/h4&gt;可靠的感应电机故障诊断对工业安全和运营连续性至关重要，可减轻昂贵的意外停机时间。传统方法往往难以捕捉复杂的多模态信号关系，局限于单模态数据或单一故障类型，并在嘈杂或跨域条件下表现出性能下降。本文提出了多模态超图对比注意力网络(MM-HCAN)，一个用于鲁棒故障诊断的统一框架。据我们所知，MM-HCAN首次将对比学习整合到专为多模态传感器融合设计的超图拓扑中，实现了模态内和模态间依赖关系的联合建模，并增强了超越欧几里得嵌入空间的泛化能力。该模型支持轴承、定子和转子故障的同时诊断，满足了工程上对整合诊断能力的需求。在三个真实世界基准测试中评估，MM-HCAN实现了高达99.82%的准确率，具有强大的跨域泛化能力和对噪声的鲁棒性，证明了其适合实际部署。消融研究验证了每个组件的贡献。MM-HCAN为全面的多故障诊断提供了可扩展和鲁棒的解决方案，支持工业环境中的预测维护和资产寿命延长。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable induction motor (IM) fault diagnosis is vital for industrial safetyand operational continuity, mitigating costly unplanned downtime. Conventionalapproaches often struggle to capture complex multimodal signal relationships,are constrained to unimodal data or single fault types, and exhibit performancedegradation under noisy or cross-domain conditions. This paper proposes theMultimodal Hypergraph Contrastive Attention Network (MM-HCAN), a unifiedframework for robust fault diagnosis. To the best of our knowledge, MM-HCAN isthe first to integrate contrastive learning within a hypergraph topologyspecifically designed for multimodal sensor fusion, enabling the jointmodelling of intra- and inter-modal dependencies and enhancing generalisationbeyond Euclidean embedding spaces. The model facilitates simultaneous diagnosisof bearing, stator, and rotor faults, addressing the engineering need forconsolidated di- agnostic capabilities. Evaluated on three real-worldbenchmarks, MM-HCAN achieves up to 99.82% accuracy with strong cross-domaingeneralisation and resilience to noise, demonstrating its suitability forreal-world deployment. An ablation study validates the contribution of eachcomponent. MM-HCAN provides a scalable and robust solution for comprehensivemulti-fault diagnosis, supporting predictive maintenance and extended assetlongevity in industrial environments.</description>
      <author>example@mail.com (Usman Ali, Ali Zia, Waqas Ali, Umer Ramzan, Abdul Rehman, Muhammad Tayyab Chaudhry, Wei Xiang)</author>
      <guid isPermaLink="false">2510.15547v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval</title>
      <link>http://arxiv.org/abs/2510.15543v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种模态组合感知框架，以提高多模态大语言模型作为统一编码器时的鲁棒性，解决了传统对比学习训练的统一编码器容易学习模态捷径的问题。&lt;h4&gt;背景&lt;/h4&gt;多模态检索支持跨模态内容检索，应用广泛。单独编码器方法如CLIP通过对比学习对齐模态特定嵌入，而多模态大语言模型(MLLMs)实现了处理组合输入的统一编码器。&lt;h4&gt;目的&lt;/h4&gt;解决统一编码器使用传统对比学习训练时容易学习模态捷径的问题，提高分布转移下的检索鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出模态组合感知框架，包含偏好损失强制多模态嵌入优于单模态对应部分，以及组合正则化目标将多模态嵌入与单模态部分组成的原型对齐，明确建模组合表示与单模态部分间的结构关系。&lt;h4&gt;主要发现&lt;/h4&gt;在各种基准测试上，该框架在分布外检索方面表现提升，证明了模态组合感知是利用MLLM作为统一编码器时鲁棒组合多模态检索的有效原则。&lt;h4&gt;结论&lt;/h4&gt;模态组合感知框架能有效提高多模态检索的鲁棒性，特别是在分布转移情况下，为多模态检索提供了新的有效原则。&lt;h4&gt;翻译&lt;/h4&gt;多模态检索寻求跨模态（如文本或图像）检索相关内容，支持从AI搜索到内容生成的应用。尽管像CLIP这样的单独编码器方法通过对比学习对齐模态特定嵌入取得了成功，但最近的多模态大语言模型(MLLMs)实现了可以直接处理组合输入的统一编码器。虽然灵活且先进，我们发现使用传统对比学习训练的统一编码器容易学习模态捷径，导致在分布转移下鲁棒性差。我们提出了一种模态组合感知框架来缓解这一问题。具体而言，偏好损失强制多模态嵌入优于其单模态对应部分，而组合正则化目标将多模态嵌入与其单模态部分组成的原型对齐。这些目标明确建模了组合表示与其单模态对应部分之间的结构关系。在各种基准测试上的实验显示分布外检索有所提升，突显了模态组合感知作为利用MLLM作为统一编码器时鲁棒组合多模态检索的有效原则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal retrieval, which seeks to retrieve relevant content acrossmodalities such as text or image, supports applications from AI search tocontents production. Despite the success of separate-encoder approaches likeCLIP align modality-specific embeddings with contrastive learning, recentmultimodal large language models (MLLMs) enable a unified encoder that directlyprocesses composed inputs. While flexible and advanced, we identify thatunified encoders trained with conventional contrastive learning are prone tolearn modality shortcut, leading to poor robustness under distribution shifts.We propose a modality composition awareness framework to mitigate this issue.Concretely, a preference loss enforces multimodal embeddings to outperformtheir unimodal counterparts, while a composition regularization objectivealigns multimodal embeddings with prototypes composed from its unimodal parts.These objectives explicitly model structural relationships between the composedrepresentation and its unimodal counterparts. Experiments on various benchmarksshow gains in out-of-distribution retrieval, highlighting modality compositionawareness as a effective principle for robust composed multimodal retrievalwhen utilizing MLLMs as the unified encoder.</description>
      <author>example@mail.com (Qiyu Wu, Shuyang Cui, Satoshi Hayakawa, Wei-Yao Wang, Hiromi Wakaki, Yuki Mitsufuji)</author>
      <guid isPermaLink="false">2510.15543v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm</title>
      <link>http://arxiv.org/abs/2510.14321v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了大型推理嵌入模型(LREM)，通过将推理过程整合到表示学习中，解决了现有嵌入模型在处理困难查询时的性能下降问题，显著提高了电子商务搜索系统的检索准确性。&lt;h4&gt;背景&lt;/h4&gt;在现代电子商务搜索系统中，密集检索已成为不可或缺的组成部分。主流嵌入模型已从BERT转向大型语言模型(LLMs)以获得更准确的文本建模，但这些模型仍采用直接嵌入方法，语义准确性不足。现有模型通过对比学习实现语义对齐，但倾向于捕捉训练数据中的统计共现模式，偏向浅层词汇和语义匹配，导致对与目标物品词汇差异大的困难查询性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;解决现有嵌入模型在处理困难查询时的性能下降问题，通过整合推理过程到表示学习中，提高检索准确性，弥合原始查询和目标物品之间的语义差距。&lt;h4&gt;方法&lt;/h4&gt;提出大型推理嵌入模型(LREM)，创新性地将推理过程整合到表示学习中。对于困难查询，LREM首先进行推理以深入理解原始查询，然后生成推理增强的查询嵌入用于检索。采用两阶段训练过程：第一阶段在精心策划的查询-思维链-物品三元组上使用SFT和InfoNCE损失优化LLM，建立初步推理和嵌入能力；第二阶段通过强化学习进一步优化推理轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;推理过程有效地弥合了原始查询和目标物品之间的语义差距，显著提高了检索准确性。大量的离线和在线实验验证了LREM的有效性。&lt;h4&gt;结论&lt;/h4&gt;LREM已被部署在中国最大的电子商务平台上，自2025年8月起开始应用。&lt;h4&gt;翻译&lt;/h4&gt;在现代电子商务搜索系统中，密集检索已成为不可或缺的组成部分。通过计算查询和物品(产品)嵌入之间的相似性，它能够从大规模存储库中高效地选择候选产品。随着大型语言模型(LLMs)的突破，主流嵌入模型已经逐渐从BERT转向LLMs以实现更准确的文本建模。然而，这些模型仍然采用直接嵌入方法，嵌入的语义准确性仍然不足。因此，对比学习被大量使用来实现正对之间的紧密语义对齐。结果，这类模型往往会捕捉训练数据中的统计共现模式，使其偏向于浅层词汇和语义匹配。对于与目标物品表现出显著词汇差异的困难查询，性能会显著下降。在这项工作中，我们提出了大型推理嵌入模型(LREM)，创新性地将推理过程整合到表示学习中。对于困难查询，LREM首先进行推理以实现对原始查询的深入理解，然后生成推理增强的查询嵌入用于检索。这个推理过程有效地弥合了原始查询和目标物品之间的语义差距，显著提高了检索准确性。具体来说，我们采用两阶段训练过程：第一阶段在精心策划的查询-思维链-物品三元组上使用SFT和InfoNCE损失优化LLM，建立初步推理和嵌入能力；第二阶段通过强化学习(RL)进一步优化推理轨迹。大量的离线和在线实验验证了LREM的有效性，使其自2025年8月起被部署在中国最大的电子商务平台上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In modern e-commerce search systems, dense retrieval has become anindispensable component. By computing similarities between query and item(product) embeddings, it efficiently selects candidate products fromlarge-scale repositories. With the breakthroughs in large language models(LLMs), mainstream embedding models have gradually shifted from BERT to LLMsfor more accurate text modeling. However, these models still adoptdirect-embedding methods, and the semantic accuracy of embeddings remainsinadequate. Therefore, contrastive learning is heavily employed to achievetight semantic alignment between positive pairs. Consequently, such models tendto capture statistical co-occurrence patterns in the training data, biasingthem toward shallow lexical and semantic matches. For difficult queriesexhibiting notable lexical disparity from target items, the performancedegrades significantly. In this work, we propose the Large Reasoning EmbeddingModel (LREM), which novelly integrates reasoning processes into representationlearning. For difficult queries, LREM first conducts reasoning to achieve adeep understanding of the original query, and then produces areasoning-augmented query embedding for retrieval. This reasoning processeffectively bridges the semantic gap between original queries and target items,significantly improving retrieval accuracy. Specifically, we adopt a two-stagetraining process: the first stage optimizes the LLM on carefully curatedQuery-CoT-Item triplets with SFT and InfoNCE losses to establish preliminaryreasoning and embedding capabilities, and the second stage further refines thereasoning trajectories via reinforcement learning (RL). Extensive offline andonline experiments validate the effectiveness of LREM, leading to itsdeployment on China's largest e-commerce platform since August 2025.</description>
      <author>example@mail.com (Jianting Tang, Dongshuai Li, Tao Wen, Fuyu Lv, Dan Ou, Linli Xu)</author>
      <guid isPermaLink="false">2510.14321v2</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>BLIP3o-NEXT: Next Frontier of Native Image Generation</title>
      <link>http://arxiv.org/abs/2510.15857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BLIP3o-NEXT是一个全开源的基础模型，统一了文本到图像生成和图像编辑功能，采用自回归+扩散架构，在多种基准测试中表现优于现有模型。&lt;h4&gt;背景&lt;/h4&gt;原生图像生成领域不断发展，需要能够同时处理图像生成和编辑的统一架构，以及探索影响模型性能的关键因素。&lt;h4&gt;目的&lt;/h4&gt;开发一个先进的全开源基础模型，推动原生图像生成的前沿发展，并探索影响模型性能的关键见解。&lt;h4&gt;方法&lt;/h4&gt;采用自回归+扩散架构，自回归模型基于多模态输入生成离散图像令牌，其隐藏状态作为扩散模型的条件信号生成高保真图像；结合后训练和数据引擎提高指令遵循能力和图像一致性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 架构选择对性能影响有限，有效扩展和快速推理是关键；2. 强化学习可进一步推动原生图像生成；3. 图像编辑具有挑战性，但可通过后训练和数据引擎改进；4. 数据质量和规模决定模型性能上限。&lt;h4&gt;结论&lt;/h4&gt;BLIP3o-NEXT通过创新的架构设计和关键见解的应用，实现了图像生成和编辑的新水平，在多种基准测试中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了BLIP3o-NEXT，这是BLIP3系列中的一个全开源基础模型，推动了原生图像生成的下一个前沿。BLIP3o-NEXT将文本到图像生成和图像编辑统一在一个架构中，展示了强大的图像生成和编辑能力。在开发最先进的原生图像生成模型过程中，我们确定了四个关键见解：(1) 大多数架构选择产生可比的性能；只要架构能有效扩展并支持快速推理，就可以被认为是有效的；(2) 强化学习的成功应用可以进一步推动原生图像生成的前沿；(3) 图像编辑仍然具有挑战性，但通过后训练和数据引擎可以显著提高指令遵循能力和生成图像与参考图像之间的一致性；(4) 数据质量和规模仍然是决定模型性能上限的决定性因素。基于这些见解，BLIP3o-NEXT采用自回归+扩散架构，其中自回归模型首先基于多模态输入生成离散图像令牌，然后其隐藏状态被用作扩散模型的条件信号以生成高保真图像。该架构结合了自回归模型的推理强度和指令遵循能力以及扩散模型的精细细节渲染能力，实现了新的连贯性和真实感水平。对各种文本到图像和图像编辑基准的广泛评估表明，BLIP3o-NEXT优于现有模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3series that advances the next frontier of native image generation. BLIP3o-NEXTunifies text-to-image generation and image editing within a singlearchitecture, demonstrating strong image generation and image editingcapabilities. In developing the state-of-the-art native image generation model,we identify four key insights: (1) Most architectural choices yield comparableperformance; an architecture can be deemed effective provided it scalesefficiently and supports fast inference; (2) The successful application ofreinforcement learning can further push the frontier of native imagegeneration; (3) Image editing still remains a challenging task, yet instructionfollowing and the consistency between generated and reference images can besignificantly enhanced through post-training and data engine; (4) Data qualityand scale continue to be decisive factors that determine the upper bound ofmodel performance. Building upon these insights, BLIP3o-NEXT leverages anAutoregressive + Diffusion architecture in which an autoregressive model firstgenerates discrete image tokens conditioned on multimodal inputs, whose hiddenstates are then used as conditioning signals for a diffusion model to generatehigh-fidelity images. This architecture integrates the reasoning strength andinstruction following of autoregressive models with the fine-detail renderingability of diffusion models, achieving a new level of coherence and realism.Extensive evaluations of various text-to-image and image-editing benchmarksshow that BLIP3o-NEXT achieves superior performance over existing models.</description>
      <author>example@mail.com (Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Junnan Li, Silvio Savarese, Caiming Xiong, Ran Xu)</author>
      <guid isPermaLink="false">2510.15857v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>SpeechLLMs for Large-scale Contextualized Zero-shot Slot Filling</title>
      <link>http://arxiv.org/abs/2510.15851v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, EMNLP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基于语音的大型语言模型（speech LLMs）在口语理解（SLU）槽填充任务中的应用，通过创建任务上界、识别性能差距并提出改进措施，显著提升了模型性能。&lt;h4&gt;背景&lt;/h4&gt;槽填充是口语理解的关键子任务，传统实现方式为级联的语音识别后跟一个或多个自然语言理解组件。新兴的语音大型语言模型为理解任务提供了更统一、生成式和遵循指令的新途径，具有数据效率、计算效率和零样本能力。&lt;h4&gt;目的&lt;/h4&gt;创建槽填充任务的经验上界，确定性能、鲁棒性和泛化差距，并提出改进措施以缩小与上界结果的差距。&lt;h4&gt;方法&lt;/h4&gt;通过改进训练数据、架构和训练策略来提升模型性能，并评估这些改进措施的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;每项改进措施都显著提高了模型性能，同时研究还指出了实际应用中面临的挑战，并为利用这些新兴模型提供了经验指导。&lt;h4&gt;结论&lt;/h4&gt;基于语音的大型语言模型为槽填充任务提供了新的有效途径，但仍需解决实践挑战，进一步优化以实现更好的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;槽填充是口语理解（SLU）中的一个关键子任务，传统实现方式为级联的语音识别后跟一个或多个自然语言理解（NLU）组件。最近出现的基于语音的大型语言模型（speech LLMs），它整合了语音和文本基础模型，为以更统一、生成式和遵循指令的方式实现语音理解任务开辟了新途径，同时承诺具有数据和计算效率，具有零样本能力，可推广到未见过的槽标签。我们通过为槽填充任务创建经验上界，确定性能、鲁棒性和泛化差距，并提出改进训练数据、架构和训练策略的建议来缩小与上界结果的差距。我们证明这些措施中的每一项都显著提高了性能，同时突出了实践挑战，并为利用这些新兴模型提供了经验指导和见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Slot filling is a crucial subtask in spoken language understanding (SLU),traditionally implemented as a cascade of speech recognition followed by one ormore natural language understanding (NLU) components. The recent advent ofspeech-based large language models (speechLLMs), which integrate speech andtextual foundation models, has opened new avenues for achieving speechunderstanding tasks in a more unified, generative, and instruction-followingmanner while promising data and compute efficiency with zero-shot abilities,generalizing to unseen slot labels. We address the slot-filling task bycreating an empirical upper bound for the task, identifying performance,robustness, and generalization gaps, and proposing improvements to the trainingdata, architecture, and training strategies to narrow the gap with the upperbound result. We show that each of these measures improve performancesubstantially, while highlighting practical challenges and providing empiricalguidance and insights for harnessing these emerging models.</description>
      <author>example@mail.com (Kadri Hacioglu, Manjunath K E, Andreas Stolcke)</author>
      <guid isPermaLink="false">2510.15851v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>PRISM: Probabilistic Runtime Insights and Scalable Performance Modeling for Large-Scale Distributed Training</title>
      <link>http://arxiv.org/abs/2510.15596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模模型训练（超过数万个GPU）中的性能变异性问题，提出了PRISM性能建模框架，考虑了大规模分布式训练的随机性质，为训练时间提供概率保证的量化度量。&lt;h4&gt;背景&lt;/h4&gt;大规模模型训练中，训练过程中的中断是必然发生的随机事件，随着训练规模扩大和GPU在受限环境下运行，动态运行时变异会变得更加频繁。在64k GPU规模下，已观察到9%的GPU时间变异性，GEMM工作负载上GPU性能最高有14%的变异。&lt;h4&gt;目的&lt;/h4&gt;理解性能变异性的潜在原因，并探索分布式训练的设计和优化空间，提出一种能考虑训练随机性质的性能建模框架。&lt;h4&gt;方法&lt;/h4&gt;提出PRISM性能建模框架，其核心是统计方法，为训练时间提供概率保证的量化度量。使用该框架探索并行化方法到下一代训练系统的设计和优化空间，并通过真实系统测量进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;1) PRISM框架的训练时间预测准确率为20.8%的Kolmogorov-Smirnov距离；2) 根据计算节点放置的不同，可获得高达1.26倍的性能提升潜力；3) 优化通信内核（如AllGather和ReduceScatter）对最小化训练步骤时间变异贡献最大。&lt;h4&gt;结论&lt;/h4&gt;PRISM框架能够有效建模和优化大规模分布式训练中的性能变异性，通过考虑并行化策略对变异的敏感性，可以显著提高训练效率。&lt;h4&gt;翻译&lt;/h4&gt;数万个GPU以上的大规模模型训练是一个未知领域。在这种规模下，训练过程中的中断不是是否会发生的问题，而是何时会发生的问题——这是一种降低训练生产力的随机过程。随着训练规模扩大和GPU在越来越受限的功率和热应力环境下运行，动态运行时变异将变得越来越频繁。在64k GPU规模下，我们已经观察到前沿基础模型训练有9%的GPU时间变异性。为了理解变异性的潜在原因，我们分析了各种平台上大规模的GPU微基准测试，显示在GEMM工作负载上，GPU性能最高有14%的变异，这取决于训练硬件和部署环境。受我们的分析和围绕性能变异性的广阔设计空间的启发，我们提出了PRISM——一个考虑大规模分布式训练随机性质的性能建模框架。PRISM的核心是一种统计方法，为训练时间提供概率保证的量化度量。使用PRISM，我们探索了分布式训练的设计和优化空间，从并行化方法到下一代训练系统。PRISM通过真实系统测量进行了验证，显示出训练时间预测准确率为20.8%的Kolmogorov-Smirnov距离。使用PRISM，我们证明，如果考虑并行化策略对变异的敏感性，根据计算节点放置的不同，可获得高达1.26倍的性能提升潜力。此外，我们使用PRISM识别了为减少性能变异而优化的内核，并预测了在变异被放大的大规模作业中减速的概率。我们发现优化通信内核，如AllGather和ReduceScatter，对最小化训练步骤时间变异贡献最大。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large model training beyond tens of thousands of GPUs is an unchartedterritory. At such scales, disruptions to the training process are not a matterof if, but a matter of when -- a stochastic process degrading trainingproductivity. Dynamic runtime variation will become increasingly more frequentas training scales up and GPUs are operated in increasingly power-limited andthermally-stressed environments. At the 64k GPU scale, we already observed 9%GPU time variability for frontier foundation model training. To understandpotential causes of variability, we analyze GPU microbenchmarks at scale acrossa variety of platforms, showing up to 14% variation in GPU performance on GEMMworkloads depending on training hardware and deployed environment.  Motivated by our analysis and the large design space around performancevariability, we present PRISM -- a performance modeling framework thatconsiders the stochastic nature of the large-scale distributed training. Thecore of PRISM is the statistical method that provides a quantifiable measurefor probabilistic guarantees on training time. Using PRISM, we explore thedesign and optimization space of distributed training, from parallelizationmethods to next-generation training systems. PRISM is validated withreal-system measurement, showing training time prediction accuracy with 20.8%Kolmogorov-Smirnov distance. Using PRISM, we demonstrate that, depending oncomputation node placement, up to 1.26x performance improvement potential isavailable if we factor in sensitivities of parallelization strategies tovariation. In addition, we use PRISM to identify kernels to optimize forreducing performance variability and predict probability of slow-down forlarge-scale jobs where variation is magnified. We find optimizing communicationkernels, such as AllGather and ReduceScatter, contribute most to minimizingvariability in training step time.</description>
      <author>example@mail.com (Alicia Golden, Michael Kuchnik, Samuel Hsia, Zachary DeVito, Gu-Yeon Wei, David Brooks, Carole-Jean Wu)</author>
      <guid isPermaLink="false">2510.15596v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.15530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为VO-DP的视觉单视图扩散策略学习方法，利用预训练的视觉基础模型实现语义和几何特征的有效融合，在机器人操作任务中表现出色，特别是在真实世界任务中显著优于基于点云的方法。&lt;h4&gt;背景&lt;/h4&gt;在模仿学习中，基于视觉运动扩散策略学习是机器人操作的主要方向之一。大多数这类方法依赖点云作为观察输入，并通过点云特征学习构建场景表示，从而实现显著精度。然而，现有文献对纯视觉解决方案的深入探索不足，尽管这些方案具有巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;探索一种仅依赖视觉的单视图扩散策略学习方法(VO-DP)，以充分利用视觉基础模型的能力，实现语义和几何特征的有效融合，并评估其在模拟和真实世界任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为VO-DP的视觉单视图扩散策略学习方法，利用预训练的视觉基础模型实现语义和几何特征的有效融合。具体包括：利用VGGT的中间特征，融合DINOv2的语义特征和交替注意力块的几何特征，通过交叉注意力融合特征，并使用CNN进行空间压缩形成策略头输入。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在模拟任务中，VO-DP平均成功率达到64.6%，与DP3(64.0%)相当，远高于DP(34.8%)；2) 在真实世界任务中，VO-DP达到87.9%的成功率，显著优于DP3(67.5%)和DP(11.2%)；3) VO-DP在颜色、大小、背景和光照等变化条件下表现出高度稳定性；4) VO-DP在真实世界任务中的性能明显优于基于点云的方法DP3。&lt;h4&gt;结论&lt;/h4&gt;VO-DP是一种有效的视觉单视图扩散策略学习方法，在模拟和真实世界任务中均表现出色，特别是在真实环境中显著优于现有方法。该方法为机器人操作领域提供了一种纯视觉解决方案，展示了视觉基础模型在机器人学习中的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;在模仿学习的背景下，基于视觉运动的扩散策略学习是机器人操作的主要方向之一。大多数这些方法依赖点云作为观察输入，并通过点云特征学习构建场景表示，从而实现显著精度。然而，现有文献对纯视觉解决方案的深入探索不足，尽管这些方案具有巨大潜力。在本文中，我们提出了一种视觉单视图扩散策略学习方法(VO-DP)，利用预训练的视觉基础模型实现语义和几何特征的有效融合。我们利用VGGT的中间特征，融合DINOv2的语义特征和交替注意力块的几何特征。特征通过交叉注意力融合，并通过CNN进行空间压缩，形成策略头的输入。大量实验证明，VO-DP不仅显著优于纯视觉基线DP，而且与基于点云的方法DP3表现出不同的性能趋势：在模拟任务中，VO-DP平均成功率达到64.6%，与DP3的64.0%相当，远高于DP的34.8%；而在真实世界任务中，它达到87.9%，以显著优势分别超过DP3的67.5%和DP的11.2%。进一步的鲁棒性评估证实，VO-DP在颜色、大小、背景和光照等变化条件下保持高度稳定。最后，我们开源了一个机器人操作训练库。该库基于Accelerate构建，支持多机多GPU并行训练和混合精度训练。它兼容DP、DP3和VO-DP等视觉运动策略，并支持RoboTwin模拟器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the context of imitation learning, visuomotor-based diffusion policylearning is one of the main directions in robotic manipulation. Most of theseapproaches rely on point clouds as observation inputs and construct scenerepresentations through point clouds feature learning, which enables them toachieve remarkable accuracy. However, the existing literature lacks an in-depthexploration of vision-only solutions that have significant potential. In thispaper, we propose a Vision-Only and single-view Diffusion Policy learningmethod (VO-DP) that leverages pretrained visual foundation models to achieveeffective fusion of semantic and geometric features. We utilize intermediatefeatures from VGGT incorporating semantic features from DINOv2 and geometricfeatures from Alternating Attention blocks. Features are fused viacross-attention and spatially compressed with a CNN to form the input to thepolicy head. Extensive experiments demonstrate that VO-DP not only outperformsthe vision-only baseline DP significantly but also exhibits distinctperformance trends against the point cloud-based method DP3: in simulationtasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,outperforming both DP3 67.5% and DP 11.2% by a notable margin. Furtherrobustness evaluations confirm that VO-DP remains highly stable under varyingconditions including color, size, background, and lighting. Lastly, weopen-source a training library for robotic manipulation. Built on Accelerate,this library supports multi-machine and multi-GPU parallel training, as well asmixed precision training. It is compatible with visuomotor policies such as DP,DP3 and VO-DP, and also supports the RoboTwin simulator.</description>
      <author>example@mail.com (Zehao Ni, Yonghao He, Lingfeng Qian, Jilei Mao, Fa Fu, Wei Sui, Hu Su, Junran Peng, Zhipeng Wang, Bin He)</author>
      <guid isPermaLink="false">2510.15530v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction</title>
      <link>http://arxiv.org/abs/2510.15386v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种名为PFGS的姿态感知3D高斯飞溅框架，用于从多姿态图像捕获中重建完整物体。该方法通过迭代融合辅助姿态的图像到主姿态的统一3DGS表示中，结合全局和局部配准策略，有效解决了现有方法在重建遮挡或自遮挡区域时的不完整问题。&lt;h4&gt;背景&lt;/h4&gt;3D高斯飞溅的最新进展已实现了从多视图图像生成高质量、实时的novel-view synthesis。然而，大多数现有方法假设物体以单一静态姿态被捕获，导致重建不完整，缺失了被遮挡或自遮挡区域。此外，最近的3D基础模型在提高配准鲁棒性和效率方面取得了进展，但仍受限于高内存需求和次优准确性。&lt;h4&gt;目的&lt;/h4&gt;解决从多姿态图像捕获中重建完整物体的实际挑战，克服现有方法在重建遮挡区域时的不完整性问题，并解决基础模型在配准过程中的高内存需求和次优准确性问题。&lt;h4&gt;方法&lt;/h4&gt;PFGS姿态感知3DGS框架，给定物体在一个主姿态和几个辅助姿态的图像，迭代地将每个辅助集融合到主姿态的统一3DGS表示中。采用姿态感知融合策略，结合全局和局部配准来有效合并视图并优化3DGS模型。通过更智能地将基础模型整合到配准过程中来克服挑战：利用背景特征进行每姿态相机姿态估计，并使用基础模型进行跨姿态配准。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PFGS在定性和定量评估中始终优于强大的基线方法，产生更完整的重建和更高保真度的3DGS模型。&lt;h4&gt;结论&lt;/h4&gt;PFGS通过智能整合基础模型到配准过程中，结合了两种方法的优势，同时解决了背景不一致问题，实现了从多姿态图像捕获中重建完整物体的目标。&lt;h4&gt;翻译&lt;/h4&gt;最近的3D高斯飞溅(3DGS)进展已经能够从多视图图像实现高质量、实时的novel-view synthesis。然而，大多数现有方法假设物体以单一静态姿态被捕获，导致重建不完整，缺失了被遮挡或自遮挡区域。我们引入了PFGS，一种姿态感知的3DGS框架，解决了从多姿态图像捕获中重建完整物体的实际挑战。给定物体在一个主姿态和几个辅助姿态的图像，PFGS迭代地将每个辅助集融合到主姿态的统一3DGS表示中。我们的姿态感知融合策略结合了全局和局部配准，以有效合并视图并优化3DGS模型。虽然最近的3D基础模型进展提高了配准的鲁棒性和效率，但仍受限于高内存需求和次优准确性。PFGS通过更智能地将它们整合到配准过程中克服了这些挑战：它利用背景特征进行每姿态相机姿态估计，并使用基础模型进行跨姿态配准。这种设计结合了两种方法的优势，同时解决了背景不一致问题。实验结果表明，PFGS在定性和定量评估中始终优于强大的基线方法，产生更完整的重建和更高保真度的3DGS模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从多姿态图像捕获中重建完整3D物体的问题。现有的3D高斯泼溅方法假设物体在单一静态姿态下被捕获，导致重建不完整，特别是会错过被物体自身遮挡的区域。这个问题在现实中非常重要，因为完整3D重建对虚拟现实、增强现实、机器人技术和数字孪生等应用至关重要，而单一姿态无法获取物体的全部表面信息，特别是在自遮挡的情况下。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有3D高斯泼溅方法在处理多姿态物体重建时的局限性，并识别出多姿态重建面临的技术挑战：物体姿态变化导致无法使用传统SfM技术、跨姿态变化破坏对应估计、合并独立重建模型会引入伪影。作者借鉴了3D基础模型（如Fast3R）来提高配准鲁棒性，采用轮廓共识融合策略对齐不同姿态相机，并使用背景特征进行姿态估计。他们设计了一个三阶段管道（全局配准、局部配准、3DGS模型完成）来解决这些问题，并通过实验验证了方法的有效性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用姿态感知融合策略，将不同姿态捕获的图像有效合并到一个统一的3D高斯泼溅表示中，结合全局和局部配准技术处理多姿态重建挑战。整体流程包括：1）预处理阶段构建初始3DGS并估计相机姿态；2）全局配准阶段选择混合姿态图像、使用3D基础模型估计姿态、通过两阶段轮廓共识融合对齐坐标系；3）局部配准阶段使用轮廓引导和光度目标优化进一步精炼对齐；4）3DGS模型完成阶段使用平衡采样策略微调模型；5）迭代过程逐步融入更多辅助姿态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）提出PFGS框架实现多姿态3D高斯泼溅物体的增量重建；2）设计有效的全局配准方法对齐不同姿态图像集；3）提出混合姿态图像选择策略确保几何一致性；4）开发两阶段轮廓共识融合策略统一坐标系；5）提出平衡采样策略处理图像数量不平衡问题。相比之前工作，PFGS能处理物体姿态变化（不同于传统SfM），解决了3D基础模型的内存和精度问题，专门针对多姿态重建优化（不同于其他3D高斯泼溅方法），不依赖顺序输入结构（不同于在线重建方法），专注于合并不同姿态的部分重建（不同于物体聚焦方法）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PFGS通过姿态感知融合策略，将多姿态图像捕获合并为完整3D高斯泼溅表示，解决了传统方法在处理物体自遮挡和跨姿态变化时的局限性，实现了更完整、更高保真度的3D物体重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality,real-time novel-view synthesis from multi-view images. However, most existingmethods assume the object is captured in a single, static pose, resulting inincomplete reconstructions that miss occluded or self-occluded regions. Weintroduce PFGS, a pose-aware 3DGS framework that addresses the practicalchallenge of reconstructing complete objects from multi-pose image captures.Given images of an object in one main pose and several auxiliary poses, PFGSiteratively fuses each auxiliary set into a unified 3DGS representation of themain pose. Our pose-aware fusion strategy combines global and localregistration to merge views effectively and refine the 3DGS model. While recentadvances in 3D foundation models have improved registration robustness andefficiency, they remain limited by high memory demands and suboptimal accuracy.PFGS overcomes these challenges by incorporating them more intelligently intothe registration process: it leverages background features for per-pose camerapose estimation and employs foundation models for cross-pose registration. Thisdesign captures the best of both approaches while resolving backgroundinconsistency issues. Experimental results demonstrate that PFGS consistentlyoutperforms strong baselines in both qualitative and quantitative evaluations,producing more complete reconstructions and higher-fidelity 3DGS models.</description>
      <author>example@mail.com (Ting-Yu Yen, Yu-Sheng Chiu, Shih-Hsuan Hung, Peter Wonka, Hung-Kuo Chu)</author>
      <guid isPermaLink="false">2510.15386v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Symmetric Entropy-Constrained Video Coding for Machines</title>
      <link>http://arxiv.org/abs/2510.15347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is prepared to submit to the IEEE Transactions&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种对称熵约束的视频编码框架(SEC-VCM)，通过建立视频编解码器与视觉主干之间的对称对齐，优化机器视觉系统的视频编码效果。&lt;h4&gt;背景&lt;/h4&gt;视频传输越来越多地服务于机器视觉系统(MVS)而非人类视觉系统(HVS)，视频编码为机器(VCM)已成为关键研究课题。现有VCM方法通常将编解码器绑定到特定下游模型，需要重新训练或有监督数据，限制了多任务场景中的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种对称熵约束的视频编码框架用于机器(SEC-VCM)，建立视频编解码器与视觉主干之间的对称对齐，使编解码器能够利用视觉主干的表示能力保留语义信息并丢弃机器视觉系统无关的信息。&lt;h4&gt;方法&lt;/h4&gt;1) 提出对称熵约束的视频编码框架(SEC-VCM)；2) 建立视频编解码器与视觉主干之间的对称对齐；3) 采用双向熵约束(BiEC)机制确保视频解码和视觉主干编码过程的对称性；4) 通过语义-像素双路径融合(SPDF)模块将像素级先验注入到最终重建中。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该框架在速率-任务性能方面达到了最先进水平，与VTM相比，在视频实例分割(节省37.41%比特率)、视频对象分割(29.83%)、目标检测(46.22%)和多目标跟踪(44.94%)任务上实现了显著的比特率节省。&lt;h4&gt;结论&lt;/h4&gt;SEC-VCM框架通过建立视频编解码器与视觉主干之间的对称对齐，有效利用了视觉主干的表示能力，保留了语义信息并丢弃了机器视觉系统无关的信息，显著提高了机器导向的重建质量。&lt;h4&gt;翻译&lt;/h4&gt;随着视频传输越来越多地服务于机器视觉系统(MVS)而非人类视觉系统(HVS)，视频编码为机器(VCM)已成为关键研究课题。现有的VCM方法通常将编解码器绑定到特定下游模型，需要重新训练或有监督数据，从而限制了多任务场景中的泛化能力。最近，统一的VCM框架采用视觉主干(VB)和视觉基础模型(VFM)来支持单个编解码器完成多个视频理解任务。它们主要利用VB/VFM来保持语义一致性或抑制非语义信息，但很少探索如何在VB/VFM指导下直接将视频编码与理解联系起来。因此，我们提出了面向机器的对称熵约束视频编码框架(SEC-VCM)。它在视频编解码器和视觉主干之间建立了对称对齐，使编解码器能够利用视觉主干的表示能力来保留语义并丢弃机器视觉系统无关的信息。具体而言，双向熵约束(BiEC)机制通过抑制条件熵确保视频解码和视觉主干编码过程的对称性。这有助于编解码器明确处理对机器视觉系统有益的语义信息，同时压缩无用信息。此外，语义-像素双路径融合(SPDF)模块将像素级先验注入到最终重建中。通过语义-像素融合，它抑制了对机器视觉系统有害的伪影，并提高了机器导向的重建质量。实验结果表明，我们的框架在速率-任务性能方面达到了最先进水平，与VTM相比，在视频实例分割(节省37.41%)、视频对象分割(29.83%)、目标检测(46.22%)和多目标跟踪(44.94%)任务上实现了显著的比特率节省。我们将发布我们的代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As video transmission increasingly serves machine vision systems (MVS)instead of human vision systems (HVS), video coding for machines (VCM) hasbecome a critical research topic. Existing VCM methods often bind codecs tospecific downstream models, requiring retraining or supervised data and thuslimiting generalization in multi-task scenarios. Recently, unified VCMframeworks have employed visual backbones (VB) and visual foundation models(VFM) to support multiple video understanding tasks with a single codec. Theymainly utilize VB/VFM to maintain semantic consistency or suppress non-semanticinformation, but seldom explore how to directly link video coding withunderstanding under VB/VFM guidance. Hence, we propose a SymmetricEntropy-Constrained Video Coding framework for Machines (SEC-VCM). Itestablishes a symmetric alignment between the video codec and VB, allowing thecodec to leverage VB's representation capabilities to preserve semantics anddiscard MVS-irrelevant information. Specifically, a bi-directionalentropy-constraint (BiEC) mechanism ensures symmetry between the process ofvideo decoding and VB encoding by suppressing conditional entropy. This helpsthe codec to explicitly handle semantic information beneficial for MVS whilesqueezing useless information. Furthermore, a semantic-pixel dual-path fusion(SPDF) module injects pixel-level priors into the final reconstruction. Throughsemantic-pixel fusion, it suppresses artifacts harmful to MVS and improvesmachine-oriented reconstruction quality. Experimental results show ourframework achieves state-of-the-art (SOTA) in rate-task performance, withsignificant bitrate savings over VTM on video instance segmentation (37.41%),video object segmentation (29.83%), object detection (46.22%), and multipleobject tracking (44.94%). We will release our code.</description>
      <author>example@mail.com (Yuxiao Sun, Yao Zhao, Meiqin Liu, Chao Yao, Jian Jin, Weisi Lin)</author>
      <guid isPermaLink="false">2510.15347v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition</title>
      <link>http://arxiv.org/abs/2510.15280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨基础模型(FMs)如GPT-4和AlphaFold如何推动科学研究向新范式转变，提出三阶段框架描述这一演变过程，并回顾当前应用、识别风险与未来方向。&lt;h4&gt;背景&lt;/h4&gt;基础模型(FMs)如GPT-4和AlphaFold正在改变科学研究格局，它们不仅加速假设生成、实验设计和结果解释等任务，还引发了一个根本性问题：FMs是仅仅增强现有科学方法，还是重新定义科学研究的进行方式？&lt;h4&gt;目的&lt;/h4&gt;支持科学界理解基础模型(FMs)的变革作用，促进对科学发现未来的反思，并通过提出的三阶段框架帮助理解FMs如何催化科学研究的范式转变。&lt;h4&gt;方法&lt;/h4&gt;提出一个三阶段框架描述基础模型(FMs)在科学中的演变：(1)元科学整合阶段，FMs增强传统范式中的工作流程；(2)混合人机共创阶段，FMs成为问题制定、推理和发现的积极合作者；(3)自主科学发现阶段，FMs作为独立运行，能够在最少人类干预下生成新科学知识。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型(FMs)正在催化向新科学范式的转变，通过三个阶段逐步演变：从增强传统工作流程，到成为人机共创的积极合作者，最终实现自主科学发现。作者还回顾了FMs在现有科学范式中的应用和新兴能力，并确定了相关风险和未来发展方向。&lt;h4&gt;结论&lt;/h4&gt;基础模型(FMs)不仅仅是增强现有科学方法论的工具，而是正在重新定义科学研究的进行方式，推动科学向新范式转变。科学社区需要理解这一变革性作用，并反思科学发现的未来。&lt;h4&gt;翻译&lt;/h4&gt;基础模型(FMs)如GPT-4和AlphaFold正在重塑科学研究格局。除了加速假设生成、实验设计和结果解释等任务外，它们还引发了一个更根本的问题：FMs仅仅是增强现有科学方法论，还是重新定义了科学研究的进行方式？在本文中，我们认为FMs正在催化向新科学范式的转变。我们引入了一个三阶段框架来描述这一演变：(1)元科学整合，FMs增强传统范式中的工作流程；(2)混合人机共创，FMs成为问题制定、推理和发现的积极合作者；(3)自主科学发现，FMs作为独立运行，能够在最少人类干预下生成新科学知识。通过这一视角，我们回顾了FMs在现有科学范式中的应用和新兴能力。我们进一步确定了FMs赋能的科学发现的风险和未来方向。这篇立场论文旨在支持科学界理解FMs的变革作用，并促进对科学发现未来的反思。我们的项目可在https://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping thelandscape of scientific research. Beyond accelerating tasks such as hypothesisgeneration, experimental design, and result interpretation, they prompt a morefundamental question: Are FMs merely enhancing existing scientificmethodologies, or are they redefining the way science is conducted? In thispaper, we argue that FMs are catalyzing a transition toward a new scientificparadigm. We introduce a three-stage framework to describe this evolution: (1)Meta-Scientific Integration, where FMs enhance workflows within traditionalparadigms; (2) Hybrid Human-AI Co-Creation, where FMs become activecollaborators in problem formulation, reasoning, and discovery; and (3)Autonomous Scientific Discovery, where FMs operate as independent agentscapable of generating new scientific knowledge with minimal human intervention.Through this lens, we review current applications and emerging capabilities ofFMs across existing scientific paradigms. We further identify risks and futuredirections for FM-enabled scientific discovery. This position paper aims tosupport the scientific community in understanding the transformative role ofFMs and to foster reflection on the future of scientific discovery. Our projectis available athttps://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.</description>
      <author>example@mail.com (Fan Liu, Jindong Han, Tengfei Lyu, Weijia Zhang, Zhe-Rui Yang, Lu Dai, Cancheng Liu, Hao Liu)</author>
      <guid isPermaLink="false">2510.15280v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Reflections from Research Roundtables at the Conference on Health, Inference, and Learning (CHIL) 2025</title>
      <link>http://arxiv.org/abs/2510.15217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;第六届健康、推理和学习年度会议(CHIL 2025)于2025年6月在美国加州大学伯克利分校举行，会议举办了8个研究圆桌会议，促进机器学习与医疗保健交叉领域的协作讨论。&lt;h4&gt;背景&lt;/h4&gt;会议由健康学习与推理协会(AHLI)主办，于2025年6月25-27日在美国加州大学伯克利分校举行。&lt;h4&gt;目的&lt;/h4&gt;促进机器学习和医疗保健交叉领域的协作小组对话，讨论关键挑战、探索新兴机会、构思可行方向。&lt;h4&gt;方法&lt;/h4&gt;举办研究圆桌会议，每个圆桌会议由高级和初级主席共同主持，强调开放交流、智力好奇心和包容性参与。&lt;h4&gt;主要发现&lt;/h4&gt;会议涵盖了8个主题：'可解释性、可解释性和透明度'、'不确定性、偏见和公平性'、'因果关系'、'领域适应'、'基础模型'、'从小型医疗数据学习'、'多模态方法'和'可扩展、可转化的医疗保健解决方案'。&lt;h4&gt;结论&lt;/h4&gt;通过8个由19位圆桌主席主持的圆桌会议，会议促进了该领域的严格讨论、机会探索和方向构思。&lt;h4&gt;翻译&lt;/h4&gt;第六届健康、推理和学习年度会议(CHIL 2025)由健康学习与推理协会(AHLI)主办，于2025年6月25-27日在美国加州大学伯克利分校举行。作为今年计划的一部分，我们举办了研究圆桌会议，以促进机器学习和医疗保健交叉领域关键及时话题的协作小组对话。每个圆桌会议由高级和初级主席团队主持，他们促进了开放交流、智力好奇心和包容性参与。会议强调对关键挑战的严格讨论、新兴机会的探索以及该领域可行方向的集体构思。总共有19位圆桌主席主持了8个圆桌会议，主题包括'可解释性、可解释性和透明度'、'不确定性、偏见和公平性'、'因果关系'、'领域适应'、'基础模型'、'从小型医疗数据学习'、'多模态方法'和'可扩展、可转化的医疗保健解决方案'。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 6th Annual Conference on Health, Inference, and Learning (CHIL 2025),hosted by the Association for Health Learning and Inference (AHLI), was held inperson on June 25-27, 2025, at the University of California, Berkeley, inBerkeley, California, USA. As part of this year's program, we hosted ResearchRoundtables to catalyze collaborative, small-group dialogue around critical,timely topics at the intersection of machine learning and healthcare. Eachroundtable was moderated by a team of senior and junior chairs who fosteredopen exchange, intellectual curiosity, and inclusive engagement. The sessionsemphasized rigorous discussion of key challenges, exploration of emergingopportunities, and collective ideation toward actionable directions in thefield. In total, eight roundtables were held by 19 roundtable chairs on topicsof "Explainability, Interpretability, and Transparency," "Uncertainty, Bias,and Fairness," "Causality," "Domain Adaptation," "Foundation Models," "Learningfrom Small Medical Data," "Multimodal Methods," and "Scalable, TranslationalHealthcare Solutions."</description>
      <author>example@mail.com (Emily Alsentzer, Marie-Laure Charpignon, Bill Chen, Niharika D'Souza, Jason Fries, Yixing Jiang, Aparajita Kashyap, Chanwoo Kim, Simon Lee, Aishwarya Mandyam, Ashery Christopher Mbilinyi, Nikita Mehandru, Nitish Nagesh, Brighton Nuwagira, Emma Pierson, Arvind Pillai, Akane Sano, Tanveer Syeda-Mahmood, Shashank Yadav, Elias Adhanom, Muhammad Umar Afza, Amelia Archer, Suhana Bedi, Vasiliki Bikia, Trenton Chang, George H. Chen, Winston Chen, Erica Chiang, Edward Choi, Octavia Ciora, Paz Dozie-Nnamah, Shaza Elsharief, Matthew Engelhard, Ali Eshragh, Jean Feng, Josh Fessel, Scott Fleming, Kei Sen Fong, Thomas Frost, Soham Gadgil, Judy Gichoya, Leeor Hershkovich, Sujeong Im, Bhavya Jain, Vincent Jeanselme, Furong Jia, Qixuan, Jin, Yuxuan Jin, Daniel Kapash, Geetika Kapoor, Behdokht Kiafar, Matthias Kleiner, Stefan Kraft, Annika Kumar, Daeun Kyung, Zhongyuan Liang, Joanna Lin, Qianchu, Liu, Chang Liu, Hongzhou Luan, Chris Lunt, Leopoldo Julían Lechuga López, Matthew B. A. McDermott, Shahriar Noroozizadeh, Connor O'Brien, YongKyung Oh, Mixail Ota, Stephen Pfohl, Meagan Pi, Tanmoy Sarkar Pias, Emma Rocheteau, Avishaan Sethi, Toru Shirakawa, Anita Silver, Neha Simha, Kamile Stankeviciute, Max Sunog, Peter Szolovits, Shengpu Tang, Jialu Tang, Aaron Tierney, John Valdovinos, Byron Wallace, Will Ke Wang, Peter Washington, Jeremy Weiss, Daniel Wolfe, Emily Wong, Hye Sun Yun, Xiaoman Zhang, Xiao Yu Cindy Zhang, Hayoung Jeong, Kaveri A. Thakoor)</author>
      <guid isPermaLink="false">2510.15217v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Dissecting Mahalanobis: How Feature Geometry and Normalization Shape OOD Detection</title>
      <link>http://arxiv.org/abs/2510.15202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了分布外(OOD)检测中马氏距离方法的可靠性问题，分析了表示几何和规范化对性能的影响，并提出了一种新的径向缩放ℓ2规范化方法。&lt;h4&gt;背景&lt;/h4&gt;OOD检测对于深度学习模型的可靠部署至关重要，而马氏距离方法虽被广泛使用，但其表示几何和规范化对性能的影响尚未被充分理解，这可能限制其下游应用。&lt;h4&gt;目的&lt;/h4&gt;解决对马氏距离方法中表示几何和规范化影响理解不足的问题，通过全面的实证研究探索这些因素与OOD性能的关系。&lt;h4&gt;方法&lt;/h4&gt;研究进行了跨不同图像基础模型、数据集和距离规范化方案的实证分析，定义了数据表示的理想几何形状，分析了规范化对OOD性能的影响，并提出了径向缩放ℓ2规范化方法。&lt;h4&gt;主要发现&lt;/h4&gt;基于马氏距离的方法并非普遍可靠；光谱和内在维度指标可以准确预测模型的OOD性能；规范化对OOD性能有显著影响。&lt;h4&gt;结论&lt;/h4&gt;提出的径向缩放ℓ2规范化方法通过引入可调参数控制特征空间的径向几何，系统性收缩或扩展表示以显著提高OOD检测性能，为设计更有效和可靠的深度学习模型提供了新见解。&lt;h4&gt;翻译&lt;/h4&gt;该研究探讨了分布外(OOD)检测中马氏距离方法的可靠性问题，分析了表示几何和规范化对性能的影响，并提出了一种新的径向缩放ℓ2规范化方法。OOD检测对于深度学习模型的可靠部署至关重要，而马氏距离方法虽被广泛使用，但其表示几何和规范化对性能的影响尚未被充分理解，这可能限制其下游应用。研究旨在解决对马氏距离方法中表示几何和规范化影响理解不足的问题，通过全面的实证研究探索这些因素与OOD性能的关系。研究进行了跨不同图像基础模型、数据集和距离规范化方案的实证分析，定义了数据表示的理想几何形状，分析了规范化对OOD性能的影响，并提出了径向缩放ℓ2规范化方法。主要发现包括：基于马氏距离的方法并非普遍可靠；光谱和内在维度指标可以准确预测模型的OOD性能；规范化对OOD性能有显著影响。结论是，提出的径向缩放ℓ2规范化方法通过引入可调参数控制特征空间的径向几何，系统性收缩或扩展表示以显著提高OOD检测性能，为设计更有效和可靠的深度学习模型提供了新见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Out-of-distribution (OOD) detection is critical for the reliable deploymentof deep learning models. hile Mahalanobis distance methods are widely used, theimpact of representation geometry and normalization on their performance is notfully understood, which may limit their downstream application. To address thisgap, we conducted a comprehensive empirical study across diverse imagefoundation models, datasets, and distance normalization schemes. First, ouranalysis shows that Mahalanobis-based methods aren't universally reliable.Second, we define the ideal geometry for data representations and demonstratethat spectral and intrinsic-dimensionality metrics can accurately predict amodel's OOD performance. Finally, we analyze how normalization impacts OODperformance. Building upon these studies, we propose radially scaled $\ell_2$normalization, a method that generalizes the standard $\ell_2$ normalizationrecently applied to Mahalanobis-based OOD detection. Our approach introduces atunable parameter to directly control the radial geometry of the feature space,systematically contracting or expanding representations to significantlyimprove OOD detection performance. By bridging the gap between representationgeometry, normalization, and OOD performance, our findings offer new insightsinto the design of more effective and reliable deep learning models.</description>
      <author>example@mail.com (Denis Janiak, Jakub Binkowski, Tomasz Kajdanowicz)</author>
      <guid isPermaLink="false">2510.15202v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>The Economics of AI Foundation Models: Openness, Competition, and Governance</title>
      <link>http://arxiv.org/abs/2510.15200v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究分析了基础模型生态系统中开放性的战略选择及其经济影响，发现开放性具有双重效应，并揭示了现有开发者的最优开放性策略与数据飞轮效应强度呈非单调关系，形成了'开放性陷阱'现象。&lt;h4&gt;背景&lt;/h4&gt;基础模型(FM)生态系统中'开放性'的战略选择已成为一个关键问题，但其背后的经济驱动因素尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;分析开放性如何影响AI价值链中的竞争，包括现有开发者、下游部署者和新进入开发者之间的互动关系。&lt;h4&gt;方法&lt;/h4&gt;构建了一个两期博弈论模型，研究开放性对竞争格局的影响。&lt;h4&gt;主要发现&lt;/h4&gt;开放性具有双重效应：增强知识溢出到新进入者，同时通过'数据飞轮效应'增强现有开发者优势；现有开发者的最优开放性策略与数据飞轮效应强度呈非单调关系；中等数据飞轮效应下，现有开发者会战略性地限制开放性；形成了'开放性陷阱'，即透明度要求可能适得其反；垂直整合和政府补贴等干预措施也可能无效。&lt;h4&gt;结论&lt;/h4&gt;通过建模开发者对竞争和监管压力的战略反应，为分析复杂且快速发展的FM生态系统中的竞争和设计有效政策提供了稳健的框架。&lt;h4&gt;翻译&lt;/h4&gt;基础模型生态系统中'开放性'的战略选择已成为一个关键问题。虽然这一选择引发了激烈辩论，但其背后的经济驱动因素尚未得到充分探索。我们构建了一个两期博弈论模型，分析开放性如何影响AI价值链中的竞争，涉及现有开发者、下游部署者和新进入开发者。开放性产生双重效应：它增强了知识溢出到新进入者，但也通过'数据飞轮效应'增强了现有开发者的优势，即更大的用户参与度进一步降低了部署者未来的微调成本。我们的分析显示，现有开发者的第一期最优开放性强度与数据飞轮效应强度呈非单调关系。当数据飞轮效应较弱或非常强时，现有开发者偏好更高水平的开放性；然而，在中等范围内，它会战略性地限制开放性以损害新进入者的学习。这种动态导致了'开放性陷阱'，这是一个关键的政策悖论，即透明度要求可能适得其反，消除企业的战略灵活性，减少投资并降低福利。我们扩展了模型，表明其他常见的干预措施可能同样无效。例如，垂直整合只有在数据飞轮效应足够强到克服潜在更高效竞争对手的损失时才有利于生态系统。同样，旨在促进采用的政府补贴可能被现有开发者通过战略性的价格和开放性调整完全获取，使价值链的其他部分处境更差。通过建模开发者对竞争和监管压力的战略反应，我们为在复杂且快速发展的FM生态系统中分析竞争和设计有效政策提供了稳健的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The strategic choice of model "openness" has become a defining issue for thefoundation model (FM) ecosystem. While this choice is intensely debated, itsunderlying economic drivers remain underexplored. We construct a two-periodgame-theoretic model to analyze how openness shapes competition in an AI valuechain, featuring an incumbent developer, a downstream deployer, and an entrantdeveloper. Openness exerts a dual effect: it amplifies knowledge spillovers tothe entrant, but it also enhances the incumbent's advantage through a "dataflywheel effect," whereby greater user engagement today further lowers thedeployer's future fine-tuning cost. Our analysis reveals that the incumbent'soptimal first-period openness is surprisingly non-monotonic in the strength ofthe data flywheel effect. When the data flywheel effect is either weak or verystrong, the incumbent prefers a higher level of openness; however, for anintermediate range, it strategically restricts openness to impair the entrant'slearning. This dynamic gives rise to an "openness trap," a critical policyparadox where transparency mandates can backfire by removing firms' strategicflexibility, reducing investment, and lowering welfare. We extend the model toshow that other common interventions can be similarly ineffective. Verticalintegration, for instance, only benefits the ecosystem when the data flywheeleffect is strong enough to overcome the loss of a potentially more efficientcompetitor. Likewise, government subsidies intended to spur adoption can becaptured entirely by the incumbent through strategic price and opennessadjustments, leaving the rest of the value chain worse off. By modeling thedeveloper's strategic response to competitive and regulatory pressures, weprovide a robust framework for analyzing competition and designing effectivepolicy in the complex and rapidly evolving FM ecosystem.</description>
      <author>example@mail.com (Fasheng Xu, Xiaoyu Wang, Wei Chen, Karen Xie)</author>
      <guid isPermaLink="false">2510.15200v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Hyperparameter Optimization and Reproducibility in Deep Learning Model Training</title>
      <link>http://arxiv.org/abs/2510.15164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了病理学基础模型训练中的可重复性挑战，调查了软件随机性、硬件非确定性和超参数报告不一致性问题，并通过系统评估不同超参数设置和数据增强策略的影响，提供了实用规则来指导未来开发可重复的数字病理学基础模型。&lt;h4&gt;背景&lt;/h4&gt;基础模型训练在病理学领域面临可重复性挑战，这些挑战通常由软件随机性、硬件非确定性以及超参数报告不一致性所阻碍。&lt;h4&gt;目的&lt;/h4&gt;调查这些问题，通过系统评估不同超参数设置和数据增强策略对模型性能的影响。&lt;h4&gt;方法&lt;/h4&gt;在QUILT-1M数据集上训练了一个CLIP模型，并在三个下游病理学数据集（PatchCamelyon、LC25000-Lung和LC25000-Colon）上系统评估了不同超参数设置和数据增强策略的影响。&lt;h4&gt;主要发现&lt;/h4&gt;• 图像裁剪策略中，中等程度的随机裁剪比更激进或更保守的设置表现更好；• 不带局部损失的分布式训练提高了模型稳定性；• 较低的学习率在所有数据集上均降低了模型性能；• 结肠组织数据集提供了最可重复的基准测试结果。&lt;h4&gt;结论&lt;/h4&gt;计算病理学中的可重复性不仅依赖于透明的文档记录，还依赖于精心选择的实验配置，并提供了实用规则来指导未来开发可重复的数字病理学基础模型的工作。&lt;h4&gt;翻译&lt;/h4&gt;可重复性仍然是病理学基础模型训练中的一个关键挑战，常常受到软件随机性、硬件非确定性和不一致的超参数报告的阻碍。为了调查这些问题，我们在QUILT-1M数据集上训练了一个CLIP模型，并系统评估了不同超参数设置和数据增强策略在三个下游病理学数据集（PatchCamelyon、LC25000-Lung和LC25000-Colon）上的影响。尽管不同运行之间存在变异性，我们确定了明显的趋势：RandomResizedCrop值为0.7-0.8比更激进(0.6)或保守(0.9)的设置表现更好，不带局部损失的分布式训练提高了稳定性，学习率低于5.0e-5在所有数据集上一致降低了性能。LC25000(Colon)数据集始终提供了最可重复的基准。这些发现强调，计算病理学中的可重复性不仅依赖于透明的文档记录，还依赖于精心选择的实验配置，我们提供了实用规则来指导未来开发可重复的数字病理学基础模型的工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reproducibility remains a critical challenge in foundation model training forhistopathology, often hindered by software randomness, hardwarenon-determinism, and inconsistent hyperparameter reporting. To investigatethese issues, we trained a CLIP model on the QUILT-1M dataset andsystematically evaluated the impact of different hyperparameter settings andaugmentation strategies across three downstream histopathology datasets(PatchCamelyon, LC25000-Lung, and LC25000-Colon). Despite variability acrossruns, we identified clear trends: RandomResizedCrop values of 0.7-0.8outperformed more aggressive (0.6) or conservative (0.9) settings, distributedtraining without local loss improved stability, and learning rates below 5.0e-5consistently degraded performance across all datasets. The LC25000 (Colon)dataset consistently provided the most reproducible benchmark. These findingshighlight that reproducibility in computational pathology depends not only ontransparent documentation but also on carefully chosen experimentalconfigurations, and we provide practical rules to guide future efforts indeveloping reproducible foundation models for digital pathology.</description>
      <author>example@mail.com (Usman Afzaal, Ziyu Su, Usama Sajjad, Hao Lu, Mostafa Rezapour, Metin Nafi Gurcan, Muhammad Khalid Khan Niazi)</author>
      <guid isPermaLink="false">2510.15164v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning</title>
      <link>http://arxiv.org/abs/2510.15026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MOBIUS是一个高效的基础模型家族，专为通用实例分割设计，能够在保持高性能的同时大幅减少计算需求，实现了从高端加速器到移动硬件的跨设备部署。&lt;h4&gt;背景&lt;/h4&gt;扩大模型规模和训练数据推动了基础模型在实例级感知方面的发展，在目标检测和分割任务中取得了最先进的性能，但高计算成本限制了它们在资源受限平台上的应用。&lt;h4&gt;目的&lt;/h4&gt;研究现有架构在实现高效边缘部署方面的局限性，同时不牺牲性能；引入MOBIUS基础模型家族，实现帕累托最优的缩放，支持跨设备部署。&lt;h4&gt;方法&lt;/h4&gt;提出瓶颈像素解码器用于高效多尺度多模态融合；提出语言引导的不确定性校准损失用于自适应解码器剪枝；提出简化的统一训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;MOBIUS将像素和transformer解码器的FLOPs分别减少了高达55%和75%，同时仅用三分之一训练迭代次数就保持了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;MOBIUS为高性能计算平台和移动设备上的高效分割建立了新的基准。&lt;h4&gt;翻译&lt;/h4&gt;扩大模型规模和训练数据推动了基础模型在实例级感知方面的发展，在目标检测和分割任务中实现了最先进的领域内和零样本性能。然而，它们的高计算成本限制了在资源受限平台上的应用。我们首先研究了现有架构在实现高效边缘部署而不牺牲性能方面的局限性。然后我们引入了MOBIUS，一个用于通用实例分割的基础模型家族，设计为帕累托最优的缩放，支持从高端加速器到移动硬件的跨设备部署。为了减少训练和推理需求，我们提出了：(i)用于高效多尺度多模态融合的瓶颈像素解码器，(ii)用于自适应解码器剪枝的语言引导不确定性校准损失，以及(iii)简化的统一训练策略。与权衡准确性和减少复杂性的高效基线不同，MOBIUS将像素和transformer解码器的FLOPs分别减少了高达55%和75%，同时在仅三分之一的训练迭代次数内保持了最先进的性能。MOBIUS为高性能计算平台和移动设备上的高效分割建立了新基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scaling up model size and training data has advanced foundation models forinstance-level perception, achieving state-of-the-art in-domain and zero-shotperformance across object detection and segmentation. However, their highcomputational cost limits adoption on resource-constrained platforms. We firstexamine the limitations of existing architectures in enabling efficient edgedeployment without compromising performance. We then introduce MOBIUS, a familyof foundation models for universal instance segmentation, designed forPareto-optimal downscaling to support deployment across devices ranging fromhigh-end accelerators to mobile hardware. To reduce training and inferencedemands, we propose: (i) a bottleneck pixel decoder for efficient multi-scaleand multi-modal fusion, (ii) a language-guided uncertainty calibration loss foradaptive decoder pruning, and (iii) a streamlined, unified training strategy.Unlike efficient baselines that trade accuracy for reduced complexity, MOBIUSreduces pixel and transformer decoder FLOPs by up to 55% and 75%, respectively,while maintaining state-of-the-art performance in just a third of the trainingiterations. MOBIUS establishes a new benchmark for efficient segmentation onboth high-performance computing platforms and mobile devices.</description>
      <author>example@mail.com (Mattia Segu, Marta Tintore Gazulla, Yongqin Xian, Luc Van Gool, Federico Tombari)</author>
      <guid isPermaLink="false">2510.15026v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Vision-Centric Activation and Coordination for Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2510.14349v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VaCo是一种优化多模态大语言模型(MLLMs)表示的新方法，通过整合多个视觉基础模型(VFMs)的视觉中心激活和协调，显著提高了MLLMs在视觉理解方面的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)通过整合视觉编码器的图像特征与LLMs展示先进理解能力，但主流MLLMs仅受文本token的下一词预测监督，忽略了分析能力所需的关键视觉中心信息。&lt;h4&gt;目的&lt;/h4&gt;解决主流MLLMs忽略视觉中心信息的问题，提高MLLMs的视觉理解能力和分析能力。&lt;h4&gt;方法&lt;/h4&gt;引入VaCo方法，包括：1)视觉判别对齐整合VFMs提取的任务感知特征；2)在MLLMs中融入可学习的模块化任务查询(MTQs)和视觉对齐层(VALs)；3)设计令牌网关掩码(TGM)协调多个VFMs之间的表示冲突。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，VaCo在各种基准测试中显著提高了不同MLLMs的性能，展示了其在视觉理解方面的优越能力。&lt;h4&gt;结论&lt;/h4&gt;VaCo通过整合多个VFMs的视觉特征，有效解决了主流MLLMs忽略视觉中心信息的问题，提高了MLLMs的综合性能。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)整合视觉编码器中的图像特征与LLMs，展示了先进的理解能力。然而，主流MLLMs仅受文本token的下一词预测监督，忽略了分析能力所需的关键视觉中心信息。为解决这一困境，我们引入了VaCo，它通过多个视觉基础模型(VFMs)的视觉中心激活和协调来优化MLLM表示。VaCo引入视觉判别对齐来整合从VFMs提取的任务感知特征，从而统一MLLM中文本和视觉输出的优化。具体而言，我们将可学习的模块化任务查询(MTQs)和视觉对齐层(VALs)整合到MLLMs中，在多样化VFMs的监督下激活特定的视觉信号。为协调VFMs之间的表示冲突，精心设计的令牌网关掩码(TGM)限制了多组MTQs之间的信息流动。大量实验表明，VaCo在各种基准测试中显著提高了不同MLLMs的性能，展示了其在视觉理解方面的卓越能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) integrate image features from visualencoders with LLMs, demonstrating advanced comprehension capabilities. However,mainstream MLLMs are solely supervised by the next-token prediction of textualtokens, neglecting critical vision-centric information essential for analyticalabilities. To track this dilemma, we introduce VaCo, which optimizes MLLMrepresentations through Vision-Centric activation and Coordination frommultiple vision foundation models (VFMs). VaCo introduces visual discriminativealignment to integrate task-aware perceptual features extracted from VFMs,thereby unifying the optimization of both textual and visual outputs in MLLMs.Specifically, we incorporate the learnable Modular Task Queries (MTQs) andVisual Alignment Layers (VALs) into MLLMs, activating specific visual signalsunder the supervision of diverse VFMs. To coordinate representation conflictsacross VFMs, the crafted Token Gateway Mask (TGM) restricts the informationflow among multiple groups of MTQs. Extensive experiments demonstrate that VaCosignificantly improves the performance of different MLLMs on variousbenchmarks, showcasing its superior capabilities in visual comprehension.</description>
      <author>example@mail.com (Yunnan Wang, Fan Lu, Kecheng Zheng, Ziyuan Huang, Ziqiang Li, Wenjun Zeng, Xin Jin)</author>
      <guid isPermaLink="false">2510.14349v2</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos</title>
      <link>http://arxiv.org/abs/2510.15018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical report. Project page: https://urbanverseproject.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UrbanVerse是一个数据驱动的真实到仿真系统，将众包城市旅游视频转换为具有物理感知能力的交互式仿真场景，包含10万多个带注释的城市3D资产库和自动场景提取管道，显著提高了城市AI代理训练的效果和导航策略的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;城市中的实体AI代理（如配送机器人、四足机器人等）日益增多，它们需要在混乱的城市街道中导航以提供最后一公里连接。训练这类代理需要多样化的、高保真的城市环境，但现有的人工制作或程序生成的仿真场景要么缺乏可扩展性，要么无法捕捉真实世界的复杂性。&lt;h4&gt;目的&lt;/h4&gt;引入UrbanVerse，一个数据驱动的真实到仿真系统，将众包的城市旅游视频转换为具有物理感知能力的交互式仿真场景。&lt;h4&gt;方法&lt;/h4&gt;UrbanVerse包括两个部分：UrbanVerse-100K（包含10万多个带注释的城市3D资产库，具有语义和物理属性）和UrbanVerse-Gen（一个自动管道，从视频中提取场景布局，并使用检索到的资产创建度量尺度的3D仿真）。在IsaacSim中运行，提供来自24个国家的160个高质量构建场景和10个艺术家设计的测试场景的精选基准。&lt;h4&gt;主要发现&lt;/h4&gt;UrbanVerse场景保留了真实世界的语义和布局，实现了与人工制作场景相当的人评估真实感。在城市导航中，在UrbanVerse中训练的策略显示出扩展幂律和强大的泛化能力，与先前的方法相比，在仿真中成功率提高了6.3%，在零样本仿真到真实迁移中提高了30.1%，仅用两次干预就完成了300米的真实世界任务。&lt;h4&gt;结论&lt;/h4&gt;UrbanVerse系统能够有效地将真实世界的城市环境转化为高质量的仿真场景，为训练城市AI代理提供了有效工具，并显著提高了导航策略的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;城市实体AI代理，从配送机器人到四足机器人，正日益增多地遍布我们的城市，在混乱的街道上导航以提供最后一公里的连接。训练此类代理需要多样化、高保真的城市环境来扩展规模，然而现有的人工制作或程序生成的仿真场景要么缺乏可扩展性，要么无法捕捉真实世界的复杂性。我们引入了UrbanVerse，一个数据驱动的真实到仿真系统，将众包的城市旅游视频转换为具有物理感知能力的交互式仿真场景。UrbanVerse包括：(i) UrbanVerse-100K，一个包含10万多个带注释的城市3D资产库，具有语义和物理属性，以及(ii) UrbanVerse-Gen，一个自动管道，从视频中提取场景布局，并使用检索到的资产创建度量尺度的3D仿真。在IsaacSim中运行，UrbanVerse提供了来自24个国家的160个高质量构建场景，以及10个艺术家设计的测试场景的精选基准。实验表明，UrbanVerse场景保留了真实世界的语义和布局，实现了与人工制作场景相当的人评估真实感。在城市导航中，在UrbanVerse中训练的策略显示出扩展幂律和强大的泛化能力，与先前的方法相比，在仿真中成功率提高了6.3%，在零样本仿真到真实迁移中提高了30.1%，仅用两次干预就完成了300米的真实世界任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决城市环境中具身AI体（如配送机器人、四足机器人等）训练所需的高保真、多样化城市环境不足的问题。这个问题很重要，因为随着城市中微型移动系统的兴起，这些AI体需要能够泛化到各种复杂的真实世界环境中，但现有的模拟场景要么缺乏可扩展性，要么无法捕捉真实世界的复杂性和动态变化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过认识到需要两个关键元素来构建解决方案：大规模3D资产数据库和自动化场景生成管道。他们借鉴了现有工作如Objaverse等3D资产库，但解决了其中资产质量、相关性和标注不足的问题；同时结合了MASt3R、YoloWorld、SAM2等多种技术来构建UrbanVerse-Gen管道，实现从未校准视频中提取场景信息。还参考了数字孪生概念，但扩展到城市街道级别场景生成。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个数据驱动的真实到模拟系统，将真实世界城市旅游视频转换为具有物理感知的交互式模拟场景，保留真实世界的语义、布局和物理特性。整体流程分为两大部分：1) UrbanVerse-100K资产数据库：收集和标注102,530个高质量3D城市对象，每个带有33种属性；2) UrbanVerse-Gen管道：从视频中提取场景信息，检索匹配资产，并在IsaacSim中生成可交互的数字孪生场景，包括场景蒸馏、资产匹配和场景组装三个阶段。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) UrbanVerse-100K大规模高质量标注资产库；2) UrbanVerse-Gen从未校准视频中自动生成高保真城市场景的管道；3) 跨24个国家的160场景训练库。相比之前工作，不同之处在于：解决了现有模拟器场景真实性不足、资产多样性有限和物理标注缺乏的问题；超越了仅使用被动数据训练的方法，通过交互式模拟实现更好的障碍物避免；扩展了数字孪生概念到城市街道级别，而非仅限于室内环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UrbanVerse通过将众源城市旅游视频转换为具有物理感知的交互式模拟场景，解决了城市环境中具身AI体训练所需的高保真、可扩展城市环境问题，实现了真实世界分布的保真度并显著提升了策略在模拟到真实世界零样本迁移中的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban embodied AI agents, ranging from delivery robots to quadrupeds, areincreasingly populating our cities, navigating chaotic streets to providelast-mile connectivity. Training such agents requires diverse, high-fidelityurban environments to scale, yet existing human-crafted or procedurallygenerated simulation scenes either lack scalability or fail to capturereal-world complexity. We introduce UrbanVerse, a data-driven real-to-simsystem that converts crowd-sourced city-tour videos into physics-aware,interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, arepository of 100k+ annotated urban 3D assets with semantic and physicalattributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scenelayouts from video and instantiates metric-scale 3D simulations using retrievedassets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructedscenes from 24 countries, along with a curated benchmark of 10 artist-designedtest scenes. Experiments show that UrbanVerse scenes preserve real-worldsemantics and layouts, achieving human-evaluated realism comparable to manuallycrafted scenes. In urban navigation, policies trained in UrbanVerse exhibitscaling power laws and strong generalization, improving success by +6.3% insimulation and +30.1% in zero-shot sim-to-real transfer comparing to priormethods, accomplishing a 300 m real-world mission with only two interventions.</description>
      <author>example@mail.com (Mingxuan Liu, Honglin He, Elisa Ricci, Wayne Wu, Bolei Zhou)</author>
      <guid isPermaLink="false">2510.15018v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.15430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Learning to Detect (LoD)框架，用于有效检测大型视觉语言模型中的未知越狱攻击，解决了现有检测方法的泛化性和效率问题。&lt;h4&gt;背景&lt;/h4&gt;尽管进行了广泛的对齐努力，大型视觉语言模型(LVLMs)仍然容易受到越狱攻击，带来严重的安全风险。&lt;h4&gt;目的&lt;/h4&gt;克服现有检测方法的局限性，提出一种能够准确检测未知越狱攻击的通用框架。&lt;h4&gt;方法&lt;/h4&gt;提出Learning to Detect (LoD)框架，将重点从特定攻击学习转向特定任务学习。该框架包括多模态安全概念激活向量模块用于安全导向的表征学习和安全模式自动编码器模块用于无监督攻击分类。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该方法在各种未知攻击上实现了持续更高的检测AUROC，同时提高了效率。&lt;h4&gt;结论&lt;/h4&gt;LoD框架通过转变学习方式，有效解决了现有检测方法在泛化性和效率方面的局限性，能够更准确、高效地检测未知攻击。&lt;h4&gt;翻译&lt;/h4&gt;尽管进行了广泛的对齐努力，大型视觉语言模型(LVLMs)仍然容易受到越狱攻击，带来严重的安全风险。为解决这个问题，现有的检测方法要么学习特定攻击的参数，这阻碍了它们对未见攻击的泛化能力；要么依赖于启发式原则，这限制了准确性和效率。为克服这些局限性，我们提出了Learning to Detect (LoD)框架，一种通过将重点从特定攻击学习转向特定任务学习来准确检测未知越狱攻击的通用框架。该框架包括用于安全导向表征学习的多模态安全概念激活向量模块和用于无监督攻击分类的安全模式自动编码器模块。大量实验表明，我们的方法在各种未知攻击上实现了持续更高的检测AUROC，同时提高了效率。代码可在https://anonymous.4open.science/r/Learning-to-Detect-51CB获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)remain vulnerable to jailbreak attacks, posing serious safety risks. To addressthis, existing detection methods either learn attack-specific parameters, whichhinders generalization to unseen attacks, or rely on heuristically soundprinciples, which limit accuracy and efficiency. To overcome these limitations,we propose Learning to Detect (LoD), a general framework that accuratelydetects unknown jailbreak attacks by shifting the focus from attack-specificlearning to task-specific learning. This framework includes a Multi-modalSafety Concept Activation Vector module for safety-oriented representationlearning and a Safety Pattern Auto-Encoder module for unsupervised attackclassification. Extensive experiments show that our method achievesconsistently higher detection AUROC on diverse unknown attacks while improvingefficiency. The code is available athttps://anonymous.4open.science/r/Learning-to-Detect-51CB.</description>
      <author>example@mail.com (Shuang Liang, Zhihao Xu, Jialing Tao, Hui Xue, Xiting Wang)</author>
      <guid isPermaLink="false">2510.15430v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Large-scale User Game Lifecycle Representation Learning</title>
      <link>http://arxiv.org/abs/2510.15412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用户游戏生命周期(UGL)方法来解决游戏推荐系统中的稀疏性和不平衡性问题，通过创新策略显著提升了游戏广告和推荐的性能。&lt;h4&gt;背景&lt;/h4&gt;视频游戏产业快速发展，需要在线游戏平台开发有效的广告和推荐系统。现有的推荐系统表示学习方法不适合游戏场景，主要面临游戏稀疏性和不平衡性挑战。&lt;h4&gt;目的&lt;/h4&gt;解决游戏推荐系统中的游戏稀疏性和不平衡性问题，提升游戏广告和推荐的性能。&lt;h4&gt;方法&lt;/h4&gt;引入用户游戏生命周期(UGL)丰富用户行为；提出两种创新策略操纵用户行为以提取短期和长期兴趣；采用逆概率掩码策略处理游戏不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;UGL表示显著提升了模型性能，离线实验显示游戏广告AUC平均增加1.83%，游戏内物品推荐AUC增加0.5%；在线实验显示游戏广告CVR平均增加21.67%，游戏内物品推荐ARPU增加0.82%。&lt;h4&gt;结论&lt;/h4&gt;UGL表示方法能有效解决游戏稀疏性和不平衡性问题，显著提升游戏广告和推荐的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;视频游戏生产的快速发展需要为在线游戏平台开发有效的广告和推荐系统。向用户推荐和宣传游戏取决于捕捉他们对游戏的兴趣。然而，为处理推荐系统中的数十亿物品而设计的现有表示学习方法不适合游戏广告和推荐。这主要是由于游戏稀疏性，其中仅有的几百个游戏不足以进行大规模用户表示学习，以及游戏不平衡性，其中用户行为被少数热门游戏主导。为解决稀疏性问题，我们引入了用户游戏生命周期(UGL)，旨在丰富用户在游戏中的行为。此外，我们提出了两种创新策略，旨在操纵用户行为以更有效地提取短期和长期兴趣。为解决游戏不平衡挑战，我们提出了用于UGL表示学习的逆概率掩码策略。离线和在线实验结果表明，UGL表示通过在游戏广告中平均实现1.83%的离线AUC增长和21.67%的在线CVR增长，以及在游戏内物品推荐中平均实现0.5%的离线AUC增长和0.82%的在线ARPU增长，显著增强了模型性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid expansion of video game production necessitates the development ofeffective advertising and recommendation systems for online game platforms.Recommending and advertising games to users hinges on capturing their interestin games. However, existing representation learning methods crafted forhandling billions of items in recommendation systems are unsuitable for gameadvertising and recommendation. This is primarily due to game sparsity, wherethe mere hundreds of games fall short for large-scale user representationlearning, and game imbalance, where user behaviors are overwhelmingly dominatedby a handful of popular games. To address the sparsity issue, we introduce theUser Game Lifecycle (UGL), designed to enrich user behaviors in games.Additionally, we propose two innovative strategies aimed at manipulating userbehaviors to more effectively extract both short and long-term interests. Totackle the game imbalance challenge, we present an Inverse Probability Maskingstrategy for UGL representation learning. The offline and online experimentalresults demonstrate that the UGL representations significantly enhance model byachieving a 1.83% AUC offline increase on average and a 21.67% CVR onlineincrease on average for game advertising and a 0.5% AUC offline increase and a0.82% ARPU online increase for in-game item recommendation.</description>
      <author>example@mail.com (Yanjie Gou, Jiangming Liu, Kouying Xue, Yi Hua)</author>
      <guid isPermaLink="false">2510.15412v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Towards Robust Zero-Shot Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.15382v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Neurips 2025, 36 pages, 18 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BREEZE的新型零样本强化学习框架，该框架基于前向-后向表示(FB)方法进行升级，通过引入行为正则化、任务条件扩散模型和基于注意力的表达架构，解决了现有方法中建模表达能力不足和离分布动作导致的外推误差问题，显著提升了学习稳定性、策略提取能力和表示学习质量。&lt;h4&gt;背景&lt;/h4&gt;零样本强化学习的最新发展为学习预训练通用策略开辟了新途径，这些策略可以以零样本方式适应任意新任务。尽管流行的前向-后向表示(FB)及相关方法在零样本RL中显示出潜力，但它们的建模存在局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有零样本RL方法中建模表达能力不足、离分布(OOD)动作在离线学习期间引起的外推误差导致有偏表示、最终造成次优性能的问题。&lt;h4&gt;方法&lt;/h4&gt;作者提出了BREEZE(具有表达能力增强的行为正则化零样本RL)，这是一个升级的基于FB的框架，主要包括：1)在零样本RL策略学习中引入行为正则化，将策略优化转化为稳定的样本内学习范式；2)使用任务条件扩散模型提取策略，生成高质量和多模态的动作分布；3)采用基于注意力的表达架构进行表示建模，以捕捉环境动力学之间的复杂关系。&lt;h4&gt;主要发现&lt;/h4&gt;在ExORL和D4RL Kitchen上的大量实验表明，BREEZE实现了最佳或接近最佳的性能，并且比先前的离线零样本RL方法表现出更强的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;BREEZE通过结合行为正则化、任务条件扩散模型和基于注意力的表达架构，有效解决了现有零样本RL方法的局限性，显著提升了学习稳定性、策略提取能力和表示学习质量，为零样本强化学习领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近零样本强化学习(RL)的发展为学习预训练通用策略开辟了新途径，这些策略可以以零样本方式适应任意新任务。虽然流行的前向-后向表示(FB)及相关方法在零样本RL中显示出潜力，但我们通过实验发现它们的建模缺乏表达能力，并且离分布(OOD)动作在离线学习期间引起的外推误差有时会导致有偏表示，最终导致次优性能。为解决这些问题，我们提出了BREEZE(具有表达能力增强的行为正则化零样本RL)，这是一个升级的基于FB的框架，同时增强学习稳定性、策略提取能力和表示学习质量。BREEZE在零样本RL策略学习中引入行为正则化，将策略优化转化为稳定的样本内学习范式。此外，BREEZE使用任务条件扩散模型提取策略，能够在零样本RL设置中生成高质量和多模态的动作分布。而且，BREEZE采用基于注意力的表达架构进行表示建模，以捕捉环境动力学之间的复杂关系。在ExORL和D4RL Kitchen上的大量实验表明，BREEZE实现了最佳或接近最佳的性能，并且比先前的离线零样本RL方法表现出更强的鲁棒性。官方实现可在https://github.com/Whiterrrrr/BREEZE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent development of zero-shot reinforcement learning (RL) has opened anew avenue for learning pre-trained generalist policies that can adapt toarbitrary new tasks in a zero-shot manner. While the popular Forward-Backwardrepresentations (FB) and related methods have shown promise in zero-shot RL, weempirically found that their modeling lacks expressivity and that extrapolationerrors caused by out-of-distribution (OOD) actions during offline learningsometimes lead to biased representations, ultimately resulting in suboptimalperformance. To address these issues, we propose Behavior-REgularizEd Zero-shotRL with Expressivity enhancement (BREEZE), an upgraded FB-based framework thatsimultaneously enhances learning stability, policy extraction capability, andrepresentation learning quality. BREEZE introduces behavioral regularization inzero-shot RL policy learning, transforming policy optimization into a stablein-sample learning paradigm. Additionally, BREEZE extracts the policy using atask-conditioned diffusion model, enabling the generation of high-quality andmultimodal action distributions in zero-shot RL settings. Moreover, BREEZEemploys expressive attention-based architectures for representation modeling tocapture the complex relationships between environmental dynamics. Extensiveexperiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the bestor near-the-best performance while exhibiting superior robustness compared toprior offline zero-shot RL methods. The official implementation is availableat: https://github.com/Whiterrrrr/BREEZE.</description>
      <author>example@mail.com (Kexin Zheng, Lauriane Teyssier, Yinan Zheng, Yu Luo, Xiayuan Zhan)</author>
      <guid isPermaLink="false">2510.15382v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>DCMIL: A Progressive Representation Learning of Whole Slide Images for Cancer Prognosis Analysis</title>
      <link>http://arxiv.org/abs/2510.14403v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DCMIL（双课程对比多实例学习）的简单到难渐进式表示学习方法，用于高效处理全切片图像（WSIs）进行癌症预后预测，无需密集标注，可直接将千兆像素级WSIs转化为结果预测。&lt;h4&gt;背景&lt;/h4&gt;计算病理学是一个新兴学科，旨在利用全切片图像量化形态异质性并开发癌症客观预后模型，但受千兆像素级输入的计算瓶颈和密集手动标注稀缺性的阻碍，当前方法常忽略多倍率WSIs上的细粒度信息和肿瘤微环境变异。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效处理WSIs的癌症预后预测方法，解决现有方法的局限性，特别是计算瓶颈和标注稀缺问题。&lt;h4&gt;方法&lt;/h4&gt;提出DCMIL（dual-curriculum contrastive multi-instance learning）方法，这是一种简单到难渐进式表示学习技术，不依赖密集标注，可直接处理千兆像素级WSIs。&lt;h4&gt;主要发现&lt;/h4&gt;在十二种癌症类型（5,954名患者，1,254万张图像块）上的实验显示，DCMIL优于标准WSI预后模型，能识别预后显著区域，提供不确定性估计，捕捉正常与肿瘤组织形态差异，并可能产生新生物学见解。&lt;h4&gt;结论&lt;/h4&gt;DCMIL方法有效解决了计算病理学中的关键挑战，为癌症预后预测提供了强大工具，所有代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;蓬勃发展的计算病理学学科展现出利用全切片图像（WSIs）量化形态异异性并为人类癌症开发客观预后模型的潜力。然而，千兆像素级输入的计算瓶颈和密集手动标注的稀缺性阻碍了其进展。当前方法常常忽略多倍率WSIs上的细粒度信息和肿瘤微环境的变异。在此，我们提出了一种简单到难渐进式表示学习，称为双课程对比多实例学习（DCMIL），以高效处理WSIs用于癌症预后。该模型不依赖密集标注，能够直接将千兆像素级WSIs转化为结果预测。在十二种癌症类型（5,954名患者，1,254万张图像块）上的大量实验表明，DCMIL优于标准的基于WSI的预后模型。此外，DCMIL能够识别细粒度的预后显著区域，提供稳健的实例不确定性估计，并捕捉正常组织和肿瘤组织之间的形态差异，有可能产生新的生物学见解。所有代码已在https://github.com/tuuuc/DCMIL公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The burgeoning discipline of computational pathology shows promise inharnessing whole slide images (WSIs) to quantify morphological heterogeneityand develop objective prognostic modes for human cancers. However, progress isimpeded by the computational bottleneck of gigapixel-size inputs and thescarcity of dense manual annotations. Current methods often overlookfine-grained information across multi-magnification WSIs and variations intumor microenvironments. Here, we propose an easy-to-hard progressiverepresentation learning, termed dual-curriculum contrastive multi-instancelearning (DCMIL), to efficiently process WSIs for cancer prognosis. The modeldoes not rely on dense annotations and enables the direct transformation ofgigapixel-size WSIs into outcome predictions. Extensive experiments on twelvecancer types (5,954 patients, 12.54 million tiles) demonstrate that DCMILoutperforms standard WSI-based prognostic models. Additionally, DCMILidentifies fine-grained prognosis-salient regions, provides robust instanceuncertainty estimation, and captures morphological differences between normaland tumor tissues, with the potential to generate new biological insights. Allcodes have been made publicly accessible at https://github.com/tuuuc/DCMIL.</description>
      <author>example@mail.com (Chao Tu, Kun Huang, Jie Zhang, Qianjin Feng, Yu Zhang, Zhenyuan Ning)</author>
      <guid isPermaLink="false">2510.14403v2</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations</title>
      <link>http://arxiv.org/abs/2510.14049v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文引入了一个新的因果表示学习(CRL)基准，使用高保真模拟视觉数据，包含约20万张图像和300万视频帧，涵盖四个领域的24个子场景，提供对底层因果结构的灵活访问，评估了不同范式的代表性CRL方法，并提供了经验见解。&lt;h4&gt;背景&lt;/h4&gt;因果表示学习旨在揭示数据生成过程并识别潜在的因果变量和关系，但其评估具有固有挑战性，因为需要已知的真实因果变量和因果结构。现有评估通常依赖简单的合成数据集或真实世界任务的下游性能，面临真实性和评估精度之间的两难困境。&lt;h4&gt;目的&lt;/h4&gt;引入一个新的CRL基准，使用高保真模拟视觉数据，既保持真实的视觉复杂性，又能访问真实的因果生成过程，提供全面的测试平台以弥合严格评估和实际应用之间的差距。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含约20万张图像和300万视频帧的数据集，涵盖四个领域(静态图像生成、动态物理模拟、机器人操作和交通情况分析)的24个子场景，从静态到动态设置，从简单到复杂结构，从单智能体到多智能体交互。提供对底层因果结构的灵活访问，允许用户修改或配置它们以符合CRL中的假设要求。&lt;h4&gt;主要发现&lt;/h4&gt;该基准提供了一个全面的测试平台，有望弥合严格评估和实际应用之间的差距；评估了不同范式的代表性CRL方法；提供了经验见解，帮助实践者和新手选择或扩展适当的CRL框架以解决特定类型的现实问题。&lt;h4&gt;结论&lt;/h4&gt;该基准有助于解决特定类型的现实问题，这些问题可以从CRL视角中受益；提供了项目页面和数据集的访问链接。&lt;h4&gt;翻译&lt;/h4&gt;因果表示学习(CRL)旨在揭示数据生成过程并识别潜在的因果变量和关系，其评估由于需要已知的真实因果变量和因果结构而仍然具有固有挑战性。现有评估通常要么依赖简单的合成数据集，要么依赖真实世界任务的下游性能，普遍面临真实性和评估精度之间的两难困境。在本文中，我们引入了一个使用高保真模拟视觉数据的新CRL基准，这些数据既保持了真实的视觉复杂性，更重要的是，可以访问真实的因果生成过程。该数据集包含四个领域(静态图像生成、动态物理模拟、机器人操作和交通情况分析)中24个子场景的约20万张图像和300万视频帧。这些场景从静态到动态设置，从简单到复杂结构，从单智能体到多智能体交互，提供了一个全面的测试平台，有望弥合严格评估和实际应用之间的差距。此外，我们提供对底层因果结构的灵活访问，允许用户修改或配置它们以符合CRL中的假设要求，如可用的领域标签、时间依赖性或干预历史。利用这个基准，我们评估了不同范式的代表性CRL方法，并提供了经验见解，帮助实践者和新手选择或扩展适当的CRL框架，以适当解决可以从CRL视角中受益的特定类型的现实问题。欢迎访问我们的：项目页面：https://causal-verse.github.io/，数据集：https://huggingface.co/CausalVerse。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal Representation Learning (CRL) aims to uncover the data-generatingprocess and identify the underlying causal variables and relations, whoseevaluation remains inherently challenging due to the requirement of knownground-truth causal variables and causal structure. Existing evaluations oftenrely on either simplistic synthetic datasets or downstream performance onreal-world tasks, generally suffering a dilemma between realism and evaluativeprecision. In this paper, we introduce a new benchmark for CRL usinghigh-fidelity simulated visual data that retains both realistic visualcomplexity and, more importantly, access to ground-truth causal generatingprocesses. The dataset comprises around 200 thousand images and 3 million videoframes across 24 sub-scenes in four domains: static image generation, dynamicphysical simulations, robotic manipulations, and traffic situation analysis.These scenarios range from static to dynamic settings, simple to complexstructures, and single to multi-agent interactions, offering a comprehensivetestbed that hopefully bridges the gap between rigorous evaluation andreal-world applicability. In addition, we provide flexible access to theunderlying causal structures, allowing users to modify or configure them toalign with the required assumptions in CRL, such as available domain labels,temporal dependencies, or intervention histories. Leveraging this benchmark, weevaluated representative CRL methods across diverse paradigms and offeredempirical insights to assist practitioners and newcomers in choosing orextending appropriate CRL frameworks to properly address specific types of realproblems that can benefit from the CRL perspective. Welcome to visit our:Project page:https://causal-verse.github.io/,Dataset:https://huggingface.co/CausalVerse.</description>
      <author>example@mail.com (Guangyi Chen, Yunlong Deng, Peiyuan Zhu, Yan Li, Yifan Shen, Zijian Li, Kun Zhang)</author>
      <guid isPermaLink="false">2510.14049v2</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Attn-JGNN: Attention Enhanced Join-Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.15583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种用于解决#SAT问题的注意力增强连接图神经网络(Attn-JGNN)模型，显著提高了求解准确性。&lt;h4&gt;背景&lt;/h4&gt;#SAT问题是计算机科学中的重要问题，涉及计算布尔公式满足元的数量，现有神经网络方法可能存在求解精度不足的问题。&lt;h4&gt;目的&lt;/h4&gt;提高解决#SAT问题的准确性，通过引入注意力机制来优化连接图神经网络模型。&lt;h4&gt;方法&lt;/h4&gt;受迭代连接图传播算法启发，使用树分解将CNF公式编码为连接图，在连接图上进行迭代消息传递，通过学习分区函数近似模型数量，并在连接图的簇内和簇间应用注意力机制，使模型更关注关键变量和簇，减少冗余计算。&lt;h4&gt;主要发现&lt;/h4&gt;注意力机制使Attn-JGNN能够在概率推理中更关注关键变量和簇，减少冗余计算，实验表明该模型比其他神经网络方法取得了更好的结果。&lt;h4&gt;结论&lt;/h4&gt;Attn-JGNN模型通过结合注意力机制和连接图神经网络，有效提高了#SAT问题的求解准确性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于解决#SAT问题的注意力增强连接图神经网络(Attn-JGNN)模型，显著提高了求解准确性。受迭代连接图传播算法启发，Attn-JGNN使用树分解将CNF公式编码为连接图，然后在连接图上进行迭代消息传递，最后通过学习分区函数来近似模型数量。为了进一步提高求解准确性，我们在连接图的簇内和簇间应用注意力机制，这使得Attn-JGNN能够在概率推理中更关注关键变量和簇，并减少冗余计算。最后，我们的实验表明，Attn-JGNN模型比其他神经网络方法取得了更好的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an Attention Enhanced Join-Graph Neural Networks(Attn-JGNN) modelfor solving #SAT problems, which significantly improves the solving accuracy.Inspired by the Iterative Join Graph Propagation (IJGP) algorithm, Attn-JGNNuses tree decomposition to encode the CNF formula into a join-graph, thenperforms iterative message passing on the join-graph, and finally approximatesthe model number by learning partition functions. In order to further improvethe accuracy of the solution, we apply the attention mechanism in and betweenclusters of the join-graphs, which makes Attn-JGNN pay more attention to thekey variables and clusters in probabilistic inference, and reduces theredundant calculation. Finally, our experiments show that our Attn-JGNN modelachieves better results than other neural network methods.</description>
      <author>example@mail.com (Jixin Zhang, Yong Lai)</author>
      <guid isPermaLink="false">2510.15583v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Fault Cause Identification across Manufacturing Lines through Ontology-Guided and Process-Aware FMEA Graph Learning with LLMs</title>
      <link>http://arxiv.org/abs/2510.15428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种过程感知框架，结合制造领域概念化与图神经网络推理，提高FMEA知识在不同制造生产线间的可重用性，有效解决了故障原因识别的挑战。&lt;h4&gt;背景&lt;/h4&gt;自动化制造线中的故障原因识别具有挑战性，主要由于系统复杂性、频繁重新配置以及现有FMEA知识的有限可重用性。FMEA工作表包含宝贵的专家见解，但由于自然语言变异性、术语不一致和工艺差异，在异构生产线之间的重用受到阻碍。&lt;h4&gt;目的&lt;/h4&gt;解决FMEA知识在不同生产线间重用的限制，提高故障原因识别的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出一个过程感知框架，首先通过本体引导的大语言模型提取，将多个生产线的FMEA工作表转换为统一的知识图谱；其次，使用带有过程感知评分函数的关系图卷积网络学习尊重语义关系和顺序流程的嵌入；最后，使用链接预测推断和排序与目标生产线流程一致的候选故障原因。&lt;h4&gt;主要发现&lt;/h4&gt;在汽车压力传感器装配线上的案例研究表明，所提出的方法优于最先进的检索增强生成基线（F1@20 = 0.267）和RGCN方法（0.400），实现了故障原因识别的最佳性能（0.523）。消融研究证实了LLM驱动的领域概念化和过程感知学习的贡献。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架显著提高了FMEA知识在异构生产线之间的可转移性，支持操作人员更可靠地诊断故障，为未来智能制造中的领域自适应LLM应用铺平道路。&lt;h4&gt;翻译&lt;/h4&gt;在自动化生产线中，由于系统复杂性、频繁重新配置以及现有故障模式与影响分析知识的有限可重用性，故障原因识别具有挑战性。尽管FMEA工作表包含宝贵的专家见解，但由于自然语言变异性、术语不一致和工艺差异，它们在异构生产线之间的重用受到阻碍。为解决这些限制，本研究提出了一种过程感知框架，通过结合制造领域概念化与图神经网络推理，提高FMEA的可重用性。首先，通过本体引导的大语言模型提取，将多个生产线的FMEA工作表转换为统一的知识图谱，捕获领域概念如动作、状态、组件和参数。其次，使用带有过程感知评分函数的关系图卷积网络学习尊重语义关系和顺序流程的嵌入。最后，使用链接预测来推断和排序与目标生产线流程一致的候选故障原因。在汽车压力传感器装配线上的案例研究表明，所提出的方法优于最先进的检索增强生成基线（F1@20 = 0.267）和RGCN方法（0.400），实现了故障原因识别的最佳性能（0.523）。消融研究证实了LLM驱动的领域概念化和过程感知学习的贡献。这些结果表明，所提出的框架显著提高了FMEA知识在异构生产线之间的可转移性，从而支持操作人员更可靠地诊断故障，并为未来智能制造中领域自适应LLM的应用铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fault cause identification in automated manufacturing lines is challengingdue to the system's complexity, frequent reconfigurations, and the limitedreusability of existing Failure Mode and Effects Analysis (FMEA) knowledge.Although FMEA worksheets contain valuable expert insights, their reuse acrossheterogeneous lines is hindered by natural language variability, inconsistentterminology, and process differences. To address these limitations, this studyproposes a process-aware framework that enhances FMEA reusability by combiningmanufacturing-domain conceptualization with graph neural network (GNN)reasoning. First, FMEA worksheets from multiple manufacturing lines aretransformed into a unified knowledge graph through ontology-guided largelanguage model (LLM) extraction, capturing domain concepts such as actions,states, components, and parameters. Second, a Relational Graph ConvolutionalNetwork (RGCN) with the process-aware scoring function learns embeddings thatrespect both semantic relationships and sequential process flows. Finally, linkprediction is employed to infer and rank candidate fault causes consistent withthe target line's process flow.  A case study on automotive pressure sensor assembly lines demonstrates thatthe proposed method outperforms a state-of-the-art retrieval-augmentedgeneration (RAG) baseline (F1@20 = 0.267) and an RGCN approach (0.400),achieving the best performance (0.523) in fault cause identification. Ablationstudies confirm the contributions of both LLM-driven domain conceptualizationand process-aware learning. These results indicate that the proposed frameworksignificantly improves the transferability of FMEA knowledge acrossheterogeneous lines, thereby supporting operators in diagnosing failures morereliably and paving the way for future domain-adaptive LLM applications insmart manufacturing.</description>
      <author>example@mail.com (Sho Okazaki, Kohei Kaminishi, Takuma Fujiu, Yusheng Wang, Jun Ota)</author>
      <guid isPermaLink="false">2510.15428v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Geometric Mixture Models for Electrolyte Conductivity Prediction</title>
      <link>http://arxiv.org/abs/2510.15403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了GeoMix框架，用于准确预测电解质系统中的离子电导率，解决了缺乏高质量基准和混合系统几何建模不足的挑战。&lt;h4&gt;背景&lt;/h4&gt;电解质系统中离子电导率的准确预测对科学和技术应用至关重要，但当前研究面临两个基本挑战：(1)缺乏高质量标准化基准，(2)对混合系统中几何结构和分子间相互作用的建模不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有研究的局限性，建立新的电解质研究基准，并提供通用的几何学习框架以推进混合系统建模。&lt;h4&gt;方法&lt;/h4&gt;重新组织和增强CALiSol和DiffMix电解质数据集，加入分子的几何图表示；提出GeoMix框架，保持Set-SE(3)等变性；设计几何交互网络(GIN)作为专门为分子间几何消息传递的等变模块。&lt;h4&gt;主要发现&lt;/h4&gt;GeoMix在两个数据集上都一致优于多种基线方法，证明了跨分子几何相互作用和等变消息传递对准确属性预测的重要性。&lt;h4&gt;结论&lt;/h4&gt;该工作不仅为电解质研究建立了新基准，还提供了通用的几何学习框架，可应用于能源材料、药物开发等领域的混合系统建模。&lt;h4&gt;翻译&lt;/h4&gt;电解质系统中离子电导率的准确预测对推进众多科学和技术应用至关重要。尽管已取得显著进展，但当前研究面临两个基本挑战：(1)缺乏高质量标准化基准，(2)对混合系统中几何结构和分子间相互作用的建模不足。为解决这些局限性，我们首先通过加入分子的几何图表示来重新组织和增强CALiSol和DiffMix电解质数据集。然后，我们提出了GeoMix，一种新型几何感知框架，保留了混合系统的重要但具有挑战性的Set-SE(3)等变性特性。GeoMix的核心是几何交互网络(GIN)，一个专门为分子间几何消息传递设计的等变模块。全面的实验证明，GeoMix在两个数据集上都一致优于多种基线方法(包括MLPs、GNNs和几何GNNs)，验证了跨分子几何相互作用和等变消息传递对准确属性预测的重要性。这项工作不仅为电解质研究建立了新基准，还提供了通用的几何学习框架，推进了能源材料、药物开发等领域中混合系统的建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of ionic conductivity in electrolyte systems is crucialfor advancing numerous scientific and technological applications. Whilesignificant progress has been made, current research faces two fundamentalchallenges: (1) the lack of high-quality standardized benchmarks, and (2)inadequate modeling of geometric structure and intermolecular interactions inmixture systems. To address these limitations, we first reorganize and enhancethe CALiSol and DiffMix electrolyte datasets by incorporating geometric graphrepresentations of molecules. We then propose GeoMix, a novel geometry-awareframework that preserves Set-SE(3) equivariance-an essential but challengingproperty for mixture systems. At the heart of GeoMix lies the GeometricInteraction Network (GIN), an equivariant module specifically designed forintermolecular geometric message passing. Comprehensive experiments demonstratethat GeoMix consistently outperforms diverse baselines (including MLPs, GNNs,and geometric GNNs) across both datasets, validating the importance ofcross-molecular geometric interactions and equivariant message passing foraccurate property prediction. This work not only establishes new benchmarks forelectrolyte research but also provides a general geometric learning frameworkthat advances modeling of mixture systems in energy materials, pharmaceuticaldevelopment, and beyond.</description>
      <author>example@mail.com (Anyi Li, Jiacheng Cen, Songyou Li, Mingze Li, Yang Yu, Wenbing Huang)</author>
      <guid isPermaLink="false">2510.15403v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Backdoor or Manipulation? Graph Mixture of Experts Can Defend Against Various Graph Adversarial Attacks</title>
      <link>http://arxiv.org/abs/2510.15333v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于专家混合(MoE)架构的统一框架，用于防御图神经网络中的多种对抗性攻击，包括后门攻击、边操纵和节点注入攻击。&lt;h4&gt;背景&lt;/h4&gt;研究表明图神经网络容易受到多种对抗性攻击，包括操纵、节点注入和后门攻击。然而，现有防御方法通常只针对单一类型的攻击，缺乏能够同时防御多种威胁的统一方法。&lt;h4&gt;目的&lt;/h4&gt;设计一个可扩展的统一框架，能够同时防御后门、边操纵和节点注入等多种图对抗攻击。&lt;h4&gt;方法&lt;/h4&gt;利用专家混合(MoE)架构的灵活性，提出基于互信息的逻辑多样性损失，鼓励专家关注不同邻域结构；引入鲁棒性感知的路由器，识别扰动模式并将受扰节点路由到鲁棒专家。&lt;h4&gt;主要发现&lt;/h4&gt;在各种对抗设置下的广泛实验表明，该方法在抵御多种图对抗攻击方面表现出优越的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架能够有效防御多种图对抗攻击，为图神经网络的安全提供了统一解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大量研究已经强调了图神经网络(GNNs)容易受到对抗性攻击的脆弱性，包括操纵、节点注入以及最近出现的后门攻击威胁。然而，现有的防御方法通常只关注单一类型的攻击，缺乏同时防御多种威胁的统一方法。在本工作中，我们利用专家混合(MoE)架构的灵活性，设计了一个可扩展的统一框架，用于防御后门、边操纵和节点注入攻击。具体来说，我们提出了一种基于互信息的逻辑多样性损失，鼓励各个专家在决策过程中关注不同的邻域结构，从而确保在局部结构受到扰动时，有足够数量的专家不受影响。此外，我们引入了一种鲁棒性感知的路由器，能够识别扰动模式并将受扰动的节点自适应地路由到相应的鲁棒专家。在各种对抗设置下进行的广泛实验表明，我们的方法在抵御多种图对抗攻击方面持续表现出优越的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extensive research has highlighted the vulnerability of graph neural networks(GNNs) to adversarial attacks, including manipulation, node injection, and therecently emerging threat of backdoor attacks. However, existing defensestypically focus on a single type of attack, lacking a unified approach tosimultaneously defend against multiple threats. In this work, we leverage theflexibility of the Mixture of Experts (MoE) architecture to design a scalableand unified framework for defending against backdoor, edge manipulation, andnode injection attacks. Specifically, we propose an MI-based logic diversityloss to encourage individual experts to focus on distinct neighborhoodstructures in their decision processes, thus ensuring a sufficient subset ofexperts remains unaffected under perturbations in local structures. Moreover,we introduce a robustness-aware router that identifies perturbation patternsand adaptively routes perturbed nodes to corresponding robust experts.Extensive experiments conducted under various adversarial settings demonstratethat our method consistently achieves superior robustness against multiplegraph adversarial attacks.</description>
      <author>example@mail.com (Yuyuan Feng, Bin Ma, Enyan Dai)</author>
      <guid isPermaLink="false">2510.15333v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Spatiotemporal Traffic Prediction in Distributed Backend Systems via Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.15215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于图神经网络的建模方法，用于解决分布式后端系统中的交通预测问题。通过将系统抽象为图结构，结合图卷积机制和门控循环结构，实现了空间结构与时间演化的联合建模，显著提高了预测的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;传统模型在捕捉分布式后端系统中的复杂依赖性和动态特征方面存在局限性，需要更先进的建模方法来提高交通预测的准确性。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于图神经网络的建模方法，克服传统模型的限制，提高分布式后端系统中交通预测的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;将系统抽象为包含节点和边的图，节点特征表示流量和资源状态，邻接关系描述服务交互；使用图卷积机制实现节点特征的多阶传播和聚合；采用门控循环结构动态建模历史序列；通过时空联合建模模块融合图表示与时间依赖性；使用解码器生成未来流量预测；使用均方误差进行模型训练。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在不同预测范围和模型深度下实现了稳定的性能和低误差，显著提高了分布式后端系统中交通预测的准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;图神经网络在复杂系统建模中具有巨大潜力，能够有效捕捉分布式后端系统中的复杂依赖性和动态特征。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了分布式后端系统中的交通预测问题，并提出了一种基于图神经网络的建模方法，以克服传统模型在捕捉复杂依赖性和动态特征方面的局限性。该系统被抽象为一个包含节点和边的图，其中节点特征表示流量和资源状态，邻接关系描述服务交互。图卷积机制实现了节点特征的多阶传播和聚合，而门控循环结构动态建模历史序列，从而将空间结构与时间演化相结合。时空联合建模模块进一步融合图表示与时间依赖性，解码器生成未来流量预测。模型使用均方误差进行训练，以最小化与实际值的偏差。基于公共分布式系统日志的实验构建了节点特征、拓扑和序列的组合输入，并使用MSE、RMSE、MAE和MAPE指标将所提出的方法与主流基线进行比较。结果表明，所提出的方法在不同预测范围和模型深度下实现了稳定的性能和低误差，显著提高了分布式后端系统中交通预测的准确性和鲁棒性，验证了图神经网络在复杂系统建模中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the problem of traffic prediction in distributed backendsystems and proposes a graph neural network based modeling approach to overcomethe limitations of traditional models in capturing complex dependencies anddynamic features. The system is abstracted as a graph with nodes and edges,where node features represent traffic and resource states, and adjacencyrelations describe service interactions. A graph convolution mechanism enablesmulti order propagation and aggregation of node features, while a gatedrecurrent structure models historical sequences dynamically, thus integratingspatial structures with temporal evolution. A spatiotemporal joint modelingmodule further fuses graph representation with temporal dependency, and adecoder generates future traffic predictions. The model is trained with meansquared error to minimize deviations from actual values. Experiments based onpublic distributed system logs construct combined inputs of node features,topology, and sequences, and compare the proposed method with mainstreambaselines using MSE, RMSE, MAE, and MAPE. Results show that the proposed methodachieves stable performance and low error across different prediction horizonsand model depths, significantly improving the accuracy and robustness oftraffic forecasting in distributed backend systems and verifying the potentialof graph neural networks in complex system modeling.</description>
      <author>example@mail.com (Zhimin Qiu, Feng Liu, Yuxiao Wang, Chenrui Hu, Ziyu Cheng, Di Wu)</author>
      <guid isPermaLink="false">2510.15215v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Structural Generalization for Microservice Routing Using Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.15210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络的端到端优化框架，用于微服务系统中的智能路由，旨在提高复杂拓扑结构下路由决策效率和整体系统性能。&lt;h4&gt;背景&lt;/h4&gt;微服务系统中的智能路由问题，需要处理复杂拓扑结构下的路由决策和系统性能优化。&lt;h4&gt;目的&lt;/h4&gt;提高路由决策效率和整体系统性能，特别是在复杂拓扑结构下。更好地评估路径质量，捕获服务通信中的不稳定性和瓶颈风险。&lt;h4&gt;方法&lt;/h4&gt;将微服务之间的调用关系建模为图，服务节点和通信链路作为图的节点和边；使用多维特征作为输入，包括节点状态、链路延迟和调用频率；采用多层图神经网络进行高阶信息聚合和结构建模；模型为每个候选服务路径输出分数，用于指导动态路由决策；引入边感知注意力机制提高模型评估路径质量的能力；在不同网络深度、拓扑密度和服务规模下进行系统性分析。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法在多个关键指标上优于现有主流策略；能够有效处理高度动态和并发的微服务环境；展示了强大的性能、鲁棒性和结构泛化能力。&lt;h4&gt;结论&lt;/h4&gt;基于图神经网络的端到端优化框架在微服务系统智能路由方面表现优异，能够提高路由决策效率和系统整体性能。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文专注于微服务系统中的智能路由，并提出了一种基于图神经网络的端到端优化框架。目标是提高复杂拓扑结构下路由决策效率和整体系统性能。该方法将微服务之间的调用关系建模为图。在该图中，服务节点和通信链路被视为图的节点和边。使用节点状态、链路延迟和调用频率等多维特征作为输入。采用多层图神经网络执行高阶信息聚合和结构建模。模型为每个候选服务路径输出分数。然后使用这些分数来指导动态路由决策。为了提高模型评估路径质量的能力，引入了边感知注意力机制。该机制帮助模型更准确地捕获服务通信中的不稳定性和瓶颈风险。论文还对该模型在不同网络深度、拓扑密度和服务规模下的性能进行了系统性分析。从路由准确性、预测误差和系统稳定性等方面评估了该方法的有效性。实验结果表明，该方法在多个关键指标上优于现有的主流策略。它能够有效处理高度动态和并发的微服务环境，并表现出强大的性能、鲁棒性和结构泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper focuses on intelligent routing in microservice systems andproposes an end-to-end optimization framework based on graph neural networks.The goal is to improve routing decision efficiency and overall systemperformance under complex topologies. The method models invocationrelationships among microservices as a graph. In this graph, service nodes andcommunication links are treated as graph nodes and edges. Multi-dimensionalfeatures such as node states, link latency, and call frequency are used asinput. A multi-layer graph neural network is employed to perform high-orderinformation aggregation and structural modeling. The model outputs a score foreach candidate service path. These scores are then used to guide dynamicrouting decisions. To improve the model's ability to assess path quality, anedge-aware attention mechanism is introduced. This mechanism helps the modelcapture instability and bottleneck risks in service communications moreaccurately. The paper also conducts a systematic analysis of the model'sperformance under different network depths, topology densities, and servicescales. It evaluates the effectiveness of the method in terms of routingaccuracy, prediction error, and system stability. Experimental results showthat the proposed method outperforms existing mainstream strategies acrossmultiple key metrics. It handles highly dynamic and concurrent microserviceenvironments effectively and demonstrates strong performance, robustness, andstructural generalization.</description>
      <author>example@mail.com (Chenrui Hu, Ziyu Cheng, Di Wu, Yuxiao Wang, Feng Liu, Zhimin Qiu)</author>
      <guid isPermaLink="false">2510.15210v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>OCR-APT: Reconstructing APT Stories from Audit Logs using Subgraph Anomaly Detection and LLMs</title>
      <link>http://arxiv.org/abs/2510.15188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OCR-APT是一种创新的APT检测系统，通过结合图神经网络和大型语言模型，实现了更准确、更可解释的攻击检测和故事重建，解决了现有系统高误报率和粗粒度警报的问题。&lt;h4&gt;背景&lt;/h4&gt;高级持续性威胁(APTs)是隐蔽的网络攻击，通常能逃避系统级审计日志的检测。现有系统将异常检测应用于这些图，但常有高误报率和粗粒度警报，且依赖节点属性导致虚假关联，降低了检测的鲁棒性和可靠性。&lt;h4&gt;目的&lt;/h4&gt;开发能够生成准确、类人的完整攻击叙述的系统，解决高误报率和粗粒度警报问题，提供更鲁棒的异常检测，并生成可解释的最终报告。&lt;h4&gt;方法&lt;/h4&gt;引入OCR-APT系统，使用图神经网络(GNNs)进行子图异常检测，学习节点周围的行为模式而非脆弱属性；然后使用大型语言模型(LLMs)迭代处理检测到的子图重建多阶段攻击故事，每个阶段在继续前进行验证以减少幻觉。&lt;h4&gt;主要发现&lt;/h4&gt;在DARPA TC3、OpTC和NODLINK数据集上的评估显示，OCR-APT在检测准确率和警报可解释性方面优于最先进的系统，且能重建全面捕获攻击故事的类人报告。&lt;h4&gt;结论&lt;/h4&gt;OCR-APT系统有效解决了现有APT检测系统的局限性，通过结合GNNs和LLMs，提供了更准确和可解释的APT检测与攻击故事重建能力。&lt;h4&gt;翻译&lt;/h4&gt;高级持续性威胁(APTs)是隐蔽的网络攻击，通常能逃避系统级审计日志中的检测。来源图将这些日志建模为连接的实体和事件，揭示了线性日志表示中遗漏的关系。现有系统将这些图应用于异常检测，但常常遭受高误报率和粗粒度警报的困扰。它们对文件路径或IP等节点属性的依赖导致虚假关联，降低了检测的鲁棒性和可靠性。为了完全理解攻击的进展和影响，安全分析师需要能够生成准确、类人的完整攻击叙述的系统。为解决这些挑战，我们引入了OCR-APT，一个用于APT检测和类人攻击故事重建的系统。OCR-APT使用图神经网络(GNNs)进行子图异常检测，学习节点周围的行为模式，而不是依赖文件路径或IP等脆弱属性。这种方法导致更鲁棒的异常检测。然后，它使用大型语言模型(LLMs)迭代处理检测到的子图，重建多阶段攻击故事。每个阶段在继续前进行验证，减少幻觉并确保可解释的最终报告。我们在DARPA TC3、OpTC和NODLINK数据集上的评估表明，OCR-APT在检测准确率和警报可解释性方面优于最先进的系统。此外，OCR-APT重建的类人报告全面捕获了攻击故事。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advanced Persistent Threats (APTs) are stealthy cyberattacks that often evadedetection in system-level audit logs. Provenance graphs model these logs asconnected entities and events, revealing relationships that are missed bylinear log representations. Existing systems apply anomaly detection to thesegraphs but often suffer from high false positive rates and coarse-grainedalerts. Their reliance on node attributes like file paths or IPs leads tospurious correlations, reducing detection robustness and reliability. To fullyunderstand an attack's progression and impact, security analysts need systemsthat can generate accurate, human-like narratives of the entire attack. Toaddress these challenges, we introduce OCR-APT, a system for APT detection andreconstruction of human-like attack stories. OCR-APT uses Graph Neural Networks(GNNs) for subgraph anomaly detection, learning behavior patterns around nodesrather than fragile attributes such as file paths or IPs. This approach leadsto a more robust anomaly detection. It then iterates over detected subgraphsusing Large Language Models (LLMs) to reconstruct multi-stage attack stories.Each stage is validated before proceeding, reducing hallucinations and ensuringan interpretable final report. Our evaluations on the DARPA TC3, OpTC, andNODLINK datasets show that OCR-APT outperforms state-of-the-art systems in bothdetection accuracy and alert interpretability. Moreover, OCR-APT reconstructshuman-like reports that comprehensively capture the attack story.</description>
      <author>example@mail.com (Ahmed Aly, Essam Mansour, Amr Youssef)</author>
      <guid isPermaLink="false">2510.15188v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Evaluation of Graph Neural Networks and Physics Informed Learning for Surrogate Modelling of Finite Element Analysis</title>
      <link>http://arxiv.org/abs/2510.15750v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 6 figures, 5 tables. Code available  at:https://github.com/SinghNayanKumar/DL-surrogate-modelling&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了图神经网络(GNNs)和3D U-Nets作为参数化I型梁有限元分析(FEA)替代模型的性能，引入了基于Navier-Cauchy方程的物理信息神经网络(PINN)框架，并证明课程学习策略对稳定训练至关重要。研究发现GNNs整体优于U-Net，MPNN和图变换器达到最高精度，PINN框架显著提高了泛化能力。&lt;h4&gt;背景&lt;/h4&gt;有限元分析(FEA)是产品设计生命周期中不可或缺的部分，但计算成本高，不适合许多设计优化问题。深度学习模型可能是一个很好的解决方案。&lt;h4&gt;目的&lt;/h4&gt;评估图神经网络(GNNs)和3D U-Nets作为FEA替代模型的性能，引入物理信息神经网络(PINN)框架，研究课程学习策略对训练稳定性的影响。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNNs)和3D U-Nets作为参数化I型梁FEA的替代模型，实现基于Navier-Cauchy方程的PINN框架，采用课程学习策略：先在数据上预训练，再进行物理信息微调。&lt;h4&gt;主要发现&lt;/h4&gt;GNNs整体上优于U-Net；最差的GNN模型(GCN)相对L2误差为8.7%，而最好的U-Net模型得分为13.0%；MPNN和图变换器达到最高精度，相对L2得分分别为3.5%和2.6%；PINN框架显著提高了泛化能力，在高信号任务上减少了高达11.3%的误差；图变换器是最准确的模型但推理速度较慢；MPNN-PINN模型在性能和效率之间提供了最佳平衡。&lt;h4&gt;结论&lt;/h4&gt;图神经网络是有限元分析的有效替代方案；课程学习策略对稳定训练至关重要；物理信息神经网络框架提高了模型的泛化能力；MPNN-PINN模型在预测性能、模型大小和推理速度之间取得了良好平衡。&lt;h4&gt;翻译&lt;/h4&gt;虽然有限元分析(FEA)是产品设计生命周期中不可或缺的部分，但分析计算成本高，使其不适合许多设计优化问题。深度学习模型可以是一个很好的解决方案。然而，选择能够以高精度模拟FEA的架构是一个挑战。本文提出了对图神经网络(GNNs)和3D U-Nets作为参数化I型梁FEA替代模型的综合评估。我们引入了一个由Navier-Cauchy方程控制的物理信息神经网络(PINN)框架，以强制执行物理定律。关键的是，我们证明课程学习策略——先在数据上预训练，再进行物理信息微调——对于稳定训练至关重要。我们的结果表明，GNNs从根本上优于U-Net。即使在GNNs中最差的GCN框架也实现了8.7%的相对L2误差，而在U-Net中最好的框架(使用高分辨率数据训练的带注意力机制的U-Net)获得了13.0%的得分。在基于图的架构中，消息传递神经网络(MPNN)和图变换器达到了最高的准确性，分别实现了3.5%和2.6%的相对L2得分。包含物理基本定律(PINN)显著提高了泛化能力，在高信号任务上减少了高达11.3%的误差。虽然图变换器是最准确的模型，但在推理时比第二好的模型MPNN-PINN慢37.5%。PINN增强的MPNN(MPNN-PINN)提供了最实用的解决方案。它在预测性能、模型大小和推理速度之间提供了良好的平衡。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决有限元分析(FEA)计算成本高、不适合实时应用和设计优化的问题。这个问题在现实中很重要，因为FEA虽然广泛应用于产品设计，但计算时间长，难以用于需要快速反馈的场景，如数字孪生或多次迭代的设计优化。传统替代方法如降阶模型存在线性假设、侵入性要求等局限性，难以处理复杂的非线性物理现象。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了FEA的计算瓶颈和传统替代方法的局限性，认识到需要更灵活、非线性、非侵入性的解决方案。他们借鉴了现有工作中的图神经网络处理网格数据的方法，以及物理信息神经网络(PINN)嵌入物理定律的思路。在此基础上，作者设计了系统性比较多种GNN架构的方案，并创新性地采用课程学习策略来稳定PINN训练，通过两阶段训练(数据驱动预训练+物理信息微调)解决了训练不稳定的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图神经网络(GNN)直接处理FEA的非结构化网格，并通过物理信息神经网络(PINN)嵌入物理定律来提高泛化能力。整体流程包括：1)使用gmsh和DOLFINx生成I梁的FEA数据，创建低信号和高信号数据集；2)实现多种GNN架构(GCN、GAT、MPNN、图变换器)和3D U-Net基线；3)设计节点特征编码和消息传递机制；4)将纳维-柯西方程嵌入损失函数，采用课程学习和损失权重退火稳定训练；5)使用多种指标评估模型性能和计算效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)系统性比较多种GNN架构，证明GNN在FEA替代模型中的优越性；2)成功将物理定律嵌入GNN，显著提高泛化能力；3)提出稳健的PINN训练策略，通过课程学习和损失权重退火解决训练不稳定问题；4)进行全面的性能-效率分析，识别最优实用架构。相比之前工作，本文提供了更全面的架构比较，解决了PINN训练不稳定问题，并考虑了实际部署中的模型大小和推理速度等实用因素。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统比较多种图神经网络架构并成功集成物理信息学习，为有限元分析提供了一个高效、准确且泛化能力强的替代模型，同时提出了稳定的训练策略和实用的部署指南。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although Finite Element Analysis (FEA) is an integral part of the productdesign lifecycle, the analysis is computationally expensive, making itunsuitable for many design optimization problems. The deep learning models canbe a great solution. However, selecting the architecture that emulates the FEAwith great accuracy is a challenge. This paper presents a comprehensiveevaluation of graph neural networks (GNNs) and 3D U-Nets as surrogates for FEAof parametric I-beams. We introduce a Physics-Informed Neural Network (PINN)framework, governed by the Navier Cauchy equations, to enforce physical laws.Crucially, we demonstrate that a curriculum learning strategy, pretraining ondata followed by physics informed fine tuning, is essential for stabilizingtraining. Our results show that GNNs fundamentally outperform the U-Net. Eventhe worst performer among GNNs, the GCN framework, achieved a relative L2 errorof 8.7% while the best framework among U Net, U Net with attention mechanismtrained on high resolution data, achieved 13.0% score. Among the graph-basedarchitectures, the Message Passing Neural Networks (MPNN) and GraphTransformers achieved the highest accuracy, achieving a relative L2 score of3.5% and 2.6% respectively. The inclusion of physics fundamental laws (PINN)significantly improved the generalization, reducing error by up to 11.3% onhigh-signal tasks. While the Graph Transformer is the most accurate model, itis more 37.5% slower during inference when compared to second best model, MPNNPINN. The PINN enhanced MPNN (MPNN PINN) provides the most practical solution.It offers a good compromise between predictive performance, model size, andinference speed.</description>
      <author>example@mail.com (Nayan Kumar Singh)</author>
      <guid isPermaLink="false">2510.15750v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand</title>
      <link>http://arxiv.org/abs/2510.12328v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合物理信息的图神经网络与极值分析技术，用于改进泰国地区的站点降雨预测，特别是在极端事件预报方面取得了显著成果。&lt;h4&gt;背景&lt;/h4&gt;准确的降雨预报，尤其是极端事件的预报，在气候学和地球系统中仍是一个重大挑战。传统方法在泰国地区的站点降雨预测中存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型预测模型，提高泰国地区站点降雨预测的准确性，特别是针对极端事件的预测能力，并提供高分辨率地图以支持长期水资源管理决策。&lt;h4&gt;方法&lt;/h4&gt;结合物理信息的图神经网络与极值分析技术，利用站点图的表示捕捉复杂时空模式，通过遥相关提供可解释性；预处理影响区域降雨的气候指标；应用基于图注意力网络和长短期记忆网络的模型；使用空间季节感知广义帕累托分布方法进行阈值超限映射以解决极端问题。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明该方法在大多数地区优于成熟基线模型，包括易发生极端事件的区域；与最先进技术保持强竞争力；与业务预报系统SEAS5相比，显著改进了极端事件的预测；能够提供支持决策的高分辨率地图。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法是改进降雨预测特别是极端事件预测的实用增强工具，可为长期水资源管理提供决策支持。&lt;h4&gt;翻译&lt;/h4&gt;准确的降雨预报，特别是对极端事件的预报，在气候学和地球系统中仍然是一个重大挑战。本文提出了结合物理信息的图神经网络与极值分析技术的新方法，以改进泰国地区的站点降雨预测。该模型利用站点图的表示来捕捉复杂的时空模式，并通过遥相关提供可解释性。我们预处理可能影响区域降雨的相关气候指标。提出的基于图注意力网络和长短期记忆网络的模型，使用简单地形降水物理公式推导的初始边特征应用注意力机制，嵌入随后由LSTM层处理。为解决极端问题，我们使用新颖的空间季节感知广义帕累托分布方法进行阈值超限映射，克服了传统机器学习模型的局限性。实验证明，我们的方法在大多数地区都优于成熟的基线模型，包括易发生极端事件的区域，并与最先进技术保持强竞争力。与业务预报系统SEAS5相比，我们的实际应用改进了极端事件的预测，并提供了实用的增强功能，可以生成支持长期水资源管理决策的高分辨率地图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate rainfall forecasting, particularly for extreme events, remains asignificant challenge in climatology and the Earth system. This paper presentsnovel physics-informed Graph Neural Networks (GNNs) combined with extreme-valueanalysis techniques to improve gauge-station rainfall predictions acrossThailand. The model leverages a graph-structured representation of gaugestations to capture complex spatiotemporal patterns, and it offersexplainability through teleconnections. We preprocess relevant climate indicesthat potentially influence regional rainfall. The proposed Graph AttentionNetwork with Long Short-Term Memory (Attention-LSTM) applies the attentionmechanism using initial edge features derived from simpleorographic-precipitation physics formulation. The embeddings are subsequentlyprocessed by LSTM layers. To address extremes, we perform Peak-Over-Threshold(POT) mapping using the novel Spatial Season-aware Generalized ParetoDistribution (GPD) method, which overcomes limitations of traditionalmachine-learning models. Experiments demonstrate that our method outperformswell-established baselines across most regions, including areas prone toextremes, and remains strongly competitive with the state of the art. Comparedwith the operational forecasting system SEAS5, our real-world applicationimproves extreme-event prediction and offers a practical enhancement to producehigh-resolution maps that support decision-making in long-term watermanagement.</description>
      <author>example@mail.com (Kiattikun Chobtham, Kanoksri Sarinnapakorn, Kritanai Torsri, Prattana Deeprasertkul, Jirawan Kamma)</author>
      <guid isPermaLink="false">2510.12328v3</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>FreqPDE: Rethinking Positional Depth Embedding for Multi-View 3D Object Detection Transformers</title>
      <link>http://arxiv.org/abs/2510.15385v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FreqPDE的新方法，用于从多视图2D图像中准确检测3D物体，解决了现有方法中深度预测质量不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;从多视图2D图像中准确检测3D物体是自动驾驶领域的一项具有挑战性但至关重要的任务。当前方法依赖深度预测来恢复空间信息，但存在深度不连续和小物体不清晰等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提供更准确深度信息的方法，解决现有深度预测中的边界不连续和小物体不清晰问题，并考虑跨视图一致性和尺度不变性。&lt;h4&gt;方法&lt;/h4&gt;提出频率感知位置深度嵌入（FreqPDE），包含三个主要模块：频率感知空间金字塔编码器（FSPE）构建特征金字塔；跨视图尺度不变深度预测器（CSDP）估计像素级深度分布；位置深度编码器（PDE）生成3D深度感知特征。同时采用混合深度监督进行互补深度学习。&lt;h4&gt;主要发现&lt;/h4&gt;现有深度预测方法存在物体边界深度不连续和小物体不清晰的问题，主要由投影点稀疏监督和高级图像特征使用引起。跨视图一致性和尺度不变性在先前方法中被忽视。&lt;h4&gt;结论&lt;/h4&gt;在nuScenes数据集上的广泛实验证明了所提出FreqPDE方法的有效性和优越性，能够显著提升3D物体检测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;从多视图2D图像中准确检测3D物体是自动驾驶领域一项具有挑战性但至关重要的任务。当前方法通过整合深度预测来恢复物体查询解码的空间信息，这需要在训练阶段使用LiDAR点进行显式监督。然而，预测的深度质量仍然不理想，如物体边界深度不连续和小物体不清晰，主要由投影点的稀疏监督和使用高级图像特征进行深度预测引起。此外，先前的方法也忽视了跨视图一致性和尺度不变性。本文引入了频率感知位置深度嵌入（FreqPDE）来为2D图像特征赋予空间信息，用于3D检测transformer解码器，这通过三个主要模块实现：频率感知空间金字塔编码器（FSPE）结合不同级别的高频边缘线索和低级语义构建特征金字塔；跨视图尺度不变深度预测器（CSDP）使用跨视图和高效通道注意力机制估计像素级深度分布；位置深度编码器（PDE）结合2D图像特征和3D位置嵌入，为查询解码生成3D深度感知特征。同时采用混合深度监督，从度量和分布方面进行互补深度学习。在nuScenes数据集上进行的广泛实验证明了所提出方法的有效性和优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从多视角2D图像中准确检测3D物体时深度预测质量不佳的问题，包括物体边界深度不连续、小物体不清晰等。这个问题在自动驾驶领域至关重要，因为准确的3D物体检测是确保自动驾驶系统安全感知周围环境的关键，而仅使用摄像头的方法比基于LiDAR的方法成本更低，但恢复3D空间信息是一个具有挑战性的病态问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前深度预测方法的三个主要缺陷：仅使用高级图像特征导致细节丢失、投影点云稀疏监督导致学习不完整、以及忽略了跨视图一致性和尺度不变性。基于这些问题，作者设计了FreqPDE方法，包含三个核心模块：频率感知空间金字塔编码器(FSPE)、跨视图尺度不变深度预测器(CSDP)和位置深度编码器(PDE)。该方法借鉴了现有工作，如BEVFormer和StreamPETR等基于Transformer的3D检测方法，以及BEVDepth等深度预测方法，同时引入了FreqFusion等频率域学习的方法，但进行了创新性改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用频率感知的位置深度嵌入为2D视觉特征提供高质量的空间信息，通过结合高频边缘细节和低频全局语义提高深度预测质量，并采用混合深度监督进行互补学习。整体流程包括：1)FSPE模块构建特征金字塔，结合不同级别的高频边缘线索和低频语义；2)CSDP模块进行分层深度预测，使用跨视图注意力和高效通道注意力确保一致性和尺度不变性；3)PDE模块结合2D图像特征和3D位置嵌入生成深度感知特征；4)采用混合深度监督，结合稀疏LiDAR和密集伪深度图；5)使用Transformer解码器生成最终检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将频率域信息引入多视图3D检测，同时利用高频和低频信息；2)FSPE模块通过低频语义提取和高频边界增强保留更多细节；3)CSDP模块引入跨视图注意力和相机感知通道注意力解决一致性和尺度不变性问题；4)混合深度监督结合稀疏LiDAR和密集伪深度图进行互补学习。相比之前工作，不同之处在于：之前方法主要使用单一频率信息，忽略了跨视图一致性和尺度不变性，且监督方式单一；而FreqPDE同时利用高频和低频信息，解决了跨视图一致性和尺度不变性问题，并引入了混合深度监督。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FreqPDE通过引入频率感知的位置深度嵌入和混合深度监督，显著提高了多视角3D物体检测的准确性，特别是在处理远距离和小物体方面。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting 3D objects accurately from multi-view 2D images is a challengingyet essential task in the field of autonomous driving. Current methods resortto integrating depth prediction to recover the spatial information for objectquery decoding, which necessitates explicit supervision from LiDAR pointsduring the training phase. However, the predicted depth quality is stillunsatisfactory such as depth discontinuity of object boundaries andindistinction of small objects, which are mainly caused by the sparsesupervision of projected points and the use of high-level image features fordepth prediction. Besides, cross-view consistency and scale invariance are alsooverlooked in previous methods. In this paper, we introduce Frequency-awarePositional Depth Embedding (FreqPDE) to equip 2D image features with spatialinformation for 3D detection transformer decoder, which can be obtained throughthree main modules. Specifically, the Frequency-aware Spatial Pyramid Encoder(FSPE) constructs a feature pyramid by combining high-frequency edge clues andlow-frequency semantics from different levels respectively. Then the Cross-viewScale-invariant Depth Predictor (CSDP) estimates the pixel-level depthdistribution with cross-view and efficient channel attention mechanism.Finally, the Positional Depth Encoder (PDE) combines the 2D image features and3D position embeddings to generate the 3D depth-aware features for querydecoding. Additionally, hybrid depth supervision is adopted for complementarydepth learning from both metric and distribution aspects. Extensive experimentsconducted on the nuScenes dataset demonstrate the effectiveness and superiorityof our proposed method.</description>
      <author>example@mail.com (Haisheng Su, Junjie Zhang, Feixiang Song, Sanping Zhou, Wei Wu, Nanning Zheng, Junchi Yan)</author>
      <guid isPermaLink="false">2510.15385v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>ERNet: Efficient Non-Rigid Registration Network for Point Sequences</title>
      <link>http://arxiv.org/abs/2510.15800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025. Project Page: https://guangzhaohe.com/ernet&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为ERNet的高效前馈模型，用于解决将物体形状注册到经历非刚性变形的点云序列的挑战。该方法通过两阶段流程预测变形图序列，能有效处理嘈杂和部分输入，并利用时间信息实现准确和一致的序列注册。&lt;h4&gt;背景&lt;/h4&gt;将物体形状注册到经历非刚性变形的点云序列是一个长期存在的挑战。主要困难来自两个方面：目标函数非凸性导致的局部极小值（特别是在嘈杂或部分输入情况下）阻碍了准确和鲁棒的变形估计；长序列中的误差累积导致跟踪失败。&lt;h4&gt;目的&lt;/h4&gt;解决非刚性变形点云序列注册中的挑战，特别是局部极小值问题和误差累积问题，同时提高处理效率。&lt;h4&gt;方法&lt;/h4&gt;采用可扩展的数据驱动方法，提出ERNet模型，这是一种在大变形数据集上训练的高效前馈模型。关键设计是通过两阶段流程预测变形图序列：首先估计帧级粗略图节点实现鲁棒初始化，然后在滑动窗口方式下随时间细化它们的轨迹。该方法能有效处理嘈杂和部分输入，同时利用时间信息进行准确和一致的序列注册。&lt;h4&gt;主要发现&lt;/h4&gt;在Deforming Things4D和D-FAUST数据集上，所提出的方法优于之前的先进方法；与之前最好的方法相比，实现了4倍以上的速度提升，显著提高了处理效率。&lt;h4&gt;结论&lt;/h4&gt;ERNet模型有效解决了非刚性变形点云序列注册中的挑战，在准确性和效率方面都表现出色。&lt;h4&gt;翻译&lt;/h4&gt;将物体形状注册到经历非刚性变形的点云序列是一个长期存在的挑战。关键困难源于两个因素：(i)由于目标函数的非凸性，特别是在嘈杂或部分输入的情况下，存在局部极小值，这阻碍了准确和鲁棒的变形估计；(ii)长序列中的误差累积导致跟踪失败。为应对这些挑战，我们采用可扩展的数据驱动方法，并提出了ERNet，一种在大变形数据集上训练的高效前馈模型。它旨在处理嘈杂和部分输入，同时有效利用时间信息进行准确和一致的序列注册。我们设计的关键是通过两阶段流程预测变形图序列，首先估计帧级粗略图节点以实现鲁棒初始化，然后在滑动窗口方式下随时间细化它们的轨迹。大量实验表明，我们提出的方法(i)在Deforming Things4D和D-FAUST数据集上都优于之前的先进方法，(ii)与之前最好的方法相比实现了4倍以上的速度提升，提供了显著的效率改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决将物体形状注册到一系列经历非刚性变形的点云上的问题。这个问题在计算机视觉和机器人领域至关重要，因为它涉及动态重建、场景理解和机器人操作等广泛应用。传统方法容易陷入局部最优解，特别是在处理噪声或部分输入的点云时，导致变形估计不准确；同时，长序列中的误差累积会导致跟踪失败，限制了这些方法在实际应用中的有效性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统优化方法容易陷入局部最优，基于神经变形场的方法难以泛化到噪声或部分输入，而预测密集对应关系的方法计算复杂度高。因此，作者设计了一个高效的前馈模型，采用数据驱动方法在大型变形数据集上训练。方法借鉴了变形图表示、三平面编码器、滑动窗口策略和局部刚性假设等现有技术，但创新性地将它们组合成一个两阶段管道：首先估计帧级粗略图节点，然后在滑动窗口方式下细化节点轨迹，实现了鲁棒且时间一致的序列配准。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用变形图作为非刚性变形的紧凑表示，通过两阶段策略（粗略匹配和时空细化）实现高效配准，并利用局部刚性假设推断节点变换属性。整体流程包括：1) 使用三平面编码器将源点云和目标点云序列编码为特征；2) 从源点云采样节点，通过节点到帧匹配初始化节点位置；3) 使用时空变换器在滑动窗口中细化节点轨迹；4) 利用局部刚性假设和Procrustes分析估计节点变换；5) 使用径向基函数线性混合皮肤将变形图转换为密集变形场，完成配准。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 高效的前馈序列非刚性注册架构，采用两阶段策略提高鲁棒性和时间一致性；2) 变形图回归作为非刚性注册的高效表示，平衡了表达能力和计算效率；3) 利用局部刚性假设推断节点变换属性，避免直接预测高维非线性变换的困难。相比传统优化方法，ERNet不易陷入局部最优且能处理噪声输入；相比基于神经变形场的方法，它不需要每帧优化，效率更高；相比预测密集对应关系的方法，它计算效率更高，更适合处理长序列。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ERNet通过引入基于变形图的两阶段预测策略，实现了高效、准确且时间一致的非刚性点云序列配准，在保持高精度的同时实现了超过4倍的速度提升，显著优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Registering an object shape to a sequence of point clouds undergoingnon-rigid deformation is a long-standing challenge. The key difficulties stemfrom two factors: (i) the presence of local minima due to the non-convexity ofregistration objectives, especially under noisy or partial inputs, whichhinders accurate and robust deformation estimation, and (ii) error accumulationover long sequences, leading to tracking failures. To address these challenges,we introduce to adopt a scalable data-driven approach and propose ERNet, anefficient feed-forward model trained on large deformation datasets. It isdesigned to handle noisy and partial inputs while effectively leveragingtemporal information for accurate and consistent sequential registration. Thekey to our design is predicting a sequence of deformation graphs through atwo-stage pipeline, which first estimates frame-wise coarse graph nodes forrobust initialization, before refining their trajectories over time in asliding-window fashion. Extensive experiments show that our proposed approach(i) outperforms previous state-of-the-art on both the DeformingThings4D andD-FAUST datasets, and (ii) achieves more than 4x speedup compared to theprevious best, offering significant efficiency improvement.</description>
      <author>example@mail.com (Guangzhao He, Yuxi Xiao, Zhen Xu, Xiaowei Zhou, Sida Peng)</author>
      <guid isPermaLink="false">2510.15800v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes</title>
      <link>http://arxiv.org/abs/2510.15467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MRASfM的多相机重建和聚合运动结构框架，专门针对驾驶场景中的运动结构(SfM)问题，解决了姿态估计不可靠、道路表面重建异常值过多以及重建效率低等挑战。&lt;h4&gt;背景&lt;/h4&gt;Structure from Motion (SfM)估计相机姿态并重建点云，是各种任务的基础。然而，将SfM应用于多相机系统捕捉的驾驶场景存在显著困难，包括不可靠的姿态估计、道路表面重建中过多的异常值以及低重建效率。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些限制，提出专门为驾驶场景设计的多相机重建和聚合运动结构(MRASfM)框架。&lt;h4&gt;方法&lt;/h4&gt;MRASfM通过以下方法解决挑战：1)在注册过程中利用多相机系统内的固定空间关系提高姿态估计可靠性；2)采用平面模型有效去除道路表面重建中的错误点；3)在捆绑调整中将多相机集视为单一单元减少优化变量提高效率；4)通过场景关联和组装模块以从粗到细的方式实现多场景聚合。&lt;h4&gt;主要发现&lt;/h4&gt;在实际车辆上部署多相机系统验证了MRASfM在不同场景中的泛化能力和在具有挑战性条件下的鲁棒性。在公共数据集上的大规模验证结果显示，MRASfM达到了最先进的性能，实现了较低的绝对姿态误差。&lt;h4&gt;结论&lt;/h4&gt;MRASfM框架有效地解决了多相机系统在驾驶场景中应用SfM时面临的主要挑战，提高了姿态估计的可靠性、道路表面重建的质量和整体重建效率。&lt;h4&gt;翻译&lt;/h4&gt;运动结构(SfM)估计相机姿态并重建点云，形成各种任务的基础。然而，将SfM应用于由多相机系统捕捉的驾驶场景存在显著困难，包括不可靠的姿态估计、道路表面重建中过多的异常值以及低重建效率。为了解决这些限制，我们提出了一种专门为驾驶场景设计的多相机重建和聚合运动结构(MRASfM)框架。MRASfM通过在注册过程中利用多相机系统内的固定空间关系来提高相机姿态估计的可靠性。为了提高道路表面重建的质量，我们的框架采用平面模型有效去除三角测量道路表面中的错误点。此外，在捆绑调整(BA)中将多相机集视为单一单元有助于减少优化变量以提高效率。此外，MRASfM通过场景关联和组装模块以从粗到细的方式实现多场景聚合。我们在实际车辆上部署了多相机系统，以验证MRASfM在各种场景中的泛化能力以及在具有挑战性条件下的鲁棒性。此外，在公共数据集上的大规模验证结果显示了MRASfM的最先进性能，实现了较低的绝对姿态误差。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决将传统SfM技术应用于多相机系统捕获的驾驶场景时的三大挑战：不可靠的姿态估计、道路表面重建中过多的离群点和低重建效率。这个问题在现实中很重要，因为准确的驾驶场景重建是高清地图构建和新视角合成等关键下游任务的基础，而传统vSLAM存在累积漂移问题，直接应用SfM又面临上述挑战，限制了自动驾驶系统对环境的精确感知和自身定位能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先深入分析了传统SfM在驾驶场景中的局限性，然后有选择地借鉴了现有工作：参考了多相机vSLAM方法如BAMF-SLAM和MAVIS，但意识到它们需要精确校准；借鉴了COLMAP和MMA等SfM方法，但发现它们在处理多相机系统时有局限；受MCSfM启发但希望进一步提高效率。作者针对性地设计了解决方案：利用多相机固定空间关系提高姿态估计可靠性，应用平面模型过滤道路离群点，将相机集视为统一单位提高效率，并设计场景聚合模块整合碎片化场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将多相机系统视为刚性单元，利用相机间的固定空间关系作为先验约束，结合语义信息提高重建质量，通过分层优化提高效率，并以粗到细方式整合碎片化场景。整体流程分为单场景重建和多场景聚合：单场景重建包括多相机对应点搜索、先验初始化和迭代重建（相机集注册、语义辅助三角测量、相机集BA）；多场景聚合包括场景关联（使用GNSS定位）和场景组装（粗组装和精组装，通过SfM优化变换矩阵）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 相机集注册模块，将多相机视为刚性单元提高姿态估计鲁棒性，不同于传统方法忽略或简单利用相机间关系；2) 相机集BA模块，优化车辆姿态和内部相对姿态而非单个相机姿态，显著减少优化变量；3) 语义辅助三角测量，使用平面模型过滤道路离群点，专门处理道路表面特殊挑战；4) 多场景聚合模块，以粗到细方式整合无共享图像的碎片化场景，突破了传统SfM聚合方法的限制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MRASfM通过将多相机系统视为刚性单元、结合语义信息过滤离群点、优化重建流程以及设计多场景聚合方法，实现了驾驶场景中更准确、高效和鲁棒的三维重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Structure from Motion (SfM) estimates camera poses and reconstructs pointclouds, forming a foundation for various tasks. However, applying SfM todriving scenes captured by multi-camera systems presents significantdifficulties, including unreliable pose estimation, excessive outliers in roadsurface reconstruction, and low reconstruction efficiency. To address theselimitations, we propose a Multi-camera Reconstruction and AggregationStructure-from-Motion (MRASfM) framework specifically designed for drivingscenes. MRASfM enhances the reliability of camera pose estimation by leveragingthe fixed spatial relationships within the multi-camera system during theregistration process. To improve the quality of road surface reconstruction,our framework employs a plane model to effectively remove erroneous points fromthe triangulated road surface. Moreover, treating the multi-camera set as asingle unit in Bundle Adjustment (BA) helps reduce optimization variables toboost efficiency. In addition, MRASfM achieves multi-scene aggregation throughscene association and assembly modules in a coarse-to-fine fashion. We deployedmulti-camera systems on actual vehicles to validate the generalizability ofMRASfM across various scenes and its robustness in challenging conditionsthrough real-world applications. Furthermore, large-scale validation results onpublic datasets show the state-of-the-art performance of MRASfM, achieving0.124 absolute pose error on the nuScenes dataset.</description>
      <author>example@mail.com (Lingfeng Xuan, Chang Nie, Yiqing Xu, Zhe Liu, Yanzi Miao, Hesheng Wang)</author>
      <guid isPermaLink="false">2510.15467v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Product Coefficients for Improved 3D LiDAR Data Classification (Part II)</title>
      <link>http://arxiv.org/abs/2510.15219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 6 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究扩展了之前关于使用乘积系数增强3D LiDAR点云分类的工作，展示了将乘积系数与自编码器表示和KNN分类器结合可以带来性能提升。&lt;h4&gt;背景&lt;/h4&gt;研究基于之前的工作，该工作引入了乘积系数（一种测度论描述符）来补充原始的LiDAR空间特征。&lt;h4&gt;目的&lt;/h4&gt;探索将乘积系数与自编码器表示和KNN分类器结合的方法，以提升LiDAR分类性能。&lt;h4&gt;方法&lt;/h4&gt;结合乘积系数与自编码器表示和KNN分类器，并与基于PCA的基线方法以及早期框架进行比较。还研究了逐级添加乘积系数的效果。&lt;h4&gt;主要发现&lt;/h4&gt;将乘积系数与自编码器表示和KNN分类器结合，在PCA基线和早期框架上带来了一致的性能提升。逐级添加乘积系数显示出更丰富的系数集合系统性地改善了类别可分离性和整体准确性。&lt;h4&gt;结论&lt;/h4&gt;结合分层乘积系数特征与自编码器对提升LiDAR分类性能具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;这项工作扩展了我们之前关于使用乘积系数增强3D LiDAR点云分类的研究，乘积系数是一种补充原始空间LiDAR特征的测度论描述符。在这里，我们展示了将乘积系数与自编码器表示和KNN分类器相结合，在基于PCA的基线和我们早期的框架上都能带来一致的性能提升。我们还研究了逐级添加乘积系数的影响，揭示了一个明显的趋势：更丰富的系数集合系统性地改善了类别可分离性和整体准确性。结果强调了将分层乘积系数特征与自编码器相结合以进一步提升LiDAR分类性能的价值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是改进3D LiDAR点云数据的分类性能。LiDAR技术广泛应用于数字高程模型更新、冰川和滑坡监测、海岸线分析和城市发展等领域，而将3D LiDAR点准确分类为语义类别（如植被、人造结构和水体）是这些应用中的关键步骤。提高分类准确率对于环境监测、城市规划、灾害评估等实际应用具有重要意义，特别是在气候变化研究中，如森林生长和碳封存能力的评估。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在之前工作[5]中引入了乘积系数作为度量论描述符，通过在原始空间Lidar特征基础上添加这些系数来增强分类性能。本研究进一步扩展了这个框架，借鉴了自编码器在表示学习方面的优势，用自编码器替代了之前工作使用的主成分分析(PCA)。作者认识到线性变换(如PCA)在捕获复杂特征依赖关系和减少冗余方面的局限性，因此引入了非线性表示学习方法。实验设计包括比较不同维度减少方法(PCA、自编码器、Nystroem)和不同分类器(KNN、随机森林)的性能，以验证新方法的有效性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 方法的核心思想是通过结合乘积系数和自编码器来增强3D LiDAR点云分类性能。乘积系数是基于度量论的特征，能够捕捉点云数据的局部结构信息，超越原始空间坐标。自编码器则学习非线性表示，能够更有效地捕获复杂特征依赖关系并减少冗余。整体实现流程包括：1) 特征生成：计算每个数据点周围局部邻域内的乘积系数，生成七个新特征；2) 特征标准化：将生成的特征标准化到单位立方体[0,1]^3；3) 维度减少：使用PCA或自编码器减少特征维度；4) 分类：使用KNN或随机森林分类器进行分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 引入自编码器替代PCA进行非线性表示学习，能够更有效地捕获复杂特征依赖关系；2) 系统地评估了不同级别乘积系数对分类性能的影响，发现更丰富的系数集能系统性地提高类别可分性和整体准确性；3) 实验证明结合乘积系数和自编码器的框架在分类准确率和F1分数上持续优于基于PCA的基线和之前的框架。相比之前的工作，主要不同在于使用了自编码器进行非线性表示学习，而不是使用PCA进行线性变换，以及更系统地评估了不同级别乘积系数的影响。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过结合基于度量论的乘积系数和自编码器非线性表示学习，显著提高了3D LiDAR点云分类的性能，为地理空间数据分析提供了一个更强大的框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work extends our previous study on enhancing 3D LiDAR point-cloudclassification with product coefficients\cite{medina2025integratingproductcoefficientsimproved}, measure-theoreticdescriptors that complement the original spatial Lidar features. Here, we showthat combining product coefficients with an autoencoder representation and aKNN classifier delivers consistent performance gains over both PCA-basedbaselines and our earlier framework. We also investigate the effect of addingproduct coefficients level by level, revealing a clear trend: richer sets ofcoefficients systematically improve class separability and overall accuracy.The results highlight the value of combining hierarchical product-coefficientfeatures with autoencoders to push LiDAR classification performance further.</description>
      <author>example@mail.com (Patricia Medina, Rasika Karkare)</author>
      <guid isPermaLink="false">2510.15219v1</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model</title>
      <link>http://arxiv.org/abs/2510.12276v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为空间强制(SF)的对齐策略，用于增强视觉-语言-动作(VLA)模型的空间理解能力，无需依赖明确的3D输入或深度估计器。&lt;h4&gt;背景&lt;/h4&gt;大多数VLA模型构建于仅基于2D数据预训练的视觉语言模型上，缺乏准确的空间感知能力，影响在3D物理世界中的操作。现有解决方案面临传感器噪声、硬件异构性和深度覆盖不完整等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法使VLA模型能够获得空间理解能力，而不依赖于明确的3D输入或深度估计器。&lt;h4&gt;方法&lt;/h4&gt;提出空间强制(SF)对齐策略，通过将VLA的中间视觉嵌入与预训练的3D基础模型生成的几何表示对齐，隐式强制VLA模型发展空间理解能力。&lt;h4&gt;主要发现&lt;/h4&gt;SF通过在中间层强制对齐，引导VLA编码更丰富的空间表示，提高动作精确度。实验表明SF取得了最先进的结果，超越了基于2D和3D的VLA，将训练速度提高了最多3.8倍，并提高了各种机器人任务的数据效率。&lt;h4&gt;结论&lt;/h4&gt;SF是一种简单有效的对齐策略，能够使VLA模型获得空间理解能力，提高性能和训练效率。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-动作(VLA)模型最近在使机器人能够遵循语言指令和执行精确动作方面显示出强大的潜力。然而，大多数VLA构建于仅基于2D数据预训练的视觉语言模型上，这些模型缺乏准确的空间感知能力，阻碍了它们在3D物理世界中的操作能力。现有解决方案尝试整合明确的3D传感器输入，如深度图或点云，但由于传感器噪声、硬件异构性和现有数据集中的深度覆盖不完整，这些方法面临挑战。从2D图像估计3D线索的替代方法也受限于深度估计器的有限性能。我们提出了空间强制(SF)，这是一种简单而有效的对齐策略，隐式强制VLA模型发展空间理解能力，而不依赖于明确的3D输入或深度估计器。SF将VLA的中间视觉嵌入与预训练的3D基础模型生成的几何表示对齐。通过在中间层强制对齐，SF引导VLA编码更丰富的空间表示，提高动作精确度。在模拟和真实环境中的大量实验表明，SF取得了最先进的结果，超越了基于2D和3D的VLA。SF进一步将训练速度提高了最多3.8倍，并提高了各种机器人任务的数据效率。项目页面位于https://spatial-forcing.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉-语言-动作模型(VLA)缺乏准确空间感知能力的问题，因为这些模型大多仅基于2D数据预训练，无法有效适应3D物理世界。这个问题在现实中很重要，因为机器人操作需要在3D世界中整合语义推理和精确控制；在研究中重要是因为现有解决方案要么依赖昂贵的3D传感器(面临噪声和硬件兼容性问题)，要么从2D图像估计3D信息(受限于深度估计器性能)，而本文方法提供了一个无需这些依赖的替代方案。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过深度探测实验观察到当前VLA模型的视觉嵌入缺乏有意义的空间结构，然后提出一种隐式增强空间理解能力的方法。他们借鉴了表示监督领域的进展，特别是表示对齐策略，利用预训练的3D基础模型VGGT提供丰富的空间表示作为监督信号。作者还参考了Huang等人的工作，发现监督相对较深但不是最深的层(第24层)效果最佳，因为太浅的层可能无法获得足够的空间信息，而太深的层则会丢失视觉特定特征。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过将VLA模型的中间视觉嵌入与预训练3D基础模型产生的几何表示进行对齐，隐式地强制模型发展空间理解能力，无需显式3D输入或深度估计器。实现流程是：1)输入多视角图像到VGGT模型生成空间表示；2)将VLA的中间视觉嵌入与这些空间表示进行对齐；3)使用余弦相似度作为对齐目标函数；4)选择第24层进行监督；5)将对齐损失与动作生成损失结合；6)推理阶段与标准VLA模型操作相同，无额外计算开销。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出Spatial Forcing方法，通过隐式对齐增强空间感知；2)不依赖显式3D传感器输入或深度估计器；3)通过中间层对齐引导模型编码丰富空间表示；4)实现训练加速(最高3.8倍)和数据效率提升。相比之前工作，不同之处在于：与显式3D输入方法相比，SF无需额外传感器；与从2D估计3D的方法相比，不受深度估计器限制；与现有表示监督方法不同，SF专注于空间表示对齐，特别针对VLA的空间感知提升。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了Spatial Forcing方法，通过将视觉-语言-动作模型的中间视觉嵌入与预训练3D基础模型的空间表示进行隐式对齐，显著提升了模型的空间感知能力、训练效率和数据利用率，无需依赖显式3D传感器输入或深度估计器。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language-action (VLA) models have recently shown strong potential inenabling robots to follow language instructions and execute precise actions.However, most VLAs are built upon vision-language models pretrained solely on2D data, which lack accurate spatial awareness and hinder their ability tooperate in the 3D physical world. Existing solutions attempt to incorporateexplicit 3D sensor inputs such as depth maps or point clouds, but theseapproaches face challenges due to sensor noise, hardware heterogeneity, andincomplete depth coverage in existing datasets. Alternative methods thatestimate 3D cues from 2D images also suffer from the limited performance ofdepth estimators. We propose Spatial Forcing (SF), a simple yet effectivealignment strategy that implicitly forces VLA models to develop spatialcomprehension capabilities without relying on explicit 3D inputs or depthestimators. SF aligns intermediate visual embeddings of VLAs with geometricrepresentations produced by pretrained 3D foundation models. By enforcingalignment at intermediate layers, SF guides VLAs to encode richer spatialrepresentations that enhance action precision. Extensive experiments insimulation and real-world environments demonstrate that SF achievesstate-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF furtheraccelerates training by up to 3.8x and improves data efficiency across diverserobotic tasks. Project page is at https://spatial-forcing.github.io/</description>
      <author>example@mail.com (Fuhao Li, Wenxuan Song, Han Zhao, Jingbo Wang, Pengxiang Ding, Donglin Wang, Long Zeng, Haoang Li)</author>
      <guid isPermaLink="false">2510.12276v2</guid>
      <pubDate>Mon, 20 Oct 2025 14:59:19 +0800</pubDate>
    </item>
    <item>
      <title>Programmatic Representation Learning with Language Models</title>
      <link>http://arxiv.org/abs/2510.14825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at https://github.com/gpoesia/leapr/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种称为'学习程序化表示'(LeaPR)的模型，它结合了决策树和通过大型语言模型(LLMs)合成的特征函数，能够在不依赖神经网络的情况下实现高质量的预测，同时保持模型的可解释性。&lt;h4&gt;背景&lt;/h4&gt;传统监督机器学习模型（如决策树）是高效且可解释的预测器，但其质量高度依赖于输入特征的选择。虽然神经网络可以直接从原始数据（如图像或文本）学习有用的表示，但这以牺牲可解释性和需要专门硬件高效运行为代价。&lt;h4&gt;目的&lt;/h4&gt;探索一种新的模型类LeaPR，它将表示为代码（从数据点到标量的函数）的任意特征与决策树预测器堆叠，从而在保持可解释性的同时实现高质量的预测。&lt;h4&gt;方法&lt;/h4&gt;1. 使用大型语言模型(LLMs)合成特征函数，利用它们在广泛领域的丰富先验知识和使用现有领域特定库编写代码的能力；2. 提出两种算法从监督数据中学习LeaPR模型：设计了FunSearch的适配版本来学习特征而非直接生成预测器；开发了经典ID3算法的新变体用于决策树学习，在分割叶节点时按需生成新特征。&lt;h4&gt;主要发现&lt;/h4&gt;在从国际象棋位置评估到图像和文本分类的实验中，该方法学习了高质量的无神经网络预测器，通常可与神经网络相媲美。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种灵活的范式，用于端到端学习可解释的表示，其中特征和预测可以轻松检查和理解。&lt;h4&gt;翻译&lt;/h4&gt;传统监督机器学习的经典模型，如决策树，是高效且可解释的预测器，但其质量高度依赖于特定输入特征的选择。虽然神经网络可以直接从原始数据（例如图像或文本）学习有用的表示，但这以牺牲可解释性和需要专门硬件高效运行为代价。在本文中，我们探索了一个称为学习程序化表示的假设类，它将表示为代码的任意特征（从数据点到标量的函数）与决策树预测器堆叠。我们使用大型语言模型合成特征函数，这些模型在广泛领域拥有丰富的先验知识，并且使用现有领域特定库编写代码的能力令人瞩目。我们提出了两种算法从监督数据中学习LeaPR模型。首先，我们设计了FunSearch的适配版本来学习特征而非直接生成预测器。然后，我们开发了经典ID3算法用于决策树学习的新变体，其中在分割叶节点时按需生成新特征。从国际象棋位置评估到图像和文本分类的实验中，我们的方法学习了高质量的无神经网络预测器，通常可与神经网络相媲美。我们的研究提出了一种灵活的范式，用于端到端学习可解释的表示，其中特征和预测可以轻松检查和理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classical models for supervised machine learning, such as decision trees, areefficient and interpretable predictors, but their quality is highly dependenton the particular choice of input features. Although neural networks can learnuseful representations directly from raw data (e.g., images or text), thiscomes at the expense of interpretability and the need for specialized hardwareto run them efficiently. In this paper, we explore a hypothesis class we callLearned Programmatic Representations (LeaPR) models, which stack arbitraryfeatures represented as code (functions from data points to scalars) anddecision tree predictors. We synthesize feature functions using Large LanguageModels (LLMs), which have rich prior knowledge in a wide range of domains and aremarkable ability to write code using existing domain-specific libraries. Wepropose two algorithms to learn LeaPR models from supervised data. First, wedesign an adaptation of FunSearch to learn features rather than directlygenerate predictors. Then, we develop a novel variant of the classical ID3algorithm for decision tree learning, where new features are generated ondemand when splitting leaf nodes. In experiments from chess position evaluationto image and text classification, our methods learn high-quality, neuralnetwork-free predictors often competitive with neural networks. Our worksuggests a flexible paradigm for learning interpretable representationsend-to-end where features and predictions can be readily inspected andunderstood.</description>
      <author>example@mail.com (Gabriel Poesia, Georgia Gabriela Sampaio)</author>
      <guid isPermaLink="false">2510.14825v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
  <item>
      <title>Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning</title>
      <link>http://arxiv.org/abs/2510.14819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PRTraj是一种新颖的轨迹表示学习框架，通过统一环境感知和路线选择建模来有效学习轨迹表示，解决了现有方法将轨迹视为孤立时空序列的局限。&lt;h4&gt;背景&lt;/h4&gt;现有轨迹表示学习方法将轨迹视为孤立的时空序列，忽略了形成轨迹的外部环境和内部路线选择行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够综合考虑外部环境和内部路线选择行为的轨迹表示学习框架，以生成更准确、更有效的轨迹嵌入表示。&lt;h4&gt;方法&lt;/h4&gt;PRTraj框架包含环境感知模块和路线选择编码器：环境感知模块通过捕获周围POI分布的多粒度环境语义增强道路网络；路线选择编码器将轨迹的组成路段转换建模为决策序列来捕获路线选择行为；最后将路线选择感知表示聚合形成全局轨迹嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;在3个真实世界数据集的5个下游任务上的广泛实验验证了PRTraj的有效性和泛化能力；PRTraj展现出强大的数据效率，在少样本场景下仍能保持稳健性能。&lt;h4&gt;结论&lt;/h4&gt;PRTraj通过结合环境感知和路线选择建模，显著提升了轨迹表示学习的效果，为各种下游任务提供了更高质量的轨迹嵌入。&lt;h4&gt;翻译&lt;/h4&gt;轨迹表示学习旨在将原始轨迹编码为低维向量，这些向量可在各种下游任务中利用，包括行程时间估计、位置预测和轨迹相似性分析。然而，现有的轨迹表示学习方法存在一个关键疏忽：将轨迹视为孤立的时空序列，而没有考虑支配其形成的外部环境和内部路线选择行为。为了弥合这一差距，我们提出了一种新颖的框架，统一了全面的环境感知和明确的路线选择建模，用于有效的轨迹表示学习，称为PRTraj。具体而言，PRTraj首先引入环境感知模块，通过捕获周围POI分布的多粒度环境语义来增强道路网络。基于这种环境感知骨干网络，路线选择编码器通过将轨迹的组成路段转换建模为决策序列来捕获每条轨迹固有的路线选择行为。这些路线选择感知表示最终被聚合形成全局轨迹嵌入。在3个真实世界数据集的5个下游任务上的广泛实验验证了PRTraj的有效性和泛化能力。此外，PRTraj展现出强大的数据效率，在少样本场景下保持稳健性能。我们的代码可在以下网址获取：https://anonymous.4open.science/r/PRTraj。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory Representation Learning (TRL) aims to encode raw trajectories intolow-dimensional vectors, which can then be leveraged in various downstreamtasks, including travel time estimation, location prediction, and trajectorysimilarity analysis. However, existing TRL methods suffer from a key oversight:treating trajectories as isolated spatio-temporal sequences, withoutconsidering the external environment and internal route choice behavior thatgovern their formation. To bridge this gap, we propose a novel framework thatunifies comprehensive environment \textbf{P}erception and explicit\textbf{R}oute choice modeling for effective \textbf{Traj}ectory representationlearning, dubbed \textbf{PRTraj}. Specifically, PRTraj first introduces anEnvironment Perception Module to enhance the road network by capturingmulti-granularity environmental semantics from surrounding POI distributions.Building on this environment-aware backbone, a Route Choice Encoder thencaptures the route choice behavior inherent in each trajectory by modeling itsconstituent road segment transitions as a sequence of decisions. Theseroute-choice-aware representations are finally aggregated to form the globaltrajectory embedding. Extensive experiments on 3 real-world datasets across 5downstream tasks validate the effectiveness and generalizability of PRTraj.Moreover, PRTraj demonstrates strong data efficiency, maintaining robustperformance under few-shot scenarios. Our code is available at:https://anonymous.4open.science/r/PRTraj.</description>
      <author>example@mail.com (Ji Cao, Yu Wang, Tongya Zheng, Zujie Ren, Canghong Jin, Gang Chen, Mingli Song)</author>
      <guid isPermaLink="false">2510.14819v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval</title>
      <link>http://arxiv.org/abs/2510.14535v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages,3 figures, 3 tables. Accepted at 2025 IEEE International  Conference on Systems, Man, and Cybernetics (IEEE SMC 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PL-SE-ADA的域调和框架，通过双编码器结构和对抗训练实现医学图像的域调和与可解释表示学习，同时保留与疾病相关的信息。&lt;h4&gt;背景&lt;/h4&gt;医学图像（如磁共振扫描）常因扫描仪和协议差异在不同成像站点间表现出域偏移，降低了机器学习在疾病分类等任务中的性能。现有方法虽能提取域不变和域特定特征，但缺乏医学应用所需的可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用的域调和框架，实现可解释的表示学习，同时保留脑磁共振图像中与疾病相关的信息。&lt;h4&gt;方法&lt;/h4&gt;提出PL-SE-ADA框架，包含两个编码器分别提取域不变和域特定特征，一个解码器用于重建图像，以及一个域预测器。模型通过对抗训练学习，并通过将域不变和域特定特征的重建求和来重构输入图像。&lt;h4&gt;主要发现&lt;/h4&gt;PL-SE-ADA在图像重建、疾病分类和域识别方面实现了与先前方法相当或更好的性能，同时能够可视化域独立的脑特征和域特定成分，提供了高可解释性。&lt;h4&gt;结论&lt;/h4&gt;PL-SE-ADA是一种有效的域调和框架，不仅提高了医学图像处理任务的性能，还提供了必要的可解释性，解决了医学应用中的实际问题。&lt;h4&gt;翻译&lt;/h4&gt;医学图像如磁共振扫描通常因扫描仪和协议差异在不同成像站点间表现出域偏移，这降低了机器学习在疾病分类等任务中的性能。域调和因此成为关键研究焦点。近期方法将脑图像编码到低维潜在空间并分离为域不变和域特定成分，但往往缺乏医学应用所需的可解释性。我们提出PL-SE-ADA框架，包含两个编码器提取域不变和域特定特征，一个解码器用于重建图像，以及一个域预测器。模型通过对抗训练学习，并通过将域不变和域特定特征的重建求和来重构输入图像，确保调和效果和信息保留。与先前方法相比，PL-SE-ADA在图像重建、疾病分类和域识别方面表现相当或更好，同时提供了高可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical images like MR scans often show domain shifts across imaging sitesdue to scanner and protocol differences, which degrade machine learningperformance in tasks such as disease classification. Domain harmonization isthus a critical research focus. Recent approaches encode brain images$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, thendisentangle it into $\boldsymbol{z_u}$ (domain-invariant) and$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, thesemethods often lack interpretability$-$an essential requirement in medicalapplications$-$leaving practical issues unresolved. We proposePseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), ageneral framework for domain harmonization and interpretable representationlearning that preserves disease-relevant information in brain MR images.PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image$f_D$, and a domain predictor $g_D$. Beyond adversarial training between theencoder and domain predictor, the model learns to reconstruct the input image$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Comparedto prior methods, PL-SE-ADA achieves equal or better performance in imagereconstruction, disease classification, and domain recognition. It also enablesvisualization of both domain-independent brain features and domain-specificcomponents, offering high interpretability across the entire framework.</description>
      <author>example@mail.com (Keima Abe, Hayato Muraki, Shuhei Tomoshige, Kenichi Oishi, Hitoshi Iyatomi)</author>
      <guid isPermaLink="false">2510.14535v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Revisit Modality Imbalance at the Decision Layer</title>
      <link>http://arxiv.org/abs/2510.14411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Some Insights in Balanced Multimodal Learning&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态学习面临模态不平衡问题，这种不平衡不仅存在于表示学习阶段，也在决策层显著存在。研究表明，即使在充分预训练后，模型仍表现出对某些模态的系统偏见，这种偏见源于特征空间和决策权重分布的内在差异，而非仅由优化动态导致。作者建议在决策层引入自适应权重分配机制以实现更平衡的模态融合。&lt;h4&gt;背景&lt;/h4&gt;多模态学习整合不同模态信息以增强模型性能，但常面临模态不平衡问题，即主导模态在联合优化过程中掩盖较弱模态。&lt;h4&gt;目的&lt;/h4&gt;揭示模态不平衡不仅在表示学习阶段存在，也在决策层显著表现，并提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;在音频-视觉数据集（CREMAD和Kinetic-Sounds）上进行实验，分析模型在预训练和平衡优化后对模态的偏见，研究特征空间和决策权重分布的差异。&lt;h4&gt;主要发现&lt;/h4&gt;1) 模态不平衡不仅存在于表示学习阶段，也在决策层显著存在；2) 即使在充分预训练和平衡优化后，模型仍表现出对某些模态（如音频）的系统偏见；3) 这种偏见源于特征空间和决策权重分布的内在差异，而非仅由优化动态导致；4) 在融合阶段聚合未校准的模态输出会导致决策层的加权偏差。&lt;h4&gt;结论&lt;/h4&gt;未来的多模态系统应该在决策层更多地纳入自适应权重分配机制，使各模态能够根据其能力实现相对平衡，从而有效利用较弱模态的贡献。&lt;h4&gt;翻译&lt;/h4&gt;多模态学习整合来自不同模态的信息以增强模型性能，但它常常遭受模态不平衡的影响，在联合优化过程中主导模态会掩盖较弱的模态。本文揭示这种不平衡不仅发生在表示学习阶段，而且在决策层也显著表现。在音频-视觉数据集（CREMAD和Kinetic-Sounds）上的实验表明，即使在广泛的预训练和平衡优化后，模型仍然表现出对某些模态（如音频）的系统偏见。进一步分析表明，这种偏见源于特征空间和决策权重分布的内在差异，而不仅仅是优化动态。我们认为，在融合阶段聚合未校准的模态输出会导致决策层的加权偏差，阻碍较弱模态的有效贡献。为此，我们建议未来的多模态系统应该更注重在决策层纳入自适应权重分配机制，使各模态能够根据其能力实现相对平衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning integrates information from different modalities toenhance model performance, yet it often suffers from modality imbalance, wheredominant modalities overshadow weaker ones during joint optimization. Thispaper reveals that such an imbalance not only occurs during representationlearning but also manifests significantly at the decision layer. Experiments onaudio-visual datasets (CREMAD and Kinetic-Sounds) show that even afterextensive pretraining and balanced optimization, models still exhibitsystematic bias toward certain modalities, such as audio. Further analysisdemonstrates that this bias originates from intrinsic disparities infeature-space and decision-weight distributions rather than from optimizationdynamics alone. We argue that aggregating uncalibrated modality outputs at thefusion stage leads to biased decision-layer weighting, hindering weakermodalities from contributing effectively. To address this, we propose thatfuture multimodal systems should focus more on incorporate adaptive weightallocation mechanisms at the decision layer, enabling relative balancedaccording to the capabilities of each modality.</description>
      <author>example@mail.com (Xiaoyu Ma, Hao Chen)</author>
      <guid isPermaLink="false">2510.14411v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis</title>
      <link>http://arxiv.org/abs/2510.14403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了DCMIL模型，用于处理全切片图像(WSI)进行癌症预后预测，解决了计算瓶颈和注释稀缺的问题，并在多种癌症类型中表现出色。&lt;h4&gt;背景&lt;/h4&gt;计算病理学新兴学科利用WSI量化形态异性和开发癌症预后模型，但受千兆像素级输入的计算瓶颈和密集手动注释稀缺的阻碍，且当前方法忽视了多倍率WSI中的细粒度信息和肿瘤微环境变异。&lt;h4&gt;目的&lt;/h4&gt;开发一个易于到难的正向表示学习模型(DCMIL)，高效处理WSI用于癌症预后预测，不依赖密集注释，并能直接将千兆像素级WSI转化为结果预测。&lt;h4&gt;方法&lt;/h4&gt;提出名为双课程对比多实例学习(DCMIL)的模型，是一种正向表示学习模型，能高效处理WSI，不需要密集注释，可直接将大型WSI图像转化为预后预测。&lt;h4&gt;主要发现&lt;/h4&gt;在12种癌症类型(5,954名患者，1,254万张图像块)的实验中，DCMIL优于标准WSI预后模型；能识别细粒度预后显著区域；提供稳健实例不确定性估计；捕获正常与肿瘤组织形态差异；有潜力产生新生物学见解。&lt;h4&gt;结论&lt;/h4&gt;DCMIL模型在癌症预后预测方面表现出色，不需要密集注释，能直接处理大型WSI图像，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;蓬勃发展的计算病理学学科显示出利用全切片图像(WSIs)量化形态异质性并为人类癌症开发客观预后模型的希望。然而，千兆像素级输入的计算瓶颈和密集手动注释的稀缺阻碍了进展。当前方法常常忽视了多倍率WSI中的细粒度信息和肿瘤微环境的变异。在这里，我们提出一个易于到难的正向表示学习模型，称为双课程对比多实例学习(DCMIL)，以高效处理WSI用于癌症预后。该模型不依赖于密集注释，并能将千兆像素级WSI直接转化为结果预测。在十二种癌症类型(5,954名患者，1,254万张图像块)的大量实验中证明，DCMIL优于标准的基于WSI的预后模型。此外，DCMIL能识别细粒度的预后显著区域，提供稳健的实例不确定性估计，并捕获正常组织和肿瘤组织之间的形态差异，有潜力产生新的生物学见解。所有代码已在https://github.com/tuuuc/DCMIL上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The burgeoning discipline of computational pathology shows promise inharnessing whole slide images (WSIs) to quantify morphological heterogeneityand develop objective prognostic modes for human cancers. However, progress isimpeded by the computational bottleneck of gigapixel-size inputs and thescarcity of dense manual annotations. Current methods often overlookfine-grained information across multi-magnification WSIs and variations intumor microenvironments. Here, we propose an easy-to-hard progressiverepresentation learning model, termed dual-curriculum contrastivemulti-instance learning (DCMIL), to efficiently process WSIs for cancerprognosis. The model does not rely on dense annotations and enables the directtransformation of gigapixel-size WSIs into outcome predictions. Extensiveexperiments on twelve cancer types (5,954 patients, 12.54 million tiles)demonstrate that DCMIL outperforms standard WSI-based prognostic models.Additionally, DCMIL identifies fine-grained prognosis-salient regions, providesrobust instance uncertainty estimation, and captures morphological differencesbetween normal and tumor tissues, with the potential to generate new biologicalinsights. All codes have been made publicly accessible athttps://github.com/tuuuc/DCMIL.</description>
      <author>example@mail.com (Chao Tu, Kun Huang, Jie Zhang, Qianjin Feng, Yu Zhang, Zhenyuan Ning)</author>
      <guid isPermaLink="false">2510.14403v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection</title>
      <link>http://arxiv.org/abs/2510.14344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了BINCTX，一种多模态学习方法，用于检测移动应用中的不良行为，通过结合代码级语义、行为触发方式和第三方库使用信息，实现了高准确率的检测。&lt;h4&gt;背景&lt;/h4&gt;移动应用市场有数百万个应用，但不良行为（如干扰性广告、非法重定向、支付欺诈）难以被发现，因为这些行为通常不依赖权限保护的API，且可通过UI或元数据编辑轻易伪装。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效检测移动应用中不良行为的机器学习方法，提高检测的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;BINCTX构建应用的三种视图：全局字节码图像视图（捕获代码级语义和家族模式）、上下文视图（显示行为触发方式）和第三方库使用视图（总结组件间调用路径上的调用频率），然后将这三种视图嵌入并融合，训练上下文感知分类器。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界恶意软件和良性应用上，BINCTX达到94.73%的宏观F1值，比强大基线方法至少高出14.92%；在商业混淆下保持84%的F1值；比最先进的仅字节码系统更能抵抗对抗样本。&lt;h4&gt;结论&lt;/h4&gt;BINCTX通过多模态表示学习，有效结合了代码级语义、行为上下文和第三方库使用信息，显著提高了移动应用不良行为的检测性能，并增强了对混淆技术和对抗攻击的抵抗力。&lt;h4&gt;翻译&lt;/h4&gt;移动应用市场托管着数百万个应用，但不良行为（例如干扰性广告、非法重定向、支付欺诈）仍然难以被发现，因为它们通常不依赖于权限保护的API，并且可以通过UI或元数据编辑轻松伪装。我们提出了BINCTX，一种学习方法，它从(i)全局字节码图像视图捕获代码级语义和家族模式，(ii)上下文视图（显示的操作、组件、声明的权限、URL/IP常量）指示行为如何被触发，以及(iii)第三方库使用视图总结组件间调用路径上的调用频率，构建应用的多模态表示。这三个视图被嵌入并融合，训练一个上下文感知分类器。在真实世界的恶意软件和良性应用上，BINCTX实现了94.73%的宏观F1值，比强大的基线方法至少高出14.92%。它在商业混淆下保持鲁棒性（混淆后F1为84%），并且比最先进的仅字节码系统更能抵抗对抗样本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobile app markets host millions of apps, yet undesired behaviors (e.g.,disruptive ads, illegal redirection, payment deception) remain hard to catchbecause they often do not rely on permission-protected APIs and can be easilycamouflaged via UI or metadata edits. We present BINCTX, a learning approachthat builds multi-modal representations of an app from (i) a globalbytecode-as-image view that captures code-level semantics and family-stylepatterns, (ii) a contextual view (manifested actions, components, declaredpermissions, URL/IP constants) indicating how behaviors are triggered, and(iii) a third-party-library usage view summarizing invocation frequencies alonginter-component call paths. The three views are embedded and fused to train acontextual-aware classifier. On real-world malware and benign apps, BINCTXattains a macro F1 of 94.73%, outperforming strong baselines by at least14.92%. It remains robust under commercial obfuscation (F1 84%post-obfuscation) and is more resistant to adversarial samples thanstate-of-the-art bytecode-only systems.</description>
      <author>example@mail.com (Zichen Liu, Shao Yang, Xusheng Xiao)</author>
      <guid isPermaLink="false">2510.14344v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm</title>
      <link>http://arxiv.org/abs/2510.14321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为大型推理嵌入模型(LREM)的新方法，通过将推理过程整合到表示学习中，解决了电子商务搜索系统中困难查询的语义匹配问题，显著提高了检索准确性。&lt;h4&gt;背景&lt;/h4&gt;在现代电子商务搜索系统中，密集检索是重要组成部分。主流嵌入模型已从BERT转向大型语言模型(LLMs)，但仍采用直接嵌入方法，语义准确性不足。对比学习虽被使用，但模型倾向于捕获统计共现模式，偏向浅层词汇和语义匹配，导致对困难查询的性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出大型推理嵌入模型(LREM)，创新地将推理过程整合到表示学习中，以解决困难查询与目标物品之间的语义匹配问题，提高检索准确性。&lt;h4&gt;方法&lt;/h4&gt;LREM对困难查询先进行推理以深入理解查询，然后生成推理增强的查询嵌入用于检索。采用两阶段训练：第一阶段在Query-CoT-Item三元组上使用SFT和InfoNCE损失优化LLM；第二阶段通过强化学习进一步优化推理轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;推理过程有效桥接了原始查询和目标物品间的语义差距，显著提高了检索准确性。大量离线和在线实验验证了LREM的有效性。&lt;h4&gt;结论&lt;/h4&gt;LREM已被成功部署在中国最大的电子商务平台上，自2025年8月起，证明了其在实际应用中的价值。&lt;h4&gt;翻译&lt;/h4&gt;在现代电子商务搜索系统中，密集检索已成为不可或缺的组成部分。通过计算查询和物品(产品)嵌入之间的相似性，它能够从大规模存储库中高效地选择候选产品。随着大型语言模型(LLMs)的突破，主流嵌入模型已逐渐从BERT转向LLMs以实现更准确的文本建模。然而，这些模型仍采用直接嵌入方法，嵌入的语义准确性仍然不足。因此，对比学习被大量使用来实现正对之间的紧密语义对齐。结果，这些模型倾向于捕获训练数据中的统计共现模式，偏向于浅层词汇和语义匹配。对于与目标物品存在明显词汇差异的困难查询，性能显著下降。在这项工作中，我们提出了大型推理嵌入模型(LREM)，创新地将推理过程整合到表示学习中。对于困难查询，LREM首先进行推理以实现对原始查询的深入理解，然后生成推理增强的查询嵌入用于检索。这一推理过程有效地桥接了原始查询和目标物品之间的语义差距，显著提高了检索准确性。具体而言，我们采用两阶段训练过程：第一阶段在精心策划的查询-思维链-物品(Query-CoT-Item)三元组上使用SFT和InfoNCE损失优化LLM，建立初步推理和嵌入能力；第二阶段通过强化学习(RL)进一步优化推理轨迹。大量的离线和在线实验验证了LREM的有效性，使其自2025年8月起被部署在中国最大的电子商务平台上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In modern e-commerce search systems, dense retrieval has become anindispensable component. By computing similarities between query and item(product) embeddings, it efficiently selects candidate products fromlarge-scale repositories. With the breakthroughs in large language models(LLMs), mainstream embedding models have gradually shifted from BERT to LLMsfor more accurate text modeling. However, these models still adoptdirect-embedding methods, and the semantic accuracy of embeddings remainsinadequate. Therefore, contrastive learning is heavily employed to achievetight semantic alignment between positive pairs. Consequently, such models tendto capture statistical co-occurrence patterns in the training data, biasingthem toward shallow lexical and semantic matches. For difficult queriesexhibiting notable lexical disparity from target items, the performancedegrades significantly. In this work, we propose the Large Reasoning EmbeddingModel (LREM), which novelly integrates reasoning processes into representationlearning. For difficult queries, LREM first conducts reasoning to achieve adeep understanding of the original query, and then produces areasoning-augmented query embedding for retrieval. This reasoning processeffectively bridges the semantic gap between original queries and target items,significantly improving retrieval accuracy. Specifically, we adopt a two-stagetraining process: the first stage optimizes the LLM on carefully curatedQuery-CoT-Item triplets with SFT and InfoNCE losses to establish preliminaryreasoning and embedding capabilities, and the second stage further refines thereasoning trajectories via reinforcement learning (RL). Extensive offline andonline experiments validate the effectiveness of LREM, leading to itsdeployment on China's largest e-commerce platform since August 2025.</description>
      <author>example@mail.com (Jianting Tang, Dongshuai Li, Tao Wen, Fuyu Lv, Dan Ou, Linli Xu)</author>
      <guid isPermaLink="false">2510.14321v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks</title>
      <link>http://arxiv.org/abs/2510.14139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review in Frontiers in Bioinformatics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;蛋白质-蛋白质相互作用的准确预测对于理解细胞功能和推进药物开发至关重要。现有的计算方法使用蛋白质语言模型(PLMs)的直接序列嵌入，或使用图神经网络(GNNs)处理3D蛋白质结构。本研究探索计算密集度较低的替代方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的框架用于下游PPI预测，通过链接预测实现。&lt;h4&gt;方法&lt;/h4&gt;引入一个两阶段的图表示学习框架ProtGram-DirectGCN。第一阶段开发ProtGram，将蛋白质的一级结构建模为全局推断的n-gram图层次结构，其中残基转移概率定义边权重。第二阶段提出DirectGCN，一种定制的有向图卷积神经网络，通过入向、出向和无向路径的转换处理信息，并通过可学习的门控机制结合这些路径。&lt;h4&gt;主要发现&lt;/h4&gt;DirectGCN在标准节点分类基准上表现良好，性能与已建立的方法相当，尤其在具有密集、异质结构的有向复杂图中表现出色。完整的ProtGram-DirectGCN框架应用于PPI预测时提供了强大的预测能力，即使在有限的训练数据下也能保持。&lt;h4&gt;结论&lt;/h4&gt;ProtGram-DirectGCN框架是一种有效的PPI预测方法，在计算资源有限的情况下也能保持良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;引言：准确预测蛋白质相互作用对于理解细胞功能和推进药物开发至关重要。现有的计算方法使用蛋白质语言模型的直接序列嵌入，或使用图神经网络处理3D蛋白质结构。本研究探索计算密集度较低的替代方法。我们引入了一种通过链接预测进行下游PPI预测的新框架。方法：我们引入了一个两阶段的图表示学习框架ProtGram-DirectGCN。首先，我们开发了ProtGram，该方法将蛋白质的一级结构建模为全局推断的n-gram图层次结构。在这些图中，残基转移概率定义边权重，每条边在有向图中连接一对残基，这些概率从大量序列集合中聚合。其次，我们提出了DirectGCN，一种定制的有向图卷积神经网络，该模型具有独特的卷积层，通过入向、出向和无向的特定路径转换处理信息，同时应用共享转换，这些路径通过可学习的门控机制结合。我们将DirectGCN应用于ProtGram图以学习残基级嵌入，并通过注意力池化生成蛋白质级嵌入进行预测。结果：我们首先在标准节点分类基准上建立了DirectGCN的有效性，其在一般数据集上的性能与已建立的方法相当，该模型在具有密集、异质结构的有向复杂图中表现出色。当应用于PPI预测时，完整的ProtGram-DirectGCN框架提供了强大的预测能力，即使在有限的训练数据下，这种强大的性能仍然保持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3389/fbinf.2025.1651623&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Introduction Accurate prediction of protein-protein interactions (PPIs) iscrucial for understanding cellular functions and advancing drug development.Existing in-silico methods use direct sequence embeddings from Protein LanguageModels (PLMs). Others use Graph Neural Networks (GNNs) for 3D proteinstructures. This study explores less computationally intensive alternatives. Weintroduce a novel framework for downstream PPI prediction through linkprediction. Methods We introduce a two-stage graph representation learningframework, ProtGram-DirectGCN. First, we developed ProtGram. This approachmodels a protein's primary structure as a hierarchy of globally inferred n-gramgraphs. In these graphs, residue transition probabilities define edge weights.Each edge connects a pair of residues in a directed graph. The probabilitiesare aggregated from a large corpus of sequences. Second, we propose DirectGCN,a custom directed graph convolutional neural network. This model features aunique convolutional layer. It processes information through separatepath-specific transformations: incoming, outgoing, and undirected. A sharedtransformation is also applied. These paths are combined via a learnable gatingmechanism. We apply DirectGCN to ProtGram graphs to learn residue-levelembeddings. These embeddings are pooled via attention to generate protein-levelembeddings for prediction. Results We first established the efficacy ofDirectGCN on standard node classification benchmarks. Its performance matchesestablished methods on general datasets. The model excels at complex, directedgraphs with dense, heterophilic structures. When applied to PPI prediction, thefull ProtGram-DirectGCN framework delivers robust predictive power. This strongperformance holds even with limited training data.</description>
      <author>example@mail.com (Islam Akef Ebeid, Haoteng Tang, Pengfei Gu)</author>
      <guid isPermaLink="false">2510.14139v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management</title>
      <link>http://arxiv.org/abs/2510.14112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为STEMS的新型安全约束多智能体强化学习框架，用于协调建筑能源管理，有效解决了多建筑系统中时空依赖关系利用和操作安全性的挑战。&lt;h4&gt;背景&lt;/h4&gt;建筑能源管理对于实现碳减排目标、提高居住者舒适度和降低能源成本至关重要。当前多建筑能源系统面临三个关键挑战：时空信息利用不足、缺乏严格的安全保证以及系统复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的安全约束多智能体强化学习框架，解决多建筑协调能源管理中的挑战，特别是在利用时空依赖关系和确保操作安全方面。&lt;h4&gt;方法&lt;/h4&gt;STEMS框架整合了两个核心组件：(1)时空图表示学习框架，使用GCN-Transformer融合架构捕捉建筑间关系和时间模式；(2)安全约束多智能体RL算法，结合控制屏障函数提供数学安全保证。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明STEMS相比现有方法具有优越性能，实现了21%的成本降低，18%的排放减少，将安全违规从35.1%大幅降低到5.6%，并保持最优舒适度，仅有0.13%的不舒适比例。该框架在极端天气条件下表现出强大的鲁棒性，并在不同类型建筑中保持有效性。&lt;h4&gt;结论&lt;/h4&gt;STEMS框架成功解决了多建筑能源管理中的关键挑战，通过整合时空信息利用和安全约束，实现了显著的能源成本降低和排放减少，同时确保了系统安全和居住者舒适度。&lt;h4&gt;翻译&lt;/h4&gt;建筑能源管理对于实现碳减排目标、提高居住者舒适度和降低能源成本至关重要。协调建筑能源管理在利用时空依赖关系的同时确保多建筑系统运行安全方面面临关键挑战。当前多建筑能源系统面临三个关键挑战：时空信息利用不足、缺乏严格的安全保证以及系统复杂性。本文提出STEMS，一种用于协调建筑能源管理的新型安全约束多智能体强化学习框架。STEMS整合了两个核心组件：(1)使用GCN-Transformer融合架构的时空图表示学习框架，用于捕捉建筑间关系和时间模式；(2)结合控制屏障函数的安全约束多智能体RL算法，提供数学安全保证。在真实建筑数据集上的大量实验表明STEMS相比现有方法具有优越性能，实现了21%的成本降低，18%的排放减少，同时将安全违规从35.1%大幅降低到5.6%，并保持最优舒适度，仅有0.13%的不舒适比例。该框架在极端天气条件下也表现出强大的鲁棒性，并且在不同类型建筑中保持有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building energy management is essential for achieving carbon reduction goals,improving occupant comfort, and reducing energy costs. Coordinated buildingenergy management faces critical challenges in exploiting spatial-temporaldependencies while ensuring operational safety across multi-building systems.Current multi-building energy systems face three key challenges: insufficientspatial-temporal information exploitation, lack of rigorous safety guarantees,and system complexity. This paper proposes Spatial-Temporal Enhanced SafeMulti-Agent Coordination (STEMS), a novel safety-constrained multi-agentreinforcement learning framework for coordinated building energy management.STEMS integrates two core components: (1) a spatial-temporal graphrepresentation learning framework using a GCN-Transformer fusion architectureto capture inter-building relationships and temporal patterns, and (2) asafety-constrained multi-agent RL algorithm incorporating Control BarrierFunctions to provide mathematical safety guarantees. Extensive experiments onreal-world building datasets demonstrate STEMS's superior performance overexisting methods, showing that STEMS achieves 21% cost reduction, 18% emissionreduction, and dramatically reduces safety violations from 35.1% to 5.6% whilemaintaining optimal comfort with only 0.13 discomfort proportion. The frameworkalso demonstrates strong robustness during extreme weather conditions andmaintains effectiveness across different building types.</description>
      <author>example@mail.com (Huiliang Zhang, Di Wu, Arnaud Zinflou, Benoit Boulet)</author>
      <guid isPermaLink="false">2510.14112v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations</title>
      <link>http://arxiv.org/abs/2510.14049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了一个新的因果表征学习(CRL)基准，使用高保真模拟视觉数据，既保留真实视觉复杂性又能访问真实因果生成过程，包含约20万张图像和300万视频帧，涵盖四个领域的24个子场景。&lt;h4&gt;背景&lt;/h4&gt;因果表征学习旨在揭示数据生成过程并识别潜在因果变量和关系，但评估具有挑战性，因为需要已知的真实因果变量和结构。现有评估方法要么依赖简化合成数据集，要么依赖现实世界任务中的下游性能，在真实性和评估精度间面临两难困境。&lt;h4&gt;目的&lt;/h4&gt;创建一个既保留真实视觉复杂性又能访问真实因果生成过程的新CRL基准，解决现有评估方法在真实性和评估精度之间的两难困境。&lt;h4&gt;方法&lt;/h4&gt;构建包含约20万张图像和300万视频帧的数据集，涵盖静态图像生成、动态物理模拟、机器人操作和交通情况分析四个领域的24个子场景，提供对底层因果结构的灵活访问，允许用户修改或配置以符合CRL假设要求。&lt;h4&gt;主要发现&lt;/h4&gt;利用此基准评估了不同范式的代表性CRL方法，提供了实证见解，帮助实践者和新手选择或扩展适当的CRL框架，以解决可以从CRL视角受益的现实问题。&lt;h4&gt;结论&lt;/h4&gt;该基准有望弥合严格评估和实际应用之间的差距，为CRL研究提供更全面、更真实的测试平台。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已为中文，无需额外翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal Representation Learning (CRL) aims to uncover the data-generatingprocess and identify the underlying causal variables and relations, whoseevaluation remains inherently challenging due to the requirement of knownground-truth causal variables and causal structure. Existing evaluations oftenrely on either simplistic synthetic datasets or downstream performance onreal-world tasks, generally suffering a dilemma between realism and evaluativeprecision. In this paper, we introduce a new benchmark for CRL usinghigh-fidelity simulated visual data that retains both realistic visualcomplexity and, more importantly, access to ground-truth causal generatingprocesses. The dataset comprises around 200 thousand images and 3 million videoframes across 24 sub-scenes in four domains: static image generation, dynamicphysical simulations, robotic manipulations, and traffic situation analysis.These scenarios range from static to dynamic settings, simple to complexstructures, and single to multi-agent interactions, offering a comprehensivetestbed that hopefully bridges the gap between rigorous evaluation andreal-world applicability. In addition, we provide flexible access to theunderlying causal structures, allowing users to modify or configure them toalign with the required assumptions in CRL, such as available domain labels,temporal dependencies, or intervention histories. Leveraging this benchmark, weevaluated representative CRL methods across diverse paradigms and offeredempirical insights to assist practitioners and newcomers in choosing orextending appropriate CRL frameworks to properly address specific types of realproblems that can benefit from the CRL perspective. Welcome to visit our:Project page:https://causal-verse.github.io/,Dataset:https://huggingface.co/CausalVerse.</description>
      <author>example@mail.com (Guangyi Chen, Yunlong Deng, Peiyuan Zhu, Yan Li, Yifan Sheng, Zijian Li, Kun Zhang)</author>
      <guid isPermaLink="false">2510.14049v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2510.14470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了结合语言模型的图基础模型在文本属性图上的安全漏洞，特别是后门攻击问题，并提出了一种双触发攻击框架。&lt;h4&gt;背景&lt;/h4&gt;图基础模型，特别是结合语言模型的模型，已革新图学习并在文本属性图上表现出色，但相比传统GNN引入了新的安全漏洞。&lt;h4&gt;目的&lt;/h4&gt;解决LM赋能的GFMs在无安全提示调整阶段的安全漏洞问题，特别是在属性不可访问的约束TAG系统中的后门攻击挑战。&lt;h4&gt;方法&lt;/h4&gt;提出一种新的双触发后门攻击框架，在文本层面和结构层面同时运作，通过利用预先建立的文本池实现无需显式优化触发节点文本属性的有效攻击。&lt;h4&gt;主要发现&lt;/h4&gt;传统图后门攻击在属性不可访问的约束TAG系统中性能显著下降；所提双触发攻击框架能保持优越的干净准确率并取得出色的攻击成功率。&lt;h4&gt;结论&lt;/h4&gt;LM赋能的GFMs在网络部署中存在关键后门风险，研究为基础模型时代开源平台开发更强大的监督机制提供了贡献。&lt;h4&gt;翻译&lt;/h4&gt;图基础模型的出现，特别是那些结合语言模型的模型，已经革新了图学习并在文本属性图上表现出色。然而，与传统GNN相比，这些由语言模型赋能的图基础模型在无安全提示调整阶段引入了独特的安全漏洞，这些漏洞在当前研究中尚未得到充分研究。通过实证研究，我们发现在属性不可访问的约束文本属性图系统中，当没有显式优化触发节点属性时，传统图后门攻击的性能会显著下降。为此，我们提出了一种新的双触发后门攻击框架，在文本层面和结构层面同时运作，通过战略性地利用预先建立的文本池，无需显式优化触发节点文本属性即可实现有效攻击。大量实验评估表明，我们的攻击方法在保持优越的干净准确率的同时，取得了出色的攻击成功率，包括在高度隐蔽的单触发节点场景中。我们的工作强调了在网络上部署的由语言模型赋能的图基础模型中的关键后门风险，并为基础模型时代开源平台开发更强大的监督机制做出了贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of graph foundation models (GFMs), particularly thoseincorporating language models (LMs), has revolutionized graph learning anddemonstrated remarkable performance on text-attributed graphs (TAGs). However,compared to traditional GNNs, these LM-empowered GFMs introduce unique securityvulnerabilities during the unsecured prompt tuning phase that remainunderstudied in current research. Through empirical investigation, we reveal asignificant performance degradation in traditional graph backdoor attacks whenoperating in attribute-inaccessible constrained TAG systems without explicittrigger node attribute optimization. To address this, we propose a noveldual-trigger backdoor attack framework that operates at both text-level andstruct-level, enabling effective attacks without explicit optimization oftrigger node text attributes through the strategic utilization of apre-established text pool. Extensive experimental evaluations demonstrate thatour attack maintains superior clean accuracy while achieving outstanding attacksuccess rates, including scenarios with highly concealed single-trigger nodes.Our work highlights critical backdoor risks in web-deployed LM-empowered GFMsand contributes to the development of more robust supervision mechanisms foropen-source platforms in the era of foundation models.</description>
      <author>example@mail.com (Xiaoyu Xue, Yuni Lai, Chenxi Huang, Yulin Zhu, Gaolei Li, Xiaoge Zhang, Kai Zhou)</author>
      <guid isPermaLink="false">2510.14470v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>DARTS-GT: Differentiable Architecture Search for Graph Transformers with Quantifiable Instance-Specific Interpretability Analysis</title>
      <link>http://arxiv.org/abs/2510.14336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的图变换器架构DARTS-GT，通过不对称注意力和可微分架构搜索实现深度异质性，并开发了首个图变换器的定量可解释性框架。实验表明该方法在多个数据集上达到最先进水平，且发现的异构架构比基线更可解释，证明图变换器无需在性能和可解释性间做出取舍。&lt;h4&gt;背景&lt;/h4&gt;图变换器(GTs)是处理图结构数据的有力架构，但受限于刚性设计和缺乏可量化可解释性。当前最先进的GT在所有层中固定使用相同的GNN类型，错过了深度特定组件选择的优势，且复杂架构变得不透明，无法区分性能提升中的有意义模式和虚假相关性。&lt;h4&gt;目的&lt;/h4&gt;重新设计GT注意力机制通过不对称性解耦结构编码与特征表示；使用DARTS在每层选择最优GNN算子；开发首个GT的定量可解释性框架；探索GT是否需要在性能和可解释性之间做出选择。&lt;h4&gt;方法&lt;/h4&gt;重新设计GT注意力：查询来自节点特征，键和值来自GNN变换；使用DARTS在transformer注意力内部实现深度异质性(DARTS-GT)；通过因果消融开发GT的定量可解释性框架；提出Head-deviation、Specialization和Focus指标；在8个基准数据集上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;DARTS-GT在4个数据集上达到最先进水平，在其他数据集上保持竞争力；发现的架构揭示了数据集特定模式；可视化注意力和因果重要性并不总是相关，表明常用可视化方法可能忽略真正重要的组件；DARTS-GT发现的异构架构比基线产生更可解释的模型。&lt;h4&gt;结论&lt;/h4&gt;Graph Transformers不需要在性能和可解释性之间做出选择。异构架构可以同时提高性能和可解释性，证明性能和可解释性并非相互排斥的目标。&lt;h4&gt;翻译&lt;/h4&gt;图变换器(GTs)已成为处理图结构数据的有力架构，但仍受限于刚性设计且缺乏可量化可解释性。当前最先进的GT在所有层中固定使用相同的GNN类型，错过了深度特定组件选择的优势，同时其复杂架构变得不透明，无法区分性能提升中的有意义模式和虚假相关性。我们通过不对称性重新设计GT注意力，解耦结构编码与特征表示：查询来自节点特征，而键和值来自GNN变换。在此框架内，我们使用可微分架构搜索(DARTS)在每层选择最优GNN算子，在transformer注意力内部实现深度异质性(DARTS-GT)。为了理解发现的架构，我们通过因果消融开发了首个GT的定量可解释性框架。我们的指标(Head-deviation、Specialization和Focus)识别出哪些头和节点驱动预测，同时实现模型比较。在八个基准数据集上的实验显示，DARTS-GT在四个数据集上达到最先进水平，在其他数据集上保持竞争力，且发现的架构揭示了数据集特定模式。我们的可解释性分析表明，可视化注意力和因果重要性并不总是相关，表明广泛使用的可视化方法可能忽略实际重要的组件。重要的是，DARTS-GT发现的异构架构始终比基线产生更可解释的模型，证明图变换器无需在性能和可解释性之间做出选择。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) have emerged as powerful architectures forgraph-structured data, yet remain constrained by rigid designs and lackquantifiable interpretability. Current state-of-the-art GTs commit to fixed GNNtypes across all layers, missing potential benefits of depth-specific componentselection, while their complex architectures become opaque where performancegains cannot be distinguished between meaningful patterns and spuriouscorrelations. We redesign GT attention through asymmetry, decoupling structuralencoding from feature representation: queries derive from node features whilekeys and values come from GNN transformations. Within this framework, we useDifferentiable ARchiTecture Search (DARTS) to select optimal GNN operators ateach layer, enabling depth-wise heterogeneity inside transformer attentionitself (DARTS-GT). To understand discovered architectures, we develop the firstquantitative interpretability framework for GTs through causal ablation. Ourmetrics (Head-deviation, Specialization, and Focus), identify which heads andnodes drive predictions while enabling model comparison. Experiments acrosseight benchmarks show DARTS-GT achieves state-of-the-art on four datasets whileremaining competitive on others, with discovered architectures revealingdataset-specific patterns. Our interpretability analysis reveals that visualattention salience and causal importance do not always correlate, indicatingwidely used visualization approaches may miss components that actually matter.Crucially, heterogeneous architectures found by DARTS-GT consistently producedmore interpretable models than baselines, establishing that Graph Transformersneed not choose between performance and interpretability.</description>
      <author>example@mail.com (Shruti Sarika Chakraborty, Peter Minary)</author>
      <guid isPermaLink="false">2510.14336v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Computing Communications for Multi-User Virtual Reality in Distributed Mobile Edge Computing Network</title>
      <link>http://arxiv.org/abs/2510.14243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submited to IEEE journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为空间计算通信(SCC)的框架，用于解决多用户沉浸式VR应用在分布式MEC网络中的延迟和能源效率问题。通过MO-CMPO算法，结合监督学习和强化学习，实现了帕累托最优的资源部署方案。&lt;h4&gt;背景&lt;/h4&gt;沉浸式VR应用对延迟、能源效率和计算资源有严格要求，特别是在多用户交互场景中。现有的分布式移动边缘计算(MEC)网络难以满足这些需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架来满足多用户VR在分布式MEC网络上的延迟和能源需求，并实现资源的高效部署。&lt;h4&gt;方法&lt;/h4&gt;提出空间计算通信(SCC)框架，将资源部署任务表述为多目标组合优化问题，并设计MO-CMPO算法，结合监督学习和强化学习，利用稀疏图神经网络生成帕累托最优解。&lt;h4&gt;主要发现&lt;/h4&gt;MO-CMPO比基线方法实现了更好的超体积性能和显著更低的推理延迟。以延迟为导向的解决方案倾向于本地MEC执行，而以能源为导向的解决方案则最小化冗余部署。&lt;h4&gt;结论&lt;/h4&gt;SCC框架和MO-CMPO算法能够有效解决多用户VR应用在分布式MEC网络中的资源部署问题，平衡延迟和能源消耗。&lt;h4&gt;翻译&lt;/h4&gt;沉浸式虚拟现实(VR)应用对延迟、能源效率和计算资源有严格要求，特别是在多用户交互场景中。为应对这些挑战，我们引入了空间计算通信(SCC)的概念，这是一个旨在满足分布式移动边缘计算(MEC)网络上多用户VR延迟和能源需求的框架。SCC使用用户动态和资源需求的概率模型，联合表示由用户和基站定义的物理空间，以及代表共享沉浸式环境的虚拟空间。然后，资源部署任务被表述为多目标组合优化(MOCO)问题，同时最小化分布式MEC资源上的系统延迟和能源消耗。为解决这个问题，我们提出了MO-CMPO，这是一种基于策略优化的多目标一致性模型，集成了监督学习和由偏好权重引导的强化学习(RL)微调。利用稀疏图神经网络(GNN)，MO-CMPO有效生成帕累托最优解。使用真实的新无线电基站数据集进行的模拟表明，MO-CMPO比基线方法实现了更好的超体积性能和显著更低的推理延迟。此外，分析揭示了实际的部署模式：以延迟为导向的解决方案倾向于本地MEC执行以减少传输延迟，而以能源为导向的解决方案则最小化冗余部署以节省能源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Immersive virtual reality (VR) applications impose stringent requirements onlatency, energy efficiency, and computational resources, particularly inmulti-user interactive scenarios. To address these challenges, we introduce theconcept of spatial computing communications (SCC), a framework designed to meetthe latency and energy demands of multi-user VR over distributed mobile edgecomputing (MEC) networks. SCC jointly represents the physical space, defined byusers and base stations, and the virtual space, representing shared immersiveenvironments, using a probabilistic model of user dynamics and resourcerequirements. The resource deployment task is then formulated as amulti-objective combinatorial optimization (MOCO) problem that simultaneouslyminimizes system latency and energy consumption across distributed MECresources. To solve this problem, we propose MO-CMPO, a multi-objectiveconsistency model with policy optimization that integrates supervised learningand reinforcement learning (RL) fine-tuning guided by preference weights.Leveraging a sparse graph neural network (GNN), MO-CMPO efficiently generatesPareto-optimal solutions. Simulations with real-world New Radio base stationdatasets demonstrate that MO-CMPO achieves superior hypervolume performance andsignificantly lower inference latency than baseline methods. Furthermore, theanalysis reveals practical deployment patterns: latency-oriented solutionsfavor local MEC execution to reduce transmission delay, while energy-orientedsolutions minimize redundant placements to save energy.</description>
      <author>example@mail.com (Caolu Xu, Zhiyong Chen, Meixia Tao, Li Song, Wenjun Zhang)</author>
      <guid isPermaLink="false">2510.14243v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Learning Wireless Interference Patterns: Decoupled GNN for Throughput Prediction in Heterogeneous Multi-Hop p-CSMA Networks</title>
      <link>http://arxiv.org/abs/2510.14137v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了解耦图卷积网络(D-GCN)来解决异构多跳无线网络中吞吐量预测的挑战。D-GCN通过分离节点自身传输概率与邻居干扰效应，使用可学习注意力替代平均聚合，实现了更准确的预测和可解释性，实验表明其显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;p持续CSMA协议是随机接入MAC分析的核心，但在异构多跳无线网络中预测饱和吞吐量仍是一个难题。简化的单一共享干扰域模型会低估吞吐量48-62%，而精确的马尔可夫链分析计算复杂度高，对大型网络不实用。&lt;h4&gt;目的&lt;/h4&gt;开发可扩展的吞吐量预测方法，解决异构多跳无线网络中的计算障碍，适用于一般网络拓扑的结构化机器学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出解耦图卷积网络(D-GCN)，一种新型架构，明确分离节点自身的传输概率与邻居干扰效应的处理。用可学习的注意力替代平均聚合，产生可解释的每邻居贡献权重，同时捕获复杂的多跳干扰模式。&lt;h4&gt;主要发现&lt;/h4&gt;D-GCN实现了3.3%的归一化平均绝对误差(NMAE)，显著优于标准GCN的63.94% NMAE。D-GCN性能优于强基线方法，即使在精确分析方法计算上不可行的情况下仍然可扩展，且使基于梯度的网络优化达到理论最优值的1%以内。&lt;h4&gt;结论&lt;/h4&gt;D-GCN通过解耦处理和注意力机制，能够更准确地捕获网络中的复杂干扰模式，有效解决了异构多跳无线网络吞吐量预测问题。&lt;h4&gt;翻译&lt;/h4&gt;p持续CSMA协议、饱和吞吐量、异构多跳无线网络、干扰域、马尔可夫链分析、结构化机器学习、图卷积网络(GNNs)、图卷积网络(GCN)、归一化平均绝对误差(NMAE)、对称归一化、级联效应、解耦图卷积网络(D-GCN)、可学习注意力、多跳干扰模式、基于梯度的网络优化&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The p-persistent CSMA protocol is central to random-access MAC analysis, butpredicting saturation throughput in heterogeneous multi-hop wireless networksremains a hard problem. Simplified models that assume a single, sharedinterference domain can underestimate throughput by 48--62\% in sparsetopologies. Exact Markov-chain analyses are accurate but scale exponentially incomputation time, making them impractical for large networks. Thesecomputational barriers motivate structural machine learning approaches likeGNNs for scalable throughput prediction in general network topologies. Yetoff-the-shelf GNNs struggle here: a standard GCN yields 63.94\% normalized meanabsolute error (NMAE) on heterogeneous networks because symmetric normalizationconflates a node's direct interference with higher-order, cascading effectsthat pertain to how interference propagates over the network graph.  Building on these insights, we propose the Decoupled Graph ConvolutionalNetwork (D-GCN), a novel architecture that explicitly separates processing of anode's own transmission probability from neighbor interference effects. D-GCNreplaces mean aggregation with learnable attention, yielding interpretable,per-neighbor contribution weights while capturing complex multihop interferencepatterns. D-GCN attains 3.3\% NMAE, outperforms strong baselines, remainstractable even when exact analytical methods become computationally infeasible,and enables gradient-based network optimization that achieves within 1\% oftheoretical optima.</description>
      <author>example@mail.com (Faezeh Dehghan Tarzjani, Bhaskar Krishnamachari)</author>
      <guid isPermaLink="false">2510.14137v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>On the expressivity of sparse maxout networks</title>
      <link>http://arxiv.org/abs/2510.14068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了稀疏maxout网络的表达能力，建立了这类网络与虚拟多面体之间的对偶关系，分析了网络深度和宽度对表达能力的影响。&lt;h4&gt;背景&lt;/h4&gt;研究聚焦于稀疏maxout网络，其中每个神经元从前一层接收固定数量的输入并采用maxout激活函数，这种结构类似于卷积神经网络或图神经网络的关键特征。&lt;h4&gt;目的&lt;/h4&gt;目的是理解稀疏maxout网络的表达能力，特别是网络深度、宽度和稀疏性如何影响其计算能力。&lt;h4&gt;方法&lt;/h4&gt;通过建立稀疏maxout网络可计算函数与虚拟多面体之间的对偶关系，推导出相关多面体维度的紧界，并基于此构建深度层次结构序列。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现足够深的稀疏maxout网络具有通用性，但如果未达到所需深度，仅靠宽度无法弥补固定入度约束的稀疏性。&lt;h4&gt;结论&lt;/h4&gt;稀疏maxout网络的表达能力不仅取决于宽度，还与深度密切相关，深度不足时宽度无法完全补偿稀疏性的限制。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了稀疏maxout网络的表达能力，其中每个神经元从前一层接收固定数量的输入，并采用可能有多参数的maxout激活函数。这种设置捕捉了卷积神经网络或图神经网络的关键特征。我们建立了此类网络可计算的函数与一类虚拟多面体之间的对偶关系，将它们的几何形状与网络表达能力的问题联系起来。特别是，我们推导出相关多面体维度的紧界，作为我们分析的中心工具。在此基础上，我们构建了一个深度层次结构序列。虽然足够深的稀疏maxout网络是通用的，但我们证明，如果未达到所需深度，仅靠宽度无法弥补固定入度约束的稀疏性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the expressivity of sparse maxout networks, where each neuron takesa fixed number of inputs from the previous layer and employs a, possiblymulti-argument, maxout activation. This setting captures key characteristics ofconvolutional or graph neural networks. We establish a duality betweenfunctions computable by such networks and a class of virtual polytopes, linkingtheir geometry to questions of network expressivity. In particular, we derive atight bound on the dimension of the associated polytopes, which serves as thecentral tool for our analysis. Building on this, we construct a sequence ofdepth hierarchies. While sufficiently deep sparse maxout networks areuniversal, we prove that if the required depth is not reached, width alonecannot compensate for the sparsity of a fixed indegree constraint.</description>
      <author>example@mail.com (Moritz Grillo, Tobias Hofmann)</author>
      <guid isPermaLink="false">2510.14068v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations</title>
      <link>http://arxiv.org/abs/2510.14035v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages content. 2 pages references&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GammaZero是一种以动作为中心的图表示框架，用于在部分可观察马尔可夫决策过程(POMDPs)中指导规划学习，解决了现有方法在可扩展性和泛化能力方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;现有方法需要特定领域的神经网络架构，并且难以处理大规模问题，限制了在POMDPs中的规划学习能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一的图表示框架，使学习到的策略能够在不同规模的问题间泛化，并减少对领域特定架构的需求。&lt;h4&gt;方法&lt;/h4&gt;GammaZero将信念状态转换为以动作为中心的图，使用图神经网络结合解码器架构从专家演示中学习价值函数和策略，然后应用这些启发式指导蒙特卡洛树搜索。&lt;h4&gt;主要发现&lt;/h4&gt;在相同规模问题上，GammaZero性能与BetaZero相当；同时能够实现零样本泛化，处理比训练时所见大2-4倍的问题，并在减少搜索需求的同时保持解决方案质量。&lt;h4&gt;结论&lt;/h4&gt;GammaZero通过统一的图表示框架有效解决了POMDPs中的规划学习问题，实现了更好的泛化能力和可扩展性，为处理大规模部分可观察环境提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种以动作为中心的图表示框架，用于学习在部分可观察马尔可夫决策过程(POMDPs)中指导规划。与需要特定领域神经网络架构且难以扩展的现有方法不同，GammaZero利用统一的基于图的信念表示，使问题能够在领域内跨规模泛化。我们的关键见解是信念状态可以系统地转换为以动作为中心的图，其中在小问题上学习的结构模式可以转移到更大的实例上。我们采用具有解码器架构的图神经网络，从计算可行问题上的专家演示中学习价值函数和策略，然后将这些学习到的启发式应用于指导更大问题上的蒙特卡洛树搜索。在标准POMDP基准测试上的实验结果表明，当在相同规模问题上训练和测试时，GammaZero与BetaZero相当，同时能够独特地实现零样本泛化到比训练时所见大2-4倍的问题，在减少搜索需求的同时保持解决方案质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce an action-centric graph representation framework for learning toguide planning in Partially Observable Markov Decision Processes (POMDPs).Unlike existing approaches that require domain-specific neural architecturesand struggle with scalability, GammaZero leverages a unified graph-based beliefrepresentation that enables generalization across problem sizes within adomain. Our key insight is that belief states can be systematically transformedinto action-centric graphs where structural patterns learned on small problemstransfer to larger instances. We employ a graph neural network with a decoderarchitecture to learn value functions and policies from expert demonstrationson computationally tractable problems, then apply these learned heuristics toguide Monte Carlo tree search on larger problems. Experimental results onstandard POMDP benchmarks demonstrate that GammaZero achieves comparableperformance to BetaZero when trained and tested on the same-sized problems,while uniquely enabling zero-shot generalization to problems 2-4 times largerthan those seen during training, maintaining solution quality with reducedsearch requirements.</description>
      <author>example@mail.com (Rajesh Mangannavar, Prasad Tadepalli)</author>
      <guid isPermaLink="false">2510.14035v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters</title>
      <link>http://arxiv.org/abs/2510.14250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为PhysAttnNet的新型物理先验引导双流注意力网络，通过引入衰减双向自注意力和相位差引导的双向交叉注意力模块，有效解决了传统深度学习模型在预测弹性Bragg防波堤运动响应时面临的泛化能力有限问题。实验证明该模型在波浪槽数据集上表现优异，且对未见环境具有良好的适应性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;准确预测弹性Bragg防波堤的运动响应对于其在海洋环境中的结构安全和运行完整性至关重要。然而，传统的深度学习模型在面对未见过的海况时往往表现出有限的泛化能力。这些缺陷源于忽视了海洋系统中自然衰减现象，以及对波-结构相互作用的不充分建模。&lt;h4&gt;目的&lt;/h4&gt;克服传统深度学习模型在预测弹性Bragg防波堤运动响应时面临的泛化能力有限问题，开发一种能够更好处理未见海况的预测模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为PhysAttnNet的物理先验引导双流注意力网络，包含三个关键模块：1)衰减双向自注意力(DBSA)模块，通过可学习的时间衰减模拟自然衰减现象；2)相位差引导的双向交叉注意力(PDG-BCA)模块，明确捕获波与结构之间的双向相互作用和相位关系；3)全局上下文融合(GCF)模块，协同整合两个流。模型使用混合时频损失函数进行训练，同时最小化时域预测误差和频域频谱差异。&lt;h4&gt;主要发现&lt;/h4&gt;在波浪槽数据集上的综合实验表明，PhysAttnNet显著优于主流模型。此外，跨场景泛化测试验证了模型对未见环境的鲁棒性和适应性。&lt;h4&gt;结论&lt;/h4&gt;PhysAttnNet有潜力作为开发海洋工程复杂系统预测模型的框架，能够有效解决传统深度学习模型在海洋环境预测中面临的泛化能力有限问题。&lt;h4&gt;翻译&lt;/h4&gt;准确预测弹性Bragg防波堤的运动响应对于其在海洋环境中的结构安全和运行完整性至关重要。然而，传统的深度学习模型在面对未见过的海况时往往表现出有限的泛化能力。这些缺陷源于忽视了海洋系统中自然衰减现象，以及对波-结构相互作用的不充分建模。为克服这些挑战，本研究提出了一种新颖的物理先验引导双流注意力网络(PhysAttnNet)。首先，衰减双向自注意力(DBSA)模块纳入了可学习的时间衰减，为最近的状态分配更高的权重，旨在模拟自然衰减现象。同时，相位差引导的双向交叉注意力(PDG-BCA)模块使用基于余弦的偏差在双向交叉计算范式中明确捕获波与结构之间的双向相互作用和相位关系。这些流通过全局上下文融合(GCF)模块协同整合。最后，PhysAttnNet使用混合时频损失进行训练，该损失函数同时最小化时域预测误差和频域频谱差异。在波浪槽数据集上的综合实验表明，PhysAttnNet显著优于主流模型。此外，跨场景泛化测试验证了模型对未见环境的鲁棒性和适应性，突显了其作为开发海洋工程复杂系统预测模型的框架的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate motion response prediction for elastic Bragg breakwaters is criticalfor their structural safety and operational integrity in marine environments.However, conventional deep learning models often exhibit limited generalizationcapabilities when presented with unseen sea states. These deficiencies stemfrom the neglect of natural decay observed in marine systems and inadequatemodeling of wave-structure interaction (WSI). To overcome these challenges,this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network(PhysAttnNet). First, the decay bidirectional self-attention (DBSA) moduleincorporates a learnable temporal decay to assign higher weights to recentstates, aiming to emulate the natural decay phenomenon. Meanwhile, the phasedifferences guided bidirectional cross-attention (PDG-BCA) module explicitlycaptures the bidirectional interaction and phase relationship between waves andthe structure using a cosine-based bias within a bidirectionalcross-computation paradigm. These streams are synergistically integratedthrough a global context fusion (GCF) module. Finally, PhysAttnNet is trainedwith a hybrid time-frequency loss that jointly minimizes time-domain predictionerrors and frequency-domain spectral discrepancies. Comprehensive experimentson wave flume datasets demonstrate that PhysAttnNet significantly outperformsmainstream models. Furthermore,cross-scenario generalization tests validate themodel's robustness and adaptability to unseen environments, highlighting itspotential as a framework to develop predictive models for complex systems inocean engineering.</description>
      <author>example@mail.com (Lianzi Jiang, Jianxin Zhang, Xinyu Han, Huanhe Dong, Xiangrong Wang)</author>
      <guid isPermaLink="false">2510.14250v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans</title>
      <link>http://arxiv.org/abs/2510.14205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为动态人格完善框架(DPRF)的新方法，用于优化大型语言模型角色扮演代理的行为与目标个体行为的一致性，通过迭代识别认知差异并完善人格配置文件，显著提高了行为对齐度。&lt;h4&gt;背景&lt;/h4&gt;新兴的大型语言模型角色扮演代理旨在模拟个体人类行为，但其人格保真度常因手动创建的配置文件(例如，精心挑选的信息和人格特征)而受损，这些配置文件未经验证是否与目标个体保持一致。&lt;h4&gt;目的&lt;/h4&gt;解决LLM RPAs行为与目标个体行为不一致的问题，通过优化LLM RPAs的行为与目标个体行为的对齐度。&lt;h4&gt;方法&lt;/h4&gt;提出动态人格完善框架(DPRF)，通过迭代识别生成行为与人类真实行为之间的认知差异(无论是自由形式还是基于理论的结构化分析)，并完善人格配置文件以减轻这些差异。在四个多样化的行为预测场景(正式辩论、心理健康问题的社交媒体帖子、公开采访和电影评论)中使用五个大型语言模型评估DPRF。&lt;h4&gt;主要发现&lt;/h4&gt;DPRF能够一致地显著提高基线人格的行为一致性，并且在模型和场景方面具有通用性。&lt;h4&gt;结论&lt;/h4&gt;该研究为创建高保真度人格配置文件和增强下游应用(如用户模拟、社会研究和个性化AI)的有效性提供了稳健的方法论。&lt;h4&gt;翻译&lt;/h4&gt;新兴的大型语言模型角色扮演代理旨在模拟个体人类行为，但其人格保真度常因手动创建的配置文件(例如，精心挑选的信息和人格特征)而受损，这些配置文件未经验证是否与目标个体保持一致。为解决这一限制，我们的工作引入了动态人格完善框架(DPRF)。DPRF旨在通过迭代识别生成行为与人类真实行为之间的认知差异(无论是自由形式还是基于理论的结构化分析)，并完善人格配置文件以减轻这些差异，从而优化LLM RPAs的行为与目标个体行为的一致性。我们在四个多样化的行为预测场景中使用五个大型语言模型评估DPRF：正式辩论、心理健康问题的社交媒体帖子、公开采访和电影评论。DPRF能够一致地显著提高基线人格的行为一致性，并且在模型和场景方面具有通用性。我们的研究为创建高保真度人格配置文件和增强下游应用(如用户模拟、社会研究和个性化AI)的有效性提供了稳健的方法论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emerging large language model role-playing agents (LLM RPAs) aim tosimulate individual human behaviors, but the persona fidelity is oftenundermined by manually-created profiles (e.g., cherry-picked information andpersonality characteristics) without validating the alignment with the targetindividuals. To address this limitation, our work introduces the DynamicPersona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLMRPAs' behaviors with those of target individuals by iteratively identifying thecognitive divergence, either through free-form or theory-grounded, structuredanalysis, between generated behaviors and human ground truth, and refining thepersona profile to mitigate these divergences.We evaluate DPRF with five LLMson four diverse behavior-prediction scenarios: formal debates, social mediaposts with mental health issues, public interviews, and movie reviews.DPRF canconsistently improve behavioral alignment considerably over baseline personasand generalizes across models and scenarios.Our work provides a robustmethodology for creating high-fidelity persona profiles and enhancing thevalidity of downstream applications, such as user simulation, social studies,and personalized AI.</description>
      <author>example@mail.com (Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang)</author>
      <guid isPermaLink="false">2510.14205v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Energy-Guided Diffusion Sampling for Long-Term User Behavior Prediction in Reinforcement Learning-based Recommendation</title>
      <link>http://arxiv.org/abs/2510.12815v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CIKM'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基于强化学习的推荐系统(RL4RS)在离线设置下面临数据效率低和依赖预收集轨迹的挑战。本文提出了一种名为DAC4Rec的新框架，整合扩散过程与强化学习，有效解决噪声数据处理和长期用户偏好捕捉问题。&lt;h4&gt;背景&lt;/h4&gt;基于强化学习的推荐系统能够适应用户的动态偏好，但在离线设置下面临数据效率低和依赖预收集轨迹的挑战。离线强化学习方法利用大量数据解决这些问题，但往往难以处理嘈杂数据且无法捕捉长期用户偏好。&lt;h4&gt;目的&lt;/h4&gt;克服现有离线强化学习推荐系统的局限性，提出一种新的框架来更有效地建模复杂的用户偏好。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Diffusion-enhanced Actor-Critic for Offline RL4RS (DAC4Rec)的新框架，该框架整合了扩散过程与强化学习。DAC4Rec利用扩散模型的去噪能力增强离线强化学习算法的鲁棒性，并采用Q值引导的策略优化策略来更好地处理次优轨迹。此外，还引入了一种基于能量的采样策略来减少推荐生成过程中的随机性。&lt;h4&gt;主要发现&lt;/h4&gt;通过在六个真实世界离线数据集和在线模拟环境中的大量实验验证了DAC4Rec的有效性，证明其能够优化长期用户偏好。此外，提出的扩散策略可以无缝集成到RL4RS中其他常用的强化学习算法中，展示了其多功能性和广泛的适用性。&lt;h4&gt;结论&lt;/h4&gt;DAC4Rec框架通过整合扩散过程与强化学习，有效解决了离线强化学习推荐系统中的数据效率、噪声处理和长期偏好捕捉等问题。&lt;h4&gt;翻译&lt;/h4&gt;基于强化学习的推荐系统(RL4RS)因其能够适应动态用户偏好而受到关注。然而，这些系统面临挑战，特别是在离线设置中，数据效率低下和对预收集轨迹的依赖限制了它们的广泛应用。虽然离线强化学习方法利用大量数据来解决这些问题，但它们通常难以处理嘈杂数据且无法捕捉长期用户偏好，导致次优的推荐策略。为了克服这些局限性，我们提出了用于离线RL4RS的扩散增强型Actor-Critic(DAC4Rec)，这是一个将扩散过程与强化学习相结合的新颖框架，能够更有效地建模复杂的用户偏好。DAC4Rec利用扩散模型的去噪能力增强离线强化学习算法的鲁棒性，并采用Q值引导的策略优化策略来更好地处理次优轨迹。此外，我们引入了一种基于能量的采样策略来减少推荐生成过程中的随机性，确保更有针对性和可靠的结果。我们在六个真实世界的离线数据集和在线模拟环境中通过大量实验验证了DAC4Rec的有效性，证明了其优化长期用户偏好的能力。此外，我们表明所提出的扩散策略可以无缝集成到RL4RS中其他常用的强化学习算法中，突显了其多功能性和广泛的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning-based recommender systems (RL4RS) have gainedattention for their ability to adapt to dynamic user preferences. However,these systems face challenges, particularly in offline settings, where datainefficiency and reliance on pre-collected trajectories limit their broaderapplicability. While offline reinforcement learning methods leverage extensivedatasets to address these issues, they often struggle with noisy data and failto capture long-term user preferences, resulting in suboptimal recommendationpolicies. To overcome these limitations, we propose Diffusion-enhancedActor-Critic for Offline RL4RS (DAC4Rec), a novel framework that integratesdiffusion processes with reinforcement learning to model complex userpreferences more effectively. DAC4Rec leverages the denoising capabilities ofdiffusion models to enhance the robustness of offline RL algorithms andincorporates a Q-value-guided policy optimization strategy to better handlesuboptimal trajectories. Additionally, we introduce an energy-based samplingstrategy to reduce randomness during recommendation generation, ensuring moretargeted and reliable outcomes. We validate the effectiveness of DAC4Recthrough extensive experiments on six real-world offline datasets and in anonline simulation environment, demonstrating its ability to optimize long-termuser preferences. Furthermore, we show that the proposed diffusion policy canbe seamlessly integrated into other commonly used RL algorithms in RL4RS,highlighting its versatility and wide applicability.</description>
      <author>example@mail.com (Xiaocong Chen, Siyu Wang, Lina Yao)</author>
      <guid isPermaLink="false">2510.12815v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>UrbanTwin: Synthetic LiDAR Datasets (LUMPI, V2X-Real-IC, and TUMTraf-I)</title>
      <link>http://arxiv.org/abs/2509.06781v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了UrbanTwin数据集，这是三个公共路边激光雷达数据集的高保真真实副本，每个包含10K个注释帧，具有丰富的标注信息，能够有效支持深度学习模型训练。&lt;h4&gt;背景&lt;/h4&gt;激光雷达感知任务需要大量高质量数据集进行模型训练，但真实数据集获取和标注成本高，且场景多样性有限。&lt;h4&gt;目的&lt;/h4&gt;创建高保真合成数据集，能够独立使用或增强现有数据集，用于激光雷达感知任务，并探索其能否替代同领域真实世界数据集。&lt;h4&gt;方法&lt;/h4&gt;基于实际位置的几何特征、道路对齐和交通模式构建数字孪生环境，使用模拟激光雷达传感器合成数据，添加3D边界框、实例分割和语义分割等标注，并通过统计和结构相似性分析评估数据质量。&lt;h4&gt;主要发现&lt;/h4&gt;合成数据集与真实数据高度相似，仅使用合成数据训练的模型在真实未见数据上表现优于使用真实数据训练的模型，数据集通过增加样本量和场景多样性有效增强了基准数据集。&lt;h4&gt;结论&lt;/h4&gt;UrbanTwin数据集是首批能够替换同领域真实世界数据集的数字合成数据集，提供了高保真数据副本，支持自定义场景测试，已公开可供使用。&lt;h4&gt;翻译&lt;/h4&gt;这篇文章介绍了UrbanTwin数据集，这是三个公共路边激光雷达数据集的高保真真实副本。每个UrbanTwin数据集包含10K个注释帧，对应一个公共数据集。注释包括6个类别的3D边界框、实例分割标签和跟踪ID，以及9个类别的语义分割标签。这些数据集使用模拟激光雷达传感器在真实数字孪生中合成，基于实际位置的周围几何形状、车道级别的道路对齐以及交叉口的车道拓扑和车辆移动模式进行建模。由于精确的数字孪生建模，合成数据集与真实数据集很好地对齐，为训练深度学习模型提供了强大的独立和增强价值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决高质量激光雷达数据集创建困难的问题。真实世界数据收集和标注成本高、耗时长，限制了智能交通系统感知算法的发展。这个问题很重要，因为激光雷达是智能交通系统中的关键技术，而高质量数据集对于训练和评估3D感知算法至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有模拟环境虽然功能强大，但与真实世界存在差距。他们借鉴了数字孪生概念，结合了CARLA模拟器和现有路边激光雷达数据集的特点。作者强调需要同时建模静态元素(如几何结构)和动态行为(如交通模式)，而非仅依赖手工制作的3D资产和简化的物理假设。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用数字孪生技术创建真实世界场景的高保真虚拟副本，模拟激光雷达传感器生成与真实数据相似的点云，并提供丰富一致的标注。实现流程包括：1)使用卫星图像和真实位置数据构建环境；2)配置虚拟传感器匹配真实规格；3)随机生成符合交通规则的动态元素；4)在CARLA模拟器中生成10K帧带标注的合成数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次专门为路边激光雷达应用创建合成数据集；采用高保真数字孪生同时整合静态和动态元素；合成数据与真实数据高度相似；证明完全在模拟数据上训练的模型可匹敌真实数据训练效果。相比之前工作，UrbanTwin专门增强真实世界基准而非通用模拟，在模拟过程中而非事后缩小sim-to-real差距，提供完整标注支持多种感知任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UrbanTwin通过创建基于数字孪生的高保真合成激光雷达数据集，成功解决了真实世界数据集创建成本高昂且sim-to-real差距大的问题，使模型能在合成数据上训练并有效应用于真实世界感知任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This article presents UrbanTwin datasets, high-fidelity, realistic replicasof three public roadside lidar datasets: LUMPI, V2X-Real-IC}}, and TUMTraf-I.Each UrbanTwin dataset contains 10K annotated frames corresponding to one ofthe public datasets. Annotations include 3D bounding boxes, instancesegmentation labels, and tracking IDs for six object classes, along withsemantic segmentation labels for nine classes. These datasets are synthesizedusing emulated lidar sensors within realistic digital twins, modeled based onsurrounding geometry, road alignment at lane level, and the lane topology andvehicle movement patterns at intersections of the actual locationscorresponding to each real dataset. Due to the precise digital twin modeling,the synthetic datasets are well aligned with their real counterparts, offeringstrong standalone and augmentative value for training deep learning models ontasks such as 3D object detection, tracking, and semantic and instancesegmentation. We evaluate the alignment of the synthetic replicas throughstatistical and structural similarity analysis with real data, and furtherdemonstrate their utility by training 3D object detection models solely onsynthetic data and testing them on real, unseen data. The high similarityscores and improved detection performance, compared to the models trained onreal data, indicate that the UrbanTwin datasets effectively enhance existingbenchmark datasets by increasing sample size and scene diversity. In addition,the digital twins can be adapted to test custom scenarios by modifying thedesign and dynamics of the simulations. To our knowledge, these are the firstdigitally synthesized datasets that can replace in-domain real-world datasetsfor lidar perception tasks. UrbanTwin datasets are publicly available athttps://dataverse.harvard.edu/dataverse/ucf-ut.</description>
      <author>example@mail.com (Muhammad Shahbaz, Shaurya Agarwal)</author>
      <guid isPermaLink="false">2509.06781v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Backdoor Unlearning by Linear Task Decomposition</title>
      <link>http://arxiv.org/abs/2510.14845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究解决了基础模型中后门攻击的安全问题，提出了一种基于后门与良性任务解耦特性的简单遗忘方法，能够在不损害模型通用能力的情况下有效移除后门。&lt;h4&gt;背景&lt;/h4&gt;基础模型通过在多样化任务中实现广泛的泛化能力彻底改变了计算机视觉领域。然而，它们仍然容易受到对抗性扰动和定向后门攻击的影响。缓解此类脆弱性仍然是一个开放的挑战，特别是考虑到模型的大规模性质使得重新训练以确保安全性变得不可行。&lt;h4&gt;目的&lt;/h4&gt;回答后门是否可以在不损害模型通用能力的情况下被移除这一问题，并研究后门如何在模型权重空间中被编码。&lt;h4&gt;方法&lt;/h4&gt;研究后门与良性任务在模型权重空间中的解耦特性，基于这种分离开发一种简单的遗忘方法，能够隔离和擦除后门对模型的影响，同时保持干净性能。通过基于CLIP的模型和常见对抗触发器进行大量实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;后门与其他良性任务是解耦的；给定攻击知识的情况下，方法实现了近乎完美的遗忘，同时平均保留了96%的干净准确率；即使当攻击及其存在未知时，方法也能通过反向工程触发器的适当估计成功遗忘后门；与当前最先进的防御相比，方法始终产生更好的遗忘和干净准确率权衡。&lt;h4&gt;结论&lt;/h4&gt;该方法在移除后门的同时，有效保留了模型的通用能力，为解决基础模型的安全问题提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;基础模型通过在多样化任务中实现广泛的泛化能力彻底改变了计算机视觉领域。然而，它们仍然容易受到对抗性扰动和定向后门攻击的影响。缓解此类脆弱性仍然是一个开放的挑战，特别是考虑到模型的大规模性质使得重新训练以确保安全性变得不可行。现有的后门移除方法依赖于昂贵的微调来覆盖有害行为，并且通常会降低在其他不相关任务上的性能。这引发了一个问题：后门是否可以在不损害模型通用能力的情况下被移除。在本研究中，我们解决了这个问题，并研究了后门如何在模型权重空间中被编码，发现它们与其他良性任务是解耦的。具体而言，这种分离使得能够隔离和擦除后门对模型的影响，同时对干净性能的影响最小。基于这一见解，我们引入了一种利用这种解耦特性的简单遗忘方法。通过对基于CLIP的模型和常见对抗触发器的大量实验，我们表明，给定攻击知识的情况下，我们的方法实现了近乎完美的遗忘，同时平均保留了96%的干净准确率。此外，我们证明即使当攻击及其存在未知时，我们的方法也能通过使用反向工程触发器的适当估计成功遗忘后门。总体而言，与当前最先进的防御相比，我们的方法始终产生更好的遗忘和干净准确率权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have revolutionized computer vision by enabling broadgeneralization across diverse tasks. Yet, they remain highly susceptible toadversarial perturbations and targeted backdoor attacks. Mitigating suchvulnerabilities remains an open challenge, especially given that thelarge-scale nature of the models prohibits retraining to ensure safety.Existing backdoor removal approaches rely on costly fine-tuning to override theharmful behavior, and can often degrade performance on other unrelated tasks.This raises the question of whether backdoors can be removed withoutcompromising the general capabilities of the models. In this work, we addressthis question and study how backdoors are encoded in the model weight space,finding that they are disentangled from other benign tasks. Specifically, thisseparation enables the isolation and erasure of the backdoor's influence on themodel with minimal impact on clean performance. Building on this insight, weintroduce a simple unlearning method that leverages such disentanglement.Through extensive experiments with CLIP-based models and common adversarialtriggers, we show that, given the knowledge of the attack, our method achievesapproximately perfect unlearning, while retaining, on average, 96% of cleanaccuracy. Additionally, we demonstrate that even when the attack and itspresence are unknown, our method successfully unlearns backdoors by properestimation using reverse-engineered triggers. Overall, our method consistentlyyields better unlearning and clean accuracy tradeoffs when compared to presentstate-of-the-art defenses.</description>
      <author>example@mail.com (Amel Abdelraheem, Alessandro Favero, Gerome Bovet, Pascal Frossard)</author>
      <guid isPermaLink="false">2510.14845v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&amp;E Whole Slide Images</title>
      <link>http://arxiv.org/abs/2510.14800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种名为PRISM的新型可解释AI模型，用于结直肠癌预后预测。该模型通过整合连续变异性谱的形态学信息，能够更准确地捕捉肿瘤的渐进式进化过程，并在III期结直肠癌患者中展现出优异的预后预测性能。&lt;h4&gt;背景&lt;/h4&gt;结直肠癌是全球第三大常见恶性肿瘤，预计2025年将有约154,000新病例和54,000例死亡。当前计算机病理学中的基础模型主要采用任务无关的方法学，可能忽略器官特定的关键形态学模式，而这些模式对肿瘤行为、治疗反应和患者结局有重要影响。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型、可解释的AI模型PRISM（预后性整合空间形态表征），纳入每种不同形态内的连续变异性谱以表征表型多样性，反映恶性肿瘤转化是通过渐进式进化过程而非表型急剧转变发生的原理。&lt;h4&gt;方法&lt;/h4&gt;PRISM模型在874万张组织学图像上进行训练，这些图像来自424名III期结直肠癌患者的手术切除标本。模型整合了空间形态学信息，以捕捉肿瘤的形态学变异性。&lt;h4&gt;主要发现&lt;/h4&gt;PRISM在五年总生存期(OS)预后方面表现优越：AUC = 0.70 ± 0.04；准确率 = 68.37% ± 4.75%；风险比(HR) = 3.34，95% CI = 2.28-4.90，p &lt; 0.0001。模型优于现有的结直肠癌特异性方法15%，比AI基础模型高约23%的准确率。PRISM显示性别无关的稳健性，在临床病理亚组中表现稳定，在不同治疗方案间的准确率波动最小（差值 = 1.44%），复现了Alliance队列的研究结果，即两种治疗方案之间无生存差异。&lt;h4&gt;结论&lt;/h4&gt;PRISM模型在结直肠癌预后预测方面表现优异，能够更好地捕捉肿瘤的形态学变异性，对不同治疗方案的患者预后有稳定的预测能力，为临床决策提供了有价值的工具。&lt;h4&gt;翻译&lt;/h4&gt;结直肠癌(CRC)仍然是全球第三大常见恶性肿瘤，预计2025年将有约154,000新病例和54,000例死亡。最近，计算病理学中基础模型的进展主要是由任务无关的方法学推动的，这些方法可能忽略器官特定的关键形态学模式，这些模式代表不同的生物学过程，能从根本上影响肿瘤行为、治疗反应和患者结局。本研究旨在开发一种新型、可解释的AI模型PRISM（预后性整合空间形态表征），该模型纳入了每种不同形态内的连续变异性谱，以表征表型多样性，并反映恶性肿瘤转化是通过渐进式进化过程而非表型急剧转变发生的原理。PRISM在从424名III期CRC患者的手术切除标本中提取的874万张组织学图像上进行训练。PRISM在五年OS预后方面取得了优异的性能（AUC = 0.70 ± 0.04；准确率 = 68.37% ± 4.75%；HR = 3.34，95% CI = 2.28-4.90；p &lt; 0.0001），比现有的CRC特异性方法高出15%，比AI基础模型高出约23%的准确率。它显示出性别无关的稳健性（AUC差值 = 0.02；准确率差值 = 0.15%），并在临床病理亚组中表现稳定，在5FU/LV和CPT-11/5FU/LV治疗方案之间的准确率波动最小（差值 = 1.44%），复现了Alliance队列的研究结果，即两种治疗方案之间无生存差异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Colorectal cancer (CRC) remains the third most prevalent malignancy globally,with approximately 154,000 new cases and 54,000 projected deaths anticipatedfor 2025. The recent advancement of foundation models in computationalpathology has been largely propelled by task agnostic methodologies that canoverlook organ-specific crucial morphological patterns that represent distinctbiological processes that can fundamentally influence tumor behavior,therapeutic response, and patient outcomes. The aim of this study is to developa novel, interpretable AI model, PRISM (Prognostic Representation of IntegratedSpatial Morphology), that incorporates a continuous variability spectrum withineach distinct morphology to characterize phenotypic diversity and reflectingthe principle that malignant transformation occurs through incrementalevolutionary processes rather than abrupt phenotypic shifts. PRISM is trainedon 8.74 million histological images extracted from surgical resection specimensof 424 patients with stage III CRC. PRISM achieved superior prognosticperformance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%;HR = 3.34, 95% CI = 2.28-4.90; p &lt; 0.0001), outperforming existing CRC-specificmethods by 15% and AI foundation models by ~23% accuracy. It showedsex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stableperformance across clinicopathological subgroups, with minimal accuracyfluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens,replicating the Alliance cohort finding of no survival difference betweentreatments.</description>
      <author>example@mail.com (Usama Sajjad, Abdul Rehman Akbar, Ziyu Su, Deborah Knight, Wendy L. Frankel, Metin N. Gurcan, Wei Chen, Muhammad Khalid Khan Niazi)</author>
      <guid isPermaLink="false">2510.14800v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes</title>
      <link>http://arxiv.org/abs/2510.14763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对大型语言模型在创意写作方面的局限性，特别是在非英语环境中的不足，提出了一个新颖的中文创意写作数据集COIG-Writer，并通过实验确定了创意写作的双组分模型及其关键发现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在创意写作方面存在系统性缺陷，特别是在非英语语境中，训练数据稀缺且缺乏过程层面的监督。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够捕捉多样化输出及其背后思维过程的中文创意写作数据集，并研究创意写作的构成要素和优化方法。&lt;h4&gt;方法&lt;/h4&gt;创建了COIG-Writer数据集，包含1665个精心挑选的三元组，涵盖51个体裁，每个三元组包含逆向工程提示、详细创意推理和最终文本。通过全面实验分析创意写作的构成要素和优化方法。&lt;h4&gt;主要发现&lt;/h4&gt;1. 过程监督非常有效，但需要通用数据稳定化，至少需要一个创意样本对应十二个通用样本才能实现最佳性能；2. 创意能力具有文化局限性，没有跨语言迁移能力，中文和英文表现之间有89.26百分点的差距；3. 词汇多样性与创意质量呈负相关（TTR悖论），高多样性信号表明对逻辑缺陷的补偿行为。&lt;h4&gt;结论&lt;/h4&gt;创意卓越来自于逻辑支架和语言基础的相互作用，类似于数学推理如何增强但不能替代基础模型中的语言能力。创意写作需要过程监督和通用数据的适当平衡。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在创意写作方面表现出系统性缺陷，特别是在非英语环境中，训练数据稀缺且缺乏过程层面的监督。我们提出了COIG-Writer，这是一个新颖的中文创意写作数据集，通过对高质量文本进行系统性的逆向工程，捕捉多样化的输出及其背后的思维过程。与仅提供输入-输出对的数据集不同，COIG-Writer包含1665个精心挑选的三元组，涵盖51个体裁，每个三元组包含：(1)逆向工程提示，(2)详细创意推理记录决策过程，(3)最终文本。通过全面实验，我们确定了创意写作的双组分模型：叙事逻辑（由过程监督提供）和语言表达（由通用数据维持）。我们的研究揭示了三个关键见解：(1)过程监督非常有效，但需要通用数据稳定化。至少需要一个创意样本对应十二个通用样本的比例才能实现最佳性能；低于此阈值，胜率会逐渐下降（从62.75%降至35.78%）；(2)创意能力具有文化局限性，没有跨语言迁移能力（中文和英文表现之间有89.26百分点的差距）；(3)词汇多样性与创意质量呈负相关（TTR悖论），表明高多样性信号表明对逻辑缺陷的补偿行为。这些发现表明，创意卓越来自于逻辑支架和语言基础的相互作用，类似于数学推理如何增强但不能替代基础模型中的语言能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models exhibit systematic deficiencies in creative writing,particularly in non-English contexts where training data is scarce and lacksprocess-level supervision. We present COIG-Writer, a novel Chinese creativewriting dataset that captures both diverse outputs and their underlying thoughtprocesses through systematic reverse-engineering of high-quality texts. Unlikeexisting datasets that provide only input-output pairs, COIG-Writer comprises1,665 meticulously curated triplets spanning 51 genres, each containing: (1) areverse-engineered prompt, (2) detailed creative reasoning documentingdecision-making processes, and (3) the final text. Through comprehensiveexperiments, we identify a two-component model of creative writing: narrativelogic (provided by process supervision) and linguistic expression (maintainedby general-purpose data). Our findings reveal three critical insights: (1)Process supervision is highly effective but requires stabilization with generaldata. A ratio of at least one creative sample to twelve general samples isneeded to achieve optimal performance; below this threshold, the win rateprogressively degrades (from 62.75% down to 35.78%)., (2) creative capabilitiesare culturally-bound with no cross-lingual transfer (89.26pp gap betweenChinese and English performance), and (3) lexical diversity inverselycorrelates with creative quality (TTR paradox), suggesting high diversitysignals compensatory behavior for logical deficiencies. These findingsestablish that creative excellence emerges from the interaction between logicalscaffolding and linguistic grounding, analogous to how mathematical reasoningenhances but cannot replace linguistic competence in foundation models.</description>
      <author>example@mail.com (Yunwen Li, Shuangshuang Ying, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Tianyu Zheng, Xeron Du, Qiguang Chen, Jiajun Shi, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Stephen Huang, Wanxiang Che, Chenghua Lin, Eli Zhang)</author>
      <guid isPermaLink="false">2510.14763v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>State-Space Models for Tabular Prior-Data Fitted Networks</title>
      <link>http://arxiv.org/abs/2510.14573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了使用Hydra（一种双向线性时间结构状态空间模型）替代TabPFN中的Transformer架构，以解决Transformer的二次复杂度问题，同时保持预测性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型在表格数据领域取得了进展，如TabPFN展示了预训练Transformer架构可以高预测性能近似贝叶斯推断。然而，Transformer在序列长度上具有二次复杂度，促使人们探索更高效的序列模型。&lt;h4&gt;目的&lt;/h4&gt;研究Hydra作为TabPFN中Transformer替代方案的潜力，解决SSM对输入标记顺序的固有敏感性这一关键挑战，特别是在表格数据集中行顺序语义无意义的情况下。&lt;h4&gt;方法&lt;/h4&gt;研究双向方法能在多大程度上保持效率并实现对称上下文聚合，以减少SSM对输入顺序的依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，这种方法减少了顺序依赖性，实现了与原始TabPFN模型相当的预测性能。&lt;h4&gt;结论&lt;/h4&gt;双向Hydra模型可以作为TabPFN中Transformer的有效替代方案，在保持预测性能的同时提高效率。&lt;h4&gt;翻译&lt;/h4&gt;最近在表格数据基础模型方面的进展，如TabPFN，表明预训练的Transformer架构可以以高预测性能近似贝叶斯推断。然而，Transformer在序列长度上具有二次复杂度，促使人们探索更高效的序列模型。在这项工作中，我们研究了使用Hydra（一种双向线性时间结构状态空间模型SSM）作为TabPFN中Transformer替代方案的潜力。一个关键挑战在于SSM对输入标记顺序的固有敏感性——对于行顺序在语义上无意义的表格数据集来说，这是一个不希望有的特性。我们研究了双向方法在多大程度上可以保持效率并实现对称上下文聚合。我们的实验表明，这种方法减少了顺序依赖性，实现了与原始TabPFN模型具有竞争力的预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in foundation models for tabular data, such as TabPFN,demonstrated that pretrained Transformer architectures can approximate Bayesianinference with high predictive performance. However, Transformers suffer fromquadratic complexity with respect to sequence length, motivating theexploration of more efficient sequence models. In this work, we investigate thepotential of using Hydra, a bidirectional linear-time structured state spacemodel (SSM), as an alternative to Transformers in TabPFN. A key challenge liesin SSM's inherent sensitivity to the order of input tokens - an undesirableproperty for tabular datasets where the row order is semantically meaningless.We investigate to what extent a bidirectional approach can preserve efficiencyand enable symmetric context aggregation. Our experiments show that thisapproach reduces the order-dependence, achieving predictive performancecompetitive to the original TabPFN model.</description>
      <author>example@mail.com (Felix Koch, Marcel Wever, Fabian Raisch, Benjamin Tischler)</author>
      <guid isPermaLink="false">2510.14573v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology</title>
      <link>http://arxiv.org/abs/2510.14532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DentVFM是首个专为牙科设计的视觉基础模型系列，解决了现有牙科AI系统的局限性，通过自监督学习和大规模多模态数据集训练，展现出卓越的泛化能力和跨模态诊断性能。&lt;h4&gt;背景&lt;/h4&gt;口腔颌面放射学在牙科医疗中至关重要，但受专业人才短缺限制。现有牙科AI系统因单一模态关注、任务特定设计和依赖标记数据而泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服现有AI系统局限性的牙科视觉基础模型，实现更广泛的应用和更好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;创建DentVFM模型系列，使用DentVista数据集(约160万多模态放射图像)进行自监督学习，基于Vision Transformer架构开发2D和3D变体，并建立DentBench基准测试涵盖8个牙科亚专科。&lt;h4&gt;主要发现&lt;/h4&gt;DentVFM表现出通用智能，能推广到多种牙科任务；显著优于各类基线模型；提供更好的泛化能力、标签效率和可扩展性；在跨模态诊断中表现优于经验丰富的牙医。&lt;h4&gt;结论&lt;/h4&gt;DentVFM为牙科AI树立新范式，提供可扩展、适应性强且标签高效的模型，有助于改善智能牙科医疗保健并解决全球口腔医疗保健差距。&lt;h4&gt;翻译&lt;/h4&gt;口腔颌面放射学在牙科医疗保健中起着重要作用，但放射图像解读受到训练专业人员短缺的限制。虽然AI方法显示出前景，但现有牙科AI系统受限于其单一模态关注、任务特定设计和依赖昂贵的标记数据，阻碍了它们在多样化临床场景中的泛化能力。为解决这些挑战，我们引入了DentVFM，这是首个为牙科设计的视觉基础模型系列。DentVFM为广泛的牙科应用生成任务无关的视觉表示，并在DentVista上使用自监督学习，这是一个精心策划的大型牙科成像数据集，包含来自不同医疗中心的约160万张多模态放射图像。DentVFM基于Vision Transformer架构包含2D和3D变体。为解决牙科智能评估和基准测试的空白，我们引入了DentBench，这是一个全面的基准测试，涵盖八个牙科亚专科、更多疾病、成像方式和广泛的地理分布。DentVFM表现出令人印象深刻的通用智能，展示了向多样化牙科任务的稳健泛化能力，如疾病诊断、治疗分析、生物标志物识别以及解剖标志物检测和分割。实验结果表明，DentVFM显著优于监督、自监督和弱监督基线，提供更好的泛化能力、标签效率和可扩展性。此外，DentVFM实现跨模态诊断，在常规成像不可用的情况下提供比经验丰富的牙医更可靠的结果。DentVFM为牙科AI树立了新范式，提供可扩展、适应性强且标签高效的模型，以改善智能牙科医疗保健并解决全球口腔医疗保健中的关键差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Oral and maxillofacial radiology plays a vital role in dental healthcare, butradiographic image interpretation is limited by a shortage of trainedprofessionals. While AI approaches have shown promise, existing dental AIsystems are restricted by their single-modality focus, task-specific design,and reliance on costly labeled data, hindering their generalization acrossdiverse clinical scenarios. To address these challenges, we introduce DentVFM,the first family of vision foundation models (VFMs) designed for dentistry.DentVFM generates task-agnostic visual representations for a wide range ofdental applications and uses self-supervised learning on DentVista, a largecurated dental imaging dataset with approximately 1.6 million multi-modalradiographic images from various medical centers. DentVFM includes 2D and 3Dvariants based on the Vision Transformer (ViT) architecture. To address gaps indental intelligence assessment and benchmarks, we introduce DentBench, acomprehensive benchmark covering eight dental subspecialties, more diseases,imaging modalities, and a wide geographical distribution. DentVFM showsimpressive generalist intelligence, demonstrating robust generalization todiverse dental tasks, such as disease diagnosis, treatment analysis, biomarkeridentification, and anatomical landmark detection and segmentation.Experimental results indicate DentVFM significantly outperforms supervised,self-supervised, and weakly supervised baselines, offering superiorgeneralization, label efficiency, and scalability. Additionally, DentVFMenables cross-modality diagnostics, providing more reliable results thanexperienced dentists in situations where conventional imaging is unavailable.DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, andlabel-efficient model to improve intelligent dental healthcare and addresscritical gaps in global oral healthcare.</description>
      <author>example@mail.com (Xinrui Huang, Fan Xiao, Dongming He, Anqi Gao, Dandan Li, Xiaofan Zhang, Shaoting Zhang, Xudong Wang)</author>
      <guid isPermaLink="false">2510.14532v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Vision Mamba for Permeability Prediction of Porous Media</title>
      <link>http://arxiv.org/abs/2510.14516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文首次引入使用Vision Mamba作为主干网络来预测三维多孔介质渗透率的神经网络，并证明了其相比ViTs和CNNs的优势。&lt;h4&gt;背景&lt;/h4&gt;Vision Mamba最近作为Vision Transformers(ViTs)的替代方案在图像分类领域受到关注。Vision Mamba的网络规模随输入图像分辨率线性增长，而ViTs则是二次增长，这使得Vision Mamba在计算和内存效率方面更具优势。此外，Vision Mamba比传统卷积神经网络(CNN)需要更少的可训练参数，因此内存效率更高。&lt;h4&gt;目的&lt;/h4&gt;首次引入使用Vision Mamba作为主干网络来预测三维多孔介质渗透率的神经网络，比较Vision Mamba与ViT和CNN模型在渗透率预测多个方面的性能，并进行消融研究以评估其组件对准确性的影响。&lt;h4&gt;方法&lt;/h4&gt;构建了一个使用Vision Mamba作为主干网络的神经网络来预测三维多孔介质的渗透率，并与ViT和CNN模型进行了性能比较，进行了消融研究评估组件对准确性的影响。&lt;h4&gt;主要发现&lt;/h4&gt;实践证明了Vision Mamba在三维多孔介质渗透率预测方面相比ViTs和CNNs具有计算效率高、内存占用少、参数量少等优势。&lt;h4&gt;结论&lt;/h4&gt;作者认为提出的框架有潜力集成到使用Vision Mamba替代ViTs的大型视觉模型中，并已公开源代码以促进可重复性并使其他研究人员能够在此基础上进行扩展。&lt;h4&gt;翻译&lt;/h4&gt;Vision Mamba最近作为Vision Transformers(ViTs)的替代方案在图像分类领域受到关注。Vision Mamba的网络规模随输入图像分辨率线性增长，而ViTs则是二次增长，这一特性提高了计算和内存效率。此外，Vision Mamba比传统卷积神经网络(CNN)需要少得多的可训练参数，因此可以更节省内存。由于这些特性，我们首次引入了一个使用Vision Mamba作为主干网络来预测三维多孔介质渗透率的神经网络。我们在渗透率预测的多个方面比较了Vision Mamba与ViT和CNN模型的性能，并进行了消融研究以评估其组件对准确性的影响。我们通过实践证明了Vision Mamba在三维多孔介质渗透率预测方面相比ViTs和CNNs具有上述优势。我们公开源代码以促进可重复性，并使其他研究人员能够在此基础上进行扩展和延伸。我们认为，在Vision Mamba替代ViTs的大型视觉模型中，所提出的框架具有集成潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Mamba has recently received attention as an alternative to VisionTransformers (ViTs) for image classification. The network size of Vision Mambascales linearly with input image resolution, whereas ViTs scale quadratically,a feature that improves computational and memory efficiency. Moreover, VisionMamba requires a significantly smaller number of trainable parameters thantraditional convolutional neural networks (CNNs), and thus, they can be morememory efficient. Because of these features, we introduce, for the first time,a neural network that uses Vision Mamba as its backbone for predicting thepermeability of three-dimensional porous media. We compare the performance ofVision Mamba with ViT and CNN models across multiple aspects of permeabilityprediction and perform an ablation study to assess the effects of itscomponents on accuracy. We demonstrate in practice the aforementionedadvantages of Vision Mamba over ViTs and CNNs in the permeability prediction ofthree-dimensional porous media. We make the source code publicly available tofacilitate reproducibility and to enable other researchers to build on andextend this work. We believe the proposed framework has the potential to beintegrated into large vision models in which Vision Mamba is used instead ofViTs.</description>
      <author>example@mail.com (Ali Kashefi, Tapan Mukerji)</author>
      <guid isPermaLink="false">2510.14516v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review</title>
      <link>http://arxiv.org/abs/2510.14462v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇PRISMA指导的范围综述综合了无监督深度生成模型在神经影像学中异常检测的最新研究进展，涵盖了2018-2025年间的49项研究，表明这些模型在大局灶性病变检测和微妙异常识别方面取得了显著进展。&lt;h4&gt;背景&lt;/h4&gt;无监督深度生成模型正在成为脑成像异常检测和分割的有前景方法，与需要大量体素级标注数据且仅限于已表征病理的完全监督方法不同，这些模型可以仅使用健康数据进行训练，并将异常识别为从学习到的正常脑结构中出现的偏差。&lt;h4&gt;目的&lt;/h4&gt;综合关于无监督深度生成模型在神经影像学中异常检测的最新工作，包括自编码器、变分自编码器、生成对抗网络和去噪扩散模型，并比较其性能指标和架构设计选择。&lt;h4&gt;方法&lt;/h4&gt;采用PRISMA指导的范围综述方法，系统检索并分析了2018-2025年间发表的49项研究，这些研究应用了各种生成模型于脑MRI和CT影像，用于检测肿瘤、中风、多发性硬化和小血管疾病等多种病理。&lt;h4&gt;主要发现&lt;/h4&gt;生成模型在大局灶性病变方面取得了令人鼓舞的性能，并在处理更微妙的异常方面取得了进展；其关键优势是能够产生可解释的伪健康重建，这在注释数据稀缺的情况下（如罕见或异质性疾病）特别有价值。&lt;h4&gt;结论&lt;/h4&gt;这些模型为异常检测提供了有吸引力的方向，能够实现半监督学习，支持新成像生物标志物的发现，并促进统一端到端框架内的疾病内和跨疾病偏差映射；未来工作应优先考虑解剖感知建模、基础模型开发、任务适当的评估指标和严格的临床验证。&lt;h4&gt;翻译&lt;/h4&gt;无监督深度生成模型正在成为脑成像异常检测和分割的替代性有前景方法，与需要大量体素级标注数据且仅限于已表征病理的完全监督方法不同，这些模型可以仅使用健康数据进行训练，并将异常识别为从学习到的正常脑结构中出现的偏差。这篇PRISMA指导的范围综述综合了无监督深度生成模型在神经影像学中异常检测的最新工作，包括自编码器、变分自编码器、生成对抗网络和去噪扩散模型。共确定了2018-2025年间发表的49项研究，涵盖了脑MRI和较少见的CT应用，应用于肿瘤、中风、多发性硬化和小血管疾病等多种病理。报告的性能指标与架构设计选择进行了比较。在纳入的研究中，生成模型在大局灶性病变方面取得了令人鼓舞的性能，并在处理更微妙的异常方面取得了进展。生成模型的一个关键优势是它们能够产生可解释的伪健康（也称为反事实）重建，这在注释数据稀缺时（如罕见或异质性疾病）特别有价值。展望未来，这些模型为异常检测提供了有吸引力的方向，能够实现半监督学习，支持新成像生物标志物的发现，并促进统一端到端框架内的疾病内和跨疾病偏差映射。为实现临床影响，未来工作应优先考虑解剖感知建模、基础模型开发、任务适当的评估指标和严格的临床验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised deep generative models are emerging as a promising alternativeto supervised methods for detecting and segmenting anomalies in brain imaging.Unlike fully supervised approaches, which require large voxel-level annotateddatasets and are limited to well-characterised pathologies, these models can betrained exclusively on healthy data and identify anomalies as deviations fromlearned normative brain structures. This PRISMA-guided scoping reviewsynthesises recent work on unsupervised deep generative models for anomalydetection in neuroimaging, including autoencoders, variational autoencoders,generative adversarial networks, and denoising diffusion models. A total of 49studies published between 2018 - 2025 were identified, covering applications tobrain MRI and, less frequently, CT across diverse pathologies such as tumours,stroke, multiple sclerosis, and small vessel disease. Reported performancemetrics are compared alongside architectural design choices. Across theincluded studies, generative models achieved encouraging performance for largefocal lesions and demonstrated progress in addressing more subtleabnormalities. A key strength of generative models is their ability to produceinterpretable pseudo-healthy (also referred to as counterfactual)reconstructions, which is particularly valuable when annotated data are scarce,as in rare or heterogeneous diseases. Looking ahead, these models offer acompelling direction for anomaly detection, enabling semi-supervised learning,supporting the discovery of novel imaging biomarkers, and facilitating within-and cross-disease deviation mapping in unified end-to-end frameworks. Torealise clinical impact, future work should prioritise anatomy-aware modelling,development of foundation models, task-appropriate evaluation metrics, andrigorous clinical validation.</description>
      <author>example@mail.com (Youwan Mahé, Elise Bannier, Stéphanie Leplaideur, Elisa Fromont, Francesca Galassi)</author>
      <guid isPermaLink="false">2510.14462v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Vision-Centric Activation and Coordination for Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2510.14349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了VaCo方法，通过视觉中心激活和多视觉基础模型的协调来优化多模态大语言模型(MLLMs)的表示，提高模型在视觉理解方面的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)通过整合视觉编码器的图像特征与LLMs，展现出先进的理解能力。然而，主流MLLMs仅通过文本标记的下一个标记预测进行监督，忽略了分析能力所需的关键视觉中心信息。&lt;h4&gt;目的&lt;/h4&gt;解决主流MLLMs忽视关键视觉中心信息的问题，通过引入视觉中心激活和协调机制，优化MLLMs的表示，提高其视觉理解能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出了VaCo方法，包括：视觉判别对齐整合从多个视觉基础模型(VFMs)中提取的任务感知特征；可学习的模块化任务查询(MTQs)在多种VFMs的监督下激活特定视觉信号；视觉对齐层(VALs)整合到MLLMs中；标记网关掩码(TGM)限制多组MTQs之间的信息流，协调VFMs之间的表示冲突。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，VaCo显著提高了不同MLLMs在各种基准测试上的性能，展示了其在视觉理解方面的卓越能力。&lt;h4&gt;结论&lt;/h4&gt;VaCo通过有效整合多种视觉基础模型的特征，解决了主流MLLMs忽视视觉中心信息的问题，显著提升了模型在视觉理解任务上的表现。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)整合视觉编码器中的图像特征与LLMs，展现出先进的理解能力。然而，主流MLLMs仅通过文本标记的下一个标记预测进行监督，忽略了分析能力所需的关键视觉中心信息。为了解决这一困境，我们引入了VaCo，它通过多个视觉基础模型(VFMs)的视觉中心激活和协调来优化MLLM表示。VaCo引入视觉判别对齐来整合从VFMs中提取的任务感知特征，从而统一MLMs中文本和视觉输出的优化。具体来说，我们将可学习的模块化任务查询(MTQs)和视觉对齐层(VALs)整合到MLLMs中，在多种VFMs的监督下激活特定的视觉信号。为了协调VFMs之间的表示冲突，精心设计的标记网关掩码(TGM)限制了多组MTQs之间的信息流。大量实验证明，VaCo显著提高了不同MLLMs在各种基准测试上的性能，展示了其在视觉理解方面的卓越能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) integrate image features from visualencoders with LLMs, demonstrating advanced comprehension capabilities. However,mainstream MLLMs are solely supervised by the next-token prediction of textualtokens, neglecting critical vision-centric information essential for analyticalabilities. To track this dilemma, we introduce VaCo, which optimizes MLLMrepresentations through Vision-Centric activation and Coordination frommultiple vision foundation models (VFMs). VaCo introduces visual discriminativealignment to integrate task-aware perceptual features extracted from VFMs,thereby unifying the optimization of both textual and visual outputs in MLLMs.Specifically, we incorporate the learnable Modular Task Queries (MTQs) andVisual Alignment Layers (VALs) into MLLMs, activating specific visual signalsunder the supervision of diverse VFMs. To coordinate representation conflictsacross VFMs, the crafted Token Gateway Mask (TGM) restricts the informationflow among multiple groups of MTQs. Extensive experiments demonstrate that VaCosignificantly improves the performance of different MLLMs on variousbenchmarks, showcasing its superior capabilities in visual comprehension.</description>
      <author>example@mail.com (Yunnan Wang, Fan Lu, Kecheng Zheng, Ziyuan Huang, Ziqiang Li, Wenjun Zeng, Xin Jin)</author>
      <guid isPermaLink="false">2510.14349v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering</title>
      <link>http://arxiv.org/abs/2510.14270v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GauSSmart的混合方法，通过结合2D基础模型和3D高斯飞溅重建技术，解决了Gaussian Splatting在捕捉精细细节和稀疏覆盖区域保持真实感方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;场景重建是计算机视觉中的核心挑战，NeRF和Gaussian Splatting等方法取得了显著进展。但Gaussian Splatting在大规模数据集上表现良好时，往往难以捕捉精细细节或在稀疏覆盖区域保持真实感，这主要是由于稀疏3D训练数据的固有局限性。&lt;h4&gt;目的&lt;/h4&gt;提出GauSSmart，一种有效桥接2D基础模型和3D高斯飞溅重建的混合方法，以提升场景重建的质量和细节表现。&lt;h4&gt;方法&lt;/h4&gt;集成成熟的2D计算机视觉技术，包括凸滤波和来自基础模型(如DINO)的语义特征监督，利用2D分割先验和高维特征嵌入，指导高斯飞溅的密集化和细化，改善代表性不足区域的覆盖，并保持复杂的结构细节。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上的验证表明，GauSSmart在大多数评估场景中一致性地优于现有的高斯飞溅方法，能够更好地捕捉场景细节并提高稀疏覆盖区域的重建质量。&lt;h4&gt;结论&lt;/h4&gt;混合2D-3D方法具有巨大潜力，将2D基础模型与3D重建管道的巧妙结合可以克服单独使用任何一种方法的固有局限性。&lt;h4&gt;翻译&lt;/h4&gt;场景重建已成为计算机视觉中的一个核心挑战，诸如神经辐射场和高斯飞溅等方法已取得显著进展。虽然高斯飞溅在大规模数据集上表现出色，但它往往难以捕捉精细细节或在稀疏覆盖区域保持真实感，这主要是由于稀疏3D训练数据的固有局限性。在本工作中，我们提出了GauSSmart，一种有效桥接2D基础模型和3D高斯飞溅重建的混合方法。我们的方法集成了成熟的2D计算机视觉技术，包括凸滤波和来自基础模型(如DINO)的语义特征监督，以增强基于高斯的场景重建。通过利用2D分割先验和高维特征嵌入，我们的方法指导高斯飞溅的密集化和细化，改善了代表性不足区域的覆盖并保持了复杂的结构细节。我们在三个数据集上验证了我们的方法，其中GauSSmart在大多数评估场景中一致性地优于现有的高斯飞溅方法。我们的结果证明了混合2D-3D方法的巨大潜力，强调了如何将2D基础模型与3D重建管道的巧妙结合可以克服单独使用任何一种方法所固有的局限性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景重建中细节捕捉不足和稀疏覆盖区域真实感差的问题。这个问题很重要，因为高质量的3D重建对虚拟现实、增强现实、自动驾驶等应用至关重要，而现有方法在处理细节和稀疏区域时存在局限性，限制了重建质量和技术应用范围。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考如何结合2D基础模型和3D重建的优势，认识到2D视觉技术（如分割和特征提取）成熟而3D方法擅长空间建模。他们借鉴了DINO等基础模型的语义特征、SAM的图像分割能力，以及凸包过滤技术，并将这些2D方法与3D高斯溅射流程巧妙融合，形成互补优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D基础模型的语义信息指导3D高斯溅射优化，改善点云质量并增强稀疏区域。整体流程包括：1)使用凸包过滤去除点云异常值；2)通过相机聚类选择代表性图像；3)应用SAM进行图像分割并关联3D点；4)基于分割掩码有针对性地增强点云密度；5)引入DINOv3特征嵌入损失提高语义一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)凸包引导的异常值去除方法；2)感知的点云增强策略，考虑语义区域重要性；3)基于DINOv3的嵌入对齐训练损失。相比之前工作，不同之处在于：不是简单拼接2D和3D方法，而是设计真正融合框架；利用语义先验指导3D重建；点云增强考虑语义区域重要性；使用特征嵌入损失而非仅传统光度损失。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GauSSmart通过融合2D基础模型的语义理解与3D高斯溅射的空间建模能力，有效提升了3D场景重建中的细节捕捉和稀疏区域真实感。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene reconstruction has emerged as a central challenge in computer vision,with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splattingachieving remarkable progress. While Gaussian Splatting demonstrates strongperformance on large-scale datasets, it often struggles to capture fine detailsor maintain realism in regions with sparse coverage, largely due to theinherent limitations of sparse 3D training data.  In this work, we propose GauSSmart, a hybrid method that effectively bridges2D foundational models and 3D Gaussian Splatting reconstruction. Our approachintegrates established 2D computer vision techniques, including convexfiltering and semantic feature supervision from foundational models such asDINO, to enhance Gaussian-based scene reconstruction. By leveraging 2Dsegmentation priors and high-dimensional feature embeddings, our method guidesthe densification and refinement of Gaussian splats, improving coverage inunderrepresented areas and preserving intricate structural details.  We validate our approach across three datasets, where GauSSmart consistentlyoutperforms existing Gaussian Splatting in the majority of evaluated scenes.Our results demonstrate the significant potential of hybrid 2D-3D approaches,highlighting how the thoughtful combination of 2D foundational models with 3Dreconstruction pipelines can overcome the limitations inherent in eitherapproach alone.</description>
      <author>example@mail.com (Alexander Valverde, Brian Xu, Yuyin Zhou, Meng Xu, Hongyun Wang)</author>
      <guid isPermaLink="false">2510.14270v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals</title>
      <link>http://arxiv.org/abs/2510.14254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了基础模型在时间序列分析中的应用，特别是比较了专家模型和通用模型在生理信号处理（特别是PPG信号）上的性能差异。&lt;h4&gt;背景&lt;/h4&gt;基础模型是大规模机器学习模型，在大规模数据上预训练后可适应各种下游任务，已广泛应用于自然语言处理和计算机视觉领域。时间序列分析领域，特别是生理信号处理，正逐渐受到关注，但大多数时间序列基础模型是专家模型，只在同类型数据上预训练和测试，如心电图、脑电图和光电容积脉搏波(PPG)。最近的工作如MOMENT尝试训练跨多个领域的通用时间序列基础模型。&lt;h4&gt;目的&lt;/h4&gt;进行全面的基准测试研究，比较专家模型和通用模型的性能，特别关注PPG信号。&lt;h4&gt;方法&lt;/h4&gt;通过总共51个任务组成的测试套件进行评估，包括心脏状态评估、实验室值估计和跨模态推理。在七个维度上全面评估两种模型：获胜分数、平均性能、特征质量、调优增益、性能方差、可转移性和可扩展性。这些指标共同捕捉模型在不同微调策略下的能力、适应性和效率。在完整微调场景下比较模型性能，并提供泛化、公平性、注意力可视化和训练数据选择重要性的进一步分析。&lt;h4&gt;主要发现&lt;/h4&gt;在完整微调场景下，专家模型的获胜分数比通用模型高27%。&lt;h4&gt;结论&lt;/h4&gt;论文提供了专家模型和通用模型在多样化下游场景中的优势和局限性的全面理解。&lt;h4&gt;翻译&lt;/h4&gt;基础模型是大规模机器学习模型，在大规模数据上预训练，并可适应各种下游任务。它们已广泛应用于自然语言处理和计算机视觉任务，如GPT、BERT和CLIP等模型。现在，时间序列分析领域，特别是生理信号处理，也日益受到关注。然而，大多数时间序列基础模型是专家模型，其预训练和测试使用相同类型的数据，如心电图、脑电图和光电容积脉搏波(PPG)。最近的工作如MOMENT，使用来自多个领域（如天气、交通和电力）的数据训练通用时间序列基础模型。本文旨在进行全面的基准测试研究，比较专家模型和通用模型的性能，特别关注PPG信号。通过涵盖心脏状态评估、实验室值估计和跨模态推理的51个任务，我们在七个维度上全面评估了两种模型，包括获胜分数、平均性能、特征质量、调优增益、性能方差、可转移性和可扩展性。这些指标共同捕捉了模型在不同微调策略下的能力、适应性和效率，为它们在多样化下游场景中的优势和局限性提供了全面理解。在完整微调场景下，我们证明专家模型的获胜分数高出27%。最后，我们对泛化、公平性、注意力可视化和训练数据选择的重要性进行了进一步分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are large-scale machine learning models that arepre-trained on massive amounts of data and can be adapted for variousdownstream tasks. They have been extensively applied to tasks in NaturalLanguage Processing and Computer Vision with models such as GPT, BERT, andCLIP. They are now also increasingly gaining attention in time-series analysis,particularly for physiological sensing. However, most time series foundationmodels are specialist models - with data in pre-training and testing of thesame type, such as Electrocardiogram, Electroencephalogram, andPhotoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist timeseries foundation model with data from multiple domains, such as weather,traffic, and electricity. This paper aims to conduct a comprehensivebenchmarking study to compare the performance of generalist and specialistmodels, with a focus on PPG signals. Through an extensive suite of total 51tasks covering cardiac state assessment, laboratory value estimation, andcross-modal inference, we comprehensively evaluate both models across sevendimensions, including win score, average performance, feature quality, tuninggain, performance variance, transferability, and scalability. These metricsjointly capture not only the models' capability but also their adaptability,robustness, and efficiency under different fine-tuning strategies, providing aholistic understanding of their strengths and limitations for diversedownstream scenarios. In a full-tuning scenario, we demonstrate that thespecialist model achieves a 27% higher win score. Finally, we provide furtheranalysis on generalization, fairness, attention visualizations, and theimportance of training data choice.</description>
      <author>example@mail.com (Saurabh Kataria, Yi Wu, Zhaoliang Chen, Hyunjung Gloria Kwak, Yuhao Xu, Lovely Yeswanth Panchumarthi, Ran Xiao, Jiaying Lu, Ayca Ermis, Anni Zhao, Runze Yan, Alex Federov, Zewen Liu, Xu Wu, Wei Jin, Carl Yang, Jocelyn Grunwell, Stephanie R. Brown, Amit Shah, Craig Jabaley, Tim Buchman, Sivasubramanium V Bhavani, Randall J. Lee, Xiao Hu)</author>
      <guid isPermaLink="false">2510.14254v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization</title>
      <link>http://arxiv.org/abs/2510.14217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures, 3 tables, SI: 8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究首次对QM9数据集上的核岭回归进行了全面的谱分析，研究了不同分子表示方法（分子指纹、预训练Transformer、全局和局部3D表示）在七种分子属性上的谱特性，发现更丰富的谱特征并不总能提高准确性，主要特征值捕获了最有信息量的特征。&lt;h4&gt;背景&lt;/h4&gt;理解核的谱特性为泛化和表示质量提供了原则性的视角。虽然深度模型在分子属性预测中实现了最先进的准确性，但核方法因其在小数据环境下的鲁棒性和透明的理论基础而被广泛使用。然而，对分子核的系统性谱分析仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;提供对QM9数据集上核岭回归的首次全面谱分析，研究不同分子表示方法在七种分子属性上的谱特性，探索谱特性与预测性能之间的关系。&lt;h4&gt;方法&lt;/h4&gt;使用四种不同的谱指标测量谱丰富度，实施截断核方法探究谱与预测性能的关系，分析七种分子属性，比较分子指纹、预训练Transformer、全局和局部3D表示等不同表示方法。&lt;h4&gt;主要发现&lt;/h4&gt;1) 更丰富的谱特征并不一致地提高准确性；2) 对于基于Transformer和局部3D表示，谱丰富度甚至可能与性能呈负相关；3) 在许多核中，仅保留前2%的特征值就能恢复几乎所有性能；4) 主要特征值捕获了最有信息量的特征。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明表示、核特征和预测性能之间存在微妙的关系，挑战了关于谱丰富度与性能关系的传统观点。这些发现对如何在数据有限的科学和实际任务中评估核方法和自监督学习方法提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;理解核的谱特性为泛化和表示质量提供了原则性的视角。虽然深度模型在分子属性预测中实现了最先进的准确性，但核方法因其在小数据环境下的鲁棒性和透明的理论基础而被广泛使用。尽管机器学习中核谱的研究广泛，但对分子核的系统性谱分析仍然稀缺。在这项工作中，我们首次对QM9数据集上的核岭回归进行了全面的谱分析，研究了分子指纹、预训练Transformer、全局和局部3D表示在七种分子属性上的谱特性。令人惊讶的是，通过四种不同的谱指标测量的更丰富的谱特征并不一致地提高准确性。皮尔逊相关性测试进一步表明，对于基于Transformer和局部3D表示，谱丰富度甚至可能与性能呈负相关。我们还实现了截断核来探究谱与预测性能之间的关系：在许多核中，仅保留前2%的特征值就能恢复几乎所有性能，这表明主要特征值捕获了最有信息量的特征。我们的结果挑战了'更丰富的谱产生更好的泛化'这一常见启发式方法，并突出了表示、核特征和预测性能之间的微妙关系。除了分子属性预测外，这些发现还指导了如何在数据有限的科学和实际任务中评估核方法和自监督学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the spectral properties of kernels offers a principledperspective on generalization and representation quality. While deep modelsachieve state-of-the-art accuracy in molecular property prediction, kernelmethods remain widely used for their robustness in low-data regimes andtransparent theoretical grounding. Despite extensive studies of kernel spectrain machine learning, systematic spectral analyses of molecular kernels arescarce. In this work, we provide the first comprehensive spectral analysis ofkernel ridge regression on the QM9 dataset, molecular fingerprint, pretrainedtransformer-based, global and local 3D representations across seven molecularproperties. Surprisingly, richer spectral features, measured by four differentspectral metrics, do not consistently improve accuracy. Pearson correlationtests further reveal that for transformer-based and local 3D representations,spectral richness can even have a negative correlation with performance. Wealso implement truncated kernels to probe the relationship between spectrum andpredictive performance: in many kernels, retaining only the top 2% ofeigenvalues recovers nearly all performance, indicating that the leadingeigenvalues capture the most informative features. Our results challenge thecommon heuristic that "richer spectra yield better generalization" andhighlight nuanced relationships between representation, kernel features, andpredictive performance. Beyond molecular property prediction, these findingsinform how kernel and self-supervised learning methods are evaluated indata-limited scientific and real-world tasks.</description>
      <author>example@mail.com (Asma Jamali, Tin Sum Cheng, Rodrigo A. Vargas-Hernández)</author>
      <guid isPermaLink="false">2510.14217v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.14176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ARM-FM是一种利用基础模型高级推理能力的框架，用于强化学习中自动化、组合式的奖励设计，解决了强化学习算法对奖励函数设定敏感的核心挑战。&lt;h4&gt;背景&lt;/h4&gt;强化学习算法对奖励函数的设定高度敏感，这仍然是限制其广泛应用的核心挑战。&lt;h4&gt;目的&lt;/h4&gt;提出ARM-FM框架，实现强化学习中自动化、组合式的奖励设计，利用基础模型的高级推理能力来自动构建奖励机。&lt;h4&gt;方法&lt;/h4&gt;使用奖励机(RMs)作为强化学习目标设定的机制，通过基础模型自动构建奖励机；将语言嵌入与每个奖励机自动机状态相关联以实现跨任务泛化；在多样化挑战性环境中评估框架效果。&lt;h4&gt;主要发现&lt;/h4&gt;ARM-FM框架在多样化的挑战性环境中展现出有效性，包括实现零样本泛化的能力；基础模型能够从自然语言规范自动生成奖励机；结构化的奖励机形式化方法能实现有效的任务分解。&lt;h4&gt;结论&lt;/h4&gt;基础模型与奖励机的结构化形式化方法相结合，能够实现有效的自动化奖励设计，促进强化学习在更广泛领域的应用。&lt;h4&gt;翻译&lt;/h4&gt;强化学习(RL)算法对奖励函数的设定高度敏感，这仍然是限制其广泛适用性的核心挑战。我们提出了ARM-FM：基于基础模型的自动奖励机，这是一个用于强化学习中自动化、组合式奖励设计的框架，利用了基础模型(FMs)的高级推理能力。奖励机(RMs)——一种基于自动机的奖励规范形式化方法——被用作强化学习目标设定的机制，并通过基础模型的使用自动构建。奖励机的结构化形式化方法能够实现有效的任务分解，而基础模型的使用则允许用自然语言进行目标规范。具体而言，我们(i)使用基础模型从自然语言规范自动生成奖励机；(ii)将语言嵌入与每个奖励机自动机状态相关联，以实现跨任务泛化；(iii)在一系列多样化的挑战性环境中提供了ARM-FM有效性的实证证据，包括零样本泛化的证据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) algorithms are highly sensitive to rewardfunction specification, which remains a central challenge limiting their broadapplicability. We present ARM-FM: Automated Reward Machines via FoundationModels, a framework for automated, compositional reward design in RL thatleverages the high-level reasoning capabilities of foundation models (FMs).Reward machines (RMs) -- an automata-based formalism for reward specification-- are used as the mechanism for RL objective specification, and areautomatically constructed via the use of FMs. The structured formalism of RMsyields effective task decompositions, while the use of FMs enables objectivespecifications in natural language. Concretely, we (i) use FMs to automaticallygenerate RMs from natural language specifications; (ii) associate languageembeddings with each RM automata-state to enable generalization across tasks;and (iii) provide empirical evidence of ARM-FM's effectiveness in a diversesuite of challenging environments, including evidence of zero-shotgeneralization.</description>
      <author>example@mail.com (Roger Creus Castanyer, Faisal Mohamed, Pablo Samuel Castro, Cyrus Neary, Glen Berseth)</author>
      <guid isPermaLink="false">2510.14176v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems</title>
      <link>http://arxiv.org/abs/2510.14133v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种代理AI系统的统一建模框架，由主机代理模型和任务生命周期模型组成，解决了当前代理间通信生态系统碎片化的问题，为多AI代理系统提供了形式化验证基础。&lt;h4&gt;背景&lt;/h4&gt;代理AI系统利用多个自主代理和大语言模型解决复杂多步骤任务，在高风险应用中安全性和功能性至关重要。当前代理间通信生态系统碎片化，各种协议被孤立分析，造成语义鸿沟，阻碍系统属性严格分析并引入架构不协调等风险。&lt;h4&gt;目的&lt;/h4&gt;解决代理AI系统中因通信碎片化导致的语义鸿沟问题，提供统一语义框架实现多AI代理系统行为的推理，支持系统化分析、设计和部署正确、可靠、稳健的代理AI系统。&lt;h4&gt;方法&lt;/h4&gt;引入由两个基础模型组成的框架：主机代理模型（正式化顶层实体与用户交互、任务分解和执行协调）和任务生命周期模型（详细说明子任务状态和转换）。基于此框架定义31个属性（主机代理17个，任务生命周期14个），分为活性、安全性、完整性和公平性四类，用时态逻辑表达以实现形式化验证。&lt;h4&gt;主要发现&lt;/h4&gt;两个基础模型共同为多AI代理系统行为推理提供统一语义框架，定义的属性能实现系统行为形式化验证，检测协调边缘情况，防止死锁和安全漏洞。&lt;h4&gt;结论&lt;/h4&gt;引入了第一个严格基础、领域无关的框架，用于代理AI系统的系统化分析、设计和部署，确保系统正确性、可靠性和稳健性。&lt;h4&gt;翻译&lt;/h4&gt;代理AI系统，即利用多个自主代理和大语言模型的系统，正被越来越多地用于解决复杂的多步骤任务。这些系统的安全性、安全性和功能性至关重要，特别是在高风险应用中。然而，当前代理间通信生态系统是碎片化的，诸如用于工具访问的模型上下文协议和用于协调的代理到代理等协议被孤立地分析。这种碎片化造成了语义鸿沟，阻碍了对系统属性的严格分析，并引入了架构不协调和可利用的协调问题等风险。为应对这些挑战，我们引入了一个由两个基础模型组成的代理AI系统建模框架。第一个是主机代理模型，它正式化与用户交互、分解任务并通过利用外部代理和工具协调执行的最高级别实体。第二个是任务生命周期模型，它详细说明从创建到完成的各个子任务的状态和转换，提供细粒度的任务管理和错误处理视图。这两个模型共同为多AI代理系统行为推理提供了统一的语义框架。基于此框架，我们为主机代理定义了17个属性，为任务生命周期定义了14个属性，分为活性、安全性、完整性和公平性四类。用时态逻辑表达的这些属性，能够实现系统行为的正式验证，检测协调边缘情况，并防止死锁和安全漏洞。通过这项工作，我们引入了第一个严格基础、领域无关的框架，用于代理AI系统的系统化分析、设计和部署，以确保正确、可靠和稳健的系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Agentic AI systems, which leverage multiple autonomous agents and LargeLanguage Models (LLMs), are increasingly used to address complex, multi-steptasks. The safety, security, and functionality of these systems are critical,especially in high-stakes applications. However, the current ecosystem ofinter-agent communication is fragmented, with protocols such as the ModelContext Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocolfor coordination being analyzed in isolation. This fragmentation creates asemantic gap that prevents the rigorous analysis of system properties andintroduces risks such as architectural misalignment and exploitablecoordination issues. To address these challenges, we introduce a modelingframework for agentic AI systems composed of two foundational models. Thefirst, the host agent model, formalizes the top-level entity that interactswith the user, decomposes tasks, and orchestrates their execution by leveragingexternal agents and tools. The second, the task lifecycle model, details thestates and transitions of individual sub-tasks from creation to completion,providing a fine-grained view of task management and error handling. Together,these models provide a unified semantic framework for reasoning about thebehavior of multi-AI agent systems. Grounded in this framework, we define 17properties for the host agent and 14 for the task lifecycle, categorized intoliveness, safety, completeness, and fairness. Expressed in temporal logic,these properties enable formal verification of system behavior, detection ofcoordination edge cases, and prevention of deadlocks and securityvulnerabilities. Through this effort, we introduce the first rigorouslygrounded, domain-agnostic framework for the systematic analysis, design, anddeployment of correct, reliable, and robust agentic AI systems.</description>
      <author>example@mail.com (Edoardo Allegrini, Ananth Shreekumar, Z. Berkay Celik)</author>
      <guid isPermaLink="false">2510.14133v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Shadow Molecular Dynamics for Flexible Multipole Models</title>
      <link>http://arxiv.org/abs/2510.14132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究将阴影分子动力学扩展到柔性多极模型，处理长程静电相互作用，提供稳定高效的原子模拟框架。&lt;h4&gt;背景&lt;/h4&gt;阴影分子动力学是处理具有长程静电相互作用的柔性电荷模型的高效稳定原子模拟框架，但之前实现仅限于原子单极电荷分布。&lt;h4&gt;目的&lt;/h4&gt;扩展阴影分子动力学方法以支持柔性多极模型，实现更准确的原子相互作用模拟。&lt;h4&gt;方法&lt;/h4&gt;推导阴影能量函数、势能和力项的详细表达式，明确包含单极-单极、偶极-单极和偶极-偶极相互作用；将原子单极和偶极视为扩展动力学变量；提出单极固定而偶极柔性的分子动力学方案。&lt;h4&gt;主要发现&lt;/h4&gt;引入额外偶极自由度保留了仅单极阴影分子动力学模拟的稳定性和准确性；扩展的阴影动力学为涉及柔性多极长程相互作用的稳定、计算高效且多功能的分子动力学模拟提供了框架。&lt;h4&gt;结论&lt;/h4&gt;该方法与现代人工智能和机器学习技术结合特别有意义，有助于开发可转移的高精度原子相互作用表示，适用于各种分子系统。&lt;h4&gt;翻译&lt;/h4&gt;阴影分子动力学为具有长程静电相互作用的柔性电荷模型提供了一种高效稳定的原子模拟框架。虽然之前的实现仅限于原子单极电荷分布，但我们将这种方法扩展到了柔性多极模型。我们推导了阴影能量函数、势能和力项的详细表达式，明确包含了单极-单极、偶极-单极和偶极-偶极相互作用。在我们的公式中，原子单极和原子偶极都被视为扩展的动力学变量，与核自由度的传播一起处理。我们证明引入额外的偶极自由度保留了之前在仅单极阴影分子动力学模拟中看到的稳定性和准确性。此外，我们提出了一种阴影分子动力学方案，其中单极电荷保持固定，而偶极保持柔性。我们的扩展阴影动力学为涉及柔性多极之间长程相互作用的稳定、计算高效且多功能的分子动力学模拟提供了框架。这与现代人工智能和机器学习技术结合特别有意义，这些技术越来越多地用于开发原子模拟的物理信息驱动和数据驱动的基础模型。这些模型旨在提供可转移的高精度原子相互作用表示，适用于各种分子系统，这需要准确处理长程电荷相互作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Shadow molecular dynamics provide an efficient and stable atomisticsimulation framework for flexible charge models with long-range electrostaticinteractions. While previous implementations have been limited to atomicmonopole charge distributions, we extend this approach to flexible multipolemodels. We derive detailed expressions for the shadow energy functions,potentials, and force terms, explicitly incorporating monopole-monopole,dipole-monopole, and dipole-dipole interactions. In our formulation, bothatomic monopoles and atomic dipoles are treated as extended dynamical variablesalongside the propagation of the nuclear degrees of freedom. We demonstratethat introducing the additional dipole degrees of freedom preserves thestability and accuracy previously seen in monopole-only shadow moleculardynamics simulations. Additionally, we present a shadow molecular dynamicsscheme where the monopole charges are held fixed while the dipoles remainflexible. Our extended shadow dynamics provide a framework for stable,computationally efficient, and versatile molecular dynamics simulationsinvolving long-range interactions between flexible multipoles. This is ofparticular interest in combination with modern artificial intelligence andmachine learning techniques, which are increasingly used to developphysics-informed and data-driven foundation models for atomistic simulations.These models aim to provide transferable, high-accuracy representations ofatomic interactions that are applicable across diverse sets of molecularsystems, which requires accurate treatment of long-range charge interactions.</description>
      <author>example@mail.com (Rae A. Corrigan Grove, Robert Stanton, Michael E. Wall, Anders M. N. Niklasson)</author>
      <guid isPermaLink="false">2510.14132v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Exploratory Causal Inference in SAEnce</title>
      <link>http://arxiv.org/abs/2510.14073v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为Neural Effect Search的新方法，可以直接从数据中发现未知的因果效应，解决了传统随机对照试验的局限性。&lt;h4&gt;背景&lt;/h4&gt;随机对照试验是科学的重要支柱，但它们依赖于手工制作的假设和昂贵的分析。这些限制阻碍了大规模因果效应估计，可能导致依赖于流行但不完整的假设。&lt;h4&gt;目的&lt;/h4&gt;直接从数据中发现治疗的未知效应。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的基础模型将试验中的非结构化数据转换为有意义的表示，通过稀疏自编码器解释这些表示，并引入Neural Effect Search这一新颖的递归过程，通过渐进分层解决多重测试问题和效应纠缠问题。&lt;h4&gt;主要发现&lt;/h4&gt;在半合成实验中评估了算法的稳健性，并在实验生态学背景下展示了在真实世界科学试验中首次成功的无监督因果效应识别。&lt;h4&gt;结论&lt;/h4&gt;Neural Effect Search方法成功解决了在神经水平发现显著因果效应的挑战。&lt;h4&gt;翻译&lt;/h4&gt;随机对照试验是科学的重要支柱；然而，它们依赖于手工制作的假设和昂贵的分析。这些限制阻碍了大规模因果效应估计，可能导致依赖于流行但不完整的假设。我们提出直接从数据中发现治疗的未知效应。为此，我们通过预训练的基础模型将试验中的非结构化数据转换为有意义的表示，并通过稀疏自编码器解释它们。然而，由于多重测试问题和效应纠缠，在神经水平发现显著的因果效应并不简单。为了解决这些挑战，我们引入了Neural Effect Search，这是一种新颖的递归过程，通过渐进分层解决了这两个问题。在半合成实验中评估了我们算法的稳健性后，我们在实验生态学的背景下展示了在真实世界科学试验中首次成功的无监督因果效应识别。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Randomized Controlled Trials are one of the pillars of science; nevertheless,they rely on hand-crafted hypotheses and expensive analysis. Such constraintsprevent causal effect estimation at scale, potentially anchoring on popular yetincomplete hypotheses. We propose to discover the unknown effects of atreatment directly from data. For this, we turn unstructured data from a trialinto meaningful representations via pretrained foundation models and interpretthem via a sparse autoencoder. However, discovering significant causal effectsat the neural level is not trivial due to multiple-testing issues and effectsentanglement. To address these challenges, we introduce Neural Effect Search, anovel recursive procedure solving both issues by progressive stratification.After assessing the robustness of our algorithm on semi-synthetic experiments,we showcase, in the context of experimental ecology, the first successfulunsupervised causal effect identification on a real-world scientific trial.</description>
      <author>example@mail.com (Tommaso Mencattini, Riccardo Cadei, Francesco Locatello)</author>
      <guid isPermaLink="false">2510.14073v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Context-Selective State Space Models: Feedback is All You Need</title>
      <link>http://arxiv.org/abs/2510.14027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了COFFEE模型，一种新颖的时变状态空间模型，通过状态反馈实现上下文相关的选择性，有效捕获长距离依赖关系，并在多项任务上取得了优于现有S6模型的结果。&lt;h4&gt;背景&lt;/h4&gt;Transformers模型基于注意力机制，是大多数基础模型的骨干，但它们具有二次复杂度，并且在处理输入序列中的长距离依赖关系时存在困难。状态空间模型(SSMs)提供了一种高效的替代方案，其中S6模块在长序列基准测试上取得了最先进的结果。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理长距离依赖关系的高效序列模型，解决Transformers模型的二次复杂度问题，并超越现有状态空间模型的性能。&lt;h4&gt;方法&lt;/h4&gt;提出COFFEE(COntext From FEEdback)模型，一种新颖的时变SSM，结合状态反馈以实现上下文相关的选择性。与S6不同，COFFEE从内部状态计算选择性，该状态作为序列历史的紧凑表示，使模型能够根据积累的上下文调节其动态。此外，采用高效的模型参数化方法消除冗余，实现更紧凑和可训练的公式。&lt;h4&gt;主要发现&lt;/h4&gt;在归纳头任务上，COFFEE与S6相比，使用少两个数量级的参数和训练序列实现了接近完美的准确性；在MNIST上，仅用3585个参数就达到了97%的准确率，大大优于S6在相同架构上的表现。&lt;h4&gt;结论&lt;/h4&gt;状态反馈是构建可扩展和高效序列模型的关键机制，COFFEE模型通过结合状态反馈和高效参数化，显著提升了序列建模能力，特别是在处理长距离依赖关系方面。&lt;h4&gt;翻译&lt;/h4&gt;Transformers模型由注意力机制驱动，是大多数基础模型的骨干，但它们受二次复杂度的困扰，并且在处理输入序列中的长距离依赖关系时存在困难。最近的研究表明，状态空间模型(SSMs)提供了一种高效的替代方案，其中S6模块作为Mamba架构的核心，在长序列基准测试上取得了最先进的结果。在本文中，我们介绍了COFFEE(COntext From FEEdback)模型，一种新颖的时变SSM，它结合了状态反馈以实现上下文相关的选择性，同时仍允许并行实现。而S6的选择性机制仅依赖于当前输入，COFFEE从内部状态计算选择性，该状态作为序列历史的紧凑表示。这种转变使模型能够根据积累的上下文调节其动态，提高其捕获长距离依赖关系的能力。除了状态反馈外，我们还采用了一种高效的模型参数化方法，消除了S6中存在的冗余，导致更紧凑和可训练的公式。在归纳头任务上，COFFEE与S6相比，使用少两个数量级的参数和训练序列实现了接近完美的准确性。在MNIST上，COFFEE在相同架构上大大优于S6，仅用3585个参数就达到了97%的准确率。这些结果展示了状态反馈作为构建可扩展和高效序列模型的关键机制的作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers, powered by the attention mechanism, are the backbone of mostfoundation models, yet they suffer from quadratic complexity and difficultiesin dealing with long-range dependencies in the input sequence. Recent work hasshown that state space models (SSMs) provide an efficient alternative, with theS6 module at the core of the Mamba architecture achieving state-of-the-artresults on long-sequence benchmarks. In this paper, we introduce the COFFEE(COntext From FEEdback) model, a novel time-varying SSM that incorporates statefeedback to enable context-dependent selectivity, while still allowing forparallel implementation. Whereas the selectivity mechanism of S6 only dependson the current input, COFFEE computes it from the internal state, which servesas a compact representation of the sequence history. This shift allows themodel to regulate its dynamics based on accumulated context, improving itsability to capture long-range dependencies. In addition to state feedback, weemploy an efficient model parametrization that removes redundancies present inS6 and leads to a more compact and trainable formulation. On the induction headtask, COFFEE achieves near-perfect accuracy with two orders of magnitude fewerparameters and training sequences compared to S6. On MNIST, COFFEE largelyoutperforms S6 within the same architecture, reaching 97% accuracy with only3585 parameters. These results showcase the role of state feedback as a keymechanism for building scalable and efficient sequence models.</description>
      <author>example@mail.com (Riccardo Zattra, Giacomo Baggio, Umberto Casti, Augusto Ferrante, Francesco Ticozzi)</author>
      <guid isPermaLink="false">2510.14027v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching</title>
      <link>http://arxiv.org/abs/2510.13721v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NExT-OMNI是一个开源的全模态基础模型，通过离散流范式实现统一建模，支持任何到任何的跨模态生成和多轮交互，克服了现有自回归架构的局限性。&lt;h4&gt;背景&lt;/h4&gt;下一代多模态基础模型将成为人工通用智能系统的核心，但现有多模态模型受限于自回归架构，无法平衡整合理解与生成能力。混合和解耦策略虽被探索，但其冗余设计限制了在广泛场景如跨模态检索中的应用。&lt;h4&gt;目的&lt;/h4&gt;引入NExT-OMNI，一个开源的全模态基础模型，通过离散流范式实现统一建模，支持任何到任何的理解和生成，并扩展应用场景。&lt;h4&gt;方法&lt;/h4&gt;利用度量诱导的概率路径和动力学最优速度，原生支持任何到任何的理解和生成，增强响应效率；通过简洁的统一表示而非任务解耦设计实现更广泛应用；在大规模交错文本、图像、视频和音频数据上训练。&lt;h4&gt;主要发现&lt;/h4&gt;NExT-OMNI在多模态生成和理解基准测试中具有竞争力，在多模态交互和跨模态检索方面优于之前的统一模型，展现了其作为下一代多模态基础模型的架构优势。&lt;h4&gt;结论&lt;/h4&gt;发布训练细节、数据协议，并开源代码和模型检查点，以促进多模态基础模型领域的进一步研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;能够进行任何到任何跨模态生成和多轮交互的下一代多模态基础模型将成为人工通用智能系统的核心组成部分，在人机交互中发挥关键作用。然而，大多数现有多模态模型仍受限于自回归架构，其固有局限性阻碍了理解与生成能力的平衡整合。虽然混合和解耦策略已被探索用于在统一框架内分别解决这些问题，但它们的冗余、非集成设计限制了它们在更广泛场景（如跨模态检索）中的适用性。在这项工作中，我们引入了NExT-OMNI，一个开源的全模态基础模型，通过离散流范式实现统一建模。通过利用度量诱导的概率路径和动力学最优速度，NExT-OMNI原生支持任何到任何的理解和生成，同时通过简洁的统一表示而非任务解耦设计，实现更广泛的应用场景，增强响应效率。在大规模交错文本、图像、视频和音频数据上训练后，NExT-OMNI在多模态生成和理解基准测试中具有竞争力，同时在多模态交互和跨模态检索方面优于之前的统一模型，凸显了其作为下一代多模态基础模型的架构优势。为进一步推进研究，我们发布了训练细节、数据协议，并开源了代码和模型检查点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next-generation multimodal foundation models capable of any-to-anycross-modal generation and multi-turn interaction will serve as core componentsof artificial general intelligence systems, playing a pivotal role inhuman-machine interaction. However, most existing multimodal models remainconstrained by autoregressive architectures, whose inherent limitations preventa balanced integration of understanding and generation capabilities. Althoughhybrid and decoupling strategies have been explored to address these taskswithin unified frameworks separately, their redundant, non-integrated designslimit their applicability to broader scenarios, such as cross-modal retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal foundation modelthat achieves unified modeling through discrete flow paradigms. By leveragingmetric-induced probability paths and kinetic optimal velocities, NExT-OMNInatively supports any-to-any understanding and generation with enhancedresponse efficiency, while enabling broader application scenarios throughconcise unified representations rather than task-decoupled designs. Trained onlarge-scale interleaved text, image, video, and audio data, NExT-OMNI deliverscompetitive performance on multimodal generation and understanding benchmarks,while outperforming prior unified models in multi-turn multimodal interactionand cross-modal retrieval, highlighting its architectural advantages as anext-generation multimodal foundation model. To advance further research, werelease training details, data protocols, and open-source both the code andmodel checkpoints.</description>
      <author>example@mail.com (Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua)</author>
      <guid isPermaLink="false">2510.13721v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning</title>
      <link>http://arxiv.org/abs/2510.13909v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种知识推理语言模型(KRLM)，用于解决归纳知识图谱推理中LLM知识与KG上下文协调的问题。通过设计KRL指令格式、KRL分词器、KRL注意力层和结构感知的下一个实体预测器，模型能够在KGR过程中实现LLM知识与KG上下文的统一协调，有效约束LLM的生成幻觉，提高推理结果的可信度。&lt;h4&gt;背景&lt;/h4&gt;归纳知识图谱推理旨在发现包含未知实体和关系的开放域知识图谱中的事实，这给KGR模型在理解不确定的KG组件方面带来了挑战。现有研究提出了知识图谱基础模型来处理这种不确定性，而大型语言模型在开放域知识推理方面展示了强大能力。最新的研究集中在基于LLM的KGFMs上，这些模型整合了LLM知识与KG上下文进行归纳KGR。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于LLM的KGR方法中LLM知识被稀疏KG上下文掩盖导致知识扭曲的问题，以及难以完全约束LLM生成幻觉的问题，提出一个知识推理语言模型(KRLM)，在KGR过程中实现LLM知识与KG上下文的统一协调。&lt;h4&gt;方法&lt;/h4&gt;设计了一种知识推理语言(KRL)指令格式和KRL分词器，以对齐LLM知识与KG表示；提出了一种KRL注意力层，通过动态知识记忆机制协调内在的LLM知识与额外的KG上下文；提出了一种结构感知的下一个实体预测器，将推理结果严格限制在可信的知识域内。&lt;h4&gt;主要发现&lt;/h4&gt;在25个真实世界的归纳KGR数据集上进行了广泛的实验，结果表明所提出的KRLM在零样本推理和微调场景下都具有显著的优越性。&lt;h4&gt;结论&lt;/h4&gt;KRLM模型有效地解决了LLM知识与KG上下文协调的问题，通过结构感知的下一个实体预测器提高了推理结果的可信度，在多个数据集上表现优异，证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;归纳知识图谱推理旨在发现包含未知实体和关系的开放域知识图谱中的事实，这给KGR模型在理解不确定的KG组件方面带来了挑战。现有研究提出了知识图谱基础模型，这些模型学习跨知识图谱的结构不变性来处理这种不确定性。最近，大型语言模型在开放域知识推理方面展示了强大的能力。因此，最新的研究集中在基于LLM的知识图谱基础模型上，这些模型整合了LLM知识与KG上下文进行归纳KGR。然而，LLM的内在知识可能被稀疏的KG上下文掩盖，导致LLM知识扭曲，这可能对模型推理造成不可逆的损害。此外，现有的基于LLM的KGR方法仍然难以完全约束LLM中的生成幻觉，严重限制了推理结果的可信度。为解决这些局限性，我们提出了一种知识推理语言模型(KRLM)，在KGR过程中实现LLM知识与KG上下文的统一协调。具体来说，我们设计了一种知识推理语言(KRL)指令格式和KRL分词器，以对齐LLM知识与KG表示。然后，我们提出了一种KRL注意力层，通过动态知识记忆机制协调内在的LLM知识与额外的KG上下文。最后，提出了一种结构感知的下一个实体预测器，将推理结果严格限制在可信的知识域内。在25个真实世界的归纳KGR数据集上的广泛实验结果表明，所提出的KRLM在零样本推理和微调场景下都具有显著的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inductive Knowledge Graph Reasoning (KGR) aims to discover facts inopen-domain KGs containing unknown entities and relations, which poses achallenge for KGR models in comprehending uncertain KG components. Existingstudies have proposed Knowledge Graph Foundation Models (KGFMs) that learnstructural invariances across KGs to handle this uncertainty. Recently, LargeLanguage Models (LLMs) have demonstrated strong capabilities for open-domainknowledge reasoning. As a result, the latest research has focused on LLM-basedKGFMs that integrate LLM knowledge with KG context for inductive KGR. However,the intrinsic knowledge of LLMs may be overshadowed by sparse KG context,leading to LLM knowledge distortion, which can cause irreversible damage tomodel reasoning. Moreover, existing LLM-based KGR methods still struggle tofully constrain generative hallucinations in LLMs, severely limiting thecredibility of reasoning results. To address these limitations, we propose aKnowledge Reasoning Language Model (KRLM) that achieves unified coordinationbetween LLM knowledge and KG context throughout the KGR process. Specifically,we design a Knowledge Reasoning Language (KRL) instruction format and a KRLtokenizer to align LLM knowledge with KG representations. Then, we propose aKRL attention layer that coordinates intrinsic LLM knowledge with additional KGcontext through a dynamic knowledge memory mechanism. Finally, astructure-aware next-entity predictor is proposed, which strictly constrainsthe reasoning results within a trustworthy knowledge domain. Extensiveexperimental results on 25 real-world inductive KGR datasets demonstrate thesignificant superiority of the proposed KRLM\footnote{Our source codes areavailable at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shotreasoning and fine-tuning scenarios.</description>
      <author>example@mail.com (Xingrui Zhuo, Jiapu Wang, Gongqing Wu, Zhongyuan Wang, Jichen Zhang, Shirui Pan, Xindong Wu)</author>
      <guid isPermaLink="false">2510.13909v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2510.14810v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了SPHeRe（结构投影Hebbian表示）方法，一种新型无监督学习技术，通过整合正交性和结构信息保留解决了传统Hebbian学习在机器学习中的局限性，在多个任务中取得了优异表现。&lt;h4&gt;背景&lt;/h4&gt;Hebbian学习是一种描述神经元通过重复刺激调整连接的生物原理，但在机器学习应用中存在连接更新无约束和缺乏反馈中介考虑等问题，限制了其在复杂网络架构和任务中的有效扩展。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服传统Hebbian学习局限性的无监督学习方法，使其能够有效扩展到复杂网络架构和任务中。&lt;h4&gt;方法&lt;/h4&gt;SPHeRe通过局部的辅助非线性块整合正交性和结构信息保留，结构信息保留的损失通过辅助轻量级投影反向传播到输入（充当反馈中介），正交性约束则确保更新幅度的有界性。&lt;h4&gt;主要发现&lt;/h4&gt;SPHeRe在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准图像分类基准测试中达到无监督突触可塑性方法的最新性能；在持续学习和迁移学习场景中表现有效；图像重建任务证明了提取特征的鲁棒性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了Hebbian无监督学习规则在现代深度学习框架中的竞争力和潜力，展示了不依赖严格反向传播的高效且受生物启发的学习算法的可能性，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;Hebbian学习是一种生物原理，直观地描述了神经元如何通过重复刺激来调整其连接。然而，当应用于机器学习时，由于连接更新的无约束性和缺乏对反馈中介的考虑，它存在严重问题。这些缺点限制了其在复杂网络架构和任务中的有效扩展。为此，我们在此引入结构投影Hebbian表示（SPHeRe），一种新型无监督学习方法，它通过一个局部的辅助非线性块整合了正交性和结构信息保留。结构信息保留的损失通过一个辅助的轻量级投影反向传播到输入，这个投影在概念上充当反馈中介，而正交性约束则考虑了更新幅度的有界性。大量实验结果表明，SPHeRe在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准图像分类基准测试的无监督突触可塑性方法中达到了最先进性能。此外，该方法在持续学习和迁移学习场景中表现出强大的有效性，图像重建任务显示了所提取特征的鲁棒性和泛化能力。这项工作证明了Hebbian无监督学习规则在现代深度学习框架中的竞争力和潜力，展示了不依赖于严格反向传播的高效且受生物启发的学习算法的可能性。我们的代码可在https://github.com/brain-intelligence-lab/SPHeRe获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hebbian learning is a biological principle that intuitively describes howneurons adapt their connections through repeated stimuli. However, when appliedto machine learning, it suffers serious issues due to the unconstrained updatesof the connections and the lack of accounting for feedback mediation. Suchshortcomings limit its effective scaling to complex network architectures andtasks. To this end, here we introduce the Structural Projection HebbianRepresentation (SPHeRe), a novel unsupervised learning method that integratesorthogonality and structural information preservation through a local auxiliarynonlinear block. The loss for structural information preservationbackpropagates to the input through an auxiliary lightweight projection thatconceptually serves as feedback mediation while the orthogonality constraintsaccount for the boundedness of updating magnitude. Extensive experimentalresults show that SPHeRe achieves SOTA performance among unsupervised synapticplasticity approaches on standard image classification benchmarks, includingCIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strongeffectiveness in continual learning and transfer learning scenarios, and imagereconstruction tasks show the robustness and generalizability of the extractedfeatures. This work demonstrates the competitiveness and potential of Hebbianunsupervised learning rules within modern deep learning frameworks,demonstrating the possibility of efficient and biologically inspired learningalgorithms without the strong dependence on strict backpropagation. Our code isavailable at https://github.com/brain-intelligence-lab/SPHeRe.</description>
      <author>example@mail.com (Shikuang Deng, Jiayuan Zhang, Yuhang Wu, Ting Chen, Shi Gu)</author>
      <guid isPermaLink="false">2510.14810v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning to Recognize Quantum Phases of Matter</title>
      <link>http://arxiv.org/abs/2510.14742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出使用无监督学习方法来确定多体系统的量子相图，该方法能够自主识别并可能揭示量子物质的新相位。&lt;h4&gt;背景&lt;/h4&gt;将多体系统的量子相图绘制视为学习问题，需要根据某种分类标准对其基态进行标记以定义不同的相。&lt;h4&gt;目的&lt;/h4&gt;采用无监督学习方法来确定多体系统的量子相图，算法无需访问任何预先标记的状态。&lt;h4&gt;方法&lt;/h4&gt;算法直接处理量子态，基于量子态之间的保真度相似性标准对基态配置进行分组。使用基于谱聚类的无监督学习算法，并结合'轮廓'和'肘部'方法来确定相位的最佳数量。&lt;h4&gt;主要发现&lt;/h4&gt;通过两个具体的自旋-1/2链进行基准测试，发现基于谱聚类的无监督学习算法能够准确重现相图。&lt;h4&gt;结论&lt;/h4&gt;无监督学习可以自主识别并可能揭示量子物质的新相位，为量子相图的确定提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;在哈密顿量参数空间中绘制多体系统的量子相图可以被视为一个学习问题，这需要根据定义相位的某种分类标准来标记相应的基态。在本工作中，我们采用无监督学习方法，其中算法无法访问任何预先标记的状态，作为确定多体系统量子相图的一种工具。该算法直接处理量子态：给定不同哈密顿量参数的基态配置，该过程基于量子态之间保真度的相似性标准揭示了对它们进行分组的最重要的方式，这种标准即使通过实验也容易估计。我们使用两个特定的自旋-1/2链来基准测试我们的方法，其状态通过张量网络技术确定。我们发现，基于谱聚类的无监督学习算法，结合用于确定相位最佳数量的'轮廓'和'肘部'方法，可以准确重现相图。我们的结果表明，无监督学习如何能够自主识别并可能揭示量子物质的新相位。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drawing the quantum phase diagram of a many-body system in the parameterspace of its Hamiltonian can be seen as a learning problem, which implieslabelling the corresponding ground states according to some classificationcriterium that defines the phases. In this work we adopt unsupervised learning,where the algorithm has no access to any priorly labeled states, as a tool fordetermining quantum phase diagrams of many-body systems. The algorithm directlyworks with quantum states: given the ground-state configurations for differentvalues of the Hamiltonian parameters, the process uncovers the most significantway of grouping them based on a similarity criterion that refers to thefidelity between quantum states, that can be easily estimated, evenexperimentally. We benchmark our method with two specific spin-$\frac{1}{2}$chains, with states determined via tensor network techniques. We find thatunsupervised learning algorithms based on spectral clustering, combined with``silhouette'' and ``elbow'' methods for determining the optimal number ofphases, can accurately reproduce the phase diagrams. Our results show howunsupervised learning can autonomously recognize and possibly unveil novelphases of quantum matter.</description>
      <author>example@mail.com (Mehran Khosrojerdi, Alessandro Cuccoli, Paola Verrucchi, Leonardo Banchi)</author>
      <guid isPermaLink="false">2510.14742v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Policy Effects under Network Interference without Network Information: A Transfer Learning Approach</title>
      <link>http://arxiv.org/abs/2510.14415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文开发了一个敏感性分析框架，将完全观测网络中的源数据的平均总处理效应转移到网络完全未知的目标数据中，以估计政策的平均社会影响。&lt;h4&gt;背景&lt;/h4&gt;研究假设源数据和目标数据共享相同的条件均值结果（协变量漂移类型假设），但由于目标网络未被观测，这一假设本身不足以确定目标数据的ATTE。&lt;h4&gt;目的&lt;/h4&gt;解决目标网络未观测情况下如何估计ATTE的问题，通过基于目标网络度分布不确定性的敏感性分析来构建ATTE的界限。&lt;h4&gt;方法&lt;/h4&gt;考虑基于目标网络度分布不确定性的敏感性分析，不确定性程度由给定参考度分布的Wasserstein距离衡量；使用基于线性规划的估计量构建目标ATTE的界限；通过函数delta方法推导界限估计量的极限分布；开发wild bootstrap方法来近似该分布。&lt;h4&gt;主要发现&lt;/h4&gt;构建了目标ATTE的界限估计量，推导了其极限分布，并开发了wild bootstrap方法来近似该分布。&lt;h4&gt;结论&lt;/h4&gt;该框架允许在目标网络完全未知的情况下，通过敏感性分析来估计政策的平均社会影响。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文开发了一个敏感性分析框架，将具有完全观测网络的源数据中的平均总处理效应（ATTE）转移到网络完全未知的目标数据中。ATTE代表了对数据集中每个个体实施政策的平均社会影响。我们提出了一个协变量漂移类型的假设，即源数据和目标数据共享相同的条件均值结果。然而，由于目标网络未被观测，这一假设本身不足以确定目标数据的ATTE。为了解决这个问题，我们考虑了基于目标网络度分布不确定性的敏感性分析，其中不确定性程度由给定参考度分布的Wasserstein距离来衡量。然后，我们使用基于线性规划的估计量构建了目标ATTE的界限。通过函数delta方法推导了界限估计量的极限分布，并开发了wild bootstrap方法来近似该分布。作为一个实证说明，我们重新研究了Cai等人（2015）关于中国农民天气保险采用的社会网络实验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper develops a sensitivity analysis framework that transfers theaverage total treatment effect (ATTE) from source data with a fully observednetwork to target data whose network is completely unknown. The ATTE representsthe average social impact of a policy that assigns the treatment to everyindividual in the dataset. We postulate a covariate-shift type assumption thatboth source and target datasets share the same conditional mean outcome.However, because the target network is unobserved, this assumption alone is notsufficient to pin down the ATTE for the target data. To address this issue, weconsider a sensitivity analysis based on the uncertainty of the targetnetwork's degree distribution, where the extent of uncertainty is measured bythe Wasserstein distance from a given reference degree distribution. We thenconstruct bounds on the target ATTE using a linear programming-based estimator.The limiting distribution of the bound estimator is derived via the functionaldelta method, and we develop a wild bootstrap approach to approximate thedistribution. As an empirical illustration, we revisit the social networkexperiment on farmers' weather insurance adoption in China by Cai et al.(2015).</description>
      <author>example@mail.com (Tadao Hoshino)</author>
      <guid isPermaLink="false">2510.14415v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Glitch noise classification in KAGRA O3GK observing data using unsupervised machine learning</title>
      <link>http://arxiv.org/abs/2510.14291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 7 figures, accepted to Physics Letters B&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了使用无监督机器学习方法对KAGRA O3GK数据中的非平稳噪声进行图像分类的有效性，成功识别出八种不同的故障噪声类别，提高了引力波观测的可靠性。&lt;h4&gt;背景&lt;/h4&gt;引力波干涉仪受到各种非平稳噪声（称为故障噪声）的干扰，这些噪声影响数据分析和干涉仪的灵敏度。&lt;h4&gt;目的&lt;/h4&gt;准确识别和分类故障噪声，以提高引力波观测的可靠性。&lt;h4&gt;方法&lt;/h4&gt;使用变分自编码器(VAE)结合谱聚类的无监督机器学习方法，对KAGRA O3GK数据中的非平稳噪声图像进行分类，将潜在变量降维后在三维空间中可视化并进行分类。&lt;h4&gt;主要发现&lt;/h4&gt;成功识别出八种不同的故障噪声类别，并更好地理解了KAGRA在O3GK期间的故障噪声特征。&lt;h4&gt;结论&lt;/h4&gt;无监督学习在故障噪声分类方面显示出潜力，这有助于干涉仪升级和未来第三代引力波天文台的发展。&lt;h4&gt;翻译&lt;/h4&gt;引力波干涉仪受到各种类型的非平稳噪声干扰，称为故障噪声，这些噪声影响数据分析和干涉仪灵敏度。准确识别和分类故障噪声对于提高引力波观测的可靠性至关重要。在本研究中，我们展示了无监督机器学习在KAGRA O3GK数据中分类含有非平稳噪声图像的有效性。使用变分自编码器(VAE)结合谱聚类，我们识别出八种不同的故障噪声类别。从VAE获得的潜在变量被降维，在三维空间中进行可视化，并使用谱聚类进行分类，以便更好地理解KAGRA在O3GK期间的故障噪声特征。我们的结果强调了无监督学习在高效故障噪声分类方面的潜力，这可能反过来促进干涉仪升级和未来第三代引力波天文台的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gravitational wave interferometers are disrupted by various types ofnonstationary noise, referred to as glitch noise, that affect data analysis andinterferometer sensitivity. The accurate identification and classification ofglitch noise are essential for improving the reliability of gravitational waveobservations. In this study, we demonstrated the effectiveness of unsupervisedmachine learning for classifying images with nonstationary noise in the KAGRAO3GK data. Using a variational autoencoder (VAE) combined with spectralclustering, we identified eight distinct glitch noise categories. The latentvariables obtained from VAE were dimensionally compressed, visualized inthree-dimensional space, and classified using spectral clustering to betterunderstand the glitch noise characteristics of KAGRA during the O3GK period.Our results highlight the potential of unsupervised learning for efficientglitch noise classification, which may in turn potentially facilitateinterferometer upgrades and the development of future third-generationgravitational wave observatories.</description>
      <author>example@mail.com (Shoichi Oshino, Yusuke Sakai, Marco Meyer-Conde, Takashi Uchiyama, Yousuke Itoh, Yutaka Shikano, Yoshikazu Terada, Hirotaka Takahashi)</author>
      <guid isPermaLink="false">2510.14291v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>High-Dimensional BWDM: A Robust Nonparametric Clustering Validation Index for Large-Scale Data</title>
      <link>http://arxiv.org/abs/2510.14145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的稳健非参数聚类验证框架HD-BWDM，用于解决高维或受污染数据中确定适当聚类数量的问题。&lt;h4&gt;背景&lt;/h4&gt;确定无监督学习中适当的聚类数量是统计学和数据科学中的核心问题。传统的有效性指标如Calinski-Harabasz、Silhouette和Davies-Bouldin依赖于基于质心的距离，在高维或受污染数据中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一个新的稳健的非参数聚类验证框架HD-BWDM，将BWDM标准扩展到高维空间，解决传统方法在高维数据中的局限性。&lt;h4&gt;方法&lt;/h4&gt;HD-BWDM整合随机投影和主成分分析缓解维度诅咒，应用修剪聚类和基于medoid的距离确保对离群点的稳健性。作者推导了理论结果，证明在Johnson-Lindenstrauss嵌入下的一致性和收敛性。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的模拟表明，HD-BWDM在高维投影和污染情况下保持稳定性和可解释性，为传统基于质心的验证标准提供了稳健的替代方案。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为现代高维应用中的非参数聚类提供了理论基础充分、计算效率高的停止规则。&lt;h4&gt;翻译&lt;/h4&gt;确定无监督学习中适当的聚类数量是统计学和数据科学中的核心问题。传统的有效性指标如Calinski-Harabasz、Silhouette和Davies-Bouldin依赖于基于质心的距离，因此在高维或受污染数据中表现不佳。本文提出了一种新的稳健的非参数聚类验证框架，即高维组内组间距离中位数（HD-BWDM），将最近引入的BWDM标准扩展到高维空间。HD-BWDM整合了随机投影和主成分分析来缓解维度诅咒，并应用修剪聚类和基于medoid的距离以确保对离群点的稳健性。作者推导了理论结果，证明了在Johnson-Lindenstrauss嵌入下的一致性和收敛性。广泛的模拟表明，在高维投影和污染情况下，HD-BWDM保持稳定性和可解释性，为传统的基于质心的验证标准提供了一个稳健的替代方案。所提出的方法为现代高维应用中的非参数聚类提供了理论基础充分、计算效率高的停止规则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Determining the appropriate number of clusters in unsupervised learning is acentral problem in statistics and data science. Traditional validity indicessuch as Calinski-Harabasz, Silhouette, and Davies-Bouldin-depend oncentroid-based distances and therefore degrade in high-dimensional orcontaminated data. This paper proposes a new robust, nonparametric clusteringvalidation framework, the High-Dimensional Between-Within Distance Median(HD-BWDM), which extends the recently introduced BWDM criterion tohigh-dimensional spaces. HD-BWDM integrates random projection and principalcomponent analysis to mitigate the curse of dimensionality and applies trimmedclustering and medoid-based distances to ensure robustness against outliers. Wederive theoretical results showing consistency and convergence underJohnson-Lindenstrauss embeddings. Extensive simulations demonstrate thatHD-BWDM remains stable and interpretable under high-dimensional projections andcontamination, providing a robust alternative to traditional centroid-basedvalidation criteria. The proposed method provides a theoretically grounded,computationally efficient stopping rule for nonparametric clustering in modernhigh-dimensional applications.</description>
      <author>example@mail.com (Mohammed Baragilly, Hend Gabr)</author>
      <guid isPermaLink="false">2510.14145v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.14828v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为RoboGPT-R1的两阶段微调框架，用于提升具身智能体的推理能力，使其能够更好地完成复杂环境中的长时程操作任务。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型和基于监督微调的视觉语言模型在规划任务中取得成功，但在复杂现实环境中执行长时程操作任务时仍面临挑战，原因是它们有限的常识和推理能力。将通用视觉语言模型通过监督微调对齐到机器人规划任务存在泛化能力差和对物理理解不足的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，提升具身智能体在复杂环境中的推理和规划能力，特别是完成长时程操作任务的能力。&lt;h4&gt;方法&lt;/h4&gt;提出RoboGPT-R1框架，包含两个阶段：首先通过监督训练使用专家序列获取基础知识，然后利用强化学习解决模型在视觉空间理解和推理方面的不足。同时设计了基于规则的奖励函数，考虑长时程性能和环境动作约束，并在Qwen2.5-VL-3B上训练推理模型。&lt;h4&gt;主要发现&lt;/h4&gt;在EmbodiedBench基准测试上，训练的推理模型显著优于更大规模的GPT-4o-mini模型，性能高出21.33%；同时超越了在Qwen2.5-VL-7B上训练的其他工作，高出20.33%。&lt;h4&gt;结论&lt;/h4&gt;RoboGPT-R1框架有效提升了具身智能体的推理能力和规划能力，使其能够更好地完成复杂环境中的长时程操作任务。&lt;h4&gt;翻译&lt;/h4&gt;提升具身智能体的推理能力对于机器人在长时程操作任务中成功完成复杂的人类指令至关重要。尽管基于监督微调的大型语言模型和视觉语言模型在规划任务中取得了成功，但由于常识和推理能力的限制，它们在复杂现实环境中执行长时程操作任务时仍面临挑战。考虑到通过监督微调将通用视觉语言模型对齐到机器人规划任务存在泛化能力差和物理理解不足的问题，我们提出了RoboGPT-R1，这是一个用于具身规划的两阶段微调框架。在该框架中，监督训练通过专家序列获取基础知识，随后使用强化学习解决模型在视觉空间理解和推理方面的不足。为了在多步推理任务中实现物理理解和动作序列一致性，我们设计了一个基于规则的奖励函数，同时考虑长时程性能和环境中的动作约束。在Qwen2.5-VL-3B上训练的推理模型在EmbodiedBench基准测试上显著优于更大规模的GPT-4o-mini模型，高出21.33%，并超越了在Qwen2.5-VL-7B上训练的其他工作，高出20.33%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决提升具身智能体在复杂长视野操作任务中的推理能力问题。当前基于监督微调的大语言模型在真实世界环境中执行长期任务时面临泛化能力不足和物理理解有限的问题。这一问题在现实中非常重要，因为机器人需要处理如'打扫厨房'或'准备晚餐'等复杂、长期的指令，而现有方法难以在动态环境中有效适应和自我纠正。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有SFT-only范式的局限性，包括缺乏环境适应能力和奖励函数设计不足。他们借鉴了强化学习在其他领域（如视频推理、数学推理）的成功应用，以及DeepSeek-R1中的'aha moment'概念。具体设计上，作者结合了REBP项目中的数据集和GRPO算法，同时创新性地设计了包含格式奖励和LCS准确奖励的奖励函数，以解决多步推理任务中的物理理解和动作序列一致性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两阶段训练框架结合监督微调和强化学习的优势，并设计针对具身任务的奖励函数。整体流程包括：1)数据准备阶段，使用从Gemini-2.0-flash提炼的SFT数据集和增强的RFT数据集；2)两阶段训练，第一阶段SFT赋予模型基础规划能力，第二阶段使用GRPO进行强化微调提升推理和泛化能力；3)奖励设计，结合格式奖励(评估结构完整性和动作有效性)和LCS准确奖励(关注动作序列顺序)；4)在EmbodiedBench基准上评估性能，包括域内(EB-ALFRED)和域外(EB-Habitat)场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)两阶段训练框架，结合SFT和GRPO强化学习；2)创新的奖励函数设计，包含格式奖励和LCS准确奖励；3)仅使用3B参数的小型模型实现高性能；4)采用零样本处理提高训练效率和泛化能力。相比之前的工作，RoboGPT-R1在EB-ALFRED基准上比GPT-4o-mini高21.33%，比其他基于Qwen2.5-VL-7B的工作高20.33%，特别是在长视野任务上达到50%的准确率，显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RoboGPT-R1通过结合监督微调和强化学习的两阶段训练框架，以及针对具身任务设计的基于规则的奖励函数，显著提升了小型视觉语言模型在复杂长视野机器人规划任务中的性能和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Improving the reasoning capabilities of embodied agents is crucial for robotsto complete complex human instructions in long-view manipulation taskssuccessfully. Despite the success of large language models and vision languagemodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continuefacing challenges in performing long-horizon manipulation tasks in complexreal-world environments, owing to their restricted common sense and reasoningcapabilities. Considering that aligning general-purpose vision language modelsto robotic planning tasks via supervised fine-tuning suffers from poorgeneralization and insufficient physical understanding, we propose RoboGPT-R1,a two-stage fine-tuning framework for embodied planning. In this framework,supervised training acquires foundational knowledge through expert sequences,followed by RL to address the model's shortcomings in visual-spatialunderstanding and reasoning. To achieve physical understanding and actionsequence consistency in multi-step reasoning tasks, we design a rule-basedreward function that simultaneously considers long-horizon performance andaction constraint in the environment. The reasoning model, trained onQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on theEmbodiedBench benchmark.</description>
      <author>example@mail.com (Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li)</author>
      <guid isPermaLink="false">2510.14828v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Spatially anchored Tactile Awareness for Robust Dexterous Manipulation</title>
      <link>http://arxiv.org/abs/2510.14647v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为SaTA的空间锚定触觉感知方法，用于解决灵巧操作中的高精度几何推理问题。该方法通过将触觉特征锚定到手部运动学框架，实现了无需物体模型或显式姿态估计的精确几何推理。&lt;h4&gt;背景&lt;/h4&gt;灵巧操作需要精确的几何推理，但现有的视觉-触觉学习方法在处理亚毫米精度任务时存在困难，而传统基于模型的方法可以轻松处理这些任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用触觉信号的感知丰富性及其与手部运动学空间关系的框架，以实现高精度的灵巧操作。&lt;h4&gt;方法&lt;/h4&gt;提出了SaTA（Spatially-anchored Tactile Awareness for dexterous manipulation）框架，一种端到端策略框架，通过正向运动学将触觉特征锚定到手部运动学框架中。&lt;h4&gt;主要发现&lt;/h4&gt;空间锚定的触觉表示使策略不仅能够检测接触发生，还能在手部坐标系中精确推断物体几何形状。SaTA在多个基准测试中显著优于强视觉-触觉基线，成功率提高高达30个百分点，任务完成时间减少27%。&lt;h4&gt;结论&lt;/h4&gt;SaTA通过将触觉特征锚定到手部运动学框架，成功解决了现有学习框架在处理高精度灵巧操作任务时的局限性，实现了无需物体模型或显式姿态估计的精确几何推理。&lt;h4&gt;翻译&lt;/h4&gt;灵巧操作需要精确的几何推理，然而现有的视觉-触觉学习方法在处理亚毫米精度任务时存在困难，而这些任务对于传统基于模型的方法来说则是常规操作。我们确定了一个关键限制：虽然触觉传感器提供了丰富的接触信息，但现有学习框架未能有效利用触觉信号的感知丰富性及其与手部运动学的空间关系。我们认为理想的触觉表示应将接触测量明确地锚定在稳定的参考框架中，同时保留详细的感官信息，使策略不仅能够检测接触发生，还能在手部坐标系中精确推断物体几何形状。我们引入了SaTA（用于灵巧操作的空间锚定触觉感知），一种端到端策略框架，通过正向运动学将触觉特征明确锚定到手部运动学框架，无需物体模型或显式姿态估计即可实现准确的几何推理。我们的关键见解是空间锚定的触觉表示使策略不仅能够检测接触发生，还能在手部坐标系中精确推断物体几何形状。我们在具有挑战性的灵巧操作任务上验证了SaTA，包括自由空间中的双臂USB-C连接（需要亚毫米级对齐精度）、需要精确螺纹啮合和旋转控制的灯泡安装，以及需要精细力调制和角度精度的卡片滑动。这些任务由于其严格的精度要求，对基于学习的方法构成了重大挑战。在多个基准测试中，SaTA显著优于强视觉-触觉基线，成功率提高高达30个百分点，同时任务完成时间减少27%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决灵巧操作中如何有效利用触觉信号进行精确几何推理的问题，特别是在需要亚毫米级精度的任务中。这个问题很重要，因为在多指多接触场景中，毫米级误差就可能导致任务失败（如USB连接器无法插入），而在接触关键时刻，视觉信息常被遮挡或失效，精确的几何信息对成功操作至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出触觉传感器虽提供丰富信息但现有学习框架未能有效利用其感知丰富性和空间关系这一关键限制。他们认为理想的触觉表示应将接触测量稳定在参考框架中，同时保留详细感官信息。设计方法借鉴了ACT框架作为基础架构，使用FiLM机制整合空间上下文与触觉特征，应用Fourier特征编码捕获多尺度几何变化，并采用模仿学习策略使用专家演示数据进行训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将触觉特征锚定到手部运动学坐标系中，同时保留完整几何信息，使策略能准确推断接触状态和物体几何形状，直接输出精确操作动作。整体流程是：接收多模态输入（RGB图像、触觉图像、关节角度）；通过正向运动学计算触觉传感器6D姿态；用Fourier特征编码空间信息；通过FiLM整合空间上下文与触觉特征；将多模态信息通过Transformer处理；生成动作序列实现亚毫米精度操作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：空间锚定触觉表示方法；端到端操作策略框架SaTA；高精度灵巧操作任务的成功验证。相比之前工作，SaTA将触觉测量显式锚定到手部坐标系而非处理为抽象特征；保留了完整触觉图像特征而非转换为简化几何形式；直接输出操作动作而非专注于感知重建；不依赖显式物体模型或离线优化过程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SaTA通过空间锚定触觉表示，使基于学习的方法能够实现亚毫米级精度的灵巧操作，成功解决了传统视觉-触觉学习方法在需要高精度几何推理任务中的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dexterous manipulation requires precise geometric reasoning, yet existingvisuo-tactile learning methods struggle with sub-millimeter precision tasksthat are routine for traditional model-based approaches. We identify a keylimitation: while tactile sensors provide rich contact information, currentlearning frameworks fail to effectively leverage both the perceptual richnessof tactile signals and their spatial relationship with hand kinematics. Webelieve an ideal tactile representation should explicitly ground contactmeasurements in a stable reference frame while preserving detailed sensoryinformation, enabling policies to not only detect contact occurrence but alsoprecisely infer object geometry in the hand's coordinate system. We introduceSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), anend-to-end policy framework that explicitly anchors tactile features to thehand's kinematic frame through forward kinematics, enabling accurate geometricreasoning without requiring object models or explicit pose estimation. Our keyinsight is that spatially grounded tactile representations allow policies tonot only detect contact occurrence but also precisely infer object geometry inthe hand's coordinate system. We validate SaTA on challenging dexterousmanipulation tasks, including bimanual USB-C mating in free space, a taskdemanding sub-millimeter alignment precision, as well as light bulbinstallation requiring precise thread engagement and rotational control, andcard sliding that demands delicate force modulation and angular precision.These tasks represent significant challenges for learning-based methods due totheir stringent precision requirements. Across multiple benchmarks, SaTAsignificantly outperforms strong visuo-tactile baselines, improving successrates by up to 30 percentage while reducing task completion times by 27percentage.</description>
      <author>example@mail.com (Jialei Huang, Yang Ye, Yuanqing Gong, Xuezhou Zhu, Yang Gao, Kaifeng Zhang)</author>
      <guid isPermaLink="false">2510.14647v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps</title>
      <link>http://arxiv.org/abs/2510.14546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICRA 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用视觉语言模型嵌入表示机器人地图语义的方法，通过自然语言同义词和反义词来训练分类器，解决机器人确定环境中与查询相关部分的挑战。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型的嵌入表示被越来越多地用于表示机器人地图中的语义，提供开放词汇的场景理解，超越了传统有限标签的表示方法。&lt;h4&gt;目的&lt;/h4&gt;解决机器人确定环境中与查询相关部分的关键挑战，提高地图和图像的查询能力。&lt;h4&gt;方法&lt;/h4&gt;利用嵌入空间中与查询相关的自然语言同义词和反义词，应用启发式方法估计与查询相关的语言空间，并使用该语言空间训练分类器来将环境划分为匹配和不匹配的部分。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验表明，该方法能够显著提高地图和图像的查询能力，且该查询技术与表示和编码器无关，只需要有限的训练。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效解决了机器人确定环境中与查询相关部分的挑战，提高了地图和图像的查询能力，具有广泛的适用性。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型的嵌入表示越来越多地被用于表示机器人地图中的语义，提供开放词汇的场景理解，超越了传统的有限标签。嵌入表示通过相似度比较将嵌入的用户文本提示与地图嵌入，实现按需查询。执行查询任务的关键挑战是机器人必须确定环境中与查询相关的部分。本文提出了这一挑战的解决方案。我们利用嵌入空间中与查询相关的自然语言同义词和反义词，应用启发式方法估计与查询相关的语言空间，并使用该语言空间训练分类器将环境划分为匹配和不匹配的部分。我们通过大量实验评估了该方法，包括对地图和标准图像基准的查询。结果表明地图和图像的查询能力得到了提高。我们的查询技术与表示和编码器无关，只需要有限的训练。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是如何更有效地从视觉-语言模型(VLM)的嵌入表示中查询相关信息的问题。具体来说，当机器人需要根据自然语言查询在地图或图像中找到相关物体或区域时，现有方法无法准确确定环境中与查询相关的部分。这个问题很重要，因为随着视觉-语言模型的发展，机器人地图能够包含更丰富的语义信息，开放词汇的场景理解能力对机器人执行复杂任务至关重要，而现有方法在匹配查询与地图嵌入时性能有限，限制了机器人对环境的理解和交互能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：现有方法主要采用阈值化余弦相似度或使用单个互补查询（如'other'）的策略，但这些方法假设所有维度对查询的重要性相同，且仅使用单个查询和单个负例无法准确估计相关区域的范围。基于这些分析，作者设计了QuASH方法，利用自然语言同义词和反义词来估计与查询相关的语言空间，通过启发式方法生成语义相关的同义词和反义词，并基于这些样本训练一个分类器。该方法借鉴了现有工作中的视觉-语言嵌入表示和查询机制，但改进了查询策略，不再依赖简单的阈值或单个负例比较。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用自然语言语义知识，通过生成查询的同义词和反义词样本来训练一个分类器，从而更准确地确定嵌入空间中与查询相关的区域。整体实现流程包括：1) 给定文本查询，生成一组语义同义词和反义词，并添加通用的负例查询；2) 使用嵌入函数将所有文本转换为嵌入表示；3) 使用这些嵌入表示作为训练数据，训练一个分类器；4) 给定一个地图，使用训练好的分类器对地图进行分类，得到与查询匹配的区域。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了一种新的查询形式化方法，将查询过程视为在嵌入空间中的分类问题；2) 设计了QuASH方法，利用自然语言启发式方法生成同义词和反义词样本来训练分类器；3) 该方法不依赖于特定的嵌入表示或编码器，具有通用性；4) 通过非线性分类器而非简单的相似度阈值或线性分割来估计相关区域。相比之前的工作，不同之处在于不再依赖单一的查询嵌入和单一的负例嵌入进行比较，不使用固定的相似度阈值，而是通过训练的分类器动态确定决策边界，考虑了嵌入空间中不同维度可能具有不同语义重要性的事实，方法更加灵活，可以适应不同的视觉-语言模型和编码器。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; QuASH通过利用自然语言启发式方法生成同义词和反义词样本来训练分类器，显著提高了机器人地图和图像中基于自然语言查询的准确性，同时保持了方法的通用性和灵活性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embeddings from Visual-Language Models are increasingly utilized to representsemantics in robotic maps, offering an open-vocabulary scene understanding thatsurpasses traditional, limited labels. Embeddings enable on-demand querying bycomparing embedded user text prompts to map embeddings via a similarity metric.The key challenge in performing the task indicated in a query is that the robotmust determine the parts of the environment relevant to the query.  This paper proposes a solution to this challenge. We leveragenatural-language synonyms and antonyms associated with the query within theembedding space, applying heuristics to estimate the language space relevant tothe query, and use that to train a classifier to partition the environment intomatches and non-matches. We evaluate our method through extensive experiments,querying both maps and standard image benchmarks. The results demonstrateincreased queryability of maps and images. Our querying technique is agnosticto the representation and encoder used, and requires limited training.</description>
      <author>example@mail.com (Matti Pekkanen, Francesco Verdoja, Ville Kyrki)</author>
      <guid isPermaLink="false">2510.14546v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Preference Rewarding for MLLMs Spatial Understanding</title>
      <link>http://arxiv.org/abs/2510.14374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出SPR（空间偏好奖励）方法，通过奖励多模态大语言模型生成具有精确物体定位的详细响应，增强其细粒度空间理解能力，实验证明该方法有效且训练开销小。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型已展现出空间理解能力，但在细粒度空间感知方面仍有不足，如无法生成详细区域描述或准确定位物体，且常无法满足用户对细粒度空间理解的需求。&lt;h4&gt;目的&lt;/h4&gt;解决现有MLLM方法缺乏对实际响应直接监督的问题，通过SPR方法提升MLLM的细粒度空间理解能力。&lt;h4&gt;方法&lt;/h4&gt;SPR方法通过随机选择图像区域和描述，引入语义和定位分数评估MLLM生成描述的质量；使用高定位精度描述完善MLLM输出，并将最佳完善与初始最低分描述配对进行直接偏好优化，增强与视觉输入的细粒度对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在标准引用和定位基准上的大量实验表明，SPR有效提高了MLLM的空间理解能力，同时训练开销最小。&lt;h4&gt;结论&lt;/h4&gt;SPR方法能够显著增强MLLM的细粒度空间理解能力，相关数据和代码将在https://github.com/hanqiu-hq/SPR发布。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型已展现出有希望的空间理解能力，如引用和定位物体描述。尽管取得了成功，MLLMs在细粒度空间感知能力方面仍有不足，例如生成详细的区域描述或准确定位物体。此外，它们经常无法响应用户对所需细粒度空间理解的要求。这个问题可能是因为现有方法主要专注于调整MLLMs以建模预标注的指令数据来注入空间知识，而没有直接监督MLLMs的实际响应。我们通过SPR（空间偏好奖励）方法解决这个问题，通过奖励MLLMs具有精确物体定位的详细响应，而不是模糊或不准确的响应，从而增强MLLMs的空间能力。使用从MLLMs中随机选择的图像区域和区域描述，SPR引入语义和定位分数来全面评估MLLM生成描述中的文本质量和定位质量。我们还使用更好的定位精度来完善MLLM描述，并将得分最高的完善与初始得分最低的描述配对，用于直接偏好优化，从而增强与视觉输入的细粒度对齐。在标准引用和基准测试上的大量实验表明，SPR有效地提高了MLLM的空间理解能力，同时训练开销最小。数据和代码将在https://github.com/hanqiu-hq/SPR发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models~(MLLMs) have demonstrated promising spatialunderstanding capabilities, such as referencing and grounding objectdescriptions. Despite their successes, MLLMs still fall short in fine-grainedspatial perception abilities, such as generating detailed region descriptionsor accurately localizing objects. Additionally, they often fail to respond tothe user's requirements for desired fine-grained spatial understanding. Thisissue might arise because existing approaches primarily focus on tuning MLLMsto model pre-annotated instruction data to inject spatial knowledge, withoutdirect supervision of MLLMs' actual responses. We address this issue by SPR, aSpatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatialcapabilities by rewarding MLLMs' detailed responses with precise objectlocalization over vague or inaccurate responses. With randomly selected imageregions and region descriptions from MLLMs, SPR introduces semantic andlocalization scores to comprehensively evaluate the text quality andlocalization quality in MLLM-generated descriptions. We also refine the MLLMdescriptions with better localization accuracy and pair the best-scoredrefinement with the initial descriptions of the lowest score for directpreference optimization, thereby enhancing fine-grained alignment with visualinput. Extensive experiments over standard referring and grounding benchmarksshow that SPR improves MLLM spatial understanding capabilities effectively withminimal overhead in training. Data and code will be released athttps://github.com/hanqiu-hq/SPR</description>
      <author>example@mail.com (Han Qiu, Peng Gao, Lewei Lu, Xiaoqin Zhang, Ling Shao, Shijian Lu)</author>
      <guid isPermaLink="false">2510.14374v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation</title>
      <link>http://arxiv.org/abs/2510.14357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SUM-AgriVLN方法，通过空间理解记忆模块改进农业视觉语言导航，解决了现有方法忽略过去经验提供空间上下文的问题，在A2A基准测试上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;农业机器人正在成为各种农业任务的有力助手，但仍严重依赖人工操作或固定轨道系统进行移动。AgriVLN方法和A2A基准率先将视觉语言导航扩展到农业领域，使机器人能够遵循自然语言指令导航到目标位置。&lt;h4&gt;目的&lt;/h4&gt;解决现有AgriVLN方法将每个导航指令视为独立片段而忽略过去经验提供空间上下文的问题，特别是在农业场景中经常出现重复导航指令的情况下。&lt;h4&gt;方法&lt;/h4&gt;提出空间理解记忆用于农业视觉语言导航(SUM-AgriVLN)方法，其中SUM模块利用空间理解并通过三维重建和表示保存空间记忆。&lt;h4&gt;主要发现&lt;/h4&gt;在A2A基准测试上，SUM-AgriVLN成功将成功率从0.47提高到0.54，导航误差仅从2.91米略微增加到2.93米，展示了在农业领域最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;SUM-AgriVLN方法有效利用了空间记忆来改进农业视觉语言导航性能，证明了在农业机器人导航中考虑历史经验的重要性。&lt;h4&gt;翻译&lt;/h4&gt;农业机器人正在成为各种农业任务的有力助手，然而，它们仍然严重依赖人工操作或固定轨道系统进行移动。AgriVLN方法和A2A基准率先将视觉语言导航扩展到农业领域，使机器人能够遵循自然语言指令导航到目标位置。在实际农业场景中，导航指令经常重复出现，但AgriVLN将每个指令视为独立片段，忽略了过去经验为后续指令提供空间上下文的潜力。为了弥合这一差距，我们提出了用于农业视觉语言导航的空间理解记忆方法，其中SUM模块利用空间理解并通过三维重建和表示保存空间记忆。在A2A基准测试上评估时，我们的SUM-AgriVLN成功将成功率从0.47提高到0.54，导航误差仅从2.91米略微增加到2.93米，展示了在农业领域最先进的性能。代码：https://github.com/AlexTraveling/SUM-AgriVLN。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决农业机器人在视觉语言导航任务中缺乏空间长期记忆的问题。这个问题很重要，因为实际农业场景中经常需要重复执行相似导航指令，而现有方法将每个指令视为独立事件，无法利用过去经验提供的空间上下文，导致机器人导航效率低下。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到人类导航时会自发形成空间记忆并在后续任务中利用，而现有农业机器人缺乏这种能力。他们受日常生活中第一次和第二次去陌生地方的差异启发，设计出空间理解记忆模块。该方法借鉴了VGGT视觉编码器用于3D重建，参考了结构运动和多视图立体等3D重建技术，并在AgriVLN基础上进行了改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入空间理解记忆模块，使机器人能够保存和利用空间记忆。整体流程包括：1)空间理解：从相机图像集中采样10帧，用VGGT生成3D重建；2)空间记忆：将3D重建渲染为点云，提取正面和倾斜两种视角的2D RGB表示并存储；3)基础模型集成：将SUM模块融入AgriVLN，在每一步加载空间记忆，结合语言指令和视觉输入预测行动；4)任务执行：持续更新子任务列表直到满足结束条件。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将空间记忆引入农业视觉语言导航；2)通过3D重建和表示保存空间记忆，而非传统基于图的方法；3)提取多视角空间表示提供丰富上下文；4)能够利用任务间经验。相比之前工作，SUM-AgriVLN不同于传统VLN方法专注于农业场景，不同于AgriVLN将任务视为独立事件，不同于现有空间记忆方法依赖图结构，也不同于传统3D重建只关注几何准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的SUM-AgriVLN方法通过引入空间理解记忆模块，使农业机器人能够保存和利用空间记忆，显著提高了在重复导航任务中的成功率，从0.47提升到0.54。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Agricultural robots are emerging as powerful assistants across a wide rangeof agricultural tasks, nevertheless, still heavily rely on manual operation orfixed rail systems for movement. The AgriVLN method and the A2A benchmarkpioneeringly extend Vision-and-Language Navigation (VLN) to the agriculturaldomain, enabling robots to navigate to the target positions following thenatural language instructions. In practical agricultural scenarios, navigationinstructions often repeatedly occur, yet AgriVLN treat each instruction as anindependent episode, overlooking the potential of past experiences to providespatial context for subsequent ones. To bridge this gap, we propose the methodof Spatial Understanding Memory for Agricultural Vision-and-Language Navigation(SUM-AgriVLN), in which the SUM module employs spatial understanding and savespatial memory through 3D reconstruction and representation. When evaluated onthe A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47to 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,demonstrating the state-of-the-art performance in the agricultural domain.Code: https://github.com/AlexTraveling/SUM-AgriVLN.</description>
      <author>example@mail.com (Xiaobei Zhao, Xingqi Lyu, Xiang Li)</author>
      <guid isPermaLink="false">2510.14357v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration</title>
      <link>http://arxiv.org/abs/2510.14354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, accepted at ICRA 2024 (International Conference on Robotics  and Automation)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用未标记RGB-D数据进行场景几何推理的新方法，通过循环一致的关键点和结合GRU循环单元的姿态模块，提高了RGB-D配准的准确性。&lt;h4&gt;背景&lt;/h4&gt;随着消费级深度相机的普及，大量未标记的RGB-D数据变得可用，如何有效利用这些数据进行场景几何推理成为一个重要问题。&lt;h4&gt;目的&lt;/h4&gt;探索如何利用未标记的RGB-D数据进行场景的几何推理，提高RGB-D配准的准确性。&lt;h4&gt;方法&lt;/h4&gt;不同于传统的基于几何和特征相似性的RGB-D配准方法，作者使用循环一致的关键点作为显著点强制执行空间一致性约束，并引入结合GRU循环单元和变换同步的姿态模块来融合历史和多视图数据。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNet和3DMatch数据集上，该方法超越了之前的自监督配准方法，甚至优于一些旧的监督方法；将组件集成到现有方法中也证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;通过创新的循环一致关键点和姿态模块设计，有效提高了RGB-D配准的准确性，为未标记RGB-D数据的利用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;随着消费级深度相机的兴起，大量未标记的RGB-D数据变得可用。这引发了一个问题：如何利用这些数据进行场景的几何推理。虽然许多RGB-D配准方法依赖于几何和基于特征的相似性，我们采取了不同的方法。我们使用循环一致的关键点作为显著点，在匹配过程中强制执行空间一致性约束，提高对应点准确性。此外，我们引入了一个新的姿态模块，将GRU循环单元与变换同步相结合，融合历史和多视图数据。我们的方法在ScanNet和3DMatch上超越了之前的自监督配准方法，甚至优于一些旧的监督方法。我们还将我们的组件集成到现有方法中，证明了它们的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用大量无标签的RGB-D数据进行场景几何推理，特别是RGB-D配准问题。这个问题很重要，因为随着消费级深度相机的普及，有大量无标签RGB-D数据可用，而RGB-D数据在机器人任务(如SLAM、无人机导航和物体姿态估计)中非常关键。传统配准方法在有噪声或特征稀少环境下表现不佳，且现有自监督方法未充分利用场景中的显著点信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考利用场景中的显著点作为锚点，这些点在多视角下易被识别且循环一致。通过空间一致性约束改善对应关系搜索，并结合GRU循环单元和变换同步融合历史和多视图信息。作者借鉴了多项现有工作：使用ResNet-18作为特征提取网络，采用LofTr的匹配策略，利用Sinkhorn归一化，参考矩阵分解方法获得循环一致匹配，受启发于GRU单元和变换同步方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用循环一致的显著点(锚点)施加空间一致性约束提高对应准确性，结合GRU和变换同步进行姿态估计。整体流程：1)使用ResNet-18提取特征；2)通过Sinkhorn归一化获得软匹配并转换为循环一致的锚点；3)使用锚点距离编码修改自注意力模块；4)定义空间一致性成本函数；5)迭代进行像素级匹配和姿态更新；6)结合GRU和变换同步改进姿态估计；7)通过内部迭代(20次)和外部迭代(3次)优化结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)循环一致关键点匹配模块，施加空间约束；2)无RANSAC的姿态估计方法，结合GRU和变换同步；3)空间一致性成本函数；4)迭代优化框架。不同之处：大多数自监督方法依赖特征相似性或几何信息，而本文利用场景显著点；之前循环一致性方法应用于所有像素，本文仅用于定位显著点；与[24]不同，本文用空间约束学习锚点而非修剪离群值；结合GRU和变换同步，而非仅使用一种方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过循环一致锚点和空间一致性约束提出新自监督RGB-D配准方法，显著提高配准精度，超越之前自监督方法并接近一些有监督方法的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICRA57147.2024.10610738&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data hasbecome available. This prompts the question of how to utilize this data forgeometric reasoning of scenes. While many RGB-D registration meth- ods rely ongeometric and feature-based similarity, we take a different approach. We usecycle-consistent keypoints as salient points to enforce spatial coherenceconstraints during matching, improving correspondence accuracy. Additionally,we introduce a novel pose block that combines a GRU recurrent unit withtransformation synchronization, blending historical and multi-view data. Ourapproach surpasses previous self- supervised registration methods on ScanNetand 3DMatch, even outperforming some older supervised methods. We alsointegrate our components into existing methods, showing their effectiveness.</description>
      <author>example@mail.com (Siddharth Tourani, Jayaram Reddy, Sarvesh Thakur, K Madhava Krishna, Muhammad Haris Khan, N Dinesh Reddy)</author>
      <guid isPermaLink="false">2510.14354v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.13993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures, 8 tables. To be published in Applied AI Letters&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了结合传统视觉模型与视觉语言模型(VLMs)以增强遥感图像分析，特别是在飞机检测和场景理解方面的应用。通过集成YOLO与LLaVA、ChatGPT和Gemini等VLMs，实现了更准确和具有上下文意识的图像解释。&lt;h4&gt;背景&lt;/h4&gt;遥感已成为城市规划、环境监测和灾害响应等领域的关键工具，数据量显著增加。然而，传统视觉模型受限于需要大量领域特定标记数据且在理解复杂环境上下文方面能力有限。视觉语言模型虽能整合视觉和文本数据，但在遥感领域的应用尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究视觉模型与VLMs的结合，以增强遥感图像分析，专注于飞机检测和场景理解任务。&lt;h4&gt;方法&lt;/h4&gt;集成YOLO与VLMs(如LLaVA、ChatGPT和Gemini)，在标记和未标记的遥感数据以及退化图像场景上评估性能，旨在实现更准确和具有上下文意识的图像解释。&lt;h4&gt;主要发现&lt;/h4&gt;在原始和退化场景中，特别是在具有挑战性的条件下，飞机检测和计数的准确性平均提高了48.46%。在遥感图像的全面理解方面，CLIPScore提高了6.17%。&lt;h4&gt;结论&lt;/h4&gt;结合传统视觉模型和VLMs的方法为更先进和高效的遥感图像分析铺平了道路，特别在少样本学习场景中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;遥感已成为城市规划、环境监测和灾害响应等跨领域的关键工具。尽管生成的数据量显著增加，但传统视觉模型通常受限于需要大量领域特定标记数据及其在理解复杂环境中上下文能力的有限性。视觉语言模型通过整合视觉和文本数据提供了一种互补方法；然而，它们在遥感领域的应用仍未得到充分探索，特别是考虑到它们的通用性质。本研究探讨了结合视觉模型和VLMs以增强遥感图像分析，专注于飞机检测和场景理解。将YOLO与LLaVA、ChatGPT和Gemini等VLMs的集成旨在实现更准确和具有上下文意识的图像解释。性能在标记和未标记的遥感数据以及退化图像场景上进行了评估，这些场景对遥感至关重要。研究显示，在原始和退化场景中，特别是在具有挑战性的条件下，飞机检测和计数的准确性在各类模型中平均提高了48.46%。在遥感图像的全面理解方面，获得了6.17%的CLIPScore提升。结合传统视觉模型和VLMs的方法为更先进和高效的遥感图像分析铺平了道路，特别是在少样本学习场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing has become a vital tool across sectors such as urban planning,environmental monitoring, and disaster response. While the volume of datagenerated has increased significantly, traditional vision models are oftenconstrained by the requirement for extensive domain-specific labelled data andtheir limited ability to understand the context within complex environments.Vision Language Models offer a complementary approach by integrating visual andtextual data; however, their application to remote sensing remainsunderexplored, particularly given their generalist nature. This workinvestigates the combination of vision models and VLMs to enhance imageanalysis in remote sensing, with a focus on aircraft detection and sceneunderstanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, andGemini aims to achieve more accurate and contextually aware imageinterpretation. Performance is evaluated on both labelled and unlabelled remotesensing data, as well as degraded image scenarios which are crucial for remotesensing. The findings show an average MAE improvement of 48.46% across modelsin the accuracy of aircraft detection and counting, especially in challengingconditions, in both raw and degraded scenarios. A 6.17% improvement inCLIPScore for comprehensive understanding of remote sensing images is obtained.The proposed approach combining traditional vision models and VLMs paves theway for more advanced and efficient remote sensing image analysis, especiallyin few-shot learning scenarios.</description>
      <author>example@mail.com (Jia Yun Chua, Argyrios Zolotas, Miguel Arana-Catania)</author>
      <guid isPermaLink="false">2510.13993v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2510.07944v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CVD-STORM是一个跨视图视频扩散模型，利用空间-时间重建变分自编码器生成长期多视图视频并具备4D重建能力。&lt;h4&gt;背景&lt;/h4&gt;生成模型已被广泛应用于世界建模和环境模拟、未来状态预测。随着自动驾驶的发展，对高质量视频生成以及深度估计等多样化有意义信息的需求日益增长。&lt;h4&gt;目的&lt;/h4&gt;提出CVD-STORM模型，能够在各种控制输入下生成长期多视图视频，具备4D重建能力。&lt;h4&gt;方法&lt;/h4&gt;首先使用辅助的4D重建任务对VAE进行微调，增强其编码3D结构和时间动态的能力；然后将这个VAE集成到视频扩散过程中提高生成质量；联合训练的高斯溅射解码器有效重建动态场景，为场景理解提供几何信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该模型在FID和FVD指标上都取得了显著改进。&lt;h4&gt;结论&lt;/h4&gt;CVD-STORM模型能够在各种控制条件下生成高质量的多视图视频，并有效重建动态场景，为场景理解提供几何信息。&lt;h4&gt;翻译&lt;/h4&gt;生成模型已被广泛应用于世界建模和环境模拟以及未来状态预测。随着自动驾驶的发展，不仅需要高质量的视频生成，还需要产生多样化和有意义的信息如深度估计。为此，我们提出了CVD-STORM，这是一个利用空间-时间重建变分自编码器的跨视图视频扩散模型，能够在各种控制输入下生成具有4D重建能力的长期多视图视频。我们的方法首先使用辅助的4D重建任务对VAE进行微调，增强其编码3D结构和时间动态的能力。随后，我们将这个VAE集成到视频扩散过程中，显著提高了生成质量。实验结果表明，我们的模型在FID和FVD指标上都取得了显著改进。此外，联合训练的高斯溅射解码器有效地重建动态场景，为全面场景理解提供了有价值的几何信息。我们的项目页面是https://sensetime-fvg.github.io/CVD-STORM。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶领域中高质量视频生成和4D场景重建的问题。具体来说，现有方法难以同时生成长期、多视角的视频并提供准确的深度信息，这限制了自动驾驶系统对环境的模拟和未来状态的预测能力。这个问题在现实中非常重要，因为自动驾驶需要准确的环境模拟来训练决策算法和验证规划输出，而深度信息对于理解场景的3D结构至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有视频扩散模型在生成长期、多视角视频方面的局限性，以及缺乏明确3D信息的问题。他们借鉴了多项现有工作：基于现有的扩散模型架构（如DiT），参考了STORM模型的空间-时间重建方法，采用了UniMLVG的多模态DiT架构和训练策略，利用了3D高斯溅射技术进行场景重建，并结合了VAE进行表示学习。作者通过整合这些技术，设计了一个两阶段训练策略：先学习场景重建，再训练条件世界模型，以实现高质量的视频生成和4D场景重建。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过STORM-VAE（一个扩展的VAE模型，集成了高斯溅射解码器）进行4D场景重建，并利用CVD-STORM框架同时生成多视角视频和重建4D场景。整体实现流程分为三部分：1）STORM-VAE训练：使用预训练的图像VAE，添加高斯溅射解码器分支，通过多视图图像和相机姿态进行训练；2）CVD-STORM训练：使用STORM-VAE作为潜在编码器，在扩散模型中集成STORM-VAE，使用三个不同的transformer块处理不同维度；3）推理过程：生成长期六视角视频，高斯溅射解码器直接从生成的潜在表示重建4D场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）STORM-VAE：一个能进行4D场景重建的扩展VAE模型；2）CVD-STORM：统一框架同时生成多视角视频和重建4D场景；3）两阶段训练策略：先学习场景重建，再训练条件世界模型；4）增强的表示学习：通过空间-时间重建模型提高生成质量；5）单阶段训练策略：简化训练过程，降低计算成本。相比之前的工作，CVD-STORM实现了真正的端到端交互（不同于MagicDrive3D的两阶段流水线），提供绝对深度估计（不同于UniFuture和GEM的相对深度），重建过程对生成模型有直接影响，并能同时完成多视角视频生成和4D场景重建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CVD-STORM通过引入STORM-VAE和统一的生成-重建框架，实现了高质量的多视角视频生成和准确的4D场景重建，为自动驾驶提供了更强大的世界模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative models have been widely applied to world modeling for environmentsimulation and future state prediction. With advancements in autonomousdriving, there is a growing demand not only for high-fidelity video generationunder various controls, but also for producing diverse and meaningfulinformation such as depth estimation. To address this, we propose CVD-STORM, across-view video diffusion model utilizing a spatial-temporal reconstructionVariational Autoencoder (VAE) that generates long-term, multi-view videos with4D reconstruction capabilities under various control inputs. Our approach firstfine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing itsability to encode 3D structures and temporal dynamics. Subsequently, weintegrate this VAE into the video diffusion process to significantly improvegeneration quality. Experimental results demonstrate that our model achievessubstantial improvements in both FID and FVD metrics. Additionally, thejointly-trained Gaussian Splatting Decoder effectively reconstructs dynamicscenes, providing valuable geometric information for comprehensive sceneunderstanding. Our project page is https://sensetime-fvg.github.io/CVD-STORM.</description>
      <author>example@mail.com (Tianrui Zhang, Yichen Liu, Zilin Guo, Yuxin Guo, Jingcheng Ni, Chenjing Ding, Dan Xu, Lewei Lu, Zehuan Wu)</author>
      <guid isPermaLink="false">2510.07944v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning</title>
      <link>http://arxiv.org/abs/2510.14672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了VTimeCoT框架，一种无需训练的方法，用于解决多模态大语言模型在视频时序定位和推理方面的缺陷，通过引入进度条视觉工具和跨模态推理过程，实现了显著的性能提升和可解释的推理过程。&lt;h4&gt;背景&lt;/h4&gt;视频问答基于多模态大语言模型近年来受到关注，但这类模型在视频时序定位和推理方面存在明显缺陷，对有效现实世界视频理解系统的发展构成挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一个简单而有效的无需训练框架，用于高性能的视频时序定位和推理，解决现有模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出VTimeCoT框架，包含两个新颖的视觉工具：即插即用的进度条集成工具和高效率高亮工具，同时引入整合视频和文本跨模态推理的视觉时序思考链过程。&lt;h4&gt;主要发现&lt;/h4&gt;在视频时序定位和基于推理的问答任务中，该方法对Qwen2VL-7B和GPT4o基线模型显示出显著的性能改进。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架实现了可组合且可解释的推理过程，有效提升了视频理解系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;近年来，基于多模态大语言模型(MLLM)的视频问答因其受益于LLMs的显著进步而受到广泛关注。然而，这些模型在视频时序定位和推理领域存在明显缺陷，对有效现实世界视频理解系统的发展构成挑战。受人类使用视频播放器与进度条交互以理解视频的启发，我们引入了VTimeCoT，一个简单而有效的无需训练框架，专为高性能视频定位和推理而设计。该框架包含两个新颖的进度条视觉工具：即插即用的进度条集成工具和高效率高亮工具。此外，为解决传统基于文本的思考链(CoT)方法的局限性，我们引入了一个整合视频和文本跨模态推理的视觉时序思考链过程。我们的方法在视频时序定位和基于推理的问答任务中，对Qwen2VL-7B和GPT4o基线模型均显示出显著的性能改进。最后，我们展示了所提出的框架实现了可组合且可解释的推理过程。项目页面：https://vtimecot.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, video question answering based on multimodal large languagemodels (MLLM) has garnered considerable attention, due to the benefits from thesubstantial advancements in LLMs. However, these models have a notabledeficiency in the domains of video temporal grounding and reasoning, posingchallenges to the development of effective real-world video understandingsystems. Inspired by how humans use video players to interact with the progressbar for video comprehension, we introduce VTimeCoT, a simple yet effectivetraining-free framework, designed for high-performance video grounding andreasoning. The proposed framework incorporates two novel visual tools of theprogress bar: a plug-and-play progress bar integration tool and ahigh-efficiency highlighting tool. In addition, to address the limitations ofconventional text-based chain-of-thought (CoT) approaches, we introduce avisuotemporal CoT process that integrates cross-modality reasoning across bothvideo and text. Our approach demonstrates significant performance improvementson both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding andreasoning-based question answering. Finally, we showcase that the proposedframework achieves a compositional and interpretable reasoning process. Projectpage: https://vtimecot.github.io</description>
      <author>example@mail.com (Jinglei Zhang, Yuanfan Guo, Rolandos Alexandros Potamias, Jiankang Deng, Hang Xu, Chao Ma)</author>
      <guid isPermaLink="false">2510.14672v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding</title>
      <link>http://arxiv.org/abs/2510.14032v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 (Spotlight). Webpage at  https://xiaoqian-shen.github.io/Vgent&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Vgent，一种基于图的检索-推理-增强生成框架，用于增强大型视频语言模型对长视频的理解能力。通过结构化图表示视频和引入中间推理步骤，有效解决了长视频处理中的时间依赖性问题和检索噪声问题，在多个基准测试上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;理解和推理长视频对大型视频语言模型(LVLMs)构成重大挑战，主要因为难以处理超出上下文窗口密集的视频token，并保留长期顺序信息。检索增强生成(RAG)虽然对处理长上下文有效，但应用于长视频时面临时间依赖性被打乱和包含无关信息等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架增强LVLMs对长视频的理解能力，解决长视频处理中的时间依赖性问题和检索噪声问题，提高模型在长视频理解任务中的准确性和上下文感知能力。&lt;h4&gt;方法&lt;/h4&gt;提出Vgent框架，包含两个关键创新：(1)使用结构化图表示视频，保留视频片段间的语义关系以提高检索效果；(2)引入中间推理步骤，利用结构化验证减少检索噪声，促进相关信息片段的显式聚合。&lt;h4&gt;主要发现&lt;/h4&gt;在MLVU基准测试上，与基础模型相比，总体性能提升了3.0%~5.4%，并比最先进的视频RAG方法高出8.6%。代码已在https://xiaoqian-shen.github.io/Vgent公开。&lt;h4&gt;结论&lt;/h4&gt;Vgent框架通过结构化图表示和中间推理步骤，有效解决了长视频理解中的关键挑战，显著提升了LVLMs的性能，为长视频理解任务提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;理解和推理长视频对大型视频语言模型(LVLMs)构成重大挑战，因为难以处理超出上下文窗口密集的视频token并保留长期顺序信息。检索增强生成(RAG)在处理大型语言模型(LLMs)的长上下文方面已显示出有效性；然而，将RAG应用于长视频面临时间依赖性被打乱和包含无关信息等挑战，这些都会妨碍准确推理。为解决这些局限性，我们提出了Vgent，一种新颖的基于图的检索-推理-增强生成框架，用于增强LVLMs对长视频的理解能力。我们的方法引入了两个关键创新：(i)它通过保留视频片段间的语义关系，使用结构化图表示视频，以提高检索效果。(ii)它引入中间推理步骤，缓解LVLMs的推理局限性，利用结构化验证减少检索噪声，促进相关信息的显式聚合，从而产生更准确和上下文感知的响应。我们在三个长视频理解基准测试上使用各种开源LVLMs全面评估了我们的框架。与基础模型相比，我们的方法在MLVU上总体性能提升了3.0%~5.4%，并比最先进的视频RAG方法高出8.6%。我们的代码已在https://xiaoqian-shen.github.io/Vgent公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and reasoning over long videos pose significant challenges forlarge video language models (LVLMs) due to the difficulty in processingintensive video tokens beyond context window and retaining long-term sequentialinformation. Retrieval-Augmented Generation (RAG) has demonstratedeffectiveness in processing long context for Large Language Models (LLMs);however, applying RAG to long video faces challenges such as disrupted temporaldependencies and inclusion of irrelevant information that can hinder accuratereasoning. To address these limitations, we propose Vgent, a novel graph-basedretrieval-reasoning-augmented generation framework to enhance LVLMs for longvideo understanding. Our approach introduces two key innovations: (i) Itrepresents videos by structured graphs with semantic relationships across videoclips preserved to improve retrieval effectiveness. (ii) It introduces anintermediate reasoning step to mitigate the reasoning limitation of LVLMs,which leverages structured verification to reduce retrieval noise andfacilitate the explicit aggregation of relevant information across clips,resulting in more accurate and context-aware responses. We comprehensivelyevaluate our framework with various open-source LVLMs on three long-videounderstanding benchmarks. Our approach yielded an overall performanceimprovement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformedstate-of-the-art video RAG methods by $8.6\%$. Our code is publicly availableat https://xiaoqian-shen.github.io/Vgent.</description>
      <author>example@mail.com (Xiaoqian Shen, Wenxuan Zhang, Jun Chen, Mohamed Elhoseiny)</author>
      <guid isPermaLink="false">2510.14032v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding</title>
      <link>http://arxiv.org/abs/2510.13016v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了时空视频动作定位(SVAG)任务，要求模型同时检测、跟踪和基于自然语言描述对视频中的相关对象进行时空定位。研究团队构建了SVAG-Bench基准数据集，提出了SVAGFormer基线框架，并开发了SVAGEVal评估工具。实验表明现有模型在SVAG任务上表现不佳，特别是在密集或复杂场景中。&lt;h4&gt;背景&lt;/h4&gt;细粒度动作理解和准确时空定位是推进下一代AI系统的基本能力，包括具身智能体、自主平台和人机交互框架。尽管视频理解最近有所进展，但现有方法主要解决粗粒度动作识别或通用目标跟踪问题，忽略了根据动作联合检测和跟踪多个对象并进行时空定位的挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在联合检测、跟踪和时空定位视频中的相关对象方面的不足，推进细粒度动作理解和对象-动作交互的推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出了时空视频动作定位(SVAG)任务，构建了SVAG-Bench大型基准数据集（包含688个视频、19,590条标注记录和903个独特动词），提出了SVAGFormer基线框架（适应最先进的视觉语言模型进行联合时空定位），并开发了SVAGEVal标准化评估工具包。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，现有模型在SVAG任务上表现不佳，特别是在密集或复杂场景中，这凸显了对长视频中细粒度对象-动作交互进行更高级推理的必要性。&lt;h4&gt;结论&lt;/h4&gt;该研究为细粒度视频理解和对象-动作交互建立了新的基准和评估框架，强调了开发能够处理复杂场景和长视频中高级推理能力的模型的重要性。&lt;h4&gt;翻译&lt;/h4&gt;理解细粒度动作并准确定位其在空间和时间中对应的执行者是推进下一代AI系统的基本能力，包括具身智能体、自主平台和人机交互框架。尽管视频理解最近有所进展，但现有方法主要解决粗粒度动作识别或通用目标跟踪问题，因此忽略了根据动作联合检测和跟踪多个对象并进行时空定位的挑战。为解决这一差距，我们引入了时空视频动作定位(SVAG)，这是一个新任务，要求模型基于自然语言描述的动作同时检测、跟踪和时空定位视频中所有相关对象。为支持此任务，我们构建了SVAG-Bench，这是一个大规模基准，包含688个视频、19,590条标注记录和903个独特动词，涵盖了多样化的对象、动作和现实世界场景。我们进一步提出了SVAGFormer，这是一个基线框架，它适应了最先进的视觉语言模型进行联合时空定位，并引入了SVAGEVal，这是一个标准化的评估工具包，用于公平和可复现的基准测试。实验结果表明，现有模型在SVAG上表现不佳，特别是在密集或复杂场景中，这凸显了需要对长视频中细粒度对象-动作交互进行更高级推理的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding fine-grained actions and accurately localizing theircorresponding actors in space and time are fundamental capabilities foradvancing next-generation AI systems, including embodied agents, autonomousplatforms, and human-AI interaction frameworks. Despite recent progress invideo understanding, existing methods predominantly address eithercoarse-grained action recognition or generic object tracking, therebyoverlooking the challenge of jointly detecting and tracking multiple objectsaccording to their actions while grounding them temporally. To address thisgap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel taskthat requires models to simultaneously detect, track, and temporally localizeall referent objects in videos based on natural language descriptions of theiractions. To support this task, we construct SVAG-Bench, a large-scale benchmarkcomprising 688 videos, 19,590 annotated records, and 903 unique verbs, coveringa diverse range of objects, actions, and real-world scenes. We further proposeSVAGFormer, a baseline framework that adapts state of the art vision languagemodels for joint spatial and temporal grounding, and introduce SVAGEval, astandardized evaluation toolkit for fair and reproducible benchmarking.Empirical results show that existing models perform poorly on SVAG,particularly in dense or complex scenes, underscoring the need for moreadvanced reasoning over fine-grained object-action interactions in long videos.</description>
      <author>example@mail.com (Tanveer Hannan, Shuaicong Wu, Mark Weber, Suprosanna Shit, Jindong Gu, Rajat Koner, Aljoša Ošep, Laura Leal-Taixé, Thomas Seidl)</author>
      <guid isPermaLink="false">2510.13016v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding</title>
      <link>http://arxiv.org/abs/2510.13891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态大语言模型在长视频理解方面面临上下文窗口和计算成本限制，现有关键帧选择方法存在信息丢失和场景连续性问题。作者提出K-frames方法，通过预测语义连贯的视频片段而非单个帧，保持时间连续性，支持灵活的多尺度关键帧选择。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在图像理解方面表现出色，但在长视频理解方面受到上下文窗口和计算成本的限制。均匀采样通常会导致大量信息丢失。&lt;h4&gt;目的&lt;/h4&gt;解决现有关键帧选择方法的问题，提出一种能够保持时间连续性的场景驱动关键帧选择方法。&lt;h4&gt;方法&lt;/h4&gt;K-frames方法预测语义连贯、与查询相关的视频片段而非单个帧，支持任意数量的关键帧选择。作者构建了包含20万个基于查询条件的视频亮点的PeakClips数据集，并采用三阶段渐进式课程学习：两个监督微调阶段（时间定位和关键片段感知）和一个强化学习阶段（优化场景驱动的预测策略）。&lt;h4&gt;主要发现&lt;/h4&gt;在主要的长视频理解基准上的大量实验表明，K-frames在各种规模的关键帧选择方面提供了有效、可解释且即插即用的解决方案。&lt;h4&gt;结论&lt;/h4&gt;K-frames方法解决了长视频理解中的关键帧选择问题，作者公开的数据集和模型将为该领域提供支持。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在图像理解方面已展现出显著能力，但在长视频处理中受限于上下文窗口和计算成本。均匀采样常导致大量信息丢失。同时，现有的关键帧选择方法如文本-帧检索或基于强化学习的帧优化通常产生稀疏且时间上不连续的帧，忽略了场景连续性，缺乏多尺度帧选择的灵活性。为解决这些问题，我们引入K-frames，一种保持时间连续性的场景驱动关键帧选择新范式。K-frames不选择单个帧，而是预测语义连贯、与查询相关的片段，支持任意数量的关键帧选择以满足不同用户需求。为实现这一方法，我们首先引入PeakClips数据集，包含20万个基于查询条件的视频亮点。基于此数据集，K-frames使用三阶段渐进式课程学习clip2frame选择，包括两个监督微调阶段（用于时间定位和关键片段感知）和一个强化学习阶段（直接优化场景驱动的预测策略，无需额外注释）。在主要长视频理解基准上的大量实验表明，K-frames为各种规模的关键帧选择提供了有效、可解释且即插即用的解决方案。我们的数据集和模型将会公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have demonstrated significantcapabilities in image understanding, but long-video are constrained by contextwindows and computational cost. Uniform frame sampling often leads tosubstantial information loss. Meanwhile existing keyframe selection methodssuch as text-frame retrieval or RL-based frame optimization typically yieldsparse and temporally disjointed frames, overlooking scene continuity andlacking flexibility for multi-scale frame selection. To address theselimitations, we introduce K-frames, a novel paradigm for scene-driven keyframeselection that preserves temporal continuity. Instead of selecting individualframes, K-frames predicts semantically coherent, query-relevant clips, whichenables any-k keyframes selection to meet diverse user budgets. To achieve thisapproach, we first introduce PeakClips, a dataset of 200K video highlightsconditioned by query. Building on this dataset, K-frames learns clip2frameselection using a three-stage progressive curriculum. It involves twoSupervised Fine-Tuning stages for temporal grounding and key-clip perception,followed by a Reinforcement Learning stage that directly optimizes thescene-driven prediction policy for downstream task without further annotations.Extensive experiments on major long-video understanding benchmarks demonstratethat K-frames provides an effective, interpretable, and plug-and-play solutionfor keyframe selection at various scales. Our dataset and model will beavailable.</description>
      <author>example@mail.com (Yifeng Yao, Yike Yun, Jing Wang, Huishuai Zhang, Dongyan Zhao, Ke Tian, Zhihao Wang, Minghui Qiu, Tao Wang)</author>
      <guid isPermaLink="false">2510.13891v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>ChangingGrounding: 3D Visual Grounding in Changing Scenes</title>
      <link>http://arxiv.org/abs/2510.14965v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了ChangingGrounding基准和Mem-ChangingGrounder方法，用于解决动态场景中3D视觉目标定位问题，通过利用过去观察信息减少探索成本。&lt;h4&gt;背景&lt;/h4&gt;现实世界机器人需要从自然语言指令中定位物体，同时周围场景不断变化。现有3D视觉目标定位方法假设已重建且最新的点云，这导致需要昂贵的重新扫描，阻碍了实际部署。&lt;h4&gt;目的&lt;/h4&gt;将3DVG表述为主动的、内存驱动的问题，引入ChangingGrounding基准来衡量代理如何有效利用过去观察、只在需要处探索，并在变化场景中提供精确3D边界框。&lt;h4&gt;方法&lt;/h4&gt;提出Mem-ChangingGrounder零样本方法，结合跨模态检索与轻量级多视图融合：识别物体类型、检索相关记忆指导动作、高效探索目标、操作无效时回退、多视图扫描目标、融合多视图证据获取准确边界框。&lt;h4&gt;主要发现&lt;/h4&gt;在ChangingGrounding基准上评估不同基线方法，Mem-ChangingGrounder实现最高定位精度，同时显著降低探索成本。&lt;h4&gt;结论&lt;/h4&gt;希望该基准和方法能推动面向实际应用的、以内存为中心的3DVG研究转变。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的机器人从自然语言指令中定位物体，同时周围场景不断变化。然而，大多数现有的3D视觉目标定位方法仍然假设已重建且最新的点云，这种假设迫使昂贵的重新扫描并阻碍部署。我们认为3DVG应表述为主动的、内存驱动的问题，并引入ChangingGrounding，这是第一个明确衡量代理如何有效利用过去观察、只在需要处探索并在变化场景中提供精确3D边界框的基准。为设定强参考点，我们还提出了Mem-ChangingGrounder，这是一种针对此任务的零样本方法，它结合了跨模态检索与轻量级多视图融合：识别查询暗示的物体类型，检索相关记忆指导动作，然后在场景中高效探索目标，在先前操作无效时回退，对目标进行多视图扫描，并将多视图扫描的融合证据投影以获得准确的物体边界框。我们在ChangingGrounding上评估了不同的基线方法，我们的Mem-ChangingGrounder实现了最高的定位精度，同时大大减少了探索成本。我们希望这个基准和方法能够推动面向实际应用的、以内存为中心的3DVG研究转变。项目页面：https://hm123450.github.io/CGB/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D视觉定位在动态变化场景中的挑战。现有方法假设场景静态且拥有完整点云，但真实环境中物体会移动或被遮挡，导致机器人需要频繁重新扫描整个场景，这非常耗时且成本高昂。这个问题在现实中很重要，因为它限制了机器人在动态环境（如家庭、办公室）中的实用性，增加了能耗并降低了效率，而人类却能利用过去记忆快速适应变化环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从人类认知方式获得灵感，人类在动态环境中会利用过去记忆高效定位目标。作者将3D视觉定位重新定义为'主动的、记忆驱动的问题'。他们借鉴了VLM-Grounder的框架（使用2D图像而非点云）、3RScan数据集（提供不同时间点的场景扫描和物体对应关系），以及视觉语言模型和开放词汇检测器等现有技术。设计的Mem-ChangingGrounder方法结合了记忆检索、智能探索和回退策略，以应对场景变化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用机器人对过去场景的记忆来指导在当前变化场景中的高效探索，避免盲目扫描整个场景。整体流程包括：1)查询分类：将查询分为'可验证'（即使目标移动，记忆中的目标仍匹配查询）和'不可验证'（记忆中的目标可能不再匹配）；2)记忆检索与定位：根据查询类型选择策略，使用全景扫描或空间关系感知扫描寻找目标；3)回退策略：当主策略失败时，从记忆检索目标类别并进行360度搜索；4)多视图投影：结合多视图信息生成精确3D边界框。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次定义动态场景中的3D视觉定位任务，强调利用过去记忆；2)提出ChangingGrounding基准数据集，包含267K个参照性描述，评估定位准确性和探索成本；3)设计Mem-ChangingGrounder方法，结合记忆检索和智能探索；4)引入探索成本指标，强调效率。相比之前工作，本文不再假设场景静态，而是设计基于智能体的方法，利用2D图像和记忆避免昂贵的点云重建，同时关注准确性和效率的平衡。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了首个面向动态场景的3D视觉定位基准和方法，通过结合记忆检索和智能探索策略，实现了在变化环境中高效且准确的物体定位。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world robots localize objects from natural-language instructions whilescenes around them keep changing. Yet most of the existing 3D visual grounding(3DVG) method still assumes a reconstructed and up-to-date point cloud, anassumption that forces costly re-scans and hinders deployment. We argue that3DVG should be formulated as an active, memory-driven problem, and we introduceChangingGrounding, the first benchmark that explicitly measures how well anagent can exploit past observations, explore only where needed, and stilldeliver precise 3D boxes in changing scenes. To set a strong reference point,we also propose Mem-ChangingGrounder, a zero-shot method for this task thatmarries cross-modal retrieval with lightweight multi-view fusion: it identifiesthe object type implied by the query, retrieves relevant memories to guideactions, then explores the target efficiently in the scene, falls back whenprevious operations are invalid, performs multi-view scanning of the target,and projects the fused evidence from multi-view scans to get accurate objectbounding boxes. We evaluate different baselines on ChangingGrounding, and ourMem-ChangingGrounder achieves the highest localization accuracy while greatlyreducing exploration cost. We hope this benchmark and method catalyze a shifttoward practical, memory-centric 3DVG research for real-world applications.Project page: https://hm123450.github.io/CGB/ .</description>
      <author>example@mail.com (Miao Hu, Zhiwei Huang, Tai Wang, Jiangmiao Pang, Dahua Lin, Nanning Zheng, Runsen Xu)</author>
      <guid isPermaLink="false">2510.14965v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.14830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://lei-kun.github.io/RL-100/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了RL-100，一个基于扩散视觉运动策略的真实世界强化学习训练框架，通过三阶段流程实现高效可靠的机器人操作，并在多个任务上达到100%成功率。&lt;h4&gt;背景&lt;/h4&gt;家庭和工厂中的真实世界机器人操作需要可靠性、效率和鲁棒性，达到或超越熟练人类操作员的水平。&lt;h4&gt;目的&lt;/h4&gt;开发一个真实世界的强化学习训练框架，实现高效、可靠且通用的机器人操作能力。&lt;h4&gt;方法&lt;/h4&gt;RL-100框架采用三阶段流程：首先通过模仿学习利用人类先验知识；其次使用离线策略评估(OPE)进行迭代离线强化学习；最后通过在线强化学习消除剩余失败模式。此外，添加轻量级一致性蒸馏头将多步采样压缩为单步策略，实现高频控制并降低延迟。&lt;h4&gt;主要发现&lt;/h4&gt;在七个真实机器人任务上评估，包括动态刚体控制、流体倾倒、布料折叠、拧螺丝和橙汁制作等，RL-100实现了900/900的100%成功率，包括连续250次试验全部成功。该方法达到接近人类远程操作或更好的时间效率，并展示了多小时的鲁棒性，可连续运行长达两小时。&lt;h4&gt;结论&lt;/h4&gt;RL-100是一个与任务、具身和表示无关的通用框架，支持多种输入和机器人平台，能够实现与人类相当或更好的机器人操作性能。&lt;h4&gt;翻译&lt;/h4&gt;家庭和工厂中的真实世界机器人操作需要可靠性、效率和鲁棒性，达到或超越熟练人类操作员的水平。我们提出了RL-100，一个基于通过监督学习训练的扩散视觉运动策略构建的真实世界强化学习训练框架。RL-100引入了一个三阶段流程。首先，模仿学习利用人类先验知识。其次，迭代离线强化学习使用离线策略评估(OPE)程序来筛选PPO风格的更新，并在去噪过程中应用这些更新，以实现保守可靠的改进。第三，在线强化学习消除剩余的失败模式。此外，添加的轻量级一致性蒸馏头将扩散中的多步采样过程压缩为单步策略，实现了高频控制，同时延迟减少一个数量级，并保留了任务性能。该框架与任务、具身和表示无关，支持3D点云和2D RGB输入，各种机器人平台，以及单步和动作块策略。我们在七个真实机器人任务上评估了RL-100，包括动态刚体控制（如推-T和敏捷保龄球）、流体和颗粒倾倒、可变形布料折叠、精确灵巧拧螺丝和多阶段橙汁制作。RL-100在总共900个评估试验中实现了100%成功率，包括在一个任务上连续250次试验全部成功。该方法实现了接近人类远程操作或更好的时间效率，并展示了多小时的鲁棒性，不间断运行时间长达两小时。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world robotic manipulation in homes and factories demands reliability,efficiency, and robustness that approach or surpass skilled human operators. Wepresent RL-100, a real-world reinforcement learning training framework built ondiffusion visuomotor policies trained bu supervised learning. RL-100 introducesa three-stage pipeline. First, imitation learning leverages human priors.Second, iterative offline reinforcement learning uses an Offline PolicyEvaluation procedure, abbreviated OPE, to gate PPO-style updates that areapplied in the denoising process for conservative and reliable improvement.Third, online reinforcement learning eliminates residual failure modes. Anadditional lightweight consistency distillation head compresses the multi-stepsampling process in diffusion into a single-step policy, enablinghigh-frequency control with an order-of-magnitude reduction in latency whilepreserving task performance. The framework is task-, embodiment-, andrepresentation-agnostic and supports both 3D point clouds and 2D RGB inputs, avariety of robot platforms, and both single-step and action-chunk policies. Weevaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,such as Push-T and Agile Bowling, fluids and granular pouring, deformable clothfolding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100attains 100\% success across evaluated trials for a total of 900 out of 900episodes, including up to 250 out of 250 consecutive trials on one task. Themethod achieves near-human teleoperation or better time efficiency anddemonstrates multi-hour robustness with uninterrupted operation lasting up totwo hours.</description>
      <author>example@mail.com (Kun Lei, Huanyu Li, Dongjie Yu, Zhenyu Wei, Lingxiao Guo, Zhennan Jiang, Ziyu Wang, Shiyu Liang, Huazhe Xu)</author>
      <guid isPermaLink="false">2510.14830v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery</title>
      <link>http://arxiv.org/abs/2510.14768v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为接触感知动态恢复(CADRE)的强化学习框架，用于在灵巧操作中处理意外错误和干扰，特别是接住下落物体并系统重置以恢复主要任务。&lt;h4&gt;背景&lt;/h4&gt;现实世界中的灵巧操作经常遇到意外错误和干扰，可能导致灾难性故障，如掉落被操作物体。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，在物体仍在抓取范围内时接住下落物体，并将系统重置为有利于恢复主要操作任务的配置。&lt;h4&gt;方法&lt;/h4&gt;提出接触感知动态恢复(CADRE)框架，这是一个强化学习框架，集成了受神经描述场(NDF)启发的模块来提取隐式接触特征，直接推理手指-物体对应关系并适应不同物体几何形状。&lt;h4&gt;主要发现&lt;/h4&gt;整合接触特征提高了训练效率，增强了强化学习的收敛性能，并最终导致更成功的恢复操作。&lt;h4&gt;结论&lt;/h4&gt;CADRE框架可以零样本泛化到具有不同几何形状的未见物体上，证明了其在实际应用中的有效性和通用性。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的灵巧操作经常遇到意外错误和干扰，可能导致灾难性故障，如掉落被操作物体。为了应对这一挑战，我们专注于在物体仍在抓取范围内时接住下落物体，并将系统重置为有利于恢复主要操作任务的配置。我们提出了接触感知动态恢复(CADRE)，这是一个强化学习框架，集成了受神经描述场(NDF)启发的模块来提取隐式接触特征。与仅依赖物体姿态或点云输入的方法相比，NDF可以直接推理手指-物体对应关系并适应不同的物体几何形状。实验表明，整合接触特征提高了训练效率，增强了强化学习的收敛性能，并最终导致更成功的恢复操作。此外，我们证明了CADRE可以零样本泛化到具有不同几何形状的未见物体上。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人在灵巧操作中遇到意外错误和干扰时的恢复问题，特别是如何抓住掉落的物体并恢复到有利于继续主要操作任务的状态。这个问题在现实中很重要，因为机器人执行实际任务时经常遇到意外干扰，如螺丝卡住导致产生意外扭矩，可能导致物体掉落。仅仅抓住物体是不够的，还需要恢复到适合继续主要任务的状态，例如从强力抓取切换到精确抓取，这对实际应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到动态恢复问题的重要性，特别是抓住掉落物体并恢复到有利于继续主要操作任务的状态。他们观察到接触在灵巧操作中的重要性，并认为保持不同物体几何形状间一致的接触行为是成功泛化的基本因素。作者借鉴了Neural Descriptor Fields (NDF)来提取隐式接触特征，NDF能够捕获3D坐标和物体点云之间的几何对应关系。他们设计了Contact-Aware Dynamic Recovery (CADRE)框架，将NDF特征作为隐式接触信息整合到强化学习中。同时，他们借鉴了强化学习(特别是PPO算法)、点云表示方法以及DexPoint中利用接触信息提高泛化的思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用神经描述场(NDF)提取隐式接触特征，使机器人能够推理手指-物体对应关系并适应不同物体几何形状，同时不仅关注抓住掉落物体，还关注恢复到有利于继续主要操作任务的状态。整体实现流程包括：1)使用预训练的NDF模型提取接触特征，在机器人手上预定义关键点并查询这些点的NDF特征形成抓取特征；2)设计强化学习框架，将观察(机器人关节角度、物体姿态、物体速度)和抓取特征结合作为输入，使用PPO算法优化策略，并设计多目标奖励函数；3)在螺丝刀和螺母恢复任务上评估方法，测试泛化能力，并在真实机器人硬件上部署验证。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出通过抓取进行恢复的问题，机器人不仅要抓住掉落的物体，还要实现能够无缝恢复主要操作任务的抓取配置；2)开发了基于NDF的隐式接触表示，用于接触丰富的灵巧操作，有效捕获手部和操作物体之间的几何对应关系；3)提出了用于动态恢复的强化学习框架，利用这种表示实现成功的抓取和有利于后续操作任务的状态；4)证明了这种接触表示能够在不同几何形状的动态恢复任务中实现有效的泛化。相比之前工作，我们的方法不仅关注稳定抓取，还考虑后续操作任务的需求；NDF可以直接推理手指-物体对应关系并适应不同物体几何形状；提供了比点云方法更全面的接触建模；相比DexPoint，我们的方法区分了应该接触和应该避免接触的区域。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了CADRE，一种利用神经描述场提取隐式接触特征的强化学习框架，使机器人能够在灵巧操作中从掉落物体中恢复并回到有利于继续主要操作任务的状态，同时实现了对不同物体几何形状的零样本泛化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world dexterous manipulation often encounters unexpected errors anddisturbances, which can lead to catastrophic failures, such as dropping themanipulated object. To address this challenge, we focus on the problem ofcatching a falling object while it remains within grasping range and,importantly, resetting the system to a configuration favorable for resuming theprimary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), areinforcement learning framework that incorporates a Neural Descriptor Field(NDF)-inspired module to extract implicit contact features. Compared to methodsthat rely solely on object pose or point cloud input, NDFs can directly reasonabout finger-object correspondence and adapt to different object geometries.Our experiments show that incorporating contact features improves trainingefficiency, enhances convergence performance for RL training, and ultimatelyleads to more successful recoveries. Additionally, we demonstrate that CADREcan generalize zero-shot to unseen objects with different geometries.</description>
      <author>example@mail.com (Fan Yang, Zixuan Huang, Abhinav Kumar, Sergio Aguilera Marinovic, Soshi Iba, Rana Soltani Zarrin, Dmitry Berenson)</author>
      <guid isPermaLink="false">2510.14768v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning</title>
      <link>http://arxiv.org/abs/2510.14584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通用的可放置性度量方法，可以直接从嘈杂点云评估放置姿态，无需任何形状先验知识。该方法联合评分稳定性、可抓取性和间隙，实现无需模型的统一抓取-放置推理。&lt;h4&gt;背景&lt;/h4&gt;在现实世界的传感噪声下可靠地抓取和放置未知物体仍然具有挑战性。现有方法依赖于强物体先验（如CAD模型）或平面支撑假设，限制了抓取和放置之间的泛化和统一推理能力。&lt;h4&gt;目的&lt;/h4&gt;引入一种通用的可放置性度量方法，直接从嘈杂点云评估放置姿态，无需任何形状先验；实现统一抓取-放置推理；在未见过的真实物体和非平面物体支撑上提供准确的稳定性预测和物理合理的放置结果。&lt;h4&gt;方法&lt;/h4&gt;引入通用的可放置性度量方法；从原始几何形状中提取物体的支撑表面；生成多样化的多方向放置候选；采样满足碰撞和稳定性约束的接触点；将抓取分数与每个候选放置相关联，实现无需模型的统一抓取-放置推理。&lt;h4&gt;主要发现&lt;/h4&gt;在未见过的真实物体和非平面物体支撑上，该方法与CAD模型相当的准确性预测稳定性损失；比基于学习的方法产生更物理合理的放置结果。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够实现无需模型的统一抓取-放置推理；在现实世界的噪声条件下，能够准确预测稳定性损失并产生物理合理的放置结果；克服了现有方法对强物体先验和平面支撑假设的依赖。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界的传感噪声下可靠地抓取和放置未知物体仍然是一项具有挑战性的任务，因为现有方法依赖于强物体先验（如CAD模型）或平面支撑假设，限制了抓取和放置之间的泛化和统一推理能力。在这项工作中，我们引入了一种通用的可放置性度量方法，直接从嘈杂点云评估放置姿态，无需任何形状先验。该度量方法联合评分稳定性、可抓取性和间隙。从原始几何形状中，我们提取物体的支撑表面，生成多样化的多方向放置候选，并采样满足碰撞和稳定性约束的接触点。通过将抓取分数与每个候选放置相关联，我们提出的方法实现了无需模型的统一抓取-放置推理，并选择导致稳定、无碰撞放置的抓取-放置对。在未见过的真实物体和非平面物体支撑上，我们的度量方法在预测稳定性损失方面提供了与CAD模型相当的准确性，并且通常比基于学习方法的预测器产生更物理合理的放置结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在现实世界感知噪声下可靠地抓取和放置未知物体的问题。这个问题很重要，因为抓取和放置能力对仓库物流、家庭辅助和医疗保健等机器人应用至关重要，而现有方法依赖于强物体先验（如CAD模型）或平面支撑假设，限制了在复杂和噪声环境中的泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：大多数方法使用强物体先验评估放置稳定性，或只评估少量预定义的放置姿态。作者借鉴了统一抓取和放置推理的思想，从点云处理中学习物体重建方法，并改进了稳定性评估以处理部分和噪声观测。新方法设计了一个通用的可放置性度量，直接从嘈杂点云评估放置姿态，融合物理可行性和机器人约束，实现无模型统一的抓取和放置推理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个通用的可放置性度量，直接从传感器数据评估放置姿态，融合稳定性、放置条件下的可抓取性和间隙三个因素，通过统一评分抓取和放置候选，选择最佳组合。整体流程包括：1)感知阶段重建工作空间和物体点云；2)生成和评分候选抓取；3)生成多样化放置候选；4)计算可放置性评分（稳定性、PCG、间隙）；5)统一推理选择最佳抓取-放置组合进行执行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)通用可放置性度量，直接从嘈杂点云评估放置姿态；2)无模型统一抓取和放置推理，在共同物体框架中评估；3)物理有效性验证，在非平面支撑和边缘情况表现优异；4)任务驱动的放置评估，支持操作偏好。相比之前工作：不需要CAD模型或平面支撑假设；能处理边缘附近的物体；提供通用分数而非单一稳定平面；适合在线部署，不依赖重型预测网络。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通用的可放置性度量，使机器人能够直接从嘈杂点云评估未知物体的稳定放置，实现无模型统一的抓取和放置推理，显著提高了在复杂环境中的操作成功率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To reliably pick and place unknown objects under real-world sensing noiseremains a challenging task, as existing methods rely on strong object priors(e.g., CAD models), or planar-support assumptions, limiting generalization andunified reasoning between grasping and placing. In this work, we introduce ageneralized placeability metric that evaluates placement poses directly fromnoisy point clouds, without any shape priors. The metric jointly scoresstability, graspability, and clearance. From raw geometry, we extract thesupport surfaces of the object to generate diverse candidates formulti-orientation placement and sample contacts that satisfy collision andstability constraints. By conditioning grasp scores on each candidateplacement, our proposed method enables model-free unified pick-and-placereasoning and selects grasp-place pairs that lead to stable, collision-freeplacements. On unseen real objects and non-planar object supports, our metricdelivers CAD-comparable accuracy in predicting stability loss and generallyproduces more physically plausible placements than learning-based predictors.</description>
      <author>example@mail.com (Benno Wingender, Nils Dengler, Rohit Menon, Sicong Pan, Maren Bennewitz)</author>
      <guid isPermaLink="false">2510.14584v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification</title>
      <link>http://arxiv.org/abs/2510.14576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了CALM-Net，一种基于曲率感知的激光雷达点云多分支神经网络，用于车辆重识别任务。&lt;h4&gt;背景&lt;/h4&gt;车辆重识别面临的主要挑战是从三维点云中学习判别性和互补性特征来区分不同车辆。&lt;h4&gt;目的&lt;/h4&gt;提出CALM-Net模型，通过整合曲率感知信息提高车辆重识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;采用多分支架构，整合了边缘卷积、点注意力和曲率嵌入（用于表征点云中的局部表面变化），学习丰富的几何和上下文特征。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验表明，CALM-Net相比最强基线方法，平均重识别准确率提高了约1.97个百分点。&lt;h4&gt;结论&lt;/h4&gt;将曲率信息整合到深度学习架构中并采用多分支特征学习，能有效提升基于激光雷达点云的车辆重识别性能。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文提出了CALM-Net，一种基于曲率感知的激光雷达点云多分支神经网络，用于车辆重识别。所提出的模型解决了从三维点云中学习判别性和互补性特征以区分车辆这一挑战。CALM-Net采用多分支架构，整合了边缘卷积、点注意力和曲率嵌入，后者用于表征点云中的局部表面变化。通过结合这些机制，模型学习更适合重识别任务的丰富几何和上下文特征。在大型nuScenes数据集上的实验评估表明，CALM-Net比我们研究中最强的基线方法平均提高了约1.97个百分点的重识别准确率。结果证实了将曲率信息整合到深度学习架构中的有效性，并突出了多分支特征学习对基于激光雷达点云的车辆重识别的益处。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于LiDAR点云的车辆重识别问题，即如何从三维点云数据中学习判别性特征来区分不同车辆。这个问题在智能交通系统中至关重要，因为它支持跨摄像头跟踪、交通分析和自动驾驶安全，能够解决传统运动跟踪在遮挡、轨迹碎片化等情况下的失败问题，同时点云数据提供准确的3D几何信息，相比摄像头数据对光照变化和视角变化具有更好的鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了基于摄像头的方法在复杂环境下的局限性，认识到点云数据的优势。他们借鉴了车辆重识别领域的多种方法，包括视角感知学习、多分支特征融合和注意力机制，同时借鉴了机器人学中利用特征值确定车辆方向的思想。基于这些现有工作，作者设计了CALM-Net，整合边缘卷积（处理局部几何）、点注意力（捕获全局上下文）和曲率嵌入（编码表面变化）三种机制，并通过混合采样策略（训练时随机采样，推理时最远点采样）进一步提升了性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; CALM-Net的核心思想是通过多分支神经网络同时捕捉点云数据中的局部几何结构、全局上下文信息和表面曲率特征，学习对视角和环境变化鲁棒的车辆嵌入。整体流程包括：1)输入点云进行下采样；2)并行处理三个分支-边缘卷支提取局部几何、点注意力捕获全局依赖、曲率嵌入编码表面变化；3)将各分支特征融合并通过卷积和批归一化处理；4)通过ReLU激活得到最终嵌入；5)使用二元交叉熵损失进行训练，判断两个点云是否对应同一车辆。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次在LiDAR点云重识别中引入曲率感知机制，通过可学习的曲率嵌入实现细粒度几何推理；2)设计多分支架构，同时处理局部、上下文和结构特征；3)提出混合点下采样策略，结合随机采样的数据增强和FPS的结构保持优势；4)精心设计特征融合方法。相比之前工作，本文专注于点云而非图像数据，引入曲率嵌入这一新特征，采用多分支而非单一架构，并使用混合采样策略，在nuScenes数据集上实现了比最强基线高1.97%的准确率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了CALM-Net，一个结合边缘卷积、点注意力和曲率嵌入的多分支神经网络，通过从LiDAR点云中学习判别性和几何驱动的特征，显著提高了车辆重识别的准确率，特别是在复杂城市环境中的视角变化和光照变化情况下。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents CALM-Net, a curvature-aware LiDAR point cloud-basedmulti-branch neural network for vehicle re-identification. The proposed modeladdresses the challenge of learning discriminative and complementary featuresfrom three-dimensional point clouds to distinguish between vehicles. CALM-Netemploys a multi-branch architecture that integrates edge convolution, pointattention, and a curvature embedding that characterizes local surface variationin point clouds. By combining these mechanisms, the model learns richergeometric and contextual features that are well suited for there-identification task. Experimental evaluation on the large-scale nuScenesdataset demonstrates that CALM-Net achieves a mean re-identification accuracyimprovement of approximately 1.97\% points compared with the strongest baselinein our study. The results confirms the effectiveness of incorporating curvatureinformation into deep learning architectures and highlight the benefit ofmulti-branch feature learning for LiDAR point cloud-based vehiclere-identification.</description>
      <author>example@mail.com (Dongwook Lee, Sol Han, Jinwhan Kim)</author>
      <guid isPermaLink="false">2510.14576v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>High-Order Meshfree Surface Integration, Including Singular Integrands</title>
      <link>http://arxiv.org/abs/2510.14236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发并测试了针对表面点云的高阶积分方法，解决了在任意分段光滑表面上进行精确积分的问题。&lt;h4&gt;背景&lt;/h4&gt;表面积分在工程和科学领域的多种应用中至关重要，特别是在涉及偏微分方程的各种积分方法中。基于网格的方法需要曲面网格才能实现高阶收敛，这在许多表面上难以可靠获得；而无网格方法通常需要在感兴趣域上精确积分一组函数，但这些积分在大多数表面上没有闭式形式。&lt;h4&gt;目的&lt;/h4&gt;开发能够在任意、分段光滑表面（有边界或无边界）上进行高精度积分的方法，且不需要特定的点排列或初始三角剖分。&lt;h4&gt;方法&lt;/h4&gt;作者提出了两种完全无网格的积分方法，适用于任意分段光滑表面。这些方法不需要特定的点排列或表面的初始三角剖分。此外，作者还展示了如何扩展这些方法以处理奇异积分。&lt;h4&gt;主要发现&lt;/h4&gt;1. 开发了两种在任意分段光滑表面上进行积分的方法；2. 这些方法完全无网格，不需要特定的点排列或初始三角剖分；3. 方法可以处理奇异积分，同时保持高精度；4. 无需在奇点附近改变点密度即可维持高精度。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为在任意表面上进行高阶积分提供了有效解决方案，克服了传统网格方法和无网格方法的局限性，并能处理奇异积分情况。&lt;h4&gt;翻译&lt;/h4&gt;我们开发并测试了针对表面点云的高阶积分方法。在表面上积分函数的任务在工程和科学的一系列应用中出现，特别是在涉及偏微分方程的各种积分方法中。基于网格的方法需要曲面网格才能实现高阶收敛，这在许多表面上难以可靠获得，而大多数无网格方法需要在感兴趣域上精确积分一组函数（如径向基函数）；这些积分在大多数表面上通常没有闭式形式。我们描述了两种在任意、分段光滑表面（有边界或无边界）上进行积分的方法。我们的方法不需要特定的点排列或表面的初始三角剖分，使它们完全无网格。我们还展示了如何扩展这些方法以处理奇异积分，同时保持高精度，而无需在奇点附近改变点密度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We develop and test high-order methods for integration on surface pointclouds. The task of integrating a function on a surface arises in a range ofapplications in engineering and the sciences, particularly those involvingvarious integral methods for partial differential equations. Mesh-based methodsrequire a curved mesh for high-order convergence, which can be difficult toreliably obtain on many surfaces, and most meshfree methods require the abilityto integrate a set of functions (such as radial basis functions) exactly on thedomain of interest; these integrals are generally not known in closed form onmost surfaces. We describe two methods for integrating on arbitrary,piecewise-smooth surfaces with or without boundary. Our approaches do notrequire a particular arrangement of points or an initial triangulation of thesurface, making them completely meshfree. We also show how the methods can beextended to handle singular integrals while maintaining high accuracy withoutchanging the point density near singularities.</description>
      <author>example@mail.com (Daniel R. Venn, Steven J. Ruuth)</author>
      <guid isPermaLink="false">2510.14236v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space</title>
      <link>http://arxiv.org/abs/2510.14234v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的无模型方法，用于带有关键点约束的三维可变形物体形状控制。该方法通过深度学习从点云中提取关键点作为特征向量，保留了物体的空间信息同时降低了特征空间维度。将操控问题简化为视觉伺服问题，使用变形雅可比矩阵描述形状动力学，并通过结合障碍李雅普诺夫函数的预设性能控制方法提高控制精度。实验验证了该方法的有效性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;操控三维可变形物体对机器人系统具有显著挑战，主要因为可变形物体具有无限维状态空间和复杂的变形动力学特性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型的无模型方法，用于带有关键点约束的可变形物体形状控制，提高操控的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;不同于依赖特征降维的现有方法，所提出的控制器利用从可变形物体点云中通过深度学习方法提取的关键点坐标作为特征向量。通过提取关键点，将可变形物体操控简化为视觉伺服问题，使用变形雅可比矩阵描述形状动力学。同时，开发了一种结合障碍李雅普诺夫函数的预设性能控制方法，以强制执行关键点的约束，提高控制精度。&lt;h4&gt;主要发现&lt;/h4&gt;通过提取关键点，成功降低了特征空间维度同时保留了物体空间信息；结合障碍李雅普诺夫函数的预设性能控制方法有效提高了控制精度；实验结果验证了所提出方法的有效性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的无模型方法通过深度学习提取关键点并结合预设性能控制，有效解决了三维可变形物体形状控制问题，具有较好的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;操控三维可变形物体对机器人系统具有显著挑战，因为它们具有无限维状态空间和复杂的变形动力学。本文提出了一种新型的带有关键点约束的无模型形状控制方法。与依赖特征降维的现有方法不同，所提出的控制器利用从可变形物体点云中通过深度学习方法提取的关键点坐标作为特征向量。这种方法不仅降低了特征空间的维度，还保留了物体的空间信息。通过提取关键点，可变形物体的操控被简化为一个视觉伺服问题，其中形状动力学使用变形雅可比矩阵描述。为了提高控制精度，开发了一种结合障碍李雅普诺夫函数的预设性能控制方法，以强制执行关键点的约束。使用李雅普诺夫方法严格分析并验证了闭环系统的稳定性。实验结果进一步证明了所提出方法的有效性和鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人操作三维可变形物体的控制挑战。可变形物体（如海绵、布料等）由于其形状可以无限变化，状态空间维度极高，且具有复杂的变形动力学特性，使得传统的机器人控制方法难以有效处理。这个问题在现实中非常重要，因为可变形物体操作在医疗手术、工业焊接、自动折叠衣物等领域有广泛应用，提高机器人对这类物体的操作能力可以扩展机器人在这些领域的应用，提高自动化水平，减少人工干预，并提高任务执行的精度和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者在设计方法时，首先分析了现有可变形物体操作方法的局限性，包括基于模型的方法依赖物理模型但参数估计困难，以及无模型方法面临高维状态空间的挑战。作者借鉴了现有工作中的深度学习方法提取关键点、基于Jacobian的视觉伺服控制以及规定性能控制（PPC）和障碍Lyapunov函数（BLF）等技术。作者的创新点在于将PPC方法从已知Jacobian矩阵的视觉伺服任务迁移到Jacobian矩阵完全未知的可变形物体操作任务中，并结合关键点提取方法，在保留空间信息的同时降低特征维度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是从可变形物体的3D点云中提取关键点作为特征向量，这些关键点保留了物体的空间信息同时降低了维度；将可变形物体操作简化为视觉伺服问题，使用变形Jacobian矩阵描述形状动力学；设计规定性能控制器，通过障碍Lyapunov函数强制执行关键点的约束；使用神经网络近似未知的Jacobian矩阵。整体实现流程包括：使用Key-Grid神经网络从点云中提取关键点；计算关键点误差；使用规定性能函数定义误差边界；将误差转换为转换误差；设计基于Jacobian的控制器，使用神经网络近似Jacobian矩阵；应用自适应律更新神经网络权重；通过Lyapunov分析确保系统稳定性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出一种新的无模型方法，使用关键点坐标作为特征向量，保留了空间信息同时降低了维度；将规定性能控制方法从视觉伺服任务迁移到可变形物体操作任务，其中Jacobian矩阵完全未知；设计障碍Lyapunov函数来确保关键点误差的边界约束；结合深度学习和自适应控制方法，提高了控制精度和鲁棒性。相比之前工作，该方法直接从3D点云提取关键点，而不是使用手动标记的关键点；使用改进的PPC框架，而不是基于图网络的MPC控制器；与传统降维方法相比，保留了物理和空间信息；与其他避免潜在抽象的方法相比，不局限于二维结构、刚性假设或强模型依赖。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于空间潜在空间的关键点约束规定性能控制方法，有效解决了三维可变形物体操作中的高维状态空间和复杂变形动力学挑战，显著提高了控制精度和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Manipulating three-dimensional (3D) deformable objects presents significantchallenges for robotic systems due to their infinite-dimensional state spaceand complex deformable dynamics. This paper proposes a novel model-freeapproach for shape control with constraints imposed on key points. Unlikeexisting methods that rely on feature dimensionality reduction, the proposedcontroller leverages the coordinates of key points as the feature vector, whichare extracted from the deformable object's point cloud using deep learningmethods. This approach not only reduces the dimensionality of the feature spacebut also retains the spatial information of the object. By extracting keypoints, the manipulation of deformable objects is simplified into a visualservoing problem, where the shape dynamics are described using a deformationJacobian matrix. To enhance control accuracy, a prescribed performance controlmethod is developed by integrating barrier Lyapunov functions (BLF) to enforceconstraints on the key points. The stability of the closed-loop system isrigorously analyzed and verified using the Lyapunov method. Experimentalresults further demonstrate the effectiveness and robustness of the proposedmethod.</description>
      <author>example@mail.com (Ning Han, Gu Gong, Bin Zhang, Yuexuan Xu, Bohan Yang, Yunhui Liu, David Navarro-Alarcon)</author>
      <guid isPermaLink="false">2510.14234v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Distributed-Memory Parallel Algorithms for Fixed-Radius Near Neighbor Graph Construction</title>
      <link>http://arxiv.org/abs/2510.14147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种使用覆盖树的可扩展稀疏感知分布式内存算法，用于计算一般度量空间中的近邻图，在各种数据集和度量标准下表现出卓越的性能和并行扩展性。&lt;h4&gt;背景&lt;/h4&gt;计算固定半径近邻图是许多数据分析算法的重要第一步。近邻图在某种度量下连接接近的点，为点云赋予组合结构。随着计算能力和数据获取方法的进步，各种大型科学数据集需要可扩展的解决方案来处理下游分析中的常见子程序。&lt;h4&gt;目的&lt;/h4&gt;解决现有并行近邻搜索工作在精确解和非欧几里得度量方面的局限性，提供一个可扩展的稀疏感知分布式内存算法，用于计算一般度量空间中的近邻图。&lt;h4&gt;方法&lt;/h4&gt;提出了一种使用覆盖树的可扩展稀疏感知分布式内存算法。提供了覆盖树构建的共享内存算法，并展示了其与最先进的固定半径搜索数据结构的竞争力。然后介绍了两种分布式内存算法：简单的点分区策略和空间分区策略，它们利用每个节点上的覆盖树算法。&lt;h4&gt;主要发现&lt;/h4&gt;算法在各种真实和合成数据集上表现出并行扩展性，适用于传统和非传统度量。在包含一百万个点的真实世界高维数据集上，对于每个顶点平均70个邻居的图，使用1024个核心实现了高达678.34倍的速度提升；对于每个顶点平均500个邻居的图，使用4096个核心实现了高达1590.99倍的速度提升。&lt;h4&gt;结论&lt;/h4&gt;该算法能够有效处理大规模数据集的近邻图计算，在多种数据集和度量标准下表现出良好的并行扩展性。&lt;h4&gt;翻译&lt;/h4&gt;计算固定半径近邻图是许多数据分析算法的重要第一步。近邻图在某种度量下连接接近的点，为点云赋予组合结构。随着计算能力和数据获取方法的进步，各种大型科学数据集需要可扩展的解决方案来处理下游分析中的常见子程序。现有的并行近邻搜索工作在最近邻和近似最近邻搜索问题上取得了很大进展，特别关注欧几里得空间。然而，许多应用程序需要精确解和非欧几里得度量。本文提出了一种使用覆盖树的可扩展稀疏感知分布式内存算法，用于计算一般度量空间中的近邻图。我们提供了覆盖树构建的共享内存算法，并展示了其与最先进的固定半径搜索数据结构的竞争力。然后，我们介绍了用于近邻图问题的两种分布式内存算法：一种简单的点分区策略和一种空间分区策略，它们利用每个节点上的覆盖树算法。我们的算法在各种真实和合成数据集上表现出并行扩展性，适用于传统和非传统度量。在包含一百万个点的真实世界高维数据集上，对于每个顶点平均70个邻居的图，使用1024个核心实现了比最先进方法高达678.34倍的速度提升；对于每个顶点平均500个邻居的图，使用4096个核心实现了高达1590.99倍的速度提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computing fixed-radius near-neighbor graphs is an important first step formany data analysis algorithms. Near-neighbor graphs connect points that areclose under some metric, endowing point clouds with a combinatorial structure.As computing power and data acquisition methods advance, diverse sources oflarge scientific datasets would greatly benefit from scalable solutions to thiscommon subroutine for downstream analysis. Prior work on parallel nearestneighbors has made great progress in problems like k-nearest and approximatenearest neighbor search problems, with particular attention on Euclideanspaces. Yet many applications need exact solutions and non-Euclidean metrics.This paper presents a scalable sparsity-aware distributed memory algorithmusing cover trees to compute near-neighbor graphs in general metric spaces. Weprovide a shared-memory algorithm for cover tree construction and demonstrateits competitiveness with state-of-the-art fixed-radius search data structures.We then introduce two distributed-memory algorithms for the near-neighbor graphproblem, a simple point-partitioning strategy and a spatial-partitioningstrategy, which leverage the cover tree algorithm on each node. Our algorithmsexhibit parallel scaling across a variety of real and synthetic datasets forboth traditional and non-traditional metrics. On real world high dimensionaldatasets with one million points, we achieve speedups up to 678.34x over thestate-of-the-art using 1024 cores for graphs with 70 neighbors per vertex (onaverage), and up to 1590.99x using 4096 cores for graphs with 500 neighbors pervertex (on average).</description>
      <author>example@mail.com (Gabriel Raulet, Dmitriy Morozov, Aydin Buluc, Katherine Yelick)</author>
      <guid isPermaLink="false">2510.14147v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Geometric local parameterization for solving Hele-Shaw problems with surface tension</title>
      <link>http://arxiv.org/abs/2510.14088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种解决二维Hele-Shaw自由边界问题（带有表面张力）的新型计算框架。该方法使用点云表示移动边界，无需全局参数化，并通过广义移动最小二乘法构建局部几何图表，实现高阶几何量近似。研究提供了严格的收敛分析，并通过数值实验验证了方法的有效性，展示了复杂形状在表面张力作用下向圆形平衡状态的正确演化。&lt;h4&gt;背景&lt;/h4&gt;Hele-Shaw自由边界问题是流体力学中的重要问题，特别是在研究具有表面张力的界面动力学时。传统方法通常需要全局参数化来表示移动边界，这在处理复杂几何形状时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的计算框架，能够高效、准确地解决带有表面张力的二维Hele-Shaw自由边界问题，克服传统方法在处理复杂几何形状时的局限性，并实现高阶收敛精度。&lt;h4&gt;方法&lt;/h4&gt;1. 使用点云表示移动边界，消除全局参数化的需求；2. 应用广义移动最小二乘法构建局部几何图表；3. 直接从点云数据高阶近似几何量（如曲率）；4. 使用局部参数化离散化控制边界积分方程；5. 包含奇异积分的解析公式；6. 进行严格的收敛分析，建立一致性和稳定性条件。&lt;h4&gt;主要发现&lt;/h4&gt;1. 所提出的方法实现了高阶空间收敛；2. 获得了预期的时域收敛率；3. 误差界限与均匀采样点云数据大小、边界光滑度和数值积分规则阶数相关；4. 复杂初始形状在表面张力作用下正确演变为圆形平衡状态。&lt;h4&gt;结论&lt;/h4&gt;该新型计算框架为解决二维Hele-Shaw自由边界问题提供了有效方法，点云表示和局部几何图表的构建使得方法能够处理复杂几何形状，同时保持高阶收敛精度。数值实验验证了理论分析的正确性和方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们介绍了一种解决带有表面张力的二维Hele-Shaw自由边界问题的新型计算框架。移动边界由点云表示，消除了对全局参数化的需求。我们的方法利用广义移动最小二乘法构建局部几何图表，能够直接从点云数据高阶近似几何量（如曲率）。这种局部参数化被系统地用于离散化控制边界积分方程，包括奇异积分的解析公式。我们为所提出的空间离散化提供了严格的收敛分析，在特定条件下建立了一致性和稳定性。导出的误差界限基于移动边界上均匀采样点云数据的大小、边界的光滑度和数值积分规则的阶数。数值实验验证了理论结果，展示了高阶空间收敛和预期的时域收敛率。通过复杂初始形状的模拟进一步说明了该方法的有效性，这些形状在表面张力的影响下正确地演变为圆形平衡状态。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we introduce a novel computational framework for solving thetwo-dimensional Hele-Shaw free boundary problem with surface tension. Themoving boundary is represented by point clouds, eliminating the need for aglobal parameterization. Our approach leverages Generalized Moving LeastSquares (GMLS) to construct local geometric charts, enabling high-orderapproximations of geometric quantities such as curvature directly from thepoint cloud data. This local parameterization is systematically employed todiscretize the governing boundary integral equation, including an analyticalformula of the singular integrals. We provide a rigorous convergence analysisfor the proposed spatial discretization, establishing consistency and stabilityunder certain conditions. The resulting error bound is derived in terms of thesize of the uniformly sampled point cloud data on the moving boundary, thesmoothness of the boundary, and the order of the numerical quadrature rule.Numerical experiments confirm the theoretical findings, demonstratinghigh-order spatial convergence and the expected temporal convergence rates. Themethod's effectiveness is further illustrated through simulations of complexinitial shapes, which correctly evolve towards circular equilibrium statesunder the influence of surface tension.</description>
      <author>example@mail.com (Zengyan Zhang, Wenrui Hao, John Harlim)</author>
      <guid isPermaLink="false">2510.14088v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation</title>
      <link>http://arxiv.org/abs/2510.14190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了ConDA(对比扩散对齐)框架，通过对比学习在扩散嵌入中组织潜在空间，使其与系统动力学对齐，从而实现更可控和可解释的生成操作。&lt;h4&gt;背景&lt;/h4&gt;扩散模型在生成任务上表现出色，但其潜在空间没有被明确组织用于可解释的控制，限制了其在需要精确控制的应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来组织扩散模型的潜在空间，使其能够支持忠实插值、外推和可控生成，同时保持生成质量。&lt;h4&gt;方法&lt;/h4&gt;提出ConDA框架，应用对比学习于扩散嵌入中，将潜在几何结构与系统动力学对齐，使遍历方向反映潜在的动力学因素，并支持非线性轨迹遍历。&lt;h4&gt;主要发现&lt;/h4&gt;在流体动力学、神经钙成像、治疗性神经刺激和面部表情等多个基准测试中，ConDA产生了比线性遍历和基于条件的基线更具可解释性的潜在表示，同时提高了可控性。&lt;h4&gt;结论&lt;/h4&gt;扩散潜变量编码了与动力学相关的结构，但要有效利用这种结构，需要沿着潜在流形进行潜在组织和遍历。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型在生成方面表现出色，但它们的潜在空间没有被明确组织用于可解释的控制。我们引入了ConDA(对比扩散对齐)，这是一个在扩散嵌入中应用对比学习的框架，将潜在几何结构与系统动力学对齐。受最近进展的启发，这些进展表明对比目标可以恢复更多解缠和结构化的表示，ConDA组织扩散潜变量，使得遍历方向反映潜在的动力学因素。在这个对比结构化的空间中，ConDA支持非线性轨迹遍历，实现忠实插值、外推和可控生成。在流体动力学、神经钙成像、治疗性神经刺激和面部表情的基准测试中，ConDA与线性遍历和基于条件的基线相比，产生了具有改进可解释性的潜在表示。这些结果表明扩散潜变量编码了动力学相关的结构，但利用这种结构需要沿着潜在流形进行潜在组织和遍历。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models excel at generation, but their latent spaces are notexplicitly organized for interpretable control. We introduce ConDA (ContrastiveDiffusion Alignment), a framework that applies contrastive learning withindiffusion embeddings to align latent geometry with system dynamics. Motivatedby recent advances showing that contrastive objectives can recover moredisentangled and structured representations, ConDA organizes diffusion latentssuch that traversal directions reflect underlying dynamical factors. Withinthis contrastively structured space, ConDA enables nonlinear trajectorytraversal that supports faithful interpolation, extrapolation, and controllablegeneration. Across benchmarks in fluid dynamics, neural calcium imaging,therapeutic neurostimulation, and facial expression, ConDA producesinterpretable latent representations with improved controllability compared tolinear traversals and conditioning-based baselines. These results suggest thatdiffusion latents encode dynamics-relevant structure, but exploiting thisstructure requires latent organization and traversal along the latent manifold.</description>
      <author>example@mail.com (Ruchi Sandilya, Sumaira Perez, Charles Lynch, Lindsay Victoria, Benjamin Zebley, Derrick Matthew Buchanan, Mahendra T. Bhati, Nolan Williams, Timothy J. Spellman, Faith M. Gunning, Conor Liston, Logan Grosenick)</author>
      <guid isPermaLink="false">2510.14190v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>ViTacGen: Robotic Pushing with Vision-to-Touch Generation</title>
      <link>http://arxiv.org/abs/2510.14117v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ViTacGen是一个创新的机器人操作框架，通过视觉到触觉的生成解决了触觉传感器限制问题，结合视觉和生成触觉数据通过强化学习实现高性能的机器人推操作，在模拟和真实实验中表现出色，成功率高达86%。&lt;h4&gt;背景&lt;/h4&gt;机器人推操作需要触觉反馈来捕捉末端执行器和物体之间的接触力和动力学，但真实触觉传感器面临高成本、脆弱性、校准困难和传感器差异等挑战，而仅基于视觉的策略难以获得满意性能。&lt;h4&gt;目的&lt;/h4&gt;提出ViTacGen框架，用于视觉机器人推操作，在强化学习中实现视觉到触觉的生成，消除对高分辨率真实触觉传感器的依赖，实现视觉系统上的有效零样本部署。&lt;h4&gt;方法&lt;/h4&gt;ViTacGen包含一个编码器-解码器视觉到触觉生成网络，直接从视觉图像序列生成接触深度图像（标准化触觉表示），以及一个基于视觉和生成触觉观察的对比学习融合视觉-触觉数据的强化学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界实验中验证了方法的有效性，展示了其卓越的性能，成功率达到86%。&lt;h4&gt;结论&lt;/h4&gt;ViTacGen框架能够在不依赖高分辨率触觉传感器的情况下实现有效的机器人推操作，通过视觉到触觉的生成实现了在视觉系统上的零样本部署。&lt;h4&gt;翻译&lt;/h4&gt;机器人推操作是一种基础的操作任务，需要触觉反馈来捕捉末端执行器和物体之间的微妙接触力和动力学。然而，真实的触觉传感器通常面临硬件限制，如高成本和脆弱性，以及部署挑战，包括校准和不同传感器之间的差异，而仅基于视觉的策略难以获得令人满意的性能。受人类从视觉推断触觉状态能力的启发，我们提出了ViTacGen，一个新颖的机器人操作框架，专为视觉机器人推操作设计，在强化学习中实现视觉到触觉的生成，以消除对高分辨率真实触觉传感器的依赖，实现仅在视觉系统上的有效零样本部署。具体而言，ViTacGen包含一个编码器-解码器视觉到触觉生成网络，直接从视觉图像序列生成接触深度图像（标准化的触觉表示），随后是一个基于视觉和生成触觉观察的对比学习融合视觉-触觉数据的强化学习策略。我们在模拟和真实世界实验中都验证了我们方法的有效性，展示了其卓越的性能，成功率达到86%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的是机器人推动任务中依赖昂贵且脆弱的触觉传感器的问题。这个问题很重要，因为触觉反馈对捕捉物体间细微接触力和动态至关重要，但真实触觉传感器成本高、易损坏、需要精确校准，且不同传感器间存在差异，限制了高性能机器人操作系统的实际部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受人类能从视觉推断触觉状态的启发，观察到现有方法要么依赖昂贵触觉传感器，要么仅使用视觉但性能不足。他们设计了编码器-解码器的视觉到触觉生成网络(VT-Gen)和强化学习策略网络(VT-Con)。借鉴了人类视觉-触觉交互能力、Soft Actor-Critic强化学习算法、MoCo对比学习框架、Tactile Gym模拟平台、注意力机制和VGG损失函数等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是模拟人类从视觉推断触觉的能力，让机器人仅通过视觉'感知'触觉，使用生成的触觉接触深度图像作为标准化表示，并通过对比学习对齐视觉和触觉特征。整体流程：1)在模拟环境中收集配对的视觉和触觉数据；2)训练VT-Gen网络从视觉生成触觉深度图像；3)冻结VT-Gen，训练VT-Con强化学习策略；4)零样本部署到真实视觉-only机器人系统。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出ViTacGen框架消除对触觉传感器的依赖；2)设计VT-Gen生成标准化触觉表示；3)提出VT-Con通过对比学习融合视觉-触觉特征；4)实现零样本部署；5)使用接触深度图解决传感器差异问题。不同之处：相比仅视觉方法提供更丰富感知；相比触觉传感器方法降低成本复杂度；相比简单特征拼接实现更有效跨模态对齐；相比校准方法具有更好通用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ViTacGen通过模拟人类从视觉推断触觉的能力，实现了仅使用视觉信息的机器人精确推动，消除了对昂贵触觉传感器的依赖，同时保持了高性能操作能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic pushing is a fundamental manipulation task that requires tactilefeedback to capture subtle contact forces and dynamics between the end-effectorand the object. However, real tactile sensors often face hardware limitationssuch as high costs and fragility, and deployment challenges involvingcalibration and variations between different sensors, while vision-onlypolicies struggle with satisfactory performance. Inspired by humans' ability toinfer tactile states from vision, we propose ViTacGen, a novel robotmanipulation framework designed for visual robotic pushing with vision-to-touchgeneration in reinforcement learning to eliminate the reliance onhigh-resolution real tactile sensors, enabling effective zero-shot deploymenton visual-only robotic systems. Specifically, ViTacGen consists of anencoder-decoder vision-to-touch generation network that generates contact depthimages, a standardized tactile representation, directly from visual imagesequence, followed by a reinforcement learning policy that fuses visual-tactiledata with contrastive learning based on visual and generated tactileobservations. We validate the effectiveness of our approach in both simulationand real world experiments, demonstrating its superior performance andachieving a success rate of up to 86\%.</description>
      <author>example@mail.com (Zhiyuan Wu, Yijiong Lin, Yongqiang Zhao, Xuyang Zhang, Zhuo Chen, Nathan Lepora, Shan Luo)</author>
      <guid isPermaLink="false">2510.14117v1</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation</title>
      <link>http://arxiv.org/abs/2510.13245v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SketchSem3D数据集和Cylinder Mamba Diffusion (CymbaDiff)方法，用于从手绘草图生成高质量的3D室外语义场景。&lt;h4&gt;背景&lt;/h4&gt;室外3D语义场景生成在都市仿真和自动驾驶等领域有重要应用，但该领域的发展受到缺乏公开可用、良好注释的数据集的制约。&lt;h4&gt;目的&lt;/h4&gt;引入首个大规模基准数据集SketchSem3D，用于从抽象手绘草图和卫星图像的伪标记注释生成3D室外语义场景，并提出一种增强空间一致性的新方法。&lt;h4&gt;方法&lt;/h4&gt;提出了Cylinder Mamba Diffusion (CymbaDiff)模型，该方法施加结构化的空间排序，明确捕获圆柱连续性和垂直层次结构，并保留生成的场景中的物理邻域关系和全局上下文。&lt;h4&gt;主要发现&lt;/h4&gt;在SketchSem3D上的大量实验表明，CymbaDiff实现了优越的语义一致性、空间真实性和跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SketchSem3数据集和CymbaDiff方法为室外3D语义场景生成提供了新的基准和解决方案，有助于推动该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;室外3D语义场景生成为都市仿真和自动驾驶等应用生成真实且语义丰富的环境。然而，这一方向的发展受到缺乏公开可用、良好注释的数据集的限制。我们引入了SketchSem3D，这是第一个大规模基准，用于从抽象手绘草图和卫星图像的伪标记注释生成3D室外语义场景。SketchSem3D包含两个子集：基于语义的KITTI草图和基于KITTI-360的草图（包含LiDAR体素及其相应的草图和注释卫星图像），以实现标准化、严格和多样化的评估。我们还提出了圆柱形Mamba扩散模型（CymbaDiff），显著增强了室外3D场景生成的空间一致性。CymbaDiff施加结构化的空间排序，明确捕获圆柱连续性和垂直层次结构，并保留生成的场景中的物理邻域关系和全局上下文。在SketchSem3D上的大量实验表明，CymbaDiff实现了优越的语义一致性、空间真实性和跨数据集泛化能力。代码和数据集将在https://github.com/Lillian-research-hub/CymbaDiff上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基于草图的3D户外语义场景生成问题，特别是缺乏公开的大规模标注数据集和现有方法在户外场景中的局限性。这个问题在现实中非常重要，因为高质量的城市场景生成对自动驾驶模拟、城市规划等应用至关重要，而传统方法要么依赖昂贵的传感器数据，要么无法生成复杂且语义丰富的户外环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D户外场景生成方法的局限性，如鸟瞰图(BEV)方法缺乏3D结构信息，多尺度方法计算复杂。他们借鉴了状态空间模型(SSMs)在图像处理和点云分析中的成功应用，结合扩散模型在生成任务中的优势。作者还利用了CLIP和SAM等现有模型进行数据集构建，但创新性地将这些技术整合到一个专门针对户外场景的框架中，通过圆柱坐标系统改进了空间表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结构化空间扩散模型，结合笛卡尔和圆柱坐标系统的优势，增强3D场景生成的空间连贯性。整体流程包括：1)构建SketchSem3D数据集，包含草图、卫星图像和3D体素；2)使用场景结构估计网络(SSEN)提取结构信息；3)通过潜在映射网络(LMN)压缩输入条件；4)利用CymbaDiff去噪网络，结合三重Mamba模块和圆柱Mamba层进行生成；5)从噪声逐步去噪，最终生成高质量的3D语义场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'基于草图的3D户外场景生成'新任务；2)构建首个专门的大规模基准数据集SketchSem3D；3)提出CymbaDiff模型，结合圆柱Mamba块增强空间连贯性；4)引入圆柱坐标系统来更好地表示户外场景的空间关系。相比之前工作，CymbaDiff避免了多尺度方法的计算复杂性，解决了BEV方法缺乏3D结构信息的问题，并首次将基于草本的3D生成扩展到复杂户外场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过CymbaDiff方法和SketchSem3D数据集，首次实现了从简单草图和卫星图像生成高质量、语义连贯的大规模3D城市场景，为自动驾驶和城市规划等应用提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outdoor 3D semantic scene generation produces realistic and semantically richenvironments for applications such as urban simulation and autonomous driving.However, advances in this direction are constrained by the absence of publiclyavailable, well-annotated datasets. We introduce SketchSem3D, the firstlarge-scale benchmark for generating 3D outdoor semantic scenes from abstractfreehand sketches and pseudo-labeled annotations of satellite images.SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-basedKITTI-360 (containing LiDAR voxels along with their corresponding sketches andannotated satellite images), to enable standardized, rigorous, and diverseevaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) thatsignificantly enhances spatial coherence in outdoor 3D scene generation.CymbaDiff imposes structured spatial ordering, explicitly captures cylindricalcontinuity and vertical hierarchy, and preserves both physical neighborhoodrelationships and global context within the generated scenes. Extensiveexperiments on SketchSem3D demonstrate that CymbaDiff achieves superiorsemantic consistency, spatial realism, and cross-dataset generalization. Thecode and dataset will be available athttps://github.com/Lillian-research-hub/CymbaDiff</description>
      <author>example@mail.com (Li Liang, Bo Miao, Xinyu Wang, Naveed Akhtar, Jordan Vice, Ajmal Mian)</author>
      <guid isPermaLink="false">2510.13245v2</guid>
      <pubDate>Fri, 17 Oct 2025 15:23:43 +0800</pubDate>
    </item>
    <item>
      <title>LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration</title>
      <link>http://arxiv.org/abs/2510.13729v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the International Symposium on Visual Computing (ISVC)  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LiFMCR，一个用于多微透镜阵列光场相机配准的新数据集，提供同步图像序列和高精度姿态数据，用于严格评估多相机光场配准方法。&lt;h4&gt;背景&lt;/h4&gt;现有光场数据集仅限于单相机设置，通常缺乏外部真实值，限制了多相机光场配准方法的评估。&lt;h4&gt;目的&lt;/h4&gt;创建一个独特的多相机光场数据集，结合高分辨率光场相机图像和精确的6自由度姿态数据，以实现多相机光场配准方法的严格评估。&lt;h4&gt;方法&lt;/h4&gt;提供两种互补的配准方法：1)基于RANSAC的鲁棒3D变换估计，使用跨视点点云；2)从单个光场图像估计外源性6-DoF姿态的光场PnP算法。两种方法都明确集成了光场相机模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法与真实值显示出良好的对齐，支持可靠的多视点光场处理。&lt;h4&gt;结论&lt;/h4&gt;LiFMCR数据集及其配套方法为多相机光场配准提供了基准，能够准确且可扩展地进行多相机配准。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了LiFMCR，一个用于多微透镜阵列光场相机配准的新颖数据集。虽然现有的光场数据集仅限于单相机设置且通常缺乏外部真实值，但LiFMCR提供了来自两个高分辨率Raytrix R32光场相机的同步图像序列，以及由Vicon动作捕捉系统记录的高精度6自由度姿态。这种独特组合能够严格评估多相机光场配准方法。作为基准，我们提供了两种互补的配准方法：一种基于RANSAC的鲁棒3D变换估计，使用跨视点点云；以及一种从单个光场图像估计外源性6-DoF姿态的光场PnP算法。两种方法都明确集成了光场相机模型，实现准确且可扩展的多相机配准。实验显示与真实值有良好的对齐，支持可靠的多视点光场处理。项目页面：https://lifmcr.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多相机光场相机（plenoptic cameras）注册缺乏标准数据集和基准测试的问题。这个问题很重要，因为准确的3D重建对自主系统和机器人应用至关重要，而结合多个光场相机可以通过立体视觉优势扩展深度范围和精度，提高深度感知能力和场景理解能力。现有的光场数据集通常局限于单相机设置且缺乏外部真实值，限制了多相机注册方法的评估和改进。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有光场数据集的局限性，特别是缺乏多相机设置和精确真实值的问题。他们借鉴了现有工作：使用LiFCal进行内参校准，采用SIFT特征提取和匹配，以及基于RANSAC的3D变换估计方法。作者设计了两种互补的注册方法：一种基于3D点云对齐的RANSAC方法，另一种是首次应用于光场数据的PnP算法。两种方法都明确集成了光场相机模型，以准确处理光场相机的特殊光学和几何特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提供一个包含同步多视角光场数据和精确6-DoF姿态真实值的数据集，并设计两种互补的多相机注册方法，一种基于3D点云对齐，另一种基于光场PnP算法，都考虑光场相机的特殊光学特性。3D RANSAC方法流程：内参校准→点云生成→SIFT特征提取与匹配→3D RANSAC对齐→计算相机间相对变换。光场PnP方法流程：仅参考相机点云→镜头畸变校正→光场模型透视投影→特征匹配→鲁棒基础矩阵估计→RANSAC PnP→Levenberg-Marquardt优化细化姿态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出LiFMCR数据集，首次提供同步多视角光场序列和精确6-DoF真实值；2)提出两种互补注册方法，包括基于RANSAC的3D变换估计和首个光场PnP算法；3)两种方法都明确集成光场相机模型；4)提供完整内参和外参校准流程；5)使用Vicon系统提供亚毫米级精度真实值。相比之前工作：现有数据集多为单相机且缺乏真实值，而LiFMCR提供多相机同步数据和精确姿态；现有方法不专门针对光场多相机注册，而本文方法考虑了光场相机特性；首次将PnP算法应用于光场数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了LiFMCR数据集和两种互补的光场多相机注册方法，填补了多视角光场数据与精确姿态真实值结合的空白，为光场相机在3D重建、SLAM等应用中的可靠使用提供了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present LiFMCR, a novel dataset for the registration of multiple microlens array (MLA)-based light field cameras. While existing light field datasetsare limited to single-camera setups and typically lack external ground truth,LiFMCR provides synchronized image sequences from two high-resolution RaytrixR32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF)poses recorded by a Vicon motion capture system. This unique combinationenables rigorous evaluation of multi-camera light field registration methods.  As a baseline, we provide two complementary registration approaches: a robust3D transformation estimation via a RANSAC-based method using cross-view pointclouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses fromsingle light field images. Both explicitly integrate the plenoptic cameramodel, enabling accurate and scalable multi-camera registration. Experimentsshow strong alignment with the ground truth, supporting reliable multi-viewlight field processing.  Project page: https://lifmcr.github.io/</description>
      <author>example@mail.com (Aymeric Fleith, Julian Zirbel, Daniel Cremers, Niclas Zeller)</author>
      <guid isPermaLink="false">2510.13729v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
  <item>
      <title>Characterizing Lidar Point-Cloud Adversities Using a Vector Field Visualization</title>
      <link>http://arxiv.org/abs/2510.13619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the preprint version of the paper published in: Proceedings  of the 37th International Technical Meeting of the Satellite Division of The  Institute of Navigation (ION GNSS+ 2024), September 2024 The final version is  available at https://doi.org/10.33012/2024.19864&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可视化方法，用于辅助分析师分类影响激光雷达扫描匹配的逆境模式，通过生成矢量场图揭示点云数据中的差异模式。&lt;h4&gt;背景&lt;/h4&gt;激光雷达扫描匹配过程中存在多种逆境模式影响数据质量，分析师需要有效方法来识别和分类这些模式。&lt;h4&gt;目的&lt;/h4&gt;开发一种离线可视化分析方法，帮助分析师识别和理解影响激光雷达扫描匹配的逆境机制。&lt;h4&gt;方法&lt;/h4&gt;提出一种生成矢量场图的可视化方法，该图能描述一对已配准点云之间的局部差异，揭示难以从原始数据中提取的模式。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟研究和现场实验验证，该方法能够帮助分析师识别和迭代移除逆境机制，逐步聚焦于更细微的数据差异。&lt;h4&gt;结论&lt;/h4&gt;所提出的可视化方法有效辅助了分析师对激光雷达扫描匹配中逆境模式的分类和分析，提高了数据处理的效率和准确性。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们介绍了一种可视化方法，用于帮助人类分析师分类影响激光雷达扫描匹配的逆境模式。我们的方法适用于离线分析而非实时分析。该方法生成一个矢量场图，用于描述一对已配准点云之间的局部差异。矢量场图能够揭示分析师难以从原始点云数据中提取的模式。在介绍我们的方法后，我们将该过程应用于两个概念验证示例：一个是模拟研究，另一个是现场实验。对于这两个数据集，人类分析师能够推理一系列逆境机制，并从原始数据中迭代地移除这些机制，以帮助将注意力集中在逐渐变小的差异上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.33012/2024.19864&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we introduce a visualization methodology to aid a human analystin classifying adversity modes that impact lidar scan matching. Our methodologyis intended for offline rather than real-time analysis. The method generates avector-field plot that characterizes local discrepancies between a pair ofregistered point clouds. The vector field plot reveals patterns that would bedifficult for the analyst to extract from raw point-cloud data. Afterintroducing our methodology, we apply the process to two proof-of-conceptexamples: one a simulation study and the other a field experiment. For bothdata sets, a human analyst was able to reason about a series of adversitymechanisms and iteratively remove those mechanisms from the raw data, to helpfocus attention on progressively smaller discrepancies.</description>
      <author>example@mail.com (Daniel Choate, Jason Rife)</author>
      <guid isPermaLink="false">2510.13619v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning</title>
      <link>http://arxiv.org/abs/2510.13307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于结构因果模型（SCM）的点云分割新类别发现方法，通过因果表示与推理的联合学习，解决仅使用已标记类别监督信息对新类别进行分割的问题。&lt;h4&gt;背景&lt;/h4&gt;点云分割中的新类别发现（3D-NCD）是一个挑战性问题，需要仅使用已标记（基础）3D类别的监督信息来学习能够分割未标记（新）3D类别的模型。&lt;h4&gt;目的&lt;/h4&gt;学习一个模型，仅使用已标记（基础）3D类别的监督信息，能够对未标记（新）3D类别进行分割。&lt;h4&gt;方法&lt;/h4&gt;引入结构因果模型（SCM）重新形式化3D-NCD问题，提出因果表示与推理的联合学习方法。通过SCM分析基础类别表示中的隐藏混杂因素以及基础类别和新类别之间的因果关系；设计消除混杂因素的因果表示原型；使用图结构建模基础类别因果表示原型与新类别原型之间的因果关系，实现从基础到新类别的因果推理。&lt;h4&gt;主要发现&lt;/h4&gt;粗略或统计相关性学习可能导致新类别推理的混淆；通过引入因果约束可以准确发现与类别对应的点云表示；所提出的方法在3D和2D NCD语义分割任务上表现出优越性。&lt;h4&gt;结论&lt;/h4&gt;基于结构因果模型的方法能够有效解决点云分割中的新类别发现问题，通过因果表示与推理的联合学习，实现仅使用已标记类别监督信息对新类别的准确分割。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们专注于点云分割的新类别发现（3D-NCD），旨在学习一个模型，仅使用已标记（基础）3D类别的监督信息，能够对未标记（新）3D类别进行分割。这项任务的关键在于建立点表示与基础类别标签之间的准确相关性，以及基础类别和新类别点之间的表示相关性。粗略或统计相关性学习可能导致新类别推理的混淆。如果在学习过程中施加因果关系作为强相关约束，应该能够准确发现与类别对应的本质点云表示。为此，我们引入结构因果模型（SCM）重新形式化3D-NCD问题，并提出一种新方法，即因果表示与推理的联合学习。具体而言，我们首先通过SCM分析基础类别表示中的隐藏混杂因素以及基础类别和新类别之间的因果关系。我们设计了一个消除混杂因素的因果表示原型，以捕获基础类别的因果表示。然后使用图结构建模基础类别因果表示原型与新类别原型之间的因果关系，实现从基础到新类别的因果推理。在3D和2D NCD语义分割任务上的大量实验和可视化结果证明了我们方法的优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决的是点云分割中的新类别发现问题，即如何仅使用已知类别的监督信息来训练模型，使其能够分割场景中未标记的新类别物体。这个问题在自动驾驶、机器人感知等真实场景中非常重要，因为这些环境中可能出现各种未预先定义的物体类别，传统'封闭世界'假设的方法无法应对这种开放世界环境，而新类别发现能够减少人工标注负担，使模型能够适应动态变化的环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法的局限性，指出它们倾向于学习捷径特征而非本质特征，且忽视了基类与新类别之间的因果关系。基于此，作者引入结构因果模型重新形式化问题，并借鉴了因果表示学习、结构因果模型、图卷积网络和生成对抗网络等现有工作。通过这些借鉴，作者设计了因果表示原型学习来消除混杂因素，并使用图结构建模基类与新类别间的因果关系，实现了从未知类别中学习更准确的分割。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过因果表示学习消除点云数据中的非因果特征，捕获基类的本质表示，并建立基类和新类别之间的因果关系模型，实现从已知到未知的知识迁移。整体流程分为三部分：1)因果表示原型学习，使用对抗训练消除混杂因素，生成基类的因果表示原型；2)因果推理图构建，创建包含基类和新类原型的图结构，设计因果自适应邻接矩阵和约束优化图结构；3)基于GCN的伪标签生成，利用图卷积网络处理优化后的图，通过多层传播和邻居聚合为新类别生成高质量伪标签。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将因果学习引入3D NCD领域，专注于学习因果关系而非统计相关性；2)提出因果表示原型学习，通过对抗机制消除混杂因素；3)提出基于图的因果推理方法，显式建模基类到新类别的因果路径。相比之前的工作，本文方法能够处理点云数据中的复杂因果关系，而非仅依赖表面特征相似性，通过因果推理更好地处理新类别的语义关系，减少错误分类，提高了在开放世界环境中的适应性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过引入因果表示学习和因果推理，首次解决了点云分割中新类别发现问题中的因果机制建模，实现了从未知类别中学习更准确、更鲁棒的语义分割。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation(3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classesusing only the supervision from labeled (base) 3D classes. The key to this taskis to setup the exact correlations between the point representations and theirbase class labels, as well as the representation correlations between thepoints from base and novel classes. A coarse or statistical correlationlearning may lead to the confusion in novel class inference. lf we impose acausal relationship as a strong correlated constraint upon the learningprocess, the essential point cloud representations that accurately correspondto the classes should be uncovered. To this end, we introduce a structuralcausal model (SCM) to re-formalize the 3D-NCD problem and propose a new method,i.e., Joint Learning of Causal Representation and Reasoning. Specifically, wefirst analyze hidden confounders in the base class representations and thecausal relationships between the base and novel classes through SCM. We devisea causal representation prototype that eliminates confounders to capture thecausal representations of base classes. A graph structure is then used to modelthe causal relationships between the base classes' causal representationprototypes and the novel class prototypes, enabling causal reasoning from baseto novel classes. Extensive experiments and visualization results on 3D and 2DNCD semantic segmentation demonstrate the superiorities of our method.</description>
      <author>example@mail.com (Yang Li, Aming Wu, Zihao Zhang, Yahong Han)</author>
      <guid isPermaLink="false">2510.13307v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping</title>
      <link>http://arxiv.org/abs/2510.13287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IROS Active Perception Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DAMM-LOAM的新型LiDAR SLAM系统，通过点云分类和退化感知算法解决了特征稀疏环境下的定位建图问题，显著提高了室内环境中的导航精度。&lt;h4&gt;背景&lt;/h4&gt;LiDAR SLAM系统对精确导航和环境重建至关重要。当前点对平面ICP算法在结构化、特征丰富的环境中表现良好，但在特征稀疏、重复几何结构和高频运动场景下表现不佳，导致6自由度姿态估计退化。虽然最先进算法通过添加额外传感模态应对挑战，但纯LiDAR解决方案在这种条件下仍有限制。&lt;h4&gt;目的&lt;/h4&gt;解决特征稀疏、重复几何结构和高频运动场景下的SLAM退化问题，提出一种新颖的退化感知多度量LiDAR里程计与建图(DAMM-LOAM)模块。&lt;h4&gt;方法&lt;/h4&gt;通过基于表面法线和邻域分析的点云分类提高建图精度，将点分类为地面、墙壁、屋顶、边缘和非平面点以实现准确对应；应用基于退化的加权最小二乘ICP算法进行精确里程计估计；实现基于ScanContext的后端以支持稳健的回环闭合。&lt;h4&gt;主要发现&lt;/h4&gt;DAMM-LOAM在里程计准确性方面有显著改进，特别是在长走廊等室内环境中表现突出。&lt;h4&gt;结论&lt;/h4&gt;DAMM-LOAM系统有效解决了传统LiDAR SLAM在特定场景下的退化问题，通过创新的点云分类和退化感知算法，提高了室内环境中的导航精度。&lt;h4&gt;翻译&lt;/h4&gt;激光雷达同步定位与建图系统对于在各种应用中实现精确导航和环境重建至关重要。尽管当前点对平面ICP算法在结构化、特征丰富的环境中能有效工作，但在特征稀疏、重复几何结构和高频运动场景下表现不佳。这会导致6自由度姿态估计的退化。大多数最先进算法通过结合额外的传感模态来解决这些挑战，但纯激光雷达解决方案在这种条件下仍然面临限制。为解决这些问题，我们提出了一种新颖的退化感知多度量激光雷达里程计与建图模块。我们的系统通过基于表面法线和邻域分析的点云分类提高了建图精度。点被分类为地面、墙壁、屋顶、边缘和非平面点，从而实现准确的对应关系。然后应用基于退化的加权最小二乘ICP算法进行精确的里程计估计。此外，实现了基于扫描上下文的后端以支持稳健的回环闭合。DAMM-LOAM在里程计准确性方面表现出显著改进，特别是在长走廊等室内环境中。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR SLAM系统在特征稀疏、重复几何结构和高频运动等场景下的退化问题，导致6自由度位姿估计不准确。这个问题很重要，因为许多实际应用场景（如走廊、隧道）都存在特征稀疏问题，而机器人导航、自动驾驶等领域需要在复杂环境中进行精确定位和地图构建，当前系统在这些挑战性场景中表现不佳，限制了技术的广泛应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析传统点对点或点对平面ICP算法的局限性，认识到在变化几何结构和特征稀疏环境下表现不佳的问题。设计方法时借鉴了现有工作：利用NV-LIOM的球形投影法线提取方法，但进一步进行几何分类；借鉴了条件数和特征值分析来检测退化，但设计了新的点级加权方案；并整合了现有的Scan Context算法作为后端。作者不是完全重新发明方法，而是在现有基础上进行了改进和整合创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合几何特征分类和退化感知的自适应加权来提高LiDAR SLAM系统在特征稀疏环境中的鲁棒性和准确性。整体流程包括：1)几何特征提取：将点云投影到球形范围图像，估计表面法线，并进行五类几何分类（地面、墙壁、屋顶、边缘、非平面点）；2)点云处理：自适应下采样并建立类别对应的点对；3)退化感知的位姿估计：分析Hessian矩阵特征值，为点分配权重，结合点对点和点对平面残差进行优化；4)后端处理：使用Scan Context进行回环检测和全局优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于法线图的语义特征提取：将点云分为五类而非传统的平面/非平面二分类；2)退化感知的逐点自适应加权：基于Hessian特征值分析为每个点分配权重，而非仅基于点类型数量；3)多度量残差整合：结合点对点和点对平面残差并动态调整权重；4)完整的端到端框架：整合几何特征提取、自适应加权优化和回环检测。相比之前工作，该方法提供了更细致的语义信息、更精确的退化处理和更全面的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DAMM-LOAM通过基于法线图的语义特征分类和退化感知的自适应加权，显著提高了LiDAR SLAM系统在特征稀疏环境（如长走廊）中的定位精度和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential forenabling precise navigation and environmental reconstruction across variousapplications. Although current point-to-plane ICP algorithms perform effec-tively in structured, feature-rich environments, they struggle in scenarioswith sparse features, repetitive geometric structures, and high-frequencymotion. This leads to degeneracy in 6- DOF pose estimation. Moststate-of-the-art algorithms address these challenges by incorporatingadditional sensing modalities, but LiDAR-only solutions continue to facelimitations under such conditions. To address these issues, we propose a novelDegeneracy-Aware Multi-Metric LiDAR Odometry and Map- ping (DAMM-LOAM) module.Our system improves mapping accuracy through point cloud classification basedon surface normals and neighborhood analysis. Points are classified intoground, walls, roof, edges, and non-planar points, enabling accuratecorrespondences. A Degeneracy-based weighted least squares-based ICP algorithmis then applied for accurate odom- etry estimation. Additionally, a ScanContext based back-end is implemented to support robust loop closures.DAMM-LOAM demonstrates significant improvements in odometry accuracy,especially in indoor environments such as long corridors</description>
      <author>example@mail.com (Nishant Chandna, Akshat Kaushal)</author>
      <guid isPermaLink="false">2510.13287v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-based Adaptation in Large-scale Vision Models: A Survey</title>
      <link>http://arxiv.org/abs/2510.13219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文是一篇关于视觉提示(Visual Prompting, VP)和视觉提示调优(Visual Prompt Tuning, VPT)的综合调查，提出了一种称为基于提示的适应(Prompt-based Adaptation, PA)的统一框架，对现有方法进行了分类，并探讨了PA在不同领域的应用、挑战和未来方向。&lt;h4&gt;背景&lt;/h4&gt;在计算机视觉领域，VP和VPT作为大规模视觉模型适应的轻量级有效替代方法，在'预训练后微调'范式中迅速发展。然而，当前研究中VP和VPT经常被互换使用，缺乏系统区分这些技术及其各自应用的明确界限。&lt;h4&gt;目的&lt;/h4&gt;重新审视VP和VPT的设计，将它们概念化为一个统一的PA框架，提供清晰的方法分类，并探索PA在不同领域的应用、挑战和未来方向，为研究人员和实践者提供明确的路线图。&lt;h4&gt;方法&lt;/h4&gt;提供了一种分类法，将现有方法分为可学习提示、生成提示和非可学习提示，并按注入粒度（像素级和令牌级）进一步组织。同时检查了PA在医学成像、3D点云和视觉语言任务等领域的整合，以及其在测试时适应和可信AI中的作用。&lt;h4&gt;主要发现&lt;/h4&gt;PA在医学成像、3D点云和视觉语言任务等不同领域有广泛应用，并且在测试时适应和可信AI中发挥重要作用。作者总结了当前基准，并确定了关键挑战和未来方向。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是第一篇专门针对PA的方法和应用的综合调查，旨在为研究人员和实践者提供清晰的路线图，以理解和探索PA相关研究的不断发展的格局。&lt;h4&gt;翻译&lt;/h4&gt;在计算机视觉中，视觉提示(Visual Prompting, VP)和视觉提示调优(Visual Prompt Tuning, VPT)最近已经出现作为轻量级且有效的替代方法，用于在'预训练后微调'范式中适应大规模视觉模型。然而，尽管进展迅速，它们的概念边界仍然模糊，因为VP和VPT在当前研究中经常被互换使用，反映了这些技术及其各自应用之间缺乏系统区分。在本调查中，我们从基本原理重新审视VP和VPT的设计，并将它们概念化为一个称为基于提示的适应(Prompt-based Adaptation, PA)的统一框架。我们提供了一个分类法，将现有方法分为可学习提示、生成提示和非可学习提示，并按注入粒度（像素级和令牌级）进一步组织。除了核心方法外，我们检查了PA在医学成像、3D点云和视觉语言任务等不同领域的整合，以及其在测试时适应和可信AI中的作用。我们还总结了当前基准，并确定了关键挑战和未来方向。据我们所知，我们是第一个专门针对PA的方法和应用的全面调查，考虑其独特特征。我们的调查旨在为所有领域的研究人员和实践者提供清晰的路线图，以理解和探索PA相关研究的不断发展的格局。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) haverecently emerged as lightweight and effective alternatives to full fine-tuningfor adapting large-scale vision models within the ``pretrain-then-finetune''paradigm. However, despite rapid progress, their conceptual boundaries remainblurred, as VP and VPT are frequently used interchangeably in current research,reflecting a lack of systematic distinction between these techniques and theirrespective applications. In this survey, we revisit the designs of VP and VPTfrom first principles, and conceptualize them within a unified framework termedPrompt-based Adaptation (PA). We provide a taxonomy that categorizes existingmethods into learnable, generative, and non-learnable prompts, and furtherorganizes them by injection granularity -- pixel-level and token-level. Beyondthe core methodologies, we examine PA's integrations across diverse domains,including medical imaging, 3D point clouds, and vision-language tasks, as wellas its role in test-time adaptation and trustworthy AI. We also summarizecurrent benchmarks and identify key challenges and future directions. To thebest of our knowledge, we are the first comprehensive survey dedicated to PA'smethodologies and applications in light of their distinct characteristics. Oursurvey aims to provide a clear roadmap for researchers and practitioners in allarea to understand and explore the evolving landscape of PA-related research.</description>
      <author>example@mail.com (Xi Xiao, Yunbei Zhang, Lin Zhao, Yiyang Liu, Xiaoying Liao, Zheda Mai, Xingjian Li, Xiao Wang, Hao Xu, Jihun Hamm, Xue Lin, Min Xu, Qifan Wang, Tianyang Wang, Cheng Han)</author>
      <guid isPermaLink="false">2510.13219v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>ADPerf: Investigating and Testing Performance in Autonomous Driving Systems</title>
      <link>http://arxiv.org/abs/2510.13078v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, accepted by ASE 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了自动驾驶系统中障碍物检测模块的性能和延迟问题，开发了一个名为ADPerf的工具用于测试和暴露检测延迟，并评估了其对系统整体可靠性的影响。&lt;h4&gt;背景&lt;/h4&gt;障碍物检测对自动驾驶系统运行至关重要，系统依赖多种传感器结合深度学习模型进行时间敏感决策。然而，障碍物检测模块的延迟及其对LiDAR点云数据变化的适应性尚未被充分了解。&lt;h4&gt;目的&lt;/h4&gt;首次全面测量和建模Apollo和Autoware两个行业级自动驾驶系统中障碍物检测模块的性能，开发ADPerf工具生成测试用例以暴露检测延迟增加，并评估其对后续模块的影响。&lt;h4&gt;方法&lt;/h4&gt;对Apollo和Autoware系统中的障碍物检测模块进行性能测量和建模，开发ADPerf工具生成真实点云数据测试用例，对3D障碍物检测模块进行压力测试，并评估这些测试对轨迹预测模块的传播影响。&lt;h4&gt;主要发现&lt;/h4&gt;障碍物检测组件（特别是3D障碍物检测）的性能测试非常必要，障碍物检测可能成为自动驾驶系统延迟增加的主要瓶颈，延迟增加的不利影响会传播到其他模块，降低系统整体可靠性。&lt;h4&gt;结论&lt;/h4&gt;需要对障碍物检测组件进行性能测试，特别是3D障碍物检测，因为它们是自动驾驶系统延迟增加的主要瓶颈，会进一步影响其他模块，降低整体系统可靠性。&lt;h4&gt;翻译&lt;/h4&gt;障碍物检测对自动驾驶系统的运行至关重要，这些系统依赖多种传感器（如摄像头和LiDAR）结合代码逻辑和深度学习模型来检测障碍物，以便进行时间敏感的决策。因此，障碍物检测延迟对自动驾驶系统的安全性和有效性至关重要。然而，障碍物检测模块的延迟及其对LiDAR点云数据各种变化的适应性尚未被充分了解。在这项工作中，我们首次对两个行业级自动驾驶系统（即Apollo和Autoware）中的障碍物检测模块性能进行了全面的测量和建模研究。从这项研究中，我们引入了ADPerf，这是一个旨在生成真实点云数据测试用例的工具，这些测试用例可以暴露检测延迟的增加。延迟降低会减少检测到障碍物的可用性，并对自动驾驶系统中后续模块的能力造成压力，即这些模块可能受到障碍物检测延迟增加的负面影响。我们将ADPerf应用于压力测试自动驾驶系统中广泛使用的3D障碍物检测模块的性能，以及此类测试对轨迹预测模块的传播影响。我们的评估强调了需要对障碍物检测组件进行性能测试，特别是3D障碍物检测，因为它们可能成为自动驾驶系统延迟增加的主要瓶颈。这种不利结果还会进一步传播到其他模块，降低自动驾驶系统的整体可靠性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统中障碍物检测模块的性能问题，特别是延迟(latency)问题。这个问题在现实中非常重要，因为障碍物检测延迟直接影响自动驾驶系统的安全性和有效性；延迟过大会导致系统无法及时做出决策，就像未检测到障碍物一样危险。同时，现有研究大多关注检测器的准确性和鲁棒性，而对其性能的研究相对不足，这导致自动驾驶系统在实际部署中可能存在未被发现性能瓶颈的风险。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了自动驾驶系统架构，识别出感知模块(特别是3D障碍物检测)是性能瓶颈。他们通过排队网络和排队Petri网对Apollo和Autoware系统进行性能建模，确认了3D障碍物检测的延迟问题。基于这些发现，他们设计了ADPerf工具，通过三种简单方法修改点云数据来增加检测延迟：添加障碍物边界外的噪声、添加新障碍物、移动现有障碍物。作者借鉴了现有的性能测试技术和障碍物检测鲁棒性测试方法，但专注于性能而非准确性，与现有的对抗攻击方法(如SlowLidar)相比，ADPerf采用更简单、更现实的修改方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过生成能增加3D障碍物检测延迟的测试场景，评估自动驾驶系统在性能压力下的行为及其对后续模块的影响。整体实现流程包括：1)数据准备，从真实世界驾驶场景数据集中提取点云表示和障碍物历史数据；2)测试场景生成，通过添加噪声、添加障碍物或移动障碍物来修改点云；3)模型执行与延迟测量，在修改和未修改的点云上运行检测模型并测量延迟；4)帧可用性估计，基于检测延迟估计哪些帧会被丢弃；5)轨迹预测评估，分析检测延迟对轨迹预测模块的级联影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次对自动驾驶系统障碍物检测模块性能进行综合研究；2)提出ADPerf工具，专门用于生成性能测试用例；3)研究性能问题的级联影响，关注检测延迟对整个系统的影响；4)采用更现实的测试方法。相比之前的工作，本文的关注点从准确性转向性能，方法上从复杂的对抗攻击转向简单的点云修改，评估范围从单个模块扩展到整个系统，且生成的测试场景更接近真实世界，具有更高的实用价值。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了ADPerf工具，通过生成增加障碍物检测延迟的测试场景，首次系统性地研究了自动驾驶系统中障碍物检测模块的性能瓶颈及其对整体系统可靠性的影响。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Obstacle detection is crucial to the operation of autonomous driving systems,which rely on multiple sensors, such as cameras and LiDARs, combined with codelogic and deep learning models to detect obstacles for time-sensitivedecisions. Consequently, obstacle detection latency is critical to the safetyand effectiveness of autonomous driving systems. However, the latency of theobstacle detection module and its resilience to various changes in the LiDARpoint cloud data are not yet fully understood. In this work, we present thefirst comprehensive investigation on measuring and modeling the performance ofthe obstacle detection modules in two industry-grade autonomous drivingsystems, i.e., Apollo and Autoware. Learning from this investigation, weintroduce ADPerf, a tool that aims to generate realistic point cloud data testcases that can expose increased detection latency. Increasing latency decreasesthe availability of the detected obstacles and stresses the capabilities ofsubsequent modules in autonomous driving systems, i.e., the modules may benegatively impacted by the increased latency in obstacle detection.  We applied ADPerf to stress-test the performance of widely used 3D obstacledetection modules in autonomous driving systems, as well as the propagation ofsuch tests on trajectory prediction modules. Our evaluation highlights the needto conduct performance testing of obstacle detection components, especially 3Dobstacle detection, as they can be a major bottleneck to increased latency ofthe autonomous driving system. Such an adverse outcome will also furtherpropagate to other modules, reducing the overall reliability of autonomousdriving systems.</description>
      <author>example@mail.com (Tri Minh-Triet Pham, Diego Elias Costa, Weiyi Shang, Jinqiu Yang)</author>
      <guid isPermaLink="false">2510.13078v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations</title>
      <link>http://arxiv.org/abs/2510.13774v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UrbanFusion是一个地理基础模型(GeoFM)，采用随机多模态融合(SMF)技术，能够有效整合多种地理空间数据，在预测城市现象方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;预测城市现象如房价和公共健康指标需要有效整合各种地理空间数据。当前方法主要使用任务特定模型，而最近的用于空间表示的基础模型通常只支持有限模态，缺乏多模态融合能力。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有方法的局限性，开发一个能够处理多种地理数据模态并具有强大泛化能力的地理基础模型。&lt;h4&gt;方法&lt;/h4&gt;UrbanFusion采用模态特定编码器处理街景图像、遥感数据、地图和兴趣点(POIs)数据，并通过基于Transformer的融合模块整合这些多模态输入，学习统一的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在全球56个城市41个任务的评估中，UrbanFusion表现出强大的泛化能力和预测性能：1)在位置编码方面优于之前的基础模型；2)在推理过程中允许多模态输入；3)对训练中未见过的区域泛化良好。&lt;h4&gt;结论&lt;/h4&gt;UrbanFusion可在预训练和推理过程中灵活利用任何可用模态的子集，使模型在不同数据可用性场景下具有广泛的适用性，所有源代码已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;预测城市现象如房价和公共健康指标需要有效整合各种地理空间数据。当前方法主要使用任务特定的模型，而最近的用于空间表示的基础模型通常只支持有限的模态，且缺乏多模态融合能力。为了克服这些挑战，我们提出了UrbanFusion，这是一个具有随机多模态融合(SMF)的地理基础模型(GeoFM)。该框架采用模态特定编码器处理不同类型的输入，包括街景图像、遥感数据、地图和兴趣点(POIs)数据。这些多模态输入通过基于Transformer的融合模块进行整合，学习统一的表示。在全球56个城市41个任务的广泛评估中，UrbanFusion与最先进的GeoAI模型相比表现出强大的泛化能力和预测性能。具体来说，它1)在位置编码方面优于之前的基础模型；2)在推理过程中允许多模态输入；3)对训练中未见过的区域泛化良好。UrbanFusion可以在预训练和推理过程中灵活利用任何可用模态的子集，使模型在不同数据可用性场景下具有广泛的适用性。所有源代码均可通过https://github.com/DominikM198/UrbanFusion获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forecasting urban phenomena such as housing prices and public healthindicators requires the effective integration of various geospatial data.Current methods primarily utilize task-specific models, while recent foundationmodels for spatial representations often support only limited modalities andlack multimodal fusion capabilities. To overcome these challenges, we presentUrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic MultimodalFusion (SMF). The framework employs modality-specific encoders to processdifferent types of inputs, including street view imagery, remote sensing data,cartographic maps, and points of interest (POIs) data. These multimodal inputsare integrated via a Transformer-based fusion module that learns unifiedrepresentations. An extensive evaluation across 41 tasks in 56 cities worldwidedemonstrates UrbanFusion's strong generalization and predictive performancecompared to state-of-the-art GeoAI models. Specifically, it 1) outperformsprior foundation models on location-encoding, 2) allows multimodal input duringinference, and 3) generalizes well to regions unseen during training.UrbanFusion can flexibly utilize any subset of available modalities for a givenlocation during both pretraining and inference, enabling broad applicabilityacross diverse data availability scenarios. All source code is available athttps://github.com/DominikM198/UrbanFusion.</description>
      <author>example@mail.com (Dominik J. Mühlematter, Lin Che, Ye Hong, Martin Raubal, Nina Wiedemann)</author>
      <guid isPermaLink="false">2510.13774v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Vision Transformers for Functional MRI with Flat Maps</title>
      <link>http://arxiv.org/abs/2510.13768v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 Workshop, Foundation Models for the Brain and Body;  Code: https://github.com/MedARC-AI/fmri-fm; Discord:  https://discord.gg/tVR4TWnRM9&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了将现代深度学习架构适应功能磁共振成像(fMRI)的方法，通过将4D体积fMRI数据转换为2D平面图视频，并使用时空掩码自编码器框架训练视觉Transformer模型。&lt;h4&gt;背景&lt;/h4&gt;现代深度学习架构如何适应功能磁共振成像(fMRI)是一个关键问题，需要解决fMRI与自然图像之间的模态差距。&lt;h4&gt;目的&lt;/h4&gt;研究如何将fMRI数据表示为模型输入，构建fMRI数据的基础模型。&lt;h4&gt;方法&lt;/h4&gt;将4D体积fMRI数据转换为2D fMRI活动平面图视频，使用时空掩码自编码器框架在人类连接体项目的2.3K小时fMRI平面图视频上训练视觉Transformer，并进行掩码建模和下游分类基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;掩码fMRI建模性能随数据集大小严格遵循幂律缩放规律而提高；模型能够学习丰富的表示，支持跨受试者的精细状态解码和跨脑状态变化的受试者特异性特征解码。&lt;h4&gt;结论&lt;/h4&gt;这是构建fMRI数据基础模型的开放科学项目的一部分，代码和数据已公开共享。&lt;h4&gt;翻译&lt;/h4&gt;将现代深度学习架构适应功能磁共振成像(fMRI)的一个关键问题是如何为模型输入表示数据。为弥合fMRI与自然图像之间的模态差距，我们将4D体积fMRI数据转换为2D fMRI活动平面图视频。我们在人类连接体项目的2.3K小时fMRI平面图视频上使用时空掩码自编码器框架训练视觉Transformer。我们观察到，根据严格的幂律缩放规律，掩码fMRI建模性能随数据集大小增加而提高。下游分类基准测试表明，我们的模型学习了丰富的表示，既支持跨受试者的精细状态解码，也支持跨脑状态变化的受试者特异性特征解码。这项工作是构建fMRI数据基础模型的开放科学项目的一部分。我们的代码和数据可在https://github.com/MedARC-AI/fmri-fm获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A key question for adapting modern deep learning architectures to functionalMRI (fMRI) is how to represent the data for model input. To bridge the modalitygap between fMRI and natural images, we transform the 4D volumetric fMRI datainto videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3Khours of fMRI flat map videos from the Human Connectome Project using thespatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRImodeling performance improves with dataset size according to a strict powerscaling law. Downstream classification benchmarks show that our model learnsrich representations supporting both fine-grained state decoding acrosssubjects, as well as subject-specific trait decoding across changes in brainstate. This work is part of an ongoing open science project to build foundationmodels for fMRI data. Our code and datasets are available athttps://github.com/MedARC-AI/fmri-fm.</description>
      <author>example@mail.com (Connor Lane, Daniel Z. Kaplan, Tanishq Mathew Abraham, Paul S. Scotti)</author>
      <guid isPermaLink="false">2510.13768v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching</title>
      <link>http://arxiv.org/abs/2510.13721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NExT-OMNI是一个开源的全模态基础模型，通过离散流范式实现统一建模，支持任意到任意的跨模态理解与生成，在多模态生成、理解、多轮交互和跨模态检索方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;下一代多模态基础模型将成为人工通用智能系统的核心组件，但现有多模态模型受限于自回归架构，难以平衡理解与生成能力；混合和解耦策略虽有探索，但冗余设计限制了在广泛场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个支持任何到任何跨模态生成和多轮交互的多模态基础模型，克服现有模型的局限性，实现更高效、更广泛的应用。&lt;h4&gt;方法&lt;/h4&gt;引入NExT-OMNI模型，利用度量诱导的概率路径和动力学最优速度，通过离散流范式实现统一建模，并在大规模交错文本、图像、视频和音频数据上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;NExT-OMNI在多模态生成和理解基准测试中具有竞争力，在多轮多模态交互和跨模态检索方面优于之前的统一模型，突显了其作为下一代多模态基础模型的架构优势。&lt;h4&gt;结论&lt;/h4&gt;NExT-OMNI通过简洁的统一表示而非任务解耦设计，实现了更广泛的应用场景，为促进进一步研究，已公开训练细节、数据协议，并开源了代码和模型检查点。&lt;h4&gt;翻译&lt;/h4&gt;能够进行任意到任意跨模态生成和多轮交互的下一代多模态基础模型将成为人工通用智能系统的核心组件，在人机交互中发挥关键作用。然而，大多数现有的多模态模型仍受限于自回归架构，其固有局限性阻碍了理解与生成能力的平衡整合。尽管已经探索了混合和解耦策略来在统一框架内分别解决这些问题，但这些冗余、非集成的设计限制了它们在更广泛场景（如跨模态检索）中的适用性。在本工作中，我们引入了NExT-OMNI，一个开源的全模态基础模型，通过离散流范式实现统一建模。通过利用度量诱导的概率路径和动力学最优速度，NExT-OMNI原生支持任意到任意的理解和生成，同时通过简洁的统一表示而非任务解耦设计，实现更广泛的应用场景，并提高响应效率。在大型交错文本、图像、视频和音频数据上训练后，NExT-OMNI在多模态生成和理解基准测试中具有竞争力，同时在多轮多模态交互和跨模态检索方面优于之前的统一模型，突显了其作为下一代多模态基础模型的架构优势。为了促进进一步研究，我们发布了训练细节、数据协议，并开源了代码和模型检查点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next-generation multimodal foundation models capable of any-to-anycross-modal generation and multi-turn interaction will serve as core componentsof artificial general intelligence systems, playing a pivotal role inhuman-machine interaction. However, most existing multimodal models remainconstrained by autoregressive architectures, whose inherent limitations preventa balanced integration of understanding and generation capabilities. Althoughhybrid and decoupling strategies have been explored to address these taskswithin unified frameworks separately, their redundant, non-integrated designslimit their applicability to broader scenarios, such as cross-modalretrieval.In this work, we introduce NExT-OMNI, an open-source omnimodalfoundation model that achieves unified modeling through discrete flowparadigms. By leveraging metric-induced probability paths and kinetic optimalvelocities, NExT-OMNI natively supports any-to-any understanding and generationwith enhanced response efficiency, while enabling broader application scenariosthrough concise unified representations rather than task-decoupled designs.Trained on large-scale interleaved text, image, video, and audio data,NExT-OMNI delivers competitive performance on multimodal generation andunderstanding benchmarks, while outperforming prior unified models inmulti-turn multimodal interaction and cross-modal retrieval, highlighting itsarchitectural advantages as a next-generation multimodal foundation model. Toadvance further research, we release training details, data protocols, andopen-source both the code and model checkpoints.</description>
      <author>example@mail.com (Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua)</author>
      <guid isPermaLink="false">2510.13721v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Axial Neural Networks for Dimension-Free Foundation Models</title>
      <link>http://arxiv.org/abs/2510.13665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种轴向神经网络(XNN)架构，解决了在物理数据上训练基础模型时面临的维度变化挑战，使模型能够有效处理不同维度的偏微分方程数据，同时保持计算效率和性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型在AI中的出现显著推进了通用学习，在零样本推理和上下文学习方面表现出色。然而，在物理数据（包括偏微分方程PDEs的解）上训练此类模型面临独特挑战，因为不同系统的维度各不相同。&lt;h4&gt;目的&lt;/h4&gt;提出一种维度不可知的神经网络架构，解决传统方法在处理不同维度数据时效率低下的问题。&lt;h4&gt;方法&lt;/h4&gt;提出轴向神经网络(XNN)，受Deep Sets和图神经网络等参数共享结构的启发。将现有的PDE基础模型转换为轴向神经网络，并在三种训练场景下评估性能：从头开始训练、在多个PDE上预训练以及在单个PDE上微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，XNNs与原始模型表现相当，并且对未见维度表现出更好的泛化能力，突显了多维预训练对基础模型的重要性。&lt;h4&gt;结论&lt;/h4&gt;XNN架构解决了在物理数据上训练基础模型的维度挑战，同时保持了性能和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在AI中的出现显著推进了通用学习，使零样本推理和上下文学习能力显著提升。然而，在包括偏微分方程(PDEs)解在内的物理数据上训练此类模型，由于不同系统间维度的变化，带来了独特挑战。传统方法要么固定最大维度，要么为不同维度使用单独的编码器，导致效率低下。为此，我们提出了一种维度不可知的神经网络架构——轴向神经网络(XNN)，其灵感来自Deep Sets和图神经网络等参数共享结构。XNN能够在保持计算效率的同时，推广到变化的张量维度。我们将现有的PDE基础模型转换为轴向神经网络，并在三种训练场景下评估其性能：从头开始训练、在多个PDE上预训练以及在单个PDE上微调。实验表明，XNNs与原始模型表现相当，并且对未见维度表现出更好的泛化能力，突显了多维预训练对基础模型的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of foundation models in AI has significantly advancedgeneral-purpose learning, enabling remarkable capabilities in zero-shotinference and in-context learning. However, training such models on physicsdata, including solutions to partial differential equations (PDEs), poses aunique challenge due to varying dimensionalities across different systems.Traditional approaches either fix a maximum dimension or employ separateencoders for different dimensionalities, resulting in inefficiencies. Toaddress this, we propose a dimension-agnostic neural network architecture, theAxial Neural Network (XNN), inspired by parameter-sharing structures such asDeep Sets and Graph Neural Networks. XNN generalizes across varying tensordimensions while maintaining computational efficiency. We convert existing PDEfoundation models into axial neural networks and evaluate their performanceacross three training scenarios: training from scratch, pretraining on multiplePDEs, and fine-tuning on a single PDE. Our experiments show that XNNs performcompetitively with original models and exhibit superior generalization tounseen dimensions, highlighting the importance of multidimensional pretrainingfor foundation models.</description>
      <author>example@mail.com (Hyunsu Kim, Jonggeon Park, Joan Bruna, Hongseok Yang, Juho Lee)</author>
      <guid isPermaLink="false">2510.13665v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Time Series Foundation Models: Benchmarking Challenges and Requirements</title>
      <link>http://arxiv.org/abs/2510.13654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列基础模型(TSFMs)是一种新的时间序列预测范式，具有零样本预测能力，但其评估面临多个挑战，包括数据集代表性问题、缺乏时空评估、信息泄露风险和全局模式记忆问题。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)代表了一种新的时间序列预测范式，提供无需领域特定预训练或微调的零样本预测能力。与大型语言模型(LLMs)类似，随着训练集的不断扩大，确保基准测试数据的完整性变得越来越困难。&lt;h4&gt;目的&lt;/h4&gt;调查现有TSFM评估的挑战，并提出改进评估方法的建议，以保障TSFM评估的完整性。&lt;h4&gt;方法&lt;/h4&gt;通过调查现有TSFM评估实践，分析数据分区问题，并提出新的评估方法建议，如在真正外来的未来数据上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;现有TSFM评估存在多个挑战，包括基准数据集的代表性问题、缺乏时空评估、信息泄露风险和全局模式记忆问题。此外，关于数据分区的普遍混乱可能导致性能估计膨胀和全球知识错误地转移到局部时间序列。&lt;h4&gt;结论&lt;/h4&gt;需要开发强大的评估方法来防止在LLM和经典时间序列基准测试中已经观察到的陷阱，并呼吁研究社区设计新的、有原则的评估方法，如在真正外来的未来数据上进行评估，以保障TSFM评估的完整性。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)代表了一种新的时间序列预测范式，提供无需领域特定预训练或微调的零样本预测能力。然而，与大型语言模型(LLMs)一样，评估TSFMs很棘手，因为随着训练集的不断扩展，确保基准测试数据的完整性变得越来越具有挑战性。我们对现有TSFM评估的调查揭示了多个挑战，从基准数据集的代表性、缺乏时空评估，到由于数据集重叠和不透明导致的信息泄露风险，以及由经济危机或疫情等外部冲击引起的全局模式记忆问题。我们的发现揭示了关于数据分区的普遍混乱，这可能导致性能估计膨胀和全球知识错误地转移到局部时间序列。我们呼吁开发强大的评估方法，以防止在LLM和经典时间序列基准测试中已经观察到的陷阱，并呼吁研究社区设计新的、有原则的方法，如在真正外来的未来数据上进行评估，以保障TSFM评估的完整性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time Series Foundation Models (TSFMs) represent a new paradigm for timeseries forecasting, offering zero-shot forecasting capabilities without theneed for domain-specific pre-training or fine-tuning. However, as with LargeLanguage Models (LLMs), evaluating TSFMs is tricky, as with ever more extensivetraining sets, it becomes more and more challenging to ensure the integrity ofbenchmarking data. Our investigation of existing TSFM evaluation highlightsmultiple challenges, ranging from the representativeness of the benchmarkdatasets, over the lack of spatiotemporal evaluation, to risks of informationleakage due to overlapping and obscure datasets, and the memorization of globalpatterns caused by external shocks like economic crises or pandemics. Ourfindings reveal widespread confusion regarding data partitions, riskinginflated performance estimates and incorrect transfer of global knowledge tolocal time series. We argue for the development of robust evaluationmethodologies to prevent pitfalls already observed in LLM and classical timeseries benchmarking, and call upon the research community to design new,principled approaches, such as evaluations on truly out-of-sample future data,to safeguard the integrity of TSFM assessment.</description>
      <author>example@mail.com (Marcel Meyer, Sascha Kaltenpoth, Kevin Zalipski, Oliver Müller)</author>
      <guid isPermaLink="false">2510.13654v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection</title>
      <link>http://arxiv.org/abs/2510.13643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了基于DINOv2等基础模型的小样本异常检测器的对抗性扰动敏感性和不确定性校准问题。作者通过在冻结的DINOv2特征上附加轻量级线性头创建对抗性攻击，评估了FGSM攻击的影响，并发现微小扰动可显著降低检测性能。同时，原始异常分数校准性较差，通过应用Platt缩放方法，作者提出了实用的攻击检测机制并降低了校准误差。&lt;h4&gt;背景&lt;/h4&gt;基础模型如DINOv2在小样本异常检测中表现出强大性能，但两个关键问题尚未得到研究：(1)这些检测器对抗性扰动的敏感性如何；(2)它们的异常分数在多大程度上反映了校准的不确定性。&lt;h4&gt;目的&lt;/h4&gt;研究DINOv2等基础模型在小样本异常检测中的对抗性鲁棒性和不确定性校准问题，并提出实用的攻击检测机制，以提高异常检测系统的可信度和安全性。&lt;h4&gt;方法&lt;/h4&gt;基于AnomalyDINO（一种在DINOv2特征上的训练深度最近邻检测器），作者在冻结的DINOv2特征上附加轻量级线性头仅用于创建对抗性扰动。评估FGSM攻击在MVTec-AD和VisA数据集上的影响，并应用后验Platt缩放方法对异常分数进行不确定性估计。&lt;h4&gt;主要发现&lt;/h4&gt;1) 微小对抗性扰动显著降低检测性能(F1、AUROC、AP和G-mean指标均下降)；2) 扰动可在特征空间翻转最近邻关系，导致有把握的错误分类；3) 原始异常分数校准性较差，置信度与正确性存在差距；4) Platt缩放得到的校验后验分布在对抗性扰动输入上产生更高预测熵；5) 该方法可用于实用攻击检测机制，同时降低校准误差(ECE)。&lt;h4&gt;结论&lt;/h4&gt;DINOv2基础的小样本异常检测器存在具体脆弱性，对抗性鲁棒性和有原则的不确定性量化不是可选的附加功能，而是异常检测系统可信度和为真实世界部署做好准备所必需的基本能力。&lt;h4&gt;翻译&lt;/h4&gt;基础模型如DINOv2在小样本异常检测中表现出强大的性能，但两个关键问题尚未得到研究：(i)这些检测器对抗性扰动的敏感性如何；(ii)它们的异常分数在多大程度上反映了校准的不确定性。基于AnomalyDINO（一种在DINOv2特征上的训练深度最近邻检测器），我们进行了此设置中对抗性攻击和不确定性估计的首次系统性研究之一。为了在保持测试时间行为的同时实现白盒梯度攻击，我们仅在创建扰动时为冻结的DINOv2特征附加了一个轻量级线性头。使用这种启发式方法，我们评估了FGSM在MVTec-AD和VisA数据集上的影响，并观察到F1、AUROC、AP和G-mean指标的一致下降，表明微小的扰动可以在特征空间中翻转最近邻关系，导致有把握的错误分类。除了鲁棒性外，我们还探测了可靠性，发现原始异常分数的校准性较差，揭示了置信度与正确性之间的差距，这限制了安全关键应用。作为迈向可信度的简单、强基线，我们对异常分数应用了后验Platt缩放进行不确定性估计。所得的校验后验分布在对抗性扰动输入上产生显著更高的预测熵，能够用于实用的攻击检测机制，同时降低校准误差（ECE）。我们的研究结果揭示了DINOv2基础小样本异常检测器的具体脆弱性，并为鲁棒、不确定性感知的异常检测建立了评估协议和基线。我们认为，对抗性鲁棒性和有原则的不确定性量化不是可选的附加功能，而是异常检测系统可信度和为真实世界部署做好准备所必需的基本能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models such as DINOv2 have shown strong performance in few-shotanomaly detection, yet two key questions remain unexamined: (i) how susceptibleare these detectors to adversarial perturbations; and (ii) how well do theiranomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, atraining-free deep nearest-neighbor detector over DINOv2 features, we presentone of the first systematic studies of adversarial attacks and uncertaintyestimation in this setting. To enable white-box gradient attacks whilepreserving test-time behavior, we attach a lightweight linear head to frozenDINOv2 features only for crafting perturbations. Using this heuristic, weevaluate the impact of FGSM across the MVTec-AD and VisA datasets and observeconsistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptibleperturbations can flip nearest-neighbor relations in feature space to induceconfident misclassification. Complementing robustness, we probe reliability andfind that raw anomaly scores are poorly calibrated, revealing a gap betweenconfidence and correctness that limits safety-critical use. As a simple, strongbaseline toward trustworthiness, we apply post-hoc Platt scaling to the anomalyscores for uncertainty estimation. The resulting calibrated posteriors yieldsignificantly higher predictive entropy on adversarially perturbed inputs thanon clean ones, enabling a practical flagging mechanism for attack detectionwhile reducing calibration error (ECE). Our findings surface concretevulnerabilities in DINOv2-based few-shot anomaly detectors and establish anevaluation protocol and baseline for robust, uncertainty-aware anomalydetection. We argue that adversarial robustness and principled uncertaintyquantification are not optional add-ons but essential capabilities if anomalydetection systems are to be trustworthy and ready for real-world deployment.</description>
      <author>example@mail.com (Akib Mohammed Khan, Bartosz Krawczyk)</author>
      <guid isPermaLink="false">2510.13643v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>The Role of Computing Resources in Publishing Foundation Model Research</title>
      <link>http://arxiv.org/abs/2510.13621v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了计算资源与基础模型科学进展之间的关系，发现增加计算资源与国家资金分配和引用量相关，但与研究环境、领域或研究方法无强相关性。&lt;h4&gt;背景&lt;/h4&gt;前沿的人工智能研究需要大量资源，包括图形处理器(GPU)、数据和人力资源。&lt;h4&gt;目的&lt;/h4&gt;评估这些资源与基础模型科学进展之间的关系。&lt;h4&gt;方法&lt;/h4&gt;回顾了2022年至2024年间发表的6517篇基础模型论文，并调查了229位第一作者关于计算资源对科研产出影响的情况。&lt;h4&gt;主要发现&lt;/h4&gt;增加的计算资源与国家资金分配和引用量相关，但未发现与研究环境(学术界或工业界)、领域或研究方法有强相关性。&lt;h4&gt;结论&lt;/h4&gt;建议个人和机构专注于创建共享且负担得起的计算机会，以减少资源不足研究者的入门障碍，这些步骤可以帮助扩大基础模型研究的参与度，促进思想贡献者的多样性，并维持人工智能的创新和进步。&lt;h4&gt;翻译&lt;/h4&gt;尖端的人工智能研究需要大量资源，包括图形处理器、数据和人力资源。在本文中，我们评估了这些资源与基础模型科学进展之间的关系。我们回顾了2022年至2024年间发表的6517篇基础模型论文，并对229位第一作者进行了调查，了解计算资源对科研产出的影响。我们发现，增加的计算资源与国家资金分配和引用量相关，但我们的研究结果未观察到与研究环境(学术界或工业界)、领域或研究方法有强相关性。我们建议个人和机构专注于创建共享且负担得起的计算机会，以降低资源不足研究者的入门门槛。这些步骤可以帮助扩大基础模型研究的参与度，促进思想贡献者的多样性，并维持人工智能的创新和进步。数据将在https://mit-calc.csail.mit.edu/提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cutting-edge research in Artificial Intelligence (AI) requires considerableresources, including Graphics Processing Units (GPUs), data, and humanresources. In this paper, we evaluate of the relationship between theseresources and the scientific advancement of foundation models (FM). We reviewed6517 FM papers published between 2022 to 2024, and surveyed 229 first-authorsto the impact of computing resources on scientific output. We find thatincreased computing is correlated with national funding allocations andcitations, but our findings don't observe the strong correlations with researchenvironment (academic or industrial), domain, or study methodology. We advisethat individuals and institutions focus on creating shared and affordablecomputing opportunities to lower the entry barrier for under-resourcedresearchers. These steps can help expand participation in FM research, fosterdiversity of ideas and contributors, and sustain innovation and progress in AI.The data will be available at: https://mit-calc.csail.mit.edu/</description>
      <author>example@mail.com (Yuexing Hao, Yue Huang, Haoran Zhang, Chenyang Zhao, Zhenwen Liang, Paul Pu Liang, Yue Zhao, Lichao Sun, Saleh Kalantari, Xiangliang Zhang, Marzyeh Ghassemi)</author>
      <guid isPermaLink="false">2510.13621v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Generalizing WiFi Gesture Recognition via Large-Model-Aware Semantic Distillation and Alignment</title>
      <link>http://arxiv.org/abs/2510.13390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE ICPADS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GLSDA的新型泛化框架，利用预训练大型基础模型的语义先验来增强WiFi手势识别的泛化能力和语义表达能力，通过双路径CSI编码、多尺度语义编码、语义感知软监督和鲁棒双蒸馏策略，实现了在域内和跨域场景中的高性能手势识别。&lt;h4&gt;背景&lt;/h4&gt;WiFi手势识别作为一种有前途的RF传感范式，可在AIoT环境中实现非接触式和隐私保护的人机交互。然而，现有方法因信道状态信息的域敏感特性和高级手势抽象的缺乏，面临泛化能力和语义表达能力有限的问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有WiFi手势识别方法泛化能力有限和语义表达能力不足的问题，提出一种能够增强域内和跨域场景中手势表示学习的新型框架。&lt;h4&gt;方法&lt;/h4&gt;1) 设计双路径CSI编码管道，通过CSI-Ratio相位序列和多普勒频谱图捕获手势模式；2) 开发多尺度语义编码器，学习时序嵌入并通过跨模态注意力机制与手势语义对齐；3) 引入语义感知软监督方案，编码类间相关性并减少标签模糊性；4) 开发鲁棒双蒸馏策略，将对齐模型压缩为轻量级网络。&lt;h4&gt;主要发现&lt;/h4&gt;在Widar3.0基准上的实验表明，GLSDA在域内和跨域手势识别任务中均优于现有最先进方法，同时显著减小了模型大小和推理延迟。&lt;h4&gt;结论&lt;/h4&gt;GLSDA为现实世界AIoT应用中的通用RF手势界面提供了可扩展和可部署的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于WiFi的手势识别已成为一种有前途的RF传感范式，能够在AIoT环境中实现非接触式和隐私保护的人机交互。然而，由于信道状态信息的域敏感特性和高级手势抽象的缺乏，现有方法通常面临泛化能力和语义表达能力有限的问题。为解决这些挑战，我们提出了一种名为Large-Model-Aware Semantic Distillation and Alignment (GLSDA)的新型泛化框架，它利用预训练大型基础模型的语义先验来增强域内和跨域场景中的手势表示学习。具体而言，我们首先设计了一个双路径CSI编码管道，通过CSI-Ratio相位序列和多普勒频谱图捕获几何和动态手势模式。然后将这些表示输入多尺度语义编码器，学习鲁棒的时序嵌入，并通过跨模态注意力机制将其与手势语义对齐。为进一步增强类别区分度，我们引入了一种语义感知软监督方案，编码类间相关性并减少标签模糊性，特别是对于语义相似的手势。最后，我们开发了一种鲁棒双蒸馏策略，将对齐的模型压缩为轻量级学生网络，从教师模型联合蒸馏中间特征和语义感知软标签。在Widar3.0基准上的大量实验表明，GLSDA在域内和跨域手势识别任务中始终优于最先进的方法，同时显著减小了模型大小和推理延迟。我们的方法为现实世界AIoT应用中的通用RF手势界面提供了可扩展和可部署的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; WiFi-based gesture recognition has emerged as a promising RF sensing paradigmfor enabling non-contact and privacy-preserving human-computer interaction inAIoT environments. However, existing methods often suffer from limitedgeneralization and semantic expressiveness due to the domain-sensitive natureof Channel State Information and the lack of high-level gesture abstraction. Toaddress these challenges, we propose a novel generalization framework, termedLarge-Model-Aware Semantic Distillation and Alignment (GLSDA), which leveragesthe semantic prior of pre-trained large foundation models to enhance gesturerepresentation learning in both in-domain and cross-domain scenarios.Specifically, we first design a dual-path CSI encoding pipeline that capturesgeometric and dynamic gesture patterns via CSI-Ratio phase sequences andDoppler spectrograms. These representations are then fed into a MultiscaleSemantic Encoder, which learns robust temporal embeddings and aligns them withgesture semantics through cross-modal attention mechanisms. To further enhancecategory discrimination, we introduce a Semantic-Aware Soft Supervision schemethat encodes inter-class correlations and reduces label ambiguity, especiallyfor semantically similar gestures. Finally, we develop a RobustDual-Distillation strategy to compress the aligned model into a lightweightstudent network, jointly distilling intermediate features and semantic-informedsoft labels from the teacher model. Extensive experiments on the Widar3.0benchmark show that GLSDA consistently outperforms state-of-the-art methods inboth in-domain and cross-domain gesture recognition tasks, while significantlyreducing model size and inference latency. Our method offers a scalable anddeployable solution for generalized RF-based gesture interfaces in real-worldAIoT applications.</description>
      <author>example@mail.com (Feng-Qi Cui, Yu-Tong Guo, Tianyue Zheng, Jinyang Huang)</author>
      <guid isPermaLink="false">2510.13390v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Document Intelligence in the Era of Large Language Models: A Survey</title>
      <link>http://arxiv.org/abs/2510.13366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了Document AI (DAI)领域在大语言模型(LLMs)影响下的发展，探讨了多模态、多语言和检索增强DAI的进展与挑战，并提出了未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;Document AI已成为重要应用领域，大语言模型的出现显著改变了这一领域，从早期的编码器-解码器架构发展为仅使用解码器的LLMs。&lt;h4&gt;目的&lt;/h4&gt;提供DAI演变的全面概述，突出LLMs在该领域的当前研究和未来前景，为DAI的最先进技术提供结构化分析。&lt;h4&gt;方法&lt;/h4&gt;通过综述形式，探索多模态、多语言和检索增强DAI的关键进展和挑战，并提出未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;解码器-only LLMs彻底改变了DAI，带来了理解和生成方面的显著进步；多模态、多语言和检索增强DAI面临关键进展和挑战；基于代理的方法和文档特定基础模型是有前景的未来研究方向。&lt;h4&gt;结论&lt;/h4&gt;DAI在大语言模型的影响下正在快速发展，为学术和实践应用提供了新的可能性和挑战。&lt;h4&gt;翻译&lt;/h4&gt;文档人工智能(DAI)已成为一个重要的应用领域，并因大型语言模型(LLMs)的出现而发生了显著变化。虽然早期方法依赖于编码器-解码器架构，但仅使用解码器的LLMs彻底改变了DAI，在理解和生成方面带来了显著进步。本综述提供了DAI演变的全面概述，突出了LLMs在该领域的当前研究和未来前景。我们探索了多模态、多语言和检索增强DAI的关键进展和挑战，同时提出了未来研究方向，包括基于代理的方法和文档特定基础模型。本文旨在为DAI的最先进技术提供结构化分析，及其对学术和实践应用的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Document AI (DAI) has emerged as a vital application area, and issignificantly transformed by the advent of large language models (LLMs). Whileearlier approaches relied on encoder-decoder architectures, decoder-only LLMshave revolutionized DAI, bringing remarkable advancements in understanding andgeneration. This survey provides a comprehensive overview of DAI's evolution,highlighting current research attempts and future prospects of LLMs in thisfield. We explore key advancements and challenges in multimodal, multilingual,and retrieval-augmented DAI, while also suggesting future research directions,including agent-based approaches and document-specific foundation models. Thispaper aims to provide a structured analysis of the state-of-the-art in DAI andits implications for both academic and practical applications.</description>
      <author>example@mail.com (Weishi Wang, Hengchang Hu, Zhijie Zhang, Zhaochen Li, Hongxin Shao, Daniel Dahlmeier)</author>
      <guid isPermaLink="false">2510.13366v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Generative model for information metamaterial design</title>
      <link>http://arxiv.org/abs/2510.13264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为InfoMetaGen的通用生成模型，用于信息超材料设计，结合预训练基础模型和轻量级功能适配器，能够智能生成从元原子到任意空间编码模式的人工结构，相比传统方法具有更高的效率和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;生成式模型如AlphaFold和MatterGen可直接生成具有理想特性的新型材料结构，AlphaFold专注于蛋白质预测，MatterGen专注于预测周期性晶体结构，而超材料的通用设计更为复杂，需要设计元原子及其在空间中的任意非均匀分布。&lt;h4&gt;目的&lt;/h4&gt;提出一个通用的生成式模型InfoMetaGen用于信息超材料设计，解决超材料设计中的复杂性问题，实现从元原子到任意空间编码模式的智能生成。&lt;h4&gt;方法&lt;/h4&gt;InfoMetaGen结合预训练基础模型和轻量级功能适配器，通过微调轻量级适配器使单一通用生成模型能够切换不同功能，避免了传统方法需要为特定功能训练专用模型的局限。&lt;h4&gt;主要发现&lt;/h4&gt;InfoMetaGen能够加速新型超材料的多样化发现，在超材料性能方面取得突破，填补了设计人工材料时通用生成框架的空白。&lt;h4&gt;结论&lt;/h4&gt;该工作将生成模型的能力从微观自然材料的被动发现扩展到宏观人工材料的主动创造，为生成模型在材料设计领域开辟了前所未有的机会。&lt;h4&gt;翻译&lt;/h4&gt;生成模型如AlphaFold和MatterGen可以直接生成具有理想特性的新型材料结构，加速新材料发现并将材料设计范式从传统的试错方法转变为智能按需生成。AlphaFold专注于具有特定非周期结构的蛋白质预测；而MatterGen专注于预测周期性和稳定的晶体结构。超材料的通用设计要复杂得多，因为它涉及设计元原子（类似于周期结构）及其在空间中的任意非均匀分布。在此，我们提出了InfoMetaGen，一种用于信息超材料设计的通用生成模型，它结合了预训练基础模型和轻量级功能适配器，智能生成从元原子到任意空间编码模式的人工结构。与需要为特定功能训练专用模型的传统智能超材料设计方法相比，InfoMetaGen使单个通用生成模型能够通过微调轻量级适配器切换不同功能，显著提高了效率和泛化能力。实验结果表明，InfoMetaGen不仅可以加速新型超材料的多样化发现，还能在超材料性能方面取得突破。这项工作填补了设计人工材料时通用生成框架的空白，并为将生成模型的能力从微观自然材料的被动发现扩展到宏观人工材料的主动创造开辟了前所未有的机会。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决信息超材料的智能设计问题。传统超材料设计依赖于试错法，周期长、效率低，且现有智能方法多局限于单一功能。超材料能实现自然材料中不存在的奇特物理特性，在无线通信、传感和超分辨率成像等领域有广泛应用，因此高效设计方法对推动这些领域发展至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了AlphaFold和MatterGen等生成模型的成功经验，认识到超材料设计比蛋白质或晶体设计更复杂，因为它涉及设计超原子和它们在空间中的任意非均匀分布。他们采用两阶段训练策略：首先预训练无条件扩散模型捕获不同设计空间中的功能编码模式，然后引入条件适配器模块微调模型引导生成过程。这种方法冻结基础模型参数，只微调轻量级适配器，使单个模型能处理多种功能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过一个预训练的基础模型结合轻量级功能适配器，智能生成从单个超原子到整个超阵列的结构配置，实现多功能的统一生成。流程包括：1)预训练阶段训练无条件扩散模型捕获不同功能的设计模式；2)微调阶段引入条件适配器，冻结基础模型参数只微调适配器；3)生成阶段根据特定功能条件将随机噪声转换为功能数字比特流；4)应用阶段实现超原子设计、波束成形、电磁聚焦和全息成像等多种功能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出InfoMetaGen通用生成框架填补设计空白；2)实现多尺度设计能力从超原子到超阵列；3)通过轻量级适配器实现高效多任务处理；4)创新比特表示方法解决离散编码与连续扩散模型兼容性；5)强大生成能力产生新颖超原子和1/3位非均匀超阵列。相比之前工作，它专注于宏观人工材料而非微观自然材料，使用单一通用模型替代专用模型，实现多功能的统一生成，并能突破训练数据集限制生成高性能宽带超原子。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InfoMetaGen开创了信息超材料设计的通用生成范式，通过结合预训练基础模型与轻量级功能适配器，实现了从超原子到超阵列的多尺度、多功能智能生成，显著加速了新超材料的发现并突破了超材料性能的极限。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative models such as AlphaFold and MatterGen can directly generate novelmaterial structures with desired properties, accelerating the new materialsdiscovery and revolutionizing the material design paradigm from traditionaltrial-and-error approach to intelligent on-demand generation. AlphaFold isfocused on protein prediction with specific aperiodic structures; whileMatterGen is focused on predicting periodic and stable crystal structures. Theuniversal design of metamaterials is much more complicated, since it involvesto design meta-atoms (similar to the periodic structures) and their arbitrarilyinhomogeneous distributions in space. Here, we propose InfoMetaGen, a universalgenerative model for information metamaterial design, which combines apre-trained foundation model with lightweight functional adapters tointelligently generate artificial structures on-demand spanning from meta-atomsto arbitrary space coding patterns. In contrast to conventional intelligentmetamaterial design methods that require training dedicated models for specificfunctionalities, InfoMetaGen enables a single universal generative modelcapable of switching across diverse functionalities by fine-tuning thelightweight adapters, significantly improving both efficiency andgeneralizability. Experimental results demonstrate that InfoMetaGen can notonly accelerate the diverse discovery of new metamaterials, but also achievebreakthroughs in metamaterial performance. This work fills the gap of universalgenerative framework in designing artificial materials, and opens upunprecedented opportunities to expand the capability of generative models fromthe passive discovery of microscopic natural material to the active creation ofmacroscopic artificial materials.</description>
      <author>example@mail.com (Jun Ming Hou, Long Chen, Xuan Zheng, Jia Wei Wu, Jian Wei You, Zi Xuan Cai, Jiahan Huang, Chen Xu Wu, Jian Lin Su, Lianlin Li, Jia Nan Zhang, Tie Jun Cui)</author>
      <guid isPermaLink="false">2510.13264v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models</title>
      <link>http://arxiv.org/abs/2510.13068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NeuroRVQ的新型脑电图基础模型，通过改进的信号令牌化方法解决了现有模型在高频动态处理和信号重建方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;脑电图(EEG)捕获了多个时间和频谱尺度的神经活动，产生的信号丰富但复杂，难以进行表示学习。现有的EEG基础模型在信号令牌化方面存在不足，无法保持高频动态，限制了信号重建的保真度。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕获完整频率神经频谱、支持高分辨率编码并实现高效训练的EEG信号令牌化器，以提高EEG信号的表示能力和重建质量。&lt;h4&gt;方法&lt;/h4&gt;NeuroRVQ令牌化器包含三个关键组件：多尺度特征提取模块，捕获完整频率神经频谱；分层残差矢量量化(RVQ)码本，用于高分辨率编码；以及EEG信号相位和幅度感知损失函数，用于高效训练。&lt;h4&gt;主要发现&lt;/h4&gt;NeuroRVQ设计支持所有频段的准确重建，同时实现高效的EEG压缩，实现了强大的生成掩码建模。实证结果表明，NeuroRVQ实现了更低的重建误差，并在各种下游任务上优于现有的大脑波模型。&lt;h4&gt;结论&lt;/h4&gt;NeuroRVQ令牌化器为基于码本的通用脑波模型建立了强有力的先验，有望推动神经解码、生成建模和多模态生物信号集成等领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)捕获了多个时间和频谱尺度的神经活动，产生的信号丰富但复杂，难以进行表示学习。最近，经过训练以预测掩码信号令牌的EEG基础模型在学习可泛化表示方面显示出前景。然而，它们的性能受到信号令牌化模块的限制。现有的神经令牌化器无法保持高频动态，限制了它们以高保真度重建EEG信号的能力。我们引入了NeuroRVQ，这是一个基于码本令牌化器的可扩展大脑波模型(LBM)。我们的令牌化器整合了：(i)捕获完整频率神经频谱的多尺度特征提取模块；(ii)用于高分辨率编码的分层残差矢量量化(RVQ)码本；以及(iii)用于高效训练的EEG信号相位和幅度感知损失函数。这种设计支持所有频段的准确重建，同时实现高效的EEG压缩，从而实现强大的生成掩码建模。我们的实证结果表明，NeuroRVQ实现了更低的重建误差，并在各种下游任务上优于现有的LBM。更广泛地说，NeuroRVQ令牌化器为基于码本的通用脑波模型建立了强有力的先验，推动了神经解码、生成建模和多模态生物信号集成的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) captures neural activity across multipletemporal and spectral scales, yielding signals that are rich but complex forrepresentation learning. Recently, EEG foundation models trained to predictmasked signal-tokens have shown promise for learning generalizablerepresentations. However, their performance is hindered by their signaltokenization modules. Existing neural tokenizers fail to preservehigh-frequency dynamics, limiting their ability to reconstruct EEG signals withhigh fidelity. We introduce NeuroRVQ, a scalable Large Brainwave Model (LBM)centered on a codebook-based tokenizer. Our tokenizer integrates: (i)multi-scale feature extraction modules that capture the full frequency neuralspectrum; (ii) hierarchical residual vector quantization (RVQ) codebooks forhigh-resolution encoding; and, (iii) an EEG signal phase- and amplitude-awareloss function for efficient training. This design enables efficient EEGcompression while supporting accurate reconstruction across all frequencybands, leading to robust generative masked modeling. Our empirical resultsdemonstrate that NeuroRVQ achieves lower reconstruction error and outperformsexisting LBMs on a variety of downstream tasks. More broadly, NeuroRVQtokenizer establishes a strong prior for codebook-based general-purposebrainwave models, enabling advances in neural decoding, generative modeling andmultimodal biosignal integration.</description>
      <author>example@mail.com (Konstantinos Barmpas, Na Lee, Alexandros Koliousis, Yannis Panagakis, Dimitrios A. Adamos, Nikolaos Laskaris, Stefanos Zafeiriou)</author>
      <guid isPermaLink="false">2510.13068v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation</title>
      <link>http://arxiv.org/abs/2510.12953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了FetalMind，一个专为胎儿超声检查设计的医学AI系统，用于报告生成和诊断。通过引入显著认识解耦（SED）方法和构建大规模数据集FetalSigma-1M，FetalMind在所有妊娠阶段都优于现有基线模型，平均提高14%的准确率，在关键条件下提高61.2%的准确率，同时保持高效、稳定和可扩展性。&lt;h4&gt;背景&lt;/h4&gt;近期的医学视觉-语言模型在VQA、报告生成和异常检测等任务上表现出色，但大多数模型适应于结构化的成人影像，在胎儿超声检查方面表现不佳。胎儿超声面临多视图图像推理、疾病多样性和图像多样性的挑战。&lt;h4&gt;目的&lt;/h4&gt;弥合现有医学视觉-语言模型在胎儿超声领域的应用差距，开发一个专门针对胎儿超声检查的AI系统，用于报告生成和诊断。&lt;h4&gt;方法&lt;/h4&gt;在临床工作流程指导下提出显著认识解耦（SED）方法，将专家构建的二分图注入模型以解耦视图-疾病关联，并通过强化学习引导偏好选择。同时构建了FetalSigma-1M数据集，包含来自十二个医疗中心的20K份胎儿超声报告，解决了领域数据稀缺问题。&lt;h4&gt;主要发现&lt;/h4&gt;FetalMind在所有妊娠阶段都优于开源和闭源基线模型，平均提高14%的准确率，在关键条件下提高61.2%的准确率，同时保持高效、稳定和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;FetalMind是一个有效的胎儿超声AI系统，通过SED方法和大规模数据集训练，成功解决了胎儿超声图像推理的挑战，在报告生成和诊断方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;近期的医学视觉-语言模型在VQA、报告生成和异常检测等任务上显示出潜力。然而，大多数模型适应于结构化的成人影像，在胎儿超声检查方面表现不佳，这带来了多视图图像推理、疾病多样性和图像多样性的挑战。为了弥合这一差距，我们引入了FetalMind，一个专为胎儿超声设计的医学AI系统，用于报告生成和诊断。在临床工作流程的指导下，我们提出了显著认识解耦（SED）方法，将专家构建的二分图注入模型中，解耦视图-疾病关联，并通过强化学习引导偏好选择，遵循临床忠实步骤。这种设计减轻了疾病间的变异性和视图间的异质性，减少了学习瓶颈，同时使模型的推理与产科实践保持一致。为了大规模训练FetalMind，我们整理了FetalSigma-1M数据集，这是第一个大规模胎儿超声报告语料库，包含来自十二个医疗中心的20K份报告，解决了领域数据稀缺的问题。大量实验表明，FetalMind在所有妊娠阶段都优于开源和闭源基线模型，平均提高14%的准确率，在关键条件下提高61.2%的准确率，同时保持高效、稳定和可扩展性。项目页面：https://hexiao0275.github.io/FetalMind。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent medical vision-language models have shown promise on tasks such asVQA, report generation, and anomaly detection. However, most are adapted tostructured adult imaging and underperform in fetal ultrasound, which poseschallenges of multi-view image reasoning, numerous diseases, and imagediversity. To bridge this gap, we introduce FetalMind, a medical AI systemtailored to fetal ultrasound for both report generation and diagnosis. Guidedby clinical workflow, we propose Salient Epistemic Disentanglement (SED), whichinjects an expert-curated bipartite graph into the model to decoupleview-disease associations and to steer preference selection along clinicallyfaithful steps via reinforcement learning. This design mitigates variabilityacross diseases and heterogeneity across views, reducing learning bottleneckswhile aligning the model's inference with obstetric practice. To trainFetalMind at scale, we curate FetalSigma-1M dataset, the first large-scalefetal ultrasound report corpus, comprising 20K reports from twelve medicalcenters, addressing the scarcity of domain data. Extensive experiments showthat FetalMind outperforms open- and closed-source baselines across allgestational stages, achieving +14% average gains and +61.2% higher accuracy oncritical conditions while remaining efficient, stable, and scalable. ProjectPage: https://hexiao0275.github.io/FetalMind.</description>
      <author>example@mail.com (Xiao He, Huangxuan Zhao, Guojia Wan, Wei Zhou, Yanxing Liu, Juhua Liu, Yongchao Xu, Yong Luo, Dacheng Tao, Bo Du)</author>
      <guid isPermaLink="false">2510.12953v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>An Investigation of Memorization Risk in Healthcare Foundation Models</title>
      <link>http://arxiv.org/abs/2510.12950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一套用于评估在电子健康记录上训练的基础模型中隐私相关记忆风险的黑盒评估测试，包括在嵌入层和生成层探测记忆的方法，并发布了开源工具包促进医疗AI中的隐私评估。&lt;h4&gt;背景&lt;/h4&gt;基础模型在大型去标识化的电子健康记录上训练有临床应用前景，但它们可能记住患者信息，引发隐私问题。&lt;h4&gt;目的&lt;/h4&gt;引入一套黑盒评估测试来评估在结构化EHR数据上训练的基础模型中的隐私相关记忆风险。&lt;h4&gt;方法&lt;/h4&gt;开发了一个框架，包括在嵌入层和生成层探测记忆的方法，旨在区分模型泛化和有害记忆，特别是在临床相关环境中。&lt;h4&gt;主要发现&lt;/h4&gt;将记忆放在可能损害患者隐私的背景下，特别是对弱势亚群体。&lt;h4&gt;结论&lt;/h4&gt;在公开可用的EHR基础模型上验证了这种方法，并发布了一个开源工具包，以促进医疗AI中可复现和协作的隐私评估。&lt;h4&gt;翻译&lt;/h4&gt;在大型去标识化电子健康记录(EHRs)上训练的基础模型在临床应用方面具有潜力。然而，它们记忆患者信息的能力引发了重要的隐私问题。在这项工作中，我们引入了一套黑盒评估测试，用于评估在结构化EHR数据上训练的基础模型中的隐私相关记忆风险。我们的框架包括在嵌入层和生成层探测记忆的方法，旨在区分临床相关环境中的模型泛化和有害记忆。我们将记忆放在可能损害患者隐私的背景下，特别是对弱势亚群体。我们在公开可用的EHR基础模型上验证了我们的方法，并发布了一个开源工具包，以促进医疗AI中可复现和协作的隐私评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models trained on large-scale de-identified electronic healthrecords (EHRs) hold promise for clinical applications. However, their capacityto memorize patient information raises important privacy concerns. In thiswork, we introduce a suite of black-box evaluation tests to assessprivacy-related memorization risks in foundation models trained on structuredEHR data. Our framework includes methods for probing memorization at both theembedding and generative levels, and aims to distinguish between modelgeneralization and harmful memorization in clinically relevant settings. Wecontextualize memorization in terms of its potential to compromise patientprivacy, particularly for vulnerable subgroups. We validate our approach on apublicly available EHR foundation model and release an open-source toolkit tofacilitate reproducible and collaborative privacy assessments in healthcare AI.</description>
      <author>example@mail.com (Sana Tonekaboni, Lena Stempfle, Adibvafa Fallahpour, Walter Gerych, Marzyeh Ghassemi)</author>
      <guid isPermaLink="false">2510.12950v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model</title>
      <link>http://arxiv.org/abs/2510.12709v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SAIL-Embedding是一个全模态嵌入基础模型，通过定制化训练策略和架构设计解决了现有多模态嵌入模型在现实应用中面临的挑战，包括模态支持有限、训练机制不稳定和工业领域差距等问题。&lt;h4&gt;背景&lt;/h4&gt;多模态嵌入模型旨在产生信息丰富的统一表示以支持各种跨模态任务。尽管从基于CLIP的双塔架构到大型视觉语言模型的发展前景广阔，但现有工作在现实应用和业务场景中仍面临不可避免的挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍SAIL-Embedding，一个全模态嵌入基础模型，通过定制化训练策略和架构设计解决现有多模态嵌入模型面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出多阶段训练方案提高表示学习效果，包括内容感知渐进训练增强模型对多样化下游任务的适应性，协作感知推荐增强训练使多模态表示适应推荐场景，以及开发随机专业化和数据集驱动的模式匹配增强模型训练的灵活性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SAIL-Embedding在不同检索任务中实现了SOTA性能。在线实验显示，Lifetime (LT)显著增加，在抖音精选场景中带来+0.5%的7天LT增益，为抖音feed排序模型带来+0.1%的AUC增益。&lt;h4&gt;结论&lt;/h4&gt;SAIL-Embedding是一个有效的全模态嵌入基础模型，通过定制化训练策略和架构设计，在各种场景中表现出色，特别是在推荐系统中。&lt;h4&gt;翻译&lt;/h4&gt;多模态嵌入模型旨在产生信息丰富的统一表示，支持各种跨模态任务。尽管从基于CLIP的双塔架构到大型视觉语言模型的发展前景广阔，但现有工作在现实应用和业务场景中仍面临不可避免的挑战，如模态支持有限、训练机制不稳定和工业领域差距等。在本工作中，我们介绍了SAIL-Embedding，一个全模态嵌入基础模型，通过定制化训练策略和架构设计解决这些问题。在优化过程中，我们提出多阶段训练方案以提高表示学习的多方面有效性。具体而言，内容感知渐进训练旨在增强模型对多样化下游任务的适应性，掌握丰富的跨模态能力。协作感知推荐增强训练通过从序列到项目和ID到项目嵌入中提炼知识，同时挖掘用户历史兴趣，使多模态表示适应推荐场景。同时，我们开发了随机专业化和数据集驱动的模式匹配，增强模型训练的灵活性和泛化能力。实验结果表明，SAIL-Embedding在不同检索任务中与其他方法相比实现了SOTA性能。在各种集成我们模型的现实场景的在线实验中，我们观察到作为推荐体验关键指标的Lifetime (LT)显著增加。例如，在抖音精选场景中，模型带来了+0.5%的7天LT增益。对于抖音feed排序模型，SAIL-Embedding产生的匹配特征带来了+0.1%的AUC增益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal embedding models aim to yield informative unified representationsthat empower diverse cross-modal tasks. Despite promising developments in theevolution from CLIP-based dual-tower architectures to large vision-languagemodels, prior works still face unavoidable challenges in real-worldapplications and business scenarios, such as the limited modality support,unstable training mechanisms, and industrial domain gaps. In this work, weintroduce SAIL-Embedding, an omni-modal embedding foundation model thataddresses these issues through tailored training strategies and architecturaldesign. In the optimization procedure, we propose a multi-stage training schemeto boost the multifaceted effectiveness of representation learning.Specifically, the content-aware progressive training aims to enhance themodel's adaptability to diverse downstream tasks and master enrichedcross-modal proficiency. The collaboration-aware recommendation enhancementtraining further adapts multimodal representations for recommendation scenariosby distilling knowledge from sequence-to-item and ID-to-item embeddings whilemining user historical interests. Concurrently, we develop the stochasticspecialization and dataset-driven pattern matching to strengthen model trainingflexibility and generalizability. Experimental results show that SAIL-Embeddingachieves SOTA performance compared to other methods in different retrievaltasks. In online experiments across various real-world scenarios integratedwith our model, we observe a significant increase in Lifetime (LT), which is acrucial indicator for the recommendation experience. For instance, the modeldelivers the 7-day LT gain of +0.5% in the Douyin-Selected scenario. For theDouyin feed rank model, the match features produced by SAIL-Embedding yield a+0.1% AUC gain.</description>
      <author>example@mail.com (Lin Lin, Jiefeng Long, Zhihe Wan, Yuchi Wang, Dingkang Yang, Shuang Yang, Yueyang Yao, Xu Chen, Zirui Guo, Shengqiang Li, Weiran Li, Hanyu Li, Yaling Mou, Yan Qiu, Haiyang Yu, Xiao Liang, Hongsheng Li, Chao Feng)</author>
      <guid isPermaLink="false">2510.12709v2</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>SWIR-LightFusion: Multi-spectral Semantic Fusion of Synthetic SWIR with {Thermal} IR {(LWIR/MWIR)} and RGB</title>
      <link>http://arxiv.org/abs/2510.13404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于合成短波红外（SWIR）的多模态融合框架，用于改善恶劣能见度条件下的场景理解，解决了传统成像模态在融合时难以提供全面场景信息的问题。&lt;h4&gt;背景&lt;/h4&gt;在恶劣能见度条件下增强场景理解对监控和自主导航系统是一个关键挑战。传统的RGB和热红外成像模态在融合时难以在大气干扰或照明不足条件下提供全面场景信息。&lt;h4&gt;目的&lt;/h4&gt;解决传统成像模态的局限性，克服SWIR成像发展和实施中因缺乏公开SWIR数据集而面临的障碍，提高融合图像质量。&lt;h4&gt;方法&lt;/h4&gt;利用先进对比度增强技术从现有长波红外（LWIR）数据合成类似SWIR的结构/对比度线索图像，提出多模态融合框架集成合成SWIR、LWIR和RGB模态，采用优化的编码器-解码器神经网络架构和softmax门控融合头。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共RGB-LWIR基准和私有真实RGB-MWIR-SWIR数据集上的实验表明，该框架提高了融合图像质量（对比度、边缘定义、结构保真度）同时保持实时性能，并优于其他三模态融合方法。&lt;h4&gt;结论&lt;/h4&gt;合成SWIR增强的多模态融合框架在监控和自主系统中有实际应用的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;在恶劣能见度条件下增强场景理解对监控和自主导航系统仍然是一个关键挑战。传统的成像模态，如RGB和热红外（中波/长波），在融合时往往难以提供全面的场景信息，特别是在大气干扰或照明不足的条件下。为解决这些限制，短波红外（SWIR）成像因其能够穿透大气干扰并以更清晰的分辨率区分材料而成为一种有前景的模态。然而，基于SWIR系统的发展和广泛实施面临重大障碍，主要是由于缺乏公开可访问的SWIR数据集。为应对这一挑战，我们的研究提出了一种方法，利用先进的对比度增强技术从现有的LWIR数据中合成类似SWIR的结构/对比度线索图像（不声称光谱再现）。然后我们提出了一个多模态融合框架，集成合成SWIR、LWIR和RGB模态，采用具有模态特定编码器和softmax门控融合头的优化编码器-解码器神经网络架构。在公共RGB-LWIR基准（M3FD、TNO、CAMEL、MSRS、RoadScene）和额外的私有真实RGB-MWIR-SWIR数据集上的全面实验表明，我们的合成SWIR增强融合框架提高了融合图像质量（对比度、边缘定义、结构保真度）同时保持实时性能。我们还添加了公平的三模态基线（LP、LatLRR、GFF）和U2Fusion/SwinFusion的级联三模态变体，采用统一协议。结果突显了在监控和自主系统中实际应用的巨大潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在不良能见度条件下增强场景理解的问题，这对监控系统和自主导航系统至关重要。传统RGB和热红外成像融合在恶劣天气（如雾、烟、低光）下难以提供全面场景信息，而短波红外（SWIR）虽能穿透大气干扰并更好区分材料，但因公开SWIR数据集稀缺限制了其应用。这一问题在自动驾驶、安防监控等领域尤为重要，因为系统需要在各种环境条件下保持可靠的环境感知能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到RGB和热红外融合的局限性，注意到SWIR的优势但受限于数据集稀缺。他们借鉴了之前自己的LightFusion工作（使用灰度作为第三模态），但发现灰度无法充分体现SWIR优势。作者还参考了传统多尺度变换技术、深度学习方法（自编码器、CNN、GAN）以及红外-可见光图像融合（IVIF）领域的研究。基于这些思考，他们设计了一种使用对比度受限自适应直方图均衡化（CLAHE）从LWIR合成SWIR的方法，并开发了三模态融合框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过合成SWIR图像解决数据稀缺问题，并融合RGB、热红外和合成SWIR三种模态以提升场景理解能力。整体流程包括：1) 数据预处理（统一分辨率）；2) 使用CLAHE技术从LWIR生成合成SWIR图像；3) 使用三个模态特定编码器（RGB、MWIR和合成SWIR）独立提取特征，每个编码器采用轻量级梯度残块（Light-GRLB）；4) 通过softmax门控融合头整合多模态特征；5) 使用解码器重建高质量融合图像；6) 采用语义损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 使用CLAHE从LWIR合成SWIR图像解决数据稀缺问题；2) 设计三模态融合框架（RGB、LWIR/MWIR和合成SWIR）；3) 开发轻量级梯度残块（Light-GRLB）进行高效特征提取；4) 为每种模态使用独立编码器确保精确特征提取；5) 采用softmax门控融合头加权整合多模态特征。相比之前工作（如作者自己的LightFusion），不同之处在于使用合成SWIR替代灰度作为第三模态，采用专门设计的Light-GRLB而非标准卷积层，以及引入三重语义一致性损失函数。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的多模态图像融合方法，通过合成生成短波红外图像并与RGB和热红外图像融合，显著提升了在不良能见度条件下的场景理解能力，同时保持了实时处理的效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enhancing scene understanding in adverse visibility conditions remains acritical challenge for surveillance and autonomous navigation systems.Conventional imaging modalities, such as RGB and thermal infrared (MWIR /LWIR), when fused, often struggle to deliver comprehensive scene information,particularly under conditions of atmospheric interference or inadequateillumination. To address these limitations, Short-Wave Infrared (SWIR) imaginghas emerged as a promising modality due to its ability to penetrate atmosphericdisturbances and differentiate materials with improved clarity. However, theadvancement and widespread implementation of SWIR-based systems facesignificant hurdles, primarily due to the scarcity of publicly accessible SWIRdatasets. In response to this challenge, our research introduces an approach tosynthetically generate SWIR-like structural/contrast cues (without claimingspectral reproduction) images from existing LWIR data using advanced contrastenhancement techniques. We then propose a multimodal fusion frameworkintegrating synthetic SWIR, LWIR, and RGB modalities, employing an optimizedencoder-decoder neural network architecture with modality-specific encoders anda softmax-gated fusion head. Comprehensive experiments on public {RGB-LWIRbenchmarks (M3FD, TNO, CAMEL, MSRS, RoadScene) and an additional private realRGB-MWIR-SWIR dataset} demonstrate that our synthetic-SWIR-enhanced fusionframework improves fused-image quality (contrast, edge definition, structuralfidelity) while maintaining real-time performance. We also add fair trimodalbaselines (LP, LatLRR, GFF) and cascaded trimodal variants ofU2Fusion/SwinFusion under a unified protocol. The outcomes highlightsubstantial potential for real-world applications in surveillance andautonomous systems.</description>
      <author>example@mail.com (Muhammad Ishfaq Hussain, Ma Van Linh, Zubia Naz, Unse Fatima, Yeongmin Ko, Moongu Jeon)</author>
      <guid isPermaLink="false">2510.13404v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning</title>
      <link>http://arxiv.org/abs/2510.13375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DepthVLA是一种简单而有效的VLA架构，通过预训练的深度预测模块明确整合空间感知能力，采用混合变压器设计统一VLM、深度变换器和动作专家，具有完全共享的注意力机制，形成具有增强空间推理能力的端到端模型。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language-Action (VLA) 模型最近展示了出色的泛化和语言引导的操作能力，但在需要精确空间推理的任务上表现不佳，这是由于从Vision-Language Models (VLMs) 继承的有限空间推理能力造成的。&lt;h4&gt;目的&lt;/h4&gt;解决现有VLA模型在需要精确空间推理的任务上表现不佳的问题，提高模型的空间理解和推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出DepthVLA架构，通过预训练的深度预测模块明确整合空间感知能力，采用混合变压器设计统一VLM、深度变换器和动作专家，具有完全共享的注意力机制，形成端到端模型。&lt;h4&gt;主要发现&lt;/h4&gt;在现实世界和模拟环境中的广泛评估表明，DepthVLA优于最先进的方法，在现实世界任务中取得了78.5%对比65.0%的进展，在LIBERO模拟器中为94.9%对比93.6%，在Simpler模拟器中为74.8%对比58.8%。&lt;h4&gt;结论&lt;/h4&gt;DepthVLA通过明确整合空间感知能力，显著提高了VLA模型在需要精确空间推理任务上的表现，代码将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action (VLA) 模型最近展示了出色的泛化和语言引导的操作能力。然而，在需要精确空间推理的任务上，它们的性能会下降，这是由于从Vision-Language Models (VLMs) 继承的有限空间推理能力。现有的VLA依赖于大量的动作数据预训练来将VLMs定位在3D空间中，这降低了训练效率，并且仍然不足以进行准确的空间理解。在这项工作中，我们提出了DepthVLA，一种简单而有效的VLA架构，通过预训练的深度预测模块明确地整合了空间感知能力。DepthVLA采用混合变压器设计，统一了VLM、深度变换器和动作专家，具有完全共享的注意力机制，形成具有增强空间推理能力的端到端模型。在现实世界和模拟环境中的广泛评估表明，DepthVLA优于最先进的方法，在现实世界任务中取得了78.5%对比65.0%的进展，在LIBERO模拟器中为94.9%对比93.6%，在Simpler模拟器中为74.8%对比58.8%。我们的代码将公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决Vision-Language-Action(VLA)模型在需要精确空间推理的任务上性能下降的问题。这个问题很重要，因为机器人需要精确的空间感知能力来完成精细操作，如抓取小物体、执行精确操作或避免碰撞。现有的VLA模型依赖大量动作数据预训练来将VLMs嵌入3D空间，这降低了训练效率，且仍无法满足精确空间理解的需求，限制了机器人在实际应用中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有VLA模型在空间推理方面的局限性，以及现有方法（如大量动作数据预训练或生成世界模型）的不足来设计新方法。他们借鉴了π0的mixture-of-transformers(MoT)设计，并利用3D感知领域的最新进展，特别是Depth Anything V2作为深度专家的基础。作者提出通过预训练的深度预测模块显式整合空间感知能力，采用混合transformers架构统一VLM、深度专家和动作专家，并使用块状掩码保持预训练模块的学习能力，同时允许每个组件在不同数据集上分别预训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过预训练的深度预测模块显式整合空间推理能力到VLA模型中，利用混合transformers架构统一三个专家（VLM、深度专家和动作专家），在保持预训练知识的同时融合语义和空间线索以生成精确动作。整体实现流程包括：1)模型架构设计，包含VLM专家（编码图像和指令）、深度专家（处理图像推断几何信息）和动作专家（生成连续动作）；2)首先在多样化3D数据集上预训练深度专家；3)然后在具身动作数据上训练整个DepthVLA模型，使用模仿学习目标和流动匹配损失；4)在推理过程中，三个专家并行处理输入，共享注意力机制，动作专家基于融合的特征生成连续动作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)DepthVLA架构，首次将预训练的深度预测专家集成到VLA框架中；2)按专家预训练策略，允许每个专家在不同数据集上分别预训练；3)在所有中间层执行空间推理，提供更丰富的几何特征；4)端到端联合优化空间推理和动作生成。与之前工作的不同之处在于：相比现有VLA模型，不依赖大量动作数据预训练；相比SpatialVLA，深度专家是端到端优化的；相比生成世界模型，明确编码当前场景的3D知识且无高延迟；相比CoT推理方法，避免了自回归生成空间令牌的高延迟问题，推理时间仅增加20毫秒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DepthVLA通过集成预训练的深度专家到混合transformers框架中，显著提升了机器人在需要精确空间推理任务上的性能，同时保持了高效的训练和推理速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have recently shown impressivegeneralization and language-guided manipulation capabilities. However, theirperformance degrades on tasks requiring precise spatial reasoning due tolimited spatial reasoning inherited from Vision-Language Models (VLMs).Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3Dspace, which reduces training efficiency and is still insufficient for accuratespatial understanding. In this work, we present DepthVLA, a simple yeteffective VLA architecture that explicitly incorporates spatial awarenessthrough a pretrained depth prediction module. DepthVLA adopts amixture-of-transformers design that unifies a VLM, a depth transformer, and anaction expert with fully shared attentions, forming an end-to-end model withenhanced spatial reasoning. Extensive evaluations in both real-world andsimulated environments show that DepthVLA outperforms state-of-the-artapproaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs.93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.Our code will be made publicly available.</description>
      <author>example@mail.com (Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Zhuoguang Chen, Tao Jiang, Hang Zhao)</author>
      <guid isPermaLink="false">2510.13375v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>FlyAwareV2: A Multimodal Cross-Domain UAV Dataset for Urban Scene Understanding</title>
      <link>http://arxiv.org/abs/2510.13243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 7 figures, 10 tables, data and code available&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FlyAwareV2是一个新的多模态数据集，包含真实和合成的无人机图像，专为城市场景理解任务设计，解决了真实数据收集和标注的挑战。&lt;h4&gt;背景&lt;/h4&gt;城市环境中无人机应用的计算机视觉算法开发严重依赖于大规模、准确标注的数据集，但收集和标注真实世界无人机数据极其困难且成本高昂。&lt;h4&gt;目的&lt;/h4&gt;解决真实数据收集和标注的局限性，提供一个包含真实和合成无人机图像的多模态数据集，用于城市场景理解任务。&lt;h4&gt;方法&lt;/h4&gt;基于SynDrone和FlyAware数据集开发FlyAwareV2，引入多模态数据(RGB、深度、语义标签)覆盖不同环境条件；通过最先进单目深度估计计算真实样本深度图；提供RGB和多模态语义分割基准；研究合成到真实域适应以评估模型泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;FlyAwareV2具有丰富的标注和环境多样性，为基于无人机的3D城市场景理解研究提供了宝贵资源。&lt;h4&gt;结论&lt;/h4&gt;FlyAwareV2通过其丰富的标注集和环境多样性，为基于无人机的3D城市场景理解研究提供了有价值的资源。&lt;h4&gt;翻译&lt;/h4&gt;针对城市环境中无人机应用开发的计算机视觉算法严重依赖于具有准确标注的大规模数据集的可用性。然而，收集和标注真实世界的无人机数据极其具有挑战性和成本高昂。为了解决这一局限性，我们提出了FlyAwareV2，这是一个新颖的多模态数据集，包含专为城市场景理解任务定制的真实和合成无人机图像。基于最近引入的SynDrone和FlyAware数据集，FlyAwareV2引入了几个新的关键贡献：1)跨不同环境条件的多模态数据(RGB、深度、语义标签)，包括变化的天气和白天时间；2)通过最先进的单目深度估计计算的真实样本深度图；3)基于标准架构的RGB和多模态语义分割基准；4)关于合成到真实域适应的研究，以评估在合成数据上训练的模型的泛化能力。凭借其丰富的标注集和环境多样性，FlyAwareV2为基于无人机的3D城市场景理解研究提供了宝贵的资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of computer vision algorithms for Unmanned Aerial Vehicle(UAV) applications in urban environments heavily relies on the availability oflarge-scale datasets with accurate annotations. However, collecting andannotating real-world UAV data is extremely challenging and costly. To addressthis limitation, we present FlyAwareV2, a novel multimodal dataset encompassingboth real and synthetic UAV imagery tailored for urban scene understandingtasks. Building upon the recently introduced SynDrone and FlyAware datasets,FlyAwareV2 introduces several new key contributions: 1) Multimodal data (RGB,depth, semantic labels) across diverse environmental conditions includingvarying weather and daytime; 2) Depth maps for real samples computed viastate-of-the-art monocular depth estimation; 3) Benchmarks for RGB andmultimodal semantic segmentation on standard architectures; 4) Studies onsynthetic-to-real domain adaptation to assess the generalization capabilitiesof models trained on the synthetic data. With its rich set of annotations andenvironmental diversity, FlyAwareV2 provides a valuable resource for researchon UAV-based 3D urban scene understanding.</description>
      <author>example@mail.com (Francesco Barbato, Matteo Caligiuri, Pietro Zanuttigh)</author>
      <guid isPermaLink="false">2510.13243v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>True Self-Supervised Novel View Synthesis is Transferable</title>
      <link>http://arxiv.org/abs/2510.13063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了XFactor，第一个无需几何信息且能实现真正新颖视图合成的自监督模型，其关键标准是姿态表示的可转移性。&lt;h4&gt;背景&lt;/h4&gt;先前关于自监督新颖视图合成的工作分析表明，它们预测的姿态不具备可转移性——相同姿态在不同3D场景中会导致不同的相机轨迹。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实现真正新颖视图合成的模型，其关键标准是姿态表示的可转移性。&lt;h4&gt;方法&lt;/h4&gt;XFactor结合了成对姿态估计和简单的输入输出增强方案，能够将相机姿态与场景内容分离并促进几何推理，使用不受约束的潜在姿态变量，无需任何3D归纳偏置或多视图几何概念。&lt;h4&gt;主要发现&lt;/h4&gt;XFactor实现了姿态表示的可转移性；引入了一种新的可转移性量化指标；大规模实验表明XFactor显著优于之前无需姿态的NVS变换器；探测实验显示潜在姿态与现实世界姿态高度相关。&lt;h4&gt;结论&lt;/h4&gt;XFactor是第一个无需几何信息且能实现真正新颖视图合成的自监督模型，通过结合成对姿态估计和输入输出增强方案，成功实现了姿态表示的可转移性。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们确定判断一个模型是否真正具备新颖视图合成(NVS)能力的关键标准是可转移性：即从一段视频序列中提取的任何姿态表示是否可用于在另一场景中重新渲染相同的相机轨迹。我们分析了之前关于自监督NVS的工作，发现它们预测的姿态不具备可转移性：相同的姿态集在不同3D场景中会导致不同的相机轨迹。在这里，我们提出了XFactor，这是第一个无需几何信息且能实现真正NVS的自监督模型。XFactor结合了成对姿态估计和简单的输入输出增强方案，能够将相机姿态与场景内容分离并促进几何推理。值得注意的是，我们证明XFactor使用不受约束的潜在姿态变量实现了可转移性，无需任何3D归纳偏置或多视图几何概念——例如将姿态显式参数化为SE(3)的元素。我们引入了一种新的量化可转移性的指标，并通过大规模实验证明XFactor显著优于之前无需姿态的NVS变换器，并通过探测实验显示潜在姿态与现实世界姿态高度相关。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自监督新视角合成(NVS)的可转移性问题，即能否从一个视频序列中提取的姿态表示用于重新渲染另一个视频序列中的相同相机轨迹。这个问题很重要，因为真正的NVS应该允许用户控制视角，相同的相机姿态应该总是渲染相同的视角。如果模型无法做到这一点，它就不是真正的NVS模型，而只是帧插值器，限制了用户在任意场景中定义想要渲染的视图的能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先确定可转移性是NVS的关键标准，然后分析现有自监督方法发现它们预测的姿态不能跨场景转移。作者提出两个关键见解：1)通过从必须外推的双视图模型开始训练来防止插值；2)将可转移性明确为训练目标，使用保持相机姿态但最小化像素内容重叠的增强方案。作者借鉴了RUST的无几何方法和CroCo的单目渲染思想，但通过创新的训练目标和架构设计解决了可转移性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是XFactor模型，通过成对姿态估计和输入输出的简单增强方案相结合，解耦相机姿态和场景内容，实现无几何约束的可转移NVS。整体流程：1)训练立体-单目模型(只用一对图像)，消除插值路径；2)应用可转移性目标训练，确保一个序列的姿态能用于渲染另一序列；3)使用保持相机姿态的增强方案(如逆掩码)生成像素差异大的图像对；4)通过二次训练将立体模型扩展为多视图模型，支持更复杂的场景渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出可转移性作为真正NVS的标准并引入TPS指标；2)识别现有方法实际是插值而非推理视角；3)提出促进可转移性的训练目标和增强策略；4)提出XFactor，首个完全自监督的可转移NVS模型；5)通过大规模实验验证有效性。相比RayZer(使用SE(3)参数化但降低可转移性)和RUST(仍受插值偏差影响)，XFactor不需要任何几何先验，实现了真正的跨场景姿态控制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; XFactor提出了第一个完全自监督且无几何的新视角合成模型，通过可转移性训练目标实现了真正的相机姿态控制，使相同的相机姿态能够在不同场景间产生一致的视角渲染。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we identify that the key criterion for determining whether amodel is truly capable of novel view synthesis (NVS) is transferability:Whether any pose representation extracted from one video sequence can be usedto re-render the same camera trajectory in another. We analyze prior work onself-supervised NVS and find that their predicted poses do not transfer: Thesame set of poses lead to different camera trajectories in different 3D scenes.Here, we present XFactor, the first geometry-free self-supervised model capableof true NVS. XFactor combines pair-wise pose estimation with a simpleaugmentation scheme of the inputs and outputs that jointly enablesdisentangling camera pose from scene content and facilitates geometricreasoning. Remarkably, we show that XFactor achieves transferability withunconstrained latent pose variables, without any 3D inductive biases orconcepts from multi-view geometry -- such as an explicit parameterization ofposes as elements of SE(3). We introduce a new metric to quantifytransferability, and through large-scale experiments, we demonstrate thatXFactor significantly outperforms prior pose-free NVS transformers, and showthat latent poses are highly correlated with real-world poses through probingexperiments.</description>
      <author>example@mail.com (Thomas W. Mitchel, Hyunwoo Ryu, Vincent Sitzmann)</author>
      <guid isPermaLink="false">2510.13063v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages</title>
      <link>http://arxiv.org/abs/2510.12845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一个新的多语言基准测试VLURes，用于评估视觉语言模型(VLMs)在细粒度视觉和语言理解能力方面的表现，特别是在长文本设置下。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)对推进智能体的感知能力至关重要，但目前的评估主要局限于以英语为中心的基准测试，且图像-文本对仅包含短文本。&lt;h4&gt;目的&lt;/h4&gt;评估VLMs在四种语言(英语、日语、斯瓦希里语和乌尔都语)的长文本设置下的细粒度能力，特别是物体识别、场景理解和关系理解等对智能体至关重要的任务。&lt;h4&gt;方法&lt;/h4&gt;开发了包含八个视觉语言任务和一个不相关性任务的多语言基准测试VLURes；从目标语言网页资源收集包含十个不同图像类别和丰富文本上下文的数据集；通过提示VLMs生成回答和理由，并由自动系统和母语人士进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了VLMs在不同语言和任务上的表现差异；表现最好的模型GPT-4o总体准确率为90.8%，比人类表现低6.7%；开源模型与人类表现之间的差距更大。&lt;h4&gt;结论&lt;/h4&gt;VLURes基准测试在开发能够处理多模态视觉推理的智能体方面发挥着关键作用，特别是在低资源语言环境中的应用。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLMs)对推进智能体的感知能力至关重要。然而，对VLMs的评估仍然主要局限于以英语为中心的基准测试，其中图像-文本对仅包含短文本。为了评估VLMs在四种语言下的细粒度能力，特别是在长文本设置下，我们引入了一个新的多语言基准测试VLURes，包含八个视觉语言任务和一个开创性的不相关性任务，用于探测VLMs在英语、日语以及低资源语言斯瓦希里语和乌尔都语中的细粒度视觉和语言理解能力。我们的数据集从目标语言的网页资源中精心策划，包含十个不同的图像类别和丰富的文本上下文，为斯瓦希里语和乌尔都语引入了宝贵的视觉语言资源。通过提示VLMs生成回答和理由，并由自动系统和母语人士评估，我们揭示了VLMs在不同语言和任务上的表现差异，这些任务对智能体至关重要，如物体识别、场景理解和关系理解。我们使用VLURes评估了十个VLMs。表现最好的模型GPT-4o总体准确率达到90.8%，比人类表现低6.7%，尽管开源模型之间的差距更大。这一差距凸显了VLURes在开发能够处理多模态视觉推理的智能体方面的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Language Models (VLMs) are pivotal for advancing perception inintelligent agents. Yet, evaluation of VLMs remains limited to predominantlyEnglish-centric benchmarks in which the image-text pairs comprise short texts.To evaluate VLM fine-grained abilities, in four languages under long-textsettings, we introduce a novel multilingual benchmark VLURes featuring eightvision-and-language tasks, and a pioneering unrelatedness task, to probe thefine-grained Visual and Linguistic Understanding capabilities of VLMs acrossEnglish, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets,curated from web resources in the target language, encompass ten diverse imagecategories and rich textual context, introducing valuable vision-languageresources for Swahili and Urdu. By prompting VLMs to generate responses andrationales, evaluated automatically and by native speakers, we uncoverperformance disparities across languages and tasks critical to intelligentagents, such as object recognition, scene understanding, and relationshipunderstanding. We conducted evaluations of ten VLMs with VLURes. The bestperforming model, GPT-4o, achieves an overall accuracy of 90.8% and lags humanperformance by 6.7%, though the gap is larger for open-source models. The gaphighlights VLURes' critical role in developing intelligent agents to tacklemulti-modal visual reasoning.</description>
      <author>example@mail.com (Jesse Atuhurra, Iqra Ali, Tomoya Iwakura, Hidetaka Kamigaito, Tatsuya Hiraoka)</author>
      <guid isPermaLink="false">2510.12845v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots</title>
      <link>http://arxiv.org/abs/2510.07869v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://vincentgu2000.github.io/u0project/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为USIM的水下机器人模拟数据集和一个名为U0的VLA模型，它们共同解决了水下环境中机器人操作面临的挑战，特别是在数据稀缺的情况下，通过提供大规模高质量数据集和有效的多任务学习方法。&lt;h4&gt;背景&lt;/h4&gt;水下环境为机器人操作带来了独特的挑战，包括复杂的流体动力学、有限的视野和受限的通信。虽然数据驱动的方法已经在陆地机器人上推动了具身智能的发展，并使专用水下机器人能够自主工作，但开发能够自主执行多项任务的水下智能仍然非常困难，因为大规模、高质量的水下数据集仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些限制，作者引入了USIM，这是一个基于模拟的多任务视觉-语言-动作数据集，用于水下机器人。&lt;h4&gt;方法&lt;/h4&gt;USIM包含来自1,852个轨迹的超过561K帧，总计约15.6小时的BlueROV2交互，涵盖9个不同场景中的20项任务，范围从视觉导航到移动操作。基于这个数据集，作者提出了U0，这是一个用于通用水下机器人的VLA模型，它通过多模态融合整合双目视觉和其他传感器模态，并进一步采用基于卷积-注意力的感知增强模块(CAP)来提高空间理解和移动操作能力。&lt;h4&gt;主要发现&lt;/h4&gt;在检查、避障、扫描和动态跟踪等任务中，该框架实现了80%的成功率，而在具有挑战性的移动操作任务中，与基线方法相比，它将到目标的距离减少了21.2%，证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;USIM和U0表明VLA模型可以有效地应用于水下机器人应用，为可扩展的数据集构建、改进的任务自主性和智能通用水下机器人的实际实现提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;水下环境为机器人操作带来了独特的挑战，包括复杂的流体动力学、有限的视野和受限的通信。虽然数据驱动的方法已经在陆地机器人上推动了具身智能的发展，并使专用水下机器人能够自主工作，但开发能够自主执行多项任务的水下智能仍然非常困难，因为大规模、高质量的水下数据集仍然稀缺。为了解决这些限制，我们引入了USIM，这是一个基于模拟的多任务视觉-语言-动作数据集，用于水下机器人。USIM包含来自1,852个轨迹的超过561K帧，总计约15.6小时的BlueROV2交互，涵盖9个不同场景中的20项任务，范围从视觉导航到移动操作。基于这个数据集，我们提出了U0，这是一个用于通用水下机器人的VLA模型，它通过多模态融合整合双目视觉和其他传感器模态，并进一步采用基于卷积-注意力的感知增强模块(CAP)来提高空间理解和移动操作能力。在检查、避障、扫描和动态跟踪等任务中，该框架实现了80%的成功率，而在具有挑战性的移动操作任务中，与基线方法相比，它将到目标的距离减少了21.2%，证明了其有效性。USIM和U0表明VLA模型可以有效地应用于水下机器人应用，为可扩展的数据集构建、改进的任务自主性和智能通用水下机器人的实际实现提供了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决水下机器人自主执行多任务的困难问题，原因是水下环境存在复杂流体动力学、有限可见性和受限通信等挑战，同时大规模高质量水下数据集稀缺。这个问题很重要，因为水下环境覆盖地球71%的面积，涉及海洋生态调查、资源开发、管道检查等多种应用，而水下操作对人类来说危险且困难，自主水下机器人能大幅提高效率和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了水下机器人面临的特殊挑战，然后选择使用仿真环境解决真实数据收集成本高的问题。他们基于现有Isaac-GR00T N1.5模型进行改进，而非从头训练，并整合了双目视觉和其他传感器模态。借鉴了Stonefish模拟器构建环境、ROS框架进行数据收集、Vision-Language Model和Diffusion Transformer架构，以及PID控制器和MoveIt进行机械手控制等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过仿真环境构建大规模水下VLA数据集，开发适应水下环境的VLA模型，建立可扩展的数据到任务框架。流程包括：1)用Stonefish模拟器构建9种水下场景和BlueROV2机器人模型；2)收集20个任务的数据，共561K帧和15.6小时交互数据；3)基于Isaac-GR00T N1.5开发U0模型，整合多模态传感器数据和CAP感知增强模块；4)通过开环离线评估和闭环在线测试验证模型性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个大规模水下多任务VLA数据集USIM，覆盖20个任务和9种场景；2)首个专为水下机器人设计的VLA模型U0，整合多模态传感器数据和CAP模块；3)使用以机器人为中心的坐标系表示目标位置。相比之前工作，USIM是首个多任务VLA数据集，而现有数据集多为特定任务；U0是首个专门针对水下环境的VLA模型，考虑了水下视觉退化和特殊运动特性，能处理多种任务而非单一任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过构建首个大规模水下多任务Vision-Language-Action数据集USIM和开发专门的水下机器人通用模型U0，解决了水下机器人高质量数据稀缺和通用任务执行能力不足的问题，为构建可扩展的水下智能机器人框架奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater environments present unique challenges for robotic operation,including complex hydrodynamics, limited visibility, and constrainedcommunication. Although data-driven approaches have advanced embodiedintelligence in terrestrial robots and enabled task-specific autonomousunderwater robots, developing underwater intelligence capable of autonomouslyperforming multiple tasks remains highly challenging, as large-scale,high-quality underwater datasets are still scarce. To address theselimitations, we introduce USIM, a simulation-based multi-taskVision-Language-Action (VLA) dataset for underwater robots. USIM comprises over561K frames from 1,852 trajectories, totaling approximately 15.6 hours ofBlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging fromvisual navigation to mobile manipulation. Building upon this dataset, wepropose U0, a VLA model for general underwater robots, which integratesbinocular vision and other sensor modalities through multimodal fusion, andfurther incorporates a convolution-attention-based perception focus enhancementmodule (CAP) to improve spatial understanding and mobile manipulation. Acrosstasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,the framework achieves a success rate of 80%, while in challenging mobilemanipulation tasks, it reduces the distance to the target by 21.2% comparedwith baseline methods, demonstrating its effectiveness. USIM and U0 show thatVLA models can be effectively applied to underwater robotic applications,providing a foundation for scalable dataset construction, improved taskautonomy, and the practical realization of intelligent general underwaterrobots.</description>
      <author>example@mail.com (Junwen Gu, Zhiheng Wu, Pengxuan Si, Shuang Qiu, Yukai Feng, Luoyang Sun, Laien Luo, Lianyi Yu, Jian Wang, Zhengxing Wu)</author>
      <guid isPermaLink="false">2510.07869v3</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs</title>
      <link>http://arxiv.org/abs/2510.13740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the Proceedings of the Third Learning on Graphs  Conference (LoG 2024)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的图构建方法Logarithmic Scalable Graph Construction (LSGC)和混合CNN-GNN模型LogViG，用于解决视觉图神经网络在大图像上计算成本高的问题，并通过引入高分辨率分支和多尺度特征融合提升了性能。&lt;h4&gt;背景&lt;/h4&gt;Vision graph neural networks (ViG)作为传统卷积神经网络(CNN)和视觉Transformer(ViT)的竞争性替代方案在视觉任务中显示出潜力，但常见的图构建方法如k近邻(KNN)在大图像上计算成本高，而现有的Sparse Vision Graph Attention (SVGA)方法存在固定步长导致的过度压缩和错过多连接的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的图构建方法，通过限制长距离链接的数量来提高视觉图神经网络的性能，同时降低计算复杂度，并构建一个结合CNN和GNN优势的混合模型。&lt;h4&gt;方法&lt;/h4&gt;提出Logarithmic Scalable Graph Construction (LSGC)方法来增强性能，并设计了LogViG这一新型混合CNN-GNN模型；同时引入高分辨率分支，并在高分辨率和低分辨率分支之间融合特征，构建了多尺度高分辨率视觉GNN网络。&lt;h4&gt;主要发现&lt;/h4&gt;LogViG在图像分类和语义分割任务上的准确率、GMACs和参数方面均优于现有的ViG、CNN和ViT架构；最小模型Ti-LogViG在ImageNet-1K上达到79.9%的平均top-1准确率，比Vision GNN高1.7%，参数减少24.3%，GMACs减少35.3%。&lt;h4&gt;结论&lt;/h4&gt;通过提出的LSGC方法在ViG中利用长距离链接可以超过当前最先进ViG的性能，证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;视觉图神经网络(ViG)作为传统卷积神经网络(CNN)和视觉Transformer(ViT)的竞争性替代方案，在视觉任务中显示出前景；然而，常见的图构建方法如k近邻(KNN)在大图像上可能计算成本高昂。虽然Sparse Vision Graph Attention (SVGA)等方法显示出前景，但SVGA的固定步长可能导致过度压缩和错过多个连接，无法从长距离链接中获取相同信息。基于这一观察，我们提出了一种新的图构建方法——对数可扩展图构建(LSGC)，通过限制长距离链接的数量来增强性能。为此，我们提出了LogViG，一种利用LSGC的新型混合CNN-GNN模型。此外，受多尺度和高分辨率架构成功的启发，我们引入并应用了一个高分辨率分支，并在高分辨率和低分辨率分支之间融合特征，构建了多尺度高分辨率视觉GNN网络。大量实验表明，LogViG在图像分类和语义分割任务上的准确率、GMACs和参数方面均优于现有的ViG、CNN和ViT架构。我们的最小模型Ti-LogViG在ImageNet-1K上达到79.9%的平均top-1准确率，标准差为0.2%，比Vision GNN高1.7%的平均准确率，参数减少24.3%，GMACs减少35.3%。我们的工作表明，通过我们提出的LSGC在ViG中利用长距离链接可以超过当前最先进ViG的性能。代码可在https://github.com/mmunir127/LogViG-Official获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision graph neural networks (ViG) have demonstrated promise in vision tasksas a competitive alternative to conventional convolutional neural nets (CNN)and transformers (ViTs); however, common graph construction methods, such ask-nearest neighbor (KNN), can be expensive on larger images. While methods suchas Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed stepscale can lead to over-squashing and missing multiple connections to gain thesame information that could be gained from a long-range link. Through thisobservation, we propose a new graph construction method, Logarithmic ScalableGraph Construction (LSGC) to enhance performance by limiting the number oflong-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN modelthat utilizes LSGC. Furthermore, inspired by the successes of multi-scale andhigh-resolution architectures, we introduce and apply a high-resolution branchand fuse features between our high-resolution and low-resolution branches for amulti-scale high-resolution Vision GNN network. Extensive experiments show thatLogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy,GMACs, and parameters on image classification and semantic segmentation tasks.Our smallest model, Ti-LogViG, achieves an average top-1 accuracy onImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher averageaccuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%reduction in GMACs. Our work shows that leveraging long-range links in graphconstruction for ViGs through our proposed LSGC can exceed the performance ofcurrent state-of-the-art ViGs. Code is available athttps://github.com/mmunir127/LogViG-Official.</description>
      <author>example@mail.com (Mustafa Munir, Alex Zhang, Radu Marculescu)</author>
      <guid isPermaLink="false">2510.13740v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Message Passing on the Edge: Towards Scalable and Expressive GNNs</title>
      <link>http://arxiv.org/abs/2510.13615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了EB-1WL和EB-GNN，一种基于边的颜色细化测试和相应的图神经网络架构。该架构受经典三角形计数算法启发，在消息传递过程中明确使用三角形信息。研究表明，EB-1WL比1-WL具有更强的表达能力，同时保持了接近线性的时间和内存复杂度。实验证明，EB-GNN是一种高效的通用架构，显著优于简单MPNN，且与任务专用GNN相比保持竞争力同时计算效率更高。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNN)领域存在对更具表达力且计算效率高的架构的需求，之前的提案在计算效率上存在问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于边的颜色细化测试(EB-1WL)和相应的GNN架构(EB-GNN)，以提高表达能力同时保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出EB-1WL（基于边的颜色细化测试）和EB-GNN架构，受Chiba和Nishizeki的经典三角形计数算法启发，在消息传递过程中明确使用三角形信息。&lt;h4&gt;主要发现&lt;/h4&gt;EB-1WL比1-WL具有更强的表达能力；提供了基于一阶逻辑的EB-1WL完整逻辑表征和基于同态计数的匹配区分度结果；EB-1WL和EB-GNN在实际图学习任务中需要接近线性的时间和内存；EB-GNN显著优于简单MPNN，与任务专用GNN相比保持竞争力同时计算效率更高。&lt;h4&gt;结论&lt;/h4&gt;EB-GNN是一种高效、通用的GNN架构，在表达能力与计算效率之间取得了良好的平衡。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了EB-1WL，一种基于边的颜色细化测试，以及相应的GNN架构EB-GNN。我们的架构受到Chiba和Nishizeki经典三角形计数算法的启发，并在消息传递过程中明确使用三角形。我们取得了以下结果：(1) EB-1WL比1-WL具有显著更强的表达能力。此外，我们基于一阶逻辑提供了EB-1WL的完整逻辑表征，并基于同态计数提供了匹配的区分度结果。(2) 与之前提出的更具表达力的GNN架构的重要区别在于，EB-1WL和EB-GNN在实际图学习任务中需要接近线性的时间和内存。(3) 从经验上看，我们表明EB-GNN是一种高效的通用架构：它显著优于简单的MPNN，并且在计算效率方面远高于任务专用的GNN的同时，与它们保持竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose EB-1WL, an edge-based color-refinement test, and a correspondingGNN architecture, EB-GNN. Our architecture is inspired by a classic trianglecounting algorithm by Chiba and Nishizeki, and explicitly uses triangles duringmessage passing. We achieve the following results: (1)~EB-1WL is significantlymore expressive than 1-WL. Further, we provide a complete logicalcharacterization of EB-1WL based on first-order logic, and matchingdistinguishability results based on homomorphism counting. (2)~In an importantdistinction from previous proposals for more expressive GNN architectures,EB-1WL and EB-GNN require near-linear time and memory on practical graphlearning tasks. (3)~Empirically, we show that EB-GNN is a highly-efficientgeneral-purpose architecture: It substantially outperforms simple MPNNs, andremains competitive with task-specialized GNNs while being significantly morecomputationally efficient.</description>
      <author>example@mail.com (Pablo Barceló, Fabian Jogl, Alexander Kozachinskiy, Matthias Lanzinger, Stefan Neumann, Cristóbal Rojas)</author>
      <guid isPermaLink="false">2510.13615v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs</title>
      <link>http://arxiv.org/abs/2510.13401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to Workshop on New Approaches for Addressing the Computing  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为F-BFQ的灵活块浮点量化加速器，用于提高BFP量化大语言模型在边缘设备上的推理效率，能够在两种BFP量化变体间动态切换而无需重新配置。&lt;h4&gt;背景&lt;/h4&gt;大语言模型(LLMs)在日常任务中应用广泛，通过llama.cpp等推理框架的优化（如KV缓存和量化），使在边缘设备上部署LLMs变得更加可行。量化技术是使LLMs在资源受限的边缘设备上运行的关键，llama.cpp采用块浮点(BFP)量化来减小模型权重和输入张量的位宽、内存占用和计算需求。通常，LLMs在各层采用混合BFP量化以减少精度损失。&lt;h4&gt;目的&lt;/h4&gt;为了高效加速BFP量化LLMs的各层计算，需要开发一种专门的加速器，使其能够支持不同的BFP量化变体而无需重新配置。&lt;h4&gt;方法&lt;/h4&gt;作者提出了F-BFQ（Flexible Block Floating Point Quantization）加速器，该加速器可以动态切换两种BFP量化变体并执行矩阵乘法(MatMul)操作。&lt;h4&gt;主要发现&lt;/h4&gt;在AMD Kria板上部署的初始F-BFQ加速器设计，在三种BFP量化LLMs上相比基于Arm NEON的CPU执行，平均减少了1.4倍的推理时间，实现了每秒5.2个token（约3.9个单词）的处理速度。&lt;h4&gt;结论&lt;/h4&gt;F-BFQ加速器有效地提高了BFP量化LLMs在边缘设备上的推理效率，通过支持多种BFP量化变体的动态切换，无需重新配置即可加速模型各层的计算。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型(LLMs)在日常任务中日益突出，从改善语音转文本翻译到生成最新视频游戏的额外帧等。借助llama.cpp等支持KV缓存和量化等优化的LLM推理框架，现在在边缘设备上部署LLMs比以往任何时候都更容易。量化是使LLMs在资源受限的边缘设备上运行的基本技术，llama.cpp利用块浮点(BFP)量化大幅减小权重和输入张量的位宽、内存占用以及运行LLMs所需的计算能力。LLMs通常在模型各层采用混合BFP量化，以减少量化导致的模型精度损失。因此，为了高效加速BFP量化LLMs的各层，专门的加速器需要支持不同的BFP变体而无需重新配置。为解决这一问题，我们提出了F-BFQ（Flexible Block Floating Point Quantization）加速器，它可以在两种BFP量化变体间动态切换并执行矩阵乘法(MatMul)操作。我们在AMD Kria板上部署的初始F-BFQ加速器设计，在三种BFP量化LLMs上相比基于Arm NEON的CPU执行，平均减少了1.4倍的推理时间，同时实现了每秒5.2个token（约3.9个单词）的处理速度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have become increasingly prominent for dailytasks, from improving sound-totext translation to generating additional framesfor the latest video games. With the help of LLM inference frameworks, such asllama.cpp, which support optimizations such as KV-caching and quantization, itis now easier than ever to deploy LLMs on edge devices. Quantization isfundamental to enable LLMs on resource-constrained edge devices, and llama.cpputilizes block floating point (BFP) quantization to drastically reduce the bitwidth of weights and input tensors, the memory footprint, and the computationalpower required to run LLMs. LLMs are typically quantized with mixed BFPquantization across the model layers to reduce the loss of model accuracy dueto quantization. Therefore, to efficiently accelerate across the layers ofBFP-quantized LLMs, specialized accelerators need to support different BFPvariants without reconfiguration. To address this issue, we propose a FlexibleBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamicallyswitch between two BFP quantization variants and perform matrix multiplication(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMDKria board, reduces inference time by 1.4x on average over the Arm NEON-basedCPU execution across three BFP quantized LLMs while achieving 5.2 tokens persecond (~3.9 words per second).</description>
      <author>example@mail.com (Jude Haris, José Cano)</author>
      <guid isPermaLink="false">2510.13401v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Going with the Flow: Approximating Banzhaf Values via Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.13391v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 8 figures, 11-page appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种使用图神经网络(GNN)来近似计算网络流游戏中Banzhaf值的方法，解决了传统方法在处理大规模系统时的计算效率问题。&lt;h4&gt;背景&lt;/h4&gt;Banzhaf值用于量化多智能体系统中智能体的影响力，应用领域广泛，但精确计算对于超过约20个智能体的系统由于指数级复杂度而不可行；蒙特卡洛采样方法虽然可提供估计但存在样本复杂度高且无法跨网络配置泛化的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的方法来近似计算网络流游戏中的Banzhaf值，使其能够处理大规模和动态系统，并具备良好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNN)将Banzhaf值计算问题框架化为图级预测任务，直接从网络拓扑和控制结构中学习智能体影响力的模式；比较了三种GNN架构(GAT、GINE和EdgeConv)在大型合成数据集上的性能。&lt;h4&gt;主要发现&lt;/h4&gt;训练后的GNN模型实现了高保真的Banzhaf值近似，计算速度比传统方法快数量级；模型展示了强大的零样本泛化能力，能够在未见过的网络结构上准确预测Banzhaf值而无需重新训练。&lt;h4&gt;结论&lt;/h4&gt;图神经网络可以作为复杂网络化系统可扩展合作博弈论分析的实用工具。&lt;h4&gt;翻译&lt;/h4&gt;计算网络流游戏中的Banzhaf值对于量化多智能体系统中的智能体影响力至关重要，应用范围从网络安全到基础设施规划。然而，对于超过约20个智能体的系统，由于指数级复杂度，精确计算是不可行的。虽然蒙特卡洛采样方法可以提供统计估计，但它们存在样本复杂度高的问题，并且无法在不同网络配置之间转移知识，使其对于大规模或动态系统不切实际。我们提出了一种基于学习的新方法，使用图神经网络(GNN)来近似基数网络流游戏中的Banzhaf值。通过将问题框架化为图级预测任务，我们的方法直接从网络拓扑和控制结构中学习智能体影响力的可泛化模式。我们进行了全面的实证研究，比较了三种最先进的GNN架构-图注意力网络(GAT)、具有边特征的图同构网络(GINE)和EdgeConv-在每个配置200,000个图的大规模合成数据集上的性能，数据集在大小(20-100个节点)、智能体数量(5-20)和边概率(0.5-1.0)上有所不同。我们的结果表明，训练后的GNN模型实现了高保真的Banzhaf值近似，与精确和基于采样的方法相比，速度提高了数量级。最重要的是，我们展示了强大的零样本泛化能力：在特定大小和拓扑的图上训练的模型，能够准确预测具有完全不同结构特性的全新网络的Banzhaf值，而无需重新训练。这项工作确立了GNN作为复杂网络化系统可扩展合作博弈论分析的实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computing the Banzhaf value in network flow games is fundamental forquantifying agent influence in multi-agent systems, with applications rangingfrom cybersecurity to infrastructure planning. However, exact computation isintractable for systems with more than $\sim20$ agents due to exponentialcomplexity $\mathcal{O}(2^m)$. While Monte Carlo sampling methods providestatistical estimates, they suffer from high sample complexity and cannottransfer knowledge across different network configurations, making themimpractical for large-scale or dynamic systems. We present a novellearning-based approach using Graph Neural Networks (GNNs) to approximateBanzhaf values in cardinal network flow games. By framing the problem as agraph-level prediction task, our method learns generalisable patterns of agentinfluence directly from network topology and control structure. We conduct acomprehensive empirical study comparing three state-of-the-art GNNarchitectures-Graph Attention Networks (GAT), Graph Isomorphism Networks withEdge features (GINE), and EdgeConv-on a large-scale synthetic dataset of200,000 graphs per configuration, varying in size (20-100 nodes), agent count(5-20), and edge probability (0.5-1.0). Our results demonstrate that trainedGNN models achieve high-fidelity Banzhaf value approximation withorder-of-magnitude speedups compared to exact and sampling-based methods. Mostsignificantly, we show strong zero-shot generalisation: models trained ongraphs of a specific size and topology accurately predict Banzhaf values forentirely new networks with different structural properties, without requiringretraining. This work establishes GNNs as a practical tool for scalablecooperative game-theoretic analysis of complex networked systems.</description>
      <author>example@mail.com (Benjamin Kempinski, Tal Kachman)</author>
      <guid isPermaLink="false">2510.13391v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Graph Domain Adaptation: A Spectral Contrastive Perspective</title>
      <link>http://arxiv.org/abs/2510.13254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by ECML-PKDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FracNet是一种频率感知对比图网络，通过频谱分析将图分解为高频和低频成分，解决了图神经网络在领域适应中的挑战，通过对比学习框架改善了领域适应的模糊边界问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在各种领域取得了显著成功，但由于结构分布的显著变化和可转移模式探索不足，它们在领域适应方面常常遇到困难。传统方法没有区分处理全局和局部模式，导致多层GNN后图中的一些局部细节可能被破坏。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来更好地理解和处理领域适应中的分布变化，特别是通过频谱分析来识别和利用不同频率成分中的模式，以提高图神经网络在领域适应中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出FracNet（频率感知对比图网络），包含两个协同模块，将原始图分解为高频和低频成分，并进行频率感知的领域适应。通过与对比学习框架集成，改善了领域适应的模糊边界问题。&lt;h4&gt;主要发现&lt;/h4&gt;领域变化可以通过频谱分析更好地理解，其中低频成分通常编码领域不变的全局模式，高频成分捕获领域特定的局部细节。通过分解图的不同频率成分，可以更有效地进行领域适应。&lt;h4&gt;结论&lt;/h4&gt;FracNet通过频谱分析和对比学习显著提高了领域适应的性能，实验证明其优于最先进的方法。研究不仅提供了实际应用价值，还提供了严格的理论证明来证明FracNet的优越性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在各种领域取得了显著成功，但由于结构分布的显著变化和可转移模式探索不足，它们在领域适应方面常常遇到困难。传统方法没有区分处理全局和局部模式，导致多层GNN后图中的一些局部细节可能被破坏。我们的关键见解是，领域变化可以通过频谱分析更好地理解，其中低频成分通常编码领域不变的全局模式，高频成分捕获领域特定的局部细节。因此，我们提出FracNet（频率感知对比图网络），包含两个协同模块，将原始图分解为高频和低频成分，并进行频率感知的领域适应。此外，通过与对比学习框架集成，改善了领域适应的模糊边界问题。除了实际应用意义外，我们还提供了严格的理论证明来证明FracNet的优越性。大量实验进一步证明了其优于最先进方法的显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-032-06106-5_26&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have achieved remarkable success in variousdomains, yet they often struggle with domain adaptation due to significantstructural distribution shifts and insufficient exploration of transferablepatterns. One of the main reasons behind this is that traditional approaches donot treat global and local patterns discriminatingly so that some local detailsin the graph may be violated after multi-layer GNN. Our key insight is thatdomain shifts can be better understood through spectral analysis, wherelow-frequency components often encode domain-invariant global patterns, andhigh-frequency components capture domain-specific local details. As such, wepropose FracNet (\underline{\textbf{Fr}}equency \underline{\textbf{A}}ware\underline{\textbf{C}}ontrastive Graph \underline{\textbf{Net}}work) with twosynergic modules to decompose the original graph into high-frequency andlow-frequency components and perform frequency-aware domain adaption. Moreover,the blurring boundary problem of domain adaptation is improved by integratingwith a contrastive learning framework. Besides the practical implication, wealso provide rigorous theoretical proof to demonstrate the superiority ofFracNet. Extensive experiments further demonstrate significant improvementsover state-of-the-art approaches.</description>
      <author>example@mail.com (Haoyu Zhang, Yuxuan Cheng, Wenqi Fan, Yulong Chen, Yifan Zhang)</author>
      <guid isPermaLink="false">2510.13254v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Universally Invariant Learning in Equivariant GNNs</title>
      <link>http://arxiv.org/abs/2510.13169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种理论上健全的框架，用于构建高效且实用的完备等变图神经网络(GNNs)，通过两个关键组件实现：完备的标量函数和满秩的可转向基集。&lt;h4&gt;背景&lt;/h4&gt;等变图神经网络在各种应用中显示出显著成功。为了实现完备性（即在等变函数空间上的通用逼近性质），网络必须有效捕捉不同节点之间复杂的多体相互作用。&lt;h4&gt;目的&lt;/h4&gt;提出一个理论上健全的框架，用于构建高效且实用的完备等变GNN，解决现有方法计算成本高且没有多项式时间解决方案的问题。&lt;h4&gt;方法&lt;/h4&gt;证明完备的等变GNN可以通过两个关键组件实现：1) 完备的标量函数，称为几何图的规范形式；2) 满秩的可转向基集。基于这一发现，提出了基于EGNN和TFN两种常见模型的高效算法。&lt;h4&gt;主要发现&lt;/h4&gt;实证结果表明，该模型在仅有几层的情况下展现出优越的完备性和出色的性能，从而显著降低了计算开销，同时保持强大的实际效能。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架为构建高效且实用的完备等变GNN提供了理论基础，解决了现有方法的计算效率问题。&lt;h4&gt;翻译&lt;/h4&gt;等变图神经网络在各种应用中已显示出显著成功。为了实现完备性——即在等变函数空间上的通用逼近性质——网络必须有效捕捉不同节点之间复杂的多体相互作用。现有方法通过更深层次的结构、增强的体阶数或增加可转向特征的维度来实现这一目标，通常计算成本高且没有多项式时间解决方案。在这项工作中，我们提出了一个理论上健全的框架，用于构建高效且实用的完备等变GNN。我们证明，完备的等变GNN可以通过两个关键组件实现：1) 完备的标量函数，称为几何图的规范形式；2) 满秩的可转向基集。利用这一发现，我们提出了基于两种常见模型(EGNN和TFN)的构建完备等变GNN的高效算法。实证结果表明，我们的模型在仅有几层的情况下展现出优越的完备性和出色的性能，从而显著降低了计算开销，同时保持强大的实际效能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决等变图神经网络(GNN)的完备性问题，即模型能否近似任何连续函数的能力。这个问题在科学计算和物理模拟中至关重要，因为它决定了模型能否准确捕捉复杂几何数据（如分子结构、蛋白质等）中的多体相互作用。现有方法需要通过增加网络深度、提高阶数或增加特征维度来获得更好的表达能力，但这会导致计算成本大幅增加，限制了等变GNN在实际应用中的使用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先重新审视了现有等变GNN模型（如EGNN、TFN、MACE等），将它们统一为基于多体高阶基函数扩展的形式，从而识别出当前方法的局限性。然后，作者从输出空间角度提出新的动态方法，借鉴了几何同构问题的研究成果，提出完全等变GNN需要两个关键组件：几何图的规范形式和满秩基集。作者还借鉴了FastEGNN等工作中关于虚拟节点学习的思想，并基于四点定位原理提出了多项式时间算法来构建规范形式。此外，作者借鉴了不对称图上的着色理论，证明了在非对称图上总能构建满秩基集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两个关键组件构建完全等变GNN：几何图的规范形式（完全标量函数）和满秩的可引导基集，而不需要通过增加网络深度、阶数或特征维度。实现流程包括：1) 构建几何图的规范形式，对于一般图使用四点定位原理（O(N^6)复杂度），对于非对称图使用E(3)等变函数生成参考点（O(N^2)复杂度）；2) 构建满秩基集，通过节点着色（⊕或⊗方法）使每个节点具有唯一特征；3) 实际模型实现，如EGNNcpl和TFNcpl，通过着色、构建虚拟节点、更新特征和全局操作来实现单层完备模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出新的完备性框架，将完全等变GNN构建转化为规范形式和满秩基集两个组件；2) 提出几何同构问题的多项式时间算法，为一般图提供O(N^6)算法，为非对称图提供O(N^2)算法；3) 证明在非对称几何图上总能构建任意度数的满秩基集；4) 提出EGNN/TFNcpl-global和EGNN/TFNcpl-local两种实际实现。相比之前的工作，传统方法需要通过增加阶数、层数或特征维度来实现完备性，计算成本高且无法保证在有限复杂度下实现完备性，而本文方法通过动态方法在保持低计算复杂度的同时实现了完备性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种名为Uni-EGNN的高效框架，通过几何图的规范形式和满秩基集两个关键组件，使等变图神经网络在保持低计算复杂度的同时实现了完备性，显著提升了模型在科学计算和物理模拟任务中的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Equivariant Graph Neural Networks (GNNs) have demonstrated significantsuccess across various applications. To achieve completeness -- that is, theuniversal approximation property over the space of equivariant functions -- thenetwork must effectively capture the intricate multi-body interactions amongdifferent nodes. Prior methods attain this via deeper architectures, augmentedbody orders, or increased degrees of steerable features, often at highcomputational cost and without polynomial-time solutions. In this work, wepresent a theoretically grounded framework for constructing completeequivariant GNNs that is both efficient and practical. We prove that a completeequivariant GNN can be achieved through two key components: 1) a completescalar function, referred to as the canonical form of the geometric graph; and2) a full-rank steerable basis set. Leveraging this finding, we propose anefficient algorithm for constructing complete equivariant GNNs based on twocommon models: EGNN and TFN. Empirical results demonstrate that our modeldemonstrates superior completeness and excellent performance with only a fewlayers, thereby significantly reducing computational overhead while maintainingstrong practical efficacy.</description>
      <author>example@mail.com (Jiacheng Cen, Anyi Li, Ning Lin, Tingyang Xu, Yu Rong, Deli Zhao, Zihe Wang, Wenbing Huang)</author>
      <guid isPermaLink="false">2510.13169v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering</title>
      <link>http://arxiv.org/abs/2510.12959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种后验流行度去偏置(PPD)方法，用于纠正基于图神经网络的协同过滤中的流行度偏见问题。&lt;h4&gt;背景&lt;/h4&gt;用户历史交互数据是协同过滤中学习用户偏好的主要信号，但训练数据通常呈现长尾分布，导致模型学习流行度偏见，降低推荐质量。图神经网络虽然有效，但其聚合过程会进一步传播和放大这种偏见。&lt;h4&gt;目的&lt;/h4&gt;开发一种直接对抗GNN邻域聚合过程中传播的流行度偏见的方法，提高推荐的个性化程度和质量。&lt;h4&gt;方法&lt;/h4&gt;提出PPD方法，该方法在预训练嵌入上操作，不需要重新训练。通过估计交互级别的流行度并使用流行度方向向量从节点表示中移除流行度组件，从而减少偏见同时保留用户偏好。&lt;h4&gt;主要发现&lt;/h4&gt;现有方法通过修改训练目标解决偏见问题，但无法直接对抗GNN邻域聚合过程中的偏见传播；在聚合过程中对交互应用权重可能缓解问题，但由于训练早期节点表示不稳定，可能导致模型学习扭曲。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，PPD方法在基于GNN的CF的流行度偏见纠正方面优于现有最先进的方法。&lt;h4&gt;翻译&lt;/h4&gt;用户历史交互数据是协同过滤中学习用户偏好的主要信号。然而，训练数据通常呈现长尾分布，只有少数项目拥有大部分交互。直接在这种不平衡数据上训练的CF模型容易学习流行度偏见，降低个性化程度，导致次优的推荐质量。图神经网络(GNN)由于其消息传递机制对CF有效，但可以通过聚合过程进一步传播和放大流行度偏见。现有方法通常通过修改训练目标来解决流行度偏见，但未能直接对抗GNN在邻域聚合过程中传播的偏见。在聚合过程中对交互应用权重可以帮助缓解此问题，但由于训练早期阶段节点表示不稳定，可能会扭曲模型学习。在本文中，我们提出了一种后验流行度去偏置(PPD)方法，用于纠正基于GNN的CF中的流行度偏见，并直接在预训练嵌入上操作，无需重新训练。通过估计交互级别的流行度并使用流行度方向向量从节点表示中移除流行度组件，PPD减少了偏见同时保留了用户偏好。实验结果表明，我们的方法在基于GNN的CF的流行度偏见纠正方面优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; User historical interaction data is the primary signal for learning userpreferences in collaborative filtering (CF). However, the training data oftenexhibits a long-tailed distribution, where only a few items have the majorityof interactions. CF models trained directly on such imbalanced data are proneto learning popularity bias, which reduces personalization and leads tosuboptimal recommendation quality. Graph Neural Networks (GNNs), whileeffective for CF due to their message passing mechanism, can further propagateand amplify popularity bias through their aggregation process. Existingapproaches typically address popularity bias by modifying training objectivesbut fail to directly counteract the bias propagated during GNN's neighborhoodaggregation. Applying weights to interactions during aggregation can helpalleviate this problem, yet it risks distorting model learning due to unstablenode representations in the early stages of training. In this paper, we proposea Post-hoc Popularity Debiasing (PPD) method that corrects for popularity biasin GNN-based CF and operates directly on pre-trained embeddings withoutrequiring retraining. By estimating interaction-level popularity and removingpopularity components from node representations via a popularity directionvector, PPD reduces bias while preserving user preferences. Experimentalresults show that our method outperforms state-of-the-art approaches forpopularity bias correction in GNN-based CF.</description>
      <author>example@mail.com (Md Aminul Islam, Elena Zheleva, Ren Wang)</author>
      <guid isPermaLink="false">2510.12959v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand</title>
      <link>http://arxiv.org/abs/2510.12328v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种创新的物理信息图神经网络方法，结合极值分析技术，有效提高了泰国地区降雨预测的准确性，特别是对极端事件的预测能力。&lt;h4&gt;背景&lt;/h4&gt;准确的降雨预报，尤其是极端事件的预报，在气候学和地球系统中仍然是一个重大挑战。泰国地区的站点降雨预测面临特殊挑战。&lt;h4&gt;目的&lt;/h4&gt;开发结合物理信息的图神经网络和极值分析技术，改进泰国地区的站点降雨预测，特别是提高极端事件的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;使用图结构表示站点捕捉时空模式；预处理影响区域降雨的气候指标；提出Attention-LSTM模型，利用地形降水物理公式推导边特征；采用空间季节感知广义帕累托分布方法进行阈值超限映射，解决极值预测问题。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在大多数地区优于成熟的基线模型，包括易发生极端事件的区域；与最先进方法保持竞争力；相比SEAS5业务预报系统，显著改进了极端事件预测；提供高分辨率地图支持长期水资源管理决策。&lt;h4&gt;结论&lt;/h4&gt;该方法在实际应用中提高了极端事件的预测能力，为长期水资源管理中的决策提供了实用增强。&lt;h4&gt;翻译&lt;/h4&gt;准确的降雨预报，尤其是极端事件的预报，在气候学和地球系统中仍然是一个重大挑战。本文提出了新颖的物理信息图神经网络结合极值分析技术，以改进泰国地区的站点降雨预测。该模型利用站点图的图结构表示来捕捉复杂的时空模式，并通过遥相关提供可解释性。我们预处理了可能影响区域降雨的相关气候指标。提出的带有长短期记忆的图注意力网络使用简单地形降水物理公式推导的初始边特征应用注意力机制。嵌入随后由LSTM层处理。为解决极值问题，我们使用新颖的空间季节感知广义帕累托分布方法进行阈值超限映射，这克服了传统机器学习模型的局限性。实验表明，我们的方法在大多数地区都优于成熟的基线模型，包括易发生极端事件的区域，并且与最先进的方法保持强劲竞争力。与业务预报系统SEAS5相比，我们的实际应用改进了极端事件的预测，并提供了实用增强，以支持长期水资源管理中的决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate rainfall forecasting, particularly for extreme events, remains asignificant challenge in climatology and the Earth system. This paper presentsnovel physics-informed Graph Neural Networks (GNNs) combined with extreme-valueanalysis techniques to improve gauge-station rainfall predictions acrossThailand. The model leverages a graph-structured representation of gaugestations to capture complex spatiotemporal patterns, and it offersexplainability through teleconnections. We preprocess relevant climate indicesthat potentially influence regional rainfall. The proposed Graph AttentionNetwork with Long Short-Term Memory (Attention-LSTM) applies the attentionmechanism using initial edge features derived from simpleorographic-precipitation physics formulation. The embeddings are subsequentlyprocessed by LSTM layers. To address extremes, we perform Peak-Over-Threshold(POT) mapping using the novel Spatial Season-aware Generalized ParetoDistribution (GPD) method, which overcomes limitations of traditionalmachine-learning models. Experiments demonstrate that our method outperformswell-established baselines across most regions, including areas prone toextremes, and remains strongly competitive with the state of the art. Comparedwith the operational forecasting system SEAS5, our real-world applicationimproves extreme-event prediction and offers a practical enhancement to producefine-resolution maps that support decision-making in long-term watermanagement.</description>
      <author>example@mail.com (Kiattikun Chobtham, Kanoksri Sarinnapakorn, Kritanai Torsri, Prattana Deeprasertkul, Jirawan Kamma)</author>
      <guid isPermaLink="false">2510.12328v2</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Cyclic Self-Supervised Diffusion for Ultra Low-field to High-field MRI Synthesis</title>
      <link>http://arxiv.org/abs/2510.13735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为循环自监督扩散(CSS-Diff)的框架，用于从低场MRI合成高质量高场MRI图像，解决了现有方法中存在的临床保真度差距问题。&lt;h4&gt;背景&lt;/h4&gt;低场MRI具有成本低、可及性高、安全性好等优点，但存在分辨率低和信噪比差的问题。从低场MRI合成高质量图像可以减少对昂贵采集的依赖并扩大数据可用性。&lt;h4&gt;目的&lt;/h4&gt;解决从低场MRI合成高场MRI时存在的临床保真度差距，保留解剖保真度，增强细粒度结构细节，弥合图像对比度中的域差距。&lt;h4&gt;方法&lt;/h4&gt;提出循环自监督扩散(CSS-Diff)框架，在循环一致性约束下重新制定基于扩散的合成过程，强制在整个生成过程中保持解剖结构。框架包含两个新过程：切片级差距感知网络通过对比学习对齐切片间不一致性；局部结构校正网络通过掩码和扰动块的自重建增强局部特征恢复。&lt;h4&gt;主要发现&lt;/h4&gt;在跨场合成任务上取得了最先进的性能，包括PSNR、SSIM和LPIPS指标的提升。与原始低场MRI相比，保留了细粒度解剖结构，左脑白质误差从12.1%降至2.1%，皮层从4.2%降至3.7%。&lt;h4&gt;结论&lt;/h4&gt;CSS-Diff可以合成既定量可靠又解剖一致的图像。&lt;h4&gt;翻译&lt;/h4&gt;从低场MRI合成高质量图像具有巨大潜力。低场MRI更便宜、更易获取且更安全，但分辨率低且信噪比差。这种合成过程可以减少对昂贵采集的依赖并扩大数据可用性。然而，从低场MRI合成高场MRI仍存在临床保真度差距。需要保留解剖保真度，增强细粒度结构细节，并弥合图像对比度中的域差距。为解决这些问题，我们提出了一个用于从真实低场MRI数据合成高场MRI的循环自监督扩散(CSS-Diff)框架。我们的核心思想是在循环一致性约束下重新制定基于扩散的合成过程。它在整个生成过程中强制保持解剖结构，而不仅仅依赖成对的像素级监督。CSS-Diff框架还进一步整合了两个新过程：切片级差距感知网络通过对比学习对齐切片间不一致性；局部结构校正网络通过掩码和扰动块的自重建增强局部特征恢复。在跨场合成任务上的广泛实验证明了我们方法的有效性，取得了最先进的性能。除了像素级保真度外，与原始低场MRI相比，我们的方法还保留了细粒度解剖结构。总之，我们的CSS-Diff可以合成既定量可靠又解剖一致的图像。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthesizing high-quality images from low-field MRI holds significantpotential. Low-field MRI is cheaper, more accessible, and safer, but suffersfrom low resolution and poor signal-to-noise ratio. This synthesis process canreduce reliance on costly acquisitions and expand data availability. However,synthesizing high-field MRI still suffers from a clinical fidelity gap. Thereis a need to preserve anatomical fidelity, enhance fine-grained structuraldetails, and bridge domain gaps in image contrast. To address these issues, wepropose a \emph{cyclic self-supervised diffusion (CSS-Diff)} framework forhigh-field MRI synthesis from real low-field MRI data. Our core idea is toreformulate diffusion-based synthesis under a cycle-consistent constraint. Itenforces anatomical preservation throughout the generative process rather thanjust relying on paired pixel-level supervision. The CSS-Diff framework furtherincorporates two novel processes. The slice-wise gap perception network alignsinter-slice inconsistencies via contrastive learning. The local structurecorrection network enhances local feature restoration throughself-reconstruction of masked and perturbed patches. Extensive experiments oncross-field synthesis tasks demonstrate the effectiveness of our method,achieving state-of-the-art performance (e.g., 31.80 $\pm$ 2.70 dB in PSNR,0.943 $\pm$ 0.102 in SSIM, and 0.0864 $\pm$ 0.0689 in LPIPS). Beyond pixel-wisefidelity, our method also preserves fine-grained anatomical structures comparedwith the original low-field MRI (e.g., left cerebral white matter error dropsfrom 12.1$\%$ to 2.1$\%$, cortex from 4.2$\%$ to 3.7$\%$). To conclude, ourCSS-Diff can synthesize images that are both quantitatively reliable andanatomically consistent.</description>
      <author>example@mail.com (Zhenxuan Zhang, Peiyuan Jing, Zi Wang, Ula Briski, Coraline Beitone, Yue Yang, Yinzhe Wu, Fanwen Wang, Liutao Yang, Jiahao Huang, Zhifan Gao, Zhaolin Chen, Kh Tohidul Islam, Guang Yang, Peter J. Lally)</author>
      <guid isPermaLink="false">2510.13735v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.13675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为KnowCoL（Knowledge-guided Contrastive Learning）的框架，用于开放域视觉实体识别，通过结合图像和文本描述，利用维基数据的结构化信息，将视觉和文本输入抽象到概念层面，支持零样本实体识别。&lt;h4&gt;背景&lt;/h4&gt;开放域视觉实体识别旨在识别和链接图像中描绘的实体，与维基数据等庞大且不断变化的真实世界概念集合相关联。与传统分类任务不同，它在开放集条件下运行，大多数目标实体在训练过程中未见，且呈现长尾分布，导致任务具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;解决开放域视觉实体识别任务中的挑战，包括有限的监督、高视觉歧义性和语义消歧的需要，特别是针对训练过程中未见到的实体。&lt;h4&gt;方法&lt;/h4&gt;提出KnowCoL框架，将图像和文本描述结合到由维基数据结构化信息支持的共享语义空间中。通过将视觉和文本输入抽象到概念层面，模型利用实体描述、类型层次结构和关系上下文来支持零样本实体识别。&lt;h4&gt;主要发现&lt;/h4&gt;在OVEN基准测试上评估显示，使用视觉、文本和结构化知识大大提高了准确性，特别是对于稀有和未见实体。最小的模型相比最先进的方法在未见实体上的准确性提高了10.5%，尽管模型尺寸缩小了35倍。&lt;h4&gt;结论&lt;/h4&gt;KnowCoL框架有效地解决了开放域视觉实体识别中的挑战，特别是在处理稀有和未见实体方面表现出色，同时模型尺寸显著减小。&lt;h4&gt;翻译&lt;/h4&gt;开放域视觉实体识别旨在识别和链接图像中描绘的实体，与维基数据等庞大且不断变化的真实世界概念集合相关联。与具有固定标签集的传统分类任务不同，它在开放集条件下运行，大多数目标实体在训练过程中未见，且呈现长尾分布。这使任务本身具有挑战性，因为监督有限、视觉歧义度高，且需要语义消歧。在这项工作中，我们提出了一个知识引导的对比学习框架，将图像和文本描述结合到由维基数据结构化信息支持的共享语义空间中。通过将视觉和文本输入抽象到概念层面，模型利用实体描述、类型层次结构和关系上下文来支持零样本实体识别。我们在OVEN基准上评估了我们的方法，OVEN是一个大规模开放域视觉识别数据集，以维基数据ID作为标签空间。我们的实验表明，使用视觉、文本和结构化知识大大提高了准确性，特别是对于稀有和未见实体。与最先进的方法相比，我们的最小模型在未见实体上的准确性提高了10.5%，尽管模型尺寸缩小了35倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-domain visual entity recognition aims to identify and link entitiesdepicted in images to a vast and evolving set of real-world concepts, such asthose found in Wikidata. Unlike conventional classification tasks with fixedlabel sets, it operates under open-set conditions, where most target entitiesare unseen during training and exhibit long-tail distributions. This makes thetask inherently challenging due to limited supervision, high visual ambiguity,and the need for semantic disambiguation. In this work, we propose aKnowledge-guided Contrastive Learning (KnowCoL) framework that combines bothimages and text descriptions into a shared semantic space grounded bystructured information from Wikidata. By abstracting visual and textual inputsto a conceptual level, the model leverages entity descriptions, typehierarchies, and relational context to support zero-shot entity recognition. Weevaluate our approach on the OVEN benchmark, a large-scale open-domain visualrecognition dataset with Wikidata IDs as the label space. Our experiments showthat using visual, textual, and structured knowledge greatly improves accuracy,especially for rare and unseen entities. Our smallest model improves theaccuracy on unseen entities by 10.5% compared to the state-of-the-art, despitebeing 35 times smaller.</description>
      <author>example@mail.com (Hongkuan Zhou, Lavdim Halilaj, Sebastian Monka, Stefan Schmid, Yuqicheng Zhu, Jingcheng Wu, Nadeem Nazer, Steffen Staab)</author>
      <guid isPermaLink="false">2510.13675v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning-Based Dependency Modeling for Anomaly Detection in Cloud Services</title>
      <link>http://arxiv.org/abs/2510.13368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合对比学习的依赖建模和异常检测方法，解决了云服务环境中复杂依赖关系和多样化异常模式的挑战&lt;h4&gt;背景&lt;/h4&gt;云服务环境中存在复杂依赖关系和多样化异常模式的挑战&lt;h4&gt;目的&lt;/h4&gt;提出一种结合对比学习的依赖建模和异常检测方法，解决云服务环境中的异常检测问题&lt;h4&gt;方法&lt;/h4&gt;将服务交互抽象为依赖图，通过嵌入函数提取时间和结构特征，使用图卷积机制聚合邻域信息实现上下文感知的服务表示，引入对比学习框架构建正负样本对增强正常和异常模式可分性，设计时间一致性约束保持表示稳定性，结合对比损失和时间一致性损失进行整体优化&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集上从超参数、环境和数据敏感性角度系统评估了该方法，在精确率、召回率、F1分数和AUC等关键指标上显著优于现有方法，在稀疏标记、监控噪声和流量波动条件下保持鲁棒性&lt;h4&gt;结论&lt;/h4&gt;验证了将依赖建模与对比学习结合的有效性，为云服务异常检测提供了完整的技术解决方案，在复杂环境中表现出强大的适应性和稳定性&lt;h4&gt;翻译&lt;/h4&gt;本文通过提出一种结合对比学习的依赖建模和异常检测方法，解决了云服务环境中复杂依赖关系和多样化异常模式的挑战。该方法将服务交互抽象为依赖图，通过嵌入函数提取时间和结构特征，并采用图卷积机制聚合邻域信息以实现上下文感知的服务表示。随后引入对比学习框架，构建正负样本对以增强正常和异常模式在表示空间中的可分性。此外，设计了时间一致性约束以保持跨时间步的表示稳定性，减少短期波动和噪声的影响。整体优化结合了对比损失和时间一致性损失，确保在多维度特征下的稳定可靠检测。在公共数据集上从超参数、环境和数据敏感性角度对该方法进行了系统评估。结果表明，在精确率、召回率、F1分数和AUC等关键指标上，所提出的方法显著优于现有方法，同时在稀疏标记、监控噪声和流量波动条件下保持鲁棒性。本研究验证了结合依赖建模与对比学习的有效性，为云服务异常检测提供了完整的技术解决方案，并在复杂环境中表现出强大的适应性和稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenges of complex dependencies and diverseanomaly patterns in cloud service environments by proposing a dependencymodeling and anomaly detection method that integrates contrastive learning. Themethod abstracts service interactions into a dependency graph, extractstemporal and structural features through embedding functions, and employs agraph convolution mechanism to aggregate neighborhood information forcontext-aware service representations. A contrastive learning framework is thenintroduced, constructing positive and negative sample pairs to enhance theseparability of normal and abnormal patterns in the representation space.Furthermore, a temporal consistency constraint is designed to maintainrepresentation stability across time steps and reduce the impact of short-termfluctuations and noise. The overall optimization combines contrastive loss andtemporal consistency loss to ensure stable and reliable detection acrossmulti-dimensional features. Experiments on public datasets systematicallyevaluate the method from hyperparameter, environmental, and data sensitivityperspectives. Results show that the proposed approach significantly outperformsexisting methods on key metrics such as Precision, Recall, F1-Score, and AUC,while maintaining robustness under conditions of sparse labeling, monitoringnoise, and traffic fluctuations. This study verifies the effectiveness ofintegrating dependency modeling with contrastive learning, provides a completetechnical solution for cloud service anomaly detection, and demonstrates strongadaptability and stability in complex environments.</description>
      <author>example@mail.com (Yue Xing, Yingnan Deng, Heyao Liu, Ming Wang, Yun Zi, Xiaoxuan Sun)</author>
      <guid isPermaLink="false">2510.13368v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Universal Image Restoration Pre-training via Masked Degradation Classification</title>
      <link>http://arxiv.org/abs/2510.13282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种掩码退化分类预训练方法（MaskDCPT），用于图像退化类型分类，从而实现全面的图像恢复预训练。该方法使用图像退化类型作为弱监督，同时利用图像重建增强性能和鲁棒性。MaskDCPT包含一个编码器和两个解码器，分别用于特征提取、退化类型分类和高质量图像重建。该方法结合了掩码图像建模和对比学习的优势，显著提升了CNN和Transformer在图像恢复任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;传统预训练方法在图像恢复任务中存在局限性，需要一种能够处理多种退化类型的通用图像恢复方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够分类图像退化类型的预训练方法，实现全面的图像恢复预训练，提高模型在通用图像恢复任务中的性能和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出MaskDCPT方法，使用图像退化类型作为弱监督。构建包含一个编码器和两个解码器的架构：编码器从掩码的低质量输入图像中提取特征；分类解码器使用这些特征识别退化类型；重建解码器重建对应的高质量图像。利用掩码图像建模和对比学习的好处。构建UIR-2.5M数据集，包含250万对恢复样本，覆盖19种退化类型和200多种退化水平。&lt;h4&gt;主要发现&lt;/h4&gt;MaskDCPT显著提高了CNN和Transformer的性能，在5D全一恢复任务中PSNR至少提高3.77分贝，在真实退化场景中PIQE减少34.8%。模型对未见过的退化类型和水平表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MaskDCPT是一种简单而强大的预训练方法，可用于通用图像恢复，能够处理多种退化类型并在各种场景中表现出色。发布的UIR-2.5M数据集、源代码和模型可供社区使用。&lt;h4&gt;翻译&lt;/h4&gt;本研究介绍了一种掩码退化分类预训练方法（MaskDCPT），旨在促进输入图像中退化类型的分类，从而实现全面的图像恢复预训练。与传统预训练方法不同，MaskDCPT使用图像的退化类型作为极弱监督，同时利用图像重建来增强性能和鲁棒性。MaskDCPT包含一个编码器和两个解码器：编码器从掩码的低质量输入图像中提取特征；分类解码器使用这些特征识别退化类型，而重建解码器旨在重建相应的高质量图像。这种设计使预训练能够受益于掩码图像建模和对比学习，从而生成适合恢复任务的通用表示。得益于简单而强大的MaskDCPT，预训练的编码器可用于解决通用图像恢复并取得卓越性能。实施MaskDCPT显著提高了卷积神经网络（CNN）和Transformer的性能，在5D全一恢复任务中PSNR最小提高3.77分贝，在真实退化场景中与基线相比PIQE减少34.8%。它还对以前未见过的退化类型和水平表现出强大的泛化能力。此外，我们整理并发布了UIR-2.5M数据集，包含250万对恢复样本，涵盖19种退化类型和200多种退化水平，包括合成和真实世界数据。该数据集、源代码和模型可在https://github.com/MILab-PKU/MaskDCPT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces a Masked Degradation Classification Pre-Training method(MaskDCPT), designed to facilitate the classification of degradation types ininput images, leading to comprehensive image restoration pre-training. Unlikeconventional pre-training methods, MaskDCPT uses the degradation type of theimage as an extremely weak supervision, while simultaneously leveraging theimage reconstruction to enhance performance and robustness. MaskDCPT includesan encoder and two decoders: the encoder extracts features from the maskedlow-quality input image. The classification decoder uses these features toidentify the degradation type, whereas the reconstruction decoder aims toreconstruct a corresponding high-quality image. This design allows thepre-training to benefit from both masked image modeling and contrastivelearning, resulting in a generalized representation suited for restorationtasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trainedencoder can be used to address universal image restoration and achieveoutstanding performance. Implementing MaskDCPT significantly improvesperformance for both convolution neural networks (CNNs) and Transformers, witha minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task anda 34.8% reduction in PIQE compared to baseline in real-world degradationscenarios. It also emergences strong generalization to previously unseendegradation types and levels. In addition, we curate and release the UIR-2.5Mdataset, which includes 2.5 million paired restoration samples across 19degradation types and over 200 degradation levels, incorporating both syntheticand real-world data. The dataset, source code, and models are available athttps://github.com/MILab-PKU/MaskDCPT.</description>
      <author>example@mail.com (JiaKui Hu, Zhengjian Yao, Lujia Jin, Yinghao Chen, Yanye Lu)</author>
      <guid isPermaLink="false">2510.13282v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive Learning and Bar-Equivariant Contact-Aware Encoding</title>
      <link>http://arxiv.org/abs/2510.13244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure. demo page: https://motionbeat2025.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MotionBeat是一个运动对齐的音乐表示学习框架，通过具身对比损失和结构节奏对齐损失，以及小节等变相旋转和接触引导注意力等创新架构，成功捕捉了音乐的具身维度，在音乐到舞蹈生成和多种音频处理任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;音乐既是听觉现象也是具身现象，与人体运动密切相关，但现有音频表示忽略了这种具身维度，限制了捕捉驱动运动的节奏和结构线索的能力。&lt;h4&gt;目的&lt;/h4&gt;提出MotionBeat框架，用于学习能够捕捉音乐运动特性的音乐表示。&lt;h4&gt;方法&lt;/h4&gt;采用两个训练目标：具身对比损失(ECL)实现细粒度节奏区分，结构节奏对齐损失(SRAL)确保节奏一致性；架构上引入小节等变相旋转捕捉循环节奏模式，以及接触引导注意力强调与音乐重音同步的运动事件。&lt;h4&gt;主要发现&lt;/h4&gt;MotionBeat在音乐到舞蹈生成方面优于最先进的音频编码器，并在节拍跟踪、音乐标记、流派和乐器分类、情感识别以及视听检索等任务中有效迁移。&lt;h4&gt;结论&lt;/h4&gt;MotionBeat框架成功捕捉了音乐的具身维度，提高了音乐表示的质量，在多种音频处理任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;音乐既是听觉现象也是具身现象，与人体运动密切相关，并通过舞蹈自然表达。然而，大多数现有的音频表示忽略了这种具身维度，限制了它们捕捉驱动运动的节奏和结构线索的能力。我们提出了MotionBeat，一个用于运动对齐的音乐表示学习框架。MotionBeat通过两个新提出的目标进行训练：具身对比损失(ECL)，一种具有速度感知和节拍抖动负样本的增强型InfoNCE公式，用于实现细粒度节奏区分；以及结构节奏对齐损失(SRAL)，通过将音乐重音与相应运动事件对齐来确保节奏一致性。在架构上，MotionBeat引入了小节等变相旋转来捕捉循环节奏模式，以及接触引导注意力来强调与音乐重音同步的运动事件。实验表明，MotionBeat在音乐到舞蹈生成方面优于最先进的音频编码器，并有效迁移到节拍跟踪、音乐标记、流派和乐器分类、情感识别以及视听检索等任务。我们的项目演示页面：https://motionbeat2025.github.io/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Music is both an auditory and an embodied phenomenon, closely linked to humanmotion and naturally expressed through dance. However, most existing audiorepresentations neglect this embodied dimension, limiting their ability tocapture rhythmic and structural cues that drive movement. We proposeMotionBeat, a framework for motion-aligned music representation learning.MotionBeat is trained with two newly proposed objectives: the EmbodiedContrastive Loss (ECL), an enhanced InfoNCE formulation with tempo-aware andbeat-jitter negatives to achieve fine-grained rhythmic discrimination, and theStructural Rhythm Alignment Loss (SRAL), which ensures rhythm consistency byaligning music accents with corresponding motion events. Architecturally,MotionBeat introduces bar-equivariant phase rotations to capture cyclicrhythmic patterns and contact-guided attention to emphasize motion eventssynchronized with musical accents. Experiments show that MotionBeat outperformsstate-of-the-art audio encoders in music-to-dance generation and transferseffectively to beat tracking, music tagging, genre and instrumentclassification, emotion recognition, and audio-visual retrieval. Our projectdemo page: https://motionbeat2025.github.io/.</description>
      <author>example@mail.com (Xuanchen Wang, Heng Wang, Weidong Cai)</author>
      <guid isPermaLink="false">2510.13244v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning</title>
      <link>http://arxiv.org/abs/2510.13176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GRACE是一个创新的编译器自动调优框架，通过利用通道协同性和加权评分方法缩小搜索空间，使用对比学习和相似感知聚类创建程序嵌入，并在聚类内进行进化搜索，生成针对未见程序具有强泛化能力的核心集通道序列。实验表明，GRACE在LLVM IR指令计数优化方面达到了最先进的性能，同时保持了高效的调优时间。&lt;h4&gt;背景&lt;/h4&gt;编译器通道选择和阶段排序是实现最优程序性能的重大挑战，特别是对于代码大小缩减等目标。标准编译器启发式方法具有通用适用性，但由于其'一刀切'的特性，通常会产生次优的、程序特定的结果。虽然迭代编译可以找到量身定制的解决方案，但其高昂的搜索成本限制了实际应用。机器学习方法承诺更快的推理速度，但经常难以泛化到未见程序。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的编译器自动调优框架，能够在保持快速调优时间的同时，为未见程序提供高质量的优化解决方案，特别是在LLVM IR指令计数优化方面。&lt;h4&gt;方法&lt;/h4&gt;GRACE框架首先利用通道协同性和加权评分方法生成初始高质量候选序列和通道池，有效缩小搜索空间。然后采用对比学习方法，使用基于通道序列的数据增强技术创建程序嵌入，促进相似感知聚类。在这些聚类内进行进化搜索，生成k个专门设计的通道序列核心集，旨在对未见程序实现强泛化能力。在测试时，GRACE高效选择最佳核心集序列并使用轻量级技术进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;在七个不同的数据集上的实验结果表明，GRACE与opt -Oz相比，在LLVM 10.0.0上将LLVM IR指令计数平均减少了10.09%，在LLVM 18.1.6上平均减少了10.19%，同时每个程序的调优时间平均不到1秒，展示了其最先进的性能和实际有效性。&lt;h4&gt;结论&lt;/h4&gt;GRACE框架成功解决了编译器自动调优中的搜索空间过大和泛化能力不足的问题，通过结合通道协同性、加权评分、对比学习和进化搜索等技术，实现了在保持高效调优时间的同时，为未见程序提供高质量优化的能力，在LLVM IR指令计数优化方面达到了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;编译器通道选择和阶段排序是实现最优程序性能的重大挑战，特别是对于代码大小缩减等目标。标准编译器启发式方法具有通用适用性，但由于其'一刀切'的特性，通常会产生次优的、程序特定的结果。虽然迭代编译可以找到量身定制的解决方案，但其高昂的搜索成本限制了实际应用。机器学习方法承诺更快的推理速度，但经常难以泛化到未见程序。本文介绍了GRACE，一个用于编译器自动调优的新颖框架，已在LLVM IR指令计数优化中得到验证。GRACE通过利用通道协同性和加权评分方法有效缩小搜索空间，生成初始高质量候选序列和通道池。然后采用对比学习方法，使用基于通道序列的数据增强技术创建程序嵌入，促进相似感知聚类。在这些聚类内进行进化搜索，生成k个专门设计的通道序列核心集，旨在对未见程序实现强泛化能力。在测试时，GRACE高效选择最佳核心集序列并使用轻量级技术进行优化。在七个不同数据集上的实验结果表明，GRACE与opt -Oz相比，在LLVM 10.0.0上将LLVM IR指令计数平均减少了10.09%，在LLVM 18.1.6上平均减少了10.19%，同时每个程序的调优时间平均不到1秒，展示了其最先进的性能和实际有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compiler pass selection and phase ordering present a significant challenge inachieving optimal program performance, particularly for objectives like codesize reduction. Standard compiler heuristics offer general applicability butoften yield suboptimal, program-specific results due to their one-size-fits-allnature. While iterative compilation can find tailored solutions, itsprohibitive search cost limits practical use. Machine learning approachespromise faster inference but frequently struggle with generalization to unseenprograms. This paper introduces GRACE, a novel framework for compilerauto-tuning, demonstrated for LLVM IR instruction count optimization. GRACEeffectively curtails the search space by leveraging pass synergies and aweighted scoring method to generate initial high-quality candidate sequencesand a pass pool. It then employs contrastive learning, using passsequence-based data augmentation, to create program embeddings that facilitatesimilarity-aware clustering. Evolutionary search within these clusters yields acoreset of $k$ specialized pass sequences designed for robust generalization tounseen programs. At test time, GRACE efficiently selects the best coresetsequence and refines it using lightweight techniques. Experimental results onseven diverse datasets show that GRACE reduces LLVM IR instruction count by anaverage of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,while incurring an average tuning time of less than 1s per program,demonstrating its state-of-the-art performance and practical effectiveness.</description>
      <author>example@mail.com (Haolin Pan, Chao Zha, Jinyuan Dong, Mingjie Xing, Yanjun Wu)</author>
      <guid isPermaLink="false">2510.13176v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>VCTR: A Transformer-Based Model for Non-parallel Voice Conversion</title>
      <link>http://arxiv.org/abs/2510.12964v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VCTR的高效非并行语音转换方法，结合了混合感知块和双剪枝自注意力机制，采用基于对比学习的对抗方法，解决了现有方法中存在的长距离依赖捕获不足的问题。&lt;h4&gt;背景&lt;/h4&gt;非并行语音转换技术旨在无需配对训练数据的情况下将源语音域转换为目标语音域。现有的CycleGAN、VAE和CVC等方法在训练效果和语义捕获方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕获语音中长距离依赖关系的高效非并行语音转换方法，以提升转换质量和全局语义表达能力。&lt;h4&gt;方法&lt;/h4&gt;提出VCTR方法，结合了Hybrid Perception Block (HPB)和Dual Pruned Self-Attention (DPSA)技术，采用基于对比学习的对抗训练框架，能够更好地捕获语音中的长距离依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;基于CNN的生成器虽然能捕获局部语义，但缺乏捕获全局语义所需的长距离依赖能力；所提出的VCTR方法通过结合HPB和DPSA有效解决了这一问题。&lt;h4&gt;结论&lt;/h4&gt;VCTR是一种高效的非并行语音转换方法，通过创新的网络结构和训练策略，显著提升了语音转换的质量和全局语义表达能力。&lt;h4&gt;翻译&lt;/h4&gt;非并行语音转换旨在无需配对训练数据的情况下将语音从源域转换到目标域。循环一致性生成对抗网络(CycleGAN)和变分自编码器(VAE)已被用于此任务，但这些模型面临训练困难和结果不理想的问题。后来，对比语音转换(Contrastive Voice Conversion, CVC)被提出，利用基于对比学习的方法解决这些问题。然而，这些方法使用基于CNN的生成器，虽然可以捕获局部语义，但缺乏捕获全局语义所需的长距离依赖能力。在本文中，我们提出了VCTR，一种用于非并行语音转换的高效方法，它结合了混合感知块(HPB)和双剪枝自注意力(DPSA)，以及基于对比学习的对抗方法。代码可在https://github.com/Maharnab-Saikia/VCTR找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Non-parallel voice conversion aims to convert voice from a source domain to atarget domain without paired training data. Cycle-Consistent GenerativeAdversarial Networks (CycleGAN) and Variational Autoencoders (VAE) have beenused for this task, but these models suffer from difficult training andunsatisfactory results. Later, Contrastive Voice Conversion (CVC) wasintroduced, utilizing a contrastive learning-based approach to address theseissues. However, these methods use CNN-based generators, which can capturelocal semantics but lacks the ability to capture long-range dependenciesnecessary for global semantics. In this paper, we propose VCTR, an efficientmethod for non-parallel voice conversion that leverages the Hybrid PerceptionBlock (HPB) and Dual Pruned Self-Attention (DPSA) along with a contrastivelearning-based adversarial approach. The code can be found inhttps://github.com/Maharnab-Saikia/VCTR.</description>
      <author>example@mail.com (Maharnab Saikia)</author>
      <guid isPermaLink="false">2510.12964v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation</title>
      <link>http://arxiv.org/abs/2510.13245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SketchSem3D，第一个从抽象手绘草图和卫星图像伪标签注释生成3D户外语义场景的大规模基准数据集，以及Cylinder Mamba Diffusion (CymbaDiff)方法，显著增强了户外3D场景生成的空间连贯性。&lt;h4&gt;背景&lt;/h4&gt;户外3D语义场景生成技术为城市模拟和自动驾驶等应用提供逼真且语义丰富的环境，但该领域发展受限于缺乏公开可用的、良好注释的数据集。&lt;h4&gt;目的&lt;/h4&gt;引入SketchSem3D基准数据集，用于从抽象手绘草图和卫星图像的伪标签注释生成3D户外语义场景。&lt;h4&gt;方法&lt;/h4&gt;SketchSem3D包含两个子集：基于Sketch的SemanticKITTI和基于Sketch的KITTI-360（包含LiDAR体素及其相应的草图和注释卫星图像）。提出Cylinder Mamba Diffusion (CymbaDiff)方法，施加结构化空间排序，捕获圆柱连续性和垂直层次结构，保持物理邻域关系和全局上下文。&lt;h4&gt;主要发现&lt;/h4&gt;在SketchSem3D上的大量实验表明，CymbaDiff实现了卓越的语义一致性、空间真实性和跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;代码和数据集将在https://github.com/Lillian-research-hub/CymbaDiff上提供。&lt;h4&gt;翻译&lt;/h4&gt;户外3D语义场景生成为城市模拟和自动驾驶等应用生成逼真且语义丰富的环境。然而，这一方向的进展受到缺乏公开可用、良好注释的数据集的限制。我们引入SketchSem3D，这是第一个从抽象手绘草图和卫星图像的伪标签注释生成3D户外语义场景的大规模基准。SketchSem3D包含两个子集：基于Sketch的SemanticKITTI和基于Sketch的KITTI-360（包含LiDAR体素及其相应的草图和注释卫星图像），以实现标准化、严格和多样化的评估。我们还提出了Cylinder Mamba Diffusion (CymbaDiff)，显著增强了户外3D场景生成的空间连贯性。CymbaDiff施加结构化空间排序，明确捕获圆柱连续性和垂直层次结构，并在生成的场景中保持物理邻域关系和全局上下文。在SketchSem3D上的大量实验表明，CymbaDiff实现了卓越的语义一致性、空间真实性和跨数据集泛化能力。代码和数据集将在https://github.com/Lillian-research-hub/CymbaDiff上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决户外3D语义场景生成的问题，特别是从手绘草图和伪标记卫星图像注释生成3D城市场景。这个问题很重要，因为生成逼真且语义丰富的户外环境对城市模拟和自动驾驶等应用至关重要，但该领域缺乏公开、良好标注的数据集，且现有方法难以处理户外场景的高语义多样性、复杂空间结构和动态上下文依赖。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到户外3D场景生成的重要性及现有方法的局限性，然后构建了SketchSem3D数据集作为基础。方法设计借鉴了状态空间模型(SSMs)在捕获长程依赖关系方面的优势，以及扩散模型在生成任务中的成功经验。作者创新性地结合了笛卡尔和圆柱坐标系统，设计了圆柱Mamba块(CylMa)来增强空间一致性，同时保留了三重Mamba模块以保持精确几何距离。整体架构包括场景结构估计网络、潜在映射网络和去噪网络，通过多尺度特征提取和维度分解残差块来提升性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结构化空间扩散结合圆柱和笛卡尔坐标系统的优势，增强户外3D场景生成的空间一致性。整体流程包括：1)数据预处理，生成草图和伪标记卫星图像注释；2)使用场景结构估计网络提取抽象结构信息；3)通过变分自编码器将输入条件压缩为潜在表示；4)在潜在空间中使用圆柱Mamba块进行去噪扩散；5)融合三重Mamba和圆柱Mamba的特征，结合径向和轴对齐的空间线索；6)生成最终的3D语义场景，每个体素被分配语义类标签。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'基于草图的3D户外场景生成'新任务；2)构建SketchSem3D数据集，提供更高分辨率(256×256×32)、更多语义类别(20类)和更丰富的地理空间语义；3)设计圆柱Mamba扩散(CymbaDiff)模型；4)通过结构化空间排序捕获圆柱连续性和垂直层次结构；5)利用草图和伪标记卫星图像注释作为多模态条件输入。相比之前工作，CymbaDiff结合了圆柱和笛卡尔坐标系统，更好地表示户外场景结构；通过状态空间模型和扩散模型结合，更高效地捕获长程依赖；将草图生成从孤立对象和简单室内场景扩展到复杂户外场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了SketchSem3D数据集和CymbaDiff方法，首次实现了从手绘草图和伪标记卫星图像注释生成高质量、空间一致的3D户外语义场景，为城市模拟和自动驾驶等应用提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outdoor 3D semantic scene generation produces realistic and semantically richenvironments for applications such as urban simulation and autonomous driving.However, advances in this direction are constrained by the absence of publiclyavailable, well-annotated datasets. We introduce SketchSem3D, the firstlarge-scale benchmark for generating 3D outdoor semantic scenes from abstractfreehand sketches and pseudo-labeled annotations of satellite images.SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-basedKITTI-360 (containing LiDAR voxels along with their corresponding sketches andannotated satellite images), to enable standardized, rigorous, and diverseevaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) thatsignificantly enhances spatial coherence in outdoor 3D scene generation.CymbaDiff imposes structured spatial ordering, explicitly captures cylindricalcontinuity and vertical hierarchy, and preserves both physical neighborhoodrelationships and global context within the generated scenes. Extensiveexperiments on SketchSem3D demonstrate that CymbaDiff achieves superiorsemantic consistency, spatial realism, and cross-dataset generalization. Thecode and dataset will be available athttps://github.com/Lillian-research-hub/CymbaDiff</description>
      <author>example@mail.com (Li Liang, Bo Miao, Xinyu Wang, Naveed Akhtar, Jordan Vice, Ajmal Mian)</author>
      <guid isPermaLink="false">2510.13245v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue</title>
      <link>http://arxiv.org/abs/2510.13747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;InteractiveOmni是一个统一的开源多模态大语言模型，参数规模从4B到8B，专注于音频视觉多轮交互，整合了视觉编码器、音频编码器、大语言模型和语音解码器，采用多阶段训练策略，具有强大的跨模态能力和类人长期对话能力。&lt;h4&gt;背景&lt;/h4&gt;轻量级模型领域需要全面的多模态理解和语音生成能力，现有模型可能在这一方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的、开源的多模态大语言模型，引领轻量级模型领域，提供全面的多模态理解和语音生成能力。&lt;h4&gt;方法&lt;/h4&gt;将视觉编码器、音频编码器、大语言模型和语音解码器整合到统一模型中；设计多阶段训练策略：预训练用于多模态理解，然后进行语音对话和视听交互的后训练；精心策划多轮训练数据集，增强处理复杂多轮交互的能力；构建多模态多轮记忆基准和多轮语音交互基准，用于评估多轮记忆和语音交互能力。&lt;h4&gt;主要发现&lt;/h4&gt;InteractiveOmni显著优于领先的开源模型；InteractiveOmni-4B在通用基准上可与更大的Qwen2.5-Omni-7B模型相媲美；InteractiveOmni-4B仅使用50%的模型大小就能保留InteractiveOmni-8B 97%的性能；在图像、音频、视频理解和语音生成任务上，与同等规模的模型相比取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;InteractiveOmni是下一代智能交互系统的可访问开源基础模型，提供了更智能的多轮音频视觉体验，特别是在长期记忆能力方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了InteractiveOmni，这是一个统一的开源多模态大语言模型，参数规模从4B到8B，专为音频视觉多轮交互设计，通过提供全面的多模态理解和语音生成能力引领轻量级模型领域。为此，我们将视觉编码器、音频编码器、大语言模型和语音解码器整合到一个统一模型中，用于理解和生成任务。我们设计了一个多阶段训练策略，以确保强大的跨模态能力，包括预训练用于多模态理解，随后进行语音对话和视听交互的后训练。为了实现类人长期对话能力，我们精心策划了一个多轮训练数据集，增强模型处理复杂多轮交互的能力。为了有效评估多轮记忆和语音交互能力，我们构建了多模态多轮记忆基准和多轮语音交互基准。实验证明，InteractiveOmni显著优于领先的开源模型，提供了更智能的多轮音频视觉体验，特别是在其长期记忆能力方面。值得注意的是，InteractiveOmni-4B在通用基准上可与大得多的Qwen2.5-Omni-7B模型相媲美，同时仅利用50%的模型大小就能保留InteractiveOmni-8B 97%的性能。在图像、音频、视频理解和语音生成任务上，与同等规模的模型相比取得了最先进的结果，InteractiveOmni是下一代智能交互系统的可访问开源基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce InteractiveOmni, a unified and open-source omni-modal largelanguage model for audio-visual multi-turn interaction, ranging from 4B to 8Bparameters, designed to lead the field of lightweight models by offeringcomprehensive omni-modal understanding and speech generation capabilities. Toachieve this, we integrate the vision encoder, audio encoder, large languagemodel, and speech decoder into a unified model for understanding and generationtasks. We design a multi-stage training strategy to ensure robust cross-modalcapabilities, including pre-training for omni-modal understanding, followed bypost-training with speech conversation and audio-visual interaction. To enablehuman-like long-term conversational ability, we meticulously curate amulti-turn training dataset that enhances the model's ability to handle complexand multi-turn interactions. To effectively evaluate the multi-turn memory andspeech interaction capabilities, we construct the multi-modal multi-turn memorybenchmark and the multi-turn speech interaction benchmark. Experimentsdemonstrate that InteractiveOmni significantly outperforms leading open-sourcemodels and provides a more intelligent multi-turn audio-visual experience,particularly in its long-term memory capabilities. Notably, InteractiveOmni-4Bis comparable to the much larger model like Qwen2.5-Omni-7B on generalbenchmarks, and it can retain 97% of the performance of the InteractiveOmni-8Bwhile utilizing only 50% of the model size. Achieving state-of-the-art resultsagainst similarly sized models across image, audio, video understanding, andspeech generation tasks, InteractiveOmni is an accessible, open-sourcefoundation for next-generation intelligent interactive systems.</description>
      <author>example@mail.com (Wenwen Tong, Hewei Guo, Dongchuan Ran, Jiangnan Chen, Jiefan Lu, Kaibin Wang, Keqiang Li, Xiaoxu Zhu, Jiakui Li, Kehan Li, Xueheng Li, Lumin Li, Chenxu Guo, Jiasheng Zhou, Jiandong Chen, Xianye Wu, Jiahao Wang, Silei Wu, Lei Chen, Hanming Deng, Yuxuan Song, Dinghao Zhou, Guiping Zhong, Ken Zheng, Shiyin Kang, Lewei Lu)</author>
      <guid isPermaLink="false">2510.13747v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Bayesian Modeling of Dengue in Recife, Brazil (2015-2024): The Role of Spatial Granularity and Data Quality for Epidemiological Risk Mapping</title>
      <link>http://arxiv.org/abs/2510.13672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 12 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究使用贝叶斯分层时空模型分析了巴西累西腓市2015-2024年的登革热病例，探讨了多种社会环境和气候因素对登革热风险的影响，并识别了高风险区域。&lt;h4&gt;背景&lt;/h4&gt;登革热是巴西主要的流行病学挑战之一，表现为城市内部的不平等以及气候和社会环境因素的影响。&lt;h4&gt;目的&lt;/h4&gt;分析2015-2024年累西腓市的登革热确诊病例，评估多种因素对登革热风险的影响。&lt;h4&gt;方法&lt;/h4&gt;使用R-INLA实现的贝叶斯分层时空模型，结合BYM2空间结构和RW1时间成分。纳入的协变量包括人口密度、家庭规模、收入、排水渠道、滞后降水量和平均温度。&lt;h4&gt;主要发现&lt;/h4&gt;人口密度和家庭规模增加登革热风险；收入和排水渠道具有保护作用；滞后降水量增加风险；高温显示反向关联，表明媒介活动的热阈值；模型拟合良好，收敛稳定；北部和西部存在持续高风险集群，与高密度和社会脆弱性区域重叠。&lt;h4&gt;结论&lt;/h4&gt;贝叶斯模型支持概率预测和早期预警系统。与经典模型相比，INLA明确整合了不确定性和时空依赖性，为城市健康管理决策提供了可信区间推断。&lt;h4&gt;翻译&lt;/h4&gt;登革热仍然是巴西主要的流行病学挑战之一，表现为城市内部的不平等以及气候和社会环境因素的影响。本研究使用R-INLA实现的贝叶斯分层时空模型分析了2015-2024年累西腓市的登革热确诊病例，结合了BYM2空间结构和RW1时间成分。协变量包括人口密度、家庭规模、收入、排水渠道、滞后降水量和平均温度。人口密度和家庭规模对登革热风险有正向影响，而收入和渠道存在具有保护作用。滞后降水量增加风险，较高温度显示反向关联，表明媒介活动的热阈值。模型拟合良好，收敛稳定，具有中等程度的残差空间自相关和2016-2019年间平滑的时间趋势。空间时间估计显示累西腓北部和西部持续存在高风险集群，与较高密度和社会脆弱性区域重叠。除了重现历史模式外，贝叶斯模型还支持概率预测和早期预警系统。与经典模型相比，INLA明确整合了不确定性和时空依赖性，为城市健康管理决策提供了可信区间推断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dengue remains one of Brazil's major epidemiological challenges, marked bystrong intra-urban inequalities and the influence of climatic andsocio-environmental factors. This study analyzed confirmed dengue cases inRecife from 2015 to 2024 using a Bayesian hierarchical spatio-temporal modelimplemented in R-INLA, combining a BYM2 spatial structure with an RW1 temporalcomponent. Covariates included population density, household size, income,drainage channels, lagged precipitation, and mean temperature. Populationdensity and household size had positive effects on dengue risk, while incomeand channel presence were protective. Lagged precipitation increased risk, andhigher temperatures showed an inverse association, suggesting thermalthresholds for vector activity. The model achieved good fit (DIC=65817;WAIC=64506) and stable convergence, with moderate residual spatialautocorrelation (phi=0.06) and a smooth temporal trend between 2016 and 2019.Spatio-temporal estimates revealed persistent high-risk clusters in northernand western Recife, overlapping with areas of higher density and socialvulnerability. Beyond reproducing historical patterns, the Bayesian modelsupports probabilistic forecasting and early warning systems. Compared withclassical models (GLM, SAR, GWR, GTWR), INLA explicitly integrates uncertaintyand spatial-temporal dependence, offering credible interval inference fordecision-making in urban health management.</description>
      <author>example@mail.com (Marcílio Ferreira dos Santos, Andreza dos Santos Rodrigues de Melo)</author>
      <guid isPermaLink="false">2510.13672v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning</title>
      <link>http://arxiv.org/abs/2510.13614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MemoTime是一种记忆增强的时间知识图谱框架，通过结构化基础、递归推理和持续经验学习解决大型语言模型在时间理解方面的挑战，显著提升了模型在时间问答任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型已展现出强大的推理能力，但在处理涉及多个实体、复合运算符和演变事件序列的时间理解问题时存在困难。时间知识图谱虽提供了结构化的时间事实，但现有基于TKG的LLM推理方法仍面临四大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有TKG-based LLM推理方法面临的四大挑战：保持多跳推理的时间忠实性、实现多实体时间同步、适应不同时间运算符的检索、重用先验推理经验以提高稳定性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出MemoTime框架，将复杂时间问题分解为层次化的时间树，实现运算符感知推理；包含动态证据检索层自适应选择策略；以及自我演化的经验记忆存储已验证推理轨迹、工具包决策和子问题嵌入用于跨类型重用。&lt;h4&gt;主要发现&lt;/h4&gt;在多个时间问答基准上，MemoTime取得了最先进的结果，比强基线模型高出24.0%；使较小模型(如Qwen3-4B)能达到与GPT-4-Turbo相当的推理性能。&lt;h4&gt;结论&lt;/h4&gt;MemoTime有效解决了现有方法面临的四大挑战，显著提升了大型语言模型在时间理解方面的能力，并使较小模型也能达到高性能水平。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)已经取得了令人印象深刻的推理能力，但在时间理解方面存在困难，特别是当问题涉及多个实体、复合运算符和不断演变的事件序列时。时间知识图谱(TKGs)以结构化格式捕获大量时间事实，为时间推理提供了可靠来源。然而，现有的基于TKG的LLM推理方法仍面临四大挑战：在多跳推理中保持时间忠实性、实现多实体时间同步、使检索适应不同的时间运算符、重用先前的推理经验以提高稳定性和效率。为解决这些问题，我们提出了MemoTime，这是一个记忆增强的时间知识图谱框架，通过结构化基础、递归推理和持续经验学习来增强LLM推理。MemoTime将复杂的时间问题分解为层次化的时间树，实现运算符感知推理，强制单调时间戳并在统一时间边界下共同约束多个实体。动态证据检索层自适应地选择特定运算符的检索策略，而自我演化的经验记忆存储已验证的推理轨迹、工具包决策和子问题嵌入用于跨类型重用。在多个时间问答基准上的综合实验显示，MemoTime取得了最先进的结果，比强大的基线模型高出24.0%。此外，MemoTime使较小的模型(如Qwen3-4B)能够实现与GPT-4-Turbo相当的推理性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have achieved impressive reasoning abilities,but struggle with temporal understanding, especially when questions involvemultiple entities, compound operators, and evolving event sequences. TemporalKnowledge Graphs (TKGs), which capture vast amounts of temporal facts in astructured format, offer a reliable source for temporal reasoning. However,existing TKG-based LLM reasoning methods still struggle with four majorchallenges: maintaining temporal faithfulness in multi-hop reasoning, achievingmulti-entity temporal synchronization, adapting retrieval to diverse temporaloperators, and reusing prior reasoning experience for stability and efficiency.To address these issues, we propose MemoTime, a memory-augmented temporalknowledge graph framework that enhances LLM reasoning through structuredgrounding, recursive reasoning, and continual experience learning. MemoTimedecomposes complex temporal questions into a hierarchical Tree of Time,enabling operator-aware reasoning that enforces monotonic timestamps andco-constrains multiple entities under unified temporal bounds. A dynamicevidence retrieval layer adaptively selects operator-specific retrievalstrategies, while a self-evolving experience memory stores verified reasoningtraces, toolkit decisions, and sub-question embeddings for cross-type reuse.Comprehensive experiments on multiple temporal QA benchmarks show that MemoTimeachieves overall state-of-the-art results, outperforming the strong baseline byup to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) toachieve reasoning performance comparable to that of GPT-4-Turbo.</description>
      <author>example@mail.com (Xingyu Tan, Xiaoyang Wang, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang)</author>
      <guid isPermaLink="false">2510.13614v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs</title>
      <link>http://arxiv.org/abs/2510.13251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 28 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了视频大语言模型（VideoLLMs）在视频问答任务中的内部工作机制和信息流动模式。通过可解释性技术分析，研究者发现了VideoLLMs处理视频和文本信息的特定阶段和模式，并展示了如何通过选择有效信息通路来保持模型性能。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型（VideoLLMs）将视觉-语言模型的能力扩展到时空输入，使视频问答（VideoQA）等任务成为可能。尽管近期VideoLLMs取得了进展，但它们在提取和传播视频与文本信息方面的内部机制仍较少被探索。&lt;h4&gt;目的&lt;/h4&gt;研究旨在探究VideoLLMs的内部信息流动机制，特别是它们在视频问答任务中如何进行时序推理以及如何整合视频和文本信息。&lt;h4&gt;方法&lt;/h4&gt;研究者使用可解释性技术来分析VideoLLMs的内部信息流动模式。&lt;h4&gt;主要发现&lt;/h4&gt;1. 时序推理从早期到中间层开始，涉及帧间积极交互；2. 随后在中间层进行视频-语言逐步整合，这得益于视频表示与包含时间概念的词嵌入之间的对齐；3. 完成整合后，模型在中间到后期层准备生成正确答案；4. 通过选择有效信息通路并抑制大量注意力边缘（例如在LLaVA-NeXT-7B-Video-FT中为58%），VideoLLMs可以保持其视频问答性能。&lt;h4&gt;结论&lt;/h4&gt;这些发现为理解VideoLLMs如何执行时序推理提供了蓝图，并为提高模型可解释性和下游泛化能力提供了实用见解。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型（VideoLLMs）将视觉-语言模型的能力扩展到时空输入，使视频问答（VideoQA）等任务成为可能。尽管近期VideoLLMs取得了进展，但它们在提取和传播视频与文本信息方面的内部机制仍较少被探索。在本研究中，我们使用可解释性技术研究了VideoLLMs的内部信息流动。我们的分析揭示了跨不同视频问答任务的一致模式：（1）VideoLLMs中的时序推理从早期到中间层的帧间积极交互开始，（2）随后在中间层进行视频-语言逐步整合。这得益于视频表示与包含时间概念的词嵌入之间的对齐。（3）完成此整合后，模型在中间到后期层准备生成正确答案。（4）基于我们的分析，我们表明VideoLLMs可以通过选择这些有效信息通路同时抑制大量注意力边缘来保持其视频问答性能，例如在LLaVA-NeXT-7B-Video-FT中为58%。这些发现为VideoLLMs如何执行时序推理提供了蓝图，并为提高模型可解释性和下游泛化能力提供了实用见解。我们的项目页面和源代码可在https://map-the-flow.github.io获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (VideoLLMs) extend the capabilities ofvision-language models to spatiotemporal inputs, enabling tasks such as videoquestion answering (VideoQA). Despite recent advances in VideoLLMs, theirinternal mechanisms on where and how they extract and propagate video andtextual information remain less explored. In this study, we investigate theinternal information flow of VideoLLMs using mechanistic interpretabilitytechniques. Our analysis reveals consistent patterns across diverse VideoQAtasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frameinteractions in early-to-middle layers, (2) followed by progressivevideo-language integration in middle layers. This is facilitated by alignmentbetween video representations and linguistic embeddings containing temporalconcepts. (3) Upon completion of this integration, the model is ready togenerate correct answers in middle-to-late layers. (4) Based on our analysis,we show that VideoLLMs can retain their VideoQA performance by selecting theseeffective information pathways while suppressing a substantial amount ofattention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide ablueprint on how VideoLLMs perform temporal reasoning and offer practicalinsights for improving model interpretability and downstream generalization.Our project page with the source code is available athttps://map-the-flow.github.io</description>
      <author>example@mail.com (Minji Kim, Taekyung Kim, Bohyung Han)</author>
      <guid isPermaLink="false">2510.13251v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation</title>
      <link>http://arxiv.org/abs/2510.13084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Edit-Your-Interest的轻量级、文本驱动、零样本视频编辑方法，通过时空特征内存和特征传播技术解决了现有视频编辑方法计算开销大、内存消耗高和视觉保真度低的问题。&lt;h4&gt;背景&lt;/h4&gt;现有文本到图像扩散模型在视频编辑方面取得了显著进展，但现有视频编辑方法受高计算开销和内存消耗的限制，且往往牺牲视觉保真度，导致时间不一致性和模糊、马赛克状伪影等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种轻量级、文本驱动、零样本的视频编辑方法，以提高效率和视觉保真度。&lt;h4&gt;方法&lt;/h4&gt;Edit-Your-Interest方法包含三个核心技术：1)时空特征内存(SFM)缓存来自先前帧的关键图像标记；2)特征最相似传播(FMP)方法将最相关标记从前一帧传播到后续帧；3)SFM更新算法持续刷新缓存特征。此外，还利用交叉注意图自动提取感兴趣实例的掩码，并将其集成到扩散去噪过程中实现细粒度控制。&lt;h4&gt;主要发现&lt;/h4&gt;SFM显著减少了计算开销；FMP保留了时间一致性；SFM更新算法确保了特征的长期相关性和有效性；掩码集成方法实现了对目标对象的高度准确编辑，同时保持背景完整性。&lt;h4&gt;结论&lt;/h4&gt;Edit-Your-Interest在效率和视觉保真度上都优于现有最先进方法，验证了其优越的有效性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;文本到图像(T2I)扩散模型最近在视频编辑方面展示了显著进展。然而，现有的视频编辑方法受到高计算开销和内存消耗的严重限制。此外，这些方法通常牺牲视觉保真度，导致不期望的时间不一致性和伪影，如模糊和明显的马赛克状图案。我们提出了Edit-Your-Interest，一种轻量级、文本驱动、零样本的视频编辑方法。Edit-Your-Interest引入了一个时空特征内存来缓存来自先前帧的特征，与完整序列时空建模方法相比显著减少了计算开销。具体来说，我们首先引入了一个时空特征内存库(SFM)，它被设计用来高效缓存和保留由空间注意力处理的关键图像标记。其次，我们提出了特征最相似传播(FMP)方法。FMP将最相关的标记从先前帧传播到后续帧，保持时间一致性。最后，我们引入了一个SFM更新算法，它不断刷新缓存的特征，确保它们在整个视频序列中的长期相关性和有效性。此外，我们利用交叉注意图自动提取感兴趣实例的掩码。这些掩码无缝集成到扩散去噪过程中，实现对目标对象的细粒度控制，并允许Edit-Your-Interest在稳健保持背景完整性的同时执行高度准确的编辑。大量实验明确证明，所提出的Edit-Your-Interest在效率和视觉保真度上都优于最先进的方法，验证了其优越的有效性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-to-image (T2I) diffusion models have recently demonstrated significantprogress in video editing.  However, existing video editing methods are severely limited by their highcomputational overhead and memory consumption.  Furthermore, these approaches often sacrifice visual fidelity, leading toundesirable temporal inconsistencies and artifacts such as blurring andpronounced mosaic-like patterns.  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot videoediting method.  Edit-Your-Interest introduces a spatio-temporal feature memory to cachefeatures from previous frames, significantly reducing computational overheadcompared to full-sequence spatio-temporal modeling approaches.  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),which is designed to efficiently cache and retain the crucial image tokensprocessed by spatial attention.  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMPpropagates the most relevant tokens from previous frames to subsequent ones,preserving temporal consistency.  Finally, we introduce an SFM update algorithm that continuously refreshes thecached features, ensuring their long-term relevance and effectivenessthroughout the video sequence.  Furthermore, we leverage cross-attention maps to automatically extract masksfor the instances of interest.  These masks are seamlessly integrated into the diffusion denoising process,enabling fine-grained control over target objects and allowingEdit-Your-Interest to perform highly accurate edits while robustly preservingthe background integrity.  Extensive experiments decisively demonstrate that the proposedEdit-Your-Interest outperforms state-of-the-art methods in both efficiency andvisual fidelity, validating its superior effectiveness and practicality.</description>
      <author>example@mail.com (Yi Zuo, Zitao Wang, Lingling Li, Xu Liu, Fang Liu, Licheng Jiao)</author>
      <guid isPermaLink="false">2510.13084v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding</title>
      <link>http://arxiv.org/abs/2510.13016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了时空视频动作定位(SVAG)任务，旨在解决细粒度动作理解和对象时空定位的挑战，构建了大规模基准数据集SVAG-Bench，提出了基线框架SVAGFormer和评估工具包SVAGEval，发现现有模型在复杂场景中表现不佳，需要更高级的推理能力。&lt;h4&gt;背景&lt;/h4&gt;细粒度动作理解和准确定位其对应的时间和空间位置是推进下一代AI系统的基础能力。然而，现有视频理解方法主要处理粗粒度动作识别或通用目标跟踪，忽略了根据动作联合检测和跟踪多个目标并在时间上定位它们的挑战。&lt;h4&gt;目的&lt;/h4&gt;引入时空视频动作定位(SVAG)新任务，构建支持该任务的基准数据集，提出基线框架，并开发标准化评估工具包，以促进细粒度动作理解和对象时空定位的研究。&lt;h4&gt;方法&lt;/h4&gt;构建SVAG-Bench基准测试，包含688个视频、19,590条标注记录和903个独特动词；提出SVAGFormer框架，适配最先进的视觉语言模型进行联合时空定位；开发SVAGEval标准化评估工具包以确保公平和可复现的基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;现有模型在SVAG任务上表现不佳，特别是在密集或复杂场景中，这突显了在长视频中针对细粒度对象-动作交互进行更高级推理的必要性。&lt;h4&gt;结论&lt;/h4&gt;需要开发能够同时处理细粒度动作理解、对象跟踪和时空定位的AI系统，SVAG任务和基准测试为这一研究方向提供了重要基础。&lt;h4&gt;翻译&lt;/h4&gt;理解细粒度动作并准确定位它们在空间和时间中对应的执行者是推进下一代AI系统的基础能力，包括具身智能体、自主平台和人机交互框架。尽管最近视频理解取得了进展，但现有方法主要处理粗粒度动作识别或通用目标跟踪，从而忽略了根据动作联合检测和跟踪多个目标并在时间上定位它们的挑战。为解决这一差距，我们引入时空视频动作定位(SVAG)，这是一个新任务，要求模型基于自然语言描述的动作同时检测、跟踪和时域定位视频中的所有指代对象。为支持此任务，我们构建了SVAG-Bench，这是一个大规模基准，包含688个视频、19,590条标注记录和903个独特动词，涵盖了多样化的对象、动作和现实世界场景。我们进一步提出了SVAGFormer，这是一个基线框架，适配最先进的视觉语言模型进行联合时空定位，并引入了SVAGEval，这是一个标准化评估工具包，用于公平和可复现的基准测试。实验结果表明，现有模型在SVAG上表现不佳，特别是在密集或复杂场景中，这突显了在长视频中针对细粒度对象-动作交互进行更高级推理的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding fine-grained actions and accurately localizing theircorresponding actors in space and time are fundamental capabilities foradvancing next-generation AI systems, including embodied agents, autonomousplatforms, and human-AI interaction frameworks. Despite recent progress invideo understanding, existing methods predominantly address eithercoarse-grained action recognition or generic object tracking, therebyoverlooking the challenge of jointly detecting and tracking multiple objectsaccording to their actions while grounding them temporally. To address thisgap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel taskthat requires models to simultaneously detect, track, and temporally localizeall referent objects in videos based on natural language descriptions of theiractions. To support this task, we construct SVAG-Bench, a large-scale benchmarkcomprising 688 videos, 19,590 annotated records, and 903 unique verbs, coveringa diverse range of objects, actions, and real-world scenes. We further proposeSVAGFormer, a baseline framework that adapts state of the art vision languagemodels for joint spatial and temporal grounding, and introduce SVAGEval, astandardized evaluation toolkit for fair and reproducible benchmarking.Empirical results show that existing models perform poorly on SVAG,particularly in dense or complex scenes, underscoring the need for moreadvanced reasoning over fine-grained object-action interactions in long videos.</description>
      <author>example@mail.com (Tanveer Hannan, Shuaicong Wu, Mark Weber, Suprosanna Shit, Jindong Gu, Rajat Koner, Aljoša Ošep, Laura Leal-Taixé, Thomas Seidl)</author>
      <guid isPermaLink="false">2510.13016v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Knee Angle Prediction Using EMG and Kinematic Data with an Attention-Based CNN-LSTM Network and Transfer Learning Across Multiple Datasets</title>
      <link>http://arxiv.org/abs/2510.13443v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于迁移学习的膝关节角度预测框架，使用轻量级注意力CNN-LSTM模型，仅需新受试者少量步态周期数据即可实现高精度预测。&lt;h4&gt;背景&lt;/h4&gt;肌电信号(EMG)广泛用于通过机器学习和深度学习预测身体关节角度，但现有方法面临实时应用性有限、测试条件不具代表性以及需要大量数据集等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种仅需少量新受试者数据即可预测膝关节角度的迁移学习框架，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;使用Georgia Tech、UCI和SMLE三个包含四个与膝关节运动相关EMG通道的数据集；开发轻量级基于注意力机制的CNN-LSTM模型，在Georgia Tech数据集上预训练后迁移到其他数据集；仅使用EMG输入，以及结合历史膝关节角度和多种传感器输入进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用EMG输入时，模型在异常受试者的一步和50步预测中NMAE分别为6.8%和13.7%；结合历史膝关节角度后，正常受试者NMAE降至3.1%和3.5%，异常受试者降至2.8%和7.5%；当使用EMG、运动学和相互作用力多种输入时，模型在一步和50步预测中NMAE分别达到1.09%和3.1%。&lt;h4&gt;结论&lt;/h4&gt;该迁移学习框架在短期和长期康复场景中均表现出稳健的性能和强大的泛化能力，仅需少量新受试者数据即可实现高精度膝关节角度预测。&lt;h4&gt;翻译&lt;/h4&gt;肌电(EMG)信号被广泛用于通过机器学习(ML)和深度学习(DL)方法预测身体关节角度。然而，这些方法通常面临实时应用性有限、测试条件不具代表性以及需要大量数据集才能实现最佳性能等挑战。本文提出了一个膝关节角度预测的迁移学习框架，只需要新受试者几个步态周期的数据。利用了三个数据集——Georgia Tech、加州大学欧文分校(UCI)和Sharif机械实验室外骨骼(SMLE)，这些数据集包含四个与膝关节运动相关的EMG通道。开发了一个轻量级的基于注意力机制的CNN-LSTM模型，在Georgia Tech数据集上进行预训练，然后转移到UCI和SMLE数据集。所提出的模型仅使用EMG输入，在异常受试者的一步和50步预测中实现了6.8%和13.7%的归一化平均绝对误差(NMAE)。结合历史膝关节角度将正常受试者的NMAE降低到3.1%和3.5%，异常受试者降低到2.8%和7.5%。当进一步适应SMLE外骨骼，使用EMG、运动学和相互作用力输入时，模型在一步和50步预测中分别实现了1.09%和3.1%的NMAE。这些结果表明模型在短期和长期康复场景中都具有稳健的性能和强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electromyography (EMG) signals are widely used for predicting body jointangles through machine learning (ML) and deep learning (DL) methods. However,these approaches often face challenges such as limited real-time applicability,non-representative test conditions, and the need for large datasets to achieveoptimal performance. This paper presents a transfer-learning framework for kneejoint angle prediction that requires only a few gait cycles from new subjects.Three datasets - Georgia Tech, the University of California Irvine (UCI), andthe Sharif Mechatronic Lab Exoskeleton (SMLE) - containing four EMG channelsrelevant to knee motion were utilized. A lightweight attention-based CNN-LSTMmodel was developed and pre-trained on the Georgia Tech dataset, thentransferred to the UCI and SMLE datasets. The proposed model achievedNormalized Mean Absolute Errors (NMAE) of 6.8 percent and 13.7 percent forone-step and 50-step predictions on abnormal subjects using EMG inputs alone.Incorporating historical knee angles reduced the NMAE to 3.1 percent and 3.5percent for normal subjects, and to 2.8 percent and 7.5 percent for abnormalsubjects. When further adapted to the SMLE exoskeleton with EMG, kinematic, andinteraction force inputs, the model achieved 1.09 percent and 3.1 percent NMAEfor one- and 50-step predictions, respectively. These results demonstraterobust performance and strong generalization for both short- and long-termrehabilitation scenarios.</description>
      <author>example@mail.com (Mojtaba Mollahossein, Gholamreza Vossoughi, Mohammad Hossein Rohban)</author>
      <guid isPermaLink="false">2510.13443v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Machine Learning-Based Ultrasonic Weld Characterization Using Hierarchical Wave Modeling and Diffusion-Driven Distribution Alignment</title>
      <link>http://arxiv.org/abs/2510.13023v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 6 page appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种端到端的机器学习工作流程，用于解决自动化超声波焊接检测中的数据有限和环境波动问题，通过结合降阶建模、扩散分布对齐和U-Net分割反演技术，实现了真实工业环境下的焊接缺陷检测。&lt;h4&gt;背景&lt;/h4&gt;自动化超声波焊接检测在无损评估领域面临两大挑战：训练数据有限（由于实验标本整理或高保真模拟的复杂性）和工业环境的环境波动性（导致实时测量数据损坏）。&lt;h4&gt;目的&lt;/h4&gt;开发一种在真实工业环境中进行声学焊接检测的端到端机器学习工作流程，克服数据整理和信号损坏问题。&lt;h4&gt;方法&lt;/h4&gt;提出的工作流程包括：1)基于Lamb波理论的降阶Helmholtz模型生成综合数据集；2)使用相对廉价的低阶解为反演模型提供训练数据，并通过迁移学习优化；3)利用引导扩散处理分布外实验LDV扫描数据，生成分布内表示供反演模型处理。&lt;h4&gt;主要发现&lt;/h4&gt;降阶模型能够生成全面的焊接异质性和裂纹缺陷数据集；迁移学习可有效利用有限的全3D弹性动力学模拟；扩散模型能够处理具有不可预测噪声分布的真实世界测量数据。&lt;h4&gt;结论&lt;/h4&gt;该集成框架为真实数据上的自动化焊接检测提供了有效的端到端解决方案，克服了传统方法在数据有限和环境波动情况下的局限性。&lt;h4&gt;翻译&lt;/h4&gt;自动化超声波焊接检测在无损评估领域仍是一个重大挑战，原因包括训练数据有限（由于整理实验标本或高保真模拟的复杂性）和许多工业环境的环境波动性（导致实时测量数据损坏）。因此，在真实（即工业）环境中进行声学焊接检测的端到端机器学习工作流程一直是一个难以实现的目标。本文通过提出包含降阶建模方案、基于扩散的分布对齐以及基于U-Net的分割和反演的工作流程，解决了数据整理和信号损坏的挑战。使用基于Lamb波理论的降阶Helmholtz模型，在变化的焊接异质性和裂纹缺陷上生成综合数据集。相对廉价的低阶解为反演模型提供了强大的训练数据集，这些模型通过使用有限的全3D弹性动力学模拟集的迁移学习阶段进行优化。为了处理具有变化且不可预测的噪声分布的分布外真实世界测量（即激光多普勒测振仪扫描），引导扩散生成OOD实验LDV扫描的分布内表示，随后由反演模型处理。这种集成框架为真实数据上的自动化焊接检测提供了端到端解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated ultrasonic weld inspection remains a significant challenge in thenondestructive evaluation (NDE) community to factors such as limited trainingdata (due to the complexity of curating experimental specimens or high-fidelitysimulations) and environmental volatility of many industrial settings(resulting in the corruption of on-the-fly measurements). Thus, an end-to-endmachine learning (ML) workflow for acoustic weld inspection in realistic (i.e.,industrial) settings has remained an elusive goal. This work addresses thechallenges of data curation and signal corruption by proposing workflowconsisting of a reduced-order modeling scheme, diffusion based distributionalignment, and U-Net-based segmentation and inversion. A reduced-orderHelmholtz model based on Lamb wave theory is used to generate a comprehensivedataset over varying weld heterogeneity and crack defects. The relativelyinexpensive low-order solutions provide a robust training dateset for inversionmodels which are refined through a transfer learning stage using a limited setof full 3D elastodynamic simulations. To handle out-of-distribution (OOD)real-world measurements with varying and unpredictable noise distributions,i.e., Laser Doppler Vibrometry scans, guided diffusion produces in-distributionrepresentations of OOD experimental LDV scans which are subsequently processedby the inversion models. This integrated framework provides an end-to-endsolution for automated weld inspection on real data.</description>
      <author>example@mail.com (Joshua R. Tempelman, Adam J. Wachtor, Eric B. Flynn)</author>
      <guid isPermaLink="false">2510.13023v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.13809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://sihuiji.github.io/PhysMaster-Page/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhysMaster是一个通过物理知识表示指导视频生成模型的框架，利用输入图像中的物理先验信息，通过强化学习和人类反馈优化物理表示，能够生成物理上更合理的视频。&lt;h4&gt;背景&lt;/h4&gt;当前视频生成模型虽能生成视觉逼真的视频，但常常不遵守物理定律，限制了其生成物理合理视频的能力，使其无法成为有效的'世界模型'。&lt;h4&gt;目的&lt;/h4&gt;提出PhysMaster模型，通过捕获物理知识作为表示来指导视频生成模型，增强其物理感知能力，使其能够生成物理上合理的视频。&lt;h4&gt;方法&lt;/h4&gt;PhysMaster基于图像到视频任务，设计PhysEncoder从输入图像编码物理信息作为额外条件；采用强化学习与人类反馈相结合的方法，使用直接偏好优化(DPO)以端到端方式优化物理表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;PhysMaster为提高PhysEncoder的物理感知能力提供了可行解决方案，在简单代理任务上证明了其能力，并展现出广泛物理场景的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;PhysMaster通过在强化学习范式中通过表示学习统一解决各种物理过程，可作为物理感知视频生成的通用即插即用解决方案，具有更广泛的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;当今的视频生成模型能够生成视觉上逼真的视频，但常常不遵守物理定律，限制了它们生成物理上合理的视频的能力，使其无法成为'世界模型'。为解决这一问题，我们提出了PhysMaster，它将物理知识捕获为一种表示，用于指导视频生成模型增强其物理感知能力。具体而言，PhysMaster基于图像到视频任务，模型需要从输入图像预测物理上合理的动态。由于输入图像提供了物理先验信息，如场景中物体的相对位置和潜在交互，我们设计了PhysEncoder从中编码物理信息作为额外条件，将物理知识注入视频生成过程。除了外观之外，模型物理性能缺乏适当的监督，这促使PhysEncoder应用强化学习与人类反馈相结合的方法进行物理表示学习，利用生成模型的反馈通过直接偏好优化(DPO)以端到端方式优化物理表示。PhysMaster为提高PhysEncoder的物理感知能力提供了可行解决方案，从而提高了视频生成的物理合理性，在简单代理任务上证明了其能力，并具有广泛物理场景的泛化能力。这意味着我们的PhysMaster通过在强化学习范式中通过表示学习统一解决各种物理过程，可以作为物理感知视频生成和更广泛应用的通用即插即用解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前视频生成模型不遵守物理规律的问题。这个问题很重要，因为物理真实性是视频生成模型能否作为'世界模型'的关键，限制了它们在模拟真实世界场景、预测物理交互等应用场景中的实用性，也阻碍了视频生成模型从内容创作者向世界模拟器的转变。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了视频生成模型在物理规律遵循方面的两个主要挑战：MSE损失关注外观拟合而非物理理解，以及生成模型难以从图像中提取物理知识。他们提出学习物理表示作为桥梁，借鉴了基于物理仿真和无仿真方法的思路，但避免了它们的局限性。同时采用了大型语言模型中的RLHF框架和DPO训练方法，设计了三阶段训练pipeline：SFT微调基础模型和PhysEncoder，然后两阶段DPO分别优化DiT模型和PhysEncoder，利用生成反馈改进物理表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习物理表示作为物理知识和视频生成之间的桥梁，通过PhysEncoder从输入图像提取物理特征作为额外条件指导视频生成。整体流程包括：1)基于DiT的扩散模型架构，结合3D VAE和T5编码器；2)PhysEncoder基于DINOv2编码器和物理头部设计；3)三阶段训练pipeline - SFT阶段同时训练DiT和PhysEncoder，DPO阶段先优化DiT模型再优化PhysEncoder；4)从'自由落体'代理任务开始，验证后扩展到一般开放世界场景；5)使用PisaBench和VIDEOPHY等评估方法验证效果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)物理表示学习作为物理知识和视频生成的桥梁；2)自上而下的优化策略，基于最终视频的物理合理性优化物理编码器；3)三阶段训练pipeline结合SFT和DPO；4)插件式物理知识注入实现通用物理属性学习；5)从特定任务到开放世界场景的泛化能力。相比之前工作，PhysMaster不依赖特定物理仿真引擎，能处理更广泛物理现象；不依赖大规模物理数据集或昂贵人工注释；专注于优化物理编码器而非整个模型；效率更高，生成5秒视频仅需26秒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PhysMaster通过学习物理表示并作为插件注入视频生成模型，利用强化学习优化物理编码器，显著提升了视频生成模型的物理合理性，使其能够从内容创作者转变为遵循物理规律的世界模拟器。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video generation models nowadays are capable of generating visually realisticvideos, but often fail to adhere to physical laws, limiting their ability togenerate physically plausible videos and serve as ''world models''. To addressthis issue, we propose PhysMaster, which captures physical knowledge as arepresentation for guiding video generation models to enhance theirphysics-awareness. Specifically, PhysMaster is based on the image-to-video taskwhere the model is expected to predict physically plausible dynamics from theinput image. Since the input image provides physical priors like relativepositions and potential interactions of objects in the scenario, we devisePhysEncoder to encode physical information from it as an extra condition toinject physical knowledge into the video generation process. The lack of propersupervision on the model's physical performance beyond mere appearancemotivates PhysEncoder to apply reinforcement learning with human feedback tophysical representation learning, which leverages feedback from generationmodels to optimize physical representations with Direct Preference Optimization(DPO) in an end-to-end manner. PhysMaster provides a feasible solution forimproving physics-awareness of PhysEncoder and thus of video generation,proving its ability on a simple proxy task and generalizability to wide-rangingphysical scenarios. This implies that our PhysMaster, which unifies solutionsfor various physical processes via representation learning in the reinforcementlearning paradigm, can act as a generic and plug-in solution for physics-awarevideo generation and broader applications.</description>
      <author>example@mail.com (Sihui Ji, Xi Chen, Xin Tao, Pengfei Wan, Hengshuang Zhao)</author>
      <guid isPermaLink="false">2510.13809v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning</title>
      <link>http://arxiv.org/abs/2510.13515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 6 figures, 11 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为UniME-V2的新型通用多模态嵌入模型，通过利用大型多模态语言模型的先进理解能力来增强表示学习。该方法通过MLLM-as-a-Judge机制评估语义对齐，生成软语义匹配分数，用于高质量困难负样本挖掘和模型优化，显著提升了模型的判别能力，并在多个检索任务上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的通用多模态嵌入模型通常采用批内负样本挖掘方法测量查询-候选对的相似性，但这些方法存在几个局限：难以捕捉候选者之间的细微语义差异，负样本缺乏多样性，以及在区分错误负样本和困难负样本方面的判别能力有限。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决现有多模态嵌入模型的局限性，提高其捕捉细微语义差异的能力，增加负样本的多样性，并增强模型对困难负样本的判别能力，从而提升通用多模态嵌入模型的整体性能。&lt;h4&gt;方法&lt;/h4&gt;研究团队提出了一种名为UniME-V2的新型通用多模态嵌入模型，主要方法包括：1) 通过全局检索构建潜在困难负样本集；2) 引入MLLM-as-a-Judge机制，利用大型多模态语言模型评估查询-候选对的语义对齐并生成软语义匹配分数；3) 将这些分数作为困难负样本挖掘的基础，减轻错误负样本影响，识别多样化高质量困难负样本；4) 将语义匹配分数作为软标签，缓解一对一映射约束；5) 通过对齐相似度矩阵与软语义匹配分数矩阵，学习候选者间的语义区别；6) 提出UniME-V2-Reranker重排序模型，通过联合成对和列表级优化方法训练。&lt;h4&gt;主要发现&lt;/h4&gt;在MMEB基准和多个检索任务上的全面实验表明，该方法在所有任务上平均达到了最先进的性能，证明了所提出方法的有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;通过利用大型多模态语言模型的先进理解能力和创新的负样本挖掘方法，UniME-V2模型显著提高了通用多模态嵌入模型的性能，特别是在捕捉细微语义差异和区分困难负样本方面，为多模态表示学习领域提供了新的思路和解决方案。&lt;h4&gt;翻译&lt;/h4&gt;通用多模态嵌入模型是各种任务的基础。现有方法通常采用批内负样本挖掘来测量查询-候选对的相似性。然而，这些方法往往难以捕捉候选者之间的细微语义差异，且负样本缺乏多样性。此外，嵌入模型在区分错误负样本和困难负样本方面的判别能力有限。在本文中，我们利用大型多模态语言模型的先进理解能力来增强表示学习，并提出了一种新颖的通用多模态嵌入模型。我们的方法首先通过全局检索构建潜在的困难负样本集。然后我们引入MLLM-as-a-Judge机制，利用大型多模态语言模型评估查询-候选对的语义对齐情况，并生成软语义匹配分数。这些分数作为困难负样本挖掘的基础，减轻了错误负样本的影响，并能够识别出多样化的高质量困难负样本。此外，语义匹配分数还被用作软标签，以缓解严格的一对一映射约束。通过将相似度矩阵与软语义匹配分数矩阵对齐，模型能够学习候选者之间的语义区别，显著提高其判别能力。为了进一步提高性能，我们提出了UniME-V2-Reranker，这是一个通过联合成对和列表级优化方法在我们挖掘的困难负样本上训练的重排序模型。我们在MMEB基准和多个检索任务上进行了全面实验，证明我们的方法在所有任务上平均达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Universal multimodal embedding models are foundational to various tasks.Existing approaches typically employ in-batch negative mining by measuring thesimilarity of query-candidate pairs. However, these methods often struggle tocapture subtle semantic differences among candidates and lack diversity innegative samples. Moreover, the embeddings exhibit limited discriminativeability in distinguishing false and hard negatives. In this paper, we leveragethe advanced understanding capabilities of MLLMs to enhance representationlearning and present a novel Universal Multimodal Embedding (UniME-V2) model.Our approach first constructs a potential hard negative set through globalretrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizesMLLMs to assess the semantic alignment of query-candidate pairs and generatesoft semantic matching scores. These scores serve as a foundation for hardnegative mining, mitigating the impact of false negatives and enabling theidentification of diverse, high-quality hard negatives. Furthermore, thesemantic matching scores are used as soft labels to mitigate the rigidone-to-one mapping constraint. By aligning the similarity matrix with the softsemantic matching score matrix, the model learns semantic distinctions amongcandidates, significantly enhancing its discriminative capacity. To furtherimprove performance, we propose UniME-V2-Reranker, a reranking model trained onour mined hard negatives through a joint pairwise and listwise optimizationapproach. We conduct comprehensive experiments on the MMEB benchmark andmultiple retrieval tasks, demonstrating that our method achievesstate-of-the-art performance on average across all tasks.</description>
      <author>example@mail.com (Tiancheng Gu, Kaicheng Yang, Kaichen Zhang, Xiang An, Ziyong Feng, Yueyi Zhang, Weidong Cai, Jiankang Deng, Lidong Bing)</author>
      <guid isPermaLink="false">2510.13515v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>DistilCLIP-EEG: Enhancing Epileptic Seizure Detection Through Multi-modal Learning and Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2510.13497v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 9 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于CLIP框架的多模态模型DistilCLIP-EEG，整合脑电图信号和文本描述进行癫痫检测，并通过知识蒸馏方法创建轻量级学生模型，在多个数据集上实现了超过97%的准确率。&lt;h4&gt;背景&lt;/h4&gt;癫痫是一种常见的神经系统疾病，特征是突然、短暂的大脑神经元过度活动，由异常放电引起。目前大多数癫痫检测的深度学习方法仅依赖单模态的脑电图信号，忽视了多模态信息的潜在优势。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的多模态模型DistilCLIP-EEG，基于CLIP框架，整合脑电图信号和文本描述，以捕捉癫痫发作的全面特征，并通过知识蒸馏方法提高效率和适应性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于CLIP框架的多模态模型DistilCLIP-EEG，整合脑电图信号和文本描述。该模型包含基于Conformer架构的脑电图编码器作为文本编码器，以及可学习BERT(BERT-LP)作为编码器内的提示学习。两者在共享的潜在空间中运行，实现有效的跨模态表示学习。同时引入知识蒸馏方法，训练好的DistilCLIP-EEG作为教师模型，指导一个更紧凑的学生模型。&lt;h4&gt;主要发现&lt;/h4&gt;在TUSZ、AUBMC和CHB-MIT数据集上，教师模型和学生模型的准确率均超过97%。在所有数据集上，F1分数持续高于0.94，证明了所提出框架的鲁棒性和可靠性。学生模型的参数数量和模型大小约为教师模型的58.1%，显著降低了模型复杂性和存储需求，同时保持高性能。&lt;h4&gt;结论&lt;/h4&gt;该模型突显了在基于脑电图的癫痫检测中的潜力，并为在资源受限环境中部署轻量级模型奠定了坚实基础。&lt;h4&gt;翻译&lt;/h4&gt;癫痫是一种常见的神经系统疾病，特征是突然、短暂的大脑神经元过度活动 episodes，由异常放电引起，可能导致一些精神障碍。目前大多数用于癫痫检测的深度学习方法仅依赖单模态的脑电图(EEG)信号，忽视了多模态信息的潜在优势。为此，我们提出了一种新颖的多模态模型 DistilCLIP-EEG，基于CLIP框架，整合了脑电图信号和文本描述，以捕捉癫痫发作的全面特征。该模型包含基于Conformer架构的脑电图编码器作为文本编码器，以及我们提出的可学习BERT(BERT-LP)作为编码器内的提示学习。两者在共享的潜在空间中运行，实现有效的跨模态表示学习。为了提高效率和适应性，我们引入了一种知识蒸馏方法，其中训练好的DistilCLIP-EEG作为教师模型，指导一个更紧凑的学生模型，以降低训练复杂度和时间。在TUSZ、AUBMC和CHB-MIT数据集上，教师模型和学生模型的准确率均超过97%。在所有数据集上，F1分数持续高于0.94，证明了所提出框架的鲁棒性和可靠性。此外，学生模型的参数数量和模型大小约为教师模型的58.1%，显著降低了模型复杂性和存储需求，同时保持高性能。这些结果突显了我们提出的模型在基于脑电图的癫痫检测中的潜力，并为在资源受限环境中部署轻量级模型奠定了坚实基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/JBHI.2025.3603022&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Epilepsy is a prevalent neurological disorder marked by sudden, briefepisodes of excessive neuronal activity caused by abnormal electricaldischarges, which may lead to some mental disorders. Most existing deeplearning methods for epilepsy detection rely solely on unimodal EEG signals,neglecting the potential benefits of multimodal information. To address this,we propose a novel multimodal model, DistilCLIP-EEG, based on the CLIPframework, which integrates both EEG signals and text descriptions to capturecomprehensive features of epileptic seizures. The model involves an EEG encoderbased on the Conformer architecture as a text encoder, the proposed LearnableBERT (BERT-LP) as prompt learning within the encoders. Both operate in a sharedlatent space for effective cross-modal representation learning. To enhanceefficiency and adaptability, we introduce a knowledge distillation method wherethe trained DistilCLIP-EEG serves as a teacher to guide a more compact studentmodel to reduce training complexity and time. On the TUSZ, AUBMC, and CHB-MITdatasets, both the teacher and student models achieved accuracy rates exceeding97%. Across all datasets, the F1-scores were consistently above 0.94,demonstrating the robustness and reliability of the proposed framework.Moreover, the student model's parameter count and model size are approximately58.1% of those of the teacher model, significantly reducing model complexityand storage requirements while maintaining high performance. These resultshighlight the potential of our proposed model for EEG-based epilepsy detectionand establish a solid foundation for deploying lightweight models inresource-constrained settings.</description>
      <author>example@mail.com (Zexin Wang, Lin Shi, Haoyu Wu, Junru Luo, Xiangzeng Kong, Jun Qi)</author>
      <guid isPermaLink="false">2510.13497v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>End-to-End Multi-Modal Diffusion Mamba</title>
      <link>http://arxiv.org/abs/2510.13253v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MDM（多模态扩散Mamba）的新型架构，通过统一的变分自编码器实现多模态处理的统一，在多个任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;当前端到端多模态模型使用不同的编码器和解码器处理输入和输出信息，这种分离阻碍了不同模态的联合表示学习。&lt;h4&gt;目的&lt;/h4&gt;为了统一多模态处理，解决现有模型中不同模态处理分离的问题。&lt;h4&gt;方法&lt;/h4&gt;MDM利用基于Mamba的多步选择扩散模型，通过统一的变分自编码器逐步生成和优化模态特定信息。&lt;h4&gt;主要发现&lt;/h4&gt;在图像生成、图像描述、视觉问答、文本理解和推理任务等领域的评估表明，MDM显著优于现有的端到端模型（如MonoFormer、LlamaGen和Chameleon等），并能与GPT-4V、Gemini Pro和Mistral等最先进模型有效竞争。&lt;h4&gt;结论&lt;/h4&gt;研究结果验证了MDM在统一多模态处理的同时保持计算效率方面的有效性，为端到端多模态架构建立了新方向。&lt;h4&gt;翻译&lt;/h4&gt;当前端到端多模态模型使用不同的编码器和解码器来处理输入和输出信息。这种分离阻碍了不同模态的联合表示学习。为了统一多模态处理，我们提出了一种名为MDM（多模态扩散Mamba）的新型架构。MDM利用基于Mamba的多步选择扩散模型，通过统一的变分自编码器逐步生成和优化模态特定信息。这种创新方法使MDM在处理高维数据时能够实现卓越的性能，特别是在同时生成高分辨率图像和扩展文本序列方面。我们在图像生成、图像描述、视觉问答、文本理解和推理任务等领域的评估表明，MDM显著优于现有的端到端模型（MonoFormer、LlamaGen和Chameleon等），并能与GPT-4V、Gemini Pro和Mistral等最先进模型有效竞争。我们的结果验证了MDM在统一多模态处理的同时保持计算效率方面的有效性，为端到端多模态架构建立了新方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current end-to-end multi-modal models utilize different encoders and decodersto process input and output information. This separation hinders the jointrepresentation learning of various modalities. To unify multi-modal processing,we propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDMutilizes a Mamba-based multi-step selection diffusion model to progressivelygenerate and refine modality-specific information through a unified variationalautoencoder for both encoding and decoding. This innovative approach allows MDMto achieve superior performance when processing high-dimensional data,particularly in generating high-resolution images and extended text sequencessimultaneously. Our evaluations in areas such as image generation, imagecaptioning, visual question answering, text comprehension, and reasoning tasksdemonstrate that MDM significantly outperforms existing end-to-end models(MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTAmodels like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM'seffectiveness in unifying multi-modal processes while maintaining computationalefficiency, establishing a new direction for end-to-end multi-modalarchitectures.</description>
      <author>example@mail.com (Chunhao Lu, Qiang Lu, Meichen Dong, Jake Luo)</author>
      <guid isPermaLink="false">2510.13253v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>A Matter of Representation: Towards Graph-Based Abstract Code Generation</title>
      <link>http://arxiv.org/abs/2510.13163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于图的抽象代码生成，提出并评估了JSON表示方法，使大型语言模型能够高精度地执行此类任务。&lt;h4&gt;背景&lt;/h4&gt;大多数大型语言模型擅长生成原始顺序代码，但很少研究基于图的抽象代码生成，这种方法在可视化编程语言和原始源代码不可用的情况下很有价值。&lt;h4&gt;目的&lt;/h4&gt;提出并评估JSON表示方法，以实现高精度的基于图的抽象代码生成，并研究不同表示方法对生成准确性的影响。&lt;h4&gt;方法&lt;/h4&gt;使用ScratchTest（基于Scratch Python重新实现的迷你基准测试）评估不同的JSON图表示方法，测试LLM在代码图空间中的表现。&lt;h4&gt;主要发现&lt;/h4&gt;大型语言模型可以在单次通过中执行基于图的抽象代码生成任务，无需依赖专门或复杂的管道，且不同表示方法会导致显著不同的准确性。&lt;h4&gt;结论&lt;/h4&gt;这项工作为基于图的抽象代码生成的表示学习奠定了基础，突显了适当表示方法的重要性。&lt;h4&gt;翻译&lt;/h4&gt;目前大多数大型语言模型擅长生成具有最小抽象和自定义结构的原始顺序代码。然而，很少有关于基于图的抽象代码生成的工作，其中重要逻辑被封装在预定义节点中，执行流程由边决定。这对于可视化编程语言，以及原始源代码对用户和LLM训练集不可用的情况相关。在这项工作中，我们提出并评估了用于图的JSON表示，以实现高精度的基于图的抽象代码生成。我们在ScratchTest上评估了这些表示，这是一个基于我们自定义的Scratch Python重新实现的迷你基准测试，用于测试LLM在代码图空间中的表现。我们的研究结果表明，LLM确实可以在单次通过中执行上述生成任务，而不依赖于专门的或复杂的管道，前提是使用正确的图表示。我们还表明，不同的表示会导致显著不同的准确性，突显了表示在此生成任务中的重要作用。总而言之，这项工作为基于图的抽象代码生成的表示学习建立了第一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most large language models (LLMs) today excel at generating raw, sequentialcode with minimal abstractions and custom structures. However, there has beenlittle work on graph-based abstract code generation, where significant logic isencapsulated in predefined nodes and execution flow is determined by edges.This is relevant for visual programming languages, and in cases where rawsource code is inaccessible to users and LLM training sets. In this work, wepropose and evaluate JSON representations for graphs to enable high accuracygraph-based abstract code generation. We evaluate these representations onScratchTest, a mini-benchmark based on our custom Python re-implementation ofScratch, which tests the LLM in code graph space. Our findings demonstrate thatLLMs can indeed perform the aforementioned generation task in a single passwithout relying on specialized or complex pipelines, given the correct graphrepresentations. We also show that different representations inducesignificantly different accuracies, highlighting the instrumental role ofrepresentations in this generation task. All in all, this work establishes thefirst steps towards representation learning for graph-based abstract codegeneration.</description>
      <author>example@mail.com (Nyx Iskandar, Hisham Bedri, Andy Tsen)</author>
      <guid isPermaLink="false">2510.13163v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>Information Shapes Koopman Representation</title>
      <link>http://arxiv.org/abs/2510.13025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过信息论视角重新思考Koopman学习，提出一种平衡表示简单性和表达性的新方法，解决了Koopman算子在深度架构中面临的子空间选择挑战。&lt;h4&gt;背景&lt;/h4&gt;Koopman算子为建模动力系统提供了强大框架，受到机器学习社区日益关注，但其无限维特性使得识别合适的有限维子空间具有挑战性，特别是在深度架构中。&lt;h4&gt;目的&lt;/h4&gt;解决Koopman学习中次优表示学习的问题，平衡潜在变量在表达性和简单性之间的权衡，克服信息瓶颈困境。&lt;h4&gt;方法&lt;/h4&gt;提出一种信息论拉格朗日公式化，明确平衡简单性和表达性的权衡；基于该公式开发新算法，促进潜在互信息（简单性）和冯·诺依曼熵（表达性）的共同优化。&lt;h4&gt;主要发现&lt;/h4&gt;潜在互信息促进简单性但过度强调可能导致潜在空间崩溃；冯·诺依曼熵维持表达性并防止崩溃，鼓励模式多样性；所提方法产生稳定且可解释的Koopman表示。&lt;h4&gt;结论&lt;/h4&gt;通过信息论视角重新审视Koopman学习，提出的新方法在多种动力系统上验证优于现有方法，实现了更好的性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;Koopman算子为建模动力系统提供了强大框架，并吸引了机器学习界的日益关注。然而，其无限维特性使得识别合适的有限维子空间具有挑战性，特别是对于深度架构。我们认为这些困难来自于次优的表示学习，其中潜在变量无法平衡表达性和简单性。这种张力与信息瓶颈(IB)困境密切相关：构建既紧凑又有预测能力的压缩表示。通过这一视角重新思考Koopman学习，我们证明潜在互信息促进简单性，但过度强调简单性可能导致潜在空间崩溃到少数主导模式。相比之下，表达性由冯·诺依曼熵维持，防止这种崩溃并鼓励模式多样性。这一见解促使我们提出一种明确平衡这种权衡的信息论拉格朗日公式化。此外，我们基于该公式提出新算法，鼓励简单性和表达性，产生稳定且可解释的Koopman表示。除了定量评估外，我们还可视化了在我们表示下学习到的流形，观察到与理论预测一致的实证结果。最后，我们在多种动力系统上验证了我们的方法，展示了与现有Koopman学习方法相比的改进性能。实现已在https://github.com/Wenxuan52/InformationKoopman公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Koopman operator provides a powerful framework for modeling dynamicalsystems and has attracted growing interest from the machine learning community.However, its infinite-dimensional nature makes identifying suitablefinite-dimensional subspaces challenging, especially for deep architectures. Weargue that these difficulties come from suboptimal representation learning,where latent variables fail to balance expressivity and simplicity. Thistension is closely related to the information bottleneck (IB) dilemma:constructing compressed representations that are both compact and predictive.Rethinking Koopman learning through this lens, we demonstrate that latentmutual information promotes simplicity, yet an overemphasis on simplicity maycause latent space to collapse onto a few dominant modes. In contrast,expressiveness is sustained by the von Neumann entropy, which prevents suchcollapse and encourages mode diversity. This insight leads us to propose aninformation-theoretic Lagrangian formulation that explicitly balances thistradeoff. Furthermore, we propose a new algorithm based on the Lagrangianformulation that encourages both simplicity and expressiveness, leading to astable and interpretable Koopman representation. Beyond quantitativeevaluations, we further visualize the learned manifolds under ourrepresentations, observing empirical results consistent with our theoreticalpredictions. Finally, we validate our approach across a diverse range ofdynamical systems, demonstrating improved performance over existing Koopmanlearning methods. The implementation is publicly available athttps://github.com/Wenxuan52/InformationKoopman.</description>
      <author>example@mail.com (Xiaoyuan Cheng, Wenxuan Yuan, Yiming Yang, Yuanzhao Zhang, Sibo Cheng, Yi He, Zhuo Sun)</author>
      <guid isPermaLink="false">2510.13025v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal XAI Framework for Trustworthy CNNs and Bias Detection in Deep Representation Learning</title>
      <link>http://arxiv.org/abs/2510.12957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新颖的多模态可解释AI框架，通过注意力增强特征融合、Grad-CAM++局部解释和Reveal-to-Revise反馈循环解决偏差检测和减轻问题，在多模态MNIST上实现了高准确率和解释保真度。&lt;h4&gt;背景&lt;/h4&gt;标准基准数据集如MNIST无法揭示潜在的偏差和多模态特征复杂性，限制了深度神经网络在高风险应用中的可信度。&lt;h4&gt;目的&lt;/h4&gt;开发一个多模态可解释AI框架，实现偏差检测和减轻，提高AI系统的透明度和可信度。&lt;h4&gt;方法&lt;/h4&gt;统一了注意力增强的特征融合、基于Grad-CAM++的局部解释以及Reveal-to-Revise反馈循环，形成一个完整的偏差检测和减轻框架。&lt;h4&gt;主要发现&lt;/h4&gt;在多模态扩展的MNIST上实现了93.2%的分类准确率、91.6%的F1分数和78.1%的解释保真度，优于单模态和不可解释的基线方法；消融研究表明可解释性与偏差感知学习的结合增强了模型的鲁棒性和人类对齐。&lt;h4&gt;结论&lt;/h4&gt;该工作弥合了性能、透明度和公平性之间的差距，为敏感领域可信AI的实际应用提供了可行途径。&lt;h4&gt;翻译&lt;/h4&gt;标准基准数据集如MNIST往往无法揭示潜在的偏差和多模态特征复杂性，限制了深度神经网络在高风险应用中的可信度。我们提出了一种新颖的多模态可解释AI(XAI)框架，统一了注意力增强的特征融合、基于Grad-CAM++的局部解释以及Reveal-to-Revise反馈循环，用于偏差检测和减轻。在多模态扩展的MNIST上评估，我们的方法实现了93.2%的分类准确率、91.6%的F1分数和78.1%的解释保真度，优于单模态和不可解释的基线。消融研究表明，将可解释性与偏差感知学习相结合可以增强鲁棒性和人类对齐。我们的工作弥合了性能、透明度和公平性之间的差距，突显了敏感领域可信AI的实际应用途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Standard benchmark datasets, such as MNIST, often fail to expose latentbiases and multimodal feature complexities, limiting the trustworthiness ofdeep neural networks in high-stakes applications. We propose a novel multimodalExplainable AI (XAI) framework that unifies attention-augmented feature fusion,Grad-CAM++-based local explanations, and a Reveal-to-Revise feedback loop forbias detection and mitigation. Evaluated on multimodal extensions of MNIST, ourapproach achieves 93.2% classification accuracy, 91.6% F1-score, and 78.1%explanation fidelity (IoU-XAI), outperforming unimodal and non-explainablebaselines. Ablation studies demonstrate that integrating interpretability withbias-aware learning enhances robustness and human alignment. Our work bridgesthe gap between performance, transparency, and fairness, highlighting apractical pathway for trustworthy AI in sensitive domains.</description>
      <author>example@mail.com (Noor Islam S. Mohammad)</author>
      <guid isPermaLink="false">2510.12957v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    <item>
      <title>FedGTEA: Federated Class-Incremental Learning with Gaussian Task Embedding and Alignment</title>
      <link>http://arxiv.org/abs/2510.12927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FedGTEA框架，用于联邦增量学习，通过高斯任务嵌入和实现对任务特定知识和模型不确定性的高效捕捉，具有可扩展性和通信效率优势。&lt;h4&gt;背景&lt;/h4&gt;联邦增量学习领域需要有效捕捉任务特定知识和模型不确定性，同时确保可扩展性和通信效率。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够捕捉任务特定知识和模型不确定性，同时保持可扩展性和通信效率的联邦学习框架。&lt;h4&gt;方法&lt;/h4&gt;客户端使用Cardinality-Agnostic Task Encoder (CATE)生成高斯分布的任务嵌入，编码任务知识并解决统计异构性；服务器端利用2-Wasserstein距离衡量任务间差距，通过Wasserstein损失强制任务间分离，同时保护任务级隐私。&lt;h4&gt;主要发现&lt;/h4&gt;在多个流行数据集上的实证评估显示，FedGTEA实现了卓越的分类性能，显著减轻了遗忘问题，持续优于现有基线方法。&lt;h4&gt;结论&lt;/h4&gt;FedGTEA框架在联邦增量学习任务中表现优异，能够有效处理任务特定知识、模型不确定性，同时保持可扩展性和通信效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种联邦增量学习的新框架，称为联邦高斯任务嵌入与对齐（FedGTEA）。FedGTEA旨在以可扩展且通信高效的方式捕捉任务特定知识和模型不确定性。在客户端，Cardinality-Agnostic Task Encoder (CATE)生成高斯分布的任务嵌入，这些嵌入编码任务知识，解决统计异构性问题，并量化数据不确定性。重要的是，CATE保持固定参数大小，无论任务数量如何，这确保了长任务序列的可扩展性。在服务器端，FedGTEA利用2-Wasserstein距离来衡量高斯嵌入之间的任务间差距。我们制定Wasserstein损失以强制实现任务间分离。这种概率性表述不仅增强了表示学习，还通过避免直接传输潜在嵌入来保护任务级隐私，符合联邦学习中的隐私约束。在流行数据集上的大量实证评估表明，FedGTEA实现了卓越的分类性能，显著减轻了遗忘问题，持续优于现有的强大基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel framework for Federated Class Incremental Learning,called Federated Gaussian Task Embedding and Alignment (FedGTEA). FedGTEA isdesigned to capture task-specific knowledge and model uncertainty in a scalableand communication-efficient manner. At the client side, theCardinality-Agnostic Task Encoder (CATE) produces Gaussian-distributed taskembeddings that encode task knowledge, address statistical heterogeneity, andquantify data uncertainty. Importantly, CATE maintains a fixed parameter sizeregardless of the number of tasks, which ensures scalability across long tasksequences. On the server side, FedGTEA utilizes the 2-Wasserstein distance tomeasure inter-task gaps between Gaussian embeddings. We formulate theWasserstein loss to enforce inter-task separation. This probabilisticformulation not only enhances representation learning but also preservestask-level privacy by avoiding the direct transmission of latent embeddings,aligning with the privacy constraints in federated learning. Extensiveempirical evaluations on popular datasets demonstrate that FedGTEA achievessuperior classification performance and significantly mitigates forgetting,consistently outperforming strong existing baselines.</description>
      <author>example@mail.com (Haolin Li, Hoda Bidkhori)</author>
      <guid isPermaLink="false">2510.12927v1</guid>
      <pubDate>Thu, 16 Oct 2025 15:21:24 +0800</pubDate>
    </item>
    </channel>
</rss>