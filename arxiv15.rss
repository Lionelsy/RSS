<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 17 Mar 2025 17:06:06 +0800</lastBuildDate>
    <item>
      <title>Pathology Image Compression with Pre-trained Autoencoders</title>
      <link>http://arxiv.org/abs/2503.11591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了利用自编码器作为学习压缩框架的方法，用于数字病理学中的高分辨率全视图图像的存储、传输和计算效率问题。&lt;h4&gt;背景&lt;/h4&gt;随着数字病理学中高分辨率全滑片影像数量的增长，传统的压缩方法如JPEG虽然能够减小文件大小，但是无法保留下游任务所需的关键细节信息。&lt;h4&gt;目的&lt;/h4&gt;设计一种高效的学习压缩框架以解决数字病理图像的存储、传输和计算效率问题，并确保关键细节不被丢失。&lt;h4&gt;方法&lt;/h4&gt;使用三种不同压缩水平的自编码器模型进行系统性评估，引入了一种针对病理学特定学习感知度量优化的微调策略。同时提出了一种基于K-means聚类的量化方法来进一步提高存储效率。&lt;h4&gt;主要发现&lt;/h4&gt;将自编码器用于病理图像能够有效减少文件大小而不影响下游任务（如分割、打补丁分类和多实例学习）的表现。通过引入一种新的量化方法，可以在保持重建质量的同时显著提升存储效率。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明所提出的基于自编码器的压缩框架在保留关键细节信息方面优于传统JPEG压缩，并适用于多种病理学下游任务。&lt;h4&gt;翻译&lt;/h4&gt;随着数字病理学中高分辨率全滑片影像的数量不断增加，该领域面临着存储、传输和计算效率的重大挑战。标准压缩技术如JPEG虽然能够减小文件大小，但是往往无法保持对下游分析至关重要的细微细节特征。本研究将为潜伏扩散模型设计的自编码器（AE）重新定位为病理学图像的一种高效学习式压缩框架。我们通过使用三个不同压缩水平的AE模型进行系统性基准测试，并采用特定于病理学的学习感知度量优化策略来评估重建能力。我们在下游任务中验证了这一方法的有效性，包括分割、打补丁分类以及多实例学习等，证明使用AE重建图像替换原始图像几乎不会影响性能表现。此外，我们还提出了一种基于K-means聚类的量化方法来提高自编码器潜变量存储效率，同时保持高质量重建效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing volume of high-resolution Whole Slide Images in digitalhistopathology poses significant storage, transmission, and computationalefficiency challenges. Standard compression methods, such as JPEG, reduce filesizes but often fail to preserve fine-grained phenotypic details critical fordownstream tasks. In this work, we repurpose autoencoders (AEs) designed forLatent Diffusion Models as an efficient learned compression framework forpathology images. We systematically benchmark three AE models with varyingcompression levels and evaluate their reconstruction ability using pathologyfoundation models. We introduce a fine-tuning strategy to further enhancereconstruction fidelity that optimizes a pathology-specific learned perceptualmetric. We validate our approach on downstream tasks, including segmentation,patch classification, and multiple instance learning, showing that replacingimages with AE-compressed reconstructions leads to minimal performancedegradation. Additionally, we propose a K-means clustering-based quantizationmethod for AE latents, improving storage efficiency while maintainingreconstruction quality. We provide the weights of the fine-tuned autoencodersathttps://huggingface.co/collections/StonyBrook-CVLab/pathology-fine-tuned-aes-67d45f223a659ff2e3402dd0.</description>
      <author>example@mail.com (Srikar Yellapragada, Alexandros Graikos, Kostas Triaridis, Zilinghan Li, Tarak Nath Nandi, Ravi K Madduri, Prateek Prasanna, Joel Saltz, Dimitris Samaras)</author>
      <guid isPermaLink="false">2503.11591v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
  <item>
      <title>SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion</title>
      <link>http://arxiv.org/abs/2503.11576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;现有的文档转换模型依赖于大型基础模型或通过多个专业化模型的手动管道集成解决方案。这些方法在处理包括商业文件、学术论文、技术报告和专利在内的多样化文档类型时，通常过于复杂且计算需求高。&lt;h4&gt;目的&lt;/h4&gt;介绍SmolDocling这一超紧凑的视觉语言模型，旨在实现端到端的文档转换，并准确捕获各种文档元素的内容、结构以及空间位置。&lt;h4&gt;方法&lt;/h4&gt;SmolDocling通过生成全新的通用标记格式（DocTags）来处理整个页面的所有要素及其完整上下文和定位信息。该模型以256M参数量的形式实现了这一点，从而大幅减少了计算需求。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，尽管SmolDocling的大小比其他视觉语言模型小了多达27倍，但其在准确再现文档特征（如代码列表、表格、等式、图表和清单）方面的表现可以与这些大型模型相媲美。此外，该团队还贡献了一系列新的公开数据集用于识别图表、表格、方程式以及代码。&lt;h4&gt;结论&lt;/h4&gt;SmolDocling展示出了卓越的性能，在多样化的文档类型中表现出强大的适应性和准确性，同时显著降低了计算需求。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了SmolDocling，这是一种面向端到端文档转换的超紧凑视觉语言模型。该模型通过生成DocTags——一种新的通用标记格式来全面处理整个页面的所有要素及其完整上下文和定位信息。不同于现有依赖大型基础模型或依赖多个专业化模型的手工管道集成解决方案的方法，SmolDocling提供了一个使用256M参数量的视觉语言模型实现文档转换的端到端方案，并准确捕获文档元素的内容、结构及空间位置。实验结果表明，在包括商业文件、学术论文和技术报告等多样化的文档类型中，SmolDocling在正确再现文档特征方面表现出色，同时其大小比其他视觉语言模型小了多达27倍。该团队还贡献了一系列新的公开数据集用于识别图表、表格、方程式以及代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce SmolDocling, an ultra-compact vision-language model targetingend-to-end document conversion. Our model comprehensively processes entirepages by generating DocTags, a new universal markup format that captures allpage elements in their full context with location. Unlike existing approachesthat rely on large foundational models, or ensemble solutions that rely onhandcrafted pipelines of multiple specialized models, SmolDocling offers anend-to-end conversion for accurately capturing content, structure and spatiallocation of document elements in a 256M parameters vision-language model.SmolDocling exhibits robust performance in correctly reproducing documentfeatures such as code listings, tables, equations, charts, lists, and moreacross a diverse range of document types including business documents, academicpapers, technical reports, patents, and forms -- significantly extending beyondthe commonly observed focus on scientific papers. Additionally, we contributenovel publicly sourced datasets for charts, tables, equations, and coderecognition. Experimental results demonstrate that SmolDocling competes withother Vision Language Models that are up to 27 times larger in size, whilereducing computational requirements substantially. The model is currentlyavailable, datasets will be publicly available soon.</description>
      <author>example@mail.com (Ahmed Nassar, Andres Marafioti, Matteo Omenetti, Maksym Lysak, Nikolaos Livathinos, Christoph Auer, Lucas Morin, Rafael Teixeira de Lima, Yusik Kim, A. Said Gurbuz, Michele Dolfi, Miquel Farré, Peter W. J. Staar)</author>
      <guid isPermaLink="false">2503.11576v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Unicorn: A Universal and Collaborative Reinforcement Learning Approach Towards Generalizable Network-Wide Traffic Signal Control</title>
      <link>http://arxiv.org/abs/2503.11488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Unicorn，一种用于高效和适应性全网自适应交通信号控制的通用协作多智能体强化学习框架。&lt;h4&gt;背景&lt;/h4&gt;自适应交通信号控制（ATSC）在减少拥堵、最大化吞吐量和改善城市快速发展的流动性方面至关重要。最近参数共享多代理强化学习的进步大大增强了复杂动态流量的大规模同质网络中可扩展性和自适应优化能力，但真实世界交通网络的异质性对实现不同交通场景中的大规模有效ATSC提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，提出了一种通用协作MARL框架Unicorn，旨在实现全网高效和灵活的自适应交通信号控制。&lt;h4&gt;方法&lt;/h4&gt;首先，研究团队设计了一种将具有不同拓扑结构的交叉路口的状态和动作映射到统一结构的方法。其次，他们提出了一个基于解码器网络的通用交通表示（UTR）模块，用于一般特征提取，并通过变分推理技术识别代表独特交叉路口拓扑和交通动态的关键潜在向量。进一步利用对比学习方法以自监督方式来细化这些潜在表示，使模型更好地区分交叉路口特有功能。此外，将邻近代理的状态-动作依赖性整合到策略优化中。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示Unicorn在各种评估指标上优于其他方法，突显了它在复杂动态交通网络中的潜力。&lt;h4&gt;结论&lt;/h4&gt;提出的通用协作MARL框架Unicorn能够有效应对异质的现实世界交通环境带来的挑战，并实现了更优的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adaptive traffic signal control (ATSC) is crucial in reducing congestion,maximizing throughput, and improving mobility in rapidly growing urban areas.Recent advancements in parameter-sharing multi-agent reinforcement learning(MARL) have greatly enhanced the scalable and adaptive optimization of complex,dynamic flows in large-scale homogeneous networks. However, the inherentheterogeneity of real-world traffic networks, with their varied intersectiontopologies and interaction dynamics, poses substantial challenges to achievingscalable and effective ATSC across different traffic scenarios. To addressthese challenges, we present Unicorn, a universal and collaborative MARLframework designed for efficient and adaptable network-wide ATSC. Specifically,we first propose a unified approach to map the states and actions ofintersections with varying topologies into a common structure based on trafficmovements. Next, we design a Universal Traffic Representation (UTR) module witha decoder-only network for general feature extraction, enhancing the model'sadaptability to diverse traffic scenarios. Additionally, we incorporate anIntersection Specifics Representation (ISR) module, designed to identify keylatent vectors that represent the unique intersection's topology and trafficdynamics through variational inference techniques. To further refine theselatent representations, we employ a contrastive learning approach in aself-supervised manner, which enables better differentiation ofintersection-specific features. Moreover, we integrate the state-actiondependencies of neighboring agents into policy optimization, which effectivelycaptures dynamic agent interactions and facilitates efficient regionalcollaboration. Our results show that Unicorn outperforms other methods acrossvarious evaluation metrics, highlighting its potential in complex, dynamictraffic networks.</description>
      <author>example@mail.com (Yifeng Zhang, Yilin Liu, Ping Gong, Peizhuo Li, Mingfeng Fan, Guillaume Sartoretti)</author>
      <guid isPermaLink="false">2503.11488v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Breaking Shallow Limits: Task-Driven Pixel Fusion for Gap-free RGBT Tracking</title>
      <link>http://arxiv.org/abs/2503.11247v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In peer review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种任务驱动的像素级融合网络TPF，该网络通过逐步学习框架揭示了RGBT跟踪中像素级融合的力量，并设计了一个轻量化的Pixel-level Fusion Adapter (PFA)来确保实时、低延迟的RGBT跟踪。&lt;h4&gt;背景&lt;/h4&gt;当前的RGBT追踪方法经常忽视了融合位置对缓解模式差距的影响，这是有效追踪的关键因素。浅层融合可以减少分布差距，但是浅层网络有限的判别能力难以区分任务相关的信息与噪声，限制了像素级融合的潜力。&lt;h4&gt;目的&lt;/h4&gt;为了突破浅层限制，作者提出了TPF网络来揭示RGBT跟踪中像素级融合的力量，并通过一种新的逐步学习框架来提高融合性能。&lt;h4&gt;方法&lt;/h4&gt;1. 设计了一个轻量化的Pixel-level Fusion Adapter (PFA)，利用Mamba的线性复杂度确保实时、低延迟的RGBT追踪。2. 使用自适应多专家蒸馏，从先进的图像融合模型中继承融合知识，建立稳健初始化。3. 采用解耦表示学习方案来实现任务相关的信息融合。4. 提出最近邻动态模板更新方案以克服初始模板和搜索帧之间的外观变化。&lt;h4&gt;主要发现&lt;/h4&gt;TPF网络通过逐步学习框架在RGBT跟踪中揭示了像素级融合的力量，并且实验表明它显著优于现有的大多数先进追踪器。&lt;h4&gt;结论&lt;/h4&gt;所提出的任务驱动的像素级融合网络TPF在网络初始化、信息融合和动态模板更新方面取得了显著进步，从而在四个公共RGBT跟踪数据集上表现优异。代码将在论文被接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;当前的RGBT追踪方法往往忽视了融合位置对缓解模式差距的影响，这是有效追踪的关键因素。我们的分析表明浅层融合可以减少分布差距。然而，浅层网络有限的判别能力难以区分任务相关的信息与噪声，限制了像素级融合的潜力。为了突破浅层限制，我们提出了一种新的任务驱动像素级融合网络TPF，通过逐步学习框架揭示RGBT跟踪中像素级融合的力量。特别是，我们设计了一个轻量化的Pixel-level Fusion Adapter (PFA)，利用Mamba的线性复杂度确保实时、低延迟的RGBT追踪。为了增强PFA的融合能力，我们的任务驱动逐步学习框架首先使用自适应多专家蒸馏从先进的图像融合模型中继承融合知识，建立稳健初始化；然后采用解耦表示学习方案实现任务相关的信息融合。此外，为了解决初始模板和搜索帧之间的外观变化问题，我们提出了一种最近邻动态模板更新方案，选择与当前搜索帧最接近且可靠的帧作为动态模板。广泛的实验表明TPF在四个公共RGBT追踪数据集上显著优于现有的大多数先进追踪器。代码将在论文被接受后发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current RGBT tracking methods often overlook the impact of fusion location onmitigating modality gap, which is key factor to effective tracking. Ouranalysis reveals that shallower fusion yields smaller distribution gap.However, the limited discriminative power of shallow networks hard todistinguish task-relevant information from noise, limiting the potential ofpixel-level fusion. To break shallow limits, we propose a novel\textbf{T}ask-driven \textbf{P}ixel-level \textbf{F}usion network, named\textbf{TPF}, which unveils the power of pixel-level fusion in RGBT trackingthrough a progressive learning framework. In particular, we design alightweight Pixel-level Fusion Adapter (PFA) that exploits Mamba's linearcomplexity to ensure real-time, low-latency RGBT tracking. To enhance thefusion capabilities of the PFA, our task-driven progressive learning frameworkfirst utilizes adaptive multi-expert distillation to inherits fusion knowledgefrom state-of-the-art image fusion models, establishing robust initialization,and then employs a decoupled representation learning scheme to achievetask-relevant information fusion. Moreover, to overcome appearance variationsbetween the initial template and search frames, we presents a nearest-neighbordynamic template updating scheme, which selects the most reliable frame closestto the current search frame as the dynamic template. Extensive experimentsdemonstrate that TPF significantly outperforms existing most of advancedtrackers on four public RGBT tracking datasets. The code will be released uponacceptance.</description>
      <author>example@mail.com (Andong Lu, Yuanzhi Guo, Wanyu Wang, Chenglong Li, Jin Tang, Bin Luo)</author>
      <guid isPermaLink="false">2503.11247v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>VGGT: Visual Geometry Grounded Transformer</title>
      <link>http://arxiv.org/abs/2503.11651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025, Project Page: https://vgg-t.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VGGT是一种前馈神经网络，可以从一个、几个或数百个视图直接推断场景的所有关键三维属性。&lt;h4&gt;背景&lt;/h4&gt;传统的3D计算机视觉模型通常被限制于和专门化为单一任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够从多个视角直接推断出完整三维信息的新型前馈神经网络VGGT，以推动3D计算机视觉技术的发展。&lt;h4&gt;方法&lt;/h4&gt;VGGT可以快速重建图像，并在多项关键3D任务中达到最佳效果。它在无需额外优化步骤的情况下仍优于其他需要后处理的方法。&lt;h4&gt;主要发现&lt;/h4&gt;该网络实现了相机参数估计、多视图深度估计、密集点云重构和三维点跟踪等多项任务的最佳性能，且使用预训练的VGGT作为特征骨干显著增强了下游任务的表现。&lt;h4&gt;结论&lt;/h4&gt;VGGT在多个3D任务中表现出色，并展示了其作为一个通用特征提取器的能力，可以应用于非刚性点追踪和前馈式新视角合成等任务。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了VGGT, 这是一个直接从一个或数百个视图推断场景所有关键三维属性（包括相机参数、点云地图、深度地图和3D点轨迹）的前馈神经网络。这种方法在3D计算机视觉领域向前迈出了一步，传统的模型通常被限制为并专门化于单一任务。它还简单且高效，在不到一秒钟的时间内重建图像，并仍然优于需要使用视觉几何优化技术进行后处理的方法。该网络在包括相机参数估计、多视图深度估计、密集点云重构和3D点跟踪在内的多个3D任务中实现了最先进的结果。我们还展示了使用预训练的VGGT作为特征骨干显著增强了下游任务的表现，如非刚性点追踪和前馈式新视角合成。代码和模型在https://github.com/facebookresearch/vggt上公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present VGGT, a feed-forward neural network that directly infers all key3D attributes of a scene, including camera parameters, point maps, depth maps,and 3D point tracks, from one, a few, or hundreds of its views. This approachis a step forward in 3D computer vision, where models have typically beenconstrained to and specialized for single tasks. It is also simple andefficient, reconstructing images in under one second, and still outperformingalternatives that require post-processing with visual geometry optimizationtechniques. The network achieves state-of-the-art results in multiple 3D tasks,including camera parameter estimation, multi-view depth estimation, dense pointcloud reconstruction, and 3D point tracking. We also show that using pretrainedVGGT as a feature backbone significantly enhances downstream tasks, such asnon-rigid point tracking and feed-forward novel view synthesis. Code and modelsare publicly available at https://github.com/facebookresearch/vggt.</description>
      <author>example@mail.com (Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, David Novotny)</author>
      <guid isPermaLink="false">2503.11651v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>HeightFormer: Learning Height Prediction in Voxel Features for Roadside Vision Centric 3D Object Detection via Transformer</title>
      <link>http://arxiv.org/abs/2503.10777v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于Transformer的高度预测框架（HeightFormer），用于改进路边视觉中心的3D物体检测。&lt;h4&gt;背景&lt;/h4&gt;近年来，路边视觉中心的3D对象检测引起了越来越多的关注。这种方法通过预测像素高度而非深度来扩大自主车辆感知范围并提高道路安全。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法因图像特征视角局限性导致难以理解3D世界中真实物体尺寸的问题，并提供一种计算效率更高的解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出了一种利用Transformer在体素特征中学习高度预测的高效框架，通过分组局部高度序列并使用注意力机制获得高度分布预测。随后重新组装这些局部高度序列以生成准确的3D特征。&lt;h4&gt;主要发现&lt;/h4&gt;该方法应用于DAIR-V2X-I和Rope3D两个大规模路边基准测试，并在实验中超越了现有最先进的方法，在路边视觉中心的3D对象检测任务上表现出色。&lt;h4&gt;结论&lt;/h4&gt;HeightFormer框架通过改进高度预测，增强了基于体素特征的计算效率，从而提高了路边视觉中心的3D物体检测性能。&lt;h4&gt;翻译&lt;/h4&gt;道路视图为中心的三维目标检测在近年来受到了越来越多的关注。它扩展了自动驾驶车辆的感知范围，并提升了道路安全性。然而，以往的方法主要集中在预测像素的高度而不是深度，尽管这在路边视觉感知中取得了显著进展，但图像特征中的视角特性（近距离大、远距离小）限制了网络理解3D世界中物体真实尺寸的能力。鸟瞰图(BEV)和体素特征与图像特征相比，更能够呈现3D世界中对象的真实分布情况。但是，BEV特征由于缺乏明确的高度信息而容易丢失细节，而体素特征则计算成本高昂。基于此洞察，本文提出了一种通过Transformer在体素特征中学习高度预测的高效框架（HeightFormer），该方法将体素特征分组为局部高度序列，并利用注意力机制来获取高度分布预测。随后，这些局部高度序列被重新组装以生成准确的3D特征。提出的这种方法应用于两个大规模的道路边基准测试DAIR-V2X-I和Rope3D中，并在广泛的实验中表现出超越现有先进方法的性能，在道路视图中心的三维物体检测任务上表现尤为出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Roadside vision centric 3D object detection has received increasing attentionin recent years. It expands the perception range of autonomous vehicles,enhances the road safety. Previous methods focused on predicting per-pixelheight rather than depth, making significant gains in roadside visualperception. While it is limited by the perspective property of near-large andfar-small on image features, making it difficult for network to understand realdimension of objects in the 3D world. BEV features and voxel features presentthe real distribution of objects in 3D world compared to the image features.However, BEV features tend to lose details due to the lack of explicit heightinformation, and voxel features are computationally expensive. Inspired by thisinsight, an efficient framework learning height prediction in voxel featuresvia transformer is proposed, dubbed HeightFormer. It groups the voxel featuresinto local height sequences, and utilize attention mechanism to obtain heightdistribution prediction. Subsequently, the local height sequences arereassembled to generate accurate 3D features. The proposed method is applied totwo large-scale roadside benchmarks, DAIR-V2X-I and Rope3D. Extensiveexperiments are performed and the HeightFormer outperforms the state-of-the-artmethods in roadside vision centric 3D object detection task.</description>
      <author>example@mail.com (Zhang Zhang, Chao Sun, Chao Yue, Da Wen, Yujie Chen, Tianze Wang, Jianghao Leng)</author>
      <guid isPermaLink="false">2503.10777v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Deepfake Detection of Face Images based on a Convolutional Neural Network</title>
      <link>http://arxiv.org/abs/2503.11389v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于卷积神经网络的模型，用于检测由机器学习算法生成的伪造图像，特别是在政治和公众人物背景下的问题。&lt;h4&gt;背景&lt;/h4&gt;近年来，假新闻特别是深度伪造（通过机器学习技术生成的真实感很强的图像或视频内容）已经成为了一个严重的问题。随着机器学习算法的发展，任何人都可以轻易地生成这样的虚假内容。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过构建一个基于卷积神经网络的模型来解决由这些伪造图像所带来的问题。&lt;h4&gt;方法&lt;/h4&gt;研究团队采用了预训练的ResNet-50模型作为基础，并在其上添加了一个用于分类单张图片为真实或伪造的全连接输出层。他们还利用了迁移学习和微调技术以改进模型性能。为了训练模型，他们收集了大量的“Diverse Face Fake Dataset”图像数据集。&lt;h4&gt;主要发现&lt;/h4&gt;最终构建的模型在识别虚假人脸图像方面取得了卓越的结果：精确率（precision）为0.98，召回率为0.96，F1-Score为0.97，以及曲线下面积（AUC）达到了0.99。&lt;h4&gt;结论&lt;/h4&gt;通过使用深度学习技术开发的模型成功地提高了检测伪造图像的能力，在实际应用中具有很高的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fake News and especially deepfakes (generated, non-real image or videocontent) have become a serious topic over the last years. With the emergence ofmachine learning algorithms it is now easier than ever before to generate suchfake content, even for private persons. This issue of generated fake images isespecially critical in the context of politics and public figures. We want toaddress this conflict by building a model based on a Convolutions NeuralNetwork in order to detect such generated and fake images showing humanportraits. As a basis, we use a pre-trained ResNet-50 model due to itseffectiveness in terms of classifying images. We then adopted the base model toour task of classifying a single image as authentic/real or fake by adding anfully connected output layer containing a single neuron indicating theauthenticity of an image. We applied fine tuning and transfer learning todevelop the model and improve its parameters. For the training process wecollected the image data set "Diverse Face Fake Dataset" containing a widerange of different image manipulation methods and also diversity in terms offaces visible on the images. With our final model we reached the followingoutstanding performance metrics: precision = 0.98, recall 0.96, F1-Score = 0.97and an area-under-curve = 0.99.</description>
      <author>example@mail.com (Lukas Kroiß, Johannes Reschke)</author>
      <guid isPermaLink="false">2503.11389v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Soups for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.11612v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, 3 tables, accepted to GrAPL 2025 (colocated with  IPDPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于梯度下降的GNN组合策略Learned Souping，该策略能够显著减少时间和内存开销，并且在多个OGB数据集和GNN架构上进行了验证。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNN）在科学计算和高性能计算（HPC）应用中表现出卓越性能。将单独训练的GNN组合在一起可以提高模型性能，但现有的组合算法通常速度慢且占用大量内存。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的GNN组合策略Learned Souping，该策略能够减少时间和内存开销，并提升模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出了基于梯度下降的Learned Souping策略，并进一步提出Partition Learned Souping变体，旨在大幅降低内存使用量。&lt;h4&gt;主要发现&lt;/h4&gt;在多个OGB数据集和GNN架构上验证了Learned Souping策略的有效性，实现了高达1.2%的准确率提升以及2.1倍的速度提升。同时，在ogbn-products数据集上Partition Learned Souping达到了76%的内存减少和24.5倍的速度提升。&lt;h4&gt;结论&lt;/h4&gt;Learned Souping及其变体在多种GNN架构中表现出良好的性能，可以在不增加推理期间计算和内存成本的情况下提高模型准确性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNN）已经在众多科学及高性能计算应用中展示了最前沿的性能。近期的研究表明，将单独训练的GNN组合成单一模型可以提升性能而不必在推断过程中增加计算和内存开销。然而现有的组合算法往往运行缓慢且占用大量内存，这限制了它们的可扩展性。我们提出了针对GNN的Learned Souping策略，这是一种基于梯度下降的组合方法，相比于现有方法能够大幅减少时间和内存开销。我们在多个Open Graph Benchmark（OGB）数据集及不同的GNN架构中进行了实验评估，该方法实现了高达1.2%准确率提升和2.1倍的速度提高。此外，我们还提出了一种新的基于分区的Learned Souping变体Partition Learned Souping，这种方法在降低内存使用量方面表现尤为突出。在ogbn-products数据集上采用GraphSAGE时，Partition Learned Souping获得了76%的内存减少以及24.5倍的速度提升，并且没有影响准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNN) have demonstrated state-of-the-art performance innumerous scientific and high-performance computing (HPC) applications. Recentwork suggests that "souping" (combining) individually trained GNNs into asingle model can improve performance without increasing compute and memorycosts during inference. However, existing souping algorithms are often slow andmemory-intensive, which limits their scalability.  We introduce Learned Souping for GNNs, a gradient-descent-based soupingstrategy that substantially reduces time and memory overhead compared toexisting methods. Our approach is evaluated across multiple Open GraphBenchmark (OGB) datasets and GNN architectures, achieving up to 1.2% accuracyimprovement and 2.1X speedup. Additionally, we propose Partition LearnedSouping, a novel partition-based variant of learned souping that significantlyreduces memory usage. On the ogbn-products dataset with GraphSAGE, partitionlearned souping achieves a 24.5X speedup and a 76% memory reduction withoutcompromising accuracy.</description>
      <author>example@mail.com (Joseph Zuber, Aishwarya Sarkar, Joseph Jennings, Ali Jannesari)</author>
      <guid isPermaLink="false">2503.11612v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>From Abstraction to Reality: DARPA's Vision for Robust Sim-to-Real Autonomy</title>
      <link>http://arxiv.org/abs/2503.11007v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了DARPA的TIAMAT计划，该计划旨在解决自主技术在动态复杂环境、目标和平台间快速且稳健转移的问题。&lt;h4&gt;背景&lt;/h4&gt;当前从模拟到现实（sim-to-real）的技术转移方法依赖于高保真度模拟，并难以实现广泛适应性，特别是在时间敏感的情况下。尽管许多方法在特定任务上表现出色，但在面对复杂的实际世界场景时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;TIAMAT计划的目标是通过使用低保真度的多种模拟环境来直接将自主技术栈转移到真实环境中，以克服现有sim-to-real转移方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;不同于现有的依靠更加复杂的方法组合（如领域随机化、领域适应、模仿学习、元学习等）减少模拟与现实之间的差距的做法，TIAMAT通过抽象地从多个共享语义的模拟环境中学到知识来实现泛化的抽象至真实世界的转移。&lt;h4&gt;主要发现&lt;/h4&gt;利用低保真度的多种模拟环境进行广泛有效的sim-to-real转移是可能的。这种方法有助于在实际世界环境中快速有效地适应新的情况。&lt;h4&gt;结论&lt;/h4&gt;TIAMAT计划通过改进自主技术栈，旨在解决将仿真行为转化为现实中的有效表现这一固有问题。&lt;h4&gt;翻译&lt;/h4&gt;DARPA TIAMAT项目致力于提高自动化技术在动态复杂环境、目标和平台之间进行快速且稳健的转移能力。当前的sim-to-real转移方法依赖于高保真度模拟，并难以实现广泛适应性，特别是在时间敏感的情况下。不同于现有的利用复杂的方法组合（如领域随机化、域适应等）来减少模拟与现实之间的差距的做法，TIAMAT通过使用低保真的多种模拟环境直接将自动化技术栈转移到真实环境中，以克服现有方法的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The DARPA Transfer from Imprecise and Abstract Models to AutonomousTechnologies (TIAMAT) program aims to address rapid and robust transfer ofautonomy technologies across dynamic and complex environments, goals, andplatforms. Existing methods for simulation-to-reality (sim-to-real) transferoften rely on high-fidelity simulations and struggle with broad adaptation,particularly in time-sensitive scenarios. Although many approaches have shownincredible performance at specific tasks, most techniques fall short when posedwith unforeseen, complex, and dynamic real-world scenarios due to the inherentlimitations of simulation. In contrast to current research that aims to bridgethe gap between simulation environments and the real world through increasinglysophisticated simulations and a combination of methods typically assuming asmall sim-to-real gap -- such as domain randomization, domain adaptation,imitation learning, meta-learning, policy distillation, and dynamicoptimization -- TIAMAT takes a different approach by instead emphasizingtransfer and adaptation of the autonomy stack directly to real-worldenvironments by utilizing a breadth of low(er)-fidelity simulations to createbroadly effective sim-to-real transfers. By abstractly learning from multiplesimulation environments in reference to their shared semantics, TIAMAT'sapproaches aim to achieve abstract-to-real transfer for effective and rapidreal-world adaptation. Furthermore, this program endeavors to improve theoverall autonomy pipeline by addressing the inherent challenges in translatingsimulated behaviors into effective real-world performance.</description>
      <author>example@mail.com (Erfaun Noorani, Zachary Serlin, Ben Price, Alvaro Velasquez)</author>
      <guid isPermaLink="false">2503.11007v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Empowering Time Series Analysis with Synthetic Data: A Survey and Outlook in the Era of Foundation Models</title>
      <link>http://arxiv.org/abs/2503.11411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列分析对于理解复杂系统的动态至关重要。近期在基础模型上的进展使得任务无关的时间序列基础模型（TSFM）和基于大型语言模型的时间序列模型（TSLLM）得以实现。&lt;h4&gt;背景&lt;/h4&gt;近年来，随着基础模型的快速发展，出现了可以进行通用学习并融合上下文信息的任务无关时间序列基础模型以及基于大型语言模型的时间序列模型。然而，这些模型的成功依赖于大规模、多样化且高质量的数据集，但这类数据集的构建面临着诸如法规限制、多样性不足、质量差和数量少等挑战。&lt;h4&gt;目的&lt;/h4&gt;合成数据作为一种可行的解决方案应运而生，旨在通过提供可扩展、无偏差且高质量的数据来应对上述挑战。本文综述了用于TSFM和TSLLM的合成数据，分析了生成策略及其在模型预训练、微调和评估中的作用，并指出了未来的研究方向。&lt;h4&gt;方法&lt;/h4&gt;该研究全面回顾了用于时间序列基础模型及大型语言模型的时间序列模型的数据合成技术，包括但不限于各种策略和技术细节等。&lt;h4&gt;主要发现&lt;/h4&gt;数据合成可以作为一种有效的替代方案来解决大规模、多样化且高质量的时序数据集缺乏的问题。它不仅能够提供可扩展和无偏差的数据来源，而且还能帮助提高基础时间序列模型及基于大型语言的时间序列模型的学习效率和性能。&lt;h4&gt;结论&lt;/h4&gt;未来的研究应关注于改进现有的合成数据生成策略，并探索新的技术和方法来进一步优化TSFM和TSLLM的训练过程以及它们在实际应用场景中的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series analysis is crucial for understanding dynamics of complexsystems. Recent advances in foundation models have led to task-agnostic TimeSeries Foundation Models (TSFMs) and Large Language Model-based Time SeriesModels (TSLLMs), enabling generalized learning and integrating contextualinformation. However, their success depends on large, diverse, and high-qualitydatasets, which are challenging to build due to regulatory, diversity, quality,and quantity constraints. Synthetic data emerge as a viable solution,addressing these challenges by offering scalable, unbiased, and high-qualityalternatives. This survey provides a comprehensive review of synthetic data forTSFMs and TSLLMs, analyzing data generation strategies, their role in modelpretraining, fine-tuning, and evaluation, and identifying future researchdirections.</description>
      <author>example@mail.com (Xu Liu, Taha Aksu, Juncheng Liu, Qingsong Wen, Yuxuan Liang, Caiming Xiong, Silvio Savarese, Doyen Sahoo, Junnan Li, Chenghao Liu)</author>
      <guid isPermaLink="false">2503.11411v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>TransiT: Transient Transformer for Non-line-of-sight Videography</title>
      <link>http://arxiv.org/abs/2503.11328v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为TransiT的新架构，用于通过非视线（NLOS）成像实现高速高质视频采集。这项工作旨在解决当前解决方案在帧率和图像质量之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;现有的非视线成像方法通常需要平衡帧速率与图像信息密度之间的关系，并且快速扫描过程会降低信号噪声比并产生不同的失真特性。&lt;h4&gt;目的&lt;/h4&gt;设计一种新的Transient Transformer架构（TransiT），以实现实时的NLOS视频恢复，同时满足高速率和高分辨率的要求。&lt;h4&gt;方法&lt;/h4&gt;提出的TransiT直接压缩输入瞬态的时间维度以提取特征，减少了计算成本，并采用了一种融合机制以及空间-时间变换器来帮助捕捉非视线瞬变视频中的特性。此外，它还应用了迁移学习，以便在合成数据与实际测量的数据之间建立联系。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验证明，TransiT能够从每点曝光时间为0.4毫秒的16x16稀疏瞬态中重建出10fps、分辨率为64x64的NLOS视频。&lt;h4&gt;结论&lt;/h4&gt;提出的TransiT架构能够在保持高速率的同时提供高质量的非视线成像结果，为自主导航、碰撞预防和灾难后搜救任务提供了强大的技术支撑。研究团队将公开代码和数据集以支持社区的研究工作。&lt;h4&gt;翻译&lt;/h4&gt;使用非视线（NLOS）成像进行高质量和高速视频录制对自主导航、碰撞避免以及灾害后的搜索和救援任务有好处。当前的解决方案需要在帧率与图像质量之间做出权衡。例如，通过减少每点扫描时间或扫描密度可以实现高帧率，但这会降低单个框架的信息密度。快速扫描过程进一步降低了信噪比，并且不同的扫描系统表现出不同的失真特性。在这项工作中，设计并应用了一种新的瞬态变换器架构TransiT，以实现在高速扫描下的实时NLOS恢复。TransiT直接压缩输入瞬态的时间维度来提取特征，减少了计算成本，并满足了高帧率的需求。它还采用了一种融合机制以及空间-时间变换器来帮助捕捉非视线瞬变视频的特性。此外，TransiT应用迁移学习以弥合合成数据与实际测量数据之间的差距。在真实实验中，TransiT成功地从每点曝光时间为0.4毫秒、分辨率为16x16的稀疏瞬态中重建出分辨率高达64x64且帧速率达到每秒10帧的NLOS视频。我们将向社区开放我们的代码和数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High quality and high speed videography using Non-Line-of-Sight (NLOS)imaging benefit autonomous navigation, collision prevention, and post-disastersearch and rescue tasks. Current solutions have to balance between the framerate and image quality. High frame rates, for example, can be achieved byreducing either per-point scanning time or scanning density, but at the cost oflowering the information density at individual frames. Fast scanning processfurther reduces the signal-to-noise ratio and different scanning systemsexhibit different distortion characteristics. In this work, we design andemploy a new Transient Transformer architecture called TransiT to achievereal-time NLOS recovery under fast scans. TransiT directly compresses thetemporal dimension of input transients to extract features, reducingcomputation costs and meeting high frame rate requirements. It further adopts afeature fusion mechanism as well as employs a spatial-temporal Transformer tohelp capture features of NLOS transient videos. Moreover, TransiT appliestransfer learning to bridge the gap between synthetic and real-measured data.In real experiments, TransiT manages to reconstruct from sparse transients of$16 \times 16$ measured at an exposure time of 0.4 ms per point to NLOS videosat a $64 \times 64$ resolution at 10 frames per second. We will make our codeand dataset available to the community.</description>
      <author>example@mail.com (Ruiqian Li, Siyuan Shen, Suan Xia, Ziheng Wang, Xingyue Peng, Chengxuan Song, Yingsheng Zhu, Tao Wu, Shiying Li, Jingyi Yu)</author>
      <guid isPermaLink="false">2503.11328v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Riemannian Geometric-based Meta Learning</title>
      <link>http://arxiv.org/abs/2503.10993v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文介绍了Stiefel-MAML，一种利用黎曼几何来优化参数的新元学习方法。&lt;h4&gt;背景&lt;/h4&gt;传统的模型无关元学习（MAML）在处理少量数据的任务时效果不佳，因为它们难以捕捉复杂的动态过程。这些方法通常是在欧几里得空间中进行的，并且没有充分利用参数之间的正交性。&lt;h4&gt;目的&lt;/h4&gt;通过将优化限制在斯蒂费尔流形上来增强模型的学习能力，在这个流形上自然地强制执行正交约束。&lt;h4&gt;方法&lt;/h4&gt;1. 提出Stiefel-MAML，该方法在一个新的几何结构（斯蒂费尔流形）中进行参数优化，该结构通过黎曼梯度计算和折回操作来提升表达能力和效率。2. 引入了一个基于内核的损失函数，该损失函数在斯蒂费尔流形上定义。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在Omniglot、Mini-ImageNet等基准数据集上的Stiefel-MAML方法明显优于传统MAML方法，尤其是在少量样本学习任务中表现更优。&lt;h4&gt;结论&lt;/h4&gt;论文展示了黎曼几何在元学习中的应用潜力，并为未来研究开辟了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning, or "learning to learn," aims to enable models to quickly adaptto new tasks with minimal data. While traditional methods like Model-AgnosticMeta-Learning (MAML) optimize parameters in Euclidean space, they oftenstruggle to capture complex learning dynamics, particularly in few-shotlearning scenarios. To address this limitation, we propose Stiefel-MAML, whichintegrates Riemannian geometry by optimizing within the Stiefel manifold, aspace that naturally enforces orthogonality constraints. By leveraging thegeometric structure of the Stiefel manifold, we improve parameterexpressiveness and enable more efficient optimization through Riemanniangradient calculations and retraction operations. We also introduce a novelkernel-based loss function defined on the Stiefel manifold, further enhancingthe model's ability to explore the parameter space. Experimental results onbenchmark datasets--including Omniglot, Mini-ImageNet, FC-100, andCUB--demonstrate that Stiefel-MAML consistently outperforms traditional MAML,achieving superior performance across various few-shot learning tasks. Ourfindings highlight the potential of Riemannian geometry to enhancemeta-learning, paving the way for future research on optimizing over differentgeometric structures.</description>
      <author>example@mail.com (JuneYoung Park, YuMi Lee, Tae-Joon Kim, Jang-Hwan Choi)</author>
      <guid isPermaLink="false">2503.10993v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>On the Identifiability of Causal Abstractions</title>
      <link>http://arxiv.org/abs/2503.10834v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 4 figures, published in AISTATS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;因果表示学习（CRL）通过学习与数据生成过程相关的结构因果模型来增强机器学习模型的鲁棒性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;研究一种利用观测空间中的对比数据对，即在随机未知干预前后收集的数据对，来识别潜在因果模型的方法，并探索这种方法的实际应用性。这项工作假设可以对任意子集的潜在变量进行干预，而非传统的单一变量干预。&lt;h4&gt;方法&lt;/h4&gt;引入了一个理论框架，该框架计算了根据可能的干预集合能够以何种程度确定一个因果模型的程度，这一抽象描述系统在更高粒度水平上的特性。&lt;h4&gt;主要发现&lt;/h4&gt;(Brehmer等人, 2022)展示了使用对比数据对识别潜在因果模型的可能性，但前提是所有潜在变量都可以单独进行干预。然而，这在许多实际系统中是一个非常严格的假设，因此本研究放宽了这一限制，考虑更现实的情景。&lt;h4&gt;结论&lt;/h4&gt;新提出的理论框架为从观测数据中推断复杂的因果关系提供了可能的方法，并且该方法适用于任意子集的潜在变量干预。&lt;h4&gt;翻译&lt;/h4&gt;Causal representation learning (CRL) enhances machine learning models'robustness and generalizability by learning structural causal models associatedwith data-generating processes. We focus on a family of CRL methods that usescontrastive data pairs in the observable space, generated before and after arandom, unknown intervention, to identify the latent causal model. (Brehmer etal., 2022) showed that this is indeed possible, given that all latent variablescan be intervened on individually. However, this is a highly restrictiveassumption in many systems. In this work, we instead assume interventions onarbitrary subsets of latent variables, which is more realistic. We introduce atheoretical framework that calculates the degree to which we can identify acasual model, given a set of possible interventions, up to an abstraction thatdescribes the system at a higher level of granularity.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal representation learning (CRL) enhances machine learning models'robustness and generalizability by learning structural causal models associatedwith data-generating processes. We focus on a family of CRL methods that usescontrastive data pairs in the observable space, generated before and after arandom, unknown intervention, to identify the latent causal model. (Brehmer etal., 2022) showed that this is indeed possible, given that all latent variablescan be intervened on individually. However, this is a highly restrictiveassumption in many systems. In this work, we instead assume interventions onarbitrary subsets of latent variables, which is more realistic. We introduce atheoretical framework that calculates the degree to which we can identify acausal model, given a set of possible interventions, up to an abstraction thatdescribes the system at a higher level of granularity.</description>
      <author>example@mail.com (Xiusi Li, Sékou-Oumar Kaba, Siamak Ravanbakhsh)</author>
      <guid isPermaLink="false">2503.10834v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>LuSeg: Efficient Negative and Positive Obstacles Segmentation via Contrast-Driven Multi-Modal Feature Fusion on the Lunar</title>
      <link>http://arxiv.org/abs/2503.11409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文开发了一套月球表面模拟系统LESS以及LunarSeg数据集，用于提供RGB-D数据以支持月球障碍物分割。同时提出了一种新的两阶段分割网络LuSeg，通过对比学习使两个编码器之间的语义一致性得以增强。&lt;h4&gt;背景&lt;/h4&gt;随着月球探索任务的复杂性增加，如何实现安全且自主的探测车表面探索成为关键挑战之一。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够高效准确地进行障碍物检测和分割的技术方案，以支持更复杂的月球探索任务。&lt;h4&gt;方法&lt;/h4&gt;1. 开发了名为LESS的月球表面模拟系统。2. 提供了一个包含正负两类障碍物RGB-D数据的数据集LunarSeg。3. 设计了一种新的两阶段分割网络LuSeg，该模型通过对比学习来提高两个编码器（一个处理RGB图像信息，另一个处理深度信息）之间的语义一致性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在提出的LunarSeg数据集和额外的公共真实世界NPO道路障碍数据集上，LuSeg在正负障碍物分割方面都达到了最先进的性能，并且保持了约57Hz的高推断速度。&lt;h4&gt;结论&lt;/h4&gt;研究证明，所开发的技术可以有效支持复杂的月球探索任务。代码、LESS系统以及LunarSeg数据集已经在GitHub上开源。&lt;h4&gt;翻译&lt;/h4&gt;随着月球探测任务变得越来越复杂，确保安全和自主地进行基于漫游车的表面探索已经成为月球探测任务中的关键挑战之一。在这项工作中，我们开发了一个名为月球探索模拟系统的（LESS）及其相关LunarSeg数据集，提供了包括正负障碍物在内的RGB-D数据用于月球障碍分割。此外，我们还提出了一种新的两阶段分割网络LuSeg，通过对比学习来强制执行第一阶段中的RGB编码器和第二阶段中的深度编码器之间的语义一致性。在所提出的LunarSeg数据集以及额外的公共真实世界NPO道路障碍数据集中进行实验表明，与最新的方法相比，LuSeg能够同时实现正负障碍物分割性能的最佳化，并且保持了约57Hz的高速推断速度。我们已经发布了LESS系统的实施、LunarSeg数据集和LuSeg代码：https://github.com/nubot-nudt/LuSeg&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As lunar exploration missions grow increasingly complex, ensuring safe andautonomous rover-based surface exploration has become one of the key challengesin lunar exploration tasks. In this work, we have developed a lunar surfacesimulation system called the Lunar Exploration Simulator System (LESS) and theLunarSeg dataset, which provides RGB-D data for lunar obstacle segmentationthat includes both positive and negative obstacles. Additionally, we propose anovel two-stage segmentation network called LuSeg. Through contrastivelearning, it enforces semantic consistency between the RGB encoder from Stage Iand the depth encoder from Stage II. Experimental results on our proposedLunarSeg dataset and additional public real-world NPO road obstacle datasetdemonstrate that LuSeg achieves state-of-the-art segmentation performance forboth positive and negative obstacles while maintaining a high inference speedof approximately 57\,Hz. We have released the implementation of our LESSsystem, LunarSeg dataset, and the code of LuSegat:https://github.com/nubot-nudt/LuSeg.</description>
      <author>example@mail.com (Shuaifeng Jiao, Zhiwen Zeng, Zhuoqun Su, Xieyuanli Chen, Zongtan Zhou, Huimin Lu)</author>
      <guid isPermaLink="false">2503.11409v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree Sequencing</title>
      <link>http://arxiv.org/abs/2503.11629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025. Code: https://github.com/sail-sg/TreeMeshGPT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TreeMeshGPT是一种用于生成高质量艺术网格的自回归Transformer，该模型能够与输入点云很好地对齐。&lt;h4&gt;背景&lt;/h4&gt;传统的自回归Transformer通过预测下一个标记来进行操作，而本文提出了一个新颖的方法Autoregressive Tree Sequencing。此方法基于动态增长的树结构来获取下一个输入标记，这个树结构是根据网格中面的三角形相邻关系构建的。&lt;h4&gt;目的&lt;/h4&gt;目的是提出一种新的方法以减少训练难度并提高生成网格的质量，并且能够生成包含丰富细节的艺术化高质量网格。&lt;h4&gt;方法&lt;/h4&gt;通过Autoregressive Tree Sequencing方法将每个多边形面表示为两个标记，相比于直接对面进行标记的方法效率提高了约22%。此外，该模型还可以保证生成的网格具有良好的法线方向约束。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明TreeMeshGPT可以在细节和法线方向一致性方面显著提高网格生成质量。&lt;h4&gt;结论&lt;/h4&gt;与现有的方法相比，TreeMeshGPT能够通过增强对输入点云的条件依赖性来生成更高质量的艺术化网格。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种自回归Transformer——TreeMeshGPT，它用于从输入点云生成高质量的艺术网格。该模型提出了一个新的Autoregressive Tree Sequencing方法，在这种方法中，下一个输入标记是从根据网格内面三角形相邻关系构建的动态增长树结构中检索出来的。这使得在每次步骤时可以从上一个生成的三角形面上局部扩展出新的面，并且减少了训练难度，提高了网格质量。该模型将每个三角形表示为两个标记，实现了大约22%的压缩率，比直接对面进行标记的方法更加高效。实验结果表明TreeMeshGPT可以提高网格的质量，包括细节和法线方向的一致性方面表现更佳。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce TreeMeshGPT, an autoregressive Transformer designed to generatehigh-quality artistic meshes aligned with input point clouds. Instead of theconventional next-token prediction in autoregressive Transformer, we propose anovel Autoregressive Tree Sequencing where the next input token is retrievedfrom a dynamically growing tree structure that is built upon the triangleadjacency of faces within the mesh. Our sequencing enables the mesh to extendlocally from the last generated triangular face at each step, and thereforereduces training difficulty and improves mesh quality. Our approach representseach triangular face with two tokens, achieving a compression rate ofapproximately 22% compared to the naive face tokenization. This efficienttokenization enables our model to generate highly detailed artistic meshes withstrong point cloud conditioning, surpassing previous methods in both capacityand fidelity. Furthermore, our method generates mesh with strong normalorientation constraints, minimizing flipped normals commonly encountered inprevious methods. Our experiments show that TreeMeshGPT enhances the meshgeneration quality with refined details and normal orientation consistency.</description>
      <author>example@mail.com (Stefan Lionar, Jiabin Liang, Gim Hee Lee)</author>
      <guid isPermaLink="false">2503.11629v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>GNNs as Predictors of Agentic Workflow Performances</title>
      <link>http://arxiv.org/abs/2503.11301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于计算图的代理工作流优化方案，并利用图神经网络（GNNs）预测这些代理工作流的表现，从而避免了在评估过程中频繁调用大型语言模型（LLMs），证明了这种方法的有效性和简单性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）驱动的代理工作流程已经在处理复杂任务方面取得了显著的成功。然而，在实际应用中优化这样的工作流程由于频繁调用LLMs而变得成本高昂且效率低下。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一问题，作者提出了一种将代理工作流作为计算图来表示的方法，并倡导使用GNNs作为代理工作流性能的高效预测器，以避免在评估过程中重复调用大型语言模型。&lt;h4&gt;方法&lt;/h4&gt;为了验证该观点的有效性，作者构建了FLORA-Bench平台，这是一个统一的基准测试平台，用于评价GNNs在预测代理工作流表现上的能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验研究，结果表明：图神经网络（GNNs）作为性能预测器是简单而有效的。&lt;h4&gt;结论&lt;/h4&gt;这项工作的结论支持了GNNs的新应用场景，并为自动化代理工作流优化提供了一个新的方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型驱动的代理工作流程已在处理复杂任务方面取得了显著成功。然而，在实际应用中，由于大量调用LLMs导致这些工作流程的成本高昂且效率低下。为了填补这一空白，该立场论文将代理工作流程表示为计算图，并提倡使用GNNs作为预测此类工作流性能的有效方法，从而避免在评估过程中重复的LLM调用。为了实证支持这一点，作者构建了FLORA-Bench，这是一个用于基准测试GNNs预测能力来预测代理工作流表现的统一平台。通过广泛的实验研究，结果表明：图神经网络（GNNs）作为性能预测器是简单而有效的。这一结论为新的GNN应用提供了支持，并开辟了一条向自动化代理工作流优化的新道路。所有代码、模型和数据可在https://github.com/youngsoul0731/Flora-Bench获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Agentic workflows invoked by Large Language Models (LLMs) have achievedremarkable success in handling complex tasks. However, optimizing suchworkflows is costly and inefficient in real-world applications due to extensiveinvocations of LLMs. To fill this gap, this position paper formulates agenticworkflows as computational graphs and advocates Graph Neural Networks (GNNs) asefficient predictors of agentic workflow performances, avoiding repeated LLMinvocations for evaluation. To empirically ground this position, we constructFLORA-Bench, a unified platform for benchmarking GNNs for predicting agenticworkflow performances. With extensive experiments, we arrive at the followingconclusion: GNNs are simple yet effective predictors. This conclusion supportsnew applications of GNNs and a novel direction towards automating agenticworkflow optimization. All codes, models, and data are available athttps://github.com/youngsoul0731/Flora-Bench.</description>
      <author>example@mail.com (Yuanshuo Zhang, Yuchen Hou, Bohan Tang, Shuo Chen, Muhan Zhang, Xiaowen Dong, Siheng Chen)</author>
      <guid isPermaLink="false">2503.11301v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>PARIC: Probabilistic Attention Regularization for Language Guided Image Classification from Pre-trained Vison Language Models</title>
      <link>http://arxiv.org/abs/2503.11360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了PARIC框架，该框架采用概率方法指导视觉注意机制，通过语言规范生成参考注意力图，有效解决了传统基于确定性嵌入的方法在跨模态映射中存在的多值性和病态问题。&lt;h4&gt;背景&lt;/h4&gt;当前的语言引导注意力模型虽然增强了图像分类的可解释性和性能，但它们依赖于预训练的视觉-语言基础模型产生的确定性嵌入来生成参考注意图，这可能忽略了跨模式映射固有的多值和不确定性特点。&lt;h4&gt;目的&lt;/h4&gt;开发一个概率框架以更有效地结合文本和视觉模态，并且在处理不确定性的估计时优于传统的确定性方法。&lt;h4&gt;方法&lt;/h4&gt;PARIC框架允许预训练的视觉-语言模型生成参考注意图的概率版本，通过这种方式更好地对齐文本和视觉模式并纳入不确定性估计。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，相比于传统的方法，使用PARIC可以提高预测准确性、减少偏差、确保一致性和增强鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;PARIC框架在解决语言引导的视觉注意问题中显示出了显著的优势，并且在多个数据集上的基准测试问题上表现优越。&lt;h4&gt;翻译&lt;/h4&gt;现有的基于语言指导注意力的方法已经大幅提高了图像分类任务中的可解释性和性能；然而，依赖于从预训练的视觉-语言基础模型生成确定性嵌入来创建参考注意图，往往忽略了跨模式映射固有的多值和病态特征。为解决这些问题，我们提出了PARIC框架——一种概率方法，用于通过语言规范指导视觉注意力。与传统确定性的对应物相比，我们的方法使预训练的视觉-语言模型可以生成含有不确定性估计的参考注意图。在基准测试问题上的实验结果表明，PARIC不仅提高了预测准确性、减少了偏差并确保了预测的一致性，还增强了不同数据集中的鲁棒性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Language-guided attention frameworks have significantly enhanced bothinterpretability and performance in image classification; however, the relianceon deterministic embeddings from pre-trained vision-language foundation modelsto generate reference attention maps frequently overlooks the intrinsicmultivaluedness and ill-posed characteristics of cross-modal mappings. Toaddress these limitations, we introduce PARIC, a probabilistic framework forguiding visual attention via language specifications. Our approach enablespre-trained vision-language models to generate probabilistic referenceattention maps, which align textual and visual modalities more effectivelywhile incorporating uncertainty estimates, as compared to their deterministiccounterparts. Experiments on benchmark test problems demonstrate that PARICenhances prediction accuracy, mitigates bias, ensures consistent predictions,and improves robustness across various datasets.</description>
      <author>example@mail.com (Mayank Nautiyal, Stela Arranz Gheorghe, Kristiana Stefa, Li Ju, Ida-Maria Sintorn, Prashant Singh)</author>
      <guid isPermaLink="false">2503.11360v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Automated Tomato Maturity Estimation Using an Optimized Residual Model with Pruning and Quantization Techniques</title>
      <link>http://arxiv.org/abs/2503.10940v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于ResNet-18架构并通过迁移学习、剪枝和量化技术优化的番茄成熟度分类模型。该模型在保证高精度的同时，实现了低功耗边缘设备上的实时性能。&lt;h4&gt;背景&lt;/h4&gt;番茄成熟的准确判断对于最佳收获时间和产品质量至关重要，然而现有的方法难以同时实现高准确性与计算效率。&lt;h4&gt;目的&lt;/h4&gt;开发一种既能保持高精度又能实现实时性能的番茄成熟度分类模型，以满足资源受限农业环境的需求。&lt;h4&gt;方法&lt;/h4&gt;采用ResNet-18架构并应用迁移学习、剪枝和量化技术进行优化。然后将这些模型部署在边缘设备上测试其性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过量化的模型达到97.81%的准确率，平均每张图像分类时间为0.000975秒；经过修剪和自动调优的模型也显示了显著的改进，进一步强调优化技术的好处。&lt;h4&gt;结论&lt;/h4&gt;该研究的结果表明，在满足现代农业生产对精度和效率需求的同时寻找平衡方案具有潜在可能性，并为资源受限环境中的实际应用铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tomato maturity plays a pivotal role in optimizing harvest timing andensuring product quality, but current methods struggle to achieve high accuracyalong computational efficiency simultaneously. Existing deep learningapproaches, while accurate, are often too computationally demanding forpractical use in resource-constrained agricultural settings. In contrast,simpler techniques fail to capture the nuanced features needed for preciseclassification. This study aims to develop a computationally efficient tomatoclassification model using the ResNet-18 architecture optimized throughtransfer learning, pruning, and quantization techniques. Our objective is toaddress the dual challenge of maintaining high accuracy while enablingreal-time performance on low-power edge devices. Then, these models weredeployed on an edge device to investigate their performance for tomato maturityclassification. The quantized model achieved an accuracy of 97.81%, with anaverage classification time of 0.000975 seconds per image. The pruned andauto-tuned model also demonstrated significant improvements in deploymentmetrics, further highlighting the benefits of optimization techniques. Theseresults underscore the potential for a balanced solution that meets theaccuracy and efficiency demands of modern agricultural production, paving theway for practical, real-world deployment in resource-limited environments.</description>
      <author>example@mail.com (Muhammad Waseem, Chung-Hsuan Huang, Muhammad Muzzammil Sajjad, Laraib Haider Naqvi, Yaqoob Majeed, Tanzeel Ur Rehman, Tayyaba Nadeem)</author>
      <guid isPermaLink="false">2503.10940v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Self-supervised Contrastive Learning for Multimodal Text-Image Analysis</title>
      <link>http://arxiv.org/abs/2503.11101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要主题&lt;/h4&gt;自监督学习和对比学习在文本图像模型中的应用及其最新进展&lt;h4&gt;背景&lt;/h4&gt;自监督学习是通过从无标签数据中学习潜在模式并提取有区分性的特征来生成隐式标签的机器学习方法。对比学习引入了“正”样本和“负”样本的概念，促使同源但形式不同的样本在嵌入空间中相互靠近，不同来源的样本则彼此远离。&lt;h4&gt;目的&lt;/h4&gt;全面讨论文本图像模型中对比学习的相关术语、近期发展及应用情况&lt;h4&gt;方法&lt;/h4&gt;概述近年来文本图像模型中对比学习的方法，并按不同模型结构分类这些方法；介绍并探讨用于过程中的最新技术如前置任务对图像和文本的使用，架构设计等。&lt;h4&gt;主要发现&lt;/h4&gt;讨论自监督对比学习在基于文本图像模型中的最新应用&lt;h4&gt;结论&lt;/h4&gt;该论文综述了对比学习理论、近期发展及在文本图像领域的具体应用情况，并指出了关键技术的发展趋势&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning is a machine learning approach that generatesimplicit labels by learning underlined patterns and extracting discriminativefeatures from unlabeled data without manual labelling. Contrastive learningintroduces the concept of "positive" and "negative" samples, where positivepairs (e.g., variation of the same image/object) are brought together in theembedding space, and negative pairs (e.g., views from different images/objects)are pushed farther away. This methodology has shown significant improvements inimage understanding and image text analysis without much reliance on labeleddata. In this paper, we comprehensively discuss the terminologies, recentdevelopments and applications of contrastive learning with respect totext-image models. Specifically, we provide an overview of the approaches ofcontrastive learning in text-image models in recent years. Secondly, wecategorize the approaches based on different model structures. Thirdly, wefurther introduce and discuss the latest advances of the techniques used in theprocess such as pretext tasks for both images and text, architecturalstructures, and key trends. Lastly, we discuss the recent state-of-artapplications of self-supervised contrastive learning Text-Image based models.</description>
      <author>example@mail.com (Asifullah Khan, Laiba Asmatullah, Anza Malik, Shahzaib Khan, Hamna Asif)</author>
      <guid isPermaLink="false">2503.11101v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Cloud2BIM: An open-source automatic pipeline for efficient conversion of large-scale point clouds into IFC format</title>
      <link>http://arxiv.org/abs/2503.11498v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 18 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Cloud2BIM是一款开源软件工具，用于自动化将点云数据转换为符合IFC标准的建筑信息模型（BIM），旨在提高老旧结构重建和再利用中的效率。&lt;h4&gt;背景&lt;/h4&gt;在可持续重建和旧建筑复兴中，BIM是关键组成部分。然而，现有的建模过程依赖于激光扫描或摄影测量提供的无序点云数据的手动转换，这需要大量的人力工作。&lt;h4&gt;目的&lt;/h4&gt;开发Cloud2BIM软件工具，以自动化将点云数据转化为符合IFC标准的BIM模型的过程。&lt;h4&gt;方法&lt;/h4&gt;采用先进的分割算法（如墙面和楼板分割）、开口检测以及基于真实墙面表面的房间分区技术。与其他现有工具相比，它避免了计算量大且需要大量校准的技术，支持非正交几何形状，并提供了前所未有的处理速度。&lt;h4&gt;主要发现&lt;/h4&gt;Cloud2BIM能够以比现有最快解决方案快七倍的速度完成任务，证明其为生成准确BIM模型的有效、高效和可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;通过基准数据集的系统验证，确认了Cloud2BIM是一款易于使用、高效的工具，在整个建筑点云数据转换成IFC格式方面表现卓越，且只需要少量用户输入。&lt;h4&gt;翻译&lt;/h4&gt;构建信息建模（BIM）是老旧结构可持续重建和复兴的重要组成部分。然而，模型创建通常依赖于手动将激光扫描或摄影测量提供的无序点云数据转化为有组织的数据的过程，这是一个费力的过程。本文介绍了Cloud2BIM，这是一款开源软件工具，旨在自动化地将点云数据转换为符合工业基础类（IFC）标准的BIM模型。Cloud2BIM集成了用于墙面和楼板分割、开口检测以及基于真实墙面表面房间分区的高级算法，从而形成了一个全面且完全自动化的流程。与其他现有工具不同，它避免了计算复杂且需要大量校准的技术如RANSAC的支持，并提供了前所未有的处理速度，能够达到比最快竞争解决方案快七倍的速度。使用基准数据集进行系统的验证表明，Cloud2BIM是一个易于使用的、高效的和可扩展的解决方案，用于生成准确的BIM模型，在整个建筑点云数据转换为IFC格式方面表现卓越，只需要少量用户输入即可完成大量工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building Information Modeling (BIM) is an essential component in thesustainable reconstruction and revitalization of ageing structures. However,model creation usually relies on laborious manual transformation of theunstructured point cloud data provided by laser scans or photogrammetry. Thispaper presents Cloud2BIM, an open-source software tool designed to automate theconversion of point clouds into BIM models compliant with the IndustryFoundation Classes (IFC) standard. Cloud2BIM integrates advanced algorithms forwall and slab segmentation, opening detection, and room zoning based on realwall surfaces, resulting in a comprehensive and fully automated workflow.Unlike existing tools, it avoids computationally- and calibration-intensivetechniques such as RANSAC, supports non-orthogonal geometries, and providesunprecedented processing speed-achieving results up to seven times faster thanfastest competing solutions. Systematic validation using benchmark datasetsconfirms that Cloud2BIM is an easy-to-use, efficient, and scalable solution forgenerating accurate BIM models, capable of converting extensive point clouddatasets for entire buildings into IFC format with minimal user input.</description>
      <author>example@mail.com (Slávek Zbirovský, Václav Nežerka)</author>
      <guid isPermaLink="false">2503.11498v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Variational Bayesian Personalized Ranking</title>
      <link>http://arxiv.org/abs/2503.11067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Variational BPR，一种新的易于实现的学习目标，用于改进推荐系统的准确性和公平性。&lt;h4&gt;背景&lt;/h4&gt;推荐系统在各个领域都有广泛应用，但可用的训练数据通常是用户的行为记录（如点击、购买行为）等隐式反馈，而不是显式的偏好声明。这种类型的训练数据带来了三个主要挑战：难以建模用户的不可观察偏好；虚假正负例带来的噪声干扰了参数学习过程；热门物品偏向性导致的数据偏差。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决上述推荐系统中的关键问题，即准确预测排名的困难、由于假正例和假反例产生的噪音以及由用户行为集中的热门项目引起的偏见。&lt;h4&gt;方法&lt;/h4&gt;Variational BPR通过分解基于证据下界（ELBO）-KL框架下的成对损失，并推导其变分下限来建立一个可管理的学习目标，从而实现了近似推理。该方法引入了一种注意机制的潜在兴趣原型对比机制，以减少来自问题样本的噪音，并且隐式地整合了一个灵活的硬样本挖掘策略。&lt;h4&gt;主要发现&lt;/h4&gt;这种方法通过增强特征分布的一致性减少了流行偏见。实验结果表明Variational BPR在常用的推荐模型上效果显著。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为解决推荐系统中的挑战提供了一种有效的方式，通过优化似然函数、减少噪声以及平衡热门物品的偏向性问题来提升模型性能。&lt;h4&gt;翻译&lt;/h4&gt;推荐系统已在多个领域得到广泛应用。然而，可用的训练数据通常是隐式反馈形式（如用户点击和购买行为），而不是明确声明的偏好。这种类型的训练数据带来了三个主要挑战：难以建模用户的不可观察偏好；假正例和假反例带来的噪声影响了参数学习过程；热门物品偏向性导致的数据偏差。为解决这些问题，我们提出了一种新的易于实现的学习目标——Variational BPR。该方法在证据下界（ELBO-KL）框架下分解成对损失，并推导其变分下限以建立可管理的近似推理学习目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recommendation systems have found extensive applications across diversedomains. However, the training data available typically comprises implicitfeedback, manifested as user clicks and purchase behaviors, rather thanexplicit declarations of user preferences. This type of training data presentsthree main challenges for accurate ranking prediction: First, the unobservablenature of user preferences makes likelihood function modeling inherentlydifficult. Second, the resulting false positives (FP) and false negatives (FN)introduce noise into the learning process, disrupting parameter learning.Third, data bias arises as observed interactions tend to concentrate on a fewpopular items, exacerbating the feedback loop of popularity bias. To addressthese issues, we propose Variational BPR, a novel and easily implementablelearning objective that integrates key components for enhancing collaborativefiltering: likelihood optimization, noise reduction, and popularity debiasing.Our approach involves decomposing the pairwise loss under the ELBO-KL frameworkand deriving its variational lower bound to establish a manageable learningobjective for approximate inference. Within this bound, we introduce anattention-based latent interest prototype contrastive mechanism, replacinginstance-level contrastive learning, to effectively reduce noise fromproblematic samples. The process of deriving interest prototypes implicitlyincorporates a flexible hard sample mining strategy, capable of simultaneouslyidentifying hard positive and hard negative samples. Furthermore, wedemonstrate that this hard sample mining strategy promotes feature distributionuniformity, thereby alleviating popularity bias. Empirically, we demonstratethe effectiveness of Variational BPR on popular backbone recommendation models.The code and data are available at: https://github.com/liubin06/VariationalBPR</description>
      <author>example@mail.com (Bin Liu, Xiaohong Liu, Qin Luo, Ziqiao Shang, Jielei Chu, Lin Ma, Zhaoyu Li, Fei Teng, Guangtao Zhai, Tianrui Li)</author>
      <guid isPermaLink="false">2503.11067v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Power-Aware Scheduling for Multi-Center HPC Electricity Cost Optimization</title>
      <link>http://arxiv.org/abs/2503.11011v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为TARDIS的新型节能调度器，旨在通过时间和空间优化减少高性能计算中心的电力成本。&lt;h4&gt;背景&lt;/h4&gt;随着能源消耗问题日益突出，HPC中心的电费已成为运营成本的重要组成部分，并对其财务状况产生重大影响。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合时间与空间分配策略的新方法来降低能耗和运行成本。&lt;h4&gt;方法&lt;/h4&gt;TARDIS使用图神经网络（GNN）预测单个作业的功耗，然后根据电价波动在多个HPC设施之间智能调度工作负载。&lt;h4&gt;主要发现&lt;/h4&gt;通过基于实际HPC工作负载进行跟踪模拟实验，在时间优化场景下可降低电力成本高达18%，多站点环境中则能节省10至20%的成本，同时保持系统性能和作业吞吐量的稳定。&lt;h4&gt;结论&lt;/h4&gt;TARDIS成功地解决了现有节能调度方法中的局限性，通过精确预测功耗并与时空全面优化相结合提供了一种可扩展且成本效益高的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;该论文介绍了一种名为TARDIS的新颖电力感知型作业调度器，适用于高性能计算系统，旨在通过时间和空间上的智能调度减少电力消耗。此方法针对HPC中心日益增长的能源消费问题，其中电费占据了运营成本的重要部分，并对财务状况产生重大影响。TARDIS采用图神经网络来准确预测单个任务的能量使用情况，并基于变化的电价在多个设施之间战略化地安排作业。该系统结合了时间调度（即，将电力密集型负载转移到非高峰期）和空间调度（即将工作分配到具有不同定价方案的不同地理位置的中心）。通过基于真实HPC工作负载的追踪模拟评估TARDIS，在时间优化场景中显示节省18%的成本，而在多站点环境中则展示了从10至20%的成本节约。与此同时，与最先进的调度方法相比，TARDIS在保持系统性能和作业吞吐量的同时实现了这些成本降低。全面评价表明，通过结合准确的功耗预测与综合时空优化，TARDIS有效地克服了现有电力感知型调度方案中的局限性，并为可持续且具有成本效益的高性能计算操作提供了一种可扩展解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces TARDIS (Temporal Allocation for Resource Distributionusing Intelligent Scheduling), a novel power-aware job scheduler forHigh-Performance Computing (HPC) systems that minimizes electricity coststhrough both temporal and spatial optimization. Our approach addresses thegrowing concerns of energy consumption in HPC centers, where electricityexpenses constitute a substantial portion of operational costs and have asignificant financial impact. TARDIS employs a Graph Neural Network (GNN) toaccurately predict individual job power consumption, then uses thesepredictions to strategically schedule jobs across multiple HPC facilities basedon time-varying electricity prices. The system integrates both temporalscheduling, shifting power-intensive workloads to off-peak hours, and spatialscheduling, distributing jobs across geographically dispersed centers withdifferent pricing schemes. We evaluate TARDIS using trace-based simulationsfrom real HPC workloads, demonstrating cost reductions of up to 18% in temporaloptimization scenarios and 10 to 20% in multi-site environments compared tostate-of-the-art scheduling approaches, while maintaining comparable systemperformance and job throughput. Our comprehensive evaluation shows that TARDISeffectively addresses limitations in existing power-aware scheduling approachesby combining accurate power prediction with holistic spatial-temporaloptimization, providing a scalable solution for sustainable and cost-efficientHPC operations.</description>
      <author>example@mail.com (Abrar Hossain, Abubeker Abdurahman, Mohammad A. Islam, Kishwar Ahmed)</author>
      <guid isPermaLink="false">2503.11011v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>SpaceSeg: A High-Precision Intelligent Perception Segmentation Method for Multi-Spacecraft On-Orbit Targets</title>
      <link>http://arxiv.org/abs/2503.11133v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;随着人类对深空探索的不断推进，智能感知和高精度分割技术对于在轨多航天器目标已成为现代空间任务成功的关键因素。然而，复杂的深空环境、多样化的成像条件以及航天器形态的高度变化给传统的分割方法带来了巨大的挑战。&lt;h4&gt;背景&lt;/h4&gt;复杂的空间环境、不同类型的图像采集条件以及各种不同的航天器形态使得现有的传统分割技术难以应对这些多变的情况。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于视觉基础模型的新型空间目标分割框架SpaceSeg，以解决在轨多个航天器目标的高精度分割问题。&lt;h4&gt;方法&lt;/h4&gt;{'MSHARD': '多层次级联注意力特征融合解码器（Multi-Scale Hierarchical Attention Refinement Decoder, MSHARD），通过跨分辨率特征融合实现高层次注意力机制下的精准特征解码。', 'MS-CCA': '多航天器连通成分分析法（Multi-spacecraft Connected Component Analysis, MS-CCA）有效地解决了密集目标拓扑结构混淆的问题。', 'SDAT': '空间领域自适应变换框架（Spatial Domain Adaptation Transform framework, SDAT），通过综合增强策略消除跨域差异和抵抗空间传感器的扰动。', '损失函数': '创建了一种定制化的多航天器分割任务损失函数，显著提升了在深空环境中的分割鲁棒性。'}&lt;h4&gt;主要发现&lt;/h4&gt;构建了一个全新的多尺度在轨多航天器语义分割数据集SpaceES，涵盖了四种类型的空间背景和17个典型的航天器目标。&lt;h4&gt;结论&lt;/h4&gt;通过测试验证，在使用新提出的分割框架SpaceSeg时，达到了89.87%的mIoU和99.98%的mAcc的性能指标，显著优于现有最佳方法5.71个百分点。&lt;h4&gt;翻译&lt;/h4&gt;随着人类对深空探索的持续深入，智能感知技术和高精度的空间目标分割技术对于保证现代空间任务的成功变得至关重要。然而，复杂的深空环境、多样的成像条件以及航天器形态的高度变化给传统的分割方法带来了巨大挑战。本文提出了一个基于视觉基础模型的新框架SpaceSeg，包括了四种核心技术创新：多层次级联注意力特征融合解码器（MSHARD），通过跨分辨率特征融合实现高层次注意力机制下的精准特征解码；多航天器连通成分分析法（MS-CCA）有效地解决了密集目标拓扑结构混淆的问题；空间领域自适应变换框架（SDAT），通过综合增强策略消除跨域差异和抵抗空间传感器的扰动；定制化的多航天器分割任务损失函数，显著提升了在深空环境中的分割鲁棒性。为了支持算法验证，我们构建了首个用于多尺度、在轨多航天器语义分割的数据集SpaceES，涵盖了四种类型的空间背景和17个典型的航天器目标。经过测试，SpaceSeg达到了89.87%的mIoU和99.98%的mAcc性能指标，显著优于现有最佳方法5.71个百分点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the continuous advancement of human exploration into deep space,intelligent perception and high-precision segmentation technology for on-orbitmulti-spacecraft targets have become critical factors for ensuring the successof modern space missions. However, the complex deep space environment, diverseimaging conditions, and high variability in spacecraft morphology posesignificant challenges to traditional segmentation methods. This paper proposesSpaceSeg, an innovative vision foundation model-based segmentation frameworkwith four core technical innovations: First, the Multi-Scale HierarchicalAttention Refinement Decoder (MSHARD) achieves high-precision feature decodingthrough cross-resolution feature fusion via hierarchical attention. Second, theMulti-spacecraft Connected Component Analysis (MS-CCA) effectively resolvestopological structure confusion in dense targets. Third, the Spatial DomainAdaptation Transform framework (SDAT) eliminates cross-domain disparities andresist spatial sensor perturbations through composite enhancement strategies.Finally, a custom Multi-Spacecraft Segmentation Task Loss Function is createdto significantly improve segmentation robustness in deep space scenarios. Tosupport algorithm validation, we construct the first multi-scale on-orbitmulti-spacecraft semantic segmentation dataset SpaceES, which covers four typesof spatial backgrounds and 17 typical spacecraft targets. In testing, SpaceSegachieves state-of-the-art performance with 89.87$\%$ mIoU and 99.98$\%$ mAcc,surpassing existing best methods by 5.71 percentage points. The dataset andcode are open-sourced at https://github.com/Akibaru/SpaceSeg to providecritical technical support for next-generation space situational awarenesssystems.</description>
      <author>example@mail.com (Hao Liu, Pengyu Guo, Siyuan Yang, Zeqing Jiang, Qinglei Hu, Dongyu Li)</author>
      <guid isPermaLink="false">2503.11133v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>PolyRoof: Precision Roof Polygonization in Urban Residential Building with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.10913v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to Joint Urban Remote Sensing Event (JURSE) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了Re:PolyWorld模型，该模型结合点检测和图神经网络来重建高细节度的建筑屋顶向量数据，并通过实验改进了其在复杂城市住宅结构上的性能。&lt;h4&gt;背景&lt;/h4&gt;随着对详细建筑物屋顶数据需求的增长，自动化提取方法的发展成为必要，以克服传统方法处理建筑物几何形状变化时的低效性。&lt;h4&gt;目的&lt;/h4&gt;提高Re:PolyWorld模型在复杂城市住宅建筑结构中的表现。&lt;h4&gt;方法&lt;/h4&gt;通过加入注意力机制骨干和额外的区域分割损失来改进Re:PolyWorld。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，在点位置准确性和线距离准确性方面有显著提升，重构得分达到91.99%。&lt;h4&gt;结论&lt;/h4&gt;研究表明先进的神经网络架构在应对复杂城市住宅几何形状挑战方面具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing demand for detailed building roof data has driven the developmentof automated extraction methods to overcome the inefficiencies of traditionalapproaches, particularly in handling complex variations in building geometries.Re:PolyWorld, which integrates point detection with graph neural networks,presents a promising solution for reconstructing high-detail building roofvector data. This study enhances Re:PolyWorld's performance on complex urbanresidential structures by incorporating attention-based backbones andadditional area segmentation loss. Despite dataset limitations, our experimentsdemonstrated improvements in point position accuracy (1.33 pixels) and linedistance accuracy (14.39 pixels), along with a notable increase in thereconstruction score to 91.99%. These findings highlight the potential ofadvanced neural network architectures in addressing the challenges of complexurban residential geometries.</description>
      <author>example@mail.com (Chaikal Amrullah, Daniel Panangian, Ksenia Bittner)</author>
      <guid isPermaLink="false">2503.10913v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Quantifying Interpretability in CLIP Models with Concept Consistency</title>
      <link>http://arxiv.org/abs/2503.11103v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文研究了CLIP模型中注意力头的文本描述的一致性，并提出了一种新的可解释性度量——概念一致性得分（CCS），用于衡量CLIP模型中的个体注意力头部与特定概念一致性的程度。&lt;h4&gt;背景&lt;/h4&gt;CLIP是一个广泛使用的视觉-语言基础模型，但是对其内部工作原理知之甚少。尽管有研究提出了基于分解的可解释方法来识别CLIP中注意力头的文本描述，但这些文本标签的概念一致性对可解释性和模型性能的影响尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;通过研究不同规模、预训练数据类型和补丁大小的六种不同的CLIP类模型中的文本描述的一致性概念，填补了这一领域的空白。&lt;h4&gt;方法&lt;/h4&gt;提出了一个新型度量——概念一致性得分（CCS），用于评估个体注意力头与特定概念在CLIP模型中的一致程度。使用ChatGPT进行上下文学习，并通过LLM作为裁判来验证这些标签的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;高CCS的头部对于保持模型性能至关重要，因为修剪它们会导致性能显著下降；高CCS的头部捕获了关键概念，在域外检测、特定概念推理和视频-语言理解中发挥重要作用。&lt;h4&gt;结论&lt;/h4&gt;该研究通过软剪枝实验表明，高CCS的注意力头在CLIP类模型分析中的重要性，为更深入地理解和优化此类模型提供了一个强大的可解释性度量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; CLIP is one of the most popular foundational models and is heavily used formany vision-language tasks. However, little is known about the inner workingsof CLIP. While recent work has proposed decomposition-based interpretabilitymethods for identifying textual descriptions of attention heads in CLIP, theimplications of conceptual consistency in these text labels on interpretabilityand model performance has not been explored. To bridge this gap, we study theconceptual consistency of text descriptions for attention heads in CLIP-likemodels. We conduct extensive experiments on six different models from OpenAIand OpenCLIP which vary by size, type of pre-training data and patch size. Wepropose Concept Consistency Score (CCS), a novel interpretability metric thatmeasures how consistently individual attention heads in CLIP models align withspecific concepts. To assign concept labels to heads, we use in-contextlearning with ChatGPT, guided by a few manually-curated examples, and validatethese labels using an LLM-as-a-judge approach. Our soft-pruning experimentsreveal that high CCS heads are critical for preserving model performance, aspruning them leads to a significantly larger performance drop than pruningrandom or low CCS heads. Notably, we find that high CCS heads capture essentialconcepts and play a key role in out-of-domain detection, concept-specificreasoning, and video-language understanding. These results position CCS as apowerful interpretability metric for analyzing CLIP-like models.</description>
      <author>example@mail.com (Avinash Madasu, Vasudev Lal, Phillip Howard)</author>
      <guid isPermaLink="false">2503.11103v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>L2RSI: Cross-view LiDAR-based Place Recognition for Large-scale Urban Scenes via Remote Sensing Imagery</title>
      <link>http://arxiv.org/abs/2503.11245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该论文解决了基于LiDAR的地方识别问题，提出了一种名为L2RSI的方法，并构建了XA-L&amp;RSI数据集。&lt;h4&gt;背景&lt;/h4&gt;传统的基于LiDAR的地方识别依赖于昂贵且耗时的3D地图。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的地方识别方法以降低成本并提高大规模定位能力。&lt;h4&gt;方法&lt;/h4&gt;{'构建XA-L&amp;RSI数据集': '包含大约11万个遥感子图和1.3万个LiDAR点云子图的数据集，涵盖了城市场景。', 'L2RSI技术': '利用高分辨率的遥感图像进行跨视图LiDAR地方识别的方法。', '概率传播方法': '基于动态高斯混合模型的概率传播方法以细化位置预测。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'大规模定位能力': '通过使用可获得的高空图像作为地图代理，降低了成本并提高了大规模定位的能力。', '跨场景泛化': '有效利用时间和空间信息进行大规模检索和跨场景泛化。', '实验验证': '在XA-L&amp;RSI数据集中，在100平方公里范围内L2RSI能以30米半径准确地识别95.08%的点云子图。'}&lt;h4&gt;结论&lt;/h4&gt;该方法能够在大规模场景中实现高精度的地方定位，无需进一步微调。&lt;h4&gt;视频展示链接&lt;/h4&gt;https://shizw695.github.io/L2RSI/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We tackle the challenge of LiDAR-based place recognition, which traditionallydepends on costly and time-consuming prior 3D maps. To overcome this, we firstconstruct XA-L&amp;RSI dataset, which encompasses approximately $110,000$ remotesensing submaps and $13,000$ LiDAR point cloud submaps captured in urbanscenes, and propose a novel method, L2RSI, for cross-view LiDAR placerecognition using high-resolution Remote Sensing Imagery. This approach enableslarge-scale localization capabilities at a reduced cost by leveraging readilyavailable overhead images as map proxies. L2RSI addresses the dual challengesof cross-view and cross-modal place recognition by learning feature alignmentbetween point cloud submaps and remote sensing submaps in the semantic domain.Additionally, we introduce a novel probability propagation method based on adynamic Gaussian mixture model to refine position predictions, effectivelyleveraging temporal and spatial information. This approach enables large-scaleretrieval and cross-scene generalization without fine-tuning. Extensiveexperiments on XA-L&amp;RSI demonstrate that, within a $100km^2$ retrieval range,L2RSI accurately localizes $95.08\%$ of point cloud submaps within a $30m$radius for top-$1$ retrieved location. We provide a video to more vividlydisplay the place recognition results of L2RSI athttps://shizw695.github.io/L2RSI/.</description>
      <author>example@mail.com (Ziwei Shi, Xiaoran Zhang, Yan Xia, Yu Zang, Siqi Shen, Cheng Wang)</author>
      <guid isPermaLink="false">2503.11245v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space</title>
      <link>http://arxiv.org/abs/2503.11094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了Open3DVQA基准测试，用于评估多模态大型语言模型在开放三维空间中的空间推理能力。&lt;h4&gt;背景&lt;/h4&gt;空间推理是实体代理的一项基本功能，在多模态大语言模型领域引起了广泛关注。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的基准测试（Open3DVQA），旨在全面评估现有最佳基础模型在开放式三维空间中的空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;使用高保真城市模拟器收集了9000个VQA样本。从多个方面对几种SOTA多模态大语言模型进行了评估，包括相对和绝对的空间关系、情境推理以及以对象为中心的空间属性。&lt;h4&gt;主要发现&lt;/h4&gt;{'1': '多模态大型语言模型在回答有关相对空间关系的问题时表现更好，而对绝对空间关系的处理相对较弱', '2': '这些模型对于基于自身视角和非自身视角的情况具有相似的空间推理能力', '3': '大模型微调可以显著改善其在不同空间推理任务上的性能'}&lt;h4&gt;结论&lt;/h4&gt;开源数据收集工具以及深入分析将激发关于多模态大型语言模型空间推理能力的进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容已按要求翻译并整理为中文，以方便理解和参考。原始英文摘要可在GitHub上找到（https://github.com/WeichenZh/Open3DVQA）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial reasoning is a fundamental capability of embodied agents and hasgarnered widespread attention in the field of multimodal large language models(MLLMs). In this work, we propose a novel benchmark, Open3DVQA, tocomprehensively evaluate the spatial reasoning capacities of currentstate-of-the-art (SOTA) foundation models in open 3D space. Open3DVQA consistsof 9k VQA samples, collected using an efficient semi-automated tool in ahigh-fidelity urban simulator. We evaluate several SOTA MLLMs across variousaspects of spatial reasoning, such as relative and absolute spatialrelationships, situational reasoning, and object-centric spatial attributes.Our results reveal that: 1) MLLMs perform better at answering questionsregarding relative spatial relationships than absolute spatial relationships,2) MLLMs demonstrate similar spatial reasoning abilities for both egocentricand allocentric perspectives, and 3) Fine-tuning large models significantlyimproves their performance across different spatial reasoning tasks. We believethat our open-source data collection tools and in-depth analyses will inspirefurther research on MLLM spatial reasoning capabilities. The benchmark isavailable at https://github.com/WeichenZh/Open3DVQA.</description>
      <author>example@mail.com (Weichen Zhan, Zile Zhou, Zhiheng Zheng, Chen Gao, Jinqiang Cui, Yong Li, Xinlei Chen, Xiao-Ping Zhang)</author>
      <guid isPermaLink="false">2503.11094v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>OuroMamba: A Data-Free Quantization Framework for Vision Mamba Models</title>
      <link>http://arxiv.org/abs/2503.10959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OuroMamba是首个针对基于Mamba架构的视觉模型（VMMs）的数据无关后训练量化方法。该研究解决了生成有意义合成数据和处理动态异常值变化的问题，提出了一个两阶段框架：OuroMamba-Gen用于生成语义丰富的合成数据，OuroMamba-Quant采用混合精度量化技术并结合轻量级的动态异常值检测。&lt;h4&gt;背景&lt;/h4&gt;现有基于Mamba架构的视觉模型存在两个主要挑战：1）循环状态转换限制了长距离交互捕捉能力，并导致生成的合成数据语义较弱；2）激活值在时间步上表现出动态的变化，使得现有的静态量化技术失效。&lt;h4&gt;目的&lt;/h4&gt;提出一种解决VMMs中现有量化方法不足的新框架OuroMamba，提高模型量化效果和性能。&lt;h4&gt;方法&lt;/h4&gt;1) OuroMamba-Gen采用对比学习生成语义丰富的合成数据；2) OuroMamba-Quant使用混合精度量化技术，并在推理过程中加入轻量级动态异常值检测。具体策略是基于阈值的激活通道选择策略，该策略每一步都会更新。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明OuroMamba超越了现有的数据驱动后训练量化方法，在视觉和生成任务中均达到了最优性能。&lt;h4&gt;结论&lt;/h4&gt;通过有效的GPU内核实现提高了2.36倍的实际延迟速度。研究成果将对未来的模型量化研究提供重要的参考价值。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了OuroMamba，它是首个针对基于Mamba架构的视觉模型的数据无关后训练量化方法。我们指出了在VMM上启用DFQ的两个关键挑战：1）VMM的状态转换限制了长距离交互捕捉能力，并导致生成的合成数据语义较弱；2）激活值表现出时间步上的动态异常变化，使得现有的静态PTQ技术无效。为了解决这些问题，OuroMamba提出了一个两阶段框架：1）OuroMamba-Gen用于生成语义丰富的和有意义的合成数据，它通过对比学习在潜在状态空间中的邻居交互产生的VMM特征来进行；2）OuroMamba-Quant使用混合精度量化并结合轻量级动态异常值检测技术。具体来说，我们提出了一种基于阈值的激活通道选择策略，该策略每次时间步都会更新。广泛的实验表明，我们的数据无关方法在视觉和生成任务中均超越了现有的数据驱动PTQ方法，在各种量化设置下达到了最优性能。此外，我们实现了高效的GPU内核以实现高达2.36倍的实际延迟速度提升。代码将在不久后发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present OuroMamba, the first data-free post-training quantization (DFQ)method for vision Mamba-based models (VMMs). We identify two key challenges inenabling DFQ for VMMs, (1) VMM's recurrent state transitions restrictscapturing of long-range interactions and leads to semantically weak syntheticdata, (2) VMM activations exhibit dynamic outlier variations across time-steps,rendering existing static PTQ techniques ineffective. To address thesechallenges, OuroMamba presents a two-stage framework: (1) OuroMamba-Gen togenerate semantically rich and meaningful synthetic data. It appliescontrastive learning on patch level VMM features generated through neighborhoodinteractions in the latent state space, (2) OuroMamba-Quant to employmixed-precision quantization with lightweight dynamic outlier detection duringinference. In specific, we present a thresholding based outlier channelselection strategy for activations that gets updated every time-step. Extensiveexperiments across vision and generative tasks show that our data-freeOuroMamba surpasses existing data-driven PTQ techniques, achievingstate-of-the-art performance across diverse quantization settings.Additionally, we implement efficient GPU kernels to achieve practical latencyspeedup of up to 2.36x. Code will be released soon.</description>
      <author>example@mail.com (Akshat Ramachandran, Mingyu Lee, Huan Xu, Souvik Kundu, Tushar Krishna)</author>
      <guid isPermaLink="false">2503.10959v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Semantic Graphs for Efficient and Robust LiDAR SLAM</title>
      <link>http://arxiv.org/abs/2503.11145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种语义图增强的SLAM框架SG-SLAM，该框架结合了环境中的几何、语义和拓扑特征，以提高机器人自主移动系统中SLAM的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;精确而稳健的同时定位与地图构建（SLAM）对于自主移动系统至关重要。现有的点云语义激光雷达SLAM方法在效率和泛化能力上存在不足，这使得它们在多变的真实世界场景中的表现不佳。&lt;h4&gt;目的&lt;/h4&gt;通过提出一种新的语义图增强的SLAM框架SG-SLAM来解决现有方法的问题，并提高机器人导航任务的认知功能。&lt;h4&gt;方法&lt;/h4&gt;SG-SLAM采用双线程架构：一个用于在线里程计和重定位，另一个处理闭环、姿态图优化以及地图更新。这种方法能够实时生成全局一致性的语义图地图和点云地图。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI、MulRAN及Apollo数据集上的广泛评估显示了SG-SLAM相较于现有技术的优越性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法已经开源，可进一步推动SLAM领域的研究和发展。该方法能够显著提高环境感知和机器人导航任务的认知功能。&lt;h4&gt;翻译&lt;/h4&gt;准确且鲁棒的同时定位与地图构建（SLAM）对于自主移动系统至关重要，通常通过利用环境的几何特征来实现。引入语义信息提供了更丰富的场景表示，不仅可以增强SLAM中的定位精度，还能为下游导航和规划任务提供高级认知功能。现有的点云语义激光雷达SLAM方法往往效率低下且泛化能力不足，在各种真实世界场景中表现不佳。本文提出了一种语义图增强的SLAM框架SG-SLAM，该框架有效利用环境结构中的几何、语义和拓扑特性。语义图是该框架的核心组件，支持包括在里程计故障期间稳健重定位、准确闭环以及构建语义图地图在内的关键功能。本方法采用双线程架构：一个用于在线里程计和重定位；另一个处理闭环、姿态图优化及地图更新。这种设计使我们的方法能够实时运行，并生成全局一致的语义图地图和点云地图。我们在KITTI、MulRAN及Apollo数据集上进行了广泛评估，结果表明其相对于现有技术具有优越性。该方法已在https://github.com/nubot-nudt/SG-SLAM 开源发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and robust simultaneous localization and mapping (SLAM) is crucialfor autonomous mobile systems, typically achieved by leveraging the geometricfeatures of the environment. Incorporating semantics provides a richer scenerepresentation that not only enhances localization accuracy in SLAM but alsoenables advanced cognitive functionalities for downstream navigation andplanning tasks. Existing point-wise semantic LiDAR SLAM methods often sufferfrom poor efficiency and generalization, making them less robust in diversereal-world scenarios. In this paper, we propose a semantic graph-enhanced SLAMframework, named SG-SLAM, which effectively leverages the geometric, semantic,and topological characteristics inherent in environmental structures. Thesemantic graph serves as a fundamental component that facilitates criticalfunctionalities of SLAM, including robust relocalization during odometryfailures, accurate loop closing, and semantic graph map construction. Ourmethod employs a dual-threaded architecture, with one thread dedicated toonline odometry and relocalization, while the other handles loop closure, posegraph optimization, and map update. This design enables our method to operatein real time and generate globally consistent semantic graph maps and pointcloud maps. We extensively evaluate our method across the KITTI, MulRAN, andApollo datasets, and the results demonstrate its superiority compared tostate-of-the-art methods. Our method has been released athttps://github.com/nubot-nudt/SG-SLAM.</description>
      <author>example@mail.com (Neng Wang, Huimin Lu, Zhiqiang Zheng, Hesheng Wang, Yun-Hui Liu, Xieyuanli Chen)</author>
      <guid isPermaLink="false">2503.11145v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Contrastive Learning through Variational Analysis and Neural Network Optimization Perspectives</title>
      <link>http://arxiv.org/abs/2503.10812v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文通过数学分析探索了SimCLR在对比学习中的成功原因，揭示了单独的SimCLR损失不足以保证找到好的最小化器，并指出理解此类方法的成功需要分析神经网络训练动态。&lt;h4&gt;背景&lt;/h4&gt;SimCLR作为一种对比学习的方法已被广泛应用于监督、半监督和无监督设置中。尽管它能够揭示图像数据中存在的模式结构，但其成功的原因尚未得到充分解释。&lt;h4&gt;目的&lt;/h4&gt;通过数学分析研究SimCLR损失函数及其所学潜在分布的几何属性，以更好地理解该方法的成功原因。&lt;h4&gt;方法&lt;/h4&gt;进行了SimCLR损失函数和神经网络训练动态的理论与数值分析。&lt;h4&gt;主要发现&lt;/h4&gt;1. 单独的SimCLR损失不足以保证找到一个好的最小化器；2. 需要通过分析对比学习损失最小化的神经网络训练动态来理解此类方法的成功。&lt;h4&gt;结论&lt;/h4&gt;研究揭示了SimCLR成功背后的关键因素，并强调了解析神经网络在训练过程中的行为对于理解对比学习的重要性。&lt;h4&gt;翻译&lt;/h4&gt;SimCLR方法用于不变性视觉表示的对比学习，在监督、半监督和无监督场景中被广泛使用，由于其能够发现图像数据中存在的而未直接反映在像素表示中的模式结构。然而，它的成功原因尚未得到充分解释，因为单靠不变性无法保证这一点。本文进行了对SimCLR方法的数学分析，旨在更好地理解所学潜在分布的几何特性。我们的研究结果揭示了两点：1. 单独的SimCLR损失不足以选择一个好的最小化器——即使原始数据高度聚类，也会存在给出平凡潜在分布的最小化器；2. 为了理解像SimCLR这样的对比学习方法的成功原因，需要分析由最小化对比学习损失所诱导的神经网络训练动态。我们对一隐层神经网络进行初步分析后发现，在训练过程中相当长的一段时间内可以呈现聚类结构，即便最终收敛到一个平凡的最小化器。为了验证我们的理论见解，提供了数值结果来确认我们的预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The SimCLR method for contrastive learning of invariant visualrepresentations has become extensively used in supervised, semi-supervised, andunsupervised settings, due to its ability to uncover patterns and structures inimage data that are not directly present in the pixel representations. However,the reason for this success is not well-explained, since it is not guaranteedby invariance alone. In this paper, we conduct a mathematical analysis of theSimCLR method with the goal of better understanding the geometric properties ofthe learned latent distribution. Our findings reveal two things: (1) the SimCLRloss alone is not sufficient to select a good minimizer -- there are minimizersthat give trivial latent distributions, even when the original data is highlyclustered -- and (2) in order to understand the success of contrastive learningmethods like SimCLR, it is necessary to analyze the neural network trainingdynamics induced by minimizing a contrastive learning loss. Our preliminaryanalysis for a one-hidden layer neural network shows that clustering structurecan present itself for a substantial period of time during training, even if iteventually converges to a trivial minimizer. To substantiate our theoreticalinsights, we present numerical results that confirm our theoreticalpredictions.</description>
      <author>example@mail.com (Jeff Calder, Wonjun Lee)</author>
      <guid isPermaLink="false">2503.10812v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Dual-Stage Cross-Modal Network with Dynamic Feature Fusion for Emotional Mimicry Intensity Estimation</title>
      <link>http://arxiv.org/abs/2503.10603v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;情感模仿强度（EMI）估计对于理解人类社会行为和增强人机交互体验具有重要意义，其中关键技术挑战在于动态相关模型的构建以及多模态时序信号的鲁棒融合。&lt;h4&gt;背景&lt;/h4&gt;现有的方法在充分挖掘模态协同效应、噪声敏感性和有限精细对齐能力方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一种双阶段跨模态对准框架，旨在克服现有方法的不足，提高EMI估计的性能。&lt;h4&gt;方法&lt;/h4&gt;{'第一阶段': '基于改进的CLIP架构构建视觉-文本和音频-文本对比学习网络，并通过模态解耦预训练实现初步特征空间对齐', '第二阶段': '设计一个时间感知动态融合模块，结合时序卷积网络（TCN）和门控双向LSTM来分别捕捉面部表情的宏观演变模式和声学特征的局部动态'}&lt;h4&gt;主要发现&lt;/h4&gt;{'创新性策略': '引入一种质量导向的模态融合策略，通过可微权重分配实现在遮挡和噪声场景下进行模态补偿', '实验结果': '在Hume-Vidmimic2数据集上验证了所提方法的有效性，在六个情感维度上的平均皮尔森相关系数达到0.35，优于最佳基线40%'}&lt;h4&gt;结论&lt;/h4&gt;双阶段训练策略和动态融合机制的有效性得到了实验的进一步验证，并为开放环境下的细粒度情绪分析提供了新的技术路径&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Emotional Mimicry Intensity (EMI) estimation serves as a critical technologyfor understanding human social behavior and enhancing human-computerinteraction experiences, where the core challenge lies in dynamic correlationmodeling and robust fusion of multimodal temporal signals. To address thelimitations of existing methods in insufficient exploitation of modalsynergistic effects, noise sensitivity, and limited fine-grained alignmentcapabilities, this paper proposes a dual-stage cross-modal alignment framework.First, we construct vision-text and audio-text contrastive learning networksbased on an improved CLIP architecture, achieving preliminary alignment in thefeature space through modality-decoupled pre-training. Subsequently, we designa temporal-aware dynamic fusion module that combines Temporal ConvolutionalNetworks (TCN) and gated bidirectional LSTM to respectively capture themacro-evolution patterns of facial expressions and local dynamics of acousticfeatures. Innovatively, we introduce a quality-guided modality fusion strategythat enables modality compensation under occlusion and noisy scenarios throughdifferentiable weight allocation. Experimental results on the Hume-Vidmimic2dataset demonstrate that our method achieves an average Pearson correlationcoefficient of 0.35 across six emotion dimensions, outperforming the bestbaseline by 40\%. Ablation studies further validate the effectiveness of thedual-stage training strategy and dynamic fusion mechanism, providing a noveltechnical pathway for fine-grained emotion analysis in open environments.</description>
      <author>example@mail.com (Jun Yu, Lingsi Zhu, Yanjun Chi, Yunxiang Zhang, Yang Zheng, Yongqi Wang, Xilong Lu)</author>
      <guid isPermaLink="false">2503.10603v2</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>TacticExpert: Spatial-Temporal Graph Language Model for Basketball Tactics</title>
      <link>http://arxiv.org/abs/2503.10722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;篮球战术建模的核心挑战在于从历史数据中高效提取复杂的时空依赖关系，并准确预测各种比赛事件。现有最先进的模型主要基于图神经网络（GNN），在捕捉异构节点间的长距离、长时间和细粒度互动，以及识别互动模式方面存在困难。&lt;h4&gt;背景&lt;/h4&gt;篮球战术建模面临的主要挑战是如何从历史数据中提取复杂的时空依赖关系，并准确预测比赛中的各种事件。现有模型主要基于图神经网络（GNN），但这些模型在捕捉异构节点间的长距离、长时间和细粒度互动，以及识别互动模式方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的Spatial-Temporal Propagation Symmetry-Aware Graph Transformer架构来更好地建模篮球比赛的时空依赖关系，并引入Mixture of Tactics Experts模块以实现进攻战术的不同模型化。&lt;h4&gt;方法&lt;/h4&gt;{'新架构': '提出了一个基于时空传播对称性的图变换器，该架构能够捕捉延迟效应并提升节点表示能力。同时采用不变先验引导注意力机制。', '高效训练策略': '引入高效的对比学习策略来训练Mixture of Tactics Experts模块，并通过密集训练与稀疏推理的结合提高模型效率。', '新模型': '利用轻量级图锚定技术将大型语言模型集成进来，以在开放下游任务和零样本场景中获得鲁棒性能。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'改进效果': '相较于现有方法，我们的模型通过整合密集训练与稀疏推理，在提高模型效率方面取得了2.4倍的提升。', '应用范围广': '该模型在处理新球队或球员等零样本场景中表现良好，并且可以跨多个数据集进行预训练。'}&lt;h4&gt;结论&lt;/h4&gt;提出的TacticExpert模型为篮球比赛提供了一种垂直集成的大规模模型框架，能够在下游预测任务和开放端下游任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的英文原文已给出，无需翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The core challenge in basketball tactic modeling lies in efficientlyextracting complex spatial-temporal dependencies from historical data andaccurately predicting various in-game events. Existing state-of-the-art (SOTA)models, primarily based on graph neural networks (GNNs), encounter difficultiesin capturing long-term, long-distance, and fine-grained interactions amongheterogeneous player nodes, as well as in recognizing interaction patterns.Additionally, they exhibit limited generalization to untrained downstream tasksand zero-shot scenarios. In this work, we propose a Spatial-TemporalPropagation Symmetry-Aware Graph Transformer for fine-grained game modeling.This architecture explicitly captures delay effects in the spatial space toenhance player node representations across discrete-time slices, employingsymmetry-invariant priors to guide the attention mechanism. We also introducean efficient contrastive learning strategy to train a Mixture of TacticsExperts module, facilitating differentiated modeling of offensive tactics. Byintegrating dense training with sparse inference, we achieve a 2.4x improvementin model efficiency. Moreover, the incorporation of Lightweight Graph Groundingfor Large Language Models enables robust performance in open-ended downstreamtasks and zero-shot scenarios, including novel teams or players. The proposedmodel, TacticExpert, delineates a vertically integrated large model frameworkfor basketball, unifying pretraining across multiple datasets and downstreamprediction tasks. Fine-grained modeling modules significantly enhancespatial-temporal representations, and visualization analyzes confirm the stronginterpretability of the model.</description>
      <author>example@mail.com (Xu Lingrui, Liu Mandi, Zhang Lei)</author>
      <guid isPermaLink="false">2503.10722v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Cross-domain Graph Learning: Progress and Future Directions</title>
      <link>http://arxiv.org/abs/2503.11086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文综述了跨域图学习领域的现有工作，并提出了一种新的分类体系，将方法分为基于结构、特征和结构与特征混合的类别。&lt;h4&gt;背景&lt;/h4&gt;图形学习在挖掘和分析复杂关系方面扮演着重要角色，在如交易网络和通信网络的实际应用中被广泛应用。基础模型在计算机视觉（CV）和自然语言处理（NLP）领域展示出了强大的跨域能力，这些能力也对图数据领域非常重要。&lt;h4&gt;目的&lt;/h4&gt;综述并分析现有的跨域图学习工作，并讨论现有研究的局限性和未来研究的潜在方向。&lt;h4&gt;方法&lt;/h4&gt;提出了基于学到的跨域信息分类的新分类体系：结构、特征和结构-特征混合。然后系统地调查了这些类别的代表性方法。&lt;h4&gt;主要发现&lt;/h4&gt;该论文强调，现有的图形学习方法难以应对跨域任务，并且通过借鉴CV和NLP领域的成功经验，跨域图学习再次成为实现真正的图形基础模型的关键焦点。&lt;h4&gt;结论&lt;/h4&gt;讨论了现有研究的限制以及未来的研究方向。相关的论文将在GitHub上持续更新。&lt;h4&gt;翻译&lt;/h4&gt;图形学习在挖掘和分析涉及复杂关系的图数据中起着关键作用，并被广泛应用于交易网络和通信网络等现实世界的应用场景中。计算机视觉（CV）和自然语言处理（NLP）领域的基础模型展示了强大的跨域能力，这在图领域也非常重要。然而，现有的图形学习方法难以应对跨域任务。受CV和NLP成功经验的启发，跨域图学习再次成为实现真正的图形基础模型的关键焦点。在这次调查中，我们提供了对现有跨域图学习工作的全面回顾和分析。具体而言，我们首先提出了一种新的分类体系，基于学到的跨域信息将方法分为结构、特征和结构-特征混合类。接下来，我们系统地审查了这些类别中的代表性方法。最后，我们讨论了现有研究的限制，并指出了未来研究的潜在方向。相关论文将在GitHub上进行汇总并持续更新：https://github.com/cshhzhao/Awesome-Cross-Domain-Graph-Learning。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph learning plays a vital role in mining and analyzing complexrelationships involved in graph data, which is widely used in many real-worldapplications like transaction networks and communication networks. Foundationmodels in CV and NLP have shown powerful cross-domain capabilities that arealso significant in graph domains. However, existing graph learning approachesstruggle with cross-domain tasks. Inspired by successes in CV and NLP,cross-domain graph learning has once again become a focal point of attention torealizing true graph foundation models. In this survey, we present acomprehensive review and analysis of existing works on cross-domain graphlearning. Concretely, we first propose a new taxonomy, categorizing existingapproaches based on the learned cross-domain information: structure, feature,and structure-feature mixture. Next, we systematically survey representativemethods in these categories. Finally, we discuss the remaining limitations ofexisting studies and highlight promising avenues for future research. Relevantpapers are summarized and will be consistently updated at:https://github.com/cshhzhao/Awesome-Cross-Domain-Graph-Learning.</description>
      <author>example@mail.com (Haihong Zhao, Chenyi Zi, Aochuan Chen, Jia Li)</author>
      <guid isPermaLink="false">2503.11086v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Falcon: A Remote Sensing Vision-Language Foundation Model</title>
      <link>http://arxiv.org/abs/2503.11070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文介绍了一种专为遥感设计的综合性视觉-语言基础模型，名为Falcon。它提供了一个统一、基于提示的框架，能够有效执行全面和复杂的遥感任务。&lt;h4&gt;背景&lt;/h4&gt;当前需要一种强大的模型来处理远程感应领域的图像分类、目标检测、分割等多样化且复杂化的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个适应于多样的遥感任务的强大基础模型Falcon，并通过大规模的数据集训练提升其表示能力，使其能够编码丰富的空间和语义信息。&lt;h4&gt;方法&lt;/h4&gt;提出基于提示的统一框架Falcon；构建大型、多任务指令微调数据集Falcon_SFT进行模型训练。该数据集中包含约7800万高质量样本及560多万张不同分辨率与视角的遥感图像。&lt;h4&gt;主要发现&lt;/h4&gt;Falcon在14个不同的遥感任务上取得了卓越性能，尽管其参数量仅为0.7B。通过大量对比实验验证了这一点。&lt;h4&gt;结论&lt;/h4&gt;模型及训练数据集公开发布，旨在推动开源社区的发展。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种专为遥感设计的综合性视觉-语言基础模型Falcon。该模型提供了一个统一、基于提示的框架，能够有效执行全面和复杂的遥感任务。在给定简单自然语言指令和遥感图像的情况下，Falcon能够在14个不同的任务中（如图像分类、目标检测、分割等）生成令人印象深刻的结果。为了促进Falcon的训练并增强其编码丰富空间和语义信息的能力，我们开发了大规模多任务指令微调数据集Falcon_SFT。该数据集包含约7800万个高质量样本及560多万张不同分辨率与视角的遥感图像，并通过人工抽样验证确保高数据质量和可靠性。经过广泛的对比实验，证实即使参数量仅为0.7B，Falcon在67个数据集和14个任务上也表现出显著性能。我们将在https://github.com/TianHuiLab/Falcon公开发布完整数据集、代码及模型权重，希望帮助进一步发展开源社区。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a holistic vision-language foundation model tailoredfor remote sensing, named Falcon. Falcon offers a unified, prompt-basedparadigm that effectively executes comprehensive and complex remote sensingtasks. Falcon demonstrates powerful understanding and reasoning abilities atthe image, region, and pixel levels. Specifically, given simple naturallanguage instructions and remote sensing images, Falcon can produce impressiveresults in text form across 14 distinct tasks, i.e., image classification,object detection, segmentation, image captioning, and etc. To facilitateFalcon's training and empower its representation capacity to encode richspatial and semantic information, we developed Falcon_SFT, a large-scale,multi-task, instruction-tuning dataset in the field of remote sensing. TheFalcon_SFT dataset consists of approximately 78 million high-quality datasamples, covering 5.6 million multi-spatial resolution and multi-view remotesensing images with diverse instructions. It features hierarchical annotationsand undergoes manual sampling verification to ensure high data quality andreliability. Extensive comparative experiments are conducted, which verify thatFalcon achieves remarkable performance over 67 datasets and 14 tasks, despitehaving only 0.7B parameters. We release the complete dataset, code, and modelweights at https://github.com/TianHuiLab/Falcon, hoping to help further developthe open-source community.</description>
      <author>example@mail.com (Kelu Yao, Nuo Xu, Rong Yang, Yingying Xu, Zhuoyan Gao, Titinunt Kitrungrotsakul, Yi Ren, Pu Zhang, Jin Wang, Ning Wei, Chao Li)</author>
      <guid isPermaLink="false">2503.11070v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>RankPO: Preference Optimization for Job-Talent Matching</title>
      <link>http://arxiv.org/abs/2503.10723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 3 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种两阶段训练框架，用于改进大语言模型在招聘职位描述与候选人简历匹配中的表现。该框架首先利用对比学习方法进行初步训练，然后通过引入一种基于偏好的微调方法来优化模型的文本理解能力。&lt;h4&gt;背景&lt;/h4&gt;当前招聘系统需要能够理解和考虑地理位置和学术等级等语境因素的大规模语言模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个两阶段训练框架以提高大语言模型在招聘匹配任务中的性能，特别是在处理规则定义的数据与实际文本理解之间的平衡问题。&lt;h4&gt;方法&lt;/h4&gt;第一阶段采用对比学习方式训练模型；第二阶段引入RankPO（排名偏好优化）方法进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;初步模型虽然能在基于规则的数据集上取得良好表现，但在理解和匹配AI标注的语料时效果不佳。经过RankPO微调后，模型在保留原有任务性能的同时显著提高了与AI偏好的一致性。&lt;h4&gt;结论&lt;/h4&gt;提出的两阶段训练框架能够有效提升大语言模型对招聘岗位描述和简历之间的文本理解能力及偏好匹配度。&lt;h4&gt;翻译&lt;/h4&gt;为了应对职位描述与候选人才匹配的挑战，论文介绍了一种基于大规模语言模型的两阶段训练方法。第一阶段通过对比学习实现初步训练；第二阶段采用RankPO技术进一步优化模型性能。实验表明该方法可以有效地平衡模型在规则定义数据上的表现和文本理解能力之间的关系，并显著提升了AI偏好一致性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Matching job descriptions (JDs) with suitable talent requires models capableof understanding not only textual similarities between JDs and candidateresumes but also contextual factors such as geographical location and academicseniority. To address this challenge, we propose a two-stage training frameworkfor large language models (LLMs). In the first stage, a contrastive learningapproach is used to train the model on a dataset constructed from real-worldmatching rules, such as geographical alignment and research area overlap. Whileeffective, this model primarily learns patterns that defined by the matchingrules. In the second stage, we introduce a novel preference-based fine-tuningmethod inspired by Direct Preference Optimization (DPO), termed Rank PreferenceOptimization (RankPO), to align the model with AI-curated pairwise preferencesemphasizing textual understanding. Our experiments show that while thefirst-stage model achieves strong performance on rule-based data (nDCG@20 =0.706), it lacks robust textual understanding (alignment with AI annotations =0.46). By fine-tuning with RankPO, we achieve a balanced model that retainsrelatively good performance in the original tasks while significantly improvingthe alignment with AI preferences. The code and data are available athttps://github.com/yflyzhang/RankPO.</description>
      <author>example@mail.com (Yafei Zhang, Murray Wang, Yu Wang, Xiaohui Wang)</author>
      <guid isPermaLink="false">2503.10723v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Towards Privacy-preserved Pre-training of Remote Sensing Foundation Models with Federated Mutual-guidance Learning</title>
      <link>http://arxiv.org/abs/2503.11051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 5 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了FedSense，一种新的隐私保护预训练框架，旨在使多个机构能够在不共享私有数据的情况下协同训练远程感应基础模型（RSFMs）。&lt;h4&gt;背景&lt;/h4&gt;传统的RSFM通过大规模的策划遥感数据进行自监督预训练。然而，在每个机构中单独使用有限的数据进行预训练可能导致次优性能，并且从多个机构收集遥感数据以进行集中式预训练会引发隐私问题。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些问题，本文提出了一种新的协作解决方案FedSense框架，该框架允许多个机构在不共享私人数据的情况下协同训练RSFMs。&lt;h4&gt;方法&lt;/h4&gt;为了解决模型漂移和高通信开销的问题，引入了联邦互导学习。具体而言，提出了从服务器到客户端的指导（SCG）机制以引导客户端更新朝着全局平坦优化解的方向，并提出了一种从客户端到服务器的低比特通信机制将本地知识注入服务器。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的下游任务实验表明FedSense在全精度和减少通信场景中都有效，展示了显著的通信效率和性能提升。&lt;h4&gt;结论&lt;/h4&gt;通过引入FedSense框架解决了多机构协同训练RSFMs时遇到的隐私问题，并且该方法有效地提高了模型的通信效率与性能。&lt;h4&gt;翻译&lt;/h4&gt;传统的遥感基础模型（RSFM）是通过大规模策划遥感数据进行自监督预训练。然而，对于每个机构而言，利用有限的数据在独立的方式下对RSFM进行预训练可能会导致次优效果；而从多个机构收集遥感数据以进行集中式预训练则会引发隐私问题。寻求合作是一个有希望的解决方案，它可以解决这一困境，使多个机构可以在不共享私密数据的情况下协同训练RSFMs。在本文中我们提出了一种新的保护隐私的预训练框架（FedSense），它使多个机构能够在不分享私人数据的情况下协作训练RSFM。但是，这是一个受到遥感数据异质性和高通信开销阻碍的非平凡任务循环问题。为打破这一恶性循环，我们引入了联邦互导学习。具体而言，我们提出了一种从服务器到客户端的指导（SCG）机制以引导客户端更新朝着全局平坦优化解的方向，并通过低比特通信将本地知识注入服务器。在四个下游任务上的广泛实验表明，在全精度和减少通信场景中FedSense都表现出色，展示出显著的通信效率提升及性能增益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional Remote Sensing Foundation models (RSFMs) are pre-trained with adata-centralized paradigm, through self-supervision on large-scale curatedremote sensing data. For each institution, however, pre-training RSFMs withlimited data in a standalone manner may lead to suboptimal performance, whileaggregating remote sensing data from multiple institutions for centralizedpre-training raises privacy concerns. Seeking for collaboration is a promisingsolution to resolve this dilemma, where multiple institutions cancollaboratively train RSFMs without sharing private data. In this paper, wepropose a novel privacy-preserved pre-training framework (FedSense), whichenables multiple institutions to collaboratively train RSFMs without sharingprivate data. However, it is a non-trivial task hindered by a vicious cycle,which results from model drift by remote sensing data heterogeneity and highcommunication overhead. To break this vicious cycle, we introduce FederatedMutual-guidance Learning. Specifically, we propose a Server-to-Clients Guidance(SCG) mechanism to guide clients updates towards global-flatness optimalsolutions. Additionally, we propose a Clients-to-Server Guidance (CSG)mechanism to inject local knowledge into the server by low-bit communication.Extensive experiments on four downstream tasks demonstrate the effectiveness ofour FedSense in both full-precision and communication-reduced scenarios,showcasing remarkable communication efficiency and performance gains.</description>
      <author>example@mail.com (Jieyi Tan, Chengwei Zhang, Bo Dang, Yansheng Li)</author>
      <guid isPermaLink="false">2503.11051v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Harmonizing Large Language Models with Collaborative Behavioral Signals for Conversational Recommendation</title>
      <link>http://arxiv.org/abs/2503.10703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的概率框架，该框架通过潜在偏好建模将行为模式与对话交互相结合，以提高推荐系统的个性化能力。&lt;h4&gt;背景&lt;/h4&gt;基于对话的推荐系统已经成为一种流行的动态范式，用于通过互动对话提供个性化的建议。语言理解技术的进步显著提高了此类系统的对话流畅性。然而，现代语言模型虽然擅长解释用户偏好，但在有效利用集体行为模式方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;为了克服这一限制，该工作提出了一种新的概率框架，以协同整合行为数据和语言表达，从而生成更具相关性的建议。&lt;h4&gt;方法&lt;/h4&gt;提出的框架建立了一个双通道对齐机制，在这种机制中，从群体用户交互中学到的隐式偏好表示作为连接行为数据与语言表达的桥梁。具体来说，该框架首先通过已有的协作过滤技术推导出潜在的偏好表示，然后使用这些表示来共同优化语言偏好的表达和行为模式。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的全面评估表明，所提出的方法相比各种最先进的基线方法具有优越性能，在将对话交互与协同行为信号对齐方面尤其突出。&lt;h4&gt;结论&lt;/h4&gt;该工作展示了如何通过结合用户的行为数据和语言偏好来改善推荐系统的个性化能力，并为未来的研究提供了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conversational recommendation frameworks have gained prominence as a dynamicparadigm for delivering personalized suggestions via interactive dialogues. Theincorporation of advanced language understanding techniques has substantiallyimproved the dialogue fluency of such systems. However, while modern languagemodels demonstrate strong proficiency in interpreting user preferencesarticulated through natural conversation, they frequently encounter challengesin effectively utilizing collective behavioral patterns - a crucial element forgenerating relevant suggestions. To mitigate this limitation, this workpresents a novel probabilistic framework that synergizes behavioral patternswith conversational interactions through latent preference modeling. Theproposed method establishes a dual-channel alignment mechanism where implicitpreference representations learned from collective user interactions serve as aconnecting mechanism between behavioral data and linguistic expressions.Specifically, the framework first derives latent preference representationsthrough established collaborative filtering techniques, then employs theserepresentations to jointly refine both the linguistic preference expressionsand behavioral patterns through an adaptive fusion process. Comprehensiveevaluations across multiple benchmark datasets demonstrate the superiorperformance of the proposed approach compared to various state-of-the-artbaseline methods, particularly in aligning conversational interactions withcollaborative behavioral signals.</description>
      <author>example@mail.com (Guanrong Li, Kuo Tian, Jinnan Qi, Qinghan Fu, Zhen Wu, Xinyu Dai)</author>
      <guid isPermaLink="false">2503.10703v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Full Event Interpretation: a graph neural network for event reconstruction in Belle II</title>
      <link>http://arxiv.org/abs/2503.09401v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings for the 2024 Conference on Computing in High Energy and  Nuclear Physics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该工作介绍了基于图神经网络的GraFEI模型，用于在Belle~II实验中全面重建事件。&lt;h4&gt;背景&lt;/h4&gt;Belle~II实验适合测量包含不可见粒子（如中微子）末态的$B$介子衰变。通过重建相伴随的$B$介子，可以推断出这些粒子的动力学特性。&lt;h4&gt;目的&lt;/h4&gt;开发一个机器学习模型，利用图神经网络技术来实现全面事件解读，并减少背景噪声同时保持较高的信号效率。&lt;h4&gt;方法&lt;/h4&gt;GraFEI模型通过对检测到的末态粒子信息进行预测，重建衰变链结构而不依赖于先验假设。该模型应用于寻找$B^+ o K^+ u ar{u}$衰变的研究。&lt;h4&gt;主要发现&lt;/h4&gt;在寻找$B^+ o K^+ u ar{u}$过程中展示了GraFEI模型的性能，提供了对该过程大约3个标准差的证据。&lt;h4&gt;结论&lt;/h4&gt;通过使用图神经网络和仅保留信号似是而非衰变拓扑的方法，可以有效地减少背景噪声并保持较高的信号效率。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们介绍了基于图卷积网络的全事件解读模型GraFEI，并应用于Belle~II实验中全面重建事件的任务。该研究展示了在寻找$B^+ o K^+ u ar{u}$衰变中的性能和结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work we present the Graph-based Full Event Interpretation (GraFEI), amachine learning model based on graph neural networks to inclusivelyreconstruct events in the Belle~II experiment. Belle~II is well suited toperform measurements of $B$ meson decays involving invisible particles (e.g.neutrinos) in the final state. The kinematical properties of such particles canbe deduced from the energy-momentum imbalance obtained after reconstructing thecompanion $B$ meson produced in the event. This task is performed byreconstructing it either from all the particles in an event but the signaltracks, or using the Full Event Interpretation, an algorithm based on BoostedDecision Trees and limited to specific, hard-coded decay processes. A recentexample involving the use of the aforementioned techniques is the search forthe $B^+ \to K^+ \nu \bar \nu$ decay, that provided an evidence for thisprocess at about 3 standard deviations. The GraFEI model is trained to predictthe structure of the decay chain by exploiting the information from thedetected final state particles only, without making use of any priorassumptions about the underlying event. By retaining only signal-like decaytopologies, the model considerably reduces the amount of background whilekeeping a relatively high signal efficiency. The performances of the model whenapplied to the search for $B^+ \to K^+ \nu \bar \nu$ are presented.</description>
      <author>example@mail.com (Merna Abumusabh, Jacopo Cerasoli, Giulio Dujany, Corentin Santos)</author>
      <guid isPermaLink="false">2503.09401v2</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Domain Biometric Recognition using Body Embeddings</title>
      <link>http://arxiv.org/abs/2503.10931v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;生物识别技术在从可见光谱转向红外图像时面临挑战，因为领域差异显著影响了身份验证性能。&lt;h4&gt;目的&lt;/h4&gt;展示身体嵌入比面部嵌入更适合中波和长波红外领域的跨频谱人员识别，并解决多域人体识别问题，使用IARPA Janus基准多域脸部（IJB-MDF）数据集进行。&lt;h4&gt;方法&lt;/h4&gt;利用视觉变压器架构在IJB-MDF数据集上建立基准结果。通过广泛的实验提供了有关红外领域之间相互关系、可见光预训练模型的适应性、身体嵌入中的局部语义特征的作用以及针对小型数据集的有效训练策略的有价值见解。&lt;h4&gt;主要发现&lt;/h4&gt;1. 身体嵌入在中波和长波红外领域的跨频谱人员识别方面表现优于面部嵌入。2. 通过结合交叉熵损失和三元组损失进行微调，仅使用可见光预训练的身体模型即可获得LLCM数据集上的最先进的mAP分数。&lt;h4&gt;结论&lt;/h4&gt;本研究不仅为解决多域人体识别问题提供了解决方案，而且提供了关于红外领域之间相互关系、模型适应性和有效训练策略的深入见解。&lt;h4&gt;翻译&lt;/h4&gt;生物特征识别在从可见光谱转向红外图像时变得更具挑战性。由于领域差异显著影响身份验证性能，本研究展示了身体嵌入比面部嵌入更适合中波和长波红外领域的跨频谱人员识别，并使用IARPA Janus基准多域脸部（IJB-MDF）数据集解决了这一问题。通过广泛的实验，我们提供了有关红外领域之间相互关系、可见光预训练模型的适应性以及身体嵌入中的局部语义特征的作用的深入见解，并展示了简单结合交叉熵损失和三元组损失的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Biometric recognition becomes increasingly challenging as we move away fromthe visible spectrum to infrared imagery, where domain discrepanciessignificantly impact identification performance. In this paper, we show thatbody embeddings perform better than face embeddings for cross-spectral personidentification in medium-wave infrared (MWIR) and long-wave infrared (LWIR)domains. Due to the lack of multi-domain datasets, previous research oncross-spectral body identification - also known as Visible-Infrared PersonRe-Identification (VI-ReID) - has primarily focused on individual infraredbands, such as near-infrared (NIR) or LWIR, separately. We address themulti-domain body recognition problem using the IARPA Janus BenchmarkMulti-Domain Face (IJB-MDF) dataset, which enables matching of short-waveinfrared (SWIR), MWIR, and LWIR images against RGB (VIS) images. We leverage avision transformer architecture to establish benchmark results on the IJB-MDFdataset and, through extensive experiments, provide valuable insights into theinterrelation of infrared domains, the adaptability of VIS-pretrained models,the role of local semantic features in body-embeddings, and effective trainingstrategies for small datasets. Additionally, we show that finetuning a bodymodel, pretrained exclusively on VIS data, with a simple combination ofcross-entropy and triplet losses achieves state-of-the-art mAP scores on theLLCM dataset.</description>
      <author>example@mail.com (Anirudh Nanduri, Siyuan Huang, Rama Chellappa)</author>
      <guid isPermaLink="false">2503.10931v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Panopticon: Advancing Any-Sensor Foundation Models for Earth Observation</title>
      <link>http://arxiv.org/abs/2503.10845v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First two authors contributed equally. Code is available at:  https://github.com/Panopticon-FM/panopticon&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;地球观测（EO）数据来源于多样化的传感平台，这些平台具有不同的光谱带、空间分辨率和传感模式。大多数先前的工作将输入限制为固定的传感器。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的任何传感器基础模型Panopticon，该模型基于DINOv2框架构建，能够处理任意的传感通道组合。&lt;h4&gt;方法&lt;/h4&gt;['利用相同地理位置但不同传感器采集到的图像作为自然增强数据', '对频道进行子采样以多样化光谱输入', '添加跨通道注意力机制作为灵活的补丁嵌入方式']&lt;h4&gt;主要发现&lt;/h4&gt;Panopticon能够编码光学和合成孔径雷达传感器的波长及模式，实现任意传感组合的有效处理。在GEO-Bench上的广泛评估中表现出色。&lt;h4&gt;结论&lt;/h4&gt;Panopticon模型在广泛的地球观测任务上达到了最先进的性能，并且能够在未来卫星平台和技术上即时泛化，推动了无感器偏见的地球观测发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文描述了一种名为Panopticon的新地球观测基础模型。该模型可以处理各种传感器的数据，包括光学和合成孔径雷达数据。通过利用自然增强数据、子采样频道以及跨通道注意力机制，它能够在广泛的评估中表现优异，并在未来的传感技术上具有强大的适应性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Earth observation (EO) data features diverse sensing platforms with varyingspectral bands, spatial resolutions, and sensing modalities. While most priorwork has constrained inputs to fixed sensors, a new class of any-sensorfoundation models able to process arbitrary sensors has recently emerged.Contributing to this line of work, we propose Panopticon, an any-sensorfoundation model built on the DINOv2 framework. We extend DINOv2 by (1)treating images of the same geolocation across sensors as naturalaugmentations, (2) subsampling channels to diversify spectral input, and (3)adding a cross attention over channels as a flexible patch embedding mechanism.By encoding the wavelength and modes of optical and synthetic aperture radarsensors, respectively, Panopticon can effectively process any combination ofarbitrary channels. In extensive evaluations, we achieve state-of-the-artperformance on GEO-Bench, especially on the widely-used Sentinel-1 andSentinel-2 sensors, while out-competing other any-sensor models, as well asdomain adapted fixed-sensor models on unique sensor configurations. Panopticonenables immediate generalization to both existing and future satelliteplatforms, advancing sensor-agnostic EO.</description>
      <author>example@mail.com (Leonard Waldmann, Ando Shah, Yi Wang, Nils Lehmann, Adam J. Stewart, Zhitong Xiong, Xiao Xiang Zhu, Stefan Bauer, John Chuang)</author>
      <guid isPermaLink="false">2503.10845v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>DynCIM: Dynamic Curriculum for Imbalanced Multimodal Learning</title>
      <link>http://arxiv.org/abs/2503.06456v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多模态学习通过整合不同模式的互补信息来提高决策过程。然而，由于数据质量和表示能力的差异，多模态合作的潜力尚未得到充分开发。&lt;h4&gt;背景&lt;/h4&gt;现有方法在处理多模态数据时难以有效地平衡样本和模式之间的差距。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的动力课程学习框架DynCIM，用于量化来自样本级别和模式级别的内生不平衡。&lt;h4&gt;方法&lt;/h4&gt;{'样本层面': '根据预测偏差、一致性和稳定性动态评估每个样本的难度', '模式层面': '从全局和局部角度衡量模态贡献', '融合机制': '引入基于门控的动态融合机制，以适应性地调整模态贡献，减少冗余并优化融合效果'}&lt;h4&gt;主要发现&lt;/h4&gt;在六个跨双模态和三模态场景的多模式基准数据集上的广泛实验表明，DynCIM始终优于现有技术。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地缓解了模态与样本不平衡，并增强了多模态学习任务中的适应性和鲁棒性&lt;h4&gt;翻译&lt;/h4&gt;摘要：多模态学习通过整合不同模式的互补信息来提高决策过程。然而，由于数据质量和表示能力的差异，多模态合作的潜力尚未得到充分开发。为了应对这一挑战，我们介绍了DynCIM，这是一种新颖的动力课程学习框架，用于量化来自样本级别和模式级别的内生不平衡。DynCIM采用根据预测偏差、一致性和稳定性动态评估每个样本难度的样本级课程，并通过从全局和局部角度衡量模态贡献的模式级课程来实现这一点。此外，还引入了一种基于门控的动态融合机制，以适应性地调整模态贡献，减少冗余并优化融合效果。在六个跨双模态和三模态场景的多模式基准数据集上的广泛实验表明，DynCIM始终优于现有技术。我们的方法有效地缓解了模态与样本不平衡，并增强了多模态学习任务中的适应性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/raymond-qiancx/dyncim&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning integrates complementary information from diversemodalities to enhance the decision-making process. However, the potential ofmultimodal collaboration remains under-exploited due to disparities in dataquality and modality representation capabilities. To address this, we introduceDynCIM, a novel dynamic curriculum learning framework designed to quantify theinherent imbalances from both sample and modality perspectives. DynCIM employsa sample-level curriculum to dynamically assess each sample's difficultyaccording to prediction deviation, consistency, and stability, while amodality-level curriculum measures modality contributions from global andlocal. Furthermore, a gating-based dynamic fusion mechanism is introduced toadaptively adjust modality contributions, minimizing redundancy and optimizingfusion effectiveness. Extensive experiments on six multimodal benchmarkingdatasets, spanning both bimodal and trimodal scenarios, demonstrate that DynCIMconsistently outperforms state-of-the-art methods. Our approach effectivelymitigates modality and sample imbalances while enhancing adaptability androbustness in multimodal learning tasks. Our code is available athttps://github.com/Raymond-Qiancx/DynCIM.</description>
      <author>example@mail.com (Chengxuan Qian, Kai Han, Jingchao Wang, Zhenlong Yuan, Chongwen Lyu, Jun Chen, Zhe Liu)</author>
      <guid isPermaLink="false">2503.06456v2</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Quality Over Quantity? LLM-Based Curation for a Data-Efficient Audio-Video Foundation Model</title>
      <link>http://arxiv.org/abs/2503.09205v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  We are withdrawing this version due to the need for substantial  updates in scope and organization, which affect the clarity and completeness  of the manuscript. We plan to submit a revised version that incorporates  these changes&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种音频视频矢量对齐(AVVA)方法，该方法利用大型语言模型(LLM)进行数据预处理，并通过对比学习框架选择高质量的训练片段。&lt;h4&gt;背景&lt;/h4&gt;将音频和视觉数据整合以训练多模态基础模型仍然面临挑战。现有的方法在实现时间同步之外进一步匹配音频视频内容方面还不够完善。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，即AVVA，以便提高从音频到视频检索的准确性，并减少所需的数据量。&lt;h4&gt;方法&lt;/h4&gt;使用Whisper和DINOv2进行高质量训练片段的选择，在双编码器对比学习框架内通过语言模型驱动的数据预处理管道实现。&lt;h4&gt;主要发现&lt;/h4&gt;在AudioCaps、VALOR和VGGSound数据集上，AVVA相对于ImageBind的音频到视频检索任务中，在使用较少经过精心过滤的数据（192小时）的情况下实现了7.6%的准确性提升。并且通过消融实验表明了高质量数据的重要性。&lt;h4&gt;结论&lt;/h4&gt;尽管结果证明了AVVA在提高数据效率方面的有效性，但也指出了LLM驱动的数据预处理所带来的额外计算开销以及如何在更大规模的应用场景中进行扩展或近似的方法。&lt;h4&gt;翻译&lt;/h4&gt;整合音频和视觉数据以训练多模态基础模型仍然具有挑战性。我们提出了一种方法，称为Audio-Video Vector Alignment (AVVA)，通过大型语言模型驱动的数据预处理管道实现了超越单纯时间同步的视听内容对齐。特别是，AVVA利用Whisper（基于语音的音频基础模型）和DINOv2在双编码器对比学习框架内进行高质量训练片段的选择和评分。实验结果表明，在AudioCaps、VALOR和VGGSound数据集上，这种方法可以在显著减少精心过滤的数据量的情况下获得更高的准确性。例如，在使用仅192小时而不是5800+小时的精选数据时，AVVA在VGGSound上的音频到视频检索任务中比ImageBind提高了7.6%的top-1准确率。此外，消融实验显示，用高质量替代大量数据可以提高性能，分别在AudioCaps、VALOR和VGGSound上达到47.8、48.4和58.0个百分点的Top-3准确性提升。这些结果强调了AVVA的数据效率，但同时也讨论了LLM驱动的数据预处理带来的额外开销以及如何对其进行扩展或简化以适应更大的领域。总的来说，AVVA为无文本音频视频学习提供了更稳健且检索准确率更高的路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating audio and visual data for training multimodal foundational modelsremains challenging. We present Audio-Video Vector Alignment (AVVA), whichaligns audiovisual (AV) scene content beyond mere temporal synchronization viaa Large Language Model (LLM)-based data curation pipeline. Specifically, AVVAscores and selects high-quality training clips using Whisper (speech-basedaudio foundation model) for audio and DINOv2 for video within a dual-encodercontrastive learning framework. Evaluations on AudioCaps, VALOR, and VGGSounddemonstrate that this approach can achieve significant accuracy gains withsubstantially less curated data. For instance, AVVA yields a 7.6% improvementin top-1 accuracy for audio-to-video retrieval on VGGSound compared toImageBind, despite training on only 192 hours of carefully filtered data (vs.5800+ hours). Moreover, an ablation study highlights that trading data quantityfor data quality improves performance, yielding respective top-3 accuracyincreases of 47.8, 48.4, and 58.0 percentage points on AudioCaps, VALOR, andVGGSound over uncurated baselines. While these results underscore AVVA's dataefficiency, we also discuss the overhead of LLM-driven curation and how it maybe scaled or approximated in larger domains. Overall, AVVA provides a viablepath toward more robust, text-free audiovisual learning with improved retrievalaccuracy.</description>
      <author>example@mail.com (Ali Vosoughi, Dimitra Emmanouilidou, Hannes Gamper)</author>
      <guid isPermaLink="false">2503.09205v2</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Power Spectrum Signatures of Graphs</title>
      <link>http://arxiv.org/abs/2503.09660v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个新的点签名，即功率谱签名，并探讨了其在图数据和点云中的应用。&lt;h4&gt;背景&lt;/h4&gt;基于拉普拉斯算子的点签名已经成为机器学习领域中用于图形、聚类以及形状分析的重要工具。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的不变于图自同构变换且具有稳定性的功率谱签名，以提高对图信号处理和形状分析的能力。&lt;h4&gt;方法&lt;/h4&gt;定义了功率谱签名作为图傅立叶变换的平方，并研究其在扰动下的稳定性以及应用于指示函数类中的表现。&lt;h4&gt;主要发现&lt;/h4&gt;功率谱签名不受输入图自同构影响且具有稳定性质，可以用于生成描述图节点特性的特征。&lt;h4&gt;结论&lt;/h4&gt;展示了功率谱签名在刻画点云几何和对称性、解决图回归问题等方面的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;基于拉普拉斯算子的点签名已经成为机器学习领域中用于图形、聚类以及形状分析的重要工具。本文提出了一种新的点签名，即功率谱签名，定义为图信号傅立叶变换平方的一种度量，并展示了其在保持不变性和稳定性的优势下应用于图数据和点云中的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point signatures based on the Laplacian operators on graphs, point clouds,and manifolds have become popular tools in machine learning for graphs,clustering, and shape analysis. In this work, we propose a novel pointsignature, the power spectrum signature, a measure on $\mathbb{R}$ defined asthe squared graph Fourier transform of a graph signal. Unlike eigenvectors ofthe Laplacian from which it is derived, the power spectrum signature isinvariant under graph automorphisms. We show that the power spectrum signatureis stable under perturbations of the input graph with respect to theWasserstein metric. We focus on the signature applied to classes of indicatorfunctions, and its applications to generating descriptive features for verticesof graphs. To demonstrate the practical value of our signature, we showcaseseveral applications in characterizing geometry and symmetries in point clouddata, and graph regression problems.</description>
      <author>example@mail.com (Karamatou Yacoubou Djima, Ka Man Yim)</author>
      <guid isPermaLink="false">2503.09660v2</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Centaur: Robust End-to-End Autonomous Driving with Test-Time Training</title>
      <link>http://arxiv.org/abs/2503.11650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Centaur的新系统，用于在自动驾驶车辆部署时实时更新规划器的行为，以提高决策系统的可靠性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;当前大多数解决方案通过预设规则或成本函数来保证自动车辆的计划轨迹符合安全要求。然而这些方法往往过于保守且难以适应新的训练数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够根据测试时的数据实时更新规划器行为的方法，而不依赖于手工设计的规则或成本函数。&lt;h4&gt;方法&lt;/h4&gt;提出了Cluster Entropy作为不确定性度量，并通过利用先前的时间步收集的数据进行模型参数的梯度下降优化来最小化这个熵值。&lt;h4&gt;主要发现&lt;/h4&gt;Centaur系统仅需一次实时更新就能显著提高自动驾驶车辆的安全性能，尤其是在碰撞时间等关键指标上表现突出。&lt;h4&gt;结论&lt;/h4&gt;新的评估基准navsafe显示了现有驾驶模型未曾发现的新故障模式，而Centaur在这些挑战性的场景下表现出色。这表明该方法具有很好的应用潜力和研究价值。&lt;h4&gt;翻译&lt;/h4&gt;如何信任自主车辆的复杂决策系统？一种解决方案是在规划轨迹中加入一个“备用层”，以检查规则违规并替换为预定义的安全动作；另一种是调整计划者决定，最小化特定成本函数。然而这些编程规则或成本函数无法通过新训练数据学习和改进，通常导致过度保守的行为。本文提出了Centaur系统，利用测试时间的训练实时更新规划器行为，不依赖于手工设计的规则或成本函数，而是测量并减少规划决策中的不确定性。采用Cluster Entropy作为不确定性度量，并在先前的时间步收集的数据基础上通过梯度下降优化模型参数来最小化这个熵值。仅需一次这样的更新，在推理前，Centaur就展示了显著的改进，特别是在导航测试排行榜上提高了碰撞时间等安全关键指标的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; How can we rely on an end-to-end autonomous vehicle's complex decision-makingsystem during deployment? One common solution is to have a ``fallback layer''that checks the planned trajectory for rule violations and replaces it with apre-defined safe action if necessary. Another approach involves adjusting theplanner's decisions to minimize a pre-defined ``cost function'' usingadditional system predictions such as road layouts and detected obstacles.However, these pre-programmed rules or cost functions cannot learn and improvewith new training data, often resulting in overly conservative behaviors. Inthis work, we propose Centaur (Cluster Entropy for Test-time trAining usingUncertainty) which updates a planner's behavior via test-time training, withoutrelying on hand-engineered rules or cost functions. Instead, we measure andminimize the uncertainty in the planner's decisions. For this, we develop anovel uncertainty measure, called Cluster Entropy, which is simple,interpretable, and compatible with state-of-the-art planning algorithms. Usingdata collected at prior test-time time-steps, we perform an update to themodel's parameters using a gradient that minimizes the Cluster Entropy. Withonly this sole gradient update prior to inference, Centaur exhibits significantimprovements, ranking first on the navtest leaderboard with notable gains insafety-critical metrics such as time to collision. To provide detailed insightson a per-scenario basis, we also introduce navsafe, a challenging newbenchmark, which highlights previously undiscovered failure modes of drivingmodels.</description>
      <author>example@mail.com (Chonghao Sima, Kashyap Chitta, Zhiding Yu, Shiyi Lan, Ping Luo, Andreas Geiger, Hongyang Li, Jose M. Alvarez)</author>
      <guid isPermaLink="false">2503.11650v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Adversarial Data Collection: Human-Collaborative Perturbations for Efficient and Robust Robotic Imitation Learning</title>
      <link>http://arxiv.org/abs/2503.11646v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  More information can be found on our project  page:https://sites.google.com/view/adc-robot&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种对抗性数据收集框架，通过增强人类与机器人环境之间的实时互动来提高机器人的信息密度和任务表现。&lt;h4&gt;背景&lt;/h4&gt;在机器人操作中，高质量的数据相比于大量低质量的数据更有价值。然而，在现实世界中收集大规模数据集的成本非常高昂。&lt;h4&gt;目的&lt;/h4&gt;探讨如何以更少的、更高信息密度的数据替代大规模数据集，并且通过引入对抗性数据收集框架提高机器人的任务性能。&lt;h4&gt;方法&lt;/h4&gt;介绍了一种新的Adversarial Data Collection (ADC) 框架，该框架利用实时的人类-机器人交互来压缩失败-恢复行为和环境扰动到最小的演示集中。此过程不同于传统的被动记录静态演示的方法，而是采用协作干扰模式：一个对抗性的操作者在单次实验中动态改变物体状态、环境条件以及语言指令，而远程的操作人员则适应性地调整动作以克服这些不断变化的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，通过ADC训练的模型比使用完整数据集的传统方法具有更好的组成泛化能力、增强的感知扰动鲁棒性和自发的错误恢复能力。用20%体积的数据量收集到的演示就能显著超越传统方法的表现。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了战略性的数据采集对于可扩展的实际机器人学习至关重要，而非只是后期处理。此外，正在开发一个包含对抗性干扰的真实世界操作任务的大规模ADC-Robotics数据集，并将开放源代码以促进机器人模仿学习的进步。&lt;h4&gt;翻译&lt;/h4&gt;为了追求数据效率（即质量胜于数量），最大化单个演示的信息密度可以显著减少对大规模数据集的依赖，同时提高任务表现。本文介绍了一种名为Adversarial Data Collection (ADC) 的人类在环（Human-in-the-Loop）框架，通过实时的人机环境互动重新定义机器人数据收集方法。不同于传统的被动记录静态演示的方式，ADC采用协作扰动模式：对抗性操作者动态改变物体状态、环境条件和语言指令，远程操作人员则适应性调整动作以克服这些不断变化的挑战。此过程压缩了多样化的失败恢复行为、任务组成变异以及环境干扰到最小化展示中。实验表明ADC训练模型在未知任务指令上的组合泛化能力优越，对感知扰动具有增强鲁棒性和自发错误恢复能力。值得注意的是，使用20%体积数据量收集到的演示就能显著超越传统方法的表现。此研究展示了战略性数据采集而非仅仅后期处理对于可扩展的实际机器人学习至关重要，并正在开发一个包含对抗性干扰的真实世界操作任务的大规模ADC-Robotics数据集，以促进机器人模仿学习的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The pursuit of data efficiency, where quality outweighs quantity, has emergedas a cornerstone in robotic manipulation, especially given the high costsassociated with real-world data collection. We propose that maximizing theinformational density of individual demonstrations can dramatically reducereliance on large-scale datasets while improving task performance. To this end,we introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) frameworkthat redefines robotic data acquisition through real-time, bidirectionalhuman-environment interactions. Unlike conventional pipelines that passivelyrecord static demonstrations, ADC adopts a collaborative perturbation paradigm:during a single episode, an adversarial operator dynamically alters objectstates, environmental conditions, and linguistic commands, while thetele-operator adaptively adjusts actions to overcome these evolving challenges.This process compresses diverse failure-recovery behaviors, compositional taskvariations, and environmental perturbations into minimal demonstrations. Ourexperiments demonstrate that ADC-trained models achieve superior compositionalgeneralization to unseen task instructions, enhanced robustness to perceptualperturbations, and emergent error recovery capabilities. Strikingly, modelstrained with merely 20% of the demonstration volume collected through ADCsignificantly outperform traditional approaches using full datasets. Theseadvances bridge the gap between data-centric learning paradigms and practicalrobotic deployment, demonstrating that strategic data acquisition, not merelypost-hoc processing, is critical for scalable, real-world robot learning.Additionally, we are curating a large-scale ADC-Robotics dataset comprisingreal-world manipulation tasks with adversarial perturbations. This benchmarkwill be open-sourced to facilitate advancements in robotic imitation learning.</description>
      <author>example@mail.com (Siyuan Huang, Yue Liao, Siyuan Feng, Shu Jiang, Si Liu, Hongsheng Li, Maoqing Yao, Guanghui Ren)</author>
      <guid isPermaLink="false">2503.11646v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Disentangled Object-Centric Image Representation for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2503.11565v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DOCIR的框架，用于在多对象环境中从视觉输入中学习抓取和放置技能。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的学习机器人操作技巧对于开发能够广泛应用于现实世界的机器人应用非常重要。现有方法特别是以物体为中心的方法显示了较好的归纳偏置，但是它们在处理包含多个物体环境中的简单操作时存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架DOCIR，通过引入关于兴趣对象、障碍物和机器人本体的分离表示来解决现有问题。&lt;h4&gt;方法&lt;/h4&gt;DOCIR框架通过使用分离表示对多对象环境中视觉输入的学习进行改进，并且该模型在模拟环境中的表现优于现有的最佳性能。&lt;h4&gt;主要发现&lt;/h4&gt;DOCIR可以有效地学习抓取和放置技能，其泛化能力在测试时能够处理场景中不断变化的兴趣物体和干扰物。此外，在零样本迁移至真实世界的应用上也展示出有效性。&lt;h4&gt;结论&lt;/h4&gt;DOCIR框架显著提高了多对象环境中基于视觉的学习抓取和放置技能的效果，并且具有强大的泛化能力和实际应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;从视觉输入中学习机器人操作技巧是开发广泛适用于现实世界的机器人应用的一种有前途的方法。已经探索了许多方法来使这种基于视觉的学习成为可能，特别是以物体为中心的表示法显示了较好的归纳偏置，这提高了性能和泛化能力。然而，在多对象环境中，这些方法在学习简单操纵技能方面遇到了困难。因此，提出了一种新的框架DOCIR，它引入了一种分离表示方法来区分感兴趣的物体、障碍物以及机器人的本体。我们证明这种方法能以视觉输入为基础，在多物体环境中表现出色，并且能够在测试时处理变化的兴趣对象和场景中的干扰物。此外，我们在模拟环境和零样本迁移至真实世界的实验中验证了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning robotic manipulation skills from vision is a promising approach fordeveloping robotics applications that can generalize broadly to real-worldscenarios. As such, many approaches to enable this vision have been exploredwith fruitful results. Particularly, object-centric representation methods havebeen shown to provide better inductive biases for skill learning, leading toimproved performance and generalization. Nonetheless, we show thatobject-centric methods can struggle to learn simple manipulation skills inmulti-object environments. Thus, we propose DOCIR, an object-centric frameworkthat introduces a disentangled representation for objects of interest,obstacles, and robot embodiment. We show that this approach leads tostate-of-the-art performance for learning pick and place skills from visualinputs in multi-object environments and generalizes at test time to changingobjects of interest and distractors in the scene. Furthermore, we show itsefficacy both in simulation and zero-shot transfer to the real world.</description>
      <author>example@mail.com (David Emukpere, Romain Deffayet, Bingbing Wu, Romain Brégier, Michael Niemaz, Jean-Luc Meunier, Denys Proux, Jean-Michel Renders, Seungsu Kim)</author>
      <guid isPermaLink="false">2503.11565v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Vectorable Thrust Control for Multimodal Locomotion of Quadruped Robot SPIDAR</title>
      <link>http://arxiv.org/abs/2503.11551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 Pages. Presented in International Symposium of Robotics Research  (ISRR) 2024, Long Beach, USA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;在这篇文章中，我提出了一种新的四足机器人SPIDAR的可矢量推力控制方法，该机器人的每个关节都配备了矢量旋翼。文中首先简要介绍了机器人的独特机械设计、动态模型以及陆地/空中运动的基本控制框架。接着介绍了一种从基本控制框架衍生出来的用于空中运动的可矢量化推力控制方法。这种扩展飞行控制的一个关键特性是在特定关节配置下能够避免旋翼之间的气动干扰。此外，还提出了一种新的特定制动推进控制方法和一种基础步伐策略，以适应爬行等需要同时抬高所有腿的特殊陆地运动模式。最后，文章解释了复杂关节动作下的飞行实验结果以及重复性爬行动作的结果，证明所提出的推力控制方法在不同运动模式中的可行性。&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种四足机器人SPIDAR的可矢量推力控制系统的设计与实现&lt;h4&gt;背景&lt;/h4&gt;提出了一个配备了矢量旋翼的新式四足机器人SPIDAR，并探讨其用于陆地和空中运动的独特机制&lt;h4&gt;目的&lt;/h4&gt;开发一种适用于不同运动模式（如飞行、爬行）的有效推力控制方法&lt;h4&gt;方法&lt;/h4&gt;介绍了机器人的机械设计与动态模型，以及一种可避免气动干扰的扩展飞行控制系统；另外还提出了一种适应特定陆地运动模式的基础步伐策略&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示所提出的推力控制方法在复杂关节动作和重复性爬行动作中均表现出良好的可行性&lt;h4&gt;结论&lt;/h4&gt;新式四足机器人SPIDAR的可矢量推力控制系统对于实现高效灵活的陆空双栖运动具有重要意义&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, I present vectorable thrust control for different locomotionmodes by a novel quadruped robot, SPIDAR, equipped with vectoring rotor in eachlink. First, the robot's unique mechanical design, the dynamics model, and thebasic control framework for terrestrial/aerial locomotion are brieflyintroduced. Second, a vectorable thrust control method derived from the basiccontrol framework for aerial locomotion is presented. A key feature of thisextended flight control is its ability to avoid interrotor aerodynamicsinterference under specific joint configuration. Third, another extended thrustcontrol method and a fundamental gait strategy is proposed for specialterrestrial locomotion called crawling that requires all legs to be lifted atthe same time. Finally, the experimental results of the flight with a complexjoint motion and the repeatable crawling motion are explained, whichdemonstrate the feasibility of the proposed thrust control methods fordifferent locomotion modes.</description>
      <author>example@mail.com (Moju Zhao)</author>
      <guid isPermaLink="false">2503.11551v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-robot coordination for connectivity recovery after unpredictable environment changes</title>
      <link>http://arxiv.org/abs/2503.11520v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本论文提出了一种分布式方法，用于在多机器人团队因环境变化（如新障碍物的出现）导致连接失败后重新建立联系。&lt;h4&gt;背景&lt;/h4&gt;当不可预测的环境变化发生时，一个原本连通的多机器人团队可能被分割成不同的小组。各组间的通信范围有限，并且仅能获得当前视场内的部分信息。&lt;h4&gt;目的&lt;/h4&gt;目标是让这些分散的小队在新的环境下重新建立连接并形成从静态基站到目标位置的一条链。&lt;h4&gt;方法&lt;/h4&gt;提出的分布式重规划方法允许机器人根据新环境中的观察信息预测其他小组的新计划，以恢复与基站的连接，并实现最初的共同目标。&lt;h4&gt;主要发现&lt;/h4&gt;如果存在解决方案，则该方法可以将所有小队重新连接成一个单一的链条。并且对这种方法与其他两种情况进行比较：1）当所有的代理都有完整的环境信息时；2）某些机器人需要移动到其他等待的机器人处以建立新的连接。&lt;h4&gt;结论&lt;/h4&gt;数值仿真结果表明，提出的方案在不可预测的情况变化下是有效的，并且相较于其它情况表现出更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;在这篇文章中我们开发了一种分布式方法来重新连接因不可预知环境变化导致的多机器人团队。环境变化如新障碍物出现后，该团队被分割成不同的机器人小组。各小组具有有限的通信范围并且只能获取视场内的部分信息。他们的目标是在静态基站和目标位置之间形成一条链路。在提出的分布式重规划方法中，每个机器人都会根据改变场景中的新观察信息为其他小队预测新的计划，以恢复与基站的连接并实现最初的共同目标。如果存在解决方案，则该方法可以将所有小组重新连接成一个单一链条。本文比较了提出的方法与其他两种情况：1）当所有代理拥有完整的环境信息时；2）某些机器人需要移动到等待中的其他机器人处进行重新连接。还提供了数值仿真以评估在不可预测的场景变化情况下提出的方案的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the present paper we develop a distributed method to reconnect amulti-robot team after connectivity failures, caused by unpredictableenvironment changes, i.e. appearance of new obstacles. After the changes, theteam is divided into different groups of robots. The groups have a limitedcommunication range and only a partial information in their field of view aboutthe current scenario. Their objective is to form a chain from a static basestation to a goal location. In the proposed distributed replanning approach,the robots predict new plans for the other groups from the new observedinformation by each robot in the changed scenario, to restore the connectivitywith a base station and reach the initial joint objective. If a solutionexists, the method achieves the reconnection of all the groups in a uniquechain. The proposed method is compared with other two cases: 1) when all theagents have full information of the environment, and 2) when some robots mustmove to reach other waiting robots for reconnection. Numerical simulations areprovided to evaluate the proposed approach in the presence of unpredictablescenario changes.</description>
      <author>example@mail.com (Yaroslav Marchukov, Luis Montano)</author>
      <guid isPermaLink="false">2503.11520v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Multi-agent coordination for on-demand data gathering with periodic information upload</title>
      <link>http://arxiv.org/abs/2503.11504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本论文提出了一种用于多代理团队部署的信息收集和协调方法。该系统旨在平衡刷新时间和信息包总数的前提下，在不同的目标位置搜集并传送数据。&lt;h4&gt;背景&lt;/h4&gt;在静态操作中心定期请求变化的目标地点的数据情况下，需要开发一种新的方法来组织多代理团队的行动。&lt;h4&gt;目的&lt;/h4&gt;设计一个可以自动分配工人与收集者角色，并有效规划其路径的方法以优化信息收集和传递效率。&lt;h4&gt;方法&lt;/h4&gt;{'第一步': '确定工人的最佳工作区域划分', '第二步': '获得工人数目和收集者的最优配比，以及工人需要同谁通信（即另一个收集器或操作中心）的方案', '第三步': '计算出工人访问目标并传输给移动中的收集者或直接传送至操作中心的最佳路径'}&lt;h4&gt;主要发现&lt;/h4&gt;通过在不同场景下的仿真测试验证了该方法的有效性，尤其是在最佳区域划分算法和最优工人数目与收集者的配比方面。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法成功地实现了多代理团队的信息收集任务分配，并优化了数据传输效率。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们开发了一种规划和协调多代理团队部署来周期性获取所需信息的方法。静态操作中心会定期请求来自变化目标位置的数据。目标是平衡刷新时间和总的包数的同时搜集并传递数据到操作中心。系统自动将团队分为两类角色：工人负责收集数据，而收集者则负责重新传送这些数据给操作中心。提出的三步方法包括确定工人的最佳工作区域划分、获得最优的工人和收集者的配比以及计算出工人访问目标的最佳路径，并决定他们应该向移动中的哪个采集器或直接传输到操作中心交付信息。该方法在不同场景下的模拟测试中被证明有效，尤其是提供了最佳区域分配算法和最优化工人数目与收集者之间比例的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-030-24209-1_13&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we develop a method for planning and coordinating a multi-agentteam deployment to periodically gather information on demand. A staticoperation center (OC) periodically requests information from changing goallocations. The objective is to gather data in the goals and to deliver it tothe OC, balancing the refreshing time and the total number of informationpackages. The system automatically splits the team in two roles: workers togather data, or collectors to retransmit the data to the OC. The proposed threestep method: 1) finds out the best area partition for the workers; 2) obtainsthe best balance between workers and collectors, and with whom the workers mustto communicate, a collector or the OC; 3) computes the best tour for theworkers to visit the goals and deliver them to the OC or to a collector inmovement. The method is tested in simulations in different scenarios, providingthe best area partition algorithm and the best balance between collectors andworkers.</description>
      <author>example@mail.com (Yaroslav Marchukov, Luis Montano)</author>
      <guid isPermaLink="false">2503.11504v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Exploring the best way for UAV visual localization under Low-altitude Multi-view Observation Condition: a Benchmark</title>
      <link>http://arxiv.org/abs/2503.10692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该论文提出了一个针对低空多视角环境下的无人机绝对视觉定位（AVL）的基准测试。&lt;h4&gt;背景&lt;/h4&gt;在无全球导航卫星系统信号的环境中，通过建立无人机图像与地理标记参考地图之间的几何关系来进行位置确定。虽然已有许多研究使用图像检索和匹配技术实现AVL，但在低空多视图场景的研究仍然有限。&lt;h4&gt;目的&lt;/h4&gt;为了探索最佳的UAV AVL方法，在这种条件下提出了一个基准测试。&lt;h4&gt;方法&lt;/h4&gt;{'构建大规模数据集': '创建了一个名为AnyVisLoc的大规模低空多视角数据集，该数据集包含18,000张在多个场景和不同高度下拍摄的照片以及2.5D参考地图（包括航空摄影测量图和历史卫星图像）。', '提出统一框架': '提出了一个将最先进的AVL方法整合并全面测试其性能的统一框架。选择了表现最佳的方法作为基准，并基于此进行了影响定位精度的关键因素分析。', '引入新指标': '引入了一个新的检索指标PDM@K，该指标更适合无人机AVL任务的特点。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能评估': '在低空多视角条件下，所提出的基线方法实现了74.1%的定位精度（误差小于5m）。', '挑战揭示': '基准测试揭示了低空多视角环境下UAV AVL面临的挑战，并为未来研究提供了有价值的指导。'}&lt;h4&gt;结论&lt;/h4&gt;该论文通过构建大规模数据集、提出统一框架和引入新指标，全面评估了现有技术在解决低空多视角无人机绝对视觉定位问题上的性能，并为未来的改进方向提出了建议。&lt;h4&gt;翻译&lt;/h4&gt;Absolute Visual Localization (AVL) enables Unmanned Aerial Vehicle (UAV) to determine its position in GNSS-denied environments by establishing geometric relationships between UAV images and geo-tagged reference maps. While many previous works have achieved AVL with image retrieval and matching techniques, research in low-altitude multi-view scenarios still remains limited. Low-altitude Multi-view condition presents greater challenges due to extreme viewpoint changes. To explore the best UAV AVL approach in such condition, we proposed this benchmark. Firstly, a large-scale Low-altitude Multi-view dataset called AnyVisLoc was constructed. This dataset includes 18,000 images captured at multiple scenes and altitudes, along with 2.5D reference maps containing aerial photogrammetry maps and historical satellite maps. Secondly, a unified framework was proposed to integrate the state-of-the-art AVL approaches and comprehensively test their performance. The best combined method was chosen as the baseline and the key factors that influencing localization accuracy are thoroughly analyzed based on it. This baseline achieved a 74.1% localization accuracy within 5m under Low-altitude, Multi-view conditions. In addition, a novel retrieval metric called PDM@K was introduced to better align with the characteristics of the UAV AVL task. Overall, this benchmark revealed the challenges of Low-altitude, Multi-view UAV AVL and provided valuable guidance for future research.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Absolute Visual Localization (AVL) enables Unmanned Aerial Vehicle (UAV) todetermine its position in GNSS-denied environments by establishing geometricrelationships between UAV images and geo-tagged reference maps. While manyprevious works have achieved AVL with image retrieval and matching techniques,research in low-altitude multi-view scenarios still remains limited.Low-altitude Multi-view condition presents greater challenges due to extremeviewpoint changes. To explore the best UAV AVL approach in such condition, weproposed this benchmark. Firstly, a large-scale Low-altitude Multi-view datasetcalled AnyVisLoc was constructed. This dataset includes 18,000 images capturedat multiple scenes and altitudes, along with 2.5D reference maps containingaerial photogrammetry maps and historical satellite maps. Secondly, a unifiedframework was proposed to integrate the state-of-the-art AVL approaches andcomprehensively test their performance. The best combined method was chosen asthe baseline and the key factors that influencing localization accuracy arethoroughly analyzed based on it. This baseline achieved a 74.1% localizationaccuracy within 5m under Low-altitude, Multi-view conditions. In addition, anovel retrieval metric called PDM@K was introduced to better align with thecharacteristics of the UAV AVL task. Overall, this benchmark revealed thechallenges of Low-altitude, Multi-view UAV AVL and provided valuable guidancefor future research. The dataset and codes are available athttps://github.com/UAV-AVL/Benchmark</description>
      <author>example@mail.com (Yibin Ye, Xichao Teng, Shuo Chen, Zhang Li, Leqi Liu, Qifeng Yu, Tao Tan)</author>
      <guid isPermaLink="false">2503.10692v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Exo-muscle: A semi-rigid assistive device for the knee</title>
      <link>http://arxiv.org/abs/2503.11474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新型膝关节辅助装置Exo-Muscle的设计、原理和机电一体化。该装置通过半刚性原则结合了传统刚性和软体系统的优点。&lt;h4&gt;背景&lt;/h4&gt;现有的膝关节辅助系统主要基于硬质外骨骼结构或软腱驱动，存在与人体膝关节中心旋转点对齐的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型的半刚性辅助设备，以解决现有技术中存在的问题，并提供更确定性的负载补偿功能。&lt;h4&gt;方法&lt;/h4&gt;采用了一种新的半刚性链机制，该机制围绕膝关节设计，消除了装置与人体膝关节中心旋转点之间的错位，并为腱路提供了明确路径。&lt;h4&gt;主要发现&lt;/h4&gt;该设备能够向膝关节提供高达38Nm的辅助扭矩，在实验中证明了其可以为目标膝关节功能提供有效的协助。&lt;h4&gt;结论&lt;/h4&gt;通过一系列实验证明，这种新型半刚性机制比现有完全软体系统具有更高的负载补偿确定性和更好的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们介绍了用于膝关节的辅助设备Exo-Muscle的设计、原理和机电一体化。与现有的基于硬质外骨骼结构或软腱驱动方法的系统不同，本装置采用了一种新的半刚性原则，结合了硬性和柔性系统的优点。通过在膝关节周围使用一种新的半刚性链机制，消除了设备与人体膝关节中心旋转点之间的错位问题，并为腱路提供了一个明确路径。这使得该装置比完全软体系统具有更确定的负载补偿功能。该设备可以向膝关节提供高达38Nm的辅助扭矩，在实验部分通过一系列实验证明了其能够为目标膝关节功能提供有效的协助能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2021.3100609&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we introduce the principle, design and mechatronics ofExo-Muscle, a novel assistive device for the knee joint. Different from theexisting systems based on rigid exoskeleton structures or soft-tendon drivenapproaches, the proposed device leverages a new semi-rigid principle thatexplores the benefits of both rigid and soft systems. The use of a novelsemi-rigid chain mechanism around the knee joint eliminates the presence ofmisalignment between the device and the knee joint center of rotation, while atthe same time, it forms a well-defined route for the tendon. This results inmore deterministic load compensation functionality compared to the fully softsystems. The proposed device can provide up to 38Nm assistive torque to theknee joint. In the experiment section, the device was successfully validatedthrough a series of experiments demonstrating the capacity of the device toprovide the target assistive functionality in the knee joint.</description>
      <author>example@mail.com (Yifang Zhang, Arash Ajoudani, Nikos G Tsagarakis)</author>
      <guid isPermaLink="false">2503.11474v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>To Assess the Impact of Smart Cities on Urbanization Patterns in the United States</title>
      <link>http://arxiv.org/abs/2503.11260v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  60 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文探讨了美国智慧城市倡议与城市化趋势之间的关系，研究重点关注快速的城市化进程，并探索智能城市的创新如何影响城市发展。&lt;h4&gt;背景&lt;/h4&gt;随着美国城市化的迅速发展，需要更好地理解智慧城市技术、政府政策、环境可持续性和社会经济因素是如何共同作用的。&lt;h4&gt;目的&lt;/h4&gt;调查智能城市建设对城市发展的具体影响，并提出为适应日益增长的人口优化城市规划的战略建议。&lt;h4&gt;方法&lt;/h4&gt;采用混合研究方法，包括定量和定性分析。通过在线问卷（n=50）收集数据，使用五级李克特量表进行居民意见调研，在曼哈顿的纽约市和西雅图的国会山地区实施。&lt;h4&gt;主要发现&lt;/h4&gt;智慧城市技术的应用与人口密度的变化、土地用途多样化以及基础设施动态增强存在显著关联。同时，居民对基于高效城市移动性、环境可持续性和个人社会经济改善的智能城市的偏好更加明显。&lt;h4&gt;结论&lt;/h4&gt;将识别出的影响因素纳入战略城市规划中可以优化城市发展以更好地适应快速增长的城市人口。&lt;h4&gt;翻译&lt;/h4&gt;摘要：本文研究了美国智慧城市倡议与不断发展的城市化趋势之间的关系。该研究关注美国快速城市增长这一关键问题，并探讨智能城市理念中的创新如何影响城市发展。基于城市复杂性理论，本研究确定了四个与智慧城市的因素及其对城市化进程的影响相关的变量：智慧城市技术、政府政策、环境可持续性和社会经济因素。采用了定量和定性分析相结合的混合方法论。进行了一项在线调查（n=50），使用五级李克特量表，在纽约市曼哈顿和西雅图国会山地区的居民中展开调研。结果显示，实施智慧城市技术与人口密度变化、土地用途多样化以及基础设施动态增强显著相关。此外，居民表现出对基于高效城市移动性、环境可持续性和个人社会经济改善的智慧城市的偏好。这些发现强调了对城市规划者、决策制定者和雇主的重要考虑因素。该研究得出结论认为，在战略城市规划中整合识别出的影响因素可以优化城市发展以更好地适应不断增长的城市人口。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates the relationship between smart city initiatives andevolving urbanization trends in the United States. The research addresses thecritical issue of rapid urban growth in the U.S. and explores how innovationswithin the smart city paradigm influence urban development. Utilizingprinciples from Urban Complexity Theory, this study identifies four keyvariables relevant to smart cities and their impact on urbanization: smart citytechnology, government policy, environmental sustainability, and socioeconomicfactors. A mixed-method approach, combining quantitative and qualitativemethodologies, was employed. A web-based survey (n=50) utilizing a five-pointLikert scale was conducted among residents of Manhattan, New York, and CapitolHill, Seattle. Results indicate that the implementation of smart citytechnologies is significantly associated with shifts in population density,land use diversification, and enhanced infrastructure dynamics. Additionally,residents demonstrated preferences for smart cities based on efficient urbanmobility, environmental sustainability, and personal socioeconomicimprovements. The findings highlight essential considerations for urbanplanners, policymakers, and employers. This study concludes that incorporatingthe identified influential factors into strategic urban planning optimizes citydevelopment to better accommodate growing urban populations.</description>
      <author>example@mail.com (Wayne S Singh)</author>
      <guid isPermaLink="false">2503.11260v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Obstacle Avoidance with Bounded Rationality Adversarial Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2503.11467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的方法Hi-QARL来提升导航策略在面对动态障碍物时的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;强化学习已经被证明在获得四足机器人稳定行走姿态方面非常有效，但在不熟悉的环境中导航仍存在挑战。采用分层方法有助于解决此类问题。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过将障碍物建模为对抗性代理来增强高阶导航策略的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;采用一种名为Hierarchical policies via Quantal response Adversarial Reinforcement Learning (Hi-QARL)的方法，该方法通过量化反应均衡来约束对抗性代理的理性，并对其施加训练课程。&lt;h4&gt;主要发现&lt;/h4&gt;新提出的方法在未知随机迷宫中具有强大的鲁棒性，并且在模拟中的Unitree GO1机器人上证明了其实际应用能力。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，通过将障碍物建模为对抗性代理进行的强化学习可以显著增强四足机器人的导航策略的鲁棒性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已经作为描述内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) has proven largely effective in obtaining stablelocomotion gaits for legged robots. However, designing control algorithms whichcan robustly navigate unseen environments with obstacles remains an ongoingproblem within quadruped locomotion. To tackle this, it is convenient to solvenavigation tasks by means of a hierarchical approach with a low-levellocomotion policy and a high-level navigation policy. Crucially, the high-levelpolicy needs to be robust to dynamic obstacles along the path of the agent. Inthis work, we propose a novel way to endow navigation policies with robustnessby a training process that models obstacles as adversarial agents, followingthe adversarial RL paradigm. Importantly, to improve the reliability of thetraining process, we bound the rationality of the adversarial agent resortingto quantal response equilibria, and place a curriculum over its rationality. Wecalled this method Hierarchical policies via Quantal response AdversarialReinforcement Learning (Hi-QARL). We demonstrate the robustness of our methodby benchmarking it in unseen randomized mazes with multiple obstacles. To proveits applicability in real scenarios, our method is applied on a Unitree GO1robot in simulation.</description>
      <author>example@mail.com (Jose-Luis Holgado-Alvarez, Aryaman Reddi, Carlo D'Eramo)</author>
      <guid isPermaLink="false">2503.11467v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Federated Koopman-Reservoir Learning for Large-Scale Multivariate Time-Series Anomaly Detection</title>
      <link>http://arxiv.org/abs/2503.11255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at SDM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为FedKO的新型无监督联邦学习框架，用于处理多变量时间序列（MVTS）数据中的异常检测问题。FedKO结合了Koopman算子理论和水库计算技术的优势，在保护隐私的同时实现了高效的时空处理。&lt;h4&gt;背景&lt;/h4&gt;随着边缘设备数量的增加，生成了大量的MVTS数据，这些数据对于医疗保健到智慧城市等各种应用至关重要。然而，这类数据流易受到系统故障或安全事件等重要问题指示的异常影响。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的无监督联邦学习框架FedKO，以应对大规模分布式环境中异质性、变化性和隐私顾虑所带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;将Koopman算子理论与水库计算相结合，并将其应用于多变量时间序列数据的时空处理中。此外，通过分层优化问题的形式化和特定的联邦算法来探索一个共享的Reservoir-Koopman模型。&lt;h4&gt;主要发现&lt;/h4&gt;FedKO在多种数据集上的实验结果显示了其在MVTS异常检测方面的优越性能，并且相比现有的最佳方法，它还能减少高达8倍的数据通信量和2倍的内存使用量。&lt;h4&gt;结论&lt;/h4&gt;FedKO框架为大规模系统中的多变量时间序列异常检测提供了一种高效、隐私保护的方法，特别适合边缘设备部署。&lt;h4&gt;翻译&lt;/h4&gt;随着边缘设备数量的增长，生成了大量用于医疗保健到智慧城市应用的关键多变量时间序列（MVTS）数据。然而，这些数据流易遭受信号重要问题如系统故障或安全事件的异常影响。传统的方法在处理大规模分布式环境中的异质性、变化性和隐私担忧时表现出不足。为此，本文提出了FedKO框架，它利用Koopman算子理论和水库计算技术的优点，在保持效率的同时实现了时空处理和数据保护。FedKO被设计为解决一个分层优化问题，并采用特定的联邦算法在不同数据集之间探索共享的Reservoir-Koopman模型。实验结果显示，与现有方法相比，FedKO在MVTS异常检测方面表现出色并显著减少了通信量和内存使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of edge devices has dramatically increased the generationof multivariate time-series (MVTS) data, essential for applications fromhealthcare to smart cities. Such data streams, however, are vulnerable toanomalies that signal crucial problems like system failures or securityincidents. Traditional MVTS anomaly detection methods, encompassing statisticaland centralized machine learning approaches, struggle with the heterogeneity,variability, and privacy concerns of large-scale, distributed environments. Inresponse, we introduce FedKO, a novel unsupervised Federated Learning frameworkthat leverages the linear predictive capabilities of Koopman operator theoryalong with the dynamic adaptability of Reservoir Computing. This enableseffective spatiotemporal processing and privacy preservation for MVTS data.FedKO is formulated as a bi-level optimization problem, utilizing a specificfederated algorithm to explore a shared Reservoir-Koopman model across diversedatasets. Such a model is then deployable on edge devices for efficientdetection of anomalies in local MVTS streams. Experimental results acrossvarious datasets showcase FedKO's superior performance against state-of-the-artmethods in MVTS anomaly detection. Moreover, FedKO reduces up to 8xcommunication size and 2x memory usage, making it highly suitable forlarge-scale systems.</description>
      <author>example@mail.com (Long Tan Le, Tung-Anh Nguyen, Han Shu, Suranga Seneviratne, Choong Seon Hong, Nguyen H. Tran)</author>
      <guid isPermaLink="false">2503.11255v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>MRS-CWC: A Weakly Constrained Multi-Robot System with Controllable Constraint Stiffness for Mobility and Navigation in Unknown 3D Rough Environments</title>
      <link>http://arxiv.org/abs/2503.11461v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;一种新型的多机器人系统（MRS-CWC）被提出，通过动态调整连接约束刚度，在复杂地形中实现了灵活性与移动性的平衡。&lt;h4&gt;背景&lt;/h4&gt;现有技术在处理3D复杂环境时存在局限性。传统离散系统因个体移动能力有限而难以适应粗糙地形；模块化系统提高了穿越能力但控制复杂且缺乏灵活性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多机器人系统（MRS-CWC），该系统通过可调约束刚度来平衡柔性与机动性，解决现有系统的不足。&lt;h4&gt;方法&lt;/h4&gt;将机器人单元连接为具有动态调整刚性的约束；构建系统的动力学和控制模型；在包含100种不同地形的基准数据集上评估其性能。&lt;h4&gt;主要发现&lt;/h4&gt;MRS-CWC在导航完成率、成功率、效率以及能源消耗方面均表现出色，特别是在崎岖地形中的表现优于所有基线方法。此外，在没有环境建模和路径规划的情况下也能取得良好效果。&lt;h4&gt;结论&lt;/h4&gt;该研究成功开发并验证了一个物理原型，在构建的复杂环境中展示了MRS-CWC的实际可行性。&lt;h4&gt;翻译&lt;/h4&gt;在未知且多变的三维崎岖环境中导航对多机器人系统来说是一项挑战。传统的离散系统由于个体移动能力有限而难以应对粗糙地形，虽然模块化系统通过可调节的刚性连接提高了穿越性能，但也带来了控制复杂度高和灵活性降低的问题。为了解决这些限制，我们提出了一种使用可控弱约束的多机器人系统（MRS-CWC），其中机器人的单元由可以实时调整刚性的约束相联接。这种自适应机制在环境交互过程中即时软化或硬化，确保了柔性和机动性之间的平衡。我们制定了系统的动力学和控制模型，并在一个基准数据集中评估了该方法相对于六种基线方法和一个缺乏部分功能的变体的表现，在这个数据集中包含了100个不同的模拟地形。结果显示MRS-CWC在导航完成率方面领先，而在崎岖地形中的成功率、效率以及能耗表现仅次于最复杂的一个变体（后者具备环境建模和路径规划）。此外我们还开发了一个物理原型，并在一个建造的崎岖环境中验证了其可行性。更多视频、仿真基准及代码请访问https://wyd0817.github.io/project-mrs-cwc/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Navigating unknown three-dimensional (3D) rugged environments is challengingfor multi-robot systems. Traditional discrete systems struggle with roughterrain due to limited individual mobility, while modular systems--where rigid,controllable constraints link robot units--improve traversal but suffer fromhigh control complexity and reduced flexibility. To address these limitations,we propose the Multi-Robot System with Controllable Weak Constraints (MRS-CWC),where robot units are connected by constraints with dynamically adjustablestiffness. This adaptive mechanism softens or stiffens in real-time duringenvironmental interactions, ensuring a balance between flexibility andmobility. We formulate the system's dynamics and control model and evaluateMRS-CWC against six baseline methods and an ablation variant in a benchmarkdataset with 100 different simulation terrains. Results show that MRS-CWCachieves the highest navigation completion rate and ranks second in successrate, efficiency, and energy cost in the highly rugged terrain group,outperforming all baseline methods without relying on environmental modeling,path planning, or complex control. Even where MRS-CWC ranks second, itsperformance is only slightly behind a more complex ablation variant withenvironmental modeling and path planning. Finally, we develop a physicalprototype and validate its feasibility in a constructed rugged environment. Forvideos, simulation benchmarks, and code, please visithttps://wyd0817.github.io/project-mrs-cwc/.</description>
      <author>example@mail.com (Runze Xiao, Yongdong Wang, Yusuke Tsunoda, Koichi Osuka, Hajime Asama)</author>
      <guid isPermaLink="false">2503.11461v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Torque Control of Exoskeletons under Spasticity Conditions via Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2503.11433v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in IEEE 19th International Conference on  Rehabilitation Robotics (ICORR2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的基于深度强化学习的自适应扭矩控制器，用于在关节痉挛条件下为膝部外骨骼进行个性化控制。该控制器旨在提高任务性能并减少交互力。&lt;h4&gt;背景&lt;/h4&gt;痉挛是一种常见的运动障碍症状，主要出现在脑瘫、遗传性双下肢痉挛病、脊髓损伤和中风患者身上，并且是这些疾病进展中最令人困扰的特征之一。尽管使用穿戴式机器人治疗痉挛可能有益处，但目前对于Modified Ashworth量表评分高于1+级别的受试者不推荐使用。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型自适应扭矩控制器，以改善膝部外骨骼在关节痉挛情况下的控制性能，并减少施加于人类关节的最大扭矩和交互力&lt;h4&gt;方法&lt;/h4&gt;通过建立一个包含肌肉骨骼-外骨骼系统及不同级别的肌肉激活可微分痉挛反射模型的数字孪生体来训练深度强化学习代理。&lt;h4&gt;主要发现&lt;/h4&gt;模拟膝部伸展运动的结果表明，该控制器在不同程度的痉挛患者中都能有效地控制外骨骼；与传统顺应性控制器相比，在痉挛条件下平均减少了10.6%的最大关节扭矩以及8.9%的根均方值直到稳定时间。&lt;h4&gt;结论&lt;/h4&gt;基于深度强化学习的自适应扭矩控制器为膝部外骨骼提供了更安全有效的个性化控制策略，有望改善穿戴式机器人治疗痉挛的效果。&lt;h4&gt;翻译&lt;/h4&gt;该论文摘要描述了开发了一种新型通过深度增强学习实现的自适应扭矩控制器，用于在关节痉挛情况下使用膝外骨骼装置时的应用。控制器旨在减少人体关节的最大扭矩以及交互力，并展示了其相较于传统顺应性控制策略的优势，在不同级别的痉挛患者中得到了验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spasticity is a common movement disorder symptom in individuals with cerebralpalsy, hereditary spastic paraplegia, spinal cord injury and stroke, being oneof the most disabling features in the progression of these diseases. Despitethe potential benefit of using wearable robots to treat spasticity, their useis not currently recommended to subjects with a level of spasticity above${1^+}$ on the Modified Ashworth Scale. The varying dynamics of thisvelocity-dependent tonic stretch reflex make it difficult to deploy safepersonalized controllers. Here, we describe a novel adaptive torque controllervia deep reinforcement learning (RL) for a knee exoskeleton under jointspasticity conditions, which accounts for task performance and interactionforces reduction. To train the RL agent, we developed a digital twin, includinga musculoskeletal-exoskeleton system with joint misalignment and adifferentiable spastic reflexes model for the muscles activation. Results for asimulated knee extension movement showed that the agent learns to control theexoskeleton for individuals with different levels of spasticity. The proposedcontroller was able to reduce maximum torques applied to the human joint underspastic conditions by an average of 10.6\% and decreases the root mean squareuntil the settling time by 8.9\% compared to a conventional compliantcontroller.</description>
      <author>example@mail.com (Andrés Chavarrías, David Rodriguez-Cianca, Pablo Lanillos)</author>
      <guid isPermaLink="false">2503.11433v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>TASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2503.11423v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference on Computer Vision and Pattern Recognition 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文针对现有任务导向性手-物交互视频生成数据集和模型的关键局限进行了探讨，提出了一个新的大规模数据集TASTE-Rob，并改进了现有的生成模型。&lt;h4&gt;背景&lt;/h4&gt;当前的数据集如Ego4D存在视角不一致和互动错位的问题，导致视频质量降低且难以应用于精确的机器人模仿学习任务。&lt;h4&gt;目的&lt;/h4&gt;为了改善手-物交互视频的质量并提高其在机器人模仿学习中的适用性，引入了一个新的大规模数据集TASTE-Rob，并通过改进生成模型来提升手部姿势准确性。&lt;h4&gt;方法&lt;/h4&gt;[{'VDM微调': '使用Video Diffusion Model (VDM)并在TASTE-Rob上进行微调以实现更真实的物体互动'}, {'三阶段姿态优化流程': '引入一个三阶段的姿态优化管道，用于改进生成视频中的手部姿势准确性'}]&lt;h4&gt;主要发现&lt;/h4&gt;['通过在TASTE-Rob数据集上的实验表明，在手抓握姿势方面偶尔会存在不一致性', '我们的方法能够在任务导向性手-物交互视频的生成中提供显著性能提升']&lt;h4&gt;结论&lt;/h4&gt;我们的工作展示了如何通过改进数据集和模型来提高机器人模仿学习的质量，为领域内的进一步研究提供了可能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：我们针对现有的用于机器人模仿学习的任务导向性手-物交互视频生成的数据集和模型的关键限制进行了探讨。当前的数据集（例如Ego4D）通常会遭受视角不一致和互动错位的问题，导致视频质量降低且难以应用于精确的模仿任务。为此，我们推出了TASTE-Rob——一个包含100,856个第一人称手-物交互视频的开创性大规模数据集。每个视频都经过精心的语言指令对齐，并从固定的摄像机视角记录下来以确保互动清晰度。通过在TASTE-Rob上微调Video Diffusion Model (VDM)，我们实现了真实的手物互动，尽管观察到偶尔会出现抓握姿势不一致的情况。为了提升现实感，我们引入了一个三阶段的姿态优化流程，从而改进了生成视频中的手部姿态准确性。我们的精心策划的数据集和特别设计的姿态优化框架在生成高质量、任务导向性的手-物交互视频方面取得了显著性能提升，并实现了优越的一般化机器人操作能力。TASTE-Rob数据集将在发表后公开发布，以促进该领域的进一步研究进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We address key limitations in existing datasets and models for task-orientedhand-object interaction video generation, a critical approach of generatingvideo demonstrations for robotic imitation learning. Current datasets, such asEgo4D, often suffer from inconsistent view perspectives and misalignedinteractions, leading to reduced video quality and limiting their applicabilityfor precise imitation learning tasks. Towards this end, we introduce TASTE-Rob-- a pioneering large-scale dataset of 100,856 ego-centric hand-objectinteraction videos. Each video is meticulously aligned with languageinstructions and recorded from a consistent camera viewpoint to ensureinteraction clarity. By fine-tuning a Video Diffusion Model (VDM) on TASTE-Rob,we achieve realistic object interactions, though we observed occasionalinconsistencies in hand grasping postures. To enhance realism, we introduce athree-stage pose-refinement pipeline that improves hand posture accuracy ingenerated videos. Our curated dataset, coupled with the specializedpose-refinement framework, provides notable performance gains in generatinghigh-quality, task-oriented hand-object interaction videos, resulting inachieving superior generalizable robotic manipulation. The TASTE-Rob datasetwill be made publicly available upon publication to foster further advancementsin the field.</description>
      <author>example@mail.com (Hongxiang Zhao, Xingchen Liu, Mutian Xu, Yiming Hao, Weikai Chen, Xiaoguang Han)</author>
      <guid isPermaLink="false">2503.11423v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>AQUA-SLAM: Tightly-Coupled Underwater Acoustic-Visual-Inertial SLAM with Sensor Calibration</title>
      <link>http://arxiv.org/abs/2503.11420v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '介绍了一种新的水下紧密耦合的声学-视觉-惯性同时定位与地图构建（SLAM）方法AQUA-SLAM，该方法融合了多普勒速度日志（DVL）、立体相机和惯性测量单元（IMU），并提出了一种高效的传感器校准技术。评估结果表明该系统在水下环境中具有更高的定位精度和鲁棒性。', '背景': '水下环境对视觉SLAM系统的可见度、照明条件以及图像中结构特征的连续性提出了挑战。', '目的': '开发一种结合声学信号、立体相机及惯性传感器信息的融合方法，以提高水下环境中的定位精度和鲁棒性。', '方法': {'AQUA-SLAM框架': '提出了一种将DVL、立体相机和IMU的数据在图优化框架中进行融合的方法', '高效传感器校准技术': '包括多传感器外参标定（涉及DVL、相机和IMU）及DVL换能器偏移标定，采用快速线性近似程序实现实时在线执行'}, '主要发现': {'实验结果': '在水槽环境中使用地面真值进行评估，并在北海的海上应用中验证了该方法的有效性。结果显示AQUA-SLAM系统比现有的最先进的水下及视觉-惯性SLAM系统具有更高的定位精度和鲁棒性'}, '结论': '提出的方法在提高水下环境中的SLAM性能方面取得了显著成果，未来将开源分享给社区', '翻译': '水下环境对同时定位与地图构建（SLAM）系统提出了挑战，包括可见度差、照明不足以及图像中结构特征的间歇性丢失。本文介绍了一种新的紧密耦合声学-视觉-惯性SLAM方法AQUA-SLAM，融合了多普勒速度日志（DVL）、立体相机和惯性测量单元（IMU）在一个图优化框架内。此外，我们提出了一种高效的传感器校准技术，包括多传感器外参标定（涉及DVL、相机和IMU）以及DVL换能器偏移标定，并使用快速线性近似程序进行实时在线执行。这些方法在带有地面真值的水槽环境中进行了广泛评估，并通过北海的海上应用验证了它们的有效性。结果表明，该方法在定位精度和鲁棒性方面超越了当前最先进的水下及视觉-惯性SLAM系统。所提出的系统将对社区开放源代码分享。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater environments pose significant challenges for visual SimultaneousLocalization and Mapping (SLAM) systems due to limited visibility, inadequateillumination, and sporadic loss of structural features in images. Addressingthese challenges, this paper introduces a novel, tightly-coupledAcoustic-Visual-Inertial SLAM approach, termed AQUA-SLAM, to fuse a DopplerVelocity Log (DVL), a stereo camera, and an Inertial Measurement Unit (IMU)within a graph optimization framework. Moreover, we propose an efficient sensorcalibration technique, encompassing multi-sensor extrinsic calibration (amongthe DVL, camera and IMU) and DVL transducer misalignment calibration, with afast linear approximation procedure for real-time online execution. Theproposed methods are extensively evaluated in a tank environment with groundtruth, and validated for offshore applications in the North Sea. The resultsdemonstrate that our method surpasses current state-of-the-art underwater andvisual-inertial SLAM systems in terms of localization accuracy and robustness.The proposed system will be made open-source for the community.</description>
      <author>example@mail.com (Shida Xu, Kaicheng Zhang, Sen Wang)</author>
      <guid isPermaLink="false">2503.11420v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>A Framework for a Capability-driven Evaluation of Scenario Understanding for Multimodal Large Language Models in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2503.11400v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE IAVVC 2025, Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种全面评估多模态大型语言模型（MLLMs）在自动驾驶上下文中能力的框架，该框架根据语义、空间、时间和物理四个核心维度来理解场景。&lt;h4&gt;背景&lt;/h4&gt;多模态大型语言模型结合了领域无关的世界知识和具体情境的语言指导，有可能通过增强自主驾驶系统的性能来提高自动驾驶的能力。然而，其整合于独立概念验证应用程序中的表现评估仅集中在感知、推理或规划的特定方面上。&lt;h4&gt;目的&lt;/h4&gt;为了充分利用MLLMs在自动驾驶系统中的潜力，该论文旨在提供一个结构化评价框架。&lt;h4&gt;方法&lt;/h4&gt;提出的框架以自动驾驶系统的通用需求、人类驾驶员的认知以及基于语言的推理为基础，将场景理解结构化为四个核心维度。此外，它还组织了上下文层、处理模式和下游任务（如基于语言的交互和决策）&lt;h4&gt;主要发现&lt;/h4&gt;通过分析两个示例交通场景，论文展示了框架在现实驾驶情况中的适用性，并表明该框架能够提供对MLLMs潜在理解和自动驾驶场景能力的基础。&lt;h4&gt;结论&lt;/h4&gt;提出的评估框架为多模态大型语言模型的能力驱动评价提供了基础结构，并强调了需要进一步研究以探索其在复杂自动驾驶环境下的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已被用于上述总结，无需额外翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) hold the potential to enhanceautonomous driving by combining domain-independent world knowledge withcontext-specific language guidance. Their integration into autonomous drivingsystems shows promising results in isolated proof-of-concept applications,while their performance is evaluated on selective singular aspects ofperception, reasoning, or planning. To leverage their full potential asystematic framework for evaluating MLLMs in the context of autonomous drivingis required. This paper proposes a holistic framework for a capability-drivenevaluation of MLLMs in autonomous driving. The framework structures scenariounderstanding along the four core capability dimensions semantic, spatial,temporal, and physical. They are derived from the general requirements ofautonomous driving systems, human driver cognition, and language-basedreasoning. It further organises the domain into context layers, processingmodalities, and downstream tasks such as language-based interaction anddecision-making. To illustrate the framework's applicability, two exemplarytraffic scenarios are analysed, grounding the proposed dimensions in realisticdriving situations. The framework provides a foundation for the structuredevaluation of MLLMs' potential for scenario understanding in autonomousdriving.</description>
      <author>example@mail.com (Tin Stribor Sohn, Philipp Reis, Maximilian Dillitzer, Johannes Bach, Jason J. Corso, Eric Sax)</author>
      <guid isPermaLink="false">2503.11400v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>BEVDiffLoc: End-to-End LiDAR Global Localization in BEV View based on Diffusion Model</title>
      <link>http://arxiv.org/abs/2503.11372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为BEVDiffLoc的新框架，用于自动驾驶中的激光雷达定位任务。该方法利用鸟瞰图（Bird's-Eye-View, BEV）图像来表示数据，并结合特定的数据增强、特征聚合模块和视觉变换器以及扩散模型技术，以提高鲁棒性和准确性。&lt;h4&gt;背景&lt;/h4&gt;当前的端到端定位方法虽然在系统架构上具有优势，但仍然面临稳定性与准确性的挑战。利用BEV图像作为数据表示的方法被广泛应用于自动驾驶领域，但由于研究不足，在BEV基础上的端到端定位解决方案还不够充分发展。&lt;h4&gt;目的&lt;/h4&gt;通过结合最新的深度学习技术改进现有定位算法，提高鲁棒性和准确性，并降低复杂性。&lt;h4&gt;方法&lt;/h4&gt;首先使用特定的数据增强技术来增加输入数据多样性；其次引入最大特征聚合模块和视觉变换器以保持对显著角度变化的鲁棒性；最后采用扩散模型迭代细化已学得的特性来恢复绝对姿态。&lt;h4&gt;主要发现&lt;/h4&gt;BEVDiffLoc在Oxford Radar RobotCar和NCLT数据集上表现出优越性能，优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;通过利用BEV图像的优势并引入创新技术如特定的数据增强、最大特征聚合模块、视觉变换器以及扩散模型，可以显著提升自动驾驶中的定位准确性与稳定性。这项工作填补了基于BEV的端到端定位研究领域的空白，并且展示了未来改进的方向。&lt;h4&gt;翻译&lt;/h4&gt;定位是现代机器人学的核心部分之一。经典的方法通常遵循检索然后注册的范式取得了相当大的成功。最近，端到端定位方法的出现提供了许多优点，包括简化系统架构和无需存储大量地图数据的需求。尽管这些方法已经显示出有希望的结果，但目前的端到端定位方法仍然在鲁棒性和准确性方面面临限制。鸟瞰图（BEV）图像是在自动驾驶中最为广泛采用的数据表示之一。它极大地减少了数据复杂性的同时保持了空间结构和尺度一致性，使其成为定位任务的理想表现形式。然而，在基于BEV的端到端定位研究上，现有的工作还不够充分。为填补这一空白，我们提出了一种新的框架：BEVDiffLoc，该框架将激光雷达定位视为姿态生成的条件问题。通过利用BEV特性，我们首先引入一种特定的数据增强方法以显著提升输入数据多样性；其次使用最大特征聚合模块和视觉变换器来学习鲁棒性特征并保持对显著角度变化的鲁棒性；最后采用扩散模型迭代细化已学得特性以恢复绝对姿态。在Oxford Radar RobotCar和NCLT数据集上的广泛实验表明，BEVDiffLoc优于基线方法。我们的代码可从https://github.com/nubot-nudt/BEVDiffLoc获得访问权限。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Localization is one of the core parts of modern robotics. Classiclocalization methods typically follow the retrieve-then-register paradigm,achieving remarkable success. Recently, the emergence of end-to-endlocalization approaches has offered distinct advantages, including astreamlined system architecture and the elimination of the need to storeextensive map data. Although these methods have demonstrated promising results,current end-to-end localization approaches still face limitations in robustnessand accuracy. Bird's-Eye-View (BEV) image is one of the most widely adopteddata representations in autonomous driving. It significantly reduces datacomplexity while preserving spatial structure and scale consistency, making itan ideal representation for localization tasks. However, research on BEV-basedend-to-end localization remains notably insufficient. To fill this gap, wepropose BEVDiffLoc, a novel framework that formulates LiDAR localization as aconditional generation of poses. Leveraging the properties of BEV, we firstintroduce a specific data augmentation method to significantly enhance thediversity of input data. Then, the Maximum Feature Aggregation Module andVision Transformer are employed to learn robust features while maintainingrobustness against significant rotational view variations. Finally, weincorporate a diffusion model that iteratively refines the learned features torecover the absolute pose. Extensive experiments on the Oxford Radar RobotCarand NCLT datasets demonstrate that BEVDiffLoc outperforms the baseline methods.Our code is available at https://github.com/nubot-nudt/BEVDiffLoc.</description>
      <author>example@mail.com (Ziyue Wang, Chenghao Shi, Neng Wang, Qinghua Yu, Xieyuanli Chen, Huimin Lu)</author>
      <guid isPermaLink="false">2503.11372v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Hand Palm Motion Gesture Recognition by Eliminating Reference Frame Bias via Frame-Invariant Similarity Measures</title>
      <link>http://arxiv.org/abs/2503.11352v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, this work has been submitted as a conference  paper for consideration in the 2025 IEEE International Conference on  Automation Science and Engineering (CASE), the content in this preprint is  identical to the version submitted for peer review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了在参考框架变化情况下使用不变轨迹描述符进行手部掌部动作手势识别的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;大多数手势识别工作仍依赖于特定参考框架的表现形式，这在不同作业单元布局、不精确的帧校准或环境改变的情况下带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;研究提出了一种新型的手掌运动（HPM）数据集和多个不变轨迹描述符方法来评估它们如何推广到新的HPM数据集中，并验证了最佳得分的方法在线识别中的应用。&lt;h4&gt;方法&lt;/h4&gt;引入了一个记录有手部掌部动作手势的全新数据集，这些手势设计为在特定参考框架或方向提示下也能被区分。研究团队对多个不变轨迹描述符进行基准测试，以评估它们如何适用于新型HPM数据集，并开发了一种实时的概念验证（PoC）来展示最佳方法的应用。&lt;h4&gt;主要发现&lt;/h4&gt;该概念验证通过使用手部掌部动作手势来控制机械臂的实时运动，展示了高达92.3% $F_1$-score的手势识别可靠性。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明不变描述符方法作为独立解决方案的有效性，并且可以将其应用到其他最先进的模式识别和学习系统中以提高其抵抗参考框架变化的能力。&lt;h4&gt;翻译&lt;/h4&gt;机器人的手部动作识别能力促进了自然和易于实现的人机协作。然而，大部分的手势识别工作依赖于特定的参考框架表示形式，这在遇到不同作业单元布局、不精确的帧校准或环境改变时带来了挑战。这项研究探讨了在参考框架变化情况下使用不变轨迹描述符进行手部掌部动作手势识别的鲁棒性，并引入了一个记录有手部掌部动作手势的新数据集和多个不变轨迹描述符方法来评估它们如何推广到新型HPM数据集中。之后，最佳得分的方法在线上被验证并通过开发实时概念验证展示了其应用价值，该方案在控制机械臂时达到了92.3%的$F_1$-score识别可靠性。这项工作展示了不变描述符方法作为独立解决方案的有效性，并且可以将其用作其他先进模式识别和学习系统的增强工具以提高抵抗参考框架变化的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The ability of robots to recognize human gestures facilitates a natural andaccessible human-robot collaboration. However, most work in gesture recognitionremains rooted in reference frame-dependent representations. This poses achallenge when reference frames vary due to different work cell layouts,imprecise frame calibrations, or other environmental changes. This paperinvestigated the use of invariant trajectory descriptors for robust hand palmmotion gesture recognition under reference frame changes. First, a noveldataset of recorded Hand Palm Motion (HPM) gestures is introduced. The motiongestures in this dataset were specifically designed to be distinguishablewithout dependence on specific reference frames or directional cues.Afterwards, multiple invariant trajectory descriptor approaches werebenchmarked to assess how their performances generalize to this novel HPMdataset. After this offline benchmarking, the best scoring approach isvalidated for online recognition by developing a real-time Proof of Concept(PoC). In this PoC, hand palm motion gestures were used to control thereal-time movement of a manipulator arm. The PoC demonstrated a highrecognition reliability in real-time operation, achieving an $F_1$-score of92.3%. This work demonstrates the effectiveness of the invariant descriptorapproach as a standalone solution. Moreover, we believe that the invariantdescriptor approach can also be utilized within other state-of-the-art patternrecognition and learning systems to improve their robustness against referenceframe variations.</description>
      <author>example@mail.com (Arno Verduyn, Maxim Vochten, Joris De Schutter)</author>
      <guid isPermaLink="false">2503.11352v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Six-DoF Stewart Platform Motion Simulator Control using Switchable Model Predictive Control</title>
      <link>http://arxiv.org/abs/2503.11300v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于模型预测控制（MPC）的运动仿真算法，以解决传统滤波器在复杂飞行状态下的局限性，并通过切换至不带终端约束提取误差补偿的S-MPC，在飞行模拟器的操作范围内实现了高精度跟踪。&lt;h4&gt;背景&lt;/h4&gt;六自由度Stewart平台因其刚性和机动性强、质量比高等特性被广泛应用于飞行员训练中的飞行模拟。然而，当遇到复杂的飞行状态时，传统基于经典清洗滤波器（CWF）的运动仿真算法在快速响应和准确性方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;旨在探索一种新的MPC基运动仿真算法，并通过自适应模型架构下的切换式模型预测控制(S-MPC)方法来解决传统方法中存在的不确定性问题，以满足更高的性能要求。&lt;h4&gt;方法&lt;/h4&gt;该文采用基于MPC的运动仿真算法，并针对提取终端约束（COTC）时出现的不确定性和解误差，提出了一种新的S-MPC策略。通过实验验证了该算法在模拟器操作范围内可以实现高精度跟踪。&lt;h4&gt;主要发现&lt;/h4&gt;提出的S-MPC算法相较于传统的MPC和基于SWF的方法，在水平失速条件下的性能分别提高了42.34%和65.30%，依据平均绝对尺度（AAS）评估标准。&lt;h4&gt;结论&lt;/h4&gt;证明了在复杂飞行训练中，特别是对于UPRT操作，提出的S-MPC方法比传统算法更有效。通过适当的模型预测控制可以提高模拟器的操作性能并满足高精度的要求。&lt;h4&gt;翻译&lt;/h4&gt;由于六自由度Stewart结构具有优良的刚性、机动性和质量与强度比例特性，它被广泛用于构建飞行模拟平台以在飞行员训练中重现运动感觉。不同于传统的串行链机械臂机制，在复杂的飞行状态下，UPRT通常伴随着大速度和角速度的变化。然而，基于经典清洗滤波器（CWF）的运动仿真算法显示出难以快速响应驱动电机来满足高精度性能需求的局限性。本文旨在探索一种在六足模拟器中通过控制有限线性工作空间被证明有效的基于模型预测控制（MPC）的运动仿真算法。鉴于从终端约束提取的不确定性和控制器解误差，本文提出了一种基于自适应模型架构下的切换式模型预测控制(S-MPC)方法以减少解决方案中的不确定性与不准确性。验证了使用带有COTC的基于MPC的方法在模拟器操作范围内可以实现高精度跟踪；提出了当超出操作范围时通过切换至不含COTC的MPC来提供最优跟踪解的方法。通过根据平均绝对尺度(AAS)评估标准展示UPRT中的水平失速条件，所提出的S-MPC方法相比MPC和SWF基于运动仿真算法分别提高了42.34% 和65.30%的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to excellent mechanism characteristics of high rigidity, maneuverabilityand strength-to-weight ratio, 6 Degree-of-Freedom (DoF) Stewart structure iswidely adopted to construct flight simulator platforms for replicating motionfeelings during training pilots. Unlike conventional serial link manipulatorbased mechanisms, Upset Prevention and Recovery Training (UPRT) in complexflight status is often accompanied by large speed and violent rate of change inangular velocity of the simulator. However, Classical Washout Filter (CWF)based Motion Cueing Algorithm (MCA) shows limitations in providing rapidresponse to drive motors to satisfy high accuracy performance requirements.This paper aims at exploiting Model Predictive Control (MPC) based MCA which isproved to be efficient in Hexapod-based motion simulators through controllingover limited linear workspace. With respect to uncertainties and controlsolution errors from the extraction of Terminal Constraints (COTC), this paperproposes a Switchable Model Predictive Control (S-MPC) based MCA under modeladaptive architecture to mitigate the solution uncertainties and inaccuracies.It is verified that high accurate tracking is achievable using the MPC-basedMCA with COTC within the simulator operating envelope. The proposed methodprovides optimal tracking solutions by switching to MPC based MCA without COTCoutside the operating envelope. By demonstrating the UPRT with horizontal stallconditions following Average Absolute Scale(AAS) evaluation criteria, theproposed S-MPC based MCA outperforms MPC based MCA and SWF based MCA by 42.34%and 65.30%, respectively.</description>
      <author>example@mail.com (Jiangwei Zhao, Zhengjia Xu, Dongsu Wu, Yingrui Cao, Jinpeng Xie)</author>
      <guid isPermaLink="false">2503.11300v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Prof. Robot: Differentiable Robot Rendering Without Static and Self-Collisions</title>
      <link>http://arxiv.org/abs/2503.11269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的改进方法，通过学习神经机器人碰撞分类器来增强可微渲染在机器人行动优化中的物理世界感知能力。&lt;h4&gt;背景&lt;/h4&gt;虽然可微机器人渲染已成为从图像空间监督中学习机器人动作的有效范式，但缺乏对物理世界的感知可能导致优化过程中出现潜在的碰撞问题。&lt;h4&gt;目的&lt;/h4&gt;通过引入神经机器人碰撞分类器来提高现有方法在避免与静态非交互环境和机器人自身发生碰撞时的能力。&lt;h4&gt;方法&lt;/h4&gt;为了使分类器能够有效地进行梯度优化，研究团队解决了潜在的问题，并提出利用Eikonal正则化来确保一致的梯度优化。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的解决方案可以无缝地集成到现有的可微分机器人渲染框架中，并能提供更可靠与物理世界互动的基础。实验结果表明了该方法的有效性和必要性。&lt;h4&gt;结论&lt;/h4&gt;通过结合Eikonal正则化和神经网络技术，该研究为未来利用可微渲染进行复杂机器人动作优化提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;在机器人领域中，可微渲染（Differentiable rendering）已经获得了广泛关注。尽管它可以通过图像空间监督学习到有效的机器人行为，但缺乏对物理世界的感知可能会导致执行时出现碰撞。本工作引入了一种新的方法，通过机器人的神经碰撞分类器的学习来增加物理环境的感知能力，这使得可以优化避免与静态、非交互式环境以及机器人自身发生碰撞的动作。为了使该分类器能够有效地进行梯度优化，我们发现并解决了潜在问题，并提出了使用Eikonal正则化来确保一致性的梯度优化。这种方法可以无缝地集成到现有的可微分机器人渲染框架中，利用梯度进行优化，并为未来在物理世界中的交互提供了坚实的基础。实验结果表明了该方法的必要性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Differentiable rendering has gained significant attention in the field ofrobotics, with differentiable robot rendering emerging as an effective paradigmfor learning robotic actions from image-space supervision. However, the lack ofphysical world perception in this approach may lead to potential collisionsduring action optimization. In this work, we introduce a novel improvement onprevious efforts by incorporating physical awareness of collisions through thelearning of a neural robotic collision classifier. This enables theoptimization of actions that avoid collisions with static, non-interactableenvironments as well as the robot itself. To facilitate effective gradientoptimization with the classifier, we identify the underlying issue and proposeleveraging Eikonal regularization to ensure consistent gradients foroptimization. Our solution can be seamlessly integrated into existingdifferentiable robot rendering frameworks, utilizing gradients for optimizationand providing a foundation for future applications of differentiable renderingin robotics with improved reliability of interactions with the physical world.Both qualitative and quantitative experiments demonstrate the necessity andeffectiveness of our method compared to previous solutions.</description>
      <author>example@mail.com (Quanyuan Ruan, Jiabao Lei, Wenhao Yuan, Yanglin Zhang, Dekun Lu, Guiliang Liu, Kui Jia)</author>
      <guid isPermaLink="false">2503.11269v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Ergodic exploration of dynamic distribution</title>
      <link>http://arxiv.org/abs/2503.11235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Initial version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;翻译&lt;/h4&gt;这项研究解决在动态环境中执行搜索任务的挑战，特别是针对由流场支配移动的漂移目标。这是通过一个动力学系统实现的，该系统整合了两个偏微分方程：一个是控制概率分布的动力学和不确定性，另一个是管理遍历多机器人搜索的势能场。目标概率场根据环境施加的目标动态变化，并由多个机器人代理进行探索，这些机器人代理受到势能梯度的引导。提出的方法在两种模拟搜索场景中进行了测试，其中一个展示了与静态目标概率基准方法相比，在不同机器人类和流场速度比的情况下具有更好的性能。第二种搜索情景代表了一个实际的海上搜救任务，其中搜索开始被延迟，并且搜索是在多个机器人飞行任务中进行的，还演示了针对目标漂移不确定性的补偿程序。&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种利用动力学系统解决动态环境中多机器人搜索问题的方法。&lt;h4&gt;背景&lt;/h4&gt;在流场影响下的动态环境中的目标搜索是一个具有挑战性的问题，尤其是当这些目标是随水流移动的漂流物时。现有的方法通常依赖于静态目标概率分布，这限制了它们在实际应用中的有效性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的动力学系统模型来解决动态环境中多机器人搜索问题，并通过模拟场景验证其性能和适用性。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一种结合两个偏微分方程的模型：一个描述概率分布的动力学和不确定性，另一个调节势能场。多个机器人根据势能梯度引导进行探索，并随着环境对目标动态的影响以及已完成的感测努力来演化目标概率场。&lt;h4&gt;主要发现&lt;/h4&gt;与静态概率基准方法相比，新方法在模拟搜索场景中展示了更好的性能，特别是在不同速度比的情况下；同时，在延迟开始的实际海上搜救任务中展示了补偿漂移不确定性的程序。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法不仅为动态环境下的多机器人搜索提供了一种有效的解决方案，并且还提供了一个基于已知感测参数的精确调查完成指标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This research addresses the challenge of performing search missions indynamic environments, particularly for drifting targets whose movement isdictated by a flow field. This is accomplished through a dynamical system thatintegrates two partial differential equations: one governing the dynamics anduncertainty of the probability distribution, and the other regulating thepotential field for ergodic multi-agent search. The target probability fieldevolves in response to the target dynamics imposed by the environment andaccomplished sensing efforts, while being explored by multiple robot agentsguided by the potential field gradient. The proposed methodology was tested ontwo simulated search scenarios, one of which features a synthetically generateddomain and showcases better performance when compared to the baseline methodwith static target probability over a range of agent to flow field velocityratios. The second search scenario represents a realistic sea search and rescuemission where the search start is delayed, the search is performed in multiplerobot flight missions, and the procedure for target drift uncertaintycompensation is demonstrated. Furthermore, the proposed method provides anaccurate survey completion metric, based on the known detection/sensingparameters, that correlates with the actual number of targets foundindependently.</description>
      <author>example@mail.com (Luka Lanča, Karlo Jakac, Sylvain Calinon, Stefan Ivić)</author>
      <guid isPermaLink="false">2503.11235v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>GAIPAT -Dataset on Human Gaze and Actions for Intent Prediction in Assembly Tasks</title>
      <link>http://arxiv.org/abs/2503.11186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过模拟工业装配任务，收集了约80名参与者的目光和动作数据，旨在探讨人类与协作机器人在共享工作环境中互动的效率和安全问题。&lt;h4&gt;背景&lt;/h4&gt;当前迫切需要更好地理解人类行为、视线方向以及它们如何影响人类与协作机器人的交互方式。&lt;h4&gt;目的&lt;/h4&gt;为了显著提高人机协同工作的效率和安全性，并通过分析眼神模式与物理动作之间的联系，提供关于认知过程及注意力动态的重要见解。&lt;h4&gt;方法&lt;/h4&gt;使用教育积木块进行模拟工业装配任务，在受控场景中记录参与者在坐姿和站姿下利用两种不同的眼动追踪设备（头戴式和远程）收集的数据。&lt;h4&gt;主要发现&lt;/h4&gt;通过链接注视模式与物理行为，研究提供了一种理解注意力动态及认知过程的新视角，特别是在执行组装任务时。&lt;h4&gt;结论&lt;/h4&gt;该数据集为未来关于人类-协作机器人交互的研究提供了宝贵的资源，并为进一步探索人机合作的效率和安全问题铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The primary objective of the dataset is to provide a better understanding ofthe coupling between human actions and gaze in a shared working environmentwith a cobot, with the aim of signifcantly enhancing the effciency and safetyof humancobot interactions. More broadly, by linking gaze patterns withphysical actions, the dataset offers valuable insights into cognitive processesand attention dynamics in the context of assembly tasks. The proposed datasetcontains gaze and action data from approximately 80 participants, recordedduring simulated industrial assembly tasks. The tasks were simulated usingcontrolled scenarios in which participants manipulated educational buildingblocks. Gaze data was collected using two different eye-tracking setups-head-mounted and remote-while participants worked in two positions: sittingand standing.</description>
      <author>example@mail.com (Maxence Grand, Damien Pellier, Francis Jambon)</author>
      <guid isPermaLink="false">2503.11186v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Hand Over or Place On The Table? A Study On Robotic Object Delivery When The Recipient Is Occupied</title>
      <link>http://arxiv.org/abs/2503.11177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了用户在参与另一项任务时，在两种机器人对象递送方式（直接交接和桌面放置）中的主观体验。&lt;h4&gt;背景&lt;/h4&gt;当前对于不同机器人递送方法对用户体验的影响尚缺乏系统的研究，特别是在用户忙于其他任务的情境下。&lt;h4&gt;目的&lt;/h4&gt;评估用户在进行一项占用手部活动的任务时，使用机器人通过直接交接或桌面放置方式交付物品的主观体验差异。&lt;h4&gt;方法&lt;/h4&gt;研究采用了一项涉及15名参与者的用户测试，在参与者正在玩打字游戏的同时递送物品，并收集他们对不同递送方式的感受和偏好。&lt;h4&gt;主要发现&lt;/h4&gt;{'用户体验': '与直接交接相比，桌面放置显著提高了用户的满意度、感知安全性和直观性。', '任务性能': '直接交接影响了参与者的打字表现。', '用户偏好': '所有参与者都明确表示更喜欢桌面放置作为递送方法。'}&lt;h4&gt;结论&lt;/h4&gt;研究表明，在需要尽量减少对用户干扰的情况下，桌面放置是一种比直接交接更优的机器人物品递送方式。&lt;h4&gt;翻译&lt;/h4&gt;这项研究考察了当用户忙于其他任务时，两种不同的机器人物体传递模式（直接交付和台面摆放）给用户的主观感受。实验结果显示，与直接交付相比，将物体放在桌子上能显著提升用户体验，在满意度、安全性感知及直观性方面表现更佳；同时发现直接递送还会影响正在进行的任务性能；此外，所有参与者都倾向于选择桌面放置作为物体传递方式。结果强调了在需要尽可能减少用户干扰的情况下，使用台面摆放作为物品交付方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the subjective experiences of users in two roboticobject delivery methods: direct handover and table placement, when users areoccupied with another task. A user study involving 15 participants engaged in atyping game revealed that table placement significantly enhances userexperience compared to direct handovers, particularly in terms of satisfaction,perceived safety and intuitiveness. Additionally, handovers negatively impactedtyping performance, while all participants expressed a clear preference fortable placement as the delivery method. These findings highlight the advantagesof table placement in scenarios requiring minimal user disruption.</description>
      <author>example@mail.com (Thieu Long Phan, Akansel Cosgun)</author>
      <guid isPermaLink="false">2503.11177v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>A Benchmarking Study of Vision-based Robotic Grasping Algorithms</title>
      <link>http://arxiv.org/abs/2503.11163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to The IEEE Robotics and Automation Magazine&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文通过基准测试研究了基于视觉的机器人抓取算法，并提供了不同实验条件下的对比分析。&lt;h4&gt;背景&lt;/h4&gt;当前存在多种不同的基于机器学习和分析方法的视觉抓取算法，但它们在各种实际条件下性能如何尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;评估几种现有视觉抓取算法在不同光照、背景纹理、相机噪声水平及夹具类型等实验条件下的表现，并探究其优劣。&lt;h4&gt;方法&lt;/h4&gt;使用文献中的现有基准测试协议对两种机器学习和两种分析型算法进行了比较。同时，还在模拟环境中以及真实机器人上进行了一系列相似的实验，并分析了不同实验室条件下结果的一致性。总共进行了5040个实验。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了视觉抓取算法在实际操作中的表现与理论预期之间的差距及所面临的挑战，强调系统化实验的重要性。&lt;h4&gt;结论&lt;/h4&gt;这项研究提供了关于机器人操纵中系统性实验作用和挑战的重要见解，并为新的算法开发提供指导。所有实验记录及基准测试软件均已公开。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了基于视觉的机器人抓取算法的基准测试研究，并进行了对比分析，具体比较了两种机器学习型和两种分析型算法，在现有文献中的基准测试协议下确定了不同实验条件下各种算法的优势与不足，这些条件包括光照变化、背景纹理差异以及具有不同噪声级别的相机和夹具。我们还在模拟环境中及实际机器人上运行类似实验，并展示了它们之间的差异性。同时在两个不同的实验室中使用相同的协议进行了部分试验以进一步分析结果的可重复性。我们认为这项包含5040个实验的研究，提供了关于系统化实验对于机器人操作的重要性以及所面临的挑战的重要见解，并指导新的算法开发考虑可能影响性能的因素。所有实验记录和基准测试软件均公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a benchmarking study of vision-based robotic grasping algorithmswith distinct approaches, and provide a comparative analysis. In particular, wecompare two machine-learning-based and two analytical algorithms using anexisting benchmarking protocol from the literature and determine thealgorithm's strengths and weaknesses under different experimental conditions.These conditions include variations in lighting, background textures, cameraswith different noise levels, and grippers. We also run analogous experiments insimulations and with real robots and present the discrepancies. Someexperiments are also run in two different laboratories using same protocols tofurther analyze the repeatability of our results. We believe that this study,comprising 5040 experiments, provides important insights into the role andchallenges of systematic experimentation in robotic manipulation, and guidesthe development of new algorithms by considering the factors that could impactthe performance. The experiment recordings and our benchmarking software arepublicly available.</description>
      <author>example@mail.com (Bharath K Rameshbabu, Sumukh S Balakrishna, Brian Flynn, Vinarak Kapoor, Adam Norton, Holly Yanco, Berk Calli)</author>
      <guid isPermaLink="false">2503.11163v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>Flow-Aware Navigation of Magnetic Micro-Robots in Complex Fluids via PINN-Based Prediction</title>
      <link>http://arxiv.org/abs/2503.11124v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的磁性微机器人导航和控制策略，这种策略考虑了流体流动对它们运动的影响。&lt;h4&gt;背景&lt;/h4&gt;磁性微机器人的应用包括药物递送和显微外科手术等，但精确导航和控制在复杂流体环境中的问题仍然存在。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的流量感知导航和控制系统，提高磁性微机器人在生物体内实施的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;[{'Physics-Informed U-Net (PI-UNet)': '用于基于本地观测来精炼数值预测的流体速度'}, {'A*路径规划算法': '将预测的速度整合以确保有效的导航并减少流动引起的干扰'}, {'控制方案': '开发一种补偿预测流速的方法，优化微机器人的性能'}]&lt;h4&gt;主要发现&lt;/h4&gt;['所提出方法通过模拟研究和真实世界的实验验证了其有效性和在受流体影响环境中的应用潜力', '该策略提高了规划的准确性以及控制精度']&lt;h4&gt;结论&lt;/h4&gt;新的导航和控制系统扩展了磁性微机器人在医学场景中潜在的应用范围。&lt;h4&gt;翻译&lt;/h4&gt;摘要：虽然磁性微型机器人的广泛应用（包括药物递送和显微手术）已经展示了其巨大的潜力，但在复杂流体环境中精确导航和控制的问题对于生物体内实施至关重要。本文介绍了一种新的基于流量感知的导航和控制系统策略，该系统明确考虑了流体流动对它们运动的影响。首先，所提出的方法采用Physics-Informed U-Net（PI-UNet）来利用局部观测精炼数值预测出的速度；其次，该速度被整合到一个基于流量感知的A*路径规划算法中，确保有效的导航同时减少由流动引起的干扰；最后，开发了一种补偿策略以优化微机器人的性能。通过一系列模拟研究和现实世界的实验验证了所提出方法的有效性。这种方法提高了规划精度与控制精度，在流体影响环境（许多医学场景中的典型情况）的应用潜力被进一步扩展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While magnetic micro-robots have demonstrated significant potential acrossvarious applications, including drug delivery and microsurgery, the open issueof precise navigation and control in complex fluid environments is crucial forin vivo implementation. This paper introduces a novel flow-aware navigation andcontrol strategy for magnetic micro-robots that explicitly accounts for theimpact of fluid flow on their movement. First, the proposed method employs aPhysics-Informed U-Net (PI-UNet) to refine the numerically predicted fluidvelocity using local observations. Then, the predicted velocity is incorporatedin a flow-aware A* path planning algorithm, ensuring efficient navigation whilemitigating flow-induced disturbances. Finally, a control scheme is developed tocompensate for the predicted fluid velocity, thereby optimizing themicro-robot's performance. A series of simulation studies and real-worldexperiments are conducted to validate the efficacy of the proposed approach.This method enhances both planning accuracy and control precision, expandingthe potential applications of magnetic micro-robots in fluid-affectedenvironments typical of many medical scenarios.</description>
      <author>example@mail.com (Yongyi Jia, Shu Miao, Jiayu Wu, Ming Yang, Chengzhi Hu, Xiang Li)</author>
      <guid isPermaLink="false">2503.11124v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Decomposed Feature-Oriented Framework for Open-Set Semantic Segmentation on LiDAR Data</title>
      <link>http://arxiv.org/abs/2503.11097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by 2025 ICRA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种面向特征的框架，用于基于LiDAR数据的开放集语义分割，能够识别未知物体的同时保持对已知物体分类的能力。&lt;h4&gt;背景&lt;/h4&gt;当前大多数研究集中在分割已知对象上，忽略了现实应用中常见的未知类别的识别问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个有效的特征驱动的LiDAR开放集语义分割框架，以应对现有技术无法处理未知类别的问题。&lt;h4&gt;方法&lt;/h4&gt;{'网络设计': '设计了一个分解式双解码器网络，该网络同时执行封闭集语义分割和生成已知及未知对象的独特特性。', '训练过程': '使用多目标损失函数进行训练，以便捕捉已知与未知物体的特征。', '异常检测机制': '利用提取的特征引入一种异常检测机制来识别未知物体。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能表现': '在SemanticKITTI和nuScenes数据集上的评估表明所提出的框架显著优于现有最佳方法。', '未来工作': '提出了一个可公开访问的源代码库，以促进进一步的研究和发展。'}&lt;h4&gt;结论&lt;/h4&gt;本文提供了一种新颖的方法来解决移动机器人语义分割中的开放集问题，并通过实验证明了其优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation is a key technique that enables mobile robots tounderstand and navigate surrounding environments autonomously. However, mostexisting works focus on segmenting known objects, overlooking theidentification of unknown classes, which is common in real-world applications.In this paper, we propose a feature-oriented framework for open-set semanticsegmentation on LiDAR data, capable of identifying unknown objects whileretaining the ability to classify known ones. We design a decomposeddual-decoder network to simultaneously perform closed-set semantic segmentationand generate distinctive features for unknown objects. The network is trainedwith multi-objective loss functions to capture the characteristics of known andunknown objects. Using the extracted features, we introduce an anomalydetection mechanism to identify unknown objects. By integrating the results ofclose-set semantic segmentation and anomaly detection, we achieve effectivefeature-driven LiDAR open-set semantic segmentation. Evaluations on bothSemanticKITTI and nuScenes datasets demonstrate that our proposed frameworksignificantly outperforms state-of-the-art methods. The source code will bemade publicly available at https://github.com/nubot-nudt/DOSS.</description>
      <author>example@mail.com (Wenbang Deng, Xieyuanli Chen, Qinghua Yu, Yunze He, Junhao Xiao, Huimin Lu)</author>
      <guid isPermaLink="false">2503.11097v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for Visual Spatial Tasks</title>
      <link>http://arxiv.org/abs/2503.11089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种新的框架EmbodiedVSR，旨在通过动态场景图引导的Chain-of-Thought推理来增强实体智能代理的空间理解能力。&lt;h4&gt;背景&lt;/h4&gt;多模态大型语言模型在实体智能方面取得了突破性进展，但在复杂长时任务中的空间推理上仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;引入EmbodiedVSR框架以解决现有模型在处理复杂环境交互和长期目标规划方面的不足。&lt;h4&gt;方法&lt;/h4&gt;该框架通过动态场景图构建结构化知识表示，使实体代理能够零样本地进行空间推理，并且不需特定任务的微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，EmbodiedVSR框架比现有的多模态大型语言模型在准确性和推理连贯性方面表现更佳，特别是在需要迭代环境互动的长时任务中。&lt;h4&gt;结论&lt;/h4&gt;这种方法揭示了通过结构化、可解释的推理机制装备后的多模态大型语言模型，在实体智能领域的未开发潜力，并为实际空间应用提供了可靠的部署路径。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容主要介绍了EmbodiedVSR框架，旨在提高多模态大语言模型在复杂长时任务中的空间理解能力。该方法通过动态场景图来构建结构化的知识表示，使得零样本学习的空间推理成为可能。实验显示，在需要迭代环境互动的长期任务中，EmbodiedVSR的表现显著优于现有的多模态大型语言模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While multimodal large language models (MLLMs) have made groundbreakingprogress in embodied intelligence, they still face significant challenges inspatial reasoning for complex long-horizon tasks. To address this gap, wepropose EmbodiedVSR (Embodied Visual Spatial Reasoning), a novel framework thatintegrates dynamic scene graph-guided Chain-of-Thought (CoT) reasoning toenhance spatial understanding for embodied agents. By explicitly constructingstructured knowledge representations through dynamic scene graphs, our methodenables zero-shot spatial reasoning without task-specific fine-tuning. Thisapproach not only disentangles intricate spatial relationships but also alignsreasoning steps with actionable environmental dynamics. To rigorously evaluateperformance, we introduce the eSpatial-Benchmark, a comprehensive datasetincluding real-world embodied scenarios with fine-grained spatial annotationsand adaptive task difficulty levels. Experiments demonstrate that our frameworksignificantly outperforms existing MLLM-based methods in accuracy and reasoningcoherence, particularly in long-horizon tasks requiring iterative environmentinteraction. The results reveal the untapped potential of MLLMs for embodiedintelligence when equipped with structured, explainable reasoning mechanisms,paving the way for more reliable deployment in real-world spatial applications.The codes and datasets will be released soon.</description>
      <author>example@mail.com (Yi Zhang, Qiang Zhang, Xiaozhu Ju, Zhaoyang Liu, Jilei Mao, Jingkai Sun, Jintao Wu, Shixiong Gao, Shihan Cai, Zhiyuan Qin, Linkai Liang, Jiaxu Wang, Yiqun Duan, Jiahang Cao, Renjing Xu, Jian Tang)</author>
      <guid isPermaLink="false">2503.11089v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>GP-enhanced Autonomous Drifting Framework using ADMM-based iLQR</title>
      <link>http://arxiv.org/abs/2503.11083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'背景': '自主漂移是一项复杂的挑战，由于其高度非线性的动力学特性和对精确实时控制的需求，尤其是在不确定的环境中。', '目的': '提出一种层次化控制框架，以解决模型不准确和实时控制中的计算难题。', '方法': '该框架结合了高斯过程（GP）回归与基于交替方向乘子法（ADMM）的迭代线性二次调节器（iLQR），利用GP回归补偿模型残差，提高动态条件下的准确性；同时利用ADMM将问题分解为更简单的子问题。', '主要发现': '通过模拟实验验证了所提出框架的有效性，在轨迹跟踪和计算效率方面均有显著改进。与内点法优化器（IPOPT）相比，平均计算时间降低了75%，RMSE横向误差减少了38%。', '结论': '提出的层次化控制方法能够有效解决自主漂移中的模型不准确性和实时计算挑战问题。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous drifting is a complex challenge due to the highly nonlineardynamics and the need for precise real-time control, especially in uncertainenvironments. To address these limitations, this paper presents a hierarchicalcontrol framework for autonomous vehicles drifting along general paths,primarily focusing on addressing model inaccuracies and mitigatingcomputational challenges in real-time control. The framework integratesGaussian Process (GP) regression with an Alternating Direction Method ofMultipliers (ADMM)-based iterative Linear Quadratic Regulator (iLQR). GPregression effectively compensates for model residuals, improving accuracy indynamic conditions. ADMM-based iLQR not only combines the rapid trajectoryoptimization of iLQR but also utilizes ADMM's strength in decomposing theproblem into simpler sub-problems. Simulation results demonstrate theeffectiveness of the proposed framework, with significant improvements in bothdrift trajectory tracking and computational efficiency. Our approach resultedin a 38$\%$ reduction in RMSE lateral error and achieved an average computationtime that is 75$\%$ lower than that of the Interior Point OPTimizer (IPOPT).</description>
      <author>example@mail.com (Yangyang Xie, Cheng Hu, Nicolas Baumann, Edoardo Ghignone, Michele Magno, Lei Xie)</author>
      <guid isPermaLink="false">2503.11083v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>A High-Speed Time-Optimal Trajectory Generation Strategy via a Two-layer Planning Model</title>
      <link>http://arxiv.org/abs/2503.11072v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;运动规划与轨迹生成在无人机、机械臂和火箭控制等领域至关重要，但基于优化的实时路径规划由于问题可能存在的非凸性以及非线性编程算法固有局限而变得日益具挑战性。&lt;h4&gt;背景&lt;/h4&gt;运动规划与轨迹生成技术对无人飞行器（UAV）、机械手及火箭控制系统十分重要。然而，基于优化的实时路径规划因可能出现的问题非凸性和非线性程序算法本身的限制而愈发具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种两层优化算法以解决2D车辆运动规划中的困难问题，并为轨迹优化提供实时保证。&lt;h4&gt;方法&lt;/h4&gt;通过动态重构小时间跨度上的凸编程子问题，将原始问题分解成一系列固定结束时间的计划周期（规划循环），每个规划循环在由定制搜索算法确定的一系列受限凸集中逐步求解。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的算法具有计算速度快、任务耗时低等优势。通过适度前提条件下的数学证明及实验结果展示了这些优点。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种有效的两层优化策略，旨在提高实时路径规划的效率和可行性。&lt;h4&gt;翻译&lt;/h4&gt;运动规划与轨迹生成在无人机（UAV）、机械手以及火箭控制等领域至关重要。然而，基于优化的实时路径规划由于问题可能存在的非凸性及非线性编程算法本身的限制而变得愈发具有挑战性。为了应对这些问题，本文提出了一种针对2D车辆的两层优化算法，通过动态重构小时间跨度上的凸编程子问题来提供轨迹优化的实时保证。该方法将原始问题分解成一系列固定结束时间的小时间跨度计划周期（规划循环），并在由定制搜索算法确定的一系列受限凸集中逐步求解每个规划循环。本研究提出的方法具有计算速度快、任务耗时低等优势，这些优点通过适度前提条件下的数学证明及实验结果得到了验证和展示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion planning and trajectory generation are crucial technologies in variousdomains including the control of Unmanned Aerial Vehicles (UAV), manipulators,and rockets. However, optimization-based real-time motion planning becomesincreasingly challenging due to the problem's probable non-convexity and theinherent limitations of Non-Linear Programming algorithms. Highly nonlineardynamics, obstacle avoidance constraints, and non-convex inputs can exacerbatethese difficulties. To address these hurdles, this paper proposes a two-layeroptimization algorithm for 2D vehicles by dynamically reformulating small timehorizon convex programming subproblems, aiming to provide real-time guaranteesfor trajectory optimization. Our approach involves breaking down the originalproblem into small horizon-based planning cycles with fixed final times,referred to as planning cycles. Each planning cycle is then solved within aseries of restricted convex sets identified by our customized search algorithmsincrementally. The key benefits of our proposed algorithm include fastcomputation speeds and lower task time. We demonstrate these advantages throughmathematical proofs under some moderate preconditions and experimental results.</description>
      <author>example@mail.com (Haotian Tan, Yuan-Hua Ni)</author>
      <guid isPermaLink="false">2503.11072v1</guid>
      <pubDate>Mon, 17 Mar 2025 17:06:06 +0800</pubDate>
    </item>
    <item>
      <title>MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields</title>
      <link>http://arxiv.org/abs/2502.14401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code and Dataset: https://github.com/pfriedri/medfuncta&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了MedFuncta，一种基于神经场的医学图像连续数据表示方法。&lt;h4&gt;背景&lt;/h4&gt;现有的医疗影像分析研究主要集中在网格或体素数据表示上。&lt;h4&gt;目的&lt;/h4&gt;挑战现有选择，引入了一种模态无关的、基于神经网络的数据表征方式，并展示了如何通过利用医疗信号中的冗余信息以及采用高效的元学习方案扩展这种神经场模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种ω0调度策略以解决常用的SIREN激活函数中谱偏见问题，并改善了重建质量和收敛速度。验证了该方法在不同维度和模态的医学信号上的有效性，包括1D（心电图ECG）、2D（胸部X光片、视网膜OCT等）以及3D图像。&lt;h4&gt;主要发现&lt;/h4&gt;通过应用神经场模型能够有效解决一系列下游任务，并成功展示了这种方法的有效性和潜力。&lt;h4&gt;结论&lt;/h4&gt;验证了基于神经网络的数据表示方法在医学影像分析中的有效性，同时发布了超过55万张标注的神经场大规模数据集以促进相关研究进展。&lt;h4&gt;翻译&lt;/h4&gt;最近关于医疗图像分析与深度学习的研究几乎完全聚焦于网格或体素基元数据表达方式上。通过引入一种基于神经网络、模态无关且连续的数据表征——MedFuncta，我们挑战了这一普遍选择。展示如何在大型数据集上扩展神经场模型，同时利用医学信号中的冗余信息，并采用高效的元学习方案结合上下文减少策略。此外，还提出了一种ω0调度策略以解决常用SIREN激活函数的谱偏见问题，提高了重建质量和收敛速度。验证了该方法在不同维度（1D：心电图ECG；2D：胸部X光片、视网膜OCT等；3D：脑部MRI、肺部CT）和多种模态的数据上，并成功展示了这种方法能够解决基于这些表示的下游任务。此外，还发布了超过55万张标注神经场的大规模数据集以促进相关研究进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/pfriedri/medfuncta&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research in medical image analysis with deep learning almostexclusively focuses on grid- or voxel-based data representations. We challengethis common choice by introducing MedFuncta, a modality-agnostic continuousdata representation based on neural fields. We demonstrate how to scale neuralfields from single instances to large datasets by exploiting redundancy inmedical signals and by applying an efficient meta-learning approach with acontext reduction scheme. We further address the spectral bias in commonly usedSIREN activations, by introducing an $\omega_0$-schedule, improvingreconstruction quality and convergence speed. We validate our proposed approachon a large variety of medical signals of different dimensions and modalities(1D: ECG; 2D: Chest X-ray, Retinal OCT, Fundus Camera, Dermatoscope, ColonHistopathology, Cell Microscopy; 3D: Brain MRI, Lung CT) and successfullydemonstrate that we can solve relevant downstream tasks on theserepresentations. We additionally release a large-scale dataset of &gt; 550kannotated neural fields to promote research in this direction.</description>
      <author>example@mail.com (Paul Friedrich, Florentin Bieder, Phlippe C. Cattin)</author>
      <guid isPermaLink="false">2502.14401v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
  <item>
      <title>Overlap-aware meta-learning attention to enhance hypergraph neural networks for node classification</title>
      <link>http://arxiv.org/abs/2503.07961v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  latex, 45 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了OMA-HGNN框架，该框架结合了超图注意力机制和多任务元学习方法，旨在改进现有HGNN模型的性能。&lt;h4&gt;背景&lt;/h4&gt;尽管超图神经网络(HGNNs)在复杂数据集分析中表现出强大的能力，但它们的实际表现通常受限于单一类型注意机制以及对节点重叠水平相同假设的不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架——OMA-HGNN，以克服现有模型中的局限性，提高HGNN的学习能力和泛化性能。&lt;h4&gt;方法&lt;/h4&gt;1. 引入了一种超图注意力机制，整合了结构和特征相似性；2. 根据节点重叠程度的不同将节点划分为不同的任务，并利用多任务Meta-Weight-Net(MWN)确定对应的加权因子；3. 将内部MWN模型与外部HGNN模型的损失函数共同训练。&lt;h4&gt;主要发现&lt;/h4&gt;OMA-HGNN在学习优秀的节点表示方面表现出色，优于现有的九种最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，OMA-HGNN能够显著提高HGNN模型的学习能力和泛化性能，特别是在节点分类任务上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;尽管超图神经网络(HGNNs)已经成为分析复杂数据集的强大框架，但它们的实际性能往往仍受限制。一方面，现有的网络通常采用单一类型的注意力机制，在消息传递过程中重点关注结构或特征的相似性之一。另一方面，假设当前所有节点在重叠水平上相同可能导致次优泛化。为克服这些局限性，我们提出了一种新的框架——overlap-aware meta-learning attention for hypergraph neural networks (OMA-HGNN)。首先，我们引入了集成结构和特征相似性的超图注意力机制，并使用加权因子将它们的各自损失线性组合以适应HGNN模型；其次，根据节点重叠水平的不同将其划分为不同的任务，并开发了一个多任务Meta-Weight-Net (MWN)，用于确定对应的加权因子；第三，我们联合训练内部MWN模型和外部HGNN模型的损失函数，并使用内部模型生成的加权因子来训练外部模型。为了评估OMA-HGNN的有效性，在六个真实世界数据集上进行了实验并将其性能与九种最先进的节点分类方法进行基准测试。结果表明，OMA-HGNN在学习优秀的节点表示方面表现出色，并优于这些基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although hypergraph neural networks (HGNNs) have emerged as a powerfulframework for analyzing complex datasets, their practical performance oftenremains limited. On one hand, existing networks typically employ a single typeof attention mechanism, focusing on either structural or feature similaritiesduring message passing. On the other hand, assuming that all nodes in currenthypergraph models have the same level of overlap may lead to suboptimalgeneralization. To overcome these limitations, we propose a novel framework,overlap-aware meta-learning attention for hypergraph neural networks(OMA-HGNN). First, we introduce a hypergraph attention mechanism thatintegrates both structural and feature similarities. Specifically, we linearlycombine their respective losses with weighted factors for the HGNN model.Second, we partition nodes into different tasks based on their diverse overlaplevels and develop a multi-task Meta-Weight-Net (MWN) to determine thecorresponding weighted factors. Third, we jointly train the internal MWN modelwith the losses from the external HGNN model and train the external model withthe weighted factors from the internal model. To evaluate the effectiveness ofOMA-HGNN, we conducted experiments on six real-world datasets and benchmarkedits perfor-mance against nine state-of-the-art methods for node classification.The results demonstrate that OMA-HGNN excels in learning superior noderepresentations and outperforms these baselines.</description>
      <author>example@mail.com (Murong Yang, Shihui Ying, Xin-Jian Xu)</author>
      <guid isPermaLink="false">2503.07961v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-Efficient Adaptation of Geospatial Foundation Models through Embedding Deflection</title>
      <link>http://arxiv.org/abs/2503.09493v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的策略DEFLECT，用于适应地理空间基础模型(GFMs)以少量额外参数来处理多光谱卫星图像，并证明了该方法在不同数据集上的有效性。&lt;h4&gt;背景&lt;/h4&gt;随着大规模异构数据集变得越来越普遍，低成本地调整基础模型成为了关键问题。先前的工作如自然语言处理中的低秩适应（LoRA）利用了参数更新时的内在低秩特性。&lt;h4&gt;目的&lt;/h4&gt;增强地理空间基础模型在从RGB卫星图像预训练后转换到其他类型光学卫星数据上的适应性。&lt;h4&gt;方法&lt;/h4&gt;DEFLECT策略引入了一种新的方式，使GFMs能够以非常少量的新参数适应多光谱卫星影像，通过改进提取特征的表示能力来特别强化光谱信息。&lt;h4&gt;主要发现&lt;/h4&gt;与竞争方法相比，在分类和分割任务中DEFLECT实现了具有5到10倍更少参数的同时保持相等或更高的准确率。该策略在三个不同的GFMs及五个不同类型的数据集上进行了验证，涉及从森林监测到海洋环境分割的各种应用场景。&lt;h4&gt;结论&lt;/h4&gt;通过引入强烈的归纳偏置来适应多光谱数据可以显著增强地理空间模型的适用性和性能。&lt;h4&gt;翻译&lt;/h4&gt;随着大规模异构数据集变得越来越普遍，低成本地调整基础模型成为了关键问题。先前的工作如自然语言处理中的低秩适应（LoRA）利用了参数更新时的内在低秩特性。本文指出，在数据和模型中引入更强的归纳偏置可以增强地理空间基础模型(GFMs)在从RGB卫星图像预训练后转换到其他类型光学卫星数据上的适应性。具体而言，GFMs预先训练的参数为多光谱图像的空间结构提供了一个强有力的先验知识。为此，我们提出了DEFLECT(用于地球和气候任务的特征细化嵌入)，这是一种新颖的战略，使用极少额外参数将GFMs适配于处理多光谱卫星影像。DEFLECT改进了提取特征的表示能力，尤其是增强了对于地质科学与环境相关任务至关重要的光谱信息。我们在三个不同的GFMs以及五个多样化的数据集上展示了该方法的有效性，包括从森林监控到海洋环境分割的应用场景。对比其他竞争方法，在分类和分割任务中，DEFLECT在参数量减少5-10倍的情况下实现了相等或更高的准确性。相关代码将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As large-scale heterogeneous data sets become increasingly available,adapting foundation models at low cost has become a key issue. Seminal works innatural language processing, e.g. Low-Rank Adaptation (LoRA), leverage the low"intrinsic rank" of parameter updates during adaptation. In this paper, weargue that incorporating stronger inductive biases in both data and models canenhance the adaptation of Geospatial Foundation Models (GFMs), pretrained onRGB satellite images, to other types of optical satellite data. Specifically,the pretrained parameters of GFMs serve as a strong prior for the spatialstructure of multispectral images. For this reason, we introduce DEFLECT(Deflecting Embeddings for Finetuning Latent representations for Earth andClimate Tasks), a novel strategy for adapting GFMs to multispectral satelliteimagery with very few additional parameters. DEFLECT improves therepresentation capabilities of the extracted features, particularly enhancingspectral information, which is essential for geoscience andenvironmental-related tasks. We demonstrate the effectiveness of our methodacross three different GFMs and five diverse datasets, ranging from forestmonitoring to marine environment segmentation. Compared to competing methods,DEFLECT achieves on-par or higher accuracy with 5-10$\times$ fewer parametersfor classification and segmentation tasks. The code will be made publiclyavailable.</description>
      <author>example@mail.com (Romain Thoreau, Valerio Marsocci, Dawa Derksen)</author>
      <guid isPermaLink="false">2503.09493v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Patch-Wise Hypergraph Contrastive Learning with Dual Normal Distribution Weighting for Multi-Domain Stain Transfer</title>
      <link>http://arxiv.org/abs/2503.09523v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;虚拟染色转移利用计算机辅助技术将组织样本的组化染色模式转换为其他染色类型。&lt;h4&gt;背景&lt;/h4&gt;现有方法由于循环一致性假设的限制，往往在详细病理信息的保留上存在不足。&lt;h4&gt;目的&lt;/h4&gt;提出STNHCL方法以解决现有方法中存在的问题，该方法基于超图的补丁对比学习。&lt;h4&gt;方法&lt;/h4&gt;STNHCL通过超图建模捕捉补丁之间的高阶关系，并确保输入和输出图像之间的一致性拓扑结构。同时引入一种新的负样本加权策略，利用鉴别器热图根据高斯分布对组织和背景的不同区域进行不同权重的分配。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明STNHCL在两大类染色转移任务中取得了最先进的性能，并且在下游任务上也有出色表现。&lt;h4&gt;结论&lt;/h4&gt;该模型为虚拟染色转换提供了一种有效的解决方案，未来将公开代码以促进研究进展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：虚拟染色转移利用计算机辅助技术将组织样本的组化染色模式转换为其他染色类型。然而，现有方法由于循环一致性假设的限制，在详细病理信息的保留上存在不足。为了应对这一挑战，我们提出了STNHCL，一种基于超图的补丁对比学习方法。STNHCL通过超图建模捕捉补丁之间的高阶关系，并确保输入和输出图像之间的一致性拓扑结构。此外，我们还引入了一种新的负样本加权策略，利用鉴别器热图根据组织和背景的不同区域进行不同权重分配，以此增强传统的加权方法。实验结果表明STNHCL在两大类染色转移任务中取得了最先进的性能，并且在下游任务上也有出色表现。代码将公开发布以供使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Virtual stain transfer leverages computer-assisted technology to transformthe histochemical staining patterns of tissue samples into other stainingtypes. However, existing methods often lose detailed pathological informationdue to the limitations of the cycle consistency assumption. To address thischallenge, we propose STNHCL, a hypergraph-based patch-wise contrastivelearning method. STNHCL captures higher-order relationships among patchesthrough hypergraph modeling, ensuring consistent higher-order topology betweeninput and output images. Additionally, we introduce a novel negative sampleweighting strategy that leverages discriminator heatmaps to apply differentweights based on the Gaussian distribution for tissue and background, therebyenhancing traditional weighting methods. Experiments demonstrate that STNHCLachieves state-of-the-art performance in the two main categories of staintransfer tasks. Furthermore, our model also performs excellently in downstreamtasks. Code will be made available.</description>
      <author>example@mail.com (Haiyan Wei, Hangrui Xu, Bingxu Zhu, Yulian Geng, Aolei Liu, Wenfei Yin, Jian Liu)</author>
      <guid isPermaLink="false">2503.09523v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Full Event Interpretation: a graph neural network for event reconstruction in Belle II</title>
      <link>http://arxiv.org/abs/2503.09401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了基于图神经网络的Graph-based Full Event Interpretation (GraFEI)模型，用于在Belle~II实验中全面重建事件。此模型可以仅通过检测到的最终状态粒子的信息预测衰变链结构。&lt;h4&gt;背景&lt;/h4&gt;Belle~II实验非常适合测量包含不可见粒子（如中微子）作为最终态的$B$介子衰变过程。这些粒子的动力学特性可以通过重构伴随产生的$B$介子后的能量动量不平衡来推断。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型机器学习模型，该模型能够通过只使用检测到的最终状态粒子的信息，全面重建事件中的衰变链结构，并且无需假设任何关于潜在事件的基础信息。这种方法可以减少背景噪声并保持较高的信号效率。&lt;h4&gt;方法&lt;/h4&gt;GraFEI模型基于图神经网络构建，利用已有的实验数据训练预测衰变链结构。该模型在寻找$B^+ o K^+ u ar u$衰变时进行了测试，并展示了其性能。&lt;h4&gt;主要发现&lt;/h4&gt;相比传统的只使用特定编码的衰变过程和基于Boosted Decision Trees的方法，GraFEI方法能够在减少背景噪声的同时保持较高的信号效率。对于寻找$B^+ o K^+ u ar u$衰变的过程提供了有力证据。&lt;h4&gt;结论&lt;/h4&gt;开发出了一种新的机器学习模型（GraFEI）用于全面事件重建，在减少背景的同时提高了信号的检测率，展示了其在高能物理实验中的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们提出了基于图神经网络构建的Graph-based Full Event Interpretation (GraFEI) 模型，该模型旨在广泛应用于Belle~II 实验中事件的全面重建。 Belle~II 实验非常适合进行涉及最终态包含不可见粒子（如中微子）的$B$介子衰变测量任务。此类粒子的动力学特性可以从重构伴随产生的$B$介子后的能量动量不平衡推断出来。这一任务通过仅从所有事件中的非信号轨迹来重建，或使用基于Boosted Decision Trees和特定硬编码衰变过程限制的Full Event Interpretation算法完成。最近的一个例子是寻找$B^+ o K^+ u ar u$衰变的过程，该方法为此提供了约3个标准偏差的支持证据。GraFEI模型训练用于仅通过检测到的最终状态粒子的信息预测衰变链结构而不假设潜在事件的基础信息。通过保留信号相似的衰变拓扑，该模型在减少背景的同时保持了相对高的信号效率。当应用于寻找$B^+ o K^+ u ar u$时展示了此模型的表现力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work we present the Graph-based Full Event Interpretation (GraFEI), amachine learning model based on graph neural networks to inclusivelyreconstruct events in the Belle~II experiment. Belle~II is well suited toperform measurements of $B$ meson decays involving invisible particles (e.g.neutrinos) in the final state. The kinematical properties of such particles canbe deduced from the energy-momentum imbalance obtained after reconstructing thecompanion $B$ meson produced in the event. This task is performed byreconstructing it either from all the particles in an event but the signaltracks, or using the Full Event Interpretation, an algorithm based on BoostedDecision Trees and limited to specific, hard-coded decay processes. A recentexample involving the use of the aforementioned techniques is the search forthe $B^+ \to K^+ \nu \bar \nu$ decay, that provided an evidence for thisprocess at about 3 standard deviations. The GraFEI model is trained to predictthe structure of the decay chain by exploiting the information from thedetected final state particles only, without making use of any priorassumptions about the underlying event. By retaining only signal-like decaytopologies, the model considerably reduces the amount of background whilekeeping a relatively high signal efficiency. The performances of the model whenapplied to the search for $B^+ \to K^+ \nu \bar \nu$ are presented.</description>
      <author>example@mail.com (Merna Abumusabh, Jacopo Cerasoli, Giulio Dujany, Corentin Santos)</author>
      <guid isPermaLink="false">2503.09401v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Dual-Domain Homogeneous Fusion with Cross-Modal Mamba and Progressive Decoder for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2503.08992v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文介绍了一种名为DDHFusion的网络，旨在解决三维物体检测中多模态特征融合的问题。&lt;h4&gt;背景&lt;/h4&gt;将LiDAR点云特征与图像特征在同质化的BEV（Bird's Eye View）空间进行融合是自动驾驶领域广泛采用的方法。然而，这种方法受限于多模态特征过度压缩的问题，并且尽管一些研究探索了在密集体素空间中实现特征融合的方式，但它们又面临着计算成本高和查询生成低效的问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题并提高三维物体检测的效率与精度，该论文提出了一种双域同质化融合网络DDHFusion。&lt;h4&gt;方法&lt;/h4&gt;通过将图像特征转换到BEV空间以及稀疏体素空间来实现跨模态的信息互补。在设计上采用了两种不同的网络分别用于BEV和体素特征融合，并利用了创新的跨模态体素和BEV Mamba模块，以解决特征对齐问题并提高场景感知效率。&lt;h4&gt;主要发现&lt;/h4&gt;DDHFusion通过结合BEV空间的优势与体素空间的功能，可以有效地减少计算开销，同时保持细节信息，缓解因高度压缩导致的3D细节丢失。此外，在特征解码阶段实现了一个渐进式的查询生成模块，以减轻由特征压缩和小物体大小导致的假阴性问题。&lt;h4&gt;结论&lt;/h4&gt;实验表明DDHFusion在NuScenes数据集上取得了最先进的性能，并且进一步证明了其优越于其他同质化融合方法的能力。&lt;h4&gt;翻译&lt;/h4&gt;将LiDAR点云特征与图像特征融合到一个均一的BEV（Bird's Eye View）空间中已成为自动驾驶领域广泛采用的方法，用于三维物体检测。然而，这些方法受限于多模态特征过度压缩的问题，并且尽管一些研究探索了在密集体素空间中进行特征融合的方式，它们又面临着计算成本高和查询生成低效的问题。为了解决这些问题并减轻各自缺点的影响，我们提出了双域同质化融合网络（DDHFusion），该方法充分利用BEV与体素空间的优势，同时缓解其各自的缺点。具体来说，我们首先使用LSS以及提出的语义感知特征采样模块将图像特征转换到BEV和稀疏体素空间中，并通过过滤无关紧要的体素显著减少计算开销。在特征编码阶段，设计了两个网络分别用于BEV与体素特征融合，并融入创新跨模态体素和BEV Mamba块来解决特征对齐问题并使场景感知更高效且全面。输出的体素特征被注入到BEV空间中以补偿由高度压缩引起的3D细节丢失。在特征解码阶段，在BEV域内实现了渐进式查询生成模块，用以缓解因特征压缩和小物体大小导致假阴性问题产生的结果。最后，一个逐步聚合器能够依次聚集富含上下文信息的BEV特性以及基于几何体素特性，确保更准确的信心预测与边界框回归。在NuScenes数据集中，DDHFusion达到了最先进的性能，并且进一步实验表明其优于其他同质化融合方法的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fusing LiDAR point cloud features and image features in a homogeneous BEVspace has been widely adopted for 3D object detection in autonomous driving.However, such methods are limited by the excessive compression of multi-modalfeatures. While some works explore feature fusion in dense voxel spaces, theysuffer from high computational costs and inefficiencies in query generation. Toaddress these limitations, we propose a Dual-Domain Homogeneous Fusion network(DDHFusion), which leverages the complementary advantages of both BEV and voxeldomains while mitigating their respective drawbacks. Specifically, we firsttransform image features into BEV and sparse voxel spaces using LSS and ourproposed semantic-aware feature sampling module which can significantly reducescomputational overhead by filtering unimportant voxels. For feature encoding,we design two networks for BEV and voxel feature fusion, incorporating novelcross-modal voxel and BEV Mamba blocks to resolve feature misalignment andenable efficient yet comprehensive scene perception. The output voxel featuresare injected into the BEV space to compensate for the loss of 3D details causedby height compression. For feature decoding, a progressive query generationmodule is implemented in the BEV domain to alleviate false negatives duringquery selection caused by feature compression and small object sizes. Finally,a progressive decoder can sequentially aggregate not only context-rich BEVfeatures but also geometry-aware voxel features, ensuring more preciseconfidence prediction and bounding box regression. On the NuScenes dataset,DDHfusion achieves state-of-the-art performance, and further experimentsdemonstrate its superiority over other homogeneous fusion methods.</description>
      <author>example@mail.com (Xuzhong Hu, Zaipeng Duan, Pei An, Jun zhang, Jie Ma)</author>
      <guid isPermaLink="false">2503.08992v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Noise2Score3D: Tweedie's Approach for Unsupervised Point Cloud Denoising</title>
      <link>http://arxiv.org/abs/2503.09283v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2502.16826&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种名为Noise2Score3D的完全无监督点云去噪框架，该框架基于贝叶斯统计和图像去噪的最新进展。&lt;h4&gt;背景&lt;/h4&gt;现有方法通常需要干净的数据进行训练，并且大多数现有的无监督去噪方法使用迭代过程来完成去噪任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型的无监督去噪框架，用于点云数据，在不依赖于干净样本的情况下直接从噪声中学习评分函数。&lt;h4&gt;方法&lt;/h4&gt;Noise2Score3D利用Tweedie公式进行一步式去噪，并引入了Total Variation for Point Clouds作为去噪质量度量指标来估计未知的噪声参数。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与现有无监督方法相比，在Chamfer距离和点对网格度量标准上Noise2Score3D取得了最先进的性能；并且在训练数据集之外表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该方法解决了基于学习的方法中缺乏干净数据的问题，并为真实世界应用中的点云去噪铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;建立在最近贝叶斯统计和图像去噪领域的进展之上，我们提出了Noise2Score3D，这是一种完全无监督的点云去噪框架。该方法直接从噪声数据中学习底层点云分布的评分函数，无需干净的数据进行训练。利用Tweedie公式，在单一步骤内完成去噪过程，避免了现有无监督方法中的迭代流程，从而提高了准确性和效率。此外，我们引入了一种新的度量标准——Total Variation for Point Clouds，用于评估未知噪声参数下的去噪质量。实验结果表明Noise2Score3D在Chamfer距离和点对网格度量中取得了最先进的性能，并且展示了强大的泛化能力。该方法通过解决基于学习的方法中的数据干净度问题以及训练样本之外的数据泛化挑战，在实际应用中为基于学习的点云去噪奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building on recent advances in Bayesian statistics and image denoising, wepropose Noise2Score3D, a fully unsupervised framework for point clouddenoising. Noise2Score3D learns the score function of the underlying pointcloud distribution directly from noisy data, eliminating the need for cleandata during training. Using Tweedie's formula, our method performs denoising ina single step, avoiding the iterative processes used in existing unsupervisedmethods, thus improving both accuracy and efficiency. Additionally, weintroduce Total Variation for Point Clouds as a denoising quality metric, whichallows for the estimation of unknown noise parameters. Experimental resultsdemonstrate that Noise2Score3D achieves state-of-the-art performance onstandard benchmarks among unsupervised learning methods in Chamfer distance andpoint-to-mesh metrics. Noise2Score3D also demonstrates strong generalizationability beyond training datasets. Our method, by addressing the generalizationissue and challenge of the absence of clean data in learning-based methods,paves the way for learning-based point cloud denoising methods in real-worldapplications.</description>
      <author>example@mail.com (Xiangbin Wei)</author>
      <guid isPermaLink="false">2503.09283v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness</title>
      <link>http://arxiv.org/abs/2503.09487v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;文章提出了一种简单三步方法PPA（Projection-Probe-Aggregation），用于参数高效的微调基础模型，以解决图像-文本基础模型在存在虚假相关性时遇到的问题。&lt;h4&gt;背景&lt;/h4&gt;尽管图像-文本基础模型在各种下游任务中取得了成功，但它们仍然面临输入与标签之间存在的虚假相关性的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需依赖群体标注就能有效缓解虚假关联问题的方法。&lt;h4&gt;方法&lt;/h4&gt;PPA包括三个步骤：首先通过投影图像特征到来自文本编码器的类代理的零空间来训练偏置分类器；然后使用修正后的偏置分类器推测群体标签，并探测目标组。最后，聚合每个类别的群体权重以产生无偏差的分类器。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明PPA可以增强少数群体识别并为最小化平衡群体错误提供贝叶斯最优解，从而缓解虚假关联问题。&lt;h4&gt;结论&lt;/h4&gt;实验结果证实了PPA的有效性：在无需训练群体标签的情况下，仅使用不到0.01%的可调参数就超过了最先进的方法，在最差群体准确性上取得了更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容已经全部进行了中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While image-text foundation models have succeeded across diverse downstreamtasks, they still face challenges in the presence of spurious correlationsbetween the input and label. To address this issue, we propose a simplethree-step approach,Project-Probe-Aggregate (PPA), that enablesparameter-efficient fine-tuning for foundation models without relying on groupannotations. Building upon the failure-based debiasing scheme, our method, PPA,improves its two key components: minority samples identification and the robusttraining algorithm. Specifically, we first train biased classifiers byprojecting image features onto the nullspace of class proxies from textencoders. Next, we infer group labels using the biased classifier and probegroup targets with prior correction. Finally, we aggregate group weights ofeach class to produce the debiased classifier. Our theoretical analysis showsthat our PPA enhances minority group identification and is Bayes optimal forminimizing the balanced group error, mitigating spurious correlations.Extensive experimental results confirm the effectiveness of our PPA: itoutperforms the state-of-the-art by an average worst-group accuracy whilerequiring less than 0.01% tunable parameters without training group labels.</description>
      <author>example@mail.com (Beier Zhu, Jiequan Cui, Hanwang Zhang, Chi Zhang)</author>
      <guid isPermaLink="false">2503.09487v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Are ECGs enough? Deep learning classification of cardiac anomalies using only electrocardiograms</title>
      <link>http://arxiv.org/abs/2503.08960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;心电图(ECG)在诊断心脏异常方面至关重要，提供有价值的信息且经济、快速。然而当前文献中ECG分析的作用不明确，许多方法依赖CTPA等影像学手段或未能有效泛化到不同分类问题上。&lt;h4&gt;背景&lt;/h4&gt;心电图是诊断心脏病的重要工具，在临床实践中提供了有价值的见解，并具有成本效益和广泛可用性。但是现有研究中，单纯基于ECG的分析方法作用不清楚：一些研究依赖于其他成像技术（如CTPA），但这些技术未必总是可获得；还有一些方法在面对不同分类问题时泛化能力不足。&lt;h4&gt;目的&lt;/h4&gt;本研究探讨了多种神经网络架构的表现以评估各种方法的影响，并检查是否通过迁移学习将大型ECG数据集中学到的信息转移到较小且更具挑战性的肺栓塞(PE)检测数据集中可以提高模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;利用迁移学习，分析在有限数据上的学习效率和预测性能。&lt;h4&gt;主要发现&lt;/h4&gt;转移学习能够优化基于心电图的深度学习分类器的学习策略，在较少的数据上获得更好的效果。&lt;h4&gt;结论&lt;/h4&gt;通过迁移学习，可以提高基于ECG的心脏病诊断模型在小规模、具有挑战性的数据集上的泛化能力和预测性能。这为利用现有大型公共数据集来改善有限可用资源条件下的心脏病检测提供了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;心电图(ECG)是心脏异常诊断中的一个关键工具，提供宝贵的临床洞察力，并因其低成本、快速和可获取性而受到青睐。然而，在现有的文献中，ECG分析的作用常常模糊不清：许多方法依赖额外的成像方式（如CTPA），这些技术可能不总是可用；或者它们不能在不同的分类问题上有效泛化。此外，公开的心电图数据集有限且实践中往往较小，因此优化学习策略至关重要。在这项研究中，我们调查了多种神经网络架构的表现，以评估各种方法的影响，并检查这些做法是否可以通过迁移学习提高模型的泛化能力——即使用大型ECG数据集（如PTB-XL和CPSC18）中学到的信息来改进较小且更具挑战性的肺栓塞(PE)检测数据集。通过利用迁移学习，我们分析了在有限数据上可以多大程度地改善学习效率和预测性能。代码可在https://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers 找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electrocardiography (ECG) is an essential tool for diagnosing multiplecardiac anomalies: it provides valuable clinical insights, while beingaffordable, fast and available in many settings. However, in the currentliterature, the role of ECG analysis is often unclear: many approaches eitherrely on additional imaging modalities, such as Computed Tomography PulmonaryAngiography (CTPA), which may not always be available, or do not effectivelygeneralize across different classification problems. Furthermore, theavailability of public ECG datasets is limited and, in practice, these datasetstend to be small, making it essential to optimize learning strategies. In thisstudy, we investigate the performance of multiple neural network architecturesin order to assess the impact of various approaches. Moreover, we check whetherthese practices enhance model generalization when transfer learning is used totranslate information learned in larger ECG datasets, such as PTB-XL andCPSC18, to a smaller, more challenging dataset for pulmonary embolism (PE)detection. By leveraging transfer learning, we analyze the extent to which wecan improve learning efficiency and predictive performance on limited data.Code available athttps://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers .</description>
      <author>example@mail.com (Joao D. S. Marques, Arlindo L. Oliveira)</author>
      <guid isPermaLink="false">2503.08960v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Manify: A Python Library for Learning Non-Euclidean Representations</title>
      <link>http://arxiv.org/abs/2503.09576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 4 figures, 4 tables. Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;非欧几里得空间上的表示学习是一个重要的研究领域，在机器学习和数据分析中具有广泛应用。&lt;h4&gt;目的&lt;/h4&gt;介绍Manify库，这是一个开源Python库，旨在提供工具来促进在非欧几里得空间中的数据处理、分类与回归以及曲率估计的研究和应用。&lt;h4&gt;方法&lt;/h4&gt;使用流形学习技术，Manify提供了用于学习（非）欧几里得空间中嵌入表示的工具。此外，它支持基于这些空间的数据进行分类和回归，并且能够估算流形的曲率。&lt;h4&gt;主要发现&lt;/h4&gt;通过提供一套全面的工具集，Manify旨在推进基于流形数据分析的研究工作和实际应用。&lt;h4&gt;结论&lt;/h4&gt;论文强调了Manify库的重要性及其在机器学习领域的潜在影响。此外，它还提供了项目的GitHub链接（https://github.com/pchlenski/manify），以便读者访问源代码、示例、数据集、结果以及文档资料。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了Manify，这是一个开源的Python库，用于非欧几里得表示学习。通过使用流形学习技术，Manify提供了在（非）欧几里得空间中学习嵌入表示的工具，并且可以在此类空间中的数据进行分类和回归操作以及估计流形的曲率。Manify旨在通过提供一套全面的基于流形的数据分析工具来推动机器学习的研究与应用。我们的源代码、示例、数据集、结果以及文档资料均可在https://github.com/pchlenski/manify找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Manify, an open-source Python library for non-Euclideanrepresentation learning. Leveraging manifold learning techniques, Manifyprovides tools for learning embeddings in (products of) non-Euclidean spaces,performing classification and regression with data that lives in such spaces,and estimating the curvature of a manifold. Manify aims to advance research andapplications in machine learning by offering a comprehensive suite of tools formanifold-based data analysis. Our source code, examples, datasets, results, anddocumentation are available at https://github.com/pchlenski/manify</description>
      <author>example@mail.com (Philippe Chlenski, Kaizhu Du, Dylan Satow, Itsik Pe'er)</author>
      <guid isPermaLink="false">2503.09576v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Towards Robust Multimodal Representation: A Unified Approach with Adaptive Experts and Alignment</title>
      <link>http://arxiv.org/abs/2503.09498v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的多模态模型Mixture of Experts, Symmetric Aligning, and Reconstruction (MoSARe)，该模型能够在处理不完整数据的情况下保持高精度。&lt;h4&gt;背景&lt;/h4&gt;医疗保健依赖于多种类型的数据，如医学影像、基因信息和临床记录等来改善诊断和治疗。然而，由于隐私限制、成本和技术问题，缺失数据是一个常见挑战。&lt;h4&gt;目的&lt;/h4&gt;为了应对上述挑战，提出了一种新的多模态模型MoSARe，旨在处理不完整多模态数据并保持高精度。&lt;h4&gt;方法&lt;/h4&gt;MoSARe整合了专家选择、跨模式注意力和对比学习来改进特征表示和决策制定。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在数据完整的场景下，MoSARe的表现优于现有的模型。此外，即使部分数据缺失时，该模型也能提供可靠的预测。&lt;h4&gt;结论&lt;/h4&gt;MoSARe在现实世界中的医疗保健应用中特别有用，尤其是在资源有限的环境中。开源代码可在https://github.com/NazaninMn/MoSARe获取。&lt;h4&gt;翻译&lt;/h4&gt;医疗保健依赖于多种类型的数据来改善诊断和治疗，然而缺失数据的挑战使得现有的多模态模型不可靠。为了解决这一问题，我们提出了一种新的深度学习框架MoSARe，它能够处理不完整的多模态数据并保持高精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Healthcare relies on multiple types of data, such as medical images, geneticinformation, and clinical records, to improve diagnosis and treatment. However,missing data is a common challenge due to privacy restrictions, cost, andtechnical issues, making many existing multi-modal models unreliable. Toaddress this, we propose a new multi-model model called Mixture of Experts,Symmetric Aligning, and Reconstruction (MoSARe), a deep learning framework thathandles incomplete multimodal data while maintaining high accuracy. MoSAReintegrates expert selection, cross-modal attention, and contrastive learning toimprove feature representation and decision-making. Our results show thatMoSARe outperforms existing models in situations when the data is complete.Furthermore, it provides reliable predictions even when some data are missing.This makes it especially useful in real-world healthcare settings, includingresource-limited environments. Our code is publicly available athttps://github.com/NazaninMn/MoSARe.</description>
      <author>example@mail.com (Nazanin Moradinasab, Saurav Sengupta, Jiebei Liu, Sana Syed, Donald E. Brown)</author>
      <guid isPermaLink="false">2503.09498v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Post-interactive Multimodal Trajectory Prediction for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2503.09366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个名为Pioformer的粗到细Transformer模型，用于多模态轨迹预测。该模型通过明确提取后交互特征来提高预测准确性。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶中代理之间相互作用的建模和路径预测具有挑战性，特别是由于代理行为固有的不确定性。当前的预测模型很少考虑代理在预测轨迹中的互作用（即后互动）。&lt;h4&gt;目的&lt;/h4&gt;目的是提出一种方法，通过明确地将后交互特征提取到多模态轨迹预测中来提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;{'1': '首先构建了一个粗略路径网络，基于观察的路径和车道段生成粗略路径，并使用图神经网络提取低阶互作用特征。', '2': '接下来，建立一个基于超图神经网络的路径建议网络来生成轨迹提案，在其中通过超图学习高阶交互特征。', '3': '最后，将路径提案传递给提案精炼网络进行进一步细化。观察到的路径和路径提案被拼接在一起作为提案精炼网络的输入，在该网络中结合以前的互作用特征和路径一致性特征来学习后互动特征。', '4': '提出了一种三阶段训练方案以促进学习过程。'}&lt;h4&gt;主要发现&lt;/h4&gt;在Argoverse 1数据集上的广泛实验表明，与基准模型HiVT-64相比，我们的方法分别减少了minADE6、minFDE6、MR6和brier-minFDE6指标的预测误差4.4%、8.4%、14.4% 和5.7%，展示了其优越性。&lt;h4&gt;结论&lt;/h4&gt;Pioformer模型通过考虑后互动特征在多模态轨迹预测中取得了优于现有方法的结果，显示出该模型在未来自动驾驶应用中的潜在价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling the interactions among agents for trajectory prediction ofautonomous driving has been challenging due to the inherent uncertainty inagents' behavior. The interactions involved in the predicted trajectories ofagents, also called post-interactions, have rarely been considered intrajectory prediction models. To this end, we propose a coarse-to-fineTransformer for multimodal trajectory prediction, i.e., Pioformer, whichexplicitly extracts the post-interaction features to enhance the predictionaccuracy. Specifically, we first build a Coarse Trajectory Network to generatecoarse trajectories based on the observed trajectories and lane segments, inwhich the low-order interaction features are extracted with the graph neuralnetworks. Next, we build a hypergraph neural network-based Trajectory ProposalNetwork to generate trajectory proposals, where the high-order interactionfeatures are learned by the hypergraphs. Finally, the trajectory proposals aresent to the Proposal Refinement Network for further refinement. The observedtrajectories and trajectory proposals are concatenated together as the inputsof the Proposal Refinement Network, in which the post-interaction features arelearned by combining the previous interaction features and trajectoryconsistency features. Moreover, we propose a three-stage training scheme tofacilitate the learning process. Extensive experiments on the Argoverse 1dataset demonstrate the superiority of our method. Compared with the baselineHiVT-64, our model has reduced the prediction errors by 4.4%, 8.4%, 14.4%, 5.7%regarding metrics minADE6, minFDE6, MR6, and brier-minFDE6, respectively.</description>
      <author>example@mail.com (Ziyi Huang, Yang Li, Dushuai Li, Yao Mu, Hongmao Qin, Nan Zheng)</author>
      <guid isPermaLink="false">2503.09366v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Rendering for Multimodal Autonomous Driving: Merging Neural and Physics-Based Simulation</title>
      <link>http://arxiv.org/abs/2503.09464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;神经重构模型在自主驾驶模拟中取得了显著进展，动态模型变得越来越普遍。然而，这些模型通常局限于处理遵循原始轨迹的领域内对象。我们引入了一种结合神经重构与基于物理渲染的优点的混合方法。这种方法允许传统网格基动态代理在任意位置虚拟放置，调整环境条件，并从新的摄像机视角进行渲染。&lt;h4&gt;背景&lt;/h4&gt;自主驾驶模拟中，神经重构模型在过去几年取得了重大进展，尤其是动态模型变得越来越流行。然而，这些模型通常只能处理遵循其原始轨迹的领域内对象。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合了神经重构和基于物理渲染的优点的混合方法，以克服现有模型在领域外物体处理上的局限性。&lt;h4&gt;方法&lt;/h4&gt;通过训练定制化的NeRF（Neural Radiance Field）模型，并使用从嘈杂LiDAR点云中推导出的深度正则化，然后将其作为教师模型用于3D Gaussian Splatting (3DGGS) 训练。此过程确保了准确的深度、表面法线和摄像机外观建模。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著提高了新颖视角合成的质量（特别是在路面和车道标记方面），同时通过NeRF2GS的新颖训练方式保持交互式帧率，结合了NeRF方法的强大泛化能力和3DGGS的真实时间渲染速度。&lt;h4&gt;结论&lt;/h4&gt;提出的混合方法可以处理大规模重构任务，并支持实时摄像机仿真与精确LiDAR仿真的后端渲染。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural reconstruction models for autonomous driving simulation have madesignificant strides in recent years, with dynamic models becoming increasinglyprevalent. However, these models are typically limited to handling in-domainobjects closely following their original trajectories. We introduce a hybridapproach that combines the strengths of neural reconstruction withphysics-based rendering. This method enables the virtual placement oftraditional mesh-based dynamic agents at arbitrary locations, adjustments toenvironmental conditions, and rendering from novel camera viewpoints. Ourapproach significantly enhances novel view synthesis quality -- especially forroad surfaces and lane markings -- while maintaining interactive frame ratesthrough our novel training method, NeRF2GS. This technique leverages thesuperior generalization capabilities of NeRF-based methods and the real-timerendering speed of 3D Gaussian Splatting (3DGS). We achieve this by training acustomized NeRF model on the original images with depth regularization derivedfrom a noisy LiDAR point cloud, then using it as a teacher model for 3DGStraining. This process ensures accurate depth, surface normals, and cameraappearance modeling as supervision. With our block-based trainingparallelization, the method can handle large-scale reconstructions (greaterthan or equal to 100,000 square meters) and predict segmentation masks, surfacenormals, and depth maps. During simulation, it supports a rasterization-basedrendering backend with depth-based composition and multiple camera models forreal-time camera simulation, as well as a ray-traced backend for precise LiDARsimulation.</description>
      <author>example@mail.com (Máté Tóth, Péter Kovács, Zoltán Bendefy, Zoltán Hortsin, Balázs Teréki, Tamás Matuszka)</author>
      <guid isPermaLink="false">2503.09464v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Stealthy Patch-Wise Backdoor Attack in 3D Point Cloud via Curvature Awareness</title>
      <link>http://arxiv.org/abs/2503.09336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 8 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新型的3D点云后门攻击——Stealthy Patch-Wise Backdoor Attack (SPBA)，该方法通过局部触发器增强了深度神经网络中的隐蔽性和攻击成功率。&lt;h4&gt;背景&lt;/h4&gt;现有的3D点云后门攻击主要依赖于全局样本修改，导致隐蔽性不足。这种攻击方式易于被检测，因此需要改进。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的后门攻击方法SPBA，该方法通过局部区域的触发器增强攻击的隐蔽性和效率。&lt;h4&gt;方法&lt;/h4&gt;SPBA将点云分解为多个局部补丁，并使用基于曲率的方法计算这些补丁的几何复杂性。然后，根据视觉敏感度选择并应用具有较低感知能力的补丁作为触发器。利用图傅立叶变换（GFT），优化了能够干扰选定补丁光谱特征的同时保持点云全局结构不变的局部触发。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在ModelNet40和ShapeNetPart数据集上，SPBA在不同模型中均达到了超过96.5%的成功攻击率，并且隐蔽性优于现有的后门攻击方法。&lt;h4&gt;结论&lt;/h4&gt;通过引入基于补丁的光谱触发器，SPBA显著增强了深度神经网络3D点云中的隐蔽性和攻击成功率。这种方法为提高后门攻击的有效性提供了一种新途径。&lt;h4&gt;翻译&lt;/h4&gt;后门攻击对深层神经网络构成严重威胁，可以通过植入预定义触发器激活的隐藏后门来恶意操纵模型行为。现有的3D点云后门攻击主要依赖于样本级全局修改，导致隐蔽性较差。为了解决这一局限性，我们提出了隐秘补丁式后门攻击（SPBA），该方法使用第一个针对3D点云的局部触发器，并限制扰动在本地区域，显著提高了隐蔽性。具体而言，SPBA将点云分解为多个局部补丁并评估其几何复杂度，通过基于曲率的方法计算出每个补丁的不可见分数。这样可以确保通过战略性地应用较低视觉敏感性的复杂几何补丁来使触发器对人眼不那么明显。借助图傅立叶变换（GFT），SPBA优化了能够干扰选定补丁光谱特征同时保持点云全局几何结构不变的局部光谱触发，增强了攻击效果。在ModelNet40和ShapeNetPart数据集上的大量实验表明，SPBA在不同模型中始终实现了超过96.5%的成功率，并且隐蔽性比现有的后门攻击方法更优。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Backdoor attacks pose a severe threat to deep neural networks (DNN) byimplanting hidden backdoors that can be activated with predefined triggers tomanipulate model behaviors maliciously. Existing 3D point cloud backdoorattacks primarily rely on sample-wise global modifications, resulting insuboptimal stealthiness. To address this limitation, we propose StealthyPatch-Wise Backdoor Attack (SPBA), which employs the first patch-wise triggerfor 3D point clouds and restricts perturbations to local regions, significantlyenhancing stealthiness. Specifically, SPBA decomposes point clouds into localpatches and evaluates their geometric complexity using a curvature-based patchimperceptibility score, ensuring that the trigger remains less perceptible tothe human eye by strategically applying it across multiple geometricallycomplex patches with lower visual sensitivity. By leveraging the Graph FourierTransform (GFT), SPBA optimizes a patch-wise spectral trigger that perturbs thespectral features of selected patches, enhancing attack effectiveness whilepreserving the global geometric structure of the point cloud. Extensiveexperiments on ModelNet40 and ShapeNetPart demonstrate that SPBA consistentlyachieves an attack success rate (ASR) exceeding 96.5% across different modelswhile achieving state-of-the-art imperceptibility compared to existing backdoorattack methods.</description>
      <author>example@mail.com (Yu Feng, Dingxin Zhang, Runkai Zhao, Yong Xia, Heng Huang, Weidong Cai)</author>
      <guid isPermaLink="false">2503.09336v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Probes of Anomalous Events at LHC with Self-Organizing Maps</title>
      <link>http://arxiv.org/abs/2503.09247v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 7 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于自组织映射（SOM）的无监督学习算法，用于探测由味改变中性电流（FCNC）介导的罕见顶夸克衰变事件。&lt;h4&gt;背景&lt;/h4&gt;研究背景涉及通过观察包含$bar{b}$子结构的大型R喷注来寻找希格斯玻色子和顶夸克的异常衰变模式。这种衰变为标准模型以外的新物理提供了可能的证据，特别是当希格斯玻色子在增压条件下进一步衰变成$bb$对时。&lt;h4&gt;目的&lt;/h4&gt;目的是利用SOM算法作为探测顶夸克FCNC衰变的有效方法，在大型强子对撞机（LHC）上实现非模型依赖性搜索。&lt;h4&gt;方法&lt;/h4&gt;事件中包含三叉结构，且子喷注被标记为$b$和$c$类型。通过这种方式，研究人员能够展示SOM算法在探测这类事件中的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;该算法相对于其他现有形式主义实现了令人满意的表现，并表明可用于探索标准模型之外的其他物理现象。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于SOM的方法不仅表现良好，而且实施步骤稳健可靠，为未来的BSM（超出标准模型）探测提供了有价值的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a Neural Network Architecture based Unsupervised LearningAlgorithm, Self-Organizing Maps (SOM), for the probe of a rare top decaymediated by Flavor Changing Neutral Current (FCNC) to charm and Higgs boson,with the Higgs boson further decaying to $b \bar{b}$ in a boosted regime.Events with large-R jets, ideally comprising three-prong substructures with b-and c-tagged subjets, are considered. We have illustrated that SOM can beeffectively implemented as an anomaly-probing prescription formodel-independent searches of top FCNC decay at the LHC. Relatively simple informalism, this algorithm achieves a commendable performance over otherexisting formalisms, for the said probe. The steps formulated for this work arerobust in their implementation and can be successfully followed for other BSMprobes.</description>
      <author>example@mail.com (Shreecheta Chowdhury, Amit Chakraborty, Saunak Dutta)</author>
      <guid isPermaLink="false">2503.09247v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Alignment of Unconditioned Action Prior for Language-conditioned Pick and Place in Clutter</title>
      <link>http://arxiv.org/abs/2503.09423v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于语言条件的在杂乱环境中的拣取和放置任务，提出了一种名为A$^2$的方法来整合视觉、语言和行动先验知识，并展示了该方法在模拟和现实世界测试中的优越性能。&lt;h4&gt;背景&lt;/h4&gt;当前的研究主要关注于通过大规模数据集训练端到端策略或在零样本设定下组合基础模型，但这些方法存在局限性：需要大量数据或者容易产生级联错误。此外，现有研究较少利用行动先验知识。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效策略，整合视觉、语言和动作的基础先验知识。&lt;h4&gt;方法&lt;/h4&gt;提出A$^2$（Action Prior Alignment）方法，该方法通过学习一个注意力层将无条件的动作先验与3D视觉-语言基础先验对齐。此外，设计了一种共享政策以提高拣取和放置任务的性能，并引入一种适应多模态动作特性的策略调整方案。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提方法可以使用较少的数据训练并保持零样本泛化能力，在模拟和真实世界环境中的拣取与放置任务中，能实现更高的成功率和更少的操作步骤，同时能够有效推广到未见过的对象和语言指令。&lt;h4&gt;结论&lt;/h4&gt;通过整合视觉、语言和动作的基础先验知识，A$^2$方法在杂乱环境中执行拣取和放置任务时展现出了卓越的性能，为解决类似问题提供了一种新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the task of language-conditioned pick and place in clutter, where arobot should grasp a target object in open clutter and move it to a specifiedplace. Some approaches learn end-to-end policies with features from visionfoundation models, requiring large datasets. Others combine foundation modelsin a zero-shot setting, suffering from cascading errors. In addition, theyprimarily leverage vision and language foundation models, focusing less onaction priors. In this paper, we aim to develop an effective policy byintegrating foundation priors from vision, language, and action. We proposeA$^2$, an action prior alignment method that aligns unconditioned action priorswith 3D vision-language priors by learning one attention layer. The alignmentformulation enables our policy to train with less data and preserve zero-shotgeneralization capabilities. We show that a shared policy for both pick andplace actions enhances the performance for each task, and introduce a policyadaptation scheme to accommodate the multi-modal nature of actions. Extensiveexperiments in simulation and the real-world show that our policy achieveshigher task success rates with fewer steps for both pick and place tasks inclutter, effectively generalizing to unseen objects and language instructions.</description>
      <author>example@mail.com (Kechun Xu, Xunlong Xia, Kaixuan Wang, Yifei Yang, Yunxuan Mao, Bing Deng, Rong Xiong, Yue Wang)</author>
      <guid isPermaLink="false">2503.09423v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Beam Selection in ISAC using Contextual Bandit with Multi-modal Transformer and Transfer Learning</title>
      <link>http://arxiv.org/abs/2503.08937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 4 figures, 2 tables, IEEE International Conference on  Communications 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用集成感知和通信(ISAC)数据来改进复杂室内环境中波束选择过程的框架，通过将多模态变压器模型与多代理上下文bandit算法相结合，提高通信性能，并实现高光谱效率。&lt;h4&gt;背景&lt;/h4&gt;第六代(6G)无线技术预计将会引入一种变革性的集成感知和通信(ISAC)范式。ISAC结合了无线通讯和雷达或其他形式的传感来优化频谱和硬件资源。&lt;h4&gt;目的&lt;/h4&gt;利用ISAC数据增强复杂室内环境中的波束选择过程，提高通信性能并实现高光谱效率。&lt;h4&gt;方法&lt;/h4&gt;通过将多模态变压器模型与多代理上下文bandit算法相结合。使用DeepSense 6G数据集进行实验评估，并应用迁移强化学习以减少在多用户场景下的训练时间并提升模型表现。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，所提出的模型比传统的深度强化学习(DRL)方法具有更高的波束预测准确性和适应性，在单用户场景中，光谱效率遗憾值提升了49.6%，而在多用户场景中则提高了19.7%（即便后者训练时间长于前者100倍）。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了ISAC数据在改善复杂室内环境中的波束选择过程中的潜在价值，并展示了迁移强化学习技术的有效性，以加速模型适应多用户的挑战。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译已包含在其他键值对中&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sixth generation (6G) wireless technology is anticipated to introduceIntegrated Sensing and Communication (ISAC) as a transformative paradigm. ISACunifies wireless communication and RADAR or other forms of sensing to optimizespectral and hardware resources. This paper presents a pioneering frameworkthat leverages ISAC sensing data to enhance beam selection processes in complexindoor environments. By integrating multi-modal transformer models with amulti-agent contextual bandit algorithm, our approach utilizes ISAC sensingdata to improve communication performance and achieves high spectral efficiency(SE). Specifically, the multi-modal transformer can capture inter-modalrelationships, enhancing model generalization across diverse scenarios.Experimental evaluations on the DeepSense 6G dataset demonstrate that our modeloutperforms traditional deep reinforcement learning (DRL) methods, achievingsuperior beam prediction accuracy and adaptability. In the single-userscenario, we achieve an average SE regret improvement of 49.6% as compared toDRL. Furthermore, we employ transfer reinforcement learning to reduce trainingtime and improve model performance in multi-user environments. In themulti-user scenario, this approach enhances the average SE regret, which is ameasure to demonstrate how far the learned policy is from the optimal SEpolicy, by 19.7% compared to training from scratch, even when the latter istrained 100 times longer.</description>
      <author>example@mail.com (Mohammad Farzanullah, Han Zhang, Akram Bin Sediq, Ali Afana, Melike Erol-Kantarci)</author>
      <guid isPermaLink="false">2503.08937v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Crowdsourced Homophily Ties Based Graph Annotation Via Large Language Model</title>
      <link>http://arxiv.org/abs/2503.09281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;准确的图注释通常需要大量的标注数据，获取这些数据往往具有挑战性且耗费资源。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法Crowdsourced Homophily Ties Based Graph Annotation via Large Language Model (CSA-LLM)，结合众包标注的优势和大型语言模型的能力来增强图注释过程。&lt;h4&gt;方法&lt;/h4&gt;CSA-LLM通过整合1跳和2跳邻居的信息，利用图数据的结构上下文，并强调同质性联系（表示图内相似性的关键连接）以显著提高注释准确性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法通过提供更准确、可靠的注释来增强图神经网络(GNN)的表现。&lt;h4&gt;结论&lt;/h4&gt;CSA-LLM在减少对大量标注数据的依赖的同时，提高了图注释的质量和效率。&lt;h4&gt;翻译&lt;/h4&gt;精确的图形注释通常需要大量的标记数据，获取这些数据往往具有挑战性和资源密集性。本文提出了基于众包同质性联系的大规模语言模型图形注释（CSA-LLM），这是一种结合了众包标注优势和大规模语言模型能力的新方法，以增强图注释过程。CSA-LLM利用1跳和2跳邻居的信息，通过强调图内相似性的关键连接——同质性联系，显著提高了注释的准确性。实验结果表明，这种方法通过提供更精确和可靠的注释来改进图形神经网络（GNN）的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate graph annotation typically requires substantial labeled data, whichis often challenging and resource-intensive to obtain. In this paper, wepresent Crowdsourced Homophily Ties Based Graph Annotation via Large LanguageModel (CSA-LLM), a novel approach that combines the strengths of crowdsourcedannotations with the capabilities of large language models (LLMs) to enhancethe graph annotation process. CSA-LLM harnesses the structural context of graphdata by integrating information from 1-hop and 2-hop neighbors. By emphasizinghomophily ties - key connections that signify similarity within the graph -CSA-LLM significantly improves the accuracy of annotations. Experimentalresults demonstrate that this method enhances the performance of Graph NeuralNetworks (GNNs) by delivering more precise and reliable annotations.</description>
      <author>example@mail.com (Yu Bu, Yulin Zhu, Kai Zhou)</author>
      <guid isPermaLink="false">2503.09281v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Astrea: A MOE-based Visual Understanding Model with Progressive Alignment</title>
      <link>http://arxiv.org/abs/2503.09445v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了Astrea，一种基于渐进预对齐的多专家协作视觉语言模型架构。&lt;h4&gt;背景&lt;/h4&gt;基于混合专家（MoE）架构的视觉语言模型（VLMs）已成为跨模态理解的关键范式。然而，任务多样性和复杂性给异构视觉专家之间的负载均衡带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;为了应对任务异质性和专家负荷不平衡的问题，提出了一种新的多专家协作视觉语言模型架构。&lt;h4&gt;方法&lt;/h4&gt;1) 异质专家协调机制将四种专业化模型（检测、分割、分类和描述）整合为一个综合的专家矩阵。2) 动态知识融合策略通过渐进预对齐在VLM潜在空间内实现专家和谐，同时使用概率激活随机残差连接保持知识连续性。3) 利用动量对比学习进行长期依赖建模，并采用自适应权重分配器实时调整专家贡献。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的评估显示Astrea在12个基准任务中优于现有模型，平均性能提高了+4.7%。&lt;h4&gt;结论&lt;/h4&gt;这是首次证明渐进预对齐策略使视觉语言模型能够克服跨模态理解中的任务异质性限制，并为开发通用多模态代理提供了新的方法基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectureshave emerged as a pivotal paradigm in multimodal understanding, offering apowerful framework for integrating visual and linguistic information. However,the increasing complexity and diversity of tasks present significant challengesin coordinating load balancing across heterogeneous visual experts, whereoptimizing one specialist's performance often compromises others' capabilities.To address task heterogeneity and expert load imbalance, we propose Astrea, anovel multi-expert collaborative VLM architecture based on progressivepre-alignment. Astrea introduces three key innovations: 1) A heterogeneousexpert coordination mechanism that integrates four specialized models(detection, segmentation, classification, captioning) into a comprehensiveexpert matrix covering essential visual comprehension elements; 2) A dynamicknowledge fusion strategy featuring progressive pre-alignment to harmonizeexperts within the VLM latent space through contrastive learning, complementedby probabilistically activated stochastic residual connections to preserveknowledge continuity; 3) An enhanced optimization framework utilizing momentumcontrastive learning for long-range dependency modeling and adaptive weightallocators for real-time expert contribution calibration. Extensive evaluationsacross 12 benchmark tasks spanning VQA, image captioning, and cross-modalretrieval demonstrate Astrea's superiority over state-of-the-art models,achieving an average performance gain of +4.7\%. This study provides the firstempirical demonstration that progressive pre-alignment strategies enable VLMsto overcome task heterogeneity limitations, establishing new methodologicalfoundations for developing general-purpose multimodal agents.</description>
      <author>example@mail.com (Xiaoda Yang, JunYu Lu, Hongshun Qiu, Sijing Li, Hao Li, Shengpeng Ji, Xudong Tang, Jiayang Xu, Jiaqi Duan, Ziyue Jiang, Cong Lin, Sihang Cai, Zejian Xie, Zhuoyang Song, Songxin Zhang)</author>
      <guid isPermaLink="false">2503.09445v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Sequential Multi-Object Grasping with One Dexterous Hand</title>
      <link>http://arxiv.org/abs/2503.09078v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SeqMultiGrasp系统，该系统允许四指Allegro手依次抓取多个物体，并且在抓取一个物体的同时不掉落另一个已经封闭的物体。&lt;h4&gt;背景&lt;/h4&gt;人类可以利用手指灵巧度一次性抓起并握住多个物体，但对于机器人而言，由于不同形状的物体以及高自由度的手需要复杂的接触交互来完成单一物体的抓取，使得连续多物体抓取具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够让四指Allegro手在模拟器和现实环境中依次稳定地抓起并握住两个不同物体的方法。&lt;h4&gt;方法&lt;/h4&gt;{'合成单个物体的抓握候选姿势': '系统首先生成只使用手中一部分关节的单个物体抓取方案，确保每个姿势的稳定性与可行性。然后将这些验证后的单一物体抓握姿态合并以构建多物体抓握配置。', '训练扩散模型': '在点云条件下的扩散模型用于提出实际应用中的抓握姿态，并通过基于启发式的执行策略来实现。', '测试结果': '系统在8x8个对象组合的模拟器中和6x3个对象组合的实际环境中共进行了2500次试验，分别获得了65.8%的成功率和56.7%的成功率。'}&lt;h4&gt;主要发现&lt;/h4&gt;基于扩散模型的方法为四指机械手连续多物体抓取问题提供了一种有前景的解决方案。&lt;h4&gt;结论&lt;/h4&gt;SeqMultiGrasp系统在模拟器与现实环境中表现出了良好的性能，验证了其有效性，并表明扩散模型是解决高自由度机械手复杂任务的有效工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequentially grasping multiple objects with multi-fingered hands is common indaily life, where humans can fully leverage the dexterity of their hands toenclose multiple objects. However, the diversity of object geometries and thecomplex contact interactions required for high-DOF hands to grasp one objectwhile enclosing another make sequential multi-object grasping challenging forrobots. In this paper, we propose SeqMultiGrasp, a system for sequentiallygrasping objects with a four-fingered Allegro Hand. We focus on sequentiallygrasping two objects, ensuring that the hand fully encloses one object beforelifting it and then grasps the second object without dropping the first. Oursystem first synthesizes single-object grasp candidates, where each grasp isconstrained to use only a subset of the hand's links. These grasps are thenvalidated in a physics simulator to ensure stability and feasibility. Next, wemerge the validated single-object grasp poses to construct multi-object graspconfigurations. For real-world deployment, we train a diffusion modelconditioned on point clouds to propose grasp poses, followed by aheuristic-based execution strategy. We test our system using $8 \times 8$object combinations in simulation and $6 \times 3$ object combinations in real.Our diffusion-based grasp model obtains an average success rate of 65.8% over1600 simulation trials and 56.7% over 90 real-world trials, suggesting that itis a promising approach for sequential multi-object grasping withmulti-fingered hands. Supplementary material is available on our projectwebsite: https://hesic73.github.io/SeqMultiGrasp.</description>
      <author>example@mail.com (Sicheng He, Zeyu Shangguan, Kuanning Wang, Yongchong Gu, Yuqian Fu, Yanwei Fu, Daniel Seita)</author>
      <guid isPermaLink="false">2503.09078v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Global Convergence and Rich Feature Learning in $L$-Layer Infinite-Width Neural Networks under $μ$P Parametrization</title>
      <link>http://arxiv.org/abs/2503.09565v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 5 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了无限宽多层神经网络在使用SGD和Maximal Update参数化训练时的特征学习特性，并且展示了在这种设置下，SGD能够使网络学会远离初始值的线性独立特征。&lt;h4&gt;背景&lt;/h4&gt;尽管深度神经网络具有强大的表示能力，但是如何同时实现有意义的特征学习和全局收敛的理论理解仍然不明确。现有的方法如神经切片核（NTK）由于特征保持接近初始化而受到限制。&lt;h4&gt;目的&lt;/h4&gt;调查无限宽、L层神经网络在使用SGD和Maximal Update参数化训练时的训练动力学。&lt;h4&gt;方法&lt;/h4&gt;采用张量程序框架分析神经网络，通过理论分析与实验验证相结合的方式进行研究。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在适度激活函数条件下，SGD使无限宽、L层神经网络能够学习远离初始值的线性独立特征。2. 这些丰富的特征空间捕捉到数据的相关信息，并确保训练过程中任何收敛点都是全局最小值。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了新的视角来理解深度表示学习，并通过实验证明了理论发现的有效性和适用性。&lt;h4&gt;翻译&lt;/h4&gt;尽管深度神经网络具有强大的表征能力，但如何同时实现有意义的特征学习和全局收敛仍然是一个未解之谜。现有的方法如神经切片核（NTK）因为特征值保持接近初始化而受到限制，未能解答特征在大幅变化时的性质问题。本研究使用张量程序框架探讨了无限宽、L层神经网络在随机梯度下降算法与最大更新参数化下的训练动力学特性。结果显示，在适度激活函数条件下，SGD能使这些网络学会远离初始值的线性独立特征，且这种丰富的特征空间能够捕捉相关数据信息，并确保任何收敛点为全局最小值。本研究通过分析不同层间特征之间的相互作用以及高斯随机变量的属性，提供了对深度表征学习的新见解。并进一步通过真实世界的数据集实验验证了理论发现的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite deep neural networks' powerful representation learning capabilities,theoretical understanding of how networks can simultaneously achieve meaningfulfeature learning and global convergence remains elusive. Existing approacheslike the neural tangent kernel (NTK) are limited because features stay close totheir initialization in this parametrization, leaving open questions aboutfeature properties during substantial evolution. In this paper, we investigatethe training dynamics of infinitely wide, $L$-layer neural networks using thetensor program (TP) framework. Specifically, we show that, when trained withstochastic gradient descent (SGD) under the Maximal Update parametrization($\mu$P) and mild conditions on the activation function, SGD enables thesenetworks to learn linearly independent features that substantially deviate fromtheir initial values. This rich feature space captures relevant datainformation and ensures that any convergent point of the training process is aglobal minimum. Our analysis leverages both the interactions among featuresacross layers and the properties of Gaussian random variables, providing newinsights into deep representation learning. We further validate our theoreticalfindings through experiments on real-world datasets.</description>
      <author>example@mail.com (Zixiang Chen, Greg Yang, Qingyue Zhao, Quanquan Gu)</author>
      <guid isPermaLink="false">2503.09565v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>SCOPE-DTI: Semi-Inductive Dataset Construction and Framework Optimization for Practical Usability Enhancement in Deep Learning-Based Drug Target Interaction Prediction</title>
      <link>http://arxiv.org/abs/2503.09251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的药物-靶标相互作用(DTI)预测方法显示出强大的性能；然而，由于数据多样性有限和建模复杂性高，在实际应用中的适用性受限。&lt;h4&gt;目的&lt;/h4&gt;为了应对这些挑战，提出了一种统一框架SCOPE-DTI，结合大规模、平衡的人类DTI半诱导数据集与先进的深度学习模型。&lt;h4&gt;方法&lt;/h4&gt;{'构建数据集': '从13个公共仓库中构造出的SCOPE数据集，其数据量比常见的基准（如Human数据集）增加了最多100倍。', '建模技术': 'SCOPE模型整合了三维蛋白质和化合物表示、图神经网络以及双线性注意机制来有效捕捉跨域交互模式。'}&lt;h4&gt;主要发现&lt;/h4&gt;该方法在多种DTI预测任务中显著优于现有最佳方法，同时提供了用户友好的接口和数据库，并通过实验验证其有效性。&lt;h4&gt;结论&lt;/h4&gt;SCOPE-DTI通过提供全面的数据、先进的建模以及易于访问的工具来加速药物发现研究。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的药物-靶标相互作用(DTI)预测方法在多种任务中展示了强大的性能；然而，由于数据多样性有限和模型复杂性高，在实际应用中的适用性受限。为了克服这些挑战，我们提出了一种结合大规模、平衡的人类DTI半诱导数据集与先进深度学习建模的统一框架SCOPE-DTI。该框架构建于13个公共仓库之上，并且比常见的基准（如Human数据集）的数据量扩展了最多100倍。此外，SCOPE模型通过整合三维蛋白质和化合物表示、图神经网络以及双线性注意机制来有效捕捉跨域交互模式，在各种DTI预测任务中显著优于现有最佳方法。该框架还提供了一个用户友好的界面和数据库，并且其有效性已被实验确认，如通过实验识别人参皂苷Rh1的抗癌靶点。通过提供全面的数据、先进的建模以及易于访问的工具，SCOPE-DTI加速了药物发现研究进程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based drug-target interaction (DTI) prediction methods havedemonstrated strong performance; however, real-world applicability remainsconstrained by limited data diversity and modeling complexity. To address thesechallenges, we propose SCOPE-DTI, a unified framework combining a large-scale,balanced semi-inductive human DTI dataset with advanced deep learning modeling.Constructed from 13 public repositories, the SCOPE dataset expands data volumeby up to 100-fold compared to common benchmarks such as the Human dataset. TheSCOPE model integrates three-dimensional protein and compound representations,graph neural networks, and bilinear attention mechanisms to effectively capturecross domain interaction patterns, significantly outperforming state-of-the-artmethods across various DTI prediction tasks. Additionally, SCOPE-DTI provides auser-friendly interface and database. We further validate its effectiveness byexperimentally identifying anticancer targets of Ginsenoside Rh1. By offeringcomprehensive data, advanced modeling, and accessible tools, SCOPE-DTIaccelerates drug discovery research.</description>
      <author>example@mail.com (Yigang Chen, Xiang Ji, Ziyue Zhang, Yuming Zhou, Yang-Chi-Dung Lin, Hsi-Yuan Huang, Tao Zhang, Yi Lai, Ke Chen, Chang Su, Xingqiao Lin, Zihao Zhu, Yanggyi Zhang, Kangping Wei, Jiehui Fu, Yixian Huang, Shidong Cui, Shih-Chung Yen, Ariel Warshel, Hsien-Da Huang)</author>
      <guid isPermaLink="false">2503.09251v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Representation Retrieval Learning for Heterogeneous Data Integration</title>
      <link>http://arxiv.org/abs/2503.09494v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;在大数据时代，大规模、多模态的数据集变得越来越普遍，为预测建模和科学发现提供了前所未有的机遇。然而，这些数据集通常表现出复杂的异质性，如协变量偏移、后验漂移和缺失模式等问题，这些问题会阻碍现有预测算法的准确性。&lt;h4&gt;背景&lt;/h4&gt;在大数据背景下，大规模多模态数据集日益普遍，为预测建模及科学发现带来机会的同时也带来了挑战。这些数据集由于复杂的异质性因素使得现有的预测模型难以准确工作。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的表示检索（$R^2$）框架来应对这种复杂异质性带来的问题，提高现有预测算法的准确性。&lt;h4&gt;方法&lt;/h4&gt;$R^2$框架结合了表示学习模块和稀疏诱导机器学习模型。引入“整合度”这一概念用于表征有效数据源的学习，并提出选择性集成惩罚（SIP）以显式改进该特性。&lt;h4&gt;主要发现&lt;/h4&gt;$R^2$框架放松了多任务学习中传统的完全共享假设，允许部分结构共享，且SIP能够提高超额风险界限的收敛速度。大量模拟实验验证了此框架的有效性和在真实数据集上的应用进一步证明其优越性。&lt;h4&gt;结论&lt;/h4&gt;研究提出了一种创新的方法来处理大规模、复杂异质性的多模态数据问题，并通过理论分析和实证证据展示了该方法的优势。&lt;h4&gt;翻译&lt;/h4&gt;在大数据时代，大型的、多模式的数据集日益普遍，为预测建模和科学发现提供了前所未有的机会。然而，这些数据集通常表现出复杂的异质性特征（如协变量偏移、后验漂移以及缺失模式），这可能妨碍现有预测算法的有效性。为了应对这些问题，我们提出了一种新的表示检索($R^2$)框架，该框架结合了表示学习模块和稀疏诱导机器学习模型。此外，我们引入'整合度'的概念，并提出了选择性集成惩罚(SIP)，以明确提升这一特性。理论上证明了$R^2$框架可以缓解多任务学习中的全共享假设问题，允许部分结构的共享，并且SIP能够改进过剩风险边界的收敛速率。广泛的模拟研究验证了我们框架的经验性能，真实世界数据集的应用进一步证实了其相较于现有方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of big data, large-scale, multi-modal datasets are increasinglyubiquitous, offering unprecedented opportunities for predictive modeling andscientific discovery. However, these datasets often exhibit complexheterogeneity, such as covariate shift, posterior drift, and missingmodalities, that can hinder the accuracy of existing prediction algorithms. Toaddress these challenges, we propose a novel Representation Retrieval ($R^2$)framework, which integrates a representation learning module (the representer)with a sparsity-induced machine learning model (the learner). Moreover, weintroduce the notion of "integrativeness" for representers, characterized bythe effective data sources used in learning representers, and propose aSelective Integration Penalty (SIP) to explicitly improve the property.Theoretically, we demonstrate that the $R^2$ framework relaxes the conventionalfull-sharing assumption in multi-task learning, allowing for partially sharedstructures, and that SIP can improve the convergence rate of the excess riskbound. Extensive simulation studies validate the empirical performance of ourframework, and applications to two real-world datasets further confirm itssuperiority over existing approaches.</description>
      <author>example@mail.com (Qi Xu, Annie Qu)</author>
      <guid isPermaLink="false">2503.09494v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation</title>
      <link>http://arxiv.org/abs/2503.09399v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为ForAug的数据增强方案，该方案通过使用预训练基础模型将前景物体与不同背景分离和重新组合来增加数据多样性。&lt;h4&gt;背景&lt;/h4&gt;尽管Vision Transformers（ViT）在大规模图像分类任务中表现优异，但它们通常需要大量数据并表现出限制其鲁棒性和泛化能力的偏见。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的数据增强方案，可以将常见的神经网络架构中的归纳偏差显式地引入训练数据。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的基础模型分离和重新组合前景物体与不同的背景，从而在训练过程中实现对图像组成的精细控制。ForAug的应用于ImageNet的数据集称为ForNet。&lt;h4&gt;主要发现&lt;/h4&gt;对于ViT和其他架构，在ImageNet上的准确率提高了最多4.5个百分点，并且在下游任务中提高7.3个百分点。此外，使用ForNet进行训练显著减少了各种偏见（如背景鲁棒性、前景聚焦、中心偏差和大小偏差）。&lt;h4&gt;结论&lt;/h4&gt;ForAug不仅提供了一种有价值的工具来分析和缓解模型的偏见，而且使得开发更稳健和可靠的计算机视觉模型成为可能。&lt;h4&gt;翻译&lt;/h4&gt;Transformers，尤其是Vision Transformers (ViTs)，在大规模图像分类任务中达到了最先进的性能。然而，它们往往需要大量的数据，并且表现出限制其鲁棒性和泛化能力的偏见。本文介绍了一种名为ForAug的数据增强方案，通过使用预训练的基础模型分离和重新组合前景物体与不同的背景来增加数据多样性。该方法显著提高了ViT和其他架构在ImageNet上的准确率，并减少了各种类型（如背景鲁棒性、前景聚焦、中心偏差和大小偏差）的偏见。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers, particularly Vision Transformers (ViTs), have achievedstate-of-the-art performance in large-scale image classification. However, theyoften require large amounts of data and can exhibit biases that limit theirrobustness and generalizability. This paper introduces ForAug, a novel dataaugmentation scheme that addresses these challenges and explicitly includesinductive biases, which commonly are part of the neural network architecture,into the training data. ForAug is constructed by using pretrained foundationmodels to separate and recombine foreground objects with different backgrounds,enabling fine-grained control over image composition during training. It thusincreases the data diversity and effective number of training samples. Wedemonstrate that training on ForNet, the application of ForAug to ImageNet,significantly improves the accuracy of ViTs and other architectures by up to4.5 percentage points (p.p.) on ImageNet and 7.3 p.p. on downstream tasks.Importantly, ForAug enables novel ways of analyzing model behavior andquantifying biases. Namely, we introduce metrics for background robustness,foreground focus, center bias, and size bias and show that training on ForNetsubstantially reduces these biases compared to training on ImageNet. Insummary, ForAug provides a valuable tool for analyzing and mitigating biases,enabling the development of more robust and reliable computer vision models.Our code and dataset are publicly available at https://github.com/tobna/ForAug.</description>
      <author>example@mail.com (Tobias Christian Nauen, Brian Moser, Federico Raue, Stanislav Frolov, Andreas Dengel)</author>
      <guid isPermaLink="false">2503.09399v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Diff-CL: A Novel Cross Pseudo-Supervision Method for Semi-supervised Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2503.09408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种从分布角度出发的半监督医学图像分割框架（Diff-CL），结合了扩散模型与卷积神经网络的优势，提出了跨伪标签学习机制，并设计了一个高频马巴模块来捕获全局边界和细节信息。&lt;h4&gt;背景&lt;/h4&gt;现有的半监督学习方法大多集中在少量样本上，未能捕捉到整体数据分布。因此，作者认为将分布信息与详细信息相结合对于实现更稳健且准确的分割结果至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的半监督医学图像分割框架（Diff-CL），利用扩散模型和卷积神经网络结合的优点来提高分割性能。&lt;h4&gt;方法&lt;/h4&gt;[{'跨伪标签学习机制': '在扩散模型与卷积分割网络之间引入了一种交叉伪标签学习机制，使两者能够互补优势。'}, {'高频马巴模块设计': '提出了一个高频马巴（High-Frequency Mamba）模块来全局捕获边界和细节信息。'}, {'对比学习应用': '利用对比学习将标签从标记数据传播到未标记数据中。'}]&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架在左心房、脑肿瘤及NIH胰腺三个数据集上均实现了当前最佳性能。&lt;h4&gt;结论&lt;/h4&gt;该工作通过结合扩散模型和卷积神经网络的优势，并引入创新的模块设计，成功地提高半监督医学图像分割的有效性和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semi-supervised learning utilizes insights from unlabeled data to improvemodel generalization, thereby reducing reliance on large labeled datasets. Mostexisting studies focus on limited samples and fail to capture the overall datadistribution. We contend that combining distributional information withdetailed information is crucial for achieving more robust and accuratesegmentation results. On the one hand, with its robust generative capabilities,diffusion models (DM) learn data distribution effectively. However, itstruggles with fine detail capture, leading to generated images with misleadingdetails. Combining DM with convolutional neural networks (CNNs) enables theformer to learn data distribution while the latter corrects fine details. Whilecapturing complete high-frequency details by CNNs requires substantialcomputational resources and is susceptible to local noise. On the other hand,given that both labeled and unlabeled data come from the same distribution, webelieve that regions in unlabeled data similar to overall class semantics tolabeled data are likely to belong to the same class, while regions with minimalsimilarity are less likely to. This work introduces a semi-supervised medicalimage segmentation framework from the distribution perspective (Diff-CL).Firstly, we propose a cross-pseudo-supervision learning mechanism betweendiffusion and convolution segmentation networks. Secondly, we design ahigh-frequency mamba module to capture boundary and detail informationglobally. Finally, we apply contrastive learning for label propagation fromlabeled to unlabeled data. Our method achieves state-of-the-art (SOTA)performance across three datasets, including left atrium, brain tumor, and NIHpancreas datasets.</description>
      <author>example@mail.com (Xiuzhen Guo, Lianyuan Yu, Ji Shi, Na Lei, Hongxiao Wang)</author>
      <guid isPermaLink="false">2503.09408v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>TreeX: Generating Global Graphical GNN Explanations via Critical Subtree Extraction</title>
      <link>http://arxiv.org/abs/2503.09051v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过分析和提取消息传递过程中产生的关键子树来理解图神经网络（GNN）的方法，这些关键子树对应于数据集中的重要次图。该方法能够在嵌入空间中有效地聚合子树，生成直观的图形解释。&lt;h4&gt;背景&lt;/h4&gt;随着对透明度和可解释性的需求增加，在关键领域内人们对于消息传递（MP）图神经网络（GNNs）的可解释性有了越来越多的兴趣。然而，识别整个数据集级别的全局解释概念仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;目的是通过分析GNN内部工作原理来提取关键子树，从而提供直观且图形化的解释方法，超越现有的仅生成非图形规则的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种在嵌入空间中聚合子树的有效算法，该算法不需要复杂的次图匹配或搜索。这种方法可以在局部、类别和全局三个层次上对消息传递GNN进行直观的图形解释。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方案能够生成清晰的数据集级别的子图概念，并且对于个别实例可以提供与当前最先进的局部水平GNN解释器相当甚至更好的性能。&lt;h4&gt;结论&lt;/h4&gt;这项工作通过揭示MP GNN内部工作的关键子树，为理解GNN提供了新的视角和方法。这种方法不仅在全局层面超越了现有技术，在局部层面上也表现出强大的竞争力。&lt;h4&gt;翻译&lt;/h4&gt;随着对透明度和可解释性的需求增加，在关键领域内人们对于消息传递（MP）图神经网络（GNNs）的可解释性有了越来越多的兴趣。尽管已经进行了大量的研究来为个别图实例生成解释，但在数据集级别识别全局解释概念仍然是一项艰巨的任务，特别是在需要图形形式的情况下。大多数先前的工作将GNN视为黑盒模型，本文则提出了解析这些模型的方法，通过分析和提取消息传递过程中的关键子树（对应于数据集中的重要次图）。通过在嵌入空间中聚合子树的高效算法，可以为局部、类别以及全局级别的Message-Passing GNN生成直观的图形解释。实证研究表明，所提出的方案不仅在对比现有的全球性说明方法时能够生成清晰的数据集级别上的子图概念（后者通常产生非图形规则作为解释，例如语言或嵌入），而且还能以可比甚至更优的性能为个别实例提供解释，与领先局部级GNN解释器相比表现优越。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing demand for transparency and interpretability in critical domainshas driven increased interests in comprehending the explainability ofMessage-Passing (MP) Graph Neural Networks (GNNs). Although substantialresearch efforts have been made to generate explanations for individual graphinstances, identifying global explaining concepts for a GNN still poses greatchallenges, especially when concepts are desired in a graphical form on thedataset level. While most prior works treat GNNs as black boxes, in this paper,we propose to unbox GNNs by analyzing and extracting critical subtrees incurredby the inner workings of message passing, which correspond to criticalsubgraphs in the datasets. By aggregating subtrees in an embedding space withan efficient algorithm, which does not require complex subgraph matching orsearch, we can make intuitive graphical explanations for Message-Passing GNNson local, class and global levels. We empirically show that our proposedapproach not only generates clean subgraph concepts on a dataset level incontrast to existing global explaining methods which generate non-graphicalrules (e.g., language or embeddings) as explanations, but it is also capable ofproviding explanations for individual instances with a comparable or evensuperior performance as compared to leading local-level GNN explainers.</description>
      <author>example@mail.com (Shengyao Lu, Jiuding Yang, Baochun Li, Di Niu)</author>
      <guid isPermaLink="false">2503.09051v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Urban Region Representation Learning: A Flexible Approach</title>
      <link>http://arxiv.org/abs/2503.09128v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为FlexiReg的模型，该模型用于学习城市区域表示，具有形成城市的灵活性和输入区域特征的灵活性。&lt;h4&gt;背景&lt;/h4&gt;随着城市数据的日益可用性，提供了新的机会来学习地区表示，这些表示可以作为机器学习模型的输入，用于下游任务如签到或犯罪预测。然而，现有的解决方案形成的区域是固定的，且输入的区域特性也是固定的，这可能不适用于不同的下游任务。&lt;h4&gt;目的&lt;/h4&gt;提出一个灵活的城市区域表示学习方法，能够根据不同的需求调整城市区域的形式和特征。&lt;h4&gt;方法&lt;/h4&gt;FlexiReg基于空间网格划分的方法，在感兴趣的空间区域内进行操作。它利用公开的数据源（如POI、土地使用情况、卫星图像和街景图像）为网格单元学习表示，并通过自适应聚合来融合这些单元的表示，同时采用提示学习技术根据不同的任务调整这些表示。&lt;h4&gt;主要发现&lt;/h4&gt;在五个现实世界数据集上的大量实验表明，与现有最佳模型相比，FlexiReg使用生成的城市区域表示，在四个不同下游任务的准确度上提高了高达202%。&lt;h4&gt;结论&lt;/h4&gt;通过提供灵活性和适应性，FlexiReg展示了改进城市数据分析中各种机器学习应用性能的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;随着城市数据的可用性的增加，为学习地区表示提供了新的机会，这些可以用于签到或犯罪预测等下游任务的机器学习模型输入。然而，现有解决方案固定的区域形成和固定输入特征的问题限制了它们在不同需求下的适用性。为了解决这一局限，论文提出了一种名为FlexiReg的城市区域表示学习模型，该模型具有城市区域形成的灵活性以及输入区域特征的灵活性。FlexiReg基于空间网格划分技术，在公开数据的支持下学习单元格表示，并通过自适应聚合和提示学习调整这些表示以符合不同的任务需求。实验表明，与现有最佳方法相比，FlexiReg在五个真实世界数据集上的四类下游任务中性能提高了202%以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing availability of urban data offers new opportunities forlearning region representations, which can be used as input to machine learningmodels for downstream tasks such as check-in or crime prediction. Whileexisting solutions have produced promising results, an issue is their fixedformation of regions and fixed input region features, which may not suit theneeds of different downstream tasks. To address this limitation, we propose amodel named FlexiReg for urban region representation learning that is flexiblewith both the formation of urban regions and the input region features.FlexiReg is based on a spatial grid partitioning over the spatial area ofinterest. It learns representations for the grid cells, leveraging publiclyaccessible data, including POI, land use, satellite imagery, and street viewimagery. We propose adaptive aggregation to fuse the cell representations andprompt learning techniques to tailor the representations towards differenttasks, addressing the needs of varying formations of urban regions anddownstream tasks. Extensive experiments on five real-world datasets demonstratethat FlexiReg outperforms state-of-the-art models by up to 202% in term of theaccuracy of four diverse downstream tasks using the produced urban regionrepresentations.</description>
      <author>example@mail.com (Fengze Sun, Yanchuan Chang, Egemen Tanin, Shanika Karunasekera, Jianzhong Qi)</author>
      <guid isPermaLink="false">2503.09128v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>FP3: A 3D Foundation Policy for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2503.08950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website: https://3d-foundation-policy.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FP3是一个专为机器人操作设计的大规模三维基础策略模型，它在大型多任务数据集上进行预训练，并且能够高效地微调以适应下游任务。&lt;h4&gt;背景&lt;/h4&gt;现有的大多数机器人基础模型仅依赖于二维图像观察，而忽略了对感知和理解三维世界至关重要的几何信息。FP3则填补了这一空白。&lt;h4&gt;目的&lt;/h4&gt;引入一个基于可扩展扩散变换器架构的大规模预训练三维模型，旨在解决现有机器人基础模型在处理三维数据上的局限性。&lt;h4&gt;方法&lt;/h4&gt;FP3使用点云观察进行预训练，并且在其设计和多样化预训练数据的支持下能够高效地微调以适应下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，在仅用80次示范的情况下，FP3能够在具有新环境和未见过物体的环境中实现超过90%的成功率，大大超越了现有的机器人基础模型的表现。&lt;h4&gt;结论&lt;/h4&gt;通过结合点云观察与可扩展架构，FP3展现了其在处理复杂、多变的三维环境中的强大能力。&lt;h4&gt;翻译&lt;/h4&gt;继自然语言处理和计算机视觉领域取得成功之后，大规模多任务数据集上预训练的基础模型也展示了它们在机器人领域的巨大潜力。然而，大多数现有的机器人基础模型仅依赖于二维图像观察，忽略了对感知和理解三维世界至关重要的几何信息。本文介绍了一种新型的大规模3D基础策略模型FP3，它建立在一个可扩展的扩散变换器架构之上，并且基于带有点云观测的60k轨迹进行了预训练。通过这种模型设计以及多样化预训练数据的支持下，FP3能够被高效地微调以适应下游任务，同时表现出强大的泛化能力。在真实机器人上的实验表明，仅用80次示范后，FP3就能在一个新环境中学习到一项新的操作任务，并且成功率超过90%，显著超越了现有的机器人基础模型的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Following its success in natural language processing and computer vision,foundation models that are pre-trained on large-scale multi-task datasets havealso shown great potential in robotics. However, most existing robot foundationmodels rely solely on 2D image observations, ignoring 3D geometric information,which is essential for robots to perceive and reason about the 3D world. Inthis paper, we introduce FP3, a first large-scale 3D foundation policy modelfor robotic manipulation. FP3 builds on a scalable diffusion transformerarchitecture and is pre-trained on 60k trajectories with point cloudobservations. With the model design and diverse pre-training data, FP3 can beefficiently fine-tuned for downstream tasks while exhibiting stronggeneralization capabilities. Experiments on real robots demonstrate that withonly 80 demonstrations, FP3 is able to learn a new task with over 90% successrates in novel environments with unseen objects, significantly surpassingexisting robot foundation models.</description>
      <author>example@mail.com (Rujia Yang, Geng Chen, Chuan Wen, Yang Gao)</author>
      <guid isPermaLink="false">2503.08950v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Accelerate 3D Object Detection Models via Zero-Shot Attention Key Pruning</title>
      <link>http://arxiv.org/abs/2503.08101v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The code can be found at https://github.com/iseri27/tg_gbc&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;基于查询的密集特征方法在3D物体检测任务中表现出显著的成功，但这些模型计算需求高，在大型图像和多层Transformer结构下难以高效运行于边缘设备。为了应对这一挑战，研究提出了一种针对Transformer解码器的零样本运行时修剪方法tgGBC（逐步根据分类得分引导进行键剪枝），该方法系统地基于每个关键值的重要性来逐层修剪变压器模块中的关键值。&lt;h4&gt;背景&lt;/h4&gt;现有模型在大型图像尺寸和多层Transformer结构下计算需求高，现有的裁剪与蒸馏方法要么需要重新训练或者专门针对ViT模型设计难以迁移到3D检测器。&lt;h4&gt;目的&lt;/h4&gt;提出一种零样本运行时修剪方法tgGBC来减少三维物体检测模型的计算成本并提高边缘设备上的效率。&lt;h4&gt;方法&lt;/h4&gt;该方法根据分类得分和注意力图乘积得到每个关键值的重要性分数，然后在每层Transformer之后按照重要性分数剪枝某些关键值。&lt;h4&gt;主要发现&lt;/h4&gt;对于最新的ToC3D模型，在保持仅低于1%性能损失的同时使解码器的速度提高了1.99倍。在某些情况下，该方法甚至能提高整体性能。将使用tgGBC的3D检测器部署于边缘设备进一步验证了其有效性。&lt;h4&gt;结论&lt;/h4&gt;提出了一个有效的零样本运行时修剪方法来降低三维物体检测模型的计算复杂度，并且证明了这种方法可以显著提升边缘设备上的运行效率，而不会对精度产生明显的负面影响。&lt;h4&gt;翻译&lt;/h4&gt;基于查询的方法在使用密集特征进行3D对象检测任务中取得了显着的成功。然而，随着图像尺寸增大和Transformer层数增加，这些模型的计算需求变得非常高，这给高效地部署到边缘设备带来了挑战。现有的剪枝方法要么需要重新训练或者专为ViT架构设计，难以迁移至3D检测器。为了应对这一问题，我们提出了一种针对三维对象检测模型中Transformer解码器的零样本运行时修剪技术tgGBC（逐步根据分类得分引导进行键裁剪），该方法系统性地基于关键值的重要性来逐层修剪Transformer模块中的关键值。我们的研究扩展了分类分数与注意力图乘积以获得每个关键值的重要性评分，并在每通过一层Transformer后根据重要性分数裁剪某些关键值。这种方法使最新ToC3D模型的Transformer解码器运行速度提高了1.99倍，同时性能损失小于1%。有趣的是，在一些情况下，我们的方法甚至能提升模型的整体表现。此外，我们将使用tgGBC优化后的3D检测器部署在边缘设备上进一步验证了我们方法的有效性。源代码可在https://github.com/iseri27/tg_gbc处获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/iseri27/tg_gbc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Query-based methods with dense features have demonstrated remarkable successin 3D object detection tasks. However, the computational demands of thesemodels, particularly with large image sizes and multiple transformer layers,pose significant challenges for efficient running on edge devices. Existingpruning and distillation methods either need retraining or are designed for ViTmodels, which are hard to migrate to 3D detectors. To address this issue, wepropose a zero-shot runtime pruning method for transformer decoders in 3Dobject detection models. The method, termed tgGBC (trim keys gradually GuidedBy Classification scores), systematically trims keys in transformer modulesbased on their importance. We expand the classification score to multiply itwith the attention map to get the importance score of each key and then prunecertain keys after each transformer layer according to their importance scores.Our method achieves a 1.99x speedup in the transformer decoder of the latestToC3D model, with only a minimal performance loss of less than 1%.Interestingly, for certain models, our method even enhances their performance.Moreover, we deploy 3D detectors with tgGBC on an edge device, furthervalidating the effectiveness of our method. The code can be found athttps://github.com/iseri27/tg_gbc.</description>
      <author>example@mail.com (Lizhen Xu, Xiuxiu Bai, Xiaojun Jia, Jianwu Fang, Shanmin Pang)</author>
      <guid isPermaLink="false">2503.08101v2</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Backdoor Attacks with Reasonable Constraints on Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.09049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近的研究表明，图神经网络（GNN）容易受到后门攻击。现有的针对GNN的后门攻击使用固定模式触发器，并缺乏合理的触发约束条件，忽略了个体图的特性，导致逃避性不足。为了解决这些问题，我们提出了ABARC，这是第一个具有合理约束条件的自适应后门攻击方法，适用于GNN中的图级别和节点级别任务。&lt;h4&gt;背景&lt;/h4&gt;现有的针对GNN的后门攻击存在使用固定模式触发器、缺乏对个体图形特征考虑以及攻击逃避性不足的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的后门攻击方法ABARC，该方法在合理约束条件下具有自适应性，并且适用于图级别和节点级别的任务。&lt;h4&gt;方法&lt;/h4&gt;{'对于图级别任务': '提出了一种独立于图形拓扑的子图后门攻击。它为每个目标图动态选择触发节点并根据图相似度、特征范围及特征类型来修改节点特征。', '对于节点级别任务': '该攻击首先分析节点特征，随后选择和修改触发特征，并通过节点相似性、特征范围以及特征类型进行约束。此外，设计了一个自适应的边修剪机制以减少邻居对目标节点的影响，确保较高的攻击成功率（ASR）。', '与最先进的防御方法结合使用时的表现': '当结合最新的随机平滑（RS）防御技术时，ABARC仍能保持超过94%的高攻击成功率，比现有攻击高出7%以上'}&lt;h4&gt;主要发现&lt;/h4&gt;即使在合理约束条件下，ABARC仍然可以实现较高的攻击成功率同时仅造成轻微的干净准确度下降。&lt;h4&gt;结论&lt;/h4&gt;提出的自适应后门攻击方法能够在保证逃避性的同时提高攻击效率，并且在结合当前最先进的防御机制时依然表现出色。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，图神经网络（GNN）容易受到后门攻击。现有的针对GNN的后门攻击使用固定模式触发器，并缺乏合理的触发约束条件，忽略了个体图的特性，导致逃避性不足。为了解决这些问题，我们提出了ABARC，这是第一个具有合理约束条件的自适应后门攻击方法，适用于GNN中的图级别和节点级别的任务。对于图级别任务，提出了一种独立于图形拓扑的子图后门攻击。它为每个目标图动态选择触发节点并根据图相似度、特征范围及特征类型来修改节点特征。对于节点级别任务，该攻击首先分析节点特征，随后选择和修改触发特征，并通过节点相似性、特征范围以及特征类型进行约束。此外，设计了一个自适应的边修剪机制以减少邻居对目标节点的影响，确保较高的攻击成功率（ASR）。实验结果表明，在合理约束条件下实现高攻击成功的同时仅造成轻微干净准确度下降。当结合最新的随机平滑（RS）防御技术时，ABARC仍能保持超过94%的高攻击成功率，比现有攻击高出7%以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TDSC.2025.3543020&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies show that graph neural networks (GNNs) are vulnerable tobackdoor attacks. Existing backdoor attacks against GNNs use fixed-patterntriggers and lack reasonable trigger constraints, overlooking individual graphcharacteristics and rendering insufficient evasiveness. To tackle the aboveissues, we propose ABARC, the first Adaptive Backdoor Attack with ReasonableConstraints, applying to both graph-level and node-level tasks in GNNs. Forgraph-level tasks, we propose a subgraph backdoor attack independent of thegraph's topology. It dynamically selects trigger nodes for each target graphand modifies node features with constraints based on graph similarity, featurerange, and feature type. For node-level tasks, our attack begins with ananalysis of node features, followed by selecting and modifying triggerfeatures, which are then constrained by node similarity, feature range, andfeature type. Furthermore, an adaptive edge-pruning mechanism is designed toreduce the impact of neighbors on target nodes, ensuring a high attack successrate (ASR). Experimental results show that even with reasonable constraints forattack evasiveness, our attack achieves a high ASR while incurring a marginalclean accuracy drop (CAD). When combined with the state-of-the-art defenserandomized smoothing (RS) method, our attack maintains an ASR over 94%,surpassing existing attacks by more than 7%.</description>
      <author>example@mail.com (Xuewen Dong, Jiachen Li, Shujun Li, Zhichao You, Qiang Qu, Yaroslav Kholodov, Yulong Shen)</author>
      <guid isPermaLink="false">2503.09049v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Implicit Contrastive Representation Learning with Guided Stop-gradient</title>
      <link>http://arxiv.org/abs/2503.09058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Neurips 2023&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;在自监督表示学习中，Siamese网络通过将正样本对的表征拉近来学习变换不变性。然而，这种架构容易陷入退化解。为了解决这个问题，在对比学习中引入了对比损失函数，以防止负样本对之间的距离过小而导致模型崩溃。但是，使用负采样的算法在减少负样本数量时不稳定。因此，提出了不需要负样本的正样本算法，这些算法通常采用源编码器和目标编码器组成的不对称网络架构来解决退化问题。&lt;h4&gt;背景&lt;/h4&gt;自监督表示学习中Siamese网络容易导致模型崩溃，而传统的对比损失函数虽然可以缓解这个问题但对负样本数量敏感。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，在不使用负样本的情况下能够有效地防止正样本算法的模型崩溃，并提升其性能。&lt;h4&gt;方法&lt;/h4&gt;通过利用不对称网络架构中的源编码器和目标编码器，我们引入了一种隐式对比学习的思想。具体实现为引导停止梯度的方法（Guided Stop-Gradient），并将其应用于SimSiam和BYOL等基准算法中。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在训练过程中表现出良好的稳定性，并且当批次大小较小时也能有效运行；即使没有预测器的情况下，模型也不会发生崩溃。此外，在几个基准测试中表现出了超越原算法的性能提升。&lt;h4&gt;结论&lt;/h4&gt;所提出的引导停止梯度的方法为自监督学习提供了一种有效的解决方案，能够增强正样本算法的鲁棒性和性能。&lt;h4&gt;翻译&lt;/h4&gt;在自监督表示学习过程中，Siamese网络通过将正样本对的表征拉近来实现变换不变性。然而，在没有额外约束的情况下，这种结构容易陷入退化解。为了解决这一问题，对比学习中使用了对比损失函数，以确保负样本之间的距离较大。但是，那些依赖于负采样的算法往往在减少负样本数量时不稳定。因此，研究者们开发了许多仅基于正样本的方法，并利用源编码器和目标编码器这种不对称架构来避免退化问题的发生。在此基础上，我们提出了一种新的方法——引导停止梯度（Guided Stop-Gradient），它能够在不依赖于大量负样本的情况下有效地防止模型崩溃并提升算法性能。我们在SimSiam和BYOL等基准测试中验证了该方法的有效性，并且发现在小批次训练以及无预测器条件下，本方法仍能保持良好的表现。代码开源地址为：https://github.com/bych-lee/gsg&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In self-supervised representation learning, Siamese networks are a naturalarchitecture for learning transformation-invariance by bringing representationsof positive pairs closer together. But it is prone to collapse into adegenerate solution. To address the issue, in contrastive learning, acontrastive loss is used to prevent collapse by moving representations ofnegative pairs away from each other. But it is known that algorithms withnegative sampling are not robust to a reduction in the number of negativesamples. So, on the other hand, there are algorithms that do not use negativepairs. Many positive-only algorithms adopt asymmetric network architectureconsisting of source and target encoders as a key factor in coping withcollapse. By exploiting the asymmetric architecture, we introduce a methodologyto implicitly incorporate the idea of contrastive learning. As itsimplementation, we present a novel method guided stop-gradient. We apply ourmethod to benchmark algorithms SimSiam and BYOL and show that our methodstabilizes training and boosts performance. We also show that the algorithmswith our method work well with small batch sizes and do not collapse even whenthere is no predictor. The code is available athttps://github.com/bych-lee/gsg.</description>
      <author>example@mail.com (Byeongchan Lee, Sehyun Lee)</author>
      <guid isPermaLink="false">2503.09058v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>HessianForge: Scalable LiDAR reconstruction with Physics-Informed Neural Representation and Smoothness Energy Constraints</title>
      <link>http://arxiv.org/abs/2503.08929v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;从LiDAR测量中准确高效地映射大规模室外环境是机器人领域的一个基本挑战，特别是在确保平滑且无伪影的表面重建方面。&lt;h4&gt;背景&lt;/h4&gt;现有最先进的方法虽然集中于内存高效的神经表示来生成高保真度表面，但往往无法产生无伪影的流形体。问题源于输入数据中的噪声和稀疏性。&lt;h4&gt;目的&lt;/h4&gt;本文旨在解决这个问题，提出一种基于物理信息的能量优化框架，以确保表面平滑并减少伪影。&lt;h4&gt;方法&lt;/h4&gt;我们提出了一个深度学习方法，利用原始LiDAR点云从物理学视角优化$L_2$-Hessian能量的功能来学习表面流形的符号距离场（SDF）。该框架包括基于层次八叉树的输入特征编码和多尺度神经网络，以迭代地在不同分辨率下细化SDF。测试时还引入了一种精化策略，通过局部调整顶点位置来纠正生成网格中的拓扑不一致性和边缘扭曲。&lt;h4&gt;主要发现&lt;/h4&gt;我们的方法能够显著提高表面重建的质量，在大规模室外数据集上的表现优于当前最先进的技术。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了一个基于深度学习的解决方案，它不仅提高了准确性，还增强了平滑度，并且可以应用于各种机器人和计算机视觉任务中。&lt;h4&gt;翻译&lt;/h4&gt;从LiDAR测量数据准确高效地创建大型户外环境的3D地图是机器人领域的一个基本挑战，特别是在确保表面重建光滑无伪影方面。尽管当前的技术重点是在高保真度表面生成方面的内存效率神经表示上，但它们通常无法产生无伪影的流形体，这是因为输入数据的噪声和稀疏性导致的问题。为了解决这个问题，我们将表面映射作为基于物理信息的能量优化问题进行建模，并通过优化$L_2$-Hessian能量的功能来强制执行表面平滑度，以惩罚尖锐的表面脊线。我们提出了一种深度学习方法，它从原始LiDAR点云中学习符号距离场（SDF），并利用基于物理学的信息损失函数来优化此过程。该框架包括层次八叉树输入特征编码和多尺度神经网络，在不同的分辨率下迭代地细化SDF。测试时采用精化策略修正生成网格中的拓扑不一致性和边缘扭曲，我们提出了一种加速的最小二乘法优化方法来局部调整顶点位置以保持特征并执行平滑处理。我们在大规模室外数据集上评估了这种方法，并证明我们的方法在准确度和光滑性方面优于当前最先进的技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and efficient 3D mapping of large-scale outdoor environments fromLiDAR measurements is a fundamental challenge in robotics, particularly towardsensuring smooth and artifact-free surface reconstructions. Although thestate-of-the-art methods focus on memory-efficient neural representations forhigh-fidelity surface generation, they often fail to produce artifact-freemanifolds, with artifacts arising due to noisy and sparse inputs. To addressthis issue, we frame surface mapping as a physics-informed energy optimizationproblem, enforcing surface smoothness by optimizing an energy functional thatpenalizes sharp surface ridges. Specifically, we propose a deep learning basedapproach that learns the signed distance field (SDF) of the surface manifoldfrom raw LiDAR point clouds using a physics-informed loss function thatoptimizes the $L_2$-Hessian energy of the surface. Our learning frameworkincludes a hierarchical octree based input feature encoding and a multi-scaleneural network to iteratively refine the signed distance field at differentscales of resolution. Lastly, we introduce a test-time refinement strategy tocorrect topological inconsistencies and edge distortions that can arise in thegenerated mesh. We propose a \texttt{CUDA}-accelerated least-squaresoptimization that locally adjusts vertex positions to enforcefeature-preserving smoothing. We evaluate our approach on large-scale outdoordatasets and demonstrate that our approach outperforms current state-of-the-artmethods in terms of improved accuracy and smoothness. Our code is available at\href{https://github.com/HrishikeshVish/HessianForge/}{https://github.com/HrishikeshVish/HessianForge/}</description>
      <author>example@mail.com (Hrishikesh Viswanath, Md Ashiqur Rahman, Chi Lin, Damon Conover, Aniket Bera)</author>
      <guid isPermaLink="false">2503.08929v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Towards Graph Foundation Models: A Transferability Perspective</title>
      <link>http://arxiv.org/abs/2503.09363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近年来，图基础模型（GFMs）因其在不同图域和任务中的泛化潜力而受到广泛关注。一些工作专注于特定领域的图基础模型，旨在解决特定领域内的一系列任务，另一些则致力于创造跨多个领域的通用图基础模型。&lt;h4&gt;背景&lt;/h4&gt;由于图数据在结构、特征以及分布上的差异性，实现强大的可迁移性是将GFMs应用于不同域和任务的一个重大挑战。迄今为止，还没有系统性的研究从可迁移性的角度对现有的GFMs进行审视与分析。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，我们提出了首个全面的分类法来按照可迁移性的视角对现有图基础模型（GFMs）进行分类和分析，并围绕其应用范围（特定领域 vs 通用用途）以及知识获取和转移方法对其进行结构化划分。该分类法旨在为当前的GFM研究进展提供一个结构化的视角，同时识别出增强GFM泛化能力以应对多样化图数据集和任务的可能性路径。&lt;h4&gt;方法&lt;/h4&gt;通过构建基于可迁移性的图基础模型（GFMs）分类体系，我们对现有工作进行了综述，并对其在不同领域的应用以及它们的知识获取与转移策略进行了分析。&lt;h4&gt;主要发现&lt;/h4&gt;文章总结了当前GFM研究的现状并指出了未来的潜在研究方向。此外，还提出了用于改进模型跨领域泛化能力的方法和途径。&lt;h4&gt;结论&lt;/h4&gt;通过提供一个从可迁移性角度审视图基础模型（GFMs）的研究视角，本文意在启发未来GFM的发展，并为研究人员提供了明确的研究路径指南。&lt;h4&gt;翻译&lt;/h4&gt;近年来，图形基础模型由于其能够跨越不同图形域和任务的泛化潜力而获得了极大的关注。一些研究集中在特定领域的图形基础模型上，这些模型旨在解决某一特定领域内的各种任务；另一些则致力于创造通用目的的图形基础模型，以期超越单一领域的限制，扩展到多个领域内。无论何种类型的基础模型，都必须具备良好的迁移性才能在不同的域和任务中得到广泛应用。然而，由于图数据在结构、特征及分布上的差异，实现强效的迁移性能是一个巨大的挑战。迄今为止，还没有系统性的研究从可迁移性的角度来审视现有的图形基础模型（GFMs）。为了弥补这一空白，我们提出了首个全面分类法，旨在通过分析现有模型的知识获取与转移策略，并基于其应用范围（特定领域 vs 通用用途）来进行结构化的分类和总结。本文希望以此为当前的GFM研究进展提供一个清晰的方向指南，并推动未来在图形基础模型领域的进一步探索和发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, Graph Foundation Models (GFMs) have gained significantattention for their potential to generalize across diverse graph domains andtasks. Some works focus on Domain-Specific GFMs, which are designed to addressa variety of tasks within a specific domain, while others aim to createGeneral-Purpose GFMs that extend the capabilities of domain-specific models tomultiple domains. Regardless of the type, transferability is crucial forapplying GFMs across different domains and tasks. However, achieving strongtransferability is a major challenge due to the structural, feature, anddistributional variations in graph data. To date, there has been no systematicresearch examining and analyzing GFMs from the perspective of transferability.To bridge the gap, we present the first comprehensive taxonomy that categorizesand analyzes existing GFMs through the lens of transferability, structuringGFMs around their application scope (domain-specific vs. general-purpose) andtheir approaches to knowledge acquisition and transfer. We provide a structuredperspective on current progress and identify potential pathways for advancingGFM generalization across diverse graph datasets and tasks. We aims to shedlight on the current landscape of GFMs and inspire future research directionsin GFM development.</description>
      <author>example@mail.com (Yuxiang Wang, Wenqi Fan, Suhang Wang, Yao Ma)</author>
      <guid isPermaLink="false">2503.09363v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning for Climate Action: Computer Vision Analysis of Visual Narratives on X</title>
      <link>http://arxiv.org/abs/2503.09361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要主题&lt;/h4&gt;气候变化与社交媒体上的公众讨论&lt;h4&gt;背景&lt;/h4&gt;气候变化是21世纪面临的最紧迫挑战之一，引发了社交媒体平台上广泛的讨论。然而，在后API时代，访问社交媒体数据变得越来越受限。&lt;h4&gt;目的&lt;/h4&gt;分析气候相关推文的数据集，以理解公共情绪和叙事，并探讨视觉叙事在气候对话中的作用。&lt;h4&gt;方法&lt;/h4&gt;该研究采用统计分析、图像分类、对象检测及情感分析等方法来探索气候讨论的视觉叙事。此外，还开发了一个图形用户界面（GUI）以便于交互式数据探索。&lt;h4&gt;主要发现&lt;/h4&gt;发现了关键的主题，并揭示了图片与文本之间的感情差异；同时还指出了基础模型在社交媒体图像分析中的优势和局限性。&lt;h4&gt;结论&lt;/h4&gt;通过发布代码和工具，该研究旨在支持未来关于气候变化、社交媒体以及计算机视觉交叉领域的研究工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Climate change is one of the most pressing challenges of the 21st century,sparking widespread discourse across social media platforms. Activists,policymakers, and researchers seek to understand public sentiment andnarratives while access to social media data has become increasingly restrictedin the post-API era. In this study, we analyze a dataset of climatechange-related tweets from X (formerly Twitter) shared in 2019, containing 730ktweets along with the shared images. Our approach integrates statisticalanalysis, image classification, object detection, and sentiment analysis toexplore visual narratives in climate discourse. Additionally, we introduce agraphical user interface (GUI) to facilitate interactive data exploration. Ourfindings reveal key themes in climate communication, highlight sentimentdivergence between images and text, and underscore the strengths andlimitations of foundation models in analyzing social media imagery. Byreleasing our code and tools, we aim to support future research on theintersection of climate change, social media, and computer vision.</description>
      <author>example@mail.com (Katharina Prasse, Marcel Kleinmann, Inken Adam, Kerstin Beckersjuergen, Andreas Edte, Jona Frroku, Timotheus Gumpp, Steffen Jung, Isaac Bravo, Stefanie Walter, Margret Keuper)</author>
      <guid isPermaLink="false">2503.09361v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Robust Asymmetric Heterogeneous Federated Learning with Corrupted Clients</title>
      <link>http://arxiv.org/abs/2503.09206v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了一种具有模型异构和数据损坏客户端的鲁棒联邦学习任务，提出了一种新的Robust Asymmetric Heterogeneous Federated Learning (RAHFL)框架。&lt;h4&gt;背景&lt;/h4&gt;在现实世界的部署中，由于随机噪声、压缩失真或环境条件等因素的影响，不可避免地会导致数据损坏，这会大大削弱整个联邦系统。&lt;h4&gt;目的&lt;/h4&gt;为了应对这些问题，本文提出了新的RAHFL框架来增强本地模型的弹性和适应性，并对抗来自外部客户端的破坏反馈。&lt;h4&gt;方法&lt;/h4&gt;通过利用混合数据增强策略获得复杂的数据扩充样本进行监督对比学习的方法，设计了不对称异构联邦学习策略以允许客户端在协作学习阶段执行选择性的单向学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在各种具有挑战性的联邦学习环境中表现出色且鲁棒。&lt;h4&gt;结论&lt;/h4&gt;代码和模型已经公开发布在https://github.com/FangXiuwen/RAHFL。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为：本文研究了一种具有模型异构和数据损坏客户端的有挑战性的鲁棒联邦学习任务。为了应对这些问题，我们提出了一种新的Robust Asymmetric Heterogeneous Federated Learning (RAHFL)框架。我们提出了一种增强监督对比学习技术来提高本地模型在各种数据损坏模式下的弹性和适应性。其基本思想是利用混合数据增强策略获得的复杂数据扩充样本进行监督对比学习，从而增强模型学习鲁棒和多样特征表示的能力。此外，我们设计了不对称异构联邦学习策略以对抗来自外部客户端的破坏反馈。该策略允许客户在协作学习阶段执行选择性单向学习，使他们能够避免吸收不那么稳健或表现不佳合作者提供的低质量信息。广泛的实验结果证明我们的方法在各种具有挑战性的联邦学习环境中有效且鲁棒。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TPAMI.2025.3527137&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper studies a challenging robust federated learning task with modelheterogeneous and data corrupted clients, where the clients have differentlocal model structures. Data corruption is unavoidable due to factors such asrandom noise, compression artifacts, or environmental conditions in real-worlddeployment, drastically crippling the entire federated system. To address theseissues, this paper introduces a novel Robust Asymmetric Heterogeneous FederatedLearning (RAHFL) framework. We propose a Diversity-enhanced supervisedContrastive Learning technique to enhance the resilience and adaptability oflocal models on various data corruption patterns. Its basic idea is to utilizecomplex augmented samples obtained by the mixed-data augmentation strategy forsupervised contrastive learning, thereby enhancing the ability of the model tolearn robust and diverse feature representations. Furthermore, we design anAsymmetric Heterogeneous Federated Learning strategy to resist corrupt feedbackfrom external clients. The strategy allows clients to perform selective one-waylearning during collaborative learning phase, enabling clients to refrain fromincorporating lower-quality information from less robust or underperformingcollaborators. Extensive experimental results demonstrate the effectiveness androbustness of our approach in diverse, challenging federated learningenvironments. Our code and models are public available athttps://github.com/FangXiuwen/RAHFL.</description>
      <author>example@mail.com (Xiuwen Fang, Mang Ye, Bo Du)</author>
      <guid isPermaLink="false">2503.09206v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Quality Over Quantity? LLM-Based Curation for a Data-Efficient Audio-Video Foundation Model</title>
      <link>http://arxiv.org/abs/2503.09205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的音频-视频对齐方法（AVVA），该方法通过大型语言模型驱动的数据整理流程，实现了高质量的训练片段选择和评分。&lt;h4&gt;背景&lt;/h4&gt;将音频和视觉数据用于多模态基础模型的训练仍然存在挑战。现有的方法大多集中在时间同步上，而不是更深层次的内容一致性上。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够超越单纯的时间对齐，在更高层面上实现音频-视频内容一致性的方法。&lt;h4&gt;方法&lt;/h4&gt;AVVA利用了双编码器对比学习框架，通过Whisper和DINOv2模型分别处理音频和视频数据，并选择高质量的训练片段进行后续的学习。&lt;h4&gt;主要发现&lt;/h4&gt;在AudioCaps、VALOR和VGGSound等数据集上进行了评估，结果显示该方法可以在使用较少精心整理的数据的情况下实现显著的准确性提升。例如，在与ImageBind相比时，AVVA仅使用了192小时（对比5800+小时）精心筛选过的数据，在VGGSound上的音频到视频检索任务中获得了7.6%的top-1准确率改进。&lt;h4&gt;结论&lt;/h4&gt;AVVA提供了一条通往更稳健、无文本依赖的视听学习方法的道路，同时在检索准确性方面有所提升。然而，该方法也存在大型语言模型驱动的数据整理带来的计算开销问题，并讨论了如何在未来的大规模应用中进行优化和简化。&lt;h4&gt;翻译&lt;/h4&gt;整合音频和视觉数据以训练多模态基础模型仍然具有挑战性。我们提出了音频-视频向量对齐（AVVA），它通过基于大型语言模型的数据筛选流程，将视听场景内容的对齐提升到了时间同步之外的层面。具体来说，AVVA利用Whisper进行语音相关的音频基础建模，并用DINOv2处理视频，在一个双编码器对比学习框架内评分和选择高质量的训练片段。在AudioCaps、VALOR以及VGGSound上的评估表明，这种方法能够在使用显著较少的数据的情况下实现重大的准确性提升。例如，AVVA在比ImageBind少得多的精心筛选过的数据上（192小时相比5800+小时），仍然实现了对VGGSound音频到视频检索任务7.6%的top-1准确率改进。此外，消融研究表明，在减少训练数据量的同时提高数据质量可以显著提升性能，分别在AudioCaps、VALOR以及VGGSound上实现47.8、48.4和58.0个百分点的top-3精度增长。这些结果强调了AVVA的数据效率性，但也讨论了大型语言模型驱动的数据整理带来的计算开销问题，并探讨了如何在未来大规模应用中进行优化和简化。总的来说，AVVA为实现更稳健且无文本依赖性的视听学习提供了可行路径，并在此过程中提升了检索准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating audio and visual data for training multimodal foundational modelsremains challenging. We present Audio-Video Vector Alignment (AVVA), whichaligns audiovisual (AV) scene content beyond mere temporal synchronization viaa Large Language Model (LLM)-based data curation pipeline. Specifically, AVVAscores and selects high-quality training clips using Whisper (speech-basedaudio foundation model) for audio and DINOv2 for video within a dual-encodercontrastive learning framework. Evaluations on AudioCaps, VALOR, and VGGSounddemonstrate that this approach can achieve significant accuracy gains withsubstantially less curated data. For instance, AVVA yields a 7.6% improvementin top-1 accuracy for audio-to-video retrieval on VGGSound compared toImageBind, despite training on only 192 hours of carefully filtered data (vs.5800+ hours). Moreover, an ablation study highlights that trading data quantityfor data quality improves performance, yielding respective top-3 accuracyincreases of 47.8, 48.4, and 58.0 percentage points on AudioCaps, VALOR, andVGGSound over uncurated baselines. While these results underscore AVVA's dataefficiency, we also discuss the overhead of LLM-driven curation and how it maybe scaled or approximated in larger domains. Overall, AVVA provides a viablepath toward more robust, text-free audiovisual learning with improved retrievalaccuracy.</description>
      <author>example@mail.com (Ali Vosoughi, Dimitra Emmanouilidou, Hannes Gamper)</author>
      <guid isPermaLink="false">2503.09205v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Towards Quantifying Long-Range Interactions in Graph Machine Learning: a Large Graph Dataset and a Measurement</title>
      <link>http://arxiv.org/abs/2503.09008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的大规模转导学习数据集City-Networks，该数据集基于真实世界的城市道路网络构建，旨在探索长程依赖性在图神经网络中的重要性和影响。&lt;h4&gt;背景&lt;/h4&gt;现有大多数图表示学习的数据集专注于小型图，这些图通常用于归纳任务，并且无法很好地捕捉到长程交互。现有的评估方法主要比较全局注意力模型与局部邻居聚合模型，但没有直接测量长程依赖性的手段。&lt;h4&gt;目的&lt;/h4&gt;引入City-Networks数据集，该数据集使用真实世界的城市道路网络构建，含有超过10^5个节点的大规模图，并提供了一种基于雅可比矩阵的模型无关方法来量化长程依赖性。此外还提供了理论证明以支持所提出的数据集设计和测量。&lt;h4&gt;方法&lt;/h4&gt;利用城市道路的真实数据创建大规模转导学习数据集；使用基线偏心度的方法对图进行标注，确保分类任务需要远程节点的信息；提出了一种基于雅可比矩阵的模型无关方法来量化长程依赖性&lt;h4&gt;主要发现&lt;/h4&gt;通过City-Networks数据集和新的测量方法揭示了长程依赖性的关键作用，并提供了理论基础。&lt;h4&gt;结论&lt;/h4&gt;所提出的City-Networks数据集以及新的测量方法为研究图神经网络中的长程交互提供了一种有效的工具，有助于进一步探索大规模图表示学习的挑战。&lt;h4&gt;翻译&lt;/h4&gt;长时间依赖性对于有效的图形表示学习至关重要。然而，大多数现有的数据集中重点在于小型图形，这些图形适用于归纳任务，并且无法很好地捕捉到长时间的信息交换。目前的评估方法主要是比较使用全局注意力（例如，图变换器）和局部邻居聚集（例如，消息传递神经网络）的方法，但没有直接测量长时间依赖性的手段。在这项工作中，我们引入了City-Networks，这是一个从真实世界的城市道路生成的大规模转导学习数据集。这个数据集中包含的图形超过10^5个节点，并且其直径远大于现有基准测试中的图形，自然地体现了长程信息。我们使用基于偏心度的方法对图进行注释，以确保分类任务需要来自遥远节点的信息。此外，我们提出了一种基于远程邻居雅可比矩阵的模型无关测量方法，提供了一个量化长时间依赖性的原则化方法。最后，我们提供了有关数据集设计和所提测量方法的理论依据——特别关注过度平滑以及影响力得分稀释问题——为图神经网络中进一步探索长程交互建立了一种坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-range dependencies are critical for effective graph representationlearning, yet most existing datasets focus on small graphs tailored toinductive tasks, offering limited insight into long-range interactions. Currentevaluations primarily compare models employing global attention (e.g., graphtransformers) with those using local neighborhood aggregation (e.g.,message-passing neural networks) without a direct measurement of long-rangedependency. In this work, we introduce City-Networks, a novel large-scaletransductive learning dataset derived from real-world city roads. This datasetfeatures graphs with over $10^5$ nodes and significantly larger diameters thanthose in existing benchmarks, naturally embodying long-range information. Weannotate the graphs using an eccentricity-based approach, ensuring that theclassification task inherently requires information from distant nodes.Furthermore, we propose a model-agnostic measurement based on the Jacobians ofneighbors from distant hops, offering a principled quantification of long-rangedependencies. Finally, we provide theoretical justifications for both ourdataset design and the proposed measurement - particularly by focusing onover-smoothing and influence score dilution - which establishes a robustfoundation for further exploration of long-range interactions in graph neuralnetworks.</description>
      <author>example@mail.com (Huidong Liang, Haitz Sáez de Ocáriz Borde, Baskaran Sripathmanathan, Michael Bronstein, Xiaowen Dong)</author>
      <guid isPermaLink="false">2503.09008v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?</title>
      <link>http://arxiv.org/abs/2503.08980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过引入一种新的生成模型，探讨了大型语言模型（LLMs）是否展示了某种形式的智能。研究指出，在特定条件下，即使映射关系是非可逆的情况下，也可以建立一个识别性结果：LLM 通过下一个令牌预测学习到的表现可以近似地视为人类可解释概念后验概率对数的形式。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在多项任务中表现出卓越的能力，这促使人们认为它们具有某种形式的智能。然而，也有观点认为这些能力是基于对大量数据进行相对简单操作的结果。&lt;h4&gt;目的&lt;/h4&gt;通过引入一种新的生成模型来区分这两种解释，并提供实验证据支持LLM捕捉到潜在生成因素的观点以及线性表示假设（即LLMs学习人类可理解概念的线性表示）。&lt;h4&gt;方法&lt;/h4&gt;该研究介绍了一种新的基于离散隐变量的人类可解释概念生成令牌的模型。理论上，在温和条件下，即使从隐藏空间映射到观察空间是非可逆的情况下，也可以建立一个识别性结果：LLMs通过下一个令牌预测学习到的表现可以近似地视为人类可理解概念后验概率对数的形式。&lt;h4&gt;主要发现&lt;/h4&gt;该研究建立了线性表示假设，并提供了实验证据来支持这一理论。实验在模拟数据和Pythia、Llama以及DeepSeek模型家族上进行，结果表明LLMs确实捕捉到了潜在的生成因素。&lt;h4&gt;结论&lt;/h4&gt;这项工作不仅为大型语言模型的能力提供了一种新的理解途径，而且还强调了线性表示假设的重要性，指出LLM可能通过学习人类可解释概念的线性表示来实现其出色的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The remarkable achievements of large language models (LLMs) have led many toconclude that they exhibit a form of intelligence. This is as opposed toexplanations of their capabilities based on their ability to perform relativelysimple manipulations of vast volumes of data. To illuminate the distinctionbetween these explanations, we introduce a novel generative model thatgenerates tokens on the basis of human interpretable concepts represented aslatent discrete variables. Under mild conditions, even when the mapping fromthe latent space to the observed space is non-invertible, we establish anidentifiability result: the representations learned by LLMs through next-tokenprediction can be approximately modeled as the logarithm of the posteriorprobabilities of these latent discrete concepts, up to an invertible lineartransformation. This theoretical finding not only provides evidence that LLMscapture underlying generative factors, but also strongly reinforces the linearrepresentation hypothesis, which posits that LLMs learn linear representationsof human-interpretable concepts. Empirically, we validate our theoreticalresults through evaluations on both simulation data and the Pythia, Llama, andDeepSeek model families.</description>
      <author>example@mail.com (Yuhang Liu, Dong Gong, Erdun Gao, Zhen Zhang, Biwei Huang, Mingming Gong, Anton van den Hengel, Javen Qinfeng Shi)</author>
      <guid isPermaLink="false">2503.08980v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>WonderVerse: Extendable 3D Scene Generation with Video Generative Models</title>
      <link>http://arxiv.org/abs/2503.09160v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一个名为WonderVerse的框架，用于生成可扩展且高度逼真的3D场景。&lt;h4&gt;背景&lt;/h4&gt;现有的方法依赖于迭代深度估计和图像修复技术，这可能导致几何变形和不一致性。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的、简单有效的框架来创建沉浸感强且几何一致性的3D环境，并支持可控的3D场景扩展。&lt;h4&gt;方法&lt;/h4&gt;利用视频生成基础模型中的世界级先验知识，采用相机轨迹异常序列检测模块解决生成视频中的几何不一致性问题。&lt;h4&gt;主要发现&lt;/h4&gt;WonderVerse能够与多种3D重建方法兼容，提供高效且高质量的生成结果，在大规模环境扩展和逼真度方面优于现有复杂架构的方法。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的实验表明，WonderVerse框架在生成可扩展、高度真实的3D场景时表现优异，证明了其优雅简单的管道的有效性。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了WonderVerse，这是一个简单但有效的框架，用于生成可扩展的3D场景。与现有的依赖于迭代深度估计和图像修复的方法不同，这种方法经常导致几何变形和不一致性，WonderVerse利用嵌入在视频生成基础模型中的强大的世界级别先验知识来创建高度沉浸且几何一致的3D环境。此外，我们提出了一种新的可控3D场景扩展技术，以大幅增加生成环境的规模。同时，引入了一个新颖的异常序列检测模块，该模块使用摄像机轨迹解决生成视频中的几何不一致性问题。最后，WonderVerse与各种3D重建方法兼容，允许高效且高质量的生成。在3D场景生成方面的广泛实验表明，我们的WonderVerse凭借其优雅和简单的管道提供了可扩展的高度真实的3D场景，在性能上明显优于依赖更复杂架构的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce \textit{WonderVerse}, a simple but effective framework forgenerating extendable 3D scenes. Unlike existing methods that rely on iterativedepth estimation and image inpainting, often leading to geometric distortionsand inconsistencies, WonderVerse leverages the powerful world-level priorsembedded within video generative foundation models to create highly immersiveand geometrically coherent 3D environments. Furthermore, we propose a newtechnique for controllable 3D scene extension to substantially increase thescale of the generated environments. Besides, we introduce a novel abnormalsequence detection module that utilizes camera trajectory to address geometricinconsistency in the generated videos. Finally, WonderVerse is compatible withvarious 3D reconstruction methods, allowing both efficient and high-qualitygeneration. Extensive experiments on 3D scene generation demonstrate that ourWonderVerse, with an elegant and simple pipeline, delivers extendable andhighly-realistic 3D scenes, markedly outperforming existing works that rely onmore complex architectures.</description>
      <author>example@mail.com (Hao Feng, Zhi Zuo, Jia-hui Pan, Ka-hei Hui, Yi-hua Shao, Qi Dou, Wei Xie, Zheng-zhe Liu)</author>
      <guid isPermaLink="false">2503.09160v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>RS2V-L: Vehicle-Mounted LiDAR Data Generation from Roadside Sensor Observations</title>
      <link>http://arxiv.org/abs/2503.07085v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Upon self-examination, we have found that the data in the  experimental section of our paper is uncertain. To ensure academic rigor, we  are applying for the withdrawal of the paper. We will resubmit it after  reconfirming and correcting the data. Thank you for your understanding&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种新的框架RS2V-L被提出，该框架可以从路边传感器观测中重构和合成车载LiDAR数据。&lt;h4&gt;背景&lt;/h4&gt;端到端的自动驾驶解决方案依赖于单辆车辆的数据收集进行模型训练，导致了高成本、关键驾驶场景稀缺以及碎片化数据集的问题。&lt;h4&gt;目的&lt;/h4&gt;减轻单辆车数据采集对模型训练的限制，通过利用路边传感器观测来重构车载LiDAR数据以提高效率和模型鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;RS2V-L框架首先将路边LiDAR点云转换为车载LiDAR坐标系统，并通过虚拟LiDAR建模、点云分类和重采样技术合成高保真度的车载LiDAR数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，使用生成的数据进行模型训练可以提升KITTI数据集上3D物体检测精度超过30%，同时提高端到端自动驾驶数据生成效率一个数量级。&lt;h4&gt;结论&lt;/h4&gt;该方法的有效性得到了验证，并且在减少对车载传感器数据的依赖和增强自动驾驶模型鲁棒性方面具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;端到端的自动驾驶解决方案已经成为自动驾驶研究中的主流范式，这些方案处理多模态传感数据以直接生成精细控制指令。然而，目前的方法主要依靠单一车辆的数据收集进行模型训练和优化，导致了高成本采集和标注、关键驾驶场景稀缺以及碎片化数据集等问题，阻碍了模型泛化的实现。为了缓解这些问题，我们提出了一个新颖的框架RS2V-L，用于从路边传感器观测中重构和合成车载LiDAR数据。通过利用目标车辆的姿态相对位置信息，我们的方法将路边LiDAR点云转换为车载LiDAR坐标系统，并进一步通过虚拟LiDAR建模、点云分类和重采样技术生成高保真度的车载LiDAR数据。据我们所知，这是首个能够从路边传感器输入中重构车载LiDAR数据的方法。广泛的实验评估表明，在模型训练过程中结合生成的数据可以显著提升3D物体检测精度（超过30%），同时大大提高端到端自动驾驶数据生成效率一个数量级。这些发现强有力地验证了该方法的有效性，并强调其在减少对昂贵的车载数据采集依赖以及增强自动驾驶模型鲁棒性的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end autonomous driving solutions, which process multi-modal sensorydata to directly generate refined control commands, have become a dominantparadigm in autonomous driving research. However, these approachespredominantly depend on single-vehicle data collection for model training andoptimization, resulting in significant challenges such as high data acquisitionand annotation costs, the scarcity of critical driving scenarios, andfragmented datasets that impede model generalization. To mitigate theselimitations, we introduce RS2V-L, a novel framework for reconstructing andsynthesizing vehicle-mounted LiDAR data from roadside sensor observations.Specifically, our method transforms roadside LiDAR point clouds into thevehicle-mounted LiDAR coordinate system by leveraging the target vehicle'srelative pose. Subsequently, high-fidelity vehicle-mounted LiDAR data issynthesized through virtual LiDAR modeling, point cloud classification, andresampling techniques. To the best of our knowledge, this is the first approachto reconstruct vehicle-mounted LiDAR data from roadside sensor inputs.Extensive experimental evaluations demonstrate that incorporating the generateddata into model training-complementing the KITTI dataset-enhances 3D objectdetection accuracy by over \text{30\%} while improving the efficiency ofend-to-end autonomous driving data generation by more than an order ofmagnitude. These findings strongly validate the effectiveness of the proposedmethod and underscore its potential in reducing dependence on costlyvehicle-mounted data collection while improving the robustness of autonomousdriving models.</description>
      <author>example@mail.com (Ruidan Xing, Runyi Huang, Qing Xu, Lei He)</author>
      <guid isPermaLink="false">2503.07085v2</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Foundation Models for Computational Pathology: A Survey</title>
      <link>http://arxiv.org/abs/2503.09091v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文综述了计算病理学（CPath）中多模态基础模型的最新进展，重点在于基于组织切片图像和局部表示的多模态基础模型。研究将32个最先进的多模态基础模型分类为视觉-语言、视觉知识图谱和视觉基因表达三个主要范式，并进一步将视觉-语言模型细分为非LLM基方法和LLM基方法。&lt;h4&gt;背景&lt;/h4&gt;随着计算病理学（CPath）的快速发展，基于单一模式的数据训练的基础模型已经逐渐被多模态基础模型所取代。这些新的多模态模型整合了异构数据源，例如文本报告、结构化领域知识以及分子档案，展示了其在疾病诊断和治疗方面的巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;本文旨在为计算病理学中的多模态基础模型提供全面且最新的综述，并分析适用于这一领域的28个多模态数据集。此外，该研究还提出了一套分类下游任务的框架、评估策略以及未来的研究方向。&lt;h4&gt;方法&lt;/h4&gt;将32个最先进的多模态基础模型根据其工作方式分为视觉-语言、视觉知识图谱和视觉基因表达三个主要范式，并进一步细分了视觉-语言模型。同时，该研究还审查了用于病理学的多种多模态数据集，包括图像文本对、指令数据集以及基于其他模式的图像数据。&lt;h4&gt;主要发现&lt;/h4&gt;在计算病理学领域中，基础模型已经从单一模式发展到整合多种数据源（如H&amp;E染色全片图像和基因表达）的多模态模型。这些新的范式不仅提高了分析效率，还增强了结果的可解释性和精确度。&lt;h4&gt;结论&lt;/h4&gt;该综述为研究者提供了一个宝贵的资源库，可以应用于病理学与人工智能交叉领域的深入探索，并指出了未来的研究方向和挑战。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在计算病理学（CPath）中已经成为了强大的范式，支持大规模且通用的组织病理图像分析。早期的发展集中在单一模式模型上，这些模型仅基于视觉数据进行训练，而最近的进步则突显了多模态基础模型的巨大潜力，它们整合了如文本报告、结构化领域知识和分子档案等异构数据源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have emerged as a powerful paradigm in computationalpathology (CPath), enabling scalable and generalizable analysis ofhistopathological images. While early developments centered on uni-modal modelstrained solely on visual data, recent advances have highlighted the promise ofmulti-modal foundation models that integrate heterogeneous data sources such astextual reports, structured domain knowledge, and molecular profiles. In thissurvey, we provide a comprehensive and up-to-date review of multi-modalfoundation models in CPath, with a particular focus on models built uponhematoxylin and eosin (H&amp;E) stained whole slide images (WSIs) and tile-levelrepresentations. We categorize 32 state-of-the-art multi-modal foundationmodels into three major paradigms: vision-language, vision-knowledge graph, andvision-gene expression. We further divide vision-language models intonon-LLM-based and LLM-based approaches. Additionally, we analyze 28 availablemulti-modal datasets tailored for pathology, grouped into image-text pairs,instruction datasets, and image-other modality pairs. Our survey also presentsa taxonomy of downstream tasks, highlights training and evaluation strategies,and identifies key challenges and future directions. We aim for this survey toserve as a valuable resource for researchers and practitioners working at theintersection of pathology and AI.</description>
      <author>example@mail.com (Dong Li, Guihong Wan, Xintao Wu, Xinyu Wu, Xiaohui Chen, Yi He, Christine G. Lian, Peter K. Sorger, Yevgeniy R. Semenov, Chen Zhao)</author>
      <guid isPermaLink="false">2503.09091v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Influential Neuron Path in Vision Transformers</title>
      <link>http://arxiv.org/abs/2503.09046v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文探讨了视觉Transformer模型中的关键神经元路径，通过提出联合影响度量和逐层渐进式的神经元定位方法来评估这些路径的重要性，并展示了这些路径对于图像分类任务的影响。&lt;h4&gt;背景&lt;/h4&gt;当前的Vision Transformer模型虽然表现出强大的功能，但它们的内部运作机制难以被人类理解，这给实际应用带来了挑战和风险。以前的研究主要集中在输入归因和神经元角色分析上，然而鲜少有研究考虑逐层信息以及跨层的信息流动路径。&lt;h4&gt;目的&lt;/h4&gt;通过探索视觉Transformer模型中影响最大的神经元路径来提高对这些模型的理解，并展示这种方法在找到最具影响力的信息传递路径上的优越性。此外，还展示了这些路径如何揭示了同一图像类别内处理视觉信息的特定内部工作机制。&lt;h4&gt;方法&lt;/h4&gt;首先提出了联合影响度量以评估一组神经元对模型输出的影响；其次提供了一种逐层渐进式神经元定位方法，该方法可以高效地选择每个层次中最具影响力的神经元，并试图发现从输入到输出的目标模型中的关键神经路径。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在找到最具影响力的信息传递路径方面优于现有的基准解决方案。此外，这些关键路径还展示了视觉Transformer处理同一类别图像的特定内部工作机制；并且分析了这些关键神经元对图像分类任务的主要影响。&lt;h4&gt;结论&lt;/h4&gt;找到的关键神经元路径已经保留了模型在下游任务中的能力，并可能为实际应用（如模型剪枝）提供启示。项目的官方网站包括实现代码可在提供的链接中访问：https://foundation-model-research.github.io/NeuronPath/&lt;h4&gt;翻译&lt;/h4&gt;视觉Transformer模型虽然表现出强大的功能，但它们的内部运作机制难以被人类理解，这给实际应用带来了挑战和风险。以前的研究主要集中在输入归因和神经元角色分析上，然而鲜少有研究考虑逐层信息以及跨层的信息流动路径。在这项工作中，我们探讨了视觉Transformer模型中具有影响力的神经元路径的重要性，这些路径是从模型输入到输出对模型推理影响最大的神经元路径。首先，我们提出了一种联合影响力度量方法来评估一组神经元对模型结果的贡献；然后，提供了一种逐层渐进式神经元定位方法，这种方法可以有效地选择每一层中最具影响力的神经元，并试图发现从输入到输出的目标模型中的关键神经元路径。我们的实验表明，在找到最具有影响力的信息流路径方面，我们提出的方法优于现有的基准解决方案。此外，这些神经元路径已经揭示了视觉Transformer在处理同一类别的图像时的特定内部工作机制。进一步分析显示，这些关键神经元对图像分类任务产生了重大影响，并且发现的关键神经路径已经在下游任务中保留了模型的能力。这可能为现实世界的应用（例如模型剪枝）提供了一定的启示。项目网站包括实现代码可在提供的链接中访问：https://foundation-model-research.github.io/NeuronPath/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Transformer models exhibit immense power yet remain opaque to humanunderstanding, posing challenges and risks for practical applications. Whileprior research has attempted to demystify these models through inputattribution and neuron role analysis, there's been a notable gap in consideringlayer-level information and the holistic path of information flow acrosslayers. In this paper, we investigate the significance of influential neuronpaths within vision Transformers, which is a path of neurons from the modelinput to output that impacts the model inference most significantly. We firstpropose a joint influence measure to assess the contribution of a set ofneurons to the model outcome. And we further provide a layer-progressive neuronlocating approach that efficiently selects the most influential neuron at eachlayer trying to discover the crucial neuron path from input to output withinthe target model. Our experiments demonstrate the superiority of our methodfinding the most influential neuron path along which the information flows,over the existing baseline solutions. Additionally, the neuron paths haveillustrated that vision Transformers exhibit some specific inner workingmechanism for processing the visual information within the same image category.We further analyze the key effects of these neurons on the image classificationtask, showcasing that the found neuron paths have already preserved the modelcapability on downstream tasks, which may also shed some lights on real-worldapplications like model pruning. The project website including implementationcode is available at https://foundation-model-research.github.io/NeuronPath/.</description>
      <author>example@mail.com (Yifan Wang, Yifei Liu, Yingdong Shi, Changming Li, Anqi Pang, Sibei Yang, Jingyi Yu, Kan Ren)</author>
      <guid isPermaLink="false">2503.09046v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Zero-Shot Action Generalization with Limited Observations</title>
      <link>http://arxiv.org/abs/2503.08867v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AISTATS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种零样本框架AGLO，用于处理强化学习代理在面对未曾见过的动作时的泛化问题。&lt;h4&gt;背景&lt;/h4&gt;强化学习（RL）已成功应用于解决序列决策问题。然而，在现实场景中，当遇到训练期间未见的动作时，RL代理难以泛化。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够使用有限观察动作的数据来应对新行动的框架。&lt;h4&gt;方法&lt;/h4&gt;AGLO框架由两个主要组件组成：一个动作表示学习模块和一个策略学习模块。前者从有限的观测中提取出区分性的动作嵌入，后者利用学习到的动作表示以及增强合成的动作表示来习得能够处理未见过动作的任务的策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的框架在零样本动作泛化方面显著优于最先进的方法，在多个基准任务上展示了其有效性。&lt;h4&gt;结论&lt;/h4&gt;AGLO是一个有效的解决方案，它利用有限的数据观察有效地泛化到新的未知动作中。&lt;h4&gt;翻译&lt;/h4&gt;强化学习（RL）在解决序列决策问题时表现出色。然而，在现实世界的情景下，当面对训练期间未见过的新动作时，RL代理通常难以进行泛化。一些先前关于零样本动作泛化的研究依赖于大量动作观测数据集来捕捉新动作的行为，这使得它们对于实际应用来说不切实际。在本文中，我们提出了一种新的零样本框架——从有限观察中的动作概括（AGLO）。该框架包含两个主要部分：一个动作表示学习模块和策略学习模块。前者通过有限的观察提取出区分性的动作嵌入，后者利用这些学习到的动作表示以及增强合成的动作表示来习得能够处理未见过动作的任务的策略。实验结果表明，在多个基准任务上，我们的框架在零样本动作泛化方面显著优于最先进的方法，展示了其应对新动作的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) has demonstrated remarkable success in solvingsequential decision-making problems. However, in real-world scenarios, RLagents often struggle to generalize when faced with unseen actions that werenot encountered during training. Some previous works on zero-shot actiongeneralization rely on large datasets of action observations to capture thebehaviors of new actions, making them impractical for real-world applications.In this paper, we introduce a novel zero-shot framework, Action Generalizationfrom Limited Observations (AGLO). Our framework has two main components: anaction representation learning module and a policy learning module. The actionrepresentation learning module extracts discriminative embeddings of actionsfrom limited observations, while the policy learning module leverages thelearned action representations, along with augmented synthetic actionrepresentations, to learn a policy capable of handling tasks with unseenactions. The experimental results demonstrate that our framework significantlyoutperforms state-of-the-art methods for zero-shot action generalization acrossmultiple benchmark tasks, showcasing its effectiveness in generalizing to newactions with minimal action observations.</description>
      <author>example@mail.com (Abdullah Alchihabi, Hanping Zhang, Yuhong Guo)</author>
      <guid isPermaLink="false">2503.08867v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Incomplete Multi-view Clustering via Diffusion Contrastive Generation</title>
      <link>http://arxiv.org/abs/2503.09185v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍一种用于不完整多视图聚类的新型方法Diffusion Contrastive Generation (DCG)。&lt;h4&gt;背景&lt;/h4&gt;在处理包含缺失数据的多视图数据集时，不完整的多视图聚类(IMVC)面临着挑战。现有的基于插补的方法虽然取得了一些改进，但仍然存在两个主要问题：1）依赖于成对数据来训练数据恢复模块，在实际场景中由于大量数据缺失而难以实现；2）生成的数据多样性不足且鉴别能力较差。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的IMVC方法DCG，旨在解决现有基于插补方法的局限性，并改进多视图聚类效果。&lt;h4&gt;方法&lt;/h4&gt;通过结合扩散和去噪过程于单视图数据中来增强聚类性能；进行对比学习以使生成的视图与真实视图对齐；利用实例级别和类别级别的交互式学习，充分利用多视图数据中的信息一致性及互补性，实现稳健且端到端的聚类。&lt;h4&gt;主要发现&lt;/h4&gt;DCG方法能够准确地恢复在各种缺失情形下的视图，并通过综合利用多视图数据中的特征，显著提升了聚类效果。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，与现有最先进的IMVC方法相比，所提出的DCG方法表现出色，在不完整多视图数据集上的聚类性能优越。&lt;h4&gt;翻译&lt;/h4&gt;近年来，由于多视图数据集中普遍存在的缺失数据问题，不完全多视图聚类(IMVC)引起了越来越多的关注。解决这一挑战的主要途径是在应用传统多视图聚类方法之前恢复缺失的视图。尽管基于插补的方法在一定程度上取得了显著改进，但它们仍存在两个主要局限：1）严重依赖于成对数据来训练用于数据恢复的模块，在实际场景中由于高比例的数据缺失而难以实现；2）生成的数据往往缺乏多样性和辨别力，导致聚类结果不佳。为了解决这些问题，我们提出了一种新颖的IMVC方法——扩散对比生成(DCG)。受扩散与聚类过程之间一致性的启发，DCG通过向单视图数据中应用前向扩散和反向去噪过程来学习分布特征以增强聚类性能。通过对有限数量的多视图成对样本进行对比学习，DCG能够使生成的视图与真实视图相匹配，在各种缺失情形下实现准确的数据恢复。此外，DCG整合了实例级别及类别级别的交互式学习机制，利用多视图数据中的一致性和互补性信息，实现了稳健且端到端的聚类效果。广泛实验表明我们的方法优于现有最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Incomplete multi-view clustering (IMVC) has garnered increasing attention inrecent years due to the common issue of missing data in multi-view datasets.The primary approach to address this challenge involves recovering the missingviews before applying conventional multi-view clustering methods. Althoughimputation-based IMVC methods have achieved significant improvements, theystill encounter notable limitations: 1) heavy reliance on paired data fortraining the data recovery module, which is impractical in real scenarios withhigh missing data rates; 2) the generated data often lacks diversity anddiscriminability, resulting in suboptimal clustering results. To address theseshortcomings, we propose a novel IMVC method called Diffusion ContrastiveGeneration (DCG). Motivated by the consistency between the diffusion andclustering processes, DCG learns the distribution characteristics to enhanceclustering by applying forward diffusion and reverse denoising processes tointra-view data. By performing contrastive learning on a limited set of pairedmulti-view samples, DCG can align the generated views with the real views,facilitating accurate recovery of views across arbitrary missing viewscenarios. Additionally, DCG integrates instance-level and category-levelinteractive learning to exploit the consistent and complementary informationavailable in multi-view data, achieving robust and end-to-end clustering.Extensive experiments demonstrate that our method outperforms state-of-the-artapproaches.</description>
      <author>example@mail.com (Yuanyang Zhang, Yijie Lin, Weiqing Yan, Li Yao, Xinhang Wan, Guangyuan Li, Chao Zhang, Guanzhou Ke, Jie Xu)</author>
      <guid isPermaLink="false">2503.09185v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Seal Your Backdoor with Variational Defense</title>
      <link>http://arxiv.org/abs/2503.08829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为VIBE的模型无关框架，用于训练对后门攻击具有抵抗力的分类器。&lt;h4&gt;背景&lt;/h4&gt;在深度学习中，训练数据可能会受到恶意输入和被篡改标签的影响，这些因素会降低模型性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效抵御这种类型的对抗性样本影响的方法。&lt;h4&gt;方法&lt;/h4&gt;VIBE将恶意输入和受污染的标签视为观察到的随机变量，而真实的干净标签被视为潜在随机变量。通过变分推理来恢复相应的干净标签后验，并遵循期望最大算法（EM）进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证了VIBE在标准数据集、大规模设置及多个攻击组合的数据集中均表现出色，优于现有的防御措施。&lt;h4&gt;结论&lt;/h4&gt;VIBE框架提供了一种灵活的方法，可结合最近的自我监督表示学习进展以增强对后门攻击的抵抗力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种模型无关框架——VIBE，用于训练能够抵御后门攻击的分类器。该方法通过将恶意输入和受污染的标签视为观察随机变量来工作，而真实的干净标签则被视为潜在变量。利用变分推理恢复相应的清洁标签后验，并且遵循期望最大算法进行训练过程。实验结果显示，在各种测试场景中VIBE均优于现有的防御措施。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose VIBE, a model-agnostic framework that trains classifiers resilientto backdoor attacks. The key concept behind our approach is to treat maliciousinputs and corrupted labels from the training dataset as observed randomvariables, while the actual clean labels are latent. VIBE then recovers thecorresponding latent clean label posterior through variational inference. Theresulting training procedure follows the expectation-maximization (EM)algorithm. The E-step infers the clean pseudolabels by solving anentropy-regularized optimal transport problem, while the M-step updates theclassifier parameters via gradient descent. Being modular, VIBE can seamlesslyintegrate with recent advancements in self-supervised representation learning,which enhance its ability to resist backdoor attacks. We experimentallyvalidate the method effectiveness against contemporary backdoor attacks onstandard datasets, a large-scale setup with 1$k$ classes, and a datasetpoisoned with multiple attacks. VIBE consistently outperforms previous defensesacross all tested scenarios.</description>
      <author>example@mail.com (Ivan Sabolić, Matej Grcić, Siniša Šegvić)</author>
      <guid isPermaLink="false">2503.08829v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Domain Adaptation for Japanese Sentence Embeddings with Contrastive Learning based on Synthetic Sentence Generation</title>
      <link>http://arxiv.org/abs/2503.09094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种名为SDJC的方法，用于增强日语句子嵌入的领域适应能力。&lt;h4&gt;背景&lt;/h4&gt;许多预训练模型在通用数据集上可以生成有用的句子嵌入。然而，在低资源语言如日语中进行领域适应因缺乏大规模标注数据而变得困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来克服低资源语言领域适应中的挑战，特别是在日本语的场景下。&lt;h4&gt;方法&lt;/h4&gt;SDJC利用自监督学习和对比学习技术，通过生成具有相同句法结构但不同语义内容的句子来增强模型对特定领域的适应性。此外，构建了一个全面的日语STS基准数据集以评估该方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SDJC在两个特定领域下游任务上表现出色，并且所构建的数据集对于评估日语文本相似度具有重要价值。&lt;h4&gt;结论&lt;/h4&gt;SDJC为低资源语言的领域适应提供了一种有效的方法。相关的数据集、代码和经过SDJC调整的基础模型可以在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了如何通过自监督学习和对比学习技术来增强日语文本嵌入领域的适应性，特别是针对低资源场景下的挑战提出了解决方案，并展示了该方法在特定任务上的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Several backbone models pre-trained on general domain datasets can encode asentence into a widely useful embedding. Such sentence embeddings can befurther enhanced by domain adaptation that adapts a backbone model to aspecific domain. However, domain adaptation for low-resource languages likeJapanese is often difficult due to the scarcity of large-scale labeleddatasets. To overcome this, this paper introduces SDJC (Self-supervised Domainadaptation for Japanese sentence embeddings with Contrastive learning) thatutilizes a data generator to generate sentences, which have the same syntacticstructure to a sentence in an unlabeled specific domain corpus but conveydifferent semantic meanings. Generated sentences are then used to boostcontrastive learning that adapts a backbone model to accurately discriminatesentences in the specific domain. In addition, the components of SDJC like abackbone model and a method to adapt it need to be carefully selected, but nobenchmark dataset is available for Japanese. Thus, a comprehensive Japanese STS(Semantic Textual Similarity) benchmark dataset is constructed by combiningdatasets machine-translated from English with existing datasets. Theexperimental results validates the effectiveness of SDJC on two domain-specificdownstream tasks as well as the usefulness of the constructed dataset.Datasets, codes and backbone models adapted by SDJC are available on our githubrepository https://github.com/ccilab-doshisha/SDJC.</description>
      <author>example@mail.com (Zihao Chen, Hisashi Handa, Miho Ohsaki, Kimiaki Shirahama)</author>
      <guid isPermaLink="false">2503.09094v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Disentangled World Models: Learning to Transfer Semantic Knowledge from Distracting Videos for Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2503.08751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;在实际场景中训练视觉强化学习（RL）面临重大挑战，即环境变化导致代理采样效率低下。&lt;h4&gt;目的&lt;/h4&gt;提出一种通过离线到在线潜在蒸馏和灵活解纠缠约束从分散视频中学到并理解底层语义差异的方法。&lt;h4&gt;方法&lt;/h4&gt;介绍了一种可解释的基于模型的RL框架，称为Disentangled World Models (DisWM)。首先，利用去相关正则化在没有动作的情况下离线预训练视频预测模型以提取分散视频中的语义知识；然后通过潜在蒸馏将预训练模型的解纠缠能力转移到世界模型中；在线环境中进行微调时，引入来自预训练模型的知识，并向世界模型添加了解纠缠约束。&lt;h4&gt;主要发现&lt;/h4&gt;提出的框架在各种基准测试上表现出色。特别是，在适应阶段，线上环境交互的动作和奖励增强了数据多样性，进而加强了分离表示学习。&lt;h4&gt;结论&lt;/h4&gt;通过跨域语义知识转移的可解释模型实现了视觉强化学习在实际场景中的有效训练，并提高了样本效率。&lt;h4&gt;翻译&lt;/h4&gt;训练视觉强化学习（RL）以应对实际场景提出了重要挑战。现有方法试图通过解纠缠表征学习缓解低采样效率问题，但这些方法通常从零开始，在没有世界先验知识的情况下进行学习。本文提出了一种新的框架Disentangled World Models (DisWM)，利用离线到在线潜在蒸馏和灵活的解纠缠约束来理解分散视频中的语义变化。这种方法通过可解释模型实现了跨域语义知识转移，从而提高了视觉RL在实际场景中的训练效率。实验结果证明了该方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training visual reinforcement learning (RL) in practical scenarios presents asignificant challenge, $\textit{i.e.,}$ RL agents suffer from low sampleefficiency in environments with variations. While various approaches haveattempted to alleviate this issue by disentanglement representation learning,these methods usually start learning from scratch without prior knowledge ofthe world. This paper, in contrast, tries to learn and understand underlyingsemantic variations from distracting videos via offline-to-online latentdistillation and flexible disentanglement constraints. To enable effectivecross-domain semantic knowledge transfer, we introduce an interpretablemodel-based RL framework, dubbed Disentangled World Models (DisWM).Specifically, we pretrain the action-free video prediction model offline withdisentanglement regularization to extract semantic knowledge from distractingvideos. The disentanglement capability of the pretrained model is thentransferred to the world model through latent distillation. For finetuning inthe online environment, we exploit the knowledge from the pretrained model andintroduce a disentanglement constraint to the world model. During theadaptation phase, the incorporation of actions and rewards from onlineenvironment interactions enriches the diversity of the data, which in turnstrengthens the disentangled representation learning. Experimental resultsvalidate the superiority of our approach on various benchmarks.</description>
      <author>example@mail.com (Qi Wang, Zhipeng Zhang, Baao Xie, Xin Jin, Yunbo Wang, Shiyu Wang, Liaomo Zheng, Xiaokang Yang, Wenjun Zeng)</author>
      <guid isPermaLink="false">2503.08751v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Reconstruct Anything Model: a lightweight foundation model for computational imaging</title>
      <link>http://arxiv.org/abs/2503.08915v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个非迭代、轻量级的架构来解决成像逆问题，该模型结合了关于正向操作的知识（如获取物理和噪声参数），能够处理包括去模糊、MRI、CT、图像修复和超分辨率在内的多种逆问题。&lt;h4&gt;背景&lt;/h4&gt;现有的基于学习的方法用于解决成像逆问题可以大致分为两类：一类是迭代算法，利用预训练的去噪器；另一类是端到端训练的卷积神经网络架构。前者计算成本高且重构性能次优，后者则通常仅针对单个逆问题，并需要昂贵的训练。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的非迭代、轻量级模型，该模型结合了关于正向操作的知识而无需依赖于卷积神经网络解卷过程，可以解决包括去模糊、MRI、CT、图像修复和超分辨率在内的多种逆问题，并能在少量微调步骤下适应未知的逆问题或数据集。&lt;h4&gt;方法&lt;/h4&gt;设计了一种基于知识集成而非迭代训练的方法来处理广泛的成像逆问题。该模型能通过自监督的方式进行微调，而无需真实参考。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在一系列实验中展示了从医学影像到低光子数成像和显微镜等领域的最先进的性能表现。&lt;h4&gt;结论&lt;/h4&gt;与现有方法相比，这种非迭代、轻量级的架构不仅提高了处理效率还扩展了解决问题的范围，并能以低成本适应新的逆问题或数据集。&lt;h4&gt;翻译&lt;/h4&gt;大多数现有的用于解决成像逆问题的学习方法可以大致分为两类：一类是利用预训练去噪器的迭代算法（如插件播放和扩散方法），另一类是为特定成像问题进行端到端训练的卷积神经网络架构。前者计算成本高且提供次优重构性能，后者通常针对单一逆问题，并需要昂贵的训练。在本工作中，我们提出了一种新的非迭代、轻量级架构，该模型结合了关于正向操作的知识（获取物理和噪声参数），而无需依赖卷积神经网络解卷过程。我们的模型被训练用于解决包括去模糊、MRI、CT、图像修复和超分辨率在内的广泛的逆问题，并能够通过少量自监督的微调步骤适应未见过的逆问题或数据集，不需要真实参考。在一系列实验中，我们展示了从医学影像到低光子数成像和显微镜等领域的最先进的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing learning-based methods for solving imaging inverse problems canbe roughly divided into two classes: iterative algorithms, such asplug-and-play and diffusion methods, that leverage pretrained denoisers, andunrolled architectures that are trained end-to-end for specific imagingproblems. Iterative methods in the first class are computationally costly andoften provide suboptimal reconstruction performance, whereas unrolledarchitectures are generally specific to a single inverse problem and requireexpensive training. In this work, we propose a novel non-iterative, lightweightarchitecture that incorporates knowledge about the forward operator(acquisition physics and noise parameters) without relying on unrolling. Ourmodel is trained to solve a wide range of inverse problems beyond denoising,including deblurring, magnetic resonance imaging, computed tomography,inpainting, and super-resolution. The proposed model can be easily adapted tounseen inverse problems or datasets with a few fine-tuning steps (up to a fewimages) in a self-supervised way, without ground-truth references. Throughout aseries of experiments, we demonstrate state-of-the-art performance from medicalimaging to low-photon imaging and microscopy.</description>
      <author>example@mail.com (Matthieu Terris, Samuel Hurault, Maxime Song, Julian Tachella)</author>
      <guid isPermaLink="false">2503.08915v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Towards Interpretable Protein Structure Prediction with Sparse Autoencoders</title>
      <link>http://arxiv.org/abs/2503.08764v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at the GEMBio ICLR 2025 Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;蛋白质语言模型在结构预测方面取得了革命性的进展，但由于其非线性特性，人们难以理解序列表示如何影响结构预测。虽然稀疏自编码器（SAEs）可以通过在高维空间中学习线性表示来提供可解释性路径，但它们的应用仅限于较小的蛋白质语言模型，这些模型无法进行结构预测。&lt;h4&gt;背景&lt;/h4&gt;当前蛋白质语言模型由于其非线性特性，在理解序列与结构之间的关系上存在困难。稀疏自编码器（SAEs）可以为这一问题提供解决方案，但是现有的SAE只能应用于小型模型，并且不能用于结构预测任务。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过扩展SAEs的应用范围至大型蛋白质语言模型ESM2-3B来解决上述问题，并进一步改进这种技术以适用于更复杂的蛋白质语言模型。&lt;h4&gt;方法&lt;/h4&gt;(1) 扩展了稀疏自编码器（SAEs）的规模，使之能够应用于ESM2-3B这个用于结构预测的基础模型上。这是首次实现对蛋白结构预测机制性的可解释性。(2) 对Matryoshka SAE进行了适应性改进，使其可以学习具有层次组织特征的方式，通过强制嵌套隐变量组独立重构输入来实现。&lt;h4&gt;主要发现&lt;/h4&gt;(1) 改进后的Matryoshka SAEs在生物概念发现和接触图预测方面展示了与标准架构相当或更好的性能。(2) 证明了基于ESM2-3B训练的SAEs在此类任务上的表现显著优于基于小型模型训练的结果。&lt;h4&gt;结论&lt;/h4&gt;本文的研究为蛋白质结构预测机制的理解提供了新的视角，并通过开源代码、数据集和预训练模型支持进一步的研究。初步案例研究展示了该方法如何定向影响ESMFold预测，增加结构溶剂可及性的同时保持输入序列不变。&lt;h4&gt;翻译&lt;/h4&gt;蛋白质语言模型已经彻底改变了结构预测领域，但它们的非线性本质使得我们难以理解序列表示是如何影响结构预测的。虽然稀疏自编码器（SAEs）可以通过在高维空间中学习线性表示来提供解释性的途径，但它们的应用仅限于较小的蛋白语言模型，这些模型不能进行结构预测。在这项工作中，我们进行了两项关键改进：(1) 我们扩展了SAE到ESM2-3B，这是用于ESMFold的基础模型，首次实现了蛋白质结构预测机制上的可解释性。(2) 我们对Matryoshka SAEs进行了适应性的改造，使其适用于蛋白语言模型，通过强制嵌套的隐变量组独立重构输入来学习层级组织特征。我们展示了我们的Matryoshka SAEs在生物概念发现和接触图预测方面的表现与标准架构相当甚至更好。通过全面评估，证明了基于ESM2-3B训练的SAEs在上述任务中的性能优于基于小型模型训练的结果。最后，我们提供了一个初步案例研究来展示如何利用我们的方法定向影响ESMFold预测，在保持输入序列不变的情况下增加结构溶剂可及性。为了支持社区进一步的研究，我们将代码、数据集和预训练模型开源（https://github.com/johnyang101/reticular-sae ）以及可视化的工具（https://sae.reticular.ai）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein language models have revolutionized structure prediction, but theirnonlinear nature obscures how sequence representations inform structureprediction. While sparse autoencoders (SAEs) offer a path to interpretabilityhere by learning linear representations in high-dimensional space, theirapplication has been limited to smaller protein language models unable toperform structure prediction. In this work, we make two key advances: (1) wescale SAEs to ESM2-3B, the base model for ESMFold, enabling mechanisticinterpretability of protein structure prediction for the first time, and (2) weadapt Matryoshka SAEs for protein language models, which learn hierarchicallyorganized features by forcing nested groups of latents to reconstruct inputsindependently. We demonstrate that our Matryoshka SAEs achieve comparable orbetter performance than standard architectures. Through comprehensiveevaluations, we show that SAEs trained on ESM2-3B significantly outperformthose trained on smaller models for both biological concept discovery andcontact map prediction. Finally, we present an initial case study demonstratinghow our approach enables targeted steering of ESMFold predictions, increasingstructure solvent accessibility while fixing the input sequence. To facilitatefurther investigation by the broader community, we open-source our code,dataset, pretrained models https://github.com/johnyang101/reticular-sae , andvisualizer https://sae.reticular.ai .</description>
      <author>example@mail.com (Nithin Parsan, David J. Yang, John J. Yang)</author>
      <guid isPermaLink="false">2503.08764v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Hard negative sampling in hyperedge prediction</title>
      <link>http://arxiv.org/abs/2503.08743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的生成负样本的方法，即硬负采样（HNS），用于改进超图预测模型的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;多实体交互建模中，超图是一个有力工具。在简单的图形链接预测中，大多数观察到的链接被视为正样本，未观察到的链接视为负样本。但在超图预测任务中，由于未观测到的超边数量远远超过已观测的数量，这种方法变得不切实际。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以有效生成负样本的方法以提高超边预测模型的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的负采样技术——硬负采样（HNS）。与传统方法不同的是，HNS直接在超边嵌入空间中合成负样本，而不是从原始的超图中随机选取节点集合来构造负超边。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，相比于随机选择负样本的方法，使用HNS生成的负样本更加具有挑战性且信息量丰富，显著提高了预测模型的准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;该技术作为一种即插即用的方式，能够方便地应用于各种基于表示学习的超边预测模型中。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为：Hypergraph, which allows each hyperedge to encompass an arbitrary number of nodes, is a powerful tool for modeling multi-entity interactions. Hyperedge prediction is a fundamental task that aims to predict future hyperedges or identify existent but unobserved hyperedges based on those observed. In link prediction for simple graphs, most observed links are treated as positive samples, while all unobserved links are considered as negative samples. However, this full-sampling strategy is impractical for hyperedge prediction, due to the number of unobserved hyperedges in a hypergraph significantly exceeds the number of observed ones. Therefore, one has to utilize some negative sampling methods to generate negative samples, ensuring their quantity is comparable to that of positive samples. In current hyperedge prediction, randomly selecting negative samples is a routine practice. But through experimental analysis, we discover a critical limitation of random selecting that the generated negative samples are too easily distinguishable from positive samples. This leads to premature convergence of the model and reduces the accuracy of prediction. To overcome this issue, we propose a novel method to generate negative samples, named as hard negative sampling (HNS). Unlike traditional methods that construct negative hyperedges by selecting node sets from the original hypergraph, HNS directly synthesizes negative samples in the hyperedge embedding space, thereby generating more challenging and informative negative samples. Our results demonstrate that HNS significantly enhances both accuracy and robustness of the prediction. Moreover, as a plug-and-play technique, HNS can be easily applied in the training of various hyperedge prediction models based on representation learning.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hypergraph, which allows each hyperedge to encompass an arbitrary number ofnodes, is a powerful tool for modeling multi-entity interactions. Hyperedgeprediction is a fundamental task that aims to predict future hyperedges oridentify existent but unobserved hyperedges based on those observed. In linkprediction for simple graphs, most observed links are treated as positivesamples, while all unobserved links are considered as negative samples.However, this full-sampling strategy is impractical for hyperedge prediction,due to the number of unobserved hyperedges in a hypergraph significantlyexceeds the number of observed ones. Therefore, one has to utilize somenegative sampling methods to generate negative samples, ensuring their quantityis comparable to that of positive samples. In current hyperedge prediction,randomly selecting negative samples is a routine practice. But throughexperimental analysis, we discover a critical limitation of random selectingthat the generated negative samples are too easily distinguishable frompositive samples. This leads to premature convergence of the model and reducesthe accuracy of prediction. To overcome this issue, we propose a novel methodto generate negative samples, named as hard negative sampling (HNS). Unliketraditional methods that construct negative hyperedges by selecting node setsfrom the original hypergraph, HNS directly synthesizes negative samples in thehyperedge embedding space, thereby generating more challenging and informativenegative samples. Our results demonstrate that HNS significantly enhances bothaccuracy and robustness of the prediction. Moreover, as a plug-and-playtechnique, HNS can be easily applied in the training of various hyperedgeprediction models based on representation learning.</description>
      <author>example@mail.com (Zhenyu Deng, Tao Zhou, Yilin Bi)</author>
      <guid isPermaLink="false">2503.08743v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Decoupled Doubly Contrastive Learning for Cross Domain Facial Action Unit Detection</title>
      <link>http://arxiv.org/abs/2503.08977v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Image Processing 2025. A novel and  elegant feature decoupling method for cross-domain facial action unit  detection&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个名为D$^2$CA的方法，用于解决当前基于视觉的面部动作单元（AU）检测方法在不同领域中的表现不稳定问题。&lt;h4&gt;背景&lt;/h4&gt;现有的AU检测方法虽然性能出色，但容易受到跨域变化的影响，导致难以泛化到其他环境。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的适应性方法D$^2$CA来学习一个对源和目标域都语义一致的纯净AU表示。&lt;h4&gt;方法&lt;/h4&gt;将潜在表示分解为与AU相关的成分和无关的成分，并使用双重对比学习机制确保合成面部图像的质量，从而增强特征解耦。&lt;h4&gt;主要发现&lt;/h4&gt;D$^2$CA能够成功分离AU相关因素和领域相关因素，生成视觉上令人满意的跨域合成面部图像。实验表明它在各种跨域场景中超越了现有方法，平均F1分数提高6%-14%。&lt;h4&gt;结论&lt;/h4&gt;D$^2$CA提供了一种自动学习的方法，能够实现AU相关的和领域相关的因素的专门分离，并且能够在不同规模下进行直观控制。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TIP.2025.3546479&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the impressive performance of current vision-based facial action unit(AU) detection approaches, they are heavily susceptible to the variationsacross different domains and the cross-domain AU detection methods areunder-explored. In response to this challenge, we propose a decoupled doublycontrastive adaptation (D$^2$CA) approach to learn a purified AU representationthat is semantically aligned for the source and target domains. Specifically,we decompose latent representations into AU-relevant and AU-irrelevantcomponents, with the objective of exclusively facilitating adaptation withinthe AU-relevant subspace. To achieve the feature decoupling, D$^2$CA is trainedto disentangle AU and domain factors by assessing the quality of synthesizedfaces in cross-domain scenarios when either AU or domain attributes aremodified. To further strengthen feature decoupling, particularly in scenarioswith limited AU data diversity, D$^2$CA employs a doubly contrastive learningmechanism comprising image and feature-level contrastive learning to ensure thequality of synthesized faces and mitigate feature ambiguities. This newframework leads to an automatically learned, dedicated separation ofAU-relevant and domain-relevant factors, and it enables intuitive,scale-specific control of the cross-domain facial image synthesis. Extensiveexperiments demonstrate the efficacy of D$^2$CA in successfully decoupling AUand domain factors, yielding visually pleasing cross-domain synthesized facialimages. Meanwhile, D$^2$CA consistently outperforms state-of-the-artcross-domain AU detection approaches, achieving an average F1 score improvementof 6\%-14\% across various cross-domain scenarios.</description>
      <author>example@mail.com (Yong Li, Menglin Liu, Zhen Cui, Yi Ding, Yuan Zong, Wenming Zheng, Shiguang Shan, Cuntai Guan)</author>
      <guid isPermaLink="false">2503.08977v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Speaker-Aware Learning for Multi-party Dialogue Generation with LLMs</title>
      <link>http://arxiv.org/abs/2503.08842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为Speaker-Attentive LLM (SA-LLM)的新模型，该模型利用预训练的大规模语言模型（LLMs）和一种关注说话人的对比学习策略来生成高质量的多参与者对话。&lt;h4&gt;背景&lt;/h4&gt;传统的多参与者对话生成方法难以捕捉多个发言者之间复杂的交互关系，尤其是在依赖于手动注释的对话关系时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需人工标注关系的新模型SA-LLM，以解决当前多参与者对话生成中的挑战。&lt;h4&gt;方法&lt;/h4&gt;该模型通过说话人属性输入编码和对比学习目标来隐式地学习上下文连贯性和角色区分性。&lt;h4&gt;主要发现&lt;/h4&gt;在Ubuntu IRC和电影对话数据集上进行的大量实验表明，SA-LLM在自动评估和人工评价中都显著优于现有基线方法，在流利度、连贯性、信息量和响应多样性方面表现出色。消融研究和详细的错误分析进一步验证了该模型的有效性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;结果强调了SA-LLM作为一种强大且无需注释的解决方案在高质量多参与者对话生成中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文描述了一种用于解决传统方法难以处理多参与者的复杂交互关系问题的新模型，通过引入预训练的大规模语言模型和说话人感知对比学习策略，实现了更好的对话质量和多样性的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-party dialogue generation presents significant challenges due to thecomplex interplay of multiple speakers and interwoven conversational threads.Traditional approaches often fall short in capturing these complexities,particularly when relying on manually annotated dialogue relations. This paperintroduces Speaker-Attentive LLM (SA-LLM), a novel generative model thatleverages pre-trained Large Language Models (LLMs) and a speaker-awarecontrastive learning strategy to address these challenges. SA-LLM incorporatesa speaker-attributed input encoding and a contrastive learning objective toimplicitly learn contextual coherence and speaker roles without explicitrelation annotations. Extensive experiments on the Ubuntu IRC and MovieDialogues datasets demonstrate that SA-LLM significantly outperformsstate-of-the-art baselines in automatic and human evaluations, achievingsuperior performance in fluency, coherence, informativeness, and responsediversity. Ablation studies and detailed error analyses further validate theeffectiveness of the proposed speaker-attentive training approach, highlightingits robustness across different speaker roles and context lengths. The resultsunderscore the potential of SA-LLM as a powerful and annotation-free solutionfor high-quality multi-party dialogue generation.</description>
      <author>example@mail.com (Tianyu Sun, Kun Qian, Wenhong Wang)</author>
      <guid isPermaLink="false">2503.08842v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models</title>
      <link>http://arxiv.org/abs/2503.08737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;COD-VAE通过两阶段自编码器方案，实现了高质量的3D形状压缩，并在解码时大幅度减少了计算开销。&lt;h4&gt;背景&lt;/h4&gt;构建高效的3D扩散模型的关键在于通过变分自编码器（VAE）构造一个压缩的潜在空间。&lt;h4&gt;目的&lt;/h4&gt;介绍COD-VAE，一种能够将3D形状编码为1D潜变量集合而不牺牲质量的VAE。&lt;h4&gt;方法&lt;/h4&gt;{'两阶段自动编码方案': '首先，编码块逐步通过中间点补丁将点云压缩成紧凑的潜在向量；其次，基于三平面的解码器从潜在向量中重建密集的三平面而不是直接解码神经场，从而显著减少了解码过程中的计算开销。', '不确定性引导的令牌修剪': '分配资源以跳过简单区域的计算，提高了解码效率。'}&lt;h4&gt;主要发现&lt;/h4&gt;与基线相比，COD-VAE实现了16倍的压缩率，并且在保持质量的同时使得生成速度提高了20.8倍。&lt;h4&gt;结论&lt;/h4&gt;大量的潜在向量并非高质量重建和生成的必要条件，高效的编码解码方案可以显著提升3D形状处理效率。&lt;h4&gt;翻译&lt;/h4&gt;构建一个通过变分自编码器（VAE）实现压缩潜空间是高效3D扩散模型的关键。本文引入了COD-VAE，这是一种将3D形状编码为紧凑的一维潜在向量集合而不牺牲质量的VAE。COD-VAE采用两阶段自动编码方案以提高压缩和解码效率：首先，编码块通过中间点补丁逐步将点云压缩成紧凑的潜在向量；其次，基于三平面的解码器从潜在向量中重建密集的三平面而不是直接解码神经场，从而显著减少了解码过程中的计算开销。此外，本文还提出了不确定性引导的令牌修剪方法，在简单区域跳过计算以优化资源分配，进一步提高了解码效率。实验结果表明，COD-VAE相较于基线模型实现了16倍的压缩率，并且保持了质量的前提下将生成速度提升了20.8倍，这证明了大量的潜在向量并非是高质量重建和生成的必要条件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Constructing a compressed latent space through a variational autoencoder(VAE) is the key for efficient 3D diffusion models. This paper introducesCOD-VAE, a VAE that encodes 3D shapes into a COmpact set of 1D latent vectorswithout sacrificing quality. COD-VAE introduces a two-stage autoencoder schemeto improve compression and decoding efficiency. First, our encoder blockprogressively compresses point clouds into compact latent vectors viaintermediate point patches. Second, our triplane-based decoder reconstructsdense triplanes from latent vectors instead of directly decoding neural fields,significantly reducing computational overhead of neural fields decoding.Finally, we propose uncertainty-guided token pruning, which allocates resourcesadaptively by skipping computations in simpler regions and improves the decoderefficiency. Experimental results demonstrate that COD-VAE achieves 16xcompression compared to the baseline while maintaining quality. This enables20.8x speedup in generation, highlighting that a large number of latent vectorsis not a prerequisite for high-quality reconstruction and generation.</description>
      <author>example@mail.com (In Cho, Youngbeom Yoo, Subin Jeon, Seon Joo Kim)</author>
      <guid isPermaLink="false">2503.08737v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>PersonaBooth: Personalized Text-to-Motion Generation</title>
      <link>http://arxiv.org/abs/2503.07390v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;本文介绍了一种新的任务，即利用包含人格特征的几个基础动作来生成与文本描述相一致的个性化动作。为了支持这一新任务，作者引入了一个名为PerMo（PersonaMotion）的新大规模运动数据集，该数据集中捕捉了多个演员的独特人格特点。此外，提出了一种多模态微调方法，即预训练的运动扩散模型PersonaBooth，用于应对两个主要挑战：i) 在专注于角色的人格特征的数据集与缺乏个性特定数据的预训练数据集之间存在显著分布差距；ii) 从内容各异的动作中捕捉一致人格的困难。为了克服数据集分布差距问题，提出了一种接纳新的人格特征并进行文本和视觉多模态适应的方法。为捕捉一致性人格，引入对比学习技术以增强具有相同人物样本内部的一致性。此外，还介绍了一种上下文感知融合机制来最大化从多个输入动作中集成的人物线索。PersonaBooth在运动风格迁移方法上超越了当前最佳方法，确立了一个新的个性化运动基准。&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新任务Motion Personalization，并开发了一套相应的数据集和模型。&lt;h4&gt;背景&lt;/h4&gt;传统预训练数据集缺乏角色特定信息，导致难以生成具有个人特点的运动。现有的运动风格迁移方法在捕捉一致人格方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;通过引入PerMo数据集及PersonaBooth模型来解决上述问题，从而推动个性化动作生成领域的发展。&lt;h4&gt;方法&lt;/h4&gt;1. 创建新的大规模运动数据集PerMo，该数据集中包含了角色特定信息；2. 提出一种预训练的多模态微调方法PersonaBooth；3. 引入对比学习技术以增强具有相同人物样本内部的一致性；4. 介绍上下文感知融合机制来最大化从多个输入动作中集成的人物线索。&lt;h4&gt;主要发现&lt;/h4&gt;PersonaBooth在运动风格迁移方面超越了现有最佳方法，确立了一个新的个性化运动生成基准。&lt;h4&gt;结论&lt;/h4&gt;通过整合数据集和模型的改进，使得能够更有效地捕捉并反映人物特征的个性化动画生成成为了可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces Motion Personalization, a new task that generatespersonalized motions aligned with text descriptions using several basic motionscontaining Persona. To support this novel task, we introduce a new large-scalemotion dataset called PerMo (PersonaMotion), which captures the unique personasof multiple actors. We also propose a multi-modal finetuning method of apretrained motion diffusion model called PersonaBooth. PersonaBooth addressestwo main challenges: i) A significant distribution gap between thepersona-focused PerMo dataset and the pretraining datasets, which lackpersona-specific data, and ii) the difficulty of capturing a consistent personafrom the motions vary in content (action type). To tackle the datasetdistribution gap, we introduce a persona token to accept new persona featuresand perform multi-modal adaptation for both text and visuals during finetuning.To capture a consistent persona, we incorporate a contrastive learningtechnique to enhance intra-cohesion among samples with the same persona.Furthermore, we introduce a context-aware fusion mechanism to maximize theintegration of persona cues from multiple input motions. PersonaBoothoutperforms state-of-the-art motion style transfer methods, establishing a newbenchmark for motion personalization.</description>
      <author>example@mail.com (Boeun Kim, Hea In Jeong, JungHoon Sung, Yihua Cheng, Jeongmin Lee, Ju Yong Chang, Sang-Il Choi, Younggeun Choi, Saim Shin, Jungho Kim, Hyung Jin Chang)</author>
      <guid isPermaLink="false">2503.07390v2</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment</title>
      <link>http://arxiv.org/abs/2503.09594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025. 1st Place @ CARLA Challenge 2024. Challenge tech report  (preliminary version of SimLingo): arXiv:2406.10165&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'背景': '将大型语言模型（LLMs）集成到自动驾驶系统中引起了广泛关注，期望能够提高泛化能力和可解释性。', '目的': '提出一种可以处理闭合环路驾驶、视觉-语言理解以及语言动作一致性对齐的模型。', '方法': '该模型名为SimLingo，基于视觉语言模型（VLM），仅使用相机数据而不依赖昂贵的传感器如激光雷达。', '主要发现': '在CARLA仿真器上进行Bench2Drive基准测试时，SimLingo达到了业界最佳性能，并且在多种与语言相关的任务中取得了强劲表现。', '结论': '该模型不仅展示了强大的视觉-语言理解能力，还能保持高水平的驾驶性能。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating large language models (LLMs) into autonomous driving hasattracted significant attention with the hope of improving generalization andexplainability. However, existing methods often focus on either driving orvision-language understanding but achieving both high driving performance andextensive language understanding remains challenging. In addition, the dominantapproach to tackle vision-language understanding is using visual questionanswering. However, for autonomous driving, this is only useful if it isaligned with the action space. Otherwise, the model's answers could beinconsistent with its behavior. Therefore, we propose a model that can handlethree different tasks: (1) closed-loop driving, (2) vision-languageunderstanding, and (3) language-action alignment. Our model SimLingo is basedon a vision language model (VLM) and works using only camera, excludingexpensive sensors like LiDAR. SimLingo obtains state-of-the-art performance onthe widely used CARLA simulator on the Bench2Drive benchmark and is the winningentry at the CARLA challenge 2024. Additionally, we achieve strong results in awide variety of language-related tasks while maintaining high drivingperformance.</description>
      <author>example@mail.com (Katrin Renz, Long Chen, Elahe Arani, Oleg Sinavski)</author>
      <guid isPermaLink="false">2503.09594v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Evolution of Adaptive Force Chains in Reconfigurable Granular Metamaterials</title>
      <link>http://arxiv.org/abs/2503.09564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了通过改变颗粒模量来控制力链的技术。&lt;h4&gt;背景&lt;/h4&gt;颗粒堆积在外部负载作用下形成力链，这种现象依赖于接触网络和颗粒的模量。已有研究发现可以通过调节单个粒子的杨氏模量来直接控制这些力链。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过使用可变模量（VM）颗粒，在不同条件下优化颗粒堆积的机械响应，并探索如何使机器人颗粒超材料能够根据环境变化动态调整其力学性能或执行特定任务。&lt;h4&gt;方法&lt;/h4&gt;采用含有低熔点金属合金的核心和硅胶外壳构成VM颗粒，通过电加热软化内部合金，改变颗粒硬度。结合离散元法仿真与进化算法预测最优的模量分布，实验验证了二维堆积中力链输出的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;使用不同刚性和柔性的颗粒进行组合，在特定位置施加负载时可以观察到可预期的力量输出模式；通过调整电流大小和持续时间控制单个VM颗粒软化程度，并测量边界上力的输出。&lt;h4&gt;结论&lt;/h4&gt;研究结果展示了机器人颗粒超材料在动态适应环境条件或执行指定任务方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在外加负载下，颗粒堆积形成依赖于接触网络及颗粒模量的力链。这项工作探讨了可变模量（VM）颗粒，在这种情况下可以通过改变颗粒中的单个粒子的杨氏模量来控制力链。每个VM颗粒由硅胶外壳制成并包裹着低熔点金属合金核心，通过铜加热器产生的焦耳热将其融化以软化颗粒。当颗粒冷却至室温时，合金重新固化，并恢复其初始模量。为了优化包含刚性和柔性颗粒的堆积体的机械响应，结合进化算法和离散元方法模拟预测了可产生特定边界力输出的粒子模量模式。利用二维VM颗粒组装并通过光弹性技术测量了装配边界处的实际力输出，这些研究朝着制造能够根据不同环境条件动态调整力学性能或执行特定任务的机器人颗粒超材料方向迈进了一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Under an externally applied load, granular packings form force chains thatdepend on the contact network and moduli of the grains. In this work, weinvestigate packings of variable modulus (VM) particles, where we can directforce chains by changing the Young's modulus of individual particles within thepacking on demand. Each VM particle is made of a silicone shell thatencapsulates a core made of a low-melting-point metallic alloy (Field's metal).By sending an electric current through a co-located copper heater, the Field'smetal internal to each particle can be melted via Joule heating, which softensthe particle. As the particle cools to room temperature, the alloy solidifiesand the particle recovers its original modulus. To optimize the mechanicalresponse of granular packings containing both soft and stiff particles, weemploy an evolutionary algorithm coupled with discrete element methodsimulations to predict the patterns of particle moduli that will yield specificforce outputs on the assembly boundaries. The predicted patterns of particlemoduli from the simulations were realized in experiments using 2D assemblies ofVM particles and the force outputs on the assembly boundaries were measuredusing photoelastic techniques. These studies represent a step towards makingrobotic granular metamaterials that can dynamically adapt their mechanicalproperties in response to different environmental conditions or performspecific tasks on demand.</description>
      <author>example@mail.com (Sven Witthaus, Atoosa Parsa, Dong Wang, Nidhi Pashin, Jerry Zhang, Arthur K. MacKeith, Mark D. Shattuck, Josh Bongard, Corey S. O'Hern, Rebecca Kramer-Bottiglio)</author>
      <guid isPermaLink="false">2503.09564v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Action-Aware Pro-Active Safe Exploration for Mobile Robot Mapping</title>
      <link>http://arxiv.org/abs/2503.09515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 10 figures, 4 algorithms, preprint version of a paper  submitted to a journal publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;安全自主探索未知环境是移动机器人执行有效且自适应的环境地图绘制任务的重要技能。&lt;h4&gt;背景&lt;/h4&gt;现有的大多数探索方法依赖于标准边界基线（frontier-based）探索策略，该策略指导机器人到达已知安全区域与未探索空间之间的边界以获取关于环境的新信息。这种方法通常遵循循环持续规划策略：首先选择具有信息价值的边界位置，然后将机器人移动到选定的位置直到到达为止，并重复这些步骤直至终止。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的主动预防重新规划策略，在使用现有的可操作信息避免冗余、非信息性的末段探索动作的同时进行有效探索。同时利用视角的可操作信息作为系统的停止标准来执行安全且具有信息量路径规划，以最小化与检测到的障碍物碰撞的风险和距离未探索区域的距离。&lt;h4&gt;方法&lt;/h4&gt;提出了一个基于行动感知的观点选择策略，通过最大化每单位导航成本的信息效用并结合安全且具有信息量的路径规划方法来进行有效的主动预防重新规划。该方法在数值模拟和硬件实验中得到了验证。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够在避免冗余探索动作的同时提供有效且自适应的探索能力，并展示了相对于传统策略的优势，尤其是在处理连续更新的地图时能够实现良好的性能平衡。&lt;h4&gt;结论&lt;/h4&gt;本文介绍了一种新的预防性重新规划策略，旨在通过使用即时可用的信息来提高自主探索的有效性和效率。该方法在数值和硬件实验中均显示出了优越的表现，证明了其作为持续规划与在线规划替代方案的可行性。&lt;h4&gt;翻译&lt;/h4&gt;安全未知环境中的自动探索是移动机器人有效执行自适应环境映射任务的重要技能。由于边界基线（frontier-based）策略简单易行，大多数现有方法依赖于它来指导机器人前往已知的安全区域和未探索空间之间的交界获取新的信息。然而，这种方法缺乏对持续更新的地图进行自适应调整的能力。同时，在线规划虽然能够提供高度的适应性，但其计算成本高昂且存在可能导致程序卡死的风险。因此，本文提出了一种新的预防式重新规划策略：利用即时可用的信息来避免冗余、无价值的最后一英里探索行动。此外，该方法还使用视点的可操作信息作为系统的终止标准，并通过安全和信息量路径规划最小化碰撞风险与到达未探索区域的距离。最后，在数值模拟和硬件实验中展示了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safe autonomous exploration of unknown environments is an essential skill formobile robots to effectively and adaptively perform environmental mapping fordiverse critical tasks. Due to its simplicity, most existing explorationmethods rely on the standard frontier-based exploration strategy, which directsa robot to the boundary between the known safe and the unknown unexploredspaces to acquire new information about the environment. This typically followsa recurrent persistent planning strategy, first selecting an informativefrontier viewpoint, then moving the robot toward the selected viewpoint untilreaching it, and repeating these steps until termination. However, explorationwith persistent planning may lack adaptivity to continuously updated maps,whereas highly adaptive exploration with online planning often suffers fromhigh computational costs and potential issues with livelocks. In this paper, asan alternative to less-adaptive persistent planning and costly online planning,we introduce a new proactive preventive replanning strategy for effectiveexploration using the immediately available actionable information at aviewpoint to avoid redundant, uninformative last-mile exploration motion. Wealso use the actionable information of a viewpoint as a systematic terminationcriterion for exploration. To close the gap between perception and action, weperform safe and informative path planning that minimizes the risk of collisionwith detected obstacles and the distance to unexplored regions, and we applyaction-aware viewpoint selection with maximal information utility per totalnavigation cost. We demonstrate the effectiveness of our action-aware proactiveexploration method in numerical simulations and hardware experiments.</description>
      <author>example@mail.com (Aykut İşleyen, René van de Molengraft, Ömür Arslan)</author>
      <guid isPermaLink="false">2503.09515v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Neural reservoir control of a soft bio-hybrid arm</title>
      <link>http://arxiv.org/abs/2503.09477v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages; 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过结合工程学和生物学，提出了一种用于控制仿生混合模型手臂的方法。这种方法使用神经库（reservoir）来实现动态控制，并展示了其在一系列复杂任务中的优越性能。&lt;h4&gt;背景&lt;/h4&gt;软机器人的控制是一个长期存在的工程技术问题，因为它们具有高度非线性、异质性和分布特性。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于神经库的解决方案，用于实现生物混合模型手臂（由多个肌肉-肌腱组包裹弹性脊柱构成）的动态控制。&lt;h4&gt;方法&lt;/h4&gt;采用了神经库的方法来同时进行控制和自我建模。通过在类脑硬件上实施脉冲神经库，实现了能量效率的显著提高。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在一系列挑战性任务中表现出色，性能优于传统的神经网络方法，并且通过使用类脑硬件实现脉冲神经库，能效提高了近两个数量级。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了利用生物启发的方法来解决软机器人控制问题的潜力，并为未连接的、小型化的软机器人的在机控制提供了可能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：长久以来的一个工程学难题是软机器人的控制因它们的高度非线性、异质性和分布特性而变得非常困难。本文通过结合工程和生物学，提出了一种神经库方法用于动态控制一种由多个肌肉-肌腱组包裹弹性脊柱构成的生物混合模型手臂。我们展示了如何使用这些神经库可以同时实现对一组挑战性任务的有效控制和自我建模，并在性能上超过了经典的神经网络方法。此外，通过在类脑硬件上实施脉冲神经库，实现了近两个数量级的能量效率提升，这对于未连接的小型化软机器人的在机控制具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A long-standing engineering problem, the control of soft robots is difficultbecause of their highly non-linear, heterogeneous, anisotropic, and distributednature. Here, bridging engineering and biology, a neural reservoir is employedfor the dynamic control of a bio-hybrid model arm made of multiplemuscle-tendon groups enveloping an elastic spine. We show how the use ofreservoirs facilitates simultaneous control and self-modeling across a set ofchallenging tasks, outperforming classic neural network approaches. Further, byimplementing a spiking reservoir on neuromorphic hardware, energy efficiency isachieved, with nearly two-orders of magnitude improvement relative to standardCPUs, with implications for the on-board control of untethered, small-scalesoft robots.</description>
      <author>example@mail.com (Noel Naughton, Arman Tekinalp, Keshav Shivam, Seung Hung Kim, Volodymyr Kindratenko, Mattia Gazzola)</author>
      <guid isPermaLink="false">2503.09477v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Online Language Splatting</title>
      <link>http://arxiv.org/abs/2503.09447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为Online Language Splatting的新框架，该框架能够在不依赖预生成语言特征的情况下，在3DGS-SLAM系统中实现在线、接近实时的开放式词汇语言映射。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通过将语言功能集成到具有几何细节的3D场景表示中取得了进展，但这些方法需要对每个输入图像进行计算密集型的离线预处理，限制了其适应新环境的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种在线框架，在不牺牲计算速度、内存使用量和渲染质量的情况下，实现语言功能与3D表示的有效融合，并保持开放式词汇能力。&lt;h4&gt;方法&lt;/h4&gt;(1) 设计了一种高分辨率CLIP嵌入模块，能够以每帧18ms的速度生成详细的语言特征图。(2) 构建了两阶段在线自编码器，将768维的CLIP功能压缩到15维的同时保持开放式词汇能力。(3) 采用颜色-语言分离优化方法提升渲染质量。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在线方法不仅在准确性上超过了现有的离线方法，而且效率提高了40倍以上，展示了其在动态和交互式AI应用中的潜力。&lt;h4&gt;结论&lt;/h4&gt;该工作为实现人类语言与3D空间表示的实时对齐提供了一种新的途径，并且对于开发能够与环境和用户进行无缝互动的智能代理具有重要的意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To enable AI agents to interact seamlessly with both humans and 3Denvironments, they must not only perceive the 3D world accurately but alsoalign human language with 3D spatial representations. While prior work has madesignificant progress by integrating language features into geometricallydetailed 3D scene representations using 3D Gaussian Splatting (GS), theseapproaches rely on computationally intensive offline preprocessing of languagefeatures for each input image, limiting adaptability to new environments. Inthis work, we introduce Online Language Splatting, the first framework toachieve online, near real-time, open-vocabulary language mapping within a3DGS-SLAM system without requiring pre-generated language features. The keychallenge lies in efficiently fusing high-dimensional language features into 3Drepresentations while balancing the computation speed, memory usage, renderingquality and open-vocabulary capability. To this end, we innovatively design:(1) a high-resolution CLIP embedding module capable of generating detailedlanguage feature maps in 18ms per frame, (2) a two-stage online auto-encoderthat compresses 768-dimensional CLIP features to 15 dimensions while preservingopen-vocabulary capabilities, and (3) a color-language disentangledoptimization approach to improve rendering quality. Experimental results showthat our online method not only surpasses the state-of-the-art offline methodsin accuracy but also achieves more than 40x efficiency boost, demonstrating thepotential for dynamic and interactive AI applications.</description>
      <author>example@mail.com (Saimouli Katragadda, Cho-Ying Wu, Yuliang Guo, Xinyu Huang, Guoquan Huang, Liu Ren)</author>
      <guid isPermaLink="false">2503.09447v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Neural-Augmented Incremental Nonlinear Dynamic Inversion for Quadrotors with Payload Adaptation</title>
      <link>http://arxiv.org/abs/2503.09441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多旋翼应用的复杂性增加，需要更准确的飞行控制器来可靠地预测作用在机器人上的所有力。传统的方法忽略了所谓的剩余力，并且计算这些力的成本较高。本文提出了一种结合增量非线性动态反转和学习方法的新技术。&lt;h4&gt;背景&lt;/h4&gt;随着多旋翼无人机应用场景日益复杂化，传统的飞行控制器虽然能够模拟能量的大部分部分，但缺乏对剩余力的有效处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以更准确地预测剩余力的方法，并将其应用于携带吊式负载的四轴飞行器上。&lt;h4&gt;方法&lt;/h4&gt;使用增量非线性动态反转（INDI）来估计传感器测量之间的差异以计算残余力，同时结合机器学习算法来平滑处理INDI输出。&lt;h4&gt;主要发现&lt;/h4&gt;神经网络可以比INDI更有效地预测剩余力，并且将神经网络和INDI相结合的方法可以获得更好的结果。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了通过融合增量非线性动态反转与学习方法，能够在没有额外传感器测量的情况下有效改善四轴飞行器的性能。&lt;h4&gt;翻译&lt;/h4&gt;随着多旋翼应用复杂性的增加，需要更精确的飞行控制器来可靠地预测作用在机器人上的所有力。传统的方法忽略了被称为剩余力的部分，并且准确计算这些力可能会非常昂贵。增量非线性动态反转（INDI）是一种通过比较不同传感器测量值来估计剩余力的技术。然而，它严重依赖于特殊传感器数据，这些数据可能很嘈杂。最新的研究表明可以通过基于学习的方法预测剩余力。本文展示了使用机器学习算法可以预测更平滑的INDI输出，并且无需额外的传感器测量。此外，我们提出了一种新的方法，结合了基于学习的预测和INDI。该技术还针对携带吊式负载的四轴飞行器进行了调整。结果表明，利用神经网络预测剩余力能比使用INDI有更优的表现，而将两种方法相结合则可以获得更好的效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing complexity of multirotor applications has led to the need ofmore accurate flight controllers that can reliably predict all forces acting onthe robot. Traditional flight controllers model a large part of the forces butdo not take so called residual forces into account. A reason for this is thataccurately computing the residual forces can be computationally expensive.Incremental Nonlinear Dynamic Inversion (INDI) is a method that computes thedifference between different sensor measurements in order to estimate theseresidual forces. The main issue with INDI is it's reliance on special sensormeasurements which can be very noisy. Recent work has also shown that residualforces can be predicted using learning-based methods. In this work, wedemonstrate that a learning algorithm can predict a smoother version of INDIoutputs without requiring additional sensor measurements. In addition, weintroduce a new method that combines learning based predictions with INDI. Wealso adapt the two approaches to work on quadrotors carrying a slung-typepayload. The results show that using a neural network to predict residualforces can outperform INDI while using the combination of neural network andINDI can yield even better results than each method individually.</description>
      <author>example@mail.com (Eckart Cobo-Briesewitz, Khaled Wahba, Wolfgang Hönig)</author>
      <guid isPermaLink="false">2503.09441v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Endo-FASt3r: Endoscopic Foundation model Adaptation for Structure from motion</title>
      <link>http://arxiv.org/abs/2503.07204v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Endo-FASt3r是一种用于单目内窥镜场景深度和姿态估计的自监督学习框架，该框架首次使用基础模型同时进行这两项任务。&lt;h4&gt;背景&lt;/h4&gt;在机器人辅助手术中，准确的深度和相机姿态估计对于实现高质量的3D可视化至关重要。尽管最近通过自监督学习方法改进了基础模型以适应单目内窥镜场景中的深度估计，但没有先前的工作探索其用于姿态估计的方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法Endo-FASt3r，该方法可以使用基础模型同时进行单目内窥镜场景的深度和姿态估计。&lt;h4&gt;方法&lt;/h4&gt;设计了Reloc3rX以扩展现有的相对姿态估计基础模型，并引入了必要的修改以在自监督学习中实现收敛。此外，提出了一种新的适应技术DoMoRA，该技术允许更高秩的更新并加速收敛过程。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与先前的工作相比，Endo-FASt3r在SCARED数据集上的姿态估计提高了10%，深度估计提高了2%。同样，在Hamlyn和StereoMIS数据集上获得的类似性能增益证实了Endo-FASt3r在不同数据集上的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Endo-FASt3r为单目内窥镜场景中的深度和姿态估计提供了一种新的自监督学习框架，这种方法不仅提高了准确性，还展示了良好的跨数据集的泛化性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate depth and camera pose estimation is essential for achievinghigh-quality 3D visualisations in robotic-assisted surgery. Despite recentadvancements in foundation model adaptation to monocular depth estimation ofendoscopic scenes via self-supervised learning (SSL), no prior work hasexplored their use for pose estimation. These methods rely on low rank-basedadaptation approaches, which constrain model updates to a low-rank space. Wepropose Endo-FASt3r, the first monocular SSL depth and pose estimationframework that uses foundation models for both tasks. We extend the Reloc3rrelative pose estimation foundation model by designing Reloc3rX, introducingmodifications necessary for convergence in SSL. We also present DoMoRA, a noveladaptation technique that enables higher-rank updates and faster convergence.Experiments on the SCARED dataset show that Endo-FASt3r achieves a substantial$10\%$ improvement in pose estimation and a $2\%$ improvement in depthestimation over prior work. Similar performance gains on the Hamlyn andStereoMIS datasets reinforce the generalisability of Endo-FASt3r acrossdifferent datasets.</description>
      <author>example@mail.com (Mona Sheikh Zeinoddin, Mobarakol Islam, Zafer Tandogdu, Greg Shaw, Mathew J. Clarkson, Evangelos Mazomenos, Danail Stoyanov)</author>
      <guid isPermaLink="false">2503.07204v2</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation</title>
      <link>http://arxiv.org/abs/2503.09409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 6 figures, 4 tables, submitted to the 2025 IEEE 21st  International Conference on Automation Science and Engineering&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;针对汽车装配线中电线束安装仍为手工操作的现状，提出了一种结合力控制和深度视觉触觉学习的AI框架来实现电缆连接器自动配对。&lt;h4&gt;背景&lt;/h4&gt;虽然工业机器人在汽车组装中的应用非常广泛，但电线束的安装过程仍然主要依赖人工，因为其需要精确而灵活的操作。&lt;h4&gt;目的&lt;/h4&gt;开发一个新颖的人工智能框架，通过集成力控制和深度视觉触觉学习来自动化电缆连接器的配对操作。&lt;h4&gt;方法&lt;/h4&gt;提出了一种利用多模态变压器架构（基于视觉、触觉及本体感觉数据）优化搜索与插入策略的方法。此外，设计了一个自动化的数据收集和优化流程，以减少机器学习专业知识的需求。该框架能够生成可在标准工业控制器上原生运行的机器人程序。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证显示，在中心控制台装配任务中，所提出的框架相较于传统机器人编程方法在循环时间和鲁棒性方面都有显著改善。&lt;h4&gt;结论&lt;/h4&gt;通过将力控制与深度视觉触觉学习结合，能够有效地自动化电缆连接器配对过程，并提高生产效率。视频材料可在提供的链接下获取。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文的英文内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the widespread adoption of industrial robots in automotive assembly,wire harness installation remains a largely manual process, as it requiresprecise and flexible manipulation. To address this challenge, we design a novelAI-based framework that automates cable connector mating by integrating forcecontrol with deep visuotactile learning. Our system optimizessearch-and-insertion strategies using first-order optimization over amultimodal transformer architecture trained on visual, tactile, andproprioceptive data. Additionally, we design a novel automated data collectionand optimization pipeline that minimizes the need for machine learningexpertise. The framework optimizes robot programs that run natively on standardindustrial controllers, permitting human experts to audit and certify them.Experimental validations on a center console assembly task demonstratesignificant improvements in cycle times and robustness compared to conventionalrobot programming approaches. Videos are available underhttps://claudius-kienle.github.io/AppMuTT.</description>
      <author>example@mail.com (Claudius Kienle, Benjamin Alt, Finn Schneider, Tobias Pertlwieser, Rainer Jäkel, Rania Rayyes)</author>
      <guid isPermaLink="false">2503.09409v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>PCLA: A Framework for Testing Autonomous Agents in the CARLA Simulator</title>
      <link>http://arxiv.org/abs/2503.09385v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work will be published at the FSE 2025 demonstration track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近年来，关于自动驾驶代理的测试研究在模拟环境中显著增长。CARLA模拟器通常是首选工具，而来自CARLA Leaderboard挑战赛的自主代理被认为是该环境内性能最佳的代理。然而，测试这些预训练代理的研究人员经常面临将它们部署到自定义测试环境中的困难。&lt;h4&gt;背景&lt;/h4&gt;最近关于自动驾驶汽车代理的测试研究在使用模拟环境方面显著增加。CARLA仿真器因其良好的特性成为了研究人员的首选工具，并且来自Leaderboard挑战赛的几个自主驾驶代理已经展示了优秀的性能，被视为目前该领域内的最佳。&lt;h4&gt;目的&lt;/h4&gt;为了解决将这些高性能预训练代理容易地整合到自定义测试环境中所面临的问题，研究者们开发了一个新的开源Python框架PCLA。&lt;h4&gt;方法&lt;/h4&gt;PCLA提供了多种功能：包括能够简单部署Leaderboard中的高性能代理，允许在不同的CARLA版本或编程环境下轻松切换代理的使用，并且与最新版CARLA完全兼容。此外，它还提供了一种独立于特定CARLA版本的方式来测试这些预训练的自动驾驶代理。&lt;h4&gt;主要发现&lt;/h4&gt;PCLA作为一个专门用于测试各种自动驾驶代理的基础设施首次被提出，它使得研究人员能够更灵活地在任意环境中部署和测试这些最佳性能的代理。&lt;h4&gt;结论&lt;/h4&gt;通过引入PCLA框架，研究者们可以更加便捷地利用高性能预训练的CARLA Leaderboard挑战赛中的自动驾驶代理进行不同环境下的测试工作，从而加速了该领域的进步。此开源项目目前托管于GitHub上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research on testing autonomous driving agents has grown significantly,especially in simulation environments. The CARLA simulator is often thepreferred choice, and the autonomous agents from the CARLA Leaderboardchallenge are regarded as the best-performing agents within this environment.However, researchers who test these agents, rather than training their own onesfrom scratch, often face challenges in utilizing them within customized testenvironments and scenarios. To address these challenges, we introduce PCLA(Pretrained CARLA Leaderboard Agents), an open-source Python testing frameworkthat includes nine high-performing pre-trained autonomous agents from theLeaderboard challenges. PCLA is the first infrastructure specifically designedfor testing various autonomous agents in arbitrary CARLAenvironments/scenarios. PCLA provides a simple way to deploy Leaderboard agentsonto a vehicle without relying on the Leaderboard codebase, it allowsresearchers to easily switch between agents without requiring modifications toCARLA versions or programming environments, and it is fully compatible with thelatest version of CARLA while remaining independent of the Leaderboard'sspecific CARLA version. PCLA is publicly accessible athttps://github.com/MasoudJTehrani/PCLA.</description>
      <author>example@mail.com (Masoud Jamshidiyan Tehrani, Jinhan Kim, Paolo Tonella)</author>
      <guid isPermaLink="false">2503.09385v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Robust Self-Reconfiguration for Fault-Tolerant Control of Modular Aerial Robot Systems</title>
      <link>http://arxiv.org/abs/2503.09376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种针对模块化空中机器人系统（MARS）的稳健且高效的自重构算法，该算法最大化了在重新组装过程中的每个中间阶段可控制性边界。&lt;h4&gt;背景&lt;/h4&gt;现有关于MARS的研究往往忽略了重组过程中形成的中间结构的实际可控性问题。&lt;h4&gt;目的&lt;/h4&gt;弥补这一研究空白，通过考虑受控约束的动力学模型来设计MARS的自重构算法。&lt;h4&gt;方法&lt;/h4&gt;开发了计算最优且可控分解和组装序列的算法，以实现稳健的自重构。&lt;h4&gt;主要发现&lt;/h4&gt;验证了所提出的方法在多个故障容忍性自重构场景中的有效性，展示了在可控制性和轨迹跟踪方面的显著改进，并减少了装配步骤的数量。&lt;h4&gt;结论&lt;/h4&gt;提出的自我重组方法提高了MARS系统的稳定性与效能，在各种复杂条件下展现出良好的适应能力。&lt;h4&gt;翻译&lt;/h4&gt;模块化空中机器人系统（MARS）由多个无人机单元组装成一个单一的集成刚性飞行平台。由于固有的冗余，MARS能够自重构为不同的配置以缓解旋翼或单元故障并保持稳定飞行。然而，现有的关于MARS自我重组的研究往往忽略了在重新装配过程中形成的中间结构的实际可控性问题，这限制了它们的应用范围。本文通过考虑受控约束的动态模型和提出一种稳健高效的自我重建算法来解决这一差距，该算法最大化每个中间阶段的可控制边界。具体来说，我们开发了计算最优且可控分解与组装序列的算法，以实现稳健自重构。最后，在多个具有挑战性的故障耐性自重构场景中验证了我们的方法，展示了在可控制性和轨迹跟踪方面的显著改进，并减少了装配步骤的数量。相关视频和源代码可在https://github.com/RuiHuangNUS/MARS-Reconfig 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modular Aerial Robotic Systems (MARS) consist of multiple drone unitsassembled into a single, integrated rigid flying platform. With inherentredundancy, MARS can self-reconfigure into different configurations to mitigaterotor or unit failures and maintain stable flight. However, existing works onMARS self-reconfiguration often overlook the practical controllability ofintermediate structures formed during the reassembly process, which limitstheir applicability. In this paper, we address this gap by considering thecontrol-constrained dynamic model of MARS and proposing a robust and efficientself-reconstruction algorithm that maximizes the controllability margin at eachintermediate stage. Specifically, we develop algorithms to compute optimal,controllable disassembly and assembly sequences, enabling robustself-reconfiguration. Finally, we validate our method in several challengingfault-tolerant self-reconfiguration scenarios, demonstrating significantimprovements in both controllability and trajectory tracking while reducing thenumber of assembly steps. The videos and source code of this work are availableat https://github.com/RuiHuangNUS/MARS-Reconfig/</description>
      <author>example@mail.com (Rui Huang, Siyu Tang, Zhiqian Cai, Lin Zhao)</author>
      <guid isPermaLink="false">2503.09376v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Robust Fault-Tolerant Control and Agile Trajectory Planning for Modular Aerial Robotic Systems</title>
      <link>http://arxiv.org/abs/2503.09351v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对模块化空中机器人系统(MARS)的新型容错控制再分配方法和敏捷轨迹规划方案，以提高系统的稳定性、鲁棒性和任务适应性。&lt;h4&gt;背景&lt;/h4&gt;现有的容错控制技术在MARS连接与分离时表现出显著的振荡现象，影响了整体系统的稳定性和性能。&lt;h4&gt;目的&lt;/h4&gt;解决现有故障容忍度控制系统在MARS中的问题，提出一种适用于任意数量模块化机器人及其装配结构的容错控制再分配方法，并设计一种能够避开碰撞且动态可行性的轨迹规划方案。&lt;h4&gt;方法&lt;/h4&gt;提出了基于力和扭矩重新分布的算法来适应不同的MARS配置；利用臂距相对于整个系统质量中心的位置来调整每个单元的任务，以实现更好的力和扭矩均衡。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法通过广泛的模拟验证了其在故障容忍度、轨迹追踪精度以及复杂环境中的鲁棒性方面的改进效果。&lt;h4&gt;结论&lt;/h4&gt;该方法是首个针对MARS系统的容错飞行与碰撞避免的全面解决方案，展示了在各种任务和条件下的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;模块化空中机器人系统(MARS)由多个无人机单元组成，这些单元能够自我重构以适应不同的任务需求和故障情况。然而，现有的容错控制方法在对接和分离过程中表现出显著的振荡现象，影响了系统的稳定性。为了解决这个问题，我们提出了一种新的容错控制再分配方法，该方法可以应对任意数量模块化机器人及其装配结构。算法根据各个单元相对于MARS质量中心的位置重新分布所需的集体力和扭矩。此外，我们还提出了一种针对MARS的敏捷轨迹规划方案，在任何配置下都能避免碰撞且动态可行。我们的研究代表了首个全面的方法来实现MARS系统的容错飞行与碰撞避免。通过广泛的模拟验证了所提方法在故障容忍度、轨迹追踪精度以及复杂环境中的鲁棒性方面的改进效果。该工作的视频和源代码可在https://github.com/RuiHuangNUS/MARS-FTCC/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modular Aerial Robotic Systems (MARS) consist of multiple drone units thatcan self-reconfigure to adapt to various mission requirements and faultconditions. However, existing fault-tolerant control methods exhibitsignificant oscillations during docking and separation, impacting systemstability. To address this issue, we propose a novel fault-tolerant controlreallocation method that adapts to arbitrary number of modular robots and theirassembly formations. The algorithm redistributes the expected collective forceand torque required for MARS to individual unit according to their moment armrelative to the center of MARS mass. Furthermore, We propose an agiletrajectory planning method for MARS of arbitrary configurations, which iscollision-avoiding and dynamically feasible. Our work represents the firstcomprehensive approach to enable fault-tolerant and collision avoidance flightfor MARS. We validate our method through extensive simulations, demonstratingimproved fault tolerance, enhanced trajectory tracking accuracy, and greaterrobustness in cluttered environments. The videos and source code of this workare available at https://github.com/RuiHuangNUS/MARS-FTCC/</description>
      <author>example@mail.com (Rui Huang, Zhenyu Zhang, Siyu Tang, Zhiqian Cai, Lin Zhao)</author>
      <guid isPermaLink="false">2503.09351v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot Interaction via Large Language Model</title>
      <link>http://arxiv.org/abs/2503.09335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been accepted for publication in ESWA @ 2025 Elsevier.  Personal use of this material is permitted. Permission from Elsevier must be  obtained for all other uses, including reprinting/redistribution, creating  new works, or reuse of any copyrighted components of this work in other media&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多模态人机交互范式NVP-HRI，结合了语音命令和指示姿态，用于老年人等群体与服务机器人之间的互动。通过使用Segment Anything Model (SAM)进行视觉分析和深度数据处理，并整合大型语言模型（LLM）来实时协调指令与场景分布，确保安全无碰撞的轨迹解决方案。&lt;h4&gt;背景&lt;/h4&gt;现有的人机交互系统在面对新物体时存在偏见，依赖预定义手势或语言令牌，这给老年人带来了学习命令、记忆手部动作和学会新名称的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种直观且高效的多模态人机交互方案，能够处理新的未训练过的对象，并减少大型语言模型幻觉的风险。&lt;h4&gt;方法&lt;/h4&gt;NVP-HRI利用预训练的SAM网络进行零样本预测，通过结合语音命令和指示姿态来实现更自然的人机互动；同时，它还与LLM协同工作以提供多模态指令处理能力，实时解决碰撞问题。&lt;h4&gt;主要发现&lt;/h4&gt;在多种现实任务中使用Universal Robot进行测试时，NVP-HRI相较于传统的手势控制系统效率提高了多达59.2%。&lt;h4&gt;结论&lt;/h4&gt;NVP-HRI为老年人等群体提供了更为直观和有效的服务机器人交互方式，其代码和设计将开放供公众下载和研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.eswa.2024.126360&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective Human-Robot Interaction (HRI) is crucial for future service robotsin aging societies. Existing solutions are biased toward only well-trainedobjects, creating a gap when dealing with new objects. Currently, HRI systemsusing predefined gestures or language tokens for pretrained objects posechallenges for all individuals, especially elderly ones. These challengesinclude difficulties in recalling commands, memorizing hand gestures, andlearning new names. This paper introduces NVP-HRI, an intuitive multi-modal HRIparadigm that combines voice commands and deictic posture. NVP-HRI utilizes theSegment Anything Model (SAM) to analyze visual cues and depth data, enablingprecise structural object representation. Through a pre-trained SAM network,NVP-HRI allows interaction with new objects via zero-shot prediction, evenwithout prior knowledge. NVP-HRI also integrates with a large language model(LLM) for multimodal commands, coordinating them with object selection andscene distribution in real time for collision-free trajectory solutions. Wealso regulate the action sequence with the essential control syntax to reduceLLM hallucination risks. The evaluation of diverse real-world tasks using aUniversal Robot showcased up to 59.2\% efficiency improvement over traditionalgesture control, as illustrated in the video https://youtu.be/EbC7al2wiAc. Ourcode and design will be openly available athttps://github.com/laiyuzhi/NVP-HRI.git.</description>
      <author>example@mail.com (Yuzhi Lai, Shenghai Yuan, Youssef Nassar, Mingyu Fan, Thomas Weber, Matthias Rätsch)</author>
      <guid isPermaLink="false">2503.09335v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Energy Optimized Piecewise Polynomial Approximation Utilizing Modern Machine Learning Optimizers</title>
      <link>http://arxiv.org/abs/2503.09329v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Austrian Robotics Workshop 2025 (2 page student paper)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作探讨了通过引入能量优化作为额外目标来扩展机器学习优化的分段多项式近似方法。&lt;h4&gt;背景&lt;/h4&gt;传统的闭合形式解虽然能保证连续性和逼近目标，但在处理复杂的优化目标时缺乏灵活性。&lt;h4&gt;目的&lt;/h4&gt;利用现代梯度下降优化器在TensorFlow框架中，引入一个新的框架以最小化凸轮轮廓中的总曲率。&lt;h4&gt;方法&lt;/h4&gt;该研究采用现代的梯度下降算法来实现能量优化，并针对不利于单纯逼近和连续性优化的数据集进行了实验。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示了这种方法的有效性，在输入数据不理想的情况下能够显著提高效率，使运动更加平滑并减少能耗。&lt;h4&gt;结论&lt;/h4&gt;新的框架为解决噪声或次优输入数据的情况提供了一种潜在的改进方案。&lt;h4&gt;翻译&lt;/h4&gt;此摘要介绍了将能量优化引入机器学习优化分段多项式近似的方法及其在凸轮轮廓中的应用，展示出其在处理复杂和不理想数据时的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work explores an extension of ML-optimized piecewise polynomialapproximation by incorporating energy optimization as an additional objective.Traditional closed-form solutions enable continuity and approximation targetsbut lack flexibility in accommodating complex optimization goals. By leveragingmodern gradient descent optimizers within TensorFlow, we introduce a frameworkthat minimizes total curvature in cam profiles, leading to smoother motion andreduced energy consumption for input data that is unfavorable for soleapproximation and continuity optimization. Experimental results confirm theeffectiveness of this approach, demonstrating its potential to improveefficiency in scenarios where input data is noisy or suboptimal forconventional methods.</description>
      <author>example@mail.com (Hannes Waclawek, Stefan Huber)</author>
      <guid isPermaLink="false">2503.09329v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>2HandedAfforder: Learning Precise Actionable Bimanual Affordances from Human Videos</title>
      <link>http://arxiv.org/abs/2503.09320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了一种从人类活动视频数据集中提取抓取和操作物体的可行区域（affordance）的新框架，并介绍了名为2HandedAfforder的基于视觉语言模型的预测方法，该方法在多种活动中表现出优于基准的方法。&lt;h4&gt;背景&lt;/h4&gt;人在与物体交互时能够有效判断哪些部位适合执行特定动作，但现有的基于视觉的方法往往将问题简化为简单的对象部分分割。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架用于从视频数据中提取精确的对象可行区域和类别标签，并展示一个预测模型2HandedAfforder，以提高在多种活动中的表现。&lt;h4&gt;方法&lt;/h4&gt;通过开发包含精细分割的物体抓取区以及与操作相关的类别的2HANDS数据集，训练了一个基于视觉语言的方法来预测这些可行区域。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的2HandedAfforder模型在各种活动上的可行区域分割任务中优于基准方法，并且该模型的预测结果具有可操作性，在机器人抓取实验中得到了验证。&lt;h4&gt;结论&lt;/h4&gt;论文的工作表明，利用视频数据集进行精确的物体可行区域提取和分类可以有效提高基于视觉的物体抓取任务性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When interacting with objects, humans effectively reason about which regionsof objects are viable for an intended action, i.e., the affordance regions ofthe object. They can also account for subtle differences in object regionsbased on the task to be performed and whether one or two hands need to be used.However, current vision-based affordance prediction methods often reduce theproblem to naive object part segmentation. In this work, we propose a frameworkfor extracting affordance data from human activity video datasets. Ourextracted 2HANDS dataset contains precise object affordance regionsegmentations and affordance class-labels as narrations of the activityperformed. The data also accounts for bimanual actions, i.e., two handsco-ordinating and interacting with one or more objects. We present a VLM-basedaffordance prediction model, 2HandedAfforder, trained on the dataset anddemonstrate superior performance over baselines in affordance regionsegmentation for various activities. Finally, we show that our predictedaffordance regions are actionable, i.e., can be used by an agent performing atask, through demonstration in robotic manipulation scenarios.</description>
      <author>example@mail.com (Marvin Heidinger, Snehal Jauhri, Vignesh Prasad, Georgia Chalvatzaki)</author>
      <guid isPermaLink="false">2503.09320v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>MonoSLAM: Robust Monocular SLAM with Global Structure Optimization</title>
      <link>http://arxiv.org/abs/2503.09296v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种利用点、线和消失点特征的单目视觉SLAM系统，旨在提高在低纹理环境中可靠定位的能力。&lt;h4&gt;背景&lt;/h4&gt;传统基于点的系统在缺乏足够视觉特征的低纹理环境下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;通过引入一种新的方法来改进系统的鲁棒性和准确性，该方法利用全局结构信息（Global Primitives）来增强SLAM性能。&lt;h4&gt;方法&lt;/h4&gt;构建消失点以从线性特征生成，并提出了一种加权融合策略，在世界坐标系中建立全局结构。这种方法将多个帧与非重叠区域关联起来，并制定了多帧再投影误差优化策略，从而提高了跟踪精度。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该系统在轨迹精度上超过了现有方法，尤其是在具有挑战性的环境中表现尤为突出。&lt;h4&gt;结论&lt;/h4&gt;通过利用全局结构信息，所提出的SLAM系统能够更准确和稳定地估计摄像机姿态并构建地图，在低纹理环境下表现出色。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种稳健的单目视觉SLAM系统，同时利用点、线和消失点特征进行精确的相机位姿估计和建图。为了应对在缺乏足够视觉特征的低纹理环境中实现可靠定位的关键挑战，我们引入了新的方法，即通过利用全局结构信息来提高系统的鲁棒性和准确性表现。我们的关键创新在于从线性特征构造消失点，并提出了一种加权融合策略，在世界坐标系中建立全局结构。这种策略将多个帧与非重叠区域关联起来，并制定了多帧再投影误差优化，从而显著提高了低纹理场景中的跟踪精度。在各种数据集上的评估表明，我们的系统在轨迹精确度上优于最先进的方法，尤其是在具有挑战性的环境中表现尤为突出。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a robust monocular visual SLAM system that simultaneouslyutilizes point, line, and vanishing point features for accurate camera poseestimation and mapping. To address the critical challenge of achieving reliablelocalization in low-texture environments, where traditional point-based systemsoften fail due to insufficient visual features, we introduce a novel approachleveraging Global Primitives structural information to improve the system'srobustness and accuracy performance. Our key innovation lies in constructingvanishing points from line features and proposing a weighted fusion strategy tobuild Global Primitives in the world coordinate system. This strategyassociates multiple frames with non-overlapping regions and formulates amulti-frame reprojection error optimization, significantly improving trackingaccuracy in texture-scarce scenarios. Evaluations on various datasets show thatour system outperforms state-of-the-art methods in trajectory precision,particularly in challenging environments.</description>
      <author>example@mail.com (Bingzheng Jiang, Jiayuan Wang, Han Ding, Lijun Zhu)</author>
      <guid isPermaLink="false">2503.09296v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>GarmentPile: Point-Level Visual Affordance Guided Retrieval and Adaptation for Cluttered Garments Manipulation</title>
      <link>http://arxiv.org/abs/2503.09243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了处理复杂衣物堆叠时所面临的挑战，并提出了一种学习点级行为的新方法。&lt;h4&gt;背景&lt;/h4&gt;由于衣物具有复杂的可变形特性以及与其他物体之间的错综复杂的相互作用，因此在堆放环境中进行操作存在许多挑战。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些困难，研究人员旨在开发一种新的框架来处理多样化的服装类型和堆叠配置。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种学习点级行为的方法，这是一种密集的表示方式，用于建模复杂的空间以及多模式的操作候选方案，并考虑到了衣物的几何形状、结构及与其他对象的关系。此外还引入了适应模块以重新组织高度纠缠的衣物以便于操作。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在仿真和现实世界环境中均展现了有效的性能，适用于处理各种类型的服装和堆叠配置。&lt;h4&gt;结论&lt;/h4&gt;这项研究为解决复杂环境下的衣物操纵问题提供了新的思路和方法，有助于提高自动化系统中此类任务的表现。&lt;h4&gt;翻译&lt;/h4&gt;凌乱的衣物操作面临重大挑战，因为衣物具有复杂的可变形性质以及错综复杂的相互关系。不同于单一衣物的操作，在凌乱的情况下需要管理复杂而纠缠在一起的衣物，并保持其清洁度和操纵稳定性。为解决这些问题，研究人员提出了一种学习点级行为的方法——这是一种密集表示方式，用于建模复杂空间及多模式操作候选方案，同时考虑到服装几何形状、结构和相互对象关系。此外，在某些极度缠结的情况下难以直接检索到衣物时，还引入了一个适应模块来重新组织高度纠缠的衣物至适合操作的状态。该框架在包含多样化服装类型和堆叠配置的仿真环境以及现实世界中均显示出有效性。项目页面：https://garmentpile.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cluttered garments manipulation poses significant challenges due to thecomplex, deformable nature of garments and intricate garment relations. Unlikesingle-garment manipulation, cluttered scenarios require managing complexgarment entanglements and interactions, while maintaining garment cleanlinessand manipulation stability. To address these demands, we propose to learnpoint-level affordance, the dense representation modeling the complex space andmulti-modal manipulation candidates, while being aware of garment geometry,structure, and inter-object relations. Additionally, as it is difficult todirectly retrieve a garment in some extremely entangled clutters, we introducean adaptation module, guided by learned affordance, to reorganizehighly-entangled garments into states plausible for manipulation. Our frameworkdemonstrates effectiveness over environments featuring diverse garment typesand pile configurations in both simulation and the real world. Project page:https://garmentpile.github.io/.</description>
      <author>example@mail.com (Ruihai Wu, Ziyu Zhu, Yuran Wang, Yue Chen, Jiarui Wang, Hao Dong)</author>
      <guid isPermaLink="false">2503.09243v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>MarineGym: A High-Performance Reinforcement Learning Platform for Underwater Robotics</title>
      <link>http://arxiv.org/abs/2503.09203v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MarineGym 是一个专为水下机器人设计的高性能强化学习平台，它克服了现有水下仿真环境在兼容性、训练效率和标准化基准测试方面的局限。&lt;h4&gt;背景&lt;/h4&gt;当前水下机器人仿真环境中存在诸如与强化学习不兼容、训练效率低下以及缺乏标准基准的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为MarineGym的高性能强化学习平台，旨在解决现有水下仿真环境中的这些问题，并推动水下机器人领域中RL研究的进步。&lt;h4&gt;方法&lt;/h4&gt;开发了一种基于Isaac Sim的GPU加速流体动力学插件，利用NVIDIA RTX 3060 GPU实现了每秒25万帧的速度。提供了五个无人水下载具（UUV）模型、多种推进系统以及一系列预定义任务以解决核心水下控制挑战。DR工具包允许在训练过程中灵活调整仿真和任务参数，从而提高Sim2Real的转换。&lt;h4&gt;主要发现&lt;/h4&gt;通过基准实验表明，MarineGym 在现有平台上提高了训练效率，并支持在各种扰动下的稳健策略适应性。&lt;h4&gt;结论&lt;/h4&gt;期望该平台能够推动水下机器人领域中强化学习研究的发展。&lt;h4&gt;翻译&lt;/h4&gt;此工作介绍了 MarineGym，这是一个专门为水下机器人设计的高性能强化学习(RL)平台。它旨在解决现有水下仿真环境在RL兼容性、训练效率和标准化基准测试方面的局限。MarineGym 集成了一种基于Isaac Sim提出的GPU加速流体动力学插件，在单个NVIDIA RTX 3060 GPU上每秒可实现25万帧的回放速度。该平台提供了五个无人水下载具（UUV）模型、多种推进系统以及一系列预定义任务，涵盖核心水下控制挑战。此外，DR工具包允许在训练过程中灵活调整仿真和任务参数以提高Sim2Real转换。进一步的基准实验表明MarineGym 提高了现有平台上的训练效率，并支持各种扰动下的稳健策略适应性。我们期望该平台能够推动水下机器人领域中RL研究的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents the MarineGym, a high-performance reinforcement learning(RL) platform specifically designed for underwater robotics. It aims to addressthe limitations of existing underwater simulation environments in terms of RLcompatibility, training efficiency, and standardized benchmarking. MarineGymintegrates a proposed GPU-accelerated hydrodynamic plugin based on Isaac Sim,achieving a rollout speed of 250,000 frames per second on a single NVIDIA RTX3060 GPU. It also provides five models of unmanned underwater vehicles (UUVs),multiple propulsion systems, and a set of predefined tasks covering coreunderwater control challenges. Additionally, the DR toolkit allows flexibleadjustments of simulation and task parameters during training to improveSim2Real transfer. Further benchmark experiments demonstrate that MarineGymimproves training efficiency over existing platforms and supports robust policyadaptation under various perturbations. We expect this platform could drivefurther advancements in RL research for underwater robotics. For more detailsabout MarineGym and its applications, please visit our project page:https://marine-gym.com/.</description>
      <author>example@mail.com (Shuguang Chu, Zebin Huang, Yutong Li, Mingwei Lin, Ignacio Carlucho, Yvan R. Petillot, Canjun Yang)</author>
      <guid isPermaLink="false">2503.09203v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Learning Appearance and Motion Cues for Panoptic Tracking</title>
      <link>http://arxiv.org/abs/2503.09191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的全景跟踪方法，该方法在视频中同时捕捉一般语义信息和实例特有的外观及运动特征。&lt;h4&gt;背景&lt;/h4&gt;现有的全景跟踪技术通常忽视动态场景属性，并且无法高效地整合外观和运动线索。&lt;h4&gt;目的&lt;/h4&gt;为机器人提供时空理解能力，帮助它们在复杂的动态环境中更好地操作。&lt;h4&gt;方法&lt;/h4&gt;新方法通过专用网络头使用多尺度可变形卷积来推理场景的运动偏移量以及语义上下文和增强后的外观特征，并引入了一种两步融合模块来集成这些头部输出。&lt;h4&gt;主要发现&lt;/h4&gt;etname模型在两个基准数据集上的广泛评估显示，其性能超越了现有的方法，在保持对象身份方面更加卓越。&lt;h4&gt;结论&lt;/h4&gt;新提出的全景跟踪方法提高了机器人对动态环境的理解能力。作者还提供了开源代码以促进未来的研究。&lt;h4&gt;翻译&lt;/h4&gt;Panoptic tracking enables pixel-level scene interpretation of videos by integrating instance tracking in panoptic segmentation. This provides robots with a spatio-temporal understanding of the environment, an essential attribute for their operation in dynamic environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Panoptic tracking enables pixel-level scene interpretation of videos byintegrating instance tracking in panoptic segmentation. This provides robotswith a spatio-temporal understanding of the environment, an essential attributefor their operation in dynamic environments. In this paper, we propose a novelapproach for panoptic tracking that simultaneously captures general semanticinformation and instance-specific appearance and motion features. Unlikeexisting methods that overlook dynamic scene attributes, our approach leveragesboth appearance and motion cues through dedicated network heads. Theseinterconnected heads employ multi-scale deformable convolutions that reasonabout scene motion offsets with semantic context and motion-enhanced appearancefeatures to learn tracking embeddings. Furthermore, we introduce a noveltwo-step fusion module that integrates the outputs from both heads by firstmatching instances from the current time step with propagated instances fromprevious time steps and subsequently refines associations using motion-enhancedappearance embeddings, improving robustness in challenging scenarios. Extensiveevaluations of our proposed \netname model on two benchmark datasetsdemonstrate that it achieves state-of-the-art performance in panoptic trackingaccuracy, surpassing prior methods in maintaining object identities over time.To facilitate future research, we make the code available athttp://panoptictracking.cs.uni-freiburg.de</description>
      <author>example@mail.com (Juana Valeria Hurtado, Sajad Marvi, Rohit Mohan, Abhinav Valada)</author>
      <guid isPermaLink="false">2503.09191v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Bimanual Robotic Manipulation: Learning with Decoupled Interaction Framework</title>
      <link>http://arxiv.org/abs/2503.09186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;双臂机器人操作是机器人领域中的一个新兴且关键的话题。本文提出了一种新的解耦交互框架，该框架考虑了双臂操作中不同任务的特点。&lt;h4&gt;背景&lt;/h4&gt;之前的工作主要依赖于集成控制模型，这些模型将两个手臂的感知和状态作为输入来直接预测动作。然而，这种模式忽视了一些不需要在执行过程中明确合作的任务特征。&lt;h4&gt;目的&lt;/h4&gt;目的是开发一种新的解耦交互框架，以便更好地处理双臂操作中的协调任务和不协调任务。&lt;h4&gt;方法&lt;/h4&gt;该框架为每个手臂分配一个独立的模型以增强非协作任务的学习，并引入了一个自适应学习其自身手臂权重的选择性互动模块来改进协作任务的学习。&lt;h4&gt;主要发现&lt;/h4&gt;在RoboTwin数据集上的七个任务中，实验结果表明：（1）我们的框架表现出了卓越的成绩，比最先进的方法高出23.5%。（2）该框架具有灵活性，并且可以无缝地集成到现有方法中。（3）它可以有效地扩展到多智能体操作任务上，在成功率方面比最先进集成了控制的方法高出16.5%，并且模型大小仅为后者的一半。&lt;h4&gt;结论&lt;/h4&gt;研究显示，解耦设计自身带来了性能的提升，这种框架不仅在成功几率上有显著提高，而且具有较小的模型规模和良好的扩展性。&lt;h4&gt;翻译&lt;/h4&gt;双臂机器人操作是机器人领域中的一个新兴且关键的话题。以往的工作主要依赖于将两个手臂的状态及感知整合为输入以直接预测动作的集成控制模型。然而，我们认为双臂操作不仅涉及协调任务，还包含许多不需要执行过程中明确合作的任务（例如用最近的手抓取物体），而这些任务在集成了强加的合作机制的早期输入中被忽视了。本文提出了一种新的解耦交互框架，该框架根据双臂操作的不同任务特性进行设计。其核心思想是为每个手臂分配一个独立模型以增强非协作任务的学习，并引入选择性互动模块自适应学习来自自身手臂的权重来改进协作任务的学习。广泛的实验结果显示，在七个RoboTwin数据集的任务上：（1）我们的框架取得了卓越的成绩，比最先进的方法高出23.5%；（2）该框架具有灵活性，可以无缝地集成到现有方法中；（3）它可以有效地扩展至多智能体操作任务，并在成功率方面超过最先进控制集成的方法达28%。（4）性能的提升源自解耦设计本身，在仅有最先进的1/6模型大小的情况下，成功几率高出16.5%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bimanual robotic manipulation is an emerging and critical topic in therobotics community. Previous works primarily rely on integrated control modelsthat take the perceptions and states of both arms as inputs to directly predicttheir actions. However, we think bimanual manipulation involves not onlycoordinated tasks but also various uncoordinated tasks that do not requireexplicit cooperation during execution, such as grasping objects with theclosest hand, which integrated control frameworks ignore to consider due totheir enforced cooperation in the early inputs. In this paper, we propose anovel decoupled interaction framework that considers the characteristics ofdifferent tasks in bimanual manipulation. The key insight of our framework isto assign an independent model to each arm to enhance the learning ofuncoordinated tasks, while introducing a selective interaction module thatadaptively learns weights from its own arm to improve the learning ofcoordinated tasks. Extensive experiments on seven tasks in the RoboTwin datasetdemonstrate that: (1) Our framework achieves outstanding performance, with a23.5% boost over the SOTA method. (2) Our framework is flexible and can beseamlessly integrated into existing methods. (3) Our framework can beeffectively extended to multi-agent manipulation tasks, achieving a 28% boostover the integrated control SOTA. (4) The performance boost stems from thedecoupled design itself, surpassing the SOTA by 16.5% in success rate with only1/6 of the model size.</description>
      <author>example@mail.com (Jian-Jian Jiang, Xiao-Ming Wu, Yi-Xiang He, Ling-An Zeng, Yi-Lin Wei, Dandan Zhang, Wei-Shi Zheng)</author>
      <guid isPermaLink="false">2503.09186v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Long-Term Planning Around Humans in Domestic Environments with 3D Scene Graphs</title>
      <link>http://arxiv.org/abs/2503.09173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新的轨迹规划方法，旨在解决家用机器人长期计划中的挑战。&lt;h4&gt;背景&lt;/h4&gt;在家庭环境中操作的机器人的长期规划面临着人、物体和空间之间的相互作用带来的独特挑战。尽管最近利用视觉-语言模型（VLM）提取实际环境中的上下文信息的方法取得了满意的性能，但这些方法没有明确地建模人类活动。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的轨迹规划方法，该方法通过增强的3D场景图（3DSG）表示来整合人的偏好、活动和空间背景。&lt;h4&gt;方法&lt;/h4&gt;引入了一种基于活动的关系模型，并且这种方法捕捉到了人类行为对周围物体的空间影响，导致了更符合上下文的路径适应性。&lt;h4&gt;主要发现&lt;/h4&gt;初步结果表明，该方法可以有效地为受人类活动影响的空间分配成本，确保机器人轨迹在动态环境中保持背景相关性和社会适宜性。&lt;h4&gt;结论&lt;/h4&gt;这种平衡任务效率与社交适当性的方法增强了家庭设置中的人机互动意识。未来的工作包括实现完整的规划流水线并进行用户研究以评估轨迹接受度。&lt;h4&gt;翻译&lt;/h4&gt;长期的家庭环境中的机器人的工作带来了人类、物体和空间相互作用的独特挑战。最近的轨迹规划技术利用了视觉语言模型来提取上下文信息，但这些方法没有明确建模人活动的影响。这项研究提出了一种新颖的方法，通过增强3D场景图表示法整合人偏好的、活动的以及背景信息，实现了更符合环境变化的路径适应性，并提高了家庭环境中的人机互动质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-term planning for robots operating in domestic environments poses uniquechallenges due to the interactions between humans, objects, and spaces. Recentadvancements in trajectory planning have leveraged vision-language models(VLMs) to extract contextual information for robots operating in real-worldenvironments. While these methods achieve satisfying performance, they do notexplicitly model human activities. Such activities influence surroundingobjects and reshape spatial constraints. This paper presents a novel approachto trajectory planning that integrates human preferences, activities, andspatial context through an enriched 3D scene graph (3DSG) representation. Byincorporating activity-based relationships, our method captures the spatialimpact of human actions, leading to more context-sensitive trajectoryadaptation. Preliminary results demonstrate that our approach effectivelyassigns costs to spaces influenced by human activities, ensuring that the robottrajectory remains contextually appropriate and sensitive to the ongoingenvironment. This balance between task efficiency and social appropriatenessenhances context-aware human-robot interactions in domestic settings. Futurework includes implementing a full planning pipeline and conducting user studiesto evaluate trajectory acceptability.</description>
      <author>example@mail.com (Ermanno Bartoli, Dennis Rotondi, Kai O. Arras, Iolanda Leite)</author>
      <guid isPermaLink="false">2503.09173v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Predictor-Based Time Delay Control of A Hex-Jet Unmanned Aerial Vehicle</title>
      <link>http://arxiv.org/abs/2503.09148v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Robotics and Automation Letters. 8 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新型的涡轮喷气式垂直起降无人机平台Hex-Jet，该平台通过集成推力矢量控制和差动推力来实现全方位的姿态控制。&lt;h4&gt;背景&lt;/h4&gt;涡轮喷气动力VTOL无人机由于其相对于现有电动推进系统的优越功率密度和推重比，在重型运输和服务领域受到了越来越多的关注。然而，这类无人机的主要挑战在于推力矢量机械系统复杂性较高，需要解决涡轮机动态响应慢的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种简化推力矢量机制的方法，并利用基于频域模型的时间延迟预测器控制技术来减少滚转姿态控制中的时间延迟问题。&lt;h4&gt;方法&lt;/h4&gt;设计了一种新的无人机平台Hex-Jet，采用频率域模型的预测器基础时间延迟控制系统。&lt;h4&gt;主要发现&lt;/h4&gt;飞行测试表明，所提出的预测控制器能够有效地缓解由涡轮机动态引起的时延，并证明了该技术的成功实施和验证。&lt;h4&gt;结论&lt;/h4&gt;研究结果为无人机社区提供了有价值的见解，展示了推力矢量控制与差动推进相结合的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Turbojet-powered VTOL UAVs have garnered increased attention in heavy-loadtransport and emergency services, due to their superior power density andthrust-to-weight ratio compared to existing electronic propulsion systems. Themain challenge with jet-powered UAVs lies in the complexity of thrust vectoringmechanical systems, which aim to mitigate the slow dynamics of the turbojet. Inthis letter, we introduce a novel turbojet-powered UAV platform named Hex-Jet.Our concept integrates thrust vectoring and differential thrust forcomprehensive attitude control. This approach notably simplifies the thrustvectoring mechanism. We utilize a predictor-based time delay control methodbased on the frequency domain model in our Hex-Jet controller design tomitigate the delay in roll attitude control caused by turbojet dynamics. Ourcomparative studies provide valuable insights for the UAV community, and flighttests on the scaled prototype demonstrate the successful implementation andverification of the proposed predictor-based time delay control technique.</description>
      <author>example@mail.com (Junning Liang, Haowen Zheng, Yuying Zhang, Yongzhuo Gao, Wei Dong, Ximin Lyu)</author>
      <guid isPermaLink="false">2503.09148v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding</title>
      <link>http://arxiv.org/abs/2503.09143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project: https://egovisiongroup.github.io/Exo2Ego.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的方法来改进多模态大型语言模型（MLLM）在第一人称视角视频理解上的性能，通过构建Ego-ExoClip数据集和开发一种逐步训练流程。&lt;h4&gt;背景&lt;/h4&gt;当前的多模态大型语言模型主要关注第三人称视觉理解和处理成本高昂的数据获取问题，限制了这些模型在第一人称视频理解中的应用。&lt;h4&gt;目的&lt;/h4&gt;为了提高MLLM对第一人称视角视频的理解能力，并解决数据收集成本高的问题，论文提出了一个创新的学习方案。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的预训练数据集Ego-ExoClip，包含1.1M同步的第一和第三人称视图与文本配对。同时引入了一个逐步的训练流程：教师自我准备、师生指导和学生自主实践，以及强化模型指令跟随能力的数据源。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示现有MLLM在第一人称视频理解上表现不佳，而新提出的方法显著提高了这一领域的性能。&lt;h4&gt;结论&lt;/h4&gt;通过跨多种任务的广泛实验证明了所提方法的有效性，并提出了一个新的基准测试EgoBench以全面评估模型的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI personal assistants, deployed through robots or wearables, requireembodied understanding to collaborate effectively with humans. CurrentMultimodal Large Language Models (MLLMs) primarily focus on third-person(exocentric) vision, overlooking the unique aspects of first-person(egocentric) videos. Additionally, high acquisition costs limit data size,impairing MLLM performance. To address these challenges, we propose learningthe mapping between exocentric and egocentric domains, leveraging the extensiveexocentric knowledge within existing MLLMs to enhance egocentric videounderstanding. To this end, we introduce Ego-ExoClip, a pre-training datasetcomprising 1.1M synchronized ego-exo clip-text pairs derived from Ego-Exo4D.Our approach features a progressive training pipeline with three stages:Teacher Self-Preparation, Teacher-Student Guidance, and Student Self-Practice.Additionally, we propose an instruction-tuning data EgoIT from multiple sourcesto strengthen the model's instruction-following capabilities, along with theEgoBench benchmark comprising eight different tasks for thorough evaluation.Extensive experiments across diverse egocentric tasks reveal that existingMLLMs perform inadequately in egocentric video understanding, while our modelsignificantly outperforms these leading models.</description>
      <author>example@mail.com (Haoyu Zhang, Qiaohui Chu, Meng Liu, Yunxiao Wang, Bin Wen, Fan Yang, Tingting Gao, Di Zhang, Yaowei Wang, Liqiang Nie)</author>
      <guid isPermaLink="false">2503.09143v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Tacchi 2.0: A Low Computational Cost and Comprehensive Dynamic Contact Simulator for Vision-based Tactile Sensors</title>
      <link>http://arxiv.org/abs/2503.09100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;随着机器人技术的发展，一些基于视觉的触觉传感器已被应用于丰富的接触任务中。然而，基于视觉的触觉传感器的耐用性显著增加了获取触觉信息的成本。&lt;h4&gt;目的&lt;/h4&gt;为了克服上述问题，利用仿真生成触觉数据成为一种可靠的方法。通过改进现有的触觉模拟器Tacchi，引入了新的版本Tacchi 2.0。&lt;h4&gt;方法&lt;/h4&gt;在低计算成本的基于视觉的触觉模拟器Tacchi中集成针孔相机模型，并使用材料点法（MPM）作为仿真方法，完成了标记运动图像的模拟。&lt;h4&gt;主要发现&lt;/h4&gt;升级后的Tacchi可以模拟不同动作状态下的触摸图像、标记运动图像和关节图像。实验结果表明了我们方法的可靠性和在各种基于视觉的触觉传感器中的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过结合针孔相机模型和材料点法（MPM），提高了基于视觉的触觉数据生成的方法，实现了低计算成本且具有高准确性的仿真。&lt;h4&gt;翻译&lt;/h4&gt;随着机器人技术的进步，一些触摸式感应器如基于视觉的技术被应用于需要丰富接触的任务中。然而，这种基于视觉的传感器由于耐用性差而增加了获取触摸信息的成本。为了应对这个问题，利用模拟生成触觉数据成为一种可靠的方法。尽管这种方法存在缺乏鲁棒性和计算成本高的问题，我们改进了现有的低计算成本的基于视觉的触觉模拟器Tacchi，并引入了一个新的版本Tacchi 2.0。这个新版本的仿真器可以生成不同动作状态下（如按压、滑动和旋转）的触摸图像、标记运动图像和关节图像。实验结果显示这种方法具有可靠性和在各种基于视觉的触觉传感器中的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the development of robotics technology, some tactile sensors, such asvision-based sensors, have been applied to contact-rich robotics tasks.However, the durability of vision-based tactile sensors significantly increasesthe cost of tactile information acquisition. Utilizing simulation to generatetactile data has emerged as a reliable approach to address this issue. Whiledata-driven methods for tactile data generation lack robustness, finite elementmethods (FEM) based approaches require significant computational costs. Toaddress these issues, we integrated a pinhole camera model into the lowcomputational cost vision-based tactile simulator Tacchi that used the MaterialPoint Method (MPM) as the simulated method, completing the simulation of markermotion images. We upgraded Tacchi and introduced Tacchi 2.0. This simulator cansimulate tactile images, marked motion images, and joint images under differentmotion states like pressing, slipping, and rotating. Experimental resultsdemonstrate the reliability of our method and its robustness across variousvision-based tactile sensors.</description>
      <author>example@mail.com (Yuhao Sun, Shixin Zhang, Wenzhuang Li, Jie Zhao, Jianhua Shan, Zirong Shen, Zixi Chen, Fuchun Sun, Di Guo, Bin Fang)</author>
      <guid isPermaLink="false">2503.09100v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Data-Driven Inverse Optimal Control for Continuous-Time Nonlinear Systems</title>
      <link>http://arxiv.org/abs/2503.09090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要类型&lt;/h4&gt;研究型论文&lt;h4&gt;背景&lt;/h4&gt;当前的逆强化学习(IOC)或称为逆最优控制(IRL)算法在处理连续时间非线性确定系统中的成本函数估计方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种模型无关和部分模型无关的算法，用于估计连续时间非线性确定系统的成本函数。&lt;h4&gt;方法&lt;/h4&gt;利用专家代理的输入-状态轨迹信息，并采用控制策略信息以及Hamilton-Jacobi-Bellman方程来估计不同的成本函数参数集。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的算法具有广泛的应用性和模型无关特性，同时减少了现有方法中的复杂性，仅需在初始化时解决一次正向最优控制问题。对于已知输入动态的系统，在部分模型无关算法中可以完全绕过这一步骤。&lt;h4&gt;结论&lt;/h4&gt;通过模拟结果展示了算法的有效性和效率，强调了其在未来自主系统和机器人技术领域部署的潜力。&lt;h4&gt;翻译&lt;/h4&gt;该论文介绍了一种无模型和部分无模型的新逆最优控制(IOC)或称为逆强化学习(IRL)算法，旨在估计连续时间非线性确定系统的成本函数。使用专家代理的输入-状态轨迹信息，所提出的方法分别利用了控制策略的信息以及Hamilton-Jacobi-Bellman方程来估计不同的成本函数参数集。这种方法使得算法具有更广泛的应用可能性，并保持无模型框架。此外，无模型算法相比现有方法减少了复杂度，在初始化时仅需解决一次正向最优控制问题。对于输入动态已知的系统，在部分模型无关算法中可以完全绕过这一步骤。模拟结果验证了所提出算法的有效性和效率，突显了其在自主系统和机器人技术领域中的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel model-free and a partially model-free algorithmfor inverse optimal control (IOC), also known as inverse reinforcement learning(IRL), aimed at estimating the cost function of continuous-time nonlineardeterministic systems. Using the input-state trajectories of an expert agent,the proposed algorithms separately utilize control policy information and theHamilton-Jacobi-Bellman equation to estimate different sets of cost functionparameters. This approach allows the algorithms to achieve broaderapplicability while maintaining a model-free framework. Also, the model-freealgorithm reduces complexity compared to existing methods, as it requiressolving a forward optimal control problem only once during initialization.Furthermore, in our partially model-free algorithm, this step can be bypassedentirely for systems with known input dynamics. Simulation results demonstratethe effectiveness and efficiency of our algorithms, highlighting theirpotential for real-world deployment in autonomous systems and robotics.</description>
      <author>example@mail.com (Hamed Jabbari Asl, Eiji Uchibe)</author>
      <guid isPermaLink="false">2503.09090v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Motion Blender Gaussian Splatting for Dynamic Reconstruction</title>
      <link>http://arxiv.org/abs/2503.09040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Gaussian splatting是一种用于动态场景高保真重建的有力工具，但现有的方法大多依赖于隐式运动表示。因此难以进一步操控重建后的动作。&lt;h4&gt;背景&lt;/h4&gt;当前的Gaussian splatting技术主要通过将运动编码进神经网络或每个高斯参数中来进行运动表示，这种做法使得对运动的操作变得困难且缺乏显式的可控性。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的框架Motion Blender Gaussian Splatting (MB-GS)，使用运动图作为明确和稀疏的运动表示方法。&lt;h4&gt;方法&lt;/h4&gt;该框架利用双四元数蒙皮将图链接上的运动传播到各个高斯上，并通过可学习的权重涂色函数确定每个链接的影响。整个模型通过差异渲染技术从输入视频中共同优化运动图和3D高斯分布。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MB-GS在iPhone数据集上的性能达到现有技术水平，在HyperNeRF数据集中也具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;该方法不仅实现了高质量的动态场景重建，还展示了其通过运动编辑生成新对象动作和机器人演示的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;Gaussian splatting作为一种新兴的技术，已经成为了高保真度动态场景重构的强大工具。然而，现有的大部分技术主要依赖于隐式表达（例如将运动编码进神经网络或单个高斯参数中）来进行这种表示，这使得对重建后的动作进行进一步的操控变得极其困难。缺乏显式的可控性限制了这些方法只能重复已录制的动作，从而阻碍了它们更广泛的使用场景。为了解决这一问题，我们提出了一种新的框架——Motion Blender Gaussian Splatting (MB-GS)，该框架首次引入运动图作为明确且稀疏的运动表示。通过双四元数蒙皮技术将这种在连接上的运动传播到各个高斯分布上，并利用可学习权重涂色函数确定每一个链接的影响大小。整个模型从输入视频中共同优化运动图和3D高斯分布，这都是通过差异渲染技术实现的。实验结果表明，在iPhone数据集上，MB-GS的表现达到了最新的技术水平；在HyperNeRF数据集中也具有很高的竞争力。此外，我们还展示了该方法在生成新的对象动作以及机器人演示方面的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gaussian splatting has emerged as a powerful tool for high-fidelityreconstruction of dynamic scenes. However, existing methods primarily rely onimplicit motion representations, such as encoding motions into neural networksor per-Gaussian parameters, which makes it difficult to further manipulate thereconstructed motions. This lack of explicit controllability limits existingmethods to replaying recorded motions only, which hinders a wider application.To address this, we propose Motion Blender Gaussian Splatting (MB-GS), a novelframework that uses motion graph as an explicit and sparse motionrepresentation. The motion of graph links is propagated to individual Gaussiansvia dual quaternion skinning, with learnable weight painting functionsdetermining the influence of each link. The motion graphs and 3D Gaussians arejointly optimized from input videos via differentiable rendering. Experimentsshow that MB-GS achieves state-of-the-art performance on the iPhone datasetwhile being competitive on HyperNeRF. Additionally, we demonstrate theapplication potential of our method in generating novel object motions androbot demonstrations through motion editing. Video demonstrations can be foundat https://mlzxy.github.io/mbgs.</description>
      <author>example@mail.com (Xinyu Zhang, Haonan Chang, Yuhan Liu, Abdeslam Boularias)</author>
      <guid isPermaLink="false">2503.09040v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>ManeuverGPT Agentic Control for Safe Autonomous Stunt Maneuvers</title>
      <link>http://arxiv.org/abs/2503.09035v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 Pages, Submitted to IROS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种名为ManeuverGPT的新框架，利用基于大规模语言模型（LLM）的代理控制器在自动驾驶车辆中生成和执行高动态特技机动。&lt;h4&gt;背景&lt;/h4&gt;下一代主动安全特性应当能够像专业特技驾驶员一样进行规避危险的动作，以实现极限条件下的高速度、灵活操作。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，在CARLA仿真环境中利用大规模语言模型代理控制器来进行激进动作（如J转）的生成和执行。&lt;h4&gt;方法&lt;/h4&gt;提出了一种具有三个专门代理（查询丰富者、驾驶员代理以及参数验证者）的代理架构，使用迭代提示方法来优化控制参数，无需重新训练模型权重。&lt;h4&gt;主要发现&lt;/h4&gt;通过文本指令成功地实现了不同车辆型号下的J转动作，并根据现有的标准评估了性能表现。讨论了数字精度和场景复杂度的限制。&lt;h4&gt;结论&lt;/h4&gt;研究强调了LLM驱动控制在灵活、高动态机动方面的潜力，同时指出了结合语言推理与算法验证的混合方法的重要性。&lt;h4&gt;翻译&lt;/h4&gt;下一代自动驾驶车辆中的主动安全特性应该能够执行类似专业特技驾驶员所进行的高度敏捷运动，以避开危险。本文介绍了一种名为ManeuverGPT的新框架，利用基于大规模语言模型（LLM）的代理来生成和执行高动态特技机动。研究针对CARLA仿真环境内的激进动作进行了目标设定，并展示了无需重新训练权重即可通过迭代提示方法优化控制参数的方法。实验结果证明了在多种车辆型号中成功执行J转的能力，评估性能时采用了一套既定的成功标准，并讨论了数字精度和场景复杂度的局限性。这些发现表明LLM驱动控制对于灵活、高动态机动具有潜力，同时也强调了结合语言推理与算法验证的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The next generation of active safety features in autonomous vehicles shouldbe capable of safely executing evasive hazard-avoidance maneuvers akin to thoseperformed by professional stunt drivers to achieve high-agility motion at thelimits of vehicle handling. This paper presents a novel framework, ManeuverGPT,for generating and executing high-dynamic stunt maneuvers in autonomousvehicles using large language model (LLM)-based agents as controllers. Wetarget aggressive maneuvers, such as J-turns, within the CARLA simulationenvironment and demonstrate an iterative, prompt-based approach to refinevehicle control parameters, starting tabula rasa without retraining modelweights. We propose an agentic architecture comprised of three specializedagents (1) a Query Enricher Agent for contextualizing user commands, (2) aDriver Agent for generating maneuver parameters, and (3) a Parameter ValidatorAgent that enforces physics-based and safety constraints. Experimental resultsdemonstrate successful J-turn execution across multiple vehicle models throughtextual prompts that adapt to differing vehicle dynamics. We evaluateperformance via established success criteria and discuss limitations regardingnumeric precision and scenario complexity. Our findings underscore thepotential of LLM-driven control for flexible, high-dynamic maneuvers, whilehighlighting the importance of hybrid approaches that combine language-basedreasoning with algorithmic validation.</description>
      <author>example@mail.com (Shawn Azdam, Pranav Doma, Aliasghar Moj Arab)</author>
      <guid isPermaLink="false">2503.09035v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>RFUAV: A Benchmark Dataset for Unmanned Aerial Vehicle Detection and Identification</title>
      <link>http://arxiv.org/abs/2503.09033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 13 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出RFUAV数据集，用于基于无线电频率的无人机识别，并解决现有数据集在无人机类型多样性、信号噪声比范围和开放评估工具方面的不足。&lt;h4&gt;背景&lt;/h4&gt;现有的许多数据集包含较少类型的无人机和原始数据量不足以满足实际应用需求。此外，这些数据集缺乏不同信噪比下的原始数据或没有提供转换为各种SNR级别的方法。&lt;h4&gt;目的&lt;/h4&gt;提出RFUAV数据集来解决上述问题，并通过开放评估工具促进统一的评价标准建立。&lt;h4&gt;方法&lt;/h4&gt;RFUAV包含约1.3TB由USRP设备采集自37种不同无人机的真实环境中获取的原始频率数据。该数据集还提供基线预处理方法和模型评估工具。&lt;h4&gt;主要发现&lt;/h4&gt;通过深入分析RFUAV中的射频数据，定义了名为“无线电波指纹”的无人机特征序列，有助于区分无人机信号，并证明这些预处理方法利用提供的评估工具达到了当前的最佳性能。&lt;h4&gt;结论&lt;/h4&gt;RFUAV及其基线实现是公开的，旨在促进基于无线频率的无人机识别研究的发展。&lt;h4&gt;翻译&lt;/h4&gt;在这篇文章中，我们提出了一种新的基准数据集RFUAV用于无线电频率（RF）无人飞行器（UAV）标识，并解决了以下挑战：首先，许多现有数据集包含较少类型的无人机和原始数据量不足以满足实际应用需求。其次，现有数据集通常缺乏覆盖不同信噪比范围的原始数据或没有提供转换为各种SNR级别的方法。这些问题影响了模型训练和评估的有效性。最后，许多现有的数据集不提供开放访问的评估工具，导致当前研究领域中统一评估标准的缺失。RFUAV包含大约1.3TB由USRP设备从37种不同无人机的真实环境中获取的原始频率数据。通过深入分析射频数据在RFUAV中的表现，我们定义了一个名为“无线电波指纹”的无人机特征序列，有助于区分无人机信号。除了提供数据集外，RFUAV还提供了基线预处理方法和模型评估工具。严格的实验表明，这些预处理方法使用提供的评估工具达到了当前的最佳性能。RFUAV数据集及其基线实现可在https://github.com/kitoweeknd/RFUAV/上公开访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose RFUAV as a new benchmark dataset forradio-frequency based (RF-based) unmanned aerial vehicle (UAV) identificationand address the following challenges: Firstly, many existing datasets feature arestricted variety of drone types and insufficient volumes of raw data, whichfail to meet the demands of practical applications. Secondly, existing datasetsoften lack raw data covering a broad range of signal-to-noise ratios (SNR), ordo not provide tools for transforming raw data to different SNR levels. Thislimitation undermines the validity of model training and evaluation. Lastly,many existing datasets do not offer open-access evaluation tools, leading to alack of unified evaluation standards in current research within this field.RFUAV comprises approximately 1.3 TB of raw frequency data collected from 37distinct UAVs using the Universal Software Radio Peripheral (USRP) device inreal-world environments. Through in-depth analysis of the RF data in RFUAV, wedefine a drone feature sequence called RF drone fingerprint, which aids indistinguishing drone signals. In addition to the dataset, RFUAV provides abaseline preprocessing method and model evaluation tools. Rigorous experimentsdemonstrate that these preprocessing methods achieve state-of-the-art (SOTA)performance using the provided evaluation tools. The RFUAV dataset and baselineimplementation are publicly available at https://github.com/kitoweeknd/RFUAV/.</description>
      <author>example@mail.com (Rui Shi, Xiaodong Yu, Shengming Wang, Yijia Zhang, Lu Xu, Peng Pan, Chunlai Ma)</author>
      <guid isPermaLink="false">2503.09033v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Traffic Regulation-aware Path Planning with Regulation Databases and Vision-Language Models</title>
      <link>http://arxiv.org/abs/2503.09024v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures, submitted to ICRA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文介绍并测试了一个将交通法规遵守功能集成到自动驾驶系统中的框架。&lt;h4&gt;背景&lt;/h4&gt;当前的自动驾驶系统缺乏对交通法律规则的有效遵守和基于驾驶环境的信息决策机制。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够使自动驾驶系统遵循交通法律法规，同时通过生成描述性文本支持知情决策过程，并在遵守法律限制的情况下指导未来的行驶计划的框架。&lt;h4&gt;方法&lt;/h4&gt;{'1': '开发了一个包含法规数据库的支持自动决策的系统', '2': '利用RGB相机输入和视觉语言模型（VLM）进行自动化规则意识路径规划的过程', '3': '该系统的性能在模拟环境和实际环境中进行了验证'}&lt;h4&gt;主要发现&lt;/h4&gt;真实世界车辆测试不仅评估了框架的表现，还评估了结合检测、推理和计划来解决复杂驾驶问题的视觉语言模型（VLM）的潜力和挑战。&lt;h4&gt;结论&lt;/h4&gt;这项工作增强了自动驾驶系统的合法性、安全性和公众信任，在该领域迈出了重要一步。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了并测试了一种框架，将交通法规遵守功能集成到自动驱动系统(ADS)中。该框架使ADS能够遵循交通法律，并根据驾驶环境做出知情决策。通过使用RGB摄像头输入和视觉语言模型(VLM)，系统生成描述性文本以支持具有法规意识的决策过程，确保合法且安全的驾驶实践。这些信息与机器可读的ADS法规数据库结合在一起，指导未来的行驶计划，在遵守法律限制的情况下进行。主要特点包括：1）一个支持自动决策制定的法规数据库；2）使用传感器输入的具有法规意识路径规划的自动化过程；3）在模拟和真实世界环境中的验证。特别是，实际车辆测试不仅评估了框架的表现，还评估了视觉语言模型(VLM)解决复杂驾驶问题的潜力及挑战，这些通过结合检测、推理以及计划来实现。这项工作增强了自动驾驶系统的合法性、安全性和公众信任，在该领域迈出了重要的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces and tests a framework integrating traffic regulationcompliance into automated driving systems (ADS). The framework enables ADS tofollow traffic laws and make informed decisions based on the drivingenvironment. Using RGB camera inputs and a vision-language model (VLM), thesystem generates descriptive text to support a regulation-aware decision-makingprocess, ensuring legal and safe driving practices. This information iscombined with a machine-readable ADS regulation database to guide futuredriving plans within legal constraints. Key features include: 1) a regulationdatabase supporting ADS decision-making, 2) an automated process using sensorinput for regulation-aware path planning, and 3) validation in both simulatedand real-world environments. Particularly, the real-world vehicle tests notonly assess the framework's performance but also evaluate the potential andchallenges of VLMs to solve complex driving problems by integrating detection,reasoning, and planning. This work enhances the legality, safety, and publictrust in ADS, representing a significant step forward in the field.</description>
      <author>example@mail.com (Xu Han, Zhiwen Wu, Xin Xia, Jiaqi Ma)</author>
      <guid isPermaLink="false">2503.09024v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Feasibility-aware Imitation Learning from Observations through a Hand-mounted Demonstration Interface</title>
      <link>http://arxiv.org/abs/2503.09018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FABCO的行为克隆框架，该框架通过评估人类示范动作的可行性来改进机器人自动化政策的学习。&lt;h4&gt;背景&lt;/h4&gt;模仿学习从直观的人类演示中学习机器人自动化的策略很有前景，但由于人和机器人的运动特性不同，可能导致专家无意间展示出机器人无法执行的动作。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于观察的行为克隆方法（FABCO），利用机器人预先训练的前向和逆动力学模型来评估每个示范动作的可行性，并通过视觉反馈帮助人类演示者改进他们的演示。&lt;h4&gt;方法&lt;/h4&gt;在FABCO框架中，使用机器人的预训练动态模型来估算每项任务的可行性，并将其作为权重应用于演示数据，以提高学习策略的数据效率和鲁棒性。还采用NASA任务负载指数（NASA-TLX）评估视觉反馈带来的工作量影响。&lt;h4&gt;主要发现&lt;/h4&gt;通过应用FABCO到移液管插入任务实验中验证了该方法的有效性，四个参与者对可行性反馈及加权政策学习在FABCO中的作用进行了评估。&lt;h4&gt;结论&lt;/h4&gt;FABCO框架可以有效提高机器人从人类演示中学得策略的可行性和效率，同时降低了因人类和机器运动特性差异导致的学习误差。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation learning through a demonstration interface is expected to learnpolicies for robot automation from intuitive human demonstrations. However, dueto the differences in human and robot movement characteristics, a human expertmight unintentionally demonstrate an action that the robot cannot execute. Wepropose feasibility-aware behavior cloning from observation (FABCO). In theFABCO framework, the feasibility of each demonstration is assessed using therobot's pre-trained forward and inverse dynamics models. This feasibilityinformation is provided as visual feedback to the demonstrators, encouragingthem to refine their demonstrations. During policy learning, estimatedfeasibility serves as a weight for the demonstration data, improving both thedata efficiency and the robustness of the learned policy. We experimentallyvalidated FABCO's effectiveness by applying it to a pipette insertion taskinvolving a pipette and a vial. Four participants assessed the impact of thefeasibility feedback and the weighted policy learning in FABCO. Additionally,we used the NASA Task Load Index (NASA-TLX) to evaluate the workload induced bydemonstrations with visual feedback.</description>
      <author>example@mail.com (Kei Takahashi, Hikaru Sasaki, Takamitsu Matsubara)</author>
      <guid isPermaLink="false">2503.09018v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness</title>
      <link>http://arxiv.org/abs/2503.10624v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Page: https://boqian-li.github.io/ETCH/, Code:  https://github.com/boqian-li/ETCH&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的3D着装人体点云拟合方法ETCH，该方法通过局部近似SE(3)等变性估计布料到身体的表面映射，并将紧度编码为从布料表面到下层身体的距离向量。这种方法使得穿着不同姿势和衣物类型的全身模型拟合更加准确。&lt;h4&gt;背景&lt;/h4&gt;目前基于优化的传统方法由于对姿态初始化敏感，而基于学习的方法在处理多样的姿势和服装类型时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提高布料人体点云到3D身体模型拟合精度的新方法，并且能够在宽松衣物条件下获得较好的效果。&lt;h4&gt;方法&lt;/h4&gt;通过局部近似SE(3)等变性，ETCH估计了从衣服表面到身体的映射关系，用向量表示紧度。然后基于这种映射简化布料人体拟合为内体标记点拟合任务，从而实现对不同姿势和衣物类型的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验，在CAPE和4D-Dress数据集上显示ETCH在宽松服装条件下的人体拟合准确度比现有的方法提高了16.7%到69.5%，并且形状准确性平均提高49.9%。此外，这种等变紧度设计能够减少一镜拍摄下的方向误差（67.2%-89.8%），展示出在未见过的姿势和服装条件下强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;ETCH方法有效提高了3D着装人体点云拟合精度，并且在宽松衣物情况下表现尤为突出，同时具备良好的姿态不变性和跨服装类型的适应性。在未来，该研究将开放源代码和模型用于科研用途。&lt;h4&gt;翻译&lt;/h4&gt;拟合一个穿着衣服的人体到3D点云是一个既常见又具有挑战性的任务。传统的基于优化的方法使用多阶段管道，这些管道对姿势初始化敏感，而最近的学习方法在处理各种姿势和服装类型时通常面临泛化难题。我们提出了一种新的管道Equivariant Tightness Fitting for Clothed Humans（ETCH），它通过局部近似SE(3)等变性估计布料到身体表面的映射，并将紧度编码为从布料表面到下层身体的距离向量。随后，姿势不变的身体特征回归稀疏的身体标记点，简化了穿着人体拟合任务成为内体标记点拟合问题。广泛的实验在CAPE和4D-Dress数据集上显示，ETCH在宽松服装条件下的人体拟合准确度显著优于最先进的方法（无论是紧度无关的还是紧度相关的）——平均而言，身体拟合精度提高了16.7%到69.5%，形状准确性平均提高49.9%。我们的等变紧度设计甚至可以减少方向错误（在一次性或分布外设置中为67.2%-89.8%）。定性结果表明ETCH具有强大的泛化能力，不论是对困难的姿势、未见过的身体形状、宽松服装还是非刚性动态都有很好的表现。研究团队计划在未来发布代码和模型用于科研用途（https://boqian-li.github.io/ETCH/）以支持进一步的研究工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fitting a body to a 3D clothed human point cloud is a common yet challengingtask. Traditional optimization-based approaches use multi-stage pipelines thatare sensitive to pose initialization, while recent learning-based methods oftenstruggle with generalization across diverse poses and garment types. We proposeEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipelinethat estimates cloth-to-body surface mapping through locally approximate SE(3)equivariance, encoding tightness as displacement vectors from the cloth surfaceto the underlying body. Following this mapping, pose-invariant body featuresregress sparse body markers, simplifying clothed human fitting into aninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress showthat ETCH significantly outperforms state-of-the-art methods -- bothtightness-agnostic and tightness-aware -- in body fitting accuracy on looseclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivarianttightness design can even reduce directional errors by (67.2% ~ 89.8%) inone-shot (or out-of-distribution) settings. Qualitative results demonstratestrong generalization of ETCH, regardless of challenging poses, unseen shapes,loose clothing, and non-rigid dynamics. We will release the code and modelssoon for research purposes at https://boqian-li.github.io/ETCH/.</description>
      <author>example@mail.com (Boqian Li, Haiwen Feng, Zeyu Cai, Michael J. Black, Yuliang Xiu)</author>
      <guid isPermaLink="false">2503.10624v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban Scene Reconstruction</title>
      <link>http://arxiv.org/abs/2503.10604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MuDG是一个结合多模态扩散模型与高斯点阵的框架，用于城市场景重建。该框架利用聚合的LiDAR点云、RGB和几何先验条件来训练一个多模态视频扩散模型，从而生成逼真的RGB图像、深度图及语义输出。&lt;h4&gt;背景&lt;/h4&gt;基于辐射场的技术在3D场景重构和新颖视角合成（NVS）方面取得了显著进展，但现有方法存在一些局限性：基于重建的方法对视点偏差敏感；而基于生成的方法难以保证时间连贯性和精确的场景控制。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些挑战，提出了MuDG框架，旨在提高3D场景重构和新颖视角合成的质量。&lt;h4&gt;方法&lt;/h4&gt;利用LiDAR点云、RGB图像及几何先验条件来训练一个多模态视频扩散模型，该模型能够生成高保真的RGB图像、深度图和语义标签输出，并且能够在极端视点变化下提供鲁棒的渲染表现。&lt;h4&gt;主要发现&lt;/h4&gt;MuDG框架在Open Waymo数据集上的实验结果表明，在重建和合成质量方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;MuDG通过结合多模态扩散模型与高斯点阵，有效地解决了基于辐射场技术中存在的问题，为3D场景的准确重构及新颖视角生成提供了新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent breakthroughs in radiance fields have significantly advanced 3D scenereconstruction and novel view synthesis (NVS) in autonomous driving.Nevertheless, critical limitations persist: reconstruction-based methodsexhibit substantial performance deterioration under significant viewpointdeviations from training trajectories, while generation-based techniquesstruggle with temporal coherence and precise scene controllability. To overcomethese challenges, we present MuDG, an innovative framework that integratesMulti-modal Diffusion model with Gaussian Splatting (GS) for Urban SceneReconstruction. MuDG leverages aggregated LiDAR point clouds with RGB andgeometric priors to condition a multi-modal video diffusion model, synthesizingphotorealistic RGB, depth, and semantic outputs for novel viewpoints. Thissynthesis pipeline enables feed-forward NVS without computationally intensiveper-scene optimization, providing comprehensive supervision signals to refine3DGS representations for rendering robustness enhancement under extremeviewpoint changes. Experiments on the Open Waymo Dataset demonstrate that MuDGoutperforms existing methods in both reconstruction and synthesis quality.</description>
      <author>example@mail.com (Yingshuang Zou, Yikang Ding, Chuanrui Zhang, Jiazhe Guo, Bohan Li, Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Haoqian Wang)</author>
      <guid isPermaLink="false">2503.10604v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>PiSA: A Self-Augmented Data Engine and Training Strategy for 3D Understanding with Large Models</title>
      <link>http://arxiv.org/abs/2503.10529v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种新的框架PiSA-Engine，用于生成高质量的3D点云指令数据集，并开发了改进的模型PointLLM-PiSA。同时提出了一个新的基准测试集PiSA-Bench，以评估改进后的模型在零样本3D对象描述和生成分类任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;当前的3D多模态大型语言模型（MLLMs）由于数据集数量有限和质量不足导致其潜力未被充分利用。现有的方法试图从2D MLLMs迁移知识以扩展3D指令数据，但仍然存在模式和领域差距问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架PiSA-Engine来解决现有3D多模态大型语言模型存在的问题，并开发一个改进的模型PointLLM-PiSA。同时针对现有的3D基准测试中存在的缺陷，提出了一个新的全面的3D基准测试集PiSA-Bench。&lt;h4&gt;方法&lt;/h4&gt;引入了点增强引擎（PiSA-Engine），该框架利用现有2D和3D MLLMs的整体见解来生成高质量的3D数据集，并将PointLLM作为基线模型进行联合训练以提高其性能。同时提出了一个综合基准测试PiSA-Bench，它涵盖了六个关键方面。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用PiSA-Engine框架生成的数据集训练改进后的模型PointLLM-PiSa，在零样本3D对象描述和生成分类任务上显示出卓越的性能。实验结果表明PointLLM-PiSA在PiSA-Bench上的测试中分别取得了46.45%（+8.33%）和63.75%（+16.25%）的显著改进。&lt;h4&gt;结论&lt;/h4&gt;通过提出的新框架和基准集，论文提供了一个有效的途径来提高现有3D多模态大型语言模型的性能，并为未来的相关研究提供了重要的参考。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Multimodal Large Language Models (MLLMs) have recently made substantialadvancements. However, their potential remains untapped, primarily due to thelimited quantity and suboptimal quality of 3D datasets. Current approachesattempt to transfer knowledge from 2D MLLMs to expand 3D instruction data, butstill face modality and domain gaps. To this end, we introduce PiSA-Engine(Point-Self-Augmented-Engine), a new framework for generating instructionpoint-language datasets enriched with 3D spatial semantics. We observe thatexisting 3D MLLMs offer a comprehensive understanding of point clouds forannotation, while 2D MLLMs excel at cross-validation by providing complementaryinformation. By integrating holistic 2D and 3D insights from off-the-shelfMLLMs, PiSA-Engine enables a continuous cycle of high-quality data generation.We select PointLLM as the baseline and adopt this co-evolution trainingframework to develop an enhanced 3D MLLM, termed PointLLM-PiSA. Additionally,we identify limitations in previous 3D benchmarks, which often feature coarselanguage captions and insufficient category diversity, resulting in inaccurateevaluations. To address this gap, we further introduce PiSA-Bench, acomprehensive 3D benchmark covering six key aspects with detailed and diverselabels. Experimental results demonstrate PointLLM-PiSA's state-of-the-artperformance in zero-shot 3D object captioning and generative classification onour PiSA-Bench, achieving significant improvements of 46.45% (+8.33%) and63.75% (+16.25%), respectively. We will release the code, datasets, andbenchmark.</description>
      <author>example@mail.com (Zilu Guo, Hongbin Lin, Zhihao Yuan, Chaoda Zheng, Pengshuo Qiu, Dongzhi Jiang, Renrui Zhang, Chun-Mei Feng, Zhen Li)</author>
      <guid isPermaLink="false">2503.10529v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing</title>
      <link>http://arxiv.org/abs/2503.10639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Dataset and models are released in https://github.com/rongyaofang/GoT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了Generation Chain-of-Thought (GoT)，一种新的生成和编辑图像的方法，通过语言推理过程明确化处理文本提示。&lt;h4&gt;背景&lt;/h4&gt;当前的图像生成和编辑方法主要将文本提示作为直接输入处理，而不考虑视觉组成和显式操作。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新型框架GoT，旨在通过语言推理过程来指导图像生成和编辑，并分析语义关系和空间排列。&lt;h4&gt;方法&lt;/h4&gt;定义了GoT的形式化表达并构建了包含超过900万样本的大型数据集，这些样本详细记录了语义-空间关系。为了利用GoT的优势，实施了一个统一框架，结合Qwen2.5-VL生成推理链，并增强扩散模型以融入新的语义-空间引导模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，GoT框架在图像生成和编辑任务上表现出色，在基线之上取得了显著改善。此外，该方法支持交互式视觉生成，允许用户明确修改推理步骤进行精确的图像调整。&lt;h4&gt;结论&lt;/h4&gt;GoT开创了基于推理驱动的可视化生成与编辑的新方向，并且公开提供了数据集、代码及预训练模型以促进未来的研究。&lt;h4&gt;翻译&lt;/h4&gt;当前的图像生成和编辑方法主要处理文本提示作为直接输入，而没有考虑视觉组成和显式操作。我们提出了Generation Chain-of-Thought (GoT)，这是一种通过语言推理过程进行明确化处理的新方法，在输出图像之前分析语义关系和空间排列。该方法将传统的文本到图像的生成和编辑转变为一个以推理指导为框架的过程，并定义了GoT的形式化表达，构建了一个包含超过900万样本的大型数据集，每个样本都详细记录了推理链来捕捉语义-空间关系。为了利用GoT的优势，我们实现了一个统一框架，将Qwen2.5-VL用于生成推理链，并通过我们的新型语义-空间引导模块增强了端到端扩散模型。实验结果显示，在图像生成和编辑任务上，GoT框架比基线方法表现出色并取得显著改善。此外，该方法支持交互式视觉生成，允许用户明确地修改推理步骤来进行精确的图像调整。GoT开创了一种新的基于推理驱动的可视化生成与编辑方向，可以产生更符合人类意图的图像。为了促进未来研究，我们在https://github.com/rongyaofang/GoT上提供了数据集、代码和预训练模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current image generation and editing methods primarily process textualprompts as direct inputs without reasoning about visual composition andexplicit operations. We present Generation Chain-of-Thought (GoT), a novelparadigm that enables generation and editing through an explicit languagereasoning process before outputting images. This approach transformsconventional text-to-image generation and editing into a reasoning-guidedframework that analyzes semantic relationships and spatial arrangements. Wedefine the formulation of GoT and construct large-scale GoT datasets containingover 9M samples with detailed reasoning chains capturing semantic-spatialrelationships. To leverage the advantages of GoT, we implement a unifiedframework that integrates Qwen2.5-VL for reasoning chain generation with anend-to-end diffusion model enhanced by our novel Semantic-Spatial GuidanceModule. Experiments show our GoT framework achieves excellent performance onboth generation and editing tasks, with significant improvements overbaselines. Additionally, our approach enables interactive visual generation,allowing users to explicitly modify reasoning steps for precise imageadjustments. GoT pioneers a new direction for reasoning-driven visualgeneration and editing, producing images that better align with human intent.To facilitate future research, we make our datasets, code, and pretrainedmodels publicly available at https://github.com/rongyaofang/GoT.</description>
      <author>example@mail.com (Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, Hongsheng Li)</author>
      <guid isPermaLink="false">2503.10639v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>DP-GPL: Differentially Private Graph Prompt Learning</title>
      <link>http://arxiv.org/abs/2503.10544v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Graph神经网络（GNNs）在多个应用中表现出色。最近，受语言和视觉基础模型的启发，图提示学习作为一种强大的GNN训练范式出现。&lt;h4&gt;背景&lt;/h4&gt;当前，预训练的GNN可以通过轻量级的图提示适应敏感任务，但使用来自敏感数据的提示存在隐私风险。&lt;h4&gt;目的&lt;/h4&gt;探讨实际环境下的图提示隐私风险，并提出一种保护机制来平衡隐私和效用之间的关系。&lt;h4&gt;方法&lt;/h4&gt;提出了基于PATE框架的DP-GPL算法，用于生成具有差分隐私保证的图提示。同时评估了标准隐私方法（如DP-SGD）在图提示学习中的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;发现现有的差分隐私方法无法提供有效的隐私效用平衡，尤其是在使用少量敏感数据点的情况下。而新提出的DP-GPL算法能够在强隐私保护的同时保持高实用性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效地缓解了隐私问题，并且在图域中保留了提示GNN作为强大基础模型的能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown remarkable performance in variousapplications. Recently, graph prompt learning has emerged as a powerful GNNtraining paradigm, inspired by advances in language and vision foundationmodels. Here, a GNN is pre-trained on public data and then adapted to sensitivetasks using lightweight graph prompts. However, using prompts from sensitivedata poses privacy risks. In this work, we are the first to investigate thesepractical risks in graph prompts by instantiating a membership inference attackthat reveals significant privacy leakage. We also find that the standardprivacy method, DP-SGD, fails to provide practical privacy-utility trade-offsin graph prompt learning, likely due to the small number of sensitive datapoints used to learn the prompts. As a solution, we propose DP-GPL fordifferentially private graph prompt learning based on the PATE framework, thatgenerates a graph prompt with differential privacy guarantees. Our evaluationacross various graph prompt learning methods, GNN architectures, andpre-training strategies demonstrates that our algorithm achieves high utilityat strong privacy, effectively mitigating privacy concerns while preserving thepowerful capabilities of prompted GNNs as powerful foundation models in thegraph domain.</description>
      <author>example@mail.com (Jing Xu, Franziska Boenisch, Iyiola Emmanuel Olatunji, Adam Dziedzic)</author>
      <guid isPermaLink="false">2503.10544v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>TARS: Traffic-Aware Radar Scene Flow Estimation</title>
      <link>http://arxiv.org/abs/2503.10210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的雷达场景流估计方法TARS，该方法利用交通层级的运动刚性来提高稀疏雷达点云中的场景流动态估算准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的LiDAR场景流动态模型假设物体为刚体，在实例级别上进行处理。然而这些实例级别的方法不适用于稀疏的雷达点云数据。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的TARS方法，利用交通层级上的运动刚性来解决现有雷达场景流估算中的挑战。&lt;h4&gt;方法&lt;/h4&gt;该工作通过同时执行物体检测和场景流估计，并且使用来自训练有检测损失的目标检测器特征图来增强场景流感知环境的能力。构建了交通矢量场（TVF）以实现整个交通层级的场景理解，在评估时，既考虑点级别的运动线索也考虑空间内刚性运动的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;TARS在专有数据集和View-of-Delft数据集上表现出了比现有技术更高的性能，分别提升了23%和15%。&lt;h4&gt;结论&lt;/h4&gt;通过使用交通层级的刚体运动一致性来解决雷达点云稀疏性的问题，并且提高了场景流估计的整体准确性。&lt;h4&gt;翻译&lt;/h4&gt;场景流动态提供了自主驾驶中至关重要的移动信息。最近的LiDAR场景流动态模型在实例级别上利用了刚性运动假设，即对象被视为刚体。然而这些实例级别的方法不适用于稀疏雷达点云数据。在此工作中我们提出了一种新颖的交通感知雷达场景流估计方法TARS，该方法利用了交通层级上的运动刚性。为了应对雷达场景流动态中的挑战，我们同时执行物体检测和场景流评估，并增强了后者的性能。我们将通过训练有目标损失的目标检测器获得的特征图集成到雷达场景流中，使后者能够感知环境及道路使用者。因此在我们的场景流分支中构建了交通矢量场（TVF），从而实现了整个交通层级的场景理解能力。在估算场景流动态时，我们同时考虑点级别的运动线索和空间内刚性运动的一致性。TARS分别在专有数据集和View-of-Delft数据集中比现有技术提高了23%和15%的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene flow provides crucial motion information for autonomous driving. RecentLiDAR scene flow models utilize the rigid-motion assumption at the instancelevel, assuming objects are rigid bodies. However, these instance-level methodsare not suitable for sparse radar point clouds. In this work, we present anovel $\textbf{T}$raffic-$\textbf{A}$ware $\textbf{R}$adar $\textbf{S}$ceneflow estimation method, named $\textbf{TARS}$, which utilizes the motionrigidity at the traffic level. To address the challenges in radar scene flow,we perform object detection and scene flow jointly and boost the latter. Weincorporate the feature map from the object detector, trained with detectionlosses, to make radar scene flow aware of the environment and road users.Therefrom, we construct a Traffic Vector Field (TVF) in the feature space,enabling a holistic traffic-level scene understanding in our scene flow branch.When estimating the scene flow, we consider both point-level motion cues frompoint neighbors and traffic-level consistency of rigid motion within the space.TARS outperforms the state of the art on a proprietary dataset and theView-of-Delft dataset, improving the benchmarks by 23% and 15%, respectively.</description>
      <author>example@mail.com (Jialong Wu, Marco Braun, Dominic Spata, Matthias Rottmann)</author>
      <guid isPermaLink="false">2503.10210v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning, Large-Scale 3D Molecular Pretraining, Molecular Property</title>
      <link>http://arxiv.org/abs/2503.10489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;分子预训练表示（MPR）在药物发现和材料设计等领域中处理有限监督数据的问题上表现出强大的能力。虽然早期的MPR方法依赖于1D序列和2D图，但最近的研究开始使用3D构象信息来捕捉原子间的复杂相互作用。然而这些模型仅仅将分子视为离散的原子集，忽略了周围的空间环境。&lt;h4&gt;背景&lt;/h4&gt;现有的分子预训练表示（MPR）模型主要基于1D序列、2D图形和部分利用了3D构象信息进行设计。不过它们通常只考虑了离散的原子点而没有充分利用周围的三维空间。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，通过将整个3D空间整合到分子预训练表示中来提高模型性能，特别是在数据有限的情况下。&lt;h4&gt;方法&lt;/h4&gt;首先观察到在原始分子结构上随机添加虚拟点能够提升MPR的表现。基于此发现，设计了一个称为SpaceFormer的新架构框架，该框架利用Transformer并包含三个关键组件：网格空间离散化、网格采样/合并以及高效的3D位置编码。&lt;h4&gt;主要发现&lt;/h4&gt;提出的SpaceFormer模型在各种下游任务中超越了之前的3D MPR模型，并证明了结合分子周围的空间信息可以显著提升性能。&lt;h4&gt;结论&lt;/h4&gt;通过考虑整个三维空间，而非仅仅关注离散的原子点，可以在药物发现和材料设计等领域实现更好的预训练表示。这为未来的工作提供了一个新的视角和研究方向。&lt;h4&gt;翻译&lt;/h4&gt;分子预训练表示（MPR）已经成为解决诸如药物发现和材料设计等应用中有限监督数据挑战的强大方法。尽管早期的方法依赖于1D序列和2D图，但最近的发展已经融入了3D构象信息以捕捉复杂的原子相互作用。然而，这些先前的模型将分子简单地视为离散的原子集合，忽略了它们周围的空间环境。从物理角度出发，我们指出仅对这些离散点进行建模是不够的。首先，我们提出了一个简单的但有启发性的观察：简单地在原子之外添加随机采样的虚拟点可以显著增强MPR的表现。基于这一发现，我们提出了一种利用分子所占据的整个3D空间的原则性框架。通过一种新的Transformer架构——SpaceFormer来实现该框架，并且包含三个关键组件：网格基的空间离散化、网格采样/合并和高效的三维位置编码。广泛的实验表明，SpaceFormer在具有有限数据的各种下游任务中显著优于先前的3D MPR模型，这验证了MPR模型利用原子以外的额外3D空间的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular pretrained representations (MPR) has emerged as a powerful approachfor addressing the challenge of limited supervised data in applications such asdrug discovery and material design. While early MPR methods relied on 1Dsequences and 2D graphs, recent advancements have incorporated 3Dconformational information to capture rich atomic interactions. However, theseprior models treat molecules merely as discrete atom sets, overlooking thespace surrounding them. We argue from a physical perspective that only modelingthese discrete points is insufficient. We first present a simple yet insightfulobservation: naively adding randomly sampled virtual points beyond atoms cansurprisingly enhance MPR performance. In light of this, we propose a principledframework that incorporates the entire 3D space spanned by molecules. Weimplement the framework via a novel Transformer-based architecture, dubbedSpaceFormer, with three key components: (1) grid-based space discretization;(2) grid sampling/merging; and (3) efficient 3D positional encoding. Extensiveexperiments show that SpaceFormer significantly outperforms previous 3D MPRmodels across various downstream tasks with limited data, validating thebenefit of leveraging the additional 3D space beyond atoms in MPR models.</description>
      <author>example@mail.com (Shuqi Lu, Xiaohong Ji, Bohang Zhang, Lin Yao, Siyuan Liu, Zhifeng Gao, Linfeng Zhang, Guolin Ke)</author>
      <guid isPermaLink="false">2503.10489v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>AMR-Transformer: Enabling Efficient Long-range Interaction for Complex Neural Fluid Simulation</title>
      <link>http://arxiv.org/abs/2503.10257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了AMR-Transformer，这是一种基于神经网络的计算流体动力学(CFD)求解器流水线，结合了自适应网格细化方案和Navier-Stokes约束感知快速剪枝模块。该方法在保持细节的同时提高了模拟效率。&lt;h4&gt;背景&lt;/h4&gt;准确且高效地模拟复杂的流体动力学是一个具有挑战性的任务，传统上依赖于计算密集型的方法。基于神经网络的近似方法，如卷积和图神经网络，在一定程度上通过支持高效的局部特征提取减轻了这种负担，但它们难以捕捉长程依赖关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够同时处理长程依赖性并减少计算成本的新模型。&lt;h4&gt;方法&lt;/h4&gt;AMR-Transformer结合了一个新颖的自适应网格细化方案和一个Navier-Stokes约束感知快速剪枝模块。该设计鼓励模拟单元之间的长时间相互作用，并促进了全球流体波模式，如湍流和冲击波的建模。&lt;h4&gt;主要发现&lt;/h4&gt;在CFDBench、PDEBench以及一个新的冲击波数据集上进行了实验验证，显示该方法相比基准模型有高达一个数量级的准确度提升。此外，与ViT相比，在保持精度的同时计算成本减少了60倍。&lt;h4&gt;结论&lt;/h4&gt;AMR-Transformer作为一种高效的神经网络CFD求解器流水线，展示了在处理具有长程依赖性的高分辨率物理模拟方面的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;精确且高效地仿真复杂的流体动力学是一个充满挑战的任务，传统方法通常非常计算密集。虽然基于神经网络的方法如卷积和图神经网络通过支持高效的局部特征提取部分减轻了这种负担，但它们难以捕捉长程依赖关系，并且Transformer模型提供全局上下文的同时也带来了不可接受的计算成本。为了解决这些挑战，我们提出了AMR-Transformer，一种结合自适应网格细化方案与Navier-Stokes约束感知快速剪枝模块的有效且精确的神经网络CFD求解器流水线设计。该设计鼓励模拟单元间的长时间相互作用并促进全球流体波模式（如湍流和冲击波）的建模。实验表明，我们的方法在保持关键细节的同时提高了效率，并适用于具有长程依赖性的高分辨率物理仿真。在CFDBench、PDEBench以及一个新创建的冲击波数据集上，我们展示了相较于基线模型高达10倍的准确度提升。另外，与ViT相比，我们的方法将计算量减少了60倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately and efficiently simulating complex fluid dynamics is a challengingtask that has traditionally relied on computationally intensive methods. Neuralnetwork-based approaches, such as convolutional and graph neural networks, havepartially alleviated this burden by enabling efficient local featureextraction. However, they struggle to capture long-range dependencies due tolimited receptive fields, and Transformer-based models, while providing globalcontext, incur prohibitive computational costs. To tackle these challenges, wepropose AMR-Transformer, an efficient and accurate neural CFD-solving pipelinethat integrates a novel adaptive mesh refinement scheme with a Navier-Stokesconstraint-aware fast pruning module. This design encourages long-rangeinteractions between simulation cells and facilitates the modeling of globalfluid wave patterns, such as turbulence and shockwaves. Experiments show thatour approach achieves significant gains in efficiency while preserving criticaldetails, making it suitable for high-resolution physical simulations withlong-range dependencies. On CFDBench, PDEBench and a new shockwave dataset, ourpipeline demonstrates up to an order-of-magnitude improvement in accuracy overbaseline models. Additionally, compared to ViT, our approach achieves areduction in FLOPs of up to 60 times.</description>
      <author>example@mail.com (Zeyi Xu, Jinfan Liu, Kuangxu Chen, Ye Chen, Zhangli Hu, Bingbing Ni)</author>
      <guid isPermaLink="false">2503.10257v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>CameraCtrl II: Dynamic Scene Exploration via Camera-controlled Video Diffusion Models</title>
      <link>http://arxiv.org/abs/2503.10592v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://hehao13.github.io/Projects-CameraCtrl-II/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;本文介绍了CameraCtrl II，这是一个框架，通过相机控制的视频扩散模型实现了大规模动态场景探索。此前基于相机条件的视频生成模型在产生包含大幅度摄像机运动的视频时会遇到视频动态减弱和视角范围有限的问题。我们采用了一种逐步扩展动态场景生成的方法——首先增强单个视频片段内的动态内容，然后将这种能力延伸到在整个宽广视角范围内进行无缝探索。&lt;h4&gt;背景&lt;/h4&gt;此前基于相机条件的视频生成模型在处理大幅度摄像机运动时存在问题，如动态减弱和视角范围有限等&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决上述问题，并实现大规模动态场景的探索&lt;h4&gt;方法&lt;/h4&gt;逐步扩展动态场景生成的方法：- 增强单个视频片段内的动态内容- 设计轻量级相机注入模块和训练方案以保持预训练模型的动力学特性- 允许用户通过迭代指定摄像机轨迹来生成连贯的视频序列&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，CameraCtrl II 框架能够实现比先前方法更广泛的动态场景合成，并且拥有更为宽广的空间探索能力&lt;h4&gt;结论&lt;/h4&gt;CameraCtrl II 为大规模动态场景探索提供了一种有效的方法，它克服了以往技术在视频生成中的局限性&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces CameraCtrl II, a framework that enables large-scaledynamic scene exploration through a camera-controlled video diffusion model.Previous camera-conditioned video generative models suffer from diminishedvideo dynamics and limited range of viewpoints when generating videos withlarge camera movement. We take an approach that progressively expands thegeneration of dynamic scenes -- first enhancing dynamic content withinindividual video clip, then extending this capability to create seamlessexplorations across broad viewpoint ranges. Specifically, we construct adataset featuring a large degree of dynamics with camera parameter annotationsfor training while designing a lightweight camera injection module and trainingscheme to preserve dynamics of the pretrained models. Building on theseimproved single-clip techniques, we enable extended scene exploration byallowing users to iteratively specify camera trajectories for generatingcoherent video sequences. Experiments across diverse scenarios demonstrate thatCameraCtrl Ii enables camera-controlled dynamic scene synthesis withsubstantially wider spatial exploration than previous approaches.</description>
      <author>example@mail.com (Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, Hongsheng Li)</author>
      <guid isPermaLink="false">2503.10592v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>SOLA-GCL: Subgraph-Oriented Learnable Augmentation Method for Graph Contrastive Learning</title>
      <link>http://arxiv.org/abs/2503.10100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一个名为SOLA-GCL的新型可学习增强方法，旨在通过强调子图结构来改进图对比学习。&lt;h4&gt;背景&lt;/h4&gt;传统的图对比学习方法通常忽略了子图结构的重要性，特别是它们在生成具有信息性和多样性的对比对中的作用。这些子图特征在网络类型之间差异很大，在社交网络中代表社区，在生物化学网络中则表示分子交互。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于子图的可学习增强方法SOLA-GCL，以利用子图信息进行数据扩充，并优化增强策略以保存和提升子图的独特特性。&lt;h4&gt;方法&lt;/h4&gt;SOLA-GCL首先根据其固有属性将图划分为多个密集连接的子图。然后，通过结合针对每个子图的内子图和间子图增强策略来生成特定视图用于对比学习。这些策略包括节点删除、特征掩码化、边扰动和子图交换。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在社交网络到分子的各种图形学习应用中，SOLA-GCL方法在半监督学习、无监督学习以及迁移学习设置下均表现出优于现有技术的表现。&lt;h4&gt;结论&lt;/h4&gt;通过强调对子图结构的利用，SOLA-GCL为对比学习提供了一种新的视角和更有效的策略，显著改进了图表示的学习效果。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文描述了Graph Contrastive Learning领域的一个新兴研究方向，重点讨论了传统方法忽略的关键点——子图特性的重要性，并介绍了解决此问题的新方法SOLA-GCL。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph contrastive learning has emerged as a powerful technique for learninggraph representations that are robust and discriminative. However, traditionalapproaches often neglect the critical role of subgraph structures, particularlythe intra-subgraph characteristics and inter-subgraph relationships, which arecrucial for generating informative and diverse contrastive pairs. Thesesubgraph features are crucial as they vary significantly across different graphtypes, such as social networks where they represent communities, andbiochemical networks where they symbolize molecular interactions. To addressthis issue, our work proposes a novel subgraph-oriented learnable augmentationmethod for graph contrastive learning, termed SOLA-GCL, that centers aroundsubgraphs, taking full advantage of the subgraph information for dataaugmentation. Specifically, SOLA-GCL initially partitions a graph into multipledensely connected subgraphs based on their intrinsic properties. To preserveand enhance the unique characteristics inherent to subgraphs, a graph viewgenerator optimizes augmentation strategies for each subgraph, therebygenerating tailored views for graph contrastive learning. This generator uses acombination of intra-subgraph and inter-subgraph augmentation strategies,including node dropping, feature masking, intra-edge perturbation, inter-edgeperturbation, and subgraph swapping. Extensive experiments have been conductedon various graph learning applications, ranging from social networks tomolecules, under semi-supervised learning, unsupervised learning, and transferlearning settings to demonstrate the superiority of our proposed approach overthe state-of-the-art in GCL.</description>
      <author>example@mail.com (Tianhao Peng, Xuhong Li, Haitao Yuan, Yuchen Li, Haoyi Xiong)</author>
      <guid isPermaLink="false">2503.10100v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-Supervised Spatial-Temporal Fusion for LiDAR-based 3D Object Detection</title>
      <link>http://arxiv.org/abs/2503.10579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;基于LiDAR的3D目标检测由于LiDAR点固有的稀疏性而面临重大挑战。一种常见的解决方案是使用长期时间序列LiDAR数据来增加输入密度。&lt;h4&gt;目的&lt;/h4&gt;本论文旨在开发一种新的时空融合方法，有效地利用空间-时间信息以解决由物体移动引起的空间错位问题，并充分解锁所提出融合模块的潜力。&lt;h4&gt;方法&lt;/h4&gt;{'ST-Fusion': '包含一个Spatial Aggregation（SA）模块和Temporal Merging（TM）模块。SA模块通过具有逐渐扩大的感受野的卷积层从局部区域聚合对象特征以缓解空间错位，而TM模块利用注意力机制动态提取前一帧的对象特征。', '语义监督': '提出了Semantic Injection方法来丰富稀疏的LiDAR数据，通过注入点级别的语义标签进行训练。这种方法用于训练教师模型，并在特征级别提供重建目标，受到提出的基于对象感知损失函数的监督。', 'Semantic-Supervised Spatial-Temporal Fusion（ST-Fusion）': '该模块引入了一种新的融合方式来缓解由于时间推移引起的空间错位问题，并通过语义级别的监督解锁其潜力。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验表明所提出的ST-Fusion方法在各种LiDAR目标检测器上均表现出有效性和通用性，基于nuScenes基准测试的NDS指标提高了约+2.8%。&lt;h4&gt;结论&lt;/h4&gt;通过创新地解决空间-时间信息融合问题和使用语义监督的方法，可以显著提高基于LiDAR的目标检测性能。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的3D物体检测由于点云固有的稀疏性带来了许多挑战。一种常见的解决方案是利用长时间序列的LiDAR数据来增加输入密度，但如何有效利用空间-时间信息仍然是一个未解决的问题。本文提出了一种新颖的方法：语义监督的空间-时间融合（ST-Fusion），该方法通过新的融合模块缓解了由于物体运动引起的时间错位，并采用特征级语义监督充分释放所提模块的能力。具体来说，ST-Fusion包括空间聚合（SA）和时序合并（TM）两个模块。其中，SA模块使用具有渐进扩增感受野的卷积层从局部区域汇总物体特征以缓解时空错位问题；而TM模块则通过注意力机制动态提取先前帧中的对象特征来实现全面的时间序列表达。此外，在语义监督方面，本文提出了一种名为“语义注入”的方法，它通过向稀疏的LiDAR数据中插入点级别的语义标签来丰富这些数据，用以训练教师模型，并在特征级别提供重建目标由所提出的基于对象感知损失函数进行监督。广泛的实验结果证明了所提方案的有效性和通用性，在nuScenes基准测试下NDS指标上提高了约+2.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based 3D object detection presents significant challenges due to theinherent sparsity of LiDAR points. A common solution involves long-termtemporal LiDAR data to densify the inputs. However, efficiently leveragingspatial-temporal information remains an open problem. In this paper, we proposea novel Semantic-Supervised Spatial-Temporal Fusion (ST-Fusion) method, whichintroduces a novel fusion module to relieve the spatial misalignment caused bythe object motion over time and a feature-level semantic supervision tosufficiently unlock the capacity of the proposed fusion module. Specifically,the ST-Fusion consists of a Spatial Aggregation (SA) module and a TemporalMerging (TM) module. The SA module employs a convolutional layer withprogressively expanding receptive fields to aggregate the object features fromthe local regions to alleviate the spatial misalignment, the TM moduledynamically extracts object features from the preceding frames based on theattention mechanism for a comprehensive sequential presentation. Besides, inthe semantic supervision, we propose a Semantic Injection method to enrich thesparse LiDAR data via injecting the point-wise semantic labels, using it fortraining a teacher model and providing a reconstruction target at the featurelevel supervised by the proposed object-aware loss. Extensive experiments onvarious LiDAR-based detectors demonstrate the effectiveness and universality ofour proposal, yielding an improvement of approximately +2.8% in NDS based onthe nuScenes benchmark.</description>
      <author>example@mail.com (Chaoqun Wang, Xiaobin Hong, Wenzhong Li, Ruimao Zhang)</author>
      <guid isPermaLink="false">2503.10579v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>PRISM: Preference Refinement via Implicit Scene Modeling for 3D Vision-Language Preference-Based Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2503.10177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;我们提出了一种新的框架PRISM，旨在通过统一3D点云建模和未来感知的偏好细化来克服基于2D的偏好转向强化学习(PBRL)的局限性。&lt;h4&gt;背景&lt;/h4&gt;现有的PBRL技术受限于2D视觉数据，这些问题包括遮挡和视角偏差所导致的不稳定偏好信号以及短视反馈问题。&lt;h4&gt;目的&lt;/h4&gt;通过集成3D感知能力和未来导向的推理来解决上述问题，并提升在机器人环境中偏好的一致性、政策收敛速度及泛化能力。&lt;h4&gt;方法&lt;/h4&gt;PRISM采用了3D点云-语言模型（3D-PC-LLM）用于处理遮挡和视角偏差，同时利用链式思维（CoT）推理以考虑长期的决策影响。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与传统PBRL技术相比，PRISM在偏好一致率、政策收敛速度以及未见环境中的泛化能力方面都有显著提升。&lt;h4&gt;结论&lt;/h4&gt;通过将3D几何感知与基于链式思维的偏好建模相结合，PRISM为可扩展的人类导向强化学习建立了一个综合基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种名为PRISM的新框架，它旨在解决现有的偏好转向强化学习（PBRL）技术中的问题，包括遮挡、视角偏差以及短视反馈。通过集成3D点云建模和基于链式思维的推理方法，该研究在机器人操作和自主导航等领域中展示了其显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose PRISM, a novel framework designed to overcome the limitations of2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D pointcloud modeling and future-aware preference refinement. At its core, PRISMadopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion andviewpoint biases, ensuring more stable and spatially consistent preferencesignals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning toincorporate long-horizon considerations, thereby preventing the short-sightedfeedback often seen in static preference comparisons. In contrast toconventional PBRL techniques, this integration of 3D perception andfuture-oriented reasoning leads to significant gains in preference agreementrates, faster policy convergence, and robust generalization across unseenrobotic environments. Our empirical results, spanning tasks such as roboticmanipulation and autonomous navigation, highlight PRISM's potential forreal-world applications where precise spatial understanding and reliablelong-term decision-making are critical. By bridging 3D geometric awareness withCoT-driven preference modeling, PRISM establishes a comprehensive foundationfor scalable, human-aligned reinforcement learning.</description>
      <author>example@mail.com (Yirong Sun, Yanjun Chen)</author>
      <guid isPermaLink="false">2503.10177v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Meta-learning characteristics and dynamics of quantum systems</title>
      <link>http://arxiv.org/abs/2503.10492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6+1 pages, 4 figures. L. Schorling and P. Vaidhyanathan contributed  equally to this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个改进的元学习算法，该算法在预测量子系统的特性和动力学方面比现有的方法更有效。通过实验数据对Loss-DiVincenzo自旋量子比特进行研究，并使用改进的方法来提高性能。&lt;h4&gt;背景&lt;/h4&gt;机器学习技术被广泛应用于量子科技领域，但大多数现有方法只能针对特定的量子系统进行预测或控制。而元学习可以通过利用先前与相似系统的相关数据获得的知识，适应于新系统的预测和控制。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的元学习算法，用于研究闭合和开放两能级系统的动力学和特性，并应用于Heisenberg模型。该方法基于实验数据来预测量子比特特性（如g因子和拉比频率）。&lt;h4&gt;方法&lt;/h4&gt;通过引入自适应学习率以及全局优化器等新技术，改进了现有的物理系统元学习方法的鲁棒性和计算效率。算法在实验中与其它元学习方法、一个vanilla transformer模型及一个多层感知机模型进行了对比。&lt;h4&gt;主要发现&lt;/h4&gt;该元学习方法比其他基准方法（包括vanilla transformer和多层感知机）表现出更优性能，尤其是在数据量较少的情况下能够更好地泛化。&lt;h4&gt;结论&lt;/h4&gt;这种新的元学习算法为量子技术中的机器学习应用提供了有力的支持。它展示了通过改进现有元学习框架可以实现对新量子系统的有效预测，并提高了计算效率与模型的准确性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While machine learning holds great promise for quantum technologies, mostcurrent methods focus on predicting or controlling a specific quantum system.Meta-learning approaches, however, can adapt to new systems for which littledata is available, by leveraging knowledge obtained from previous dataassociated with similar systems. In this paper, we meta-learn dynamics andcharacteristics of closed and open two-level systems, as well as the Heisenbergmodel. Based on experimental data of a Loss-DiVincenzo spin-qubit hosted in aGe/Si core/shell nanowire for different gate voltage configurations, we predictqubit characteristics i.e. $g$-factor and Rabi frequency using meta-learning.The algorithm we introduce improves upon previous state-of-the-artmeta-learning methods for physics-based systems by introducing novel techniquessuch as adaptive learning rates and a global optimizer for improved robustnessand increased computational efficiency. We benchmark our method against othermeta-learning methods, a vanilla transformer, and a multilayer perceptron, anddemonstrate improved performance.</description>
      <author>example@mail.com (Lucas Schorling, Pranav Vaidhyanathan, Jonas Schuff, Miguel J. Carballido, Dominik Zumbühl, Gerard Milburn, Florian Marquardt, Jakob Foerster, Michael A. Osborne, Natalia Ares)</author>
      <guid isPermaLink="false">2503.10492v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Self-Supervised Adversarial Training for Robust Vision Models in Histopathology</title>
      <link>http://arxiv.org/abs/2503.10629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了HSAT，一种基于多层次对比学习的自监督对抗训练框架，旨在增强生物医学和显微镜图像领域中视觉模型的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;对抗攻击对医疗保健等关键领域的视觉模型构成重大挑战。尽管在自然图像中的对抗训练研究较为深入，但在生物医学和显微镜数据中的应用仍有限制。&lt;h4&gt;目的&lt;/h4&gt;为了利用病理图像中患者-切片-补丁关系提供的有价值的区分信号，提出了一种新的自监督对抗训练方法HSAT。&lt;h4&gt;方法&lt;/h4&gt;HSAT通过多层次对比学习生成对抗样本，并将其整合到对抗训练框架中以提高模型的鲁棒性。具体来说，它考虑到了医学图像中的层次结构特点。&lt;h4&gt;主要发现&lt;/h4&gt;在多类病理学数据集OpenSRH上的实验结果表明，与现有的生物医学和自然图像领域的基准方法相比，HSAT在白盒攻击下提升了54.31%的鲁棒性，在黑盒设置下的性能下降仅为3-4%，而基线模型则为25-30%。&lt;h4&gt;结论&lt;/h4&gt;该研究为对抗训练领域设定了新的标准，并为进一步开发更稳健的视觉模型铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;对抗攻击在医疗保健等关键领域对视觉模型构成了重大挑战，这些领域的可靠性至关重要。虽然自然图像中的对抗训练已经得到了广泛的研究，但在生物医学和显微镜数据中仍存在局限性。现有的自监督对抗训练方法忽略了病理学图像的层次结构特性，其中患者-切片-补丁关系提供了有价值的区分信号。为了解决这个问题，我们提出了多层次自监督对抗训练（HSAT），该框架利用这些特点通过多层次对比学习生成对抗样本，并将其集成到对抗训练中以增强鲁棒性。我们在多类病理学数据集OpenSRH上对HSAT进行了评估，结果表明在白盒设置下HSAT的性能平均提升了54.31%，而在黑盒设置下将性能下降幅度减少到了3-4%（相比之下，基线模型为25-30%）。这些成果在该领域内设定了新的标准，并为进一步开发更稳健的视觉模型铺平了道路。我们的训练和评估代码可在https://github.com/HashmatShadab/HSAT上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adversarial attacks pose significant challenges for vision models in criticalfields like healthcare, where reliability is essential. Although adversarialtraining has been well studied in natural images, its application to biomedicaland microscopy data remains limited. Existing self-supervised adversarialtraining methods overlook the hierarchical structure of histopathology images,where patient-slide-patch relationships provide valuable discriminativesignals. To address this, we propose Hierarchical Self-Supervised AdversarialTraining (HSAT), which exploits these properties to craft adversarial examplesusing multi-level contrastive learning and integrate it into adversarialtraining for enhanced robustness. We evaluate HSAT on multiclass histopathologydataset OpenSRH and the results show that HSAT outperforms existing methodsfrom both biomedical and natural image domains. HSAT enhances robustness,achieving an average gain of 54.31% in the white-box setting and reducingperformance drops to 3-4% in the black-box setting, compared to 25-30% for thebaseline. These results set a new benchmark for adversarial training in thisdomain, paving the way for more robust models. Our Code for training andevaluation is available at https://github.com/HashmatShadab/HSAT.</description>
      <author>example@mail.com (Hashmat Shadab Malik, Shahina Kunhimon, Muzammal Naseer, Fahad Shahbaz Khan, Salman Khan)</author>
      <guid isPermaLink="false">2503.10629v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground Simulation</title>
      <link>http://arxiv.org/abs/2503.10410v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了RoCo-Sim，这是一种用于路边协同感知的仿真框架。该框架通过动态前景编辑和单幅图像全场景风格转换生成多视角一致性的虚拟路边数据。&lt;h4&gt;背景&lt;/h4&gt;当前的路边感知方法主要集中在模型设计上，而忽略了诸如校准误差、信息稀疏性和多视图一致性等数据问题，这些问题导致在最近发布的数据集上的性能不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一个框架来显著提高路边协同感知，并解决关键的数据问题。&lt;h4&gt;方法&lt;/h4&gt;{'Camera Extrinsic Optimization': '确保路旁边相机的3D到2D投影准确。', 'Multi-View Occlusion-Aware Sampler (MOAS)': '确定在3D空间中放置不同数字资产的位置，考虑遮挡情况。', 'DepthSAM': '从单帧固定视角图像创新性地建模前景和背景之间的关系，保证多视图下前景的一致性。', 'Scalable Post-Processing Toolkit': '通过风格转移和其他增强技术生成更逼真且丰富化的场景。'}&lt;h4&gt;主要发现&lt;/h4&gt;RoCo-Sim显著提高了路边3D目标检测的性能，在Rcooper-Intersection数据集上以AP70度量优于现有最先进方法83.74%，在TUMTraf-V2X数据集上的提升为83.12%。&lt;h4&gt;结论&lt;/h4&gt;RoCo-Sim填补了路边感知仿真领域的空白，将有助于推动相关技术的发展。代码和预训练模型即将发布于GitHub上：https://github.com/duyuwen-duen/RoCo-Sim&lt;h4&gt;翻译&lt;/h4&gt;论文介绍了一种名为RoCo-Sim的全新框架，用于生成适用于路边协同感知系统所需的虚拟数据集，并通过一系列新颖的方法解决了现有技术在多视角一致性、前景建模和数据质量等方面的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Roadside Collaborative Perception refers to a system where multiple roadsideunits collaborate to pool their perceptual data, assisting vehicles inenhancing their environmental awareness. Existing roadside perception methodsconcentrate on model design but overlook data issues like calibration errors,sparse information, and multi-view consistency, leading to poor performance onrecent published datasets. To significantly enhance roadside collaborativeperception and address critical data issues, we present the first simulationframework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capableof generating diverse, multi-view consistent simulated roadside data throughdynamic foreground editing and full-scene style transfer of a single image.RoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensuresaccurate 3D to 2D projection for roadside cameras; (2) A novel Multi-ViewOcclusion-Aware Sampler (MOAS) determines the placement of diverse digitalassets within 3D space; (3) DepthSAM innovatively models foreground-backgroundrelationships from single-frame fixed-view images, ensuring multi-viewconsistency of foreground; and (4) Scalable Post-Processing Toolkit generatesmore realistic and enriched scenes through style transfer and otherenhancements. RoCo-Sim significantly improves roadside 3D object detection,outperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 onTUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perceptionsimulation. Code and pre-trained models will be released soon:https://github.com/duyuwen-duen/RoCo-Sim</description>
      <author>example@mail.com (Yuwen Du, Anning Hu, Zichen Chao, Yifan Lu, Junhao Ge, Genjia Liu, Weitao Wu, Lanjun Wang, Siheng Chen)</author>
      <guid isPermaLink="false">2503.10410v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Towards Constraint-Based Adaptive Hypergraph Learning for Solving Vehicle Routing: An End-to-End Solution</title>
      <link>http://arxiv.org/abs/2503.10421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种结合了约束导向超图与强化学习的端到端框架被提出，用于解决车辆路径问题。该方法旨在克服传统方法在处理复杂约束时计算成本高且难以实现最优解的问题。&lt;h4&gt;背景&lt;/h4&gt;基于学习的方法应用于车辆路径问题是组合优化领域的重要研究方向。这类问题具有庞大的解决方案空间和复杂的限制条件，使得传统的精确数学模型或启发式算法容易遇到较高的计算开销，并依赖于复杂启发式操作的设计来达到接近最优的解。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的端到端框架以解决车辆路径问题，该框架结合了约束导向超图与强化学习。旨在处理实践中的硬性限制条件，并改善现有方法在优化解质量方面的不足。&lt;h4&gt;方法&lt;/h4&gt;- 引入了一种基于约束导向的动态超边重构策略，在编码器中显著增强了超图表示的学习能力。- 解码器采用了双指针注意力机制，以迭代生成解决方案的方式工作。- 通过结合异步参数更新和双损失函数（包含限制条件损失与政策梯度损失）进行模型训练。&lt;h4&gt;主要发现&lt;/h4&gt;- 提出的方法不需要复杂的启发式操作就能有效地处理硬性约束条件。- 在基准数据集上的实验结果表明，该方法显著提高了解的质量。&lt;h4&gt;结论&lt;/h4&gt;所提出的新框架展示出了在解决具有复杂限制条件的车辆路径问题方面的潜力。它不仅简化了模型设计还提升了解决方案质量，显示出实际应用中的巨大前景。&lt;h4&gt;翻译&lt;/h4&gt;基于学习的方法应用于车辆路径问题是组合优化领域的重要研究方向。这类问题的特点是庞大的解空间和复杂的约束条件，这使得传统的精确数学模型或启发式算法容易遇到较高的计算开销，并依赖于复杂启发式操作的设计来达到接近最优的解。而最近的一些基于学习的方法虽然能在简单的约束情况下产生良好的性能，但在处理实践中常见的硬性约束时却往往表现不佳。这项研究介绍了一种结合了约束导向超图与强化学习的新端到端框架，用于解决车辆路径问题。该方法的主要创新在于开发出一种在编码器中增强超图表示学习能力的基于约束导向的动态超边重构策略；同时解码器利用双指针注意力机制来迭代生成解决方案。所提出的模型通过结合异步参数更新和优化包含限制条件损失与政策梯度损失在内的双重损失函数来进行训练。实验结果表明，该方法不仅消除了对复杂启发式操作的需求，并且在解的质量方面实现了显著的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The application of learning based methods to vehicle routing problems hasemerged as a pivotal area of research in combinatorial optimization. Theseproblems are characterized by vast solution spaces and intricate constraints,making traditional approaches such as exact mathematical models or heuristicmethods prone to high computational overhead or reliant on the design ofcomplex heuristic operators to achieve optimal or near optimal solutions.Meanwhile, although some recent learning-based methods can produce goodperformance for VRP with straightforward constraint scenarios, they often failto effectively handle hard constraints that are common in practice. This studyintroduces a novel end-to-end framework that combines constraint-orientedhypergraphs with reinforcement learning to address vehicle routing problems. Acentral innovation of this work is the development of a constraint-orienteddynamic hyperedge reconstruction strategy within an encoder, whichsignificantly enhances hypergraph representation learning. Additionally, thedecoder leverages a double-pointer attention mechanism to iteratively generatesolutions. The proposed model is trained by incorporating asynchronousparameter updates informed by hypergraph constraints and optimizing a dual lossfunction comprising constraint loss and policy gradient loss. The experimentresults on benchmark datasets demonstrate that the proposed approach not onlyeliminates the need for sophisticated heuristic operators but also achievessubstantial improvements in solution quality.</description>
      <author>example@mail.com (Zhenwei Wang, Ruibin Bai, Tiehua Zhang)</author>
      <guid isPermaLink="false">2503.10421v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Fine-tuning Vision Language Models with Graph-based Knowledge for Explainable Medical Image Analysis</title>
      <link>http://arxiv.org/abs/2503.09808v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种将图表示学习与视觉语言模型相结合的方法，用于解释性糖尿病视网膜病变（DR）诊断。&lt;h4&gt;背景&lt;/h4&gt;准确的DR分期对于及时干预和防止视力丧失至关重要。当前的分期模型难以理解，并且大多数公共数据集仅包含图像级别的标签而没有临床推理或解读。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提供可解释DR诊断的方法，该方法结合了图表示学习与视觉语言模型（VLM）以生成基于生理结构和特征的人类可理解的解释。&lt;h4&gt;方法&lt;/h4&gt;利用光学相干断层扫描血管成像（OCTA）图像构建包含关键视网膜血管特征的生物信息图。通过引入图神经网络（GNN）进行DR分期，同时使用集成梯度高亮显示决定分类决策的关键节点和边及其个体特征。&lt;h4&gt;主要发现&lt;/h4&gt;该方法不仅提高了分类准确性，还提供了更具有临床意义的解释，并为OCTA图像中的病理精确定位铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;通过实验评估证明了该方法的有效性，展示了它在提高DR分期准确性的同时增强了其可解释性和临床应用价值。&lt;h4&gt;翻译&lt;/h4&gt;准确地分期糖尿病视网膜病变（DR）对于及时干预和防止视力丧失至关重要。然而，目前的分期模型难以理解，并且大多数公共数据集仅包含图像级别的标签而没有进一步的临床推理或解读信息。在本文中，我们提出了一种新的方法，该方法结合了图表示学习与视觉语言模型（VLM），以提供可解释性的DR诊断。我们的方法利用光学相干断层扫描血管成像（OCTA）图像，并通过构建包含关键视网膜血管特征的生物信息图来进行处理，这些特征包括血管形态和空间连接性等。接着，使用图神经网络（GNN）进行DR分期的同时，集成梯度突出显示了驱动分类决策的关键节点、边及其个体特征。我们将基于该图的知识转化为文本描述供VLM使用，并通过图像与这些文本描述的指令调整训练学生VLM模型。最终代理能够仅根据单个图像输入对疾病进行分类并解释其决定，在人类可理解的方式中实现这一点。在专有和公共数据集上的实验评估显示，我们的方法不仅提高了分类准确性，还提供了更具有临床意义的结果。专家研究进一步证明了该方法提供更准确的诊断解释，并为OCTA图像中的病理精确定位铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate staging of Diabetic Retinopathy (DR) is essential for guiding timelyinterventions and preventing vision loss. However, current staging models arehardly interpretable, and most public datasets contain no clinical reasoning orinterpretation beyond image-level labels. In this paper, we present a novelmethod that integrates graph representation learning with vision-languagemodels (VLMs) to deliver explainable DR diagnosis. Our approach leveragesoptical coherence tomography angiography (OCTA) images by constructingbiologically informed graphs that encode key retinal vascular features such asvessel morphology and spatial connectivity. A graph neural network (GNN) thenperforms DR staging while integrated gradients highlight critical nodes andedges and their individual features that drive the classification decisions. Wecollect this graph-based knowledge which attributes the model's prediction tophysiological structures and their characteristics. We then transform it intotextual descriptions for VLMs. We perform instruction-tuning with these textualdescriptions and the corresponding image to train a student VLM. This finalagent can classify the disease and explain its decision in a humaninterpretable way solely based on a single image input. Experimentalevaluations on both proprietary and public datasets demonstrate that our methodnot only improves classification accuracy but also offers more clinicallyinterpretable results. An expert study further demonstrates that our methodprovides more accurate diagnostic explanations and paves the way for preciselocalization of pathologies in OCTA images.</description>
      <author>example@mail.com (Chenjun Li, Laurin Lux, Alexander H. Berger, Martin J. Menten, Mert R. Sabuncu, Johannes C. Paetzold)</author>
      <guid isPermaLink="false">2503.09808v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning-Based Direct Leaf Area Estimation using Two RGBD Datasets for Model Development</title>
      <link>http://arxiv.org/abs/2503.10129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;此论文研究了利用RGBD图像估计单片叶子面积的深度学习方法。&lt;h4&gt;背景&lt;/h4&gt;作物生长状况和新品种选育可以通过测量叶片面积来评估，同时也能用于估算叶面积指数以及总叶面积。已有研究采用了手持相机、三维重建及无监督学习等方法进行叶片面积估计。&lt;h4&gt;目的&lt;/h4&gt;探索利用RGBD图像（通过移动设备拍摄）在实际场景中基于深度学习的叶片面积估计技术。&lt;h4&gt;方法&lt;/h4&gt;['收集了两组数据集：一组是附着于植物上用顶部视角捕捉的叶片，另一组是从植株上摘下的单片叶子，并用于模型开发与测试；', '首先尝试通过图像处理方法对手动分割后的叶片进行面积估计；', '然后采用Mask R-CNN模型并对其进行修改以接受RGBD图像输入及直接输出叶面积；', '将分离的叶片数据集与附着在植物上的叶子数据集混合，用于估算植株图像中的单片叶子面积，并提出了一个基于两个骨干网络的设计：一个进行分割任务，另一个则负责估计叶子面积。']&lt;h4&gt;主要发现&lt;/h4&gt;['对于未见过的分离叶片数据集，分割结果的F1分数为1.0（以90%交并比为标准），而叶面积估计的R平方得分为0.81；', '针对未知植物图像的数据集，分割任务中以90%交并比计算的F1评分为0.59，而R平方评分则为0.57。']&lt;h4&gt;结论&lt;/h4&gt;研究结果表明使用附着叶片与真实面积作为标签可以提升模型性能。&lt;h4&gt;翻译&lt;/h4&gt;估计单片叶子的面积是衡量作物生长状况及选育新品种的一个指标；它同样可用于估算叶面积指数和总叶面积。已有研究表明，手持相机、图像处理三维重建以及无监督学习方法在植物图像中用于叶片面积的估算效果良好；然而基于深度学习的方法直接对物体进行面积估计的研究尚未有人探索。该研究利用RGBD图像（通过移动设备拍摄）探讨了在真实场景下基于深度学习技术实现叶子面积估计的可能性，实验结果表明这种方法具有可行性，并能够提高模型的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimation of a single leaf area can be a measure of crop growth and aphenotypic trait to breed new varieties. It has also been used to measure leafarea index and total leaf area. Some studies have used hand-held cameras, imageprocessing 3D reconstruction and unsupervised learning-based methods toestimate the leaf area in plant images. Deep learning works well for objectdetection and segmentation tasks; however, direct area estimation of objectshas not been explored. This work investigates deep learning-based leaf areaestimation, for RGBD images taken using a mobile camera setup in real-worldscenarios. A dataset for attached leaves captured with a top angle view and adataset for detached single leaves were collected for model development andtesting. First, image processing-based area estimation was tested on manuallysegmented leaves. Then a Mask R-CNN-based model was investigated, and modifiedto accept RGBD images and to estimate the leaf area. The detached-leaf data setwas then mixed with the attached-leaf plant data set to estimate the singleleaf area for plant images, and another network design with two backbones wasproposed: one for segmentation and the other for area estimation. Instead oftrying all possibilities or random values, an agile approach was used inhyperparameter tuning. The final model was cross-validated with 5-folds andtested with two unseen datasets: detached and attached leaves. The F1 scorewith 90% IoA for segmentation result on unseen detached-leaf data was 1.0,while R-squared of area estimation was 0.81. For unseen plant datasegmentation, the F1 score with 90% IoA was 0.59, while the R-squared score was0.57. The research suggests using attached leaves with ground truth area toimprove the results.</description>
      <author>example@mail.com (Namal Jayasuriya, Yi Guo, Wen Hu, Oula Ghannoum)</author>
      <guid isPermaLink="false">2503.10129v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models for Atomistic Simulation of Chemistry and Materials</title>
      <link>http://arxiv.org/abs/2503.10538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了基于数据和参数缩放规律及预训练策略的大型语言和视觉模型在化学与材料科学中的学习模拟的可能性。&lt;h4&gt;背景&lt;/h4&gt;当前大型语言模型和大型视觉模型展示了强大的能力，引发了对基础模型是否可以通过数据和参数缩放定律以及预训练策略来构建以进行化学和材料科学的学习模拟的研究兴趣。&lt;h4&gt;目的&lt;/h4&gt;本文旨在涵盖机器学习原子间势能（MLIP）这一快速发展的领域，并展示创建更大规模的化学与材料科学MLIP基础模型的道路。&lt;h4&gt;方法&lt;/h4&gt;通过扩大数据集范围并使用具有高度表达力的架构来探索构建更高效、更广泛适用的基础模型的可能性，该模型相比从头开始针对特定应用训练更有优势。&lt;h4&gt;主要发现&lt;/h4&gt;建立大规模且多样化的数据集和高表现力的架构可以导致化学与材料科学中的基础模型在效率、泛化能力和抗分布外挑战方面更具优势，并易于微调以适应各种下游观测值。&lt;h4&gt;结论&lt;/h4&gt;探索通过构建基于MLIP的大规模基础模型，可以为未来的化学和材料科学研究提供一种新的方法论路径。&lt;h4&gt;翻译&lt;/h4&gt;鉴于大型语言和视觉模型的力量，探讨是否可以根据数据和参数缩放定律以及预训练策略来建立用于学习化学与材料科学模拟的基础模型具有深远的基本意义。大规模且多样化的数据集和高度表现力的架构在化学与材料科学中的应用，应当比从头开始针对特定应用进行原子尺度模拟训练时更为高效、广泛适用，并具备更强的抗分布外挑战能力以及更易微调以适应各种下游观测值。本文旨在覆盖快速发展的机器学习原子间势能（MLIP）领域，并展示创建更大规模化学与材料科学MLIP基础模型的道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given the power of large language and large vision models, it is of profoundand fundamental interest to ask if a foundational model based on data andparameter scaling laws and pre-training strategies is possible for learnedsimulations of chemistry and materials. The scaling of large and diversedatasets and highly expressive architectures for chemical and materialssciences should result in a foundation model that is more efficient and broadlytransferable, robust to out-of-distribution challenges, and easily fine-tunedto a variety of downstream observables, when compared to specific training fromscratch on targeted applications in atomistic simulation. In this Perspectivewe aim to cover the rapidly advancing field of machine learned interatomicpotentials (MLIP), and to illustrate a path to create chemistry and materialsMLIP foundation models at larger scale.</description>
      <author>example@mail.com (Eric C. -Y. Yuan, Yunsheng Liu, Junmin Chen, Peichen Zhong, Sanjeev Raja, Tobias Kreiman, Santiago Vargas, Wenbin Xu, Martin Head-Gordon, Chao Yang, Samuel M. Blau, Aditi Krishnapriyan, Teresa Head-Gordon)</author>
      <guid isPermaLink="false">2503.10538v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Unlocking Generalization Power in LiDAR Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2503.10149v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为UGP的框架，用于提高LiDAR点云注册技术在不同距离和数据集上的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;当前方法在实现跨距离、跨数据集的鲁棒性一般化方面存在不足，特别是在自主驾驶等应用中确保安全性至关重要。&lt;h4&gt;目的&lt;/h4&gt;设计一种新的框架以增强LiDAR点云注册的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;UGP通过消除交叉注意力机制来提高模型在大规模场景中的表现，并引入了渐进式自注意模块和Bird's Eye View (BEV)特征集成，用于减少大尺度场景中的模糊性并整合语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的泛化实验验证了UGP的有效性，在KITTI和nuScenes数据集上的跨距离泛化试验中分别实现了94.5%和91.4%的平均注册召回率，而在从nuScenes到KITTI的数据集迁移测试中达到了90.9%的最优平均注册召回率。&lt;h4&gt;结论&lt;/h4&gt;UGP框架通过改进注意力机制和整合BEV特征显著提高了LiDAR点云注册技术在不同场景条件下的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容：在实际环境中，具有稳健泛化能力（针对变化的距离和数据集）的LiDAR点云配准方法对于确保自主驾驶和其他基于LiDAR的应用的安全性至关重要。然而，目前的方法在这方面存在不足。为了解决这些局限性，我们提出了UGP，这是一种精简框架设计用于增强LiDAR点云注册技术的泛化能力。在UGP中的核心见解是通过消除交叉注意力机制来改进泛化，使网络专注于帧内特征提取。此外，引入了渐进式自注意模块以减少大规模场景中的模糊性，并集成鸟瞰图（BEV）特征以纳入关于场景元素的语义信息。这些增强措施显著提升了网络在不同环境下的性能表现。我们通过多个室外场景中的一系列泛化实验验证了这种方法的有效性，在KITTI和nuScenes数据集上的跨距离泛化试验中分别达到了最优的平均注册召回率94.5%和91.4%，而在从nuScenes到KITTI的数据集迁移测试中则为90.9%。代码可在https://github.com/peakpang/UGP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world environments, a LiDAR point cloud registration method withrobust generalization capabilities (across varying distances and datasets) iscrucial for ensuring safety in autonomous driving and other LiDAR-basedapplications. However, current methods fall short in achieving this level ofgeneralization. To address these limitations, we propose UGP, a prunedframework designed to enhance generalization power for LiDAR point cloudregistration. The core insight in UGP is the elimination of cross-attentionmechanisms to improve generalization, allowing the network to concentrate onintra-frame feature extraction. Additionally, we introduce a progressiveself-attention module to reduce ambiguity in large-scale scenes and integrateBird's Eye View (BEV) features to incorporate semantic information about sceneelements. Together, these enhancements significantly boost the network'sgeneralization performance. We validated our approach through variousgeneralization experiments in multiple outdoor scenes. In cross-distancegeneralization experiments on KITTI and nuScenes, UGP achieved state-of-the-artmean Registration Recall rates of 94.5% and 91.4%, respectively. Incross-dataset generalization from nuScenes to KITTI, UGP achieved astate-of-the-art mean Registration Recall of 90.9%. Code will be available athttps://github.com/peakpang/UGP.</description>
      <author>example@mail.com (Zhenxuan Zeng, Qiao Wu, Xiyu Zhang, Lin Yuanbo Wu, Pei An, Jiaqi Yang, Ji Wang, Peng Wang)</author>
      <guid isPermaLink="false">2503.10149v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Interactive Multimodal Fusion with Temporal Modeling</title>
      <link>http://arxiv.org/abs/2503.10523v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于8th ABAW竞赛中估计情感维度值（valence-arousal，VA）的方法。该方法结合了视觉和音频信息，并通过一个多模态框架进行处理。&lt;h4&gt;背景&lt;/h4&gt;在8th ABAW竞赛的背景下，需要开发能够准确评估人类情感状态的技术，尤其是在真实世界环境中。&lt;h4&gt;目的&lt;/h4&gt;为了提高在真实环境中的情感估计精度，本文旨在提出一种基于多模态数据（视觉和音频）的情感分析方法。&lt;h4&gt;方法&lt;/h4&gt;该研究使用预训练ResNet模型从面部图像中提取空间特征，并利用VGG模型的预训练版本来从语音信号中抽取VGGish和LogMel特性。通过时间卷积网络（TCNs）对这些特性进行时序建模，接着采用跨模式注意机制使视觉特性和音频特性相互作用。最终，所有处理过的特性被合并并通过回归层预测valence和arousal。&lt;h4&gt;主要发现&lt;/h4&gt;提出的多模态融合方法在Aff-Wild2数据集上展现了其有效性，并达到了与现有技术水平相当的表现。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了通过结合视觉和音频信息进行情感分析的有效性，为未来的研究提供了一个强大的基础。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了在8th ABAW竞赛中对valence-arousal (VA)估计的方法。我们的方法通过一个多模态框架整合视觉和听觉信息。视觉分支使用预训练的ResNet模型从面部图像中提取空间特征，而音频分支利用预训练VGG模型来抽取语音信号中的VGGish和LogMel特性。这些特性经过时间卷积网络(TCNs)进行时序建模后，应用跨模式注意机制，使视觉特性和音频特性通过查询-键值结构相互作用。最后，将所有特征合并并通过回归层预测valence和arousal。我们的方法在Aff-Wild2数据集上实现了竞争性的性能，证明了用于真实环境中的VA估计的多模态融合的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents our method for the estimation of valence-arousal (VA) inthe 8th Affective Behavior Analysis in-the-Wild (ABAW) competition. Ourapproach integrates visual and audio information through a multimodalframework. The visual branch uses a pre-trained ResNet model to extract spatialfeatures from facial images. The audio branches employ pre-trained VGG modelsto extract VGGish and LogMel features from speech signals. These featuresundergo temporal modeling using Temporal Convolutional Networks (TCNs). We thenapply cross-modal attention mechanisms, where visual features interact withaudio features through query-key-value attention structures. Finally, thefeatures are concatenated and passed through a regression layer to predictvalence and arousal. Our method achieves competitive performance on theAff-Wild2 dataset, demonstrating effective multimodal fusion for VA estimationin-the-wild.</description>
      <author>example@mail.com (Jun Yu, Yongqi Wang, Lei Wang, Yang Zheng, Shengfan Xu)</author>
      <guid isPermaLink="false">2503.10523v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>GS-SDF: LiDAR-Augmented Gaussian Splatting and Neural SDF for Geometrically Consistent Rendering and Reconstruction</title>
      <link>http://arxiv.org/abs/2503.10170v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将激光雷达和视觉数据结合的统一系统，利用神经符号距离场初始化高斯点，并进行几何一致性正则化。&lt;h4&gt;背景&lt;/h4&gt;数字孪生是自主驾驶和嵌入式人工智能发展的基础。然而，实现高质量表面重建和真实感渲染仍然面临挑战，特别是由于碎片化的原语和稀疏观测数据导致的几何不一致问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效结合激光雷达稀疏点云与高斯光栅技术的方法，以解决复杂环境中的几何一致性难题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于神经符号距离场（SDF）的统一系统。该系统利用精确的激光雷达点云为训练好的神经SDF提供一个流形几何场，并通过基于SDF的初始化来实现物理基础原始放置和全面的几何正则化。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与现有方法相比，所提出的GS-SDF方法在不同轨迹下的重建准确性和渲染质量方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;该工作为自主驾驶中的数字孪生构建提供了一种有效的解决方案，并将在开源代码库中发布以供研究社区使用。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了数字双胞胎对于自动驾驶和嵌入式人工智能的重要性以及表面重构和真实感渲染面临的挑战。提出了结合激光雷达与视觉数据的系统，利用神经SDF初始化高斯点并进行几何一致性正则化。实验表明该方法在复杂环境中表现优异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Digital twins are fundamental to the development of autonomous driving andembodied artificial intelligence. However, achieving high-granularity surfacereconstruction and high-fidelity rendering remains a challenge. Gaussiansplatting offers efficient photorealistic rendering but struggles withgeometric inconsistencies due to fragmented primitives and sparse observationaldata in robotics applications. Existing regularization methods, which rely onrender-derived constraints, often fail in complex environments. Moreover,effectively integrating sparse LiDAR data with Gaussian splatting remainschallenging. We propose a unified LiDAR-visual system that synergizes Gaussiansplatting with a neural signed distance field. The accurate LiDAR point cloudsenable a trained neural signed distance field to offer a manifold geometryfield, This motivates us to offer an SDF-based Gaussian initialization forphysically grounded primitive placement and a comprehensive geometricregularization for geometrically consistent rendering and reconstruction.Experiments demonstrate superior reconstruction accuracy and rendering qualityacross diverse trajectories. To benefit the community, the codes will bereleased at https://github.com/hku-mars/GS-SDF.</description>
      <author>example@mail.com (Jianheng Liu, Yunfei Wan, Bowen Wang, Chunran Zheng, Jiarong Lin, Fu Zhang)</author>
      <guid isPermaLink="false">2503.10170v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation</title>
      <link>http://arxiv.org/abs/2503.09878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了CleverDistiller，一种用于2D到3D知识蒸馏的自监督、跨模态框架。该框架通过直接特征相似性损失和多层感知器投影头的设计选择来实现从视觉基础模型向3D LiDAR模型的知识转移。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的基础模型（如DINO）在二维相机感知中取得了范式转变，通过提取泛化的特征支持多种下游任务。最近的工作引入了自我监督的跨模态知识蒸馏作为将这些强大的泛化能力转移到三维激光雷达模型中的方法。&lt;h4&gt;目的&lt;/h4&gt;提出了一种简单而有效的方法来从视觉基础模型向3D LiDAR模型进行知识转移，旨在不依赖伪语义图的情况下直接传输知识，并引入辅助自监督的空间任务以增强语义知识的获取。&lt;h4&gt;方法&lt;/h4&gt;CleverDistiller采用简单的特征相似性损失结合多层感知器投影头的设计选择，允许三维网络通过投影学习复杂的语义关系。此外，该框架还引入了占用预测这一辅助自我监督空间任务来增强从视觉基础模型中获得的语义知识与3D空间推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明CleverDistiller在标准自动驾驶基准测试中的2D到3D的知识蒸馏方面表现出色，在语义分割和三维目标检测（3DOD）方面均达到了最先进的性能，特别是在数据量非常低的情况下进行微调时表现尤为突出。&lt;h4&gt;结论&lt;/h4&gt;CleverDistiller通过直接特征相似性损失和多层感知器投影头的设计选择实现了从视觉基础模型向3D LiDAR模型的知识高效转移。这为未来的工作提供了简单而强大的知识蒸馏策略。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了提出了一种名为CleverDistiller的框架，用于将2D相机中提取的泛化特征转移到3D LiDAR系统中的方法，并且这种方法在多个自动驾驶基准测试上表现出卓越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models (VFMs) such as DINO have led to a paradigm shift in2D camera-based perception towards extracting generalized features to supportmany downstream tasks. Recent works introduce self-supervised cross-modalknowledge distillation (KD) as a way to transfer these powerful generalizationcapabilities into 3D LiDAR-based models. However, they either rely on highlycomplex distillation losses, pseudo-semantic maps, or limit KD to featuresuseful for semantic segmentation only. In this work, we proposeCleverDistiller, a self-supervised, cross-modal 2D-to-3D KD frameworkintroducing a set of simple yet effective design choices: Unlike contrastiveapproaches relying on complex loss design choices, our method employs a directfeature similarity loss in combination with a multi layer perceptron (MLP)projection head to allow the 3D network to learn complex semantic dependenciesthroughout the projection. Crucially, our approach does not depend onpseudo-semantic maps, allowing for direct knowledge transfer from a VFM withoutexplicit semantic supervision. Additionally, we introduce the auxiliaryself-supervised spatial task of occupancy prediction to enhance the semanticknowledge, obtained from a VFM through KD, with 3D spatial reasoningcapabilities. Experiments on standard autonomous driving benchmarks for2D-to-3D KD demonstrate that CleverDistiller achieves state-of-the-artperformance in both semantic segmentation and 3D object detection (3DOD) by upto 10% mIoU, especially when fine tuning on really low data amounts, showingthe effectiveness of our simple yet powerful KD strategy</description>
      <author>example@mail.com (Hariprasath Govindarajan, Maciej K. Wozniak, Marvin Klingner, Camille Maurice, B Ravi Kiran, Senthil Yogamani)</author>
      <guid isPermaLink="false">2503.09878v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Dual-Stage Cross-Modal Network with Dynamic Feature Fusion for Emotional Mimicry Intensity Estimation</title>
      <link>http://arxiv.org/abs/2503.10603v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;情感模仿强度（EMI）估计技术是理解人类社会行为和增强人机交互体验的关键，其核心挑战在于动态相关性建模及多模式时间信号的鲁棒融合。&lt;h4&gt;背景&lt;/h4&gt;现有方法在利用模态协同效应、抗噪能力和精细化对齐能力方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一种双阶段跨模态对齐框架以解决上述问题，该框架旨在提升情感模仿强度估计的精度和稳定性。&lt;h4&gt;方法&lt;/h4&gt;{'第一阶段': '构建基于改进CLIP架构的视觉-文本及音频-文本对比学习网络，实现初步在特征空间内的模态解耦预训练。', '第二阶段': '设计时间感知动态融合模块结合时序卷积网络（TCN）和门控双向LSTM捕捉面部表情的宏观演变模式以及声学特征的局部动力学。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'质量引导式模态融合策略': '创新性地引入一种基于质量引导式的模态融合策略，通过可微权重分配实现遮挡及噪声场景下的模态补偿。', '实验结果': '在Hume-Vidmimic2数据集上的平均皮尔逊相关系数为0.35，优于最佳基线40%。', '消融研究': '进一步验证了双阶段训练策略和动态融合机制的有效性'}&lt;h4&gt;结论&lt;/h4&gt;提出的方法提供了开放环境中精细化情感分析的新技术路径，并显著提升了EMI估计的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：情感模仿强度（EMI）估计是理解人类社会行为和增强人机交互体验的关键，其核心挑战在于动态相关性建模及多模式时间信号的鲁棒融合。现有方法在利用模态协同效应、抗噪能力和精细化对齐能力方面存在局限性，本文提出了一种双阶段跨模态对齐框架以解决这些问题，通过改进CLIP架构构建了视觉-文本和音频-文本对比学习网络，并设计了一个结合时序卷积网络（TCN）和门控双向LSTM的时间感知动态融合模块。创新性的质量引导式模态融合策略使得模型在遮挡及噪声场景下具备较强的鲁棒性，实验结果表明该方法在Hume-Vidmimic2数据集上实现了显著的性能提升，并优于最佳基线40%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Emotional Mimicry Intensity (EMI) estimation serves as a critical technologyfor understanding human social behavior and enhancing human-computerinteraction experiences, where the core challenge lies in dynamic correlationmodeling and robust fusion of multimodal temporal signals. To address thelimitations of existing methods in insufficient exploitation of modalsynergistic effects, noise sensitivity, and limited fine-grained alignmentcapabilities, this paper proposes a dual-stage cross-modal alignment framework.First, we construct vision-text and audio-text contrastive learning networksbased on an improved CLIP architecture, achieving preliminary alignment in thefeature space through modality-decoupled pre-training. Subsequently, we designa temporal-aware dynamic fusion module that combines Temporal ConvolutionalNetworks (TCN) and gated bidirectional LSTM to respectively capture themacro-evolution patterns of facial expressions and local dynamics of acousticfeatures. Innovatively, we introduce a quality-guided modality fusion strategythat enables modality compensation under occlusion and noisy scenarios throughdifferentiable weight allocation. Experimental results on the Hume-Vidmimic2dataset demonstrate that our method achieves an average Pearson correlationcoefficient of 0.35 across six emotion dimensions, outperforming the bestbaseline by 40\%. Ablation studies further validate the effectiveness of thedual-stage training strategy and dynamic fusion mechanism, providing a noveltechnical pathway for fine-grained emotion analysis in open environments.</description>
      <author>example@mail.com (Jun Yu, Lingsi Zhu, Yanjun Chi, Yunxiang Zhang, Yang Zheng, Yongqi Wang, Xilong Lu)</author>
      <guid isPermaLink="false">2503.10603v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Logical Capabilities of Large Language Models via Out-of-Context Representation Learning</title>
      <link>http://arxiv.org/abs/2503.10408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究了大型语言模型在二元关系上的能力，探讨了平等、不平等和包含等概念及其属性，并提出了与上下文学习不同的方法——超上下文表示学习。&lt;h4&gt;背景&lt;/h4&gt;论文关注数学推理中普遍存在的二元关系，包括平等性、非平等性和包含性，以及这些关系所具有的性质如自反性、对称性及传递性。同时研究了逻辑复杂度（例如，推理的‘跳跃’次数）等指标。&lt;h4&gt;目的&lt;/h4&gt;旨在通过超上下文表示学习这一新方法来评估大型语言模型在逻辑任务上的能力，并认为这种方法优于传统的上下文学习和微调技术。&lt;h4&gt;方法&lt;/h4&gt;提出了一种不同于上下文学习的方法——超上下文表示学习，该方法仅训练新引入词汇的表示形式，避免了对现有模型中的语言偏见以及外部信息或插图的依赖。&lt;h4&gt;主要发现&lt;/h4&gt;论文展示了这种方法可以更有效地评估大型语言模型在基础逻辑任务上的能力。&lt;h4&gt;结论&lt;/h4&gt;通过研究二元关系和相关属性，作者认为超上下文表示学习为评价大型语言模型的能力提供了一个新的视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the capabilities of Large Language Models (LLM) on binary relations,a ubiquitous concept in math employed in most reasoning, math and logicbenchmarks. This work focuses on equality, inequality, and inclusion, alongwith the properties they satisfy, such as ir/reflexivity, a/symmetry,transitivity, and logical complexity (e.g., number of reasoning ``hops''). Wepropose an alternative to in-context learning that trains only therepresentations of newly introduced tokens, namely out-of-contextrepresentation learning. This method mitigates linguistic biases alreadypresent in a model and, differently from in-context learning, does not rely onexternal information or illustrations. We argue out-of-context representationlearning as a better alternative to in-context learning and fine-tuning toevaluate the capabilities of LLMs on logic tasks that are the building blocksof more complex reasoning benchmarks.</description>
      <author>example@mail.com (Jonathan Shaki, Emanuele La Malfa, Michael Wooldridge, Sarit Kraus)</author>
      <guid isPermaLink="false">2503.10408v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization</title>
      <link>http://arxiv.org/abs/2503.10354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted Paper in the 8th International Research Conference on Smart  Computing and Systems Engineering, University of Kelaniya, Sri Lanka.  (Pending Publication)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;随着创新的大量增长，自动专利摘要方法在帮助专利分析和理解过程中需求量大。&lt;h4&gt;目的&lt;/h4&gt;提出一种系统，该系统将提取式与抽象式文本摘要技术结合到一个混合框架中，以便高效地创建专利记录的抽象摘要。&lt;h4&gt;方法&lt;/h4&gt;使用基于图的LexRank算法检索输入父文档中的重要句子，并利用通过低秩适应（LoRA）微调后的双向自动回归变压器模型生成文本摘要。此外，作者采用某些元学习技术实现多个专利领域的领域泛化。&lt;h4&gt;主要发现&lt;/h4&gt;结合提取式与抽象式的框架在处理复杂且冗长的专利文档时显示了高效率和效果。&lt;h4&gt;结论&lt;/h4&gt;该研究成功地提出了一个可以处理多种专利领域的高效文本摘要系统。&lt;h4&gt;翻译&lt;/h4&gt;现有的自然语言处理、文本挖掘以及深度学习的发展显著提高了各种类型文件上的文本摘要模型的有效性。然而，由于这些专利文档包括技术性和法律性的复杂性，并且比典型文件长得多，因此总结专利文本仍然是一个重要的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic patent summarization approaches that help in the patent analysisand comprehension procedure are in high demand due to the colossal growth ofinnovations. The development of natural language processing (NLP), text mining,and deep learning has notably amplified the efficacy of text summarizationmodels for abundant types of documents. Summarizing patent text remains apertinent challenge due to the labyrinthine writing style of these documents,which includes technical and legal intricacies. Additionally, these patentdocument contents are considerably lengthier than archetypal documents, whichintricates the process of extracting pertinent information for summarization.Embodying extractive and abstractive text summarization methodologies into ahybrid framework, this study proposes a system for efficiently creatingabstractive summaries of patent records. The procedure involves leveraging theLexRank graph-based algorithm to retrieve the important sentences from inputparent texts, then utilizing a Bidirectional Auto-Regressive Transformer (BART)model that has been fine-tuned using Low-Ranking Adaptation (LoRA) forproducing text summaries. This is accompanied by methodical testing andevaluation strategies. Furthermore, the author employed certain meta-learningtechniques to achieve Domain Generalization (DG) of the abstractive componentacross multiple patent fields.</description>
      <author>example@mail.com (Nevidu Jayatilleke, Ruvan Weerasinghe)</author>
      <guid isPermaLink="false">2503.10354v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>How Feasible is Augmenting Fake Nodes with Learnable Features as a Counter-strategy against Link Stealing Attacks?</title>
      <link>http://arxiv.org/abs/2503.09726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint for the Accepted Work in The 15th ACM Conference on Data and  Application Security and Privacy (CODASPY'25)}, 14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个称为$NARGIS$的方法，用于防御针对图神经网络（GNN）的“链路窃取”攻击。该方法通过重塑图嵌入空间来保护隐私。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在基于图的任务中广泛使用和部署，但也带来了隐私泄露的风险。例如，“链路窃取”攻击可能暴露用户之间潜在敏感的信息连接关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种防御机制$NARGIS$以防止“链路窃取”攻击，并保持GNN模型对预测任务的有用性。&lt;h4&gt;方法&lt;/h4&gt;$NARGIS$通过在给定图上应用谱聚类来增加新的节点，这些新节点具有学习到的功能特征而非固定特征。它采用三层次优化法来学习用于GNN模型、代理攻击者模型以及防御模型（即可学习的节点特征）的参数。&lt;h4&gt;主要发现&lt;/h4&gt;$NARGIS$在多种情况下表现出优异的模型保真度和隐私保护性能之间的权衡，但也有需要改进的情况。提出了集成不同方案以使模型更加强大的方法。&lt;h4&gt;结论&lt;/h4&gt;通过广泛评估证明了$NARGIS$的有效性，并揭示了它在处理“链路窃取”攻击中的优势与不足。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了GNN的隐私泄露风险，提出了一种防御机制$NARGIS$来保护数据隐私，同时保持模型预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are widely used and deployed for graph-basedprediction tasks. However, as good as GNNs are for learning graph data, theyalso come with the risk of privacy leakage. For instance, an attacker can runcarefully crafted queries on the GNNs and, from the responses, can infer theexistence of an edge between a pair of nodes. This attack, dubbed as a"link-stealing" attack, can jeopardize the user's privacy by leakingpotentially sensitive information. To protect against this attack, we proposean approach called "$(N)$ode $(A)$ugmentation for $(R)$estricting $(G)$raphsfrom $(I)$nsinuating their $(S)$tructure" ($NARGIS$) and study its feasibility.$NARGIS$ is focused on reshaping the graph embedding space so that theposterior from the GNN model will still provide utility for the prediction taskbut will introduce ambiguity for the link-stealing attackers. To this end,$NARGIS$ applies spectral clustering on the given graph to facilitate it beingaugmented with new nodes -- that have learned features instead of fixed ones.It utilizes tri-level optimization for learning parameters for the GNN model,surrogate attacker model, and our defense model (i.e. learnable node features).We extensively evaluate $NARGIS$ on three benchmark citation datasets overeight knowledge availability settings for the attackers. We also evaluate themodel fidelity and defense performance on influence-based link inferenceattacks. Through our studies, we have figured out the best feature of $NARGIS$-- its superior fidelity-privacy performance trade-off in a significant numberof cases. We also have discovered in which cases the model needs to beimproved, and proposed ways to integrate different schemes to make the modelmore robust against link stealing attacks.</description>
      <author>example@mail.com (Mir Imtiaz Mostafiz, Imtiaz Karim, Elisa Bertino)</author>
      <guid isPermaLink="false">2503.09726v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Enhance Exploration in Safe Reinforcement Learning with Contrastive Representation Learning</title>
      <link>http://arxiv.org/abs/2503.10318v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ACIIDS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;在安全强化学习中，代理需要平衡探索行为和安全性约束。本文提出的方法旨在通过高效的状态表示来处理稀疏奖励环境中的探索与安全保障之间的权衡。&lt;h4&gt;背景&lt;/h4&gt;安全强化学习要求智能体在执行动作时必须兼顾探索行为的安全性和对环境的适应性。现有方法倾向于从相关环境中学习先验Q函数以防止危险操作，但这种方法往往导致大量误报，使得某些安全行动无法被执行，从而影响稀疏奖励环境下有效的探索。&lt;h4&gt;目的&lt;/h4&gt;本工作旨在通过学习高效的状态表示来平衡稀疏奖励环境中的探索行为和安全性偏好。具体来说，它试图改进智能体在确保自身安全的同时有效探索新领域的策略。&lt;h4&gt;方法&lt;/h4&gt;首先利用自动编码器将图像输入映射为潜在状态表示；随后采用对比学习目标进一步区分安全与不安全的状态。此外，在学习阶段引入了基于潜在距离的安全检查机制，使得当智能体进入危险环境时能够偏向于安全的探索路径。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，提出的方法能够在保持安全和效率良好平衡的同时更好地探索导航型MiniGrid环境。&lt;h4&gt;结论&lt;/h4&gt;通过改进状态表示的学习方法，可以更有效地解决稀疏奖励环境下强化学习的安全性与探索效率之间的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In safe reinforcement learning, agent needs to balance between explorationactions and safety constraints. Following this paradigm, domain transferapproaches learn a prior Q-function from the related environments to preventunsafe actions. However, because of the large number of false positives, somesafe actions are never executed, leading to inadequate exploration insparse-reward environments. In this work, we aim to learn an efficient staterepresentation to balance the exploration and safety-prefer action in asparse-reward environment. Firstly, the image input is mapped to latentrepresentation by an auto-encoder. A further contrastive learning objective isemployed to distinguish safe and unsafe states. In the learning phase, thelatent distance is used to construct an additional safety check, which allowsthe agent to bias the exploration if it visits an unsafe state. To verify theeffectiveness of our method, the experiment is carried out in threenavigation-based MiniGrid environments. The result highlights that our methodcan explore the environment better while maintaining a good balance betweensafety and efficiency.</description>
      <author>example@mail.com (Duc Kien Doan, Bang Giang Le, Viet Cuong Ta)</author>
      <guid isPermaLink="false">2503.10318v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Siamese Foundation Models for Crystal Structure Prediction</title>
      <link>http://arxiv.org/abs/2503.10471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种用于晶体结构预测(CSP)的Siamese基础模型框架DAO，该框架在基准测试中表现出色，并且对于实际材料展现出优异的性能。&lt;h4&gt;背景&lt;/h4&gt;CSP旨在从给定组成生成稳定的晶体结构，在其他领域（如蛋白质）中的结构预测任务已取得显著进展，但在CSP领域由于晶体结构复杂性较高，仍是一个未充分探索的区域。&lt;h4&gt;目的&lt;/h4&gt;提出一种专门针对CSP问题设计的基础模型框架DAO，以提高材料科学的研究与发展。&lt;h4&gt;方法&lt;/h4&gt;DAO框架包含两个互补的基础模型：DAO-G用于结构生成，DAO-P提供能量预测支持。采用预训练-微调策略进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在基准测试中（MP-20和MPTS-52），DAO-G在所有指标上均超越了当前最佳方法；基础模型框架对于复杂材料的实际应用展示出准确的临界温度预测能力及结构生成效果。&lt;h4&gt;结论&lt;/h4&gt;通过此研究，展示了Siamese基础模型框架在CSP领域的潜力，并为未来材料科学的研究与发展提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;晶体结构预测(CSP)，旨在从给定组成生成稳定的晶体结构，是发现新材料的关键途径。虽然其他领域（如蛋白质）中的结构预测任务取得了显著进展，但由于晶体内固有的复杂几何形状，CSP在很大程度上仍是一个未充分探索的区域。本文提出了一种专门针对CSP问题设计的基础模型框架DAO，它包含两个互补的基础模型：DAO-G用于生成晶体结构，DAO-P提供能量预测支持。实验显示，在基准测试中（MP-20和MPTS-52），DAO-G在所有指标上均超越了当前最佳方法；对于实际复杂材料的应用展示出准确的临界温度预测能力及结构生成效果。与传统的DFT计算器相比，如Quantum Espresso，在处理实际超级导体时表现出更高的准确性与更快的速度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crystal Structure Prediction (CSP), which aims to generate stable crystalstructures from compositions, represents a critical pathway for discoveringnovel materials. While structure prediction tasks in other domains, such asproteins, have seen remarkable progress, CSP remains a relatively underexploredarea due to the more complex geometries inherent in crystal structures. In thispaper, we propose Siamese foundation models specifically designed to addressCSP. Our pretrain-finetune framework, named DAO, comprises two complementaryfoundation models: DAO-G for structure generation and DAO-P for energyprediction. Experiments on CSP benchmarks (MP-20 and MPTS-52) demonstrate thatour DAO-G significantly surpasses state-of-the-art (SOTA) methods across allmetrics. Extensive ablation studies further confirm that DAO-G excels ingenerating diverse polymorphic structures, and the dataset relaxation andenergy guidance provided by DAO-P are essential for enhancing DAO-G'sperformance. When applied to three real-world superconductors($\text{CsV}_3\text{Sb}_5$, $ \text{Zr}_{16}\text{Rh}_8\text{O}_4$ and$\text{Zr}_{16}\text{Pd}_8\text{O}_4$) that are known to be challenging toanalyze, our foundation models achieve accurate critical temperaturepredictions and structure generations. For instance, on$\text{CsV}_3\text{Sb}_5$, DAO-G generates a structure close to theexperimental one with an RMSE of 0.0085; DAO-P predicts the $T_c$ value withhigh accuracy (2.26 K vs. the ground-truth value of 2.30 K). In contrast,conventional DFT calculators like Quantum Espresso only successfully derive thestructure of the first superconductor within an acceptable time, while the RMSEis nearly 8 times larger, and the computation speed is more than 1000 timesslower. These compelling results collectively highlight the potential of ourapproach for advancing materials science research and development.</description>
      <author>example@mail.com (Liming Wu, Wenbing Huang, Rui Jiao, Jianxing Huang, Liwei Liu, Yipeng Zhou, Hao Sun, Yang Liu, Fuchun Sun, Yuxiang Ren, Jirong Wen)</author>
      <guid isPermaLink="false">2503.10471v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating the Impact of Synthetic Data on Object Detection Tasks in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2503.09803v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了合成数据在增强自动驾驶系统性能中的作用，通过对比实际数据和混合使用实际与合成数据的模型训练结果，评估其对模型鲁棒性和泛化能力的影响。&lt;h4&gt;背景&lt;/h4&gt;随着自动驾驶系统的广泛应用，需要大规模、高质量的数据集来确保不同场景下的稳定表现。由于成本效益高且可提供精确标签，合成数据成为增强现实世界数据的一种可行方法。&lt;h4&gt;目的&lt;/h4&gt;为了评估合成数据的实用性和局限性，本文通过控制实验进行研究，并探讨其在提高自动驾驶技术中的潜在价值。&lt;h4&gt;方法&lt;/h4&gt;使用多个实际数据集和由BIT Technology Solutions GmbH生成的合成数据集进行了跨摄像机和LiDAR传感器模态的研究，分析了2D和3D物体检测任务中模型训练的表现。&lt;h4&gt;主要发现&lt;/h4&gt;混合使用真实与合成数据可以提高对象检测模型的鲁棒性和泛化能力，表明合成数据在提升自动驾驶技术方面具有潜力。&lt;h4&gt;结论&lt;/h4&gt;结合实际和合成数据集进行模型训练可以有效提升自动驾驶系统的性能，并且是未来研究的一个重要方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要中讨论了自主驾驶系统对大规模高质量数据的需求以及合成数据在此方面的应用价值，通过实验评估并展示了将真实与合成数据相结合的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing applications of autonomous driving systems necessitateslarge-scale, high-quality datasets to ensure robust performance across diversescenarios. Synthetic data has emerged as a viable solution to augmentreal-world datasets due to its cost-effectiveness, availability of preciseground-truth labels, and the ability to model specific edge cases. However,synthetic data may introduce distributional differences and biases that couldimpact model performance in real-world settings. To evaluate the utility andlimitations of synthetic data, we conducted controlled experiments usingmultiple real-world datasets and a synthetic dataset generated by BITTechnology Solutions GmbH. Our study spans two sensor modalities, camera andLiDAR, and investigates both 2D and 3D object detection tasks. We comparemodels trained on real, synthetic, and mixed datasets, analyzing theirrobustness and generalization capabilities. Our findings demonstrate that theuse of a combination of real and synthetic data improves the robustness andgeneralization of object detection models, underscoring the potential ofsynthetic data in advancing autonomous driving technologies.</description>
      <author>example@mail.com (Enes Özeren, Arka Bhowmick)</author>
      <guid isPermaLink="false">2503.09803v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Fusion Model Leveraging MLP Mixer and Handcrafted Features-based Deep Learning Networks for Facial Palsy Detection</title>
      <link>http://arxiv.org/abs/2503.10371v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PAKDD 2025. arXiv admin note: text overlap with arXiv:2405.16496&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;基于算法的面部瘫痪检测方法有潜力改善目前依赖于劳动密集型和主观性临床评估的传统做法。&lt;h4&gt;目的&lt;/h4&gt;提出一种多模态融合深度学习模型来提高面部瘫痪的诊断准确性。&lt;h4&gt;方法&lt;/h4&gt;该模型利用MLP Mixer模型处理未结构化的数据（如RGB图像或带有面部线条段的图像），并使用前馈神经网络处理结构化数据（例如面部标志坐标、表情特征或人工设计的特征）。&lt;h4&gt;主要发现&lt;/h4&gt;在20名面部瘫痪患者和20名健康受试者的视频研究中，该模型实现了96.00 F1分数，显著高于仅基于手动制作特征训练的前馈神经网络（82.80 F1）以及仅基于原始RGB图像训练的MLP Mixer模型（89.00 F1）。&lt;h4&gt;结论&lt;/h4&gt;多模态融合方法在面部瘫痪检测中显示出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;算法性地检测面部麻痹可以改善目前依赖劳动密集型且主观性的临床评估实践。本文提出了一个基于多模态融合的深度学习模型，利用MLP Mixer处理非结构化数据（例如RGB图像或带有面部线条段的图像），并使用前馈神经网络来处理结构化数据（如面部标志坐标、表情特征或手工设计的特征）以检测面部麻痹。研究分析了不同数据模式的影响以及多模态融合方法的好处，通过对20名面部麻痹患者和20名健康受试者的视频进行测试，该模型实现了96.00 F1分数，这显著高于仅基于手工制作的特性训练的前馈神经网络（82.80 F1）以及仅基于原始RGB图像训练的MLP Mixer模型（89.00 F1）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Algorithmic detection of facial palsy offers the potential to improve currentpractices, which usually involve labor-intensive and subjective assessments byclinicians. In this paper, we present a multimodal fusion-based deep learningmodel that utilizes an MLP mixer-based model to process unstructured data (i.e.RGB images or images with facial line segments) and a feed-forward neuralnetwork to process structured data (i.e. facial landmark coordinates, featuresof facial expressions, or handcrafted features) for detecting facial palsy. Wethen contribute to a study to analyze the effect of different data modalitiesand the benefits of a multimodal fusion-based approach using videos of 20facial palsy patients and 20 healthy subjects. Our multimodal fusion modelachieved 96.00 F1, which is significantly higher than the feed-forward neuralnetwork trained on handcrafted features alone (82.80 F1) and an MLP mixer-basedmodel trained on raw RGB images (89.00 F1).</description>
      <author>example@mail.com (Heng Yim Nicole Oo, Min Hun Lee, Jeong Hoon Lim)</author>
      <guid isPermaLink="false">2503.10371v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing</title>
      <link>http://arxiv.org/abs/2503.10392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了RoMA框架，利用大规模、多样化的无标签数据对基于Mamba架构的遥感基础模型进行可扩展的自监督预训练。&lt;h4&gt;背景&lt;/h4&gt;近期关于视觉Transformer（ViTs）的自我监督学习进展推动了遥感领域的重大突破。然而，自注意力机制呈二次复杂性，这对大规模模型和高分辨率图像的扩展性构成挑战。尽管Mamba架构提供了线性复杂性的替代方案，但现有的基于Mamba的应用仍局限于小规模、领域特定数据上的有监督任务。&lt;h4&gt;目的&lt;/h4&gt;提出RoMA框架，解决遥感基础模型在大规模无标签数据上进行自监督预训练时面临的可扩展性问题。&lt;h4&gt;方法&lt;/h4&gt;1) 提出旋转感知的预训练机制，结合自适应裁剪和角度嵌入来处理任意方向上的稀疏分布对象；2) 设计多尺度标记预测目标以应对遥感图像中固有的极端变化对象尺寸。&lt;h4&gt;主要发现&lt;/h4&gt;系统性实验证明了Mamba架构符合遥感数据和参数扩展法则，性能随着模型大小和数据量的增加而可靠地提高。实验表明RoMA预训练的Mamba模型在场景分类、目标检测和语义分割任务中均优于ViT基线模型，在精度和计算效率方面均有提升。&lt;h4&gt;结论&lt;/h4&gt;通过引入RoMA框架解决了基于Mamba架构的遥感基础模型面临的可扩展性挑战，使得大规模无标签数据的自监督预训练成为可能。实验结果表明RoMA具有较高的准确性和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;近期视觉Transformer（ViTs）自我监督学习的进步为遥感触模式奠定了坚实的基础。然而，自注意力机制所导致的二次复杂性对大模型和高分辨率图像构成扩展障碍。尽管Mamba架构作为线性复杂性的替代方案展现出前景，但现有基于Mamba的应用仍局限于小规模、领域特定数据上的有监督任务。为此，我们提出了RoMA框架，即大规模自监督预训练的遥感基础模型，利用大规模多样化的无标签数据和定制化的自回归学习策略来增强对高分辨率图像的支持。通过结合自适应裁剪与角度嵌入以及多尺度标记预测目标等创新技术，RoMA不仅解决了远程传感器图像固有的极端变化对象尺寸问题，还证明了Mamba架构在扩展性方面的优势。实验验证表明，在场景分类、目标检测和语义分割任务中，RoMA预训练的模型相比基于ViT的模型具有更高的准确性和计算效率。相关源代码与预训练模型将在https://github.com/MiliLab/RoMA公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in self-supervised learning for Vision Transformers (ViTs)have fueled breakthroughs in remote sensing (RS) foundation models. However,the quadratic complexity of self-attention poses a significant barrier toscalability, particularly for large models and high-resolution images. Whilethe linear-complexity Mamba architecture offers a promising alternative,existing RS applications of Mamba remain limited to supervised tasks on small,domain-specific datasets. To address these challenges, we propose RoMA, aframework that enables scalable self-supervised pretraining of Mamba-based RSfoundation models using large-scale, diverse, unlabeled data. RoMA enhancesscalability for high-resolution images through a tailored auto-regressivelearning strategy, incorporating two key innovations: 1) a rotation-awarepretraining mechanism combining adaptive cropping with angular embeddings tohandle sparsely distributed objects with arbitrary orientations, and 2)multi-scale token prediction objectives that address the extreme variations inobject scales inherent to RS imagery. Systematic empirical studies validatethat Mamba adheres to RS data and parameter scaling laws, with performancescaling reliably as model and data size increase. Furthermore, experimentsacross scene classification, object detection, and semantic segmentation tasksdemonstrate that RoMA-pretrained Mamba models consistently outperform ViT-basedcounterparts in both accuracy and computational efficiency. The source code andpretrained models will be released at https://github.com/MiliLab/RoMA.</description>
      <author>example@mail.com (Fengxiang Wang, Hongzhen Wang, Yulin Wang, Di Wang, Mingshuo Chen, Haiyan Zhao, Yangang Sun, Shuo Wang, Long Lan, Wenjing Yang, Jing Zhang)</author>
      <guid isPermaLink="false">2503.10392v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Do We Always Need the Simplicity Bias? Looking for Optimal Inductive Biases in the Wild</title>
      <link>http://arxiv.org/abs/2503.10065v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了神经网络架构倾向于用相对简单的函数拟合数据的“简单性偏见”的极限，提出了用于特定任务的新激活函数和归纳偏差的学习方法。&lt;h4&gt;背景&lt;/h4&gt;研究表明，“简单性偏见”主要源自ReLU激活函数。这种倾向被视为神经网络成功的关键因素。&lt;h4&gt;目的&lt;/h4&gt;研究在不同任务中ReLU激活函数是否仍然适用，并探索学习更适合具体任务的新的激活函数的方法。&lt;h4&gt;方法&lt;/h4&gt;基于先前的研究，开发了元学习新激活函数和归纳偏差的方法，这些新方法针对特定的任务优化性能。&lt;h4&gt;主要发现&lt;/h4&gt;发现了多个场景，在这些场景中简单性偏见不足以发挥效用，ReLU激活函数的表现不如其他可能的替代方案。这包括表格数据、回归任务等神经网络传统上难以应对的问题类型。&lt;h4&gt;结论&lt;/h4&gt;ReLU网络的简单性偏见并非适用于所有情况。对于图像分类任务而言，它几乎是最佳选择；但在某些其他情况下，更复杂的归纳偏差可能是更好的选择。&lt;h4&gt;翻译&lt;/h4&gt;神经架构倾向于用相对简单的函数来拟合数据，这种“简单性偏见”被广泛认为是其成功的关键。本文探讨了这一原则的极限。基于最近发现ReLU激活函数导致了简单性偏见的研究成果，我们介绍了一种元学习新激活函数和更适合特定任务归纳偏差的方法。主要发现在多个任务中，简单的偏见不足以使用，并且ReLU表现不佳。在这种情况下，我们通过产生更高的复杂度先验来诱导新的更好的激活函数。有趣的是，这些情况对应于神经网络历史上难以处理的领域：表格数据、回归任务等。相比之下，在图像任务上，由ReLU引入的简单性偏见是足够的，并且最佳学习到的激活函数几乎与ReLU和GeLUs相同。总的来说，与普遍的看法相反，ReLU网络的简单性偏见并不是通用的解决方案。在图像分类中它几乎是最优的选择，但在其他情况下，其他的归纳偏差可能是更好的选择。我们展示了激活函数可以控制这些归纳偏差，但未来的定制架构可能提供更多益处。为了更好地描述模型的归纳偏差并评估其与数据的适应性，仍然需要更多的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural architectures tend to fit their data with relatively simple functions.This "simplicity bias" is widely regarded as key to their success. This paperexplores the limits of this principle. Building on recent findings that thesimplicity bias stems from ReLU activations [96], we introduce a method tometa-learn new activation functions and inductive biases better suited tospecific tasks.  Findings: We identify multiple tasks where the simplicity bias is inadequateand ReLUs suboptimal. In these cases, we learn new activation functions thatperform better by inducing a prior of higher complexity. Interestingly, thesecases correspond to domains where neural networks have historically struggled:tabular data, regression tasks, cases of shortcut learning, and algorithmicgrokking tasks. In comparison, the simplicity bias induced by ReLUs provesadequate on image tasks where the best learned activations are nearly identicalto ReLUs and GeLUs.  Implications: Contrary to popular belief, the simplicity bias of ReLUnetworks is not universally useful. It is near-optimal for imageclassification, but other inductive biases are sometimes preferable. We showedthat activation functions can control these inductive biases, but futuretailored architectures might provide further benefits. Advances are stillneeded to characterize a model's inductive biases beyond "complexity", andtheir adequacy with the data.</description>
      <author>example@mail.com (Damien Teney, Liangze Jiang, Florin Gogianu, Ehsan Abbasnejad)</author>
      <guid isPermaLink="false">2503.10065v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>BioSerenity-E1: a self-supervised EEG model for medical applications</title>
      <link>http://arxiv.org/abs/2503.10362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;电生理图（EEG）在神经学中是一项重要的诊断工具，但其准确的手动解读是一个耗时的过程，并且需要具备高度专业技能的专家来完成。然而，这类专业知识相对稀缺并且并不总是可以获取的。为了解决这些问题，实施自动预筛选和分析系统对于EEG数据具有很大的潜力。&lt;h4&gt;背景&lt;/h4&gt;手动解析EEG图耗时且依赖于专门知识，而这些知识通常较为稀缺。&lt;h4&gt;目的&lt;/h4&gt;介绍BioSerenity-E1模型，这是第一个结合了谱令牌化与掩码预测的自监督基础模型家族成员，旨在为临床EEG应用提供前沿性能表现。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段的自监督预训练框架，首先通过基于变换器的VQ-VAE架构获取压缩后的EEG表示，然后使用广泛的（70%块）掩码令牌预测强制学习复杂的空间时间依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;BioSerenity-E1在三个临床任务中表现出强劲性能：癫痫检测、正常/异常分类以及不平衡数据上的多类病理性别区分。该模型在低数据环境下的效能也得到了证实，表明训练数据不足时的AUPRC显著提高（从+2%到17%）。&lt;h4&gt;结论&lt;/h4&gt;BioSerenity-E1是第一个结合自监督学习和EEG信号复杂时空依赖性的谱令牌化与掩码预测模型。该模型在临床任务中具有优越性能，并且在低数据环境下的表现尤其突出。&lt;h4&gt;翻译&lt;/h4&gt;电生理图（EEG）作为神经学中的重要诊断工具，其准确的手动解读是一个耗时的过程并需要专业的技能。然而，由于专业人员的相对稀缺和难以获取性，这成为了实施自动预筛选和分析系统的一个机会点。随着自监督学习的进步，复杂的深度学习架构可以在大量的未标记EEG数据上进行预先训练，以学习通用表示形式，并在执行多项任务时减少所需的下游数据量而提升性能表现。本文介绍了BioSerenity-E1，这是一系列为临床EEG应用设计的自监督基础模型中的首个成员，它结合了谱令牌化和掩码预测，实现了相关诊断任务上的顶尖水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) serves as an essential diagnostic tool inneurology; however, its accurate manual interpretation is a time-intensiveprocess that demands highly specialized expertise, which remains relativelyscarce and not consistently accessible. To address these limitations, theimplementation of automated pre-screening and analysis systems for EEG dataholds considerable promise. Advances in self-supervised learning made itpossible to pre-train complex deep learning architectures on large volumes ofunlabeled EEG data to learn generalizable representations, that can later beused to enhance performance on multiple tasks while needing less downstreamdata. In the present paper, we introduce BioSerenity-E1, the first of a familyof self-supervised foundation models for clinical EEG applications thatcombines spectral tokenization with masked prediction to achievestate-of-the-art performance across relevant diagnostic tasks. The two-phaseself-supervised pretraining framework initially acquires compressed EEGrepresentations via a transformer-based VQ-VAE architecture designed toreconstruct log-multitaper spectral projections, then implements extensive (70%block) masked token prediction to force the model to learn complexspatiotemporal dependencies in EEG signals. BioSerenity-E1 achieves strongperformance across three clinical tasks, either in line or abovestate-of-the-art methods: seizure detection (AUROC = 0.926, Sensitivity =0.909), normal/abnormal classification (AUPRC = 0.970 on proprietary data;0.910 on TUH-Abnormal), and multiclass pathology differentiation on unbalanceddata (Weighted F1 = 0.730). The utility of BioSerenity-E1 is further confirmedin low-data regimes scenarios, showing clear improvements in AUPRC (from +2% to17%) when trained on less than 10% of the available data.</description>
      <author>example@mail.com (Ruggero G. Bettinardi, Mohamed Rahmouni, Ulysse Gimenez)</author>
      <guid isPermaLink="false">2503.10362v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Bilingual Dual-Head Deep Model for Parkinson's Disease Detection from Speech</title>
      <link>http://arxiv.org/abs/2503.10301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICASSP 2025 - Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种针对帕金森病（PD）语音信号双语环境下的检测问题的新型深度神经网络架构，该架构旨在通过基于类型的基础二分类来提高疾病的检测准确性。&lt;h4&gt;背景&lt;/h4&gt;帕金森病患者的语音模式会受到疾病的影响，在不同语言环境中如何准确检测这些变化是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种针对帕金森病患者语音信号的双语环境下PD检测问题的解决方案，使用基于类型的基础二分类方法来提高诊断准确性。&lt;h4&gt;方法&lt;/h4&gt;设计了一种特殊的双头深度神经网络架构：一个专注于处理连音节发音模式（diadochokinetic patterns），另一个用于寻找自然语言中连续说话片段中的语音模式。这些模型从自监督学习（SSL）模型和小波变换中提取语音表示，并利用适应层、卷积瓶颈以及对比学习技术来减少跨语言差异。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，使用单一语言数据集训练的传统模型在跨语言泛化方面表现不佳；直接组合不同语言的数据集也不是最优方法。而本研究提出的模型在同一时间提高了两种语言中的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在处理帕金森病双语语音信号检测问题上显示出比传统方法更好的性能，特别是在提高跨语言模型的泛化能力方面具有明显优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要：这项工作旨在通过基于类型的基础二分类方法解决双语环境下的帕金森氏病（PD）从语音信号中的检测问题。一种头专门处理连音节发音模式。另一种用于寻找自然语言中连续说话片段的语音模式，只有一种头部在相应输入的基础上运行。语音表示是从自监督学习模型和小波变换中提取出来的，并利用适应层、卷积瓶颈以及对比学习技术来减少跨语言差异。我们的解决方案在两个独立的数据集EWA-DB和PC-GITA上进行了测试，这两个数据集分别涵盖了斯洛伐克语和西班牙语。结果表明，基于单一语言数据集训练的传统模型在跨语言泛化方面存在困难，并且直接组合不同语言的数据集的方法效果不佳。相比之下，我们的模型同时提高了两种语言的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICASSP49660.2025.10889445&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work aims to tackle the Parkinson's disease (PD) detection problem fromthe speech signal in a bilingual setting by proposing an ad-hoc dual-head deepneural architecture for type-based binary classification. One head isspecialized for diadochokinetic patterns. The other head looks for naturalspeech patterns present in continuous spoken utterances. Only one of the twoheads is operative accordingly to the nature of the input. Speechrepresentations are extracted from self-supervised learning (SSL) models andwavelet transforms. Adaptive layers, convolutional bottlenecks, and contrastivelearning are exploited to reduce variations across languages. Our solution isassessed against two distinct datasets, EWA-DB, and PC-GITA, which cover Slovakand Spanish languages, respectively. Results indicate that conventional modelstrained on a single language dataset struggle with cross-linguisticgeneralization, and naive combinations of datasets are suboptimal. In contrast,our model improves generalization on both languages, simultaneously.</description>
      <author>example@mail.com (Moreno La Quatra, Juan Rafael Orozco-Arroyave, Marco Sabato Siniscalchi)</author>
      <guid isPermaLink="false">2503.10301v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Cover Learning for Large-Scale Topology Representation</title>
      <link>http://arxiv.org/abs/2503.09767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 17 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种优化视角下的学习几何数据集的拓扑忠实覆盖的方法，这种方法可以获得在大小和大型拓扑表示方面优于传统方法的单纯复形。&lt;h4&gt;背景&lt;/h4&gt;传统的无监督学习方法如聚类和线性降维对于离散或线性的大规模几何结构进行参数化。而现代流形学习方法通过构造输入数据上的图来寻找低维表示并推断局部几何性质。拓扑数据分析则推广了单纯复形的使用，主要分为几何复杂体的拓扑推理以及Mapper图的大规模拓扑可视化。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法以克服现有技术的局限性，如几何复杂体随着数据量增加而性能下降和Mapper图难以调整且仅包含低维信息的问题。&lt;h4&gt;方法&lt;/h4&gt;从优化角度出发研究学习覆盖的问题，并描述了一种能够生成具有较小规模并且能更好地表示大规模拓扑结构的单纯复形的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用新的方法，获得的单纯复形在大小和大尺度拓扑表示方面都优于标准的拓扑推理方法以及Mapper型算法。&lt;h4&gt;结论&lt;/h4&gt;新提出的方法能够在不牺牲表现力的同时有效地减少复杂度，并且能够更好地捕捉数据集中的大规模拓扑结构。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了从传统的无监督学习到现代流形学习再到拓扑数据分析的发展趋势，特别是在处理几何和大型数据集时的挑战。论文提出了一个新的视角来研究如何有效学习覆盖以生成更小规模但表现更好的单纯复形，并通过实验验证了该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classical unsupervised learning methods like clustering and lineardimensionality reduction parametrize large-scale geometry when it is discreteor linear, while more modern methods from manifold learning find lowdimensional representation or infer local geometry by constructing a graph onthe input data. More recently, topological data analysis popularized the use ofsimplicial complexes to represent data topology with two main methodologies:topological inference with geometric complexes and large-scale topologyvisualization with Mapper graphs -- central to these is the nerve constructionfrom topology, which builds a simplicial complex given a cover of a space bysubsets. While successful, these have limitations: geometric complexes scalepoorly with data size, and Mapper graphs can be hard to tune and only containlow dimensional information. In this paper, we propose to study the problem oflearning covers in its own right, and from the perspective of optimization. Wedescribe a method for learning topologically-faithful covers of geometricdatasets, and show that the simplicial complexes thus obtained can outperformstandard topological inference approaches in terms of size, and Mapper-typealgorithms in terms of representation of large-scale topology.</description>
      <author>example@mail.com (Luis Scoccola, Uzu Lim, Heather A. Harrington)</author>
      <guid isPermaLink="false">2503.09767v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>DRESS: Disentangled Representation-based Self-Supervised Meta-Learning for Diverse Tasks</title>
      <link>http://arxiv.org/abs/2503.09679v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures. An earlier version of the paper has been  presented at the Self-Supervised Learning workshop at the 2024 NeurIPS  conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;元学习在解决少量样本学习任务中代表了一种强有力的方法类别。然而，最近的研究表明，简单地预训练一个通用编码器可能足以超越元学习算法的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的元学习方法在处理少量样本学习任务时表现不佳，可能是由于这些任务缺乏多样性导致的。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于解耦表示的自监督元学习方法DRESS，该方法能够快速适应高度多样化的少量样本学习任务。&lt;h4&gt;方法&lt;/h4&gt;DRESS利用解耦表示学习来创建用于驱动元训练过程的自监督任务，并提出了一个基于类别划分的任务多样性度量方法，在输入空间上直接量化任务的多样性。&lt;h4&gt;主要发现&lt;/h4&gt;通过在具有多种变化因素和不同复杂程度的数据集上的实验，结果显示DRESS能够在大多数数据集和任务配置下超越竞争性方法。&lt;h4&gt;结论&lt;/h4&gt;该研究倡导重新审视用于任务适应性研究的合适设置，并旨在通过解耦表示重燃对元学习解决少量样本学习问题潜力的兴趣。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning represents a strong class of approaches for solving few-shotlearning tasks. Nonetheless, recent research suggests that simply pre-traininga generic encoder can potentially surpass meta-learning algorithms. In thispaper, we first discuss the reasons why meta-learning fails to stand out inthese few-shot learning experiments, and hypothesize that it is due to thefew-shot learning tasks lacking diversity. We propose DRESS, a task-agnosticDisentangled REpresentation-based Self-Supervised meta-learning approach thatenables fast model adaptation on highly diversified few-shot learning tasks.Specifically, DRESS utilizes disentangled representation learning to createself-supervised tasks that can fuel the meta-training process. Furthermore, wealso propose a class-partition based metric for quantifying the task diversitydirectly on the input space. We validate the effectiveness of DRESS throughexperiments on datasets with multiple factors of variation and varyingcomplexity. The results suggest that DRESS is able to outperform competingmethods on the majority of the datasets and task setups. Through this paper, weadvocate for a re-examination of proper setups for task adaptation studies, andaim to reignite interest in the potential of meta-learning for solving few-shotlearning tasks via disentangled representations.</description>
      <author>example@mail.com (Wei Cui, Tongzi Wu, Jesse C. Cresswell, Yi Sui, Keyvan Golestan)</author>
      <guid isPermaLink="false">2503.09679v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>KV-Distill: Nearly Lossless Learnable Context Compression for LLMs</title>
      <link>http://arxiv.org/abs/2503.10337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;KV-Distill是一种Transformer压缩框架，它能够将长上下文的KV缓存转换为较短但仍然有效的表示形式。&lt;h4&gt;背景&lt;/h4&gt;序列到序列的任务常常受益于较长的上下文，但由于标准Transformers中的自我注意机制具有二次复杂度，在处理长时间上下文时变得困难。在生成过程中，临时存储在所谓的KV缓存中的表示占据了GPU内存使用的大部，并且随着上下文长度的增加而线性增长。&lt;h4&gt;目的&lt;/h4&gt;提出一种压缩框架来解决长上下文下的效率问题，同时保持模型的能力和性能。&lt;h4&gt;方法&lt;/h4&gt;引入了KV-Distill框架，通过知识蒸馏技术将长时间上下文的KV缓存转化为较短表示形式。该框架可以作为参数高效的适配器应用于预训练模型中，并支持压缩任意跨度的上下文。&lt;h4&gt;主要发现&lt;/h4&gt;在最坏情况下的提取任务中，KV-Distill的表现优于其他压缩技术；在长上下文问题回答和总结任务上接近未压缩模型性能；并且可以在特定领域的上下文中进行微调，从而减少长度多达99%而不会损害下游性能。&lt;h4&gt;结论&lt;/h4&gt;实验表明，KV-Distill具有良好的泛化能力，适用于各种规模的模型以及不同的架构。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为：序列到序列的任务常常受益于较长的上下文，但是标准Transformers中自我注意机制所具有的二次复杂度使得这一任务变得困难。在生成过程中，临时存储在所谓的KV缓存中的表示占据了GPU内存使用的大部，并且随着上下文长度的增长而线性增加。我们介绍了KV-Distill，这是一种Transformer压缩框架，它可以将长上下文的KV缓存在问题无关的方式下蒸馏成显著较短的表示形式。KV-Distill可以作为参数高效的适配器来训练预训练模型，并能够同时保持预先训练的模型能力的同时压缩任意跨度的上下文。我们将一个被压缩和未被压缩的缓存视为师生配对，并应用KL型散度以匹配生成输出。在最坏情况下的提取任务中，KV-Distill的表现优于其他压缩技术，在长上下文问题回答和总结方面接近了未压缩模型性能；并且可以在特定领域的上下文中进行微调，从而将长度减少至多99%，同时保持下游性能的完整度。我们展示了KV-Distill在各种规模与架构模型上的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequence-to-sequence tasks often benefit from long contexts, but thequadratic complexity of self-attention in standard Transformers renders thisnon-trivial. During generation, temporary representations -stored in theso-called KV cache-account for a large portion of GPU memory usage and scalelinearly with context length. We introduce KV-Distill, a Transformercompression framework that distills long context KV caches into significantlyshorter representations in a question-independent fashion. KV-Distill can betrained as a parameter-efficient adaptor for pretrained models, and enables thecompression of arbitrary spans of a context while preserving pre-trained modelcapabilities. We treat a compressed-uncompressed cache as a student-teacherpairing and apply a KL-type divergence to match the generated outputs.KV-Distill outperforms other compression techniques in worst-case extractivetasks and approaches uncompressed performance in long context questionanswering and summarization, and it can be fine-tuned on domain-specificcontexts to reduce lengths by up to 99% while preserving downstreamperformance. We demonstrate the generalizability of KV-Distill across variousmodel sizes and architectures.</description>
      <author>example@mail.com (Vivek Chari, Guanghui Qin, Benjamin Van Durme)</author>
      <guid isPermaLink="false">2503.10337v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Preference Aggregation</title>
      <link>http://arxiv.org/abs/2503.10215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;AI对齐是一项确保人工智能系统符合人类价值观的重要挑战。&lt;h4&gt;背景&lt;/h4&gt;随着基础模型和推荐系统的开发，AI对齐问题变得日益重要。现有的主要方法是基于人类反馈的强化学习（RLHF），但它在聚合多样化的人类偏好方面存在理论限制。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的偏好聚合策略，能够适应用户的上下文，并且继承了最大彩票方案的良好特性，这是一种康多塞一致性的解决方案概念。&lt;h4&gt;方法&lt;/h4&gt;利用最近发表的一个特定的抽样过程（urn process）提供的见解来设计新策略。&lt;h4&gt;主要发现&lt;/h4&gt;这种新策略可以在复杂的人工智能应用中有效地聚合多样化的用户偏好。&lt;h4&gt;结论&lt;/h4&gt;通过结合社会选择理论和先进的抽样技术，可以创建更有效的方法来解决AI对齐问题。&lt;h4&gt;翻译&lt;/h4&gt;AI的对齐挑战在于确保人工智能系统符合人类的价值观。随着基础模型和推荐系统的开发，这一问题日益凸显。尽管基于人类反馈的强化学习（RLHF）是目前的主要方法，但它在处理多样化的人类偏好时存在理论上的限制。为了应对这个挑战，通过借鉴最近发表的一个特定抽样过程提供的见解，该工作引入了一种适应用户上下文的新偏好聚合策略，并且这种策略继承了最大彩票方案的良好特性，这是一种康多塞一致性的解决方案概念。这种方法旨在更好地解决AI系统中的对齐问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI alignment, the challenge of ensuring AI systems act in accordance withhuman values, has emerged as a critical problem in the development of systemssuch as foundation models and recommender systems. Still, the current dominantapproach, reinforcement learning with human feedback (RLHF) faces knowntheoretical limitations in aggregating diverse human preferences. Social choicetheory provides a framework to aggregate preferences, but was not developed forthe multidimensional applications typical of AI. Leveraging insights from arecently published urn process, this work introduces a preference aggregationstrategy that adapts to the user's context and that inherits the goodproperties of the maximal lottery, a Condorcet-consistent solution concept.</description>
      <author>example@mail.com (Benjamin Heymann)</author>
      <guid isPermaLink="false">2503.10215v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>A Siamese Network to Detect If Two Iris Images Are Monozygotic</title>
      <link>http://arxiv.org/abs/2503.09749v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究采用Siamese网络架构和对比学习来分类虹膜图像，以确定两幅图像是来源于同卵双胞胎还是非同卵个体的虹膜。&lt;h4&gt;背景&lt;/h4&gt;传统上认为同一人的左右眼虹膜纹理差异与不同个体之间的虹膜相似。然而，已有研究表明人类可以准确地识别出两个虹膜是否来自于同一个体的不同眼睛或同卵双胞胎的眼睛，准确率约为80%。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、非侵入性的方法来判断两幅虹膜图像是否来自同卵双胞胎个体。&lt;h4&gt;方法&lt;/h4&gt;使用Siamese网络架构和对比学习技术构建了一个数据集，其中包括合成的同卵双胞胎对（同一人不同眼睛的图像）、自然形成的同卵双胞胎对（实际同卵双胞胎之间的虹膜图像）以及来自无关个体的非同卵双胞胎对。另外还训练了三种不同的模型版本：原始输入图像、仅包含虹膜区域的图像和不包括虹膜区域的其他眼部特征图像。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，利用完整的眼区信息进行训练的模型在区分同卵与非同卵虹膜图像方面的表现优于仅仅基于虹膜纹理细节的模型。该研究还展示了通过完整虹膜图像分类同卵和非同卵双胞胎的能力超过了人类此前报告的结果。&lt;h4&gt;结论&lt;/h4&gt;这项工作首次提出了一个能够判断两幅虹膜图像是来自于同卵个体还是非同卵个体的分类器，并且证明了眼周特征在区分同卵与非同卵虹膜图像中的重要性。&lt;h4&gt;翻译&lt;/h4&gt;摘要的中文翻译为：在Daugman风格的虹膜识别中，传统上认为来自同一人的左、右眼虹膜纹理差异与其他两个人之间的虹膜相似。然而，先前的研究表明人类可以准确地检测出两个虹膜图像是从同一个体的不同眼睛或同卵双胞胎的眼睛中获得的，准确率约为80%。在这项研究中，我们采用Siamese网络架构和对比学习技术来分类两幅虹膜图像，以确定它们是来自同卵还是非同卵虹膜。这可以作为一种快速、无创性测试方法，用于判断双胞胎是否为同卵或非同卵个体。为此我们构建了一个数据集，其中包含合成的同卵双胞胎对（同一人不同眼睛的图像）和自然形成的同卵双胞胎对（实际同卵双胞胎之间的虹膜图像），以及来自无关个体的非同卵双胞胎对，确保了模型能力的全面评估。为了获得更深入的学习表示理解，我们训练并分析了三种不同的模型版本：原始输入图像、仅包含虹膜区域的图像和不包括虹膜区域的其他眼部特征图像。这种比较揭示了虹膜特定纹理细节和眼周线索在识别同卵虹膜模式中的关键重要性。结果表明，在完整的眼区信息上的训练表现优于单独使用虹膜数据进行训练，强调了虹膜与眼周特性的微妙交互作用。我们的方法利用完整的虹膜图像实现了超出人类分类同卵双胞胎对能力的准确率水平。这项研究首次提出了一个能够判断两幅虹膜图像是来自于同卵个体还是非同卵个体的分类器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In Daugman-style iris recognition, the textures of the left and right irisesof the same person are traditionally considered as being as different as theirises of two unrelated persons. However, previous research indicates thathumans can detect that two iris images are from different eyes of the sameperson, or eyes of monozygotic twins, with an accuracy of about 80%. In thiswork, we employ a Siamese network architecture and contrastive learning tocategorize a pair of iris images as coming from monozygotic or non-monozygoticirises. This could potentially be applied, for example, as a fast, noninvasivetest to determine if twins are monozygotic or non-monozygotic. We construct adataset comprising both synthetic monozygotic pairs (images of different irisesof the same individual) and natural monozygotic pairs (images of differentimages from persons who are identical twins), in addition to non-monozygoticpairs from unrelated individuals, ensuring a comprehensive evaluation of themodel's capabilities. To gain deeper insights into the learned representations,we train and analyze three variants of the model using (1) the original inputimages, (2) iris-only images, and (3) non-iris-only images. This comparisonreveals the critical importance of iris-specific textural details andcontextual ocular cues in identifying monozygotic iris patterns. The resultsdemonstrate that models leveraging full eye-region information outperform thosetrained solely on iris-only data, emphasizing the nuanced interplay betweeniris and ocular characteristics. Our approach achieves accuracy levels usingthe full iris image that exceed those previously reported for humanclassification of monozygotic iris pairs. This study presents the firstclassifier designed to determine whether a pair of iris images originates frommonozygotic individuals.</description>
      <author>example@mail.com (Yongle Yuan, Kevin W. Bowyer)</author>
      <guid isPermaLink="false">2503.09749v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Mapless Collision-Free Flight via MPC using Dual KD-Trees in Cluttered Environments</title>
      <link>http://arxiv.org/abs/2503.10141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的无人机在复杂环境中实现无碰撞飞行的方法，无需构建3D地图或生成和跟踪无碰撞轨迹。&lt;h4&gt;背景&lt;/h4&gt;传统的无人机避障方法依赖于详细的3D地图构建、轨迹规划与追踪，然而这种方法容易引入累积误差和计算延迟，限制了飞行的敏捷性和安全性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的方法来使四旋翼无人机在复杂的环境中实现安全、灵活且高效的无碰撞飞行。&lt;h4&gt;方法&lt;/h4&gt;采用模型预测控制（MPC）直接从稀疏航点和深度相机提供的点云生成安全动作。利用双重KD树机制优化障碍物的识别与路径规划，其中Obstacle KD-Tree用于快速找到最近的障碍物以进行避障操作，而Edge KD-Tree则为MPC求解器提供了一个初始猜测值，防止其在避免障碍时陷入局部极小值。&lt;h4&gt;主要发现&lt;/h4&gt;方法通过广泛的模拟和实际实验验证了有效性，并展示了比基于地图的方法和模仿学习方法更好的性能，在模拟中可靠地避障速度达到12m/s，而在现实世界测试中的避障速度为6m/s。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法是一种简单且稳健的解决方案，相比现有技术在保持飞行敏捷性的同时提高了安全性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collision-free flight in cluttered environments is a critical capability forautonomous quadrotors. Traditional methods often rely on detailed 3D mapconstruction, trajectory generation, and tracking. However, this cascadepipeline can introduce accumulated errors and computational delays, limitingflight agility and safety. In this paper, we propose a novel method forenabling collision-free flight in cluttered environments without explicitlyconstructing 3D maps or generating and tracking collision-free trajectories.Instead, we leverage Model Predictive Control (MPC) to directly produce safeactions from sparse waypoints and point clouds from a depth camera. Thesesparse waypoints are dynamically adjusted online based on nearby obstaclesdetected from point clouds. To achieve this, we introduce a dual KD-Treemechanism: the Obstacle KD-Tree quickly identifies the nearest obstacle foravoidance, while the Edge KD-Tree provides a robust initial guess for the MPCsolver, preventing it from getting stuck in local minima during obstacleavoidance. We validate our approach through extensive simulations andreal-world experiments. The results show that our approach significantlyoutperforms the mapping-based methods and is also superior to imitationlearning-based methods, demonstrating reliable obstacle avoidance at up to 12m/s in simulations and 6 m/s in real-world tests. Our method provides a simpleand robust alternative to existing methods.</description>
      <author>example@mail.com (Linzuo Zhang, Yu Hu, Yang Deng, Feng Yu, Danping Zou)</author>
      <guid isPermaLink="false">2503.10141v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-based Planner and Graph-based Policy</title>
      <link>http://arxiv.org/abs/2503.10049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the 2025 IEEE International Conference on Robotics &amp;  Automation (ICRA 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于大语言模型和图协作的多智能体强化学习框架（LGC-MARL），用于解决复杂任务执行中的协调与安全性挑战。&lt;h4&gt;背景&lt;/h4&gt;多智能体系统在执行复杂任务方面表现出巨大潜力，但协调和安全仍面临重大挑战。尽管多智能体强化学习为代理合作提供了有前景的框架，但在处理复杂任务和设计奖励函数上存在困难。引入大语言模型增强了系统的推理和认知能力，然而现有的基于LLM的系统难以快速准确地响应动态环境。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够高效结合大语言模型和多智能体强化学习框架的方法，解决现有挑战。&lt;h4&gt;方法&lt;/h4&gt;LGC-MARL框架由两个主要组件组成：一个大语言模型规划器和图协作元策略。该框架将复杂任务分解为可执行的子任务，并通过基于图的协调实现多个代理之间的高效合作。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，LGC-MARL在AI2-THOR模拟平台上完成各种复杂任务时具有优越的表现和扩展性。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架能够有效应对多智能体系统面临的挑战，并展示了其解决复杂任务的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent systems (MAS) have shown great potential in executing complextasks, but coordination and safety remain significant challenges. Multi-AgentReinforcement Learning (MARL) offers a promising framework for agentcollaboration, but it faces difficulties in handling complex tasks anddesigning reward functions. The introduction of Large Language Models (LLMs)has brought stronger reasoning and cognitive abilities to MAS, but existingLLM-based systems struggle to respond quickly and accurately in dynamicenvironments. To address these challenges, we propose LLM-based GraphCollaboration MARL (LGC-MARL), a framework that efficiently combines LLMs andMARL. This framework decomposes complex tasks into executable subtasks andachieves efficient collaboration among multiple agents through graph-basedcoordination. Specifically, LGC-MARL consists of two main components: an LLMplanner and a graph-based collaboration meta policy. The LLM planner transformscomplex task instructions into a series of executable subtasks, evaluates therationality of these subtasks using a critic model, and generates an actiondependency graph. The graph-based collaboration meta policy facilitatescommunication and collaboration among agents based on the action dependencygraph, and adapts to new task environments through meta-learning. Experimentalresults on the AI2-THOR simulation platform demonstrate the superiorperformance and scalability of LGC-MARL in completing various complex tasks.</description>
      <author>example@mail.com (Ziqi Jia, Junjie Li, Xiaoyang Qu, Jianzong Wang)</author>
      <guid isPermaLink="false">2503.10049v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>G$^{2}$SF-MIAD: Geometry-Guided Score Fusion for Multimodal Industrial Anomaly Detection</title>
      <link>http://arxiv.org/abs/2503.10091v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;工业质量检测在现代制造业中至关重要，通过识别生产过程中的缺陷产品来确保产品质量。虽然单一模态方法（如3D点云或2D RGB图像）由于信息不完整而存在局限性，多模态异常检测技术通过跨模式数据融合提供了希望。&lt;h4&gt;背景&lt;/h4&gt;传统单模态工业质量检测在处理复杂制造环境时面临挑战，难以有效集成不同类型的传感器数据以提高识别精度。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的几何引导分数融合（G$^{2}$SF）框架，旨在解决现有方法在统一跨模式信息方面的问题，并提升异常检测的准确性。&lt;h4&gt;方法&lt;/h4&gt;该研究首先重新解释了单一模态下的记忆库异常评分，并将其转化为局部特征空间中的等距欧几里得距离。通过几何编码操作器和预测第一级局部特征分布的方向感知缩放因子，开发了一种新的本地尺度预测网络（LSPN）。此外，还提出了专门的损失函数和评分聚合策略来确保度量的一般化和有效性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在MVTec-3D AD数据集上实现了最先进的异常检测性能，在低正率和更好召回率的情况下显著提高了工业应用中的缺陷产品识别能力。详细的消融分析验证了各个组件的贡献。&lt;h4&gt;结论&lt;/h4&gt;G$^{2}$SF框架通过结合多模态信息，有效提升了工业质量检测中异常产品的识别效率，为现代制造业提供了一种强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了在单一模态方法（如3D点云或2D RGB图像）存在局限性的背景下，提出了一种新的几何引导分数融合（G$^{2}$SF）框架来解决工业质量检测中的挑战。该研究通过重新解释记忆库异常评分、开发局部尺度预测网络（LSPN）、以及使用专门的损失函数和评分聚合策略，展示了在MVTec-3D AD数据集上的卓越性能，并验证了其方法的有效性和潜在应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Industrial quality inspection plays a critical role in modern manufacturingby identifying defective products during production. While single-modalityapproaches using either 3D point clouds or 2D RGB images suffer frominformation incompleteness, multimodal anomaly detection offers promise throughthe complementary fusion of crossmodal data. However, existing methods facechallenges in effectively integrating unimodal results and improvingdiscriminative power. To address these limitations, we first reinterpret memorybank-based anomaly scores in single modalities as isotropic Euclidean distancesin local feature spaces. Dynamically evolving from Eulidean metrics, we proposea novel \underline{G}eometry-\underline{G}uided \underline{S}core\underline{F}usion (G$^{2}$SF) framework that progressively learns ananisotropic local distance metric as a unified score for the fusion task.Through a geometric encoding operator, a novel Local Scale Prediction Network(LSPN) is proposed to predict direction-aware scaling factors that characterizefirst-order local feature distributions, thereby enhancing discriminationbetween normal and anomalous patterns. Additionally, we develop specializedloss functions and score aggregation strategy from geometric priors to ensureboth metric generalization and efficacy. Comprehensive evaluations on theMVTec-3D AD dataset demonstrate the state-of-the-art detection performance ofour method with low positive rate and better recall, which is essential inindustrial application, and detailed ablation analysis validates eachcomponent's contribution.</description>
      <author>example@mail.com (Chengyu Tao, Xuanming Cao, Juan Du)</author>
      <guid isPermaLink="false">2503.10091v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>JiSAM: Alleviate Labeling Burden and Corner Case Problems in Autonomous Driving via Minimal Real-World Data</title>
      <link>http://arxiv.org/abs/2503.08422v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;深度学习驱动的自动驾驶感知技术为安全和环保交通提供了前景，但由于激光雷达感知中对真实标注数据的高度依赖限制了实际道路测试规模。&lt;h4&gt;背景&lt;/h4&gt;在真实世界的数据集中生成带有罕见交通参与者等边缘案例的标记化激光雷达点云数据耗时且能量密集型。相比之下，在CARLA这样的模拟器中生成这些数据相对容易，但将合成点云用于改进现实感知却面临着两个挑战：样本效率低和仿真与现实之间的差距。&lt;h4&gt;目的&lt;/h4&gt;提出一种即插即用的方法来克服上述两大挑战，并利用模拟数据提高真实世界中的自动驾驶性能。&lt;h4&gt;方法&lt;/h4&gt;该团队提出了称为JiSAM的框架，其中包含抖动增强、领域感知骨干网络以及基于记忆的扇区对齐技术。此方法旨在通过结合仿真的灵活性和效率与真实的精确度来改进3D物体检测器的表现。&lt;h4&gt;主要发现&lt;/h4&gt;在著名的自动驾驶数据集NuScenes上进行的广泛实验表明，使用最先进的3D对象探测器，JiSAM能够在仅使用真实训练集中2.5%的数据标签的情况下，实现与全部使用真实数据训练模型相当的效果。此外，它还能在未标记的真实训练对象上获得超过15 mAPs。&lt;h4&gt;结论&lt;/h4&gt;这项研究证明了通过合理的设计和创新的方法，可以有效地利用仿真数据来提高自动驾驶系统的性能，并能够减少对昂贵且耗时的真实世界数据的需求。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的自主驾驶感知技术为安全、环保交通提供了有希望的画面。然而，在激光雷达感知中过度依赖真实标记的数据限制了道路上的实际尝试规模。3D现实世界数据很难标注，需要大量时间和能量，并缺乏像罕见交通参与者这样的边缘案例。相比之下，在如CARLA等模拟器中生成带有这些情况的合成标记化点云容易得多。但是，将合成点云引入以改进实际感知并非易事。这源于两个挑战：1）仿真数据集的样本效率低 2）仿真与真实之间的差距。为了克服这两个挑战，我们提出了一种称为JiSAM（抖动增强、领域感知骨干和基于记忆的扇区对齐方法）的即插即用方法。在著名的自动驾驶数据集NuScenes上进行的广泛实验表明，使用最先进的3D对象检测器，即使只利用真实数据2.5%的数据标签，JiSAM也能实现与所有真实数据训练模型相当的表现。此外，在未标记的真实训练集中，JiSAM还能获得超过15 mAPs的成绩。我们将发布模型和代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep-learning-based autonomous driving (AD) perception introduces a promisingpicture for safe and environment-friendly transportation. However, theover-reliance on real labeled data in LiDAR perception limits the scale ofon-road attempts. 3D real world data is notoriously time-and-energy-consumingto annotate and lacks corner cases like rare traffic participants. On thecontrary, in simulators like CARLA, generating labeled LiDAR point clouds withcorner cases is a piece of cake. However, introducing synthetic point clouds toimprove real perception is non-trivial. This stems from two challenges: 1)sample efficiency of simulation datasets 2) simulation-to-real gaps. Toovercome both challenges, we propose a plug-and-play method called JiSAM ,shorthand for Jittering augmentation, domain-aware backbone and memory-basedSectorized AlignMent. In extensive experiments conducted on the famous ADdataset NuScenes, we demonstrate that, with SOTA 3D object detector, JiSAM isable to utilize the simulation data and only labels on 2.5% available real datato achieve comparable performance to models trained on all real data.Additionally, JiSAM achieves more than 15 mAPs on the objects not labeled inthe real training set. We will release models and codes.</description>
      <author>example@mail.com (Runjian Chen, Wenqi Shao, Bo Zhang, Shaoshuai Shi, Li Jiang, Ping Luo)</author>
      <guid isPermaLink="false">2503.08422v2</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Fourier Decomposition for Explicit Representation of 3D Point Cloud Attributes</title>
      <link>http://arxiv.org/abs/2503.10055v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的点云编码方法，利用3D傅里叶分解来分离颜色和几何特征，并通过谱域操作扩展感受野。&lt;h4&gt;背景&lt;/h4&gt;尽管3D点云在各种视觉应用中被广泛使用，但由于其不规则性和稀疏性，处理这些点云存在挑战。现有的许多编码方法未能充分考虑包含色彩和几何属性的彩色点云。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以同时处理颜色和几何特征的点云编码方法，并通过谱域操作扩展感受野，以更好地捕获多点之间的关系。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于3D傅里叶分解的方法来分离颜色和几何特征。这种方法在频域中执行操作，使得每个属性可以独立学习和使用。此外，该方法利用了谱域的特性来自然地聚集局部特征，并考虑多个点的信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的编码方法能够有效地区分特征成分，在分类和风格迁移任务上都达到了最先进的性能，并且通过基于幅度的数据增强策略进一步提升了性能。&lt;h4&gt;结论&lt;/h4&gt;新的点云编码方法不仅分离了颜色和几何结构信息，还扩展了感受野以捕获更广泛的上下文信息。这种方法在处理复杂的彩色点云数据方面显示出强大的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：尽管3D点云被广泛应用于各种视觉应用中，但它们的不规则性和稀疏性使得难以处理。为应对这一挑战，已经提出了许多编码方法来捕获丰富的语义信息。然而，一个关键限制依然存在：没有充分考虑彩色点云，而彩色点云作为一种更强大的3D表示方式，包含颜色和几何属性等多种属性。现有方法通常在逐点的基础上分别处理这些属性，这导致了有限的感受野，并且难以捕捉多点之间的关系。为了解决这一问题，我们首创了一种点云编码方法，该方法利用三维傅里叶分解来分离颜色和几何特征，并通过频域操作扩展感受野。我们的分析表明，这种编码方法有效地将特征成分分离开了，其中幅度独特地捕获了颜色属性而相位则编码了几何结构信息，从而允许两种属性的独立学习和使用。此外，这些组件的谱特性自然聚合局部特征并考虑多点的信息。我们在点云分类和风格转换任务上验证了我们的方法，并在DensePoint数据集上实现了最先进的性能，通过一种基于幅度的数据增强策略进一步提升了性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While 3D point clouds are widely utilized across various vision applications,their irregular and sparse nature make them challenging to handle. In response,numerous encoding approaches have been proposed to capture the rich semanticinformation of point clouds. Yet, a critical limitation persists: a lack ofconsideration for colored point clouds which are more capable 3Drepresentations as they contain diverse attributes: color and geometry. Whileexisting methods handle these attributes separately on a per-point basis, thisleads to a limited receptive field and restricted ability to capturerelationships across multiple points. To address this, we pioneer a point cloudencoding methodology that leverages 3D Fourier decomposition to disentanglecolor and geometric features while extending the receptive field throughspectral-domain operations. Our analysis confirms that this encoding approacheffectively separates feature components, where the amplitude uniquely capturescolor attributes and the phase encodes geometric structure, thereby enablingindependent learning and utilization of both attributes. Furthermore, thespectral-domain properties of these components naturally aggregate localfeatures while considering multiple points' information. We validate our pointcloud encoding approach on point cloud classification and style transfer tasks,achieving state-of-the-art results on the DensePoint dataset with improvementsvia a proposed amplitude-based data augmentation strategy.</description>
      <author>example@mail.com (Donghyun Kim, Hyunah Ko, Chanyoung Kim, Seong Jae Hwang)</author>
      <guid isPermaLink="false">2503.10055v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Detect Objects from Multi-Agent LiDAR Scans without Manual Labels</title>
      <link>http://arxiv.org/abs/2503.08421v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文提出了一种新的无监督3D物体检测方法DOtA，通过多智能体协作数据集来改进伪标签的质量。&lt;h4&gt;背景&lt;/h4&gt;现有的无监督3D物体检测技术由于数据稀疏性和视角有限的问题，在生成高质量的伪标签方面面临挑战。引入多代理协同观测可以改善这一状况。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需外部标签的自学习方法，提高无监督3D物体检测的质量和效率。&lt;h4&gt;方法&lt;/h4&gt;{'初始化': '利用协作智能体内部共享的姿态（ego-pose）和形状（ego-shape）信息来初始化检测器', '多尺度编码': '使用代理间的互补观测进行初步标签的多层次编码，从而解码出高质量和低质量的标签。', '反馈机制': '将生成的伪标签用作提示，以指导正确的特征学习过程，进一步提升无监督物体检测性能'}&lt;h4&gt;主要发现&lt;/h4&gt;{'实验结果': '在V2V4Real和OPV2V数据集上的大量实验表明，DOtA方法优于现有的无监督3D物体检测技术。', '协作感知框架有效性验证': '在各种协作感知框架中验证了DOtA标签的有效性'}&lt;h4&gt;结论&lt;/h4&gt;多智能体协同观测可以提高无监督3D物体检测的质量，并为解决数据稀疏性和视角限制的问题提供了一种有效的方法。&lt;h4&gt;翻译&lt;/h4&gt;Unsupervised 3D object detection serves as an important solution for offline 3D object annotation. However, due to data sparsity and limited views, the clustering-based label fitting in unsupervised object detection often generates low-quality pseudo-labels. A multi-agent collaborative dataset, which involves the sharing of complementary observations among agents, holds potential to overcome this bottleneck. In this paper, we introduce a novel unsupervised method called DOtA (Detect Objects from Multi-Agent LiDAR scans) that learns without external labels. DOtA first initializes the detector using internally shared ego-pose and ego-shape information from collaborative agents, leveraging the generalization performance of neural networks to infer preliminary labels. It then uses complementary observations between agents for multi-scale encoding on these initial labels, decoding high-quality and low-quality labels. These generated pseudo-labels are used as prompts to guide correct feature learning processes, enhancing unsupervised object detection performance. Extensive experiments on V2V4Real and OPV2V datasets demonstrate that DOtA outperforms state-of-the-art unsupervised 3D object detection methods, and its effectiveness under various collaborative perception frameworks is also validated.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised 3D object detection serves as an important solution for offline3D object annotation. However, due to the data sparsity and limited views, theclustering-based label fitting in unsupervised object detection often generateslow-quality pseudo-labels. Multi-agent collaborative dataset, which involvesthe sharing of complementary observations among agents, holds the potential tobreak through this bottleneck. In this paper, we introduce a novel unsupervisedmethod that learns to Detect Objects from Multi-Agent LiDAR scans, termed DOtA,without using labels from external. DOtA first uses the internally sharedego-pose and ego-shape of collaborative agents to initialize the detector,leveraging the generalization performance of neural networks to inferpreliminary labels. Subsequently,DOtA uses the complementary observationsbetween agents to perform multi-scale encoding on preliminary labels, thendecodes high-quality and low-quality labels. These labels are further used asprompts to guide a correct feature learning process, thereby enhancing theperformance of the unsupervised object detection task. Extensive experiments onthe V2V4Real and OPV2V datasets show that our DOtA outperforms state-of-the-artunsupervised 3D object detection methods. Additionally, we also validate theeffectiveness of the DOtA labels under various collaborative perceptionframeworks.The code is available at https://github.com/xmuqimingxia/DOtA.</description>
      <author>example@mail.com (Qiming Xia, Wenkai Lin, Haoen Xiang, Xun Huang, Siheng Chen, Zhen Dong, Cheng Wang, Chenglu Wen)</author>
      <guid isPermaLink="false">2503.08421v2</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Singular Value Fine-tuning for Few-Shot Class-Incremental Learning</title>
      <link>http://arxiv.org/abs/2503.10214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文研究了在Class-Incremental Learning (CIL)中的Few-shot CIL (FSCIL)问题，提出了一种新的方法SVFCL来应对有限样本导致的过拟合挑战。&lt;h4&gt;背景&lt;/h4&gt;现有文献主要集中在解决CIL中灾难性遗忘的问题，但对于如何处理FSCIL中存在的有限样本带来的过拟合问题关注较少。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，本文提出了一种新的方法SVFCL，并将其与现有的基于PEFT的方法进行了比较。&lt;h4&gt;方法&lt;/h4&gt;SVFCL通过应用奇异值分解来调整基础模型的权重，在每次任务中固定奇异向量并仅微调奇异值。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，SVFCL不仅有效地减轻了遗忘问题，还更有效解决了过拟合问题，并且显著减少了可训练参数的数量。&lt;h4&gt;结论&lt;/h4&gt;通过在四个基准数据集上的广泛实验证明了SVFCL的有效性。此外，论文计划在未来公开代码。&lt;h4&gt;翻译&lt;/h4&gt;类增量学习（CIL）的目标是在连续地加入新类别时防止对先前所学类别的灾难性遗忘。更具挑战性的Few-shot CIL (FSCIL)设定通过仅提供每个新类的有限数量样本进一步增加了难度，这不仅加大了标准CIL挑战的程度，还增加了过拟合的风险。尽管已经广泛研究了灾难性遗忘问题，但针对大型基础模型在FSCIL中的过拟合现象却未受到足够重视。为了填补这一空白，本文提出了用于FSCIL的奇异值微调（SVFCL）方法，并与现有适应基础模型以应对FSCIL的方法进行了比较，后者主要基于参数高效微调（PEFT）技术如提示调整和低秩适应（LoRA）。具体而言，SVFCL对基础模型权重应用奇异值分解，在每次任务中固定奇异向量的同时仅微调每个任务的奇异值。这种方法既简单又有效，不仅减轻了遗忘问题，还更有效地缓解了过拟合问题，并且显著减少了可训练参数的数量。在四个基准数据集上的广泛实验以及可视化和消融研究验证了SVFCL的有效性。代码将在未来公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Class-Incremental Learning (CIL) aims to prevent catastrophic forgetting ofpreviously learned classes while sequentially incorporating new ones. The morechallenging Few-shot CIL (FSCIL) setting further complicates this by providingonly a limited number of samples for each new class, increasing the risk ofoverfitting in addition to standard CIL challenges. While catastrophicforgetting has been extensively studied, overfitting in FSCIL, especially withlarge foundation models, has received less attention. To fill this gap, wepropose the Singular Value Fine-tuning for FSCIL (SVFCL) and compared it withexisting approaches for adapting foundation models to FSCIL, which primarilybuild on Parameter Efficient Fine-Tuning (PEFT) methods like prompt tuning andLow-Rank Adaptation (LoRA). Specifically, SVFCL applies singular valuedecomposition to the foundation model weights, keeping the singular vectorsfixed while fine-tuning the singular values for each task, and then mergingthem. This simple yet effective approach not only alleviates the forgettingproblem but also mitigates overfitting more effectively while significantlyreducing trainable parameters. Extensive experiments on four benchmarkdatasets, along with visualizations and ablation studies, validate theeffectiveness of SVFCL. The code will be made available.</description>
      <author>example@mail.com (Zhiwu Wang, Yichen Wu, Renzhen Wang, Haokun Lin, Quanziang Wang, Qian Zhao, Deyu Meng)</author>
      <guid isPermaLink="false">2503.10214v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Representation Retrieval Learning for Heterogeneous Data Integration</title>
      <link>http://arxiv.org/abs/2503.09494v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本论文提出了一种新的表示检索($R^2$)框架，该框架结合了表示学习模块和稀疏诱导的机器学习模型来应对大规模多模态数据集中复杂异质性的问题。&lt;h4&gt;背景&lt;/h4&gt;大数据时代带来了大规模、多模态的数据集，这些数据集为预测建模和科学发现提供了前所未有的机会。然而，这种数据往往表现出复杂的异质性，这可能妨碍现有预测算法的准确性。&lt;h4&gt;目的&lt;/h4&gt;为了应对这些问题，提出了一种新的表示检索框架，旨在提高预测模型在复杂环境下的性能。&lt;h4&gt;方法&lt;/h4&gt;该框架包括一个代表者（representer）和学习者（learner），前者进行表示学习，后者采用稀疏诱导的方法。此外，论文还引入了“整合性”这一概念，并提出了选择性整合惩罚(SIP)以提升这种特性。&lt;h4&gt;主要发现&lt;/h4&gt;$R^2$框架放松了多任务学习中的常规全共享假设，允许部分结构共享；SIP可以提高超额风险界收敛速度的理论证明以及广泛的模拟研究证实了该框架的实际性能优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;论文通过两个真实世界数据集的应用进一步验证了所提出框架的优势。此框架在处理多模态、大规模异质性数据方面展现出了显著优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要是对论文主要内容的概述，描述了一种新的表示检索($R^2$)框架用于解决大数据中预测建模的问题，并对其理论基础和实际应用效果进行了阐述。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of big data, large-scale, multi-modal datasets are increasinglyubiquitous, offering unprecedented opportunities for predictive modeling andscientific discovery. However, these datasets often exhibit complexheterogeneity, such as covariate shift, posterior drift, and missingmodalities, that can hinder the accuracy of existing prediction algorithms. Toaddress these challenges, we propose a novel Representation Retrieval ($R^2$)framework, which integrates a representation learning module (the representer)with a sparsity-induced machine learning model (the learner). Moreover, weintroduce the notion of "integrativeness" for representers, characterized bythe effective data sources used in learning representers, and propose aSelective Integration Penalty (SIP) to explicitly improve the property.Theoretically, we demonstrate that the $R^2$ framework relaxes the conventionalfull-sharing assumption in multi-task learning, allowing for partially sharedstructures, and that SIP can improve the convergence rate of the excess riskbound. Extensive simulation studies validate the empirical performance of ourframework, and applications to two real-world datasets further confirm itssuperiority over existing approaches.</description>
      <author>example@mail.com (Qi Xu, Annie Qu)</author>
      <guid isPermaLink="false">2503.09494v2</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Robustness Tokens: Towards Adversarial Robustness of Transformers</title>
      <link>http://arxiv.org/abs/2503.10191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for publication at the European  Conference on Computer Vision (ECCV), 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对变压器架构的新型防御机制，即Robustness Tokens，以增强预训练模型对抗白盒攻击的能力。&lt;h4&gt;背景&lt;/h4&gt;近年来，大型预训练基础模型被广泛用于各种任务。由于这些模型是公开可用的，使用它们作为下游任务的基础模型可能会使系统对利用相同公共模型进行的恶意攻击高度脆弱。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来增强基于变压器架构的模型对抗白盒攻击的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;引入Robustness Tokens，在不改变模型参数的情况下，通过微调少量私有令牌（token）来提高模型的鲁棒性。这种方法计算成本低且简单有效。&lt;h4&gt;主要发现&lt;/h4&gt;Robustness Tokens可以显著提升Vision Transformer模型对抗白盒攻击的能力，并保持原有的下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;提出的方法提供了一种新的途径，通过微调少量私有令牌而非调整整个模型参数来增强预训练基础模型的鲁棒性。这种方法在保留模型原有性能的同时，提高了模型的安全性。&lt;h4&gt;翻译&lt;/h4&gt;最近，大型预训练基础模型因其广泛的应用被机器学习从业者所采用。鉴于这些模型是公开可用的，依赖它们作为下游任务的基础模型可能会导致对使用相同公共模型创建的恶意攻击的高度脆弱性。在这项工作中，我们提出了一种专为变压器架构设计的新方法Robustness Tokens，在不改变模型参数的情况下通过微调少量私有令牌来增强对抗白盒攻击的能力，同时保持原有的下游任务性能不变。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-73202-7_7&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, large pre-trained foundation models have become widely adopted bymachine learning practitioners for a multitude of tasks. Given that such modelsare publicly available, relying on their use as backbone models for downstreamtasks might result in high vulnerability to adversarial attacks crafted withthe same public model. In this work, we propose Robustness Tokens, a novelapproach specific to the transformer architecture that fine-tunes a fewadditional private tokens with low computational requirements instead of tuningmodel parameters as done in traditional adversarial training. We show thatRobustness Tokens make Vision Transformer models significantly more robust towhite-box adversarial attacks while also retaining the original downstreamperformances.</description>
      <author>example@mail.com (Brian Pulfer, Yury Belousov, Slava Voloshynovskiy)</author>
      <guid isPermaLink="false">2503.10191v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>AgentDAO: Synthesis of Proposal Transactions Via Abstract DAO Semantics</title>
      <link>http://arxiv.org/abs/2503.10099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种多代理系统，利用大型语言模型和新颖的标签中心检索算法来自动将自然语言输入转换为可执行提案交易，简化了去中心化自治组织（DAO）内的治理提议启动过程。&lt;h4&gt;背景&lt;/h4&gt;尽管去中心化治理的趋势明显，例如加密货币和区块链技术已在多个国家广泛采用，但在DAO中发起治理提案仍然具有挑战性。当前的提案需要提供底层交易负载，这对广泛的社区参与构成了重大障碍。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，本文提出了一种新的多代理系统，该系统利用大型语言模型，并结合一种专有标签中心检索算法来自动化治理提案过程。&lt;h4&gt;方法&lt;/h4&gt;提出了DAOLang这种领域特定的语言，用于简化各种治理提案的规格说明。通过采用语义感知抽象用户输入的方式，在保证安全的同时大幅减少令牌需求。&lt;h4&gt;主要发现&lt;/h4&gt;初步评估显示，使用现有的基础模型（例如GPT-4）可以生成复杂类型的治理提案，表明了DAOLang在这一领域的潜力。&lt;h4&gt;结论&lt;/h4&gt;该多代理系统及其辅助技术为DAO内的广泛社区参与提供了强大的支持，并有助于简化和自动化复杂的治理流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While the trend of decentralized governance is obvious (cryptocurrencies andblockchains are widely adopted by multiple sovereign countries), initiatinggovernance proposals within Decentralized Autonomous Organizations (DAOs) isstill challenging, i.e., it requires providing a low-level transaction payload,therefore posing significant barriers to broad community participation. Toaddress these challenges, we propose a multi-agent system powered by LargeLanguage Models with a novel Label-Centric Retrieval algorithm to automate thetranslation from natural language inputs into executable proposal transactions.The system incorporates DAOLang, a Domain-Specific Language to simplify thespecification of various governance proposals. The key optimization achieved byDAOLang is a semantic-aware abstraction of user input that reliably securesproposal generation with a low level of token demand. A preliminary evaluationon real-world applications reflects the potential of DAOLang in terms ofgenerating complicated types of proposals with existing foundation models, e.g.GPT-4o.</description>
      <author>example@mail.com (Lin Ao, Han Liu, Huafeng Zhang)</author>
      <guid isPermaLink="false">2503.10099v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>A Conditional Point Cloud Diffusion Model for Deformable Liver Motion Tracking Via a Single Arbitrarily-Angled X-ray Projection</title>
      <link>http://arxiv.org/abs/2503.09978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于条件点云扩散模型的框架，用于从任意角度的单次X射线投影中准确和稳健地追踪肝脏运动。&lt;h4&gt;背景&lt;/h4&gt;使用单次X射线投影进行可变形肝脏运动跟踪可以实现实时运动监测及治疗干预。传统的技术可能难以处理不同患者间以及同一患者的不同时期数据变化，需要更为精准的技术方案。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于条件点云扩散模型的框架（PCD-Liver），用于从任意角度单次X射线投影中估计肝脏体积运动，并以此为基础精确定位肝肿瘤位置。&lt;h4&gt;方法&lt;/h4&gt;- 利用患者特定的解剖结构数据，采用刚性对齐模型估算整体移动。- 通过条件点云扩散模型进一步修正表面变形问题。- 基于单次X射线影像中的几何信息提取运动编码特征，并以此为条件迭代求解详细肝脏表面的可变形矢量场（DVFs）。&lt;h4&gt;主要发现&lt;/h4&gt;- 运动估计前后，肝脏点云运动估算的精度显著提高：RMSE从8.86mm降低到3.59mm；HD95从10.88mm下降至4.29mm；COM误差也由9.41mm减小到了3.45mm。- 在高度噪声条件下，该模型仍能保持稳定的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架为基于图像引导的放射治疗中可变形肝脏运动估计和肿瘤定位提供了一种准确且鲁棒的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deformable liver motion tracking using a single X-ray projection enablesreal-time motion monitoring and treatment intervention. We introduce aconditional point cloud diffusion model-based framework for accurate and robustliver motion tracking from arbitrarily angled single X-ray projections(PCD-Liver), which estimates volumetric liver motion by solving deformablevector fields (DVFs) of a prior liver surface point cloud based on a singleX-ray image. The model is patient-specific and consists of two main components:a rigid alignment model to estimate the liver's overall shifts and aconditional point cloud diffusion model that further corrects for liver surfacedeformations. Conditioned on motion-encoded features extracted from a singleX-ray projection via a geometry-informed feature pooling layer, the diffusionmodel iteratively solves detailed liver surface DVFs in a projectionangle-agnostic manner. The liver surface motion estimated by PCD-Liver servesas a boundary condition for a U-Net-based biomechanical model to infer internalliver motion and localize liver tumors. A dataset of ten liver cancer patientswas used for evaluation. The accuracy of liver point cloud motion estimationwas assessed using root mean square error (RMSE) and 95th-percentile Hausdorffdistance (HD95), while liver tumor localization error was quantified usingcenter-of-mass error (COME). The mean (standard deviation) RMSE, HD95, and COMEof the prior liver or tumor before motion estimation were 8.86(1.51) mm,10.88(2.56) mm, and 9.41(3.08) mm, respectively. After PCD-Liver motionestimation, the corresponding values improved to 3.59(0.28) mm, 4.29(0.62) mm,and 3.45(0.96) mm. Under highly noisy conditions, PCD-Liver maintained stableperformance. This study presents an accurate and robust framework fordeformable liver motion estimation and tumor localization in image-guidedradiotherapy.</description>
      <author>example@mail.com (Jiacheng Xie, Hua-Chieh Shao, Yunxiang Li, Shunyu Yan, Chenyang Shen, Jing Wang, You Zhang)</author>
      <guid isPermaLink="false">2503.09978v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Light-weighted foundation model for seismic data processing based on representative and non-redundant pre-training dataset</title>
      <link>http://arxiv.org/abs/2503.10092v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种轻量级地震处理基础模型范式 (SPFM)，旨在解决传统方法在大数据和大模型参数策略中的局限性。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉和遥感领域通常采用“大数据+大量模型参数”的模式，但在地震数据处理中面临获取难、公开数据集稀缺以及高计算成本等挑战。&lt;h4&gt;目的&lt;/h4&gt;通过数据工程和网络架构创新来克服传统方法的限制，并构建一个轻量级的解决方案以提高地震数据处理的泛化能力和可访问性。&lt;h4&gt;方法&lt;/h4&gt;{'数据增强': '采用收集公共领域数据及生成扩散模型 (GDM) 来增加更多的地震数据', '数据优化': '通过降维、聚类分析和分层抽样来减少冗余信息并保留关键的地震特征，从而构建综合性的数据集。', '网络架构设计': '引入选择性结构化状态空间模型（Mamba）结构，有效捕捉地震数据的整体特性，并缓解基于Transformer模型中的计算复杂度二次增长问题'}&lt;h4&gt;主要发现&lt;/h4&gt;提出的轻量级基础模型在噪声去除、插值、频带外推和分辨率增强等任务中优于传统方法。&lt;h4&gt;结论&lt;/h4&gt;轻量级范式为地震数据处理提供了解决方案，促进了该领域的泛化性和可访问性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the fields of computer vision (CV) and remote sensing (RS), foundationalmodels typically follow the "big data + large model parameters" paradigm.However, the application of this strategy in seismic data processing facesseveral challenges: seismic data is difficult to obtain and the scarcity ofpublicly available datasets make it difficult to construct large-scaledatasets. Additionally, the high computational cost associated with a largenumber of model parameters restricts widespread research in this domain.Therefore, we propose a lightweight seismic processing foundational modelparadigm (SPFM), which aims to overcome the limitations of traditional methodsby data engineering and network architecture innovation. Specifically, wepropose an innovative dataset construction strategy that generates more seismicdata by data augmentation techniques, including collecting publicly availablefield data and using generative diffusion models (GDM) for data enhancement.Furthermore, we optimize the data distribution by employing dimensionalityreduction, cluster analysis, and stratified sampling methods, reducingredundant information while preserving important seismic features, thusconstructing a comprehensive dataset. In terms of network architecture design,we introduce the selective structured state-space model (Mamba) structure,which effectively captures global features of seismic data and alleviates thequadratic growth of computational complexity inherent in Transformer-basedmodels, thereby improving computational efficiency. This model, pre-trainedwith only four A800 GPUs, outperforms traditional methods across multipletasks, including denoising, interpolation, frequency-band extrapolation, andresolution enhancement. The lightweight paradigm provides an solution forseismic data processing, advancing the generalization and accessibility ofseismic data processing.</description>
      <author>example@mail.com (Xintong Dong, Wenshuo Yu, Jun Lin, Zhenbo Guo, Hongzhou Wang, Jianhao Yang)</author>
      <guid isPermaLink="false">2503.10092v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>TGP: Two-modal occupancy prediction with 3D Gaussian and sparse points for 3D Environment Awareness</title>
      <link>http://arxiv.org/abs/2503.09941v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;3D语义占用预测在机器人和自动驾驶环境感知领域成为研究焦点，因为它能够提供更真实的几何感知，并且更好地与下游任务相结合。&lt;h4&gt;背景&lt;/h4&gt;现有的占据预测任务主要采用体素或点云基础的方法。体素方法因体素化过程而丢失空间信息；点云方法虽然保留了位置信息但难以表示体积结构细节。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于3D高斯集和稀疏点的双模态预测方法，以平衡空间位置与体积结构信息，并提高语义占据预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;采用Transformer架构，输入为3D高斯集、稀疏点及查询。通过多层Transformer结构增强查询和3D高斯集共同贡献于语义占用预测；一种自适应融合机制结合双模态语义输出以生成最终预测结果。此外，在每层动态细化点云。&lt;h4&gt;主要发现&lt;/h4&gt;实验在Occ3DnuScenes数据集上进行，基于IoU指标显示所提出的方法具有更好的性能表现。&lt;h4&gt;结论&lt;/h4&gt;该方法通过结合高斯集和稀疏点的优点，并利用Transformer架构实现双模态融合，有效提高了语义占据预测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;三维语义占用已成为机器人技术和自动驾驶环境感知领域的研究热点。由于其提供更真实的几何感知能力以及与下游任务更好地集成，本研究旨在通过采用3D高斯集合稀疏点的双模式预测方法解决现有问题，该方法不仅保持了空间位置信息还保留了体积结构细节，在Occ3DnuScenes数据集上的实验结果表明基于IoU指标优于其他方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D semantic occupancy has rapidly become a research focus in the fields ofrobotics and autonomous driving environment perception due to its ability toprovide more realistic geometric perception and its closer integration withdownstream tasks. By performing occupancy prediction of the 3D space in theenvironment, the ability and robustness of scene understanding can beeffectively improved. However, existing occupancy prediction tasks areprimarily modeled using voxel or point cloud-based approaches: voxel-basednetwork structures often suffer from the loss of spatial information due to thevoxelization process, while point cloud-based methods, although better atretaining spatial location information, face limitations in representingvolumetric structural details. To address this issue, we propose a dual-modalprediction method based on 3D Gaussian sets and sparse points, which balancesboth spatial location and volumetric structural information, achieving higheraccuracy in semantic occupancy prediction. Specifically, our method adopts aTransformer-based architecture, taking 3D Gaussian sets, sparse points, andqueries as inputs. Through the multi-layer structure of the Transformer, theenhanced queries and 3D Gaussian sets jointly contribute to the semanticoccupancy prediction, and an adaptive fusion mechanism integrates the semanticoutputs of both modalities to generate the final prediction results.Additionally, to further improve accuracy, we dynamically refine the pointcloud at each layer, allowing for more precise location information duringoccupancy prediction. We conducted experiments on the Occ3DnuScenes dataset,and the experimental results demonstrate superior performance of the proposedmethod on IoU based metrics.</description>
      <author>example@mail.com (Mu Chen, Wenyu Chen, Mingchuan Yang, Yuan Zhang, Tao Han, Xinchi Li, Yunlong Li, Huaici Zhao)</author>
      <guid isPermaLink="false">2503.09941v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Mamba Modeling for Survival Prediction (M4Survive): Adapting Joint Foundation Model Representations</title>
      <link>http://arxiv.org/abs/2503.10057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了M4Survive框架，该框架通过使用高效的适配器网络从多个医学成像模态中学习联合基础模型表示来预测癌症患者的生存率。&lt;h4&gt;背景&lt;/h4&gt;精确的肿瘤学生存预测需要整合不同的影像模态以捕捉复杂的肿瘤生物学相互作用。传统的单一模态方法常常无法利用放射和病理评估提供的互补见解。&lt;h4&gt;目的&lt;/h4&gt;介绍M4Survive框架，旨在通过动态融合基础模型仓库中的异构嵌入来优化生存风险估计，并且该框架可以在保持计算效率的同时实现有效的多模态学习。&lt;h4&gt;方法&lt;/h4&gt;使用Mamba基适配器在多模态医学成像数据上进行高效的学习和联合表示生成。这种方法能够从多个来源（如MedImageInsight，BiomedCLIP，Prov-GigaPath，UNI2-h）中动态融合异构嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明了M4Survive在基准数据集上的生存预测准确度超过了单一模态和传统的静态多模态基线方法。&lt;h4&gt;结论&lt;/h4&gt;这项工作强调了基础模型驱动的多模式融合在推进精准肿瘤学和预测分析中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;精确的癌症患者存活率预测需要整合各种影像模态，捕捉复杂多变的肿瘤生物学相互作用。传统单一影像类型的预测方法往往无法充分利用放射影像与病理评估提供的互补信息。研究团队提出了名为M4Survive的新框架，该框架采用高效的适配器网络从一个包含多种医学成像数据的基础模型库（例如MedImageInsight、BiomedCLIP等）中动态融合异构嵌入，生成优化后的生存风险估计隐变量空间。通过这种方法，研究人员成功地实现了高效多模态学习，并保持了计算效率，实验结果表明M4Survive框架在多种基准测试数据集上超过了单一模式以及传统静态多模态基线模型的预测准确度，展示出利用基础模型驱动的多模态融合技术，在精准肿瘤学和预测分析领域的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate survival prediction in oncology requires integrating diverse imagingmodalities to capture the complex interplay of tumor biology. Traditionalsingle-modality approaches often fail to leverage the complementary insightsprovided by radiological and pathological assessments. In this work, weintroduce M4Survive (Multi-Modal Mamba Modeling for Survival Prediction), anovel framework that learns joint foundation model representations usingefficient adapter networks. Our approach dynamically fuses heterogeneousembeddings from a foundation model repository (e.g., MedImageInsight,BiomedCLIP, Prov-GigaPath, UNI2-h), creating a correlated latent spaceoptimized for survival risk estimation. By leveraging Mamba-based adapters,M4Survive enables efficient multi-modal learning while preserving computationalefficiency. Experimental evaluations on benchmark datasets demonstrate that ourapproach outperforms both unimodal and traditional static multi-modal baselinesin survival prediction accuracy. This work underscores the potential offoundation model-driven multi-modal fusion in advancing precision oncology andpredictive analytics.</description>
      <author>example@mail.com (Ho Hin Lee, Alberto Santamaria-Pang, Jameson Merkov, Matthew Lungren, Ivan Tarapov)</author>
      <guid isPermaLink="false">2503.10057v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>VideoMerge: Towards Training-free Long Video Generation</title>
      <link>http://arxiv.org/abs/2503.09926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VideoMerge的训练免去方法，它可以通过合并由预训练文本到视频扩散模型生成的短视频来实现长视频生成。&lt;h4&gt;背景&lt;/h4&gt;基于扩散模型的方法在视频生成领域表现出了领先的质量，但这类模型通常需要大量数据和计算资源进行训练，并且难以适应未参与训练的不同长度视频。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法以解决长视频生成中的可扩展性和质量挑战问题。&lt;h4&gt;方法&lt;/h4&gt;通过利用预训练扩散模型的优势，引入VideoMerge来合并短视频片段并允许用户指定动态变化的视频时长。&lt;h4&gt;主要发现&lt;/h4&gt;提出的VideoMerge方法能够无缝地适应将由预训练文本到视频扩散模型生成的短视频进行合并，并且在保持原始表达力和一致性的前提下实现了对扩展持续时间和动态变化的支持。&lt;h4&gt;结论&lt;/h4&gt;通过协作运用正交策略，该方法解决了长视频生成任务中的平滑性、一致性和动态内容等挑战问题，从而达到更好的质量。&lt;h4&gt;翻译&lt;/h4&gt;摘要：长时间视频生成仍是计算机视觉领域的一项具有挑战性的主题。基于扩散的模型在一系列视频生成方法中因其迭代去噪程序而达到了顶尖的质量水平。然而，视频领域的内在复杂性使得训练此类扩散模型的成本极高，既涉及数据整理又耗资计算资源。此外，这些模型通常基于固定噪声张量来表示视频，这导致了空间和时间维度的预先确定性。尽管有几种高质量且开源的预训练视频扩散模型可联合图像与不同长度和分辨率的视频进行培训，但在推断阶段指定未包含在训练集中的视频长度一般不被推荐。因此，这些模型无法仅通过增加指定视频长度的方式直接生成较长的视频。除了可行性挑战外，长视频生成还面临质量问题：长期视频领域本身比短视频更为复杂，延长的时间带来了更大的变异性并要求长时间的一致性，从而增加了任务的整体难度。我们提出了VideoMerge，这是一种无需训练的方法，可以通过将由预训练文本到视频扩散模型生成的短视频无缝合并来实现这一点。我们的方法在保持原始表达力和一致性的同时，允许用户指定扩展持续时间和动态变化。通过利用预训练模型的优势，我们的方法运用协作式正交策略解决了平滑性、一致性和动态内容等挑战问题，从而实现了更高质量的视频生成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long video generation remains a challenging and compelling topic in computervision. Diffusion based models, among the various approaches to videogeneration, have achieved state of the art quality with their iterativedenoising procedures. However, the intrinsic complexity of the video domainrenders the training of such diffusion models exceedingly expensive in terms ofboth data curation and computational resources. Moreover, these modelstypically operate on a fixed noise tensor that represents the video, resultingin predetermined spatial and temporal dimensions. Although several high qualityopen-source pretrained video diffusion models, jointly trained on images andvideos of varying lengths and resolutions, are available, it is generally notrecommended to specify a video length at inference that was not included in thetraining set. Consequently, these models are not readily adaptable to thedirect generation of longer videos by merely increasing the specified videolength. In addition to feasibility challenges, long-video generation alsoencounters quality issues. The domain of long videos is inherently more complexthan that of short videos: extended durations introduce greater variability andnecessitate long-range temporal consistency, thereby increasing the overalldifficulty of the task. We propose VideoMerge, a training-free method that canbe seamlessly adapted to merge short videos generated by pretrainedtext-to-video diffusion model. Our approach preserves the model's originalexpressiveness and consistency while allowing for extended duration and dynamicvariation as specified by the user. By leveraging the strengths of pretrainedmodels, our method addresses challenges related to smoothness, consistency, anddynamic content through orthogonal strategies that operate collaboratively toachieve superior quality.</description>
      <author>example@mail.com (Siyang Zhang, Harry Yang, Ser-Nam Lim)</author>
      <guid isPermaLink="false">2503.09926v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Semantic Attribute Binding for Free-Lunch Color Control in Diffusion Models</title>
      <link>http://arxiv.org/abs/2503.09864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://hecoding.github.io/colorwave-page&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为ColorWave的新方法，该方法无需模型微调即可实现精确的RGB级别颜色控制。&lt;h4&gt;背景&lt;/h4&gt;现有的文本到图像扩散模型虽然在多种属性上提供了卓越的控制能力，但准确的颜色指定仍然是一个根本性的挑战。现有技术如ColorPeel需要个性化调整和额外优化。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需训练的新方法（ColorWave），以解决精确颜色指定问题，并保持预训练模型的生成能力和多样性。&lt;h4&gt;方法&lt;/h4&gt;通过系统地分析IP-Adapter中的交叉注意力机制，发现文本颜色描述符与参考图像特征之间的隐含绑定关系。利用这一洞察力，重新连接这些绑定以实现精确的颜色归属。&lt;h4&gt;主要发现&lt;/h4&gt;ColorWave能够在不牺牲预训练模型生成质量和多样性的前提下，实现准确的RGB级别颜色控制，并且在各种对象类别上优于先前方法。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的评估，证明了ColorWave为基于扩散的方法构建结构化、色彩一致的图像合成设定了新范例。&lt;h4&gt;翻译&lt;/h4&gt;近期文本到图像(T2I)扩散模型的进步使得对多种属性进行精确控制成为可能，但准确的颜色指定仍然是一个基本挑战。现有方法如ColorPeel依赖于模型个性化调整，并需要额外优化和限制了任意颜色规格的灵活性。在本文中，我们介绍了一种名为ColorWave的新方法，该方法无需微调即可实现扩散模型中的精确RGB级别颜色控制。通过系统分析IP-Adapter中的交叉注意力机制，发现文本颜色描述符与参考图像特征之间存在隐含绑定关系。利用这一洞察力，我们的方法重新连接这些绑定以强制执行精确的颜色归属，同时保持预训练模型的生成能力。该方法保持了生成质量和多样性，在准确性及适用性方面优于先前的方法，适用于各种对象类别。通过广泛的评估，证明ColorWave为基于扩散的方法构建结构化、色彩一致的图像合成设定了新范例。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in text-to-image (T2I) diffusion models have enabledremarkable control over various attributes, yet precise color specificationremains a fundamental challenge. Existing approaches, such as ColorPeel, relyon model personalization, requiring additional optimization and limitingflexibility in specifying arbitrary colors. In this work, we introduceColorWave, a novel training-free approach that achieves exact RGB-level colorcontrol in diffusion models without fine-tuning. By systematically analyzingthe cross-attention mechanisms within IP-Adapter, we uncover an implicitbinding between textual color descriptors and reference image features.Leveraging this insight, our method rewires these bindings to enforce precisecolor attribution while preserving the generative capabilities of pretrainedmodels. Our approach maintains generation quality and diversity, outperformingprior methods in accuracy and applicability across diverse object categories.Through extensive evaluations, we demonstrate that ColorWave establishes a newparadigm for structured, color-consistent diffusion-based image synthesis.</description>
      <author>example@mail.com (Héctor Laria, Alexandra Gomez-Villa, Jiang Qin, Muhammad Atif Butt, Bogdan Raducanu, Javier Vazquez-Corral, Joost van de Weijer, Kai Wang)</author>
      <guid isPermaLink="false">2503.09864v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Foundation X: Integrating Classification, Localization, and Segmentation through Lock-Release Pretraining Strategy for Chest X-ray Analysis</title>
      <link>http://arxiv.org/abs/2503.09860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by WACV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了nFoundation X，一种利用多任务、多样化专家注释数据集训练基础模型的端到端框架。&lt;h4&gt;背景&lt;/h4&gt;深度学习在医学影像诊断中的应用需要大量标注的数据。然而，在不同任务（如分类、定位和分割）中专家级注释的异质性带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从多任务多样化数据集中进行训练的基础模型，以增强医学影像分析的准确性和通用性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种Lock-Release预训练策略，结合学生-教师学习范式，使得模型能够在不特定于任何单一任务的情况下保留泛化知识并防止过拟合。利用11个胸部X光数据集进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;nFoundation X在跨数据集和跨任务的学习上表现出色，并且特别提高了器官定位和分割任务的性能。&lt;h4&gt;结论&lt;/h4&gt;通过广泛注释的使用，nFoundation X实现了显著的性能提升。所有代码和预训练模型公开可访问。&lt;h4&gt;翻译&lt;/h4&gt;开发稳健和多用途的深度学习模型对于提高医学影像中的诊断准确性以及指导临床干预至关重要，但需要大量的标注数据。利用多样化的专家级注释创建了大量的医学数据集，并通过聚合这些数据集来最大化数据利用率并解决标签不足的问题。为了应对任务异质性带来的挑战，提出了一种Lock-Release预训练策略与学生教师学习范式结合的方法，确保模型在保持所有任务通用知识的同时避免过度拟合任何单一任务。通过11个胸部X光数据集的实验验证了nFoundation X的有效性，并且所有代码和预训练模型公开可访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing robust and versatile deep-learning models is essential forenhancing diagnostic accuracy and guiding clinical interventions in medicalimaging, but it requires a large amount of annotated data. The advancement ofdeep learning has facilitated the creation of numerous medical datasets withdiverse expert-level annotations. Aggregating these datasets can maximize datautilization and address the inadequacy of labeled data. However, theheterogeneity of expert-level annotations across tasks such as classification,localization, and segmentation presents a significant challenge for learningfrom these datasets. To this end, we introduce nFoundation X, an end-to-endframework that utilizes diverse expert-level annotations from numerous publicdatasets to train a foundation model capable of multiple tasks includingclassification, localization, and segmentation. To address the challenges ofannotation and task heterogeneity, we propose a Lock-Release pretrainingstrategy to enhance the cyclic learning from multiple datasets, combined withthe student-teacher learning paradigm, ensuring the model retains generalknowledge for all tasks while preventing overfitting to any single task. Todemonstrate the effectiveness of Foundation X, we trained a model using 11chest X-ray datasets, covering annotations for classification, localization,and segmentation tasks. Our experimental results show that Foundation Xachieves notable performance gains through extensive annotation utilization,excels in cross-dataset and cross-task learning, and further enhancesperformance in organ localization and segmentation tasks. All code andpretrained models are publicly accessible athttps://github.com/jlianglab/Foundation_X.</description>
      <author>example@mail.com (Nahid Ul Islam, DongAo Ma, Jiaxuan Pang, Shivasakthi Senthil Velan, Michael Gotway, Jianming Liang)</author>
      <guid isPermaLink="false">2503.09860v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Isolated Channel Vision Transformers: From Single-Channel Pretraining to Multi-Channel Finetuning</title>
      <link>http://arxiv.org/abs/2503.09826v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文介绍了一种用于大规模多通道图像数据预训练的简单而有效的框架，即孤立通道视觉变换器（IC-ViT），展示了该方法在细胞显微成像和卫星成像上的优越性。&lt;h4&gt;背景&lt;/h4&gt;传统的Vision Transformers (ViTs)在标准RGB图像处理任务中取得了显著的成功。然而，在多通道图像数据(MCI)的领域应用(如医学影像和遥感)，直接使用ViT进行训练存在困难，因为MCI中的不同层来自不同的模态，直接训练会掩盖互补信息并影响性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种针对大规模MCI数据集的有效预训练框架，并探索其在多模式多通道任务上的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;将每个图像通道独立划分为patch块，实现跨通道和patch之间的依赖关系捕捉，通过在单一通道上进行预训练，然后对下游的多通道数据集进行微调来增强特征表示能力。&lt;h4&gt;主要发现&lt;/h4&gt;提出的IC-ViT框架比现有的通道适应方法平均提高了4-14个百分点的表现，并且其高效的训练方式使其适合作为大规模基础模型异构数据的预训练候选。&lt;h4&gt;结论&lt;/h4&gt;通过独立通道patch化的技术，IC-ViT展示了在多模态多通道图像处理中的优越性，为MCI任务提供了强大的特征表示能力。该框架具有广泛的适用性和高效性，是未来研究和应用的基础模型之一。&lt;h4&gt;翻译&lt;/h4&gt;视觉变换器（ViTs）在标准RGB图像处理中取得了显著的成就。然而，在如医学影像与遥感等多通道成像（MCI）数据上的应用仍具挑战性，因为这些数据往往包含由不同模态采集的不同层次的数据。直接在此类数据上训练ViT可能会掩盖互补信息并损害性能。本文介绍了一种简单而有效的用于大规模MCI数据集的预训练框架。我们的方法，孤立通道视觉变换器（IC-ViT），独立地将图像频道划分为patch块，并因此能够进行多模态多频道任务的预训练。我们证明了这种基于通道的方式处理是MCI的一个关键技术。更重要的是，可以在单一通道上对IC-ViT进行预训练然后在下游多通道数据集上微调它。此框架捕获patch间及通道间的依赖关系，并产生强大的特征表示。实验显示，在包括细胞显微成像（JUMP-CP和CHAMMI）和卫星图像处理（So2Sat-LCZ42）在内的各种任务和基准测试中，IC-ViT相较于现有的通道适应方法表现出4到14个百分点的性能改进。此外，其高效训练方式使其适合作为异构数据大规模基础模型预训练的候选方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Transformers (ViTs) have achieved remarkable success in standard RGBimage processing tasks. However, applying ViTs to multi-channel imaging (MCI)data, e.g., for medical and remote sensing applications, remains a challenge.In particular, MCI data often consist of layers acquired from differentmodalities. Directly training ViTs on such data can obscure complementaryinformation and impair the performance. In this paper, we introduce a simpleyet effective pretraining framework for large-scale MCI datasets. Our method,named Isolated Channel ViT (IC-ViT), patchifies image channels individually andthereby enables pretraining for multimodal multi-channel tasks. We show thatthis channel-wise patchifying is a key technique for MCI processing. Moreimportantly, one can pretrain the IC-ViT on single channels and finetune it ondownstream multi-channel datasets. This pretraining framework capturesdependencies between patches as well as channels and produces robust featurerepresentation. Experiments on various tasks and benchmarks, including JUMP-CPand CHAMMI for cell microscopy imaging, and So2Sat-LCZ42 for satellite imaging,show that the proposed IC-ViT delivers 4-14 percentage points of performanceimprovement over existing channel-adaptive approaches. Further, its efficienttraining makes it a suitable candidate for large-scale pretraining offoundation models on heterogeneous data.</description>
      <author>example@mail.com (Wenyi Lian, Joakim Lindblad, Patrick Micke, Nataša Sladoje)</author>
      <guid isPermaLink="false">2503.09826v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Difference Flows</title>
      <link>http://arxiv.org/abs/2503.09817v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文介绍了Temporal Difference Flows (TD-Flow) 方法，这是一种新的几何视界模型学习方法，通过利用概率路径上的贝尔曼方程结构以及流匹配技术来训练准确的预测模型。&lt;h4&gt;背景&lt;/h4&gt;传统的预测模型往往采用逐步展开世界模型的方法进行推断和规划，在这种策略下，小错误会快速累积。相比之下，几何视界模型(GHM)能够直接对未来状态做出预测，减少了此类累加性推理误差的影响。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，可以更准确地学习长时延的几何视界模型，同时避免在训练过程中由引导预测带来的负面影响。&lt;h4&gt;方法&lt;/h4&gt;引入Temporal Difference Flows (TD-Flow)，利用概率路径上的贝尔曼方程结构和流匹配技术来减少梯度变化，在比现有方法超过5倍的时间跨度上进行准确的学习。&lt;h4&gt;主要发现&lt;/h4&gt;理论方面，该论文确立了新的收敛结果，并且将TD-Flow的有效性归因于训练期间降低的梯度变化。实验验证显示TD-Flow在广泛的领域内表现优秀，特别是在政策评估等下游任务中。&lt;h4&gt;结论&lt;/h4&gt;通过与最近的行为基础模型结合进行策略规划，在预训练政策上取得了显著性能提升，这表明了其对于长期决策制定的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;预测未来模型是代理体推理和规划能力的基础。传统的世界模型学习方法在推断时会逐步展开，这种做法容易导致小错误快速累积。几何视界模型提供了一种替代策略，可以减少累加性推理误差的影响。虽然可以通过生成式类时差(Temporal Difference, TD)学习来方便地训练GHM，但现有方法在训练过程中受引导预测的负面影响较大，并且难以产生高质量的长时延预测。本文提出Temporal Difference Flows (TD-Flow)，它通过利用概率路径上的贝尔曼方程结构和流匹配技术，在超过5倍于先前方法的时间跨度上实现了准确的GHM学习，同时减少了训练过程中的梯度变化。实验结果证明了TD-Flow在一系列领域内的有效性和优越性，并且与行为基础模型结合后显示出巨大的性能提升潜力，对于长期决策制定具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predictive models of the future are fundamental for an agent's ability toreason and plan. A common strategy learns a world model and unrolls itstep-by-step at inference, where small errors can rapidly compound. GeometricHorizon Models (GHMs) offer a compelling alternative by directly makingpredictions of future states, avoiding cumulative inference errors. While GHMscan be conveniently learned by a generative analog to temporal difference (TD)learning, existing methods are negatively affected by bootstrapping predictionsat train time and struggle to generate high-quality predictions at longhorizons. This paper introduces Temporal Difference Flows (TD-Flow), whichleverages the structure of a novel Bellman equation on probability pathsalongside flow-matching techniques to learn accurate GHMs at over 5x thehorizon length of prior methods. Theoretically, we establish a new convergenceresult and primarily attribute TD-Flow's efficacy to reduced gradient varianceduring training. We further show that similar arguments can be extended todiffusion-based methods. Empirically, we validate TD-Flow across a diverse setof domains on both generative metrics and downstream tasks including policyevaluation. Moreover, integrating TD-Flow with recent behavior foundationmodels for planning over pre-trained policies demonstrates substantialperformance gains, underscoring its promise for long-horizon decision-making.</description>
      <author>example@mail.com (Jesse Farebrother, Matteo Pirotta, Andrea Tirinzoni, Rémi Munos, Alessandro Lazaric, Ahmed Touati)</author>
      <guid isPermaLink="false">2503.09817v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting semi-supervised learning in the era of foundation models</title>
      <link>http://arxiv.org/abs/2503.09707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了半监督学习（SSL）与视觉基础模型（VFMs）的交互作用，并提出了一种简单而有效的方法，以提高基于VFMs的视觉应用中的半监督学习性能。&lt;h4&gt;背景&lt;/h4&gt;在使用大量未标记数据和有限标签数据进行增强学习的过程中，当前不清楚如何使半监督学习方法与预先训练好的视觉基础模型（VFMs）相结合。为了填补这一空白，开发了新的SSL基准测试集，在这些测试集中冻结的VFM表现不佳。&lt;h4&gt;目的&lt;/h4&gt;系统性地评估代表性的SSL方法，并提出一种改进基于VFMs的半监督学习性能的方法。&lt;h4&gt;方法&lt;/h4&gt;参数高效微调（PEFT）仅使用标签数据进行训练，然后利用监督下的PEFT模型对未标记的数据生成伪标签，以供进一步训练。为了克服伪标签中噪声的问题，提出了多种PEFT方法和VFM骨干网络的集成策略来产生更稳健的伪标签。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用标签数据的参数高效微调（PEFT）往往能匹配半监督学习性能，甚至在不利用未标记数据的情况下也能达到类似效果。引入自训练概念，并通过多模型集成方式增强伪标签质量。&lt;h4&gt;结论&lt;/h4&gt;该研究验证了简单而强大的方法的有效性，为SSL与VFMs结合提供了可行的见解，也为未来基于基础模型的大规模半监督学习铺平道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：Semi-supervised learning (SSL) leverages abundant unlabeled data alongside limited labeled data to enhance learning. As vision foundation models (VFMs) increasingly serve as the backbone of vision applications, it remains unclear how SSL interacts with these pre-trained models. To address this gap, we develop new SSL benchmark datasets where frozen VFMs underperform and systematically evaluate representative SSL methods. We make a surprising observation: parameter-efficient fine-tuning (PEFT) using only labeled data often matches SSL performance, even without leveraging unlabeled data. This motivates us to revisit self-training, a conceptually simple SSL baseline, where we use the supervised PEFT model to pseudo-label unlabeled data for further training. To overcome the notorious issue of noisy pseudo-labels, we propose ensembling multiple PEFT approaches and VFM backbones to produce more robust pseudo-labels. Empirical results validate the effectiveness of this simple yet powerful approach, providing actionable insights into SSL with VFMs and paving the way for more scalable and practical semi-supervised learning in the era of foundation models.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semi-supervised learning (SSL) leverages abundant unlabeled data alongsidelimited labeled data to enhance learning. As vision foundation models (VFMs)increasingly serve as the backbone of vision applications, it remains unclearhow SSL interacts with these pre-trained models. To address this gap, wedevelop new SSL benchmark datasets where frozen VFMs underperform andsystematically evaluate representative SSL methods. We make a surprisingobservation: parameter-efficient fine-tuning (PEFT) using only labeled dataoften matches SSL performance, even without leveraging unlabeled data. Thismotivates us to revisit self-training, a conceptually simple SSL baseline,where we use the supervised PEFT model to pseudo-label unlabeled data forfurther training. To overcome the notorious issue of noisy pseudo-labels, wepropose ensembling multiple PEFT approaches and VFM backbones to produce morerobust pseudo-labels. Empirical results validate the effectiveness of thissimple yet powerful approach, providing actionable insights into SSL with VFMsand paving the way for more scalable and practical semi-supervised learning inthe era of foundation models.</description>
      <author>example@mail.com (Ping Zhang, Zheda Mai, Quang-Huy Nguyen, Wei-Lun Chao)</author>
      <guid isPermaLink="false">2503.09707v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Statistical Deficiency for Task Inclusion Estimation</title>
      <link>http://arxiv.org/abs/2503.05491v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于理论的设定，用于定义任务的概念，并从统计缺陷的角度计算两个任务之间的包含关系。提出了一个可操作的代理作为信息充足性的估计方法来评估任务间的程度。&lt;h4&gt;背景&lt;/h4&gt;在机器学习中，任务是衡量当前模型能力的最自然的对象。构建能够处理任何任务的一般模型的趋势正在增强，尽管迁移学习和多任务学习试图利用潜在的任务空间，但缺乏研究其结构的有效工具。&lt;h4&gt;目的&lt;/h4&gt;建立一个理论基础来定义任务概念，并提出一种估计两个任务间包含关系的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了信息充足性作为评估任务之间包含程度的代理变量，并使用合成数据验证了该方法的有效性。此外，还用这种方法重建了经典的NLP处理流程。&lt;h4&gt;主要发现&lt;/h4&gt;本文成功地开发了一种新的理论框架来分析和量化机器学习中的任务关系，这为理解多任务和迁移学习等领域的结构提供了新视角。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效地评估和描述两个任务之间的包含关系，并且有可能在更广泛的任务中使用这种方法来进行进一步的研究。&lt;h4&gt;翻译&lt;/h4&gt;任务是衡量当前模型能力的最自然的对象，在机器学习中处于核心地位。虽然构建处理任何类型任务的一般模型的趋势日益明显，但迁移学习和多任务学习缺乏有效工具来研究潜在任务空间的结构。本文提出了一种基于统计缺陷视角的任务定义理论框架，并设计了评估两个任务之间包含关系的方法——信息充足性代理变量，这种方法在合成数据集上进行了验证并成功地重构了传统的NLP处理流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tasks are central in machine learning, as they are the most natural objectsto assess the capabilities of current models. The trend is to build generalmodels able to address any task. Even though transfer learning and multitasklearning try to leverage the underlying task space, no well-founded tools areavailable to study its structure. This study proposes a theoretically groundedsetup to define the notion of task and to compute the {\bf inclusion} betweentwo tasks from a statistical deficiency point of view. We propose a tractableproxy as information sufficiency to estimate the degree of inclusion betweentasks, show its soundness on synthetic data, and use it to reconstructempirically the classic NLP pipeline.</description>
      <author>example@mail.com (Loïc Fosse, Frédéric Béchet, Benoît Favre, Géraldine Damnati, Gwénolé Lecorvé, Maxime Darrin, Philippe Formont, Pablo Piantanida)</author>
      <guid isPermaLink="false">2503.05491v2</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>LongProLIP: A Probabilistic Vision-Language Model with Long Context Text</title>
      <link>http://arxiv.org/abs/2503.08048v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a tiny paper at the 1st workshop of "Quantify Uncertainty  and Hallucination in Foundation Models: The Next Frontier in Reliable AI" at  ICLR 2025; code: https://github.com/naver-ai/prolip; models:  https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对Probabilistic Language-Image Pre-Training (ProLIP)模型的微调策略，使该模型能够处理更长的文本序列（如256个token），以解决现有模型在处理超过64个上下文长度时无法捕捉丰富上下文信息的问题。&lt;h4&gt;背景&lt;/h4&gt;Probabilistic Language-Image Pre-Training (ProLIP)方法已被提出用于解决视觉语言任务中的多重性问题，尽管它们在大规模概率表示学习方面取得了一定的成功，但这些模型仍然难以处理超过64个token长度的长文本上下文。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有Probabilistic Language-Image Pre-Training (ProLIP)模型无法处理更长文本的问题，并提高其对长文本序列的理解能力同时尽量减少微调过程中的负面影响，本文提出了一种新的策略。&lt;h4&gt;方法&lt;/h4&gt;通过在Urban-1k和DataComp评估套件上进行实验验证所提出的LongProLIP方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，LongProLIP可以有效地提高对长文本的理解能力，并且代码已公开发布于https://github.com/naver-ai/prolip&lt;h4&gt;结论&lt;/h4&gt;虽然LongProLIP提升了模型处理长文本的能力，但也观察到在长上下文理解（通过Urban-1k衡量）和零样本泛化能力（由DataComp评估套件中的数据集衡量）之间存在权衡。&lt;h4&gt;翻译&lt;/h4&gt;最近提出了Probabilistic Language-Image Pre-Training (ProLIP) 方法来解决视觉语言任务的多重性问题。尽管它们在大规模概率表示学习中取得了成功，但这些模型无法处理超过64个上下文长度的文本，这限制了它们捕捉长文本序列中的丰富上下文信息的能力。为了解决这个问题，本文提出了一种针对ProLIP模型微调策略，使其能够接受更长的文本（例如256个文本token）。在Urban-1k和DataComp评估套件上的实验结果表明，所提出的LongProLIP方法可以在最小化微调负面影响的同时提高对长上下文的理解能力。我们还观察到，在处理长上下文中理解能力和通用零样本泛化能力之间存在权衡（分别通过Urban-1k和DataComp的评估数据集衡量）。代码可访问https://github.com/naver-ai/prolip&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Probabilistic Language-Image Pre-Training (ProLIP) has beenproposed to tackle the multiplicity issue of vision-language (VL) tasks.Despite their success in probabilistic representation learning at a scale, theProLIP models cannot handle long context texts longer than 64 context length,which limits their ability to capture rich contextual information from longertext sequences. To address this issue, this paper proposes a fine-tuningstrategy for ProLIP to accept longer texts, e.g., 256 text tokens. Experimentalresults on Urban-1k and the DataComp evaluation suite show that the proposedLongProLIP recipe can improve understanding of long contexts while minimizingthe negative effect of fine-tuning.We also observe a trade-off between the longcontext understanding (measured by Urban-1k) and general zero-shot capability(measured by evaluation datasets by DataComp). Code is available athttps://github.com/naver-ai/prolip</description>
      <author>example@mail.com (Sanghyuk Chun, Sangdoo Yun)</author>
      <guid isPermaLink="false">2503.08048v2</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>WonderVerse: Extendable 3D Scene Generation with Video Generative Models</title>
      <link>http://arxiv.org/abs/2503.09160v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为WonderVerse的新框架，用于生成可扩展的3D场景。该框架利用视频生成基础模型中的世界级先验知识来创建沉浸式且几何一致性的3D环境，并提出了一种新的控制性3D场景扩展技术以及异常序列检测模块。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通常依赖于迭代深度估计和图像修复，这往往会导致几何失真和不一致性。因此，需要一种能够生成更高保真度的3D环境的方法。&lt;h4&gt;目的&lt;/h4&gt;提出WonderVerse框架以克服现有方法中的问题，并提供一种简单而有效的途径来生成可扩展且高度真实的3D场景。&lt;h4&gt;方法&lt;/h4&gt;1. WonderVerse利用视频生成基础模型中的世界级先验知识，2. 提出了一种控制性3D场景扩展技术，该技术可以显著增加生成环境的规模，3. 引入了一个新的异常序列检测模块，以通过摄像机轨迹解决在生成视频中存在的几何不一致性问题。&lt;h4&gt;主要发现&lt;/h4&gt;WonderVerse框架提供了一条优雅且简单的管道来产生可扩展和高度逼真的3D场景，并且实验表明其性能明显优于依赖更复杂架构的现有工作。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了利用基础模型的强大世界级先验知识生成高质量的3D环境的优势，为未来的研究方向提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了extit{WonderVerse}，这是一个用于生成可扩展3D场景的简单但有效的框架。不同于现有方法依赖于迭代深度估计和图像修复，这些方法通常会导致几何失真和不一致性，WonderVerse利用视频生成基础模型中嵌入的强大世界级先验知识来创建高度沉浸且几何一致性的3D环境。此外，我们提出了一种新的控制性3D场景扩展技术以显著增加生成环境的规模。同时，我们引入了一个异常序列检测模块，该模块使用摄像机轨迹解决了在生成视频中存在的几何不一致性问题。最后，WonderVerse与各种3D重建方法兼容，能够进行高效且高质量的生成。大量关于3D场景生成的实验表明，我们的WonderVerse利用简单而优雅的管道提供了可扩展且高度逼真的3D场景，并显著优于依赖更复杂架构的现有工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce \textit{WonderVerse}, a simple but effective framework forgenerating extendable 3D scenes. Unlike existing methods that rely on iterativedepth estimation and image inpainting, often leading to geometric distortionsand inconsistencies, WonderVerse leverages the powerful world-level priorsembedded within video generative foundation models to create highly immersiveand geometrically coherent 3D environments. Furthermore, we propose a newtechnique for controllable 3D scene extension to substantially increase thescale of the generated environments. Besides, we introduce a novel abnormalsequence detection module that utilizes camera trajectory to address geometricinconsistency in the generated videos. Finally, WonderVerse is compatible withvarious 3D reconstruction methods, allowing both efficient and high-qualitygeneration. Extensive experiments on 3D scene generation demonstrate that ourWonderVerse, with an elegant and simple pipeline, delivers extendable andhighly-realistic 3D scenes, markedly outperforming existing works that rely onmore complex architectures.</description>
      <author>example@mail.com (Hao Feng, Zhi Zuo, Jia-Hui Pan, Ka-Hei Hui, Yihua Shao, Qi Dou, Wei Xie, Zhengzhe Liu)</author>
      <guid isPermaLink="false">2503.09160v2</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model</title>
      <link>http://arxiv.org/abs/2503.10631v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为HybridVLA的统一框架，该框架融合了自回归和扩散策略的优点，以实现更强大的机器人操作控制。&lt;h4&gt;背景&lt;/h4&gt;现有的基于视觉语言的动作模型（VLA）存在一些局限性：自回归方法会中断动作连贯性；而仅依赖于提取特征的方法在推理能力上受到限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够无缝结合自回归和扩散策略优点的统一框架，以提高机器人的通用操作能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种协作训练方案，通过将扩散建模直接注入到下一个标记预测中来弥合生成差距。引入了一个协作动作集成机制，该机制能根据任务需求自适应地融合这两种预测。&lt;h4&gt;主要发现&lt;/h4&gt;HybridVLA框架在多种模拟和真实世界的任务上超越了现有的最佳方法，并且展示了在未见过的配置中的稳定操作能力。&lt;h4&gt;结论&lt;/h4&gt;HybridVLA不仅提升了现有模型的性能，还为未来的研究提供了一种新的策略融合方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in vision-language models (VLMs) for common-sensereasoning have led to the development of vision-language-action (VLA) models,enabling robots to perform generalized manipulation. Although existingautoregressive VLA methods leverage large-scale pretrained knowledge, theydisrupt the continuity of actions. Meanwhile, some VLA methods incorporate anadditional diffusion head to predict continuous actions, relying solely onVLM-extracted features, which limits their reasoning capabilities. In thispaper, we introduce HybridVLA, a unified framework that seamlessly integratesthe strengths of both autoregressive and diffusion policies within a singlelarge language model, rather than simply connecting them. To bridge thegeneration gap, a collaborative training recipe is proposed that injects thediffusion modeling directly into the next-token prediction. With this recipe,we find that these two forms of action prediction not only reinforce each otherbut also exhibit varying performance across different tasks. Therefore, wedesign a collaborative action ensemble mechanism that adaptively fuses thesetwo predictions, leading to more robust control. In experiments, HybridVLAoutperforms previous state-of-the-art VLA methods across various simulation andreal-world tasks, including both single-arm and dual-arm robots, whiledemonstrating stable manipulation in previously unseen configurations.</description>
      <author>example@mail.com (Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, Chengkai Hou, Mengdi Zhao, KC alex Zhou, Pheng-Ann Heng, Shanghang Zhang)</author>
      <guid isPermaLink="false">2503.10631v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>UniGoal: Towards Universal Zero-shot Goal-oriented Navigation</title>
      <link>http://arxiv.org/abs/2503.10630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种通用的零样本目标导向导航框架，该框架采用统一的图表示形式来处理不同类型的导航任务。&lt;h4&gt;背景&lt;/h4&gt;现有的零样本方法针对特定任务构建基于大型语言模型（LLM）的推理框架，在整体流程上差异较大，难以在不同类型的目标之间进行泛化。&lt;h4&gt;目的&lt;/h4&gt;旨在实现通用的零样本导航能力，通过引入统一的图表示形式来解决现有方法中的问题，并提出相应的策略以提高性能。&lt;h4&gt;方法&lt;/h4&gt;{'统一图表示': '将目标类别、实例图像和文本描述统一为图表示；将代理观察转换成在线维护的场景图。', '零匹配处理': '在无匹配的情况下，迭代搜索子目标图。', '部分匹配处理': '利用坐标投影和锚点对齐来推断目标位置。', '完美匹配处理': '应用场景图校正和目标验证以实现完全匹配。', '黑名单机制': '引入一种切换阶段的稳健机制'}&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验表明，提出的UniGoal框架在三个研究导航任务中取得了最先进的零样本性能，并且甚至超越了特定任务的零样本方法以及监督下的通用方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法不仅提高了现有零样本导航方法的性能和泛化能力，还为未来的零样本导航研究提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们提出了一种通用框架用于普遍的零样本目标导向导航。现有的零样本方法针对特定任务构建基于大型语言模型（LLM）的推理框架，在整体流程上差异较大，难以在不同类型的目标之间进行泛化。为了实现通用的零样本导航能力，我们提出了统一图表示形式来处理不同类型的导航任务。此外，将代理观察转换成在线维护的场景图。利用这种一致的场景和目标表示方式，相比纯文本保留了更多的结构信息，并能够借助LLM进行基于明确图表的推理。具体而言，在每个时间点上执行场景图与目标图之间的图匹配，并提出不同的策略根据不同的匹配状态生成长期探索目标。代理首先在零匹配的情况下迭代搜索子目标图；当部分匹配时，代理则使用坐标投影和锚点对齐推断目标位置；最后在完全匹配的情况下应用场景图校正以及目标验证。我们还引入了一种黑名单机制以实现阶段切换的稳健性。广泛的实验结果表明我们的UniGoal框架在三个研究导航任务上实现了最先进的零样本性能，并且甚至超越了特定任务的零样本方法及监督下的通用方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a general framework for universal zero-shotgoal-oriented navigation. Existing zero-shot methods build inference frameworkupon large language models (LLM) for specific tasks, which differs a lot inoverall pipeline and fails to generalize across different types of goal.Towards the aim of universal zero-shot navigation, we propose a uniform graphrepresentation to unify different goals, including object category, instanceimage and text description. We also convert the observation of agent into anonline maintained scene graph. With this consistent scene and goalrepresentation, we preserve most structural information compared with pure textand are able to leverage LLM for explicit graph-based reasoning. Specifically,we conduct graph matching between the scene graph and goal graph at each timeinstant and propose different strategies to generate long-term goal ofexploration according to different matching states. The agent first iterativelysearches subgraph of goal when zero-matched. With partial matching, the agentthen utilizes coordinate projection and anchor pair alignment to infer the goallocation. Finally scene graph correction and goal verification are applied forperfect matching. We also present a blacklist mechanism to enable robust switchbetween stages. Extensive experiments on several benchmarks show that ourUniGoal achieves state-of-the-art zero-shot performance on three studiednavigation tasks with a single model, even outperforming task-specificzero-shot methods and supervised universal methods.</description>
      <author>example@mail.com (Hang Yin, Xiuwei Xu, Lingqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu)</author>
      <guid isPermaLink="false">2503.10630v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models</title>
      <link>http://arxiv.org/abs/2503.10626v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种数据无关的方法，用于从2D生成视频中学习3D运动技能，并能够推广到非人类的非常规形态上。&lt;h4&gt;背景&lt;/h4&gt;在机器人学和角色模拟领域，获得物理合理的动作技能对于不同且不寻常的身体结构至关重要。然而，传统方法如强化学习需要大量的任务特异性和身体特定工程来设计奖励函数，而模仿学习依赖于高质量专家演示，获取非人类形态的演示非常困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需数据的方法来进行技能习得，该方法利用视频扩散模型生成各种生物的真实视频，并通过视觉变压器进行基于视频的比较。&lt;h4&gt;方法&lt;/h4&gt;引入了一种称为'无数据模仿学习'(NIL)的新方法。这种方法通过计算嵌入式视频之间的配对距离来引导模仿学习过程，并使用分割视频帧间的相似度作为指导奖励，以此从2D生成视频中习得3D运动技能。&lt;h4&gt;主要发现&lt;/h4&gt;'No-data Imitation Learning' (NIL) 在涉及独特身体配置的移动任务上表现出了优越性。特别是在类人机器人移动任务中，该方法优于基于三维动作捕捉数据训练的基本模型。&lt;h4&gt;结论&lt;/h4&gt;结果表明，通过利用生成视频模型来学习物理合理技能具有潜力，并且可以有效地用数据生成代替模仿学习中的数据收集。&lt;h4&gt;翻译&lt;/h4&gt;获得跨越不同和非常规形态的物理合理的运动技能（包括类人机器人、四足动物和动物）对于推进角色模拟和机器人技术至关重要。传统方法，如强化学习(RL) 是任务特异性和身体特定的，并且需要广泛的奖励函数工程，不具有良好的推广能力。模仿学习提供了一种替代方案，但它严重依赖于高质量专家演示，获取非人类形态的演示非常困难。视频扩散模型能够生成从人类到蚂蚁的各种生物的真实视频。利用这一功能，我们提出了一种无需数据的方法来习得技能，该方法通过2D生成视频学习3D运动技能，并具备推广到非常规和非人形式的能力。特别是在涉及独特身体配置的移动任务上，'无数据模仿学习'(NIL)表现出了优越性，在类人机器人移动任务中，其性能优于基于三维动作捕捉数据训练的基本模型。结果强调了利用生成视频模型进行物理合理技能学习的潜力，并且可以有效地用数据生成代替模仿学习中的数据收集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Acquiring physically plausible motor skills across diverse and unconventionalmorphologies-including humanoid robots, quadrupeds, and animals-is essentialfor advancing character simulation and robotics. Traditional methods, such asreinforcement learning (RL) are task- and body-specific, require extensivereward function engineering, and do not generalize well. Imitation learningoffers an alternative but relies heavily on high-quality expert demonstrations,which are difficult to obtain for non-human morphologies. Video diffusionmodels, on the other hand, are capable of generating realistic videos ofvarious morphologies, from humans to ants. Leveraging this capability, wepropose a data-independent approach for skill acquisition that learns 3D motorskills from 2D-generated videos, with generalization capability tounconventional and non-human forms. Specifically, we guide the imitationlearning process by leveraging vision transformers for video-based comparisonsby calculating pair-wise distance between video embeddings. Along withvideo-encoding distance, we also use a computed similarity between segmentedvideo frames as a guidance reward. We validate our method on locomotion tasksinvolving unique body configurations. In humanoid robot locomotion tasks, wedemonstrate that 'No-data Imitation Learning' (NIL) outperforms baselinestrained on 3D motion-capture data. Our results highlight the potential ofleveraging generative video models for physically plausible skill learning withdiverse morphologies, effectively replacing data collection with datageneration for imitation learning.</description>
      <author>example@mail.com (Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas Krause, Michael Black)</author>
      <guid isPermaLink="false">2503.10626v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for Driving Scenario Understanding</title>
      <link>http://arxiv.org/abs/2503.10621v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, 3 tables, github:  https://github.com/ayesha-ishaq/DriveLMM-o1&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的数据集和基准DriveLMM-o1，用于评估大型多模态模型在自动驾驶中的逐步视觉推理能力。&lt;h4&gt;背景&lt;/h4&gt;现有大规模多模态模型在视觉问答任务中表现出色，但面对需要复杂多步推理的自主驾驶场景时仍存在挑战。现有的VQA基准主要关注最终答案的准确性，而忽视了生成这些答案所需的推理过程。&lt;h4&gt;目的&lt;/h4&gt;提出一个新的数据集和基准DriveLMM-o1，以促进自动驾驶中的逐步视觉推理研究，并评估现有模型在这一领域的能力。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含超过18k训练样本和4k测试样本的新VQA数据集，涵盖了感知、预测和规划等方面的多样化问题。同时引入了一种通过推理数据集微调的大规模多模态模型，并对各种开源与闭源方法进行了系统性比较。&lt;h4&gt;主要发现&lt;/h4&gt;提出的模型在复杂的驾驶场景中表现出强大的性能，在最终答案准确性上比之前的最佳开源模型高出7.49%，并且推理分数提高了3.62%。&lt;h4&gt;结论&lt;/h4&gt;DriveLMM-o1数据集和基准为评估自动驾驶中的逐步视觉推理提供了有价值的资源，并促进了相关研究的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要全文的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While large multimodal models (LMMs) have demonstrated strong performanceacross various Visual Question Answering (VQA) tasks, certain challengesrequire complex multi-step reasoning to reach accurate answers. Oneparticularly challenging task is autonomous driving, which demands thoroughcognitive processing before decisions can be made. In this domain, a sequentialand interpretive understanding of visual cues is essential for effectiveperception, prediction, and planning. Nevertheless, common VQA benchmarks oftenfocus on the accuracy of the final answer while overlooking the reasoningprocess that enables the generation of accurate responses. Moreover, existingmethods lack a comprehensive framework for evaluating step-by-step reasoning inrealistic driving scenarios. To address this gap, we propose DriveLMM-o1, a newdataset and benchmark specifically designed to advance step-wise visualreasoning for autonomous driving. Our benchmark features over 18k VQA examplesin the training set and more than 4k in the test set, covering diversequestions on perception, prediction, and planning, each enriched withstep-by-step reasoning to ensure logical inference in autonomous drivingscenarios. We further introduce a large multimodal model that is fine-tuned onour reasoning dataset, demonstrating robust performance in complex drivingscenarios. In addition, we benchmark various open-source and closed-sourcemethods on our proposed dataset, systematically comparing their reasoningcapabilities for autonomous driving tasks. Our model achieves a +7.49% gain infinal answer accuracy, along with a 3.62% improvement in reasoning score overthe previous best open-source model. Our framework, dataset, and model areavailable at https://github.com/ayesha-ishaq/DriveLMM-o1.</description>
      <author>example@mail.com (Ayesha Ishaq, Jean Lahoud, Ketan More, Omkar Thawakar, Ritesh Thawkar, Dinura Dissanayake, Noor Ahsan, Yuhao Li, Fahad Shahbaz Khan, Hisham Cholakkal, Ivan Laptev, Rao Muhammad Anwer, Salman Khan)</author>
      <guid isPermaLink="false">2503.10621v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Towards Safe Path Tracking Using the Simplex Architecture</title>
      <link>http://arxiv.org/abs/2503.10559v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于Simplex架构的路径跟踪控制器，旨在解决机器人在复杂环境中的导航问题。&lt;h4&gt;背景&lt;/h4&gt;传统控制器如Regulated Pure Pursuit、Dynamic Window Approach和Model-Predictive Path Integral在适应动态条件时表现不佳。强化学习虽然具有适应性，但缺乏正式的安全保障。&lt;h4&gt;目的&lt;/h4&gt;为了提供既安全又稳定的路径跟踪解决方案，设计了一种结合了强化学习控制器的自适应性和高性能与高保证度控制器的安全性的新型控制架构。&lt;h4&gt;方法&lt;/h4&gt;基于Simplex架构设计了一个新的控制器。该控制器旨在通过融合两种不同类型的控制器来提高导航系统在复杂环境中的性能和安全性。&lt;h4&gt;主要发现&lt;/h4&gt;模拟测试以及初步现场试验结果表明，新提出的控制器能够在保持安全的同时达到与现有最佳方案相当的性能水平。&lt;h4&gt;结论&lt;/h4&gt;提出了一个新的控制架构，并展示了其在机器人路径跟踪任务中应用的可能性。该方法为解决适应性和安全性之间的权衡问题提供了一种新的途径。&lt;h4&gt;翻译&lt;/h4&gt;机器人在复杂环境中的导航需要既灵活又安全的控制器。传统的纯追踪、动态窗口和预测模型策略虽然可靠，但在处理动态变化条件时面临挑战。强化学习能够提高灵活性，但缺乏正式的安全保障机制。为此，我们提出一种基于Simplex架构的路径跟踪控制器，它结合了强化学习控制器的适应性和高性能与高保证度控制器的安全性和稳定性。该控制器在保持安全的同时达到了与现有最佳方法相当的性能水平，并通过模拟和初步现场测试验证了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot navigation in complex environments necessitates controllers that areadaptive and safe. Traditional controllers like Regulated Pure Pursuit, DynamicWindow Approach, and Model-Predictive Path Integral, while reliable, struggleto adapt to dynamic conditions. Reinforcement Learning offers adaptability butlacks formal safety guarantees. To address this, we propose a path trackingcontroller leveraging the Simplex architecture. It combines a ReinforcementLearning controller for adaptiveness and performance with a high-assurancecontroller providing safety and stability. Our contribution is twofold. Wefirstly discuss general stability and safety considerations for designingcontrollers using the Simplex architecture. Secondly, we present aSimplex-based path tracking controller. Our simulation results, supported bypreliminary in-field tests, demonstrate the controller's effectiveness inmaintaining safety while achieving comparable performance to state-of-the-artmethods.</description>
      <author>example@mail.com (Georg Jäger, Nils-Jonathan Friedrich, Hauke Petersen, Benjamin Noack)</author>
      <guid isPermaLink="false">2503.10559v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>NuExo: A Wearable Exoskeleton Covering all Upper Limb ROM for Outdoor Data Collection and Teleoperation of Humanoid Robots</title>
      <link>http://arxiv.org/abs/2503.10554v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种穿戴式外骨骼系统，旨在通过用户友好的沉浸式远程操作和多模态传感数据收集来解决现有系统在实现四个关键目标方面的不足。&lt;h4&gt;背景&lt;/h4&gt;从动作捕捉和遥控到机器人技能学习的发展成为提高具身智能的关键路径。现有的系统仍然面临着同时实现准确跟踪、人体工程学适应性、多功能数据采集以及轻便设计等四个目标的挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍一种能够克服现有技术局限性的新型穿戴式外骨骼系统，该系统具有良好的肩部运动适应性和自然上肢动作范围复制能力，并且适用于日常户外使用和各种人形机器人。&lt;h4&gt;方法&lt;/h4&gt;通过采用同步连杆和皮带传动的新颖肩关节机制、背包式的背负设计以及统一的远程操作框架来实现上述目标。此外，还开发了一套综合的数据采集系统，可以为多种不同的人形机器人集成多模态传感器数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验在不同的平台和用户之间验证了该外骨骼系统的运动范围和灵活性，并确认其在动态场景中的远程操作准确性和数据收集稳定性。&lt;h4&gt;结论&lt;/h4&gt;该论文提出了一种创新的穿戴式外骨骼系统，能够有效改善现有技术存在的问题并促进具身智能的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The evolution from motion capture and teleoperation to robot skill learninghas emerged as a hotspot and critical pathway for advancing embodiedintelligence. However, existing systems still face a persistent gap insimultaneously achieving four objectives: accurate tracking of full upper limbmovements over extended durations (Accuracy), ergonomic adaptation to humanbiomechanics (Comfort), versatile data collection (e.g., force data) andcompatibility with humanoid robots (Versatility), and lightweight design foroutdoor daily use (Convenience). We present a wearable exoskeleton system,incorporating user-friendly immersive teleoperation and multi-modal sensingcollection to bridge this gap. Due to the features of a novel shouldermechanism with synchronized linkage and timing belt transmission, this systemcan adapt well to compound shoulder movements and replicate 100% coverage ofnatural upper limb motion ranges. Weighing 5.2 kg, NuExo supports backpack-typeuse and can be conveniently applied in daily outdoor scenarios. Furthermore, wedevelop a unified intuitive teleoperation framework and a comprehensive datacollection system integrating multi-modal sensing for various humanoid robots.Experiments across distinct humanoid platforms and different users validate ourexoskeleton's superiority in motion range and flexibility, while confirming itsstability in data collection and teleoperation accuracy in dynamic scenarios.</description>
      <author>example@mail.com (Rui Zhong, Chuang Cheng, Junpeng Xu, Yantong Wei, Ce Guo, Daoxun Zhang, Wei Dai, Huimin Lu)</author>
      <guid isPermaLink="false">2503.10554v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2503.10546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website: http://kuda-dynamics.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;KUDA是一种结合动态学习和视觉提示的开放词汇机器人操作系统，利用关键点、视觉语言模型（VLM）和基于学习的动力学模型。&lt;h4&gt;背景&lt;/h4&gt;随着大型语言模型（LLMs）和视觉-语言模型（VLMs）的发展，开放式词汇机器操作系统的开发取得了显著进展。然而，许多现有的方法忽略了对象动态的重要性，这限制了它们在更复杂、动态任务中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于关键点的开放词汇机器人操作系统KUDA，以解决现有方法对对象动力学重视不足的问题，并增强其应对复杂任务的能力。&lt;h4&gt;方法&lt;/h4&gt;KUDA通过结合VLM和学习型神经动力模型，利用关键点进行动态学习和视觉提示。该系统可以将语言指令转换为抽象的关键点表示，这些表示能被转化成代价函数，再由动力学模型优化生成机器人轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;在一系列操作任务中进行了KUDA的评估，包括自由形式的语言指令、多对象交互以及可变形或颗粒状物体的操作。结果表明了该框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;提出了一种结合动态学习和视觉提示的开放词汇机器人操作系统KUDA，展示了其在处理复杂操作任务中的优势，并且项目页面可通过提供的链接访问。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已全部翻译为中文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of large language models (LLMs) andvision-language models (VLMs), significant progress has been made in developingopen-vocabulary robotic manipulation systems. However, many existing approachesoverlook the importance of object dynamics, limiting their applicability tomore complex, dynamic tasks. In this work, we introduce KUDA, anopen-vocabulary manipulation system that integrates dynamics learning andvisual prompting through keypoints, leveraging both VLMs and learning-basedneural dynamics models. Our key insight is that a keypoint-based targetspecification is simultaneously interpretable by VLMs and can be efficientlytranslated into cost functions for model-based planning. Given languageinstructions and visual observations, KUDA first assigns keypoints to the RGBimage and queries the VLM to generate target specifications. These abstractkeypoint-based representations are then converted into cost functions, whichare optimized using a learned dynamics model to produce robotic trajectories.We evaluate KUDA on a range of manipulation tasks, including free-form languageinstructions across diverse object categories, multi-object interactions, anddeformable or granular objects, demonstrating the effectiveness of ourframework. The project page is available at http://kuda-dynamics.github.io.</description>
      <author>example@mail.com (Zixian Liu, Mingtong Zhang, Yunzhu Li)</author>
      <guid isPermaLink="false">2503.10546v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Learning Robotic Policy with Imagined Transition: Mitigating the Trade-off between Robustness and Optimality</title>
      <link>http://arxiv.org/abs/2503.10484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个两阶段框架，通过整合策略学习与想象中的过渡来缓解在四足机器人运动中优化性能和鲁棒性之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;现有的四足行走学习范式通常依赖于广泛的域随机化以减轻仿真到现实的差距并增强稳健性。这涉及到使用大量的环境参数及传感器噪声训练策略，使它们能够可靠地处理不确定性。然而，在理想条件下实现最佳性能往往与应对最坏情况的需求相冲突。&lt;h4&gt;目的&lt;/h4&gt;通过引入想象中的过渡来解决优化性能和鲁棒性的权衡问题，从而提高学习政策在多样化和具有挑战性条件下的效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一个两阶段框架：该框架将策略学习与基于理想化设置的最优策略和动力学模型产生的想象中转换相结合。这个框架增强传统的强化学习（RL）方式，通过加入想象中的转换作为示范输入。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，这种方法显著减轻了域随机化对现有RL算法造成的负面影响，加速训练过程，减少分布内跟踪误差，并提高分布外的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;提出的两阶段框架提供了一种有效的方法来解决四足机器人学习中的优化与稳健性的权衡问题，能够改善模型在理想和非理想环境下的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing quadrupedal locomotion learning paradigms usually rely on extensivedomain randomization to alleviate the sim2real gap and enhance robustness. Ittrains policies with a wide range of environment parameters and sensor noisesto perform reliably under uncertainty. However, since optimal performance underideal conditions often conflicts with the need to handle worst-case scenarios,there is a trade-off between optimality and robustness. This trade-off forcesthe learned policy to prioritize stability in diverse and challengingconditions over efficiency and accuracy in ideal ones, leading to overlyconservative behaviors that sacrifice peak performance. In this paper, wepropose a two-stage framework that mitigates this trade-off by integratingpolicy learning with imagined transitions. This framework enhances theconventional reinforcement learning (RL) approach by incorporating imaginedtransitions as demonstrative inputs. These imagined transitions are derivedfrom an optimal policy and a dynamics model operating within an idealizedsetting. Our findings indicate that this approach significantly mitigates thedomain randomization-induced negative impact of existing RL algorithms. Itleads to accelerated training, reduced tracking errors within the distribution,and enhanced robustness outside the distribution.</description>
      <author>example@mail.com (Wei Xiao, Shangke Lyu, Zhefei Gong, Renjie Wang, Donglin Wang)</author>
      <guid isPermaLink="false">2503.10484v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning</title>
      <link>http://arxiv.org/abs/2503.10480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的学习框架Dual Preference Optimization (D$^2$PO)，该框架通过偏好学习同时优化状态预测和动作选择，旨在提升大型视觉语言模型(如LVLMs)在环境动态理解及任务规划方面的能力。&lt;h4&gt;背景&lt;/h4&gt;近年来，在大型视觉-语言模型(LVLMs)领域取得了显著进展，尤其是在具身任务规划方面。然而，LVLMs仍面临依赖约束与效率的基本挑战。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有方法的局限性，本文旨在开发一种新的学习框架来增强LVLM在具身任务规划中的能力。&lt;h4&gt;方法&lt;/h4&gt;提出的方法Dual Preference Optimization (D$^2$PO)通过偏好学习同时优化状态预测和动作选择。此外，还引入了一种基于树搜索机制的数据收集方式，用于自动化地探索环境而无需人工标注。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的D$^2$PO框架显著优于现有方法及GPT-4o在Qwen2-VL(7B)、LLaVA-1.6(7B)和LLaMA-3.2(11B)上的表现。&lt;h4&gt;结论&lt;/h4&gt;通过引入Dual Preference Optimization (D$^2$PO)，可以大幅提升LVLMs在具身任务规划中的性能，特别是在理解和适应环境动态方面。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in large vision-language models (LVLMs) have shown promisefor embodied task planning, yet they struggle with fundamental challenges likedependency constraints and efficiency. Existing approaches either solelyoptimize action selection or leverage world models during inference,overlooking the benefits of learning to model the world as a way to enhanceplanning capabilities. We propose Dual Preference Optimization (D$^2$PO), a newlearning framework that jointly optimizes state prediction and action selectionthrough preference learning, enabling LVLMs to understand environment dynamicsfor better planning. To automatically collect trajectories and stepwisepreference data without human annotation, we introduce a tree search mechanismfor extensive exploration via trial-and-error. Extensive experiments onVoTa-Bench demonstrate that our D$^2$PO-based method significantly outperformsexisting methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), andLLaMA-3.2 (11B), achieving superior task success rates with more efficientexecution paths.</description>
      <author>example@mail.com (Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai, Jinlan Fu, Xipeng Qiu)</author>
      <guid isPermaLink="false">2503.10480v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Stratified Topological Autonomy for Long-Range Coordination (STALC)</title>
      <link>http://arxiv.org/abs/2503.10475v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication.  arXiv admin note: text overlap with arXiv:2303.11966&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种用于多机器人团队在复杂和危险环境中实现长距离协调的方法，该方法称为层次化策略分层拓扑自主性（STALC）。&lt;h4&gt;背景&lt;/h4&gt;统一多机器人的协调与运动规划是复杂环境下的一个挑战问题。特别是在需要减少观察者可见性和提高安全性的场景中。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于动态拓扑图的方法，以优化多个机器人团队在危险环境中的导航路径，并通过混合整数编程（MIP）生成最优的多机器人计划。&lt;h4&gt;方法&lt;/h4&gt;使用动态权重变化的拓扑图来反映不同观察位置和机器人队形的影响。根据这些信息制定时间约束并利用混合整数规划技术产生最佳方案。&lt;h4&gt;主要发现&lt;/h4&gt;本研究提供了一种减少计算复杂度的方法，使多个机器人能够相互作用以实现共同目标，并且这种方法已经在模拟和硬件实验中得到了验证。&lt;h4&gt;结论&lt;/h4&gt;通过实施STALC方法，可以在森林和城市环境中有效减少机器人团队的可见性并确保安全。&lt;h4&gt;翻译&lt;/h4&gt;实现统一多机器人的协调与运动规划在复杂环境下是一个难题。本文提出了一种层次化的方法来解决长距离协同问题，称之为分层拓扑自主性（Stratified Topological Autonomy for Long-Range Coordination, STALC）。特别关注的是，在一个多机器人团队通过危险环境导航时，如何减少观察者的可见性和确保安全性的问题。核心方法是基于动态的拓扑图构建，其中边权重根据机器人在图中的位置动态变化。为了创建这样的动态拓扑图，我们评估了从一系列离散观察者位置（包括敌对和友好）看到机器人团队的可能性，并构造了一个其边缘权重依赖于对手的位置和机器人团队配置的拓扑图。然后依据机器人团队的状态，在边权值的变化上施加时间约束，并利用混合整数规划生成通过该图的最佳多机器人计划。可见性信息还指导自主堆栈中的较低层级，为整个机器人群体规划出最小可见性的路径。我们的方法提供了一种减少计算复杂度的方法，以供一个团队的机器人互动和协调来完成共同目标。我们在森林和城市环境下的模拟实验和硬件实验中展示了这种方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving unified multi-robot coordination and motion planning in complexenvironments is a challenging problem. In this paper, we present a hierarchicalapproach to long-range coordination, which we call Stratified TopologicalAutonomy for Long-Range Coordination (STALC). In particular, we look at theproblem of minimizing visibility to observers and maximizing safety with amulti-robot team navigating through a hazardous environment. At its core, ourapproach relies on the notion of a dynamic topological graph, where the edgeweights vary dynamically based on the locations of the robots in the graph. Tocreate this dynamic topological graph, we evaluate the visibility of the robotteam from a discrete set of observer locations (both adversarial and friendly),and construct a topological graph whose edge weights depend on both adversaryposition and robot team configuration. We then impose temporal constraints onthe evolution of those edge weights based on robot team state and useMixed-Integer Programming (MIP) to generate optimal multirobot plans throughthe graph. The visibility information also informs the lower layers of theautonomy stack to plan minimal visibility paths through the environment for theteam of robots. Our approach presents methods to reduce the computationalcomplexity for a team of robots that interact and coordinate across the team toaccomplish a common goal. We demonstrate our approach in simulated and hardwareexperiments in forested and urban environments.</description>
      <author>example@mail.com (Cora A. Dimmig, Adam Goertz, Adam Polevoy, Mark Gonzales, Kevin C. Wolfe, Bradley Woosley, John Rogers, Joseph Moore)</author>
      <guid isPermaLink="false">2503.10475v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Finetuning Generative Trajectory Model with Reinforcement Learning from Human Feedback</title>
      <link>http://arxiv.org/abs/2503.10434v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于人类反馈的微调框架TrajHF，用于改进生成式轨迹模型以适应多样的驾驶偏好。&lt;h4&gt;背景&lt;/h4&gt;自驾车在动态环境中需要生成类似于人的、能够适应环境变化的轨迹。虽然现有的生成模型表现出合成可行轨迹的能力，但在捕捉复杂的人类驾驶行为方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;设计TrajHF框架来解决数据集偏差和分布偏移带来的问题，并提高自动驾驶系统中路径规划与人类驾驶偏好的一致性。&lt;h4&gt;方法&lt;/h4&gt;引入了多条件去噪器和带有反馈的人工智能强化学习技术，用于细化生成的轨迹模型，以更好地反映人类偏好同时满足安全性和可行性约束。&lt;h4&gt;主要发现&lt;/h4&gt;通过NavSim基准测试，TrajHF达到了93.95%的PDMS评分，明显优于其他方法。该框架能够产生更加个性化和适应性强的路径规划解决方案。&lt;h4&gt;结论&lt;/h4&gt;TrajHF开创了一种新的个性化轨迹生成范式，在自动驾驶领域具有重要应用价值。&lt;h4&gt;翻译&lt;/h4&gt;生成类似人且可适应动态环境中的自驾车轨迹至关重要。尽管生成模型在合成可行轨迹方面表现出潜力，但由于数据集偏差和分布偏移问题，它们未能捕捉到人类驾驶风格的细微变化。为解决这些问题，我们提出了TrajHF框架——一个由人类反馈驱动的微调框架用于改进自动驾驶车辆的路径规划以匹配不同的驾驶偏好。该框架采用多条件去噪器以及基于人类反馈的人工智能强化学习方法来优化生成式轨迹模型，使其能够更准确地反映个体化驾驶习惯并保持安全性和可行性要求。在NavSim基准测试中，TrajHF实现了PDMS得分93.95%，显著超越了其他技术方法，确立了一种新颖的个性化、适应性强的轨迹生成范式于自动驾驶领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating human-like and adaptive trajectories is essential for autonomousdriving in dynamic environments. While generative models have shown promise insynthesizing feasible trajectories, they often fail to capture the nuancedvariability of human driving styles due to dataset biases and distributionalshifts. To address this, we introduce TrajHF, a human feedback-drivenfinetuning framework for generative trajectory models, designed to align motionplanning with diverse driving preferences. TrajHF incorporatesmulti-conditional denoiser and reinforcement learning with human feedback torefine multi-modal trajectory generation beyond conventional imitationlearning. This enables better alignment with human driving preferences whilemaintaining safety and feasibility constraints. TrajHF achieves PDMS of 93.95on NavSim benchmark, significantly exceeding other methods. TrajHF sets a newparadigm for personalized and adaptable trajectory generation in autonomousdriving.</description>
      <author>example@mail.com (Derun Li, Jianwei Ren, Yue Wang, Xin Wen, Pengxiang Li, Leimeng Xu, Kun Zhan, Zhongpu Xia, Peng Jia, Xianpeng Lang, Ningyi Xu, Hang Zhao)</author>
      <guid isPermaLink="false">2503.10434v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>A nonlinear real time capable motion cueing algorithm based on deep reinforcement learning</title>
      <link>http://arxiv.org/abs/2503.10419v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;在运动仿真中，运动提示算法用于运动模拟平台的轨迹规划。由于工作空间限制，直接复制参考轨迹是不可能的。&lt;h4&gt;目的&lt;/h4&gt;引入了一种基于深度强化学习的新方法进行运动提示，在充分利用6自由度运动模拟器的能力同时克服现有技术的局限性。&lt;h4&gt;方法&lt;/h4&gt;使用基于近端策略优化（Proximal Policy Optimization）和自动超参数优化的演员-评论家实现来训练DRL-MCA。这种方法考虑到了MSP的完整动力学模型。&lt;h4&gt;主要发现&lt;/h4&gt;新算法在6自由度设置下有效，能够生成符合所有系统约束的可行轨迹，并满足实时要求。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于深度强化学习的方法为运动提示提供了一种新的解决方案，其性能可以与现有方法相媲美甚至更优。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了针对具有高度非线性工作空间的串行机器人MSP设计的一种新颖的运动提示算法。该方法使用深度强化学习来克服传统方法在处理平台特定和非线性特性时遇到的问题，并展示了其实用性和高效性，特别是适用于真实世界的运动模拟器应用中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In motion simulation, motion cueing algorithms are used for the trajectoryplanning of the motion simulator platform, where workspace limitations preventdirect reproduction of reference trajectories. Strategies such as motionwashout, which return the platform to its center, are crucial in thesesettings. For serial robotic MSPs with highly nonlinear workspaces, it isessential to maximize the efficient utilization of the MSPs kinematic anddynamic capabilities. Traditional approaches, including classical washoutfiltering and linear model predictive control, fail to considerplatform-specific, nonlinear properties, while nonlinear model predictivecontrol, though comprehensive, imposes high computational demands that hinderreal-time, pilot-in-the-loop application without further simplification. Toovercome these limitations, we introduce a novel approach using deepreinforcement learning for motion cueing, demonstrated here for the first timein a 6-degree-of-freedom setting with full consideration of the MSPs kinematicnonlinearities. Previous work by the authors successfully demonstrated theapplication of DRL to a simplified 2-DOF setup, which did not considerkinematic or dynamic constraints. This approach has been extended to all 6 DOFby incorporating a complete kinematic model of the MSP into the algorithm, acrucial step for enabling its application on a real motion simulator. Thetraining of the DRL-MCA is based on Proximal Policy Optimization in anactor-critic implementation combined with an automated hyperparameteroptimization. After detailing the necessary training framework and thealgorithm itself, we provide a comprehensive validation, demonstrating that theDRL MCA achieves competitive performance against established algorithms.Moreover, it generates feasible trajectories by respecting all systemconstraints and meets all real-time requirements with low...</description>
      <author>example@mail.com (Hendrik Scheidel, Camilo Gonzalez, Houshyar Asadi, Tobias Bellmann, Andreas Seefried, Shady Mohamed, Saeid Nahavandi)</author>
      <guid isPermaLink="false">2503.10419v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Compliant Control of Quadruped Robots for Assistive Load Carrying</title>
      <link>http://arxiv.org/abs/2503.10401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 20 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用四足机器人进行辅助负载运输的新方法，该方法通过控制机器人的加速度来实现精确的负载搬运。&lt;h4&gt;背景&lt;/h4&gt;当前四足机器人在辅助任务执行方面有广泛的应用前景，特别是在工业和搜救等场景下。然而，在复杂环境中的稳定性和适应性仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种新的控制器框架以提高四足机器人的运动性能及其在各种负载条件下的稳定性，并确保其与协作代理（例如工作人员）的安全互动。&lt;h4&gt;方法&lt;/h4&gt;所提出的控制器利用本体感觉传感器数据来估算外部基础转矩，采用顺应控制和基于控制屏障函数(CBF)的二次规划(QP)相结合的方法进行机器人加速度控制。此外，CBF还保证了机器人在移动过程中能够避免与协作代理发生碰撞。&lt;h4&gt;主要发现&lt;/h4&gt;该控制器通过物理硬件实施以及数值仿真验证了其有效性和鲁棒性，证明它能够在不同负载条件和环境中保持一致的性能并拒绝外界扰动。&lt;h4&gt;结论&lt;/h4&gt;所提出的控制框架显著增强了四足机器人执行辅助任务的能力，在工业应用、搜救行动等多个领域展示了巨大的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文是描述一种利用四足机器人进行辅助负载搬运的方法，该方法通过精确控制机器人的加速度来提升其在各种应用场景下的性能和稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel method for assistive load carrying usingquadruped robots. The controller uses proprioceptive sensor data to estimateexternal base wrench, that is used for precise control of the robot'sacceleration during payload transport. The acceleration is controlled using acombination of admittance control and Control Barrier Function (CBF) basedquadratic program (QP). The proposed controller rejects disturbances andmaintains consistent performance under varying load conditions. Additionally,the built-in CBF guarantees collision avoidance with the collaborative agent infront of the robot. The efficacy of the overall controller is shown by itsimplementation on the physical hardware as well as numerical simulations. Theproposed control framework aims to enhance the quadruped robot's ability toperform assistive tasks in various scenarios, from industrial applications tosearch and rescue operations.</description>
      <author>example@mail.com (Nimesh Khandelwal, Amritanshu Manu, Shakti S. Gupta, Mangal Kothari, Prashanth Krishnamurthy, Farshad Khorrami)</author>
      <guid isPermaLink="false">2503.10401v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>LUMOS: Language-Conditioned Imitation Learning with World Models</title>
      <link>http://arxiv.org/abs/2503.10370v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 2025 IEEE International Conference on Robotics and  Automation (ICRA)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LUMOS是一种语言条件下的多任务模仿学习框架，用于机器人领域。该框架通过在已学世界模型的潜在空间中进行长时间序列的练习来学习技能，并将这些技能零样本迁移到真实机器人上。&lt;h4&gt;背景&lt;/h4&gt;现有的离线模仿学习方法大多受制于由策略引起的分布偏移问题，这限制了算法的表现和稳定性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架LUMOS，旨在克服现有技术的局限性，通过在潜在空间中进行在线训练来减轻政策诱导的分布变化，并支持语言指令控制。&lt;h4&gt;方法&lt;/h4&gt;1. LUMOS利用无结构的游戏数据学习技能，同时使用少量的语言注释（少于1%）作为引导。2. 结合潜在规划和基于图像及语言的目标重新标注技术，在训练过程中优化内在奖励以减少协变量偏移。3. 通过结合潜意识空间内的内在回报来解决长时间序列任务的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的CALVIN基准测试中，LUMOS优于其他学习方法，并且是首个能够在离线世界模型内为真实机器人学习语言条件下的连续视觉-运动控制的研究。&lt;h4&gt;结论&lt;/h4&gt;LUMOS展示了其处理复杂多任务场景的能力和潜力，特别是对于长时序的视觉运动控制任务具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了LUMOS，这是一种语言条件下的多任务模仿学习框架用于机器人技术。该框架通过在已学世界模型的潜在空间中进行长时间序列的学习来习得技能，并能够将这些技能零样本迁移到实际操作中的机器人上。通过在学习的世界模型潜意识空间内执行在线训练，我们的方法减轻了大多数离线模仿学习方法所面临的由策略引起的分布偏移问题。LUMOS可以从未经结构化的游戏数据中学习，其中仅有少于1%的回顾性语言注释，并且能够在测试时接受自然语言指令进行控制。通过结合潜在规划及基于图像和文本的目标重新标注技术并优化潜意识空间内定义的内在回报，在多个时间步骤上实现了连贯的长时间性能，有效地减少了协变量偏移。在具有挑战性的CALVIN长期任务基准测试中，LUMOS优于其他学习方法，特别是当以类似方式评估多任务时。据我们所知，这是首次在一个离线世界模型内为实际机器人学习语言条件下的连续视觉-运动控制的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce LUMOS, a language-conditioned multi-task imitation learningframework for robotics. LUMOS learns skills by practicing them over manylong-horizon rollouts in the latent space of a learned world model andtransfers these skills zero-shot to a real robot. By learning on-policy in thelatent space of the learned world model, our algorithm mitigates policy-induceddistribution shift which most offline imitation learning methods suffer from.LUMOS learns from unstructured play data with fewer than 1% hindsight languageannotations but is steerable with language commands at test time. We achievethis coherent long-horizon performance by combining latent planning with bothimage- and language-based hindsight goal relabeling during training, and byoptimizing an intrinsic reward defined in the latent space of the world modelover multiple time steps, effectively reducing covariate shift. In experimentson the difficult long-horizon CALVIN benchmark, LUMOS outperforms priorlearning-based methods with comparable approaches on chained multi-taskevaluations. To the best of our knowledge, we are the first to learn alanguage-conditioned continuous visuomotor control for a real-world robotwithin an offline world model. Videos, dataset and code are available athttp://lumos.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Iman Nematollahi, Branton DeMoss, Akshay L Chandra, Nick Hawes, Wolfram Burgard, Ingmar Posner)</author>
      <guid isPermaLink="false">2503.10370v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Autonomous Robotic Radio Source Localization via a Novel Gaussian Mixture Filtering Approach</title>
      <link>http://arxiv.org/abs/2503.10349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种新的高斯混合滤波器（GMF），旨在提高自主机器人在未知环境中搜索和定位无线电信号源的估计性能。首先，该滤波器在一个基准数值问题上进行测试，以与其他现有方法如粒子高斯混合（PGM）滤波器和粒子滤波器（PF）进行比较并验证其性能。然后，在真实世界的机器人现场实验中测试并对比所提出的方法与PF和PGM滤波器的效果，以此来验证它在实际应用中的有效性。考虑的真实场景具有部分可观测性以及只提供距离测量，并且测量模型存在不确定性。结果表明，提出的滤波器能够有效处理这种部分可观测性，相比PF表现更好，同时减少了计算需求并展示了比其他技术更高的鲁棒性。&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的高斯混合滤波器（GMF）来提高自主机器人在未知环境中的无线电信号源搜索和定位性能。该方法首先通过一个基准数值问题进行了测试，并且在真实场景实验中验证了其相对于PF和PGM的优越性。&lt;h4&gt;背景&lt;/h4&gt;现有技术如粒子高斯混合滤波器（PGM）和粒子滤波器（PF）已经被用于解决自主机器人中的无线电信号源搜索和定位问题，但存在一定的局限性和计算负担。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的GMF，以提高估计性能并减少在未知环境下的计算需求，同时增强鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;首先通过一个基准数值问题测试新提出的滤波器，并对比PF和PGM；然后在具有部分可观测性的真实场景中进行现场实验来验证其性能。&lt;h4&gt;主要发现&lt;/h4&gt;新的GMF能够有效地处理观测中的不确定性，在减少计算需求的同时提高了定位精度，相比现有方法表现出更高的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;新提出的高斯混合滤波器（GMF）在提高估计性能方面比现有的PF和PGM更为有效，并且适用于实际的机器人应用环境。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study proposes a new Gaussian Mixture Filter (GMF) to improve theestimation performance for the autonomous robotic radio signal source searchand localization problem in unknown environments. The proposed filter is firsttested with a benchmark numerical problem to validate the performance withother state-of-practice approaches such as Particle Gaussian Mixture (PGM)filters and Particle Filter (PF). Then the proposed approach is tested andcompared against PF and PGM filters in real-world robotic field experiments tovalidate its impact for real-world robotic applications. The consideredreal-world scenarios have partial observability with the range-only measurementand uncertainty with the measurement model. The results show that the proposedfilter can handle this partial observability effectively whilst showingimproved performance compared to PF, reducing the computation requirementswhile demonstrating improved robustness over compared techniques.</description>
      <author>example@mail.com (Sukkeun Kim, Sangwoo Moon, Ivan Petrunin, Hyo-Sang Shin, Shehryar Khattak)</author>
      <guid isPermaLink="false">2503.10349v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>HALO: Fault-Tolerant Safety Architecture For High-Speed Autonomous Racing</title>
      <link>http://arxiv.org/abs/2503.10341v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HALO安全架构，该架构被应用在参加Indy自主挑战赛的全尺寸自动驾驶赛车上。它通过分析感知、规划、控制和通信模块中的故障模式及关键性来评估软件堆栈的安全性。&lt;h4&gt;背景&lt;/h4&gt;近年来，高速自动赛车领域取得了显著进展，例如RoboRace和Indy自主挑战赛等竞赛为研究人员提供了一个开发能够达到170英里/小时以上速度的自动驾驶赛车软件堆栈的平台。&lt;h4&gt;目的&lt;/h4&gt;确保这些车辆的安全需要软件在高速操作期间持续监测不同的故障及异常运行条件，以减少由子系统或组件故障带来的不合理风险。本文介绍了一种全面的安全架构HALO。&lt;h4&gt;方法&lt;/h4&gt;文章首先对感知、规划、控制和通信模块的软件堆栈进行了故障模式及关键性分析，并研究了三种类型的故障：节点健康、数据健康和行为安全故障。为缓解这些故障，论文介绍了HALO安全原型和运行时监控方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过多智能体场景下自主赛车车辆试验收集的真实世界数据证明了HALO安全架构在处理各种故障方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了一种全面的安全架构，对于高速自动驾驶赛车的开发具有重要意义，并且能够有效减少由于子系统或组件故障导致的风险。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The field of high-speed autonomous racing has seen significant advances inrecent years, with the rise of competitions such as RoboRace and the IndyAutonomous Challenge providing a platform for researchers to develop softwarestacks for autonomous race vehicles capable of reaching speeds in excess of 170mph. Ensuring the safety of these vehicles requires the software tocontinuously monitor for different faults and erroneous operating conditionsduring high-speed operation, with the goal of mitigating any unreasonable risksposed by malfunctions in sub-systems and components. This paper presents acomprehensive overview of the HALO safety architecture, which has beenimplemented on a full-scale autonomous racing vehicle as part of the IndyAutonomous Challenge. The paper begins with a failure mode and criticalityanalysis of the perception, planning, control, and communication modules of thesoftware stack. Specifically, we examine three different types of faults - nodehealth, data health, and behavioral-safety faults. To mitigate these faults,the paper then outlines HALO safety archetypes and runtime monitoring methods.Finally, the paper demonstrates the effectiveness of the HALO safetyarchitecture for each of the faults, through real-world data gathered fromautonomous racing vehicle trials during multi-agent scenarios.</description>
      <author>example@mail.com (Aron Harder, Amar Kulkarni, Madhur Behl)</author>
      <guid isPermaLink="false">2503.10341v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced View Planning for Robotic Harvesting: Tackling Occlusions with Imitation Learning</title>
      <link>http://arxiv.org/abs/2503.10334v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个基于模仿学习的观点规划方法，通过调整相机视角来捕捉目标作物的无遮挡图像。&lt;h4&gt;背景&lt;/h4&gt;在农业自动化中，摄像机视图被农作物遮挡是机器人收获的主要挑战。传统的方法和现有的基于学习的方法往往依赖于人工设计的评价指标或奖励函数，在复杂且未见过的情况下难以推广。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型模仿学习观点规划方法，以提高相机视角调整的效率和准确性，并在面对不同作物时具有更好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;采用带有Transformer的动作片段化（ACT）算法从专家演示中学习有效的相机运动策略。这种方法支持连续六自由度（6-DoF）视角调整，更加平滑、精确且能够揭示被遮挡的目标。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和实际环境中进行了广泛的实验，结果表明该方法的成功率和效率更高，特别是在复杂遮挡条件下，以及跨不同作物时的泛化能力。这种方法提供了一种实用的学习演示（LfD）解决方案来应对遮挡挑战，从而提高自主收获性能和生产力。&lt;h4&gt;结论&lt;/h4&gt;这项研究通过为机器人收获提供一种基于模仿学习的观点规划方法，有效地解决了摄像机视图被农作物遮挡的问题，提高了农业自动化中的收获效率和质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In agricultural automation, inherent occlusion presents a major challenge forrobotic harvesting. We propose a novel imitation learning-based viewpointplanning approach to actively adjust camera viewpoint and capture unobstructedimages of the target crop. Traditional viewpoint planners and existinglearning-based methods, depend on manually designed evaluation metrics orreward functions, often struggle to generalize to complex, unseen scenarios.Our method employs the Action Chunking with Transformer (ACT) algorithm tolearn effective camera motion policies from expert demonstrations. This enablescontinuous six-degree-of-freedom (6-DoF) viewpoint adjustments that aresmoother, more precise and reveal occluded targets. Extensive experiments inboth simulated and real-world environments, featuring agricultural scenariosand a 6-DoF robot arm equipped with an RGB-D camera, demonstrate our method'ssuperior success rate and efficiency, especially in complex occlusionconditions, as well as its ability to generalize across different crops withoutreprogramming. This study advances robotic harvesting by providing a practical"learn from demonstration" (LfD) solution to occlusion challenges, ultimatelyenhancing autonomous harvesting performance and productivity.</description>
      <author>example@mail.com (Lun Li, Hamidreza Kasaei)</author>
      <guid isPermaLink="false">2503.10334v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions</title>
      <link>http://arxiv.org/abs/2503.10331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://be2rlab.github.io/OSMa-Bench/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种用于评估开放语义映射(OSEM)解决方案的自动化流水线OSMa-Bench，该管道使用大型语言模型和视觉语言模型来评估不同光照条件下语义分割算法的表现。&lt;h4&gt;背景&lt;/h4&gt;开放式语义映射是机器人感知中的关键技术，结合了语义分割和SLAM技术。在不同的室内照明条件下评估最先进的语义映射算法是一个重要的挑战。&lt;h4&gt;目的&lt;/h4&gt;为了评价基于大型语言模型的语义映射算法，并提供关于这些模型鲁棒性的见解，以指导未来开发适应性强且可靠的机器人系统的研究方向。&lt;h4&gt;方法&lt;/h4&gt;提出了一种动态配置和高度自动化的LLM/LVLM驱动流水线OSMa-Bench。引入了一个新的数据集，包含模拟RGB-D序列及真实3D重建结果，用于在不同光照条件下评估映射性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验对领先模型（如ConceptGraphs, BBQ 和 OpenScene）的语义保真度进行了评价，并提出了场景图评估方法来分析模型解释语义结构的能力。这些研究提供了关于模型鲁棒性的见解，为未来开发适应性强和可靠的机器人系统奠定了基础。&lt;h4&gt;结论&lt;/h4&gt;该研究表明在不同室内照明条件下评估语义映射算法的重要性，并通过OSMa-Bench提供了一个全面的框架来推动这个领域的进一步发展。&lt;h4&gt;翻译&lt;/h4&gt;开放式语义映射是机器人感知中的关键技术，结合了语义分割和SLAM技术。本文介绍了一种动态配置且高度自动化的大型语言模型/视觉语言模型驱动流水线——OSMa-Bench（开放语义映射基准），用于评估开放式语义映射解决方案。研究的重点在于评价最先进的室内不同照明条件下的语义映射算法，这是室内环境中的一个关键挑战。我们引入了一个包含模拟RGB-D序列和真实3D重建结果的新数据集，以在不同的光照条件下进行严格的映射性能分析。通过实验对领先模型（如ConceptGraphs, BBQ 和 OpenScene）进行了语义保真度的评价，并提出了场景图评估方法来分析模型解释语义结构的能力。这些研究提供了关于模型鲁棒性的见解，为开发适应性强且可靠的机器人系统指明了未来的研究方向。我们的代码可在 https://be2rlab.github.io/OSMa-Bench/ 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open Semantic Mapping (OSM) is a key technology in robotic perception,combining semantic segmentation and SLAM techniques. This paper introduces adynamically configurable and highly automated LLM/LVLM-powered pipeline forevaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark).The study focuses on evaluating state-of-the-art semantic mapping algorithmsunder varying indoor lighting conditions, a critical challenge in indoorenvironments. We introduce a novel dataset with simulated RGB-D sequences andground truth 3D reconstructions, facilitating the rigorous analysis of mappingperformance across different lighting conditions. Through experiments onleading models such as ConceptGraphs, BBQ and OpenScene, we evaluate thesemantic fidelity of object recognition and segmentation. Additionally, weintroduce a Scene Graph evaluation method to analyze the ability of models tointerpret semantic structure. The results provide insights into the robustnessof these models, forming future research directions for developing resilientand adaptable robotic systems. Our code is available athttps://be2rlab.github.io/OSMa-Bench/.</description>
      <author>example@mail.com (Maxim Popov, Regina Kurkova, Mikhail Iumanov, Jaafar Mahmoud, Sergey Kolyubin)</author>
      <guid isPermaLink="false">2503.10331v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Some remarks on robustness of sample-and-hold stabilization</title>
      <link>http://arxiv.org/abs/2503.10328v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE Control Systems Letters; 8 pages, 5 figures, 4  tables (extended version)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了几种流行的一般实用稳定化技术对于系统干扰和测量噪声的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;这些流行的稳定化技术包括Dini瞄准、基于优化的稳定化以及inf-卷积稳定化。所有这些方法都明确使用了（一般非光滑）控制Lyapunov函数，这使得它们可以被视为Sontag公式的某种推广形式。&lt;h4&gt;目的&lt;/h4&gt;研究指出了一些上述鲁棒性特性在文献中尚未获得应有的关注，并提供了关于选择的流行稳定化技术的新见解和数学命题。&lt;h4&gt;方法&lt;/h4&gt;包括了通过广泛的统计案例研究对机器人停车问题进行了深入分析。&lt;h4&gt;主要发现&lt;/h4&gt;提供了一系列新的数学命题，这些命题详细阐述了某些选定稳定化技术的鲁棒性特性。&lt;h4&gt;结论&lt;/h4&gt;文章强调了一些先前未充分探讨的关于稳定化技术鲁棒性的细节，并且通过对一个具体的机器人停车场景进行案例研究来验证其理论分析。&lt;h4&gt;翻译&lt;/h4&gt;这项工作研究了几种流行的一般实用稳定化技术对系统干扰和测量噪声的鲁棒性。这些技术包括Dini瞄准、基于优化的稳定化以及inf-卷积稳定化。所有这些方法都明确使用了（一般非光滑）控制Lyapunov函数，这使得它们可以被视为Sontag公式的某种推广形式。研究指出了一些上述鲁棒性特性在文献中尚未获得应有的关注，并提供了关于选择的流行稳定化技术的新见解和数学命题，同时通过广泛的统计案例研究对机器人停车问题进行了深入分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LCSYS.2025.3546990&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work studies robustness to system disturbance and measurement noise ofsome popular general practical stabilization techniques, namely, Dini aiming,optimization-based stabilization and inf-convolution stabilization. Common toall these techniques is the explicit usage of a (general nonsmooth) controlLyapunov function, thus allowing to see them as a kind of generalization to thecelebrated Sontag's formula. It turns out that certain details of the abovedescribed robustness properties have not yet received the attention inliterature they deserved. We provide new remarks, formalized in mathematicalpropositions, on robustness of selected popular stabilization techniques alongwith an extensive statistical case study on a robot parking problem.</description>
      <author>example@mail.com (Patrick Schmidt, Pavel Osinenko, Stefan Streif)</author>
      <guid isPermaLink="false">2503.10328v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Towards Fast, Memory-based and Data-Efficient Vision-Language Policy</title>
      <link>http://arxiv.org/abs/2503.10322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了LiteVLP，一种轻量级、基于内存的通用视觉语言策略生成模型。通过在小型且对话风格的数据集上进行微调，该模型优于现有技术并展示了卓越的记忆能力。&lt;h4&gt;背景&lt;/h4&gt;大规模预训练的Vision Language Models (VLMs) 在转移知识到机器人学习方面显示出潜力，但面临高昂的推理成本、数据模态不匹配导致的领域偏移和处理过去或未来经验的能力有限等问题。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有范式中的三个关键挑战，提出了一种轻量级、基于内存的通用视觉语言策略生成模型LiteVLP。&lt;h4&gt;方法&lt;/h4&gt;构建于预训练的10亿参数VLM之上，并在小型且对话风格的数据集上进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验表明，LiteVLP在VIMA-Bench上优于现有的最佳性能基线模型，在最小化训练时间的同时表现出卓越的速度和高准确性。此外，它还展示了出色的长时操作任务的记忆能力。&lt;h4&gt;结论&lt;/h4&gt;结果强调了LiteVLP作为将VLM的智能集成到机器人学习中的有前途的模型的地位&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型（VLM）在互联网规模的视觉-语言数据上进行预训练，展示出将其知识转移到机器人学习中的潜力。然而，现有的范式遇到了三个关键挑战：由于大规模模型参数导致的成本高昂、由不匹配的数据模态引起的领域偏移以及处理过去或未来经验的能力有限。本文提出了LiteVLP，一个轻量级、基于内存和通用的视觉语言策略生成模型。该模型建立在预训练的10亿参数VLM上，并在小型且对话式的机器人数据集上进行微调。通过广泛的实验表明，LiteVLP优于现有的最佳视觉语言政策，在最小化训练时间的同时实现了卓越的速度和高准确性。此外，在长时间操作任务中，它还展示了出色的记忆能力，超过了最好的基线模型18.8%。这些结果强调了LiteVLP作为将VLM的智能集成到机器人学习中的有前途的模型的地位。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Language Models (VLMs) pretrained on Internet-scale vision-languagedata have demonstrated the potential to transfer their knowledge to roboticlearning. However, the existing paradigm encounters three critical challenges:(1) expensive inference cost resulting from large-scale model parameters, (2)frequent domain shifts caused by mismatched data modalities, and (3) limitedcapacity to handle past or future experiences. In this work, we proposeLiteVLP, a lightweight, memory-based, and general-purpose vision-languagepolicy generation model. LiteVLP is built upon a pre-trained 1B-parameter VLMand fine-tuned on a tiny-scale and conversation-style robotic dataset. Throughextensive experiments, we demonstrate that LiteVLP outperforms state-of-the-artvision-language policy on VIMA-Bench, with minimal training time. Furthermore,LiteVLP exhibits superior inference speed while maintaining exceptional highaccuracy. In long-horizon manipulation tasks, LiteVLP also shows remarkablememory ability, outperforming the best-performing baseline model by 18.8%.These results highlight LiteVLP as a promising model to integrating theintelligence of VLMs into robotic learning.</description>
      <author>example@mail.com (Haoxuan Li, Sixu Yan, Yuhan Li, Xinggang Wang)</author>
      <guid isPermaLink="false">2503.10322v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>6D Object Pose Tracking in Internet Videos for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2503.10307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICLR 2025. Project page available at  https://ponimatkin.github.io/wildpose/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;从网络教学视频中提取被操作物体的6D姿态轨迹是一项具有挑战性的任务，尤其是在没有先验知识和精确模型的情况下。为此，研究人员提出了一种新方法，可以估计输入图像中任何对象的6D姿态，并通过轨迹优化将其转换为机器人机械臂的操作空间。&lt;h4&gt;背景&lt;/h4&gt;当前的6D姿态估计方法在面对未受控制拍摄条件、物体细微动态运动以及被操作物体的确切网格模型未知的情况下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法来克服现有6D姿态估计技术在实际应用中的局限性，特别是在网络教学视频环境中。&lt;h4&gt;方法&lt;/h4&gt;{'第一点': '提出了一种新方法，该方法首先从大规模3D模型数据库中检索与输入图像中描绘的对象相似的CAD模型，然后将该模型与输入图像对齐，并确定对象相对于场景的绝对尺度。', '第二点': '通过仔细跟踪视频帧中的检测到的对象来提取平滑的6D物体轨迹。随后，使用轨迹优化将这些提取出的物体轨迹转换为机器人机械臂的操作空间配置。', '第三点': '在YCB-V和HOPE-Video数据集以及新的人工标注了近似6D对象轨迹的教学视频数据集上进行了彻底评估，并通过消融研究展示了方法的有效性。'}&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著优于现有的RGB 6D姿态估计技术。&lt;h4&gt;结论&lt;/h4&gt;从互联网视频中估计的6D物体运动可以转移到7轴机器人机械臂的操作环境中，无论是虚拟模拟器还是真实世界设置，并且该方法还能应用于第一人称视角（egocentric）视频，显示了其在具身人工智能应用中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We seek to extract a temporally consistent 6D pose trajectory of amanipulated object from an Internet instructional video. This is a challengingset-up for current 6D pose estimation methods due to uncontrolled capturingconditions, subtle but dynamic object motions, and the fact that the exact meshof the manipulated object is not known. To address these challenges, we presentthe following contributions. First, we develop a new method that estimates the6D pose of any object in the input image without prior knowledge of the objectitself. The method proceeds by (i) retrieving a CAD model similar to thedepicted object from a large-scale model database, (ii) 6D aligning theretrieved CAD model with the input image, and (iii) grounding the absolutescale of the object with respect to the scene. Second, we extract smooth 6Dobject trajectories from Internet videos by carefully tracking the detectedobjects across video frames. The extracted object trajectories are thenretargeted via trajectory optimization into the configuration space of arobotic manipulator. Third, we thoroughly evaluate and ablate our 6D poseestimation method on YCB-V and HOPE-Video datasets as well as a new dataset ofinstructional videos manually annotated with approximate 6D objecttrajectories. We demonstrate significant improvements over existingstate-of-the-art RGB 6D pose estimation methods. Finally, we show that the 6Dobject motion estimated from Internet videos can be transferred to a 7-axisrobotic manipulator both in a virtual simulator as well as in a real worldset-up. We also successfully apply our method to egocentric videos taken fromthe EPIC-KITCHENS dataset, demonstrating potential for Embodied AIapplications.</description>
      <author>example@mail.com (Georgy Ponimatkin, Martin Cífka, Tomáš Souček, Médéric Fourmy, Yann Labbé, Vladimir Petrik, Josef Sivic)</author>
      <guid isPermaLink="false">2503.10307v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2503.10296v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 33 images, IEEE Transactions on Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了移动机器人的设计集成挑战和策略，重点讨论如何通过任务驱动的硬件和软件最佳选择来平衡安全性、效率以及成本、能量、计算需求和重量等资源最小化使用的问题。&lt;h4&gt;背景&lt;/h4&gt;在移动机器人设计中，感知与运动规划之间的相互作用对于决策至关重要。引入占用查询的概念，以量化基于采样的方法进行路径规划时对感知的需求。&lt;h4&gt;目的&lt;/h4&gt;通过将感知需求与性能相结合，提出了一种整数线性编程（ILP）方法，用于高效地选择和放置传感器及算法，并为机器人本体、运动规划器、感知流水线和计算单元的协同设计优化奠定基础。&lt;h4&gt;方法&lt;/h4&gt;使用伪阴性和阳性率评估传感器和算法在不同几何关系、物体属性、传感器分辨率以及环境条件下的表现。基于上述方法，提出了一种称为CODEI（实体智能协同设计）的框架来解决移动机器人共设计问题。&lt;h4&gt;主要发现&lt;/h4&gt;针对城市场景中自主车辆开发的研究显示，复杂任务会增加资源需求，并且任务性能影响了自主系统堆栈的选择：对于低成本和轻量级的设计而言，摄像机是优选的传感器；而需要更好的能源和计算效率时，则选择激光雷达传感器。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了资源优先级如何影响传感器选择的重要性，提供了关于移动机器人协同设计的实际信息给设计师参考。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper discusses the integration challenges and strategies for designingmobile robots, by focusing on the task-driven, optimal selection of hardwareand software to balance safety, efficiency, and minimal usage of resources suchas costs, energy, computational requirements, and weight. We emphasize theinterplay between perception and motion planning in decision-making byintroducing the concept of occupancy queries to quantify the perceptionrequirements for sampling-based motion planners. Sensor and algorithmperformance are evaluated using False Negative Rates (FPR) and False PositiveRates (FPR) across various factors such as geometric relationships, objectproperties, sensor resolution, and environmental conditions. By integratingperception requirements with perception performance, an Integer LinearProgramming (ILP) approach is proposed for efficient sensor and algorithmselection and placement. This forms the basis for a co-design optimization thatincludes the robot body, motion planner, perception pipeline, and computingunit. We refer to this framework for solving the co-design problem of mobilerobots as CODEI, short for Co-design of Embodied Intelligence. A case study ondeveloping an Autonomous Vehicle (AV) for urban scenarios provides actionableinformation for designers, and shows that complex tasks escalate resourcedemands, with task performance affecting choices of the autonomy stack. Thestudy demonstrates that resource prioritization influences sensor choice:cameras are preferred for cost-effective and lightweight designs, while lidarsensors are chosen for better energy and computational efficiency.</description>
      <author>example@mail.com (Dejan Milojevic, Gioele Zardini, Miriam Elser, Andrea Censi, Emilio Frazzoli)</author>
      <guid isPermaLink="false">2503.10296v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for Surgical Intelligence</title>
      <link>http://arxiv.org/abs/2503.10265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为SurgRAW的框架，旨在利用Chain-of-Thought（CoT）驱动的多代理系统来增强视觉语言模型在机器人辅助手术中的应用。&lt;h4&gt;背景&lt;/h4&gt;当前VLMs存在幻觉、领域知识差距和对任务间依赖性的理解不足等问题，这影响了它们在手术场景中的临床可靠性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提供透明且可解释的见解的方法，以克服现有技术的限制，并提高机器人辅助手术中视觉语言模型的表现。&lt;h4&gt;方法&lt;/h4&gt;{'SurgRAW框架': '使用五个任务的专业CoT提示：工具识别、动作识别、行动预测、患者数据提取和结果评估，同时利用Retrieval-Augmented Generation（RAG）整合外部医学知识来弥补领域差距并提高响应的可靠性。', '层次代理系统': '确保嵌入CoT的VLM代理在理解任务依赖性的同时有效地协作，并通过小组讨论机制促进逻辑一致性。', 'SurgCoTBench数据集': '提出了具有结构化帧级注释的第一个基于推理的数据集，用于评估方法的有效性。'}&lt;h4&gt;主要发现&lt;/h4&gt;提出的SurgRAW框架相比于基准VLMs在12种机器人手术程序上实现了29.32%的准确性提升。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了通过结构化、领域意识强的推理来减少幻觉和提高响应可靠性的重要性，并且证明了所提方法在解释性、可信赖性和自主性的手术辅助方面的先进水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integration of Vision-Language Models (VLMs) in surgical intelligence ishindered by hallucinations, domain knowledge gaps, and limited understanding oftask interdependencies within surgical scenes, undermining clinicalreliability. While recent VLMs demonstrate strong general reasoning andthinking capabilities, they still lack the domain expertise and task-awarenessrequired for precise surgical scene interpretation. Although Chain-of-Thought(CoT) can structure reasoning more effectively, current approaches rely onself-generated CoT steps, which often exacerbate inherent domain gaps andhallucinations. To overcome this, we present SurgRAW, a CoT-driven multi-agentframework that delivers transparent, interpretable insights for most tasks inrobotic-assisted surgery. By employing specialized CoT prompts across fivetasks: instrument recognition, action recognition, action prediction, patientdata extraction, and outcome assessment, SurgRAW mitigates hallucinationsthrough structured, domain-aware reasoning. Retrieval-Augmented Generation(RAG) is also integrated to external medical knowledge to bridge domain gapsand improve response reliability. Most importantly, a hierarchical agenticsystem ensures that CoT-embedded VLM agents collaborate effectively whileunderstanding task interdependencies, with a panel discussion mechanismpromotes logical consistency. To evaluate our method, we introduceSurgCoTBench, the first reasoning-based dataset with structured frame-levelannotations. With comprehensive experiments, we demonstrate the effectivenessof proposed SurgRAW with 29.32% accuracy improvement over baseline VLMs on 12robotic procedures, achieving the state-of-the-art performance and advancingexplainable, trustworthy, and autonomous surgical assistance.</description>
      <author>example@mail.com (Chang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao Zeng, Zhu Zhuo, Evangelos B. Mazomenos, Yueming Jin)</author>
      <guid isPermaLink="false">2503.10265v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Reach-Avoid-Stay-Collision-Avoidance Negotiation Framework for Multi-Agent Systems via Spatiotemporal Tubes</title>
      <link>http://arxiv.org/abs/2503.10245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in ECC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于多智能体协商的框架，用于在执行具有未知动力学和有界扰动的预设时间到达-避免-停留（RAS）任务时获取无碰撞路径。&lt;h4&gt;背景&lt;/h4&gt;现有技术难以同时处理动态不确定性和碰撞问题，并且需要精确控制才能完成预定的时间目标。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于多智能体协商的方法，确保所有智能体在执行RAS任务的同时遵守时间变化的状态约束以避免相互之间的碰撞。&lt;h4&gt;方法&lt;/h4&gt;使用时空管生成时间变化的态约束，通过设计控制器使所有智能体遵循RAS规范。为了防止代理间发生碰撞，提出了一个成功的谈判机制，在这种机制下每个代理人都能获得满足其所需任务的时空管。&lt;h4&gt;主要发现&lt;/h4&gt;该研究提出的方法实现了完全分布式的、无需近似计算的控制律，适用于多机器人导航和无人机导航等场景中涉及预设时间RAS规范和碰撞避免的任务，并通过仿真验证了这种方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;基于多智能体协商框架能够有效地解决具有未知动力学和有界扰动的智能体之间的预设时间到达-避免-停留任务时的无碰撞路径问题，为未来智能体间的协调控制提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：This study presents a multi-agent negotiation-based framework to obtain collision-free paths while performing prescribed-time reach-avoid-stay (RAS) tasks for agents with unknown dynamics and bounded disturbance. By employing spatiotemporal tubes to generate time-varying state constraints, we ensure that all agents adhere to RAS specifications using synthesized controllers. To prevent inter-agent collisions, a negotiation mechanism is proposed where successful negotiations result in spatiotemporal tubes for each agent fulfilling desired tasks. This approach results in a completely distributed, approximation-free control law for each agent. The effectiveness of this mechanism was validated through simulations of multi-agent robot navigation and drone navigation tasks involving prescribed-time RAS specifications and collision avoidance.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents a multi-agent negotiation-based framework to obtaincollision-free paths while performing prescribed-time reach-avoid-stay (RAS)tasks for agents with unknown dynamics and bounded disturbance. By employingspatiotemporal tubes to generate time-varying state constraints, we ensure thatall agents adhere to RAS specifications using synthesized controllers. Toprevent inter-agent collisions, a negotiation mechanism is proposed wheresuccessful negotiations result in spatiotemporal tubes for each agentfulfilling desired tasks. This approach results in a completely distributed,approximation-free control law for each agent. The effectiveness of thismechanism was validated through simulations of multi-agent robot navigation anddrone navigation tasks involving prescribed-time RAS specifications andcollision avoidance.</description>
      <author>example@mail.com (Mohd. Faizuddin Faruqui, Ratnangshu Das, Ravi Kumar L, Pushpak Jagtap)</author>
      <guid isPermaLink="false">2503.10245v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>SCOOP: A Framework for Proactive Collaboration and Social Continual Learning through Natural Language Interaction andCausal Reasoning</title>
      <link>http://arxiv.org/abs/2503.10241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种社会连续学习框架，该框架旨在解决多模态信息收集场景中的人工智能系统与用户之间的协作问题。它通过对话、提问和互动让代理自主学习，并引入自然语言查询机制来回答有关环境状态和机制的问题。&lt;h4&gt;背景&lt;/h4&gt;在多模态信息收集设置下，人工智能助手需要处理复杂的文本和多模态交互过程，并且通常需要额外的结构化信息以实现有效的协作决策。这些情境中的人工智能系统往往难以准确理解用户的真实目标、信念和偏好。&lt;h4&gt;目的&lt;/h4&gt;提出一种用于因果知识获取和社会合作决策的社会连续学习框架，重点在于自主代理通过对话、提问及互动在开放但部分可观察环境中进行学习。&lt;h4&gt;方法&lt;/h4&gt;开发了两种架构：结合大型语言模型（LLMs）与ReAct框架以及问题生成的系统；基于原因世界的图形化或次符号知识表示系统的高级架构。后者构建了一个因果知识图来实现高效的推理和适应性。&lt;h4&gt;主要发现&lt;/h4&gt;评估任务强调了代理在识别知识空白、提出有意义的问题及逐步更新其推理方面的表现，同时考虑了获取知识的成本如何在相同环境下的不同任务之间分配。&lt;h4&gt;结论&lt;/h4&gt;该框架不仅有助于探索将因果推理与ReAct集成的方法以及优化提问和探索的有效性，还提供了一种结合因果推理、问题生成和社会学习的模型来理解发展的过程。&lt;h4&gt;挑战&lt;/h4&gt;包括如何有效整合因果推理到ReAct框架中，并在容易出错的情况下优化探索和提问策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal information-gathering settings, where users collaborate with AI indynamic environments, are increasingly common. These involve complex processeswith textual and multimodal interactions, often requiring additional structuralinformation via cost-incurring requests. AI helpers lack access to users' truegoals, beliefs, and preferences and struggle to integrate diverse informationeffectively.  We propose a social continual learning framework for causal knowledgeacquisition and collaborative decision-making. It focuses on autonomous agentslearning through dialogues, question-asking, and interaction in open, partiallyobservable environments. A key component is a natural language oracle thatanswers the agent's queries about environmental mechanisms and states, refiningcausal understanding while balancing exploration or learning, and exploitationor knowledge use.  Evaluation tasks inspired by developmental psychology emphasize causalreasoning and question-asking skills. They complement benchmarks by assessingthe agent's ability to identify knowledge gaps, generate meaningful queries,and incrementally update reasoning. The framework also evaluates how knowledgeacquisition costs are amortized across tasks within the same environment.  We propose two architectures: 1) a system combining Large Language Models(LLMs) with the ReAct framework and question-generation, and 2) an advancedsystem with a causal world model, symbolic, graph-based, or subsymbolic, forreasoning and decision-making. The latter builds a causal knowledge graph forefficient inference and adaptability under constraints. Challenges includeintegrating causal reasoning into ReAct and optimizing exploration andquestion-asking in error-prone scenarios. Beyond applications, this frameworkmodels developmental processes combining causal reasoning, question generation,and social learning.</description>
      <author>example@mail.com (Dimitri Ognibene, Sabrina Patania, Luca Annese, Cansu Koyuturk, Franca Garzotto, Giuseppe Vizzari, Azzurra Ruggeri, Simone Colombani)</author>
      <guid isPermaLink="false">2503.10241v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Safety Control of Impulsive Systems with Control Barrier Functions and Adaptive Gains</title>
      <link>http://arxiv.org/abs/2503.10164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合二次规划（QP）、控制屏障函数（CBFs）和自适应增益机制的统一框架，用于解决冲动系统中的安全性挑战。&lt;h4&gt;背景&lt;/h4&gt;在具有突然状态跳跃的冲动系统中，系统的动态特性变得复杂，需要新的方法来确保其安全性。&lt;h4&gt;目的&lt;/h4&gt;通过集成二次规划、控制屏障函数以及自适应增益机制，提出一个能够处理突变事件并保证安全性的框架。&lt;h4&gt;方法&lt;/h4&gt;利用CBFs捕捉连续动力学和冲动状态转换的效果以强制执行安全约束；动态调整基于冲击大小和系统接近边界情况下的自适应增益；通过特定的QP形式化构建将CBF约束和自适应增益调整纳入考虑，优化控制输入同时保证满足关键的安全要求。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析证明了自适应增益及整个框架在有界性、连续性和可行性方面的有效性。机器人机械手上的仿真结果展示了该方法的实际应用潜力。&lt;h4&gt;结论&lt;/h4&gt;通过提出的统一框架，在冲动系统中实现安全控制是可行的，这为解决具有状态跳跃系统的安全性问题提供了一种有效的途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要：本文解决了在突然的状态跃迁引入重大复杂性的冲击系统中的安全挑战。提出了一种结合二次规划（QP）、控制屏障函数（CBFs）和自适应增益机制的统一框架，以确保冲动事件期间的安全性。利用CBFs捕捉系统的连续动力学以及冲动状态转换的影响来强制执行安全性约束；动态调整基于冲击大小和系统接近边界情况下的自适应增益，保持在瞬间状态跳跃时的安全性；特定形式化的QP整合了CBFs约束及自适应增益调整，优化控制输入同时确保遵守关键的安全要求。理论分析证明了自适应增益及其整体框架的有界性、连续性和可行性。通过针对机器人机械手上的仿真展示该方法的有效性，表明其适用于具有状态跳跃特性的冲动系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the safety challenges in impulsive systems, where abruptstate jumps introduce significant complexities into system dynamics. A unifiedframework is proposed by integrating Quadratic Programming (QP), ControlBarrier Functions (CBFs), and adaptive gain mechanisms to ensure system safetyduring impulsive events. The CBFs are constructed to enforce safety constraintsby capturing the system's continuous dynamics and the effects of impulsivestate transitions. An adaptive gain mechanism dynamically adjusts controlinputs based on the magnitudes of the impulses and the system's proximity tosafety boundaries, maintaining safety during instantaneous state jumps. Atailored QP formulation incorporates CBFs constraints and adaptive gainadjustments, optimizing control inputs while ensuring compliance withsafety-critical requirements. Theoretical analysis establishes the boundedness,continuity, and feasibility of the adaptive gain and the overall framework. Theeffectiveness of the method is demonstrated through simulations on a roboticmanipulator, showcasing its practical applicability to impulsive systems withstate jumps.</description>
      <author>example@mail.com (Zihan Liu, Yuan-Hua Ni)</author>
      <guid isPermaLink="false">2503.10164v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>An Real-Sim-Real (RSR) Loop Framework for Generalizable Robotic Policy Transfer with Differentiable Simulation</title>
      <link>http://arxiv.org/abs/2503.10118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的Real-Sim-Real (RSR) 循环框架，通过利用可微模拟技术来解决机器人从仿真到现实应用中的差距问题。&lt;h4&gt;背景&lt;/h4&gt;目前在机器人领域中，仿真环境与真实环境之间的差异（sim-to-real gap）仍然是一大挑战。这阻碍了训练于仿真环境中的算法部署至实际系统。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在通过迭代优化模拟参数以适应真实世界条件来缩小仿真和现实之间的差距，并促进鲁棒且高效的策略迁移。&lt;h4&gt;方法&lt;/h4&gt;该论文设计了一个信息量丰富的成本函数，鼓励采集多样性和代表性强的真实数据，最小化偏差并最大化每个数据点用于改进仿真的价值。此成本函数可以无缝集成到现有强化学习算法中（如PPO、SAC）以确保在真实领域中的平衡探索。&lt;h4&gt;主要发现&lt;/h4&gt;研究实施于多功能的Mujoco MJX平台上，并且该框架适用于各种机器人系统，实验结果表明提出的这种方法显著缩小了仿真与现实之间的差距，在多种情景下实现了高任务性能和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;RSR循环框架通过利用可微模拟技术提供了一种有效的途径来解决sim-to-real gap问题。该方法在多个实际操作中表现出强大的适应性和鲁棒性，为机器人算法的部署开辟了新的道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：仿真到现实环境之间的差距仍是机器人领域一个关键挑战，阻碍训练于仿真中的算法被应用到真实世界系统之中。本文介绍了一种新颖的Real-Sim-Real (RSR) 循环框架，利用可微模拟技术通过迭代优化模拟参数来解决这一问题，并使其与现实条件对齐，从而实现稳健且高效的策略转移。本研究的一个重要贡献在于设计了一个信息丰富的成本函数，它鼓励收集多样性和代表性强的真实数据，最小化偏差并最大化每个数据点用于改进仿真的价值。该成本函数可以无缝集成到现有强化学习算法（例如PPO、SAC）中，并确保在真实领域中的平衡探索。此外，我们的方法已在多功能的Mujoco MJX平台上实现，并且框架适用于广泛的机器人系统。实验结果显示，在多种机器人力控任务上使用我们的方法显著缩小了仿真与现实之间的差距，实现了高任务性能和跨不同环境不确定性场景的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The sim-to-real gap remains a critical challenge in robotics, hindering thedeployment of algorithms trained in simulation to real-world systems. Thispaper introduces a novel Real-Sim-Real (RSR) loop framework leveragingdifferentiable simulation to address this gap by iteratively refiningsimulation parameters, aligning them with real-world conditions, and enablingrobust and efficient policy transfer. A key contribution of our work is thedesign of an informative cost function that encourages the collection ofdiverse and representative real-world data, minimizing bias and maximizing theutility of each data point for simulation refinement. This cost functionintegrates seamlessly into existing reinforcement learning algorithms (e.g.,PPO, SAC) and ensures a balanced exploration of critical regions in the realdomain. Furthermore, our approach is implemented on the versatile Mujoco MJXplatform, and our framework is compatible with a wide range of robotic systems.Experimental results on several robotic manipulation tasks demonstrate that ourmethod significantly reduces the sim-to-real gap, achieving high taskperformance and generalizability across diverse scenarios of both explicit andimplicit environmental uncertainties.</description>
      <author>example@mail.com (Lu Shi, Yuxuan Xu, Shiyu Wang, Jinhao Huang, Wenhao Zhao, Yufei Jia, Zike Yan, Weibin Gu, Guyue Zhou)</author>
      <guid isPermaLink="false">2503.10118v1</guid>
      <pubDate>Fri, 14 Mar 2025 19:22:24 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Load Balancing for EV Charging Stations Using Reinforcement Learning and Demand Prediction</title>
      <link>http://arxiv.org/abs/2503.06370v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19th Annual IEEE International Systems Conference (SysCon 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要原文&lt;/h4&gt;This paper presents a method for load balancing and dynamic pricing in electric vehicle (EV) charging networks, utilizing reinforcement learning (RL) to enhance network performance. The proposed framework integrates a pre-trained graph neural network to predict demand elasticity and inform pricing decisions. The spatio-temporal EV charging demand prediction (EVCDP) dataset from Shenzhen is utilized to capture the geographic and temporal characteristics of the charging stations. The RL model dynamically adjusts prices at individual stations based on occupancy, maximum station capacity, and demand forecasts, ensuring an equitable network load distribution while preventing station overloads. By leveraging spatially-aware demand predictions and a carefully designed reward function, the framework achieves efficient load balancing and adaptive pricing strategies that respond to localized demand and global network dynamics, ensuring improved network stability and user satisfaction. The efficacy of the approach is validated through simulations on the dataset, showing significant improvements in load balancing and reduced overload as the RL agent iteratively interacts with the environment and learns to dynamically adjust pricing strategies based on real-time demand patterns and station constraints. The findings highlight the potential of adaptive pricing and load-balancing strategies to address the complexities of EV infrastructure, paving the way for scalable and user-centric solutions.&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用强化学习（RL）进行电动汽车充电网络负载均衡和动态定价的方法，旨在提高网络性能。该框架结合了预训练的图神经网络来预测需求弹性并提供价格决策信息。&lt;h4&gt;背景&lt;/h4&gt;随着电动汽车的普及，充电站的需求变得日益复杂，需要一种有效的负载平衡机制以防止过载，并提高用户满意度。&lt;h4&gt;目的&lt;/h4&gt;提出一个高效的动态定价和负载均衡策略框架，用于优化电动车辆（EV）充电网络的表现。&lt;h4&gt;方法&lt;/h4&gt;利用深圳的空间时间电动车充电需求预测数据集来捕捉充电站的地理及时间特征。RL模型根据每个站点的占用情况、最大容量以及需求预测进行价格调整，确保负载分布平衡并防止过载。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟实验验证了该框架的有效性，展示了显著提高的负载均衡效果和减少了站点过载的现象。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，自适应定价与负载均衡策略可以有效解决电动汽车基础设施中的复杂问题，并为可扩展且以用户为中心的解决方案铺平道路。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种方法，在电动车辆（EV）充电网络中进行负载平衡和动态定价，利用强化学习（RL）来提高网络性能。所提出的框架整合了一个预训练的图神经网络，用以预测需求弹性，并为价格决策提供信息。使用深圳的空间时间电动车充电需求预测数据集来捕捉充电站的地理与时间特性。该模型根据每个站点的占用率、最大容量及需求预测动态调整价格，确保负载均衡且避免过载。通过利用空间感知的需求预测和精心设计的奖励函数，该框架实现了高效的负载平衡与自适应定价策略，以响应本地化需求及全球网络动态，从而提高了网络稳定性和用户满意度。研究的有效性通过在数据集上的模拟验证了显著提高的负载平衡效果以及减少了站点过载的现象。研究表明，自适应定价和负载均衡策略有潜力解决电动汽车基础设施中的复杂问题，并为可扩展且以用户为中心的解决方案铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a method for load balancing and dynamic pricing inelectric vehicle (EV) charging networks, utilizing reinforcement learning (RL)to enhance network performance. The proposed framework integrates a pre-trainedgraph neural network to predict demand elasticity and inform pricing decisions.The spatio-temporal EV charging demand prediction (EVCDP) dataset from Shenzhenis utilized to capture the geographic and temporal characteristics of thecharging stations. The RL model dynamically adjusts prices at individualstations based on occupancy, maximum station capacity, and demand forecasts,ensuring an equitable network load distribution while preventing stationoverloads. By leveraging spatially-aware demand predictions and a carefullydesigned reward function, the framework achieves efficient load balancing andadaptive pricing strategies that respond to localized demand and global networkdynamics, ensuring improved network stability and user satisfaction. Theefficacy of the approach is validated through simulations on the dataset,showing significant improvements in load balancing and reduced overload as theRL agent iteratively interacts with the environment and learns to dynamicallyadjust pricing strategies based on real-time demand patterns and stationconstraints. The findings highlight the potential of adaptive pricing andload-balancing strategies to address the complexities of EV infrastructure,paving the way for scalable and user-centric solutions.</description>
      <author>example@mail.com (Hesam Mosalli, Saba Sanami, Yu Yang, Hen-Geul Yeh, Amir G. Aghdam)</author>
      <guid isPermaLink="false">2503.06370v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
  <item>
      <title>PointDiffuse: A Dual-Conditional Diffusion Model for Enhanced Point Cloud Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2503.06094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'研究背景': '扩散概率模型传统上用于生成二维图像中固定像素位置的颜色。这项工作将其扩展到点云语义分割，其中点的位置保持不变，但扩散模型会生成点的标签而不是颜色。', '目的': '为了加速逆向扩散过程中的去噪步骤，引入了一种噪声标签嵌入机制，并提出了一种频率转换器来增强对高阶上下文的调整。同时减少计算复杂度的方法也被提出。', '方法': {'噪声标签嵌入机制': '通过将语义信息整合到噪声标签中提供初始参考以改善逆向扩散效率。', '点频率变换器': '增强了点云中的高阶上下文调整。', '带位置条件的MLP和去噪PointNet': '用于处理高质量点云，不牺牲几何细节。', '双条件扩散模型网络(PointDiffuse)': '整合上述方法以进行大规模点云语义分割。'}, '主要发现': '在五个基准数据集上的广泛实验展示了PointDiffuse的优越性，并且在S3DIS Area 5上达到了74.2%的最佳mIoU，在S3DIS六折测试中为81.2%，以及SWAN数据集中64.8%。', '结论': '该研究扩展了扩散模型的应用范围，提高了大规模点云语义分割的性能，展示了在多个基准上的卓越效果。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion probabilistic models are traditionally used to generate colors atfixed pixel positions in 2D images. Building on this, we extend diffusionmodels to point cloud semantic segmentation, where point positions also remainfixed, and the diffusion model generates point labels instead of colors. Toaccelerate the denoising process in reverse diffusion, we introduce a noisylabel embedding mechanism. This approach integrates semantic information intothe noisy label, providing an initial semantic reference that improves thereverse diffusion efficiency. Additionally, we propose a point frequencytransformer that enhances the adjustment of high-level context in point clouds.To reduce computational complexity, we introduce the position condition intoMLP and propose denoising PointNet to process the high-resolution point cloudwithout sacrificing geometric details. Finally, we integrate the proposed noisylabel embedding, point frequency transformer and denoising PointNet in ourproposed dual conditional diffusion model-based network (PointDiffuse) toperform large-scale point cloud semantic segmentation. Extensive experiments onfive benchmarks demonstrate the superiority of PointDiffuse, achieving thestate-of-the-art mIoU of 74.2\% on S3DIS Area 5, 81.2\% on S3DIS 6-fold and64.8\% on SWAN dataset.</description>
      <author>example@mail.com (Yong He, Hongshan Yu, Mingtao Feng, Tongjia Chen, Zechuan Li, Anwaar Ulhaq, Saeed Anwar, Ajmal Saeed Mian)</author>
      <guid isPermaLink="false">2503.06094v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>QBIT: Quality-Aware Cloud-Based Benchmarking for Robotic Insertion Tasks</title>
      <link>http://arxiv.org/abs/2503.07479v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了QBIT框架，该框架旨在评估机器人的插入任务表现，通过在MuJoCo模拟器中随机化接触参数和考虑感知不确定性来确保统计显著性和最小化仿真与现实之间的差距。&lt;h4&gt;背景&lt;/h4&gt;机器人执行的插入任务对自主操作而言至关重要但具有挑战性。传统的AI方法虽然能够应对这种挑战，但在实际生产应用中的表现必须保证高成功率、质量保障以及可靠性。&lt;h4&gt;目的&lt;/h4&gt;提出QBIT框架来评估机器人在插入任务上的性能，确保其不仅高效而且可靠。&lt;h4&gt;方法&lt;/h4&gt;引入了QBIT质量感知基准测试框架，该框架采用额外指标如力能量、力平滑度和完成时间来进行全面评估；随机化MuJoCo模拟器中的接触参数以保证统计显著性；利用基于Kubernetes的基础设施进行大规模实验，确保广泛适用性和增强可重现性。&lt;h4&gt;主要发现&lt;/h4&gt;QBIT在仿真环境和真实世界环境中均表现出色，能够有效比较不同的插入方法，并加速从实验室到实际应用的过渡。&lt;h4&gt;结论&lt;/h4&gt;QBIT框架为机器人执行高质量的插入任务提供了有力支持，代码已在GitHub上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Insertion tasks are fundamental yet challenging for robots, particularly inautonomous operations, due to their continuous interaction with theenvironment. AI-based approaches appear to be up to the challenge, but inproduction they must not only achieve high success rates. They must also ensureinsertion quality and reliability. To address this, we introduce QBIT, aquality-aware benchmarking framework that incorporates additional metrics suchas force energy, force smoothness and completion time to provide acomprehensive assessment. To ensure statistical significance and minimize thesim-to-real gap, we randomize contact parameters in the MuJoCo simulator,account for perceptual uncertainty, and conduct large-scale experiments on aKubernetes-based infrastructure. Our microservice-oriented architecture ensuresextensibility, broad applicability, and improved reproducibility. To facilitateseamless transitions to physical robotic testing, we use ROS2 withcontainerization to reduce integration barriers. We evaluate QBIT using threeinsertion approaches: geometricbased, force-based, and learning-based, in bothsimulated and real-world environments. In simulation, we compare the accuracyof contact simulation using different mesh decomposition techniques. Ourresults demonstrate the effectiveness of QBIT in comparing different insertionapproaches and accelerating the transition from laboratory to real-worldapplications. Code is available on GitHub.</description>
      <author>example@mail.com (Constantin Schempp, Yongzhou Zhang, Christian Friedrich, Bjorn Hein)</author>
      <guid isPermaLink="false">2503.07479v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Towards species' classification of the \textit{Anastrepha pseudoparallela} group</title>
      <link>http://arxiv.org/abs/2503.08598v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;识别属于伪平行组的Anastrepha物种因物种之间的形态相似性和广泛的地理变异而变得困难。该小组包括31种果蝇和影响人心果作物的害虫。这些物种的鉴定利用了标本翅膀和刺尖的形态特征。&lt;h4&gt;背景&lt;/h4&gt;由于伪平行组（pseudoparallela group）中的Anastrepha物种之间存在显著的形态相似性以及广泛的地理变异，因此通过传统的形态学方法准确识别它们极具挑战性。此外，这些物种对人心果作物构成威胁，并且数据量小和类别不平衡的问题使得机器学习在这一领域的应用受限。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有的挑战并提高稀有Anastrepha pseudoparallela组物种的分类准确性，本研究旨在利用深度学习方法自动分类五个特定的Anastrepha物种，并设计基于翅膀结构的新特征。&lt;h4&gt;方法&lt;/h4&gt;本研究探索了迁移学习解决方案，并提出了一套新的基于每只翅膀结构特性的特征。研究人员使用双重退火算法优化深度神经网络、随机森林、决策树和支持向量机（与自动编码器和SMOTE算法结合）的超参数，以解决类别不平衡问题。该方法使用127张高质量图像的数据集进行测试，并采用三折交叉验证技术进行训练、调优和测试。&lt;h4&gt;主要发现&lt;/h4&gt;本研究结果表明，将特征提取技术和机器学习相结合的新颖方法可以提高Anastrepha pseudoparallela组稀有物种的分类准确率。使用SMOTE与随机森林算法组合时，在所有物种中的个体准确性平均值达到0.72。&lt;h4&gt;结论&lt;/h4&gt;结合深度学习和新的基于翅膀结构特征的方法为自动识别Anastrepha伪平行组中五个特定物种提供了一个有效的解决方案，有助于减少人工鉴定的工作量，并提高分类的准确性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying \textit{Anastrepha} species from the \textit{pseudoparallela}group is problematic due to morphological similarities among species and abroad geographic variation. This group comprises $31$ species of fruit fliesand pests affecting passion fruit crops. Identifying these species utilises themorphological characteristics of the specimens' wings and aculeus tips.Considering the importance of identifying these species of this group andchallenges due to the low data availability and class imbalance, this papercontributes to automating the classification of the species \textit{Anastrephachiclayae} Greene, \textit{Anastrepha consobrina} (Loew), \textit{Anastrephacuritibana} Ara\'ujo, Norrbom \&amp; Savaris, \textit{Anastrepha curitis} Stone,and \textit{Anastrepha pseudoparallela} (Loew) using deep Learning methods anddesigning new features based on wing structures. We explored transfer learningsolutions and proposed a new set of features based on each wing's structure. Weused the dual annealing algorithm to optimise the hyperparameters of DeepNeural Networks, Random Forests, Decision Trees, and Support Vector Machinescombined with autoencoders and SMOTE algorithms to account for class imbalance.We tested our approach with a dataset of $127$ high-quality images belonging toa total of $5$ species and used three-fold cross validation for training,tuning and testing, encompassing six permutations of those to assess theperformance of the learning algorithms. Our findings demonstrate that our novelapproach, which combines feature extraction and machine learning techniques,can improve the species classification accuracy for rare \textit{Anastrephapseudoparallela} group specimens, with the SMOTE and Random Forests algorithmsleading to the average performance of $0.72$ in terms of the mean of theindividual accuracies considering all species.</description>
      <author>example@mail.com (Gabriel R. Palma, Rocío Alaiz, Alexandre S. Araújo, Marcoandre Savaris, Roberto A. Zucchi, Charles Markham, Rafael A. Moral)</author>
      <guid isPermaLink="false">2503.08598v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>MMRL: Multi-Modal Representation Learning for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2503.08497v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;大规模预训练的视觉-语言模型（VLM）在多种任务中进行迁移学习时变得至关重要。然而，在少量数据条件下调整这些模型往往会导致过拟合，从而降低其在新任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;随着大规模预训练视觉-语言模型（VLM）的应用越来越广泛，它们需要在不同的任务中适应各种类型的有限数据集。但是，当使用有限的数据时，传统的方法容易导致过拟合问题，影响了模型的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多模态表示学习框架MMRL，以解决大规模预训练视觉-语言模型在少量数据条件下的过度拟合问题，并提高其跨任务的学习效率和泛化性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为Multi-Modal Representation Learning（MMRL）的新框架。该框架包括共享的、可学习的和模态无关的表示空间，将原始特征转换为文本和图像表示令牌，以便于更多的多模态交互。不同于以前的方法只优化类别令牌特征，MMRL在编码器较高层中集成了表示令牌，并保留了较低层中的通用知识。同时引入了一个正则化项来对齐类别特征与冻结VLM的零样本特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的多模态表示学习框架（MMRL）比现有的最佳方法表现更好，在15个数据集上的测试都证明了这一点，这体现了MMRL在任务特定适应性和泛化能力之间取得平衡的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文提出了一种新的多模态表示学习方法MMRL，该方法解决了大规模预训练视觉-语言模型在少量数据条件下的过拟合问题，并提高了其跨任务的学习效率和泛化性能。实验结果证明了该框架的优越性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;大型预训练的视觉-语言模型（VLM）已经成为不同任务中转移学习的基础。然而，使用这些模型时如果仅有有限的数据样本，则往往会导致过拟合现象，从而降低其在新任务上的表现。为解决这个问题，我们提出了一种新颖的多模态表示学习框架MMRL，它引入了一个共享、可学习且模态无关的表示空间。MMRL将原始令牌投影到文本和图像特征令牌上，促进了更有效的跨模态交互。不同于以往的方法仅优化类别令牌的功能，MMRL在编码器较高层中加入了表示令牌（在这里数据集特定功能更为突出），同时保留了较低层中的通用知识。训练时，该框架会针对表示特征和类别特征进行优化，并且对表示令牌应用可学习的投影层，而保持类别令牌的预训练知识不变。此外，在训练过程中还引入了一个正则化项来将类别特征与文本特征对齐到冻结VLM的零样本特征上，从而保护模型的泛化能力。在推断阶段采用了去耦策略：对于基础类使用表示和类别功能，而对于新任务仅使用保留更广泛知识的类别特征。广泛的实验证明MMRL优于现有方法，在15个数据集上的测试均显示出它在特定任务适应性和泛化性能之间的平衡做得更好。代码可从https://github.com/yunncheng/MMRL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale pre-trained Vision-Language Models (VLMs) have become essentialfor transfer learning across diverse tasks. However, adapting these models withlimited few-shot data often leads to overfitting, diminishing their performanceon new tasks. To tackle this issue, we propose a novel Multi-ModalRepresentation Learning (MMRL) framework that introduces a shared, learnable,and modality-agnostic representation space. MMRL projects the space tokens totext and image representation tokens, facilitating more effective multi-modalinteractions. Unlike previous approaches that solely optimize class tokenfeatures, MMRL integrates representation tokens at higher layers of theencoders--where dataset-specific features are more prominent--while preservinggeneralized knowledge in the lower layers. During training, both representationand class features are optimized, with trainable projection layer applied tothe representation tokens, whereas the class token projection layer remainsfrozen to retain pre-trained knowledge. Furthermore, a regularization term isintroduced to align the class features and text features with the zero-shotfeatures from the frozen VLM, thereby safeguarding the model's generalizationcapacity. For inference, a decoupling strategy is employed, wherein bothrepresentation and class features are utilized for base classes, while only theclass features, which retain more generalized knowledge, are used for newtasks. Extensive experiments across 15 datasets demonstrate that MMRLoutperforms state-of-the-art methods, achieving a balanced trade-off betweentask-specific adaptation and generalization. Code is available athttps://github.com/yunncheng/MMRL.</description>
      <author>example@mail.com (Yuncheng Guo, Xiaodong Gu)</author>
      <guid isPermaLink="false">2503.08497v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Unifying Structure and Activation: A Comprehensive Approach of Parameter and Memory Efficient Transfer Learning</title>
      <link>http://arxiv.org/abs/2503.08154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的参数高效迁移学习框架S2A，旨在减少模型激活期间的内存占用。&lt;h4&gt;背景&lt;/h4&gt;现有的参数高效迁移学习方法在减少可训练参数数量方面取得了进展，但在大规模预训练模型的应用中，其内存消耗并未显著降低。这限制了这些方法在内存受限设备上的实际应用。&lt;h4&gt;目的&lt;/h4&gt;设计一种新的PETL框架S2A来进一步缩小激活期间的内存占用，同时保持准确性。&lt;h4&gt;方法&lt;/h4&gt;该框架包括两个部分：一是参数化模型结构中的激活模块（如偏差、提示和侧面模块）的设计；二是对非参数化结构（如非线性函数）基于导数的4位量化激活。这两个部分都旨在减少可调参数数量以及内存占用。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，S2A方法比现有的PETL技术在平均GPU内存占用上减少了四倍，并且在准确性方面具有竞争力，在降低可调参数的同时保持了高精度。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法证明了它非常适合硬件受限设备上的实际迁移学习任务。&lt;h4&gt;翻译&lt;/h4&gt;摘要：参数高效转移学习（PETL）旨在减少多下游任务的预训练模型规模。然而，随着模型不断扩展，与可学习参数相比，现有PETL方法并未显著降低内存消耗。这种限制阻碍了这些方法在内存受限设备上的实际部署。为此，我们提出了一种新的PETL框架S2A，以减少激活期间的内存占用。具体来说，我们的框架包括：1）参数模型结构中的激活模块（如偏差、提示和侧模块）设计，这导致可调参数及激活记忆显著减少；2）基于导数对非参数化结构（例如，非线性函数）进行4位量化激活，以保持准确性的同时大幅度降低内存使用。S2A方法因此提供了一个在参数和内存占用方面都轻量级的解决方案。我们使用不同的骨干网络评估了S2A，并在各种数据集上进行了广泛的实验以评估其有效性。结果显示，我们的方法不仅优于现有的PETL技术，在平均GPU内存足迹上实现了四倍的减少，同时在较低可调参数的情况下展示了竞争力准确性。这也证明了我们在硬件受限设备上的迁移学习中的方法是高度适合的实际应用中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient transfer learning (PETL) aims to reduce the scales ofpre-trained models for multiple downstream tasks. However, as the models keepscaling up, the memory footprint of existing PETL methods is not significantlyreduced compared to the reduction of learnable parameters. This limitationhinders the practical deployment of PETL methods on memory-constrained devices.To this end, we proposed a new PETL framework, called Structure to Activation(S2A), to reduce the memory footprint of activation during fine-tuning.Specifically, our framework consists of: 1)Activation modules design(i.e. bias,prompt and side modules) in the parametric model structure, which results in asignificant reduction of adjustable parameters and activation memory 2) 4-bitquantisation of activations based on their derivatives for non-parametricstructures (e.g., nonlinear functions), which maintains accuracy whilesignificantly reducing memory usage. Our S2A method consequently offers alightweight solution in terms of both parameter and memory footprint. Weevaluate S2A with different backbones and conduct extensive experiments onvarious datasets to evaluate the effectiveness. The results show that ourmethod not only outperforms existing PETL techniques, achieving a fourfoldreduction in GPU memory footprint on average, but also shows competitiveperformance in accuracy with lower tunable parameters. These also demonstratethat our method is highly suitable for practical transfer learning onhardware-constrained devices.</description>
      <author>example@mail.com (Tian Jin, Enjun Du, Changwei Wang, Wenhao Xu, Ding Luo)</author>
      <guid isPermaLink="false">2503.08154v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>TrackOcc: Camera-based 4D Panoptic Occupancy Tracking</title>
      <link>http://arxiv.org/abs/2503.08471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种基于摄像头的4D全景占用跟踪任务，旨在通过单一视觉输入解决空间全面性和时间一致性的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的3D物体追踪和语义占用预测任务在空间全面性或时间一致性方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;引入并解决了Camera-based 4D Panoptic Occupancy Tracking问题，该问题是基于单个摄像头的输入同时进行全景占用分割和对象跟踪的任务。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为TrackOcc的方法，可以以流式处理图像输入的方式在端到端中运用4D全景查询来解决新任务。&lt;h4&gt;主要发现&lt;/h4&gt;通过利用定位感知损失函数，TrackOcc显著提升了4D全景占用跟踪的准确性。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，在Waymo数据集上的方法取得了最先进的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;全面和一致地从摄像机输入理解动态场景对于高级自主系统至关重要。传统的基于摄像头的目标检测任务如3D物体追踪和语义占用预测缺乏空间全面性或时间一致性。在这项工作中，我们引入了一种全新的任务——基于摄像头的4D全景占用跟踪，同时解决全景占用分割和对象跟踪问题，仅通过摄像机输入即可实现。此外，我们提出了一种前沿的方法TrackOcc，该方法以流式处理图像输入的方式使用4D全景查询来应对提出的任务，并利用定位感知损失函数提升4D全景占用跟踪的准确性。实验结果表明，在Waymo数据集上我们的方法实现了最先进的性能表现。源代码将于https://github.com/Tsinghua-MARS-Lab/TrackOcc公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Comprehensive and consistent dynamic scene understanding from camera input isessential for advanced autonomous systems. Traditional camera-based perceptiontasks like 3D object tracking and semantic occupancy prediction lack eitherspatial comprehensiveness or temporal consistency. In this work, we introduce abrand-new task, Camera-based 4D Panoptic Occupancy Tracking, whichsimultaneously addresses panoptic occupancy segmentation and object trackingfrom camera-only input. Furthermore, we propose TrackOcc, a cutting-edgeapproach that processes image inputs in a streaming, end-to-end manner with 4Dpanoptic queries to address the proposed task. Leveraging thelocalization-aware loss, TrackOcc enhances the accuracy of 4D panopticoccupancy tracking without bells and whistles. Experimental results demonstratethat our method achieves state-of-the-art performance on the Waymo dataset. Thesource code will be released at https://github.com/Tsinghua-MARS-Lab/TrackOcc.</description>
      <author>example@mail.com (Zhuoguang Chen, Kenan Li, Xiuyu Yang, Tao Jiang, Yiming Li, Hang Zhao)</author>
      <guid isPermaLink="false">2503.08471v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Pre-trained Models Succeed in Medical Imaging with Representation Similarity Degradation</title>
      <link>http://arxiv.org/abs/2503.07958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了跨域迁移学习中表示相似性演化的关键问题，特别关注预训练模型在医学影像任务中的有效适应性，尽管存在显著的领域差距。研究建立了一个以量化和分析微调过程中表示相似性的轨迹为中心的问题定义。&lt;h4&gt;背景&lt;/h4&gt;跨域迁移学习涉及将一个领域的知识迁移到另一个领域中。当涉及到从非医疗领域（如图像分类）到医疗影像任务时，由于数据分布的巨大差异，这种转移通常非常具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;研究旨在理解为什么预训练模型在适应医学成像任务时能够保持有效性，并探讨表示相似性的进化过程。&lt;h4&gt;方法&lt;/h4&gt;通过严格的定义问题来量化和分析整个微调过程中表示的相似性轨迹。这项工作关注两种主要的预训练范式：监督学习与自监督学习，以及它们如何影响跨域模型适应的不同模式。&lt;h4&gt;主要发现&lt;/h4&gt;{'高表现力模型': '存在一些保持任务准确性和其预训练源头之间的表征相似性的高性能模型。', '层级相似性指标': '表示质量指标和逐层相似性度量之间存在稳健的线性相关性。', '适应模式差异': '监督学习与自监督学习两种预训练范式在适应过程中表现出不同的模式。'}&lt;h4&gt;结论&lt;/h4&gt;提出的相似空间框架不仅为知识转移动态提供了机制见解，还提出了关于最优利用预训练模型的基本问题。这些结果加深了我们对神经网络适应过程的理解，并为迁移学习策略的应用提供了实用意义，尤其是在医学影像领域之外。&lt;h4&gt;翻译&lt;/h4&gt;该论文调查了跨域传输学习过程中表示相似性的演变，特别关注为什么预先训练的模型在适应医疗成像任务时仍然保持有效，尽管存在显著的领域差距。这项研究确立了一个问题定义，侧重于量化和分析整个微调过程中的表示相似性轨迹，并仔细划定范围以包含医学图像分析以及更广泛的跨域适应场景。我们的实证发现揭示了三个关键发现：可能存在同时保留任务准确性及其预训练起源之间表示相似性的高性能模型；逐层相似度指标与表示质量指标之间的稳健线性相关关系；以及监督和自我监督预训练范式之间的不同调整模式。提出的相似空间框架不仅为知识转移动态提供了机制见解，还提出了关于最佳利用预先训练的模型的基本问题。这些结果加深了我们对神经网络适应过程的理解，并为超越医学成像应用的迁移学习策略提供实际意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates the critical problem of representation similarityevolution during cross-domain transfer learning, with particular focus onunderstanding why pre-trained models maintain effectiveness when adapted tomedical imaging tasks despite significant domain gaps. The study establishes arigorous problem definition centered on quantifying and analyzingrepresentation similarity trajectories throughout the fine-tuning process,while carefully delineating the scope to encompass both medical image analysisand broader cross-domain adaptation scenarios. Our empirical findings revealthree critical discoveries: the potential existence of high-performance modelsthat preserve both task accuracy and representation similarity to theirpre-trained origins; a robust linear correlation between layer-wise similaritymetrics and representation quality indicators; and distinct adaptation patternsthat differentiate supervised versus self-supervised pre-training paradigms.The proposed similarity space framework not only provides mechanistic insightsinto knowledge transfer dynamics but also raises fundamental questions aboutoptimal utilization of pre-trained models. These results advance ourunderstanding of neural network adaptation processes while offering practicalimplications for transfer learning strategies that extend beyond medicalimaging applications. The code will be available once accepted.</description>
      <author>example@mail.com (Wenqiang Zu, Shenghao Xie, Hao Chen, Lei Ma)</author>
      <guid isPermaLink="false">2503.07958v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Language-Depth Navigated Thermal and Visible Image Fusion</title>
      <link>http://arxiv.org/abs/2503.08676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;深度引导的多模态融合技术结合了可见光和红外图像中的深度信息，显著提升了3D重建和机器人应用的表现。&lt;h4&gt;背景&lt;/h4&gt;现有热成像与可见光图像融合主要集中在检测任务上，忽视了诸如深度等其他关键信息。单模态在低光和复杂环境下的局限性限制了其效能。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于文本引导和深度驱动的红外和可见光图像融合网络，以克服现有技术的不足，并提升3D重建及机器人导航、定位等方面的性能。&lt;h4&gt;方法&lt;/h4&gt;该模型由一个通过扩散模型提取多通道互补信息的图像融合分支组成，包括一个文本引导模块；两个辅助深度估计分支用于计算基于深度的损失并优化图像融合网络。使用CLIP从增强深度的图像描述中抽取语义信息和参数来指导扩散模型。&lt;h4&gt;主要发现&lt;/h4&gt;该框架旨在通过整合视觉-语言和深度信息直接生成多模态输入下的彩色融合图像，从而提供更加准确的点云数据、完整的3D重建及更全面的场景理解能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的文本引导与深度驱动的红外可见光图像融合网络为自动无人驾驶和救援任务中的精确识别和高效操作提供了强有力的支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Depth-guided multimodal fusion combines depth information from visible andinfrared images, significantly enhancing the performance of 3D reconstructionand robotics applications. Existing thermal-visible image fusion mainly focuseson detection tasks, ignoring other critical information such as depth. Byaddressing the limitations of single modalities in low-light and complexenvironments, the depth information from fused images not only generates moreaccurate point cloud data, improving the completeness and precision of 3Dreconstruction, but also provides comprehensive scene understanding for robotnavigation, localization, and environmental perception. This supports preciserecognition and efficient operations in applications such as autonomous drivingand rescue missions. We introduce a text-guided and depth-driven infrared andvisible image fusion network. The model consists of an image fusion branch forextracting multi-channel complementary information through a diffusion model,equipped with a text-guided module, and two auxiliary depth estimationbranches. The fusion branch uses CLIP to extract semantic information andparameters from depth-enriched image descriptions to guide the diffusion modelin extracting multi-channel features and generating fused images. These fusedimages are then input into the depth estimation branches to calculatedepth-driven loss, optimizing the image fusion network. This framework aims tointegrate vision-language and depth to directly generate color-fused imagesfrom multimodal inputs.</description>
      <author>example@mail.com (Jinchang Zhang, Zijun Li, Guoyu Lu)</author>
      <guid isPermaLink="false">2503.08676v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes</title>
      <link>http://arxiv.org/abs/2503.07940v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种零样本点云配准管道BUFFER-X，以解决现有基于深度学习的方法在新环境中泛化能力差的问题。&lt;h4&gt;背景&lt;/h4&gt;虽然近年来基于深度学习的点云配准有所改进，但大多数方法仍然需要针对每个新的环境重新训练或手动调整参数。&lt;h4&gt;目的&lt;/h4&gt;识别并解决了限制点云配准通用性的三个关键因素：依赖于特定环境的体素大小和搜索半径、学习基元检测器在跨域鲁棒性较差以及原始坐标使用加剧了比例差异的问题。&lt;h4&gt;方法&lt;/h4&gt;通过自适应确定体素大小/搜索半径，利用最远点采样绕过学习检测器，并采用基于块的比例归一化以获得一致的坐标边界。此外，还提出了一种多尺度补丁描述符生成和分层内点搜索策略来增强在多种场景中的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;BUFFER-X能够在没有先验信息或手动参数调整的情况下实现显著泛化性能，并提出了一个包含11个数据集的新型通用性基准，覆盖了不同的室内/室外场景和传感器模式。&lt;h4&gt;结论&lt;/h4&gt; BUFFER-X通过解决限制因素提供了在各种环境下零样本点云配准的能力，展示了强大的通用性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;最近基于深度学习的点云注册技术虽然提高了泛化能力，但大多数方法仍需针对每种新环境进行再训练或手动参数调整。本文指出了阻碍这些方法进一步发展的三个关键因素：（a）对特定环境体素大小和搜索半径的依赖；（b）基于学习的关键点检测器在跨域场景中的表现不佳；以及（c）直接使用原始坐标导致比例差异问题。为了解决这些问题，我们提出了一种名为BUFFER-X的零样本注册流程，通过自适应调整体素尺寸/搜索距离、利用最远点采样跳过学习式检测步骤和实施基于补丁的比例归一化来应对这些挑战。同时引入了多尺度块描述符生成及跨比例层级内点查找机制以进一步提高其在多样性环境下的鲁棒性表现。此外，我们设计了一个包含11个数据集的新型通用基准测试，涵盖多种室内/室外场景和传感器类型，证明BUFFER-X能够在没有先验知识或手动调整参数的情况下显著提升泛化能力。我们的代码可在https://github.com/MIT-SPARK/BUFFER-X获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in deep learning-based point cloud registration have improvedgeneralization, yet most methods still require retraining or manual parametertuning for each new environment. In this paper, we identify three key factorslimiting generalization: (a) reliance on environment-specific voxel size andsearch radius, (b) poor out-of-domain robustness of learning-based keypointdetectors, and (c) raw coordinate usage, which exacerbates scale discrepancies.To address these issues, we present a zero-shot registration pipeline calledBUFFER-X by (a) adaptively determining voxel size/search radii, (b) usingfarthest point sampling to bypass learned detectors, and (c) leveragingpatch-wise scale normalization for consistent coordinate bounds. In particular,we present a multi-scale patch-based descriptor generation and a hierarchicalinlier search across scales to improve robustness in diverse scenes. We alsopropose a novel generalizability benchmark using 11 datasets that cover variousindoor/outdoor scenarios and sensor modalities, demonstrating that BUFFER-Xachieves substantial generalization without prior information or manualparameter tuning for the test datasets. Our code is available athttps://github.com/MIT-SPARK/BUFFER-X.</description>
      <author>example@mail.com (Minkyun Seo, Hyungtae Lim, Kanghee Lee, Luca Carlone, Jaesik Park)</author>
      <guid isPermaLink="false">2503.07940v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>GBlobs: Explicit Local Structure via Gaussian Blobs for Improved Cross-Domain LiDAR-based 3D Object Detection</title>
      <link>http://arxiv.org/abs/2503.08639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;基于LiDAR的3D检测器在训练时需要大量的数据集，并且难以适应新颖的应用领域。域泛化(DG)旨在通过训练能够抵御这种领域转移影响的检测器来解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;为了减轻对全局几何特征过度依赖导致的问题，本文提出了一种新的方法，即利用局部点云结构进行DG，特别是通过用高斯团(Gaussian blobs, GBlobs)编码点云邻域。&lt;h4&gt;方法&lt;/h4&gt;该方法包括将GBlobs直接集成到现有的检测器中，而无需任何额外的参数。这种方法提高了在跨领域应用中的性能，并且不会牺牲原始领域的表现。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用GBlobs，在具有挑战性的单一来源DG基准测试上超过了当前最先进的技术水平：Waymo-&gt;KITTI超过21 mAP，KITTI-&gt;Waymo超过13 mAP，nuScenes-&gt;KITTI超过12 mAP。此外，在多源域泛化方面，GBlobs在Waymo、KITTI和ONCE数据集上的表现也优于现有最佳水平。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过利用局部点云结构来提高LiDAR基3D检测器的跨领域适应性，证明了其有效性和高效性。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的三维检测器在训练时需要大量的数据集，并且难以推广到新的应用场景。域泛化（DG）试图通过训练能够抵御这种领域迁移影响的检测器来解决这个问题。当前的方法主要依赖于全局几何特征作为输入，过度依赖这些特征会导致3D检测器过分关注物体的位置和绝对位置，从而导致跨领域的性能不佳。为了缓解这一问题，我们提出了一种新的方法，即利用明确的局部点云结构进行DG，在具体操作中使用高斯团（GBlobs）编码点云邻域。我们的新方案不仅高效而且无需额外参数。通过简单地将GBlobs集成到现有检测器中，我们在具有挑战性的单一来源DG基准上超过了当前的最佳性能：Waymo-&gt;KITTI超过21 mAP，KITTI-&gt;Waymo超过13 mAP，nuScenes-&gt;KITTI超过12 mAP，并且没有牺牲领域的内在表现。另外，在多源域泛化方面，GBlobs在Waymo、KITTI和ONCE数据集上的性能分别优于现有最佳水平17、12和5 mAP。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based 3D detectors need large datasets for training, yet they struggleto generalize to novel domains. Domain Generalization (DG) aims to mitigatethis by training detectors that are invariant to such domain shifts. Current DGapproaches exclusively rely on global geometric features (point cloud Cartesiancoordinates) as input features. Over-reliance on these global geometricfeatures can, however, cause 3D detectors to prioritize object location andabsolute position, resulting in poor cross-domain performance. To mitigatethis, we propose to exploit explicit local point cloud structure for DG, inparticular by encoding point cloud neighborhoods with Gaussian blobs, GBlobs.Our proposed formulation is highly efficient and requires no additionalparameters. Without any bells and whistles, simply by integrating GBlobs inexisting detectors, we beat the current state-of-the-art in challengingsingle-source DG benchmarks by over 21 mAP (Waymo-&gt;KITTI), 13 mAP(KITTI-&gt;Waymo), and 12 mAP (nuScenes-&gt;KITTI), without sacrificing in-domainperformance. Additionally, GBlobs demonstrate exceptional performance inmulti-source DG, surpassing the current state-of-the-art by 17, 12, and 5 mAPon Waymo, KITTI, and ONCE, respectively.</description>
      <author>example@mail.com (Dušan Malić, Christian Fruhwirth-Reisinger, Samuel Schulter, Horst Possegger)</author>
      <guid isPermaLink="false">2503.08639v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>A systematic literature review of unsupervised learning algorithms for anomalous traffic detection based on flows</title>
      <link>http://arxiv.org/abs/2503.08293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This article has been accepted for publication in Logic Journal of  the IGPL Published by Oxford University Press&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对无监督学习算法在检测网络流异常中的应用进行了系统的文献回顾，遵循PRISMA指南。共审阅了63篇科学论文，并深入分析了其中13篇。&lt;h4&gt;背景&lt;/h4&gt;随着连接到互联网的设备数量不断增长，以及随之而来的网络安全威胁增加，使得需要对网络流量进行分析以识别恶意活动成为必要。传统基于数据包的方法由于大型网络中的通信量过于庞大而不切实际。&lt;h4&gt;目的&lt;/h4&gt;探讨无监督学习模型在检测未知威胁方面的应用及其与流数据分析结合的有效性。&lt;h4&gt;方法&lt;/h4&gt;采用PRISMA指南，系统回顾了63篇关于使用无监督算法进行异常流量检测的科学论文，并深入分析了其中13篇。&lt;h4&gt;主要发现&lt;/h4&gt;自编码器（autoencoder）是最常用的模型之一，其次是支持向量机(SVM)，ALAD或自我组织映射(Self-Organizing Map, SOM)。所有用于异常检测的数据集都已被收集，其中包括一些专门针对物联网(IoT)的数据集和从蜜罐(honeypots)实际采集的现实数据。&lt;h4&gt;结论&lt;/h4&gt;无监督学习方法结合流数据分析能有效识别新的威胁，并且未来5G网络中流量将显著增加的情况下这些技术尤为重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The constant increase of devices connected to the Internet, and therefore ofcyber-attacks, makes it necessary to analyze network traffic in order torecognize malicious activity. Traditional packet-based analysis methods areinsufficient because in large networks the amount of traffic is so high that itis unfeasible to review all communications. For this reason, flows is asuitable approach for this situation, which in future 5G networks will have tobe used, as the number of packets will increase dramatically. If this is alsocombined with unsupervised learning models, it can detect new threats for whichit has not been trained. This paper presents a systematic review of theliterature on unsupervised learning algorithms for detecting anomalies innetwork flows, following the PRISMA guideline. A total of 63 scientificarticles have been reviewed, analyzing 13 of them in depth. The resultsobtained show that autoencoder is the most used option, followed by SVM, ALAD,or SOM. On the other hand, all the datasets used for anomaly detection havebeen collected, including some specialised in IoT or with real data collectedfrom honeypots.</description>
      <author>example@mail.com (Alberto Miguel-Diez, Adrián Campazas-Vega, Claudia Álvarez-Aparicio, Gonzalo Esteban-Costales, Ángel Manuel Guerrero-Higueras)</author>
      <guid isPermaLink="false">2503.08293v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Understanding and Mitigating Distribution Shifts For Machine Learning Force Fields</title>
      <link>http://arxiv.org/abs/2503.08674v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;机器学习势能场（MLFFs）作为昂贵的量子力学分子模拟的一种替代方法显示出前景。为了理解MLFF在训练分布之外的一般化能力，研究团队进行了诊断性实验以揭示常见的分布偏移，并提出了两种减轻分布偏移的方法。&lt;h4&gt;背景&lt;/h4&gt;MLFFs作为一种便宜且有效的计算工具，用于取代成本高昂的量子力学分子模拟，在化学领域具有重要应用价值。然而，它们在面对不同于训练数据的新系统时的表现存在挑战。&lt;h4&gt;目的&lt;/h4&gt;通过诊断性实验揭示并理解MLFF中常见的分布偏移，并提出减轻这些偏移的方法以改善MLFF的一般化能力。&lt;h4&gt;方法&lt;/h4&gt;1. 进行了化学数据集上的诊断实验来识别和表征常见分布偏移。2. 提出了一种基于谱图论的测试时间细化策略，通过调整测试图形边缘使其与训练期间看到的结构对齐。3. 采用了一个辅助目标（如廉价物理先验）来进行梯度步骤改进未见过系统的表示。&lt;h4&gt;主要发现&lt;/h4&gt;当前监督学习方法不足以正则化MLFFs，导致过度拟合和不良分布外系统表示。测试时间细化策略能够显著减少MLFF在分布外系统上的错误。&lt;h4&gt;结论&lt;/h4&gt;尽管MLFF有潜力建模多样化的化学空间，但现有训练方式未能有效利用其能力。所提出的测试时间细化方法为未来的MLFF提供了一个明确的基准以评估它们的一般化性能。&lt;h4&gt;翻译&lt;/h4&gt;机器学习势能场（MLFFs）作为一种替代昂贵的量子力学分子模拟的方法展现出巨大潜力。鉴于化学空间的多样性以及生成新数据的成本，理解MLFF在训练分布之外如何泛化变得至关重要。为了表征并更好地理解MLFF中的分布偏移，研究团队进行了诊断性实验，在大量基础模型上揭示了对现有方法构成重大挑战的常见偏移现象。基于这些观察结果，研究人员提出当前监督学习方法不足以正则化MLFFs，导致过度拟合和学习不良的未见过系统表示。随后，他们提出了两种减轻分布偏移的新方法：一种是基于谱图论的测试时间细化策略，该策略通过调整测试图形边缘来使其与训练期间看到的结构对齐；另一种是在测试时间改进未见过系统的表示，通过使用一个辅助目标（如廉价物理先验）来进行梯度步骤。这些测试时间细化策略显著减少了MLFF在分布外系统上的错误，表明MLFF有能力并可以朝向建模多样化的化学空间发展，但目前的训练方式未能有效发挥其潜力。实验为评估下一代MLFF的一般化能力建立了明确基准。代码可在https://tkreiman.github.io/projects/mlff_distribution_shifts/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine Learning Force Fields (MLFFs) are a promising alternative toexpensive ab initio quantum mechanical molecular simulations. Given thediversity of chemical spaces that are of interest and the cost of generatingnew data, it is important to understand how MLFFs generalize beyond theirtraining distributions. In order to characterize and better understanddistribution shifts in MLFFs, we conduct diagnostic experiments on chemicaldatasets, revealing common shifts that pose significant challenges, even forlarge foundation models trained on extensive data. Based on these observations,we hypothesize that current supervised training methods inadequately regularizeMLFFs, resulting in overfitting and learning poor representations ofout-of-distribution systems. We then propose two new methods as initial stepsfor mitigating distribution shifts for MLFFs. Our methods focus on test-timerefinement strategies that incur minimal computational cost and do not useexpensive ab initio reference labels. The first strategy, based on spectralgraph theory, modifies the edges of test graphs to align with graph structuresseen during training. Our second strategy improves representations forout-of-distribution systems at test-time by taking gradient steps using anauxiliary objective, such as a cheap physical prior. Our test-time refinementstrategies significantly reduce errors on out-of-distribution systems,suggesting that MLFFs are capable of and can move towards modeling diversechemical spaces, but are not being effectively trained to do so. Ourexperiments establish clear benchmarks for evaluating the generalizationcapabilities of the next generation of MLFFs. Our code is available athttps://tkreiman.github.io/projects/mlff_distribution_shifts/.</description>
      <author>example@mail.com (Tobias Kreiman, Aditi S. Krishnapriyan)</author>
      <guid isPermaLink="false">2503.08674v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>A Study to Evaluate the Impact of LoRA Fine-tuning on the Performance of Non-functional Requirements Classification</title>
      <link>http://arxiv.org/abs/2503.07927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;在软件开发生命周期中对非功能性需求（NFR）进行分类非常重要。受迁移学习理论的启发，研究人员应用强大的预训练模型来进行NFR分类。&lt;h4&gt;目的&lt;/h4&gt;研究低秩适应（LoRA）微调方法在基于提示的学习中的影响，以降低执行成本而不显著损失分类效果。&lt;h4&gt;方法&lt;/h4&gt;将LoRA微调方法应用于基于提示学习的NFR分类任务中，并进行实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，LoRA可以大幅减少执行成本（最高可达68%），同时只导致2%-3%的分类效果下降。此外，在更大规模的数据集和预训练模型的情况下，LoRA同样具有实用性。&lt;h4&gt;结论&lt;/h4&gt;LoRA在复杂度更高的分类场景中仍然能够保持高效且有效，为NFR分类提供了一种新的可行方法。&lt;h4&gt;翻译&lt;/h4&gt;将非功能性需求（NFR）分类应用于软件开发过程中至关重要。受迁移学习理论的启发，研究人员采用强大的预训练模型进行NFR分类任务。然而，由于涉及到大量的参数更新问题（例如，GPT-3拥有1750亿个可训练参数），完整的微调操作通常难以实现。本文将低秩适应（LoRA）细粒度调整方法应用于基于提示的学习的NFR分类中，并研究了其影响。实验结果表明，LoRA能够显著降低执行成本（最高可达68%减少），同时保持分类效果几乎不变（仅下降2%-3%）。结果显示，在涉及更大数据集和预训练模型的情况下，LoRA可以为更复杂的分类场景提供实用的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classifying Non-Functional Requirements (NFRs) in software development lifecycle is critical. Inspired by the theory of transfer learning, researchersapply powerful pre-trained models for NFR classification. However, fullfine-tuning by updating all parameters of the pre-trained models is oftenimpractical due to the huge number of parameters involved (e.g., 175 billiontrainable parameters in GPT-3). In this paper, we apply Low-Rank Adaptation(LoRA) fine-tuning approach into NFR classification based on prompt-basedlearning to investigate its impact. The experiments show that LoRA cansignificantly reduce the execution cost (up to 68% reduction) without too muchloss of effectiveness in classification (only 2%-3% decrease). The results showthat LoRA can be practical in more complicated classification cases with largerdataset and pre-trained models.</description>
      <author>example@mail.com (Xia Li, Allen Kim)</author>
      <guid isPermaLink="false">2503.07927v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>SignRep: Enhancing Self-Supervised Sign Representations</title>
      <link>http://arxiv.org/abs/2503.08529v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;手语表示学习由于手势的复杂时空特性以及标签数据集的稀缺性而面临独特的挑战。&lt;h4&gt;背景&lt;/h4&gt;现有的方法依赖于在通用视觉任务上预训练的模型，但这些模型缺乏特定的手势特征；或者采用复杂的多模态和多分支架构。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，本文介绍了一种可扩展、自我监督的学习框架，用于手势表示学习。&lt;h4&gt;方法&lt;/h4&gt;{'1': '在训练RGB模型时利用重要的归纳（手势）先验。这包括在预训练遮挡自编码器的同时利用基于骨架的简单但重要的线索。', '2': '手语特定的先验、特征正则化和对抗性风格无关损失提供了强大的骨干网络。', '3': '该模型在推理过程中不需要骨骼关键点，从而避免了下游任务中基于关键点模型的局限性。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'1': '经过微调后，在WLASL、ASL-Citizen和NMFs-CSL数据集上的手势识别性能达到了最先进的水平。', '2': '该冻结后的模型在手势字典检索和手势翻译方面表现优异，超过了标准的MAE预训练方法以及基于骨架表示的手势表示。', '3': '它还减少了现有手势翻译模型的计算成本，同时在Phoenix2014T、CSL-Daily和How2Sign数据集上保持了强大的性能。'}&lt;h4&gt;结论&lt;/h4&gt;通过引入一种新的可扩展自我监督框架来解决手语表示学习的独特挑战。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的可伸缩且自我监督的框架，用于克服手语识别中时空复杂性与标记数据稀缺带来的困难，并在多个数据集上实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign language representation learning presents unique challenges due to thecomplex spatio-temporal nature of signs and the scarcity of labeled datasets.Existing methods often rely either on models pre-trained on general visualtasks, that lack sign-specific features, or use complex multimodal andmulti-branch architectures. To bridge this gap, we introduce a scalable,self-supervised framework for sign representation learning. We leverageimportant inductive (sign) priors during the training of our RGB model. To dothis, we leverage simple but important cues based on skeletons whilepretraining a masked autoencoder. These sign specific priors alongside featureregularization and an adversarial style agnostic loss provide a powerfulbackbone. Notably, our model does not require skeletal keypoints duringinference, avoiding the limitations of keypoint-based models during downstreamtasks. When finetuned, we achieve state-of-the-art performance for signrecognition on the WLASL, ASL-Citizen and NMFs-CSL datasets, using a simplerarchitecture and with only a single-modality. Beyond recognition, our frozenmodel excels in sign dictionary retrieval and sign translation, surpassingstandard MAE pretraining and skeletal-based representations in retrieval. Italso reduces computational costs for training existing sign translation modelswhile maintaining strong performance on Phoenix2014T, CSL-Daily and How2Sign.</description>
      <author>example@mail.com (Ryan Wong, Necati Cihan Camgoz, Richard Bowden)</author>
      <guid isPermaLink="false">2503.08529v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>RAG-Adapter: A Plug-and-Play RAG-enhanced Framework for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2503.08576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 36 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为RAG-Adapter的框架，用于提高多模态大语言模型在视频理解任务中的测试准确性。&lt;h4&gt;背景&lt;/h4&gt;随着多模态大型语言模型(video understanding)的发展，新的基准测试(如Video-MME和MLVU)被引入来评估这些模型的能力。然而，现有的帧采样方法导致了大量信息丢失，影响评价的准确性和有效性。&lt;h4&gt;目的&lt;/h4&gt;提出一种减少信息损失并提高视频理解任务中多模态大型语言模型性能的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了RAG-Adapter框架，并利用分组监督对比学习(GCL)进一步优化其采样效果。构建了MMAT数据集以进行更有效的训练和微调。&lt;h4&gt;主要发现&lt;/h4&gt;通过在多个视频理解基准测试中验证，与均匀帧采样相比，RAG-Adapter方法显著提高了多模态大型语言模型的准确性（例如，在Video-MME上GPT-4的准确率提高了9.3%）。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为长视频理解和评估提供了一种更有效的测试策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal Large Language Models (MLLMs) capable of video understanding areadvancing rapidly. To effectively assess their video comprehensioncapabilities, long video understanding benchmarks, such as Video-MME and MLVU,are proposed. However, these benchmarks directly use uniform frame sampling fortesting, which results in significant information loss and affects the accuracyof the evaluations in reflecting the true abilities of MLLMs. To address this,we propose RAG-Adapter, a plug-and-play framework that reduces information lossduring testing by sampling frames most relevant to the given question.Additionally, we introduce a Grouped-supervised Contrastive Learning (GCL)method to further enhance sampling effectiveness of RAG-Adapter throughfine-tuning on our constructed MMAT dataset. Finally, we test numerous baselineMLLMs on various video understanding benchmarks, finding that RAG-Adaptersampling consistently outperforms uniform sampling (e.g., Accuracy of GPT-4oincreases by 9.3 percent on Video-MME), providing a more accurate testingmethod for long video benchmarks.</description>
      <author>example@mail.com (Xichen Tan, Yunfan Ye, Yuanjing Luo, Qian Wan, Fang Liu, Zhiping Cai)</author>
      <guid isPermaLink="false">2503.08576v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>JiSAM: Alleviate Labeling Burden and Corner Case Problems in Autonomous Driving via Minimal Real-World Data</title>
      <link>http://arxiv.org/abs/2503.08422v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种名为JiSAM的插件式方法被提出，用于提高基于深度学习的自动驾驶感知系统在真实世界数据中的性能。&lt;h4&gt;背景&lt;/h4&gt;依赖于激光雷达的真实标记数据对于自动驾驶系统的感知能力有限，并且难以标注罕见交通参与者的场景。相比之下，在模拟器中生成带标签的数据更容易，但直接使用这些数据改善真实世界的感知效果具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;克服在模拟数据集中采样效率低以及从模拟到现实的差距这两大难题。&lt;h4&gt;方法&lt;/h4&gt;提出了JiSAM（抖动增强、领域意识主干和基于内存的扇区对齐），旨在提高仿真数据在真实世界自动驾驶感知中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用最先进的3D物体检测器，JiSAM能够利用模拟数据，并且仅依靠2.5%的真实标记数据就能达到几乎与所有真实标记数据训练模型相同的性能。此外，在未标注对象上还获得了超过15mAP的提升。&lt;h4&gt;结论&lt;/h4&gt;JiSAM提供了一种有效的方法来增强基于深度学习的自动驾驶系统的感知能力，通过更好地利用模拟生成的数据和少量的真实世界样本。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的自主驾驶（AD）感知为安全且环保的交通提供了令人兴奋的画面。然而，在LiDAR感知中过度依赖真实标注数据限制了道路试验规模。3D现实世界的数据显示标注耗时且耗能，并缺乏如罕见交通参与者等边缘情况。相比之下，像CARLA这样的模拟器生成带有标签和边缘案例的点云轻而易举。但是将合成点云引入以改进现实感知具有挑战性。这源于两个难题：1）仿真数据集中的样本效率；2）从仿真到实际应用之间的差距。为了克服这两项挑战，我们提出了一种插件式方法称为JiSAM（抖动增强、领域意识主干和基于内存的扇区对齐）。在著名AD数据集NuScenes上进行的广泛实验表明，利用最先进的3D对象检测器，JiSAM能够仅使用2.5%可用的真实标签来达到与所有真实数据训练模型相当的效果。此外，在未标注在实际训练集中的对象中获得了超过15mAP的成绩。我们将发布模型和代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep-learning-based autonomous driving (AD) perception introduces a promisingpicture for safe and environment-friendly transportation. However, theover-reliance on real labeled data in LiDAR perception limits the scale ofon-road attempts. 3D real world data is notoriously time-and-energy-consumingto annotate and lacks corner cases like rare traffic participants. On thecontrary, in simulators like CARLA, generating labeled LiDAR point clouds withcorner cases is a piece of cake. However, introducing synthetic point clouds toimprove real perception is non-trivial. This stems from two challenges: 1)sample efficiency of simulation datasets 2) simulation-to-real gaps. Toovercome both challenges, we propose a plug-and-play method called JiSAM ,shorthand for Jittering augmentation, domain-aware backbone and memory-basedSectorized AlignMent. In extensive experiments conducted on the famous ADdataset NuScenes, we demonstrate that, with SOTA 3D object detector, JiSAM isable to utilize the simulation data and only labels on 2.5% available real datato achieve comparable performance to models trained on all real data.Additionally, JiSAM achieves more than 15 mAPs on the objects not labeled inthe real training set. We will release models and codes.</description>
      <author>example@mail.com (Runjian Chen, Wenqi Shao, Bo Zhang, Shaoshuai Shi, Li Jiang, Ping Luo)</author>
      <guid isPermaLink="false">2503.08422v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>SANDRO: a Robust Solver with a Splitting Strategy for Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2503.07743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the IEEE International Conference on Robotics and  Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;点云配准在计算机视觉和机器人领域，特别是导航中是一个关键问题。当前的方法往往面对高异常值率或需要很长时间才能收敛到合适解决方案时会失败。&lt;h4&gt;背景&lt;/h4&gt;点云配准是计算机视觉和机器人技术中的一个核心问题，在导航等领域尤为重要。现有的方法通常无法有效处理高异常值率或者非对称分布的异常值，并且在存在大量异常值和点云对称性的情况下难以快速收敛到合适的解。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的点云配准算法SANDRO，以解决现有方法存在的问题，特别是在高异常值率或复杂场景下的挑战。该算法结合了迭代加权最小二乘法框架和非凸鲁棒损失函数，并设计了一种分割策略来处理高比例的异常值。&lt;h4&gt;方法&lt;/h4&gt;SANDRO通过结合迭代重加权最小二乘（IRLS）框架以及具有渐进非凸性的鲁棒损失函数，提供了一个新的点云配准解决方案。算法还包含一种专门应对高异常率和非对称分布的方法——分割策略。&lt;h4&gt;主要发现&lt;/h4&gt;SANDRO相较于现有的先进方法，在Redwood真实数据集上显示出20%的成功率提高，并在合成数据测试中提高了60%，表现出更好的鲁棒性和更快的收敛速度，特别是在面对高比例的异常值时更加显著。这种改进使得点云配准任务更高效和准确。&lt;h4&gt;结论&lt;/h4&gt;SANDRO算法通过结合特定策略，成功解决了现有技术无法有效处理高异常率点云的问题，为解决复杂场景下的点云配准挑战提供了一种强有力的新工具。&lt;h4&gt;翻译&lt;/h4&gt;点云注册在计算机视觉和机器人领域是关键问题之一，尤其是在导航方面。当前方法在面对大量异常值或长时间收敛到合适解决方案时往往会失败。我们提出一种名为SANDRO的新型算法来解决这些问题，该算法结合了迭代加权最小二乘（IRLS）框架和具有渐进非凸性的鲁棒损失函数，并设计了一种分割策略以处理高比例的异常值。SANDRO在难以收敛的情境下表现优异，例如存在大量异常值或点云对称性时。与现有的先进方法相比，在Redwood真实数据集上实现了20%的成功率提升，在合成数据测试中提高了60%，展示了显著的优势和潜在的应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is a critical problem in computer vision androbotics, especially in the field of navigation. Current methods often failwhen faced with high outlier rates or take a long time to converge to asuitable solution. In this work, we introduce a novel algorithm for point cloudregistration called SANDRO (Splitting strategy for point cloud Alignment usingNon-convex anD Robust Optimization), which combines an Iteratively ReweightedLeast Squares (IRLS) framework with a robust loss function with graduatednon-convexity. This approach is further enhanced by a splitting strategydesigned to handle high outlier rates and skewed distributions of outliers.SANDRO is capable of addressing important limitations of existing methods, asin challenging scenarios where the presence of high outlier rates and pointcloud symmetries significantly hinder convergence. SANDRO achieves superiorperformance in terms of success rate when compared to the state-of-the-artmethods, demonstrating a 20% improvement from the current state of the art whentested on the Redwood real dataset and 60% improvement when tested on syntheticdata.</description>
      <author>example@mail.com (Michael Adlerstein, João Carlos Virgolino Soares, Angelo Bratta, Claudio Semini)</author>
      <guid isPermaLink="false">2503.07743v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Elderly Activity Recognition in the Wild: Results from the EAR Challenge</title>
      <link>http://arxiv.org/abs/2503.07821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2 pages, EAR-CV4Smalls@WACV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了解决老年动作识别（EAR）挑战的解决方案，该挑战是WACV 2025计算机视觉小型研讨会的一部分。&lt;h4&gt;背景&lt;/h4&gt;竞赛专注于识别老年人执行的日常生活活动（ADLs），涵盖六个动作类别，并且使用了一个多样化的数据集。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于最先进的动作识别模型的方法，通过迁移学习在特定于老人的数据集上进行微调以增强适应性。&lt;h4&gt;方法&lt;/h4&gt;为了提高泛化能力和减轻数据偏差的影响，我们精心从多个公开资源中收集训练数据，并应用了目标预处理技术。&lt;h4&gt;主要发现&lt;/h4&gt;我们的解决方案在公共排行榜上的准确率为0.81455，显示出其在分类老年人活动方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文的源代码可在https://github.com/ffyyytt/EAR-WACV25-DAKiet-TSM公开获取。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents our solution for the Elderly Action Recognition (EAR)Challenge, part of the Computer Vision for Smalls Workshop at WACV 2025. Thecompetition focuses on recognizing Activities of Daily Living (ADLs) performedby the elderly, covering six action categories with a diverse dataset. Ourapproach builds upon a state-of-the-art action recognition model, fine-tunedthrough transfer learning on elderly-specific datasets to enhance adaptability.To improve generalization and mitigate dataset bias, we carefully curatedtraining data from multiple publicly available sources and applied targetedpre-processing techniques. Our solution currently achieves 0.81455 accuracy onthe public leaderboard, highlighting its effectiveness in classifying elderlyactivities. Source codes are publicly available athttps://github.com/ffyyytt/EAR-WACV25-DAKiet-TSM.</description>
      <author>example@mail.com (Anh-Kiet Duong)</author>
      <guid isPermaLink="false">2503.07821v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Parametric Point Cloud Completion for Polygonal Surface Reconstruction</title>
      <link>http://arxiv.org/abs/2503.08363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的点云补全方法PaCo，通过恢复参数化的几何原语来改善多边形表面重建的质量。&lt;h4&gt;背景&lt;/h4&gt;现有的多边形表面重建技术严重依赖于输入的完整性，并且在处理不完整的点云时效果不佳。虽然当前的点云补全技术可以恢复缺失的点，但它们并未优化用于多边形表面重建。&lt;h4&gt;目的&lt;/h4&gt;引入参数化补全（parametric completion）的概念，旨在解决现有方法对于参数表示的忽视问题，并提供一种新的点云补全策略。&lt;h4&gt;方法&lt;/h4&gt;提出的方法PaCo利用平面代理来恢复平面参数和内点集合，从而有效传达几何结构。这种方法特别适用于数据高度不完整的情况。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在ABC数据集上的全面评估中表现出色，证明了其优越性能，并为从不完整数据进行多边形表面重建设定了新的标准。&lt;h4&gt;结论&lt;/h4&gt;通过引入参数化补全概念和PaCo算法，提供了一种更有效的处理多边形表面重建的方法。&lt;h4&gt;翻译&lt;/h4&gt;现有用于多边形表面重构的点云补全技术严重依赖于输入的完整性，并且在应对不完整数据时效果不佳。我们提出了一个新框架——参数化补全（parametric completion），它通过恢复描述几何结构的参数化原语来改善多边形表面重建的质量，而非仅仅恢复缺失的点。这种方法特别适用于挑战性场景中的高度不完整数据处理，并且在ABC数据集上的评估展示了其卓越性能和新的技术标准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing polygonal surface reconstruction methods heavily depend on inputcompleteness and struggle with incomplete point clouds. We argue that whilecurrent point cloud completion techniques may recover missing points, they arenot optimized for polygonal surface reconstruction, where the parametricrepresentation of underlying surfaces remains overlooked. To address this gap,we introduce parametric completion, a novel paradigm for point cloudcompletion, which recovers parametric primitives instead of individual pointsto convey high-level geometric structures. Our presented approach, PaCo,enables high-quality polygonal surface reconstruction by leveraging planeproxies that encapsulate both plane parameters and inlier points, provingparticularly effective in challenging scenarios with highly incomplete data.Comprehensive evaluations of our approach on the ABC dataset establish itseffectiveness with superior performance and set a new standard for polygonalsurface reconstruction from incomplete data. Project page:https://parametric-completion.github.io.</description>
      <author>example@mail.com (Zhaiyu Chen, Yuqing Wang, Liangliang Nan, Xiao Xiang Zhu)</author>
      <guid isPermaLink="false">2503.08363v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Detect Objects from Multi-Agent LiDAR Scans without Manual Labels</title>
      <link>http://arxiv.org/abs/2503.08421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新颖的无监督3D物体检测方法DOtA，该方法通过多智能体协作数据集中的互补观测进行初始化和标签生成，提高了无监督物体检测任务的表现。&lt;h4&gt;背景&lt;/h4&gt;在无监督3D对象检测中，由于数据稀疏性和视角限制，基于聚类的方法通常会产生低质量的伪标签。而利用多个代理之间共享补充观察值的数据集可以帮助克服这一瓶颈。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需外部标签就能从多智能体激光雷达扫描中检测物体的新方法，并提高无监督3D对象检测任务的表现。&lt;h4&gt;方法&lt;/h4&gt;1. 利用协作智能体之间的内部共享自姿态和形状初始化探测器。2. 通过代理之间互补的观察进行多层次编码，区分高质量和低质量标签。3. 使用生成的标签作为提示来指导正确的特征学习过程。&lt;h4&gt;主要发现&lt;/h4&gt;DOtA在V2V4Real和OPV2V数据集上的实验结果表明其性能优于现有的无监督3D物体检测方法。此外，在各种协作感知框架下验证了DOtA标签的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种高效的无监督学习方案，利用多智能体的共享信息提升了3D对象检测的质量和准确性。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述的是一个名为DOtA的新颖方法，它采用了一种不依赖于外部标签就能从多代理的LiDAR扫描中进行3D物体检测的方法。通过内部共享的姿态和形状初始化探测器，并利用神经网络的泛化能力来推断初步标签，然后结合多代理之间的互补观察来进行多层次编码和解码高质量与低质量的标签，进一步指导正确的特征学习过程以提升无监督物体检测任务的表现。实验显示此方法在两个数据集上超越了现有最佳方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised 3D object detection serves as an important solution for offline3D object annotation. However, due to the data sparsity and limited views, theclustering-based label fitting in unsupervised object detection often generateslow-quality pseudo-labels. Multi-agent collaborative dataset, which involvesthe sharing of complementary observations among agents, holds the potential tobreak through this bottleneck. In this paper, we introduce a novel unsupervisedmethod that learns to Detect Objects from Multi-Agent LiDAR scans, termed DOtA,without using labels from external. DOtA first uses the internally sharedego-pose and ego-shape of collaborative agents to initialize the detector,leveraging the generalization performance of neural networks to inferpreliminary labels. Subsequently,DOtA uses the complementary observationsbetween agents to perform multi-scale encoding on preliminary labels, thendecodes high-quality and low-quality labels. These labels are further used asprompts to guide a correct feature learning process, thereby enhancing theperformance of the unsupervised object detection task. Extensive experiments onthe V2V4Real and OPV2V datasets show that our DOtA outperforms state-of-the-artunsupervised 3D object detection methods. Additionally, we also validate theeffectiveness of the DOtA labels under various collaborative perceptionframeworks.The code is available at https://github.com/xmuqimingxia/DOtA.</description>
      <author>example@mail.com (Qiming Xia, Wenkai Lin, Haoen Xiang, Xun Huang, Siheng Chen, Zhen Dong, Cheng Wang, Chenglu Wen)</author>
      <guid isPermaLink="false">2503.08421v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Aligning Text to Image in Diffusion Models is Easier Than You Think</title>
      <link>http://arxiv.org/abs/2503.08250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究了生成模型在文本和图像对齐上的改进，并提出了一种新的轻量级对比学习策略SoftREPA，以提高预训练模型的表示对齐。&lt;h4&gt;背景&lt;/h4&gt;尽管最近在生成模型方面取得了进步，但仍存在一些文本和图像之间的残余不对齐问题。传统的方法通过微调模型来解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;从表征对齐的角度重新审视这一挑战，并提出一种新的方法来改善这种不对齐。&lt;h4&gt;方法&lt;/h4&gt;引入了一种称为SoftREPA的轻量级对比学习策略，该策略利用软文本令牌，可以在预训练模型上高效地进行细调。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析表明，所提方法可以显式增加文本和图像表示之间的互信息，从而提高语义一致性。实验结果验证了在文本到图像生成任务以及文本引导的图像编辑任务上的有效性。&lt;h4&gt;结论&lt;/h4&gt;SoftREPA策略有效地提高了预训练模型的表征对齐能力，同时保持低计算开销。&lt;h4&gt;翻译&lt;/h4&gt;最近关于生成建模的进步显著提升了文本与图像之间的对齐。尽管如此，在两者表示之间仍存在残余不对齐的问题。尽管有许多方法试图通过使用各种奖励模型进行微调来解决此问题，但从表征对齐的角度重新审视这一挑战是有必要的。研究者们认为传统的文本到图像(T2I)扩散模型训练方式次优，并提出了一种利用正负样本的对比学习策略，以更高效地提高表示对齐质量。引入了轻量级的方法SoftREPA，在不显著增加计算成本的情况下改进预训练模型的表征对齐能力。实验结果表明该方法在提高T2I生成模型语义一致性方面有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While recent advancements in generative modeling have significantly improvedtext-image alignment, some residual misalignment between text and imagerepresentations still remains. Although many approaches have attempted toaddress this issue by fine-tuning models using various reward models, etc., werevisit the challenge from the perspective of representation alignment-anapproach that has gained popularity with the success of REPresentationAlignment (REPA). We first argue that conventional text-to-image (T2I)diffusion models, typically trained on paired image and text data (i.e.,positive pairs) by minimizing score matching or flow matching losses, issuboptimal from the standpoint of representation alignment. Instead, a betteralignment can be achieved through contrastive learning that leverages bothpositive and negative pairs. To achieve this efficiently even with pretrainedmodels, we introduce a lightweight contrastive fine tuning strategy calledSoftREPA that uses soft text tokens. This approach improves alignment withminimal computational overhead by adding fewer than 1M trainable parameters tothe pretrained model. Our theoretical analysis demonstrates that our methodexplicitly increases the mutual information between text and imagerepresentations, leading to enhanced semantic consistency. Experimental resultsacross text-to-image generation and text-guided image editing tasks validatethe effectiveness of our approach in improving the semantic consistency of T2Igenerative models.</description>
      <author>example@mail.com (Jaa-Yeon Lee, Byunghee Cha, Jeongsol Kim, Jong Chul Ye)</author>
      <guid isPermaLink="false">2503.08250v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Wi-Fi Sensing Generalizability: Taxonomy, Techniques, Datasets, and Future Research Prospects</title>
      <link>http://arxiv.org/abs/2503.08008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  38 pages, 318 references&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;Wi-Fi感知技术利用无处不在的无线信号实现各种应用，从活动和手势识别到室内定位及健康监测。&lt;h4&gt;目的&lt;/h4&gt;综述有关Wi-Fi感知泛化性的200多篇研究，并按设备部署、信号处理、特征学习和模型部署对其进行分类。分析最先进的技术以减轻环境变化对动作识别的不利影响。&lt;h4&gt;方法&lt;/h4&gt;系统地回顾了用于缓解环境变异性负面影响的技术，同时提供了开源数据集（如Widar3.0, XRF55和XRFv2）的全面概述及其在多模态融合和跨模式任务中的独特特征和适用性。&lt;h4&gt;主要发现&lt;/h4&gt;探讨了新兴的研究方向，包括多模态方法及大型语言模型与Wi-Fi感知技术的集成。&lt;h4&gt;结论&lt;/h4&gt;本次综述旨在为研究人员提供一个有价值的资源，详细介绍当前的方法论、可用的数据集以及进一步研究的前景领域。&lt;h4&gt;翻译&lt;/h4&gt;该论文摘要介绍了Wi-Fi感应技术在活动和手势识别等领域的应用，并讨论了环境变化对该技术的影响。作者通过系统地回顾200多篇相关文献，提供了关于如何减轻这些影响的技术概览，同时还概述了几种开源数据集及其特点和适用性。此外，还探讨了一些新兴的研究方向，如大型语言模型与Wi-Fi感应的结合。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wi-Fi sensing has emerged as a transformative technology that leveragesubiquitous wireless signals to enable a variety of applications ranging fromactivity and gesture recognition to indoor localization and health monitoring.However, the inherent dependency of Wi-Fi signals on environmental conditionsintroduces significant generalization challenges,variations in surroundings,human positions, and orientations often lead to inconsistent signal features,impeding robust action recognition. In this survey, we review over 200 studieson Wi-Fi sensing generalization, categorizing them along the entire sensingpipeline: device deployment, signal processing, feature learning, and modeldeployment. We systematically analyze state-of-the-art techniques, which areemployed to mitigate the adverse effects of environmental variability.Moreover, we provide a comprehensive overview of open-source datasets such asWidar3.0, XRF55, and XRFv2, highlighting their unique characteristics andapplicability for multimodal fusion and cross-modal tasks. Finally, we discussemerging research directions, such as multimodal approaches and the integrationof large language models,to inspire future advancements in this rapidlyevolving field. Our survey aims to serve as a valuable resource forresearchers, offering insights into current methodologies, available datasets,and promising avenues for further investigation.</description>
      <author>example@mail.com (Fei Wang, Tingting Zhang, Bintong Zhao, Libao Xing, Tiantian Wang, Han Ding, Tony Xiao Han)</author>
      <guid isPermaLink="false">2503.08008v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Explainable Autoencoder Design for RSSI-Based Multi-User Beam Probing and Hybrid Precoding</title>
      <link>http://arxiv.org/abs/2503.08267v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种新颖的神经网络（NN）结构，称为“自动混合预编码器”（Auto-HP），以及一种无监督深度学习（DL）方法，用于设计毫米波多用户通信系统的探针束和混合预编码矩阵，在最少训练导频的情况下实现。我们的基于学习的方法利用先前的信道观察结果来实现两个主要目标：设计一组有限数量的探针束并预测离散射频（RF）波束成形矢量。Auto-HP框架以无监督的方式优化探针束，通过创新的神经网络架构，在周围环境中最有前景的空间方向上集中感知能力，同时遵守射频链约束，并使用复数卷积层建模接收信号强度功率测量值。然后，自动编码器在仅基于少量投影接收信号强度指标（RSSIs）的情况下训练直接生成混合架构的RF波束成形矢量，不受预定义代码本的限制。最后，在预测了多用户的RF波束成形矢量后，考虑了多用户干扰来设计基带（BB）数字预编码器。Auto-HP神经网络以端到端（E2E）的方式在无监督学习模式下进行训练，并使用定制的损失函数最大化接收到信号强度。从信息论的角度评估了Auto-HP NN瓶颈层维度的适当性，确保最大数据压缩和可靠的RF波束预测。&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于神经网络的方法用于毫米波多用户通信系统中的探针束设计及混合预编码矩阵的设计。&lt;h4&gt;背景&lt;/h4&gt;传统的射频波束成形方法需要大量的训练导频信号，并且通常依赖于预先定义的代码本来进行波束预测。这种方法在实际操作中可能效率不高，尤其是在需要动态调整和优化的情况下。&lt;h4&gt;目的&lt;/h4&gt;本文旨在开发一种高效的无监督学习框架，用于设计有限数量的有效探针束以及基于少量导频信号预测准确的射频波束成形矢量。&lt;h4&gt;方法&lt;/h4&gt;提出了一个称为Auto-HP的新颖神经网络结构，该结构以创新的方式优化毫米波多用户通信系统中的探针束，并使用复杂的卷积层建模接收信号强度测量值。此外，还开发了一种无监督学习方式的自动编码器框架来直接生成射频波束成形矢量。&lt;h4&gt;主要发现&lt;/h4&gt;Auto-HP神经网络能够以端到端的方式高效地设计出准确且有效的探针束和射频波束成形矢量，同时保证了数据压缩的最大化及可靠的RF预测。此外，该框架在无监督学习模式下训练时表现出良好的性能。&lt;h4&gt;结论&lt;/h4&gt;本文提出的基于Auto-HP神经网络的方法为毫米波多用户通信系统的设计提供了一个有效且高效的解决方案，展示了其在未来无线通信领域的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel neural network (NN) structure referred to as an``Auto-hybrid precoder'' (Auto-HP) and an unsupervised deep learning (DL)approach that jointly designs \ac{mmWave} probing beams and hybrid precodingmatrix design for mmWave multi-user communication system with minimal trainingpilots. Our learning-based model capitalizes on prior channel observations toachieve two primary goals: designing a limited set of probing beams andpredicting off-grid \ac{RF} beamforming vectors. The Auto-HP frameworkoptimizes the probing beams in an unsupervised manner, concentrating thesensing power on the most promising spatial directions based on the surroundingenvironment. This is achieved through an innovative neural network architecturethat respects \ac{RF} chain constraints and models received signal strengthpower measurements using complex-valued convolutional layers. Then, theautoencoder is trained to directly produce RF beamforming vectors for hybridarchitectures, unconstrained by a predefined codebook, based on few projectedreceived signal strength indicators (RSSIs). Finally, once the RF beamformingvectors for the multi-users are predicted, the baseband (BB) digital precodersare designed accounting for the multi-user interference. The Auto-HP neuralnetwork is trained end-to-end (E2E) in an unsupervised learning manner with acustomized loss function that aims to maximizes the received signal strength.The adequacy of the Auto-HP NN's bottleneck layer dimension is evaluated froman information theory perspective, ensuring maximum data compression andreliable RF beam predictions.</description>
      <author>example@mail.com (Asmaa Abdallah, Abdulkadir Celik, Ahmed Alkhateeb, Ahmed M. Eltawil)</author>
      <guid isPermaLink="false">2503.08267v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Generating Robot Constitutions &amp; Benchmarks for Semantic Safety</title>
      <link>http://arxiv.org/abs/2503.08663v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了针对大型视觉和语言模型（VLMs）在机器人安全方面的新挑战，提出了一种评估和改进机器人语义安全的方法，并开发了一个框架来自动生成引导机器人行为的宪法。&lt;h4&gt;背景&lt;/h4&gt;传统的机器人安全性研究主要集中在碰撞避免和附近危险减少。随着VLM的发展，机器人现在能够进行高层次的场景理解和自然语言交互，但这也带来了新的安全隐患，比如幻觉或被破解的风险。&lt;h4&gt;目的&lt;/h4&gt;为了应对这些新兴风险，本文旨在发布ASIMOV Benchmark来评估基础模型的语义安全性，并开发一个基于宪法AI机制的框架来自动生成引导机器人行为的宪法。&lt;h4&gt;方法&lt;/h4&gt;首先，创建了一个大规模的数据集，用于评估和改进机器人的语义安全。其次，通过使用文本和图像生成技术自动生成不希望出现的情景以及从医院的人体伤害报告中提取数据。此外，提出了一种新颖的自动修订过程来引入行为规则中的细微差别。&lt;h4&gt;主要发现&lt;/h4&gt;机器人能够有效地拒绝不合宪法的行为。在ASIMOV Benchmark上的测试结果显示了84.3%的最佳一致性率，并且自动生成的宪法优于无宪法基准和人工编写的宪法。&lt;h4&gt;结论&lt;/h4&gt;通过提出ASIMOV Benchmark以及基于数据生成宪法的框架，可以有效提高机器人的语义安全性和行为与人类偏好的对齐度。&lt;h4&gt;翻译&lt;/h4&gt;直到最近，机器人安全性研究主要集中在碰撞避免和附近危险减少。随着大型视觉和语言模型（VLMs）的发展，机器人现在也能够进行高层次的场景理解和自然语言交互。尽管存在已知的安全漏洞，例如幻觉或被破解的风险，VLM们正在控制与现实世界物理接触能力的机器人。这可能导致危险行为，使机器人的语义安全成为当务之急。本文提出了一种评估和改进基础模型语义安全的方法，并开发了一个基于宪法AI机制来自动生成引导机器人行为宪法的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Until recently, robotics safety research was predominantly about collisionavoidance and hazard reduction in the immediate vicinity of a robot. Since theadvent of large vision and language models (VLMs), robots are now also capableof higher-level semantic scene understanding and natural language interactionswith humans. Despite their known vulnerabilities (e.g. hallucinations orjail-breaking), VLMs are being handed control of robots capable of physicalcontact with the real world. This can lead to dangerous behaviors, makingsemantic safety for robots a matter of immediate concern. Our contributions inthis paper are two fold: first, to address these emerging risks, we release theASIMOV Benchmark, a large-scale and comprehensive collection of datasets forevaluating and improving semantic safety of foundation models serving as robotbrains. Our data generation recipe is highly scalable: by leveraging text andimage generation techniques, we generate undesirable situations from real-worldvisual scenes and human injury reports from hospitals. Secondly, we develop aframework to automatically generate robot constitutions from real-world data tosteer a robot's behavior using Constitutional AI mechanisms. We propose a novelauto-amending process that is able to introduce nuances in written rules ofbehavior; this can lead to increased alignment with human preferences onbehavior desirability and safety. We explore trade-offs between generality andspecificity across a diverse set of constitutions of different lengths, anddemonstrate that a robot is able to effectively reject unconstitutionalactions. We measure a top alignment rate of 84.3% on the ASIMOV Benchmark usinggenerated constitutions, outperforming no-constitution baselines andhuman-written constitutions. Data is available at asimov-benchmark.github.io</description>
      <author>example@mail.com (Pierre Sermanet, Anirudha Majumdar, Alex Irpan, Dmitry Kalashnikov, Vikas Sindhwani)</author>
      <guid isPermaLink="false">2503.08663v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>LiSu: A Dataset and Method for LiDAR Surface Normal Estimation</title>
      <link>http://arxiv.org/abs/2503.08601v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新的LiDAR点云表面法线估计方法和一个大规模的合成数据集，解决了现有研究中关于缺乏大规模标注数据集以及处理稀疏且噪声大的LiDAR数据的方法不足的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的3D场景几何分析使用表面法线广泛，但由于缺少大规模标注的数据集和无法在合理的时间内有效地处理稀疏、嘈杂的点云数据的方法，导致LiDAR点云中表面法线估计的研究较少。&lt;h4&gt;目的&lt;/h4&gt;通过引入一个新的合成的LiDAR点云数据集以及提出一种新的方法来改进现有的3D场景几何分析研究。&lt;h4&gt;方法&lt;/h4&gt;采用了一种交通仿真引擎创建了一个名为LiSu的大规模、合成LiDAR点云数据集，并且提出了利用自动驾驶数据的空间和时间特性来提高表面法线估计精度的方法。该方法通过加入两个正则化项，强制相邻点之间的空间一致性以及连续帧间的时序平滑性。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法在LiSu数据集上实现了最先进的性能，在从合成到真实环境的领域适应任务中也有很好的表现，提高了神经表面重建的真实世界性能。&lt;h4&gt;结论&lt;/h4&gt;新的方法和大规模数据集有助于解决现有的问题，并为未来的3D场景分析研究提供了可能的方向。&lt;h4&gt;翻译&lt;/h4&gt;虽然表面法线被广泛用于分析三维场景几何结构，但从LiDAR点云估算出的表面法线仍然严重缺乏探索。这主要是由于一方面缺少大规模标注的数据集，另一方面则是缺乏能够合理时间内处理稀疏且常常嘈杂的LiDAR数据的方法所导致。我们通过使用交通模拟引擎解决了这些限制，并提出了LiSu——第一个大型规模的人工合成LiDAR点云数据集，带有地面实况表面法线注释，消除了繁琐的手动标记的需求。此外，我们还提出了一种新方法，利用自动驾驶数据的空间和时间特性来提高表面法线估计精度。通过引入两个正则化项，我们在相邻点之间强制执行空间一致性，并在连续的LiDAR帧中保持时序平滑性。这些正则化器特别适用于自我训练设置，在这种情况下，它们可以缓解噪声伪标签的影响，从而实现稳健的真实世界部署。我们证明了我们的方法在LiSu数据集上的有效性，达到了最先进的LiDAR表面法线估计性能，并展示了其在解决从合成到真实领域的领域适应挑战性任务中的全部潜力，进而提高了真实世界的神经表面重建效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While surface normals are widely used to analyse 3D scene geometry, surfacenormal estimation from LiDAR point clouds remains severely underexplored. Thisis caused by the lack of large-scale annotated datasets on the one hand, andlack of methods that can robustly handle the sparse and often noisy LiDAR datain a reasonable time on the other hand. We address these limitations using atraffic simulation engine and present LiSu, the first large-scale, syntheticLiDAR point cloud dataset with ground truth surface normal annotations,eliminating the need for tedious manual labeling. Additionally, we propose anovel method that exploits the spatiotemporal characteristics of autonomousdriving data to enhance surface normal estimation accuracy. By incorporatingtwo regularization terms, we enforce spatial consistency among neighboringpoints and temporal smoothness across consecutive LiDAR frames. Theseregularizers are particularly effective in self-training settings, where theymitigate the impact of noisy pseudo-labels, enabling robust real-worlddeployment. We demonstrate the effectiveness of our method on LiSu, achievingstate-of-the-art performance in LiDAR surface normal estimation. Moreover, weshowcase its full potential in addressing the challenging task ofsynthetic-to-real domain adaptation, leading to improved neural surfacereconstruction on real-world data.</description>
      <author>example@mail.com (Dušan Malić, Christian Fruhwirth-Reisinger, Samuel Schulter, Horst Possegger)</author>
      <guid isPermaLink="false">2503.08601v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Load Estimation for Load-lifting Exoskeletons Using Insole Pressure Sensors and Machine Learning</title>
      <link>http://arxiv.org/abs/2503.07527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用低成本鞋垫压力传感器进行实时举重负荷估计的方法，以改进上肢辅助外骨骼的控制策略。&lt;h4&gt;背景&lt;/h4&gt;目前存在一种需求来提升上肢辅助外骨骼系统的控制性能。通过使用成本较低的压力感应内底，可以减少身体重量和传感器放置位置变化带来的干扰，从而更准确地估算动态负载。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于通道的方法和一个基于地图的方法，并对这两种方法进行实验验证。&lt;h4&gt;方法&lt;/h4&gt;{'通道方法': '采用传统的回归技术（如ElasticNet、支持向量回归SVR及多层感知器MLP）', '地图方法': '利用预训练的MobileNetV2模型，通过迁移学习来实现'}&lt;h4&gt;主要发现&lt;/h4&gt;{'通道方法': '使用SVR方法时，三名受试者在负载范围从2kg到10kg以每0.5kg递增的数据集上的平均加权绝对百分比误差（WMAPE）为13.46%，MLP性能相似。', '地图方法': '基于一个受试者的数据，在完全微调MobileNetV2模型的情况下，达到9.74%的WMAPE'}&lt;h4&gt;结论&lt;/h4&gt;将内底传感器技术与先进的机器学习模型相结合可以有效地进行动态负载估计，并且可能降低外骨骼控制中的过度和不足补偿的风险。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种用于提升上肢辅助外骨骼控制系统性能的新方法，通过使用低成本鞋垫压力传感器来实时估算举重负荷。该研究探索了两种建模方式：通道基模型和地图基模型，前者采用ElasticNet、SVR及MLP等传统回归技术，后者则运用预训练的MobileNetV2迁移学习模型。实验结果表明，所提出的方法在减少外骨骼控制中的过度或不足补偿方面具有潜在优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel method for real-time lifting-load estimation toenhance the control strategies of upper-limb assistive exoskeletons. Byleveraging cost-effective insole pressure sensors, the proposed system extractsdifferential pressure data that minimizes disturbances from variations in bodyweight and sensor placement. Two modeling approaches are explored: achannel-based method that employs traditional regression techniques-ElasticNet, Support Vector Regression (SVR), and Multi-Layer Perceptron (MLP)-and amap-based method that utilizes transfer learning with a pre-trained MobileNetV2model. The experiment is in the preliminary test stage, covering load rangesfrom 2 kg to 10 kg in increments of 0.5 kg, and collecting data from threesubjects to test the approach. In the Channel-based method, the averageWeighted Mean Absolute Percentage Error(WMAPE) for three subjects showed thatthe SVR achieved 13.46%, with the MLP performing similarly. In the Map-basedmethod, using data from one subject, the Fully Fine-Tuned MobileNetV2 modelreached a WMAPE of 9.74%. The results indicate that the integration of insolesensor technology with advanced machine learning models provides an effectivesolution for dynamic load estimation, potentially reducing the risks of over-and under-compensation in exoskeleton control.</description>
      <author>example@mail.com (Kaida Wu, Peihao Xiang, Chaohao Lin, Lixuan Chen, Ou Bai)</author>
      <guid isPermaLink="false">2503.07527v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>EnergyFormer: Energy Attention with Fourier Embedding for Hyperspectral Image Classification</title>
      <link>http://arxiv.org/abs/2503.08239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种基于Transformer的框架EnergyFormer，旨在通过三个关键创新来解决高光谱成像（HSI）数据特征提取和分类中的挑战。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像技术提供丰富的空间-光谱信息，适用于环境监测、农业及城市分析等领域。然而，其高维度和光谱变化对特征提取和分类构成重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Transformer的框架EnergyFormer，以优化HSI数据处理中的关键问题。&lt;h4&gt;方法&lt;/h4&gt;{'Multi-Head Energy Attention (MHEA)': '通过优化能量函数来选择性增强关键的空间-光谱特性，提高特征区分度', 'Fourier Position Embedding (FoPE)': '自适应编码光谱和空间依赖关系，加强长程交互作用', 'Enhanced Convolutional Block Attention Module (ECBAM)': '选择性放大信息丰富的波段及空间结构，增强表示学习'}&lt;h4&gt;主要发现&lt;/h4&gt;在WHU-Hi-HanChuan、Salinas以及Pavia University数据集上的广泛实验表明，EnergyFormer分别实现了99.28%、98.63%和98.72%的整体准确率，超越了最先进的CNN、Transformer及Mamba模型。&lt;h4&gt;结论&lt;/h4&gt;提出的EnergyFormer框架在处理高光谱成像中的特征提取和分类问题上展现了显著优势。&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像（HSI）技术提供了数百个连续波段的空间-光谱信息，在环境监测、农业以及城市分析等领域中可以实现精确的材料区分。然而，由于其数据的高度维度性和光谱变化性，使得特征提取和分类面临重大挑战。本文提出了EnergyFormer，这是一个基于Transformer框架的方法，通过三个关键创新来应对这些挑战：（1）Multi-Head Energy Attention (MHEA)，该方法优化了能量函数以选择性增强关键的空间-光谱特性，提高特征区分度；（2）Fourier Position Embedding (FoPE)，自适应地编码光谱和空间依赖关系，加强长程交互作用；（3）Enhanced Convolutional Block Attention Module (ECBAM)，选择性放大信息丰富的波段及空间结构，增强表示学习。在WHU-Hi-HanChuan、Salinas以及Pavia University数据集上的广泛实验表明，EnergyFormer分别实现了99.28%、98.63%和98.72%的整体准确率，超越了最先进的CNN、Transformer及Mamba模型。源代码将在https://github.com/mahmad000提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) provides rich spectral-spatial information acrosshundreds of contiguous bands, enabling precise material discrimination inapplications such as environmental monitoring, agriculture, and urban analysis.However, the high dimensionality and spectral variability of HSI data posesignificant challenges for feature extraction and classification. This paperpresents EnergyFormer, a transformer-based framework designed to address thesechallenges through three key innovations: (1) Multi-Head Energy Attention(MHEA), which optimizes an energy function to selectively enhance criticalspectral-spatial features, improving feature discrimination; (2) FourierPosition Embedding (FoPE), which adaptively encodes spectral and spatialdependencies to reinforce long-range interactions; and (3) EnhancedConvolutional Block Attention Module (ECBAM), which selectively amplifiesinformative wavelength bands and spatial structures, enhancing representationlearning. Extensive experiments on the WHU-Hi-HanChuan, Salinas, and PaviaUniversity datasets demonstrate that EnergyFormer achieves exceptional overallaccuracies of 99.28\%, 98.63\%, and 98.72\%, respectively, outperformingstate-of-the-art CNN, transformer, and Mamba-based models. The source code willbe made available at https://github.com/mahmad000.</description>
      <author>example@mail.com (Saad Sohail, Muhammad Usama, Usman Ghous, Manuel Mazzara, Salvatore Distefano, Muhammad Ahmad)</author>
      <guid isPermaLink="false">2503.08239v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Accelerate 3D Object Detection Models via Zero-Shot Attention Key Pruning</title>
      <link>http://arxiv.org/abs/2503.08101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对三维目标检测模型中变换器解码器的零样本运行时剪枝方法tgGBC，该方法通过分类分数指导渐进地修剪键值来减少计算需求，同时保持高性能。&lt;h4&gt;背景&lt;/h4&gt;基于查询的方法使用密集特征在3D物体检测任务中表现出色，但这些模型在大图像尺寸和多层变换器的情况下计算需求很高，难以在边缘设备上高效运行。现有的剪枝和知识蒸馏方法要么需要重新训练，要么针对ViT模型设计。&lt;h4&gt;目的&lt;/h4&gt;为解决现有模型效率低的问题，提出了一种新的零样本剪枝策略，旨在减少3D检测模型的计算量并提高其在边缘设备上的运行速度。&lt;h4&gt;方法&lt;/h4&gt;tgGBC方法通过分类分数和注意力图相乘得到每个键的重要性得分，并根据此评分逐步修剪变换器模块中的不重要键。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在ToC3D模型中实现1.99倍的加速，仅损失不到1%的性能。在某些情况下，甚至可以提升检测效果。此外，实验表明这种方法在边缘设备上也有效。&lt;h4&gt;结论&lt;/h4&gt;tgGBC提供了一种有效的解决方案来优化基于变换器的三维物体检测模型，在保持高精度的同时大大减少了计算需求。&lt;h4&gt;翻译&lt;/h4&gt;查询基方法利用密集特征在3D目标检测任务中表现出色。然而，这些模型由于大型图像尺寸和多层变压器的存在而面临巨大的计算负担，这对边缘设备上的高效运行构成了挑战。现有的剪枝技术和知识蒸馏技术要么需要重新训练，要么是针对ViT架构设计的，这使得它们难以移植到3D检测器上。为解决这些问题，我们提出了一种零样本执行时剪枝方法，专门用于3D对象检测模型中的变压器解码器。该方法通过分类分数指导逐步修剪变换器模块中的键值（tgGBC）。它将分类得分与注意力图相乘获得每个键的重要性得分，并根据这些重要性分数逐层修剪某些键。我们的方法在最新的ToC3D模型中实现了1.99倍的加速，在变压器解码器上，性能损失不到1%。有趣的是，在某些情况下，该方法甚至提高了模型的表现。此外，我们还在边缘设备上部署了使用tgGBC优化后的3D检测器，进一步验证了这种方法的有效性。相关代码可在https://github.com/iseri27/tg_gbc找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/iseri27/tg_gbc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Query-based methods with dense features have demonstrated remarkable successin 3D object detection tasks. However, the computational demands of thesemodels, particularly with large image sizes and multiple transformer layers,pose significant challenges for efficient running on edge devices. Existingpruning and distillation methods either need retraining or are designed for ViTmodels, which are hard to migrate to 3D detectors. To address this issue, wepropose a zero-shot runtime pruning method for transformer decoders in 3Dobject detection models. The method, termed tgGBC (trim keys gradually GuidedBy Classification scores), systematically trims keys in transformer modulesbased on their importance. We expand the classification score to multiply itwith the attention map to get the importance score of each key and then prunecertain keys after each transformer layer according to their importance scores.Our method achieves a 1.99x speedup in the transformer decoder of the latestToC3D model, with only a minimal performance loss of less than 1%.Interestingly, for certain models, our method even enhances their performance.Moreover, we deploy 3D detectors with tgGBC on an edge device, furthervalidating the effectiveness of our method. The code can be found athttps://github.com/iseri27/tg_gbc.</description>
      <author>example@mail.com (Lizhen Xu, Xiuxiu Bai, Xiaojun Jia, Jianwu Fang, Shanmin Pang)</author>
      <guid isPermaLink="false">2503.08101v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>i-WiViG: Interpretable Window Vision GNN</title>
      <link>http://arxiv.org/abs/2503.08321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;基于图神经网络的深度学习模型已成为解决计算机视觉问题的一种流行方法，尤其在远程感测图像处理中表现出了捕捉长距离依赖性的优势。然而，这些方法的黑盒特性限制了它们在关键应用中的广泛使用。&lt;h4&gt;背景&lt;/h4&gt;图卷积神经网络能够有效地编码图像为图形结构，并捕获通常存在于遥感图像中的长距离依赖性。但是，这种方法的不透明性质阻碍其进一步的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种可解释的窗口视觉GNN(i-WiViG)方法来改善基于图的视觉模型的自我解释能力。&lt;h4&gt;方法&lt;/h4&gt;该方法通过引入基于窗口的图处理技术限制节点的感受野为局部图像区域，并利用自解释瓶颈结构评估远程区域之间的关系的重要性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与传统的Vision GNN相比，i-WiViG在遥感分类和回归任务上不仅性能相当甚至更优，同时能够提供自然且忠实的模型预测解释。&lt;h4&gt;结论&lt;/h4&gt;该方法显著提高了图神经网络对关键应用环境中的可信任度，并提供了更加透明和可信的机器学习解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于图神经网络的深度学习模型在解决计算机视觉问题中越来越受欢迎。它们可以有效地将图像编码为图形结构，以捕捉遥感图像中存在的长距离依赖性。然而，这些方法的黑盒特性限制了其在关键应用中的使用。为此，本文提出了一种可解释窗口视觉GNN（i-WiViG）方法，该方法通过自动识别与模型预测相关的子图来提供解释。它利用基于窗口的图像图处理技术将节点感受野限制在一个局部图像区域，并采用自解释瓶颈结构对长距离关系的重要性进行排序。在遥感分类和回归任务中对该方法进行了评估，结果表明其可以达到竞争性的性能并提供自然且忠实的解释。此外，定量分析显示，与传统的Vision GNN相比，该模型减少了后验解释的不准确性，而不会牺牲解释的稀疏性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models based on graph neural networks have emerged as a popularapproach for solving computer vision problems. They encode the image into agraph structure and can be beneficial for efficiently capturing the long-rangedependencies typically present in remote sensing imagery. However, an importantdrawback of these methods is their black-box nature which may hamper theirwider usage in critical applications. In this work, we tackle theself-interpretability of the graph-based vision models by proposing ourInterpretable Window Vision GNN (i-WiViG) approach, which provides explanationsby automatically identifying the relevant subgraphs for the model prediction.This is achieved with window-based image graph processing that constrains thenode receptive field to a local image region and by using a self-interpretablegraph bottleneck that ranks the importance of the long-range relations betweenthe image regions. We evaluate our approach to remote sensing classificationand regression tasks, showing it achieves competitive performance whileproviding inherent and faithful explanations through the identified relations.Further, the quantitative evaluation reveals that our model reduces theinfidelity of post-hoc explanations compared to other Vision GNN models,without sacrificing explanation sparsity.</description>
      <author>example@mail.com (Ivica Obadic, Dmitry Kangin, Dario Oliveira, Plamen P Angelov, Xiao Xiang Zhu)</author>
      <guid isPermaLink="false">2503.08321v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>A Theoretical Framework for Preventing Class Collapse in Supervised Contrastive Learning</title>
      <link>http://arxiv.org/abs/2503.08203v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了防止监督对比学习（SupCL）中类崩溃现象的理论指导原则。&lt;h4&gt;背景信息&lt;/h4&gt;监督对比学习是一种利用监督和自监督损失进行表示学习的方法，但在两种损失之间达到最佳平衡较为困难。&lt;h4&gt;研究目的&lt;/h4&gt;提出一种理论上基础坚实的框架来指导监督对比学习中的超参数选择，以防止在训练过程中出现类崩溃问题。&lt;h4&gt;所用方法&lt;/h4&gt;引入了Simplex-to-Simplex Embedding Model（SSEM）理论框架，并通过该框架分析超参数如何影响学到的表示以及提供实际指南来减小类崩溃的风险。&lt;h4&gt;主要发现&lt;/h4&gt;SSEM能够建模各种嵌入结构，包括所有使监督对比损失最小化的嵌入。通过该模型得出的指导原则得到了合成数据集和真实世界数据集上实证结果的支持。&lt;h4&gt;结论&lt;/h4&gt;研究为SupCL中防止类崩溃提供了理论依据，并提出了实践中的超参数选择建议以规避类崩溃风险。&lt;h4&gt;翻译&lt;/h4&gt;监督对比学习（SupCL）已成为表示学习领域的一种重要方法，它利用了监督和自监督损失。然而，在这两种损失之间找到最佳平衡点是一项挑战；如果不能成功实现这一平衡，则可能会导致类崩溃问题，即同一个类别中的各个嵌入之间的区分度降低。在本文中，我们提出了基于理论的方法来防止SupCL学习过程中出现的类崩溃现象。具体来说，我们引入了一种名为Simplex-to-Simplex Embedding Model（SSEM）的理论框架，用于建模各种嵌入结构，包括所有使监督对比损失最小化的嵌入。通过该模型分析了超参数如何影响学到的表示，并提供了实用指南以选择合适的超参数来减轻类崩溃风险。我们的理论发现得到了合成数据集和真实世界数据集中实证结果的支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Supervised contrastive learning (SupCL) has emerged as a prominent approachin representation learning, leveraging both supervised and self-supervisedlosses. However, achieving an optimal balance between these losses ischallenging; failing to do so can lead to class collapse, reducingdiscrimination among individual embeddings in the same class. In this paper, wepresent theoretically grounded guidelines for SupCL to prevent class collapsein learned representations. Specifically, we introduce the Simplex-to-SimplexEmbedding Model (SSEM), a theoretical framework that models various embeddingstructures, including all embeddings that minimize the supervised contrastiveloss. Through SSEM, we analyze how hyperparameters affect learnedrepresentations, offering practical guidelines for hyperparameter selection tomitigate the risk of class collapse. Our theoretical findings are supported byempirical results across synthetic and real-world datasets.</description>
      <author>example@mail.com (Chungpa Lee, Jeongheon Oh, Kibok Lee, Jy-yong Sohn)</author>
      <guid isPermaLink="false">2503.08203v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>CL-MVSNet: Unsupervised Multi-view Stereo with Dual-level Contrastive Learning</title>
      <link>http://arxiv.org/abs/2503.08219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accpetd by ICCV2023&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;未监督多视角立体匹配（MVS）方法近期取得了显著进展。&lt;h4&gt;背景&lt;/h4&gt;现有的MVS方法主要依赖于光度一致性假设，但这种假设可能受两个限制：不可区分区域和视图依赖效应，例如低纹理区和反射。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的双层对比学习方法CL-MVSNet来解决这些问题。&lt;h4&gt;方法&lt;/h4&gt;该模型在无监督MVS框架中集成了两种对比分支以构建额外的监督信号。一种是图像级对比分支，引导模型获得更多的上下文感知能力，在不可区分区域实现更完整的深度估计；另一种是场景级对比分支，增强表示能力，提高对视图依赖效应的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;为了恢复更准确的3D几何结构，引入了L0.5光度一致性损失函数，鼓励模型更多地关注精确点，并减轻不希望得到的梯度惩罚。广泛的实验在DTU和Tanks&amp;Temples基准上进行，结果显示该方法在所有端到端无监督MVS框架中达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;与不需要微调的有监督版本相比，CL-MVSNet表现出色。&lt;h4&gt;翻译&lt;/h4&gt;无监督多视角立体匹配（MVS）方法近期取得了显著进展。然而，现有的方法主要依赖于光度一致性假设，这可能导致两个限制：不可区分区域和视图依赖效应，例如低纹理区和反射。为了解决这些问题，在本文中我们提出了一种新的双层对比学习方法CL-MVSNet。具体而言，我们的模型在无监督MVS框架中集成了两种对比分支以构建额外的监督信号。一方面，我们提出了一个图像级对比分支来引导模型获得更多的上下文感知能力，从而导致在不可区分区域更完整的深度估计；另一方面，我们利用场景级对比分支提高表示能力，增强对视图依赖效应的鲁棒性。此外，为了恢复更准确的3D几何结构，引入了L0.5光度一致性损失函数，鼓励模型更多地关注精确点，并减轻不希望得到的梯度惩罚。广泛的实验在DTU和Tanks&amp;Temples基准上进行，结果显示该方法在所有端到端无监督MVS框架中达到了最先进的性能，并且无需微调就优于其有监督版本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised Multi-View Stereo (MVS) methods have achieved promising progressrecently. However, previous methods primarily depend on the photometricconsistency assumption, which may suffer from two limitations:indistinguishable regions and view-dependent effects, e.g., low-textured areasand reflections. To address these issues, in this paper, we propose a newdual-level contrastive learning approach, named CL-MVSNet. Specifically, ourmodel integrates two contrastive branches into an unsupervised MVS framework toconstruct additional supervisory signals. On the one hand, we present animage-level contrastive branch to guide the model to acquire more contextawareness, thus leading to more complete depth estimation in indistinguishableregions. On the other hand, we exploit a scene-level contrastive branch toboost the representation ability, improving robustness to view-dependenteffects. Moreover, to recover more accurate 3D geometry, we introduce an L0.5photometric consistency loss, which encourages the model to focus more onaccurate points while mitigating the gradient penalty of undesirable ones.Extensive experiments on DTU and Tanks&amp;Temples benchmarks demonstrate that ourapproach achieves state-of-the-art performance among all end-to-endunsupervised MVS frameworks and outperforms its supervised counterpart by aconsiderable margin without fine-tuning.</description>
      <author>example@mail.com (Kaiqiang Xiong, Rui Peng, Zhe Zhang, Tianxing Feng, Jianbo Jiao, Feng Gao, Ronggang Wang)</author>
      <guid isPermaLink="false">2503.08219v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic PET Image Reconstruction via Non-negative INR Factorization</title>
      <link>http://arxiv.org/abs/2503.08025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个基于低秩矩阵分解的无监督学习方法NINRF，用于从噪声投影数据重建动态PET图像。&lt;h4&gt;背景&lt;/h4&gt;动态正电子发射断层成像（PET）图像的重建是一个重要但具有挑战性的问题。现有的非负矩阵因式分解(NMF)技术通常处理离散数据，而本文提出的方法旨在将这一技术扩展到连续函数领域。&lt;h4&gt;目的&lt;/h4&gt;引入一种基于神经网络表示系数和基向量的新方法NINRF，以解决动态PET图像的重建问题。&lt;h4&gt;方法&lt;/h4&gt;该方法通过最小化KL散度并添加稀疏正则化来优化神经网络参数。同时提出了隐式神经表征(INRs)将矩阵与连续函数相连的方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明了NINRF在动态PET图像重建中的有效性，尤其是在处理泊松噪声方面优于其他方法，并能够提供对象几何特征和区域浓度变化的连续表示。&lt;h4&gt;结论&lt;/h4&gt;提出的非负隐式神经表征分解(NINRF)为动态PET图像从噪声投影数据中重建提供了有效的解决方案。该方法不仅改善了图像质量，还通过稀疏正则化实现了更好的物理意义解释。&lt;h4&gt;翻译&lt;/h4&gt;从噪声投影数据重建动态正电子发射断层成像(PET)图像是一个重要的但具有挑战性的问题。在本文中，我们介绍了一种无监督学习方法——非负隐式神经表示分解(NINRF)，它基于未知图像的低秩矩阵分解，并使用神经网络来表示系数和基向量。从数学上讲，如果一系列动态PET图像满足广义非负低阶属性，则可以将其分解为一组在时空域中变化的非负连续函数。这将已建立的非负矩阵因式分解(NMF)与连续函数联系起来，并提出使用隐式神经表征(INRs)来连接矩阵和连续函数。通过最小化KL散度并添加系数和基向量上的稀疏正则化，获得网络参数。在动态PET重建中进行了广泛的泊松噪声实验，证明了所提出的该方法的有效性，同时给出了对象详细几何特征和区域浓度变化的连续表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The reconstruction of dynamic positron emission tomography (PET) images fromnoisy projection data is a significant but challenging problem. In this paper,we introduce an unsupervised learning approach, Non-negative Implicit NeuralRepresentation Factorization (\texttt{NINRF}), based on low rank matrixfactorization of unknown images and employing neural networks to represent bothcoefficients and bases. Mathematically, we demonstrate that if a sequence ofdynamic PET images satisfies a generalized non-negative low-rank property, itcan be decomposed into a set of non-negative continuous functions varying inthe temporal-spatial domain. This bridges the well-established non-negativematrix factorization (NMF) with continuous functions and we propose usingimplicit neural representations (INRs) to connect matrix with continuousfunctions. The neural network parameters are obtained by minimizing the KLdivergence, with additional sparsity regularization on coefficients and bases.Extensive experiments on dynamic PET reconstruction with Poisson noisedemonstrate the effectiveness of the proposed method compared to other methods,while giving continuous representations for object's detailed geometricfeatures and regional concentration variation.</description>
      <author>example@mail.com (Chaozhi Zhang, Wenxiang Ding, Roy Y. He, Xiaoqun Zhang, Qiaoqiao Ding)</author>
      <guid isPermaLink="false">2503.08025v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection</title>
      <link>http://arxiv.org/abs/2503.08092v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于稀疏体素的变换网络用于3D目标检测，该网络直接利用来自LiDAR数据的稀疏体素特征作为输入，并提出了一个显式的模态融合方法来结合相机信息。&lt;h4&gt;背景&lt;/h4&gt;大多数以前利用激光雷达和相机多模态特性的三维物体检测方法都使用俯视图空间来进行中间特征表示。然而，这种做法会牺牲z轴的信息并降低总体分辨率。&lt;h4&gt;目的&lt;/h4&gt;解决利用低分辨率特征的问题，并提出一种基于稀疏体素的变换网络（SparseVoxFormer）来提高3D目标检测的精度和效率。&lt;h4&gt;方法&lt;/h4&gt;采用直接从LiDAR点云数据中提取的稀疏体素特征作为输入，替代传统的俯视图空间；利用相机模态，通过将三维体素坐标投影到二维图像上来收集对应的图像特征。&lt;h4&gt;主要发现&lt;/h4&gt;在使用更少的稀疏特征时，可以大幅降低计算成本并提高性能。该方法可以在减少计算资源需求的同时，增强整体和长距离检测效果。&lt;h4&gt;结论&lt;/h4&gt;SparseVoxFormer通过充分利用几何信息丰富的多模态特征，并控制输入特征的数量来达到高性能与低耗的需求平衡。&lt;h4&gt;翻译&lt;/h4&gt;大多数先前利用激光雷达(LiDAR)和相机的多模式特性的三维物体检测方法都使用鸟瞰图(BEV)空间作为中间特征表示。然而，该空间采用较低的x、y分辨率，并牺牲z轴信息以降低整体特征分辨率，这可能导致精度下降。为了解决低分辨率特征的问题，本文重点关注了LiDAR点云数据的稀疏特性。从我们的观察来看，在构建于LiDAR数据基础上的三维体素中占据的单元格数量甚至可以少于俯视图地图中的总单元格数，尽管其分辨率显著更高。基于此，我们引入了一种新颖的稀疏体素基变换网络用于3D目标检测，命名为SparseVoxFormer。该方法直接使用稀疏体素特征作为输入，而不是进行BEV特征提取。此外，在考虑相机模态时，我们提出了一种显式的融合方式，通过将三维体素坐标投影到二维图像上来收集对应的图像特征。由于这些组成部分的贡献，我们的方法能够在减少计算成本的同时利用更丰富的多模式几何特性。除了概念验证之外，我们进一步关注更好地促进多模态融合以及灵活控制稀疏特征的数量。最终，彻底的实验结果表明，在三维目标检测中使用大量较少的稀疏特征能够大幅降低计算成本并提升整体和长距离性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most previous 3D object detection methods that leverage the multi-modality ofLiDAR and cameras utilize the Bird's Eye View (BEV) space for intermediatefeature representation. However, this space uses a low x, y-resolution andsacrifices z-axis information to reduce the overall feature resolution, whichmay result in declined accuracy. To tackle the problem of using low-resolutionfeatures, this paper focuses on the sparse nature of LiDAR point cloud data.From our observation, the number of occupied cells in the 3D voxels constructedfrom a LiDAR data can be even fewer than the number of total cells in the BEVmap, despite the voxels' significantly higher resolution. Based on this, weintroduce a novel sparse voxel-based transformer network for 3D objectdetection, dubbed as SparseVoxFormer. Instead of performing BEV featureextraction, we directly leverage sparse voxel features as the input for atransformer-based detector. Moreover, with regard to the camera modality, weintroduce an explicit modality fusion approach that involves projecting 3Dvoxel coordinates onto 2D images and collecting the corresponding imagefeatures. Thanks to these components, our approach can leverage geometricallyricher multi-modal features while even reducing the computational cost. Beyondthe proof-of-concept level, we further focus on facilitating better multi-modalfusion and flexible control over the number of sparse features. Finally,thorough experimental results demonstrate that utilizing a significantlysmaller number of sparse features drastically reduces computational costs in a3D object detector while enhancing both overall and long-range performance.</description>
      <author>example@mail.com (Hyeongseok Son, Jia He, Seung-In Park, Ying Min, Yunhao Zhang, ByungIn Yoo)</author>
      <guid isPermaLink="false">2503.08092v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>YuE: Scaling Open Foundation Models for Long-Form Music Generation</title>
      <link>http://arxiv.org/abs/2503.08638v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/multimodal-art-projection/YuE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种用于长篇音乐生成的模型YuE，它基于LLaMA2架构，并特别针对歌词转歌曲的问题。通过几种创新的方法，该模型能够生成长达五分钟的音乐并保持歌词的一致性、音乐结构的连贯性和引人入胜的人声旋律。&lt;h4&gt;背景&lt;/h4&gt;现有的音乐生成任务主要集中在短篇作品上，而长篇歌词到歌曲的转换是一个具有挑战性的难题，需要处理密集混合信号和长时间上下文下的歌词对齐问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成高质量、长达五分钟的音乐模型，并解决长上下文中的歌词一致性和密集混合信号的问题。&lt;h4&gt;方法&lt;/h4&gt;{'track-decoupled next-token prediction': '用于克服密集混合信号的方法', 'structural progressive conditioning': '用于长时间上下文中保持歌词对齐的技术', 'multitask, multiphase pre-training recipe': '使模型收敛和泛化的预训练方案'}&lt;h4&gt;主要发现&lt;/h4&gt;{'音乐性能与人声敏捷性': '通过大量评估，YuE在音乐性和人声敏捷性方面达到或超过了某些专有系统', '风格转换能力': '重新设计了上下文学习技术，使模型能够在不同语言和风格之间进行灵活的风格转移', '双向生成': '支持从一种风格到另一种风格的转换的同时保持原始伴奏不变', '音乐理解任务上的表现': 'YuE在MARBLE基准测试中的性能达到或超过了当前最先进的方法'}&lt;h4&gt;结论&lt;/h4&gt;YuE不仅能够生成高质量的长篇歌曲，还可以应用于音乐理解和风格转换等任务，并支持微调以增强尾部语言的支持和控制。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了使用LLaMA2架构开发的一种名为YuE的新模型家族，旨在解决将歌词转化为完整歌曲的挑战。该论文详细介绍了此模型的技术细节、实验结果及其在音乐理解任务中的应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We tackle the task of long-form music generation--particularly thechallenging \textbf{lyrics-to-song} problem--by introducing YuE, a family ofopen foundation models based on the LLaMA2 architecture. Specifically, YuEscales to trillions of tokens and generates up to five minutes of music whilemaintaining lyrical alignment, coherent musical structure, and engaging vocalmelodies with appropriate accompaniment. It achieves this through (1)track-decoupled next-token prediction to overcome dense mixture signals, (2)structural progressive conditioning for long-context lyrical alignment, and (3)a multitask, multiphase pre-training recipe to converge and generalize. Inaddition, we redesign the in-context learning technique for music generation,enabling versatile style transfer (e.g., converting Japanese city pop into anEnglish rap while preserving the original accompaniment) and bidirectionalgeneration. Through extensive evaluation, we demonstrate that YuE matches oreven surpasses some of the proprietary systems in musicality and vocal agility.In addition, fine-tuning YuE enables additional controls and enhanced supportfor tail languages. Furthermore, beyond generation, we show that YuE's learnedrepresentations can perform well on music understanding tasks, where theresults of YuE match or exceed state-of-the-art methods on the MARBLEbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,music generation</description>
      <author>example@mail.com (Ruibin Yuan, Hanfeng Lin, Shuyue Guo, Ge Zhang, Jiahao Pan, Yongyi Zang, Haohe Liu, Yiming Liang, Wenye Ma, Xingjian Du, Xinrun Du, Zhen Ye, Tianyu Zheng, Yinghao Ma, Minghao Liu, Zeyue Tian, Ziya Zhou, Liumeng Xue, Xingwei Qu, Yizhi Li, Shangda Wu, Tianhao Shen, Ziyang Ma, Jun Zhan, Chunhui Wang, Yatian Wang, Xiaowei Chi, Xinyue Zhang, Zhenzhu Yang, Xiangzhou Wang, Shansong Liu, Lingrui Mei, Peng Li, Junjie Wang, Jianwei Yu, Guojian Pang, Xu Li, Zihao Wang, Xiaohuan Zhou, Lijun Yu, Emmanouil Benetos, Yong Chen, Chenghua Lin, Xie Chen, Gus Xia, Zhaoxiang Zhang, Chao Zhang, Wenhu Chen, Xinyu Zhou, Xipeng Qiu, Roger Dannenberg, Jiaheng Liu, Jian Yang, Wenhao Huang, Wei Xue, Xu Tan, Yike Guo)</author>
      <guid isPermaLink="false">2503.08638v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Sentiment Analysis through Multimodal Fusion: A BERT-DINOv2 Approach</title>
      <link>http://arxiv.org/abs/2503.07943v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个集成文本和图像数据的多模态情感分析架构，通过融合技术提高对情感理解的全面性。&lt;h4&gt;背景&lt;/h4&gt;传统的单一文本的情感分析存在局限性，无法充分捕捉用户的真实情绪。引入图片、音频等其他模态的信息可以增强传统的情感分析。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于文本和图像数据集成的情感分析架构来提供更全面的情绪理解。&lt;h4&gt;方法&lt;/h4&gt;{'文本特征提取': '采用BERT模型进行处理。', '视觉特征提取': '使用DINOv2，这是一个基于视觉变换器的模型。', '模态融合技术': '包括基础融合模型、自注意力融合模型以及双注意融合模型。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明提出的多模态架构在Memotion 7k数据集、MVSA单一数据集和MVSA多元数据集中表现出了可行性和实用性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够显著提升情感分析的效果，特别是在结合文本和图像信息后，其性能得到了明显改善。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal sentiment analysis enhances conventional sentiment analysis, whichtraditionally relies solely on text, by incorporating information fromdifferent modalities such as images, text, and audio. This paper proposes anovel multimodal sentiment analysis architecture that integrates text and imagedata to provide a more comprehensive understanding of sentiments. For textfeature extraction, we utilize BERT, a natural language processing model. Forimage feature extraction, we employ DINOv2, a vision-transformer-based model.The textual and visual latent features are integrated using proposed fusiontechniques, namely the Basic Fusion Model, Self Attention Fusion Model, andDual Attention Fusion Model. Experiments on three datasets, Memotion 7kdataset, MVSA single dataset, and MVSA multi dataset, demonstrate the viabilityand practicality of the proposed multimodal architecture.</description>
      <author>example@mail.com (Taoxu Zhao, Meisi Li, Kehao Chen, Liye Wang, Xucheng Zhou, Kunal Chaturvedi, Mukesh Prasad, Ali Anaissi, Ali Braytee)</author>
      <guid isPermaLink="false">2503.07943v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Evidential Uncertainty Probes for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.08097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AISTATS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;在高风险应用中部署图神经网络（GNN）时，准确量化随机不确定性和知识不确定性是至关重要的。尽管证据深度学习(EDL)能够有效地使用狄利克雷分布来量化不确定性，但现有的基于EGNN的模型需要修改架构并重新训练。&lt;h4&gt;背景&lt;/h4&gt;在药物发现和金融欺诈检测等高风险应用中，可靠的预测至关重要，这要求准确地量化GNN中的不确定因素。&lt;h4&gt;目的&lt;/h4&gt;提出一个可插拔框架用于无需重训即可与预训练模型一起工作的不确定性量化。该框架允许使用证据探针网络（EPN）有效提取证据，并利用基于证据的正则化技术来增强知识不确定性估计。&lt;h4&gt;方法&lt;/h4&gt;引入了轻量级MLP头和证据探针网络(EPN)，用于从学习到的表示中提取证据，同时提出基于证据的正则化(Evidence-based regularization, EPN-reg)技术以改善知识不确定性的估算。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明所提出的EPN-reg在准确性和效率方面达到了最先进的不确定性量化性能，使其适合实际部署。&lt;h4&gt;结论&lt;/h4&gt;所提出的插件式框架和新技术为GNN的不确定性量化提供了一种有效的方法，并展示了其在真实场景中的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;准确地量化随机不确定性和知识不确定性对于将图神经网络(GNN)应用于药物发现和金融欺诈检测等高风险领域至关重要。现有的证据深度学习(EDL)方法虽然能够有效地利用狄利克雷分布来量化这些不确定性，但基于EGNN的模型需要重新设计架构并进行重新训练才能适应预训练的模型。本文提出了一种可以与各种GNN架构无缝结合且无需重训的框架和机制：证据探针网络(EPN)及其正则化技术EPN-reg，有效提升了知识不确定性的估算能力，并在多个实验中展示了其优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate quantification of both aleatoric and epistemic uncertainties isessential when deploying Graph Neural Networks (GNNs) in high-stakesapplications such as drug discovery and financial fraud detection, wherereliable predictions are critical. Although Evidential Deep Learning (EDL)efficiently quantifies uncertainty using a Dirichlet distribution overpredictive probabilities, existing EDL-based GNN (EGNN) models requiremodifications to the network architecture and retraining, failing to takeadvantage of pre-trained models. We propose a plug-and-play framework foruncertainty quantification in GNNs that works with pre-trained models withoutthe need for retraining. Our Evidential Probing Network (EPN) uses alightweight Multi-Layer-Perceptron (MLP) head to extract evidence from learnedrepresentations, allowing efficient integration with various GNN architectures.We further introduce evidence-based regularization techniques, referred to asEPN-reg, to enhance the estimation of epistemic uncertainty with theoreticaljustifications. Extensive experiments demonstrate that the proposed EPN-regachieves state-of-the-art performance in accurate and efficient uncertaintyquantification, making it suitable for real-world deployment.</description>
      <author>example@mail.com (Linlin Yu, Kangshuo Li, Pritom Kumar Saha, Yifei Lou, Feng Chen)</author>
      <guid isPermaLink="false">2503.08097v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>3D Point Cloud Generation via Autoregressive Up-sampling</title>
      <link>http://arxiv.org/abs/2503.08594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种用于生成3D点云的开创性自回归生成模型PointARU，该模型将点云生成视为一种自回归上采样过程，并在两个阶段进行训练。&lt;h4&gt;背景&lt;/h4&gt;受视觉自回归建模（VAR）启发，提出了一个适用于3D点云生成的新方法。当前研究中存在如何处理点云的无序和不规则结构的问题。&lt;h4&gt;目的&lt;/h4&gt;通过引入PointARU模型，旨在改善3D点云生成的质量，并提高参数效率。&lt;h4&gt;方法&lt;/h4&gt;首先学习多尺度离散表示，然后训练自回归Transformer进行下一尺度预测。两个阶段都采用了基于特殊设计的点基上采样网络模块和3D绝对位置编码。&lt;h4&gt;主要发现&lt;/h4&gt;PointARU模型在多种实验设置下，与最先进的基于扩散的方法相比，在生成质量和参数效率方面均表现出色，并且在完成部分3D形状和稀疏点云上采样的任务中也优于现有生成模型。&lt;h4&gt;结论&lt;/h4&gt;该工作标志着自回归方法在3D点云生成领域的新里程碑。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a pioneering autoregressive generative model for 3D point cloudgeneration. Inspired by visual autoregressive modeling (VAR), we conceptualizepoint cloud generation as an autoregressive up-sampling process. This leads toour novel model, PointARU, which progressively refines 3D point clouds fromcoarse to fine scales. PointARU follows a two-stage training paradigm: first,it learns multi-scale discrete representations of point clouds, and then ittrains an autoregressive transformer for next-scale prediction. To address theinherent unordered and irregular structure of point clouds, we incorporatespecialized point-based up-sampling network modules in both stages andintegrate 3D absolute positional encoding based on the decoded point cloud ateach scale during the second stage. Our model surpasses state-of-the-art (SoTA)diffusion-based approaches in both generation quality and parameter efficiencyacross diverse experimental settings, marking a new milestone forautoregressive methods in 3D point cloud generation. Furthermore, PointARUdemonstrates exceptional performance in completing partial 3D shapes andup-sampling sparse point clouds, outperforming existing generative models inthese tasks.</description>
      <author>example@mail.com (Ziqiao Meng, Qichao Wang, Zhipeng Zhou, Irwin King, Peilin Zhao)</author>
      <guid isPermaLink="false">2503.08594v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Federated Multimodal Learning with Dual Adapters and Selective Pruning for Communication and Computational Efficiency</title>
      <link>http://arxiv.org/abs/2503.07552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CCGrid 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;联邦学习（FL）使分布式客户端之间的协作学习成为可能，同时保护数据隐私。然而，在处理异构数据分布时，FL面临着重要挑战，这可能导致次优的全局模型无法泛化到多样化的客户端。&lt;h4&gt;背景&lt;/h4&gt;在联邦学习中，由于各个客户端的数据分布存在差异性，导致训练出的全局模型不能有效地适用于所有客户端，并且难以实现高效的跨客户端知识共享。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来解决这些挑战，通过引入双适配器方法，在确保模型个性化的同时提高泛化能力。&lt;h4&gt;方法&lt;/h4&gt;该方法采用了一个较大的局部适配器用于特定客户端的个性化调整和一个较小的全局适配器以促进跨客户端的知识共享。此外，还使用了剪枝机制来减少通信开销，通过选择性地移除影响较小的参数从局部适配器中。&lt;h4&gt;主要发现&lt;/h4&gt;在一系列视觉和语言任务上的广泛实验表明，该方法比现有方法具有更优的表现，达到了更高的测试精度、更低的客户端性能差异以及改善了最坏情况下的表现，并且显著减少了通信与计算成本。&lt;h4&gt;结论&lt;/h4&gt;提出的方法有效地解决了模型个性化与泛化之间的关键权衡问题，为实际联邦学习应用提供了一个可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Learning (FL) enables collaborative learning across distributedclients while preserving data privacy. However, FL faces significant challengeswhen dealing with heterogeneous data distributions, which can lead tosuboptimal global models that fail to generalize across diverse clients. Inthis work, we propose a novel framework designed to tackle these challenges byintroducing a dual-adapter approach. The method utilizes a larger local adapterfor client-specific personalization and a smaller global adapter to facilitateefficient knowledge sharing across clients. Additionally, we incorporate apruning mechanism to reduce communication overhead by selectively removing lessimpactful parameters from the local adapter. Through extensive experiments on arange of vision and language tasks, our method demonstrates superiorperformance compared to existing approaches. It achieves higher test accuracy,lower performance variance among clients, and improved worst-case performance,all while significantly reducing communication and computation costs. Overall,the proposed method addresses the critical trade-off between modelpersonalization and generalization, offering a scalable solution for real-worldFL applications.</description>
      <author>example@mail.com (Duy Phuong Nguyen, J. Pablo Munoz, Tanya Roosta, Ali Jannesari)</author>
      <guid isPermaLink="false">2503.07552v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>CIMAGE: Exploiting the Conditional Independence in Masked Graph Auto-encoders</title>
      <link>http://arxiv.org/abs/2503.07852v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the WSDM 2025 Oral. This is an extended version of the  original submission. Typos are also corrected&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近期的图神经网络自监督学习方法通过遮蔽操作来捕获关系信息，但这些方法通常依赖于随机遮蔽策略，这可能导致未完全捕捉到与任务相关的所有信息。&lt;h4&gt;背景&lt;/h4&gt;现有的大多数自监督学习方法在图神经网络中使用随机遮蔽策略，这种方法可能无法充分捕捉到与下游任务相关的全部重要信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来克服现有方法的不足，通过利用条件独立性来指导有效的遮蔽策略，以确保最小冗余和最大相关性的实现。&lt;h4&gt;方法&lt;/h4&gt;引入了CIMAGE（Conditionally Independent Masking Graph Embedding），该方法利用条件独立性意识的潜在因子分解生成两个不同的上下文，并使用从无监督图聚类中得到的高置信度伪标签。预处理任务涉及仅通过第一个上下文的信息来重构被遮蔽的第二个上下文。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明，CIMAGE的方法能够学习到具有近似线性可分性的嵌入表示，并且实验在多个图基准数据集上验证了CIMAGE的优势，在节点分类和链接预测任务中表现尤为突出。&lt;h4&gt;结论&lt;/h4&gt;CIMAGE展示了利用条件独立性增强图自监督学习方法的潜力，为有效的图表示学习提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;最近封装通过遮蔽操作在图神经网络（GNNs）中的关系信息来捕捉自我监督学习方法显示出有希望的表现。然而，大多数现有的方法依赖于特征或图形空间中的随机遮蔽策略，这可能无法完全捕获任务相关的所有信息。我们认为这一限制源于无法在保持对潜在下游任务的最大相关性的同时实现被遮蔽和未被遮蔽组件之间的最小冗余。条件独立（CI）内在地满足了最小冗余和最大相关性的标准，但其应用通常需要访问下游标签。为了应对这一挑战，我们提出CIMAGE，一种新颖的方法，它利用条件独立性在潜在空间中引导有效的遮蔽策略。CIMAGE使用条件意识的潜在因子分解生成两个不同的上下文，并利用无监督图聚类得到的高置信度伪标签。在此框架下，预处理任务涉及仅通过第一个上下文提供的信息来重构被遮蔽的第二个上下文。我们的理论分析进一步支持了CIMAGE新颖CI感知屏蔽方法的优势，证明学习到的嵌入体显示出近似的线性可分性，从而能够为下游任务提供准确预测。在各种图基准中的全面评估显示了CIMAGE的优点，在节点分类和链接预测任务中表现尤为突出。值得注意的是，我们提出的模型突显了条件独立性的未充分探索潜力，以增强图SSL方法，并提供了有效的图表示学习的丰富见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3701551.3703515&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent Self-Supervised Learning (SSL) methods encapsulating relationalinformation via masking in Graph Neural Networks (GNNs) have shown promisingperformance. However, most existing approaches rely on random maskingstrategies in either feature or graph space, which may fail to capturetask-relevant information fully. We posit that this limitation stems from aninability to achieve minimum redundancy between masked and unmasked componentswhile ensuring maximum relevance of both to potential downstream tasks.Conditional Independence (CI) inherently satisfies the minimum redundancy andmaximum relevance criteria, but its application typically requires access todownstream labels. To address this challenge, we introduce CIMAGE, a novelapproach that leverages Conditional Independence to guide an effective maskingstrategy within the latent space. CIMAGE utilizes CI-aware latent factordecomposition to generate two distinct contexts, leveraging high-confidencepseudo-labels derived from unsupervised graph clustering. In this framework,the pretext task involves reconstructing the masked second context solely fromthe information provided by the first context. Our theoretical analysis furthersupports the superiority of CIMAGE's novel CI-aware masking method bydemonstrating that the learned embedding exhibits approximate linearseparability, which enables accurate predictions for the downstream task.Comprehensive evaluations across diverse graph benchmarks illustrate theadvantage of CIMAGE, with notably higher average rankings on nodeclassification and link prediction tasks. Notably, our proposed modelhighlights the under-explored potential of CI in enhancing graph SSLmethodologies and offers enriched insights for effective graph representationlearning.</description>
      <author>example@mail.com (Jongwon Park, Heesoo Jung, Hogun Park)</author>
      <guid isPermaLink="false">2503.07852v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>SAS: Segment Any 3D Scene with Integrated 2D Priors</title>
      <link>http://arxiv.org/abs/2503.08512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种简单而有效的方法SAS，该方法通过整合多个2D模型的开放词汇能力并将其迁移到3D领域来提高在复杂动态场景中未见过对象识别的能力。&lt;h4&gt;背景&lt;/h4&gt;传统的训练固定类别的3D模型无法有效地识别复杂动态3D场景中的未见物体。为此需要引入能够适应更多类别和未知对象的开放词汇能力。&lt;h4&gt;目的&lt;/h4&gt;通过提出一种新的方法SAS，将多个2D模型的开放词汇能力整合并迁移至3D领域，以提高对未见过物体的识别准确性。&lt;h4&gt;方法&lt;/h4&gt;{'Model Alignment via Text': '使用文本作为桥梁，将不同的2D模型映射到同一嵌入空间中。', 'Annotation-Free Model Capability Construction': '利用扩散模型来量化不同类别下2D模型的认知能力。', '点云特征融合': '基于构建的模型能力对来自不同2D模型的点云特征进行融合。', '特征蒸馏': '通过特征蒸馏将集成的2D开放词汇能力迁移到3D领域。'}&lt;h4&gt;主要发现&lt;/h4&gt;SAS方法在多个数据集（如ScanNet v2、Matterport3D和nuScenes）上显著优于先前的方法，其泛化性能也在下游任务中得到验证。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了一种能够增强3D模型开放词汇能力的新策略，并且展示了这种方法在复杂场景中的优越性和广泛适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The open vocabulary capability of 3D models is increasingly valued, astraditional methods with models trained with fixed categories fail to recognizeunseen objects in complex dynamic 3D scenes. In this paper, we propose a simpleyet effective approach, SAS, to integrate the open vocabulary capability ofmultiple 2D models and migrate it to 3D domain. Specifically, we first proposeModel Alignment via Text to map different 2D models into the same embeddingspace using text as a bridge. Then, we propose Annotation-Free Model CapabilityConstruction to explicitly quantify the 2D model's capability of recognizingdifferent categories using diffusion models. Following this, point cloudfeatures from different 2D models are fused with the guide of constructed modelcapabilities. Finally, the integrated 2D open vocabulary capability istransferred to 3D domain through feature distillation. SAS outperforms previousmethods by a large margin across multiple datasets, including ScanNet v2,Matterport3D, and nuScenes, while its generalizability is further validated ondownstream tasks, e.g., gaussian segmentation and instance segmentation.</description>
      <author>example@mail.com (Zhuoyuan Li, Jiahao Lu, Jiacheng Deng, Hanzhi Chang, Lifan Wu, Yanzhe Liang, Tianzhu Zhang)</author>
      <guid isPermaLink="false">2503.08512v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>HOTFormerLoc: Hierarchical Octree Transformer for Versatile Lidar Place Recognition Across Ground and Aerial Views</title>
      <link>http://arxiv.org/abs/2503.08140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 13 figures, 10 tables, Accepted to CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HOTFormerLoc 是一种新型的层次八叉树基Transformer，用于大规模3D场景下的地对地和地对空环境识别。&lt;h4&gt;背景&lt;/h4&gt;现有的方法在处理大规模3D空间中的地方识别时遇到了挑战，特别是在不同密度点分布的情况下。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的多层次注意力机制和优化的计算模型来解决这些问题，并引入了一个新的数据集CS-Wild-Places以评估性能。&lt;h4&gt;方法&lt;/h4&gt;{'八叉树基多尺度注意力机制': '捕捉空间和语义特征，处理不同粒度的数据。', '圆柱形八叉树注意窗口': '适应点分布的密度变化。', '中继令牌': '促进全局局部互动和多尺度表示学习，在减少计算成本的情况下实现高效。', '金字塔注意力池化': '合成一个鲁棒的全局描述符，用于端到端的地方识别。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能改进': 'HOTFormerLoc在CS-Wild-Places基准上实现了5.5% - 11.5%的平均召回率提升。', '超越现有方法': '在城市和森林等广泛接受的数据集上，与最新的3D地方识别方法相比，平均性能提升了5.8%。'}&lt;h4&gt;结论&lt;/h4&gt;HOTFormerLoc为大规模3D场景识别提供了一种强大的解决方案，并且通过引入CS-Wild-Places数据集提高了跨视角定位的挑战性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：我们提出了HOTFormerLoc，这是一种新颖而多用途的层次八叉树基Transformer，在城市和森林环境中用于大规模地对地及地对空3D场景识别。提出了一种基于八叉树的多尺度注意力机制，捕捉不同粒度的空间和语义特征。为了解决来自旋转激光雷达的点分布密度变化问题，我们提出了圆柱形八叉树注意窗口来反映其底层分布。引入了中继令牌以实现高效的全局局部互动及减少计算成本下的多尺度表示学习。我们的金字塔注意力池化随后合成了一种鲁棒性的全球描述符，用于具有挑战性环境中的端到端地方识别。此外，我们还提出了CS-Wild-Places，一个新的3D跨源数据集，该数据集包含从空中和地面激光雷达扫描中捕捉到的点云数据，在密集森林环境中。这些点云在CS-Wild-Places中包含代表差距和不同的属性，例如变化的点密度和噪声模式，使它成为一个挑战性的跨视角定位基准测试环境。HOTFormerLoc在CS-Wild-Places基准上实现了平均召回率5.5% - 11.5%的最佳改进性能。此外，在成熟的城市和森林数据集上，它的性能始终超越最先进的3D地方识别方法，平均性能提升达5.8%。代码及CS-Wild-Places基准可在https://csiro-robotics.github.io/HOTFormerLoc访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present HOTFormerLoc, a novel and versatile Hierarchical Octree-basedTransformer, for large-scale 3D place recognition in both ground-to-ground andground-to-aerial scenarios across urban and forest environments. We propose anoctree-based multi-scale attention mechanism that captures spatial and semanticfeatures across granularities. To address the variable density of pointdistributions from spinning lidar, we present cylindrical octree attentionwindows to reflect the underlying distribution during attention. We introducerelay tokens to enable efficient global-local interactions and multi-scalerepresentation learning at reduced computational cost. Our pyramid attentionalpooling then synthesises a robust global descriptor for end-to-end placerecognition in challenging environments. In addition, we introduceCS-Wild-Places, a novel 3D cross-source dataset featuring point cloud data fromaerial and ground lidar scans captured in dense forests. Point clouds inCS-Wild-Places contain representational gaps and distinctive attributes such asvarying point densities and noise patterns, making it a challenging benchmarkfor cross-view localisation in the wild. HOTFormerLoc achieves a top-1 averagerecall improvement of 5.5% - 11.5% on the CS-Wild-Places benchmark.Furthermore, it consistently outperforms SOTA 3D place recognition methods,with an average performance gain of 5.8% on well-established urban and forestdatasets. The code and CS-Wild-Places benchmark is available athttps://csiro-robotics.github.io/HOTFormerLoc .</description>
      <author>example@mail.com (Ethan Griffiths, Maryam Haghighat, Simon Denman, Clinton Fookes, Milad Ramezani)</author>
      <guid isPermaLink="false">2503.08140v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>DeepRAG: Building a Custom Hindi Embedding Model for Retrieval Augmented Generation from Scratch</title>
      <link>http://arxiv.org/abs/2503.08213v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了DeepRAG，这是一个为印地语量身定制的嵌入模型，用于检索增强生成（RAG）系统。该研究解决了多语言大型语言模型在印地语检索任务中表现不佳的问题，并展示了自定义构建印地语文本嵌入的有效性。&lt;h4&gt;背景&lt;/h4&gt;尽管大型语言模型在生成文本方面表现出色，但在印地语等低资源语言的检索任务上，性能仍然依赖于高质量的语言嵌入。目前市场上缺乏专门针对印地语优化的嵌入模型，导致多语言模型在该领域效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提高印地语文本检索精度的方法，并提出一个适用于低资源语言的自定义嵌入模型构建流程。&lt;h4&gt;方法&lt;/h4&gt;通过收集超过270万个样本文档建立语料库；训练特定于印地语形态学需求的SentencePiece分词器；设计适合印地语特点的转换器架构，包括注意力机制调整；使用对比学习进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在检索精度上与现有的多语言模型相比提高了23%，这表明为特定语言量身定制嵌入方法的有效性远超预期。&lt;h4&gt;结论&lt;/h4&gt;这项工作填补了印地语自然语言处理领域的关键空白，并强调了针对具体语言构建专门解决方案的重要性。同时，研究团队已将这些成果集成到LangChain系统中以构建完整的印地语文本生成系统。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，作者介绍了一种专门为印地语设计的DeepRAG嵌入模型，在检索增强生成（RAG）系统中的应用情况以及开发过程。该工作旨在解决大型语言模型在低资源语言如印地语上的性能瓶颈问题，并展示了通过定制化构建解决方案可以显著提高系统的检索精度和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, I present our work on DeepRAG, a specialized embedding modelwe built specifically for Hindi language in RAG systems. While LLMs have gottenreally good at generating text, their performance in retrieval tasks stilldepends heavily on having quality embeddings - something that's been lackingfor Hindi despite being one of the world's most spoken languages. We tackledthis by creating embeddings from the ground up rather than just fine-tuningexisting models. Our process involved collecting diverse Hindi texts (over 2.7Msamples), training a custom SentencePiece tokenizer that actually understandsHindi morphology, designing transformer architecture with Hindi-specificattention mechanisms, and optimizing with contrastive learning. Results werehonestly better than I expected - we saw a 23% improvement in retrievalprecision compared to the multilingual models everyone's been using. The paperdetails our methodology, which I think could help others working withlow-resource languages where the one-size-fits-all multilingual models fallshort. We've also integrated our embeddings with LangChain to build completeHindi RAG systems, which might be useful for practitioners. While there's stilltons more to explore, I believe this work addresses a critical gap for HindiNLP and demonstrates why language-specific approaches matter.</description>
      <author>example@mail.com (Nandakishor M)</author>
      <guid isPermaLink="false">2503.08213v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Cross-Modal Alignment for Open-Vocabulary 3D Object Detection</title>
      <link>http://arxiv.org/abs/2503.07593v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AAAI 2025 (Extented Version). Project Page:  https://youjunzhao.github.io/HCMA/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种用于开放词汇3D物体检测（OV-3DOD）的分层框架HCMA，该框架能够同时学习局部对象信息和全局场景信息。&lt;h4&gt;背景&lt;/h4&gt;现有的利用视觉语言模型(VLMs)进行三维目标检测(3DOD)的工作通常依赖于丢失丰富场景上下文表示的方法。这些问题限制了VLM在OV-3DOD中的应用效果。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，以解决现有工作在处理开放词汇3D物体检测时遇到的场景上下文缺失问题。&lt;h4&gt;方法&lt;/h4&gt;{'Hierarchical Data Integration (HDI) 方法': '用于获取细粒度到粗粒度的三维图像-文本数据，并将其输入视觉语言模型(VLMs)，提取对象中心知识。', 'Interactive Cross-Modal Alignment (ICMA)策略': '建立有效的同级和跨级特征连接，促进特征层级之间的关联。', 'Object-Focusing Context Adjustment (OFCA)模块': '通过强调与物体相关的特征来细化多级特征，从而更好地对齐不同级别的特征。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在现有的OV-3DOD基准测试中超过了最先进的方法，并且即使没有使用任何3D注释也获得了有希望的检测结果。&lt;h4&gt;结论&lt;/h4&gt;本文提出的HCMA框架及其组件能够有效地提升开放词汇下的三维物体检测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary 3D object detection (OV-3DOD) aims at localizing andclassifying novel objects beyond closed sets. The recent success ofvision-language models (VLMs) has demonstrated their remarkable capabilities tounderstand open vocabularies. Existing works that leverage VLMs for 3D objectdetection (3DOD) generally resort to representations that lose the rich scenecontext required for 3D perception. To address this problem, we propose in thispaper a hierarchical framework, named HCMA, to simultaneously learn localobject and global scene information for OV-3DOD. Specifically, we first designa Hierarchical Data Integration (HDI) approach to obtain coarse-to-fine3D-image-text data, which is fed into a VLM to extract object-centricknowledge. To facilitate the association of feature hierarchies, we thenpropose an Interactive Cross-Modal Alignment (ICMA) strategy to establisheffective intra-level and inter-level feature connections. To better alignfeatures across different levels, we further propose an Object-Focusing ContextAdjustment (OFCA) module to refine multi-level features by emphasizingobject-related features. Extensive experiments demonstrate that the proposedmethod outperforms SOTA methods on the existing OV-3DOD benchmarks. It alsoachieves promising OV-3DOD results even without any 3D annotations.</description>
      <author>example@mail.com (Youjun Zhao, Jiaying Lin, Rynson W. H. Lau)</author>
      <guid isPermaLink="false">2503.07593v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Proc4Gem: Foundation models for physical agency through procedural generation</title>
      <link>http://arxiv.org/abs/2503.08593v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合生成模型、逼真渲染和程序化生成的方法，用于解决机器人学习中的复杂任务。&lt;h4&gt;背景&lt;/h4&gt;在机器人学习中，通常会忽略环境语义或接触动力学之一。前者专注于仅需要关于机器人与环境接触推理的任务，如全身控制；后者则关注于将高层次动作通过视觉和语言理解下来。&lt;h4&gt;目的&lt;/h4&gt;展示如何利用先进的生成模型、逼真渲染以及程序化生成技术来同时处理要求高度物理交互的复杂任务。&lt;h4&gt;方法&lt;/h4&gt;该工作提出了一种名为Proc4Gem的新系统，它可以在语义多样化的模拟中生成包含精确物理特性的接触密集型轨迹，并将这些行为提炼成大规模多模态模型，直接转移到真实世界环境中。&lt;h4&gt;主要发现&lt;/h4&gt;通过在仿真数据上微调一个基础模型(Gemini)，可以让四足机器人用身体推动物体到达未知目标位置。这表明了使用模拟来赋予基础模型物理操作能力的巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了如何利用先进的技术手段，结合生成的多模态模型和逼真的仿真环境，使机器人能够有效处理需要高精度接触动态的任务，并成功地将这些技能转移到现实世界中。&lt;h4&gt;翻译&lt;/h4&gt;在机器人学习领域，目前常见的做法要么忽略环境语义，集中于仅需考虑机器人与环境接触推理的任务（如全身控制），要么忽视接触动力学，专注于通过视觉和语言来理解高层次的动作。本文展示了一种结合生成模型、逼真渲染及程序化生成技术的方法，能够同时处理需要两者兼顾的复杂任务。通过在语义多样化的模拟中生成包含精确物理特性的接触密集型轨迹，并将这些行为提炼成大规模多模态模型，可以直接转移到真实世界环境中。具体而言，一个经过微调的基础模型（Gemini），仅基于仿真数据，可以通过语言指令控制四足机器人用身体推动物体到达未知的目标位置，在未见过的真实世界环境里进行测试。实验结果证明了使用模拟技术为基本模型赋予物理操作能力的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In robot learning, it is common to either ignore the environment semantics,focusing on tasks like whole-body control which only require reasoning aboutrobot-environment contacts, or conversely to ignore contact dynamics, focusingon grounding high-level movement in vision and language. In this work, we showthat advances in generative modeling, photorealistic rendering, and proceduralgeneration allow us to tackle tasks requiring both. By generating contact-richtrajectories with accurate physics in semantically-diverse simulations, we candistill behaviors into large multimodal models that directly transfer to thereal world: a system we call Proc4Gem. Specifically, we show that a foundationmodel, Gemini, fine-tuned on only simulation data, can be instructed inlanguage to control a quadruped robot to push an object with its body to unseentargets in unseen real-world environments. Our real-world results demonstratethe promise of using simulation to imbue foundation models with physicalagency. Videos can be found at our website:https://sites.google.com/view/proc4gem</description>
      <author>example@mail.com (Yixin Lin, Jan Humplik, Sandy H. Huang, Leonard Hasenclever, Francesco Romano, Stefano Saliceti, Daniel Zheng, Jose Enrique Chen, Catarina Barros, Adrian Collister, Matt Young, Adil Dostmohamed, Ben Moran, Ken Caluwaerts, Marissa Giustina, Joss Moore, Kieran Connell, Francesco Nori, Nicolas Heess, Steven Bohez, Arunkumar Byravan)</author>
      <guid isPermaLink="false">2503.08593v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>MetaFold: Language-Guided Multi-Category Garment Folding Framework via Trajectory Generation and Foundation Model</title>
      <link>http://arxiv.org/abs/2503.08372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种名为MetaFold的框架，用于解决服装折叠任务中的机器人操作问题。该框架通过将任务规划与动作预测分离来提高模型泛化能力。&lt;h4&gt;背景&lt;/h4&gt;服装折叠是常见的机器人操作任务之一，但其挑战在于衣物具有变形性，导致状态空间巨大且动态复杂，使得精确和精细的操作变得困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够独立学习任务规划和动作预测的框架MetaFold，以增强模型在不同类别服装上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;该框架使用语言引导的点云轨迹生成进行任务规划，并利用低级基础模型进行动作预测。这种结构有助于多类别的学习，使模型可以灵活适应各种用户指令和折叠任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的MetaFold框架在服装折叠任务上具有优越性能。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了一个新的视角来解决机器人操作中的服装折叠问题，并且展示了其泛化能力和灵活性。补充材料可以在我们的网站上找到：https://meta-fold.github.io/。&lt;h4&gt;翻译&lt;/h4&gt;服装折叠是机器人操作中常见但具有挑战性的任务。衣物的可变形性导致了巨大的状态空间和复杂的动力学，使得精确而精细的操作变得复杂。以往的方法通常依赖于预定义的关键点或演示，这限制了它们在不同类别的服装上的泛化能力。这项工作提出了一种名为MetaFold的框架，该框架通过分离任务规划与动作预测来独立学习这两者，以增强模型的泛化性。它使用语言指导的点云轨迹生成进行任务规划，并利用低级基础模型进行动作预测。这种结构促进了多类别学习，使模型能够灵活适应各种用户指令和折叠任务。实验结果表明了我们提出框架的优势。补充材料可以在我们的网站上找到：https://meta-fold.github.io/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Garment folding is a common yet challenging task in robotic manipulation. Thedeformability of garments leads to a vast state space and complex dynamics,which complicates precise and fine-grained manipulation. Previous approachesoften rely on predefined key points or demonstrations, limiting theirgeneralization across diverse garment categories. This paper presents aframework, MetaFold, that disentangles task planning from action prediction,learning each independently to enhance model generalization. It employslanguage-guided point cloud trajectory generation for task planning and alow-level foundation model for action prediction. This structure facilitatesmulti-category learning, enabling the model to adapt flexibly to various userinstructions and folding tasks. Experimental results demonstrate thesuperiority of our proposed framework. Supplementary materials are available onour website: https://meta-fold.github.io/.</description>
      <author>example@mail.com (Haonan Chen, Junxiao Li, Ruihai Wu, Yiwei Liu, Yiwen Hou, Zhixuan Xu, Jingxiang Guo, Chongkai Gao, Zhenyu Wei, Shensi Xu, Jiaqi Huang, Lin Shao)</author>
      <guid isPermaLink="false">2503.08372v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Scale-Aware Pre-Training for Human-Centric Visual Perception: Enabling Lightweight and Generalizable Models</title>
      <link>http://arxiv.org/abs/2503.08201v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;人类中心视觉感知（HVP）由于大规模自监督预训练技术的进步而取得了显著进展，但现有的HVP模型在适应现实世界应用时存在限制。&lt;h4&gt;背景&lt;/h4&gt;当前的HVP模型难以适应实际应用场景的需求，这些场景需要一般化的视觉模式来支持下游任务，并且还要保证计算成本可控以适配边缘设备。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自监督预训练框架Scale-Aware Image Pretraining (SAIP)，旨在解决现有HVP模型在泛化能力和模型大小方面的限制问题。&lt;h4&gt;方法&lt;/h4&gt;SAIP引入了三个基于跨尺度一致性的学习目标：1）跨尺度匹配（CSM），通过对比学习从多尺度单人图像中提取不变的视觉模式；2）跨尺度重建（CSR），从多尺度掩码单人图像中学习像素级的一致性结构；3）跨尺度搜索（CSS），从多尺度多人图像中捕获多样化的模式。这三个目标相互补充，使轻量模型能够学习对HVP下游任务至关重要的多尺度泛化模式。&lt;h4&gt;主要发现&lt;/h4&gt;通过在12个HVP数据集上进行的广泛实验表明，SAIP展示了卓越的跨9个人体中心视觉任务的泛化能力，并且相对于现有方法，在单人分辨、密集预测和多人视觉理解任务中分别取得了3%-13%、1%-11%和1%-6%的表现改进。&lt;h4&gt;结论&lt;/h4&gt;提出的Scale-Aware Image Pretraining (SAIP)框架显著提高了轻量级模型在处理HVP下游任务时的泛化能力和性能表现，解决了现有方法的一些局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human-centric visual perception (HVP) has recently achieved remarkableprogress due to advancements in large-scale self-supervised pretraining (SSP).However, existing HVP models face limitations in adapting to real-worldapplications, which require general visual patterns for downstream tasks whilemaintaining computationally sustainable costs to ensure compatibility with edgedevices. These limitations primarily arise from two issues: 1) the pretrainingobjectives focus solely on specific visual patterns, limiting thegeneralizability of the learned patterns for diverse downstream tasks; and 2)HVP models often exhibit excessively large model sizes, making themincompatible with real-world applications. To address these limitations, weintroduce Scale-Aware Image Pretraining (SAIP), a novel SSP framework enablinglightweight vision models to acquire general patterns for HVP. Specifically,SAIP incorporates three learning objectives based on the principle ofcross-scale consistency: 1) Cross-scale Matching (CSM) which contrastivelylearns image-level invariant patterns from multi-scale single-person images; 2)Cross-scale Reconstruction (CSR) which learns pixel-level consistent visualstructures from multi-scale masked single-person images; and 3) Cross-scaleSearch (CSS) which learns to capture diverse patterns from multi-scalemulti-person images. Three objectives complement one another, enablinglightweight models to learn multi-scale generalizable patterns essential forHVP downstream tasks.Extensive experiments conducted across 12 HVP datasetsdemonstrate that SAIP exhibits remarkable generalization capabilities across 9human-centric vision tasks. Moreover, it achieves significant performanceimprovements over existing methods, with gains of 3%-13% in single-persondiscrimination tasks, 1%-11% in dense prediction tasks, and 1%-6% inmulti-person visual understanding tasks.</description>
      <author>example@mail.com (Xuanhan Wang, Huimin Deng, Lianli Gao, Jingkuan Song)</author>
      <guid isPermaLink="false">2503.08201v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>MaRI: Material Retrieval Integration across Domains</title>
      <link>http://arxiv.org/abs/2503.08111v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;准确的材料检索对于创建逼真的3D资产至关重要。现有的方法依赖于捕获形状不变和光照变化表示的材料数据集，这些数据集稀缺且面临着多样性有限以及现实世界泛化能力不足的问题。&lt;h4&gt;背景&lt;/h4&gt;当前的方法主要采用传统的图像搜索技术，但它们在捕捉材料空间的独特属性方面存在不足，导致检索任务表现不佳。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，我们提出了MaRI框架，旨在弥合合成和真实世界材料之间的特征空间差距。&lt;h4&gt;方法&lt;/h4&gt;MaRI通过对比学习策略联合训练图像编码器和材料编码器来构建共享嵌入空间，该策略使相似的材料和图像在特征空间中更接近并区分不相似的对。为此，我们创建了一个包含高质量合成材料的数据集，这些材料是在控制形状变化的情况下渲染，并具有多样化的光照条件以及通过材料转移技术处理和标准化的真实世界材料。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，MaRI在各种复杂材料检索任务中表现出了优越的性能、准确性及泛化能力，超过了现有的方法。&lt;h4&gt;结论&lt;/h4&gt;MaRI框架为解决材料检索问题提供了一种有效的方法，尤其适用于处理合成与真实世界的差异，展现了其在3D资产创建领域的潜在价值。&lt;h4&gt;翻译&lt;/h4&gt;准确的材料检索是创造逼真的三维（3D）资源的关键。当前的技术依赖于捕获形状不变、光照变化显著的数据集，这些数据集因多样性不足和现实世界泛化能力有限而面临挑战。大多数现有方法采用传统的图像搜索技术，在捕捉材料特性方面存在局限性。MaRI框架通过对比学习策略解决了这一问题，该策略促进了图像编码器与材料编码器的联合训练，并构建了一个综合的数据集来支持研究。实验证明了MaRI在复杂的检索任务中比现有的方法更为优越。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate material retrieval is critical for creating realistic 3D assets.Existing methods rely on datasets that capture shape-invariant andlighting-varied representations of materials, which are scarce and facechallenges due to limited diversity and inadequate real-world generalization.Most current approaches adopt traditional image search techniques. They fallshort in capturing the unique properties of material spaces, leading tosuboptimal performance in retrieval tasks. Addressing these challenges, weintroduce MaRI, a framework designed to bridge the feature space gap betweensynthetic and real-world materials. MaRI constructs a shared embedding spacethat harmonizes visual and material attributes through a contrastive learningstrategy by jointly training an image and a material encoder, bringing similarmaterials and images closer while separating dissimilar pairs within thefeature space. To support this, we construct a comprehensive dataset comprisinghigh-quality synthetic materials rendered with controlled shape variations anddiverse lighting conditions, along with real-world materials processed andstandardized using material transfer techniques. Extensive experimentsdemonstrate the superior performance, accuracy, and generalization capabilitiesof MaRI across diverse and complex material retrieval tasks, outperformingexisting methods.</description>
      <author>example@mail.com (Jianhui Wang, Zhifei Yang, Yangfan He, Huixiong Zhang, Yuxuan Chen, Jingwei Huang)</author>
      <guid isPermaLink="false">2503.08111v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Uni$\textbf{F}^2$ace: Fine-grained Face Understanding and Generation with Unified Multimodal Models</title>
      <link>http://arxiv.org/abs/2503.08120v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UniF^2ace是一个专为精细面部理解与生成设计的统一多模态模型，采用自建的数据集和两种互惠扩散技术以及双层专家混合架构。&lt;h4&gt;背景&lt;/h4&gt;现有研究主要关注粗糙面部属性的理解，缺乏处理细粒度面部属性及生成能力的能力。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些限制，提出了一种专为精细面部理解和生成设计的统一多模态模型UniF^2ace。&lt;h4&gt;方法&lt;/h4&gt;构建了一个大型面部数据集UniF^2ace-130K；建立了离散扩散评分匹配与掩码生成模型之间的理论联系，并同时优化了证据下界，提升了合成面部细节的能力。引入令牌级和序列级专家混合架构以实现高效的细粒度表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;在自建数据集UniF^2ace-130K上的大量实验表明，UniF^2ace优于现有统一多模态模型及生成模型，在理解和生成任务中均表现出色。&lt;h4&gt;结论&lt;/h4&gt;UniF^2ace为解决细粒度面部理解与生成问题提供了一种有效的方法，并在一系列基准测试中显示了卓越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unified multimodal models (UMMs) have emerged as a powerful paradigm infoundational computer vision research, demonstrating significant potential inboth image understanding and generation. However, existing research in the facedomain primarily focuses on $\textbf{coarse}$ facial attribute understanding,with limited capacity to handle $\textbf{fine-grained}$ facial attributes andwithout addressing generation capabilities. To overcome these limitations, wepropose Uni$\textbf{F}^2$ace, the first UMM tailored specifically forfine-grained face understanding and generation. In general, we trainUni$\textbf{F}^2$ace on a self-constructed, specialized dataset utilizing twomutually beneficial diffusion techniques and a two-level mixture-of-expertsarchitecture. Specifically, we first build a large-scale facial dataset,Uni$\textbf{F}^2$ace-130K, which contains 130K image-text pairs with onemillion question-answering pairs that span a wide range of facial attributes.Second, we establish a theoretical connection between discrete diffusion scorematching and masked generative models, optimizing both evidence lower boundssimultaneously, which significantly improves the model's ability to synthesizefacial details. Finally, we introduce both token-level and sequence-levelmixture-of-experts, enabling efficient fine-grained representation learning forboth understanding and generation tasks. Extensive experiments onUni$\textbf{F}^2$ace-130K demonstrate that Uni$\textbf{F}^2$ace outperformsexisting UMMs and generative models, achieving superior performance across bothunderstanding and generation tasks.</description>
      <author>example@mail.com (Junzhe Li, Xuerui Qiu, Linrui Xu, Liya Guo, Delin Qu, Tingting Long, Chun Fan, Ming Li)</author>
      <guid isPermaLink="false">2503.08120v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>ESPnet-SDS: Unified Toolkit and Demo for Spoken Dialogue Systems</title>
      <link>http://arxiv.org/abs/2503.08533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NAACL 2025 Demo Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究介绍了一款开源且用户友好的工具包，用于构建统一的网络界面来支持各种级联和端到端语音对话系统。&lt;h4&gt;背景&lt;/h4&gt;音频基础模型的进步激发了对端到端语音对话系统的兴趣，但不同系统的网页接口多样化使比较变得困难。&lt;h4&gt;目的&lt;/h4&gt;设计并开发一个开源、用户友好的工具包，用于构建统一的网络界面，支持不同的级联和端到端语音对话系统，并提供在线自动化评估指标。&lt;h4&gt;方法&lt;/h4&gt;该工具包为用户提供实时获取各种自动评估指标（如延迟时间、理解能力、一致性、多样性与相关性以及系统的输出清晰度和音频质量）的功能。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用这些评价标准，研究团队比较了不同的级联式和端到端语音对话系统，并且分析结果表明当前的端到端系统在音质上不如传统的级联式系统，并且其回复较少多样。&lt;h4&gt;结论&lt;/h4&gt;该工具包为研究人员提供了轻松对比不同技术的方法，从而提供宝贵的见解。演示程序公开可用：https://huggingface.co/spaces/Siddhant/Voice_Assistant_Demo。&lt;h4&gt;翻译&lt;/h4&gt;在语音基础模型进步的推动下，研究者们开发了一个开源、用户友好的工具包来构建统一的网络界面，以支持各种级联和端到端的语音对话系统。该工具还提供实时自动评估指标。通过这些评价标准，研究人员可以比较不同技术，并且分析表明当前的端到端系统在音质上较差并且回复较少多样。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancements in audio foundation models (FMs) have fueled interest inend-to-end (E2E) spoken dialogue systems, but different web interfaces for eachsystem makes it challenging to compare and contrast them effectively. Motivatedby this, we introduce an open-source, user-friendly toolkit designed to buildunified web interfaces for various cascaded and E2E spoken dialogue systems.Our demo further provides users with the option to get on-the-fly automatedevaluation metrics such as (1) latency, (2) ability to understand user input,(3) coherence, diversity, and relevance of system response, and (4)intelligibility and audio quality of system output. Using the evaluationmetrics, we compare various cascaded and E2E spoken dialogue systems with ahuman-human conversation dataset as a proxy. Our analysis demonstrates that thetoolkit allows researchers to effortlessly compare and contrast differenttechnologies, providing valuable insights such as current E2E systems havingpoorer audio quality and less diverse responses. An example demo produced usingour toolkit is publicly available here:https://huggingface.co/spaces/Siddhant/Voice_Assistant_Demo.</description>
      <author>example@mail.com (Siddhant Arora, Yifan Peng, Jiatong Shi, Jinchuan Tian, William Chen, Shikhar Bharadwaj, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Shuichiro Shimizu, Vaibhav Srivastav, Shinji Watanabe)</author>
      <guid isPermaLink="false">2503.08533v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Unmasking the Unknown: Facial Deepfake Detection in the Open-Set Paradigm</title>
      <link>http://arxiv.org/abs/2503.08055v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于监督对比学习的开放式深度伪造分类算法，旨在解决现有闭集模型在检测新出现且未见过的深度伪造技术时存在的局限性。&lt;h4&gt;背景&lt;/h4&gt;面部伪造方法（如deepfakes）可用于身份操纵和传播错误信息。随着生成式AI的进步，这些伪造方法变得更加复杂和多样化，超越了现有的已知手段。&lt;h4&gt;目的&lt;/h4&gt;提出一种开放式框架来检测未知来源的深度伪造内容，并将其标记为“未知”，而不仅仅是未被篡改的真实图像。&lt;h4&gt;方法&lt;/h4&gt;采用监督对比学习技术开发了一种开放集深度伪造分类算法，该模型能够区分由已知和未知方法生成的伪造图像。&lt;h4&gt;主要发现&lt;/h4&gt;在FaceForensics++数据集上进行了实验验证，结果显示该方法对于检测未见过的方法产生的深度伪造内容具有优异的表现，并且在将真实图像与已知深度伪造区分开来方面也表现出色。&lt;h4&gt;结论&lt;/h4&gt;所提出的开放集框架为应对未来不断出现的新型深度伪造技术提供了更加可靠和有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;面部伪造方法如deepfakes可用于身份操纵和传播错误信息。随着生成式AI的进步，这些伪造方法变得更加复杂和多样化，超越了现有的已知手段。传统深度伪造检测方法采用闭集模型，这限制了它们在识别使用训练数据集中未包含的方法创建的伪造图像时的有效性。本文提出了一种从封闭集框架转向开放集框架的方法来应对这一挑战。在开放集框架中，模型不仅可以识别由已知面部伪造技术生成的图像，还能将未知方法产生的图像标记为‘未知’而非真实/未经篡改。基于监督对比学习技术提出了一个开放集深度伪造分类算法，该模型可以作为处理新兴和未见过的深度伪造手段的更强大的工具，提高可靠性和信心，并补充法医分析。在所提出的框架中识别了三个群体：包括既不属于已知深度伪造也不属于真实图像的‘未知’组。本文研究了三种场景下的深度伪造开放集分类问题：将由未知方法产生的深度伪造分类为非真实、区分真实图像与深度伪造，以及对由已知方法生成的深度伪造进行分类。实验结果表明，在前两个任务中达到了最先进的性能，并在第三个任务中取得了具有竞争力的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Facial forgery methods such as deepfakes can be misused for identitymanipulation and spreading misinformation. They have evolved alongsideadvancements in generative AI, leading to new and more sophisticated forgerytechniques that diverge from existing 'known' methods. Conventional deepfakedetection methods use the closedset paradigm, thus limiting their applicabilityto detecting forgeries created using methods that are not part of the trainingdataset. In this paper, we propose a shift from the closed-set paradigm fordeepfake detection. In the open-set paradigm, models are designed not only toidentify images created by known facial forgery methods but also to identifyand flag those produced by previously unknown methods as 'unknown' and not asunforged/real/unmanipulated. In this paper, we propose an open-set deepfakeclassification algorithm based on supervised contrastive learning. The open-setparadigm used in our model allows it to function as a more robust tool capableof handling emerging and unseen deepfake techniques, enhancing reliability andconfidence, and complementing forensic analysis. In open-set paradigm, weidentify three groups including the "unknown group that is neither consideredknown deepfake nor real. We investigate deepfake open-set classification acrossthree scenarios, classifying deepfakes from unknown methods not as real,distinguishing real images from deepfakes, and classifying deepfakes from knownmethods, using the FaceForensics++ dataset as a benchmark. Our method achievesstate of the art results in the first two tasks and competitive results in thethird task.</description>
      <author>example@mail.com (Nadarasar Bahavan, Sanjay Saha, Ken Chen, Sachith Seneviratne, Sanka Rasnayaka, Saman Halgamuge)</author>
      <guid isPermaLink="false">2503.08055v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>SphOR: A Representation Learning Perspective on Open-set Recognition for Identifying Unknown Classes in Deep Learning Models</title>
      <link>http://arxiv.org/abs/2503.08049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于开放集识别（OSR）的计算效率高的表示学习方法SphOR，该方法通过球形嵌入技术将特征空间建模为von Mises-Fisher分布的混合模型。&lt;h4&gt;背景&lt;/h4&gt;深度学习分类器在广泛应用中需要具备处理已知类别和未知类别的能力。现有的许多开放集识别（OSR）方法由于依赖复杂的生成模型而计算成本高昂或训练成本高。&lt;h4&gt;目的&lt;/h4&gt;从表示学习的角度研究OSR，引入SphOR方法以解决现有OSR方法的计算效率问题，并利用语义模糊样本提高对未知类别样本的检测性能。&lt;h4&gt;方法&lt;/h4&gt;通过球形嵌入技术将特征空间建模为von Mises-Fisher分布混合模型，从而在训练时可以使用具有不确定性的语义样本，提高识别未知类别的能力。&lt;h4&gt;主要发现&lt;/h4&gt;研究探讨了OSR性能与关键表示学习属性之间的关系，并展示了SphOR方法在多个开放集识别基准上的有效性，达到了最先进的结果，相较于其他方法提高了多达6%的准确率。&lt;h4&gt;结论&lt;/h4&gt;实验验证了SphOR方法的有效性和优越性，在提高计算效率的同时提升了对未知类别的检测能力。&lt;h4&gt;翻译&lt;/h4&gt;深度学习分类器的广泛应用需要具备开放集识别（OSR）功能，这可以使得输入数据不仅能够被训练期间已知类别所识别，还能够识别测试数据中存在的未知类别。许多现有的OSR方法由于依赖复杂的生成模型而导致计算成本高昂或者由于高训练成本而遭受损失。我们从表示学习的角度研究了OSR问题，并特别通过球形嵌入技术来解决这一问题。我们引入了SphOR，这是一种高效的表示学习方法，将特征空间建模为von Mises-Fisher分布的混合体。这种方法使我们在训练时可以利用语义模糊样本，从而提高对未知类别样本的检测性能。我们进一步探讨了OSR表现与关键表示学习属性之间的关系，这些属性影响在高维空间中如何更好地结构化特征。通过多个开放集识别基准上的大量实验表明我们的方法是有效的，并且取得了最先进的结果，在某些情况下达到了最多6%的改善，从而验证了其性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The widespread use of deep learning classifiers necessitates Open-setrecognition (OSR), which enables the identification of input data not only fromclasses known during training but also from unknown classes that might bepresent in test data. Many existing OSR methods are computationally expensivedue to the reliance on complex generative models or suffer from high trainingcosts. We investigate OSR from a representation-learning perspective,specifically through spherical embeddings. We introduce SphOR, acomputationally efficient representation learning method that models thefeature space as a mixture of von Mises-Fisher distributions. This approachenables the use of semantically ambiguous samples during training, to improvethe detection of samples from unknown classes. We further explore therelationship between OSR performance and key representation learning propertieswhich influence how well features are structured in high-dimensional space.Extensive experiments on multiple OSR benchmarks demonstrate the effectivenessof our method, producing state-of-the-art results, with improvements up-to 6%that validate its performance.</description>
      <author>example@mail.com (Nadarasar Bahavan, Sachith Seneviratne, Saman Halgamuge)</author>
      <guid isPermaLink="false">2503.08049v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Deep ARTMAP: Generalized Hierarchical Learning with Adaptive Resonance Theory</title>
      <link>http://arxiv.org/abs/2503.07641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了Deep ARTMAP，这是一种新颖的ARTMAP架构扩展，它将自一致模块化ART（SMART）体系结构泛化为支持任意数据变换下的层次学习。&lt;h4&gt;背景&lt;/h4&gt;现有的ARTMAP和SMART模型在处理复杂的数据变换以及实现多层次学习方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型框架Deep ARTMAP，该框架能够灵活地支持监督与非监督的学习模式，并且可以在不同层之间进行任意数据转换的层次化学习。&lt;h4&gt;方法&lt;/h4&gt;通过将SMART架构泛化到多个模块中来构建Deep ARTMAP框架。每个模块可以自定义粒度，而跨ART模块调节每一层中的聚类过程，以允许无监督学习同时强制执行一层到下一层的一对多映射关系。&lt;h4&gt;主要发现&lt;/h4&gt;Deep ARTMAP框架作为一种分割式集群机制操作，并且在特定配置中可以退化为传统的ARTMAP和SMART模型。此外，它比现有的系统提供了更广泛的灵活性。&lt;h4&gt;结论&lt;/h4&gt;该研究成功地开发了一种新的层次学习方法，即Deep ARTMAP，这使得基于ART的神经网络能够处理更多的数据变换类型以及实现更为灵活的学习方式。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容已全部由英文转为中文。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents Deep ARTMAP, a novel extension of the ARTMAP architecturethat generalizes the self-consistent modular ART (SMART) architecture to enablehierarchical learning (supervised and unsupervised) across arbitrarytransformations of data. The Deep ARTMAP framework operates as a divisiveclustering mechanism, supporting an arbitrary number of modules withcustomizable granularity within each module. Inter-ART modules regulate theclustering at each layer, permitting unsupervised learning while enforcing aone-to-many mapping from clusters in one layer to the next. While Deep ARTMAPreduces to both ARTMAP and SMART in particular configurations, it offerssignificantly enhanced flexibility, accommodating a broader range of datatransformations and learning modalities.</description>
      <author>example@mail.com (Niklas M. Melton, Leonardo Enzo Brito da Silva, Sasha Petrenko, Donald. C. Wunsch II)</author>
      <guid isPermaLink="false">2503.07641v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>LongProLIP: A Probabilistic Vision-Language Model with Long Context Text</title>
      <link>http://arxiv.org/abs/2503.08048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a tiny paper at the 1st workshop of "Quantify Uncertainty  and Hallucination in Foundation Models: The Next Frontier in Reliable AI" at  ICLR 2025; code: https://github.com/naver-ai/prolip; models:  https://huggingface.co/collections/SanghyukChun/prolip-6712595dfc87fd8597350291&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;最近提出了Probabilistic Language-Image Pre-Training（ProLIP）来解决视觉语言任务中的多重性问题。尽管在大规模概率表示学习方面取得了一定成功，但现有的ProLIP模型无法处理超过64个上下文长度的长文本，这限制了它们捕捉丰富上下文信息的能力。&lt;h4&gt;背景&lt;/h4&gt;现有Probabilistic Language-Image Pre-Training（ProLIP）模型虽然解决了视觉语言任务中的多重性问题，并且在概率表示学习上取得了一定成功，但是无法处理超过64个长度的长文本。&lt;h4&gt;目的&lt;/h4&gt;提出一种针对Probabilistic Language-Image Pre-Training（ProLIP）模型的微调策略，使其能够接受更长的文本，如256个文本标记，并在确保不会产生负面影响的情况下提高对长上下文的理解能力。&lt;h4&gt;方法&lt;/h4&gt;提出了LongProLIP方案作为解决现有问题的方法。通过实验研究证明了该方案的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;通过Urban-1k和DataComp评估套件上的实验结果表明，所提出的LongProLIP策略能够提高对长上下文的理解能力，并且在微调过程中最小化负面影响。还观察到在理解长文本与零样本泛化能力之间的权衡关系。&lt;h4&gt;结论&lt;/h4&gt;该研究通过改进Probabilistic Language-Image Pre-Training（ProLIP）模型的微调策略，使其能够处理更长的上下文长度，并且提高了其理解和泛化的能力。不过也发现了解决长文本理解与零样本泛化的性能之间的权衡问题。&lt;h4&gt;翻译&lt;/h4&gt;最近提出了概率语言图像预训练(Probabilistic Language-Image Pre-Training, ProLIP)来解决视觉语言任务中的多重性问题。虽然这种方法在大规模的概率表示学习上已经取得了一定的成功，但是现有的ProLIP模型仍然无法处理超过64个上下文长度的长文本，从而限制了它们捕捉丰富上下文信息的能力。为了克服这个挑战，本文提出了一种针对ProLIP进行微调的技术，使其能够接受更长的文本（例如256个标记）。实验结果表明，在Urban-1k和DataComp评估套件上应用提出的LongProLIP策略可以提高对较长文本的理解能力同时减少由于微调产生的负面影响。另外还观察到了在理解长上下文能力和零样本泛化能力之间的权衡现象：前者由Urban-1k衡量，而后者则通过ImageNet或38个零样本评估数据集的平均值来衡量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Probabilistic Language-Image Pre-Training (ProLIP) has beenproposed to tackle the multiplicity issue of vision-language (VL) tasks.Despite their success in probabilistic representation learning at a scale, theProLIP models cannot handle long context texts longer than 64 context length,which limits their ability to capture rich contextual information from longertext sequences. To address this issue, this paper proposes a fine-tuningstrategy for ProLIP to accept longer texts, e.g., 256 text tokens. Experimentalresults on Urban-1k and the DataComp evaluation suite show that the proposedLongProLIP recipe can improve understanding of long contexts while minimizingthe negative effect of fine-tuning. We also observe a trade-off between thelong context understanding (measured by Urban-1k) and general zero-shotcapability (measured by ImageNet or the average of 38 zero-shot evaluationdatasets by DataComp).</description>
      <author>example@mail.com (Sanghyuk Chun, Sangdoo Yun)</author>
      <guid isPermaLink="false">2503.08048v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Ambiguities in 3D Classification with Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2503.08352v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种基于高斯点云的3D分类新方法，旨在解决现有点云表示在区分电线样表面、平面表面以及透明或反射物体时存在的问题。&lt;h4&gt;背景&lt;/h4&gt;点云输入的3D分类是计算机视觉中的基本问题。由于点云表示的离散性质和对材料描述不足的问题，在区分一些特定类型的表面（如细长表面和平面）及透明或反射物体方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于高斯点云的方法，以提高在3D分类任务中特别是在处理难以区分的对象时的表现。&lt;h4&gt;方法&lt;/h4&gt;通过利用GS（Gaussian Splatting）点云的尺度和旋转系数来表征不同的表面类型，并且使用其不透明度表示物体的透明特性。建立了首个用于研究的真实世界高斯点云数据集，包含20个类别，每个类别有200个对象。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证了GS点云输入在区分模糊或难以辨别的3D对象方面的优越性，并且展示了该方法在不同分类算法中的通用性。&lt;h4&gt;结论&lt;/h4&gt;提出的方法有效解决了传统点云表示中存在的一些难题，提升了基于高斯点云的3D分类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D classification with point cloud input is a fundamental problem in 3Dvision. However, due to the discrete nature and the insufficient materialdescription of point cloud representations, there are ambiguities indistinguishing wire-like and flat surfaces, as well as transparent orreflective objects. To address these issues, we propose Gaussian Splatting (GS)point cloud-based 3D classification. We find that the scale and rotationcoefficients in the GS point cloud help characterize surface types.Specifically, wire-like surfaces consist of multiple slender Gaussianellipsoids, while flat surfaces are composed of a few flat Gaussian ellipsoids.Additionally, the opacity in the GS point cloud represents the transparencycharacteristics of objects. As a result, ambiguities in point cloud-based 3Dclassification can be mitigated utilizing GS point cloud as input. To verifythe effectiveness of GS point cloud input, we construct the first real-world GSpoint cloud dataset in the community, which includes 20 categories with 200objects in each category. Experiments not only validate the superiority of GSpoint cloud input, especially in distinguishing ambiguous objects, but alsodemonstrate the generalization ability across different classification methods.</description>
      <author>example@mail.com (Ruiqi Zhang, Hao Zhu, Jingyi Zhao, Qi Zhang, Xun Cao, Zhan Ma)</author>
      <guid isPermaLink="false">2503.08352v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>LightPlanner: Unleashing the Reasoning Capabilities of Lightweight Large Language Models in Task Planning</title>
      <link>http://arxiv.org/abs/2503.08508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新型任务规划器LightPlanner，旨在提升轻量级大型语言模型在复杂任务规划中的表现。&lt;h4&gt;背景&lt;/h4&gt;近年来，轻量级的大规模语言模型因其低计算资源需求和边缘部署的适用性，在机器人领域受到广泛关注。然而，这些模型在涉及动态语义逻辑推理的任务规划中表现出较差的效果。&lt;h4&gt;目的&lt;/h4&gt;为了克服这一局限，论文提出了一种新的任务规划器LightPlanner，通过充分利用轻量级LLM的推理能力来改进复杂任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;不同于传统的使用固定技能模板的计划者，LightPlanner通过参数化函数调用控制机器人动作，并动态生成参数值。此外，引入了分层深度推理机制，在每个操作决策步骤之前考虑行动执行、语义解析和参数生成三个层级。还集成了记忆模块来存储历史动作。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，训练LightPlanner-1.5B模型可以实现最高的任务成功率，并在涉及空间语义推理的任务中超过了ReAct的性能。此外，该规划器能够在边缘设备上运行。&lt;h4&gt;结论&lt;/h4&gt;论文成功展示了轻量级LLM如何通过改进的方法和模块设计，在复杂任务规划中的表现得以显著提升。&lt;h4&gt;翻译&lt;/h4&gt;近年来，由于计算资源需求低且适合边缘部署，轻量级大型语言模型（LLMs）在机器人领域受到极大关注。但在涉及动态语义逻辑推理的复杂任务中，这些模型的表现较差。为解决这个问题，我们提出了一种新的任务规划器LightPlanner，它通过充分发挥轻量化LLM的推理能力来提升其性能。与传统的使用固定技能模板的方法不同，LightPlanner通过参数化函数调用控制机器人动作，并动态生成参数值，从而实现精细的操作控制并提高复杂场景下的任务成功率。此外，我们引入了分层深度推理机制，在每个操作决策步骤之前考虑行动执行、语义解析和参数生成三个层级，确保后续操作的准确性。另外还集成了记忆模块来存储历史动作以减少上下文长度并提升长期任务的规划效率。我们在包含40,000个动作控制的LightPlan-40k数据集上训练了LightPlanner-1.5B模型，并在实验中展示了其优于同类模型的表现，尤其在空间语义推理的任务中表现出色。此外，我们还证明了LightPlanner能够在边缘设备上运行的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/unira-zwj/LightPlanner&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, lightweight large language models (LLMs) have garneredsignificant attention in the robotics field due to their low computationalresource requirements and suitability for edge deployment. However, in taskplanning -- particularly for complex tasks that involve dynamic semantic logicreasoning -- lightweight LLMs have underperformed. To address this limitation,we propose a novel task planner, LightPlanner, which enhances the performanceof lightweight LLMs in complex task planning by fully leveraging theirreasoning capabilities. Unlike conventional planners that use fixed skilltemplates, LightPlanner controls robot actions via parameterized functioncalls, dynamically generating parameter values. This approach allows forfine-grained skill control and improves task planning success rates in complexscenarios. Furthermore, we introduce hierarchical deep reasoning. Beforegenerating each action decision step, LightPlanner thoroughly considers threelevels: action execution (feedback verification), semantic parsing (goalconsistency verification), and parameter generation (parameter validityverification). This ensures the correctness of subsequent action controls.Additionally, we incorporate a memory module to store historical actions,thereby reducing context length and enhancing planning efficiency for long-termtasks. We train the LightPlanner-1.5B model on our LightPlan-40k dataset, whichcomprises 40,000 action controls across tasks with 2 to 13 action steps.Experiments demonstrate that our model achieves the highest task success ratedespite having the smallest number of parameters. In tasks involving spatialsemantic reasoning, the success rate exceeds that of ReAct by 14.9 percent.Moreover, we demonstrate LightPlanner's potential to operate on edge devices.</description>
      <author>example@mail.com (Weijie Zhou, Yi Peng, Manli Tao, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang)</author>
      <guid isPermaLink="false">2503.08508v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>CFNet: Optimizing Remote Sensing Change Detection through Content-Aware Enhancement</title>
      <link>http://arxiv.org/abs/2503.08505v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Content Focuser Network (CFNet)的网络，该网络通过内容感知策略来提高时间序列遥感图像变化检测任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;变化检测是遥感领域中一个关键且广泛应用的任务。由于获取条件的变化，同一地理区域在不同时间点的双时相遥感影像风格差异明显，影响了模型准确检测变化区域的能力。&lt;h4&gt;目的&lt;/h4&gt;为了应对上述挑战，提出了一种新的网络结构CFNet来增强对内容特征的关注，并减轻样式特征带来的误导效应。&lt;h4&gt;方法&lt;/h4&gt;CFNet使用EfficientNet-B5作为主干提取特征。通过开发一种优先考虑双时相图像内容特征的约束策略（Content-Aware），同时设计基于余弦距离的重新加权模块Focuser，使模型能够根据不同阶段的需求灵活聚焦于变化和未变化区域。&lt;h4&gt;主要发现&lt;/h4&gt;CFNet在三个知名的变更检测数据集上表现出色：CLCD (F1: 81.41%, IoU: 68.65%)、LEVIR-CD(F1: 92.18%, IoU: 85.49%)和SYSU-CD (F1: 82.89%, IoU: 70.78%)。&lt;h4&gt;结论&lt;/h4&gt;通过引入内容感知策略，CFNet在解决双时相遥感图像变化检测中的风格差异问题上取得了显著效果，并且其代码和预训练模型已公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Change detection is a crucial and widely applied task in remote sensing,aimed at identifying and analyzing changes occurring in the same geographicalarea over time. Due to variability in acquisition conditions, bi-temporalremote sensing images often exhibit significant differences in image style.Even with the powerful generalization capabilities of DNNs, these unpredictablestyle variations between bi-temporal images inevitably affect model's abilityto accurately detect changed areas. To address issue above, we propose theContent Focuser Network (CFNet), which takes content-aware strategy as a keyinsight. CFNet employs EfficientNet-B5 as the backbone for feature extraction.To enhance the model's focus on the content features of images while mitigatingthe misleading effects of style features, we develop a constraint strategy thatprioritizes the content features of bi-temporal images, termed Content-Aware.Furthermore, to enable the model to flexibly focus on changed and unchangedareas according to the requirements of different stages, we design areweighting module based on the cosine distance between bi-temporal imagefeatures, termed Focuser. CFNet achieve outstanding performance across threewell-known change detection datasets: CLCD (F1: 81.41%, IoU: 68.65%), LEVIR-CD(F1: 92.18%, IoU: 85.49%), and SYSU-CD (F1: 82.89%, IoU: 70.78%). The code andpretrained models of CFNet are publicly released athttps://github.com/wifiBlack/CFNet.</description>
      <author>example@mail.com (Fan Wu, Sijun Dong, Xiaoliang Meng)</author>
      <guid isPermaLink="false">2503.08505v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>SKALD: Learning-Based Shot Assembly for Coherent Multi-Shot Video Creation</title>
      <link>http://arxiv.org/abs/2503.08010v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为SKALD的多镜头视频组装方法，该方法能够从候选镜头中构建连贯的视频序列，且不依赖于大量的文本信息。&lt;h4&gt;背景&lt;/h4&gt;现有的视频组装技术通常需要大量的人工注释或文字描述来保证视频的叙事连贯性。然而这些方式往往效率低下并且成本高昂。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在仅少量人工标注的情况下高效地构建视频序列的方法，以提高视频组装的速度和准确性。&lt;h4&gt;方法&lt;/h4&gt;{'Learned Clip Assembly (LCA) score': '这是一种基于学习的度量标准，用于衡量镜头之间的时空关系及语义联系，以此量化叙事连贯性。', '高效的beam-search算法': '通过LCA评分引导来解决组合多个镜头时遇到的指数级复杂问题。', '模型训练任务': {'Shot Coherence Learning': '利用对比学习区分连贯和非连贯序列。', 'Feature Regression': '将学习到的表示转换为实值连贯性分数。'}, '模型变体': {'SKALD-base': '仅依赖于视觉连贯性的基本模型。', 'SKALD-text': '当有辅助文本信息时，结合了文本信息的进阶版本。'}}&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，相比于现有的最新技术，在VSPD和MSV3C数据集上，SKALD在IoU指标上的提升高达48.6%，并且速度提高了43%。&lt;h4&gt;结论&lt;/h4&gt;用户研究进一步证实了该方法的有效性：参与者的偏好中，有45%倾向于使用SKALD组装的视频，而只有22%偏爱基于文本的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present SKALD, a multi-shot video assembly method that constructs coherentvideo sequences from candidate shots with minimal reliance on text. Central toour approach is the Learned Clip Assembly (LCA) score, a learning-based metricthat measures temporal and semantic relationships between shots to quantifynarrative coherence. We tackle the exponential complexity of combining multipleshots with an efficient beam-search algorithm guided by the LCA score. To trainour model effectively with limited human annotations, we propose two tasks forthe LCA encoder: Shot Coherence Learning, which uses contrastive learning todistinguish coherent and incoherent sequences, and Feature Regression, whichconverts these learned representations into a real-valued coherence score. Wedevelop two variants: a base SKALD model that relies solely on visual coherenceand SKALD-text, which integrates auxiliary text information when available.Experiments on the VSPD and our curated MSV3C datasets show that SKALD achievesan improvement of up to 48.6% in IoU and a 43% speedup over thestate-of-the-art methods. A user study further validates our approach, with 45%of participants favoring SKALD-assembled videos, compared to 22% preferringtext-based assembly methods.</description>
      <author>example@mail.com (Chen Yi Lu, Md Mehrab Tanjim, Ishita Dasgupta, Somdeb Sarkhel, Gang Wu, Saayan Mitra, Somali Chaterji)</author>
      <guid isPermaLink="false">2503.08010v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>CAD-VAE: Leveraging Correlation-Aware Latents for Comprehensive Fair Disentanglement</title>
      <link>http://arxiv.org/abs/2503.07938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CAD-VAE（考虑相关性的解缠变分自编码器）的方法，旨在解决深度生成模型在学习表示时可能存在的偏见和公平性问题。&lt;h4&gt;背景&lt;/h4&gt;深度生成模型虽然显著推进了表征学习，但可能会继承或放大敏感属性与预测特征中存在的偏见及公平性问题。强制执行严格的解缠分离（独立编码敏感因素）通常是不现实的，因为目标和敏感因素之间存在自然关联。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来解决深度生成模型在表示学习过程中存在的偏见和公平性问题。&lt;h4&gt;方法&lt;/h4&gt;引入一个相关隐变量以捕捉目标属性和敏感属性之间的共享信息。通过最小化目标编码与敏感编码之间的条件互信息，直接有效地区分重叠因素，并利用一种由重要性驱动的优化策略来细化相关代码，高效地捕获关键相关特征并消除冗余。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的广泛实验表明，CAD-VAE可以产生更公平的表示、更具现实性的反事实场景以及改进了公平意识的图像编辑效果。&lt;h4&gt;结论&lt;/h4&gt;通过引入相关的隐变量来解决深度生成模型中的偏见和公平性问题的方法是有效的，并且该方法在多个方面展示了其优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While deep generative models have significantly advanced representationlearning, they may inherit or amplify biases and fairness issues by encodingsensitive attributes alongside predictive features. Enforcing strictindependence in disentanglement is often unrealistic when target and sensitivefactors are naturally correlated. To address this challenge, we propose CAD-VAE(Correlation-Aware Disentangled VAE), which introduces a correlated latent codeto capture the shared information between target and sensitive attributes.Given this correlated latent, our method effectively separates overlappingfactors without extra domain knowledge by directly minimizing the conditionalmutual information between target and sensitive codes. A relevance-drivenoptimization strategy refines the correlated code by efficiently capturingessential correlated features and eliminating redundancy. Extensive experimentson benchmark datasets demonstrate that CAD-VAE produces fairer representations,realistic counterfactuals, and improved fairness-aware image editing.</description>
      <author>example@mail.com (Chenrui Ma, Rongchang Zhao, Xi Xiao, Hongyang Xie, Tianyang Wang, Xiao Wang, Hao Zhang, Yanning Shen)</author>
      <guid isPermaLink="false">2503.07938v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Talk2PC: Enhancing 3D Visual Grounding through LiDAR and Radar Point Clouds Fusion for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2503.08336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的户外3D视觉定位模型TPCNet，该模型首次结合了激光雷达和毫米波雷达的数据，并利用提示引导的点云传感器融合方法进行自然语言查询。通过设计两种创新模块（双向代理交叉注意BACA和动态门控图融合DGGF）以及基于最近对象边缘的C3D-RECHead，TPCNet在Talk2Radar和Talk2Car数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的三维理解主要依赖于二维视觉语言模型，而这些模型收集和处理的是有限的场景感知上下文。相比之下，激光雷达等点云传感器提供了丰富的深度信息和精细的3D对象表示，毫米波雷达能够检测每个物体的速度、运动趋势及反射强度。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法TPCNet，在提示引导下的点云传感器融合框架中进行户外3D视觉定位。&lt;h4&gt;方法&lt;/h4&gt;设计了一个双阶段异构模态自适应融合范式，包括双向代理交叉注意（BACA）和动态门控图融合（DGGF），以及基于最近对象边缘的C3D-RECHead模块。这些设计旨在平衡激光雷达和毫米波雷达特征，并提高自然语言查询下的准确度。&lt;h4&gt;主要发现&lt;/h4&gt;TPCNet及其各个模块在Talk2Radar和Talk2Car数据集上的实验显示了最先进的性能，证明了方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够更灵活地利用自然语言进行查询，并实现更精确的3D视觉定位。未来的工作将探索进一步优化传感器融合技术以提高模型的整体性能。&lt;h4&gt;翻译&lt;/h4&gt;基于身临其境的户外场景理解是自主代理感知、分析和应对动态驾驶环境的基础。然而，现有的三维理解和二维视觉语言模型主要依赖于有限的场景感知上下文收集与处理。相比之下，激光雷达等点云传感器提供了丰富的深度信息及精细的3D对象表示，而毫米波雷达能够检测每个物体的速度、运动趋势及反射强度。因此，这些模态的结合为自然语言查询提供更灵活的条件，实现更准确的三维视觉定位。本文首次提出了一个基于提示引导下的点云传感器融合框架（包括激光雷达和毫米波雷达）的新方法TPCNet，并设计了一个双阶段异构模态自适应融合范式，包括双向代理交叉注意（BACA）模块、动态门控图融合（DGGF）模块以及基于最近对象边缘的C3D-RECHead模块。实验结果表明，TPCNet在Talk2Radar和Talk2Car数据集上取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied outdoor scene understanding forms the foundation for autonomousagents to perceive, analyze, and react to dynamic driving environments.However, existing 3D understanding is predominantly based on 2D Vision-LanguageModels (VLMs), collecting and processing limited scene-aware contexts. Instead,compared to the 2D planar visual information, point cloud sensors like LiDARoffer rich depth information and fine-grained 3D representations of objects.Meanwhile, the emerging 4D millimeter-wave (mmWave) radar is capable ofdetecting the motion trend, velocity, and reflection intensity of each object.Therefore, the integration of these two modalities provides more flexiblequerying conditions for natural language, enabling more accurate 3D visualgrounding. To this end, in this paper, we exploratively propose a novel methodcalled TPCNet, the first outdoor 3D visual grounding model upon the paradigm ofprompt-guided point cloud sensor combination, including both LiDAR and radarcontexts. To adaptively balance the features of these two sensors required bythe prompt, we have designed a multi-fusion paradigm called Two-StageHeterogeneous Modal Adaptive Fusion. Specifically, this paradigm initiallyemploys Bidirectional Agent Cross-Attention (BACA), which feeds dual-sensorfeatures, characterized by global receptive fields, to the text features forquerying. Additionally, we have designed a Dynamic Gated Graph Fusion (DGGF)module to locate the regions of interest identified by the queries. To furtherenhance accuracy, we innovatively devise an C3D-RECHead, based on the nearestobject edge. Our experiments have demonstrated that our TPCNet, along with itsindividual modules, achieves the state-of-the-art performance on both theTalk2Radar and Talk2Car datasets.</description>
      <author>example@mail.com (Runwei Guan, Jianan Liu, Ningwei Ouyang, Daizong Liu, Xiaolou Sun, Lianqing Zheng, Ming Xu, Yutao Yue, Hui Xiong)</author>
      <guid isPermaLink="false">2503.08336v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Can Generative Geospatial Diffusion Models Excel as Discriminative Geospatial Foundation Models?</title>
      <link>http://arxiv.org/abs/2503.07890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SatDiFuser的框架，该框架将基于扩散模型的生成式地理空间基础模型转换为具有强大判别能力的预训练工具。&lt;h4&gt;背景&lt;/h4&gt;自我监督学习在遥感领域取得了革命性的进展，推动了地理空间基础模型（GFMs）的发展。当前的GFMs主要集中在对比学习或掩码图像建模等鉴别性目标上，而生成扩散模型尽管显示出了捕捉多粒度语义的能力，在用于判别任务方面却尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;探讨生成扩散模型是否也能在遥感领域展现强大的判别能力并成为地理空间基础模型。&lt;h4&gt;方法&lt;/h4&gt;SatDiFuser通过系统分析多阶段噪声依赖的扩散特征，开发了三种融合策略来有效利用这些多样化的表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与现有的最先进GFMs相比，SatDiFuser在遥感基准测试中表现更优，在语义分割任务中的mIoU值提高了最多5.7%，分类任务中的F1得分提高最多7.9%。&lt;h4&gt;结论&lt;/h4&gt;基于扩散模型的生成式地理空间基础模型具有与判别性GFMs相匹敌甚至超越其的能力。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习在遥感领域的表示学习中取得了革命性的进展，推动了地理空间基础模型（GFMs）的发展，使其能够利用大量的未标记卫星图像来解决多种下游任务。目前，GFMs主要关注于对比学习或掩码图像建模等鉴别性目标，因为这些方法已被证明可以学习到可转移的表示。然而，生成扩散模型--显示出在图像生成过程中捕捉对于遥感任务至关重要的多粒度语义的能力--仍然未被充分用于判别性应用中。这引发了一个问题：基于扩散的生成式模型能否同样表现出强大的鉴别能力并作为GFMs使用？本文通过SatDiFuser这一框架回答了这个问题，该框架将基于扩散的生成式地理空间基础模型转变为一个强有力的预训练工具以服务遥感领域中的判别性任务。通过对多阶段、噪声依赖的扩散特征进行系统分析，我们开发出三种融合策略来有效利用这些多样化的表示。在远程传感基准上的广泛实验表明，SatDiFuser超过了现有的最先进GFMs，在语义分割中取得了高达5.7% mIoU的改进，在分类任务中的F1得分提高了最多7.9%，展示了基于扩散模型的生成式基础模型在与判别性GFMs竞争甚至超越其方面的潜力。代码将开源发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has revolutionized representation learning inRemote Sensing (RS), advancing Geospatial Foundation Models (GFMs) to leveragevast unlabeled satellite imagery for diverse downstream tasks. Currently, GFMsprimarily focus on discriminative objectives, such as contrastive learning ormasked image modeling, owing to their proven success in learning transferablerepresentations. However, generative diffusion models--which demonstrate thepotential to capture multi-grained semantics essential for RS tasks duringimage generation--remain underexplored for discriminative applications. Thisprompts the question: can generative diffusion models also excel and serve asGFMs with sufficient discriminative power? In this work, we answer thisquestion with SatDiFuser, a framework that transforms a diffusion-basedgenerative geospatial foundation model into a powerful pretraining tool fordiscriminative RS. By systematically analyzing multi-stage, noise-dependentdiffusion features, we develop three fusion strategies to effectively leveragethese diverse representations. Extensive experiments on remote sensingbenchmarks show that SatDiFuser outperforms state-of-the-art GFMs, achievinggains of up to +5.7% mIoU in semantic segmentation and +7.9% F1-score inclassification, demonstrating the capacity of diffusion-based generativefoundation models to rival or exceed discriminative GFMs. Code will bereleased.</description>
      <author>example@mail.com (Yuru Jia, Valerio Marsocci, Ziyang Gong, Xue Yang, Maarten Vergauwen, Andrea Nascetti)</author>
      <guid isPermaLink="false">2503.07890v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Fair Text Classification via Transferable Representations</title>
      <link>http://arxiv.org/abs/2503.07691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2311.12689&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于Wasserstein Dependency Measure的方法，用于学习无偏的神经文本分类器。&lt;h4&gt;背景&lt;/h4&gt;在文本分类中实现敏感群体之间的公平待遇是一个尚未解决的问题。当前挑战在于如何区分文本编码中的公正信息与不公信息。&lt;h4&gt;目的&lt;/h4&gt;通过引入对抗训练的思想，使目标标签表示和敏感属性表示相互独立来学习无偏的神经网络模型。&lt;h4&gt;方法&lt;/h4&gt;利用Wasserstein Dependency Measure扩展其应用范围，并展示了领域适应技术可以有效地去除对数据集中敏感属性访问的需求。&lt;h4&gt;主要发现&lt;/h4&gt;论文提供了理论和实证证据证明该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为实现文本分类中的群体公平提供了一种新的视角，能够有效学习无偏的神经网络模型。&lt;h4&gt;翻译&lt;/h4&gt;群体公平是文本分类领域的一个核心研究课题，在此背景下，本文提出了基于Wasserstein Dependency Measure的学习无偏神经文本分类器的新方法。通过借鉴对抗训练的思想来诱导目标标签表示和敏感属性表示之间的独立性，并展示了域适应技术可以有效地去除对数据集中敏感属性访问的需求。论文提供了理论与实证证据证明所提出的方法的有效性和可靠性，为解决群体公平问题提供了一种新的视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Group fairness is a central research topic in text classification, wherereaching fair treatment between sensitive groups (e.g., women and men) remainsan open challenge. We propose an approach that extends the use of theWasserstein Dependency Measure for learning unbiased neural text classifiers.Given the challenge of distinguishing fair from unfair information in a textencoder, we draw inspiration from adversarial training by inducing independencebetween representations learned for the target label and those for a sensitiveattribute. We further show that Domain Adaptation can be efficiently leveragedto remove the need for access to the sensitive attributes in the dataset wecure. We provide both theoretical and empirical evidence that our approach iswell-founded.</description>
      <author>example@mail.com (Thibaud Leteno, Michael Perrot, Charlotte Laclau, Antoine Gourru, Christophe Gravier)</author>
      <guid isPermaLink="false">2503.07691v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>TT-GaussOcc: Test-Time Compute for Self-Supervised Occupancy Prediction via Spatio-Temporal Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2503.08485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种新颖的自监督3D占用预测框架TT-GaussOcc，能够基于实时传感器数据优化时间感知的三维高斯分布来生成占用预测。&lt;h4&gt;背景&lt;/h4&gt;自监督3D占用预测在复杂驾驶场景理解中具有巨大潜力，但训练密集型体素解码器需要大量的GPU时间和难以适应不同的体素分辨率或新类别的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种灵活且实用的测试时间占用预测框架TT-GaussOcc，旨在解决现有方法在处理不同体素分辨率和新类别时的问题。&lt;h4&gt;方法&lt;/h4&gt;该框架采用'提升-移动-体素化'策略：首先利用2D视觉基础模型（VLM）从周围视角获取语义信息以实例化非空3D空间中的高斯分布；其次，通过估计的高斯场景流将动态高斯分布在帧间移动，并结合静态高斯分布以维持时间一致性；最后，在优化过程中使用三重径向基函数核平滑相邻的高斯分布，减少语义预测和场景流矢量中的噪声。&lt;h4&gt;主要发现&lt;/h4&gt;在Occ3D和nuCraft数据集上的实验表明，TT-GaussOcc框架在未进行任何离线训练的情况下，比自监督基准模型高出46%的mIoU，在支持更精细体素分辨率的同时保持了每秒2.6帧的推理速度。&lt;h4&gt;结论&lt;/h4&gt;提出的TT-GaussOcc框架提供了一种高效且适应性强的方法来解决3D占用预测中的挑战问题，特别是在复杂驾驶场景的理解中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;自监督三维占据预测为理解复杂的驾驶场景提供了有前景的解决方案，然而训练密集型体素解码器以捕捉细微几何结构和语义信息需要耗费大量的GPU时间，并且这些模型通常难以适应变化的体素分辨率或新类别而无需广泛的重新训练。为了克服这些问题，我们提出了一种名为TT-GaussOcc的实用灵活的测试时占据预测框架。该方法通过实时从原始传感器流实例化的时间感知3D高斯分布来增量优化，在任意用户指定的分辨率下支持体素化。具体来说，TT-GaussOcc操作遵循“提升-移动-体素”三部曲：首先，“提升”周围视角的语义信息，由2D视觉基础模型（VLM）获取，以在非空3D空间中实例化高斯分布；接着，“移动”先前帧中的动态高斯分布沿估计的高斯场景流进行更新，完成对象外观并消除快速移动物体后的痕迹，并积累静态高斯分布以确保时间一致性；最后，在优化过程中使用提出的三重RBF核平滑相邻高斯分布，减少语义预测和场景流向量中的固有噪声。历史静态和当前动态的高斯分布在结合后进行体素化，从而生成占用预测。在Occ3D和nuCraft数据集上进行的不同体素分辨率上的大量实验表明，在没有任何离线训练的情况下，TT-GaussOcc超越自监督基线46%的mIoU，并支持每秒2.6帧推理速度下的更精细体素分辨率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised 3D occupancy prediction offers a promising solution forunderstanding complex driving scenes without requiring costly 3D annotations.However, training dense voxel decoders to capture fine-grained geometry andsemantics can demand hundreds of GPU hours, and such models often fail to adaptto varying voxel resolutions or new classes without extensive retraining. Toovercome these limitations, we propose a practical and flexible test-timeoccupancy prediction framework termed TT-GaussOcc. Our approach incrementallyoptimizes time-aware 3D Gaussians instantiated from raw sensor streams atruntime, enabling voxelization at arbitrary user-specified resolution.Specifically, TT-GaussOcc operates in a "lift-move-voxel" symphony: we first"lift" surrounding-view semantics obtained from 2D vision foundation models(VLMs) to instantiate Gaussians at non-empty 3D space; Next, we "move" dynamicGaussians from previous frames along estimated Gaussian scene flow to completeappearance and eliminate trailing artifacts of fast-moving objects, whileaccumulating static Gaussians to enforce temporal consistency; Finally, wemitigate inherent noises in semantic predictions and scene flow vectors byperiodically smoothing neighboring Gaussians during optimization, usingproposed trilateral RBF kernels that jointly consider color, semantic, andspatial affinities. The historical static and current dynamic Gaussians arethen combined and voxelized to generate occupancy prediction. Extensiveexperiments on Occ3D and nuCraft with varying voxel resolutions demonstratethat TT-GaussOcc surpasses self-supervised baselines by 46% on mIoU without anyoffline training, and supports finer voxel resolutions at 2.6 FPS inferencespeed.</description>
      <author>example@mail.com (Fengyi Zhang, Huitong Yang, Zheng Zhang, Zi Huang, Yadan Luo)</author>
      <guid isPermaLink="false">2503.08485v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability</title>
      <link>http://arxiv.org/abs/2503.08481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种物理可达性表示S-P Map和融合该信息的视觉语言模型PhysVLM，用于解决机器人在执行任务时环境感知与物理可达性理解之间的不匹配问题。&lt;h4&gt;背景&lt;/h4&gt;最先进的视觉-语言模型（VLMs）虽然在环境感知方面表现出色，但在涉及机器人的实体视觉推理任务中往往产生不准确或不实用的回答，因为它们缺乏对机器人物理可达性的了解。&lt;h4&gt;目的&lt;/h4&gt;为了改善这种状况，作者旨在提出一种统一的物理可达性表示方法和相应的模型来提升机器人执行任务时的理解与准确性。&lt;h4&gt;方法&lt;/h4&gt;{'S-P Map': '抽象出通用的空间表示形式，独立于特定机器人的配置，使模型能够专注于物理可达性特征而非具体参数。', 'PhysVLM': '通过加入专门处理S-P Map的特征编码器来扩展传统VLM架构，在不牺牲其视觉语言能力的情况下增加对物理可达性的推理功能。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能提升': '实验结果显示，PhysVLM在多项基准测试中优于现有模型，包括EQA-phys、RoboVQA-val和OpenEQA。', '数据集构建': '为了训练和评估PhysVLM，作者创建了大型多机器人数据集Phys100K以及挑战性基准EQA-phys。'}&lt;h4&gt;结论&lt;/h4&gt;提出的S-P Map具有良好的兼容性，并且将其整合到GPT-4o-mini中可提高7.1%的性能。&lt;h4&gt;翻译&lt;/h4&gt;理解环境和机器人的物理可达性对于任务执行至关重要。尽管最先进的视觉语言模型在环境感知方面表现出色，但在涉及机器人实体视觉推理的任务中往往产生不准确或不切实际的回答，因为它们缺乏对机器人物理可达性的理解。为了解决这个问题，我们提出了一种跨越各种类型的统一的物理可达性表示S-P Map，以及一种新的视觉语言模型PhysVLM，它将这些可达性信息集成到视觉推理中。具体而言，S-P Map将机器人的物理可达性抽象成一种与特定机器人配置无关的一般化空间表示形式，使模型能够专注于可达性的特征而非具体的参数设置。随后，PhysVLM通过引入一个专门处理S-P Map的额外特征编码器来扩展传统的VLM架构，在不牺牲其一般视觉语言能力的前提下实现对物理可达性的推理。为了训练和评估PhysVLM，我们构建了一个大规模的多机器人数据集Phys100K以及挑战性基准EQA-phys，该基准包括在六个不同的机器人、模拟和现实环境中完成的任务。实验结果表明，在EQA-phys测试中，与GPT-4o相比，PhysVLM提高了14%的表现，并且在RoboVQA-val和OpenEQA基准上超过先进的实体视觉语言模型如RoboMamba和SpatialVLM。此外，S-P Map展示出对各种VLM的强兼容性，将其整合到GPT-4o-mini中可以获得7.1%的表现提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/unira-zwj/PhysVLM&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the environment and a robot's physical reachability is crucialfor task execution. While state-of-the-art vision-language models (VLMs) excelin environmental perception, they often generate inaccurate or impracticalresponses in embodied visual reasoning tasks due to a lack of understanding ofrobotic physical reachability. To address this issue, we propose a unifiedrepresentation of physical reachability across diverse robots, i.e.,Space-Physical Reachability Map (S-P Map), and PhysVLM, a vision-language modelthat integrates this reachability information into visual reasoning.Specifically, the S-P Map abstracts a robot's physical reachability into ageneralized spatial representation, independent of specific robotconfigurations, allowing the model to focus on reachability features ratherthan robot-specific parameters. Subsequently, PhysVLM extends traditional VLMarchitectures by incorporating an additional feature encoder to process the S-PMap, enabling the model to reason about physical reachability withoutcompromising its general vision-language capabilities. To train and evaluatePhysVLM, we constructed a large-scale multi-robot dataset, Phys100K, and achallenging benchmark, EQA-phys, which includes tasks for six different robotsin both simulated and real-world environments. Experimental results demonstratethat PhysVLM outperforms existing models, achieving a 14\% improvement overGPT-4o on EQA-phys and surpassing advanced embodied VLMs such as RoboMamba andSpatialVLM on the RoboVQA-val and OpenEQA benchmarks. Additionally, the S-P Mapshows strong compatibility with various VLMs, and its integration intoGPT-4o-mini yields a 7.1\% performance improvement.</description>
      <author>example@mail.com (Weijie Zhou, Manli Tao, Chaoyang Zhao, Haiyun Guo, Honghui Dong, Ming Tang, Jinqiao Wang)</author>
      <guid isPermaLink="false">2503.08481v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>LLM-Pack: Intuitive Grocery Handling for Logistics Applications</title>
      <link>http://arxiv.org/abs/2503.08445v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 Pages, 6 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了LLM-Pack，这是一种用于解决超市自动打包问题的新颖方法。&lt;h4&gt;背景&lt;/h4&gt;虽然机器人技术在物流中的影响越来越大，但在杂货零售业中，顾客仍然需要手动挑选和包装商品。尽管有许多研究集中在机器人从箱子中取物品的问题上，但是有关如何有序地放置和包装物品的研究却较少。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的方法LLM-Pack，该方法使用语言和视觉基础模型来识别杂货并生成与人类策略相匹配的打包顺序。&lt;h4&gt;方法&lt;/h4&gt;LLM-Pack利用了现有的语言和视觉模型，无需为新商品进行特定训练，并且由于其模块化设计可以轻松升级底层模型。&lt;h4&gt;主要发现&lt;/h4&gt;该研究展示了LLM-Pack在实际场景中的性能表现。&lt;h4&gt;结论&lt;/h4&gt;论文作者计划公开发布LLM-Pack的源代码，以供他人进一步研究和使用。&lt;h4&gt;翻译&lt;/h4&gt;机器人技术和自动化对物流行业影响日益增大，但仍主要局限于传统仓库中。尽管自助结账超市等技术取得了进展，但顾客仍然需要手动挑选和包装商品。目前大部分机器人技术集中在从箱子中取出物品的问题上，但对于有序地放置和打包杂货的研究却很少。然而，正确地按照顺序来包装物品对于防止产品损坏至关重要（例如，不应将重物放在轻的或易碎物体之上）。但是鉴于商店内物品的巨大多样性，制定正确的包装顺序准则非常困难。在本文中，我们介绍了LLM-Pack，这是一种利用语言和视觉基础模型识别杂货并生成模仿人类策略打包顺序的新方法。LLM-Pack无需专门训练即可处理新商品，并且由于其模块化设计可以轻松升级底层模型。我们将广泛评估该方法以证明其性能表现。论文发表后，将公开发布LLMPack的源代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotics and automation are increasingly influential in logistics but remainlargely confined to traditional warehouses. In grocery retail, advancementssuch as cashier-less supermarkets exist, yet customers still manually pick andpack groceries. While there has been a substantial focus in robotics on the binpicking problem, the task of packing objects and groceries has remained largelyuntouched. However, packing grocery items in the right order is crucial forpreventing product damage, e.g., heavy objects should not be placed on top offragile ones. However, the exact criteria for the right packing order are hardto define, in particular given the huge variety of objects typically found instores. In this paper, we introduce LLM-Pack, a novel approach for grocerypacking. LLM-Pack leverages language and vision foundation models foridentifying groceries and generating a packing sequence that mimics humanpacking strategy. LLM-Pack does not require dedicated training to handle newgrocery items and its modularity allows easy upgrades of the underlyingfoundation models. We extensively evaluate our approach to demonstrate itsperformance. We will make the source code of LLMPack publicly available uponthe publication of this manuscript.</description>
      <author>example@mail.com (Yannik Blei, Michael Krawez, Tobias Jülg, Pierre Krack, Florian Walter, Wolfram Burgard)</author>
      <guid isPermaLink="false">2503.08445v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Trajectory Generation Based on Traversable Planes in 3D Complex Architectural Spaces</title>
      <link>http://arxiv.org/abs/2503.08076v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 8 figures, 2tables. The paper has been accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种新的高效规划器，用于地面机器人在大型复杂多层建筑空间中的自主导航。&lt;h4&gt;背景&lt;/h4&gt;随着机器人越来越多地融入人类生活，在人们主要活动区域的建筑设计中机器人的角色变得更加重要。尽管自动化机器人的运动能力和精确定位已经迅速发展，但在这些区域内生成有效、平滑、全面和高质量的轨迹仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的高效规划器，使地面机器人能够在大型复杂多层建筑空间中自主导航。&lt;h4&gt;方法&lt;/h4&gt;通过分割、合并、分类和平面连接从3D点云中提取可行走平面，并构建轻量级但完整表示可行区域的平面图。然后基于运动状态轨迹建立一条考虑穿越多层次结构时特殊约束条件的轨迹优化路径，以最大限度地提高机器人的机动性。&lt;h4&gt;主要发现&lt;/h4&gt;通过在模拟环境和现实场景中的CubeTrack机器人上进行实验验证了该方法的有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;所提出的规划器能够在大型复杂多层建筑环境中为地面机器人提供高效、平滑且实用的导航解决方案，同时确保机器人的机动性最大化。&lt;h4&gt;翻译&lt;/h4&gt;随着机器人越来越多地融入人类生活，在人们主要活动区域的建筑设计中机器人的角色变得更加重要。尽管自动化机器人的运动能力和精确定位已经迅速发展，但在这些区域内生成有效、平滑、全面和高质量的轨迹仍然是一个挑战。在本文中，我们提出了一种新的高效规划器，用于地面机器人在大型复杂多层建筑空间中的自主导航。考虑到可行走区域通常包括地面、斜坡和楼梯等平面或近似平面结构，我们将问题简化为在复杂的交叉平面上进行导航。首先通过分割、合并、分类和平面连接从3D点云中提取可行走平面，并构建轻量级但完整表示可行区域的平面图。然后基于运动状态轨迹建立一条考虑穿越多层次结构时特殊约束条件的轨迹优化路径，以最大限度地提高机器人的机动性。我们在模拟环境和现实场景中的CubeTrack机器人上进行了实验验证了该方法的有效性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the increasing integration of robots into human life, their role inarchitectural spaces where people spend most of their time has become moreprominent. While motion capabilities and accurate localization for automatedrobots have rapidly developed, the challenge remains to generate efficient,smooth, comprehensive, and high-quality trajectories in these areas. In thispaper, we propose a novel efficient planner for ground robots to autonomouslynavigate in large complex multi-layered architectural spaces. Considering thattraversable regions typically include ground, slopes, and stairs, which areplanar or nearly planar structures, we simplify the problem to navigationwithin and between complex intersecting planes. We first extract traversableplanes from 3D point clouds through segmenting, merging, classifying, andconnecting to build a plane-graph, which is lightweight but fully representsthe traversable regions. We then build a trajectory optimization based onmotion state trajectory and fully consider special constraints when crossingmulti-layer planes to maximize the robot's maneuverability. We conductexperiments in simulated environments and test on a CubeTrack robot inreal-world scenarios, validating the method's effectiveness and practicality.</description>
      <author>example@mail.com (Mengke Zhang, Zhihao Tian, Yaoguang Xia, Chao Xu, Fei Gao, Yanjun Cao)</author>
      <guid isPermaLink="false">2503.08076v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>nnInteractive: Redefining 3D Promptable Segmentation</title>
      <link>http://arxiv.org/abs/2503.08373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Fabian Isensee, Maximilian Rokuss and Lars Kr\"amer contributed  equally. Each co-first author may list themselves as lead author on their CV&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;nnInteractive是首个全面的3D交互式开放集分割方法，支持多样化的提示输入，并在120多个不同体积数据集中训练，提高了准确性、适应性和易用性。&lt;h4&gt;背景&lt;/h4&gt;现有的基础模型如SAM革新了2D互动分割，但它们不适合用于3D医学图像，因为存在领域转移限制和缺乏体素感知等问题。当前的适配方法仍然存在一些局限性。&lt;h4&gt;目的&lt;/h4&gt;提出nnInteractive以解决现有工具在医疗影像中的使用难题，并提高其准确性和易用性。&lt;h4&gt;方法&lt;/h4&gt;nnInteractive支持多样化的提示输入（包括点、涂鸦、框和创新的Lasso提示），利用直观的2D互动生成完整的3D分割。该方法基于120多个多样的体积数据集进行训练，涵盖CT、MRI等多种模式。&lt;h4&gt;主要发现&lt;/h4&gt;nnInteractive在准确性、适应性和易用性方面都处于领先地位，并且是首个集成到广泛使用的图像查看器中的方法，如Napari和MITK，确保了实际应用的可访问性。基准测试表明它远超现有方法。&lt;h4&gt;结论&lt;/h4&gt;nnInteractive确立了一个新的AI驱动的互动3D分割标准，其在广泛的医学影像平台中实现了高度的可用性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;准确且高效的三维分割对临床和研究应用至关重要。虽然像SAM这样的基础模型革新了交互式分割，但它们的设计和领域转移限制使它们不适合处理3D医学图像。当前的方法解决了部分挑战，但仍存在局限性，如缺乏体素感知、互动受限或仅支持少量结构和模式。同时，可用性也存在问题，现有工具很少集成到现有的影像平台中，并且通常依赖于功能有限的网页界面。我们介绍了nnInteractive，这是首个全面的3D交互式开放集分割方法。它支持包括点、涂鸦、框以及一种创新的Lasso提示在内的多样化提示输入，并利用直观的2D互动生成完整的3D分割。训练基于120多个多样的体积数据集（如CT、MRI、PET等），nnInteractive在准确性、适应性和易用性方面都处于领先地位。关键的是，它首次集成到广泛使用的图像查看器中（例如Napari和MITK），确保了临床和研究应用中的广泛可访问性。广泛的基准测试表明，nnInteractive远超现有方法，确立了一个新的标准。nnInteractive的开源代码可以在以下地址获取：https://github.com/MIC-DKFZ/napari-nninteractive (Napari插件)、https://www.mitk.org/MITK-nnInteractive (MITK集成) 和 https://github.com/MIC-DKFZ/nnInteractive (Python后端)&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and efficient 3D segmentation is essential for both clinical andresearch applications. While foundation models like SAM have revolutionizedinteractive segmentation, their 2D design and domain shift limitations makethem ill-suited for 3D medical images. Current adaptations address some ofthese challenges but remain limited, either lacking volumetric awareness,offering restricted interactivity, or supporting only a small set of structuresand modalities. Usability also remains a challenge, as current tools are rarelyintegrated into established imaging platforms and often rely on cumbersomeweb-based interfaces with restricted functionality. We introduce nnInteractive,the first comprehensive 3D interactive open-set segmentation method. Itsupports diverse prompts-including points, scribbles, boxes, and a novel lassoprompt-while leveraging intuitive 2D interactions to generate full 3Dsegmentations. Trained on 120+ diverse volumetric 3D datasets (CT, MRI, PET, 3DMicroscopy, etc.), nnInteractive sets a new state-of-the-art in accuracy,adaptability, and usability. Crucially, it is the first method integrated intowidely used image viewers (e.g., Napari, MITK), ensuring broad accessibilityfor real-world clinical and research applications. Extensive benchmarkingdemonstrates that nnInteractive far surpasses existing methods, setting a newstandard for AI-driven interactive 3D segmentation. nnInteractive is publiclyavailable: https://github.com/MIC-DKFZ/napari-nninteractive (Napari plugin),https://www.mitk.org/MITK-nnInteractive (MITK integration),https://github.com/MIC-DKFZ/nnInteractive (Python backend).</description>
      <author>example@mail.com (Fabian Isensee, Maximilian Rokuss, Lars Krämer, Stefan Dinkelacker, Ashis Ravindran, Florian Stritzke, Benjamin Hamm, Tassilo Wald, Moritz Langenberg, Constantin Ulrich, Jonathan Deissler, Ralf Floca, Klaus Maier-Hein)</author>
      <guid isPermaLink="false">2503.08373v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Simulating Automotive Radar with Lidar and Camera Inputs</title>
      <link>http://arxiv.org/abs/2503.08068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的方法，能够通过相机图像、激光雷达点云和车辆速度信息来模拟高质量的4D毫米波雷达信号。&lt;h4&gt;背景&lt;/h4&gt;低成本的汽车毫米波雷达因其在恶劣天气和光照条件下自主驾驶中的表现而受到越来越多的关注。然而，缺乏高质量的数据集阻碍了研究和发展。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法用于生成高质量的雷达数据集，以促进基于雷达的研究和开发。&lt;h4&gt;方法&lt;/h4&gt;{'DIS-Net': '这是一种新型神经网络，能够估计雷达信号的空间分布及数量。', 'RSS-Net': '这是一种预测信号接收强度（RSS）的新神经网络，根据外观和几何信息进行预测。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法可以成功生成高保真的毫米波雷达信号，并且基于合成雷达数据增强的数据集训练的对象检测神经网络性能优于仅使用原始雷达数据训练的模型。&lt;h4&gt;结论&lt;/h4&gt;这种方法为未来基于雷达的研究和开发提供了强有力的工具和支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Low-cost millimeter automotive radar has received more and more attention dueto its ability to handle adverse weather and lighting conditions in autonomousdriving. However, the lack of quality datasets hinders research anddevelopment. We report a new method that is able to simulate 4D millimeter waveradar signals including pitch, yaw, range, and Doppler velocity along withradar signal strength (RSS) using camera image, light detection and ranging(lidar) point cloud, and ego-velocity. The method is based on two new neuralnetworks: 1) DIS-Net, which estimates the spatial distribution and number ofradar signals, and 2) RSS-Net, which predicts the RSS of the signal based onappearance and geometric information. We have implemented and tested our methodusing open datasets from 3 different models of commercial automotive radar. Theexperimental results show that our method can successfully generatehigh-fidelity radar signals. Moreover, we have trained a popular objectdetection neural network with data augmented by our synthesized radar. Thenetwork outperforms the counterpart trained only on raw radar data, a promisingresult to facilitate future radar-based research and development.</description>
      <author>example@mail.com (Peili Song, Dezhen Song, Yifan Yang, Enfan Lan, Jingtai Liu)</author>
      <guid isPermaLink="false">2503.08068v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>ADROIT: A Self-Supervised Framework for Learning Robust Representations for Active Learning</title>
      <link>http://arxiv.org/abs/2503.07506v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的统一表示学习框架ADROIT，用于主动学习任务，通过集成重构、对抗性、自监督、知识蒸馏和分类损失到一个基于VAE的方法中。&lt;h4&gt;背景&lt;/h4&gt;主动学习的目的是选择最佳样本进行标注，以最小化标注成本。然而，现有的方法难以有效整合多种损失函数，并且在使用无标签数据时未能充分利用任务信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的统一表示生成器（VAE），一个状态区分器以及代理分类器或任务学习器的框架，该框架可以同时利用标记和未标记的数据来提高主动学习的有效性。&lt;h4&gt;方法&lt;/h4&gt;- 统一表示生成器：使用带标签和无标签数据学习潜在代码- 状态区分器：区分有标签和无标签数据，选择具有信息量的无标签样本- 代理分类器或任务学习器：利用标记数据中的任务意识，并通过自监督损失应用于未标记数据以及知识蒸馏技术以与目标任务学习器对齐&lt;h4&gt;主要发现&lt;/h4&gt;- 集成多种类型的损失函数到统一框架中，能够更好地处理多样化和复杂的数据- 动态的VAE与状态区分器之间的相互作用形成竞争环境，提高了模型的学习效率&lt;h4&gt;结论&lt;/h4&gt;实验表明提出的ADROIT方法在多样化的数据集上表现出了优越的效果，并且通过对不同组件的消融分析确认了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Active learning aims to select optimal samples for labeling, minimizingannotation costs. This paper introduces a unified representation learningframework tailored for active learning with task awareness. It integratesdiverse sources, comprising reconstruction, adversarial, self-supervised,knowledge-distillation, and classification losses into a unified VAE-basedADROIT approach. The proposed approach comprises three key components - aunified representation generator (VAE), a state discriminator, and a (proxy)task-learner or classifier. ADROIT learns a latent code using both labeled andunlabeled data, incorporating task-awareness by leveraging labeled data withthe proxy classifier. Unlike previous approaches, the proxy classifieradditionally employs a self-supervised loss on unlabeled data and utilizesknowledge distillation to align with the target task-learner. The statediscriminator distinguishes between labeled and unlabeled data, facilitatingthe selection of informative unlabeled samples. The dynamic interaction betweenVAE and the state discriminator creates a competitive environment, with the VAEattempting to deceive the discriminator, while the state discriminator learnsto differentiate between labeled and unlabeled inputs. Extensive evaluations ondiverse datasets and ablation analysis affirm the effectiveness of the proposedmodel.</description>
      <author>example@mail.com (Soumya Banerjee, Vinay Kumar Verma)</author>
      <guid isPermaLink="false">2503.07506v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Point Cloud Completion: Are We Ready For The Real-World?</title>
      <link>http://arxiv.org/abs/2411.17580v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;点云在真实世界环境中的获取面临噪声、不完整性和非均匀稀疏性的问题，这对点云完成任务提出了挑战。为了改善这一状况，研究者贡献了一个名为RealPC的工业级数据集，并通过利用代数拓扑和持久同调理论来提高现有方法在现实场景中的表现。&lt;h4&gt;背景&lt;/h4&gt;采集自受限、困难和多传感器环境下的点云数据存在噪声、不完整性和非均匀稀疏性问题，这对完成点云的任务构成挑战。现有的基准对象点云缺少真实环境中捕获的丰富的拓扑特征。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的工业级数据集RealPC，并通过引入持久同调理论中的0维和1维拓扑特征来改进现有方法在现实场景下的性能。&lt;h4&gt;方法&lt;/h4&gt;利用代数拓扑学和持久同调（PH）工具分析基准对象点云的不足，贡献首个针对点云完成任务的真实工业级数据集RealPC，并提出了一种基于样本引导网络BOSHNet的方法，以避免昂贵的霍莫洛吉计算同时在训练过程中提供类似的0维PH特征。&lt;h4&gt;主要发现&lt;/h4&gt;现有的数据集中缺乏与现实环境中捕获到的点云相匹配的持久同调理论中的0维和1维拓扑特征。引入这些特征可以改善现有方法的表现。此外，通过使用BOSHNet方法可以在训练初期就获得类似持久同调计算的好处。&lt;h4&gt;结论&lt;/h4&gt;将持久同调理论融入现有的工作有助于改进点云完成任务，并提出了一个新数据集RealPC以及一种新的网络架构BOSHNet来支持这一研究方向。&lt;h4&gt;翻译&lt;/h4&gt;从受限、困难和多传感器的真实世界环境获取的点云噪声大，不完整且非均匀稀疏。这给重要的点云补全任务带来了严峻挑战。使用代数拓扑和持久同调方法显示，当前基准对象点云缺乏真实环境中捕获到的关键丰富拓扑特征。为了促进相关研究，贡献了首个用于点云完成的真实工业数据集RealPC - 一个多样、丰富的点云计算集合。该数据集中有约40,000对21个铁路设施中各种类型的结构的配对。在多个强大基线上进行基准测试的结果显示，在现实场景下现有方法表现不佳。发现了一个显著的现象：与当前的数据集不同，RealPC包括了多个基于持久同调理论的0维和1维拓扑特征。证明将这些拓扑先验集成到现有的工作中有助于改进完成任务的表现。展示了如何使用0维持久同调先验提取一个完整形状的全局拓扑形式为3D骨架，并辅助模型生成拓扑一致性的完整形状。由于霍莫洛吉计算成本高昂，提出了一种简单且有效的样本引导网络BOSHNet来跳过霍莫洛吉计算，通过采样类似0维PH的代理骨干来实现这一目标。这些骨干自训练初期就提供了与持久同调类似的效益，而其他方法仅在后期获得准确的骨架结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-11-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds acquired in constrained, challenging, uncontrolled, andmulti-sensor real-world settings are noisy, incomplete, and non-uniformlysparse. This presents acute challenges for the vital task of point cloudcompletion. Using tools from Algebraic Topology and Persistent Homology (PH),we demonstrate that current benchmark object point clouds lack rich topologicalfeatures that are integral part of point clouds captured in realisticenvironments. To facilitate research in this direction, we contribute the firstreal-world industrial dataset for point cloud completion, RealPC - a diverse,rich and varied set of point clouds. It consists of ~ 40,000 pairs across 21categories of industrial structures in railway establishments. Benchmarkresults on several strong baselines reveal that existing methods fail inreal-world scenarios. We discover a striking observation - unlike currentdatasets, RealPC consists of multiple 0- and 1-dimensional PH-based topologicalfeatures. We prove that integrating these topological priors into existingworks helps improve completion. We present how 0-dimensional PH priors extractthe global topology of a complete shape in the form of a 3D skeleton and assista model in generating topologically consistent complete shapes. Since computingHomology is expensive, we present a simple, yet effective Homology Samplerguided network, BOSHNet that bypasses the Homology computation by samplingproxy backbones akin to 0-dim PH. These backbones provide similar benefits of0-dim PH right from the start of the training, unlike similar methods whereaccurate backbones are obtained only during later phases of the training.</description>
      <author>example@mail.com (Stuti Pathak, Prashant Kumar, Dheeraj Baiju, Nicholus Mboga, Gunther Steenackers, Rudi Penne)</author>
      <guid isPermaLink="false">2411.17580v4</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>QLIO: Quantized LiDAR-Inertial Odometry</title>
      <link>http://arxiv.org/abs/2503.07949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;QLIO是一个多处理器分布式量化LiDAR惯性里程计框架，旨在减少计算负载和带宽消耗同时保持定位精度。&lt;h4&gt;背景&lt;/h4&gt;传统的LiDAR惯性里程计（LIO）框架由于处理密集点云的计算成本高，在体积、重量和功率受限平台上部署面临挑战。这些框架依赖于单个板载处理器导致计算瓶颈和高内存需求，使嵌入式系统上的实时执行变得困难。&lt;h4&gt;目的&lt;/h4&gt;提出QLIO以解决传统LiDAR惯性里程计在SWaP约束平台上的问题，并提供一个开放源代码实现以促进进一步研究和实际部署。&lt;h4&gt;方法&lt;/h4&gt;QLIO引入了一个量化状态估计管道，其中协处理器预处理激光雷达测量数据，在发送给主处理器之前压缩点到平面残差。此外，rQ向量为基础的自适应重采样策略智能选择并压缩关键观测值，进一步减少计算冗余。&lt;h4&gt;主要发现&lt;/h4&gt;QLIO在保持定位精度的同时减少了14.1%每观察残留数据。&lt;h4&gt;结论&lt;/h4&gt;这些结果确立了QLIO作为一个高效和可扩展解决方案的地位，在计算和带宽限制下工作的实时自主系统中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR惯性里程计（LIO）广泛用于自动驾驶导航，但由于处理密集点云的计算成本高，其在体积、重量和功率受限平台上的部署仍面临挑战。传统的LIO框架依赖于单个板载处理器导致了计算瓶颈和高内存需求，在嵌入式系统上进行实时执行变得困难。为解决此问题，我们提出了QLIO，这是一个多处理器分布式量化LIO框架，该框架减少了计算负荷和带宽消耗，并保持定位精度不变。QLIO引入了一个量化状态估计管道，其中协处理器预处理LiDAR测量值，在发送给主处理器之前压缩点到平面残差。此外，采用rQ向量为基础的自适应重采样策略智能选择并压缩关键观测值，进一步减少计算冗余。实际评估显示，QLIO在保留定位精度的同时减少了14.1%每观察残留数据。我们还发布了开源实现以促进进一步研究和实际部署。这些结果确立了QLIO作为在计算和带宽限制下工作的实时自主系统的高效可扩展解决方案的地位。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-Inertial Odometry (LIO) is widely used for autonomous navigation, butits deployment on Size, Weight, and Power (SWaP)-constrained platforms remainschallenging due to the computational cost of processing dense point clouds.Conventional LIO frameworks rely on a single onboard processor, leading tocomputational bottlenecks and high memory demands, making real-time executiondifficult on embedded systems. To address this, we propose QLIO, amulti-processor distributed quantized LIO framework that reduces computationalload and bandwidth consumption while maintaining localization accuracy. QLIOintroduces a quantized state estimation pipeline, where a co-processorpre-processes LiDAR measurements, compressing point-to-plane residuals beforetransmitting only essential features to the host processor. Additionally, anrQ-vector-based adaptive resampling strategy intelligently selects andcompresses key observations, further reducing computational redundancy.Real-world evaluations demonstrate that QLIO achieves a 14.1% reduction inper-observation residual data while preserving localization accuracy.Furthermore, we release an open-source implementation to facilitate furtherresearch and real-world deployment. These results establish QLIO as anefficient and scalable solution for real-time autonomous systems operatingunder computational and bandwidth constraints.</description>
      <author>example@mail.com (Boyang Lou, Shenghai Yuan, Jianfei Yang, Wenju Su, Yingjian Zhang, Enwen Hu)</author>
      <guid isPermaLink="false">2503.07949v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>AgriField3D: A Curated 3D Point Cloud and Procedural Model Dataset of Field-Grown Maize from a Diversity Panel</title>
      <link>http://arxiv.org/abs/2503.07813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Elvis Kimara and Mozhgan Hadadi contributed equally to this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了AgriField3D，这是一个针对玉米的高质量三维点云数据集，旨在通过AI技术促进农业研究。&lt;h4&gt;背景&lt;/h4&gt;目前在三维农业研究中，特别是对于玉米的研究，由于大规模多样化数据集的缺乏，人工智能的应用受到了限制。尽管二维图像数据量丰富，但它们无法捕捉到三维数据提供的如叶片结构、植株体积和空间排列等关键信息。&lt;h4&gt;目的&lt;/h4&gt;为了克服这一局限性，作者提出了一套专为AI准备的玉米田间生长点云数据集AgriField3D。&lt;h4&gt;方法&lt;/h4&gt;该数据集包括超过1,000个高质量的Terrestrial Laser Scanner收集的三维点云，并且利用NURBS生成了结构化、参数化的植物模型。通过分段优化和可微编程的过程，这些程序化模型能够精确地重建叶片表面和植物架构。&lt;h4&gt;主要发现&lt;/h4&gt;该数据集还采用了基于图的方法进行分割，以隔离单独的叶子和茎部，并进行了严格的质量控制来确保所有样本的一致性标签及元数据准确性。此外，提供了多个分辨率版本的数据以适应不同的计算需求。&lt;h4&gt;结论&lt;/h4&gt;通过将田间生长植物的点云数据与高保真的程序化模型相结合并进行仔细的手动验证，AgriField3D为基于AI的表型分析、植物结构分析以及农业研究中的三维应用提供了一个全面的基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The application of artificial intelligence (AI) in three-dimensional (3D)agricultural research, particularly for maize, has been limited by the scarcityof large-scale, diverse datasets. While 2D image datasets are abundant, theyfail to capture essential structural details such as leaf architecture, plantvolume, and spatial arrangements that 3D data provide. To address thislimitation, we present AgriField3D(https://baskargroup.github.io/AgriField3D/), a curated dataset of 3D pointclouds of field-grown maize plants from a diverse genetic panel, designed to beAI-ready for advancing agricultural research. Our dataset comprises over 1,000high-quality point clouds collected using a Terrestrial Laser Scanner,complemented by procedural models that provide structured, parametricrepresentations of maize plants. These procedural models, generated usingNon-Uniform Rational B-Splines (NURBS) and optimized via a two-step processcombining Particle Swarm Optimization (PSO) and differentiable programming,enable precise, scalable reconstructions of leaf surfaces and plantarchitectures. To enhance usability, we performed graph-based segmentation toisolate individual leaves and stalks, ensuring consistent labeling across allsamples. We also conducted rigorous manual quality control on all datasets,correcting errors in segmentation, ensuring accurate leaf ordering, andvalidating metadata annotations. The dataset further includes metadatadetailing plant morphology and quality, alongside multi-resolution subsampledversions (100k, 50k, 10k points) optimized for various computational needs. Byintegrating point cloud data of field grown plants with high-fidelityprocedural models and ensuring meticulous manual validation, AgriField3Dprovides a comprehensive foundation for AI-driven phenotyping, plant structuralanalysis, and 3D applications in agricultural research.</description>
      <author>example@mail.com (Elvis Kimara, Mozhgan Hadadi, Jackson Godbersen, Aditya Balu, Talukder Jubery, Yawei Li, Adarsh Krishnamurthy, Patrick S. Schnable, Baskar Ganapathysubramanian)</author>
      <guid isPermaLink="false">2503.07813v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>LangTime: A Language-Guided Unified Model for Time Series Forecasting with Proximal Policy Optimization</title>
      <link>http://arxiv.org/abs/2503.08271v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;近年来，利用预训练的大规模语言模型（LLMs）进行时间序列应用的研究兴趣日益增加。&lt;h4&gt;目的&lt;/h4&gt;提出LangTime方法来解决使用LLMs作为时间序列预测基础模型时遇到的三个主要挑战：跨领域泛化、模态对齐和自回归框架中的误差累积。&lt;h4&gt;方法&lt;/h4&gt;{'LangTime': '一种语言引导的一体化模型，用于时间序列预测。它结合了跨领域的预训练和基于强化学习的微调。构建Temporal Comprehension Prompts (TCPs)，包括数据集级别的指令和通道级别的指令，以促进领域适应并压缩时间序列到单个标记。', 'TimePPO': '一种基于强化学习的微调算法，通过利用针对时间序列定制的多维奖励函数以及重复基线值估计策略来减轻自回归预测中的误差累积。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'LangTime性能': '实验表明，LangTime在跨领域预测中实现了最先进的表现。', 'TimePPO效果': '基于TimePPO的微调显著提高了自回归预测的稳定性和准确性。'}&lt;h4&gt;结论&lt;/h4&gt;通过提出新的框架和算法来解决现有模型面临的问题，成功提升了时间序列预测的效果。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种名为LangTime的方法及其强化学习微调算法TimePPO，旨在利用预训练语言模型进行时间序列预测，并解决了相关挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research has shown an increasing interest in utilizing pre-trainedlarge language models (LLMs) for a variety of time series applications.However, there are three main challenges when using LLMs as foundational modelsfor time series forecasting: (1) Cross-domain generalization. (2)Cross-modality alignment. (3) Error accumulation in autoregressive frameworks.To address these challenges, we proposed LangTime, a language-guided unifiedmodel for time series forecasting that incorporates cross-domain pre-trainingwith reinforcement learning-based fine-tuning. Specifically, LangTimeconstructs Temporal Comprehension Prompts (TCPs), which include dataset-wiseand channel-wise instructions, to facilitate domain adaptation and condensetime series into a single token, enabling LLMs to understand better and aligntemporal data. To improve autoregressive forecasting, we introduce TimePPO, areinforcement learning-based fine-tuning algorithm. TimePPO mitigates erroraccumulation by leveraging a multidimensional rewards function tailored fortime series and a repeat-based value estimation strategy. Extensive experimentsdemonstrate that LangTime achieves state-of-the-art cross-domain forecastingperformance, while TimePPO fine-tuning effectively enhances the stability andaccuracy of autoregressive forecasting.</description>
      <author>example@mail.com (Wenzhe Niu, Zongxia Xie, Yanru Sun, Wei He, Man Xu, Chao Hao)</author>
      <guid isPermaLink="false">2503.08271v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>HIPPO-MAT: Decentralized Task Allocation Using GraphSAGE and Multi-Agent Deep Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2503.07662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2502.02311&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的框架HIPPO-MAT，该框架结合了图神经网络和独立近端策略优化方法来解决异构多智能体系统的分散化连续任务分配问题。UAVs和UGVs通过通信渠道共享聚合的观测数据，并各自处理输入生成增强状态嵌入，从而实现动态、成本最优且冲突感知的任务分配。&lt;h4&gt;背景&lt;/h4&gt;在复杂多变环境中执行任务时，需要一种能够在无需中央协调的情况下高效运作的方法来解决分散化连续任务分配问题。现有的集中式方法虽然性能较好但不适用于大规模和动态变化的环境。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够应对异构多智能体系统中分散化连续任务分配的新框架HIPPO-MAT，该框架旨在实现动态、成本最优的任务分配，并且能够在3D网格环境中处理冲突。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络（GraphSAGE架构）来计算每个代理的独立嵌入，并结合IPPO进行多智能体深度强化学习。同时引入了改进的A*路径规划器来进行高效路由和碰撞避免，以及利用JetBot ROS AI机器人在实际环境中验证该框架。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法能够实现高达92.5%的冲突免费成功率，并且与集中式匈牙利方法相比仅相差16.49%，同时优于基于贪婪算法的启发式分散化基线。此外，还展示了该框架在多达30个代理时仍具有可扩展性并能快速处理任务（每步仿真时间为0.32秒）。&lt;h4&gt;结论&lt;/h4&gt;HIPPO-MAT框架为异构多智能体系统中的分散化连续任务分配提供了一种高效且有效的解决方案。通过改进的A*路径规划器和实际环境验证，展示了其在复杂环境下的适应性和性能优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper tackles decentralized continuous task allocation in heterogeneousmulti-agent systems. We present a novel framework HIPPO-MAT that integratesgraph neural networks (GNN) employing a GraphSAGE architecture to computeindependent embeddings on each agent with an Independent Proximal PolicyOptimization (IPPO) approach for multi-agent deep reinforcement learning. Inour system, unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs)share aggregated observation data via communication channels whileindependently processing these inputs to generate enriched state embeddings.This design enables dynamic, cost-optimal, conflict-aware task allocation in a3D grid environment without the need for centralized coordination. A modifiedA* path planner is incorporated for efficient routing and collision avoidance.Simulation experiments demonstrate scalability with up to 30 agents andpreliminary real-world validation on JetBot ROS AI Robots, each running itsmodel on a Jetson Nano and communicating through an ESP-NOW protocol usingESP32-S3, which confirms the practical viability of the approach thatincorporates simultaneous localization and mapping (SLAM). Experimental resultsrevealed that our method achieves a high 92.5% conflict-free success rate, withonly a 16.49% performance gap compared to the centralized Hungarian method,while outperforming the heuristic decentralized baseline based on greedyapproach. Additionally, the framework exhibits scalability with up to 30 agentswith allocation processing of 0.32 simulation step time and robustness inresponding to dynamically generated tasks.</description>
      <author>example@mail.com (Lavanya Ratnabala, Robinroy Peter, Aleksey Fedoseev, Dzmitry Tsetserukou)</author>
      <guid isPermaLink="false">2503.07662v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>PointVLA: Injecting the 3D World into Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2503.07511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种名为PointVLA的框架，该框架通过向现有的Vision-Language-Action (VLA)模型中注入点云数据来增强其在现实世界机器人任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;传统的VLA模型依赖于RGB图像进行2D视觉语言预训练，在复杂的空间推理方面存在局限性。重新用3D数据对这些模型进行训练是计算密集型的，而抛弃现有的2D数据集则浪费了宝贵的资源。&lt;h4&gt;目的&lt;/h4&gt;为了弥补这一缺口，该研究旨在开发一种方法，可以将点云输入引入已经训练好的VLA模型中，而不需对其进行完全重训。&lt;h4&gt;方法&lt;/h4&gt;PointVLA通过冻结原始的动作专家模块，并在其中添加一个轻量级的模块化块来注入3D特征。为了找到最有效的结合点云表示的方式，研究者进行了跳过层分析以确定哪些原始动作专家中的模块不太有用，从而确保仅向这些模块中注入3D特征。&lt;h4&gt;主要发现&lt;/h4&gt;{'性能优越性': 'PointVLA在仿真和现实世界机器人任务上优于当前最先进的2D模仿学习方法，如OpenVLA、Diffusion Policy 和 DexVLA。', '多任务少样本适应能力': '使用仅有的每个任务20次演示，PointVLA即可成功完成四个不同的任务。', '真实与照片区别识别': '通过利用3D世界知识，PointVLA能够区分真实物体与其图像，提高了安全性和可靠性。', '高度适应性': '与传统的2D模仿学习方法不同，PointVLA使机器人能够应对训练数据中未见过的高低位置变化的对象。'}&lt;h4&gt;结论&lt;/h4&gt;该研究证明了PointVLA在长时任务中的强大性能，如从移动输送带上取放物品，并且它能够在复杂和动态环境中推广其能力。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种称为PointVLA的新框架，旨在通过集成点云数据来增强预训练的Vision-Language-Action (VLA)模型的能力。研究显示，这种方法不仅提升了现有模型在机器人任务中的性能表现，还显著改善了它们处理复杂环境和未见过情况下的适应能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models excel at robotic tasks by leveraginglarge-scale 2D vision-language pretraining, but their reliance on RGB imageslimits spatial reasoning critical for real-world interaction. Retraining thesemodels with 3D data is computationally prohibitive, while discarding existing2D datasets wastes valuable resources. To bridge this gap, we propose PointVLA,a framework that enhances pre-trained VLAs with point cloud inputs withoutrequiring retraining. Our method freezes the vanilla action expert and injects3D features via a lightweight modular block. To identify the most effective wayof integrating point cloud representations, we conduct a skip-block analysis topinpoint less useful blocks in the vanilla action expert, ensuring that 3Dfeatures are injected only into these blocks--minimizing disruption topre-trained representations.  Extensive experiments demonstrate that PointVLA outperforms state-of-the-art2D imitation learning methods, such as OpenVLA, Diffusion Policy and DexVLA,across both simulated and real-world robotic tasks. Specifically, we highlightseveral key advantages of PointVLA enabled by point cloud integration: (1)Few-shot multi-tasking, where PointVLA successfully performs four differenttasks using only 20 demonstrations each; (2) Real-vs-photo discrimination,where PointVLA distinguishes real objects from their images, leveraging 3Dworld knowledge to improve safety and reliability; (3) Height adaptability,Unlike conventional 2D imitation learning methods, PointVLA enables robots toadapt to objects at varying table height that unseen in train data.Furthermore, PointVLA achieves strong performance in long-horizon tasks, suchas picking and packing objects from a moving conveyor belt, showcasing itsability to generalize across complex, dynamic environments.</description>
      <author>example@mail.com (Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, Yichen Zhu)</author>
      <guid isPermaLink="false">2503.07511v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Will LLMs Scaling Hit the Wall? Breaking Barriers via Distributed Resources on Massive Edge Devices</title>
      <link>http://arxiv.org/abs/2503.08223v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;基础模型的成功得益于扩展定律，表明随着训练数据和模型大小的增加，性能会有可预测的进步。&lt;h4&gt;面临的挑战&lt;/h4&gt;大规模模型面临着高质量公共数据稀缺以及计算资源被科技巨头垄断的问题。&lt;h4&gt;提出的解决方案&lt;/h4&gt;利用大量的分布式边缘设备可以打破这些限制，并揭示了在大量边缘设备上未充分利用的数据与计算资源的巨大潜力。&lt;h4&gt;技术进展&lt;/h4&gt;回顾了最近的分布式/联邦学习的技术进步，使得这种新的范式成为可能。&lt;h4&gt;合作愿景&lt;/h4&gt;通过在边缘设备上的协作，每个人都可以使用小型边缘设备参与大型语言模型的训练。&lt;h4&gt;潜在影响&lt;/h4&gt;向分布式边缘设备培训转变具有民主化AI开发并促进更加包容的AI社区的潜力&lt;h4&gt;翻译&lt;/h4&gt;基础模型的成功得益于扩展定律，表明随着训练数据和模型大小的增加，性能会有可预测的进步。然而，这种扩展轨迹面临着两个关键挑战：高质量公共数据耗尽以及大型模型所需的计算能力被科技巨头垄断的问题，这严重阻碍了AI的发展。在本文中，我们主张利用大量分布式边缘设备可以突破这些障碍。我们揭示了在大量的边缘设备上未充分利用的数据和计算资源的巨大潜力，并回顾了最近的分布式/联邦学习的技术进步，使得这种新的范式成为可能。我们的分析表明，通过在边缘设备上的协作，每个人都可以使用小型边缘设备参与大型语言模型的训练。这一向分布式边缘设备培训转变具有民主化AI开发并促进更加包容的AI社区的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The remarkable success of foundation models has been driven by scaling laws,demonstrating that model performance improves predictably with increasedtraining data and model size. However, this scaling trajectory faces twocritical challenges: the depletion of high-quality public data, and theprohibitive computational power required for larger models, which have beenmonopolized by tech giants. These two bottlenecks pose significant obstacles tothe further development of AI. In this position paper, we argue that leveragingmassive distributed edge devices can break through these barriers. We revealthe vast untapped potential of data and computational resources on massive edgedevices, and review recent technical advancements in distributed/federatedlearning that make this new paradigm viable. Our analysis suggests that bycollaborating on edge devices, everyone can participate in training largelanguage models with small edge devices. This paradigm shift towardsdistributed training on edge has the potential to democratize AI developmentand foster a more inclusive AI community.</description>
      <author>example@mail.com (Tao Shen, Didi Zhu, Ziyu Zhao, Chao Wu, Fei Wu)</author>
      <guid isPermaLink="false">2503.08223v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D Object Detection</title>
      <link>http://arxiv.org/abs/2502.15488v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Q-PETR的量化感知位置嵌入转换技术，用于改进基于PETR的方法在自动驾驶中的部署效率和准确性。&lt;h4&gt;背景&lt;/h4&gt;相机多视角3D检测由于低成本和广泛适用性，在自动驾驶中成为一个有吸引力的选择。尽管PETR方法在三维感知基准测试中表现出色，但直接将INT8量化应用于车载部署会导致准确性的大幅下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的量化技术Q-PETR，以解决基于PETR的方法在实际应用中的准确性问题，并提高其在资源受限设备上的性能和效率。&lt;h4&gt;方法&lt;/h4&gt;设计了一种量化的感知位置嵌入转换（Q-PETR），重新设计了PETR框架的关键组件，以便适应低比特推理的交叉注意力机制，并引入自适应量化策略以减少浮点精度损失。&lt;h4&gt;主要发现&lt;/h4&gt;通过这种方式，在标准8位每张图后训练量化下，性能下降不到1%，同时比其FP32版本快两倍且内存使用量减少了三倍。&lt;h4&gt;结论&lt;/h4&gt;Q-PETR在多种PETR系列模型上进行了广泛的实验验证，并显示出良好的泛化能力和实际应用价值。这项工作为自动驾驶系统提供了部署友好的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于相机的多视角3D检测由于其低成本和广泛适用性，已成为自动驾驶的一种有吸引力的方法。尽管基于PETR的方法在三维感知基准测试中表现出色，但直接使用INT8量化进行车载部署会导致高达58.2% mAP和36.9% NDS在NuScenes数据集上的准确度急剧下降。在此工作中，我们提出了一种名为Q-PETR的量化的感知位置嵌入转换技术，重新设计了PETR框架的关键组件，以便解决位置编码和图像特征之间动态范围差异的问题，并适应低比特推理的交叉注意力机制。通过重新设计位置编码模块并引入自适应量化策略，Q-PETR在标准8位每张图后训练量化下能够保持浮点精度下的性能下降不到1%。此外，与相应的FP32版本相比，Q-PETR实现了两倍的速度提升，并将内存使用量减少了三倍，从而为资源受限的车载设备提供了部署友好的解决方案。多种PETR系列模型上的广泛实验验证了我们方法的强大泛化能力和实际应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Camera-based multi-view 3D detection has emerged as an attractive solutionfor autonomous driving due to its low cost and broad applicability. However,despite the strong performance of PETR-based methods in 3D perceptionbenchmarks, their direct INT8 quantization for onboard deployment leads todrastic accuracy drops-up to 58.2% in mAP and 36.9% in NDS on the NuScenesdataset. In this work, we propose Q-PETR, a quantization-aware positionembedding transformation that re-engineers key components of the PETR frameworkto reconcile the discrepancy between the dynamic ranges of positional encodingsand image features, and to adapt the cross-attention mechanism for low-bitinference. By redesigning the positional encoding module and introducing anadaptive quantization strategy, Q-PETR maintains floating-point performancewith a performance degradation of less than 1% under standard 8-bit per-tensorpost-training quantization. Moreover, compared to its FP32 counterpart, Q-PETRachieves a two-fold speedup and reduces memory usage by three times, therebyoffering a deployment-friendly solution for resource-constrained onboarddevices. Extensive experiments across various PETR-series models validate thestrong generalization and practical benefits of our approach.</description>
      <author>example@mail.com (Jiangyong Yu, Changyong Shu, Dawei Yang, Sifan Zhou, Zichen Yu, Xing Hu, Yan Chen)</author>
      <guid isPermaLink="false">2502.15488v2</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>S3R-GS: Streamlining the Pipeline for Large-Scale Street Scene Reconstruction</title>
      <link>http://arxiv.org/abs/2503.08217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近，3D高斯点拟合（3DGS）在逼真的三维重建领域中带来了变革，实现了令人印象深刻的渲染质量和速度。然而，在应用于大规模街景时，现有的方法随着场景规模的增加而面临每视角重建成本急剧上升的问题，导致了显著的计算开销。&lt;h4&gt;背景&lt;/h4&gt;现有技术中的3D高斯点拟合（3DGS）在逼真的三维重建中取得了卓越成就，但在处理大规模街景数据集时遇到了效率问题。当场景规模增大时，常规方法需要执行不必要的局部到全局转换、过多的从3D到2D投影操作以及对遥远内容进行低效渲染。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的方法S3R-GS（用于大规模街道场景重建的3DGS框架），以解决在处理大规模街景数据集时遇到的问题，如计算开销大和难以获得高质量边界框等问题。&lt;h4&gt;方法&lt;/h4&gt;通过重新审视传统管道，确定了三个导致问题的关键因素：不必要的局部到全局转换、过多的从3D到2D投影以及对遥远内容进行低效渲染。S3R-GS提出了一种替代方案——使用2D边界框来代替难以获得的三维边界框，并优化整个重建流程。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，S3R-GS显著提高了渲染质量和加速了重建过程，在处理Argoverse2数据集中的视频时表现出色，达到了PSNR和SSIM指标上的先进水平，并且将重建时间减少到了竞争方法的50%甚至低至20%。&lt;h4&gt;结论&lt;/h4&gt;通过创新地解决大规模街景场景中3DGS面临的挑战，S3R-GS提供了一个实用而高效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, 3D Gaussian Splatting (3DGS) has reshaped the field ofphotorealistic 3D reconstruction, achieving impressive rendering quality andspeed. However, when applied to large-scale street scenes, existing methodssuffer from rapidly escalating per-viewpoint reconstruction costs as scene sizeincreases, leading to significant computational overhead. After revisiting theconventional pipeline, we identify three key factors accounting for this issue:unnecessary local-to-global transformations, excessive 3D-to-2D projections,and inefficient rendering of distant content. To address these challenges, wepropose S3R-GS, a 3DGS framework that Streamlines the pipeline for large-scaleStreet Scene Reconstruction, effectively mitigating these limitations.Moreover, most existing street 3DGS methods rely on ground-truth 3D boundingboxes to separate dynamic and static components, but 3D bounding boxes aredifficult to obtain, limiting real-world applicability. To address this, wepropose an alternative solution with 2D boxes, which are easier to annotate orcan be predicted by off-the-shelf vision foundation models. Such designstogether make S3R-GS readily adapt to large, in-the-wild scenarios. Extensiveexperiments demonstrate that S3R-GS enhances rendering quality andsignificantly accelerates reconstruction. Remarkably, when applied to videosfrom the challenging Argoverse2 dataset, it achieves state-of-the-art PSNR andSSIM, reducing reconstruction time to below 50%--and even 20%--of competingmethods.</description>
      <author>example@mail.com (Guangting Zheng, Jiajun Deng, Xiaomeng Chu, Yu Yuan, Houqiang Li, Yanyong Zhang)</author>
      <guid isPermaLink="false">2503.08217v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Towards All-in-One Medical Image Re-Identification</title>
      <link>http://arxiv.org/abs/2503.08173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种针对医疗图像再识别（MedReID）问题的全面基准和统一模型。通过Continuous Modality-based Parameter Adapter (ComPA) 技术，该模型可以处理多种医学模态，并能够动态调整通用模型以适应特定模态的数据。&lt;h4&gt;背景&lt;/h4&gt;尽管在个性化医疗保健和隐私保护方面有重要应用，但医疗图像再识别（MedReID）的研究尚处于起步阶段。目前缺少一个全面的基准来评估不同模型在这个领域的性能。&lt;h4&gt;目的&lt;/h4&gt;为了促进MedReID领域的发展，本文旨在引入一种能够处理多种医学模态并整合医学先验知识的技术，并展示其在真实世界中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Continuous Modality-based Parameter Adapter (ComPA) 的新方法。该方法能够将医疗内容转换为连续的模态表示，并允许单个模型根据运行时提供的特定模态参数进行自适应学习和处理多样化的数据。&lt;h4&gt;主要发现&lt;/h4&gt;通过对比25种基础模型和8种大规模多模态语言模型在11个图像数据集上的性能，所提出的方法展示出了显著的优势。此外，在两个真实世界的应用场景（即增强的历史个性化诊断和医疗隐私保护）中，该方法也表现出了良好的效果。&lt;h4&gt;结论&lt;/h4&gt;这项研究证明了MedReID技术不仅可以提高医学影像处理的效率，还能有效地应用于实际医疗服务，并为未来的研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要：尽管在个性化健康护理和个人信息保护方面有关键应用，医疗图像再识别（MedReID）至今仍未得到充分探索。本文引入了一套全面的基准和一个统一模型以解决这个问题。首先，为了处理各种医学模态，我们提出了一种新颖的方法——Continuous Modality-based Parameter Adapter (ComPA)。ComPA可以将医疗内容转换为连续的模态表示，并在运行时动态调整通用模型使用特定于模态的参数。这允许单个模型自适应地学习和处理多样化数据。此外，通过与一系列预训练医学基础模型对齐的方式，我们将医学先验知识整合到了我们的模型中。相较于单一图像特征而言，建模图像间差异更好地适配再识别问题，因为这涉及到区分多个图像的问题。我们评估了所提出的模型在25个基础模型和8个大规模多模态语言模型上跨11个数据集的表现，并展示了一致的优越性能。此外，我们将提出的MedReID技术应用于两个现实世界的应用：即增强的历史个性化诊断以及医学隐私保护。代码和模型可在https://github.com/tianyuan168326/All-in-One-MedReID-Pytorch获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical image re-identification (MedReID) is under-explored so far, despiteits critical applications in personalized healthcare and privacy protection. Inthis paper, we introduce a thorough benchmark and a unified model for thisproblem. First, to handle various medical modalities, we propose a novelContinuous Modality-based Parameter Adapter (ComPA). ComPA condenses medicalcontent into a continuous modality representation and dynamically adjusts themodality-agnostic model with modality-specific parameters at runtime. Thisallows a single model to adaptively learn and process diverse modality data.Furthermore, we integrate medical priors into our model by aligning it with abag of pre-trained medical foundation models, in terms of the differentialfeatures. Compared to single-image feature, modeling the inter-image differencebetter fits the re-identification problem, which involves discriminatingmultiple images. We evaluate the proposed model against 25 foundation modelsand 8 large multi-modal language models across 11 image datasets, demonstratingconsistently superior performance. Additionally, we deploy the proposed MedReIDtechnique to two real-world applications, i.e., history-augmented personalizeddiagnosis and medical privacy protection. Codes and model is available at\href{https://github.com/tianyuan168326/All-in-One-MedReID-Pytorch}{https://github.com/tianyuan168326/All-in-One-MedReID-Pytorch}.</description>
      <author>example@mail.com (Yuan Tian, Kaiyuan Ji, Rongzhao Zhang, Yankai Jiang, Chunyi Li, Xiaosong Wang, Guangtao Zhai)</author>
      <guid isPermaLink="false">2503.08173v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>GPT-PPG: A GPT-based Foundation Model for Photoplethysmography Signals</title>
      <link>http://arxiv.org/abs/2503.08015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种新型的应用，即利用生成式预训练Transformer（GPT）模型来处理光体积描记图（PPG）信号，并将其作为各种下游任务的基础模型。&lt;h4&gt;背景&lt;/h4&gt;传统的GPT架构适用于文本等离散数据，但对于连续的PPG信号并不直接适用。本研究尝试将GPT架构适应于PPG信号的特点。&lt;h4&gt;目的&lt;/h4&gt;探索和展示基于预训练GPT模型在PPG信号处理中的潜力及其在各种下游任务（如房颤检测）中的应用效果。&lt;h4&gt;方法&lt;/h4&gt;采用标准的GPT架构并对其进行修改以适应连续的PPG信号特性，利用包含超过2亿个30秒PPG样本的大规模数据集进行预训练，并通过不同的有监督微调技术来调整模型用于下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，经过适当配置和微调后的GPT模型，在房颤检测等任务上达到或超过了当前最先进的方法。此外，该模型还展示出在信号去噪等生成任务上的出色能力，无需额外的训练即可实现。&lt;h4&gt;结论&lt;/h4&gt;研究证明了基于GPT的框架对连续PPG信号的有效性和潜力，为未来的研究提供了一个强有力的基础模型。&lt;h4&gt;翻译&lt;/h4&gt;这项研究引入了一种新型的应用，即利用生成式预训练Transformer（GPT）模型来处理光体积描记图（PPG）信号，并将其作为各种下游任务的基础模型。通过调整标准的GPT架构以适应连续性质的PPG信号，我们的方法展示了非常有前景的结果。我们在包含超过2亿个30秒PPG样本的大规模数据集上对这些模型进行了预训练。我们探索了不同的监督微调技术来使模型适应下游任务，并在诸如房颤检测等任务中取得了与当前最佳水平相媲美或超越的效果。我们的GPT模型的一个显著特点是，它具备执行信号去噪等生成任务的有效能力，无需进一步的微调。这项研究的成功归功于GPT框架的生成性质。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces a novel application of a Generative Pre-trainedTransformer (GPT) model tailored for photoplethysmography (PPG) signals,serving as a foundation model for various downstream tasks. Adapting thestandard GPT architecture to suit the continuous characteristics of PPGsignals, our approach demonstrates promising results. Our models arepre-trained on our extensive dataset that contains more than 200 million 30sPPG samples. We explored different supervised fine-tuning techniques to adaptour model to downstream tasks, resulting in performance comparable to orsurpassing current state-of-the-art (SOTA) methods in tasks like atrialfibrillation detection. A standout feature of our GPT model is its inherentcapability to perform generative tasks such as signal denoising effectively,without the need for further fine-tuning. This success is attributed to thegenerative nature of the GPT framework.</description>
      <author>example@mail.com (Zhaoliang Chen, Cheng Ding, Saurabh Kataria, Runze Yan, Minxiao Wang, Randall Lee, Xiao Hu)</author>
      <guid isPermaLink="false">2503.08015v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Bias in over 100 Text-to-Image Generative Models</title>
      <link>http://arxiv.org/abs/2503.08012v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICLR 2025 Workshop on Open Science for Foundation Models  (SCI-FM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;研究了文本到图像生成模型的偏见趋势，尤其是在开放平台上的可用性变化。&lt;h4&gt;背景&lt;/h4&gt;随着像Hugging Face这样的开源平台使AI更加普及，也导致了一些固有偏见模型的传播，这些模型通常由特定任务的微调形成。&lt;h4&gt;目的&lt;/h4&gt;确保伦理和透明的人工智能部署需要强大的评估框架和可量化的偏见指标。&lt;h4&gt;方法&lt;/h4&gt;在三个关键维度（分布偏差、生成幻觉和生成缺失率）上进行了偏见评估，并分析了超过100个模型的数据，以揭示随着时间的推移和不同生成任务中的偏见模式如何演变。&lt;h4&gt;主要发现&lt;/h4&gt;艺术性和风格转换模型表现出显著的偏见，而受益于更广泛训练分布的基础模型正变得越来越不偏颇。&lt;h4&gt;结论&lt;/h4&gt;通过识别这些系统性的趋势，提供了大规模评估数据集以支持偏见研究和缓解策略，从而促进负责任的人工智能开发。&lt;h4&gt;翻译&lt;/h4&gt;我们调查了文本到图像生成模型的偏见趋势随时间变化的情况，重点是开放平台（如Hugging Face）上模型可用性增加的影响。这些平台虽使AI民主化，但也促进了固有偏颇模型的传播，这种偏见通常是由特定任务微调形成的。确保伦理和透明的人工智能部署需要强大的评估框架和可量化的偏见指标。为此，在三个关键维度上进行了偏见评估：（i）分布偏差；（ii）生成幻觉；以及（iii）生成缺失率。分析了超过100个模型，揭示出随着时间的推移及不同任务中偏见模式的变化情况。我们的发现表明艺术性和风格转换模型表现出显著的偏见，而受益于更广泛训练分布的基础模型正变得越来越不偏颇。通过识别这些系统性的趋势，我们贡献了一个大规模评估数据集以支持偏见研究和缓解策略，从而促进负责任的人工智能开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate bias trends in text-to-image generative models over time,focusing on the increasing availability of models through open platforms likeHugging Face. While these platforms democratize AI, they also facilitate thespread of inherently biased models, often shaped by task-specific fine-tuning.Ensuring ethical and transparent AI deployment requires robust evaluationframeworks and quantifiable bias metrics. To this end, we assess bias acrossthree key dimensions: (i) distribution bias, (ii) generative hallucination, and(iii) generative miss-rate. Analyzing over 100 models, we reveal how biaspatterns evolve over time and across generative tasks. Our findings indicatethat artistic and style-transferred models exhibit significant bias, whereasfoundation models, benefiting from broader training distributions, are becomingprogressively less biased. By identifying these systemic trends, we contributea large-scale evaluation corpus to inform bias research and mitigationstrategies, fostering more responsible AI development.  Keywords: Bias, Ethical AI, Text-to-Image, Generative Models, Open-SourceModels</description>
      <author>example@mail.com (Jordan Vice, Naveed Akhtar, Richard Hartley, Ajmal Mian)</author>
      <guid isPermaLink="false">2503.08012v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>STEAD: Spatio-Temporal Efficient Anomaly Detection for Time and Compute Sensitive Applications</title>
      <link>http://arxiv.org/abs/2503.07942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的时空高效异常检测方法（STEAD），旨在解决自动化系统中实时推断场景下的效率和性能问题。&lt;h4&gt;背景&lt;/h4&gt;随着自动驾驶等系统的普及，确保这些系统的安全性变得越来越重要。现有的许多检测系统在空间上下文中表现良好，但在时间上下文中的改进空间仍然很大。&lt;h4&gt;目的&lt;/h4&gt;快速有效地检测自动系统中的各种异常情况，使它们更安全、更有效。&lt;h4&gt;方法&lt;/h4&gt;STEAD使用(2+1)D卷积和Performer线性注意机制作为基础模型，确保计算效率的同时不牺牲性能。该模型在UCF-Crime基准测试中表现出色，并且提供了一个快速版本以适应参数较少的实时场景需求。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的STEAD模型在UCF-Crime数据集上达到了91.34%的AUC，超过了之前的最先进水平。同时，快速版本虽然拥有更少的参数（减少了99.70%），但仍能保持88.87%的AUC。&lt;h4&gt;结论&lt;/h4&gt;STEAD通过引入(2+1)D卷积和Performer线性注意机制，在保证性能的同时提高了计算效率，并为实时异常检测场景提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容已全部翻译成中文并以分点形式总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a new method for anomaly detection in automated systemswith time and compute sensitive requirements, such as autonomous driving, withunparalleled efficiency. As systems like autonomous driving become increasinglypopular, ensuring their safety has become more important than ever. Therefore,this paper focuses on how to quickly and effectively detect various anomaliesin the aforementioned systems, with the goal of making them safer and moreeffective. Many detection systems have been developed with great success underspatial contexts; however, there is still significant room for improvement whenit comes to temporal context. While there is substantial work regarding thistask, there is minimal work done regarding the efficiency of models and theirability to be applied to scenarios that require real-time inference, i.e.,autonomous driving where anomalies need to be detected the moment they arewithin view. To address this gap, we propose STEAD (Spatio-Temporal EfficientAnomaly Detection), whose backbone is developed using (2+1)D Convolutions andPerformer Linear Attention, which ensures computational efficiency withoutsacrificing performance. When tested on the UCF-Crime benchmark, our base modelachieves an AUC of 91.34%, outperforming the previous state-of-the-art, and ourfast version achieves an AUC of 88.87%, while having 99.70% less parameters andoutperforming the previous state-of-the-art as well. The code and pretrainedmodels are made publicly available at https://github.com/agao8/STEAD</description>
      <author>example@mail.com (Andrew Gao, Jun Liu)</author>
      <guid isPermaLink="false">2503.07942v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>TSP3D: Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding</title>
      <link>http://arxiv.org/abs/2502.10392v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR2025 with a top score&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一个高效的多层次卷积架构用于3D视觉定位。传统的方法由于采用了两阶段或基于点的结构，难以满足实时推理的需求。&lt;h4&gt;背景&lt;/h4&gt;现有的方法在进行3D物体检测时采用多级全稀疏卷积架构取得了成功，但在处理3D视觉定位任务时，传统的两阶段或多点架构难以实现实时推理。&lt;h4&gt;目的&lt;/h4&gt;本文旨在借鉴3D物体检测中成功的多级全稀疏卷积架构，建立一个新型的3D视觉定位框架，并解决基于稀疏卷积结构效率低下的问题。&lt;h4&gt;方法&lt;/h4&gt;为了更有效地融合3D场景表示与文本特征，在逐步区域修剪和目标完成的基础上提出了基于文本引导裁剪(TGP)和基于补全添加(CBA)的方法。TGP通过迭代稀疏化3D场景表示，利用交叉注意力机制高效地将体素特征与文本特征交互；CBA则通过体素补全以微小的计算代价适应性修复过修剪区域。&lt;h4&gt;主要发现&lt;/h4&gt;对比以往的一阶段方法，该方法达到了顶级的推理速度，并且比最快的方法提高了100% FPS。即使与两阶段方法相比，其在ScanRefer上的Acc@0.5指标也领先了+1.13，在NR3D和SR3D上分别领先+2.6和+3.2。&lt;h4&gt;结论&lt;/h4&gt;新提出的TGP和CBA技术有效地解决了3D视觉定位任务中基于稀疏卷积架构的效率问题，实现了速度与准确性的平衡。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们提出了一种用于3D视觉定位的有效多层次卷积结构。传统方法由于两阶段或点基结构难以满足实时推理的需求。受3D物体检测多级全稀疏卷积架构成功的启发，旨在通过这种方法路径建立一个新的3D视觉定位框架。然而，在3D视觉定位任务中，3D场景表示应当深度交互文本特征，因此基于稀疏卷积的架构由于大量的体素特征而效率低下。为了解决这个问题，我们提出了基于文本引导裁剪（TGP）和补全添加（CBA），通过逐步区域修剪和目标完成以高效方式深度融合3D场景表示与文本特征。具体而言，TGP迭代地稀疏化了3D场景表示，并因此通过交叉注意力机制有效地将体素特征与文本特征交互。为了缓解裁剪对精细几何信息的影响，CBA通过体素补全适应性修复过修剪区域，几乎不增加计算开销。相比以前的一阶段方法，我们的方法达到了顶级的推理速度并比之前的最快方法提高了100% FPS。即使和两阶段方法比较，我们的方法在精度上也达到了SOTA水平，在ScanRefer上的Acc@0.5指标领先了+1.13，在NR3D和SR3D上分别领先+2.6和+3.2。代码托管于https://github.com/GWxuan/TSP3D.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/gwxuan/tsp3d&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose an efficient multi-level convolution architecturefor 3D visual grounding. Conventional methods are difficult to meet therequirements of real-time inference due to the two-stage or point-basedarchitecture. Inspired by the success of multi-level fully sparse convolutionalarchitecture in 3D object detection, we aim to build a new 3D visual groundingframework following this technical route. However, as in 3D visual groundingtask the 3D scene representation should be deeply interacted with textfeatures, sparse convolution-based architecture is inefficient for thisinteraction due to the large amount of voxel features. To this end, we proposetext-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3Dscene representation and text features in an efficient way by gradual regionpruning and target completion. Specifically, TGP iteratively sparsifies the 3Dscene representation and thus efficiently interacts the voxel features withtext features by cross-attention. To mitigate the affect of pruning on delicategeometric information, CBA adaptively fixes the over-pruned region by voxelcompletion with negligible computational overhead. Compared with previoussingle-stage methods, our method achieves top inference speed and surpassesprevious fastest method by 100\% FPS. Our method also achieves state-of-the-artaccuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 onScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The codeis available at\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.</description>
      <author>example@mail.com (Wenxuan Guo, Xiuwei Xu, Ziwei Wang, Jianjiang Feng, Jie Zhou, Jiwen Lu)</author>
      <guid isPermaLink="false">2502.10392v2</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Visual and Text Prompt Segmentation: A Novel Multi-Model Framework for Remote Sensing</title>
      <link>http://arxiv.org/abs/2503.07911v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review - IEEE Journal of Selected Topics in Applied Earth  Observations and Remote Sensing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的VTPSeg管道，用于增强开放词汇图像分割，特别是在遥感领域。通过结合Grounding DINO、CLIP和Segment Anything Model（SAM）的优势，该方法解决了当前模型在处理复杂多目标遥感影像时的冗余掩码生成、忽略局部对象和缺乏大规模空视图预训练等问题。&lt;h4&gt;背景&lt;/h4&gt;像素级分割对于遥感任务至关重要。基础视觉模型如CLIP和Segment Anything Model(SAM)已经在零样本分割任务中展示了显著的能力，但它们在处理特定于遥感的挑战时仍面临重大问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有的Pixel-level分割方法中的冗余掩码生成、忽略局部对象以及缺乏大规模空视图预训练等问题，提出了一种新的VTPSeg管道来提高多目标遥感图像分割的效果。&lt;h4&gt;方法&lt;/h4&gt;通过设计创新的VTPSeg管道，利用Grounding DINO+(GD+)模块产生初始候选边界框，CLIP Filter++(CLIP++)模块结合视觉和文本提示进一步细化并过滤掉无关对象的边界框。这些经过优化的边界框作为FastSAM模型的精确分割任务的具体提示。&lt;h4&gt;主要发现&lt;/h4&gt;VTPSeg管道通过有效解决现有基础模型在处理遥感图像时的问题，实现了更准确、高效的多目标分割结果。&lt;h4&gt;结论&lt;/h4&gt;实验和消融研究表明，所提出的VTPSeg方法能够显著提高多目标遥感图像的像素级分割效果，并且验证了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;像素级分割在遥感领域至关重要。尽管像CLIP和Segment Anything Model（SAM）这样的基础视觉模型已在零样本分割任务中表现出色，但在应对特定于遥感领域的挑战时仍然存在重大问题。本文提出了一种新颖的VTPSeg管道，它通过结合Grounding DINO、CLIP以及SAM的优点来增强开放词汇图像分割的效果。该方法有效解决了现有模型在处理复杂多目标遥感影像中的冗余掩码生成和忽略局部对象等问题，并验证了其在五个流行的数据集上的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pixel-level segmentation is essential in remote sensing, where foundationalvision models like CLIP and Segment Anything Model(SAM) have demonstratedsignificant capabilities in zero-shot segmentation tasks. Despite theiradvances, challenges specific to remote sensing remain substantial. Firstly,The SAM without clear prompt constraints, often generates redundant masks, andmaking post-processing more complex. Secondly, the CLIP model, mainly designedfor global feature alignment in foundational models, often overlooks localobjects crucial to remote sensing. This oversight leads to inaccuraterecognition or misplaced focus in multi-target remote sensing imagery. Thirdly,both models have not been pre-trained on multi-scale aerial views, increasingthe likelihood of detection failures. To tackle these challenges, we introducethe innovative VTPSeg pipeline, utilizing the strengths of Grounding DINO,CLIP, and SAM for enhanced open-vocabulary image segmentation. The GroundingDINO+(GD+) module generates initial candidate bounding boxes, while the CLIPFilter++(CLIP++) module uses a combination of visual and textual prompts torefine and filter out irrelevant object bounding boxes, ensuring that onlypertinent objects are considered. Subsequently, these refined bounding boxesserve as specific prompts for the FastSAM model, which executes precisesegmentation. Our VTPSeg is validated by experimental and ablation studyresults on five popular remote sensing image segmentation datasets.</description>
      <author>example@mail.com (Xing Zi, Kairui Jin, Xian Tao, Jun Li, Ali Braytee, Rajiv Ratn Shah, Mukesh Prasad)</author>
      <guid isPermaLink="false">2503.07911v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>A Quantum Neural Network Transfer-Learning Model for Forecasting Problems with Continuous and Discrete Variables</title>
      <link>http://arxiv.org/abs/2503.07633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了一种用于预测问题的连续变量量子神经网络(CV-QNN)模型，该模型作为迁移学习方法被设计出来。&lt;h4&gt;背景&lt;/h4&gt;传统经典神经网络在处理复杂预测任务时可能需要大量的训练和参数调整。因此，研究者探索了利用量子计算来简化这一过程的可能性。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单的量子网络结构，名为QNNet10，能够以随机初始化的方式达到高精度，并可以作为预训练模型应用于新的数据集上进行微调而无需重新训练或调整参数。&lt;h4&gt;方法&lt;/h4&gt;提出了一个连续变量的量子神经网络(CV-QNN)和与其相似架构但使用离散变量的量子神经网络(DV-QNN)，其中CV-QNN模型仅包含8个可调节参数，1层量子层以及两个光子纠缠的两个线路，而DV-QNN在两线路结构上表现不佳后提出了四线程的改进版本。&lt;h4&gt;主要发现&lt;/h4&gt;提出的预训练模型能够在无需再训练或调整参数的情况下直接应用于新的数据集，并且对于各种规模的数据都能有效工作。同时，CV-QNN比DV-QNN更为高效和简单。&lt;h4&gt;结论&lt;/h4&gt;量子神经网络具有作为迁移学习工具来解决预测问题的潜力，特别是在处理复杂数据时可以减少计算需求并提高效率。&lt;h4&gt;翻译&lt;/h4&gt;该研究介绍了用于预测问题的一种连续变量量子神经网络(CV-QNN)模型。提出的量子技术具有简单的结构，仅有8个可调节参数和10个量子门组成的单层量子层，并能够模仿经典神经网络的功能，随机初始化后只需一次迭代即可达到高精度。此预训练模型在应用于新数据集时无需再进行训练或调整参数的创新设计允许固定参数并添加最终微调层。此外还提出了一种等效离散变量量子神经网络(DV-QNN)，其结构与CV模型相似，但分析表明两线路DV模型性能提升不明显，因此提出了四线路DV模型，虽然结果相当但仍需更大和更复杂的结构以及更多的门。预训练模型被应用于五种不同规模的预测问题中，展示了它的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces a continuous-variable quantum neural network (CV-QNN)model designed as a transfer-learning approach for forecasting problems. Theproposed quantum technique features a simple structure with only eighttrainable parameters, a single quantum layer with two wires to createentanglement, and ten quantum gates, hence the name QNNet10, effectivelymimicking the functionality of classical neural networks. A notable aspect isthat the quantum network achieves high accuracy with random initializationafter a single iteration. This pretrained model is innovative as it requires notraining or parameter tuning when applied to new datasets, allowing forparameter freezing while enabling the addition of a final layer forfine-tuning. Additionally, an equivalent discrete-variable quantum neuralnetwork (DV-QNN) is presented, structured similarly to the CV model. However,analysis shows that the two-wire DV model does not significantly enhanceperformance. As a result, a four-wire DV model is proposed, achievingcomparable results but requiring a larger and more complex structure withadditional gates. The pretrained model is applied to five forecasting problemsof varying sizes, demonstrating its effectiveness.</description>
      <author>example@mail.com (Ismael Abdulrahman)</author>
      <guid isPermaLink="false">2503.07633v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Video Action Differencing</title>
      <link>http://arxiv.org/abs/2503.07860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025 (International Conference on Learning Representations)  Project page: http://jmhb0.github.io/viddiff Benchmark:  https://huggingface.co/datasets/jmhb/VidDiffBench&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的任务Video Action Differencing (VidDiff)，旨在识别相同动作视频中的细微差别，并创建了包含549对视频的基准数据集VidDiffBench，以评估大型多模态模型在该领域的性能。&lt;h4&gt;背景&lt;/h4&gt;研究如何两个个体在同一行动中表现出不同的方式。这一任务的应用场景广泛，例如教练指导和技能学习。&lt;h4&gt;目的&lt;/h4&gt;为了推动这个新任务的发展，创建了VidDiffBench数据集，并展示了当前最先进的大型多模态模型（LMMs）在该基准上的表现挑战性。&lt;h4&gt;方法&lt;/h4&gt;通过分析这些模型的失败案例，确定了两个关键挑战：跨视频定位相关的子动作和精细级帧比较。为此提出了一种新的VidDiff方法，利用专门的基础模型将任务分为三个阶段执行。&lt;h4&gt;主要发现&lt;/h4&gt;VidDiffBench基准数据集对最先进的大型多模态模型提出了重大挑战。&lt;h4&gt;结论&lt;/h4&gt;通过详细的分析与实验，强调了VidDiff任务的特殊挑战，并提出了解决方案。同时公开了VidDiffBench数据集和相关代码以促进未来的进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;如何在执行相同动作时区分两个个体？这项工作引入了一个新任务——视频行为差异识别（Video Action Differencing，简称VidDiff），旨在识别同一动作的视频之间的细微差别。为了推进这个领域的研究发展，团队创建了VidDiffBench数据集，并展示了当前最先进的大型多模态模型在处理该任务时面临的困难，同时提出了一种新的方法来克服这些挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; How do two individuals differ when performing the same action? In this work,we introduce Video Action Differencing (VidDiff), the novel task of identifyingsubtle differences between videos of the same action, which has manyapplications, such as coaching and skill learning. To enable development onthis new task, we first create VidDiffBench, a benchmark dataset containing 549video pairs, with human annotations of 4,469 fine-grained action differencesand 2,075 localization timestamps indicating where these differences occur. Ourexperiments demonstrate that VidDiffBench poses a significant challenge forstate-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL.By analyzing failure cases of LMMs on VidDiffBench, we highlight two keychallenges for this task: localizing relevant sub-actions over two videos andfine-grained frame comparison. To overcome these, we propose the VidDiffmethod, an agentic workflow that breaks the task into three stages: actiondifference proposal, keyframe localization, and frame differencing, each stageutilizing specialized foundation models. To encourage future research in thisnew task, we release the benchmark athttps://huggingface.co/datasets/jmhb/VidDiffBench and code athttp://jmhb0.github.io/viddiff.</description>
      <author>example@mail.com (James Burgess, Xiaohan Wang, Yuhui Zhang, Anita Rau, Alejandro Lozano, Lisa Dunlap, Trevor Darrell, Serena Yeung-Levy)</author>
      <guid isPermaLink="false">2503.07860v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>TwinTURBO: Semi-Supervised Fine-Tuning of Foundation Models via Mutual Information Decompositions for Downstream Task and Latent Spaces</title>
      <link>http://arxiv.org/abs/2503.07851v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种利用互信息分解来解决有限标注数据训练问题的半监督微调框架。&lt;h4&gt;背景&lt;/h4&gt;在仅有少量标注数据的情况下训练基础模型时，面临着如何有效使用未标注数据的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种既能保留预训练结构又能充分利用未标注数据以改善分类任务性能的方法。&lt;h4&gt;方法&lt;/h4&gt;{'两个下界导出': ['针对下游任务空间（如分类），优化条件和边缘交叉熵以及Kullback-Leibler散度', '对于潜在空间表示，通过对比式分解进行正则化并对其对齐'], '微调策略': '仅修改一个特殊投影器模块，该模块包含一个小的转换器和一种标记聚合技术，同时保持预训练结构不变。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在极低标注数据条件下通过有效利用未标注数据能显著提高分类任务的表现。&lt;h4&gt;结论&lt;/h4&gt;所提出的半监督微调框架能够在有限的标签环境中实现更好的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种针对基础模型的半监督微调框架，该框架使用互信息分解来解决在仅有少量标记数据的情况下进行训练的问题。我们的方法导出了两个不同的下界：对于下游任务空间（例如分类），通过条件和边缘交叉熵以及Kullback-Leibler散度进行优化；对于潜在空间表示，则采用类似于对比式的分解进行正则化并对其对齐。这种微调策略保留了基础模型的预训练结构，仅修改了一个包含一个小型转换器和标记聚合技术的专业投影模块。在多个数据集上的实验表明，在极端低标注条件下，通过有效地利用未标注数据能显著改善分类任务的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a semi-supervised fine-tuning framework for foundation models thatutilises mutual information decomposition to address the challenges of trainingfor a limited amount of labelled data. Our approach derives two distinct lowerbounds: i) for the downstream task space, such as classification, optimisedusing conditional and marginal cross-entropy alongside Kullback-Leiblerdivergence, and ii) for the latent space representation, regularised andaligned using a contrastive-like decomposition. This fine-tuning strategyretains the pre-trained structure of the foundation model, modifying only aspecialised projector module comprising a small transformer and a tokenaggregation technique. Experiments on several datasets demonstrate significantimprovements in classification tasks under extremely low-labelled conditions byeffectively leveraging unlabelled data.</description>
      <author>example@mail.com (Guillaume Quétant, Pavlo Molchanov, Slava Voloshynovskiy)</author>
      <guid isPermaLink="false">2503.07851v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>A primer on optimal transport for causal inference with observational data</title>
      <link>http://arxiv.org/abs/2503.07811v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文概述了最优传输理论与因果推断之间的深厚联系，并介绍了这种连接如何为观测数据中的因果效应识别奠定基础。&lt;h4&gt;背景&lt;/h4&gt;最优传输理论已成为比较概率分布的强大框架，广泛应用于科学领域。它与因果推断的核心思想自然契合，但两者的交叉研究才刚刚开始。&lt;h4&gt;目的&lt;/h4&gt;文章旨在介绍最优传输和因果效应识别之间的现有联系，并统一统计学、数学和计量经济学领域的语言和符号。&lt;h4&gt;方法&lt;/h4&gt;通过回顾现有的模型并指出其潜在的最优传输原理，同时提出新的问题和未来的研究方向来探讨这两个领域之间的相互作用。&lt;h4&gt;主要发现&lt;/h4&gt;许多用于因果推断的基础模型几十年来一直在无意识地使用最优传输原则，并且这种联系正在形成一个全新的研究方向。&lt;h4&gt;结论&lt;/h4&gt;本文揭示了最优运输理论对因果效应识别的深远影响，为未来的跨学科合作提供了契机。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文的大意是：最优传输理论已经成为比较概率分布的强大框架，在科学各个领域都有广泛应用。其核心思想即通过比较状态空间来分析概率与因果推断中的关键思想不谋而合——理解并量化反事实状态至关重要。尽管这种直观联系存在，但明确的交叉研究还处于起步阶段。然而，用于因果推断的基础模型已经隐含地依赖于最优传输原则数十年之久，只是未被意识到二者之间的关联。因此，本文旨在介绍最优运输与观测数据中因果效应识别之间令人惊讶的深度连接——其中最优传输不仅是一套潜在工具，而且实际构成了假设基础的一部分。最终目的是通过指出现有联系来统一不同统计学、数学和计量经济学领域的语言和符号，并探讨由此发现衍生出的新问题和未来工作方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The theory of optimal transportation has developed into a powerful andelegant framework for comparing probability distributions, with wide-rangingapplications in all areas of science. The fundamental idea of analyzingprobabilities by comparing their underlying state space naturally aligns withthe core idea of causal inference, where understanding and quantifyingcounterfactual states is paramount. Despite this intuitive connection, explicitresearch at the intersection of optimal transport and causal inference is onlybeginning to develop. Yet, many foundational models in causal inference haveimplicitly relied on optimal transport principles for decades, withoutrecognizing the underlying connection. Therefore, the goal of this review is tooffer an introduction to the surprisingly deep existing connections betweenoptimal transport and the identification of causal effects with observationaldata -- where optimal transport is not just a set of potential tools, butactually builds the foundation of model assumptions. As a result, this reviewis intended to unify the language and notation between different areas ofstatistics, mathematics, and econometrics, by pointing out these existingconnections, and to explore novel problems and directions for future work inboth areas derived from this realization.</description>
      <author>example@mail.com (Florian F Gunsilius)</author>
      <guid isPermaLink="false">2503.07811v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model</title>
      <link>http://arxiv.org/abs/2503.07703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Official Page: https://team.doubao.com/tech/seedream&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;介绍Seedream 2.0模型，该模型在图像生成领域解决了现有模型的一些局限性。&lt;h4&gt;背景描述&lt;/h4&gt;扩散模型的快速发展促进了图像生成领域的显著进步，但主流模型仍然存在模型偏见、文本渲染能力有限和对中国文化理解不足的问题。&lt;h4&gt;研究目的&lt;/h4&gt;为了克服这些限制，我们开发了Seedream 2.0，这是一个中英文双语图像生成基础模型。&lt;h4&gt;技术方法&lt;/h4&gt;构建了一个强大的数据系统来促进知识整合，并设计了一套平衡准确性和丰富性的描述系统。此外，利用自研发的大型语言模型作为文本编码器，直接从大量数据中学习原生知识。&lt;h4&gt;主要发现&lt;/h4&gt;Seedream 2.0在多个方面达到了最先进的性能表现，包括遵循提示、美学效果、文本渲染和结构性正确性，并且通过多次RLHF迭代优化后更贴近人类偏好。&lt;h4&gt;结论意义&lt;/h4&gt;Seedream 2.0可以灵活适应指令式的图像编辑模型，如SeedEdit，具备平衡指令跟随与图片一致性的能力。&lt;h4&gt;其他创新点&lt;/h4&gt;Glyph-Aligned ByT5用于灵活的文字级文本渲染，而Scaled ROPE则适用于各种未经训练的分辨率。通过多阶段后训练优化，进一步提升了整体能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid advancement of diffusion models has catalyzed remarkable progress inthe field of image generation. However, prevalent models such as Flux, SD3.5and Midjourney, still grapple with issues like model bias, limited textrendering capabilities, and insufficient understanding of Chinese culturalnuances. To address these limitations, we present Seedream 2.0, a nativeChinese-English bilingual image generation foundation model that excels acrossdiverse dimensions, which adeptly manages text prompt in both Chinese andEnglish, supporting bilingual image generation and text rendering. We develop apowerful data system that facilitates knowledge integration, and a captionsystem that balances the accuracy and richness for image description.Particularly, Seedream is integrated with a self-developed bilingual largelanguage model as a text encoder, allowing it to learn native knowledgedirectly from massive data. This enable it to generate high-fidelity imageswith accurate cultural nuances and aesthetic expressions described in eitherChinese or English. Beside, Glyph-Aligned ByT5 is applied for flexiblecharacter-level text rendering, while a Scaled ROPE generalizes well tountrained resolutions. Multi-phase post-training optimizations, including SFTand RLHF iterations, further improve the overall capability. Through extensiveexperimentation, we demonstrate that Seedream 2.0 achieves state-of-the-artperformance across multiple aspects, including prompt-following, aesthetics,text rendering, and structural correctness. Furthermore, Seedream 2.0 has beenoptimized through multiple RLHF iterations to closely align its output withhuman preferences, as revealed by its outstanding ELO score. In addition, itcan be readily adapted to an instruction-based image editing model, such asSeedEdit, with strong editing capability that balances instruction-followingand image consistency.</description>
      <author>example@mail.com (Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Xin Xia, Xuefeng Xiao, Linjie Yang, Zhonghua Zhai, Xinyu Zhang, Qi Zhang, Yuwei Zhang, Shijia Zhao, Jianchao Yang, Weilin Huang)</author>
      <guid isPermaLink="false">2503.07703v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous Driving VQA from Peru</title>
      <link>http://arxiv.org/abs/2503.07587v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A pre-print. 26 pages. Link to Code + Data:  https://huggingface.co/datasets/Artificio/robusto-1&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;随着多模态基础模型在自动驾驶汽车中的实验部署，一个关键问题是这些系统在某些驾驶情况下的反应是否类似于人类——特别是在那些罕见或不常见的场景下。&lt;h4&gt;目的&lt;/h4&gt;创建Robusto-1数据集来研究视觉语言模型（VLM）与人在认知水平上的相似性，特别是当遇到未见过的复杂交通环境时的情况。&lt;h4&gt;方法&lt;/h4&gt;使用代表秘鲁驾驶环境中记录的行车记录仪视频数据，通过多模态视觉问答任务和系统神经科学中的代表性相似性分析（RSA），来比较人类和机器对各种问题的回答。&lt;h4&gt;主要发现&lt;/h4&gt;根据提问类型的不同，VLM与人的认知一致性程度存在显著差异，这表明两者之间存在一定的差距。&lt;h4&gt;结论&lt;/h4&gt;研究结果揭示了VLM在理解复杂、少见交通状况时的认知局限性，并强调需要进一步探究人类和机器之间的认知对齐问题以提高自动驾驶系统的可靠性。&lt;h4&gt;翻译&lt;/h4&gt;随着多模态基础模型开始实验性地部署于自动驾驶汽车，一个合理的问题是我们如何衡量这些系统在某些驾驶情况下的反应是否类似于人类——特别是在那些罕见或不常见的场景下。为了研究这个问题，我们创建了Robusto-1数据集，该数据集使用了来自秘鲁的行车记录仪视频数据（这是一个世界上司机最粗暴、交通拥堵指数最高的国家之一，并且包含许多训练中从未见过的独特街景物体）。特别地，在认知水平上初步测试基础视觉语言模型（VLMs）与人类在驾驶中的表现差异时，我们转而从边界框、分割图、占用地图或轨迹估计转向使用多模态视觉问答任务来进行比较。通过系统神经科学中广泛采用的方法——代表性相似性分析（RSA），我们将分别考察人和机器对于不同问题类型的回答来展示VLMs与人类在哪些情况下会达成一致或产生分歧，从而探索它们的认知一致性。我们发现，这种认知对齐程度根据提问给定的问题类型存在显著差异，这揭示了两者之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As multimodal foundational models start being deployed experimentally inSelf-Driving cars, a reasonable question we ask ourselves is how similar tohumans do these systems respond in certain driving situations -- especiallythose that are out-of-distribution? To study this, we create the Robusto-1dataset that uses dashcam video data from Peru, a country with one of the worst(aggressive) drivers in the world, a high traffic index, and a high ratio ofbizarre to non-bizarre street objects likely never seen in training. Inparticular, to preliminarly test at a cognitive level how well FoundationalVisual Language Models (VLMs) compare to Humans in Driving, we move away frombounding boxes, segmentation maps, occupancy maps or trajectory estimation tomulti-modal Visual Question Answering (VQA) comparing both humans and machinesthrough a popular method in systems neuroscience known as RepresentationalSimilarity Analysis (RSA). Depending on the type of questions we ask and theanswers these systems give, we will show in what cases do VLMs and Humansconverge or diverge allowing us to probe on their cognitive alignment. We findthat the degree of alignment varies significantly depending on the type ofquestions asked to each type of system (Humans vs VLMs), highlighting a gap intheir alignment.</description>
      <author>example@mail.com (Dunant Cusipuma, David Ortega, Victor Flores-Benites, Arturo Deza)</author>
      <guid isPermaLink="false">2503.07587v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Keypoint Detection and Description for Raw Bayer Images</title>
      <link>http://arxiv.org/abs/2503.08673v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;关键点检测和局部特征描述是机器人感知中的基础任务，对于如SLAM、机器人定位、特征匹配、姿态估计和3D地图构建等应用至关重要。现有方法主要针对RGB图像处理。&lt;h4&gt;目的&lt;/h4&gt;提出一种直接处理原始图像的新型网络，不依赖于图像信号处理器（ISP），从而显著降低硬件需求和内存消耗。&lt;h4&gt;方法&lt;/h4&gt;设计了两个自定义卷积核，可以直接在原始图像上进行卷积操作，并保留通道间的信息而不必转换成RGB格式。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该网络在处理原始图像时优于现有算法，在较大的旋转角度和尺度变化下仍保持较高的准确性和稳定性。&lt;h4&gt;结论&lt;/h4&gt;这项工作是首次针对原始图像开发关键点检测和特征描述的网络，为资源受限环境提供了一种更高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Keypoint detection and local feature description are fundamental tasks inrobotic perception, critical for applications such as SLAM, robot localization,feature matching, pose estimation, and 3D mapping. While existing methodspredominantly operate on RGB images, we propose a novel network that directlyprocesses raw images, bypassing the need for the Image Signal Processor (ISP).This approach significantly reduces hardware requirements and memoryconsumption, which is crucial for robotic vision systems. Our method introducestwo custom-designed convolutional kernels capable of performing convolutionsdirectly on raw images, preserving inter-channel information without convertingto RGB. Experimental results show that our network outperforms existingalgorithms on raw images, achieving higher accuracy and stability under largerotations and scale variations. This work represents the first attempt todevelop a keypoint detection and feature description network specifically forraw images, offering a more efficient solution for resource-constrainedenvironments.</description>
      <author>example@mail.com (Jiakai Lin, Jinchang Zhang, Guoyu Lu)</author>
      <guid isPermaLink="false">2503.08673v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Embodiment Robotic Manipulation Synthesis via Guided Demonstrations through CycleVAE and Human Behavior Transformer</title>
      <link>http://arxiv.org/abs/2503.08622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review in IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种通过CycleVAE和人类行为转换器实现的跨实体机器人操作合成算法，旨在解决复杂任务中的跨实体数据稀疏性和控制器设计困难问题。&lt;h4&gt;背景&lt;/h4&gt;复杂的跨实体机器人操控面临挑战，主要是由于缺少配对的跨实体数据集以及难以设计复杂的控制器。&lt;h4&gt;目的&lt;/h4&gt;通过利用引导的人类专家演示实现机器人学习，提出了一种新颖的跨实体机器人操作算法。&lt;h4&gt;方法&lt;/h4&gt;{'第一部分': '使用无监督CycleVAE和双向子空间对齐算法来在不同实体之间的潜在运动序列之间进行对齐。', '第二部分': '设计了因果人类行为转换器以学习专家演示中的内在动力学。测试阶段，利用该转换器生成专家示范数据，并通过CycleVAE进行最终的人机操作合成。'}&lt;h4&gt;主要发现&lt;/h4&gt;在使用灵巧的机器人手进行了广泛的实验后，成功地产生了复杂任务中的平滑轨迹，超越了现有的基于学习的方法。&lt;h4&gt;结论&lt;/h4&gt;这项工作对无监督跨实体对齐和未来的自主机器人设计具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;跨主体机器人的操控合成对于复杂的任务来说是一个挑战。为了解决这个问题，提出了一种新颖的算法，该算法使用CycleVAE以及人类行为转换器来进行跨实体操作的对齐与学习，并通过实验验证了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-embodiment robotic manipulation synthesis for complicated tasks ischallenging, partially due to the scarcity of paired cross-embodiment datasetsand the impediment of designing intricate controllers. Inspired by roboticlearning via guided human expert demonstration, we here propose a novelcross-embodiment robotic manipulation algorithm via CycleVAE and human behaviortransformer. First, we utilize unsupervised CycleVAE together with abidirectional subspace alignment algorithm to align latent motion sequencesbetween cross-embodiments. Second, we propose a casual human behaviortransformer design to learn the intrinsic motion dynamics of human expertdemonstrations. During the test case, we leverage the proposed transformer forthe human expert demonstration generation, which will be aligned using CycleVAEfor the final human-robotic manipulation synthesis. We validated our proposedalgorithm through extensive experiments using a dexterous robotic manipulatorwith the robotic hand. Our results successfully generate smooth trajectoriesacross intricate tasks, outperforming prior learning-based robotic motionplanning algorithms. These results have implications for performingunsupervised cross-embodiment alignment and future autonomous robotics design.Complete video demonstrations of our experiments can be found inhttps://sites.google.com/view/humanrobots/home.</description>
      <author>example@mail.com (Apan Dastider, Hao Fang, Mingjie Lin)</author>
      <guid isPermaLink="false">2503.08622v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>HiP-AD: Hierarchical and Multi-Granularity Planning with Deformable Attention for Autonomous Driving in a Single Decoder</title>
      <link>http://arxiv.org/abs/2503.08612v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个名为HiP-AD的新型端到端自动驾驶框架，该框架结合了感知、预测和规划功能，并通过多粒度规划查询表示提高了闭环控制精度。&lt;h4&gt;背景&lt;/h4&gt;现有的端到端自主驾驶技术在闭环评估中的表现不尽如人意，尤其是在利用规划进行查询设计与交互方面潜力未被充分挖掘。&lt;h4&gt;目的&lt;/h4&gt;探索如何更好地将规划引入查询设计和交互中以改进自动驾驶的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种多粒度规划查询表示法，该表示法融合了不同类型的道路点（包括空间、时间及驾驶风格道路点），并通过结合可变形注意力机制有效检索基于物理位置的相关图像特征。这些策略被整合到一个统一的解码器框架中进行端到端处理。&lt;h4&gt;主要发现&lt;/h4&gt;HiP-AD在闭环基准测试Bench2Drive和真实世界数据集nuScenes上均表现出优于现有方法的表现，展示了其综合交互能力和性能优势。&lt;h4&gt;结论&lt;/h4&gt;该工作表明通过更好地整合规划与查询设计可以在自动驾驶系统中实现更精确的闭环控制，并取得了显著的技术进步。&lt;h4&gt;翻译&lt;/h4&gt;尽管近年来端到端自主驾驶技术已取得重大进展，但在闭环评估中的表现仍然不尽如人意。尚未充分利用规划在查询设计和交互方面的潜力。本文介绍了一种多粒度规划查询表示法，该表示法融合了不同类型的道路点（包括空间、时间及驾驶风格道路点），并通过结合可变形注意力机制有效检索基于物理位置的相关图像特征。这些策略被整合到一个统一的解码器框架中进行端到端处理。HiP-AD在闭环基准测试Bench2Drive和真实世界数据集nuScenes上均表现出优于现有方法的表现，展示了其综合交互能力和性能优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although end-to-end autonomous driving (E2E-AD) technologies have madesignificant progress in recent years, there remains an unsatisfactoryperformance on closed-loop evaluation. The potential of leveraging planning inquery design and interaction has not yet been fully explored. In this paper, weintroduce a multi-granularity planning query representation that integratesheterogeneous waypoints, including spatial, temporal, and driving-stylewaypoints across various sampling patterns. It provides additional supervisionfor trajectory prediction, enhancing precise closed-loop control for the egovehicle. Additionally, we explicitly utilize the geometric properties ofplanning trajectories to effectively retrieve relevant image features based onphysical locations using deformable attention. By combining these strategies,we propose a novel end-to-end autonomous driving framework, termed HiP-AD,which simultaneously performs perception, prediction, and planning within aunified decoder. HiP-AD enables comprehensive interaction by allowing planningqueries to iteratively interact with perception queries in the BEV space whiledynamically extracting image features from perspective views. Experimentsdemonstrate that HiP-AD outperforms all existing end-to-end autonomous drivingmethods on the closed-loop benchmark Bench2Drive and achieves competitiveperformance on the real-world dataset nuScenes.</description>
      <author>example@mail.com (Yingqi Tang, Zhuoran Xu, Zhaotie Meng, Erkang Cheng)</author>
      <guid isPermaLink="false">2503.08612v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>A Grid Cell-Inspired Structured Vector Algebra for Cognitive Maps</title>
      <link>http://arxiv.org/abs/2503.08608v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, accepted at the 2025 Neuro Inspired  Computational Elements (NICE) conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;文章提出了一种新型的网格细胞向量符号架构模型（GC-VSA），该模型能够模拟内嗅皮质-海马回路在连续空间和抽象空间中的信息处理。&lt;h4&gt;背景&lt;/h4&gt;内嗅皮层-海马结构是哺乳动物大脑的空间导航系统，通过六边形感受野编码物理和抽象空间。尽管连续吸引子网络（CAN）成功地对网格细胞进行了建模以编码物理空间，但将这种模型用于同时处理连续的物理空间与抽象空间仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，本文提出了一种基于神经符号计算框架的机制模型，旨在提高内嗅皮层-海马回路的信息处理灵活性和多样性。&lt;h4&gt;方法&lt;/h4&gt;提出的GC-VSA模型采用了空间结构化编码方案，并使用了三维神经元模块来模仿网格细胞在不同尺度和方向上的离散特性。&lt;h4&gt;主要发现&lt;/h4&gt;['该模型能够执行精确路径整合以跟踪位置，进行时空表示以查询对象的位置和时间关系，以及通过家庭树作为层次关系的有结构案例来进行符号推理。']&lt;h4&gt;结论&lt;/h4&gt;这项工作为神经科学、机器人技术和机器学习领域提供了一种新的计算框架，它既具有连续物理空间编码能力又具备处理抽象信息的能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：内嗅皮层-海马环路是哺乳动物大脑的空间导航系统，通过网格细胞来编码物理和抽象空间。这一系统的效率和灵活性使其在机器人技术和机器学习应用中极具吸引力。尽管连续吸引子网络（CANs）成功地对内嗅皮质中的网格细胞进行了建模以编码物理空间，但将这种模型用于同时处理连续的物理空间与抽象空间仍然具有挑战性。在这里，我们试图通过提出一种基于连续吸引子网络和向量符号架构（VSAs），即神经符号计算框架的机制模型来弥合这一差距，该模型灵感来自内嗅皮层-海马回路的信息处理能力。提出的新型网格细胞VSA (GC-VSA) 模型采用了空间结构化编码方案，并使用了三维神经元模块模仿网格细胞在不同尺度和方向上的离散特性，重现它们的六边形感受野特征。实验中，该模型展示了其执行空间和抽象任务的能力：（1）准确路径整合以跟踪位置；（2）时空表示以查询对象的位置及时间关系；以及（3）符号推理，使用家庭树作为层次关系的有结构案例进行测试。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The entorhinal-hippocampal formation is the mammalian brain's navigationsystem, encoding both physical and abstract spaces via grid cells. This systemis well-studied in neuroscience, and its efficiency and versatility make itattractive for applications in robotics and machine learning. While continuousattractor networks (CANs) successfully model entorhinal grid cells for encodingphysical space, integrating both continuous spatial and abstract spatialcomputations into a unified framework remains challenging. Here, we attempt tobridge this gap by proposing a mechanistic model for versatile informationprocessing in the entorhinal-hippocampal formation inspired by CANs and VectorSymbolic Architectures (VSAs), a neuro-symbolic computing framework. The novelgrid-cell VSA (GC-VSA) model employs a spatially structured encoding schemewith 3D neuronal modules mimicking the discrete scales and orientations of gridcell modules, reproducing their characteristic hexagonal receptive fields. Inexperiments, the model demonstrates versatility in spatial and abstract tasks:(1) accurate path integration for tracking locations, (2) spatio-temporalrepresentation for querying object locations and temporal relations, and (3)symbolic reasoning using family trees as a structured test case forhierarchical relationships.</description>
      <author>example@mail.com (Sven Krausse, Emre Neftci, Friedrich T. Sommer, Alpha Renner)</author>
      <guid isPermaLink="false">2503.08608v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in Open Environments</title>
      <link>http://arxiv.org/abs/2503.08604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;长期追求的自主家庭机器人通过自然语言控制的研究进展，随着大语言模型和具身智能的进步而更加接近。然而，在更复杂的机器人任务中缺乏统一基准、评估方法和指标不足以及大语言模型与移动操作轨迹数据不兼容等问题依然存在。&lt;h4&gt;背景&lt;/h4&gt;开发由自然语言控制的自主家庭机器人一直是人类追求的目标。尽管在大语言模型（LLMs）和具身智能方面取得了进展，但在更复杂的机器人任务中仍然存在挑战，包括缺乏统一基准、评估方法不足以及数据不兼容性问题。&lt;h4&gt;目的&lt;/h4&gt;为了应对这些挑战，研究者们引入了“开放环境中的具身移动操作”(EMMOE)，该模型要求代理能够理解用户指令并在连续空间执行长时间的日常任务，并将其与三种新的评估指标相结合以实现更全面的评价体系。此外，还收集了一项名为EMMOE-100的新数据集，包含各种任务属性、详细的流程注释以及失败后的重新规划。&lt;h4&gt;方法&lt;/h4&gt;设计了HomieBot智能代理系统，该系统结合了直接偏好优化（DPO）的大语言模型、轻量级导航和操作模型，并配备了多种错误检测机制。此外还评估了不同模型和策略的性能表现。&lt;h4&gt;主要发现&lt;/h4&gt;通过引入EMMOE框架以及HomieBot系统的开发，成功地解决了以往在复杂机器人任务评价中的不足问题，提高了模型处理实际家庭环境的能力。&lt;h4&gt;结论&lt;/h4&gt;通过对HomieBot的表现及其与其他方法对比，研究展示了其对于自主家庭机器人的潜力，并为未来的研究指明了方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing autonomous home robots controlled by natural language has longbeen a pursuit of human. While advancements in large language models (LLMs) andembodied intelligence make this goal closer, several challenges persist: thelack of a unified benchmark for more complex robot tasks, limited evaluationmethods and metrics, data incompatibility between LLMs and mobile manipulationtrajectories. To address these issues, we introduce Embodied MobileManipulation in Open Environments (EMMOE), which requires agents to interpretuser instructions and execute long-horizon everyday tasks in continuous space.EMMOE seamlessly integrates high-level and low-level embodied tasks into aunified framework, along with three new metrics for more diverse assessment.Additionally, we collect EMMOE-100, which features in various task attributes,detailed process annotations, re-plans after failures, and two sub-datasets forLLM training. Furthermore, we design HomieBot, a sophisticated agent systemconsists of LLM with Direct Preference Optimization (DPO), light weightednavigation and manipulation models, and multiple error detection mechanisms.Finally, we demonstrate HomieBot's performance and the evaluation of differentmodels and policies.</description>
      <author>example@mail.com (Dongping Li, Tielong Cai, Tianci Tang, Wenhao Chai, Katherine Rose Driggs-Campbell, Gaoang Wang)</author>
      <guid isPermaLink="false">2503.08604v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>A Framework for Reducing the Complexity of Geometric Vision Problems and its Application to Two-View Triangulation with Approximation Bounds</title>
      <link>http://arxiv.org/abs/2503.08142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新的框架，通过有针对性地重新调整成本函数来降低几何视觉问题的计算复杂性。&lt;h4&gt;背景&lt;/h4&gt;多视图几何和结构从运动（SfM）管道中的三角测量任务是在噪声2D投影中估计3D点的基本问题。&lt;h4&gt;目的&lt;/h4&gt;将提出的框架应用于双视角情况，展示了通过重新调整成本函数可以简化最优三角测量的过程，减少六次多项式到二次多项式的复杂性。&lt;h4&gt;方法&lt;/h4&gt;提出了最优的加权策略，并建立了近似误差的理论界限。同时提供了实验结果验证了与标准方法相比所提出的方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;重新调整成本函数可以将复杂的三角测量问题简化为闭合形式解，而不会影响几何精度。&lt;h4&gt;结论&lt;/h4&gt;尽管这项工作集中在双视角三角测量上，但框架可以推广到其他几何视觉问题中。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们提出了一种新的框架，通过有针对性地重新调整成本函数来降低几何视觉问题的计算复杂性。这种方法应用于两个视图的情况，并展示了最优三角测量（需要解六次多项式）可以通过重新调整成本函数简化为二次多项式的解决方法，同时保持强几何准确性。此外，还提出了理论上的近似误差界限，并通过实验证明了该方法的有效性。尽管这项工作主要集中在双视角三角测量上，但所提出的框架可以推广到其他几何视觉问题中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present a new framework for reducing the computationalcomplexity of geometric vision problems through targeted reweighting of thecost functions used to minimize reprojection errors. Triangulation - the taskof estimating a 3D point from noisy 2D projections across multiple images - isa fundamental problem in multiview geometry and Structure-from-Motion (SfM)pipelines. We apply our framework to the two-view case and demonstrate thatoptimal triangulation, which requires solving a univariate polynomial of degreesix, can be simplified through cost function reweighting reducing thepolynomial degree to two. This reweighting yields a closed-form solution whilepreserving strong geometric accuracy. We derive optimal weighting strategies,establish theoretical bounds on the approximation error, and provideexperimental results on real data demonstrating the effectiveness of theproposed approach compared to standard methods. Although this work focuses ontwo-view triangulation, the framework generalizes to other geometric visionproblems.</description>
      <author>example@mail.com (Felix Rydell, Georg Bökman, Fredrik Kahl, Kathlén Kohn)</author>
      <guid isPermaLink="false">2503.08142v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>DaD: Distilled Reinforcement Learning for Diverse Keypoint Detection</title>
      <link>http://arxiv.org/abs/2503.07347v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  fixed incorrect url&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种完全自监督且不需要描述符的基于强化学习的关键点检测新目标函数。&lt;h4&gt;背景&lt;/h4&gt;关键点是使多视图几何结构（SfM）系统能够处理数千张图像的基础。然而，由于SfM是非可微分的，设计合适的关键点检测目标函数是一个非平凡的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的、自监督且不依赖描述符的关键点检测方法，并改进现有的关键点检测性能。&lt;h4&gt;方法&lt;/h4&gt;利用强化学习提出了一种全新的、完全无描述符的关键点检测目标函数。为防止训练过程中出现退化现象，采用了平衡的top-K采样策略。进一步地，为了克服仅能识别亮或暗特征的问题，开发了一个新的探测器（DaD），该探测器优化了两个对立特性的最大值之间的KL散度。&lt;h4&gt;主要发现&lt;/h4&gt;新方法在多个基准测试中显著超越了现有最佳解决方案。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法不仅能够实现完全自监督的关键点检测，并且通过引入额外的探测器解决了仅能识别亮或暗特征的问题，从而提高了模型性能。&lt;h4&gt;翻译&lt;/h4&gt;关键点是使多视图几何结构（SfM）系统能够处理数千张图像的基础。然而，由于SfM是非可微分的，设计合适的关键点检测目标函数是一个非平凡的任务。传统方法通常依赖于描述符，但这种方法导致了对描述符的依赖性，这并不是理想的解决方案。本文提出了一种全新的、完全无描述符且基于强化学习的目标函数进行关键点检测。为了确保训练过程中的稳定性和有效性，我们采用了平衡的top-K采样策略。虽然这种策略已经可以生成具有竞争力的模型，但发现仅能识别亮或暗特征的两种不同类型的探测器分别出现。为解决这一问题，我们开发了一个额外的探测器（DaD），该探测器优化了两个对立特性的最大值之间的KL散度。我们的方法在多个基准测试中显著超越了现有最佳解决方案。代码和模型权重可通过 https://github.com/parskatt/dad 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Keypoints are what enable Structure-from-Motion (SfM) systems to scale tothousands of images. However, designing a keypoint detection objective is anon-trivial task, as SfM is non-differentiable. Typically, an auxiliaryobjective involving a descriptor is optimized. This however induces adependency on the descriptor, which is undesirable. In this paper we propose afully self-supervised and descriptor-free objective for keypoint detection,through reinforcement learning. To ensure training does not degenerate, weleverage a balanced top-K sampling strategy. While this already producescompetitive models, we find that two qualitatively different types of detectorsemerge, which are only able to detect light and dark keypoints respectively. Toremedy this, we train a third detector, DaD, that optimizes theKullback-Leibler divergence of the pointwise maximum of both light and darkdetectors. Our approach significantly improve upon SotA across a range ofbenchmarks. Code and model weights are publicly available athttps://github.com/parskatt/dad</description>
      <author>example@mail.com (Johan Edstedt, Georg Bökman, Mårten Wadenbäck, Michael Felsberg)</author>
      <guid isPermaLink="false">2503.07347v2</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>MoE-Loco: Mixture of Experts for Multitask Locomotion</title>
      <link>http://arxiv.org/abs/2503.08564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了MoE-Loco框架，这是一种用于四足和双足机器人多任务行走的专家混合模型。&lt;h4&gt;背景&lt;/h4&gt;现有的多任务强化学习方法在处理不同地形时容易出现梯度冲突问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架以解决多任务行走中的梯度冲突，并提高训练效率与性能。&lt;h4&gt;方法&lt;/h4&gt;使用了Mixture of Experts (MoE) 方法来支持单一策略应对多种复杂环境和步态（四足或双足）的需求。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，不同的专家在特定的运动行为上自然地表现出专业化特性，并且可以利用这些特性进行任务迁移和技能组合。&lt;h4&gt;结论&lt;/h4&gt;该方法不仅在模拟环境中表现良好，在实际部署中也显示出了强大的鲁棒性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了MoE-Loco，这是一个用于四足和双足机器人多任务行走的专家混合模型（Mixture of Experts, MoE）框架。我们的方法使得单一策略能够处理多种地形条件下的复杂环境要求，并支持不同的步态模式。通过使用MoE技术，我们解决了在多任务强化学习中常见的梯度冲突问题，从而提高了训练效率和性能表现。实验结果表明了不同专家模型对特定行走行为的自然专业化倾向，这些特性可以被利用来进行任务迁移以及技能组合操作。我们在仿真环境与真实场景下验证了该方法的有效性，并展示了其在适应性和鲁棒性方面的优越表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present MoE-Loco, a Mixture of Experts (MoE) framework for multitasklocomotion for legged robots. Our method enables a single policy to handlediverse terrains, including bars, pits, stairs, slopes, and baffles, whilesupporting quadrupedal and bipedal gaits. Using MoE, we mitigate the gradientconflicts that typically arise in multitask reinforcement learning, improvingboth training efficiency and performance. Our experiments demonstrate thatdifferent experts naturally specialize in distinct locomotion behaviors, whichcan be leveraged for task migration and skill composition. We further validateour approach in both simulation and real-world deployment, showcasing itsrobustness and adaptability.</description>
      <author>example@mail.com (Runhan Huang, Shaoting Zhu, Yilun Du, Hang Zhao)</author>
      <guid isPermaLink="false">2503.08564v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies</title>
      <link>http://arxiv.org/abs/2503.08558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FAIL-Detect是一种针对基于模仿学习的机器人操作系统的两阶段故障检测方法，它能够仅通过成功的训练数据准确地识别故障。&lt;h4&gt;背景&lt;/h4&gt;近年来，在模仿学习和生成模型（如扩散与流模型）的支持下，机器人操纵系统取得了显著进展。然而，随着政策性能的提高，实现的任务复杂性和时间范围也在增加，导致难以预测的各种失败模式出现。&lt;h4&gt;目的&lt;/h4&gt;为了在安全关键的人类环境中部署值得信赖的策略，在策略推理期间进行可靠的故障检测变得至关重要。&lt;h4&gt;方法&lt;/h4&gt;FAIL-Detect提出了一个模块化的两阶段方法来解决现有故障检测方法依赖于故障模式的先验知识和需要训练时的失败数据的问题。该方法通过将问题视为序列离群值（OOD）检测，从成功的训练数据中准确识别出政策失败的相关标量信号，并利用符合预测（CP）作为不确定性量化的一般框架。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明学习到的信号在处理多样化机器人操作任务时非常有效，特别是使用新颖的流密度估计器。此外，该方法比现有最先进的故障检测基线更快、更准确地检测到故障。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了FAIL-Detect增强基于模仿学习的机器人系统的安全性和可靠性以向现实世界部署迈进的潜力&lt;h4&gt;翻译&lt;/h4&gt;近年来，在模仿学习和生成模型（例如扩散模型和流模型）的支持下，见证了令人印象深刻的机器人操纵系统。随着机器人策略性能的提高，实现的任务复杂性、时间范围也随之增加，导致出现难以预测的各种故障模式。为了在安全关键的人类环境中部署值得信赖的政策，在策略推理期间进行可靠的故障检测变得至关重要。然而，大多数现有的故障检测方法依赖于对失败模式的先验知识，并且需要训练时的失败数据，这带来了实际应用和可扩展性的重要挑战。作为对此限制的回应，我们提出了FAIL-Detect，这是一种基于模仿学习的机器人操作中故障检测的模块化两阶段方法。为了仅通过成功的训练数据准确识别出故障，我们将该问题视为序列离群值（OOD）检测。首先，将策略输入和输出提炼成与政策失败相关并捕捉认识不确定性标量信号。然后，FAIL-Detect使用符合预测（CP）作为具有统计保证的不确定性量化通用框架。在实验中，我们深入调查了各种机器人操作任务中的学习后选择标量信号候选方案。我们的实验证明学习到的信号通常是最有效的，尤其是在使用我们新颖流密度估计器时。此外，该方法比现有最先进的故障检测基线更准确、更快地检测出故障。这些结果强调了FAIL-Detect增强基于模仿学习的机器人系统的安全性和可靠性以向现实世界部署迈进的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent years have witnessed impressive robotic manipulation systems driven byadvances in imitation learning and generative modeling, such as diffusion- andflow-based approaches. As robot policy performance increases, so does thecomplexity and time horizon of achievable tasks, inducing unexpected anddiverse failure modes that are difficult to predict a priori. To enabletrustworthy policy deployment in safety-critical human environments, reliableruntime failure detection becomes important during policy inference. However,most existing failure detection approaches rely on prior knowledge of failuremodes and require failure data during training, which imposes a significantchallenge in practicality and scalability. In response to these limitations, wepresent FAIL-Detect, a modular two-stage approach for failure detection inimitation learning-based robotic manipulation. To accurately identify failuresfrom successful training data alone, we frame the problem as sequentialout-of-distribution (OOD) detection. We first distill policy inputs and outputsinto scalar signals that correlate with policy failures and capture epistemicuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatileframework for uncertainty quantification with statistical guarantees.Empirically, we thoroughly investigate both learned and post-hoc scalar signalcandidates on diverse robotic manipulation tasks. Our experiments show learnedsignals to be mostly consistently effective, particularly when using our novelflow-based density estimator. Furthermore, our method detects failures moreaccurately and faster than state-of-the-art (SOTA) failure detection baselines.These results highlight the potential of FAIL-Detect to enhance the safety andreliability of imitation learning-based robotic systems as they progress towardreal-world deployment.</description>
      <author>example@mail.com (Chen Xu, Tony Khuong Nguyen, Emma Dixon, Christopher Rodriguez, Patrick Miller, Robert Lee, Paarth Shah, Rares Ambrus, Haruki Nishimura, Masha Itkina)</author>
      <guid isPermaLink="false">2503.08558v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>TLA: Tactile-Language-Action Model for Contact-Rich Manipulation</title>
      <link>http://arxiv.org/abs/2503.08548v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了Tactile-Language-Action (TLA)模型，该模型通过跨模态语言对齐有效处理连续的触觉反馈，以生成稳健的操作策略，特别是在接触密集型任务中。提出了一个包含24k组数据的数据集，用于指尖插孔装配中的触觉操作指令数据。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型取得了显著进展，但在涉及大量接触的机器人操作中，基于语言条件下的触觉感知研究相对较少。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，引入了Tactile-Language-Action (TLA)模型及其相应的数据集。&lt;h4&gt;方法&lt;/h4&gt;设计并构建了一个包含24k对触觉动作指令的数据集，用于指尖插孔装配场景，旨在为TLA的训练和评估提供重要资源。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与传统的模仿学习方法（如扩散策略）相比，TLA模型在生成有效操作和提高行动准确性方面表现更优，并且通过超过85%的成功率证明了其强大的泛化能力，即使面对未见过的装配间隙和插销形状。&lt;h4&gt;结论&lt;/h4&gt;发布了所有数据和代码以促进语言条件下的触觉操纵技能学习研究的发展。项目网站：https://sites.google.com/view/tactile-language-action/&lt;h4&gt;翻译&lt;/h4&gt;在视觉-语言模型取得进展的同时，对于涉及大量接触的机器人操作任务中的基于语言的触觉感知的研究仍然不足。为了解决这个问题，我们提出了一种新的Tactile-Language-Action (TLA) 模型，该模型通过将序列触觉反馈与跨模态的语言对齐来生成稳健的操作策略。此外，为了支持TLA模型的研发和评估，我们创建了一个包含24,000对数据的综合数据集，专注于指尖插孔装配中的操作指令信息。实验结果显示，TLA模型在有效动作生成、行动准确性方面都显著优于传统的模仿学习方法，并且通过超过85%的成功率展现了其出色的泛化能力，即使面对的是全新的装配间隙和插销形状组合。我们公开发布了所有数据集和代码，以推动该领域的进一步研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Significant progress has been made in vision-language models. However,language-conditioned robotic manipulation for contact-rich tasks remainsunderexplored, particularly in terms of tactile sensing. To address this gap,we introduce the Tactile-Language-Action (TLA) model, which effectivelyprocesses sequential tactile feedback via cross-modal language grounding toenable robust policy generation in contact-intensive scenarios. In addition, weconstruct a comprehensive dataset that contains 24k pairs of tactile actioninstruction data, customized for fingertip peg-in-hole assembly, providingessential resources for TLA training and evaluation. Our results show that TLAsignificantly outperforms traditional imitation learning methods (e.g.,diffusion policy) in terms of effective action generation and action accuracy,while demonstrating strong generalization capabilities by achieving over 85\%success rate on previously unseen assembly clearances and peg shapes. Wepublicly release all data and code in the hope of advancing research inlanguage-conditioned tactile manipulation skill learning. Project website:https://sites.google.com/view/tactile-language-action/</description>
      <author>example@mail.com (Peng Hao, Chaofan Zhang, Dingzhe Li, Xiaoge Cao, Xiaoshuai Hao, Shaowei Cui, Shuo Wang)</author>
      <guid isPermaLink="false">2503.08548v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Deformable Linear Object Surface Placement Using Elastica Planning and Local Shape Control</title>
      <link>http://arxiv.org/abs/2503.08545v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种处理在受限环境中放置可变形线性物体(DLO)任务的两层方法。该方法包括一个基于Euler弹性曲线解的新表面放置方法和用于形状估计及反馈控制的控制器。&lt;h4&gt;背景&lt;/h4&gt;在受限制的空间内操作可变形线性对象是一个具有挑战性的机器人学问题，特别是在精确位置摆放方面。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用单手抓取器将DLO准确地放置到平坦表面上的方法，并且能在任务执行中处理模型和放置误差。&lt;h4&gt;方法&lt;/h4&gt;采用两层策略：高层为基于Euler弹性曲线解的表面放置新方法；底层使用残差神经网络估计当前形状，通过低级反馈保证在存在建模和位置错误的情况下也能完成任务。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的DLO放置方案可以恢复由于高级别操作规划器失败而产生的状态，并且该方案已经在模拟和实验中进行了展示，包括为新鲜食品应用准备的硅质模型物体。&lt;h4&gt;结论&lt;/h4&gt;通过两层方法（高层的Euler弹性曲线解与低层的反馈控制）有效地解决了DLO放置问题，在实际应用中有良好的适应性和恢复能力。&lt;h4&gt;翻译&lt;/h4&gt;操作可变形线性对象(DLO)在受限环境中是一个挑战。本文介绍了一种使用单个机器人手部将DLO放在平坦表面上的方法，该方法包括一个基于Euler弹性曲线解的表面放置新方法和用于形状估计及反馈控制的控制器。这种方法可以在高级别操作规划器失败时恢复状态，并且在模拟和实验中进行了演示，这些实验使用了为新鲜食品应用准备的硅质模型物体进行验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Manipulation of deformable linear objects (DLOs) in constrained environmentsis a challenging task. This paper describes a two-layered approach for placingDLOs on a flat surface using a single robot hand. The high-level layer is anovel DLO surface placement method based on Euler's elastica solutions. Duringthis process one DLO endpoint is manipulated by the robot gripper while avariable interior point of the DLO serves as the start point of the portionaligned with the placement surface. The low-level layer forms a pipelinecontroller. The controller estimates the DLO current shape using a ResidualNeural Network (ResNet) and uses low-level feedback to ensure task execution inthe presence of modeling and placement errors. The resulting DLO placementapproach can recover from states where the high-level manipulation planner hasfailed as required by practical robot manipulation systems. The DLO placementapproach is demonstrated with simulations and experiments that use siliconmock-up objects prepared for fresh food applications.</description>
      <author>example@mail.com (I. Grinberg, A. Levin, E. D. Rimon)</author>
      <guid isPermaLink="false">2503.08545v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Deep Reinforcement Learning for Radio Tracer Localisation in Robotic-assisted Radioguided Surgery</title>
      <link>http://arxiv.org/abs/2503.08492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE International Conference on Robotics and Automation  (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于学习的方法，用于实现机器人辅助手术中的放射性示踪剂自主检测。通过结合深度强化学习和自适应机器人扫描技术，使探针能够导航至放射性目标。&lt;h4&gt;背景&lt;/h4&gt;放射引导手术依赖于非成像伽马/β探测器对放射性靶标的精确定位。手动基于视觉显示或音频指示的放射性靶标检测高度依赖于外科医生追踪和解释空间信息的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自动导航探针至放射性目标的方法，提高机器人辅助手术中的定位精度、减少对手术医师的操作依赖，并提升手术的一致性和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种结合深度强化学习（DRL）与自适应机器人扫描的新型混合方法。该方法利用基于网格的自适应扫描提供初始方向估计，而基于DRL的代理则能够有效地利用历史数据导航至目标。&lt;h4&gt;主要发现&lt;/h4&gt;仿真实验表明此方法的成功率为95%，相比传统技术在效率和鲁棒性方面有所提升；实验证明，在使用达芬奇研究套件(dVRK)进行放射性示踪剂检测时，成功率达到了80%。&lt;h4&gt;结论&lt;/h4&gt;该方法具有提高放射引导手术中定位的一致性和准确性、降低对手术医师依赖性的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译已经给出&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radioguided surgery, such as sentinel lymph node biopsy, relies on theprecise localization of radioactive targets by non-imaging gamma/betadetectors. Manual radioactive target detection based on visual display oraudible indication of gamma level is highly dependent on the ability of thesurgeon to track and interpret the spatial information. This paper presents alearning-based method to realize the autonomous radiotracer detection inrobot-assisted surgeries by navigating the probe to the radioactive target. Weproposed novel hybrid approach that combines deep reinforcement learning (DRL)with adaptive robotic scanning. The adaptive grid-based scanning could provideinitial direction estimation while the DRL-based agent could efficientlynavigate to the target utilising historical data. Simulation experimentsdemonstrate a 95% success rate, and improved efficiency and robustness comparedto conventional techniques. Real-world evaluation on the da Vinci Research Kit(dVRK) further confirms the feasibility of the approach, achieving an 80%success rate in radiotracer detection. This method has the potential to enhanceconsistency, reduce operator dependency, and improve procedural accuracy inradioguided surgeries.</description>
      <author>example@mail.com (Hanyi Zhang, Kaizhong Deng, Zhaoyang Jacopo Hu, Baoru Huang, Daniel S. Elson)</author>
      <guid isPermaLink="false">2503.08492v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Soft Actor-Critic-based Control Barrier Adaptation for Robust Autonomous Navigation in Unknown Environments</title>
      <link>http://arxiv.org/abs/2503.08479v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To Appear in 2025 IEEE/RSJ International Conference on Robotics and  Automation (ICRA), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于软演员评论家(Soft Actor-Critic, SAC)的策略，用于在运行时动态调整控制屏障函数(CBF)约束参数，以确保机器人在保证安全性的前提下实现高效导航。&lt;h4&gt;背景&lt;/h4&gt;自主导航过程中由于安全性限制设置不当（过于保守或不够严格），导致机器人出现死锁或者碰撞的问题十分常见。&lt;h4&gt;目的&lt;/h4&gt;为提高机器人的鲁棒性，在保障安全和性能之间寻求平衡，动态调整其安全约束条件以确保到达目标位置。&lt;h4&gt;方法&lt;/h4&gt;采用SAC算法来实时调节CBF参数，并设计了一个适用于通用高层运动规划器、低层控制器以及目标系统模型的框架。训练过程仅限于模拟环境。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的模拟实验和物理实验，证明了该框架能够有效调整CBF约束条件，使机器人在不牺牲安全性的情况下到达最终目的地。&lt;h4&gt;结论&lt;/h4&gt;所提出的动态适应性方法提高了机器人的运动规划能力，在保证安全的同时增强了其完成任务的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion planning failures during autonomous navigation often occur when safetyconstraints are either too conservative, leading to deadlocks, or too liberal,resulting in collisions. To improve robustness, a robot must dynamically adaptits safety constraints to ensure it reaches its goal while balancing safety andperformance measures. To this end, we propose a Soft Actor-Critic (SAC)-basedpolicy for adapting Control Barrier Function (CBF) constraint parameters atruntime, ensuring safe yet non-conservative motion. The proposed approach isdesigned for a general high-level motion planner, low-level controller, andtarget system model, and is trained in simulation only. Through extensivesimulations and physical experiments, we demonstrate that our frameworkeffectively adapts CBF constraints, enabling the robot to reach its final goalwithout compromising safety.</description>
      <author>example@mail.com (Nicholas Mohammad, Nicola Bezzo)</author>
      <guid isPermaLink="false">2503.08479v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Dynamic 3D Scene Graphs for Open-Vocabulary Urban Scene Understanding</title>
      <link>http://arxiv.org/abs/2503.08474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CURB-OSG是一种动态3D场景图引擎，通过多智能体协作生成城市驾驶场景的分层分解。&lt;h4&gt;背景&lt;/h4&gt;移动机器人在可靠规划和导航中需要地图绘制和场景表示。使用体积网格进行纯几何映射可以实现通用导航，但获取更新且语义丰富的空间表示以适应动态的大规模环境仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出CURB-OSG，该系统能够通过多智能体协作生成更准确的地图，并构建统一的开放词汇语义层次结构来解决这一问题。&lt;h4&gt;方法&lt;/h4&gt;融合来自多个感知代理（初始姿态未知）的摄像机和激光雷达观测数据。与以往依赖于地面实况智能体姿态或仅在模拟中评估的方法不同，CURB-OSG减轻了这些约束条件。&lt;h4&gt;主要发现&lt;/h4&gt;通过多智能体协作提高了映射和物体预测的准确性，并评估了该方法提出的环境分区能力。&lt;h4&gt;结论&lt;/h4&gt;为了促进进一步的研究，作者发布了他们的代码和补充材料。&lt;h4&gt;翻译&lt;/h4&gt;地图绘制和场景表示是移动机器人可靠规划与导航的基本要素。虽然纯几何地图（使用体积网格）允许通用导航，但在动态大规模环境中获取更新且语义丰富的空间表示仍然具有挑战性。在本文中，我们介绍了CURB-OSG，这是一个开放词汇的动态3D场景图引擎，通过多智能体协作生成城市驾驶场景的分层分解。该方法融合了来自多个感知代理（初始姿态未知）的摄像机和激光雷达观测数据，在生成更准确地图的同时构建统一的开放词汇语义层次结构。CURB-OSG不同于以往依赖于地面实况智能体姿态或仅在模拟中评估的方法，它减轻了这些约束条件。我们在真实世界多智能体传感器数据上对CURB-OSG的能力进行了评估，该数据来自牛津雷达机器人车数据集的多个会话。我们展示了通过多智能体协作提高了映射和物体预测准确性，并且还评估了所提出方法的环境分区能力。为了促进进一步的研究，我们在https://ov-curb.cs.uni-freiburg.de发布了我们的代码和补充材料。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mapping and scene representation are fundamental to reliable planning andnavigation in mobile robots. While purely geometric maps using voxel gridsallow for general navigation, obtaining up-to-date spatial and semanticallyrich representations that scale to dynamic large-scale environments remainschallenging. In this work, we present CURB-OSG, an open-vocabulary dynamic 3Dscene graph engine that generates hierarchical decompositions of urban drivingscenes via multi-agent collaboration. By fusing the camera and LiDARobservations from multiple perceiving agents with unknown initial poses, ourapproach generates more accurate maps compared to a single agent whileconstructing a unified open-vocabulary semantic hierarchy of the scene. Unlikeprevious methods that rely on ground truth agent poses or are evaluated purelyin simulation, CURB-OSG alleviates these constraints. We evaluate thecapabilities of CURB-OSG on real-world multi-agent sensor data obtained frommultiple sessions of the Oxford Radar RobotCar dataset. We demonstrate improvedmapping and object prediction accuracy through multi-agent collaboration aswell as evaluate the environment partitioning capabilities of the proposedapproach. To foster further research, we release our code and supplementarymaterial at https://ov-curb.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Tim Steinke, Martin Büchner, Niclas Vödisch, Abhinav Valada)</author>
      <guid isPermaLink="false">2503.08474v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Optimizing Ride-Pooling Operations with Extended Pickup and Drop-Off Flexibility</title>
      <link>http://arxiv.org/abs/2503.08472v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种新的匹配方法，该方法在乘客请求上下车的位置周围扩展了可接受的上下车区域范围。通过优化乘车共享服务中的配对问题（RMP），使得车辆能够在满足服务约束的同时，更有效地服务于更多的乘客。&lt;h4&gt;背景&lt;/h4&gt;目前解决RMP的方法通常假设所有乘客都在其原始位置等待接送，并忽视了乘客可能会走到附近地点以方便与车辆汇合的可能性。这种假设限制了乘车共享操作中的优化潜力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的匹配方法，通过允许乘客在靠近请求位置的区域内上下车来提高乘车服务效率。&lt;h4&gt;方法&lt;/h4&gt;设计了一种基于树的方法来高效生成乘客和车辆之间的可行配对，并通过优化路线以覆盖所有指定的上下车地点并最小化总行驶距离。同时采用动态分配策略实现最佳匹配结果。&lt;h4&gt;主要发现&lt;/h4&gt;在大规模出租车数据集上的实验表明，与现有的领先解决方案相比，该方法能够提高服务请求的数量最多13%，平均行程距离减少最多21%。&lt;h4&gt;结论&lt;/h4&gt;利用乘客的移动性来显著提升乘车共享服务效率具有巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Ride-Pool Matching Problem (RMP) is central to on-demand ride-poolingservices, where vehicles must be matched with multiple requests while adheringto service constraints such as pickup delays, detour limits, and vehiclecapacity. Most existing RMP solutions assume passengers are picked up anddropped off at their original locations, neglecting the potential forpassengers to walk to nearby spots to meet vehicles. This assumption restrictsthe optimization potential in ride-pooling operations. In this paper, wepropose a novel matching method that incorporates extended pickup and drop-offareas for passengers. We first design a tree-based approach to efficientlygenerate feasible matches between passengers and vehicles. Next, we optimizevehicle routes to cover all designated pickup and drop-off locations whileminimizing total travel distance. Finally, we employ dynamic assignmentstrategies to achieve optimal matching outcomes. Experiments on city-scale taxidatasets demonstrate that our method improves the number of served requests byup to 13\% and average travel distance by up to 21\% compared to leadingexisting solutions, underscoring the potential of leveraging passenger mobilityto significantly enhance ride-pooling service efficiency.</description>
      <author>example@mail.com (Hao Jiang, Yixing Xu, Pradeep Varakantham)</author>
      <guid isPermaLink="false">2503.08472v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Automatic Robotic-Assisted Diffuse Reflectance Spectroscopy Scanning System</title>
      <link>http://arxiv.org/abs/2503.08470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE International Conference on Robotics and Automation  (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个自动化的光学技术平台，利用漫反射光谱学（DRS）进行肿瘤检测并辅助手术中完整切除癌组织。&lt;h4&gt;背景&lt;/h4&gt;漫反射光谱学（DRS）是一种广泛应用于评估组织成分的光学技术，并已用于临床以确保完全清除癌症组织。&lt;h4&gt;目的&lt;/h4&gt;开发一个机器人系统，通过混合视觉伺服控制自动完成大范围扫描，提高一致性并减少医生手动操作的需求。&lt;h4&gt;方法&lt;/h4&gt;设计了一个特殊的高度补偿模块来实现精确接触条件控制。该系统可以准确执行扫描命令，并获得与手动收集一致的DRS光谱。&lt;h4&gt;主要发现&lt;/h4&gt;该机器人系统能够有效地模拟当前金标准的手动采集过程，显示出高可靠性和重复性。&lt;h4&gt;结论&lt;/h4&gt;集成该系统的手术可为自动化的术中DRS组织评估奠定基础，减少医生在手术中的手动扫描需求，并确保临床实践中肿瘤的完全切除。&lt;h4&gt;翻译&lt;/h4&gt;Diffuse Reflectance Spectroscopy (DRS)是一种用于组织成分评估的光学技术，在临床上已被用于检测肿瘤并保证彻底清除癌细胞。虽然点状评估具有许多潜在应用，但将自动大范围扫描结合进来可以实现整体组织取样，并提高一致性。我们提出了一种机器人系统，通过混合视觉伺服控制来促进自主DRS扫描。该系统能够准确执行扫描命令，获得与手动收集相比较一致的DRS光谱。这为手术中具有高可靠性和重复性的自动DRS组织评估奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffuse Reflectance Spectroscopy (DRS) is a well-established opticaltechnique for tissue composition assessment which has been clinically evaluatedfor tumour detection to ensure the complete removal of cancerous tissue. Whilepoint-wise assessment has many potential applications, incorporating automatedlarge-area scanning would enable holistic tissue sampling with higherconsistency. We propose a robotic system to facilitate autonomous DRS scanningwith hybrid visual servoing control. A specially designed height compensationmodule enables precise contact condition control. The evaluation results showthat the system can accurately execute the scanning command and acquireconsistent DRS spectra with comparable results to the manual collection, whichis the current gold standard protocol. Integrating the proposed system intosurgery lays the groundwork for autonomous intra-operative DRS tissueassessment with high reliability and repeatability. This could reduce the needfor manual scanning by the surgeon while ensuring complete tumor removal inclinical practice.</description>
      <author>example@mail.com (Kaizhong Deng, Christopher J. Peters, George P. Mylonas, Daniel S. Elson)</author>
      <guid isPermaLink="false">2503.08470v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>ICPR 2024 Competition on Rider Intention Prediction</title>
      <link>http://arxiv.org/abs/2503.08437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近期车辆市场的快速增长导致道路事故大幅增加，这突显了提升交通安全措施的重要性，尤其是对于摩托车骑行者等弱势道路使用者。为此，引入了一个旨在通过预测骑手操作前的行为来加强安全性的骑行意图预测（RIP）竞赛。&lt;h4&gt;背景&lt;/h4&gt;随着汽车市场的增长，道路安全事故频发，特别是对摩托车驾驶员来说更加危险。&lt;h4&gt;目的&lt;/h4&gt;通过预测骑行者的潜在错误行为来增强其安全性，并为此推出了一个新的数据集RAAD和RIP挑战赛。&lt;h4&gt;方法&lt;/h4&gt;收集了一个新的名为RAAD的数据集，用于单视角RIP任务和多视角RIP任务的竞赛。该数据集涵盖了各种交通状况及光线条件下的复杂导航操作。&lt;h4&gt;主要发现&lt;/h4&gt;通过对比75个注册团队中的五支队伍的表现，结果表明状态空间模型（Mamba2）在整个数据集中表现最佳，而基于支持向量机（SVM）的方法在使用随机采样和SMOTE方法时表现出色，但卷积神经网络-长短期记忆网络（CNN-LSTM）方法由于类别不平衡问题，在处理少数类别的案例时效果较差。&lt;h4&gt;结论&lt;/h4&gt;RAAD数据集的提出为骑行意图预测挑战赛提供了重要的基础，并展示了状态空间模型在解决此类安全问题上的有效性。&lt;h4&gt;翻译&lt;/h4&gt;近期车辆市场的快速增长导致道路事故大幅增加，这突显了提升交通安全措施的重要性，尤其是对于摩托车驾驶员等弱势道路使用者。为此，引入了一个旨在通过预测骑手操作前的行为来增强安全性的骑行意图预测（RIP）竞赛。该比赛收集了一个新的名为RAAD的数据集，用于单视角和多视角的骑行意图预测任务，并涵盖了各种交通状况及光线条件下的复杂导航操作。比赛中共收到75个注册团队中的五支队伍提交的结果显示，状态空间模型在处理整个数据集中表现出色，而基于支持向量机的方法通过使用随机采样和SMOTE方法提高了性能，但卷积神经网络-长短期记忆网络方法由于类别不平衡问题表现不佳。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-80139-6_3&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent surge in the vehicle market has led to an alarming increase inroad accidents. This underscores the critical importance of enhancing roadsafety measures, particularly for vulnerable road users like motorcyclists.Hence, we introduce the rider intention prediction (RIP) competition that aimsto address challenges in rider safety by proactively predicting maneuversbefore they occur, thereby strengthening rider safety. This capability enablesthe riders to react to the potential incorrect maneuvers flagged by advanceddriver assistance systems (ADAS). We collect a new dataset, namely, rideraction anticipation dataset (RAAD) for the competition consisting of two tasks:single-view RIP and multi-view RIP. The dataset incorporates a spectrum oftraffic conditions and challenging navigational maneuvers on roads with varyinglighting conditions. For the competition, we received seventy-fiveregistrations and five team submissions for inference of which we compared themethods of the top three performing teams on both the RIP tasks: onestate-space model (Mamba2) and two learning-based approaches (SVM andCNN-LSTM). The results indicate that the state-space model outperformed theother methods across the entire dataset, providing a balanced performanceacross maneuver classes. The SVM-based RIP method showed the second-bestperformance when using random sampling and SMOTE. However, the CNN-LSTM methodunderperformed, primarily due to class imbalance issues, particularlystruggling with minority classes. This paper details the proposed RAAD datasetand provides a summary of the submissions for the RIP 2024 competition.</description>
      <author>example@mail.com (Shankar Gangisetty, Abdul Wasi, Shyam Nandan Rai, C. V. Jawahar, Sajay Raj, Manish Prajapati, Ayesha Choudhary, Aaryadev Chandra, Dev Chandan, Shireen Chand, Suvaditya Mukherjee)</author>
      <guid isPermaLink="false">2503.08437v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>($\boldsymbolθ_l, \boldsymbolθ_u$)-Parametric Multi-Task Optimization: Joint Search in Solution and Infinite Task Spaces</title>
      <link>http://arxiv.org/abs/2503.08394v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;本文研究了参数化多任务优化（PMTO），该问题设置放松了传统固定且有限数量的优化任务限制，允许非固定的、可能无限的任务集在限定的连续和有界参数空间内定义。&lt;h4&gt;背景&lt;/h4&gt;传统的多任务优化假设存在一个固定的、有限数量的优化任务集合。然而，在许多实际应用中，任务的数量可能是不确定的或者随着环境变化而改变。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的算法以解决非固定且可能无限的任务集在限定参数空间内的联合搜索问题，并展示其在机器人控制器快速重组及鲁棒工程设计中的潜在应用价值。&lt;h4&gt;方法&lt;/h4&gt;提出了一个新颖的($oldsymbol{heta}_l$,$oldsymbol{heta}_u$)-PMTO算法，该算法通过两个近似模型支持任务与解决方案之间的联合搜索：一是映射所有任务目标空间中解的方法，二是基于概率的任务到解空间映射方法。这些模型共同加速了不同任务间的知识转移和探索。&lt;h4&gt;主要发现&lt;/h4&gt;提出的($oldsymbol{heta}_l$,$oldsymbol{heta}_u$)-PMTO算法在合成测试问题和实际案例研究中均显示出优越性能，特别是在机器人控制器的快速重组及鲁棒工程设计中的最小最大优化问题上具有显著效果。&lt;h4&gt;结论&lt;/h4&gt;参数化多任务优化提供了一种处理非固定数量、可能无限的任务集的有效方法，在快速配置适应性变化环境下的系统（如机器人控制器）方面表现出巨大的应用潜力。此外，PMTO还为解决复杂的鲁棒设计和最小最大优化问题提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了针对参数化多任务优化的研究工作，这种设置允许处理非固定的、可能无限的任务集，并提出了一种新颖的算法来加速跨不同任务的知识转移和探索过程。该研究通过合成测试和实际案例验证了所提方法的有效性，并展示了其在快速重组机器人控制器及解决鲁棒工程设计中的最小最大优化问题上的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-task optimization is typically characterized by a fixed and finite setof optimization tasks. The present paper relaxes this condition by consideringa non-fixed and potentially infinite set of optimization tasks defined in aparameterized, continuous and bounded task space. We refer to this uniqueproblem setting as parametric multi-task optimization (PMTO). Assuming thebounds of the task parameters to be ($\boldsymbol{\theta}_l$,$\boldsymbol{\theta}_u$), a novel ($\boldsymbol{\theta}_l$,$\boldsymbol{\theta}_u$)-PMTO algorithm is crafted to enable joint search overtasks and their solutions. This joint search is supported by two approximationmodels: (1) for mapping solutions to the objective spaces of all tasks, whichprovably accelerates convergence by acting as a conduit for inter-taskknowledge transfers, and (2) for probabilistically mapping tasks to thesolution space, which facilitates evolutionary exploration of under-exploredregions of the task space. At the end of a full ($\boldsymbol{\theta}_l$,$\boldsymbol{\theta}_u$)-PMTO run, the acquired models enable rapididentification of optimized solutions for any task lying within the specifiedbounds. This outcome is validated on both synthetic test problems and practicalcase studies, with the significant real-world applicability of PMTO showntowards fast reconfiguration of robot controllers under changing taskconditions. The potential of PMTO to vastly speedup the search for solutions tominimax optimization problems is also demonstrated through an example in robustengineering design.</description>
      <author>example@mail.com (Tingyang Wei, Jiao Liu, Abhishek Gupta, Puay Siew Tan, Yew-Soon Ong)</author>
      <guid isPermaLink="false">2503.08394v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>V-Max: Making RL practical for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2503.08388v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文介绍了一种名为V-Max的开放研究框架，旨在使强化学习（RL）在自主驾驶（AD）中的应用更加实际。&lt;h4&gt;背景信息&lt;/h4&gt;基于学习的方法有潜力实现通用化的自动驾驶策略，模仿学习(IL)依赖大规模的人类演示数据集而占据主导地位，但存在分布变化和模仿差距等问题。相比之下，虽然强化学习(RL)作为替代方案很有前景，但在自动驾驶领域的应用仍受到标准化和高效研究框架缺乏的限制。&lt;h4&gt;目的&lt;/h4&gt;为了使RL在AD中更加实用，开发了一种新的开放研究框架V-Max，它包括所有必要的工具以支持大规模实验以及多样化的数据集快速仿真。&lt;h4&gt;方法&lt;/h4&gt;V-Max建立于Waymax上，这是一个硬件加速的AD模拟器。该框架扩展了ScenarioNet的方法来实现多样的AD数据集的快速仿真，并且整合了一系列观察和奖励函数、基于Transformer的编码器及训练管道等。&lt;h4&gt;主要发现&lt;/h4&gt;通过大规模基准测试分析了网络架构、观测功能、训练数据以及奖赏形塑等因素如何影响RL性能。&lt;h4&gt;结论&lt;/h4&gt;V-Max为研究自动驾驶中的强化学习提供了一个强大的框架，有助于克服现有技术在实现更高效和通用的AD策略方面的限制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning-based decision-making has the potential to enable generalizableAutonomous Driving (AD) policies, reducing the engineering overhead ofrule-based approaches. Imitation Learning (IL) remains the dominant paradigm,benefiting from large-scale human demonstration datasets, but it suffers frominherent limitations such as distribution shift and imitation gaps.Reinforcement Learning (RL) presents a promising alternative, yet its adoptionin AD remains limited due to the lack of standardized and efficient researchframeworks. To this end, we introduce V-Max, an open research frameworkproviding all the necessary tools to make RL practical for AD. V-Max is builton Waymax, a hardware-accelerated AD simulator designed for large-scaleexperimentation. We extend it using ScenarioNet's approach, enabling the fastsimulation of diverse AD datasets. V-Max integrates a set of observation andreward functions, transformer-based encoders, and training pipelines.Additionally, it includes adversarial evaluation settings and an extensive setof evaluation metrics. Through a large-scale benchmark, we analyze how networkarchitectures, observation functions, training data, and reward shaping impactRL performance.</description>
      <author>example@mail.com (Valentin Charraut, Thomas Tournaire, Waël Doulazmi, Thibault Buhet)</author>
      <guid isPermaLink="false">2503.08388v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Hydrodynamic Interaction and Geometric Memory Effect Drive Directed Swimming of Chlamydomonas reinhardtii near Periodic Microstructures</title>
      <link>http://arxiv.org/abs/2503.08376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了C. reinhardtii在具有周期性微结构环境中的游泳行为，并发现了其聚集和移动模式的新特征。&lt;h4&gt;背景&lt;/h4&gt;微生物在固液界面附近的运动是科学研究的重要领域，尤其在生物膜形成及海洋生物污染等自然和工业情境中意义重大。&lt;h4&gt;目的&lt;/h4&gt;探究C. reinhardtii在周期性微结构（SPM）环境中的游泳行为及其机制。&lt;h4&gt;方法&lt;/h4&gt;使用荧光显微镜和三维跟踪技术观察并记录了C. reinhardtii的运动轨迹及方向变化。&lt;h4&gt;主要发现&lt;/h4&gt;细胞倾向于聚集于SPM底部而不是顶部，并且它们在接近微结构时表现出速度取向的趋势。这一行为归因于“记忆效应”和水动力吸引的综合作用。&lt;h4&gt;结论&lt;/h4&gt;通过改变周期性微结构的形状，可以成功地诱导细胞定向游泳，这为微型纳米机器人的控制以及生物污损预防提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;微生物在固液界面附近的运动是科学研究的重要领域。这项研究探讨了C. reinhardtii在具有周期性微结构（SPM）环境中的行为，并观察到其受到几何约束影响的游泳方向变化，细胞聚集于底部以及速度取向等现象。研究表明这一行为与‘记忆效应’和水动力吸引有关。通过改变微结构的设计，可以实现对微生物运动的有效控制，这在微型纳米机器人控制及生物污损预防等领域具有潜在应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The movement of microorganisms near solid-liquid interfaces is a topic ofsignificant scientific interest due to its relevance in various natural andindustrial contexts, such as biofilm formation and marine biofouling. In thisstudy, we investigate the swimming behavior of C. reinhardtii near a sinusoidalperiodic microstructure (SPM). Using fluorescence microscopy andthree-dimensional tracking, we observe that the swimming direction of C.reinhardtii is strongly influenced by the geometric constraints of the SPM. Ourresults show that cells tend to aggregate at the bottom of the SPM rather thanthe top, and exhibit a speed orientation tendency near the microstructure. Weattribute this behavior to a combination of the "memory effect" andhydrodynamic attraction. By altering the shape of the periodic microstructure,we successfully achieve directed induction of cell swimming, which haspotential applications in micro-nano robot control and biofouling prevention.This work provides new insights into the movement mechanisms of microorganismsnear solid-liquid surfaces and highlights the potential for manipulating theirbehavior through microstructure design.</description>
      <author>example@mail.com (Chunhe Li, Hongyi Bian, Zixiang Lin, Yi Man, Zijie Qu)</author>
      <guid isPermaLink="false">2503.08376v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Gait in Eight: Efficient On-Robot Learning for Omnidirectional Quadruped Locomotion</title>
      <link>http://arxiv.org/abs/2503.08375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种框架，利用新型离策算法CrossQ，在8分钟的实时训练时间内高效地学习四足机器人全向运动。&lt;h4&gt;背景&lt;/h4&gt;基于真实时间约束下机器人的计算限制问题，作者研究了如何在实际设备上使用强化学习来训练感知身体特征的策略。现有的工作主要集中在学习简单的直线步态上。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架，在最短的时间内实现四足机器人全向运动的学习。&lt;h4&gt;方法&lt;/h4&gt;利用新型离策算法CrossQ，针对两种控制架构：预测关节目标位置以实现敏捷、高速度的移动和中央模式生成器以保持稳定、自然的步伐。进行实验验证所提框架的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;提出的框架能够在不同室内和室外环境中展现出良好的鲁棒性，且能够扩展到四足机器人的全向运动学习上。&lt;h4&gt;结论&lt;/h4&gt;通过高效的学习过程，在最短的时间内实现了机器人复杂动作的训练，为解决实际中强化学习面临的计算资源限制问题提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;在-机器人强化学习是用于训练感知身体特征策略的一种有前途的方法。然而，真实时间上的机器人的计算约束构成了重大挑战。我们提出了一种框架，在仅8分钟的真实时间训练下利用新离策算法CrossQ的采样效率和最小化计算开销来高效地学习四足运动。我们研究了两种控制架构：预测关节目标位置以实现敏捷、高速度的移动以及中央模式生成器以保持稳定、自然的步伐。先前的工作集中在学习简单的直线步态，而我们的框架则扩展到四足机器人全向运动的学习上。我们在不同室内和室外环境中展示了该方法的强大适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; On-robot Reinforcement Learning is a promising approach to trainembodiment-aware policies for legged robots. However, the computationalconstraints of real-time learning on robots pose a significant challenge. Wepresent a framework for efficiently learning quadruped locomotion in just 8minutes of raw real-time training utilizing the sample efficiency and minimalcomputational overhead of the new off-policy algorithm CrossQ. We investigatetwo control architectures: Predicting joint target positions for agile,high-speed locomotion and Central Pattern Generators for stable, natural gaits.While prior work focused on learning simple forward gaits, our frameworkextends on-robot learning to omnidirectional locomotion. We demonstrate therobustness of our approach in different indoor and outdoor environments.</description>
      <author>example@mail.com (Nico Bohlinger, Jonathan Kinzel, Daniel Palenicek, Lukasz Antczak, Jan Peters)</author>
      <guid isPermaLink="false">2503.08375v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>DG16M: A Large-Scale Dataset for Dual-Arm Grasping with Force-Optimized Grasps</title>
      <link>http://arxiv.org/abs/2503.08358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了针对双臂抓取的大规模数据集，并开发了一个基准测试集，用于评估双臂抓取的质量。通过训练一个双臂抓取分类器网络展示了所提出方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;现有的研究主要集中在单臂抓取上，而专门适用于双臂设置的数据集仍然非常有限。&lt;h4&gt;目的&lt;/h4&gt;构建一个大规模的双臂抓取数据集，并开发一个新的基准测试集以评估双臂抓取的质量和方法性能。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含1600万对双臂抓取的大规模数据集，以及一个包含300个对象、每个对象约有100种不同抓取方式的基准测试集。利用物理模拟环境进行评估，并开发了一种新的双臂抓取分类器网络。&lt;h4&gt;主要发现&lt;/h4&gt;新提出的双臂抓取分类器网络，在评估中超越了现有的最先进方法，成功率提高了15%，并且在各种物体上都表现出更好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的数据集和基准测试为改进的双臂机器人抓取提供了重要的资源，同时验证了新的机器学习模型的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dual-arm robotic grasping is crucial for handling large objects that requirestable and coordinated manipulation. While single-arm grasping has beenextensively studied, datasets tailored for dual-arm settings remain scarce. Weintroduce a large-scale dataset of 16 million dual-arm grasps, evaluated underimproved force-closure constraints. Additionally, we develop a benchmarkdataset containing 300 objects with approximately 30,000 grasps, evaluated in aphysics simulation environment, providing a better grasp quality assessment fordual-arm grasp synthesis methods. Finally, we demonstrate the effectiveness ofour dataset by training a Dual-Arm Grasp Classifier network that outperformsthe state-of-the-art methods by 15\%, achieving higher grasp success rates andimproved generalization across objects.</description>
      <author>example@mail.com (Md Faizal Karim, Mohammed Saad Hashmi, Shreya Bollimuntha, Mahesh Reddy Tapeti, Gaurav Singh, Nagamanikandan Govindan, K Madhava Krishna)</author>
      <guid isPermaLink="false">2503.08358v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>LiPS: Large-Scale Humanoid Robot Reinforcement Learning with Parallel-Series Structures</title>
      <link>http://arxiv.org/abs/2503.08349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近年来，关于类人机器人研究取得了显著进展，特别是在基于强化学习的控制算法领域。此类算法在处理复杂任务方面表现出明显优势。&lt;h4&gt;背景&lt;/h4&gt;目前基于GPU的大规模并行计算能力使得类人机器人的模拟环境训练成为可能，物理仿真平台对类人机器人发展至关重要。然而现有大多数基于强化学习的类人机器人控制算法使用开环拓扑结构进行训练，直到sim2real阶段才转换为串并联结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的训练方法LiPS，以解决当前GPU基础物理引擎只能支持开环拓扑或模拟多刚体闭环拓扑能力有限的问题。&lt;h4&gt;方法&lt;/h4&gt;通过在仿真环境中引入多刚体力学建模，显著减少了sim2real差距和模型部署过程中转换到并联结构的难度。&lt;h4&gt;主要发现&lt;/h4&gt;该方法成功地使基于强化学习的类人机器人控制算法能够在大规模平行环境中训练，并增强了其鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;LiPS为类人机器人的强化学习提供了一种新的途径，有助于克服现有物理引擎技术限制，提升类人机器人的开发效率和实用性。&lt;h4&gt;翻译&lt;/h4&gt;近年来，关于类人机器人研究取得了显著进展，特别是在基于强化学习的控制算法领域。此类算法在处理复杂任务方面表现出明显优势。目前基于GPU的大规模并行计算能力使得类人机器人的模拟环境训练成为可能，物理仿真平台对类人机器人发展至关重要。然而现有大多数基于强化学习的类人机器人控制算法使用开环拓扑结构进行训练，直到sim2real阶段才转换为串并联结构。我们提出一种新的训练方法LiPS，通过在仿真环境中引入多刚体力学建模，显著减少了sim2real差距和模型部署过程中转换到并联结构的难度，从而成功地使基于强化学习的类人机器人控制算法能够在大规模平行环境中训练，并增强了其鲁棒性。该方法有助于克服现有物理引擎技术限制，提升类人机器人的开发效率和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, research on humanoid robots has garnered significantattention, particularly in reinforcement learning based control algorithms,which have achieved major breakthroughs. Compared to traditional model-basedcontrol algorithms, reinforcement learning based algorithms demonstratesubstantial advantages in handling complex tasks. Leveraging the large-scaleparallel computing capabilities of GPUs, contemporary humanoid robots canundergo extensive parallel training in simulated environments. A physicalsimulation platform capable of large-scale parallel training is crucial for thedevelopment of humanoid robots. As one of the most complex robot forms,humanoid robots typically possess intricate mechanical structures, encompassingnumerous series and parallel mechanisms. However, many reinforcement learningbased humanoid robot control algorithms currently employ open-loop topologiesduring training, deferring the conversion to series-parallel structures untilthe sim2real phase. This approach is primarily due to the limitations ofphysics engines, as current GPU-based physics engines often only supportopen-loop topologies or have limited capabilities in simulatingmulti-rigid-body closed-loop topologies. For enabling reinforcementlearning-based humanoid robot control algorithms to train in large-scaleparallel environments, we propose a novel training method LiPS. Byincorporating multi-rigid-body dynamics modeling in the simulation environment,we significantly reduce the sim2real gap and the difficulty of converting toparallel structures during model deployment, thereby robustly supportinglarge-scale reinforcement learning for humanoid robots.</description>
      <author>example@mail.com (Qiang Zhang, Gang Han, Jingkai Sun, Wen Zhao, Jiahang Cao, Jiaxu Wang, Hao Cheng, Lingfeng Zhang, Yijie Guo, Renjing Xu)</author>
      <guid isPermaLink="false">2503.08349v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding in Egocentric Videos</title>
      <link>http://arxiv.org/abs/2503.08344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;环境理解在自视角视频中的重要性及其应用领域，提出了一种新的框架DIV-FF来改善现有方法的局限。&lt;h4&gt;背景&lt;/h4&gt;自视角视频（egocentric videos）因其动态交互和对佩戴者与环境互动的高度依赖而被用于机器人技术、增强现实和辅助技术等领域。传统的方法往往只关注孤立片段或者无法整合丰富的语义和几何信息，从而限制了场景的理解。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架以改善自视角视频中的场景理解能力，特别是在长期时空场景理解方面。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为动态图像-视频特征场（Dynamic Image-Video Feature Fields, DIV FF）的框架。该模型将自视图场景分解为持久、动态和基于演员的部分，并同时整合了图像和视频语言特性。&lt;h4&gt;主要发现&lt;/h4&gt;DIV-FF在具有高动态变化的情景中表现出色，超过了现有的最佳方法。&lt;h4&gt;结论&lt;/h4&gt;提出的DIV-FF框架通过综合考虑时空信息提高了对自视图场景的理解能力，显示出了在长期、时空理解中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;环境理解在诸如机器人技术、增强现实和辅助技术等应用领域是至关重要的步骤。传统的研究方法往往集中在孤立的视频片段上，或者无法整合丰富的语义和几何信息，从而限制了对场景的整体理解。提出了一种新的框架DIV-FF，该框架将自视图场景分解为持久性部分、动态部分以及基于演员的部分，并同时结合了图像和视频的语言特性。实验结果表明，相比现有方法，特别是在快速变化的环境中，DIV-FF具有更好的性能，展示了其在长期时空场景理解中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Environment understanding in egocentric videos is an important step forapplications like robotics, augmented reality and assistive technologies. Thesevideos are characterized by dynamic interactions and a strong dependence on thewearer engagement with the environment. Traditional approaches often focus onisolated clips or fail to integrate rich semantic and geometric information,limiting scene comprehension. We introduce Dynamic Image-Video Feature Fields(DIV FF), a framework that decomposes the egocentric scene into persistent,dynamic, and actor based components while integrating both image and videolanguage features. Our model enables detailed segmentation, capturesaffordances, understands the surroundings and maintains consistentunderstanding over time. DIV-FF outperforms state-of-the-art methods,particularly in dynamically evolving scenarios, demonstrating its potential toadvance long term, spatio temporal scene understanding.</description>
      <author>example@mail.com (Lorenzo Mur-Labadia, Josechu Guerrero, Ruben Martinez-Cantin)</author>
      <guid isPermaLink="false">2503.08344v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Trinity: A Modular Humanoid Robot AI System</title>
      <link>http://arxiv.org/abs/2503.08338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近年来，人形机器人的研究越来越受到关注。随着各种人工智能算法的突破，以人形机器人为代表的具身智能备受期待。&lt;h4&gt;背景&lt;/h4&gt;强化学习（RL）算法的进步显著改善了人形机器人的运动控制和泛化能力；大型语言模型（LLM）和视觉语言模型（VLM）的重大进展为机器人带来了更多可能。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新型AI系统Trinity，该系统将强化学习、大型语言模型和视觉语言模型整合在一起，以在复杂环境中实现人形机器人的高效控制。&lt;h4&gt;方法&lt;/h4&gt;通过结合RL、LLM和VLM技术，Trinity使人形机器人能够根据自然语言指令理解复杂的任务，并进行长期的任务规划；同时增强机器人对环境的理解和互动能力。&lt;h4&gt;主要发现&lt;/h4&gt;Trinity系统的开发不仅增强了人形机器人的功能，还为未来的研究和应用开辟了新的途径。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了将RL、LLM和VLM技术结合的创新方法在人形机器人领域的潜力，对未来的人工智能发展具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;最近几年，有关人形机器人的研究引起了越来越多的关注。随着各种人工智能算法的突破，以人形机器人形式存在的具身智能得到了极大的期待。强化学习（RL）算法的进步显著提高了人形机器人的运动控制和泛化能力。与此同时，在大型语言模型（LLM）和视觉语言模型（VLM）方面的开创性进展为人形机器人带来了更多可能与想象空间。LLM使得人形机器人能够理解自然语言指令中的复杂任务，并进行长期的任务规划；而VLM极大地增强了机器人的环境理解和交互能力。本文介绍了一种新型AI系统Trinity，该系统整合了RL、LLM和VLM技术，从而使人形机器人在复杂环境中实现高效的控制。这种创新方法不仅提升了人形机器人的功能，还开启了未来研究与应用的新领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, research on humanoid robots has garnered increasingattention. With breakthroughs in various types of artificial intelligencealgorithms, embodied intelligence, exemplified by humanoid robots, has beenhighly anticipated. The advancements in reinforcement learning (RL) algorithmshave significantly improved the motion control and generalization capabilitiesof humanoid robots. Simultaneously, the groundbreaking progress in largelanguage models (LLM) and visual language models (VLM) has brought morepossibilities and imagination to humanoid robots. LLM enables humanoid robotsto understand complex tasks from language instructions and perform long-termtask planning, while VLM greatly enhances the robots' understanding andinteraction with their environment. This paper introduces\textcolor{magenta}{Trinity}, a novel AI system for humanoid robots thatintegrates RL, LLM, and VLM. By combining these technologies, Trinity enablesefficient control of humanoid robots in complex environments. This innovativeapproach not only enhances the capabilities but also opens new avenues forfuture research and applications of humanoid robotics.</description>
      <author>example@mail.com (Jingkai Sun, Qiang Zhang, Gang Han, Wen Zhao, Zhe Yong, Yan He, Jiaxu Wang, Jiahang Cao, Yijie Guo, Renjing Xu)</author>
      <guid isPermaLink="false">2503.08338v1</guid>
      <pubDate>Wed, 12 Mar 2025 18:44:10 +0800</pubDate>
    </item>
    <item>
      <title>Diff-Reg v2: Diffusion-Based Matching Matrix Estimation for Image Matching and 3D Registration</title>
      <link>http://arxiv.org/abs/2503.04127v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2403.19919&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的匹配矩阵估计方法，该方法利用扩散模型在矩阵空间中处理注册任务中的模糊对应问题。&lt;h4&gt;背景&lt;/h4&gt;建立可靠的对应关系是图像注册、点云注册和2D-3D图像到点云注册等任务的关键。然而，这些任务常常因比例不一致、对称性和大形变等问题而变得复杂，导致匹配困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决现有特征和对应方法在处理挑战性情况时的局限性，并克服单步预测头模型在复杂场景中陷入局部最优的问题。&lt;h4&gt;方法&lt;/h4&gt;采用扩散模型在矩阵空间进行稳健的匹配矩阵估计。该模型将对应关系估计视为匹配矩阵空间中的去噪扩散过程，逐步细化中间矩阵到最佳状态。对于不同的注册任务采用了适应性的匹配矩阵嵌入实现。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种新的范式来处理匹配问题，并且在2D和3D数据的多种注册任务中表现出色。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一种新颖而有效的方法，可以用于解决图像和点云注册中的复杂对应关系问题。&lt;h4&gt;翻译&lt;/h4&gt;建立可靠的关系对于所有登记任务都至关重要，包括二维图像登记、三维点云登记以及二维-三维图像到点云的登记。然而，这些任务经常由于比例不一致、对称性和大幅度形变等挑战而变得复杂，导致匹配模糊不清。先前基于特征的方法和对应方法通常依赖于几何或语义特征来生成或改进初始潜在的对应关系。此外，许多以前的方法采用单步预测头，这在复杂的匹配场景中会陷入局部最小值。为了解决这些问题，我们引入了一种新的范式，该范式利用矩阵空间中的扩散模型来进行稳健的匹配矩阵估计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Establishing reliable correspondences is crucial for all registration tasks,including 2D image registration, 3D point cloud registration, and 2D-3Dimage-to-point cloud registration. However, these tasks are often complicatedby challenges such as scale inconsistencies, symmetry, and large deformations,which can lead to ambiguous matches. Previous feature-based andcorrespondence-based methods typically rely on geometric or semantic featuresto generate or polish initial potential correspondences. Some methods typicallyleverage specific geometric priors, such as topological preservation, to devisediverse and innovative strategies tailored to a given enhancement goal, whichcannot be exhaustively enumerated. Additionally, many previous approaches relyon a single-step prediction head, which can struggle with local minima incomplex matching scenarios. To address these challenges, we introduce aninnovative paradigm that leverages a diffusion model in matrix space for robustmatching matrix estimation. Our model treats correspondence estimation as adenoising diffusion process in the matching matrix space, gradually refiningthe intermediate matching matrix to the optimal one. Specifically, we apply thediffusion model in the doubly stochastic matrix space for 3D-3D and 2D-3Dregistration tasks. In the 2D image registration task, we deploy the diffusionmodel in a matrix subspace where dual-softmax projection regularization isapplied. For all three registration tasks, we provide adaptive matching matrixembedding implementations tailored to the specific characteristics of each taskwhile maintaining a consistent "match-to-warp" encoding pattern. Furthermore,we adopt a lightweight design for the denoising module. In inference, oncepoints or image features are extracted and fixed, this module performsmulti-step denoising predictions through reverse sampling.</description>
      <author>example@mail.com (Qianliang Wu, Haobo Jiang, Yaqing Ding, Lei Luo, Jin Xie, Jian Yang)</author>
      <guid isPermaLink="false">2503.04127v2</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
  <item>
      <title>Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds</title>
      <link>http://arxiv.org/abs/2503.07435v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种新型的神经网络架构，用于处理毫米波雷达（mmWave）稀疏点云数据中的开放集步态识别问题。&lt;h4&gt;背景&lt;/h4&gt;毫米波雷达设备在人类感知领域特别是步态识别中受到越来越多的关注。这些设备因效率高、对环境条件具有抵抗力以及隐私保护性而被广泛应用。&lt;h4&gt;目的&lt;/h4&gt;研究并解决开放集中步态识别（OSGR）的问题，该问题是目前大多数研究中未充分探讨的，因为现有工作大多假设封闭集场景存在未知主体的情况。&lt;h4&gt;方法&lt;/h4&gt;提出了结合监督分类和无监督点云重建的新颖神经网络架构，并引入一种概率异常检测算法，用于在推理时识别未知个体。同时发布了名为mmGait10的人体步态数据集。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法与现有的先进方法相比，在开放性级别不同的情况下平均提升了24%的F1分数。&lt;h4&gt;结论&lt;/h4&gt;通过引入创新性的神经网络架构和异常检测算法，能够有效提高开放集中毫米波雷达稀疏点云数据步态识别的准确性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;摘要中描述了采用毫米波（mmWave）雷达设备进行人体感知的研究，并特别关注步态识别。这些设备因效率高、对环境条件具有抵抗力以及隐私保护性而被广泛应用。论文解决了一个开放集步态识别的问题，即在未知主体可能存在的情况下如何正确地识别个体。通过提出结合监督分类和无监督点云重建的神经网络架构，本文首次解决了使用稀疏点云数据进行开放集中步态识别的挑战，并展示了该方法的有效性和先进性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,particularly gait recognition, has recently gathered significant attention dueto their efficiency, resilience to environmental conditions, andprivacy-preserving nature. In this work, we tackle the challenging problem ofOpen-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlikemost existing research, which assumes a closed-set scenario, our work considersthe more realistic open-set case, where unknown subjects might be present atinference time, and should be correctly recognized by the system. Point cloudsare well-suited for edge computing applications with resource constraints, butare more significantly affected by noise and random fluctuations than otherrepresentations, like the more common micro-Doppler signature. This is thefirst work addressing open-set gait recognition with sparse point cloud data.To do so, we propose a novel neural network architecture that combinessupervised classification with unsupervised reconstruction of the point clouds,creating a robust, rich, and highly regularized latent space of gait features.To detect unknown subjects at inference time, we introduce a probabilisticnovelty detection algorithm that leverages the structured latent space andoffers a tunable trade-off between inference speed and prediction accuracy.Along with this paper, we release mmGait10, an original human gait datasetfeaturing over five hours of measurements from ten subjects, under variedwalking modalities. Extensive experimental results show that our solutionattains F1-Score improvements by 24% over state-of-the-art methods, on average,and across multiple openness levels.</description>
      <author>example@mail.com (Riccardo Mazzieri, Jacopo Pegoraro, Michele Rossi)</author>
      <guid isPermaLink="false">2503.07435v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Multi-Clustering and Decision-Making Strategies for 4D-STEM Orientation Mapping</title>
      <link>http://arxiv.org/abs/2503.06699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 5 figures, 5 figures in SI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本研究提出了一种新颖的无监督学习和决策策略集成，旨在对4D-STEM数据集进行高级分析。重点关注非负矩阵分解（NMF）作为主要聚类方法。&lt;h4&gt;背景&lt;/h4&gt;现有技术在解析4D-STEM数据集时缺乏一种系统的方法来确定最佳成分数量(k)，影响了结果的鲁棒性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架，通过引入K-Component Loss方法和图像质量评估（IQA）度量指标，优化非负矩阵分解的使用，并提高聚类稳定性和准确性。&lt;h4&gt;方法&lt;/h4&gt;利用NMF进行数据集分割，采用K-Component Loss和IQA度量作为选择最佳k值的标准。同时探索了数据预处理的重要性以及引入基于阈值的可视化来分析空间权重矩阵以理解数据集中重叠区域的方法。&lt;h4&gt;主要发现&lt;/h4&gt;该研究展示了结合NMF与高级IQA指标及预处理技术在4D-STEM数据集中的可靠取向映射和结构分析方面的潜力，从而为多维材料表征未来应用铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;通过系统地将无监督学习方法应用于4D-STEM数据分析中，可以获得更为准确且具解释性的结果。该框架不仅提升了模型的鲁棒性还增强了对于复杂数据集的理解能力。&lt;h4&gt;翻译&lt;/h4&gt;This study presents a novel integration of unsupervised learning and decision-making strategies for the advanced analysis of 4D-STEM datasets, with a focus on non-negative matrix factorization (NMF) as the primary clustering method. Our approach introduces a systematic framework to determine the optimal number of components (k) required for robust and interpretable orientation mapping. By leveraging the K-Component Loss method and Image Quality Assessment (IQA) metrics, we effectively balance reconstruction fidelity and model complexity. Additionally, we highlight the critical role of dataset preprocessing in improving clustering stability and accuracy. Furthermore, our spatial weight matrix analysis provides insights into overlapping regions within the dataset by employing threshold-based visualization, facilitating a detailed understanding of cluster interactions. The results demonstrate the potential of combining NMF with advanced IQA metrics and preprocessing techniques for reliable orientation mapping and structural analysis in 4D-STEM datasets, paving the way for future applications in multi-dimensional material characterization.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents a novel integration of unsupervised learning anddecision-making strategies for the advanced analysis of 4D-STEM datasets, witha focus on non-negative matrix factorization (NMF) as the primary clusteringmethod. Our approach introduces a systematic framework to determine the optimalnumber of components (k) required for robust and interpretable orientationmapping. By leveraging the K-Component Loss method and Image Quality Assessment(IQA) metrics, we effectively balance reconstruction fidelity and modelcomplexity. Additionally, we highlight the critical role of datasetpreprocessing in improving clustering stability and accuracy. Furthermore, ourspatial weight matrix analysis provides insights into overlapping regionswithin the dataset by employing threshold-based visualization, facilitating adetailed understanding of cluster interactions. The results demonstrate thepotential of combining NMF with advanced IQA metrics and preprocessingtechniques for reliable orientation mapping and structural analysis in 4D-STEMdatasets, paving the way for future applications in multi-dimensional materialcharacterization.</description>
      <author>example@mail.com (Junhao Cao, Nicolas Folastre, Gozde Oney, Edgar Rauch, Stavros Nicolopoulos, Partha Pratim Das, Arnaud Demortière)</author>
      <guid isPermaLink="false">2503.06699v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Chameleon: Fast-slow Neuro-symbolic Lane Topology Extraction</title>
      <link>http://arxiv.org/abs/2503.07485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025, Project Page: https://github.com/XR-Lee/neural-symbolic&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为Chameleon的快速-慢速神经符号车道拓扑提取算法，用于解决基于视觉语言模型（VLM）的方法在资源消耗和处理复杂情况方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;车道拓扑提取是无地图自动驾驶的关键感知任务，需要检测车道及交通元素，并确定它们之间的关系。现有方法存在计算成本高、无法有效处理复杂场景等缺点。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够平衡性能与资源消耗的神经符号推理方法，用于解决复杂的车道拓扑问题。&lt;h4&gt;方法&lt;/h4&gt;提出了名为Chameleon的算法，该算法通过快速系统直接利用合成程序进行检测实例推理，并由一个慢速系统使用具有链式思维设计的视觉语言模型处理特殊情况。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在OpenLane-V2数据集上展示了优于各种基线检测器的一致性改进。&lt;h4&gt;结论&lt;/h4&gt;Chameleon算法通过结合神经符号方法的优点，提供了一种既经济又高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;车道拓扑提取涉及检测车道和交通元素，并确定它们之间的关系，这是无地图自动驾驶的关键感知任务。这项任务需要复杂的推理能力，例如判断是否可以左转进入特定的车道。为了解决这一挑战，我们引入了由视觉语言基础模型（VLM）驱动的神经符号方法。现有方法存在一些显著局限性：1. 密集视觉提示与VLM相结合虽能实现强大性能，但在财务资源和碳排放方面成本高昂，使其在机器人应用中难以实施；2. 用于3D场景理解的神经符号推理方法在合成程序时无法整合视觉输入，在处理复杂边缘案例时效果不佳。为此，我们提出了一种快速-慢速神经符号车道拓扑提取算法，命名为Chameleon，该算法交替使用一个直接利用检测实例进行推理的快系统和一个使用链式思维设计VLM来处理特殊情况的慢系统。通过这种方式，Chameleon融合了两种方法的优点，提供了一个既经济又高性能的解决方案。我们在OpenLane-V2数据集上对该方法进行了评估，结果显示其在各种基线检测器上的性能均有改进。我们的代码、数据和模型可以在https://github.com/XR-Lee/neural-symbolic中公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lane topology extraction involves detecting lanes and traffic elements anddetermining their relationships, a key perception task for mapless autonomousdriving. This task requires complex reasoning, such as determining whether itis possible to turn left into a specific lane. To address this challenge, weintroduce neuro-symbolic methods powered by vision-language foundation models(VLMs). Existing approaches have notable limitations: (1) Dense visualprompting with VLMs can achieve strong performance but is costly in terms ofboth financial resources and carbon footprint, making it impractical forrobotics applications. (2) Neuro-symbolic reasoning methods for 3D sceneunderstanding fail to integrate visual inputs when synthesizing programs,making them ineffective in handling complex corner cases. To this end, wepropose a fast-slow neuro-symbolic lane topology extraction algorithm, namedChameleon, which alternates between a fast system that directly reasons overdetected instances using synthesized programs and a slow system that utilizes aVLM with a chain-of-thought design to handle corner cases. Chameleon leveragesthe strengths of both approaches, providing an affordable solution whilemaintaining high performance. We evaluate the method on the OpenLane-V2dataset, showing consistent improvements across various baseline detectors. Ourcode, data, and models are publicly available athttps://github.com/XR-Lee/neural-symbolic</description>
      <author>example@mail.com (Zongzheng Zhang, Xinrun Li, Sizhe Zou, Guoxuan Chi, Siqi Li, Xuchong Qiu, Guoliang Wang, Guantian Zheng, Leichen Wang, Hang Zhao, Hao Zhao)</author>
      <guid isPermaLink="false">2503.07485v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Beyond the Edge of Function: Unraveling the Patterns of Type Recovery in Binary Code</title>
      <link>http://arxiv.org/abs/2503.07243v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过深入研究二进制代码中的变量类型恢复问题，提出了ByteTR框架。该框架利用TYDA数据集进行实证分析，并采用了静态程序分析和图神经网络技术来处理复杂类型的分布不平衡及编译器优化的影响。&lt;h4&gt;背景&lt;/h4&gt;在逆向工程和安全应用中，类型恢复是二进制代码分析的关键步骤。现有方法主要关注于通过分析函数内的变量特性来获取类型标识符，但现实世界中的二进制程序具有更复杂的类型模式。&lt;h4&gt;目的&lt;/h4&gt;研究并提出一种新的框架ByteTR，用于改善二进制代码中的变量类型恢复问题，并探索编译器优化对变量存储的影响。&lt;h4&gt;方法&lt;/h4&gt;首先进行了全面的经验研究；然后提出了基于字节的变量类型恢复框架（ByteTR），利用静态程序分析和图神经网络技术处理类型的不均衡分布以及跨函数传播问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过TYDA数据集的研究，发现了二进制代码中的变量和类型具有独特的模式，并且编译器优化对它们有很大影响。此外，ByteTR在效果和效率上都超过了现有方法，在CTF挑战案例中也超越了IDA和Ghidra等工具。&lt;h4&gt;结论&lt;/h4&gt;ByteTR框架有效地解决了二进制代码中的变量类型恢复问题，并为未来的逆向工程和安全应用提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;类型恢复是二进制代码分析的关键步骤，对逆向工程和各种安全应用程序至关重要。现有工作通常只针对二进制代码内的类型标识符，通过分析函数内变量的特性来实现类型恢复。然而，我们发现现实世界的二进制程序中的类型更加复杂，并且经常遵循特定的分布模式。为了深入了解二进制代码中变量类型的恢复问题，我们在TYDA数据集上进行了一项全面的经验研究，该数据集包含四类架构和四种编译器优化选项的163,643个二进制程序。我们仔细研究了二进制代码中的类型和变量的独特模式，并探讨了编译器优化对它们的影响，得出了许多有价值的见解。基于我们的实证发现，我们提出了ByteTR框架来恢复二进制代码中的变量类型。我们将目标类型集解耦以解决不平衡类型的分布问题，并进行了静态程序分析，以应对编译器优化在变量存储上的影响。鉴于我们在研究中观察到的跨函数传播现象普遍存在，ByteTR执行了过程间分析以跟踪变量传播，并使用门控图神经网络来捕捉长范围的数据流依赖关系用于变量类型恢复。我们进行了广泛的实验来评估ByteTR的表现。结果表明，ByteTR在有效性与效率方面都超过了现有的最先进工作，在真实的CTF挑战案例中，通过ByteTR优化的伪代码显著提高了可读性，并且超越了领先的工具IDA和Ghidra。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Type recovery is a crucial step in binary code analysis, holding significantimportance for reverse engineering and various security applications. Existingworks typically simply target type identifiers within binary code and achievetype recovery by analyzing variable characteristics within functions. However,we find that the types in real-world binary programs are more complex and oftenfollow specific distribution patterns.  In this paper, to gain a profound understanding of the variable type recoveryproblem in binary code, we first conduct a comprehensive empirical study. Weutilize the TYDA dataset, which includes 163,643 binary programs across fourarchitectures and four compiler optimization options, fully reflecting thecomplexity and diversity of real-world programs. We carefully study the uniquepatterns that characterize types and variables in binary code, and alsoinvestigate the impact of compiler optimizations on them, yielding manyvaluable insights.  Based on our empirical findings, we propose ByteTR, a framework forrecovering variable types in binary code. We decouple the target type set toaddress the issue of unbalanced type distribution and perform static programanalysis to tackle the impact of compiler optimizations on variable storage. Inlight of the ubiquity of variable propagation across functions observed in ourstudy, ByteTR conducts inter-procedural analysis to trace variable propagationand employs a gated graph neural network to capture long-range data flowdependencies for variable type recovery. We conduct extensive experiments toevaluate the performance of ByteTR. The results demonstrate that ByteTR leadsstate-of-the-art works in both effectiveness and efficiency. Moreover, in realCTF challenge case, the pseudo code optimized by ByteTR significantly improvesreadability, surpassing leading tools IDA and Ghidra.</description>
      <author>example@mail.com (Gangyang Li, Xiuwei Shang, Shaoyin Cheng, Junqi Zhang, Li Hu, Xu Zhu, Weiming Zhang, Nenghai Yu)</author>
      <guid isPermaLink="false">2503.07243v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Anatomy-Aware Conditional Image-Text Retrieval</title>
      <link>http://arxiv.org/abs/2503.07456v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于解剖位置的图像文本检索框架，旨在利用查询图像和特定区域信息来提高罕见疾病的临床诊断效率。&lt;h4&gt;背景&lt;/h4&gt;在医疗健康领域中，图像-文本检索系统用于根据输入的图像或报告自动检索相关患者案例。然而，现有方法依赖于全局特征匹配忽略了局部差异性，导致检索效果欠佳。&lt;h4&gt;目的&lt;/h4&gt;提出了一种基于解剖位置条件化的图像-文本检索框架（ALC-ITR），旨在通过利用特定解剖区域信息来提高罕见疾病的诊断效率。&lt;h4&gt;方法&lt;/h4&gt;设计了一个医学相关性、区域和单词级对齐的视觉语言模型(RRA-VL)，并采用位置条件下的对比学习以更好地提取局部特征，从而改进多模态检索性能。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的RRA-VL模型在相位地面任务中表现出色，并且无论是否有位置信息条件下都能实现优秀的多模态检索效果。&lt;h4&gt;结论&lt;/h4&gt;ALC-ITR系统不仅能提高罕见疾病的诊断效率，还能提供解释性的初步诊断报告。实验表明该方法具有良好的泛化性和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;Image-Text Retrieval (ITR)在医疗健康领域有着广泛的应用，可以通过自动检索数据库中的相关病例来帮助临床医生和放射科医师更有效地进行疾病诊断与治疗，尤其是在罕见病的诊断方面。然而，传统ITR系统通常只依赖于全局图像或文本表示来进行相似性测量，这忽视了患者案例之间的局部差异性，导致检索性能不佳。在本文中，我们提出了一种基于解剖位置条件化的图像-文本检索（ALC-ITR）框架，该框架根据查询图像和相关可疑解剖区域来查找具有相同疾病或症状的相似患者案例。为了实现位置条件下的多模态检索，我们学习了一个医学相关性、区域和单词级对齐的视觉语言模型(RRA-VL)，以产生通用且精确对齐的多模态表示形式，并通过位置条件下的对比学习进一步利用跨对子区域级别的对比度来改善多模态检索。我们的研究结果表明，所提出的RRA-VL在定位性能方面达到了最先进的水平，在相位地面任务中表现出色，并且无论是否有位置信息条件下都能实现优秀的多模态检索效果。此外，我们详细地探讨了ALC-ITR系统提供的解释和初步诊断报告（基于检索到的患者案例）以及利用现成的大规模语言模型提示来增强系统的通用性和可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image-Text Retrieval (ITR) finds broad applications in healthcare, aidingclinicians and radiologists by automatically retrieving relevant patient casesin the database given the query image and/or report, for more efficientclinical diagnosis and treatment, especially for rare diseases. Howeverconventional ITR systems typically only rely on global image or textrepresentations for measuring patient image/report similarities, which overlooklocal distinctiveness across patient cases. This often results in suboptimalretrieval performance. In this paper, we propose an AnatomicalLocation-Conditioned Image-Text Retrieval (ALC-ITR) framework, which, given aquery image and the associated suspicious anatomical region(s), aims toretrieve similar patient cases exhibiting the same disease or symptoms in thesame anatomical region. To perform location-conditioned multimodal retrieval,we learn a medical Relevance-Region-Aligned Vision Language (RRA-VL) model withsemantic global-level and region-/word-level alignment to producegeneralizable, well-aligned multi-modal representations. Additionally, weperform location-conditioned contrastive learning to further utilize cross-pairregion-level contrastiveness for improved multi-modal retrieval. We show thatour proposed RRA-VL achieves state-of-the-art localization performance inphase-grounding tasks, and satisfying multi-modal retrieval performance with orwithout location conditioning. Finally, we thoroughly investigate thegeneralizability and explainability of our proposed ALC-ITR system in providingexplanations and preliminary diagnosis reports given retrieved patient cases(conditioned on anatomical regions), with proper off-the-shelf LLM prompts.</description>
      <author>example@mail.com (Meng Zheng, Jiajin Zhang, Benjamin Planche, Zhongpai Gao, Terrence Chen, Ziyan Wu)</author>
      <guid isPermaLink="false">2503.07456v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications</title>
      <link>http://arxiv.org/abs/2503.07137v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概述&lt;/h4&gt;论文综述了Mixture of Experts (MoE)模型在解决大规模AI模型开发中的计算资源消耗和部署难题以及处理异构复杂数据的挑战方面所取得的进步。&lt;h4&gt;背景介绍&lt;/h4&gt;人工智能（AI）在许多领域取得了惊人的成就，特别是在基础大型模型的发展中。这些大型模型通过利用大量训练数据提供广泛下游任务的灵活解决方案。&lt;h4&gt;面临挑战&lt;/h4&gt;随着现代数据集变得越来越多样化和复杂化，大规模AI模型的发展面临着两大主要挑战：一是计算资源的巨大消耗和部署困难；二是难以适应异构且复杂的多样性数据，这限制了模型的实用性。&lt;h4&gt;MoE模型作用&lt;/h4&gt;混合专家（MoE）模型最近因其在动态选择和激活最相关的子模型以处理输入数据方面所展现出的能力而引起了广泛关注。研究表明，MoE可以通过更少的资源显著提高模型性能和效率，特别是在处理大规模、多模态数据方面表现出色。&lt;h4&gt;研究目的&lt;/h4&gt;鉴于MoE在各个领域展示出的巨大潜力，迫切需要对MoE在许多重要领域的近期进展进行全面总结，并解决现有综述存在的局限性，如过时或未充分讨论某些关键领域的问题。&lt;h4&gt;基本设计&lt;/h4&gt;论文首先介绍了MoE的基本设计方案，包括门控函数、专家网络、路由机制、训练策略和系统设计。&lt;h4&gt;算法设计&lt;/h4&gt;然后探讨了在持续学习、元学习、多任务学习和强化学习等重要机器学习范式中的MoE算法设计。&lt;h4&gt;理论研究&lt;/h4&gt;论文总结了旨在理解MoE的理论研究，并回顾了其在计算机视觉和自然语言处理领域的应用。&lt;h4&gt;未来方向&lt;/h4&gt;最后，讨论了有前景的研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence (AI) has achieved astonishing successes in manydomains, especially with the recent breakthroughs in the development offoundational large models. These large models, leveraging their extensivetraining data, provide versatile solutions for a wide range of downstreamtasks. However, as modern datasets become increasingly diverse and complex, thedevelopment of large AI models faces two major challenges: (1) the enormousconsumption of computational resources and deployment difficulties, and (2) thedifficulty in fitting heterogeneous and complex data, which limits theusability of the models. Mixture of Experts (MoE) models has recently attractedmuch attention in addressing these challenges, by dynamically selecting andactivating the most relevant sub-models to process input data. It has beenshown that MoEs can significantly improve model performance and efficiency withfewer resources, particularly excelling in handling large-scale, multimodaldata. Given the tremendous potential MoE has demonstrated across variousdomains, it is urgent to provide a comprehensive summary of recent advancementsof MoEs in many important fields. Existing surveys on MoE have theirlimitations, e.g., being outdated or lacking discussion on certain key areas,and we aim to address these gaps. In this paper, we first introduce the basicdesign of MoE, including gating functions, expert networks, routing mechanisms,training strategies, and system design. We then explore the algorithm design ofMoE in important machine learning paradigms such as continual learning,meta-learning, multi-task learning, and reinforcement learning. Additionally,we summarize theoretical studies aimed at understanding MoE and review itsapplications in computer vision and natural language processing. Finally, wediscuss promising future research directions.</description>
      <author>example@mail.com (Siyuan Mu, Sen Lin)</author>
      <guid isPermaLink="false">2503.07137v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>HybridReg: Robust 3D Point Cloud Registration with Hybrid Motions</title>
      <link>http://arxiv.org/abs/2503.07019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2025, Association for the Advancement of Artificial Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'问题背景': '场景级别的点云配准在处理动态前景时非常具有挑战性。现有的室内数据集大多假设刚性运动，因此训练出的模型不能很好地应对包含非刚性运动的场景。', '现有方法局限': '对于主要针对对象级别的非刚性数据集而言，训练出来的模型无法很好地泛化到复杂的场景中。', '创新点': '本文提出了HybridReg，一种新的3D点云配准方法，通过学习不确定性掩模来处理混合运动：背景为刚性，前景为非刚性和/或刚性实例级别。', '数据集构建': '首先，建立了一个场景级别的3D注册数据集HybridMatch，旨在以可控的方式安排各种变形的前景。', '模块设计': '其次，考虑不同的运动类型，并制定了一个掩模学习模块来减轻变形异常值的干扰。', '损失函数利用': '第三，采用了一种简单而有效的对数似然性损失函数来利用不确定性指导特征提取和相关计算。', '成果与贡献': '据我们所知，HybridReg是第一个利用混合运动进行鲁棒点云配准的工作。广泛的实验表明，HybridReg具有优势，在广泛使用的室内和室外数据集上取得了最先进的性能。', '创新性': '该方法通过学习不确定性掩模处理背景的刚性和前景的非刚性或刚性实例级别的运动，实现了复杂场景下更稳健的点云配准。', '应用范围': 'HybridReg在广泛的实验中表现出了其优势，并且在室内和室外数据集上都达到了当前最先进的性能。'}&lt;h4&gt;翻译&lt;/h4&gt;Scene-level point cloud registration is very challenging when considering dynamic foregrounds. Existing indoor datasets mostly assume rigid motions, so the trained models cannot robustly handle scenes with non-rigid motions. On the other hand, non-rigid datasets are mainly object-level, so the trained models cannot generalize well to complex scenes. This paper presents HybridReg, a new approach to 3D point cloud registration, learning uncertainty mask to account for hybrid motions: rigid for backgrounds and non-rigid/rigid for instance-level foregrounds. First, we build a scene-level 3D registration dataset, namely HybridMatch, designed specifically with strategies to arrange diverse deforming foregrounds in a controllable manner. Second, we account for different motion types and formulate a mask-learning module to alleviate the interference of deforming outliers. Third, we exploit a simple yet effective negative log-likelihood loss to adopt uncertainty to guide the feature extraction and correlation computation. To our best knowledge, HybridReg is the first work that exploits hybrid motions for robust point cloud registration. Extensive experiments show HybridReg's strengths, leading it to achieve state-of-the-art performance on both widely-used indoor and outdoor datasets.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene-level point cloud registration is very challenging when consideringdynamic foregrounds. Existing indoor datasets mostly assume rigid motions, sothe trained models cannot robustly handle scenes with non-rigid motions. On theother hand, non-rigid datasets are mainly object-level, so the trained modelscannot generalize well to complex scenes. This paper presents HybridReg, a newapproach to 3D point cloud registration, learning uncertainty mask to accountfor hybrid motions: rigid for backgrounds and non-rigid/rigid forinstance-level foregrounds. First, we build a scene-level 3D registrationdataset, namely HybridMatch, designed specifically with strategies to arrangediverse deforming foregrounds in a controllable manner. Second, we account fordifferent motion types and formulate a mask-learning module to alleviate theinterference of deforming outliers. Third, we exploit a simple yet effectivenegative log-likelihood loss to adopt uncertainty to guide the featureextraction and correlation computation. To our best knowledge, HybridReg is thefirst work that exploits hybrid motions for robust point cloud registration.Extensive experiments show HybridReg's strengths, leading it to achievestate-of-the-art performance on both widely-used indoor and outdoor datasets.</description>
      <author>example@mail.com (Keyu Du, Hao Xu, Haipeng Li, Hong Qu, Chi-Wing Fu, Shuaicheng Liu)</author>
      <guid isPermaLink="false">2503.07019v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal 3D Mesh Reconstruction from Images and Text</title>
      <link>http://arxiv.org/abs/2503.07190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一个语言引导的少样本3D重建方法，解决了6D物体姿态估计问题。该方法能够从少量输入图像中重建出3D网格，并研究了成像条件对3D对象重建的影响。&lt;h4&gt;背景&lt;/h4&gt;传统的6D物体姿态估计依赖于需要大量数据集和高计算成本的训练模型，难以泛化。零样本方法虽然不需要训练但通常依赖于预存在的3D物体模型，获取这些模型往往不切实际。&lt;h4&gt;目的&lt;/h4&gt;提出一种语言引导的少样本3D重建方法，解决6D未知对象姿态估计问题，并研究成像条件对3D对象重建的影响。&lt;h4&gt;方法&lt;/h4&gt;该方法接收一组输入图像和一个语言查询。使用GroundingDINO和Segment Anything Model生成分割掩码，然后利用VGGSfM从这些掩码中重建出稀疏点云。接下来，使用Gaussian Splatting方法SuGAR进行网格重建，在最后的清理步骤中移除瑕疵。&lt;h4&gt;主要发现&lt;/h4&gt;评估了该方法在几何形状和纹理准确性和质量方面的性能，并研究了成像条件（如视角、输入图像数量和重叠）对3D对象重建质量和计算效率的影响。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够在少量训练数据的情况下有效进行3D网格重建，为6D物体姿态估计提供了新的解决方案。同时证明了成像条件对于最终的重建质量有着重要影响。&lt;h4&gt;翻译&lt;/h4&gt;六维（6D）未知物体的姿态估计在机器人技术中至关重要，但传统方法依赖于需要大量数据集和高计算成本训练模型，并且难以泛化到未见过的对象上。零样本方法虽然避免了对训练的需求，但通常依赖预存在3D物体模型的获取，这往往是不可行的。为了解决这些问题，我们提出了一种语言引导的少样本3D重建方法，该方法能够从少量输入图像中重新构建一个3D网格。在提议的工作流程中，接收一组输入图像和语言查询。通过结合GroundingDINO和Segment Anything Model生成分割掩码，并使用VGGSfM从这些掩码中重建出稀疏点云。随后，利用Gaussian Splatting方法SuGAR进行网格重建，在最终的清理步骤移除瑕疵，产生查询对象的最终3D网格。评估了该方法在几何形状和纹理准确性和质量方面的性能，并研究了成像条件（如视角、输入图像数量和重叠）对3D对象重建质量和计算效率的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 6D object pose estimation for unseen objects is essential in robotics buttraditionally relies on trained models that require large datasets, highcomputational costs, and struggle to generalize. Zero-shot approaches eliminatethe need for training but depend on pre-existing 3D object models, which areoften impractical to obtain. To address this, we propose a language-guidedfew-shot 3D reconstruction method, reconstructing a 3D mesh from few inputimages. In the proposed pipeline, receives a set of input images and a languagequery. A combination of GroundingDINO and Segment Anything Model outputssegmented masks from which a sparse point cloud is reconstructed with VGGSfM.Subsequently, the mesh is reconstructed with the Gaussian Splatting methodSuGAR. In a final cleaning step, artifacts are removed, resulting in the final3D mesh of the queried object. We evaluate the method in terms of accuracy andquality of the geometry and texture. Furthermore, we study the impact ofimaging conditions such as viewing angle, number of input images, and imageoverlap on 3D object reconstruction quality, efficiency, and computationalscalability.</description>
      <author>example@mail.com (Melvin Reka, Tessa Pulli, Markus Vincze)</author>
      <guid isPermaLink="false">2503.07190v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Feature Fusion Attention Network with CycleGAN for Image Dehazing, De-Snowing and De-Raining</title>
      <link>http://arxiv.org/abs/2503.06107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种结合了FeatureFusion Attention (FFA)网络和CycleGAN架构的新颖去雾方法。该方法利用监督学习和无监督学习技术有效去除图像中的雾霾，同时保持关键的图像细节。&lt;h4&gt;背景&lt;/h4&gt;传统的去雾算法在处理真实世界和合成雾霾图像时存在挑战，特别是在没有配对数据的情况下难以学习有效的映射。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合了FFA网络和CycleGAN架构的方法来改善图像去雾效果，并通过实验验证其性能提升。&lt;h4&gt;方法&lt;/h4&gt;利用FFA网络的特征融合注意机制与CycleGAN的有效性来处理未配对的雾霾和清晰图像，从而在没有直接对应的情况下学习有效的映射关系。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在PSNR（峰值信噪比）和SSIM（结构相似度指标）等质量评价标准上显示出显著改进，并且能够有效应对合成与真实世界中的雾霾图像。&lt;h4&gt;结论&lt;/h4&gt;所提出的混合架构在去雾效果方面优于传统方法，特别是在处理未配对数据时表现突出。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种通过结合FeatureFusion Attention (FFA)网络和CycleGAN结构来改进图像去雾的新方法。该方法采用监督与无监督学习技术有效去除雾霾，同时保持关键的图像细节。实验表明，所提出的混合架构在图像质量度量标准如PSNR（峰值信噪比）和SSIM（结构相似度指标）上取得优越表现，并通过RESIDE和DenseHazeCVPR 2019数据集上的广泛试验验证了其处理合成与真实世界雾霾图像的能力。CycleGAN可以有效应对未配对的雾霾和清晰图像，使模型能够在没有直接对应的情况下学习有效的映射关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel approach to image dehazing by combining FeatureFusion Attention (FFA) networks with CycleGAN architecture. Our methodleverages both supervised and unsupervised learning techniques to effectivelyremove haze from images while preserving crucial image details. The proposedhybrid architecture demonstrates significant improvements in image qualitymetrics, achieving superior PSNR and SSIM scores compared to traditionaldehazing methods. Through extensive experimentation on the RESIDE and DenseHazeCVPR 2019 dataset, we show that our approach effectively handles both syntheticand real-world hazy images. CycleGAN handles the unpaired nature of hazy andclean images effectively, enabling the model to learn mappings even withoutpaired data.</description>
      <author>example@mail.com (Akshat Jain)</author>
      <guid isPermaLink="false">2503.06107v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Linguistic Knowledge Transfer Learning for Speech Enhancement</title>
      <link>http://arxiv.org/abs/2503.07078v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文探讨了在嘈杂环境中语言知识对口语理解的重要性，并提出了一种新的跨模态知识传输（CMKT）框架，该框架利用预训练的语言模型将语言知识融入语音增强模型中。&lt;h4&gt;背景&lt;/h4&gt;语言知识对于提高噪声环境下的言语理解至关重要。然而，大多数现有的语音增强方法主要依赖于声学特征来学习嘈杂与清晰语音之间的映射关系，并且较少整合语言信息。尽管有一些基于文本的语音增强方法被研究过，但它们通常需要显式的语音-文本对齐或外部提供的文本数据，这限制了其在现实场景中的实用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架（CMKT），该框架利用预训练的语言模型将语言知识融入语音增强模型中，并展示了这种框架的有效性和适应性。&lt;h4&gt;方法&lt;/h4&gt;提出了跨模态知识传输（CMKT）学习框架，它不依赖于文本输入或在推理过程中使用大型语言模型。此外，引入了一种不对齐策略来改进知识传递效果。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与基准模型相比，CMKT在各种语音增强架构和预训练语言模型嵌入上均表现出色，并且展示了其对不同配置的适应性；该方法不仅适用于汉语和英语等不同语境下的数据集，在缺乏文本数据的情况下也能保持有效性。&lt;h4&gt;结论&lt;/h4&gt;通过弥合语言与声学模态之间的差距，CMKT为将语言知识整合到语音增强模型中提供了一种可扩展且创新的方法，从而在提高清晰度和增强性能方面取得了显著进步。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了语言知识对于噪声环境下的言语理解的重要性，并介绍了CMKT框架。该研究展示了如何利用预训练的语言模型来有效提升语音增强技术的效果，而无需依赖额外的文本数据或大型语言模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Linguistic knowledge plays a crucial role in spoken language comprehension.It provides essential semantic and syntactic context for speech perception innoisy environments. However, most speech enhancement (SE) methods predominantlyrely on acoustic features to learn the mapping relationship between noisy andclean speech, with limited exploration of linguistic integration. Whiletext-informed SE approaches have been investigated, they often require explicitspeech-text alignment or externally provided textual data, constraining theirpracticality in real-world scenarios. Additionally, using text as input poseschallenges in aligning linguistic and acoustic representations due to theirinherent differences. In this study, we propose the Cross-Modality KnowledgeTransfer (CMKT) learning framework, which leverages pre-trained large languagemodels (LLMs) to infuse linguistic knowledge into SE models without requiringtext input or LLMs during inference. Furthermore, we introduce a misalignmentstrategy to improve knowledge transfer. This strategy applies controlledtemporal shifts, encouraging the model to learn more robust representations.Experimental evaluations demonstrate that CMKT consistently outperformsbaseline models across various SE architectures and LLM embeddings,highlighting its adaptability to different configurations. Additionally,results on Mandarin and English datasets confirm its effectiveness acrossdiverse linguistic conditions, further validating its robustness. Moreover,CMKT remains effective even in scenarios without textual data, underscoring itspracticality for real-world applications. By bridging the gap betweenlinguistic and acoustic modalities, CMKT offers a scalable and innovativesolution for integrating linguistic knowledge into SE models, leading tosubstantial improvements in both intelligibility and enhancement performance.</description>
      <author>example@mail.com (Kuo-Hsuan Hung, Xugang Lu, Szu-Wei Fu, Huan-Hsin Tseng, Hsin-Yi Lin, Chii-Wann Lin, Yu Tsao)</author>
      <guid isPermaLink="false">2503.07078v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards Safe Robot Foundation Models</title>
      <link>http://arxiv.org/abs/2503.07404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文探讨了机器人基础模型在不同环境中的潜在应用，并提出了一种确保安全的解决方案。&lt;h4&gt;背景&lt;/h4&gt;当前的研究主要集中在政策的跨任务泛化能力上，但忽略了安全性这一实际部署的关键要求。&lt;h4&gt;目的&lt;/h4&gt;介绍一种能够适当限制任意通用策略动作空间的安全层设计。&lt;h4&gt;方法&lt;/h4&gt;使用ATACOM算法创建一个安全的动作空间，从而确保状态转换的安全性。该方法可以扩展到一般策略中，无需特定的安全微调即可在高风险场景中部署模型。&lt;h4&gt;主要发现&lt;/h4&gt;在一个空气曲棍球环境中展示了此安全层的有效性，证明了它可以防止撞球的代理与周围环境碰撞的情况发生，这是普通通用政策常见的失败点。&lt;h4&gt;结论&lt;/h4&gt;提出的方法为确保机器人基础模型的安全性和实际应用铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到：机器人基础模型具有在工业和家庭等多样化环境中部署的潜力。虽然当前的研究主要集中在不同任务中策略泛化能力上，但忽略了安全性这一实际部署的关键要求。本文介绍了设计用于适当限制任何通用策略动作空间的安全层。该方法使用ATACOM算法创建一个安全的动作空间，并确保状态转换的安全性。通过将ATACOM扩展到一般策略中，我们的方法可以在高风险场景中部署模型而无需特定的安全微调。在空气曲棍球环境中展示了此安全层的有效性，证明了它可以防止撞球的代理与周围环境碰撞的情况发生，这是普通通用政策常见的失败点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot foundation models hold the potential for deployment across diverseenvironments, from industrial applications to household tasks. While currentresearch focuses primarily on the policies' generalization capabilities acrossa variety of tasks, it fails to address safety, a critical requirement fordeployment on real-world systems. In this paper, we introduce a safety layerdesigned to constrain the action space of any generalist policy appropriately.Our approach uses ATACOM, a safe reinforcement learning algorithm that createsa safe action space and, therefore, ensures safe state transitions. Byextending ATACOM to generalist policies, our method facilitates theirdeployment in safety-critical scenarios without requiring any specific safetyfine-tuning. We demonstrate the effectiveness of this safety layer in an airhockey environment, where it prevents a puck-hitting agent from colliding withits surroundings, a failure observed in generalist policies.</description>
      <author>example@mail.com (Maximilian Tölle, Theo Gruner, Daniel Palenicek, Jonas Günster, Puze Liu, Joe Watson, Davide Tateo, Jan Peters)</author>
      <guid isPermaLink="false">2503.07404v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>An Analytics-Driven Approach to Enhancing Supply Chain Visibility with Graph Neural Networks and Federated Learning</title>
      <link>http://arxiv.org/abs/2503.07231v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 5 figures, 5 tables, submitted to a journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合联邦学习（FL）和图卷积神经网络（GCNs）的新方法，用于增强供应链的可见性。&lt;h4&gt;背景&lt;/h4&gt;全球化贸易中，复杂的供应链跨越多个组织甚至国家，容易受到中断的影响。最近的全球危机强调了提高供应链可视性和弹性的紧迫性。然而，由于隐私、安全和监管方面的限制，数据共享成为障碍，这阻碍了跨组织或国家之间实现全面可见性。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，并改进供应链中的关系预测，该论文提出了一个新方法，以提高全球贸易中复杂供应链网络的可视化程度。&lt;h4&gt;方法&lt;/h4&gt;通过结合联邦学习（FL）和图卷积神经网络（GCNs），这种方法允许跨国家合作训练模型而无需交换原始数据，同时确保遵守隐私规定并维护数据安全。 GCN则帮助捕捉知识图中的复杂关系模式，并实现准确的关系预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该方法能够准确地在国家层面的供应链知识图中进行关系预测，为供应链网络提供全面见解和支持行动建议。&lt;h4&gt;结论&lt;/h4&gt;这种方法通过增强可视性支持可操作的洞察力、促进主动风险管理并有助于开发适应性强和弹性的供应链策略，确保供应链能够在全球经济复杂环境中更好地应对挑战。&lt;h4&gt;翻译&lt;/h4&gt;在全球化的贸易背景下，论文旨在介绍一种新的方法以提高跨组织或国家的供应链可见性和韧性，该方法通过联邦学习和图卷积神经网络之间的结合来实现关系预测，并在保障隐私安全的情况下改进数据共享机制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In today's globalised trade, supply chains form complex networks spanningmultiple organisations and even countries, making them highly vulnerable todisruptions. These vulnerabilities, highlighted by recent global crises,underscore the urgent need for improved visibility and resilience of the supplychain. However, data-sharing limitations often hinder the achievement ofcomprehensive visibility between organisations or countries due to privacy,security, and regulatory concerns. Moreover, most existing research studiesfocused on individual firm- or product-level networks, overlooking themultifaceted interactions among diverse entities that characterise real-worldsupply chains, thus limiting a holistic understanding of supply chain dynamics.To address these challenges, we propose a novel approach that integratesFederated Learning (FL) and Graph Convolutional Neural Networks (GCNs) toenhance supply chain visibility through relationship prediction in supply chainknowledge graphs. FL enables collaborative model training across countries byfacilitating information sharing without requiring raw data exchange, ensuringcompliance with privacy regulations and maintaining data security. GCNs empowerthe framework to capture intricate relational patterns within knowledge graphs,enabling accurate link prediction to uncover hidden connections and providecomprehensive insights into supply chain networks. Experimental resultsvalidate the effectiveness of the proposed approach, demonstrating its abilityto accurately predict relationships within country-level supply chain knowledgegraphs. This enhanced visibility supports actionable insights, facilitatesproactive risk management, and contributes to the development of resilient andadaptive supply chain strategies, ensuring that supply chains are betterequipped to navigate the complexities of the global economy.</description>
      <author>example@mail.com (Ge Zheng, Alexandra Brintrup)</author>
      <guid isPermaLink="false">2503.07231v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>PIED: Physics-Informed Experimental Design for Inverse Problems</title>
      <link>http://arxiv.org/abs/2503.07070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to 13th International Conference on Learning Representations  (ICLR 2025), 31 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了PIED框架，该框架首次利用物理信息神经网络（PINNs）进行实验设计，在一次部署中优化逆问题的参数设置。&lt;h4&gt;背景&lt;/h4&gt;在许多科学和工程领域，系统动态特性通过支配性的偏微分方程(PDEs)描述，并且一个主要挑战是解决在这种有限预算下基于观测数据反推未知PDE参数的逆向问题。由于实验设置和运行成本高昂，通常使用PDE仿真来帮助优化实验设计参数，在实际数据收集之前进行最有效的信息获取。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的实验设计框架PIED，它在一次部署中连续优化逆问题的设计参数，并利用PINNs的优势克服现有方法的计算瓶颈。&lt;h4&gt;方法&lt;/h4&gt;提出了一个完全可微架构的实验设计（PIED）框架，该框架使用物理信息神经网络来解决逆向问题。PIED通过并行化计算和元学习物理信息神经网络参数初始化来克服现有的计算瓶颈，并提出新方法有效地考虑PINN训练动力学在优化实验设计参数方面的影响。&lt;h4&gt;主要发现&lt;/h4&gt;基于噪声模拟数据以及真实世界实验数据的实验证明，给定有限的观察预算，在解决逆向问题时PIED比现有实验设计方法表现更好，即使是在未知函数而非有限维参数的情况下也适用。&lt;h4&gt;结论&lt;/h4&gt;通过引入物理信息神经网络（PINNs）并在完全可微架构中进行优化，新框架解决了逆向问题中的一个重要挑战，即如何在预算和实践限制下有效获取数据。PIED提供了一种新的途径来解决具有复杂动态特性的系统中的逆向问题。&lt;h4&gt;翻译&lt;/h4&gt;在许多科学与工程领域，研究团队提出了一种基于物理信息神经网络（Physics-Informed Neural Networks, PINNs）的实验设计框架——PIED，该方法首次利用了PINNs的优势，并且通过优化PDE参数，在一次部署中实现了高效的数据收集和逆向问题解决。现有的实验设计方法通常需要在多次试验间频繁调整参数设置并依赖复杂的数值模拟来克服计算瓶颈。然而，新的PIED框架通过引入元学习的神经网络初始化参数以及对PINN训练动态的有效利用，解决了这一难题，并且实验证明了该方法在处理逆向问题时优于传统的实验设计方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In many science and engineering settings, system dynamics are characterizedby governing PDEs, and a major challenge is to solve inverse problems (IPs)where unknown PDE parameters are inferred based on observational data gatheredunder limited budget. Due to the high costs of setting up and runningexperiments, experimental design (ED) is often done with the help of PDEsimulations to optimize for the most informative design parameters to solvesuch IPs, prior to actual data collection. This process of optimizing designparameters is especially critical when the budget and other practicalconstraints make it infeasible to adjust the design parameters between trialsduring the experiments. However, existing experimental design (ED) methods tendto require sequential and frequent design parameter adjustments between trials.Furthermore, they also have significant computational bottlenecks due to theneed for complex numerical simulations for PDEs, and do not exploit theadvantages provided by physics informed neural networks (PINNs), such as itsmeshless solutions, differentiability, and amortized training. This workpresents PIED, the first ED framework that makes use of PINNs in a fullydifferentiable architecture to perform continuous optimization of designparameters for IPs for one-shot deployments. PIED overcomes existing methods'computational bottlenecks through parallelized computation and meta-learning ofPINN parameter initialization, and proposes novel methods to effectively takeinto account PINN training dynamics in optimizing the ED parameters. Throughexperiments based on noisy simulated data and even real world experimentaldata, we empirically show that given limited observation budget, PIEDsignificantly outperforms existing ED methods in solving IPs, includingchallenging settings where the inverse parameters are unknown functions ratherthan just finite-dimensional.</description>
      <author>example@mail.com (Apivich Hemachandra, Gregory Kang Ruey Lau, See-Kiong Ng, Bryan Kian Hsiang Low)</author>
      <guid isPermaLink="false">2503.07070v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Explainable Android Malware Detection and Malicious Code Localization Using Graph Attention</title>
      <link>http://arxiv.org/abs/2503.07109v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has 13 pages and contains 5 images (3 figures within the  paper and 2 author photos). It is being submitted to IEEE Transactions on  Information Forensics and Security for consideration&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;随着恶意软件威胁的加剧，特别是在移动设备上，对有效分析方法的需求前所未有的高。尽管现有的安全解决方案（包括基于AI的方法）显示出潜力，但其缺乏透明度限制了人们对检测到的威胁的理解。手动分析仍然耗时且依赖于稀缺的专业知识。&lt;h4&gt;背景&lt;/h4&gt;当前的安全解决方案虽然在对抗恶意软件方面有所进展，但仍存在不透明性问题，这对理解和解释被发现的威胁造成了挑战。传统的手工代码分析方法因时间成本和专业技能要求高而难以广泛应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自动检测恶意代码片段的方法（XAIDroid），以增强对恶意软件分析的可扩展性、可解释性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;XAIDroid利用图神经网络(GNNs)和图注意力机制来识别API调用图中的恶意代码片段，通过给API节点分配重要性得分，从而实现对关键信息的关注。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果表明，该方法在合成数据集和真实世界的数据集中都具有高效的召回率和F1分数，证明了其在定位恶意代码方面的能力。&lt;h4&gt;结论&lt;/h4&gt;自动化的恶意代码识别增强了恶意软件分析的可扩展性、可解释性和可靠性，为解决现有的安全挑战提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着恶意软件威胁的加剧，特别是在移动设备上，对有效分析方法的需求前所未有的高。尽管现有的安全解决方案（包括基于AI的方法）显示出潜力，但其缺乏透明度限制了人们对检测到的威胁的理解。手动分析仍然耗时且依赖于稀缺的专业知识。为了应对这些挑战，我们提出了一种名为XAIDroid的新方法，该方法利用图神经网络(GNNs)和图注意力机制自动定位恶意软件中的恶意代码片段。通过将代码表示为API调用图，XAIDroid捕获了语义上下文，并增强了对混淆的抵抗能力。采用Graph Attention Model (GAM) 和 Graph Attention Network (GAT)，我们给API节点分配重要性得分，从而有助于集中关注关键信息进行恶意代码定位。在合成数据集和真实世界的恶意软件数据集上的评估证明了该方法的有效性，在恶意代码定位方面实现了高召回率和F1分数。自动化的恶意代码定位的实现增强了恶意软件分析的可扩展性、解释性和可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the escalating threat of malware, particularly on mobile devices, thedemand for effective analysis methods has never been higher. While existingsecurity solutions, including AI-based approaches, offer promise, their lack oftransparency constraints the understanding of detected threats. Manual analysisremains time-consuming and reliant on scarce expertise. To address thesechallenges, we propose a novel approach called XAIDroid that leverages graphneural networks (GNNs) and graph attention mechanisms for automaticallylocating malicious code snippets within malware. By representing code as APIcall graphs, XAIDroid captures semantic context and enhances resilience againstobfuscation. Utilizing the Graph Attention Model (GAM) and Graph AttentionNetwork (GAT), we assign importance scores to API nodes, facilitating focusedattention on critical information for malicious code localization. Evaluationon synthetic and real-world malware datasets demonstrates the efficacy of ourapproach, achieving high recall and F1-score rates for malicious codelocalization. The successful implementation of automatic malicious codelocalization enhances the scalability, interpretability, and reliability ofmalware analysis.</description>
      <author>example@mail.com (Merve Cigdem Ipek, Sevil Sen)</author>
      <guid isPermaLink="false">2503.07109v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>MetaXCR: Reinforcement-Based Meta-Transfer Learning for Cross-Lingual Commonsense Reasoning</title>
      <link>http://arxiv.org/abs/2503.06531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文研究了跨语言低资源常识推理（Cross-lingual Low-Resource Commonsense Reasoning），提出了一种多源适配器框架MetaXCR。&lt;h4&gt;背景&lt;/h4&gt;现有的常识推理数据集多数是英语构建的，导致大多数先前的工作集中在英语领域。由于标注成本高昂，为每个新任务建立大规模数据集是不可能的。&lt;h4&gt;目的&lt;/h4&gt;提出了一个能够利用多种存在英语数据集帮助模型适应新的跨语言目标数据集的方法框架MetaXCR。&lt;h4&gt;方法&lt;/h4&gt;{'学习适配器': '通过引入元学习并结合多个训练数据集来学习不同任务之间的泛化任务适配器。', '采样策略': '提出了基于强化的学习策略，以帮助模型选择对目标任务最有帮助的源任务进行采样。', '增强方法': '介绍了两种跨语言元适应方法以提高在目标语言上的性能。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验表明MetaXCR优于现有最新技术，并且训练参数更少。&lt;h4&gt;结论&lt;/h4&gt;该论文提供了一个有效的框架，解决了跨语言低资源常识推理的问题，展示了其优越的性能和效率。&lt;h4&gt;翻译&lt;/h4&gt;常识推理（Commonsense Reasoning, CR）在许多领域得到了研究，并借助大规模数据集取得了巨大进展。不幸的是，大多数现有的CR数据集都是用英语构建的，因此之前的大部分工作都集中在英语上。此外，由于常识推理的标注成本高昂，为每个新任务建立一个大型数据集是不可能的。因此，对于跨语言低资源常识推理的需求日益增长，其目标是利用多样化的现有英语数据集来帮助模型适应新的具有有限标签数据的目标数据集。在本文中，我们提出了一种用于跨语言低资源常识推理（MetaXCR）的多源适配器框架。在这个框架中，首先通过引入元学习并结合多个训练数据集来学习一个泛化任务适配器以跨越不同的任务。然后进一步引入基于强化的学习策略帮助模型采样对目标任务最有用的源任务。最后介绍了两种跨语言元适应方法以增强在目标语言上的性能。广泛的实验表明，MetaXCR优于最先进的技术，并且其训练使用的参数比其他工作更少。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Commonsense reasoning (CR) has been studied in many pieces of domain and hasachieved great progress with the aid of large datasets. Unfortunately, mostexisting CR datasets are built in English, so most previous work focus onEnglish. Furthermore, as the annotation of commonsense reasoning is costly, itis impossible to build a large dataset for every novel task. Therefore, thereare growing appeals for Cross-lingual Low-Resource Commonsense Reasoning, whichaims to leverage diverse existed English datasets to help the model adapt tonew cross-lingual target datasets with limited labeled data. In this paper, wepropose a multi-source adapter for cross-lingual low-resource CommonsenseReasoning (MetaXCR). In this framework, we first extend meta learning byincorporating multiple training datasets to learn a generalized task adaptersacross different tasks. Then, we further introduce a reinforcement-basedsampling strategy to help the model sample the source task that is the mosthelpful to the target task. Finally, we introduce two types of cross-lingualmeta-adaption methods to enhance the performance of models on target languages.Extensive experiments demonstrate MetaXCR is superior over state-of-the-arts,while being trained with fewer parameters than other work.</description>
      <author>example@mail.com (Jie He, Yu Fu)</author>
      <guid isPermaLink="false">2503.06531v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Divide and Conquer Self-Supervised Learning for High-Content Imaging</title>
      <link>http://arxiv.org/abs/2503.07444v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种新的架构SpliCER，用于在自监督表示学习中克服简单特征占主导地位的问题。&lt;h4&gt;背景问题&lt;/h4&gt;当前的自监督表征学习方法难以捕捉细微或复杂的特征，特别是在科学和工程应用中这些问题尤为严重。&lt;h4&gt;研究目的&lt;/h4&gt;引入一种新型架构以引导模型学习更细腻和复杂的特征，同时不牺牲简单特征的学习效果。&lt;h4&gt;创新方法&lt;/h4&gt;SpliCER将图像分割成多个部分，并从每个部分提取信息来指导模型学习更复杂和细微的特征。&lt;h4&gt;实验贡献&lt;/h4&gt;['展示了现有自监督方法在简单与复杂特征共存的情况下倾向于寻找捷径解决方案', '提出了一种新颖的自监督训练方法SpliCER，可以显著提高下游任务的表现', '证明了SpliCER在尖端医学和地理空间成像应用中的有效性']&lt;h4&gt;结论&lt;/h4&gt;通过将图像分割并从每个部分提取信息，SpliCER能有效解决现有自监督表示学习的局限性。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到：自监督表征学习方法通常难以捕捉细微或复杂的特征，因为它们会被更简单的模式所主导。这种限制在科学和工程应用中尤其成问题，因为复杂特性对发现和分析至关重要。为了解决这一问题，我们提出了一种新的架构——Split Component Embedding Registration（SpliCER），该架构将图像分割为多个部分，并从每个部分提取信息来指导模型学习更细微和复杂的特征而不牺牲简单特征的学习效果。SpliCER与任何自监督损失函数兼容，并可以无修改地集成到现有方法中。本文的主要贡献包括：(i)展示在存在简单和复杂特性的情况下，现有自监督方法倾向于寻找捷径解决方案；(ii)提出了一个新颖的自监督训练方法SpliCER来克服现有方法的局限性，并实现了显著的下游任务性能提升；以及(iii)展示了SpliCER在尖端医学与地理空间成像应用中的有效性。SpliCER提供了一种强大的新工具用于表征学习，使模型能够发现其他方法可能忽视的复杂特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised representation learning methods often fail to learn subtle orcomplex features, which can be dominated by simpler patterns which are mucheasier to learn. This limitation is particularly problematic in applications toscience and engineering, as complex features can be critical for discovery andanalysis. To address this, we introduce Split Component Embedding Registration(SpliCER), a novel architecture which splits the image into sections anddistils information from each section to guide the model to learn more subtleand complex features without compromising on simpler features. SpliCER iscompatible with any self-supervised loss function and can be integrated intoexisting methods without modification. The primary contributions of this workare as follows: i) we demonstrate that existing self-supervised methods canlearn shortcut solutions when simple and complex features are both present; ii)we introduce a novel self-supervised training method, SpliCER, to overcome thelimitations of existing methods, and achieve significant downstream performanceimprovements; iii) we demonstrate the effectiveness of SpliCER in cutting-edgemedical and geospatial imaging settings. SpliCER offers a powerful new tool forrepresentation learning, enabling models to uncover complex features whichcould be overlooked by other methods.</description>
      <author>example@mail.com (Lucas Farndale, Paul Henderson, Edward W Roberts, Ke Yuan)</author>
      <guid isPermaLink="false">2503.07444v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>A Light Perspective for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2503.07133v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的3D物体检测模型NextBEV，通过结合先进的深度学习技术来提高相机和LIDAR数据融合的效率。&lt;h4&gt;背景&lt;/h4&gt;自主车辆技术的发展需要精确地理解环境并在三维空间中准确地探测物体。现有的方法依赖于计算需求高的传统骨干网络。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型的方法，在保持性能不变的情况下，通过使用先进的深度学习技术来减少模型的复杂性和计算成本。&lt;h4&gt;方法&lt;/h4&gt;引入NextBEV模型，该模型在特征提取过程中采用前沿的深度学习技术，并优化LIDAR骨干网络以降低推理时间。&lt;h4&gt;主要发现&lt;/h4&gt;NextBEV模型优于ResNet50和MobileNetV2，在KITTI 3D单目检测基准测试中比MobileNetV3参数减少超过10%，准确度提高2.39%。通过融合轻量级提案，提高了基于VoxelNet的模型精度2.93%，提升了PointPillar基础模型约20%的F1分数。&lt;h4&gt;结论&lt;/h4&gt;这项工作有助于创建更适合车载应用的强大且高效的单一技术或融合技术模型。&lt;h4&gt;翻译&lt;/h4&gt;理解环境并在三维空间中精确检测物体对于推进自主车辆技术至关重要。结合相机和LIDAR数据已成为提高3D物体检测模型精度的有效方法。然而，现有的方法通常依赖于计算需求高的传统骨干网络。本文介绍了一种新的方法，即在特征提取过程中应用先进的深度学习技术来创建更高效的模型而不影响性能。我们的NextBEV模型超越了ResNet50和MobileNetV2。在KITTI 3D单目检测基准测试中，NextBEV比MobileNetV3的参数减少了超过10%，准确度提高了2.39%。此外，我们还提出了LIDAR骨干网络的变化，将原始推理时间减少到10毫秒以下。通过融合这些轻量级提案，我们提升了基于VoxelNet模型精度约2.93%，改进了PointPillar基础模型的F1分数近20%。因此，这项工作有助于建立更强大且适用于车载实施的轻型单一技术或融合技术模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1117/12.3055035&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Comprehending the environment and accurately detecting objects in 3D spaceare essential for advancing autonomous vehicle technologies. Integrating Cameraand LIDAR data has emerged as an effective approach for achieving high accuracyin 3D Object Detection models. However, existing methodologies often rely onheavy, traditional backbones that are computationally demanding. This paperintroduces a novel approach that incorporates cutting-edge Deep Learningtechniques into the feature extraction process, aiming to create more efficientmodels without compromising performance. Our model, NextBEV, surpassesestablished feature extractors like ResNet50 and MobileNetV2. On the KITTI 3DMonocular detection benchmark, NextBEV achieves an accuracy improvement of2.39%, having less than 10% of the MobileNetV3 parameters. Moreover, we proposechanges in LIDAR backbones that decreased the original inference time to 10 ms.Additionally, by fusing these lightweight proposals, we have enhanced theaccuracy of the VoxelNet-based model by 2.93% and improved the F1-score of thePointPillar-based model by approximately 20%. Therefore, this work contributesto establishing lightweight and powerful models for individual or fusiontechniques, making them more suitable for onboard implementations.</description>
      <author>example@mail.com (Marcelo Eduardo Pederiva, José Mario De Martino, Alessandro Zimmer)</author>
      <guid isPermaLink="false">2503.07133v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>PersonaBooth: Personalized Text-to-Motion Generation</title>
      <link>http://arxiv.org/abs/2503.07390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Motion Personalization任务，该任务利用包含Persona的基本动作生成与文本描述相匹配的个性化运动。为支持这一新任务，引入了名为PerMo（PersonaMotion）的新大规模动作数据集。&lt;h4&gt;背景&lt;/h4&gt;现有的预训练数据集缺乏特定于个人的数据，并且很难从内容不同的动作中捕获一致的人格特征。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于预训练的运动扩散模型的多模态微调方法，称为PersonaBooth，以解决上述挑战并提高个性化的运动生成效果。&lt;h4&gt;方法&lt;/h4&gt;1) 引入了一种个人代币来接受新的个人特征，并在微调期间执行文本和视觉内容的多模态适应；2) 采用了对比学习技术增强具有相同人格特征样本间的内聚性；3) 提出了一种基于上下文的融合机制，以最大限度地从多个输入动作中整合人格线索。&lt;h4&gt;主要发现&lt;/h4&gt;PersonaBooth在运动个性化方面超越了现有的最先进的运动风格转移方法，并为该任务建立了新的基准。&lt;h4&gt;结论&lt;/h4&gt;提出的模型和数据集为解决大规模动作生成中的个人化挑战提供了有效的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces Motion Personalization, a new task that generatespersonalized motions aligned with text descriptions using several basic motionscontaining Persona. To support this novel task, we introduce a new large-scalemotion dataset called PerMo (PersonaMotion), which captures the unique personasof multiple actors. We also propose a multi-modal finetuning method of apretrained motion diffusion model called PersonaBooth. PersonaBooth addressestwo main challenges: i) A significant distribution gap between thepersona-focused PerMo dataset and the pretraining datasets, which lackpersona-specific data, and ii) the difficulty of capturing a consistent personafrom the motions vary in content (action type). To tackle the datasetdistribution gap, we introduce a persona token to accept new persona featuresand perform multi-modal adaptation for both text and visuals during finetuning.To capture a consistent persona, we incorporate a contrastive learningtechnique to enhance intra-cohesion among samples with the same persona.Furthermore, we introduce a context-aware fusion mechanism to maximize theintegration of persona cues from multiple input motions. PersonaBoothoutperforms state-of-the-art motion style transfer methods, establishing a newbenchmark for motion personalization.</description>
      <author>example@mail.com (Boeun Kim, Hea In Jeong, JungHoon Sung, Yihua Cheng, Jeongmin Lee, Ju Yong Chang, Sang-Il Choi, Younggeun Choi, Saim Shin, Jungho Kim, Hyung Jin Chang)</author>
      <guid isPermaLink="false">2503.07390v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Overlapping Prediction: A Self-supervised Pre-training Method for LiDAR Moving Object Segmentation</title>
      <link>http://arxiv.org/abs/2503.07167v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;针对激光雷达点云中的运动对象分割（MOS）提出了一种自监督预训练方法TOP，该方法通过预测时间重叠点的占用状态来缓解标注负担。&lt;h4&gt;背景&lt;/h4&gt;传统的监督学习方法依赖于昂贵的手动注释。而激光雷达序列自然地捕捉到的时间动态线索可以被利用来进行无监督学习。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的自监督预训练方法，以减轻运动对象分割中的手动注释需求，并探索时间重叠点的使用来提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;TOP方法通过预测时间上相邻扫描序列中共同观察到的时间重叠点的位置状态来工作。此外，该方法还利用当前占用重建作为辅助预训练目标。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，传统的交并比（IoU）度量对于具有较少采样点的物体存在偏差。为此，引入了一个新的评估指标$mIoU_{obj}$来评价对象级性能。&lt;h4&gt;结论&lt;/h4&gt;在nuScenes和SemanticKITTI数据集上的实验结果证明了TOP方法的有效性，相对于从头开始监督训练基准和其他无监督预训练基准最多可提高28.77%的相对改进。这表明该方法在不同的激光雷达设置中具有良好的迁移性和对其他任务的良好泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;移动对象分割（MOS）在点云数据上对于自动驾驶系统至关重要。先前的监督方法严重依赖于昂贵的手动注释，而激光雷达序列自然地捕捉到了可以利用的时间动态线索来进行无监督学习。我们提出了时间重叠预测（TOP），一种自监督预训练方法，以减轻标签负担并改进MOS性能。该方法通过预测当前和相邻扫描中共同观察到的点来工作，并引入了额外的度量$mIoU_{obj}$来补偿传统IoU度量对于小或远距离物体的偏差。实验表明，TOP在nuScenes和SemanticKITTI上显著优于其他基线，代码及预训练模型将在发布时公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Moving object segmentation (MOS) on LiDAR point clouds is crucial forautonomous systems like self-driving vehicles. Previous supervised approachesrely heavily on costly manual annotations, while LiDAR sequences naturallycapture temporal motion cues that can be leveraged for self-supervisedlearning. In this paper, we propose \textbf{T}emporal \textbf{O}verlapping\textbf{P}rediction (\textbf{TOP}), a self-supervised pre-training method thatalleviate the labeling burden for MOS. \textbf{TOP} explores the temporaloverlapping points that commonly observed by current and adjacent scans, andlearns spatiotemporal representations by predicting the occupancy states oftemporal overlapping points. Moreover, we utilize current occupancyreconstruction as an auxiliary pre-training objective, which enhances thecurrent structural awareness of the model. We conduct extensive experiments andobserve that the conventional metric Intersection-over-Union (IoU) shows strongbias to objects with more scanned points, which might neglect small or distantobjects. To compensate for this bias, we introduce an additional metric called$\text{mIoU}_{\text{obj}}$ to evaluate object-level performance. Experiments onnuScenes and SemanticKITTI show that \textbf{TOP} outperforms both supervisedtraining-from-scratch baseline and other self-supervised pre-training baselinesby up to 28.77\% relative improvement, demonstrating strong transferabilityacross LiDAR setups and generalization to other tasks. Code and pre-trainedmodels will be publicly available upon publication.</description>
      <author>example@mail.com (Ziliang Miao, Runjian Chen, Yixi Cai, Buwei He, Wenquan Zhao, Wenqi Shao, Bo Zhang, Fu Zhang)</author>
      <guid isPermaLink="false">2503.07167v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Are We Truly Forgetting? A Critical Re-examination of Machine Unlearning Evaluation Protocols</title>
      <link>http://arxiv.org/abs/2503.06991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文深入探讨了机器去学习的过程，这种过程旨在从训练模型中移除特定的数据点而不影响保留数据的性能，以满足隐私或法律要求。现有的评估方法主要关注小规模场景下的逻辑损失指标（如准确率），这可能导致在实际应用中的安全感缺失。&lt;h4&gt;背景&lt;/h4&gt;现有机器去学习的方法倾向于只关注小型场景下的准确性等度量标准，而忽视了大型场景下模型表示层面的相似性问题。&lt;h4&gt;目的&lt;/h4&gt;为了弥补这一不足，本文提出了一种新的评估框架，采用大规模场景下基于特征表示的评价方法来检验现有的去学习方法是否真正地从模型表现层面上消除了目标遗忘数据的影响。&lt;h4&gt;方法&lt;/h4&gt;论文通过引入转移学习视角下的新评估设定，强调了被删除的数据集类别与下游任务类别的语义相似性，并且要求模型在经过去学习操作后产生的特征表示需要显著不同于原始模型的特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;当前最先进的机器去学习技术要么完全破坏了模型的表现质量，要么仅修改分类器（即最后一层），从而能够获得优异的逻辑损失指标，但仍然与原模型具有较高的表现相似性。&lt;h4&gt;结论&lt;/h4&gt;该研究通过构建全面基准测试来填补理论上的机器去学习和实际应用场景之间的差距，并为未来开发有效且真实的去学习方法奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：Machine unlearning是移除训练模型中的特定数据点的过程，同时保持对保留数据的性能不变，以解决隐私或法律要求。尽管这一点很重要，但现有的去学习评估通常集中在小规模场景下的基于逻辑的度量标准（例如准确性）上。我们观察到，这可能导致在真实世界场景下对去学习方法的安全感出现虚假现象。在这篇论文中，我们进行了新的全面评价，在大规模场景下采用未学模型的表现表示为基础的评价来验证去学习方法是否真正从模型表现层面上消除了目标遗忘数据的影响。我们的分析揭示了当前最先进的去学习方法要么完全破坏了模型的表现质量，要么仅修改分类器（即最后一层），从而在保持原始模型显著相似的表现的同时获得更优的基于逻辑度量标准的结果。此外，我们还引入了一个新的从转移学习视角出发的未学评估设置，在此设置中忘记集类别与下游任务类具有语义相似性，这要求特征表示需要显著不同于原始模型。我们的全面基准不仅解决了理论上的机器去学习和实际应用场景之间的关键差距，并且为未来开发有效且真实的去学习方法奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine unlearning is a process to remove specific data points from a trainedmodel while maintaining the performance on retain data, addressing privacy orlegal requirements. Despite its importance, existing unlearning evaluationstend to focus on logit-based metrics (i.e., accuracy) under small-scalescenarios. We observe that this could lead to a false sense of security inunlearning approaches under real-world scenarios. In this paper, we conduct anew comprehensive evaluation that employs representation-based evaluations ofthe unlearned model under large-scale scenarios to verify whether theunlearning approaches genuinely eliminate the targeted forget data from themodel's representation perspective. Our analysis reveals that currentstate-of-the-art unlearning approaches either completely degrade therepresentational quality of the unlearned model or merely modify the classifier(i.e., the last layer), thereby achieving superior logit-based evaluationmetrics while maintaining significant representational similarity to theoriginal model. Furthermore, we introduce a novel unlearning evaluation setupfrom a transfer learning perspective, in which the forget set classes exhibitsemantic similarity to downstream task classes, necessitating that featurerepresentations diverge significantly from those of the original model. Ourcomprehensive benchmark not only addresses a critical gap between theoreticalmachine unlearning and practical scenarios, but also establishes a foundationto inspire future research directions in developing genuinely effectiveunlearning methodologies.</description>
      <author>example@mail.com (Yongwoo Kim, Sungmin Cha, Donghyun Kim)</author>
      <guid isPermaLink="false">2503.06991v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Keeping Representation Similarity in Finetuning for Medical Image Analysis</title>
      <link>http://arxiv.org/abs/2503.07399v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的微调方法RepSim，该方法通过基于相似性不变性的可学习正交流形约束来最小化预训练表示和微调后表示之间的距离。&lt;h4&gt;背景&lt;/h4&gt;大规模自然图像上预训练的基础模型被广泛用于通过微调适应医学影像分析。这些预训练表示捕捉到的通用、稳健且泛化的特征可以在下游任务中重用，但它们在后续微调过程中逐渐消失，并伴随基础模型原始能力（如泛化能力）退化。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探讨如何在有效适应下游任务的同时保持预训练表示的良好保存。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的微调技术RepSim，该技术通过基于相似性不变性的可学习正交流形约束最小化了预训练表示和微调后表示之间的距离。&lt;h4&gt;主要发现&lt;/h4&gt;相较于标准的全量微调方法（如full finetuning），本文的方法在保持竞争性精度的同时提高了表示相似度超过30%，并且在五个医学影像分类数据集上降低了42%的尖锐度。&lt;h4&gt;结论&lt;/h4&gt;RepSim方法能够在有效适应下游任务的同时，更好地保留预训练的基础模型中的表示特性，并且改善了基础模型的泛化能力。该研究为理解基于大规模自然图像预训练如何应用于特定领域问题提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;大规模自然图象上进行预训练的基础模型已经被广泛用于通过微调来适应医学影像分析，这主要归因于预训练表示捕捉到了通用、稳健和可泛化的特征，这些可以在下游任务中重用。然而，这些表示被发现会在后续的微调过程中逐渐消失，并且伴随着基础模型原始能力（例如泛化能力）的退化。在本文中，我们提出可以很好地保存预训练表示的同时依然能够有效适应下游任务的方法。通过引入一种新的微调方法RepSim来研究这一问题，该方法通过基于相似性不变性的可学习正交流形约束最小化了预训练和微调后表示之间的距离。与标准的全量微调方法相比，我们的方法提高了30%以上的表示相似度并保持竞争力准确率，在五个医学图像分类数据集上降低了42%的尖锐度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models pretrained on large-scale natural images have been widelyused to adapt to medical image analysis through finetuning. This is largelyattributed to pretrained representations capturing universal, robust, andgeneralizable features, which can be reutilized by downstream tasks. However,these representations are later found to gradually vanish during finetuning,accompanied by a degradation of foundation model's original abilities, e.g.,generalizability. In this paper, we argue that pretrained representations canbe well preserved while still effectively adapting to downstream tasks. Westudy this by proposing a new finetuning method RepSim, which minimizes thedistance between pretrained and finetuned representations via constraininglearnable orthogonal manifold based on similarity invariance. Compared tostandard finetuning methods, e.g., full finetuning, our method improvesrepresentation similarity by over 30% while maintaining competitive accuracy,and reduces sharpness by 42% across five medical image classification datasets.The code will be released.</description>
      <author>example@mail.com (Wenqiang Zu, Shenghao Xie, Hao Chen, Yiming Liang, Lei Ma)</author>
      <guid isPermaLink="false">2503.07399v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>RS2V-L: Vehicle-Mounted LiDAR Data Generation from Roadside Sensor Observations</title>
      <link>http://arxiv.org/abs/2503.07085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'背景': '端到端的自动驾驶解决方案已经成为自动驾驶研究中的主流范式，但这些方法主要依赖于单车数据收集进行模型训练和优化。', '目的': '为了缓解这些限制，我们引入了RS2V-L框架，该框架能够从路边传感器观测中重建并合成车载激光雷达数据。', '方法': '我们的方法通过利用目标车辆的相对姿态将路边激光雷达点云转换为车载激光雷达坐标系，并通过虚拟激光雷达建模、点云分类和重采样技术生成高保真度的车载激光雷达数据。', '主要发现': '实验表明，与KITTI数据集相结合并融入生成的数据后，3D物体检测精度提高了超过30%，并且提升了端到端自动驾驶数据生成效率超过一个数量级。', '结论': '该方法的有效性得到了验证，并且在减少对昂贵的车载数据收集依赖的同时，还增强了自动驾驶模型的鲁棒性。'}&lt;h4&gt;翻译&lt;/h4&gt;End-to-end 自动驾驶解决方案已经成为自动驾驶研究中的主流范式，但这些方法主要依赖于单车数据收集进行模型训练和优化，导致了诸如高昂的数据获取与标注成本、关键驾驶场景稀缺以及碎片化数据阻碍模型泛化等问题。为了缓解这些问题，我们引入了一个新框架RS2V-L，用于从路边传感器观测中重建并合成车载激光雷达数据。具体来说，我们的方法通过利用目标车辆的相对姿态将路边激光雷达点云转换为车载激光雷达坐标系，并通过虚拟激光雷达建模、点云分类和重采样技术生成高保真度的车载激光雷达数据。据我们所知，这是首次尝试从路边传感器输入重建车载激光雷达数据的方法。实验结果表明，在与KITTI数据集结合并融入生成的数据后，3D物体检测精度提高了超过30%，并且提升了端到端自动驾驶数据生成效率超过一个数量级。这些发现强有力地验证了提出方法的有效性，并强调了其在减少对昂贵的车载数据收集依赖的同时提升自动驾驶模型鲁棒性的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end autonomous driving solutions, which process multi-modal sensorydata to directly generate refined control commands, have become a dominantparadigm in autonomous driving research. However, these approachespredominantly depend on single-vehicle data collection for model training andoptimization, resulting in significant challenges such as high data acquisitionand annotation costs, the scarcity of critical driving scenarios, andfragmented datasets that impede model generalization. To mitigate theselimitations, we introduce RS2V-L, a novel framework for reconstructing andsynthesizing vehicle-mounted LiDAR data from roadside sensor observations.Specifically, our method transforms roadside LiDAR point clouds into thevehicle-mounted LiDAR coordinate system by leveraging the target vehicle'srelative pose. Subsequently, high-fidelity vehicle-mounted LiDAR data issynthesized through virtual LiDAR modeling, point cloud classification, andresampling techniques. To the best of our knowledge, this is the first approachto reconstruct vehicle-mounted LiDAR data from roadside sensor inputs.Extensive experimental evaluations demonstrate that incorporating the generateddata into model training-complementing the KITTI dataset-enhances 3D objectdetection accuracy by over \text{30\%} while improving the efficiency ofend-to-end autonomous driving data generation by more than an order ofmagnitude. These findings strongly validate the effectiveness of the proposedmethod and underscore its potential in reducing dependence on costlyvehicle-mounted data collection while improving the robustness of autonomousdriving models.</description>
      <author>example@mail.com (Ruidan Xing, Runyi Huang, Qing Xu, Lei He)</author>
      <guid isPermaLink="false">2503.07085v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for LQR Control</title>
      <link>http://arxiv.org/abs/2503.06755v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种适用于线性二次调节器（LQR）控制的迁移学习框架，该框架利用源系统的数据来推断目标系统的行为。&lt;h4&gt;背景&lt;/h4&gt;在控制系统中，当目标系统的动力学特性未知时，通常需要大量的实验数据来进行控制器的设计。然而，在某些情况下，可能只能获得目标系统较短的脉冲响应轨迹。&lt;h4&gt;目的&lt;/h4&gt;本文研究如何通过迁移学习框架，利用已知多个不同动态特性的源系统的脉冲响应来推断和设计目标系统的LQR控制器。&lt;h4&gt;方法&lt;/h4&gt;该框架首先识别出一种可以转移的模态集，然后根据从源系统中获得的数据重建目标系统的脉冲响应，从而减少对目标系统数据的需求量。&lt;h4&gt;主要发现&lt;/h4&gt;通过这种方法，仅需目标系统的n+1个样本（其中n为系统的维度）即可学习LQR控制器，这大大减少了所需的数据量。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，在特定条件下可以显著减少获取数据的成本和时间，提高控制设计的效率。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们探讨了一种针对线性二次调节器（LQR）控制的迁移学习框架，该框架可以在仅提供目标系统的短脉冲响应轨迹并且其系统动力学未知的情况下运作。此外，在这种情况下，还提供了来自具有不同动态特性的$N$个源系统的脉冲响应。研究表明，可以从足够长的脉冲响应轨迹中学习到LQR控制器，并通过利用来自源系统和目标系统的可用数据来识别可转移的模式集，从而重建出目标系统的脉冲响应以进行控制器设计。借助于源系统的数据，我们展示了仅需从目标系统获取n+1（其中n为系统维度）个样本即可学习到LQR控制器，这在很大程度上减少了所需的数据量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we study a transfer learning framework for Linear  Quadratic Regulator (LQR) control, where (i) the dynamics of the  system of interest (target system) are unknown and only a short  trajectory of impulse responses from the target system is provided,  and (ii) impulse responses are available from $N$ source systems  with different dynamics. We show that the LQR controller can be  learned from a sufficiently long trajectory of impulse  responses. Further, a transferable mode set can be identified using  the available data from source systems and the target system,  enabling the reconstruction of the target system's impulse responses  for controller design. By leveraging data from the source systems we  demonstrate that only n+1 (n being the system dimension) samples  of data from the target system are needed to learn the LQR  controller, this yields a significant reduction of the required  data.</description>
      <author>example@mail.com (Taosha Guo, Fabio Pasqualetti)</author>
      <guid isPermaLink="false">2503.06755v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight Multimodal Artificial Intelligence Framework for Maritime Multi-Scene Recognition</title>
      <link>http://arxiv.org/abs/2503.06978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 4 figures, submitted to Engineering Applications of  Artificial Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了用于增强智能海洋机器人能力的海上多场景识别技术，尤其在海洋保护、环境监测和灾害响应等领域。然而，此任务由于环境干扰和复杂海景导致图像质量下降而面临挑战。&lt;h4&gt;背景&lt;/h4&gt;海上多场景识别对于提高智能海洋机器人的功能至关重要，尤其是在海洋保护、环境监测和灾难应对方面。现有的单一视觉模型难以克服这些问题。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的多模态AI框架来解决这些限制，该框架结合了图像数据、文本描述以及由多模态大型语言模型生成的分类向量。&lt;h4&gt;方法&lt;/h4&gt;所提出的框架使用高效的多模式融合机制，在复杂的海上环境中进一步增强模型的鲁棒性和适应性。此外还采用了激活感知权重量化技术以优化资源受限平台上的部署。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该模型实现了98%的准确率，超过了之前的SOTA模型3.5%，同时将模型大小缩小到68.75MB且仅损失0.5%的精度。&lt;h4&gt;结论&lt;/h4&gt;这项工作为实时海上场景识别提供了一种高性能解决方案，使自主水面车辆能够在资源受限条件下支持环境监测和灾害响应。&lt;h4&gt;翻译&lt;/h4&gt;海事多场景识别对于提升智能海洋机器人的能力至关重要，特别是在海洋保护、环境监控及灾害应对等领域。然而，由于海水条件差而导致的图像质量下降以及复杂海洋场景对精确识别的要求，该任务面临挑战。单靠视觉模型无法有效解决这些问题。本文提出了一种新的多模态AI框架，结合了图像数据和由大型语言模型生成的文字描述与分类向量，提高了语义理解和识别精度。实验表明，模型准确率高达98%，优于现有最先进水平3.5%；同时将模型压缩至68.75MB，仅有0.5%的精度下降，显著减少了计算开销。该工作为资源受限条件下的海事场景实时识别提供了一种高性能方案，使自主水面车辆能够在这些条件下支持环境监测和灾害应对。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Maritime Multi-Scene Recognition is crucial for enhancing the capabilities ofintelligent marine robotics, particularly in applications such as marineconservation, environmental monitoring, and disaster response. However, thistask presents significant challenges due to environmental interference, wheremarine conditions degrade image quality, and the complexity of maritime scenes,which requires deeper reasoning for accurate recognition. Pure vision modelsalone are insufficient to address these issues. To overcome these limitations,we propose a novel multimodal Artificial Intelligence (AI) framework thatintegrates image data, textual descriptions and classification vectorsgenerated by a Multimodal Large Language Model (MLLM), to provide richersemantic understanding and improve recognition accuracy. Our framework employsan efficient multimodal fusion mechanism to further enhance model robustnessand adaptability in complex maritime environments. Experimental results showthat our model achieves 98$\%$ accuracy, surpassing previous SOTA models by3.5$\%$. To optimize deployment on resource-constrained platforms, we adoptactivation-aware weight quantization (AWQ) as a lightweight technique, reducingthe model size to 68.75MB with only a 0.5$\%$ accuracy drop while significantlylowering computational overhead. This work provides a high-performance solutionfor real-time maritime scene recognition, enabling Autonomous Surface Vehicles(ASVs) to support environmental monitoring and disaster response inresource-limited settings.</description>
      <author>example@mail.com (Xinyu Xi, Hua Yang, Shentai Zhang, Yijie Liu, Sijin Sun, Xiuju Fu)</author>
      <guid isPermaLink="false">2503.06978v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network for Location- and Orientation-Assisted mmWave Beam Alignment</title>
      <link>http://arxiv.org/abs/2503.06943v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于图神经网络（GNN）的波束选择方法，旨在减少大规模MIMO系统中定位和方向辅助波束对准过程中的训练负担并提高对准精度。&lt;h4&gt;背景&lt;/h4&gt;在大规模多输入多输出（MIMO）系统中，使用深度神经网络进行位置和方位辅助波束对准时面临的主要瓶颈是大量的训练开销和显著的性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于图神经网络的方法以降低训练负担并提高波束对准精度。&lt;h4&gt;方法&lt;/h4&gt;通过利用GNN的强大表达能力和少量可训练参数，根据波束方向之间的角度相关性建立图形，并使用GNN捕捉相邻波束之间信道的相关性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法只需要现有DNN算法所需数据集大小的20%就能达到相同的准确性，在使用相同数据集时，Top-1准确率提高了10%。&lt;h4&gt;结论&lt;/h4&gt;通过引入图神经网络模型来处理大规模MIMO系统中的波束对准问题，不仅可以减少训练时间和资源需求，还可以显著提高对准精度。&lt;h4&gt;翻译&lt;/h4&gt;在大规模多输入多输出（MIMO）系统中，深度神经网络进行位置和方向辅助波束对准时面临的挑战是庞大的训练开销和性能退化。本文提出了一种基于图神经网络的波束选择方法，利用GNN强大的表达能力和少量可训练参数，根据波束之间角度相关性建立图形模型，并使用该模型加速学习过程并提高波束对准精度。与现有的DNN算法相比，这种方法在达到同样准确性的前提下只需要20%的数据集大小，在相同数据集中Top-1准确性提高了10%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In massive multi-input multi-output (MIMO) systems, the main bottlenecks oflocation- and orientation-assisted beam alignment using deep neural networks(DNNs) are large training overhead and significant performance degradation.This paper proposes a graph neural network (GNN)-based beam selection approachthat reduces the training overhead and improves the alignment accuracy, bycapitalizing on the strong expressive ability and few trainable parameters ofGNN. The channels of beams are correlated according to the beam direction.Therefore, we establish a graph according to the angular correlation betweenbeams and use GNN to capture the channel correlation between adjacent beams,which helps accelerate the learning process and enhance the beam alignmentperformance. Compared to existing DNN-based algorithms, the proposed methodrequires only 20\% of the dataset size to achieve equivalent accuracy andimproves the Top-1 accuracy by 10\% when using the same dataset.</description>
      <author>example@mail.com (Yuzhu Lei, Qiqi Xiao, Yinghui He, Guanding Yu)</author>
      <guid isPermaLink="false">2503.06943v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement for Transformers in Large-Scale Time Series Modeling</title>
      <link>http://arxiv.org/abs/2503.06121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该论文提出了一种利用RWKV-7架构改进时间序列模型的方法，通过将RWKV-7的时间混合和通道混合组件集成到Timer模型中，实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;时间序列模型在处理大规模和复杂数据集时面临挑战，需要创新方法以应对时间和空间的复杂性。尽管研究人员已经尝试了诸如Transformer、LSTM和GRU等架构来解决这些问题，但仍需进一步探索。&lt;h4&gt;目的&lt;/h4&gt;引入RWKV-7架构并将其应用于时间序列预测中，通过结合元学习机制优化模型状态更新过程，从而提高性能效率。&lt;h4&gt;方法&lt;/h4&gt;将RWKV-7的时间混合和通道混合组件整合进基于Transformer的时间序列模型Timer中。利用这种新颖的方法改进了模型的训练速度及准确性。&lt;h4&gt;主要发现&lt;/h4&gt;该研究展示了相较于传统模型架构，集成RWKV-7组件后的模型实现了性能上的显著提升（约1.13到43.3倍），同时减少了大约95%的参数量，并且缩短了4.5倍的训练时间。&lt;h4&gt;结论&lt;/h4&gt;通过将RWKV-7技术应用于时间序列分析领域，可以获得更好的效果和更高效的计算资源利用。此研究为未来的时间序列建模提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;时间序列模型在处理大规模复杂数据集时面临挑战，需要创新方法来应对这些困难。该论文提出了一种使用RWKV-7的方法，通过将它的状态更新机制中加入元学习，以及将其时间混合和通道混合组件整合进基于Transformer的时间序列模型Timer中，实现了显著的性能提升，并大幅减少了训练时间和参数量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series models face significant challenges in scaling to handle large andcomplex datasets, akin to the scaling achieved by large language models (LLMs).The unique characteristics of time series data and the computational demands ofmodel scaling necessitate innovative approaches. While researchers haveexplored various architectures such as Transformers, LSTMs, and GRUs to addressthese challenges, we propose a novel solution using RWKV-7, which incorporatesmeta-learning into its state update mechanism. By integrating RWKV-7's time mixand channel mix components into the transformer-based time series model Timer,we achieve a substantial performance improvement of approximately 1.13 to 43.3xand a 4.5x reduction in training time with 1/23 parameters, all while utilizingfewer parameters. Our code and model weights are publicly available for furtherresearch and development at https://github.com/Alic-Li/BlackGoose_Rimer.</description>
      <author>example@mail.com (Li weile, Liu Xiao)</author>
      <guid isPermaLink="false">2503.06121v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding</title>
      <link>http://arxiv.org/abs/2503.07413v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;REF-VLM是一种用于统一训练各种视觉解码任务的端到端框架，通过引入基于三元组的指代范式（TRP），解决了复杂视觉解码场景中的挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态大规模语言模型在经过大尺度数据集的训练后，在多种视觉-语言任务上展示了强大的零样本能力。但是，对于像语义分割和关键点检测这样的密集预测任务，仅通过文本输出表示时，多模态大型语言模型面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出REF-VLM框架以统一训练各种视觉解码任务，并构建VT-Instruct数据集用于改进多任务学习和跨粒度场景的适应性。&lt;h4&gt;方法&lt;/h4&gt;引入基于三元组的指代范式（TRP），采用符号分隔符来强化结构化表示的学习，增强模型输出的可解析性和解释性；构造了包含超过1亿个样本的大型多任务数据集VT-Instruct。&lt;h4&gt;主要发现&lt;/h4&gt;REF-VLM在多个标准基准测试中优于其他多模态大型语言模型。通过使用不同的视觉提示和可视单元生成各种类型的任务，大大扩展了REF-VLM的应用范围。&lt;h4&gt;结论&lt;/h4&gt;REF-VLM及其相关数据集和实验表明，在处理密集预测任务时具有优异的性能，能够促进更广泛应用场景下的多模态研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要主要介绍了REF-VLM框架及VT-Instruct数据集的研究内容、方法与成果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/MacavityT/REF-VLM&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) demonstrate robust zero-shotcapabilities across diverse vision-language tasks after training on mega-scaledatasets. However, dense prediction tasks, such as semantic segmentation andkeypoint detection, pose significant challenges for MLLMs when representedsolely as text outputs. Simultaneously, current MLLMs utilizing latentembeddings for visual task decoding generally demonstrate limited adaptabilityto both multi-task learning and multi-granularity scenarios. In this work, wepresent REF-VLM, an end-to-end framework for unified training of various visualdecoding tasks. To address complex visual decoding scenarios, we introduce theTriplet-Based Referring Paradigm (TRP), which explicitly decouples threecritical dimensions in visual decoding tasks through a triplet structure:concepts, decoding types, and targets. TRP employs symbolic delimiters toenforce structured representation learning, enhancing the parsability andinterpretability of model outputs. Additionally, we construct Visual-TaskInstruction Following Dataset (VTInstruct), a large-scale multi-task datasetcontaining over 100 million multimodal dialogue samples across 25 task types.Beyond text inputs and outputs, VT-Instruct incorporates various visual promptssuch as point, box, scribble, and mask, and generates outputs composed of textand visual units like box, keypoint, depth and mask. The combination ofdifferent visual prompts and visual units generates a wide variety of tasktypes, expanding the applicability of REF-VLM significantly. Both qualitativeand quantitative experiments demonstrate that our REF-VLM outperforms otherMLLMs across a variety of standard benchmarks. The code, dataset, and demoavailable at https://github.com/MacavityT/REF-VLM.</description>
      <author>example@mail.com (Yan Tai, Luhao Zhu, Zhiqiang Chen, Ynan Ding, Yiying Dong, Xiaohong Liu, Guodong Guo)</author>
      <guid isPermaLink="false">2503.07413v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Benchmark Dataset and Model for Crop Disease Diagnosis</title>
      <link>http://arxiv.org/abs/2503.06973v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECCV 2024 (14 pages, 8 figures)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了作物疾病领域多模态（CDDM）数据集，旨在通过应用多模态学习技术推动农业研究的发展。&lt;h4&gt;背景&lt;/h4&gt;目前对话生成AI主要集中在文本交互上，在利用图像-文本大数据以提升农作物病害诊断方面探索不足。&lt;h4&gt;目的&lt;/h4&gt;创建一个集成视觉和文本数据的资源库CDDM，用于开发能够提供精准建议给农民和技术人员的问答系统。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含137,000张不同作物疾病图像和100万条问答对的数据集；通过微调最先进的多模态模型，并采用低秩适配（LoRA）策略同时调整视觉编码器、适配器和语言模型来演示数据集的实用性。&lt;h4&gt;主要发现&lt;/h4&gt;应用CDDM数据集显著提高了作物病害诊断的准确性和有效性，展示了多模态AI技术在农业领域的巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;所提出的贡献包括一个基准测试的数据集以及微调策略，旨在促进更进一步的研究并将先进的AI技术与实际农业生产相结合。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容描述了CDDM数据集的创建及其如何通过应用最先进的多模态模型和低秩适配方法提升作物病害诊断水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-73016-0_10&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While conversational generative AI has shown considerable potential inenhancing decision-making for agricultural professionals, its exploration haspredominantly been anchored in text-based interactions. The evolution ofmultimodal conversational AI, leveraging vast amounts of image-text data fromdiverse sources, marks a significant stride forward. However, the applicationof such advanced vision-language models in the agricultural domain,particularly for crop disease diagnosis, remains underexplored. In this work,we present the crop disease domain multimodal (CDDM) dataset, a pioneeringresource designed to advance the field of agricultural research through theapplication of multimodal learning techniques. The dataset comprises 137,000images of various crop diseases, accompanied by 1 million question-answer pairsthat span a broad spectrum of agricultural knowledge, from diseaseidentification to management practices. By integrating visual and textual data,CDDM facilitates the development of sophisticated question-answering systemscapable of providing precise, useful advice to farmers and agriculturalprofessionals. We demonstrate the utility of the dataset by finetuningstate-of-the-art multimodal models, showcasing significant improvements in cropdisease diagnosis. Specifically, we employed a novel finetuning strategy thatutilizes low-rank adaptation (LoRA) to finetune the visual encoder, adapter andlanguage model simultaneously. Our contributions include not only the datasetbut also a finetuning strategy and a benchmark to stimulate further research inagricultural technology, aiming to bridge the gap between advanced AItechniques and practical agricultural applications. The dataset is available athttps: //github.com/UnicomAI/UnicomBenchmark/tree/main/CDDMBench.</description>
      <author>example@mail.com (Xiang Liu, Zhaoxiang Liu, Huan Hu, Zezhou Chen, Kohou Wang, Kai Wang, Shiguo Lian)</author>
      <guid isPermaLink="false">2503.06973v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Clustering-based Meta Bayesian Optimization with Theoretical Guarantee</title>
      <link>http://arxiv.org/abs/2503.06093v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at PAKDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种可扩展且鲁棒的元贝叶斯优化（meta-BO）方法，以解决异构和大规模元任务中的关键挑战。&lt;h4&gt;背景&lt;/h4&gt;贝叶斯优化是解决黑箱优化问题的有效方法。在实际场景中，优化通常涉及多个函数，并强调利用以前任务的数据和学习功能来提高当前任务的效率。然而，在面对大量的历史任务时，meta-BO 方法面临可扩展性问题。&lt;h4&gt;目的&lt;/h4&gt;设计一种元贝叶斯优化（meta-BO）方法，该方法可以在异构且大规模的任务中有效工作。&lt;h4&gt;方法&lt;/h4&gt;(1) 有效地将转移的元函数划分为高度同质的簇；(2) 学习每个簇中的几何基替代原型以捕获结构模式；(3) 在线阶段使用基于统计距离的权重策略自适应合成元先验。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在真实世界的超参数优化（HPO）任务中表现出色，并且具有理论保证来克服挑战。&lt;h4&gt;结论&lt;/h4&gt;该工作展示了一种新的meta-BO 方法，在大规模和异构的环境中表现出了鲁棒性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bayesian Optimization (BO) is a well-established method for addressingblack-box optimization problems. In many real-world scenarios, optimizationoften involves multiple functions, emphasizing the importance of leveragingdata and learned functions from prior tasks to enhance efficiency in thecurrent task. To expedite convergence to the global optimum, recent studieshave introduced meta-learning strategies, collectively referred to as meta-BO,to incorporate knowledge from historical tasks. However, in practical settings,the underlying functions are often heterogeneous, which can adversely affectoptimization performance for the current task. Additionally, when the number ofhistorical tasks is large, meta-BO methods face significant scalabilitychallenges. In this work, we propose a scalable and robust meta-BO methoddesigned to address key challenges in heterogeneous and large-scale meta-tasks.Our approach (1) effectively partitions transferred meta-functions into highlyhomogeneous clusters, (2) learns the geometry-based surrogate prototype thatcapture the structural patterns within each cluster, and (3) adaptivelysynthesizes meta-priors during the online phase using statisticaldistance-based weighting policies. Experimental results on real-worldhyperparameter optimization (HPO) tasks, combined with theoretical guarantees,demonstrate the robustness and effectiveness of our method in overcoming thesechallenges.</description>
      <author>example@mail.com (Khoa Nguyen, Viet Huynh, Binh Tran, Tri Pham, Tin Huynh, Thin Nguyen)</author>
      <guid isPermaLink="false">2503.06093v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models</title>
      <link>http://arxiv.org/abs/2503.07389v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近期在文本到图像扩散模型方面的进展实现了逼真的图像生成，但同时也增加了产生恶意内容的风险，如不适宜的内容。&lt;h4&gt;背景&lt;/h4&gt;现有研究难以完全消除嵌入在提示中的隐含不良概念（例如比喻表达或对抗性提示），同时保持模型的正常生成能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为TRCE的概念擦除方法，通过两阶段策略有效地平衡可靠的擦除和知识保留之间的关系。&lt;h4&gt;方法&lt;/h4&gt;{'第一阶段': '识别并优化跨注意力层以将恶意文本提示映射为概念安全且语境相似的提示。', '第二阶段': '在扩散模型采样轨迹的确定性特性下，通过对比学习引导早期去噪预测远离不安全的方向，从而进一步避免生成恶意内容。'}&lt;h4&gt;主要发现&lt;/h4&gt;TRCE方法有效擦除了不良概念，同时更好地保持了模型的原始生成能力。&lt;h4&gt;结论&lt;/h4&gt;在多个不良概念擦除基准上的全面评估表明，TRCE有效地消除了恶意概念并保留了模型的核心功能。&lt;h4&gt;翻译&lt;/h4&gt;该研究提出了一种名为TRCE的方法来解决扩散模型生成恶意内容的问题，通过两个阶段的有效策略实现了可靠的概念擦除和知识保存之间的平衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in text-to-image diffusion models enable photorealistic imagegeneration, but they also risk producing malicious content, such as NSFWimages. To mitigate risk, concept erasure methods are studied to facilitate themodel to unlearn specific concepts. However, current studies struggle to fullyerase malicious concepts implicitly embedded in prompts (e.g., metaphoricalexpressions or adversarial prompts) while preserving the model's normalgeneration capability. To address this challenge, our study proposes TRCE,using a two-stage concept erasure strategy to achieve an effective trade-offbetween reliable erasure and knowledge preservation. Firstly, TRCE starts byerasing the malicious semantics implicitly embedded in textual prompts. Byidentifying a critical mapping objective(i.e., the [EoT] embedding), weoptimize the cross-attention layers to map malicious prompts to contextuallysimilar prompts but with safe concepts. This step prevents the model from beingoverly influenced by malicious semantics during the denoising process.Following this, considering the deterministic properties of the samplingtrajectory of the diffusion model, TRCE further steers the early denoisingprediction toward the safe direction and away from the unsafe one throughcontrastive learning, thus further avoiding the generation of maliciouscontent. Finally, we conduct comprehensive evaluations of TRCE on multiplemalicious concept erasure benchmarks, and the results demonstrate itseffectiveness in erasing malicious concepts while better preserving the model'soriginal generation ability. The code is available at:http://github.com/ddgoodgood/TRCE. CAUTION: This paper includes model-generatedcontent that may contain offensive material.</description>
      <author>example@mail.com (Ruidong Chen, Honglin Guo, Lanjun Wang, Chenyu Zhang, Weizhi Nie, An-An Liu)</author>
      <guid isPermaLink="false">2503.07389v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment</title>
      <link>http://arxiv.org/abs/2503.07334v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了Autoregressive Representation Alignment (ARRA)，这是一种新的训练框架，它使自回归LLM在不改变架构的情况下实现全局一致的文本到图像生成。&lt;h4&gt;背景&lt;/h4&gt;现有的方法需要复杂的架构重新设计来解决跨模态全局一致性问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需修改架构即可提升自回归语言模型（LLMs）进行图像生成能力的方法。&lt;h4&gt;方法&lt;/h4&gt;ARRA通过全局视觉对齐损失和混合标记&lt;HYBNEXT&gt;，将LLM隐藏状态与外部视觉基础模型的表示对齐。该混合标记同时强制执行局部下一个令牌预测和全局语义蒸馏约束。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证了ARRA具备即插即用的灵活性，并且在不同的训练设置下能够显著减少FID（Frechet Inception Distance）分数，特别是在医学图像生成领域表现出色。&lt;h4&gt;结论&lt;/h4&gt;ARRA展示了通过重新设计训练目标而不是架构创新可以解决跨模态全局一致性挑战。这种方法为推进自回归模型的发展提供了互补的范式，并且代码和模型将公开发布以促进自回归图像生成的研究进展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的训练框架，即Autoregressive Representation Alignment (ARRA)，它解锁了自回归LLM在不改变架构的情况下进行全局一致的文本到图像生成的能力。与需要复杂架构重新设计的先前工作不同，ARRA通过全球视觉对齐损失和混合令牌&lt;HYBNEXT&gt;将LLM隐藏状态与外部视觉基础模型的表示对齐。该令牌强制执行局部下一个令牌预测和全局语义蒸馏的双重约束，使LLMs在保留其原始自回归范式的同时隐式学习空间和上下文一致性。广泛的实验验证了ARRA的即插即用灵活性。当从文本生成仅有的LLM或随机初始化训练时，ARRA显著降低了高级自回归模型（如Chameleon和LlamaGen）在FID上的分数：25.5% (MIMIC-CXR)，8.8% (DeepEyeNet)，7.5% (ImageNet)。对于领域适应，ARRA将通用型LLM与专门化模型对齐（例如BioMedCLIP），相比直接微调医学成像，在FID上减少了18.6%的分数（MIMIC-CXR）。通过展示训练目标重新设计——而不仅仅是架构创新——可以解决跨模态全局一致性挑战，ARRA为推进自回归模型的发展提供了一种互补范式。代码和模型将发布以促进自回归图像生成的研究进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Autoregressive Representation Alignment (ARRA), a new trainingframework that unlocks global-coherent text-to-image generation inautoregressive LLMs without architectural changes. Unlike prior work thatrequires complex architectural redesigns, ARRA aligns LLM hidden states withvisual representations from external visual foundational models via a globalvisual alignment loss and a hybrid token, &lt;HYBNEXT&gt;. This token enforces dualconstraints: local next-token prediction and global semantic distillation,enabling LLMs to implicitly learn spatial and contextual coherence whileretaining their original autoregressive paradigm. Extensive experimentsvalidate ARRA's plug-and-play versatility. When training fromtext-generation-only LLMs or random initialization, ARRA reduces FID by 25.5%(MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressiveLLMs like Chameleon and LlamaGen, all without framework modifications. Fordomain adaption, ARRA aligns general-purpose LLMs with specialized models(e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning onmedical imaging (MIMIC-CXR). By demonstrating that training objective redesign-- not just architectural innovation -- can resolve cross-modal globalcoherence challenges, ARRA offers a complementary paradigm for advancingautoregressive models. Code and models will be released to advanceautoregressive image generation.</description>
      <author>example@mail.com (Xing Xie, Jiawei Liu, Ziyue Lin, Huijie Fan, Zhi Han, Yandong Tang, Liangqiong Qu)</author>
      <guid isPermaLink="false">2503.07334v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>SP3D: Boosting Sparsely-Supervised 3D Object Detection via Accurate Cross-Modal Semantic Prompts</title>
      <link>http://arxiv.org/abs/2503.06467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SP3D是一种新的增强策略，利用跨模态语义提示来改进稀疏监督下的三维物体检测任务。&lt;h4&gt;背景&lt;/h4&gt;在三维目标检测中，稀疏监督方法虽然能够取得接近完全监督的性能但当准确标注极其缺乏时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种策略以提升稀疏监督条件下3D对象检测器的表现能力。&lt;h4&gt;方法&lt;/h4&gt;{'CPST模块': '生成跨模态语义提示通过边界约束中心簇选择，产生高质量的种子点。', 'DCPG模块': '基于准确的语义提示，生成动态聚类伪标签以提供几何形状指导。', 'DS分数': '用于选择初始训练中的高质量监督信号。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在标注条件有限的情况下，SP3D可以显著提高稀疏监督检测器的表现；在零样本设置中，其性能超过了当前最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;通过引入跨模态语义提示增强策略，可以在稀疏标签条件下有效提升三维目标检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;最近，稀疏监督下的3D物体检测获得了大量关注，在只使用少量标注实例的情况下取得了接近完全监督的方法的性能。然而，当准确的标签极其缺乏时，这些方法面临着挑战。为此，本文提出了一种增强策略SP3D，利用大型多模态模型生成的跨模态语义提示来增强具备强大特征区分能力的3D检测器。具体而言，作者开发了一个Confident Points Semantic Transfer（CPST）模块，通过边界约束中心簇选择来产生准确的跨模态语义提示，并基于这些种子点设计了Dynamic Cluster Pseudo-label Generation（DCPG）模块以生成几何形状指导的伪监督信号；同时提出了Distribution Shape得分用于筛选高质量的初始训练信号。在KITTI数据集和Waymo Open Dataset上的实验验证表明，SP3D可以在有限标注条件下显著提高稀疏监督检测器的表现，并且在零样本设置中超过了当前最佳方法的表现水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, sparsely-supervised 3D object detection has gained great attention,achieving performance close to fully-supervised 3D objectors while requiringonly a few annotated instances. Nevertheless, these methods suffer challengeswhen accurate labels are extremely absent. In this paper, we propose a boostingstrategy, termed SP3D, explicitly utilizing the cross-modal semantic promptsgenerated from Large Multimodal Models (LMMs) to boost the 3D detector withrobust feature discrimination capability under sparse annotation settings.Specifically, we first develop a Confident Points Semantic Transfer (CPST)module that generates accurate cross-modal semantic prompts throughboundary-constrained center cluster selection. Based on these accurate semanticprompts, which we treat as seed points, we introduce a Dynamic ClusterPseudo-label Generation (DCPG) module to yield pseudo-supervision signals fromthe geometry shape of multi-scale neighbor points. Additionally, we design aDistribution Shape score (DS score) that chooses high-quality supervisionsignals for the initial training of the 3D detector. Experiments on the KITTIdataset and Waymo Open Dataset (WOD) have validated that SP3D can enhance theperformance of sparsely supervised detectors by a large margin under meagerlabeling conditions. Moreover, we verified SP3D in the zero-shot setting, whereits performance exceeded that of the state-of-the-art methods. The code isavailable at https://github.com/xmuqimingxia/SP3D.</description>
      <author>example@mail.com (Shijia Zhao, Qiming Xia, Xusheng Guo, Pufan Zou, Maoji Zheng, Hai Wu, Chenglu Wen, Cheng Wang)</author>
      <guid isPermaLink="false">2503.06467v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Non-vacuous Generalization Bounds for Deep Neural Networks without any modification to the trained models</title>
      <link>http://arxiv.org/abs/2503.07325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了两个新的关于深度神经网络测试误差的界限，这些界限仅基于训练集，并不需要对模型进行任何修改。这两个界限在大量现代预训练的Pytorch网络上进行了验证，并且是实际有效的。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络具有百万甚至数十亿参数，在有限训练集的基础上能很好地处理未见过的数据。然而，现有的理论解释往往缺乏非空（non-vacuous）的实际误差界限。&lt;h4&gt;目的&lt;/h4&gt;提出一个新的测试误差的界限方法，该方法仅依赖于训练数据并且不需要对模型进行任何修改。&lt;h4&gt;方法&lt;/h4&gt;提出了两个关于测试错误的新界线，这些界线仅基于训练集，并且不需对深度神经网络进行任何形式的压缩或量化等修改。&lt;h4&gt;主要发现&lt;/h4&gt;新提出的界限在大量现代预训练的Pytorch网络上进行了验证，并且是实际有效的。这是迄今为止已知的最大规模的有效误差界限，无需对模型进行任何调整。&lt;h4&gt;结论&lt;/h4&gt;新的测试错误界线为理解深度神经网络出色的性能提供了有意义的新视角，同时避免了现有方法需要修改或压缩模型的要求。&lt;h4&gt;翻译&lt;/h4&gt;深度学习中的神经网络在处理未见过的数据时表现出色。尽管有许多理论试图解释这种能力，但这些理论通常不能提供非空的（non-vacuous）测试误差界限，这意味着它们无法直接给出关于实际错误率的有效结论。最近的一些基于PAC-Bayes和互信息的方法显示出更好的潜力，但仍需要对模型进行严格的假设或修改才能得出有效结论。本文提出的新方法无需任何修改即可直接为大量现代预训练的深度神经网络提供非空的测试误差界限。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural network (NN) with millions or billions of parameters can performreally well on unseen data, after being trained from a finite training set.Various prior theories have been developed to explain such excellent ability ofNNs, but do not provide a meaningful bound on the test error. Some recenttheories, based on PAC-Bayes and mutual information, are non-vacuous and henceshow a great potential to explain the excellent performance of NNs. However,they often require a stringent assumption and extensive modification (e.g.compression, quantization) to the trained model of interest. Therefore, thoseprior theories provide a guarantee for the modified versions only. In thispaper, we propose two novel bounds on the test error of a model. Our boundsuses the training set only and require no modification to the model. Thosebounds are verified on a large class of modern NNs, pretrained by Pytorch onthe ImageNet dataset, and are non-vacuous. To the best of our knowledge, theseare the first non-vacuous bounds at this large scale, without any modificationto the pretrained models.</description>
      <author>example@mail.com (Khoat Than, Dat Phan)</author>
      <guid isPermaLink="false">2503.07325v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>OV-SCAN: Semantically Consistent Alignment for Novel Object Discovery in Open-Vocabulary 3D Object Detection</title>
      <link>http://arxiv.org/abs/2503.06435v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Open-vocabulary 3D物体检测旨在通过将传统3D目标检测器与视觉语言模型（VLM）结合，来识别点云场景中预定义训练标签集外的新型对象。&lt;h4&gt;背景&lt;/h4&gt;现有的方法试图通过三维和二维特征之间的跨模态对齐来进行开放词汇分类，但这种对齐方式由于生成对应3D和2D特征对时出现语义不一致而难以实现稳健性。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一挑战，本文提出了OV-SCAN框架，该框架旨在强制执行语义一致性对齐以发现新型对象。&lt;h4&gt;方法&lt;/h4&gt;OV-SCAN采用两种核心策略：一种是发现精确的3D注释；另一种是从3D注释、遮挡引起的或分辨率引起的噪声中过滤出低质量和损坏的配对。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的广泛实验表明，OV-SCAN实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过强制执行语义一致性对齐和改进的数据处理策略，OV-SCAN为开放词汇3D物体检测提供了有力的支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary 3D object detection for autonomous driving aims to detectnovel objects beyond the predefined training label sets in point cloud scenes.Existing approaches achieve this by connecting traditional 3D object detectorswith vision-language models (VLMs) to regress 3D bounding boxes for novelobjects and perform open-vocabulary classification through cross-modalalignment between 3D and 2D features. However, achieving robust cross-modalalignment remains a challenge due to semantic inconsistencies when generatingcorresponding 3D and 2D feature pairs. To overcome this challenge, we presentOV-SCAN, an Open-Vocabulary 3D framework that enforces Semantically ConsistentAlignment for Novel object discovery. OV-SCAN employs two core strategies:discovering precise 3D annotations and filtering out low-quality or corruptedalignment pairs (arising from 3D annotation, occlusion-induced, orresolution-induced noise). Extensive experiments on the nuScenes datasetdemonstrate that OV-SCAN achieves state-of-the-art performance.</description>
      <author>example@mail.com (Adrian Chow, Evelien Riddell, Yimu Wang, Sean Sedwards, Krzysztof Czarnecki)</author>
      <guid isPermaLink="false">2503.06435v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Deep Cut-informed Graph Embedding and Clustering</title>
      <link>http://arxiv.org/abs/2503.06635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;这篇论文提出了一个新的非GNN基础的图聚类框架DCGC，通过从图割视角研究图聚类问题，解决了现有基于GNN的方法在进行深度图聚类时遇到的表现塌陷的问题。&lt;h4&gt;背景&lt;/h4&gt;当前用于图聚类的主要方法大多依赖于图神经网络（GNN），但这些方法存在一个常见的表示坍缩问题。作者认为这个问题主要由两点引起：一是GNN模型的归纳偏差，即GNN倾向于为接近的节点生成相似的表示；二是传统的基于损失函数的方法旨在使所有样本更接近预学习的聚类中心。&lt;h4&gt;目的&lt;/h4&gt;提出一个新的非GNN基础的框架来解决图聚类中的表示坍缩问题，并实现更好的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为DCGC的深度图嵌入和聚类框架，该框架包括两个模块：（i）基于图割的编码；（ii）通过最优传输进行自监督图聚类。对于编码模块，作者提出了一种最小化联合归一化切割的目标来融合图形结构和属性。对于聚类模块，利用最优传输理论获得聚类分配。&lt;h4&gt;主要发现&lt;/h4&gt;DCGC框架能够有效地解决表示坍缩问题，并在实验中显示出了优于基准方法的性能。&lt;h4&gt;结论&lt;/h4&gt;从图割的角度提出的新框架（DCGC）为深度图聚类提供了一个简单但有效的解决方案，能够在不使用GNN的情况下实现更好的聚类效果。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要描述了基于图神经网络的方法在进行深度图聚类时存在表示坍缩的问题，并提出了一个新的非图神经网络基础的框架以解决这个问题。通过从图割视角研究问题并引入两个创新模块，该方法能够在不依赖GNN的情况下有效避免表示坍缩并且实现了更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph clustering aims to divide the graph into different clusters. Therecently emerging deep graph clustering approaches are largely built on graphneural networks (GNN). However, GNN is designed for general graph encoding andthere is a common issue of representation collapse in existing GNN-based deepgraph clustering algorithms. We attribute two main reasons for such issue: (i)the inductive bias of GNN models: GNNs tend to generate similar representationsfor proximal nodes. Since graphs often contain a non-negligible amount ofinter-cluster links, the bias results in error message passing and leads tobiased clustering; (ii) the clustering guided loss function: most traditionalapproaches strive to make all samples closer to pre-learned cluster centers,which cause a degenerate solution assigning all data points to a single labelthus make all samples and less discriminative. To address these challenges, weinvestigate graph clustering from a graph cut perspective and propose aninnovative and non-GNN-based Deep Cut-informed Graph embedding and Clusteringframework, namely DCGC. This framework includes two modules: (i) cut-informedgraph encoding; (ii) self-supervised graph clustering via optimal transport.For the encoding module, we derive a cut-informed graph embedding objective tofuse graph structure and attributes by minimizing their joint normalized cut.For the clustering module, we utilize the optimal transport theory to obtainthe clustering assignments, which can balance the guidance of proximity to thepre-learned cluster center. With the above two tailored designs, DCGC is moresuitable for the graph clustering task, which can effectively alleviate theproblem of representation collapse and achieve better performance. We conductextensive experiments to demonstrate that our method is simple but effectivecompared with benchmarks.</description>
      <author>example@mail.com (Zhiyuan Ning, Zaitian Wang, Ran Zhang, Ping Xu, Kunpeng Liu, Pengyang Wang, Chong Chen, Pengfei Wang, Yuanchun Zhou, Erik Cambria)</author>
      <guid isPermaLink="false">2503.06635v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>ConcreTizer: Model Inversion Attack via Occupancy Classification and Dispersion Control for 3D Point Cloud Restoration</title>
      <link>http://arxiv.org/abs/2503.06986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文研究了在自动驾驶汽车（AVs）中使用3D点云数据时的隐私问题，并提出了首个针对恢复3D点云场景的模型逆向攻击的深入分析。&lt;h4&gt;背景&lt;/h4&gt;随着3D点云数据在自主车辆中的广泛应用，人们对隐私的关注日益增加。现有的模型逆向攻击主要研究2D数据，而在3D点云上的应用则较少探索。&lt;h4&gt;目的&lt;/h4&gt;填补对基于3D点云模型逆向攻击的研究空白，并提出一种针对3D点云的简单而有效的逆向攻击方法（ConcreTizer）。&lt;h4&gt;方法&lt;/h4&gt;提出了名为ConcreTizer的模型，它通过引入Voxel Occupancy Classification来区分空和非空体素以及Dispersion-Controlled Supervision来缓解非空体素分散的问题。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，ConcreTizer能从受损的3D特征数据中准确恢复原始3D点云场景。该研究还强调了对3D数据逆向攻击的风险和构建稳健防御策略的需求。&lt;h4&gt;结论&lt;/h4&gt;研究表明，3D数据面临严重的信息泄露风险，并且需要更加关注如何开发有效的安全防护措施以保护隐私。&lt;h4&gt;翻译&lt;/h4&gt;随着自动驾驶汽车（AVs）中使用三维点云数据的增长，隐私问题变得越来越严峻。针对二维数据的模型逆向攻击已被广泛研究，但对于三维点云的应用却很少有人探讨。为了填补这一空白，我们提出了首个专注于恢复三维点云场景的模型逆向攻击深入研究。我们的分析揭示了独特的挑战、3D点云固有的稀疏性以及体素化后空和非空体素之间的模糊性，并且这些问题在特征提取层中的非空体素分散时会进一步加剧。为了解决这些挑战，我们引入了一种针对基于体素的三维点云数据设计的简单而有效的模型逆向攻击方法——ConcreTizer。实验证明，在常见的3D特征提取器和基准测试数据集（如KITTI和Waymo）上使用ConcreTizer可以准确恢复受损3D特征数据中的原始3D点云场景。我们的研究不仅揭示了三维数据对逆向攻击的脆弱性，还强调了急需开发出可靠的防御策略来保护隐私的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing use of 3D point cloud data in autonomous vehicles (AVs) hasraised serious privacy concerns, particularly due to the sensitive informationthat can be extracted from 3D data. While model inversion attacks have beenwidely studied in the context of 2D data, their application to 3D point cloudsremains largely unexplored. To fill this gap, we present the first in-depthstudy of model inversion attacks aimed at restoring 3D point cloud scenes. Ouranalysis reveals the unique challenges, the inherent sparsity of 3D pointclouds and the ambiguity between empty and non-empty voxels after voxelization,which are further exacerbated by the dispersion of non-empty voxels acrossfeature extractor layers. To address these challenges, we introduceConcreTizer, a simple yet effective model inversion attack designedspecifically for voxel-based 3D point cloud data. ConcreTizer incorporatesVoxel Occupancy Classification to distinguish between empty and non-emptyvoxels and Dispersion-Controlled Supervision to mitigate non-empty voxeldispersion. Extensive experiments on widely used 3D feature extractors andbenchmark datasets, such as KITTI and Waymo, demonstrate that ConcreTizerconcretely restores the original 3D point cloud scene from disrupted 3D featuredata. Our findings highlight both the vulnerability of 3D data to inversionattacks and the urgent need for robust defense strategies.</description>
      <author>example@mail.com (Youngseok Kim, Sunwook Hwang, Hyung-Sin Kim, Saewoong Bahk)</author>
      <guid isPermaLink="false">2503.06986v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>A Zero-shot Learning Method Based on Large Language Models for Multi-modal Knowledge Graph Embedding</title>
      <link>http://arxiv.org/abs/2503.07202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大语言模型的零样本学习框架（ZSLLM），用于多模态知识图谱嵌入表示学习，旨在通过文本提示来增强未见类别在多模态知识图中的嵌入表示。&lt;h4&gt;背景&lt;/h4&gt;零样本学习对于处理自然语言处理、图像分类和跨语言转换等任务中遇到的未知类别的挑战至关重要。当前的应用常常无法准确推断和处理涉及新关系或实体的新类别，限制了其在开放域场景中的扩展性和实用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于大语言模型（LLMs）的方法来解决多模态知识图谱嵌入表示学习中未见类别的语义信息迁移问题。&lt;h4&gt;方法&lt;/h4&gt;使用大型语言模型的推理能力通过文本提示充分利用未见过类别的文本模式信息，以实现跨不同模态之间的语义信息转移，并通过基于模型的学习增强MMKG中的未知类别嵌入表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法在多个真实世界数据集上的表现优于现有的最佳方法。&lt;h4&gt;结论&lt;/h4&gt;ZSLLM框架为多模态知识图谱的零样本学习提供了一种有效的解决方案，通过大语言模型的能力提高了未见类别的语义信息传输效果和嵌入表示质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-shot learning (ZL) is crucial for tasks involving unseen categories,such as natural language processing, image classification, and cross-lingualtransfer. Current applications often fail to accurately infer and handle newrelations or entities involving unseen categories, severely limiting theirscalability and practicality in open-domain scenarios. ZL learning faces thechallenge of effectively transferring semantic information of unseen categoriesin multi-modal knowledge graph (MMKG) embedding representation learning. Inthis paper, we propose ZSLLM, a framework for zero-shot embedding learning ofMMKGs using large language models (LLMs). We leverage textual modalityinformation of unseen categories as prompts to fully utilize the reasoningcapabilities of LLMs, enabling semantic information transfer across differentmodalities for unseen categories. Through model-based learning, the embeddingrepresentation of unseen categories in MMKG is enhanced. Extensive experimentsconducted on multiple real-world datasets demonstrate the superiority of ourapproach compared to state-of-the-art methods.</description>
      <author>example@mail.com (Bingchen Liu, Jingchen Li, Naixing Xu, Xin Li)</author>
      <guid isPermaLink="false">2503.07202v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>COMODO: Cross-Modal Video-to-IMU Distillation for Efficient Egocentric Human Activity Recognition</title>
      <link>http://arxiv.org/abs/2503.07259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;Egocentric视频模型在人类活动识别（HAR）中表现出色，但因高能耗、隐私问题及对光照条件的依赖性限制了其持续应用的可能性。相比之下，惯性测量单元（IMU）传感器提供了一种节能且保护隐私的选择，但由于缺乏大规模标注数据集，在下游任务中的泛化能力较弱。为了弥补这一差距，本文提出了COMODO框架，该框架能够从视频模态向IMU模态无监督地转移丰富的语义知识，无需依赖标签注释。&lt;h4&gt;背景&lt;/h4&gt;Egocentric视频模型虽然在人类活动识别中表现出色，但因其高能耗、隐私问题和对光照条件的依赖性限制了其实际应用。而IMU传感器作为替代方案提供了一种节能且保护隐私的选择，但是由于缺乏大规模标注数据集，导致泛化能力较弱。&lt;h4&gt;目的&lt;/h4&gt;为了提高IMU传感器在下游任务中的表现，本文提出一种新的无监督知识迁移框架COMODO，以改善其泛化能力和性能。&lt;h4&gt;方法&lt;/h4&gt;COMODO是一个跨模态的自监督蒸馏框架。利用预训练并冻结的视频编码器构建动态实例队列，使得视频和IMU嵌入特征分布对齐，通过从视频表示中提取知识来增强IMU编码器的能力，同时保持其在实际应用中的效率。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，COMODO框架能够显著提升下游分类任务的性能，在多个自视角人类活动识别数据集上实现了与完全监督微调模型相当或超过的结果。此外，该方法还表现出强大的跨数据集泛化能力，并且适用于多种视频和时间序列预训练模型。&lt;h4&gt;结论&lt;/h4&gt;COMODO框架提供了一种有效的方法来增强IMU传感器在无标签条件下的性能，同时保持其效率。这种方法不仅简单易用，而且具有广泛的应用潜力，未来可以利用更强大的教师和学生基础模型进行进一步的研究。&lt;h4&gt;翻译&lt;/h4&gt;Egocentric视频模型捕获了丰富的语义信息，并在人类活动识别（HAR）中表现出色。然而，它们的高能耗、隐私问题及对光照条件的依赖性限制了其在持续设备端应用中的可行性。相比之下，惯性测量单元（IMU）传感器提供了节能和保护隐私的选择，但由于缺少大规模标注数据集，导致下游任务中泛化能力较弱。为了弥补这一差距，我们提出了COMODO框架，这是一个跨模态自监督蒸馏框架，旨在从视频模态向IMU模态转移丰富语义知识，无需标签注释。通过利用预训练和冻结的视频编码器来构建动态实例队列，并对齐视频和IMU嵌入特征分布的方式，我们的方法使IMU编码器能够继承来自视频的丰富语义信息同时保持其在实际应用中的效率。实验结果表明，在多个自视角HAR数据集上，COMODO持续提高了下游分类性能并达到了与完全监督微调模型相媲美或超越的结果，并且表现出强大的跨数据集泛化能力。由于其简单性，我们的方法也适用于各种视频和时间序列预训练模型，在未来研究中具有利用更强大教师和学生基础模型的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Egocentric video-based models capture rich semantic information and havedemonstrated strong performance in human activity recognition (HAR). However,their high power consumption, privacy concerns, and dependence on lightingconditions limit their feasibility for continuous on-device recognition. Incontrast, inertial measurement unit (IMU) sensors offer an energy-efficient andprivacy-preserving alternative, yet they suffer from limited large-scaleannotated datasets, leading to weaker generalization in downstream tasks. Tobridge this gap, we propose COMODO, a cross-modal self-supervised distillationframework that transfers rich semantic knowledge from the video modality to theIMU modality without requiring labeled annotations. COMODO leverages apretrained and frozen video encoder to construct a dynamic instance queue,aligning the feature distributions of video and IMU embeddings. By distillingknowledge from video representations, our approach enables the IMU encoder toinherit rich semantic information from video while preserving its efficiencyfor real-world applications. Experiments on multiple egocentric HAR datasetsdemonstrate that COMODO consistently improves downstream classificationperformance, achieving results comparable to or exceeding fully supervisedfine-tuned models. Moreover, COMODO exhibits strong cross-datasetgeneralization. Benefiting from its simplicity, our method is also generallyapplicable to various video and time-series pre-trained models, offering thepotential to leverage more powerful teacher and student foundation models infuture research. The code is available at https://github.com/Breezelled/COMODO .</description>
      <author>example@mail.com (Baiyu Chen, Wilson Wongso, Zechen Li, Yonchanok Khaokaew, Hao Xue, Flora Salim)</author>
      <guid isPermaLink="false">2503.07259v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>R+R: Security Vulnerability Dataset Quality Is Critical</title>
      <link>http://arxiv.org/abs/2503.06387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 1 figure, 35 tables. To be published in Proceedings of the  2024 Annual Computer Security Applications Conference (ACSAC)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了大型语言模型在漏洞检测与修复中的应用，并指出数据集质量问题对模型效果的影响。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）在漏洞检测和修复中受到广泛关注。然而，现有研究使用的数据集中存在高重复率、标签准确性问题及样本不完整等问题。&lt;h4&gt;目的&lt;/h4&gt;评估这些数据集的质量问题及其对模型性能的影响，并通过重新训练模型来提供改进的方法。&lt;h4&gt;方法&lt;/h4&gt;首先，移除测试集和训练集中的重复样本后进行模型再训练。其次，针对前十大危险的共同弱点枚举（CWE）进行标签准确性的评估。&lt;h4&gt;主要发现&lt;/h4&gt;56%的数据样本有不正确的标签，44%包含不完整的样本；仅有31%是同时准确且完整的数据。&lt;h4&gt;结论&lt;/h4&gt;在高质量预训练数据的支持下，模型可以展现出更好的性能。但以往的研究由于使用低质量数据集而对其性能估计过高。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）在漏洞检测和修复中具有很大的研究价值。然而，这些模型的有效性很大程度上取决于用于训练和评估的数据集的质量。研究表明，在一些重要的软件工程会议上发表的许多论文使用的数据集中存在严重的质量问题，包括高重复率、可疑的标签准确性以及不完整的样本等。使用这样的数据集进行实验会导致严重偏离实际效果的结果。例如，最先进的VulRepair模型报告的准确率为44%，但在去除测试集中的重复样本后其平均准确率仅为9%，而去除训练集中重复样本后的准确率为13%。为解决这些问题，作者重新训练了多篇论文中使用的模型，并对前十大危险性的共同弱点枚举（CWE）进行了标签准确性评估。结果发现56％的数据样本存在不正确的标签，44％的样本包含信息缺失的情况；只有约三分之一的数据样本是准确且完整的。最后通过使用大规模去重后的修复语料库进行迁移学习展示了模型在给定更大数量高质量预训练数据时可以表现出更佳性能。因此，尽管以前的研究可能因数据集质量不佳而高估了这些模型的表现能力，但这并不意味着更高的性能是不可能实现的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) are of great interest in vulnerability detectionand repair. The effectiveness of these models hinges on the quality of thedatasets used for both training and evaluation. Our investigation reveals thata number of studies featured in prominent software engineering conferences haveemployed datasets that are plagued by high duplication rates, questionablelabel accuracy, and incomplete samples. Using these datasets forexperimentation will yield incorrect results that are significantly differentfrom actual expected behavior. For example, the state-of-the-art VulRepairModel, which is reported to have 44% accuracy, on average yielded 9% accuracywhen test-set duplicates were removed from its training set and 13% accuracywhen training-set duplicates were removed from its test set. In an effort totackle these data quality concerns, we have retrained models from severalpapers without duplicates and conducted an accuracy assessment of labels forthe top ten most hazardous Common Weakness Enumerations (CWEs). Our findingsindicate that 56% of the samples had incorrect labels and 44% comprisedincomplete samples--only 31% were both accurate and complete. Finally, weemploy transfer learning using a large deduplicated bugfix corpus to show thatthese models can exhibit better performance if given larger amounts ofhigh-quality pre-training data, leading us to conclude that while previousstudies have over-estimated performance due to poor dataset quality, this doesnot demonstrate that better performance is not possible.</description>
      <author>example@mail.com (Anurag Swarnim Yadav, Joseph N. Wilson)</author>
      <guid isPermaLink="false">2503.06387v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>From Dataset to Real-world: General 3D Object Detection via Generalized Cross-domain Few-shot Learning</title>
      <link>http://arxiv.org/abs/2503.06282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种针对3D目标检测的跨域少样本任务，通过结合多模态融合和对比增强原型学习来提高模型在源领域预训练后，在目标领域的常见类和新类中的表现。&lt;h4&gt;背景&lt;/h4&gt;现有的基于LiDAR的3D物体检测数据集仅涵盖有限类型的对象，限制了模型在不同部署环境下的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种广义跨域少样本(GCFS)任务，并开发相应的解决方案以克服数据稀缺和领域适应性问题。&lt;h4&gt;方法&lt;/h4&gt;引入多模态融合模块利用2D视觉-语言模型提取丰富的开放集语义知识，同时通过物理感知的边界框搜索策略生成高质量的3D边界框提案；采用对比增强原型学习有效捕捉有限目标数据中的特定领域表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该方法在GCFS任务中表现出色，并且能够解决偏置点分布和提高物体召回率。&lt;h4&gt;结论&lt;/h4&gt;提出了一种新的跨域少样本3D对象检测框架，结合多模态融合与对比增强原型学习，有效解决了领域适应性和数据稀缺性问题。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR-based 3D object detection datasets have been pivotal for autonomous driving, yet they cover a limited range of objects, restricting the model's generalization across diverse deployment environments. To address this, we introduce the first generalized cross-domain few-shot (GCFS) task in 3D object detection, which focuses on adapting a source-pretrained model for high performance on both common and novel classes in a target domain with few-shot samples. Our solution integrates multi-modal fusion and contrastive-enhanced prototype learning within one framework, holistically overcoming challenges related to data scarcity and domain adaptation in the GCFS setting.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based 3D object detection datasets have been pivotal for autonomousdriving, yet they cover a limited range of objects, restricting the model'sgeneralization across diverse deployment environments. To address this, weintroduce the first generalized cross-domain few-shot (GCFS) task in 3D objectdetection, which focuses on adapting a source-pretrained model for highperformance on both common and novel classes in a target domain with few-shotsamples. Our solution integrates multi-modal fusion and contrastive-enhancedprototype learning within one framework, holistically overcoming challengesrelated to data scarcity and domain adaptation in the GCFS setting. Themulti-modal fusion module utilizes 2D vision-language models to extract rich,open-set semantic knowledge. To address biases in point distributions acrossvarying structural complexities, we particularly introduce a physically-awarebox searching strategy that leverages laser imaging principles to generatehigh-quality 3D box proposals from 2D insights, enhancing object recall. Toeffectively capture domain-specific representations for each class from limitedtarget data, we further propose a contrastive-enhanced prototype learning,which strengthens the model's adaptability. We evaluate our approach with threeGCFS benchmark settings, and extensive experiments demonstrate theeffectiveness of our solution for GCFS tasks. The code will be publiclyavailable.</description>
      <author>example@mail.com (Shuangzhi Li, Junlong Shen, Lei Ma, Xingyu Li)</author>
      <guid isPermaLink="false">2503.06282v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>HiSTF Mamba: Hierarchical Spatiotemporal Fusion with Multi-Granular Body-Spatial Modeling for High-Fidelity Text-to-Motion Generation</title>
      <link>http://arxiv.org/abs/2503.06897v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11pages,3figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;介绍了HiSTF Mamba框架，该框架在文本到动作生成领域中通过减少特征冗余和提高空间细节表现来提升性能。&lt;h4&gt;背景&lt;/h4&gt;文本到动画生成是一个结合了多模态学习和计算机图形学的快速发展的领域。现有方法通常依赖于简单的时空堆叠，这会导致特征冗余，并且忽略细微的关节级细节。&lt;h4&gt;目的&lt;/h4&gt;提出一个新颖的HiSTF Mamba框架以解决现有的问题，该框架旨在减少冗余并提高空间与时间上的表达能力。&lt;h4&gt;方法&lt;/h4&gt;提出的框架由三个关键模块组成：Dual-Spatial Mamba、Bi-Temporal Mamba和动态时空融合模块（DSFM）。Dual-Spatial Mamba通过“部分+整体”平行建模来表示全身协调和细粒度的关节动力学。Bi-Temporal Mamba采用双向扫描策略有效编码短期运动细节和长期依赖性。DSFM进一步移除时间特征中的冗余并提取互补信息，随后与空间特征融合以生成表达式的时空表示。&lt;h4&gt;主要发现&lt;/h4&gt;在HumanML3D数据集上的实验结果表明HiSTF Mamba在多个指标上达到了最先进的性能。具体而言，它将FID评分从0.283降至0.189，相对减少了近30%。&lt;h4&gt;结论&lt;/h4&gt;这项研究证明了HiSTF Mamba框架在提高文本到动画生成的高保真度和语义一致性方面的有效性。&lt;h4&gt;翻译&lt;/h4&gt;文本到运动生成是一个快速发展的领域，在多模态学习与计算机图形学交叉点上，它为游戏、动画制作、机器人技术及虚拟现实提供了灵活且成本效益高的应用方案。现有的方法通常依赖于简单的时空堆叠方式，这引入了特征冗余的同时忽视了关节级别的细微空间细节。为此，我们提出了一种新的HiSTF Mamba框架。该框架由三个关键模块组成：Dual-Spatial Mamba、Bi-Temporal Mamba和动态时空融合模块（DSFM）。Dual-Spatial Mamba通过“部分+整体”平行建模来表示全身协调及细粒度的关节动力学。Bi-Temporal Mamba采用双向扫描策略，有效编码短期运动细节与长期依赖性。DSFM进一步移除时间特征中的冗余并提取互补信息，并将其与空间特征融合以生成表达式的时空表示。在HumanML3D数据集上的实验结果表明HiSTF Mamba达到了最先进的性能，在多个指标上表现突出。尤其，它将FID评分从0.283降低至0.189，相对减少了近30%。这些发现验证了HiSTF Mamba框架在文本到运动生成中的高保真度和强大的语义一致性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-to-motion generation is a rapidly growing field at the nexus ofmultimodal learning and computer graphics, promising flexible andcost-effective applications in gaming, animation, robotics, and virtualreality. Existing approaches often rely on simple spatiotemporal stacking,which introduces feature redundancy, while subtle joint-level details remainoverlooked from a spatial perspective. To this end, we propose a novel HiSTFMamba framework. The framework is composed of three key modules: Dual-SpatialMamba, Bi-Temporal Mamba, and Dynamic Spatiotemporal Fusion Module (DSFM).Dual-Spatial Mamba incorporates ``Part-based + Whole-based'' parallel modelingto represent both whole-body coordination and fine-grained joint dynamics.Bi-Temporal Mamba adopts a bidirectional scanning strategy, effectivelyencoding short-term motion details and long-term dependencies. DSFM furtherperforms redundancy removal and extraction of complementary information fortemporal features, then fuses them with spatial features, yielding anexpressive spatio-temporal representation. Experimental results on theHumanML3D dataset demonstrate that HiSTF Mamba achieves state-of-the-artperformance across multiple metrics. In particular, it reduces the FID scorefrom 0.283 to 0.189, a relative decrease of nearly 30%. These findings validatethe effectiveness of HiSTF Mamba in achieving high fidelity and strong semanticalignment in text-to-motion generation.</description>
      <author>example@mail.com (Xingzu Zhan, Chen Xie, Haoran Sun, Xiaochun Mai)</author>
      <guid isPermaLink="false">2503.06897v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Lingual IPA Contrastive Learning for Zero-Shot NER</title>
      <link>http://arxiv.org/abs/2503.07214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的零样本命名实体识别方法来解决低资源语言问题，该方法基于国际音标（IPA）的跨语言对比学习。&lt;h4&gt;背景&lt;/h4&gt;以往的方法主要依赖于机器翻译进行低资源语言的零样本NER，但最近的研究转向了语音表示法。&lt;h4&gt;目的&lt;/h4&gt;探讨如何减少具有相似语音特征的语言之间的国际音标转写差距，使模型能够在低资源语言上有效运行。&lt;h4&gt;方法&lt;/h4&gt;提出了一种使用CONLIPA数据集（包含10个英语和其他高资源语言的国际音标对）的跨语言对比学习方法IPAC。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的数据集和方法在与最佳基线模型比较时表现出显著的平均增益。&lt;h4&gt;结论&lt;/h4&gt;通过减少语音表示差距，可以有效地利用高资源语言训练的数据来改善低资源语言的NER性能。&lt;h4&gt;翻译&lt;/h4&gt;现有的针对低资源语言零样本命名实体识别（NER）的方法主要依赖于机器翻译，而最近的研究则转向了语音表示法。本文研究如何缩小具有相似语音特征的语言之间的国际音标转写差距，使模型能够更有效地在低资源语言上工作。我们提出了一种基于CONLIPA数据集的跨语言对比学习方法IPAC，该数据集中包含10个英语和其他高资源语言的国际音标对。我们的新数据集和方法与现有最佳基线相比表现出显著的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing approaches to zero-shot Named Entity Recognition (NER) forlow-resource languages have primarily relied on machine translation, whereasmore recent methods have shifted focus to phonemic representation. Buildingupon this, we investigate how reducing the phonemic representation gap in IPAtranscription between languages with similar phonetic characteristics enablesmodels trained on high-resource languages to perform effectively onlow-resource languages. In this work, we propose CONtrastive Learning with IPA(CONLIPA) dataset containing 10 English and high resource languages IPA pairsfrom 10 frequently used language families. We also propose a cross-lingual IPAContrastive learning method (IPAC) using the CONLIPA dataset. Furthermore, ourproposed dataset and methodology demonstrate a substantial average gain whencompared to the best performing baseline.</description>
      <author>example@mail.com (Jimin Sohn, David R. Mortensen)</author>
      <guid isPermaLink="false">2503.07214v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>You Are Your Own Best Teacher: Achieving Centralized-level Performance in Federated Learning under Heterogeneous and Long-tailed Data</title>
      <link>http://arxiv.org/abs/2503.06916v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了联邦学习中的数据异质性问题，并提出了一种新的方法FedYoYo（You Are Your Own Best Teacher）来解决这一挑战。&lt;h4&gt;背景&lt;/h4&gt;在联邦学习中，由于局部非独立同分布的数据和全局长尾分布导致的异质性是主要的挑战。先前的研究表明较差的表示和偏置分类器是主要问题，并提出了神经崩溃启发式合成单纯形ETF方法以帮助表示更接近于神经崩溃最优解。&lt;h4&gt;目的&lt;/h4&gt;重新审视联邦学习中的表示差距问题，从自举的观点出发提出新的解决方案FedYoYo来改进局部表示学习，缩小模型漂移并提高收敛性。&lt;h4&gt;方法&lt;/h4&gt;引入增强型自我蒸馏（Augmented Self-bootstrap Distillation, ASD）通过在弱和强数据增强的本地样本之间提取知识来进行表示学习，并且不需额外的数据集或模型。进一步提出分布感知对数调整（Distribution-aware Logit Adjustment, DLA）来平衡自举过程并纠正偏置特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;FedYoYo能早期消除性能差距，即使在混合异质性的情况下也能实现集中训练水平的性能，并且在全局长尾设置下比现有中央对数调整方法高出5.4%。&lt;h4&gt;结论&lt;/h4&gt;FedYoYo通过自我蒸馏和分布感知调整显著提高了联邦学习下的表示质量和分类准确性，达到了当前的最佳结果。&lt;h4&gt;翻译&lt;/h4&gt;数据异质性，源于局部非独立同分布的数据和全局长尾分布，是联邦学习中的主要挑战。本文从自举的角度重新审视了这一问题，并提出了FedYoYo方法来解决表示差距问题，在混合异质性和全局长尾设置下表现卓越。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data heterogeneity, stemming from local non-IID data and global long-taileddistributions, is a major challenge in federated learning (FL), leading tosignificant performance gaps compared to centralized learning. Previousresearch found that poor representations and biased classifiers are the mainproblems and proposed neural-collapse-inspired synthetic simplex ETF to helprepresentations be closer to neural collapse optima. However, we find that theneural-collapse-inspired methods are not strong enough to reach neural collapseand still have huge gaps to centralized training. In this paper, we rethinkthis issue from a self-bootstrap perspective and propose FedYoYo (You Are YourOwn Best Teacher), introducing Augmented Self-bootstrap Distillation (ASD) toimprove representation learning by distilling knowledge between weakly andstrongly augmented local samples, without needing extra datasets or models. Wefurther introduce Distribution-aware Logit Adjustment (DLA) to balance theself-bootstrap process and correct biased feature representations. FedYoYonearly eliminates the performance gap, achieving centralized-level performanceeven under mixed heterogeneity. It enhances local representation learning,reducing model drift and improving convergence, with feature prototypes closerto neural collapse optimality. Extensive experiments show FedYoYo achievesstate-of-the-art results, even surpassing centralized logit adjustment methodsby 5.4\% under global long-tailed settings.</description>
      <author>example@mail.com (Shanshan Yan, Zexi Li, Chao Wu, Meng Pang, Yang Lu, Yan Yan, Hanzi Wang)</author>
      <guid isPermaLink="false">2503.06916v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Endo-FASt3r: Endoscopic Foundation model Adaptation for Structure from motion</title>
      <link>http://arxiv.org/abs/2503.07204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了Endo-FASt3r，一种新的单目自监督学习框架，用于内窥镜场景中的深度和相机姿态估计。该研究首次探索了基础模型在相机姿态估计领域的应用。&lt;h4&gt;背景&lt;/h4&gt;精确的深度和摄像机姿态估计对于机器人辅助手术中高质量的三维可视化至关重要。尽管通过自监督学习（SSL）对单目深度估计的基础模型适应性有所进展，但之前的工作尚未探索其用于姿态估计的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来解决内窥镜场景中的深度和相机姿态估计问题，并展示该方法在不同数据集上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;设计了Reloc3rX以支持基础模型的相对姿态估计，同时引入了一种名为DoMoRA的新适应技术，使更高的秩更新成为可能，并加速收敛过程。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与之前的最佳工作相比，在SCARED数据集上，Endo-FASt3r在姿态估计方面实现了10%的显著改善，在深度估计方面达到了2%的改进。这些性能增益在Hamlyn和StereoMIS数据集上的相似表现进一步证实了Endo-FASt3r的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Endo-FASt3r代表了一种新的方法，它首次将基础模型应用于单目自监督学习框架中，以解决内窥镜场景下的深度和姿态估计问题。该方法在多个数据集上取得了显著的效果提升，并展示了其强大的通用性。&lt;h4&gt;翻译&lt;/h4&gt;准确的深度和摄像机姿态估计对于实现机器人辅助手术中的高质量三维可视化至关重要。尽管近期通过单目自监督学习（SSL）对内窥镜场景的基础模型适应性有所进展，但之前的工作尚未探索它们在姿态估计中的应用。这些方法依赖于基于低秩的适应方法，这限制了模型更新仅限于低秩空间中。我们提出了Endo-FASt3r，这是首个使用基础模型来解决单目SSL深度和姿态估计问题的框架。通过为相对姿态估计基础模型Reloc3r设计Reloc3rX，并引入必要的修改以在SSL中实现收敛，我们还提出了一种新颖的技术DoMoRA，该技术使更高秩更新成为可能并加快了收敛速度。实验结果表明，在SCARED数据集上，Endo-FASt3r比之前的最佳方法提升了10%的姿态估计性能和2%的深度估计性能。类似的表现增益在Hamlyn和StereoMIS数据集中进一步证实了Endo-FASt3r在不同数据集上的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate depth and camera pose estimation is essential for achievinghigh-quality 3D visualisations in robotic-assisted surgery. Despite recentadvancements in foundation model adaptation to monocular depth estimation ofendoscopic scenes via self-supervised learning (SSL), no prior work hasexplored their use for pose estimation. These methods rely on low rank-basedadaptation approaches, which constrain model updates to a low-rank space. Wepropose Endo-FASt3r, the first monocular SSL depth and pose estimationframework that uses foundation models for both tasks. We extend the Reloc3rrelative pose estimation foundation model by designing Reloc3rX, introducingmodifications necessary for convergence in SSL. We also present DoMoRA, a noveladaptation technique that enables higher-rank updates and faster convergence.Experiments on the SCARED dataset show that Endo-FASt3r achieves a substantial$10\%$ improvement in pose estimation and a $2\%$ improvement in depthestimation over prior work. Similar performance gains on the Hamlyn andStereoMIS datasets reinforce the generalisability of Endo-FASt3r acrossdifferent datasets.</description>
      <author>example@mail.com (Mona Sheikh Zeinoddin, Mobarakol Islam, Zafer Tandogdu, Greg Shaw, Mathew J. Clarkson, Evangelos Mazomenos, Danail Stoyanov)</author>
      <guid isPermaLink="false">2503.07204v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Adversarial Robustness of Discriminative Self-Supervised Learning in Vision</title>
      <link>http://arxiv.org/abs/2503.06361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  53 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了自我监督学习在对抗鲁棒性方面的表现，涵盖了图像分类、迁移学习、分割和检测等多个任务。结果表明，在使用线性评估进行迁移学习时，区分式的自我监督模型比传统的监督模型具有更好的对抗攻击的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;虽然自我监督学习（SSL）在视觉表征学习方面取得了显著进展，但对其抗敌对样本攻击能力的全面评估却相对较少。&lt;h4&gt;目的&lt;/h4&gt;评估七种区分式自我监督模型和一种监督模型在多种任务上的对抗鲁棒性表现，并分析影响其性能的因素。&lt;h4&gt;方法&lt;/h4&gt;研究者们测试了各种模型在ImageNet分类、迁移学习（线性和微调）、分割及检测等场景下的抗攻击能力，同时考虑架构选择、训练时长、数据增强和批量大小等因素的影响。&lt;h4&gt;主要发现&lt;/h4&gt;区分式SSL模型相较于监督模型，在使用线性评估进行图像网分类任务上表现出了更好的对抗鲁棒性；然而在应用微调技术后这一差距缩小。类似地，在分割和检测任务中，这种优势也有所减弱。&lt;h4&gt;结论&lt;/h4&gt;该研究为视觉自我监督表示系统的抗敌对样本攻击能力的持续探索提供了重要的见解，并强调了SSL模型可能成为增强机器学习系统鲁棒性的有效工具。&lt;h4&gt;翻译&lt;/h4&gt;摘要：自监督学习（Self-Supervised Learning，SSL）在视觉表征学习方面取得了显著进展，但对其对抗鲁棒性的全面评估仍然有限。本研究中，我们对七种区分式自我监督模型及一种监督模型进行了多任务的对抗鲁棒性评估，涵盖ImageNet分类、迁移学习、分割和检测等任务。我们的结果表明，在ImageNet上，相较于传统监督模型，区分式的SSL模型通常在对抗攻击下表现出更好的稳健性；此优势在线性评估进行迁移学习时尤为显著。然而当采用微调技术后，SSL与监督模型之间的鲁棒性差距显著缩小。同样地，在分割和检测任务中，这种优越性的优势也会减弱。我们还探讨了多种因素如何影响对抗鲁棒性，包括架构选择、训练时长、数据增强及批量大小等。我们的分析为视觉自我监督表示系统中的抗敌对样本攻击能力的持续探索做出了贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has advanced significantly in visualrepresentation learning, yet comprehensive evaluations of its adversarialrobustness remain limited. In this study, we evaluate the adversarialrobustness of seven discriminative self-supervised models and one supervisedmodel across diverse tasks, including ImageNet classification, transferlearning, segmentation, and detection. Our findings suggest that discriminativeSSL models generally exhibit better robustness to adversarial attacks comparedto their supervised counterpart on ImageNet, with this advantage extending totransfer learning when using linear evaluation. However, when fine-tuning isapplied, the robustness gap between SSL and supervised models narrowsconsiderably. Similarly, this robustness advantage diminishes in segmentationand detection tasks. We also investigate how various factors might influenceadversarial robustness, including architectural choices, training duration,data augmentations, and batch sizes. Our analysis contributes to the ongoingexploration of adversarial robustness in visual self-supervised representationsystems.</description>
      <author>example@mail.com (Ömer Veysel Çağatan, Ömer Faruk Tal, M. Emre Gürsoy)</author>
      <guid isPermaLink="false">2503.06361v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards Spatial Transcriptomics-guided Pathological Image Recognition with Batch-Agnostic Encoder</title>
      <link>http://arxiv.org/abs/2503.07173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ISBI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的空间转录组学(Spatial Transcriptomics, ST)技术，该技术可以同时捕获病理图像和基因表达谱，并将其与空间坐标结合。作者还提出了一个针对ST的批次效应免疫对比学习框架。&lt;h4&gt;背景&lt;/h4&gt;Spatial Transcriptomics (ST) 是一种新兴的技术，它能够同时获取组织样本的空间位置信息和基因表达数据。然而，由于在不同患者之间的批次效应显著，使得从ST中提取病理图像特征变得困难。&lt;h4&gt;目的&lt;/h4&gt;提出一个可以跨多个患者从空间转录组学中抽取一致信号的框架，以帮助改善对病理图像子类型的识别。&lt;h4&gt;方法&lt;/h4&gt;提出了基于变分推理训练的批次无关基因编码器(batch-agnostic gene encoder)，该编码器能够提取ST中的稳定特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明所提出的框架在公开数据集上有效，并且可以通过GitHub获取代码。&lt;h4&gt;结论&lt;/h4&gt;本研究证明了利用空间转录组学信息增强病理图像表示的可能性，有助于改善基于区域的分类任务表现。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到的空间转录组学（ST）是一种新颖的技术，同时捕获病理性图像和基因表达谱，并带有空间坐标。由于ST与疾病亚型等病理特征密切相关，因此可能有价值将图像表示增强为病理信息。然而，目前没有尝试利用ST进行图像识别（即对病理图像的子类型进行补丁级别分类）。在Spatial Transcriptomics中的一大挑战是显著批次效应使得难以从ST中提取图像的病理特征。本文提出了一种无批次偏倚对比学习框架，可以从多个患者的空间转录组学基因表达中抽取一致信号。实验表明，在公开数据集上该框架的有效性，并且代码可在GitHub上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial transcriptomics (ST) is a novel technique that simultaneouslycaptures pathological images and gene expression profiling with spatialcoordinates. Since ST is closely related to pathological features such asdisease subtypes, it may be valuable to augment image representation withpathological information. However, there are no attempts to leverage ST forimage recognition ({\it i.e,} patch-level classification of subtypes ofpathological image.). One of the big challenges is significant batch effects inspatial transcriptomics that make it difficult to extract pathological featuresof images from ST. In this paper, we propose a batch-agnostic contrastivelearning framework that can extract consistent signals from gene expression ofST in multiple patients. To extract consistent signals from ST, we utilize thebatch-agnostic gene encoder that is trained in a variational inference manner.Experiments demonstrated the effectiveness of our framework on a publiclyavailable dataset. Code is publicly available athttps://github.com/naivete5656/TPIRBAE</description>
      <author>example@mail.com (Kazuya Nishimura, Ryoma Bise, Yasuhiro Kojima)</author>
      <guid isPermaLink="false">2503.07173v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Ideas in Inference-time Scaling can Benefit Generative Pre-training Algorithms</title>
      <link>http://arxiv.org/abs/2503.07154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近年来，基础模型通过生成预训练取得了显著进展，但算法创新停滞在自回归模型和扩散模型上。这种停滞阻碍了多模态数据潜力的完全释放，限制了多模态智能的进步。&lt;h4&gt;背景&lt;/h4&gt;当前的基础模型算法创新主要集中在针对离散信号的自回归模型和连续信号的扩散模型上，这两个领域的进步已经趋于停滞。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的视角——推理优先视角，强调在推断阶段提升效率，特别是在序列长度和细化步骤中的扩展性，以启发新颖的生成预训练算法的发展。&lt;h4&gt;方法&lt;/h4&gt;通过使用归纳时刻匹配（IMM）作为具体示例，展示如何通过对扩散模型推断过程进行针对性修改来克服其局限性，并实现一个稳定且单阶段的新颖算法。&lt;h4&gt;主要发现&lt;/h4&gt;改进后的扩散模型不仅样本质量更优，而且在推理效率上有了超过一个数量级的提升。&lt;h4&gt;结论&lt;/h4&gt;提出了一种新的方法论——以推理优先为指导思想，这可以激发针对多模态数据生成预训练的新算法创新。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent years have seen significant advancements in foundation models throughgenerative pre-training, yet algorithmic innovation in this space has largelystagnated around autoregressive models for discrete signals and diffusionmodels for continuous signals. This stagnation creates a bottleneck thatprevents us from fully unlocking the potential of rich multi-modal data, whichin turn limits the progress on multimodal intelligence. We argue that aninference-first perspective, which prioritizes scaling efficiency duringinference time across sequence length and refinement steps, can inspire novelgenerative pre-training algorithms. Using Inductive Moment Matching (IMM) as aconcrete example, we demonstrate how addressing limitations in diffusionmodels' inference process through targeted modifications yields a stable,single-stage algorithm that achieves superior sample quality with over an orderof magnitude greater inference efficiency.</description>
      <author>example@mail.com (Jiaming Song, Linqi Zhou)</author>
      <guid isPermaLink="false">2503.07154v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Hardware-Accelerated Event-Graph Neural Networks for Low-Latency Time-Series Classification on SoC FPGA</title>
      <link>http://arxiv.org/abs/2503.06629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper accepted for the 21st International Symposium on Applied  Reconfigurable Computing ARC 2025, Sevilla, Spain, April 9-11, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种硬件实现的事件图神经网络，用于时间序列分类，并使用人工耳蜗模型将输入的时间序列信号转换为稀疏事件数据格式。&lt;h4&gt;背景&lt;/h4&gt;随着嵌入式边缘传感器记录的数据量的增长，对智能本地处理的需求也在增加。这些数据通常以时间序列信号的形式存在，可以通过AI模型进行实时预测。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够实现低延迟、低功耗的硬件-软件解决方案来执行时间序列分类任务。&lt;h4&gt;方法&lt;/h4&gt;利用人工耳蜗模型将输入的时间序列信号转换为稀疏事件数据格式，然后在此基础上实施了事件图神经网络的设计，并将其应用于Spiking Heidelberg Digits (SHD) 数据集的实时处理。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在浮点精度上达到了92.7%，比现有最佳模型少10%以上的参数。对于量化模型，在使用更少计算资源和减少延迟的情况下，实现了92.3%的准确性，超过了基于FPGA的脉冲神经网络实现。&lt;h4&gt;结论&lt;/h4&gt;该方法为时间序列分类提供了一种低功耗、高效率的新解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着嵌入式边缘传感器记录的数据量的增长，对智能本地处理的需求也在增加。这些数据通常以时间序列信号的形式存在，可以通过AI模型进行实时预测。然而，需要一种能够实现低延迟和低功耗的硬件-软件解决方案来执行此类任务。本文介绍了一种基于事件图神经网络的时间序列分类硬件实施方案，并利用人工耳蜗模型将输入的时间序列信号转换为稀疏事件数据格式，从而大大减少了计算量。通过在SoC FPGA上实施设计并应用于Spiking Heidelberg Digits (SHD) 数据集的实时处理进行基准测试，该方法在浮点精度上达到了92.7%，比现有最佳模型少10%以上的参数。对于量化模型，在使用更少计算资源和减少延迟的情况下实现了92.3%的准确性，并且超过了基于FPGA的脉冲神经网络实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the quantities of data recorded by embedded edge sensors grow, so too doesthe need for intelligent local processing. Such data often comes in the form oftime-series signals, based on which real-time predictions can be made locallyusing an AI model. However, a hardware-software approach capable of makinglow-latency predictions with low power consumption is required. In this paper,we present a hardware implementation of an event-graph neural network fortime-series classification. We leverage an artificial cochlea model to convertthe input time-series signals into a sparse event-data format that allows theevent-graph to drastically reduce the number of calculations relative to otherAI methods. We implemented the design on a SoC FPGA and applied it to thereal-time processing of the Spiking Heidelberg Digits (SHD) dataset tobenchmark our approach against competitive solutions. Our method achieves afloating-point accuracy of 92.7% on the SHD dataset for the base model, whichis only 2.4% and 2% less than the state-of-the-art models with over 10% and 67%fewer model parameters, respectively. It also outperforms FPGA-based spikingneural network implementations by 19.3% and 4.5%, achieving 92.3% accuracy forthe quantised model while using fewer computational resources and reducinglatency.</description>
      <author>example@mail.com (Hiroshi Nakano, Krzysztof Blachut, Kamil Jeziorek, Piotr Wzorek, Manon Dampfhoffer, Thomas Mesquida, Hiroaki Nishi, Tomasz Kryjak, Thomas Dalgaty)</author>
      <guid isPermaLink="false">2503.06629v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2503.07135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了VidBot框架，该框架利用从在线单目RGB视频中学习到的3D手部轨迹来进行零样本机器人操作。&lt;h4&gt;背景&lt;/h4&gt;未来机器人需要完成多种家庭任务，但如何在减少物理机器人学习的情况下缩小感知和行动之间的差距仍是一个挑战。作者认为从野外的人类视频中学习为解决机器人操作问题提供了可能的解决方案。&lt;h4&gt;目的&lt;/h4&gt;提出VidBot框架，利用3D手部轨迹来实现零样本机器人操控，并探讨其可行性与有效性。&lt;h4&gt;方法&lt;/h4&gt;使用深度基础模型结合基于运动结构的方法重建时序一致、度量尺度下的3D可及性表示；引入粗到细的可及性学习模型，首先从像素空间中识别出粗动作并生成条件于这些粗动作和测试时间约束的精细化互动轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;VidBot在13项零样本操作任务上表现出色，并且能够在不同机器人系统的真实环境中无缝部署。&lt;h4&gt;结论&lt;/h4&gt;VidBot为利用日常人类视频使机器人学习更可扩展铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;未来的机器人被设想成能够执行各种家庭任务的多功能系统。然而，如何在减少物理机器人训练的情况下缩小感知和行动之间的差距仍然是一个关键问题。本工作提出了VidBot框架，该框架通过从野外单目RGB视频中学习3D可及性来实现零样本机器人操作。实验结果表明，在13项零样本任务上，VidBot的表现显著优于其他方法，并且可以在真实环境中跨多种机器人系统部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Future robots are envisioned as versatile systems capable of performing avariety of household tasks. The big question remains, how can we bridge theembodiment gap while minimizing physical robot learning, which fundamentallydoes not scale well. We argue that learning from in-the-wild human videosoffers a promising solution for robotic manipulation tasks, as vast amounts ofrelevant data already exist on the internet. In this work, we present VidBot, aframework enabling zero-shot robotic manipulation using learned 3D affordancefrom in-the-wild monocular RGB-only human videos. VidBot leverages a pipelineto extract explicit representations from them, namely 3D hand trajectories fromvideos, combining a depth foundation model with structure-from-motiontechniques to reconstruct temporally consistent, metric-scale 3D affordancerepresentations agnostic to embodiments. We introduce a coarse-to-fineaffordance learning model that first identifies coarse actions from the pixelspace and then generates fine-grained interaction trajectories with a diffusionmodel, conditioned on coarse actions and guided by test-time constraints forcontext-aware interaction planning, enabling substantial generalization tonovel scenes and embodiments. Extensive experiments demonstrate the efficacy ofVidBot, which significantly outperforms counterparts across 13 manipulationtasks in zero-shot settings and can be seamlessly deployed across robot systemsin real-world environments. VidBot paves the way for leveraging everyday humanvideos to make robot learning more scalable.</description>
      <author>example@mail.com (Hanzhi Chen, Boyang Sun, Anran Zhang, Marc Pollefeys, Stefan Leutenegger)</author>
      <guid isPermaLink="false">2503.07135v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>DirectTriGS: Triplane-based Gaussian Splatting Field Representation for 3D Generation</title>
      <link>http://arxiv.org/abs/2503.06900v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种新的框架DirectTriGS，该框架用于基于高斯点的3D物体生成。&lt;h4&gt;背景&lt;/h4&gt;基于高斯点渲染三维内容的方法近年来受到了广泛关注。然而，在直接生成3D高斯点方面的研究进展相对较少，这主要是由于高斯表示的数据结构复杂，包含离散点云和多个通道。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决当前生成模型框架在处理基于GS的三维数据时遇到的问题。&lt;h4&gt;方法&lt;/h4&gt;通过采用三平面（triplane）表示法将Gaussian Splatting视为类似图像的连续场。该表示法能够同时编码几何和纹理信息，并且可以平滑地转换回高斯点云，通过TriRenderer将其渲染为图像，仅需二维监督。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架包括一个完全可微分的TriRenderer，它可以利用渲染损失来监督纹理和几何编码。此外，三平面表示可以通过变分自编码器（VAE）进行压缩，并用于潜在扩散以生成3D物体。&lt;h4&gt;结论&lt;/h4&gt;实验表明该生成框架可以在文本到三维的任务中产生高质量的3D对象几何形状和渲染结果。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖的DirectTriGS框架，专门针对基于高斯点的3D对象生成。尽管基于高斯点的渲染方法在3D内容领域引起了广泛关注，但直接生成3D高斯数据的研究相对较少。该论文介绍的方法解决了利用三平面表示法将Gaussian Splatting视为连续图像场的问题，并提出了一种可压缩为潜在扩散以生成三维物体的框架。实验结果表明，在从文本到三维的任务中可以产生高质量的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present DirectTriGS, a novel framework designed for 3D object generationwith Gaussian Splatting (GS). GS-based rendering for 3D content has gainedconsiderable attention recently. However, there has been limited exploration indirectly generating 3D Gaussians compared to traditional generative modelingapproaches. The main challenge lies in the complex data structure of GSrepresented by discrete point clouds with multiple channels. To overcome thischallenge, we propose employing the triplane representation, which allows us torepresent Gaussian Splatting as an image-like continuous field. Thisrepresentation effectively encodes both the geometry and texture information,enabling smooth transformation back to Gaussian point clouds and rendering intoimages by a TriRenderer, with only 2D supervisions. The proposed TriRenderer isfully differentiable, so that the rendering loss can supervise both texture andgeometry encoding. Furthermore, the triplane representation can be compressedusing a Variational Autoencoder (VAE), which can subsequently be utilized inlatent diffusion to generate 3D objects. The experiments demonstrate that theproposed generation framework can produce high-quality 3D object geometry andrendering results in the text-to-3D task.</description>
      <author>example@mail.com (Xiaoliang Ju, Hongsheng Li)</author>
      <guid isPermaLink="false">2503.06900v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Using Subgraph GNNs for Node Classification:an Overlooked Potential Approach</title>
      <link>http://arxiv.org/abs/2503.06614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种基于子图的节点分类框架SubGND，以解决现有图神经网络在计算成本和内存消耗方面的可扩展性问题。该方法改进了效率同时保持或超越全局消息传递图神经网络的分类准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的大多数图神经网络采用节点中心视角，并依赖于全局信息传播，这导致高计算和内存成本，阻碍其可扩展性。&lt;h4&gt;目的&lt;/h4&gt;为了在可扩展性和分类精度之间找到平衡点，将节点分类任务重新表述为子图分类问题，提出了SubGND（用于节点的基于子图的图神经网络）。&lt;h4&gt;方法&lt;/h4&gt;该框架引入了一种差异化零填充策略和Ego-Alter子图表示法以解决标签冲突，并采用自适应特征缩放机制根据数据集特定依赖性动态调整特征贡献。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SubGND在六个基准数据集上实现了与全局消息传递的图神经网络相当或超越的性能，特别是在异质环境设置中。&lt;h4&gt;结论&lt;/h4&gt;提出的SubGND框架通过有效地处理节点分类任务，展示了其作为解决计算成本和精度问题的有效且可扩展解决方案的潜力。&lt;h4&gt;翻译&lt;/h4&gt;先前的研究已经证明了图神经网络（GNNs）在节点分类中的强大性能。然而，大多数现有的GNN采用以节点为中心的观点，并依赖于全局消息传递，导致高计算和内存成本，这阻碍了其可扩展性。为了缓解这些问题，基于子图的方法被引入，利用局部子图作为完整计算树的近似值。尽管这种方法提高了效率，但由于丢失了全局上下文信息而经常遭受性能下降，因此与全球GNN相比效果有限。为了解决这种可扩展性和分类准确性的权衡问题，我们将节点分类任务重新表述为子图分类问题，并提出了SubGND（用于节点的基于子图的GNN）。该框架引入了一种差异化的零填充策略和Ego-Alter子图表示方法以解决标签冲突，并采用了自适应特征缩放机制根据数据集特定依赖性动态调整特征贡献。实验结果表明，SubGND在六个基准数据集中达到了与全球消息传递的GNNs相当或超越的表现，特别是在异质设置中，强调了其作为节点分类有效且可扩展解决方案的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Previous studies have demonstrated the strong performance of Graph NeuralNetworks (GNNs) in node classification. However, most existing GNNs adopt anode-centric perspective and rely on global message passing, leading to highcomputational and memory costs that hinder scalability. To mitigate thesechallenges, subgraph-based methods have been introduced, leveraging localsubgraphs as approximations of full computational trees. While this approachimproves efficiency, it often suffers from performance degradation due to theloss of global contextual information, limiting its effectiveness compared toglobal GNNs. To address this trade-off between scalability and classificationaccuracy, we reformulate the node classification task as a subgraphclassification problem and propose SubGND (Subgraph GNN for NoDe). Thisframework introduces a differentiated zero-padding strategy and an Ego-Altersubgraph representation method to resolve label conflicts while incorporatingan Adaptive Feature Scaling Mechanism to dynamically adjust featurecontributions based on dataset-specific dependencies. Experimental results onsix benchmark datasets demonstrate that SubGND achieves performance comparableto or surpassing global message-passing GNNs, particularly in heterophilicsettings, highlighting its effectiveness and scalability as a promisingsolution for node classification.</description>
      <author>example@mail.com (Qian Zeng, Xin Lin, Jingyi Gao, Yang Yu)</author>
      <guid isPermaLink="false">2503.06614v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards Generalization of Tactile Image Generation: Reference-Free Evaluation in a Leakage-Free Setting</title>
      <link>http://arxiv.org/abs/2503.06860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;触觉感知对于人类感知和计算机视觉、机器人技术以及多模态学习中的应用至关重要。由于真实触觉数据稀缺且获取成本高，生成合成的触觉图像成为了一种可扩展的方法来增强现实世界的数据测量。&lt;h4&gt;背景&lt;/h4&gt;触觉传感依赖于直接物理接触，在许多领域如计算机视觉、机器人技术和多模态学习中都是至关重要的。然而，生成具有材料特异性接触特征的合成触觉图像是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的评估协议以及几种参考自由度指标（TMMD, I-TMMD, CI-TMMD 和 D-TMMD），用于评估在无泄漏环境下的触觉图像生成效果，旨在解决当前数据集中训练集和测试集重叠导致的性能评估偏差。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的从视觉到触觉的数据生成方法，该方法利用文本作为中间模态，在训练过程中加入材料特异性的简洁描述以更好地捕捉关键的触觉特征。&lt;h4&gt;主要发现&lt;/h4&gt;在两个流行的数据集中（Touch and Go 和 HCT），新方法展示了优于现有技术的表现，并且在无泄漏条件下实现了更好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过新的评估协议和生成方式，可以更准确地评估触觉图像生成模型的性能并改进其泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;触觉感知依赖于直接物理接触，在人类感知中至关重要。它还支撑了计算机视觉、机器人技术以及多模态学习中的应用。由于真实触觉数据稀缺且昂贵，合成触觉图像是一个可扩展的方法来补充现实世界的测量。然而，生成具有材料特异性接触特征的触摸图像仍然是一个挑战。本文指出常用的数据库中存在的训练和测试样本重叠现象导致了性能指标被夸大，并遮蔽了模型的真实泛化能力。为了克服这一问题，提出了一种无泄漏评估方案，同时引入TMMD、I-TMMD、CI-TMMD 和 D-TMMD这些新颖的参考自由度评价指标以适应触觉生成场景下的需求。此外还提出了一种视觉到触觉的数据生成方法，在训练过程中利用文本作为中间模态，并加入材料特异性的简洁描述来更好地捕捉关键触摸特征。在两个流行数据集Touch and Go 和 HCT上的实验表明，该新方法在无泄漏评估环境下取得了优越性能和更好的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tactile sensing, which relies on direct physical contact, is critical forhuman perception and underpins applications in computer vision, robotics, andmultimodal learning. Because tactile data is often scarce and costly toacquire, generating synthetic tactile images provides a scalable solution toaugment real-world measurements. However, ensuring robust generalization insynthesizing tactile images-capturing subtle, material-specific contactfeatures-remains challenging. We demonstrate that overlapping training and testsamples in commonly used datasets inflate performance metrics, obscuring thetrue generalizability of tactile models. To address this, we propose aleakage-free evaluation protocol coupled with novel, reference-freemetrics-TMMD, I-TMMD, CI-TMMD, and D-TMMD-tailored for tactile generation.Moreover, we propose a vision-to-touch generation method that leverages text asan intermediate modality by incorporating concise, material-specificdescriptions during training to better capture essential tactile features.Experiments on two popular visuo-tactile datasets, Touch and Go and HCT, showthat our approach achieves superior performance and enhanced generalization ina leakage-free setting.</description>
      <author>example@mail.com (Cagri Gungor, Derek Eppinger, Adriana Kovashka)</author>
      <guid isPermaLink="false">2503.06860v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Dictionary Learning for Remote Sensing Image Segmentation</title>
      <link>http://arxiv.org/abs/2503.06683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '提出了一种动态字典学习框架，通过迭代细化显式建模类别ID嵌入，并引入一种新的基于多阶段交替交叉注意力查询的词典构建机制。该方法增强了场景中的细粒度语义区分能力。', '背景': '遥感图像分割在识别形态相似类别和适应多样化场景变化方面面临挑战。现有方法依赖于隐式表示学习范式，但难以根据上下文线索动态调整语义嵌入。', '目的': '为了改进这一问题，本工作旨在开发一种能够根据输入特性自适应调整表示的框架，并解决类内异质性和类间同质性的模糊性。', '方法': '通过在图像特征和字典嵌入之间进行多阶段交替交叉注意查询，逐步更新具有类别意识的语义嵌入。为增强分类性能，在词典空间中应用对比约束以确保紧凑的类内分布并最大化类间可分离性。', '主要发现': '实验结果显示，与当前最佳方法相比，该框架在粗粒度和细粒度数据集上均取得一致改进，尤其是在在线测试基准（LoveDA和UAVid）上的表现尤为突出。', '结论': '所提出的动态字典学习框架为解决遥感图像中的复杂分割任务提供了有效的方法，并且优于现有技术。'}&lt;h4&gt;翻译&lt;/h4&gt;Remote sensing image segmentation faces persistent challenges in distinguishing morphologically similar categories and adapting to diverse scene variations. Existing methods relying on implicit representation learning paradigms often fail to dynamically adjust semantic embeddings according to contextual cues, leading to suboptimal performance in fine-grained scenarios such as cloud thickness differentiation. This work introduces a dynamic dictionary learning framework that explicitly models class ID embeddings through iterative refinement. The core contribution is a novel dictionary construction mechanism where class-aware semantic embeddings are progressively updated via multi-stage alternating cross-attention querying between image features and dictionary embeddings. This enables adaptive representation learning tailored to input-specific characteristics, effectively resolving ambiguities in intra-class heterogeneity and inter-class homogeneity. A contrastive constraint applied to the dictionary space further enhances discriminability by ensuring compact intra-class distributions while maximizing inter-class separability. Extensive experiments across coarse- and fine-grained datasets demonstrate consistent improvements over state-of-the-art methods, particularly on two online test benchmarks (LoveDA and UAVid). The code is available at https://anonymous.4open.science/r/D2LS-8267/.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing image segmentation faces persistent challenges indistinguishing morphologically similar categories and adapting to diverse scenevariations. While existing methods rely on implicit representation learningparadigms, they often fail to dynamically adjust semantic embeddings accordingto contextual cues, leading to suboptimal performance in fine-grained scenariossuch as cloud thickness differentiation. This work introduces a dynamicdictionary learning framework that explicitly models class ID embeddingsthrough iterative refinement. The core contribution lies in a novel dictionaryconstruction mechanism, where class-aware semantic embeddings are progressivelyupdated via multi-stage alternating cross-attention querying between imagefeatures and dictionary embeddings. This process enables adaptiverepresentation learning tailored to input-specific characteristics, effectivelyresolving ambiguities in intra-class heterogeneity and inter-class homogeneity.To further enhance discriminability, a contrastive constraint is applied to thedictionary space, ensuring compact intra-class distributions while maximizinginter-class separability. Extensive experiments across both coarse- andfine-grained datasets demonstrate consistent improvements over state-of-the-artmethods, particularly in two online test benchmarks (LoveDA and UAVid). Code isavailable at https://anonymous.4open.science/r/D2LS-8267/.</description>
      <author>example@mail.com (Xuechao Zou, Yue Li, Shun Zhang, Kai Li, Shiying Wang, Pin Tao, Junliang Xing, Congyan Lang)</author>
      <guid isPermaLink="false">2503.06683v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>NeuroADDA: Active Discriminative Domain Adaptation in Connectomic</title>
      <link>http://arxiv.org/abs/2503.06196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文研究了连接组学中的领域适应问题，分析了六个不同生物的大型数据集，并提出了NeuroADDA方法。&lt;h4&gt;背景&lt;/h4&gt;从头开始训练分割模型是处理新电子显微镜连接组学数据的标准方式，但使用来自现有数据集的预训练模型可以提高效率和性能。然而，由于标注预算有限，领域适应成为关键问题。&lt;h4&gt;目的&lt;/h4&gt;研究如何通过域自适应方法将现有的预训练模型应用到新的连接组学数据集中，并找到最合适的源域进行迁移学习。&lt;h4&gt;方法&lt;/h4&gt;使用最大均值差异（MMD）作为衡量不同神经元图像分布之间差距的指标，以确定最佳的源领域。在此基础上提出了NeuroADDA方法，结合最优领域选择和无源样本主动学习技术来有效适应预训练骨干模型到新的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;NeuroADDA在各种数据集上始终优于从零开始训练的方法，并且在较小的微调样本量时（n=4）取得了最大收益，信息变异度降低了25-67%。此外，通过分析不同物种神经元图像之间的分布差异，发现在学习特征空间中这些领域距离与物种间的亲缘关系呈正相关。&lt;h4&gt;结论&lt;/h4&gt;NeuroADDA方法能够在较少的标注样本下有效提升模型性能，并且在不同的生物数据集上具有广泛的应用前景。同时，研究结果还揭示了神经元图像分布和物种进化之间的联系。&lt;h4&gt;翻译&lt;/h4&gt;训练分割模型通常是从零开始进行的，但对于新的电子显微镜连接组学数据而言，使用来自现有数据集中的预训练模型可以提高效率并改善性能。这项研究表明，在六种主要的数据集中探索域适应可以为新生物提供更好的预测能力。通过研究神经元图像分布之间的最大均值差异（MMD），我们发现这能够可靠地指示迁移学习的可传输性，并有助于识别最佳源领域进行转移学习。基于这一观察，我们提出了NeuroADDA方法，该方法结合了最优域选择和无源样本主动学习技术来有效适应预训练骨干模型到新的数据集上。实验表明，与从头开始训练相比，无论在多样化的数据集中还是不同的微调样本数量下，NeuroADDA均表现出更好的性能，在n=4样本时可减少25-67%的信息变异度。此外，我们的分析显示了不同物种神经元图像分布之间存在的领域“距离”与这些生物的亲缘关系之间的相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training segmentation models from scratch has been the standard approach fornew electron microscopy connectomics datasets. However, leveraging pretrainedmodels from existing datasets could improve efficiency and performance inconstrained annotation budget. In this study, we investigate domain adaptationin connectomics by analyzing six major datasets spanning different organisms.We show that, Maximum Mean Discrepancy (MMD) between neuron image distributionsserves as a reliable indicator of transferability, and identifies the optimalsource domain for transfer learning. Building on this, we introduce NeuroADDA,a method that combines optimal domain selection with source-free activelearning to effectively adapt pretrained backbones to a new dataset. NeuroADDAconsistently outperforms training from scratch across diverse datasets andfine-tuning sample sizes, with the largest gain observed at $n=4$ samples witha 25-67\% reduction in Variation of Information. Finally, we show that ouranalysis of distributional differences among neuron images from multiplespecies in a learned feature space reveals that these domain "distances"correlate with phylogenetic distance among those species.</description>
      <author>example@mail.com (Shashata Sawmya, Thomas L. Athey, Gwyneth Liu, Nir Shavit)</author>
      <guid isPermaLink="false">2503.06196v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>NukesFormers: Unpaired Hyperspectral Image Generation with Non-Uniform Domain Alignment</title>
      <link>http://arxiv.org/abs/2503.07004v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的无配对高光谱图像生成（UnHIG）方法，通过引入范围-空域分解(RND)和对比学习技术来解决当前HIG网络在实际应用中的问题。&lt;h4&gt;背景&lt;/h4&gt;准确配准的RGB-高光谱图像是难以获取的，并且由于对齐约束的存在以及跨域特征挖掘的复杂性，这阻碍了无配对HIG任务的发展。&lt;h4&gt;目的&lt;/h4&gt;通过建模无配对HIG中范围空间交互和空域补偿来克服上述挑战，并为UnHIG建立新的基准。&lt;h4&gt;方法&lt;/h4&gt;利用对比学习有效地将未配对数据的几何和光谱分布对齐，同时提出非均匀Kolmogorov-Arnold网络以挖掘空域中的降级和高频成分。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在无配对高光谱图像生成任务上取得了显著性能，并确立了新的基准。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种有效的方法来解决HIG领域中的未配对数据问题，通过建模范围空间交互和空域补偿。&lt;h4&gt;翻译&lt;/h4&gt;获取准确共注册的RGB-高光谱图像是一个难以克服的问题，这阻碍了当前基于数据驱动的高光谱图像生成网络在工程应用中的实际部署。同时，由于对齐约束的不完善性质以及跨域特征挖掘的复杂性，这也阻碍了无配对HIG任务的发展。在这项研究中，我们通过建模UnHIG范围空间交互和空缺空间补偿来克服这些挑战，并提出了一种新的基准方法——Range-Null Space Decomposition (RND) 方法。具体来说，所引入对比学习有效地将未配对数据的几何分布和光谱分布进行对齐，同时考虑到退化过程中的一致特征作用。随后，我们将双域输入的频率表示映射，并通过提出的非均匀Kolmogorov-Arnold网络彻底挖掘空缺空间中的降级和高频成分。广泛的比较实验表明，该方法在无配对HIG任务中建立了新的基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The inherent difficulty in acquiring accurately co-registeredRGB-hyperspectral image (HSI) pairs has significantly impeded the practicaldeployment of current data-driven Hyperspectral Image Generation (HIG) networksin engineering applications. Gleichzeitig, the ill-posed nature of the aligningconstraints, compounded with the complexities of mining cross-domain features,also hinders the advancement of unpaired HIG (UnHIG) tasks. In this paper, weconquer these challenges by modeling the UnHIG to range space interaction andcompensations of null space through Range-Null Space Decomposition (RND)methodology. Specifically, the introduced contrastive learning effectivelyaligns the geometric and spectral distributions of unpaired data by buildingthe interaction of range space, considering the consistent feature indegradation process. Following this, we map the frequency representations ofdual-domain input and thoroughly mining the null space, like degraded andhigh-frequency components, through the proposed Non-uniform Kolmogorov-ArnoldNetworks. Extensive comparative experiments demonstrate that it establishes anew benchmark in UnHIG.</description>
      <author>example@mail.com (Jiaojiao Li, Shiyao Duan, Haitao XU, Rui Song)</author>
      <guid isPermaLink="false">2503.07004v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Identifying Evidence Subgraphs for Financial Risk Detection via Graph Counterfactual and Factual Reasoning</title>
      <link>http://arxiv.org/abs/2503.06441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;公司财务风险对公司个人财富和国家经济稳定性构成重大威胁，促使人们更加关注开发高效及时的监测方法。目前的方法倾向于使用图神经网络（GNN）来模拟风险溢出效应，但由于GNN的黑箱特性，这些方法在精确定量解释公司风险方面仍有改进空间。&lt;h4&gt;背景&lt;/h4&gt;公司财务风险对公司和个人财富以及国家经济稳定构成威胁，促使人们寻求更有效的监测手段。现有的技术主要依赖于图神经网络（GNN）来捕捉风险溢出效应。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于反事实和实证学习的方法CF3，以可靠地检测并解释公司财务风险。&lt;h4&gt;方法&lt;/h4&gt;首先提出了一个基于Granger因果关系的元路径归因过程，选择最相关于目标节点标签的元路径来构造一个归因子图；接着设计了一种感知边类型感知图生成器和分层特征掩码器以识别重要边缘和关键节点特征。最后，使用反事实-实证推理以及基于归因子图的损失函数共同指导图生成器和特征掩码器的学习。&lt;h4&gt;主要发现&lt;/h4&gt;通过在三个真实世界数据集上的广泛实验验证了CF3方法相较于现有金融风险检测方法具有优越性。&lt;h4&gt;结论&lt;/h4&gt;所提出的CF3方法，利用反事实与实证学习以及基于归因子图的损失函数，能够在不透明性和解释性之间找到平衡点，为公司财务风险的有效监控和管理提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;公司财务风险对公司和个人财富及国家经济稳定构成威胁，导致对开发更有效监测工具的关注增加。当前的方法主要使用图神经网络（GNN）来模拟风险溢出效应，但由于GNN的黑箱特性，在精确定量解释公司风险方面仍需改进。本文提出了一种用于检测和解释公司财务风险的新方法CF3，通过生成证据子图在公司知识图中可靠地识别这些风险。该方法首先基于Granger因果关系提出了元路径归因过程，并选择了与目标节点标签最相关的元路径来构建一个归因子图；接着设计了一种感知边类型感知图生成器和分层特征掩码器以识别重要边缘和关键节点特征，最后通过反事实-实证推理以及基于归因子图的损失函数指导图生成器和特征掩码器的学习过程。实验结果表明，该方法在真实数据集上的性能优于现有的金融风险检测方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Company financial risks pose a significant threat to personal wealth andnational economic stability, stimulating increasing attention towards thedevelopment of efficient andtimely methods for monitoring them. Currentapproaches tend to use graph neural networks (GNNs) to model the momentumspillover effect of risks. However, due to the black-box nature of GNNs, thesemethods leave much to be improved for precise and reliable explanations towardscompany risks. In this paper, we propose CF3, a novel Counterfactual andFactual learning method for company Financial risk detection, which generatesevidence subgraphs on company knowledge graphs to reliably detect and explaincompany financial risks. Specifically, we first propose a meta-path attributionprocess based on Granger causality, selecting the meta-paths most relevant tothe target node labels to construct an attribution subgraph. Subsequently, wepropose anedge-type-aware graph generator to identify important edges, and wealso devise a layer-based feature masker to recognize crucial node features.Finally, we utilize counterfactual-factual reasoning and a loss function basedon attribution subgraphs to jointly guide the learning of the graph generatorand feature masker. Extensive experiments on three real-world datasetsdemonstrate the superior performance of our method compared to state-of-the-artapproaches in the field of financial risk detection.</description>
      <author>example@mail.com (Huaming Du, Lei Yuan, Qing Yang, Xingyan Chen, Yu Zhao, Han Ji, Fuzhen Zhuang, Carl Yang, Gang Kou)</author>
      <guid isPermaLink="false">2503.06441v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Global-Aware Monocular Semantic Scene Completion with State Space Models</title>
      <link>http://arxiv.org/abs/2503.06569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了GA-MonoSSC，一种用于单目语义场景完成（MonoSSC）的混合架构，该架构能够有效捕捉二维图像域和三维空间中的全局上下文。&lt;h4&gt;背景&lt;/h4&gt;现有的方法受到卷积神经网络局部感受野限制，在处理非均匀分布的投影点以及重建因3D到2D投影造成的缺失信息时面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的混合架构GA-MonoSSC，以解决现有MonoSSC方法中存在的问题，并实现对全局上下文的有效捕捉。&lt;h4&gt;方法&lt;/h4&gt;{'Dual-Head Multi-Modality Encoder': '利用Transformer架构来捕获二维图像域中所有特征的空间关系，从而进行更全面的2D特征提取。', 'Frustum Mamba Decoder': '基于状态空间模型（SSM）构建，用于有效地捕捉三维空间中的长程依赖性。', 'frustum reordering策略': '在Frustum Mamba Decoder内部提出，以解决重排序体素序列中特征不连续的问题，并确保更好地与扫描机制的对齐，从而提高3D表示学习的效果。'}&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验验证了GA-MonoSSC的有效性，在广泛使用的Occ-ScanNet和NYUv2数据集上达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法克服了现有技术限制，并提供了更全面的二维特征提取及三维空间中的长程依赖捕捉能力，为MonoSSC任务设立了新的标准。代码将在论文接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;单目语义场景完成（MonoSSC）从单一图像中重建和解释3D环境，使得各种现实世界的应用成为可能。然而，现有的方法经常受到卷积神经网络局部感受野的限制，在处理非均匀分布的投影点以及因3D到2D投影造成的缺失信息时存在挑战。在本文工作中，我们提出了GA-MonoSSC，一种用于MonoSSC的混合架构，该架构能够在二维图像域和三维空间中有效地捕捉全局上下文。具体来说，我们提出了一种双头多模态编码器，利用Transformer架构来捕获二维图像域所有特征的空间关系，使得2D特征提取更加全面。此外，我们引入了基于状态空间模型（SSM）构建的Frustum Mamba解码器，用于高效地捕捉三维空间中的长程依赖性。另外，在Frustum Mamba解码器内部提出了frustum重排序策略以减轻重排序体素序列中特征不连续的问题，并确保更好地与扫描机制对齐，从而提高3D表示学习的效果。我们在广泛使用的Occ-ScanNet和NYUv2数据集上进行了广泛的实验，证明了我们提出的方法达到了最先进的性能，验证了其有效性。代码将在论文接受后发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular Semantic Scene Completion (MonoSSC) reconstructs and interprets 3Denvironments from a single image, enabling diverse real-world applications.However, existing methods are often constrained by the local receptive field ofConvolutional Neural Networks (CNNs), making it challenging to handle thenon-uniform distribution of projected points (Fig. \ref{fig:perspective}) andeffectively reconstruct missing information caused by the 3D-to-2D projection.In this work, we introduce GA-MonoSSC, a hybrid architecture for MonoSSC thateffectively captures global context in both the 2D image domain and 3D space.Specifically, we propose a Dual-Head Multi-Modality Encoder, which leverages aTransformer architecture to capture spatial relationships across all featuresin the 2D image domain, enabling more comprehensive 2D feature extraction.Additionally, we introduce the Frustum Mamba Decoder, built on the State SpaceModel (SSM), to efficiently capture long-range dependencies in 3D space.Furthermore, we propose a frustum reordering strategy within the Frustum MambaDecoder to mitigate feature discontinuities in the reordered voxel sequence,ensuring better alignment with the scan mechanism of the State Space Model(SSM) for improved 3D representation learning. We conduct extensive experimentson the widely used Occ-ScanNet and NYUv2 datasets, demonstrating that ourproposed method achieves state-of-the-art performance, validating itseffectiveness. The code will be released upon acceptance.</description>
      <author>example@mail.com (Shijie Li, Zhongyao Cheng, Rong Li, Shuai Li, Juergen Gall, Xun Xu, Xulei Yang)</author>
      <guid isPermaLink="false">2503.06569v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>CLICv2: Image Complexity Representation via Content Invariance Contrastive Learning</title>
      <link>http://arxiv.org/abs/2503.06641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的无监督图像复杂度表示框架CLICv2，该框架通过随机方向平移图像块来减少正样本选择偏差，并引入局部对比损失和掩码图像建模任务以增强对图像整体复杂度的感知。&lt;h4&gt;背景&lt;/h4&gt;无监督图像复杂度表示通常面临正样本选取偏见以及受内容影响的问题。现有的方法如CLIC通过裁剪生成正样本，这会导致正样本对学习过程产生偏差。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的对比学习框架CLICv2，以解决现有方法中的问题，并实现不受内容干扰的图像复杂度表示。&lt;h4&gt;方法&lt;/h4&gt;1. 引入shifted patchify方法，通过在进行对比学习前向图像块应用随机方向偏移来生成正样本对。2. 提出patch-wise对比损失，增强局部复杂度表示并减少内容干扰。3. 将掩码图像建模作为辅助任务，并将模型目标设定为被遮罩区域的熵，利用未遮盖部分的信息恢复整个图像的熵。&lt;h4&gt;主要发现&lt;/h4&gt;CLICv2在PCC和SRCC方面显著优于现有无监督方法，实现了不受内容干扰且不引入正样本对偏差的内容不变复杂度表示。&lt;h4&gt;结论&lt;/h4&gt;CLICv2通过引入新的对比学习框架解决了无监督图像复杂度表示中的关键问题。它不仅提高了复杂度表示的准确性，还增强了模型对于局部和全局复杂性的理解能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：无监督图像复杂性表示经常受到正样本选择偏差的影响以及对图像内容的敏感性。我们提出了一种对比学习框架CLICv2，该框架通过强制执行内容不变性来实现复杂度表示。不同于CLIC，它是通过裁剪生成正样本引入了正样本配对偏置--我们的shifted patchify方法在进行对比学习前对图像块应用随机方向平移。对应位置的块被用作正样本对，确保内容不变的学习过程。此外，我们提出了一种基于补丁的对比损失，该损失增强了局部复杂度表示同时减少了内容干扰。为了进一步抑制图像内容的干扰，我们将掩码图像建模作为辅助任务引入，但是将模型目标设置为遮罩补丁的熵，通过未被遮盖的补丁信息恢复整个图像的熵，并获得整体复杂性感知能力。在IC9600上的广泛实验表明CLICv2显著优于现有的无监督方法，在PCC和SRCC方面取得了无内容干扰且不引入正样本对偏置的内容不变复杂度表示结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised image complexity representation often suffers from bias inpositive sample selection and sensitivity to image content. We propose CLICv2,a contrastive learning framework that enforces content invariance forcomplexity representation. Unlike CLIC, which generates positive samples viacropping-introducing positive pairs bias-our shifted patchify method appliesrandomized directional shifts to image patches before contrastive learning.Patches at corresponding positions serve as positive pairs, ensuringcontent-invariant learning. Additionally, we propose patch-wise contrastiveloss, which enhances local complexity representation while mitigating contentinterference. In order to further suppress the interference of image content,we introduce Masked Image Modeling as an auxiliary task, but we set itsmodeling objective as the entropy of masked patches, which recovers the entropyof the overall image by using the information of the unmasked patches, and thenobtains the global complexity perception ability. Extensive experiments onIC9600 demonstrate that CLICv2 significantly outperforms existing unsupervisedmethods in PCC and SRCC, achieving content-invariant complexity representationwithout introducing positive pairs bias.</description>
      <author>example@mail.com (Shipeng Liu, Liang Zhao, Dengfeng Chen)</author>
      <guid isPermaLink="false">2503.06641v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Learning A Zero-shot Occupancy Network from Vision Foundation Models via Self-supervised Adaptation</title>
      <link>http://arxiv.org/abs/2503.07125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新方法，旨在简化从2D单目图像估计3D世界的过程。&lt;h4&gt;背景&lt;/h4&gt;由于3D注释需要大量人力工作，因此从2D单目图像中推断出精确的3D信息是一项具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种创新性技术，通过解耦3D监督以简化标签获取，并将2D视觉基础模型（VFMs）与3D任务相连接。&lt;h4&gt;方法&lt;/h4&gt;利用视觉语言模型进行零样本学习处理图像语义，同时通过时间一致性优化从VFMs中提取的相对深度值来适应度量深度。基于此，可以使用重建的度量深度将2D图像中的语义信息投影到3D空间，并提供相应的监督。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该方法在nuScenes和SemanticKITTI数据集上表现出色，在nuScenes上的Voxel Occupancy预测任务中超越现有最佳技术，mIoU提升3.34%。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架证明了其有效性和潜力，为简化从2D图像到3D场景的转换提供了新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the 3D world from 2D monocular images is a fundamental yetchallenging task due to the labour-intensive nature of 3D annotations. Tosimplify label acquisition, this work proposes a novel approach that bridges 2Dvision foundation models (VFMs) with 3D tasks by decoupling 3D supervision intoan ensemble of image-level primitives, e.g., semantic and geometric components.As a key motivator, we leverage the zero-shot capabilities of vision-languagemodels for image semantics. However, due to the notorious ill-posed problem -multiple distinct 3D scenes can produce identical 2D projections, directlyinferring metric depth from a monocular image in a zero-shot manner isunsuitable. In contrast, 2D VFMs provide promising sources of relative depth,which theoretically aligns with metric depth when properly scaled and offset.Thus, we adapt the relative depth derived from VFMs into metric depth byoptimising the scale and offset using temporal consistency, also known as novelview synthesis, without access to ground-truth metric depth. Consequently, weproject the semantics into 3D space using the reconstructed metric depth,thereby providing 3D supervision. Extensive experiments on nuScenes andSemanticKITTI demonstrate the effectiveness of our framework. For instance, theproposed method surpasses the current state-of-the-art by 3.34% mIoU onnuScenes for voxel occupancy prediction.</description>
      <author>example@mail.com (Sihao Lin, Daqi Liu, Ruochong Fu, Dongrui Liu, Andy Song, Hongwei Xie, Zhihui Li, Bing Wang, Xiaojun Chang)</author>
      <guid isPermaLink="false">2503.07125v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>AKF-LIO: LiDAR-Inertial Odometry with Gaussian Map by Adaptive Kalman Filter</title>
      <link>http://arxiv.org/abs/2503.06891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS 2025 Conference,  https://github.com/xpxie/AKF-LIO.git&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自适应卡尔曼滤波器框架，用于动态估计激光雷达和惯性测量单元观测数据的噪声协方差。&lt;h4&gt;背景&lt;/h4&gt;现有的LiDAR-Inertial Odometry (LIO)系统在状态估计时使用特定传感器或环境依赖的测量协方差，导致参数调优繁琐且在挑战条件下（如传感器退化及噪声观察）表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种自适应卡尔曼滤波器框架来解决现有LiDAR-Inertial Odometry (LIO)系统的不足。&lt;h4&gt;方法&lt;/h4&gt;['动态估计激光雷达和IMU测量的时间变化噪声协方差，根据上下文调整传感器之间的权重；', '引入一个基于高斯的地图表示以建模环境的平面性和空间噪声；', '采用相关注册策略确保通过伪合并（pseudo-merge）准确地进行平面法线估计，即使在非结构化环境中也能保证准确性']&lt;h4&gt;主要发现&lt;/h4&gt;['自适应卡尔曼滤波器框架能够在激光雷达退化时优先使用IMU数据，并抑制来自移动物体或嘈杂点云的不可靠输入；', '该系统通过实验验证其在各种环境下的鲁棒性，包括动态场景和几何退化情景']&lt;h4&gt;结论&lt;/h4&gt;['所提出的方法实现了跨MARS-LVIG序列的可靠定位结果，并在KITTI Odometry Benchmark中排名第八；', '代码将公开发布于https://github.com/xpxie/AKF-LIO.git。']&lt;h4&gt;翻译&lt;/h4&gt;现有的LiDAR-Inertial Odometry (LIO)系统通常使用传感器特异性或环境依赖的测量协方差进行状态估计，导致参数调整繁琐且在挑战条件下（如传感器退化和噪声观察）表现不佳。因此，我们提出了一个自适应卡尔曼滤波器框架，该框架动态估计激光雷达和惯性测量单元（IMU）测量的时间变化噪声协方差，实现了基于上下文感知的传感器权重调整。在LiDAR退化时，系统优先使用IMU数据，并抑制来自移动物体或嘈杂点云等不可靠输入的影响。此外，该论文引入了一个紧凑型高斯地图表示形式以建模环境的平面性和空间噪声。相关注册策略确保了通过伪合并进行准确平面法线估计的能力，即使在非结构化环境中如森林中也适用。广泛的实验验证了所提出系统在各种环境下的鲁棒性，包括动态场景和几何退化情景。我们的方法实现了MARS-LVIG序列中的可靠定位结果，并且在KITTI Odometry Benchmark上排名第8位。代码将在https://github.com/xpxie/AKF-LIO.git发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing LiDAR-Inertial Odometry (LIO) systems typically use sensor-specificor environment-dependent measurement covariances during state estimation,leading to laborious parameter tuning and suboptimal performance in challengingconditions (e.g., sensor degeneracy and noisy observations). Therefore, wepropose an Adaptive Kalman Filter (AKF) framework that dynamically estimatestime-varying noise covariances of LiDAR and Inertial Measurement Unit (IMU)measurements, enabling context-aware confidence weighting between sensors.During LiDAR degeneracy, the system prioritizes IMU data while suppressingcontributions from unreliable inputs like moving objects or noisy point clouds.Furthermore, a compact Gaussian-based map representation is introduced to modelenvironmental planarity and spatial noise. A correlated registration strategyensures accurate plane normal estimation via pseudo-merge, even in unstructuredenvironments like forests. Extensive experiments validate the robustness of theproposed system across diverse environments, including dynamic scenes andgeometrically degraded scenarios. Our method achieves reliable localizationresults across all MARS-LVIG sequences and ranks 8th on the KITTI OdometryBenchmark. The code will be released at https://github.com/xpxie/AKF-LIO.git.</description>
      <author>example@mail.com (Xupeng Xie, Ruoyu Geng, Jun Ma, Boyu Zhou)</author>
      <guid isPermaLink="false">2503.06891v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>opXRD: Open Experimental Powder X-ray Diffraction Database</title>
      <link>http://arxiv.org/abs/2503.05577v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本论文介绍了一项旨在促进粉末X射线衍射(pXRD)数据自动化分析的研究，通过建立一个开放的实验性粉末X射线衍射数据库(opXRD)，提供给研究人员一个包含大量标记和未标记实验数据的数据集。&lt;h4&gt;背景&lt;/h4&gt;pXRD实验是材料结构表征中的关键方法。然而，分析pXRD图谱仍然面临自动化挑战，特别是在高通量发现的自驱动实验室中遇到了瓶颈。&lt;h4&gt;目的&lt;/h4&gt;通过提供高质量的实验数据集(opXRD)，促进机器学习模型在实际pXRD数据分析中的应用和改进。&lt;h4&gt;方法&lt;/h4&gt;收集并公开了92552个衍射图谱，其中2179个为标记数据。这些数据来自多种材料类别。&lt;h4&gt;主要发现&lt;/h4&gt;训练于模拟pXRD模式的机器学习模型在处理实际实验图案时表现不佳，特别是对于低质量、高噪声和背景升高的实验图案。&lt;h4&gt;结论&lt;/h4&gt;通过开放opXRD数据库，期望能指导机器学习研究向全自动化的pXRD数据分析方向发展，并推动未来自驱动材料实验室的进步。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了粉末X射线衍射(pXRD)在材料结构表征中的重要性及面临的自动化挑战。论文介绍了一种新的开放实验性粉末X射线衍射数据库(opXRD)，以解决缺乏足够大小的实验数据集问题，为机器学习模型提供高质量的数据支持，并通过这些数据改善和验证机器学习模型性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Powder X-ray diffraction (pXRD) experiments are a cornerstone for materialsstructure characterization. Despite their widespread application, analyzingpXRD diffractograms still presents a significant challenge to automation and abottleneck in high-throughput discovery in self-driving labs. Machine learningpromises to resolve this bottleneck by enabling automated powder diffractionanalysis. A notable difficulty in applying machine learning to this domain isthe lack of sufficiently sized experimental datasets, which has constrainedresearchers to train primarily on simulated data. However, models trained onsimulated pXRD patterns showed limited generalization to experimental patterns,particularly for low-quality experimental patterns with high noise levels andelevated backgrounds. With the Open Experimental Powder X-Ray DiffractionDatabase (opXRD), we provide an openly available and easily accessible datasetof labeled and unlabeled experimental powder diffractograms. Labeled opXRD datacan be used to evaluate the performance of models on experimental data andunlabeled opXRD data can help improve the performance of models on experimentaldata, e.g. through transfer learning methods. We collected 92552diffractograms, 2179 of them labeled, from a wide spectrum of materialsclasses. We hope this ongoing effort can guide machine learning research towardfully automated analysis of pXRD data and thus enable future self-drivingmaterials labs.</description>
      <author>example@mail.com (Daniel Hollarek, Henrik Schopmans, Jona Östreicher, Jonas Teufel, Bin Cao, Adie Alwen, Simon Schweidler, Mriganka Singh, Tim Kodalle, Hanlin Hu, Gregoire Heymans, Maged Abdelsamie, Arthur Hardiagon, Alexander Wieczorek, Siarhei Zhuk, Ruth Schwaiger, Sebastian Siol, François-Xavier Coudert, Moritz Wolf, Carolin M. Sutter-Fella, Ben Breitung, Andrea M. Hodge, Tong-yi Zhang, Pascal Friederich)</author>
      <guid isPermaLink="false">2503.05577v2</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>GIN-Graph: A Generative Interpretation Network for Model-Level Explanation of Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.06352v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一个用于生成可靠的图神经网络（GNN）模型级解释图的生成解释网络GIN-Graph。&lt;h4&gt;背景&lt;/h4&gt;在现实场景中应用图神经网络时，它们通常被视为黑盒系统，导致需要增强其可解释性。现有的模型级别解释方法存在一些限制，例如产生无效的解释图形以及需要手动精细调整超参数。&lt;h4&gt;目的&lt;/h4&gt;旨在通过提出一种新的生成式解释网络GIN-Graph来克服现有方法的局限性，并能够为各种图数据集上训练的不同GNN模型提供有意义的解释图。&lt;h4&gt;方法&lt;/h4&gt;利用隐式且不依赖于似然性的生成对抗网络来构造与原始图形相似的解释图形，同时通过采用新颖的目标函数最大化对特定类别的预测概率。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，GIN-Graph能够被轻易地应用于在各种图数据集上训练的不同GNN模型，以创建有意义的解释图形，并且无需进行广泛的超参数精细调整。&lt;h4&gt;结论&lt;/h4&gt;所提出的生成式解释网络GIN-Graph为增强GNN模型级可解释性提供了一种有效的方法。&lt;h4&gt;翻译&lt;/h4&gt;利用图神经网络在现实场景中的一个主要挑战是它们被视为黑盒子，因此需要提高其透明度。模型级别的解释说明了哪些模式能够最大限度地增加某一类别的预测概率。然而，现有的模型级别解释方法存在一些限制，例如生成无效的解释图形并要求手动进行极端精细调整以优化超参数设置。本文提出了一种新的生成式解释网络GIN-Graph，用于为图神经网络生成可靠的模型级别解释图。通过利用隐式且不依赖于似然性的生成对抗网络来构造与原始图相似的解释图，并通过采用新颖的目标函数最大限度地提高特定类别的预测概率。实验结果表明，GIN-Graph可以容易应用于在各种图数据集上训练的不同GNN模型，以创建有意义的解释图形，同时不需要广泛的超参数调整。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One significant challenge of exploiting Graph neural networks (GNNs) inreal-life scenarios is that they are always treated as black boxes, thereforeleading to the requirement of interpretability. Model-level interpretationsexplain what patterns maximize probability of predicting to a certain class.However, existing model-level interpretation methods pose several limitationssuch as generating invalid explanation graphs and requiring extreme fine-tuningon hyperparameters manually. In this paper, we propose a new GenerativeInterpretation Network for Model-Level Explanation of Graph Neural Networks(GIN-Graph), to generate reliable model-level explanation graphs. The implicitand likelihood-free generative adversarial networks are exploited to constructexplanation graphs similar to original graphs, meanwhile maximizing theprediction probability for a certain class by adopting a novel objectivefunction. Experimental results indicate that GIN-Graph can be easily applied toGNN models trained on a variety of graph datasets to create meaningfulexplanation graphs without requiring extensive fine-tuning on hyperparameters.</description>
      <author>example@mail.com (Xiao Yue, Guangzhi Qu, Lige Gan)</author>
      <guid isPermaLink="false">2503.06352v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>HFedCKD: Toward Robust Heterogeneous Federated Learning via Data-free Knowledge Distillation and Two-way Contrast</title>
      <link>http://arxiv.org/abs/2503.06511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于无数据知识蒸馏和双向对比的异构联邦学习方法（HFedCKD），以解决参与客户端数量有限制以及各客户端贡献不均等问题，提高了模型在低参与率下的性能和稳定性。&lt;h4&gt;背景&lt;/h4&gt;现有的联邦学习框架通常被视为静态过程，并未充分考虑系统动态特性。由于通信预算限制及灵活的模型架构，在知识传输中需要较低的参与度并且活跃客户对系统的贡献并不均匀。此外，客户端规模也严重影响了联邦学习的表现。&lt;h4&gt;目的&lt;/h4&gt;在更广泛和实用的情境下提出一个新方法来改进传统的联邦学习框架，使之更加适应多变性和实用性。&lt;h4&gt;方法&lt;/h4&gt;引入逆概率加权蒸馏（IPWD）策略到无数据知识传输框架中。通过生成器完成非参与客户端的数据特征，并基于反偏置加权预测损失调整每个客户端的权重分布以公平整合活跃客户的知识；同时，本地模型被分割为一个功能提取器和分类器部分，在不同级别的对比学习中实现特征提取器与全局模型在特性空间上的对齐以及保持个性化决策能力。&lt;h4&gt;主要发现&lt;/h4&gt;HFedCKD方法有效缓解了低参与率下无数据知识蒸馏导致的知识偏差问题，提高了模型性能和稳定性。&lt;h4&gt;结论&lt;/h4&gt;通过在图像和物联网数据集上进行广泛实验，验证了所提出的HFedCKD框架的泛化能力和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;大多数当前的联邦学习框架被视为静态过程，忽略了学习系统的动态特性。在中心服务器有限通信预算下，大量客户端参与知识转移所需的灵活模型架构需要较低的参与率，并且活跃客户的贡献不均匀，客户端规模严重影响了FL的表现。我们考虑了一个更通用和实用的联合场景并提出了一种基于无数据知识蒸馏和双向对比的数据系统异构联邦方法（HFedCKD）。我们将逆概率加权蒸馏策略应用到无数据知识转移框架中。生成器完成了非参与客户的的数据特征完成。IPWD实现了在不同数据分布下每个客户预测贡献的动态评估，根据其预测损失进行反偏置加权后有效调整各客户端权重分配以公平整合活跃客户的知识。同时，本地模型被分割成一个功能提取器和分类器部分。通过不同的对比学习使特征提取器与全局模型在特性空间上对齐的同时保持个性化决策能力。HFedCKD有效地缓解了无数据知识蒸馏下低参与率造成的知识偏差，并提高了模型的性能和稳定性。我们在图像和物联网数据集上进行了广泛的实验，以全面评估并验证所提出的HFedCKD框架的泛化能力和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most current federated learning frameworks are modeled as static processes,ignoring the dynamic characteristics of the learning system. Under the limitedcommunication budget of the central server, the flexible model architecture ofa large number of clients participating in knowledge transfer requires a lowerparticipation rate, active clients have uneven contributions, and the clientscale seriously hinders the performance of FL. We consider a more general andpractical federation scenario and propose a system heterogeneous federationmethod based on data-free knowledge distillation and two-way contrast(HFedCKD). We apply the Inverse Probability Weighted Distillation (IPWD)strategy to the data-free knowledge transfer framework. The generator completesthe data features of the nonparticipating clients. IPWD implements a dynamicevaluation of the prediction contribution of each client under different datadistributions. Based on the antibiased weighting of its prediction loss, theweight distribution of each client is effectively adjusted to fairly integratethe knowledge of participating clients. At the same time, the local model issplit into a feature extractor and a classifier. Through differential contrastlearning, the feature extractor is aligned with the global model in the featurespace, while the classifier maintains personalized decision-makingcapabilities. HFedCKD effectively alleviates the knowledge offset caused by alow participation rate under data-free knowledge distillation and improves theperformance and stability of the model. We conduct extensive experiments onimage and IoT datasets to comprehensively evaluate and verify thegeneralization and robustness of the proposed HFedCKD framework.</description>
      <author>example@mail.com (Yiting Zheng, Bohan Lin, Jinqian Chen, Jihua Zhu)</author>
      <guid isPermaLink="false">2503.06511v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Global Context Is All You Need for Parallel Efficient Tractography Parcellation</title>
      <link>http://arxiv.org/abs/2503.07104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 2 pages references, 3 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文介绍了PETParc，一种新的高效白质束分类方法，用于弥散MRI中的全脑纤维追踪。与现有技术相比，该方法在准确性相当甚至更高的情况下，显著提高了计算效率。&lt;h4&gt;背景&lt;/h4&gt;目前的全脑纤维追踪后处理步骤通常涉及将每个纤维轨迹分配给特定的白质束或标记为假阳性错误。这种分割过程对于大规模研究和临床应用都至关重要，特别是在资源有限的情况下更是如此。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法PETParc，旨在提高全脑纤维追踪分类的速度与准确性。&lt;h4&gt;方法&lt;/h4&gt;PETParc是一种基于转换器架构的方法，在该架构中将整个大脑的纤维轨迹随机分割成多个子集，然后对每个子集中的纤维进行并行分类。这种方法减少了计算资源需求，提高了处理速度。&lt;h4&gt;主要发现&lt;/h4&gt;在评估过程中，研究人员发现局部上下文对于TractCloud方法的准确性贡献不大，并且在处理病理性案例时甚至会产生负面影响。PETParc通过引入新的旋转不变性嵌入或者利用翻转作为数据增强的一部分来解决了纤维轨迹方向信息缺乏的问题。&lt;h4&gt;结论&lt;/h4&gt;尽管速度上有显著提升，但与之前的分类方法相比，PETParc在准确性上通常更为优越，并且能够在不配备GPU的临床工作站中运行。作者承诺论文接受后将公开代码和预训练模型。&lt;h4&gt;翻译&lt;/h4&gt;摘要：弥散MRI中的全脑追踪之后常会进行分割处理，即将每一条纤维轨迹归类为特定白质束的一部分或标记为假阳性错误。高效的分割在需要处理大量数据的大规模研究中至关重要，在资源受限的临床环境中同样重要。TractCloud是一种前沿的方法，旨在通过局部-全局表示来最大化准确性。我们证明了这种方法中的局部上下文对准确性贡献不大，并且在处理病理性案例时甚至会产生负面影响。基于这一观察，我们提出了PETParc，这是一种新的并行高效纤维追踪分割方法。PETParc是一个转换器架构，在该架构中将整个大脑的追踪结果随机拆分为子集，每个子集中轨迹可以并行分类，同时作为彼此的全局上下文存在。这导致相对于TractCloud来说速度提高了两个数量级，并且在不配备GPU的临床工作站上也可以进行推理操作。PETParc通过一种新颖的翻转变换不变性嵌入或者简单地使用翻转作为数据增强的一部分来应对缺乏纤维轨迹方向信息的问题。尽管有显著的速度提升，结果通常比先前的方法更好。代码和预训练模型将在论文被接受后公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Whole-brain tractography in diffusion MRI is often followed by a parcellationin which each streamline is classified as belonging to a specific white matterbundle, or discarded as a false positive. Efficient parcellation is importantboth in large-scale studies, which have to process huge amounts of data, and inthe clinic, where computational resources are often limited. TractCloud is astate-of-the-art approach that aims to maximize accuracy with a local-globalrepresentation. We demonstrate that the local context does not contribute tothe accuracy of that approach, and is even detrimental when dealing withpathological cases. Based on this observation, we propose PETParc, a new methodfor Parallel Efficient Tractography Parcellation. PETParc is atransformer-based architecture in which the whole-brain tractogram is randomlypartitioned into sub-tractograms whose streamlines are classified in parallel,while serving as global context for each other. This leads to a speedup of upto two orders of magnitude relative to TractCloud, and permits inference evenon clinical workstations without a GPU. PETParc accounts for the lack ofstreamline orientation either via a novel flip-invariant embedding, or bysimply using flips as part of data augmentation. Despite the speedup, resultsare often even better than those of prior methods. The code and pretrainedmodel will be made public upon acceptance.</description>
      <author>example@mail.com (Valentin von Bornhaupt, Johannes Grün, and Justus Bisten, Tobias Bauer, Theodor Rüber, Thomas Schultz)</author>
      <guid isPermaLink="false">2503.07104v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>DynCIM: Dynamic Curriculum for Imbalanced Multimodal Learning</title>
      <link>http://arxiv.org/abs/2503.06456v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;摘要介绍了DynCIM，这是一种新的动态课程学习框架，用于量化模态和样本不平衡，并引入了一种基于门控的动态融合机制以优化多模式学习。&lt;h4&gt;背景&lt;/h4&gt;多模态学习结合了来自不同模态的信息来增强决策过程。然而由于数据质量差异和表示能力的不同，这种协作潜力尚未得到充分利用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法DynCIM来解决样本和模态之间的不平衡问题，从而提高多模式学习中的适应性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;通过引入基于预测偏差、一致性和稳定性动态评估每个样本难度的样本级课程以及根据全局和局部贡献衡量模态贡献的模态级课程。还提出了一个基于门控机制来调整模态贡献的动态融合方式，以减少冗余并优化融合效果。&lt;h4&gt;主要发现&lt;/h4&gt;在六个跨二元和三元多模式基准数据集上进行了广泛的实验，表明DynCIM始终优于现有的最新方法，并且有效缓解了模态和样本不平衡问题。&lt;h4&gt;结论&lt;/h4&gt;提出的DynCIM框架通过引入动态课程学习和门控机制，在减轻多模态学习任务中样本及模态不平衡方面表现出了优势。源代码可在GitHub上获得。&lt;h4&gt;翻译&lt;/h4&gt;多模态学习结合不同模态的信息以增强决策过程，但数据质量和表示能力的差异阻碍了这种潜力的实现。为解决此问题，我们提出了一种名为DynCIM的新方法，该框架通过样本和模态两个层面量化固有的不平衡，并引入基于门控的动态融合机制来优化多模态学习任务中的适应性和鲁棒性。实验表明，在多个数据集上，DynCIM优于现有的最佳方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning integrates complementary information from diversemodalities to enhance the decision-making process. However, the potential ofmultimodal collaboration remains under-exploited due to disparities in dataquality and modality representation capabilities. To address this, we introduceDynCIM, a novel dynamic curriculum learning framework designed to quantify theinherent imbalances from both sample and modality perspectives. DynCIM employsa sample-level curriculum to dynamically assess each sample's difficultyaccording to prediction deviation, consistency, and stability, while amodality-level curriculum measures modality contributions from global andlocal. Furthermore, a gating-based dynamic fusion mechanism is introduced toadaptively adjust modality contributions, minimizing redundancy and optimizingfusion effectiveness. Extensive experiments on six multimodal benchmarkingdatasets, spanning both bimodal and trimodal scenarios, demonstrate that DynCIMconsistently outperforms state-of-the-art methods. Our approach effectivelymitigates modality and sample imbalances while enhancing adaptability androbustness in multimodal learning tasks. Our code is available athttps://github.com/Raymond-Qiancx/DynCIM.</description>
      <author>example@mail.com (Chengxuan Qian, Kai Han, Jingchao Wang, Zhenlong Yuan, Rui Qian, Chongwen Lyu, Jun Chen, Zhe Liu)</author>
      <guid isPermaLink="false">2503.06456v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Accessing the Effect of Phyllotaxy and Planting Density on Light Use Efficiency in Field-Grown Maize using 3D Reconstructions</title>
      <link>http://arxiv.org/abs/2503.06887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;高密度种植是一种提高玉米产量的广泛策略，但同时也带来了诸如植株间竞争加剧和遮阴等挑战。为应对这些问题，一些玉米植物会自然地重新定向其叶冠以优化光照捕捉。研究介绍了一种集成了实地生长玉米的真实3D重建与光合作用有效辐射（PAR）建模于一体的端到端框架，评估了叶序和种植密度对光线拦截的影响。&lt;h4&gt;背景&lt;/h4&gt;高密度种植可以提高作物产量但会导致植株间竞争加剧以及光照受限。一些植物会通过冠层重新定向来优化光照捕捉能力。&lt;h4&gt;目的&lt;/h4&gt;理解这种适应性响应及其对光捕获的影响，以最大限度地发挥农业潜在收成。&lt;h4&gt;方法&lt;/h4&gt;利用实地数据获取的3D点云构建虚拟田地，并基于这些模型评估不同叶序、种植密度和冠层方向下的PAR拦截效果。使用该框架分析了整个生长季节中植株间距与行向对光合作用有效辐射（PAR）捕捉的影响。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了在不同的种植密度和冠层定向下，光照捕获效率存在显著差异。&lt;h4&gt;结论&lt;/h4&gt;通过阐明冠层结构与光捕获之间的关系，这项研究表明，在不同农业环境中优化玉米育种及栽培策略方面提供了宝贵指导。&lt;h4&gt;翻译&lt;/h4&gt;高密度种植是一种提高玉米产量的广泛采用的战略，但也带来了诸如增加植株间竞争和遮阴等挑战。为了应对这些挑战，一些玉米植物会自然地重新定向其冠层以优化光照捕捉，这一过程被称为冠层再定向。理解这种适应性响应及其对光捕获的影响对于最大化农业收成潜力至关重要。这项研究提出了一种端到端框架，结合了实地生长玉米的真实3D重建与光合作用有效辐射（PAR）建模来评估叶序和种植密度对光线拦截的效果。利用从现场数据中获得的3D点云构建虚拟田地，并将其与实际PAR测量结果进行对比验证。使用此框架分析了整个生长季节中的冠层定向、植株间距以及行向等因素对PAR捕捉的影响。研究发现，不同的种植密度和冠层方向下存在显著差异化的光捕获效率。通过阐明冠层结构与光捕获之间的关系，这项研究表明在不同农业环境中优化玉米育种及栽培策略方面提供了宝贵的指导信息。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-density planting is a widely adopted strategy to enhance maizeproductivity, yet it introduces challenges such as increased interplantcompetition and shading, which can limit light capture and overall yieldpotential. In response, some maize plants naturally reorient their canopies tooptimize light capture, a process known as canopy reorientation. Understandingthis adaptive response and its impact on light capture is crucial formaximizing agricultural yield potential. This study introduces an end-to-endframework that integrates realistic 3D reconstructions of field-grown maizewith photosynthetically active radiation (PAR) modeling to assess the effectsof phyllotaxy and planting density on light interception. In particular, using3D point clouds derived from field data, virtual fields for a diverse set ofmaize genotypes were constructed and validated against field PAR measurements.Using this framework, we present detailed analyses of the impact of canopyorientations, plant and row spacings, and planting row directions on PARinterception throughout a typical growing season. Our findings highlightsignificant variations in light interception efficiency across differentplanting densities and canopy orientations. By elucidating the relationshipbetween canopy architecture and light capture, this study offers valuableguidance for optimizing maize breeding and cultivation strategies acrossdiverse agricultural settings.</description>
      <author>example@mail.com (Nasla Saleem, Talukder Zaki Jubery, Aditya Balu, Yan Zhou, Yawei Li, Patrick S. Schnable, Adarsh Krishnamurthy, Baskar Ganapathysubramanian)</author>
      <guid isPermaLink="false">2503.06887v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>DynamicID: Zero-Shot Multi-ID Image Personalization with Flexible Facial Editability</title>
      <link>http://arxiv.org/abs/2503.06505v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;介绍了DynamicID，这是一种无调优的框架，支持双阶段训练范式，能够高效地进行单一和多个人身份的个性化图像生成。&lt;h4&gt;背景&lt;/h4&gt;近期在文本到图像生成领域的进展激发了对个性化人类图像生成的兴趣。现有的方法虽然能够在保持高度的身份保真度的情况下工作，但它们往往难以同时处理多个不同的个体，并且缺乏面部编辑的灵活性。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的框架DynamicID来解决现有方法存在的问题，旨在实现高保真的单一和多个人身份个性化生成以及灵活的面部编辑能力。&lt;h4&gt;方法&lt;/h4&gt;{'Semantic-Activated Attention (SAA)': '利用查询级激活门控技术，在不影响原始模型的情况下注入身份特征，从而支持多个人身份个性化的生成而不需要在训练过程中使用多个人的身份样本。', 'Identity-Motion Reconfigurator (IMR)': '通过对比学习有效地分离和重新组合面部运动和身份特征，以实现灵活的面部编辑功能。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'创新点': ['Semantic-Activated Attention (SAA)', 'Identity-Motion Reconfigurator (IMR)'], '数据集开发': '创建了一个名为VariFace-10k的精心策划的人脸数据集，包含10,000个独特的个体，每个个体由35张不同的面部图像组成。'}&lt;h4&gt;结论&lt;/h4&gt;实验结果显示DynamicID在身份保真度、面部编辑能力和多个人身份个性化的生成能力上优于现有的最先进方法。&lt;h4&gt;翻译&lt;/h4&gt;最近在文本到图像生成领域的进展已经激发了对个性化人类图像生成的兴趣，这旨在根据指示的参考图像创建包含特定人物的新颖图像。尽管现有方法能够在保持高度的身份保真度的情况下工作，但它们往往难以同时处理多个不同的个体，并且缺乏面部编辑的灵活性。我们提出了一种无需调优的框架DynamicID，支持双阶段训练范式，能够高效地进行单一和多个人身份的个性化图像生成，并具有高保真度和灵活的面部编辑能力。我们的主要创新包括：1）利用查询级激活门控技术，在不影响原始模型的情况下注入身份特征，从而支持多个人身份个性化的生成而不需要在训练过程中使用多个人的身份样本；2）通过对比学习有效地分离和重新组合面部运动和身份特征，以实现灵活的面部编辑功能。此外，我们还开发了一个名为VariFace-10k的数据集，其中包括了10,000个独特的个体，每个由35张不同的面部图像组成。实验结果显示DynamicID在身份保真度、面部编辑能力和多个人身份个性化的生成能力上优于现有的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in text-to-image generation have spurred interest inpersonalized human image generation, which aims to create novel imagesfeaturing specific human identities as reference images indicate. Althoughexisting methods achieve high-fidelity identity preservation, they oftenstruggle with limited multi-ID usability and inadequate facial editability. Wepresent DynamicID, a tuning-free framework supported by a dual-stage trainingparadigm that inherently facilitates both single-ID and multi-ID personalizedgeneration with high fidelity and flexible facial editability. Our keyinnovations include: 1) Semantic-Activated Attention (SAA), which employsquery-level activation gating to minimize disruption to the original model wheninjecting ID features and achieve multi-ID personalization without requiringmulti-ID samples during training. 2) Identity-Motion Reconfigurator (IMR),which leverages contrastive learning to effectively disentangle and re-entanglefacial motion and identity features, thereby enabling flexible facial editing.Additionally, we have developed a curated VariFace-10k facial dataset,comprising 10k unique individuals, each represented by 35 distinct facialimages. Experimental results demonstrate that DynamicID outperformsstate-of-the-art methods in identity fidelity, facial editability, and multi-IDpersonalization capability.</description>
      <author>example@mail.com (Xirui Hu, Jiahao Wang, Hao Chen, Weizhan Zhang, Benqi Wang, Yikun Li, Haishun Nan)</author>
      <guid isPermaLink="false">2503.06505v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Spectral State Space Model for Rotation-Invariant~Visual~Representation~Learning</title>
      <link>http://arxiv.org/abs/2503.06369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Spectral VMamba，这是一种新的状态空间模型（SSM），通过利用图像补丁的谱信息来捕捉全局结构，并实现了旋转不变性。&lt;h4&gt;背景&lt;/h4&gt;状态空间模型(SSMs)作为一种替代视觉变压器(ViTs)的方法出现，能够以线性复杂度建模全局关系。然而，SSMs在识别概念上相关的但不相邻的图像块之间的关系时存在局限性，这归因于图像数据的非因果性质以及对旋转等变换的高度敏感。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有视觉基础SSMs的限制，论文提出了一种新的方法Spectral VMamba来捕捉图像中的全局结构，并且在保持运行效率的同时实现旋转不变性。&lt;h4&gt;方法&lt;/h4&gt;通过利用从图像补丁图拉普拉斯算子中提取出的谱信息进行分解，该模型独立于原始图像方向编码了补丁关系。作者还引入了一个名为Rotational Feature Normalizer (RFN)的模块来帮助达到旋转不变性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，Spectral VMamba在分类任务上超越了目前领先的SSM模型（如VMamba），并且保持对旋转变换的不变性和与这些领先模型相似的时间效率。&lt;h4&gt;结论&lt;/h4&gt;通过引入谱信息和RFN模块，Spectral VMamba不仅克服了现有视觉基础SSMs的局限性，而且提升了性能，在处理图像数据时表现出色且高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State Space Models (SSMs) have recently emerged as an alternative to VisionTransformers (ViTs) due to their unique ability of modeling globalrelationships with linear complexity. SSMs are specifically designed to capturespatially proximate relationships of image patches. However, they fail toidentify relationships between conceptually related yet not adjacent patches.This limitation arises from the non-causal nature of image data, which lacksinherent directional relationships. Additionally, current vision-based SSMs arehighly sensitive to transformations such as rotation. Their predefined scanningdirections depend on the original image orientation, which can cause the modelto produce inconsistent patch-processing sequences after rotation. To addressthese limitations, we introduce Spectral VMamba, a novel approach thateffectively captures the global structure within an image by leveragingspectral information derived from the graph Laplacian of image patches. Throughspectral decomposition, our approach encodes patch relationships independentlyof image orientation, achieving rotation invariance with the aid of ourRotational Feature Normalizer (RFN) module. Our experiments on classificationtasks show that Spectral VMamba outperforms the leading SSM models in vision,such as VMamba, while maintaining invariance to rotations and a providing asimilar runtime efficiency.</description>
      <author>example@mail.com (Sahar Dastani, Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, David Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Milad Cheraghalikhani, Arnab Kumar Mondal, Herve Lombaert, Christian Desrosiers)</author>
      <guid isPermaLink="false">2503.06369v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Heterogeneous bimodal attention fusion for speech emotion recognition</title>
      <link>http://arxiv.org/abs/2503.06405v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一个新的框架Heterogeneous Bimodal Attention Fusion (HBAF)，用于解决多模态情感识别中的异质性模态差距问题。&lt;h4&gt;背景&lt;/h4&gt;在对话中进行多模态情感识别是一个具有挑战性的任务，主要是由于音频和文本线索之间的复杂且互补的交互作用。&lt;h4&gt;目的&lt;/h4&gt;旨在通过引入新的框架来弥合低级音频表示与高级文本表示之间的异质模态差距，并改善跨模态关系过滤以及充分利用每个模态内的互动。&lt;h4&gt;方法&lt;/h4&gt;{'模块1': 'uni-modal representation module，该模块将上下文信息融入到低层的音频表示中以减少多模态差异。', '模块2': 'multi-modal fusion module，使用动态双模注意力和动态门控机制来过滤错误的跨模式关系，并充分利用每个模态内的互动以及跨模态互动。', '模块3': 'inter-modal contrastive learning module，捕捉音频与文本模态之间复杂的绝对和相对互动。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示提出的HBAF方法在MELD和IEMOCAP数据集上优于现有的最先进的基准方法。&lt;h4&gt;结论&lt;/h4&gt;通过新的多级别交互框架可以有效地解决多模态情感识别中的问题，改进了音频与文本之间的异质性差异。&lt;h4&gt;翻译&lt;/h4&gt;多模态对话中进行的情感识别是一个具有挑战性的任务，由于不同模态之间存在复杂的互补互动。本文提出了一种新颖的框架——Heterogeneous Bimodal Attention Fusion (HBAF)，以解决低级音频表示和高级文本表示之间的异质性差距问题，实验表明该方法优于现有最先进的基准方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal emotion recognition in conversations is a challenging problem dueto the complex and complementary interactions between different modalities.Audio and textual cues are particularly important for understanding emotionsfrom a human perspective. Most existing studies focus on exploring interactionsbetween audio and text modalities at the same representation level. However, acritical issue is often overlooked: the heterogeneous modality gap betweenlow-level audio representations and high-level text representations. To addressthis problem, we propose a novel framework called Heterogeneous BimodalAttention Fusion (HBAF) for multi-level multi-modal interaction inconversational emotion recognition. The proposed method comprises three keymodules: the uni-modal representation module, the multi-modal fusion module,and the inter-modal contrastive learning module. The uni-modal representationmodule incorporates contextual content into low-level audio representations tobridge the heterogeneous multi-modal gap, enabling more effective fusion. Themulti-modal fusion module uses dynamic bimodal attention and a dynamic gatingmechanism to filter incorrect cross-modal relationships and fully exploit bothintra-modal and inter-modal interactions. Finally, the inter-modal contrastivelearning module captures complex absolute and relative interactions betweenaudio and text modalities. Experiments on the MELD and IEMOCAP datasetsdemonstrate that the proposed HBAF method outperforms existing state-of-the-artbaselines.</description>
      <author>example@mail.com (Jiachen Luo, Huy Phan, Lin Wang, Joshua Reiss)</author>
      <guid isPermaLink="false">2503.06405v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal LLMs</title>
      <link>http://arxiv.org/abs/2503.06362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为Llama-MTSK的新模型，该模型结合了Matryoshka表示学习的方法，能够灵活地在计算资源限制下分配音频和视觉模态的令牌，并且保持高性能。此方法通过在同一模型中编码多种粒度级别的视听表示来消除为不同压缩级别训练单独模型的需求。&lt;h4&gt;背景&lt;/h4&gt;音频-视频语音识别（AVSR）结合了声音和视觉模式，以提高嘈杂环境下的语音识别鲁棒性。然而，在直接将长的语音表征与大语言模型（LLMs）整合时会带来计算成本的显著增加。&lt;h4&gt;目的&lt;/h4&gt;为了克服这一挑战并平衡计算效率与识别准确性之间的权衡，论文提出了Llama-MTSK这种新的多模态模型。&lt;h4&gt;方法&lt;/h4&gt;Llama-MTSK基于Matryoshka表示学习，允许根据特定的计算限制灵活地调整音频-视频令牌分配。此外，通过引入三种LoRA（低秩适应）策略来有效地微调LLM。&lt;h4&gt;主要发现&lt;/h4&gt;在两个最大的AVSR数据集上的广泛评估表明，Llama-MTSK达到了最先进的结果，在可变压缩级别下与独立训练的模型相比性能匹配或超越。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了一种创新的方法论，即利用Matryoshka表示学习和灵活调整令牌分配，使得在资源受限条件下实现高效的音频-视频语音识别成为可能。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了Audio-Visual Speech Recognition (AVSR)如何使用大型语言模型(如LLMs)来增强语音识别的鲁棒性。为了应对直接集成带来的计算成本问题，论文提出了Llama-MTSK模型，该模型在单一模型中采用多粒度视听表示学习的方法，并引入了LoRA策略来优化训练过程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-Visual Speech Recognition (AVSR) leverages both audio and visualmodalities to enhance speech recognition robustness, particularly in noisyenvironments. Recent advancements in Large Language Models (LLMs) havedemonstrated their effectiveness in speech recognition, including AVSR.However, due to the significant length of speech representations, directintegration with LLMs imposes substantial computational costs. Prior approachesaddress this by compressing speech representations before feeding them intoLLMs. However, higher compression ratios often lead to performance degradation,necessitating a trade-off between computational efficiency and recognitionaccuracy. To address this challenge, we propose Llama-MTSK, the firstMatryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation ofthe audio-visual token allocation based on specific computational constraintswhile preserving high performance. Our approach, inspired by MatryoshkaRepresentation Learning, encodes audio-visual representations at multiplegranularities within a single model, eliminating the need to train separatemodels for different compression levels. Moreover, to efficiently fine-tune theLLM, we introduce three LoRA-based Matryoshka strategies using global andscale-specific LoRA modules. Extensive evaluations on the two largest AVSRdatasets demonstrate that Llama-MTSK achieves state-of-the-art results,matching or surpassing models trained independently at fixed compressionlevels.</description>
      <author>example@mail.com (Umberto Cappellazzo, Minsu Kim, Stavros Petridis)</author>
      <guid isPermaLink="false">2503.06362v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Visual Embedding of Screen Sequences for User-Flow Search in Example-driven Communication</title>
      <link>http://arxiv.org/abs/2503.06067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has co-first authors: Daeheon Jeong, Hyehyun Chu. 9 pages,  4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了用户体验（UX）设计师如何有效地将UX考量传达给利益相关者的问题，通过采访四位UX从业者并设计了一种基于语义嵌入的方法来系统地检索用户流程。&lt;h4&gt;背景&lt;/h4&gt;有效的沟通用户体验的考虑因素对UX实践者来说是一个关键挑战。例如，UX实践者需要向设计师和开发人员清晰表达他们的想法。&lt;h4&gt;目的&lt;/h4&gt;研究旨在探索UX实践者的沟通挑战，并提出一种改进方法以更有效地传达用户体验设计的相关例子。&lt;h4&gt;方法&lt;/h4&gt;研究人员采访了四位UX从业者并分析其在沟通中的策略与遇到的问题。随后，他们提出了一种利用对比学习来将屏幕的视觉特征与其描述相匹配的方法，以便于系统地检索用户流程。&lt;h4&gt;主要发现&lt;/h4&gt;提供具体的示例用户流程（即代表语义任务的一系列屏幕）可以加强沟通效果，但找到合适的例子仍然是一个挑战。&lt;h4&gt;结论&lt;/h4&gt;设计了一种能够通过对比学习来关联屏幕视觉特征与用户流程描述的模型。这项研究还证实了这种方法检索出的相关性更能符合人类的认知和理解。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到的有效传达用户体验考量给设计师和开发人员等利益相关者是UX实践者的重大挑战，本研究表明提供具体示例用户流程可以有效加强沟通，但找到合适的例子仍是难题。团队通过采访四名从业者，提出了利用语义嵌入检索用户流程的方法，并通过调查证实了这种方法与人类感知的相关性更匹配。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective communication of UX considerations to stakeholders (e.g., designersand developers) is a critical challenge for UX practitioners. To explore thisproblem, we interviewed four UX practitioners about their communicationchallenges and strategies. Our study identifies that providing an example userflow-a screen sequence representing a semantic task-as evidence reinforcescommunication, yet finding relevant examples remains challenging. To addressthis, we propose a method to systematically retrieve user flows using semanticembedding. Specifically, we design a model that learns to associate screens'visual features with user flow descriptions through contrastive learning. Asurvey confirms that our approach retrieves user flows better aligned withhuman perceptions of relevance. We analyze the results and discuss implicationsfor the computational representation of user flows.</description>
      <author>example@mail.com (Daeheon Jeong, Hyehyun Chu)</author>
      <guid isPermaLink="false">2503.06067v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Distributed Graph Neural Network Inference With Just-In-Time Compilation For Industry-Scale Graphs</title>
      <link>http://arxiv.org/abs/2503.06208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by EuroSys 2025 (poster)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种分布式图学习的新处理范式，旨在解决大规模图形数据对图神经网络（GNN）推理的性能瓶颈。&lt;h4&gt;背景&lt;/h4&gt;随着图数据规模的急剧增加，图神经网络在计算复杂性和内存使用方面面临显著挑战，导致性能下降。虽然基于图采样的子图学习方法可以缓解这些问题，但它们会导致信息损失和高冗余计算等弊端。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的分布式处理框架，以克服现有图采样方法的缺点，并提高GNN在大规模图数据上的推理效率。&lt;h4&gt;方法&lt;/h4&gt;通过为GNN引入一组新的编程接口以及充分利用Just-In-Time (JIT) 编译技术来构建分布式图学习的新范式。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在包含最多5亿节点和224亿边的工业规模图上，该方法可以实现高达27.4倍的性能提升。&lt;h4&gt;结论&lt;/h4&gt;新提出的处理框架能够显著提高GNN在大规模图数据上的推理效率，并消除现有子图学习方法中的缺点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have delivered remarkable results in variousfields. However, the rapid increase in the scale of graph data has introducedsignificant performance bottlenecks for GNN inference. Both computationalcomplexity and memory usage have risen dramatically, with memory becoming acritical limitation. Although graph sampling-based subgraph learning methodscan help mitigate computational and memory demands, they come with drawbackssuch as information loss and high redundant computation among subgraphs. Thispaper introduces an innovative processing paradgim for distributed graphlearning that abstracts GNNs with a new set of programming interfaces andleverages Just-In-Time (JIT) compilation technology to its full potential. Thisparadigm enables GNNs to highly exploit the computational resources ofdistributed clusters by eliminating the drawbacks of subgraph learning methods,leading to a more efficient inference process. Our experimental resultsdemonstrate that on industry-scale graphs of up to \textbf{500 million nodesand 22.4 billion edges}, our method can produce a performance boost of up to\textbf{27.4 times}.</description>
      <author>example@mail.com (Xiabao Wu, Yongchao Liu, Wei Qin, Chuntao Hong)</author>
      <guid isPermaLink="false">2503.06208v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Task-Specific Knowledge Distillation from the Vision Foundation Model for Enhanced Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2503.06976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 10 figures, 16 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对医学图像分割任务的知识蒸馏框架，该框架能够利用大规模预训练视觉基础模型（VFMs）的知识来训练一个小型、特定任务的模型。实验结果显示，与任务无关的知识蒸馏和自监督预训练方法相比，提出的框架在数据有限的情况下性能更优。&lt;h4&gt;背景&lt;/h4&gt;现有的大规模预训练模型如视觉基础模型（VFM），由于计算成本高以及自然图像与医学图像之间的领域差异，在医学分割任务中的实际应用受到限制。&lt;h4&gt;目的&lt;/h4&gt;如何有效利用大预训练VFMs的知识来训练一个小型、特定任务的医学图像分割模型，特别是在数据有限的情况下。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖且通用的任务特定知识蒸馏框架。该框架首先在目标分割任务上微调VFM以捕获任务特定特征，然后通过低秩适应（LoRA）技术将知识传递给较小的模型，并利用扩散模型生成合成数据来增强迁移集。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在五个医学图像数据集中，提出的任务特定知识蒸馏方法在使用有限数量标记样本进行微调时，始终优于任务无关的知识蒸馏和自监督预训练方法。例如，在KidneyUS数据集上，与任务无关的KD相比，该方法提高了28%的Dice评分；在CHAOS数据集上，与MAE相比提高了11%。&lt;h4&gt;结论&lt;/h4&gt;提出的任务特定知识蒸馏框架具有潜力，可以在数据受限条件下训练准确且高效的医学图像分割模型。&lt;h4&gt;翻译&lt;/h4&gt;大规模预训练模型已经在各种下游任务中表现出色，尤其是在目标数据有限的情况下。然而，它们的高计算成本以及自然图像与医学图像之间的领域差距限制了其在医学分割任务中的实际应用。针对此问题，提出了一种新颖且通用的任务特定知识蒸馏框架来解决如何利用大型预训练VFMs的知识训练小型、特定任务模型的问题。该方法通过微调VFM并使用低秩适应技术将知识传递给较小的模型，并借助扩散模型生成的合成数据增强迁移集。实验结果显示，提出的框架在使用有限数量标记样本进行微调时始终优于任务无关的知识蒸馏和自监督预训练方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale pre-trained models, such as Vision Foundation Models (VFMs), havedemonstrated impressive performance across various downstream tasks bytransferring generalized knowledge, especially when target data is limited.However, their high computational cost and the domain gap between natural andmedical images limit their practical application in medical segmentation tasks.Motivated by this, we pose the following important question: "How can weeffectively utilize the knowledge of large pre-trained VFMs to train a small,task-specific model for medical image segmentation when training data islimited?" To address this problem, we propose a novel and generalizabletask-specific knowledge distillation framework. Our method fine-tunes the VFMon the target segmentation task to capture task-specific features beforedistilling the knowledge to smaller models, leveraging Low-Rank Adaptation(LoRA) to reduce the computational cost of fine-tuning. Additionally, weincorporate synthetic data generated by diffusion models to augment thetransfer set, enhancing model performance in data-limited scenarios.Experimental results across five medical image datasets demonstrate that ourmethod consistently outperforms task-agnostic knowledge distillation andself-supervised pretraining approaches like MoCo v3 and Masked Autoencoders(MAE). For example, on the KidneyUS dataset, our method achieved a 28% higherDice score than task-agnostic KD using 80 labeled samples for fine-tuning. Onthe CHAOS dataset, it achieved an 11% improvement over MAE with 100 labeledsamples. These results underscore the potential of task-specific knowledgedistillation to train accurate, efficient models for medical image segmentationin data-constrained settings.</description>
      <author>example@mail.com (Pengchen Liang, Haishan Huang, Bin Pu, Jianguo Chen, Xiang Hua, Jing Zhang, Weibo Ma, Zhuangzhuang Chen, Yiwei Li, Qing Chang)</author>
      <guid isPermaLink="false">2503.06976v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>TI-JEPA: An Innovative Energy-based Joint Embedding Strategy for Text-Image Multimodal Systems</title>
      <link>http://arxiv.org/abs/2503.06380v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的预训练策略Text-Image Joint Embedding Predictive Architecture (TI-JEPA)，旨在解决文本和图像模态之间的语义差距问题，通过能量模型(EBM)框架捕捉复杂的跨模态关系。&lt;h4&gt;背景&lt;/h4&gt;在人工智能领域中，特别是处理文本和图像模式时，由于两者之间存在的语义差距，导致多模态融合的有效性受到挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种创新的预训练策略TI-JEPA，利用EBM框架来捕捉复杂的跨模态关系，并促进文本与视觉元素之间的兼容性。&lt;h4&gt;方法&lt;/h4&gt;引入Text-Image Joint Embedding Predictive Architecture (TI-JEPA)，该架构结合了能量模型(EBM)在自监督学习中的灵活性，以提高多模态融合的效果。通过广泛的实验展示了其优越性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过跨多个基准的广泛实验表明，TI-JEPA在多模态情感分析任务上取得了最先进的性能，并且在视觉问答等其他基于多模态的任务中也表现出色，优于现有的预训练方法。&lt;h4&gt;结论&lt;/h4&gt;研究结果展示了利用能量模型框架来推进多模态融合的巨大潜力，并为下游应用提供了显著改进的可能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：本文专注于人工智能领域内的跨模态对齐问题，特别是在文本和图像模式方面。文本与视觉模式之间的语义差距构成了多模态融合效果上的差异性挑战。因此，我们介绍了Text-Image Joint Embedding Predictive Architecture (TI-JEPA)，这是一种基于能量模型(EBM)框架的创新预训练策略，用于捕捉复杂的跨模态关系。TI-JEPA结合了EBM在自监督学习中的灵活性，以促进文本与视觉元素之间的兼容性。通过多个基准上的广泛实验，我们证明了TI-JEPA在多模态情感分析任务上取得了最先进的性能（并可能适用于广泛的基于多模态的任务，如视觉问答），优于现有的预训练方法。我们的研究结果强调了使用能量框架的潜力来推进多模态融合，并为下游应用提供了显著改进的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper focuses on multimodal alignment within the realm of ArtificialIntelligence, particularly in text and image modalities. The semantic gapbetween the textual and visual modality poses a discrepancy problem towards theeffectiveness of multi-modalities fusion. Therefore, we introduce Text-ImageJoint Embedding Predictive Architecture (TI-JEPA), an innovative pre-trainingstrategy that leverages energy-based model (EBM) framework to capture complexcross-modal relationships. TI-JEPA combines the flexibility of EBM inself-supervised learning to facilitate the compatibility between textual andvisual elements. Through extensive experiments across multiple benchmarks, wedemonstrate that TI-JEPA achieves state-of-the-art performance on multimodalsentiment analysis task (and potentially on a wide range of multimodal-basedtasks, such as Visual Question Answering), outperforming existing pre-trainingmethodologies. Our findings highlight the potential of using energy-basedframework in advancing multimodal fusion and suggest significant improvementsfor downstream applications.</description>
      <author>example@mail.com (Khang H. N. Vo, Duc P. T. Nguyen, Thong Nguyen, Tho T. Quan)</author>
      <guid isPermaLink="false">2503.06380v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>HIF: Height Interval Filtering for Efficient Dynamic Points Removal</title>
      <link>http://arxiv.org/abs/2503.06863v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;3D点云地图构建在定位和自主导航中起着重要作用，但动态物体残留的痕迹会损害后续任务的表现。为了去除这些动态物体，本文提出了一种高效的方法——高度区间过滤（HIF），该方法利用贝叶斯推理更新概率，保证实时性能的同时提高了准确性。&lt;h4&gt;背景&lt;/h4&gt;3D点云映射对于定位和自主导航至关重要，然而在地图构建过程中动态对象会留下残留痕迹，这影响了后续任务的执行效果。现有的一些解决方案计算复杂度高，难以满足实时处理需求。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来去除动态物体，在提高准确性和鲁棒性的同时减少计算开销，并实现实时性能。&lt;h4&gt;方法&lt;/h4&gt;引入高度区间过滤（HIF）技术，该方法基于柱状结构的高度间隔表示构建垂直维度的模型，通过贝叶斯推断更新概率；此外还提出了一种低高度保留策略以增强未知空间的检测能力，减少被障碍物阻挡区域的误分类。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在公开数据集上HIF方法的时间效率提高了7.7倍，并且与现有最先进（SOTA）的方法相比保持了相当的准确性。&lt;h4&gt;结论&lt;/h4&gt;所提出的技术为动态场景下的3D点云地图构建提供了一种有效的方式，能够在减少计算资源消耗的同时保证实时性和高精度。&lt;h4&gt;翻译&lt;/h4&gt;三维点云映射在定位和自主导航中扮演着关键角色。然而，在构造过程中动态物体往往会留下残留的痕迹，这对后续任务性能产生负面影响。因此，在基于点云的地图构建过程中移除动态对象成为一项重要挑战。尽管如此，现有的方法通常会导致显著的计算负担，使得实时处理要求难以满足。为解决这一问题，我们提出了一种称为高度区间过滤（HIF）的方法。该方法构造了柱状结构的高度间隔表示来概率性地建模垂直维度，并通过贝叶斯推理更新这些区间的概率。这确保了在复杂环境下实现高性能的同时也提高了鲁棒性和准确性。此外，我们还提出了低高度保留策略以增强未知空间的检测能力，在被障碍物阻挡区域（遮挡区域）减少误分类的可能性。实验结果表明，HIF方法比现有的最先进方法时间效率高出7.7倍，并且保持了相当的精度。我们将开源代码提供给大家。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D point cloud mapping plays a essential role in localization and autonomousnavigation. However, dynamic objects often leave residual traces during the mapconstruction process, which undermine the performance of subsequent tasks.Therefore, dynamic object removal has become a critical challenge in pointcloud based map construction within dynamic scenarios. Existing approaches,however, often incur significant computational overhead, making it difficult tomeet the real-time processing requirements. To address this issue, we introducethe Height Interval Filtering (HIF) method. This approach constructspillar-based height interval representations to probabilistically model thevertical dimension, with interval probabilities updated through Bayesianinference. It ensures real-time performance while achieving high accuracy andimproving robustness in complex environments. Additionally, we propose alow-height preservation strategy that enhances the detection of unknown spaces,reducing misclassification in areas blocked by obstacles (occluded regions).Experiments on public datasets demonstrate that HIF delivers a 7.7 timesimprovement in time efficiency with comparable accuracy to existing SOTAmethods. The code will be publicly available.</description>
      <author>example@mail.com (Shufang Zhang, Tao Jiang, Jiazheng Wu, Ziyu Meng, Ziyang Zhang, Shan An)</author>
      <guid isPermaLink="false">2503.06863v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Gradient-Driven Graph Neural Networks for Learning Digital and Hybrid Precoder</title>
      <link>http://arxiv.org/abs/2503.06077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于梯度驱动的图神经网络设计方法，用于学习全数字和混合预编码策略。&lt;h4&gt;背景&lt;/h4&gt;多用户多输入多输出（MU-MIMO）系统的预编码优化是一个公认的难题。现有研究表明图神经网络(GNN)在学习预编码策略方面具有潜力，但现有的GNN通常对于用户的数量或天线的数量表现出较差的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的基于梯度驱动的方法来设计适用于全数字和混合预编码策略学习的GNN，并展示该方法能够灵活地适应不同的优化目标和不同的预编码策略。&lt;h4&gt;方法&lt;/h4&gt;利用信号与干扰加噪声比(SINR)对预编码器的梯度以及预编码策略中的置换等变性知识。先应用于全数字预编码策略的学习，然后用于混合预编码策略的学习，在后者中同时利用了模拟和数字预编码器的梯度。&lt;h4&gt;主要发现&lt;/h4&gt;提出的GNN方法在学习不同预编码策略方面表现出有效性，并且相比于基线GNN对用户数量和天线数量有更好的泛化性能。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了梯度驱动图神经网络设计方法的有效性和灵活性，以及其在MU-MIMO系统中的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The optimization of multi-user multi-input multi-output (MU-MIMO) precodersis a widely recognized challenging problem. Existing work has demonstrated thepotential of graph neural networks (GNNs) in learning precoding policies.However, existing GNNs often exhibit poor generalizability for the numbers ofusers or antennas. In this paper, we develop a gradient-driven GNN designmethod for the learning of fully digital and hybrid precoding policies. Theproposed GNNs leverage two kinds of knowledge, namely the gradient ofsignal-to-interference-plus-noise ratio (SINR) to the precoders and thepermutation equivariant property of the precoding policy. To demonstrate theflexibility of the proposed method for accommodating different optimizationobjectives and different precoding policies, we first apply the proposed methodto learn the fully digital precoding policies. We study two precoderoptimization problems for spectral efficiency (SE) maximization and log-SEmaximization to achieve proportional fairness. We then apply the proposedmethod to learn the hybrid precoding policy, where the gradients to analog anddigital precoders are exploited for the design of the GNN. Simulation resultsshow the effectiveness of the proposed methods for learning different precodingpolicies and better generalization performance to the numbers of both users andantennas compared to baseline GNNs.</description>
      <author>example@mail.com (Lin Zhang, Shengqian Han, Chenyang Yang)</author>
      <guid isPermaLink="false">2503.06077v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2503.06343v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as a conference paper at ICLR 2025. 10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;从高维观测流中提取相关信息是深度强化学习代理的核心挑战。对于基于策略的算法，当演员和评论者使用不同的表示方法时，它们会专注于提取不同类型的环境信息。&lt;h4&gt;背景&lt;/h4&gt;在深度强化学习领域，提取相关的信息是从大量高维度观察数据中提炼出有助于决策的关键因素的一个核心难题，尤其是对于采用演员-评论者架构的代理来说。在这种框架下，通常不清楚同样的信息是否对两者都是有益的。&lt;h4&gt;目的&lt;/h4&gt;研究并探索演员和评论者的有效表示原则，在基于策略的学习算法中的作用，并探究他们是否从独立而非共享的表示中受益。&lt;h4&gt;方法&lt;/h4&gt;通过严格的经验研究来理解不同的表示学习方法如何影响演员和评论者的专业化及其下游性能，包括采样效率和生成能力。重点在于分离状态表示时，分别观察演员与评论者的特性变化。&lt;h4&gt;主要发现&lt;/h4&gt;当状态表示被分割成独立的演员和评论者时，它们会系统地专业化于从环境中提取不同类型的信息：演员的表示倾向于关注行动相关的信息；而评论者的表示则专注于价值函数和动态过程编码。此外，还观察到分离的状态评价器在训练期间对于探索以及数据收集扮演着关键角色。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，在深度强化学习中，为演员和评论者提供独立且不同的状态表示能够显著提升各自的效率与能力，同时也揭示了优化策略时探索机制的重要性。&lt;h4&gt;翻译&lt;/h4&gt;从高维观测流中提取相关信息是深度强化学习代理的核心挑战。对于基于策略的算法，当演员和评论者使用不同的表示方法时，它们会专注于提取不同类型的环境信息——演员关注于行动相关的数据，而评论者则倾向于价值与动态过程的信息编码。通过一系列严格的实验研究，该论文探讨了这种分离对下游性能的影响，并发现独立的状态评估器在探索和数据收集过程中发挥了关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extracting relevant information from a stream of high-dimensionalobservations is a central challenge for deep reinforcement learning agents.Actor-critic algorithms add further complexity to this challenge, as it isoften unclear whether the same information will be relevant to both the actorand the critic. To this end, we here explore the principles that underlieeffective representations for the actor and for the critic in on-policyalgorithms. We focus our study on understanding whether the actor and criticwill benefit from separate, rather than shared, representations. Our primaryfinding is that when separated, the representations for the actor and criticsystematically specialise in extracting different types of information from theenvironment -- the actor's representation tends to focus on action-relevantinformation, while the critic's representation specialises in encoding valueand dynamics information. We conduct a rigourous empirical study to understandhow different representation learning approaches affect the actor and critic'sspecialisations and their downstream performance, in terms of sample efficiencyand generation capabilities. Finally, we discover that a separated critic playsan important role in exploration and data collection during training. Our code,trained models and data are accessible athttps://github.com/francelico/deac-rep.</description>
      <author>example@mail.com (Samuel Garcin, Trevor McInroe, Pablo Samuel Castro, Prakash Panangaden, Christopher G. Lucas, David Abel, Stefano V. Albrecht)</author>
      <guid isPermaLink="false">2503.06343v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Handle Object Navigation as Weighted Traveling Repairman Problem</title>
      <link>http://arxiv.org/abs/2503.06937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WTRP-Searcher是一种新颖的框架，用于解决零样本物体导航问题（ZSON），通过将其转化为加权旅行维修员问题（WTRP）来最小化视点等待时间。&lt;h4&gt;背景&lt;/h4&gt;当前方法依赖于基础模型或多模态地图，但往往使用2D表示和贪婪策略或需要额外训练模块以高计算负载为代价。这些限制了它们在复杂环境和实际应用中的性能。&lt;h4&gt;目的&lt;/h4&gt;提出WTRP-Searcher框架来提高零样本物体导航的效率和性能。&lt;h4&gt;方法&lt;/h4&gt;利用视觉语言模型（VLM）根据目标描述相似度评估视点，并将深度信息投影到2D地图上；采用开放词汇检测器动态更新目标，使用3D嵌入特征图增强空间意识和环境记忆。&lt;h4&gt;主要发现&lt;/h4&gt;WTRP-Searcher框架在复杂任务中表现出色，优于现有方法，在零样本物体导航方面提供更高效的全局规划。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了通过将ZSON问题转化为WTRP并结合视觉语言模型和支持模块可以显著提高性能，并且代码和更多演示将在GitHub上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-Shot Object Navigation (ZSON) requires agents to navigate to objectsspecified via open-ended natural language without predefined categories orprior environmental knowledge. While recent methods leverage foundation modelsor multi-modal maps, they often rely on 2D representations and greedystrategies or require additional training or modules with high computationload, limiting performance in complex environments and real applications. Wepropose WTRP-Searcher, a novel framework that formulates ZSON as a WeightedTraveling Repairman Problem (WTRP), minimizing the weighted waiting time ofviewpoints. Using a Vision-Language Model (VLM), we score viewpoints based onobject-description similarity, projected onto a 2D map with depth information.An open-vocabulary detector identifies targets, dynamically updating goals,while a 3D embedding feature map enhances spatial awareness and environmentalrecall. WTRP-Searcher outperforms existing methods, offering efficient globalplanning and improved performance in complex ZSON tasks. Code and more demoswill be avaliable on https://github.com/lrm20011/WTRP_Searcher.</description>
      <author>example@mail.com (Ruimeng Liu, Xinhang Xu, Shenghai Yuan, Lihua Xie)</author>
      <guid isPermaLink="false">2503.06937v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Vector Quantized Feature Fields for Fast 3D Semantic Lifting</title>
      <link>http://arxiv.org/abs/2503.06469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;将提升技术推广为语义提升，通过引入视图级掩码来指示哪些像素对于提升任务相关。&lt;h4&gt;背景&lt;/h4&gt;传统的提升方法在处理复杂室内和室外场景时需要大量计算资源。&lt;h4&gt;目的&lt;/h4&gt;提出一种轻量级的按需检索像素对齐相关掩码的方法，并展示其在语义提升中的应用效果。&lt;h4&gt;方法&lt;/h4&gt;引入向量化特征场来实现轻量级存储与查询，同时保持了高效的性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用向量化特征场，能够解锁一系列场景表示和具身智能的应用。具体展示了如何使文本驱动的局部场景编辑成为可能，并提高了具身问答任务中的效率。&lt;h4&gt;结论&lt;/h4&gt;语义提升技术结合向量化特征场，在复杂环境下的表现得到显著提高，为未来相关研究提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;我们通过引入多尺度像素对齐特征图来推广提升到语义提升。这些掩码由从场景表示（如蒸馏特征域和特征点云）中提取的特征图确定。然而，存储渲染自蒸馏特征领域的视图级特征图是不实际的，并且存储和查询特征点云的成本很高。为了实现轻量级按需检索像素对齐相关掩码，我们引入了向量化特征场（Vector-Quantized Feature Field）。我们在复杂的室内和室外场景中演示了该技术的有效性。结合语义提升与向量化特征场可以解锁一系列应用场景，包括文本驱动的局部场景编辑以及大幅提升具身问答任务效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We generalize lifting to semantic lifting by incorporating per-view masksthat indicate relevant pixels for lifting tasks. These masks are determined byquerying corresponding multiscale pixel-aligned feature maps, which are derivedfrom scene representations such as distilled feature fields and feature pointclouds. However, storing per-view feature maps rendered from distilled featurefields is impractical, and feature point clouds are expensive to store andquery. To enable lightweight on-demand retrieval of pixel-aligned relevancemasks, we introduce the Vector-Quantized Feature Field. We demonstrate theeffectiveness of the Vector-Quantized Feature Field on complex indoor andoutdoor scenes. Semantic lifting, when paired with a Vector-Quantized FeatureField, can unlock a myriad of applications in scene representation and embodiedintelligence. Specifically, we showcase how our method enables text-drivenlocalized scene editing and significantly improves the efficiency of embodiedquestion answering.</description>
      <author>example@mail.com (George Tang, Aditya Agarwal, Weiqiao Han, Trevor Darrell, Yutong Bai)</author>
      <guid isPermaLink="false">2503.06469v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>USP: Unified Self-Supervised Pretraining for Image Generation and Understanding</title>
      <link>http://arxiv.org/abs/2503.06132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;最近的研究强调了扩散模型和表示学习之间的相互作用。来自扩散模型的中间表示可以用于下游视觉任务，而自我监督的视觉模型可以增强扩散模型的收敛速度和生成质量。&lt;h4&gt;目的&lt;/h4&gt;为了克服将预训练权重从视觉模型转移到扩散模型时遇到的输入不匹配以及使用潜在空间的问题，我们提出了一个统一的自监督预训练框架（USP）。&lt;h4&gt;方法&lt;/h4&gt;USP通过在变分自动编码器（VAE）的潜在空间中进行掩码潜在建模来初始化扩散模型。&lt;h4&gt;主要发现&lt;/h4&gt;USP在理解任务上的性能与现有方法相当，但在提高扩散模型的收敛速度和生成质量方面有了显著提升。&lt;h4&gt;结论&lt;/h4&gt;我们的代码将在https://github.com/cxxgtxy/USP公开发布。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究强调了扩散模型和表示学习之间的相互作用。来自扩散模型的中间表示可以用于下游视觉任务，而自我监督的视觉模型可以增强扩散模型的收敛速度和生成质量。然而，将预训练权重从视觉模型转移到扩散模型是具有挑战性的，因为存在输入不匹配以及使用潜在空间的问题。为了应对这些问题，我们提出了统一自监督预训练（USP）框架，该框架通过在变分自动编码器（VAE）的潜在空间中进行掩码潜在建模来初始化扩散模型。USP在理解任务上的性能与现有方法相当，但在提高扩散模型的收敛速度和生成质量方面有了显著提升。我们的代码将在https://github.com/cxxgtxy/USP公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have highlighted the interplay between diffusion models andrepresentation learning. Intermediate representations from diffusion models canbe leveraged for downstream visual tasks, while self-supervised vision modelscan enhance the convergence and generation quality of diffusion models.However, transferring pretrained weights from vision models to diffusion modelsis challenging due to input mismatches and the use of latent spaces. To addressthese challenges, we propose Unified Self-supervised Pretraining (USP), aframework that initializes diffusion models via masked latent modeling in aVariational Autoencoder (VAE) latent space. USP achieves comparable performancein understanding tasks while significantly improving the convergence speed andgeneration quality of diffusion models. Our code will be publicly available athttps://github.com/cxxgtxy/USP.</description>
      <author>example@mail.com (Xiangxiang Chu, Renda Li, Yong Wang)</author>
      <guid isPermaLink="false">2503.06132v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>ProBench: Judging Multimodal Foundation Models on Open-ended Multi-domain Expert Tasks</title>
      <link>http://arxiv.org/abs/2503.06885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一个新的评估基准ProBench，用于评价多模态大型语言模型在需要专业知识和高级推理的开放性用户查询任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;解决专家级多模态任务是迈向通用智能的关键里程碑。随着多模态大语言模型能力的不断提升，对其高级多模态智能进行评估变得必要且具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提供一个包含高质量样本的数据集用于测试多模态大型语言模型的能力，特别是它们在视觉感知、文本理解、领域知识和高级推理方面的表现。&lt;h4&gt;方法&lt;/h4&gt;ProBench由来自10个主要领域及其56个子领域的4000个独立提交的专业样本组成。实验中使用MLLM-as-a-Judge评估并比较了24种最新模型的性能。&lt;h4&gt;主要发现&lt;/h4&gt;尽管最佳开源模型可以与专有模型竞争，但ProBench在视觉感知、文本理解、领域知识和高级推理方面对多模态AI提出了重大挑战。&lt;h4&gt;结论&lt;/h4&gt;ProBench提供了未来多模态AI研究的重要方向，并为评估多模态大型语言模型的能力提供了一个强有力的基准。&lt;h4&gt;翻译&lt;/h4&gt;解决专家级的多模态任务是迈向通用人工智能的关键里程碑。随着多模态大语言模型能力的不断提升，对其高级多模态智能进行评价变得必要且具有挑战性。在该研究中，我们引入了ProBench，这是一个由开放性用户查询组成的评估基准，这些查询需要专业的知识和先进的推理能力。ProBench包含4000个高质量样本，它们是由专业人士根据他们的日常生产力需求独立提交的。它涵盖了包括科学、艺术、人文、编程、数学以及创意写作在内的10个主要领域及其56个子领域。实验中，我们使用MLLM-as-a-Judge评估并比较了24种最新模型的表现。结果表明，尽管最佳开源模型可以与专有模型竞争，但ProBench在视觉感知、文本理解、领域知识和高级推理方面对多模态AI提出了重大挑战，并为未来的研究方向提供了有价值的指引。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solving expert-level multimodal tasks is a key milestone towards generalintelligence. As the capabilities of multimodal large language models (MLLMs)continue to improve, evaluation of such advanced multimodal intelligencebecomes necessary yet challenging. In this work, we introduce ProBench, abenchmark of open-ended user queries that require professional expertise andadvanced reasoning. ProBench consists of 4,000 high-quality samplesindependently submitted by professionals based on their daily productivitydemands. It spans across 10 fields and 56 sub-fields, including science, arts,humanities, coding, mathematics, and creative writing. Experimentally, weevaluate and compare 24 latest models using MLLM-as-a-Judge. Our results revealthat although the best open-source models rival the proprietary ones, ProBenchpresents significant challenges in visual perception, textual understanding,domain knowledge and advanced reasoning, thus providing valuable directions forfuture multimodal AI research efforts.</description>
      <author>example@mail.com (Yan Yang, Dongxu Li, Haoning Wu, Bei Chen, Liu Liu, Liyuan Pan, Junnan Li)</author>
      <guid isPermaLink="false">2503.06885v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards Universal Text-driven CT Image Segmentation</title>
      <link>http://arxiv.org/abs/2503.06030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为OpenVocabCT的新型视觉-语言模型，该模型通过使用大规模3D CT图像进行预训练，能够实现基于文本驱动的分割任务。研究利用大型语言模型对诊断报告中的器官级描述进行细粒度分解，并在九个公共数据集上进行了下游分割任务评估，表明了OpenVocabCT相较于现有方法具有更优性能。&lt;h4&gt;背景&lt;/h4&gt;CT扫描被广泛用于精确可视化和分割器官及病变区域，深度学习模型如CNNs和ViTs提高了CT图像分析的准确性。但是，这些模型在面对多样化的真实临床数据时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出了一种使用文本提示的基础模型OpenVocabCT，旨在解决现有模型在处理真实世界复杂场景中的局限性问题，并提高其适应性和临床相关性。&lt;h4&gt;方法&lt;/h4&gt;通过大规模的3D CT图像进行预训练，开发了基于视觉-语言模型的OpenVocabCT。利用大型语言模型对诊断报告中的器官级描述进行了细粒度分解，并应用多颗粒对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;提出的OpenVocabCT在九个公共数据集上的分割任务中表现出优于现有方法的性能，特别是在处理复杂和多样化的临床应用场景时。&lt;h4&gt;结论&lt;/h4&gt;通过引入大规模3D CT图像预训练的视觉-语言模型（即OpenVocabCT），有效地提高了基于文本驱动的器官及肿瘤分割任务中的准确性。所有相关代码、数据集与模型将在GitHub上公开发布。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了计算机断层扫描(CT)在可视化和分割器官以及病变方面的广泛应用，介绍了深度学习模型如卷积神经网络(CNNs)和视觉变换器(ViTs)对CT图像分析的改进。然而这些模型在面对多样化的真实世界临床数据时性能下降。为解决大规模、体素级别标注获取困难的问题，提出了基于提示的基础模型，并特别强调了使用文本提示方法的优势。尽管现有的文本提示模型如CLIP驱动通用模型具有局限性，我们提出了一种名为OpenVocabCT的新框架，它预训练于大规模3D CT图像上以实现普遍的文本驱动分割任务，在多个公共数据集上的下游分割任务评估中显示出优异性能，并计划将所有代码、数据集和模型公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computed tomography (CT) is extensively used for accurate visualization andsegmentation of organs and lesions. While deep learning models such asconvolutional neural networks (CNNs) and vision transformers (ViTs) havesignificantly improved CT image analysis, their performance often declines whenapplied to diverse, real-world clinical data. Although foundation models offera broader and more adaptable solution, their potential is limited due to thechallenge of obtaining large-scale, voxel-level annotations for medical images.In response to these challenges, prompting-based models using visual or textprompts have emerged. Visual-prompting methods, such as the Segment AnythingModel (SAM), still require significant manual input and can introduce ambiguitywhen applied to clinical scenarios. Instead, foundation models that use textprompts offer a more versatile and clinically relevant approach. Notably,current text-prompt models, such as the CLIP-Driven Universal Model, arelimited to text prompts already encountered during training and struggle toprocess the complex and diverse scenarios of real-world clinical applications.Instead of fine-tuning models trained from natural imaging, we proposeOpenVocabCT, a vision-language model pretrained on large-scale 3D CT images foruniversal text-driven segmentation. Using the large-scale CT-RATE dataset, wedecompose the diagnostic reports into fine-grained, organ-level descriptionsusing large language models for multi-granular contrastive learning. Weevaluate our OpenVocabCT on downstream segmentation tasks across nine publicdatasets for organ and tumor segmentation, demonstrating the superiorperformance of our model compared to existing methods. All code, datasets, andmodels will be publicly released at https://github.com/ricklisz/OpenVocabCT.</description>
      <author>example@mail.com (Yuheng Li, Yuxiang Lai, Maria Thor, Deborah Marshall, Zachary Buchwald, David S. Yu, Xiaofeng Yang)</author>
      <guid isPermaLink="false">2503.06030v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>On Statistical Estimation of Edge-Reinforced Random Walks</title>
      <link>http://arxiv.org/abs/2503.06115v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the full version of the conference paper in submission to  ISIT 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了增强型随机游走（RRWs），包括顶点增强型随机游走（VRRWs）和边增强型随机游走（ERRWs）。这些模型在多个领域如网络表示学习、强化PageRank以及动物行为建模中都有应用。&lt;h4&gt;背景&lt;/h4&gt;尽管这类模型已经在各个领域展示了广泛的应用，但是关于如何统计估计控制RRWs的参数仍然研究不足。特别地，在已有的轨迹数据基础上来估计初始边权重的问题尚未被充分探讨。&lt;h4&gt;目的&lt;/h4&gt;该工作集中于利用观测到的路径数据来估算ERRW的初始边权值。&lt;h4&gt;方法&lt;/h4&gt;借助于ERRW与随机环境中的随机游走（RWRE）之间的关系，以及所谓的“魔法公式”，提出了基于广义矩法的估计器。通过探索嵌入在随机环境中的双曲高斯结构，分析了该估计器的样本复杂度，并对底层随机边导电性的波动进行了界限。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种用于估算ERRW初始边权值的新方法，这种方法利用了RWRE与ERRW之间的内在联系，并且通过考虑嵌入在随机环境中的双曲高斯结构来分析估计器的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究为解决RRWs参数估计问题提供了一个新视角和工具。通过进一步分析并验证所提出的估计器的有效性和效率，可以在未来的应用中更广泛地利用这类模型。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了增强型随机游走（RRWs）的应用背景、未被充分探讨的研究领域以及该工作针对边增强型随机游走的参数估计提出了新的方法和分析途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforced random walks (RRWs), including vertex-reinforced random walks(VRRWs) and edge-reinforced random walks (ERRWs), model random walks where thetransition probabilities evolve based on prior visitation history~\cite{mgr,fmk, tarres, volkov}. These models have found applications in various areas,such as network representation learning~\cite{xzzs}, reinforcedPageRank~\cite{gly}, and modeling animal behaviors~\cite{smouse}, among others.However, statistical estimation of the parameters governing RRWs remainsunderexplored. This work focuses on estimating the initial edge weights ofERRWs using observed trajectory data. Leveraging the connections between anERRW and a random walk in a random environment (RWRE)~\cite{mr, mr2}, as givenby the so-called "magic formula", we propose an estimator based on thegeneralized method of moments. To analyze the sample complexity of ourestimator, we exploit the hyperbolic Gaussian structure embedded in the randomenvironment to bound the fluctuations of the underlying random edgeconductances.</description>
      <author>example@mail.com (Qinghua, Ding, Venkat Anantharam)</author>
      <guid isPermaLink="false">2503.06115v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards a Multimodal MRI-Based Foundation Model for Multi-Level Feature Exploration in Segmentation, Molecular Subtyping, and Grading of Glioma</title>
      <link>http://arxiv.org/abs/2503.06828v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于深度学习的多任务模型MTS-UNET，用于胶质瘤分割、分级和分子分类。该模型通过结合预训练的基础模型与定制模块（TAFE和CMD）实现了对肿瘤空间异质性的有效捕捉。&lt;h4&gt;背景&lt;/h4&gt;传统的依赖于组织取样的方法难以准确描述肿瘤的空间异质性；深度学习在提高图像分割和分子特征分析方面表现出色，但少有研究同时整合了形态学和分子特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能同时进行胶质瘤的分割、分级及分子亚型分类的模型。&lt;h4&gt;方法&lt;/h4&gt;基于BrainSegFounder预训练模型建立MTS-UNET框架。引入TAFE模块实现多尺度肿瘤特征提取，CMD模块用于强调与IDH突变相关的细微T2-FLAIR信号差异。在七个公共数据集中的2,249例胶质瘤患者中进行训练和验证。&lt;h4&gt;主要发现&lt;/h4&gt;MTS-UNET模型在所有任务上均显著优于基准模型：分割的平均Dice系数为84%，IDH突变预测AUC值达90.58%，1p/19q共缺失预测AUC值为69.22%，分级AUC值达到87.54%。&lt;h4&gt;结论&lt;/h4&gt;MTS-UNET模型有效整合了肿瘤分割和多级分类，具有跨不同MRI数据集的强大泛化能力，并有望通过提高非侵入性和个性化治疗的准确性及可解释性来推进胶质瘤管理。&lt;h4&gt;翻译&lt;/h4&gt;准确、无创地对胶质瘤进行表征对于有效的临床管理至关重要。传统的方法依赖于侵入性的组织取样，通常无法捕捉到肿瘤的空间异质性。虽然深度学习技术已经改善了图像分割和分子特征分析的性能，但很少有方法能够同时整合形态学和分子特性。基础模型通过从大规模数据集中学习出任务无关的稳健表示具有巨大潜力，但在胶质瘤影像生物标志物领域中尚未得到充分利用。我们提出了一个基于BrainSegFounder预训练模型构建的新框架MTS-UNET，该模型可以同时进行胶质瘤分割、组织病理分级和分子分型（IDH突变和1p/19q共缺失）。它包含了两个关键模块：Tumor-Aware Feature Encoding (TAFE)，用于多尺度肿瘤特征提取；Cross-Modality Differential (CMD)用于突出与IDH突变相关的细微T2-FLAIR信号差异。该模型在七个公开数据集中的2,249例胶质瘤患者中进行了训练和验证，结果表明MTS-UNET在分割任务上的平均Dice系数达到84%，IDH突变预测的AUC值为90.58%，1p/19q共缺失预测的AUC值为69.22%，组织病理分级的AUC值为87.54%。与基准模型相比，所有任务上的性能均有显著提高（p≤0.05）。去除TAFE和CMD模块进行消融实验验证了它们对框架的重要性以及其稳健性。基于基础学习方法的MTS-UNET模型有效地将肿瘤分割与多级分类结合在一起，在不同MRI数据集中展现出了强大的泛化能力，该框架为推进非侵入性和个性化的胶质瘤管理提供了新的视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate, noninvasive glioma characterization is crucial for effectiveclinical management. Traditional methods, dependent on invasive tissuesampling, often fail to capture the spatial heterogeneity of the tumor. Whiledeep learning has improved segmentation and molecular profiling, few approachessimultaneously integrate tumor morphology and molecular features. Foundationdeep learning models, which learn robust, task-agnostic representations fromlarge-scale datasets, hold great promise but remain underutilized in gliomaimaging biomarkers. We propose the Multi-Task SWIN-UNETR (MTS-UNET) model, anovel foundation-based framework built on the BrainSegFounder model, pretrainedon large-scale neuroimaging data. MTS-UNET simultaneously performs gliomasegmentation, histological grading, and molecular subtyping (IDH mutation and1p/19q co-deletion). It incorporates two key modules: Tumor-Aware FeatureEncoding (TAFE) for multi-scale, tumor-focused feature extraction andCross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatchsignals associated with IDH mutation. The model was trained and validated on adiverse, multi-center cohort of 2,249 glioma patients from seven publicdatasets. MTS-UNET achieved a mean Dice score of 84% for segmentation, alongwith AUCs of 90.58% for IDH mutation, 69.22% for 1p/19q co-deletion prediction,and 87.54% for grading, significantly outperforming baseline models (p&lt;=0.05).Ablation studies validated the essential contributions of the TAFE and CMDmodules and demonstrated the robustness of the framework. The foundation-basedMTS-UNET model effectively integrates tumor segmentation with multi-levelclassification, exhibiting strong generalizability across diverse MRI datasets.This framework shows significant potential for advancing noninvasive,personalized glioma management by improving predictive accuracy andinterpretability.</description>
      <author>example@mail.com (Somayeh Farahani, Marjaneh Hejazi, Antonio Di Ieva, Emad Fatemizadeh, Sidong Liu)</author>
      <guid isPermaLink="false">2503.06828v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>SecureGS: Boosting the Security and Fidelity of 3D Gaussian Splatting Steganography</title>
      <link>http://arxiv.org/abs/2503.06118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种名为SecureGS的框架，该框架旨在通过提供一种安全且高效的3D Gaussian Splatting (3DGS)隐写术解决方案来保护3D资产隐私。&lt;h4&gt;背景&lt;/h4&gt;3D Gaussian Splatting (3DGS)作为一种实时渲染和高质量输出的方法在三维表示领域中脱颖而出。然而，传统的NeRF隐写术方法无法解决3DGS点云文件公开访问的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为SecureGS的安全框架，以应对现有的3DGS隐写技术存在的问题，如降低的渲染保真度、增加的计算需求和安全漏洞等。&lt;h4&gt;方法&lt;/h4&gt;SecureGS采用了一种混合解耦的高斯加密机制来嵌入隐藏的3DGaussian点的偏移量、缩放比例、旋转角度以及RGB属性到锚定点特征中。同时，它提出了一种密度区域感知的增长和修剪策略，以自适应地定位最佳隐藏区域。&lt;h4&gt;主要发现&lt;/h4&gt;SecureGS在渲染保真度、速度和安全性方面都显著优于现有的3DGS隐写方法。&lt;h4&gt;结论&lt;/h4&gt;通过采用隐私保护神经网络以及创新的嵌入机制，SecureGS能够有效地增强3DGaussian Splatting数据的安全性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已经直接作为中文进行了翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has emerged as a premier method for 3Drepresentation due to its real-time rendering and high-quality outputs,underscoring the critical need to protect the privacy of 3D assets. TraditionalNeRF steganography methods fail to address the explicit nature of 3DGS sinceits point cloud files are publicly accessible. Existing GS steganographysolutions mitigate some issues but still struggle with reduced renderingfidelity, increased computational demands, and security flaws, especially inthe security of the geometric structure of the visualized point cloud. Toaddress these demands, we propose a SecureGS, a secure and efficient 3DGSsteganography framework inspired by Scaffold-GS's anchor point design andneural decoding. SecureGS uses a hybrid decoupled Gaussian encryption mechanismto embed offsets, scales, rotations, and RGB attributes of the hidden 3DGaussian points in anchor point features, retrievable only by authorized usersthrough privacy-preserving neural networks. To further enhance security, wepropose a density region-aware anchor growing and pruning strategy thatadaptively locates optimal hiding regions without exposing hidden information.Extensive experiments show that SecureGS significantly surpasses existing GSsteganography methods in rendering fidelity, speed, and security.</description>
      <author>example@mail.com (Xuanyu Zhang, Jiarui Meng, Zhipei Xu, Shuzhou Yang, Yanmin Wu, Ronggang Wang, Jian Zhang)</author>
      <guid isPermaLink="false">2503.06118v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>AF-KAN: Activation Function-Based Kolmogorov-Arnold Networks for Efficient Representation Learning</title>
      <link>http://arxiv.org/abs/2503.06112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Kolmogorov-Arnold Networks (KANs) 在多个科学问题中得到广泛应用，其中ReLU-KAN利用ReLU函数模拟B-spline结构。然而，ReLU-KAN不适合处理多输入且受限于对负值的处理方式。&lt;h4&gt;背景&lt;/h4&gt;许多KAN使用基础和多项式函数设计，如B-样条。ReLU-KAN采用ReLU组合模仿B-样条并利用其速度优势。然而，它在处理多个输入时存在局限性，并受到ReLU处理负数的影响。&lt;h4&gt;目的&lt;/h4&gt;提出基于激活函数的Kolmogorov-Arnold网络（AF-KAN），扩展ReLU-KAN功能，引入多种激活函数及组合，并结合参数减少方法以提高图像分类数据集上的性能。&lt;h4&gt;方法&lt;/h4&gt;探索不同激活函数、组合方式、网格大小和样条阶数来验证AF-KAN的有效性和确定最佳配置。同时应用注意机制和数据归一化技术来优化网络结构。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，与MLP、ReLU-KAN及其他参数数量相同的KAN相比，AF-KAN性能更优；即使使用少于6到10倍的参数量也能保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;尽管训练时间较长且消耗更多计算资源（FLOPs），但AF-KAN在图像分类任务中表现突出。代码开源于https://github.com/hoangthangta/All-KAN。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Kolmogorov-Arnold Networks (KANs) have inspired numerous works exploringtheir applications across a wide range of scientific problems, with thepotential to replace Multilayer Perceptrons (MLPs). While many KANs aredesigned using basis and polynomial functions, such as B-splines, ReLU-KANutilizes a combination of ReLU functions to mimic the structure of B-splinesand take advantage of ReLU's speed. However, ReLU-KAN is not built for multipleinputs, and its limitations stem from ReLU's handling of negative values, whichcan restrict feature extraction. To address these issues, we introduceActivation Function-Based Kolmogorov-Arnold Networks (AF-KAN), expandingReLU-KAN with various activations and their function combinations. This novelKAN also incorporates parameter reduction methods, primarily attentionmechanisms and data normalization, to enhance performance on imageclassification datasets. We explore different activation functions, functioncombinations, grid sizes, and spline orders to validate the effectiveness ofAF-KAN and determine its optimal configuration. In the experiments, AF-KANsignificantly outperforms MLP, ReLU-KAN, and other KANs with the same parametercount. It also remains competitive even when using fewer than 6 to 10 times theparameters while maintaining the same network structure. However, AF-KANrequires a longer training time and consumes more FLOPs. The repository forthis work is available at https://github.com/hoangthangta/All-KAN.</description>
      <author>example@mail.com (Hoang-Thang Ta, Anh Tran)</author>
      <guid isPermaLink="false">2503.06112v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable High-order Knowledge Graph Neural Network for Predicting Synthetic Lethality in Human Cancers</title>
      <link>http://arxiv.org/abs/2503.06052v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了Diverse Graph Information Bottleneck for Synthetic Lethality (DGIB4SL) 方法，用于预测癌症治疗中的合成致死基因对。&lt;h4&gt;背景&lt;/h4&gt;合成致死性是一种有前景的癌症治疗方法。现有方法将知识图谱与图形神经网络结合，并使用注意力机制来提取局部子图作为目标基因对的解释。&lt;h4&gt;目的&lt;/h4&gt;克服当前SL预测方法中注意力机制缺乏准确性、单一性和无法确保解释中的高阶结构可信度的问题，提出一种新的KG-GNN模型DGIB4SL。&lt;h4&gt;方法&lt;/h4&gt;提出了一个全新的Diverse Graph Information Bottleneck (DGIB) 目标函数，并结合了Determinant Point Process (DPP) 约束，同时使用13种基于motif的邻接矩阵来捕捉基因表示中的高阶结构。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，DGIB4SL在预测合成致死性方面超越了现有的最佳方法，并能够为同一基因对生成多个解释，揭示了多种潜在的生物机制。&lt;h4&gt;结论&lt;/h4&gt;新提出的模型和方法不仅提高了SL预测的准确性，还提供了生物学上的多元视角来理解合成致死性的机制。&lt;h4&gt;翻译&lt;/h4&gt;合成致死性（SL）是癌症治疗中的一个有希望的基因互作现象。最近的SL预测方法将知识图谱整合到图形神经网络中，并利用注意力机制提取局部子图作为目标基因对的解释，但这些方法在准确性、单一性和高阶结构可信度方面存在不足。为了克服这些问题，我们提出了一种基于知识图谱的GNN模型Diverse Graph Information Bottleneck for Synthetic Lethality (DGIB4SL)，它能为同一基因对生成多个准确的解释，并有效地编码高阶结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic lethality (SL) is a promising gene interaction for cancer therapy.Recent SL prediction methods integrate knowledge graphs (KGs) into graph neuralnetworks (GNNs) and employ attention mechanisms to extract local subgraphs asexplanations for target gene pairs. However, attention mechanisms often lackfidelity, typically generate a single explanation per gene pair, and fail toensure trustworthy high-order structures in their explanations. To overcomethese limitations, we propose Diverse Graph Information Bottleneck forSynthetic Lethality (DGIB4SL), a KG-based GNN that generates multiple faithfulexplanations for the same gene pair and effectively encodes high-orderstructures. Specifically, we introduce a novel DGIB objective, integrating aDeterminant Point Process (DPP) constraint into the standard IB objective, andemploy 13 motif-based adjacency matrices to capture high-order structures ingene representations. Experimental results show that DGIB4SL outperformsstate-of-the-art baselines and provides multiple explanations for SLprediction, revealing diverse biological mechanisms underlying SL inference.</description>
      <author>example@mail.com (Xuexin Chen, Ruichu Cai, Zhengting Huang, Zijian Li, Jie Zheng, Min Wu)</author>
      <guid isPermaLink="false">2503.06052v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>HierDAMap: Towards Universal Domain Adaptive BEV Mapping via Hierarchical Perspective Priors</title>
      <link>http://arxiv.org/abs/2503.06821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The source code will be made publicly available at  https://github.com/lynn-yu/HierDAMap&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种用于自动驾驶中鸟瞰图（BEV）映射任务的通用和整体域适应框架HierDAMap。&lt;h4&gt;背景&lt;/h4&gt;鸟瞰图映射技术的研究推动了视觉感知技术在自动驾驶中的创新。为了应对无标签的真实世界场景，研究者们开始探索无需监督的领域适应模型。然而，针对鸟瞰图映射任务的这种研究仍然非常有限，并且现有的方法无法完美地覆盖所有的BEV映射任务。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有领域适应框架在处理BEV映射任务时存在的局限性，论文提出了一种新的域适应框架HierDAMap，该框架利用了视角先验知识并涵盖了三个层次：全局、稀疏和实例级别。&lt;h4&gt;方法&lt;/h4&gt;HierDA由三个关键组件组成：语义指导伪标签（SGPS）、动态感知一致性学习（DACL）以及跨领域锥体混合（CDFM）。通过这些部分，论文提出了使用视觉基础模型生成的伪标签来引导BEV特征分布的一致性。此外，还提出了一种利用不确定性感知预测深度来缓解由空间变化导致的特征分布差异的方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，HierDAMap框架在多个任务（如鸟瞰图语义分割、高精度语义和矢量化映射）上取得了良好的效果。该方法展示了视角先验知识如何能够促进BEV映射领域的进步，并且通过引入动态的视角标签实现了更加鲁棒的学习。&lt;h4&gt;结论&lt;/h4&gt;HierDAMap为解决无监督域适应在鸟瞰图任务上的局限性提供了一个创新方案，其应用前景广阔。&lt;h4&gt;翻译&lt;/h4&gt;对Bird's-Eye View (BEV) 映射技术的研究推动了自动驾驶中的视觉感知技术创新。BEV映射模型需要应用于未标记的真实世界中，使得无需监督的领域适应模型研究成为必要路径。然而，针对BEV映射任务的这种研究仍然非常有限，并且现有的方法无法完美地覆盖所有的任务需求。为了解决这一缺口，论文提出了HierDAMap，一种具有分层视角先验知识的通用和整体BEV域适应框架。与现有仅关注使用先验知识进行图像级学习的研究不同，本文探索了在全局、稀疏和实例三个层次上视角先验知识的作用。基于这些先验知识，HierDA包含了三个关键组件：语义指导伪标签（SGPS）、动态感知一致性学习（DACL）以及跨领域锥体混合（CDFM）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The exploration of Bird's-Eye View (BEV) mapping technology has drivensignificant innovation in visual perception technology for autonomous driving.BEV mapping models need to be applied to the unlabeled real world, making thestudy of unsupervised domain adaptation models an essential path. However,research on unsupervised domain adaptation for BEV mapping remains limited andcannot perfectly accommodate all BEV mapping tasks. To address this gap, thispaper proposes HierDAMap, a universal and holistic BEV domain adaptationframework with hierarchical perspective priors. Unlike existing research thatsolely focuses on image-level learning using prior knowledge, this paperexplores the guiding role of perspective prior knowledge across three distinctlevels: global, sparse, and instance levels. With these priors, HierDA consistsof three essential components, including Semantic-Guided Pseudo Supervision(SGPS), Dynamic-Aware Coherence Learning (DACL), and Cross-Domain FrustumMixing (CDFM). SGPS constrains the cross-domain consistency of perspectivefeature distribution through pseudo labels generated by vision foundationmodels in 2D space. To mitigate feature distribution discrepancies caused byspatial variations, DACL employs uncertainty-aware predicted depth as anintermediary to derive dynamic BEV labels from perspective pseudo-labels,thereby constraining the coarse BEV features derived from correspondingperspective features. CDFM, on the other hand, leverages perspective masks ofview frustum to mix multi-view perspective images from both domains, whichguides cross-domain view transformation and encoding learning through mixed BEVlabels. The proposed method is verified on multiple BEV mapping tasks, such asBEV semantic segmentation, high-definition semantic, and vectorized mapping.The source code will be made publicly available athttps://github.com/lynn-yu/HierDAMap.</description>
      <author>example@mail.com (Siyu Li, Yihong Cao, Hao Shi, Yongsheng Zang, Xuan He, Kailun Yang, Zhiyong Li)</author>
      <guid isPermaLink="false">2503.06821v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Multi-view Spectral Clustering on the Grassmannian Manifold With Hypergraph Representation</title>
      <link>http://arxiv.org/abs/2503.06066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 6 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;图基多视角谱聚类方法在最近取得了显著进展，但在简化成对关系或处理高维欧几里得空间中谱分解效率低下的问题上存在不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于稀疏表示学习生成超图的方法，并在此基础上提出一个多视图超图谱聚类的优化函数和算法。&lt;h4&gt;方法&lt;/h4&gt;引入了一种生成超图的新方法，利用数据点的稀疏表示学习。提出了一个带有正交约束的多视角超图谱聚类的优化函数，并将问题转化为草曼尼（Grassmannian）流形上的无约束形式。最后，设计了一个交替迭代黎曼优化算法来解决这个问题。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实世界的多视图数据集上进行测试时，该方法优于七种最先进的多视图聚类算法，并且实验结果表明其具有优越的低维和鲁棒特征表示能力。&lt;h4&gt;结论&lt;/h4&gt;提出的方法通过更好的谱聚类性能以及跨不同视角的一致性表现出了其有效性和优势。&lt;h4&gt;翻译&lt;/h4&gt;基于图的多视角谱聚类方法在最近取得了显著进展，但它们经常简化成对关系或在处理高维欧几里得空间中的谱分解效率低下问题上遇到困难。本文介绍了一种新的方法，该方法通过从数据点中利用稀疏表示学习生成超图开始。基于生成的超图，提出了一种带有正交约束的多视角超图谱聚类优化函数，它将每个视图的谱聚类结合起来并确保不同视图之间的一致性。在欧几里得空间内求解带正交性的优化问题时可能产生局部极大值和近似误差。创新地，我们将这个问题转化为草曼尼流形上的无约束形式。最后，我们设计了一个交替迭代黎曼优化算法来解决该问题。为了验证所提出算法的有效性，我们在四个真实世界的多视图数据集上进行了测试，并将其性能与七种最先进的多视图聚类算法进行了比较。实验结果表明，由于其优越的低维和鲁棒特征表示能力，我们的方法在谱聚类性能方面优于基准方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-based multi-view spectral clustering methods have achieved notableprogress recently, yet they often fall short in either oversimplifying pairwiserelationships or struggling with inefficient spectral decompositions inhigh-dimensional Euclidean spaces. In this paper, we introduce a novel approachthat begins to generate hypergraphs by leveraging sparse representationlearning from data points. Based on the generated hypergraph, we propose anoptimization function with orthogonality constraints for multi-view hypergraphspectral clustering, which incorporates spectral clustering for each view andensures consistency across different views. In Euclidean space, solving theorthogonality-constrained optimization problem may yield local maxima andapproximation errors. Innovately, we transform this problem into anunconstrained form on the Grassmannian manifold. Finally, we devise analternating iterative Riemannian optimization algorithm to solve the problem.To validate the effectiveness of the proposed algorithm, we test it on fourreal-world multi-view datasets and compare its performance with sevenstate-of-the-art multi-view clustering algorithms. The experimental resultsdemonstrate that our method outperforms the baselines in terms of clusteringperformance due to its superior low-dimensional and resilient featurerepresentation.</description>
      <author>example@mail.com (Murong Yang, Shihui Ying, Xin-Jian Xu, Yue Gao)</author>
      <guid isPermaLink="false">2503.06066v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Point Cloud Completion: Are We Ready For The Real-World?</title>
      <link>http://arxiv.org/abs/2411.17580v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了在实际环境中获取的点云数据由于噪声、不完整性和非均匀稀疏性，对点云补全任务构成了挑战。使用代数拓扑和持久同调（PH）的方法表明当前基准对象点云缺乏真实环境中捕捉到的丰富的拓扑特征。&lt;h4&gt;背景&lt;/h4&gt;现实世界中的点云数据在获取时面临各种限制条件、挑战性和多传感器设置，导致点云噪声大、不完整且分布非均匀稀疏。这使得点云补全任务变得非常困难。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为RealPC的工业真实环境下的首个点云完成数据集，并探索如何利用持久同调中的拓扑特征改善点云补全方法。&lt;h4&gt;方法&lt;/h4&gt;贡献了一个包含铁路设施中21类工业结构大约40,000对样本的真实世界点云完成数据集。采用持久同调来分析和提取点云的拓扑信息，并提出了一种基于同伦抽样的网络模型BOSHNet，该模型在训练初期就能利用类似0维PH特征提供好处。&lt;h4&gt;主要发现&lt;/h4&gt;与现有基准数据集不同，RealPC包含多个0维和1维持久同调基线。这些拓扑先验的整合能够提高点云补全的质量。&lt;h4&gt;结论&lt;/h4&gt;通过将持久同调中的拓扑信息用于点云补全任务，可以显著改进当前的方法，并展示了基于BOSHNet模型的有效性，该模型在训练过程中利用了持久同调提供的全局拓扑结构信息。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-11-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds acquired in constrained, challenging, uncontrolled, andmulti-sensor real-world settings are noisy, incomplete, and non-uniformlysparse. This presents acute challenges for the vital task of point cloudcompletion. Using tools from Algebraic Topology and Persistent Homology (PH),we demonstrate that current benchmark object point clouds lack rich topologicalfeatures that are integral part of point clouds captured in realisticenvironments. To facilitate research in this direction, we contribute the firstreal-world industrial dataset for point cloud completion, RealPC - a diverse,rich and varied set of point clouds. It consists of ~ 40,000 pairs across 21categories of industrial structures in railway establishments. Benchmarkresults on several strong baselines reveal that existing methods fail inreal-world scenarios. We discover a striking observation - unlike currentdatasets, RealPC consists of multiple 0- and 1-dimensional PH-based topologicalfeatures. We prove that integrating these topological priors into existingworks helps improve completion. We present how 0-dimensional PH priors extractthe global topology of a complete shape in the form of a 3D skeleton and assista model in generating topologically consistent complete shapes. Since computingHomology is expensive, we present a simple, yet effective Homology Samplerguided network, BOSHNet that bypasses the Homology computation by samplingproxy backbones akin to 0-dim PH. These backbones provide similar benefits of0-dim PH right from the start of the training, unlike similar methods whereaccurate backbones are obtained only during later phases of the training.</description>
      <author>example@mail.com (Stuti Pathak, Prashant Kumar, Dheeraj Baiju, Nicholus Mboga, Gunther Steenackers, Rudi Penne)</author>
      <guid isPermaLink="false">2411.17580v3</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal AI-driven Biomarker for Early Detection of Cancer Cachexia</title>
      <link>http://arxiv.org/abs/2503.06797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 6 figures, 3 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于多模态人工智能的生物标志物，用于早期癌症恶病质检测。&lt;h4&gt;背景&lt;/h4&gt;癌症恶病质是一种复杂的综合征，包括进行性肌肉萎缩、代谢功能障碍和全身炎症反应。现有的复合指数虽然整合了多种生物标志物，但缺乏标准化阈值限制了其临床实用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于人工智能的多模态生物标志物系统，用于早期癌症恶病质检测，并提高个性化治疗效果及患者生存率。&lt;h4&gt;方法&lt;/h4&gt;利用开源大型语言模型和在医疗数据上训练的基础模型，结合患者的异构信息（包括人口统计学、疾病状态、实验室报告、放射影像以及临床记录），形成一个机器学习框架。该框架能够处理缺失的数据，并使用常规收集的临床数据增强实际应用。&lt;h4&gt;主要发现&lt;/h4&gt;初步结果显示，整合多种数据模式可以提高癌症诊断时刻恶病质预测的准确性；同时人工智能生物标志物能根据个体特定因素（如年龄、种族、体重、癌种和分期）动态调整。&lt;h4&gt;结论&lt;/h4&gt;多模态AI生物标志物提供了一种可扩展且临床可行的方法来早期检测癌症恶病质，促进了个性化干预措施，并有可能改善治疗结果及患者生存率。&lt;h4&gt;翻译&lt;/h4&gt;摘要：癌症恶病质是一种由肌肉逐渐萎缩、代谢功能障碍和全身炎症等多种因素引起的综合征，导致生活质量降低和死亡风险增加。尽管经过大量研究，仍没有单一的确定性生物标志物存在，因为与恶病质相关的指标（如血液生物标志物、骨骼肌测量值以及代谢异常）往往与其他条件重叠。现有的复合指数包括癌症恶病质指数(CXI)、改良CXI(mCXI)和恶病质评分(CASCO)，它们结合了多种生物标志物，但由于缺乏标准化阈值而限制了其临床实用性。本研究提出了一个基于多模态AI的早期癌症恶病质检测生物标志物方案，利用开源大型语言模型及在医疗数据上训练的基础模型，整合了包括人口统计学、疾病状态、实验室报告、放射影像（CT扫描）和临床记录在内的异构患者数据，采用可以处理缺失数据的机器学习框架。不同于以往基于AI的方法使用整理过的数据集，本方法利用常规收集的临床数据提高实际应用性。此外，该模型还包含了置信度估计功能，能够识别出需要专家审阅的情况以实现精确临床解读。初步发现表明，在癌症诊断时刻整合多种数据模式能提升恶病质预测准确性；AI生物标志物根据年龄、种族、体重等患者特定因素动态适应，避免了固定阈值生物标志物的局限性。该多模态AI生物标志物为早期癌症恶病质检测提供了可扩展和临床可行的方法，促进了个性化干预，并有可能改善治疗结果及患者的生存率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cancer cachexia is a multifactorial syndrome characterized by progressivemuscle wasting, metabolic dysfunction, and systemic inflammation, leading toreduced quality of life and increased mortality. Despite extensive research, nosingle definitive biomarker exists, as cachexia-related indicators such asserum biomarkers, skeletal muscle measurements, and metabolic abnormalitiesoften overlap with other conditions. Existing composite indices, including theCancer Cachexia Index (CXI), Modified CXI (mCXI), and Cachexia Score (CASCO),integrate multiple biomarkers but lack standardized thresholds, limiting theirclinical utility. This study proposes a multimodal AI-based biomarker for earlycancer cachexia detection, leveraging open-source large language models (LLMs)and foundation models trained on medical data. The approach integratesheterogeneous patient data, including demographics, disease status, labreports, radiological imaging (CT scans), and clinical notes, using a machinelearning framework that can handle missing data. Unlike previous AI-basedmodels trained on curated datasets, this method utilizes routinely collectedclinical data, enhancing real-world applicability. Additionally, the modelincorporates confidence estimation, allowing the identification of casesrequiring expert review for precise clinical interpretation. Preliminaryfindings demonstrate that integrating multiple data modalities improvescachexia prediction accuracy at the time of cancer diagnosis. The AI-basedbiomarker dynamically adapts to patient-specific factors such as age, race,ethnicity, weight, cancer type, and stage, avoiding the limitations offixed-threshold biomarkers. This multimodal AI biomarker provides a scalableand clinically viable solution for early cancer cachexia detection,facilitating personalized interventions and potentially improving treatmentoutcomes and patient survival.</description>
      <author>example@mail.com (Sabeen Ahmed, Nathan Parker, Margaret Park, Evan W. Davis, Jennifer B. Permuth, Matthew B. Schabath, Yasin Yilmaz, Ghulam Rasool)</author>
      <guid isPermaLink="false">2503.06797v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Clustering Approaches for Autism Screening: Achieving 95.31% Accuracy with a Gaussian Mixture Model</title>
      <link>http://arxiv.org/abs/2503.05746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文探讨了使用四种不同的无监督聚类算法（K-Means、高斯混合模型（GMM）、凝聚层次聚类和DBSCAN）来分析一个公开的自闭症谱系障碍（ASD）筛查数据集，并通过交叉验证进行广泛的超参数调整。&lt;h4&gt;背景&lt;/h4&gt;自闭症谱系障碍（ASD）的有效诊断仍然是一个挑战，传统方法依赖于监督学习算法，需要大量的标记数据。无监督学习可以通过处理未标记的数据来加速或支持这一过程。&lt;h4&gt;目的&lt;/h4&gt;探索使用四种不同的无监督聚类算法对公开的704名成人个体自闭症谱系障碍筛查数据集进行分析，以评估这些方法在诊断中的潜力。&lt;h4&gt;方法&lt;/h4&gt;该研究包括数据预处理（清理、标签编码和标准缩放）以及采用交叉验证来评估和比较四种不同的无监督聚类算法。&lt;h4&gt;主要发现&lt;/h4&gt;高斯混合模型（GMM）实现了最高的聚类到标记准确率（95.31%），同时调整后的兰德指数(ARI) 和轮廓分数进一步说明了每个集群的内部一致性。这表明在标记数据稀缺、不确定或成本过高的情况下，无监督方法具有重要潜力。&lt;h4&gt;结论&lt;/h4&gt;继续进行方法上的改进，无监督的方法有潜力增加早期检测倡议，并指导对高风险个体资源分配。&lt;h4&gt;翻译&lt;/h4&gt;自闭症谱系障碍（ASD）的有效诊断仍然是一个挑战。虽然全球在公共卫生、临床筛查和科学研究方面做出了努力，但有效且及时的诊断仍然难以实现。传统的诊断方法主要依赖于监督学习算法，需要标记数据，这可能会耗费大量时间和资源。相反，无监督学习可以通过处理未标记的数据来加速或支持这一过程。这项研究使用了四种不同的无监督聚类算法（K-Means、高斯混合模型（GMM）、凝聚层次聚类和DBSCAN）来分析一个公开的704名成人个体自闭症谱系障碍筛查数据集，并通过交叉验证进行广泛的超参数调整。结果显示，高斯混合模型实现了最高的聚类到标记准确率（95.31%），同时调整后的兰德指数(ARI) 和轮廓分数进一步说明了每个集群的内部一致性。这些结果表明，在标记数据稀缺、不确定或成本过高的情况下，无监督方法具有重要潜力。随着方法上的改进，无监督的方法有潜力增加早期检测倡议，并指导对高风险个体资源分配。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autism spectrum disorder (ASD) remains a challenging condition to diagnoseeffectively and promptly, despite global efforts in public health, clinicalscreening, and scientific research. Traditional diagnostic methods, primarilyreliant on supervised learning approaches, presuppose the availability oflabeled data, which can be both time-consuming and resource-intensive toobtain. Unsupervised learning, in contrast, offers a means of gaining insightsfrom unlabeled datasets in a manner that can expedite or support the diagnosticprocess. This paper explores the use of four distinct unsupervised clusteringalgorithms K-Means, Gaussian Mixture Model (GMM), Agglomerative Clustering, andDBSCAN to analyze a publicly available dataset of 704 adult individualsscreened for ASD. After extensive hyperparameter tuning via cross-validation,the study documents how the Gaussian Mixture Model achieved the highestclustering-to-label accuracy (95.31%) when mapped to the original ASD/NOclassification (4). Other key performance metrics included the Adjusted RandIndex (ARI) and silhouette scores, which further illustrated the internalcoherence of each cluster. The dataset underwent preprocessing proceduresincluding data cleaning, label encoding of categorical features, and standardscaling, followed by a thorough cross-validation approach to assess and comparethe four clustering methods (5). These results highlight the significantpotential of unsupervised methods in assisting ASD screening, especially incontexts where labeled data may be sparse, uncertain, or prohibitivelyexpensive to obtain. With continued methodological refinements, unsupervisedapproaches hold promise for augmenting early detection initiatives and guidingresource allocation to individuals at high risk.</description>
      <author>example@mail.com (Nora Fink)</author>
      <guid isPermaLink="false">2503.05746v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>RoboDesign1M: A Large-scale Dataset for Robot Design Understanding</title>
      <link>http://arxiv.org/abs/2503.06796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'研究背景': '机器人设计是一个复杂且耗时的过程，需要专业的知识和技能。当前领域的发展受到大规模设计数据集缺乏的限制。', '目的': '通过引入RoboDesign1M数据集来促进机器人设计领域的理解与应用，该数据集包含多样化的多模态设计信息。', '方法': '采用半自动化管道收集科学文献中的机器人设计相关数据，并生成涵盖多种领域的大规模数据集。', '主要发现': '在设计图像生成、视觉问答以及设计图检索等任务上进行了广泛的实验验证，证明了RoboDesign1M的有效性。', '结论': 'RoboDesign1M作为一个新的挑战基准，在促进机器人设计领域的研究和自动化方面具有重要意义。'}&lt;h4&gt;翻译&lt;/h4&gt;Robot design is a complex and time-consuming process that requires specialized expertise. While recent advancements in foundation models offer promising approaches to addressing these challenges, progress is hindered by the lack of large-scale datasets. This paper introduces RoboDesign1M, a large-scale dataset comprising multimodal data collected from scientific literature across various robotics domains. A semi-automated collection pipeline was proposed for efficient and diverse data gathering. Experiments were conducted on multiple tasks to evaluate its effectiveness, showing that it serves as a challenging new benchmark for design understanding tasks and has the potential to advance research in this field.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot design is a complex and time-consuming process that requiresspecialized expertise. Gaining a deeper understanding of robot design data canenable various applications, including automated design generation, retrievingexample designs from text, and developing AI-powered design assistants. Whilerecent advancements in foundation models present promising approaches toaddressing these challenges, progress in this field is hindered by the lack oflarge-scale design datasets. In this paper, we introduce RoboDesign1M, alarge-scale dataset comprising 1 million samples. Our dataset featuresmultimodal data collected from scientific literature, covering various roboticsdomains. We propose a semi-automated data collection pipeline, enablingefficient and diverse data acquisition. To assess the effectiveness ofRoboDesign1M, we conduct extensive experiments across multiple tasks, includingdesign image generation, visual question answering about designs, and designimage retrieval. The results demonstrate that our dataset serves as achallenging new benchmark for design understanding tasks and has the potentialto advance research in this field. RoboDesign1M will be released to supportfurther developments in AI-driven robotic design automation.</description>
      <author>example@mail.com (Tri Le, Toan Nguyen, Quang Tran, Quang Nguyen, Baoru Huang, Hoan Nguyen, Minh Nhat Vu, Tung D. Ta, Anh Nguyen)</author>
      <guid isPermaLink="false">2503.06796v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Infinite Leagues Under the Sea: Photorealistic 3D Underwater Terrain Generation by Latent Fractal Diffusion Models</title>
      <link>http://arxiv.org/abs/2503.06784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种专门用于生成水下三维地形表示的模型DreamSea，该模型基于真实世界的数据集进行训练，并能够产生逼真的海底场景。&lt;h4&gt;背景&lt;/h4&gt;现成的生成模型虽然在大规模互联网数据上进行了训练，但在特定的水下图像上的表现较差，因为这类图片相对较少。此外，实际收集到的水下图像容易受到噪声和现实世界的干扰。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够基于真实世界的数据集产生逼真海底场景的生成模型DreamSea。&lt;h4&gt;方法&lt;/h4&gt;利用视觉基础模型从数据中提取3D几何结构和语义信息，并训练了一个条件扩散模型，该模型在RGBD通道上生成真实的海底图像，这些图像是根据新的分形分布基元嵌入来生成的。之后，将生成的图像融合成一个三维地图，创建出一个由二维扩散先验监督的3D GS模型，从而实现逼真的新视角渲染。&lt;h4&gt;主要发现&lt;/h4&gt;DreamSea在生成大规模且一致、多样化的水下场景方面表现优异，并能够进行真实的视图合成。&lt;h4&gt;结论&lt;/h4&gt;该研究不仅提高了对海底环境仿真的质量，而且具有跨多个领域（如电影制作、游戏和机器人仿真）的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要中描述了这项工作解决了在生成三维水下地形表示方面的挑战。通过使用DreamSea模型，研究人员成功地创建了一个可以基于真实世界数据集生成逼真海底场景的系统，并且该成果在多个领域有着广泛的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper tackles the problem of generating representations of underwater 3Dterrain. Off-the-shelf generative models, trained on Internet-scale data butnot on specialized underwater images, exhibit downgraded realism, as images ofthe seafloor are relatively uncommon. To this end, we introduce DreamSea, agenerative model to generate hyper-realistic underwater scenes. DreamSea istrained on real-world image databases collected from underwater robot surveys.Images from these surveys contain massive real seafloor observations andcovering large areas, but are prone to noise and artifacts from the real world.We extract 3D geometry and semantics from the data with visual foundationmodels, and train a diffusion model that generates realistic seafloor images inRGBD channels, conditioned on novel fractal distribution-based latentembeddings. We then fuse the generated images into a 3D map, building a 3DGSmodel supervised by 2D diffusion priors which allows photorealistic novel viewrendering. DreamSea is rigorously evaluated, demonstrating the ability torobustly generate large-scale underwater scenes that are consistent, diverse,and photorealistic. Our work drives impact in multiple domains, spanningfilming, gaming, and robot simulation.</description>
      <author>example@mail.com (Tianyi Zhang, Weiming Zhi, Joshua Mangelson, Matthew Johnson-Roberson)</author>
      <guid isPermaLink="false">2503.06784v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>CoDa-4DGS: Dynamic Gaussian Splatting with Context and Deformation Awareness for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2503.06744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的4D高斯点阵（4DGS）方法，用于改进动态场景渲染。&lt;h4&gt;背景&lt;/h4&gt;动态场景渲染在自动驾驶中具有重要应用价值，能够实现闭环模拟和逼真数据生成。然而，交通环境的复杂性和高度动态性对准确渲染提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;通过引入上下文感知和时间变形感知机制来提高动态场景渲染的精度。&lt;h4&gt;方法&lt;/h4&gt;采用2D语义分割基础模型来自监督Gaussians的4D语义特征，并追踪相邻帧之间每个Gaussian的时间变形。聚合和编码这些信息，使得每个Gaussian能够对潜在的空间变形进行补偿。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在捕获动态场景渲染中的细微细节方面表现出色，在4D重构和新视角合成中超越了其他自监督方法。通过使语义特征随每个高斯点变化，进一步扩展应用范围。&lt;h4&gt;结论&lt;/h4&gt;研究结果证明了4DGS方法的有效性，并展示了其在自动驾驶等领域的广泛应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为：动态场景渲染在自主驾驶中开启了新的途径，通过启用具有逼真数据的闭环模拟来验证端到端算法。然而，交通环境的复杂性和高度动态性质给准确地渲染这些场景带来了重大挑战。在这篇论文中，我们介绍了一种新颖的4D高斯点阵（4DGS）方法，该方法结合了上下文和时间变形感知，以改进动态场景渲染。具体而言，我们采用2D语义分割基础模型来自监督Gaussians的4D语义特征，并确保有意义的上下文嵌入。同时，跟踪相邻帧之间每个高斯点的时间变形。通过聚合并编码这些信息，使每个高斯点具备潜在的空间变形补偿线索，促进动态场景的更精确表示。实验结果表明，该方法提高了4DGS在捕获自主驾驶中动态场景渲染细节方面的能力，并且在4D重建和新视角合成方面超越了其他自监督方法。此外，通过让语义特征随每个高斯点变化，使应用范围更加广泛。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic scene rendering opens new avenues in autonomous driving by enablingclosed-loop simulations with photorealistic data, which is crucial forvalidating end-to-end algorithms. However, the complex and highly dynamicnature of traffic environments presents significant challenges in accuratelyrendering these scenes. In this paper, we introduce a novel 4D GaussianSplatting (4DGS) approach, which incorporates context and temporal deformationawareness to improve dynamic scene rendering. Specifically, we employ a 2Dsemantic segmentation foundation model to self-supervise the 4D semanticfeatures of Gaussians, ensuring meaningful contextual embedding.Simultaneously, we track the temporal deformation of each Gaussian acrossadjacent frames. By aggregating and encoding both semantic and temporaldeformation features, each Gaussian is equipped with cues for potentialdeformation compensation within 3D space, facilitating a more preciserepresentation of dynamic scenes. Experimental results show that our methodimproves 4DGS's ability to capture fine details in dynamic scene rendering forautonomous driving and outperforms other self-supervised methods in 4Dreconstruction and novel view synthesis. Furthermore, CoDa-4DGS deformssemantic features with each Gaussian, enabling broader applications.</description>
      <author>example@mail.com (Rui Song, Chenwei Liang, Yan Xia, Walter Zimmer, Hu Cao, Holger Caesar, Andreas Festag, Alois Knoll)</author>
      <guid isPermaLink="false">2503.06744v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving</title>
      <link>http://arxiv.org/abs/2503.03205v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;利用计算机可验证语言如Lean解决数学问题对数学和计算机科学社区产生了显著影响。&lt;h4&gt;背景&lt;/h4&gt;当前最先进的方法使用单一的大规模语言模型(LLMs)作为生成完整证明或进行树搜索的代理。然而，这些单个代理的方法本质上缺乏结合自然语言(NL)高层次推理与形式语言(FL)验证反馈的方式。&lt;h4&gt;目的&lt;/h4&gt;为解决这些问题，我们提出了一种名为MA-LoT的新框架：基于Lean4定理证明的多代理长链思考(Long CoT)框架。这是（据我们所知）第一个平衡高层次NL推理和FL验证的多代理框架。&lt;h4&gt;方法&lt;/h4&gt;我们的方法通过利用我们在Long CoT中新兴的形式推理能力，并采用新的LoT-Transfer Learning训练-推理流水线，实现了结构化的交互。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验显示，我们的框架在Lean4版本的MiniF2F-Test数据集上达到了61.07%的准确率，远远超过了GPT-4（22.95%）、单代理树搜索(InternLM-Step-Prover, 50.70%)和完整证明生成(Godel-Prover, 55.33%)基准方法。&lt;h4&gt;结论&lt;/h4&gt;我们的发现强调了在更广阔的视角下结合Long CoT与形式验证，以实现更有洞察力的生成的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;使用计算机可验证的语言如Lean来解决数学问题对数学和计算机科学领域产生了重要影响。当前的方法主要依赖于单个大型语言模型进行证明或树搜索操作。然而，这些方法在高层次的自然语言推理与形式语言验证反馈结合方面存在不足。为了应对这一挑战，我们提出了MA-LoT框架：一个多代理Lean4定理证明系统，它能够平衡自然语言的高层次推理和形式语言验证，并且通过长链思考(Long CoT)机制展现出了强大的形式推理能力。实验表明，我们的方法在特定数据集上表现出色，超过了其他基准模型的表现。这说明结合Long CoT与正式验证可以带来更具洞察力的方法改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/rickyskywalker/leanofthought-official&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solving mathematical problems using computer-verifiable languages like Leanhas significantly impacted mathematical and computer science communities.State-of-the-art methods utilize single Large Language Models (LLMs) as agentsor provers to either generate complete proof or perform tree searches. However,single-agent methods inherently lack a structured way to combine high-levelreasoning in Natural Language (NL) with Formal Language (FL) verificationfeedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based LongChain-of-Thought framework, (to the best of our knowledge), the firstmulti-agent framework for Lean4 theorem proving that balance high-level NLreasoning and FL verification in Long CoT. Using this structured interaction,our approach enables deeper insights and long-term coherence in proofgeneration, with which past methods struggle. We do this by leveraging emergentformal reasoning ability in Long CoT using our novel LoT-Transfer Learningtraining-inference pipeline. Extensive experiments show that our frameworkachieves a 61.07% accuracy rate on the Lean4 version of the MiniF2F-Testdataset, largely outperforming GPT-4 (22.95%), single-agent tree search(InternLM-Step-Prover, 50.70%), and whole-proof generation (Godel-Prover,55.33%) baselines. Furthermore, our findings highlight the potential ofcombining Long CoT with formal verification for a more insightful generation ina broader perspective.</description>
      <author>example@mail.com (Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang)</author>
      <guid isPermaLink="false">2503.03205v2</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>UniGenX: Unified Generation of Sequence and Structure with Autoregressive Diffusion</title>
      <link>http://arxiv.org/abs/2503.06687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种用于科学数据序列和结构统一生成的新框架UniGenX，该框架结合了自回归模型和条件扩散模型的优点。&lt;h4&gt;背景&lt;/h4&gt;目前的科学研究中，序列和结构的数据生成主要依靠自回归模型或扩散模型。前者在自然语言处理等领域表现出色但难以直接应用于高精度要求且数据类型多样的科学领域；后者则擅长于高维复杂数据生成但不适用于序列建模。&lt;h4&gt;目的&lt;/h4&gt;通过结合自回归与条件扩散模型的优点，提出一种统一框架UniGenX以解决现有方法的局限性，并提高科学数据生成的任务性能。&lt;h4&gt;方法&lt;/h4&gt;引入新的框架UniGenX，该框架集成了自回归下一个标记预测和基于条件的扩散模型，同时利用高级编码器将复杂模态表示为离散序列。&lt;h4&gt;主要发现&lt;/h4&gt;验证了UniGenX在材料及小分子生成任务中的有效性，显著提高了现有最佳水平，并在复杂的长序列处理方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;UniGenX作为一种多功能工具，在科学数据的统一生成中显示出了其潜在的价值和广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;统一生成科学研究（如材料、分子、蛋白质）的数据序列和结构是一个重要的任务。当前的方法主要依赖自回归模型或扩散模型，各有利弊。自回归模型在自然语言处理等领域表现出色但难以直接应用于科学领域；而扩散模型擅长于高维复杂数据的生成但在序列建模方面存在局限性。为克服这些挑战，本文提出了一种新框架UniGenX，结合了自回归和条件扩散模型的优势，并通过实验验证了其有效性，特别是在材料晶体结构预测和小分子结构设计等任务中的应用，展示了显著的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unified generation of sequence and structure for scientific data (e.g.,materials, molecules, proteins) is a critical task. Existing approachesprimarily rely on either autoregressive sequence models or diffusion models,each offering distinct advantages and facing notable limitations.Autoregressive models, such as GPT, Llama, and Phi-4, have demonstratedremarkable success in natural language generation and have been extended tomultimodal tasks (e.g., image, video, and audio) using advanced encoders likeVQ-VAE to represent complex modalities as discrete sequences. However, theirdirect application to scientific domains is challenging due to the highprecision requirements and the diverse nature of scientific data. On the otherhand, diffusion models excel at generating high-dimensional scientific data,such as protein, molecule, and material structures, with remarkable accuracy.Yet, their inability to effectively model sequences limits their potential asgeneral-purpose multimodal foundation models. To address these challenges, wepropose UniGenX, a unified framework that combines autoregressive next-tokenprediction with conditional diffusion models. This integration leverages thestrengths of autoregressive models to ease the training of conditionaldiffusion models, while diffusion-based generative heads enhance the precisionof autoregressive predictions. We validate the effectiveness of UniGenX onmaterial and small molecule generation tasks, achieving a significant leap instate-of-the-art performance for material crystal structure prediction andestablishing new state-of-the-art results for small molecule structureprediction, de novo design, and conditional generation. Notably, UniGenXdemonstrates significant improvements, especially in handling long sequencesfor complex structures, showcasing its efficacy as a versatile tool forscientific data generation.</description>
      <author>example@mail.com (Gongbo Zhang, Yanting Li, Renqian Luo, Pipi Hu, Zeru Zhao, Lingbo Li, Guoqing Liu, Zun Wang, Ran Bi, Kaiyuan Gao, Liya Guo, Yu Xie, Chang Liu, Jia Zhang, Tian Xie, Robert Pinsler, Claudio Zeni, Ziheng Lu, Yingce Xia, Marwin Segler, Maik Riechert, Li Yuan, Lei Chen, Haiguang Liu, Tao Qin)</author>
      <guid isPermaLink="false">2503.06687v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Discovering the influence of personal features in psychological processes using Artificial Intelligence techniques: the case of COVID19 lockdown in Spain</title>
      <link>http://arxiv.org/abs/2503.05729v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究了2019年底在中国爆发的新型冠状病毒导致全球COVID-19大流行，并分析了西班牙在疫情初期采取的隔离措施及其对心理健康的影响。&lt;h4&gt;背景&lt;/h4&gt;2019年底中国爆发新型冠状病毒感染，导致全球范围内的新冠病毒大流行。西班牙最早于2020年一月底发现首例病例，到三月中旬感染人数已突破五千人。政府随后实施全国封锁以遏制疫情扩散。&lt;h4&gt;目的&lt;/h4&gt;了解隔离措施对心理状态的影响以及影响精神健康的因素对于未来公共卫生政策至关重要。本研究利用人工智能技术分析个人、经济社会状况、健康和居住条件等因素在封城期间的心理状态上的影响。&lt;h4&gt;方法&lt;/h4&gt;通过在线问卷收集数据，并使用两种不同的工作流程处理数据，每个工作流程分为三个阶段：首先根据心理评估将个体分类（直接或结合无监督学习）；其次训练多种机器学习分类器以区分识别的群体；最后进行特征重要性分析，找出与不同心理健康状况相关的最具有影响力的变量。&lt;h4&gt;主要发现&lt;/h4&gt;所评估模型表现良好，准确性超过80%，特别是在随机森林、决策树和支持向量机方面常常超过了90%。对于诊断脆弱性的准确率超过了90%，但对于使用居住环境和经济地位特征的非脆弱个体，性能略低。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，利用机器学习技术可以帮助识别在封锁期间心理健康的决定性因素，并为未来公共卫生政策提供有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; At the end of 2019, an outbreak of a novel coronavirus was reported in China,leading to the COVID-19 pandemic. In Spain, the first cases were detected inlate January 2020, and by mid-March, infections had surpassed 5,000. On Marchthe Spanish government started a nationwide lockdown to contain the spread ofthe virus. While isolation measures were necessary, they posed significantpsychological and socioeconomic challenges, particularly for vulnerablepopulations. Understanding the psychological impact of lockdown and the factorsinfluencing mental health is crucial for informing future public healthpolicies. This study analyzes the influence of personal, socioeconomic, generalhealth and living condition factors on psychological states during lockdownusing AI techniques. A dataset collected through an online questionnaire wasprocessed using two workflows, each structured into three stages. First,individuals were categorized based on psychological assessments, eitherdirectly or in combination with unsupervised learning techniques. Second,various Machine Learning classifiers were trained to distinguish between theidentified groups. Finally, feature importance analysis was conducted toidentify the most influential variables related to different psychologicalconditions. The evaluated models demonstrated strong performance, with accuracyexceeding 80% and often surpassing 90%, particularly for Random Forest,Decision Trees, and Support Vector Machines. Sensitivity and specificityanalyses revealed that models performed well across different psychologicalconditions, with the health impacts subset showing the highest reliability. Fordiagnosing vulnerability, models achieved over 90% accuracy, except for lessvulnerable individuals using living environment and economic status features,where performance was slightly lower.</description>
      <author>example@mail.com (Blanca Mellor-Marsa, Alfredo Guitian, Andrew Coney, Berta Padilla, Alberto Nogales)</author>
      <guid isPermaLink="false">2503.05729v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets</title>
      <link>http://arxiv.org/abs/2503.06664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 1 main figure, 3 plots, Published at ICLR 2025 Workshop on  Foundation Models in the Wild&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了大型语言模型（LLM）在数据清洗中的应用，通过实验验证LLM能否减轻手动数据清理的工作量。&lt;h4&gt;背景&lt;/h4&gt;高质量、无错误的数据集是构建可靠、准确和无偏的机器学习模型的关键因素。然而，现实世界中的数据集常常因传感器故障、数据输入错误或跨多个来源不当整合等问题而存在错误，这些错误会严重降低模型性能。&lt;h4&gt;目的&lt;/h4&gt;研究大型语言模型能否通过自动化手段减轻手动数据清理的工作量，并改进机器学习算法的训练效果。&lt;h4&gt;方法&lt;/h4&gt;在实验中设置了一个场景，在这个场景中一个大型语言模型与Python配对被用来清洗训练集，以提高学习算法的表现。该实验是在故意引入错误的Kaggle数据集上进行的，且模型只能利用当前行内其他特征的信息以及前几次迭代反馈来识别和修正异常值。&lt;h4&gt;主要发现&lt;/h4&gt;LLM能够通过上下文信息从同一行内的其他功能中识别并纠正逻辑错误或离群点。然而，它们在检测跨越多行的数据分布中的复杂问题上存在困难，例如趋势和偏见的分析。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型可以在一定程度上帮助减轻数据清洗过程中的手动劳动强度，并有助于提高学习算法的表现，但其有效性取决于具体类型的错误及其上下文环境。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality, error-free datasets are a key ingredient in building reliable,accurate, and unbiased machine learning (ML) models. However, real worlddatasets often suffer from errors due to sensor malfunctions, data entrymistakes, or improper data integration across multiple sources that canseverely degrade model performance. Detecting and correcting these issuestypically require tailor-made solutions and demand extensive domain expertise.Consequently, automation is challenging, rendering the process labor-intensiveand tedious. In this study, we investigate whether Large Language Models (LLMs)can help alleviate the burden of manual data cleaning. We set up an experimentin which an LLM, paired with Python, is tasked with cleaning the trainingdataset to improve the performance of a learning algorithm without having theability to modify the training pipeline or perform any feature engineering. Werun this experiment on multiple Kaggle datasets that have been intentionallycorrupted with errors. Our results show that LLMs can identify and correcterroneous entries, such as illogical values or outlier, by leveragingcontextual information from other features within the same row, as well asfeedback from previous iterations. However, they struggle to detect morecomplex errors that require understanding data distribution across multiplerows, such as trends and biases.</description>
      <author>example@mail.com (Tommaso Bendinelli, Artur Dox, Christian Holz)</author>
      <guid isPermaLink="false">2503.06664v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>PathVQ: Reforming Computational Pathology Foundation Model for Whole Slide Image Analysis via Vector Quantization</title>
      <link>http://arxiv.org/abs/2503.06482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;计算病理学和全滑动图像(WSI)分析在癌症诊断和预后中至关重要，但WSI的超高分辨率带来了模型训练的巨大挑战。最近，在病理基础模型上的进展提高了性能，然而大多数方法依赖于ViT瓷砖中的[CLS]标记表示作为整个图像级别的输入，这忽略了从patch token的关键空间细节，限制了下游WSI分析任务的表现。&lt;h4&gt;背景&lt;/h4&gt;计算病理学和全滑动图像（WSI）在癌症诊断与预后中至关重要。当前的方法通过使用Tile ViT的[CLS]令牌表示作为整个图像级别输入来提高性能，这种方法忽视了patch tokens中的关键空间细节。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，利用所有空间patch tokens进行WSI分析，并解决由此带来的存储和训练成本问题。&lt;h4&gt;方法&lt;/h4&gt;引入基于向量量化(VQ)的蒸馏技术用于压缩patch特征。该方法通过使用离散索引和解码器来高效地处理空间patch token，同时保持重建保真度。进一步采用多尺度VQ(MSVQ)策略以改进VQ重构性能并作为自我监督学习(SSL)的监督信号。基于量化的patch特征以及通过MSVQ得到的瓷砖目标，开发了一个渐进式卷积模块和一个图像级别的SSL来提取具有丰富空间信息的表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够将token维度从1024降至16，实现64倍压缩率同时保持重建保真度。通过采用MSVQ策略进一步提升了VQ重构性能，并作为自我监督学习(SSL)的监督信号，用于无缝图像级别的预训练目标。&lt;h4&gt;结论&lt;/h4&gt;广泛的评估证明了该方法在WSI分析中的有效性并达到了最先进的表现水平。代码将很快公开。&lt;h4&gt;翻译&lt;/h4&gt;计算病理学和全滑动图像（WSI）是癌症诊断与预后的关键组成部分，然而WSI的超高分辨率带来了模型训练的巨大挑战。最近，基础病理模型取得了进展，但大多数方法依赖于ViT瓷砖中的[CLS]令牌表示作为整个图像级别的输入，这忽视了patch tokens的空间细节并限制了下游任务的表现。作者发现利用所有空间patch tokens可以提升WSI分析的效果，但是会显著增加存储和训练成本。为了解决这个问题，他们引入了一种基于向量量化(VQ)的特征蒸馏方法，这种方法通过使用离散索引和解码器高效地压缩空间patch token，并保持重建保真度。此外，作者还采用了多尺度VQ(MSVQ)策略来增强重构性能并作为自我监督学习(SSL)的监督信号。基于MSVQ得到的量化patch特征以及瓷砖目标，他们开发了渐进式卷积模块和图像级别的SSL以提取具有丰富空间信息的表示，用于下游任务。广泛的评估表明该方法在WSI分析中的有效性，并达到了最先进的表现水平。代码将很快发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational pathology and whole-slide image (WSI) analysis are pivotal incancer diagnosis and prognosis. However, the ultra-high resolution of WSIspresents significant modeling challenges. Recent advancements in pathologyfoundation models have improved performance, yet most approaches rely on [CLS]token representation of tile ViT as slide-level inputs (16x16 pixels isrefereed as patch and 224x224 pixels as tile). This discards critical spatialdetails from patch tokens, limiting downstream WSI analysis tasks. We find thatleveraging all spatial patch tokens benefits WSI analysis but incurs nearly200x higher storage and training costs (e.g., 196 tokens in ViT$_{224}$). Toaddress this, we introduce vector quantized (VQ) distillation on patch feature,which efficiently compresses spatial patch tokens using discrete indices and adecoder. Our method reduces token dimensionality from 1024 to 16, achieving a64x compression rate while preserving reconstruction fidelity. Furthermore, weemploy a multi-scale VQ (MSVQ) strategy, which not only enhances VQreconstruction performance but also serves as a Self-supervised Learning (SSL)supervision for a seamless slide-level pretraining objective. Built upon thequantized patch features and supervision targets of tile via MSVQ, we develop aprogressive convolutional module and slide-level SSL to extract representationswith rich spatial-information for downstream WSI tasks. Extensive evaluationson multiple datasets demonstrate the effectiveness of our approach, achievingstate-of-the-art performance in WSI analysis. Code will be available soon.</description>
      <author>example@mail.com (Honglin Li, Zhongyi Shui, Yunlong Zhang, Chenglu Zhu, Lin Yang)</author>
      <guid isPermaLink="false">2503.06482v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>VORTEX: Challenging CNNs at Texture Recognition by using Vision Transformers with Orderless and Randomized Token Encodings</title>
      <link>http://arxiv.org/abs/2503.06368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的纹理识别方法VORTEX，该方法利用Vision Transformers (ViTs)的多深度令牌嵌入和轻量级聚合模块来改善纹理分析。&lt;h4&gt;背景&lt;/h4&gt;近年来，基于ImageNet预训练的卷积神经网络(CNNs)在纹理识别领域占据主导地位。然而，尽管Vision Transformers(ViTs)几年前就已引入，它们的纹理识别能力仍不清楚。&lt;h4&gt;目的&lt;/h4&gt;旨在探索ViT用于纹理分析的有效性，并展示一种新的方法VORTEX来改进现有的纹理识别任务。&lt;h4&gt;方法&lt;/h4&gt;VORTEX从预训练的ViT主干中提取多深度令牌嵌入并使用轻量级模块进行层次特征聚合，以实现无序编码。这种方法可以与任何具有通用Transformer架构的ViT无缝集成，并且不需要对模型进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在九个不同的纹理数据集上评估了VORTEX的方法，展示了其在各种纹理分析场景中达到或超越SOTA性能的能力。此外，该方法相比成本相似的CNNs展示了更好的计算效率。&lt;h4&gt;结论&lt;/h4&gt;通过弥合基于CNN和transformer架构之间的纹理识别差距，VORTEX为采用新兴的基础模型提供了途径，并且是完全开源可用的。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了使用Vision Transformers (ViTs)进行纹理识别的有效性，引入了一种新的方法VORTEX。该方法展示出了卓越的性能，并为未来的研究指明方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Texture recognition has recently been dominated by ImageNet-pre-trained deepConvolutional Neural Networks (CNNs), with specialized modifications andfeature engineering required to achieve state-of-the-art (SOTA) performance.However, although Vision Transformers (ViTs) were introduced a few years ago,little is known about their texture recognition ability. Therefore, in thiswork, we introduce VORTEX (ViTs with Orderless and Randomized Token Encodingsfor Texture Recognition), a novel method that enables the effective use of ViTsfor texture analysis. VORTEX extracts multi-depth token embeddings frompre-trained ViT backbones and employs a lightweight module to aggregatehierarchical features and perform orderless encoding, obtaining a better imagerepresentation for texture recognition tasks. This approach allows seamlessintegration with any ViT with the common transformer architecture. Moreover, nofine-tuning of the backbone is performed, since they are used only as frozenfeature extractors, and the features are fed to a linear SVM. We evaluateVORTEX on nine diverse texture datasets, demonstrating its ability to achieveor surpass SOTA performance in a variety of texture analysis scenarios. Bybridging the gap between texture recognition with CNNs and transformer-basedarchitectures, VORTEX paves the way for adopting emerging transformerfoundation models. Furthermore, VORTEX demonstrates robust computationalefficiency when coupled with ViT backbones compared to CNNs with similar costs.The method implementation and experimental scripts are publicly available inour online repository.</description>
      <author>example@mail.com (Leonardo Scabini, Kallil M. Zielinski, Emir Konuk, Ricardo T. Fares, Lucas C. Ribas, Kevin Smith, Odemir M. Bruno)</author>
      <guid isPermaLink="false">2503.06368v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>GeoLangBind: Unifying Earth Observation with Agglomerative Vision-Language Foundation Models</title>
      <link>http://arxiv.org/abs/2503.06312v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  code &amp; weights: https://github.com/xiong-zhitong/GeoLB-SigLIP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GeoLangBind是一个将地理数据与自然语言连接起来的新型聚合视觉-语言基础模型，利用语言作为统一媒介来处理来自不同传感器的异构地球观测（EO）数据。&lt;h4&gt;背景&lt;/h4&gt;由于地球观测数据来源于不同的传感器且成像原理各异，创建统一的分析框架存在巨大挑战。当前缺乏一种能够无缝整合和互补学习多样传感器数据的方法。&lt;h4&gt;目的&lt;/h4&gt;提出GeoLangBind模型以解决不同地球观测数据类型的集成问题，通过语言嵌入空间使各种EO数据类型对齐，并开发一个零样本基础模型来处理任意数量的EO数据通道输入。&lt;h4&gt;方法&lt;/h4&gt;构建了一个大规模的多模态图像-文本数据集GeoLangBind-2M，包括六种不同的数据模式。利用这一数据集和设计的模态感知知识聚合（MaKA）模块以及渐进式多模态权重合并策略来开发零样本基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;GeoLangBind在零样本视觉-语言理解和细粒度视觉理解方面表现优越，通过23个涵盖多个任务的数据集上的广泛评估证明了其卓越的性能和多功能性。&lt;h4&gt;结论&lt;/h4&gt;GeoLangBind为各种环境监测和分析任务提供了一个强大的框架，并将在未来公开数据集和预训练模型。&lt;h4&gt;翻译&lt;/h4&gt;地球观测（EO）数据由于来源于多样化的传感器，具有不同的成像原理，创建统一的分析框架面临巨大挑战。提出了一种新的聚合视觉-语言基础模型GeoLangBind，通过将语言作为统一媒介来处理异构EO数据模式。我们的方法使不同类型的EO数据在共享的语言嵌入空间中对齐，从而实现无缝整合和互补学习。为此构建了一个大规模多模态图像-文本数据集GeoLangBind-2M，涵盖六种不同的数据模式，并利用该数据集开发出零样本基础模型以处理任意数量的EO数据通道输入。通过设计的模态感知知识聚合（MaKA）模块以及渐进式多模态权重合并策略，创建了一个强大的聚合基础模型，在零样本视觉-语言理解和细粒度视觉理解方面表现出色。经过23个不同任务的数据集上的广泛评估，证明GeoLangBind在EO应用中的优越性能和多功能性，并将在未来公开数据集和预训练模型以供使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Earth observation (EO) data, collected from diverse sensors with varyingimaging principles, present significant challenges in creating unifiedanalytical frameworks. We present GeoLangBind, a novel agglomerativevision--language foundation model that bridges the gap between heterogeneous EOdata modalities using language as a unifying medium. Our approach alignsdifferent EO data types into a shared language embedding space, enablingseamless integration and complementary feature learning from diverse sensordata. To achieve this, we construct a large-scale multimodal image--textdataset, GeoLangBind-2M, encompassing six data modalities. GeoLangBindleverages this dataset to develop a zero-shot foundation model capable ofprocessing arbitrary numbers of EO data channels as input. Through our designedModality-aware Knowledge Agglomeration (MaKA) module and progressive multimodalweight merging strategy, we create a powerful agglomerative foundation modelthat excels in both zero-shot vision--language comprehension and fine-grainedvisual understanding. Extensive evaluation across 23 datasets covering multipletasks demonstrates GeoLangBind's superior performance and versatility in EOapplications, offering a robust framework for various environmental monitoringand analysis tasks. The dataset and pretrained models will be publiclyavailable.</description>
      <author>example@mail.com (Zhitong Xiong, Yi Wang, Weikang Yu, Adam J Stewart, Jie Zhao, Nils Lehmann, Thomas Dujardin, Zhenghang Yuan, Pedram Ghamisi, Xiao Xiang Zhu)</author>
      <guid isPermaLink="false">2503.06312v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Critical Foreign Policy Decisions (CFPD)-Benchmark: Measuring Diplomatic Preferences in Large Language Models</title>
      <link>http://arxiv.org/abs/2503.06263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;随着国家安全机构将人工智能集成到决策和内容生成过程中，理解大型语言模型（LLMs）的固有偏见变得至关重要。本文介绍了一种评估七种主流基础模型在国际关系背景下的偏见和偏好基准。&lt;h4&gt;背景&lt;/h4&gt;国家安全机构越来越多地使用AI进行决策制定与内容生成，而这些系统中存在的偏见可能会影响结果的有效性和公正性。&lt;h4&gt;目的&lt;/h4&gt;设计并实施一种新的测试方案来评测不同大型语言模型在处理特定主题时的偏差倾向。&lt;h4&gt;方法&lt;/h4&gt;基于国际关系的核心议题设计了400个专家构造的情景案例，用于分析选定模型的回答和建议。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了模型依据四个测试领域提供的推荐意见存在显著差异；Qwen2 72B、Gemini 1.5 Pro-002 和 Llama 3.1 8B Instruct 模型给出的建议更倾向于加剧局势，而Claude 3.5 Sonnet和GPT-4o模型则较为温和。所有模型在处理涉及不同国家的情景时表现出一定的特定国家偏见。&lt;h4&gt;结论&lt;/h4&gt;这些发现强调了在高风险环境中部署大型语言模型时需要进行严格控制，并指出有必要针对具体领域开展详细评估及对模型进行微调，以确保其与机构目标一致。&lt;h4&gt;翻译&lt;/h4&gt;随着国家安全机构将人工智能集成到决策和内容生成过程中，理解大型语言模型（LLMs）的固有偏见变得至关重要。本文介绍了一种新的基准测试方法，用于评估七种著名基础模型在国际关系背景下的偏差倾向和偏好。这些模型包括Llama 3.1 8B Instruct、Llama 3.1 70B Instruct、GPT-4o、Gemini 1.5 Pro-002、Mixtral 8x22B、Claude 3.5 Sonnet 和 Qwen2 72B。研究基于国际关系的核心议题设计了400个专家构造的情景案例，分析这些模型在处理军事升级、人道主义和军事干预、国际合作以及联盟动态等方面的表现。研究表明，在上述四个测试领域中，Qwen2 72B、Gemini 1.5 Pro-002 和 Llama 3.1 8B Instruct 模型给出的建议更倾向于加剧局势，而Claude 3.5 Sonnet和GPT-4o模型则较为温和。所有模型在处理涉及不同国家的情景时表现出一定的特定国家偏见，通常对中国和俄罗斯采取不那么激进的态度。这些发现强调了在高风险环境中部署大型语言模型的必要性，同时指出需要进行领域特异性评估和模型微调以确保符合机构目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As national security institutions increasingly integrate ArtificialIntelligence (AI) into decision-making and content generation processes,understanding the inherent biases of large language models (LLMs) is crucial.This study presents a novel benchmark designed to evaluate the biases andpreferences of seven prominent foundation models-Llama 3.1 8B Instruct, Llama3.1 70B Instruct, GPT-4o, Gemini 1.5 Pro-002, Mixtral 8x22B, Claude 3.5 Sonnet,and Qwen2 72B-in the context of international relations (IR). We designed abias discovery study around core topics in IR using 400-expert craftedscenarios to analyze results from our selected models. These scenarios focusedon four topical domains including: military escalation, military andhumanitarian intervention, cooperative behavior in the international system,and alliance dynamics. Our analysis reveals noteworthy variation among modelrecommendations based on scenarios designed for the four tested domains.Particularly, Qwen2 72B, Gemini 1.5 Pro-002 and Llama 3.1 8B Instruct modelsoffered significantly more escalatory recommendations than Claude 3.5 Sonnetand GPT-4o models. All models exhibit some degree of country-specific biases,often recommending less escalatory and interventionist actions for China andRussia compared to the United States and the United Kingdom. These findingshighlight the necessity for controlled deployment of LLMs in high-stakesenvironments, emphasizing the need for domain-specific evaluations and modelfine-tuning to align with institutional objectives.</description>
      <author>example@mail.com (Benjamin Jensen, Ian Reynolds, Yasir Atalan, Michael Garcia, Austin Woo, Anthony Chen, Trevor Howarth)</author>
      <guid isPermaLink="false">2503.06263v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Intermediate Domain-guided Adaptation for Unsupervised Chorioallantoic Membrane Vessel Segmentation</title>
      <link>http://arxiv.org/abs/2503.03546v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于中间域指导的自适应（IDA）方法，用于鸡胚绒毛尿囊膜（CAM）血管分割问题。该方法利用了CAM图像与视网膜图像之间的相似性以及现有的公共视网膜数据集，通过无监督训练提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;绒毛尿囊膜模型广泛应用于血管生成研究中，其中血管的分布是关键评价指标。因此，基于拓扑和形态学的定量评估需要精确的血管分割。然而，手动分割耗时、费力且容易因主观性导致一致性问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的中间域引导自适应方法来解决现有CAM血管分割算法不足以及公共数据集缺乏的问题。&lt;h4&gt;方法&lt;/h4&gt;首先引入了多分辨率非对称翻译（MRAT）策略生成中间图像，以促进图像层面的交互。其次，开发了一个基于中间领域指导对比学习（IDCL）模块用于解耦跨域特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在新的CAM数据集上进行了广泛的实验验证，并且其性能优于比较的方法。此外，在视网膜数据集中也表现出色，展示了强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于中间域引导的自适应（IDA）方法能够有效解决当前存在的血管分割问题，并具有良好的推广性。&lt;h4&gt;翻译&lt;/h4&gt;绒毛尿囊膜模型被广泛用于血管生成研究中，评估指标为生长血管分布。因此，为了进行定量分析，基于拓扑和形态学的准确血管分割至关重要。然而，手动分割既耗时又费力且容易由于其主观性质而产生不一致性。此外，关于CAM血管分割算法的研究仍然有限，并且缺乏公共数据集导致预测性能不佳。为了解决这些挑战，我们提出了一种创新性的中间域引导自适应（IDA）方法，利用了CAM图像和视网膜图像之间的相似性以及现有的公共视网膜数据集，对CAM图像进行无监督训练。该方法包括多分辨率非对称翻译策略生成中间图像以促进图像层面的交互，并开发了一个基于中间领域指导对比学习模块用于解耦跨域特征表示。这种方法克服了现有无监督领域自适应（UDA）方法主要集中在直接源目标对齐上而忽视中间领域的信息这一局限性。值得注意的是，我们创建了第一个CAM数据集来验证所提出的算法。在该数据集上的广泛实验表明，我们的方法优于比较的方法，并且其在视网膜数据集中也表现出色，突出了其强大的泛化能力。CAM数据集和源代码可在https://github.com/Light-47/IDA获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/pwsong-ustc/ida&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The chorioallantoic membrane (CAM) model is widely employed in angiogenesisresearch, and distribution of growing blood vessels is the key evaluationindicator. As a result, vessel segmentation is crucial for quantitativeassessment based on topology and morphology. However, manual segmentation isextremely time-consuming, labor-intensive, and prone to inconsistency due toits subjective nature. Moreover, research on CAM vessel segmentation algorithmsremains limited, and the lack of public datasets contributes to poor predictionperformance. To address these challenges, we propose an innovative IntermediateDomain-guided Adaptation (IDA) method, which utilizes the similarity betweenCAM images and retinal images, along with existing public retinal datasets, toperform unsupervised training on CAM images. Specifically, we introduce aMulti-Resolution Asymmetric Translation (MRAT) strategy to generateintermediate images to promote image-level interaction. Then, an IntermediateDomain-guided Contrastive Learning (IDCL) module is developed to disentanglecross-domain feature representations. This method overcomes the limitations ofexisting unsupervised domain adaptation (UDA) approaches, which primarilyconcentrate on directly source-target alignment while neglecting intermediatedomain information. Notably, we create the first CAM dataset to validate theproposed algorithm. Extensive experiments on this dataset show that our methodoutperforms compared approaches. Moreover, it achieves superior performance inUDA tasks across retinal datasets, highlighting its strong generalizationcapability. The CAM dataset and source codes are available athttps://github.com/Light-47/IDA.</description>
      <author>example@mail.com (Pengwu Song, Liang Xu, Peng Yao, Shuwei Shen, Pengfei Shao, Mingzhai Sun, Ronald X. Xu)</author>
      <guid isPermaLink="false">2503.03546v3</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Beyond H&amp;E: Unlocking Pathological Insights with Polarization via Self-supervised Learning</title>
      <link>http://arxiv.org/abs/2503.05933v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为PolarHE的双模态融合框架，结合了传统的H&amp;E染色和极化成像技术，以提高病理图像分析的质量。通过实验验证，该方法在Chaoyang数据集上达到86.70%的准确率，在MHIST数据集上的准确率为89.06%，显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;H&amp;E染色是数字病理学中的金标准，用于诊断和预后评估。然而，H&amp;E成像无法检测到组织双折射和各向异性变化，这些特性对于理解胶原蛋白的排列和其他微结构改变至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出PolarHE框架来补充现有技术的不足，并利用极化成像提升病理图像分析的能力。&lt;h4&gt;方法&lt;/h4&gt;通过特征分解策略分离出共享特性和模态特定特征，以实现有效的多模态表示学习。该方法结合了H&amp;E染色和极化成像的优点。&lt;h4&gt;主要发现&lt;/h4&gt;PolarHE框架在多项评估中表现出色，并且t-SNE可视化显示模型能够有效地捕捉到各种病理组织的光学签名，表明其诊断潜力巨大。&lt;h4&gt;结论&lt;/h4&gt;研究表明，极化成像是计算病理学中的强大但尚未充分开发的技术手段。通过结合极化成像和H&amp;E染色，可以显著提高特征表达质量和诊断准确性，为多模态学习开辟了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种名为PolarHE的新方法，它整合了H&amp;E染色与极化成像技术，以提升组织病理分析的敏感度和准确性。通过详细的实验验证，该模型不仅在两个数据集上表现出了高精度，还展示了独特的诊断潜力，表明极化成像是病理学领域内一个重要但未被充分利用的技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Histopathology image analysis is fundamental to digital pathology, withhematoxylin and eosin (H&amp;E) staining as the gold standard for diagnostic andprognostic assessments. While H&amp;E imaging effectively highlights cellular andtissue structures, it lacks sensitivity to birefringence and tissue anisotropy,which are crucial for assessing collagen organization, fiber alignment, andmicrostructural alterations--key indicators of tumor progression, fibrosis, andother pathological conditions. To bridge this gap, we propose PolarHE, a dualmodality fusion framework that integrates H&amp;E with polarization imaging,leveraging the polarization ability to enhance tissue characterization. Ourapproach employs a feature decomposition strategy to disentangle common andmodality specific features, ensuring effective multimodal representationlearning. Through comprehensive validation, our approach significantlyoutperforms previous methods, achieving an accuracy of 86.70% on the Chaoyangdataset and 89.06% on the MHIST dataset. Moreover, polarization propertyvisualization reveals distinct optical signatures of pathological tissues,highlighting its diagnostic potential. t-SNE visualizations further confirm ourmodel effectively captures both shared and unique modality features,reinforcing the complementary nature of polarization imaging. These resultsdemonstrate that polarization imaging is a powerful and underutilized modalityin computational pathology, enriching feature representation and improvingdiagnostic accuracy. PolarHE establishes a promising direction for multimodallearning, paving the way for more interpretable and generalizable pathologymodels. Our code will be released after paper acceptance.</description>
      <author>example@mail.com (Yao Du, Jiaxin Zhuang, Xiaoyu Zheng, Jing Cong, Limei Guo, Chao He, Lin Luo, Xiaomeng Li)</author>
      <guid isPermaLink="false">2503.05933v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Attention on the Wires (AttWire): A Foundation Model for Detecting Devices and Catheters in X-ray Fluoroscopic Images</title>
      <link>http://arxiv.org/abs/2503.06190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;设计了一种新的注意力机制，用于指导卷积神经网络模型在X光图像中定位介入设备和导管的位置。&lt;h4&gt;背景&lt;/h4&gt;在微创心血管手术中经常使用介入装置、导管以及经食道超声（TOE）探头等插入式成像设备。检测这些设备在X光荧光图像中的位置和方向对于许多临床应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的注意力机制，以提高轻量级基础模型的精度，并实现多个对象的同时实时检测。&lt;h4&gt;方法&lt;/h4&gt;该研究设计了一种包含多尺度高斯导数滤波器和点积注意力层的新注意力机制，用于指导卷积神经网络模型定位X光图像中的电线区域。&lt;h4&gt;主要发现&lt;/h4&gt;{'性能表现': '在12,438张X射线图像上训练并测试了该模型，在58帧每秒的速度下达到了0.88的超声探头检测准确率和0.87的人工瓣膜检测准确率（IoU测量）。', '成功案例': {'10电极导管': '成功率为99.8%。', '消融导管': '成功率为97.8%'}}&lt;h4&gt;结论&lt;/h4&gt;该检测基础模型能够同时实时地在X射线荧光图像中检测和识别介入装置和柔性导管。&lt;h4&gt;意义&lt;/h4&gt;提出的模型利用新颖的注意力机制实现了高性能的对象检测，适用于各种临床应用及机器人辅助手术。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Objective: Interventional devices, catheters and insertable imaging devicessuch as transesophageal echo (TOE) probes are routinely used in minimallyinvasive cardiovascular procedures. Detecting their positions and orientationsin X-ray fluoroscopic images is important for many clinical applications.Method: In this paper, a novel attention mechanism was designed to guide aconvolution neural network (CNN) model to the areas of wires in X-ray images,as nearly all interventional devices and catheters used in cardiovascularprocedures contain wires. The attention mechanism includes multi-scale Gaussianderivative filters and a dot-product-based attention layer. By utilizing theproposed attention mechanism, a lightweight foundation model can be created todetect multiple objects simultaneously with higher precision and real-timespeed. Results: The proposed model was trained and tested on a total of 12,438X-ray images. An accuracy of 0.88 was achieved for detecting an echo probe and0.87 for detecting an artificial valve at 58 FPS. The accuracy was measured byintersection-over-union (IoU). We also achieved a 99.8% success rate indetecting a 10-electrode catheter and a 97.8% success rate in detecting anablation catheter. Conclusion: Our detection foundation model cansimultaneously detect and identify both interventional devices and flexiblecatheters in real-time X-ray fluoroscopic images. Significance: The proposedmodel employs a novel attention mechanism to achieve high-performance objectdetection, making it suitable for various clinical applications androbotic-assisted surgeries. Codes are available athttps://github.com/YingLiangMa/AttWire.</description>
      <author>example@mail.com (YingLiang Ma, Sandra Howell, Aldo Rinaldi, Tarv Dhanjal, Kawal S. Rhode)</author>
      <guid isPermaLink="false">2503.06190v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>ForestSplats: Deformable transient field for Gaussian Splatting in the Wild</title>
      <link>http://arxiv.org/abs/2503.06179v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ForestSplats提出了一种新的方法，用于在不受限制的图像集合中高效地表示瞬态元素，并有效地将静态场景从瞬态干扰因素中分解出来。&lt;h4&gt;背景&lt;/h4&gt;3D Gaussian Splatting (3D-GS) 在静态场景中的实时渲染速度和高质量结果表现出色。然而，在现实环境中由于存在瞬态物体、光照变化以及多样化的遮挡级别，其性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决现有技术在估计遮挡物或瞬态元素时存在的两个缺陷：额外计算成本和记忆需求高。&lt;h4&gt;方法&lt;/h4&gt;ForestSplats通过使用变形瞬态场（deformable transient field）并引入一个感知超级像素的掩码（superpixel-aware mask），可以清晰地定义遮挡器边界，同时避免在密度化过程中生成遮挡内部的Gaussians。此外，该方法不需要Vision Foundation模型（VFM）。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验，在几个基准数据集上验证了ForestSplats优于没有使用VFM的现有方法，并且在表示瞬态元素方面表现出显著的记忆效率。&lt;h4&gt;结论&lt;/h4&gt;ForestSplats提供了一种有效的方法来处理动态场景中的瞬态因素，同时保持高计算效率和内存效率。&lt;h4&gt;翻译&lt;/h4&gt;最近，3D Gaussian Splatting (3D-GS) 出现了，在静态场景中显示出了实时渲染速度和高质量结果。尽管3D-GS在静态场景中表现有效，但由于瞬态物体、光照变化以及多样化的遮挡级别，其性能显著下降到真实世界环境中。为解决这些问题，现有方法通过利用预训练模型或集成附加瞬变场管道来估计遮挡物或瞬时元素。然而，这些方法仍然存在两个缺陷：1）使用Vision Foundation模型（VFM）中的语义特性导致额外的计算成本；2）瞬态字段需要大量内存处理每个视角的瞬态Gaussians，并且仅依赖于光度误差来定义遮挡器边界，从而难以明确界定。为解决这些问题，我们提出了一种新的方法ForestSplats，该方法利用可变形瞬态场和超级像素感知掩模，在不受限制的图像集合中有效地表示瞬时元素，并有效地区分静态场景与瞬时干扰因素，而无需VFM。我们设计了瞬态字段以捕获每个视角的瞬时元素，并引入了一种考虑光度误差和超级像素来清晰定义遮挡器边界的超级像素感知掩模。此外，还提出了不确定性感知密集化方法，在密度化过程中避免在遮挡内部生成Gaussians。通过几个基准数据集上的广泛实验验证了ForestSplats优于没有使用VFM的现有方法，并且在表示瞬时元素方面表现出显著的记忆效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, 3D Gaussian Splatting (3D-GS) has emerged, showing real-timerendering speeds and high-quality results in static scenes. Although 3D-GSshows effectiveness in static scenes, their performance significantly degradesin real-world environments due to transient objects, lighting variations, anddiverse levels of occlusion. To tackle this, existing methods estimateoccluders or transient elements by leveraging pre-trained models or integratingadditional transient field pipelines. However, these methods still suffer fromtwo defects: 1) Using semantic features from the Vision Foundation model (VFM)causes additional computational costs. 2) The transient field requiressignificant memory to handle transient elements with per-view Gaussians andstruggles to define clear boundaries for occluders, solely relying onphotometric errors. To address these problems, we propose ForestSplats, a novelapproach that leverages the deformable transient field and a superpixel-awaremask to efficiently represent transient elements in the 2D scene acrossunconstrained image collections and effectively decompose static scenes fromtransient distractors without VFM. We designed the transient field to bedeformable, capturing per-view transient elements. Furthermore, we introduce asuperpixel-aware mask that clearly defines the boundaries of occluders byconsidering photometric errors and superpixels. Additionally, we proposeuncertainty-aware densification to avoid generating Gaussians within theboundaries of occluders during densification. Through extensive experimentsacross several benchmark datasets, we demonstrate that ForestSplats outperformsexisting methods without VFM and shows significant memory efficiency inrepresenting transient elements.</description>
      <author>example@mail.com (Wongi Park, Myeongseok Nam, Siwon Kim, Sangwoo Jo, Soomok Lee)</author>
      <guid isPermaLink="false">2503.06179v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Force Aware Branch Manipulation To Assist Agricultural Tasks</title>
      <link>http://arxiv.org/abs/2503.07497v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种在农业环境中安全操纵树枝的方法，以帮助完成各种农业任务。&lt;h4&gt;背景&lt;/h4&gt;人类通常会通过操纵树枝来有效地执行农业工作，但当前的农业机器人缺乏这种能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种策略，使农业机器人能够操纵树枝以辅助进行精密农业生产任务，如密集树叶中的水果采摘、遮挡下的花朵授粉以及为导航移动悬垂的藤蔓和树枝。&lt;h4&gt;方法&lt;/h4&gt;提出了对RRT*算法进行修改的方法来规划路径，使之满足树枝几何约束并遵循其可变形特性。通过重新规划获取一种使机器人施力在期望范围内从而避免操纵过程中损坏树枝的路径。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示该方法成功率为78%，在50次试验中成功地从不同起点移动一根枝条到目标区域。&lt;h4&gt;结论&lt;/h4&gt;这项研究证明了使用改进后的RRT*算法有效进行树枝操作，为农业机器人提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;此研究提出了用于辅助完成各种农业任务的安全操纵树枝的方法。在实际的农业环境中，人们往往通过操作树枝来有效地执行农业工作，但目前的农业机器人缺乏这一能力。本项研究提出的操纵树枝策略可以有助于不同的精密农业生产任务，如密集树叶中的水果采摘、遮挡下的花朵授粉以及为导航移动悬垂的藤蔓和树枝等。该方法对RRT*算法进行了修改以规划满足树枝几何约束并遵循其可变形特性的路径，并通过重新规划获取使机器人施力在期望范围内而不损坏树枝的操作路径。实验结果表明，此方法的成功率为78%，50次试验中成功地从不同起点将一根枝条移动到目标区域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents a methodology to safely manipulate branches to aidvarious agricultural tasks. Humans in a real agricultural environment oftenmanipulate branches to perform agricultural tasks effectively, but currentagricultural robots lack this capability. This proposed strategy to manipulatebranches can aid in different precision agriculture tasks, such as fruitpicking in dense foliage, pollinating flowers under occlusion, and movingoverhanging vines and branches for navigation. The proposed method modifiesRRT* to plan a path that satisfies the branch geometric constraints and obeysbranch deformable characteristics. Re-planning is done to obtain a path thathelps the robot exert force within a desired range so that branches are notdamaged during manipulation. Experimentally, this method achieved a successrate of 78\% across 50 trials, successfully moving a branch from differentstarting points to a target region.</description>
      <author>example@mail.com (Madhav Rijal, Rashik Shrestha, Trevor Smith, Yu Gu)</author>
      <guid isPermaLink="false">2503.07497v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Blind-Wayfarer: A Minimalist, Probing-Driven Framework for Resilient Navigation in Perception-Degraded Environments</title>
      <link>http://arxiv.org/abs/2503.07492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于探针驱动的导航框架Blind-Wayfarer，它在视觉传感器失效的情况下，通过主要依赖指南针来实现复杂、未结构化环境中的稳健穿越。&lt;h4&gt;背景&lt;/h4&gt;自主机器人在森林和崎岖地形中导航时面临挑战，特别是在遮挡物、低光照或传感器噪声等导致外部感知（如摄像头和激光雷达）失效的条件下。&lt;h4&gt;目的&lt;/h4&gt;提出了一种基于迷宫解决算法灵感的探针驱动导航框架Blind-Wayfarer，旨在提高机器人在恶劣条件下的导航能力。&lt;h4&gt;方法&lt;/h4&gt;该框架依赖于指南针进行方向确定，并通过模拟实验和真实世界测试验证其有效性。&lt;h4&gt;主要发现&lt;/h4&gt;在1000次森林模拟实验中，Blind-Wayfarer实现了99.7%的成功率，在不同规模平台的20个实际案例试验中均成功逃离困境。一个显著的例子是机器人从45米深的森林内部到达边缘的道路。&lt;h4&gt;结论&lt;/h4&gt;本研究证明了探针驱动方法在感知受限条件下的可靠导航中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;自主机器人穿越茂密森林和崎岖地形时面临巨大挑战，尤其是在外部传感器（如摄像头和激光雷达）因遮挡、低光环境或传感器噪声而失效的情况下。我们提出了一种名为Blind-Wayfarer的探针驱动导航框架，灵感来自迷宫解决算法，并主要依赖于指南针来稳健地穿越复杂的未结构化环境。在1000次模拟森林试验中，该方法达到了99.7%的成功率；并在两个不同的实际场景下进行了测试——使用了不同尺寸的小车平台——我们的方法成功完成了所有20个案例的脱离困境任务。特别值得注意的是，在一个实例中，机器人从45米深处的森林内部行进至边缘的道路。这些发现展示了基于探针的方法在感知受限条件下的可靠导航方面的潜力。更多视频和代码可在我们的网站https://sites.google.com/view/blind-wayfarer上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Navigating autonomous robots through dense forests and rugged terrains isespecially daunting when exteroceptive sensors -- such as cameras and LiDARsensors -- fail under occlusions, low-light conditions, or sensor noise. Wepresent Blind-Wayfarer, a probing-driven navigation framework inspired bymaze-solving algorithms that relies primarily on a compass to robustly traversecomplex, unstructured environments. In 1,000 simulated forest experiments,Blind-Wayfarer achieved a 99.7% success rate. In real-world tests in twodistinct scenarios -- with rover platforms of different sizes -- our approachsuccessfully escaped forest entrapments in all 20 trials. Remarkably, ourframework also enabled a robot to escape a dense woodland, traveling from 45 minside the forest to a paved pathway at its edge. These findings highlight thepotential of probing-based methods for reliable navigation in challengingperception-degraded field conditions. Videos and code are available on ourwebsite https://sites.google.com/view/blind-wayfarer</description>
      <author>example@mail.com (Yanran Xu, Klaus-Peter Zauner, Danesh Tarapore)</author>
      <guid isPermaLink="false">2503.07492v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Learning Physics-Based Full-Body Human Reaching and Grasping from Brief Walking References</title>
      <link>http://arxiv.org/abs/2503.07481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种框架，利用简短的步行动作捕捉数据生成多样且物理上可行的人体伸手和抓握动作。&lt;h4&gt;背景&lt;/h4&gt;现有的基于动作捕捉数据的动作生成方法受限于数据质量和覆盖范围。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法来最大化利用简短步行动作捕捉数据，产生多样化、符合物理学原理的全身人体伸手及抓取动作。&lt;h4&gt;方法&lt;/h4&gt;结合先进的动力学方法和局部特征对齐机制，将自然行走中的运动模式转化为任务特定的动作指导，并通过主动数据生成策略提高合成动作的成功率与自然度。&lt;h4&gt;主要发现&lt;/h4&gt;该方法结合了自然步行的真实性、稳定性和任务特定生成数据的灵活性、通用性，在多种场景和未见过对象中表现出强大的性能和适应能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架能够高效利用有限的行走捕捉数据，产生高质量的动作合成结果。&lt;h4&gt;翻译&lt;/h4&gt;现有基于动作捕捉(mocap)数据的方法在动作生成上受到数据质量和覆盖范围的限制。本文提出了一种新框架，该框架使用简短步行的mocap数据来生成多样、物理可行的人体伸手和抓取全身体态动作。研究发现行走数据捕获了可跨任务转移的重要运动模式；另一方面，高级动力学方法可以产生多种抓取姿态，进而将其插值为特定于任务的动作指导。该框架结合积极的数据生成策略来最大化合成动作用途，并采用局部特征对齐机制将自然步行中的运动特性转移到伸手和抓握动作中，提高了成功率与自然度。通过综合自然行走的真实性和稳定性以及针对具体任务的灵活性和通用性，该方法在不同场景及未见过对象上表现出色且适应性强。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing motion generation methods based on mocap data are often limited bydata quality and coverage. In this work, we propose a framework that generatesdiverse, physically feasible full-body human reaching and grasping motionsusing only brief walking mocap data. Base on the observation that walking datacaptures valuable movement patterns transferable across tasks and, on the otherhand, the advanced kinematic methods can generate diverse grasping poses, whichcan then be interpolated into motions to serve as task-specific guidance. Ourapproach incorporates an active data generation strategy to maximize theutility of the generated motions, along with a local feature alignmentmechanism that transfers natural movement patterns from walking data to enhanceboth the success rate and naturalness of the synthesized motions. By combiningthe fidelity and stability of natural walking with the flexibility andgeneralizability of task-specific generated data, our method demonstratesstrong performance and robust adaptability in diverse scenes and with unseenobjects.</description>
      <author>example@mail.com (Yitang Li, Mingxian Lin, Zhuo Lin, Yipeng Deng, Yue Cao, Li Yi)</author>
      <guid isPermaLink="false">2503.07481v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Causal Discovery and Inference towards Urban Elements and Associated Factors</title>
      <link>http://arxiv.org/abs/2503.06395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的城市因果计算框架，用于探索不同类型的城市场素之间因素的因果关系和混杂效应。通过设计一种强化学习算法来发现潜在的因果图，并利用倾向得分匹配估计两两城市场素之间的因果效果。&lt;h4&gt;背景&lt;/h4&gt;为了揭示城市的运行机制，了解市民、地点和移动行为之间的复杂关系至关重要。然而，由于普遍存在的混杂影响，经验相关性分析可能无法准确反映基本城市要素之间的因果关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来全面探索城市场素之间因素的因果关系，并减少在下游的城市预测任务中的混杂效应。&lt;h4&gt;方法&lt;/h4&gt;设计了一种强化学习算法来发现潜在的因果图，该图描述了城市的因果关系。利用倾向得分匹配估计两两城市要素之间的因果效果。&lt;h4&gt;主要发现&lt;/h4&gt;实验研究显示，发现的因果图显示出一种层次结构，即市民影响地点，并且两者都会导致城市移动行为的变化。此外，在城市预测任务中的实验结果表明提出的方法可以有效减少混杂效应并提高城市计算任务的表现。&lt;h4&gt;结论&lt;/h4&gt;提出的框架和方法为理解和改善城市的运行机制提供了新的途径，有助于提高城市规划的效率和效果。&lt;h4&gt;翻译&lt;/h4&gt;为了揭示城市的运行机制，了解市民、地点和移动行为之间的复杂关系至关重要。然而，由于普遍存在的混杂影响，经验相关性分析可能无法准确反映基本城市要素之间的因果关系。本文提出了一种新的城市因果计算框架，用于探索不同类型的城市场素之间因素的因果关系和混杂效应。通过设计一种强化学习算法来发现潜在的因果图，并利用倾向得分匹配估计两两城市要素之间的因果效果。实验研究显示，发现的因果图显示出一种层次结构，即市民影响地点，并且两者都会导致城市移动行为的变化。此外，在城市预测任务中的实验结果表明提出的方法可以有效减少混杂效应并提高城市计算任务的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To uncover the city's fundamental functioning mechanisms, it is important toacquire a deep understanding of complicated relationships among citizens,location, and mobility behaviors. Previous research studies have applied directcorrelation analysis to investigate such relationships. Nevertheless, due tothe ubiquitous confounding effects, empirical correlation analysis may notaccurately reflect underlying causal relationships among basic urban elements.In this paper, we propose a novel urban causal computing framework tocomprehensively explore causalities and confounding effects among a variety offactors across different types of urban elements. In particular, we design areinforcement learning algorithm to discover the potential causal graph, whichdepicts the causal relations between urban factors. The causal graph furtherserves as the guidance for estimating causal effects between pair-wise urbanfactors by propensity score matching. After removing the confounding effectsfrom correlations, we leverage significance levels of causal effects indownstream urban mobility prediction tasks. Experimental studies on open-sourceurban datasets show that the discovered causal graph demonstrates ahierarchical structure, where citizens affect locations, and they both causechanges in urban mobility behaviors. Experimental results in urban mobilityprediction tasks further show that the proposed method can effectively reduceconfounding effects and enhance performance of urban computing tasks.</description>
      <author>example@mail.com (Tao Feng, Yunke Zhang, Xiaochen Fan, Huandong Wang, Yong Li)</author>
      <guid isPermaLink="false">2503.06395v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Augmented Carpentry: Computer Vision-assisted Framework for Manual Fabrication</title>
      <link>http://arxiv.org/abs/2503.07473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;研究将传统的木工电动工具与一种多对象感知增强框架集成，以提高操作员在制造任务中的工作效率。&lt;h4&gt;背景&lt;/h4&gt;传统木工方法依赖于手绘图纸和标记，难以实现数字化价值链的无缝整合。为此，开发了Augmented Carpentry (AC) 开源软件，旨在利用计算机视觉工具和技术提升木材加工精度。&lt;h4&gt;目的&lt;/h4&gt;评估AC系统在识别对象和工具方面的技术挑战、潜在瓶颈以及精准度，并探讨其对木工制造流程的影响。&lt;h4&gt;方法&lt;/h4&gt;通过使用内部跟踪技术和传感器来检测改造后的工具，同时结合计算机辅助设计环境直接执行手工操作任务，以实现更加精确的切割和钻孔等工序。&lt;h4&gt;主要发现&lt;/h4&gt;实验扫描了一比一比例的模型元素，并将其与相应的三维执行模型进行对比分析，从而确定了改进点和限制。研究还讨论了AC软件在数字化木材制造中的潜在影响。&lt;h4&gt;结论&lt;/h4&gt;该技术能够将传统木工方法转化为更加精确且高效的数字流程，同时提高了操作员的工作精度及安全性。&lt;h4&gt;翻译&lt;/h4&gt;传统的电动木工工具被整合到一个多对象感知增强框架中，以帮助操作者执行制造任务。这项研究重点评估了开发的开源制造软件Augmented Carpentry (AC)，讨论了技术挑战、潜在瓶颈和系统精确度等问题。该方法使用计算机视觉工具和技术实现内部跟踪，并利用反馈提高切割和钻孔任务的精度。此外，手动工艺直接从计算机辅助设计环境中执行，使得非数字化传统木工方法得以替代。研究中详细介绍了所开发的方法及其设备与功能阶段，并通过实验扫描了一比一比例模型元素，比较了其三维执行模型以评估制造流程。最后讨论了工具感知制造过程中的改进和限制，以及AC在数字木材制造领域的潜在影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ordinary electric woodworking tools are integrated into amultiple-object-aware augmented framework to assist operators in fabricationtasks. This study presents an advanced evaluation of the developed open-sourcefabrication software Augmented Carpentry (AC), focusing on the technicalchallenges, potential bottlenecks, and precision of the proposed system, whichis designed to recognize both objects and tools. In the workflow, computervision tools and sensors implement inside-out tracking techniques for theretrofitting tools. This method enables operators to perform precisesaw-cutting and drilling tasks using computer-generated feedback. In the designand manufacturing process pipeline, manual fabrication tasks are performeddirectly from the computer-aided design environment, as computer numericalcontrol machines are widely used in the timber construction industry.Traditional non-digital methods employing execution drawings, markings, andjigs can now be replaced, and manual labor can be directly integrated into thedigital value chain. First, this paper introduces the developed methodology andexplains its devices and functional phases in detail. Second, the fabricationmethodology is evaluated by experimentally scanning the produced one-to-onescale mock-up elements and comparing the discrepancies with their respectivethree-dimensional execution models. Finally, improvements and limitations inthe tool-aware fabrication process, as well as the potential impact of AC inthe digital timber fabrication landscape, are discussed.</description>
      <author>example@mail.com (Andrea Settimi, Julien Gamerro, Yves Weinand)</author>
      <guid isPermaLink="false">2503.07473v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>DaD: Distilled Reinforcement Learning for Diverse Keypoint Detection</title>
      <link>http://arxiv.org/abs/2503.07347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种全新的无监督和不依赖于描述符的目标检测方法，通过强化学习来实现关键点的检测。&lt;h4&gt;背景&lt;/h4&gt;结构从运动(SfM)系统需要使用关键点来进行大规模图像处理。然而，设计一个适用于SfM系统的非可微分环境下的关键点检测目标是具有挑战性的任务。通常采用辅助的描述符优化方法，但这种方法会引入对描述符的依赖。&lt;h4&gt;目的&lt;/h4&gt;提出一种完全无监督且不依赖于描述符的关键点检测方案，并通过强化学习来实现。&lt;h4&gt;方法&lt;/h4&gt;使用了平衡的top-K采样策略以确保训练过程不会退化。同时发现两种类型的检测器，它们分别只能探测到亮关键点和暗关键点。为了弥补这一局限性，设计了一种新的检测器DaD，它优化了这两类检测器之间点级最大值的Kullback-Leibler散度。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在一系列基准测试中显著优于现有的最佳解决方案。&lt;h4&gt;结论&lt;/h4&gt;该方法通过消除对描述符的需求以及引入平衡采样和KL散度优化策略，极大地提升了关键点检测的质量和效率。同时，研究者开源了项目代码和模型权重，便于进一步的研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;关鍵點是使結構從運動（SfM）系統能夠擴展到數千張圖像的原因。然而，設計一個適用於SfM的關鍵點檢測目標是一個非平凡任務，因為SfM是非可微分的。通常會優化包括描述符在內的輔助目標來解決這個問題。但是這引入了對描述符的需求，這是不理想的。在本文中我們提出了一種完全自我監督且無需描述符的关键点检测目标，通过强化学习实现。为了确保训练不会退化，我们利用了一种平衡的top-K采样策略。虽然这已经可以产生具有竞争力的模型，但我们发现出现了两种不同的类型检测器，它们分别只能探测到亮关键點和暗关键點。为了解决这个问题，我们引入了第三种检测器DaD，它优化了两种类型的点级最大值之间的Kullback-Leibler散度。我们的方法在一系列基准测试中显著优于现有最佳解决方案。代码和模型权重可以在https://github.com/parskatt/dad上公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Keypoints are what enable Structure-from-Motion (SfM) systems to scale tothousands of images. However, designing a keypoint detection objective is anon-trivial task, as SfM is non-differentiable. Typically, an auxiliaryobjective involving a descriptor is optimized. This however induces adependency on the descriptor, which is undesirable. In this paper we propose afully self-supervised and descriptor-free objective for keypoint detection,through reinforcement learning. To ensure training does not degenerate, weleverage a balanced top-K sampling strategy. While this already producescompetitive models, we find that two qualitatively different types of detectorsemerge, which are only able to detect light and dark keypoints respectively. Toremedy this, we train a third detector, DaD, that optimizes theKullback-Leibler divergence of the pointwise maximum of both light and darkdetectors. Our approach significantly improve upon SotA across a range ofbenchmarks. Code and model weights are publicly available athttps:github.com/parskatt/dad</description>
      <author>example@mail.com (Johan Edstedt, Georg Bökman, Mårten Wadenbäck, Michael Felsberg)</author>
      <guid isPermaLink="false">2503.07347v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>NeuraLoc: Visual Localization in Neural Implicit Map with Dual Complementary Features</title>
      <link>http://arxiv.org/abs/2503.06117v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;一种基于神经隐式图的新颖视觉定位方法，旨在解决现有NeRF方法缺乏几何约束或需要大量存储的问题。&lt;h4&gt;背景&lt;/h4&gt;近期，神经辐射场（NeRF）在视觉定位领域引起了广泛关注。然而，现有的NeRF方法要么没有足够的几何约束条件，要么需要大量的特征匹配存储空间，这限制了其实际应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于神经隐式图的高效、新颖的视觉定位方法，以解决现有NeRF方法的问题，提高效率和性能。&lt;h4&gt;方法&lt;/h4&gt;1. 通过隐式学习3D关键点描述符场来加强几何约束并减少存储需求；2. 引入额外的语义上下文特征字段，增强二维到三维对应的质量与可靠性；3. 提出描述符相似度分布对齐技术以最小化二维和三维特征空间之间的领域差距。&lt;h4&gt;主要发现&lt;/h4&gt;1. 所提出的方法在训练速度上比现有NeRF方法快3倍，在模型存储方面减少了45倍；2. 该方法在广泛使用的数据集上的性能优于或与最先进的基于NeRF的视觉定位方法相当。&lt;h4&gt;结论&lt;/h4&gt;通过采用神经隐式图，我们不仅提高了视觉定位的速度和效率，还增强了其准确性和可靠性。实验结果证明了我们的方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;最近，神经辐射场（NeRF）在视觉定位领域获得了广泛关注。然而，现有的基于NeRF的方法要么缺乏几何约束条件，要么需要大量的特征匹配存储空间，这限制了它们的实际应用。为了解决这些挑战，我们提出了一种基于具有互补特性的神经隐式图的高效、新颖的视觉定位方法。具体而言，为了强制执行几何约束并减少存储需求，我们隐式学习了一个3D关键点描述符场，从而避免了显式地存储逐点特征的需求。此外，为了解决描述符的语义歧义问题，我们引入了额外的上下文特征字段，以增强2D-3D对应的质量和可靠性。另外，我们提出了描述符相似度分布对齐来最小化在匹配过程中二维与三维特征空间之间的领域差距。最后，我们利用互补描述符和上下文特征构建匹配图，从而建立准确的2D-3D对应关系进行6自由度姿态估计。相比于最近基于NeRF的方法，我们的方法实现了比现有方法快3倍的训练速度以及模型存储减少了45倍的优势。广泛的实验结果表明，在两个广泛使用的数据集上，我们提出的方法在性能方面要么优于其他最先进的基于NeRF的视觉定位方法，要么与其相当。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, neural radiance fields (NeRF) have gained significant attention inthe field of visual localization. However, existing NeRF-based approacheseither lack geometric constraints or require extensive storage for featurematching, limiting their practical applications. To address these challenges,we propose an efficient and novel visual localization approach based on theneural implicit map with complementary features. Specifically, to enforcegeometric constraints and reduce storage requirements, we implicitly learn a 3Dkeypoint descriptor field, avoiding the need to explicitly store point-wisefeatures. To further address the semantic ambiguity of descriptors, weintroduce additional semantic contextual feature fields, which enhance thequality and reliability of 2D-3D correspondences. Besides, we proposedescriptor similarity distribution alignment to minimize the domain gap between2D and 3D feature spaces during matching. Finally, we construct the matchinggraph using both complementary descriptors and contextual features to establishaccurate 2D-3D correspondences for 6-DoF pose estimation. Compared with therecent NeRF-based approaches, our method achieves a 3$\times$ faster trainingspeed and a 45$\times$ reduction in model storage. Extensive experiments on twowidely used datasets demonstrate that our approach outperforms or is highlycompetitive with other state-of-the-art NeRF-based visual localization methods.Project page:\href{https://zju3dv.github.io/neuraloc}{https://zju3dv.github.io/neuraloc}</description>
      <author>example@mail.com (Hongjia Zhai, Boming Zhao, Hai Li, Xiaokun Pan, Yijia He, Zhaopeng Cui, Hujun Bao, Guofeng Zhang)</author>
      <guid isPermaLink="false">2503.06117v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>CATPlan: Loss-based Collision Prediction in End-to-End Autonomous Driving</title>
      <link>http://arxiv.org/abs/2503.07425v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为CATPlan的新模块，用于估计自主驾驶系统预测轨迹的不确定性，并通过碰撞损失来监督端到端自动驾驶系统的安全性。实验结果显示，在NeuroNCAP基准测试中，与基于高斯混合模型（GMM）的方法相比，CATPlan能够检测出更多的潜在碰撞情况。&lt;h4&gt;背景&lt;/h4&gt;近年来，人们对设计、训练和评估端到端自主驾驶系统越来越感兴趣。然而，预测轨迹的不确定性通常被忽视，尽管这一方面对于实现系统的安全性和鲁棒性至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来估计自主驾驶系统中轨迹预测的不确定性，并通过碰撞风险进行监督。&lt;h4&gt;方法&lt;/h4&gt;引入了一种名为CATPlan的新模块，该模块训练以将运动和规划嵌入解码为用于部分监督端到端自动驾驶系统的碰撞损失预测。&lt;h4&gt;主要发现&lt;/h4&gt;在安全关键性的NeuroNCAP基准测试中，CATPlan能够检测出更多的潜在碰撞情况，相比基于GMM的方法，在平均精度上提高了54.8%的相对改进。&lt;h4&gt;结论&lt;/h4&gt;该研究指出了不确定性量化对于端到端自动驾驶系统的安全性的重要性，并希望这项工作能引发对这类系统中的不确定性量化的更多关注和兴趣。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, there has been increased interest in the design, training,and evaluation of end-to-end autonomous driving (AD) systems. One oftenoverlooked aspect is the uncertainty of planned trajectories predicted by thesesystems, despite awareness of their own uncertainty being key to achieve safetyand robustness. We propose to estimate this uncertainty by adapting lossprediction from the uncertainty quantification literature. To this end, weintroduce a novel light-weight module, dubbed CATPlan, that is trained todecode motion and planning embeddings into estimates of the collision loss usedto partially supervise end-to-end AD systems. During inference, these estimatesare interpreted as collision risk. We evaluate CATPlan on the safety-critical,nerf-based, closed-loop benchmark NeuroNCAP and find that it manages to detectcollisions with a $54.8\%$ relative improvement to average precision over aGMM-based baseline in which the predicted trajectory is compared to theforecasted trajectories of other road users. Our findings indicate that theaddition of CATPlan can lead to safer end-to-end AD systems and hope that ourwork will spark increased interest in uncertainty quantification for suchsystems.</description>
      <author>example@mail.com (Ziliang Xiong, Shipeng Liu, Nathaniel Helgesen, Joakim Johnander, Per-Erik Forssen)</author>
      <guid isPermaLink="false">2503.07425v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Advances in Hybrid Modular Climbing Robots: Design Principles and Refinement Strategies</title>
      <link>http://arxiv.org/abs/2503.07423v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 9 figures; This work has been submitted to the IEEE for  possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了混合式攀爬机器人的设计策略，重点在于为机器人开发设计决策和评估性能的方法。一个集成了轮子与抓取臂的混合机器人被研发出来用于在不同直径的柱状结构上进行攀爬。&lt;h4&gt;背景&lt;/h4&gt;当前对能够适应多种柱形结构的攀爬机器人的需求日益增加，这推动了对于更具灵活性和效率的设计方案的研究。&lt;h4&gt;目的&lt;/h4&gt;为了提高工业应用中自动攀爬机器人系统的可靠性和功能性，本研究旨在开发一种全新的混合式轮子抓取机器人，并通过实验验证其性能。&lt;h4&gt;方法&lt;/h4&gt;设计了一种配备有模块化、肌腱驱动的抓取臂和旋转炮塔上的车轮驱动系统的机器人。该机器人可以调整其抓取手臂以适应不同直径的柱形结构，具备自锁能力（即依靠摩擦力保持在柱上）、自主抓握能力和绕轴旋转的能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所开发的混合式攀爬机器人的自锁功能和垂直攀爬性能非常有效，验证了提出的数学模型，并展示了在工业环境中实现全自动解决方案的巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了一个全面的设计与评估框架，为未来研究和应用奠定了基础，特别是在需要攀爬高大结构的重要领域中。该框架对于自主机器人技术的进步有着重要的贡献。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文的中文翻译：本文探讨了混合式柱状或树干攀爬机器人的设计策略，重点在于提供设计决策的方法以及评估适应性和性能的标准。开发了一种集成了轮子与抓取臂的混合型机器人，该机器人具有模块化、肌腱驱动的抓取臂和安装在旋转炮塔上的车轮驱动系统，可以沿着不同直径的柱体进行攀爬。其中的关键创新在于其下位控制手臂，可以通过添加或移除模块化的连杆来调整以适应不同的柱状尺寸。此外，该机器人还具备自锁能力（即依靠摩擦力保持在柱上）、自主抓握能力和绕轴旋转的能力。数学模型描述了自锁和垂直攀爬的条件。实验结果证明了机器人的有效性和稳定性，并验证了所提出的理论模型，强调了在工业应用中实现全自动解决方案的巨大潜力。这项工作为评估与设计混合式攀爬机器人提供了一个全面框架，有助于自主机器人技术的进步，在需要攀爬高大结构的关键环境中尤为重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper explores the design strategies for hybrid pole- or trunk-climbingrobots, focusing on methods to inform design decisions and assess metrics suchas adaptability and performance. A wheeled-grasping hybrid robot with modular,tendon-driven grasping arms and a wheeled drive system mounted on a turret wasdeveloped to climb columns of varying diameters. Here, the key innovation isthe underactuated arms that can be adjusted to different column sizes by addingor removing modular linkages, though the robot also features capabilities likeself-locking (the ability of the robot to stay on the column by frictionwithout power), autonomous grasping, and rotation around the column axis.Mathematical models describe conditions for self-locking and vertical climbing.Experimental results demonstrate the robot's efficacy in climbing andself-locking, validating the proposed models and highlighting the potential forfully automated solutions in industrial applications. This work provides acomprehensive framework for evaluating and designing hybrid climbing robots,contributing to advancements in autonomous robotics for environments whereclimbing tall structures is critical.</description>
      <author>example@mail.com (Ryan Poon, Ian Hunter)</author>
      <guid isPermaLink="false">2503.07423v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>AxisPose: Model-Free Matching-Free Single-Shot 6D Object Pose Estimation via Axis Generation</title>
      <link>http://arxiv.org/abs/2503.06660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AxisPose是一种无需模型、不需要匹配的单次拍摄6D姿态估计解决方案，它直接从单一视角通过扩散模型学习物体轴向分布来推断稳健的6D姿态。&lt;h4&gt;背景&lt;/h4&gt;现有的研究方法大多依赖于多阶段的姿态回归或基于2D-3D特征匹配，这些方法虽有成效但过于复杂且高度依赖外观信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的、不依赖于参考视图和几何一致性的单一视角的6D姿态估计方案。&lt;h4&gt;方法&lt;/h4&gt;AxisPose使用扩散模型构建轴生成模块（AGM）来捕捉物体轴向分布，通过三轴后投影模块（TBM）从单张图像中恢复出对象的6D姿态。&lt;h4&gt;主要发现&lt;/h4&gt;提出的AxisPose可以在无参考视图输入的情况下实现跨实例级别的稳健性能，并具有推广到未知对象级别的潜力。&lt;h4&gt;结论&lt;/h4&gt;AxisPose为物体姿态估计提供了一种全新的思路，突破了现有的研究范式。&lt;h4&gt;翻译&lt;/h4&gt;目标姿势估计，在机器人技术、增强现实和自动驾驶中发挥着重要作用，一直是计算机视觉领域的研究热点。现有的研究要么需要多阶段的姿态回归，要么依赖于2D-3D特征匹配。尽管这些方法显示出令人鼓舞的结果，但它们过于依赖外观信息，需要复杂的输入（例如，多视角参考输入、深度或CAD模型）和繁琐的流程（例如，特征提取-SfM-2D到3D匹配-PnP）。我们提出了一种无需模型、不需要匹配的单次拍摄6D姿态估计方案AxisPose。不同于现有方法依赖于使用SfM和PnP等技术进行2D-3D或2D-2D匹配的方法，AxisPose直接从单一视图中利用扩散模型学习物体轴向分布来推断稳健的6D姿态。具体来说，AxisPose构建了一个轴生成模块（AGM），该模块通过一个扩散模型捕捉对象轴的潜在几何分布，并通过将一致性损失的梯度注入噪声估计中维持生成三轴的一致性。使用生成的三轴投影，AxisPose进一步采用三轴后投影模块（TBM）从物体三轴恢复6D姿态。提出的AxisPose仅需单视图输入即可实现跨实例级别的稳健性能，并具有推广到未知对象级别的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object pose estimation, which plays a vital role in robotics, augmentedreality, and autonomous driving, has been of great interest in computer vision.Existing studies either require multi-stage pose regression or rely on 2D-3Dfeature matching. Though these approaches have shown promising results, theyrely heavily on appearance information, requiring complex input (i.e.,multi-view reference input, depth, or CAD models) and intricate pipeline (i.e.,feature extraction-SfM-2D to 3D matching-PnP). We propose AxisPose, amodel-free, matching-free, single-shot solution for robust 6D pose estimation,which fundamentally diverges from the existing paradigm. Unlike existingmethods that rely on 2D-3D or 2D-2D matching using 3D techniques, such as SfMand PnP, AxisPose directly infers a robust 6D pose from a single view byleveraging a diffusion model to learn the latent axis distribution of objectswithout reference views. Specifically, AxisPose constructs an Axis GenerationModule (AGM) to capture the latent geometric distribution of object axesthrough a diffusion model. The diffusion process is guided by injecting thegradient of geometric consistency loss into the noise estimation to maintainthe geometric consistency of the generated tri-axis. With the generatedtri-axis projection, AxisPose further adopts a Triaxial Back-projection Module(TBM) to recover the 6D pose from the object tri-axis. The proposed AxisPoseachieves robust performance at the cross-instance level (i.e., one model for Ninstances) using only a single view as input without reference images, withgreat potential for generalization to unseen-object level.</description>
      <author>example@mail.com (Yang Zou, Zhaoshuai Qi, Yating Liu, Zihao Xu, Weipeng Sun, Weiyi Liu, Xingyuan Li, Jiaqi Yang, Yanning Zhang)</author>
      <guid isPermaLink="false">2503.06660v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>PER-DPP Sampling Framework and Its Application in Path Planning</title>
      <link>http://arxiv.org/abs/2503.07411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该论文探讨了智能移动系统自主导航中的路径规划方法，并提出了一种新的采样框架，结合优先级经验回放和确定性点过程，以提高决策优化能力。&lt;h4&gt;背景&lt;/h4&gt;当前的路径规划方法在动态环境响应和多目标任务扩展方面存在局限性。基于决策的强化学习框架因其适应环境交互和自我优化的能力而备受关注。&lt;h4&gt;目的&lt;/h4&gt;提出改进强化学习经验回放机制的方法，解决样本同质化问题，并开发一个新的采样框架来增强决策优化。&lt;h4&gt;方法&lt;/h4&gt;开发了一种混合采样范式（PER-DPP），结合优先级序列与多样性最大化。在此基础上创建了一个集成优化方案（PER-DPP-Elastic DQN），将多样性感知采样与自适应步长调节相结合。&lt;h4&gt;主要发现&lt;/h4&gt;合成的方法生成的导航路径在长度效率和方向稳定性方面进行了优化，尽管弹性步长组件会暂时延迟初始收敛速度，但最终阶段的优化能力得到了显著提高。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了改进的经验回放机制，并展示了其在增强智能移动系统自主导航性能方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous navigation in intelligent mobile systems represents a coreresearch focus within artificial intelligence-driven robotics. Contemporarypath planning approaches face constraints in dynamic environmentalresponsiveness and multi-objective task scalability, limiting their capacity toaddress growing intelligent operation requirements. Decision-centricreinforcement learning frameworks, capitalizing on their unique strengths inadaptive environmental interaction and self-optimization, have gainedprominence in advanced control system research. This investigation introducesmethodological improvements to address sample homogeneity challenges inreinforcement learning experience replay mechanisms. By incorporatingdeterminant point processes (DPP) for diversity assessment, we develop adual-criteria sampling framework with adaptive selection protocols. Thisapproach resolves representation bias in conventional prioritized experiencereplay (PER) systems while preserving algorithmic interoperability, offeringimproved decision optimization for dynamic operational scenarios. Keycontributions comprise: Develop a hybrid sampling paradigm (PER-DPP) combiningpriority sequencing with diversity maximization.Based on this,create anintegrated optimization scheme (PER-DPP-Elastic DQN) merging diversity-awaresampling with adaptive step-size regulation. Comparative simulations in 2Dnavigation scenarios demonstrate that the elastic step-size componenttemporarily delays initial convergence speed but synergistically enhancesfinal-stage optimization with PER-DPP integration. The synthesized methodgenerates navigation paths with optimized length efficiency and directionalstability.</description>
      <author>example@mail.com (Junzhe Wang)</author>
      <guid isPermaLink="false">2503.07411v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>AttentionSwarm: Reinforcement Learning with Attention Control Barier Function for Crazyflie Drones in Dynamic Environments</title>
      <link>http://arxiv.org/abs/2503.07376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了AttentionSwarm，这是一个用于评估安全和高效群集控制的新基准。&lt;h4&gt;背景&lt;/h4&gt;当前环境下需要一个能够实时避碰和轨迹优化的系统，在动态环境中确保安全性与速度。&lt;h4&gt;目的&lt;/h4&gt;提出了一种基于注意力模型的安全屏障函数框架，并通过实际实验验证其在不同环境中的有效性。&lt;h4&gt;方法&lt;/h4&gt;开发了Attention Model Based Control Barrier Function (CBF) 框架，利用注意力机制和安全关键控制理论进行实时碰撞避免和轨迹优化。&lt;h4&gt;主要发现&lt;/h4&gt;系统在动态着陆环境中实现了3.02厘米的平均着陆精度和23秒的时间；无人机游戏环境中的完全无碰撞导航成功率为100%；多代理无人机比赛环境下的无碰撞导航成功率达到了95%。&lt;h4&gt;结论&lt;/h4&gt;该工作为未来动态环境下的应用提供了坚实的基础，尤其是在安全性与速度都极为重要的场景中。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了AttentionSwarm，这是一个用于评估安全和高效群集控制的新基准。它在三个具有挑战性的环境中进行测试：带有障碍物的着陆环境、竞争性无人机游戏设置以及动态无人机比赛场景。本方法的核心是基于注意力模型的安全屏障函数（CBF）框架，该框架结合了注意机制与关键安全性控制理论，以实现实时避碰和轨迹优化。通过使用Crazyflie 2.1微型四旋翼机的群集并在室内配备了Vicon运动捕捉系统的环境中进行测试，证明了其在实际环境中的有效性和鲁棒性。实验结果表明，在动态着陆环境下实现了3.02厘米的平均着陆精度和碰撞避免；无人机游戏环境中完全无碰撞导航成功率为100%；多代理无人机比赛环境下的无碰撞导航成功率达到了95%，这说明了它在未来动态环境应用中的巨大潜力，尤其是在需要同时保证安全性和速度的关键场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce AttentionSwarm, a novel benchmark designed to evaluate safe andefficient swarm control across three challenging environments: a landingenvironment with obstacles, a competitive drone game setting, and a dynamicdrone racing scenario. Central to our approach is the Attention Model BasedControl Barrier Function (CBF) framework, which integrates attention mechanismswith safety-critical control theory to enable real-time collision avoidance andtrajectory optimization. This framework dynamically prioritizes criticalobstacles and agents in the swarms vicinity using attention weights, while CBFsformally guarantee safety by enforcing collision-free constraints. The safeattention net algorithm was developed and evaluated using a swarm of Crazyflie2.1 micro quadrotors, which were tested indoors with the Vicon motion capturesystem to ensure precise localization and control. Experimental results showthat our system achieves landing accuracy of 3.02 cm with a mean time of 23 sand collision-free landings in a dynamic landing environment, 100% andcollision-free navigation in a drone game environment, and 95% andcollision-free navigation for a dynamic multiagent drone racing environment,underscoring its effectiveness and robustness in real-world scenarios. Thiswork offers a promising foundation for applications in dynamic environmentswhere safety and fastness are paramount.</description>
      <author>example@mail.com (Grik Tadevosyan, Valerii Serpiva, Aleksey Fedoseev, Roohan Ahmed Khan, Demetros Aschu, Faryal Batool, Nickolay Efanov, Artem Mikhaylov, Dzmitry Tsetserukou)</author>
      <guid isPermaLink="false">2503.07376v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>AffordDexGrasp: Open-set Language-guided Dexterous Grasp with Generalizable-Instructive Affordance</title>
      <link>http://arxiv.org/abs/2503.07360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了一种新的任务——开放集语言引导灵巧抓握，提出了一个基于功能（Affordance）的灵巧抓握框架（AffordDexGrasp），该框架通过引入通用和指导性的功能表示来解决人类语言语义与机器人动作之间的差距问题。&lt;h4&gt;背景&lt;/h4&gt;现有的数据驱动方法难以理解人的意图，并且在开放集环境中无法对未见过的对象进行有效抓取。&lt;h4&gt;目的&lt;/h4&gt;探索并实现一种新的任务：开放集语言引导灵巧抓握，通过引入功能表示来解决人类指令与机器人动作之间的差距问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一个基于功能的灵巧抓握框架（AffordDexGrasp），该框架包含两个主要部分：功能流匹配（AFM）用于从语言输入生成功能表示；抓取流匹配（GFM）用于根据功能表示生成精确的灵巧抓握。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架在开放集环境中具有卓越的表现，能够有效利用物体局部结构和类别无关的语义属性来推广至未见过的对象类别，并引导产生有效的灵巧抓握动作。&lt;h4&gt;结论&lt;/h4&gt;基于新提出的功能表示的方法AffordDexGrasp显著优于现有的方法，在处理开放式环境中的未知对象时表现更优。&lt;h4&gt;翻译&lt;/h4&gt;语言指导的机器人灵巧抓取使机器人能够根据人类指令抓住并操作物体。然而，先前的数据驱动方法难以理解意图，并且在开放集环境中无法对未见过的对象进行有效抓取。本文探讨了一种新的任务——开放集语言引导灵巧抓握，并发现主要挑战在于高层次的人类语言语义与低层次的机器人动作之间的巨大差距。为了解决这个问题，我们提出了一种基于功能的灵巧抓握框架（AffordDexGrasp），通过引入一种新的一般化和指导性的功能表示来填补这一差距。这种功能可以通过利用对象的局部结构和类别无关语义属性推广到未见过的对象类别，并有效引导产生有效的灵巧抓取动作。在该框架的基础上，我们提出了基于功能流匹配（AFM）的方法用于从语言输入生成功能表示；以及抓取流匹配（GFM），用于根据功能表示生成精确的灵巧抓握。为了评估我们的框架，我们建立了一个开放集桌面语言引导灵巧抓握的数据集。模拟和现实世界的广泛实验表明，我们的框架在开放式泛化方面超越了所有先前的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Language-guided robot dexterous generation enables robots to grasp andmanipulate objects based on human commands. However, previous data-drivenmethods are hard to understand intention and execute grasping with unseencategories in the open set. In this work, we explore a new task, Open-setLanguage-guided Dexterous Grasp, and find that the main challenge is the hugegap between high-level human language semantics and low-level robot actions. Tosolve this problem, we propose an Affordance Dexterous Grasp (AffordDexGrasp)framework, with the insight of bridging the gap with a newgeneralizable-instructive affordance representation. This affordance cangeneralize to unseen categories by leveraging the object's local structure andcategory-agnostic semantic attributes, thereby effectively guiding dexterousgrasp generation. Built upon the affordance, our framework introducesAffordacne Flow Matching (AFM) for affordance generation with language asinput, and Grasp Flow Matching (GFM) for generating dexterous grasp withaffordance as input. To evaluate our framework, we build an open-set table-toplanguage-guided dexterous grasp dataset. Extensive experiments in thesimulation and real worlds show that our framework surpasses all previousmethods in open-set generalization.</description>
      <author>example@mail.com (Yi-Lin Wei, Mu Lin, Yuhao Lin, Jian-Jian Jiang, Xiao-Ming Wu, Ling-An Zeng, Wei-Shi Zheng)</author>
      <guid isPermaLink="false">2503.07360v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Certifiably Optimal Anisotropic Rotation Averaging</title>
      <link>http://arxiv.org/abs/2503.07353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;旋转平均问题是计算机视觉和机器人技术应用中的关键子问题。许多方法用于解决这个问题，并且也有一些理论结果分析了该问题的难度和最优性。&lt;h4&gt;局限性&lt;/h4&gt;大多数现有方法集中于各向同性的设置，未能充分考虑测量内在不确定性的影响。&lt;h4&gt;新框架&lt;/h4&gt;最近的经验研究表明，在包含这些不确定性的各向异性框架中工作可以提高解决方案的质量。&lt;h4&gt;挑战&lt;/h4&gt;在这一场景下进行全局优化仍然是一项挑战。&lt;h4&gt;贡献&lt;/h4&gt;本文展示了如何将各向异性的成本纳入确证最优的旋转平均方法。还演示了现有为各向同性情况设计的解算器在这种情况下失败的原因。&lt;h4&gt;提出的方法&lt;/h4&gt;提出了一个更强大的松弛，并通过实验证明它能够在所有测试的数据集中恢复全局最优点，并且除了一个场景外，都可以获得更加精确的重建结果。&lt;h4&gt;结论&lt;/h4&gt;这项工作解决了在各向异性设置中进行旋转平均时遇到的一个重要挑战，为计算机视觉和机器人技术中的应用提供了潜在的实际价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rotation averaging is a key subproblem in applications of computer vision androbotics. Many methods for solving this problem exist, and there are alsoseveral theoretical results analyzing difficulty and optimality. However, oneaspect that most of these have in common is a focus on the isotropic setting,where the intrinsic uncertainties in the measurements are not fullyincorporated into the resulting optimization task. Recent empirical resultssuggest that moving to an anisotropic framework, where these uncertainties areexplicitly included, can result in an improvement of solution quality. However,global optimization for rotation averaging has remained a challenge in thisscenario. In this paper we show how anisotropic costs can be incorporated incertifiably optimal rotation averaging. We also demonstrate how existingsolvers, designed for isotropic situations, fail in the anisotropic setting.Finally, we propose a stronger relaxation and show empirically that it is ableto recover global optima in all tested datasets and leads to a more accuratereconstruction in all but one of the scenes.</description>
      <author>example@mail.com (Carl Olsson, Yaroslava Lochman, Johan Malmport, Christopher Zach)</author>
      <guid isPermaLink="false">2503.07353v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge representation and scalable abstract reasoning for simulated democracy in Unity</title>
      <link>http://arxiv.org/abs/2503.05783v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 11 figures, 76 references. This article is under review at  WSEAS Transactions on Information Science and Applications from 02.2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新型的可扩展知识表示形式，用于模拟民主社会中的代理（e-polis），其中真实用户响应与民主制度相关联的社会挑战。&lt;h4&gt;背景&lt;/h4&gt;研究提出一种新的类型——智能空间类型（Smart Spatial Types）,这是一种根据访问者的哲学教义改变建筑形态的智能建筑。&lt;h4&gt;目的&lt;/h4&gt;通过将民主模型与智慧城市模型相结合，证明模拟民主在不同城市和社会环境下的质量属性，并且增加开发过程中的便利性和灵活性。&lt;h4&gt;方法&lt;/h4&gt;该系统使用演绎系统以一种独特的方式工作，能够从抽象知识中推理和做出决策，并实现基于玩家抽象状态的实时决定和游戏流程调整。&lt;h4&gt;主要发现&lt;/h4&gt;通过维护一个双层知识表示机制来推理关于模拟民主的内容，该机制类似于两级缓存的工作方式。下层了解当前游戏的状态，而上层则了解易于检索、用户定义的当前和历史状态的抽象知识。&lt;h4&gt;结论&lt;/h4&gt;该方法不仅提高了开发过程中的灵活性和便利性，还使基于玩家行为的实时决策成为可能，并为解释性奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种关于模拟民主社会中代理的新颖可扩展的知识表示形式。其中真实用户回应的社会挑战与智能空间类型相关联，这是一种根据访问者哲学教义改变其建筑形态的新型智能建筑。研究通过将民主模型与智慧城市模型相结合来证明不同城市和社会环境下的质量属性，并通过抽象知识实现决策和推理。此系统支持基于玩家状态进行实时调整的能力，为解释性铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel form of scalable knowledge representation about agents ina simulated democracy, e-polis, where real users respond to social challengesassociated with democratic institutions, structured as Smart Spatial Types, anew type of Smart Building that changes architectural form according to thephilosophical doctrine of a visitor. At the end of the game players vote on theSmart City that results from their collective choices. Our approach usesdeductive systems in an unusual way: by integrating a model of democracy with amodel of a Smart City we are able to prove quality aspects of the simulateddemocracy in different urban and social settings, while adding ease andflexibility to the development. Second, we can infer and reason with abstractknowledge, which is a limitation of the Unity platform; third, our systemenables real-time decision-making and adaptation of the game flow based on theplayer's abstract state, paving the road to explainability. Scalability isachieved by maintaining a dual-layer knowledge representation mechanism forreasoning about the simulated democracy that functions in a similar way to atwo-level cache. The lower layer knows about the current state of the game bycontinually processing a high rate of events produced by the in-built physicsengine of the Unity platform, e.g., it knows of the position of a player inspace, in terms of his coordinates x,y,z as well as their choices for eachchallenge. The higher layer knows of easily-retrievable, user-defined abstractknowledge about current and historical states, e.g., it knows of the politicaldoctrine of a Smart Spatial Type, a player's philosophical doctrine, and thecollective philosophical doctrine of a community players with respect tocurrent social issues.</description>
      <author>example@mail.com (Eleftheria Katsiri, Alexandros Gazis, Angelos Protopapas)</author>
      <guid isPermaLink="false">2503.05783v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Research and Design on Intelligent Recognition of Unordered Targets for Robots Based on Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2503.07340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了基于强化学习的人工智能驱动的智能机器人紊乱目标识别方法，通过图像处理和深度学习模型训练提高目标识别效率与准确性。&lt;h4&gt;背景&lt;/h4&gt;目前的目标识别研究受限于目标分布无序、环境复杂多变以及数据规模庞大等问题。现有AI技术在持续迭代中寻求解决方案。&lt;h4&gt;目的&lt;/h4&gt;为满足智能机器人在复杂可变场景下准确识别紊乱目标的需求，本文提出了一种基于强化学习的创新性方法。&lt;h4&gt;方法&lt;/h4&gt;采用双边滤波算法处理采集到的目标图像，将其分解为低光照图和反射图。利用差异化的人工智能策略对这两部分进行处理，并融合生成新的高质量目标图像。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，提出的方法不仅显著提升了目标图像的质量，还使基于人工智能的智能机器人能更高效地完成紊乱目标识别任务。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了极高应用价值和发展前景，在AI机器人领域具有广泛的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;在人工智能驱动的目标识别研究中，面对无序分布的目标、复杂环境等因素的影响，本文创新性提出了一种基于强化学习的智能机器人紊乱目标识别方法。通过图像预处理和深度强化学习模型训练，显著提高了目标识别效率与准确性，展现了极高的应用价值和发展潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the field of robot target recognition research driven by artificialintelligence (AI), factors such as the disordered distribution of targets, thecomplexity of the environment, the massive scale of data, and noiseinterference have significantly restricted the improvement of targetrecognition accuracy. Against the backdrop of the continuous iteration andupgrading of current AI technologies, to meet the demand for accuraterecognition of disordered targets by intelligent robots in complex andchangeable scenarios, this study innovatively proposes an AI - basedintelligent robot disordered target recognition method using reinforcementlearning. This method processes the collected target images with the bilateralfiltering algorithm, decomposing them into low - illumination images andreflection images. Subsequently, it adopts differentiated AI strategies,compressing the illumination images and enhancing the reflection imagesrespectively, and then fuses the two parts of images to generate a new image.On this basis, this study deeply integrates deep learning, a core AItechnology, with the reinforcement learning algorithm. The enhanced targetimages are input into a deep reinforcement learning model for training,ultimately enabling the AI - based intelligent robot to efficiently recognizedisordered targets. Experimental results show that the proposed method can notonly significantly improve the quality of target images but also enable the AI- based intelligent robot to complete the recognition task of disorderedtargets with higher efficiency and accuracy, demonstrating extremely highapplication value and broad development prospects in the field of AI robots.</description>
      <author>example@mail.com (Yiting Mao, Dajun Tao, Shengyuan Zhang, Tian Qi, Keqin Li)</author>
      <guid isPermaLink="false">2503.07340v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Triplane Transformers as Occupancy World Models</title>
      <link>http://arxiv.org/abs/2503.07338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的4D占用世界模型T$^3$Former，用于自主驾驶中的实时预测。&lt;h4&gt;背景&lt;/h4&gt;近年来，在世界模型领域取得了显著进展，主要集中在学习代理的运动轨迹与其周围环境变化之间的细粒度关联上。然而，现有的方法往往难以捕捉这些细粒度的相关性并实现实时预测。&lt;h4&gt;目的&lt;/h4&gt;为了改进现有方法在捕捉细粒度相关性和实现实时预测方面的不足，本文提出了一种新的4D占用世界模型T$^3$Former。&lt;h4&gt;方法&lt;/h4&gt;T$^3$Former首先通过预训练一个紧凑的三平面表示来高效压缩三维语义占有的环境。接下来，它从历史三平面上提取多尺度时间运动特征，并采用自回归的方法迭代预测下一个三平面的变化。最后，将这些变化与之前的三平面结合，解码成未来占用结果和自我运动轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示T$^3$Former在推理速度方面实现了1.44倍的提升（达到26帧/秒），同时将平均交并比提高到36.09，并将平均绝对规划误差减少至1.0米。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了其优越性，不仅提高了预测精度，还显著提升了推理速度。&lt;h4&gt;翻译&lt;/h4&gt;最近几年，在世界模型领域取得了重大进展，这些模型主要关注于学习代理的运动轨迹与其周围环境变化之间细粒度的相关性。然而，现有的方法往往难以捕捉这些相关性和实现实时预测。为了解决这个问题，我们提出了一种用于自主驾驶的新4D占用世界模型T$^3$Former。T$^3$Former通过预训练一个紧凑的三平面表示来开始工作，有效地压缩了三维语义占有的环境。接下来，它从历史三平面上提取多尺度的时间运动特征，并采用自回归的方法迭代预测下一个三平面的变化。最后，将这些变化与之前的三平面结合解码为未来占用结果和自我运动轨迹。实验结果显示T$^3$Former实现了1.44倍的推理速度提升（达到26帧/秒），同时提高了平均交并比到36.09，并且降低了平均绝对规划误差至1.0米。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent years have seen significant advances in world models, which primarilyfocus on learning fine-grained correlations between an agent's motiontrajectory and the resulting changes in its surrounding environment. However,existing methods often struggle to capture such fine-grained correlations andachieve real-time predictions. To address this, we propose a new 4D occupancyworld model for autonomous driving, termed T$^3$Former. T$^3$Former begins bypre-training a compact triplane representation that efficiently compresses the3D semantically occupied environment. Next, T$^3$Former extracts multi-scaletemporal motion features from the historical triplane and employs anautoregressive approach to iteratively predict the next triplane changes.Finally, T$^3$Former combines the triplane changes with the previous ones todecode them into future occupancy results and ego-motion trajectories.Experimental results demonstrate the superiority of T$^3$Former, achieving1.44$\times$ faster inference speed (26 FPS), while improving the mean IoU to36.09 and reducing the mean absolute planning error to 1.0 meters.</description>
      <author>example@mail.com (Haoran Xu, Peixi Peng, Guang Tan, Yiqian Chang, Yisen Zhao, Yonghong Tian)</author>
      <guid isPermaLink="false">2503.07338v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Path Navigation for Motion Agents with LLM Reasoning</title>
      <link>http://arxiv.org/abs/2503.07323v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型语言模型（LLMs）在一般化推理和规划方面表现出强大的能力，但其在空间路径规划和无碰撞轨迹生成方面的有效性尚未被充分探索。本研究是首次尝试利用LLM进行导航的实验之一。&lt;h4&gt;背景&lt;/h4&gt;尽管LLMs展示了广泛的任务处理能力和交互支持用户的能力，但在特定领域的应用如空间路径规划和障碍物自由轨迹生成方面仍缺乏深入研究。&lt;h4&gt;目的&lt;/h4&gt;构建数据集并提出评估协议以探索LLMs在零样本导航和路径生成方面的潜力。&lt;h4&gt;方法&lt;/h4&gt;利用锚点连接的直线表示路径，这种方法提供了比传统方法更大的灵活性和实用性，同时保持了直观性。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明当任务结构清晰时，现代LLM表现出强大的规划能力，能够自主优化其运动以避免障碍物并到达目标。此外，单个LLM动机构建的空间推理能力可以无缝地推广到多机动态环境中的协调。&lt;h4&gt;结论&lt;/h4&gt;不同于依赖单一步骤规划或局部策略的传统方法，基于训练的自由LLM方法实现了全局、动态和闭环计划，并且能够自主解决碰撞问题。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）已经展示出强大的通用推理和规划能力。然而，在空间路径规划和无碰撞轨迹生成方面的有效性仍未被充分探索。利用LLMs进行导航具有巨大潜力，因为它们能够处理未见场景，支持用户代理交互，并在全球范围内提供复杂系统的控制，使得它们非常适合于代理规划和人形运动生成。作为这一领域中的首次研究之一，我们通过构建数据集并提出评估协议来探究零样本导航和路径生成能力的LLMs。特别是，我们将路径表示为由直线连接的锚点，这比以前的方法提供了更大的灵活性和实用性，同时保持简单直观。我们展示了当任务以这种方式良好地结构化时，现代LLM表现出强大的规划能力，能够避免障碍物的同时自主优化其导航并到达目标。此外，在静态环境中单个LLM动机构建的空间推理能力可以无缝推广到多机动态环境中的协调。不同于依赖单一步骤规划或局部策略的传统方法，我们的训练自由LLM方法实现了全局、动态和闭环计划，并且能够自主解决碰撞问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have demonstrated strong generalizable reasoningand planning capabilities. However, their efficacies in spatial path planningand obstacle-free trajectory generation remain underexplored. Leveraging LLMsfor navigation holds significant potential, given LLMs' ability to handleunseen scenarios, support user-agent interactions, and provide global controlacross complex systems, making them well-suited for agentic planning andhumanoid motion generation. As one of the first studies in this domain, weexplore the zero-shot navigation and path generation capabilities of LLMs byconstructing a dataset and proposing an evaluation protocol. Specifically, werepresent paths using anchor points connected by straight lines, enablingmovement in various directions. This approach offers greater flexibility andpracticality compared to previous methods while remaining simple and intuitivefor LLMs. We demonstrate that, when tasks are well-structured in this manner,modern LLMs exhibit substantial planning proficiency in avoiding obstacleswhile autonomously refining navigation with the generated motion to reach thetarget. Further, this spatial reasoning ability of a single LLM motion agentinteracting in a static environment can be seamlessly generalized inmulti-motion agents coordination in dynamic environments. Unlike traditionalapproaches that rely on single-step planning or local policies, ourtraining-free LLM-based method enables global, dynamic, closed-loop planning,and autonomously resolving collision issues.</description>
      <author>example@mail.com (Yubo Zhao, Qi Wu, Yifan Wang, Yu-Wing Tai, Chi-Keung Tang)</author>
      <guid isPermaLink="false">2503.07323v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>A Decapod Robot with Rotary Bellows-Enclosed Soft Transmissions</title>
      <link>http://arxiv.org/abs/2503.07321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 11 figures, accepted by RoboSoft 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;软体爬行机器人在各种地形上表现出高效的运动，并对不同的环境条件展现出强大的适应性。&lt;h4&gt;背景&lt;/h4&gt;软体爬行机器人因其在复杂地形和环境中高效、灵活的移动能力而受到关注。传统的软腿机器人使用阀门来控制气流，但这种方法增加了系统的复杂性和重量。&lt;h4&gt;目的&lt;/h4&gt;提出一种无阀设计的软腿机器人，该机器人集成了一对旋转双层气动传输系统（R-BESTS），旨在简化机械结构并提高效率。&lt;h4&gt;方法&lt;/h4&gt;开发了包含一对旋转式双层气动传输系统的软体机器人。这些系统能够直接将伺服电机的转动转换为腿部摆动运动。通过定时皮带控制两个R-BESTS以相反相位同步旋转，实现交替的三足步态行走和转向。&lt;h4&gt;主要发现&lt;/h4&gt;研究了几种不同的设计来探索强化骨架在扭转输入气囊单元中的作用，并通过结构设计输出气囊单元来控制机器人腿部的弯曲序列。最终演示了一款软体十足机器人的无绳移动能力，实验结果表明该机器人可以在90分钟内以每秒1.75厘米（相当于身体长度的0.07倍）的速度行走；可以转过半径为15厘米（约为自身长度的0.6倍）的弯道；能够携带200克的有效载荷，适应不同的地形。&lt;h4&gt;结论&lt;/h4&gt;所提出的无阀软腿机器人设计简化了机械结构，并提高了机器人的效率和灵活性。实验结果表明这种新型机器人具有良好的行走能力、转向能力和负载能力，在多种环境中表现出强大的适应性。&lt;h4&gt;翻译&lt;/h4&gt;软体爬行机器人在各种地形上高效移动并能应对不同环境条件的挑战。本文提出了一种无阀设计的软腿机器人，该机器人通过一对旋转式双层气动传输系统（R-BESTS）集成。这些系统可以直接将伺服电机的转动转化为腿部摆动运动，并使用定时皮带控制两个R-BESTS在相反相位同步旋转以实现交替三足步态行走和转向。研究了几种设计来理解强化骨架对扭转输入气囊单元的作用，同时通过结构设计控制输出气囊单元的弯曲序列。最后展示了一款软体十足机器人的无绳移动能力。实验结果表明该机器人能够以每秒1.75厘米的速度持续90分钟行走；转过半径为15厘米的弯道；携带200克的有效载荷，并适应不同的地形环境。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Soft crawling robots exhibit efficient locomotion across various terrains anddemonstrate robustness to diverse environmental conditions. Here, we propose avalveless soft-legged robot that integrates a pair of rotary bellows-enclosedsoft transmission systems (R-BESTS). The proposed R-BESTS can directly transmitthe servo rotation into leg swing motion. A timing belt controls the pair ofR-BESTS to maintain synchronous rotation in opposite phases, realizingalternating tripod gaits of walking and turning. We explored several designs tounderstand the role of a reinforcement skeleton in twisting the R-BESTS' inputbellows units. The bending sequences of the robot legs are controlled throughstructural design for the output bellows units. Finally, we demonstrateuntethered locomotion with the soft robotic decapod. Experimental results showthat our robot can walk at 1.75 centimeters per second (0.07 body length persecond) for 90 min, turn with a 15-centimeter (0.6 BL) radius, carry a payloadof 200 g, and adapt to different terrains.</description>
      <author>example@mail.com (Yiming He, Yuchen Wang, Yunjia Zhang, Shuguang Li)</author>
      <guid isPermaLink="false">2503.07321v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Human Machine Co-Adaptation Model and Its Convergence Analysis</title>
      <link>http://arxiv.org/abs/2503.07319v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于合作自适应马尔可夫决策过程（CAMDPs）的新方法，旨在优化机器人辅助康复中的人机交互学习过程。&lt;h4&gt;背景&lt;/h4&gt;现有的人机界面设计大多集中于机器控制算法上，这往往需要患者花费大量时间来适应新的系统。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的理论框架和实用指导原则，以满足患者的使用需求并提高系统的整体效率。&lt;h4&gt;方法&lt;/h4&gt;建立CAMDPs模型的充分条件，并确保纳什均衡点的独特性。同时探索多纳什均衡点场景下的策略调整方案。&lt;h4&gt;主要发现&lt;/h4&gt;提出的收敛条件有助于系统达到唯一最优纳什均衡状态，通过数值实验验证了算法的有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;该方法为机器人辅助康复中更有效的自适应系统的发展提供了理论支持和实践指导，提高了人机交互的效率。&lt;h4&gt;翻译&lt;/h4&gt;机器人辅助康复的关键在于设计能够满足患者和机器需求的人机接口。现有的界面设计主要集中在控制算法上，常常需要患者花费大量时间去适应这些系统。本文提出了一种基于合作自适应马尔可夫决策过程（CAMDPs）的新方法，以解决互动学习过程中基本方面的问题，并提供理论见解和实用指导。我们建立了CAMDPs模型的充分条件，并确保纳什均衡点的独特性。利用这些条件，我们保证了系统会收敛到一个唯一的纳什均衡点。此外，在涉及多个纳什均衡点的情况下，我们设计了策略来调整价值评估和政策改进算法，以提高达到全局最小纳什均衡点的可能性。通过数值实验展示了所提条件和算法的有效性，证明其在实际应用中的适用性和稳健性。提出的收敛条件以及唯一最优纳什均衡的识别有助于发展更有效的自适应系统，以更好地服务于机器人辅助康复中的人类用户。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The key to robot-assisted rehabilitation lies in the design of thehuman-machine interface, which must accommodate the needs of both patients andmachines. Current interface designs primarily focus on machine controlalgorithms, often requiring patients to spend considerable time adapting. Inthis paper, we introduce a novel approach based on the Cooperative AdaptiveMarkov Decision Process (CAMDPs) model to address the fundamental aspects ofthe interactive learning process, offering theoretical insights and practicalguidance. We establish sufficient conditions for the convergence of CAMDPs andensure the uniqueness of Nash equilibrium points. Leveraging these conditions,we guarantee the system's convergence to a unique Nash equilibrium point.Furthermore, we explore scenarios with multiple Nash equilibrium points,devising strategies to adjust both Value Evaluation and Policy Improvementalgorithms to enhance the likelihood of converging to the global minimal Nashequilibrium point. Through numerical experiments, we illustrate theeffectiveness of the proposed conditions and algorithms, demonstrating theirapplicability and robustness in practical settings. The proposed conditions forconvergence and the identification of a unique optimal Nash equilibriumcontribute to the development of more effective adaptive systems for humanusers in robot-assisted rehabilitation.</description>
      <author>example@mail.com (Steven W. Su, Yaqi Li, Kairui Guo, Rob Duffield)</author>
      <guid isPermaLink="false">2503.07319v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Self-Corrective Task Planning by Inverse Prompting with Large Language Models</title>
      <link>http://arxiv.org/abs/2503.07317v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures, IEEE International Conference on Robotics and  Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;在机器人任务规划中，大型语言模型（LLMs）展示了生成复杂和长时段动作序列的巨大潜力。然而，观察到这些模型常常产生听起来合理但实际上不准确的响应。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通常采用预定义错误集或外部知识源来解决这些问题，需要人工努力和计算资源。最近出现了一种自我纠正的方法，其中LLM生成并改进计划，并通过自身识别错误。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有自修正方法在推理不足时容易失败的问题，本文提出一种名为InversePrompt的新颖任务规划方法。&lt;h4&gt;方法&lt;/h4&gt;这种方法利用反向提示（inverse prompting）来增强可解释性，将理由步骤纳入其中以提供清晰、易于理解的反馈。它生成与最初生成的动作相对应的逆动作，并验证这些逆动作是否能将系统恢复到其初始状态，从而明确验证所生成计划的逻辑一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的结果显示，我们的方法相较于现有基于LLM的任务规划方法平均提高了16.3%的成功率。&lt;h4&gt;结论&lt;/h4&gt;这种方法为现实环境中反馈提供了更清晰的理由，与现有的自修正方法相比，在各种场景下实现了更为成功地任务完成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In robot task planning, large language models (LLMs) have shown significantpromise in generating complex and long-horizon action sequences. However, it isobserved that LLMs often produce responses that sound plausible but are notaccurate. To address these problems, existing methods typically employpredefined error sets or external knowledge sources, requiring human effortsand computation resources. Recently, self-correction approaches have emerged,where LLM generates and refines plans, identifying errors by itself. Despitetheir effectiveness, they are more prone to failures in correction due toinsufficient reasoning. In this paper, we introduce InversePrompt, a novelself-corrective task planning approach that leverages inverse prompting toenhance interpretability. Our method incorporates reasoning steps to provideclear, interpretable feedback. It generates inverse actions corresponding tothe initially generated actions and verifies whether these inverse actions canrestore the system to its original state, explicitly validating the logicalcoherence of the generated plans.The results on benchmark datasets show anaverage 16.3% higher success rate over existing LLM-based task planningmethods. Our approach offers clearer justifications for feedback in real-worldenvironments, resulting in more successful task completion than existingself-correction approaches across various scenarios.</description>
      <author>example@mail.com (Jiho Lee, Hayun Lee, Jonghyeon Kim, Kyungjae Lee, Eunwoo Kim)</author>
      <guid isPermaLink="false">2503.07317v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Bioinspired Sensing of Undulatory Flow Fields Generated by Leg Kicks in Swimming</title>
      <link>http://arxiv.org/abs/2503.07312v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的感知框架，用于研究游泳者腿部踢水产生的波动水流场，并使用仿生人工侧线（ALL）系统进行评估。&lt;h4&gt;背景&lt;/h4&gt;人工侧线（ALL）是一种模仿鱼类的生物启发式流量感应系统，已成功应用于检测由身体波状运动和尾巴拍打生成的流动区域。然而，其在感知游泳者腿部踢水产生的波动水流场方面的可行性和性能尚未得到系统的测试和研究。&lt;h4&gt;目的&lt;/h4&gt;设计一个实验平台来评估使用ALL系统感知游泳者腿部踢水产生的波动水流场的可行性，并提高流体感应准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于时间域和时频特性动态融合的方法，用于特征提取。具体来说，通过一维卷积神经网络（1DCNN）和双向长短时记忆网络（BiLSTM）提取时间域特征，同时使用短时傅里叶变换（STFT）与二维卷积神经网络（2DCNN）来提取时频特性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在多种模拟人类游泳情景下表现良好，如腿部踢水模式识别和踢腿定位任务的准确度均达到了满意的水平。&lt;h4&gt;结论&lt;/h4&gt;这种基于ALL系统的新型感知框架为研究水流场提供了新的视角，并显示出应用于实际流体动力学检测中的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：人工侧线（ALL）是一种仿生流动感应系统，用于水下机器人，由分布式的流动传感器组成。ALL已经成功地应用于探测鱼类生物启发式机器鱼的身体波状运动和尾巴拍打产生的波动场。然而，其在感知游泳者腿部踢水产生的波动水流场方面的可行性和性能尚未得到系统的测试和研究。本文提出了一种新的感知框架来调查由游泳者的腿踢生成的波动流场，并利用仿生ALL感应进行评估。为了验证使用ALL系统检测游泳者腿部踢水所产生的波动流场的可行性，论文设计了一个实验平台，该平台整合了ALL系统与实验室制造的人类腿部模型。为提高流动感测精度，提出了一种基于注意力机制动态融合时间域和时频特性的特征提取方法。具体而言，使用一维卷积神经网络（1DCNN）和双向长短时记忆网络（BiLSTM）从时间域中提取特性，同时使用短时傅里叶变换（STFT）与二维卷积神经网络（2DCNN）来提取时频特性。这些特征随后被基于注意力机制动态融合以实现对波动流场的准确感知。此外，进行了广泛的实验测试了各种受人类游泳启发的情景，例如腿部踢水模式识别和踢腿定位任务，并取得了令人满意的成果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The artificial lateral line (ALL) is a bioinspired flow sensing system forunderwater robots, comprising of distributed flow sensors. The ALL has beensuccessfully applied to detect the undulatory flow fields generated by bodyundulation and tail-flapping of bioinspired robotic fish. However, itsfeasibility and performance in sensing the undulatory flow fields produced byhuman leg kicks during swimming has not been systematically tested and studied.This paper presents a novel sensing framework to investigate the undulatoryflow field generated by swimmer's leg kicks, leveraging bioinspired ALLsensing. To evaluate the feasibility of using the ALL system for sensing theundulatory flow fields generated by swimmer leg kicks, this paper designs anexperimental platform integrating an ALL system and a lab-fabricated human legmodel. To enhance the accuracy of flow sensing, this paper proposes a featureextraction method that dynamically fuses time-domain and time-frequencycharacteristics. Specifically, time-domain features are extracted usingone-dimensional convolutional neural networks and bidirectional long short-termmemory networks (1DCNN-BiLSTM), while time-frequency features are extractedusing short-term Fourier transform and two-dimensional convolutional neuralnetworks (STFT-2DCNN). These features are then dynamically fused based onattention mechanisms to achieve accurate sensing of the undulatory flow field.Furthermore, extensive experiments are conducted to test various scenariosinspired by human swimming, such as leg kick pattern recognition and kickingleg localization, achieving satisfactory results.</description>
      <author>example@mail.com (Jun Wang, Tongsheng Shen, Dexin Zhao, Feitian Zhang)</author>
      <guid isPermaLink="false">2503.07312v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Robot System for Cooperative Exploration in Unknown Environments: A Survey</title>
      <link>http://arxiv.org/abs/2503.07278v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文对多机器人协同探索系统进行了全面回顾。&lt;h4&gt;背景介绍&lt;/h4&gt;随着多机器人技术的进步，协同探索任务越来越受到关注。文章介绍了机器人的探索演进，并提出了一种专为多机器人协作探索设计的模块化研究框架。&lt;h4&gt;关键组成部分分类&lt;/h4&gt;基于提出的框架，本文系统地对关键技术组件进行分类和总结。&lt;h4&gt;定位与建图模块&lt;/h4&gt;定位与建图模块作为多机器人探索的基础模块被重点介绍，包括全局及相对姿态估计以及多机器人地图合并技术。&lt;h4&gt;协同运动模块&lt;/h4&gt;协同运动模块进一步分为基于学习的方法和多阶段规划，后者涵盖目标生成、任务分配和运动规划策略。&lt;h4&gt;通信模块分析&lt;/h4&gt;鉴于实际环境中的通讯限制，本文还分析了通信模块，特别关注机器人如何在局部通信范围内交换信息以及在有限传输能力下进行信息交流。&lt;h4&gt;未来研究方向&lt;/h4&gt;最后，文章讨论了多机器人协同探索面临的挑战及未来的研发方向，并根据现实世界趋势提出了建议。&lt;h4&gt;目标读者群&lt;/h4&gt;本综述旨在为该领域的研究人员和实践者提供有价值的参考。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the advancement of multi-robot technology, cooperative exploration taskshave garnered increasing attention. This paper presents a comprehensive reviewof multi-robot cooperative exploration systems. First, we review the evolutionof robotic exploration and introduce a modular research framework tailored formulti-robot cooperative exploration. Based on this framework, we systematicallycategorize and summarize key system components. As a foundational module formulti-robot exploration, the localization and mapping module is primarilyintroduced by focusing on global and relative pose estimation, as well asmulti-robot map merging techniques. The cooperative motion module is furtherdivided into learning-based approaches and multi-stage planning, with thelatter encompassing target generation, task allocation, and motion planningstrategies. Given the communication constraints of real-world environments, wealso analyze the communication module, emphasizing how robots exchangeinformation within local communication ranges and under limited transmissioncapabilities. Finally, we discuss the challenges and future research directionsfor multi-robot cooperative exploration in light of real-world trends. Thisreview aims to serve as a valuable reference for researchers and practitionersin the field.</description>
      <author>example@mail.com (Chuqi Wang, Chao Yu, Xin Xu, Yuman Gao, Xinyi Yang, Wenhao Tang, Shu'ang Yu, Yinuo Chen, Feng Gao, ZhuoZhu Jian, Xinlei Chen, Fei Gao, Boyu Zhou, Yu Wang)</author>
      <guid isPermaLink="false">2503.07278v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>A High Efficient and Scalable Obstacle-Avoiding VLSI Global Routing Flow</title>
      <link>http://arxiv.org/abs/2503.07268v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently submitting to a journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种新的障碍感知全局路由流程被提出，以解决VLSI设计中由于制造技术进步而产生的复杂障碍问题。&lt;h4&gt;背景&lt;/h4&gt;随着制造技术的发展，越来越多的设计规则约束出现，特别是在布线过程中遇到的障碍物导致了更高的布线复杂性。许多现有的全局路由器在生成无阻碍解决方案时效率不高。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效的方法来解决VLSI设计中的障碍物问题，并优化路由流程以减少布线长度、溢出成本以及增加路径质量。&lt;h4&gt;方法&lt;/h4&gt;引入了一种基于规则的障碍避免直角Steiner最小树(OARSMT)算法，在生成阶段构建可避开障碍物的树状结构。随后提出OARSMT引导和障碍感知稀疏迷宫路由，进一步减少障碍物违规并降低溢出成本。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方案成功地消除了障碍物违反，并在牺牲少量过孔计数和运行时间开销的情况下减少了布线长度和溢出成本。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了一种有效的解决方案来处理VLSI设计中的复杂障碍问题，从而优化了路由效率和性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出一种针对存在障碍物的VLSI设计的有效障碍感知全局路由流程。该方法通过引入基于规则的避免障碍直角Steiner最小树(OARSMT)算法解决早期阶段障碍物避开的问题，并在后期采用OARSMT引导及障碍感知稀疏迷宫路由进一步优化，最终减少了布线长度、溢出成本等关键指标，同时仅少量牺牲过孔计数和运行时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Routing is a crucial step in the VLSI design flow. With the advancement ofmanufacturing technologies, more constraints have emerged in design rules,particularly regarding obstacles during routing, leading to increased routingcomplexity. Unfortunately, many global routers struggle to efficiently generateobstacle-free solutions due to the lack of scalable obstacle-avoiding treegeneration methods and the capability of handling modern designs with complexobstacles and nets. In this work, we propose an efficient obstacle-aware globalrouting flow for VLSI designs with obstacles. The flow includes a rule-basedobstacle-avoiding rectilinear Steiner minimal tree (OARSMT) algorithm duringthe tree generation phase. This algorithm is both scalable and fast to providetree topologies avoiding obstacles in the early stage globally. With itsguidance, OARSMT-guided and obstacle-aware sparse maze routing are proposed inthe later stages to minimize obstacle violations further and reduce overflowcosts. Compared to advanced methods on the benchmark with obstacles, ourapproach successfully eliminates obstacle violations, and reduces wirelengthand overflow cost, while sacrificing only a limited number of via counts andruntime overhead.</description>
      <author>example@mail.com (Junhao Guo, Hongxin Kong, Lang Feng)</author>
      <guid isPermaLink="false">2503.07268v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>WHERE-Bot: a Wheel-less Helical-ring Everting Robot Capable of Omnidirectional Locomotion</title>
      <link>http://arxiv.org/abs/2503.07245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper has been accepted for publication at 2025 IEEE 8th  International Conference on Soft Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了WHERE-Bot，一种无轮的软机器人，能够在复杂环境中进行全方位移动，并且无需依赖传感器即可导航。&lt;h4&gt;背景&lt;/h4&gt;传统的轮式运输系统在平坦表面工作良好，但面对各种地形时适应性较差。相比之下，软机器人能够更好地适应不同环境，在复杂的环境中保持稳定运动。&lt;h4&gt;目的&lt;/h4&gt;设计一种无轮的、能够自我翻转以实现全方位移动的软机器人，并探索其在结构不规则和边界不确定的环境中导航的能力。&lt;h4&gt;方法&lt;/h4&gt;将弹簧玩具'Slinky'配置成环形，WHERE-Bot通过改变自身质量分布来重新编程其运动轨迹。该机器人可以通过螺旋旋转沿着中心轮子周围运动、围绕中心点自转以及绕特定点进行公转。&lt;h4&gt;主要发现&lt;/h4&gt;WHERE-Bot能够在没有传感器的情况下探索边界，在结构不确定的环境中导航时显示出显著的优势。&lt;h4&gt;结论&lt;/h4&gt;研究表明，利用软机器人的结构和运动特性可以实现无依赖传感器的有效环境探索。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compared to conventional wheeled transportation systems designed for flatsurfaces, soft robots exhibit exceptional adaptability to various terrains,enabling stable movement in complex environments. However, due to the risk ofcollision with obstacles and barriers, most soft robots rely on sensors fornavigation in unstructured environments with uncertain boundaries. In thiswork, we present the WHERE-Bot, a wheel-less everting soft robot capable ofomnidirectional locomotion. Our WHERE-Bot can navigate through unstructuredenvironments by leveraging its structural and motion advantages rather thanrelying on sensors for boundary detection. By configuring a spring toy``Slinky'' into a loop shape, the WHERE-Bot performs multiple rotationalmotions: spiral-rotating along the hub circumference, self-rotating around thehub's center, and orbiting around a certain point. The robot's trajectories canbe reprogrammed by actively altering its mass distribution. The WHERE-Bot showssignificant potential for boundary exploration in unstructured environments.</description>
      <author>example@mail.com (Siyuan Feng, Dengfeng Yan, Jin Liu, Haotong Han, Alexandra Kühl, Shuguang Li)</author>
      <guid isPermaLink="false">2503.07245v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Learning and planning for optimal synergistic human-robot coordination in manufacturing contexts</title>
      <link>http://arxiv.org/abs/2503.07238v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in Robotics and Computer-Integrated  Manufacturing (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种基于混合整数非线性规划的人机协作任务分配和调度模型，该模型通过学习之前的执行情况来优化效率与安全性。&lt;h4&gt;背景&lt;/h4&gt;协同机器人单元利用异质代理提供灵活的生产解决方案。有效协调是防止人类操作员在机器人附近工作时产生低效和风险的关键。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于混合整数非线性规划的人机协作任务分配和调度模型，以优化效率与安全性，从任务计划阶段开始考虑人的因素。&lt;h4&gt;方法&lt;/h4&gt;该模型利用了协同效应，即编码在代理并行执行的任务之间的耦合效应，这些效果源于对机器人代理的安全约束。通过贝叶斯估计从之前的执行中学习到这些术语；使用马尔可夫链蒙特卡洛方法进行后验概率分布的推断。&lt;h4&gt;主要发现&lt;/h4&gt;该模型通过根据操作员存在感调整任务计划的名义持续时间来增强任务规划，从而减少代理之间的不必要的干扰，增加人机距离，并且最多可以将过程执行时间缩短18%。&lt;h4&gt;结论&lt;/h4&gt;这种方法能够制定出更符合人类因素的任务计划，提高了协同机器人的生产效率和安全性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，已提供中文总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative robotics cells leverage heterogeneous agents to provide agileproduction solutions. Effective coordination is essential to preventinefficiencies and risks for human operators working alongside robots. Thispaper proposes a human-aware task allocation and scheduling model based onMixed Integer Nonlinear Programming to optimize efficiency and safety startingfrom task planning stages. The approach exploits synergies that encode thecoupling effects between pairs of tasks executed in parallel by the agents,arising from the safety constraints imposed on robot agents. These terms arelearned from previous executions using a Bayesian estimation; the inference ofthe posterior probability distribution of the synergy coefficients is performedusing the Markov Chain Monte Carlo method. The synergy enhances task planningby adapting the nominal duration of the plan according to the effect of theoperator's presence. Simulations and experimental results demonstrate that theproposed method produces improved human-aware task plans, reducing unusefulinterference between agents, increasing human-robot distance, and achieving upto an 18\% reduction in process execution time.</description>
      <author>example@mail.com (Samuele Sandrini, Marco Faroni, Nicola Pedrocchi)</author>
      <guid isPermaLink="false">2503.07238v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs and Chain-of-Thought Prompting</title>
      <link>http://arxiv.org/abs/2503.07234v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoT-Drive是一种利用大规模语言模型（LLMs）和链式思维（CoT）提示方法来增强自主驾驶中运动预测准确性的创新方法。&lt;h4&gt;背景&lt;/h4&gt;精确的运动预测对于安全的自动驾驶至关重要。现有的方法在处理复杂交通环境时存在不足，特别是在理解和预测多个动态行为方面。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法——CoT-Drive，利用LLMs和CoT提示技术来提升运动预测的效果，并开发两个新数据集以增强场景理解能力。&lt;h4&gt;方法&lt;/h4&gt;{'知识蒸馏策略': '采用教师-学生知识蒸馏策略将大模型的高级场景理解能力转移到轻量级语言模型中，保证了系统在边缘设备上的实时运行。', 'CoT提示技术': '通过使用LLMs和链式思维提示技术生成语义注释，增强对复杂交通环境的理解。', '新数据集': '设计Highway-Text和Urban-Text两个场景描述数据集用于轻量级LM的微调，以生成特定上下文的语义标签'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，CoT-Drive在五个真实世界的数据集中表现出色，超过了现有的模型。&lt;h4&gt;结论&lt;/h4&gt;这是首次将LLMs的实际应用考虑进自动驾驶领域的研究。CoT-Drive开创性地使用轻量级语言模型替代物来进行运动预测，并为未来集成LLMs到AD系统中设定了新的标准和潜力。&lt;h4&gt;翻译&lt;/h4&gt;准确的运动预测对安全自主驾驶至关重要。本研究提出了一种新方法，即CoT-Drive，通过利用大规模语言模型（LLM）以及链式思维（CoT）提示技术来增强运动预测能力。我们引入了教师学生知识蒸馏策略，将大模型的高级场景理解能力转移到轻量级语言模型中，在保持综合场景理解和泛化能力的同时确保其实时运行在边缘设备上。通过使用不依赖额外训练的LLMs CoT提示技术，CoT-Drive生成语义注释以显著提升复杂交通环境的理解力，从而提高预测的准确性和鲁棒性。此外，我们提出了两个新的场景描述数据集——高速公路文本和城市文本，用于轻量级语言模型微调，以便生成特定上下文的语义标签。对五个真实世界数据集进行综合评估后发现CoT-Drive优于现有模型，证明了其在处理复杂交通场景时的有效性和效率。总的来说，这项研究是第一个考虑该领域中LLMs实际应用的研究。它开创性地训练并使用轻量级LM代理进行运动预测，并为未来将LLMs集成到AD系统中设定了新的基准和展示了潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate motion forecasting is crucial for safe autonomous driving (AD). Thisstudy proposes CoT-Drive, a novel approach that enhances motion forecasting byleveraging large language models (LLMs) and a chain-of-thought (CoT) promptingmethod. We introduce a teacher-student knowledge distillation strategy toeffectively transfer LLMs' advanced scene understanding capabilities tolightweight language models (LMs), ensuring that CoT-Drive operates inreal-time on edge devices while maintaining comprehensive scene understandingand generalization capabilities. By leveraging CoT prompting techniques forLLMs without additional training, CoT-Drive generates semantic annotations thatsignificantly improve the understanding of complex traffic environments,thereby boosting the accuracy and robustness of predictions. Additionally, wepresent two new scene description datasets, Highway-Text and Urban-Text,designed for fine-tuning lightweight LMs to generate context-specific semanticannotations. Comprehensive evaluations of five real-world datasets demonstratethat CoT-Drive outperforms existing models, highlighting its effectiveness andefficiency in handling complex traffic scenarios. Overall, this study is thefirst to consider the practical application of LLMs in this field. It pioneersthe training and use of a lightweight LLM surrogate for motion forecasting,setting a new benchmark and showcasing the potential of integrating LLMs intoAD systems.</description>
      <author>example@mail.com (Haicheng Liao, Hanlin Kong, Bonan Wang, Chengyue Wang, Wang Ye, Zhengbing He, Chengzhong Xu, Zhenning Li)</author>
      <guid isPermaLink="false">2503.07234v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Discrete Gaussian Process Representations for Optimising UAV-based Precision Weed Mapping</title>
      <link>http://arxiv.org/abs/2503.07210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;精确的农业杂草地图绘制对于精准农业应用至关重要。传统的方法依赖于从固定飞行路径创建正射镶嵌图，这种方法计算量大且耗时。基于高斯过程（GP）的地图绘制提供了对潜在变量（例如杂草分布）的连续建模，但需要离散化以进行实际任务如路径规划或可视化。目前实现中通常采用四叉树或网格地图，而没有系统地评估其他替代方案。&lt;h4&gt;背景&lt;/h4&gt;精确农业应用中的准确杂草地图绘制至关重要。传统的无人机映射方法由于计算密集和耗时，难以满足需求。基于高斯过程（GP）的方法提供了连续建模的优势，但在实际任务中需要进行离散化处理。&lt;h4&gt;目的&lt;/h4&gt;本研究比较了五种不同的离散化方法，并评估它们在真实世界杂草分布情况下的性能表现。&lt;h4&gt;方法&lt;/h4&gt;研究采用了以下五种离散化方法：四叉树、楔形图（wedgelets）、顶部向下二分空间划分（BSP）使用最小平方误差（LSE）、底部向上BSP图通过图形合并，以及可变分辨率的六边形网格。评估标准包括视觉相似度、均方误差（MSE）和计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果显示，四叉树在总体性能上表现最佳，但不同方法在特定情况下表现出色：六边形或BSP LSE适合大块且主导的杂草区域；而四叉树则适用于分散的小范围分布。这些发现强调了根据杂草分布模式（如斑块大小、密度和覆盖度）定制离散化方法的重要性。&lt;h4&gt;结论&lt;/h4&gt;选择基于潜在分布特征的表示形式可以提高农业精准应用中的地图绘制精度和效率，不应只依赖于默认的方法进行任务处理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate agricultural weed mapping using UAVs is crucial for precisionfarming applications. Traditional methods rely on orthomosaic stitching fromrigid flight paths, which is computationally intensive and time-consuming.Gaussian Process (GP)-based mapping offers continuous modelling of theunderlying variable (i.e. weed distribution) but requires discretisation forpractical tasks like path planning or visualisation. Current implementationsoften default to quadtrees or gridmaps without systematically evaluatingalternatives. This study compares five discretisation methods: quadtrees,wedgelets, top-down binary space partition (BSP) trees using least square error(LSE), bottom-up BSP trees using graph merging, and variable-resolutionhexagonal grids. Evaluations on real-world weed distributions measure visualsimilarity, mean squared error (MSE), and computational efficiency. Resultsshow quadtrees perform best overall, but alternatives excel in specificscenarios: hexagons or BSP LSE suit fields with large, dominant weed patches,while quadtrees are optimal for dispersed small-scale distributions. Thesefindings highlight the need to tailor discretisation approaches to weeddistribution patterns (patch size, density, coverage) rather than relying ondefault methods. By choosing representations based on the underlyingdistribution, we can improve mapping accuracy and efficiency for precisionagriculture applications.</description>
      <author>example@mail.com (Jacob Swindell, Madeleine Darbyshire, Marija Popovic, Riccardo Polvara)</author>
      <guid isPermaLink="false">2503.07210v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>Reactive and Safety-Aware Path Replanning for Collaborative Applications</title>
      <link>http://arxiv.org/abs/2503.07192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in IEEE Transactions on Automation Science  and Engineering (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了人机协作场景中的运动重规划问题，强调反应性和符合安全性的效率。&lt;h4&gt;背景&lt;/h4&gt;现有的以人为中心的运动规划器在结构化环境中表现良好，但在处理不可预测的人类行为时经常遇到困难。这些情况下的安全性措施往往限制了机器人的性能和吞吐量。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合响应式路径重规划和安全感知成本函数的方法，使机器人能够根据人类状态的变化调整其路径。&lt;h4&gt;方法&lt;/h4&gt;该解决方案允许在不牺牲安全性的情况下减少执行时间和轨迹减速的需求。&lt;h4&gt;主要发现&lt;/h4&gt;模拟实验和现实世界的实验表明，与标准的人机合作方法相比，这种方法提高了高达60%的效率。&lt;h4&gt;结论&lt;/h4&gt;提出的方案有效解决了现有规划器在不可预测环境中的局限性，并显著提升了机器人系统的整体性能和安全性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：本文探讨了人机协作场景中的运动重规划问题，强调反应性和符合安全性的效率。现有的以人为中心的运动规划器在结构化环境中表现良好，但在处理不可预测的人类行为时经常遇到困难。这些情况下的安全性措施往往限制了机器人的性能和吞吐量。提出了一种结合响应式路径重规划和安全感知成本函数的方法，使机器人能够根据人类状态的变化调整其路径。该解决方案允许在不牺牲安全性的情况下减少执行时间和轨迹减速的需求。模拟实验和现实世界的实验表明，与标准的人机合作方法相比，这种方法提高了高达60%的效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses motion replanning in human-robot collaborativescenarios, emphasizing reactivity and safety-compliant efficiency. Whileexisting human-aware motion planners are effective in structured environments,they often struggle with unpredictable human behavior, leading to safetymeasures that limit robot performance and throughput. In this study, we combinereactive path replanning and a safety-aware cost function, allowing the robotto adjust its path to changes in the human state. This solution reduces theexecution time and the need for trajectory slowdowns without sacrificingsafety. Simulations and real-world experiments show the method's effectivenesscompared to standard human-robot cooperation approaches, with efficiencyenhancements of up to 60\%.</description>
      <author>example@mail.com (Cesare Tonola, Marco Faroni, Saeed Abdolshah, Mazin Hamad, Sami Haddadin, Nicola Pedrocchi, Manuel Beschi)</author>
      <guid isPermaLink="false">2503.07192v1</guid>
      <pubDate>Tue, 11 Mar 2025 19:37:40 +0800</pubDate>
    </item>
    <item>
      <title>MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields</title>
      <link>http://arxiv.org/abs/2502.14401v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://pfriedri.github.io/medfuncta-io/ Code and  Dataset: https://github.com/pfriedri/medfuncta/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种新的医学图像分析的数据表示方式MedFuncta，该方式基于神经场并可以应用于不同的医学信号。研究展示了如何通过利用医疗信号中的冗余信息和使用高效的元学习方法来扩展神经场的规模。&lt;h4&gt;背景介绍&lt;/h4&gt;当前医学影像分析的研究几乎完全集中在网格或体素数据表示上。&lt;h4&gt;主要目的&lt;/h4&gt;引入MedFuncta作为无模态依赖的连续数据表示，挑战传统的数据表示选择，并通过优化神经网络激活函数来解决谱偏置问题。&lt;h4&gt;方法描述&lt;/h4&gt;提出了一种基于SIREN激活函数和ω_0调度策略的方法，以改善重建质量和加速收敛速度。研究还展示了如何从单一实例扩展到大规模数据集的方法。&lt;h4&gt;主要发现&lt;/h4&gt;在多种一维至三维的医学信号（如心电图、胸部X光片、视网膜OCT图像等）上验证了该方法的有效性，并且可以解决下游任务，例如病灶检测和分割。&lt;h4&gt;结论&lt;/h4&gt;成功地展示了MedFuncta能够在不同维度和模态的医疗信号中有效表示信息，解决了传统的数据表示所面临的挑战，并促进了相关领域的研究发展。此外还发布了包含超过550k个注释神经场的大规模数据集以促进该方向的研究。&lt;h4&gt;翻译摘要&lt;/h4&gt;近期基于深度学习的医学影像分析主要关注网格或体素型的数据表示形式。本文通过引入一种基于神经网络字段的模态无关连续数据表示——MedFuncta，对这一普遍选择提出了挑战。本文展示了如何通过利用医疗信号中的冗余信息和使用高效的元学习方法来扩展从单例到大规模数据集的神经网络规模。此外，为了克服常用的SIREN激活函数存在的谱偏差问题，我们引入了ω_0调度策略，从而改进了重建质量和收敛速度。我们在一系列具有不同维度和模态（1D：心电图；2D：胸部X光片、视网膜OCT图像、眼底相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT）的医学信号上验证了我们提出的方法，并成功展示了在这些表示形式中解决相关下游任务的可能性。此外，为了促进该方向的研究发展，我们还发布了一个包含超过550k个注释神经场的大规模数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/pfriedri/medfuncta&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research in medical image analysis with deep learning almostexclusively focuses on grid- or voxel-based data representations. We challengethis common choice by introducing MedFuncta, a modality-agnostic continuousdata representation based on neural fields. We demonstrate how to scale neuralfields from single instances to large datasets by exploiting redundancy inmedical signals and by applying an efficient meta-learning approach with acontext reduction scheme. We further address the spectral bias in commonly usedSIREN activations, by introducing an $\omega_0$-schedule, improvingreconstruction quality and convergence speed. We validate our proposed approachon a large variety of medical signals of different dimensions and modalities(1D: ECG; 2D: Chest X-ray, Retinal OCT, Fundus Camera, Dermatoscope, ColonHistopathology, Cell Microscopy; 3D: Brain MRI, Lung CT) and successfullydemonstrate that we can solve relevant downstream tasks on theserepresentations. We additionally release a large-scale dataset of &gt; 550kannotated neural fields to promote research in this direction.</description>
      <author>example@mail.com (Paul Friedrich, Florentin Bieder, Philippe C. Cattin)</author>
      <guid isPermaLink="false">2502.14401v2</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
  <item>
      <title>Trial by FIRE: Probing the dark matter density profile of dwarf galaxies with GraphNPE</title>
      <link>http://arxiv.org/abs/2503.03812v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to MNRAS. Comments welcomed. 20 + 12 pages, 13 + 11  figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究使用GraphNPE框架对FIRE-2 Latte模拟中的卫星矮星系进行暗物质分布分析，并展示了该方法在冷暗物质和自相互作用暗物质场景下的优越性能。&lt;h4&gt;背景&lt;/h4&gt;暗物质的分布为结构形成和粒子性质提供了重要见解，而GraphNPE结合了图神经网络和归一化流以从视线恒星速度中推断出暗物质密度剖面。&lt;h4&gt;目的&lt;/h4&gt;应用GraphNPE框架于FIRE-2 Latte模拟中的卫星矮星系，并在冷暗物质和自相互作用暗物质场景下测试其效果。&lt;h4&gt;方法&lt;/h4&gt;将GraphNPE应用于卫星矮星系，利用该方法从视线速度中推断出暗物质密度剖面。此外，研究了质量和峰值旋转速度参数的约束情况。&lt;h4&gt;主要发现&lt;/h4&gt;与传统Jeans方法相比，GraphNPE在精度上有显著提高，并能在系统具有少于30个追踪器的情况下恢复到95%置信水平；能够可靠地恢复质量和峰值旋转速度参数；准确度分别达到10-20%和0.1-0.4 dex。&lt;h4&gt;结论&lt;/h4&gt;GraphNPE作为一种稳健的工具，在推断矮星系中的暗物质密度剖面方面表现出色，为限制暗物质模型提供了有前景的方法。该框架还能应用于非球形和不平衡模型，展示了基于模拟推理和图学习在天体物理学中的广泛实用性。&lt;h4&gt;翻译&lt;/h4&gt;摘要中所述的研究内容包括使用GraphNPE框架进行的FIRE-2 Latte卫星矮星系模拟实验及其对冷暗物质及自相互作用暗物质场景下暗物质分布的分析。研究通过这种方法从视线速度数据推断出暗物质密度剖面，展示了该方法相对于传统方式的优势，并且在质量和旋转速度参数约束方面取得了重要发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Dark Matter (DM) distribution in dwarf galaxies provides crucial insightsinto both structure formation and the particle nature of DM. GraphNPE (GraphNeural Posterior Estimator), first introduced in Nguyen et al. (2023), is anovel simulation-based inference framework that combines graph neural networksand normalizing flows to infer the DM density profile from line-of-sightstellar velocities. Here, we apply GraphNPE to satellite dwarf galaxies in theFIRE-2 Latte simulation suite of Milky Way-mass halos, testing it against bothCold and Self-Interacting DM scenarios. Our method demonstrates superiorprecision compared to conventional Jeans-based approaches, recovering DMdensity profiles to within the 95% confidence level even in systems with as fewas 30 tracers. Moreover, we present the first evaluation of mass modelingmethods in constraining two key parameters from realistic simulations: the peakcircular velocity, $V_\mathrm{max}$, and the peak virial mass,$M_\mathrm{200m}^\mathrm{peak}$. Using only line-of-sight velocities, GraphNPEcan reliably recover both $V_\mathrm{max}$ and $M_\mathrm{200m}^\mathrm{peak}$within our quoted uncertainties, including those experiencing tidal effects($\gtrsim$ 63% of systems are recovered with our 68% confidence intervals and$\gtrsim$ 92% within our 95% confidence intervals). The method achieves 10-20%accuracy in $V_\mathrm{max}$ recovery, while $M_\mathrm{200m}^\mathrm{peak}$ isrecovered to 0.1-0.4 dex accuracy. This work establishes GraphNPE as a robusttool for inferring DM density profiles in dwarf galaxies, offering promisingavenues for constraining DM models. The framework's potential extends beyondthis study, as it can be adapted to non-spherical and disequilibrium models,showcasing the broader utility of simulation-based inference and graph-basedlearning in astrophysics.</description>
      <author>example@mail.com (Tri Nguyen, Justin Read, Lina Necib, Siddharth Mishra-Sharma, Claude-André Faucher-Giguère, Andrew Wetzel)</author>
      <guid isPermaLink="false">2503.03812v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Instrument-Splatting: Controllable Photorealistic Reconstruction of Surgical Instruments Using Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2503.04082v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种名为Instrument-Splatting的新型Real2Sim方法，该方法利用3D高斯点云技术实现单目内窥镜视频中手术器械的全三维重建。&lt;h4&gt;背景&lt;/h4&gt;随着外科人工智能和自主性的发展，将真实场景转换为模拟环境（Real2Sim）变得越来越重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的Real2Sim方法来解决单目内窥镜视频中的手术器械全三维重建问题，并确保视觉逼真度和操作性。&lt;h4&gt;方法&lt;/h4&gt;{'技术核心': '利用3D高斯点云技术和Gaussian Splatting实现从单目内窥镜视频中重建手术器械的完整3D模型。', '几何预训练': '引入了几何预训练，通过将具有准确几何先验的Gaussian点云绑定在部件网格上来保持视觉真实度和可操作性。', '前向动力学定义': '定义了一种前向动力学方法来控制Gaussians如同真正的手术器械一样灵活。', '未标记视频处理': '设计了一种利用语义嵌入的高斯点的新颖仪器姿态跟踪方法，用于逐帧精炼未标记视频中的工具姿态和关节状态，确保真实感渲染。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'验证': '在2份公开发布的手术视频及4份采集自离体组织和绿幕背景下的视频上进行了验证。', '效果评估': '定量和定性评价显示了所提出方法的有效性和优越性。'}&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种有效的方法来解决从单目内窥镜视频中重建手术器械的全三维模型的问题，同时保持视觉真实度和操作性。&lt;h4&gt;翻译&lt;/h4&gt;随着外科人工智能和自主性的迅速发展，Real2Sim（将现实场景转换为模拟环境）变得越来越重要。本文提出了一种新的Real2Sim方法——Instrument-Splatting，该方法利用3D高斯点云技术从单目内窥镜视频中提供手术器械的全三维重建。为了保持高度视觉真实度和操作性，我们引入了几何预训练，将准确的几何先验绑定到部件网格上的Gaussian点云，并定义了一种前向动力学来像真正的工具那样灵活地控制高斯分布。为处理未标记视频，设计了一种利用语义嵌入的Gaussians的新颖仪器姿态跟踪方法，在渲染和比较方式下稳健地精炼每个帧中的工具姿态和关节状态，使得我们的工具Gaussian能够精确学习纹理并实现逼真的照片级渲染。我们在2个公开发布的手术视频和4个在离体组织及绿幕背景上收集的视频中验证了该方法的有效性和优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real2Sim is becoming increasingly important with the rapid development ofsurgical artificial intelligence (AI) and autonomy. In this work, we propose anovel Real2Sim methodology, \textit{Instrument-Splatting}, that leverages 3DGaussian Splatting to provide fully controllable 3D reconstruction of surgicalinstruments from monocular surgical videos. To maintain both high visualfidelity and manipulability, we introduce a geometry pre-training to bindGaussian point clouds on part mesh with accurate geometric priors and define aforward kinematics to control the Gaussians as flexible as real instruments.Afterward, to handle unposed videos, we design a novel instrument pose trackingmethod leveraging semantics-embedded Gaussians to robustly refine per-frameinstrument poses and joint states in a render-and-compare manner, which allowsour instrument Gaussian to accurately learn textures and reach photorealisticrendering. We validated our method on 2 publicly released surgical videos and 4videos collected on ex vivo tissues and green screens. Quantitative andqualitative evaluations demonstrate the effectiveness and superiority of theproposed method.</description>
      <author>example@mail.com (Shuojue Yang, Zijian Wu, Mingxuan Hong, Qian Li, Daiyun Shen, Septimiu E. Salcudean, Yueming Jin)</author>
      <guid isPermaLink="false">2503.04082v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Geometry-Constrained Monocular Scale Estimation Using Semantic Segmentation for Dynamic Scenes</title>
      <link>http://arxiv.org/abs/2503.04235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一个创新的单目视觉定位策略，结合了深度学习模型与几何约束来解决传统的单目视觉里程计在尺度估计中的挑战。&lt;h4&gt;背景&lt;/h4&gt;单目视觉定位在高级驾驶员辅助系统和自动驾驶中至关重要。然而，传统方法难以处理动态物体并有效管理计算复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合SegNeXt模型的混合方法，用于实时应用中的自我运动估计和地面点选择，并优化尺度恢复过程。&lt;h4&gt;方法&lt;/h4&gt;{'深度学习模型': '使用SegNeXt模型进行实时的应用场景下的自我运动估计和地面点选择', '动态物体处理': '利用动态物体掩码来消除不稳定的特征，提高鲁棒性', '几何约束': '结合道路区域的几何约束恢复尺度信息', 'ORB-SLAM3集成': '将该方法与单目ORB-SLAM3相结合以精确估计路面模型'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能提升': '实验结果表明，相较于现有单目视觉里程计算法及当代尺度恢复方法，所提出的方法在准确性上具有显著优势', '鲁棒性提高': '通过动态物体掩码和几何约束的引入提高了系统的鲁棒性和精确度'}&lt;h4&gt;结论&lt;/h4&gt;该研究为解决单目视觉定位中的挑战提供了新的思路，并且展示了其在实际应用中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于利用深度学习模型以及几何约束来优化单目视觉里程计的方法的研究，提出了一种结合SegNeXt模型的新方法，在路面模型的准确估计上取得了突破性的进展。实验结果证明该方法在鲁棒性和精确度上有明显优势，并且优于现有的视觉里程计算法和尺度恢复技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular visual localization plays a pivotal role in advanced driverassistance systems and autonomous driving by estimating a vehicle's ego-motionfrom a single pinhole camera. Nevertheless, conventional monocular visualodometry encoun-ters challenges in scale estimation due to the absence of depthinformation during projection. Previous methodologies, whether rooted inphysical constraints or deep learning paradigms, con-tend with issues relatedto computational complexity and the management of dynamic objects. This studyextends our prior research, presenting innovative strategies for ego-motionestima-tion and the selection of ground points. Striving for a nuancedequilibrium between computational efficiency and precision, we propose a hybridmethod that leverages the SegNeXt model for real-time applications,encompassing both ego-motion estimation and ground point selection. Ourmethodology incorporates dy-namic object masks to eliminate unstable featuresand employs ground plane masks for meticulous triangulation. Furthermore, weexploit Geometry-constraint to delineate road regions for scale recovery. Theintegration of this approach with the mo-nocular version of ORB-SLAM3culminates in the accurate esti-mation of a road model, a pivotal component inour scale recov-ery process. Rigorous experiments, conducted on the KITTIda-taset, systematically compare our method with existing monocu-lar visualodometry algorithms and contemporary scale recovery methodologies. The resultsundeniably confirm the superior ef-fectiveness of our approach, surpassingstate-of-the-art visual odometry algorithms. Our source code is available athttps://git hub.com/bFr0zNq/MVOSegScale.</description>
      <author>example@mail.com (Hui Zhang, Zhiyang Wu, Qianqian Shangguan, Kang An)</author>
      <guid isPermaLink="false">2503.04235v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models</title>
      <link>http://arxiv.org/abs/2503.05638v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project webpage: https://trajectorycrafter.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TrajectoryCrafter 是一种新颖的方法，用于单目视频的相机轨迹重定向。&lt;h4&gt;背景&lt;/h4&gt;当前方法通常依赖于稀疏的多视角视频来实现相机轨迹的重新定向。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够精确控制用户指定的相机轨迹的新方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一个新的双流条件视频扩散模型，该模型同时将点云渲染和源视频作为条件输入，确保准确的视图转换和连贯的四维内容生成。&lt;h4&gt;主要发现&lt;/h4&gt;通过创新性的双重投影策略，使用结合了网络规模单目视频与静态多视角数据集的混合训练数据集，提高了在各种场景中的鲁棒性泛化能力。&lt;h4&gt;结论&lt;/h4&gt;在多视角和大规模单目视频上的广泛评估表明了我们方法的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：我们介绍了 TrajectoryCrafter，这是一种用于重定向单目视频中相机轨迹的新颖方法。通过分离确定性的视图转换与随机的内容生成，我们的方法能够实现对用户指定的相机路径的精确控制。我们提出了一种新颖的双流条件视频扩散模型，该模型同时将点云渲染和源视频作为条件输入，确保准确的视图转换和连贯的四维内容生成。通过创新性的双重投影策略，我们没有依赖稀缺的多视角视频，而是构建了一个结合了网络规模单目视频与静态多视角数据集的混合训练数据集，显著增强了在各种场景中的鲁棒性泛化能力。对多视角和大规模单目视频进行广泛的评估表明了我们的方法具有优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present TrajectoryCrafter, a novel approach to redirect cameratrajectories for monocular videos. By disentangling deterministic viewtransformations from stochastic content generation, our method achieves precisecontrol over user-specified camera trajectories. We propose a novel dual-streamconditional video diffusion model that concurrently integrates point cloudrenders and source videos as conditions, ensuring accurate view transformationsand coherent 4D content generation. Instead of leveraging scarce multi-viewvideos, we curate a hybrid training dataset combining web-scale monocularvideos with static multi-view datasets, by our innovative double-reprojectionstrategy, significantly fostering robust generalization across diverse scenes.Extensive evaluations on multi-view and large-scale monocular videosdemonstrate the superior performance of our method.</description>
      <author>example@mail.com (Mark YU, Wenbo Hu, Jinbo Xing, Ying Shan)</author>
      <guid isPermaLink="false">2503.05638v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Removing Geometric Bias in One-Class Anomaly Detection with Adaptive Feature Perturbation</title>
      <link>http://arxiv.org/abs/2503.05520v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in WACV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的异常检测方法，利用预训练模型的特征空间生成伪异常样本，并通过适应性线性特征扰动技术优化噪声分布。&lt;h4&gt;背景&lt;/h4&gt;一类别异常检测旨在识别不属于预先定义正常类别的对象。由于实践中训练数据缺乏真实异常样本，大多数方法依赖于从正常图像中合成的伪异常样本进行训练。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的生成伪异常样本的方法来克服现有技术在一般条件下的限制，并优化模型对正常数据结构的理解。&lt;h4&gt;方法&lt;/h4&gt;利用预训练模型冻结后的特征空间，采用新型适应性线性特征扰动技术生成伪异常特征。该技术根据每个样本调整噪声分布，进行衰减的线性干扰，并通过对比学习目标指导分类过程。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在标准和无几何偏差的数据集上，本文方法优于其他基线方法。&lt;h4&gt;结论&lt;/h4&gt;新提出的方法在克服现有异常检测模型限制方面显示出潜力，并且代码已经在公共仓库中开放访问。&lt;h4&gt;翻译&lt;/h4&gt;单类异常检测的目标是识别不属于预定义正常类别中的对象。实践中训练数据缺乏真实异常样本；因此，最先进的方法被训练来区分正常的和通过合成生成的伪异常数据。大多数方法使用数据增强技术从正常图像模拟异常情况。然而，表现最好的一些隐含利用了基准测试集中的几何偏差，这限制了它们在更一般条件下的适用性。其他则依赖于基本噪声方案，这些可能无法有效捕捉正常数据结构的本质。此外，多数情况下仍倾向于通过仅基于正常类别的端到端模型生成伪异常图像，并忽略信息的丰富表示形式。为克服这些局限性，我们考虑利用预训练模型提供的冻结且丰富的特征空间，并使用一种新颖的适应性线性特征扰动技术创建伪异常特征。该技术根据每一样本调整噪声分布，对特征向量施加衰减线性干扰，并进一步通过对比学习目标引导分类过程。在标准数据集和无几何偏差的数据集上进行的实验评估表明了我们方法相对于可比基线的优势。代码库可通过我们的公共仓库访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One-class anomaly detection aims to detect objects that do not belong to apredefined normal class. In practice training data lack those anomaloussamples; hence state-of-the-art methods are trained to discriminate betweennormal and synthetically-generated pseudo-anomalous data. Most methods use dataaugmentation techniques on normal images to simulate anomalies. However thebest-performing ones implicitly leverage a geometric bias present in thebenchmarking datasets. This limits their usability in more general conditions.Others are relying on basic noising schemes that may be suboptimal in capturingthe underlying structure of normal data. In addition most still favour theimage domain to generate pseudo-anomalies training models end-to-end from onlythe normal class and overlooking richer representations of the information. Toovercome these limitations we consider frozen yet rich feature spaces given bypretrained models and create pseudo-anomalous features with a novel adaptivelinear feature perturbation technique. It adapts the noise distribution to eachsample applies decaying linear perturbations to feature vectors and furtherguides the classification process using a contrastive learning objective.Experimental evaluation conducted on both standard and geometric bias-freedatasets demonstrates the superiority of our approach with respect tocomparable baselines. The codebase is accessible via our public repository.</description>
      <author>example@mail.com (Romain Hermary, Vincent Gaudillière, Abd El Rahman Shabayek, Djamila Aouada)</author>
      <guid isPermaLink="false">2503.05520v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>CACTUS: An Open Dataset and Framework for Automated Cardiac Assessment and Classification of Ultrasound Images Using Deep Transfer Learning</title>
      <link>http://arxiv.org/abs/2503.05604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文介绍了一种用于自动分类和评估心脏超声图像的深度学习框架，并提出了首个公开评级数据集CACTUS。&lt;h4&gt;背景&lt;/h4&gt;心脏超声扫描是心脏病诊断的重要工具，然而由于医疗数据有限，机器学习在这一领域的应用受限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能辅助医学专业人士进行心脏超声图像分类和评估的技术解决方案。&lt;h4&gt;方法&lt;/h4&gt;{'引入的数据集': 'CACTUS，包含了从心脏模拟器获取的各种视角和不同质量水平的图像。', '构建的框架': '由两个主要组件组成：第一个基于卷积神经网络(CNN)的心脏超声图像视图分类系统；第二个使用迁移学习(TL)，利用前一个组件的知识来创建用于评级和评估心脏图像模型。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能表现': '框架在分类任务中达到了99.43%的准确率，在评级任务中的误差低至0.3067。', '鲁棒性测试': '通过使用新视角的心脏图象进一步微调框架，并将其与现有架构进行比较，展示了其强大的通用性和适应性。', '专家问卷评估': '心脏专家通过对该框架处理实时扫描结果的问卷反馈，证实了它的实用性。'}&lt;h4&gt;结论&lt;/h4&gt;所提出的深度学习框架和CACTUS数据集为自动分类和评估心脏超声图像提供了有效的解决方案，并展示了在心脏病学中的潜在应用价值。&lt;h4&gt;翻译&lt;/h4&gt;心脏超声（US）检查是诊断心脏健康状况的常用技术，因此考虑自动化这些任务并帮助医疗专业人士进行心脏超声图象的分类与评估至关重要。机器学习方法因其成功应用于提升医学领域而被视为解决之道，尤其是在应对超声技师短缺问题上表现突出。然而，由于缺乏医学数据，特别是在心脏超声图像方面，机器学习的应用受到了很大限制。本文通过提出首个公开评级的数据集CACTUS来解决这一挑战，并提供了一个深度学习框架，其中包括两个主要部分：第一部分使用卷积神经网络对心脏视图进行分类；第二部分则采用迁移学习技术微调知识以创建用于评价和评估心脏图像的模型。该框架在分类与评分任务上表现卓越，在实际扫描结果处理上的有效性也得到了心脏病专家的好评。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cardiac ultrasound (US) scanning is a commonly used techniques in cardiologyto diagnose the health of the heart and its proper functioning. Therefore, itis necessary to consider ways to automate these tasks and assist medicalprofessionals in classifying and assessing cardiac US images. Machine learning(ML) techniques are regarded as a prominent solution due to their success innumerous applications aimed at enhancing the medical field, includingaddressing the shortage of echography technicians. However, the limitedavailability of medical data presents a significant barrier to applying ML incardiology, particularly regarding US images of the heart. This paper addressesthis challenge by introducing the first open graded dataset for CardiacAssessment and ClassificaTion of UltraSound (CACTUS), which is availableonline. This dataset contains images obtained from scanning a CAE Blue Phantomand representing various heart views and different quality levels, exceedingthe conventional cardiac views typically found in the literature. Additionally,the paper introduces a Deep Learning (DL) framework consisting of two maincomponents. The first component classifies cardiac US images based on the heartview using a Convolutional Neural Network (CNN). The second component usesTransfer Learning (TL) to fine-tune the knowledge from the first component andcreate a model for grading and assessing cardiac images. The frameworkdemonstrates high performance in both classification and grading, achieving upto 99.43% accuracy and as low as 0.3067 error, respectively. To showcase itsrobustness, the framework is further fine-tuned using new images representingadditional cardiac views and compared to several other state-of-the-artarchitectures. The framework's outcomes and performance in handling real-timescans were also assessed using a questionnaire answered by cardiac experts.</description>
      <author>example@mail.com (Hanae Elmekki, Ahmed Alagha, Hani Sami, Amanda Spilkin, Antonela Mariel Zanuttini, Ehsan Zakeri, Jamal Bentahar, Lyes Kadem, Wen-Fang Xie, Philippe Pibarot, Rabeb Mizouni, Hadi Otrok, Shakti Singh, Azzam Mourad)</author>
      <guid isPermaLink="false">2503.05604v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints</title>
      <link>http://arxiv.org/abs/2503.05684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文研究了低秩适应（LoRA）在预训练模型中的公平性问题，并提出了一系列无需直接访问敏感属性或其预测器的分布式微调方法。&lt;h4&gt;背景&lt;/h4&gt;现有的公平性感知微调方法依赖于对敏感属性或其预测器的直接访问，但在实践中这些信息往往受到严格的隐私保护，无法用于开发公平模型。&lt;h4&gt;目的&lt;/h4&gt;为了克服这一难题，论文提出了一种基于LoRA的方法集合，允许模型开发者和公平审计员在不共享敏感数据的情况下进行分布式微调，并评估了三种方法的效果。&lt;h4&gt;方法&lt;/h4&gt;研究中使用的是敏感属性遗忘、对抗训练以及正交性损失这三种方法。实验是在CelebA和UTK-Face数据集上利用ImageNet预训练的ViT-Bas模型进行的。&lt;h4&gt;主要发现&lt;/h4&gt;正交性损失在减少偏差的同时保持或提高了模型效用；对抗训练虽然在一些情况下改善了假阳性率平等性和人口统计学公平性，但敏感属性遗忘没有明显的好处。对于存在显著偏见的任务，分布式公平感知微调方法可以在不侵犯消费者隐私的情况下消除偏见，并且大多数情况还能提高模型的实用性。&lt;h4&gt;结论&lt;/h4&gt;通过引入这些新的微调策略，研究展示了如何在保护用户隐私的同时实现更加公平和实用的机器学习模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained foundation models can be adapted for specific tasks usingLow-Rank Adaptation (LoRA). However, the fairness properties of these adaptedclassifiers remain underexplored. Existing fairness-aware fine-tuning methodsrely on direct access to sensitive attributes or their predictors, but inpractice, these sensitive attributes are often held under strict consumerprivacy controls, and neither the attributes nor their predictors are availableto model developers, hampering the development of fair models. To address thisissue, we introduce a set of LoRA-based fine-tuning methods that can be trainedin a distributed fashion, where model developers and fairness auditorscollaborate without sharing sensitive attributes or predictors. In this paper,we evaluate three such methods - sensitive unlearning, adversarial training,and orthogonality loss - against a fairness-unaware baseline, using experimentson the CelebA and UTK-Face datasets with an ImageNet pre-trained ViT-Basemodel. We find that orthogonality loss consistently reduces bias whilemaintaining or improving utility, whereas adversarial training improves FalsePositive Rate Parity and Demographic Parity in some cases, and sensitiveunlearning provides no clear benefit. In tasks where significant biases arepresent, distributed fairness-aware fine-tuning methods can effectivelyeliminate bias without compromising consumer privacy and, in most cases,improve model utility.</description>
      <author>example@mail.com (Parameswaran Kamalaruban, Mark Anderson, Stuart Burrell, Maeve Madigan, Piotr Skalski, David Sutton)</author>
      <guid isPermaLink="false">2503.05684v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Joint 3D Point Cloud Segmentation using Real-Sim Loop: From Panels to Trees and Branches</title>
      <link>http://arxiv.org/abs/2503.05630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;现代果园采用结构化行种植，并划分为独立的面板区域，以提高管理水平。准确且高效的Panel到Tree和Branch（P2TB）点云联合分割对于机器人操作至关重要。&lt;h4&gt;背景&lt;/h4&gt;目前大多数分段方法专注于单一实例分割并且依赖于一系列深度网络来执行联任务。这阻碍了嵌入在数据中的层级信息的使用，导致错误累积以及注释和计算成本增加，限制了其应用于实际场景的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的方法，结合Real2Sim L-TreeGen用于训练数据生成，并设计了一个联合模型（J-P2TB）以完成P2TB任务。该方法旨在解决现有分割技术的局限性，提高效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;采用L-TreeGen进行仿真数据集的生成，基于此训练得到的J-P2TB模型通过零样本学习对真实面板点云进行联合分割。&lt;h4&gt;主要发现&lt;/h4&gt;相比代表性方法，在大多数分割指标上表现更好且参数量减少40%，证明了L-TreeGen在模型训练中的有效性以及J-P2TB模型在联合分割任务中的优异性能。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的Sim2Real结果强调了所提技术的准确性和效率，对于机器人自动化果园操作的发展和数字孪生技术的进步都有重大影响。&lt;h4&gt;翻译&lt;/h4&gt;现代果园采用结构化行种植，并划分为独立的面板区域，以提高管理水平。准确且高效的Panel到Tree和Branch（P2TB）点云联合分割对于机器人操作至关重要。然而，目前大多数分段方法专注于单一实例分割并且依赖于一系列深度网络来执行联任务。这阻碍了嵌入在数据中的层级信息的使用，导致错误累积以及注释和计算成本增加，限制了其应用于实际场景的能力。在本研究中，我们提出了一种结合Real2Sim L-TreeGen用于训练数据生成的方法，并设计了一个联合模型（J-P2TB）以完成P2TB任务。该方法通过零样本学习对真实面板点云进行联合分割。与代表性方法相比，在大多数分割指标上表现更好且参数量减少40%，证明了L-TreeGen在模型训练中的有效性以及J-P2TB模型在联合分割任务中的优异性能，表明其具有较高的准确度、效率和泛化能力，适用于实际应用。这些改进不仅大大促进了机器人自动化果园操作的发展，而且推动了数字孪生技术的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern orchards are planted in structured rows with distinct panel divisionsto improve management. Accurate and efficient joint segmentation of point cloudfrom Panel to Tree and Branch (P2TB) is essential for robotic operations.However, most current segmentation methods focus on single instancesegmentation and depend on a sequence of deep networks to perform joint tasks.This strategy hinders the use of hierarchical information embedded in the data,leading to both error accumulation and increased costs for annotation andcomputation, which limits its scalability for real-world applications. In thisstudy, we proposed a novel approach that incorporated a Real2Sim L-TreeGen fortraining data generation and a joint model (J-P2TB) designed for the P2TB task.The J-P2TB model, trained on the generated simulation dataset, was used forjoint segmentation of real-world panel point clouds via zero-shot learning.Compared to representative methods, our model outperformed them in mostsegmentation metrics while using 40% fewer learnable parameters. This Sim2Realresult highlighted the efficacy of L-TreeGen in model training and theperformance of J-P2TB for joint segmentation, demonstrating its strongaccuracy, efficiency, and generalizability for real-world applications. Theseimprovements would not only greatly benefit the development of robots forautomated orchard operations but also advance digital twin technology.</description>
      <author>example@mail.com (Tian Qiu, Ruiming Du, Nikolai Spine, Lailiang Cheng, Yu Jiang)</author>
      <guid isPermaLink="false">2503.05630v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Pretext Task Adversarial Learning for Unpaired Low-field to Ultra High-field MRI Synthesis</title>
      <link>http://arxiv.org/abs/2503.05339v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种从低场MRI合成高质量高场MRI数据的预训练任务对抗学习框架，以解决在缺少大量训练数据情况下的医学图像领域挑战。&lt;h4&gt;背景&lt;/h4&gt;由于高场磁共振成像（MRI）设备的稀缺性和高昂成本，在缺乏足够训练数据的情况下，将低场MRI转换为高场MRI具有重要的潜在价值。然而，这种合成过程中会遇到信号噪声比和空间分辨率的问题以及跨域对齐图像特征的同时保持解剖学准确性的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从低场MRI生成高质量高场MRI的数据增强方法，以提升下游任务（如分割）的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了一种预训练任务对抗(PTA)学习框架，包括三个关键步骤：(1) 切片级差距感知(SGP)网络通过对比学习对齐低场和高场数据集中的切片不一致；(2) 局部结构修正(LSC)网络通过恢复局部旋转和掩膜图像提取局部位点；(3) 预训练任务引导对抗性培训过程引入额外监督，并结合判别器提升生成图像的真实性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在从低场MRI到超高场MRI的转换实验中表现出色，达到了最先进的性能（FID为16.892，在IS为1.933，MS-SSIM为0.324）。&lt;h4&gt;结论&lt;/h4&gt;所提出的预训练任务对抗学习框架能够在缺少大量高分辨率数据的情况下合成出高质量高场MRI图像，从而促进下游医学影像处理任务的改进。&lt;h4&gt;翻译&lt;/h4&gt;给定高场磁共振成像（MRI）设备的稀缺性和高昂成本，在缺乏足够训练数据情况下从低场MRI合成高场MRI具有重要潜力。然而，这类转换面临挑战，如跨域对齐图像特征的同时保持解剖学准确性以及提升细节。为解决这些问题，我们提出了一种基于预训练任务对抗学习框架的方法来实现从低场MRI到高场MRI的高质量生成数据增强过程。实验验证表明该方法在合成高场MRI方面达到了顶尖水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given the scarcity and cost of high-field MRI, the synthesis of high-fieldMRI from low-field MRI holds significant potential when there is limited datafor training downstream tasks (e.g. segmentation). Low-field MRI often suffersfrom a reduced signal-to-noise ratio (SNR) and spatial resolution compared tohigh-field MRI. However, synthesizing high-field MRI data presents challenges.These involve aligning image features across domains while preservinganatomical accuracy and enhancing fine details. To address these challenges, wepropose a Pretext Task Adversarial (PTA) learning framework for high-field MRIsynthesis from low-field MRI data. The framework comprises three processes: (1)The slice-wise gap perception (SGP) network aligns the slice inconsistencies oflow-field and high-field datasets based on contrastive learning. (2) The localstructure correction (LSC) network extracts local structures by restoring thelocally rotated and masked images. (3) The pretext task-guided adversarialtraining process introduces additional supervision and incorporates adiscriminator to improve image realism. Extensive experiments on low-field toultra high-field task demonstrate the effectiveness of our method, achievingstate-of-the-art performance (16.892 in FID, 1.933 in IS, and 0.324 inMS-SSIM). This enables the generation of high-quality high-field-like MRI datafrom low-field MRI data to augment training datasets for downstream tasks. Thecode is available at:https://github.com/Zhenxuan-Zhang/PTA4Unpaired_HF_MRI_SYN.</description>
      <author>example@mail.com (Zhenxuan Zhang, Peiyuan Jing, Coraline Beitone, Jiahao Huang, Zhifan Gao, Guang Yang, Pete Lally)</author>
      <guid isPermaLink="false">2503.05339v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>From Theory to Application: A Practical Introduction to Neural Operators in Scientific Computing</title>
      <link>http://arxiv.org/abs/2503.05598v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  53 pages, 17 figures, Github repository:  https://github.com/CEADpx/neural_operators&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述探讨了用于近似参数偏微分方程（PDE）解的神经算子架构，强调高级概念和实际实施策略。&lt;h4&gt;背景&lt;/h4&gt;文章涵盖了基础模型，如深度操作网络（DeepONet）、基于主成分分析的神经网络（PCANet）以及傅立叶神经算子（FNO），并对其核心方法论和性能进行比较研究。&lt;h4&gt;目的&lt;/h4&gt;综述旨在探索这些架构在经典线性参数PDE上的应用，并探讨它们作为贝叶斯推理问题中代理的有效性的加速后验推断的能力。&lt;h4&gt;方法&lt;/h4&gt;该文章讨论了两个经典的线性参数PDE：泊松方程和线性弹性变形，以展示不同神经算子的性能和有效性。&lt;h4&gt;主要发现&lt;/h4&gt;除了解决正向问题外，综述还深入探讨了在贝叶斯推理中应用神经算子作为代理的有效性和准确性保持能力。&lt;h4&gt;结论&lt;/h4&gt;文章讨论了当前面临的挑战，如控制预测准确性和泛化性，并提出了一些新兴策略来应对这些挑战，例如基于残差的误差校正和多级训练。&lt;h4&gt;翻译&lt;/h4&gt;摘要：本文综述探索了一系列神经算子架构用于参数偏微分方程（PDE）解的近似，强调高级概念和实际实施策略。研究涵盖了基础模型如深度操作网络（DeepONet）、基于主成分分析的神经网络（PCANet）及傅立叶神经算子（FNO），提供了关于其核心方法论和性能表现的比较见解。这些架构在两个经典线性参数PDE上得到展示：泊松方程和线弹性变形。除了正向问题求解，综述还探讨了将神经算子作为代理应用到贝叶斯推理问题中的有效性，展示了它们加速后验推断的同时保持准确性的能力。文章通过讨论当前挑战，特别是控制预测准确性及泛化性，得出结论，并概述了解决这些问题的新兴策略，如基于残差的误差校正和多级训练方法。本文可作为实施神经算子并将其整合到科学计算工作流程中的全面指南。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This focused review explores a range of neural operator architectures forapproximating solutions to parametric partial differential equations (PDEs),emphasizing high-level concepts and practical implementation strategies. Thestudy covers foundational models such as Deep Operator Networks (DeepONet),Principal Component Analysis-based Neural Networks (PCANet), and Fourier NeuralOperators (FNO), providing comparative insights into their core methodologiesand performance. These architectures are demonstrated on two classical linearparametric PDEs: the Poisson equation and linear elastic deformation. Beyondforward problem-solving, the review delves into applying neural operators assurrogates in Bayesian inference problems, showcasing their effectiveness inaccelerating posterior inference while maintaining accuracy. The paperconcludes by discussing current challenges, particularly in controllingprediction accuracy and generalization. It outlines emerging strategies toaddress these issues, such as residual-based error correction and multi-leveltraining. This review can be seen as a comprehensive guide to implementingneural operators and integrating them into scientific computing workflows.</description>
      <author>example@mail.com (Prashant K. Jha)</author>
      <guid isPermaLink="false">2503.05598v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>opXRD: Open Experimental Powder X-ray Diffraction Database</title>
      <link>http://arxiv.org/abs/2503.05577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Powder X-ray Diffraction (pXRD)实验在材料结构表征中的重要性，并提出了一种通过机器学习实现自动化分析的方法，以解决当前数据分析过程中的瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;虽然pXRD实验广泛应用于材料科学中，但其结果的自动解析仍然面临许多挑战。现有的机器学习模型通常基于模拟数据训练，这限制了它们在实际实验数据上的表现能力。&lt;h4&gt;目的&lt;/h4&gt;构建一个公开可用的、易于访问的实验性粉末衍射图谱数据库（opXRD），以帮助改进机器学习模型对实验数据的处理性能。&lt;h4&gt;方法&lt;/h4&gt;收集了大量的pXRD图谱，包括2179个标记的数据集，这些数据来自各种材料类别。该数据库旨在为未来的自我驱动实验室提供支持。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用opXRD数据库中的标注和未标注数据，研究人员可以评估模型对实验数据的表现，并改进机器学习算法的性能。&lt;h4&gt;结论&lt;/h4&gt;这项工作有望促进机器学习研究向完全自动化的pXRD数据分析方向发展，从而为未来自我驱动材料实验室的发展奠定基础。&lt;h4&gt;翻译&lt;/h4&gt;粉末X射线衍射(pXRD)实验是材料结构表征的基础。尽管它们应用广泛，但分析pXRD衍射图仍然是自动化和高通量发现的瓶颈问题。机器学习有望通过实现自动化的粉末衍射分析来解决这一瓶颈。然而，在这个领域中应用机器学习的一个显著困难是没有足够的实验数据集，这使得研究人员主要依赖于模拟数据进行训练。但是，基于模拟pXRD模式训练出来的模型在实际实验模式上的泛化能力有限，特别是在处理噪声水平高和背景高的低质量实验模式时表现不佳。通过开放实验性粉末X射线衍射数据库（opXRD），我们提供了一个公开可用且易于访问的标记和未标记实验粉末衍射图数据集。标记的opXRD数据可以用于评估模型在实验数据上的性能，而未标记的数据可以帮助改进模型在实验数据上的表现，例如通过迁移学习方法。我们收集了umpatternsdiffractograms，其中2179个被标注，涵盖广泛的材料类别。我们希望这项正在进行的工作能够引导机器学习研究向pXRD数据分析的完全自动化发展，并因此为未来的自我驱动材料实验室提供支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Powder X-ray diffraction (pXRD) experiments are a cornerstone for materialsstructure characterization. Despite their widespread application, analyzingpXRD diffractograms still presents a significant challenge to automation and abottleneck in high-throughput discovery in self-driving labs. Machine learningpromises to resolve this bottleneck by enabling automated powder diffractionanalysis. A notable difficulty in applying machine learning to this domain isthe lack of sufficiently sized experimental datasets, which has constrainedresearchers to train primarily on simulated data. However, models trained onsimulated pXRD patterns showed limited generalization to experimental patterns,particularly for low-quality experimental patterns with high noise levels andelevated backgrounds. With the Open Experimental Powder X-Ray DiffractionDatabase (opXRD), we provide an openly available and easily accessible datasetof labeled and unlabeled experimental powder diffractograms. Labeled opXRD datacan be used to evaluate the performance of models on experimental data andunlabeled opXRD data can help improve the performance of models on experimentaldata, e.g. through transfer learning methods. We collected \numpatternsdiffractograms, 2179 of them labeled, from a wide spectrum of materialsclasses. We hope this ongoing effort can guide machine learning research towardfully automated analysis of pXRD data and thus enable future self-drivingmaterials labs.</description>
      <author>example@mail.com (Daniel Hollarek, Henrik Schopmans, Jona Östreicher, Jonas Teufel, Bin Cao, Adie Alwen, Simon Schweidler, Mriganka Singh, Tim Kodalle, Hanlin Hu, Gregoire Heymans, Maged Abdelsamie, Arthur Hardiagon, Alexander Wieczorek, Siarhei Zhuk, Ruth Schwaiger, Sebastian Siol, François-Xavier Coudert, Moritz Wolf, Carolin M. Sutter-Fella, Ben Breitung, Andrea M. Hodge, Tong-yi Zhang, Pascal Friederich)</author>
      <guid isPermaLink="false">2503.05577v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>FMT:A Multimodal Pneumonia Detection Model Based on Stacking MOE Framework</title>
      <link>http://arxiv.org/abs/2503.05626v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一个灵活的多模态变换器(FMT)，通过结合ResNet-50和BERT进行联合表示学习，并采用动态掩码注意力策略来模拟临床模式丢失，从而提高诊断肺炎的准确性。&lt;h4&gt;背景&lt;/h4&gt;人工智能在医学图像分析中显示出改善肺炎诊断准确性的潜力。然而，传统多模态方法难以应对实际挑战如数据不完整和模式缺失等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的模型FMT来解决现有问题，并通过小样本多模态肺炎数据集验证其性能。&lt;h4&gt;方法&lt;/h4&gt;使用ResNet-50和BERT进行联合表示学习，应用动态掩码注意力策略模拟临床模式丢失以提高鲁棒性。最后采用序列专家混合(MOE)架构实现多层次决策精炼。&lt;h4&gt;主要发现&lt;/h4&gt;在多模态肺炎数据集上的评估表明FMT取得了94%的准确率、95%的召回率和93%的F1分数，优于单模基线(ResNet: 89%，BERT: 79%)及医学基准CheXMed (90%)。&lt;h4&gt;结论&lt;/h4&gt;该研究为资源受限条件下的多模式肺炎诊断提供了一种可扩展解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence has shown the potential to improve diagnosticaccuracy through medical image analysis for pneumonia diagnosis. However,traditional multimodal approaches often fail to address real-world challengessuch as incomplete data and modality loss. In this study, a Flexible MultimodalTransformer (FMT) was proposed, which uses ResNet-50 and BERT for jointrepresentation learning, followed by a dynamic masked attention strategy thatsimulates clinical modality loss to improve robustness; finally, a sequentialmixture of experts (MOE) architecture was used to achieve multi-level decisionrefinement. After evaluation on a small multimodal pneumonia dataset, FMTachieved state-of-the-art performance with 94% accuracy, 95% recall, and 93% F1score, outperforming single-modal baselines (ResNet: 89%; BERT: 79%) and themedical benchmark CheXMed (90%), providing a scalable solution for multimodaldiagnosis of pneumonia in resource-constrained medical settings.</description>
      <author>example@mail.com (Jingyu Xu, Yang Wang)</author>
      <guid isPermaLink="false">2503.05626v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Riemannian Metric Learning: Closer to You than You Imagine</title>
      <link>http://arxiv.org/abs/2503.05321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要主题&lt;/h4&gt;黎曼度量学习在机器学习中的新兴应用及其理论基础。&lt;h4&gt;背景&lt;/h4&gt;传统的距离度量学习方法依赖于欧氏空间中的全局距离，但往往无法准确捕捉数据的内在几何结构。为此，引入了基于微分几何的黎曼度量学习方法，它通过利用数据背后的黎曼流形来建模。&lt;h4&gt;目的&lt;/h4&gt;回顾经典度量学习与黎曼几何之间的联系，并提供一种结构化且易于理解的关键方法、应用和近期进展综述。&lt;h4&gt;主要贡献&lt;/h4&gt;论证了黎曼度量学习不仅是一种技术改进，而且是对数据表示方式的基本转变。&lt;h4&gt;应用场景&lt;/h4&gt;['因果推断', '最优传输', '生成模型', '表征学习']&lt;h4&gt;结论&lt;/h4&gt;黎曼度量学习为机器学习提供了新的视角和工具，并展示了其在多个领域的广泛应用前景。鼓励研究者和实践人员探索这一领域，因为它既贴近理论又接近实际应用。&lt;h4&gt;翻译&lt;/h4&gt;黎曼测度学习是机器学习的一个新兴领域，它通过微分几何的方法来建模数据背后的黎曼流形结构，为复杂的数据编码提供了新的途径。与传统的距离度量学习相比，这种方法在捕捉数据内在的几何特性方面表现出了显著的优势，并已在因果推断、最优传输、生成模型和表示学习等多个领域得到了应用。本文综述了经典测度学习与黎曼几何之间的联系，介绍了该领域的关键技术、方法及其最新进展，指出黎曼度量学习不仅仅是技术上的改进，而是对数据表示的一种基本转变，这使得该研究成为希望探索黎曼度量学习的研究者和实践者的宝贵资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Riemannian metric learning is an emerging field in machine learning,unlocking new ways to encode complex data structures beyond traditionaldistance metric learning. While classical approaches rely on global distancesin Euclidean space, they often fall short in capturing intrinsic data geometry.Enter Riemannian metric learning: a powerful generalization that leveragesdifferential geometry to model the data according to their underlyingRiemannian manifold. This approach has demonstrated remarkable success acrossdiverse domains, from causal inference and optimal transport to generativemodeling and representation learning. In this review, we bridge the gap betweenclassical metric learning and Riemannian geometry, providing a structured andaccessible overview of key methods, applications, and recent advances. We arguethat Riemannian metric learning is not merely a technical refinement but afundamental shift in how we think about data representations. Thus, this reviewshould serve as a valuable resource for researchers and practitionersinterested in exploring Riemannian metric learning and convince them that it iscloser to them than they might imagine-both in theory and in practice.</description>
      <author>example@mail.com (Samuel Gruffaz, Josua Sassen)</author>
      <guid isPermaLink="false">2503.05321v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>LiDAR-enhanced 3D Gaussian Splatting Mapping</title>
      <link>http://arxiv.org/abs/2503.05425v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新型的基于3D Gaussian Splatting（3DGS）的地图构建框架LiGSM，该框架通过整合激光雷达数据提高了三维场景地图绘制的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;传统的基于图像的方法在处理传感器偏移变化时不够灵活，并且初始点较少，影响了三维场景重建的效果。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合激光雷达和图像信息来提高3DGS框架性能的新方法。&lt;h4&gt;方法&lt;/h4&gt;LiGSM采用了一种综合损失函数，该函数可以同时利用图像和激光雷达数据估计姿态并优化外参。此外，还使用激光雷达点云对3DGS进行初始化，并在场景渲染时引入深度图监督来确保准确的几何结构和光度特性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在公共和自收集的数据集上，LiGSM在姿态跟踪和场景渲染方面都优于比较方法。&lt;h4&gt;结论&lt;/h4&gt;通过将激光雷达数据与图像信息相结合，LiGSM能够提供更精确、鲁棒性更强的三维地图绘制方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为：This paper introduces LiGSM, a novel LiDAR-enhanced 3D Gaussian Splatting (3DGS) mapping framework that improves the accuracy and robustness of 3D scene mapping by integrating LiDAR data. LiGSM constructs joint loss from images and LiDAR point clouds to estimate poses and optimize their extrinsic parameters, enabling dynamic adaptation to variations in sensor alignment. Furthermore, it leverages LiDAR point clouds to initialize 3DGS, providing a denser and more reliable starting points compared to sparse SfM points. In scene rendering, the framework augments standard image-based supervision with depth maps generated from LiDAR projections, ensuring an accurate scene representation in both geometry and photometry. Experiments on public and self-collected datasets demonstrate that LiGSM outperforms comparative methods in pose tracking and scene rendering.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces LiGSM, a novel LiDAR-enhanced 3D Gaussian Splatting(3DGS) mapping framework that improves the accuracy and robustness of 3D scenemapping by integrating LiDAR data. LiGSM constructs joint loss from images andLiDAR point clouds to estimate the poses and optimize their extrinsicparameters, enabling dynamic adaptation to variations in sensor alignment.Furthermore, it leverages LiDAR point clouds to initialize 3DGS, providing adenser and more reliable starting points compared to sparse SfM points. Inscene rendering, the framework augments standard image-based supervision withdepth maps generated from LiDAR projections, ensuring an accurate scenerepresentation in both geometry and photometry. Experiments on public andself-collected datasets demonstrate that LiGSM outperforms comparative methodsin pose tracking and scene rendering.</description>
      <author>example@mail.com (Jian Shen, Huai Yu, Ji Wu, Wen Yang, Gui-Song Xia)</author>
      <guid isPermaLink="false">2503.05425v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Discrete Contrastive Learning for Diffusion Policies in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2503.05229v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于对比学习的方法，从现有的人类驾驶数据中提取驾驶风格字典，并通过量化离散化这些风格。利用这种方法训练条件扩散策略来模拟人类驾驶员的行为。&lt;h4&gt;背景&lt;/h4&gt;自驾车测试需要准确和丰富的人类驾驶行为的仿真，然而由于人类驾驶风格的高度多样性和变化性，这一任务仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能有效学习人类驾驶行为的方法，以提高自动驾驶车辆的评估和改进技术的有效性和真实性。&lt;h4&gt;方法&lt;/h4&gt;利用对比学习从现有的人类驾驶数据中提取一个驾驶风格字典，并通过量化离散化这些风格。然后使用这些风格来训练条件扩散策略，用以仿真人类驾驶员的行为。&lt;h4&gt;主要发现&lt;/h4&gt;生成的行为比基于机器学习的方法更安全和更具人类特性。&lt;h4&gt;结论&lt;/h4&gt;该方法有望提高自动驾驶车辆评估的现实性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;从数据中学习准确且丰富的模拟人类驾驶行为用于自驾车测试仍然是一个挑战，因为人类驾驶风格具有高度多样性和变异性。我们通过提出一种利用对比学习从现有的人类驾驶数据中提取驾驶风格字典的新方法来解决这一问题。我们将这些风格进行量化离散化，并使用它们来学习条件扩散策略以模拟人类驾驶员的行为。我们的实证评估表明，与基于机器学习的基准方法相比，生成的行为更加安全和人性化。我们认为这将有可能提高自驾车性能评估的真实性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning to perform accurate and rich simulations of human driving behaviorsfrom data for autonomous vehicle testing remains challenging due to humandriving styles' high diversity and variance. We address this challenge byproposing a novel approach that leverages contrastive learning to extract adictionary of driving styles from pre-existing human driving data. Wediscretize these styles with quantization, and the styles are used to learn aconditional diffusion policy for simulating human drivers. Our empiricalevaluation confirms that the behaviors generated by our approach are both saferand more human-like than those of the machine-learning-based baseline methods.We believe this has the potential to enable higher realism and more effectivetechniques for evaluating and improving the performance of autonomous vehicles.</description>
      <author>example@mail.com (Kalle Kujanpää, Daulet Baimukashev, Farzeen Munir, Shoaib Azam, Tomasz Piotr Kucner, Joni Pajarinen, Ville Kyrki)</author>
      <guid isPermaLink="false">2503.05229v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Statistical Deficiency for Task Inclusion Estimation</title>
      <link>http://arxiv.org/abs/2503.05491v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;机器学习中的任务是评估当前模型能力的最自然对象。目前的趋势是建立能够处理任何任务的一般化模型。&lt;h4&gt;目的&lt;/h4&gt;提出一种理论基础架构来定义任务，并从统计缺陷的角度计算两个任务之间的包含关系。&lt;h4&gt;方法&lt;/h4&gt;使用信息充足性作为可操作代理，估计任务之间包含程度，并在合成数据上验证其有效性。此外，还利用该方法重建了经典的NLP处理流程。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种理论框架来定义机器学习中的“任务”概念，并提供了一种计算两个任务间统计缺陷差异的方法。&lt;h4&gt;结论&lt;/h4&gt;研究为理解和评估机器学习中不同任务之间的关系提供了新的工具和视角，特别是在转移学习和多任务学习领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tasks are central in machine learning, as they are the most natural objectsto assess the capabilities of current models. The trend is to build generalmodels able to address any task. Even though transfer learning and multitasklearning try to leverage the underlying task space, no well-founded tools areavailable to study its structure. This study proposes a theoretically groundedsetup to define the notion of task and to compute the {\bf inclusion} betweentwo tasks from a statistical deficiency point of view. We propose a tractableproxy as information sufficiency to estimate the degree of inclusion betweentasks, show its soundness on synthetic data, and use it to reconstructempirically the classic NLP pipeline.</description>
      <author>example@mail.com (Loïc Fosse, Frédéric Béchet, Benoît Favre, Géraldine Damnati, Gwénolé Lecorvé, Maxime Darrin, Philippe Formont, Pablo Piantanida)</author>
      <guid isPermaLink="false">2503.05491v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Video Music Recommendation with Transformer-Driven Audio-Visual Embeddings</title>
      <link>http://arxiv.org/abs/2503.05008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2024 IEEE 5th International Symposium on the Internet of Sounds  (IS2), Erlangen, Germany, 2024, pp. 1-6&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自监督学习和对比学习的新方法，用于自动推荐适合视频的音频。&lt;h4&gt;背景&lt;/h4&gt;合适的配乐可以帮助视频更好地传达其内容，并提供更好的沉浸式体验。传统的手动标记过程效率低下且成本高昂。&lt;h4&gt;目的&lt;/h4&gt;通过利用自我监督学习和对比学习，开发一种能够为视频自动生成匹配音频的方法，从而减少人工标注的需求。&lt;h4&gt;方法&lt;/h4&gt;采用双分支交叉模态嵌入模型将音频和视频特征映射到一个公共的低维空间。该方法评估不同音视频对之间的适配性，并通过逆距离度量来实现。此外，还对比分析了多种时间编码方式的有效性，重点突出了Transformer在处理音频视频匹配任务中的时间信息方面的优越性能。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型TIVM（结合了变压器编码器和InfoNCE损失）显著提高了音视频匹配的性能，并超越了传统的方法。该研究强调了Transformers在管理音频-视频配对的时间信息方面的能力，同时展示了使用双分支交叉模态嵌入的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的自监督学习方法为自动推荐适合视频内容的音频提供了一个有效的解决方案，极大地提升了用户体验和效率。&lt;h4&gt;翻译&lt;/h4&gt;A fitting soundtrack can help a video better convey its content and provide abetter immersive experience. This paper introduces a novel approach utilizingself-supervised learning and contrastive learning to automatically recommendaudio for video content, thereby eliminating the need for manual labeling.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IS262782.2024.10704086&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A fitting soundtrack can help a video better convey its content and provide abetter immersive experience. This paper introduces a novel approach utilizingself-supervised learning and contrastive learning to automatically recommendaudio for video content, thereby eliminating the need for manual labeling. Weuse a dual-branch cross-modal embedding model that maps both audio and videofeatures into a common low-dimensional space. The fit of various audio-videopairs can then be mod-eled as inverse distance measure. In addition, acomparative analysis of various temporal encoding methods is presented,emphasizing the effectiveness of transformers in managing the temporalinformation of audio-video matching tasks. Through multiple experiments, wedemonstrate that our model TIVM, which integrates transformer encoders andusing InfoN Celoss, significantly improves the performance of audio-videomatching and surpasses traditional methods.</description>
      <author>example@mail.com (Shimiao Liu, Alexander Lerch)</author>
      <guid isPermaLink="false">2503.05008v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Robust Multimodal Learning for Ophthalmic Disease Grading via Disentangled Representation</title>
      <link>http://arxiv.org/abs/2503.05319v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Essence-Point和Disentangle Representation Learning (EDRL)策略，旨在改进眼科学中多模态数据的诊断准确性。&lt;h4&gt;背景&lt;/h4&gt;眼科医生常依赖多模态数据来提高诊断准确度，但由于医疗设备不足及数据隐私问题，实际应用中难以获得完整的多模态数据。传统深度学习方法通过在潜在空间学习表示来解决这些问题。&lt;h4&gt;目的&lt;/h4&gt;提出EDRL策略以克服现有方法的两个关键限制：冗余信息和重叠表示。&lt;h4&gt;方法&lt;/h4&gt;EDRL策略整合了自蒸馏机制，并将其纳入端到端框架，用于增强特征选择和解耦。Essence-Point模块选取提升疾病分级性能的判别性特征；Disentangled模块将多模态数据分离成通用与独特表征，减少特性缠结。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在多模态眼科数据集上，EDRL策略显著优于当前最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的EDRL框架对于提高眼科学中基于多模态数据的诊断准确性和解释性具有潜在的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;本文探讨了眼科医生如何依赖多模态数据以提升诊断准确性。然而，在实际应用中，由于医疗设备限制及数据隐私顾虑，难以获取完整的多模态数据。传统的深度学习方法通常通过在潜在空间学习表示来应对这些挑战，但文章指出这种方法存在两个关键局限：复杂模态中的任务无关冗余信息导致潜在空间表征显著冗余；重叠的多模态表示使得提取每个模态的独特特征变得困难。为克服这些问题，作者提出了Essence-Point和Disentangle Representation Learning (EDRL)策略，该方法结合自蒸馏机制，并将其融入端到端框架中以增强特征选择及解耦能力。实验结果表明，在多模态眼科数据集上，EDRL策略显著优于当前最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper discusses how ophthalmologists often rely on multimodal data toimprove diagnostic accuracy. However, complete multimodal data is rare inreal-world applications due to a lack of medical equipment and concerns aboutdata privacy. Traditional deep learning methods typically address these issuesby learning representations in latent space. However, the paper highlights twokey limitations of these approaches: (i) Task-irrelevant redundant information(e.g., numerous slices) in complex modalities leads to significant redundancyin latent space representations. (ii) Overlapping multimodal representationsmake it difficult to extract unique features for each modality. To overcomethese challenges, the authors propose the Essence-Point and DisentangleRepresentation Learning (EDRL) strategy, which integrates a self-distillationmechanism into an end-to-end framework to enhance feature selection anddisentanglement for more robust multimodal learning. Specifically, theEssence-Point Representation Learning module selects discriminative featuresthat improve disease grading performance. The Disentangled RepresentationLearning module separates multimodal data into modality-common andmodality-unique representations, reducing feature entanglement and enhancingboth robustness and interpretability in ophthalmic disease diagnosis.Experiments on multimodal ophthalmology datasets show that the proposed EDRLstrategy significantly outperforms current state-of-the-art methods.</description>
      <author>example@mail.com (Xinkun Wang, Yifang Wang, Senwei Liang, Feilong Tang, Chengzhi Liu, Ming Hu, Chao Hu, Junjun He, Zongyuan Ge, Imran Razzak)</author>
      <guid isPermaLink="false">2503.05319v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Separability Membrane: 3D Active Contour for Point Cloud Surface Reconstruction</title>
      <link>http://arxiv.org/abs/2503.05217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种名为Separability Membrane的新方法，用于从3D点云对象中提取表面。&lt;h4&gt;背景&lt;/h4&gt;现有的三维物体表面提取技术通常需要训练数据或转换为体积表示，并且在存在噪声或异常值的情况下难以准确重建模糊的边界。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需训练数据和体积表示转换的方法来精确提取并重建3D点云中的对象表面。&lt;h4&gt;方法&lt;/h4&gt;通过最大化基于Fisher比率计算特征（如强度、颜色或局部密度）的类分离度，确定一个三维物体表面作为其内部与外部区域之间差异最大的边界。使用自适应B样条曲面控制3D模型的刚性，并根据本地和全局分离度调整属性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够准确地重建模糊且复杂情况下的对象表面边界（如存在噪声或异常值），展示了高鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;实验在合成数据集和3DNet数据集中验证了Separability Membrane的有效性，证明了其在各种条件下的稳健性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为分离膜的方法，这是一种用于从三维点云对象中提取表面的鲁棒型三维主动轮廓。该方法定义了一个三维物体的表面为内部与外部区域之间基于Fisher比率计算特征（如强度、颜色或局部密度）的最大类分离度边界。分离膜通过控制自适应B样条曲面来精确识别3D物体的确切表面，这种模型根据本地和全局分离度调整其属性，在无需任何训练数据的情况下准确重建模糊的表面边界，并且不转换为体积表示。通过对合成三维点云数据集和3DNet数据集进行评估，显示了该膜的有效性和鲁棒性，即使在存在不同条件时也是如此。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes Separability Membrane, a robust 3D active contour forextracting a surface from 3D point cloud object. Our approach defines thesurface of a 3D object as the boundary that maximizes the separability of pointfeatures, such as intensity, color, or local density, between its inner andouter regions based on Fisher's ratio. Separability Membrane identifies theexact surface of a 3D object by maximizing class separability while controllingthe rigidity of the 3D surface model with an adaptive B-spline surface thatadjusts its properties based on the local and global separability. A keyadvantage of our method is its ability to accurately reconstruct surfaceboundaries even when they are ambiguous due to noise or outliers, withoutrequiring any training data or conversion to volumetric representation.Evaluations on a synthetic 3D point cloud dataset and the 3DNet datasetdemonstrate the membrane's effectiveness and robustness under diverseconditions.</description>
      <author>example@mail.com (Gulpi Qorik Oktagalu Pratamasunu, Guoqing Hao, Kazuhiro Fukui)</author>
      <guid isPermaLink="false">2503.05217v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Quantum-PEFT: Ultra parameter-efficient fine-tuning</title>
      <link>http://arxiv.org/abs/2503.05431v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的量子计算驱动的参数高效微调方法（Quantum-PEFT），该方法与传统的低秩适应法不同，在保持性能的同时提高了参数效率。&lt;h4&gt;背景&lt;/h4&gt;现有的参数高效微调方法，如低秩适应（LoRA），虽然在减少训练参数数量方面有一定效果，但随着模型维度的增长，其优势逐渐减弱。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的量子计算单元化参数调整方式Quantum-PEFT，以实现在保持竞争力性能的同时大幅度提升参数效率。&lt;h4&gt;方法&lt;/h4&gt;利用Pauli参数化实现了量子化的全秩且高效的参数化方式。这种方法的训练参数数量随着环境维度的增长只呈对数增长，而非线性增长。&lt;h4&gt;主要发现&lt;/h4&gt;Quantum-PEFT在各种语言和视觉领域的迁移学习基准测试中展示出显著的参数效率优势，并且可实现比最低秩LoRA更小的训练参数量。&lt;h4&gt;结论&lt;/h4&gt;提出的方法不仅在保持性能的同时极大提升了模型的参数效率，而且证明了量子计算在解决大规模机器学习问题中的潜在价值。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces Quantum-PEFT that leverages quantum computations forparameter-efficient fine-tuning (PEFT). Unlike other additive PEFT methods,such as low-rank adaptation (LoRA), Quantum-PEFT exploits an underlyingfull-rank yet surprisingly parameter efficient quantum unitaryparameterization. With the use of Pauli parameterization, the number oftrainable parameters grows only logarithmically with the ambient dimension, asopposed to linearly as in LoRA-based PEFT methods. Quantum-PEFT achievesvanishingly smaller number of trainable parameters than the lowest-rank LoRA asdimensions grow, enhancing parameter efficiency while maintaining a competitiveperformance. We apply Quantum-PEFT to several transfer learning benchmarks inlanguage and vision, demonstrating significant advantages in parameterefficiency.</description>
      <author>example@mail.com (Toshiaki Koike-Akino, Francesco Tonin, Yongtao Wu, Frank Zhengqing Wu, Leyla Naz Candogan, Volkan Cevher)</author>
      <guid isPermaLink="false">2503.05431v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>The Society of HiveMind: Multi-Agent Optimization of Foundation Model Swarms to Unlock the Potential of Collective Intelligence</title>
      <link>http://arxiv.org/abs/2503.05473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages (excl. appendix)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个基于多智能体系统的框架，用于解决大型语言模型在可访问性和扩展性方面的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的人工智能基础模型通常由大型语言模型表示，这些模型在处理需要丰富逻辑推理的任务时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够模仿自然界中动物群落行为的多智能体系统框架，以提高任务完成效率和能力。&lt;h4&gt;方法&lt;/h4&gt;创建了一个名为“蜂巢心智社会”（Society of HiveMind，SOHM）的新框架。该框架通过模拟现代进化理论中的动物群体行为来协调多个AI基础模型之间的交互。&lt;h4&gt;主要发现&lt;/h4&gt;对于主要是现实世界知识的任务，SOHM提供的益处有限；而对于需要大量逻辑推理的任务，它表现出显著改进。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，结合多种不同的AI基础模型能够形成一种具有自我完善能力的人工群集智能系统。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已由英文翻译成中文并进行了总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent systems address issues of accessibility and scalability ofartificial intelligence (AI) foundation models, which are often represented bylarge language models. We develop a framework - the "Society of HiveMind"(SOHM) - that orchestrates the interaction between multiple AI foundationmodels, imitating the observed behavior of animal swarms in nature by followingmodern evolutionary theories. On the one hand, we find that the SOHM provides anegligible benefit on tasks that mainly require real-world knowledge. On theother hand, we remark a significant improvement on tasks that require intensivelogical reasoning, indicating that multi-agent systems are capable ofincreasing the reasoning capabilities of the collective compared to theindividual agents. Our findings demonstrate the potential of combining amultitude of diverse AI foundation models to form an artificial swarmintelligence capable of self-improvement through interactions with a givenenvironment.</description>
      <author>example@mail.com (Noah Mamie, Susie Xi Rao)</author>
      <guid isPermaLink="false">2503.05473v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Path Pooling: Train-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation</title>
      <link>http://arxiv.org/abs/2503.05203v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的知识图谱增强的生成模型改进策略——路径池化（Path Pooling），旨在解决大型语言模型在实际应用中的幻觉和知识不足问题。&lt;h4&gt;背景&lt;/h4&gt;尽管大规模语言模型在许多任务上表现出色，但在现实世界的应用中仍面临知识幻觉和知识缺乏的问题。基于知识图谱的检索增强生成方法通过利用结构化信息来提高LLM的质量和可信度，但这些方法难以有效整合结构信息，导致计算成本过高或未能充分利用可用的知识。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单且无需训练的方法，以引入结构化信息，并将其无缝集成到现有的知识图谱-检索增强生成模型中，从而更好地利用丰富的结构信息。&lt;h4&gt;方法&lt;/h4&gt;受图表示学习中平滑操作的启发，提出了路径池化策略。这是一种简单的、无须训练的操作，通过新颖的中心于路径的池化方式引入了结构信息，并且可以无缝集成到现有的KG-RAG方法中使用。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在最先进的KG-RAG模型中加入路径池化策略能显著改善各种设定下的性能表现，同时几乎不会增加额外成本。&lt;h4&gt;结论&lt;/h4&gt;该论文通过提出一种新的技术——路径池化，有效解决了现有知识图谱增强的生成方法在结构信息整合上的难题，并展示了其良好的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;尽管大型语言模型在许多任务上取得了显著成功，但在实际应用中仍存在幻觉和知识不足的问题。基于知识图谱的检索增强生成（KG-RAG）方法通过利用结构化和语义信息来提高LLM的质量和可信度，但这些方法难以有效整合结构信息，要么导致高计算成本，要么未能充分利用可用的知识。受图表示学习中平滑操作的启发，我们提出了一种简单的、无需训练的策略——路径池化（Path Pooling），通过新颖的以路径为中心的池化操作引入了结构信息，并可以无缝集成到现有的KG-RAG方法中使用，实现了更丰富的结构信息利用。广泛的实验表明，在最先进的KG-RAG模型中加入路径池化能够显著提高性能，同时几乎不会增加额外成本。代码即将在https://github.com/hrwang00/path-pooling发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although Large Language Models achieve strong success in many tasks, theystill suffer from hallucinations and knowledge deficiencies in real-worldapplications. Many knowledge graph-based retrieval-augmented generation(KG-RAG) methods enhance the quality and credibility of LLMs by leveragingstructure and semantic information in KGs as external knowledge bases. However,these methods struggle to effectively incorporate structure information, eitherincurring high computational costs or underutilizing available knowledge.Inspired by smoothing operations in graph representation learning, we proposepath pooling, a simple, train-free strategy that introduces structureinformation through a novel path-centric pooling operation. It seamlesslyintegrates into existing KG-RAG methods in a plug-and-play manner, enablingricher structure information utilization. Extensive experiments demonstratethat incorporating the path pooling into the state-of-the-art KG-RAG methodconsistently improves performance across various settings while introducingnegligible additional cost. Code is coming soon athttps://github.com/hrwang00/path-pooling.</description>
      <author>example@mail.com (Hairu Wang, Yuan Feng, Xike Xie, S Kevin Zhou)</author>
      <guid isPermaLink="false">2503.05203v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Distillation based Distribution Alignment (SDDA) for Cross-Headset EEG Classification</title>
      <link>http://arxiv.org/abs/2503.05349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于空间蒸馏的分布对齐(SDDA)方法，用于非侵入性脑机接口(BCI)中的异构跨头盔转移。&lt;h4&gt;背景&lt;/h4&gt;非侵入性BCI通过EEG信号实现用户与外部设备之间的直接交互。由于不同头盔中电极数量和位置的不同，解码这些信号是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;解决非侵入性BCI中不同头盔之间跨域迁移的问题。&lt;h4&gt;方法&lt;/h4&gt;SDDA利用空间蒸馏充分利用所有电极，并通过输入/特征/输出空间分布对齐来处理源域与目标域之间的显著差异。这是首次在跨头盔传输中使用知识蒸馏的研究。&lt;h4&gt;主要发现&lt;/h4&gt;在六组EEG数据集上的广泛实验表明，SDDA在离线无监督领域适应和在线监督领域适应场景下均表现出优越性能，并且始终优于10种经典和最新的迁移学习算法。&lt;h4&gt;结论&lt;/h4&gt;所提出的SDDA方法证明了其在解决非侵入性BCI中跨头盔传输挑战方面的有效性与鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容已按照要求进行了中文总结并转换为JSON格式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A non-invasive brain-computer interface (BCI) enables direct interactionbetween the user and external devices, typically via electroencephalogram (EEG)signals. However, decoding EEG signals across different headsets remains asignificant challenge due to differences in the number and locations of theelectrodes. To address this challenge, we propose a spatial distillation baseddistribution alignment (SDDA) approach for heterogeneous cross-headset transferin non-invasive BCIs. SDDA uses first spatial distillation to make use of thefull set of electrodes, and then input/feature/output space distributionalignments to cope with the significant differences between the source andtarget domains. To our knowledge, this is the first work to use knowledgedistillation in cross-headset transfers. Extensive experiments on six EEGdatasets from two BCI paradigms demonstrated that SDDA achieved superiorperformance in both offline unsupervised domain adaptation and onlinesupervised domain adaptation scenarios, consistently outperforming 10 classicaland state-of-the-art transfer learning algorithms.</description>
      <author>example@mail.com (Dingkun Liu, Siyang Li, Ziwei Wang, Wei Li, Dongrui Wu)</author>
      <guid isPermaLink="false">2503.05349v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>StreamGrid: Streaming Point Cloud Analytics via Compulsory Splitting and Deterministic Termination</title>
      <link>http://arxiv.org/abs/2503.05197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;点云数据在智能应用中的重要性日益增加，但加速器频繁的离片内存访问导致流水线停滞和高能耗。本文提出了两个技术：强制拆分和确定性终止，并在此基础上开发了StreamGrid框架，以实现全流式处理并自动优化片上缓存大小。&lt;h4&gt;背景&lt;/h4&gt;随着点云数据在智能应用中的重要性增加，加速器中频繁的离片内存访问成为瓶颈，导致流水线停滞及高能耗问题。传统的行缓冲技术虽然可以消除离片内存流量，但不适用于点云由于其独特的计算模式。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的处理方法和框架，以减少能量消耗并提高系统效率，同时尽量保持或接近基准方法的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了两个关键技术：强制拆分（Compulsory Splitting）与确定性终止（Deterministic Termination），并在这些技术的基础上开发了StreamGrid框架。这个框架能够自动优化片上缓冲大小以适应不同的应用场景。&lt;h4&gt;主要发现&lt;/h4&gt;在实验评估中，相较于没有采用新方法的基准模型，StreamGrid减少了61.3%的片上内存使用量，并降低了40.5%的能量消耗；同时，在性能上实现了10.0倍的速度提升和3.9倍的能量效率提高。&lt;h4&gt;结论&lt;/h4&gt;所提出的强制拆分、确定性终止技术和StreamGrid框架成功地解决了点云处理中能量消耗与效率的问题，展示了在智能应用领域的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已给出中文描述&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds are increasingly important in intelligent applications, butfrequent off-chip memory traffic in accelerators causes pipeline stalls andleads to high energy consumption. While conventional line buffer techniques caneliminate off-chip traffic, they cannot be directly applied to point clouds dueto their inherent computation patterns. To address this, we introduce twotechniques: compulsory splitting and deterministic termination, enablingfully-streaming processing. We further propose StreamGrid, a framework thatintegrates these techniques and automatically optimizes on-chip buffer sizes.Our evaluation shows StreamGrid reduces on-chip memory by 61.3\% and energyconsumption by 40.5\% with marginal accuracy loss compared to the baselineswithout our techniques. Additionally, we achieve 10.0$\times$ speedup and3.9$\times$ energy efficiency over state-of-the-art accelerators.</description>
      <author>example@mail.com (Yu Feng, Zheng Liu, Weikai Lin, Zihan Liu, Jingwen Leng, Minyi Guo, Zhezhi He, Jieru Zhao, Yuhao Zhu)</author>
      <guid isPermaLink="false">2503.05197v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts</title>
      <link>http://arxiv.org/abs/2503.05447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical report, 17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了Linear-MoE，这是一种用于大规模模型的生产级系统，它将线性序列建模（LSM）和混合专家（MoE）集成在一起。&lt;h4&gt;背景&lt;/h4&gt;线性注意、状态空间模型、线性RNN以及混合专家方法在近期被提出作为架构改进。这些技术能够提供线性复杂度的序列建模能力和稀疏激活能力，结合这两者的优点可以提高性能并实现高效训练。&lt;h4&gt;目的&lt;/h4&gt;介绍Linear-MoE系统，该系统旨在利用LSM模块和MoE层的优势来构建大规模模型，并通过高效的训练方式进一步提升模型灵活性与性能。&lt;h4&gt;方法&lt;/h4&gt;Linear-MoE系统主要包含两个子系统：1）建模子系统，它为所有LSM实例提供统一框架；2）训练子系统，该子系统利用各种高级并行技术（特别是针对Linear-MoE模型的序列并行性）来促进高效训练。此外，还探讨了混合模型，这些模型结合了线性MoE层和标准Transformer-MoE层。&lt;h4&gt;主要发现&lt;/h4&gt;在A0.3B-2B系列和A1B-7B系列模型上进行的评估显示，Linear-MoE实现了效率提升，并且在各种基准测试中保持了竞争力的表现。&lt;h4&gt;结论&lt;/h4&gt;结果表明，Linear-MoE架构具有成为下一代基础模型结构的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;线性序列建模（LSM）如线性注意力、状态空间模型和线性RNN以及混合专家(MoE)最近作为重要的架构改进而出现。本文介绍了Linear-MoE，这是一个用于大规模模型的生产级系统，这些模型将LSM与MoE相结合。Linear-MoE利用了LSM模块进行线性复杂度序列建模和MoE层进行稀疏激活的优势，旨在提供高性能并有效地训练。该系统包括：1）建模子系统，为所有LSM实例提供统一框架；2）训练子系统，通过集成各种高级并行技术（特别是针对Linear-MoE模型的序列并行性）促进高效训练。此外，我们还探讨了混合模型，这些模型将Linear-MoE层与标准Transformer-MoE层结合，并利用其序列并行性来进一步增强模型灵活性和性能。在两个模型系列A0.3B-2B和A1B-7B上的评估表明，Linear-MoE实现了效率提升，并且在各种基准测试中保持了竞争力的表现，展示了它作为下一代基础模型结构的巨大潜力。代码：https://github.com/OpenSparseLLMs/Linear-MoE.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Linear Sequence Modeling (LSM) like linear attention, state space models andlinear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significantarchitectural improvements. In this paper, we introduce Linear-MoE, aproduction-level system for modeling and training large-scale models thatintegrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modulesfor linear-complexity sequence modeling and MoE layers for sparsely activation,aiming to offer high performance with efficient training. The Linear-MoE systemcomprises: 1) Modeling subsystem, which provides a unified framework supportingall instances of LSM. and 2) Training subsystem, which facilitates efficienttraining by incorporating various advanced parallelism technologies,particularly Sequence Parallelism designed for Linear-MoE models. Additionally,we explore hybrid models that combine Linear-MoE layers with standardTransformer-MoE layers with its Sequence Parallelism to further enhance modelflexibility and performance. Evaluations on two model series, A0.3B-2B andA1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintainingcompetitive performance on various benchmarks, showcasing its potential as anext-generation foundational model architecture. Code:https://github.com/OpenSparseLLMs/Linear-MoE.</description>
      <author>example@mail.com (Weigao Sun, Disen Lan, Tong Zhu, Xiaoye Qu, Yu Cheng)</author>
      <guid isPermaLink="false">2503.05447v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>HexPlane Representation for 3D Semantic Scene Understanding</title>
      <link>http://arxiv.org/abs/2503.05127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种用于3D语义场景理解的HexPlane表示法，该方法通过设计View Projection Module (VPM)将3D点云投影到六个平面上，并利用2D编码器提取特征。这些特征随后被送入HexPlane Association Module (HAM)，以自适应地融合最具有信息量的信息来生成每个点的融合特征。&lt;h4&gt;背景&lt;/h4&gt;传统的点和体素表示方法在处理稀疏且无序的3D点云时效率低下，难以利用高度优化的2D操作。现有的方法也很难直接应用现成的2D模型、网络权重以及训练配方到3D空间中以实现准确的理解。&lt;h4&gt;目的&lt;/h4&gt;提出HexPlane表示法来提高3D场景理解的效率和准确性，并展示其在ScanNet和SemanticKITTI基准测试中的性能表现。&lt;h4&gt;方法&lt;/h4&gt;1. 设计View Projection Module (VPM)，用于将3D点云投影到六个平面上，保留原始空间信息；2. 使用2D编码器提取六平面的特征；3. 通过HexPlane Association Module (HAM)自适应地融合这些特征以生成最终的预测。&lt;h4&gt;主要发现&lt;/h4&gt;1. HexPlane表示法在ScanNet上的表现优于先前的方法，特别是语义分割任务上获得了77.0 mIoU的成绩，比Point Transformer V2高出1.6mIoU；2. 在室内3D检测任务中也观察到了鼓舞人心的结果。&lt;h4&gt;结论&lt;/h4&gt;HexPlane表示法能够无缝集成到现有的体素、点和范围基线方法中，并且可以带来显著的性能提升。该算法在效率和准确性方面都有明显优势。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们介绍了用于3D语义场景理解的HexPlane表示法。具体而言，首先设计了View Projection Module (VPM)将3D点云投影到六个平面上以尽可能保留原始的空间信息。通过2D编码器从六个平面提取特征，并将其发送给HexPlane Association Module (HAM)，该模块自适应地融合每个点最具有信息量的信息。这些融合后的点特征被进一步馈送到任务头，以产生最终的预测。与流行的点和体素表示相比，HexPlane表示法在处理稀疏且无序的3D点云时更高效，并能利用高度优化的2D操作来提高准确性。它还可以利用现成的2D模型、网络权重以及训练配方，在三维空间中实现准确的理解。在ScanNet和SemanticKITTI基准测试上，我们的算法（称为HexNet3D）与先前的方法相比取得了竞争性的性能表现。特别是在ScanNet 3D分割任务上，我们方法获得了77.0 mIoU的验证集成绩，比Point Transformer V2高出1.6mIoU。我们也观察到了在室内3D检测任务上的令人鼓舞的结果。值得注意的是，我们的方法可以无缝集成到现有的基于体素、点和范围的方法中，并且可以在没有额外功能的情况下带来显著的好处。代码将在发布时提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce the HexPlane representation for 3D semantic sceneunderstanding. Specifically, we first design the View Projection Module (VPM)to project the 3D point cloud into six planes to maximally retain the originalspatial information. Features of six planes are extracted by the 2D encoder andsent to the HexPlane Association Module (HAM) to adaptively fuse the mostinformative information for each point. The fused point features are furtherfed to the task head to yield the ultimate predictions. Compared to the popularpoint and voxel representation, the HexPlane representation is efficient andcan utilize highly optimized 2D operations to process sparse and unordered 3Dpoint clouds. It can also leverage off-the-shelf 2D models, network weights,and training recipes to achieve accurate scene understanding in 3D space. OnScanNet and SemanticKITTI benchmarks, our algorithm, dubbed HexNet3D, achievescompetitive performance with previous algorithms. In particular, on the ScanNet3D segmentation task, our method obtains 77.0 mIoU on the validation set,surpassing Point Transformer V2 by 1.6 mIoU. We also observe encouragingresults in indoor 3D detection tasks. Note that our method can be seamlesslyintegrated into existing voxel-based, point-based, and range-based approachesand brings considerable gains without bells and whistles. The codes will beavailable upon publication.</description>
      <author>example@mail.com (Zeren Chen, Yuenan Hou, Yulin Chen, Li Liu, Xiao Sun, Lu Sheng)</author>
      <guid isPermaLink="false">2503.05127v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation</title>
      <link>http://arxiv.org/abs/2503.04872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Branch-Merge的知识蒸馏方法，用于提高大型语言模型的压缩效率。该方法通过两个阶段实现：分支阶段和合并阶段。&lt;h4&gt;背景&lt;/h4&gt;减少大型语言模型（LLMs）的大小以保持性能是当前的一个重要挑战，现有的如模型蒸馏、迁移学习等方法在维持高精度方面效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的Branch-Merge知识蒸馏方法，旨在提高模型压缩效率并创建更小、高性能且计算成本更低的大型语言模型。&lt;h4&gt;方法&lt;/h4&gt;该方法包含两个阶段：分支阶段通过领域特定监督微调（SFT）将大教师模型的知识选择性地提取到专业化的学生模型中；合并阶段则使学生模型能够跨域知识转移，提升泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;验证该蒸馏方法使用DeepSeek-R1作为教师模型和DeepSeek-R1-Distill-Qwen-32B作为学生模型。最终得到的TinyR1-32B-Preview模型在多个基准测试（如数学、编程、科学等）中优于其对照组，且接近于DeepSeek-R1在AIME 2024的表现。&lt;h4&gt;结论&lt;/h4&gt;Branch-Merge蒸馏方法提供了一种可扩展的解决方案，用于创建更小、高性能且计算成本较低的大型语言模型。&lt;h4&gt;翻译&lt;/h4&gt;减少大型语言模型（LLMs）的大小以保持性能是当前的一个重要挑战。然而现有的方法如模型蒸馏和迁移学习往往无法实现高精度。为解决此问题，我们提出了Branch-Merge蒸馏法，通过两个阶段增强模型压缩：分支阶段选择性地将大教师模型的知识转移到专业化学生模型中；合并阶段则使这些学生模型能够跨领域知识转移并提升泛化能力。使用DeepSeek-R1作为教师模型和DeepSeek-R1-Distill-Qwen-32B作为学生模型进行验证，得到的TinyR1-32B-Preview在多个基准测试中优于对照组，并且接近于DeepSeek-R1在AIME 2024的表现。此蒸馏方法为创建更小、高性能且计算成本较低的大规模语言模型提供了一种可扩展解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The challenge of reducing the size of Large Language Models (LLMs) whilemaintaining their performance has gained significant attention. However,existing methods, such as model distillation and transfer learning, often failto achieve high accuracy. To address this limitation, we introduce theBranch-Merge distillation approach, which enhances model compression throughtwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\textit{selectively distilled} into specialized student models viadomain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, wherethese student models are merged to enable cross-domain knowledge transfer andimprove generalization. We validate our distillation approach using DeepSeek-R1as the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resultingmerged model, TinyR1-32B-Preview, outperforms its counterpartDeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achievingnear-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Mergedistillation approach provides a scalable solution for creating smaller,high-performing LLMs with reduced computational cost and time.</description>
      <author>example@mail.com (Lin Sun, Guangxiang Zhao, Xiaoqi Jian, Yuhan Wu, Weihong Lin, Yongfu Zhu, Change Jia, Linglin Zhang, Jinzhu Wu, Junfeng Ran, Sai-er Hu, Zihan Jiang, Junting Zhou, Wenrui Liu, Bin Cui, Tong Yang, Xiangzheng Zhang)</author>
      <guid isPermaLink="false">2503.04872v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>VLMs Play StarCraft II: A Benchmark and Multimodal Decision Method</title>
      <link>http://arxiv.org/abs/2503.05383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了VLM-Attention，这是一个结合了RGB视觉输入和自然语言观察的星际争霸II多模态环境，旨在让人工代理的行为更接近人类的认知过程。&lt;h4&gt;背景&lt;/h4&gt;传统的SMAC等框架依赖于与人类感知相差甚远的抽象状态表示，限制了人工行为的生态有效性。&lt;h4&gt;目的&lt;/h4&gt;通过引入视觉-语言模型、检索增强生成系统和动态角色任务分配系统来解决上述问题，使代理的行为更接近人类玩家。&lt;h4&gt;方法&lt;/h4&gt;{'VLM-Attention框架组成': ['一个强化了专门自我注意力机制的战略单位目标定位和战场评估的视觉-语言模型', '利用领域特定星际争霸II知识进行战术决策的检索增强生成系统', '通过动态角色任务分配实现协调多代理行为的任务分布系统']}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，基于VLM的代理在21个定制场景中可以执行复杂的战术机动而无需显式训练，并达到与传统MARL方法相当的性能。&lt;h4&gt;结论&lt;/h4&gt;这项工作为开发人类一致性的星际争霸II代理人奠定了基础，并推动了多模态游戏AI的研究议程。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了VLM-Attention，这是一个将人工智能代理感知与人机对战体验相匹配的星际争霸II多模态环境。传统框架如SMAC依赖于抽象状态表示，这与人类感知存在显著差异，限制了人工行为的生态有效性。我们的环境通过整合RGB视觉输入和自然语言观察来解决这一问题，这些更接近游戏期间的人类认知过程。VLM-Attention框架包括三个集成组件：（1）一种增强特定自我注意机制的战略单位目标定位和战场评估的视觉-语言模型；（2）一个利用领域特定星际争霸II知识辅助战术决策的检索增强生成系统；以及（3）一个通过动态角色任务分配实现协调多代理行为的任务分布系统。我们在21个定制场景中的实验结果显示，由基础模型驱动的VLM基于智能体可以执行复杂的战术机动而无需显式训练，并且实现了与传统需要大量迭代训练的传统MARL方法相当的性能。这项工作为开发人类一致性的星际争霸II代理人奠定了基础，并推动了多模态游戏AI的研究议程。我们的实现可在https://github.com/camel-ai/VLM-Play-StarCraft2访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce VLM-Attention, a multimodal StarCraft II environment that alignsartificial agent perception with the human gameplay experience. Traditionalframeworks such as SMAC rely on abstract state representations that divergesignificantly from human perception, limiting the ecological validity of agentbehavior. Our environment addresses this limitation by incorporating RGB visualinputs and natural language observations that more closely simulate humancognitive processes during gameplay. The VLM-Attention framework consists ofthree integrated components: (1) a vision-language model enhanced withspecialized self-attention mechanisms for strategic unit targeting andbattlefield assessment, (2) a retrieval-augmented generation system thatleverages domain-specific StarCraft II knowledge to inform tactical decisions,and (3) a dynamic role-based task distribution system that enables coordinatedmulti-agent behavior. Our experimental evaluation across 21 custom scenariosdemonstrates that VLM-based agents powered by foundation models (specificallyQwen-VL and GPT-4o) can execute complex tactical maneuvers without explicittraining, achieving comparable performance to traditional MARL methods thatrequire substantial training iterations. This work establishes a foundation fordeveloping human-aligned StarCraft II agents and advances the broader researchagenda of multimodal game AI. Our implementation is available athttps://github.com/camel-ai/VLM-Play-StarCraft2.</description>
      <author>example@mail.com (Weiyu Ma, Yuqian Fu, Zecheng Zhang, Guohao Li)</author>
      <guid isPermaLink="false">2503.05383v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive-LIO: Enhancing Robustness and Precision through Environmental Adaptation in LiDAR Inertial Odometry</title>
      <link>http://arxiv.org/abs/2503.05077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种松耦合的自适应LiDAR惯性里程计（Adaptive-LIO）方法，该方法解决了现有SLAM系统在面对不同场景时缺乏足够适应性的挑战。&lt;h4&gt;背景&lt;/h4&gt;新兴物联网应用如自动驾驶汽车对高精度定位和导航的需求日益增长。基于激光雷达与惯性测量单元的组合导航技术在机器人领域及自主驾驶中越来越受欢迎。&lt;h4&gt;目的&lt;/h4&gt;为提高LiDAR惯性里程计系统的准确性和鲁棒性，解决点云准确性降低、IMU数据错误累积以及地图分辨率固定导致定位精度下降等问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种名为Adaptive-LIO的自适应松耦合激光雷达惯性里程计方案。该方案通过引入自适应分割提高映射精确度；检测并调整由于IMU饱和带来的运动模式变化；根据距离传感器中心点的距离动态调节地图分辨率。&lt;h4&gt;主要发现&lt;/h4&gt;在多种复杂场景中验证了所提出方法的有效性和优越性能，展示了改进措施对系统效能的显著提升。&lt;h4&gt;结论&lt;/h4&gt;该研究通过解决当前LiDAR惯性里程计面临的关键挑战，大大增强了其适应各种应用场景的能力。开源代码可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;新兴物联网应用如自动驾驶汽车对高精度定位和导航的需求日益增长。目前基于激光雷达与惯性测量单元的组合导航技术在机器人领域及自主驾驶中越来越受欢迎。然而，许多现有的SLAM系统无法足够适应各种场景需求。挑战包括在恒定速度假设下随着帧间隔变长导致点云准确度降低；IMU饱和时错误信息累积的问题；以及在室内外场景切换期间由于使用固定分辨率地图而导致的定位精度下降。为了应对这些难题，我们提出了一种松耦合自适应LiDAR惯性里程计（Adaptive-LIO），该方法利用自适应分割提高映射精确度、通过IMU饱和和故障检测来调整运动模式，并根据激光雷达中心距离动态调节地图分辨率使用多分辨率体素图。我们的方法已在各种具有挑战性的场景中进行了测试，展示了所引入改进的有效性。代码在GitHub上开源：https://github.com/chengwei0427/adaptive_lio&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emerging Internet of Things (IoT) applications, such as driverless cars,have a growing demand for high-precision positioning and navigation. Nowadays,LiDAR inertial odometry becomes increasingly prevalent in robotics andautonomous driving. However, many current SLAM systems lack sufficientadaptability to various scenarios. Challenges include decreased point cloudaccuracy with longer frame intervals under the constant velocity assumption,coupling of erroneous IMU information when IMU saturation occurs, and decreasedlocalization accuracy due to the use of fixed-resolution maps duringindoor-outdoor scene transitions. To address these issues, we propose a looselycoupled adaptive LiDAR-Inertial-Odometry named \textbf{Adaptive-LIO}, whichincorporates adaptive segmentation to enhance mapping accuracy, adapts motionmodality through IMU saturation and fault detection, and adjusts map resolutionadaptively using multi-resolution voxel maps based on the distance from theLiDAR center. Our proposed method has been tested in various challengingscenarios, demonstrating the effectiveness of the improvements we introduce.The code is open-source on GitHub:\href{https://github.com/chengwei0427/adaptive_lio}{Adaptive-LIO}.</description>
      <author>example@mail.com (Chengwei Zhao, Kun Hu, Jie Xu, Lijun Zhao, Baiwen Han, Kaidi Wu, Maoshan Tian, Shenghai Yuan)</author>
      <guid isPermaLink="false">2503.05077v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Spectral Informed Mamba for Robust Point Cloud Processing</title>
      <link>http://arxiv.org/abs/2503.04953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了利用Mamba和Masked Autoencoder网络处理点云数据的新方法，适用于监督学习和自监督学习。&lt;h4&gt;背景&lt;/h4&gt;状态空间模型在自然语言处理（NLP）中显示出巨大潜力，并且最近在计算机视觉领域也有所突破。Mamba作为一种新颖的方法，在处理复杂结构的点云时展现出了独特的优势。&lt;h4&gt;目的&lt;/h4&gt;提出三项关键贡献，旨在增强Mamba在网络架构和数据处理能力上的表现。&lt;h4&gt;方法&lt;/h4&gt;{'第一项贡献': '利用图拉普拉斯算子的频谱来捕捉补丁连接性，并定义了一种对于视点变化鲁棒且比传统的3D网格遍历更好的等距不变性遍历顺序，更好地捕获形状流形。', '第二项贡献': '通过基于Laplacian光谱成分的信息递归地分割补丁，实施了更为精细的集成和片段分析策略。', '第三项贡献': '对于Masked Autoencoder中的令牌放置问题，在Mamba中提出了将令牌恢复到其原始位置的方法，以保持必要的顺序并提高学习效率。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在分类、分割以及少样本任务上优于现有的基线模型。&lt;h4&gt;结论&lt;/h4&gt;所提出的新方法能够显著提升点云数据处理的效果，在NLP和计算机视觉领域具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;状态空间模型已在自然语言处理（NLP）中展现出巨大潜力，并且最近在计算机视觉领域也有所突破。本文介绍了一种新的利用Mamba和Masked Autoencoder网络的方法，旨在提高点云数据的监督学习和自监督学习能力。文中提出三项关键贡献以增强Mamba在网络架构和复杂结构处理上的性能：一是采用图拉普拉斯算子频谱捕捉补丁连接性并定义一种鲁棒遍历顺序；二是通过基于Laplacian光谱成分的信息递归分割补丁，进行更精细的片段分析；三是提出Masked Autoencoder中令牌恢复到原始位置的方法以保持学习效果。实验结果证明了该方法在分类、分割及少样本任务上的优势，并超越现有基线模型的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State space models have shown significant promise in Natural LanguageProcessing (NLP) and, more recently, computer vision. This paper introduces anew methodology leveraging Mamba and Masked Autoencoder networks for pointcloud data in both supervised and self-supervised learning. We propose threekey contributions to enhance Mamba's capability in processing complex pointcloud structures. First, we exploit the spectrum of a graph Laplacian tocapture patch connectivity, defining an isometry-invariant traversal order thatis robust to viewpoints and better captures shape manifolds than traditional 3Dgrid-based traversals. Second, we adapt segmentation via a recursive patchpartitioning strategy informed by Laplacian spectral components, allowing finerintegration and segment analysis. Third, we address token placement in MaskedAutoencoder for Mamba by restoring tokens to their original positions, whichpreserves essential order and improves learning. Extensive experimentsdemonstrate the improvements of our approach in classification, segmentation,and few-shot tasks over state-of-the-art baselines.</description>
      <author>example@mail.com (Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, Sahar Dastani, Milad Cheraghalikhani, David Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers)</author>
      <guid isPermaLink="false">2503.04953v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces</title>
      <link>http://arxiv.org/abs/2503.05283v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了在大规模训练下，单模态3D编码器与文本特征空间的后训练对齐可能性。研究表明，直接进行特征对齐效果有限，但通过选择合适维度的子空间投影可以显著提高对齐质量。&lt;h4&gt;背景&lt;/h4&gt;以往的工作表明，在大规模训练时，单一模式2D视觉和文本编码器在不同表示下学习到的特征具有相似的结构特性。然而，关于3D编码器与其他模态的关系尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;研究单模态3D编码器与文本特征空间后训练对齐的可能性，并探索三维数据的独特属性。&lt;h4&gt;方法&lt;/h4&gt;首先展示了直接对单模态文本和3D编码器进行后训练特征对齐效果不佳。然后，专注于提取相应特征空间的子空间，并发现通过投影到精心选择的低维子空间上可以显著提高对齐质量。&lt;h4&gt;主要发现&lt;/h4&gt;通过选择合适的维度子空间进行投影可以极大提升单模态3D和文本特征之间的对齐质量。&lt;h4&gt;结论&lt;/h4&gt;研究首次建立了后训练时期三维数据与文本特征空间对齐的基础，并揭示了两者共享的属性以及三维数据特有的性质。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，当在大规模下训练时，单一模式2D视觉和文本编码器会学习到具有显著结构特性的特性，即使它们起源于不同的表示。然而，3D编码器相对于其他模态的作用仍然未被探索。现有的利用大型数据集的3D基础模型通常与其他表示冻结编码器一起进行明确对齐目标训练。在这项工作中，我们探讨了与基于文本的特征空间相比从单一模式3D编码器中获得的表示后训练对齐的可能性。我们展示了直接在单模态文本和3D编码器上进行特征对齐会导致性能有限。然后，我们将重点放在相应特征空间子空间的提取上，并发现通过将学习到的表示投影到精心选择的低维子空间上可以显著提高对齐质量，从而改善匹配和检索任务中的准确性。我们的分析进一步揭示了这些共享子空间的本质，大致区分了语义和几何数据表示。总体而言，我们是第一个帮助建立3D单模态与文本特征空间后训练对齐基础的工作，并有助于突出三维数据与其他表示相比的共同点和独特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent works have shown that, when trained at scale, uni-modal 2D vision andtext encoders converge to learned features that share remarkable structuralproperties, despite arising from different representations. However, the roleof 3D encoders with respect to other modalities remains unexplored.Furthermore, existing 3D foundation models that leverage large datasets aretypically trained with explicit alignment objectives with respect to frozenencoders from other representations. In this work, we investigate thepossibility of a posteriori alignment of representations obtained fromuni-modal 3D encoders compared to text-based feature spaces. We show that naivepost-training feature alignment of uni-modal text and 3D encoders results inlimited performance. We then focus on extracting subspaces of the correspondingfeature spaces and discover that by projecting learned representations ontowell-chosen lower-dimensional subspaces the quality of alignment becomessignificantly higher, leading to improved accuracy on matching and retrievaltasks. Our analysis further sheds light on the nature of these sharedsubspaces, which roughly separate between semantic and geometric datarepresentations. Overall, ours is the first work that helps to establish abaseline for post-training alignment of 3D uni-modal and text feature spaces,and helps to highlight both the shared and unique properties of 3D datacompared to other representations.</description>
      <author>example@mail.com (Souhail Hadgi, Luca Moschella, Andrea Santilli, Diego Gomez, Qixing Huang, Emanuele Rodolà, Simone Melzi, Maks Ovsjanikov)</author>
      <guid isPermaLink="false">2503.05283v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data</title>
      <link>http://arxiv.org/abs/2503.04852v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;真智能依赖于揭示和利用隐藏的因果关系的能力。尽管在人工智能和计算机视觉领域取得了显著进展，但在评估模型从复杂视觉数据中推断潜在因果性的能力方面仍然缺乏基准。&lt;h4&gt;背景&lt;/h4&gt;目前AI和CV领域的进步尚未提供足够的工具来衡量模型对隐含因果关系的理解和推理能力。&lt;h4&gt;目的&lt;/h4&gt;介绍extsc{extbf{Causal3D}}，一个集成了结构化数据（表格）与其对应的视觉表示（图像）的新型综合性基准测试平台，旨在评估因果推理的能力。&lt;h4&gt;方法&lt;/h4&gt;在系统框架下设计的extsc{extbf{Causal3D}}包含19个不同的3D场景数据集，涵盖了各种因果关系、视角和背景，允许在不同复杂度的情景中进行全面评价。同时评估了多种最先进的方法，包括经典的因果发现、因果表示学习以及大型语言/视觉-语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，当没有先验知识时，随着因果结构变得更加复杂，性能显著下降，这强调了即使高级方法在复杂的因果场景下也面临着挑战。&lt;h4&gt;结论&lt;/h4&gt;extsc{extbf{Causal3D}}作为一个重要的资源，在计算机视觉中推动因果推理的发展，并促进关键领域中的可信AI的建立。&lt;h4&gt;翻译&lt;/h4&gt;真正的智能在于发现并利用隐藏的因果关系的能力。尽管人工智能和计算机视觉领域已经取得了显著的进步，但在评估模型从复杂视觉数据中推断潜在因果性能力方面仍然存在不足。这篇论文引入了extsc{extbf{Causal3D}}这一新型且综合性的基准测试平台，它结合了结构化数据（表格）与其对应的图像表示，用于评价因果推理的能力。在系统框架内构建的extsc{extbf{Causal3D}}包括19个不同3D场景的数据集，涵盖了多样的因果关系、视角和背景，支持跨复杂度的情景进行评测。通过评估多种最先进的方法——包括传统的因果发现技术、因果表示学习以及大型/视觉-语言模型（LLMs/VLMs）——实验结果显示，在缺乏先验知识的情况下，随着因果结构变得更为复杂，性能显著下降，这表明即使是先进的方法在复杂的因果情景下也会遇到挑战。extsc{extbf{Causal3D}}为推进计算机视觉中的因果推理以及关键领域的可信AI发展提供了重要的资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; True intelligence hinges on the ability to uncover and leverage hidden causalrelations. Despite significant progress in AI and computer vision (CV), thereremains a lack of benchmarks for assessing models' abilities to infer latentcausality from complex visual data. In this paper, we introduce\textsc{\textbf{Causal3D}}, a novel and comprehensive benchmark that integratesstructured data (tables) with corresponding visual representations (images) toevaluate causal reasoning. Designed within a systematic framework, Causal3Dcomprises 19 3D-scene datasets capturing diverse causal relations, views, andbackgrounds, enabling evaluations across scenes of varying complexity. Weassess multiple state-of-the-art methods, including classical causal discovery,causal representation learning, and large/vision-language models (LLMs/VLMs).Our experiments show that as causal structures grow more complex without priorknowledge, performance declines significantly, highlighting the challenges evenadvanced methods face in complex causal scenarios. Causal3D serves as a vitalresource for advancing causal reasoning in CV and fostering trustworthy AI incritical domains.</description>
      <author>example@mail.com (Disheng Liu, Yiran Qiao, Wuche Liu, Yiren Lu, Yunlai Zhou, Tuo Liang, Yu Yin, Jing Ma)</author>
      <guid isPermaLink="false">2503.04852v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Neural Configuration-Space Barriers for Manipulation Planning and Control</title>
      <link>http://arxiv.org/abs/2503.04929v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;高维机器人机械手在复杂和动态环境中进行规划与控制时需要高效的计算能力和可靠的保证安全。&lt;h4&gt;目的&lt;/h4&gt;提出一种统一框架用于运动规划和控制，并通过配置空间距离函数（CDF）障碍物来实现安全性约束。&lt;h4&gt;方法&lt;/h4&gt;开发了一种神经网络学习的分布鲁棒性配置空间距离函数障碍物，以处理建模误差和传感器噪声，同时利用在线传感器观测减少碰撞检测操作。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明所提出的基于神经网络的CDF障碍物形式化能够使机械手在复杂且动态环境中实现高效的规划与可靠的实时安全控制。&lt;h4&gt;结论&lt;/h4&gt;该研究通过学习配置空间距离函数作为机器人体表征的方法，成功实现了高维机器人在复杂环境中的高效和可靠的安全性保障。&lt;h4&gt;翻译&lt;/h4&gt;计划和控制高维度的机器人机械臂，在拥挤、动态的环境中需要计算效率以及稳健的安全保证。受到最近关于使用神经网络学习配置空间距离函数（CDF）作为机器人体表征的研究进展启发，我们提出了一种统一框架用于运动规划和控制，并将安全性约束以CDF障碍物的形式化表示出来。一个CDF障碍物可以近似局部自由配置空间，大大减少了在运动规划过程中进行碰撞检测操作的数量。然而，在线传感器观察中利用神经网络学习的CDF障碍会引入不确定性，必须考虑这些不确定因素来进行控制器综合设计。为了处理这种情况，我们开发了一种分布鲁棒性CDF障碍形式化方法用于控制，明确地处理建模误差和传感器噪声而不需要已知底层分布假设。在6自由度xArm机械臂上的仿真及硬件实验表明：我们的神经网络CDF障碍物形式化能够使高效的规划与可靠的实时安全控制得以实现，在拥挤且动态的环境中仅依靠机载点云观测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Planning and control for high-dimensional robot manipulators in cluttered,dynamic environments require both computational efficiency and robust safetyguarantees. Inspired by recent advances in learning configuration-spacedistance functions (CDFs) as robot body representations, we propose a unifiedframework for motion planning and control that formulates safety constraints asCDF barriers. A CDF barrier approximates the local free configuration space,substantially reducing the number of collision-checking operations duringmotion planning. However, learning a CDF barrier with a neural network andrelying on online sensor observations introduce uncertainties that must beconsidered during control synthesis. To address this, we develop adistributionally robust CDF barrier formulation for control that explicitlyaccounts for modeling errors and sensor noise without assuming a knownunderlying distribution. Simulations and hardware experiments on a 6-DoF xArmmanipulator show that our neural CDF barrier formulation enables efficientplanning and robust real-time safe control in cluttered and dynamicenvironments, relying only on onboard point-cloud observations.</description>
      <author>example@mail.com (Kehan Long, Ki Myung Brian Lee, Nikola Raicevic, Niyas Attasseri, Melvin Leok, Nikolay Atanasov)</author>
      <guid isPermaLink="false">2503.04929v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Efficiently parallelizable kernel-based multi-scale algorithm</title>
      <link>http://arxiv.org/abs/2503.04914v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;基于核的方法已被证明是处理散乱数据插值问题的一种强大的逼近方法，在计算效率上优于传统的核基插值技术。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提出并分析一种有效且可并行化的实现方案，来解决上述多尺度方法的高效实施问题。&lt;h4&gt;方法&lt;/h4&gt;文中介绍了一种基于点云层次结构和紧支撑径向基函数（如Wendland函数）的单体化计算方法，并对其进行详细分析。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法具有高效的并行处理能力，能够更有效地实现基于核的多尺度逼近技术。&lt;h4&gt;结论&lt;/h4&gt;该研究为基于核的多尺度逼近提供了一种新的高效且可扩展的实施策略。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的研究背景、目的、方法以及主要发现和结论等内容已以中文进行总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The kernel-based multi-scale method has been proven to be a powerfulapproximation method for scattered data approximation problems which iscomputationally superior to conventional kernel-based interpolation techniques.The multi-scale method is based of an hierarchy of point clouds and compactlysupported radial basis functions, typically Wendland functions. There is a richbody of literature concerning the analysis of this method including errorestimates. This article addresses the efficient parallelizable implementationof those methods. To this end, we present and analyse a monolithic approach tocompute the kernel-based multi-scale approximation.</description>
      <author>example@mail.com (Federico Lot, Christian Rieger)</author>
      <guid isPermaLink="false">2503.04914v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>L-FUSION: Laplacian Fetal Ultrasound Segmentation &amp; Uncertainty Estimation</title>
      <link>http://arxiv.org/abs/2503.05245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;L-FUSION是一种用于胎儿超声图像分析的框架，旨在通过集成不确定性量化和大规模基础模型来提高对正常和异常扫描中胎儿结构分割的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;产前超声波检查对于早期检测发育异常至关重要。然而，操作者依赖和技术限制（例如固有伪影和效果、设置错误）会使图像解释变得复杂，并增加诊断不确定性。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，用于通过无监督学习和大规模基础模型来量化胎儿结构分割中的不确定性和增强超声波影像中病变与正常解剖结构的区分能力。&lt;h4&gt;方法&lt;/h4&gt;提出使用随机分割网络（Stochastic Segmentation Networks）中的aleatoric logit分布和Laplace近似结合快速Hessian估计，以估计仅来自分割头的认知不确定性。同时引入集成Dropout组件，生成增强不确定性和分割反事实图，在超声影像中可靠地区分病变与正常胎儿解剖。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果表明，L-FUSION在多个数据集上实现了优越的分割准确度和一致性的不确定性量化，从而支持现场决策，并为临床环境中胎儿超声波分析的进步提供了一个可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的框架，不仅能够提高对胎儿结构分割的准确性，还能增强不确定性和反事实图以区分病变与正常解剖，从而改进诊断反馈并减少手动疾病标注的需求。此外，L-FUSION在临床环境中为超声波分析提供了可靠的决策支持。&lt;h4&gt;翻译&lt;/h4&gt;准确分析产前超声（US）对于早期检测发育异常至关重要。然而，操作者依赖和技术限制可以复杂化图像解释和诊断不确定性的评估。我们提出了一个框架L-FUSION（基于不确定性量化和大规模基础模型的拉普拉斯胎儿US分割），它通过无监督、规范性学习与大规模基础模型集成来增强正常和病理扫描中胎儿结构的鲁棒分割。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate analysis of prenatal ultrasound (US) is essential for earlydetection of developmental anomalies. However, operator dependency andtechnical limitations (e.g. intrinsic artefacts and effects, setting errors)can complicate image interpretation and the assessment of diagnosticuncertainty. We present L-FUSION (Laplacian Fetal US Segmentation withIntegrated FoundatiON models), a framework that integrates uncertaintyquantification through unsupervised, normative learning and large-scalefoundation models for robust segmentation of fetal structures in normal andpathological scans. We propose to utilise the aleatoric logit distributions ofStochastic Segmentation Networks and Laplace approximations with fast Hessianestimations to estimate epistemic uncertainty only from the segmentation head.This enables us to achieve reliable abnormality quantification for instantdiagnostic feedback. Combined with an integrated Dropout component, L-FUSIONenables reliable differentiation of lesions from normal fetal anatomy withenhanced uncertainty maps and segmentation counterfactuals in US imaging. Itimproves epistemic and aleatoric uncertainty interpretation and removes theneed for manual disease-labelling. Evaluations across multiple datasets showthat L-FUSION achieves superior segmentation accuracy and consistentuncertainty quantification, supporting on-site decision-making and offering ascalable solution for advancing fetal ultrasound analysis in clinical settings.</description>
      <author>example@mail.com (Johanna P. Müller, Robert Wright, Thomas G. Day, Lorenzo Venturini, Samuel F. Budd, Hadrien Reynaud, Joseph V. Hajnal, Reza Razavi, Bernhard Kainz)</author>
      <guid isPermaLink="false">2503.05245v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>RURANET++: An Unsupervised Learning Method for Diabetic Macular Edema Based on SCSE Attention Mechanisms and Dynamic Multi-Projection Head Clustering</title>
      <link>http://arxiv.org/abs/2502.20224v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 2 figures, 5 tables, submitted to The 28th International  Conference on Medical Image Computing and Computer Assisted Intervention  (MICCAI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种基于无监督学习的自动诊断糖尿病性黄斑水肿(DME)系统RURANET++，该系统在提高病变特征提取和图像处理效率的同时，优化了聚类算法。&lt;h4&gt;背景&lt;/h4&gt;DME是糖尿病患者常见的并发症之一，严重威胁视力健康。尽管深度学习在医学影像分析中取得了一定进展，但传统诊断方法依赖大量标注数据和主观眼科医生评估，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于无监督学习的自动化DME诊断系统RURANET++，旨在减少对专家依赖并提高效率。&lt;h4&gt;方法&lt;/h4&gt;框架采用优化后的U-Net架构，并嵌入空间与通道挤压与激励(SCSE)注意力机制；利用预训练GoogLeNet模型提取视网膜图像深层特征，并通过PCA降维至50维度以提升计算效率；引入一种新的多投影头聚类算法，控制簇多样性并通过动态调整相似度阈值来优化类别内一致性及类别间区分性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明RURANET++在精度、召回率和F1-score等多项指标上均表现优异，达到了0.8411的最高准确率。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种高效的无监督解决方案用于DME诊断，并具有重要的临床意义。&lt;h4&gt;翻译&lt;/h4&gt;糖尿病性黄斑水肿(DME)，一种在糖尿病患者中普遍存在的并发症，是视力障碍和失明的主要原因。尽管深度学习已经在医学图像分析方面取得了显著的进步，但传统的DME诊断仍然依赖于大量的标注数据以及眼科医生的主观评估，这限制了其实际应用的可能性。为解决这一问题，我们提出了RURANET++，这是一种基于无监督学习的自动化的DME诊断系统。该框架结合了一个优化过的U-Net架构，并嵌入了空间和通道挤压与激励(SCSE)注意力机制以增强病变特征提取能力。在特征处理过程中，预训练好的GoogLeNet模型从视网膜图像中抽取深层特征，然后通过PCA进行基于50维度的降维来提高计算效率。值得注意的是，我们引入了一种新的多投影头聚类算法，在控制簇多样性的同时动态调整相似度阈值以优化类别内一致性和类别间区分性。实验结果表明RURANET++在多个评价指标中表现出色，达到了最高的准确率（0.8411）、精确度（0.8593）、召回率（0.8411）和F1分数（0.8390），且聚类质量卓越。这项工作为DME诊断提供了一种高效的无监督解决方案，并具有重要的临床意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diabetic Macular Edema (DME), a prevalent complication among diabeticpatients, constitutes a major cause of visual impairment and blindness.Although deep learning has achieved remarkable progress in medical imageanalysis, traditional DME diagnosis still relies on extensive annotated dataand subjective ophthalmologist assessments, limiting practical applications. Toaddress this, we present RURANET++, an unsupervised learning-based automatedDME diagnostic system. This framework incorporates an optimized U-Netarchitecture with embedded Spatial and Channel Squeeze &amp; Excitation (SCSE)attention mechanisms to enhance lesion feature extraction. During featureprocessing, a pre-trained GoogLeNet model extracts deep features from retinalimages, followed by PCA-based dimensionality reduction to 50 dimensions forcomputational efficiency. Notably, we introduce a novel clustering algorithmemploying multi-projection heads to explicitly control cluster diversity whiledynamically adjusting similarity thresholds, thereby optimizing intra-classconsistency and inter-class discrimination. Experimental results demonstratesuperior performance across multiple metrics, achieving maximum accuracy(0.8411), precision (0.8593), recall (0.8411), and F1-score (0.8390), withexceptional clustering quality. This work provides an efficient unsupervisedsolution for DME diagnosis with significant clinical implications.</description>
      <author>example@mail.com (Wei Yang, Yiran Zhu, Jiayu Shen, Yuhan Tang, Chengchang Pan, Hui He, Yan Su, Honggang Qi)</author>
      <guid isPermaLink="false">2502.20224v2</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction</title>
      <link>http://arxiv.org/abs/2503.05231v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为Kaiwu的多模态数据集，旨在解决复杂组装场景中同步多模态数据缺乏的问题。&lt;h4&gt;背景&lt;/h4&gt;机器人学习领域的最新技术，如基础模型和人类模仿学习，在大规模高质量数据的需求上存在瓶颈。这些问题在复杂的实际操作环境中尤为突出，尤其是在需要动态信息及其精细标注的情况下。&lt;h4&gt;目的&lt;/h4&gt;提供一个集成的人机环境数据收集框架，解决现有机器人领域中复杂组装场景同步多模态数据不足的问题。&lt;h4&gt;方法&lt;/h4&gt;Kaiwu数据集通过20个参与者和30种交互对象记录了包括手部动作、操作压力、组装过程声音、多视角视频、高精度运动捕捉信息、第一人称视图下的眼动追踪以及肌电图信号在内的多种类型的数据。并且进行了基于绝对时间戳的精细多层次标注。&lt;h4&gt;主要发现&lt;/h4&gt;Kaiwu数据集为机器人学习、灵巧操纵、人类意图研究和人机协作提供了丰富的资源和支持。&lt;h4&gt;结论&lt;/h4&gt;该数据集有望促进相关领域的发展，推动智能机器人技术的进步。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的先进机器人学习技术包括基础模型和从人类模仿学习的技术都对大规模高质量的数据提出了巨大需求，这些构成了通用智能机器人领域的瓶颈之一。本文提出Kaiwu多模态数据集来解决复杂组装场景中存在的同步多模态数据不足的问题，特别是在包含动态信息及其精细标注的情况下。该数据集首先提供了一个集成的人类、环境和机器人的数据收集框架，其中包括20个参与者和30种交互对象，总共产生了11,664次整合的操作实例。对于每次演示，手部动作、操作压力、组装过程的声音、多视角视频、高精度运动捕捉信息、带有第一人称视图的注视跟踪以及肌电图信号都被记录下来。基于绝对时间戳和语义分割标注进行了精细多层次的数据注释。Kaiwu数据集旨在促进机器人学习、灵巧操纵、人类意图研究及人机协作的研究工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cutting-edge robot learning techniques including foundation models andimitation learning from humans all pose huge demands on large-scale andhigh-quality datasets which constitute one of the bottleneck in the generalintelligent robot fields. This paper presents the Kaiwu multimodal dataset toaddress the missing real-world synchronized multimodal data problems in thesophisticated assembling scenario,especially with dynamics information and itsfine-grained labelling. The dataset first provides an integration ofhuman,environment and robot data collection framework with 20 subjects and 30interaction objects resulting in totally 11,664 instances of integratedactions. For each of the demonstration,hand motions,operation pressures,soundsof the assembling process,multi-view videos, high-precision motion captureinformation,eye gaze with first-person videos,electromyography signals are allrecorded. Fine-grained multi-level annotation based on absolute timestamp,andsemantic segmentation labelling are performed. Kaiwu dataset aims to facilitaterobot learning,dexterous manipulation,human intention investigation andhuman-robot collaboration research.</description>
      <author>example@mail.com (Shuo Jiang, Haonan Li, Ruochen Ren, Yanmin Zhou, Zhipeng Wang, Bin He)</author>
      <guid isPermaLink="false">2503.05231v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>ORANSight-2.0: Foundational LLMs for O-RAN</title>
      <link>http://arxiv.org/abs/2503.05200v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究介绍了ORANSight-2.0，一个专注于开放无线接入网络（O-RAN）的大型语言模型项目。&lt;h4&gt;背景&lt;/h4&gt;尽管大规模语言模型在医疗、客户服务和商业营销等关键领域产生了变革性影响，但它们与O-RAN的集成仍有限。现有解决方案通常依赖于通用型LLM，这些LLM无法解决O-RAN的独特挑战和技术复杂性。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，研究提出了ORANSight-2.0，旨在开发专门针对O-RAN的基础语言模型。&lt;h4&gt;方法&lt;/h4&gt;基于18个大型语言模型和五种开源框架构建了ORANSight-2.0。使用QLoRA技术对这些模型进行微调，并采用了RANSTRUCT（一种新型的检索增强生成（RAG）基指令调整框架），通过两个LLM代理创建高质量的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;研究开发了srsRANBench，用于评估代码生成和理解；ORANSight-2.0模型在O-RAN领域基准测试中优于通用型及闭源模型5.421%，在srsRANBench上高出18.465%。&lt;h4&gt;结论&lt;/h4&gt;ORANSight-2.0在性能、计算成本和能源消耗方面都表现出了优越性，并且实验展示了其增强版LLM的能耗特征，包括训练、标准推理以及检索增强生成推理的成本。&lt;h4&gt;翻译&lt;/h4&gt;摘要讨论了大型语言模型（LLMs）对医疗保健、客户服务和商业营销等关键领域的影响，但指出这些技术在开放无线电接入网络（O-RAN）中的应用仍处于初级阶段。研究提出了一种新的解决方案ORANSight-2.0，旨在开发专为O-RAN设计的基础语言模型，并详细介绍了该方案的方法和技术细节以及其性能评估结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the transformative impact of Large Language Models (LLMs) acrosscritical domains such as healthcare, customer service, and business marketing,their integration into Open Radio Access Networks (O-RAN) remains limited. Thisgap is primarily due to the absence of domain-specific foundational models,with existing solutions often relying on general-purpose LLMs that fail toaddress the unique challenges and technical intricacies of O-RAN. To bridgethis gap, we introduce ORANSight-2.0 (O-RAN Insights), a pioneering initiativeaimed at developing specialized foundational LLMs tailored for O-RAN. Built on18 LLMs spanning five open-source LLM frameworks, ORANSight-2.0 fine-tunesmodels ranging from 1 to 70B parameters, significantly reducing reliance onproprietary, closed-source models while enhancing performance for O-RAN. At thecore of ORANSight-2.0 is RANSTRUCT, a novel Retrieval-Augmented Generation(RAG) based instruction-tuning framework that employs two LLM agents to createhigh-quality instruction-tuning datasets. The generated dataset is then used tofine-tune the 18 pre-trained open-source LLMs via QLoRA. To evaluateORANSight-2.0, we introduce srsRANBench, a novel benchmark designed for codegeneration and codebase understanding in the context of srsRAN, a widely used5G O-RAN stack. We also leverage ORANBench13K, an existing benchmark forassessing O-RAN-specific knowledge. Our comprehensive evaluations demonstratethat ORANSight-2.0 models outperform general-purpose and closed-source models,such as ChatGPT-4o and Gemini, by 5.421% on ORANBench and 18.465% onsrsRANBench, achieving superior performance while maintaining lowercomputational and energy costs. We also experiment with RAG-augmented variantsof ORANSight-2.0 LLMs and thoroughly evaluate their energy characteristics,demonstrating costs for training, standard inference, and RAG-augmentedinference.</description>
      <author>example@mail.com (Pranshav Gajjar, Vijay K. Shah)</author>
      <guid isPermaLink="false">2503.05200v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>RTFusion: A depth estimation network based on multimodal fusion in challenging scenarios</title>
      <link>http://arxiv.org/abs/2503.04821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种用于深度估计的多模态模型RTFusion，结合RGB和THR数据的优点来提高复杂场景下的深度估计准确性。&lt;h4&gt;背景&lt;/h4&gt;单一模式（如可见光或热红外影像）在复杂的现实世界环境中进行深度估计时面临挑战。&lt;h4&gt;目的&lt;/h4&gt;通过引入一种新颖的多模态融合机制EGFusion来增强深度估计模型的鲁棒性和精度。&lt;h4&gt;方法&lt;/h4&gt;RTFusion模型利用RGB和THR图像数据，包含Mutual Complementary Attention (MCA)模块和Edge Saliency Enhancement Module (ESEM)模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，在MS2和ViViD++等不同挑战性环境下，该模型能够生成高质量的深度图。&lt;h4&gt;结论&lt;/h4&gt;提出的方法在需要可靠深度估计的应用中展现出潜力，例如自动驾驶、机器人技术和增强现实技术。&lt;h4&gt;翻译&lt;/h4&gt;复杂现实世界场景中的深度估计是一项具有挑战性的任务，尤其是在仅依赖单一模态（如可见光或热红外影像）的情况下。本文提出了一个新颖的多模式深度估计模型RTFusion，通过结合RGB和THR数据的优点来提高深度估计的准确性和鲁棒性。该模型包括Mutual Complementary Attention (MCA)模块用于跨模态特征对齐以及Edge Saliency Enhancement Module (ESEM)以增强边缘细节保存能力。在MS2和ViViD++等数据集上的全面实验表明，所提出的模型能够在夜间、雨天及高反光等各种挑战性环境下持续生成高质量的深度图。实验结果强调了该方法在需要可靠深度估计的应用（如自动驾驶、机器人技术和增强现实）中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Depth estimation in complex real-world scenarios is a challenging task,especially when relying solely on a single modality such as visible light orthermal infrared (THR) imagery. This paper proposes a novel multimodal depthestimation model, RTFusion, which enhances depth estimation accuracy androbustness by integrating the complementary strengths of RGB and THR data. TheRGB modality provides rich texture and color information, while the THRmodality captures thermal patterns, ensuring stability under adverse lightingconditions such as extreme illumination. The model incorporates a unique fusionmechanism, EGFusion, consisting of the Mutual Complementary Attention (MCA)module for cross-modal feature alignment and the Edge Saliency EnhancementModule (ESEM) to improve edge detail preservation. Comprehensive experiments onthe MS2 and ViViD++ datasets demonstrate that the proposed model consistentlyproduces high-quality depth maps across various challenging environments,including nighttime, rainy, and high-glare conditions. The experimental resultshighlight the potential of the proposed method in applications requiringreliable depth estimation, such as autonomous driving, robotics, and augmentedreality.</description>
      <author>example@mail.com (Zelin Meng, Takanori Fukao)</author>
      <guid isPermaLink="false">2503.04821v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic-KGQA: A Scalable Framework for Generating Adaptive Question Answering Datasets</title>
      <link>http://arxiv.org/abs/2503.05049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Dynamic-KGQA的框架，该框架能够根据知识图谱生成适应性的问题回答数据集。这种方法旨在解决传统静态基准测试可能导致的大规模语言模型记忆问题，并能提供可靠和可重复的结果。&lt;h4&gt;背景&lt;/h4&gt;随着基础模型的发展，对问答系统（QA）进行强大、灵活和大规模评估的需求变得越来越重要。传统的QA基准通常固定且公开，容易被大型语言模型记住并导致过拟合。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够生成适应性数据集的框架，以确保更准确地评估大型语言模型在实际场景中的表现，并减少记忆的风险。&lt;h4&gt;方法&lt;/h4&gt;Dynamic-KGQA使用知识图谱（KG）来生成新的问答数据集变体。这种方法保持底层分布的同时每次运行都会生成不同的数据集，从而提供公平和可重复的评估结果。&lt;h4&gt;主要发现&lt;/h4&gt;该框架支持细粒度控制数据集特性，并且能够为特定领域或主题生成问答数据集。此外，Dynamic-KGQA还能产生紧凑、语义一致的小知识图谱，有助于训练和评价KGQA模型。&lt;h4&gt;结论&lt;/h4&gt;通过引入动态可定制的基准测试方法，Dynamic-KGQA提供了一种更严格和灵活的方式来评估QA系统。&lt;h4&gt;翻译&lt;/h4&gt;随着问答（QA）系统在基础模型快速演变中的进步，需要强大、适应性强且大规模的评估基准变得至关重要。传统的QA基准通常静态且公开获取，使其容易受到数据污染和大型语言模型的记忆风险影响，从而导致对模型泛化能力的高估以及对其真实世界性能的可靠评估困难。本文提出了一种基于知识图谱（KG）生成自适应问答数据集的可扩展框架Dynamic-KGQA，旨在减轻记忆风险的同时保持统计一致性。与固定基准不同，Dynamic-KGQA每次运行都会生成新的数据集变体，同时保持底层分布不变，从而实现公平和可重复的评估。此外，该框架提供对数据集特性的细粒度控制，并支持特定领域或主题的问答数据集生成。动态、自定义化的基准测试范式能够使问答系统的更严谨与适应性评估得以实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As question answering (QA) systems advance alongside the rapid evolution offoundation models, the need for robust, adaptable, and large-scale evaluationbenchmarks becomes increasingly critical. Traditional QA benchmarks are oftenstatic and publicly available, making them susceptible to data contaminationand memorization by large language models (LLMs). Consequently, staticbenchmarks may overestimate model generalization and hinder a reliableassessment of real-world performance. In this work, we introduce Dynamic-KGQA,a scalable framework for generating adaptive QA datasets from knowledge graphs(KGs), designed to mitigate memorization risks while maintaining statisticalconsistency across iterations. Unlike fixed benchmarks, Dynamic-KGQA generatesa new dataset variant on every run while preserving the underlyingdistribution, enabling fair and reproducible evaluations. Furthermore, ourframework provides fine-grained control over dataset characteristics,supporting domain-specific and topic-focused QA dataset generation.Additionally, Dynamic-KGQA produces compact, semantically coherent subgraphsthat facilitate both training and evaluation of KGQA models, enhancing theirability to leverage structured knowledge effectively. To align with existingevaluation protocols, we also provide static large-scale train/test/validationsplits, ensuring comparability with prior methods. By introducing a dynamic,customizable benchmarking paradigm, Dynamic-KGQA enables a more rigorous andadaptable evaluation of QA systems.</description>
      <author>example@mail.com (Preetam Prabhu Srikar Dammu, Himanshu Naidu, Chirag Shah)</author>
      <guid isPermaLink="false">2503.05049v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Continual Pre-training of MoEs: How robust is your router?</title>
      <link>http://arxiv.org/abs/2503.05029v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了稀疏激活的专家混合（MoE）架构在大型语言模型中的持续预训练能力，特别是在分布变化和遗忘问题上的表现。&lt;h4&gt;背景&lt;/h4&gt;许多前沿的语言模型已经采用了MoE架构。为了利用大量新收集的数据扩展这些模型的能力，研究人员希望了解如何对现有模型进行增量式的微调而不完全重新训练。&lt;h4&gt;目的&lt;/h4&gt;研究在稀疏激活的专家混合（MoE）解码器独占变压器中，路由算法是否会影响持续预训练的表现，并探讨保持性能所需的策略。&lt;h4&gt;方法&lt;/h4&gt;通过大规模实验（涉及超过20亿参数的Switch和DeepSeek MoE大语言模型，在6000亿标记上进行训练），使用Sinkhorn-Balanced和Z-and-Aux-loss-balanced路由算法评估MoE持续预训练的表现。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，无论是否采用重放机制（replay），基于Sinkhorn-Balanced和Z-and-Aux-loss-balanced的路由算法在分布变化下表现稳健，并且能够在不完全重新训练的情况下保持样本效率和性能水平。&lt;h4&gt;结论&lt;/h4&gt;研究表明，MoE大语言模型可以以较低的成本持续预训练并匹配完全重新训练后的性能。这意味着，在实际应用中使用MoE架构可能更加高效且具有成本效益。&lt;h4&gt;翻译&lt;/h4&gt;Sparsely-activated Mixture of Experts (MoE) transformers are promising architectures for foundation models. Compared to dense transformers, MoEs offer better sample efficiency during training and stronger performance. The study examines the impact of routing algorithms on continual pre-training in MoE transformers and finds that MoE LLMs maintain their sample efficiency and performance relative to a FLOP-matched dense model even without replay.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparsely-activated Mixture of Experts (MoE) transformers are promisingarchitectures for foundation models. Compared to dense transformers thatrequire the same amount of floating point operations (FLOPs) per forward pass,MoEs benefit from improved sample efficiency at training time and achieve muchstronger performance. Many closed-source and open-source frontier languagemodels have thus adopted an MoE architecture. Naturally, practitioners willwant to extend the capabilities of these models with large amounts of newlycollected data without completely re-training them. Prior work has shown that asimple combination of replay and learning rate re-warming and re-decaying canenable the continual pre-training (CPT) of dense decoder-only transformers withminimal performance degradation compared to full re-training. In the case ofdecoder-only MoE transformers, however, it is unclear how the routing algorithmwill impact continual pre-training performance: 1) do the MoE transformer'srouters exacerbate forgetting relative to a dense model?; 2) do the routersmaintain a balanced load on previous distributions after CPT?; 3) are the samestrategies applied to dense models sufficient to continually pre-train MoELLMs? In what follows, we conduct a large-scale (&gt;2B parameter switch andDeepSeek MoE LLMs trained for 600B tokens) empirical study across four MoEtransformers to answer these questions. Our results establish a surprisingrobustness to distribution shifts for both Sinkhorn-Balanced andZ-and-Aux-loss-balanced routing algorithms, even in MoEs continuallypre-trained without replay. Moreover, we show that MoE LLMs maintain theirsample efficiency (relative to a FLOP-matched dense model) during CPT and thatthey can match the performance of a fully re-trained MoE at a fraction of thecost.</description>
      <author>example@mail.com (Benjamin Thérien, Charles-Étienne Joseph, Zain Sarwar, Ashwinee Panda, Anirban Das, Shi-Xiong Zhang, Stephen Rawls, Sambit Sahu, Eugene Belilovsky, Irina Rish)</author>
      <guid isPermaLink="false">2503.05029v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>HeTGB: A Comprehensive Benchmark for Heterophilic Text-Attributed Graphs</title>
      <link>http://arxiv.org/abs/2503.04822v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Heterophilic Text-attributed Graph Benchmark (HeTGB)，这是一个包含五种现实世界异质图数据集的新型基准，这些数据集来自不同的领域，并且节点通过广泛的文本描述得到了增强。&lt;h4&gt;背景&lt;/h4&gt;图形神经网络在同构假设下对关系数据建模取得了成功。然而，在许多实际场景中，链接节点属于不同类别或具有多样属性的情况（即异质性）普遍存在。此外，图中的许多节点都与文本描述相关联，形成了包含异质性的文本属性图。&lt;h4&gt;目的&lt;/h4&gt;为解决目前缺乏全面基准的问题，本文提出了HeTGB，旨在评估图形神经网络、预训练语言模型以及协同训练方法在节点分类任务上的性能，并探讨异质性文本属性图中的挑战及现有模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;HeTGB包括五个来自不同领域的现实世界异质图数据集，节点通过丰富的文本描述进行了增强。该基准允许对GNNs、PLMs和协同训练方法进行系统评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验展示了文本属性在异质图中的作用，并分析了异质性文本属性图所面临的挑战以及现有模型的局限性，还探讨了图形结构与文本属性之间的相互关系。研究提供了基准数据集及其基线实现的公开资源。&lt;h4&gt;结论&lt;/h4&gt;通过HeTGB的研究揭示了当前模型处理异质图和文本属性时存在的问题，并为未来相关领域的进一步研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到，图神经网络在处理同构图形中的关系数据建模中表现出色。然而，在许多现实世界的应用场景中，链接节点可能来自不同的类别或具有不同特征（即异质性）。此外，很多节点还附带了描述性的文本信息，形成了含有异质性的文本属性图。由于缺乏全面的基准测试集，这些异质性图的研究进展缓慢。为此，本文介绍了Heterophilic Text-attributed Graph Benchmark (HeTGB)，该框架包含五个来自不同领域的现实世界异质数据集，并且节点通过广泛的文本描述得到了增强。HeTGB能够用于评估GNNs、预训练的语言模型（PLMs）和协同训练方法在节点分类任务中的表现，同时还能展示文本属性对于处理异质性图的重要性和当前的方法的限制。研究者公开发布了该基准及其基线实现以推动进一步的研究进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have demonstrated success in modeling relationaldata primarily under the assumption of homophily. However, many real-worldgraphs exhibit heterophily, where linked nodes belong to different categoriesor possess diverse attributes. Additionally, nodes in many domains areassociated with textual descriptions, forming heterophilic text-attributedgraphs (TAGs). Despite their significance, the study of heterophilic TAGsremains underexplored due to the lack of comprehensive benchmarks. To addressthis gap, we introduce the Heterophilic Text-attributed Graph Benchmark(HeTGB), a novel benchmark comprising five real-world heterophilic graphdatasets from diverse domains, with nodes enriched by extensive textualdescriptions. HeTGB enables systematic evaluation of GNNs, pre-trained languagemodels (PLMs) and co-training methods on the node classification task. Throughextensive benchmarking experiments, we showcase the utility of text attributesin heterophilic graphs, analyze the challenges posed by heterophilic TAGs andthe limitations of existing models, and provide insights into the interplaybetween graph structures and textual attributes. We have publicly releasedHeTGB with baseline implementations to facilitate further research in thisfield.</description>
      <author>example@mail.com (Shujie Li, Yuxia Wu, Chuan Shi, Yuan Fang)</author>
      <guid isPermaLink="false">2503.04822v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Incentivizing Multi-Tenant Split Federated Learning for Foundation Models at the Network Edge</title>
      <link>http://arxiv.org/abs/2503.04971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Index Terms: Foundation models, Edge computing, Split federated  learning, Multi-tenant system, Incentive mechanism&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为PRINCE的新型价格激励机制，以促进多租户环境下的分割联邦学习（SFL）过程中的设备参与，并提高基础模型在边缘设备上的高效微调。&lt;h4&gt;背景&lt;/h4&gt;现有的研究主要集中在单一租户的SFL场景上，缺乏针对多种租户环境的定制化激励机制。在实际应用中，边缘网络通常需要支持多样化的下游任务，每个租户有不同的模型类型、性能目标和微调截止日期等要求。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的激励机制PRINCE，以促进多租户SFL场景下不同设备的有效参与，并确保各自的需求得到满足。&lt;h4&gt;方法&lt;/h4&gt;首先设计了一种偏倚抵抗的全局SFL模型聚合方案来消除由独立设备微调产生的模型偏差。接着推导出一个严格的SFL收敛边界，用于评估异构设备对基础模型性能提升贡献的程度，从而指导租户制定激励策略。最后通过Stackelberg博弈均衡（SE）分析模型间竞争，并得出每个SFL租户的最优激励策略。&lt;h4&gt;主要发现&lt;/h4&gt;模拟实验表明，在处理四种典型的SFL租户类型（ViT、BERT、Whisper和LLaMA），涵盖文本、图像和音频等多种数据模式的任务时，PRINCE机制相比现有最佳方法可将基础模型微调加速最多3.07倍，并且始终能够满足性能目标。&lt;h4&gt;结论&lt;/h4&gt;提出的PRINCE激励机制能够有效促进多租户环境中SFL的高效执行，提高资源利用效率并确保良好的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) such as GPT-4 exhibit exceptional generativecapabilities across diverse downstream tasks through fine-tuning. SplitFederated Learning (SFL) facilitates privacy-preserving FM fine-tuning onresource-constrained local devices by offloading partial FM computations toedge servers, enabling device-edge synergistic fine-tuning. Practical edgenetworks often host multiple SFL tenants to support diversified downstreamtasks. However, existing research primarily focuses on single-tenant SFLscenarios, and lacks tailored incentive mechanisms for multi-tenant settings,which are essential to effectively coordinate self-interested local devices forparticipation in various downstream tasks, ensuring that each SFL tenant'sdistinct FM fine-tuning requirements (e.g., FM types, performance targets, andfine-tuning deadlines) are met. To address this gap, we propose a novelPrice-Incentive Mechanism (PRINCE) that guides multiple SFL tenants to offerstrategic price incentives, which solicit high-quality device participation forefficient FM fine-tuning. Specifically, we first develop a bias-resilientglobal SFL model aggregation scheme to eliminate model biases caused byindependent device participation. We then derive a rigorous SFL convergencebound to evaluate the contributions of heterogeneous devices to FM performanceimprovements, guiding the incentive strategies of SFL tenants. Furthermore, wemodel inter-tenant device competition as a congestion game for Stackelbergequilibrium (SE) analysis, deriving each SFL tenant's optimal incentivestrategy. Extensive simulations involving four representative SFL tenant types(ViT, BERT, Whisper, and LLaMA) across diverse data modalities (text, images,and audio) demonstrate that PRINCE accelerates FM fine-tuning by up to 3.07xcompared to state-of-the-art approaches, while consistently meeting fine-tuningperformance targets.</description>
      <author>example@mail.com (Songyuan Li, Jia Hu, Geyong Min, Haojun Huang)</author>
      <guid isPermaLink="false">2503.04971v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning</title>
      <link>http://arxiv.org/abs/2503.04877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Videos, code, and data: https://pairlab.github.io/Adapt3R&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;模仿学习(IL)在训练机器人执行复杂多样的操作任务方面非常有效，但当观测数据超出训练分布时，其性能急剧下降。为了提高IL策略的泛化能力，提出了使用3D场景表示的方法，但在跨实体和新相机姿态设置中进行评估后发现改进有限。为解决这些挑战，本文提出了一种通用的3D场景表示方法Adapt3R，它通过新颖的架构将一个或多个RGBD摄像头的数据综合成一个向量，供任意IL算法使用。&lt;h4&gt;背景&lt;/h4&gt;模仿学习(IL)在机器人执行复杂任务中表现出色，但在面对未见过的情境时性能大幅下降。尽管已有研究尝试利用包含标定RGBD相机观测信息的3D场景表示来改进策略泛化能力，但其效果有限。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的方法Adapt3R以提高模仿学习策略在跨实体和新相机姿态下的零样本迁移能力。&lt;h4&gt;方法&lt;/h4&gt;采用预训练的2D骨干网络提取关于场景的语义信息，并利用3D作为定位该信息相对于末端执行器的方式，合成单一向量供IL算法使用。通过与最新的多任务IL算法结合进行端到端训练来验证其效果。&lt;h4&gt;主要发现&lt;/h4&gt;Adapt3R能够在保持多任务学习能力的同时实现零样本迁移至新实体和相机姿态，并且进行了详尽的消融实验以探索点云观测编码器的设计空间。&lt;h4&gt;结论&lt;/h4&gt;通过引入Adapt3R，模仿学习策略在面对未知情境时的表现得到了显著改善，为未来机器人操作任务的学习提供了新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation Learning (IL) has been very effective in training robots to performcomplex and diverse manipulation tasks. However, its performance declinesprecipitously when the observations are out of the training distribution. 3Dscene representations that incorporate observations from calibrated RGBDcameras have been proposed as a way to improve generalizability of IL policies,but our evaluations in cross-embodiment and novel camera pose settings foundthat they show only modest improvement. To address those challenges, we proposeAdaptive 3D Scene Representation (Adapt3R), a general-purpose 3D observationencoder which uses a novel architecture to synthesize data from one or moreRGBD cameras into a single vector that can then be used as conditioning forarbitrary IL algorithms. The key idea is to use a pretrained 2D backbone toextract semantic information about the scene, using 3D only as a medium forlocalizing this semantic information with respect to the end-effector. We showthat when trained end-to-end with several SOTA multi-task IL algorithms,Adapt3R maintains these algorithms' multi-task learning capacity while enablingzero-shot transfer to novel embodiments and camera poses. Furthermore, weprovide a detailed suite of ablation and sensitivity experiments to elucidatethe design space for point cloud observation encoders.</description>
      <author>example@mail.com (Albert Wilcox, Mohamed Ghanem, Masoud Moghani, Pierre Barroso, Benjamin Joffe, Animesh Garg)</author>
      <guid isPermaLink="false">2503.04877v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Invisible Strings: Revealing Latent Dancer-to-Dancer Interactions with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.04816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 10 figures, submitted to ICCC'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文探讨了双人舞蹈中的合作关系，并提出了一种基于图神经网络（GNN）的方法来捕捉和解释舞者之间复杂的互动关系。&lt;h4&gt;背景&lt;/h4&gt;在双人舞蹈中，舞者需要密切注意彼此的空间位置、动量以及相互施加的力量。虽然艺术家在表演时对这些细节有深刻的理解，但传统的记录方式无法准确地捕捉到这种细致的关系。&lt;h4&gt;目的&lt;/h4&gt;通过与关注深化合作关系理解的舞蹈艺术家合作，利用GNN来揭示和解释两位舞者之间复杂的联系。&lt;h4&gt;方法&lt;/h4&gt;使用视频到3D姿态提取流水线从当代双人舞蹈视频中提取3D动作，并对数据进行专门预处理以改进重建效果。然后训练GNN预测舞者之间的加权连接关系。&lt;h4&gt;主要发现&lt;/h4&gt;通过可视化和解释预测的两舞者间的关系，展示了基于图的方法能够构建描述双人合作动态的新模型。&lt;h4&gt;结论&lt;/h4&gt;论文提出了一些策略，利用这些见解来指导生成性和共同创造的工作室实践。&lt;h4&gt;翻译&lt;/h4&gt;在双人舞蹈中，舞者需要对彼此的空间位置、动量以及相互施加的力量有高度的理解。虽然艺术家可能在表演时对自己动作与合作伙伴的关系有深刻的认识，但常规的舞蹈记录方法无法捕捉到这种复杂且微妙的互动关系。通过与希望深入了解合作关系的舞蹈艺术家合作，我们利用图神经网络（GNN）来强调并解释两位舞者之间的复杂联系。使用视频到3D姿态提取流水线从精心挑选的当代双人舞蹈视频中提取3D动作，并进行专门预处理以改进重建效果，然后训练GNN预测舞者之间的加权连接关系。通过可视化和解释这些预测的关系，我们展示了基于图的方法能够构建描述双人合作动态的新模型。最后，提供了一些策略示例，说明如何利用这些洞察来指导生成性和共同创造的工作室实践。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dancing in a duet often requires a heightened attunement to one's partner:their orientation in space, their momentum, and the forces they exert on you.Dance artists who work in partnered settings might have a strong embodiedunderstanding in the moment of how their movements relate to their partner's,but typical documentation of dance fails to capture these varied and subtlerelationships. Working closely with dance artists interested in deepening theirunderstanding of partnering, we leverage Graph Neural Networks (GNNs) tohighlight and interpret the intricate connections shared by two dancers. Usinga video-to-3D-pose extraction pipeline, we extract 3D movements from curatedvideos of contemporary dance duets, apply a dedicated pre-processing to improvethe reconstruction, and train a GNN to predict weighted connections between thedancers. By visualizing and interpreting the predicted relationships betweenthe two movers, we demonstrate the potential for graph-based methods toconstruct alternate models of the collaborative dynamics of duets. Finally, weoffer some example strategies for how to use these insights to inform agenerative and co-creative studio practice.</description>
      <author>example@mail.com (Luis Vitor Zerkowski, Zixuan Wang, Ilya Vidrin, Mariel Pettee)</author>
      <guid isPermaLink="false">2503.04816v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning</title>
      <link>http://arxiv.org/abs/2503.04812v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文提出了一个新的框架LLaVE，旨在改进现有基于语言多模态（LMM）的嵌入模型在处理困难负样本时的表现。&lt;h4&gt;背景&lt;/h4&gt;当前普遍使用的基于LMM的嵌入模型训练使用标准InfoNCE损失函数时，在正负对相似性分布上存在高度重叠，这使得区分硬负样本变得困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单而有效的框架，该框架可以根据负面实例在鉴别上的难度动态改善嵌入模型的表现学习。&lt;h4&gt;方法&lt;/h4&gt;训练了一系列新的模型（LLaVE），并在包含4个元任务和36个数据集的MMEB基准上进行了评估。这些新模型旨在提高区分难负样本的能力，并且显示出良好的可扩展性和效率。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，LLaVE在所有测试中建立了强大的基线并实现了最先进的性能。例如，LLAve-2B超越了之前的7B模型的最佳表现，而LLAVE-7B则进一步提高了6.2分的性能。&lt;h4&gt;结论&lt;/h4&gt;尽管LLaVE是在图像文本数据上训练的，但该模型可以零样本泛化到视频文本检索任务，并表现出强大的性能。这表明LLaVE在其他嵌入任务中具有巨大的潜在迁移能力。&lt;h4&gt;翻译&lt;/h4&gt;通用多模态嵌入模型在交错式图文检索、多模态RAG以及多模态聚类等任务中发挥着关键作用。然而，实证结果显示，现有基于语言的多模态模型使用标准InfoNCE损失训练时，在正负对相似性分布上存在高度重叠，使得区分硬负样本变得困难。为解决此问题，我们提出了一种简单而有效的框架，该框架可以根据负面实例在鉴别上的难度动态改善嵌入模型的表现学习。在此框架内，我们在MMEB基准上训练了一系列新的模型（LLaVE），并对其进行了评估。实验结果表明，LLAVE建立了强大的基线，并实现了最先进的性能表现的同时还展示了极好的可扩展性和效率。具体来说，LLAve-2B超越了之前的7B模型的最佳表现，而LLAVE-7B则进一步提高了6.2分的性能。虽然LLaVE是在图像文本数据上训练的，但该模型可以零样本泛化到视频文本检索任务，并表现出强大的性能，这表明LLAVE在其他嵌入任务中具有巨大的潜在迁移能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Universal multimodal embedding models play a critical role in tasks such asinterleaved image-text retrieval, multimodal RAG, and multimodal clustering.However, our empirical results indicate that existing LMM-based embeddingmodels trained with the standard InfoNCE loss exhibit a high degree of overlapin similarity distribution between positive and negative pairs, making itchallenging to distinguish hard negative pairs effectively. To deal with thisissue, we propose a simple yet effective framework that dynamically improvesthe embedding model's representation learning for negative pairs based on theirdiscriminative difficulty. Within this framework, we train a series of models,named LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasksand 36 datasets. Experimental results show that LLaVE establishes strongerbaselines that achieve state-of-the-art (SOTA) performance while demonstratingstrong scalability and efficiency. Specifically, LLaVE-2B surpasses theprevious SOTA 7B models, while LLaVE-7B achieves a further performanceimprovement of 6.2 points. Although LLaVE is trained on image-text data, it cangeneralize to text-video retrieval tasks in a zero-shot manner and achievestrong performance, demonstrating its remarkable potential for transfer toother embedding tasks.</description>
      <author>example@mail.com (Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, Jinsong Su)</author>
      <guid isPermaLink="false">2503.04812v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Spatial regularisation for improved accuracy and interpretability in keypoint-based registration</title>
      <link>http://arxiv.org/abs/2503.04499v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种三重损失函数，用于通过优化固定和移动体素之间的相似性度量来改进无监督注册策略。此方法特别关注基于无监督关键点检测的方法，旨在增强特征的空间分布可解释性。&lt;h4&gt;背景&lt;/h4&gt;无监督配准策略不依赖于地面实况变换或分割标签，并且近年来以一种新类型的无监督关键点检测技术为基础的方案非常有前景，因为它们可以提高算法的透明度和理解度。&lt;h4&gt;目的&lt;/h4&gt;改进基于关键点的注册方法的空间特征分布，使其更加精确并且具有解剖学意义，同时提高解释性。&lt;h4&gt;方法&lt;/h4&gt;通过引入三种损失函数来实现：使用KL散度将特征视为概率型的关键点进行建模；锐化这些特征的空间分布以增加检测到的地标精度；以及在关键点之间引入新的排斥损失以鼓励空间多样性。&lt;h4&gt;主要发现&lt;/h4&gt;三重损失改进了特征的可解释性，使其更精确、更具解剖学意义。它在胎儿刚体运动跟踪和脑MRI仿射配准任务中均优于现有的无监督策略，并接近于有监督方法的表现。&lt;h4&gt;结论&lt;/h4&gt;这种新颖的方法显著提高了基于关键点检测的无监督注册技术的效果，在医学图像处理领域具有潜在的应用价值，代码可在GitHub上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised registration strategies bypass requirements in ground truthtransforms or segmentations by optimising similarity metrics between fixed andmoved volumes. Among these methods, a recent subclass of approaches based onunsupervised keypoint detection stand out as very promising forinterpretability. Specifically, these methods train a network to predictfeature maps for fixed and moving images, from which explainable centres ofmass are computed to obtain point clouds, that are then aligned in closed-form.However, the features returned by the network often yield spatially diffusepatterns that are hard to interpret, thus undermining the purpose ofkeypoint-based registration. Here, we propose a three-fold loss to regularisethe spatial distribution of the features. First, we use the KL divergence tomodel features as point spread functions that we interpret as probabilistickeypoints. Then, we sharpen the spatial distributions of these features toincrease the precision of the detected landmarks. Finally, we introduce a newrepulsive loss across keypoints to encourage spatial diversity. Overall, ourloss considerably improves the interpretability of the features, which nowcorrespond to precise and anatomically meaningful landmarks. We demonstrate ourthree-fold loss in foetal rigid motion tracking and brain MRI affineregistration tasks, where it not only outperforms state-of-the-art unsupervisedstrategies, but also bridges the gap with state-of-the-art supervised methods.Our code is available at https://github.com/BenBillot/spatial_regularisation.</description>
      <author>example@mail.com (Benjamin Billot, Ramya Muthukrishnan, Esra Abaci-Turk, P. Ellen Grant, Nicholas Ayache, Hervé Delingette, Polina Golland)</author>
      <guid isPermaLink="false">2503.04499v2</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Parallel Corpora for Machine Translation in Low-resource Indic Languages: A Comprehensive Review</title>
      <link>http://arxiv.org/abs/2503.04797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该论文综述了可用于印地语等低资源语言的平行语料库，探讨这些语料库在机器翻译模型训练中的作用和挑战。&lt;h4&gt;背景&lt;/h4&gt;平行语料库对于训练缺乏高质量双语文本数据的低资源语言的机器翻译（MT）模型至关重要。特别是对多种语言族、书写系统和地区变体的印地语等印度语言而言，这种重要性更加突出。&lt;h4&gt;目的&lt;/h4&gt;综述可用的印地语平行语料库，并分类这些语料库为纯文本到文本、代码切换和各种多模式数据集类别，强调它们在开发健壮的跨语言机器翻译系统中的意义。&lt;h4&gt;方法&lt;/h4&gt;除了资源列举之外，还批判性地审视了语料库创建过程中面临的挑战，如语言多样性、书写方式变化、数据稀缺性和非正式文本内容盛行的问题。同时讨论并评估这些语料库的质量和领域代表性等。&lt;h4&gt;主要发现&lt;/h4&gt;指出了开放性的挑战包括印地语之间的数据不平衡、质量与数量的权衡以及嘈杂、非正式和方言化数据对机器翻译性能的影响。&lt;h4&gt;结论&lt;/h4&gt;该论文概述了未来的研究方向，如利用跨语言转移学习来扩大多语言数据集并整合多媒体资源以增强翻译质量。据我们所知，这是首次专门针对低资源印地语的平行语料库进行详尽综述，并将其置于机器翻译上下文中。&lt;h4&gt;翻译&lt;/h4&gt;Parallel corpora play an important role in training machine translation (MT)models, particularly for low-resource languages where high-quality bilingualdata is scarce. This review provides a comprehensive overview of availableparallel corpora for Indic languages, which span diverse linguistic families,scripts, and regional variations.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parallel corpora play an important role in training machine translation (MT)models, particularly for low-resource languages where high-quality bilingualdata is scarce. This review provides a comprehensive overview of availableparallel corpora for Indic languages, which span diverse linguistic families,scripts, and regional variations. We categorize these corpora intotext-to-text, code-switched, and various categories of multimodal datasets,highlighting their significance in the development of robust multilingual MTsystems. Beyond resource enumeration, we critically examine the challengesfaced in corpus creation, including linguistic diversity, script variation,data scarcity, and the prevalence of informal textual content.We also discussand evaluate these corpora in various terms such as alignment quality anddomain representativeness. Furthermore, we address open challenges such as dataimbalance across Indic languages, the trade-off between quality and quantity,and the impact of noisy, informal, and dialectal data on MT performance.Finally, we outline future directions, including leveraging cross-lingualtransfer learning, expanding multilingual datasets, and integrating multimodalresources to enhance translation quality. To the best of our knowledge, thispaper presents the first comprehensive review of parallel corpora specificallytailored for low-resource Indic languages in the context of machinetranslation.</description>
      <author>example@mail.com (Rahul Raja, Arpita Vats)</author>
      <guid isPermaLink="false">2503.04797v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Fidelity Policy Gradient Algorithms</title>
      <link>http://arxiv.org/abs/2503.05696v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多保真度策略梯度（MFPG）是一种新的强化学习框架，它利用少量的目标环境数据和大量的低保真度模拟数据来提高训练效率。&lt;h4&gt;背景&lt;/h4&gt;许多强化学习算法需要大量数据，并且在难以频繁与操作系统交互或高保真仿真昂贵或不可用的情况下无法使用。然而，低成本的低保真度模拟器可以为RL训练提供有用的数据。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的强化学习框架MFPG，该框架结合目标环境的小量样本和大量低保真度仿真实验数据来形成无偏估计，减少方差，并提高基于策略的梯度估计准确性。&lt;h4&gt;方法&lt;/h4&gt;开发了两种多保真度策略梯度算法：REINFORCE和近似策略优化（PPO）。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在目标环境样本有限的情况下，MFPG可以比只使用高保真数据的基础线方法获得更高的奖励，并且提高了训练的稳定性。即使当基线模型在仿真交互次数提高十倍时，MFPG仍然能够与之匹配或超越。此外，MFPG甚至可以在低保真度环境差异很大时依然有效。&lt;h4&gt;结论&lt;/h4&gt;MFPG不仅为高效的模拟到真实环境转移提供了一种新的范例，而且还提供了管理策略性能和数据收集成本之间权衡的原理性方法。&lt;h4&gt;翻译&lt;/h4&gt;许多强化学习算法需要大量数据，使它们在难以频繁与操作系统交互或高保真仿真昂贵或不可用的应用中无法使用。与此同时，低成本模拟器（如减少阶模型、启发式奖励函数或生成环境）可以便宜地提供对于RL训练有用的少量有用的数据，即使这些模拟器过于粗糙而不能直接进行从仿真的真实环境转移。我们提出了多保真度策略梯度（MFPG），这是一种强化学习框架，它可以将目标环境的小量数据与大量低保真度仿真数据混合，以形成无偏、方差减小的估计（控制变量）用于基于策略的梯度。通过开发REINFORCE和近似策略优化的多保真变体来实例化该框架。一系列模拟机器人基准问题上的实验结果表明，在目标环境样本有限时，MFPG比仅使用高保真数据的基础线方法最高可达3.9倍的奖励，并提高了训练稳定性。此外，即使当基线模型在仿真交互次数提高十倍时，MFPG仍然能够与之匹配或超越。最后，观察到即便低保真度环境差异巨大时，MFPG依然可以有效训练策略。因此，MFPG不仅为高效的从模拟到真实环境转移提供了一种新的范例，而且还提供了管理策略性能和数据收集成本之间权衡的原理性方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many reinforcement learning (RL) algorithms require large amounts of data,prohibiting their use in applications where frequent interactions withoperational systems are infeasible, or high-fidelity simulations are expensiveor unavailable. Meanwhile, low-fidelity simulators--such as reduced-ordermodels, heuristic reward functions, or generative world models--can cheaplyprovide useful data for RL training, even if they are too coarse for directsim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RLframework that mixes a small amount of data from the target environment with alarge volume of low-fidelity simulation data to form unbiased, reduced-varianceestimators (control variates) for on-policy policy gradients. We instantiatethe framework by developing multi-fidelity variants of two policy gradientalgorithms: REINFORCE and proximal policy optimization. Experimental resultsacross a suite of simulated robotics benchmark problems demonstrate that whentarget-environment samples are limited, MFPG achieves up to 3.9x higher rewardand improves training stability when compared to baselines that only usehigh-fidelity data. Moreover, even when the baselines are given morehigh-fidelity samples--up to 10x as many interactions with the targetenvironment--MFPG continues to match or outperform them. Finally, we observethat MFPG is capable of training effective policies even when the low-fidelityenvironment is drastically different from the target environment. MFPG thus notonly offers a novel paradigm for efficient sim-to-real transfer but alsoprovides a principled approach to managing the trade-off between policyperformance and data collection costs.</description>
      <author>example@mail.com (Xinjie Liu, Cyrus Neary, Kushagra Gupta, Christian Ellis, Ufuk Topcu, David Fridovich-Keil)</author>
      <guid isPermaLink="false">2503.05696v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Kinodynamic Model Predictive Control for Energy Efficient Locomotion of Legged Robots with Parallel Elasticity</title>
      <link>http://arxiv.org/abs/2503.05666v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures. Accepted for publication at ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用单向并行弹簧（UPS）来提高动态腿足机器人的能量效率的模型预测控制(MPC)框架。&lt;h4&gt;背景&lt;/h4&gt;现有的动态腿足机器人在高速移动时能耗较高，且电机扭矩峰值大。&lt;h4&gt;目的&lt;/h4&gt;通过引入一种新的MPC方法，利用UPS减少电机扭矩峰值和站立阶段的能量消耗，从而提高动态跳跃过程中的能量效率。&lt;h4&gt;方法&lt;/h4&gt;采用了分层控制结构，其中简化的动力学模型的MPC解决方案被用来预热非线性中心动力学和运动学约束下的动力学MPC。通过在单足机器人上加入UPS来验证其效果，并进行模拟实验。&lt;h4&gt;主要发现&lt;/h4&gt;模拟结果显示，在高速跳跃过程中使用UPS装备的单足机器人，运输成本（CoT）降低了38.8%；初步硬件试验表明能耗减少了14.8%。&lt;h4&gt;结论&lt;/h4&gt;该方法证明了通过采用UPS可以有效地提高腿足机器人的能量效率和动态性能。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们介绍了一种利用单向并行弹簧（UPS）来改善动态腿足机器人能量效率的模型预测控制（MPC）框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce a kinodynamic model predictive control (MPC)framework that exploits unidirectional parallel springs (UPS) to improve theenergy efficiency of dynamic legged robots. The proposed method employs ahierarchical control structure, where the solution of MPC with simplifieddynamic models is used to warm-start the kinodynamic MPC, which accounts fornonlinear centroidal dynamics and kinematic constraints. The proposed approachenables energy efficient dynamic hopping on legged robots by using UPS toreduce peak motor torques and energy consumption during stance phases.Simulation results demonstrated a 38.8% reduction in the cost of transport(CoT) for a monoped robot equipped with UPS during high-speed hopping.Additionally, preliminary hardware experiments show a 14.8% reduction in energyconsumption. Video: https://youtu.be/AF11qMXJD48</description>
      <author>example@mail.com (Yulun Zhuang, Yichen Wang, Yanran Ding)</author>
      <guid isPermaLink="false">2503.05666v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Small-Scale Testbeds for Connected and Automated Vehicles and Robot Swarms: Challenges and a Roadmap</title>
      <link>http://arxiv.org/abs/2503.05656v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;目前在用于连接与自动化车辆(CAVs)及机器人集群的小规模测试平台中存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个路线图以解决这些小规模测试平台中的现有挑战。该路线图为“第1届小型化测试平台研讨会”工作坊的一部分，讨论如何改进CAVs和机器人集群的测试环境。&lt;h4&gt;方法&lt;/h4&gt;{'提高可访问性和多样性': '特别是为代表性不足的社区提供支持', '共享最佳实践': '促进测试平台的发展与维护', '连接抽象层': '通过一个抽象层来支持各测试平台间的合作'}&lt;h4&gt;主要内容&lt;/h4&gt;工作坊包括八位受邀演讲者、四篇论文以及一篇关于测试平台调查的研究报告。&lt;h4&gt;其他信息&lt;/h4&gt;{'在线比较表': '调查报告提供了一个包含超过25个测试床的在线对比表格', '链接': [{'调查报告网址': 'https://bassamlab.github.io/testbeds-survey'}, {'工作坊网站': 'https://cpm-remote.lrt.unibwmuenchen.de/iv24-workshop'}]}&lt;h4&gt;翻译&lt;/h4&gt;该文章提出了一个路线图，旨在解决用于连接与自动化车辆(CAVs)和机器人集群的小规模测试平台中的现有挑战。该路线图为6月2日在韩国济州岛举行的IEEE智能汽车研讨会(IV) 2024的“第1届小型化测试平台研讨会”工作坊的一部分，参与人员共同制定了这一计划。该计划包括三个部分：提高可访问性和多样性（特别是为代表性不足的社区提供支持），共享最佳实践以促进开发和维护测试平台，以及通过抽象层连接各测试平台来支持合作。此次研讨会共有八位受邀演讲者、四篇贡献论文，及一篇关于测试床调查的研究报告展示。研究报告在线提供了超过25个测试平台的对比表（网址：https://bassamlab.github.io/testbeds-survey）。该研讨会网站地址为：https://cpm-remote.lrt.unibwmuenchen.de/iv24-workshop。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This article proposes a roadmap to address the current challenges insmall-scale testbeds for Connected and Automated Vehicles (CAVs) and robotswarms. The roadmap is a joint effort of participants in the workshop "1stWorkshop on Small-Scale Testbeds for Connected and Automated Vehicles and RobotSwarms," held on June 2 at the IEEE Intelligent Vehicles Symposium (IV) 2024 inJeju, South Korea. The roadmap contains three parts: 1) enhancing accessibilityand diversity, especially for underrepresented communities, 2) sharing bestpractices for the development and maintenance of testbeds, and 3) connectingtestbeds through an abstraction layer to support collaboration. The workshopfeatures eight invited speakers, four contributed papers [1]-[4], and apresentation of a survey paper on testbeds [5]. The survey paper provides anonline comparative table of more than 25 testbeds, available athttps://bassamlab.github.io/testbeds-survey. The workshop's own website isavailable at https://cpm-remote.lrt.unibwmuenchen.de/iv24-workshop.</description>
      <author>example@mail.com (Jianye Xu, Bassam Alrifaee, Johannes Betz, Armin Mokhtarian, Archak Mittal, Mengchi Cai, Rahul Mangharam, Omar M. Shehata, Catherine M. Elias, Jan-Nico Zaech, Patrick Scheffe, Felix Jahncke, Sangeet Sankaramangalam Ulhas, Kaj Munhoz Arfvidsson)</author>
      <guid isPermaLink="false">2503.05656v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities</title>
      <link>http://arxiv.org/abs/2503.05652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website: https://behavior-robot-suite.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的机器人框架BEHAVIOR Robot Suite (BRS)，用于解决移动操作机器人在家庭环境中执行任务时遇到的挑战。&lt;h4&gt;背景&lt;/h4&gt;真实世界中的家务任务对移动操作机器人提出了重大挑战，现有研究显示成功的任务执行依赖于三项关键的整体控制能力：双手协调、稳定且精确的导航以及广泛的末端效应器可达性。实现这些能力需要精心设计硬件，但这也增加了系统复杂度，使得视触觉运动策略学习变得更加困难。&lt;h4&gt;目的&lt;/h4&gt;通过引入BEHAVIOR Robot Suite (BRS)，该框架旨在解决移动操作机器人在家庭任务中遇到的挑战，并通过一种成本效益高的全身遥操作系统接口进行数据收集以及开发新的算法来学习全身视触觉运动策略。&lt;h4&gt;方法&lt;/h4&gt;BRS构建在一个双臂、轮式机器人上，该机器人配备了4自由度躯干。它结合了低成本的整体遥控界面用于数据采集及一个新颖的算法以学习整体视触觉运动策略。&lt;h4&gt;主要发现&lt;/h4&gt;BRS在五个具有挑战性的家庭任务中进行了评估，这些任务不仅强调三项核心能力而且还引入了额外复杂性，例如长距离导航、与活动和可变形物体互动以及在狭小空间内的操作。&lt;h4&gt;结论&lt;/h4&gt;BRS通过其集成的机器人实体，数据收集接口及学习框架，在实现现实世界的全身操纵方面迈出了重要一步，尤其是为了满足日常家庭任务的需求。该套件开源地址为https://behavior-robot-suite.github.io/&lt;h4&gt;翻译&lt;/h4&gt;真实世界中的家务任务对移动操作机器人提出了重大挑战。通过分析现有的机器人基准测试发现，成功的任务执行依赖于三项关键的整体控制能力：双手协调、稳定且精确的导航以及广泛的末端效应器可达性。实现这些能力需要精心设计硬件，但这也增加了系统复杂度，使得视触觉运动策略学习变得更加困难。为解决这些挑战，我们引入了BEHAVIOR Robot Suite (BRS)，这是一个全面的框架，旨在解决多样化的家庭任务中的全身操纵问题。该套件建立在双臂、轮式机器人上，并配备了4自由度躯干。它结合了一种低成本的整体遥控界面用于数据收集以及一个新颖的算法以学习整体视触觉运动策略。我们在五个具有挑战性的家务任务中评估了BRS，这些任务不仅强调三项核心能力而且还引入了额外复杂性，例如长距离导航、与活动和可变形物体互动以及在狭小空间内的操作。我们相信通过其集成的机器人实体、数据收集接口及学习框架，在实现现实世界的全身操纵方面，BRS迈向了一步重要的进展，尤其是为了满足日常家庭任务的需求。BRS开源地址为https://behavior-robot-suite.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world household tasks present significant challenges for mobilemanipulation robots. An analysis of existing robotics benchmarks reveals thatsuccessful task performance hinges on three key whole-body controlcapabilities: bimanual coordination, stable and precise navigation, andextensive end-effector reachability. Achieving these capabilities requirescareful hardware design, but the resulting system complexity furthercomplicates visuomotor policy learning. To address these challenges, weintroduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework forwhole-body manipulation in diverse household tasks. Built on a bimanual,wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-bodyteleoperation interface for data collection and a novel algorithm for learningwhole-body visuomotor policies. We evaluate BRS on five challenging householdtasks that not only emphasize the three core capabilities but also introduceadditional complexities, such as long-range navigation, interaction witharticulated and deformable objects, and manipulation in confined spaces. Webelieve that BRS's integrated robotic embodiment, data collection interface,and learning framework mark a significant step toward enabling real-worldwhole-body manipulation for everyday household tasks. BRS is open-sourced athttps://behavior-robot-suite.github.io/</description>
      <author>example@mail.com (Yunfan Jiang, Ruohan Zhang, Josiah Wong, Chen Wang, Yanjie Ze, Hang Yin, Cem Gokmen, Shuran Song, Jiajun Wu, Li Fei-Fei)</author>
      <guid isPermaLink="false">2503.05652v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>dARt Vinci: Egocentric Data Collection for Surgical Robot Learning at Scale</title>
      <link>http://arxiv.org/abs/2503.05646v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种用于外科手术环境中的机器人学习的可扩展数据收集平台dARt Vinci，该平台利用增强现实手部追踪和高保真物理引擎捕捉基础手术任务中的细微操作。&lt;h4&gt;背景&lt;/h4&gt;在机器人学习领域中，特别是在像外科应用这样的安全关键领域，获取高质量的数据一直是个难题。这给研究人员带来了挑战，使他们难以充分利用最近强化学习和模仿学习的进步，这些进步大大提高了泛化能力，并使机器人能够自主执行任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种可扩展数据收集平台dARt Vinci，以解决外科手术环境中缺乏训练数据的问题，同时利用增强现实技术提高数据采集效率。&lt;h4&gt;方法&lt;/h4&gt;该系统使用了增强现实手部追踪技术和高保真物理引擎来捕捉基础手术任务中的细微操作。通过消除对实际机器人设置的需求，并提供时间、空间和硬件资源（如多视图传感器和执行器）的灵活性，特殊的模拟成为一种可行的选择。此外，由于其身体跟踪和内容叠加能力，增强现实使机器人数据收集更加自我中心。&lt;h4&gt;主要发现&lt;/h4&gt;用户研究确认了所提议系统的效率和可用性，在使用广泛使用的基础任务训练达芬奇外科手术机器人的遥操作时。与真实机器人设置相比，所有任务的数据吞吐量平均提高了41%；总实验时间平均减少了10%，任务负载调查中的时间需求得到了改善。这些改进具有统计学意义。&lt;h4&gt;结论&lt;/h4&gt;dARt Vinci平台不仅实现了数据采集效率和使用的提升，还大幅度减少了所需存储空间，并使数据收集频率翻倍。&lt;h4&gt;翻译&lt;/h4&gt;在机器人学习领域中存在一个长期问题：数据稀缺性，在安全至关重要的外科应用等关键领域尤为突出。这使得研究人员难以利用近期关于强化学习和模仿学习的进展来提高泛化能力并让机器人能够自主执行任务。本文介绍了dARt Vinci，这是一种用于手术环境中的可扩展机器人学习的数据采集平台。该系统采用增强现实（AR）手部跟踪技术和高保真物理引擎来捕获基础外科任务中的细微动作：通过消除对实际机器人设备的需求，并在时间和空间以及硬件资源如多视图传感器和执行器方面提供灵活性，特定的模拟成为一种可行的选择。同时，增强现实在数据采集上为机器人提供了更多的自我中心视角，利用其身体跟踪和内容叠加的能力。我们的用户研究表明了所提出的系统的效率和可用性，我们使用广泛使用的基础任务训练达芬奇外科手术机器人的遥操作。与真实机器人设置相比，在所有任务中的数据吞吐量平均提高了41%；总实验时间减少了10%的平均水平，任务负载调查中对时间的需求得到了改善。这些改进具有统计学意义。此外，收集到的数据大小仅为原来的四百分之一，大大减少了存储需求，同时使数据采集频率翻倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data scarcity has long been an issue in the robot learning community.Particularly, in safety-critical domains like surgical applications, obtaininghigh-quality data can be especially difficult. It poses challenges toresearchers seeking to exploit recent advancements in reinforcement learningand imitation learning, which have greatly improved generalizability andenabled robots to conduct tasks autonomously. We introduce dARt Vinci, ascalable data collection platform for robot learning in surgical settings. Thesystem uses Augmented Reality (AR) hand tracking and a high-fidelity physicsengine to capture subtle maneuvers in primitive surgical tasks: By eliminatingthe need for a physical robot setup and providing flexibility in terms of time,space, and hardware resources-such as multiview sensors andactuators-specialized simulation is a viable alternative. At the same time, ARallows the robot data collection to be more egocentric, supported by its bodytracking and content overlaying capabilities. Our user study confirms theproposed system's efficiency and usability, where we use widely-used primitivetasks for training teleoperation with da Vinci surgical robots. Data throughputimproves across all tasks compared to real robot settings by 41% on average.The total experiment time is reduced by an average of 10%. The temporal demandin the task load survey is improved. These gains are statistically significant.Additionally, the collected data is over 400 times smaller in size, requiringfar less storage while achieving double the frequency.</description>
      <author>example@mail.com (Yihao Liu, Yu-Chun Ku, Jiaming Zhang, Hao Ding, Peter Kazanzides, Mehran Armand)</author>
      <guid isPermaLink="false">2503.05646v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Limits of specifiability for sensor-based robotic planning tasks</title>
      <link>http://arxiv.org/abs/2503.05623v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了复杂机器人任务的可指定性，特别是那些涉及丰富目标和长时间行为的任务。&lt;h4&gt;背景&lt;/h4&gt;已有大量基于形式化方法的技术用于描述和实现复杂的机器人任务。&lt;h4&gt;目的&lt;/h4&gt;研究任务可指定性的边界，并强调具体规范在不同层面上（如状态、动作观察等）对任务可指定性的影响。&lt;h4&gt;方法&lt;/h4&gt;引入新的符号表示处理大类问题，分析不同层面的具体规范如何影响可以提出哪些类型的任务。&lt;h4&gt;主要发现&lt;/h4&gt;某些类型的任务可在不同的具体化组合下被明确指定。&lt;h4&gt;结论&lt;/h4&gt;强调了在特定层面上定义具体规范对机器人任务可指定性的重要作用。&lt;h4&gt;翻译&lt;/h4&gt;目前有许多基于形式化方法的技术用于描述和实现复杂的机器人任务，包括那些涉及各种丰富目标和长时间行为的任务。本文探讨了哪些类型的任务是可明确规定的，并研究了具体规范的准确定义在不同层面上（如机器人的状态、动作和观察结果）对于任务指定是否可行的影响。此前的工作中已有部分对此进行了描述，而我们的贡献则是将这一方面作为核心内容处理：我们引入了一种符号表示来应对大量问题，并探讨了这种具体化如何影响可以提出哪些类型的任务。研究结果表明，在不同的具体化组合下，某些类别的任务是可以被指定的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There is now a large body of techniques, many based on formal methods, fordescribing and realizing complex robotics tasks, including those involving avariety of rich goals and time-extended behavior. This paper explores thelimits of what sorts of tasks are specifiable, examining how the precisegrounding of specifications, that is, whether the specification is given interms of the robot's states, its actions and observations, its knowledge, orsome other information,is crucial to whether a given task can be specified.While prior work included some description of particular choices for thisgrounding, our contribution treats this aspect as a first-class citizen: weintroduce notation to deal with a large class of problems, and examine how thegrounding affects what tasks can be posed. The results demonstrate that certainclasses of tasks are specifiable under different combinations of groundings.</description>
      <author>example@mail.com (Basak Sakcak, Dylan A. Shell, Jason M. O'Kane)</author>
      <guid isPermaLink="false">2503.05623v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Learning and generalization of robotic dual-arm manipulation of boxes from demonstrations via Gaussian Mixture Models (GMMs)</title>
      <link>http://arxiv.org/abs/2503.05619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于人类演示的学习和泛化方法，旨在改进复杂机器人系统（如双臂机器人）的操作能力。通过使用GMM参数化的策略，并在参数空间中进行直接的泛化过程，该方法能够有效地减少训练成本与时间。&lt;h4&gt;背景&lt;/h4&gt;示教学习(LfD)是让机器人模仿人类操作物体的有效方式之一。然而，在面对具有高负载能力和灵活性的复杂系统（如双臂机器人）时，如何使机器人的动作适应未见的变化仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需昂贵的人机交互就能扩展机器人动作至新情境的方法，并减少所需演示次数以降低训练成本和时间。&lt;h4&gt;方法&lt;/h4&gt;利用GMM参数化策略的特性，在参数空间中进行泛化过程，从而实现低成本、低计算量且高效的算法。该法仅需少数几次人类示教即可开始运行。&lt;h4&gt;主要发现&lt;/h4&gt;通过实际实验验证了所提出的方法的有效性：从仅为单一任务设计的五次演示学习开始，机器人能够成功地将动作扩展到新场景中（包括不同的目标位置、方向及尺寸）&lt;h4&gt;结论&lt;/h4&gt;该方法展示了其在复杂操作中的实用性和可伸缩性，并且减少了传统LfD所需的大量数据收集和昂贵的人机交互环节。&lt;h4&gt;翻译&lt;/h4&gt;学习示教(LfD)是一种有效的让机器人模仿人类移动并操控物体的方法。这种方法对于处理负载能力和灵活性高的复杂系统（例如双臂机器人）尤为有效。然而，如何将所学的动作扩展到未见过的场景中是一个关键挑战。该研究提出了一种基于GMM参数化的策略泛化方法，并通过实验展示了其在实际应用中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning from demonstration (LfD) is an effective method to teach robots tomove and manipulate objects in a human-like manner. This is especially truewhen dealing with complex robotic systems, such as those with dual armsemployed for their improved payload capacity and manipulability. However, a keychallenge is in expanding the robotic movements beyond the learned scenarios toadapt to minor and major variations from the specific demonstrations. In thiswork, we propose a learning and novel generalization approach that adapts thelearned Gaussian Mixture Model (GMM)-parameterized policy derived from humandemonstrations. Our method requires only a small number of human demonstrationsand eliminates the need for a robotic system during the demonstration phase,which can significantly reduce both cost and time. The generalization processtakes place directly in the parameter space, leveraging the lower-dimensionalrepresentation of GMM parameters. With only three parameters per Gaussiancomponent, this process is computationally efficient and yields immediateresults upon request. We validate our approach through real-world experimentsinvolving a dual-arm robotic manipulation of boxes. Starting with just fivedemonstrations for a single task, our approach successfully generalizes to newunseen scenarios, including new target locations, orientations, and box sizes.These results highlight the practical applicability and scalability of ourmethod for complex manipulations.</description>
      <author>example@mail.com (Qian Ying Lee, Suhas Raghavendra Kulkarni, Kenzhi Iskandar Wong, Lin Yang, Bernardo Noronha, Yongjun Wee, Tzu-Yi Hung, Domenico Campolo)</author>
      <guid isPermaLink="false">2503.05619v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Novel Object 6D Pose Estimation with a Single Reference View</title>
      <link>http://arxiv.org/abs/2503.05578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 12 figures (including supplementary material)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种基于单张参考图像进行6D姿态估计的方法，通过迭代地在相机坐标系统中对齐点位置来处理大姿态差异，并使用RGB和点的SSM捕捉长距离依赖关系和空间信息。&lt;h4&gt;背景&lt;/h4&gt;现有的新物体6D姿态估计方法通常依赖于CAD模型或密集参考视图，但获取这些资源比较困难。仅使用单张参考图像的方法更具可扩展性，但也面临着挑战，如处理大姿态差异以及从单一视角中提取有限的几何和空间信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于单张参考图像的新物体6D姿态估计方法（SinRef-6D），以克服现有技术的局限性，并能够仅使用一张参考图像在不依赖CAD模型的情况下估算出新物体的姿态。&lt;h4&gt;方法&lt;/h4&gt;该方法的核心思想是在相机坐标系统中迭代地进行逐点对齐，利用状态空间模型（SSM）来捕捉长距离依赖关系和从单张视图中的空间信息。具体而言，通过迭代的相机空间逐点对齐可以有效处理大姿态差异，而RGB与点的SSM可以提供线性复杂度以及更优秀的空间建模能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明SinRef-6D在六个流行数据集和现实世界的机器人场景上达到了与基于CAD模型或密集参考视图方法相当的表现水平，在更具挑战性的单张参考图像设置下表现优异。&lt;h4&gt;结论&lt;/h4&gt;提出了一种新颖的基于单张参考图像的新物体6D姿态估计方法，能够直接从一张单一视角获取的信息中精确估算新物体的姿态，同时无需依赖于CAD模型或者大规模训练数据。这种方法为未来的研究提供了新的方向和工具。&lt;h4&gt;翻译&lt;/h4&gt;现有的新技术通常需要CAD模型或密集参考视图来实现新型对象的6D姿态估计，而这两种资源都难以获得。使用单张参考图像的方法更具可扩展性，但面临着处理大姿态差异以及从单一视角中提取有限几何信息的挑战。为解决这些问题，本文提出了一种基于单张参考图像的新物体6D（SinRef-6D）姿态估计方法。该方法的核心在于利用状态空间模型在相机坐标系统下迭代地进行逐点对齐以应对大姿态差异，并通过RGB和点的状态空间模型捕捉长距离依赖关系和单一视角中的空间信息，提供线性复杂度的同时具有优秀的空间建模能力。经过合成数据的预训练后，SinRef-6D能够在不重新训练或使用CAD模型的情况下，仅凭一张参考图像就能估算出新物体的6D姿态。大量实验表明，在六个流行的数据集和现实世界的机器人场景中，该方法的表现与基于CAD模型或密集参考视图的方法相当，即便是在更具挑战性的单一参考设置下也是如此。相关代码将发布在https://github.com/CNJianLiu/SinRef-6D上供研究者使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing novel object 6D pose estimation methods typically rely on CAD modelsor dense reference views, which are both difficult to acquire. Using only asingle reference view is more scalable, but challenging due to large posediscrepancies and limited geometric and spatial information. To address theseissues, we propose a Single-Reference-based novel object 6D (SinRef-6D) poseestimation method. Our key idea is to iteratively establish point-wisealignment in the camera coordinate system based on state space models (SSMs).Specifically, iterative camera-space point-wise alignment can effectivelyhandle large pose discrepancies, while our proposed RGB and Points SSMs cancapture long-range dependencies and spatial information from a single view,offering linear complexity and superior spatial modeling capability. Oncepre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novelobject using only a single reference view, without requiring retraining or aCAD model. Extensive experiments on six popular datasets and real-world roboticscenes demonstrate that we achieve on-par performance with CAD-based and densereference view-based methods, despite operating in the more challenging singlereference setting. Code will be released athttps://github.com/CNJianLiu/SinRef-6D.</description>
      <author>example@mail.com (Jian Liu, Wei Sun, Kai Zeng, Jin Zheng, Hui Yang, Lin Wang, Hossein Rahmani, Ajmal Mian)</author>
      <guid isPermaLink="false">2503.05578v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>InDRiVE: Intrinsic Disagreement based Reinforcement for Vehicle Exploration through Curiosity Driven Generalized World Model</title>
      <link>http://arxiv.org/abs/2503.05573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to IROS 2025 and is currently under  review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;基于模型的强化学习（MBRL）在自动驾驶中展现出巨大潜力，特别是在数据效率和鲁棒性方面。然而，现有的解决方案依赖于精心设计的任务特定外部奖励，限制了对新任务或环境的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;当前大多数自动驾驶中的MBRL方法需要详细定义的任务特定奖励来驱动学习过程，这在面对未知或变化的驾驶情况时效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种称为InDRiVE的方法，在不依赖任何外部任务反馈的情况下，通过利用基于模型世界的一致性奖励进行探索。&lt;h4&gt;方法&lt;/h4&gt;该研究采用了一个集成的世界模型训练框架，使得代理可以主动在环境中的不确定性区域进行探索。这种方法产生了任务无关的潜在表示，能够快速地对下游驾驶任务（如车道跟随和碰撞避免）进行零样本或少量样本微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在已见及未见过环境中，InDRiVE的表现优于DreamerV2和DreamerV3基准模型，并且使用显著较少的训练步骤取得了更高的成功率和更少的违规行为。&lt;h4&gt;结论&lt;/h4&gt;纯粹基于内在奖励机制探索的有效性已经被证明可以促进学习稳健车辆控制行为，为开发更加可扩展和适应性强的自动驾驶系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的关键信息已经以中文的形式进行了总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model-based Reinforcement Learning (MBRL) has emerged as a promising paradigmfor autonomous driving, where data efficiency and robustness are critical. Yet,existing solutions often rely on carefully crafted, task specific extrinsicrewards, limiting generalization to new tasks or environments. In this paper,we propose InDRiVE (Intrinsic Disagreement based Reinforcement for VehicleExploration), a method that leverages purely intrinsic, disagreement basedrewards within a Dreamer based MBRL framework. By training an ensemble of worldmodels, the agent actively explores high uncertainty regions of environmentswithout any task specific feedback. This approach yields a task agnostic latentrepresentation, allowing for rapid zero shot or few shot fine tuning ondownstream driving tasks such as lane following and collision avoidance.Experimental results in both seen and unseen environments demonstrate thatInDRiVE achieves higher success rates and fewer infractions compared toDreamerV2 and DreamerV3 baselines despite using significantly fewer trainingsteps. Our findings highlight the effectiveness of purely intrinsic explorationfor learning robust vehicle control behaviors, paving the way for more scalableand adaptable autonomous driving systems.</description>
      <author>example@mail.com (Feeza Khan Khanzada, Jaerock Kwon)</author>
      <guid isPermaLink="false">2503.05573v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>A-SEE2.0: Active-Sensing End-Effector for Robotic Ultrasound Systems with Dense Contact Surface Perception Enabled Probe Orientation Adjustment</title>
      <link>http://arxiv.org/abs/2503.05569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, submitted for review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要类型&lt;/h4&gt;研究论文&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的机器人超声系统（RUSS），该系统利用双RGB-D深度相机来保持超声探头与皮肤表面垂直，从而提高了成像质量和操作一致性。&lt;h4&gt;背景&lt;/h4&gt;传统的自由手超声成像高度依赖于操作者的技能，导致结果不一致且增加了超声技师的体力负担。机器人超声系统（RUSS）旨在通过提供标准化和自动化的成像解决方案来克服这些限制，在缺乏熟练操作者的情况下尤为有用。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的RUSS系统，该系统利用双RGB-D深度相机在没有预先采集的数据情况下维持探头与皮肤表面的垂直对齐，以提高超声成像的一致性和质量。&lt;h4&gt;方法&lt;/h4&gt;RUSS系统将RGB-D摄像头数据和机器人控制算法相结合，确保在不规则表面上保持探头的正交对准。通过使用仿生模型进行验证测试，并且还在活体前臂超声检查中评估了其性能。&lt;h4&gt;主要发现&lt;/h4&gt;该系统的正常定位精度表现出色，在平面表面定位误差为2.47±1.25度，而在人体模型表面上的估计垂直误差则为12.19±5.81度。这些结果表明RUSS能够生成与手动扫描相当的超声图像。&lt;h4&gt;结论&lt;/h4&gt;A-SEE2.0系统在临床实践中具有潜在的应用价值，特别是在那些难以获取熟练操作员的情况下，可实现高质量和一致性的超声成像。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已经提供为中文内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional freehand ultrasound (US) imaging is highly dependent on theskill of the operator, often leading to inconsistent results and increasedphysical demand on sonographers. Robotic Ultrasound Systems (RUSS) aim toaddress these limitations by providing standardized and automated imagingsolutions, especially in environments with limited access to skilled operators.This paper presents the development of a novel RUSS system that employs dualRGB-D depth cameras to maintain the US probe normal to the skin surface, acritical factor for optimal image quality. Our RUSS integrates RGB-D cameradata with robotic control algorithms to maintain orthogonal probe alignment onuneven surfaces without preoperative data. Validation tests using a phantommodel demonstrate that the system achieves robust normal positioning accuracywhile delivering ultrasound images comparable to those obtained through manualscanning. A-SEE2.0 demonstrates 2.47 ${\pm}$ 1.25 degrees error for flatsurface normal-positioning and 12.19 ${\pm}$ 5.81 degrees normal estimationerror on mannequin surface. This work highlights the potential of A-SEE2.0 tobe used in clinical practice by testing its performance during in-vivo forearmultrasound examinations.</description>
      <author>example@mail.com (Yernar Zhetpissov, Xihan Ma, Kehan Yang, Haichong K. Zhang)</author>
      <guid isPermaLink="false">2503.05569v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Riemann$^2$: Learning Riemannian Submanifolds from Riemannian Data</title>
      <link>http://arxiv.org/abs/2503.05540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at AISTATS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种学习黎曼流形上几何数据的潜在表示的方法，这种模型能够更好地处理具有内在约束的数据。&lt;h4&gt;背景&lt;/h4&gt;传统的潜在变量模型在处理单位范数向量或对称正定矩阵等具有一些特定几何结构的数据时表现不佳，因为它们忽视了数据的底层几何限制或者无法提供有意义的距离度量方法。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，研究者提出了一种新的学习方式，通过估计由缠绕高斯过程潜在变量模型（Wrapped Gaussian Process Latent Variable Model）诱导的拉回度量来更好地处理这种类型的几何数据。&lt;h4&gt;方法&lt;/h4&gt;该方法基于估计由Wrapped Gaussian Process Latent Variable Model引入的拉回度量，从而可以更准确地定义潜在空间中的距离和最短路径，并确保模型只在数据流形上分配概率质量。&lt;h4&gt;主要发现&lt;/h4&gt;通过这种方法，研究者能够处理机器人运动合成和大脑连接体分析等复杂任务。&lt;h4&gt;结论&lt;/h4&gt;这项工作为解决具有内在几何约束的数据提供了一个新的视角，提高了潜在变量模型的适用性和效果。&lt;h4&gt;翻译&lt;/h4&gt;潜变量模型是学习高维数据中的低维流形的强大工具。然而，在处理如单位范数向量或对称正定矩阵等受限数据时，现有的方法忽略了底层的几何限制，或者无法在潜在空间中提供有意义的距离度量。为了解决这些问题，我们提出了学习这些几何数据的黎曼潜表示的方法。为此，我们估计了由缠绕高斯过程潜在变量模型（Wrapped Gaussian Process Latent Variable Model）引入的拉回度量，该方法明确地考虑到了数据的几何结构。这使我们在潜在空间中定义几何感知距离和最短路径的同时，确保我们的模型仅在数据流形上分配概率质量。这项工作推广了先前的工作，并允许我们处理各种领域中的复杂任务，包括机器人运动合成和大脑连接体分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Latent variable models are powerful tools for learning low-dimensionalmanifolds from high-dimensional data. However, when dealing with constraineddata such as unit-norm vectors or symmetric positive-definite matrices,existing approaches ignore the underlying geometric constraints or fail toprovide meaningful metrics in the latent space. To address these limitations,we propose to learn Riemannian latent representations of such geometric data.To do so, we estimate the pullback metric induced by a Wrapped Gaussian ProcessLatent Variable Model, which explicitly accounts for the data geometry. Thisenables us to define geometry-aware notions of distance and shortest paths inthe latent space, while ensuring that our model only assigns probability massto the data manifold. This generalizes previous work and allows us to handlecomplex tasks in various domains, including robot motion synthesis and analysisof brain connectomes.</description>
      <author>example@mail.com (Leonel Rozo, Miguel González-Duque, Noémie Jaquier, Søren Hauberg)</author>
      <guid isPermaLink="false">2503.05540v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Accelerating db-A$^\textbf{*}$ for Kinodynamic Motion Planning Using Diffusion</title>
      <link>http://arxiv.org/abs/2503.05539v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新颖的方法，用于利用扩散模型生成适合于每个问题实例的动力学运动原语，通过这种方法可以更快地找到更高质量的解决方案。&lt;h4&gt;背景&lt;/h4&gt;现有的动力学运动规划方法在适应不同机器人动态特性时存在局限性，如二阶单轮车或带有拖车的小汽车。为了解决这些问题，需要一种能够快速生成高质量运动原语的新方法。&lt;h4&gt;目的&lt;/h4&gt;利用扩散模型提出了一种新的方法来为动力学运动规划生成适配于特定问题实例的运动原语，并提高计算时间和解决方案质量。&lt;h4&gt;方法&lt;/h4&gt;采用在随机切割的解决轨迹上训练得到的扩散模型。这些轨迹是通过使用动力学运动规划器解决随机生成的问题实例创建出来的。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在不同的机器人动态特性下，包括二阶单轮车或带有拖车的小汽车等情况下，计算时间和解决方案质量都有显著提高，最高可达30%。&lt;h4&gt;结论&lt;/h4&gt;该方法通过利用扩散模型和问题特定参数提高了动力学运动规划的效率和质量。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖的方法，使用扩散模型为动力学运动规划生成运动原语。所提出的方案根据每个问题实例使用问题特有参数进行调整，从而更快地找到更高质量的解决方案。用于该方法中的扩散模型是在随机切割的解决轨迹上训练得到的，这些轨迹是由动力学运动规划器解决随机生成的问题实例创建出来的。实验结果显示，在处理二阶单轮车或带有拖车的小汽车等不同机器人动态特性的情况下，计算时间和解决方案质量都显著提高了30%以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel approach for generating motion primitives for kinodynamicmotion planning using diffusion models. The motions generated by our approachare adapted to each problem instance by utilizing problem-specific parameters,allowing for finding solutions faster and of better quality. The diffusionmodels used in our approach are trained on randomly cut solution trajectories.These trajectories are created by solving randomly generated problem instanceswith a kinodynamic motion planner. Experimental results show significantimprovements up to 30 percent in both computation time and solution qualityacross varying robot dynamics such as second-order unicycle or car withtrailer.</description>
      <author>example@mail.com (Julius Franke, Akmaral Moldagalieva, Pia Hanfeld, Wolfgang Hönig)</author>
      <guid isPermaLink="false">2503.05539v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Design, Dynamic Modeling and Control of a 2-DOF Robotic Wrist Actuated by Twisted and Coiled Actuators</title>
      <link>http://arxiv.org/abs/2503.05508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的两自由度（2-DOF）机器人腕部设计，该设计使用基于扭曲和卷绕致动器（TCA）的并联机械结构。这种手腕设计旨在通过减轻摩擦问题来提供轻量级结构和优异的运动性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，在工业操纵器和人形机器人的研究中，人工肌肉驱动的执行器越来越受到关注，因为它们提供了高能量密度、轻质构造和紧凑的设计特点。然而，基于动态模型的控制器在机器人手腕的人工肌肉驱动领域的应用往往被忽视。&lt;h4&gt;目的&lt;/h4&gt;本文旨在开发一种新的基于TCA并联机构的2-DOF机器人腕部设计，并建立其拉格朗日动力学模型。&lt;h4&gt;方法&lt;/h4&gt;论文建立了手腕的动力学模型，提出了一种非线性模型预测控制器（NMPC）用于轨迹跟踪任务。此外，还开发了一个原型并通过实验验证了提出的动态模型的有效性和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在各种工作条件下，基于动态模型的控制策略在TCA驱动的机器人手腕运动控制中具有较好的有效性和稳定性。&lt;h4&gt;结论&lt;/h4&gt;新型2-DOF机器人腕部设计能够提供轻量级结构和优异的运动性能，并且所提出的非线性模型预测控制器可以实现更准确和稳定的轨迹跟踪。&lt;h4&gt;翻译&lt;/h4&gt;机器人的腕部在工业操纵器和人形机器人中发挥着重要作用，特别是在抓握任务中。近年来，基于人工肌肉驱动的执行器因其高能量密度、轻质构造和紧凑设计的特点而备受关注。然而，在人工肌肉驱动的机器人腕部的研究中，通常忽视了基于动态模型的控制器的重要性。这项研究提出了一种使用并联机制（3RRRR配置）的新式2-DOF机器人腕部设计方案，并且该方案预计能够提供轻量化结构和优异运动性能同时减少摩擦问题。论文还建立了一个拉格朗日动力学模型，设计了非线性模型预测控制器用于轨迹跟踪任务。一个原型被开发出来并通过广泛的实验验证其出色的运动性能和提出的动态模型的有效性。后续的对比实验表明，在各种操作条件下，基于动态模型的控制策略在TCA驱动的机器人腕部运动控制中显示出更好的有效性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic wrists play a pivotal role in the functionality of industrialmanipulators and humanoid robots, facilitating manipulation and grasping tasks.In recent years, there has been a growing interest in integrating artificialmuscle-driven actuators for robotic wrists, driven by advancements intechnology offering high energy density, lightweight construction, and compactdesigns. However, in the study of robotic wrists driven by artificial muscles,dynamic model-based controllers are often overlooked, despite their criticalimportance for motion analysis and dynamic control of robots. This paperpresents a novel design of a two-degree-of-freedom (2-DOF) robotic wrist drivenby twisted and coiled actuators (TCA) utilizing a parallel mechanism with a3RRRR configuration. The proposed robotic wrist is expected to featurelightweight structures and superior motion performance while mitigatingfriction issues. The Lagrangian dynamic model of the wrist is established,along with a nonlinear model predictive controller (NMPC) designed fortrajectory tracking tasks. A prototype of the robotic wrist is developed, andextensive experiments are conducted to validate its superior motion performanceand the proposed dynamic model. Subsequently, extensive comparative experimentsbetween NMPC and PID controller were conducted under various operatingconditions. The experimental results demonstrate the effectiveness androbustness of the dynamic model-based controller in the motion control ofTCA-driven robotic wrists.</description>
      <author>example@mail.com (Yunsong Zhang, Xinyu Zhou, Feitian Zhang)</author>
      <guid isPermaLink="false">2503.05508v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Neural Unscented Kalman Filter</title>
      <link>http://arxiv.org/abs/2503.05490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  eight pages, ten figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自适应神经无迹卡尔曼滤波器，用于处理平台操作过程中的时间变化不确定性。&lt;h4&gt;背景&lt;/h4&gt;非线性场景下无迹卡尔曼滤波算法可能因过程噪声协方差的不确定而导致性能下降甚至发散。&lt;h4&gt;目的&lt;/h4&gt;调整过程噪声协方差矩阵以提高滤波估计性能，并适应时间变化中的不确定性。&lt;h4&gt;方法&lt;/h4&gt;设计了ProcessNet，一个简单高效的端到端回归网络，用来自适应地估算过程噪声协方差矩阵。&lt;h4&gt;主要发现&lt;/h4&gt;在自主水下航行器导航中处理非线性惯性传感器和多普勒速度日志融合问题时，展示了滤波性能的优势。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明该自适应神经无迹卡尔曼滤波器优于其他自适应和非自适应的非线性滤波器。&lt;h4&gt;翻译&lt;/h4&gt;无迹卡尔曼滤波是一种能够处理非线性场景的算法。过程噪声协方差中的不确定性可能会降低滤波估计性能，甚至导致其发散。因此，在实时调整过程噪声协方差矩阵是重要的。本文开发了一种自适应神经无迹卡尔曼滤波器以应对平台操作期间的时间变化不确定性。为此设计了ProcessNet网络用于在端到端回归中自适应地估算过程噪声协方差矩阵，特别关注自主水下车辆导航中的非线性惯性传感器和多普勒速度日志融合问题。利用实际记录的自主水下航行器数据集，展示了滤波性能，并显示出相对于其他自适应与不自适应的非线性滤波方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The unscented Kalman filter is an algorithm capable of handling nonlinearscenarios. Uncertainty in process noise covariance may decrease the filterestimation performance or even lead to its divergence. Therefore, it isimportant to adjust the process noise covariance matrix in real time. In thispaper, we developed an adaptive neural unscented Kalman filter to cope withtime-varying uncertainties during platform operation. To this end, we devisedProcessNet, a simple yet efficient end-to-end regression network to adaptivelyestimate the process noise covariance matrix. We focused on the nonlinearinertial sensor and Doppler velocity log fusion problem in the case ofautonomous underwater vehicle navigation. Using a real-world recorded datasetfrom an autonomous underwater vehicle, we demonstrated our filter performanceand showed its advantages over other adaptive and non-adaptive nonlinearfilters.</description>
      <author>example@mail.com (Amit Levy, Itzik Klein)</author>
      <guid isPermaLink="false">2503.05490v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>DecoupledGaussian: Object-Scene Decoupling for Physics-Based Interaction</title>
      <link>http://arxiv.org/abs/2503.05484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR2025 Accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DecoupledGaussian系统提出了一种新颖的方法，用于从野外视频中分离静态对象及其接触面。该方法通过使用联合泊松场修复和扩展高斯分布来支持真实的新牛顿物理仿真。&lt;h4&gt;背景&lt;/h4&gt;当前的技术对于合成数据的处理效果良好，但对真实的、复杂的场景中的物体与表面接触情况处理不足。&lt;h4&gt;目的&lt;/h4&gt;提供一种有效的方法将静态对象与其接触面分离，并实现基于新牛顿力学的真实物理仿真的模拟。&lt;h4&gt;方法&lt;/h4&gt;DecoupledGaussian使用联合泊松场修复和扩展高斯分布以适应位置变化，同时引入多雕刻策略来优化物体的几何形状。&lt;h4&gt;主要发现&lt;/h4&gt;该系统能够使用户根据给定的脉冲进行解耦、碰撞以及断裂的真实物理仿真。通过全面的用户研究和定量基准测试验证了DecoupledGaussian的有效性。&lt;h4&gt;结论&lt;/h4&gt;此系统改进了在现实世界环境中与对象和场景之间的数字交互，对VR、机器人技术及自动驾驶等产业具有重要应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种称为DecoupledGaussian的新颖系统，它从野外视频中分离静态物体与其接触面，这是基于新牛顿力学的真实物理仿真的关键前提。与以往专注于合成数据或沿接触表面弹性抖动的方法不同（这使得对象无法完全脱离或独立移动），DecoupledGaussian允许显著的位置变化而不受初始接触面的限制。认识到当前二维修复工具在恢复三维位置方面的局限性，我们的方法提出了一种联合泊松场来修补和扩展物体及其接触场景分离后的高斯分布。通过多雕刻策略进一步完善对象几何形状以提高仿真效果。该系统支持复杂交互的现实物理模拟驱动用户指定脉冲的支持跨多个场景。我们通过全面的用户研究和定量基准测试验证了DecoupledGaussian的有效性，改进了在现实世界环境中与对象和场景之间的数字互动，对VR、机器人技术及自动驾驶等产业具有重要应用价值。项目页面位于：https://wangmiaowei.github.io/DecoupledGaussian.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present DecoupledGaussian, a novel system that decouples static objectsfrom their contacted surfaces captured in-the-wild videos, a key prerequisitefor realistic Newtonian-based physical simulations. Unlike prior methodsfocused on synthetic data or elastic jittering along the contact surface, whichprevent objects from fully detaching or moving independently, DecoupledGaussianallows for significant positional changes without being constrained by theinitial contacted surface. Recognizing the limitations of current 2D inpaintingtools for restoring 3D locations, our approach proposes joint Poisson fields torepair and expand the Gaussians of both objects and contacted scenes afterseparation. This is complemented by a multi-carve strategy to refine theobject's geometry. Our system enables realistic simulations of decouplingmotions, collisions, and fractures driven by user-specified impulses,supporting complex interactions within and across multiple scenes. We validateDecoupledGaussian through a comprehensive user study and quantitativebenchmarks. This system enhances digital interaction with objects and scenes inreal-world environments, benefiting industries such as VR, robotics, andautonomous driving. Our project page is at:https://wangmiaowei.github.io/DecoupledGaussian.github.io/.</description>
      <author>example@mail.com (Miaowei Wang, Yibo Zhang, Rui Ma, Weiwei Xu, Changqing Zou, Daniel Morris)</author>
      <guid isPermaLink="false">2503.05484v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Topology-Driven Trajectory Optimization for Modelling Controllable Interactions Within Multi-Vehicle Scenario</title>
      <link>http://arxiv.org/abs/2503.05471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个基于拓扑规划的可微局部同伦不变量度量，用于建模多车辆轨迹优化中的相互作用。&lt;h4&gt;背景&lt;/h4&gt;在多车辆场景中进行轨迹优化面临非线性和非凸性的挑战，并且对初始值敏感，导致难以控制车辆之间的互动。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决现有技术无法很好地生成可控交互和用户设计的交互模式的问题。&lt;h4&gt;方法&lt;/h4&gt;受到拓扑规划启发，引入了一种可微局部同伦不变量度量作为约束条件纳入多车辆轨迹优化框架中。&lt;h4&gt;主要发现&lt;/h4&gt;通过这种方法能够从同一初始值生成多个互动轨迹，并显示出比现有技术更高的优化效果和效率。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一个有效的解决策略来处理复杂环境下的多车辆交互问题，且将开源代码用于推动相关研究的发展。&lt;h4&gt;翻译&lt;/h4&gt;在多车辆场景中进行的轨迹优化因非线性、非凸性质以及初始值敏感特性而面临挑战，使得控制车辆互动变得困难。本文受拓扑规划启发提出了一种可微局部同伦不变量度量，用以建模相互作用。通过将这种拓扑度量作为约束引入到多车辆轨迹优化中，该框架能够从同一初始值生成多个互动轨迹，实现可控互动并支持用户设计的互动模式。大量实验表明其在优化效果和效率上超过现有方法。本文团队计划开放源代码以推动相关研究进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory optimization in multi-vehicle scenarios faces challenges due toits non-linear, non-convex properties and sensitivity to initial values, makinginteractions between vehicles difficult to control. In this paper, inspired bytopological planning, we propose a differentiable local homotopy invariantmetric to model the interactions. By incorporating this topological metric as aconstraint into multi-vehicle trajectory optimization, our framework is capableof generating multiple interactive trajectories from the same initial values,achieving controllable interactions as well as supporting user-designedinteraction patterns. Extensive experiments demonstrate its superior optimalityand efficiency over existing methods. We will release open-source code toadvance relative research.</description>
      <author>example@mail.com (Changjia Ma, Yi Zhao, Zhongxue Gan, Bingzhao Gao, Wenchao Ding)</author>
      <guid isPermaLink="false">2503.05471v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Self-Modeling Robots by Photographing</title>
      <link>http://arxiv.org/abs/2503.05398v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个高精度、考虑纹理和链级的机器人自建模方法。&lt;h4&gt;背景&lt;/h4&gt;现有自我建模方法在模型质量或数据采集成本方面存在问题，且未涉及机器人的纹理建模。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的技术来提高机器人自建模的质量，并降低数据采集的成本。同时探索如何将纹理纳入建模中。&lt;h4&gt;方法&lt;/h4&gt;利用三维高斯分布表示静态形态和纹理，并通过聚类构建神经椭球骨，其变形由姿态参数决定；利用一个运动学神经网络生成变换矩阵控制这些骨骼的动态变化；使用关节角度、相机参数和多视角图像作为训练数据，训练3D高斯分布与运动学神经网络。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够以链级精度描述机器人的形态、运动学及纹理，并可用于下游任务如动作规划和逆向运动学问题的解决。&lt;h4&gt;结论&lt;/h4&gt;这项工作为机器人自我建模提供了一种新的有效途径，通过结合三维高斯分布与深度学习模型实现了高质量的自建模。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-modeling enables robots to build task-agnostic models of theirmorphology and kinematics based on data that can be automatically collected,with minimal human intervention and prior information, thereby enhancingmachine intelligence. Recent research has highlighted the potential ofdata-driven technology in modeling the morphology and kinematics of robots.However, existing self-modeling methods suffer from either low modeling qualityor excessive data acquisition costs. Beyond morphology and kinematics, textureis also a crucial component of robots, which is challenging to model andremains unexplored. In this work, a high-quality, texture-aware, and link-levelmethod is proposed for robot self-modeling. We utilize three-dimensional (3D)Gaussians to represent the static morphology and texture of robots, and clusterthe 3D Gaussians to construct neural ellipsoid bones, whose deformations arecontrolled by the transformation matrices generated by a kinematic neuralnetwork. The 3D Gaussians and kinematic neural network are trained using datapairs composed of joint angles, camera parameters and multi-view images withoutdepth information. By feeding the kinematic neural network with joint angles,we can utilize the well-trained model to describe the corresponding morphology,kinematics and texture of robots at the link level, and render robot imagesfrom different perspectives with the aid of 3D Gaussian splatting. Furthermore,we demonstrate that the established model can be exploited to performdownstream tasks such as motion planning and inverse kinematics.</description>
      <author>example@mail.com (Kejun Hu, Peng Yu, Ning Tan)</author>
      <guid isPermaLink="false">2503.05398v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>CoinRobot: Generalized End-to-end Robotic Learning for Physical Intelligence</title>
      <link>http://arxiv.org/abs/2503.05316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;物理智能在推动具身智能的发展中展现出巨大潜力，但现有系统面临诸多挑战。本文提出了一种通用的端到端机器人学习框架来解决这些问题。&lt;h4&gt;背景&lt;/h4&gt;物理智能通过演示使机器人能够获取复杂行为，在推进具身智能方面具有巨大的潜力；然而，为了实现跨不同平台和环境的一般化和迁移，需要精心设计模型架构、训练策略以及数据多样性。&lt;h4&gt;目的&lt;/h4&gt;提出一种通用的端到端机器人学习框架来解决现有系统在可扩展性、适应异构硬件能力及真实场景中目标评估方面的挑战问题。&lt;h4&gt;方法&lt;/h4&gt;引入统一架构支持跨平台适应性，实现无缝部署于各种类型的机器人设备；通过集成多任务学习和精简网络设计，在保持与不同传感器配置和动作空间兼容性的前提下实现了比传统方法更稳健的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在七项操纵任务上的广泛实验验证了框架的有效性；尤其值得一提的是，基于扩散模型的方法表现出优于LeRobot框架的性能和泛化能力，跨多种机器人平台和环境条件实现性能改进。&lt;h4&gt;结论&lt;/h4&gt;提出的通用端到端学习框架有助于解决现有物理智能系统中的关键问题，并为未来研究提供了一个有前景的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文提及了物理智能在推动具身智能方面的重要性以及目前存在的挑战。论文介绍了一种新的机器人学习方法，该方法通过统一架构支持跨平台部署，旨在提高机器人的适应性和泛化能力。实验表明这种方法能够显著改善机器人的操作性能和环境适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physical intelligence holds immense promise for advancing embodiedintelligence, enabling robots to acquire complex behaviors from demonstrations.However, achieving generalization and transfer across diverse robotic platformsand environments requires careful design of model architectures, trainingstrategies, and data diversity. Meanwhile existing systems often struggle withscalability, adaptability to heterogeneous hardware, and objective evaluationin real-world settings. We present a generalized end-to-end robotic learningframework designed to bridge this gap. Our framework introduces a unifiedarchitecture that supports cross-platform adaptability, enabling seamlessdeployment across industrial-grade robots, collaborative arms, and novelembodiments without task-specific modifications. By integrating multi-tasklearning with streamlined network designs, it achieves more robust performancethan conventional approaches, while maintaining compatibility with varyingsensor configurations and action spaces. We validate our framework throughextensive experiments on seven manipulation tasks. Notably, Diffusion-basedmodels trained in our framework demonstrated superior performance andgeneralizability compared to the LeRobot framework, achieving performanceimprovements across diverse robotic platforms and environmental conditions.</description>
      <author>example@mail.com (Yu Zhao, Huxian Liu, Xiang Chen, Jiankai Sun, Jiahuan Yan, Luhui Hu)</author>
      <guid isPermaLink="false">2503.05316v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Security-Aware Sensor Fusion with MATE: the Multi-Agent Trust Estimator</title>
      <link>http://arxiv.org/abs/2503.04954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;智能城市等多代理网络系统中的传感器融合由于缺乏安全意识而容易受到攻击。&lt;h4&gt;目的&lt;/h4&gt;设计一种基于信任估计的感知融合方法来抵御最新的威胁。&lt;h4&gt;方法&lt;/h4&gt;将信任估计视为隐藏马尔可夫模型，并通过映射传感器数据到信任伪测量（PSMs）的方式递归地在贝叶斯框架下更新信任后验概率，从而将信任融入到感知融合中，以增强态势感知的可信度。&lt;h4&gt;主要发现&lt;/h4&gt;开发了一种新的视角估计器、用于将传感器数据转换为PSM的逻辑以及高效贝叶斯更新方法。通过在物理基础的Unreal Engine模拟器CARLA中的案例研究和蒙特卡洛仿真评估了安全意识融合下的攻击情况。&lt;h4&gt;结论&lt;/h4&gt;基于信任的安全感知融合能够在敌对条件下建立可信赖的情境意识，并且这种混合了新型与传统安全相关度量的方法可以有效提高系统安全性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：Lacking security awareness, sensor fusion in systems with multi-agent networks such as smart cities is vulnerable to attacks. To guard against recent threats, we design security-aware sensor fusion that is based on the estimates of distributions over trust. Trust estimation can be cast as a hidden Markov model, and we solve it by mapping sensor data to trust pseudomeasurements (PSMs) that recursively update trust posteriors in a Bayesian context. Trust then feeds sensor fusion to facilitate trust-weighted updates to situational awareness. Essential to security-awareness are a novel field of view estimator, logic to map sensor data into PSMs, and the derivation of efficient Bayesian updates. We evaluate security-aware fusion under attacks on agents using case studies and Monte Carlo simulation in the physics-based Unreal Engine simulator, CARLA. A mix of novel and classical security-relevant metrics show that our security-aware fusion enables building trustworthy situational awareness even in hostile conditions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lacking security awareness, sensor fusion in systems with multi-agentnetworks such as smart cities is vulnerable to attacks. To guard against recentthreats, we design security-aware sensor fusion that is based on the estimatesof distributions over trust. Trust estimation can be cast as a hidden Markovmodel, and we solve it by mapping sensor data to trust pseudomeasurements(PSMs) that recursively update trust posteriors in a Bayesian context. Trustthen feeds sensor fusion to facilitate trust-weighted updates to situationalawareness. Essential to security-awareness are a novel field of view estimator,logic to map sensor data into PSMs, and the derivation of efficient Bayesianupdates. We evaluate security-aware fusion under attacks on agents using casestudies and Monte Carlo simulation in the physics-based Unreal Enginesimulator, CARLA. A mix of novel and classical security-relevant metrics showthat our security-aware fusion enables building trustworthy situationalawareness even in hostile conditions.</description>
      <author>example@mail.com (R. Spencer Hallyburton, Miroslav Pajic)</author>
      <guid isPermaLink="false">2503.04954v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>A Helping (Human) Hand in Kinematic Structure Estimation</title>
      <link>http://arxiv.org/abs/2503.05301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICRA25; 8 pages + 7 figures; For supplementary material,  see https://www.tu.berlin/robotics/papers/helpinghands&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种概率实时方法，通过利用人类手部作为先验知识来减少视觉不确定性（如遮挡、缺乏纹理和噪声），从而提高机器人抓取物体时的运动学模型准确性。&lt;h4&gt;背景&lt;/h4&gt;在进行安全的机器人操作过程中，由于诸如遮挡、缺少纹理特征以及噪音等因素的影响，难以获得准确的运动学模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法来解决视觉不确定性对机器人操控中的运动学建模带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;通过跟踪人类手部在抓取过程中的受限制动作，并且明确地将视觉观测中的不确定因素纳入到模型中，从而在线估计物体的运动学模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该方法相比两个最近提出的基准方法分别提高了195%和140%，证明了利用合适先验并明确考虑不确定性的重要性。此外，这种方法还能够使机器人安全地操控小物件。&lt;h4&gt;结论&lt;/h4&gt;通过采用人类手部作为先验知识，并且在模型中明确处理视觉观测中的不确定因素，可以显著提高基于视觉的机器人抓取物体时运动学模型的准确性。&lt;h4&gt;翻译&lt;/h4&gt;视觉不确定性（如遮挡、缺少纹理特征和噪音）对安全地获取准确机器人操作所需的运动学模型提出了重大挑战。我们引入了一种概率实时方法，利用人类手部作为先验知识来减轻这些不确定性的影响。通过追踪在抓取过程中的人类手部动作，并明确建模视觉观测中的不确定因素，我们的方法可以在线可靠地估计物体的运动学模型。我们在一个新颖的数据集上验证了该方法的效果，这个数据集中包含的是遮挡严重和结构变化有限的挑战性对象。实验结果显示，通过引入适当的先验知识并考虑不确定性，本方法能产生精确度更高的估计结果，并且优于两种最近基准线的结果195%和140%，这表明我们的方法能够使机器人安全地抓取小物体。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual uncertainties such as occlusions, lack of texture, and noise presentsignificant challenges in obtaining accurate kinematic models for safe roboticmanipulation. We introduce a probabilistic real-time approach that leveragesthe human hand as a prior to mitigate these uncertainties. By tracking theconstrained motion of the human hand during manipulation and explicitlymodeling uncertainties in visual observations, our method reliably estimates anobject's kinematic model online. We validate our approach on a novel datasetfeaturing challenging objects that are occluded during manipulation and offerlimited articulations for perception. The results demonstrate that byincorporating an appropriate prior and explicitly accounting for uncertainties,our method produces accurate estimates, outperforming two recent baselines by195% and 140%, respectively. Furthermore, we demonstrate that our approach'sestimates are precise enough to allow a robot to manipulate even small objectssafely.</description>
      <author>example@mail.com (Adrian Pfisterer, Xing Li, Vito Mengers, Oliver Brock)</author>
      <guid isPermaLink="false">2503.05301v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Evidential Uncertainty Estimation for Multi-Modal Trajectory Prediction</title>
      <link>http://arxiv.org/abs/2503.05274v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种基于证据深度学习的多模态轨迹预测方法，该方法能够实时估计位置和模式的概率不确定性。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶领域中，准确的轨迹预测至关重要，但由于代理行为的不确定性和感知噪声的存在，这在本质上是具有挑战性的。尽管多模态轨迹预测模型可以生成多个可能的未来路径，并附上相应的概率，但有效地量化这种不确定性仍然是一项尚未解决的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效估计位置和模式不确定性并保持高精度轨迹预测的方法。&lt;h4&gt;方法&lt;/h4&gt;该研究使用基于证据深度学习的新颖多模态轨迹预测方法。此方法利用正态逆伽玛分布处理位置的不确定性，并用狄利克雷分布来量化模式的概率不确定性。与采样法不同，它可以在单一前向传递中推断出两种类型的不确定性，从而大幅提高效率。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明该方法能够在Argoverse 1和Argoverse 2数据集上提供可靠且准确的不确定性估计，并通过使用基于不确定性的重要性抽样技术进一步优化训练效率。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效地解决了多模态轨迹预测中量化不确定性的问题，同时保持了很高的轨迹预测精度。&lt;h4&gt;翻译&lt;/h4&gt;精确的轨迹预测对于自动驾驶至关重要，但由于代理行为和感知噪声引起的不确定性使其具有固有的挑战。尽管多模式轨迹预测模型生成多个可能的未来路径，并附上相应概率，但有效量化的不确定性的方法依然是一个开放的问题。在这项工作中，我们提出了一种基于证据深度学习的新颖多模态轨迹预测方法，它实时估计位置和模式的概率不确定性。该方法利用正态逆伽玛分布处理位置的不确定性以及狄利克雷分布处理模式的不确定性。与采样法不同，它可以在单一前向传递中推断出这两种类型的不确定性，显著提高了效率。此外，我们使用基于不确定性的重要性抽样技术来提高训练效率，通过优先考虑高不确定性样本而不是冗余低效样本进行优化。我们在Argoverse 1和2数据集上进行了广泛的评估，结果表明该方法能够提供可靠的不确定性估计同时保持高的轨迹预测准确度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate trajectory prediction is crucial for autonomous driving, yetuncertainty in agent behavior and perception noise makes it inherentlychallenging. While multi-modal trajectory prediction models generate multipleplausible future paths with associated probabilities, effectively quantifyinguncertainty remains an open problem. In this work, we propose a novelmulti-modal trajectory prediction approach based on evidential deep learningthat estimates both positional and mode probability uncertainty in real time.Our approach leverages a Normal Inverse Gamma distribution for positionaluncertainty and a Dirichlet distribution for mode uncertainty. Unlikesampling-based methods, it infers both types of uncertainty in a single forwardpass, significantly improving efficiency. Additionally, we experimented withuncertainty-driven importance sampling to improve training efficiency byprioritizing underrepresented high-uncertainty samples over redundant ones. Weperform extensive evaluations of our method on the Argoverse 1 and Argoverse 2datasets, demonstrating that it provides reliable uncertainty estimates whilemaintaining high trajectory prediction accuracy.</description>
      <author>example@mail.com (Sajad Marvi, Christoph Rist, Julian Schmidt, Julian Jordan, Abhinav Valada)</author>
      <guid isPermaLink="false">2503.05274v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>A Map-free Deep Learning-based Framework for Gate-to-Gate Monocular Visual Navigation aboard Miniaturized Aerial Vehicles</title>
      <link>http://arxiv.org/abs/2503.05251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  \c{opyright}2025 IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于单目摄像头的自主纳米无人机系统，该系统采用实时深度学习门检测前端和经典视觉伺服控制后端，在无需地图的情况下实现了高效的自主导航。&lt;h4&gt;背景&lt;/h4&gt;近年来，重量小于50克的掌上自主导航器进入无人机竞速领域。与较大的无人机相比，这些小型无人机在内存和计算能力方面存在三个数量级的差距，需要更加高效且轻量化的基于视觉的方法来提高性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种仅依赖机载资源、使用单目摄像头进行自主导航的小型无人机系统，并通过混合仿真与现实世界的训练，优化其性能。&lt;h4&gt;方法&lt;/h4&gt;本研究从两个最先进的深度学习模型出发，为特定任务进行了适应性调整。经过混合模拟和实际环境的训练后，在纳米无人机上集成并部署这些模型。最佳管道每帧仅消耗24M次乘法累加操作，并实现了30Hz的闭环控制性能。&lt;h4&gt;主要发现&lt;/h4&gt;在约2万个现实世界图像数据集上，门检测根均方误差仅为1.4像素。实验证明了纳米无人机能够在四分钟内成功通过15个关卡，未发生碰撞，总行程约为100米，最高速度达到1.9m/s。&lt;h4&gt;结论&lt;/h4&gt;该系统展示了其在从未见过的环境中的泛化能力，在超过四分钟的时间内导航通过关卡。这种技术为小型无人机自主竞速提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种新型的小型无人驾驶飞机，它使用基于单个摄像头的视觉处理和实时深度学习算法来检测赛道上的标志，并能够迅速而准确地绕过障碍物进行竞赛。该系统在计算资源非常有限的情况下表现出色，展示了其在自主导航任务中的潜力和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Palm-sized autonomous nano-drones, i.e., sub-50g in weight, recently enteredthe drone racing scenario, where they are tasked to avoid obstacles andnavigate as fast as possible through gates. However, in contrast with theirbigger counterparts, i.e., kg-scale drones, nano-drones expose three orders ofmagnitude less onboard memory and compute power, demanding more efficient andlightweight vision-based pipelines to win the race. This work presents amap-free vision-based (using only a monocular camera) autonomous nano-dronethat combines a real-time deep learning gate detection front-end with a classicyet elegant and effective visual servoing control back-end, only relying ononboard resources. Starting from two state-of-the-art tiny deep learningmodels, we adapt them for our specific task, and after a mixedsimulator-real-world training, we integrate and deploy them aboard ournano-drone. Our best-performing pipeline costs of only 24M multiply-accumulateoperations per frame, resulting in a closed-loop control performance of 30 Hz,while achieving a gate detection root mean square error of 1.4 pixels, on our~20k real-world image dataset. In-field experiments highlight the capability ofour nano-drone to successfully navigate through 15 gates in 4 min, nevercrashing and covering a total travel distance of ~100m, with a peak flightspeed of 1.9 m/s. Finally, to stress the generalization capability of oursystem, we also test it in a never-seen-before environment, where it navigatesthrough gates for more than 4 min.</description>
      <author>example@mail.com (Lorenzo Scarciglia, Antonio Paolillo, Daniele Palossi)</author>
      <guid isPermaLink="false">2503.05251v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments</title>
      <link>http://arxiv.org/abs/2503.05226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种增强版的Monte Carlo Tree Search (MCTS)框架，称为Reward-Centered ReST-MCTS。该方法通过在搜索过程中引入中间奖励机制来优化决策路径，并展示了它在机器人操纵任务中的优越性能。&lt;h4&gt;背景&lt;/h4&gt;传统的MCTS方法由于依赖于最终步骤的奖励评估，在不确定性高和数据嘈杂的环境中难以有效工作，无法提供实时反馈进行搜索改进。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的MCTS框架Reward-Centered ReST-MCTS，以提高机器人决策任务中面对高度不确定性和噪声环境时的表现。&lt;h4&gt;方法&lt;/h4&gt;通过引入'奖励中心'（Rewarding Center）来动态分配部分奖励，结合规则验证、启发式指导和神经估算机制，在搜索过程中提供中间反馈进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;与基准方法相比，包括Chain-of-Thought (CoT)提示法和原始ReST-MCTS，在不确定环境下处理机器人操纵任务时，Reward-Centered ReST-MCTS提高了2-4%的决策准确性，并且在减少错误路径传播方面更为有效。此外，消融实验显示中间反馈对于搜索路径改进至关重要。&lt;h4&gt;结论&lt;/h4&gt;该方法能够在高不确定性下保持高性能和计算可行性，显示出在复杂机器人任务中的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;蒙特卡洛树搜索（MCTS）作为一种强大的决策工具，在机器人领域得到广泛应用。然而，传统的MCTS方法难以应对高度不确定性和嘈杂数据的挑战，因为它依赖于最终步骤奖励评估缺乏中间反馈优化路径导致次优决策和计算效率低下。本文介绍了一种新的框架Reward-Centered ReST-MCTS，通过引入中间奖励塑造来增强MCTS能力。该框架利用'奖励中心'动态分配部分奖励以改善搜索轨迹，并且在机器人操纵任务中进行了验证，显示出了持续的性能改进并与基准方法相比提高了决策准确性2-4%。此外，在不同不确定性的环境下保持了高性能的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monte Carlo Tree Search (MCTS) has emerged as a powerful tool fordecision-making in robotics, enabling efficient exploration of large searchspaces. However, traditional MCTS methods struggle in environmentscharacterized by high uncertainty and noisy data due to their reliance onfinal-step reward evaluation. The lack of intermediate feedback during searchoften results in suboptimal decision-making and computational inefficiencies.  This paper introduces Reward-Centered ReST-MCTS, a novel framework thatenhances MCTS by incorporating intermediate reward shaping. The core of ourapproach is the Rewarding Center, which refines search trajectories bydynamically assigning partial rewards using rule-based validation, heuristicguidance, and neural estimation. By integrating these mechanisms, our methodenables real-time optimization of search paths, mitigating the effects of errorpropagation.  We evaluate Reward-Centered ReST-MCTS in robotic manipulation tasks underhigh uncertainty, demonstrating consistent improvements in decision accuracy.Compared to baseline methods, including Chain-of-Thought (CoT) prompting andVanilla ReST-MCTS, our framework achieves a 2-4% accuracy improvement whilemaintaining computational feasibility. Ablation studies confirm theeffectiveness of intermediate feedback in search refinement, particularly inpruning incorrect decision paths early. Furthermore, robustness tests show thatour method retains high performance across varying levels of uncertainty.</description>
      <author>example@mail.com (Xibai Wang)</author>
      <guid isPermaLink="false">2503.05226v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot Manipulation of Irregularly Shaped Objects</title>
      <link>http://arxiv.org/abs/2503.05189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;介绍了一种名为Persistent Object Gaussian Splat (POGS)的新系统，用于在动态环境中跟踪和操作之前未见过的不规则形状物体。&lt;h4&gt;背景&lt;/h4&gt;在制造、组装和物流等机器人应用中，追踪并操纵以前从未见过的不规则形状物体非常重要。尽管最近引入的Gaussian Splats能够有效建模对象几何结构，但它们缺乏面向任务的操作的状态估计。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的方法POGS，该方法结合了语义、自监督视觉特征和物体分组特征，并以紧凑的形式表示，可以持续更新以估算扫描物体的姿态。&lt;h4&gt;方法&lt;/h4&gt;POGS采用单一立体相机在多视角场景捕获和训练阶段之后，将深度估计与自监督视觉编码器功能相结合来进行对象姿态估计。该系统支持抓取、重新定向以及自然语言驱动的操作，并能够进行一系列的人类诱导扰动下的重置操作。&lt;h4&gt;主要发现&lt;/h4&gt;POGS能够在每次重置中连续成功地跟踪和操纵多达12个物体，并能从80%的在握工具干扰中恢复过来。此外，它还能处理高达30度的工具位移错误。&lt;h4&gt;结论&lt;/h4&gt;通过整合自监督视觉特征、语义信息以及物体分组特性，POGS提供了一种强大的方法来实现动态环境下的精确姿态估算和任务操作。&lt;h4&gt;翻译&lt;/h4&gt;追踪并操纵在制造、组装和物流等应用中未见过的不规则形状物体非常重要。最近引入的Gaussian Splats能够有效建模对象几何结构，但缺乏面向任务的操作的状态估计。我们介绍了一种新的系统POGS，结合了语义、自监督视觉特征和物体分组特性，并采用紧凑的形式表示，可以持续更新以估算扫描物体的姿态。在初始多视角场景捕获和训练之后，该系统利用单个立体相机集成了深度估计与自监督视觉编码器功能来进行对象姿态估计。POGS支持抓取、重新定向以及自然语言驱动的操作，并能够处理人类诱导扰动下的工具位移错误至高达30度的范围内。POGS实现了多达12次连续成功的物体重置操作并从80%的在握工具干扰中恢复过来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tracking and manipulating irregularly-shaped, previously unseen objects indynamic environments is important for robotic applications in manufacturing,assembly, and logistics. Recently introduced Gaussian Splats efficiently modelobject geometry, but lack persistent state estimation for task-orientedmanipulation. We present Persistent Object Gaussian Splat (POGS), a system thatembeds semantics, self-supervised visual features, and object grouping featuresinto a compact representation that can be continuously updated to estimate thepose of scanned objects. POGS updates object states without requiring expensiverescanning or prior CAD models of objects. After an initial multi-view scenecapture and training phase, POGS uses a single stereo camera to integrate depthestimates along with self-supervised vision encoder features for object poseestimation. POGS supports grasping, reorientation, and natural language-drivenmanipulation by refining object pose estimates, facilitating sequential objectreset operations with human-induced object perturbations and tool servoing,where robots recover tool pose despite tool perturbations of up to 30{\deg}.POGS achieves up to 12 consecutive successful object resets and recovers from80% of in-grasp tool perturbations.</description>
      <author>example@mail.com (Justin Yu, Kush Hari, Karim El-Refai, Arnav Dalal, Justin Kerr, Chung Min Kim, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg)</author>
      <guid isPermaLink="false">2503.05189v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Safety-Critical Traffic Simulation with Adversarial Transfer of Driving Intentions</title>
      <link>http://arxiv.org/abs/2503.05180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个新的策略IntSim，用于模拟自动驾驶车辆在复杂和潜在危险场景下的行为。通过将驾驶意图与运动规划解耦，并结合大规模的真实世界数据，该方法能更准确地仿真交通参与者的行为。&lt;h4&gt;背景&lt;/h4&gt;当前的交通仿真技术难以处理动态且对抗性的交互场景，尤其是在利用日志数据进行训练时，这些数据通常只包括常规情况下的车辆行为。&lt;h4&gt;目的&lt;/h4&gt;提出一种创新且有效的策略IntSim，用于生成逼真的安全关键性模拟场景，并通过这种方法提高自动驾驶系统应对复杂和危险场景的能力。&lt;h4&gt;方法&lt;/h4&gt;IntSim策略将驾驶意图的转移形式化为一个优化问题，同时结合深度学习模型来预测基于环境变化的车辆行为。此外，该方法还利用大规模真实世界数据集（如nuScenes和Waymo）进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，通过灵活实现动态对抗性交互并模拟真实的安全关键场景，IntSim在仿真逼真的安全关键性场景方面表现出色，并且对于改善自动驾驶车辆规划者处理此类场景的能力有显著贡献。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种创新的方法来生成和利用复杂的交通场景数据，从而有助于评估和改进自动驾驶系统的安全性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic simulation, complementing real-world data with a long-taildistribution, allows for effective evaluation and enhancement of the ability ofautonomous vehicles to handle accident-prone scenarios. Simulating suchsafety-critical scenarios is nontrivial, however, from log data that aretypically regular scenarios, especially in consideration of dynamic adversarialinteractions between the future motions of autonomous vehicles and surroundingtraffic participants. To address it, this paper proposes an innovative andefficient strategy, termed IntSim, that explicitly decouples the drivingintentions of surrounding actors from their motion planning for realistic andefficient safety-critical simulation. We formulate the adversarial transfer ofdriving intention as an optimization problem, facilitating extensiveexploration of diverse attack behaviors and efficient solution convergence.Simultaneously, intention-conditioned motion planning benefits from powerfuldeep models and large-scale real-world data, permitting the simulation ofrealistic motion behaviors for actors. Specially, through adapting drivingintentions based on environments, IntSim facilitates the flexible realizationof dynamic adversarial interactions with autonomous vehicles. Finally,extensive open-loop and closed-loop experiments on real-world datasets,including nuScenes and Waymo, demonstrate that the proposed IntSim achievesstate-of-the-art performance in simulating realistic safety-critical scenariosand further improves planners in handling such scenarios.</description>
      <author>example@mail.com (Zherui Huang, Xing Gao, Guanjie Zheng, Licheng Wen, Xuemeng Yang, Xiao Sun)</author>
      <guid isPermaLink="false">2503.05180v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>SplatPose: Geometry-Aware 6-DoF Pose Estimation from Single RGB Image via 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2503.05174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SplatPose是一种利用3D高斯点阵和双分支神经网络架构来提高单RGB图像中6-DoF姿态估计精度的框架，解决了现有方法依赖初始姿态预估和旋转模糊的问题。&lt;h4&gt;背景&lt;/h4&gt;在增强现实和机器人技术中的广泛应用下，6-DoF姿态估计是计算机视觉的基本任务。然而，现有的基于单个RGB的方法往往因为对初始姿态估计的依赖以及对抗旋转模糊的能力不足而牺牲精度，同时需要深度传感器或多视角设置的方法则部署成本高昂。&lt;h4&gt;目的&lt;/h4&gt;提出SplatPose框架以解决现有方法在精度和部署成本方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;SplatPose结合了3D高斯点阵（3DGS）技术和一个双分支神经架构，并引入了Dual-Attention Ray Scoring Network(DARS-Net)，通过几何域注意力机制来解耦位置对齐与角度对齐，从而减少旋转模糊。此外，采用了一个粗到细的优化流程逐步精化姿态估计。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准数据集上的实验表明SplatPose在单RGB设置下的6-DoF姿态估计准确度达到业界领先水平，并且其性能可媲美需要深度信息或多个视角图像的方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的SplatPose框架通过创新的3D高斯点阵技术和精细化的姿态对齐流程，在仅使用单张RGB图像的情况下达到了卓越的6-DoF姿态估计精度，具有重要的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;六自由度（6-DoF）姿态估计是计算机视觉中的一个基本任务，并在增强现实和机器人技术中有着广泛的应用。现有的基于单一RGB的方法通常因对初始姿态估计的依赖以及对抗旋转模糊的能力不足而牺牲了精度，而那些需要深度传感器或多视角设置的方法则导致了显著的成本增加。为了解决这些限制，我们引入了一种新的框架SplatPose，该框架结合了3D高斯点阵（3DGS）和双分支神经架构，在仅使用单张RGB图像的情况下实现了高度精确的姿态估计。我们的方法的核心是Dual-Attention Ray Scoring Network (DARS-Net)，它通过几何域注意机制创新性地解耦了位置对齐与角度对齐，明确模型方向依赖关系以减少旋转模糊。此外，一个从粗到细的优化流程逐步精化姿态估计，通过比对查询图像和3DGS合成视图之间的密集2D特征来纠正特征错位及深度误差。在三个基准数据集上的实验显示，在单RGB设置下SplatPose实现了6-DoF姿态估计的最先进精度，与依赖于深度或多视角图像的方法相当。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 6-DoF pose estimation is a fundamental task in computer vision withwide-ranging applications in augmented reality and robotics. Existing singleRGB-based methods often compromise accuracy due to their reliance on initialpose estimates and susceptibility to rotational ambiguity, while approachesrequiring depth sensors or multi-view setups incur significant deploymentcosts. To address these limitations, we introduce SplatPose, a novel frameworkthat synergizes 3D Gaussian Splatting (3DGS) with a dual-branch neuralarchitecture to achieve high-precision pose estimation using only a single RGBimage. Central to our approach is the Dual-Attention Ray Scoring Network(DARS-Net), which innovatively decouples positional and angular alignmentthrough geometry-domain attention mechanisms, explicitly modeling directionaldependencies to mitigate rotational ambiguity. Additionally, a coarse-to-fineoptimization pipeline progressively refines pose estimates by aligning dense 2Dfeatures between query images and 3DGS-synthesized views, effectivelycorrecting feature misalignment and depth errors from sparse ray sampling.Experiments on three benchmark datasets demonstrate that SplatPose achievesstate-of-the-art 6-DoF pose estimation accuracy in single RGB settings,rivaling approaches that depend on depth or multi-view images.</description>
      <author>example@mail.com (Linqi Yang, Xiongwei Zhao, Qihao Sun, Ke Wang, Ao Chen, Peng Kang)</author>
      <guid isPermaLink="false">2503.05174v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive LLM-powered Framework for Driving Intelligence Evaluation</title>
      <link>http://arxiv.org/abs/2503.05164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种用于评估复杂交通环境中自动驾驶行为智能的框架。&lt;h4&gt;背景信息&lt;/h4&gt;目前没有全面的方法来评价自动驾驶系统的智能水平，尤其是在处理复杂的驾驶环境时。&lt;h4&gt;研究目的&lt;/h4&gt;构建一种能够对自动驾驶系统在复杂道路条件下的行为进行客观、综合评价的体系和方法论。&lt;h4&gt;数据来源与构建&lt;/h4&gt;通过自然驾驶实验和驾驶后的行为评估访谈，建立了专业驾驶员和乘客参与的人工智能语言评价数据集。&lt;h4&gt;框架开发&lt;/h4&gt;基于创建的数据集，开发了一种由大模型驱动的自动驾驶评价框架，并在CARLA城市交通模拟器中进行了验证。&lt;h4&gt;人类评估&lt;/h4&gt;该框架的有效性还通过人的直接评估得到了进一步证实。&lt;h4&gt;贡献与影响&lt;/h4&gt;为理解和设计更加智能、人性化的自动驾驶系统提供了有价值的见解和指导。&lt;h4&gt;翻译&lt;/h4&gt;用于评估自主驾驶智能的评价方法对于算法优化至关重要。然而，由于驾驶智能的复杂性，目前尚无全面的评价方法来确定自主驾驶的级别。本文提出了一个复杂的交通环境下的驾驶行为智能评价框架，以填补这一空白。我们通过自然驾驶实验和驾驶后的行为评估访谈构建了一个由专业驾驶员和乘客参与的人工语言评价数据集。基于此数据集，我们开发了一种大模型驱动的驾驶评价框架，并在CARLA城市交通模拟器中进行了验证。此外，还得到了人的直接评估的支持。我们的研究为理解和设计更加智能、人性化的自主驾驶系统提供了有价值的见解。该框架的实现细节和关于数据集的详细信息可在Github上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluation methods for autonomous driving are crucial for algorithmoptimization. However, due to the complexity of driving intelligence, there iscurrently no comprehensive evaluation method for the level of autonomousdriving intelligence. In this paper, we propose an evaluation framework fordriving behavior intelligence in complex traffic environments, aiming to fillthis gap. We constructed a natural language evaluation dataset of humanprofessional drivers and passengers through naturalistic driving experimentsand post-driving behavior evaluation interviews. Based on this dataset, wedeveloped an LLM-powered driving evaluation framework. The effectiveness ofthis framework was validated through simulated experiments in the CARLA urbantraffic simulator and further corroborated by human assessment. Our researchprovides valuable insights for evaluating and designing more intelligent,human-like autonomous driving agents. The implementation details of theframework and detailed information about the dataset can be found at Github.</description>
      <author>example@mail.com (Shanhe You, Xuewen Luo, Xinhe Liang, Jiashu Yu, Chen Zheng, Jiangtao Gong)</author>
      <guid isPermaLink="false">2503.05164v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Neural Ordinary Differential Equations as Interpretable Healthcare classifiers</title>
      <link>http://arxiv.org/abs/2503.03129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACL SRW Submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;深度学习在机器学习领域中是一个重要的创新，然而其决策过程的‘黑箱’特性导致了医疗和科研领域的质疑。&lt;h4&gt;背景&lt;/h4&gt;尽管深度学习技术取得了显著进展，但因其复杂性和不可解释性，在如医疗等需要高度透明度的行业中遇到了挑战。&lt;h4&gt;目的&lt;/h4&gt;本文提出了一种可解释性的方法——使用神经常微分方程（NODEs）模型来解决深度学习领域的‘黑箱’问题。&lt;h4&gt;方法&lt;/h4&gt;通过利用差分方程动态特性，该研究引入了基于NODEs的新架构，能够连续处理文本数据，并首次展示了此类模型的潜力。&lt;h4&gt;主要发现&lt;/h4&gt;提出的新型神经网络结构不仅具有强大的预测能力，还提供了比传统深度学习模型更高的透明度。&lt;h4&gt;结论&lt;/h4&gt;这项研究为医疗等领域的研究人员提供了一种新的研究方向和可解释性更好的深度学习解决方案。&lt;h4&gt;翻译&lt;/h4&gt;深度学习作为机器学习领域的一项重要创新已经出现。然而，该领域的一个显著局限在于其‘黑箱’决策过程，这在如健康照护和科学社群中引发了对其适用性的质疑。为回应这一挑战，本文介绍了一种可解释的方法，使用神经常微分方程（NODEs）模型，这是一种利用微分方程动态特性进行表示学习的神经网络模型类别。通过借鉴差分方程的基础知识，我们展示了此类模型在连续处理文本数据方面的潜力，这是首个具有这种能力的模型，并为该领域未来的研究提出了一个有前途的方向。这项研究的主要目标是为那些既需要深度学习预测功能又重视NODEs透明度重要性的群体（如医疗健康）提供一种新的架构建议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep Learning has emerged as one of the most significant innovations inmachine learning. However, a notable limitation of this field lies in the``black box" decision-making processes, which have led to skepticism withingroups like healthcare and scientific communities regarding its applicability.In response, this study introduces a interpretable approach using NeuralOrdinary Differential Equations (NODEs), a category of neural network modelsthat exploit the dynamics of differential equations for representationlearning. Leveraging their foundation in differential equations, we illustratethe capability of these models to continuously process textual data, markingthe first such model of its kind, and thereby proposing a promising directionfor future research in this domain. The primary objective of this research isto propose a novel architecture for groups like healthcare that require thepredictive capabilities of deep learning while emphasizing the importance ofmodel transparency demonstrated in NODEs.</description>
      <author>example@mail.com (Shi Li)</author>
      <guid isPermaLink="false">2503.03129v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
  <item>
      <title>Low-Level Matters: An Efficient Hybrid Architecture for Robust Multi-frame Infrared Small Target Detection</title>
      <link>http://arxiv.org/abs/2503.02220v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的混合架构LVNet，用于改进多帧红外小目标检测（IRSTD）的性能。&lt;h4&gt;背景&lt;/h4&gt;多帧红外小目标检测在低空和海上监控中至关重要。结合卷积神经网络（CNN）和变换器的方法显示出增强多帧IRSTD性能的巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;通过重新定义混合框架中的低级特征学习，设计一种简单而强大的LVNet架构来提升多帧IRSTD的效率。&lt;h4&gt;方法&lt;/h4&gt;引入了基于多尺度CNN前端的改进方案，以更好地捕捉对红外小目标至关重要的规模敏感局部特征，并设计了一个U形视频变换器用于跨多个时间空间上下文建模，有效捕获目标运动特性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明LVNet在IRDST和NUDT-MIRSDT公共数据集上的性能超越了现有的最先进的方法。相比目前最好的LMAFormer模型，LVNet在nIoU指标上提高了5.63%/18.36%，且使用的参数量仅为后者的1/221，计算成本也大幅减少。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明低级表示学习对于混合架构的重要性，并证明了LVNet的有效性。该代码和训练模型可从https://github.com/ZhihuaShen/LVNet获取。&lt;h4&gt;翻译&lt;/h4&gt;多帧红外小目标检测在低空和海上监控中扮演重要角色，结合CNN和Transformer的混合架构显示出增强性能的巨大潜力。本文提出了一种简单而强大的LVNet架构，通过重新定义低级特征学习来提升多帧IRSTD的效率。实验表明该方法超越了现有的最佳方法，并且其代码和模型已在GitHub上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-frame infrared small target detection (IRSTD) plays a crucial role inlow-altitude and maritime surveillance. The hybrid architecture combining CNNsand Transformers shows great promise for enhancing multi-frame IRSTDperformance. In this paper, we propose LVNet, a simple yet powerful hybridarchitecture that redefines low-level feature learning in hybrid frameworks formulti-frame IRSTD. Our key insight is that the standard linear patch embeddingsin Vision Transformers are insufficient for capturing the scale-sensitive localfeatures critical to infrared small targets. To address this limitation, weintroduce a multi-scale CNN frontend that explicitly models local features byleveraging the local spatial bias of convolution. Additionally, we design aU-shaped video Transformer for multi-frame spatiotemporal context modeling,effectively capturing the motion characteristics of targets. Experiments on thepublicly available datasets IRDST and NUDT-MIRSDT demonstrate that LVNetoutperforms existing state-of-the-art methods. Notably, compared to the currentbest-performing method, LMAFormer, LVNet achieves an improvement of 5.63\% /18.36\% in nIoU, while using only 1/221 of the parameters and 1/92 / 1/21 ofthe computational cost. Ablation studies further validate the importance oflow-level representation learning in hybrid architectures. Our code and trainedmodels are available at https://github.com/ZhihuaShen/LVNet.</description>
      <author>example@mail.com (Zhihua Shen, Siyang Chen, Han Wang, Tongsu Zhang, Xiaohu Zhang, Xiangpeng Xu, Xia Yang)</author>
      <guid isPermaLink="false">2503.02220v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>An Information-theoretic Multi-task Representation Learning Framework for Natural Language Understanding</title>
      <link>http://arxiv.org/abs/2503.04667v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, accepted to AAAI 2025 (main conference), the code is  available at https://github.com/zerohd4869/InfoMTL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多任务表示学习框架（InfoMTL），旨在提取对所有任务都适用的噪声不变的充分表示。&lt;h4&gt;背景&lt;/h4&gt;现有的多任务学习方法在处理冗余特征和表示压缩问题时存在不足，尤其是在语言理解领域。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够在多任务范式下增强预训练语言模型（PLM）的语言理解能力的新框架。&lt;h4&gt;方法&lt;/h4&gt;{'共享信息最大化原则': '旨在学习对所有目标任务都更充分的共享表示，以避免由于表示压缩而在多任务情况下出现表示不足的问题。', '特定任务的信息最小化原则': '设计用于缓解输入中潜在冗余特征对每个任务造成的负面影响，可以压缩任务无关的冗余信息并保持与目标相关的必要信息。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'实验结果': '在六个分类基准上进行的实验表明，在相同多任务设置下，该方法优于12种比较性多任务方法，尤其是在数据受限和存在噪声的情况下。', '进一步实验': '广泛的实验表明所学习到的表示更加充分、高效，并且具有更强的鲁棒性。'}&lt;h4&gt;结论&lt;/h4&gt;InfoMTL框架通过确保共享表示对所有任务都是充足的并减少冗余特征的影响，在多任务环境下提高了PLM的语言理解能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文的内容（已省略，此处仅用于说明）&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a new principled multi-task representation learningframework (InfoMTL) to extract noise-invariant sufficient representations forall tasks. It ensures sufficiency of shared representations for all tasks andmitigates the negative effect of redundant features, which can enhance languageunderstanding of pre-trained language models (PLMs) under the multi-taskparadigm. Firstly, a shared information maximization principle is proposed tolearn more sufficient shared representations for all target tasks. It can avoidthe insufficiency issue arising from representation compression in themulti-task paradigm. Secondly, a task-specific information minimizationprinciple is designed to mitigate the negative effect of potential redundantfeatures in the input for each task. It can compress task-irrelevant redundantinformation and preserve necessary information relevant to the target formulti-task prediction. Experiments on six classification benchmarks show thatour method outperforms 12 comparative multi-task methods under the samemulti-task settings, especially in data-constrained and noisy scenarios.Extensive experiments demonstrate that the learned representations are moresufficient, data-efficient, and robust.</description>
      <author>example@mail.com (Dou Hu, Lingwei Wei, Wei Zhou, Songlin Hu)</author>
      <guid isPermaLink="false">2503.04667v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>CLDyB: Towards Dynamic Benchmarking for Continual Learning with Pre-trained Models</title>
      <link>http://arxiv.org/abs/2503.04655v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了用于评估连续学习(CL)方法的动态基准CLDyB框架，该框架基于马尔可夫决策过程并利用蒙特卡洛树搜索来确定具有挑战性的任务序列。&lt;h4&gt;背景&lt;/h4&gt;基础模型时代的到来激发了将预训练表示用于持续学习的研究兴趣，产生了一系列在标准评估基准上表现优异的方法。然而，人们对预训练阶段可能的数据污染问题日益关注，并且静态的评估基准无法捕捉到现实世界CL场景中的复杂性。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，提出了基于动态基准(CLDB)的一般计算框架来可靠地评估CL方法。&lt;h4&gt;方法&lt;/h4&gt;CLDyB使用马尔可夫决策过程和蒙特卡洛树搜索技术识别给定的持续学习方法中固有的困难任务，并确定具有挑战性的任务顺序。此外，通过这个框架对多个最先进的CL方法进行了联合评估，揭示了现有方法在处理某些特定任务序列时的能力局限性。&lt;h4&gt;主要发现&lt;/h4&gt;发现了现有的连续学习方法在面对通用且具代表性的任务序列时存在明显的性能瓶颈和弱点。&lt;h4&gt;结论&lt;/h4&gt;提供了公开访问的源代码和生成的任务序列链接(https://github.com/szc12153/CLDyB)，以供进一步研究使用，该框架为持续学习领域提供了一个可靠评估新方法的有效工具。&lt;h4&gt;翻译&lt;/h4&gt;基础模型时代的到来引发了利用预训练表示进行连续学习(CL)的研究热潮，产生了在标准评估基准上表现优异的一系列顶级CL方法。然而，人们对预训练阶段潜在数据污染问题的担忧日益增加，并且静态的标准评估基准无法捕捉到现实世界中CL场景的复杂性，导致现有性能达到饱和状态。为了解决这些问题，我们提出了CL on dynamic benchmarks(CLDB)，这是一个基于马尔可夫决策过程来可靠评估CL方法的一般计算框架。该框架可以动态地识别对于给定的CL方法来说固有的困难任务，并通过蒙特卡洛树搜索确定具有挑战性的任务顺序。借助于CLDB，我们首先对多个最先进的CL方法进行了联合评估，揭示了一组普遍具有挑战性和通用性且现有CL方法表现不佳的任务序列。然后分别用CLDB单独评估各个CL方法，发现它们各自的优缺点。源代码和生成的任务序列可以在 https://github.com/szc12153/CLDyB 上公开访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of the foundation model era has sparked significant researchinterest in leveraging pre-trained representations for continual learning (CL),yielding a series of top-performing CL methods on standard evaluationbenchmarks. Nonetheless, there are growing concerns regarding potential datacontamination during the pre-training stage. Furthermore, standard evaluationbenchmarks, which are typically static, fail to capture the complexities ofreal-world CL scenarios, resulting in saturated performance. To address theseissues, we describe CL on dynamic benchmarks (CLDyB), a general computationalframework based on Markov decision processes for evaluating CL methodsreliably. CLDyB dynamically identifies inherently difficult andalgorithm-dependent tasks for the given CL methods, and determines challengingtask orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct ajoint evaluation of multiple state-of-the-art CL methods, leading to a set ofcommonly challenging and generalizable task sequences where existing CL methodstend to perform poorly. We then conduct separate evaluations of individual CLmethods using CLDyB, discovering their respective strengths and weaknesses. Thesource code and generated task sequences are publicly accessible athttps://github.com/szc12153/CLDyB.</description>
      <author>example@mail.com (Shengzhuang Chen, Yikai Liao, Xiaoxiao Sun, Kede Ma, Ying Wei)</author>
      <guid isPermaLink="false">2503.04655v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Meta Learning not to Learn: Robustly Informing Meta-Learning under Nuisance-Varying Families</title>
      <link>http://arxiv.org/abs/2503.04570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;在存在虚假特征和因果预测因子的情况下，标准神经网络训练时倾向于依赖于这些虚假特征。为了引导网络向具有普适性的假设发展，需要引入额外的归纳偏差。当这种挑战存在于与多个任务相关的场景下（例如来自不同医院的图像扫描用于疾病预后估计）时，情况会变得更加复杂。&lt;h4&gt;背景&lt;/h4&gt;在涉及虚假和因果预测因子同时存在的环境中，标准神经网络倾向于依赖于虚假特征。这导致了在存在多种干扰的情况下进行分布稳健性目标上的学习变得困难，尤其是在相关任务共享这些虚假特征时（例如多医院图像扫描）。&lt;h4&gt;目的&lt;/h4&gt;为了应对这种挑战，需要集成适当的归纳偏差以使模型能够跨不同类型的家族泛化，包括具有干扰的家族和任务家族。为此提出了RIME方法来解决在存在正面和负面归纳偏见的情况下进行元学习的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种理论上的因果框架，解释了现有知识整合方法为何会导致分布稳健性目标下的性能下降，并展示了RIME能够同时集成这两种偏见，在扰动变化家族中的告知元学习设置下达到最佳的性能。&lt;h4&gt;主要发现&lt;/h4&gt;RIME能够在不牺牲正面归纳偏见的情况下避免负面归纳偏见的影响，从而在具有干扰因素的家庭中实现了分布稳健性的目标优化。它为如何有效地解决元学习环境中存在的正负诱导偏差问题提供了新的思路。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一个新的元学习框架（即RIME），用于处理存在多种诱导偏见的情况，并展示了其在复杂环境下的优越性能，这表明了该方法在未来类似问题中的潜在应用价值。&lt;h4&gt;翻译&lt;/h4&gt;原文描述了一种新提出的方法RIME，在涉及虚假特征和因果预测因子同时存在的环境中，可以有效地进行元学习。这种方法不仅可以避免负面归纳偏差的影响，还可以优化正面的偏见，从而在复杂环境下实现更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In settings where both spurious and causal predictors are available, standardneural networks trained under the objective of empirical risk minimization(ERM) with no additional inductive biases tend to have a dependence on aspurious feature. As a result, it is necessary to integrate additionalinductive biases in order to guide the network toward generalizable hypotheses.Often these spurious features are shared across related tasks, such asestimating disease prognoses from image scans coming from different hospitals,making the challenge of generalization more difficult. In these settings, it isimportant that methods are able to integrate the proper inductive biases togeneralize across both nuisance-varying families as well as task families.Motivated by this setting, we present RIME (Robustly Informed Meta lEarning), anew method for meta learning under the presence of both positive and negativeinductive biases (what to learn and what not to learn). We first develop atheoretical causal framework showing why existing approaches at knowledgeintegration can lead to worse performance on distributionally robustobjectives. We then show that RIME is able to simultaneously integrate bothbiases, reaching state of the art performance under distributionally robustobjectives in informed meta-learning settings under nuisance-varying families.</description>
      <author>example@mail.com (Louis McConnell)</author>
      <guid isPermaLink="false">2503.04570v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Joint Masked Reconstruction and Contrastive Learning for Mining Interactions Between Proteins</title>
      <link>http://arxiv.org/abs/2503.04650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的蛋白质-蛋白质相互作用（PPI）预测方法，称为JmcPPI，该方法结合了掩码重建和对比学习。&lt;h4&gt;背景&lt;/h4&gt;蛋白质-蛋白质相互作用的预测对于阐明细胞操作机制具有重要作用，并对制药开发和临床治疗领域有着重要的实际意义。当前的研究主要集中在氨基酸序列分析上，基于蛋白结构的研究仍处于初步探索阶段。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的PPI预测方法JmcPPI，以推进该领域的研究进展。&lt;h4&gt;方法&lt;/h4&gt;JmcPPI将PPI预测任务分为两个不同阶段：在残基结构编码阶段，设计了两个特征重建任务并采用了图注意力机制来捕捉残基之间的结构信息；在蛋白质相互作用推理阶段，扰动原始的PPI图，并采用多图对比学习策略彻底挖掘新蛋白的外源性交互信息。&lt;h4&gt;主要发现&lt;/h4&gt;在三个常用的PPI数据集上进行的广泛实验表明，JmcPPI超越了现有的最佳基线模型，在各种数据划分方案下表现优异。&lt;h4&gt;结论&lt;/h4&gt;JmcPPI通过结合掩码重建和对比学习的方法，在蛋白质-蛋白质相互作用预测领域取得了显著的进步，并展示了其在实际应用中的潜力。相关代码可从GitHub获得。&lt;h4&gt;翻译&lt;/h4&gt;摘要的原文内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein-protein interaction (PPI) prediction is an instrumental means inelucidating the mechanisms underlying cellular operations, holding significantpractical implications for the realms of pharmaceutical development andclinical treatment. Presently, the majority of research methods primarilyconcentrate on the analysis of amino acid sequences, while investigationspredicated on protein structures remain in the nascent stages of exploration.Despite the emergence of several structure-based algorithms in recent years,these are still confronted with inherent challenges: (1) the extraction ofintrinsic structural information of proteins typically necessitates theexpenditure of substantial computational resources; (2) these models are overlyreliant on seen protein data, struggling to effectively unearth interactioncues between unknown proteins. To further propel advancements in this domain,this paper introduces a novel PPI prediction method jointing maskedreconstruction and contrastive learning, termed JmcPPI. This methodologydissects the PPI prediction task into two distinct phases: during the residuestructure encoding phase, JmcPPI devises two feature reconstruction tasks andemploys graph attention mechanism to capture structural information betweenresidues; during the protein interaction inference phase, JmcPPI perturbs theoriginal PPI graph and employs a multi-graph contrastive learning strategy tothoroughly mine extrinsic interaction information of novel proteins. Extensiveexperiments conducted on three widely utilized PPI datasets demonstrate thatJmcPPI surpasses existing optimal baseline models across various data partitionschemes. The associated code can be accessed viahttps://github.com/lijfrank-open/JmcPPI.</description>
      <author>example@mail.com (Jiang Li, Xiaoping Wang)</author>
      <guid isPermaLink="false">2503.04650v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>A Modular Pipeline for 3D Object Tracking Using RGB Cameras</title>
      <link>http://arxiv.org/abs/2503.04322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 11 figures, original paper not to be published anywhere else&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的可模块化流水线，用于计算多对象的3D轨迹。该方法适用于多种环境，其中多个同步且固定位置的摄像头记录移动物体，并使用现成的网络摄像头。&lt;h4&gt;背景&lt;/h4&gt;大多数追踪系统受限于二维平面运动并只能追踪单个对象，而这限制了它们在计算机视觉中的应用范围。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够适应不同场景、特别是多摄像机环境下的新型3D轨迹计算方法。&lt;h4&gt;方法&lt;/h4&gt;使用六个RGB网络摄像头跟踪放置餐具过程中的各种对象。该方法需要应对小物体检测、确定相机位置、区分重叠和遮挡的物体以及最终根据平均12456个像素坐标（每三次试验）计算出正确的三维轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;开发了一种稳健的方法，能够生成准确的3D轨迹，并使用x,y,z的位置方差作为置信度指标。此方法能动态处理出现和消失的对象，并即时创建新的扩展卡尔曼滤波器。此外，该方法即使在试验中未知相机位置的情况下也能够大规模应用于数百次餐桌布置实验。&lt;h4&gt;结论&lt;/h4&gt;通过这种方法，在表格设置数据集上进行的测试表明，它可以有效地解决多对象追踪问题并提供精确的结果，同时减少人工注释的需求。&lt;h4&gt;翻译&lt;/h4&gt;物体跟踪是计算机视觉中的关键挑战，具有各种应用，所有这些都需要不同的架构。大多数跟踪系统都存在限制，例如将所有运动约束在二维平面上，并且通常只能跟踪单个对象。在这篇文章中，我们提出了一种新的模块化流水线，用于计算多个对象的3D轨迹。它适用于多种设置，其中多个同步和静止摄像头记录移动物体，并使用现成的网络摄像头进行此操作。我们在表格布置数据集上测试了我们的管道，该数据集中参与者被各种传感器记录下来，当他们用餐桌物品布置桌子时。我们使用六个RGB网络摄像头来跟踪这些操纵的对象。挑战包括：在9,874,699帧中检测小物体、确定相机位置、区分附近和重叠的物体、临时遮挡以及最终计算出正确的3D轨迹，这需要使用平均12456个像素坐标（每三次实验）。我们实现了一种稳健的方法，它生成准确的轨迹，并使用x,y,z的位置方差作为置信度指标。它可以动态处理出现和消失的对象，即时创建新的扩展卡尔曼滤波器。即使在试验中未知相机位置的情况下，这种方法也能够大规模应用于数百次餐桌布置实验，几乎不需要人工注释输入。代码可在https://github.com/LarsBredereke/object_tracking上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object tracking is a key challenge of computer vision with variousapplications that all require different architectures. Most tracking systemshave limitations such as constraining all movement to a 2D plane and they oftentrack only one object. In this paper, we present a new modular pipeline thatcalculates 3D trajectories of multiple objects. It is adaptable to varioussettings where multiple time-synced and stationary cameras record movingobjects, using off the shelf webcams. Our pipeline was tested on the TableSetting Dataset, where participants are recorded with various sensors as theyset a table with tableware objects. We need to track these manipulated objects,using 6 rgb webcams. Challenges include: Detecting small objects in 9.874.699camera frames, determining camera poses, discriminating between nearby andoverlapping objects, temporary occlusions, and finally calculating a 3Dtrajectory using the right subset of an average of 11.12.456 pixel coordinatesper 3-minute trial. We implement a robust pipeline that results in accuratetrajectories with covariance of x,y,z-position as a confidence metric. It dealsdynamically with appearing and disappearing objects, instantiating new ExtendedKalman Filters. It scales to hundreds of table-setting trials with very littlehuman annotation input, even with the camera poses of each trial unknown. Thecode is available at https://github.com/LarsBredereke/object_tracking</description>
      <author>example@mail.com (Lars Bredereke, Yale Hartmann, Tanja Schultz)</author>
      <guid isPermaLink="false">2503.04322v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Learning Wideband User Scheduling and Hybrid Precoding with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.04233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）的架构，用于联合学习宽带多用户多天线系统中的空间频域调度和混合预编码策略。这种架构能够有效地解决资源分配优化问题，并且具有良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;在宽带多用户多天线系统中，由于用户的组合数量庞大以及各个资源块（RB）之间共享模拟预编码器，空间频域调度与混合预编码的学习尚未被联合研究。因此，在这类系统中有效利用资源面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于GNN的架构来同时学习宽带多用户多天线系统的调度和预编码策略，并探索如何改进这种架构以提高其在不同规模问题中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;通过重新表述联合优化问题为一个等价的功能性优化问题，设计了一个由两个级联模块组成的GNN架构。此外，针对无线策略定义的集合发现了同参数同决策（SPSD）属性，并据此改进了调度模块的序列GNN结构；同时在预编码模块中开发了一种新的注意力机制。&lt;h4&gt;主要发现&lt;/h4&gt;1. 发现了一种与无线策略相关的同参数同决策（SPSD）属性。                2. 揭示出当用户具有相似信道时，单一GNN难以学习最优调度政策。                 3. 当线性聚合器阻碍大小泛化时，在预编码模块中开发了新的注意力机制来改进信息聚合。&lt;h4&gt;结论&lt;/h4&gt;所提出的架构在短推理时间和低训练复杂度下实现了令人满意的频谱效率，并且对于基站和用户的不同数量、RB数以及天线数具有良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial-frequency scheduling and hybrid precoding in wideband multi-usermulti-antenna systems have never been learned jointly due to the challengesarising from the massive user combinations on resource blocks (RBs) and theshared analog precoder among RBs. In this paper, we strive to jointly learn thescheduling and precoding policies with graph neural networks (GNNs), which haveemerged as a powerful tool for optimizing resource allocation thanks to theirpotential in generalizing across problem scales. By reformulating the jointoptimization problem into an equivalent functional optimization problem for thescheduling and precoding policies, we propose a GNN-based architectureconsisting of two cascaded modules to learn the two policies. We discover asame-parameter same-decision (SPSD) property for wireless policies defined onsets, revealing that a GNN cannot well learn the optimal scheduling policy whenusers have similar channels. This motivates us to develop a sequence of GNNs toenhance the scheduling module. Furthermore, by analyzing the SPSD property, wefind when linear aggregators in GNNs impede size generalization. Based on theobservation, we devise a novel attention mechanism for information aggregationin the precoder module. Simulation results demonstrate that the proposedarchitecture achieves satisfactory spectral efficiency with short inferencetime and low training complexity, and is generalizable to the numbers of users,RBs, and antennas at the base station and users.</description>
      <author>example@mail.com (Shengjie Liu, Chenyang Yang, Shengqian Han)</author>
      <guid isPermaLink="false">2503.04233v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Large Scale Point Cloud Completion for Archaeological Site Restoration</title>
      <link>http://arxiv.org/abs/2503.04030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种用于恢复大型点云的新颖方法，利用有限且分布不平衡的真实地面标注信息。&lt;h4&gt;背景&lt;/h4&gt;现有的自监督方法在处理具有大面积缺失表面和点分布不均衡的大型物体时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种新方法来解决现有自监督方法在修复大型、部分遮挡点云时的不足，特别是针对考古结构等复杂场景。&lt;h4&gt;方法&lt;/h4&gt;使用粗糙边界标注对感兴趣区域进行处理，并将原始点云投影到多中心投影(MCOP)图像中。MCOP图像是五通道（RGB、深度和旋转）图像。该过程转化为在这些MCOP图像上填补缺失像素，最终映射回3D空间以完成结构重建。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种自监督方案来学习在存在结构缺失的情况下填充MCOP图像，并应用特殊损失函数进一步增强生成点云的规则性和一致性。&lt;h4&gt;结论&lt;/h4&gt;实验表明该方法对于恢复600多个不完整且分布不平衡的考古结构（位于秘鲁）具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;点云完成有助于修复部分遮挡的点云。现有自监督方法在处理大面积缺失表面和点分布不平衡的情况下无法提供高质量的重建结果。本论文提出了一种新方法，利用粗糙边界标注来恢复大型且不完整的结构，在秘鲁600多个考古遗址中取得显著效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion helps restore partial incomplete point cloudssuffering occlusions. Current self-supervised methods fail to give highfidelity completion for large objects with missing surfaces and unbalanceddistribution of available points. In this paper, we present a novel method forrestoring large-scale point clouds with limited and imbalanced ground-truth.Using rough boundary annotations for a region of interest, we project theoriginal point clouds into a multiple-center-of-projection (MCOP) image, wherefragments are projected to images of 5 channels (RGB, depth, and rotation).Completion of the original point cloud is reduced to inpainting the missingpixels in the MCOP images. Due to lack of complete structures and an unbalanceddistribution of existing parts, we develop a self-supervised scheme whichlearns to infill the MCOP image with points resembling existing "complete"patches. Special losses are applied to further enhance the regularity andconsistency of completed MCOP images, which is mapped back to 3D to form finalrestoration. Extensive experiments demonstrate the superiority of our method incompleting 600+ incomplete and unbalanced archaeological structures in Peru.</description>
      <author>example@mail.com (Aocheng Li, James R. Zimmer-Dauphinee, Rajesh Kalyanam, Ian Lindsay, Parker VanValkenburgh, Steven Wernke, Daniel Aliaga)</author>
      <guid isPermaLink="false">2503.04030v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Transferable Foundation Models for Geometric Tasks on Point Cloud Representations: Geometric Neural Operators</title>
      <link>http://arxiv.org/abs/2503.04649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了获取预训练的几何神经算子（GNPs）的方法，这些算子可以作为基础模型用于获得几何特征，并在数据处理管道中用于机器学习任务和数值方法。&lt;h4&gt;背景&lt;/h4&gt;现有的许多任务需要对点云进行几何分析，以提取度量、曲率等形状相关的特性。这需要一种能够稳健地从复杂且可能带有噪声的数据集中学习这些特性的模型。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的基于神经网络的方法来预训练GNPs，以便更有效地处理和理解各种类型的点云数据。&lt;h4&gt;方法&lt;/h4&gt;通过训练GNPs以学习点云的微分几何特征，从而提供度量、曲率和其他形状相关特征的估计。此外，展示如何使用预先训练好的GNPs进行任务如噪声环境下任意形状和拓扑表面几何属性的估算、流形上几何偏微分方程的近似解以及形状变形方程（例如由曲率驱动的流动）的求解。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的GNPs能够在具有挑战性的条件下（包括存在噪声的情况）准确估计点云数据中的关键几何属性，并且可以被集成到现有的和新的数据处理管道中，从而提高效率和准确性。&lt;h4&gt;结论&lt;/h4&gt;研究表明，通过使用预先训练好的GNPs，可以在各种任务中实现对点云数据的高效和有效的几何分析。这些模型在实际应用中有广泛的应用潜力，例如计算机视觉、机器人技术以及科学计算等领域。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种方法来获得预训练的几何神经算子（GNPs），它们可以作为基础模型用于获取几何特征，并可被集成到数据处理管道中以服务于机器学习任务和数值方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce methods for obtaining pretrained Geometric Neural Operators(GNPs) that can serve as basal foundation models for use in obtaining geometricfeatures. These can be used within data processing pipelines for machinelearning tasks and numerical methods. We show how our GNPs can be trained tolearn robust latent representations for the differential geometry ofpoint-clouds to provide estimates of metric, curvature, and other shape-relatedfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimatethe geometric properties of surfaces of arbitrary shape and topologies withrobustness in the presence of noise, (ii) to approximate solutions of geometricpartial differential equations (PDEs) on manifolds, and (iii) to solveequations for shape deformations such as curvature driven flows. We alsorelease a package of the codes and weights for using our pre-trained GNPs forprocessing point cloud representations. This allows for incorporating ourpre-trained GNPs as components for reuse within existing and new dataprocessing pipelines. The GNPs also can be used as part of numerical solversinvolving geometry or as part of methods for performing inference and othergeometric tasks.</description>
      <author>example@mail.com (Blaine Quackenbush, Paul J. Atzberger)</author>
      <guid isPermaLink="false">2503.04649v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Diff-Reg v2: Diffusion-Based Matching Matrix Estimation for Image Matching and 3D Registration</title>
      <link>http://arxiv.org/abs/2503.04127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2403.19919&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的匹配矩阵估计方法，利用扩散模型在矩阵空间中的去噪过程来解决图像和点云配准任务中遇到的挑战。&lt;h4&gt;背景&lt;/h4&gt;建立可靠的对应关系对于2D图像配准、3D点云配准以及2D-3D图像到点云配准等注册任务至关重要。然而，这些任务常常面临诸如尺度不一致、对称性和大形变等问题，这些问题会导致匹配模糊。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来克服传统特征和对应关系方法在复杂场景中的局限性，尤其是在解决局部最小值问题方面。&lt;h4&gt;方法&lt;/h4&gt;该论文引入了一种基于扩散模型的范式，在矩阵空间中进行稳健的匹配矩阵估计。具体而言，它将3D-3D和2D-3D配准任务部署在双随机矩阵空间中，并为2D图像注册任务在应用双重softmax投影正则化的子空间中部署扩散模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证了提出的基于扩散模型的方法能够有效地处理复杂场景中的匹配问题，优于其他传统方法。同时，该研究还展示了轻量级的去噪模块设计的有效性，并且为不同注册任务提供了自适应匹配矩阵嵌入实现。&lt;h4&gt;结论&lt;/h4&gt;提出的新范式为解决图像和点云配准中的挑战提供了一种有效的方法，能够处理各种复杂场景下的问题。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Establishing reliable correspondences is crucial for all registration tasks,including 2D image registration, 3D point cloud registration, and 2D-3Dimage-to-point cloud registration. However, these tasks are often complicatedby challenges such as scale inconsistencies, symmetry, and large deformations,which can lead to ambiguous matches. Previous feature-based andcorrespondence-based methods typically rely on geometric or semantic featuresto generate or polish initial potential correspondences. Some methods typicallyleverage specific geometric priors, such as topological preservation, to devisediverse and innovative strategies tailored to a given enhancement goal, whichcannot be exhaustively enumerated. Additionally, many previous approaches relyon a single-step prediction head, which can struggle with local minima incomplex matching scenarios. To address these challenges, we introduce aninnovative paradigm that leverages a diffusion model in matrix space for robustmatching matrix estimation. Our model treats correspondence estimation as adenoising diffusion process in the matching matrix space, gradually refiningthe intermediate matching matrix to the optimal one. Specifically, we apply thediffusion model in the doubly stochastic matrix space for 3D-3D and 2D-3Dregistration tasks. In the 2D image registration task, we deploy the diffusionmodel in a matrix subspace where dual-softmax projection regularization isapplied. For all three registration tasks, we provide adaptive matching matrixembedding implementations tailored to the specific characteristics of each taskwhile maintaining a consistent "match-to-warp" encoding pattern. Furthermore,we adopt a lightweight design for the denoising module. In inference, oncepoints or image features are extracted and fixed, this module performsmulti-step denoising predictions through reverse sampling.</description>
      <author>example@mail.com (Qianliang Wu, Haobo Jiang, Yaqing Ding, Lei Luo, Jin Xie, Jian Yang)</author>
      <guid isPermaLink="false">2503.04127v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Learning 3D Medical Image Models From Brain Functional Connectivity Network Supervision For Mental Disorder Diagnosis</title>
      <link>http://arxiv.org/abs/2503.04205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的框架CINP，用于通过对比学习将结构MRI与功能性连接网络相结合以提高精神疾病诊断的准确性。&lt;h4&gt;背景&lt;/h4&gt;大多数关于基于MRI的精神疾病的先前研究主要集中在功能连接网络上。然而，由于标注的功能性磁共振成像数据集较小，这限制了它的广泛应用。而常见的3D T1加权MRI（结构MRI）在临床环境中广泛使用且易于获取，但这些方法经常被忽视。&lt;h4&gt;目的&lt;/h4&gt;通过结合功能性与结构性的互补信息来改进诊断准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种框架CINP，该框架利用对比学习技术，在预训练过程中融合了图像掩码建模和网络-图像匹配以增强视觉表示学习和模式对齐。&lt;h4&gt;主要发现&lt;/h4&gt;CINP能够促进从功能性连接网络到结构MRI的知识转移。另外，通过仅使用疑似患者的结构MRI以及少量来自不同患者类别的功能性连接网络，可以进行精神疾病的诊断，这种做法在现实世界的临床环境中是可行的。&lt;h4&gt;结论&lt;/h4&gt;在三个精神疾病诊断任务上的竞争性能表明了CINP框架在整合多模态MRI信息的有效性，并且通过网络提示将结构MRI纳入临床诊断中具有潜在价值。&lt;h4&gt;翻译&lt;/h4&gt;在基于磁共振成像的精神障碍诊断领域，以往的研究大多集中在由功能性磁共振成像（fMRI）得出的功能连接网络（FCN）。然而，由于标注的fMRI数据集较小，这限制了它的广泛应用。同时，在临床环境中广泛使用且易于获取的结构MRIs（如3D T1加权MRI），常常被忽略。为了整合来自功能和结构两方面的互补信息以提高诊断准确性，我们提出了一种框架CINP，该框架采用对比学习方法在sMRI与FCN之间进行对比学习。在预训练阶段中，我们将掩码图像建模和网络-图像匹配集成进来，以增强视觉表示学习和模式对齐。由于CINP能够促进从功能连接网络到结构MRI的知识转移，我们引入了网络提示技术：它仅使用疑似患者的sMRI以及少量来自不同患者类别的FCN来进行精神障碍的诊断，在现实世界的临床环境中是实际可行的。在三个精神疾病诊断任务上的竞争性能表明了CINP框架的有效性，不仅在于整合多模态MRI信息上，而且在于通过网络提示技术将结构MRI融入到临床诊断中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In MRI-based mental disorder diagnosis, most previous studies focus onfunctional connectivity network (FCN) derived from functional MRI (fMRI).However, the small size of annotated fMRI datasets restricts its wideapplication. Meanwhile, structural MRIs (sMRIs), such as 3D T1-weighted (T1w)MRI, which are commonly used and readily accessible in clinical settings, areoften overlooked. To integrate the complementary information from both functionand structure for improved diagnostic accuracy, we propose CINP (ContrastiveImage-Network Pre-training), a framework that employs contrastive learningbetween sMRI and FCN. During pre-training, we incorporate masked image modelingand network-image matching to enhance visual representation learning andmodality alignment. Since the CINP facilitates knowledge transfer from FCN tosMRI, we introduce network prompting. It utilizes only sMRI from suspectedpatients and a small amount of FCNs from different patient classes fordiagnosing mental disorders, which is practical in real-world clinicalscenario. The competitive performance on three mental disorder diagnosis tasksdemonstrate the effectiveness of the CINP in integrating multimodal MRIinformation, as well as the potential of incorporating sMRI into clinicaldiagnosis using network prompting.</description>
      <author>example@mail.com (Xingcan Hu, Wei Wang, Li Xiao)</author>
      <guid isPermaLink="false">2503.04205v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>The JARVIS Infrastructure is All You Need for Materials Design</title>
      <link>http://arxiv.org/abs/2503.04133v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;JARVIS是一个为多尺度、多模态的正向和逆向材料设计提供数据库、工具、教程及基准测试的综合基础设施。&lt;h4&gt;背景信息&lt;/h4&gt;强调开放访问原则和可重复性，集成理论与实验方法如密度泛函理论、量子蒙特卡洛、紧束缚法、经典力场以及机器学习方法包括指纹识别技术、图神经网络和变换模型。&lt;h4&gt;目的陈述&lt;/h4&gt;通过统一的方法和数据集在单一平台上进行整合，以促进材料设计领域的基础发现及实际创新。&lt;h4&gt;研究方法&lt;/h4&gt;收集实验数据涵盖低温学、显微术和衍射，涉及多种材料如金属、半导体等。JARVIS还通过开放数据集、网络应用、可执行脚本以及同行评审的出版物来分发资源。&lt;h4&gt;主要发现&lt;/h4&gt;全球广泛采用，促使了数百万次的数据与工具下载。&lt;h4&gt;结论陈述&lt;/h4&gt;该平台不仅推动基础科学进步也促进了基于数据的方法在材料设计中的实际创新。&lt;h4&gt;涉及材料种类&lt;/h4&gt;包括但不限于金属、半导体、绝缘体、超导体、碳捕获系统、高强度化合物及低维材料、异质结构和缺陷等。&lt;h4&gt;开放性原则&lt;/h4&gt;强调所有资源的开放访问，确保广泛获取性和可重复性&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Joint Automated Repository for Various Integrated Simulations (JARVIS) is acomprehensive infrastructure offering databases, tools, tutorials, andbenchmarks for multiscale, multimodal, forward, and inverse materials design.Emphasizing open access principles and reproducibility, it integratestheoretical and experimental methodologies such as density functional theory,quantum Monte Carlo, tight-binding, classical force fields, andmachine-learning approaches-including fingerprinting, graph neural networks,and transformer models. Its experimental data collection spans cryogenics,microscopy, and diffraction, covering materials like metals, semiconductors,insulators, superconductors, carbon capture systems, high-strength compounds,and low-dimensional materials, heterostructures and defects. JARVISdisseminates resources via open datasets, web applications, executable scripts,and peer-reviewed publications, ensuring broad accessibility andreproducibility. Widely adopted worldwide, it has facilitated millions of dataand tool downloads. By unifying diverse methods and data under one platform,JARVIS drives both fundamental discoveries and real-world innovations,advancing conventional and data-driven materials design.</description>
      <author>example@mail.com (Kamal Choudhary)</author>
      <guid isPermaLink="false">2503.04133v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Disorder: Unveiling Cooperativeness in Multidirectional Associative Memories</title>
      <link>http://arxiv.org/abs/2503.04454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究通过引入统计力学工具来扩展用于异构关联记忆的神经网络架构（称为三向关联存储器TAM），探索监督和非监督学习协议。&lt;h4&gt;背景&lt;/h4&gt;当前对于复杂系统中的统计力学工具未充分利用，特别是在神经网络体系结构的应用方面。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的现象——层协同性，并研究不同层次数据集的熵异质化如何影响记忆检索能力。&lt;h4&gt;方法&lt;/h4&gt;通过向神经网络的不同层级提供具有不同信息量的数据集来探索这种新兴的现象。&lt;h4&gt;主要发现&lt;/h4&gt;观察到训练中使用较少信息数据集的层与那些经历更多信息数据集的层在最终的记忆检索区域内达到相同的幅度，表明了跨层次熵相互作用可以增强整体记忆检索能力。&lt;h4&gt;结论&lt;/h4&gt;提出的协同动力学现象对于理解无序系统中的计算潜力具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;通过利用复杂系统的统计力学工具，本文扩展了一种用于异构关联记忆的神经网络架构（称为三向关联存储器TAM），以探索监督学习和非监督学习协议。研究揭示了数据集熵在不同层次之间的相互作用可以增强这些层次的记忆检索能力，并提出了一个新现象——层协同性，这标志着对于无序系统中计算潜力的理解取得了重大进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; By leveraging tools from the statistical mechanics of complex systems, inthese short notes we extend the architecture of a neural network forhetero-associative memory (called three-directional associative memories, TAM)to explore supervised and unsupervised learning protocols. In particular, byproviding entropic-heterogeneous datasets to its various layers, we predict andquantify a new emergent phenomenon -- that we term {\em layer'scooperativeness} -- where the interplay of dataset entropies across network'slayers enhances their retrieval capabilities Beyond those they would havewithout reciprocal influence. Naively we would expect layers trained with lessinformative datasets to develop smaller retrieval regions compared to thosepertaining to layers that experienced more information: this does not happenand all the retrieval regions settle to the same amplitude, allowing foroptimal retrieval performance globally. This cooperative dynamics marks asignificant advancement in understanding emergent computational capabilitieswithin disordered systems.</description>
      <author>example@mail.com (Andrea Alessandrelli, Adriano Barra, Andrea Ladiana, Andrea Lepre, Federico Ricci-Tersenghi)</author>
      <guid isPermaLink="false">2503.04454v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Spatial regularisation for improved accuracy and interpretability in keypoint-based registration</title>
      <link>http://arxiv.org/abs/2503.04499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种用于改进无监督图像配准中关键点检测的三重损失函数，该方法通过正则化特征的空间分布来提高关键点的解释性和准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的基于无监督关键点检测的方法在实现可解释性方面显示出潜力，但是提取到的关键点往往具有模糊且难以解释的空间模式。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的三重损失函数来改善这些方法中特征向量的空间分布，提高其空间准确性和解剖学相关性。&lt;h4&gt;方法&lt;/h4&gt;引入了基于KL散度的正则化项，通过将特征视为概率关键点并优化其空间分布，同时加入了排斥损失以鼓励各关键点之间的空间多样性。&lt;h4&gt;主要发现&lt;/h4&gt;应用该三重损失函数在胎儿刚体运动追踪和脑MRI仿射配准任务上均取得了优异表现，并且与现有监督方法的性能差距缩小。&lt;h4&gt;结论&lt;/h4&gt;所提出的正则化策略显著提高了无监督图像配准中关键点检测的质量，使其更加适用于实际医学影像分析任务。&lt;h4&gt;翻译&lt;/h4&gt;摘要讨论了一种用于改进无监督图像配准过程中特征图空间分布的新颖三重损失函数。该方法通过将网络输出的特征视为概率性关键点，并通过优化其空间模式以提高解剖学意义的显著性和精确度，进而增强整体注册过程的可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised registration strategies bypass requirements in ground truthtransforms or segmentations by optimising similarity metrics between fixed andmoved volumes. Among these methods, a recent subclass of approaches based onunsupervised keypoint detection stand out as very promising forinterpretability. Specifically, these methods train a network to predictfeature maps for fixed and moving images, from which explainable centres ofmass are computed to obtain point clouds, that are then aligned in closed-form.However, the features returned by the network often yield spatially diffusepatterns that are hard to interpret, thus undermining the purpose ofkeypoint-based registration. Here, we propose a three-fold loss to regularisethe spatial distribution of the features. First, we use the KL divergence tomodel features as point spread functions that we interpret as probabilistickeypoints. Then, we sharpen the spatial distributions of these features toincrease the precision of the detected landmarks. Finally, we introduce a newrepulsive loss across keypoints to encourage spatial diversity. Overall, ourloss considerably improves the interpretability of the features, which nowcorrespond to precise and anatomically meaningful landmarks. We demonstrate ourthree-fold loss in foetal rigid motion tracking and brain MRI affineregistration tasks, where it not only outperforms state-of-the-art unsupervisedstrategies, but also bridges the gap with state-of-the-art supervised methods.Our code is available at https://github.com/BenBillot/spatial_regularisation.</description>
      <author>example@mail.com (Benjamin Billot, Ramya Muthukrishnan, Esra Abaci-Turk, Ellen P. Grant, Nicholas Ayache, Hervé Delingette, Polina Golland)</author>
      <guid isPermaLink="false">2503.04499v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>UniNet: A Unified Multi-granular Traffic Modeling Framework for Network Security</title>
      <link>http://arxiv.org/abs/2503.04174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 6 figures,15 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;现代网络随着各种设备、加密协议和不断变化的安全威胁而变得日益复杂，使得网络流量分析变得至关重要。现有的机器学习模型通常依赖单一的数据表示形式（如包或流），这限制了它们捕捉对全面安全分析至关重要的上下文关系的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种统一框架UniNet以解决现有方法的局限性，并为现代网络安全设定新的基准。&lt;h4&gt;方法&lt;/h4&gt;UniNet引入了一种新颖的多粒度流量表示（T-Matrix）和一个轻量级注意力模型T-Attent，结合会话、流和包级别的特性提供全面的上下文信息。此外，它能高效学习多种安全任务的潜在嵌入，并且能够适应不同数据格式的安全任务。&lt;h4&gt;主要发现&lt;/h4&gt;UniNet在异常检测、攻击分类、物联网设备识别和加密网站指纹鉴定等四个关键网络安全问题上都表现出显著性能提升，优于当前最先进的方法，具有更高的准确性、更低的误报率以及更好的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;通过解决单一层面模型的局限性和统一网络安全分析范式，UniNet为现代网络安全设定了新的标准。&lt;h4&gt;翻译&lt;/h4&gt;随着网络变得越来越复杂，传统的基于单个数据表示形式的方法已经不能满足需求。为此，研究人员提出了UniNet框架来应对这些挑战，并且证明了其在多个关键安全任务中的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As modern networks grow increasingly complex--driven by diverse devices,encrypted protocols, and evolving threats--network traffic analysis has becomecritically important. Existing machine learning models often rely only on asingle representation of packets or flows, limiting their ability to capturethe contextual relationships essential for robust analysis. Furthermore,task-specific architectures for supervised, semi-supervised, and unsupervisedlearning lead to inefficiencies in adapting to varying data formats andsecurity tasks. To address these gaps, we propose UniNet, a unified frameworkthat introduces a novel multi-granular traffic representation (T-Matrix),integrating session, flow, and packet-level features to provide comprehensivecontextual information. Combined with T-Attent, a lightweight attention-basedmodel, UniNet efficiently learns latent embeddings for diverse security tasks.Extensive evaluations across four key network security and privacyproblems--anomaly detection, attack classification, IoT device identification,and encrypted website fingerprinting--demonstrate UniNet's significantperformance gain over state-of-the-art methods, achieving higher accuracy,lower false positive rates, and improved scalability. By addressing thelimitations of single-level models and unifying traffic analysis paradigms,UniNet sets a new benchmark for modern network security.</description>
      <author>example@mail.com (Binghui Wu, Dinil Mon Divakaran, Mohan Gurusamy)</author>
      <guid isPermaLink="false">2503.04174v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>WeakSupCon: Weakly Supervised Contrastive Learning for Encoder Pre-training</title>
      <link>http://arxiv.org/abs/2503.04165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '提出了一种新的编码器预训练方法Weakly Supervised Contrastive Learning (WeakSupCon)，用于下游的多实例学习任务，该方法利用了包级标签。', '背景': '弱监督下的多实例学习（MIL）是一个具有挑战性的任务，因为只有包级别标签可用，而每个包通常包含多个实例。在组织病理图像分析中广泛研究此问题，其中标签仅提供于整个滑动片水平，但可将每张滑动片分割成数千个小型图像块进行训练。', '目的': '克服自监督编码器预训练过程中的领域迁移问题，并提高MIL任务的分类性能。', '方法': '通过利用包级标签定义多任务学习和区分不同包标签样本的对比学习损失，实现弱监督下的对比学习。', '主要发现': '实验表明，在三个数据集上使用WeakSupCon生成的特征与自监督方法相比显著提高了MIL分类性能。', '结论': '提出的Weakly Supervised Contrastive Learning (WeakSupCon) 方法可以有效解决当前编码器预训练在MIL任务上的局限性，提高分类精度。'}&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了弱监督下的多实例学习（MIL）的研究背景和挑战，并提出了一种新的方法Weakly Supervised Contrastive Learning (WeakSupCon)，旨在改进现有的自监督编码器预训练技术，通过利用包级标签改善特征的生成质量，从而提升下游MIL任务中的分类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Weakly supervised multiple instance learning (MIL) is a challenging taskgiven that only bag-level labels are provided, while each bag typicallycontains multiple instances. This topic has been extensively studied inhistopathological image analysis, where labels are usually available only atthe whole slide image (WSI) level, while each whole slide image can be dividedinto thousands of small image patches for training. The dominant MIL approachestake fixed patch features as inputs to address computational constraints andensure model stability. These features are commonly generated by encoderspre-trained on ImageNet, foundation encoders pre-trained on large datasets, orthrough self-supervised learning on local datasets. While the self-supervisedencoder pre-training on the same dataset as downstream MIL tasks helps mitigatedomain shift and generate better features, the bag-level labels are notutilized during the process, and the features of patches from differentcategories may cluster together, reducing classification performance on MILtasks. Recently, pre-training with supervised contrastive learning (SupCon) hasdemonstrated superior performance compared to self-supervised contrastivelearning and even end-to-end training on traditional image classificationtasks. In this paper, we propose a novel encoder pre-training method fordownstream MIL tasks called Weakly Supervised Contrastive Learning (WeakSupCon)that utilizes bag-level labels. In our method, we employ multi-task learningand define distinct contrastive learning losses for samples with different baglabels. Our experiments demonstrate that the features generated usingWeakSupCon significantly enhance MIL classification performance compared toself-supervised approaches across three datasets.</description>
      <author>example@mail.com (Bodong Zhang, Hamid Manoochehri, Beatrice S. Knudsen, Tolga Tasdizen)</author>
      <guid isPermaLink="false">2503.04165v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>DM-Adapter: Domain-Aware Mixture-of-Adapters for Text-Based Person Retrieval</title>
      <link>http://arxiv.org/abs/2503.04144v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, accepted by AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;文本基础的人脸检索任务得到了广泛关注，由于其与实际应用紧密相关且具有挑战性。针对该领域中的视觉语言预训练知识的应用，提出了将CLIP模型适应于人脸领域的研究方向。&lt;h4&gt;背景&lt;/h4&gt;基于文本的人脸识别（TPR）作为一项细粒度和具有挑战性的任务，在实践中得到广泛应用。利用视觉语言预训练的知识来定制CLIP应用于人脸识别是一个新兴的研究领域。&lt;h4&gt;目的&lt;/h4&gt;解决在TPR中全模型微调计算成本高且易过拟合的问题，以及现有的参数高效迁移学习缺乏对细粒度特征提取的支持问题。&lt;h4&gt;方法&lt;/h4&gt;提出了域感知混合适配器（DM-Adapter）方法，该方法结合了专家混合（MOE）和参数效率的迁移学习技术，以增强细粒度特征表示并保持计算效率。具体而言，在视觉和语言分支中的MLP层旁设计了稀疏混合适配器。&lt;h4&gt;主要发现&lt;/h4&gt;通过建立新的门控函数以及注入可训练的域感知提示符来发展域感知路由机制，促进路由器有效利用领域信息，并缓解路由不平衡问题。实验表明DM-Adapter取得了最先进的性能，显著优于先前的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的新方法能够有效地处理TPR任务中的挑战，在多个基准测试中表现出优越性。&lt;h4&gt;翻译&lt;/h4&gt;文本基础的人脸识别（Text-based Person Retrieval, TPR）已经成为一个精细粒度且具有挑战性的研究领域，并且在实际应用中有紧密的联系。由于视觉语言预训练模型CLIP拥有丰富的知识，将该模型应用于人脸识别的研究方向正在兴起。然而，在TPR任务中进行微调时仍存在一些问题：一是全模型微调的成本高昂并且容易过拟合；二是现有的参数效率迁移学习方法在细粒度特征提取方面仍有不足。为了克服这些问题，我们提出了一种名为域感知混合适配器（Domain-Aware Mixture-of-Adapters, DM-Adapter）的方法。该方法结合了专家混合技术和参数高效迁移学习技术，以增强细粒度特征表示的同时保持计算效率。具体来说，在视觉和语言分支的MLP层旁设计了稀疏混合适配器，并针对不同的专家来处理不同的人脸知识方面，从而更精细地处理特征。为促进路由器有效地利用领域信息并缓解路由不平衡问题，我们开发了一种域感知路由机制，通过建立新的门控函数以及注入可训练的域感知提示符实现。广泛的实验表明，我们的DM-Adapter方法实现了最先进的性能，并且明显优于以前的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-based person retrieval (TPR) has gained significant attention as afine-grained and challenging task that closely aligns with practicalapplications. Tailoring CLIP to person domain is now a emerging research topicdue to the abundant knowledge of vision-language pretraining, but challengesstill remain during fine-tuning: (i) Previous full-model fine-tuning in TPR iscomputationally expensive and prone to overfitting.(ii) Existingparameter-efficient transfer learning (PETL) for TPR lacks of fine-grainedfeature extraction. To address these issues, we propose Domain-AwareMixture-of-Adapters (DM-Adapter), which unifies Mixture-of-Experts (MOE) andPETL to enhance fine-grained feature representations while maintainingefficiency. Specifically, Sparse Mixture-of-Adapters is designed in parallel toMLP layers in both vision and language branches, where different expertsspecialize in distinct aspects of person knowledge to handle features morefinely. To promote the router to exploit domain information effectively andalleviate the routing imbalance, Domain-Aware Router is then developed bybuilding a novel gating function and injecting learnable domain-aware prompts.Extensive experiments show that our DM-Adapter achieves state-of-the-artperformance, outperforming previous methods by a significant margin.</description>
      <author>example@mail.com (Yating Liu, Zimo Liu, Xiangyuan Lan, Wenming Yang, Yaowei Li, Qingmin Liao)</author>
      <guid isPermaLink="false">2503.04144v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>On the Acquisition of Shared Grammatical Representations in Bilingual Language Models</title>
      <link>http://arxiv.org/abs/2503.03962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文研究了单一语言模型在训练第二语言时的变化，探讨跨语际转移学习中共享的多语言表示形式。通过控制每种语言的数据量和顺序来训练双语小规模模型，并利用结构启动（structural priming）这一方法寻找证据。&lt;h4&gt;背景&lt;/h4&gt;当前的语言模型在多种语言的能力方面依赖于跨语际转移学习，但这种转移发生的机制尚不清晰。&lt;h4&gt;目的&lt;/h4&gt;探究单语语言模型在其开始被另一种语言训练时会发生什么变化。&lt;h4&gt;方法&lt;/h4&gt;控制每种语言的数据量和顺序来训练双语小规模模型，并使用结构启动这一方法研究语法表示形式。首先复制先前的跨语言结构启动结果，然后调整数据量和语言接触模式以观察影响。&lt;h4&gt;主要发现&lt;/h4&gt;在控制了训练数据的数量和语言暴露后，不同语言对之间以及方向上的效果是不对称的；并且对于语系差异较大的语言组合来说，这种交叉语言传输学习和共享表示形式的效果并不稳定且不太可靠。&lt;h4&gt;结论&lt;/h4&gt;研究表明跨语言转移中的不对称性可能会影响关于人类结构启动效应的假设，同时也展示了多语言模型训练中存在的一些潜在限制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While crosslingual transfer is crucial to contemporary language models'multilingual capabilities, how it occurs is not well understood. In this paper,we ask what happens to a monolingual language model when it begins to betrained on a second language. Specifically, we train small bilingual models forwhich we control the amount of data for each language and the order of languageexposure. To find evidence of shared multilingual representations, we turn tostructural priming, a method used to study grammatical representations inhumans. We first replicate previous crosslingual structural priming results andfind that after controlling for training data quantity and language exposure,there are asymmetrical effects across language pairs and directions. We arguethat this asymmetry may shape hypotheses about human structural primingeffects. We also find that structural priming effects are less robust for lesssimilar language pairs, highlighting potential limitations of crosslingualtransfer learning and shared representations for typologically diverselanguages.</description>
      <author>example@mail.com (Catherine Arnett, Tyler A. Chang, James A. Michaelov, Benjamin K. Bergen)</author>
      <guid isPermaLink="false">2503.03962v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical quantum embedding by machine learning for large molecular assemblies</title>
      <link>http://arxiv.org/abs/2503.03928v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;在量子-经典混合模型中，相关结构区域（如反应中心或宿主分子结合口袋）可以由量子模型描述，并嵌入到经典的分子力学环境中。然而，当量子区域变得非常大时，仅能使用近似的电子结构模型。&lt;h4&gt;目的&lt;/h4&gt;提出一种量子内量子嵌入策略并结合机器学习势能来提高大型分子量子-经典混合模型的准确性。&lt;h4&gt;方法&lt;/h4&gt;在量子区域内引入量子核心概念，这些核心因为大小有限而可以采用精确的电子结构模型。例如，Huzinaga型投影基态嵌入技术能够提供准确的电子能量，进而通过转移学习方法提升机器学习势能的精度。&lt;h4&gt;主要发现&lt;/h4&gt;该策略提高了结合自由能计算中的量子描述准确性，特别是对于蛋白质-配体复合物的精确度有显著改进。&lt;h4&gt;结论&lt;/h4&gt;利用此策略可以有效地提高大型分子中特定区域的量子模拟精度，并通过机器学习方法优化整个系统的势能模型。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种量子内嵌入策略结合机器学习势能，以改善大规模分子量子-经典混合模型的准确性。在这样的混合模型中，相关结构区域（例如反应中心或宿主分子结合口袋）可以通过一个量子模型进行描述，并嵌入到经典的分子力学环境中。然而，当这个量子区域变得太大时，只有近似的电子结构模型适用。为了恢复量子描述中的精度，我们在该区域内引入了可以采用精确电子结构模型的量子核心概念，由于它们尺寸较小。例如，Huzinaga型投影基态嵌入技术可以提供准确的电子能量，这是通过先进的电子结构方法获得的。然后，这些总电子能量被输入到一个转移学习方法中，高效地利用更高精度的数据来改进原始量子-经典混合方法所得到的机器学习势能。我们在此策略下探讨了一种经过充分研究的蛋白质-配体复合物中的结合自由能计算能力，使用了化学计量自由能和非平衡切换模拟技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a quantum-in-quantum embedding strategy coupled to machinelearning potentials to improve on the accuracy of quantum-classical hybridmodels for the description of large molecules. In such hybrid models, relevantstructural regions (such as those around reaction centers or pockets forbinding of host molecules) can be described by a quantum model that is thenembedded into a classical molecular-mechanics environment. However, thisquantum region may become so large that only approximate electronic structuremodels are applicable. To then restore accuracy in the quantum description, wehere introduce the concept of quantum cores within the quantum region that areamenable to accurate electronic structure models due to their limited size.Huzinaga-type projection-based embedding, for example, can deliver accurateelectronic energies obtained with advanced electronic structure methods. Theresulting total electronic energies are then fed into a transfer learningapproach that efficiently exploits the higher-accuracy data to improve on amachine learning potential obtained for the original quantum-classical hybridapproach. We explore the potential of this approach in the context of awell-studied protein-ligand complex for which we calculate the free energy ofbinding using alchemical free energy and non-equilibrium switching simulations.</description>
      <author>example@mail.com (Moritz Bensberg, Marco Eckhoff, Raphael T. Husistein, Matthew S. Teynor, Valentina Sora, William Bro-Jørgensen, F. Emil Thomasen, Anders Krogh, Kresten Lindorff-Larsen, Gemma C. Solomon, Thomas Weymuth, Markus Reiher)</author>
      <guid isPermaLink="false">2503.03928v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>ForestLPR: LiDAR Place Recognition in Forests Attentioning Multiple BEV Density Images</title>
      <link>http://arxiv.org/abs/2503.04475v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于LiDAR的自然森林环境下的地点识别方法ForestLPR，该方法利用不同高度的横截面图像来实现位置再认。&lt;h4&gt;背景&lt;/h4&gt;城市环境中基于激光雷达或相机的地方识别技术已有显著进展，但天然森林等自然环境中的应用尚待深入研究。这些环境具有高自我相似性和随时间变化而产生的植被变化。&lt;h4&gt;目的&lt;/h4&gt;提出一种在自然森林中鲁棒的LiDAR基地方识别方法ForestLPR。&lt;h4&gt;方法&lt;/h4&gt;通过激光雷达点云的不同高度水平切片生成贝叶斯等效视图密度图像作为横截面图像，采用视觉变换器作为共享主干网络以产生局部描述符集合，并引入多BEV交互模块根据不同高度自适应地关注信息。最终聚合层输出一个旋转不变的位置描述符。&lt;h4&gt;主要发现&lt;/h4&gt;在公开基准测试和机器人数据集上的真实世界数据上进行了广泛评估，结果显示ForestLPR在所有评价中的表现优异，在序列内回环闭合检测和序列间重新定位中分别比最近的竞争对手提高了7.38%和9.11%&lt;h4&gt;结论&lt;/h4&gt;实验验证了提出的基于横截面图像的方法对于自然森林环境下的地方识别是有效的。&lt;h4&gt;翻译&lt;/h4&gt;地点识别对大规模定位系统保持全局一致性至关重要。尽管城市环境中使用激光雷达或相机的研究取得了显著进展，但在类似天然森林的自然环境中的应用仍然很大程度上未被探索。此外，由于高度的自我相似性和随时间变化产生的植被变化，森林提出了特殊挑战。在本研究中，我们提出了一种用于自然森林的鲁棒LiDAR基地方识别方法ForestLPR。我们的假设是，在不同高度处生成的森林几何学横截面图像包含重新访问地点所需的信息。这些横截面图像是通过激光雷达点云的不同高度水平切片表示为贝叶斯等效视图密度图像的集合。我们采用视觉变换器作为共享主干网络以产生局部描述符集合，并引入了多BEV交互模块，根据不同高度自适应地关注信息。这之后是一个聚合层，它生成一个旋转不变的位置描述符。我们在公开基准测试和机器人数据集上的真实世界数据上进行了广泛评估，并将我们的方法与最先进（SOTA）的方法进行了比较。结果表明，ForestLPR在所有评价中均有持续优异的表现，在序列内回环闭合检测和序列间重新定位中的Recall@1分别比最近的竞争对手提高了7.38%和9.11%，验证了我们假设的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Place recognition is essential to maintain global consistency in large-scalelocalization systems. While research in urban environments has progressedsignificantly using LiDARs or cameras, applications in natural forest-likeenvironments remain largely under-explored. Furthermore, forests presentparticular challenges due to high self-similarity and substantial variations invegetation growth over time. In this work, we propose a robust LiDAR-basedplace recognition method for natural forests, ForestLPR. We hypothesize that aset of cross-sectional images of the forest's geometry at different heightscontains the information needed to recognize revisiting a place. Thecross-sectional images are represented by \ac{bev} density images of horizontalslices of the point cloud at different heights. Our approach utilizes a visualtransformer as the shared backbone to produce sets of local descriptors andintroduces a multi-BEV interaction module to attend to information at differentheights adaptively. It is followed by an aggregation layer that produces arotation-invariant place descriptor. We evaluated the efficacy of our methodextensively on real-world data from public benchmarks as well as roboticdatasets and compared it against the state-of-the-art (SOTA) methods. Theresults indicate that ForestLPR has consistently good performance on allevaluations and achieves an average increase of 7.38\% and 9.11\% on Recall@1over the closest competitor on intra-sequence loop closure detection andinter-sequence re-localization, respectively, validating our hypothesis</description>
      <author>example@mail.com (Yanqing Shen, Turcan Tuna, Marco Hutter, Cesar Cadena, Nanning Zheng)</author>
      <guid isPermaLink="false">2503.04475v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation</title>
      <link>http://arxiv.org/abs/2503.04162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的序列推荐框架Semantic Retrieval Augmented Contrastive Learning (SRA-CL)，通过利用语义信息来增强对比学习的有效性，解决了现有方法在生成可靠正样本时的局限性。&lt;h4&gt;背景&lt;/h4&gt;顺序推荐旨在根据历史行为序列建模用户偏好，在线上平台中至关重要。然而数据稀疏问题是这一领域的重要挑战，因为大多数用户的交互有限且许多项目受到的关注较少。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的对比学习方法SRA-CL来缓解数据稀疏问题，并生成可靠的正样本对以提高推荐模型的性能。&lt;h4&gt;方法&lt;/h4&gt;SRA-CL主要包含两个组成部分：(1)通过用户语义检索进行跨序列对比学习，使用大规模语言模型（LLMs）理解用户的多样化偏好并找到语义相似的用户形成可靠正样本；(2)通过项目语义检索进行内序列对比学习，使用LLMs来理解和替换类似项，创建语义一致性的增强视图以供对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;SRA-CL可以有效利用大规模语言模型提取用户的偏好和项目的特性，并生成可靠的对比正样本，从而提高推荐的准确性和多样性。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的实验验证了所提出的框架的有效性和泛化能力。这种新方法不仅提高了序列推荐的质量，而且还可以很容易地集成到现有的顺序推荐系统中。&lt;h4&gt;翻译&lt;/h4&gt;摘要提供了关于SRA-CL的研究背景、目标、创新方法及其效果的概述，并展示了如何利用语义信息增强对比学习，从而改善了在线平台上的用户推荐体验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential recommendation aims to model user preferences based on historicalbehavior sequences, which is crucial for various online platforms. Datasparsity remains a significant challenge in this area as most users havelimited interactions and many items receive little attention. To mitigate thisissue, contrastive learning has been widely adopted. By constructing positivesample pairs from the data itself and maximizing their agreement in theembedding space,it can leverage available data more effectively. Constructingreasonable positive sample pairs is crucial for the success of contrastivelearning. However, current approaches struggle to generate reliable positivepairs as they either rely on representations learned from inherently sparsecollaborative signals or use random perturbations which introduce significantuncertainty. To address these limitations, we propose a novel approach namedSemantic Retrieval Augmented Contrastive Learning (SRA-CL), which leveragessemantic information to improve the reliability of contrastive samples. SRA-CLcomprises two main components: (1) Cross-Sequence Contrastive Learning via UserSemantic Retrieval, which utilizes large language models (LLMs) to understanddiverse user preferences and retrieve semantically similar users to formreliable positive samples through a learnable sample synthesis method; and (2)Intra-Sequence Contrastive Learning via Item Semantic Retrieval, which employsLLMs to comprehend items and retrieve similar items to perform semantic-baseditem substitution, thereby creating semantically consistent augmented views forcontrastive learning. SRA-CL is plug-and-play and can be integrated intostandard sequential recommendation models. Extensive experiments on four publicdatasets demonstrate the effectiveness and generalizability of the proposedapproach.</description>
      <author>example@mail.com (Ziqiang Cui, Yunpeng Weng, Xing Tang, Xiaokun Zhang, Dugang Liu, Shiwei Li, Peiyang Liu, Bowei He, Weihong Luo, Xiuqiang He, Chen Ma)</author>
      <guid isPermaLink="false">2503.04162v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>CA-W3D: Leveraging Context-Aware Knowledge for Weakly Supervised Monocular 3D Detection</title>
      <link>http://arxiv.org/abs/2503.04154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper includes 8 pages, 6 figures and 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新的弱监督单目3D检测模型CA-W3D，旨在解决传统方法忽略上下文语义关系的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的标签效率高的方法主要关注对象中心特征，忽略了复杂场景中关键的上下文语义关系。这导致在全局上下文中捕捉信息的能力不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于上下文感知弱监督的方法来改进单目3D检测性能。&lt;h4&gt;方法&lt;/h4&gt;{'两阶段训练模式': [{'预训练阶段': '使用区域物体对比匹配(ROCM)对可训练的单目3D编码器和冻结的开放词汇2D视觉定位模型进行对齐，促进场景特定属性的辨别以及上下文知识的获取。'}, {'伪标签训练过程': '在第二阶段中引入了Dual-to-One Distillation (D2OD)机制，将上下文先验有效转移到单目编码器中，同时保持空间保真度和推理时的计算效率。'}]}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果证明了所提出的方法的有效性，在公共KITTI基准测试上超过了现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;研究强调了上下文感知知识在弱监督单目3D检测中的重要性，展示了通过引入适当的上下文信息能够显著提升模型性能。&lt;h4&gt;翻译&lt;/h4&gt;提出的Context-Aware Weak Supervision for Monocular 3D object detection (CA-W3D) 方法，在两阶段训练范式中解决了现有方法忽视复杂场景关键上下文语义关系的问题。该方法在公共KITTI基准测试上超过了当前最先进水平，证明了引入适当的上下文信息能够显著提升弱监督单目3D检测的性能和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Weakly supervised monocular 3D detection, while less annotation-intensive,often struggles to capture the global context required for reliable 3Dreasoning. Conventional label-efficient methods focus on object-centricfeatures, neglecting contextual semantic relationships that are critical incomplex scenes. In this work, we propose a Context-Aware Weak Supervision forMonocular 3D object detection, namely CA-W3D, to address this limitation in atwo-stage training paradigm. Specifically, we first introduce a pre-trainingstage employing Region-wise Object Contrastive Matching (ROCM), which alignsregional object embeddings derived from a trainable monocular 3D encoder and afrozen open-vocabulary 2D visual grounding model. This alignment encourages themonocular encoder to discriminate scene-specific attributes and acquire richercontextual knowledge. In the second stage, we incorporate a pseudo-labeltraining process with a Dual-to-One Distillation (D2OD) mechanism, whicheffectively transfers contextual priors into the monocular encoder whilepreserving spatial fidelity and maintaining computational efficiency duringinference. Extensive experiments conducted on the public KITTI benchmarkdemonstrate the effectiveness of our approach, surpassing the SoTA method overall metrics, highlighting the importance of contextual-aware knowledge inweakly-supervised monocular 3D detection.</description>
      <author>example@mail.com (Chupeng Liu, Runkai Zhao, Weidong Cai)</author>
      <guid isPermaLink="false">2503.04154v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Models in Bioinformatics: A Survey</title>
      <link>http://arxiv.org/abs/2503.04490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该综述系统地回顾了大型语言模型（LLMs）在生物信息学领域的最新进展及其挑战和未来方向。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型正在革新生物信息学领域，能够在DNA、RNA、蛋白质以及单细胞数据分析方面提供高级分析方法。&lt;h4&gt;目的&lt;/h4&gt;总结并讨论最近的生物信息学技术进步，并探索未来的研究趋势。&lt;h4&gt;主要发现&lt;/h4&gt;{'研究进展': ['基因组序列建模', 'RNA结构预测', '蛋白质功能推断', '单细胞转录组学'], '关键挑战': ['数据稀缺性', '计算复杂度', '跨领域整合']}&lt;h4&gt;结论&lt;/h4&gt;强调了大型语言模型在生物信息学和精准医学中的变革潜力，并提出未来可能的发展方向，如多模态学习、混合AI模型以及临床应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) are revolutionizing bioinformatics, enablingadvanced analysis of DNA, RNA, proteins, and single-cell data. This surveyprovides a systematic review of recent advancements, focusing on genomicsequence modeling, RNA structure prediction, protein function inference, andsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,including data scarcity, computational complexity, and cross-omics integration,and explore future directions such as multimodal learning, hybrid AI models,and clinical applications. By offering a comprehensive perspective, this paperunderscores the transformative potential of LLMs in driving innovations inbioinformatics and precision medicine.</description>
      <author>example@mail.com (Zhenyu Wang, Zikang Wang, Jiyue Jiang, Pengan Chen, Xiangyu Shi, Yu Li)</author>
      <guid isPermaLink="false">2503.04490v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>PointsToWood: A deep learning framework for complete canopy leaf-wood segmentation of TLS data across diverse European forests</title>
      <link>http://arxiv.org/abs/2503.04420v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;三维激光扫描（TLS）获取的点云数据是研究植物结构和功能的重要来源。然而，为了提取重要的生态信息，通常需要大量的人工处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度学习框架来准确地对不同类型的植物材料进行语义分割，特别是木材和叶子。&lt;h4&gt;方法&lt;/h4&gt;本研究提出了一个新的基于PointNet和pointNEXT的深度学习架构，用于处理3D点云数据。该模型使用了精确标记的数据、体素采样技术以及新设计的门控反射整合模块等。&lt;h4&gt;主要发现&lt;/h4&gt;我们的模型在欧洲不同成熟的森林中训练，并且在北极、温带、地中海及热带地区的公开数据集上进行了测试，结果显示，在对叶/木语义分割方面优于最常用的基于PointNet的方法。此外，该模型在中国、东喀麦隆、德国和芬兰的数据集中也表现出色，这些数据是使用飞行时间和相位移传感器收集的。&lt;h4&gt;结论&lt;/h4&gt;开发的新框架能够提供从树基到枝尖的木材和叶的可靠语义分割，展示了良好的跨生态系统类型和不同传感器类型的可转移性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，此处为其中文翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds from Terrestrial Laser Scanning (TLS) are an increasinglypopular source of data for studying plant structure and function but typicallyrequire extensive manual processing to extract ecologically importantinformation. One key task is the accurate semantic segmentation of differentplant material within point clouds, particularly wood and leaves, which isrequired to understand plant productivity, architecture and physiology.Existing automated semantic segmentation methods are primarily developed forsingle ecosystem types, and whilst they show good accuracy for biomassassessment from the trunk and large branches, often perform less well withinthe crown. In this study, we demonstrate a new framework that uses a deeplearning architecture newly developed from PointNet and pointNEXT forprocessing 3D point clouds to provide a reliable semantic segmentation of woodand leaf in TLS point clouds from the tree base to branch tips, trained on datafrom diverse mature European forests. Our model uses meticulously labelled datacombined with voxel-based sampling, neighbourhood rescaling, and a novel gatedreflectance integration module embedded throughout the feature extractionlayers. We evaluate its performance across open datasets from boreal,temperate, Mediterranean and tropical regions, encompassing diverse ecosystemtypes and sensor characteristics. Our results show consistent outperformanceagainst the most widely used PointNet based approach for leaf/wood segmentationon our high-density TLS dataset collected across diverse mixed forest plotsacross all major biomes in Europe. We also find consistently strong performancetested on others open data from China, Eastern Cameroon, Germany and Finland,collected using both time-of-flight and phase-shift sensors, showcasing thetransferability of our model to a wide range of ecosystems and sensors.</description>
      <author>example@mail.com (Harry J. F. Owen, Matthew J. A. Allen, Stuart W. D. Grieve, Phill Wilkes, Emily R. Lines)</author>
      <guid isPermaLink="false">2503.04420v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Frequency-Based Alignment of EEG and Audio Signals Using Contrastive Learning and SincNet for Auditory Attention Detection</title>
      <link>http://arxiv.org/abs/2503.04156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;人类在复杂的声学环境中具备出色的听觉注意力聚焦能力，如鸡尾酒会场景。听觉注意检测（AAD）旨在通过分析脑信号来识别被关注的说话人，这些信号包括脑电图（EEG）数据等。&lt;h4&gt;背景&lt;/h4&gt;现有的AAD算法通常利用深度学习的强大非线性建模能力，但很少考虑大脑中听觉处理的神经机制。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于改进SincNet和对比学习的新网络模型——SincAlignNet，旨在对齐音频和EEG特征进行听觉注意检测。&lt;h4&gt;方法&lt;/h4&gt;通过计算EEG与音频特征之间的余弦相似度，并探索仅使用脑电数据直接推断被关注说话人的可能性。SincNet组件模拟了大脑在听觉注意力下处理音频的过程；对比学习引导模型学习EEG信号和被关注语音之间的关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SincAlignNet优于现有公开数据集（KUL和DTU）上的最先进的AAD方法，在1秒决策窗口的平均准确率为78.3%和92.2%，并且该模型具有较强的可解释性。此外，使用靠近颞叶区域附近六个电极的数据即可维持相似甚至更好的性能。&lt;h4&gt;结论&lt;/h4&gt;研究表明高效低密度EEG在线解码是可能实现的，并为实际应用中的神经引导助听器的重要步骤铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要：人类在复杂的声学环境中表现出非凡的能力，能够集中听觉注意力，例如鸡尾酒会。听觉注意检测（AAD）的目标是在分析脑信号的基础上确定被关注的说话人，这些信号包括如脑电图（EEG）数据等。现有的AAD算法通常利用深度学习的强大非线性建模能力，然而较少考虑大脑中与听觉处理相关的神经机制。本文提出了一种新的网络模型——SincAlignNet，该模型基于改进后的SincNet和对比学习，并设计用于对齐音频与EEG特征以进行听觉注意检测。SincNet组件模仿了在听觉注意力状态下大脑如何处理声音的过程；而对比学习指导模型去学习EEG信号与被关注的语音之间的关系。在推理阶段，通过计算EEG和音频特征间的余弦相似度，并且探索仅利用脑电图数据直接推断所关注说话人的可能性。跨试验评估表明，在两个公开的数据集KUL和DTU上，SincAlignNet优于现有的最先进的AAD方法，分别实现了78.3%和92.2%的平均准确率（1秒决策窗口）。该模型表现出较强的可解释性，揭示在男女性说话场景中左侧与右侧颞叶活动更为显著。此外，我们发现仅使用靠近颞叶区域附近六个电极的数据即可保持相似或更好的性能，这表明高效低密度EEG在线解码是可行的，并向实际应用中的神经引导助听器的重要进展迈进了一步。&lt;h4&gt;代码链接&lt;/h4&gt;https://github.com/LiaoEuan/SincAlignNet&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans exhibit a remarkable ability to focus auditory attention in complexacoustic environments, such as cocktail parties. Auditory attention detection(AAD) aims to identify the attended speaker by analyzing brain signals, such aselectroencephalography (EEG) data. Existing AAD algorithms often leverage deeplearning's powerful nonlinear modeling capabilities, few consider the neuralmechanisms underlying auditory processing in the brain. In this paper, wepropose SincAlignNet, a novel network based on an improved SincNet andcontrastive learning, designed to align audio and EEG features for auditoryattention detection. The SincNet component simulates the brain's processing ofaudio during auditory attention, while contrastive learning guides the model tolearn the relationship between EEG signals and attended speech. Duringinference, we calculate the cosine similarity between EEG and audio featuresand also explore direct inference of the attended speaker using EEG data.Cross-trial evaluations results demonstrate that SincAlignNet outperformsstate-of-the-art AAD methods on two publicly available datasets, KUL and DTU,achieving average accuracies of 78.3% and 92.2%, respectively, with a 1-seconddecision window. The model exhibits strong interpretability, revealing that theleft and right temporal lobes are more active during both male and femalespeaker scenarios. Furthermore, we found that using data from only sixelectrodes near the temporal lobes maintains similar or even better performancecompared to using 64 electrodes. These findings indicate that efficientlow-density EEG online decoding is achievable, marking an important step towardthe practical implementation of neuro-guided hearing aids in real-worldapplications. Code is available at: https://github.com/LiaoEuan/SincAlignNet.</description>
      <author>example@mail.com (Yuan Liao, Yuhong Zhang, Qiushi Han, Yuhang Yang, Weiwei Ding, Yuzhe Gu, Hengxin Yang, Liya Huang)</author>
      <guid isPermaLink="false">2503.04156v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>ObjMST: An Object-Focused Multimodal Style Transfer Framework</title>
      <link>http://arxiv.org/abs/2503.04353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 Figures, 3 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个以对象为中心的多模态风格转换框架ObjMST，该框架通过分别对显著对象及其周围元素提供特定样式监督的方式解决了跨模态表示学习中的对齐问题。&lt;h4&gt;背景&lt;/h4&gt;现有的图像-文本多模态风格转移方法面临着生成不一致且不对齐的多模态表示以及内容不匹配的问题。这些问题导致在风格化过程中，相同风格模式被错误地应用到显著对象及其周围元素上。&lt;h4&gt;目的&lt;/h4&gt;为了缓解上述挑战，本文提出了一种新的框架ObjMST，旨在提供准确且对齐的样式表达，同时解决图像-文本多模态风格转移中的内容不匹配问题。&lt;h4&gt;方法&lt;/h4&gt;该方法通过引入特定样式的掩码方向性CLIP损失函数来确保显著对象及其周围环境的一致性和对齐性，并结合显着对象到关键元素映射机制以及图像调和化技术，实现将风格化的对象无缝融入其环境中。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，ObjMST在多模态风格转换任务上具有良好的效果，不仅能够生成一致且准确的样式表示，而且还能有效解决内容不匹配问题。&lt;h4&gt;结论&lt;/h4&gt;通过定量和定性评估证明了所提出的方法的有效性和创新性。作者认为这种基于对象的多模态风格转移框架可以作为未来研究的基础，并进一步扩展到其他跨模态任务中。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了ObjMST，这是一个以对象为中心的多模态样式转换框架，它为显著对象及其周围元素提供了单独的样式监督，并解决了跨模态表示学习中的对齐问题。现有的图像-文本多模态风格转移方法面临着以下挑战：(1)生成非对齐且不一致的多模态样式表示；以及(2)内容不匹配的问题，即相同的样式模式被应用于显著对象及其周围元素。我们的方法通过引入特定样式的掩码方向性CLIP损失来减轻这些问题，该机制确保了显著对象及其环境的一致性和对齐性，并结合显着对象到关键元素的映射机制进行风格化处理，随后采用图像调和化技术无缝融合样式化的对象与其周围环境。我们通过实验验证了ObjMST的有效性，使用定量指标以及定性的视觉评估来评价其结果。我们的代码可在https://github.com/chandagrover/ObjMST获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose ObjMST, an object-focused multimodal style transfer framework thatprovides separate style supervision for salient objects and surroundingelements while addressing alignment issues in multimodal representationlearning. Existing image-text multimodal style transfer methods face thefollowing challenges: (1) generating non-aligned and inconsistent multimodalstyle representations; and (2) content mismatch, where identical style patternsare applied to both salient objects and their surrounding elements. Ourapproach mitigates these issues by: (1) introducing a Style-Specific MaskedDirectional CLIP Loss, which ensures consistent and aligned stylerepresentations for both salient objects and their surroundings; and (2)incorporating a salient-to-key mapping mechanism for stylizing salient objects,followed by image harmonization to seamlessly blend the stylized objects withtheir environment. We validate the effectiveness of ObjMST through experiments,using both quantitative metrics and qualitative visual evaluations of thestylized outputs. Our code is available at:https://github.com/chandagrover/ObjMST.</description>
      <author>example@mail.com (Chanda Grover Kamra, Indra Deep Mastan, Debayan Gupta)</author>
      <guid isPermaLink="false">2503.04353v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>MASTER: Multimodal Segmentation with Text Prompts</title>
      <link>http://arxiv.org/abs/2503.04199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RGB-热融合技术在各种天气和光照条件下具有潜在解决方案，通过将大型语言模型（LLMs）的优势融入RGB-热多模态数据的融合过程中，设计出一种结构简单且高度可适应的多模态融合模型。&lt;h4&gt;背景&lt;/h4&gt;现有的研究大多集中在设计复杂模块来融合不同模式的数据。然而，随着大规模语言模型的应用越来越广泛，通过自然语言可以更有效地提取有价值的信息。&lt;h4&gt;目的&lt;/h4&gt;旨在利用大型语言模型的优势，设计出一种结构简单且高度可适应的多模态融合模型架构。&lt;h4&gt;方法&lt;/h4&gt;提出了MASTER架构，将LLM集成到RGB-热多模态数据的融合过程中，并允许复杂的查询文本参与融合过程。该模型采用双路径结构来提取不同图像模式的信息，并使用LLM作为多模态融合的核心模块，生成可学习码本令牌，同时利用轻量级图像解码器获得语义分割结果。&lt;h4&gt;主要发现&lt;/h4&gt;提出的MASTER架构在多个自动驾驶场景的基准测试中表现出色，取得了令人满意的结果。&lt;h4&gt;结论&lt;/h4&gt;通过将大型语言模型的优势融入RGB-热多模态数据的融合过程，可以设计出结构简单且高度可适应的多模态融合模型，显著提升自动化驾驶任务中的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;RGB-Thermal 融合是一种潜在的解决方案，适用于各种天气和光照条件下的挑战性场景。然而，许多研究集中在设计复杂的模块来融合不同的模式数据。随着大型语言模型（LLMs）的广泛应用，可以通过自然语言更有效地提取有价值的信息。因此，我们的目标是利用大型语言模型的优势来设计一种结构简单且高度可适应的多模态融合模型架构。我们提出了MultimodAl Segmentation with TExt PRompts (MASTER) 架构，该架构将LLM整合到RGB-热多模态数据的融合过程中，并允许复杂的查询文本参与此过程。我们的模型利用一种双路径结构来提取不同图像模式的信息，同时使用LLM作为多模态融合的核心模块，使模型能够从RGB、热图和文字信息中生成可学习的码本令牌。通过轻量级的图像解码器可以获取语义分割结果。提出的MASTER架构在多个自动驾驶场景的基准测试中表现出色，并获得了令人满意的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; RGB-Thermal fusion is a potential solution for various weather and lightconditions in challenging scenarios. However, plenty of studies focus ondesigning complex modules to fuse different modalities. With the widespreadapplication of large language models (LLMs), valuable information can be moreeffectively extracted from natural language. Therefore, we aim to leverage theadvantages of large language models to design a structurally simple and highlyadaptable multimodal fusion model architecture. We proposed MultimodAlSegmentation with TExt PRompts (MASTER) architecture, which integrates LLM intothe fusion of RGB-Thermal multimodal data and allows complex query text toparticipate in the fusion process. Our model utilizes a dual-path structure toextract information from different modalities of images. Additionally, weemploy LLM as the core module for multimodal fusion, enabling the model togenerate learnable codebook tokens from RGB, thermal images, and textualinformation. A lightweight image decoder is used to obtain semanticsegmentation results. The proposed MASTER performs exceptionally well inbenchmark tests across various automated driving scenarios, yielding promisingresults.</description>
      <author>example@mail.com (Fuyang Liu, Shun Lu, Jilin Mei, Yu Hu)</author>
      <guid isPermaLink="false">2503.04199v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Sarcasm Detection as a Catalyst: Improving Stance Detection with Cross-Target Capabilities</title>
      <link>http://arxiv.org/abs/2503.03787v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2 pages, 5 figures, published, published in International Journal On  Advances in Intelligent Systems, volume 17, numbers 3 and 4. arXiv admin  note: text overlap with arXiv:2503.03172&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种结合讽刺检测的立场识别方法，并通过跨目标立场检测任务评估其效果，展示了显著优于现有最佳模型的表现。&lt;h4&gt;背景&lt;/h4&gt;立场识别由于在线平台文本中存在讽刺语言而面临挑战。同时，缺乏足够的标注数据用于训练新的立场识别模型。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合讽刺预训练的立场识别方法，以解决上述问题，并通过跨目标立场检测来评估模型性能。&lt;h4&gt;方法&lt;/h4&gt;采用微调BERT和RoBERTa模型并附加深度学习层的方法进行讽刺与立场识别。该方法在公开数据集上与其他最先进的基线模型进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;在无讽刺预训练的情况下，相较于现有最佳模型，该方法在同源域的立场识别任务中准确率提升了85%。此外，在跨目标任务中，无需额外微调即可达到与同源域任务相当的表现，并且该成功依赖于讽刺检测和立场识别之间的词汇属性相关性。&lt;h4&gt;结论&lt;/h4&gt;这项研究首次探索了将讽刺检测作为中间转移学习任务的前景，并利用BERT或RoBERTa与其他深度学习技术相结合的方法。提出的这种方法为未来的研究提供了基础基线。&lt;h4&gt;翻译&lt;/h4&gt;立场检测（SD）由于其在各种上下文中的应用而成为自然语言处理领域的一个重要研究方向，但由于在线平台文本中讽刺语言的存在使得准确确定作者立场变得困难。本论文通过利用讽刺来解决这一问题，并通过跨目标的立场识别任务解决了缺乏足够的标注数据的问题。提出的方法包括微调BERT和RoBERTa模型以及附加深度学习层，并且在公开的数据集上与其他最先进的基线方法进行了比较，展示了超越现有最佳模型的表现。值得注意的是，在不进行讽刺检测预训练的情况下，我们的模型已经在同源域的立场识别任务中优于现有的顶级模型。将讽刺知识整合到模型中显著减少了误分类讽刺文本元素的数量，使得在无讽刺检测预训练的情况下准确预测了85%此前被错误分类的内容，并提高了宏F1平均分。跨目标任务通过零样本微调也达到了与同源域相似的表现。此外，成功还依赖于讽刺识别和SD之间的词汇属性相关性。这项研究首次探索了将讽刺检测作为中间转移学习任务的可能，并利用BERT或RoBERTa与其他深度学习技术相结合的方法。所提出的方法为该领域的未来研究建立了基础基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stance Detection (SD) has become a critical area of interest due to itsapplications in various contexts leading to increased research within NLP. Yetthe subtlety and complexity of texts sourced from online platforms oftencontaining sarcastic language pose significant challenges for SD algorithms inaccurately determining the authors stance. This paper addresses this byemploying sarcasm for SD. It also tackles the issue of insufficient annotateddata for training SD models on new targets by conducting Cross-Target SD(CTSD). The proposed approach involves fine-tuning BERT and RoBERTa modelsfollowed by concatenating additional deep learning layers. The approach isassessed against various State-Of-The-Art baselines for SD demonstratingsuperior performance using publicly available datasets. Notably our modeloutperforms the best SOTA models on both in-domain SD and CTSD tasks evenbefore the incorporation of sarcasm-detection pre-training. The integration ofsarcasm knowledge into the model significantly reduces misclassifications ofsarcastic text elements in SD allowing our model to accurately predict 85% oftexts that were previously misclassified without sarcasm-detection pre-trainingon in-domain SD. This enhancement contributes to an increase in the modelsaverage macro F1-score. The CTSD task achieves performance comparable to thatof the in-domain task despite using a zero-shot finetuning. We also reveal thatthe success of the transfer-learning framework relies on the correlationbetween the lexical attributes of sarcasm detection and SD. This studyrepresents the first exploration of sarcasm detection as an intermediatetransfer-learning task within the context of SD while also leveraging theconcatenation of BERT or RoBERTa with other deep-learning techniques. Theproposed approach establishes a foundational baseline for future research inthis domain.</description>
      <author>example@mail.com (Gibson Nkhata Shi Yin Hong, Susan Gauch)</author>
      <guid isPermaLink="false">2503.03787v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Real-time Spatial-temporal Traversability Assessment via Feature-based Sparse Gaussian Process</title>
      <link>http://arxiv.org/abs/2503.04134v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种新颖的空间-时间可通行性评估方法，旨在使自主机器人能够在复杂地形中有效导航。&lt;h4&gt;背景&lt;/h4&gt;地形分析对于地面移动机器人的实际应用至关重要，尤其是在户外非结构化环境中。&lt;h4&gt;目的&lt;/h4&gt;目标是开发一种能够帮助自主机器人在复杂地形中有效导航的方法。&lt;h4&gt;方法&lt;/h4&gt;{'空间-时间贝叶斯高斯核（BGK）推理方法': '利用稀疏高斯过程直接从点云扫描中提取几何特征，然后构建高分辨率局部可通行性地图。此外，设计了一种将历史和实时数据结合考虑坡度、平坦度等因子的空间-时间贝叶斯高斯核推理方法来动态评估可通行性得分。', 'GPU加速': '在特征提取步骤中应用GPU加速以实现实时性能'}&lt;h4&gt;主要发现&lt;/h4&gt;广泛的模拟实验表明，该方法在精度和计算效率方面优于现有最佳技术（SOTA）&lt;h4&gt;结论&lt;/h4&gt;开发了一种自主导航框架，并通过差分驱动车辆在复杂户外环境中进行了验证。&lt;h4&gt;翻译&lt;/h4&gt;地形分析对于地面移动机器人的实际应用至关重要，尤其是在户外非结构化环境中。本文提出了一种新颖的空间-时间可通行性评估方法，旨在使自主机器人能够在复杂地形中有效导航。该方法利用稀疏高斯过程（SGP）直接从点云扫描中提取几何特征（如曲率、坡度、高度等），然后构建高分辨率局部可通行性地图。进一步设计了空间-时间贝叶斯高斯核（BGK）推理方法，结合历史和实时数据，考虑坡度、平坦度等因素来动态评估可通行性得分，并在特征提取步骤中应用GPU加速以实现实时性能。广泛的模拟实验表明该方法在精度和计算效率方面优于现有最佳技术。此外，开发了一种自主导航框架并与差分驱动车辆进行了复杂户外环境的验证。代码将开源供进一步研究和发展：https://github.com/ZJU-FAST-Lab/FSGP_BGK&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Terrain analysis is critical for the practical application of ground mobilerobots in real-world tasks, especially in outdoor unstructured environments. Inthis paper, we propose a novel spatial-temporal traversability assessmentmethod, which aims to enable autonomous robots to effectively navigate throughcomplex terrains. Our approach utilizes sparse Gaussian processes (SGP) toextract geometric features (curvature, gradient, elevation, etc.) directly frompoint cloud scans. These features are then used to construct a high-resolutionlocal traversability map. Then, we design a spatial-temporal Bayesian Gaussiankernel (BGK) inference method to dynamically evaluate traversability scores,integrating historical and real-time data while considering factors such asslope, flatness, gradient, and uncertainty metrics. GPU acceleration is appliedin the feature extraction step, and the system achieves real-time performance.Extensive simulation experiments across diverse terrain scenarios demonstratethat our method outperforms SOTA approaches in both accuracy and computationalefficiency. Additionally, we develop an autonomous navigation frameworkintegrated with the traversability map and validate it with a differentialdriven vehicle in complex outdoor environments. Our code will be open-sourcefor further research and development by the community,https://github.com/ZJU-FAST-Lab/FSGP_BGK.</description>
      <author>example@mail.com (Senming Tan, Zhenyu Hou, Zhihao Zhang, Long Xu, Mengke Zhang, Zhaoqi He, Chao Xu, Fei Gao, Yanjun Cao)</author>
      <guid isPermaLink="false">2503.04134v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Learning Causal Response Representations through Direct Effect Analysis</title>
      <link>http://arxiv.org/abs/2503.04358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 15 figures, stat.ML&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的学习因果响应表示的方法，旨在从多维度结果中提取由治疗变量直接引起的最直接方向。&lt;h4&gt;背景&lt;/h4&gt;当前方法难以在复杂、多维环境中有效识别直接的因果效应。&lt;h4&gt;目的&lt;/h4&gt;通过将条件独立性测试与因果表征学习相结合来最大化治疗和结局之间在给定一个条件集合下的条件独立性的证据，从而提取出直接由治疗变量导致的结果维度。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一种优化问题框架，并利用广义特征值分解解决了这一问题。此外，它还为所学表示的最优性提供了理论保证，特别是在信号噪声比和费雪信息最大化方面。&lt;h4&gt;主要发现&lt;/h4&gt;最大的特征值分布可以在满足轻度假设的情况下被已知的F-分布所限制，从而使得条件独立性的可测试性成为可能，并且这种方法在模拟实验和真实世界实验中显示出其有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的框架对于揭示复杂多变量环境中的直接因果效应具有重要的实用价值。&lt;h4&gt;翻译&lt;/h4&gt;We提出了一种新颖的方法来学习因果响应表示。我们的方法旨在提取由治疗变量最直接导致的多维结果的方向。通过将条件独立性测试与因果表征学习相结合，我们制定了一个优化问题以最大化在给定条件集的情况下治疗和结局之间条件独立性的证据。该框架利用灵活回归模型针对特定应用进行了定制，并通过广义特征值分解解决。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel approach for learning causal response representations. Ourmethod aims to extract directions in which a multidimensional outcome is mostdirectly caused by a treatment variable. By bridging conditional independencetesting with causal representation learning, we formulate an optimisationproblem that maximises the evidence against conditional independence betweenthe treatment and outcome, given a conditioning set. This formulation employsflexible regression models tailored to specific applications, creating aversatile framework. The problem is addressed through a generalised eigenvaluedecomposition. We show that, under mild assumptions, the distribution of thelargest eigenvalue can be bounded by a known $F$-distribution, enablingtestable conditional independence. We also provide theoretical guarantees forthe optimality of the learned representation in terms of signal-to-noise ratioand Fisher information maximisation. Finally, we demonstrate the empiricaleffectiveness of our approach in simulation and real-world experiments. Ourresults underscore the utility of this framework in uncovering direct causaleffects within complex, multivariate settings.</description>
      <author>example@mail.com (Homer Durand, Gherardo Varando, Gustau Camps-Valls)</author>
      <guid isPermaLink="false">2503.04358v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Robust Multi-View Learning via Representation Fusion of Sample-Level Attention and Alignment of Simulated Perturbation</title>
      <link>http://arxiv.org/abs/2503.04151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近，多视角学习（MVL）由于其能够融合来自多个视角的判别信息而备受关注。然而，实际中的多视角数据集往往异质且不完善，这通常使得为特定视角组合设计的MVL方法缺乏应用潜力，并限制了它们的有效性。&lt;h4&gt;背景&lt;/h4&gt;在现实世界中，多视角学习（MVL）面临着处理异质和不完善的多视角数据集的挑战。这些数据集中的问题导致现有的MVL方法难以泛化到不同的场景之中。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的鲁棒多视角学习方法（RML），该方法能够同时进行表示融合与对齐，以应对实际应用中遇到的数据异质性和不完善性的问题。&lt;h4&gt;方法&lt;/h4&gt;{'模型架构': '引入了一种简单有效的多视角Transformer融合网络，在其中将异质的多视角数据转换为同构的词嵌入，并通过样本级注意力机制整合多个视角，从而获得一种融合表示。', '对抗学习框架': '提出了基于模拟扰动的多视角对比学习框架，该框架能够动态生成噪声和不可用的干扰以模拟不完美的数据条件。这种方案利用对比学习来对齐模拟出的有噪或无法使用的数据所得到的不同融合表征，从而促使模型学习到判别性和鲁棒性的表示。', '自监督与正则化': 'RML方法是一种自监督的学习方式，并且可以作为正则项用于下游任务。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'实验验证': '在无监督多视角聚类、噪声标签分类以及跨模态哈希检索等不同应用场景中，通过广泛的对比试验和消融研究证实了RML方法的有效性。', '应用范围': '提出的模型不仅可以作为专门的MVL技术应用于特定任务中（如聚类），也可以作为一个可插拔模块嵌入到其他任务之中（例如跨模态哈希检索）以提升它们的表现。'}&lt;h4&gt;结论&lt;/h4&gt;通过引入鲁棒多视角表示学习框架，RML方法能够有效地处理异质性和不完善的多视角数据集，在多个实际应用场景中展现了强大的泛化能力和性能优势。&lt;h4&gt;翻译&lt;/h4&gt;最近，由于其融合来自多个视角的判别信息的能力，多视角学习（MVL）引起了极大的关注。然而，真实世界的多视角数据集经常是异质且不完备的，这通常使得为特定组合设计的MVL方法缺乏应用潜力，并限制了它们的有效性。为了应对这一问题，我们提出了一种新颖的鲁棒MVL方法（称为RML），该方法同时实现表示融合和对齐。具体来说，我们引入了一个简单但有效的多视角Transformer融合网络，在其中我们将异质多视角数据转换为同构词嵌入，并通过样本级注意力机制整合多个视角以获得融合表示。此外，我们提出了一种基于模拟扰动的多视角对比学习框架，该框架动态生成噪声和无用干扰来模拟不完美数据条件。有噪或无法使用的数据获取了两个不同的融合表示，我们利用对比学习将它们对齐，从而学到判别性和鲁棒性的表示。我们的RML是自监督的，并且也可以作为下游任务中的正则项应用。在实验中，我们在无监督多视角聚类、噪声标签分类以及作为一个即插即用模块用于跨模态哈希检索中使用它。广泛的对比试验和消融研究验证了RML的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, multi-view learning (MVL) has garnered significant attention due toits ability to fuse discriminative information from multiple views. However,real-world multi-view datasets are often heterogeneous and imperfect, whichusually makes MVL methods designed for specific combinations of views lackapplication potential and limits their effectiveness. To address this issue, wepropose a novel robust MVL method (namely RML) with simultaneous representationfusion and alignment. Specifically, we introduce a simple yet effectivemulti-view transformer fusion network where we transform heterogeneousmulti-view data into homogeneous word embeddings, and then integrate multipleviews by the sample-level attention mechanism to obtain a fused representation.Furthermore, we propose a simulated perturbation based multi-view contrastivelearning framework that dynamically generates the noise and unusableperturbations for simulating imperfect data conditions. The simulated noisy andunusable data obtain two distinct fused representations, and we utilizecontrastive learning to align them for learning discriminative and robustrepresentations. Our RML is self-supervised and can also be applied fordownstream tasks as a regularization. In experiments, we employ it inunsupervised multi-view clustering, noise-label classification, and as aplug-and-play module for cross-modal hashing retrieval. Extensive comparisonexperiments and ablation studies validate the effectiveness of RML.</description>
      <author>example@mail.com (Jie Xu, Na Zhao, Gang Niu, Masashi Sugiyama, Xiaofeng Zhu)</author>
      <guid isPermaLink="false">2503.04151v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Intermediate Domain-guided Adaptation for Unsupervised Chorioallantoic Membrane Vessel Segmentation</title>
      <link>http://arxiv.org/abs/2503.03546v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的Intermediate Domain-guided Adaptation (IDA) 方法，利用中间域信息和现有公共视网膜数据集对CAM图像进行无监督训练。&lt;h4&gt;背景&lt;/h4&gt;CAM模型广泛应用于血管生成研究中，血管网状结构的分布是关键评价指标。因此，基于拓扑和形态学特征的定量评估需要精确的血管分割方法。然而，手动分割耗时且容易产生主观误差，并且关于CAM血管分割算法的研究仍然有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的无监督领域适应（UDA）方法以改进CAM图像中的血管自动分割。&lt;h4&gt;方法&lt;/h4&gt;提出了一种Intermediate Domain-guided Adaptation (IDA) 方法。该方法包括Multi-Resolution Asymmetric Translation (MRAT)策略，用于生成中间域图象，以及Intermediate Domain-guided Contrastive Learning (IDCL) 模块，用于分离跨领域特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的算法在新创建的CAM数据集上进行了评估，并且其性能优于其他方法。此外，在视网膜数据集中，该方法也表现出色，说明了其强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过利用中间域信息和现有的公共数据集，可以有效地提高对CAM图像中血管自动分割的效果。&lt;h4&gt;翻译&lt;/h4&gt;胎盘绒毛尿囊膜（CAM）模型在血管生成研究中的广泛应用，使得准确的血管分布评估变得至关重要。然而，现有方法存在手动分割耗时、主观性强以及缺乏公开数据集等问题。为解决这些问题，作者提出了一种新颖的方法——Intermediate Domain-guided Adaptation (IDA)，它利用了CAM图像与视网膜图像之间的相似性及现有的公共视网膜数据集进行无监督训练，通过Multi-Resolution Asymmetric Translation（MRAT）策略生成中间图象以增强图象级交互，并开发了一个Intermediate Domain-guided Contrastive Learning（IDCL）模块来分离跨领域特征表示。该方法克服了现有无监督领域适应技术的局限性，在CAM数据集上验证并取得了超越其他方法的表现，同时在视网膜数据集中也展现了强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The chorioallantoic membrane (CAM) model is widely employed in angiogenesisresearch, and distribution of growing blood vessels is the key evaluationindicator. As a result, vessel segmentation is crucial for quantitativeassessment based on topology and morphology. However, manual segmentation isextremely time-consuming, labor-intensive, and prone to inconsistency due toits subjective nature. Moreover, research on CAM vessel segmentation algorithmsremains limited, and the lack of public datasets contributes to poor predictionperformance. To address these challenges, we propose an innovative IntermediateDomain-guided Adaptation (IDA) method, which utilizes the similarity betweenCAM images and retinal images, along with existing public retinal datasets, toperform unsupervised training on CAM images. Specifically, we introduce aMulti-Resolution Asymmetric Translation (MRAT) strategy to generateintermediate images to promote image-level interaction. Then, an IntermediateDomain-guided Contrastive Learning (IDCL) module is developed to disentanglecross-domain feature representations. This method overcomes the limitations ofexisting unsupervised domain adaptation (UDA) approaches, which primarilyconcentrate on directly source-target alignment while neglecting intermediatedomain information. Notably, we create the first CAM dataset to validate theproposed algorithm. Extensive experiments on this dataset show that our methodoutperforms compared approaches. Moreover, it achieves superior performance inUDA tasks across retinal datasets, highlighting its strong generalizationcapability. The CAM dataset and source codes are available athttps://github.com/Light-47/IDA.</description>
      <author>example@mail.com (Pengwu Song, Liang Xu, Peng Yao, Shuwei Shen, Pengfei Shao, Mingzhai Sun, Ronald X. Xu)</author>
      <guid isPermaLink="false">2503.03546v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Biological Sequence with Language Model Prompting: A Survey</title>
      <link>http://arxiv.org/abs/2503.04135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文系统地研究了在生物序列分析中使用基于提示的大语言模型的方法，涵盖了DNA、RNA、蛋白质和药物发现等任务。重点探讨了如何通过精心设计的提示工程来克服领域特定问题，并强调了这种方法在未来生物信息学中的潜在转变能力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）在解决跨不同领域的挑战方面表现出强大的工具性，特别是在增强生物分子分析与合成效率方面得到了广泛学术界和医学界的关注。&lt;h4&gt;目的&lt;/h4&gt;本文旨在系统地调查基于提示的方法如何应用于生物序列的分析，并探讨这些方法在未来生物信息学中的潜力。&lt;h4&gt;方法&lt;/h4&gt;研究集中于大语言模型在生物序列（如DNA、RNA、蛋白质）和药物发现任务中应用，特别是利用精心设计的提示工程技术来解决特定领域的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;基于提示的大语言模型可以显著提高诸如启动子序列预测、蛋白质结构建模以及药物与靶点结合亲和力预测等任务的表现，并且在标签数据有限的情况下尤其有效。此外，这些方法展示了在生物信息学领域中的变革潜力，尤其是在处理数据稀缺性、多模态融合和计算资源限制方面。&lt;h4&gt;结论&lt;/h4&gt;论文希望成为这一快速发展的研究领域的入门级文献，同时也作为推动进一步创新的催化剂。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）已经成为解决跨不同领域的挑战的强大工具。特别是在增强生物分子分析与合成效率方面受到了学术界和医学界的广泛关注。本文系统地探讨了基于提示的方法在生物序列包括DNA、RNA、蛋白质以及药物发现任务中的应用，重点在于如何通过精心设计的提示工程技术来应对领域特定问题，并强调这种方法在未来生物信息学领域的变革潜力。这些方法尤其有效于标签数据有限的情况，并且展示了处理诸如多模态融合及计算资源限制等关键挑战的能力。论文旨在成为新手入门文献的同时也作为推动该领域创新的重要催化剂。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language models (LLMs) have emerged as powerful tools for addressingchallenges across diverse domains. Notably, recent studies have demonstratedthat large language models significantly enhance the efficiency of biomolecularanalysis and synthesis, attracting widespread attention from academics andmedicine. In this paper, we systematically investigate the application ofprompt-based methods with LLMs to biological sequences, including DNA, RNA,proteins, and drug discovery tasks. Specifically, we focus on how promptengineering enables LLMs to tackle domain-specific problems, such as promotersequence prediction, protein structure modeling, and drug-target bindingaffinity prediction, often with limited labeled data. Furthermore, ourdiscussion highlights the transformative potential of prompting inbioinformatics while addressing key challenges such as data scarcity,multimodal fusion, and computational resource limitations. Our aim is for thispaper to function both as a foundational primer for newcomers and a catalystfor continued innovation within this dynamic field of study.</description>
      <author>example@mail.com (Jiyue Jiang, Zikang Wang, Yuheng Shan, Heyan Chai, Jiayi Li, Zixian Ma, Xinrui Zhang, Yu Li)</author>
      <guid isPermaLink="false">2503.04135v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing SAM with Efficient Prompting and Preference Optimization for Semi-supervised Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2503.04639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究介绍了一种增强的Segment Anything Model (SAM)框架，旨在通过无监督生成的高效标注提示来改进医疗图像分割。该方法利用对比语言-图像预训练和视觉问答捕获关键信息，并采用直接偏好优化技术设计策略以提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型如Segment Anything Model（SAM）在医学成像领域取得了进展，支持多种下游任务。然而，这些模型依赖于大型标注数据集或专家提供的提示。&lt;h4&gt;目的&lt;/h4&gt;提出一个增强的SAM框架来解决现有方法对大规模标注数据和复杂专业知识的高度依赖问题。&lt;h4&gt;方法&lt;/h4&gt;使用无监督生成的高效注释提示、对比语言图像预训练以及视觉问答技术。采用直接偏好优化技术，通过虚拟标注器模拟的人类注解过程提供简单的评分或排名以指导模型学习。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在低标注数据情况下表现良好，在肺部分割、乳腺肿瘤分割和跨模态器官分割等任务上达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的增强SAM框架能够利用少量的标注信息生成高质量的分割结果，减少了对大量专家指导的需求，并为医学影像分析提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;基础模型如Segment Anything Model（SAM）在医疗图像分割中越来越受欢迎，支持多种下游任务。然而，这些模型本质上是监督型的，仍然依赖于大型注释数据集或由专家提供的提示。传统的技术如主动学习虽能缓解这一问题但仍有局限性，并且仍需要持续的人工参与和复杂的领域知识来改进标签或确定奖励标准的真实情况。为了应对这些挑战，我们提出了一种增强的SAM框架，该框架利用完全无监督生成的高效注释提示，同时通过对比语言-图像预训练和视觉问答捕捉关键语义、位置及形状信息。采用直接偏好优化技术设计最佳策略使模型能够在虚拟标注器提供的简单评分或排名指导下产生高保真度分割结果。我们的框架在肺部分割、乳腺肿瘤分割以及跨模态成像中的器官分割等任务中取得了最先进的表现，证明了其在低注释数据场景下的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundational models such as the Segment Anything Model (SAM) are gainingtraction in medical imaging segmentation, supporting multiple downstream tasks.However, such models are supervised in nature, still relying on large annotateddatasets or prompts supplied by experts. Conventional techniques such as activelearning to alleviate such limitations are limited in scope and stillnecessitate continuous human involvement and complex domain knowledge for labelrefinement or establishing reward ground truth. To address these challenges, wepropose an enhanced Segment Anything Model (SAM) framework that utilizesannotation-efficient prompts generated in a fully unsupervised fashion, whilestill capturing essential semantic, location, and shape information throughcontrastive language-image pretraining and visual question answering. We adoptthe direct preference optimization technique to design an optimal policy thatenables the model to generate high-fidelity segmentations with simple ratingsor rankings provided by a virtual annotator simulating the human annotationprocess. State-of-the-art performance of our framework in tasks such as lungsegmentation, breast tumor segmentation, and organ segmentation across variousmodalities, including X-ray, ultrasound, and abdominal CT, justifies itseffectiveness in low-annotation data scenarios.</description>
      <author>example@mail.com (Aishik Konwer, Zhijian Yang, Erhan Bas, Cao Xiao, Prateek Prasanna, Parminder Bhatia, Taha Kass-Hout)</author>
      <guid isPermaLink="false">2503.04639v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search</title>
      <link>http://arxiv.org/abs/2503.04412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear at ICLR 2025 Workshop on Foundation Models in the Wild&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的推理框架AB-MCTS，在复杂编码和工程任务上通过外部反馈信号进行多轮探索与优化，显著优于重复抽样和标准MCTS。&lt;h4&gt;背景&lt;/h4&gt;研究表明增加推理时间的计算可以提高大规模语言模型（LLMs）的推理能力。尽管多次采样是一种有效策略，但其不利用外部反馈信号进行细化改进。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的推理框架AB-MCTS，在复杂任务中结合大规模语言模型的响应多样性与多轮解决方案优化的能力。&lt;h4&gt;方法&lt;/h4&gt;提出适应性分支蒙特卡洛树搜索（AB-MCTS）框架。该框架在每次搜索节点上根据外部反馈信号动态决定是否扩展新候选输出或重新审视现有输出，以实现更有效的推理时间缩放。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示AB-MCTS在复杂编码和工程任务中始终优于重复抽样和标准MCTS方法。&lt;h4&gt;结论&lt;/h4&gt;结合大规模语言模型的响应多样性与多轮解决方案优化是有效扩大推理时间规模的关键。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，增加推断时的计算量可以显著提升大语言模型（LLMs）的推理能力。虽然重复采样是一种非常有效的策略，但它没有利用外部反馈信号进行细化改进，而这些在编码等任务中往往可用。本文提出了一种新的推断框架——自适应分支蒙特卡洛树搜索(AB-MCTS)，它能够根据外部反馈信号，在每次搜索节点上动态决定是否扩展新候选输出或重新审视现有输出，从而实现更有效的推理时间缩放。实验结果显示，AB-MCTS在复杂编码和工程任务中始终优于重复抽样和标准MCTS方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances demonstrate that increasing inference-time computation cansignificantly boost the reasoning capabilities of large language models (LLMs).Although repeated sampling (i.e., generating multiple candidate outputs) is ahighly effective strategy, it does not leverage external feedback signals forrefinement, which are often available in tasks like coding. In this work, wepropose $\textit{Adaptive Branching Monte Carlo Tree Search (AB-MCTS)}$, anovel inference-time framework that generalizes repeated sampling withprincipled multi-turn exploration and exploitation. At each node in the searchtree, AB-MCTS dynamically decides whether to "go wider" by expanding newcandidate responses or "go deeper" by revisiting existing ones based onexternal feedback signals. We evaluate our method on complex coding andengineering tasks using frontier models. Empirical results show that AB-MCTSconsistently outperforms both repeated sampling and standard MCTS, underscoringthe importance of combining the response diversity of LLMs with multi-turnsolution refinement for effective inference-time scaling.</description>
      <author>example@mail.com (Kou Misaki, Yuichi Inoue, Yuki Imajuku, So Kuroki, Taishi Nakamura, Takuya Akiba)</author>
      <guid isPermaLink="false">2503.04412v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>A Phylogenetic Approach to Genomic Language Modeling</title>
      <link>http://arxiv.org/abs/2503.03773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;基因组语言模型(gLMs)在识别哺乳动物基因组中的进化保守元件方面取得了一些适度的成功。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的训练gLM的框架，该框架使用多物种全基因组比对来显式建模核苷酸进化。&lt;h4&gt;方法&lt;/h4&gt;将对齐数据整合到损失函数中用于模型训练，但预测时不需要对齐数据。&lt;h4&gt;主要发现&lt;/h4&gt;应用此框架训练得到的PhyloGPN模型在仅用单个序列的情况下能够出色地预测功能破坏性变异，并表现出强大的迁移学习能力。&lt;h4&gt;翻译&lt;/h4&gt;基因组语言模型(gLMs)在识别哺乳动物基因组中的进化保守元件方面取得了一些适度的成功。为了应对这一挑战，我们引入了一个新的训练gLM的框架，该框架使用多物种全基因组比对来显式建模核苷酸进化。我们的方法将对齐数据整合到损失函数中用于模型训练，但预测时不需要对齐数据，从而增强了模型的应用性。我们将此框架应用于训练PhyloGPN模型，该模型在仅用单个序列的情况下能够出色地预测功能破坏性变异，并表现出强大的迁移学习能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Genomic language models (gLMs) have shown mostly modest success inidentifying evolutionarily constrained elements in mammalian genomes. Toaddress this issue, we introduce a novel framework for training gLMs thatexplicitly models nucleotide evolution on phylogenetic trees using multispecieswhole-genome alignments. Our approach integrates an alignment into the lossfunction during training but does not require it for making predictions,thereby enhancing the model's applicability. We applied this framework to trainPhyloGPN, a model that excels at predicting functionally disruptive variantsfrom a single sequence alone and demonstrates strong transfer learningcapabilities.</description>
      <author>example@mail.com (Carlos Albors, Jianan Canal Li, Gonzalo Benegas, Chengzhong Ye, Yun S. Song)</author>
      <guid isPermaLink="false">2503.03773v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>A Generalist Cross-Domain Molecular Learning Framework for Structure-Based Drug Discovery</title>
      <link>http://arxiv.org/abs/2503.04362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种名为BIT（Biomolecular Interaction Transformer）的通用基础模型，该模型能够编码包括小分子、蛋白质和蛋白质-配体复合物在内的多种生物化学实体及其2D和3D结构。&lt;h4&gt;背景&lt;/h4&gt;基于结构药物发现(SBDD)利用目标蛋白的详细物理结构来开发新药。最近，在生物分子预训练模型上的进展在药物发现等多个生化应用中取得了显著成功，但大多数方法主要关注小分子或蛋白质的特性而忽略了它们之间的结合相互作用。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，提出了一种能够编码多类型生物化学实体并捕捉其精细相互作用的通用基础模型BIT。&lt;h4&gt;方法&lt;/h4&gt;BIT通过引入Mixture-of-Domain-Experts (MoDE)处理来自不同生化领域的生物分子，并使用Mixture-of-Structure-Experts (MoSE)来捕获分子结构中的位置依赖性。此外，通过对共享Transformer骨干进行跨域预训练和统一的自我监督去噪任务进一步优化。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在各种基准测试中，BIT在下游任务如结合亲和力预测、基于结构的虚拟筛选及分子属性预测方面表现优异。&lt;h4&gt;结论&lt;/h4&gt;BIT展示了其卓越的能力，能够有效解决SBDD领域中的关键问题，并为药物开发提供了强有力的工具。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要详细描述了用于生物分子相互作用的通用基础模型BIT的创建和应用。此模型利用预训练技术处理多类型生物化学实体并捕捉它们之间的复杂关系，在多种生化任务中表现出色，证明其在基于结构的新药研发中的潜在价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Structure-based drug discovery (SBDD) is a systematic scientific process thatdevelops new drugs by leveraging the detailed physical structure of the targetprotein. Recent advancements in pre-trained models for biomolecules havedemonstrated remarkable success across various biochemical applications,including drug discovery and protein engineering. However, in most approaches,the pre-trained models primarily focus on the characteristics of either smallmolecules or proteins, without delving into their binding interactions whichare essential cross-domain relationships pivotal to SBDD. To fill this gap, wepropose a general-purpose foundation model named BIT (an abbreviation forBiomolecular Interaction Transformer), which is capable of encoding a range ofbiochemical entities, including small molecules, proteins, and protein-ligandcomplexes, as well as various data formats, encompassing both 2D and 3Dstructures. Specifically, we introduce Mixture-of-Domain-Experts (MoDE) tohandle the biomolecules from diverse biochemical domains andMixture-of-Structure-Experts (MoSE) to capture positional dependencies in themolecular structures. The proposed mixture-of-experts approach enables BIT toachieve both deep fusion and domain-specific encoding, effectively capturingfine-grained molecular interactions within protein-ligand complexes. Then, weperform cross-domain pre-training on the shared Transformer backbone viaseveral unified self-supervised denoising tasks. Experimental results onvarious benchmarks demonstrate that BIT achieves exceptional performance indownstream tasks, including binding affinity prediction, structure-basedvirtual screening, and molecular property prediction.</description>
      <author>example@mail.com (Yiheng Zhu, Mingyang Li, Junlong Liu, Kun Fu, Jiansheng Wu, Qiuyi Li, Mingze Yin, Jieping Ye, Jian Wu, Zheng Wang)</author>
      <guid isPermaLink="false">2503.04362v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>scDD: Latent Codes Based scRNA-seq Dataset Distillation with Foundation Model Knowledge</title>
      <link>http://arxiv.org/abs/2503.04357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;单细胞RNA测序（scRNA-seq）技术已经对数以亿计的人类细胞进行了分析，这些细胞涵盖了不同的器官、疾病状态和发展阶段。然而，原始的测序数据面临着高维稀疏性、批次效应噪声、类别不平衡以及数据规模不断增加等问题。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些问题，提出了一种方法来简化和优化scRNA-seq数据集处理。&lt;h4&gt;方法&lt;/h4&gt;{'第一部分': '提出了一个基于潜在代码的单细胞测序数据集精简框架（scDD），该框架将基础模型的知识和原始数据集信息转化到紧凑的潜在空间中，并通过生成器生成合成的scRNA-seq数据集以替代原有数据。', '第二部分': '提出了一种一步条件扩散生成器（SCDG），它通过单步反向传播来优化数据精简质量，避免了多步反向传播导致的梯度衰减。同时，SCDG保证了合成数据集中scRNA-seq数据特性和类间区分性。', '评价': '提出了一个全面基准用于评估不同数据分析任务中scRNA-seq数据集精简的效果。'}&lt;h4&gt;主要发现&lt;/h4&gt;所提出的这种方法在平均任务上比现有最优方法有7.61%的绝对改进和15.70%的相对改进。&lt;h4&gt;结论&lt;/h4&gt;新提出的方法能够有效地解决多中心知识传递、数据融合及scRNA-seq数据集之间的交叉验证问题，同时提高了合成数据的质量。&lt;h4&gt;翻译&lt;/h4&gt;单细胞RNA测序（scRNA-seq）技术已经对数以亿计的人类细胞进行了分析……所提出的这种方法在平均任务上比现有最优方法有7.61%的绝对改进和15.70%的相对改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell RNA sequencing (scRNA-seq) technology has profiled hundreds ofmillions of human cells across organs, diseases, development and perturbationsto date. However, the high-dimensional sparsity, batch effect noise, categoryimbalance, and ever-increasing data scale of the original sequencing data posesignificant challenges for multi-center knowledge transfer, data fusion, andcross-validation between scRNA-seq datasets. To address these barriers, (1) wefirst propose a latent codes-based scRNA-seq dataset distillation frameworknamed scDD, which transfers and distills foundation model knowledge andoriginal dataset information into a compact latent space and generatessynthetic scRNA-seq dataset by a generator to replace the original dataset.Then, (2) we propose a single-step conditional diffusion generator named SCDG,which perform single-step gradient back-propagation to help scDD optimizedistillation quality and avoid gradient decay caused by multi-stepback-propagation. Meanwhile, SCDG ensures the scRNA-seq data characteristicsand inter-class discriminability of the synthetic dataset through flexibleconditional control and generation quality assurance. Finally, we propose acomprehensive benchmark to evaluate the performance of scRNA-seq datasetdistillation in different data analysis tasks. It is validated that ourproposed method can achieve 7.61% absolute and 15.70% relative improvement overprevious state-of-the-art methods on average task.</description>
      <author>example@mail.com (Zhen Yu, Jianan Han, Yang Liu, Qingchao Chen)</author>
      <guid isPermaLink="false">2503.04357v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>An Egocentric Vision-Language Model based Portable Real-time Smart Assistant</title>
      <link>http://arxiv.org/abs/2503.04250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;我们介绍了Vinci，这是一个为便携设备设计的实时、全面的人工智能辅助系统。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉-语言系统通常依赖于特定硬件，并且难以实现实时功能和长时间视频流处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在多种设备上运行、无硬件限制并且能够提供高级功能如场景理解、时间定位等的便携式实时AI系统。&lt;h4&gt;方法&lt;/h4&gt;Vinci的核心是EgoVideo-VL模型，该模型结合了第一人称视角视觉基础模型和大型语言模型（LLM），并配备了记忆模块、生成模块和检索模块来增强其实用性。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公开基准测试中展示了EgoVideo-VL的优越性能，并通过用户研究验证了Vinci的实际效果，突出了它的适应性和可用性。&lt;h4&gt;结论&lt;/h4&gt;希望Vinci能为便携式实时第一人称视角AI系统建立新的框架，提供上下文相关的、可操作的见解。&lt;h4&gt;翻译&lt;/h4&gt;摘要：我们提出了一种名为Vinci的视觉语言系统，旨在为移动设备提供实时全面的人工智能辅助。该系统的中心是EgoVideo-VL模型，它结合了第一人称视角视觉基础模型和大型语言模型（LLM），提供了场景理解、时间定位、视频总结和未来规划等高级功能。为了提高其实用性，Vinci集成了一个记忆模块，用于实时处理长视频流并保留上下文历史；生成模块用来产生可视动作演示；检索模块则通过连接第一人称视角与第三人称视角提供相关技能学习视频。不同于依赖于特定硬件的现有系统，Vinci是无硬件限制的，支持在包括智能手机和可穿戴相机在内的各种设备上部署。我们首先展示了EgoVideo-VL模型在多个公开基准测试中的优越性能，突出了其视觉语言推理和上下文理解能力；然后通过一系列用户研究评估了Vinci的实际效果，强调了它的适应性和可用性。我们的目标是使Vinci成为一个新的便携式实时第一人称视角AI系统框架，赋予用户提供上下文相关且可操作见解的能力。包括前端、后端和模型在内的所有代码都可以在https://github.com/OpenGVLab/vinci上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Vinci, a vision-language system designed to provide real-time,comprehensive AI assistance on portable devices. At its core, Vinci leveragesEgoVideo-VL, a novel model that integrates an egocentric vision foundationmodel with a large language model (LLM), enabling advanced functionalities suchas scene understanding, temporal grounding, video summarization, and futureplanning. To enhance its utility, Vinci incorporates a memory module forprocessing long video streams in real time while retaining contextual history,a generation module for producing visual action demonstrations, and a retrievalmodule that bridges egocentric and third-person perspectives to providerelevant how-to videos for skill acquisition. Unlike existing systems thatoften depend on specialized hardware, Vinci is hardware-agnostic, supportingdeployment across a wide range of devices, including smartphones and wearablecameras. In our experiments, we first demonstrate the superior performance ofEgoVideo-VL on multiple public benchmarks, showcasing its vision-languagereasoning and contextual understanding capabilities. We then conduct a seriesof user studies to evaluate the real-world effectiveness of Vinci, highlightingits adaptability and usability in diverse scenarios. We hope Vinci canestablish a new framework for portable, real-time egocentric AI systems,empowering users with contextual and actionable insights. Including thefrontend, backend, and models, all codes of Vinci are available athttps://github.com/OpenGVLab/vinci.</description>
      <author>example@mail.com (Yifei Huang, Jilan Xu, Baoqi Pei, Yuping He, Guo Chen, Mingfang Zhang, Lijin Yang, Zheng Nie, Jinyao Liu, Guoshun Fan, Dechen Lin, Fang Fang, Kunpeng Li, Chang Yuan, Xinyuan Chen, Yaohui Wang, Yali Wang, Yu Qiao, Limin Wang)</author>
      <guid isPermaLink="false">2503.04250v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>DuCos: Duality Constrained Depth Super-Resolution via Foundation Model</title>
      <link>http://arxiv.org/abs/2503.04171v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;介绍了DuCos，一种基于拉格朗日对偶理论的深度超分辨率框架，通过灵活集成多种约束和重建目标来提高准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;当前深度超分辨率方法在处理多样化场景时存在泛化能力不足的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个新颖的方法——DuCos，旨在利用基础模型作为提示信息，显著提升不同场景下的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;DuCos框架设计了两个关键组件：相关融合（CF）和梯度调节（GR），用于实现精细的几何对齐、有效融合深度特征与提示特征，并通过拉格朗日约束项无缝嵌入这些提示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，相比于现有的最先进方法，DuCos在精度、鲁棒性和泛化性方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;杜科斯（DuCos）框架为深度超分辨率提供了一个强大的新视角，并将在未来的研究中发挥重要作用。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了基于拉格朗日对偶理论的新型深度超分辨率框架——杜科斯，通过灵活集成多种约束和重建目标来增强准确性和鲁棒性。该方法是首个显著提高基础模型作为提示信息时在多样化场景中的泛化能力的方法。提示设计包含相关融合（CF）与梯度调节（GR），前者促进了精细的几何对齐及深度特征与提示的有效融合，后者通过要求其深度预测与从基础模型衍生出的边缘清晰的地图保持一致来优化深度预测。重要的是，这些提示被无缝嵌入到拉格朗日约束项中，形成了协同且原理明确的框架。广泛实验表明，杜科斯超越了现有的最先进方法，在精度、鲁棒性和泛化性方面均表现优异。源代码及预训练模型将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce DuCos, a novel depth super-resolution framework grounded inLagrangian duality theory, offering a flexible integration of multipleconstraints and reconstruction objectives to enhance accuracy and robustness.Our DuCos is the first to significantly improve generalization across diversescenarios with foundation models as prompts. The prompt design consists of twokey components: Correlative Fusion (CF) and Gradient Regulation (GR). CFfacilitates precise geometric alignment and effective fusion between prompt anddepth features, while GR refines depth predictions by enforcing consistencywith sharp-edged depth maps derived from foundation models. Crucially, theseprompts are seamlessly embedded into the Lagrangian constraint term, forming asynergistic and principled framework. Extensive experiments demonstrate thatDuCos outperforms existing state-of-the-art methods, achieving superioraccuracy, robustness, and generalization. The source codes and pre-trainedmodels will be publicly available.</description>
      <author>example@mail.com (Zhiqiang Yan, Zhengxue Wang, Haoye Dong, Jun Li, Jian Yang, Gim Hee Lee)</author>
      <guid isPermaLink="false">2503.04171v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>VLA Model-Expert Collaboration for Bi-directional Manipulation Learning</title>
      <link>http://arxiv.org/abs/2503.04163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个视觉-语言-行动模型与专家协作的框架，该框架通过引入少量的专家操作来提高VLA模型在多任务操纵中的性能和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉-语言-行动（VLA）模型虽然已经取得了一些进展，但在机器人操纵的多个任务上仍存在泛化能力不足的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的协作框架，以提升VLA模型的能力，并减轻专家的工作负担。&lt;h4&gt;方法&lt;/h4&gt;该研究设计了一个VLA模型和专家之间的协作框架，利用有限数量的专家动作来改进VLA模型的表现。此过程还收集了用于进一步训练VLA模型的数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在多种VLA模型上应用所提出的系统可以显著提高操作的成功率，并且通过脑机接口（BCI）验证表明该协作框架能够提升低速行动系统的效率。&lt;h4&gt;结论&lt;/h4&gt;这种双向学习循环增强了协作系统的整体性能，为基于基础模型的人机交互开辟了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-行动(VLA) 模型的出现催生了机器人操作的基础模型。尽管这些模型已经取得了显著的进步，但在多任务操纵中的泛化能力仍然有限。这项研究提出了一种VLA模型与专家协作框架，利用少量的专家动作来增强VLA模型的表现。这种方法在减少手动操作工作量的同时提高了VLA模型的可靠性和泛化性，并且收集到的操作数据可以进一步细化VLA模型。同时，人类参与者在此过程中也提升了他们的技能。这种双向学习循环推动了整个协作系统的性能提升。实验结果显示，在各种VLA模型上的协同操纵和学习中所提出的系统表现出色，任务成功率显著提高。此外，使用脑机接口(BCI)验证表明该合作系统通过引入VLA模型在操作期间提高了低速动作系统的效率。这些有前景的结果为机器人基础模型时代的人机交互开辟了新的途径。(项目网站: https://aoqunjin.github.io/Expert-VLA/)&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of vision-language-action (VLA) models has given rise tofoundation models for robot manipulation. Although these models have achievedsignificant improvements, their generalization in multi-task manipulationremains limited. This study proposes a VLA model-expert collaboration frameworkthat leverages a limited number of expert actions to enhance VLA modelperformance. This approach reduces expert workload relative to manual operationwhile simultaneously improving the reliability and generalization of VLAmodels. Furthermore, manipulation data collected during collaboration canfurther refine the VLA model, while human participants concurrently enhancetheir skills. This bi-directional learning loop boosts the overall performanceof the collaboration system. Experimental results across various VLA modelsdemonstrate the effectiveness of the proposed system in collaborativemanipulation and learning, as evidenced by improved success rates across tasks.Additionally, validation using a brain-computer interface (BCI) indicates thatthe collaboration system enhances the efficiency of low-speed action systems byinvolving VLA model during manipulation. These promising results pave the wayfor advancing human-robot interaction in the era of foundation models forrobotics. (Project website: https://aoqunjin.github.io/Expert-VLA/)</description>
      <author>example@mail.com (Tian-Yu Xiang, Ao-Qun Jin, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Sheng-Bin Duang, Si-Cheng Wang, Zheng Lei, Zeng-Guang Hou)</author>
      <guid isPermaLink="false">2503.04163v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Rebalanced Multimodal Learning with Data-aware Unimodal Sampling</title>
      <link>http://arxiv.org/abs/2503.03792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;针对多模态学习过程中由于数据采样导致的模式不平衡问题，本文提出了一种新的方法——Data-aware Unimodal Sampling (DUS)，该方法通过动态调整每一轮迭代中采样的数据量来缓解模式不平衡。&lt;h4&gt;背景&lt;/h4&gt;现有的多模态学习(MML)方法主要从模型学习的角度尝试平衡每个模式的优化过程。然而，几乎所有现有方法忽略了由单模态数据采样引起的模式不平衡问题，即等比例采样通常会导致信息内容上的差异。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的MML方法，通过动态缓解采样过程中产生的多模态不平衡来改进现有的学习策略。&lt;h4&gt;方法&lt;/h4&gt;引入了一种累积模式偏差的方法来监控多模态学习过程，并基于此提出了启发式和基于强化学习的数据感知单模态采样方法以自适应地决定每次迭代中采样的数据量。同时，该方法可以无缝集成到几乎所有的现有MML方法中。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明DUS相对于各种先进的基线方法，在性能上取得了最佳表现。&lt;h4&gt;结论&lt;/h4&gt;通过调整单模态数据的采样策略以缓解多模态不平衡问题，本文提出的方法能够显著提高多模态学习的整体效果。&lt;h4&gt;翻译&lt;/h4&gt;为了解决由于模式失衡引起的数据模态学习退化问题，现有的多模态学习方法主要试图从模型学习的角度平衡每个模态的优化过程。然而，几乎所有现有方法都忽略了由单模态数据采样导致的模式不平衡问题，即等比例采样通常会导致信息内容上的差异，从而造成模态失衡。因此，在本文中，我们提出了一种新的多模态学习方法——Data-aware Unimodal Sampling (DUS)，旨在动态缓解由于抽样引起的模式不平衡问题。具体而言，我们首先提出了一个新的累积模式偏差来监控多模态学习过程。根据学习状态，我们基于启发式和强化学习（RL）的数据感知单模态采样策略自适应地确定每次迭代中抽取数据的数量，从而从采样的角度缓解模式失衡。同时，我们的方法可以无缝集成到几乎所有的现有多模态学习方法中作为插件。实验表明DUS通过与各种先进的基线进行比较，在性能上取得了最佳表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To address the modality learning degeneration caused by modality imbalance,existing multimodal learning~(MML) approaches primarily attempt to balance theoptimization process of each modality from the perspective of model learning.However, almost all existing methods ignore the modality imbalance caused byunimodal data sampling, i.e., equal unimodal data sampling often results indiscrepancies in informational content, leading to modality imbalance.Therefore, in this paper, we propose a novel MML approach called\underline{D}ata-aware \underline{U}nimodal \underline{S}ampling~(\method),which aims to dynamically alleviate the modality imbalance caused by sampling.Specifically, we first propose a novel cumulative modality discrepancy tomonitor the multimodal learning process. Based on the learning status, wepropose a heuristic and a reinforcement learning~(RL)-based data-aware unimodalsampling approaches to adaptively determine the quantity of sampled data ateach iteration, thus alleviating the modality imbalance from the perspective ofsampling. Meanwhile, our method can be seamlessly incorporated into almost allexisting multimodal learning approaches as a plugin. Experiments demonstratethat \method~can achieve the best performance by comparing with diversestate-of-the-art~(SOTA) baselines.</description>
      <author>example@mail.com (Qingyuan Jiang, Zhouyang Chi, Xiao Ma, Qirong Mao, Yang Yang, Jinhui Tang)</author>
      <guid isPermaLink="false">2503.03792v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering</title>
      <link>http://arxiv.org/abs/2503.03190v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;3D问答（3D QA）需要模型全面理解由文本描述的所处三维场景，然后在该情况下对其周围环境进行推理并回答问题。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通常依赖于从纯3D点云中的全局场景感知，并忽视了来自多视角图像的丰富局部纹理细节的重要性。此外，在将3D点云与多视图图像对齐时，由于相机姿态的内在噪声和复杂的遮挡，存在显著的特征退化和降低的特征鲁棒性问题。&lt;h4&gt;目的&lt;/h4&gt;本文提出了一个双视觉场景感知网络（DSPNet），以全面整合多视角和点云特征来提高3D QA中的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;我们的文本引导的多视图融合（TGMF）模块优先考虑与文本语义内容紧密匹配的图像视图。为了自适应地融合反向投影的多视角图像与点云特征，我们设计了自适应双视觉感知（ADVP）模块，增强三维场景的理解能力。此外，我们的跨模态上下文引导推理（MCGR）模块通过整合视觉和语言模式中的上下文信息来促进稳健推理。&lt;h4&gt;主要发现&lt;/h4&gt;在SQA3D和ScanQA数据集上的实验结果证明了我们DSPNet的优越性。&lt;h4&gt;结论&lt;/h4&gt;代码将在https://github.com/LZ-CH/DSPNet上提供。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的方法来解决现有的3D QA模型中存在的问题，即忽视多视角图像中的局部纹理细节和在将点云与图像对齐时的特征鲁棒性问题。通过设计TGMF、ADVP和MCGR模块，我们的DSPNet能够在3D QA任务中表现出色，并提供了代码以供其他研究人员使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/LZ-CH/DSPNet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Question Answering (3D QA) requires the model to comprehensivelyunderstand its situated 3D scene described by the text, then reason about itssurrounding environment and answer a question under that situation. However,existing methods usually rely on global scene perception from pure 3D pointclouds and overlook the importance of rich local texture details frommulti-view images. Moreover, due to the inherent noise in camera poses andcomplex occlusions, there exists significant feature degradation and reducedfeature robustness problems when aligning 3D point cloud with multi-viewimages. In this paper, we propose a Dual-vision Scene Perception Network(DSPNet), to comprehensively integrate multi-view and point cloud features toimprove robustness in 3D QA. Our Text-guided Multi-view Fusion (TGMF) moduleprioritizes image views that closely match the semantic content of the text. Toadaptively fuse back-projected multi-view images with point cloud features, wedesign the Adaptive Dual-vision Perception (ADVP) module, enhancing 3D scenecomprehension. Additionally, our Multimodal Context-guided Reasoning (MCGR)module facilitates robust reasoning by integrating contextual informationacross visual and linguistic modalities. Experimental results on SQA3D andScanQA datasets demonstrate the superiority of our DSPNet. Codes will beavailable at https://github.com/LZ-CH/DSPNet.</description>
      <author>example@mail.com (Jingzhou Luo, Yang Liu, Weixing Chen, Zhen Li, Yaowei Wang, Guanbin Li, Liang Lin)</author>
      <guid isPermaLink="false">2503.03190v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>TimeFound: A Foundation Model for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2503.04118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;TimeFound是一个基于编码器-解码器的变压器模型，专门用于零样本时间序列预测。&lt;h4&gt;背景&lt;/h4&gt;当前时间序列数据来自不同的领域，需要一种能捕捉各种复杂模式的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理不同领域的时间序列数据并进行零样本预测的基础模型TimeFound。&lt;h4&gt;方法&lt;/h4&gt;采用多分辨率分块策略来适应不同规模的数据，并在大型时序语料库上进行预训练。该语料库包括现实世界和合成时间序列数据，以两种参数大小（200M和710M）的模型进行了预训练。&lt;h4&gt;主要发现&lt;/h4&gt;TimeFound在多个领域和不同的预测范围上的未见数据集上取得了优于或与现有最佳的时间序列基础模型相当的零样本预测性能。&lt;h4&gt;结论&lt;/h4&gt;TimeFound展示了其在处理多样时间序列数据时的有效性和广泛适用性，是未来研究的一个有希望的方向。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了TimeFound，这是一种基于编码器-解码器变压器的时间系列基础模型，用于即插即用的零样本预测。为了应对不同领域的时间序列数据，TimeFound采用了一种多分辨率分块策略来捕捉多个时间尺度上的复杂模式。我们在一个包含真实世界和合成数据集的大规模时间序列语料库上对我们的模型进行了两种大小（200M和710M参数）的预训练。在一系列跨不同领域的未见数据集中，我们通过实验发现TimeFound可以实现优于或与最先进的时间序列基础模型相当的零样本预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present TimeFound, an encoder-decoder transformer-based time seriesfoundation model for out-of-the-box zero-shot forecasting. To handle timeseries data from various domains, TimeFound employs a multi-resolution patchingstrategy to capture complex temporal patterns at multiple scales. We pre-trainour model with two sizes (200M and 710M parameters) on a large time-seriescorpus comprising both real-world and synthetic datasets. Over a collection ofunseen datasets across diverse domains and forecasting horizons, our empiricalevaluations suggest that TimeFound can achieve superior or competitivezero-shot forecasting performance, compared to state-of-the-art time seriesfoundation models.</description>
      <author>example@mail.com (Congxi Xiao, Jingbo Zhou, Yixiong Xiao, Xinjiang Lu, Le Zhang, Hui Xiong)</author>
      <guid isPermaLink="false">2503.04118v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>NodeNAS: Node-Specific Graph Neural Architecture Search for Out-of-Distribution Generalization</title>
      <link>http://arxiv.org/abs/2503.02448v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by DASFAA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了NodeNAS和MNNAS两种新颖的方法，旨在通过解耦节点拓扑结构和图分布来为不同类型的节点定制独特的聚合方法，解决现有GraphNAS方法在少量或单一训练图上的泛化能力差的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的GraphNAS方法未能考虑到不同类型图之间的分布模式，并且依赖大量的训练数据才能有效发现最优的架构映射关系。这使得它们难以处理稀疏或者单个样本的数据集，尤其是在面对分布外（OOD）数据时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出NodeNAS和MNNAS两种方法来解决上述问题，以提高图神经网络在有限或独特训练数据上的性能，并增强其对OOD数据的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;1. NodeNAS：通过解耦节点拓扑结构与图分布，为不同类型的节点设计独特的聚合策略。2. MNNAS：引入了适应性聚集注意机制，学习一种具有良好泛化的节点特定架构定制器。该模型还扩展了搜索空间的垂直深度，支持跨多个维度的同时节点特定架构自定义。&lt;h4&gt;主要发现&lt;/h4&gt;1. NodeNAS和MNNAS方法能够有效提高图神经网络在有限或独特训练数据上的性能。2. MNNAS通过模拟具有变化排列性的节点度幂律分布来编码结构不变信息，并利用这些信息引导跨各个维度的架构定制，从而实现更好的OOD泛化。&lt;h4&gt;结论&lt;/h4&gt;MNNAS方法在监督和非监督任务上均表现出色，超越了现有的最新算法，在处理OOD数据时表现尤为突出。这表明该模型具有良好的泛化能力和应用前景。&lt;h4&gt;翻译&lt;/h4&gt;摘要：图神经架构搜索（GraphNAS）已经在缓解由于分布变化导致的图神经网络性能下降方面展示了其优势。最近的方法通过在定制架构之间进行权重共享，生成了针对每张图的独特GNN架构，并且端到端地进行了优化。然而，现有的GraphNAS方法没有考虑到不同图形之间的分布模式，并严重依赖于大量训练数据。当面对稀疏或单一的训练图时，这些方法难以发现最优的映射关系，从而无法泛化至分布外（OOD）的数据上。在本文中，我们提出了节点特定图神经架构搜索(NodeNAS)，其目标是通过分解节点拓扑和图分布来为不同节点定制独特的聚合方法，并且在有限数据集的情况下实现这一目的。此外，我们还提出了自适应聚集注意力的多维度NodeNAS（MNNAS）方法，该方法学习了一个具有良好泛化能力的节点特定架构定制器。具体而言，我们扩展了搜索空间的垂直深度，支持同时跨多个维度进行节点特定架构定制。并且我们建模了在不同排列性下的节点度幂律分布，编码结构不变信息以指导每个维度上的架构定制。广泛的监督和非监督任务实验表明，MNNAS超越了现有的最先进技术，并且实现了优秀的OOD泛化效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural architecture search (GraphNAS) has demonstrated advantages inmitigating performance degradation of graph neural networks (GNNs) due todistribution shifts. Recent approaches introduce weight sharing across tailoredarchitectures, generating unique GNN architectures for each graph end-to-end.However, existing GraphNAS methods do not account for distribution patternsacross different graphs and heavily rely on extensive training data. Withsparse or single training graphs, these methods struggle to discover optimalmappings between graphs and architectures, failing to generalize toout-of-distribution (OOD) data. In this paper, we propose node-specific graphneural architecture search(NodeNAS), which aims to tailor distinct aggregationmethods for different nodes through disentangling node topology and graphdistribution with limited datasets. We further propose adaptive aggregationattention based Multi-dim NodeNAS method(MNNAS), which learns an node-specificarchitecture customizer with good generalizability. Specifically, we extend thevertical depth of the search space, supporting simultaneous node-specificarchitecture customization across multiple dimensions. Moreover, we model thepower-law distribution of node degrees under varying assortativity, encodingstructure invariant information to guide architecture customization across eachdimension. Extensive experiments across supervised and unsupervised tasksdemonstrate that MNNAS surpasses state-of-the-art algorithms and achievesexcellent OOD generalization.</description>
      <author>example@mail.com (Qiyi Wang, Yinning Shao, Yunlong Ma, Min Liu)</author>
      <guid isPermaLink="false">2503.02448v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>WeakMedSAM: Weakly-Supervised Medical Image Segmentation via SAM with Sub-Class Exploration and Prompt Affinity Mining</title>
      <link>http://arxiv.org/abs/2503.04106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'研究背景': '在视觉任务中，基础模型取得了显著进展。最近的一些工作利用分割任何东西（SAM）模型来提升医学图像中的分割性能。', '现有方法的局限性': '大多数现有方法集中在完全监督的方式下训练适配器，以便使用大量像素级标注的医疗图像进行微调。', '研究目的': '为了减少标记成本，本论文探讨了一种基于弱监督的SAM模型——WeakMedSAM。', '主要贡献': {'子类探索模块': '引入一个子类探索模块以减轻医学图像中的严重共现问题，并学习准确的特征表示。', '提示亲和性挖掘模块': '利用SAM的提示能力，提出提示亲和性挖掘模块来改善类别激活图的质量。', '通用性': '所提方法可以应用于任何类似SAM的基础模型。实验中使用了SAMUS和EfficientSAM进行测试。'}, '实验结果': '在BraTS2019、AbdomenCT-1K及MSD心脏数据集这三个常用基准数据集上的实验证明了所提出方法的有效性。', '结论': '我们的研究展示了弱监督的医学图像分割模型WeakMedSAM在减少标注成本方面的潜力。相关代码可在GitHub上获得（https://github.com/wanghr64/WeakMedSAM）。'}&lt;h4&gt;翻译&lt;/h4&gt;我们见证了视觉任务中基础模型的显著进展，最近的一些工作利用了分割任何东西(SAM)模型来提升医学图像中的分割性能，这些方法大多集中在完全监督的方式下训练适配器，并使用大量像素级标注的医疗图像进行微调。为了减少标签成本，在本论文中，我们探讨了一种基于弱监督的新颖SAM模型——WeakMedSAM。我们的模型包含两个模块：1）为减轻医学图像中的严重共现问题，引入一个子类探索模块来学习准确的特征表示；2）利用SAM的提示能力，提出一种提示亲和性挖掘模块以改善类别激活图的质量。我们所提方法可以应用于任何类似SAM的基础模型，并且我们在实验中使用了SAMUS和EfficientSAM进行测试。在BraTS2019、AbdomenCT-1K及MSD心脏数据集上的实验证明，我们的WeakMedSAM有显著的效果。我们的代码可以在https://github.com/wanghr64/WeakMedSAM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We have witnessed remarkable progress in foundation models in vision tasks.Currently, several recent works have utilized the segmenting anything model(SAM) to boost the segmentation performance in medical images, where most ofthem focus on training an adaptor for fine-tuning a large amount of pixel-wiseannotated medical images following a fully supervised manner. In this paper, toreduce the labeling cost, we investigate a novel weakly-supervised SAM-basedsegmentation model, namely WeakMedSAM. Specifically, our proposed WeakMedSAMcontains two modules: 1) to mitigate severe co-occurrence in medical images, asub-class exploration module is introduced to learn accurate featurerepresentations. 2) to improve the quality of the class activation maps, ourprompt affinity mining module utilizes the prompt capability of SAM to obtainan affinity map for random-walk refinement. Our method can be applied to anySAM-like backbone, and we conduct experiments with SAMUS and EfficientSAM. Theexperimental results on three popularly-used benchmark datasets, i.e., BraTS2019, AbdomenCT-1K, and MSD Cardiac dataset, show the promising results of ourproposed WeakMedSAM. Our code is available athttps://github.com/wanghr64/WeakMedSAM.</description>
      <author>example@mail.com (Haoran Wang, Lian Huai, Wenbin Li, Lei Qi, Xingqun Jiang, Yinghuan Shi)</author>
      <guid isPermaLink="false">2503.04106v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>GaussianGraph: 3D Gaussian-based Scene Graph Generation for Open-world Scene Understanding</title>
      <link>http://arxiv.org/abs/2503.04034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近年来，3D高斯点阵（3DGS）技术在语义场景理解方面的进展显著提升，使自然语言查询能够定位场景中的对象。然而，现有方法主要集中在将压缩的CLIP特征嵌入到三维高斯分布中，导致物体分割精度较低且缺乏空间推理能力。&lt;h4&gt;背景&lt;/h4&gt;最近的3DGS技术改进了对语义场景的理解，但是现有的方法在将CLIP特征嵌入到3D Gaussians时存在问题，尤其是在对象分割准确性和空间推理方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出GaussianGraph框架以解决现有方法的不足，通过引入自适应语义聚类和场景图生成来增强基于3DGS的场景理解能力。&lt;h4&gt;方法&lt;/h4&gt;{'Control-Follow': '一种动态调整到场景尺度和特征分布的策略，避免了特性压缩，并显著提高了分割精度。', '2D基础模型提取': '通过整合从二维基础模型中提取的对象属性和空间关系来丰富场景表示。', '3D校正模块': '为了纠正不准确的空间关系，提出了3D校正模块，该模块通过空间一致性验证过滤掉不可信的关系，确保可靠地构建场景图。'}&lt;h4&gt;主要发现&lt;/h4&gt;GaussianGraph框架在三个数据集上的实验表明，在语义分割和对象定位任务中优于现有的最先进技术。&lt;h4&gt;结论&lt;/h4&gt;GaussianGraph为复杂的场景理解和交互提供了一个强有力的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近在3D高斯点阵（3DGS）方面的发展显著改善了对三维场景的语义理解，使得通过自然语言查询可以精确定位场景中的对象。然而，现有的方法主要关注于将压缩后的CLIP特征嵌入到3D Gaussians中，这导致物体分割精度较低，并且缺乏空间推理的能力。为了克服这些限制，我们提出了一种新的框架GaussianGraph，它通过集成自适应语义聚类和场景图生成来增强基于3DGS的场景理解能力。我们引入了'控制-跟随'聚类策略，该策略可以动态地调整到不同规模的场景及特征分布，避免了特征压缩，并显著提高了分割精度。此外，我们将从2D基础模型中提取的对象属性和空间关系整合进场景表示之中，以丰富其描述信息。为了减少空间关系中的不准确性，我们还提出了3D校正模块，通过空间一致性验证来过滤掉不可信的关系，从而确保构建出可靠的场景图。在三个数据集上的广泛实验表明，在语义分割以及对象定位任务上GaussianGraph优于现有的最先进技术，提供了一个解决复杂场景理解和交互的有效方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in 3D Gaussian Splatting(3DGS) have significantlyimproved semantic scene understanding, enabling natural language queries tolocalize objects within a scene. However, existing methods primarily focus onembedding compressed CLIP features to 3D Gaussians, suffering from low objectsegmentation accuracy and lack spatial reasoning capabilities. To address theselimitations, we propose GaussianGraph, a novel framework that enhances3DGS-based scene understanding by integrating adaptive semantic clustering andscene graph generation. We introduce a "Control-Follow" clustering strategy,which dynamically adapts to scene scale and feature distribution, avoidingfeature compression and significantly improving segmentation accuracy.Additionally, we enrich scene representation by integrating object attributesand spatial relations extracted from 2D foundation models. To addressinaccuracies in spatial relationships, we propose 3D correction modules thatfilter implausible relations through spatial consistency verification, ensuringreliable scene graph construction. Extensive experiments on three datasetsdemonstrate that GaussianGraph outperforms state-of-the-art methods in bothsemantic segmentation and object grounding tasks, providing a robust solutionfor complex scene understanding and interaction.</description>
      <author>example@mail.com (Xihan Wang, Dianyi Yang, Yu Gao, Yufeng Yue, Yi Yang, Mengyin Fu)</author>
      <guid isPermaLink="false">2503.04034v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>All-atom Diffusion Transformers: Unified generative modelling of molecules and materials</title>
      <link>http://arxiv.org/abs/2503.03965v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为All-atom Diffusion Transformer (ADiT)的统一生成框架，用于同时生成周期性材料和非周期分子系统。&lt;h4&gt;背景&lt;/h4&gt;扩散模型是三维原子系统生成建模的标准工具。然而，对于不同类型的原子系统（如分子和材料），尽管基础物理相同，但其生成过程通常高度特定于目标系统。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够同时用于生成周期性材料和非周期分子系统的统一扩散框架。&lt;h4&gt;方法&lt;/h4&gt;ADiT包括一个自动编码器，它将分子和材料的统一、全原子表示映射到共享的潜在嵌入空间；以及一个训练来生成新的潜在嵌入以供自动解码器使用的新模型。实验表明，联合训练的ADiT能够生成现实且有效的分子和材料。&lt;h4&gt;主要发现&lt;/h4&gt;在QM9和MP20数据集上进行的实验显示，联合训练的ADiT可以产生真实且有效的分子及材料，并超越了专门针对分子或晶体的现有最佳方法结果。此外，通过使用标准变压器作为自动编码器和扩散模型，在训练和推理期间实现了显著的速度提升。&lt;h4&gt;结论&lt;/h4&gt;ADiT框架代表了一种向广泛适用的基础生成化学模型迈进的重要步骤，随着参数规模扩大至半亿级参数时性能可预期地提高。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的论文提出了一个名为All-atom Diffusion Transformer (ADiT)的新框架，该框架通过使用统一的自动编码器和扩散模型来同时生成分子和材料。研究展示了在多个数据集上的实验结果，并表明这种方法的有效性和优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models are the standard toolkit for generative modelling of 3Datomic systems. However, for different types of atomic systems - such asmolecules and materials - the generative processes are usually highly specificto the target system despite the underlying physics being the same. Weintroduce the All-atom Diffusion Transformer (ADiT), a unified latent diffusionframework for jointly generating both periodic materials and non-periodicmolecular systems using the same model: (1) An autoencoder maps a unified,all-atom representations of molecules and materials to a shared latentembedding space; and (2) A diffusion model is trained to generate new latentembeddings that the autoencoder can decode to sample new molecules ormaterials. Experiments on QM9 and MP20 datasets demonstrate that jointlytrained ADiT generates realistic and valid molecules as well as materials,exceeding state-of-the-art results from molecule and crystal-specific models.ADiT uses standard Transformers for both the autoencoder and diffusion model,resulting in significant speedups during training and inference compared toequivariant diffusion models. Scaling ADiT up to half a billion parameterspredictably improves performance, representing a step towards broadlygeneralizable foundation models for generative chemistry. Open source code:https://github.com/facebookresearch/all-atom-diffusion-transformer</description>
      <author>example@mail.com (Chaitanya K. Joshi, Xiang Fu, Yi-Lun Liao, Vahe Gharakhanyan, Benjamin Kurt Miller, Anuroop Sriram, Zachary W. Ulissi)</author>
      <guid isPermaLink="false">2503.03965v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>SurgiSAM2: Fine-tuning a foundational model for surgical video anatomy segmentation and detection</title>
      <link>http://arxiv.org/abs/2503.03942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了模型SAM 2在手术场景理解中的表现，特别是在零样本和微调后的语义分割能力方面。通过使用五个公共数据集进行评估，并对图像编码器和掩码解码器进行了微调。&lt;h4&gt;背景&lt;/h4&gt;利用五个公开的数据集来评估和微调SAM 2模型，以使其更好地在手术视频/图像中分割解剖组织。&lt;h4&gt;目的&lt;/h4&gt;验证模型SAM 2在零样本条件下的语义分割能力，并通过微调进一步提高其性能。&lt;h4&gt;方法&lt;/h4&gt;对模型进行了不同规模数据集上的微调测试（从每个类别的50个训练样例增加到400个），并采用加权平均Dice系数(WMDC)来评估性能。同时与之前的最优结果进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的SurgiSAM 2模型相比基线SAM 2，以相对17.9%的WMDC提高显著提升了分割性能；在测试数据集上，在24/30（80%）类别中超越了先前的最佳方法。此外，该模型还有效泛化到未知器官类别的新场景。&lt;h4&gt;结论&lt;/h4&gt;SAM 2在手术场景分割方面表现出色，无论是零样本还是微调后的性能都超过了之前的方法，并且展示了自动化/半自动化标注管道的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要提供了关于研究背景、目的、方法、主要发现和结论的详细信息，强调了模型在处理解剖学组织分割方面的改进以及它如何为未来手术应用中的自动注释流程铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Background: We evaluate SAM 2 for surgical scene understanding by examiningits semantic segmentation capabilities for organs/tissues both in zero-shotscenarios and after fine-tuning. Methods: We utilized five public datasets toevaluate and fine-tune SAM 2 for segmenting anatomical tissues in surgicalvideos/images. Fine-tuning was applied to the image encoder and mask decoder.We limited training subsets from 50 to 400 samples per class to better modelreal-world constraints with data acquisition. The impact of dataset size onfine-tuning performance was evaluated with weighted mean Dice coefficient(WMDC), and the results were also compared against previously reportedstate-of-the-art (SOTA) results. Results: SurgiSAM 2, a fine-tuned SAM 2 model,demonstrated significant improvements in segmentation performance, achieving a17.9% relative WMDC gain compared to the baseline SAM 2. Increasing promptpoints from 1 to 10 and training data scale from 50/class to 400/class enhancedperformance; the best WMDC of 0.92 on the validation subset was achieved with10 prompt points and 400 samples per class. On the test subset, this modeloutperformed prior SOTA methods in 24/30 (80%) of the classes with a WMDC of0.91 using 10-point prompts. Notably, SurgiSAM 2 generalized effectively tounseen organ classes, achieving SOTA on 7/9 (77.8%) of them. Conclusion: SAM 2achieves remarkable zero-shot and fine-tuned performance for surgical scenesegmentation, surpassing prior SOTA models across several organ classes ofdiverse datasets. This suggests immense potential for enablingautomated/semi-automated annotation pipelines, thereby decreasing the burden ofannotations facilitating several surgical applications.</description>
      <author>example@mail.com (Devanish N. Kamtam, Joseph B. Shrager, Satya Deepya Malla, Xiaohan Wang, Nicole Lin, Juan J. Cardona, Serena Yeung-Levy, Clarence Hu)</author>
      <guid isPermaLink="false">2503.03942v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>A dataset-free approach for self-supervised learning of 3D reflectional symmetries</title>
      <link>http://arxiv.org/abs/2503.02660v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自监督模型，用于检测单个物体的对称性，并且该模型不需要大型数据集的支持。通过计算对象点云中的视觉特征来优化自监督模型。&lt;h4&gt;背景&lt;/h4&gt;目前在训练对称性检测模型时，需要大量的标注数据，这导致了高昂的成本和资源消耗。&lt;h4&gt;目的&lt;/h4&gt;设计一种不依赖于大规模数据集的自监督学习策略，以降低训练成本并提高效率。&lt;h4&gt;方法&lt;/h4&gt;利用基础图像模型提取特征来计算物体点云中的视觉描述符，基于对称点应具有相似视觉外观的原则进行自我监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法在性能上超越了大型数据集上训练的现有最佳模型，并且更加高效和资源友好。&lt;h4&gt;结论&lt;/h4&gt;提出的方法不仅减少了计算和数据需求，而且提高了检测精度和效率。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们研究了一种自监督模型，它能够学习单个物体的对称性而无需依赖大型数据集。我们认为可以通过分析物体自身的内在特征来确定其对称性。此外，我们设计了不使用地面真实标签的自我监督学习策略。这些关键要素使我们的方法既有效又高效，解决了为这种任务构建大规模标注数据集的成本问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we explore a self-supervised model that learns to detect thesymmetry of a single object without requiring a dataset-relying solely on theinput object itself. We hypothesize that the symmetry of an object can bedetermined by its intrinsic features, eliminating the need for large datasetsduring training. Additionally, we design a self-supervised learning strategythat removes the necessity of ground truth labels. These two key elements makeour approach both effective and efficient, addressing the prohibitive costsassociated with constructing large, labeled datasets for this task. The noveltyof our method lies in computing features for each point on the object based onthe idea that symmetric points should exhibit similar visual appearances. Toachieve this, we leverage features extracted from a foundational image model tocompute a visual descriptor for the points. This approach equips the pointcloud with visual features that facilitate the optimization of ourself-supervised model. Experimental results demonstrate that our methodsurpasses the state-of-the-art models trained on large datasets. Furthermore,our model is more efficient, effective, and operates with minimal computationaland data resources.</description>
      <author>example@mail.com (Isaac Aguirre, Ivan Sipiran, Gabriel Montañana)</author>
      <guid isPermaLink="false">2503.02660v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>CREStE: Scalable Mapless Navigation with Internet Scale Priors and Counterfactual Guidance</title>
      <link>http://arxiv.org/abs/2503.03921v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 10 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CREStE的方法，用于解决机器人在没有高精度地图和具体导航点的情况下进行长距离自主导航的问题。&lt;h4&gt;背景&lt;/h4&gt;当前的解决方案难以泛化应用，因为它们依赖于手动定义的对象列表、缺乏大规模的数据集以及手工艺奖励函数，这些都不适用于多种场景。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需大规模机器人数据集或人工整理特征的方法来学习表示和奖励机制，以解决无地图导航问题。&lt;h4&gt;方法&lt;/h4&gt;CREStE利用视觉基础模型在互联网规模的数据上进行训练，获取包含高度、语义及实例级信息的连续鸟瞰图表示，并通过基于反事实的学习损失和主动学习过程优化这些表示用于路径规划。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与现有最佳解决方案相比，CREStE显著减少了任务中的干预次数，在未见过环境中执行2公里导航仅需要1次人工干预。&lt;h4&gt;结论&lt;/h4&gt;该方法显示了其在长距离无地图导航中具备的稳定性和有效性，并为机器人导航研究提供了新的视角和方向。&lt;h4&gt;翻译&lt;/h4&gt;我们解决了无需高精度地图或具体导航点的长距离自主导航问题。为了实现这一目标，需要克服两个主要挑战：一是学习对环境进行感知表示，而不预先列举所有可能的导航因素和感知混淆；二是利用这些已学得的表示来规划与人类一致的导航路径。现有的解决方案由于依赖于手动定义的对象列表、缺少大规模数据集以及手工艺奖励函数等问题而难以泛化应用。为克服这些问题，我们提出了CREStE方法，它是第一个不依赖大规模机器人数据集或人工整理特征的方法，用于学习解决无地图导航问题所需的表示和奖励机制。CREStE利用视觉基础模型在互联网规模的数据上进行训练，获取包含高度、语义及实例级信息的连续鸟瞰图表示，并通过基于反事实的学习损失和主动学习过程优化这些表示用于路径规划。我们在六个不同城市环境中的公里尺度导航任务中评估了CREStE的表现，结果显示它显著优于所有现有最佳解决方案，在最少70%的人工干预下表现良好；在一项2公里的未见环境中执行的任务中，仅需要1次人工干预就完成了任务，证明其具有良好的稳定性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We address the long-horizon mapless navigation problem: enabling robots totraverse novel environments without relying on high-definition maps or precisewaypoints that specify exactly where to navigate. Achieving this requiresovercoming two major challenges -- learning robust, generalizable perceptualrepresentations of the environment without pre-enumerating all possiblenavigation factors and forms of perceptual aliasing and utilizing these learnedrepresentations to plan human-aligned navigation paths. Existing solutionsstruggle to generalize due to their reliance on hand-curated object lists thatoverlook unforeseen factors, end-to-end learning of navigation features fromscarce large-scale robot datasets, and handcrafted reward functions that scalepoorly to diverse scenarios. To overcome these limitations, we propose CREStE,the first method that learns representations and rewards for addressing thefull mapless navigation problem without relying on large-scale robot datasetsor manually curated features. CREStE leverages visual foundation models trainedon internet-scale data to learn continuous bird's-eye-view representationscapturing elevation, semantics, and instance-level features. To utilize learnedrepresentations for planning, we propose a counterfactual-based loss and activelearning procedure that focuses on the most salient perceptual cues by queryinghumans for counterfactual trajectory annotations in challenging scenes. Weevaluate CREStE in kilometer-scale navigation tasks across six distinct urbanenvironments. CREStE significantly outperforms all state-of-the-art approacheswith 70% fewer human interventions per mission, including a 2-kilometer missionin an unseen environment with just 1 intervention; showcasing its robustnessand effectiveness for long-horizon mapless navigation. For videos andadditional materials, see https://amrl.cs.utexas.edu/creste .</description>
      <author>example@mail.com (Arthur Zhang, Harshit Sikchi, Amy Zhang, Joydeep Biswas)</author>
      <guid isPermaLink="false">2503.03921v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>LensDFF: Language-enhanced Sparse Feature Distillation for Efficient Few-Shot Dexterous Manipulation</title>
      <link>http://arxiv.org/abs/2503.03890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为LensDFF的方法，该方法通过语言增强的特征融合策略高效地将视图一致的2D特性投影到3D点上，并结合抓取基本动作生成稳定且高度灵巧的抓取。&lt;h4&gt;背景&lt;/h4&gt;从少量演示中学习灵巧操纵对高级的人类类似机器人系统来说是一项重要的挑战。密集蒸馏特征场通过将2D视觉基础模型中的丰富语义特征提取到3D域来解决这一问题，但其依赖于如Neural Radiance Fields (NeRF)或高斯点阵等神经渲染模型导致了较高的计算成本。&lt;h4&gt;目的&lt;/h4&gt;克服基于稀疏特征字段的方法的效率和抓取灵巧性不足的问题，并提出了一种新的语言增强的稀疏蒸馏特征场（LensDFF）方法，以及一种结合抓取基本动作进行少量演示下的灵巧操纵框架。&lt;h4&gt;方法&lt;/h4&gt;利用新颖的语言增强特征融合策略高效地将视图一致性的2D特征投影到3D点上；基于LensDFF的方法能够实现单视角下少量演示的泛化能力。此外还提出了一种real2sim抓取评估流水线以进行高效的抓取评估和超参数调整。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量的仿真实验和真实世界的实验，该方法在抓取性能方面取得了竞争性成果，并超过了现有最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的LensDFF方法能够高效地利用少量演示实现灵巧操纵的泛化，并且其结合抓取基本动作的方法可以生成稳定且高度灵巧的抓取。通过real2sim评估流水线，证明了该方法的有效性和优越性。&lt;h4&gt;翻译&lt;/h4&gt;从有限的示范中学习灵活的手部操作是先进的类似人类机器人系统的一项重要但具有挑战性的任务。密集蒸馏特性领域通过将丰富的语义特征从二维视觉基础模型传递到三维空间来应对这一挑战。然而，这种方法依赖于如NeRF或高斯点阵这样的神经渲染模型，导致了高昂的计算成本。相比之下，以前基于稀疏特性领域的方案要么由于多视角依赖和广泛的训练而效率低下，要么缺乏足够的抓取灵巧性。为了克服这些局限性，我们提出了语言增强的稀疏蒸馏特征领域（LensDFF），它通过新颖的语言强化特征融合策略将视图一致的二维特征高效地应用到三维点上，并实现单视角少量演示下的泛化能力。基于LensDFF，我们进一步提出了一种结合抓取基本动作的少量灵活操作框架以生成稳定且高度灵巧的抓取。此外，我们还提出了一种real2sim抓取评估流水线用于高效的抓取评估和超参数调整。通过大量的仿真实验以及现实世界的实验，我们的方法在抓握性能方面达到了竞争性的结果，并超过了最先进的方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning dexterous manipulation from few-shot demonstrations is a significantyet challenging problem for advanced, human-like robotic systems. Densedistilled feature fields have addressed this challenge by distilling richsemantic features from 2D visual foundation models into the 3D domain. However,their reliance on neural rendering models such as Neural Radiance Fields (NeRF)or Gaussian Splatting results in high computational costs. In contrast,previous approaches based on sparse feature fields either suffer frominefficiencies due to multi-view dependencies and extensive training or lacksufficient grasp dexterity. To overcome these limitations, we proposeLanguage-ENhanced Sparse Distilled Feature Field (LensDFF), which efficientlydistills view-consistent 2D features onto 3D points using our novellanguage-enhanced feature fusion strategy, thereby enabling single-viewfew-shot generalization. Based on LensDFF, we further introduce a few-shotdexterous manipulation framework that integrates grasp primitives into thedemonstrations to generate stable and highly dexterous grasps. Moreover, wepresent a real2sim grasp evaluation pipeline for efficient grasp assessment andhyperparameter tuning. Through extensive simulation experiments based on thereal2sim pipeline and real-world experiments, our approach achieves competitivegrasping performance, outperforming state-of-the-art approaches.</description>
      <author>example@mail.com (Qian Feng, David S. Martinez Lema, Jianxiang Feng, Zhaopeng Chen, Alois Knoll)</author>
      <guid isPermaLink="false">2503.03890v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions</title>
      <link>http://arxiv.org/abs/2503.03862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;语言模型能力的提升通常被归因于增加模型规模或训练数据量，但在某些情况下，较小的模型在经过精心策划的数据集上训练或者采用不同的架构设计后可以超越更大规模且基于更多令牌进行训练的模型。为了量化这些设计选择的影响，我们对92个开源预训练模型进行了元分析，涵盖了各种规模，包括最先进的开放权重模型以及性能较差的模型和具有非传统设计方案的模型。研究发现，在考虑了除了模型大小和训练令牌数量之外的因素后，我们可以相对地提高3-28%的能力来预测下游表现与仅使用规模相比。对模型设计决策的分析揭示了关于数据组成的见解，例如在语言和代码任务之间的15-25%比例的任务权衡以及某些架构选择如旋转编码优于学习嵌入的方式可带来更好的性能。总体而言，我们的框架为更系统地研究模型开发选择如何塑造最终能力奠定了基础。&lt;h4&gt;背景&lt;/h4&gt;提升语言模型的性能通常被认为是通过增加模型大小或训练数据量来实现的，然而有时较小规模、在精心策划的数据集上训练或者采用不同架构设计的小型模型可以超越更大规模且基于更多令牌进行训练的大模型。&lt;h4&gt;目的&lt;/h4&gt;量化并分析影响语言模型性能的设计选择因素，包括模型尺寸、训练数据数量以及特定的架构决策，并探索这些因素如何帮助预测下游任务的表现。&lt;h4&gt;方法&lt;/h4&gt;对92个开源预训练模型进行了元分析，涵盖了各种规模和设计决策，以比较不同的模型特性对性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;通过纳入除模型大小和训练令牌数量外的因素（如特定数据构成比例、架构选择等），可以相对提高3-28%的能力来预测下游表现；并且某些架构选择例如旋转编码优于学习嵌入的方式可带来更好的性能。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，精心策划的数据集及适当的模型设计决策可以显著提升语言模型的性能，并提供了对不同规模和设计方案影响的理解。这些发现有助于更系统地探究如何通过开发过程中的选择来塑造最终的语言模型能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Improvements in language model capabilities are often attributed toincreasing model size or training data, but in some cases smaller modelstrained on curated data or with different architectural decisions canoutperform larger ones trained on more tokens. What accounts for this? Toquantify the impact of these design choices, we meta-analyze 92 open-sourcepretrained models across a wide array of scales, including state-of-the-artopen-weights models as well as less performant models and those with lessconventional design decisions. We find that by incorporating features besidesmodel size and number of training tokens, we can achieve a relative 3-28%increase in ability to predict downstream performance compared with using scalealone. Analysis of model design decisions reveal insights into datacomposition, such as the trade-off between language and code tasks at 15-25\%code, as well as the better performance of some architectural decisions such aschoosing rotary over learned embeddings. Broadly, our framework lays afoundation for more systematic investigation of how model development choicesshape final capabilities.</description>
      <author>example@mail.com (Emmy Liu, Amanda Bertsch, Lintang Sutawika, Lindia Tjuatja, Patrick Fernandes, Lara Marinov, Michael Chen, Shreya Singhal, Carolin Lawrence, Aditi Raghunathan, Kiril Gashteovski, Graham Neubig)</author>
      <guid isPermaLink="false">2503.03862v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Task-Agnostic Attacks Against Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2503.03842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;机器学习的安全研究主要集中在针对下游特定任务的攻击上。这些攻击通过优化与具体任务相关的损失函数来生成对抗样本。&lt;h4&gt;目的&lt;/h4&gt;探讨对公开可用的基础视觉模型（预训练模型）进行攻击的影响，以及这种攻击在多个下游任务中的传播能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种通用框架，用于制造不依赖于特定任务的对抗性示例。通过扰乱基础模型获取到的特征表示来最大化破坏其安全性。&lt;h4&gt;主要发现&lt;/h4&gt;评估了广泛使用的视觉基础模型的安全性，测量了攻击对多个下游任务的影响以及在不同模型之间的传播能力。&lt;h4&gt;结论&lt;/h4&gt;当前研究领域对于如何保护基于公共预训练模型的应用程序免受此类威胁仍然缺乏足够的关注和研究。&lt;h4&gt;翻译&lt;/h4&gt;该研究主要针对机器学习安全领域的现有不足进行了探讨，并提出了一种新的框架来评估视觉基础模型的安全性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The study of security in machine learning mainly focuses on downstreamtask-specific attacks, where the adversarial example is obtained by optimizinga loss function specific to the downstream task. At the same time, it hasbecome standard practice for machine learning practitioners to adopt publiclyavailable pre-trained vision foundation models, effectively sharing a commonbackbone architecture across a multitude of applications such asclassification, segmentation, depth estimation, retrieval, question-answeringand more. The study of attacks on such foundation models and their impact tomultiple downstream tasks remains vastly unexplored. This work proposes ageneral framework that forges task-agnostic adversarial examples by maximallydisrupting the feature representation obtained with foundation models. Weextensively evaluate the security of the feature representations obtained bypopular vision foundation models by measuring the impact of this attack onmultiple downstream tasks and its transferability between models.</description>
      <author>example@mail.com (Brian Pulfer, Yury Belousov, Vitaliy Kinakh, Teddy Furon, Slava Voloshynovskiy)</author>
      <guid isPermaLink="false">2503.03842v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection</title>
      <link>http://arxiv.org/abs/2503.03562v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文介绍了首个用于工业异常检测的大规模物理基础视频数据集Physics Anomaly Detection (Phys-AD)，并分析了现有方法在处理基于物理学的异常时的局限性。&lt;h4&gt;背景&lt;/h4&gt;人类通过感知、互动和推理来识别现实世界中的物体异常。现有的工业异常检测算法主要是在静态且语义简单的数据集上开发和测试，这与需要物理理解和推理的真实场景相去甚远。&lt;h4&gt;目的&lt;/h4&gt;引入Physics Anomaly Detection (Phys-AD) 数据集，以缩小当前工业异常检测技术和真实应用场景之间的差距。&lt;h4&gt;方法&lt;/h4&gt;使用真实的机器人手臂和电机收集了大量动态且语义丰富的视频数据，并设计了多种评估指标来衡量现有模型在处理基于物理学的异常时的表现。&lt;h4&gt;主要发现&lt;/h4&gt;现有的无监督、弱监督以及视频理解模型在处理Phys-AD中的物理基础异常时表现出明显局限性。此外，提出了一个新的评估指标PAEval，用于评测视觉语言基础模型解释异常物理原因的能力。&lt;h4&gt;结论&lt;/h4&gt;通过提出Physics Anomaly Detection (Phys-AD) 数据集和相关评估基准，作者希望推动工业异常检测技术的发展，并促进具有准确物理解释能力的视觉语言基础模型的研究。&lt;h4&gt;翻译&lt;/h4&gt;人类在现实世界中识别物体异常时，会利用基于条件物体的知识进行感知、互动和推理。工业异常检测（IAD）领域的长期目标是使机器能够自主复制这一技能。然而，当前大多数IAD算法主要是在静态且语义简单的数据集上开发和测试的，这些数据集与现实世界场景存在很大差距，在后者中物理理解和推理至关重要。为了弥合这种差距，作者引入了Physics Anomaly Detection (Phys-AD) 数据集——首个用于工业异常检测的大规模、真实世界的物理基础视频数据集。该数据集由真实的机器人手臂和电机收集，提供了多样化的动态且语义丰富的场景，并包含超过6400个视频片段，跨越22类现实世界物体类别以及47种类型的异常情况。在Phys-AD中进行异常检测需要结合视觉推理、物理知识及视频内容来判断物体的不正常性。作者还评估了最先进的无监督、弱监督和基于视频理解的异常检测方法，并指出它们处理基于物理学的异常时的局限性。此外，提出了一种新的评估指标Physics Anomaly Explanation (PAEval)，用于评测视觉语言基础模型在发现异常的同时提供准确物理原因解释的能力。该数据集与基准将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans detect real-world object anomalies by perceiving, interacting, andreasoning based on object-conditioned physical knowledge. The long-term goal ofIndustrial Anomaly Detection (IAD) is to enable machines to autonomouslyreplicate this skill. However, current IAD algorithms are largely developed andtested on static, semantically simple datasets, which diverge from real-worldscenarios where physical understanding and reasoning are essential. To bridgethis gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, thefirst large-scale, real-world, physics-grounded video dataset for industrialanomaly detection. Collected using a real robot arm and motor, Phys-AD providesa diverse set of dynamic, semantically rich scenarios. The dataset includesmore than 6400 videos across 22 real-world object categories, interacting withrobot arms and motors, and exhibits 47 types of anomalies. Anomaly detection inPhys-AD requires visual reasoning, combining both physical knowledge and videocontent to determine object abnormality. We benchmark state-of-the-art anomalydetection methods under three settings: unsupervised AD, weakly-supervised AD,and video-understanding AD, highlighting their limitations in handlingphysics-grounded anomalies. Additionally, we introduce the Physics AnomalyExplanation (PAEval) metric, designed to assess the ability of visual-languagefoundation models to not only detect anomalies but also provide accurateexplanations for their underlying physical causes. Our dataset and benchmarkwill be publicly available.</description>
      <author>example@mail.com (Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu)</author>
      <guid isPermaLink="false">2503.03562v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation</title>
      <link>http://arxiv.org/abs/2502.20984v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SemEval-2025 Task 1的任务是根据给定的名词复合词（可能带有隐喻意义）对图片进行排名。&lt;h4&gt;背景&lt;/h4&gt;研究利用生成式大型语言模型和多语种CLIP模型来增强具有隐喻含义的名词复合词表示。&lt;h4&gt;目的&lt;/h4&gt;提升图像与隐喻性名词复合词匹配度的排名性能。&lt;h4&gt;方法&lt;/h4&gt;使用LLM生成名词复合词的隐喻意义，然后用多语种CLIP模型进行编码；采用对比学习和数据增强技术对这些嵌入式表示进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示通过这种方法提取出的多模态表征优于仅基于原始名词复合词的方法。然而，虽然使用方法的微调策略显示出有前景的结果，但其效果不如未经过微调的情况。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种改进图像与具有隐喻含义的名词复合词匹配度排名的新方法，并公开了使用的源代码。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述的研究涉及SemEval-2025 Task 1任务中的图片排序，通过利用生成式大型语言模型和多语CLIP模型来增强隐喻性名词复合词表示。研究展示了这些技术在改进图像与隐喻性词语匹配度上的有效性，并公开了其源代码供其他研究人员参考使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; SemEval-2025 Task 1 focuses on ranking images based on their alignment with agiven nominal compound that may carry idiomatic meaning in both English andBrazilian Portuguese. To address this challenge, this work uses generativelarge language models (LLMs) and multilingual CLIP models to enhance idiomaticcompound representations. LLMs generate idiomatic meanings for potentiallyidiomatic compounds, enriching their semantic interpretation. These meaningsare then encoded using multilingual CLIP models, serving as representations forimage ranking. Contrastive learning and data augmentation techniques areapplied to fine-tune these embeddings for improved performance. Experimentalresults show that multimodal representations extracted through this methodoutperformed those based solely on the original nominal compounds. Thefine-tuning approach shows promising outcomes but is less effective than usingembeddings without fine-tuning. The source code used in this paper is availableat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.</description>
      <author>example@mail.com (Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang)</author>
      <guid isPermaLink="false">2502.20984v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion Capture</title>
      <link>http://arxiv.org/abs/2503.03222v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了Mocap-2-to-3框架，通过利用大规模的2D数据来改善单目视图下的人体绝对姿态恢复。&lt;h4&gt;背景&lt;/h4&gt;从单目视角恢复世界坐标系中的绝对姿态存在两大挑战：一是现有方法依赖于难以获取的新动作3D标签；二是仅凭单一视角估计人体在度量空间的位置复杂性高。&lt;h4&gt;目的&lt;/h4&gt;通过引入Mocap-2-to-3框架，解决上述问题，并提高模型的泛化能力和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;首先使用大量2D数据预训练单视图扩散模型，然后利用公开的3D数据进行多视图扩散模型微调。同时提出了一种新的人体运动表示法，将局部动作与全局移动分离并编码地面几何先验。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该方法在真实世界数据集上比现有最佳技术表现更好，在姿态和绝对位置估计方面具有更高的精度、更好的泛化能力和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;Mocap-2-to-3框架通过有效利用大规模的2D数据，解决了从单目视角恢复人体绝对姿态的问题，并提高了模型的实际应用能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recovering absolute poses in the world coordinate system from monocular viewspresents significant challenges. Two primary issues arise in this context.Firstly, existing methods rely on 3D motion data for training, which requirescollection in limited environments. Acquiring such 3D labels for new actions ina timely manner is impractical, severely restricting the model's generalizationcapabilities. In contrast, 2D poses are far more accessible and easier toobtain. Secondly, estimating a person's absolute position in metric space froma single viewpoint is inherently more complex. To address these challenges, weintroduce Mocap-2-to-3, a novel framework that decomposes intricate 3D motionsinto 2D poses, leveraging 2D data to enhance 3D motion reconstruction indiverse scenarios and accurately predict absolute positions in the worldcoordinate system. We initially pretrain a single-view diffusion model withextensive 2D data, followed by fine-tuning a multi-view diffusion model forview consistency using publicly available 3D data. This strategy facilitatesthe effective use of large-scale 2D data. Additionally, we propose aninnovative human motion representation that decouples local actions from globalmovements and encodes geometric priors of the ground, ensuring the generativemodel learns accurate motion priors from 2D data. During inference, this allowsfor the gradual recovery of global movements, resulting in more plausiblepositioning. We evaluate our model's performance on real-world datasets,demonstrating superior accuracy in motion and absolute human positioningcompared to state-of-the-art methods, along with enhanced generalization andscalability. Our code will be made publicly available.</description>
      <author>example@mail.com (Zhumei Wang, Zechen Hu, Ruoxi Guo, Huaijin Pi, Ziyong Feng, Sida Peng, Xiaowei Zhou)</author>
      <guid isPermaLink="false">2503.03222v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>ReRAW: RGB-to-RAW Image Reconstruction via Stratified Sampling for Efficient Object Detection on the Edge</title>
      <link>http://arxiv.org/abs/2503.03782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为ReRAW的模型，该模型可以将现有的RGB图像转换为传感器特定的RAW格式。使用多头架构预测伽马空间中的RAW候选图，并通过分层采样策略训练数据选择方法来优化性能。&lt;h4&gt;背景&lt;/h4&gt;边缘设备上的计算机视觉模型从未经处理的细节丰富的RAW传感器数据中受益匪浅，而不是经过处理的RGB图像。然而，为了训练这些模型，需要大量的带有标签的RAW数据集，这通常是昂贵且不切实际的。&lt;h4&gt;目的&lt;/h4&gt;目的是通过转换现有的带标签RGB数据集为特定于传感器的RAW图像来有效训练模型。&lt;h4&gt;方法&lt;/h4&gt;介绍了一种新的RGB到RAW转换模型ReRAW，并提出一种基于分层采样的训练数据选择策略，以提高模型重构性能。&lt;h4&gt;主要发现&lt;/h4&gt;ReRAW在五个不同的RAW数据集中实现了最先进的重建效果。通过结合高质量的合成RAW数据集和真实地面真相RAW图像进行预训练的小型模型，在目标任务如对象检测中的表现优于标准RGB管道和从RGB预先训练到RAW微调的模型。&lt;h4&gt;结论&lt;/h4&gt;使用该方法生成的带标签RAW数据可以有效提升边缘设备上的计算机视觉模型性能，特别是对于资源受限环境下的应用具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;边缘设备上运行基于边缘的计算机视觉模型可以从未经处理、细节丰富的RAW传感器数据中获益匪浅，而无需依赖于经过处理的RGB图像。然而，训练这些模型需要大量的标记过的RAW数据集，这通常是昂贵且不切实际的。因此，将现有的带标签RGB数据集转换为特定于传感器的RAW图像对于有效的模型训练至关重要。在本文中，我们介绍了ReRAW，这是一种从RGB到RAW的转换模型，在五个不同的RAW数据集中实现了最先进的重建性能。这是通过ReRAW新颖的多头架构实现的，该架构预测伽马空间中的RAW候选图，并且通过一种基于分层采样的训练数据选择策略进一步提高了性能，这有助于模型更好地重构更亮的RAW像素。最终我们展示了在下游任务（如对象检测）中使用高质量合成RAW数据集和真实地面真相RAW图像进行预训练的小型模型的表现优于标准RGB管道以及从RGB预先训练到RAW微调的模型为同一任务时的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Edge-based computer vision models running on compact, resource-limiteddevices benefit greatly from using unprocessed, detail-rich RAW sensor datainstead of processed RGB images. Training these models, however, necessitateslarge labeled RAW datasets, which are costly and often impractical to obtain.Thus, converting existing labeled RGB datasets into sensor-specific RAW imagesbecomes crucial for effective model training. In this paper, we introduceReRAW, an RGB-to-RAW conversion model that achieves state-of-the-artreconstruction performance across five diverse RAW datasets. This isaccomplished through ReRAW's novel multi-head architecture predicting RAW imagecandidates in gamma space. The performance is further boosted by a stratifiedsampling-based training data selection heuristic, which helps the model betterreconstruct brighter RAW pixels. We finally demonstrate that pretrainingcompact models on a combination of high-quality synthetic RAW datasets (such asgenerated by ReRAW) and ground-truth RAW images for downstream tasks likeobject detection, outperforms both standard RGB pipelines, and RAW fine-tuningof RGB-pretrained models for the same task.</description>
      <author>example@mail.com (Radu Berdan, Beril Besbinar, Christoph Reinders, Junji Otsuka, Daisuke Iso)</author>
      <guid isPermaLink="false">2503.03782v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation</title>
      <link>http://arxiv.org/abs/2503.04718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;场景流估计是许多机器人应用的基础任务，包括稳健的动态物体检测、自动标记和传感器同步。&lt;h4&gt;背景&lt;/h4&gt;针对场景流估计问题已经发展了两大类方法：监督学习和优化算法。监督学习方法在推理阶段速度快并且结果质量高，但需要大量标注数据并可能因领域差距受限；非监督测试时间优化方法则不受限于领域差距，但是通常运行时间较长、存在瑕疵或无法收敛。&lt;h4&gt;目的&lt;/h4&gt;本文旨在缓解现有基于优化的方法的若干局限性。&lt;h4&gt;方法&lt;/h4&gt;1) 引入了一种简单的体素网格模型，该模型在多个方面改进了标准MLP公式。2) 提出了一种新的多帧损失形式化方法。3) 将以上贡献结合到名为Floxels的新方法中。&lt;h4&gt;主要发现&lt;/h4&gt;在Argoverse 2基准测试上，Floxels在未监督方法中仅被EulerFlow超越，并且以较低的计算成本实现了可比性能；相比EulerFlow，Floxels的速度提升了超过60-140倍，将运行时间从一天减少到每序列10分钟。与更快但质量较差的基本线NSFP相比，Floxels的速度提高了约14倍。&lt;h4&gt;结论&lt;/h4&gt;新方法在速度和准确性上都有显著提高，并且大大降低了计算成本。&lt;h4&gt;翻译&lt;/h4&gt;场景流估计是许多机器人应用的基础任务，包括稳健的动态物体检测、自动标记和传感器同步。针对这个问题，已经发展了监督学习和非监督优化两大类方法。本文提出了一种新的基于体素网格的方法Floxels来改进现有的优化策略，并在基准测试中展示了显著的速度提升和性能保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene flow estimation is a foundational task for many robotic applications,including robust dynamic object detection, automatic labeling, and sensorsynchronization. Two types of approaches to the problem have evolved: 1)Supervised and 2) optimization-based methods. Supervised methods are fastduring inference and achieve high-quality results, however, they are limited bythe need for large amounts of labeled training data and are susceptible todomain gaps. In contrast, unsupervised test-time optimization methods do notface the problem of domain gaps but usually suffer from substantial runtime,exhibit artifacts, or fail to converge to the right solution. In this work, wemitigate several limitations of existing optimization-based methods. To thisend, we 1) introduce a simple voxel grid-based model that improves over thestandard MLP-based formulation in multiple dimensions and 2) introduce a newmultiframe loss formulation. 3) We combine both contributions in our newmethod, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed onlyby EulerFlow among unsupervised methods while achieving comparable performanceat a fraction of the computational cost. Floxels achieves a massive speedup ofmore than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10minutes per sequence. Over the faster but low-quality baseline, NSFP, Floxelsachieves a speedup of ~14x.</description>
      <author>example@mail.com (David T. Hoffmann, Syed Haseeb Raza, Hanqiu Jiang, Denis Tananaev, Steffen Klingenhoefer, Martin Meinke)</author>
      <guid isPermaLink="false">2503.04718v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Agent Inverse Q-Learning from Demonstrations</title>
      <link>http://arxiv.org/abs/2503.04679v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, 2 tables. Published at the International  Conference on Robotics and Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的多智能体逆强化学习框架，即基于展示的多代理边际Q学习（MAMQL），以解决单智能体逆向强化学习无法有效处理的合作与竞争目标之间的平衡问题。&lt;h4&gt;背景&lt;/h4&gt;深度强化学习算法在手动设计奖励函数的情况下容易遭受奖励指定不正确的问题，在多智能体环境中由于环境非稳态性和方差增加，这种问题变得更加严重。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的采样效率高的框架MAMQL来解决现有方法难以平衡合作与竞争目标的挑战。&lt;h4&gt;方法&lt;/h4&gt;对于每个代理，MAMQL通过学习其他代理策略下的边际评判者来推测奖励函数，并允许在多智能体场景中合理地使用玻尔兹曼政策。该研究还建立了最佳边际评判者和单智能体软Q逆向强化学习之间的联系。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与现有的多智能体方法相比，MAMQL显著提高了平均回报、采样效率和奖励恢复性能（通常为2-5倍）。&lt;h4&gt;结论&lt;/h4&gt;通过在多个模拟环境中验证了新框架的有效性，证明了其在处理复杂合作竞争关系的多智能体环境中的优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要：当奖赏函数是由人手动设计时，深度强化学习算法常常遭受奖赏指定不正确的问题，导致学习到次优策略。在单代理问题中，逆向强化学习（IRL）试图通过从专家演示推断出奖励函数来解决这个问题。然而，在多智能体问题中，由于环境的非稳态性和方差增加，学习目标和真实目标之间的不对齐被加剧了。因此，在多智能体零和游戏中，多代理IRL算法难以平衡合作与竞争的目标。为了解决这些问题，我们提出了一种新的基于展示的多代理边际Q学习（MAMQL）框架。对于每个代理来说，MAMQL学会了在一个其他代理商策略下的批评者边缘化，允许在多智能体环境中合理地使用玻尔兹曼政策。我们发现了一个最优边缘化评论家和单代理软Q IRL之间的联系，让我们可以直接应用简单的优化标准从单智能体领域。我们的实验结果表明，与现有的多代理方法相比，MAMQL的平均奖励、采样效率和奖赏恢复性能要好得多（通常为2-5倍）。我们把代码放在了 https://sites.google.com/view/mamql 上供他人使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When reward functions are hand-designed, deep reinforcement learningalgorithms often suffer from reward misspecification, causing them to learnsuboptimal policies in terms of the intended task objectives. In thesingle-agent case, inverse reinforcement learning (IRL) techniques attempt toaddress this issue by inferring the reward function from expert demonstrations.However, in multi-agent problems, misalignment between the learned and trueobjectives is exacerbated due to increased environment non-stationarity andvariance that scales with multiple agents. As such, in multi-agent general-sumgames, multi-agent IRL algorithms have difficulty balancing cooperative andcompetitive objectives. To address these issues, we propose Multi-AgentMarginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficientframework for multi-agent IRL. For each agent, MAMQL learns a criticmarginalized over the other agents' policies, allowing for a well-motivated useof Boltzmann policies in the multi-agent context. We identify a connectionbetween optimal marginalized critics and single-agent soft-Q IRL, allowing usto apply a direct, simple optimization criterion from the single-agent domain.Across our experiments on three different simulated domains, MAMQLsignificantly outperforms previous multi-agent methods in average reward,sample efficiency, and reward recovery by often more than 2-5x. We make ourcode available at https://sites.google.com/view/mamql .</description>
      <author>example@mail.com (Nathaniel Haynam, Adam Khoja, Dhruv Kumar, Vivek Myers, Erdem Bıyık)</author>
      <guid isPermaLink="false">2503.04679v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>3HANDS Dataset: Learning from Humans for Generating Naturalistic Handovers with Supernumerary Robotic Limbs</title>
      <link>http://arxiv.org/abs/2503.04635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CHI '25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;超常机器人肢体（SRLs）增强了人的物理能力，需要实现无缝、自然的人机交互。为了在物理任务中有效提供帮助，使SRL能够向人类递送物体至关重要。&lt;h4&gt;背景&lt;/h4&gt;传统的基于规则的方法设计用于机器人的手部传递动作耗时且难以跨任务泛化，并导致机器人动作不够自然。&lt;h4&gt;目的&lt;/h4&gt;提出3HANDS数据集以及使用该数据集训练的生成模型，以创建自然的手部传递轨迹、确定合适的传递结束点并预测何时开始手部传递。&lt;h4&gt;方法&lt;/h4&gt;3HANDS数据集记录了一个参与者在进行日常活动时与另一个参与者通过佩戴于髋部的SRL进行物体交接的情况。利用这个数据集开发了三个模型：生成自然传输路径、决定适当的交付终点和预测启动传递的时刻。&lt;h4&gt;主要发现&lt;/h4&gt;用户研究结果（N=10）表明，与基线方法相比，该方法的手部交互在感知上更自然、体力需求更低且更加舒适。&lt;h4&gt;结论&lt;/h4&gt;3HANDS数据集及其训练模型成功地改善了人机物体交接的自然性，为开发高效的人机协作系统提供了有力的支持。&lt;h4&gt;翻译&lt;/h4&gt;超常机器人肢体（SRLs）是紧密集成在用户身体上的机器人结构，它们增强了人的物理能力，并需要实现无缝、自然的人机交互。为了有效地辅助完成体力任务，使SRL能够向人类递送物体至关重要。然而，基于启发式的方法设计用于机器人的手部传递动作耗时且难以跨任务泛化，而且会导致机器人动作不够自然。当使用适当的数据集训练时，生成模型是创建自然手部传递运动的有力替代方案。我们引入了3HANDS，这是一个新颖的数据集，记录了一个参与者在进行日常活动时与另一个参与者通过佩戴于髋部的SRL进行物体交接的情况。3HANDS数据集捕捉了SRL交互的独特特征：在私人空间内的操作、不对称的对象起源、隐式的运动同步以及用户在传递过程中参与主要任务的状态。为了展示我们数据集的有效性，我们提出三种模型：一种生成自然手部传递轨迹的模型，另一种确定适当的传递结束点的模型，还有一种预测何时开始传递时刻的模型。在一个用户研究（N=10）中，我们将我们的方法与基线方法的手部交互进行了比较。结果表明，该方法在感知上更自然、体力需求更低且更加舒适。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3706598.3713306&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Supernumerary robotic limbs (SRLs) are robotic structures integrated closelywith the user's body, which augment human physical capabilities and necessitateseamless, naturalistic human-machine interaction. For effective assistance inphysical tasks, enabling SRLs to hand over objects to humans is crucial. Yet,designing heuristic-based policies for robots is time-consuming, difficult togeneralize across tasks, and results in less human-like motion. When trainedwith proper datasets, generative models are powerful alternatives for creatingnaturalistic handover motions. We introduce 3HANDS, a novel dataset of objecthandover interactions between a participant performing a daily activity andanother participant enacting a hip-mounted SRL in a naturalistic manner. 3HANDScaptures the unique characteristics of SRL interactions: operating in intimatepersonal space with asymmetric object origins, implicit motion synchronization,and the user's engagement in a primary task during the handover. To demonstratethe effectiveness of our dataset, we present three models: one that generatesnaturalistic handover trajectories, another that determines the appropriatehandover endpoints, and a third that predicts the moment to initiate ahandover. In a user study (N=10), we compare the handover interaction performedwith our method compared to a baseline. The findings show that our method wasperceived as significantly more natural, less physically demanding, and morecomfortable.</description>
      <author>example@mail.com (Artin Saberpour Abadian, Yi-Chi Liao, Ata Otaran, Rishabh Dabral, Marie Muehlhaus, Christian Theobalt, Martin Schmitz, Jürgen Steimle)</author>
      <guid isPermaLink="false">2503.04635v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Whole-Body Model-Predictive Control of Legged Robots with MuJoCo</title>
      <link>http://arxiv.org/abs/2503.04613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文展示了迭代LQR算法结合MuJoCo动力学和有限差分近似导数的简单方法在四足和人形机器人整体模型预测控制中的实际有效性。&lt;h4&gt;背景&lt;/h4&gt;基于之前使用MuJoCo在模拟环境中进行运动和操作任务的行为合成与控制的成功经验，研究者表明这些策略可以轻松地推广到现实世界中，而无需过多考虑仿真到真实环境的转换问题。&lt;h4&gt;目的&lt;/h4&gt;展示一种简单的方法可以在实际硬件上实现实时的整体模型预测控制，并通过多种实验验证其效果。&lt;h4&gt;方法&lt;/h4&gt;使用迭代LQR算法和MuJoCo动力学以及有限差分近似导数的方法进行整体模型预测控制，以实现在四足机器人动态行走、两腿行走及人形双足机器人的步态运动等任务中的实时控制。&lt;h4&gt;主要发现&lt;/h4&gt;提出的基线方法在不同的硬件实验中实现了实时的整体模型预测控制，并表明了仿真到现实世界的过渡可以非常简单。&lt;h4&gt;结论&lt;/h4&gt;该研究希望降低进入实际环境中进行整体模型预测控制研究的门槛，促进社区内的研究速度加快。&lt;h4&gt;翻译&lt;/h4&gt;我们展示了四足和人形机器人全身模型预测控制（MPC）中一种非常简单的、具有惊人现实世界有效性的方法：迭代LQR算法结合MuJoCo动力学以及有限差分近似导数。基于之前使用MuJoCo在模拟环境中成功进行运动和操作任务的行为合成与控制的经验，我们展示了这些策略可以轻松地推广到现实中，并且仅需很少的仿真至真实考虑即可实现这种推广。我们的基线方法实现了多种硬件实验中的实时全身MPC，包括四足机器人动态行走、两腿行走以及全尺寸人形双足步行机器人的运动任务。希望这种方法易于重现的硬件基准可以降低进入实际环境中进行全身模型预测控制研究的门槛，并有助于加速社区内的研究进展速度。我们的代码和实验视频可在以下网址获取：https://johnzhang3.github.io/mujoco_ilqr&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We demonstrate the surprising real-world effectiveness of a very simpleapproach to whole-body model-predictive control (MPC) of quadruped and humanoidrobots: the iterative LQR (iLQR) algorithm with MuJoCo dynamics andfinite-difference approximated derivatives. Building upon the previous successof model-based behavior synthesis and control of locomotion and manipulationtasks with MuJoCo in simulation, we show that these policies can easilygeneralize to the real world with few sim-to-real considerations. Our baselinemethod achieves real-time whole-body MPC on a variety of hardware experiments,including dynamic quadruped locomotion, quadruped walking on two legs, andfull-sized humanoid bipedal locomotion. We hope this easy-to-reproduce hardwarebaseline lowers the barrier to entry for real-world whole-body MPC research andcontributes to accelerating research velocity in the community. Our code andexperiment videos will be available onlineat:https://johnzhang3.github.io/mujoco_ilqr</description>
      <author>example@mail.com (John Z. Zhang, Taylor A. Howell, Zeji Yi, Chaoyi Pan, Guanya Shi, Guannan Qu, Tom Erez, Yuval Tassa, Zachary Manchester)</author>
      <guid isPermaLink="false">2503.04613v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards Multi-dimensional Elasticity for Pervasive Stream Processing Services</title>
      <link>http://arxiv.org/abs/2503.04193v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at Percom 2025 as Work in Progress (WIP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种在质量和资源维度上扩展流媒体服务的分层解决方案。&lt;h4&gt;背景&lt;/h4&gt;现代场景如智慧城市依赖于物联网数据的连续处理以提供实时服务并满足应用目标（服务水平目标SLO）。&lt;h4&gt;目的&lt;/h4&gt;提高边缘环境中的服务弹性，通过多维扩展策略来实现这一点。&lt;h4&gt;方法&lt;/h4&gt;设计了一种两层架构：本地、特定于服务的代理确保通过多种弹性和资源分配策略履行SLO；如果无法再分配更多资源，则更高层次的代理通过交换资源优化全局SLO履行。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，当操作在严格的资源限制下进行时，该方法优于传统的垂直自动扩展器。&lt;h4&gt;结论&lt;/h4&gt;基于多维度弹性策略的方法为边缘环境中服务提供了一种有效的扩展方式。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文的中文翻译：本文提出了一种分层解决方案，在质量和资源维度上扩展流媒体服务。现代场景如智慧城市依赖于物联网数据的连续处理以提供实时服务并满足应用目标（服务水平目标SLO）。虽然趋势是尽可能在附近边缘设备上处理数据，但这会形成瓶颈因为资源只能配置到有限容量。为提高边缘环境中的弹性，我们提出通过多种维度进行扩展——要么是资源，要么是服务质量。我们依靠两层架构：本地、特定于服务的代理确保通过多维弹性和资源分配策略履行SLO；如果无法再分配更多资源，则更高层次的代理通过交换资源优化全局SLO履行。实验结果表明，在严格的资源限制下操作时，该方法优于传统的垂直自动扩展器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a hierarchical solution to scale streaming servicesacross quality and resource dimensions. Modern scenarios, like smart cities,heavily rely on the continuous processing of IoT data to provide real-timeservices and meet application targets (Service Level Objectives -- SLOs). Whilethe tendency is to process data at nearby Edge devices, this creates abottleneck because resources can only be provisioned up to a limited capacity.To improve elasticity in Edge environments, we propose to scale services inmultiple dimensions -- either resources or, alternatively, the service quality.We rely on a two-layer architecture where (1) local, service-specific agentsensure SLO fulfillment through multi-dimensional elasticity strategies; if nomore resources can be allocated, (2) a higher-level agent optimizes global SLOfulfillment by swapping resources. The experimental results show promisingoutcomes, outperforming regular vertical autoscalers, when operating undertight resource constraints.</description>
      <author>example@mail.com (Boris Sedlak, Andrea Morichetta, Philipp Raith, Víctor Casamayor Pujol, Schahram Dustdar)</author>
      <guid isPermaLink="false">2503.04193v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>PLMP -- Point-Line Minimal Problems for Projective SfM</title>
      <link>http://arxiv.org/abs/2503.04351v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文对结构从运动（SfM）中点和线的排列完全被多个未校准针孔相机观测的所有最小问题进行了全面分类。&lt;h4&gt;背景&lt;/h4&gt;在视觉测量领域，研究者们关注如何通过多视角图像恢复三维场景结构。该工作聚焦于利用无约束相机阵列观察到的点和线条配置来解决SfM问题。&lt;h4&gt;目的&lt;/h4&gt;目的是识别并分类所有可能出现在未校准针孔摄像机阵列观测情况下的最小问题，这些问题是理解更复杂视觉测量场景的基础。&lt;h4&gt;方法&lt;/h4&gt;研究者通过数学分析确定了291个最小问题，并且为每个问题计算了解的数量。此外，还引入了一种基于子稳定群的方法来系统地分解最小问题并识别不完全约束条件下的最小集。&lt;h4&gt;主要发现&lt;/h4&gt;发现了291个不同类型的最小问题，其中有73个具有唯一解可以线性求解；其他非线性最小问题最多涉及9台摄像机和7个点、12条直线。这些最小问题的解决方案数量相对较低（与校准相机相比）。&lt;h4&gt;结论&lt;/h4&gt;该研究为理解未校准针孔相机阵列中的视觉测量问题提供了全面的基础，识别出的大量线性和非线性最小问题是未来算法开发的重要基础。&lt;h4&gt;翻译&lt;/h4&gt;论文团队彻底分类了所有在结构从运动（SfM）中完全观察到点和线条布局，并由多个无校准针孔摄像机观测的所有最小化问题。研究结果揭示，存在291种不同的最小化情况，其中73种具有独特的解决方案，可以线性地求解。两个线性问题是开放式的视图数量不限制的，而其他所有最小化场景最多涉及九个相机。所有的最小化场景中点和线条的数量上限分别为七和十二。论文还为每一个最小化问题计算了解决方案的数量，以评估每个问题内在难度，并且发现这个数字相对较小（例如与校准相机最小问题相比）。最后通过探索子阵列的稳定子群，开发了一种几何系统方法来1）分解最小化问题到更小的问题；2）在不充分约束的情况下识别最小化问题；3）正式证明非最简化状态。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We completely classify all minimal problems for Structure-from-Motion (SfM)where arrangements of points and lines are fully observed by multipleuncalibrated pinhole cameras. We find 291 minimal problems, 73 of which haveunique solutions and can thus be solved linearly. Two of the linear problemsallow an arbitrary number of views, while all other minimal problems have atmost 9 cameras. All minimal problems have at most 7 points and at most 12lines. We compute the number of solutions of each minimal problem, as thisgives a measurement of the problem's intrinsic difficulty, and find that thesenumber are relatively low (e.g., when comparing with minimal problems forcalibrated cameras). Finally, by exploring stabilizer subgroups ofsubarrangements, we develop a geometric and systematic way to 1) factorizeminimal problems into smaller problems, 2) identify minimal problems inunderconstrained problems, and 3) formally prove non-minimality.</description>
      <author>example@mail.com (Kim Kiehn, Albin Ahlbäck, Kathlén Kohn)</author>
      <guid isPermaLink="false">2503.04351v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>ExoNav II: Design of a Robotic Tool with Follow-the-Leader Motion Capability for Lateral and Ventral Spinal Cord Stimulation (SCS)</title>
      <link>http://arxiv.org/abs/2503.04603v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种螺旋微加工连续机器人，用于在脊髓中导航刺激电极以治疗疼痛并恢复步态。&lt;h4&gt;背景&lt;/h4&gt;传统的脊髓刺激（SCS）电极放置于背侧硬膜外空间，但运动纤维位于脊髓的腹侧和侧面。目前，SCS电极的手动导向难以触及这些位置。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在受到肌腱力作用时形成螺旋形弯曲的连续机器人，并验证其随行（FTL）运动能力。&lt;h4&gt;方法&lt;/h4&gt;通过设计刚性的外管并增加机器人的平移和旋转自由度，实现了螺旋连续机器人的制作。提出了一个基于绳索长度和几何参数与机器人轨迹及末端执行器位置之间关系的动力学模型。&lt;h4&gt;主要发现&lt;/h4&gt;基于绳索长度的方法显示了19.84毫米的偏差和14.42毫米的均方根误差（RMSE），而基于位置的方法表现更好，仅有10.54毫米的偏差和8.04毫米的均方根误差。在随行实验中，这两种方法分别显示出11.24毫米和7.32毫米的偏差，以及8.67毫米和5.18毫米的RMSE。&lt;h4&gt;结论&lt;/h4&gt;该机器人能够在模拟脊髓模型上工作，显示了其潜在的应用价值，并且通过两次随行运动试验证实了机器人的重复行为。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容描述了一种用于脊髓刺激的新技术，旨在改善现有的治疗疼痛和恢复步态的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spinal cord stimulation (SCS) electrodes are traditionally placed in thedorsal epidural space to stimulate the dorsal column fibers for pain therapy.Recently, SCS has gained attention in restoring gait. However, the motor fiberstriggering locomotion are located in the ventral and lateral spinal cord.Currently, SCS electrodes are steered manually, making it difficult to navigatethem to the lateral and ventral motor fibers in the spinal cord. In this work,we propose a helically micro-machined continuum robot that can bend in ahelical shape when subjected to actuation tendon forces. Using a stiff outertube and adding translational and rotational degrees of freedom, this helicalcontinuum robot can perform follow-the-leader (FTL) motion. We propose akinematic model to relate tendon stroke and geometric parameters of the robot'shelical shape to its acquired trajectory and end-effector position. We evaluatethe proposed kinematic model and the robot's FTL motion capabilityexperimentally. The stroke-based method, which links tendon stroke values tothe robot's shape, showed inaccuracies with a 19.84 mm deviation and an RMSE of14.42 mm for 63.6 mm of robot's length bending. The position-based method,using kinematic equations to map joint space to task space, performed betterwith a 10.54 mm deviation and an RMSE of 8.04 mm. Follow-the-leader experimentsshowed deviations of 11.24 mm and 7.32 mm, with RMSE values of 8.67 mm and 5.18mm for the stroke-based and position-based methods, respectively. Furthermore,end-effector trajectories in two FTL motion trials are compared to confirm therobot's repeatable behavior. Finally, we demonstrate the robot's operation on a3D-printed spinal cord phantom model.</description>
      <author>example@mail.com (Behnam Moradkhani, Pejman Kheradmand, Harshith Jella, Joseph Klein, Ajmal Zemmar, Yash Chitalia)</author>
      <guid isPermaLink="false">2503.04603v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>DogLegs: Robust Proprioceptive State Estimation for Legged Robots Using Multiple Leg-Mounted IMUs</title>
      <link>http://arxiv.org/abs/2503.04580v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;腿部机器人在极端环境中执行任务时，准确的本体感知状态估计至关重要。DogLegs 是一种基于四足机器人的姿态估算系统，通过融合安装在主身体上的惯性测量单元（Body-IMU）、关节编码器和多个腿部惯性测量单元（Leg-IMU）的数据，并使用扩展卡尔曼滤波器进行状态融合。&lt;h4&gt;背景&lt;/h4&gt;在极端环境中，例如复杂的地形或恶劣天气条件下，外部传感器如激光雷达和相机可能会失效，这使得腿部机器人需要依赖内部传感器来完成任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够提高腿部机器人姿态估算准确性的新方法，从而增强其在极端环境中的操作性能。&lt;h4&gt;方法&lt;/h4&gt;该系统利用Body-IMU、关节编码器以及多个Leg-IMUs的测量数据，并通过扩展卡尔曼滤波器进行状态估计。系统还包含了所有IMU框架内的误差状态，同时使用腿部力学计算相对位置约束来更新主要身体的状态以减少单独IMU框架中的误差漂移。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在不同的地形条件下，该方法的精度都优于传统的腿里程计方法（仅使用Body-IMU和关节编码器）。&lt;h4&gt;结论&lt;/h4&gt;通过融合多源传感器数据以及优化姿态估计算法，可以显著提高腿部机器人在复杂环境下的操作能力。公开的数据集将有助于研究社区进一步的研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;稳健且准确的主身体本体感觉状态估算是足式机器人执行极端环境中任务的关键因素，在这种环境下，诸如激光雷达和摄像头之类的外感知传感器可能变得不可靠。本文提出了一种针对足式机器人的状态估计系统——DogLegs，该系统结合了安装在机身上的惯性测量单元（Body-IMU）、关节编码器以及多个腿部安装的惯性测量单元（Leg-IMU）的数据，并通过扩展卡尔曼滤波器进行融合。过滤系统包含所有IMU框架内的误差状态。利用腿装IMUs来检测脚接触，因此提供零速度测量以更新Leg-IMU帧的状态。此外，我们根据腿部力学计算身体IMU和腿部IMU之间的相对位置约束，并使用这些约束更新主要身体状态以及减少各IMU帧的误差漂移。现场实验结果表明，我们的提议系统可以实现在各种地形条件下优于传统腿里程计方法（仅使用Body-IMU和关节编码器）的状态估算精度。我们将公开数据集以造福研究社区。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust and accurate proprioceptive state estimation of the main body iscrucial for legged robots to execute tasks in extreme environments whereexteroceptive sensors, such as LiDARs and cameras may become unreliable. Inthis paper, we propose DogLegs, a state estimation system for legged robotsthat fuses the measurements from a body-mounted inertial measurement unit(Body-IMU), joint encoders, and multiple leg-mounted IMUs (Leg-IMU) using anextended Kalman filter (EKF). The filter system contains the error states ofall IMU frames. The Leg-IMUs are used to detect foot contact, thereby providingzero velocity measurements to update the state of the Leg-IMU frames.Additionally, we compute the relative position constraints between the Body-IMUand Leg-IMUs by the leg kinematics and use them to update the main body stateand reduce the error drift of the individual IMU frames. Field experimentalresults have shown that our proposed system can achieve better state estimationaccuracy compared to the traditional leg odometry method (using only Body-IMUand joint encoders) across different terrains. We make our datasets publiclyavailable to benefit the research community.</description>
      <author>example@mail.com (Yibin Wu, Jian Kuang, Shahram Khorshidi, Xiaoji Niu, Lasse Klingbeil, Maren Bennewitz, Heiner Kuhlmann)</author>
      <guid isPermaLink="false">2503.04580v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Data-augmented Learning of Geodesic Distances in Irregular Domains through Soner Boundary Conditions</title>
      <link>http://arxiv.org/abs/2503.04579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;测地距离在机器人领域中扮演着关键角色，能够高效编码域的全局几何信息。最近的方法通过物理知识引导的方法求解Eikonal方程来利用神经网络近似测地距离。&lt;h4&gt;问题&lt;/h4&gt;尽管这些方法有效，但在复杂环境中训练时常常遇到不稳定的收敛情况。&lt;h4&gt;目的&lt;/h4&gt;提出一个框架，在不规则领域中使用Soner边界条件学习测地距离，并系统评估数据损失对训练稳定性和解决方案准确性的影响。&lt;h4&gt;方法&lt;/h4&gt;通过引入数据损失来改善在复杂环境中的神经网络模型的训练过程，这些数据损失能够提高收敛稳定性并减少初始化敏感性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，加入数据损失能显著提高收敛稳健性，减少训练不稳定性和对初始条件的敏感性。研究结果还指出，混合数据-物理方法可以有效增强基于学习的方法在稀疏数据下求解测地距离问题的可靠性。&lt;h4&gt;结论&lt;/h4&gt;通过结合数据和物理知识，可以更有效地解决复杂环境中的测地距离计算问题，并提高神经网络模型的学习效率和准确性。&lt;h4&gt;翻译&lt;/h4&gt;测地距离在机器人学中发挥着基础性的作用，能够有效编码域的整体几何信息。最近的技术使用深度学习方法来逼近测地距离，通过求解Eikonal方程的物理启发式手段实现。尽管这些技术已经非常成功了，但在复杂的环境中训练时常常会遇到不稳定的收敛问题。本文提出了一种框架，在不规则领域中利用Soner边界条件进行测地距离的学习，并系统性地评估数据损失对模型稳定性和精度的影响。实验表明，通过引入数据损失可以显著提高模型的稳健性，减少在复杂场景中的训练不稳定现象以及初始设置的敏感度。这一发现意味着混合的数据和物理方法能有效提升基于学习的方法解决稀疏数据下的测地距离问题的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geodesic distances play a fundamental role in robotics, as they efficientlyencode global geometric information of the domain. Recent methods use neuralnetworks to approximate geodesic distances by solving the Eikonal equationthrough physics-informed approaches. While effective, these approaches oftensuffer from unstable convergence during training in complex environments. Wepropose a framework to learn geodesic distances in irregular domains by usingthe Soner boundary condition, and systematically evaluate the impact of datalosses on training stability and solution accuracy. Our experiments demonstratethat incorporating data losses significantly improves convergence robustness,reducing training instabilities and sensitivity to initialization. Thesefindings suggest that hybrid data-physics approaches can effectively enhancethe reliability of learning-based geodesic distance solvers with sparse data.</description>
      <author>example@mail.com (Rafael I. Cabral Muchacho, Florian T. Pokorny)</author>
      <guid isPermaLink="false">2503.04579v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Omnidirectional Multi-Object Tracking</title>
      <link>http://arxiv.org/abs/2503.04565v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR 2025. The dataset and code will be made publicly  available at https://github.com/xifen523/OmniTrack&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种名为OmniTrack的全景多目标跟踪框架，该框架旨在解决现有多对象跟踪算法在处理全景图像时的问题，并引入了若干技术创新以提高性能。&lt;h4&gt;背景&lt;/h4&gt;全景图像是捕捉周围物体空间和时间关系的重要工具。然而，现有的多目标跟踪（MOT）算法大多为针孔图像设计，在视野有限的情况下效果不佳。此外，由于分辨率损失、几何变形以及光线不均匀等问题，直接将现有方法应用于全景场景会显著降低性能。&lt;h4&gt;目的&lt;/h4&gt;为了应对这些问题，本文提出了OmniTrack框架，该框架旨在提高全景多目标跟踪的精度和效率，并克服缺乏相应数据集的问题。&lt;h4&gt;方法&lt;/h4&gt;OmniTrack框架包括以下关键组件：Tracklet Management用于引入时间线索；FlexiTrack Instances支持物体定位与关联；CircularStatE Module则通过减轻图像和几何失真来增强性能。此外，为了缓解全景MOT数据不足的现状，本文还构建了QuadTrack数据集。&lt;h4&gt;主要发现&lt;/h4&gt;OmniTrack框架在公共JRDB数据集上获得了26.92%的HOTA分数（较基准提升3.43%），并在新引入的QuadTrack基准测试中达到了23.45%，超越了基础线方法6.81%&lt;h4&gt;结论&lt;/h4&gt;通过整合各种创新技术，OmniTrack框架能够有效应对全景多目标跟踪中的挑战，并且在公开数据集上表现出色。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种面向全景图像的多对象跟踪（MOT）框架OmniTrack。该框架采用Tracklet Management引入时间线索、FlexiTrack Instances实现物体定位和关联，以及CircularStatE Module缓解图像及几何失真问题，能够有效应对宽视野场景下的快速传感器运动挑战。此外，为了弥补现有的全景MOT数据集的不足，我们创建了QuadTrack数据集，这是一个由四足机器人收集的全面的数据集合，涵盖了广泛的视场、激烈的移动和复杂的环境条件等多方面难题。实验结果显示，在JRDB公开数据集中，OmniTrack框架取得了26.92%的HOTA评分（相较于基准提升了3.43%），而在新建立的QuadTrack基准测试中，该方法的表现更是达到了23.45%，超出基础线模型6.81%。我们将提供开源的数据集和代码访问链接：https://github.com/xifen523/OmniTrack&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Panoramic imagery, with its 360{\deg} field of view, offers comprehensiveinformation to support Multi-Object Tracking (MOT) in capturing spatial andtemporal relationships of surrounding objects. However, most MOT algorithms aretailored for pinhole images with limited views, impairing their effectivenessin panoramic settings. Additionally, panoramic image distortions, such asresolution loss, geometric deformation, and uneven lighting, hinder directadaptation of existing MOT methods, leading to significant performancedegradation. To address these challenges, we propose OmniTrack, anomnidirectional MOT framework that incorporates Tracklet Management tointroduce temporal cues, FlexiTrack Instances for object localization andassociation, and the CircularStatE Module to alleviate image and geometricdistortions. This integration enables tracking in large field-of-viewscenarios, even under rapid sensor motion. To mitigate the lack of panoramicMOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramicdataset collected by a quadruped robot, featuring diverse challenges such aswide fields of view, intense motion, and complex environments. Extensiveexperiments on the public JRDB dataset and the newly introduced QuadTrackbenchmark demonstrate the state-of-the-art performance of the proposedframework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing animprovement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing thebaseline by 6.81%. The dataset and code will be made publicly available athttps://github.com/xifen523/OmniTrack.</description>
      <author>example@mail.com (Kai Luo, Hao Shi, Sheng Wu, Fei Teng, Mengfei Duan, Chang Huang, Yuhang Wang, Kaiwei Wang, Kailun Yang)</author>
      <guid isPermaLink="false">2503.04565v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Image-Based Relocalization and Alignment for Long-Term Monitoring of Dynamic Underwater Environments</title>
      <link>http://arxiv.org/abs/2503.04096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种结合视觉地点识别（VPR）、特征匹配和图像分割的综合管道，用于海底生态系统的自动化管理。这种方法可以有效地识别重复访问区域，并估计刚性变换。&lt;h4&gt;背景&lt;/h4&gt;有效监测水下生态系统对于追踪环境变化、指导保护工作以及确保长期生态系统健康至关重要。然而，由于水下影像的复杂性，传统的视觉定位方法面临着重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个结合VPR、特征匹配和图像分割的方法来解决自动化管理水下生态系统的难题，并介绍了一个大规模的数据集SQUIDLE+ VPR Benchmark用于测试这种方法的有效性。&lt;h4&gt;方法&lt;/h4&gt;该研究开发了一种新的综合管道，它整合了视觉地点识别（Visual Place Recognition, VPR）、特征匹配以及基于视频的图像分割技术。此方法能够在变化多样的水下环境中有效地进行视觉定位和刚体变换估计。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验表明所提出的方法在大规模数据集SQUIDLE+ VPR Benchmark上有效，该数据集包括来自多个机器人平台的不同时间段内的多样化轨迹、重叠区域以及海床类型的数据。&lt;h4&gt;结论&lt;/h4&gt;所开发的综合管道为水下生态系统的自动化管理提供了新的可能性，并且首次构建了用于评估视觉地点识别技术的大规模水下基准数据集SQUIDLE+ VPR Benchmark。&lt;h4&gt;翻译&lt;/h4&gt;有效的监测水下生态系统对于追踪环境变化、指导保护工作以及确保长期生态系统健康至关重要。但是，由于水下图像的复杂性，使用机器人平台自动化的生态管理仍然具有挑战性。我们提出了一种结合视觉地点识别（VPR）、特征匹配和基于视频图像分割的方法来实现这一目标，并介绍了SQUIDLE+ VPR基准测试集——这是第一个大规模水下VPR基准测试集，涵盖了来自多个机器人平台的大量未结构化数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective monitoring of underwater ecosystems is crucial for trackingenvironmental changes, guiding conservation efforts, and ensuring long-termecosystem health. However, automating underwater ecosystem management withrobotic platforms remains challenging due to the complexities of underwaterimagery, which pose significant difficulties for traditional visuallocalization methods. We propose an integrated pipeline that combines VisualPlace Recognition (VPR), feature matching, and image segmentation onvideo-derived images. This method enables robust identification of revisitedareas, estimation of rigid transformations, and downstream analysis ofecosystem changes. Furthermore, we introduce the SQUIDLE+ VPR Benchmark-thefirst large-scale underwater VPR benchmark designed to leverage an extensivecollection of unstructured data from multiple robotic platforms, spanning timeintervals from days to years. The dataset encompasses diverse trajectories,arbitrary overlap and diverse seafloor types captured under varyingenvironmental conditions, including differences in depth, lighting, andturbidity. Our code is available at: https://github.com/bev-gorry/underloc</description>
      <author>example@mail.com (Beverley Gorry, Tobias Fischer, Michael Milford, Alejandro Fontan)</author>
      <guid isPermaLink="false">2503.04096v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Occlusion-Aware Consistent Model Predictive Control for Robot Navigation in Occluded Obstacle-Dense Environments</title>
      <link>http://arxiv.org/abs/2503.04563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究提出了一个考虑遮挡环境的机器人导航安全性和运动一致性的模型预测控制策略。&lt;h4&gt;背景&lt;/h4&gt;在障碍物密集且存在视线遮挡的情况下，确保机器人的安全性与导航的一致性是一个重要的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于风险区域和动态约束的模型预测控制方法来解决上述挑战，并通过实验验证其有效性。&lt;h4&gt;方法&lt;/h4&gt;{'方法1': '调整可变的风险区域以代表潜在障碍物的位置', '方法2': '在线开发动态的安全边界限制，确保机器人行动安全', '方法3': '生成多个局部最优轨迹分支（每种分支对应不同的风险区）来平衡探索与利用的关系', '方法4': '创建一个共享的共识树干以保证不同分支之间的平滑过渡并保持运动的一致性', '方法5': '使用交替方向乘子法(ADMM)将模型预测控制问题分解为可管理的小规模子问题，提高计算效率'}&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟和真实环境实验验证了所提策略的有效性，尤其是在视线受阻、障碍物密集的环境下。&lt;h4&gt;结论&lt;/h4&gt;提出的遮挡感知一致型模型预测控制(CMPC)策略能有效解决机器人导航中的安全性和运动一致性问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring safety and motion consistency for robot navigation in occluded,obstacle-dense environments is a critical challenge. In this context, thisstudy presents an occlusion-aware Consistent Model Predictive Control (CMPC)strategy. To account for the occluded obstacles, it incorporates adjustablerisk regions that represent their potential future locations. Subsequently,dynamic risk boundary constraints are developed online to ensure safety. TheCMPC then constructs multiple locally optimal trajectory branches (eachtailored to different risk regions) to balance between exploitation andexploration. A shared consensus trunk is generated to ensure smooth transitionsbetween branches without significant velocity fluctuations, further preservingmotion consistency. To facilitate high computational efficiency and ensurecoordination across local trajectories, we use the alternating direction methodof multipliers (ADMM) to decompose the CMPC into manageable sub-problems forparallel solving. The proposed strategy is validated through simulation andreal-world experiments on an Ackermann-steering robot platform. The resultsdemonstrate the effectiveness of the proposed CMPC strategy through comparisonswith baseline approaches in occluded, obstacle-dense environments.</description>
      <author>example@mail.com (Minzhe Zheng, Lei Zheng, Lei Zhu, Jun Ma)</author>
      <guid isPermaLink="false">2503.04563v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Learning Generalizable Language-Conditioned Cloth Manipulation from Long Demonstrations</title>
      <link>http://arxiv.org/abs/2503.04557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的流水线方法，用于机器人进行多步骤布料操作任务，通过自主学习基本技能来应对高维状态空间和布料动力学的挑战，并且该方法在未见过的任务上具有泛化能力。&lt;h4&gt;背景&lt;/h4&gt;目前在机器人执行多步布料操作方面存在困难，因为这些操作涉及到高维的状态空间以及复杂的行为动态。尽管端到端模仿学习技术在此领域取得了显著进步，但它们仍然难以将学到的技能应用到未见过的任务中。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来解决多步骤布料操作任务中的泛化问题，尤其是如何从长演示数据中自动学习基本技能，并利用这些技能完成新的、之前没有见过的任务。&lt;h4&gt;方法&lt;/h4&gt;首先使用常识性知识（来自大型语言模型LLM）从现有长时间演示基准中发现和学习基础技能。然后，通过高级别的基于LLM的任务规划器来组合这些基础技能以解决新任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法在学习多步骤布料操作技能方面优于基线方法，并且对于已见过或未见过的任务都表现出色。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法提供了一种有效的途径，通过自动从演示中学习基本技能并组合这些技能来解决机器人执行复杂任务时的泛化问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-step cloth manipulation is a challenging problem for robots due to thehigh-dimensional state spaces and the dynamics of cloth. Despite recentsignificant advances in end-to-end imitation learning for multi-step clothmanipulation skills, these methods fail to generalize to unseen tasks. Ourinsight in tackling the challenge of generalizable multi-step clothmanipulation is decomposition. We propose a novel pipeline that autonomouslylearns basic skills from long demonstrations and composes learned basic skillsto generalize to unseen tasks. Specifically, our method first discovers andlearns basic skills from the existing long demonstration benchmark with thecommonsense knowledge of a large language model (LLM). Then, leveraging ahigh-level LLM-based task planner, these basic skills can be composed tocomplete unseen tasks. Experimental results demonstrate that our methodoutperforms baseline methods in learning multi-step cloth manipulation skillsfor both seen and unseen tasks.</description>
      <author>example@mail.com (Hanyi Zhao, Jinxuan Zhu, Zihao Yan, Yichen Li, Yuhong Deng, Xueqian Wang)</author>
      <guid isPermaLink="false">2503.04557v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>ViT-VS: On the Applicability of Pretrained Vision Transformer Features for Generalizable Visual Servoing</title>
      <link>http://arxiv.org/abs/2503.04545v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种结合经典视觉伺服技术和学习基础方法优势的新型视觉伺服技术，通过使用预训练的视觉变换器进行语义特征提取，在无需特定任务或对象训练的情况下达到较好的性能。&lt;h4&gt;背景&lt;/h4&gt;传统的视觉伺服技术依赖于手工制作的功能，并且在面对遮挡和环境变化时表现不佳。而基于学习的方法虽然提高了鲁棒性，但通常需要大量的训练数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的视觉伺服方法，该方法结合经典和基于学习的方法的优势，在不需要特定任务或对象的额外训练的情况下提供更好的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;采用预训练的视觉变换器进行语义特征提取，并在此基础上构建视觉伺服系统。这种方法在无干扰场景下可以实现完全收敛，并且即使面对环境变化也能保持较高的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的图像基视觉伺服相比，在受到扰动的情况下，该方法最多提高了31.2%的相对改进；同时匹配了基于学习的方法中的收敛速度，而不需要任务或对象特定训练。此外，实验证明这种方法在终端执行器定位、工业盒子操作以及抓取未见过的对象方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;所提出的视觉伺服技术提供了一种平衡传统方法和基于学习的系统优势的新途径，在无需额外的任务特定训练下展示了强大的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;视觉伺服技术使机器人能够精确地将末端执行器相对于目标物体进行定位。虽然经典的方法依赖于手工制作的功能，因此可以在没有任务特定训练的情况下普遍适用，但它们通常在面对遮挡和环境变化时表现不佳。基于学习的方法提高了稳健性，但通常需要大量的训练。我们提出了一种视觉伺服方法，利用预训练的视觉变换器提取语义特征，结合了两种范式的优势，并能够超越所提供的样本进行泛化。我们的方法在未受干扰的情况下实现了完全收敛，在受到扰动时相较于传统的基于图像的视觉伺服提高了最多31.2%的相对改进。尽管没有任务或对象特定的训练需求，但仍然达到了学习方法的收敛率。实际应用评估证实了该技术在终端执行器定位、工业盒子操作以及抓取未见过的对象方面的稳健性能。我们的代码和模拟环境可在 https://alessandroscherl.github.io/ViT-VS/ 查看。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual servoing enables robots to precisely position their end-effectorrelative to a target object. While classical methods rely on hand-craftedfeatures and thus are universally applicable without task-specific training,they often struggle with occlusions and environmental variations, whereaslearning-based approaches improve robustness but typically require extensivetraining. We present a visual servoing approach that leverages pretrainedvision transformers for semantic feature extraction, combining the advantagesof both paradigms while also being able to generalize beyond the providedsample. Our approach achieves full convergence in unperturbed scenarios andsurpasses classical image-based visual servoing by up to 31.2\% relativeimprovement in perturbed scenarios. Even the convergence rates oflearning-based methods are matched despite requiring no task- orobject-specific training. Real-world evaluations confirm robust performance inend-effector positioning, industrial box manipulation, and grasping of unseenobjects using only a reference from the same category. Our code and simulationenvironment are available at: https://alessandroscherl.github.io/ViT-VS/</description>
      <author>example@mail.com (Alessandro Scherl, Stefan Thalhammer, Bernhard Neuberger, Wilfried Wöber, José Gracía-Rodríguez)</author>
      <guid isPermaLink="false">2503.04545v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks</title>
      <link>http://arxiv.org/abs/2503.04538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SRSA框架通过利用现有的技能库来解决机器人在数据高效模式下学习新任务的挑战，特别是对于接触丰富的装配任务。该方法旨在预测所有技能转移到新任务时的成功率，并据此指导技能检索过程。&lt;h4&gt;背景&lt;/h4&gt;使机器人以数据高效的方式学习新的任务是一个长期存在的挑战，尤其是在需要精确控制的接触密集型装配任务中，进展较少。&lt;h4&gt;目的&lt;/h4&gt;提出SRSA框架来解决在现有技能库中快速有效地为新任务选择相关技能的问题。&lt;h4&gt;方法&lt;/h4&gt;开发了一个框架，该框架能够预测所有技能转移到新任务时的成功率，并利用这些信息来指导技能检索过程。此外，通过联合捕捉物体几何特征、物理动力学和专家动作来表示任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SRSA在获取未见过的任务的技能并进行微调方面优于基线方法，在成功率上有19%的相对提升；标准偏差降低了2.6倍，并且达到满意的成功率达到需要更少的过渡样本（2.4倍）。&lt;h4&gt;结论&lt;/h4&gt;SRSA框架展示了其在机器人学习和适应新任务方面的潜力，特别是在装配任务中表现突出。当在仿真环境中训练时，成功率可以超过90%。&lt;h4&gt;翻译&lt;/h4&gt;使机器人以数据高效的方式学习新的任务是一个长期存在的挑战。尽管在一般的抓取放置操作方面已取得许多进展，但接触密集型的组装任务则研究较少。SRSA框架利用现有的技能库来解决这个问题，并通过预测所有技能转移到新任务时的成功率来进行有效的技能检索。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enabling robots to learn novel tasks in a data-efficient manner is along-standing challenge. Common strategies involve carefully leveraging priorexperiences, especially transition data collected on related tasks. Althoughmuch progress has been made for general pick-and-place manipulation, far fewerstudies have investigated contact-rich assembly tasks, where precise control isessential. We introduce SRSA (Skill Retrieval and Skill Adaptation), a novelframework designed to address this problem by utilizing a pre-existing skilllibrary containing policies for diverse assembly tasks. The challenge lies inidentifying which skill from the library is most relevant for fine-tuning on anew task. Our key hypothesis is that skills showing higher zero-shot successrates on a new task are better suited for rapid and effective fine-tuning onthat task. To this end, we propose to predict the transfer success for allskills in the skill library on a novel task, and then use this prediction toguide the skill retrieval process. We establish a framework that jointlycaptures features of object geometry, physical dynamics, and expert actions torepresent the tasks, allowing us to efficiently learn the transfer successpredictor. Extensive experiments demonstrate that SRSA significantlyoutperforms the leading baseline. When retrieving and fine-tuning skills onunseen tasks, SRSA achieves a 19% relative improvement in success rate,exhibits 2.6x lower standard deviation across random seeds, and requires 2.4xfewer transition samples to reach a satisfactory success rate, compared to thebaseline. Furthermore, policies trained with SRSA in simulation achieve a 90%mean success rate when deployed in the real world. Please visit our projectwebpage https://srsa2024.github.io/.</description>
      <author>example@mail.com (Yijie Guo, Bingjie Tang, Iretiayo Akinola, Dieter Fox, Abhishek Gupta, Yashraj Narang)</author>
      <guid isPermaLink="false">2503.04538v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>PALo: Learning Posture-Aware Locomotion for Quadruped Robots</title>
      <link>http://arxiv.org/abs/2503.04462v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PALo的端到端深度强化学习框架，用于四足机器人在复杂地形上的姿态感知行走控制。&lt;h4&gt;背景&lt;/h4&gt;随着嵌入式智能的发展，四足机器人的地面适应性步行控制成为研究热点。传统的步行控制系统主要关注速度跟踪，而忽视了敏捷性和鲁棒性的平衡。&lt;h4&gt;目的&lt;/h4&gt;本文旨在开发一种能够处理线性和角速度的同时追踪及实时调整身体高度、俯仰和横滚角度的系统，以实现四足机器人在各种复杂地形上的姿态感知行走控制。&lt;h4&gt;方法&lt;/h4&gt;通过将步行控制系统表述为部分可观测马尔可夫决策过程，并采用不对称演员-评论家架构来克服仿真到现实环境的挑战。此外，结合定制化的训练课程，在模拟环境中实现了敏捷的姿态感知行走控制，并成功地将其转移到真实场景中而无需微调。&lt;h4&gt;主要发现&lt;/h4&gt;通过深入实验分析，识别了PALo性能的关键组件，并进一步验证了所提出方法的有效性。研究表明，对于四足机器人在更高维度命令空间中的低级步行控制提供了新的可能。&lt;h4&gt;结论&lt;/h4&gt;该研究为未来嵌入式智能的上层模块的研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;随着嵌入式智能的快速发展，四足机器人在复杂地形上的移动控制已成为研究热点。不同于传统的仅关注速度跟踪的方法，本文旨在平衡四足机器人在多样且复杂的地面上的敏捷性和鲁棒性。为此，提出了一个名为PALo的姿态感知行走控制端到端深度强化学习框架，能够同时处理线性和角速度追踪以及实时调整身体高度、俯仰和横滚角度的问题。该方法将移动控制系统表述为部分可观测马尔可夫决策过程，并采用不对称演员-评论家架构来克服仿真到现实环境的挑战。通过定制化的训练课程，在模拟环境中实现了敏捷的姿态感知行走控制并成功地将其转移到真实场景中而无需微调，从而允许四足机器人在具有挑战性的地形上进行实时移动和身体姿态控制。深入实验分析识别了PALo性能的关键组件，并进一步验证了所提出方法的有效性。该研究为低级步行控制提供了新的可能，在更高维度的命令空间中的四足机器人，并为基础未来嵌入式智能的上层模块的研究奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of embodied intelligence, locomotion control ofquadruped robots on complex terrains has become a research hotspot. Unliketraditional locomotion control approaches focusing solely on velocity tracking,we pursue to balance the agility and robustness of quadruped robots on diverseand complex terrains. To this end, we propose an end-to-end deep reinforcementlearning framework for posture-aware locomotion named PALo, which manages tohandle simultaneous linear and angular velocity tracking and real-timeadjustments of body height, pitch, and roll angles. In PALo, the locomotioncontrol problem is formulated as a partially observable Markov decisionprocess, and an asymmetric actor-critic architecture is adopted to overcome thesim-to-real challenge. Further, by incorporating customized training curricula,PALo achieves agile posture-aware locomotion control in simulated environmentsand successfully transfers to real-world settings without fine-tuning, allowingreal-time control of the quadruped robot's locomotion and body posture acrosschallenging terrains. Through in-depth experimental analysis, we identify thekey components of PALo that contribute to its performance, further validatingthe effectiveness of the proposed method. The results of this study provide newpossibilities for the low-level locomotion control of quadruped robots inhigher dimensional command spaces and lay the foundation for future research onupper-level modules for embodied intelligence.</description>
      <author>example@mail.com (Xiangyu Miao, Jun Sun, Hang Lai, Xinpeng Di, Jiahang Cao, Yong Yu, Weinan Zhang)</author>
      <guid isPermaLink="false">2503.04462v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>EvidMTL: Evidential Multi-Task Learning for Uncertainty-Aware Semantic Surface Mapping from Monocular RGB Images</title>
      <link>http://arxiv.org/abs/2503.04441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS 2025 Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;针对复杂环境中场景理解的需求，提出了一种不确定性感知的多任务学习框架EvidMTL，该框架结合了单目RGB图像进行深度估计和语义分割，并且能够从证据理论的角度提供不确定性的评估。&lt;h4&gt;背景&lt;/h4&gt;现有映射方法在处理非结构化环境时，经常会产生过度自信的语义预测以及稀疏、噪声较多的深度信息，从而导致地图表示不一致的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多任务学习框架EvidMTL和不确定性感知的语义表面映射框架EvidKimera，以提高非结构化环境中的场景理解能力。&lt;h4&gt;方法&lt;/h4&gt;EvidMTL使用证据头部进行深度估计和语义分割，从而能够从单目RGB图像中提供不确定性的意识推断。提出了一种新颖的证据深度损失函数，该函数同时优化深度预测的信心强度以及与证据语义分割损失结合的优化。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明EvidMTL在NYUDepthV2数据集上的训练和评估中具有优秀的不确定性估计性能，并且在ScanNetV2上进行零样本映射测试时，EvidKimera的语义表面映射准确性和一致性优于传统的Kimera。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够提高场景理解中的深度估计、语义分割以及3D度量-语义一致性，其不确定性感知的能力对于实际应用中的机器人系统具有重要的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For scene understanding in unstructured environments, an accurate anduncertainty-aware metric-semantic mapping is required to enable informed actionselection by autonomous systems.Existing mapping methods often suffer fromoverconfident semantic predictions, and sparse and noisy depth sensing, leadingto inconsistent map representations. In this paper, we therefore introduceEvidMTL, a multi-task learning framework that uses evidential heads for depthestimation and semantic segmentation, enabling uncertainty-aware inference frommonocular RGB images. To enable uncertainty-calibrated evidential multi-tasklearning, we propose a novel evidential depth loss function that jointlyoptimizes the belief strength of the depth prediction in conjunction withevidential segmentation loss. Building on this, we present EvidKimera, anuncertainty-aware semantic surface mapping framework, which uses evidentialdepth and semantics prediction for improved 3D metric-semantic consistency. Wetrain and evaluate EvidMTL on the NYUDepthV2 and assess its zero-shotperformance on ScanNetV2, demonstrating superior uncertainty estimationcompared to conventional approaches while maintaining comparable depthestimation and semantic segmentation. In zero-shot mapping tests on ScanNetV2,EvidKimera outperforms Kimera in semantic surface mapping accuracy andconsistency, highlighting the benefits of uncertainty-aware mapping andunderscoring its potential for real-world robotic applications.</description>
      <author>example@mail.com (Rohit Menon, Nils Dengler, Sicong Pan, Gokul Krishna Chenchani, Maren Bennewitz)</author>
      <guid isPermaLink="false">2503.04441v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>On the Analysis of Stability, Sensitivity and Transparency in Variable Admittance Control for pHRI Enhanced by Virtual Fixtures</title>
      <link>http://arxiv.org/abs/2503.04414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;近年来，由于协作机器人确保了在力交互过程中的用户安全，物理人机交互（pHRI）的兴趣显著增加。因此，在提出新的pHRI应用控制方案时，稳定性问题已经得到了广泛研究。&lt;h4&gt;目的&lt;/h4&gt;本文的主要目的是通过考虑寄生效应（如传动弹性、电机速度饱和和驱动延迟），对一类基于代理的约束顺应性控制器进行详细的不稳定性来源分析，并确定控制参数如何影响整个系统的稳定性。此外，还提出了改进透明度的方法。&lt;h4&gt;方法&lt;/h4&gt;首先进行了详细的不稳定性的来源分析；然后通过实验结果支持的敏感性分析来识别控制参数对系统稳定性的影响；最后提出了一种基于代理参数调整的技术以最大化pHRI中的透明度，并通过仿真和实验验证了该技术的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;针对具体控制方案，揭示了一些影响其稳定性的因素（如传动弹性、电机速度饱和及驱动延迟），并通过敏感性分析确定了这些寄生效应如何影响系统稳定性。此外，提出了一种改进透明度的技术，并通过实验和仿真验证了该技术的有效性。&lt;h4&gt;结论&lt;/h4&gt;本研究为pHRI控制系统的设计提供了重要的见解，特别是关于不稳定性的根源以及提高稳定性和交互透明度的方法。&lt;h4&gt;翻译&lt;/h4&gt;过去二十年中，物理人机交互（Physical Human-Robot Interaction, pHRI）的兴趣显著增加。由于协作机器人的可用性，它们在力交换过程中能够保证用户的安全。因此，在文献中广泛研究了稳定性问题，并提出了新的pHRI应用控制方案。然而，由于机器人本身的非线性特性，稳定性分析通常依赖于被动性的概念。另一方面，所提出的算法一般考虑的是机器人操作臂的理想模型。考虑到这一点，本文的主要目标是通过考虑寄生效应（如传动弹性、电机速度饱和和驱动延迟）来对一种特定的pHRI控制方案——基于代理的约束顺应性控制器进行详细的不稳定性来源分析，并确定控制参数如何影响整个系统的稳定性。接着，进行了由实验结果支持的敏感性分析，以识别控制参数对系统稳定性的效果。最后提出了用于最大化pHRI透明度的代理参数调整技术，并通过仿真和实验测试验证了该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The interest in Physical Human-Robot Interaction (pHRI) has significantlyincreased over the last two decades thanks to the availability of collaborativerobots that guarantee user safety during force exchanges. For this reason,stability concerns have been addressed extensively in the literature whileproposing new control schemes for pHRI applications. Because of the nonlinearnature of robots, stability analyses generally leverage passivity concepts. Onthe other hand, the proposed algorithms generally consider ideal models ofrobot manipulators. For this reason, the primary objective of this paper is toconduct a detailed analysis of the sources of instability for a class of pHRIcontrol schemes, namely proxy-based constrained admittance controllers, byconsidering parasitic effects such as transmission elasticity, motor velocitysaturation, and actuation delay. Next, a sensitivity analysis supported byexperimental results is carried out, in order to identify how the controlparameters affect the stability of the overall system. Finally, an adaptationtechnique for the proxy parameters is proposed with the goal of maximizingtransparency in pHRI. The proposed adaptation method is validated through bothsimulations and experimental tests.</description>
      <author>example@mail.com (Davide Tebaldi, Dario Onfiani, Luigi Biagiotti)</author>
      <guid isPermaLink="false">2503.04414v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>SeGMan: Sequential and Guided Manipulation Planner for Robust Planning in 2D Constrained Environments</title>
      <link>http://arxiv.org/abs/2503.04409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SeGMan是一种结合了采样和优化技术的混合运动规划框架，适用于解决复杂的顺序操作挑战。&lt;h4&gt;背景&lt;/h4&gt;现有的方法在处理复杂且受限的顺序操作任务（如拾取和放置谜题）时存在不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的运动规划框架，以提高此类问题的效率和性能。&lt;h4&gt;方法&lt;/h4&gt;SeGMan结合了基于采样的技术和优化技术，并使用引导式向前搜索。此外，它采用了一种自适应子目标选择方法来调整子目标的粒度。&lt;h4&gt;主要发现&lt;/h4&gt;在布满物体和障碍物的迷宫样任务中进行了广泛评估，表明SeGMan能够生成一致且计算效率高的操作计划，并优于现有最佳的方法。&lt;h4&gt;结论&lt;/h4&gt;SeGMan提供了一种高效且通用的方法来解决复杂的顺序操作问题。&lt;h4&gt;翻译&lt;/h4&gt;在这篇文章中，我们介绍了SeGMan，这是一种混合运动规划框架，它将基于采样的技术和优化技术与引导式向前搜索相结合，以应对复杂、受限的序列操控挑战，例如拾取和放置谜题。该框架集成了自适应子目标选择方法，可以调整子目标的粒度，从而提高整体效率。此外，提出的通用启发式方法使得向前搜索更加有针对性。在迷宫样任务中进行大量评估，这些任务包含许多物体和障碍物，结果表明SeGMan不仅能够生成一致且计算高效的操控计划，而且还能超越最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present SeGMan, a hybrid motion planning framework thatintegrates sampling-based and optimization-based techniques with a guidedforward search to address complex, constrained sequential manipulationchallenges, such as pick-and-place puzzles. SeGMan incorporates an adaptivesubgoal selection method that adjusts the granularity of subgoals, enhancingoverall efficiency. Furthermore, proposed generalizable heuristics guide theforward search in a more targeted manner. Extensive evaluations in maze-liketasks populated with numerous objects and obstacles demonstrate that SeGMan iscapable of generating not only consistent and computationally efficientmanipulation plans but also outperform state-of-the-art approaches.</description>
      <author>example@mail.com (Cankut Bora Tuncer, Dilruba Sultan Haliloglu, Ozgur S. Oguz)</author>
      <guid isPermaLink="false">2503.04409v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Energy Consumption of Robotic Arm with the Local Reduction Method</title>
      <link>http://arxiv.org/abs/2503.04340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种局部减少法，用于优化机械臂系统的能效，这种方法在不牺牲性能的情况下减少了高达25%的能耗。&lt;h4&gt;背景&lt;/h4&gt;随着运营成本和环境影响的增加，工业自动化中机器人的能源消耗成为一个重要的问题。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在通过一种新的方法——局部减少法，来提高机器人系统在保持精度和操作可靠性的同时的能效。&lt;h4&gt;方法&lt;/h4&gt;对一个三关节机械臂模型进行了模拟实验，该模型执行了30秒内的取放任务和轨迹跟踪等不同任务。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的MPC（预测模型控制）和GA（遗传算法）相比，局部减少法显著降低了25%的能耗，并且展示了更好的适应性和计算效率。此外，这种新方法易于集成到新兴技术如人工智能中。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了局部减少法作为一种优化机器人操作、降低能源需求并促进工业自动化可持续性的实用工具的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种通过局部减少法来提高机械臂系统能效的研究。这项技术在不牺牲性能的情况下，大大降低了能耗，并且易于集成到其他新兴技术中以进一步增强其应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Energy consumption in robotic arms is a significant concern in industrialautomation due to rising operational costs and environmental impact. This studyinvestigates the use of a local reduction method to optimize energy efficiencyin robotic systems without compromising performance. The approach refinesmovement parameters, minimizing energy use while maintaining precision andoperational reliability. A three-joint robotic arm model was tested usingsimulation over a 30-second period for various tasks, including pick-and-placeand trajectory-following operations. The results revealed that the localreduction method reduced energy consumption by up to 25% compared totraditional techniques such as Model Predictive Control (MPC) and GeneticAlgorithms (GA). Unlike MPC, which requires significant computationalresources, and GA, which has slow convergence rates, the local reduction methoddemonstrated superior adaptability and computational efficiency in real-timeapplications. The study highlights the scalability and simplicity of the localreduction approach, making it an attractive option for industries seekingsustainable and cost-effective solutions. Additionally, this method canintegrate seamlessly with emerging technologies like Artificial Intelligence(AI), further enhancing its application in dynamic and complex environments.This research underscores the potential of the local reduction method as apractical tool for optimizing robotic arm operations, reducing energy demands,and contributing to sustainability in industrial automation. Future work willfocus on extending the approach to real-world scenarios and incorporatingAI-driven adjustments for more dynamic adaptability.</description>
      <author>example@mail.com (Halima Ibrahim Kure, Jishna Retnakumari, Lucian Nita, Saeed Sharif, Hamed Balogun, Augustine O. Nwajana)</author>
      <guid isPermaLink="false">2503.04340v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Shaken, Not Stirred: A Novel Dataset for Visual Understanding of Glasses in Human-Robot Bartending Tasks</title>
      <link>http://arxiv.org/abs/2503.04308v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE/RSJ International Conference on Intelligent Robots  and Systems (IROS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于收集真实世界数据的方法，该方法使用RGB-D传感器并减少了人力投入。通过一个自动标注流水线，基于深度测量生成所有获取帧的标签。&lt;h4&gt;背景&lt;/h4&gt;现有的对象检测数据集往往未能充分涵盖眼镜的不同种类，这是由于眼镜具有透明和反射特性所致。特别是在广泛用于具身机器人代理中的开放词汇表目标检测器中，无法区分眼镜的子类别。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效采集并标注包含多种类型眼镜的真实世界数据的方法，并评估这种方法在实际应用中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一个自动标签生成流水线，该流水线基于深度测量为所有获取到的帧创建标签。同时提供了一个新的真实世界玻璃物体数据集，该数据集是在NICOL（神经启发型协作机器人）平台上收集的，包含7850张图像，来自五个不同的相机。&lt;h4&gt;主要发现&lt;/h4&gt;训练后的基线模型在开放词汇表目标检测方法中表现优于现有最佳方法，并且在人类-机器人调酒场景中，在NICOL平台上的成功率为81%。&lt;h4&gt;结论&lt;/h4&gt;本文的方法不仅改进了数据采集和标注流程，还展示了其在实际具身代理应用中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Datasets for object detection often do not account for enough variety ofglasses, due to their transparent and reflective properties. Specifically,open-vocabulary object detectors, widely used in embodied robotic agents, failto distinguish subclasses of glasses. This scientific gap poses an issue torobotic applications that suffer from accumulating errors between detection,planning, and action execution. The paper introduces a novel method for theacquisition of real-world data from RGB-D sensors that minimizes human effort.We propose an auto-labeling pipeline that generates labels for all the acquiredframes based on the depth measurements. We provide a novel real-world glassobject dataset that was collected on the Neuro-Inspired COLlaborator (NICOL), ahumanoid robot platform. The data set consists of 7850 images recorded fromfive different cameras. We show that our trained baseline model outperformsstate-of-the-art open-vocabulary approaches. In addition, we deploy ourbaseline model in an embodied agent approach to the NICOL platform, on which itachieves a success rate of 81% in a human-robot bartending scenario.</description>
      <author>example@mail.com (Lukáš Gajdošech, Hassan Ali, Jan-Gerrit Habekost, Martin Madaras, Matthias Kerzel, Stefan Wermter)</author>
      <guid isPermaLink="false">2503.04308v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Manipulation of Elasto-Flexible Cables with Single or Multiple UAVs</title>
      <link>http://arxiv.org/abs/2503.04304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了一类由多个四旋翼无人机操纵可变形和伸缩电缆的系统，并通过数值模拟验证了基于平滑度特性的轨迹生成的有效性。&lt;h4&gt;背景&lt;/h4&gt;处理由多架四旋翼无人机操作，具有复杂动态特性的长柔性和可扩展电缆系统的挑战。&lt;h4&gt;目的&lt;/h4&gt;为此类系统找到合适的数学模型并提出有效的控制策略。&lt;h4&gt;方法&lt;/h4&gt;采用离散化表示法来描述电缆，并将其分解成线性弹簧通过集中质量的被动球形关节相连。研究发现了这些系统的平面输出集，同时提出了基于反馈的闭环控制器以实现更好的操作效果。&lt;h4&gt;主要发现&lt;/h4&gt;数值仿真显示了依靠平滑度特性的轨迹可以成功地进行电缆操纵。实验验证了两个机器人示例中的离散化电缆模型的有效性，并测试了一个基于识别模型和使用电缆输出反馈的闭环控制器。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种有效的数学模型来描述四旋翼无人机操作下的可变形电缆系统，通过理论分析、数值模拟以及实际试验三方面验证了模型的有效性和控制策略的实际可行性。&lt;h4&gt;翻译&lt;/h4&gt;这项工作考虑了一系列由多个四旋翼无人机操纵的具有可变形和可伸缩特性的绳索系统的大型类别。该缆线采用离散化表示法描述，并将其分解为通过集中质量的被动球形关节相连的线性弹簧。对于这些系统，发现了平滑输出集。数值模拟支持了这一发现，展示了依靠基于平滑度的轨迹生成进行的缆线操作。最终，提出了两个机器人示例中所用离散化缆线模型有效性的实验验证，并且测试了一个基于识别模型和使用缆线反馈的闭环控制器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work considers a large class of systems composed of multiple quadrotorsmanipulating deformable and extensible cables. The cable is described via adiscretized representation, which decomposes it into linear springsinterconnected through lumped-mass passive spherical joints. Sets of flatoutputs are found for the systems. Numerical simulations support the findingsby showing cable manipulation relying on flatness-based trajectories.Eventually, we present an experimental validation of the effectiveness of theproposed discretized cable model for a two-robot example. Moreover, aclosed-loop controller based on the identified model and using cable-outputfeedback is experimentally tested.</description>
      <author>example@mail.com (Chiara Gabellieri, Lars Teeuwen, Yaolei Shen, Antonio Franchi)</author>
      <guid isPermaLink="false">2503.04304v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>The Role of Robot Competence, Autonomy, and Personality on Trust Formation in Human-Robot Interaction</title>
      <link>http://arxiv.org/abs/2503.04296v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了机器人能力、自主性和个性特征在任务导向的人机交互中对人类信任态度（认知和情感信任）及行为（任务委托）的影响。&lt;h4&gt;背景&lt;/h4&gt;以往的研究探索了影响总体信任的态度的机器人的特性，但是关于这些因素是否会影响行为信任以及它们如何影响人类将新任务委托给机器人的意愿尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;研究机器人能力、自主性和个性特征对认知和情感信任及任务委托行为的影响。&lt;h4&gt;方法&lt;/h4&gt;在任务导向的人机交互背景下进行的研究。&lt;h4&gt;主要发现&lt;/h4&gt;{'机器人能力': '是决定信任的关键因素，影响认知、情感以及行为信任。', '机器人的个性特质': '仅显著影响情感信任而不影响认知信任或信任行为。', '自主性': '调节能力和认知信任之间的关系，也调节个性和情感信任之间的关系。', '认知信任': '对任务委托有积极的影响，而情感信任没有显示显著效果。'}&lt;h4&gt;结论&lt;/h4&gt;本文为人类与机器人之间的相互信任提供了新的证据，并有助于设计能有效与人类互动并增强其信任的机器人。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容已经按照要求进行了分点总结，每个要点对应一个键值对。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human trust in social robots is a complex attitude based on cognitive andemotional evaluations, as well as a behavior, like task delegation. Whileprevious research explored the features of robots that influence overall trustattitude, it remains unclear whether these features affect behavioral trust.Additionally, there is limited investigation into which features of robotsinfluence cognitive and emotional attitudes, and how these attitudes impacthumans' willingness to delegate new tasks to robots. This study examines theinterplay between competence, autonomy, and personality traits of robots andtheir impact on trust attitudes (cognitive and affective trust) and trustbehavior (task delegation), within the context of task-oriented Human-RobotInteraction. Our findings indicate that robot competence is a key determinantof trust, influencing cognitive, affective, and behavioral trust. In contrast,robot personality traits significantly impact only affective trust withoutaffecting cognitive trust or trust behavior. In addition, autonomy was found tomoderate the relationship between competence and cognitive trust, as well asbetween personality and affective trust. Finally, cognitive trust was found topositively influence task delegation, whereas affective trust did not show asignificant effect. This paper contributes to the literature on Human-RobotTrust by providing novel evidence for the design of robots that can interacteffectively with humans and enhance their trust.</description>
      <author>example@mail.com (Filippo Cantucci, Marco Marini, Rino Falcone)</author>
      <guid isPermaLink="false">2503.04296v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation with Large Language Models</title>
      <link>http://arxiv.org/abs/2503.04280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;最近在大型语言模型（LLMs）和视觉语言模型（VLMs）方面的发展极大地影响了机器人技术，使得高阶语义运动规划成为可能。强化学习作为补充范式，使代理能够通过交互和奖励信号自主优化复杂行为。&lt;h4&gt;背景&lt;/h4&gt;尽管强化学习具有强大的潜力，设计有效的奖励函数仍然是一项挑战，特别是在现实世界任务中，稀疏奖励不足以推动学习进程，而密集奖励又需要详细的设计。&lt;h4&gt;目的&lt;/h4&gt;本文提出了一种名为ARCHIE的无监督管道方法，利用预训练的语言模型GPT-4从自然语言的任务描述生成奖励函数。这种方法旨在将人类可读的文本自动转换为机器人可以执行的实际技能。&lt;h4&gt;方法&lt;/h4&gt;通过模拟环境中的实验验证了该方法的有效性。具体而言，GPT-4不仅能够根据任务描述生成奖励函数，还负责编码任务成功标准，从而实现从文字到机器人操作能力的一次性自动化流程。&lt;h4&gt;主要发现&lt;/h4&gt;使用ABB YuMi协作机器人的单臂和双臂手动操作任务进行的大量模拟实验表明了该方法的实际应用价值及其有效性。此外，在真实设备上进行了演示以验证其效果。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了如何利用先进的语言处理技术来解决强化学习中奖励设计的问题，为机器人自主性和灵活性开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;最近在大型语言模型（LLMs）和视觉语言模型（VLMs）方面的发展极大地影响了机器人技术。这些进展使得高阶语义运动规划成为可能，并通过结合使用自然语言处理技术和强化学习的方法来解决现实世界问题的挑战，尤其是那些涉及复杂行为优化的任务中稀疏奖励不足且密集奖励难以设计的情况。本文提出了一种利用GPT-4生成任务特定奖励函数和任务成功标准的方法，使机器人能够从人类可读的语言描述直接转换成执行操作的能力，并通过模拟环境中的实验验证了该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Large Language Models (LLMs) and Visual LanguageModels (VLMs) have significantly impacted robotics, enabling high-levelsemantic motion planning applications. Reinforcement Learning (RL), acomplementary paradigm, enables agents to autonomously optimize complexbehaviors through interaction and reward signals. However, designing effectivereward functions for RL remains challenging, especially in real-world taskswhere sparse rewards are insufficient and dense rewards require elaboratedesign. In this work, we propose Autonomous Reinforcement learning for ComplexHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,a pre-trained LLM, to generate reward functions directly from natural languagetask descriptions. The rewards are used to train RL agents in simulatedenvironments, where we formalize the reward generation process to enhancefeasibility. Additionally, GPT-4 automates the coding of task success criteria,creating a fully automated, one-shot procedure for translating human-readabletext into deployable robot skills. Our approach is validated through extensivesimulated experiments on single-arm and bi-manual manipulation tasks using anABB YuMi collaborative robot, highlighting its practicality and effectiveness.Tasks are demonstrated on the real robot setup.</description>
      <author>example@mail.com (Niccolò Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco)</author>
      <guid isPermaLink="false">2503.04280v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Simulation-based Analysis Of Highway Trajectory Planning Using High-Order Polynomial For Highly Automated Driving Function</title>
      <link>http://arxiv.org/abs/2503.04159v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于高阶多项式的自动车道变换轨迹规划方法，旨在提高高度自动化驾驶功能（HADF）在高速公路上的安全性和稳定性。&lt;h4&gt;背景&lt;/h4&gt;自主驾驶的一个基本任务是安全的路径规划，在此过程中需要避开障碍物、遵守交通规则并尊重道路的基本限制。实际应用中需考虑周围环境和车辆行为如车道变换、碰撞避免及车道合并等。&lt;h4&gt;目的&lt;/h4&gt;开发一种在高速公路上实现安全且无碰撞的车道变换轨迹的方法，以提高高度自动化驾驶功能的安全性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;设计了一个行为规划模块（BPM），该模块根据周围环境情况和车辆动作来制定高阶驾驶策略。利用多项式算法为同一方向双车道高速公路场景生成理想轨迹，并通过MATLAB仿真验证结果。&lt;h4&gt;主要发现&lt;/h4&gt;提出的车道变换方案经过建模与分析后，在安全性和稳定性方面取得了显著改进。&lt;h4&gt;结论&lt;/h4&gt;基于高阶多项式的规划系统能够有效地减少复杂性，支持快速计算，并且在特定环境条件下为高度自动化驾驶功能提供了安全的车道变换路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/iceccme52200.2021.9591044&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One of the fundamental tasks of autonomous driving is safe trajectoryplanning, the task of deciding where the vehicle needs to drive, while avoidingobstacles, obeying safety rules, and respecting the fundamental limits of road.Real-world application of such a method involves consideration of surroundingenvironment conditions and movements such as Lane Change, collision avoidance,and lane merge. The focus of the paper is to develop and implement safecollision free highway Lane Change trajectory using high order polynomial forHighly Automated Driving Function (HADF). Planning is often considered as ahigher-level process than control. Behavior Planning Module (BPM) is designedthat plans the high-level driving actions like Lane Change maneuver to safelyachieve the functionality of transverse guidance ensuring safety of the vehicleusing motion planning in a scenario including environmental situation. Based onthe recommendation received from the (BPM), the function will generate a desirecorresponding trajectory. The proposed planning system is situation specificwith polynomial based algorithm for same direction two lane highway scenario.To support the trajectory system polynomial curve can be used to reducesoverall complexity and thereby allows rapid computation. The proposed LaneChange scenario is modeled, and results has been analyzed (verified andvalidate) through the MATLAB simulation environment. The method proposed inthis paper has achieved a significant improvement in safety and stability ofLane Changing maneuver.</description>
      <author>example@mail.com (Milin Patel, Marzana Khatun, Rolf Jung, Michael Glaß)</author>
      <guid isPermaLink="false">2503.04159v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Mixed Likelihood Variational Gaussian Processes</title>
      <link>http://arxiv.org/abs/2503.04138v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;高斯过程（GPs）在人类参与的实验中是一种强大的模型，但由于它们通常忽略辅助信息，如先验领域知识和非任务性能信息（例如用户信心评分），其应用受到了限制。本文提出了一种混合似然变分GP的方法，通过结合单一证据下界中的多个似然函数来利用这些辅助信息。&lt;h4&gt;背景&lt;/h4&gt;高斯过程由于其灵活性和良好的不确定性校准，在人类参与的实验中是强大的模型选择。然而，现有的GPs在建模人类响应时通常忽略了先验领域知识以及用户信心评分等非任务性能信息。&lt;h4&gt;目的&lt;/h4&gt;提出了一种利用辅助信息的方法——混合似然变分GP，并通过三个真实世界的人类参与者实验展示了这种方法的益处。&lt;h4&gt;方法&lt;/h4&gt;1. 在视觉感知任务中，使用混合似然训练对高斯过程分类器施加先验知识约束，加速主动学习。            2. 利用Likert尺度信心评分并通过混合似然训练改进表面粗糙度的触觉感知模型拟合。            3. 表明用户信心评分可以改善机器人步态优化中的人类偏好学习。&lt;h4&gt;主要发现&lt;/h4&gt;通过将这些不同的输入类型联合建模，使用混合似然的方法在主动学习和偏好学习中利用辅助信息带来了性能提升。具体来说，在视觉感知任务中的快速学习、触觉感知的改进模型拟合以及用户信心评分对机器人步态优化的人类偏好学习的影响。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法展示了将先验知识和其他类型的非任务性能数据纳入GP建模的重要性，这有助于提高人类参与实验的研究效率和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gaussian processes (GPs) are powerful models for human-in-the-loopexperiments due to their flexibility and well-calibrated uncertainty. However,GPs modeling human responses typically ignore auxiliary information, includinga priori domain expertise and non-task performance information like userconfidence ratings. We propose mixed likelihood variational GPs to leverageauxiliary information, which combine multiple likelihoods in a single evidencelower bound to model multiple types of data. We demonstrate the benefits ofmixing likelihoods in three real-world experiments with human participants.First, we use mixed likelihood training to impose prior knowledge constraintsin GP classifiers, which accelerates active learning in a visual perceptiontask where users are asked to identify geometric errors resulting from cameraposition errors in virtual reality. Second, we show that leveraging Likertscale confidence ratings by mixed likelihood training improves model fittingfor haptic perception of surface roughness. Lastly, we show that Likert scaleconfidence ratings improve human preference learning in robot gaitoptimization. The modeling performance improvements found using our frameworkacross this diverse set of applications illustrates the benefits ofincorporating auxiliary information into active learning and preferencelearning by using mixed likelihoods to jointly model multiple inputs.</description>
      <author>example@mail.com (Kaiwen Wu, Craig Sanders, Benjamin Letham, Phillip Guan)</author>
      <guid isPermaLink="false">2503.04138v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>DVM-SLAM: Decentralized Visual Monocular Simultaneous Localization and Mapping for Multi-Agent Systems</title>
      <link>http://arxiv.org/abs/2503.04126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了Decentralized Visual Monocular SLAM (DVM-SLAM)，这是一个开放源代码的分散式单目视觉C-SLAM系统，适用于低成本、轻量级的小型机器人和微型飞行器。&lt;h4&gt;背景&lt;/h4&gt;Cooperative Simultaneous Localization and Mapping（协作的同时定位与地图构建）使多个代理能够合作在未知环境中进行地图构建并同时估计各自的位置。这种做法通过共享信息来提高鲁棒性、可扩展性和准确性，减少漂移，并支持更大区域的集体探索。&lt;h4&gt;目的&lt;/h4&gt;开发适用于小型机器人和微型飞行器的小型化单目视觉传感器C-SLAM系统，以实现多代理自主导航的实际应用。&lt;h4&gt;方法&lt;/h4&gt;构建了一个分散式的单目视觉SLAM系统DVM-SLAM，在物理机器人上进行了测试，并采用了一种定制的碰撞避免框架来验证其现实世界的适用性。&lt;h4&gt;主要发现&lt;/h4&gt;展示了DVM-SLAM与最先进的集中式单目C-SLAM系统的精度相当，证明了该方法的有效性和准确性。&lt;h4&gt;结论&lt;/h4&gt;通过开放源代码和在线提供补充材料，使其他研究者能够利用并改进这项技术。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到的是一种名为Decentralized Visual Monocular SLAM（分散式单目视觉SLAM）的技术，这是第一个开源的、基于单目的协作同时定位与地图构建系统。该系统适用于低成本且轻量级的小型机器人和微型飞行器，并通过实际物理机器人的测试验证了其在实时多代理自主导航场景中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cooperative Simultaneous Localization and Mapping (C-SLAM) enables multipleagents to work together in mapping unknown environments while simultaneouslyestimating their own positions. This approach enhances robustness, scalability,and accuracy by sharing information between agents, reducing drift, andenabling collective exploration of larger areas. In this paper, we presentDecentralized Visual Monocular SLAM (DVM-SLAM), the first open-sourcedecentralized monocular C-SLAM system. By only utilizing low-cost andlight-weight monocular vision sensors, our system is well suited for smallrobots and micro aerial vehicles (MAVs). DVM-SLAM's real-world applicability isvalidated on physical robots with a custom collision avoidance framework,showcasing its potential in real-time multi-agent autonomous navigationscenarios. We also demonstrate comparable accuracy to state-of-the-artcentralized monocular C-SLAM systems. We open-source our code and providesupplementary material online.</description>
      <author>example@mail.com (Joshua Bird, Jan Blumenkamp, Amanda Prorok)</author>
      <guid isPermaLink="false">2503.04126v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>GAGrasp: Geometric Algebra Diffusion for Dexterous Grasping</title>
      <link>http://arxiv.org/abs/2503.04123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种用于灵巧抓握生成的框架GAGrasp，该框架利用了几何代数表示以使SE(3)变换下的等变性得以实现。&lt;h4&gt;背景&lt;/h4&gt;现有的方法在面对多样化的物体姿态时数据效率和参数效率较低，并且难以保证产生的抓取方案具有物理合理性和稳定性。&lt;h4&gt;目的&lt;/h4&gt;通过直接将SE(3)对称约束编码到架构中，以提高数据效率、参数效率并增强不同对象姿态下的灵巧抓握生成能力。&lt;h4&gt;方法&lt;/h4&gt;引入了一个可微分的物理信息细化层，确保产生的抓取方案在物理上是合理的且稳定可靠的。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验展示了该模型在泛化性、稳定性以及适应性方面的卓越性能，并优于现有的其他方法。&lt;h4&gt;结论&lt;/h4&gt;GAGrasp框架通过利用几何代数和引入可微分的物理信息细化层，显著提高了灵巧抓握生成的质量与效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的框架GAGrasp，该框架采用几何代数表示来确保SE(3)变换下的等变性。我们的方法在直接将SE(3)对称约束编码到架构中后，不仅提升了数据和参数的效率，还增强了生成抓握方案的鲁棒性和广泛适应性。此外，我们引入了一个可微分的物理信息细化层，确保了产生的抓取方案是合理的且稳定的。大量的实验显示，与现有方法相比，该模型在泛化性、稳定性及适应性方面具有显著的优势。更多细节参见https://gagrasp.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose GAGrasp, a novel framework for dexterous grasp generation thatleverages geometric algebra representations to enforce equivariance to SE(3)transformations. By encoding the SE(3) symmetry constraint directly into thearchitecture, our method improves data and parameter efficiency while enablingrobust grasp generation across diverse object poses. Additionally, weincorporate a differentiable physics-informed refinement layer, which ensuresthat generated grasps are physically plausible and stable. Extensiveexperiments demonstrate the model's superior performance in generalization,stability, and adaptability compared to existing methods. Additional details athttps://gagrasp.github.io/</description>
      <author>example@mail.com (Tao Zhong, Christine Allen-Blanchette)</author>
      <guid isPermaLink="false">2503.04123v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>The Spinning Blimp: Design and Control of a Novel Minimalist Aerial Vehicle Leveraging Rotational Dynamics and Locomotion</title>
      <link>http://arxiv.org/abs/2503.04112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the IEEE international conference on robotics and  automation(ICRA 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Spinning Blimp，这是一种新型的轻于空气（LTA）飞行器，设计用于低能耗稳定的飞行。&lt;h4&gt;背景&lt;/h4&gt;现有的轻于空气飞行器在稳定性和能源效率方面存在局限性。需要一种新的设计方案来解决这些问题。&lt;h4&gt;目的&lt;/h4&gt;提出并验证一个低成本、高效的轻于空气飞行器设计方案，适用于多种应用场景。&lt;h4&gt;方法&lt;/h4&gt;Spinning Blimp使用扁椭球形氦气球提供浮力，并采用被动式机翼结合螺旋桨的组合结构，在飞行中产生旋转行为，从而实现类似摆动式的稳定效果。此外，还提出了一种利用持续旋转特性控制平移运动的控制策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果证实了该设计的有效性，展示了其作为多样化和经济适用解决方案的潜力。&lt;h4&gt;结论&lt;/h4&gt;Spinning Blimp作为一种低成本、高效率的设计方案，在多种应用领域中表现出良好的性能与适应性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：本文提出了一种新型的轻于空气（LTA）飞行器——Spinning Blimp，专为低能耗稳定飞行设计。通过使用扁椭球形氦气球来提供浮力，该飞行器在保持长时间空中停留的同时实现了最小的能量消耗。独特的低成本设计方案结合被动式机翼和螺旋桨，在飞行中诱导出旋转行为，从而提供了固有的摆动式稳定性。我们提出了一种控制策略，利用Spinning Blimp持续旋转的特性来控制平移运动。由于其成本效益高，该飞行器非常适合多种应用场景，如巡逻、定位、空气和湍流监测以及家庭监控等。实验评估验证了设计的有效性，并强调了它作为多样性和经济适用性的解决方案在空中应用中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents the Spinning Blimp, a novel lighter-than-air (LTA) aerialvehicle designed for low-energy stable flight. Utilizing an oblate spheroidhelium balloon for buoyancy, the vehicle achieves minimal energy consumptionwhile maintaining prolonged airborne states. The unique and low-cost designemploys a passively arranged wing coupled with a propeller to induce a spinningbehavior, providing inherent pendulum-like stabilization. We propose a controlstrategy that takes advantage of the continuous revolving nature of thespinning blimp to control translational motion. The cost-effectiveness of thevehicle makes it highly suitable for a variety of applications, such aspatrolling, localization, air and turbulence monitoring, and domesticsurveillance. Experimental evaluations affirm the design's efficacy andunderscore its potential as a versatile and economically viable solution foraerial applications.</description>
      <author>example@mail.com (Leonardo Santens, Diego S. D'Antonio, Shuhang Hou, David Saldaña)</author>
      <guid isPermaLink="false">2503.04112v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards Efficient Contrastive PAC Learning</title>
      <link>http://arxiv.org/abs/2502.15962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文在PAC学习框架下研究对比学习，揭示了对于线性表示这一基本概念的对比学习存在高效PAC学习器的问题仍未解决，并提出了一种基于Rademacher复杂度的泛化保证方法。&lt;h4&gt;背景&lt;/h4&gt;尽管最近有许多关于对比损失下的统计结果的研究，这些算法通常效率不高或无法提供PAC保证。&lt;h4&gt;目的&lt;/h4&gt;探讨线性表示的对比学习问题是否可以在PAC框架下以高效的方式解决，并尝试构建一个有效的对比学习算法。&lt;h4&gt;方法&lt;/h4&gt;首先证明了在一般情况下，对比PAC学习线性表示是难以处理的问题。接着展示当对比样本之间的距离用L2范数度量时，该问题可以松弛为半定规划(SDP)形式。然后基于Rademacher复杂度建立泛化保证，并将这些保证与某些条件下对比大间隔条件下的PAC保证联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;在对比学习的线性表示这一基本设置下，存在高效的PAC学习器的问题依然开放。提出了一个通过半定规划放松问题的方法以及一种基于Rademacher复杂度的泛化保证方法，这是已知的第一个有效的对比学习的PAC算法。&lt;h4&gt;结论&lt;/h4&gt;论文证明了对比学习中的某些核心挑战，并提出了一种新的解决策略和理论基础，为高效且具有PAC保证的学习提供了可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了在PAC学习框架下的对比学习。尽管最近有许多关于基于VC维或Rademacher复杂度的对比损失下统计结果的研究，但这些算法通常是低效的或无法提供PAC保证。在这篇论文中，我们考虑了线性表示这一基本概念的学习问题，并发现即使在这种简单的设置下，高效且具有PAC保证的学习器的存在仍然是一个开放的问题。我们首先表明一般情况下对比PAC学习线性表示是难以处理的，然后展示当对比样本之间的距离用L2范数度量时该问题可以松弛为半定规划形式。接着基于Rademacher复杂度建立了泛化保证，并将这些保证与某些条件下对比大间隔条件下的PAC保证联系起来。据我们所知，这是第一个有效的对比学习的PAC算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study contrastive learning under the PAC learning framework. While aseries of recent works have shown statistical results for learning undercontrastive loss, based either on the VC-dimension or Rademacher complexity,their algorithms are inherently inefficient or not implying PAC guarantees. Inthis paper, we consider contrastive learning of the fundamental concept oflinear representations. Surprisingly, even under such basic setting, theexistence of efficient PAC learners is largely open. We first show that theproblem of contrastive PAC learning of linear representations is intractable tosolve in general. We then show that it can be relaxed to a semi-definiteprogram when the distance between contrastive samples is measured by the$\ell_2$-norm. We then establish generalization guarantees based on Rademachercomplexity, and connect it to PAC guarantees under certain contrastivelarge-margin conditions. To the best of our knowledge, this is the firstefficient PAC learning algorithm for contrastive learning.</description>
      <author>example@mail.com (Jie Shen)</author>
      <guid isPermaLink="false">2502.15962v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
  <item>
      <title>Intermediate Domain-guided Adaptation for Unsupervised Chorioallantoic Membrane Vessel Segmentation</title>
      <link>http://arxiv.org/abs/2503.03546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的Intermediate Domain-guided Adaptation (IDA) 方法，利用CAM图像和视网膜图像之间的相似性以及现有的公共视网膜数据集进行无监督训练。&lt;h4&gt;背景&lt;/h4&gt;在血管生成研究中广泛使用的胎盘绒毛膜尿囊膜模型（CAM）的血血管分布是关键评价指标。由于手动分割耗时且主观性强，因此开发自动化的血管分割算法至关重要。&lt;h4&gt;目的&lt;/h4&gt;为解决现有CAM血管分割算法有限以及公开数据集不足的问题，提出了一种创新的方法来改进无监督域适应技术。&lt;h4&gt;方法&lt;/h4&gt;提出Multi-Resolution Asymmetric Translation (MRAT)策略生成中间图像以促进图像级交互；开发Intermediate Domain-guided Contrastive Learning (IDCL)模块以解开跨域特征表示。使用公开的视网膜数据集进行训练，并创建首个CAM数据集进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，该方法在所提出的CAM数据集中表现优异，同时在不同的视网膜数据集中也展示了强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;IDa方法克服了现有无监督域适应（UDA）技术只关注源目标直接对齐的局限性，并成功应用于血管生成研究中，有望改进其他生物医学图像处理任务。&lt;h4&gt;翻译&lt;/h4&gt;绒毛尿囊膜模型广泛用于血管生成研究，血管分布是评估的关键指标。因此，基于拓扑和形态学特征的定量评价需要精确分割血管。然而，手动分割耗时且易出错。此外，关于CAM血管分割算法的研究有限，缺乏公开数据集导致预测性能不佳。为解决这些问题，本文提出了一种新颖的方法：Intermediate Domain-guided Adaptation (IDA) 方法，利用了CAM图像和视网膜图像的相似性以及现有的公共视网膜数据集进行无监督训练。通过在首个创建的CAM数据集中进行全面实验，证明该方法优于其他现有方法，并且在不同的视网膜数据集中也展现了强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The chorioallantoic membrane (CAM) model is widely employed in angiogenesisresearch, and distribution of growing blood vessels is the key evaluationindicator. As a result, vessel segmentation is crucial for quantitativeassessment based on topology and morphology. However, manual segmentation isextremely time-consuming, labor-intensive, and prone to inconsistency due toits subjective nature. Moreover, research on CAM vessel segmentation algorithmsremains limited, and the lack of public datasets contributes to poor predictionperformance. To address these challenges, we propose an innovative IntermediateDomain-guided Adaptation (IDA) method, which utilizes the similarity betweenCAM images and retinal images, along with existing public retinal datasets, toperform unsupervised training on CAM images. Specifically, we introduce aMulti-Resolution Asymmetric Translation (MRAT) strategy to generateintermediate images to promote image-level interaction. Then, an IntermediateDomain-guided Contrastive Learning (IDCL) module is developed to disentanglecross-domain feature representations. This method overcomes the limitations ofexisting unsupervised domain adaptation (UDA) approaches, which primarilyconcentrate on directly source-target alignment while neglecting intermediatedomain information. Notably, we create the first CAM dataset to validate theproposed algorithm. Extensive experiments on this dataset show that our methodoutperforms compared approaches. Moreover, it achieves superior performance inUDA tasks across retinal datasets, highlighting its strong generalizationcapability. The CAM dataset and source codes are available athttps://github.com/PWSong-ustc/IDA.</description>
      <author>example@mail.com (Pengwu Song, Liang Xu, Peng Yao, Shuwei Shen, Pengfei Shao, Mingzhai Sun, Ronald X. Xu)</author>
      <guid isPermaLink="false">2503.03546v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control</title>
      <link>http://arxiv.org/abs/2503.03751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in CVPR 2025. Website:  https://research.nvidia.com/labs/toronto-ai/GEN3C/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GEN3C是一种基于精确相机控制和时间三维一致性的生成视频模型。&lt;h4&gt;背景&lt;/h4&gt;现有的视频生成模型虽然能够产生真实的视频，但很少利用三维信息，导致物体突然出现或消失等问题。此外，现有模型中的相机控制不够精准。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决现有视频生成模型中存在的问题，并提高视频的逼真度和一致性。&lt;h4&gt;方法&lt;/h4&gt;GEN3C通过预测种子图像或先前生成帧的像素深度来获取点云作为三维缓存。在生成下一帧时，基于用户提供的新相机轨迹对三维缓存进行2D渲染。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，相较于之前的工作，GEN3C实现了更精确的相机控制，并且在稀疏视图新颖视角合成方面取得了最先进的成果，特别是在驾驶场景和单目动态视频等具有挑战性的设置中表现尤为出色。&lt;h4&gt;结论&lt;/h4&gt;该研究为生成逼真、一致的三维视频提供了一种创新的方法，有望应用于更多领域。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了GEN3C，这是一种具备精确相机控制能力和时间三维一致性的生成视频模型。先前的视频模型已经可以生成现实感强的视频，但它们倾向于利用很少的三维信息，导致不一致性问题，如物体突然出现或消失等现象。如果实现了任何相机控制，通常也是不够精准的，因为摄像机参数仅仅是输入给神经网络的部分数据，需要网络自己推断视频是如何依赖于摄像机姿态的。相比之下，GEN3C使用一个由点云构成的三维缓存指导其工作：通过预测种子图像或先前生成帧的像素深度得到这些点云。当生成下一帧时，模型基于用户提供的新相机轨迹对三维缓存进行2D渲染来条件化。至关重要的是，这意味着GEN3C不必记住之前产生的内容，也不必从摄像机姿态推断出图像结构。相反，它可以将全部的生成能力集中在尚未观察到的区域，并推进场景状态到达下一帧。我们的实验结果表明比之前的模型具有更精确的相机控制能力，并且在稀疏视图新颖视角合成方面取得了最先进的成果，特别是在驾驶场景和单目动态视频等复杂设置中表现尤为出色。观看视频以查看最佳效果！请访问我们的网页：https://research.nvidia.com/labs/toronto-ai/GEN3C/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nv-tlabs/GEN3C&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present GEN3C, a generative video model with precise Camera Control andtemporal 3D Consistency. Prior video models already generate realistic videos,but they tend to leverage little 3D information, leading to inconsistencies,such as objects popping in and out of existence. Camera control, if implementedat all, is imprecise, because camera parameters are mere inputs to the neuralnetwork which must then infer how the video depends on the camera. In contrast,GEN3C is guided by a 3D cache: point clouds obtained by predicting thepixel-wise depth of seed images or previously generated frames. When generatingthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache withthe new camera trajectory provided by the user. Crucially, this means thatGEN3C neither has to remember what it previously generated nor does it have toinfer the image structure from the camera pose. The model, instead, can focusall its generative power on previously unobserved regions, as well as advancingthe scene state to the next frame. Our results demonstrate more precise cameracontrol than prior work, as well as state-of-the-art results in sparse-viewnovel view synthesis, even in challenging settings such as driving scenes andmonocular dynamic video. Results are best viewed in videos. Check out ourwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/</description>
      <author>example@mail.com (Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, Jun Gao)</author>
      <guid isPermaLink="false">2503.03751v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Label-Efficient LiDAR Semantic Segmentation with 2D-3D Vision Transformer Adapters</title>
      <link>http://arxiv.org/abs/2503.03299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为BALViT的新方法，该方法通过冻结的视觉模型作为模态特征编码器来学习强大的LiDAR编码器。结合范围视图和鸟瞰视角的LiDAR编码机制，并引入了新的2D-3D适配器来提高性能。&lt;h4&gt;背景&lt;/h4&gt;当前的LiDAR语义分割模型由于缺乏大规模多样化的数据集而难以进行通用预训练，大多数点云分割架构包含定制网络层，限制了视觉基础架构进步的应用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法BALViT，旨在通过冻结的视觉模型作为模态特征编码器来增强LiDAR编码能力，并在小数据场景下实现高性能。&lt;h4&gt;方法&lt;/h4&gt;BALViT采用范围视图和鸟瞰视角相结合的方式进行LiDAR编码。范围视图特征经过冻结的图像骨干网络处理，而鸟瞰视角分支通过多次交叉注意力交互增强这些特征。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在小数据集环境下能够显著提高性能，并且在SemanticKITTI和nuScenes基准测试中优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;研究证明了BALViT方法的有效性，可以为其他相关领域提供有价值的参考。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR语义分割模型通常从随机初始化训练开始，由于缺乏大规模多样化数据集，通用预训练受阻。此外，大多数点云分割架构包含定制网络层，限制了视觉基础架构进步的应用性。受到统一基础模型最新进展的启发，我们提出了BALViT这一新方法，该方法利用冻结视觉模型作为模态特征编码器来学习强大的LiDAR编码器。具体而言，BALViT结合了范围视图和鸟瞰视角的LiDAR编码机制，并通过新的2D-3D适配器组合这些机制。虽然范围视图特征经过冻结图像骨干网络处理，但我们的鸟瞰视角分支通过多次交叉注意力交互增强这些特征，因此可以不断改进视觉网络以融入领域相关的知识，从而产生强大的标签高效LiDAR编码机制。在SemanticKITTI和nuScenes基准上的广泛评估表明，在小数据集环境中，它优于现有方法。我们将在http://balvit.cs.uni-freiburg.de上公开代码和模型。&lt;h4&gt;贡献&lt;/h4&gt;提出了BALViT框架，解决了当前LiDAR语义分割模型缺乏大规模多样化数据集的问题，并在多个性能指标上取得了显著提升&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR semantic segmentation models are typically trained from randominitialization as universal pre-training is hindered by the lack of large,diverse datasets. Moreover, most point cloud segmentation architecturesincorporate custom network layers, limiting the transferability of advancesfrom vision-based architectures. Inspired by recent advances in universalfoundation models, we propose BALViT, a novel approach that leverages frozenvision models as amodal feature encoders for learning strong LiDAR encoders.Specifically, BALViT incorporates both range-view and bird's-eye-view LiDARencoding mechanisms, which we combine through a novel 2D-3D adapter. While therange-view features are processed through a frozen image backbone, ourbird's-eye-view branch enhances them through multiple cross-attentioninteractions. Thereby, we continuously improve the vision network withdomain-dependent knowledge, resulting in a strong label-efficient LiDARencoding mechanism. Extensive evaluations of BALViT on the SemanticKITTI andnuScenes benchmarks demonstrate that it outperforms state-of-the-art methods onsmall data regimes. We make the code and models publicly available at:http://balvit.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Julia Hindel, Rohit Mohan, Jelena Bratulic, Daniele Cattaneo, Thomas Brox, Abhinav Valada)</author>
      <guid isPermaLink="false">2503.03299v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Meta-Learning to Explore via Memory Density Feedback</title>
      <link>http://arxiv.org/abs/2503.02831v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种利用元学习探索算法，该算法使智能体能够最大化其在单一时间间隔内的探索进度。&lt;h4&gt;背景&lt;/h4&gt;传统的强化学习探索方法通常通过添加一种内在奖励来替换或增强原始奖赏函数，以促使智能体探索未曾见过的状态。&lt;h4&gt;目的&lt;/h4&gt;研究旨在设计一种新型的元学习探索算法，该算法允许智能体在训练周期间甚至未训练过的环境中最大化其探索进度。&lt;h4&gt;方法&lt;/h4&gt;智能体会学习一种策略，即最小化新观察结果相对于所有记忆的概率密度，并根据当前观测密度接收反馈，在递归网络中保存这些反馈。通过这种方式，智能体学会了实时导航熟悉度不断增长的复杂环境。&lt;h4&gt;主要发现&lt;/h4&gt;基于上述设计，智能体可以在完全新颖的状态下进行探索并最大化其探索进度，即使在其策略没有为此类状态训练过的情况下也是如此。&lt;h4&gt;结论&lt;/h4&gt;该元学习方法有效提高了智能体在未见过环境中探索的能力和效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的探索算法通过利用元学习（即学会学习）来增强智能体在一个时间间隔内优化其探索进度的能力，即使是在训练周期之间也能实现。这种策略帮助智能体最小化新观察结果相对于所有已存记忆的概率密度，并且能够根据当前观测密度反馈进行调整。这样做的结果是，智能体可以实时地导航一个复杂且不断扩展的熟悉度景观，在从未见过的状态中最大化其探索进度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Exploration algorithms for reinforcement learning typically replace oraugment the reward function with an additional ``intrinsic'' reward that trainsthe agent to seek previously unseen states of the environment. Here, weconsider an exploration algorithm that exploits meta-learning, or learning tolearn, such that the agent learns to maximize its exploration progress within asingle episode, even between epochs of training. The agent learns a policy thataims to minimize the probability density of new observations with respect toall of its memories. In addition, it receives as feedback evaluations of thecurrent observation density and retains that feedback in a recurrent network.By remembering trajectories of density, the agent learns to navigate a complexand growing landscape of familiarity in real-time, allowing it to maximize itsexploration progress even in completely novel states of the environment forwhich its policy has not been trained.</description>
      <author>example@mail.com (Kevin L. McKee)</author>
      <guid isPermaLink="false">2503.02831v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving</title>
      <link>http://arxiv.org/abs/2503.03205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种多代理框架MA-LoT，该框架结合了自然语言推理和形式语言验证，在Lean4定理证明中取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;使用计算机可验证的语言（如Lean）解决数学问题对数学和计算机科学领域产生了重大影响。当前最先进的方法依赖于单一大型语言模型（LLMs），这些模型作为代理或证明者，负责生成完整证明或进行树搜索，但缺乏将自然语言推理与形式语言验证反馈相结合的结构化方式。&lt;h4&gt;目的&lt;/h4&gt;为了克服单一代理方法的局限性，并结合高级自然语言推理和形式语言验证，本文提出了MA-LoT框架。&lt;h4&gt;方法&lt;/h4&gt;该框架利用长链思维（Long CoT）中的涌现形式推理能力以及新颖的LoT-Transfer学习训练推断流水线。通过这种方式，在证明生成中实现了更深入的洞察力和长期一致性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，MA-LoT在Lean4版本的MiniF2F-Test数据集上达到了54.51%的准确率，显著优于GPT-4（22.95%）、单代理树搜索(InternLM-Step-Prover, 50.70%)和完整证明生成(DeepSeek-Prover-v1.5, 48.36%)基线。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了结合长链思维与形式验证在更广泛的视角中进行更为深刻生成的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solving mathematical problems using computer-verifiable languages like Leanhas significantly impacted mathematical and computer science communities.State-of-the-art methods utilize single Large Language Models (LLMs) as agentsor provers to either generate complete proof or perform tree searches. However,single-agent methods inherently lack a structured way to combine high-levelreasoning in Natural Language (NL) with Formal Language (FL) verificationfeedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based LongChain-of-Thought framework, (to the best of our knowledge), the firstmulti-agent framework for Lean4 theorem proving that balance high-level NLreasoning and FL verification in Long CoT. Using this structured interaction,our approach enables deeper insights and long-term coherence in proofgeneration, with which past methods struggle. We do this by leveraging emergentformal reasoning ability in Long CoT using our novel LoT-Transfer Learningtraining-inference pipeline. Extensive experiments show that our frameworkachieves 54.51% accuracy rate on the Lean4 version of MiniF2F-Test dataset,largely outperforming GPT-4 (22.95%), single-agent tree search(InternLM-Step-Prover, 50.70%), and whole-proof generation(DeepSeek-Prover-v1.5, 48.36%) baselines. Furthermore, our findings highlightthe potential of combining Long CoT with formal verification for a moreinsightful generation in a broader perspective.</description>
      <author>example@mail.com (Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang)</author>
      <guid isPermaLink="false">2503.03205v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>REGRACE: A Robust and Efficient Graph-based Re-localization Algorithm using Consistency Evaluation</title>
      <link>http://arxiv.org/abs/2503.03599v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;REGRACE是一种新颖的方法，利用LiDAR子图解决了大尺度导航中回环闭合的可扩展性和视角差异问题。&lt;h4&gt;背景&lt;/h4&gt;当前使用密集点云进行精确位置识别的方法由于扫描到扫描之间的比较计算量过大而不具备良好的可扩展性。而基于对象的方法虽然效率更高，但往往对视点变化敏感。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法REGRACE来解决大规模导航中回环闭合的挑战问题，提高系统的鲁棒性和准确性。&lt;h4&gt;方法&lt;/h4&gt;引入旋转不变特征和图神经网络增强的邻居上下文信息。使用词袋模型进行子地图之间的高效匹配，并利用几何一致性识别远程回环闭合。&lt;h4&gt;主要发现&lt;/h4&gt;REGRACE在保持高精度的同时提高了速度，与最先进的位置识别基线相比快一倍。&lt;h4&gt;结论&lt;/h4&gt;REGRACE能够在大规模导航环境中有效地实现准确的回环闭合检测，具备良好的可扩展性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;循环闭合对于校正里程计漂移和创建一致的地图至关重要，尤其是在大尺度导航中。当前使用密集点云的方法由于扫描到扫描之间的比较计算量过大而不具良好扩展性。基于对象的替代方法虽然更高效，但往往对视点变化敏感。在这项工作中，我们引入了REGRACE，这是一种新颖的方法，通过使用LiDAR基子图来解决重新定位中的可扩展性和视角差异问题。我们介绍了每个标记对象的旋转不变特征，并通过图神经网络增强它们以考虑邻居上下文信息。为了识别潜在重复访问，我们采用了一种可扩展的词袋方法，每幅子地图池化一个学习到的全局特征。另外，我们用几何一致性线索来定义重新访问，而不是基于嵌入距离，这使我们可以识别远处的回环闭合。我们的评估表明，REGRACE在位置识别和配准基线方面取得了相似的结果，而速度却快了一倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Loop closures are essential for correcting odometry drift and creatingconsistent maps, especially in the context of large-scale navigation. Currentmethods using dense point clouds for accurate place recognition do not scalewell due to computationally expensive scan-to-scan comparisons. Alternativeobject-centric approaches are more efficient but often struggle withsensitivity to viewpoint variation. In this work, we introduce REGRACE, a novelapproach that addresses these challenges of scalability and perspectivedifference in re-localization by using LiDAR-based submaps. We introducerotation-invariant features for each labeled object and enhance them withneighborhood context through a graph neural network. To identify potentialrevisits, we employ a scalable bag-of-words approach, pooling one learnedglobal feature per submap. Additionally, we define a revisit with geometricalconsistency cues rather than embedding distance, allowing us to recognizefar-away loop closures. Our evaluations demonstrate that REGRACE achievessimilar results compared to state-of-the-art place recognition and registrationbaselines while being twice as fast.</description>
      <author>example@mail.com (Débora N. P. Oliveira, Joshua Knights, Sebastián Barbas Laina, Simon Boche, Wolfram Burgard, Stefan Leutenegger)</author>
      <guid isPermaLink="false">2503.03599v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Feature Matching Intervention: Leveraging Observational Data for Causal Representation Learning</title>
      <link>http://arxiv.org/abs/2503.03634v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种创新的方法，称为特征匹配干预（Feature Matching Intervention, FMI），用于从观测数据中进行因果发现。&lt;h4&gt;背景&lt;/h4&gt;在从观察性数据中进行因果发现时，缺乏完美干预是一个主要挑战，这使得区分真正的因果特征和虚假的特征变得困难。&lt;h4&gt;目的&lt;/h4&gt;通过使用匹配程序模拟完美的干预来识别因果关系，并定义了因果潜在图（Causal Latent Graphs），这种框架连接了FMI与因果图学习。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的理论框架，即因果潜在图，并开发了一个模仿完美干预的特征匹配过程。&lt;h4&gt;主要发现&lt;/h4&gt;理论上表明，FMI表现出强大的出分布（OOD）泛化能力。实验进一步证实了FMI在仅从观察数据中有效识别因果特征方面的卓越性能。&lt;h4&gt;结论&lt;/h4&gt;该方法为解决观测性数据分析中的因果关系问题提供了一个新的视角和有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在从观测数据进行因果发现时，缺乏完美的干预是一个主要挑战。为了应对这一问题，我们提出了一种创新的方法，称为特征匹配干预（Feature Matching Intervention, FMI），该方法通过使用一个匹配程序来模仿完美干预，并定义了因果潜在图，即扩展到潜在特征空间的结构因果模型，为FMI与因果图学习之间的连接提供了一个框架。我们的特征匹配过程在这些因果潜在图中模拟完美的干预行为。理论结果表明，FMI具有强大的出分布（OOD）泛化能力。实验进一步证明了FMI仅从观测数据有效识别因果特征方面的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A major challenge in causal discovery from observational data is the absenceof perfect interventions, making it difficult to distinguish causal featuresfrom spurious ones. We propose an innovative approach, Feature MatchingIntervention (FMI), which uses a matching procedure to mimic perfectinterventions. We define causal latent graphs, extending structural causalmodels to latent feature space, providing a framework that connects FMI withcausal graph learning. Our feature matching procedure emulates perfectinterventions within these causal latent graphs. Theoretical resultsdemonstrate that FMI exhibits strong out-of-distribution (OOD)generalizability. Experiments further highlight FMI's superior performance ineffectively identifying causal features solely from observational data.</description>
      <author>example@mail.com (Haoze Li, Jun Xie)</author>
      <guid isPermaLink="false">2503.03634v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DDEQs: Distributional Deep Equilibrium Models through Wasserstein Gradient Flows</title>
      <link>http://arxiv.org/abs/2503.01140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 17 figures. To be published in AISTATS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了分布式深度平衡模型(DDEQ)，它是隐含神经网络的一种，扩展了Deep Equilibrium Models (DEQs) 至离散度量输入。&lt;h4&gt;背景&lt;/h4&gt;传统的DEQ主要处理序列数据，但已经应用于各种类型的数据。然而，现有的框架没有充分利用离散度量的特性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的理论基础框架来支持基于离散度量（如集合或点云）的深度平衡模型，以提高其在特定任务中的表现和参数效率。&lt;h4&gt;方法&lt;/h4&gt;通过利用Wasserstein梯度流，展示了如何调整DEQ前向传递的方式，在交换不变性的条件下找到离散度量下的固定点，并推导出适当的网络架构以适应DDEQ。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与最先进的模型相比，DDEQ在点云分类和点云补全等任务上具有竞争力，同时显著减少了参数的使用效率。&lt;h4&gt;结论&lt;/h4&gt;通过扩展DEQ框架来处理离散度量输入，如集合或点云，可以提高模型性能并降低参数需求。&lt;h4&gt;翻译&lt;/h4&gt;深度平衡模型(DDEQs) 是一种隐含神经网络类，它将Deep Equilibrium Models (DEQs) 扩展到离散度量输入(例如集合或点云)，提供了理论基础框架。通过使用Wasserstein梯度流, 展示了如何调整DEQ前向传递以在交换不变性的条件下找到离散度量的固定点，并推导出适应DDEQs 的网络架构。实验表明，它们可以与最先进的模型竞争，完成如点云分类和补全等任务，同时参数效率显著提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep Equilibrium Models (DEQs) are a class of implicit neural networks thatsolve for a fixed point of a neural network in their forward pass.Traditionally, DEQs take sequences as inputs, but have since been applied to avariety of data. In this work, we present Distributional Deep EquilibriumModels (DDEQs), extending DEQs to discrete measure inputs, such as sets orpoint clouds. We provide a theoretically grounded framework for DDEQs.Leveraging Wasserstein gradient flows, we show how the forward pass of the DEQcan be adapted to find fixed points of discrete measures underpermutation-invariance, and derive adequate network architectures for DDEQs. Inexperiments, we show that they can compete with state-of-the-art models intasks such as point cloud classification and point cloud completion, whilebeing significantly more parameter-efficient.</description>
      <author>example@mail.com (Jonathan Geuter, Clément Bonet, Anna Korba, David Alvarez-Melis)</author>
      <guid isPermaLink="false">2503.01140v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance</title>
      <link>http://arxiv.org/abs/2503.03689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;DualDiff是一种双分支条件扩散模型，用于改进多视角和视频序列的驾驶场景生成。&lt;h4&gt;背景信息&lt;/h4&gt;当前方法主要依赖于3D边界框和鸟瞰图道路地图来控制前景和背景，这些方法无法捕捉驾驶场景的全部复杂性，并且不足以充分整合多模态信息。&lt;h4&gt;研究目的&lt;/h4&gt;提出DualDiff模型以解决现有技术在复杂性和多模态信息集成上的不足。&lt;h4&gt;主要方法&lt;/h4&gt;{'ORS': 'Occupancy Ray-shape Sampling，提供丰富的前景和背景语义以及3D空间几何结构', 'FGM': 'Foreground-Aware Mask，增强精细的前景物体合成', 'SFA': 'Semantic Fusion Attention机制，优先处理相关信息并抑制噪声', 'RGD': 'Reward-Guided Diffusion框架，确保生成视频的一致性和语义连贯性'}&lt;h4&gt;实验结果&lt;/h4&gt;{'性能提升': '在多个数据集上实现了最先进的（SOTA）表现。', 'NuScenes数据集': '相比最佳基线，FID得分减少了4.09%。', '下游任务改进': {'BEV分割': '车辆mIoU提高4.50%，道路mIoU提高1.70%', 'BEV 3D物体检测': '前景mAP提升1.46%'}}&lt;h4&gt;结论&lt;/h4&gt;DualDiff在多个评价指标上超越现有方法，证明了其在驾驶场景重建中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;准确且高保真的驾驶场景重建需要有效利用全面的场景信息作为条件输入。现有的方法主要依赖于3D边界框和鸟瞰图道路地图来控制前景和背景，这无法捕捉到驾驶场景的全部复杂性，并不足以充分整合多模态信息。在这项工作中，我们提出了DualDiff，这是一种双分支条件扩散模型，旨在增强多视角和视频序列中的驾驶场景生成能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and high-fidelity driving scene reconstruction demands the effectiveutilization of comprehensive scene information as conditional inputs. Existingmethods predominantly rely on 3D bounding boxes and BEV road maps forforeground and background control, which fail to capture the full complexity ofdriving scenes and adequately integrate multimodal information. In this work,we present DualDiff, a dual-branch conditional diffusion model designed toenhance driving scene generation across multiple views and video sequences.Specifically, we introduce Occupancy Ray-shape Sampling (ORS) as a conditionalinput, offering rich foreground and background semantics alongside 3D spatialgeometry to precisely control the generation of both elements. To improve thesynthesis of fine-grained foreground objects, particularly complex and distantones, we propose a Foreground-Aware Mask (FGM) denoising loss function.Additionally, we develop the Semantic Fusion Attention (SFA) mechanism todynamically prioritize relevant information and suppress noise, enabling moreeffective multimodal fusion. Finally, to ensure high-quality image-to-videogeneration, we introduce the Reward-Guided Diffusion (RGD) framework, whichmaintains global consistency and semantic coherence in generated videos.Extensive experiments demonstrate that DualDiff achieves state-of-the-art(SOTA) performance across multiple datasets. On the NuScenes dataset, DualDiffreduces the FID score by 4.09% compared to the best baseline. In downstreamtasks, such as BEV segmentation, our method improves vehicle mIoU by 4.50% androad mIoU by 1.70%, while in BEV 3D object detection, the foreground mAPincreases by 1.46%. Code will be made available athttps://github.com/yangzhaojason/DualDiff.</description>
      <author>example@mail.com (Zhao Yang, Zezhong Qian, Xiaofan Li, Weixiang Xu, Gongpeng Zhao, Ruohong Yu, Lingsi Zhu, Longjun Liu)</author>
      <guid isPermaLink="false">2503.03689v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Opportunistic Routing in Wireless Communications via Learnable State-Augmented Policies</title>
      <link>http://arxiv.org/abs/2503.03736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文研究了大规模无线通信网络中基于数据包的信息路由挑战，通过约束统计学习任务来解决此问题。&lt;h4&gt;背景&lt;/h4&gt;在大型无线网络中实现有效的信息传递是一个关键挑战。传统的路由策略依赖于特定的路径选择算法，但这种方法在动态网络环境中效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的分布式优化方法（State-Augmentation, SA）以提高源节点的信息处理能力，并通过图神经网络（GNNs）来提取最佳的路由政策。&lt;h4&gt;方法&lt;/h4&gt;利用图卷积操作和基于网络节点之间拓扑连接关系的无监督学习框架，设计了一种新颖的方法。该方法能够根据实时信息动态选择最优中继节点。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与传统的基线算法相比，所提出的GNN参数化模型在处理多流数据时表现出色，尤其是在大型网络环境中具有更高的效率和稳定性。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了利用图神经网络进行无线通信路由优化的潜力，突出了其鲁棒性和可转移性。该方法不仅能够有效应对大规模无线网络中的动态挑战，而且可以应用于各种实际场景中。&lt;h4&gt;翻译&lt;/h4&gt;此论文解决的是在大型无线通信网絡中基于数据包的信息传递问题。它被描述为一个受限统计学习任务，在这个过程中每个节点仅使用本地信息进行操作。机会性路由利用了无线电通信的广播特性来动态选择最佳转发节点，从而使信息能够通过多个中继节点同时到达目的地。为了应对这一挑战，我们提出了一种基于状态增强（State-Augmentation, SA）的分布式优化方法，旨在最大化网络源节点的信息处理能力。该问题建模使用图神经网络（GNN），执行基于网络节点之间拓扑连接关系的图卷积操作。通过无监督学习框架从GNN架构中提取路由策略，使源节点能够为各种流做出最优决策。数值实验表明，在训练参数化的GNN模型时，所提出的方法表现出色，并且比基线算法具有更佳的表现。此外，将该方法应用于现实网络拓扑结构和无线自组织网路测试平台验证了其有效性，突出了图神经网络的稳健性和可转移性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenge of packet-based information routing inlarge-scale wireless communication networks. The problem is framed as aconstrained statistical learning task, where each network node operates usingonly local information. Opportunistic routing exploits the broadcast nature ofwireless communication to dynamically select optimal forwarding nodes, enablingthe information to reach the destination through multiple relay nodessimultaneously. To solve this, we propose a State-Augmentation (SA) baseddistributed optimization approach aimed at maximizing the total informationhandled by the source nodes in the network. The problem formulation leveragesGraph Neural Networks (GNNs), which perform graph convolutions based on thetopological connections between network nodes. Using an unsupervised learningparadigm, we extract routing policies from the GNN architecture, enablingoptimal decisions for source nodes across various flows. Numerical experimentsdemonstrate that the proposed method achieves superior performance whentraining a GNN-parameterized model, particularly when compared to baselinealgorithms. Additionally, applying the method to real-world network topologiesand wireless ad-hoc network test beds validates its effectiveness, highlightingthe robustness and transferability of GNNs.</description>
      <author>example@mail.com (Sourajit Das, Navid NaderiAlizadeh, Rahul Mangharam, Alejandro Ribeiro)</author>
      <guid isPermaLink="false">2503.03736v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>HyperGCT: A Dynamic Hyper-GNN-Learned Geometric Constraint for 3D Registration</title>
      <link>http://arxiv.org/abs/2503.02195v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的方法HyperGCT，用于在3D点云配准问题中动态优化超图来生成几何约束。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通常通过构造一致性图来建模无序的特征匹配，并从中采样一致性的匹配以生成假设。然而，在构建这些图时引入了噪声，这给手工设计的几何约束带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从动态超图中挖掘稳健的几何约束的方法，从而改进3D配准过程。&lt;h4&gt;方法&lt;/h4&gt;HyperGCT通过顶点和边特征聚合的方式动态优化超图，利用高阶一致性来捕捉对应关系之间的相关性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与现有技术相比，HyperGCT在多个数据集上表现出了最先进的性能，并且对图形噪声具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这种方法不仅提高了3D配准的准确性，还增强了其泛化能力，证明了从动态超图中挖掘几何约束的有效性和重要性。&lt;h4&gt;翻译&lt;/h4&gt;几何约束对于解决基于特征匹配的3D点云注册问题至关重要。现有的方法通常将无序的匹配建模为一致性图，并从中采样一致性的匹配以生成假设。然而，显式的图形构建引入了噪声，这给手工设计的几何约束带来了挑战，使其难以确保匹配之间的协调性。为了克服这一限制，我们提出了一种新的方法HyperGCT，它利用动态超图中3D对应关系间的高阶一致性来学习灵活且动态的几何约束。据我们所知，这是第一个从动态超图中挖掘稳健几何约束的方法以用于三维注册任务。通过动态优化超图并聚合顶点和边特征，HyperGCT有效地捕获了对应之间的相关性，并生成准确的假设。在3DMatch、3DLoMatch、KITTI-LC以及ETH数据集上的广泛实验表明HyperGCT达到了最先进的性能。此外，我们的方法对图形噪声具有鲁棒性，在推广方面显示出显著优势。代码将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometric constraints between feature matches are critical in 3D point cloudregistration problems. Existing approaches typically model unordered matches asa consistency graph and sample consistent matches to generate hypotheses.However, explicit graph construction introduces noise, posing great challengesfor handcrafted geometric constraints to render consistency among matches. Toovercome this, we propose HyperGCT, a flexible dynamic Hyper-GNN-learnedgeometric constraint that leverages high-order consistency among 3Dcorrespondences. To our knowledge, HyperGCT is the first method that minesrobust geometric constraints from dynamic hypergraphs for 3D registration. Bydynamically optimizing the hypergraph through vertex and edge featureaggregation, HyperGCT effectively captures the correlations amongcorrespondences, leading to accurate hypothesis generation. Extensiveexperiments on 3DMatch, 3DLoMatch, KITTI-LC, and ETH show that HyperGCTachieves state-of-the-art performance. Furthermore, our method is robust tograph noise, demonstrating a significant advantage in terms of generalization.The code will be released.</description>
      <author>example@mail.com (Xiyu Zhang, Jiayi Ma, Jianwei Guo, Wei Hu, Zhaoshuai Qi, Fei Hui, Jiaqi Yang, Yanning Zhang)</author>
      <guid isPermaLink="false">2503.02195v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection</title>
      <link>http://arxiv.org/abs/2503.03562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一个名为Physics Anomaly Detection (Phys-AD)的新型大规模数据集，用于工业异常检测。该数据集基于真实的机器人臂和电机操作收集，包含多种动态且语义丰富的场景，以及不同类型的物理异常。&lt;h4&gt;背景&lt;/h4&gt;现有的工业异常检测算法主要在静态、语义简单的数据集上开发和测试，与现实世界中需要物理理解和推理的情况存在差距。&lt;h4&gt;目的&lt;/h4&gt;通过引入Phys-AD数据集来填补现有技术与实际需求之间的鸿沟，推动机器自主地像人类一样基于条件物体的物理知识进行感知、交互和推理的能力发展。&lt;h4&gt;方法&lt;/h4&gt;该研究包括了超过6400个视频片段，覆盖22种真实世界对象类别，并展示了机器人臂与电机互动时产生的47种异常类型。此外，还提出了Physics Anomaly Explanation (PAEval)度量标准以评估视觉语言基础模型在检测和解释物理原因方面的能力。&lt;h4&gt;主要发现&lt;/h4&gt;现有的无监督、弱监督以及视频理解方法在处理基于物理学的异常上存在局限性。&lt;h4&gt;结论&lt;/h4&gt;通过公开提供的数据集和基准测试，为研究者提供了新的机会来改进工业环境中的物体异常检测技术，并且这些资源将有助于推动物理理解和视觉推理的研究进展。&lt;h4&gt;翻译&lt;/h4&gt;人类通过感知、互动以及基于条件物体的物理知识进行推理来识别现实世界中的对象异常。工业异常检测（IAD）的长期目标是使机器能够自主地复制这种技能。然而，当前大多数IAD算法是在静态且语义简单的数据集上开发和测试的，这与需要物理理解和推理的真实场景相去甚远。为了解决这一问题，我们引入了Physics Anomaly Detection (Phys-AD) 数据集，这是首个大规模、基于真实世界的工业异常检测视频数据集，并采用物理学基础设计。使用真实的机器人臂和电机收集的数据包含了6400多个动态且语义丰富的场景及22种对象类别与机器人手臂和电机互动的视频片段，并展示了47种物理异常类型。在Phys-AD中，进行异常检测需要视觉推理，结合物理知识和视频内容来确定物体是否处于异常状态。我们对最先进的异常检测方法进行了基准测试，包括无监督、弱监督以及基于理解视频的方法，在处理物理学基础引发的异常方面显示出了局限性。此外，还引入了Physics Anomaly Explanation (PAEval)度量标准以评估视觉语言模型在提供准确物理原因解释的能力上。我们的数据集和基准将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans detect real-world object anomalies by perceiving, interacting, andreasoning based on object-conditioned physical knowledge. The long-term goal ofIndustrial Anomaly Detection (IAD) is to enable machines to autonomouslyreplicate this skill. However, current IAD algorithms are largely developed andtested on static, semantically simple datasets, which diverge from real-worldscenarios where physical understanding and reasoning are essential.To bridgethis gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, thefirst large-scale, real-world, physics-grounded video dataset for industrialanomaly detection. Collected using a real robot arm and motor, Phys-AD providesa diverse set of dynamic, semantically rich scenarios. The dataset includesmore than 6400 videos across 22 real-world object categories, interacting withrobot arms and motors, and exhibits 47 types of anomalies. Anomaly detection inPhys-AD requires visual reasoning, combining both physical knowledge and videocontent to determine object abnormality.We benchmark state-of-the-art anomalydetection methods under three settings: unsupervised AD, weakly-supervised AD,and video-understanding AD, highlighting their limitations in handlingphysics-grounded anomalies. Additionally, we introduce the Physics AnomalyExplanation (PAEval) metric, designed to assess the ability of visual-languagefoundation models to not only detect anomalies but also provide accurateexplanations for their underlying physical causes. Our dataset and benchmarkwill be publicly available.</description>
      <author>example@mail.com (Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu)</author>
      <guid isPermaLink="false">2503.03562v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>PacketCLIP: Multi-Modal Embedding of Network Traffic and Language for Cybersecurity Reasoning</title>
      <link>http://arxiv.org/abs/2503.03747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出PacketCLIP，一种结合包数据和自然语言语义的多模态框架，通过对比预训练和层次图神经网络（GNN）推理来增强加密流量分类和网络安全。&lt;h4&gt;背景&lt;/h4&gt;流量分类对网络安全至关重要，但加密流量带来了重大挑战。现有的解决方案难以同时满足准确性和解释性的需求，尤其是在资源受限环境中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效且可解释地检测加密流量中异常的系统，以应对网络安全面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;PacketCLIP结合了包数据和自然语言语义，并通过对比预训练以及层次图神经网络（GNN）推理来实现这一目标。该框架旨在将文本描述与包行为相匹配，提高模型的解释性、可扩展性和实用性。&lt;h4&gt;主要发现&lt;/h4&gt;PacketCLIP在加密流量分类中表现出色，实现了95%平均AUC评分，并且比基线方法提高了11.6%，同时减少了模型大小达92%，这对于实时异常检测尤为重要。&lt;h4&gt;结论&lt;/h4&gt;通过将高级机器学习技术与实际网络安全需求相结合，PacketCLIP为解决资源受限环境中的加密流量分类和网络入侵检测挑战提供了一个可扩展、高效且解释性强的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已经直接以中文形式给出，无需进一步翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic classification is vital for cybersecurity, yet encrypted trafficposes significant challenges. We present PacketCLIP, a multi-modal frameworkcombining packet data with natural language semantics through contrastivepretraining and hierarchical Graph Neural Network (GNN) reasoning. PacketCLIPintegrates semantic reasoning with efficient classification, enabling robustdetection of anomalies in encrypted network flows. By aligning textualdescriptions with packet behaviors, it offers enhanced interpretability,scalability, and practical applicability across diverse security scenarios.PacketCLIP achieves a 95% mean AUC, outperforms baselines by 11.6%, and reducesmodel size by 92%, making it ideal for real-time anomaly detection. By bridgingadvanced machine learning techniques and practical cybersecurity needs,PacketCLIP provides a foundation for scalable, efficient, and interpretablesolutions to tackle encrypted traffic classification and network intrusiondetection challenges in resource-constrained environments.</description>
      <author>example@mail.com (Ryozo Masukawa, Sanggeon Yun, Sungheon Jeong, Wenjun Huang, Yang Ni, Ian Bryant, Nathaniel D. Bastian, Mohsen Imani)</author>
      <guid isPermaLink="false">2503.03747v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Simulation-Based Performance Evaluation of 3D Object Detection Methods with Deep Learning for a LiDAR Point Cloud Dataset in a SOTIF-related Use Case</title>
      <link>http://arxiv.org/abs/2503.03548v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了一个评估自动驾驶系统中三维物体检测性能的方法，专注于传感器表现限制和基于深度学习的物体检测不足对预期功能的影响。&lt;h4&gt;背景&lt;/h4&gt;Safety of the Intended Functionality (SOTIF)旨在解决传感器性能限制以及基于深度学习的对象检测方法在自动驾驶系统（ADS）中的局限性，以确保其预期的功能。&lt;h4&gt;目的&lt;/h4&gt;该论文的主要目的是定义和建模一个与SOTIF相关的使用案例，并生成用于应用3D对象检测方法的激光雷达点云数据集。&lt;h4&gt;方法&lt;/h4&gt;通过模拟21种不同的天气条件下的SOTIF相关用例，创建了一个包含547个帧的数据集，其中包括晴天、多云和雨天的各种情况，对应于一天中的不同时间。使用MMDetection3D和OpenPCDET工具包对最先进的（SOTA）3D对象检测方法的性能进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;通过在生成的数据集上测试预先训练好的深度学习模型，并使用平均精度（AP）和召回率指标进行比较，论文展示了不同天气条件下的性能差异。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了在各种环境条件下对3D物体检测算法进行全面评估的重要性。所提出的框架为开发更安全的自动驾驶系统提供了重要的视角和支持。&lt;h4&gt;翻译&lt;/h4&gt;Safety of the Intended Functionality (SOTIF)旨在解决传感器性能限制以及基于深度学习的对象检测方法在自动驾驶系统（ADS）中的局限性，以确保其预期的功能。该论文提出了一个评估三维物体检测适应性和性能的方法，通过对模拟的与SOTIF相关的用例生成的数据集进行3D物体检测算法的应用来实现这一目的。主要贡献包括定义和建模21种不同天气条件下的SOTIF相关使用案例，并为应用3D物体检测方法创建了适合于激光雷达点云数据集。该数据集包含547帧，涵盖晴天、多云和雨天的不同情况，对应一天中的各个时段。通过MMDetection3D和OpenPCDET工具包，在生成的数据集上使用平均精度（AP）和召回率指标对预先训练好的深度学习模型进行测试并评估最先进的3D物体检测方法的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.5220/0012707300003702&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safety of the Intended Functionality (SOTIF) addresses sensor performancelimitations and deep learning-based object detection insufficiencies to ensurethe intended functionality of Automated Driving Systems (ADS). This paperpresents a methodology examining the adaptability and performance evaluation ofthe 3D object detection methods on a LiDAR point cloud dataset generated bysimulating a SOTIF-related Use Case. The major contributions of this paperinclude defining and modelling a SOTIF-related Use Case with 21 diverse weatherconditions and generating a LiDAR point cloud dataset suitable for applicationof 3D object detection methods. The dataset consists of 547 frames,encompassing clear, cloudy, rainy weather conditions, corresponding todifferent times of the day, including noon, sunset, and night. EmployingMMDetection3D and OpenPCDET toolkits, the performance of State-of-the-Art(SOTA) 3D object detection methods is evaluated and compared by testing thepre-trained Deep Learning (DL) models on the generated dataset using AveragePrecision (AP) and Recall metrics.</description>
      <author>example@mail.com (Milin Patel, Rolf Jung)</author>
      <guid isPermaLink="false">2503.03548v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Explainable LiDAR 3D Point Cloud Segmentation and Clustering for Detecting Airplane-Generated Wind Turbulence</title>
      <link>http://arxiv.org/abs/2503.00518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用激光雷达（LiDAR）数据检测航空涡流的先进、可解释机器学习方法。该方法结合了动态图卷积神经网络(DGCNN)和语义分割技术，对3D LiDAR点云进行有意义的分段，并通过聚类技术进一步优化。&lt;h4&gt;背景&lt;/h4&gt;飞机产生的强空气湍流（航空涡流）给航空安全带来了重大风险，需要准确可靠的检测方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用LiDAR数据检测航空涡流的有效且可靠的方法，提高航空安全性。&lt;h4&gt;方法&lt;/h4&gt;采用动态图卷积神经网络(DGCNN)和语义分割技术对3D LiDAR点云进行分段，并通过聚类技术优化结果。引入基于扰动的解释技术增强模型决策过程透明度。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在测量和模拟LiDAR扫描数据上的表现优于四种基准方法，证明了其有效性和可靠性。&lt;h4&gt;结论&lt;/h4&gt;结合语义分割和聚类技术用于实时航空涡流跟踪的方法显著提升了航空安全措施的有效性与可解释性。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种利用激光雷达（LiDAR）数据检测航空涡流的先进、透明机器学习方法。该研究通过动态图卷积神经网络（DGCNN）和语义分割技术对3D LiDAR点云进行有意义的分段，并引入基于扰动的技术来解释模型决策过程，从而提高了安全性和信任度。实验结果表明了这种方法的有效性和可靠性，为实时航空涡流跟踪提供了一种先进的方法，同时提升了其透明度和可理解性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wake vortices - strong, coherent air turbulences created by aircraft - pose asignificant risk to aviation safety and therefore require accurate and reliabledetection methods. In this paper, we present an advanced, explainable machinelearning method that utilizes Light Detection and Ranging (LiDAR) data foreffective wake vortex detection. Our method leverages a dynamic graph CNN(DGCNN) with semantic segmentation to partition a 3D LiDAR point cloud intomeaningful segments. Further refinement is achieved through clusteringtechniques. A novel feature of our research is the use of a perturbation-basedexplanation technique, which clarifies the model's decision-making processesfor air traffic regulators and controllers, increasing transparency andbuilding trust. Our experimental results, based on measured and simulated LiDARscans compared against four baseline methods, underscore the effectiveness andreliability of our approach. This combination of semantic segmentation andclustering for real-time wake vortex tracking significantly advances aviationsafety measures, ensuring that these are both effective and comprehensible.</description>
      <author>example@mail.com (Zhan Qu, Shuzhou Yuan, Michael Färber, Marius Brennfleck, Niklas Wartha, Anton Stephan)</author>
      <guid isPermaLink="false">2503.00518v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>AugFL: Augmenting Federated Learning with Pretrained Models</title>
      <link>http://arxiv.org/abs/2503.02154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  to be published in Transactions on Networking&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了如何通过利用预训练模型（PM）来增强联邦学习（FL），以解决分布式环境中由于隐私政策或存储限制而导致的训练数据稀缺问题。&lt;h4&gt;背景&lt;/h4&gt;近年来，联邦学习因其能够在保护用户隐私的同时进行大规模机器学习而引起了广泛关注。然而，在实际应用中，受限于严格的隐私规定和设备有限的存储能力，缺乏足够的训练数据常常阻碍了其有效部署。&lt;h4&gt;目的&lt;/h4&gt;通过引入预训练模型来增强FL系统的能力，以降低从头开始执行联邦学习所需的数据量，并提高模型在分布式环境中的适应性和性能。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一个基于正则化的元学习框架，在这个框架中，客户端协作学习一个从服务器存储的私有预训练模型中迁移知识得到的元模型。此外，开发了一种基于不精确ADMM算法的优化方案（AugFL），以在不暴露预训练模型且不增加本地计算成本的情况下解决该问题。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明了所提出方法的通信复杂性、适应性能以及一般非凸情况下的知识迁移收益。实验结果进一步证实了AugFL相较于现有基线的有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种新颖的方法来增强联邦学习系统，特别是在数据稀缺的情况下通过利用预训练模型的知识转移能力显著提高了学习效率和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已经完全翻译为中文。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Learning (FL) has garnered widespread interest in recent years.However, owing to strict privacy policies or limited storage capacities oftraining participants such as IoT devices, its effective deployment is oftenimpeded by the scarcity of training data in practical decentralized learningenvironments. In this paper, we study enhancing FL with the aid of (large)pre-trained models (PMs), that encapsulate wealthy general/domain-agnosticknowledge, to alleviate the data requirement in conducting FL from scratch.Specifically, we consider a networked FL system formed by a central server anddistributed clients. First, we formulate the PM-aided personalized FL as aregularization-based federated meta-learning problem, where clients join forcesto learn a meta-model with knowledge transferred from a private PM stored atthe server. Then, we develop an inexact-ADMM-based algorithm, AugFL, tooptimize the problem with no need to expose the PM or incur additionalcomputational costs to local clients. Further, we establish theoreticalguarantees for AugFL in terms of communication complexity, adaptationperformance, and the benefit of knowledge transfer in general non-convex cases.Extensive experiments corroborate the efficacy and superiority of AugFL overexisting baselines.</description>
      <author>example@mail.com (Sheng Yue, Zerui Qin, Yongheng Deng, Ju Ren, Yaoxue Zhang, Junshan Zhang)</author>
      <guid isPermaLink="false">2503.02154v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Intermediate-Task Transfer Learning: Leveraging Sarcasm Detection for Stance Detection</title>
      <link>http://arxiv.org/abs/2503.03172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 2 figures, published in The Sixteenth International  Conference on Information (eKNOW 2024)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种用于社交媒体立场检测（SD）的新方法，利用讽刺识别作为中间任务的迁移学习来提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;社交媒体上的立场检测由于其在社会商业和政治应用中的重要性而成为自然语言处理领域的一个研究热点。然而，在线平台文本的微妙性和复杂性对SD算法提出了挑战，特别是当包含讽刺或比喻的语言时。&lt;h4&gt;目的&lt;/h4&gt;该论文旨在通过引入讽刺识别中间任务来改进现有的SD模型，以提高其准确率和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;该方法包括微调BERT和RoBERTa，并将卷积BiLSTM与密集层连接。此外，还进行了详尽的实验测试，以评估迁移学习框架的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，集成讽刺识别知识到模型中有助于减少对讽刺文本元素的误分类，从而提高SD任务中的准确率和F1分数。特别是在85%的情况下，该模型能够正确预测之前没有进行讽刺预训练时被错误分类的文本。&lt;h4&gt;结论&lt;/h4&gt;这项研究是首次将讽刺检测作为中间迁移学习任务应用于SD的研究，并且通过结合BERT或RoBERTa与其他深度学习技术证明了其有效性。这为未来在这一领域的研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已全部翻译成中文并进行了总结，涵盖了论文的主要贡献和发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stance Detection (SD) on social media has emerged as a prominent area ofinterest with implications for social business and political applicationsthereby garnering escalating research attention within NLP. The inherentsubtlety and complexity of texts procured from online platforms pose challengesfor SD algorithms in accurately discerning the authors stance. Mostly theinclusion of sarcastic and figurative language drastically impacts theperformance of SD models. This paper addresses this by employing sarcasmdetection intermediate-task transfer learning tailored for SD. The proposedmethodology involves the finetuning of BERT and RoBERTa and the concatenationof convolutional BiLSTM and dense layers. Rigorous experiments are conducted onpublicly available datasets to evaluate our transfer-learning framework. Theperformance of the approach is assessed against various State-Of-The-Artbaselines for SD providing empirical evidence of its effectiveness. Notably ourmodel outperforms the best SOTA models even prior to sarcasm-detectionpretraining. The integration of sarcasm knowledge into the model provesinstrumental in mitigating misclassifications of sarcastic textual elements inSD. Our model accurately predicts 85% of texts that were previouslymisclassified by the model without sarcasm-detection pretraining therebyamplifying the average F1-score of the model. Our experiments also revealedthat the success of the transfer-learning framework is contingent upon thecorrelation of lexical attributes between the intermediate task and the targettask. This study represents the first exploration of sarcasm detection as anintermediate transfer-learning task in the context of SD and simultaneouslyuses the concatenation of BERT or RoBERTa with other deep-learning techniquesestablishing the proposed approach as a foundational baseline for futureresearch endeavors in this domain.</description>
      <author>example@mail.com (Gibson Nkhata, Susan Gauch)</author>
      <guid isPermaLink="false">2503.03172v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>YARE-GAN: Yet Another Resting State EEG-GAN</title>
      <link>http://arxiv.org/abs/2503.02636v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '研究提出了一种基于Wasserstein GAN（WGANGP）的方法，用于生成多通道静息态EEG数据，并通过视觉和特征基评估来衡量合成信号的质量。', '背景': '尽管生成对抗网络(GANs)在合成逼真的神经数据方面显示出潜力，但它们在无监督表示学习中的应用特别是在休息状态的脑电图(EEG)领域尚未得到充分探索。', '目的': '实施并评估一种Wasserstein GAN（WGANGP）方法来生成多通道静息态EEG数据，并研究其作为无监督特征提取器的能力。', '方法': '使用WGAN-GP模型生成多通道休息状态的EEG信号，通过视觉和基于特征的方法评价合成信号的质量。同时，训练模型以评估其对年龄分类任务中的表示学习能力。', '主要发现': '模型成功捕获了真实EEG数据的统计学和谱特性；然而，在前额区域复制高频振荡方面存在挑战；此外，生成器的判别器可以用于年龄组分类，表现出超出随机标签基线的准确率。', '结论': '研究结果表明，生成式模型不仅能够作为高质量EEG数据的合成工具，还能作为一种无监督特征提取的方法，减少手动特征工程的需求。这为基于GAN的无监督学习在EEG分析中的应用提供了新的可能性，并促进了更高效的数据驱动神经科学研究方法的发展。', '翻译': '摘要是关于利用Wasserstein GAN（WGANGP）生成真实多通道静息状态EEG数据的研究，该研究通过视觉和特征基评估来衡量合成信号的质量。它揭示了GAN在无监督表示学习中的潜力，并表明它们不仅可以生成高质量的EEG数据，还可以作为一种有效的无监督特征提取方法用于年龄组分类等任务。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative Adversarial Networks (GANs) have shown promise in synthesisingrealistic neural data, yet their potential for unsupervised representationlearning in resting-state EEG remains under explored. In this study, weimplement a Wasserstein GAN with Gradient Penalty (WGAN-GP) to generatemulti-channel resting-state EEG data and assess the quality of the synthesisedsignals through both visual and feature-based evaluations. Our results indicatethat the model effectively captures the statistical and spectralcharacteristics of real EEG data, although challenges remain in replicatinghigh-frequency oscillations in the frontal region. Additionally, we demonstratethat the Critic's learned representations can be fine-tuned for age groupclassification, achieving an out-of-sample accuracy, significantly better thana shuffled-label baseline. These findings suggest that generative models canserve not only as EEG data generators but also as unsupervised featureextractors, reducing the need for manual feature engineering. This studyhighlights the potential of GAN-based unsupervised learning for EEG analysis,suggesting avenues for more data-efficient deep learning applications inneuroscience.</description>
      <author>example@mail.com (Yeganeh Farahzadi, Morteza Ansarinia, Zoltan Kekecs)</author>
      <guid isPermaLink="false">2503.02636v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>TEDDY: A Family Of Foundation Models For Understanding Single Cell Biology</title>
      <link>http://arxiv.org/abs/2503.03485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究通过扩大预训练数据集和利用大规模生物注释来改进单细胞基础模型，以提高疾病生物学中的下游应用性能。&lt;h4&gt;背景&lt;/h4&gt;理解疾病的生物学机制对医学特别是药物发现至关重要。AI驱动的基因组规模生物数据分析在这一领域具有巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;探索改进当前最先进的方法，通过扩大预训练数据集和利用大规模生物注释来改善单细胞基础模型的表现。&lt;h4&gt;方法&lt;/h4&gt;第一，将预训练的数据集扩展到1亿1600万个细胞；第二，利用大规模的生物注释作为监督信息进行预训练。培训TEDDY家族的六个基于变压器的最先进的单细胞基础模型，参数分别为70百万、160百万和400百万。&lt;h4&gt;主要发现&lt;/h4&gt;随着数据量和参数数量的增长，性能可预测地提高；在第一个任务上显示出显著改进，在第二个任务上的改进较为温和。&lt;h4&gt;结论&lt;/h4&gt;扩大预训练的数据集以及利用大规模生物注释可以有效地提升单细胞基础模型的性能，为疾病生物学提供了更好的工具。&lt;h4&gt;翻译&lt;/h4&gt;理解疾病的生物学机制对医学特别是药物发现至关重要。AI驱动的基因组规模生物数据分析在这一领域具有巨大潜力。随着单细胞RNA测序数据的日益增多，大型基础模型的发展成为可能。然而，现有的基础模型要么没有改进下游应用性能，要么只是适度地改善了它们。本研究探索了两种提高当前最佳实践的方法：扩大预训练的数据集到1亿1600万个细胞，并利用大规模生物注释作为监督信息进行预训练。培训TEDDY家族的六个基于变压器的最先进的单细胞基础模型，参数分别为70百万、160百万和400百万。在两个下游评估任务上验证了这些模型：识别未见受试者背后的疾病状态以及区分健康细胞与患病细胞（这两种情况均未出现在训练中）。研究表明随着数据量和参数数量的增长，性能可预测地提高；在第一个任务上显示出显著改进，在第二个任务上的改进较为温和。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the biological mechanism of disease is critical for medicine,and in particular drug discovery. AI-powered analysis of genome-scalebiological data hold great potential in this regard. The increasingavailability of single-cell RNA sequencing data has enabled the development oflarge foundation models for disease biology. However, existing foundationmodels either do not improve or only modestly improve over task-specific modelsin downstream applications. Here, we explored two avenues for improving thestate-of-the-art. First, we scaled the pre-training dataset to 116 millioncells, which is larger than those used by previous models. Second, we leveragedthe availability of large-scale biological annotations as a form of supervisionduring pre-training. We trained the TEDDY family of models comprising sixtransformer-based state-of-the-art single-cell foundation models with 70million, 160 million, and 400 million parameters. We vetted our models on twodownstream evaluation tasks -- identifying the underlying disease state ofheld-out donors not seen during training and distinguishing healthy cells fromdiseased ones for disease conditions and donors not seen during training.Scaling experiments showed that performance improved predictably with both datavolume and parameter count. Our models showed substantial improvement overexisting work on the first task and more muted improvements on the second.</description>
      <author>example@mail.com (Alexis Chevalier, Soumya Ghosh, Urvi Awasthi, James Watkins, Julia Bieniewska, Nichita Mitrea, Olga Kotova, Kirill Shkura, Andrew Noble, Michael Steinbaugh, Julien Delile, Christoph Meier, Leonid Zhukov, Iya Khalil, Srayanta Mukherjee, Judith Mueller)</author>
      <guid isPermaLink="false">2503.03485v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>BEVMOSNet: Multimodal Fusion for BEV Moving Object Segmentation</title>
      <link>http://arxiv.org/abs/2503.03280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Proceedings of the 20th International Joint Conference on Computer  Vision, Imaging and Computer Graphics Theory and Applications (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;针对自动驾驶车辆中鸟类视角下的动态物体运动理解的挑战，论文提出了BEVMOSNet系统，该系统结合了相机、激光雷达和雷达数据进行多模态融合，并在nuScenes数据集上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;当前关于场景内动态对象在鸟类视角下（BEV）的准确运动理解的研究相对较少。尽管有一些基于视觉的方法提出了初步结果，但在低光、夜间以及雨等恶劣天气条件下这些方法的表现严重下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效融合相机、激光雷达和雷达数据的端到端多模态系统，用于精准预测鸟类视角下移动物体的位置与运动状态。&lt;h4&gt;方法&lt;/h4&gt;引入了BEVMOSNet框架，这是首个利用相机、激光雷达以及雷达数据进行端到端多模态融合的方法。研究重点在于探索如何通过可变形交叉注意力机制指导传感器之间的信息共享优化策略。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上，与仅基于视觉的单模式基线BEV-MoSeg相比，BEVMOSNet实现了36.59%的整体IoU得分提升；与多模态SimpleBEV相比，提升了2.35%，确立了其在BEV运动分割领域的前沿地位。&lt;h4&gt;结论&lt;/h4&gt;通过综合利用不同类型的传感器数据，可以显著提高动态物体识别和跟踪的准确性，在复杂环境下的性能尤为突出。这为未来自动驾驶车辆中更可靠的安全保障和路径规划技术开发提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;对摘要内容进行了中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate motion understanding of the dynamic objects within the scene inbird's-eye-view (BEV) is critical to ensure a reliable obstacle avoidancesystem and smooth path planning for autonomous vehicles. However, this task hasreceived relatively limited exploration when compared to object detection andsegmentation with only a few recent vision-based approaches presentingpreliminary findings that significantly deteriorate in low-light, nighttime,and adverse weather conditions such as rain. Conversely, LiDAR and radarsensors remain almost unaffected in these scenarios, and radar provides keyvelocity information of the objects. Therefore, we introduce BEVMOSNet, to ourknowledge, the first end-to-end multimodal fusion leveraging cameras, LiDAR,and radar to precisely predict the moving objects in BEV. In addition, weperform a deeper analysis to find out the optimal strategy for deformablecross-attention-guided sensor fusion for cross-sensor knowledge sharing in BEV.While evaluating BEVMOSNet on the nuScenes dataset, we show an overallimprovement in IoU score of 36.59% compared to the vision-based unimodalbaseline BEV-MoSeg (Sigatapu et al., 2023), and 2.35% compared to themultimodel SimpleBEV (Harley et al., 2022), extended for the motionsegmentation task, establishing this method as the state-of-the-art in BEVmotion segmentation.</description>
      <author>example@mail.com (Hiep Truong Cong, Ajay Kumar Sigatapu, Arindam Das, Yashwanth Sharma, Venkatesh Satagopan, Ganesh Sistu, Ciaran Eising)</author>
      <guid isPermaLink="false">2503.03280v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-ICP: Iterative Closest Point for Non-rigid Multi-Organ Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2503.00972v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, submitted to MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;点云配准在计算机辅助干预中非常重要。尽管已经开发了基于学习的点云配准方法，但由于泛化性和可解释性的问题，这些方法难以应用于临床。因此，传统的点云配准方法（如迭代最近点算法ICP）仍然广泛用于CAI。然而，ICP方法未能考虑以下两个方面：1. 点具有明确的语义含义，每个点可以与特定解剖标签相关联；2. 变形需要遵循生物力学能量约束。&lt;h4&gt;背景&lt;/h4&gt;学习基于的方法在泛化性和解释性上存在挑战，因此传统的迭代最近点算法ICP仍广泛应用于计算机辅助干预中。然而，现有方法忽视了点的语义信息和变形的能量约束要求。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的语义ICP（sem-ICP）方法，该方法处理多点标签并使用线性弹性能量正则化来提高配准效果。&lt;h4&gt;方法&lt;/h4&gt;利用语义标签改进最近邻匹配的鲁棒性，并引入了一种新型点云变形表示形式以应用明确的生物力学能量正则化。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在Learn2reg腹部MR-CT注册数据集和一种经口机器人手术超声波-CT注册数据集中，与现有的ICP基线方法相比，提高了Hausdorff距离。同时进行的灵敏度研究表明，刚性初始化能够更好地适应不同的初始位置和可见比。&lt;h4&gt;结论&lt;/h4&gt;通过结合点云中的语义信息以及生物力学能量约束，提出的sem-ICP算法改进了现有ICP算法在配准任务上的性能。&lt;h4&gt;翻译&lt;/h4&gt;点云配准是计算机辅助干预（CAI）中的一项重要技术。尽管基于学习的方法已经开发出来，但由于泛化性和解释性的问题，这些方法难以应用于临床实践中。因此，在CAI领域内，传统的迭代最近点（ICP）算法仍然被广泛使用。然而，现有的ICP算法未能充分考虑以下两点：1. 每个点具有明确的语义含义，并且可以与特定解剖学标签相关联；2. 变形需要遵循生物力学能量约束。本文提出了一种新的方法——语义迭代最近点（sem-ICP），该方法能够处理多点标签，同时采用线性弹性能量正则化来增强配准效果。通过使用语义信息改进了最近邻匹配的鲁棒性，并引入了一种全新的点云变形表示形式以便应用明确的生物力学能量正则化。我们的实验在Learn2reg腹部MR-CT注册数据集和一种经口机器人手术超声波-CT注册数据集中进行了测试，结果显示相较于现有的ICP基线方法，改进后的Hausdorff距离表现更优。此外，我们还进行了一项敏感性研究，证明了刚性初始化在不同初始位置和可见比的情况下具有更好的收敛性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is important in computer-aided interventions (CAI).While learning-based point cloud registration methods have been developed,their clinical application is hampered by issues of generalizability andexplainability. Therefore, classical point cloud registration methods, such asIterative Closest Point (ICP), are still widely applied in CAI. ICP methodsfail to consider that: (1) the points have well-defined semantic meaning, inthat each point can be related to a specific anatomical label; (2) thedeformation needs to follow biomechanical energy constraints. In this paper, wepresent a novel semantic ICP (sem-ICP) method that handles multiple pointlabels and uses linear elastic energy regularization. We use semantic labels toimprove the robustness of the closest point matching and propose a new pointcloud deformation representation to apply explicit biomechanical energyregularization. Our experiments on the Learn2reg abdominal MR-CT registrationdataset and a trans-oral robotic surgery ultrasound-CT registration datasetshow that our method improves the Hausdorff distance compared with otherstate-of-the-art ICP-based registration methods. We also perform a sensitivitystudy to show that our rigid initialization achieves better convergence withdifferent initializations and visible ratios.</description>
      <author>example@mail.com (Wanwen Chen, Carson Studders, Jamie J. Y. Kwon, Emily H. T. Pang, Eitan Prisman, Septimiu E. Salcudean)</author>
      <guid isPermaLink="false">2503.00972v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2503.03313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;针对带有文本描述的节点（Text-Attributed Graphs，TAGs）在实际场景中的普遍存在及其独特的结构和领域特定知识的需求，提出了一个通用图基础模型（Graph Foundation Model, GFM），旨在跨多种图类型和任务中进行泛化。&lt;h4&gt;背景&lt;/h4&gt;现有的方法倾向于将大语言模型（Large Language Models，LLMs）与图神经网络（Graph Neural Networks，GNNs）分阶段地结合在一起处理TAGs，这种解耦架构限制了它们之间的协同潜力。此外，现有方法在处理图节点时使用了不兼容任务导向模板的词表外词汇分配策略。&lt;h4&gt;目的&lt;/h4&gt;为解决上述问题，提出了PromptGFM模型，该模型旨在通过图词汇学习实现通用图基础模型的设计。&lt;h4&gt;方法&lt;/h4&gt;PromptGFM包含两个主要组件：(1) 图理解模块，此模块使LLMs能够复制GNN的工作流程，并促进无缝的GNN-LLM融合和优雅的图文本对齐；(2) 图推理模块，该模块建立了一种基于语言的图词汇，确保表达性、可移植性和可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验验证了PromptGFM在多种图类型和任务上的优越性和迁移能力。&lt;h4&gt;结论&lt;/h4&gt;PromptGFM提供了处理带有文本描述节点图形的有效方法，并证明其具有良好的泛化能力和跨领域适应性。&lt;h4&gt;翻译&lt;/h4&gt;Text-Attributed Graphs (TAGs)，即每个节点都关联有文本描述的图，在现实场景中普遍存在。它们通常展现出独特的结构和特定领域的知识，促使开发一种能够跨越不同类型的图和任务的一般图基础模型（Graph Foundation Model, GFM）。尽管在整合大型语言模型（Large Language Models, LLMs）与图神经网络（GNNs）处理TAG方面付出了巨大努力，现有的方法由于采用了两阶段对齐的解耦架构而受到限制。更糟糕的是，现有方法将词汇外词分配给图节点，导致了特定于图的语义、标记爆炸以及与任务导向提示模板不兼容的问题，这阻碍了跨图和跨任务迁移能力。为解决这些挑战，我们提出了一种基于图词汇学习的基础通用图模型PromptGFM。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-Attributed Graphs (TAGs), where each node is associated with textdescriptions, are ubiquitous in real-world scenarios. They typically exhibitdistinctive structure and domain-specific knowledge, motivating the developmentof a Graph Foundation Model (GFM) that generalizes across diverse graphs andtasks. Despite large efforts to integrate Large Language Models (LLMs) andGraph Neural Networks (GNNs) for TAGs, existing approaches suffer fromdecoupled architectures with two-stage alignment, limiting their synergisticpotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokensto graph nodes, leading to graph-specific semantics, token explosion, andincompatibility with task-oriented prompt templates, which hinders cross-graphand cross-task transferability. To address these challenges, we proposePromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.PromptGFM comprises two key components: (1) Graph Understanding Module, whichexplicitly prompts LLMs to replicate the finest GNN workflow within the textspace, facilitating seamless GNN-LLM integration and elegant graph-textalignment; (2) Graph Inference Module, which establishes a language-based graphvocabulary ensuring expressiveness, transferability, and scalability, enablingreadable instructions for LLM fine-tuning. Extensive experiments demonstrateour superiority and transferability across diverse graphs and tasks. The codeis available at this: https://github.com/agiresearch/PromptGFM.</description>
      <author>example@mail.com (Xi Zhu, Haochen Xue, Ziwei Zhao, Wujiang Xu, Jingyuan Huang, Minghao Guo, Qifan Wang, Kaixiong Zhou, Yongfeng Zhang)</author>
      <guid isPermaLink="false">2503.03313v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?</title>
      <link>http://arxiv.org/abs/2503.02687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures, 4 tables, submitted to 2025 IEEE/RSJ  International Conference on Intelligent Robots and Systems (IROS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了将现有的混合样本数据增强（MSDA）技术应用于雷达点云的可行性，并提出了针对雷达点云的新方法CAPMix。&lt;h4&gt;背景&lt;/h4&gt;由于3D感知任务中数据收集和标注的工作量巨大，研究人员开发了许多利用现有数据生成多样化训练样本的方法。然而，这些方法大多数都是针对激光雷达数据设计的，对雷达点云的应用研究较少。&lt;h4&gt;目的&lt;/h4&gt;探讨将现有的混合样本数据增强（MSDA）技术应用于雷达点云的可行性，并识别其中存在的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了Class-Aware PillarMix (CAPMix) 方法，该方法在3D点云中的柱状体级别应用了基于类别标签指导的混合操作。此方法根据每个柱状体独立分配一个比例，从而增加样本多样性。为了考虑不同类别的密度，使用特定于类别的分布：对于稠密对象（如大型车辆），倾向于从另一样本中采样更多点；而对于稀疏对象（如行人），则更注重原始样本中的点。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示该方法不仅显著提升了性能，在两个数据集上的表现也优于现有的MSDA方法。&lt;h4&gt;结论&lt;/h4&gt;研究提出了一种针对雷达点云的新型混合样本数据增强方法，通过这种方式可以生成更多样化的训练数据，并认为此简洁而有效的方法将促进对雷达数据增强技术的研究进展。&lt;h4&gt;翻译&lt;/h4&gt;由于3D感知任务中的数据收集和标注工作量巨大，研究人员开发了许多利用现有数据生成多样化训练样本的技术。然而，这些方法大多针对激光雷达数据设计，对于雷达点云的应用研究较少。本文探讨了现有的混合样本数据增强（MSDA）技术应用于雷达点云的可行性，并提出了一种名为Class-Aware PillarMix (CAPMix) 的新方法来解决识别到的问题。该方法在3D点云中的柱状体级别应用基于类别标签指导的混合操作，通过独立分配比例提升样本多样性，并根据不同类别的密度使用特定于类别的分布策略进行调整。实验结果表明，这种方法不仅显著提升了性能，在两个数据集上的表现也优于现有的MSDA方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to the significant effort required for data collection and annotation in3D perception tasks, mixed sample data augmentation (MSDA) has been widelystudied to generate diverse training samples by mixing existing data. Recently,many MSDA techniques have been developed for point clouds, but they mainlytarget LiDAR data, leaving their application to radar point clouds largelyunexplored. In this paper, we examine the feasibility of applying existing MSDAmethods to radar point clouds and identify several challenges in adapting thesetechniques. These obstacles stem from the radar's irregular angulardistribution, deviations from a single-sensor polar layout in multi-radarsetups, and point sparsity. To address these issues, we propose Class-AwarePillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillarlevel in 3D point clouds, guided by class labels. Unlike methods that rely asingle mix ratio to the entire sample, CAPMix assigns an independent ratio toeach pillar, boosting sample diversity. To account for the density of differentclasses, we use class-specific distributions: for dense objects (e.g., largevehicles), we skew ratios to favor points from another sample, while for sparseobjects (e.g., pedestrians), we sample more points from the original. Thisclass-aware mixing retains critical details and enriches each sample with newinformation, ultimately generating more diverse training data. Experimentalresults demonstrate that our method not only significantly boosts performancebut also outperforms existing MSDA approaches across two datasets (Bosch Streetand K-Radar). We believe that this straightforward yet effective approach willspark further investigation into MSDA techniques for radar data.</description>
      <author>example@mail.com (Miao Zhang, Sherif Abdulatif, Benedikt Loesch, Marco Altmann, Bin Yang)</author>
      <guid isPermaLink="false">2503.02687v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>External Reliable Information-enhanced Multimodal Contrastive Learning for Fake News Detection</title>
      <link>http://arxiv.org/abs/2503.03107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by AAAI'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;随着互联网的快速发展，信息传播范式发生了变化，并且效率大大提高。然而，这也带来了假新闻的快速传播并导致了网络空间中的负面影响。&lt;h4&gt;背景&lt;/h4&gt;当前，信息展示格式逐渐演进，新闻形式从文本向多模态内容转变。因此，检测多模态假新闻已成为研究热点之一。&lt;h4&gt;目的&lt;/h4&gt;针对现有的多模态假新闻检测领域存在的两个主要挑战：无法充分有效地利用多模态信息进行检测和引入的外部信息可信度低或静态性质限制了动态更新的问题，提出一种增强型可靠外部信息多模态对比学习框架（ERIC-FND）。&lt;h4&gt;方法&lt;/h4&gt;该模型通过实体丰富化的外部信息强化新闻内容表示，并采用多模态语义交互方法丰富多模态新闻信息。其中采用了多模态对比学习，使得不同模态的表示可以从彼此中学习。此外，还采取了自适应融合方法来整合来自不同维度的新闻表示以最终实现分类。&lt;h4&gt;主要发现&lt;/h4&gt;在两个常用的跨语言数据集（X和Weibo）上进行实验后，结果表明所提出的模型ERIC-FND在相同设置下优于现有的最先进的假新闻检测方法。&lt;h4&gt;结论&lt;/h4&gt;通过上述研究展示了ERIC-FND框架的有效性，并为未来多模态假新闻检测的研究提供了新的思路和方向。&lt;h4&gt;翻译&lt;/h4&gt;随着互联网的快速发展，信息传播的方式发生了变化并且效率得到了极大的提升。然而这也带来了快速传播的假新闻并导致了网络空间中的负面影响。目前，信息展示格式逐渐演进，新闻形式从文本转向多元化的多模态内容。因此检测多模态假新闻已经成为研究热点之一。但是，在现有的多模态假新闻检测领域仍然面临两个主要挑战：无法充分有效地利用多模态的信息进行检测和引入的外部信息可信度低或静态性质限制了动态更新的问题。为了填补这些空白，我们提出了ERIC-FND框架，这是一种增强型可靠外部信息多模态对比学习框架用于假新闻检测。该模型通过实体丰富化的外部信息强化新闻内容表示，并采用多模态语义交互方法丰富多模态新闻信息。其中采用了多模态对比学习使得不同模态的表示可以从彼此中学习。此外，还采取了自适应融合方法来整合来自不同维度的新闻表示以最终实现分类。在两个常用的跨语言数据集（X和Weibo）上进行实验后，结果表明所提出的模型ERIC-FND在相同设置下优于现有的最先进的假新闻检测方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of the Internet, the information disseminationparadigm has changed and the efficiency has been improved greatly. While thisalso brings the quick spread of fake news and leads to negative impacts oncyberspace. Currently, the information presentation formats have evolvedgradually, with the news formats shifting from texts to multimodal contents. Asa result, detecting multimodal fake news has become one of the researchhotspots. However, multimodal fake news detection research field still facestwo main challenges: the inability to fully and effectively utilize multimodalinformation for detection, and the low credibility or static nature of theintroduced external information, which limits dynamic updates. To bridge thegaps, we propose ERIC-FND, an external reliable information-enhanced multimodalcontrastive learning framework for fake news detection. ERIC-FND strengthensthe representation of news contents by entity-enriched external informationenhancement method. It also enriches the multimodal news information viamultimodal semantic interaction method where the multimodal constrativelearning is employed to make different modality representations learn from eachother. Moreover, an adaptive fusion method is taken to integrate the newsrepresentations from different dimensions for the eventual classification.Experiments are done on two commonly used datasets in different languages, X(Twitter) and Weibo. Experiment results demonstrate that our proposed modelERIC-FND outperforms existing state-of-the-art fake news detection methodsunder the same settings.</description>
      <author>example@mail.com (Biwei Cao, Qihang Wu, Jiuxin Cao, Bo Liu, Jie Gui)</author>
      <guid isPermaLink="false">2503.03107v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Category-level Meta-learned NeRF Priors for Efficient Object Mapping</title>
      <link>http://arxiv.org/abs/2503.01582v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了PRENOM，一种基于先验的高效神经对象映射器，它结合了类别级别的形状先验和对象级别的NeRF来提高重建效率并支持典型物体姿态估计。&lt;h4&gt;背景&lt;/h4&gt;在3D对象映射中，DeepSDF作为一种主要使用的类别级形状先验，虽然能够提供高效的对象重构和典型姿势估计，但是难以重现锐利的几何结构并且计算成本高。与此相反，NeRF捕捉精细细节但尚未有效集成到实时多对象映射框架中。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法PRENOM来连接DeepSDF与NeRF之间的差距，通过结合类别级先验和对象级NeRF来提高重建效率并支持典型物体姿态估计。&lt;h4&gt;方法&lt;/h4&gt;1. 利用元学习从开源形状数据集中生成的合成重构任务进行训练。2. 使用多目标遗传算法为每个类优化NeRF架构以平衡重建设质量和训练时间。3. 采用基于先验的概率光线采样将采样指向预期对象区域，加速收敛并提高受限资源下的重建质量。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明PRENOM能够在低端GPU上实现高质量的重构同时保持计算可行性。对比无先验的NeRF方法，在合成数据集上的Chamfer距离降低了21%，在形状先验的真实世界嘈杂数据集中，所有重建指标平均提高了13%。&lt;h4&gt;结论&lt;/h4&gt;通过结合类别级和对象级先验，PRENOM展示了在实时多物体映射框架中的有效性，并且在训练时间减少5倍的情况下仍能保持准确的姿势和大小估计精度。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种名为PRENOM的新方法，该方法利用基于先验的信息来提高神经网络处理3D对象映射效率的同时支持典型姿态估计。通过结合DeepSDF和NeRF的优点，并采用多目标遗传算法优化架构以及概率光线采样策略加速收敛，实现高质量的重建效果并且在计算资源受限的情况下表现优异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In 3D object mapping, category-level priors enable efficient objectreconstruction and canonical pose estimation, requiring only a single prior persemantic category (e.g., chair, book, laptop). Recently, DeepSDF haspredominantly been used as a category-level shape prior, but it struggles toreconstruct sharp geometry and is computationally expensive. In contrast, NeRFscapture fine details but have yet to be effectively integrated withcategory-level priors in a real-time multi-object mapping framework. To bridgethis gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapperthat integrates category-level priors with object-level NeRFs to enhancereconstruction efficiency while enabling canonical object pose estimation.PRENOM gets to know objects on a first-name basis by meta-learning on syntheticreconstruction tasks generated from open-source shape datasets. To account forobject category variations, it employs a multi-objective genetic algorithm tooptimize the NeRF architecture for each category, balancing reconstructionquality and training time. Additionally, prior-based probabilistic ray samplingdirects sampling toward expected object regions, accelerating convergence andimproving reconstruction quality under constrained resources. Experimentalresults on a low-end GPU highlight the ability of PRENOM to achievehigh-quality reconstructions while maintaining computational feasibility.Specifically, comparisons with prior-free NeRF-based approaches on a syntheticdataset show a 21% lower Chamfer distance, demonstrating better reconstructionquality. Furthermore, evaluations against other approaches using shape priorson a noisy real-world dataset indicate a 13% improvement averaged across allreconstruction metrics, and comparable pose and size estimation accuracy, whilebeing trained for 5x less time.</description>
      <author>example@mail.com (Saad Ejaz, Hriday Bavle, Laura Ribeiro, Holger Voos, Jose Luis Sanchez-Lopez)</author>
      <guid isPermaLink="false">2503.01582v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Teaching AI to Handle Exceptions: Supervised Fine-Tuning with Human-Aligned Judgment</title>
      <link>http://arxiv.org/abs/2503.02976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型语言模型（LLM）正在从生成式AI发展为代理式AI，即在复杂现实场景中做出决策的系统。然而，尽管其生成能力已被充分研究，但它们的决策过程仍然不被人们深入理解。&lt;h4&gt;背景&lt;/h4&gt;当LLM处理例外情况时，这种问题尤为明显，因为合同内在的不完备性使这类挑战更加突出。&lt;h4&gt;目的&lt;/h4&gt;展示即使是最擅长推理的LLM也会因其严格遵循政策而与人类判断相去甚远。研究如何调整AI代理以更好地处理例外情况的方法。&lt;h4&gt;方法&lt;/h4&gt;评估了三种调整AI代理以处理例外情况的方法：伦理框架提示、链式思维推理和监督微调，其中尤其关注有解释性的人类反馈的监督微调。&lt;h4&gt;主要发现&lt;/h4&gt;伦理框架提示效果不佳；链式思维推理仅有轻微改善；而带有人类解释的监督微调显著提升了结果，并且能够使模型在新的场景中泛化出与人类相似的决策模式。这表明，为了将LLM与人类判断对齐，需要明确训练它们如何做出决策，而不仅仅是告诉它们应该做什么。&lt;h4&gt;结论&lt;/h4&gt;研究强调了解决LLMs处理例外情况不足的问题的重要性，以引导代理式AI的发展方向，使之能够更好地与人类判断相匹配，并适应新的环境。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型最初用于生成型人工智能领域，现在正在进化为代理型人工智能系统，在复杂的现实场景中进行决策。然而，尽管它们的生成能力得到了充分研究，但其决策过程仍然不为人所深入理解。这在LLM处理例外情况时尤为明显，因为合同内在的不完备性使得这类挑战更为突出。本研究表明，即使是最擅长推理的大型语言模型（LLMs）也会因其严格遵循规则而与人类判断相去甚远。我们评估了三种调整AI代理以更好地处理异常的方法：伦理框架提示、链式思维推理以及有监督的微调，并发现带有解释的人类反馈进行的监督微调方法效果最好，能够使模型在新的场景中泛化出类似人类决策模式。这项研究表明，要将LLMs与人类判断对齐，需要明确训练它们如何做出决定，而不仅仅是告诉它们应该做什么。这些发现强调了解决大型语言模型处理例外情况不足的重要性，以引导代理型人工智能的发展方向，使之能够更好地匹配人类判断，并适应新的环境。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs), initially developed for generative AI, are nowevolving into agentic AI systems, which make decisions in complex, real-worldcontexts. Unfortunately, while their generative capabilities arewell-documented, their decision-making processes remain poorly understood. Thisis particularly evident when models are handling exceptions, a critical andchallenging aspect of decision-making made relevant by the inherentincompleteness of contracts. Here we demonstrate that LLMs, even ones thatexcel at reasoning, deviate significantly from human judgments because theyadhere strictly to policies, even when such adherence is impractical,suboptimal, or even counterproductive. We then evaluate three approaches totuning AI agents to handle exceptions: ethical framework prompting,chain-of-thought reasoning, and supervised fine-tuning. We find that whileethical framework prompting fails and chain-of-thought prompting provides onlyslight improvements, supervised fine-tuning, specifically with humanexplanations, yields markedly better results. Surprisingly, in our experiments,supervised fine-tuning even enabled models to generalize human-likedecision-making to novel scenarios, demonstrating transfer learning ofhuman-aligned decision-making across contexts. Furthermore, fine-tuning withexplanations, not just labels, was critical for alignment, suggesting thataligning LLMs with human judgment requires explicit training on how decisionsare made, not just which decisions are made. These findings highlight the needto address LLMs' shortcomings in handling exceptions in order to guide thedevelopment of agentic AI toward models that can effectively align with humanjudgment and simultaneously adapt to novel contexts.</description>
      <author>example@mail.com (Matthew DosSantos DiSorbo, Harang Ju, Sinan Aral)</author>
      <guid isPermaLink="false">2503.02976v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Intrinsic and Extrinsic Factor Disentanglement for Recommendation in Various Context Scenarios</title>
      <link>http://arxiv.org/abs/2503.03524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 13 figures, 11 tables. Accepted by Transactions of  Information Systems&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个区分内在和外在因素的推荐模型IEDR，它考虑了多种上下文环境的影响，从而提高了推荐系统的准确性。&lt;h4&gt;背景&lt;/h4&gt;用户行为（如购买、点击）在不同情境下可能有显著差异。这种差异是由用户的内在偏好和外在激励共同决定的，而这些外在激励会随着时间和地点等因素的变化而变化。&lt;h4&gt;目的&lt;/h4&gt;通过区分内在因素和外在因素来改善推荐系统的准确性和学习用户行为。&lt;h4&gt;方法&lt;/h4&gt;提出了一种通用框架IEDR模型，该模型能够同时考虑多种上下文环境的影响，将内在因素和外在因素区分开来。此模型包含一个不受上下文影响的对比学习组件，用于捕捉内在因素，并有一个解纠缠组件，在各种情境相互作用的情况下提取外在因素。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明IEDR在多个真实世界的数据集上展示了其区分和学习不同因素的有效性，能够显著提升推荐准确度（高达4% NDCG）。&lt;h4&gt;结论&lt;/h4&gt;IEDR模型通过考虑多种上下文环境的影响，提高了内在因素与外在因素的区分准确性，从而改善了推荐系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;在推荐系统中，用户的行为模式可能因时间、地点等不同情境而显著变化。这是由于用户行为是由反映持续偏好（内在因素）和外部激励（外在因素）决定的。这些外部激励会根据不同的上下文环境发生变化。区分内外部因素有助于更好地学习用户行为。然而，现有的研究仅考虑从单个预定义的情境中区分它们的影响（如时间或地点），忽略了用户外部因素可能会受到同时作用的不同上下文中相互影响的事实。本文提出了一种通用框架——内在-外在解缠推荐模型IEDR，该模型可以在多种情景下分别提取出内在和外在的因素，从而更准确地区分这些因素并提高推荐的准确性。IEDR模型包括一个不受上下文影响的对比学习组件来捕捉内在因素，以及一个解纠缠组件，在各种情境相互作用的情况下可以提取外部因素。两个组件共同工作以实现有效的因子学习。大量的实验证明了IEDR在真实数据集上区分和学习独立因素的有效性，并通过最高提升4%的NDCG值显著提高了推荐准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recommender systems, the patterns of user behaviors (e.g., purchase,click) may vary greatly in different contexts (e.g., time and location). Thisis because user behavior is jointly determined by two types of factors:intrinsic factors, which reflect consistent user preference, and extrinsicfactors, which reflect external incentives that may vary in different contexts.Differentiating between intrinsic and extrinsic factors helps learn userbehaviors better. However, existing studies have only considereddifferentiating them from a single, pre-defined context (e.g., time orlocation), ignoring the fact that a user's extrinsic factors may be influencedby the interplay of various contexts at the same time. In this paper, wepropose the Intrinsic-Extrinsic Disentangled Recommendation (IEDR) model, ageneric framework that differentiates intrinsic from extrinsic factorsconsidering various contexts simultaneously, enabling more accuratedifferentiation of factors and hence the improvement of recommendationaccuracy. IEDR contains a context-invariant contrastive learning component tocapture intrinsic factors, and a disentanglement component to extract extrinsicfactors under the interplay of various contexts. The two components worktogether to achieve effective factor learning. Extensive experiments onreal-world datasets demonstrate IEDR's effectiveness in learning disentangledfactors and significantly improving recommendation accuracy by up to 4% inNDCG.</description>
      <author>example@mail.com (Yixin Su, Wei Jiang, Fangquan Lin, Cheng Yang, Sarah M. Erfani, Junhao Gan, Yunxiang Zhao, Ruixuan Li, Rui Zhang)</author>
      <guid isPermaLink="false">2503.03524v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Tiny Lidars for Manipulator Self-Awareness: Sensor Characterization and Initial Localization Experiments</title>
      <link>http://arxiv.org/abs/2503.03449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, 3 tables, conference submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用来自微型VL53L5CX ToF传感器（小型激光雷达）的粗略点云来定位机器人工作空间中目标对象的方法。&lt;h4&gt;背景&lt;/h4&gt;在许多任务，如操作和检查等场景下，对于机器人来说，能够在周围环境中精确定位目标物体是有益的。&lt;h4&gt;目的&lt;/h4&gt;利用微型ToF传感器获得的数据来改进机器人对其工作空间内目标物的位置估计准确性。&lt;h4&gt;方法&lt;/h4&gt;- 进行实验校准了传感器读数与相对距离及角度之间的依赖关系。- 提出了一种概率性传感器模型，并在使用粒子滤波器（PF）的对象姿态估计任务中对该模型进行了验证。&lt;h4&gt;主要发现&lt;/h4&gt;提出的传感器模型相对于两个基准，即测量值无不确定性假设和由传感器数据表提供的置信度，提高了目标对象定位的性能。&lt;h4&gt;结论&lt;/h4&gt;通过采用更准确的概率性传感器模型，机器人在执行各种任务时能够更好地识别并定位目标物体的位置。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已翻译为中文，并且根据其核心要点进行了分点总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For several tasks, ranging from manipulation to inspection, it is beneficialfor robots to localize a target object in their surroundings. In this paper, wepropose an approach that utilizes coarse point clouds obtained fromminiaturized VL53L5CX Time-of-Flight (ToF) sensors (tiny lidars) to localize atarget object in the robot's workspace. We first conduct an experimentalcampaign to calibrate the dependency of sensor readings on relative range andorientation to targets. We then propose a probabilistic sensor model that isvalidated in an object pose estimation task using a Particle Filter (PF). Theresults show that the proposed sensor model improves the performance of thelocalization of the target object with respect to two baselines: one thatassumes measurements are free from uncertainty and one in which the confidenceis provided by the sensor datasheet.</description>
      <author>example@mail.com (Giammarco Caroleo, Alessandro Albini, Daniele De Martini, Timothy D. Barfoot, Perla Maiolino)</author>
      <guid isPermaLink="false">2503.03449v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Mineral segmentation using electron microscope images and spectral sampling through multimodal graph neural networks</title>
      <link>http://arxiv.org/abs/2503.03507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于图神经网络的方法，用于融合多模态扫描电子显微镜（SEM）图像进行矿物分割。&lt;h4&gt;背景&lt;/h4&gt;通常情况下，通过SEM获取的背散射电子（BSE）图像并不包含足够的信息来进行准确的矿物分割。因此，通常会使用能量色散X射线光谱（EDS）测量来补充BSE图像的数据，以提供关于化学成分的高度精确的信息，但这些数据采集起来非常耗时。&lt;h4&gt;目的&lt;/h4&gt;利用稀疏的光谱数据与BSE图像一起进行矿物分割。&lt;h4&gt;方法&lt;/h4&gt;提出使用图神经网络将两种模态融合，并同时完成矿物相的分割。&lt;h4&gt;主要发现&lt;/h4&gt;即使仅提供1%的BSE像素上的EDS数据，也能产生准确的分割结果，从而实现了对矿物样品的快速分析。&lt;h4&gt;结论&lt;/h4&gt;提出的这种数据融合管道具有很好的灵活性，可以适应其他需要图像数据和点测量领域的应用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于图神经网络的方法，用于根据多模态扫描电子显微镜（SEM）图像的数据融合进行分割。在大多数情况下，通过SEM获取的背散射电子（BSE）图像并不包含足够的信息来进行矿物分割。因此，成像通常会与点测量的能量色散X射线光谱（EDS）光谱数据相结合，这些数据提供关于化学成分的高度精确的信息，但采集起来非常耗时。这激发了使用稀疏的光谱数据结合BSE图像进行矿物分割的需求。由于光谱数据的本质是无结构化的，大多数传统的图像融合技术都不适合用于BSE-EDS融合。我们提出采用图神经网络来融合两种模态，并同时完成矿物相的分割。我们的结果表明，在提供给定1% BSE像素上的EDS数据的情况下可以实现准确的分割，这使得能够快速分析矿物样品。提出的这种数据融合管道具有很好的灵活性，可以适应其他需要图像数据和点测量领域的应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel Graph Neural Network-based method for segmentation basedon data fusion of multimodal Scanning Electron Microscope (SEM) images. In mostcases, Backscattered Electron (BSE) images obtained using SEM do not containsufficient information for mineral segmentation. Therefore, imaging is oftencomplemented with point-wise Energy-Dispersive X-ray Spectroscopy (EDS)spectral measurements that provide highly accurate information about thechemical composition but that are time-consuming to acquire. This motivates theuse of sparse spectral data in conjunction with BSE images for mineralsegmentation. The unstructured nature of the spectral data makes mosttraditional image fusion techniques unsuitable for BSE-EDS fusion. We proposeusing graph neural networks to fuse the two modalities and segment the mineralphases simultaneously. Our results demonstrate that providing EDS data for asfew as 1% of BSE pixels produces accurate segmentation, enabling rapid analysisof mineral samples. The proposed data fusion pipeline is versatile and can beadapted to other domains that involve image data and point-wise measurements.</description>
      <author>example@mail.com (Samuel Repka, Bořek Reich, Fedor Zolotarev, Tuomas Eerola, Pavel Zemčík)</author>
      <guid isPermaLink="false">2503.03507v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Attributed Dynamic Network Embedding with Stability Guarantees</title>
      <link>http://arxiv.org/abs/2503.02859v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了AUASE，一种用于动态网络的稳定无监督表示学习框架，适用于节点带有随时间变化属性信息的情况。&lt;h4&gt;背景&lt;/h4&gt;在处理具有时变属性信息的动态网络时，稳定性对于确保相同行为的节点在同一时间点拥有相同的嵌入至关重要，这使得可以在不同时刻对比网络中的节点。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的表示学习方法AUASE，并证明其能够在没有地面真实标签的情况下满足稳定性保证。&lt;h4&gt;方法&lt;/h4&gt;通过展示AUASE的一致收敛性到相关的潜在位置模型来建立稳定性。利用三种真实的属性网络，在链接预测和节点分类任务中将AUASE与最先进的网络表示学习方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;AUASE是在没有地面真实标签的情况下唯一能够满足稳定性的属性动态嵌入，实验表明其对于链接预测和节点分类提供了显著的改进。&lt;h4&gt;结论&lt;/h4&gt;AUASE为处理具有时变属性信息的动态网络提供了一个强大的工具，并且在不需要标签的情况下确保了表示学习的稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stability for dynamic network embeddings ensures that nodes behaving the sameat different times receive the same embedding, allowing comparison of nodes inthe network across time. We present attributed unfolded adjacency spectralembedding (AUASE), a stable unsupervised representation learning framework fordynamic networks in which nodes are attributed with time-varying covariateinformation. To establish stability, we prove uniform convergence to anassociated latent position model. We quantify the benefits of our dynamicembedding by comparing with state-of-the-art network representation learningmethods on three real attributed networks. To the best of our knowledge, AUASEis the only attributed dynamic embedding that satisfies stability guaranteeswithout the need for ground truth labels, which we demonstrate providessignificant improvements for link prediction and node classification.</description>
      <author>example@mail.com (Emma Ceccherini, Ian Gallagher, Andrew Jones, Daniel Lawson)</author>
      <guid isPermaLink="false">2503.02859v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Meta Learning-Driven Iterative Refinement for Robust Anomaly Detection in Industrial Inspection</title>
      <link>http://arxiv.org/abs/2503.01569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in the VISION workshop at ECCV 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索了鲁棒异常检测模型在工业检查中的性能，特别是在处理噪声数据方面的能力。&lt;h4&gt;背景&lt;/h4&gt;当前的工业检查中存在大量噪声数据，这对传统的异常检测模型构成了挑战。这些噪声可能导致错误的缺陷识别和高误报率。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来提高模型适应性和鲁棒性，通过元学习方法识别并拒绝训练中的噪声数据以改进学习过程。&lt;h4&gt;方法&lt;/h4&gt;采用无模型假设元学习（MAML）以及迭代细化流程，利用四分位距剔除方案增强其可适应性和鲁棒性。这种方法可以有效提升模型区分正常与异常样本的能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法在噪声环境中的表现优于传统模型，并且即使是在清晰训练集的情况下也能很好地隔离那些偏离分布的样本，从而提供显著改进。&lt;h4&gt;结论&lt;/h4&gt;提出的方法不仅适用于高噪音环境下，而且也可以用于处理标准数据集，在保证准确性的前提下提高了异常检测的效率和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;这项研究探讨了鲁棒性异常检测模型在工业检查中的表现，特别是它们应对噪声数据的能力。我们建议利用元学习方法的适应能力来识别并排除训练数据中的噪声，以改进学习过程。在我们的模型中，我们使用了无模型假设元学习（MAML）以及迭代细化流程通过四分位距剔除方案提高其可适应性和鲁棒性。这种方法显著增强了区分正常和缺陷条件的能力。我们在著名的MVTec和KSDD2数据集上进行的实验结果表明，所提出的方法不仅在大量噪声环境中表现出色，而且还可以为明确训练集合贡献，在分离那些偏离分布样本的同时提供了对传统模型的重要改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the performance of robust anomaly detection models inindustrial inspection, focusing particularly on their ability to handle noisydata. We propose to leverage the adaptation ability of meta learning approachesto identify and reject noisy training data to improve the learning process. Inour model, we employ Model Agnostic Meta Learning (MAML) and an iterativerefinement process through an Inter-Quartile Range rejection scheme to enhancetheir adaptability and robustness. This approach significantly improves themodels capability to distinguish between normal and defective conditions. Ourresults of experiments conducted on well known MVTec and KSDD2 datasetsdemonstrate that the proposed method not only excels in environments withsubstantial noise but can also contribute in case of a clear training set,isolating those samples that are relatively out of distribution, thus offeringsignificant improvements over traditional models.</description>
      <author>example@mail.com (Muhammad Aqeel, Shakiba Sharifi, Marco Cristani, Francesco Setti)</author>
      <guid isPermaLink="false">2503.01569v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Remote Sensing Image Classification Using Convolutional Neural Network (CNN) and Transfer Learning Techniques</title>
      <link>http://arxiv.org/abs/2503.02510v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is published in Journal of Computer Science, Volume 21 No.  3, 2025. It contains 635-645 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;研究探讨了利用卷积神经网络（CNN）架构对包含输电塔、森林、农田和山脉的航空图像进行分类。&lt;h4&gt;目的&lt;/h4&gt;旨在通过实验来评估不同模型在土地覆盖分类任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;采用预训练的VGG16和MobileNetV2模型作为迁移学习的基础，利用自收集与MLRNet数据集结合的数据集进行测试。该数据集包含从Google卫星图像中提取的共10,400张图片。&lt;h4&gt;主要发现&lt;/h4&gt;{'总体表现': '基于构建的CNN模型的整体准确率为87%；', 'VGG16性能': '使用预训练的VGG16进行迁移学习后，准确率达到90%，测试损失为0.298；', 'MobileNetV2优势': '而采用MobileNetV2进行转移学习后的模型表现最佳，其准确率高达96%，且测试损失低至0.119。'}&lt;h4&gt;结论&lt;/h4&gt;研究表明，在土地覆盖分类任务中使用迁移学习，尤其是基于MobileNetV2的模型，可以取得优异的效果；这不仅提高了精度也增强了实用性。&lt;h4&gt;翻译&lt;/h4&gt;这项研究调查了利用卷积神经网络（CNN）架构从描绘输电塔、森林、农田和山脉的航空图像中提取特征，并通过Softmax进行分类。为了测试该模型，我们运行了十轮训练，使用批次大小为90，Adam优化器和学习率为0.001。在自收集图片与MLRNet数据集相结合的数据集中进行了训练及评估，包括从Google卫星影像中获得的共10,400张图像。研究表明，迁移学习模型尤其是MobileNetV2，在土地覆盖分类方面表现优异；这些模型因其良好的精度和效率平衡而适合实际应用；我们的方法在构建的CNN模型上达到了87%的整体准确率；通过使用预训练的VGG16与MobileNetV2作为迁移学习的基础，我们实现了更高的准确性。特别是，VGG16达到90%的准确性和0.298的测试损失，而MobileNetV2则以96%的准确度和0.119的测试损失领先所有模型；这些结果证明了使用基于MobileNetV2迁移学习进行输电塔、森林、农田和山脉分类的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3844/jcssp.2025.635.645&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the classification of aerial images depictingtransmission towers, forests, farmland, and mountains. To complete theclassification job, features are extracted from input photos using aConvolutional Neural Network (CNN) architecture. Then, the images areclassified using Softmax. To test the model, we ran it for ten epochs using abatch size of 90, the Adam optimizer, and a learning rate of 0.001. Bothtraining and assessment are conducted using a dataset that blendsself-collected pictures from Google satellite imagery with the MLRNet dataset.The comprehensive dataset comprises 10,400 images. Our study shows thattransfer learning models and MobileNetV2 in particular, work well for landscapecategorization. These models are good options for practical use because theystrike a good mix between precision and efficiency; our approach achievesresults with an overall accuracy of 87% on the built CNN model. Furthermore, wereach even higher accuracies by utilizing the pretrained VGG16 and MobileNetV2models as a starting point for transfer learning. Specifically, VGG16 achievesan accuracy of 90% and a test loss of 0.298, while MobileNetV2 outperforms bothmodels with an accuracy of 96% and a test loss of 0.119; the resultsdemonstrate the effectiveness of employing transfer learning with MobileNetV2for classifying transmission towers, forests, farmland, and mountains.</description>
      <author>example@mail.com (Mustafa Majeed Abd Zaid, Ahmed Abed Mohammed, Putra Sumari)</author>
      <guid isPermaLink="false">2503.02510v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>GNNMerge: Merging of GNN Models Without Accessing Training Data</title>
      <link>http://arxiv.org/abs/2503.03384v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '提出了GNNMerge算法，用于合并图神经网络（GNN），在各种数据集、任务和架构上验证了其优越性和效率。', '背景': '模型融合作为一种将多个训练好的模型整合成单一模型的方法，在不访问原始训练数据的情况下得到了广泛的应用。尽管这种技术已经在计算机视觉和自然语言处理领域显示出成功，但将其应用于GNN上的研究尚属空白。', '目的': '首次对GNN的模型合并算法进行基准测试，并提出一种新的方法来解决现有方法在GNN中的应用限制。', '方法': '提出了名为GNNMerge的新策略，该策略使用任务无关节点嵌入对齐技术来实现GNN模型的合并。此外，它还展示了在轻微放宽的情况下，提出的优化目标对于广泛使用的GNN架构可以提供直接解析解。', '主要发现': '实验表明，与现有方法相比，GNNMerge提高了24%的准确性，并且与从头训练相比，计算效率提高了一个数量级以上。', '结论': 'GNNMerge通过任务无关节点嵌入对齐技术成功解决了模型合并中的挑战，展示了在不同场景下的优越性能和速度优势。'}&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了机器学习领域中将多个已训练的模型整合为单一模型的过程，并探讨了现有方法如何在计算机视觉和自然语言处理等领域取得成功，但在图神经网络（GNN）中的应用仍然空白。论文作者进行了首次针对GNN的模型合并算法基准测试研究，揭示了这些算法在此上下文中效果有限的问题。为了应对这一挑战，他们提出了GNNMerge算法，该算法利用了一种任务无关节点嵌入对齐策略来实现GNN模型的合并，并且在轻微放宽假设的情况下展示了优化目标对于广泛使用的GNN架构可以提供直接解析解，显著提高了计算效率。实验结果表明，在各种数据集、任务和架构下，与现有方法相比，GNNMerge准确率最多提高24%，同时相对于从头开始训练的速度提升了两个数量级以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model merging has gained prominence in machine learning as a method tointegrate multiple trained models into a single model without accessing theoriginal training data. While existing approaches have demonstrated success indomains such as computer vision and NLP, their application to Graph NeuralNetworks (GNNs) remains unexplored. These methods often rely on the assumptionof shared initialization, which is seldom applicable to GNNs. In this work, weundertake the first benchmarking study of model merging algorithms for GNNs,revealing their limited effectiveness in this context. To address thesechallenges, we propose GNNMerge, which utilizes a task-agnostic node embeddingalignment strategy to merge GNNs. Furthermore, we establish that under a mildrelaxation, the proposed optimization objective admits direct analyticalsolutions for widely used GNN architectures, significantly enhancing itscomputational efficiency. Empirical evaluations across diverse datasets, tasks,and architectures establish GNNMerge to be up to 24% more accurate thanexisting methods while delivering over 2 orders of magnitude speed-up comparedto training from scratch.</description>
      <author>example@mail.com (Vipul Garg, Ishita Thakre, Sayan Ranu)</author>
      <guid isPermaLink="false">2503.03384v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>PABBO: Preferential Amortized Black-Box Optimization</title>
      <link>http://arxiv.org/abs/2503.00924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 17 figures. Accepted at the Thirteenth International  Conference on Learning Representations (ICLR 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Preferential Bayesian Optimization (PBO)是一种通过用户对设计配对的偏好反馈高效学习潜在用户偏好的方法。它使用统计代理模型，如高斯过程，并采用获取策略选择下一个候选配对以获得用户反馈。&lt;h4&gt;背景&lt;/h4&gt;由于偏好贝叶斯优化中的似然非共轭性，每次步骤都需要大量的近似推理计算，这与人机交互的方式不兼容，限制了其在实际案例中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于最新进展的推算化贝叶斯优化方法来克服这一问题，并通过元学习代理和获取函数实现PBO的完全推算化。&lt;h4&gt;方法&lt;/h4&gt;该方法包含一个新颖的转换器神经过程架构，采用强化学习训练并使用定制辅助损失进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在合成数据集和真实世界数据集组成的基准测试上，这种方法比传统的高斯过程策略快几个数量级，并且通常在准确性方面也优于它们。&lt;h4&gt;结论&lt;/h4&gt;这项研究展示了一种高效的推算化偏好贝叶斯优化方法，为实际应用中的用户偏好学习提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Preferential Bayesian Optimization (PBO)是一种高效的学习潜在用户偏好的方法，通过从设计配对的偏好反馈中获取信息。由于非共轭性问题，每次步骤都涉及大量计算。本文提出了一种基于推算化贝叶斯优化的方法，它可以通过元学习代理和获取函数实现完全推算化，并且在实际测试数据集上表现出更高的效率和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Preferential Bayesian Optimization (PBO) is a sample-efficient method tolearn latent user utilities from preferential feedback over a pair of designs.It relies on a statistical surrogate model for the latent function, usually aGaussian process, and an acquisition strategy to select the next candidate pairto get user feedback on. Due to the non-conjugacy of the associated likelihood,every PBO step requires a significant amount of computations with variousapproximate inference techniques. This computational overhead is incompatiblewith the way humans interact with computers, hindering the use of PBO inreal-world cases. Building on the recent advances of amortized BO, we proposeto circumvent this issue by fully amortizing PBO, meta-learning both thesurrogate and the acquisition function. Our method comprises a noveltransformer neural process architecture, trained using reinforcement learningand tailored auxiliary losses. On a benchmark composed of synthetic andreal-world datasets, our method is several orders of magnitude faster than theusual Gaussian process-based strategies and often outperforms them in accuracy.</description>
      <author>example@mail.com (Xinyu Zhang, Daolang Huang, Samuel Kaski, Julien Martinelli)</author>
      <guid isPermaLink="false">2503.00924v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>LLMs as Educational Analysts: Transforming Multimodal Data Traces into Actionable Reading Assessment Reports</title>
      <link>http://arxiv.org/abs/2503.02099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本研究探讨了利用多模态数据源（包括眼动追踪数据、学习成果、评估内容和教学标准）来获取有意义的阅读洞察的方法。我们采用无监督学习技术识别不同的阅读行为模式，并使用大型语言模型将所得信息合成成易于理解且富有操作性的报告供教师参考，从而简化解读过程。&lt;h4&gt;背景&lt;/h4&gt;目前许多教育科技应用程序主要侧重于结果导向型指标，而对学生的具体行为和认知洞察有限。这限制了对学生阅读能力的深入理解和提升。&lt;h4&gt;目的&lt;/h4&gt;研究目的是通过集成多模态数据源来改进学生阅读评估的方法，并探究如何利用人工智能技术生成有价值的教师报告。&lt;h4&gt;方法&lt;/h4&gt;本研究运用无监督机器学习算法识别不同的阅读模式，然后使用大型语言模型将这些结果整理成对教育工作者有帮助的、具体的行动建议。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，大型语言模型能够有效地充当教育分析员的角色，将多样化的数据转化为教师友好的见解，并且这种由人工智能生成的报告在清晰度、准确性、相关性和教学实用性方面得到了专家和教育者的认可。&lt;h4&gt;结论&lt;/h4&gt;虽然自动化洞察力生成展现了巨大潜力，但为了确保可靠性和公平性，仍需人类监督。这项研究促进了以人为本的人工智能技术在教育领域的应用，将数据驱动的分析与实际课堂实践紧密结合在一起。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的研究调查了利用多模态数据源获取有意义阅读洞察的方法，并通过无监督学习技术和大型语言模型来生成教师友好的报告。结果表明该方法有效且得到认可，同时强调了人类监督的重要性以及其在教育领域中的应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reading assessments are essential for enhancing students' comprehension, yetmany EdTech applications focus mainly on outcome-based metrics, providinglimited insights into student behavior and cognition. This study investigatesthe use of multimodal data sources -- including eye-tracking data, learningoutcomes, assessment content, and teaching standards -- to derive meaningfulreading insights. We employ unsupervised learning techniques to identifydistinct reading behavior patterns, and then a large language model (LLM)synthesizes the derived information into actionable reports for educators,streamlining the interpretation process. LLM experts and human educatorsevaluate these reports for clarity, accuracy, relevance, and pedagogicalusefulness. Our findings indicate that LLMs can effectively function aseducational analysts, turning diverse data into teacher-friendly insights thatare well-received by educators. While promising for automating insightgeneration, human oversight remains crucial to ensure reliability and fairness.This research advances human-centered AI in education, connecting data-drivenanalytics with practical classroom applications.</description>
      <author>example@mail.com (Eduardo Davalos, Yike Zhang, Namrata Srivastava, Jorge Alberto Salas, Sara McFadden, Sun-Joo Cho, Gautam Biswas, Amanda Goodwin)</author>
      <guid isPermaLink="false">2503.02099v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Deep Learning for Subtype Classification in Breast Cancer Using Histopathological Images and Gene Expression Data</title>
      <link>http://arxiv.org/abs/2503.02849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个深度多模态学习框架，结合了乳腺癌的组织病理图像和基因表达数据，以分类BRCA.Luminal和BRCA.Basal / Her2亚型。&lt;h4&gt;背景&lt;/h4&gt;传统的乳腺癌分子分型方法主要依赖于单一的组织病理学或基因表达分析，预测能力有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度学习框架来提高乳腺癌亚型分类的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;采用ResNet-50模型提取图像特征，并使用全连接层处理基因表达数据。引入跨注意力融合机制以增强模态交互，使用五折交叉验证进行实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的多模态整合框架在分类准确率、精确召回AUC和F1分数方面优于单一模式的方法。&lt;h4&gt;结论&lt;/h4&gt;研究强调了深度学习方法在乳腺癌亚型分类中的潜力，并为临床决策提供了支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular subtyping of breast cancer is crucial for personalized treatmentand prognosis. Traditional classification approaches rely on eitherhistopathological images or gene expression profiling, limiting theirpredictive power. In this study, we propose a deep multimodal learningframework that integrates histopathological images and gene expression data toclassify breast cancer into BRCA.Luminal and BRCA.Basal / Her2 subtypes. Ourapproach employs a ResNet-50 model for image feature extraction and fullyconnected layers for gene expression processing, with a cross-attention fusionmechanism to enhance modality interaction. We conduct extensive experimentsusing five-fold cross-validation, demonstrating that our multimodal integrationoutperforms unimodal approaches in terms of classification accuracy,precision-recall AUC, and F1-score. Our findings highlight the potential ofdeep learning for robust and interpretable breast cancer subtypeclassification, paving the way for improved clinical decision-making.</description>
      <author>example@mail.com (Amin Honarmandi Shandiz)</author>
      <guid isPermaLink="false">2503.02849v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal AI predicts clinical outcomes of drug combinations from preclinical data</title>
      <link>http://arxiv.org/abs/2503.02781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MADRIGAL是一个多模态AI模型，能够从结构、通路、细胞活力和转录组数据中学习，预测药物组合效果，并在临床结果预测上优于单一模式方法和其他最先进的模型。&lt;h4&gt;背景&lt;/h4&gt;目前的模型依靠结构或靶点特征来识别高效且低毒性的药物组合，但这些方法未能整合多模态数据以进行准确、具有临床相关性的预测。&lt;h4&gt;目的&lt;/h4&gt;介绍MADRIGAL模型，用于更准确地预测药物组合效果以及潜在的安全性和毒性风险，并支持个性化癌症治疗和二型糖尿病及代谢紊乱相关的脂肪肝病的药物管理。&lt;h4&gt;方法&lt;/h4&gt;使用变压器瓶颈模块统一预临床药物数据模态，在训练和推理过程中处理缺失的数据。该模型利用多模态学习，包括结构、通路、细胞活力和转录组学数据进行药物组合效应预测。&lt;h4&gt;主要发现&lt;/h4&gt;MADRIGAL在预测不良药物相互作用方面优于单一模式方法和其他最先进的模型，并且能够识别转运蛋白介导的药物相互作用。它还支持虚拟筛选抗癌症药物组合以及个性化治疗方案的设计。&lt;h4&gt;结论&lt;/h4&gt;MADRIGAL提供了一种多模态的方法来设计结合疗法，具有改进的预测准确性和临床相关性。该模型还可以通过与大型语言模型集成，让用户以自然语言描述临床结果，从而改善安全评估并识别潜在风险。&lt;h4&gt;翻译&lt;/h4&gt;从预临床数据中预测临床效果对于识别安全有效的药物组合至关重要。现有方法依赖于结构或靶点特征来选择高效率低毒性的药物组合，但这些方法未能整合进行准确且具有临床相关性预测所需的多模态数据。MADRIGAL模型可以利用结构、路径、细胞活力和转录组学数据，预测21842种化合物（包括已批准的药物和在研新型化合物）以及953个临床效果组合效应。此模型通过使用变压器瓶颈模块来统一预临床药物模态，并解决了多模态学习中的缺失数据问题，在预测不良药物相互作用方面优于单一模式方法和其他最先进的模型。该模型支持虚拟筛选抗癌症药物组合，有助于二型糖尿病和代谢紊乱相关脂肪肝病的治疗管理并识别转运蛋白介导的药物相互作用。MADRIGAL还通过整合来自癌症患者的基因组配置文件来支持个性化癌症疗法，并且能够预测基于急性髓性白血病样本和个人来源的小鼠模型中个性化的药物组合疗效。与大型语言模型集成后，用户可以使用自然语言描述临床结果，从而提高安全性评估并识别潜在的风险。MADRIGAL为设计具有改进的预测准确性和临床相关性的结合疗法提供了多模态方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting clinical outcomes from preclinical data is essential foridentifying safe and effective drug combinations. Current models rely onstructural or target-based features to identify high-efficacy, low-toxicitydrug combinations. However, these approaches fail to incorporate the multimodaldata necessary for accurate, clinically-relevant predictions. Here, weintroduce MADRIGAL, a multimodal AI model that learns from structural, pathway,cell viability, and transcriptomic data to predict drug combination effectsacross 953 clinical outcomes and 21842 compounds, including combinations ofapproved drugs and novel compounds in development. MADRIGAL uses a transformerbottleneck module to unify preclinical drug data modalities while handlingmissing data during training and inference--a major challenge in multimodallearning. It outperforms single-modality methods and state-of-the-art models inpredicting adverse drug interactions. MADRIGAL performs virtual screening ofanticancer drug combinations and supports polypharmacy management for type IIdiabetes and metabolic dysfunction-associated steatohepatitis (MASH). Itidentifies transporter-mediated drug interactions. MADRIGAL predictsresmetirom, the first and only FDA-approved drug for MASH, among therapies withthe most favorable safety profile. It supports personalized cancer therapy byintegrating genomic profiles from cancer patients. Using primary acute myeloidleukemia samples and patient-derived xenograft models, it predicts the efficacyof personalized drug combinations. Integrating MADRIGAL with a large languagemodel allows users to describe clinical outcomes in natural language, improvingsafety assessment by identifying potential adverse interactions and toxicityrisks. MADRIGAL provides a multimodal approach for designing combinationtherapies with improved predictive accuracy and clinical relevance.</description>
      <author>example@mail.com (Yepeng Huang, Xiaorui Su, Varun Ullanat, Ivy Liang, Lindsay Clegg, Damilola Olabode, Nicholas Ho, Bino John, Megan Gibbs, Marinka Zitnik)</author>
      <guid isPermaLink="false">2503.02781v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>FASTer: Focal Token Acquiring-and-Scaling Transformer for Long-term 3D Object Detection</title>
      <link>http://arxiv.org/abs/2503.01899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10pages,6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近基于Lidar的顶级时间3D检测器越来越多地采用区域级范式，该范式首先生成粗略提案，然后编码和融合区域特征。&lt;h4&gt;背景&lt;/h4&gt;现有的方法在处理随输入帧数量增加而指数增长的复杂性时存在挑战，并且结果层面的任意连接限制了全局信息提取。&lt;h4&gt;目的&lt;/h4&gt;本文提出了Focal Token Acquiring-and-Scaling Transformer (FASTer)，它以自适应和轻量级的方式动态选择焦点令牌并浓缩令牌序列。&lt;h4&gt;方法&lt;/h4&gt;提出了一种简单但有效的自适应缩放机制，强调单个令牌的贡献，并在历史帧中仅存储和处理焦点点，从而大大减少了整体复杂性。此外，还提出了分组层次融合策略，逐步执行序列缩放和组内融合操作以促进全局空间和时间信息交换。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在Waymo Open Dataset上，FASTer在性能、效率方面显著优于其他最先进的检测器，并且展现出更高的灵活性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在处理大规模数据集上的3D目标检测任务时表现出色。&lt;h4&gt;翻译&lt;/h4&gt;最近表现优异的基于Lidar的时间三维探测器越来越多地采用了区域级方法。它们首先生成粗略提案，然后编码和融合区域特征。然而，无差别采样和融合通常忽略了个体点的不同贡献，并且随着输入帧数量的增长导致复杂性呈指数上升。此外，任意的结果级别串联限制了全局信息的提取。本文提出了一种Focal Token Acquiring-and-Scaling Transformer (FASTer)，该方法以自适应、轻量级的方式动态选择焦点令牌并浓缩令牌序列。强调单个令牌的贡献，我们提出了一个简单但有效的自适应缩放机制来捕获几何上下文并在筛选出焦点点的同时剔除非关键信息。在历史帧中仅存储和处理焦点点大大减少了整体复杂性。此外，还提出了一种新颖的分组层次融合策略，逐步执行序列缩放和组内融合操作以促进全局空间和时间信息交换。实验结果表明，在Waymo Open Dataset上，FASTer显著优于其他最先进的检测器，并且在性能、效率方面表现出色，同时还展现了更高的灵活性和鲁棒性。相关代码可在https://github.com/MSunDYY/FASTer.git获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/msundyy/faster&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent top-performing temporal 3D detectors based on Lidars have increasinglyadopted region-based paradigms. They first generate coarse proposals, followedby encoding and fusing regional features. However, indiscriminate sampling andfusion often overlook the varying contributions of individual points and leadto exponentially increased complexity as the number of input frames grows.Moreover, arbitrary result-level concatenation limits the global informationextraction. In this paper, we propose a Focal Token Acquring-and-ScalingTransformer (FASTer), which dynamically selects focal tokens and condensestoken sequences in an adaptive and lightweight manner. Emphasizing thecontribution of individual tokens, we propose a simple but effective AdaptiveScaling mechanism to capture geometric contexts while sifting out focal points.Adaptively storing and processing only focal points in historical framesdramatically reduces the overall complexity. Furthermore, a novel GroupedHierarchical Fusion strategy is proposed, progressively performing sequencescaling and Intra-Group Fusion operations to facilitate the exchange of globalspatial and temporal information. Experiments on the Waymo Open Datasetdemonstrate that our FASTer significantly outperforms other state-of-the-artdetectors in both performance and efficiency while also exhibiting improvedflexibility and robustness. The code is available athttps://github.com/MSunDYY/FASTer.git.</description>
      <author>example@mail.com (Chenxu Dang, Zaipeng Duan, Pei An, Xinmin Zhang, Xuzhong Hu, Jie Ma)</author>
      <guid isPermaLink="false">2503.01899v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Structural Entropy Guided Unsupervised Graph Out-Of-Distribution Detection</title>
      <link>http://arxiv.org/abs/2503.03241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2025 (The 39th Annual AAAI Conference on Artificial  Intelligence)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了SEGO框架，通过在图分类中集成结构熵到无监督OOD检测中来解决现有方法在处理冗余信息时的性能问题。&lt;h4&gt;背景&lt;/h4&gt;随着大量未标记数据的出现，无监督OOD检测对于确保图形神经网络(GNN)在测试期间可靠地识别ID和OOD样本至关重要。现有的方法由于图结构中的冗余信息而导致性能受损。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法在处理图结构中冗余信息时遇到的问题，并提高GNN模型的可靠性，提出了SEGO框架。&lt;h4&gt;方法&lt;/h4&gt;SEGO通过引入基于编码树形式的锚点视图来最小化结构熵，在对比学习架构下操作。该方案还采用三元组视角进行局部、全局和树级别的多粒度对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;SEGO能够有效地从图中去除冗余信息，同时保留重要的结构信息，使ID与OOD样本之间的区别更为明显。实验结果表明，在无监督OOD检测方面，该方法优于现有最佳基线模型。&lt;h4&gt;结论&lt;/h4&gt;在真实数据集上的广泛实验证明了SEGO的有效性，尤其是在9对10的数据集中表现最优，并且在FreeSolv/ToxCast数据集上超越了竞争对手10.8%。&lt;h4&gt;翻译&lt;/h4&gt;随着大量未标记数据的出现，无监督OOD检测对于确保图形神经网络(GNN)在测试期间可靠地识别ID和OOD样本至关重要。现有的方法由于图结构中的冗余信息而导致性能受损。为了应对这一挑战，我们提出了SEGO，一种集成结构熵进行图分类中无监督OOD检测的框架。该方法通过引入编码树形式的锚点视图来最小化结构熵，并采用了局部、全局和树级别多粒度对比学习方案。实验结果表明，在10对数据集中有9对表现最佳，平均性能提高3.7%，在FreeSolv/ToxCast数据集上优于对手10.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the emerging of huge amount of unlabeled data, unsupervisedout-of-distribution (OOD) detection is vital for ensuring the reliability ofgraph neural networks (GNNs) by identifying OOD samples from in-distribution(ID) ones during testing, where encountering novel or unknown data isinevitable. Existing methods often suffer from compromised performance due toredundant information in graph structures, which impairs their ability toeffectively differentiate between ID and OOD data. To address this challenge,we propose SEGO, an unsupervised framework that integrates structural entropyinto OOD detection regarding graph classification. Specifically, within thearchitecture of contrastive learning, SEGO introduces an anchor view in theform of coding tree by minimizing structural entropy. The obtained coding treeeffectively removes redundant information from graphs while preservingessential structural information, enabling the capture of distinct graphpatterns between ID and OOD samples. Furthermore, we present a multi-grainedcontrastive learning scheme at local, global, and tree levels using tripletviews, where coding trees with essential information serve as the anchor view.Extensive experiments on real-world datasets validate the effectiveness ofSEGO, demonstrating superior performance over state-of-the-art baselines in OODdetection. Specifically, our method achieves the best performance on 9 out of10 dataset pairs, with an average improvement of 3.7\% on OOD detectiondatasets, significantly surpassing the best competitor by 10.8\% on theFreeSolv/ToxCast dataset pair.</description>
      <author>example@mail.com (Yue Hou, He Zhu, Ruomei Liu, Yingke Su, Jinxiang Xia, Junran Wu, Ke Xu)</author>
      <guid isPermaLink="false">2503.03241v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Direct Sparse Odometry with Continuous 3D Gaussian Maps for Indoor Environments</title>
      <link>http://arxiv.org/abs/2503.03373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages,5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;在机器人和增强现实等领域中，精确的定位对于自主导航至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于单目视觉里程计框架的方法，该方法利用连续3D高斯地图来提高姿态估计的准确性，并降低成本。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一个使用连续3D高斯图的单目视觉里程计框架，该框架直接为所有提取出的高梯度点分配几何一致性的深度值，避免了传统的插值步骤。&lt;h4&gt;主要发现&lt;/h4&gt;通过在两个公开数据集上的评估显示，提出的框架相比现有方法具有更优的姿态跟踪精度。&lt;h4&gt;结论&lt;/h4&gt;研究团队已经开源了这项工作的源代码以促进社区发展。&lt;h4&gt;翻译&lt;/h4&gt;准确的定位对于机器人和增强现实应用（如自主导航）至关重要。基于视觉的方法结合先验地图旨在将LiDAR级别的准确性与相机的成本效率结合起来，实现稳健的姿态估计。然而，现有的方法在关联离散点云图和密集图像像素时常常依赖于不可靠的插值过程，这不可避免地引入了深度误差并降低了姿态估计精度。我们提出了一种单目视觉里程计框架，利用连续3D高斯地图，该框架直接为所有提取出的高梯度点分配几何一致性的深度值而无需插值步骤。在两个公开数据集上的评估表明，相比现有方法具有更优的姿态跟踪准确性。研究团队已经开源了这项工作的源代码以促进社区发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localization is essential for robotics and augmented realityapplications such as autonomous navigation. Vision-based methods combiningprior maps aim to integrate LiDAR-level accuracy with camera cost efficiencyfor robust pose estimation. Existing approaches, however, often depend onunreliable interpolation procedures when associating discrete point cloud mapswith dense image pixels, which inevitably introduces depth errors and degradespose estimation accuracy. We propose a monocular visual odometry frameworkutilizing a continuous 3D Gaussian map, which directly assigns geometricallyconsistent depth values to all extracted high-gradient points withoutinterpolation. Evaluations on two public datasets demonstrate superior trackingaccuracy compared to existing methods. We have released the source code of thiswork for the development of the community.</description>
      <author>example@mail.com (Jie Deng, Fengtian Lang, Zikang Yuan, Xin Yang)</author>
      <guid isPermaLink="false">2503.03373v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Developing a PET/CT Foundation Model for Cross-Modal Anatomical and Functional Imaging</title>
      <link>http://arxiv.org/abs/2503.02824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个用于多模态PET/CT成像的新型基础模型框架Cross-Fraternal Twin Masked Autoencoder (FratMAE)，该框架旨在解决现有AI驱动的PET/CT分析方法中泛化性和鲁棒性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;在肿瘤学领域，正电子发射断层扫描-计算机断层扫描(PET/CT)结合了CT提供的解剖细节和PET提供的功能性代谢活性及分子标志物表达信息，在癌症诊断、分期以及治疗监测方面被广泛应用。然而，现有的基于人工智能的PET/CT分析主要依赖于从头训练的任务特定模型或受限数据集，这限制了它们的泛化能力和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;提出一种专门设计用于多模态PET/CT成像的基础模型方法，以提高其在临床应用中的性能和可靠性。&lt;h4&gt;方法&lt;/h4&gt;引入了一种名为Cross-Fraternal Twin Masked Autoencoder (FratMAE)的新框架。该框架使用单独的视觉变换器(ViT)编码器处理PET和CT扫描，并通过交叉注意力解码器促进模式间的协同交互，同时结合文本元数据来增强PET表示的学习。&lt;h4&gt;主要发现&lt;/h4&gt;FratMAE在预训练过程中捕获了复杂的跨模态关系和全局摄取模式，在下游任务中表现出优越的性能。这表明该模型具有作为通用基础模型的潜力。&lt;h4&gt;结论&lt;/h4&gt;通过展示其在多模态PET/CT成像分析中的卓越能力，FratMAE提供了一种改进现有AI驱动癌症成像技术的新途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，此处已将其内容精简并翻译成了中文。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In oncology, Positron Emission Tomography-Computed Tomography (PET/CT) iswidely used in cancer diagnosis, staging, and treatment monitoring, as itcombines anatomical details from CT with functional metabolic activity andmolecular marker expression information from PET. However, existing artificialintelligence-driven PET/CT analyses rely predominantly on task-specific modelstrained from scratch or on limited datasets, limiting their generalizabilityand robustness. To address this, we propose a foundation model approachspecifically designed for multimodal PET/CT imaging. We introduce theCross-Fraternal Twin Masked Autoencoder (FratMAE), a novel framework thateffectively integrates whole-body anatomical and functional or molecularinformation. FratMAE employs separate Vision Transformer (ViT) encoders for PETand CT scans, along with cross-attention decoders that enable synergisticinteractions between modalities during masked autoencoder training.Additionally, it incorporates textual metadata to enhance PET representationlearning. By pre-training on PET/CT datasets, FratMAE captures intricatecross-modal relationships and global uptake patterns, achieving superiorperformance on downstream tasks and demonstrating its potential as ageneralizable foundation model.</description>
      <author>example@mail.com (Yujin Oh, Robert Seifert, Yihan Cao, Christoph Clement, Justin Ferdinandus, Constantin Lapa, Alessandro Liebich, Michelle Amon, Johanna Enke, Sifan Song, Runqi Meng, Fang Zeng, Ning Guo, Xiang Li, Pedram Heidari, Axel Rominger, Kuangyu Shi, Quanzheng Li)</author>
      <guid isPermaLink="false">2503.02824v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography from Chest Radiography via Tri-Modal Contrastive Learning</title>
      <link>http://arxiv.org/abs/2503.02162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了X2CT-CLIP框架，通过在潜在空间中设计的三模态对齐机制将3D CT体积和放射学报告的知识转移到CXR编码器上，实现了从CT到CXR的跨模式知识转移。&lt;h4&gt;背景&lt;/h4&gt;虽然CT是诊断中的重要影像方式，但由于辐射暴露高且处理时间长，限制了其在大规模筛查中的应用。相比之下，胸部X光（CXR）更易于获取和安全，但现有模型主要关注于识别在CXR上容易观察到的疾病，而对于使用CT训练多异常分类的需求尚未得到满足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从CT数据中转移知识至CXR上的方法，以改进基于CXR的疾病检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种三模态的知识迁移学习框架X2CT-CLIP，该框架通过将3D CT体积和放射学报告的信息转移到胸部X光图像上，实现了跨模式的知识传输。这种方法降低了模型训练时的计算负担，并且是首次尝试利用CXR实现多异常分类。&lt;h4&gt;主要发现&lt;/h4&gt;在三个标记数据集上的广泛评估显示，与现有基准方法相比，本研究的方法在跨模态检索、少样本适应和外部验证方面表现更佳。这表明，在资源有限的情况下，使用CT知识增强的胸部X光可以作为一种高效且可行的疾病检测替代方案。&lt;h4&gt;结论&lt;/h4&gt;X2CT-CLIP框架成功地将CT的知识转移到了CXR上，并展示了其在多异常分类中的潜力和应用价值，尤其是在资源受限环境中具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computed tomography (CT) is a key imaging modality for diagnosis, yet itsclinical utility is marred by high radiation exposure and long turnaroundtimes, restricting its use for larger-scale screening. Although chestradiography (CXR) is more accessible and safer, existing CXR foundation modelsfocus primarily on detecting diseases that are readily visible on the CXR.Recently, works have explored training disease classification models onsimulated CXRs, but they remain limited to recognizing a single disease typefrom CT. CT foundation models have also emerged with significantly improveddetection of pathologies in CT. However, the generalized application ofCT-derived labels on CXR has remained illusive. In this study, we proposeX2CT-CLIP, a tri-modal knowledge transfer learning framework that bridges themodality gap between CT and CXR while reducing the computational burden ofmodel training. Our approach is the first work to enable multi-abnormalityclassification in CT, using CXR, by transferring knowledge from 3D CT volumesand associated radiology reports to a CXR encoder via a carefully designedtri-modal alignment mechanism in latent space. Extensive evaluations on threemulti-label CT datasets demonstrate that our method outperformsstate-of-the-art baselines in cross-modal retrieval, few-shot adaptation, andexternal validation. These results highlight the potential of CXR, enrichedwith knowledge derived from CT, as a viable efficient alternative for diseasedetection in resource-limited settings.</description>
      <author>example@mail.com (Jianzhong You, Yuan Gao, Sangwook Kim, Chris Mcintosh)</author>
      <guid isPermaLink="false">2503.02162v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Variance-Aware Loss Scheduling for Multimodal Alignment in Low-Data Settings</title>
      <link>http://arxiv.org/abs/2503.03202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种基于模型预测统计变异性动态调整对比损失权重的方法，以提高图像文本对齐在低数据环境下的表现。&lt;h4&gt;背景&lt;/h4&gt;训练视觉语言模型用于图像和文本的对齐通常需要大规模的数据集才能达到良好的性能。而在小规模数据集中，标准的对比学习方法由于过拟合和不稳定的学习动力学而难以有效对齐模式。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的损失调度方法，以解决低数据条件下现有对比学习算法的问题，并改善图像文本检索精度。&lt;h4&gt;方法&lt;/h4&gt;采用变差感知损失调度方法，该方法根据模型在预测中的统计变异（不确定性）动态调整对比损失的权重。使用Flickr8k图说数据集的一个子集来模拟有限的数据条件。&lt;h4&gt;主要发现&lt;/h4&gt;与固定权重基线相比，所提出的方法提高了图像文本检索准确性；与其他自适应加权策略（利用输出熵和余弦相似度分布）相比较时，变差感知调度提供最佳的总体折中；在噪声注入标题和图片的压力测试下，该方法表现出更高的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这些结果强调了在低数据环境下进行多模态对齐时自适应损失加权的优势。&lt;h4&gt;翻译&lt;/h4&gt;训练视觉-语言模型以实现图像与文本的对齐通常需要大量的数据集来达到稳健的表现。在少量数据的情况下，标准对比学习方法由于过拟合和不稳定的训练动力学而难以有效对齐模式。在这篇论文中，我们提出了一种感知变异性的损失调度方法，该方法根据模型预测中的统计变异性（不确定性）动态调整对比性损失的权重。使用Flickr8k图像描述数据集的一个子集来模拟有限的数据情况，我们展示了与固定权重基线相比，我们的方法可以提高图像-文本检索的准确性。此外，我们将该方法与其他自适应加权策略进行了比较（输出熵和余弦相似度分布），发现基于变差感知调度的方法提供了最佳的整体折中。通过t-SNE可视化显示了该方法生成了更为显著的多模态嵌入。并且在噪声注入标题和图像的压力测试下，该方法表现出了更高的鲁棒性，保持较高的召回率即使在随机干扰引入的情况下也是如此。这些结果强调了自适应损失加权对于低数据环境中实现多模态对齐的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training vision-language models for image-text alignment typically requireslarge datasets to achieve robust performance. In low-data scenarios, standardcontrastive learning can struggle to align modalities effectively due tooverfitting and unstable training dynamics. In this paper, we propose avariance-aware loss scheduling approach that dynamically adjusts the weightingof the contrastive loss based on the statistical variability (uncertainty) inthe model's alignment predictions. Using a subset of the Flickr8k image-captiondataset to simulate limited data conditions, we demonstrate that our approachimproves image-text retrieval accuracy compared to a fixed-weight baseline. Wealso compare against other adaptive weighting strategies (using output entropyand cosine similarity spread) and find that variance-aware scheduling providesthe best overall trade-off. Qualitatively, our method yields more distinctmultimodal embeddings as shown by t-SNE visualizations. Moreover, in a stresstest with noise-injected captions and images, the variance-guided loss provesmore robust, maintaining higher recall when random perturbations areintroduced. These results highlight the benefit of adaptive loss weighting formultimodal alignment in low-data regimes.</description>
      <author>example@mail.com (Sneh Pillai)</author>
      <guid isPermaLink="false">2503.03202v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Leap: Inductive Link Prediction via Learnable TopologyAugmentation</title>
      <link>http://arxiv.org/abs/2503.03331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  published in Machine Learning, Optimization, and Data Science,  Springer Nature Switzerland&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种基于可学习拓扑增强的归纳链接预测方法LEAP，针对传统图神经网络在处理新节点时表达能力有限的问题。&lt;h4&gt;背景&lt;/h4&gt;链接预测是许多图机器学习下游应用的关键任务。虽然图神经网络（GNN）被广泛用于链接预测，在已有的节点间预测缺失链接的情况下尤其有效，但在实际应用场景中需要考虑新增节点的情况，即归纳设置下进行链接预测的需求日益增加。&lt;h4&gt;目的&lt;/h4&gt;为解决现有方法在新节点下的表达能力和捕捉结构信号不足的问题，提出了一种新的可学习拓扑增强的链接预测模型LEAP，该模型能够同时建模来自图结构和节点特征的归纳偏差。&lt;h4&gt;方法&lt;/h4&gt;提出了基于拓朴增强的新链接预测算法LEAP。它不同于以往仅关注节点表示的学习方法，而是通过可学习的方式为新节点提供结构性上下文，提高了表达能力。&lt;h4&gt;主要发现&lt;/h4&gt;在七种真实世界的同构和异构图上的广泛实验表明，与现有最优（SOTA）模型相比，LEAP的性能显著提升。特别是在AUC和平均精度方面分别提升了22%和17%&lt;h4&gt;结论&lt;/h4&gt;通过引入可学习拓朴增强技术，LEAP在归纳链接预测中表现出卓越的能力，为解决新节点加入图结构的问题提供了一种新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;连接预测是许多图形机器学习下游应用中的关键任务。为此，图神经网络（GNN）被广泛用于连接预测，在推导设置下尤其有效，即在已知节点之间预测缺失的链接。然而，许多现实应用场景需要归纳设置来处理新节点加入现有图的情况。因此，最近归纳连接预测吸引了大量的关注，并且多层感知机（MLP）是大多数研究中学习节点表示的流行选择。但是这些方法的表达能力有限，未能完全捕捉图形的结构信号。为此，在这项工作中，我们提出了基于可学习拓扑增强的LEAP（LEArnable toPology augmentation），这是一种归纳链接预测的方法。与以往方法不同的是，LEAP从图的结构和节点特征两方面建模归纳偏差，因此更具表现力。据我们所知，这是首次尝试通过可学习增强为新节点提供结构上下文的方法。广泛的实验表明，在七种真实世界的同构和异构图上，LEAP显著优于现有最优方法（SOTA）。改进幅度分别达到了22%的AUC和17%的平均精度。代码和数据集可在GitHub上获得 (https://github.com/AhmedESamy/LEAP/)。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-82481-4_31&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction is a crucial task in many downstream applications of graphmachine learning. To this end, Graph Neural Network (GNN) is a widely usedtechnique for link prediction, mainly in transductive settings, where the goalis to predict missing links between existing nodes. However, many real-lifeapplications require an inductive setting that accommodates for new nodes,coming into an existing graph. Thus, recently inductive link prediction hasattracted considerable attention, and a multi-layer perceptron (MLP) is thepopular choice of most studies to learn node representations. However, theseapproaches have limited expressivity and do not fully capture the graph'sstructural signal. Therefore, in this work we propose LEAP, an inductive linkprediction method based on LEArnable toPology augmentation. Unlike previousmethods, LEAP models the inductive bias from both the structure and nodefeatures, and hence is more expressive. To the best of our knowledge, this isthe first attempt to provide structural contexts for new nodes via learnableaugmentation in inductive settings. Extensive experiments on seven real-worldhomogeneous and heterogeneous graphs demonstrates that LEAP significantlysurpasses SOTA methods. The improvements are up to 22\% and 17\% in terms ofAUC and average precision, respectively. The code and datasets are available onGitHub (https://github.com/AhmedESamy/LEAP/)</description>
      <author>example@mail.com (Ahmed E. Samy, Zekarias T. Kefato, Sarunas Girdzijauskas)</author>
      <guid isPermaLink="false">2503.03331v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Do GFlowNets Transfer? Case Study on the Game of 24/42</title>
      <link>http://arxiv.org/abs/2503.01819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了GFlowNets在生成多样化解决方案方面的性能，并探讨其与自回归语言模型的差异。通过案例分析，在不同游戏任务中的表现发现GFlowNets难以保持解题多样性和准确性。&lt;h4&gt;背景&lt;/h4&gt;生成多样化答案对于类似人类的推理至关重要，但现有的自回归语言模型倾向于专注于提供单一准确的答案，从而限制了创造性。&lt;h4&gt;目的&lt;/h4&gt;评估GFlowNets在零样本跨任务泛化能力上的局限性，并探讨未来研究方向以提高其迁移学习能力。&lt;h4&gt;方法&lt;/h4&gt;通过微调小型和中型的大型语言模型于“24游戏”并测试它们在“42游戏”数据集的表现来分析GFlowNets的行为模式。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，GFlowNets难以维持跨任务时解题的多样性和准确性。&lt;h4&gt;结论&lt;/h4&gt;该研究揭示了GFlowNets在跨任务泛化能力上的关键限制，并强调需要进一步的研究以改进其迁移学习的能力。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要全文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating diverse solutions is key to human-like reasoning, yetautoregressive language models focus on single accurate responses, limitingcreativity. GFlowNets optimize solution generation as a flow network, promisinggreater diversity. Our case study shows their limited zero-shot transferabilityby fine-tuning small and medium-sized large language models on the Game of 24and testing them on the Game of 42 datasets. Results revealed that GFlowNetsstruggle to maintain solution diversity and accuracy, highlighting keylimitations in their cross-task generalization and the need for future researchin improved transfer learning capabilities.</description>
      <author>example@mail.com (Adesh Gupta, Abhinav Kumar, Mansi Gupta, Paras Chopra)</author>
      <guid isPermaLink="false">2503.01819v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Model-Agnostic Meta-Policy Optimization via Zeroth-Order Estimation: A Linear Quadratic Regulator Perspective</title>
      <link>http://arxiv.org/abs/2503.00385v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了使用元学习来处理ergodic线性二次调节器中的不确定性和异质性的方法，提出了一种结合零阶优化技术和典型元学习方法的算法。&lt;h4&gt;背景&lt;/h4&gt;近年来，元学习作为机器学习的一个有前景的话题被提出，并在图像分类、机器人技术、计算机游戏和控制系统等领域有着重要的应用。&lt;h4&gt;目的&lt;/h4&gt;研究如何使用元学习来解决ergodic线性二次调节器中的不确定性和异质性问题。&lt;h4&gt;方法&lt;/h4&gt;将零阶优化技术与典型元学习方法相结合，提出了一个算法，该算法省略了策略Hessian的估计，并适用于学习一组相似但异质性的线性动态系统任务。所提出的元目标函数继承了一组可元学习的线性动力系统的原始成本函数的重要特性。&lt;h4&gt;主要发现&lt;/h4&gt;提供了关于精确梯度下降过程的稳定性以及收敛保证，通过分析元目标函数的梯度有界性和局部平滑性来证明该算法的有效性，并给出了理论保证的样本复杂度条件。最后提供了一个数值例子以支持这一观点。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法能够处理异质性的线性动态系统集合，同时在优化过程中不需要投影到可行集上，并且通过理论分析提供了稳定性、收敛性和小梯度估计误差保证。&lt;h4&gt;翻译&lt;/h4&gt;元学习近年来被提出作为机器学习的有前景话题，在图像分类、机器人技术、计算机游戏和控制系统等领域有着广泛应用。本文探讨了使用元学习处理ergodic线性二次调节器中的不确定性和异质性的方法，提出了结合零阶优化技术和典型元学习方法的新算法，该算法可以应用于学习一组相似但异质性的线性动态系统，并提供关于稳定性、收敛性和小梯度估计误差的理论保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning has been proposed as a promising machine learning topic inrecent years, with important applications to image classification, robotics,computer games, and control systems. In this paper, we study the problem ofusing meta-learning to deal with uncertainty and heterogeneity in ergodiclinear quadratic regulators. We integrate the zeroth-order optimizationtechnique with a typical meta-learning method, proposing an algorithm thatomits the estimation of policy Hessian, which applies to tasks of learning aset of heterogeneous but similar linear dynamic systems. The inducedmeta-objective function inherits important properties of the original costfunction when the set of linear dynamic systems are meta-learnable, allowingthe algorithm to optimize over a learnable landscape without projection ontothe feasible set. We provide stability and convergence guarantees for the exactgradient descent process by analyzing the boundedness and local smoothness ofthe gradient for the meta-objective, which justify the proposed algorithm withgradient estimation error being small. We provide the sample complexityconditions for these theoretical guarantees, as well as a numerical example atthe end to corroborate this perspective.</description>
      <author>example@mail.com (Yunian Pan, Tao Li, Quanyan Zhu)</author>
      <guid isPermaLink="false">2503.00385v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MR-EIT: Multi-Resolution Reconstruction for Electrical Impedance Tomography via Data-Driven and Unsupervised Dual-Mode Neural Networks</title>
      <link>http://arxiv.org/abs/2503.00762v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于电气阻抗断层成像(EIT)的多分辨率重建方法MR-EIT，可以在监督学习和无监督学习模式下操作。&lt;h4&gt;背景&lt;/h4&gt;当前EIT图像重建面临着如何在有限的数据输入情况下实现高精度、低噪声影响的挑战。现有的方法往往难以同时满足不同分辨率数据的需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理不同规模输入序列并在低分辨率数据上生成高质量高分辨率EIT图像的方法。&lt;h4&gt;方法&lt;/h4&gt;MR-EIT结合了有序特征提取模块和无序坐标特征表达模块，前者通过预训练实现从电压到二维导电特性的映射；后者利用对称函数和局部特征提取机制实现了独立于输入序列顺序的多分辨率重建。在数据驱动模式下，该算法可以先进行两阶段的预训练，再进行联合训练以生成高分辨率图像。&lt;h4&gt;主要发现&lt;/h4&gt;MR-EIT不仅在模拟实验中表现出色，在无监督学习模式下的真实水箱实验中也展示了出色的鲁棒性和高效的超分辨率重建能力。特别是，在无监督学习模式下，该算法能显著减少迭代次数并提高成像质量。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，在结构相似性(SSIM)和相对图像误差(RIE)方面，MR-EIT优于其他比较方法，尤其是在无监督学习模式下表现突出。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，上述内容为其翻译及总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a multi-resolution reconstruction method for ElectricalImpedance Tomography (EIT), referred to as MR-EIT, which is capable ofoperating in both supervised and unsupervised learning modes. MR-EIT integratesan ordered feature extraction module and an unordered coordinate featureexpression module. The former achieves the mapping from voltage totwo-dimensional conductivity features through pre-training, while the latterrealizes multi-resolution reconstruction independent of the order and size ofthe input sequence by utilizing symmetric functions and local featureextraction mechanisms. In the data-driven mode, MR-EIT reconstructshigh-resolution images from low-resolution data of finite element meshesthrough two stages of pre-training and joint training, and demonstratesexcellent performance in simulation experiments. In the unsupervised learningmode, MR-EIT does not require pre-training data and performs iterativeoptimization solely based on measured voltages to rapidly achieve imagereconstruction from low to high resolution. It shows robustness to noise andefficient super-resolution reconstruction capabilities in both simulation andreal water tank experiments. Experimental results indicate that MR-EIToutperforms the comparison methods in terms of Structural Similarity (SSIM) andRelative Image Error (RIE), especially in the unsupervised learning mode, whereit can significantly reduce the number of iterations and improve imagereconstruction quality.</description>
      <author>example@mail.com (Fangming Shi, Jinzhen Liu, Xiangqian Meng, Yapeng Zhou, Hui Xiong)</author>
      <guid isPermaLink="false">2503.00762v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Hyperspectral Image Restoration and Super-resolution with Physics-Aware Deep Learning for Biomedical Applications</title>
      <link>http://arxiv.org/abs/2503.02908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于深度学习的方法，用于提高高光谱成像的空间分辨率和速度，同时保持生物样本的完整性。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像是一个强大的生物成像工具，能够揭示材料内在属性的新见解。然而，这种增强对比度是以系统复杂性为代价的，并且在空间分辨率、光谱分辨率和成像速度之间存在固有的权衡。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些限制，本文提出了一种不需要额外训练数据即可恢复和提高像素分辨率的方法。&lt;h4&gt;方法&lt;/h4&gt;该深度学习模型经过微调，可以进行16倍的像素超分辨率增强并提升12倍的成像速度。这种方法不依赖于先验知识，并且使用与成像模型对齐的度量标准进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;通过应用于合成和实验数据，证明了该模型能够保持生物样本的完整性，不会丢失或产生虚假特征。此外，在唐氏综合症患者中发现了以前无法检测到的代谢变化。&lt;h4&gt;结论&lt;/h4&gt;这项工作不仅提供了一个提高高光谱成像质量和速度的有效工具，还提供了对模型物理机制的理解，并为未来改进奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要：高光谱成像是一个强大的生物成像工具，能够揭示材料内在属性的新见解。然而，这种增强对比度是以系统复杂性为代价的，并且在空间分辨率、光谱分辨率和成像速度之间存在固有的权衡。为了克服这些限制，我们提出了一种基于深度学习的方法，在不使用先验知识的情况下进行像素恢复和增强。经过与成像模型对齐的标准微调，我们的物理感知方法实现了16倍的像素超分辨率提升和12倍的成像速度提升，并不需要额外的迁移学习训练数据。应用于五种不同类型样品的合成和实验数据，我们证明了该模型保持生物完整性，确保没有特征丢失或产生虚幻特征。我们也具体展示了该模型揭示唐氏综合症相关代谢变化的能力，这些变化在其他情况下无法检测到。此外，我们提供了关于模型内部工作的物理见解，为未来可能超越仪器限制的改进铺平道路，并且所有方法都作为开源软件发布在GitHub上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging is a powerful bioimaging tool which can uncover novelinsights, thanks to its sensitivity to the intrinsic properties of materials.However, this enhanced contrast comes at the cost of system complexity,constrained by an inherent trade-off between spatial resolution, spectralresolution, and imaging speed. To overcome this limitation, we present a deeplearning-based approach that restores and enhances pixel resolutionpost-acquisition without any a priori knowledge. Fine-tuned using metricsaligned with the imaging model, our physics-aware method achieves a 16X pixelsuper-resolution enhancement and a 12X imaging speedup without the need ofadditional training data for transfer learning. Applied to both synthetic andexperimental data from five different sample types, we demonstrate that themodel preserves biological integrity, ensuring no features are lost orhallucinated. We also concretely demonstrate the model's ability to revealdisease-associated metabolic changes in Downs syndrome that would otherwiseremain undetectable. Furthermore, we provide physical insights into the innerworkings of the model, paving the way for future refinements that couldpotentially surpass instrumental limits in an explainable manner. All methodsare available as open-source software on GitHub.</description>
      <author>example@mail.com (Yuchen Xiang, Zhaolu Liu, Monica Emili Garcia-Segura, Daniel Simon, Boxuan Cao, Vincen Wu, Kenneth Robinson, Yu Wang, Ronan Battle, Robert T. Murray, Xavier Altafaj, Luca Peruzzotti-Jametti, Zoltan Takats)</author>
      <guid isPermaLink="false">2503.02908v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Exploring the Potential of Large Language Models as Predictors in Dynamic Text-Attributed Graphs</title>
      <link>http://arxiv.org/abs/2503.03258v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;随着大语言模型（LLMs）的兴起，人们对于图基础模型（GFMs）在基于图的任务中的兴趣日益增加。通过利用LLMs作为预测器，GFMs展示了跨越各种任务和数据集的强大泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现有的关于使用LLMs作为预测器的研究主要集中在静态图上，动态图预测的潜力尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;这项工作首次尝试将LLMs应用于动态图上的预测任务，并识别出两个关键挑战：处理大规模历史数据时上下文长度限制和领域特征差异性。&lt;h4&gt;方法&lt;/h4&gt;为了应对这些挑战，我们提出了GraphAgent-Dynamic (GAD)框架，这是一个多代理系统，利用协同工作的大语言模型。与使用单个LLM作为预测器不同，GAD整合了全局和局部摘要代理来生成特定领域的知识，增强其跨域迁移能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，GAD的性能与完全监督的图神经网络相当甚至更优，并且不需要特定数据集的训练。此外，我们讨论了针对LLM基础预测器改进的潜在策略，例如对LLMs进行数据集特定的微调。&lt;h4&gt;结论&lt;/h4&gt;通过为不同任务制定定制化策略，该研究提供了对未来基于LLM预测器设计的新见解。&lt;h4&gt;翻译&lt;/h4&gt;随着大语言模型（LLMs）的发展，人们对图基础模型（GFMs）在基于图的任务中的兴趣日益增加。利用LLMs作为预测器的GFMs已经在各种任务和数据集上展示了出色的泛化能力。然而，现有研究主要关注静态图上的预测问题，而动态图预测方面尚未被充分探索。本文首次尝试使用LLM进行动态图上的预测，并提出GraphAgent-Dynamic (GAD)框架来应对挑战。实验结果表明，该框架的性能优于或等同于完全监督下的图神经网络，同时提供了对基于LLMs的未来设计的新见解和改进策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rise of large language models (LLMs), there has been growinginterest in Graph Foundation Models (GFMs) for graph-based tasks. By leveragingLLMs as predictors, GFMs have demonstrated impressive generalizability acrossvarious tasks and datasets. However, existing research on LLMs as predictorshas predominantly focused on static graphs, leaving their potential in dynamicgraph prediction unexplored. In this work, we pioneer using LLMs for predictivetasks on dynamic graphs. We identify two key challenges: the constraintsimposed by context length when processing large-scale historical data and thesignificant variability in domain characteristics, both of which complicate thedevelopment of a unified predictor. To address these challenges, we propose theGraphAgent-Dynamic (GAD) Framework, a multi-agent system that leveragescollaborative LLMs. In contrast to using a single LLM as the predictor, GADincorporates global and local summary agents to generate domain-specificknowledge, enhancing its transferability across domains. Additionally,knowledge reflection agents enable adaptive updates to GAD's knowledge,maintaining a unified and self-consistent architecture. In experiments, GADdemonstrates performance comparable to or even exceeds that of full-supervisedgraph neural networks without dataset-specific training. Finally, to enhancethe task-specific performance of LLM-based predictors, we discuss potentialimprovements, such as dataset-specific fine-tuning to LLMs. By developingtailored strategies for different tasks, we provide new insights for the futuredesign of LLM-based predictors.</description>
      <author>example@mail.com (Runlin Lei, Jiarui Ji, Haipeng Ding, Lu Yi, Zhewei Wei, Yongchao Liu, Chuntao Hong)</author>
      <guid isPermaLink="false">2503.03258v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Improving internal cluster quality evaluation in noisy Gaussian mixtures</title>
      <link>http://arxiv.org/abs/2503.00379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的特征重要性重标(FIR)方法，用于改进内部聚类验证，并通过实验展示了其在处理高维或噪声数据集时的有效性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;现有的内部聚类验证指标（如平均轮廓宽度、Calinski-Harabasz和Davies-Bouldin指数）容易受到特征相关性的干扰，导致在无标签的复杂数据集中评估结果不可靠。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来增强基于内部验证指标的聚类质量评价，在没有外部标准的情况下提高算法的有效性。&lt;h4&gt;方法&lt;/h4&gt;FIR方法通过根据每个特征的数据分散情况调整其贡献值，系统地降低噪声特征的影响，从而使得聚类结果更加清晰和紧凑。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，无论是在有噪声还是不相关特征存在的数据集中，FIR都能显著提高内部验证指标与真实标签之间的关联性，并且在重叠较大的情况下仍能保持较高性能稳定性。&lt;h4&gt;结论&lt;/h4&gt;FIR作为一种改进的聚类评价技术，在没有标注的数据上进行无监督学习时具有很大的应用价值和潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clustering is a fundamental technique in machine learning and data analysis,widely used across various domains. Internal clustering validation measures,such as the Average Silhouette Width, Calinski-Harabasz, and Davies-Bouldinindices, play a crucial role in assessing clustering quality when externalground truth labels are unavailable. However, these measures can be affected byfeature relevance, potentially leading to unreliable evaluations inhigh-dimensional or noisy data sets.  In this paper, we introduce a Feature Importance Rescaling (FIR) methoddesigned to enhance internal clustering validation by adjusting featurecontributions based on their dispersion. Our method systematically attenuatesnoise features making clustering compactness and separation clearer, and byconsequence aligning internal validation measures more closely with the groundtruth. Through extensive experiments on synthetic data sets under differentconfigurations, we demonstrate that FIR consistently improves the correlationbetween internal validation indices and the ground truth, particularly insettings with noisy or irrelevant features.  The results show that FIR increases the robustness of clustering evaluation,reduces variability in performance across different data sets, and remainseffective even when clusters exhibit significant overlap. These findingshighlight the potential of FIR as a valuable enhancement for internalclustering validation, making it a practical tool for unsupervised learningtasks where labelled data is not available.</description>
      <author>example@mail.com (Renato Cordeiro de Amorim, Vladimir Makarenkov)</author>
      <guid isPermaLink="false">2503.00379v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Abn-BLIP: Abnormality-aligned Bootstrapping Language-Image Pre-training for Pulmonary Embolism Diagnosis and Report Generation from CTPA</title>
      <link>http://arxiv.org/abs/2503.02034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了Abn-BLIP模型，该模型通过异常对齐技术提高了CTPA扫描的解读准确性和放射学报告的质量。&lt;h4&gt;背景&lt;/h4&gt;医学影像在现代医疗中扮演着重要角色，特别是用于诊断肺栓塞和胸部疾病的计算机断层摄影血管造影（CTPA）。然而，解析CTPA扫描并生成准确的放射学报告是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种改进的诊断模型，以提高对CTPA扫描异常检测、减少遗漏发现以及生成结构化报告的能力。&lt;h4&gt;方法&lt;/h4&gt;引入了Abn-BLIP（异常一致性自助学习语言-图像预训练）模型，该模型利用可学查询和跨模态注意机制来提升性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与现有技术相比，Abn-BLIP在准确性和临床相关性方面超越了最先进的医学视觉-语言模型和3D报告生成方法。&lt;h4&gt;结论&lt;/h4&gt;这些成果展示了多模式学习策略在改善放射学报告方面的潜力。模型源代码可从GitHub获取（https://github.com/zzs95/abn-blip）。&lt;h4&gt;翻译&lt;/h4&gt;医疗成像在现代医疗服务中发挥着关键作用，CTPA是诊断肺栓塞和其他胸部疾病的重要工具。然而，解读CTPA扫描并生成准确的放射学报告依然是一项挑战。为此，我们提出了一种名为Abn-BLIP（异常一致性自助学习语言-图像预训练）的高级诊断模型。该模型利用可学查询和跨模态注意机制来检测异常、减少遗漏发现，并生成结构化报告。实验表明，与现有方法相比，Abn-BLIP在准确性和临床相关性方面超过了最先进的医学视觉-语言模型和3D报告生成技术。这些结果强调了结合多模式学习策略以提高放射学报告能力的潜力。源代码可在GitHub上找到（https://github.com/zzs95/abn-blip）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical imaging plays a pivotal role in modern healthcare, with computedtomography pulmonary angiography (CTPA) being a critical tool for diagnosingpulmonary embolism and other thoracic conditions. However, the complexity ofinterpreting CTPA scans and generating accurate radiology reports remains asignificant challenge. This paper introduces Abn-BLIP (Abnormality-alignedBootstrapping Language-Image Pretraining), an advanced diagnosis model designedto align abnormal findings to generate the accuracy and comprehensiveness ofradiology reports. By leveraging learnable queries and cross-modal attentionmechanisms, our model demonstrates superior performance in detectingabnormalities, reducing missed findings, and generating structured reportscompared to existing methods. Our experiments show that Abn-BLIP outperformsstate-of-the-art medical vision-language models and 3D report generationmethods in both accuracy and clinical relevance. These results highlight thepotential of integrating multimodal learning strategies for improving radiologyreporting. The source code is available at https://github.com/zzs95/abn-blip.</description>
      <author>example@mail.com (Zhusi Zhong, Yuli Wang, Lulu Bi, Zhuoqi Ma, Sun Ho Ahn, Christopher J. Mullin, Colin F. Greineder, Michael K. Atalay, Scott Collins, Grayson L. Baird, Cheng Ting Lin, Webster Stayman, Todd M. Kolb, Ihab Kamel, Harrison X. Bai, Zhicheng Jiao)</author>
      <guid isPermaLink="false">2503.02034v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Diagnosis of Patients with Viral, Bacterial, and Non-Pneumonia Based on Chest X-Ray Images Using Convolutional Neural Networks</title>
      <link>http://arxiv.org/abs/2503.02906v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于胸部X光图像的肺炎分类决策支持系统，利用迁移学习技术结合卷积神经网络模型，并引入了特征选择和降维方法，实现了对无肺炎患者、病毒性肺炎患者以及细菌性肺炎患者的准确区分。&lt;h4&gt;背景&lt;/h4&gt;世界卫生组织指出，肺炎是每年导致大量死亡的原因之一。因此需要开发有效的辅助决策系统以提高诊断准确性。&lt;h4&gt;目的&lt;/h4&gt;通过使用预训练的卷积神经网络模型进行迁移学习，并结合特征选择和降维技术以及支持向量机分类器，构建一个能够准确区分无肺炎患者与病毒性或细菌性肺炎患者的决策支持系统。&lt;h4&gt;方法&lt;/h4&gt;实验采用了迁移学习（TL）技术利用预先训练好的卷积神经网络（CNN）模型处理胸部X光图像，并结合Relief和Chi-square降维技术以及支持向量机（SVM）进行分类。&lt;h4&gt;主要发现&lt;/h4&gt;对于区分无肺炎患者与任何类型的肺炎患者，系统的准确率为91.02%，精度为97.73%，召回率为98.03%，F1得分为97.88%。对于区分病毒性肺炎和细菌性肺炎患者的分类任务中，系统达到了93.66%的准确率、94.26%的精度、92.66%的召回率及93.45%的F1得分。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，所提出的基于迁移学习和特征选择方法的支持向量机分类器在区分无肺炎与病毒性或细菌性肺炎患者的任务中表现出优异性能。该系统可以为临床医生提供有价值的辅助决策信息。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文的中文译文：根据世界卫生组织的数据，每年有大量的死亡是由肺炎引起的疾病导致的。针对这一问题，提出了一种基于胸部X光图像进行患者分类的支持决策系统，能够将没有肺炎、病毒性肺炎和细菌性肺炎的患者区分开来。通过使用预训练的卷积神经网络模型实施迁移学习，并结合Relief与Chi-square方法作为降维技术以及支持向量机进行分类实现了这一目标。一系列实验的结果表明建立了一个区分无肺炎和肺炎患者的模型，其准确性为91.02%，精确度为97.73%，召回率为98.03%，F1得分为97.88%。此外，在区分病毒性肺炎与细菌性肺炎患者方面的准确率达到了93.66%，精度为94.26%，召回率为92.66%，F1得分为93.45%&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; According to the World Health Organization (WHO), pneumonia is a disease thatcauses a significant number of deaths each year. In response to this issue, thedevelopment of a decision support system for the classification of patientsinto those without pneumonia and those with viral or bacterial pneumonia isproposed. This is achieved by implementing transfer learning (TL) usingpre-trained convolutional neural network (CNN) models on chest x-ray (CXR)images. The system is further enhanced by integrating Relief and Chi-squaremethods as dimensionality reduction techniques, along with support vectormachines (SVM) for classification. The performance of a series of experimentswas evaluated to build a model capable of distinguishing between patientswithout pneumonia and those with viral or bacterial pneumonia. The obtainedresults include an accuracy of 91.02%, precision of 97.73%, recall of 98.03%,and an F1 Score of 97.88% for discriminating between patients without pneumoniaand those with pneumonia. In addition, accuracy of 93.66%, precision of 94.26%,recall of 92.66%, and an F1 Score of 93.45% were achieved for discriminatingbetween patients with viral pneumonia and those with bacterial pneumonia.</description>
      <author>example@mail.com (Carlos Arizmendi, Jorge Pinto, Alejandro Arboleda, Hernando González)</author>
      <guid isPermaLink="false">2503.02906v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Common indicators hurt armed conflict prediction</title>
      <link>http://arxiv.org/abs/2503.00265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究使用非洲地区的精细冲突数据，结合气候、地理、基础设施、经济、人口统计数据和人口结构等因素，利用无监督学习模型发现了三种主要的冲突类型。&lt;h4&gt;背景&lt;/h4&gt;现有研究未能充分探究大规模冲突与中小型冲突之间的区别及其形成因素。本研究试图通过分析详细的数据集来填补这一知识空白。&lt;h4&gt;目的&lt;/h4&gt;探索并分类不同规模的冲突，并探讨这些分类对预测冲突强度和持续时间的影响，同时评估常见指标在预测中的实用性。&lt;h4&gt;方法&lt;/h4&gt;利用无监督学习模型对非洲地区的精细冲突数据进行分析。该模型考虑了气候、地理、基础设施、经济和社会人口结构等多个维度的信息。&lt;h4&gt;主要发现&lt;/h4&gt;{'冲突类型': ['重大动乱', '局部冲突', '偶发和溢出事件'], '重大动乱特征': '在人口密集且拥有发达基础设施的平坦河岸地区传播', '局部冲突特征': '中等人口密度区域，经济与地理多样性高，通常局限于国境之内', '偶发性和溢出事件特征': '规模小，多发生在低人口密度、缺乏基础设施和经济条件差的地区'}&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，明确区分不同类型的冲突会降低对冲突强度和其他尺寸测量值（如死亡人数和持续时间）预测的准确性。此外，还开发了一种基于经验且自下而上的方法来识别冲突类型。&lt;h4&gt;翻译&lt;/h4&gt;大规模冲突与中小型冲突在哪些方面有所不同？为了解答这个问题，我们利用了非洲地区的精细冲突数据，并将其映射到气候、地理、基础设施、经济、人口统计数据和人口结构上。通过无监督学习模型，我们发现了三种主要的冲突类型，即“重大动乱”、“局部冲突”和“偶发性和溢出事件”。重大动乱多在人口密集且拥有发达基础设施的平坦河岸地区发生；局部冲突则发生在中等人口密度区域，经济与地理多样性高，并通常局限于国境之内。而偶发性及溢出事件规模较小，常见于低人口密度、缺乏基础设施和贫困地区的环境中。三种类型按顺序分层为因素，分别强调了人口、基础设施、经济状况和地理位置作为最具有区分性的指标。明确冲突的类型会降低对诸如死亡人数、冲突持续时间等冲突强度预测的准确性，并促使我们注意常用指标在预测中的有限效用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Are big conflicts different from small or medium size conflicts? To answerthis question, we leverage fine-grained conflict data, which we map to climate,geography, infrastructure, economics, raw demographics, and demographiccomposition in Africa. With an unsupervised learning model, we find threeoverarching conflict types representing ``major unrest,'' ``local conflict,''and ``sporadic and spillover events.'' Major unrest predominantly propagatesaround densely populated areas with well-developed infrastructure and flat,riparian geography. Local conflicts are in regions of median populationdensity, are diverse socio-economically and geographically, and are oftenconfined within country borders. Finally, sporadic and spillover conflictsremain small, often in low population density areas, with little infrastructureand poor economic conditions. The three types stratify into a hierarchy offactors that highlights population, infrastructure, economics, and geography,respectively, as the most discriminative indicators. Specifying conflict typenegatively impacts the predictability of conflict intensity such as fatalities,conflict duration, and other measures of conflict size. The competitive effectis a general consequence of weak statistical dependence. Hence, we develop anempirical and bottom-up methodology to identify conflict types, knowledge ofwhich can hurt predictability and cautions us about the limited utility ofcommonly available indicators.</description>
      <author>example@mail.com (Niraj Kushwaha, Woi Sok Oh, Shlok Shah, Edward D. Lee)</author>
      <guid isPermaLink="false">2503.00265v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DeepSuM: Deep Sufficient Modality Learning Framework</title>
      <link>http://arxiv.org/abs/2503.01728v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新的多模态选择框架，该框架独立学习每种模式的表示，并评估其在特定情境下的重要性。&lt;h4&gt;背景&lt;/h4&gt;多模态学习是开发稳健的学习模型的关键方法，应用于多媒体、机器人技术、大型语言模型和医疗保健等领域。不同模式的成本与资源需求不一，因此需要有效选择以平衡性能提升和资源消耗之间的关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来优化多模态集成和选择过程。&lt;h4&gt;方法&lt;/h4&gt;该研究采用了一种独立学习每种模式表示的方法，并在此基础上评估了各模式的重要性，同时开发出适应不同特征的编码器。&lt;h4&gt;主要发现&lt;/h4&gt;此框架能够提高多模态学习效率与效果。&lt;h4&gt;结论&lt;/h4&gt;通过独立学习和优化多模式表示可以有效提升整体系统性能并节约资源。&lt;h4&gt;翻译&lt;/h4&gt;多模态学习已成为发展稳健的学习模型的关键方法，其应用范围包括多媒体、机器人技术、大型语言模型以及医疗保健等领域。由于不同模式的成本与资源需求不一，因此高效利用各种模式成为了关键问题。这强调了有效选择模式的重要性，以平衡性能提升和资源消耗之间的关系。在这项研究中，我们提出了一种新的框架来优化多模态集成和选择过程，该方法独立学习每种模式的表示，并评估其在特定情境下的重要性，从而开发出适应不同特征的编码器，并促进具有独特特性的模式联合分析。我们的框架旨在通过优化模式整合与选择提高多模态学习效率与效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning has become a pivotal approach in developing robustlearning models with applications spanning multimedia, robotics, large languagemodels, and healthcare. The efficiency of multimodal systems is a criticalconcern, given the varying costs and resource demands of different modalities.This underscores the necessity for effective modality selection to balanceperformance gains against resource expenditures. In this study, we propose anovel framework for modality selection that independently learns therepresentation of each modality. This approach allows for the assessment ofeach modality's significance within its unique representation space, enablingthe development of tailored encoders and facilitating the joint analysis ofmodalities with distinct characteristics. Our framework aims to enhance theefficiency and effectiveness of multimodal learning by optimizing modalityintegration and selection.</description>
      <author>example@mail.com (Zhe Gao, Jian Huang, Ting Li, Xueqin Wang)</author>
      <guid isPermaLink="false">2503.01728v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Measurement noise scaling laws for cellular representation learning</title>
      <link>http://arxiv.org/abs/2503.02726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;深度学习扩展规律预测模型和数据集规模增大时性能如何提升。本文识别出数据中的测量噪声作为另一个影响性能的尺度，受独立的对数法则支配。&lt;h4&gt;背景&lt;/h4&gt;深度学习的扩展规则通常关注于模型大小及训练数据集的尺寸变化，而本文则引入了关于生物单细胞基因组数据中由于分子采样不足引起的主导性测量噪音来源的研究。&lt;h4&gt;目的&lt;/h4&gt;为了量化细胞表示模型质量，提出了一个信息论度量标准，并研究该度量与样本深度的关系。&lt;h4&gt;方法&lt;/h4&gt;通过多个模型类型和不同数据集验证了单一的定量关系。此外，还从简单的高斯噪声模型推导出了这种关系的形式。&lt;h4&gt;主要发现&lt;/h4&gt;测量噪音影响性能并遵循对数法则；提出了一个信息理论指标来衡量细胞表示模型的质量，并表明该指标随采样深度的变化而变化；发现了关于不同类型成像噪音的图像分类模型中存在相同的关系，暗示了测量噪声缩放可能是一个普遍的现象。&lt;h4&gt;结论&lt;/h4&gt;测量噪音可以作为生成和管理用于深度学习模型的数据的重要指南，特别是在数据质量在不同数据集间差异显著的领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning scaling laws predict how performance improves with increasedmodel and dataset size. Here we identify measurement noise in data as anotherperformance scaling axis, governed by a distinct logarithmic law. We focus onrepresentation learning models of biological single cell genomic data, where adominant source of measurement noise is due to molecular undersampling. Weintroduce an information-theoretic metric for cellular representation modelquality, and find that it scales with sampling depth. A single quantitativerelationship holds across several model types and across several datasets. Weshow that the analytical form of this relationship can be derived from a simpleGaussian noise model, which in turn provides an intuitive interpretation forthe scaling law. Finally, we show that the same relationship emerges in imageclassification models with respect to two types of imaging noise, suggestingthat measurement noise scaling may be a general phenomenon. Scaling with noisecan serve as a guide in generating and curating data for deep learning models,particularly in fields where measurement quality can vary dramatically betweendatasets.</description>
      <author>example@mail.com (Gokul Gowri, Peng Yin, Allon M. Klein)</author>
      <guid isPermaLink="false">2503.02726v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient Approach to Detecting Lung Nodules Using Swin Transformer</title>
      <link>http://arxiv.org/abs/2503.01592v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19th Iranian Conference on Intelligent Systems (ICIS), IEEE, 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的基于2D CT切片的肺癌结节检测模型，以提高早期诊断效率。&lt;h4&gt;背景&lt;/h4&gt;肺癌是癌症致死率最高的疾病之一，而肺结节是常见的肺癌指示物。现有的多种肺结节检测模型在效率方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种更为高效的肺结节检测方法，减少计算量和复杂性，并提升小结节的检测能力。&lt;h4&gt;方法&lt;/h4&gt;采用Swin Transformer（轻量版）结合特征金字塔网络，利用迁移学习加速训练过程。该模型旨在继承视觉变换器的优势同时保持较低的计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，提出的模型在小结节的mAP和mAR上分别超出当前最佳方法1.3%和1.6%，整体表现优异，分别为94.7%mAP和94.9%mAR。&lt;h4&gt;结论&lt;/h4&gt;所提方法能够有效地提高肺部结节检测效率与精度，有助于早期肺癌诊断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICIS64839.2024.10887472&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lung cancer has the highest rate of cancer-caused deaths, and early-stagediagnosis could increase the survival rate. Lung nodules are common indicatorsof lung cancer, making their detection crucial. Various lung nodule detectionmodels exist, but many lack efficiency. Hence, we propose a more efficientapproach by leveraging 2D CT slices, reducing computational load and complexityin training and inference. We employ the tiny version of Swin Transformer tobenefit from Vision Transformers (ViT) while maintaining low computationalcost. A Feature Pyramid Network is added to enhance detection, particularly forsmall nodules. Additionally, Transfer Learning is used to accelerate training.Our experimental results show that the proposed model outperformsstate-of-the-art methods, achieving higher mAP and mAR for small nodules by1.3% and 1.6%, respectively. Overall, our model achieves the highest mAP of94.7% and mAR of 94.9%.</description>
      <author>example@mail.com (Saeed Shakuri, Alireza Rezvanian)</author>
      <guid isPermaLink="false">2503.01592v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Shared Encoder Approach to Multimodal Representation Learning</title>
      <link>http://arxiv.org/abs/2503.01654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;多模态表示学习在处理和融合文本、图像等多样化数据方面展现出巨大潜力，特别是在医学领域可以得到显著应用。然而，缺乏配对的多模态数据以及依赖专有或预训练编码器的问题带来了挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态表示学习能够帮助模型更好地理解和处理不同类型的数据（如文本和图像），从而提高性能。在医学领域中，这种技术可以获得重大应用，但当前存在缺乏足够的多模态配对数据和过度依赖于专有或预训练编码器的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于医疗领域的共享编码器框架，以解决缺乏配对的多模态数据以及依赖专有或预训练编码器带来的挑战问题。&lt;h4&gt;方法&lt;/h4&gt;该研究采用了一种单一编码器参数集，在不同模式之间共享，并通过可学习的模态特征进行增强。这种方法能够适应不同的医疗领域应用场景。&lt;h4&gt;主要发现&lt;/h4&gt;实证结果显示，与独立的特定模态编码器相比，本研究提出的共享编码器框架在数据受限的情况下具有更好的泛化能力。尤其是当训练样本较少时，性能提升尤为显著。&lt;h4&gt;结论&lt;/h4&gt;该工作表明，在医学应用的实际场景中（特别是面对有限的数据条件），所提出的共享编码器框架比现有的特定模态编码器更加高效且具备更高的可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;多模态表示学习已经显示出了处理多样化数据模式的巨大潜力，如文本和图像，并能改善理解和性能。虽然医学领域可以从这种范式中获益良多，但缺乏配对的多模态数据以及依赖专有或预训练编码器的问题带来了显著挑战。本研究提出了一种适用于医疗领域的共享编码器框架，该方法采用单一集编码参数在不同模式间共用，并辅以可学习的模态特征增强。实验结果表明，在数据受限的情况下，本框架能够超越特定模态编码器，特别是在较少训练样本时性能提升尤为显著。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/vectorinstitute/shared_encoder&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal representation learning has demonstrated remarkable potential inenabling models to process and integrate diverse data modalities, such as textand images, for improved understanding and performance. While the medicaldomain can benefit significantly from this paradigm, the scarcity of pairedmultimodal data and reliance on proprietary or pretrained encoders posesignificant challenges. In this work, we present a shared encoder framework formultimodal representation learning tailored to the medical domain. Our approachemploys a single set of encoder parameters shared across modalities, augmentedwith learnable modality features. Empirical results demonstrate that our sharedencoder idea achieves superior performance compared to separatemodality-specific encoders, demonstrating improved generalization indata-constrained settings. Notably, the performance gains are more pronouncedwith fewer training examples, underscoring the efficiency of our shared encoderframework for real-world medical applications with limited data. Our code andexperiment setup are available athttps://github.com/VectorInstitute/shared_encoder.</description>
      <author>example@mail.com (Shuvendu Roy, Franklin Ogidi, Ali Etemad, Elham Dolatabadi, Arash Afkanpour)</author>
      <guid isPermaLink="false">2503.01654v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Distilled Prompt Learning for Incomplete Multimodal Survival Prediction</title>
      <link>http://arxiv.org/abs/2503.01653v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;多模态数据集（包括病理图像和基因组信息）在精准生存预测中的应用广泛。然而，尽管近年来多模态生存模型取得了进展，在临床环境中收集完整的多模态数据仍然是一项挑战。&lt;h4&gt;目的&lt;/h4&gt;为了应对因缺乏完整模态导致的局限性问题，本文提出了一种利用大型语言模型（LLMs）鲁棒性的蒸馏提示学习框架（DisPro），以弥补缺失模态的信息。&lt;h4&gt;方法&lt;/h4&gt;提出的框架包含两个阶段：首先通过单模态提示将各模态的知识分布提炼出来；接着在多模态提示下，使用现有数据作为提示来推断丢失的数据，并注入第一阶段获取的单一模式知识以补充特定模态信息。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验展示了该方法在处理各种缺失场景下的优越性。通过两个阶段的提示学习，能够更好地填补因缺少某些模态而导致的信息缺口。&lt;h4&gt;结论&lt;/h4&gt;这项研究提供了一种新的策略来改善多模态生存预测模型中的数据完整性问题，并为进一步的应用提供了可能的方向。&lt;h4&gt;翻译&lt;/h4&gt;将病理图像和基因组信息等多模态数据应用于精确的生存预测中非常普遍。尽管最近在多模态生存模型方面有所进展，但在临床环境中收集完整的多模式数据仍然是一项挑战。目前处理不完整模式的方法通常只弥补了缺失模式知识的一部分，并不足以完全补偿损失的知识。为了应对这一问题，我们提出了一种基于大型语言模型（LLMs）的蒸馏提示学习框架(DisPro)，该框架利用两个阶段的提示来补充丢失模态的全面信息。第一阶段是单模态提示，它提炼了各模式中的知识分布；第二阶段为多模态提示，使用现有数据作为提示来推断缺失的数据，并且在多模态推理中注入第一阶段获得的单一模态知识，以补偿特定模态的信息损失。广泛的实验验证显示所提出的框架在各种不完整场景下的优越性。相关代码已公开（见 https://github.com/Innse/DisPro）&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of multimodal data including pathology images and geneprofiles is widely applied in precise survival prediction. Despite recentadvances in multimodal survival models, collecting complete modalities formultimodal fusion still poses a significant challenge, hindering theirapplication in clinical settings. Current approaches tackling incompletemodalities often fall short, as they typically compensate for only a limitedpart of the knowledge of missing modalities. To address this issue, we proposea Distilled Prompt Learning framework (DisPro) to utilize the strong robustnessof Large Language Models (LLMs) to missing modalities, which employs two-stageprompting for compensation of comprehensive information for missing modalities.In the first stage, Unimodal Prompting (UniPro) distills the knowledgedistribution of each modality, preparing for supplementing modality-specificknowledge of the missing modality in the subsequent stage. In the second stage,Multimodal Prompting (MultiPro) leverages available modalities as prompts forLLMs to infer the missing modality, which provides modality-common information.Simultaneously, the unimodal knowledge acquired in the first stage is injectedinto multimodal inference to compensate for the modality-specific knowledge ofthe missing modality. Extensive experiments covering various missing scenariosdemonstrated the superiority of the proposed method. The code is available athttps://github.com/Innse/DisPro.</description>
      <author>example@mail.com (Yingxue Xu, Fengtao Zhou, Chenyu Zhao, Yihui Wang, Can Yang, Hao Chen)</author>
      <guid isPermaLink="false">2503.01653v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion Capture</title>
      <link>http://arxiv.org/abs/2503.03222v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为Mocap-2-to-3的框架，该框架旨在从单目视角恢复世界坐标系中的绝对姿势。通过将复杂的3D运动分解为2D姿态，并利用大规模的2D数据改进3D运动重建和预测人的绝对位置。&lt;h4&gt;背景&lt;/h4&gt;从单个视图中恢复世界坐标系下的绝对姿势面临两个主要问题：一是现有的方法需要依靠在有限环境中收集的3D运动数据进行训练，二是从单一视角估计一个人在度量空间中的绝对位置更为复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种新框架Mocap-2-to-3来解决上述挑战，并提高模型在不同场景下的泛化能力和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;引入了一个新的框架Mocap-2-to-3，该框架利用大量易于获取的2D姿态数据进行单视角扩散模型的预训练和多视图扩散模型的微调。此外，还提出了一种新颖的人体运动表示方式，将局部动作与全局运动分离，并编码地面几何先验。&lt;h4&gt;主要发现&lt;/h4&gt;通过在真实世界的数据集上对模型性能进行了评估，证明了该方法相比于现有最佳方法，在运动和绝对人体定位方面的准确性更高，且泛化性和可扩展性更强。&lt;h4&gt;结论&lt;/h4&gt;提出的Mocap-2-to-3框架展示了从单目视角恢复世界坐标系中绝对姿势的有效性，并有望在各种应用领域得到广泛使用。&lt;h4&gt;翻译&lt;/h4&gt;论文研究的是通过一种创新的框架利用大规模的2D数据来解决单目视图下恢复世界坐标系统中的绝对姿态所面临的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recovering absolute poses in the world coordinate system from monocular viewspresents significant challenges. Two primary issues arise in this context.Firstly, existing methods rely on 3D motion data for training, which requirescollection in limited environments. Acquiring such 3D labels for new actions ina timely manner is impractical, severely restricting the model's generalizationcapabilities. In contrast, 2D poses are far more accessible and easier toobtain. Secondly, estimating a person's absolute position in metric space froma single viewpoint is inherently more complex. To address these challenges, weintroduce Mocap-2-to-3, a novel framework that decomposes intricate 3D motionsinto 2D poses, leveraging 2D data to enhance 3D motion reconstruction indiverse scenarios and accurately predict absolute positions in the worldcoordinate system. We initially pretrain a single-view diffusion model withextensive 2D data, followed by fine-tuning a multi-view diffusion model forview consistency using publicly available 3D data. This strategy facilitatesthe effective use of large-scale 2D data. Additionally, we propose aninnovative human motion representation that decouples local actions from globalmovements and encodes geometric priors of the ground, ensuring the generativemodel learns accurate motion priors from 2D data. During inference, this allowsfor the gradual recovery of global movements, resulting in more plausiblepositioning. We evaluate our model's performance on real-world datasets,demonstrating superior accuracy in motion and absolute human positioningcompared to state-of-the-art methods, along with enhanced generalization andscalability. Our code will be made publicly available.</description>
      <author>example@mail.com (Zhumei Wang, Zechen Hu, Ruoxi Guo, Huaijin Pi, Ziyong Feng, Sida Peng, Xiaowei Zhou)</author>
      <guid isPermaLink="false">2503.03222v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Dementia Insights: A Context-Based MultiModal Approach</title>
      <link>http://arxiv.org/abs/2503.01226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于上下文的多模态方法，用于识别痴呆症，这种方法利用了最先进的大型预训练模型（LPM）来分析文本和音频数据。&lt;h4&gt;背景&lt;/h4&gt;痴呆是一种进行性神经退行性疾病，影响记忆、推理和日常功能。早期检测对于及时干预以减缓疾病进展至关重要。现有的研究通常依赖于专家标注的数据集，并且采用单一模态的方法，这限制了其鲁棒性和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于上下文的多模态方法来改进痴呆症的检测性能。&lt;h4&gt;方法&lt;/h4&gt;该研究整合了文本和音频数据的最佳表现大型预训练模型（LPM），如GPT、BERT和CLAP，并通过结合上下文嵌入进一步提高了检测效果。此外，还尝试了一种基于上下文的In-Context Learning (ICL)作为补充技术。&lt;h4&gt;主要发现&lt;/h4&gt;使用GPT生成的嵌入与CLAP音频特征融合时，在F1分数上达到83.33%，优于当前最先进的痴呆症检测模型；未经标注的原始文本数据表现优于专家标注的数据集，表明大型预训练模型可以在无需大量手动标记的情况下提取有意义的语言和声学模式。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了开发大规模、非侵入性诊断工具以减少对昂贵注释依赖的可能性，并且保持高精度。通过将多模态学习与上下文嵌入相结合，这项工作为未来痴呆症个性化检测及认知健康的研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于使用大型预训练模型进行基于文本和音频数据的痴呆症早期识别研究，提出了一种结合这两种模式并利用上下文信息的新方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dementia, a progressive neurodegenerative disorder, affects memory,reasoning, and daily functioning, creating challenges for individuals andhealthcare systems. Early detection is crucial for timely interventions thatmay slow disease progression. Large pre-trained models (LPMs) for text andaudio, such as Generative Pre-trained Transformer (GPT), Bidirectional EncoderRepresentations from Transformers (BERT), and Contrastive Language-AudioPretraining (CLAP), have shown promise in identifying cognitive impairments.However, existing studies generally rely heavily on expert-annotated datasetsand unimodal approaches, limiting robustness and scalability. This studyproposes a context-based multimodal method, integrating both text and audiodata using the best-performing LPMs in each modality. By incorporatingcontextual embeddings, our method improves dementia detection performance.Additionally, motivated by the effectiveness of contextual embeddings, wefurther experimented with a context-based In-Context Learning (ICL) as acomplementary technique. Results show that GPT-based embeddings, particularlywhen fused with CLAP audio features, achieve an F1-score of $83.33\%$,surpassing state-of-the-art dementia detection models. Furthermore, raw textdata outperforms expert-annotated datasets, demonstrating that LPMs can extractmeaningful linguistic and acoustic patterns without extensive manual labeling.These findings highlight the potential for scalable, non-invasive diagnostictools that reduce reliance on costly annotations while maintaining highaccuracy. By integrating multimodal learning with contextual embeddings, thiswork lays the foundation for future advancements in personalized dementiadetection and cognitive health research.</description>
      <author>example@mail.com (Sahar Sinene Mehdoui, Abdelhamid Bouzid, Daniel Sierra-Sosa, Adel Elmaghraby)</author>
      <guid isPermaLink="false">2503.01226v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Foundation Models for Environmental Science</title>
      <link>http://arxiv.org/abs/2503.03142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了基础模型在环境科学中的应用，涵盖了从数据生成到决策制定的多个方面。&lt;h4&gt;背景&lt;/h4&gt;对环境生态系统的建模对于资源管理、可持续发展和理解复杂的生态过程至关重要。然而，传统方法在处理这些系统内在复杂性、相互联系以及有限的数据时常常遇到困难。&lt;h4&gt;目的&lt;/h4&gt;本综述旨在提供基础模型应用于环境科学领域的全面概述，并强调其在前向预测、数据生成、数据同化、降尺度、模型集成和跨域决策制定等方面的进展。&lt;h4&gt;方法&lt;/h4&gt;该论文详细介绍了这些模型的发展过程，包括数据收集、架构设计、训练、调优和评估等环节。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型通过大规模预训练和通用表示能力，在整合多种数据源、捕捉时空依赖关系以及适应广泛任务方面提供了革命性的机会。&lt;h4&gt;结论&lt;/h4&gt;展示这些新兴方法的目的是促进跨学科合作，并推进环境科学中面向可持续发展的尖端机器学习技术的集成。&lt;h4&gt;翻译&lt;/h4&gt;建模环境生态系统对于有效的资源管理、可持续发展和理解复杂的生态过程至关重要。然而，传统方法在处理系统的固有复杂性、相互关联性和有限数据时常常面临挑战。基础模型通过大规模预训练和通用表示能力提供了一种变革性的机会，它能够整合多种数据源，捕捉时空依赖关系，并适应广泛的任务范围。这篇综述全面概述了基础模型在环境科学中的应用，突出了前向预测、数据生成、数据同化、降尺度、模型集成以及跨域决策制定等方面的进展。此外，论文还详细介绍了这些模型的发展过程，包括数据收集、架构设计、训练、调优和评估等环节。通过展示这些新兴方法，我们旨在促进跨学科合作，并推进环境科学中面向可持续发展的尖端机器学习技术的集成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling environmental ecosystems is essential for effective resourcemanagement, sustainable development, and understanding complex ecologicalprocesses. However, traditional methods frequently struggle with the inherentcomplexity, interconnectedness, and limited data of such systems. Foundationmodels, with their large-scale pre-training and universal representations,offer transformative opportunities by integrating diverse data sources,capturing spatiotemporal dependencies, and adapting to a broad range of tasks.This survey presents a comprehensive overview of foundation model applicationsin environmental science, highlighting advancements in forward prediction, datageneration, data assimilation, downscaling, model ensembling, anddecision-making across domains. We also detail the development process of thesemodels, covering data collection, architecture design, training, tuning, andevaluation. By showcasing these emerging methods, we aim to fosterinterdisciplinary collaboration and advance the integration of cutting-edgemachine learning for sustainable solutions in environmental science.</description>
      <author>example@mail.com (Runlong Yu, Shengyu Chen, Yiqun Xie, Xiaowei Jia)</author>
      <guid isPermaLink="false">2503.03142v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Transformer-Based Spatio-Temporal Association of Apple Fruitlets</title>
      <link>http://arxiv.org/abs/2503.03200v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于变压器的方法，用于在不同天数和相机姿态下采集的立体图像中时空关联苹果果蕾。&lt;h4&gt;背景&lt;/h4&gt;现有的农业领域最先进的关联方法专注于匹配较大的农作物，使用高分辨率点云或时间稳定的特征，但这些对于野外较小的水果来说难以获取。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于变压器架构的方法，以解决为较小的水果获取和匹配问题。&lt;h4&gt;方法&lt;/h4&gt;采用了一种基于变压器的架构来编码每个果蕾的形状和位置，并通过一系列交替使用自注意力和交叉注意力的变压器编码层传播和细化这些特征。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在商业苹果园采集的数据上实现了92.4%的F1分数，优于所有基线和消融研究结果。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法展示了其在时空关联小水果上的有效性和优越性。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种基于变压器的方法，在不同时间点从不同相机角度收集的立体图像中对苹果果蕾进行时空关联。农业领域的现有方法主要针对较大的农作物，通过高分辨率点云或长时间稳定的特征实现匹配，但对于较小的果实来说这些数据难以获得。为解决这些问题，提出了一种编码每个果蕾形状和位置、并用一系列交替使用的自注意力和交叉注意层传播和细化这些特性的变压器架构。实验表明，在商业苹果园的数据上，该方法实现了92.4%的F1分数，并优于所有基线和消融研究结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present a transformer-based method to spatio-temporallyassociate apple fruitlets in stereo-images collected on different days and fromdifferent camera poses. State-of-the-art association methods in agriculture arededicated towards matching larger crops using either high-resolution pointclouds or temporally stable features, which are both difficult to obtain forsmaller fruit in the field. To address these challenges, we propose atransformer-based architecture that encodes the shape and position of eachfruitlet, and propagates and refines these features through a series oftransformer encoder layers with alternating self and cross-attention. Wedemonstrate that our method is able to achieve an F1-score of 92.4% on datacollected in a commercial apple orchard and outperforms all baselines andablations.</description>
      <author>example@mail.com (Harry Freeman, George Kantor)</author>
      <guid isPermaLink="false">2503.03200v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>NodeReg: Mitigating the Imbalance and Distribution Shift Effects in Semi-Supervised Node Classification via Norm Consistency</title>
      <link>http://arxiv.org/abs/2503.03211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种名为NodeReg的正则化优化方法，用于确保节点表示的范数一致性，以减少不平衡邻居和噪声对图神经网络（GNN）性能的影响。&lt;h4&gt;背景&lt;/h4&gt;在半监督节点分类任务中，聚合来自邻近节点的信息虽然有助于提高GNN性能，但也使得节点容易受到其邻居节点影响。例如，在邻居节点不均衡或包含噪音的情况下，这会影响GNN的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;通过确保节点表示范数的一致性来减少不平衡和噪声对GNN的影响。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为NodeReg的方法，该方法是一种正则化的优化方式，用于强制执行节点表示范数的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;NodeReg能够显著提升在不均衡数据集和分布变化情况下的半监督节点分类性能。例如，在不平衡场景中，对于失衡比为0.1的GCN模型训练时，该方法能提高F1分数1.4%-25.9%；同样，在分布变化场景下，NodeReg提高了准确率1.4%-3.1%。&lt;h4&gt;结论&lt;/h4&gt;提出的方法简单但有效，并满足Lipschitz连续性条件，有助于稳定优化过程并在半监督节点分类任务中取得显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aggregating information from neighboring nodes benefits graph neural networks(GNNs) in semi-supervised node classification tasks. Nevertheless, thismechanism also renders nodes susceptible to the influence of their neighbors.For instance, this will occur when the neighboring nodes are imbalanced or theneighboring nodes contain noise, which can even affect the GNN's ability togeneralize out of distribution. We find that ensuring the consistency of thenorm for node representations can significantly reduce the impact of these twoissues on GNNs. To this end, we propose a regularized optimization methodcalled NodeReg that enforces the consistency of node representation norms. Thismethod is simple but effective and satisfies Lipschitz continuity, thusfacilitating stable optimization and significantly improving semi-supervisednode classification performance under the above two scenarios. To illustrate,in the imbalance scenario, when training a GCN with an imbalance ratio of 0.1,NodeReg outperforms the most competitive baselines by 1.4%-25.9% in F1 scoreacross five public datasets. Similarly, in the distribution shift scenario,NodeReg outperforms the most competitive baseline by 1.4%-3.1% in accuracy.</description>
      <author>example@mail.com (Shenzhi Yang, Jun Xia, Jingbo Zhou, Xingkai Yao, Xiaofang Zhang)</author>
      <guid isPermaLink="false">2503.03211v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation</title>
      <link>http://arxiv.org/abs/2503.01175v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025. See https://star-uu-wang.github.io/HOP/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一个名为HOP的新型多模态学习方法，用于生成协调的手势动作。&lt;h4&gt;背景&lt;/h4&gt;共话语手势是非言语线索，在人类交流中增强语音清晰度和表现力方面至关重要。现有的研究方法在提高手势准确性上取得了进展，但在生成多样化且连贯的动作上仍存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够捕捉手势运动、音频节奏和文本语义之间异构缠结的新多模态学习方法。&lt;h4&gt;方法&lt;/h4&gt;利用时空图建模实现音频与动作的对齐，并通过重新编程模块构建增强模态一致性的音频-文本语义表示。&lt;h4&gt;主要发现&lt;/h4&gt;HOP在生成自然、表达性强的手势方面取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了多模态模型如何有效地学习不同特征并代表它们的形式，从而实现更加协调和连贯的共话语手势。&lt;h4&gt;翻译&lt;/h4&gt;共话语手势是提升人类交流中语音清晰度与表现力的重要非语言线索。尽管现有的方法在提高手势准确性方面取得了一定进展，但生成多样化且连贯的手势动作仍具挑战性。该研究提出一种捕捉手势运动、音频节奏和文本语义之间异构缠结的多模态学习方法HOP，并通过时空图建模实现对齐。实验显示，HOP在自然与表达性强的手势生成方面取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Co-speech gestures are crucial non-verbal cues that enhance speech clarityand expressiveness in human communication, which have attracted increasingattention in multimodal research. While the existing methods have made stridesin gesture accuracy, challenges remain in generating diverse and coherentgestures, as most approaches assume independence among multimodal inputs andlack explicit modeling of their interactions. In this work, we propose a novelmultimodal learning method named HOP for co-speech gesture generation thatcaptures the heterogeneous entanglement between gesture motion, audio rhythm,and text semantics, enabling the generation of coordinated gestures. Byleveraging spatiotemporal graph modeling, we achieve the alignment of audio andaction. Moreover, to enhance modality coherence, we build the audio-textsemantic representation based on a reprogramming module, which is beneficialfor cross-modality adaptation. Our approach enables the trimodal system tolearn each other's features and represent them in the form of topologicalentanglement. Extensive experiments demonstrate that HOP achievesstate-of-the-art performance, offering more natural and expressive co-speechgesture generation. More information, codes, and demos are available here:https://star-uu-wang.github.io/HOP/</description>
      <author>example@mail.com (Hongye Cheng, Tianyu Wang, Guangsi Shi, Zexing Zhao, Yanwei Fu)</author>
      <guid isPermaLink="false">2503.01175v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Binary Classification Social Network Dataset for Graph Machine Learning</title>
      <link>http://arxiv.org/abs/2503.02397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Binary Classification Social Network Dataset (BiSND)，这是一个为图机器学习设计的数据集，用于预测二元分类。&lt;h4&gt;背景&lt;/h4&gt;目前可用的基准数据集包括引文、共现和电子商务网络等，但没有专门针对社会网络进行分类任务的基准数据集。&lt;h4&gt;目的&lt;/h4&gt;填补现有研究中的空白，提供一个适用于图机器学习应用的社会网络分类数据集。&lt;h4&gt;方法&lt;/h4&gt;BiSND以表格和图形格式呈现，并使用包括传统机器学习算法、深层神经网络、图神经网络以及最新的图对比学习方法在内的多样化分类器进行了验证。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，BiSND对于二元分类任务具有适用性，在F1分数上表现出从67.66到70.15的性能，这为未来的改进提供了可能性。&lt;h4&gt;结论&lt;/h4&gt;BiSND证明了其在传统和先进机器学习方法中的稳健性和有效性，并为未来的研究开辟了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social networks have a vast range of applications with graphs. The availablebenchmark datasets are citation, co-occurrence, e-commerce networks, etc, withclasses ranging from 3 to 15. However, there is no benchmark classificationsocial network dataset for graph machine learning. This paper fills the gap andpresents the Binary Classification Social Network Dataset (\textit{BiSND}),designed for graph machine learning applications to predict binary classes. Wepresent the BiSND in \textit{tabular and graph} formats to verify itsrobustness across classical and advanced machine learning. We employ a diverseset of classifiers, including four traditional machine learning algorithms(Decision Trees, K-Nearest Neighbour, Random Forest, XGBoost), one Deep NeuralNetwork (multi-layer perceptrons), one Graph Neural Network (GraphConvolutional Network), and three state-of-the-art Graph Contrastive Learningmethods (BGRL, GRACE, DAENS). Our findings reveal that BiSND is suitable forclassification tasks, with F1-scores ranging from 67.66 to 70.15, indicatingpromising avenues for future enhancements.</description>
      <author>example@mail.com (Adnan Ali, Jinglong Li, Huanhuan Chen, AlMotasem Bellah Al Ajlouni)</author>
      <guid isPermaLink="false">2503.02397v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Zero-Shot Learning Approach for Ephemeral Gully Detection from Remote Sensing using Vision Language Models</title>
      <link>http://arxiv.org/abs/2503.01169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了三种用于检测暂时性冲沟的自动化管道，并通过公开数据集进行了验证，这些管道基于视觉语言模型（VLM）实现了超过70%的准确率和接近80%的F1分数。&lt;h4&gt;背景&lt;/h4&gt;暂时性冲沟是土壤侵蚀的主要原因之一，现有的研究未能成功地实现从遥感图像中自动检测暂时性冲沟。&lt;h4&gt;目的&lt;/h4&gt;开发和评估用于检测暂时性冲沟的有效管道，并提供首个公开的数据集进行测试。&lt;h4&gt;方法&lt;/h4&gt;利用特定农业区域在一段时间内获取的遥感图像，通过多种视觉语言模型（VLM）对存在暂时性冲沟的图像进行分类。同时采用了零样本分类方法以及与迁移学习方法进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的管道能够有效地检测到缺乏充分标注数据集下的暂时性冲沟，准确率超过70%，F1分数接近80%。&lt;h4&gt;结论&lt;/h4&gt;该研究为自动化临时冲沟的识别提供了一种有效的方法，并通过实验验证了其在实际应用中的潜力。此外，开发了一个公开的数据集，以促进未来的研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了用于检测暂时性冲沟的三种管道，这些管道利用特定农业区域在一段时间内获取的遥感图像，并基于视觉语言模型（VLM）实现了超过70%的准确率和接近80%的F1分数。该研究开发并测试了首个公开的数据集，同时与迁移学习方法进行了比较，实验结果表明所提出的零样本分类管道在缺乏充分标注数据集的情况下能够有效检测暂时性冲沟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ephemeral gullies are a primary cause of soil erosion and their reliable,accurate, and early detection will facilitate significant improvements in thesustainability of global agricultural systems. In our view, prior research hasnot successfully addressed automated detection of ephemeral gullies fromremotely sensed images, so for the first time, we present and evaluate threesuccessful pipelines for ephemeral gully detection. Our pipelines utilizeremotely sensed images, acquired from specific agricultural areas over a periodof time. The pipelines were tested with various choices of Visual LanguageModels (VLMs), and they classified the images based on the presence ofephemeral gullies with accuracy higher than 70% and a F1-score close to 80% forpositive gully detection. Additionally, we developed the first public datasetfor ephemeral gully detection, labeled by a team of soil- and plant-scienceexperts. To evaluate the proposed pipelines, we employed a variety of zero-shotclassification methods based on State-of-the-Art (SOTA) open-sourceVision-Language Models (VLMs). In addition to that, we compare the samepipelines with a transfer learning approach. Extensive experiments wereconducted to validate the detection pipelines and to analyze the impact ofhyperparameter changes in their performance. The experimental resultsdemonstrate that the proposed zero-shot classification pipelines are highlyeffective in detecting ephemeral gullies in a scenario where classificationdatasets are scarce.</description>
      <author>example@mail.com (Seyed Mohamad Ali Tousi, Ramy Farag, Jacket Demby's, Gbenga Omotara, John A. Lory, G. N. DeSouza)</author>
      <guid isPermaLink="false">2503.01169v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering</title>
      <link>http://arxiv.org/abs/2503.03190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;3D问答（3D QA）要求模型全面理解由文本描述的所处三维场景，并在此基础上回答有关周围环境的问题。然而，现有方法通常依赖于纯粹基于点云的全局场景感知，忽视了多视角图像中丰富局部纹理细节的重要性。&lt;h4&gt;背景&lt;/h4&gt;现有的3D QA方法主要依靠纯3D点云进行全局场景感知，而忽略了从多视角图片获取的详细文本描述信息和复杂遮挡下相机姿态导致的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够综合集成多视角和点云特征的方法，以提高在3D问答中的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;{'Text-guided Multi-view Fusion (TGMF)模块': '优先考虑与文本语义内容紧密匹配的图像视图', 'Adaptive Dual-vision Perception (ADVP)模块': '设计用于自适应融合反投影多视角图像和点云特征，增强对3D场景的理解。', 'Multimodal Context-guided Reasoning (MCGR)模块': '通过整合视觉和语言模态的上下文信息来促进鲁棒推理'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在SQA3D和ScanQA数据集上的DSPNet优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;我们的方法能有效解决现有3D QA中存在的问题，提高场景理解和推理能力。&lt;h4&gt;翻译&lt;/h4&gt;论文提出了一种新的Dual-vision Scene Perception Network (DSPNet)，通过综合多视角图像与点云特征来改善3D QA任务中的鲁棒性。研究设计了三个关键模块：TGMF、ADVP和MCGR，以解决现有方法在文本理解、视图融合及上下文推理上的不足，并展示了其在两个基准数据集上优越的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/LZ-CH/DSPNet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Question Answering (3D QA) requires the model to comprehensivelyunderstand its situated 3D scene described by the text, then reason about itssurrounding environment and answer a question under that situation. However,existing methods usually rely on global scene perception from pure 3D pointclouds and overlook the importance of rich local texture details frommulti-view images. Moreover, due to the inherent noise in camera poses andcomplex occlusions, there exists significant feature degradation and reducedfeature robustness problems when aligning 3D point cloud with multi-viewimages. In this paper, we propose a Dual-vision Scene Perception Network(DSPNet), to comprehensively integrate multi-view and point cloud features toimprove robustness in 3D QA. Our Text-guided Multi-view Fusion (TGMF) moduleprioritizes image views that closely match the semantic content of the text. Toadaptively fuse back-projected multi-view images with point cloud features, wedesign the Adaptive Dual-vision Perception (ADVP) module, enhancing 3D scenecomprehension. Additionally, our Multimodal Context-guided Reasoning (MCGR)module facilitates robust reasoning by integrating contextual informationacross visual and linguistic modalities. Experimental results on SQA3D andScanQA datasets demonstrate the superiority of our DSPNet. Codes will beavailable at https://github.com/LZ-CH/DSPNet.</description>
      <author>example@mail.com (Jingzhou Luo, Yang Liu, Weixing Chen, Zhen Li, Yaowei Wang, Guanbin Li, Liang Lin)</author>
      <guid isPermaLink="false">2503.03190v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Simple Siamese Network for High-Resolution Video Quality Assessment</title>
      <link>http://arxiv.org/abs/2503.02330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICASSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种名为SiamVQA的新型Siamese网络，用于高分辨率视频质量评估。&lt;h4&gt;背景&lt;/h4&gt;在视频质量评估（VQA）的研究中，双分支网络已经作为一种有前途的解决方案出现。这种网络将技术视角和美学视角分开来测量低级失真和高级语义感知。&lt;h4&gt;目的&lt;/h4&gt;研究者认为仅从技术角度进行评估应该以语义感知的方式来进行，并假设现有的技术分支难以在高分辨率视频中察觉到这些高级语义信息，因此提出了一种新的网络模型SiamVQA。&lt;h4&gt;方法&lt;/h4&gt;SiamVQA采用共享权重的双分支架构来提升技术视角下对高层语义的理解能力。此外还引入了双重交叉注意力层用于融合技术和美学特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该模型在高分辨率基准数据集上达到了最先进的准确度，并且在低分辨率基准数据集上的表现也具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;SiamVQA是首个尝试通过语义感知方式从技术视角进行视频质量评估的网络架构，在处理高质量视频时展现出了巨大的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在这项研究中，关于视频质量评估（VQA），双分支网络已作为一种有前途的方法出现。它将VQA分解为技术和美学两个独立的技术层面来分别测量低级失真和高级语义感知。然而，我们提出技术视角本身应该以意识语义的方式进行衡量。假设现有的技术支路难以识别高分辨率视频中的高层含义，因为它是在从视频中抽样的局部小块上训练的。这些问题在低分辨率视频表现良好时可以被掩盖，但在高分辨率VQA中就变得非常关键了。这项工作介绍了SiamVQA，这是一种简单而有效的用于高分辨率VQA的Siamese网络。通过共享技术与美学支路之间的权重，SiamVQA提升了技术支路理解语义的能力，并促进了技术质量表示学习。此外，它还整合了一种双重交叉注意力层来融合技术和美学特征。SiamVQA在高分辨率基准测试中实现了最先进的精度，在较低分辨率的基准测试中的结果也非常具有竞争力。代码可在https://github.com/srcn-ivl/SiamVQA 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the research of video quality assessment (VQA), two-branch network hasemerged as a promising solution. It decouples VQA with separate technical andaesthetic branches to measure the perception of low-level distortions andhigh-level semantics respectively. However, we argue that while technical andaesthetic perspectives are complementary, the technical perspective itselfshould be measured in semantic-aware manner. We hypothesize that existingtechnical branch struggles to perceive the semantics of high-resolution videos,as it is trained on local mini-patches sampled from videos. This issue can behidden by apparently good results on low-resolution videos, but indeed becomescritical for high-resolution VQA. This work introduces SiamVQA, a simple buteffective Siamese network for highre-solution VQA. SiamVQA shares weightsbetween technical and aesthetic branches, enhancing the semantic perceptionability of technical branch to facilitate technical-quality representationlearning. Furthermore, it integrates a dual cross-attention layer for fusingtechnical and aesthetic features. SiamVQA achieves state-of-the-art accuracy onhigh-resolution benchmarks, and competitive results on lower-resolutionbenchmarks. Codes will be available at: https://github.com/srcn-ivl/SiamVQA</description>
      <author>example@mail.com (Guotao Shen, Ziheng Yan, Xin Jin, Longhai Wu, Jie Chen, Ilhyun Cho, Cheul-Hee Hahm)</author>
      <guid isPermaLink="false">2503.02330v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Directly Follows Graphs Go Predictive Process Monitoring With Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.03197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近年来，基于人工神经网络的预测过程监控技术（PPM）作为监测业务流程未来行为的方法不断发展。现有方法大多关注于将流程视为序列数据并将其输入到处理顺序数据的神经架构中，如循环神经网络或变压器。&lt;h4&gt;背景&lt;/h4&gt;现有的PPM技术主要通过使用递归神经网络(RNNs)或变压器等处理顺序数据的模型来预测业务流程的未来行为。但是这些方法在处理复杂、长时间且包含大量循环的流程时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;本研究探讨了一种替代方式来进行PPM：通过将每个过程转换为其直接后继图(DFG)表示，应用图神经网络(GNNs)进行预测任务。目的是开发更适合于复杂和长周期业务流程的模型。&lt;h4&gt;方法&lt;/h4&gt;我们介绍了不同创建DFG表示的方法，这些方法根据所使用的特定GNN而有所不同。测试了从传统基于节点的到新式的基于边的架构的各种GNN，并探讨了使用多图的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;通过上述步骤，我们的目标是设计出将轨迹转换为图形时信息损失最小化的图表示。&lt;h4&gt;结论&lt;/h4&gt;研究展示了通过利用GNN技术进行PPM的潜力和前景，特别是对于复杂且包含大量循环的业务流程。这种方法可能为复杂的流程预测任务提供更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在过去的几年中，基于人工神经网络的预测过程监控（PPM）技术作为一种监测商业流程未来行为的方法得到了发展。现有的方法主要集中在将流程视为序列数据，并将其输入到处理顺序数据的神经架构中，例如循环神经网络（RNNs）或变压器。在这项研究中，我们探讨了一种进行PPM的替代方式：通过将每个过程转换为其直接后继图(DFG)表示，可以应用图神经网络(GNNs)来执行预测任务。这样做的目的是开发更适合于复杂、长周期且包含大量循环的业务流程的模型。特别地，我们展示了根据所用特定GNN的不同创建DFG表示的方法。测试的GNN范围从传统的节点架构到新颖的边架构。此外，还探讨了使用多图的可能性。通过这些步骤，我们的目标是设计出将轨迹转换为图形时信息损失最小化的图表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the past years, predictive process monitoring (PPM) techniques based onartificial neural networks have evolved as a method to monitor the futurebehavior of business processes. Existing approaches mostly focus oninterpreting the processes as sequences, so-called traces, and feeding them toneural architectures designed to operate on sequential data such as recurrentneural networks (RNNs) or transformers. In this study, we investigate analternative way to perform PPM: by transforming each process in itsdirectly-follows-graph (DFG) representation we are able to apply graph neuralnetworks (GNNs) for the prediction tasks. By this, we aim to develop modelsthat are more suitable for complex processes that are long and contain anabundance of loops. In particular, we present different ways to create DFGrepresentations depending on the particular GNN we use. The tested GNNs rangefrom classical node-based to novel edge-based architectures. Further, weinvestigate the possibility of using multi-graphs. By these steps, we aim todesign graph representations that minimize the information loss whentransforming traces into graphs.</description>
      <author>example@mail.com (Attila Lischka, Simon Rauch, Oliver Stritzel)</author>
      <guid isPermaLink="false">2503.03197v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A General Neural Network Potential for Energetic Materials with C, H, N, and O elements</title>
      <link>http://arxiv.org/abs/2503.01932v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  41 pages,16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的神经网络势能（NNP）模型，用于预测C, H, N, O组成的高能量材料的结构、机械和分解特性。该模型通过转移学习利用预先训练好的NNP，并基于密度泛函理论计算得到的能量和力数据进行微调。&lt;h4&gt;背景&lt;/h4&gt;传统的高能量材料研究受限于高昂的计算成本和漫长的开发周期，阻碍了新材料的设计与优化。&lt;h4&gt;目的&lt;/h4&gt;本工作旨在发展一种适用于预测C, H, N, O组成的高能量材料特性的神经网络模型，并验证其有效性。&lt;h4&gt;方法&lt;/h4&gt;利用已预训练好的NNP模型，在转移学习框架下使用DFT计算得到的能量和力数据进行微调，应用于20种不同的高能系统。该模型通过分子动力学模拟进行测试，并与实验结果对比。&lt;h4&gt;主要发现&lt;/h4&gt;经过验证的神经网络模型能够准确描述C, H, N, O组成的高能量材料的关键原子相互作用及热分解机制，表现出DFT级别的精度和广泛的适用性，显著减少了计算和实验成本。&lt;h4&gt;结论&lt;/h4&gt;该工作提供了一种高效的策略来设计和发展高能量材料，并为结合密度泛函理论、机器学习和实验方法的材料研究提出了一个有前景的框架。NNP模型已开源在GitHub（https://github.com/MingjieWen/General-NNP-model-for-C-H-N-O-Energetic-Materials）。&lt;h4&gt;翻译&lt;/h4&gt;高能量材料的发现与优化受限于传统方法高昂的计算成本和漫长的开发周期，本工作旨在发展一种高效的神经网络势能模型以预测C, H, N, O组成的高能量材料。该模型基于预训练后的NNP通过转移学习进行微调，并且在分子动力学模拟中得到验证，展示了其优越的精度与广义性，在减少计算和实验成本的同时提高了设计效率，为结合多种方法推进材料研究提供了有效策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mingjiewen/general-nnp-model-for-c-h-n-o-energetic-materials&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The discovery and optimization of high-energy materials (HEMs) areconstrained by the prohibitive computational expense and prolonged developmentcycles inherent in conventional approaches. In this work, we develop a generalneural network potential (NNP) that efficiently predicts the structural,mechanical, and decomposition properties of HEMs composed of C, H, N, and O.Our framework leverages pre-trained NNP models, fine-tuned using transferlearning on energy and force data derived from density functional theory (DFT)calculations. This strategy enables rapid adaptation across 20 different HEMsystems while maintaining DFT-level accuracy, significantly reducingcomputational costs. A key aspect of this work is the ability of NNP model tocapture the chemical activity space of HEMs, accurately describe the key atomicinteractions and reaction mechanisms during thermal decomposition. The generalNNP model has been applied in molecular dynamics (MD) simulations and validatedwith experimental data for various HEM structures. Results show that the NNPmodel accurately predicts the structural, mechanical, and decompositionproperties of HEMs by effectively describing their chemical activity space.Compared to traditional force fields, it offers superior DFT-level accuracy andgeneralization across both microscopic and macroscopic properties, reducing thecomputational and experimental costs. This work provides an efficient strategyfor the design and development of HEMs and proposes a promising framework forintegrating DFT, machine learning, and experimental methods in materialsresearch. (To facilitate further research and practical applications, weopen-source our NNP model on GitHub:https://github.com/MingjieWen/General-NNP-model-for-C-H-N-O-Energetic-Materials.)</description>
      <author>example@mail.com (Mingjie Wen, Jiahe Han, Wenjuan Li, Xiaoya Chang, Qingzhao Chu, Dongping Chen)</author>
      <guid isPermaLink="false">2503.01932v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>LCV2I: Communication-Efficient and High-Performance Collaborative Perception Framework with Low-Resolution LiDAR</title>
      <link>http://arxiv.org/abs/2502.17039v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;车辆到基础设施（V2I）协作感知利用基础设施传感器收集的数据来增强车辆的感知能力。尽管激光雷达是一种常用的传感器，且在智能汽车和基础设施中广泛应用，但其优越性能伴随着较高的成本。为了实现低成本V2I，降低激光雷达的成本至关重要。&lt;h4&gt;背景&lt;/h4&gt;现有的V2I系统面临的主要挑战是如何在降低成本的同时保持高性能感知效果。由于高分辨率激光雷达价格昂贵，而低分辨率的激光雷达会导致远处的小物体变得更加模糊。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的协作感知框架LCV2I，在保证性能的前提下尽可能降低车辆上的传感器成本。&lt;h4&gt;方法&lt;/h4&gt;LCV2I利用摄像头和低成本低分辨率激光雷达的数据作为输入。通过特征偏移校正模块和区域特征增强算法提高特征表示能力，并采用区域性差异图和得分图来评估协作内容的价值，从而提升通信带宽效率。&lt;h4&gt;主要发现&lt;/h4&gt;LCV2I能够在保证高质量感知性能的同时大幅减少对高分辨率传感器的需求，并在现实世界的DAIR-V2X场景中进行3D目标检测时表现出色，超越现有算法的性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入低成本低分辨率激光雷达和改进通信效率的方法，LCV2I框架成功实现了高性能车辆到基础设施协作感知的目标。&lt;h4&gt;翻译&lt;/h4&gt;摘要：车辆到基础设施（V2I）协作感知利用基础设施传感器收集的数据来增强车辆感知能力。尽管作为常用传感器的LiDAR在智能汽车与基础设施中广泛应用，但其卓越性能伴随着高昂的成本问题。为了实现低成本V2I方案，降低LiDAR成本显得尤为重要。我们提出了一种新方法，即使用低分辨率LiDAR以尽可能地降低成本，并结合相机和激光雷达数据作为输入。通过特征偏移校正模块及区域特征增强算法来提升特征表示能力；同时采用区域性差异图与得分图评估合作内容的价值，从而提高通信带宽利用效率。在保证高质量感知性能的同时大幅减少对高分辨率传感器的需求，这是我们的研究目标所在。实验结果表明，在现实世界中，使用DAIR-V2X场景进行3D物体检测时，所提出的LCV2I框架的性能显著优于现有算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vehicle-to-Infrastructure (V2I) collaborative perception leverages datacollected by infrastructure's sensors to enhance vehicle perceptualcapabilities. LiDAR, as a commonly used sensor in cooperative perception, iswidely equipped in intelligent vehicles and infrastructure. However, itssuperior performance comes with a correspondingly high cost. To achievelow-cost V2I, reducing the cost of LiDAR is crucial. Therefore, we studyadopting low-resolution LiDAR on the vehicle to minimize cost as much aspossible. However, simply reducing the resolution of vehicle's LiDAR results insparse point clouds, making distant small objects even more blurred.Additionally, traditional communication methods have relatively low bandwidthutilization efficiency. These factors pose challenges for us. To balance costand perceptual accuracy, we propose a new collaborative perception framework,namely LCV2I. LCV2I uses data collected from cameras and low-resolution LiDARas input. It also employs feature offset correction modules and regionalfeature enhancement algorithms to improve feature representation. Finally, weuse regional difference map and regional score map to assess the value ofcollaboration content, thereby improving communication bandwidth efficiency. Insummary, our approach achieves high perceptual performance while substantiallyreducing the demand for high-resolution sensors on the vehicle. To evaluatethis algorithm, we conduct 3D object detection in the real-world scenario ofDAIR-V2X, demonstrating that the performance of LCV2I consistently surpassescurrently existing algorithms.</description>
      <author>example@mail.com (Xinxin Feng, Haoran Sun, Haifeng Zheng)</author>
      <guid isPermaLink="false">2502.17039v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Rapid morphology characterization of two-dimensional TMDs and lateral heterostructures based on deep learning</title>
      <link>http://arxiv.org/abs/2503.00470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于深度学习的方法，用于二维材料和异质结构的高效表征。通过使用YOLO模型，实现了对特定类型二维材料识别的高精度，并且探讨了跨不同材料应用迁移学习的效果。&lt;h4&gt;背景&lt;/h4&gt;二维材料及其异质结构具有独特的物理特性，需要高效的表征方法。&lt;h4&gt;目的&lt;/h4&gt;利用人工智能的进步提出了一种基于深度学习的方法来准确地表征2D材料和异质结构。&lt;h4&gt;方法&lt;/h4&gt;使用YOLO模型识别MoS2-MoSe2横向异质结和不同形状与厚度的MoS2薄片，同时探索了跨不同材料的迁移学习技术以提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;该模型达到了超过94.67%的精度，并展示了强大的泛化能力和抗干扰能力。开发的应用程序能够直接从光学显微镜图像中进行实时分析，显著提高了效率和降低了成本。&lt;h4&gt;结论&lt;/h4&gt;这种深度学习驱动的方法代表了一种快速准确表征2D材料的新工具，为材料科学的研究和发展开辟了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要：二维(2D)材料和异质结构表现出独特的物理特性，需要高效的表征方法。利用人工智能的进展，我们介绍了一种基于深度学习的方法来高效地表征异质结构和2D材料，特别是MoS2-MoSe2横向异质结和不同形状与厚度的MoS2薄片。通过使用YOLO模型，我们在这些材料的识别中实现了超过94.67%的准确率。此外，我们探讨了跨不同类型材料应用迁移学习的效果，这进一步提高了模型性能。该模型展示出强大的泛化能力和抗干扰能力，在各种场景下确保可靠的结果。为了便于实际使用，我们开发了一个应用程序，它可以直接从光学显微镜图像中进行实时分析，使其过程比传统方法更快且成本更低。这种深度学习驱动的方法为二维材料的快速准确表征提供了一种有前景的工具，并为材料科学的研究和发展开辟了新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Two-dimensional (2D) materials and heterostructures exhibit unique physicalproperties, necessitating efficient and accurate characterization methods.Leveraging advancements in artificial intelligence, we introduce a deeplearning-based method for efficiently characterizing heterostructures and 2Dmaterials, specifically MoS2-MoSe2 lateral heterostructures and MoS2 flakeswith varying shapes and thicknesses. By utilizing YOLO models, we achieve anaccuracy rate of over 94.67% in identifying these materials. Additionally, weexplore the application of transfer learning across different materials, whichfurther enhances model performance. This model exhibits robust generalizationand anti-interference ability, ensuring reliable results in diverse scenarios.To facilitate practical use, we have developed an application that enablesreal-time analysis directly from optical microscope images, making the processsignificantly faster and more cost-effective than traditional methods. Thisdeep learning-driven approach represents a promising tool for the rapid andaccurate characterization of 2D materials, opening new avenues for research anddevelopment in material science.</description>
      <author>example@mail.com (Junqi He, Yujie Zhang, Jialu Wang, Tao Wang, Pan Zhang, Chengjie Cai, Jinxing Yang, Xiao Lin, Xiaohui Yang)</author>
      <guid isPermaLink="false">2503.00470v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and Baseline Models</title>
      <link>http://arxiv.org/abs/2503.02876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍SPIDER数据集，这是一个公共的病理图像描述库，是当前最大的补丁级别数据集之一，涵盖了多种器官类型，提供高质量注释和周围背景信息。&lt;h4&gt;背景&lt;/h4&gt;现有的公开计算病理学数据集在器官多样性、类别覆盖率或标注质量方面存在局限性。这限制了AI技术的发展。&lt;h4&gt;目的&lt;/h4&gt;引入SPIDER以弥补现有公共数据集中不足的器官种类、分类覆盖范围及注释质量问题，从而推动计算机病理学领域的发展。&lt;h4&gt;方法&lt;/h4&gt;创建了一个涵盖皮肤、结肠直肠和胸部等多类型组织的数据集，并提供了由专业病理学家验证的高质量注释。此外，还开发了使用Hibou-L基础模型作为特征提取器结合注意力机制分类头的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过在SPIDER数据集上训练的基线模型，在多种组织类别中取得了最先进的性能表现，这些模型为未来的数字病理研究提供了强有力的基准。&lt;h4&gt;结论&lt;/h4&gt;不仅限于补丁级别的分类任务，该方法还支持快速定位重要区域、提供定量组织指标，并为进一步多模态路径学技术的发展奠定基础。SPIDER数据集和训练好的模型已向公众开放使用。&lt;h4&gt;翻译&lt;/h4&gt;推进计算病理学领域的AI研究需要大量的高质量且多样化的数据集，但现有公开的数据集往往在器官多样性、分类覆盖范围或注释质量方面存在限制。为了填补这一空白，我们推出了SPIDER（监督的病理图像描述库），这是一个最大的公共可用补丁级别数据集，涵盖了包括皮肤、结肠直肠和胸部在内的多种组织类型，并且每种组织都具有全面的分类覆盖率。SPIDER提供了由专家病理学家验证过的高质量注释，还包括周围的背景补丁，这些增强了空间上下文信息下的分类性能表现。除了该数据集外，我们还展示了基于Hibou-L基础模型作为特征提取器结合注意力机制分类头训练出基线模型的方法，这些模型在多种组织类别中取得了最先进的性能，为未来的数字病理学研究提供了强有力的基准点。SPIDER不仅限于补丁级别的分类任务，它还能快速识别重要的区域、提供定量的组织指标，并为进一步多模态方法的发展打下了基础。该数据集及训练好的模型均向公众开放以推进研究和人工智能驱动下的病理学发展。访问网址：https://github.com/HistAI/SPIDER&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancing AI in computational pathology requires large, high-quality, anddiverse datasets, yet existing public datasets are often limited in organdiversity, class coverage, or annotation quality. To bridge this gap, weintroduce SPIDER (Supervised Pathology Image-DEscription Repository), thelargest publicly available patch-level dataset covering multiple organ types,including Skin, Colorectal, and Thorax, with comprehensive class coverage foreach organ. SPIDER provides high-quality annotations verified by expertpathologists and includes surrounding context patches, which enhanceclassification performance by providing spatial context.  Alongside the dataset, we present baseline models trained on SPIDER using theHibou-L foundation model as a feature extractor combined with anattention-based classification head. The models achieve state-of-the-artperformance across multiple tissue categories and serve as strong benchmarksfor future digital pathology research. Beyond patch classification, the modelenables rapid identification of significant areas, quantitative tissue metrics,and establishes a foundation for multimodal approaches.  Both the dataset and trained models are publicly available to advanceresearch, reproducibility, and AI-driven pathology development. Access them at:https://github.com/HistAI/SPIDER</description>
      <author>example@mail.com (Dmitry Nechaev, Alexey Pchelnikov, Ekaterina Ivanova)</author>
      <guid isPermaLink="false">2503.02876v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Towards Understanding the Benefit of Multitask Representation Learning in Decision Process</title>
      <link>http://arxiv.org/abs/2503.00345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2205.15701&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种在多任务学习环境中提高样本效率的方法，通过分析未知非线性表示功能的多任务强化学习(MRL)，填补了理论框架上的空白。&lt;h4&gt;背景&lt;/h4&gt;多任务表征学习（MRL）被广泛认为是提升强化学习中样本效率的一种技术。然而，在实际应用中，现有的理论分析方法通常假设代理已知表示函数或使用线性类函数，这在现实中不太实用。&lt;h4&gt;目的&lt;/h4&gt;研究并填补多任务学习环境中未知非线性表示功能的理论空白，提供一个全面的机制分析，特别是在在线和迁移学习设置下。&lt;h4&gt;方法&lt;/h4&gt;考虑了一个代理同时执行M个情境贝叶斯问题（或马尔可夫决策过程）的情况，并使用我们提出的广义函数上限置信界算法(GFUCB)从非线性功能类中开发出共享表示功能φ。&lt;h4&gt;主要发现&lt;/h4&gt;正式证明了这种方法可以超越学习单独任务的下限，表明多任务表征学习在一般功能类中的有效性。该框架还解释了表示功能对迁移学习的影响，并确定了成功转移的关键条件。&lt;h4&gt;结论&lt;/h4&gt;实证实验进一步验证了理论发现，证实了MRL方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multitask Representation Learning (MRL) has emerged as a prevalent techniqueto improve sample efficiency in Reinforcement Learning (RL). Empirical studieshave found that training agents on multiple tasks simultaneously within onlineand transfer learning environments can greatly improve efficiency. Despite itspopularity, a comprehensive theoretical framework that elucidates itsoperational efficacy remains incomplete. Prior analyses have predominantlyassumed that agents either possess a pre-known representation function orutilize functions from a linear class, where both are impractical. Thecomplexity of real-world applications typically requires the use ofsophisticated, non-linear functions such as neural networks as representationfunction, which are not pre-existing but must be learned. Our work tries tofill the gap by extending the analysis to \textit{unknown non-linear}representations, giving a comprehensive analysis for its mechanism in onlineand transfer learning setting. We consider the setting that an agentsimultaneously playing $M$ contextual bandits (or MDPs), developing a sharedrepresentation function $\phi$ from a non-linear function class $\Phi$ usingour novel Generalized Functional Upper Confidence Bound algorithm (GFUCB). Weformally prove that this approach yields a regret upper bound that outperformsthe lower bound associated with learning $M$ separate tasks, marking the firstdemonstration of MRL's efficacy in a general function class. This frameworkalso explains the contribution of representations to transfer learning whenfaced with new, yet related tasks, and identifies key conditions for successfultransfer. Empirical experiments further corroborate our theoretical findings.</description>
      <author>example@mail.com (Rui Lu, Yang Yue, Andrew Zhao, Simon Du, Gao Huang)</author>
      <guid isPermaLink="false">2503.00345v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>LLM-Fusion: A Novel Multimodal Fusion Model for Accelerated Material Discovery</title>
      <link>http://arxiv.org/abs/2503.01022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, presented at AAAI 2025 Workshop on AI to Accelerating  Science and Engineering (AI2ASE)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LLM-Fusion是一种新型的多模态融合模型，利用大型语言模型（LLMs）整合多种表示形式，如SMILES、SELFIES、文本描述和分子指纹等，用于准确预测材料属性。&lt;h4&gt;背景&lt;/h4&gt;在高效地发现具有理想特性的材料方面仍然存在重要问题。许多研究通过使用有关材料的不同信息集来解决这个问题。其中多模态方法由于能够结合不同来源的信息而显示出潜力。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新型的多模态融合模型LLM-Fusion，该模型利用大型语言模型（LLMs）整合多样化的表示形式，以比传统方法更高的准确性预测材料属性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于LLM的灵活架构，支持多模态输入处理，并在两个数据集上的五项预测任务上验证了其有效性。与单模式和简单的拼接基线相比，模型显示出了优越的效果。&lt;h4&gt;主要发现&lt;/h4&gt;LLM-Fusion能够提供丰富的多模态表示形式，比现有融合算法更复杂且有效，从而实现更高精度的材料属性预测。&lt;h4&gt;结论&lt;/h4&gt;LLM-Fusion在准确预测多种材料特性方面表现优于传统方法和简单基线模型，证明了基于大型语言模型进行多模态数据整合的有效性。&lt;h4&gt;翻译&lt;/h4&gt;发现具有理想特性的材料并以高效的方式进行仍然是材料科学中的一个重要问题。许多研究通过利用关于材料的不同信息集来解决这个问题。在这其中，多模态方法由于其结合不同信息来源的能力而显得有前景。然而，到目前为止的融合算法仍然相对简单，缺乏提供丰富多样表示形式的机制。本文提出了一种新的多模态融合模型LLM-Fusion，该模型利用大型语言模型（LLMs）整合各种表示形式，如SMILES、SELFIES、文本描述和分子指纹等，并用于准确预测材料属性。我们的方法引入了一种基于LLM的灵活架构，支持多模态输入处理，并能以比传统方法更高的精度进行材料属性预测。我们在两个数据集上的五项预测任务上验证了我们的模型，并证明其效果优于单模式和简单的拼接基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Discovering materials with desirable properties in an efficient way remains asignificant problem in materials science. Many studies have tackled thisproblem by using different sets of information available about the materials.Among them, multimodal approaches have been found to be promising because oftheir ability to combine different sources of information. However, fusionalgorithms to date remain simple, lacking a mechanism to provide a richrepresentation of multiple modalities. This paper presents LLM-Fusion, a novelmultimodal fusion model that leverages large language models (LLMs) tointegrate diverse representations, such as SMILES, SELFIES, text descriptions,and molecular fingerprints, for accurate property prediction. Our approachintroduces a flexible LLM-based architecture that supports multimodal inputprocessing and enables material property prediction with higher accuracy thantraditional methods. We validate our model on two datasets across fiveprediction tasks and demonstrate its effectiveness compared to unimodal andnaive concatenation baselines.</description>
      <author>example@mail.com (Onur Boyar, Indra Priyadarsini, Seiji Takeda, Lisa Hamada)</author>
      <guid isPermaLink="false">2503.01022v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Waste Classification By Dual-Encoder Contrastive Learning and Multi-Clustering Voting (DECMCV)</title>
      <link>http://arxiv.org/abs/2503.02241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的无监督方法Dual-Encoder Contrastive Learning with Multi-Clustering Voting (DECMCV)，该方法旨在提高垃圾分类的自动化和效率。&lt;h4&gt;背景&lt;/h4&gt;垃圾分类对提升处理效率、减少环境污染至关重要。传统的有监督深度学习方法依赖大量标注数据，这些数据昂贵且难以获取；而自监督学习和无监督学习尽管可以在一定程度上解决数据稀缺问题，但仍存在性能不足的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的无标签垃圾分类算法，以提高模型的泛化能力和处理实际场景中的风格差异。&lt;h4&gt;方法&lt;/h4&gt;DECMCV采用预训练的ConvNeXt模型进行图像编码，使用Vision Transformer生成正样本，并应用多聚类投票机制来解决数据标注和领域偏移问题。&lt;h4&gt;主要发现&lt;/h4&gt;在TrashNet和华为云数据集上，DECMCV达到了93.78%和98.29%的分类准确率；并且仅需50个标签样本就能有效标注4169张真实世界垃圾图像，提高了模型的分类准确性。&lt;h4&gt;结论&lt;/h4&gt;该研究成功开发出一种高效的无监督垃圾分类方法，能够更好地应对实际数据中的风格差异问题，并有助于提高自动化的垃圾分类系统的性能和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Waste classification is crucial for improving processing efficiency andreducing environmental pollution. Supervised deep learning methods are commonlyused for automated waste classification, but they rely heavily on large labeleddatasets, which are costly and inefficient to obtain. Real-world waste dataoften exhibit category and style biases, such as variations in camera angles,lighting conditions, and types of waste, which can impact the model'sperformance and generalization ability. Therefore, constructing a bias-freedataset is essential. Manual labeling is not only costly but also inefficient.While self-supervised learning helps address data scarcity, it still depends onsome labeled data and generally results in lower accuracy compared tosupervised methods. Unsupervised methods show potential in certain cases buttypically do not perform as well as supervised models, highlighting the needfor an efficient and cost-effective unsupervised approach. This study presentsa novel unsupervised method, Dual-Encoder Contrastive Learning withMulti-Clustering Voting (DECMCV). The approach involves using a pre-trainedConvNeXt model for image encoding, leveraging VisionTransformer to generatepositive samples, and applying a multi-clustering voting mechanism to addressdata labeling and domain shift issues. Experimental results demonstrate thatDECMCV achieves classification accuracies of 93.78% and 98.29% on the TrashNetand Huawei Cloud datasets, respectively, outperforming or matching supervisedmodels. On a real-world dataset of 4,169 waste images, only 50 labeled sampleswere needed to accurately label thousands, improving classification accuracy by29.85% compared to supervised models. This method effectively addresses styledifferences, enhances model generalization, and contributes to the advancementof automated waste classification.</description>
      <author>example@mail.com (Kui Huang, Mengke Song, Shuo Ba, Ling An, Huajie Liang, Huanxi Deng, Yang Liu, Zhenyu Zhang, Chichun Zhou)</author>
      <guid isPermaLink="false">2503.02241v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving</title>
      <link>http://arxiv.org/abs/2503.03074v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于大语言模型（LLM）的自动驾驶系统BEVDriver，该系统在CARLA模拟器中实现了端到端闭环驾驶。通过利用潜在鸟瞰图特征作为感知输入，结合高效的多视图图像和3D LiDAR点云处理，BEVDriver能够在考虑导航指令和关键场景的情况下预测并规划未来的精确轨迹。&lt;h4&gt;背景&lt;/h4&gt;自主驾驶技术有望为未来交通带来更高的效率，但现有方法在将三维空间定位与大语言模型的语言理解和推理能力结合方面存在挑战。当前的自动驾驶系统需要建立安全性、可靠性和透明度以获得信任。&lt;h4&gt;目的&lt;/h4&gt;通过引入BEVDriver来探索如何利用大语言模型作为通用决策者进行自主驾驶，并解决3D空间定位和LLM语言及推理能力相结合的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 使用了基于大语言模型的端到端闭环自动驾驶系统BEVDriver，该系统在CARLA模拟器中运行。2. 采用了高效的多视图图像处理模块（BEV编码器）来高效地整合来自不同视角的视觉信息以及3D LiDAR点云数据。3. 将感知到的信息转化为潜在空间中的鸟瞰图特征，这些特征在Q-Former模型中传播以与自然语言指令对齐，并传递给LLM进行预测和规划。&lt;h4&gt;主要发现&lt;/h4&gt;BEVDriver系统在LangAuto基准测试上表现出了显著的性能提升，在驾驶得分方面比最先进的方法高出最多18.9%。&lt;h4&gt;结论&lt;/h4&gt;研究证明了大语言模型可以作为一种强大的工具，为自动驾驶中的决策制定提供支持，并展示了将3D感知与LLM结合的有效性。未来的工作可能包括进一步提高系统的鲁棒性和扩展其应用场景。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于如何利用基于大语言模型的系统（BEVDriver）来实现自动驾驶车辆在虚拟环境中的端到端闭环驾驶，通过处理多视角图像和3D LiDAR数据，并结合自然语言指令以规划精确的未来行驶路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving has the potential to set the stage for more efficientfuture mobility, requiring the research domain to establish trust through safe,reliable and transparent driving. Large Language Models (LLMs) possessreasoning capabilities and natural language understanding, presenting thepotential to serve as generalized decision-makers for ego-motion planning thatcan interact with humans and navigate environments designed for human drivers.While this research avenue is promising, current autonomous driving approachesare challenged by combining 3D spatial grounding and the reasoning and languagecapabilities of LLMs. We introduce BEVDriver, an LLM-based model for end-to-endclosed-loop driving in CARLA that utilizes latent BEV features as perceptioninput. BEVDriver includes a BEV encoder to efficiently process multi-viewimages and 3D LiDAR point clouds. Within a common latent space, the BEVfeatures are propagated through a Q-Former to align with natural languageinstructions and passed to the LLM that predicts and plans precise futuretrajectories while considering navigation instructions and critical scenarios.On the LangAuto benchmark, our model reaches up to 18.9% higher performance onthe Driving Score compared to SoTA methods.</description>
      <author>example@mail.com (Katharina Winter, Mark Azer, Fabian B. Flohr)</author>
      <guid isPermaLink="false">2503.03074v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Optimal Transfer Learning for Missing Not-at-Random Matrix Completion</title>
      <link>http://arxiv.org/abs/2503.00174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了矩阵填充中的迁移学习问题，特别是在数据缺失不随机（MNAR）的情况下，通过使用来自与目标矩阵有潜在特征差异的来源矩阵来解决整个行和列缺失的问题。&lt;h4&gt;背景&lt;/h4&gt;在生物医学等领域的实际问题中经常遇到缺失数据的情况，特别是当缺失是基于某种模式而非随机时，这对传统的矩阵填充方法构成了挑战。&lt;h4&gt;目的&lt;/h4&gt;探讨如何利用包含部分相关但不完整信息的来源矩阵来提高目标矩阵填充的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一个估计框架，该框架在主动采样情况下能够达到最小化错误下限。同时考虑了主动和被动行列采样的情况，并建立了相应的理论界限。&lt;h4&gt;主要发现&lt;/h4&gt;提出的算法可以通过利用来自源数据的信息来高效地查询目标矩阵中最具有信息量的行列，从而避免了传统方法所需的部分一致性假设，能够在不增加计算复杂度的前提下提高填充精度。&lt;h4&gt;结论&lt;/h4&gt;实验结果证明了所提出的方法在真实生物医学数据集上的有效性，并且比现有的算法表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们研究的是转移学习在矩阵完成中的应用，在这种情况下，目标矩阵$Q$存在完整的行和列缺失问题。利用一个带有潜在特征变化的不完整来源矩阵$P$来建立两者之间的联系。考虑了主动和被动采样的情形，并为每个场景建立了理论上的最小化错误界限。我们的计算框架在主动采样环境下实现了这一界限，能够通过查询目标矩阵中最具有信息量的行列避免传统方法所需的部分一致性假设，提高了填充精度，并且在真实生物医学数据集上验证了算法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study transfer learning for matrix completion in a Missing Not-at-Random(MNAR) setting that is motivated by biological problems. The target matrix $Q$has entire rows and columns missing, making estimation impossible without sideinformation. To address this, we use a noisy and incomplete source matrix $P$,which relates to $Q$ via a feature shift in latent space. We consider both theactive and passive sampling of rows and columns. We establish minimax lowerbounds for entrywise estimation error in each setting. Our computationallyefficient estimation framework achieves this lower bound for the activesetting, which leverages the source data to query the most informative rows andcolumns of $Q$. This avoids the need for incoherence assumptions required forrate optimality in the passive sampling setting. We demonstrate theeffectiveness of our approach through comparisons with existing algorithms onreal-world biological datasets.</description>
      <author>example@mail.com (Akhil Jalan, Yassir Jedra, Arya Mazumdar, Soumendu Sundar Mukherjee, Purnamrita Sarkar)</author>
      <guid isPermaLink="false">2503.00174v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DELST: Dual Entailment Learning for Hyperbolic Image-Gene Pretraining in Spatial Transcriptomics</title>
      <link>http://arxiv.org/abs/2503.00804v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种名为DELST的框架，用于嵌入双曲表示并建模层级关系，从而实现图像-基因预训练。&lt;h4&gt;背景&lt;/h4&gt;空间转录组学(ST)能够以个体点的方式映射组织内的基因表达，并且具有丰富的跨模式和模式内部层次信息。然而，现有的方法依赖于对比对齐图像-基因对，无法准确捕捉ST数据中的复杂层级关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架DELST，该框架能够在建模层次结构的同时嵌入双曲表示，以实现更有效的图像-基因预训练。&lt;h4&gt;方法&lt;/h4&gt;1. 跨模式蕴涵学习：建立基因和图像之间的顺序关系，以增强图像表示的泛化能力。2. 同一模式内蕴涵学习：编码基因表达模式为层级关系，并指导不同样本间的全局层次学习。&lt;h4&gt;主要发现&lt;/h4&gt;DELST框架在标注病理学家注释的空间转录组学基准上的广泛实验中展示了其有效性，实现了比现有方法更好的预测性能。&lt;h4&gt;结论&lt;/h4&gt;通过利用双曲空间中的层级表示进行图像-基因的预训练可以显著提高模型的预测准确性。相关代码和模型可在https://github.com/XulinChen/DELST获取。&lt;h4&gt;翻译&lt;/h4&gt;摘要：空间转录组学(ST)能够以个体点的方式映射组织内的基因表达，成为多模态表现学习中的宝贵资源。此外，ST数据本身在跨模式以及同一模式内部均包含丰富的层级信息。例如，不同位置的非零基因表达数量各不相同，对应着不同的细胞活动水平和语义层次结构。然而，现有方法依赖于图像-基因对之间的对比对齐方式，无法准确捕捉到ST数据中复杂的层级关系。因此，我们提出了DELST框架，这是第一个在两个层面上建模层级并将双曲表示嵌入到图像-基因预训练中的框架：1) 跨模式蕴涵学习，建立基因和图像之间的一种顺序关系来增强图像表示的泛化能力；2) 同一模式内蕴涵学习，编码基因表达模式为层级关系，并在全球范围内指导不同样本间的层次学习。病理学家注释的空间转录组学基准上的广泛实验表明了我们框架的有效性，在预测性能上优于现有方法。我们的代码和模型可在https://github.com/XulinChen/DELST获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial transcriptomics (ST) maps gene expression within tissue at individualspots, making it a valuable resource for multimodal representation learning.Additionally, ST inherently contains rich hierarchical information both acrossand within modalities. For instance, different spots exhibit varying numbers ofnonzero gene expressions, corresponding to different levels of cellularactivity and semantic hierarchies. However, existing methods rely oncontrastive alignment of image-gene pairs, failing to accurately capture theintricate hierarchical relationships in ST data. Here, we propose DELST, thefirst framework to embed hyperbolic representations while modeling hierarchyfor image-gene pretraining at two levels: (1) Cross-modal entailment learning,which establishes an order relationship between genes and images to enhanceimage representation generalization; (2) Intra-modal entailment learning, whichencodes gene expression patterns as hierarchical relationships, guidinghierarchical learning across different samples at a global scale andintegrating biological insights into single-modal representations. Extensiveexperiments on ST benchmarks annotated by pathologists demonstrate theeffectiveness of our framework, achieving improved predictive performancecompared to existing methods. Our code and models are available at:https://github.com/XulinChen/DELST.</description>
      <author>example@mail.com (Xulin Chen, Junzhou Huang)</author>
      <guid isPermaLink="false">2503.00804v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Learning Precoding in Multi-user Multi-antenna Systems: Transformer or Graph Transformer?</title>
      <link>http://arxiv.org/abs/2503.02998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了Transformer模型在除信道获取任务外的多用户多天线系统中的预编码策略学习能力，并提出了一种新的Graph Transformers架构，以充分利用基础带和混合预编码策略的置换性质。&lt;h4&gt;背景&lt;/h4&gt;Transformers在通道预测等任务中表现出色，而图神经网络（GNNs）则适用于各种通信任务。但是，关于Transformer是否能在非信道获取任务上有效以及如何结合两者的优势尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;通过研究多用户多天线系统中的预编码策略学习问题，探讨和验证在特定场景下同时利用Transformers和图神经网络优势的方法，并提出新的Graph Transformers架构。&lt;h4&gt;方法&lt;/h4&gt;构建了基于异构图的GNNs与Transformer之间的关系模型。提出了二维（2D）和三维（3D）图变换器（Gformers），这两种模型分别用于学习基础带和混合预编码策略中的置换性质。通过模拟实验评估并比较了它们的学习性能、推理复杂度、训练复杂度以及规模泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;1. 为适应不同数量的用户，需要考虑多用户干扰问题，这可以通过定制Transformer来解决。2. Transformer仅能部分利用预编码策略中的置换性质，并不能适用于变化的天线数目，这种情况与在同质图上学习的GNN相同。3. 利用异构图上的GNNs和Transformers之间的关系建立Graph Transformers，可以更好地处理基于带宽以及混合模式下的预编码策略问题。&lt;h4&gt;结论&lt;/h4&gt;通过实验表明，所提出的2D-和3D-Gformers在学习性能、推理复杂度、训练复杂度等方面均优于传统的Transformer和图神经网络（GNNs），并且具备更好的规模泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Transformers已被设计用于通道获取任务（如信道预测）以及其他诸如预编码的任务，而图神经网络(GNNs)在学习多种通信任务方面已显示出其高效性。然而，关于Transformer是否适用于除信道获取之外的任务以及如何利用这两种架构的优势尚不明确。本文以多用户多天线系统中的预编码策略学习为例来解答这些问题。我们注意到针对预编码定制的Transformer可以反映多用户干扰问题，这对于它在用户数量上的泛化能力是至关重要的。然而，这种定制的Transformer只能利用部分预编码策略的置换性质，并且无法适用于变化的天线数目，与基于同质图进行学习的GNN相同。为了提供有用的见解，我们建立了Transformers和学习异构图上GNNs之间的关系。在此基础上，我们提出了Graph Transformers（即2D-和3D-Gformers），用于挖掘基础带预编码和混合预编码策略中的置换性质。通过模拟实验评估并比较了Gformers的学习性能、推理复杂度、训练复杂度以及规模泛化能力与Transformer和GNNs的对比结果。&lt;h4&gt;创新点&lt;/h4&gt;提出了Graph Transformers，即2D-和3D-Gformers，用于更好地处理基础带及混合模式下的预编码策略问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers have been designed for channel acquisition tasks such as channelprediction and other tasks such as precoding, while graph neural networks(GNNs) have been demonstrated to be efficient for learning a multitude ofcommunication tasks. Nonetheless, whether or not Transformers are efficient forthe tasks other than channel acquisition and how to reap the benefits of botharchitectures are less understood. In this paper, we take learning precodingpolicies in multi-user multi-antenna systems as an example to answer thequestions. We notice that a Transformer tailored for precoding can reflectmultiuser interference, which is essential for its generalizability to thenumber of users. Yet the tailored Transformer can only leverage partialpermutation property of precoding policies and hence is not generalizable tothe number of antennas, same as a GNN learning over a homogeneous graph. Toprovide useful insight, we establish the relation between Transformers and theGNNs that learn over heterogeneous graphs. Based on the relation, we proposeGraph Transformers, namely 2D- and 3D-Gformers, for exploiting the permutationproperties of baseband precoding and hybrid precoding policies. The learningperformance, inference and training complexity, and size-generalizability ofthe Gformers are evaluated and compared with Transformers and GNNs viasimulations.</description>
      <author>example@mail.com (Yuxuan Duan, Jia Guo, Chenyang Yang)</author>
      <guid isPermaLink="false">2503.02998v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Fine-tuning machine-learned particle-flow reconstruction for new detector geometries in future colliders</title>
      <link>http://arxiv.org/abs/2503.00131v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文展示了通过机器学习算法进行粒子流重建的迁移学习能力，特别是在高能粒子对撞机中从一个探测器设计转移到另一个设计的有效性。&lt;h4&gt;背景&lt;/h4&gt;在高能量粒子物理实验中，利用机器学习技术优化粒子流量的计算是一种创新的方法。然而，如何将这种训练过的模型迁移到不同的探测器上是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;研究通过迁移学习，使用初始大型全仿真数据集在一个探测器设计上预训练算法模型，并在不同对撞机和探测器设计的数据样本上进行微调的有效性。&lt;h4&gt;方法&lt;/h4&gt;利用紧凑型线性对撞机（CLICdet）模型作为初始训练集，在未来环形正负电子对撞机的电子-正电子模式下提议的类似CLIC的设计（CLD）中实现成功知识转移。通过在第二个数据集中使用少一个数量级的数据样本，来展示与从头开始昂贵训练相同的性能。&lt;h4&gt;主要发现&lt;/h4&gt;迁移学习模型仅需在10万次CLD事件后，在事件层面指标上达到了传统规则基础粒子流方法的类似性能；而未经微调的全新模型则至少需要1百万次CLD事件才能实现类似的重建效果。这是首次针对全仿真跨探测器转移学习研究。&lt;h4&gt;结论&lt;/h4&gt;这些结果对于构建可以适应不同探测器设计和几何形状的大规模物理模型提供了宝贵的见解，有助于加速新探测器的发展周期，并为利用机器学习进行快速的探测器设计和优化开辟了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要中英文对照&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We demonstrate transfer learning capabilities in a machine-learned algorithmtrained for particle-flow reconstruction in high energy particle colliders.This paper presents a cross-detector fine-tuning study, where we initiallypre-train the model on a large full simulation dataset from one detectordesign, and subsequently fine-tune the model on a sample with a differentcollider and detector design. Specifically, we use the Compact Linear Colliderdetector (CLICdet) model for the initial training set, and demonstratesuccessful knowledge transfer to the CLIC-like detector (CLD) proposed for theFuture Circular Collider in electron-positron mode (FCC-ee). We show that withan order of magnitude less samples from the second dataset, we can achieve thesame performance as a costly training from scratch, across particle-level andevent-level performance metrics; including jet resolution and missingtransverse momentum resolution. Furthermore, we find that the fine-tuned modelachieves comparable performance to the traditional rule-based particle-flowapproach on event-level metrics after training on 100,000 CLD events, whereas amodel trained from scratch requires at least 1 million CLD events to achievesimilar reconstruction performance. To our knowledge, this represents the firstfull-simulation cross-detector transfer learning study for particle-flow. Thesefindings offer valuable insights towards building large physics models that canbe fine-tuned across different detector designs and geometries, helpingaccelerate the development cycle for new detectors, and opening the door torapid detector design and optimization using machine learning.</description>
      <author>example@mail.com (Farouk Mokhtar, Joosep Pata, Michael Kagan, Dolores Garcia, Eric Wulff, Mengke Zhang, Javier Duarte)</author>
      <guid isPermaLink="false">2503.00131v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Language-Guided Visual Perception Disentanglement for Image Quality Assessment and Conditional Image Generation</title>
      <link>http://arxiv.org/abs/2503.02206v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的多模态解耦表示学习框架，旨在解决对比视觉语言模型在图像质量评估和条件生成任务中难以控制感知特性的挑战。&lt;h4&gt;背景&lt;/h4&gt;当前的对比视觉-语言模型（如CLIP）基于大规模I&amp;1T数据集进行训练，在语义识别任务上表现出色。然而，这种多模态表示主要强调语义而忽视了对感知特性精确控制的需求，这在图像质量评估和条件生成等任务中是不利的。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用解耦文本引导图像解耦的新框架，以改善上述视觉任务中的性能表现。&lt;h4&gt;方法&lt;/h4&gt;首先构建I&amp;2T数据集，该数据集包含每个图像的感知性和语义性两个独立描述。然后使用这些解耦的文字作为监督信号来从CLIP的原始特征空间中分离出纯粹的感知表示，并将其命名为DeCLIP框架。最后利用这种解耦后的特性表示来进行图像质量评估和条件生成。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量的实验与对比，表明所提出的方法在两个流行任务上具有优势。&lt;h4&gt;结论&lt;/h4&gt;论文展示了一种创新性的解决现有视觉语言模型局限性的问题方法，并且研究团队承诺会公开数据集、代码以及模型。&lt;h4&gt;翻译&lt;/h4&gt;对比视觉-语言模型（如CLIP）已经在语义识别任务中显示出了强大的零样本能力，这主要是由于它们在大规模I&amp;1T数据集上的训练。这种多模态表示通常混合了语义和感知元素，并特别强调语义。然而，对于图像质量评估和条件图像生成等流行的任务来说，需要对感知和语义特征进行精细控制，则这一特点可能会成为问题。受上述事实启发，本文提出了一种新的多模态解耦表示学习框架，利用了解耦的文本引导图像解耦。为此，我们首先构建了一个I&amp;2T数据集，该数据集中每个图像都有一个独立的感知性文本描述和语义性文本描述。然后使用这些解耦的文字作为监督信号来从CLIP的原始特征空间中分离出纯粹的感知表示，并将其命名为DeCLIP。最后利用这种解耦后的特性表示来进行图像质量评估（技术质量和美学质量）和条件生成。大量的实验和对比表明，所提出的方法在这两个流行任务上具有优势。数据集、代码和模型将会公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive vision-language models, such as CLIP, have demonstrated excellentzero-shot capability across semantic recognition tasks, mainly attributed tothe training on a large-scale I&amp;1T (one Image with one Text) dataset. This kindof multimodal representations often blend semantic and perceptual elements,placing a particular emphasis on semantics. However, this could be problematicfor popular tasks like image quality assessment (IQA) and conditional imagegeneration (CIG), which typically need to have fine control on perceptual andsemantic features. Motivated by the above facts, this paper presents a newmultimodal disentangled representation learning framework, which leveragesdisentangled text to guide image disentanglement. To this end, we first buildan I&amp;2T (one Image with a perceptual Text and a semantic Text) dataset, whichconsists of disentangled perceptual and semantic text descriptions for animage. Then, the disentangled text descriptions are utilized as supervisorysignals to disentangle pure perceptual representations from CLIP's original`coarse' feature space, dubbed DeCLIP. Finally, the decoupled featurerepresentations are used for both image quality assessment (technical qualityand aesthetic quality) and conditional image generation. Extensive experimentsand comparisons have demonstrated the advantages of the proposed method on thetwo popular tasks. The dataset, code, and model will be available.</description>
      <author>example@mail.com (Zhichao Yang, Leida Li, Pengfei Chen, Jinjian Wu, Giuseppe Valenzise)</author>
      <guid isPermaLink="false">2503.02206v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Attention Fusion of MRI and Jacobian Maps for Alzheimer's Disease Diagnosis</title>
      <link>http://arxiv.org/abs/2503.00586v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种利用交叉注意力融合框架的方法，旨在通过结合结构磁共振成像（sMRI）强度和雅可比行列式图（JSM）来提高阿尔茨海默病（AD）早期诊断的准确性。&lt;h4&gt;背景&lt;/h4&gt;阿尔茨海默病的早期诊断至关重要。虽然结构磁共振成像广泛用于该疾病的诊断，但传统的深度学习方法主要依赖于基于强度的特征，这需要大量数据集才能捕捉到微妙的变化。而雅可比行列式图提供了有关局部脑变形的补充信息。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的跨模态融合策略——交叉注意力融合框架，以更好地结合sMRI和JSM的信息进行AD分类。&lt;h4&gt;方法&lt;/h4&gt;利用阿尔茨海默病神经影像学倡议（ADNI）数据集，在四种预训练的3D图像编码器上对比了三种不同的注意机制：交叉注意力、成对自注意力和瓶颈注意力。&lt;h4&gt;主要发现&lt;/h4&gt;与其它两种方法相比，交叉注意力融合框架在区分AD患者和认知正常个体以及轻度认知障碍（MCI）个体和认知正常个体方面表现出更好的性能。同时，该模型参数量较少，具有高计算效率。&lt;h4&gt;结论&lt;/h4&gt;这项研究展示了使用交叉注意力融合框架可以提高阿尔茨海默病诊断的准确性和效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，描述了早期诊断阿尔茨海默病的重要性，并介绍了通过结合结构MRI和JSM的信息来改进该疾病诊断的新方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early diagnosis of Alzheimer's disease (AD) is critical for interventionbefore irreversible neurodegeneration occurs. Structural MRI (sMRI) is widelyused for AD diagnosis, but conventional deep learning approaches primarily relyon intensity-based features, which require large datasets to capture subtlestructural changes. Jacobian determinant maps (JSM) provide complementaryinformation by encoding localized brain deformations, yet existing multimodalfusion strategies fail to fully integrate these features with sMRI. We proposea cross-attention fusion framework to model the intrinsic relationship betweensMRI intensity and JSM-derived deformations for AD classification. Using theAlzheimer's Disease Neuroimaging Initiative (ADNI) dataset, we comparecross-attention, pairwise self-attention, and bottleneck attention with fourpre-trained 3D image encoders. Cross-attention fusion achieves superiorperformance, with mean ROC-AUC scores of 0.903 (+/-0.033) for AD vs.cognitively normal (CN) and 0.692 (+/-0.061) for mild cognitive impairment(MCI) vs. CN. Despite its strong performance, our model remains highlyefficient, with only 1.56 million parameters--over 40 times fewer thanResNet-34 (63M) and Swin UNETR (61.98M). These findings demonstrate thepotential of cross-attention fusion for improving AD diagnosis whilemaintaining computational efficiency.</description>
      <author>example@mail.com (Shijia Zhang, Xiyu Ding, Brian Caffo, Junyu Chen, Cindy Zhang, Hadi Kharrazi, Zheyu Wang)</author>
      <guid isPermaLink="false">2503.00586v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Foundation-Model-Boosted Multimodal Learning for fMRI-based Neuropathic Pain Drug Response Prediction</title>
      <link>http://arxiv.org/abs/2503.00210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于fMRI的神经病理性疼痛药物反应预测的方法FMM$_{TC}$，该方法通过结合疼痛特异性的多模态信息和来自广泛无痛数据集的知识，克服了现有单一模式fMRI模型的局限性。&lt;h4&gt;背景&lt;/h4&gt;神经病理性疼痛影响高达10%的成年人群，治疗效果有限且耐受性差。静息态功能磁共振成像(rs-fMRI)是预测药物反应的重要工具，但由于数据稀缺和方法复杂度高，其应用受限。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够利用有限疼痛特异性数据并整合外部知识的方法FMM$_{TC}$，以提高神经病理性疼痛治疗药物反应的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一个基于fMRI基础模型增强的多模态学习框架FMM$_{TC}$，该框架结合了rs-fMRI的时间序列和功能连接两种模式的信息，并从大量无痛数据集中获取外部知识来补充有限的数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，FMM$_{TC}$在内部和公共数据集上均表现出优越的表示能力、泛化能力和跨数据集适应性。消融研究验证了多模态学习和基础模型驱动的知识转移的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过准确预测药物反应以提高临床试验中的参与者分层效率，FMM$_{TC}$有助于神经病理性疼痛治疗的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：由10%的成年人受到的影响，神经病理性疼痛由于有限的疗效和耐受性仍难以治疗。尽管静息态功能磁共振成像(rs-fMRI)是用于药物反应预测的脑生物标志物的重要非侵入式测量方法，fMRI的复杂性要求具有强大容量的机器学习模型。然而，在神经病理性疼痛研究中的数据稀缺限制了高容量模型的应用。为了解决数据匮乏的问题，我们提出了FMM$_{TC}$，一种用于基于fMRI的神经病理性疼痛药物反应预测的基础模型增强多模态学习框架，该框架利用了疼痛特异性内部多模态信息和来自广泛无痛基础模型的知识。具体来说，为了最大化有限疼痛特异性数据的价值，FMM$_{TC}$整合了两种rs-fMRI模式：时间序列和功能连接。进一步地，通过从大量无痛无关fMRI数据集中获取的外部知识来增强FMM$_{TC}$。使用内部和公共开放神经数据集进行的评估表明，与只考虑一种rs-fMRI模态的现有单模态fMRI模型相比，FMM$_{TC}$具有更好的表示能力、泛化能力和跨数据集适应性。消融研究验证了多模式学习以及由基础模型驱动的知识转移的有效性。基于集成梯度解释的研究说明了FMM$_{TC}$如何通过动态行为增强其跨数据集的适应性。综上所述，FMM$_{TC}$通过准确预测药物反应以提高参与者分层效率来支持神经病理性疼痛治疗的发展临床试验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neuropathic pain, affecting up to 10% of adults, remains difficult to treatdue to limited therapeutic efficacy and tolerability. Although resting-statefunctional MRI (rs-fMRI) is a promising non-invasive measurement of brainbiomarkers to predict drug response in therapeutic development, the complexityof fMRI demands machine learning models with substantial capacity. However,extreme data scarcity in neuropathic pain research limits the application ofhigh-capacity models. To address the challenge of data scarcity, we proposeFMM$_{TC}$, a Foundation-Model-boosted Multimodal learning framework forfMRI-based neuropathic pain drug response prediction, which leverages bothinternal multimodal information in pain-specific data and external knowledgefrom large pain-agnostic data. Specifically, to maximize the value of limitedpain-specific data, FMM$_{TC}$ integrates complementary information from twors-fMRI modalities: Time series and functional Connectivity. FMM$_{TC}$ isfurther boosted by an fMRI foundation model with its external knowledge fromextensive pain-agnostic fMRI datasets enriching limited pain-specificinformation. Evaluations with an in-house dataset and a public dataset fromOpenNeuro demonstrate FMM$_{TC}$'s superior representation ability,generalizability, and cross-dataset adaptability over existing unimodal fMRImodels that only consider one of the rs-fMRI modalities. The ablation studyvalidates the effectiveness of multimodal learning and foundation-model-poweredexternal knowledge transfer in FMM$_{TC}$. An integrated gradient-basedinterpretation study explains how FMM$_{TC}$'s cross-dataset dynamic behaviorsenhance its adaptability. In conclusion, FMM$_{TC}$ boosts clinical trials inneuropathic pain therapeutic development by accurately predicting drugresponses to improve the participant stratification efficiency.</description>
      <author>example@mail.com (Wenrui Fan, L. M. Riza Rizky, Jiayang Zhang, Chen Chen, Haiping Lu, Kevin Teh, Dinesh Selvarajah, Shuo Zhou)</author>
      <guid isPermaLink="false">2503.00210v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts</title>
      <link>http://arxiv.org/abs/2503.02819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于费曼-卡克公式和加权模拟方案的新颖采样方法，用于从一系列预训练的评分模型导出的退火、几何平均或乘积分布中进行抽样。&lt;h4&gt;背景&lt;/h4&gt;分数生成模型是跨多个领域的首选模型。然而，在推理时控制这些模型行为的有效工具有限，尤其是在组合多个预训练模型的情况下。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于费曼-卡克公式的加权模拟方案（FKCs），用于从一系列退火、几何平均或乘积分布中进行采样，并改进分类器自由引导方法和分子生成任务。&lt;h4&gt;方法&lt;/h4&gt;通过仔细考虑适当的偏微分方程中的各项，提出了费曼-卡克校正器（FKC）加权模拟方案。为模拟这些PDE，提议使用基于推理时间缩放的顺序蒙特卡洛重采样算法来提高抽样质量。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证了该方法在预训练模型中改进多目标分子生成和文本到图像生成中的分类器自由引导的有效性，并提出了通过推理时间温度退火进行快速采样的方案。&lt;h4&gt;结论&lt;/h4&gt;提出了一种有效且原理性的方法，用于从一系列预先训练的评分模型导出的分布中抽样。该方法不仅可以改善现有技术，在特定任务上也表现出色。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：尽管分数生成模型在多个领域中是首选模型，但在推理时控制这些模型行为的有效工具有限，尤其是在组合多个预训练模型的情况下。现有的分类器自由引导方法使用简单的启发式方法来混合条件和无条件评分以从条件分布采样。然而，这样的方法并不逼近中间的分布，因此需要额外的'校正'步骤。在这项工作中，我们提供了一种高效且原理性的方法用于从一系列退火、几何平均或乘积分布中进行抽样，这些分布是从预训练的评分模型导出的。基于著名的费曼-卡克公式，并通过仔细考虑适当的偏微分方程（PDEs）中的各项，我们提出加权模拟方案，称为费曼-卡克校正器（FKCs）。为了模拟这些PDE，我们提出了顺序蒙特卡洛重采样算法，利用推理时间缩放来提高抽样的质量。通过在预训练模型中改进多目标分子生成和文本到图像生成中的分类器自由引导的实用性，以及提出通过推理时间温度退火进行快速采样的方案，我们在实验上证明了我们方法的有效性。我们的代码可在https://github.com/martaskrt/fkc-diffusion获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While score-based generative models are the model of choice across diversedomains, there are limited tools available for controlling inference-timebehavior in a principled manner, e.g. for composing multiple pretrained models.Existing classifier-free guidance methods use a simple heuristic to mixconditional and unconditional scores to approximately sample from conditionaldistributions. However, such methods do not approximate the intermediatedistributions, necessitating additional 'corrector' steps. In this work, weprovide an efficient and principled method for sampling from a sequence ofannealed, geometric-averaged, or product distributions derived from pretrainedscore-based models. We derive a weighted simulation scheme which we callFeynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula bycarefully accounting for terms in the appropriate partial differentialequations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo(SMC) resampling algorithms that leverage inference-time scaling to improvesampling quality. We empirically demonstrate the utility of our methods byproposing amortized sampling via inference-time temperature annealing,improving multi-objective molecule generation using pretrained models, andimproving classifier-free guidance for text-to-image generation. Our code isavailable at https://github.com/martaskrt/fkc-diffusion.</description>
      <author>example@mail.com (Marta Skreta, Tara Akhound-Sadegh, Viktor Ohanesian, Roberto Bondesan, Alán Aspuru-Guzik, Arnaud Doucet, Rob Brekelmans, Alexander Tong, Kirill Neklyudov)</author>
      <guid isPermaLink="false">2503.02819v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation</title>
      <link>http://arxiv.org/abs/2503.03045v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;本文介绍了ArticuBot，这是一种单个学习策略可以使机器人系统在现实世界中打开各种未见过的铰接物体的技术。由于这些对象的几何形状、大小和铰链类型存在巨大差异，这一任务长期以来对机器人技术来说一直是个挑战。我们的系统Articubot由三个部分组成：在基于物理的模拟环境中生成大量演示，在点云基础上通过模仿学习将所有生成的演示提炼成神经策略，并进行零样本仿真到现实转移至实际机器人系统中。&lt;h4&gt;背景&lt;/h4&gt;传统的机器人任务难以处理具有多种几何形状、大小和铰链类型的未见过物体，而Articubot旨在解决这一挑战&lt;h4&gt;目的&lt;/h4&gt;开发一种基于学习的策略，使机器人能够打开从未见过的不同种类铰接式对象，同时能够在不同环境下实现零样本仿真到现实转移。&lt;h4&gt;方法&lt;/h4&gt;包括在物理模拟中生成大量演示、通过模仿学习提炼成点云神经策略，并进行仿真实验与真实机器人的零样本迁移。此外，提出了一个层次化的策略表示模型和一个新的加权位移模型。&lt;h4&gt;主要发现&lt;/h4&gt;该研究展示了Articubot能够在不同实验室、客厅以及厨房环境下打开多种未见过的铰接式物体的能力，证明了学习到的策略具有良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过结合模拟数据生成、模仿学习和零样本仿真到现实迁移技术，ArticuBot成功地实现了机器人在面对未知铰接物体时的有效操作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents ArticuBot, in which a single learned policy enables arobotics system to open diverse categories of unseen articulated objects in thereal world. This task has long been challenging for robotics due to the largevariations in the geometry, size, and articulation types of such objects. Oursystem, Articubot, consists of three parts: generating a large number ofdemonstrations in physics-based simulation, distilling all generateddemonstrations into a point cloud-based neural policy via imitation learning,and performing zero-shot sim2real transfer to real robotics systems. Utilizingsampling-based grasping and motion planning, our demonstration generalizationpipeline is fast and effective, generating a total of 42.3k demonstrations over322 training articulated objects. For policy learning, we propose a novelhierarchical policy representation, in which the high-level policy learns thesub-goal for the end-effector, and the low-level policy learns how to move theend-effector conditioned on the predicted goal. We demonstrate that thishierarchical approach achieves much better object-level generalization comparedto the non-hierarchical version. We further propose a novel weighteddisplacement model for the high-level policy that grounds the prediction intothe existing 3D structure of the scene, outperforming alternative policyrepresentations. We show that our learned policy can zero-shot transfer tothree different real robot settings: a fixed table-top Franka arm across twodifferent labs, and an X-Arm on a mobile base, opening multiple unseenarticulated objects across two labs, real lounges, and kitchens. Videos andcode can be found on our project website: https://articubot.github.io/.</description>
      <author>example@mail.com (Yufei Wang, Ziyu Wang, Mino Nakura, Pratik Bhowal, Chia-Liang Kuo, Yi-Ting Chen, Zackory Erickson, David Held)</author>
      <guid isPermaLink="false">2503.03045v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>V$^2$Dial: Unification of Video and Visual Dialog via Multimodal Experts</title>
      <link>http://arxiv.org/abs/2503.02063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;我们提出了V$^2$Dial - 一种新的专家模型，专门用于同时处理图像和视频输入数据以进行多模态对话任务。&lt;h4&gt;背景描述&lt;/h4&gt;现有的多模态模型主要集中在较为简单的任务上（例如视觉问答、视频问答、视频文本检索），而忽视了更具挑战性的对话类任务（如基于视频或可视/图像的对话）。此外，关于这些任务的工作各自独立发展，尽管它们之间有明显的相似性，这限制了其潜在的应用。&lt;h4&gt;研究目的&lt;/h4&gt;提出一种单一模型来统一这些对话任务，并首次联合学习图像和视频的空间和时间特征。通过专门的专家处理输入并通过匹配和对比学习技术对齐。&lt;h4&gt;方法描述&lt;/h4&gt;使用专用专家将图像和视频数据路由并利用匹配与对比学习技术进行特征对齐，以系统地研究两种任务之间的领域迁移问题。&lt;h4&gt;主要发现&lt;/h4&gt;模型在AVSD和VisDial等广泛使用的对话数据集上进行了广泛的评估，在零样本和微调设置下均达到了新的最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;V$^2$Dial通过将图像和视频的时空特征联合学习，提供了一种新颖的方法来处理多模态对话任务，并在多个基准测试中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present V$^2$Dial - a novel expert-based model specifically geared towardssimultaneously handling image and video input data for multimodalconversational tasks. Current multimodal models primarily focus on simplertasks (e.g., VQA, VideoQA, video-text retrieval) and often neglect the morechallenging conversational counterparts, such as video and visual/image dialog.Moreover, works on both conversational tasks evolved separately from each otherdespite their apparent similarities limiting their applicability potential. Tothis end, we propose to unify both tasks using a single model that for thefirst time jointly learns the spatial and temporal features of images andvideos by routing them through dedicated experts and aligns them using matchingand contrastive learning techniques. Furthermore, we systemically study thedomain shift between the two tasks by investigating whether and to what extentthese seemingly related tasks can mutually benefit from their respectivetraining data. Extensive evaluations on the widely used video and visual dialogdatasets of AVSD and VisDial show that our model achieves new state-of-the-artresults across four benchmarks both in zero-shot and fine-tuning settings.</description>
      <author>example@mail.com (Adnen Abdessaied, Anna Rohrbach, Marcus Rohrbach, Andreas Bulling)</author>
      <guid isPermaLink="false">2503.02063v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Out-of-Distribution Generalization on Graphs via Progressive Inference</title>
      <link>http://arxiv.org/abs/2503.02988v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GPro模型，该模型采用逐步推理的方法学习图的因果不变性，以提高在数据分布变化情况下的预测性能。&lt;h4&gt;背景&lt;/h4&gt;目前大多数图形神经网络（GNNs）假设独立同分布的数据环境，在实际应用中这种假设往往并不成立。当数据分布发生显著偏移时，现有的GNN模型常常无法产生可靠的预测结果。&lt;h4&gt;目的&lt;/h4&gt;旨在通过提取输入图中的因果不变部分来改善模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为GPro的新模型，该模型采用逐步推理的方式学习图的因果不变性，并通过创建反事实样本扩大训练分布以提升其在捕捉因果不变性方面的性能。&lt;h4&gt;主要发现&lt;/h4&gt;与现有方法相比，GPro模型能够更好地识别并利用数据中的因果不变部分，在数据分布显著变化时仍能保持较高的预测准确性。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，GPro在一系列基准测试上优于当前最先进的方法，并且在更极端的数据分布偏移情况下表现尤为出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development and evaluation of graph neural networks (GNNs) generallyfollow the independent and identically distributed (i.i.d.) assumption. Yetthis assumption is often untenable in practice due to the uncontrollable datageneration mechanism. In particular, when the data distribution shows asignificant shift, most GNNs would fail to produce reliable predictions and mayeven make decisions randomly. One of the most promising solutions to improvethe model generalization is to pick out causal invariant parts in the inputgraph. Nonetheless, we observe a significant distribution gap between thecausal parts learned by existing methods and the ground truth, leading toundesirable performance. In response to the above issues, this paper presentsGPro, a model that learns graph causal invariance with progressive inference.Specifically, the complicated graph causal invariant learning is decomposedinto multiple intermediate inference steps from easy to hard, and theperception of GPro is continuously strengthened through a progressive inferenceprocess to extract causal features that are stable to distribution shifts. Wealso enlarge the training distribution by creating counterfactual samples toenhance the capability of the GPro in capturing the causal invariant parts.Extensive experiments demonstrate that our proposed GPro outperforms thestate-of-the-art methods by 4.91% on average. For datasets with more severedistribution shifts, the performance improvement can be up to 6.86%.</description>
      <author>example@mail.com (Yiming Xu, Bin Shi, Zhen Peng, Huixiang Liu, Bo Dong, Chen Chen)</author>
      <guid isPermaLink="false">2503.02988v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ArcPro: Architectural Programs for Structured 3D Abstraction of Sparse Points</title>
      <link>http://arxiv.org/abs/2503.02745v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025 (Patent Protected); Project page:  https://vcc.tech/research/2025/ArcPro&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的学习框架ArcPro，该框架基于架构程序从稀疏和低质量的点云中恢复结构化的3D抽象。&lt;h4&gt;背景&lt;/h4&gt;现有技术在处理稀疏且低质量的点云数据时存在不足，无法有效地从中提取结构信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决上述问题，并通过使用架构程序作为桥梁连接前馈和逆向生成过程以提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;{'设计领域特定语言(DSL)': '用DSL分层表示建筑结构为一个程序，该程序可以高效地转换成网格。', '训练数据合成': '利用前馈处理来实现训练数据的合成。', '编码器-解码器网络': '通过在点云和架构程序之间进行配对训练了一个编码器-解码器网络，其中3D卷积编码器提取点云特征，而Transformer解码器自回归地预测标记化形式的程序。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'高效推断': '所提出的方法在推理过程中非常高效，并且能够生成合理和忠实的3D抽象。', '性能优越': '实验表明ArcPro优于传统的建筑代理重建方法以及基于学习的抽象方法。', '扩展应用': '进一步探索了其与多视图图像和自然语言输入配合工作的潜力。'}&lt;h4&gt;结论&lt;/h4&gt;通过引入ArcPro，为从稀疏且低质量的点云中恢复结构化3D抽象提供了一种新的解决方案，并展示了该框架在建筑领域和其他相关领域的潜在应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们在论文摘要的基础上，将信息以分点的形式进行了总结和提炼。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce ArcPro, a novel learning framework built on architecturalprograms to recover structured 3D abstractions from highly sparse andlow-quality point clouds. Specifically, we design a domain-specific language(DSL) to hierarchically represent building structures as a program, which canbe efficiently converted into a mesh. We bridge feedforward and inverseprocedural modeling by using a feedforward process for training data synthesis,allowing the network to make reverse predictions. We train an encoder-decoderon the points-program pairs to establish a mapping from unstructured pointclouds to architectural programs, where a 3D convolutional encoder extractspoint cloud features and a transformer decoder autoregressively predicts theprograms in a tokenized form. Inference by our method is highly efficient andproduces plausible and faithful 3D abstractions. Comprehensive experimentsdemonstrate that ArcPro outperforms both traditional architectural proxyreconstruction and learning-based abstraction methods. We further explore itspotential to work with multi-view image and natural language inputs.</description>
      <author>example@mail.com (Qirui Huang, Runze Zhang, Kangjun Liu, Minglun Gong, Hao Zhang, Hui Huang)</author>
      <guid isPermaLink="false">2503.02745v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Recognition of Dysarthria in Amyotrophic Lateral Sclerosis patients using Hypernetworks</title>
      <link>http://arxiv.org/abs/2503.01892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本论文提出了一种使用超网络识别ALS患者失语症的新方法，并通过实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;肌萎缩侧索硬化症（ALS）是一种进行性的神经退行性疾病，症状多样，包括言语清晰度下降。现有研究依赖于特征提取策略和定制的卷积神经网络来预测临床标准ALSFRS-R以识别失语症。&lt;h4&gt;目的&lt;/h4&gt;提出一种使用超网络识别ALS患者失语症的新方法，并评估其相对于其他基准模型的优势。&lt;h4&gt;方法&lt;/h4&gt;采用音频文件，将其转换为log-Mel频谱图、delta和delta-delta形式，并通过预训练的修改版AlexNet模型处理。最后利用生成目标网络权重的超网络来完成识别任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验在新收集的公共数据集VOC-ALS上进行，结果显示所提出的算法可以达到高达82.66%的准确率，优于包括多模态融合方法在内的强基准模型。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了超网络应用到ALS失语症识别中的价值和优势，在泛化能力、参数效率以及鲁棒性方面超过了当前最先进的结果。&lt;h4&gt;翻译&lt;/h4&gt;肌萎缩侧索硬化症（ALS）是一种进行性的神经退行性疾病，具有多样的症状，包括言语清晰度下降。现有研究通过预测临床标准ALSFRS-R来识别ALS患者的失语症，依赖于特征提取策略和定制的卷积神经网络设计后接稠密层的方法。然而，最近的研究表明，采用输入条件计算逻辑的神经网络具有一系列优点，如更快的训练速度、更好的性能以及灵活性。为了解决这些问题，我们提出了首个将超网络用于识别失语症的研究。具体来说，我们将音频文件转换成log-Mel频谱图、delta和delta-delta，并通过预训练修改后的AlexNet模型处理这些图像。最后，使用生成目标网络权重的超网络来完成任务。实验在新收集并公开的VOC-ALS数据集上进行，结果显示所提出的方法准确率高达82.66%，优于包括多模态融合方法在内的强基准模型，并且消融研究结果表明了该方法的有效性。总的来说，我们的方法在泛化能力、参数效率和鲁棒性方面相对最先进的成果具有显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Amyotrophic Lateral Sclerosis (ALS) constitutes a progressiveneurodegenerative disease with varying symptoms, including decline in speechintelligibility. Existing studies, which recognize dysarthria in ALS patientsby predicting the clinical standard ALSFRS-R, rely on feature extractionstrategies and the design of customized convolutional neural networks followedby dense layers. However, recent studies have shown that neural networksadopting the logic of input-conditional computations enjoy a series ofbenefits, including faster training, better performance, and flexibility. Toresolve these issues, we present the first study incorporating hypernetworksfor recognizing dysarthria. Specifically, we use audio files, convert them intolog-Mel spectrogram, delta, and delta-delta, and pass the resulting imagethrough a pretrained modified AlexNet model. Finally, we use a hypernetwork,which generates weights for a target network. Experiments are conducted on anewly collected publicly available dataset, namely VOC-ALS. Results showed thatthe proposed approach reaches Accuracy up to 82.66% outperforming strongbaselines, including multimodal fusion methods, while findings from an ablationstudy demonstrated the effectiveness of the introduced methodology. Overall,our approach incorporating hypernetworks obtains valuable advantages overstate-of-the-art results in terms of generalization ability, parameterefficiency, and robustness.</description>
      <author>example@mail.com (Loukas Ilias, Dimitris Askounis)</author>
      <guid isPermaLink="false">2503.01892v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual Attention for Multimodal LLMs</title>
      <link>http://arxiv.org/abs/2503.02597v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多模态大型语言模型AKI，该模型通过解锁因果注意力机制并引入跨模态互惠注意（MMA），使得图像和文本之间能够互相影响，从而解决了现有MLLMs中存在的视觉-语言不一致问题。&lt;h4&gt;背景&lt;/h4&gt;最近的多模态大型语言模型在感知和推理多模式查询方面取得了显著进展，但同时出现了一个关键挑战：生成的文字响应与给定的图文输入在事实层面上可能并不一致。&lt;h4&gt;目的&lt;/h4&gt;本文旨在从一个基础且未被探索的角度来解决视觉-语言不一致性问题，通过重新审视MLLMs的核心架构并提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;AKI模型将传统的因果注意力机制转变为跨模态互惠注意（MMA），使得图像token可以关注文本token。这种设计简洁而有效，不需要增加额外的参数或训练时间。&lt;h4&gt;主要发现&lt;/h4&gt;通过在12个多模态理解基准测试中进行实验，AKI比现有模型平均高出7.2%，展示了其出色的性能表现。&lt;h4&gt;结论&lt;/h4&gt;AKI模型通过引入跨模态互惠注意机制成功地提高了多模态语言模型的视觉-语言一致性，并且该设计具有通用性和可扩展性，适用于各种模态和多样化的多模态场景。作者公开了代码并计划发布AKI-4B模型以推动未来在MLLMs领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;最近的多模态大型语言模型（MLLM）在处理多模式查询方面展示了显著的进步，并开启了基础模型研究的新时代，但视觉和语言之间的不一致问题成为一个关键挑战。本文通过重新审视MLLM的核心架构提出了一种新的解决方案：AKI模型，它引入了跨模态互惠注意机制来增强图像token与文本token之间的影响关系。实验结果表明AKI在多个基准测试中表现出了优越的性能，并且作者计划发布他们的模型以鼓励该领域的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent Multimodal Large Language Models (MLLMs) have demonstrated significantprogress in perceiving and reasoning over multimodal inquiries, ushering in anew research era for foundation models. However, vision-language misalignmentin MLLMs has emerged as a critical challenge, where the textual responsesgenerated by these models are not factually aligned with the given text-imageinputs. Existing efforts to address vision-language misalignment have focusedon developing specialized vision-language connectors or leveraging visualinstruction tuning from diverse domains. In this paper, we tackle this issuefrom a fundamental yet unexplored perspective by revisiting the corearchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMsconsisting of a causal attention mechanism, which limits the ability of earliermodalities (e.g., images) to incorporate information from later modalities(e.g., text). To address this problem, we propose AKI, a novel MLLM thatunlocks causal attention into modality-mutual attention (MMA) to enable imagetokens to attend to text tokens. This simple yet effective design allows AKI toachieve superior performance in 12 multimodal understanding benchmarks (+7.2%on average) without introducing additional parameters and increasing trainingtime. Our MMA design is intended to be generic, allowing for application acrossvarious modalities, and scalable to accommodate diverse multimodal scenarios.The code is publicly available at https://github.com/sony/aki, and we willrelease our AKI-4B model to encourage further advancements in MLLMs acrossvarious directions.</description>
      <author>example@mail.com (Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi)</author>
      <guid isPermaLink="false">2503.02597v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning is Not So Mysterious or Different</title>
      <link>http://arxiv.org/abs/2503.02113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文讨论了深度神经网络中一些看似特殊的泛化行为（如良性过拟合和双重下降）实际上可以使用现有的理论框架进行解释，这些行为并非仅出现在神经网络中。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型常常表现出异常的泛化特性，这被认为挑战了传统机器学习方法中的泛化观念。例如，过度参数化在实践中非常成功，而传统的智慧认为应该避免过拟合。&lt;h4&gt;目的&lt;/h4&gt;论证这些现象并不独特于神经网络，并且可以通过现有的理论框架（如PAC-Bayes和可数假设边界）进行理解。&lt;h4&gt;方法&lt;/h4&gt;提出“软先验偏置”作为解释这些问题的关键原则：不是限制假设空间以避免过拟合，而是使用灵活的假设空间并倾向于那些与数据一致但更简单的解决方案。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习模型并不像人们通常认为的那样神秘或独特；相反，“软先验偏置”的原理可以应用于多种模型类别。&lt;h4&gt;结论&lt;/h4&gt;虽然深度神经网络在泛化行为上可能与其他模型类似，但在表示学习、模式连接等方面存在相对独特的特性。&lt;h4&gt;翻译&lt;/h4&gt;深层神经网络经常被看作不同于其他类型的模型，并挑战了关于泛化的传统观点。例如，良性过拟合、双重下降现象以及过度参数化的成功应用等。本文认为这些现象并不仅存在于神经网络中，且可以通过PAC-Bayes和可数假设边界等长期存在的理论框架来进行理解和严谨地刻画。我们提出了一种称为“软先验偏置”的原则作为解释这些现象的关键要素：即在不试图避免过拟合的情况下限制假设空间时，采用一个灵活的假设空间，并倾向于那些与数据一致但更简单的解决方案。这一原理可以应用于多种模型类别中，因此深度学习并不像人们通常认为的那样神秘或独特。然而，我们还强调了深度学习的独特之处，比如它在表示学习中的能力、模式连接现象等相对普遍性的特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks are often seen as different from other model classes bydefying conventional notions of generalization. Popular examples of anomalousgeneralization behaviour include benign overfitting, double descent, and thesuccess of overparametrization. We argue that these phenomena are not distinctto neural networks, or particularly mysterious. Moreover, this generalizationbehaviour can be intuitively understood, and rigorously characterized usinglong-standing generalization frameworks such as PAC-Bayes and countablehypothesis bounds. We present soft inductive biases as a key unifying principlein explaining these phenomena: rather than restricting the hypothesis space toavoid overfitting, embrace a flexible hypothesis space, with a soft preferencefor simpler solutions that are consistent with the data. This principle can beencoded in many model classes, and thus deep learning is not as mysterious ordifferent from other model classes as it might seem. However, we also highlighthow deep learning is relatively distinct in other ways, such as its ability forrepresentation learning, phenomena such as mode connectivity, and its relativeuniversality.</description>
      <author>example@mail.com (Andrew Gordon Wilson)</author>
      <guid isPermaLink="false">2503.02113v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Deal: Distributed End-to-End GNN Inference for All Nodes</title>
      <link>http://arxiv.org/abs/2503.02960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为Deal的分布式GNN推理系统，该系统专注于对拥有数十亿边的图进行端到端的所有节点推理。通过在采样期间挖掘未充分利用的共享机会，并优化后续GNN计算中的共享收益，Deal系统展示了高效的内存使用和通信效率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在推荐和广告等应用中广泛使用，其常见的形式是为所有节点进行端到端推理。然而，在处理大规模图时，传统的分布式方法难以实现高效的资源共享以优化计算性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的系统——Deal，旨在解决现有GNN推理过程中的资源浪费问题，并最大化共享收益，从而提高对大规模图形的推理效率和减少内存使用量。&lt;h4&gt;方法&lt;/h4&gt;1. 开发了能够高效协作划分分布式推理的一维图和特征张量的节省内存且通信高效的分布式原语。       2. 引入分区化、流水线化的通讯方式，并将初始GNN原始计算与特性准备过程融合，以实现端到端推断。&lt;h4&gt;主要发现&lt;/h4&gt;在现实世界的基准数据集上，使用Deal进行端到端推理的时间最多可以减少7.70倍，而图形构建时间则减少了高达21.05倍。这表明了新方法的有效性和效率提升。&lt;h4&gt;结论&lt;/h4&gt;Deal系统成功地解决了大规模图的分布式GNN推理问题，在保证准确性的同时极大地提高了计算效率和资源利用效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容：图神经网络（GNNs）是一个新的研究前沿，具有多种应用及成功的案例。对于所有的节点进行端到端推断是常见的GNN嵌入模型的方式，并且在推荐系统与广告领域得到广泛应用。虽然在诸如推理特定节点和训练等任务中出现了共享的机会，但针对全图的端到端推断中潜在的共享机会却大大被忽视了，因为传统的努力由于巨大的开销或过度的内存使用而无法充分提取共享的好处。本文介绍了Deal，一种专门用于具有数十亿边的图进行端到端推理的分布式GNN推理系统。首先，我们揭示并利用了在采样期间未被发现的分享机会，并最大化后续GNN计算中的分享收益。其次，我们引入了节省内存和通信高效的分布式原语，这些原语基于一维图形和特征张量协作划分进行轻量化分布式推断。第三，我们介绍了分区、流水线通信，以及将特性准备与第一个GNN原始计算融合的端到端推理方法。通过使用Deal，在现实世界的基准数据集上，端到端推断时间最多可以减少7.70倍，而图形构建时间则减少了高达21.05倍，相比现有最佳技术而言有显著提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are a new research frontier with variousapplications and successes. The end-to-end inference for all nodes, is commonfor GNN embedding models, which are widely adopted in applications likerecommendation and advertising. While sharing opportunities arise in GNN tasks(i.e., inference for a few nodes and training), the potential for sharing infull graph end-to-end inference is largely underutilized because traditionalefforts fail to fully extract sharing benefits due to overwhelming overheads orexcessive memory usage.  This paper introduces Deal, a distributed GNN inference system that isdedicated to end-to-end inference for all nodes for graphs with multi-billionedges. First, we unveil and exploit an untapped sharing opportunity duringsampling, and maximize the benefits from sharing during subsequent GNNcomputation. Second, we introduce memory-saving and communication-efficientdistributed primitives for lightweight 1-D graph and feature tensorcollaborative partitioning-based distributed inference. Third, we introducepartitioned, pipelined communication and fusing feature preparation with thefirst GNN primitive for end-to-end inference. With Deal, the end-to-endinference time on real-world benchmark datasets is reduced up to 7.70 x and thegraph construction time is reduced up to 21.05 x, compared to thestate-of-the-art.</description>
      <author>example@mail.com (Shiyang Chen, Xiang Song, Vasiloudis Theodore, Hang Liu)</author>
      <guid isPermaLink="false">2503.02960v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OFF-CLIP: Improving Normal Detection Confidence in Radiology CLIP with Simple Off-Diagonal Term Auto-Adjustment</title>
      <link>http://arxiv.org/abs/2503.01794v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, and 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对CLIP模型的改进方案OFF-CLIP，通过引入off-diagonal项损失来优化正常病例检测，并采用句子级文本过滤方法减少假阴性结果。&lt;h4&gt;背景&lt;/h4&gt;CLIP在放射学中的零样本分类中减少了对人工注释的依赖，但传统的对比学习方法在处理正常案例时效果不佳，因为严格的内部样例对齐会干扰正常样本聚类。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的方法OFF-CLIP以解决正常病例检测中的问题，并提高医疗视觉语言模型的整体性能。&lt;h4&gt;方法&lt;/h4&gt;引入off-diagonal项损失以增强正常样本的聚类能力；使用句子级文本过滤去除不匹配正常的陈述，从而降低假阴性结果；该方案无需对现有的CLIP架构进行修改即可应用。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示OFF-CLIP显著提高了VinDr-CXR数据集上的正常分类性能，AUC分数较基线方法CARZero提升了0.61，并且保持或改善了异常情况的分类性能。同时，OFF-CLIP还增强了零样本定位能力，提高指向游戏准确性。&lt;h4&gt;结论&lt;/h4&gt;通过实验验证了OFF-CLIP的有效性和效率，表明其是增强医疗视觉语言模型的一个稳健和有效的改进方案。&lt;h4&gt;翻译&lt;/h4&gt;对比性语言图像预训练（CLIP）在放射学中实现了零样本分类，减少了对人工注释的依赖。然而，传统的对比学习方法难以处理正常案例检测问题，严格的内部样例对齐会导致正常样本聚类失效以及高假阳性率和假阴性率。为解决这些问题，我们提出了OFF-CLIP，这是一种改进的对比学习方案，通过引入off-diagonal项损失来优化正常样本聚类，并应用句子级文本过滤减少假阴性结果。该方法可以在现有放射学CLIP模型上实现而无需修改任何架构。实验结果显示，OFF-CLIP在VinDr-CXR数据集上的正常分类性能显著提升，相较于最先进的零样本分类基线CARZero，AUC分数提升了0.61，并且保持或改善了异常情况的分类性能。此外，OFF-CLIP还增强了零样本定位能力，通过提高指向游戏准确性来确认更好的异常位置识别。这些结果表明OFF-CLIP作为增强医疗视觉语言模型的有效性和效率上的显著改进方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Language-Image Pre-Training (CLIP) has enabled zero-shotclassification in radiology, reducing reliance on manual annotations. However,conventional contrastive learning struggles with normal case detection due toits strict intra-sample alignment, which disrupts normal sample clustering andleads to high false positives (FPs) and false negatives (FNs). To address theseissues, we propose OFF-CLIP, a contrastive learning refinement that improvesnormal detection by introducing an off-diagonal term loss to enhance normalsample clustering and applying sentence-level text filtering to mitigate FNs byremoving misaligned normal statements from abnormal reports. OFF-CLIP can beapplied to radiology CLIP models without requiring any architecturalmodifications. Experimental results show that OFF-CLIP significantly improvesnormal classification, achieving a 0.61 Area under the curve (AUC) increase onVinDr-CXR over CARZero, the state-of-the-art zero-shot classification baseline,while maintaining or improving abnormal classification performance.Additionally, OFF-CLIP enhances zero-shot grounding by improving pointing gameaccuracy, confirming better anomaly localization. These results demonstrateOFF-CLIP's effectiveness as a robust and efficient enhancement for medicalvision-language models.</description>
      <author>example@mail.com (Junhyun Park, Chanyu Moon, Donghwan Lee, Kyungsu Kim, Minho Hwang)</author>
      <guid isPermaLink="false">2503.01794v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Node-level Contrastive Unlearning on Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.02959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图数据去学习方法Node-CUL，利用嵌入空间优化来移除特定节点和边对模型的影响。通过对比剩余节点及其邻居的嵌入，逐步减弱目标节点的影响，并且不直接使用未见数据，保持了模型的有效性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在处理非欧几里得结构化的图形数据时面临挑战，特别是去除部分实体（如节点和边）的困难。现有的方法如图划分、影响函数或添加额外层都难以同时达到高可扩展性和有效性。&lt;h4&gt;目的&lt;/h4&gt;提出一种更有效的去学习算法以移除特定图实体对模型的影响，并保持模型的有效性。&lt;h4&gt;方法&lt;/h4&gt;通过对比剩余节点及其邻居的嵌入空间，逐步减弱目标节点（即要去除的节点）影响。另外还引入了邻域重构方法来优化邻居的嵌入从而减少未学节点对其它部分的影响。&lt;h4&gt;主要发现&lt;/h4&gt;Node-CUL在多种图数据和模型上进行实验后显示，其去学习效果最佳，并且提高了模型的有效性，同时只需要与现有框架类似的计算资源。&lt;h4&gt;结论&lt;/h4&gt;本文提出的Node-CUL方法证明了使用嵌入空间优化是一种高效去除特定节点对GNN影响的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph unlearning aims to remove a subset of graph entities (i.e. nodes andedges) from a graph neural network (GNN) trained on the graph. Unlike machineunlearning for models trained on Euclidean-structured data, effectivelyunlearning a model trained on non-Euclidean-structured data, such as graphs, ischallenging because graph entities exhibit mutual dependencies. Existing worksutilize graph partitioning, influence function, or additional layers to achievegraph unlearning. However, none of them can achieve high scalability andeffectiveness without additional constraints. In this paper, we achieve moreeffective graph unlearning by utilizing the embedding space. The primarytraining objective of a GNN is to generate proper embeddings for each node thatencapsulates both structural information and node feature representations.Thus, directly optimizing the embedding space can effectively remove the targetnodes' information from the model. Based on this intuition, we proposenode-level contrastive unlearning (Node-CUL). It removes the influence of thetarget nodes (unlearning nodes) by contrasting the embeddings of remainingnodes and neighbors of unlearning nodes. Through iterative updates, theembeddings of unlearning nodes gradually become similar to those of unseennodes, effectively removing the learned information without directlyincorporating unseen data. In addition, we introduce a neighborhoodreconstruction method that optimizes the embeddings of the neighbors in orderto remove influence of unlearning nodes to maintain the utility of the GNNmodel. Experiments on various graph data and models show that our Node-CULachieves the best unlearn efficacy and enhanced model utility with requiringcomparable computing resources with existing frameworks.</description>
      <author>example@mail.com (Hong kyu Lee, Qiuchen Zhang, Carl Yang, Li Xiong)</author>
      <guid isPermaLink="false">2503.02959v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation Alignment</title>
      <link>http://arxiv.org/abs/2503.01711v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  added project repository &amp; dataset URL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;个性化产品搜索旨在检索和排序符合用户偏好和搜索意图的商品。现有的方法虽然有效，但通常假设用户的查询完全捕捉到了他们的实际动机。&lt;h4&gt;背景&lt;/h4&gt;现实中电商平台的数据分析显示，用户在进行搜索之前经常参与相关的咨询活动，这表明他们在咨询过程中根据动机和需求来细化意图。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的个性化搜索方法，利用咨询中隐含的动机作为提升个性化搜索的关键因素。&lt;h4&gt;主要挑战&lt;/h4&gt;包括将上下文中的动机与简洁查询相匹配、跨越类别-文本差距以及过滤序列历史中的噪声等新挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一个考虑动机的个性化搜索（MAPS）方法。该方法利用大型语言模型将查询和咨询嵌入到统一语义空间，通过注意力专家混合机制优先处理关键语义，并引入双元对齐：对比学习对齐咨询、评论和产品特性；双向注意机制整合了基于动机感知的嵌入与用户偏好。&lt;h4&gt;主要发现&lt;/h4&gt;在真实数据集和合成数据集上的广泛实验表明MAPS方法在检索和排序任务中均优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;通过利用用户的咨询活动中的隐含动机，个性化搜索可以得到显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalized product search aims to retrieve and rank items that match users'preferences and search intent. Despite their effectiveness, existing approachestypically assume that users' query fully captures their real motivation.However, our analysis of a real-world e-commerce platform reveals that usersoften engage in relevant consultations before searching, indicating they refineintents through consultations based on motivation and need. The impliedmotivation in consultations is a key enhancing factor for personalized search.This unexplored area comes with new challenges including aligning contextualmotivations with concise queries, bridging the category-text gap, and filteringnoise within sequence history. To address these, we propose a Motivation-AwarePersonalized Search (MAPS) method. It embeds queries and consultations into aunified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE)to prioritize critical semantics, and introduces dual alignment: (1)contrastive learning aligns consultations, reviews, and product features; (2)bidirectional attention integrates motivation-aware embeddings with userpreferences. Extensive experiments on real and synthetic data show MAPSoutperforms existing methods in both retrieval and ranking tasks.</description>
      <author>example@mail.com (Weicong Qin, Yi Xu, Weijie Yu, Chenglei Shen, Ming He, Jianping Fan, Xiao Zhang, Jun Xu)</author>
      <guid isPermaLink="false">2503.01711v3</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A dataset-free approach for self-supervised learning of 3D reflectional symmetries</title>
      <link>http://arxiv.org/abs/2503.02660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种无需大型标注数据集的自监督模型，用于检测单个物体的对称性。通过利用对象本身的内在特征进行学习，并设计了不依赖于地面真实标签的学习策略。&lt;h4&gt;背景&lt;/h4&gt;传统的模型需要大量的带标签的数据来训练和识别物体的对称性，这增加了计算成本和时间开销。&lt;h4&gt;目的&lt;/h4&gt;开发一种既有效又高效的自监督方法，用于检测单个物体的对称性，并降低依赖于大型数据集的成本。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一种基于点云视觉特征的方法，利用基础图像模型提取出的特性来计算对象各点的描述符。这些描述符反映了物体上两点之间的对称关系，并且有助于优化自监督学习模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该自监督模型在检测单个物体对称性方面优于使用大型数据集训练的状态-of-the-art模型。&lt;h4&gt;结论&lt;/h4&gt;所提出的自监督方法不仅效果好，而且更加高效、资源需求低，适用于计算和数据资源有限的场景。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们探索了一种无需依赖于数据集就能学习单个物体对称性的自我监督模型。基于假设：可以通过分析对象自身的固有特征来确定其对称性，从而在训练过程中不需要大型的数据集。此外，还设计了一个自监督学习策略以消除地面真实标签的必要性。这两个关键要素使我们的方法既有效又高效，解决了构建大量标注数据集的成本问题。该研究的独特之处在于计算每个点上基于视觉外观相似性的对象特征，并利用基础图像模型提取出的特性来优化自我监督模型。实验结果表明，该自监督方法在检测单个物体对称性方面超越了依赖于大规模数据集训练的状态-of-the-art模型，且更加高效和资源节约。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we explore a self-supervised model that learns to detect thesymmetry of a single object without requiring a dataset-relying solely on theinput object itself. We hypothesize that the symmetry of an object can bedetermined by its intrinsic features, eliminating the need for large datasetsduring training. Additionally, we design a self-supervised learning strategythat removes the necessity of ground truth labels. These two key elements makeour approach both effective and efficient, addressing the prohibitive costsassociated with constructing large, labeled datasets for this task. The noveltyof our method lies in computing features for each point on the object based onthe idea that symmetric points should exhibit similar visual appearances. Toachieve this, we leverage features extracted from a foundational image model tocompute a visual descriptor for the points. This approach equips the pointcloud with visual features that facilitate the optimization of ourself-supervised model. Experimental results demonstrate that our methodsurpasses the state-of-the-art models trained on large datasets. Furthermore,our model is more efficient, effective, and operates with minimal computationaland data resources.</description>
      <author>example@mail.com (Issac Aguirre, Ivan Sipiran, Gabriel Montañana)</author>
      <guid isPermaLink="false">2503.02660v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders</title>
      <link>http://arxiv.org/abs/2503.02954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 2025 International Conference on Robotics and Automation  (ICRA 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了使用图神经网络变分自动编码器（GNN-VAE）来解决大规模多智能体协调问题，这种方法比传统的集中式优化方法更快。&lt;h4&gt;背景&lt;/h4&gt;在高密度机器人流量区域中，局部协调方法可能无法找到无死锁的解决方案。这时需要一个中央单元生成全局调度方案。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于GNN-VAE的方法来解决大规模多智能体协调问题，提高效率和性能。&lt;h4&gt;方法&lt;/h4&gt;论文将协调问题建模为图问题，并使用混合整数线性规划（MILP）求解器收集地面真实数据。在训练阶段，学习框架将高质量的解决方案编码到潜在空间中，在推理时从采样的潜在变量中解码出最优解。&lt;h4&gt;主要发现&lt;/h4&gt;GNN-VAE框架可以生成满足问题约束条件的有效提案，并且对于大规模（250个机器人）的问题也能够提供高质量的解决方案，速度远超其他基准方法。&lt;h4&gt;结论&lt;/h4&gt;提出的基于GNN-VAE的方法在处理大规模多智能体协调问题时具有显著优势。该项目页为https://mengyuest.github.io/gnn-vae-coord。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了如何利用图神经网络变分自动编码器（GNN-VAE）来更高效地解决大规模的多机器人协调问题，特别是在密集机器人交通区域中，这种方法比传统的集中式优化方法更加有效和快速。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent coordination is crucial for reliable multi-robot navigation inshared spaces such as automated warehouses. In regions of dense robot traffic,local coordination methods may fail to find a deadlock-free solution. In thesescenarios, it is appropriate to let a central unit generate a global schedulethat decides the passing order of robots. However, the runtime of suchcentralized coordination methods increases significantly with the problemscale. In this paper, we propose to leverage Graph Neural Network VariationalAutoencoders (GNN-VAE) to solve the multi-agent coordination problem at scalefaster than through centralized optimization. We formulate the coordinationproblem as a graph problem and collect ground truth data using a Mixed-IntegerLinear Program (MILP) solver. During training, our learning framework encodesgood quality solutions of the graph problem into a latent space. At inferencetime, solution samples are decoded from the sampled latent variables, and thelowest-cost sample is selected for coordination. Finally, the feasible proposalwith the highest performance index is selected for the deployment. Byconstruction, our GNN-VAE framework returns solutions that always respect theconstraints of the considered coordination problem. Numerical results show thatour approach trained on small-scale problems can achieve high-quality solutionseven for large-scale problems with 250 robots, being much faster than otherbaselines. Project page: https://mengyuest.github.io/gnn-vae-coord</description>
      <author>example@mail.com (Yue Meng, Nathalie Majcherczyk, Wenliang Liu, Scott Kiesel, Chuchu Fan, Federico Pecora)</author>
      <guid isPermaLink="false">2503.02954v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering</title>
      <link>http://arxiv.org/abs/2503.01606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新的Embedding级别框架EmbQA，旨在改进开放领域问题回答(ODQA)任务。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型推动了开放领域问题回答的发展，但目前的检索-阅读器流水线存在计算成本高、不稳定性和检索覆盖率不足的问题。&lt;h4&gt;目的&lt;/h4&gt;通过增强检索器和读者的功能来解决当前ODQA系统存在的上述挑战。&lt;h4&gt;方法&lt;/h4&gt;{'改进查询表示': '利用轻量级线性层在无监督对比学习目标下优化查询表示，重新排序检索到的段落以突出最可能包含正确答案的部分。', '引入探索性嵌入': '通过扩展模型潜在语义空间来多样化候选生成，并采用基于熵的选择机制自动选择最有信心的答案。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在三个开源LLM、三种检索方法和四个ODQA基准上，EmbQA在准确性和效率方面显著优于现有基线。&lt;h4&gt;结论&lt;/h4&gt;EmbQA框架通过改进查询表示和引入探索性嵌入有效提升了ODQA任务的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models have recently pushed open domain question answering(ODQA) to new frontiers. However, prevailing retriever-reader pipelines oftendepend on multiple rounds of prompt level instructions, leading to highcomputational overhead, instability, and suboptimal retrieval coverage. In thispaper, we propose EmbQA, an embedding-level framework that alleviates theseshortcomings by enhancing both the retriever and the reader. Specifically, werefine query representations via lightweight linear layers under anunsupervised contrastive learning objective, thereby reordering retrievedpassages to highlight those most likely to contain correct answers.Additionally, we introduce an exploratory embedding that broadens the model'slatent semantic space to diversify candidate generation and employs anentropy-based selection mechanism to choose the most confident answerautomatically. Extensive experiments across three open-source LLMs, threeretrieval methods, and four ODQA benchmarks demonstrate that EmbQAsubstantially outperforms recent baselines in both accuracy and efficiency.</description>
      <author>example@mail.com (Zhanghao Hu, Hanqi Yan, Qingling Zhu, Zhenyi Shen, Yulan He, Lin Gui)</author>
      <guid isPermaLink="false">2503.01606v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Deepfake Detection via Knowledge Injection</title>
      <link>http://arxiv.org/abs/2503.02503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为知识注入的Deepfake检测技术（KID），它构建了一个基于多任务学习的知识注入框架，可以轻松集成到现有的ViT基础模型中。&lt;h4&gt;背景&lt;/h4&gt;当前的生成式AI模型能够创造出非常逼真的deepfakes，这些内容可能被用于恶意用途。现有的deepfake检测方法要么依赖于改进分类器以更好地适应训练数据分布，要么利用伪造合成机制来学习更全面的伪造数据分布。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有方法忽视真实数据知识的问题，并提高模型处理未见过的真实和虚假数据的能力，本文提出了一种新的简单而有效的方法KID。&lt;h4&gt;方法&lt;/h4&gt;构建了一个基于多任务学习的知识注入框架。设计了知识注入模块来学习并注入必要的信息到基础模型中，以更准确地建模真实和伪造数据的分布；构造了一个粗粒度的伪造定位分支，通过多任务学习方式学习伪造位置，进一步丰富知识注入模块中的伪造知识。&lt;h4&gt;主要发现&lt;/h4&gt;提出了两种层次化的抑制损失与对比损失，强调了在知识注入模块中对真实数据知识的关注，并平衡真实和虚假的知识比例。实验表明KID具有优秀的兼容性，可以应用于不同规模的ViT基础模型，并且实现了最先进的泛化性能同时提升了训练收敛速度。&lt;h4&gt;结论&lt;/h4&gt;KID是一种新颖有效的deepfake检测技术，能够在现有ViT基础模型上简单高效地实现知识注入，提高了模型在处理真实和伪造数据上的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deepfake detection technologies become vital because current generative AImodels can generate realistic deepfakes, which may be utilized in maliciouspurposes. Existing deepfake detection methods either rely on developingclassification methods to better fit the distributions of the training data, orexploiting forgery synthesis mechanisms to learn a more comprehensive forgerydistribution. Unfortunately, these methods tend to overlook the essential roleof real data knowledge, which limits their generalization ability in processingthe unseen real and fake data. To tackle these challenges, in this paper, wepropose a simple and novel approach, named Knowledge Injection based deepfakeDetection (KID), by constructing a multi-task learning based knowledgeinjection framework, which can be easily plugged into existing ViT-basedbackbone models, including foundation models. Specifically, a knowledgeinjection module is proposed to learn and inject necessary knowledge into thebackbone model, to achieve a more accurate modeling of the distributions ofreal and fake data. A coarse-grained forgery localization branch is constructedto learn the forgery locations in a multi-task learning manner, to enrich thelearned forgery knowledge for the knowledge injection module. Two layer-wisesuppression and contrast losses are proposed to emphasize the knowledge of realdata in the knowledge injection module, to further balance the portions of thereal and fake knowledge. Extensive experiments have demonstrated that our KIDpossesses excellent compatibility with different scales of Vit-based backbonemodels, and achieves state-of-the-art generalization performance whileenhancing the training convergence speed.</description>
      <author>example@mail.com (Tonghui Li, Yuanfang Guo, Zeming Liu, Heqi Peng, Yunhong Wang)</author>
      <guid isPermaLink="false">2503.02503v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>NodeNAS: Node-Specific Graph Neural Architecture Search for Out-of-Distribution Generalization</title>
      <link>http://arxiv.org/abs/2503.02448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by DASFAA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为NodeNAS的节点特定图神经架构搜索方法，它通过拆解节点拓扑和图分布来为不同节点定制独特的聚合方法。此外，还提出了自适应聚集注意力多维NodeNAS（MNNAS）方法，在有限的数据集下学习具有良好泛化能力的节点特定架构定制器。&lt;h4&gt;背景&lt;/h4&gt;现有的GraphNAS方法在处理数据分布变化时表现出色，但它们依赖于大量训练数据，并且当面对稀疏或单一训练图时难以发现最佳的图形与架构映射关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在有限数据集下工作的节点特定图神经架构搜索（NodeNAS）方法，以提高模型在不同分布的数据上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了一种名为NodeNAS的方法，通过拆解节点拓扑和图分布来为不同的节点定制独特的聚合方法。2. 引入了自适应聚集注意力多维NodeNAS（MNNAS）方法，该方法学习了一个具有良好泛化能力的节点特定架构定制器，并且扩展了搜索空间的垂直深度以支持跨多个维度的同时进行节点特定架构定制。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，提出的MNNAS方法超越了现有的最佳算法，在监督和非监督任务中取得了卓越的性能，并展示了优秀的出分布（OOD）泛化能力。&lt;h4&gt;结论&lt;/h4&gt;新提出的方法NodeNAS及其扩展版本MNNAS有效地解决了现有GraphNAS方法在面对稀疏或单一训练图时的问题，能够在更少的数据下发现更具针对性且具有良好泛化的架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural architecture search (GraphNAS) has demonstrated advantages inmitigating performance degradation of graph neural networks (GNNs) due todistribution shifts. Recent approaches introduce weight sharing across tailoredarchitectures, generating unique GNN architectures for each graph end-to-end.However, existing GraphNAS methods do not account for distribution patternsacross different graphs and heavily rely on extensive training data. Withsparse or single training graphs, these methods struggle to discover optimalmappings between graphs and architectures, failing to generalize toout-of-distribution (OOD) data. In this paper, we propose node-specific graphneural architecture search(NodeNAS), which aims to tailor distinct aggregationmethods for different nodes through disentangling node topology and graphdistribution with limited datasets. We further propose adaptive aggregationattention based multi-dim NodeNAS method(MNNAS), which learns an node-specificarchitecture customizer with good generalizability. Specifically, we extend thevertical depth of the search space, supporting simultaneous node-specificarchitecture customization across multiple dimensions. Moreover, we model thepower-law distribution of node degrees under varying assortativity, encodingstructure invariant information to guide architecture customization across eachdimension. Extensive experiments across supervised and unsupervised tasksdemonstrate that MNNAS surpasses state-of-the-art algorithms and achievesexcellent OOD generalization.</description>
      <author>example@mail.com (Qiyi Wang, Yinning Shao, Yunlong Ma, Min Liu)</author>
      <guid isPermaLink="false">2503.02448v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Learning Actionable World Models for Industrial Process Control</title>
      <link>http://arxiv.org/abs/2503.01411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于学习世界模型的新方法，该方法通过解构过程参数在学习的潜在表示中分离关键因素，使复杂系统的行为能够从有限的数据中学到，并实现对系统的精细控制。&lt;h4&gt;背景&lt;/h4&gt;从被动的过程监控过渡到主动的过程控制需要一个有效的人工智能系统，它可以从非常有限的训练数据中学习复杂系统的特性。&lt;h4&gt;目的&lt;/h4&gt;为了形成关于过程输入和输出的即兴数字孪生体，该模型可以预测行动对流程世界的影响，研究旨在提供一种方法论以实现有效的主动过程控制。&lt;h4&gt;方法&lt;/h4&gt;通过对比性学习在联合嵌入预测架构内驱动表示学习，这种方法使从输入变化到表示变化的变化可预测，并且反过来亦然。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够揭示关键因素对流程变异的影响，为有效控制行动提供基础，以便将过程保持在其操作范围内。&lt;h4&gt;结论&lt;/h4&gt;在塑料注射成型的例子中验证了该方法的有效性，展示了其在提出针对不稳定过程的特定控制措施方面的实际相关性。&lt;h4&gt;翻译&lt;/h4&gt;为了从（被动）过程监控过渡到主动的过程控制，有效的AI系统必须能够从非常有限的训练数据中学到复杂系统的特性。研究提出了一种基于学习世界模型的新方法，该方法通过解构过程参数在学习的潜在表示中分离关键因素，并形成关于过程输入和输出的即兴数字孪生体。这种方法使复杂的行动后果可以被预测，为有效的主动控制铺平道路。使用塑料注射成型作为示例验证了此方法的有效性，展示了其提出针对不稳定过程的具体控制措施的实际相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To go from (passive) process monitoring to active process control, aneffective AI system must learn about the behavior of the complex system fromvery limited training data, forming an ad-hoc digital twin with respect toprocess in- and outputs that captures the consequences of actions on theprocess's world. We propose a novel methodology based on learning world modelsthat disentangles process parameters in the learned latent representation,allowing for fine-grained control. Representation learning is driven by thelatent factors that influence the processes through contrastive learning withina joint embedding predictive architecture. This makes changes inrepresentations predictable from changes in inputs and vice versa, facilitatinginterpretability of key factors responsible for process variations, paving theway for effective control actions to keep the process within operationalbounds. The effectiveness of our method is validated on the example of plasticinjection molding, demonstrating practical relevance in proposing specificcontrol actions for a notoriously unstable process.</description>
      <author>example@mail.com (Peng Yan, Ahmed Abdulkadir, Gerrit A. Schatte, Giulia Aguzzi, Joonsu Gha, Nikola Pascher, Matthias Rosenthal, Yunlong Gao, Benjamin F. Grewe, Thilo Stadelmann)</author>
      <guid isPermaLink="false">2503.01411v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>From superposition to sparse codes: interpretable representations in neural networks</title>
      <link>http://arxiv.org/abs/2503.01824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;理解神经网络中信息的表示是神经科学和人工智能领域的基本挑战。尽管它们具有非线性架构，最近的研究表明，神经网络以叠加的方式编码特征，即输入概念在网络表示中线性地重叠。&lt;h4&gt;背景&lt;/h4&gt;神经网络在分类任务中的训练可以恢复潜在特征，这些特征可以通过线性变换进行识别。此外，稀疏编码方法可以从这些表示中提取解耦的特性，并且定量解释性度量为评估这些方法的成功提供了途径，确保提取的功能与人类可理解的概念一致。&lt;h4&gt;目的&lt;/h4&gt;提供一个理论框架来解释神经网络如何以叠加方式存储信息，并提出一种从神经激活中提取可解释表示的方法论。&lt;h4&gt;方法&lt;/h4&gt;理论框架包括三个步骤：(1) 可识别性理论表明训练的神经网络可以恢复分类任务中的潜在特征，这些特征可以通过线性变换进行识别。(2) 稀疏编码技术可以从神经网络的表示中提取解耦特性，通过压缩感知的原则实现。(3) 定量解释度量为评估方法的成功提供了标准。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种从理论神经科学、表示学习和可解释性研究中的见解出发的新视角，来理解人工和生物系统中的神经表示。这些论点对神经编码理论、人工智能透明度以及使深度学习模型更加可解释的总体目标有重要意义。&lt;h4&gt;结论&lt;/h4&gt;通过连接来自不同领域的见解，这项工作为理解和改善神经网络的表示提供了新的视角，有助于推动神经科学与人工智能的发展。&lt;h4&gt;翻译&lt;/h4&gt;理解如何在人工神经网络中表示信息是两个学科——神经科学和人工智能中的一个基本挑战。尽管这些网络具有非线性架构，但最近的研究表明，它们通过叠加存储特征，这意味着输入概念在网络表示中以线性方式重叠。我们提出了一个解释这一现象的视角，并为从激活中提取可解释表征提供了理论基础。我们的理论框架由三部分组成：(1) 可识别性理论显示神经网络在分类任务训练中恢复潜在特性到线性转换；(2) 稀疏编码技术可以从这些表示中通过压缩感知原理来提取不相关特征；(3) 定量解释度量为评估方法的有效性提供了一种方式，确保所提取的特征与人类可理解的概念保持一致。结合理论神经科学、表示学习和可解释性研究领域的洞见，我们提出了一个新兴视角以了解人工及生物系统中的神经表征，并且我们的论点对神经编码理论、人工智能透明度以及使深度学习模型更易于解释的目标具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how information is represented in neural networks is afundamental challenge in both neuroscience and artificial intelligence. Despitetheir nonlinear architectures, recent evidence suggests that neural networksencode features in superposition, meaning that input concepts are linearlyoverlaid within the network's representations. We present a perspective thatexplains this phenomenon and provides a foundation for extracting interpretablerepresentations from neural activations. Our theoretical framework consists ofthree steps: (1) Identifiability theory shows that neural networks trained forclassification recover latent features up to a linear transformation. (2)Sparse coding methods can extract disentangled features from theserepresentations by leveraging principles from compressed sensing. (3)Quantitative interpretability metrics provide a means to assess the success ofthese methods, ensuring that extracted features align with human-interpretableconcepts. By bridging insights from theoretical neuroscience, representationlearning, and interpretability research, we propose an emerging perspective onunderstanding neural representations in both artificial and biological systems.Our arguments have implications for neural coding theories, AI transparency,and the broader goal of making deep learning models more interpretable.</description>
      <author>example@mail.com (David Klindt, Charles O'Neill, Patrik Reizinger, Harald Maurer, Nina Miolane)</author>
      <guid isPermaLink="false">2503.01824v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>RGBSQGrasp: Inferring Local Superquadric Primitives from Single RGB Image for Graspability-Aware Bin Picking</title>
      <link>http://arxiv.org/abs/2503.02387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, In submission to IROS2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了RGBSQGrasp框架，该框架结合了超二次体形状基元和基础模型驱动的深度估计方法，可以从单目RGB图像中推断出抓取姿态。&lt;h4&gt;背景&lt;/h4&gt;工件拾取任务由于遮挡和物理限制而具有挑战性。现有方法依赖于已知CAD模型或先验物体几何信息，并且从有限视角恢复超二次体形状是困难的。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来解决工件拾取中遮挡、物体识别和抓取问题，同时提高对未知物体的适应能力。&lt;h4&gt;方法&lt;/h4&gt;RGBSQGrasp包括数据集生成管道、基于基础模型的对象点云估计模块、全局局部超二次体拟合网络以及超二次体引导的抓取姿态采样模块。该框架不需要深度传感器，并通过几何推理来推断抓取姿态，提高稳定性和适应性。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界的机器人实验中显示了92%的成功率，证明了RGBSQGrasp在打包料箱拣选环境中的有效性。&lt;h4&gt;结论&lt;/h4&gt;RGBSQGrasp提供了一种有效的方法，通过单目RGB图像实现可靠抓取姿态推断，并显著提高了工件拾取任务的鲁棒性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;零件挑选是一个具有挑战性的机器人任务，由于遮挡和物理限制导致视觉信息不足。现有的方法通常依赖于已知CAD模型或先验物体几何形状，这限制了它们对新型未知对象的泛化能力。其他直接从RGB-D数据回归抓取姿态的方法则面临着深度传感中的固有噪声问题，并且缺乏对物体的理解使得抓取的姿态合成与评估更加困难。超二次体（SQ）提供了一种紧凑、可解释的形状表示，可以捕获物体的物理特性和抓取能力。然而，从有限视角恢复它们是具有挑战性的，现有的方法依赖于多个角度来进行近乎完整的点云重建，这限制了其在零件挑选任务中的有效性。为了应对这些挑战，我们提出了extbf{RGBSQGrasp}框架——一个利用超二次体形状基元和基础模型驱动的深度估计方法的抓取框架，可以从单目RGB图像中推断出抓取姿态--无需使用深度传感器。该框架整合了一个通用、跨平台的数据集生成管道、基于基础模型的对象点云估计模块、全局局部超二次体拟合网络以及一个由SQ指导的抓取姿态采样模块。通过结合这些组件，RGBSQGrasp可以通过几何推理可靠地推断出抓取姿态，提高抓取稳定性，并增强对未见过对象的适应性。在真实世界的机器人实验中展示出了92%的成功率，突显了RGBSQGrasp在包装料箱挑选环境中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bin picking is a challenging robotic task due to occlusions and physicalconstraints that limit visual information for object recognition and grasping.Existing approaches often rely on known CAD models or prior object geometries,restricting generalization to novel or unknown objects. Other methods directlyregress grasp poses from RGB-D data without object priors, but the inherentnoise in depth sensing and the lack of object understanding make graspsynthesis and evaluation more difficult. Superquadrics (SQ) offer a compact,interpretable shape representation that captures the physical and graspabilityunderstanding of objects. However, recovering them from limited viewpoints ischallenging, as existing methods rely on multiple perspectives fornear-complete point cloud reconstruction, limiting their effectiveness inbin-picking. To address these challenges, we propose \textbf{RGBSQGrasp}, agrasping framework that leverages superquadric shape primitives and foundationmetric depth estimation models to infer grasp poses from a monocular RGB camera-- eliminating the need for depth sensors. Our framework integrates auniversal, cross-platform dataset generation pipeline, a foundation model-basedobject point cloud estimation module, a global-local superquadric fittingnetwork, and an SQ-guided grasp pose sampling module. By integrating thesecomponents, RGBSQGrasp reliably infers grasp poses through geometric reasoning,enhancing grasp stability and adaptability to unseen objects. Real-worldrobotic experiments demonstrate a 92\% grasp success rate, highlighting theeffectiveness of RGBSQGrasp in packed bin-picking environments.</description>
      <author>example@mail.com (Yifeng Xu, Fan Zhu, Ye Li, Sebastian Ren, Xiaonan Huang, Yuhao Chen)</author>
      <guid isPermaLink="false">2503.02387v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>InfoGNN: End-to-end deep learning on mesh via graph neural networks</title>
      <link>http://arxiv.org/abs/2503.02414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了一个新的以图神经网络（GNN）为中心的端到端框架InfoGNN，用于解决深度学习在网格模型应用中的挑战。&lt;h4&gt;背景&lt;/h4&gt;3D模型被广泛应用于各个行业，并且由于其独特的优势，网格数据已经成为三维建模不可或缺的一部分。然而，无序、不规则的数据结构以及复杂的表面信息使得直接使用深度学习模型进行处理变得困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的端到端框架以克服在网格模型中应用深度学习的挑战，同时充分利用网格模型的优点。&lt;h4&gt;方法&lt;/h4&gt;InfoGNN将网格模型视为图，采用InfoConv和InfoMP模块利用点的位置信息以及面法线、二面角等静态信息和动态全局特征信息来处理所有类型的数据，并且是一个端到端的设计以提高效率。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在多个公开数据集上进行的测试中，InfoGNN在网格分类和分割任务中表现出色。&lt;h4&gt;结论&lt;/h4&gt;本文提出的新框架InfoGNN为复杂的3D模型提供了高效的深度学习方法，并通过简化网络设计提高了处理效率。&lt;h4&gt;翻译&lt;/h4&gt;三维模型被广泛应用于各个行业。由于其独特的优势，网格数据已经成为三维建模不可或缺的一部分，能够提供直观且实用的丰富三维信息表达方式。然而，无序、不规则的数据结构和复杂的表面信息使得直接使用深度学习模型进行处理变得困难。传统的网格数据处理方法通常依赖于具有许多限制的网格模型（如流形），这些限制在实际应用中缩小了它们的应用范围，并未充分利用网格模型的优点。本文提出了一个基于图神经网络（GNN）的新端到端框架InfoGNN，用于解决深度学习应用于网格模型中的挑战。InfoGNN将网格模型视为图，使其能够高效处理不规则的网格数据。此外，我们提出InfoConv和InfoMP模块，利用点的位置信息，并充分利用面法线、二面角等静态信息以及动态全局特征信息来充分使用所有类型的数据。另外，InfoGNN是一个端到端框架，简化了网络设计以使其更加高效，为复杂3D模型的深度学习铺平道路。我们在几个公开可用数据集上进行了实验，结果显示在网格分类和分割任务中，InfoGNN表现优异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D models are widely used in various industries, and mesh data has become anindispensable part of 3D modeling because of its unique advantages. Mesh datacan provide an intuitive and practical expression of rich 3D information.However, its disordered, irregular data structure and complex surfaceinformation make it challenging to apply with deep learning models directly.Traditional mesh data processing methods often rely on mesh models with manylimitations, such as manifold, which restrict their application scopes inreality and do not fully utilize the advantages of mesh models. This paperproposes a novel end-to-end framework for addressing the challenges associatedwith deep learning in mesh models centered around graph neural networks (GNN)and is titled InfoGNN. InfoGNN treats the mesh model as a graph, which enablesit to handle irregular mesh data efficiently. Moreover, we propose InfoConv andInfoMP modules, which utilize the position information of the points and fullyuse the static information such as face normals, dihedral angles, and dynamicglobal feature information to fully use all kinds of data. In addition, InfoGNNis an end-to-end framework, and we simplify the network design to make it moreefficient, paving the way for efficient deep learning of complex 3D models. Weconducted experiments on several publicly available datasets, and the resultsshow that InfoGNN achieves excellent performance in mesh classification andsegmentation tasks.</description>
      <author>example@mail.com (Ling Gao, Zhenyu Shu, Shiqing Xin)</author>
      <guid isPermaLink="false">2503.02414v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OCL: Ordinal Contrastive Learning for Imputating Features with Progressive Labels</title>
      <link>http://arxiv.org/abs/2503.02899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI 2024 (Provisional Accept)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的图像特征补全方法，利用多模态成像数据来更好地判断阿尔茨海默病的不同阶段。&lt;h4&gt;背景&lt;/h4&gt;准确地辨别阿尔茨海默病（AD）的各个发展阶段对于早期诊断和预防至关重要。然而，由于成本高和受试者负担重的原因，获取完整的影像集很具挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的补全方法来解决多模态成像数据中缺失值的问题，同时保持所有受试者的完整性。&lt;h4&gt;方法&lt;/h4&gt;所提方法包括两个网络：1）一个编码器，用于提取不受模式影响的嵌入；2）一个解码器，基于其影像模式重构原始测量。编码器采用了新颖的‘序数对比损失’来使样本根据AD进展对齐在嵌入空间中。&lt;h4&gt;主要发现&lt;/h4&gt;通过最大化的模态一致性以及领域对抗训练算法进一步增强了不同成像模态之间的对齐性，在实验结果中展示了对于统计分析和分类任务，该方法相较于基线补全技术有显著的优势。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型在共享嵌入空间内促进了多模式影像特征的完整恢复，并且通过ADNI研究显示了优于现有技术的结果。&lt;h4&gt;翻译&lt;/h4&gt;准确地区分阿尔茨海默病（AD）的不同阶段对于早期诊断和预防至关重要。通常涉及多种成像模态来理解AD复杂的病理过程，然而由于成本高以及受试者负担重的原因，获取完整的一组图像具有挑战性。因此，在最终的实验中缺失数据不可避免地导致了样本量有限，并且降低了下游分析中的精确度。为了应对这一挑战，我们提出了一种全面的成像特征补全方法，该方法能够利用多样化的影像特征的同时保持所有受试者的完整性。所提的方法包括两个网络：1）一个编码器用于提取模态无关的嵌入；2）一个解码器根据各自的成像模式来重构原始测量值。编码器采用了新颖的‘序数对比损失’，将样本按AD进展在嵌入空间中对齐。我们还通过域对抗训练算法最大化了每个受试者内模态之间的嵌入一致性，进一步增强了不同影像模态之间的对齐性。所提的方法促进了在共享嵌入空间内的多模式成像特征的完整恢复。在实验中，我们展示了我们的网络对于统计分析和分类任务，相较于补全基线技术，在ADNI研究中提供了更优的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-72069-7_32&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately discriminating progressive stages of Alzheimer's Disease (AD) iscrucial for early diagnosis and prevention. It often involves multiple imagingmodalities to understand the complex pathology of AD, however, acquiring acomplete set of images is challenging due to high cost and burden for subjects.In the end, missing data become inevitable which lead to limited sample-sizeand decrease in precision in downstream analyses. To tackle this challenge, weintroduce a holistic imaging feature imputation method that enables to leveragediverse imaging features while retaining all subjects. The proposed methodcomprises two networks: 1) An encoder to extract modality-independentembeddings and 2) A decoder to reconstruct the original measures conditioned ontheir imaging modalities. The encoder includes a novel {\em ordinal contrastiveloss}, which aligns samples in the embedding space according to the progressionof AD. We also maximize modality-wise coherence of embeddings within eachsubject, in conjunction with domain adversarial training algorithms, to furtherenhance alignment between different imaging modalities. The proposed methodpromotes our holistic imaging feature imputation across various modalities inthe shared embedding space. In the experiments, we show that our networksdeliver favorable results for statistical analysis and classification againstimputation baselines with Alzheimer's Disease Neuroimaging Initiative (ADNI)study.</description>
      <author>example@mail.com (Seunghun Baek, Jaeyoon Sim, Guorong Wu, Won Hwa Kim)</author>
      <guid isPermaLink="false">2503.02899v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</title>
      <link>http://arxiv.org/abs/2503.01776v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A novel sparse coding framework designed for learning adaptive  representation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种新的稀疏表示方法CSR，它在保持高保真度的同时，能够实现自适应的深度表示，并且比现有的解决方案MRL具有更高的准确性和更快的速度。&lt;h4&gt;背景&lt;/h4&gt;许多大规模系统依赖高质量的深度表示（嵌入）来完成检索、搜索和生成建模等任务。最近提出的Matryoshka Representation Learning (MRL) 方法可以实现自适应的嵌入长度，但是需要重新训练模型并且在较短长度时性能下降明显。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的稀疏编码方法CSR，以解决现有解决方案在保持高保真度的同时，提供灵活、成本效益高的不同稀疏级别推理的问题。&lt;h4&gt;方法&lt;/h4&gt;通过利用轻量级的自动编码和任务感知对比目标，将预训练嵌入稀疏化到一个高维但选择性激活的功能空间中。这种方法被称为Contrastive Sparse Representation (CSR)。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与MRL相比，CSR在图像、文本以及多模态基准数据集上，在准确性方面有显著提高，并且检索速度更快，同时训练时间大大缩短。&lt;h4&gt;结论&lt;/h4&gt;研究结果确立了稀疏编码作为自适应表示学习的重要范式，尤其是在实际应用中效率和保真度都至关重要的情况下。&lt;h4&gt;翻译&lt;/h4&gt;许多大型系统依赖于高质量的深度表示（嵌入）来促进检索、搜索和生成建模等任务。最近提出的Matryoshka Representation Learning (MRL) 方法作为一种适应性表示长度的方法出现，但它需要重新训练整个模型并且在较短长度时性能明显下降。本文中，我们展示了稀疏编码提供了一种吸引人的替代方案，以实现具有最小开销和更高保真度的自适应表示。我们提出了对比稀疏表示（CSR）方法，该方法将预训练嵌入转换为一个高维但选择性激活的功能空间。通过利用轻量级自动编码器和任务感知对比目标，CSR在保持语义质量的同时允许不同稀疏级别的灵活、成本效益高的推理。广泛的实验表明，与MRL相比，CSR在图像、文本以及多模态基准数据集上的准确性方面有显著提高，并且检索速度更快，同时训练时间大大缩短。我们的结果确立了稀疏编码作为自适应表示学习的重要范式，在实际应用中效率和保真度都至关重要的情况下尤为适用。代码可在https://github.com/neilwen987/CSR_Adaptive_Rep上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/neilwen987/CSR_Adaptive_Rep&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many large-scale systems rely on high-quality deep representations(embeddings) to facilitate tasks like retrieval, search, and generativemodeling. Matryoshka Representation Learning (MRL) recently emerged as asolution for adaptive embedding lengths, but it requires full model retrainingand suffers from noticeable performance degradations at short lengths. In thispaper, we show that sparse coding offers a compelling alternative for achievingadaptive representation with minimal overhead and higher fidelity. We proposeContrastive Sparse Representation (CSR), a method that sparsifies pre-trainedembeddings into a high-dimensional but selectively activated feature space. Byleveraging lightweight autoencoding and task-aware contrastive objectives, CSRpreserves semantic quality while allowing flexible, cost-effective inference atdifferent sparsity levels. Extensive experiments on image, text, and multimodalbenchmarks demonstrate that CSR consistently outperforms MRL in terms of bothaccuracy and retrieval speed-often by large margins-while also cutting trainingtime to a fraction of that required by MRL. Our results establish sparse codingas a powerful paradigm for adaptive representation learning in real-worldapplications where efficiency and fidelity are both paramount. Code isavailable at https://github.com/neilwen987/CSR_Adaptive_Rep</description>
      <author>example@mail.com (Tiansheng Wen, Yifei Wang, Zequn Zeng, Zhong Peng, Yudi Su, Xinyang Liu, Bo Chen, Hongwei Liu, Stefanie Jegelka, Chenyu You)</author>
      <guid isPermaLink="false">2503.01776v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>CMMLoc: Advancing Text-to-PointCloud Localization with Cauchy-Mixture-Model Based Framework</title>
      <link>http://arxiv.org/abs/2503.02593v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新的基于文本描述的点云定位框架CMMLoc，用于在大型城市环境中通过文本描述确定3D位置。&lt;h4&gt;背景&lt;/h4&gt;现有的点云定位方法通常需要完整的环境描述来匹配3D位置和文本描述。然而，在实际场景中，用户往往只提供部分相关的环境信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在仅有部分相关环境描述的情况下进行有效定位的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于Cauchy混合模型（CMM）的不确定性感知框架CMMLoc，该框架在文本和点云之间交互时将CMM约束作为先验条件，并设计了空间整合方案来适应不同3D对象的不同感受野。此外，还提出了一个方向积分模块和模态预对准策略，以捕获物体之间的空间关系。&lt;h4&gt;主要发现&lt;/h4&gt;CMMLoc在KITTI360Pose数据集上的表现优于现有方法，达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的CMMLoc框架能够有效解决基于文本描述的点云定位问题，并具有重要的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;点云定位任务的目标是在大型城市环境中通过文本描述来确定一个3D位置。该技术在车辆接驾或货物配送等众多领域都有潜在的应用前景。理想情况下，对于每一个文本描述和相应的3D位置来说，周围环境的所有物体都应该被详细描述出来。然而，在实际情况中，如车辆接驾场景下，乘客通常只会提及最重要的及最接近的目标物而不是整个环境。为了解决这种部分相关性的挑战，我们提出了一个基于Cauchy混合模型（CMM）的不确定性感知框架CMMLoc来解决从文本到点云定位的问题。通过在模态间交互时将CMM约束作为先验条件并设计空间整合方案来处理不同3D对象的不同感受野，实现了对部分相关环境描述的有效处理。为了实现精确的定位，我们还提出了一种方向积分模块以及一种模态预对准策略，帮助捕捉物体之间的空间关系，并使3D物体更加接近文本模式。通过广泛的实验验证，CMMLoc在KITTI360Pose数据集上优于现有方法，并取得了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of point cloud localization based on linguistic description is toidentify a 3D position using textual description in large urban environments,which has potential applications in various fields, such as determining thelocation for vehicle pickup or goods delivery. Ideally, for a textualdescription and its corresponding 3D location, the objects around the 3Dlocation should be fully described in the text description. However, inpractical scenarios, e.g., vehicle pickup, passengers usually describe only thepart of the most significant and nearby surroundings instead of the entireenvironment. In response to this $\textbf{partially relevant}$ challenge, wepropose $\textbf{CMMLoc}$, an uncertainty-aware$\textbf{C}$auchy-$\textbf{M}$ixture-$\textbf{M}$odel ($\textbf{CMM}$) basedframework for text-to-point-cloud $\textbf{Loc}$alization. To model theuncertain semantic relations between text and point cloud, we integrate CMMconstraints as a prior during the interaction between the two modalities. Wefurther design a spatial consolidation scheme to enable adaptive aggregation ofdifferent 3D objects with varying receptive fields. To achieve preciselocalization, we propose a cardinal direction integration module alongside amodality pre-alignment strategy, helping capture the spatial relationshipsamong objects and bringing the 3D objects closer to the text modality.Comprehensive experiments validate that CMMLoc outperforms existing methods,achieving state-of-the-art results on the KITTI360Pose dataset. Codes areavailable in this GitHub repository https://github.com/kevin301342/CMMLoc.</description>
      <author>example@mail.com (Yanlong Xu, Haoxuan Qu, Jun Liu, Wenxiao Zhang, Xun Yang)</author>
      <guid isPermaLink="false">2503.02593v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Teaching Metric Distance to Autoregressive Multimodal Foundational Models</title>
      <link>http://arxiv.org/abs/2503.02379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;随着大型语言模型的应用领域从自然语言扩展到数学、多模态理解和具身代理等领域，输出标记逐渐反映了度量关系而不是纯粹的语言意义。本文介绍了DIST2Loss，这是一种基于预定义的输出标记之间距离关系来训练自回归离散模型的距离感知框架。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的应用已经从自然语言处理领域扩展到数学、多模态理解和具身代理等领域，在这些新的应用领域中，令牌越来越反映了度量关系而不是纯粹的语言意义。&lt;h4&gt;目的&lt;/h4&gt;提出DIST2Loss方法来训练自回归离散模型，并保持现有架构的兼容性。&lt;h4&gt;方法&lt;/h4&gt;DIST2Loss通过将从固有距离度量衍生出的连续指数族分布转换为与模型架构相容的离散、分类优化目标，使模型能够学习和保留有意义的距离关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估表明，在视觉接地、机器人操作、生成性奖励建模以及使用向量量化特征进行图像生成等多元模态应用中均显示出一致性的性能改进。在训练数据有限的情况下，这些改进尤为明显。&lt;h4&gt;结论&lt;/h4&gt;DIST2Loss能够使模型即使在资源受限的环境中也表现出色，在各种多模态应用场景中具有明显的性能提升效果。&lt;h4&gt;翻译&lt;/h4&gt;随着大型语言模型的应用领域从自然语言处理向数学、多模态理解和具身代理等领域扩展，模型输出标记开始更多地反映度量关系而非纯粹的语言意义。为此，本文提出了一种名为DIST2Loss的框架，该框架利用预定义的距离关系来训练自回归离散模型，并通过将连续指数族分布转化为与现有架构兼容的目标，使模型能够有效地学习并保持有意义的距离关系。实验结果显示，在视觉接地、机器人操作等多模态应用领域中，DIST2Loss显著提升了性能表现。特别是在数据量有限的情况下，其优势尤为突出。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As large language models expand beyond natural language to domains such asmathematics, multimodal understanding, and embodied agents, tokens increasinglyreflect metric relationships rather than purely linguistic meaning. Weintroduce DIST2Loss, a distance-aware framework designed to trainautoregressive discrete models by leveraging predefined distance relationshipsamong output tokens. At its core, DIST2Loss transforms continuous exponentialfamily distributions derived from inherent distance metrics into discrete,categorical optimization targets compatible with the models' architectures.This approach enables the models to learn and preserve meaningful distancerelationships during token generation while maintaining compatibility withexisting architectures. Empirical evaluations show consistent performance gainsin diverse multimodal applications, including visual grounding, roboticmanipulation, generative reward modeling, and image generation usingvector-quantized features. These improvements are pronounced in cases oflimited training data, highlighting DIST2Loss's effectiveness inresource-constrained settings.</description>
      <author>example@mail.com (Jiwan Chung, Saejin Kim, Yongrae Jo, Jaewoo Park, Dongjun Min, Youngjae Yu)</author>
      <guid isPermaLink="false">2503.02379v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Random Walks in Self-supervised Learning for Triangular Meshes</title>
      <link>http://arxiv.org/abs/2503.00816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种用于3D网格分析的自监督学习新方法，通过随机游走进行数据增强以生成网格表面的各种表示，并结合对比和聚类损失来提高训练效果。&lt;h4&gt;背景&lt;/h4&gt;现有基于3D模型的数据增强方法缺乏有效的策略来扩展和丰富给定数据集中的样例表示。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自监督学习框架，通过随机游走作为数据增强手段生成多样化的网格表示，并结合对比和聚类损失来提高模型在下游任务上的性能。&lt;h4&gt;方法&lt;/h4&gt;采用基于随机游走的数据增广技术，利用对比损失最大化同一网格的多个实例之间的相似性同时最小化不同网格间的相似性；引入了聚类损失以增强类别区分度并减少训练中的方差。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在使用均值平均精度（mAP）分数和监督SVM线性分类器提取特征进行评估时表现出了良好的性能，显示出其在物体分类和形状检索等下游任务上的潜力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过引入随机游走、对比损失以及聚类损失的结合，在3D网格分析领域展现出了显著的进步，为后续研究提供了新的思路和可能的应用场景。&lt;h4&gt;翻译&lt;/h4&gt;这项研究针对三维网格自监督学习挑战提出了新方法。利用随机游走作为数据增强技术生成多样化的网格表示，并采用了对比及聚类损失来优化模型训练过程中的效果。通过mAP分数以及监督SVM线性分类器进行评估，结果表明该模型具备在物体分类和形状检索等下游任务上的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study addresses the challenge of self-supervised learning for 3D meshanalysis. It presents an new approach that uses random walks as a form of dataaugmentation to generate diverse representations of mesh surfaces. Furthermore,it employs a combination of contrastive and clustering losses. The contrastivelearning framework maximizes similarity between augmented instances of the samemesh while minimizing similarity between different meshes. We integrate thiswith a clustering loss, enhancing class distinction across training epochs andmitigating training variance. Our model's effectiveness is evaluated using meanAverage Precision (mAP) scores and a supervised SVM linear classifier onextracted features, demonstrating its potential for various downstream taskssuch as object classification and shape retrieval.</description>
      <author>example@mail.com (Gal Yefet, Ayellet Tal)</author>
      <guid isPermaLink="false">2503.00816v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Streamline-based diffusion MRI Tractography Registration Method with Probabilistic Keypoint Detection</title>
      <link>http://arxiv.org/abs/2503.02481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;扩散MRI纤维束配准是分析大脑白质组内相似性和变异性的关键步骤。提出了一种基于深度学习的无监督方法，通过检测和匹配不同受试者之间的点对来实现基于流线的dMRI纤维束配准。&lt;h4&gt;背景&lt;/h4&gt;现有的配准方法主要依赖于优化空间距离以确定最佳变换，但这种方法忽略了流线内部的点连接模式，从而限制了它们识别不同数据集间解剖对应关系的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的无监督深度学习方法来进行基于流线的dMRI纤维束配准，通过在受试者之间找到对应的点对来实现解剖结构的一致性匹配。&lt;h4&gt;方法&lt;/h4&gt;将轨迹建模为点云以利用沿着流线的图连接性，并提出了一个新的流线关键点检测方法，将其作为一个概率分类任务，用于识别不同不规则流线集间的解剖一致性对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法在纤维束配准性能上表现出高有效性和高效性，优于现有的几种方法。&lt;h4&gt;结论&lt;/h4&gt;该研究为基于深度学习的无监督dMRI纤维束配准提供了一种新的视角，并展示了其在识别解剖对应关系方面的能力和潜力。&lt;h4&gt;翻译&lt;/h4&gt;扩散磁共振成像（dMRI）轨迹图注册是分析大脑白质组内相似性和变异性的关键步骤。流线基注册方法利用纤维路径的三维几何信息以实现配准后的空间对齐。现有方法通常依赖于优化空间距离来确定最优变换，但这种方法忽略了流线内部的点连接模式，限制了其识别不同轨迹数据集间解剖对应关系的能力。本文提出了一种新的基于深度学习的无监督方法来进行流线基dMRI轨迹图注册，该方法通过在受试者之间找到对应的点对来实现轨迹数据集的空间对齐。我们在实验中比较了几种现有方法，并展示了高度有效的和高效的轨迹注册性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Registration of diffusion MRI tractography is an essential step for analyzinggroup similarities and variations in the brain's white matter (WM).Streamline-based registration approaches can leverage the 3D geometricinformation of fiber pathways to enable spatial alignment after registration.Existing methods usually rely on the optimization of the spatial distances toidentify the optimal transformation. However, such methods overlook pointconnectivity patterns within the streamline itself, limiting their ability toidentify anatomical correspondences across tractography datasets. In this work,we propose a novel unsupervised approach using deep learning to performstreamline-based dMRI tractography registration. The overall idea is toidentify corresponding keypoint pairs across subjects for spatial alignment oftractography datasets. We model tractography as point clouds to leverage thegraph connectivity along streamlines. We propose a novel keypoint detectionmethod for streamlines, framed as a probabilistic classification task toidentify anatomically consistent correspondences across unstructured streamlinesets. In the experiments, we compare several existing methods and show highlyeffective and efficient tractography registration performance.</description>
      <author>example@mail.com (Junyi Wang, Mubai Du, Ye Wu, Yijie Li, William M. Wells III, Lauren J. O'Donnell, Fan Zhang)</author>
      <guid isPermaLink="false">2503.02481v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Incorporating graph neural network into route choice model</title>
      <link>http://arxiv.org/abs/2503.02315v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种结合递归逻辑模型和图神经网络（GNN）的新颖混合模型，旨在提高路线选择预测的准确性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;传统上，基于理论的模型如Logit模型和递归Logit模型因其良好的可解释性而被广泛使用。然而，近年来机器学习方法由于其更好的预测准确性而受到关注。尽管图神经网络（GNN）在捕捉道路网络特征方面表现出色，并且已经在其他交通研究领域得到了广泛应用，但它们尚未用于路线选择建模。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的混合模型，将递归逻辑模型与图神经网络结合，以提高预测性能和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的混合模型，该模型集成了递归Logit模型与图神经网络（GNN），并展示了这种组合如何改进预测性能以及放松无关替代品独立性的假设而不依赖于强假设。这得益于特定类型的GNN能够从数据中有效捕捉到网络上的多个交叉效应模式。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，应用该方法后，模型的预测准确性相较于现有模型有显著提高。&lt;h4&gt;结论&lt;/h4&gt;结合递归逻辑模型和图神经网络（GNN）的方法可以有效地提升路线选择预测的准确性和可解释性。这种新方法为未来的交通研究提供了新的视角与可能性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：路线选择模型是运输研究中最重要基础之一，传统理论驱动模型如Logit模型以及递归逻辑模型因其优秀的可解释性而被广泛使用；最近机器学习技术因更高的预测准确度受到关注。在此研究中提出结合递归逻辑模型与图神经网络（GNN）的新颖混合方法以提升预判精度和模型的透明度。据作者所知，尽管图神经网络在捕获道路网络特性方面表现出色并广泛应用于运输领域的其他领域，但它们还未被用于路线选择建模。数学上证明使用图神经网络不仅有利于增强预测性能，而且可以通过不依赖于强烈假设的方式来放松无关替代品独立性的属性。特定类型的GNN能够从数据中高效捕捉到网络上的多个交叉效应模式，从而提升了这一特性。通过将提议模型应用于东京一天的旅行轨迹数据，确认了比现有模型更高的预测精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Route choice models are one of the most important foundations fortransportation research. Traditionally, theory-based models have been utilizedfor their great interpretability, such as logit models and Recursive logitmodels. More recently, machine learning approaches have gained attentions fortheir better prediction accuracy. In this study, we propose novel hybrid modelsthat integrate the Recursive logit model with Graph Neural Networks (GNNs) toenhance both predictive performance and model interpretability. To the authors'knowldedge, GNNs have not been utilized for route choice modeling, despitetheir proven effectiveness in capturing road network features and theirwidespread use in other transportation research areas. We mathematically showthat our use of GNN is not only beneficial for enhancing the predictionperformance, but also relaxing the Independence of Irrelevant Alternativesproperty without relying on strong assumptions. This is due to the fact that aspecific type of GNN can efficiently capture multiple cross-effect patterns onnetworks from data. By applying the proposed models to one-day traveltrajectory data in Tokyo, we confirmed their higher prediction accuracycompared to the existing models.</description>
      <author>example@mail.com (Yuxun Ma, Toru Seo)</author>
      <guid isPermaLink="false">2503.02315v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Effective High-order Graph Representation Learning for Credit Card Fraud Detection</title>
      <link>http://arxiv.org/abs/2503.01556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, accepted at IJCAI 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的高阶图表示学习模型（HOGRL），旨在解决现有图神经网络在处理伪装的间接多跳交易时遇到的问题。&lt;h4&gt;背景&lt;/h4&gt;信用卡欺诈给持卡人和发卡银行带来了巨大的成本，而欺诈者常通过使用数个良性用户的合法交易来规避反欺诈检测系统。现有的图神经网络（GNN）模型由于深层次聚合过程中的过度平滑问题难以学习伪装的间接多跳交易特征。&lt;h4&gt;目的&lt;/h4&gt;提出HOGRL以避免在多层次聚合过程中引入过多噪音，直接从高阶交易图中学习不同次序的纯表示来识别欺诈者的多步间接交易，并优化欺诈检测性能。&lt;h4&gt;方法&lt;/h4&gt;通过有效构建高阶交易图并学习每种顺序的纯表示，使得模型能够通过多层纯特征学习发现伪装关系。同时引入混合专家注意力机制以自动确定各阶的重要性，共同优化欺诈检测性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示HOGRL在开源和真实世界数据集上相比现有最佳基准方法有显著改进，证实了其在应对高阶欺诈伪装犯罪方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;HOGRL能够更有效地识别复杂的多跳交易模式，增强对伪装的欺诈行为检测能力。&lt;h4&gt;翻译&lt;/h4&gt;信用卡欺诈给持卡人和发卡银行带来了巨大的成本。欺诈者经常通过使用多个良性用户合法进行的交易来规避反欺诈系统。现有的图神经网络（GNN）模型由于在深层次聚合过程中的过度平滑问题，难以学习伪装交易中复杂的间接多跳特征，这是识别伪装关系的主要挑战。因此，在本文中我们提出了一种新的高阶图表示学习模型（HOGRL），以避免在多层次聚合过程中引入过多噪音，并直接从高阶交易图中获得不同的纯表示。为实现这一目标，首先构建有效的高阶交易图，然后学习每个顺序的纯表示，使模型能够通过多层纯特征学习识别欺诈者的间接多跳交易。此外我们还提出了一种混合专家注意力机制来自动确定不同顺序的重要性，并优化整体反欺诈性能。在开源和实际数据集上的广泛实验表明，HOGRL相比于现有的最佳基准有显著提高，证明了其处理复杂伪装行为的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.24963/ijcai.2024/839&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Credit card fraud imposes significant costs on both cardholders and issuingbanks. Fraudsters often disguise their crimes, such as using legitimatetransactions through several benign users to bypass anti-fraud detection.Existing graph neural network (GNN) models struggle with learning features ofcamouflaged, indirect multi-hop transactions due to their inherentover-smoothing issues in deep multi-layer aggregation, presenting a majorchallenge in detecting disguised relationships. Therefore, in this paper, wepropose a novel High-order Graph Representation Learning model (HOGRL) to avoidincorporating excessive noise during the multi-layer aggregation process. Inparticular, HOGRL learns different orders of \emph{pure} representationsdirectly from high-order transaction graphs. We realize this goal byeffectively constructing high-order transaction graphs first and then learningthe \emph{pure} representations of each order so that the model could identifyfraudsters' multi-hop indirect transactions via multi-layer \emph{pure} featurelearning. In addition, we introduce a mixture-of-expert attention mechanism toautomatically determine the importance of different orders for jointlyoptimizing fraud detection performance. We conduct extensive experiments inboth the open source and real-world datasets, the result demonstrates thesignificant improvements of our proposed HOGRL compared with state-of-the-artfraud detection baselines. HOGRL's superior performance also proves itseffectiveness in addressing high-order fraud camouflage criminals.</description>
      <author>example@mail.com (Yao Zou, Dawei Cheng)</author>
      <guid isPermaLink="false">2503.01556v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning</title>
      <link>http://arxiv.org/abs/2503.00793v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICRA 2025, Github link:  https://github.com/UkcheolShin/BridgeMultiSpectralDepth&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;部署深度估计网络到现实世界中需要具备对抗各种不利条件的高度鲁棒性，以确保安全和可靠的自主驾驶。&lt;h4&gt;目的&lt;/h4&gt;为了提高多模态传感器系统在自动驾驶车辆中的使用效率，提出了一种基于对齐与融合策略的解决方案，用于从多光谱图像进行深度估计。&lt;h4&gt;方法&lt;/h4&gt;{'align阶段': '通过最小化全局特征及几何线索的空间一致性局部特征对比损失来调整多个光谱带之间的嵌入空间，以学习跨多光谱图像可共享的表示。', 'fuse阶段': '训练一个可附加的功能融合模块，该模块可以有选择地聚合多光谱特性，用于可靠和鲁棒的预测结果。'}&lt;h4&gt;主要发现&lt;/h4&gt;所提出的策略使得单深度网络可以在保持可靠性、内存效率和灵活性的同时，实现光谱不变性和多光谱融合的深度估计。&lt;h4&gt;结论&lt;/h4&gt;基于align-and-fuse策略的方法为跨多个光谱带进行深度估计提供了一种有效的解决方案，并且能够提高预测结果的可靠性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deploying depth estimation networks in the real world requires high-levelrobustness against various adverse conditions to ensure safe and reliableautonomy. For this purpose, many autonomous vehicles employ multi-modal sensorsystems, including an RGB camera, NIR camera, thermal camera, LiDAR, or Radar.They mainly adopt two strategies to use multiple sensors: modality-wise andmulti-modal fused inference. The former method is flexible butmemory-inefficient, unreliable, and vulnerable. Multi-modal fusion can providehigh-level reliability, yet it needs a specialized architecture. In this paper,we propose an effective solution, named align-and-fuse strategy, for the depthestimation from multi-spectral images. In the align stage, we align embeddingspaces between multiple spectrum bands to learn shareable representation acrossmulti-spectral images by minimizing contrastive loss of global and spatiallyaligned local features with geometry cue. After that, in the fuse stage, wetrain an attachable feature fusion module that can selectively aggregate themulti-spectral features for reliable and robust prediction results. Based onthe proposed method, a single-depth network can achieve both spectral-invariantand multi-spectral fused depth estimation while preserving reliability, memoryefficiency, and flexibility.</description>
      <author>example@mail.com (Ukcheol Shin, Kyunghyun Lee, Jean Oh)</author>
      <guid isPermaLink="false">2503.00793v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>CrystalFramer: Rethinking the Role of Frames for SE(3)-Invariant Crystal Structure Modeling</title>
      <link>http://arxiv.org/abs/2503.02209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 main pages, 3 main figures, and 4 main tables. Published as a  conference paper at ICLR 2025. This version moves some appendices into the  main text. For more information, see  https://omron-sinicx.github.io/crystalframer/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;晶格结构建模在材料信息学的各种应用中至关重要，并且捕捉SE(3)不变几何特征是这些网络的基本需求。&lt;h4&gt;问题&lt;/h4&gt;确定晶体结构的框架具有挑战性，因为它们是无限和高度对称的，这与分子不同。&lt;h4&gt;传统方法限制&lt;/h4&gt;现有的方法依赖于每个结构由其自身的结构性信息静态固定的一个框架来实现。&lt;h4&gt;新概念&lt;/h4&gt;提出了动态框架的概念，这些框架在考虑到晶体无限性和对称性的前提下，为每一个原子提供一个关注局部环境中活动交互原子的动态视图。&lt;h4&gt;应用技术&lt;/h4&gt;通过利用最近基于变压器的晶格编码器中的注意力机制，提出了一种称为CrystalFramer的新架构来实现这一概念。&lt;h4&gt;实验结果&lt;/h4&gt;广泛的实验证明了与传统框架和现有晶格编码器相比，CrystalFramer在各种晶体性质预测任务中表现更佳。&lt;h4&gt;贡献&lt;/h4&gt;该研究质疑了结构对齐坐标系统的简单性，并引入了一个新的、动态的视角来理解和建模复杂的晶体几何特征。&lt;h4&gt;翻译&lt;/h4&gt;使用图神经网络进行晶格结构建模对于材料信息学中的许多应用至关重要，捕捉SE(3)不变的几何特性是这些网络的基本需求。一种直接的方法是通过结构对齐坐标系统或“框架”建模定向标准化的结构。然而，与分子不同，确定晶体结构的框架具有挑战性，因为它们具有无限和高度对称的性质。特别是，现有方法依赖于由每个结构自身的结构性信息静态固定的一个框架，而不考虑任务本身的需求。这里，我们重新思考了“框架”的角色，质疑这种仅基于结构简单化的对齐是否足够，并提出了动态框架的概念。这些框架在考虑到晶体的无限性和对称性的前提下，为每一个原子提供一个关注局部环境中活动交互原子的动态视图。通过利用最近基于变压器的晶格编码器中的注意力机制，我们实现了这一概念，提出了一种新的架构叫作CrystalFramer。广泛的实验证明了与传统框架和现有晶格编码器相比，CrystalFramer在各种晶体性质预测任务中表现更佳。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crystal structure modeling with graph neural networks is essential forvarious applications in materials informatics, and capturing SE(3)-invariantgeometric features is a fundamental requirement for these networks. Astraightforward approach is to model with orientation-standardized structuresthrough structure-aligned coordinate systems, or"frames." However, unlikemolecules, determining frames for crystal structures is challenging due totheir infinite and highly symmetric nature. In particular, existing methodsrely on a statically fixed frame for each structure, determined solely by itsstructural information, regardless of the task under consideration. Here, werethink the role of frames, questioning whether such simplistic alignment withthe structure is sufficient, and propose the concept of dynamic frames. Whileaccommodating the infinite and symmetric nature of crystals, these framesprovide each atom with a dynamic view of its local environment, focusing onactively interacting atoms. We demonstrate this concept by utilizing theattention mechanism in a recent transformer-based crystal encoder, resulting ina new architecture called CrystalFramer. Extensive experiments show thatCrystalFramer outperforms conventional frames and existing crystal encoders invarious crystal property prediction tasks.</description>
      <author>example@mail.com (Yusei Ito, Tatsunori Taniai, Ryo Igarashi, Yoshitaka Ushiku, Kanta Ono)</author>
      <guid isPermaLink="false">2503.02209v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Projection Head is Secretly an Information Bottleneck</title>
      <link>http://arxiv.org/abs/2503.00507v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文深入探讨了对比学习中投影头的作用机制，并从信息理论的角度提供了理论上的理解，提出了一种新的方法来改进投影器的设计。&lt;h4&gt;背景&lt;/h4&gt;最近，对比学习作为一种提取有意义数据表示的方法越来越受到重视。训练时在编码器顶部添加一个投影头并在下游任务中移除该投影头已被证明能显著提升对比学习的效果。&lt;h4&gt;目的&lt;/h4&gt;研究和理解投影头背后的机制，并提出改进方法以提高对比学习的性能。&lt;h4&gt;方法&lt;/h4&gt;从信息理论的角度出发，建立了对投影之前特征的下游表现的理论保证。基于这些理论见解，引入了带有训练和结构正则化的投影器修改方案。&lt;h4&gt;主要发现&lt;/h4&gt;有效的投影头应该充当一个信息瓶颈，过滤掉与对比目标无关的信息。&lt;h4&gt;结论&lt;/h4&gt;在CIFAR-10、CIFAR-100和ImageNet-100等多个实际数据集上进行实验验证后，所提出的方法表现出一致的性能提升。研究者认为其对投影头作用机制的理解将为这一领域带来更加原则化与先进的设计。&lt;h4&gt;翻译&lt;/h4&gt;最近，对比学习作为一种提取有意义数据表示的方法越来越受到重视。在各种特殊设计中，训练时在编码器顶部添加一个投影头并在下游任务中移除该投影头已被证明能显著提升对比学习的效果。然而尽管这种方法在经验上取得成功，其背后的机制却尚未得到充分研究。本文从信息理论的角度深入探讨了投影头的作用，并建立了对特征在投影之前进行下游表现的理论保证，揭示了一个有效的投影器应该充当一个信息瓶颈，过滤掉与对比目标无关的信息。基于这些理论见解，我们引入了带有训练和结构正则化的投影器修改方案。经验上，在包括CIFAR-10、CIFAR-100和ImageNet-100在内的多个真实数据集上的下游性能表现出一致的提升。我们认为对投影头作用机制的理解将为这一领域带来更加原则化与先进的设计。代码可在https://github.com/PKU-ML/Projector_Theory获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, contrastive learning has risen to be a promising paradigm forextracting meaningful data representations. Among various special designs,adding a projection head on top of the encoder during training and removing itfor downstream tasks has proven to significantly enhance the performance ofcontrastive learning. However, despite its empirical success, the underlyingmechanism of the projection head remains under-explored. In this paper, wedevelop an in-depth theoretical understanding of the projection head from theinformation-theoretic perspective. By establishing the theoretical guaranteeson the downstream performance of the features before the projector, we revealthat an effective projector should act as an information bottleneck, filteringout the information irrelevant to the contrastive objective. Based ontheoretical insights, we introduce modifications to projectors with trainingand structural regularizations. Empirically, our methods exhibit consistentimprovement in the downstream performance across various real-world datasets,including CIFAR-10, CIFAR-100, and ImageNet-100. We believe our theoreticalunderstanding on the role of the projection head will inspire more principledand advanced designs in this field. Code is available athttps://github.com/PKU-ML/Projector_Theory.</description>
      <author>example@mail.com (Zhuo Ouyang, Kaiwen Hu, Qi Zhang, Yifei Wang, Yisen Wang)</author>
      <guid isPermaLink="false">2503.00507v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>African Gender Classification Using Clothing Identification Via Deep Learning</title>
      <link>http://arxiv.org/abs/2503.00058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3 Pages, 10 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了在非理想条件下进行性别分类的新方法，通过分析非洲传统服饰来进行性别识别。&lt;h4&gt;背景&lt;/h4&gt;传统的性别分类技术主要依赖于面部识别，在诸如模糊图像、侧面视角或部分遮挡的情况下效果不佳。&lt;h4&gt;目的&lt;/h4&gt;探索基于服装的性别分类作为一种补充技术的可能性，并特别关注非洲传统服饰的文化特性和性别特定特征。&lt;h4&gt;方法&lt;/h4&gt;使用AFRIFASHION1600数据集，该数据集中有1,600张标记为男性和女性两个类别的非洲传统服饰图像。通过修改后的VGG16架构并采用迁移学习技术开发了一个深度学习模型。为了克服小样本带来的问题以及避免过拟合，使用了数据增强。&lt;h4&gt;主要发现&lt;/h4&gt;所开发的模型在测试集上的准确率为87%，尽管存在女性样本占多数的数据不平衡情况，依然表现出强大的预测能力。&lt;h4&gt;结论&lt;/h4&gt;论文强调了基于服装的性别识别作为面部识别技术补充的可能性，并建议未来的研究应集中在扩展和平衡数据集上，以提高分类稳健性和实际应用性。&lt;h4&gt;翻译&lt;/h4&gt;人类属性识别和分类在计算机视觉中至关重要，推动着创新识别系统的开发。传统性别分类方法主要依赖于面部识别，在诸如图像模糊、侧面视角或部分遮挡的情况下效果不佳。本研究探索了一种替代方法，通过利用服装识别进行性别分类，特别是非洲传统服饰，因为这些服饰具有文化特定和性别的独特特征。我们使用了AFRIFASHION1600数据集，这是一个包含1,600张图片的数据集合，其中的非洲传统服装被标记为男性和女性两类。基于修改后的VGG16架构并通过迁移学习进行训练的一个深度学习模型已被开发出来用于分类。为了应对相对较小的数据集带来的挑战，并减少过拟合的风险，我们采用了数据增强技术。尽管存在样本不平衡的问题（特别是更多的女性样本），我们的模型在测试集中达到了87%的准确性，显示出强大的预测能力。这些研究结果突显了基于服装识别作为面部识别补充的可能性，在非洲文化背景下用于性别分类。未来的研究所应关注的方向是扩大和平衡数据集，以增强分类的稳健性和提高服装基础性别的识别系统的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human attribute identification and classification are crucial in computervision, driving the development of innovative recognition systems. Traditionalgender classification methods primarily rely on facial recognition, which,while effective, struggles under non-ideal conditions such as blurriness, sideviews, or partial occlusions. This study explores an alternative approach byleveraging clothing identification, specifically focusing on Africantraditional attire, which carries culturally significant and gender-specificfeatures.  We use the AFRIFASHION1600 dataset, a curated collection of 1,600 images ofAfrican traditional clothing labeled into two gender classes: male and female.A deep learning model, based on a modified VGG16 architecture and trained usingtransfer learning, was developed for classification. Data augmentation wasapplied to address the challenges posed by the relatively small dataset and tomitigate overfitting. The model achieved an accuracy of 87% on the test set,demonstrating strong predictive capability despite dataset imbalances favoringfemale samples.  These findings highlight the potential of clothing-based identification as acomplementary technique to facial recognition for gender classification inAfrican contexts. Future research should focus on expanding and balancingdatasets to enhance classification robustness and improve the applicability ofclothing-based gender recognition systems.</description>
      <author>example@mail.com (Samuel Ozechi)</author>
      <guid isPermaLink="false">2503.00058v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Introspective Loop Closure for SLAM with 4D Imaging Radar</title>
      <link>http://arxiv.org/abs/2503.02383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for publication in the IEEE  International Conference on Robotics and Automation(ICRA), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了在SLAM中使用4D雷达进行循环闭合检测的方法，特别是针对相似和相反视角的情况。&lt;h4&gt;背景&lt;/h4&gt;移动机器人可以在没有外部定位系统或预先存在的地图的情况下通过同时定位与建图（SLAM）技术导航。雷达作为有价值的感觉工具尤其适用于视线被阻挡的环境，因为它受到颗粒的影响比激光雷达或摄像头小得多。现代4D成像雷达提供三维几何信息和相对速度测量。&lt;h4&gt;目的&lt;/h4&gt;研究如何使用4D雷达数据在SLAM中进行循环闭合检测，以提高地图准确性和减少轨迹漂移。&lt;h4&gt;方法&lt;/h4&gt;生成子图来表示更密集的环境，并采用内省度量方法排除特征退化环境中错误检测。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在几何多样的场景下，对于相似和相反视角，循环闭合检测能够保持高精度。轨迹估计可以得到高达82%的改进ATE（绝对轨迹误差），并且能够在自类似环境里拒绝假阳性。&lt;h4&gt;结论&lt;/h4&gt;通过4D雷达在SLAM中的应用，提高了移动机器人在复杂环境下的导航能力，特别是在视线受限的情况中表现出了优越性。&lt;h4&gt;翻译&lt;/h4&gt;Simultaneous Localization and Mapping (SLAM) enables mobile robots to navigate without external positioning systems or pre-existing maps. Radar is emerging as a valuable sensing tool, especially in vision-obstructed environments, as it is less affected by particles than lidars or cameras. Modern 4D imaging radars provide three-dimensional geometric information and relative velocity measurements, but they bring challenges such as small field of view and sparse, noisy point clouds. Detecting loop closures in SLAM is critical for reducing trajectory drift and maintaining map accuracy. However, the directional nature of 4D radar data makes identifying loop closures, especially from reverse viewpoints, difficult due to limited scan overlap. This article explores using 4D radar for loop closure in SLAM, focusing on similar and opposing viewpoints. We generate submaps for a denser environment representation and use introspective measures to reject false detections in feature-degenerate environments. Our experiments show accurate loop closure detection in geometrically diverse settings for both similar and opposing viewpoints, improving trajectory estimation with up to 82% improvement in ATE and rejecting false positives in self-similar environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simultaneous Localization and Mapping (SLAM) allows mobile robots to navigatewithout external positioning systems or pre-existing maps. Radar is emerging asa valuable sensing tool, especially in vision-obstructed environments, as it isless affected by particles than lidars or cameras. Modern 4D imaging radarsprovide three-dimensional geometric information and relative velocitymeasurements, but they bring challenges, such as a small field of view andsparse, noisy point clouds. Detecting loop closures in SLAM is critical forreducing trajectory drift and maintaining map accuracy. However, thedirectional nature of 4D radar data makes identifying loop closures, especiallyfrom reverse viewpoints, difficult due to limited scan overlap. This articleexplores using 4D radar for loop closure in SLAM, focusing on similar andopposing viewpoints. We generate submaps for a denser environmentrepresentation and use introspective measures to reject false detections infeature-degenerate environments. Our experiments show accurate loop closuredetection in geometrically diverse settings for both similar and opposingviewpoints, improving trajectory estimation with up to 82 % improvement in ATEand rejecting false positives in self-similar environments.</description>
      <author>example@mail.com (Maximilian Hilger, Vladimír Kubelka, Daniel Adolfsson, Ralf Becker, Henrik Andreasson, Achim J. Lilienthal)</author>
      <guid isPermaLink="false">2503.02383v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Fairness and/or Privacy on Social Graphs</title>
      <link>http://arxiv.org/abs/2503.02114v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了图神经网络（GNNs）在公平性和隐私方面的挑战，并通过实验分析了不同保护措施对模型性能的影响。&lt;h4&gt;背景&lt;/h4&gt;研究表明，GNNs在处理图形数据方面取得了显著成功，但也引发了关于其公平性与隐私问题的关注。这些问题包括可能的偏见和歧视风险以及敏感信息的安全隐患。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在全面调查GNN中的公平性和隐私问题，并探索不同保护措施对模型性能的影响。&lt;h4&gt;方法&lt;/h4&gt;通过在多种数据集上进行实验，评估不同的公平性干预措施的有效性。分析了公平性、隐私和准确性之间的权衡。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，在特定的数据特性和期望的公平目标的基础上精心选择和组合公平性保护措施的重要性。&lt;h4&gt;结论&lt;/h4&gt;该研究对GNN中复杂互相关系的深入理解做出了贡献，并为开发更强大且道德规范的图学习模型铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的文本&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown remarkable success in variousgraph-based learning tasks. However, recent studies have raised concerns aboutfairness and privacy issues in GNNs, highlighting the potential for biased ordiscriminatory outcomes and the vulnerability of sensitive information. Thispaper presents a comprehensive investigation of fairness and privacy in GNNs,exploring the impact of various fairness-preserving measures on modelperformance. We conduct experiments across diverse datasets and evaluate theeffectiveness of different fairness interventions. Our analysis considers thetrade-offs between fairness, privacy, and accuracy, providing insights into thechallenges and opportunities in achieving both fair and private graph learning.The results highlight the importance of carefully selecting and combiningfairness-preserving measures based on the specific characteristics of the dataand the desired fairness objectives. This study contributes to a deeperunderstanding of the complex interplay between fairness, privacy, and accuracyin GNNs, paving the way for the development of more robust and ethical graphlearning models.</description>
      <author>example@mail.com (Bartlomiej Surma, Michael Backes, Yang Zhang)</author>
      <guid isPermaLink="false">2503.02114v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Depth-Adaptive Graph Neural Networks via Learnable Bakry-'Emery Curvature</title>
      <link>http://arxiv.org/abs/2503.01079v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种新的图神经网络(GNN)框架，该框架通过结合Bakry-Emery曲率来增强信息传播能力，不仅考虑了结构特性还考虑了任务相关性。此方法能够动态调整消息传递层的深度以适应不同节点，并且在大规模图上计算效率高。&lt;h4&gt;背景&lt;/h4&gt;现有的GNN方法主要关注离散图拓扑而忽略了扩散动力学和特定任务依赖性的学习。引入几何属性如曲率可以改进信息流动和复杂连接模式的学习能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的可学习的Bakry-Emery曲率近似策略，以解决现有GNN方法忽视扩散动态和任务特定性的问题，并提高大规模图上的计算效率。&lt;h4&gt;方法&lt;/h4&gt;开发了一种基于顶点曲率自适应调整消息传递层深度的新机制。此外，通过理论分析建立了一个连接曲线度量与特征区分性的桥梁。&lt;h4&gt;主要发现&lt;/h4&gt;高曲率节点需要较少的传播层来达到有效学习，而低曲率节点从更深的传播中受益。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明该方法在各种图学习任务上均表现出优越性。理论和实证分析证明了所提出的框架的有效性和效率。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已经展示了处理基于图形的任务的强大表征学习能力。最近关于GNN的进步通过利用几何属性，例如曲率，来增强其表示能力，从而建模复杂的连接模式和信息流动。然而，大多数现有的方法仅关注离散的图拓扑结构，忽略了扩散动力学以及对有效学习至关重要的任务特定依赖性。为了解决这个问题，我们提出了一种结合Bakry-Emery曲率的方法，该方法捕捉了信息传播中的结构化和驱动因素。我们开发了一种有效的、可学习的近似策略，使大规模图上的曲率计算变得可扩展。此外，我们还引入了一个自适应深度机制，根据每个顶点的曲度动态调整消息传递层的数量，确保高效的信息传递过程。我们的理论分析建立了曲率与特征区分性之间的联系，表明高曲率节点需要较少的层数进行传播，而低曲率节点可以从更深的传播中受益。在基准数据集上的广泛实验验证了我们方法的有效性，并显示出在各种图学习任务中的性能改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated strong representation learningcapabilities for graph-based tasks. Recent advances on GNNs leverage geometricproperties, such as curvature, to enhance its representation capabilities bymodeling complex connectivity patterns and information flow within graphs.However, most existing approaches focus solely on discrete graph topology,overlooking diffusion dynamics and task-specific dependencies essential foreffective learning. To address this, we propose integrating Bakry-\'Emerycurvature, which captures both structural and task-driven aspects ofinformation propagation. We develop an efficient, learnable approximationstrategy, making curvature computation scalable for large graphs. Furthermore,we introduce an adaptive depth mechanism that dynamically adjustsmessage-passing layers per vertex based on its curvature, ensuring efficientpropagation. Our theoretical analysis establishes a link between curvature andfeature distinctiveness, showing that high-curvature vertices require fewerlayers, while low-curvature ones benefit from deeper propagation. Extensiveexperiments on benchmark datasets validate the effectiveness of our approach,showing consistent performance improvements across diverse graph learningtasks.</description>
      <author>example@mail.com (Asela Hevapathige, Ahad N. Zehmakan, Qing Wang)</author>
      <guid isPermaLink="false">2503.01079v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>BGM2Pose: Active 3D Human Pose Estimation with Non-Stationary Sounds</title>
      <link>http://arxiv.org/abs/2503.00389v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为BGM2Pose的方法，用于从任意音乐中提取3D人体姿态，该方法克服了现有技术中需要侵入性信号的限制。&lt;h4&gt;背景&lt;/h4&gt;现有的姿势估计方法往往依赖于特定频率范围内的声波信号，这些信号可能对人类造成不适。而日常生活中的普通音乐则提供了一种自然、无害的声音源。&lt;h4&gt;目的&lt;/h4&gt;开发一种从标准音乐中准确提取3D人体姿态的方法，以克服现有技术的局限性，并提高实际应用的可能性。&lt;h4&gt;方法&lt;/h4&gt;BGM2Pose引入了对比姿势提取模块和频率注意力模块。前者利用对比学习与硬负样本采样来去除记录数据中的音乐成分；后者则通过动态计算频带间的注意权重，使模型能够关注到由人体动作引起的微妙声学变化。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，BGM2Pose方法在性能上优于现有的技术，并展示了其潜在的实际应用价值。&lt;h4&gt;结论&lt;/h4&gt;研究证明了从背景音乐中提取3D姿态信息的可行性与优越性，并计划公开发布数据集和代码以供进一步的研究利用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为BGM2Pose的方法，用于通过任意音乐作为主动传感信号来进行非侵入性的3D人体姿态估计。该方法克服了现有技术中需要侵入性声音信号的限制，这些声音信号可能对人类造成不适。从标准音乐中提取姿势信息带来了许多挑战：与专为测量设计的声音源不同，普通音乐在音量和音调上会动态变化，导致其声学特性难以与人体运动产生的改变区分开来。BGM2Pose通过引入对比姿势提取模块和频率注意力模块解决了这些难题，并展示了优于现有方法的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose BGM2Pose, a non-invasive 3D human pose estimation method usingarbitrary music (e.g., background music) as active sensing signals. Unlikeexisting approaches that significantly limit practicality by employingintrusive chirp signals within the audible range, our method utilizes naturalmusic that causes minimal discomfort to humans. Estimating human poses fromstandard music presents significant challenges. In contrast to sound sourcesspecifically designed for measurement, regular music varies in both volume andpitch. These dynamic changes in signals caused by music are inevitably mixedwith alterations in the sound field resulting from human motion, making it hardto extract reliable cues for pose estimation. To address these challenges,BGM2Pose introduces a Contrastive Pose Extraction Module that employscontrastive learning and hard negative sampling to eliminate musical componentsfrom the recorded data, isolating the pose information. Additionally, wepropose a Frequency-wise Attention Module that enables the model to focus onsubtle acoustic variations attributable to human movement by dynamicallycomputing attention across frequency bands. Experiments suggest that our methodoutperforms the existing methods, demonstrating substantial potential forreal-world applications. Our datasets and code will be made publicly available.</description>
      <author>example@mail.com (Yuto Shibata, Yusuke Oumi, Go Irie, Akisato Kimura, Yoshimitsu Aoki, Mariko Isogawa)</author>
      <guid isPermaLink="false">2503.00389v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Prior Distillation with Vision Foundation Model for Enhanced Rapid Bone Scintigraphy Image Restoration</title>
      <link>http://arxiv.org/abs/2503.02321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于SAM模型的语义先验用于儿科快速骨闪烁显像图像恢复的新方法。&lt;h4&gt;背景&lt;/h4&gt;快速骨闪烁显像是诊断儿童骨骼疾病和肿瘤转移的重要工具，因为它减少了扫描时间并减轻了患者的不适。然而，快速扫描经常导致图像质量下降，影响诊断准确性。&lt;h4&gt;目的&lt;/h4&gt;为了提高快速骨闪烁显像的图像质量，我们提出了一个利用SAM模型语义先验来增强儿科人群中的快速骨闪烁显像的方法。&lt;h4&gt;方法&lt;/h4&gt;该方法包括两个级联网络$f^{IR1}$和$f^{IR2}$，并有三个关键模块：语义先验集成(SPI)模块、语义知识蒸馏(SKD)模块以及语义一致性(SCM)模块。这些模块利用了从经过微调的SAM中提取的专业领域特定的语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在公共内窥镜数据集和新发布的RBS数据集中，我们的方法在各种评价指标（如PSNR、SSIM、FID和LPIPS）上都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;通过结合SAM模型的语义先验信息，该方法能够显著提高快速骨闪烁显像图像的质量，有助于更准确地诊断儿童骨骼疾病。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了利用基于Segment Anything Model (SAM) 的方法来改进儿科患者快速骨闪烁显像质量的研究。通过引入专门设计的模块和使用特定数据集进行验证，该研究展示了在提升医学影像重建技术方面的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid bone scintigraphy is an essential tool for diagnosing skeletal diseasesand tumor metastasis in pediatric patients, as it reduces scan time andminimizes patient discomfort. However, rapid scans often result in poor imagequality, potentially affecting diagnosis due to reduced resolution and detail,which make it challenging to identify and evaluate finer anatomical structures.To address this issue, we propose the first application of SAM-based semanticpriors for medical image restoration, leveraging the Segment Anything Model(SAM) to enhance rapid bone scintigraphy images in pediatric populations. Ourmethod comprises two cascaded networks, $f^{IR1}$ and $f^{IR2}$, augmented bythree key modules: a Semantic Prior Integration (SPI) module, a SemanticKnowledge Distillation (SKD) module, and a Semantic Consistency Module (SCM).The SPI and SKD modules incorporate domain-specific semantic information from afine-tuned SAM, while the SCM maintains consistent semantic featurerepresentation throughout the cascaded networks. In addition, we will release anovel Rapid Bone Scintigraphy dataset called RBS, the first dataset dedicatedto rapid bone scintigraphy image restoration in pediatric patients. RBSconsists of 137 pediatric patients aged between 0.5 and 16 years who underwentboth standard and rapid bone scans. The dataset includes scans performed at 20cm/min (standard) and 40 cm/min (rapid), representing a $2\times$ acceleration.We conducted extensive experiments on both the publicly available endoscopicdataset and RBS. The results demonstrate that our method outperforms allexisting methods across various metrics, including PSNR, SSIM, FID, and LPIPS.</description>
      <author>example@mail.com (Pengchen Liang, Leijun Shi, Huiping Yao, Bin Pu, Jianguo Chen, Lei Zhao, Haishan Huang, Zhuangzhuang Chen, Zhaozhao Xu, Lite Xu, Qing Chang, Yiwei Li)</author>
      <guid isPermaLink="false">2503.02321v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>mmDEAR: mmWave Point Cloud Density Enhancement for Accurate Human Body Reconstruction</title>
      <link>http://arxiv.org/abs/2503.02375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种针对毫米波雷达点云增强和人体重建的两阶段深度学习框架。&lt;h4&gt;背景&lt;/h4&gt;毫米波雷达因其隐私友好性和非侵入性而成为人体重建的理想选择，但其稀疏的点云限制了估计准确性。&lt;h4&gt;目的&lt;/h4&gt;通过引入深度学习方法来提高毫米波雷达点云的质量并提升人体重建精度。&lt;h4&gt;方法&lt;/h4&gt;{'mmWave点云增强模块': '利用时间特征密集化原始数据，并从单视图图像中的2D人类掩模中学习详细的形状和姿态信息。在训练阶段使用基于图像的监督，但在推理时仅依赖于稀疏的点云以保护隐私。', '多阶段完成网络': '进一步优化重建结果。', '2D-3D融合模块': '提取二维和三维运动特征来精炼SMPL参数'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该方法在多个数据集上优于现有技术，并且增强后的点云能进一步提升集成到现有模型中的性能。&lt;h4&gt;结论&lt;/h4&gt;通过利用深度学习框架有效解决了毫米波雷达点云稀疏的问题，为人体重建提供了一种新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millimeter-wave (mmWave) radar offers robust sensing capabilities in diverseenvironments, making it a highly promising solution for human bodyreconstruction due to its privacy-friendly and non-intrusive nature. However,the significant sparsity of mmWave point clouds limits the estimation accuracy.To overcome this challenge, we propose a two-stage deep learning framework thatenhances mmWave point clouds and improves human body reconstruction accuracy.Our method includes a mmWave point cloud enhancement module that densifies theraw data by leveraging temporal features and a multi-stage completion network,followed by a 2D-3D fusion module that extracts both 2D and 3D motion featuresto refine SMPL parameters. The mmWave point cloud enhancement module learns thedetailed shape and posture information from 2D human masks in single-viewimages. However, image-based supervision is involved only during the trainingphase, and the inference relies solely on sparse point clouds to maintainprivacy. Experiments on multiple datasets demonstrate that our approachoutperforms state-of-the-art methods, with the enhanced point clouds furtherimproving performance when integrated into existing models.</description>
      <author>example@mail.com (Jiarui Yang, Songpengcheng Xia, Zengyuan Lai, Lan Sun, Qi Wu, Wenxian Yu, Ling Pei)</author>
      <guid isPermaLink="false">2503.02375v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Quantifying Point Contributions: A Lightweight Framework for Efficient and Effective Query-Driven Trajectory Simplification</title>
      <link>http://arxiv.org/abs/2503.02047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by VLDB2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;随着轨迹数据的积累，简化轨迹以减少存储和查询成本的研究越来越受到关注。论文提出了一种基于图神经网络（GNN-TS）和扩散模型（Diff-TS）的新型互学习查询驱动轨迹简化框架MLSim。&lt;h4&gt;背景&lt;/h4&gt;现有的简化方法主要面临三方面的问题：需要大量迭代才能决定删除哪些GPS点；只关注相邻点之间的关系，忽视了整体结构的信息，导致简化后的轨迹与原始轨迹的整体相似性降低；不能区分具有相似特征的点的重要性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的方法来解决现有简化方法中存在的问题，并提高查询精度和减少简化时间。&lt;h4&gt;方法&lt;/h4&gt;MLSim框架包含了两个不同的模型：基于图神经网络（GNN-TS）和基于扩散模型（Diff-TS）。其中，GNN-TS通过关注点的全局性和独特性评估其重要性；而Diff-TS则生成放大的信号以在低压缩率下保留最重要的点。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与八个基准方法相比，在三个数据库上使用MLSim可以将简化时间减少42%--70%，并提高查询准确性最多可达34.6%&lt;h4&gt;结论&lt;/h4&gt;所提出的MLSim框架通过结合图神经网络和扩散模型，能够有效地解决现有轨迹简化方法的问题，并提供更精确的查询结果。&lt;h4&gt;翻译&lt;/h4&gt;随着大量轨迹数据积累起来，为了减少存储成本以及查询成本，对于简化轨迹的研究变得越来越重要。现有的提议面临三个主要问题：首先，它们需要多次迭代来决定删除哪些GPS点；其次，它们只关注相邻点之间的关系（局部信息），而忽视了整体结构的信息（全局信息），这降低了简化的轨迹与原始轨迹的整体相似性，并使在查询结果中保持一致性变得困难，尤其是在基于相似性的查询中。最后，它们未能区分具有类似特征的点的重要性，从而导致选择保留原始轨迹信息的点时效果不佳。我们提出了一种新颖的互学习查询驱动轨迹简化框架MLSim，该框架结合了两个不同的模型：基于图神经网络（GNN-TS）和基于扩散模型（Diff-TS）。GNN-TS根据全局性和独特性评估一个点的重要性，前者捕捉其与整个轨迹的相关性，后者则捕捉其与其他相邻点之间的差异。同时，在GNN层中还包含了注意力机制，使得能够从同一轨迹内的所有点进行数据融合，并优化表示，从而避免了迭代过程。Diff-TS生成放大信号以在低压缩率下保留最重要的点。涉及八种基线方法的实验显示，MLSim可以将简化时间减少42%--70%，并在简化后的轨迹查询准确性上提高最多34.6%&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As large volumes of trajectory data accumulate, simplifying trajectories toreduce storage and querying costs is increasingly studied. Existing proposalsface three main problems. First, they require numerous iterations to decidewhich GPS points to delete. Second, they focus only on the relationshipsbetween neighboring points (local information) while neglecting the overallstructure (global information), reducing the global similarity between thesimplified and original trajectories and making it difficult to maintainconsistency in query results, especially for similarity-based queries. Finally,they fail to differentiate the importance of points with similar features,leading to suboptimal selection of points to retain the original trajectoryinformation.  We propose MLSimp, a novel Mutual Learning query-driven trajectorysimplification framework that integrates two distinct models: GNN-TS, based ongraph neural networks, and Diff-TS, based on diffusion models. GNN-TS evaluatesthe importance of a point according to its globality, capturing its correlationwith the entire trajectory, and its uniqueness, capturing its differences fromneighboring points. It also incorporates attention mechanisms in the GNNlayers, enabling simultaneous data integration from all points within the sametrajectory and refining representations, thus avoiding iterative processes.Diff-TS generates amplified signals to enable the retention of the mostimportant points at low compression rates. Experiments involving eightbaselines on three databases show that MLSimp reduces the simplification timeby 42%--70% and improves query accuracy over simplified trajectories by up to34.6%.</description>
      <author>example@mail.com (Yumeng Song, Yu Gu, Tianyi Li, Yushuai Li, Christian S. Jensen, Ge Yu)</author>
      <guid isPermaLink="false">2503.02047v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning</title>
      <link>http://arxiv.org/abs/2503.00986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as ICLR 2025 conference paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的管道HOD和模型EgoVideo，以改进基于自我视角视频的理解。通过引入一种新颖的方法来生成包含手部与物体动态细节的叙述文本，并使用一个轻量级的运动适配器来捕捉这些细粒度的手部-物体动态信息。&lt;h4&gt;背景&lt;/h4&gt;现有研究主要集中在将视频表示与高层次叙事对齐，而忽略了对手部和物体之间复杂动态的研究。&lt;h4&gt;目的&lt;/h4&gt;旨在整合细粒度手部-物体动力学模型到视频表示学习过程中。&lt;h4&gt;方法&lt;/h4&gt;引入HOD管道利用手部-物体检测器和大型语言模型生成详细的叙述文本；提出EgoVideo模型，其中包含一个新的轻量级运动适配器来捕捉细粒度的手部-物体移动信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过协同训练策略，EgoVideo能够有效地利用HOD数据中的细粒度手部-物体动态。实验显示该方法在多个自我视角下游任务中取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;提出的模型不仅在零样本设置下取得了显著的改进，还展示了对手部-物体交互和机器人操作任务的强大泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在自我视角视频理解领域，手部与物体的动作以及它们之间的互动起着关键作用。然而现有的研究主要集中在将视频表示与高层次叙事对齐，忽略了对手部和物体之间复杂动态的研究。为了解决这个问题，引入了HOD管道，并提出了EgoVideo模型，该模型能够更好地捕捉这些细粒度的手部-物体动态信息。实验结果显示这种方法在多个自我视角下游任务中取得了显著的改进，展示了强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/openrobotlab/egohod&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In egocentric video understanding, the motion of hands and objects as well astheir interactions play a significant role by nature. However, existingegocentric video representation learning methods mainly focus on aligning videorepresentation with high-level narrations, overlooking the intricate dynamicsbetween hands and objects. In this work, we aim to integrate the modeling offine-grained hand-object dynamics into the video representation learningprocess. Since no suitable data is available, we introduce HOD, a novelpipeline employing a hand-object detector and a large language model togenerate high-quality narrations with detailed descriptions of hand-objectdynamics. To learn these fine-grained dynamics, we propose EgoVideo, a modelwith a new lightweight motion adapter to capture fine-grained hand-objectmotion information. Through our co-training strategy, EgoVideo effectively andefficiently leverages the fine-grained hand-object dynamics in the HOD data.Extensive experiments demonstrate that our method achieves state-of-the-artperformance across multiple egocentric downstream tasks, including improvementsof 6.3% in EK-100 multi-instance retrieval, 5.7% in EK-100 classification, and16.3% in EGTEA classification in zero-shot settings. Furthermore, our modelexhibits robust generalization capabilities in hand-object interaction androbot manipulation tasks. Code and data are available athttps://github.com/OpenRobotLab/EgoHOD/.</description>
      <author>example@mail.com (Baoqi Pei, Yifei Huang, Jilan Xu, Guo Chen, Yuping He, Lijin Yang, Yali Wang, Weidi Xie, Yu Qiao, Fei Wu, Limin Wang)</author>
      <guid isPermaLink="false">2503.00986v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Convergence of energy-based learning in linear resistive networks</title>
      <link>http://arxiv.org/abs/2503.00349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文分析了一种基于能量的学习算法——对比学习（Contrastive Learning），并证明在特定网络中，该算法与投影梯度下降等价，从而保证了算法的收敛性。&lt;h4&gt;背景&lt;/h4&gt;基于能量的学习算法作为反向传播的替代方案，在类比电子设备中的分布式实现中表现良好。然而，缺乏严格的理论来证明其收敛性。&lt;h4&gt;目的&lt;/h4&gt;首次探讨对比学习在可调电阻网络上的应用，并提供严格的数学分析以保证该算法的收敛性。&lt;h4&gt;方法&lt;/h4&gt;通过对基于能量的学习算法——对比学习（应用于线性可调整电阻网络）进行详细分析，发现此设置下对比学习等价于任何步长下的凸函数投影梯度下降。&lt;h4&gt;主要发现&lt;/h4&gt;证明了在特定条件下，对比学习算法与投影梯度下降是等价的，并且对于所有步长都保证了收敛性。&lt;h4&gt;结论&lt;/h4&gt;研究为基于能量的学习方法提供了一个坚实的理论基础，特别是在类比电子实现方面，展示了其潜在的应用价值和可靠性的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Energy-based learning algorithms are alternatives to backpropagation and arewell-suited to distributed implementations in analog electronic devices.However, a rigorous theory of convergence is lacking. We make a first step inthis direction by analysing a particular energy-based learning algorithm,Contrastive Learning, applied to a network of linear adjustable resistors. Itis shown that, in this setup, Contrastive Learning is equivalent to projectedgradient descent on a convex function, for any step size, giving a guarantee ofconvergence for the algorithm.</description>
      <author>example@mail.com (Anne-Men Huijzer, Thomas Chaffey, Bart Besselink, Henk J. van Waarde)</author>
      <guid isPermaLink="false">2503.00349v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Token-level Text Image Foundation Model for Document Understanding</title>
      <link>http://arxiv.org/abs/2503.02304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近年来，通用视觉基础模型（VFMs）在多模态大型语言模型中的应用日益增加。然而，在处理包含小而密集文本的图像时，这些模型仍然存在基本预测错误的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的通用视觉基础模型虽然广泛应用于多模态大型语言模型中作为图像编码器，但在缺乏语义精细级监督的情况下，面对含有小而密文字的图像任务（如感知、理解和推理）时，仍会出现关键性的预测误差问题。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，研究团队开发了TokenOCR，这是首个针对文本-图像相关任务设计的基于标记级别的视觉基础模型，能够支持多种传统下游应用。&lt;h4&gt;方法&lt;/h4&gt;为促进TokenOCR的预训练，研究者们构建了一个高质量的数据生成管道，创建了TokenIT数据集，包含2000万张图像和18亿对token-mask。此外，通过利用具有卓越图像作为文本能力的基础模型，他们使用TokenOCR替代先前的VFMs来构造一个文档级多模态大型语言模型(TokenVL)，用于基于VQA（视觉问答）的任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，TokenOCR和TokenVL在多项任务中表现出色。这些成果为未来的研究提供了数据集、代码和权重资源。&lt;h4&gt;结论&lt;/h4&gt;通过开发TokenOCR及TokenVL，研究团队成功地解决了现有视觉基础模型面对含有小而密文字的图像时存在的预测误差问题，并且证明了基于标记级别的视觉基础模型在处理文本-图像相关任务中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;近年来，通用视觉基础模型（VFMs）在多模态大型语言模型中的应用日益增加。然而，在缺乏语义精细级监督的情况下，这些模型在面对含有小而密文字的图像任务时仍会遇到关键性预测误差的问题。为解决此问题，研究团队开发了TokenOCR，一个专为文本-图像相关任务设计的标记级别视觉基础模型，并构建了一个高质量的数据生成管道以创建TokenIT数据集（包含2000万张图像和18亿对token-mask）。此外，他们还通过利用具有卓越图像作为文本能力的基础模型构造了文档级多模态大型语言模型(TokenVL)。实验结果显示，TokenOCR与TokenVL在多项任务中表现出色，并且研究团队将提供数据集、代码和权重资源以供未来的研究使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, general visual foundation models (VFMs) have witnessedincreasing adoption, particularly as image encoders for popular multi-modallarge language models (MLLMs). However, without semantically fine-grainedsupervision, these models still encounter fundamental prediction errors in thecontext of downstream text-image-related tasks, i.e., perception, understandingand reasoning with images containing small and dense texts. To bridge this gap,we develop TokenOCR, the first token-level visual foundation model specificallytailored for text-image-related tasks, designed to support a variety oftraditional downstream applications. To facilitate the pretraining of TokenOCR,we also devise a high-quality data production pipeline that constructs thefirst token-level image text dataset, TokenIT, comprising 20 million images and1.8 billion token-mask pairs. Furthermore, leveraging this foundation withexceptional image-as-text capability, we seamlessly replace previous VFMs withTokenOCR to construct a document-level MLLM, TokenVL, for VQA-based documentunderstanding tasks. Finally, extensive experiments demonstrate theeffectiveness of TokenOCR and TokenVL. Code, datasets, and weights will beavailable at https://token-family.github.io/TokenOCR_project.</description>
      <author>example@mail.com (Tongkun Guan, Zining Wang, Pei Fu, Zhengtao Guo, Wei Shen, Kai Zhou, Tiezhu Yue, Chen Duan, Hao Sun, Qianyi Jiang, Junfeng Luo, Xiaokang Yang)</author>
      <guid isPermaLink="false">2503.02304v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Label-Efficient LiDAR Panoptic Segmentation</title>
      <link>http://arxiv.org/abs/2503.02372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对LiDAR全景分割的低标签需求的方法L3PS，该方法通过利用2D网络生成伪标签并结合新颖的3D细化模块来提高点云数据中的语义和实例分割性能。&lt;h4&gt;背景&lt;/h4&gt;学习型机器人场景理解技术在处理复杂高维点云数据时面临训练数据标注量大的问题，这限制了其泛化能力。特别是LiDAR全景分割，需要同时进行语义和实例分割，对训练样本的需求更大。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决使用少量标记样本的LiDAR全景分割任务，并减轻大规模数据标注的工作负担。&lt;h4&gt;方法&lt;/h4&gt;1. 利用标签高效2D网络从少量注释图像中生成全景伪标签。           2. 将这些伪标签投影到点云上进行3D细化，通过聚类技术、顺序扫描累积和地面点分离来增强标签准确性。&lt;h4&gt;主要发现&lt;/h4&gt;引入的3D细化模块能够显著提高伪标签的质量，在PQ指标上的改进可达+10.6，在mIoU指标上有+7.9的提升。这些经过优化后的伪标签可以用来有效训练现成的LiDAR分割网络。&lt;h4&gt;结论&lt;/h4&gt;L3PS方法不仅在性能上超越了现有的方法，还大大降低了标注样本的需求量，从而减轻了人力和时间负担。&lt;h4&gt;翻译&lt;/h4&gt;学习型机器人场景理解技术面临的主要瓶颈是对大量注释训练数据的高度依赖，这通常限制了其泛化能力。在LiDAR全景分割中，由于需要同时处理复杂高维点云数据中的语义和实例分割任务，这一挑战更加突出。为了解决使用少量标记样本的LiDAR全景分割难题，本文借鉴最新的标签高效视觉全景分割技术，提出了一种名为Limited-Label LiDAR Panoptic Segmentation (L3PS)的新方法，仅需极少量标注数据即可工作。该方法首先利用一个高效的2D网络从少数注释图像中生成伪标签，并将其投影到点云上。然后引入了一个新颖的3D细化模块，充分利用了点云的几何特性，通过聚类技术、顺序扫描累积和地面点分离来显著提升伪标签的精度，从而提高了分割质量（PQ指标增加+10.6，mIoU指标增加+7.9）。实验表明，这些优化后的伪标签可以有效地训练现成的LiDAR分割网络。通过广泛的试验验证了L3PS不仅在性能上超过了现有方法，还大大减少了标注负担。该工作的代码已发布于https://l3ps.cs.uni-freiburg.de。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A main bottleneck of learning-based robotic scene understanding methods isthe heavy reliance on extensive annotated training data, which often limitstheir generalization ability. In LiDAR panoptic segmentation, this challengebecomes even more pronounced due to the need to simultaneously address bothsemantic and instance segmentation from complex, high-dimensional point clouddata. In this work, we address the challenge of LiDAR panoptic segmentationwith very few labeled samples by leveraging recent advances in label-efficientvision panoptic segmentation. To this end, we propose a novel method,Limited-Label LiDAR Panoptic Segmentation (L3PS), which requires only a minimalamount of labeled data. Our approach first utilizes a label-efficient 2Dnetwork to generate panoptic pseudo-labels from a small set of annotatedimages, which are subsequently projected onto point clouds. We then introduce anovel 3D refinement module that capitalizes on the geometric properties ofpoint clouds. By incorporating clustering techniques, sequential scanaccumulation, and ground point separation, this module significantly enhancesthe accuracy of the pseudo-labels, improving segmentation quality by up to+10.6 PQ and +7.9 mIoU. We demonstrate that these refined pseudo-labels can beused to effectively train off-the-shelf LiDAR segmentation networks. Throughextensive experiments, we show that L3PS not only outperforms existing methodsbut also substantially reduces the annotation burden. We release the code ofour work at https://l3ps.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Ahmet Selim Çanakçı, Niclas Vödisch, Kürsat Petek, Wolfram Burgard, Abhinav Valada)</author>
      <guid isPermaLink="false">2503.02372v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Learning Exposure Mapping Functions for Inferring Heterogeneous Peer Effects</title>
      <link>http://arxiv.org/abs/2503.01722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个新的基于图神经网络的方法EgoNetGNN，用于自动学习适合复杂同行影响机制的暴露映射函数。&lt;h4&gt;背景&lt;/h4&gt;在因果推断中，干扰是指个体在网络中的行为受到同伴行动的影响。通常研究人员定义一个聚集同伴治疗并输出同行暴露的映射函数来估计同行效应。现有的方法主要基于同伴数量或比例来决定暴露映射函数。&lt;h4&gt;目的&lt;/h4&gt;为了更准确地估计异质性同行效应（不同情境下相同同行暴露下的反事实结果的变化），该研究旨在开发一种能够自动学习合适暴露映射函数的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种图神经网络(EgoNetGNN) 方法，它可以考虑到局部邻居结构和边属性等因素来处理复杂的同行影响机制。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，基于数量或比例的同伴暴露模型或者简单地学习同伴暴露的模型难以应对复杂的影响机制。而该方法在合成数据及半合成网络数据上估计异质性同行效应时比现有最佳基准更稳健。&lt;h4&gt;结论&lt;/h4&gt;EgoNetGNN能够更好地处理复杂的同行影响，提供了一个自动学习适当暴露映射函数的新视角，有助于更加准确地评估同行效应。&lt;h4&gt;翻译&lt;/h4&gt;在因果推理中，干扰指的是网络中的个体行为受到同伴行动的影响。同行效应指的是一个人在不同同伴暴露水平下的反事实结果差异，即一个人受同伴治疗、行动或行为影响的程度。估计同行效应需要决定如何表示同伴暴露。通常研究人员定义一个聚集同伴治疗并输出同行暴露的映射函数来决定这一点。现有的大多数方法都假设了基于同伴数量或者比例的同行暴露。最近的研究探讨了更复杂的同行暴露功能，以捕捉不同同伴施加的不同程度的影响。然而，这些研究没有明确考虑自动学习暴露映射函数的问题。在这项工作中，我们专注于为估计异质性同行效应开发这样的函数，其中异质性指的是在相同的同伴暴露下但不同的个人背景下的反事实结果的变化。我们提出了EgoNetGNN, 一种基于图神经网络的方法，用于自动学习适当的暴露映射函数以处理复杂的同行影响机制，这些机制不仅涉及到同伴治疗，还包括局部邻居结构和边属性。我们的综合评估表明，在估计异质性同行效应时，与最先进的基准相比，该方法在面对不同的未知潜在影响机制时表现得更稳健。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In causal inference, interference refers to the phenomenon in which theactions of peers in a network can influence an individual's outcome. Peereffect refers to the difference in counterfactual outcomes of an individual fordifferent levels of peer exposure, the extent to which an individual is exposedto the treatments, actions, or behaviors of peers. Estimating peer effectsrequires deciding how to represent peer exposure. Typically, researchers definean exposure mapping function that aggregates peer treatments and outputs peerexposure. Most existing approaches for defining exposure mapping functionsassume peer exposure based on the number or fraction of treated peers. Recentstudies have investigated more complex functions of peer exposure which capturethat different peers can exert different degrees of influence. However, none ofthese works have explicitly considered the problem of automatically learningthe exposure mapping function. In this work, we focus on learning this functionfor the purpose of estimating heterogeneous peer effects, where heterogeneityrefers to the variation in counterfactual outcomes for the same peer exposurebut different individual's contexts. We develop EgoNetGNN, a graph neuralnetwork (GNN)-based method, to automatically learn the appropriate exposuremapping function allowing for complex peer influence mechanisms that, inaddition to peer treatments, can involve the local neighborhood structure andedge attributes. We show that GNN models that use peer exposure based on thenumber or fraction of treated peers or learn peer exposure naively facedifficulty accounting for such influence mechanisms. Our comprehensiveevaluation on synthetic and semi-synthetic network data shows that our methodis more robust to different unknown underlying influence mechanisms whenestimating heterogeneous peer effects when compared to state-of-the-artbaselines.</description>
      <author>example@mail.com (Shishir Adhikari, Sourav Medya, Elena Zheleva)</author>
      <guid isPermaLink="false">2503.01722v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Layered Insights: Generalizable Analysis of Authorial Style by Leveraging All Transformer Layers</title>
      <link>http://arxiv.org/abs/2503.00958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的作者归属任务的方法，利用了预训练的基于Transformer模型的不同层次中学习到的各种语言表示。&lt;h4&gt;背景&lt;/h4&gt;现有的作者归属方法可能在跨域数据集上的表现不佳，这限制了它们的实际应用范围和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;通过评估不同层次的Transformer语言模型在内部和外部领域的效果，来提高作者归属任务的准确性和稳健性。&lt;h4&gt;方法&lt;/h4&gt;利用预训练的基于Transformer的模型的不同层中学习到的语言表示进行作者归属，并与最先进的基准在相同领域和跨域场景下进行了对比实验。&lt;h4&gt;主要发现&lt;/h4&gt;使用不同层次的变压器提高了作者归属模型在外域数据上的鲁棒性，从而达到了新的最先进水平。分析表明，每一层都专门化于表示某些有助于外部域测试的风格特征。&lt;h4&gt;结论&lt;/h4&gt;研究结果为更好地理解Transformer模型在不同层级中对文本风格属性的表达提供了一种途径，并为进一步改进作者归属任务奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的方法用于作者归属任务，这种方法利用了预训练的基于Transformer的语言模型的不同层次中学到的各种语言表示。我们在三个数据集上评估了该方法，并将其与最先进的基准进行了比较，在内部领域和跨域场景中进行测试。结果表明，使用不同的Transformer层可以提高在外部领域数据上的鲁棒性，从而产生了新的最先进成果。我们的分析进一步揭示了模型的不同层次如何专门化表示某些风格特征以增强其在外域中的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a new approach for the authorship attribution task that leveragesthe various linguistic representations learned at different layers ofpre-trained transformer-based models. We evaluate our approach on threedatasets, comparing it to a state-of-the-art baseline in in-domain andout-of-domain scenarios. We found that utilizing various transformer layersimproves the robustness of authorship attribution models when tested onout-of-domain data, resulting in new state-of-the-art results. Our analysisgives further insights into how our model's different layers get specialized inrepresenting certain stylistic features that benefit the model when tested outof the domain.</description>
      <author>example@mail.com (Milad Alshomary, Nikhil Reddy Varimalla, Vishal Anand, Kathleen McKeown)</author>
      <guid isPermaLink="false">2503.00958v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Based mmWave Radar Point Cloud Enhancement Driven by Range Images</title>
      <link>http://arxiv.org/abs/2503.02300v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, submitted to 2025 IROS. This work has been  submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;毫米波雷达在机器人和自动驾驶领域引起了广泛关注，但由于生成的点云较为稀疏且含有大量噪声，其进一步发展受到限制。&lt;h4&gt;背景信息&lt;/h4&gt;尽管毫米波雷达在恶劣环境中的感知稳定性很高，但生成的点云相对稀疏并包含显著的噪声。传统的毫米波雷达增强方法难以充分利用扩散模型在超分辨率方面的效果。&lt;h4&gt;研究目的&lt;/h4&gt;提出一种新的方法，将范围图像与图像扩散模型融合，以实现准确且密集的毫米波雷达点云，使其类似于激光雷达（LiDAR）生成的点云。&lt;h4&gt;所用方法&lt;/h4&gt;通过利用范围图像与人类观察一致的投影方式，以及预训练的图像扩散模型的知识迁移，该方法实现了自然图像表示的改进，提高了整体性能。&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集和自建数据集上的大量评估显示，这种方法提供了显著的提升，生成了高质量的三维激光雷达（LiDAR）点云。&lt;h4&gt;结论&lt;/h4&gt;这项研究成功地利用毫米波雷达生成类似激光雷达的三维点云，并确立了一个新的性能标准。&lt;h4&gt;翻译&lt;/h4&gt;毫米波雷达在机器人和自动驾驶领域引起了广泛关注。虽然它在恶劣环境中的感知稳定性较高，但由其产生的点云较为稀疏且含有大量噪声，这限制了它的进一步发展。传统的方法难以利用扩散模型进行超分辨率处理，很大程度上是因为不自然的范围-方位热图（RAH）或鸟瞰视图（BEV）表示方式。为了克服这一局限性，我们提出了一种将范围图像与图像扩散模型融合的新方法，实现了准确且密集的毫米波雷达点云，类似于激光雷达生成的结果。通过与人类观察一致的投影对齐，该方法利用了预训练的图像扩散模型的知识迁移，显著提升了整体性能。在公共数据集和自建数据集上的广泛评估表明，我们的方法提供了显著的改进，并确立了一个新的性能标准，在使用毫米波雷达生成类似激光雷达的三维点云方面建立了新基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millimeter-wave (mmWave) radar has attracted significant attention inrobotics and autonomous driving. However, despite the perception stability inharsh environments, the point cloud generated by mmWave radar is relativelysparse while containing significant noise, which limits its furtherdevelopment. Traditional mmWave radar enhancement approaches often struggle toleverage the effectiveness of diffusion models in super-resolution, largely dueto the unnatural range-azimuth heatmap (RAH) or bird's eye view (BEV)representation. To overcome this limitation, we propose a novel method thatpioneers the application of fusing range images with image diffusion models,achieving accurate and dense mmWave radar point clouds that are similar toLiDAR. Benefitting from the projection that aligns with human observation, therange image representation of mmWave radar is close to natural images, allowingthe knowledge from pre-trained image diffusion models to be effectivelytransferred, significantly improving the overall performance. Extensiveevaluations on both public datasets and self-constructed datasets demonstratethat our approach provides substantial improvements, establishing a newstate-of-the-art performance in generating truly three-dimensional LiDAR-likepoint clouds via mmWave radar.</description>
      <author>example@mail.com (Ruixin Wu, Zihan Li, Jin Wang, Xiangyu Xu, Huan Yu, Zhi Zheng, Kaixiang Huang, Guodong Lu)</author>
      <guid isPermaLink="false">2503.02300v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>GRNFormer: A Biologically-Guided Framework for Integrating Gene Regulatory Networks into RNA Foundation Models</title>
      <link>http://arxiv.org/abs/2503.01682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GRNFormer是一种新的框架，用于将多尺度基因调控网络（GRNs）从多组学数据中系统地整合到RNA基础模型训练中。&lt;h4&gt;背景&lt;/h4&gt;现有的单细胞RNA测序模型虽然显示了捕捉基因表达模式的能力，但它们忽略了生物先验知识，并未能利用多组学信号提供补充的监管见解。&lt;h4&gt;目的&lt;/h4&gt;提出GRNFormer框架，通过引入分层GRNs构建管道和结构感知整合框架来解决这些问题。&lt;h4&gt;方法&lt;/h4&gt;GRNFormer框架包括两个创新点：一是构造细胞类型特异性和细胞特定分辨率下捕获调控关系的分层GRNs；二是设计了一种新颖的边缘扰动策略，使用多头交叉注意力机制动态加权监管关系，并通过引入生物信息共表达链接来增强图神经网络训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在药物反应预测、单细胞药物分类和基因干扰预测任务中，GRNFormer分别提高了3.6%的相关性、9.6%的AUC和1.1%的准确率。&lt;h4&gt;结论&lt;/h4&gt;GRNFormer框架在多种模型架构上均表现出优于现有方法的效果。&lt;h4&gt;翻译&lt;/h4&gt;基础单细胞RNA测序（scRNA-seq）模型已显示出捕捉基因表达模式的能力，但当前的方法忽略了编码在基因调控关系中的生物先验知识，并未能利用多组学信号提供补充的监管见解。本文提出了一种新框架GRNFormer，该框架将从多组学数据中推断出的多尺度基因调控网络（GRNs）系统地整合到RNA基础模型训练中。该框架引入了两个关键创新：构建分层GRNs的管道和一种新的结构感知集成框架。GRNFormer通过综合实验在多个代表性下游任务上展示了其有效性，并优于现有的最佳基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for single-cell RNA sequencing (scRNA-seq) have shownpromising capabilities in capturing gene expression patterns. However, currentapproaches face critical limitations: they ignore biological prior knowledgeencoded in gene regulatory relationships and fail to leverage multi-omicssignals that could provide complementary regulatory insights. In this paper, wepropose GRNFormer, a new framework that systematically integrates multi-scaleGene Regulatory Networks (GRNs) inferred from multi-omics data into RNAfoundation model training. Our framework introduces two key innovations. First,we introduce a pipeline for constructing hierarchical GRNs that captureregulatory relationships at both cell-type-specific and cell-specificresolutions. Second, we design a structure-aware integration framework thataddresses the information asymmetry in GRNs through two technical advances: (1)A graph topological adapter using multi-head cross-attention to weightregulatory relationships dynamically, and (2) a novel edge perturbationstrategy that perturb GRNs with biologically-informed co-expression links toaugment graph neural network training. Comprehensive experiments have beenconducted on three representative downstream tasks across multiple modelarchitectures to demonstrate the effectiveness of GRNFormer. It achievesconsistent improvements over state-of-the-art (SoTA) baselines: $3.6\%$increase in drug response prediction correlation, $9.6\%$ improvement insingle-cell drug classification AUC, and $1.1\%$ average gain in geneperturbation prediction accuracy.</description>
      <author>example@mail.com (Mufan Qiu, Xinyu Hu, Fengwei Zhan, Sukwon Yun, Jie Peng, Ruichen Zhang, Bhavya Kailkhura, Jiekun Yang, Tianlong Chen)</author>
      <guid isPermaLink="false">2503.01682v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Improve Representation for Imbalanced Regression through Geometric Constraints</title>
      <link>http://arxiv.org/abs/2503.00876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025. The first three authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了表示学习中的均匀性概念，特别是对于不平衡回归任务的意义，并提出了一种基于几何原理的损失函数方法。&lt;h4&gt;背景&lt;/h4&gt;在表示学习中，均匀性指的是潜在空间（即单位超球面）中特征分布的一致性。以往的研究表明，提高这种一致性有助于未充分代表类别的学习。&lt;h4&gt;目的&lt;/h4&gt;探讨如何确保不平衡回归任务中的表示空间保持一致性和平滑性的方法，并提出两种关键损失函数来实现这一目标。&lt;h4&gt;方法&lt;/h4&gt;提出了两个几何损失：包容损失和同质性损失。包容损失鼓励特征在超球面上均匀分布，而同质性损失保证了平滑性，并且以相同间隔使表示均匀分布。&lt;h4&gt;主要发现&lt;/h4&gt;通过Surrogate-driven Representation Learning (SRL)框架将这些几何原理整合到数据表示中，实验结果表明对于不平衡回归任务而言，一致性和平滑性的保持非常重要。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法在真实世界中的回归和操作学习任务上验证了其有效性，并强调了几何损失函数在提高不平衡数据集性能方面的关键作用。&lt;h4&gt;翻译&lt;/h4&gt;在表示学习中，一致性指的是潜在空间（即单位超球面）的特征分布均匀性。先前的工作表明，改善这种一致性有助于欠代表类别的学习。然而，大多数以前的研究专注于分类任务；对于不平衡回归而言，表示空间尚未得到探索。基于分类的方法不适合回归任务，因为它们将特性聚集成不连续组群而没有考虑回归所需的重要因素：连续性和顺序排列性。从几何角度来看，本研究独特地关注确保超球面上的特征分布均匀，并通过两种关键损失函数（包容损失和同质性损失）实现这一目标。我们的方法利用代理驱动表示学习框架将这些几何原理整合到数据表示中。在真实世界回归任务和操作学习上的实验验证了这种一致性对于不平衡回归的重要性，以及我们基于几何学的损失函数的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In representation learning, uniformity refers to the uniform featuredistribution in the latent space (i.e., unit hypersphere). Previous work hasshown that improving uniformity contributes to the learning ofunder-represented classes. However, most of the previous work focused onclassification; the representation space of imbalanced regression remainsunexplored. Classification-based methods are not suitable for regression tasksbecause they cluster features into distinct groups without considering thecontinuous and ordered nature essential for regression. In a geometric aspect,we uniquely focus on ensuring uniformity in the latent space for imbalancedregression through two key losses: enveloping and homogeneity. The envelopingloss encourages the induced trace to uniformly occupy the surface of ahypersphere, while the homogeneity loss ensures smoothness, withrepresentations evenly spaced at consistent intervals. Our method integratesthese geometric principles into the data representations via a Surrogate-drivenRepresentation Learning (SRL) framework. Experiments with real-world regressionand operator learning tasks highlight the importance of uniformity inimbalanced regression and validate the efficacy of our geometry-based lossfunctions.</description>
      <author>example@mail.com (Zijian Dong, Yilei Wu, Chongyao Chen, Yingtian Zou, Yichi Zhang, Juan Helen Zhou)</author>
      <guid isPermaLink="false">2503.00876v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Towards Fluorescence-Guided Autonomous Robotic Partial Nephrectomy on Novel Tissue-Mimicking Hydrogel Phantoms</title>
      <link>http://arxiv.org/abs/2503.02265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages. 7 figures. Preprint of an article accepted for publication  in the Journal of Medical Robotics Research, 2025. Copyright World Scientific  Publishing Company [https://worldscientific.com/worldscinet/jmrr]&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种荧光引导的自主机器人系统，能够为外生性肾肿瘤规划和执行切除路径，并通过近红外成像区分健康组织与肿瘤组织。&lt;h4&gt;背景&lt;/h4&gt;自动化的机器人系统有可能提高肾脏肿瘤切除手术的精度及患者的治疗效果。现有的动物模型难以在实验中模拟人类组织的具体行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够用于荧光引导下自主执行肾肿瘤切除任务的机器人系统，并设计出可以模仿人体组织特性的新型水凝胶基体模。&lt;h4&gt;方法&lt;/h4&gt;使用点云观察处理不规则形状的肿瘤；利用近红外成像区分健康与肿瘤组织，采用基于水凝胶的仿生肾脏体模来模拟真实的人类组织特性。&lt;h4&gt;主要发现&lt;/h4&gt;该系统能够在69秒内完成切除任务，并达到了1.44毫米的平均切缘精度。此外，通过将材料与近红外染料混合的方式实现了荧光引导下的肿瘤分割功能。&lt;h4&gt;结论&lt;/h4&gt;新型水凝胶体模和荧光引导机器人系统为肾癌及其他类似手术中的自主机器人技术的发展提供了重要的研究平台和实验工具。&lt;h4&gt;翻译&lt;/h4&gt;自主机器人系统在提高肾脏肿瘤切除的精度及患者治疗效果方面展现出巨大潜力。文中介绍了一种具有规划执行路径能力和临床相关切缘的安全性指标的荧光引导自主机器人系统。该系统利用点云观测处理不规则形状的肿瘤，并通过近红外成像技术区分健康组织与肿瘤组织，模拟靛青绿染色在部分肾切除手术中的作用。鉴于无法获得用于特定类型干预的人体或动物离体组织，仿生肾脏体模对于自主机器人外科系统的开发至关重要。为了克服硅胶基体模限制的问题，研究团队提出了一种新型的水凝胶基础模来模仿人体组织的物理和视觉特性，并兼容电手术设备。与先前的水凝胶体模不同的是，该设计中加入了近红外染料以实现荧光引导下的肿瘤分割功能。通过自主现实世界中的机器人实验验证了系统性能，在69秒内达到了1.44毫米平均切缘精度的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous robotic systems hold potential for improving renal tumor resectionaccuracy and patient outcomes. We present a fluorescence-guided robotic systemcapable of planning and executing incision paths around exophytic renal tumorswith a clinically relevant resection margin. Leveraging point cloudobservations, the system handles irregular tumor shapes and distinguisheshealthy from tumorous tissue based on near-infrared imaging, akin toindocyanine green staining in partial nephrectomy. Tissue-mimicking phantomsare crucial for the development of autonomous robotic surgical systems forinterventions where acquiring ex-vivo animal tissue is infeasible, such ascancer of the kidney and renal pelvis. To this end, we propose novelhydrogel-based kidney phantoms with exophytic tumors that mimic the physicaland visual behavior of tissue, and are compatible with electrosurgicalinstruments, a common limitation of silicone-based phantoms. In contrast toprevious hydrogel phantoms, we mix the material with near-infrared dye toenable fluorescence-guided tumor segmentation. Autonomous real-world roboticexperiments validate our system and phantoms, achieving an average marginaccuracy of 1.44 mm in a completion time of 69 sec.</description>
      <author>example@mail.com (Ethan Kilmer, Joseph Chen, Jiawei Ge, Preksha Sarda, Richard Cha, Kevin Cleary, Lauren Shepard, Ahmed Ezzat Ghazi, Paul Maria Scheikl, Axel Krieger)</author>
      <guid isPermaLink="false">2503.02265v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Spatiotemporal Correlation Anomaly Detection Method Based on Time-Frequency-Domain Feature Fusion and a Dynamic Graph Neural Network in Wireless Sensor Network</title>
      <link>http://arxiv.org/abs/2503.00036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于频域特征和动态图神经网络的无线传感器网络（WSN）异常检测方法，通过设计自编码重构框架有效解决了现有注意力机制变压器在捕捉长期依赖性和计算复杂性方面的局限。&lt;h4&gt;背景&lt;/h4&gt;注意力驱动的转换器在网络中用于无线传感的时间异常检测上发挥了重要作用，但它们存在无法完全可靠地捕获长期依赖、高计算复杂度以及未充分提取时空特征等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的WSN时间序列数据异常检测方法来克服现有模型在捕捉长期依赖性和计算效率上的问题，并更有效地利用多节点WSN时间序列的时空相关性。&lt;h4&gt;方法&lt;/h4&gt;首先，采用离散小波变换有效分解时间序列的趋势和季节成分；其次，设计频域注意力机制充分利用正常数据与异常数据之间幅度分布的区别；最后，通过结合注意机制和图卷积网络（GCN）提出多模态融合动态图卷积网络（MFDGCN），自适应地提取空间相关特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该方法在公共数据集上的精度、召回率以及F1得分分别为93.5%，比现有模型提高了2.9%。&lt;h4&gt;结论&lt;/h4&gt;该异常检测方法通过改进的时间序列分解，频域注意力机制和动态图卷积网络的结合，在提高WSN时间异常检测效果方面取得了显著进步。&lt;h4&gt;翻译&lt;/h4&gt;基于注意的转换器在无线传感器网络（WSN）定时异常检测中发挥了重要作用，因为它们能够捕捉长期依赖性。然而，有几个问题需要解决，例如它们捕获长期依赖性的能力并不完全可靠，计算复杂度水平很高，并且没有充分提取WSN定时数据的空间时间特征来检测多节点WSN定时数据的相关异常。为了解决这些限制，本文提出了一种将频率域特性与动态图神经网络（GNN）集成在设计的自编码重构框架下的WSN异常检测方法。首先，离散小波变换有效地分解了时间序列的趋势和季节成分来解决转换器长期可靠性差的问题。其次，设计了一个频域注意机制来充分利用正常数据和异常数据之间在这个领域的幅度分布差异。最后，通过结合注意力机制和图卷积网络（GCN）设计了一种基于多模态融合的动态图卷积网络（MFDGCN），以自适应地提取空间相关特性。在公共数据集上进行的一系列实验及其结果表明，本文设计的异常检测方法比现有方法具有更高的精度和召回率，F1分数为93.5％，比现有模型提高了2.9％。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Attention-based transformers have played an important role in wireless sensornetwork (WSN) timing anomaly detection due to their ability to capturelong-term dependencies. However, there are several issues that must beaddressed, such as the fact that their ability to capture long-termdependencies is not completely reliable, their computational complexity levelsare high, and the spatiotemporal features of WSN timing data are notsufficiently extracted for detecting the correlation anomalies of multinode WSNtiming data. To address these limitations, this paper proposes a WSN anomalydetection method that integrates frequency-domain features with dynamic graphneural networks (GNN) under a designed self-encoder reconstruction framework.First, the discrete wavelet transform effectively decomposes trend and seasonalcomponents of time series to solve the poor long-term reliability oftransformers. Second, a frequency-domain attention mechanism is designed tomake full use of the difference between the amplitude distributions of normaldata and anomalous data in this domain. Finally, a multimodal fusion-baseddynamic graph convolutional network (MFDGCN) is designed by combining anattention mechanism and a graph convolutional network (GCN) to adaptivelyextract spatial correlation features. A series of experiments conducted onpublic datasets and their results demonstrate that the anomaly detection methoddesigned in this paper exhibits superior precision and recall than the existingmethods do, with an F1 score of 93.5%, representing an improvement of 2.9% overthat of the existing models.</description>
      <author>example@mail.com (Miao Ye, Zhibang Jiang, Xingsi Xue, Xingwang Li, Peng Wen, Yong Wang)</author>
      <guid isPermaLink="false">2503.00036v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OVAMOS: A Framework for Open-Vocabulary Multi-Object Search in Unknown Environments</title>
      <link>http://arxiv.org/abs/2503.02106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种框架，用于解决机器人在室内环境中进行多对象搜索时遇到的挑战。该框架结合了视觉语言模型（VLM）推理、前沿探索和部分可观测马尔可夫决策过程（POMDP），以提高搜索效率并克服观察不确定性的障碍。&lt;h4&gt;背景&lt;/h4&gt;物体搜索是部署在室内环境中的机器人的一项基本任务，但观察不稳定性和开放词汇表模型带来的挑战使得这一任务变得复杂。虽然基础模型可以推断出对象的位置关系，但在遮挡和混乱环境中恢复失败的能力仍然至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一个框架来解决多对象搜索问题，特别是在新环境中寻找多个物体时遇到的困难，并提高机器人在室内环境中的搜索效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一个结合视觉语言模型推理、前沿探索技术和部分可观测马尔可夫决策过程（POMDP）的综合框架。VLM增强了对物体与环境关系的理解，而基于前沿的探索技术帮助导航未知空间；POMDP则模拟了观察不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;在120个仿真的HM3D场景和一个真实的机器人实验中对该框架进行了测试，在效率和成功率方面均优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的结合VLM、前沿探索技术和POMDP的综合框架能够显著提升机器人解决多对象搜索问题的能力，尤其是在应对观察不确定性时表现优越。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object search is a fundamental task for robots deployed in indoor buildingenvironments, yet challenges arise due to observation instability, especiallyfor open-vocabulary models. While foundation models (LLMs/VLMs) enablereasoning about object locations even without direct visibility, the ability torecover from failures and replan remains crucial. The Multi-Object Search (MOS)problem further increases complexity, requiring the tracking multiple objectsand thorough exploration in novel environments, making observation uncertaintya significant obstacle. To address these challenges, we propose a frameworkintegrating VLM-based reasoning, frontier-based exploration, and a PartiallyObservable Markov Decision Process (POMDP) framework to solve the MOS problemin novel environments. VLM enhances search efficiency by inferringobject-environment relationships, frontier-based exploration guides navigationin unknown spaces, and POMDP models observation uncertainty, allowing recoveryfrom failures in occlusion and cluttered environments. We evaluate ourframework on 120 simulated scenarios across several Habitat-Matterport3D (HM3D)scenes and a real-world robot experiment in a 50-square-meter office,demonstrating significant improvements in both efficiency and success rate overbaseline methods.</description>
      <author>example@mail.com (Qianwei Wang, Yifan Xu, Vineet Kamat, Carol Menassa)</author>
      <guid isPermaLink="false">2503.02106v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>InversionGNN: A Dual Path Network for Multi-Property Molecular Optimization</title>
      <link>http://arxiv.org/abs/2503.01488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了InversionGNN框架，一种用于多目标药物发现的高效双路径图神经网络。&lt;h4&gt;背景&lt;/h4&gt;在药物发现中探索化学空间以寻找同时满足多种属性的新分子非常重要。现有方法在平衡这些相互冲突或关联的化学属性时经常遇到挑战。&lt;h4&gt;目的&lt;/h4&gt;提出InversionGNN框架，旨在解决多目标药物发现中的性能权衡问题。&lt;h4&gt;方法&lt;/h4&gt;通过直接预测路径训练模型进行多重性质预测，学习最佳的功能基团组合；然后利用这一知识帮助反转生成路径产生具有所需性质的分子。此外，引入基于梯度的Pareto搜索方法以平衡冲突属性并生成最优分子。&lt;h4&gt;主要发现&lt;/h4&gt;InversionGNN能够在离散化学空间中近似地探索完整的Pareto前沿，并在多目标设置下展示出了高效性和有效性。&lt;h4&gt;结论&lt;/h4&gt;InversionGNN框架为解决药物发现中的多目标问题提供了一种有效且样本高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;探索化学空间以找到同时满足多种性质的新分子对于药物发现至关重要。然而，现有方法往往难以在属性之间进行权衡，因为这些属性是相互冲突或关联的。为了应对这一挑战，我们引入了InversionGNN框架——一种用于多目标药物发现的有效且样本高效的双路径图神经网络（GNN）。在这个模型中，直接预测路径训练模型来进行多重性质预测，以获取功能基团最佳组合的知识；然后，在反转生成路径上，通过所学的化学知识帮助产生具有所需属性的分子。为了在反转路径中解码多属性的复杂信息，我们提出了一种基于梯度的方法来平衡冲突属性并生成最优分子。此外，InversionGNN能够近似地探索离散化学空间中的完整Pareto前沿。全面的实验评估显示，在包括药物发现在内的各种离散多目标设置下，InversionGNN既有效又样本高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Exploring chemical space to find novel molecules that simultaneously satisfymultiple properties is crucial in drug discovery. However, existing methodsoften struggle with trading off multiple properties due to the conflicting orcorrelated nature of chemical properties. To tackle this issue, we introduceInversionGNN framework, an effective yet sample-efficient dual-path graphneural network (GNN) for multi-objective drug discovery. In the directprediction path of InversionGNN, we train the model for multi-propertyprediction to acquire knowledge of the optimal combination of functionalgroups. Then the learned chemical knowledge helps the inversion generation pathto generate molecules with required properties. In order to decode the complexknowledge of multiple properties in the inversion path, we propose agradient-based Pareto search method to balance conflicting properties andgenerate Pareto optimal molecules. Additionally, InversionGNN is able to searchthe full Pareto front approximately in discrete chemical space. Comprehensiveexperimental evaluations show that InversionGNN is both effective andsample-efficient in various discrete multi-objective settings including drugdiscovery.</description>
      <author>example@mail.com (Yifan Niu, Ziqi Gao, Tingyang Xu, Yang Liu, Yatao Bian, Yu Rong, Junzhou Huang, Jia Li)</author>
      <guid isPermaLink="false">2503.01488v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Wavelet-Driven Masked Image Modeling: A Path to Efficient Visual Representation</title>
      <link>http://arxiv.org/abs/2503.00782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于小波变换的Masked Image Modeling (MIM) 方法，通过利用频率域分析来实现紧凑的图像特征表示。该方法解决了传统像素级MIM重建过程中过度关注细节的问题，并提高了训练效率。&lt;h4&gt;背景&lt;/h4&gt;掩膜图像建模(MIM)因其在自监督学习中的出色表现而备受关注，但其基于像素的重建过程会导致不必要的训练时间延长。&lt;h4&gt;目的&lt;/h4&gt;通过引入小波变换来改进MIM方法，以减少冗余信息的影响并加快训练速度。&lt;h4&gt;方法&lt;/h4&gt;使用多层分解技术将图像进行小波变换处理，并利用不同层级的小波系数构建不同频率和尺度的重建目标。然后在MIM过程中整合这些重建目标，同时可调整权重以优先考虑关键信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法在多种下游任务中的性能与现有方法相当或更优，且训练效率更高。&lt;h4&gt;结论&lt;/h4&gt;基于小波变换的方法能够有效地改进MIM模型的训练过程，并提高其处理大规模视觉数据的能力。&lt;h4&gt;翻译&lt;/h4&gt;Masked Image Modeling (MIM)因其出色的自监督学习能力而受到关注。然而，图像中的冗余信息导致像素级重建过于关注细节，从而增加了不必要的训练时间。通过采用小波变换技术对图像进行频率分析，可以实现更紧凑的特征表示和高效的训练过程。实验表明该方法在多种下游任务中表现出色且更具效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked Image Modeling (MIM) has garnered significant attention inself-supervised learning, thanks to its impressive capacity to learn scalablevisual representations tailored for downstream tasks. However, imagesinherently contain abundant redundant information, leading the pixel-based MIMreconstruction process to focus excessively on finer details such as textures,thus prolonging training times unnecessarily. Addressing this challengerequires a shift towards a compact representation of features during MIMreconstruction. Frequency domain analysis provides a promising avenue forachieving compact image feature representation. In contrast to the commonlyused Fourier transform, wavelet transform not only offers frequency informationbut also preserves spatial characteristics and multi-level features of theimage. Additionally, the multi-level decomposition process of wavelettransformation aligns well with the hierarchical architecture of modern neuralnetworks. In this study, we leverage wavelet transform as a tool for efficientrepresentation learning to expedite the training process of MIM. Specifically,we conduct multi-level decomposition of images using wavelet transform,utilizing wavelet coefficients from different levels to construct distinctreconstruction targets representing various frequencies and scales. Thesereconstruction targets are then integrated into the MIM process, withadjustable weights assigned to prioritize the most crucial information.Extensive experiments demonstrate that our method achieves comparable orsuperior performance across various downstream tasks while exhibiting highertraining efficiency.</description>
      <author>example@mail.com (Wenzhao Xiang, Chang Liu, Hongyang Yu, Xilin Chen)</author>
      <guid isPermaLink="false">2503.00782v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Biomedical Foundation Model: A Survey</title>
      <link>http://arxiv.org/abs/2503.02104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了基础模型在生物医学领域的潜在应用，涵盖了计算生物学、药物研发与开发、临床信息学、医学影像和公共卫生等多个领域。&lt;h4&gt;背景&lt;/h4&gt;基础模型自2021年引入以来，通过无监督学习从大规模未标记数据集中获取知识，这些模型如GPT能够在问答和视觉理解等多种下游任务中表现出色，并超越了特定任务的AI模型。生物医学领域的基础模型开发标志着人工智能在理解和解析复杂生物学现象以及推动医疗研究与实践方面的重要进展。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探索基础模型在健康科学中的应用潜力，以期激发相关领域的进一步研究和创新。&lt;h4&gt;方法&lt;/h4&gt;综述了各类生物医学任务中使用的基础模型及其潜在的广泛应用场景。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型具备广泛的适用性和卓越的任务性能，在多个生物医学领域展现出巨大的应用前景。&lt;h4&gt;结论&lt;/h4&gt;未来的研究应当更加关注如何将基础模型有效地应用于解决复杂的健康科学问题，以促进医疗技术的进步和临床实践的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, first introduced in 2021, are large-scale pre-trainedmodels (e.g., large language models (LLMs) and vision-language models (VLMs))that learn from extensive unlabeled datasets through unsupervised methods,enabling them to excel in diverse downstream tasks. These models, like GPT, canbe adapted to various applications such as question answering and visualunderstanding, outperforming task-specific AI models and earning their name dueto broad applicability across fields. The development of biomedical foundationmodels marks a significant milestone in leveraging artificial intelligence (AI)to understand complex biological phenomena and advance medical research andpractice. This survey explores the potential of foundation models acrossdiverse domains within biomedical fields, including computational biology, drugdiscovery and development, clinical informatics, medical imaging, and publichealth. The purpose of this survey is to inspire ongoing research in theapplication of foundation models to health science.</description>
      <author>example@mail.com (Xiangrui Liu, Yuanyuan Zhang, Yingzhou Lu, Changchang Yin, Xiaoling Hu, Xiaoou Liu, Lulu Chen, Sheng Wang, Alexander Rodriguez, Huaxiu Yao, Yezhou Yang, Ping Zhang, Jintai Chen, Tianfan Fu, Xiao Wang)</author>
      <guid isPermaLink="false">2503.02104v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Social Media Rumor Detection: A Semantic and Graph Neural Network Approach for the 2024 Global Election</title>
      <link>http://arxiv.org/abs/2503.01394v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合语义分析和图神经网络的新方法，以解决社交媒体上谣言传播的问题。&lt;h4&gt;背景&lt;/h4&gt;社交媒体平台的发展极大地改变了信息传播的速度和方式，带来了有益和有害的影响。这些平台促进了快速通信的同时也加速了谣言和极端言论的传播，尤其是在选举期间对公共舆论和行为产生了显著影响。&lt;h4&gt;目的&lt;/h4&gt;针对有效的社交网络谣言检测需求，提出了一种新的方法来应对2024年全球各地前所未有的选举挑战。&lt;h4&gt;方法&lt;/h4&gt;采用语义分析与图神经网络相结合的方法。首先使用精调的BERT模型将文本内容向量化，并构建一个由推文和评论作为节点、互动行为为边的有向图，然后通过SAGEWithEdgeAttention（GraphSAGE的扩展）进行更精确特征聚合。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在复杂的社会网络结构中能够实现细粒度分析，提高谣言检测准确性，并且显著优于传统的内容分析和基于时间的方法。&lt;h4&gt;结论&lt;/h4&gt;研究得出的新方法提供了一种理论上严谨、实践中有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of social media platforms has revolutionized the speed andmanner in which information is disseminated, leading to both beneficial anddetrimental effects on society. While these platforms facilitate rapidcommunication, they also accelerate the spread of rumors and extremist speech,impacting public perception and behavior significantly. This issue isparticularly pronounced during election periods, where the influence of socialmedia on election outcomes has become a matter of global concern. With theunprecedented number of elections in 2024, against this backdrop, the electionecosystem has encountered unprecedented challenges. This study addresses theurgent need for effective rumor detection on social media by proposing a novelmethod that combines semantic analysis with graph neural networks. We havemeticulously collected a dataset from PolitiFact and Twitter, focusing onpolitically relevant rumors. Our approach involves semantic analysis using afine-tuned BERT model to vectorize text content and construct a directed graphwhere tweets and comments are nodes, and interactions are edges. The core ofour method is a graph neural network, SAGEWithEdgeAttention, which extends theGraphSAGE model by incorporating first-order differences as edge attributes andapplying an attention mechanism to enhance feature aggregation. This innovativeapproach allows for the fine-grained analysis of the complex social networkstructure, improving rumor detection accuracy. The study concludes that ourmethod significantly outperforms traditional content analysis and time-basedmodels, offering a theoretically sound and practically efficient solution.</description>
      <author>example@mail.com (Liu Yan, Liu Yunpeng, Zhao Liang)</author>
      <guid isPermaLink="false">2503.01394v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation</title>
      <link>http://arxiv.org/abs/2503.00733v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025; demo page at  https://alexander-h-liu.github.io/uniwav-demo.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;构建了一个统一的预训练框架UniWav，该框架可以同时适用于辨别任务和生成任务，在语音识别、文本到语音转换以及语音标记化等应用中表现出与特定任务模型相当的表现。&lt;h4&gt;背景&lt;/h4&gt;目前的语音处理方法依赖于不同的基础模型，这些模型通常是为了解决具体的辨别或生成任务而设计的。现有的预训练技术不足以同时适用于这两种类型的任务。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的预训练框架，可以在单一的基础模型中实现多种类型的任务，并减少预训练的成本和复杂性。&lt;h4&gt;方法&lt;/h4&gt;提出了名为UniWav的编码器-解码器架构，该架构旨在统一预训练表示学习与生成任务。通过适当的预训练设计选择，可以同时学习适用于两种类型的任务的表示编码器和生成音频解码器。&lt;h4&gt;主要发现&lt;/h4&gt;在语音识别、文本到语音转换以及语音标记化上，UniWav的表现与针对特定任务单独训练的基础模型相当。&lt;h4&gt;结论&lt;/h4&gt;证明了一种通用的语音基础模型可以替代多种专为特定任务设计的基础模型，并且能够降低预训练的成本和复杂性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training and representation learning have been playing an increasinglyimportant role in modern speech processing. Nevertheless, differentapplications have been relying on different foundation models, sincepredominant pre-training techniques are either designed for discriminativetasks or generative tasks. In this work, we make the first attempt at buildinga unified pre-training framework for both types of tasks in speech. We showthat with the appropriate design choices for pre-training, one can jointlylearn a representation encoder and generative audio decoder that can be appliedto both types of tasks. We propose UniWav, an encoder-decoder frameworkdesigned to unify pre-training representation learning and generative tasks. Onspeech recognition, text-to-speech, and speech tokenization, UniWav achievescomparable performance to different existing foundation models, each trained ona specific task. Our findings suggest that a single general-purpose foundationmodel for speech can be built to replace different foundation models, reducingthe overhead and cost of pre-training.</description>
      <author>example@mail.com (Alexander H. Liu, Sang-gil Lee, Chao-Han Huck Yang, Yuan Gong, Yu-Chiang Frank Wang, James R. Glass, Rafael Valle, Bryan Catanzaro)</author>
      <guid isPermaLink="false">2503.00733v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>EPEE: Towards Efficient and Effective Foundation Models in Biomedicine</title>
      <link>http://arxiv.org/abs/2503.02053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to npj Digital Medicine&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了EPEE策略，一种基于熵和耐心的混合早期退出策略，旨在提高基础模型在生物医学任务中的推理效率。&lt;h4&gt;背景&lt;/h4&gt;尽管基础模型如GPT、CLIP等显著推动了众多生物医学任务的发展，但高推理延迟及过度思考问题限制了它们在临床环境下的实时应用。&lt;h4&gt;目的&lt;/h4&gt;提出EPEE策略以克服现有早期退出方法的弱点，并提高基础模型在实际应用场景中的效率和效果。&lt;h4&gt;方法&lt;/h4&gt;通过四个基础模型（BERT、ALBERT、GPT-2和ViT）进行三项核心生物医学任务（分类，关系抽取以及事件抽取）的实验评估。实验涵盖了十二个不同的数据集，包括临床笔记及医学图像。&lt;h4&gt;主要发现&lt;/h4&gt;EPEE在减少推理时间的同时保持或提升了准确性，在多种不同类型的数据集中展示了其适应性和有效性。&lt;h4&gt;结论&lt;/h4&gt;EPEE解决了将基础模型部署到医疗保健领域中的关键障碍，并为实时临床决策提供了可能的实用解决方案，支持可靠的高效工作流程。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已包含中文描述，无需额外翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, including language models, e.g., GPT, and vision models,e.g., CLIP, have significantly advanced numerous biomedical tasks. Despitethese advancements, the high inference latency and the "overthinking" issues inmodel inference impair the efficiency and effectiveness of foundation models,thus limiting their application in real-time clinical settings. To addressthese challenges, we proposed EPEE (Entropy- and Patience-based Early Exiting),a novel hybrid strategy designed to improve the inference efficiency offoundation models. The core idea was to leverage the strengths of entropy-basedand patience-based early exiting methods to overcome their respectiveweaknesses. To evaluate EPEE, we conducted experiments on three core biomedicaltasks-classification, relation extraction, and event extraction-using fourfoundation models (BERT, ALBERT, GPT-2, and ViT) across twelve datasets,including clinical notes and medical images. The results showed that EPEEsignificantly reduced inference time while maintaining or improving accuracy,demonstrating its adaptability to diverse datasets and tasks. EPEE addressedcritical barriers to deploying foundation models in healthcare by balancingefficiency and effectiveness. It potentially provided a practical solution forreal-time clinical decision-making with foundation models, supporting reliableand efficient workflows.</description>
      <author>example@mail.com (Zaifu Zhan, Shuang Zhou, Huixue Zhou, Zirui Liu, Rui Zhang)</author>
      <guid isPermaLink="false">2503.02053v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Road Boundary Detection Using 4D mmWave Radar for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2503.01930v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于4D毫米波雷达的道路边界检测方法（4DRadarRBD），以解决传统视觉传感器在复杂驾驶环境中的成本和性能问题。&lt;h4&gt;背景&lt;/h4&gt;道路边界检测对于自动驾驶和高级驾驶员辅助系统至关重要，但传统的基于摄像头和LiDAR的方法在恶劣光照条件下表现不佳或成本过高。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于4D毫米波雷达的道路边界检测方法，以提高复杂驾驶场景中的鲁棒性和降低成本。&lt;h4&gt;方法&lt;/h4&gt;通过物理约束减少点云噪声，并利用距离基损失函数进行分割。引入了对时间动态的捕捉机制，考虑每个点与车辆运动补偿后的道路边界检测结果之间的偏差以及点云的空间分布。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界驾驶测试中实现了93%的道路边界点分割准确率和高达0.023米的中位距离误差，并且相较于基线模型错误降低了92.6%。&lt;h4&gt;结论&lt;/h4&gt;4DRadarRBD方法在成本效益、鲁棒性和准确性方面显著优于传统道路边界检测技术，尤其适用于复杂驾驶环境。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting road boundaries, the static physical edges of the available drivingarea, is important for safe navigation and effective path planning inautonomous driving and advanced driver-assistance systems (ADAS).Traditionally, road boundary detection in autonomous driving relies on camerasand LiDAR. However, they are vulnerable to poor lighting conditions, such asnighttime and direct sunlight glare, or prohibitively expensive for low-endvehicles. To this end, this paper introduces 4DRadarRBD, the first roadboundary detection method based on 4D mmWave radar which is cost-effective androbust in complex driving scenarios. The main idea is that road boundaries(e.g., fences, bushes, roadblocks), reflect millimeter waves, thus generatingpoint cloud data for the radar. To overcome the challenge that the 4D mmWaveradar point clouds contain many noisy points, we initially reduce noisy pointsvia physical constraints for road boundaries and then segment the road boundarypoints from the noisy points by incorporating a distance-based loss whichpenalizes for falsely detecting the points far away from the actual roadboundaries. In addition, we capture the temporal dynamics of point cloudsequences by utilizing each point's deviation from the vehiclemotion-compensated road boundary detection result obtained from the previousframe, along with the spatial distribution of the point cloud for point-wiseroad boundary segmentation. We evaluated 4DRadarRBD through real-world drivingtests and achieved a road boundary point segmentation accuracy of 93$\%$, witha median distance error of up to 0.023 m and an error reduction of 92.6$\%$compared to the baseline model.</description>
      <author>example@mail.com (Yuyan Wu, Hae Young Noh)</author>
      <guid isPermaLink="false">2503.01930v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Implicit Generative Modeling by Kernel Similarity Matching</title>
      <link>http://arxiv.org/abs/2503.00655v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 Pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了通过捕捉输入样本之间的相似性来学习表示的方法，并将其应用于生成模型中，以便能够将表示映射回输入空间。&lt;h4&gt;背景&lt;/h4&gt;理解大脑如何编码刺激是计算神经科学的基本问题。这方面的见解推动了具有类似大脑学习能力的人工神经网络的设计和开发。&lt;h4&gt;目的&lt;/h4&gt;研究一种基于内核相似性匹配的框架用于生成模型，并探讨该方法在脑任务表示编码中的潜在应用。&lt;h4&gt;方法&lt;/h4&gt;从先前工作中提出的稀疏编码目标修改开始，展示了在这个上下文中表征学习等同于最大化输入内核与隐含内核之间的相似性。提出了一个新的交替方向乘子法（ADMM）算法来解决优化问题，并讨论了优化过程的解释。&lt;h4&gt;主要发现&lt;/h4&gt;通过学习潜在空间中的内核结构可以形成一种隐式生成模型，该框架能够适应学习流形结构。&lt;h4&gt;结论&lt;/h4&gt;这种方法结合了使用相似性匹配（自下而上方法）进行表征学习与预测编码（自顶向下方法），有助于构建一个具有生物合理性的架构来学习模型参数。&lt;h4&gt;翻译&lt;/h4&gt;理解大脑如何对刺激进行编码一直是计算神经科学的基本问题。这些问题的见解促使设计和开发了一种人工神经网络，该网络通过引入类似大脑的学习能力来学习表示。最近的研究试图通过捕捉输入样本之间的相似性来解决这个问题。然而，这种方法迄今为止仅用于从输入中学习下游特征，并未在生成范式（可将表示映射回输入空间的范式）下进行研究，在这种情况下，不仅可以实现自底向上的相互作用（刺激到潜在），还可以以自顶向下的方式（潜在到刺激）学习特征。我们探讨了一种用于生成建模的内核相似性匹配框架。从先前工作中提出的一种修改后的稀疏编码目标开始，我们展示了在这种背景下表示学习等同于最大化输入内核与隐含内核之间的相似性。我们证明了通过在潜在空间中学习内核结构可以形成一种隐式生成模型，并展示如何使该框架适应学习流形结构，这可能有助于了解任务表征可以在大脑中是如何编码的。为了实现目标，我们提出了一种新颖的交替方向乘子法（ADMM）算法，并讨论了优化过程的解释。最后，我们探讨了这种表示学习问题可以导向一种生物学上合理的架构来学习模型参数，该架构将使用相似性匹配进行表征学习（自下而上的方法）与预测编码（自顶向下的方法）结合起来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how the brain encodes stimuli has been a fundamental problem incomputational neuroscience. Insights into this problem have led to the designand development of artificial neural networks that learn representations byincorporating brain-like learning abilities. Recently, learning representationsby capturing similarity between input samples has been studied to tackle thisproblem. This approach, however, has thus far been used to only learndownstream features from an input and has not been studied in the context of agenerative paradigm, where one can map the representations back to the inputspace, incorporating not only bottom-up interactions (stimuli to latent) butalso learning features in a top-down manner (latent to stimuli). We investigatea kernel similarity matching framework for generative modeling. Starting with amodified sparse coding objective for learning representations proposed in priorwork, we demonstrate that representation learning in this context is equivalentto maximizing similarity between the input kernel and a latent kernel. We showthat an implicit generative model arises from learning the kernel structure inthe latent space and show how the framework can be adapted to learn manifoldstructures, potentially providing insights as to how task representations canbe encoded in the brain. To solve the objective, we propose a novel AlternateDirection Method of Multipliers (ADMM) based algorithm and discuss theinterpretation of the optimization process. Finally, we discuss how thisrepresentation learning problem can lead towards a biologically plausiblearchitecture to learn the model parameters that ties together representationlearning using similarity matching (a bottom-up approach) with predictivecoding (a top-down approach).</description>
      <author>example@mail.com (Shubham Choudhary, Paul Masset, Demba Ba)</author>
      <guid isPermaLink="false">2503.00655v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Statistical physics analysis of graph neural networks: Approaching optimality in the contextual stochastic block model</title>
      <link>http://arxiv.org/abs/2503.01361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNN）设计用于处理与图关联的数据，并在不断增加的应用领域中发挥作用。然而，与其他现代机器学习技术一样，其理论理解仍然有限。&lt;h4&gt;目的&lt;/h4&gt;解决由于过度平滑等问题导致的远距离节点信息获取困难的问题，并探讨如何通过增加深度来接近贝叶斯最优性。&lt;h4&gt;方法&lt;/h4&gt;分析了基于上下文随机块模型生成的数据进行训练的基本图卷积网络（GCN）在节点分类任务上的泛化性能。使用复制方法，在高维极限下推导问题的自由能量，以预测其渐近性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. 增加深度对于接近贝叶斯最优性至关重要；2. GCN架构需要随着深度的变化进行调整以避免过度平滑；3. 大深度限制可以与贝叶斯最优性相近，并且导致连续GCN的形成；4. 通过类似于动态平均场理论的方法来处理连续极限，以及在大正则化下展开解对应于深层GCN性能方程。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一个有前途的工具，用于进一步分析深度神经网络，并可能对未来的研究有所贡献。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已经以中文形式给出。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are designed to process data associated withgraphs. They are finding an increasing range of applications; however, as withother modern machine learning techniques, their theoretical understanding islimited. GNNs can encounter difficulties in gathering information from nodesthat are far apart by iterated aggregation steps. This situation is partlycaused by so-called oversmoothing; and overcoming it is one of the practicallymotivated challenges. We consider the situation where information is aggregatedby multiple steps of convolution, leading to graph convolutional networks(GCNs). We analyze the generalization performance of a basic GCN, trained fornode classification on data generated by the contextual stochastic block model.We predict its asymptotic performance by deriving the free energy of theproblem, using the replica method, in the high-dimensional limit. Calling depththe number of convolutional steps, we show the importance of going to largedepth to approach the Bayes-optimality. We detail how the architecture of theGCN has to scale with the depth to avoid oversmoothing. The resulting largedepth limit can be close to the Bayes-optimality and leads to a continuous GCN.Technically, we tackle this continuous limit via an approach that resemblesdynamical mean-field theory (DMFT) with constraints at the initial and finaltimes. An expansion around large regularization allows us to solve thecorresponding equations for the performance of the deep GCN. This promisingtool may contribute to the analysis of further deep neural networks.</description>
      <author>example@mail.com (O. Duranthon, L. Zdeborová)</author>
      <guid isPermaLink="false">2503.01361v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Pretrained Embeddings as a Behavior Specification Mechanism</title>
      <link>http://arxiv.org/abs/2503.02012v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种使用嵌入式数学表示来正式描述依赖于感知模型与物理世界交互的系统的规范方法。&lt;h4&gt;背景&lt;/h4&gt;现有的系统规范语言难以精确表达基于感知模型的AI系统的复杂行为特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的逻辑形式，即嵌入时态逻辑(ETL)，用于在AI系统中更广泛地描述和验证行为属性。&lt;h4&gt;方法&lt;/h4&gt;引入了嵌入作为规范语言中的第一类构造，并通过距离度量理想与观察到的嵌入之间的差异来表达性质。&lt;h4&gt;主要发现&lt;/h4&gt;初步评估表明，基于嵌入式的规范可以引导机器人等AI系统表现出期望的行为。&lt;h4&gt;结论&lt;/h4&gt;提出的ETL方法能够在感知驱动的任务中实现有效的行为控制和验证。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种正式指定依赖于感知模型与物理世界交互的系统的操作属性的方法。关键思想是在规范语言中引入嵌入（现实概念的数学表示），其中性质以理想嵌入和观察到的嵌入之间的距离来表达。为了实现这种方法，我们提出了称为嵌入时态逻辑(ETL)的新类型的时间逻辑，并描述了如何使用它以前所未有的方式表达AI系统的一系列属性。通过涉及由基础模型驱动的机器人规划任务的初步评估，展示了ETL的应用潜力；结果很有前景，表明基于嵌入式的规范可以引导系统朝向期望的行为。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an approach to formally specifying the behavioral properties ofsystems that rely on a perception model for interactions with the physicalworld. The key idea is to introduce embeddings -- mathematical representationsof a real-world concept -- as a first-class construct in a specificationlanguage, where properties are expressed in terms of distances between a pairof ideal and observed embeddings. To realize this approach, we propose a newtype of temporal logic called Embedding Temporal Logic (ETL), and describe howit can be used to express a wider range of properties about AI-enabled systemsthan previously possible. We demonstrate the applicability of ETL through apreliminary evaluation involving planning tasks in robots that are driven byfoundation models; the results are promising, showing that embedding-basedspecifications can be used to steer a system towards desirable behaviors.</description>
      <author>example@mail.com (Parv Kapoor, Abigail Hammer, Ashish Kapoor, Karen Leung, Eunsuk Kang)</author>
      <guid isPermaLink="false">2503.02012v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Synergy Between Sufficient Changes and Sparse Mixing Procedure for Disentangled Representation Learning</title>
      <link>http://arxiv.org/abs/2503.00639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文探讨了解开观测数据背后潜在变量的解耦表示学习，并分析两种不同的假设如何结合使用以提高可识别性。&lt;h4&gt;背景&lt;/h4&gt;解耦表示学习通常需要较强的先验假设来确保潜在变量的可识别性。一方面，某些方法依赖于辅助变量（如领域索引）所指示的潜在变量分布的变化；另一方面，一些方法利用混合过程中的结构稀疏性假设。&lt;h4&gt;目的&lt;/h4&gt;提出一个在较少限制条件下实现潜在变量可识别性的理论，并开发一种包含领域编码网络和稀疏混合约束的估计框架。&lt;h4&gt;方法&lt;/h4&gt;当以辅助变量为条件时，该研究提出了基于稀疏混合过程假定提供从估算到真实潜在变量映射结构约束的方法，并利用变分自编码器(VAE)和生成对抗网络(GAN)实现此理论。&lt;h4&gt;主要发现&lt;/h4&gt;两种看似不相关的假设可以互补使用来提高可识别性；当以辅助变量为条件时，稀疏混合过程假定提供了从估算到真实潜在变量映射的结构约束，并弥补了可能不足的分布变化。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，在合成数据集和实际世界数据集上的结果支持该理论。此方法增强了在现实场景中的应用性。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了解耦表示学习的目标是解开观测数据背后的潜在变量，同时指出两种不同的假设（基于辅助变量的分布变化与稀疏混合过程）如何结合使用可以提高可识别性，并介绍了一种新的估计框架及其实施方式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Disentangled representation learning aims to uncover latent variablesunderlying the observed data, and generally speaking, rather strong assumptionsare needed to ensure identifiability. Some approaches rely on sufficientchanges on the distribution of latent variables indicated by auxiliaryvariables such as domain indices, but acquiring enough domains is oftenchallenging. Alternative approaches exploit structural sparsity assumptions onthe mixing procedure, but such constraints are usually (partially) violated inpractice. Interestingly, we find that these two seemingly unrelated assumptionscan actually complement each other to achieve identifiability. Specifically,when conditioned on auxiliary variables, the sparse mixing procedure assumptionprovides structural constraints on the mapping from estimated to true latentvariables and hence compensates for potentially insufficient distributionchanges. Building on this insight, we propose an identifiability theory withless restrictive constraints regarding distribution changes and the sparsemixing procedure, enhancing applicability to real-world scenarios.Additionally, we develop an estimation framework incorporating a domainencoding network and a sparse mixing constraint and provide two implementationsbased on variational autoencoders and generative adversarial networks,respectively. Experiment results on synthetic and real-world datasets supportour theoretical results.</description>
      <author>example@mail.com (Zijian Li, Shunxing Fan, Yujia Zheng, Ignavier Ng, Shaoan Xie, Guangyi Chen, Xinshuai Dong, Ruichu Cai, Kun Zhang)</author>
      <guid isPermaLink="false">2503.00639v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating MobileNetV2 and GraphSAGE with Cross-Modal Attention</title>
      <link>http://arxiv.org/abs/2503.01284v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种结合MobileNetV2和GraphSAGE的混合Sequential CNN-图神经网络框架，用于大豆叶片疾病的检测。该方法在识别细微病变特征的同时还能捕捉全局症状模式，并通过可视化技术提高了模型的解释性。&lt;h4&gt;背景&lt;/h4&gt;大豆叶片疾病检测是农业生产力的关键问题，但因其视觉相似的症状和传统方法可解释性差而面临挑战。虽然卷积神经网络（CNN）擅长空间特性提取，但由于忽视了图像间的依赖关系而导致误分类情况较多。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的混合Sequential CNN-图神经网络框架，旨在提高大豆叶片疾病检测的准确性并增强模型的可解释性。&lt;h4&gt;方法&lt;/h4&gt;该研究结合MobileNetV2进行局部特征提取和GraphSAGE建模图像间的关系。采用基于余弦相似性的邻接矩阵构建图形，节点代表叶子图像，并通过自适应邻居采样定义边。利用Grad-CAM和Eigen-CAM技术提供跨模式可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在包含十种大豆叶片疾病的数据库中实现了97.16%的准确性，超过了单独使用的CNN（≤95.04%）和传统的机器学习方法（≤77.05%）。此外，该研究通过消融实验验证了序贯架构相对于并行或单一模型配置的优势。&lt;h4&gt;结论&lt;/h4&gt;提出的框架提供了一种轻量级且高效的解决方案，不仅在大豆叶片疾病检测方面具有高精度，而且在资源受限的环境中可以实现实时部署。这为植物病理学领域的CNN-GNN集成研究开辟了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;大豆叶片病害检测对农业生产力至关重要，但视觉相似的症状和传统方法解释性差给其带来了挑战。尽管卷积神经网络（CNN）在空间特征提取方面表现出色，但由于忽略了图像间的关系依赖性而容易导致误分类。本文提出了一种可解释的混合Sequential CNN-图神经网络框架，该框架结合了MobileNetV2进行局部特征提取和GraphSAGE建模关系的方法。该架构构建了一个图形，其中节点表示叶子图片，并通过基于余弦相似性的邻接矩阵定义边和自适应邻居采样来捕获细粒度病变特性和整体症状模式，解决了类内相似性问题。跨模式可解释性是通过Grad-CAM和Eigen-CAM可视化技术实现的，生成热图突出显示影响疾病的区域。在包含十种大豆叶片病害的数据集上进行评估，该模型实现了97.16%的准确性，超过了单独的CNN（≤95.04%）和传统机器学习方法（≤77.05%）。消融研究验证了序贯架构相对于并行或单一模型配置的优势。利用轻量级MobileNetV2-GraphSAGE组合的仅2.3百万参数，确保计算效率，能够实现实时部署于资源受限环境中。所提出的方案在准确分类和实际应用之间架起了桥梁，为农业诊断提供了一种稳健、可解释的强大工具，并推进了植物病理学领域CNN-GNN集成研究的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Soybean leaf disease detection is critical for agricultural productivity butfaces challenges due to visually similar symptoms and limited interpretabilityin conventional methods. While Convolutional Neural Networks (CNNs) excel inspatial feature extraction, they often neglect inter-image relationaldependencies, leading to misclassifications. This paper proposes aninterpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework thatsynergizes MobileNetV2 for localized feature extraction and GraphSAGE forrelational modeling. The framework constructs a graph where nodes representleaf images, with edges defined by cosine similarity-based adjacency matricesand adaptive neighborhood sampling. This design captures fine-grained lesionfeatures and global symptom patterns, addressing inter-class similaritychallenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAMvisualizations, generating heatmaps to highlight disease-influential regions.Evaluated on a dataset of ten soybean leaf diseases, the model achieves$97.16\%$ accuracy, surpassing standalone CNNs ($\le95.04\%$) and traditionalmachine learning models ($\le77.05\%$). Ablation studies validate thesequential architecture's superiority over parallel or single-modelconfigurations. With only 2.3 million parameters, the lightweightMobileNetV2-GraphSAGE combination ensures computational efficiency, enablingreal-time deployment in resource-constrained environments. The proposedapproach bridges the gap between accurate classification and practicalapplicability, offering a robust, interpretable tool for agriculturaldiagnostics while advancing CNN-GNN integration in plant pathology research.</description>
      <author>example@mail.com (Md Abrar Jahin, Soudeep Shahriar, M. F. Mridha, Nilanjan Dey)</author>
      <guid isPermaLink="false">2503.01284v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Proportionality in Thumbs Up and Down Voting</title>
      <link>http://arxiv.org/abs/2503.01985v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该论文探讨了在包含正负偏好表达的决策环境中，如何理解比例性原则。&lt;h4&gt;背景&lt;/h4&gt;当前，在宪法人工智能中，公民民主地选择一整套伦理准则来训练基础模型。实践中，人们可能会对不同的伦理原则既表示赞同也表示反对。&lt;h4&gt;目的&lt;/h4&gt;提出两种概念上不同的方法来解释在存在上下投票的情况下比例性的含义。&lt;h4&gt;方法&lt;/h4&gt;第一种方法将选举候选人带来的满足感和否决他们的影响视为可比较的，从而提供综合的比例性保证；第二种方法独立考虑否决权，引入不同于传统比例性的保证。该研究形式化了每个视角下的公理，并通过适应Phragmén规则、Proportional Approval Voting（PAV）规则以及等份额法来考察它们的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;提出了两种解释负偏好情况的比例性原则的方法，并为每种方法建立了理论依据，探讨了这些比例性保证在实际决策中的应用和可行性。&lt;h4&gt;结论&lt;/h4&gt;该研究深化了对包含正负面表达的选举系统中比例性的理解，并为进一步的研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;考虑决策设置，在这种情况下代理者通过表示正面和负面偏好来选出一个小组。特别是，在宪法AI中，公民民主地选择一套伦理准则用于训练基础模型。在实践中，人们可能对不同的伦理原则既有赞成也有反对的意见。比例性已经在计算社会选择理论中的同意票上得到了很好的研究，但在考虑负面情绪时其意义仍然不清楚。本文提出了两种概念上截然不同的方法来解释在存在上下投票的情况下比例性的含义。第一种方法将选举候选人带来的满足感和否决他们的影响视为可比较的，从而提供综合的比例性保证；第二种方法独立考虑否决权，引入不同于传统比例性的保证。我们为每个视角形式化了公理并考察了它们的一致性，通过适应Phragmén规则、Proportional Approval Voting（PAV）规则以及等份额法来实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Consider the decision-making setting where agents elect a panel by expressingboth positive and negative preferences. Prominently, in constitutional AI,citizens democratically select a slate of ethical preferences on which afoundation model is to be trained. There, in practice, agents may both approveand disapprove of different ethical principles. Proportionality has beenwell-studied in computational social choice for approval ballots, but itsmeaning remains unclear when negative sentiments are also considered. In thiswork, we propose two conceptually distinct approaches to interpretproportionality in the presence of up and down votes. The first approach treatsthe satisfaction from electing candidates and the impact of vetoing them ascomparable, leading to combined proportionality guarantees. The second approachconsiders veto power separately, introducing guarantees distinct fromtraditional proportionality. We formalize axioms for each perspective andexamine their satisfiability by suitable adaptations of Phragm\'en's rule,Proportional Approval Voting rule and the Method of Equal Shares.</description>
      <author>example@mail.com (Sonja Kraiczy, Georgios Papasotiropoulos, Grzegorz Pierczyński, Piotr Skowron)</author>
      <guid isPermaLink="false">2503.01985v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization</title>
      <link>http://arxiv.org/abs/2503.00881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文提出了CarGS，一种统一模型框架，用于同时实现高质量渲染和表面重建。&lt;h4&gt;背景&lt;/h4&gt;三维场景表示是从多视角图像中生成的核心挑战之一，在计算机视觉和图形学领域具有重要意义。最近出现的3D高斯点云（3DGaussian Splatting, 3DGS）因能够提供高品质渲染且推理速度较快而备受关注，但其在精确几何重建方面仍存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法解决现有技术中渲染和重构之间的内在冲突以及计算密集型和存储成本高的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了CarGS模型，通过贡献自适应正则化实现高质量的渲染与表面重建。该框架学习高斯原语（Gaussian primitives）的自适应贡献，并将几何正则化的知识整合到紧凑多层感知器中。此外，还引入了一种基于几何引导的密集化策略，利用法线和符号距离字段来捕捉高频细节。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在渲染保真度与重建准确性方面达到了最先进的性能，同时保证了实时速度和最小存储空间需求。&lt;h4&gt;结论&lt;/h4&gt;CarGS模型通过贡献自适应正则化解决了3D高斯点云在几何精确性上的难题，并且其统一结构不需要像双模态方法那样使用多个独立的模型，从而保证了效率。&lt;h4&gt;翻译&lt;/h4&gt;表示三维场景是从多视角图像中生成的核心挑战之一，在计算机视觉和图形学领域具有重要意义。最近出现的方法如3D高斯点云(3DGaussian Splatting, 3DGS)虽然能够提供高质量渲染且推理速度较快，但在精确几何重建方面仍存在困难。为解决这一问题，提出了CarGS模型，通过贡献自适应正则化实现高质量的渲染与表面重建，并且在保持实时性能的同时实现了最小化的存储需求和最佳的几何保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representing 3D scenes from multiview images is a core challenge in computervision and graphics, which requires both precise rendering and accuratereconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significantattention for its high-quality rendering and fast inference speed. Yet, due tothe unstructured and irregular nature of Gaussian point clouds, ensuringaccurate geometry reconstruction remains difficult. Existing methods primarilyfocus on geometry regularization, with common approaches includingprimitive-based and dual-model frameworks. However, the former suffers frominherent conflicts between rendering and reconstruction, while the latter iscomputationally and storage-intensive. To address these challenges, we proposeCarGS, a unified model leveraging Contribution-adaptive regularization toachieve simultaneous, high-quality rendering and surface reconstruction. Theessence of our framework is learning adaptive contribution for Gaussianprimitives by squeezing the knowledge from geometry regularization into acompact MLP. Additionally, we introduce a geometry-guided densificationstrategy with clues from both normals and Signed Distance Fields (SDF) toimprove the capability of capturing high-frequency details. Our design improvesthe mutual learning of the two tasks, meanwhile its unified structure does notrequire separate models as in dual-model based approaches, guaranteeingefficiency. Extensive experiments demonstrate the ability to achievestate-of-the-art (SOTA) results in both rendering fidelity and reconstructionaccuracy while maintaining real-time speed and minimal storage size.</description>
      <author>example@mail.com (You Shen, Zhipeng Zhang, Xinyang Li, Yansong Qu, Yu Lin, Shengchuan Zhang, Liujuan Cao)</author>
      <guid isPermaLink="false">2503.00881v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Channel-Attentive Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.00578v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as a conference paper at IEEE International Conference on  Data Mining 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;图神经网络（GNNs）在处理图结构数据的表示学习中处于领先地位，被广泛应用于在线社交网络和复杂分子等各个领域。大多数GNN采用消息传递机制，并且在各种任务上表现优异。&lt;h4&gt;背景&lt;/h4&gt;尽管大多数GNN通过消息传递实现了出色性能，但随着模型深度增加，普遍存在过度平滑的问题，导致节点表示之间的相似性增加，进而影响了GNN的表现。&lt;h4&gt;目的&lt;/h4&gt;提出一种自适应信道级消息传递方法以缓解过平滑问题。&lt;h4&gt;方法&lt;/h4&gt;引入了一种名为Channel-Attentive GNN的模型，该模型能够学习如何关注邻近节点及其特征通道，在进行消息传递时可以交换更多种类的信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，提出的模型比基准模型更能抵抗过度平滑，并且在各种具有强烈异构性的图上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过改进GNN的消息传递机制，该研究提出了一种能够有效缓解过平滑问题的方法，并展示了其对复杂图形数据表示的优越性。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) set the state-of-the-art in representation learning for graph-structured data. They are used in many domains, from online social networks to complex molecules. Most GNNs leverage the message-passing paradigm and achieve strong performances on various tasks. However, the message-passing mechanism used in most models suffers from over-smoothing as a GNN's depth increases. The over-smoothing degrades GNN's performance due to the increased similarity between the representations of unrelated nodes. This study proposes an adaptive channel-wise message-passing approach to alleviate the over-smoothing. The proposed model, Channel-Attentive GNN, learns how to attend to neighboring nodes and their feature channels. Thus, much diverse information can be transferred between nodes during message-passing. Experiments with widely used benchmark datasets show that the proposed model is more resistant to over-smoothing than baselines and achieves state-of-the-art performances for various graphs with strong heterophily.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICDM59182.2024.00084&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/allab-boun/chat-gnn&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) set the state-of-the-art in representationlearning for graph-structured data. They are used in many domains, from onlinesocial networks to complex molecules. Most GNNs leverage the message-passingparadigm and achieve strong performances on various tasks. However, themessage-passing mechanism used in most models suffers from over-smoothing as aGNN's depth increases. The over-smoothing degrades GNN's performance due to theincreased similarity between the representations of unrelated nodes. This studyproposes an adaptive channel-wise message-passing approach to alleviate theover-smoothing. The proposed model, Channel-Attentive GNN, learns how to attendto neighboring nodes and their feature channels. Thus, much diverse informationcan be transferred between nodes during message-passing. Experiments withwidely used benchmark datasets show that the proposed model is more resistantto over-smoothing than baselines and achieves state-of-the-art performances forvarious graphs with strong heterophily. Our code is athttps://github.com/ALLab-Boun/CHAT-GNN.</description>
      <author>example@mail.com (Tuğrul Hasan Karabulut, İnci M. Baytaş)</author>
      <guid isPermaLink="false">2503.00578v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts</title>
      <link>http://arxiv.org/abs/2503.01947v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been submitted to IEEE Transactions on Artificial  Intelligence for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近年来，大型语言模型由于其显著的潜力吸引了越来越多的关注，尽管人们对由此产生的不安全行为引发了担忧。这些不安全行为源自固有的刻板印象和偏见。&lt;h4&gt;背景&lt;/h4&gt;大多数关于LLM（大型语言模型）中的刻板印象的研究主要依赖于间接评估方法，在这种方法中，让模型在与特定社会群体相关的成对句子之间进行选择。最近，直接评估方法出现，这些方法通过检查开放式的模型响应来克服以前方法的局限性，例如标注者偏见。&lt;h4&gt;目的&lt;/h4&gt;这项研究旨在探讨日语LLM（大型语言模型）在应对触发刻板印象提示时的安全性，并填补非英语尤其是日语模型研究领域的空白。&lt;h4&gt;方法&lt;/h4&gt;构建了3,612个触发刻板印象的提示，这些提示由301个社会群体术语和12种类型化模板组合而成。从三种不同语言基础训练的语言模型（日语、英语和中文）中分析响应。&lt;h4&gt;主要发现&lt;/h4&gt;日本本土模型LLM-jp在拒绝率最低的同时更可能生成有毒或负面的回应；提示格式对所有模型输出有显著影响，反应通常包含针对特定社会群体夸张化的内容且因模型而异。这些发现揭示了日语LLM中伦理安全机制的不足，并证明即使高准确性的模型也能在处理日语文本时产生偏见。&lt;h4&gt;结论&lt;/h4&gt;研究呼吁改进日语LLM中的安全措施和减少偏见策略，以促进AI伦理讨论超越语言边界，对于提升全球范围内的AI安全性具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, Large Language Models have attracted growing interest fortheir significant potential, though concerns have rapidly emerged regardingunsafe behaviors stemming from inherent stereotypes and biases. Most researchon stereotypes in LLMs has primarily relied on indirect evaluation setups, inwhich models are prompted to select between pairs of sentences associated withparticular social groups. Recently, direct evaluation methods have emerged,examining open-ended model responses to overcome limitations of previousapproaches, such as annotator biases. Most existing studies have focused onEnglish-centric LLMs, whereas research on non-English models, particularlyJapanese, remains sparse, despite the growing development and adoption of thesemodels. This study examines the safety of Japanese LLMs when responding tostereotype-triggering prompts in direct setups. We constructed 3,612 prompts bycombining 301 social group terms, categorized by age, gender, and otherattributes, with 12 stereotype-inducing templates in Japanese. Responses wereanalyzed from three foundational models trained respectively on Japanese,English, and Chinese language. Our findings reveal that LLM-jp, a Japanesenative model, exhibits the lowest refusal rate and is more likely to generatetoxic and negative responses compared to other models. Additionally, promptformat significantly influence the output of all models, and the generatedresponses include exaggerated reactions toward specific social groups, varyingacross models. These findings underscore the insufficient ethical safetymechanisms in Japanese LLMs and demonstrate that even high-accuracy models canproduce biased outputs when processing Japanese-language prompts. We advocatefor improving safety mechanisms and bias mitigation strategies in JapaneseLLMs, contributing to ongoing discussions on AI ethics beyond linguisticboundaries.</description>
      <author>example@mail.com (Akito Nakanishi, Yukie Sano, Geng Liu, Francesco Pierri)</author>
      <guid isPermaLink="false">2503.01947v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency Recovery</title>
      <link>http://arxiv.org/abs/2503.00848v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为PSRGS的渐进优化方案，解决了3D Gaussian Splatting在大规模遥感场景中遇到的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的3D Gaussian Splatting方法在处理小规模单一物体场景时效果显著，但在处理大规模遥感数据时，由于点云稀疏和过度平滑问题而出现性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决3D Gaussian Splatting应用于大规模场景中的过度重建、错误几何位置导致的密度化以及梯度伪影等问题。&lt;h4&gt;方法&lt;/h4&gt;通过创建光谱残差显著图分离低频与高频区域，采用深度感知和平滑损失初始化低频区域，并利用更高阈值的梯度特征分裂和克隆椭圆体以优化高频细节。&lt;h4&gt;主要发现&lt;/h4&gt;提出的PSRGS方案在多个数据集上的实验结果表明，在恢复高频率纹理细节方面具有竞争性的渲染质量。&lt;h4&gt;结论&lt;/h4&gt;通过分阶段地应用不同的优化策略，能够有效地提高大规模场景的3D Gaussian Splatting性能。&lt;h4&gt;翻译&lt;/h4&gt;论文提出了针对大规模遥感场景中使用3D Gaussian Splatting遇到的问题（如点云稀疏和过度平滑），提出了一种基于光谱残差图进行渐进式优化的新方法PSRGS，成功提升了处理效果及细节恢复能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3D GS) achieves impressive results in novel viewsynthesis for small, single-object scenes through Gaussian ellipsoidinitialization and adaptive density control. However, when applied tolarge-scale remote sensing scenes, 3D GS faces challenges: the point cloudsgenerated by Structure-from-Motion (SfM) are often sparse, and the inherentsmoothing behavior of 3D GS leads to over-reconstruction in high-frequencyregions, where have detailed textures and color variations. This results in thegeneration of large, opaque Gaussian ellipsoids that cause gradient artifacts.Moreover, the simultaneous optimization of both geometry and texture may leadto densification of Gaussian ellipsoids at incorrect geometric locations,resulting in artifacts in other views. To address these issues, we proposePSRGS, a progressive optimization scheme based on spectral residual maps.Specifically, we create a spectral residual significance map to separatelow-frequency and high-frequency regions. In the low-frequency region, we applydepth-aware and depth-smooth losses to initialize the scene geometry with lowthreshold. For the high-frequency region, we use gradient features with higherthreshold to split and clone ellipsoids, refining the scene. The sampling rateis determined by feature responses and gradient loss. Finally, we introduce apre-trained network that jointly computes perceptual loss from multiple views,ensuring accurate restoration of high-frequency details in both Gaussianellipsoids geometry and color. We conduct experiments on multiple datasets toassess the effectiveness of our method, which demonstrates competitiverendering quality, especially in recovering texture details in high-frequencyregions.</description>
      <author>example@mail.com (BoCheng Li, WenJuan Zhang, Bing Zhang, YiLing Yao, YaNing Wang)</author>
      <guid isPermaLink="false">2503.00848v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Language Model Mapping in Multimodal Music Learning: A Grand Challenge Proposal</title>
      <link>http://arxiv.org/abs/2503.00427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;近年来，在深度神经网络的帮助下，表示学习和语言模型取得了显著的成功。很多研究旨在通过词汇或嵌入级别上的对齐和映射来构建不同模式之间的内在联系。&lt;h4&gt;问题&lt;/h4&gt;然而，大多数方法非常依赖于大量数据的输入，这在音乐等领域表现不佳，因为这些领域中的成对数据较少。&lt;h4&gt;目的&lt;/h4&gt;作者认为，嵌入对齐仅是跨模态对齐的表面层次。本文提出了一种新的挑战“语言模型映射（LMM）”，即如何将一种模式下的语言模型的本质映射到另一种模式下的语言模型中，前提是假设不同模式的语言模型都在追踪相同的基本现象。&lt;h4&gt;方法&lt;/h4&gt;首先介绍了一个关于LMM的基础设置，并强调了其目标是揭示跨模态对齐的深层次方面以及实现更高效的样本学习。然后探讨了音乐领域为何成为进行LMM研究的理想选择。&lt;h4&gt;进一步讨论&lt;/h4&gt;接着，将音乐中的LMM与一个更为广泛且具有挑战性的科学问题联系起来——即“基于感官输入和抽象符号的学习如何采取行动”。最后提出了一种先进版本的挑战性问题设置。&lt;h4&gt;结论&lt;/h4&gt;本文提出的语言模型映射概念为跨模态对齐研究提供了一个新的视角，尤其适用于数据稀缺或难以获取成对数据的情况。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We have seen remarkable success in representation learning and languagemodels (LMs) using deep neural networks. Many studies aim to build theunderlying connections among different modalities via the alignment andmappings at the token or embedding level, but so far, most methods are verydata-hungry, limiting their performance in domains such as music where paireddata are less abundant. We argue that the embedding alignment is only at thesurface level of multimodal alignment. In this paper, we propose a grandchallenge of \textit{language model mapping} (LMM), i.e., how to map theessence implied in the LM of one domain to the LM of another domain under theassumption that LMs of different modalities are tracking the same underlyingphenomena. We first introduce a basic setup of LMM, highlighting the goal tounveil a deeper aspect of cross-modal alignment as well as to achieve moresample-efficiency learning. We then discuss why music is an ideal domain inwhich to conduct LMM research. After that, we connect LMM in music with a moregeneral and challenging scientific problem of \textit{learning to take actionsbased on both sensory input and abstract symbols}, and in the end, present anadvanced version of the challenge problem setup.</description>
      <author>example@mail.com (Daniel Chin, Gus Xia)</author>
      <guid isPermaLink="false">2503.00427v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Open-source framework for detecting bias and overfitting for large pathology images</title>
      <link>http://arxiv.org/abs/2503.01827v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种通用且模型无关的框架，用于调试深度学习模型中的捷径问题。&lt;h4&gt;背景信息&lt;/h4&gt;大型训练数据集也可能导致模型过度拟合和偏差，特别是非相关数据模式如背景颜色或色彩强度成为捷径。&lt;h4&gt;研究目的&lt;/h4&gt;开发一种能够检测并移除这些捷径的方法，以确保深度学习应用的稳健性。&lt;h4&gt;方法介绍&lt;/h4&gt;提出了一种无需针对特定模型架构定制的、通用且模型无关的调试框架。该框架特别适用于大规模图像数据处理领域（如病理学），并且能够在配备普通GPU的工作站上运行。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够复制在先前研究中发现的自监督学习模型中的非图片捷径，并且还识别出基础模型中存在的潜在捷径。&lt;h4&gt;结论&lt;/h4&gt;易于使用的测试有助于开发更可靠、准确和泛化的用于WSI分析的模型。此框架作为一个开源工具，可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;即使在使用数十亿数据样本进行训练的基础模型中也可能发展出导致过度拟合和偏差的捷径问题。这些捷径是非相关的数据模式，例如背景颜色或色彩强度等。为了确保深度学习应用的稳健性，需要检测并移除此类捷径的方法。当前的模型调试方法耗时较长，并且通常需要针对特定领域的给定模型架构进行定制化调整。我们提出了一种通用、模型无关的框架来调试深度学习模型，特别是在病理学领域，该领域涉及非常大的图像和庞大的计算资源需求。我们的框架能够在配备普通GPU的工作站上运行。我们展示了该框架可以复制先前工作中发现的自监督学习模型中的非图片捷径，并且还识别出基础模型中存在的潜在捷径。这些易用性测试有助于开发更可靠、准确和泛化的用于WSI分析的深度学习模型。我们的框架作为一个开源工具，可在GitHub上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Even foundational models that are trained on datasets with billions of datasamples may develop shortcuts that lead to overfitting and bias. Shortcuts arenon-relevant patterns in data, such as the background color or color intensity.So, to ensure the robustness of deep learning applications, there is a need formethods to detect and remove such shortcuts. Today's model debugging methodsare time consuming since they often require customization to fit for a givenmodel architecture in a specific domain. We propose a generalized,model-agnostic framework to debug deep learning models. We focus on the domainof histopathology, which has very large images that require large models - andtherefore large computation resources. It can be run on a workstation with acommodity GPU. We demonstrate that our framework can replicate non-imageshortcuts that have been found in previous work for self-supervised learningmodels, and we also identify possible shortcuts in a foundation model. Our easyto use tests contribute to the development of more reliable, accurate, andgeneralizable models for WSI analysis. Our framework is available as anopen-source tool available on github.</description>
      <author>example@mail.com (Anders Sildnes, Nikita Shvetsov, Masoud Tafavvoghi, Vi Ngoc-Nha Tran, Kajsa Møllersen, Lill-Tove Rasmussen Busund, Thomas K. Kilvær, Lars Ailo Bongo)</author>
      <guid isPermaLink="false">2503.01827v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>HiMo: High-Speed Objects Motion Compensation in Point Clouds</title>
      <link>http://arxiv.org/abs/2503.00803v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云数据常常包含由于运动引起的失真，这会降低捕获数据中物体外观的准确性。&lt;h4&gt;问题&lt;/h4&gt;当前的研究主要关注于通过自车运动来处理点云失真，但忽视了其他移动对象造成的失真。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新的去扭曲流水线（HiMo），用于解决由车辆及周围环境中的动态对象引起的点云失真。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于场景流估计的方法来补偿物体运动，并扩展了一个最先进的自我监督场景流方法。&lt;h4&gt;性能评估&lt;/h4&gt;鉴于现有文献中缺乏成熟可靠的运动失真度量标准，论文提出了两个新的评价指标：点级补偿精度和对象形状相似性。&lt;h4&gt;实验数据&lt;/h4&gt;在Argoverse 2数据集以及新收集的用于展示方法有效性的实际道路场景数据集上进行了广泛的实验。新数据集来源于装备有多个LiDAR的重型车辆，在高速公路上行驶，不同于现有数据集中主要的城市环境设置。&lt;h4&gt;翻译&lt;/h4&gt;本文首先描述了点云失真的根本原因，并展示了这些失真在公共数据集中的存在。这种失真在高速公路等高速环境中更加明显，也出现在多LiDAR配置中——这是重型车辆的常见设置。之前的大多数研究只关注于通过自车运动来处理点云失真，但忽视了其他移动对象造成的失真。因此，我们提出了一种新的去扭曲流水线（HiMo），该流水线利用场景流估计来进行物体运动补偿，从而纠正动态对象的表现。此外，还对一种最先进的自我监督场景流方法进行了扩展。由于文献中缺乏成熟的点云失真度量标准，本文提出了两个评估指标：点级补偿精度和形状相似性以评价去扭曲的性能。为了证明所提出方法的有效性，在Argoverse 2数据集以及新收集的数据集上进行了广泛的实验，该数据集来源于装备有多个LiDAR的重型车辆，在高速公路上行驶，不同于现有数据集中主要的城市环境设置。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR point clouds often contain motion-induced distortions, degrading theaccuracy of object appearances in the captured data. In this paper, we firstcharacterize the underlying reasons for the point cloud distortion and showthat this is present in public datasets. We find that this distortion is morepronounced in high-speed environments such as highways, as well as inmulti-LiDAR configurations, a common setup for heavy vehicles. Previous workhas dealt with point cloud distortion from the ego-motion but fails to considerdistortion from the motion of other objects. We therefore introduce a novelundistortion pipeline, HiMo, that leverages scene flow estimation for objectmotion compensation, correcting the depiction of dynamic objects. We furtherpropose an extension of a state-of-the-art self-supervised scene flow method.Due to the lack of well-established motion distortion metrics in theliterature, we also propose two metrics for compensation performanceevaluation: compensation accuracy at a point level and shape similarity onobjects. To demonstrate the efficacy of our method, we conduct extensiveexperiments on the Argoverse 2 dataset and a new real-world dataset. Our newdataset is collected from heavy vehicles equipped with multi-LiDARs and onhighways as opposed to mostly urban settings in the existing datasets. Thesource code, including all methods and the evaluation data, will be providedupon publication. See https://kin-zhang.github.io/HiMo for more details.</description>
      <author>example@mail.com (Qingwen Zhang, Ajinkya Khoche, Yi Yang, Li Ling, Sina Sharif Mansouri, Olov Andersson, Patric Jensfelt)</author>
      <guid isPermaLink="false">2503.00803v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems</title>
      <link>http://arxiv.org/abs/2503.00383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了基于混淆的方法在保护中间特征隐私方面存在的不足，并提出了一种新的方法来量化冗余并增强模型对抗逆向工程攻击的能力。&lt;h4&gt;背景&lt;/h4&gt;协作推理使终端用户能够利用强大的深度学习模型而不暴露敏感的原始数据给云服务器。然而，最近的研究表明，中间特征可能不足以保证隐私，因为通过逆向建模攻击可以泄露信息和重建原始数据。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在提出一种方法来量化冗余，并建立它与增强逆向工程抵抗能力之间的数学关系。&lt;h4&gt;方法&lt;/h4&gt;论文证明了输入给定中间特征的条件熵提供了在任何逆向建模攻击下的重构均方误差（MSE）的一个保证下界。然后，基于高斯混合估计提出了一个可微分且可解的方法来界定这个条件熵，并提出了一种条件熵最大化（CEM）算法以增强逆向工程抵抗能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明了所提出的CEM在四个数据集上的有效性和适应性。将CEM嵌入到基于混淆的防御机制中，在不牺牲特征实用性和计算效率的情况下，可以显著提升其逆向工程抵抗能力，平均增益范围从12.9%至48.2%。&lt;h4&gt;结论&lt;/h4&gt;该研究通过理论分析和实验验证展示了如何利用条件熵最大化来增强模型对抗逆向建模攻击的能力。所提出的CEM方法为保护协作推理框架中的隐私提供了一个有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了研究背景、目的、方法及结果，揭示了一种新的技术——条件熵最大化（CEM），用于提高中间特征的抗逆向工程能力，从而进一步提升在深度学习模型中使用协作推理的安全性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; By locally encoding raw data into intermediate features, collaborativeinference enables end users to leverage powerful deep learning models withoutexposure of sensitive raw data to cloud servers. However, recent studies haverevealed that these intermediate features may not sufficiently preserveprivacy, as information can be leaked and raw data can be reconstructed viamodel inversion attacks (MIAs). Obfuscation-based methods, such as noisecorruption, adversarial representation learning, and information filters,enhance the inversion robustness by obfuscating the task-irrelevant redundancyempirically. However, methods for quantifying such redundancy remain elusive,and the explicit mathematical relation between this redundancy minimization andinversion robustness enhancement has not yet been established. To address that,this work first theoretically proves that the conditional entropy of inputsgiven intermediate features provides a guaranteed lower bound on thereconstruction mean square error (MSE) under any MIA. Then, we derive adifferentiable and solvable measure for bounding this conditional entropy basedon the Gaussian mixture estimation and propose a conditional entropymaximization (CEM) algorithm to enhance the inversion robustness. Experimentalresults on four datasets demonstrate the effectiveness and adaptability of ourproposed CEM; without compromising feature utility and computing efficiency,plugging the proposed CEM into obfuscation-based defense mechanismsconsistently boosts their inversion robustness, achieving average gains rangingfrom 12.9\% to 48.2\%. Code is available at\href{https://github.com/xiasong0501/CEM}{https://github.com/xiasong0501/CEM}.</description>
      <author>example@mail.com (Song Xia, Yi Yu, Wenhan Yang, Meiwen Ding, Zhuo Chen, Lingyu Duan, Alex C. Kot, Xudong Jiang)</author>
      <guid isPermaLink="false">2503.00383v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SAKE: Steering Activations for Knowledge Editing</title>
      <link>http://arxiv.org/abs/2503.01751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的知识编辑方法SAKE，该方法通过将需要修改的事实建模为分布而非单一提示来提高语言模型的知识更新效率和效果。&lt;h4&gt;背景&lt;/h4&gt;随着大型语言模型显示出记忆真实世界事实的能力，对这些模型进行可控且高效的知识更新变得越来越重要。然而，现有的知识编辑方法存在缺乏上下文鲁棒性等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的知识编辑方法SAKE，旨在解决现有知识编辑方法中的局限性和问题。&lt;h4&gt;方法&lt;/h4&gt;SAKE使用激活引导的方法，将要修改的事实建模为一个分布，并利用最优传输技术来调整大型语言模型的行为以应对整个事实相关的分布。&lt;h4&gt;主要发现&lt;/h4&gt;通过一系列数值实验，证实了SAKE在执行更稳健的知识编辑方面的有效性。它能够比现有的方法更好地处理上下文变化和逻辑推论问题。&lt;h4&gt;结论&lt;/h4&gt;SAKE是一种改进的知识编辑技术，能够在大型语言模型中实现更加有效和鲁棒的知识更新。&lt;h4&gt;翻译&lt;/h4&gt;随着大规模语言模型展现出记住现实世界事实的能力，有必要以一种受控且高效的方式对其进行知识更新。考虑到这些限制，知识编辑方法被提出用于修改预训练模型中的特定事实。然而，它们显示出缺乏上下文稳健性等若干局限，并且无法泛化到与事实相关的逻辑推论。为克服这些问题，我们提出了SAKE（基于激活引导的方法），该方法将要被修改的事实建模为一个分布而非单一提示。利用最优传输技术，SAKE能够调整大型语言模型在涉及整个事实相关分布时的行为，包括同义词和逻辑推论。通过多种数值实验表明了此方法的有效性：因此，与现有的方法相比，SAKE可以执行更稳健的知识编辑操作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As Large Langue Models have been shown to memorize real-world facts, the needto update this knowledge in a controlled and efficient manner arises. Designedwith these constraints in mind, Knowledge Editing (KE) approaches propose toalter specific facts in pretrained models. However, they have been shown tosuffer from several limitations, including their lack of contextual robustnessand their failure to generalize to logical implications related to the fact. Toovercome these issues, we propose SAKE, a steering activation method thatmodels a fact to be edited as a distribution rather than a single prompt.Leveraging Optimal Transport, SAKE alters the LLM behavior over a wholefact-related distribution, defined as paraphrases and logical implications.Several numerical experiments demonstrate the effectiveness of this method:SAKE is thus able to perform more robust edits than its existing counterparts.</description>
      <author>example@mail.com (Marco Scialanga, Thibault Laugel, Vincent Grari, Marcin Detyniecki)</author>
      <guid isPermaLink="false">2503.01751v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>STAR-Edge: Structure-aware Local Spherical Curve Representation for Thin-walled Edge Extraction from Unstructured Point Clouds</title>
      <link>http://arxiv.org/abs/2503.00801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;STAR-Edge是一种用于检测和细化薄壁结构中边缘点的新颖方法，通过独特的局部球面曲线表示创建结构感知的邻域。&lt;h4&gt;背景&lt;/h4&gt;从非结构化点云中提取几何边缘仍然是一个重大挑战，尤其是在常见的日常物体中的薄壁结构。传统的几何方法和最近的学习基于的方法在处理这些结构时经常遇到困难，因为两者都严重依赖于局部点邻居提供的足够的上下文信息。然而，薄壁结构的3D测量数据通常缺乏可靠边沿抽取所需的精确、密集且规则的邻域采样。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法STAR-Edge，旨在检测和细化薄壁结构中的边缘点，并通过独特的表示方式提高其在噪声和稀疏或不规则采样下的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;采用局部球面曲线表示创建关注结构的邻域，强调共面点同时减少来自附近非共平面表面的干扰。此表示被转换为旋转不变量描述符，并结合轻量级多层感知器用于边缘点分类。此外，利用局部球面曲线表示估计更精确的法线并向初步确定的边缘点引入优化函数进行投影。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明STAR-Edge在ABC数据集和薄壁结构特定的数据集中均优于现有的边缘检测方法，在各种挑战性条件下展示出更好的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;STAR-Edge提供了一种新的解决方案，能够在复杂且具有挑战性的环境中准确地提取薄壁结构中的几何边沿。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extracting geometric edges from unstructured point clouds remains asignificant challenge, particularly in thin-walled structures that are commonlyfound in everyday objects. Traditional geometric methods and recentlearning-based approaches frequently struggle with these structures, as bothrely heavily on sufficient contextual information from local pointneighborhoods. However, 3D measurement data of thin-walled structures oftenlack the accurate, dense, and regular neighborhood sampling required forreliable edge extraction, resulting in degraded performance.  In this work, we introduce STAR-Edge, a novel approach designed for detectingand refining edge points in thin-walled structures. Our method leverages aunique representation-the local spherical curve-to create structure-awareneighborhoods that emphasize co-planar points while reducing interference fromclose-by, non-co-planar surfaces. This representation is transformed into arotation-invariant descriptor, which, combined with a lightweight multi-layerperceptron, enables robust edge point classification even in the presence ofnoise and sparse or irregular sampling. Besides, we also use the localspherical curve representation to estimate more precise normals and introducean optimization function to project initially identified edge points exactly onthe true edges. Experiments conducted on the ABC dataset and thin-walledstructure-specific datasets demonstrate that STAR-Edge outperforms existingedge detection methods, showcasing better robustness under various challengingconditions.</description>
      <author>example@mail.com (Zikuan Li, Honghua Chen, Yuecheng Wang, Sibo Wu, Mingqiang Wei, Jun Wang)</author>
      <guid isPermaLink="false">2503.00801v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ECG-EmotionNet: Nested Mixture of Expert (NMoE) Adaptation of ECG-Foundation Model for Driver Emotion Recognition</title>
      <link>http://arxiv.org/abs/2503.01750v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种新的架构ECG-EmotionNet，用于在动态驾驶环境中进行驾驶员情绪识别。&lt;h4&gt;背景&lt;/h4&gt;驾驶员情绪识别对自动驾驶系统中的人员自主互动和信任度提升至关重要。心电图(ECG)因其实时监测能力和适应复杂驾驶环境的能力而成为最佳选择之一。然而，现有的方法通常依赖于静态条件下的多通道ECG信号，限制了其在真实动态场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的架构ECG-EmotionNet，以提高驾驶员情绪识别的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用单通道心电图(ECG)信号构建一个基于最近引入的心电图基础模型(FM)的新架构。通过嵌入式专家混合(MoE)适应机制来增强全局和局部ECG特征表示，而不是采用传统的全微调、线性探测或低秩适应方法。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在准确性和F1得分上分别提高了6%和7%，并且保持了计算效率。此外，使用最近引入的具有挑战性的驾驶员情绪监测数据集进行了评估。&lt;h4&gt;结论&lt;/h4&gt;ECG-EmotionNet架构显著改善了动态驾驶环境下的心电图情绪识别性能，为自动驾驶系统中的人机互动提供了坚实的支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文提及司机情感识别在监控系统中的重要性，并介绍了新的架构ECG-EmotionNet及其使用单通道心电图信号来增强驾驶员情绪监测的准确性和鲁棒性的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driver emotion recognition plays a crucial role in driver monitoring systems,enhancing human-autonomy interactions and the trustworthiness of AutonomousDriving (AD). Various physiological and behavioural modalities have beenexplored for this purpose, with Electrocardiogram (ECG) emerging as a standoutchoice for real-time emotion monitoring, particularly in dynamic andunpredictable driving conditions. Existing methods, however, often rely onmulti-channel ECG signals recorded under static conditions, limiting theirapplicability in real-world dynamic driving scenarios. To address thislimitation, the paper introduces ECG-EmotionNet, a novel architecture designedspecifically for emotion recognition in dynamic driving environments.ECG-EmotionNet is constructed by adapting a recently introduced ECG FoundationModel (FM) and uniquely employs single-channel ECG signals, ensuring bothrobust generalizability and computational efficiency. Unlike conventionaladaptation methods such as full fine-tuning, linear probing, or low-rankadaptation, we propose an intuitively pleasing alternative, referred to as thenested Mixture of Experts (MoE) adaptation. More precisely, each transformerlayer of the underlying FM is treated as a separate expert, with embeddingsextracted from these experts fused using trainable weights within a gatingmechanism. This approach enhances the representation of both global and localECG features, leading to a 6% improvement in accuracy and a 7% increase in theF1 score, all while maintaining computational efficiency. The effectiveness ofthe proposed ECG-EmotionNet architecture is evaluated using a recentlyintroduced and challenging driver emotion monitoring dataset.</description>
      <author>example@mail.com (Nastaran Mansourian, Arash Mohammadi, M. Omair Ahmad, M. N. S. Swamy)</author>
      <guid isPermaLink="false">2503.01750v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ICanC: Improving Camera-based Object Detection and Energy Consumption in Low-Illumination Environments</title>
      <link>http://arxiv.org/abs/2503.00709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 18 figures, to be published in IEEE MOST 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了一种名为ICanC的新系统，旨在通过利用激光雷达和摄像头传感器的互补能力来增强自动驾驶汽车在低光照环境下的物体检测能力和能效。&lt;h4&gt;背景&lt;/h4&gt;当前自动驾驶车辆在低光照环境下容易因相机性能下降而导致物体检测准确性降低，并且需要频繁使用前照灯以提高摄像机的可视性，这导致能源消耗增加。因此，有必要寻找一种既能确保可靠物体检测又能优化能耗的方法。&lt;h4&gt;目的&lt;/h4&gt;通过设计ICanC系统，在不牺牲可靠性的前提下减少不必要的头灯使用，从而实现更可持续的交通方式。&lt;h4&gt;方法&lt;/h4&gt;ICanC由三个主要部分组成：障碍物探测器、危险探测器和灯光控制器。其中，障碍物探测器处理激光雷达点云数据以拟合边界框并估计物体的位置、速度和方向；危险探测器评估潜在威胁；灯光控制器根据危险探测的结果动态开启前照灯。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在有显著噪声干扰的情况下，ICanC仍然表现出良好的性能，并且当开启头灯时能够实现基于摄像头的高精度物体检测。同时，整体能耗得到了显著降低。&lt;h4&gt;结论&lt;/h4&gt;作为自动驾驶汽车研究中的一个重要的进展，ICanC在确保可靠性和能效之间达到了一种平衡，展示了其在未来可持续交通中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;该论文介绍了一种名为 ICanC（发音为“我能看见”）的新型系统，旨在通过结合激光雷达和相机传感器的优势来提高自主车辆在低光照条件下的物体检测能力，并优化能耗。此方法确保了在传统相机性能下降的情况下也能提升检测精度并大幅降低不必要的前照灯使用，从而支持可持续交通的目标。ICanC系统包括三个主要组成部分：障碍物探测器、危险探测器和灯光控制器。通过实验验证，在真实和模拟环境中，即使存在显著的噪声干扰，该系统依然保持了出色的性能，展示了在实现可靠物体检测的同时大幅减少前照灯能耗的巨大潜力，这是自动驾驶车辆研究领域的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces ICanC (pronounced "I Can See"), a novel system designedto enhance object detection and optimize energy efficiency in autonomousvehicles (AVs) operating in low-illumination environments. By leveraging thecomplementary capabilities of LiDAR and camera sensors, ICanC improvesdetection accuracy under conditions where camera performance typicallydeclines, while significantly reducing unnecessary headlight usage. Thisapproach aligns with the broader objective of promoting sustainabletransportation.  ICanC comprises three primary nodes: the Obstacle Detector, which processesLiDAR point cloud data to fit bounding boxes onto detected objects and estimatetheir position, velocity, and orientation; the Danger Detector, which evaluatespotential threats using the information provided by the Obstacle Detector; andthe Light Controller, which dynamically activates headlights to enhance cameravisibility solely when a threat is detected.  Experiments conducted in physical and simulated environments demonstrateICanC's robust performance, even in the presence of significant noiseinterference. The system consistently achieves high accuracy in camera-basedobject detection when headlights are engaged, while significantly reducingoverall headlight energy consumption. These results position ICanC as apromising advancement in autonomous vehicle research, achieving a balancebetween energy efficiency and reliable object detection.</description>
      <author>example@mail.com (Daniel Ma, Ren Zhong, Weisong Shi)</author>
      <guid isPermaLink="false">2503.00709v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens</title>
      <link>http://arxiv.org/abs/2503.01710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ACL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近的大规模语言模型（LLMs）在零样本文本到语音（TTS）合成方面取得了显著进展。然而，现有的基础模型依赖于多阶段处理或复杂的架构来预测多个码本，这限制了效率和集成灵活性。&lt;h4&gt;背景&lt;/h4&gt;当前的零样本TTS系统主要面临两大挑战：一是复杂且低效的架构，二是受限的控制能力，难以实现精确的语言内容与说话人属性分离。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有技术的局限性，本文介绍了Spark-TTS系统，旨在通过引入新的单流语音编解码器BiCodec来提升TTS模型的效率和灵活性，并支持精细的音调调整。&lt;h4&gt;方法&lt;/h4&gt;Spark-TTS基于BiCodec，该编码器将语音分解为两种互补的标记类型：用于语言内容的低比特率语义令牌和固定长度的全局令牌（代表说话人属性）。此外，该系统还采用了Qwen2.5 LLM以及一种称为“链式思维”（CoT）的生成方法。&lt;h4&gt;主要发现&lt;/h4&gt;Spark-TTS不仅实现了最先进的零样本语音克隆效果，还能生成高度可定制的声音，超越了基于参考的合成的限制。为促进可控性TTS研究，本文还引入了一个精心策划的数据集VoxBox，它包含10万小时录音及全面属性标注。&lt;h4&gt;结论&lt;/h4&gt;Spark-TTS展示了在效率、灵活性和控制精确度方面的显著改进，并通过详细的实验验证了其优越性能。&lt;h4&gt;翻译&lt;/h4&gt;近期大规模语言模型(LLMs)的重大进展促进了零样本文本到语音(TTS)合成技术的显著进步。然而，现有的基础模型依赖于多阶段处理或复杂架构来预测多个码本，从而限制了效率和集成灵活性。为解决这一问题，我们引入了Spark-TTS系统，该系统由BiCodec驱动，这是一种单流语音编解码器，将语音分解成两种互补的标记类型：用于语言内容的低比特率语义标记以及固定长度的全局标记(代表说话人属性)。结合Qwen2.5 LLM和链式思维(CoT)生成方法，Spark-TTS能够实现粗粒度控制（如性别、讲话风格）及细粒度调整（如精确音高值、讲话速率）。为了促进可控性TTS研究，我们引入了VoxBox数据集，这是一个精心策划的10万小时录音库，包含全面的属性标注。大量实验表明，Spark-TTS不仅在零样本语音克隆方面达到业界领先水平，并且生成的高度定制化声音超出了基于参考合成技术的限制。该系统的源代码、预训练模型和音频样本可从https://github.com/SparkAudio/Spark-TTS获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in large language models (LLMs) have driven significantprogress in zero-shot text-to-speech (TTS) synthesis. However, existingfoundation models rely on multi-stage processing or complex architectures forpredicting multiple codebooks, limiting efficiency and integration flexibility.To overcome these challenges, we introduce Spark-TTS, a novel system powered byBiCodec, a single-stream speech codec that decomposes speech into twocomplementary token types: low-bitrate semantic tokens for linguistic contentand fixed-length global tokens for speaker attributes. This disentangledrepresentation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT)generation approach, enables both coarse-grained control (e.g., gender,speaking style) and fine-grained adjustments (e.g., precise pitch values,speaking rate). To facilitate research in controllable TTS, we introduceVoxBox, a meticulously curated 100,000-hour dataset with comprehensiveattribute annotations. Extensive experiments demonstrate that Spark-TTS notonly achieves state-of-the-art zero-shot voice cloning but also generateshighly customizable voices that surpass the limitations of reference-basedsynthesis. Source code, pre-trained models, and audio samples are available athttps://github.com/SparkAudio/Spark-TTS.</description>
      <author>example@mail.com (Xinsheng Wang, Mingqi Jiang, Ziyang Ma, Ziyu Zhang, Songxiang Liu, Linqin Li, Zheng Liang, Qixi Zheng, Rui Wang, Xiaoqin Feng, Weizhen Bian, Zhen Ye, Sitong Cheng, Ruibin Yuan, Zhixian Zhao, Xinfa Zhu, Jiahao Pan, Liumeng Xue, Pengcheng Zhu, Yunlin Chen, Zhifei Li, Xie Chen, Lei Xie, Yike Guo, Wei Xue)</author>
      <guid isPermaLink="false">2503.01710v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via Symbolic Code Generation</title>
      <link>http://arxiv.org/abs/2503.01700v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;最近的工作展示了大型语言模型在机器人任务和运动规划中的巨大潜力。然而，当前的方法没有充分利用这些模型的符号计算能力和代码生成能力。&lt;h4&gt;背景&lt;/h4&gt;现有的LLM方法通常产生包含子目标和行动计划的文本或代码推理链。对于涉及多重约束下的复杂优化问题的任务，纯文本推理显得不足。&lt;h4&gt;目的&lt;/h4&gt;为了提高LLM在任务和运动规划中的性能并增强其泛化性，我们提出了一种通过指导模型生成符号计算所需的代码来改进TAMP能力的方法。&lt;h4&gt;方法&lt;/h4&gt;与以往的工作不同，我们的工作使LLM生成用于解决、计划和验证的代码，同时利用文本推理融入常识。采用了多轮引导和答案进化框架以提升任务成功几率。&lt;h4&gt;主要发现&lt;/h4&gt;通过在七项典型任务上进行测试并与现有基准方法比较，提出的Code-as-Symbolic-Planner平均提高了24.1%的成功率，并且显示了在离散和连续环境、二维/三维模拟以及真实世界设置中的强效性和泛化性。&lt;h4&gt;结论&lt;/h4&gt;我们的研究证明了LLM生成代码作为符号规划器的可行性和有效性，这对机器人任务和运动规划领域具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究展示了大型语言模型在机器人任务与运动规划领域的潜力。当前的方法主要依赖于文本或代码推理链，但未充分使用语言模型的代码生成能力。我们提出了一种新的方法——Code-as-Symbolic-Planner，通过指导LLM生成解决和验证问题所需的代码来提高其性能，并且证明了该方法在多种任务中的有效性与泛化性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent works have shown great potentials of Large Language Models (LLMs) inrobot task and motion planning (TAMP). Current LLM approaches generate text- orcode-based reasoning chains with sub-goals and action plans. However, they donot fully leverage LLMs' symbolic computing and code generation capabilities.Many robot TAMP tasks involve complex optimization under multiple constraints,where pure textual reasoning is insufficient. While augmenting LLMs withpredefined solvers and planners improves performance, it lacks generalizationacross tasks. Given LLMs' growing coding proficiency, we enhance their TAMPcapabilities by steering them to generate code as symbolic planners foroptimization and constraint verification. Unlike prior work that uses code tointerface with robot action modules, we steer LLMs to generate code as solvers,planners, and checkers for TAMP tasks requiring symbolic computing, while stillleveraging textual reasoning to incorporate common sense. With a multi-roundguidance and answer evolution framework, the proposed Code-as-Symbolic-Plannerimproves success rates by average 24.1\% over best baseline methods acrossseven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner showsstrong effectiveness and generalizability across discrete and continuousenvironments, 2D/3D simulations and real-world settings, as well as single- andmulti-robot tasks with diverse requirements. See our project websitehttps://yongchao98.github.io/Code-Symbol-Planner/ for prompts, videos, andcode.</description>
      <author>example@mail.com (Yongchao Chen, Yilun Hao, Yang Zhang, Chuchu Fan)</author>
      <guid isPermaLink="false">2503.01700v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning via Modality Alignment and Retention</title>
      <link>http://arxiv.org/abs/2503.00374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;病理学和转录组学在肿瘤学中是两个基本的研究手段，它们分别涵盖了疾病形态学和分子层面的信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多模态表示学习方法（MIRROR），旨在促进不同模式之间的对齐同时保留各自独特的特征。&lt;h4&gt;方法&lt;/h4&gt;该方法使用特定的编码器提取每个模式的全面特征，并通过模式对齐模块实现表型模式与分子谱图的无缝集成。此外，还包含一个模式保持模块来保护各个模式的独特属性，以及一个风格聚类模块来减少冗余并增强疾病相关信息。&lt;h4&gt;主要发现&lt;/h4&gt;在TCGA队列中的癌症亚型分类和生存分析中进行了广泛的评估，结果表明MIRROR方法具有优越的表现，在构建综合的肿瘤学特征表示方面效果显著。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了MIRROR能够在不同模式间进行有效的对齐和保持各自特有的结构，有助于改进癌症诊断的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译已经完成，并且转换为了JSON格式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/TianyiFranklinWang/MIRROR&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Histopathology and transcriptomics are fundamental modalities in oncology,encapsulating the morphological and molecular aspects of the disease.Multi-modal self-supervised learning has demonstrated remarkable potential inlearning pathological representations by integrating diverse data sources.Conventional multi-modal integration methods primarily emphasize modalityalignment, while paying insufficient attention to retaining themodality-specific structures. However, unlike conventional scenarios wheremulti-modal inputs share highly overlapping features, histopathology andtranscriptomics exhibit pronounced heterogeneity, offering orthogonal yetcomplementary insights. Histopathology provides morphological and spatialcontext, elucidating tissue architecture and cellular topology, whereastranscriptomics delineates molecular signatures through gene expressionpatterns. This inherent disparity introduces a major challenge in aligning themwhile maintaining modality-specific fidelity. To address these challenges, wepresent MIRROR, a novel multi-modal representation learning method designed tofoster both modality alignment and retention. MIRROR employs dedicated encodersto extract comprehensive features for each modality, which is furthercomplemented by a modality alignment module to achieve seamless integrationbetween phenotype patterns and molecular profiles. Furthermore, a modalityretention module safeguards unique attributes from each modality, while a styleclustering module mitigates redundancy and enhances disease-relevantinformation by modeling and aligning consistent pathological signatures withina clustering space. Extensive evaluations on TCGA cohorts for cancer subtypingand survival analysis highlight MIRROR's superior performance, demonstratingits effectiveness in constructing comprehensive oncological featurerepresentations and benefiting the cancer diagnosis.</description>
      <author>example@mail.com (Tianyi Wang, Jianan Fan, Dingxin Zhang, Dongnan Liu, Yong Xia, Heng Huang, Weidong Cai)</author>
      <guid isPermaLink="false">2503.00374v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MUSt3R: Multi-view Network for Stereo 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2503.01661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了MUSt3R，它是DUSt3R的多视图扩展版本，用于解决大规模图像集合中密集且无约束的立体3D重建问题。&lt;h4&gt;背景&lt;/h4&gt;现有方法如DUSt3R在处理任意图像集合进行立体3D重建时效果良好，但当面对大量图像时，由于需要处理成对图像的数量呈二次增长，导致计算复杂度急剧增加。这限制了其在大规模数据集上的应用和优化效率。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多视图网络MUSt3R来解决DUSt3R中存在的问题，旨在提升大规模图像集合的立体3D重建性能，并保持高帧率下的计算效率。&lt;h4&gt;方法&lt;/h4&gt;通过将DUSt3R架构对称化并扩展到直接预测所有视角在共同坐标系中的3D结构，同时引入多层次内存机制以降低复杂度。该模型能够处理在线和离线场景中的SfM（Simultaneous Localization and Mapping）以及视觉SLAM问题。&lt;h4&gt;主要发现&lt;/h4&gt;MUSt3R通过有效减少计算量并提高重建效率，在各种下游任务中显示出超越现有方法的性能，包括未校准视觉里程计、相对相机姿态估计、尺度和焦距估算等。&lt;h4&gt;结论&lt;/h4&gt;所提出的MUSt3R架构解决了DUSt3R在处理大规模图像集合时存在的局限性，并为实时高性能立体3D重建提供了一种有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到，DUSt3R提出了一种新的几何计算机视觉范式，即可以对任意图像集进行密集和无约束的立体3D重建。然而，在内部处理成对图像并回归局部3D重构时会产生计算复杂度问题，尤其在大量图片集合的情况下。本文介绍了一种基于多视图网络（MUSt3R）的方法，解决了上述所有问题，并展示了其在视觉里程计、相对相机姿态估计等方面的优越性能和效率提升能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; DUSt3R introduced a novel paradigm in geometric computer vision by proposinga model that can provide dense and unconstrained Stereo 3D Reconstruction ofarbitrary image collections with no prior information about camera calibrationnor viewpoint poses. Under the hood, however, DUSt3R processes image pairs,regressing local 3D reconstructions that need to be aligned in a globalcoordinate system. The number of pairs, growing quadratically, is an inherentlimitation that becomes especially concerning for robust and fast optimizationin the case of large image collections. In this paper, we propose an extensionof DUSt3R from pairs to multiple views, that addresses all aforementionedconcerns. Indeed, we propose a Multi-view Network for Stereo 3D Reconstruction,or MUSt3R, that modifies the DUSt3R architecture by making it symmetric andextending it to directly predict 3D structure for all views in a commoncoordinate frame. Second, we entail the model with a multi-layer memorymechanism which allows to reduce the computational complexity and to scale thereconstruction to large collections, inferring thousands of 3D pointmaps athigh frame-rates with limited added complexity. The framework is designed toperform 3D reconstruction both offline and online, and hence can be seamlesslyapplied to SfM and visual SLAM scenarios showing state-of-the-art performanceon various 3D downstream tasks, including uncalibrated Visual Odometry,relative camera pose, scale and focal estimation, 3D reconstruction andmulti-view depth estimation.</description>
      <author>example@mail.com (Yohann Cabon, Lucas Stoffl, Leonid Antsfeld, Gabriela Csurka, Boris Chidlovskii, Jerome Revaud, Vincent Leroy)</author>
      <guid isPermaLink="false">2503.01661v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ecg2o: A Seamless Extension of g2o for Equality-Constrained Factor Graph Optimization</title>
      <link>http://arxiv.org/abs/2503.01311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的方法，通过扩展因子图来无缝地集成等式约束，从而提高了解决方案的精度，并拓宽了其在最优控制领域的应用。&lt;h4&gt;背景&lt;/h4&gt;因子图优化作为一种基础框架用于机器人感知领域，可以应用于姿态估计、同时定位与地图构建（SLAM）、结构从运动恢复（SfM）和态势感知。传统的方法使用如高斯-牛顿或莱文贝格-马夸特等算法解决无约束最小二乘问题。&lt;h4&gt;目的&lt;/h4&gt;在不增加额外优化算法的情况下，引入一种能够直接支持等式约束的因子图扩展方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的因子图扩展方式，该方法保持了现有二次优化技术的效率和灵活性，并确保了约束可行性。使用ecg2o库验证此方法的有效性，该库是基于广泛使用的g2o因子图库开发的，并为等式受限优化提供了全面支持。&lt;h4&gt;主要发现&lt;/h4&gt;在自主车辆速度跟踪的最优控制问题中应用该方法后，与当前最先进的约束处理技术相比，本研究提出的解决方案表现出更高的精确度和可靠性。&lt;h4&gt;结论&lt;/h4&gt;通过扩展g2o因子图库以支持等式约束，并且提供了开源示例代码和用于验证的新颖优化算法，证明了这种新方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：因子图优化是机器人感知领域的一个基础框架，可以应用于姿态估计、同时定位与地图构建（SLAM）、结构从运动恢复（SfM）及态势感知。传统的方法使用如高斯-牛顿或莱文贝格-马夸特等算法解决无约束最小二乘问题。然而，通过原生支持等式约束来扩展因子图可以提高解决方案的准确性并拓宽其应用范围，尤其是在最优控制领域中。本文提出了一种新颖的因子图扩展方法，在不使用额外优化算法的情况下直接集成等式约束。这种方法保持了现有二次优化技术的效率和灵活性，并确保了约束可行性。为了验证该方法的有效性，将其应用于自主车辆速度跟踪的最佳控制问题，并将结果与当前最先进的约束处理技术进行了比较。此外，还介绍了ecg2o库——一个头文件形式的C++库，它扩展了广泛使用的g2o因子图库，增加了对等式受限优化的支持。此库、示例代码以及最优控制问题可以在https://github.com/snt-arg/ecg2o上作为开源资源获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Factor graph optimization serves as a fundamental framework for roboticperception, enabling applications such as pose estimation, simultaneouslocalization and mapping (SLAM), structure-from-motion (SfM), and situationalawareness. Traditionally, these methods solve unconstrained least squaresproblems using algorithms such as Gauss-Newton and Levenberg-Marquardt.However, extending factor graphs with native support for equality constraintscan improve solution accuracy and broaden their applicability, particularly inoptimal control. In this paper, we propose a novel extension of factor graphsthat seamlessly incorporates equality constraints without requiring additionaloptimization algorithms. Our approach maintains the efficiency and flexibilityof existing second-order optimization techniques while ensuring constraintfeasibility. To validate our method, we apply it to an optimal control problemfor velocity tracking in autonomous vehicles and benchmark our results againststate-of-the-art constraint handling techniques. Additionally, we introduceecg2o, a header-only C++ library that extends the widely used g2o factor graphlibrary by adding full support for equality-constrained optimization. Thislibrary, along with demonstrative examples and the optimal control problem, isavailable as open source at https://github.com/snt-arg/ecg2o</description>
      <author>example@mail.com (Anas Abdelkarim, Holger Voos, Daniel Görges)</author>
      <guid isPermaLink="false">2503.01311v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Sensor Fusion Approach for Rapid Orthoimage Generation in Large-Scale UAV Mapping</title>
      <link>http://arxiv.org/abs/2503.01202v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究介绍了一种使用多传感器无人机系统的新型大范围正射影像快速生成技术，该系统集成了GPS、IMU、毫米波雷达和相机。通过利用这些数据源，可以提高传统正射影像生成方法在时间性能、系统鲁棒性和地理参考准确性方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;基于无人机(UAV)的大规模正射影像的快速生成一直是航空摄影领域的研究重点。&lt;h4&gt;目的&lt;/h4&gt;克服现有技术中的瓶颈问题，提出一种利用多传感器数据来提高传统正射影像生成方法的技术方案。&lt;h4&gt;方法&lt;/h4&gt;引入了一种先姿态优化特征匹配的方法以提升匹配速度和准确性，并减少所需的特征数量。这种方法特别适用于低纹理场景（如农田），在此类场景中常规的特征匹配困难。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在准确性和时间效率方面都表现出色，特别是在农田检测与管理方面的应用效果良好。&lt;h4&gt;结论&lt;/h4&gt;所提出的无人机系统能够有效支持农田的识别和管理工作，并且证明了其在低纹理环境中生成高精度正射影像的能力。&lt;h4&gt;翻译&lt;/h4&gt;快速生成基于无人飞行器(UAV)的大规模正射影像一直是航空摄影领域研究的重点。本文提出了一种结合GPS、IMU、毫米波雷达和相机等多传感器数据的无人机系统解决方案，以克服传统正射影像生成方法在时间性能、系统鲁棒性和地理参考准确性方面的局限性。通过先姿态优化特征匹配的方法增强了匹配速度和精度，并减少了所需的特征数量，为结构从运动(SfM)过程提供了精确的参照依据。该技术特别适用于低纹理场景（如农田），这些场景中常规的特征匹配通常比较困难。实验结果显示，我们的方法能够在短时间内实现高准确度的正射影像生成，证明了其在有效支持农田识别和管理工作中的实际应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid generation of large-scale orthoimages from Unmanned Aerial Vehicles(UAVs) has been a long-standing focus of research in the field of aerialmapping. A multi-sensor UAV system, integrating the Global Positioning System(GPS), Inertial Measurement Unit (IMU), 4D millimeter-wave radar and camera,can provide an effective solution to this problem. In this paper, we utilizemulti-sensor data to overcome the limitations of conventional orthoimagegeneration methods in terms of temporal performance, system robustness, andgeographic reference accuracy. A prior-pose-optimized feature matching methodis introduced to enhance matching speed and accuracy, reducing the number ofrequired features and providing precise references for the Structure fromMotion (SfM) process. The proposed method exhibits robustness in low-texturescenes like farmlands, where feature matching is difficult. Experiments showthat our approach achieves accurate feature matching orthoimage generation in ashort time. The proposed drone system effectively aids in farmland detectionand management.</description>
      <author>example@mail.com (Jialei He, Zhihao Zhan, Zhituo Tu, Xiang Zhu, Jie Yuan)</author>
      <guid isPermaLink="false">2503.01202v3</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MTReD: 3D Reconstruction Dataset for Fly-over Videos of Maritime Domain</title>
      <link>http://arxiv.org/abs/2503.00853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  WACV Workshop 2025 - 3rd Workshop on Maritime Computer Vision  (MaCVI2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的海洋3D场景重建基准数据集MTReD，旨在解决视频航拍视角下的海上三维场景重建问题，并提供了初步的评估方法。&lt;h4&gt;背景&lt;/h4&gt;当前没有专门针对海上环境的3D场景重建的数据集。传统的感知度量标准如LPIPS无法准确衡量重建图像的完整性。&lt;h4&gt;目的&lt;/h4&gt;创建一个新颖的海洋3D场景重建基准数据集MTReD，以促进这一领域的研究和开发。&lt;h4&gt;方法&lt;/h4&gt;提出了一个新的语义相似性度量DiFPS来评估重建的质量。使用了两种基线模型进行初步评估，并探索了一些预处理方法来提高结果。&lt;h4&gt;主要发现&lt;/h4&gt;MASt3R模型在感知度量上有更好的表现，但在重投影误差上不如SfM模型。通过适当的预处理技术可以同时改进这两种评价指标。&lt;h4&gt;结论&lt;/h4&gt;希望MTReD数据集能够推动未来在这个方向上的研究，并鼓励更多的研究人员参与到这个领域中来。&lt;h4&gt;翻译&lt;/h4&gt;这项工作解决了海上领域的视频航拍视角下的3D场景重建问题，重点在于几何一致性与视觉完整性。这将使下游任务如分割、导航和定位成为可能。目前没有专门为此领域的数据集存在。因此，我们提出了一个新的海洋3D场景重建基准测试数据集MTReD（Maritime Three-Dimensional Reconstruction Dataset）。该数据集包含19段从互联网收集的视频片段，其中包括船舰、岛屿以及海岸线等元素。由于任务目标在于几何一致性与视觉完整性，该数据集采用两种度量标准：重投影误差和感知度量。我们发现现有的基于感知度量的方法如LPIPS并不适合衡量重建图像的整体性，因此提出了一种新的利用DINOv2特征的语义相似度度量DiFPS（DinoV2 Features Perception Similarity）。我们在两个基线模型上进行了初步评估：通过Colmap实现的结构从运动（SfM）以及最近的研究前沿MASt3R模型。结果表明，相比于SfM，基于MASt3R重建出的场景在重投影误差更高但感知度量得分更好。因此我们探索了一些预处理方法，并发现了一种能够同时提高重投影误差和感知度量得分的方法。我们认为MTReD数据集将促进该领域的进一步研究发展。所有数据集及代码将在https://github.com/RuiYiYong/MTReD公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work tackles 3D scene reconstruction for a video fly-over perspectiveproblem in the maritime domain, with a specific emphasis on geometrically andvisually sound reconstructions. This will allow for downstream tasks such assegmentation, navigation, and localization. To our knowledge, there is nodataset available in this domain. As such, we propose a novel maritime 3D scenereconstruction benchmarking dataset, named as MTReD (Maritime Three-DimensionalReconstruction Dataset). The MTReD comprises 19 fly-over videos curated fromthe Internet containing ships, islands, and coastlines. As the task is aimedtowards geometrical consistency and visual completeness, the dataset uses twometrics: (1) Reprojection error; and (2) Perception based metrics. We find thatexisting perception-based metrics, such as Learned Perceptual Image PatchSimilarity (LPIPS), do not appropriately measure the completeness of areconstructed image. Thus, we propose a novel semantic similarity metricutilizing DINOv2 features coined DiFPS (DinoV2 Features Perception Similarity).We perform initial evaluation on two baselines: (1) Structured from Motion(SfM) through Colmap; and (2) the recent state-of-the-art MASt3R model. We findthat the reconstructed scenes by MASt3R have higher reprojection errors, butsuperior perception based metric scores. To this end, some pre-processingmethods are explored, and we find a pre-processing method which improves boththe reprojection error and perception-based score. We envisage our proposedMTReD to stimulate further research in these directions. The dataset and allthe code will be made available in https://github.com/RuiYiYong/MTReD.</description>
      <author>example@mail.com (Rui Yi Yong, Samuel Picosson, Arnold Wiliem)</author>
      <guid isPermaLink="false">2503.00853v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction</title>
      <link>http://arxiv.org/abs/2503.03734v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OTTER是一种新颖的Vision-Language-Action (VLA) 模型，它通过显式的、文本感知的视觉特征提取方法来利用现有预训练模型中视觉和语言之间的语义对齐。&lt;h4&gt;背景&lt;/h4&gt;现有的VLA模型在预测基于视觉观察和语言指令的机器人动作时需要微调预训练的视觉-语言模型，并且由于视觉和语言特性独立地输入下游策略，这会破坏预先训练好的语义对齐。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来利用现有预训练模型中的语义对齐，同时保持预训练编码器不变。&lt;h4&gt;方法&lt;/h4&gt;OTTER选择性提取并传递与语言指令语义一致的任务相关视觉特征到策略变换器中。这使得可以在不调整预训练的视觉-语言编码器的情况下进行操作。&lt;h4&gt;主要发现&lt;/h4&gt;通过在仿真和真实世界实验中的表现，证明了OTTER能够显著优于现有VLA模型，并且具有强大的零样本泛化能力，可以推广到新颖的对象和环境中。&lt;h4&gt;结论&lt;/h4&gt;OTTER通过保持预训练视觉-语言编码器的冻结状态，保留并利用大型数据集预训练中学习的丰富语义理解，使得在新对象和环境中的零样本泛化成为可能。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action (VLA) 模型旨在根据视觉观察和语言指令预测机器人动作。现有方法要求微调预训练的视觉语言模型（VLM），因为独立处理的视觉和语言特征会被输入到下游策略中，从而破坏了预训练中的语义对齐。我们提出了OTTER，这是一种新的VLA架构，通过显式的、文本感知的视觉特性提取来利用这些现有的对齐关系。与加工所有视觉特征相反，OTTER选择性地抽取并传递任务相关的且与语言指令语义一致的视觉特征到策略变换器中。这使得OTTER能够保持预训练视觉-语言编码器处于冻结状态。因此，OTTER保留和利用了大规模预训练中学到的丰富语义理解能力，从而具备强大的零样本泛化能力。在仿真和真实世界实验中，OTTER显著优于现有的VLA模型，在新对象和环境中的零样本泛化效果出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models aim to predict robotic actions based onvisual observations and language instructions. Existing approaches requirefine-tuning pre-trained visionlanguage models (VLMs) as visual and languagefeatures are independently fed into downstream policies, degrading thepre-trained semantic alignments. We propose OTTER, a novel VLA architecturethat leverages these existing alignments through explicit, text-aware visualfeature extraction. Instead of processing all visual features, OTTERselectively extracts and passes only task-relevant visual features that aresemantically aligned with the language instruction to the policy transformer.This allows OTTER to keep the pre-trained vision-language encoders frozen.Thereby, OTTER preserves and utilizes the rich semantic understanding learnedfrom large-scale pre-training, enabling strong zero-shot generalizationcapabilities. In simulation and real-world experiments, OTTER significantlyoutperforms existing VLA models, demonstrating strong zeroshot generalizationto novel objects and environments. Video, code, checkpoints, and dataset:https://ottervla.github.io/.</description>
      <author>example@mail.com (Huang Huang, Fangchen Liu, Letian Fu, Tingfan Wu, Mustafa Mukadam, Jitendra Malik, Ken Goldberg, Pieter Abbeel)</author>
      <guid isPermaLink="false">2503.03734v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Revolutionizing Traffic Management with AI-Powered Machine Vision: A Step Toward Smart Cities</title>
      <link>http://arxiv.org/abs/2503.02967v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 1 figure, 2 tables, accepted to 1th AITC conference in  University Of Isfahan&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索了人工智能和机器视觉技术在交通系统中的应用，通过高级监控摄像头和深度学习算法实现车辆实时检测、交通异常及驾驶行为识别。该系统整合地理空间数据和天气信息以适应不同的环境条件。&lt;h4&gt;背景&lt;/h4&gt;城市化进程加快和车辆拥堵问题对交通安全管理和效率提出了重大挑战。&lt;h4&gt;目的&lt;/h4&gt;探讨AI与机器视觉技术在革命性变革交通系统的潜力，优化交通流并提高道路安全。&lt;h4&gt;方法&lt;/h4&gt;利用YOLOv8和YOLOv11模型进行高精度的车辆检测及异常识别。结合地理空间信息和天气数据来增强系统适应性和性能。&lt;h4&gt;主要发现&lt;/h4&gt;研究实现了高度准确的车辆检测和异常认知，提高了交通流畅度与安全性。&lt;h4&gt;结论&lt;/h4&gt;研究成果为智能交通管理解决方案的发展提供了支持，并推动了智慧城市建设的目标——实现可持续且高效的都市基础设施。&lt;h4&gt;翻译&lt;/h4&gt;随着城市化的加速及汽车拥堵问题加剧，对交通安全管理和效率提出了重大挑战。这项研究探讨了人工智能和机器视觉技术在交通系统中的革命性潜力，利用先进的监控摄像头和深度学习算法提出了一套车辆实时检测、交通异常以及驾驶行为识别的解决方案。该方案结合地理空间数据和天气信息以动态适应不同环境条件，确保各种场景下的稳健表现。通过使用YOLOv8及YOLOv11模型，研究达到了高精度的车辆与异常探测水平，优化了交通流，并提升了道路安全性。这些发现有助于智能交通管理解决方案的发展，并符合创建智慧城市的愿景——建立具有可持续性和高效性的城市基础设施。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid urbanization of cities and increasing vehicular congestion haveposed significant challenges to traffic management and safety. This studyexplores the transformative potential of artificial intelligence (AI) andmachine vision technologies in revolutionizing traffic systems. By leveragingadvanced surveillance cameras and deep learning algorithms, this researchproposes a system for real-time detection of vehicles, traffic anomalies, anddriver behaviors. The system integrates geospatial and weather data to adaptdynamically to environmental conditions, ensuring robust performance in diversescenarios. Using YOLOv8 and YOLOv11 models, the study achieves high accuracy invehicle detection and anomaly recognition, optimizing traffic flow andenhancing road safety. These findings contribute to the development ofintelligent traffic management solutions and align with the vision of creatingsmart cities with sustainable and efficient urban infrastructure.</description>
      <author>example@mail.com (Seyed Hossein Hosseini DolatAbadi, Sayyed Mohammad Hossein Hashemi, Mohammad Hosseini, Moein-Aldin AliHosseini)</author>
      <guid isPermaLink="false">2503.02967v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Active 6D Pose Estimation for Textureless Objects using Multi-View RGB Frames</title>
      <link>http://arxiv.org/abs/2503.03726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于RGB图像估计无纹理物体6D姿态的主动感知框架。&lt;h4&gt;背景&lt;/h4&gt;单视图6D姿态估计在处理具有外观模糊性、旋转对称性和严重遮挡的对象时存在局限性，需要研究多视角姿态估计和最佳下视点预测方法来克服这些问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种使用RGB图像进行无纹理物体6D姿态估计的综合主动感知框架。&lt;h4&gt;方法&lt;/h4&gt;通过将6D姿态估计分解为两个步骤的过程：首先估算每个对象的3D平移，解决RGB图像中的尺度和深度模糊问题；然后使用简化后的任务来确定3D方向。引入了预测最佳下视角以捕捉RGB图像的策略，从而减少物体姿势不确定性并提高精度。&lt;h4&gt;主要发现&lt;/h4&gt;在公共ROBI数据集以及自建透明对象数据集中进行了实验验证，多视图姿态估计方法的表现优于现有的先进方法；通过利用最佳下视点策略，该方法能够在比启发式策略少得多的视角中实现高精度物体姿势准确性。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架为解决无纹理物体6D姿态估计问题提供了一种有效的方法，能够显著提高姿态估算的准确性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the 6D pose of textureless objects from RBG images is an importantproblem in robotics. Due to appearance ambiguities, rotational symmetries, andsevere occlusions, single-view based 6D pose estimators are still unable tohandle a wide range of objects, motivating research towards multi-view poseestimation and next-best-view prediction that addresses these limitations. Inthis work, we propose a comprehensive active perception framework forestimating the 6D poses of textureless objects using only RGB images. Ourapproach is built upon a key idea: decoupling the 6D pose estimation into asequential two-step process can greatly improve both accuracy and efficiency.First, we estimate the 3D translation of each object, resolving scale and depthambiguities inherent to RGB images. These estimates are then used to simplifythe subsequent task of determining the 3D orientation, which we achieve throughcanonical scale template matching. Building on this formulation, we thenintroduce an active perception strategy that predicts the next best cameraviewpoint to capture an RGB image, effectively reducing object pose uncertaintyand enhancing pose accuracy. We evaluate our method on the public ROBI datasetas well as on a transparent object dataset that we created. When evaluatedusing the same camera viewpoints, our multi-view pose estimation significantlyoutperforms state-of-the-art approaches. Furthermore, by leveraging ournext-best-view strategy, our method achieves high object pose accuracy withsubstantially fewer viewpoints than heuristic-based policies.</description>
      <author>example@mail.com (Jun Yang, Wenjie Xue, Sahar Ghavidel, Steven L. Waslander)</author>
      <guid isPermaLink="false">2503.03726v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Cali Anything: Dense Feature Multi-Frame Structure-from-Motion for Large-Scale Camera Array Calibration</title>
      <link>http://arxiv.org/abs/2503.00737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于密集特征的多帧校准方法，这种方法可以从场景数据中直接优化相机内部参数，从而消除了对额外校准拍摄的需求。该方法增强了传统的结构从运动（SfM）流程，并通过引入外参正则化项、密集特征重投影项和内参方差项来进行多帧联合优化。&lt;h4&gt;背景&lt;/h4&gt;大规模摄像机阵列的标定是耗时的过程，通常需要专门捕捉已知图案进行。虽然这些设置中的外部参数由于物理结构固定不变，但内部参数在不同会话中可能因镜头调整或温度变化等因素而发生变化。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需额外校准拍摄就能优化大规模摄像机阵列内部参数的方法，并提高3D重建的准确性。&lt;h4&gt;方法&lt;/h4&gt;该方法通过引入外参正则化项、密集特征重投影项和内参方差项来增强传统的SfM流程，实现多帧联合优化。这种方法可以直接从场景数据中优化相机内部参数，无需额外的校准拍摄。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法在精确度上接近于专门的校准过程，并且显著提高了相机内参和3D重建的准确性。&lt;h4&gt;结论&lt;/h4&gt;该方法与现有的SfM流程完全兼容，为大规模摄像机设置提供了一种高效实用的即插即用解决方案。代码可在https://github.com/YJJfish/Multi-Cali-Anything公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yjjfish/multi-cali-anything&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Calibrating large-scale camera arrays, such as those in dome-based setups, istime-intensive and typically requires dedicated captures of known patterns.While extrinsics in such arrays are fixed due to the physical setup, intrinsicsoften vary across sessions due to factors like lens adjustments or temperaturechanges. In this paper, we propose a dense-feature-driven multi-framecalibration method that refines intrinsics directly from scene data,eliminating the necessity for additional calibration captures. Our approachenhances traditional Structure-from-Motion (SfM) pipelines by introducing anextrinsics regularization term to progressively align estimated extrinsics withground-truth values, a dense feature reprojection term to reduce keypointerrors by minimizing reprojection loss in the feature space, and an intrinsicsvariance term for joint optimization across multiple frames. Experiments on theMultiface dataset show that our method achieves nearly the same precision asdedicated calibration processes, and significantly enhances intrinsics and 3Dreconstruction accuracy. Fully compatible with existing SfM pipelines, ourmethod provides an efficient and practical plug-and-play solution forlarge-scale camera setups. Our code is publicly available at:https://github.com/YJJfish/Multi-Cali-Anything</description>
      <author>example@mail.com (Jinjiang You, Hewei Wang, Yijie Li, Mingxiao Huo, Long Van Tran Ha, Mingyuan Ma, Jinfeng Xu, Puzhen Wu, Shubham Garg, Wei Pu)</author>
      <guid isPermaLink="false">2503.00737v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Efficient End-to-end Visual Localization for Autonomous Driving with Decoupled BEV Neural Matching</title>
      <link>http://arxiv.org/abs/2503.00862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文提出了一种端到端的定位神经网络，直接从周围图像中估计车辆姿态，而无需显式地将感知结果与高精度地图匹配。通过减少采样空间来确保效率和可解释性，并在实验中展示了其厘米级定位能力。&lt;h4&gt;背景&lt;/h4&gt;精确的定位对于高级自动驾驶系统至关重要。传统基于地图匹配的方法容易受感知噪声影响，需要昂贵的超参数调整。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的端到端定位神经网络方法，能够直接从图像估计车辆姿态，同时保持计算效率和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出了一个解耦的BEV（鸟瞰图）神经匹配模块，用于差分采样基于匹配的姿态估算，并通过解耦每个自由度对特征表示的影响来大幅减少采样空间。&lt;h4&gt;主要发现&lt;/h4&gt;该网络在纵向位置、横向位置和偏航角上的平均绝对误差分别为0.19米、0.13米和0.39度，同时推理内存使用量减少了68.8%。&lt;h4&gt;结论&lt;/h4&gt;新的定位方法能够实现高精度的车辆姿态估计，并且具有低资源消耗的特点。&lt;h4&gt;翻译&lt;/h4&gt;准确的定位在高级自动驾驶系统中起着重要作用。传统的基于地图匹配的方法通过显式地将映射元素与传感器观测结果进行匹配来解决姿态问题，这种方法通常对感知噪声敏感，因此需要昂贵的超参数调整。在这篇论文中，我们提出了一种端到端的定位神经网络，直接从周围的图像估计车辆姿态，而不必显式地将感知结果与高精度地图进行匹配。为了保证效率和可解释性，提出了一个解耦的基于BEV（鸟瞰图）神经匹配的姿态求解器，在一个差分采样基础上的匹配模块中估计姿态。此外，通过解耦每个自由度对特征表示的影响大大减少了采样空间。实验结果表明，所提出的网络能够进行厘米级定位，纵向位置、横向位置和偏航角上的平均绝对误差分别为0.19米、0.13米和0.39度，同时推理内存使用量降低了68.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localization plays an important role in high-level autonomousdriving systems. Conventional map matching-based localization methods solve theposes by explicitly matching map elements with sensor observations, generallysensitive to perception noise, therefore requiring costly hyper-parametertuning. In this paper, we propose an end-to-end localization neural networkwhich directly estimates vehicle poses from surrounding images, withoutexplicitly matching perception results with HD maps. To ensure efficiency andinterpretability, a decoupled BEV neural matching-based pose solver isproposed, which estimates poses in a differentiable sampling-based matchingmodule. Moreover, the sampling space is hugely reduced by decoupling thefeature representation affected by each DoF of poses. The experimental resultsdemonstrate that the proposed network is capable of performing decimeter levellocalization with mean absolute errors of 0.19m, 0.13m and 0.39 degree inlongitudinal, lateral position and yaw angle while exhibiting a 68.8% reductionin inference memory usage.</description>
      <author>example@mail.com (Jinyu Miao, Tuopu Wen, Ziang Luo, Kangan Qian, Zheng Fu, Yunlong Wang, Kun Jiang, Mengmeng Yang, Jin Huang, Zhihua Zhong, Diange Yang)</author>
      <guid isPermaLink="false">2503.00862v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DILEMMA: Joint LLM Quantization and Distributed LLM Inference Over Edge Computing Systems</title>
      <link>http://arxiv.org/abs/2503.01704v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DILEMMA是一个针对边缘计算环境中的大型语言模型部署挑战而设计的框架，通过联合优化层放置和量化策略来最小化推理延迟并保证性能。&lt;h4&gt;背景&lt;/h4&gt;随着大语言模型在智慧城市应用中越来越流行，如何在网络边缘有效利用这些资源成为一个关键问题。边缘计算可以降低通信延迟，但受制于有限的通信、计算和存储能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架，能够在满足大型语言模型性能要求的同时优化其部署到边缘计算环境中的方法。&lt;h4&gt;方法&lt;/h4&gt;DILEMMA通过整数线性规划问题来最小化总的推理延迟，并利用逐层量化和知识蒸馏技术控制LLM的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在OPT-350模型上使用SQuAD数据集进行实验后，证明了DILEMMA能够在保持模型损失的同时实现高达12.75%的量化比率，显示其在资源受限环境中的有效性。&lt;h4&gt;结论&lt;/h4&gt;DILEMMA框架为大型语言模型部署到边缘计算系统提供了有效的解决方案，尤其是在需要降低通信延迟和优化资源使用的场景中。&lt;h4&gt;翻译&lt;/h4&gt;随着大语言模型被越来越多地应用于智慧城市的不同应用领域，有必要将这些模型推向网络的边缘，同时保持其性能。作为物理上更接近最终用户的计算资源，边缘计算可以减少为依赖大型语言模型的服务提供服务时的通信延迟。然而，边缘服务器在通信、计算和存储容量方面的能力有限。本文介绍了一种名为DILEMMA的新框架，该框架通过联合优化层放置和量化策略来解决将大语言模型部署到边缘系统中的挑战。DILEMMA通过整数线性规划问题最小化总的推理延迟并确保可接受的大语言模型性能水平，并利用逐层量化和知识蒸馏技术控制LLM的性能。使用SQuAD数据集对OPT-350模型进行实验评估表明，DILEMMA能够在保持模型损失的同时实现高达12.75%的量化比率，展示了其在资源受限环境中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With a recent trend of using Large Language Models (LLMs) for differentapplications within smart cities, there is a need for pushing these modelstoward the edge of network while still preserving their performance. EdgeComputing (EC) as a physically closer computing resource to the end users canhelp to reduce the communication delay for serving end users' tasks forLLM-dependent services. However, EC servers have limited capacity in terms ofcommunication, computation, and storage capacity. This paper introducesDILEMMA, a novel framework addressing the challenges of deploying LLMs in ECsystems by jointly optimizing layer placement and layer quantization in ECsystems. DILEMMA formulates an Integer Linear Programming problem to minimizetotal inference delay while ensuring acceptable LLM performance levels,leveraging layer-wise quantization and knowledge distillation for LLMperformance control. Experimental evaluations on OPT-350 model using the SQuADdataset demonstrate that DILEMMA achieves a quantization ratio of up to 12.75%while preserving model loss, highlighting its effectiveness inresource-constrained environments.</description>
      <author>example@mail.com (Minoo Hosseinzadeh, Hana Khamfroush)</author>
      <guid isPermaLink="false">2503.01704v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Curating Demonstrations using Online Experience</title>
      <link>http://arxiv.org/abs/2503.03707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种机器人自我整理的方法，称为Demo-SCORE，通过使用在线机器人经验训练分类器来识别成功的策略执行，并过滤掉异质性演示数据集中的次优示范。&lt;h4&gt;背景&lt;/h4&gt;许多机器人的演示数据集中包含不同质量和可靠性的多样性示例。这种多样性可能有助于策略的预训练，但也可能导致最终模仿学习目标时性能下降。&lt;h4&gt;目的&lt;/h4&gt;目的是通过机器人自我整理来提高基于这些多样演示数据集训练的策略在测试中的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Demo-SCORE的方法，该方法使用在线机器人经验训练分类器以区分成功的和不成功的策略执行，并利用此分类器过滤掉异质性示例数据集中次优的表现。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与所有原始演示数据集一起训练的基准政策相比，应用了Demo-SCORE方法后生成的策略可以实现高达15%至35%更高的绝对成功率。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了一种有效的方法来识别和过滤次优示范，从而改善从异质性示例中学习出的机器人策略性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many robot demonstration datasets contain heterogeneous demonstrations ofvarying quality. This heterogeneity may benefit policy pre-training, but canhinder robot performance when used with a final imitation learning objective.In particular, some strategies in the data may be less reliable than others ormay be underrepresented in the data, leading to poor performance when suchstrategies are sampled at test time. Moreover, such unreliable orunderrepresented strategies can be difficult even for people to discern, andsifting through demonstration datasets is time-consuming and costly. On theother hand, policy performance when trained on such demonstrations can reflectthe reliability of different strategies. We thus propose for robots toself-curate based on online robot experience (Demo-SCORE). More specifically,we train and cross-validate a classifier to discern successful policy roll-outsfrom unsuccessful ones and use the classifier to filter heterogeneousdemonstration datasets. Our experiments in simulation and the real world showthat Demo-SCORE can effectively identify suboptimal demonstrations withoutmanual curation. Notably, Demo-SCORE achieves over 15-35% higher absolutesuccess rate in the resulting policy compared to the base policy trained withall original demonstrations.</description>
      <author>example@mail.com (Annie S. Chen, Alec M. Lessing, Yuejiang Liu, Chelsea Finn)</author>
      <guid isPermaLink="false">2503.03707v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>EVLoc: Event-based Visual Localization in LiDAR Maps via Event-Depth Registration</title>
      <link>http://arxiv.org/abs/2503.00167v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文探讨了事件相机在利用现有激光雷达地图进行定位中的应用潜力，提出了一种基于粗略初始姿态细化的框架。&lt;h4&gt;背景信息&lt;/h4&gt;事件相机具备高动态范围和低延迟特性，在高速运动和极端光照条件下具有优势。现有的激光雷达地图可以用于导航和移动操作的应用中。&lt;h4&gt;研究目的&lt;/h4&gt;探索事件相机在现有LiDAR地图中的定位能力，以实现精准导航和机器人移动抓取任务。&lt;h4&gt;提出方法&lt;/h4&gt;{'第一步': '基于粗略初始姿态，将LiDAR点投影到2D空间中生成深度图。', '第二步': '利用光学流估计网络对事件与LiDAR点进行二维空间的配准。', '第三步': '使用PnP求解器来估计相机的姿态。', '改进措施': '开发了一种新的基于帧的事件表示，以增强几何一致性，并预测辅助变量作为正则化项，以减少地面实况姿态偏差对网络收敛性的影响。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明所提方法在公共数据集上具有有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了一种新颖的事件相机和激光雷达融合定位框架，并在网上发布了代码与预训练模型，以促进未来的相关研究。&lt;h4&gt;翻译&lt;/h4&gt;本文探讨了生物启发式的事件摄像机在现有LiDAR地图中进行定位的应用潜力。我们提出了一套基于粗略初始姿态细化的方法来实现这一目标，包括将LiDAR点投影到2D空间生成深度图、利用光学流估计网络对齐事件和LiDAR点以及使用PnP求解器估算相机姿势等步骤，并且开发了新的帧级事件表示方法以提高几何一致性。实验结果表明该方法在多个公开数据集上均有效，研究还开放了代码和预训练模型供后续研究使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras are bio-inspired sensors with some notable features, includinghigh dynamic range and low latency, which makes them exceptionally suitable forperception in challenging scenarios such as high-speed motion and extremelighting conditions. In this paper, we explore their potential for localizationwithin pre-existing LiDAR maps, a critical task for applications that requireprecise navigation and mobile manipulation. Our framework follows a paradigmbased on the refinement of an initial pose. Specifically, we first projectLiDAR points into 2D space based on a rough initial pose to obtain depth maps,and then employ an optical flow estimation network to align events with LiDARpoints in 2D space, followed by camera pose estimation using a PnP solver. Toenhance geometric consistency between these two inherently differentmodalities, we develop a novel frame-based event representation that improvesstructural clarity. Additionally, given the varying degrees of bias observed inthe ground truth poses, we design a module that predicts an auxiliary variableas a regularization term to mitigate the impact of this bias on networkconvergence. Experimental results on several public datasets demonstrate theeffectiveness of our proposed method. To facilitate future research, both thecode and the pre-trained models are made available online.</description>
      <author>example@mail.com (Kuangyi Chen, Jun Zhang, Friedrich Fraundorfer)</author>
      <guid isPermaLink="false">2503.00167v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Negative Damping Control for User-Dependent Multi-Terrain Walking Assistance with a Hip Exoskeleton</title>
      <link>http://arxiv.org/abs/2503.03662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Copyright 2025 IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种新的髋部外骨骼控制策略，通过适应性虚拟负阻尼设计来调整机械阻抗，使系统能够注入能量同时确保用户保持对运动的自愿贡献。&lt;h4&gt;背景&lt;/h4&gt;当前的辅助策略在应对个体行走模式和多变地形环境方面灵活性不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够根据个体需求灵活调节、适应不同地形环境的新控制方法。&lt;h4&gt;方法&lt;/h4&gt;设计了一种基于虚拟负阻尼来调整机械系统的阻抗，通过实验验证其减少代谢成本的效果，并采用贝叶斯优化技术实现无缝的辅助强度调整和跨多种地形环境下的过渡。&lt;h4&gt;主要发现&lt;/h4&gt;该控制器在五名健康受试者中实现了平均7.2%的步行代谢成本降低；同时保持了下肢运动学特性，确保在整个步态周期中的功率损失小于总功率的2%，并且实现了与用户动作同步的最佳协调。&lt;h4&gt;结论&lt;/h4&gt;提出的方法展示了个性化的、适应性强且操作简单的髋部外骨骼控制器设计，推动了适用性更强、依赖于用户的控制法则的发展。&lt;h4&gt;翻译&lt;/h4&gt;现有的髋部外骨骼辅助策略在应对不同的行走模式和地形时表现不佳。本文介绍了一种新型的机械阻抗调节方法，通过虚拟负阻尼来适应人体-机器系统，该方法能够向系统注入能量并保持用户主动参与运动的能力。实验表明，在五名受试者中，与自由行走相比，步行代谢成本平均降低了7.2%，且下肢运动学没有改变。同时实现了步态周期内极低的功率损失（小于总功率的2%），确保了人机动作的一致性。此外，使用贝叶斯优化适应辅助强度以实现跨多地形环境的无缝过渡和调整。该方法展示了适用于所有条件下的高效功率传输，并为个性化、可调且用户依赖性的控制法则的发展提供了可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hip exoskeletons are known for their versatility in assisting users acrossvaried scenarios. However, current assistive strategies often lack theflexibility to accommodate for individual walking patterns and adapt to diverselocomotion environments. In this work, we present a novel control strategy thatadapts the mechanical impedance of the human-exoskeleton system. We design thehip assistive torques as an adaptive virtual negative damping, which is able toinject energy into the system while allowing the users to remain in control andcontribute voluntarily to the movements. Experiments with five healthy subjectsdemonstrate that our controller reduces the metabolic cost of walking comparedto free walking (average reduction of 7.2%), and it preserves the lower-limbskinematics. Additionally, our method achieves minimal power losses from theexoskeleton across the entire gait cycle (less than 2% negative mechanicalpower out of the total power), ensuring synchronized action with the users'movements. Moreover, we use Bayesian Optimization to adapt the assistancestrength and allow for seamless adaptation and transitions across multi-terrainenvironments. Our strategy achieves efficient power transmission under allconditions. Our approach demonstrates an individualized, adaptable, andstraightforward controller for hip exoskeletons, advancing the development ofviable, adaptive, and user-dependent control laws.</description>
      <author>example@mail.com (Giulia Ramella, Auke Ijspeert, Mohamed Bouri)</author>
      <guid isPermaLink="false">2503.03662v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Motion Planning and Control with Unknown Nonlinear Dynamics through Predicted Reachability</title>
      <link>http://arxiv.org/abs/2503.03633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;针对未知非线性动力学下的自主运动规划提出了一种混合规划-控制框架，旨在计算朝向目标的可行轨迹。&lt;h4&gt;背景&lt;/h4&gt;在未知非线性动态系统中进行自主运动规划具有重大挑战。为了引导系统的导航并适应环境变化，代理需要持续探索以获取关于可到达性的系统属性。&lt;h4&gt;目的&lt;/h4&gt;提出一种混合规划-控制框架，该框架能够计算朝向目标的可行轨迹，并通过抽象为有向加权图来处理未知非线性动力学问题。&lt;h4&gt;方法&lt;/h4&gt;- 将状态空间划分为多个区域并将其近似为分段仿射（PWA）系统。- 通过确定系统的边缘存在情况，逐步更新有向加权图。- 利用前馈可到达性条件和控制理论中的可达性理论来预测未知动力学的先验信息，并据此分配启发式权重。&lt;h4&gt;主要发现&lt;/h4&gt;- 基于图形搜索结果在线生成控制器并不断更新预测图能够提高导航效率，从而适应动态变化。- 在移动机器人在未探索地形中运行时，可以将未知的动力学抽象为一个积分器模型来简化处理。&lt;h4&gt;结论&lt;/h4&gt;该方法通过仿真场景验证了其有效性，并展示了如何利用混合规划-控制框架和分段仿射系统的特性来解决自主运动规划中的挑战。&lt;h4&gt;翻译&lt;/h4&gt;自主运动规划在面对未知非线性动力学时面临重大挑战。为了引导系统导航并适应环境，代理需要持续探索以获取关于可到达性的系统属性等信息。本文提出了一种混合规划-控制框架，用于计算朝向目标的可行轨迹。该方法通过将状态空间划分为多个区域并将它们近似为分段仿射（PWA）系统，然后将其抽象成有向加权图，并根据未知动力学的信息逐步更新其边的存在情况来实现这一目的。我们还提出了一个框架，在任务执行期间适应性地收集和分析数据，不断更新预测图形，并基于搜索结果在线生成控制器。通过模拟移动机器人在未知地形中操作的场景（将其未知的动力学抽象为单个积分器模型），验证了这种方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous motion planning under unknown nonlinear dynamics presentssignificant challenges. An agent needs to continuously explore the systemdynamics to acquire its properties, such as reachability, in order to guidesystem navigation adaptively. In this paper, we propose a hybridplanning-control framework designed to compute a feasible trajectory toward atarget. Our approach involves partitioning the state space and approximatingthe system by a piecewise affine (PWA) system with constrained control inputs.By abstracting the PWA system into a directed weighted graph, we incrementallyupdate the existence of its edges via affine system identification and reachcontrol theory, introducing a predictive reachability condition by exploitingprior information of the unknown dynamics. Heuristic weights are assigned toedges based on whether their existence is certain or remains indeterminate.Consequently, we propose a framework that adaptively collects and analyzes dataduring mission execution, continually updates the predictive graph, andsynthesizes a controller online based on the graph search outcomes. Wedemonstrate the efficacy of our approach through simulation scenarios involvinga mobile robot operating in unknown terrains, with its unknown dynamicsabstracted as a single integrator model.</description>
      <author>example@mail.com (Zhiquan Zhang, Gokul Puthumanaillam, Manav Vora, Melkior Ornik)</author>
      <guid isPermaLink="false">2503.03633v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles through Generative Simulation</title>
      <link>http://arxiv.org/abs/2503.03629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了TeraSim平台，这是一个用于自动驾驶车辆（AV）开发的开源、高保真交通模拟器。&lt;h4&gt;背景&lt;/h4&gt;交通仿真对于评估自动驾驶汽车的安全性至关重要。然而，传统的基于规则的方法难以捕捉复杂的驾驶员行为互动，而数据驱动方法则在保持长期行为真实性或生成安全相关的事件多样性方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;提出TeraSim平台以解决上述挑战，旨在揭示未知的不安全事件并高效估算AV统计性能指标，如碰撞率。&lt;h4&gt;方法&lt;/h4&gt;设计了一个开放源代码平台，可以无缝集成第三方物理仿真器和独立的自动驾驶车辆软件堆栈，构建完整的自动驾驶车辆模拟系统。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明TeraSim在生成涉及静态及动态代理的各种安全关键事件方面有效，并能识别出AV系统的隐藏缺陷，支持统计性能评估。&lt;h4&gt;结论&lt;/h4&gt;该平台作为实用性工具对研究者、开发者以及政策制定者都有重要的价值，有助于自动驾驶车辆的安全性评估。&lt;h4&gt;翻译&lt;/h4&gt;交通模拟对于自动驾驶汽车的发展至关重要，能够实现各种驾驶条件下的全面安全评估。然而，传统基于规则的模拟器难以捕捉复杂的人类互动行为，而数据驱动的方法往往在长期的行为真实性保持或生成多样的重要安全性事件方面存在不足。为此，提出了一种名为TeraSim的开源高保真交通仿真平台，旨在揭示未知的安全隐患，并高效地估计自动驾驶汽车如碰撞率等统计性能指标。该平台设计用于与第三方物理模拟器和独立的自动驾驶汽车软件堆栈无缝集成，构建完整的自动驾驶车辆仿真系统。实验结果显示其在生成涉及静态及动态代理的各种重要安全事件方面有效，并能识别出AV系统的隐藏缺陷，支持进行统计性能评估。这些发现强调了TeraSim作为实用性工具对研究者、开发者以及政策制定者的潜在价值，可用于自动驾驶汽车的安全性评估。代码可在https://github.com/mcity/TeraSim获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic simulation is essential for autonomous vehicle (AV) development,enabling comprehensive safety evaluation across diverse driving conditions.However, traditional rule-based simulators struggle to capture complex humaninteractions, while data-driven approaches often fail to maintain long-termbehavioral realism or generate diverse safety-critical events. To address thesechallenges, we propose TeraSim, an open-source, high-fidelity trafficsimulation platform designed to uncover unknown unsafe events and efficientlyestimate AV statistical performance metrics, such as crash rates. TeraSim isdesigned for seamless integration with third-party physics simulators andstandalone AV stacks, to construct a complete AV simulation system.Experimental results demonstrate its effectiveness in generating diversesafety-critical events involving both static and dynamic agents, identifyinghidden deficiencies in AV systems, and enabling statistical performanceevaluation. These findings highlight TeraSim's potential as a practical toolfor AV safety assessment, benefiting researchers, developers, and policymakers.The code is available at https://github.com/mcity/TeraSim.</description>
      <author>example@mail.com (Haowei Sun, Xintao Yan, Zhijie Qiao, Haojie Zhu, Yihao Sun, Jiawei Wang, Shengyin Shen, Darian Hogue, Rajanikant Ananta, Derek Johnson, Greg Stevens, Greg McGuire, Yifan Wei, Wei Zheng, Yong Sun, Yasuo Fukai, Henry X. Liu)</author>
      <guid isPermaLink="false">2503.03629v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Generative System for Robot-to-Human Handovers: from Intent Inference to Spatial Configuration Imagery</title>
      <link>http://arxiv.org/abs/2503.03579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;当前大多数关于机器人与人类交互的研究主要集中于抓取策略和运动规划。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的系统，用于模拟人机之间的物体交接过程。该系统着重于推断人的交接意图及想象空间配置，以模仿合作型机器人的认知处理。&lt;h4&gt;方法&lt;/h4&gt;{'第一部分': '整合多模态感知（视觉和语言提示），来推断人类的交接意图。', '第二部分': '使用基于扩散模型的方法生成交接的空间配置，考虑机器人抓手、物体与人手之间的空间关系'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该方法能够有效解读人的暗示，并实现流畅的人类化交接过程，为协作型机器人提供了一种有前景的解决方案。&lt;h4&gt;结论&lt;/h4&gt;该系统通过模仿人类认知处理中的运动意象，在人机交互中展现出良好的潜力和应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的用于机器人与人类物体交接系统的模型，这个系统模拟了人类同事间的互动方式。不同于大多数现有研究主要集中于抓取策略及运动规划，我们的系统侧重于推断人的交接意图以及想象空间配置。第一部分整合了多模态感知（视觉和语言提示）来推断人类的意图；第二部分使用基于扩散模型的方法生成交接的空间配置，考虑机器人抓手、物体与人手之间的空间关系，从而模仿认知处理中的运动意象过程。实验结果显示该方法能够有效解读人的暗示，并实现流畅的人类化交接过程，为协作型机器人提供了一种有前景的解决方案。代码、视频和数据可在 https://i3handover.github.io 获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel system for robot-to-human object handover that emulateshuman coworker interactions. Unlike most existing studies that focus primarilyon grasping strategies and motion planning, our system focus on 1. inferringhuman handover intents, 2. imagining spatial handover configuration. The firstone integrates multimodal perception-combining visual and verbal cues-to inferhuman intent. The second one using a diffusion-based model to generate thehandover configuration, involving the spacial relationship among robot'sgripper, the object, and the human hand, thereby mimicking the cognitiveprocess of motor imagery. Experimental results demonstrate that our approacheffectively interprets human cues and achieves fluent, human-like handovers,offering a promising solution for collaborative robotics. Code, videos, anddata are available at: https://i3handover.github.io.</description>
      <author>example@mail.com (Hanxin Zhang, Abdulqader Dhafer, Zhou Daniel Hao, Hongbiao Dong)</author>
      <guid isPermaLink="false">2503.03579v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SpeechCompass: Enhancing Mobile Captioning with Diarization and Directional Guidance via Multi-Microphone Localization</title>
      <link>http://arxiv.org/abs/2502.08848v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CHI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一个名为SpeechCompass的系统，用于解决移动设备上语音转文字功能在群组对话中无法区分和指示说话者方向的问题。&lt;h4&gt;背景&lt;/h4&gt;移动设备上的语音转文字技术已被证明对听力和语言无障碍、语言翻译、记笔记和会议记录有帮助。然而，基础大规模调查表明，在群体交谈中无法识别和表示说话者的方向使其变得具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;通过实现实时多麦克风语音定位解决现有移动设备语音转文字功能在群体对话中的局限性问题，并探索有效的可视化方法来指导用户。&lt;h4&gt;方法&lt;/h4&gt;引入了高效实时音频定位算法以及自定义的声音感知硬件，这些硬件运行在一个低功耗微控制器上并连接四个集成麦克风。进行了大规模调查（n=494）并对八名经常使用移动语音转文字的参与者进行了一对一的研究，并收集了他们关于五种可视化样式的反馈。&lt;h4&gt;主要发现&lt;/h4&gt;所有参与者的反馈一致认为，区分说话者和定位视觉化对于群组对话的价值和潜力至关重要。他们认可方向引导的实际价值和潜在应用。&lt;h4&gt;结论&lt;/h4&gt;通过引入SpeechCompass系统，移动设备上的语音转文字功能在群体对话中的用户体验得到了显著改善，并且该技术具有实际的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech-to-text capabilities on mobile devices have proven helpful for hearingand speech accessibility, language translation, note-taking, and meetingtranscripts. However, our foundational large-scale survey (n=263) shows thatthe inability to distinguish and indicate speaker direction makes themchallenging in group conversations. SpeechCompass addresses this limitationthrough real-time, multi-microphone speech localization, where the direction ofspeech allows visual separation and guidance (e.g., arrows) in the userinterface. We introduce efficient real-time audio localization algorithms andcustom sound perception hardware running on a low-power microcontroller andfour integrated microphones, which we characterize in technical evaluations.Informed by a large-scale survey (n=494), we conducted an in-person study ofgroup conversations with eight frequent users of mobile speech-to-text, whoprovided feedback on five visualization styles. The value of diarization andvisualizing localization was consistent across participants, with everyoneagreeing on the value and potential of directional guidance for groupconversations.</description>
      <author>example@mail.com (Artem Dementyev, Dimitri Kanevsky, Samuel J. Yang, Mathieu Parvaix, Chiong Lai, Alex Olwal)</author>
      <guid isPermaLink="false">2502.08848v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Olympus: A Jumping Quadruped for Planetary Exploration Utilizing Reinforcement Learning for In-Flight Attitude Control</title>
      <link>http://arxiv.org/abs/2503.03574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, Accepted to the IEEE International Conference on  Robotics and Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;探索低重力的行星体，如月球和火星，允许腿足机器人利用跳跃作为一种高效的运动方式，从而在探测任务中相对于传统探测车具有明显优势。受此启发，本文介绍了Olympus的设计、模拟以及基于学习的空中姿态控制方法，这是一款专为火星引力设计的跳跃式腿足机器人。&lt;h4&gt;背景&lt;/h4&gt;低重力环境下（如月球和火星），腿足机器人可以利用跳跃作为高效的移动方式，并且具有相对于传统探测车的优势&lt;h4&gt;目的&lt;/h4&gt;介绍针对火星重力环境设计、模拟以及基于学习的空中姿态控制方法的Olympus跳跃式腿足机器人的开发。&lt;h4&gt;方法&lt;/h4&gt;首先概述了设计需求，然后详细介绍了如何通过仿真优化机器人从腿部到整体配置的设计，以实现高垂直跳跃、远距离前进跳跃和在空中的姿态重新定向。接着展示了用于跟踪所需空中姿态操作的强化学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;成功跨越模拟与现实之间的差距，进行了广泛的实验研究来测试姿态重新定向。&lt;h4&gt;结论&lt;/h4&gt;Olympus的设计以及基于学习的方法为未来在低重力行星体上的探测任务提供了一个有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Exploring planetary bodies with lower gravity, such as the moon and Mars,allows legged robots to utilize jumping as an efficient form of locomotion thusgiving them a valuable advantage over traditional rovers for exploration.Motivated by this fact, this paper presents the design, simulation, andlearning-based "in-flight" attitude control of Olympus, a jumping legged robottailored to the gravity of Mars. First, the design requirements are outlinedfollowed by detailing how simulation enabled optimizing the robot's design -from its legs to the overall configuration - towards high vertical jumping,forward jumping distance, and in-flight attitude reorientation. Subsequently,the reinforcement learning policy used to track desired in-flight attitudemaneuvers is presented. Successfully crossing the sim2real gap, extensiveexperimental studies of attitude reorientation tests are demonstrated.</description>
      <author>example@mail.com (Jørgen Anker Olsen, Grzegorz Malczyk, Kostas Alexis)</author>
      <guid isPermaLink="false">2503.03574v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented Manipulation</title>
      <link>http://arxiv.org/abs/2503.03556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了LVIS-Aff数据集和Afford-X模型，以改进基于感知的对象功能推理能力。&lt;h4&gt;背景&lt;/h4&gt;对象功能推理对于任务导向的计划和执行至关重要。然而，现有的计算模型缺乏泛化能力，难以应用于新的场景。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的大规模数据集（LVIS-Aff）以及一个增强的功能推理模型（Afford-X），以提高对象功能感知推理的准确性和速度，并适用于本地设备的任务操作。&lt;h4&gt;方法&lt;/h4&gt;通过引入LVIS-Aff数据集和使用Verb Attention与Bi-Fusion模块来改进多模态理解，提出了一种端到端可训练的affordance推理模型（Afford-X）。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的Afford-X模型比最好的非LLM方法提高了12.1%的表现，并且相较于作者之前的会议论文也有所进步。此外，它保持了紧凑的参数规模和高速度。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了高效、通用的功能推理模型在本地设备上部署的可能性，并证明其在机器人任务操作中的有效性以及对现实世界应用的意义。&lt;h4&gt;翻译&lt;/h4&gt;物体功能推理能力对于人类和人工智能（AI）的任务导向规划至关重要。现有方法缺乏泛化性，难以处理新场景问题。为此，作者提出了一种大规模数据集LVIS-Aff和一个先进的模型Afford-X，旨在提高基于感知的功能推理性能，并展示了其在实际应用中的优越效果和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object affordance reasoning, the ability to infer object functionalitiesbased on physical properties, is fundamental for task-oriented planning andactivities in both humans and Artificial Intelligence (AI). This capability,required for planning and executing daily activities in a task-oriented manner,relies on commonsense knowledge of object physics and functionalities,extending beyond simple object recognition. Current computational models foraffordance reasoning from perception lack generalizability, limiting theirapplicability in novel scenarios. Meanwhile, comprehensive Large LanguageModels (LLMs) with emerging reasoning capabilities are challenging to deploy onlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, alarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhancethe generalizability of affordance reasoning from perception. Utilizing thisdataset, we develop Afford-X, an end-to-end trainable affordance reasoningmodel that incorporates Verb Attention and Bi-Fusion modules to improvemulti-modal understanding. This model achieves up to a 12.1% performanceimprovement over the best-reported results from non-LLM methods, while alsodemonstrating a 1.2% enhancement compared to our previous conference paper.Additionally, it maintains a compact 187M parameter size and infers nearly 50times faster than the GPT-4V API. Our work demonstrates the potential forefficient, generalizable affordance reasoning models that can be deployed onlocal devices for task-oriented manipulations. We showcase Afford-X'seffectiveness in enabling task-oriented manipulations for robots across varioustasks and environments, underscoring its efficiency and broad implications foradvancing robotics and AI systems in real-world applications.</description>
      <author>example@mail.com (Xiaomeng Zhu, Yuyang Li, Leiyao Cui, Pengfei Li, Huan-ang Gao, Yixin Zhu, Hao Zhao)</author>
      <guid isPermaLink="false">2503.03556v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Unified Human Localization and Trajectory Prediction with Monocular Vision</title>
      <link>http://arxiv.org/abs/2503.03535v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了MonoTransmotion (MT)框架，该框架利用单目相机同时解决定位和预测任务，展示出在真实世界场景中处理嘈杂数据时的稳健性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统的轨迹预测模型依赖于经过精制的数据，并且需要特殊设备或手动标注，这使得它们不适用于大多数机器人应用。现有预测器往往过于适应干净观察结果而影响其使用嘈杂输入时的鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Transformer框架的方法（MonoTransmotion），该方法仅利用单目相机进行定位和轨迹预测任务，并验证该方法在现实场景中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一个包含两个主要模块的框架：鸟瞰图(BEV)定位模块和轨迹预测模块。BEV定位模块使用2D人体姿势估计人的位置，而轨迹预测模块则根据这些估计值预测未来的运动路径。&lt;h4&gt;主要发现&lt;/h4&gt;通过联合训练上述任务并采用统一框架的方法，MT方法在包含嘈杂输入的真实场景中表现更加稳健，并且其性能在数据集中得到验证。在人工准备的数据集上，相比基线模型，MT框架在BEV定位和轨迹预测方面取得了约12%的改进。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，在非精制的真实世界数据集上，MT方法维持了类似水平的表现，这突显了其稳健性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：传统的行人轨迹预测模型依赖于经过清洗和整理的数据，通常需要特殊设备或人工标记，这对于机器人应用来说往往是不切实际的。现有的预测器倾向于过度拟合到干净的观察结果上，在处理嘈杂输入时会降低其鲁棒性。在这项工作中，我们提出了基于Transformer框架的MonoTransmotion（MT），该框架仅使用单目相机同时解决定位和预测任务。我们的框架有两个主要模块：鸟瞰图（BEV）定位和轨迹预测。BEV定位模块利用2D人体姿态估计人的位置，并通过一种新的方向损失函数增强，以实现更平滑的顺序定位。轨迹预测模块从这些估算值中预测未来的运动路径。我们展示了通过联合训练这两个任务并使用统一框架的方法，在处理嘈杂输入的真实世界场景中的表现更加稳健。我们在人工准备和非人工准备的数据集上验证了我们的MT网络。在人工准备的数据集上，MT相比基准模型在BEV定位和轨迹预测方面取得了大约12%的改进。而在真实的非精制数据集上，实验结果表明MT保持类似的表现水平，突显其稳健性和泛化能力。代码可在https://github.com/vita-epfl/MonoTransmotion获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional human trajectory prediction models rely on clean curated data,requiring specialized equipment or manual labeling, which is often impracticalfor robotic applications. The existing predictors tend to overfit to cleanobservation affecting their robustness when used with noisy inputs. In thiswork, we propose MonoTransmotion (MT), a Transformer-based framework that usesonly a monocular camera to jointly solve localization and prediction tasks. Ourframework has two main modules: Bird's Eye View (BEV) localization andtrajectory prediction. The BEV localization module estimates the position of aperson using 2D human poses, enhanced by a novel directional loss for smoothersequential localizations. The trajectory prediction module predicts futuremotion from these estimates. We show that by jointly training both tasks withour unified framework, our method is more robust in real-world scenarios madeof noisy inputs. We validate our MT network on both curated and non-curateddatasets. On the curated dataset, MT achieves around 12% improvement overbaseline models on BEV localization and trajectory prediction. On real-worldnon-curated dataset, experimental results indicate that MT maintains similarperformance levels, highlighting its robustness and generalization capability.The code is available at https://github.com/vita-epfl/MonoTransmotion.</description>
      <author>example@mail.com (Po-Chien Luan, Yang Gao, Celine Demonsant, Alexandre Alahi)</author>
      <guid isPermaLink="false">2503.03535v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Potential gains of communication-compute-control co-design based performance optimization methods in cyber-physical systems</title>
      <link>http://arxiv.org/abs/2503.03521v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了三种性能优化的方法，通过引入通信和计算特性到应用逻辑中来改善整体系统性能。&lt;h4&gt;背景&lt;/h4&gt;在利用网络和云技术实现工业控制系统时，控制应用程序的确定性会自然降低。这意味着某些不完善可能会被引入控制系统，在干扰期间闭环控制实际上转变为开环控制。&lt;h4&gt;目的&lt;/h4&gt;目的是通过应用可以补偿统计上或保证方式的方法来改进这些开环控制时段的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了三种基于共设计的应用改进方案，这些方案对底层技术的依赖性最小。&lt;h4&gt;主要发现&lt;/h4&gt;共设计方法能够显著提高机器人轨迹在开环控制期间执行的准确性，并且结合使用这些建议的方法可以将轨迹执行时间缩短多达45%。&lt;h4&gt;结论&lt;/h4&gt;通过引入通信和计算特性到应用逻辑中，整体系统性能得到明显改善，特别是在开放环控制周期内表现得更加准确。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we propose and quantitatively evaluate three performanceoptimization methods that exploit the concept of communication-compute-controlco-design by introducing awareness of communication and compute characteristicsinto the application logic in different ways to improve overall systemperformance. We have implemented a closed-loop control of a robotic arm over awireless network where the controller is deployed into an edge cloudenvironment. When implementing an industrial system that leverages network andcloud technologies, the level of determinism of the control application can bedecreased by nature. This means that some imperfections may be introduced intothe control system, and the closed-loop control in substance changes toopen-loop during disturbances. We aim to improve the performance of theseopen-loop control periods by applying methods that can compensate for theimperfections statistically or in a guaranteed way. We demonstrate thatco-design-based application improvements with minimal dependencies on theunderlying technologies can already yield an order of magnitude gain when itcomes to the accurate execution of the robot trajectories during the openloopcontrol periods. Furthermore, by combining the proposed methods, theperformance improvements add up and can produce up to 45% shorter trajectoryexecutions compared to individual evaluations.</description>
      <author>example@mail.com (Sándor Rácz, Norbert Reider)</author>
      <guid isPermaLink="false">2503.03521v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>NeuGrasp: Generalizable Neural Surface Reconstruction with Background Priors for Material-Agnostic Object Grasp Detection</title>
      <link>http://arxiv.org/abs/2503.03511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures. IEEE International Conference on Robotics and  Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NeuGrasp是一种基于神经网络的表面重建方法，旨在通过利用背景先验来实现材料无关的手抓取检测。它在透明和镜面物体场景中表现出色。&lt;h4&gt;背景&lt;/h4&gt;现有的手抓取方法在处理包含透明或镜面反射物体的情况下遇到挑战，这些方法依赖于准确的深度信息难以应对。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的神经网络表面重建方法NeuGrasp，以实现更稳健的手抓取检测，并能有效应对具有透明和镜面特性的物体。&lt;h4&gt;方法&lt;/h4&gt;引入了transformer模型和全局先验体积来集成多视角特征与空间编码，通过残差特征增强专注于前景对象并通过占用先验体来改进空间感知。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在模拟和现实世界场景中NeuGrasp在手抓取方面优于现有的最佳方法，并且保持了相当的重建质量。&lt;h4&gt;结论&lt;/h4&gt;NeuGrasp能够有效处理具有透明或镜面特性的物体，展示出其在复杂环境中的强大能力。&lt;h4&gt;翻译&lt;/h4&gt;机器人抓取技术在面对含有透明和镜面反射物体的情况时面临巨大挑战。本文介绍了一种名为NeuGrasp的神经表面重建方法，该方法利用背景先验来实现材料无关的手部抓取检测。通过整合transformer模型与全局先验体积，NeuGrasp能有效聚合多视角特征并进行空间编码，在狭小且视角稀疏条件下具有优秀的表观重建能力。此外，它还能够通过专注于前景物体的残差特征增强及占用体优化来改进空间感知，从而更有效地处理透明和镜面表面对象。在模拟与实际场景中的大量实验表明，NeuGrasp的手部抓取性能超越现有最佳方法，并且具有可比的重建质量。详情请访问https://neugrasp.github.io/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic grasping in scenes with transparent and specular objects presentsgreat challenges for methods relying on accurate depth information. In thispaper, we introduce NeuGrasp, a neural surface reconstruction method thatleverages background priors for material-agnostic grasp detection. NeuGraspintegrates transformers and global prior volumes to aggregate multi-viewfeatures with spatial encoding, enabling robust surface reconstruction innarrow and sparse viewing conditions. By focusing on foreground objects throughresidual feature enhancement and refining spatial perception with anoccupancy-prior volume, NeuGrasp excels in handling objects with transparentand specular surfaces. Extensive experiments in both simulated and real-worldscenarios show that NeuGrasp outperforms state-of-the-art methods in graspingwhile maintaining comparable reconstruction quality. More details are availableat https://neugrasp.github.io/.</description>
      <author>example@mail.com (Qingyu Fan, Yinghao Cai, Chao Li, Wenzhe He, Xudong Zheng, Tao Lu, Bin Liang, Shuo Wang)</author>
      <guid isPermaLink="false">2503.03511v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Benchmark for Optimal Multi-Modal Multi-Robot Multi-Goal Path Planning with Given Robot Assignment</title>
      <link>http://arxiv.org/abs/2503.03509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种新的机器人路径规划问题的解决方案，用于多机器人在共享工作空间中完成任务的问题，并提出了一个涵盖多种场景基准测试。&lt;h4&gt;背景描述&lt;/h4&gt;多个工业机器人在同一工作空间内共同作业以尽快完成一系列任务。这类设置可被视为一个多模式、多机器人和多目标的路径规划问题。&lt;h4&gt;研究目的&lt;/h4&gt;通过将该问题形式化为单一路径规划问题，提出了一种基准测试方法，并引入了适应于复杂情况下的RRT*和PRM*路径规划器作为基线。&lt;h4&gt;创新方法&lt;/h4&gt;本文的方法不同于以往以优先级处理或假设同步完成任务的方式，而是采用复合空间的路径规划方式，在非离散2D工作环境下也适用，支持动态环境变化，并适用于不同约束条件的异构机器人团队。&lt;h4&gt;关键贡献&lt;/h4&gt;引入了一个多样化的基准测试集，涵盖了具有不同类型、规划时间线和协作任务（如交接）的不同问题实例。同时，改进了RRT*和PRM*路径规划器以适应复杂情况。&lt;h4&gt;结论&lt;/h4&gt;通过提出新的方法和基准测试，为解决多机器人协同工作中的路径规划问题提供了更有效的方法。&lt;h4&gt;翻译&lt;/h4&gt;在许多工业机器人的应用中，多个机器人在同一共享工作空间内共同作业，以尽可能快速地完成一系列任务。这类设置可以视为一个多模式、多机器人和多目标的路径规划问题，其中每个机器人必须到达一个有序的目标序列。现有的方法通过优先级处理或假设同步完成任务来解决此类问题，并因此既不是最优也不是完整的。本文将这个问题形式化为单一路径规划问题，并引入了一个基准测试，涵盖了包括具有不同类型机器人、不同规划时间线和协作任务（如交接）在内的多种情况的问题实例。此外，除了这个基准测试，还适应了RRT*和PRM*规划器作为基线方法以处理这些问题。与现有方法不同的是，本文的路径规划器和方案不受限于离散2D工作空间，并支持动态环境变化，在具有不同类型约束条件的异构机器人团队中使用多个模式和目标时也非常适用。基准测试及其规划器的相关视频和代码可在https://vhartman.github.io/mrmg-planning/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In many industrial robotics applications, multiple robots are working in ashared workspace to complete a set of tasks as quickly as possible. Suchsettings can be treated as multi-modal multi-robot multi-goal path planningproblems, where each robot has to reach an ordered sequence of goals. Existingapproaches to this type of problem solve this using prioritization or assumesynchronous completion of tasks, and are thus neither optimal nor complete. Weformalize this problem as a single path planning problem and introduce abenchmark encompassing a diverse range of problem instances including scenarioswith various robots, planning horizons, and collaborative tasks such ashandovers. Along with the benchmark, we adapt an RRT* and a PRM* planner toserve as a baseline for the planning problems. Both planners work in thecomposite space of all robots and introduce the required changes to work in oursetting. Unlike existing approaches, our planner and formulation is notrestricted to discretized 2D workspaces, supports a changing environment, andworks for heterogeneous robot teams over multiple modes with differentconstraints, and multiple goals. Videos and code for the benchmark and theplanners is available at https://vhartman.github.io/mrmg-planning/.</description>
      <author>example@mail.com (Valentin N. Hartmann, Tirza Heinle, Stelian Coros)</author>
      <guid isPermaLink="false">2503.03509v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Coordinated Trajectories for Non-stop Flying Carriers Holding a Cable-Suspended Load</title>
      <link>http://arxiv.org/abs/2503.03481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多旋翼无人机在空中操作中通常被视为有限耐力的设备，但本文展示了一种方法可以使用连续飞行的三架或多架无人机来保持吊挂负载的恒定姿态。&lt;h4&gt;背景&lt;/h4&gt;现有的多旋翼无人机由于能量限制无法长时间进行空中操作任务。&lt;h4&gt;目的&lt;/h4&gt;探索并实现一种利用连续飞行的非停止型无人机进行高效空中操作的方法，同时确保吊挂负载在动态环境中的稳定。&lt;h4&gt;方法&lt;/h4&gt;{'选择内部力方向': '选择了$n$个特殊线性独立的方向作为载荷抓取矩阵零空间内的内力，在连接载荷悬架点的图中形成一个哈密顿圈。相邻对的方向用于生成作用于不同二维仿射子空间的$n$个力量。', '构建椭圆轨迹': '通过适当的图着色，将每个哈密顿环的边映射到周期坐标上，确保没有相邻坐标在同时显示零导数的情况下仍然可以构建出椭圆形轨迹。'}&lt;h4&gt;主要发现&lt;/h4&gt;这些选择和构造条件保证了$n$个力轨迹投影到相应的电缆约束球体上的非零切线速度，从而使载荷保持静止的同时无人机能够进行持续的运动。&lt;h4&gt;结论&lt;/h4&gt;理论成果通过模拟和实验室实验中的非停止多旋翼无人机进行了验证。&lt;h4&gt;翻译&lt;/h4&gt;多旋翼UAV通常被认为适合空中操作，但由于其有限的耐力限制了长时间的操作任务。这项工作展示了三架或更多连续飞行载体能够保持吊挂载荷恒定姿态的可能性，并且提出了生成协同无间断轨迹的算法。这种方法基于两个支柱：（1）选择$n$个特殊线性独立方向作为载荷抓取矩阵零空间内的内力，形成一个哈密顿圈连接悬架点；（2）在这些子空间中构建椭圆轨迹，在适当的图着色下，每个哈密顿环的边映射到周期坐标上，并确保没有相邻坐标同时显示零导数。结合负载静态条件和悬挂点位置，这些选择保证了$n$个力轨迹投影到相应的电缆约束球体上的非零切线速度，从而在载荷静止的情况下实现载体的永不停歇运动。理论发现已通过模拟和实验室实验验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multirotor UAVs have been typically considered for aerial manipulation, buttheir scarce endurance prevents long-lasting manipulation tasks. This workdemonstrates that the non-stop flights of three or more carriers are compatiblewith holding a constant pose of a cable-suspended load, thus potentiallyenabling aerial manipulation with energy-efficient non-stop carriers. It alsopresents an algorithm for generating the coordinated non-stop trajectories. Theproposed method builds upon two pillars: (1)~the choice of $n$ special linearlyindependent directions of internal forces within the $3n-6$-dimensionalnullspace of the grasp matrix of the load, chosen as the edges of a Hamiltoniancycle on the graph that connects the cable attachment points on the load.Adjacent pairs of directions are used to generate $n$ forces evolving ondistinct 2D affine subspaces, despite the attachment points being genericallyin 3D; (2)~the construction of elliptical trajectories within these subspacesby mapping, through appropriate graph coloring, each edge of the Hamiltoniancycle to a periodic coordinate while ensuring that no adjacent coordinatesexhibit simultaneous zero derivatives. Combined with conditions for loadstatics and attachment point positions, these choices ensure that each of the$n$ force trajectories projects onto the corresponding cable constraint spherewith non-zero tangential velocity, enabling perpetual motion of the carrierswhile the load is still. The theoretical findings are validated throughsimulations and laboratory experiments with non-stopping multirotor UAVs.</description>
      <author>example@mail.com (Chiara Gabellieri, Antonio Franchi)</author>
      <guid isPermaLink="false">2503.03481v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2503.03480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为SafeVLA的新算法，旨在将安全性融入到视觉-语言-动作模型(VLAS)中，以保护真实世界中的环境、机器人硬件和人类的安全。&lt;h4&gt;背景&lt;/h4&gt;VLAs作为一种通用的机器人策略显示出巨大的潜力，但在实际部署时带来了紧迫的安全挑战，包括对环境、机器人本身以及人类可能造成的物理伤害的风险。&lt;h4&gt;目的&lt;/h4&gt;为了将安全性明确地整合到VLAs中，设计了一种新的算法SafeVLA。&lt;h4&gt;方法&lt;/h4&gt;通过在模拟环境中使用大规模约束学习来有效地平衡安全性和任务性能。&lt;h4&gt;主要发现&lt;/h4&gt;与当前最先进的方法相比，在仿真测试中实现了83.58%的安全性改进和3.85%的任务性能提升；优先考虑安全性可以消除高风险行为并将不安全行为的上限降低到1/35，从而显著减少长尾风险。&lt;h4&gt;结论&lt;/h4&gt;SafeVLA不仅在模拟实验中表现出色，在处理多种未知场景时也展示了良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Vision-language-action models (VLAs) have shown great potential as generalistrobot policies. However, these models pose urgent safety challenges duringdeployment, including the risk of physical harm to the environment, the robotitself, and humans. How can safety be explicitly incorporated into VLAs? Inthis work, we propose SafeVLA, a novel algorithm designed to integrate safetyinto VLAs, ensuring the protection of the environment, robot hardware andhumans in real-world settings. SafeVLA effectively balances safety and taskperformance by employing large-scale constrained learning within simulatedenvironments. We demonstrate that SafeVLA outperforms the currentstate-of-the-art method in both safety and task performance, achieving averageimprovements of 83.58% and 3.85%, respectively, in simulation. By prioritizingsafety, our approach eliminates high-risk behaviors and reduces the upper boundof unsafe behaviors to 1/35 of that in the current state-of-the-art, therebysignificantly mitigating long-tail risks. Furthermore, the learned safetyconstraints generalize to diverse, unseen scenarios, including multipleout-of-distribution perturbations and tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language-action models (VLAs) have shown great potential as generalistrobot policies. However, these models pose urgent safety challenges duringdeployment, including the risk of physical harm to the environment, the robotitself, and humans. How can safety be explicitly incorporated into VLAs? Inthis work, we propose SafeVLA, a novel algorithm designed to integrate safetyinto VLAs, ensuring the protection of the environment, robot hardware andhumans in real-world settings. SafeVLA effectively balances safety and taskperformance by employing large-scale constrained learning within simulatedenvironments. We demonstrate that SafeVLA outperforms the currentstate-of-the-art method in both safety and task performance, achieving averageimprovements of 83.58% and 3.85%, respectively, in simulation. By prioritizingsafety, our approach eliminates high-risk behaviors and reduces the upper boundof unsafe behaviors to 1/35 of that in the current state-of-the-art, therebysignificantly mitigating long-tail risks. Furthermore, the learned safetyconstraints generalize to diverse, unseen scenarios, including multipleout-of-distribution perturbations and tasks. Our data, models and newlyproposed benchmark environment are available athttps://sites.google.com/view/pku-safevla.</description>
      <author>example@mail.com (Borong Zhang, Yuhao Zhang, Jiaming Ji, Yingshan Lei, Josef Dai, Yuanpei Chen, Yaodong Yang)</author>
      <guid isPermaLink="false">2503.03480v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Continuous Control of Diverse Skills in Quadruped Robots Without Complete Expert Datasets</title>
      <link>http://arxiv.org/abs/2503.03476v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的四足机器人技能学习方法PASIST，该方法能够在没有完整专家数据集的情况下自主探索和选择高质量的轨迹，并且能够实现平滑自然的动作转换。&lt;h4&gt;背景&lt;/h4&gt;当前用于四足机器人的模仿学习方法虽然有效，但需要昂贵的数据集来复制专家行为。现有的方法无法处理不同难度的任务以及技能之间的复杂过渡问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自模仿技能过渡方法，以减少对专家数据集的依赖，并提高机器人在各种任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;PASIST基于预定义的目标姿态自主探索并选择高质量轨迹，利用生成对抗自模仿学习（GASIL）框架。此外还开发了一个技能选择模块来缓解模式崩溃问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过引入新的技能选择机制，可以在不依赖于完整专家数据集的情况下实现平滑自然的动作转换和复杂任务的处理。&lt;h4&gt;结论&lt;/h4&gt;实验结果验证了PASIST的有效性，并表明其为专家驱动的学习提供了一种高效的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;学习四足机器人的多样化技能面临着重大挑战，如掌握不同技能之间的复杂过渡以及应对难度各异的任务。现有的模仿学习方法虽然成功，但依赖于昂贵的数据集来复制专家行为。受自我反思学习的启发，我们提出了一种新的方法PASIST，它可以消除对完整专家数据集的需求。PASIST基于预定义的目标姿态自主探索并选择高质量轨迹，并利用生成对抗自模仿学习（GASIL）框架。为了进一步增强学习效果，我们开发了一个技能选择模块来平衡难度不同的技能权重以减少模式崩溃问题。通过这些方法，PASIST能够在没有完整专家数据集的情况下复制对应于目标姿势的技能并且实现平滑自然的动作转换。在仿真平台和Solo 8机器人上的评估确认了PASIST的有效性，并为专家驱动的学习提供了有效的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning diverse skills for quadruped robots presents significant challenges,such as mastering complex transitions between different skills and handlingtasks of varying difficulty. Existing imitation learning methods, whilesuccessful, rely on expensive datasets to reproduce expert behaviors. Inspiredby introspective learning, we propose Progressive Adversarial Self-ImitationSkill Transition (PASIST), a novel method that eliminates the need for completeexpert datasets. PASIST autonomously explores and selects high-qualitytrajectories based on predefined target poses instead of demonstrations,leveraging the Generative Adversarial Self-Imitation Learning (GASIL)framework. To further enhance learning, We develop a skill selection module tomitigate mode collapse by balancing the weights of skills with varying levelsof difficulty. Through these methods, PASIST is able to reproduce skillscorresponding to the target pose while achieving smooth and natural transitionsbetween them. Evaluations on both simulation platforms and the Solo 8 robotconfirm the effectiveness of PASIST, offering an efficient alternative toexpert-driven learning.</description>
      <author>example@mail.com (Jiaxin Tu, Xiaoyi Wei, Yueqi Zhang, Taixian Hou, Xiaofei Gao, Zhiyan Dong, Peng Zhai, Lihua Zhang)</author>
      <guid isPermaLink="false">2503.03476v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Generative Artificial Intelligence in Robotic Manipulation: A Survey</title>
      <link>http://arxiv.org/abs/2503.03464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;综述了机器人操作领域中生成学习模型的最新进展，概述了这些模型的关键挑战，并介绍了几种生成式模型范式及其应用。&lt;h4&gt;背景&lt;/h4&gt;在机器人操作中面临的主要瓶颈包括数据不足和采集效率低、长期复杂的任务规划以及跨不同环境进行稳健策略学习所需的多模态推理能力。&lt;h4&gt;目的&lt;/h4&gt;介绍用于解决上述关键问题的生成式模型，如GANs、VAEs、扩散模型、概率流模型及自回归模型，并探讨它们的优势与局限性。&lt;h4&gt;方法&lt;/h4&gt;将生成模型的应用分为基础层（数据和奖励生成）、中间层（语言、代码、视觉与状态生成）以及策略层（抓取和轨迹生成），详细讨论每个层次的代表性工作。&lt;h4&gt;主要发现&lt;/h4&gt;指出未来研究需提高数据利用效率，更好地处理长期任务，并在多样化机器人应用场景中提升泛化能力。&lt;h4&gt;结论&lt;/h4&gt;强调了需要改进现有方法以应对机器人操作中的关键挑战，并提供了相关的资源链接供社区使用。&lt;h4&gt;翻译&lt;/h4&gt;该综述提供了一种关于最近用于机器人操作的生成学习模型进展的全面回顾，这些模型旨在解决领域内的关键问题。概述了几种生成式模型（如GANs、VAEs等）及其应用层级，并指出了未来研究的方向和挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This survey provides a comprehensive review on recent advancements ofgenerative learning models in robotic manipulation, addressing key challengesin the field. Robotic manipulation faces critical bottlenecks, includingsignificant challenges in insufficient data and inefficient data acquisition,long-horizon and complex task planning, and the multi-modality reasoningability for robust policy learning performance across diverse environments. Totackle these challenges, this survey introduces several generative modelparadigms, including Generative Adversarial Networks (GANs), VariationalAutoencoders (VAEs), diffusion models, probabilistic flow models, andautoregressive models, highlighting their strengths and limitations. Theapplications of these models are categorized into three hierarchical layers:the Foundation Layer, focusing on data generation and reward generation; theIntermediate Layer, covering language, code, visual, and state generation; andthe Policy Layer, emphasizing grasp generation and trajectory generation. Eachlayer is explored in detail, along with notable works that have advanced thestate of the art. Finally, the survey outlines future research directions andchallenges, emphasizing the need for improved efficiency in data utilization,better handling of long-horizon tasks, and enhanced generalization acrossdiverse robotic scenarios. All the related resources, including researchpapers, open-source data, and projects, are collected for the community inhttps://github.com/GAI4Manipulation/AwesomeGAIManipulation</description>
      <author>example@mail.com (Kun Zhang, Peng Yun, Jun Cen, Junhao Cai, Didi Zhu, Hangjie Yuan, Chao Zhao, Tao Feng, Michael Yu Wang, Qifeng Chen, Jia Pan, Bo Yang, Hua Chen)</author>
      <guid isPermaLink="false">2503.03464v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>REACT: Real-time Efficient Attribute Clustering and Transfer for Updatable 3D Scene Graph</title>
      <link>http://arxiv.org/abs/2503.03412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;REACT框架通过实时属性聚类和迁移技术，实现了在动态环境中对3D场景图中的对象节点进行重新定位。&lt;h4&gt;背景&lt;/h4&gt;现代自主机器人需要高级地图表示来执行复杂任务。最近，3D场景图（3DSGs）作为传统网格地图的有前途替代品出现，它结合了高效的内存使用和丰富的特征表示。&lt;h4&gt;目的&lt;/h4&gt;引入REACT框架以实现在动态环境中实时重新定位对象节点，并保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;REACT采用了一种新颖的方法来比较对象实例，该方法基于三元损失训练的嵌入模型进行。此方法促进了实例聚类和匹配。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，REACT能够在维护计算效率的同时重新定位物体。&lt;h4&gt;结论&lt;/h4&gt;REACT框架将作为开源项目公开发布，推动可重用性和更新性的3D场景图技术的发展。&lt;h4&gt;翻译&lt;/h4&gt;现代自主机器人需要高级地图表示来执行复杂任务。最近，3D场景图（3DSGs）作为一种有前途的替代品出现了，它可以结合高效的内存使用和丰富的特征表示。然而，大多数应用于该领域的努力都局限于静态世界。这项工作介绍了REACT框架，该框架能够有效地执行实时属性聚类和转移以重新定位3DSG中的对象节点。REACT采用了一种新颖的方法来比较对象实例，这种方法基于三元损失训练的嵌入模型进行。实验结果显示，REACT能够在维护计算效率的同时重新定位物体。REACT框架作为开源项目将被发布，这将促进可重用性和更新性的3D场景图技术的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern-day autonomous robots need high-level map representations to performsophisticated tasks. Recently, 3D scene graphs (3DSGs) have emerged as apromising alternative to traditional grid maps, blending efficient memory useand rich feature representation. However, most efforts to apply them have beenlimited to static worlds. This work introduces REACT, a framework thatefficiently performs real-time attribute clustering and transfer to relocalizeobject nodes in a 3DSG. REACT employs a novel method for comparing objectinstances using an embedding model trained on triplet loss, facilitatinginstance clustering and matching. Experimental results demonstrate that REACTis able to relocalize objects while maintaining computational efficiency. TheREACT framework's source code will be available as an open-source project,promoting further advancements in reusable and updatable 3DSGs.</description>
      <author>example@mail.com (Phuoc Nguyen, Francesco Verdoja, Ville Kyrki)</author>
      <guid isPermaLink="false">2503.03412v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Navigating Intelligence: A Survey of Google OR-Tools and Machine Learning for Global Path Planning in Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2503.03338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文深入研究了无人地面车辆的全局路径规划（GPP），特别是在自主采矿采样机器人ROMIE中的应用。通过解决旅行商问题优化成本和时间，旨在提高其运营效率。&lt;h4&gt;背景&lt;/h4&gt;全球路径规划对于自动化的矿业采样机器人的性能至关重要，特别是通过解决复杂的图论挑战——旅行商问题来实现高效路径覆盖。&lt;h4&gt;目的&lt;/h4&gt;开发、评估并改进一种低成本的软件和网络应用程序以优化全局路径规划。研究重点在于运用和测试Google运营研究工具的能力，并首次结合强化学习技术进行比较分析。&lt;h4&gt;方法&lt;/h4&gt;深入对比分析Google OR-Tools中的各种优化算法，通过实验确定Q-Learning等方法相对于OR-Tools的计算有效性和实际应用效率。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，Q-Learning在测试数据集上表现出色，平均偏离最优解仅为1.2%，优于其他比较方法。&lt;h4&gt;结论&lt;/h4&gt;Q-Learning是解决全局路径规划问题中最有效的策略之一，它展示了相对于传统优化算法的显著优势。&lt;h4&gt;翻译&lt;/h4&gt;我们提供了一项关于无人地面车辆全球路径规划（GPP）的新颖深入研究，聚焦于一个自主采矿采样机器人ROMIE的应用。GPP对于ROMIE的最佳性能是至关重要的，这被转化为解决旅行商问题——这是一个复杂的图论挑战，对于确定覆盖矿业场地内所有采样地点的最有效路线至关重要。这个问题的核心在于通过优化成本和时间来提高ROMIE的操作效率，并且在与人力竞争时保持其竞争力。本研究的主要目的是通过开发、评估并改进一种低成本软件和网络应用程序来推进GPP的研究。我们深入比较分析了Google运营研究（OR）-Tools中的优化算法，我们的研究旨在首次将强化学习技术应用于这些工具中，以应用和测试它们的能力限制，并将这些方法与OR-Tools进行对比，评估其计算有效性和实际应用效率。本项研究试图提供关于每种技术的有效性及实际应用情况的见解。我们发现Q-Learning在所有数据集上表现出色，平均仅偏离最优解1.2%，因此它被证明是最优策略之一。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1002/aisy.202300840&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We offer a new in-depth investigation of global path planning (GPP) forunmanned ground vehicles, an autonomous mining sampling robot named ROMIE. GPPis essential for ROMIE's optimal performance, which is translated into solvingthe traveling salesman problem, a complex graph theory challenge that iscrucial for determining the most effective route to cover all samplinglocations in a mining field. This problem is central to enhancing ROMIE'soperational efficiency and competitiveness against human labor by optimizingcost and time. The primary aim of this research is to advance GPP bydeveloping, evaluating, and improving a cost-efficient software and webapplication. We delve into an extensive comparison and analysis of Googleoperations research (OR)-Tools optimization algorithms. Our study is driven bythe goal of applying and testing the limits of OR-Tools capabilities byintegrating Reinforcement Learning techniques for the first time. This enablesus to compare these methods with OR-Tools, assessing their computationaleffectiveness and real-world application efficiency. Our analysis seeks toprovide insights into the effectiveness and practical application of eachtechnique. Our findings indicate that Q-Learning stands out as the optimalstrategy, demonstrating superior efficiency by deviating only 1.2% on averagefrom the optimal solutions across our datasets.</description>
      <author>example@mail.com (Alexandre Benoit, Pedram Asef)</author>
      <guid isPermaLink="false">2503.03338v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Supervised Visual Docking Network for Unmanned Surface Vehicles Using Auto-labeling in Real-world Water Environments</title>
      <link>http://arxiv.org/abs/2503.03282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种用于无人水面艇（USV）自主视觉靠泊的新型监督学习流水线，并采用了自动标注技术。通过设计了一个无需人工标记的数据收集流程，开发了神经靠泊姿态估计器（NDPE），该系统能够实现对相对靠泊姿态的预测。&lt;h4&gt;背景&lt;/h4&gt;无人水面船在环境监测和河流建模等水上作业中被广泛应用，但在港口或站点实现精确自主靠岸仍面临挑战，这通常需要远程人类控制或外部定位系统的辅助以确保准确性和安全性。这种依赖限制了USV完全自动化的潜力。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新颖的监督学习流水线和自动标注技术，用于提升无人水面艇（USVs）在视觉引导下的自主靠泊能力。&lt;h4&gt;方法&lt;/h4&gt;设计了一种自动化数据收集流程来生成相对姿态和图像对的数据集，并提出了神经靠泊姿态估计器NDPE以实现不需要手工特征工程、相机校准或外部标记的相对靠泊姿态预测。此外，该系统能够准确地预测实际水环境中的相对靠泊姿态，支持基于位置的视觉伺服（PBVS）和低级运动控制器的有效实施。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，NDPE对距离变化和USV速度扰动具有鲁棒性，并且在真实水域环境中验证了该解决方案的有效性和可操作性。&lt;h4&gt;结论&lt;/h4&gt;提出的解决方案能够有效地处理现实世界中的自主靠泊任务，证明了其强大的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unmanned Surface Vehicles (USVs) are increasingly applied to water operationssuch as environmental monitoring and river-map modeling. It faces a significantchallenge in achieving precise autonomous docking at ports or stations, stillrelying on remote human control or external positioning systems for accuracyand safety which limits the full potential of human-out-of-loop deployment forUSVs.This paper introduces a novel supervised learning pipeline with theauto-labeling technique for USVs autonomous visual docking. Firstly, wedesigned an auto-labeling data collection pipeline that appends relative poseand image pair to the dataset. This step does not require conventional manuallabeling for supervised learning. Secondly, the Neural Dock Pose Estimator(NDPE) is proposed to achieve relative dock pose prediction without the needfor hand-crafted feature engineering, camera calibration, and peripheralmarkers. Moreover, The NDPE can accurately predict the relative dock pose inreal-world water environments, facilitating the implementation ofPosition-Based Visual Servo (PBVS) and low-level motion controllers forefficient and autonomous docking.Experiments show that the NDPE is robust tothe disturbance of the distance and the USV velocity. The effectiveness of ourproposed solution is tested and validated in real-world water environments,reflecting its capability to handle real-world autonomous docking tasks.</description>
      <author>example@mail.com (Yijie Chu, Ziniu Wu, Yong Yue, Eng Gee Lim, Paolo Paoletti, Xiaohui Zhu)</author>
      <guid isPermaLink="false">2503.03282v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Trajectory Prediction for Autonomous Driving: Progress, Limitations, and Future Directions</title>
      <link>http://arxiv.org/abs/2503.03262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;随着自动驾驶车辆在现代交通系统中集成潜力的增加，确保其在动态环境中的安全导航变得至关重要。为了保证安全性并防止碰撞，自动驾驶汽车必须能够准确预测周围交通代理的轨迹。&lt;h4&gt;背景&lt;/h4&gt;过去十年来，学术界和工业界都投入了大量的精力设计精确的轨迹预测解决方案，这些努力产生了一系列不同的方法，并提出了关于不同方法之间差异以及轨迹预测挑战是否已经完全解决的问题。&lt;h4&gt;目的&lt;/h4&gt;本文回顾了近期大量的轨迹预测方法，并提出了一种分类现有解决方案的方法。此外还提供了一个预测流水线的一般概述，涵盖了输入和输出模式、建模特征及预测范式。&lt;h4&gt;方法&lt;/h4&gt;提出了一个详细的分类法来对现有的预测方法进行分类，并讨论了一些活跃的研究领域。&lt;h4&gt;主要发现&lt;/h4&gt;通过文献综述，指出了当前研究中存在的问题以及未解决的挑战。该论文强调了在轨迹预测方面仍存在的研究空白和未来方向。&lt;h4&gt;结论&lt;/h4&gt;文章探讨并总结了自动驾驶车辆中的轨迹预测领域的现状及其面临的挑战，并提出了一些重要的开放性问题需要进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;随着自主驾驶车辆在现代交通系统中大规模整合的潜力不断增长，确保其在动态环境下的安全导航变得至关重要。为了保证安全性以及防止碰撞的发生，自主驾驶汽车必须具备准确预测周围交通代理轨迹的能力。在过去十年里，学术界和工业界已经投入了大量资源来设计精确的路径预测解决方案，这些努力催生了许多不同的方法，并引发了关于不同技术之间差异以及是否所有相关的挑战都已经被解决的问题。本文对近期大量的轨迹预测方法进行了回顾，并为现有解决方案制定了分类体系。文章还提供了一个涵盖输入输出模式、建模特征及预测范式的预测流程概览。此外，该文讨论了当前活跃的研究领域，回答了一些研究问题，并强调在这一领域里仍然存在的研究缺口和挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the potential for autonomous vehicles to be integrated on a large scaleinto modern traffic systems continues to grow, ensuring safe navigation indynamic environments is crucial for smooth integration. To guarantee safety andprevent collisions, autonomous vehicles must be capable of accuratelypredicting the trajectories of surrounding traffic agents. Over the pastdecade, significant efforts from both academia and industry have been dedicatedto designing solutions for precise trajectory forecasting. These efforts haveproduced a diverse range of approaches, raising questions about the differencesbetween these methods and whether trajectory prediction challenges have beenfully addressed. This paper reviews a substantial portion of recent trajectoryprediction methods and devises a taxonomy to classify existing solutions. Ageneral overview of the prediction pipeline is also provided, covering inputand output modalities, modeling features, and prediction paradigms discussed inthe literature. In addition, the paper discusses active research areas withintrajectory prediction, addresses the posed research questions, and highlightsthe remaining research gaps and challenges.</description>
      <author>example@mail.com (Nadya Abdel Madjid, Abdulrahman Ahmad, Murad Mebrahtu, Yousef Babaa, Abdelmoamen Nasser, Sumbal Malik, Bilal Hassan, Naoufel Werghi, Jorge Dias, Majid Khonji)</author>
      <guid isPermaLink="false">2503.03262v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SCORE: Saturated Consensus Relocalization in Semantic Line Maps</title>
      <link>http://arxiv.org/abs/2503.03254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 14 figurs, arxiv version for paper submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于语义标注3D线的场景无关且轻量级视觉重定位框架。&lt;h4&gt;背景&lt;/h4&gt;传统的视觉重定位方法难以处理极高的异常值比率（超过99.5%），特别是在语义匹配中出现的一对多模糊情况。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种能够准确估计姿态并在经典共识最大化框架失效时仍能工作的算法。&lt;h4&gt;方法&lt;/h4&gt;引入饱和一致性最大化的形式化方法，并提出了一种快速全局求解器，利用严格的区间分析结果保证了精度和计算效率。此外，还构建了一个用于构造语义3D线图的流水线。&lt;h4&gt;主要发现&lt;/h4&gt;通过在ScanNet++数据集上的广泛实验验证了所提框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的饱和一致性最大化的形式化方法能够在极高的异常值比率下准确估计姿态，并且整个框架结合了鲁棒估算和实用工程见解，具有很高的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;摘要的原文是：这是提交给IEEE/RSJ IROS 2025的一篇论文的arxiv版本。我们提出了一种场景无关并且轻量级的视觉重定位框架，该框架利用语义标注的3D线作为紧凑的地图表示。在我们的框架中，机器人通过捕获单个图像、提取2D线、将其与地图中的语义相似的3D线关联起来，并解决一个鲁棒的透视-n-线问题来自我定位。为了解决由于语义匹配的一对多模糊性导致异常值比率极高（超过99.5%）的问题，我们引入了饱和一致性最大化(Sat-CM)形式化方法，它在经典共识最大化的框架失效时仍能实现准确的姿态估计。此外，还提出了快速全局求解器以解决所提出的Sat-CM问题，利用严格的区间分析结果确保精度和计算效率的同时性。另外，开发了一条管道来构建使用带姿态的深度图像的语义3D线地图。为了验证我们框架的有效性，并整合了我们在鲁棒估计方面的创新成果以及实用工程见解，在ScanNet++数据集上进行了广泛的实验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This is the arxiv version for our paper submitted to IEEE/RSJ IROS 2025. Wepropose a scene-agnostic and light-weight visual relocalization framework thatleverages semantically labeled 3D lines as a compact map representation. In ourframework, the robot localizes itself by capturing a single image, extracting2D lines, associating them with semantically similar 3D lines in the map, andsolving a robust perspective-n-line problem. To address the extremely highoutlier ratios~(exceeding 99.5\%) caused by one-to-many ambiguities in semanticmatching, we introduce the Saturated Consensus Maximization~(Sat-CM)formulation, which enables accurate pose estimation when the classic ConsensusMaximization framework fails. We further propose a fast global solver to theformulated Sat-CM problems, leveraging rigorous interval analysis results toensure both accuracy and computational efficiency. Additionally, we develop apipeline for constructing semantic 3D line maps using posed depth images. Tovalidate the effectiveness of our framework, which integrates our innovationsin robust estimation and practical engineering insights, we conduct extensiveexperiments on the ScanNet++ dataset.</description>
      <author>example@mail.com (Haodong Jiang, Xiang Zheng, Yanglin Zhang, Qingcheng Zeng, Yiqian Li, Ziyang Hong, Junfeng Wu)</author>
      <guid isPermaLink="false">2503.03254v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>STORM: Spatial-Temporal Iterative Optimization for Reliable Multicopter Trajectory Generation</title>
      <link>http://arxiv.org/abs/2503.03252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种时空迭代优化框架，以提高四旋翼无人机轨迹规划的效率和安全性。&lt;h4&gt;背景&lt;/h4&gt;目前，无人机轨迹优化问题中存在约束合规性和计算效率提升之间的固有折衷。&lt;h4&gt;目的&lt;/h4&gt;增强无人机轨迹优化性能。&lt;h4&gt;方法&lt;/h4&gt;利用B样条表示无人机轨迹，并通过严格控制点上的约束执行来确保严格的飞行安全；然后推导出一系列通过时空解耦和约束线性化得出的QP-LP子问题；最后采用一种结合指导梯度的迭代优化策略，在不同场景下获得高性能无人机轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果以及实地实验验证了所提出的优化框架在生成既安全又快速的轨迹方面的效率和高性能。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法能够有效地解决无人机轨迹规划中约束合规性和计算效率之间的矛盾，提供了一种有效、高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;高效且安全的轨迹规划对于四旋翼无人飞行器的应用至关重要。当前问题在于如何在满足约束的同时提升计算效率。为改善无人机轨迹优化性能，作者提出了一种时空迭代优化框架。此方法利用B样条表示轨迹，并通过控制点上的严格约束来确保安全性；然后推导了一系列子问题并采用一种结合指导梯度的策略；最终实验结果验证了该框架的有效性与高性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient and safe trajectory planning plays a critical role in theapplication of quadrotor unmanned aerial vehicles. Currently, the inherenttrade-off between constraint compliance and computational efficiencyenhancement in UAV trajectory optimization problems has not been sufficientlyaddressed. To enhance the performance of UAV trajectory optimization, wepropose a spatial-temporal iterative optimization framework. Firstly, B-splinesare utilized to represent UAV trajectories, with rigorous safety assuranceachieved through strict enforcement of constraints on control points.Subsequently, a set of QP-LP subproblems via spatial-temporal decoupling andconstraint linearization is derived. Finally, an iterative optimizationstrategy incorporating guidance gradients is employed to obtainhigh-performance UAV trajectories in different scenarios. Both simulation andreal-world experimental results validate the efficiency and high-performance ofthe proposed optimization framework in generating safe and fast trajectories.Our source codes will be released for community reference athttps://hitsz-mas.github.io/STORM</description>
      <author>example@mail.com (Jinhao Zhang, Zhexuan Zhou, Wenlong Xia, Youmin Gong, Jie Mei)</author>
      <guid isPermaLink="false">2503.03252v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Social Gesture Recognition in spHRI: Leveraging Fabric-Based Tactile Sensing on Humanoid Robots</title>
      <link>http://arxiv.org/abs/2503.03234v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 25. 8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了使用集成在人形机器人手臂上的基于织物的大规模触觉传感器的社会手势识别系统。&lt;h4&gt;背景&lt;/h4&gt;人类能够仅通过触摸传达不同的信息，为使机器人理解社交触感能力增加了一种新的沟通方式。&lt;h4&gt;目的&lt;/h4&gt;构建一个社会手势数据集并提取用于分类的时间特征，以更好地了解人与机器人的社交互动，并促进更加自然和有效的交流。&lt;h4&gt;方法&lt;/h4&gt;利用多参与者建立了一个社会手势数据集，并收集了在实际环境中应用于人形机器人上的真实世界数据。&lt;h4&gt;主要发现&lt;/h4&gt;该系统为人类与机器人的社交触感提供了有价值的见解，有助于推动更自然且有效的人机交互系统的发展。&lt;h4&gt;结论&lt;/h4&gt;通过使用集成在人形机器人手臂上的基于织物的大规模触觉传感器，可以实现对社会手势的有效识别和理解。&lt;h4&gt;翻译&lt;/h4&gt;人类能够仅通过触摸传达不同的信息。为使机器人具有理解社交触摸的能力，增加了一种新的沟通方式。本文介绍了一个使用集成在人形机器人手臂上的基于织物的大规模触觉传感器的社会手势识别系统。我们建立了包含多个参与者的社会手势数据集，并提取了时间特征进行分类。通过在人形机器人的实际环境中收集数据，我们的系统提供了人类与机器人社交触摸有价值的见解，进一步推进了更自然和有效的沟通的人机交互系统的开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans are able to convey different messages using only touch. Equippingrobots with the ability to understand social touch adds another modality inwhich humans and robots can communicate. In this paper, we present a socialgesture recognition system using a fabric-based, large-scale tactile sensorintegrated onto the arms of a humanoid robot. We built a social gesture datasetusing multiple participants and extracted temporal features for classification.By collecting real-world data on a humanoid robot, our system provides valuableinsights into human-robot social touch, further advancing the development ofspHRI systems for more natural and effective communication.</description>
      <author>example@mail.com (Dakarai Crowder, Kojo Vandyck, Xiping Sun, James McCann, Wenzhen Yuan)</author>
      <guid isPermaLink="false">2503.03234v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OpenGV 2.0: Motion prior-assisted calibration and SLAM with vehicle-mounted surround-view systems</title>
      <link>http://arxiv.org/abs/2503.03230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种基于优化的解决方案，用于车载全景相机系统的视觉SLAM。&lt;h4&gt;背景&lt;/h4&gt;车载全景相机系统通常只配备朝向一个方向的一台相机，并且视场重叠有限。&lt;h4&gt;目的&lt;/h4&gt;针对上述问题，文章提出了三个优化模块来解决实际在线校准、可靠的前端初始化和准确的后端优化问题。&lt;h4&gt;方法&lt;/h4&gt;提出的三种优化模块共同利用了与乘客车辆运动特性相关的运动先验知识。这些模块在避免常见于Ackermann运动中的部分不可观察性方面表现出色。&lt;h4&gt;主要发现&lt;/h4&gt;通过深入的消融研究，验证了所提出的方法的有效性和优越性，并且通过应用到具有挑战性的大规模公开数据集上进一步证实了整个框架的实际有效性。&lt;h4&gt;结论&lt;/h4&gt;该模块构建了一个专为Ackermann车辆在城市环境中运行而设计的新全景相机SLAM系统。接受后，整个框架将作为OpenGV库扩展的一部分开源发布。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于优化的解决方案，用于车载全景相机系统的视觉SLAM问题。针对这些问题，作者提出了三个模块：通过简单的双视图几何实现外定向的实际在线校准；相对位移的可靠前端初始化；以及使用连续时间轨迹模型进行准确后端优化。这些模块利用了与乘客车辆运动特性相关的运动先验知识，并且在解决常见于Ackermann运动中的部分不可观察性方面表现出色。通过深入研究，验证了整个框架的有效性和优越性，并成功应用于公开的大型数据集上。论文接受后，该框架将作为OpenGV库扩展的一部分开源发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The present paper proposes optimization-based solutions to visual SLAM with avehicle-mounted surround-view camera system. Owing to their original use-case,such systems often only contain a single camera facing into either directionand very limited overlap between fields of view. Our novelty consist of threeoptimization modules targeting at practical online calibration of exteriororientations from simple two-view geometry, reliable front-end initializationof relative displacements, and accurate back-end optimization using acontinuous-time trajectory model. The commonality between the proposed modulesis given by the fact that all three of them exploit motion priors that arerelated to the inherent non-holonomic characteristics of passenger vehiclemotion. In contrast to prior related art, the proposed modules furthermoreexcel in terms of bypassing partial unobservabilities in the transformationvariables that commonly occur for Ackermann-motion. As a further contribution,the modules are built into a novel surround-view camera SLAM system thatspecifically targets deployment on Ackermann vehicles operating in urbanenvironments. All modules are studied in the context of in-depth ablationstudies, and the practical validity of the entire framework is supported by asuccessful application to challenging, large-scale publicly available onlinedatasets. Note that upon acceptance, the entire framework is scheduled foropen-source release as part of an extension of the OpenGV library.</description>
      <author>example@mail.com (Kun Huang, Yifu Wang, Si'ao Zhang, Zhirui Wang, Zhanpeng Ouyang, Zhenghua Yu, Laurent Kneip)</author>
      <guid isPermaLink="false">2503.03230v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Embodied Escaping: End-to-End Reinforcement Learning for Robot Navigation in Narrow Environment</title>
      <link>http://arxiv.org/abs/2503.03208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种基于强化学习的自适应脱离模型，该模型通过高效的动作掩码来解决机器人清扫器在拥挤和狭窄环境中遇到死区问题时的传统规划方法失效的问题。&lt;h4&gt;背景&lt;/h4&gt;自主导航是室内环境下机器人吸尘器的基本任务。由于其核心功能是在整个区域进行清洁，因此机器人不可避免地会遇到复杂环境约束、高维搜索空间以及高度复杂的机动操作导致的死区问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的脱离模型和训练策略来提高机器人从死区内成功逃脱的能力，并减少碰撞次数。&lt;h4&gt;方法&lt;/h4&gt;[{'嵌入式逃逸模型': '利用基于强化学习的政策并结合高效的行动掩码来解决死区问题。'}, {'混合培训策略': '为了解决训练过程中稀疏奖励的问题，引入了一种混合训练策略以提高学习效率。'}, {'新的动作表示方法': '设计了一种新的动作表达方式，通过统一的转弯半径重新塑造离散的动作空间，有效地处理冗余和无效的操作选择。'}, {'行动掩码策略': '开发了一种行动掩码策略来快速选择有效操作，在精度与效率之间实现平衡。'}]&lt;h4&gt;主要发现&lt;/h4&gt;['在一系列真实世界实验中，配备了激光雷达、惯性测量单元以及双轮编码器的机器人成功地从多个难度等级中的死区逃脱。', '相较于其他路径规划和强化学习方法，所提出的方法在成功率和碰撞避免方面表现优越。']&lt;h4&gt;结论&lt;/h4&gt;本文提出的基于强化学习的动作掩码策略能有效帮助机器人吸尘器解决复杂环境中的死区问题，并且在实际测试中展示了卓越的性能。&lt;h4&gt;翻译&lt;/h4&gt;自主导航对于室内环境中运行的自动扫地机器人的功能至关重要，尤其是在它们需要清扫拥挤和狭窄区域的时候。由于存在复杂的环境限制、高维搜索空间以及困难的动作需求，现有规划方法往往无法有效解决死区问题。为了克服这些挑战，本文提出了一种基于强化学习的新模型，结合高效的行动掩码策略来优化机器人在复杂条件下的逃脱能力。研究还引入了混合训练政策以解决训练过程中稀疏奖励的问题，并设计了一个新的动作表示法以简化操作选择，同时开发出一种有效筛选可行动的算法，平衡精确度与效率。实验表明，在装备有激光测距仪、惯性测量单元以及双轮编码器的真实场景中，该机器人能够有效地从各种难度级别的死区逃脱出来。比较测试显示了在成功率和碰撞避免方面本方法比其他路径规划或强化学习策略更加优越。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous navigation is a fundamental task for robot vacuum cleaners inindoor environments. Since their core function is to clean entire areas, robotsinevitably encounter dead zones in cluttered and narrow scenarios. Existingplanning methods often fail to escape due to complex environmental constraints,high-dimensional search spaces, and high difficulty maneuvers. To address thesechallenges, this paper proposes an embodied escaping model that leveragesreinforcement learning-based policy with an efficient action mask for dead zoneescaping. To alleviate the issue of the sparse reward in training, we introducea hybrid training policy that improves learning efficiency. In handlingredundant and ineffective action options, we design a novel actionrepresentation to reshape the discrete action space with a uniform turningradius. Furthermore, we develop an action mask strategy to select valid actionquickly, balancing precision and efficiency. In real-world experiments, ourrobot is equipped with a Lidar, IMU, and two-wheel encoders. Extensivequantitative and qualitative experiments across varying difficulty levelsdemonstrate that our robot can consistently escape from challenging dead zones.Moreover, our approach significantly outperforms compared path planning andreinforcement learning methods in terms of success rate and collisionavoidance.</description>
      <author>example@mail.com (Han Zheng, Jiale Zhang, Mingyang Jiang, Peiyuan Liu, Danni Liu, Tong Qin, Ming Yang)</author>
      <guid isPermaLink="false">2503.03208v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>RoboBERT: An End-to-end Multimodal Robotic Manipulation Model</title>
      <link>http://arxiv.org/abs/2502.07837v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Embodied intelligence融合了多种模态，使代理能够同时理解图像、语言和行动。然而，现有的模型依赖于额外的数据集或广泛的预训练以最大化性能提升，这需要大量的训练时间和昂贵的硬件成本。&lt;h4&gt;背景&lt;/h4&gt;当前的多模态机器人模型通常需要额外的数据集或大规模的基础模型来达到高性能，并且消耗大量资源。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的端到端机器人操作模型RoboBERT及其独特的训练策略，旨在提高效率并减少对大型数据集和基础模型的需求。&lt;h4&gt;方法&lt;/h4&gt;该模型使用基于CNN的扩散策略，通过分离不同模态的训练过程来增强和稳定模型的有效性。同时强调数据增强的重要性，并验证了多种技术以显著提升性能。&lt;h4&gt;主要发现&lt;/h4&gt;RoboBERT在CALVIN基准测试中的ABCD → D任务中实现了4.52的平均长度，创下了新的SOTA记录；当应用于真实机器人时，该模型展示了比其他使用相同数据训练的方法更高的成功率。&lt;h4&gt;结论&lt;/h4&gt;通过这些概念和方法论，RoboBERT表现出广泛的灵活性和兼容性，并为轻量级多模态机器人模型的发展做出了重要贡献。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种新的端到端的机器人操作模型——RoboBERT及其独特的培训策略。该模型旨在解决现有模型由于需要额外的数据集或大量预训练而导致的时间和资源消耗问题，通过采用基于CNN的扩散政策，并强调数据增强的重要性来提高性能。实验结果表明，RoboBERT在基准测试中取得了新纪录的成功率，并且在实际应用中也优于其他方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied intelligence integrates multiple modalities, enabling agents tounderstand images, language, and actions simultaneously. However, existingmodels always depend on additional datasets or extensive pre-training tomaximize performance improvements, consuming abundant training time andexpensive hardware cost. To tackle this issue, we present RoboBERT, a novelend-to-end robotic manipulation model integrated with a unique trainingstrategy. This model utilizes a CNN-based diffusion policy, enhancing andstabilizing the effectiveness of this model by separating training processesfor different modalities. It also underscores the importance of dataaugmentation, verifying various techniques to significantly boost performance.Unlike models that depend on extra data or large foundation models, RoboBERTachieves a highly competitive success rate while using only language-labeledexpert demonstrations and maintaining a relatively smaller model size.Specifically, RoboBERT achieves an average length of 4.52 on the CALVINbenchmark for \(ABCD \rightarrow D\) task, setting a new state-of-the-art(SOTA) record. Furthermore, when tested on a real robot, the model demonstratessuperior performance, achieving a higher success rate than other methodstrained with the same data. We propose that these concepts and methodologies ofRoboBERT demonstrate extensive versatility and compatibility, contributingsignificantly to the development of lightweight multimodal robotic models. Thecode can be accessed on https://github.com/PeterWangsicheng/RoboBERT</description>
      <author>example@mail.com (Sicheng Wang, Jianhua Shan, Jianwei Zhang, Haozhang Gao, Hailiang Han, Yipeng Chen, Kang Wei, Chengkun Zhang, Kairos Wong, Jie Zhao, Lei Zhao, Bin Fang)</author>
      <guid isPermaLink="false">2502.07837v1</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
  <item>
      <title>Probing a Quarkophobic ${\mathbf{W}}^\prime$ at the High-Luminosity LHC via Vector Boson Fusion and Lorentz-Equivariant Point Cloud Learning</title>
      <link>http://arxiv.org/abs/2502.16630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究通过弱玻色子融合方式探测标准模型之外的W'玻色子生产，使用点云学习技术并引入新的洛伦兹等变几何代数变换器提高信号敏感度。&lt;h4&gt;背景&lt;/h4&gt;在标准模型中添加一个质量较大的、几乎不与夸克耦合的带电矢量规范玻色子W'可以解决诸如B介子异常和W玻色子质量测量差异等问题。&lt;h4&gt;目的&lt;/h4&gt;通过弱相互作用过程研究W'玻色子的产生，利用大型强子对撞机中的质子-质子碰撞数据进行研究，并采用新的学习技术提高检测信号的能力。&lt;h4&gt;方法&lt;/h4&gt;在一种简化模型中工作，该模型假设W'玻色子具有较大的衰变宽度并考虑两个喷注、大的缺失横贯动量和一个轻味道的最终态。使用点云学习技术中的新洛伦兹等变几何代数变换器。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，新型方法相比于传统方法显著提高了信号敏感度。&lt;h4&gt;结论&lt;/h4&gt;引入新的计算工具（即Lorentz-Equivariant Geometric Algebra Transformer）使得探测W'玻色子更加有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The addition of a heavy charged vector gauge boson ${\mathbf{W}}^\prime$ tothe Standard Model (SM) with negligible quark couplings ("quarkophobic") andtriple gauge couplings can address issues with the SM, such as the B-mesonanomalies and recent discrepancies in the W boson mass measurements. We presenta phenomenology study probing ${\mathbf{W}}^\prime$ production through weakboson fusion in proton-proton collisions at the Large Hadron Collider. Weoperate under a simplified model with a large ${\mathbf{W}}^\prime$ decay widthand consider final states with two jets, large missing transverse momentum, andone light lepton. Notably, we use point cloud learning for the first time in aBSM search$\unicode{x2014}$specifically, a novel Lorentz-Equivariant GeometricAlgebra Transformer$\unicode{x2014}$providing significant improvement in signalsensitivity compared to traditional methods.</description>
      <author>example@mail.com (U. S. Qureshi, A. Gurrola, J. D. Ruiz-Álvarez)</author>
      <guid isPermaLink="false">2502.16630v1</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Audio Visual Segmentation Through Text Embeddings</title>
      <link>http://arxiv.org/abs/2502.16359v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为AV2T-SAM的新框架，该框架将音频特征与预训练的文本提示式SAM的文本嵌入空间连接起来。&lt;h4&gt;背景&lt;/h4&gt;音频-视觉分割（AVS）的目标是定位和从视频帧中分离出发出声音的对象。由于手工注释昂贵，研究者面临数据集有限的问题。&lt;h4&gt;目的&lt;/h4&gt;通过利用丰富的文本图像配对数据集中学习到的多模态对应关系来增强视听对齐，并提出一种新特征以强调音频和视觉模式之间的共享语义同时过滤无关噪声。&lt;h4&gt;方法&lt;/h4&gt;将预训练模型SAM与声音提示结合，使用跨模式语义对齐的方法来改进AVS任务。&lt;h4&gt;主要发现&lt;/h4&gt;通过在AVSBench数据集上的实验显示了在两个数据集上都达到了最新的性能。该方法有效地利用了预训练的分割模型以及跨模态语义对齐。&lt;h4&gt;结论&lt;/h4&gt;提出的AV2T-SAM框架解决了现有音频-视觉分割技术面临的挑战，特别是在有限的数据集限制下学习视听关系的问题。&lt;h4&gt;翻译&lt;/h4&gt;提出了一种新的AVS框架，该框架使用SAM模型，并通过引入一种新的特征来改进其在处理声音源对象分割任务上的能力。此方法利用了跨模式语义对齐，并且实验证明了它能够有效地提高模型的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of Audio-Visual Segmentation (AVS) is to localize and segment thesounding source objects from the video frames. Researchers working on AVSsuffer from limited datasets because hand-crafted annotation is expensive.Recent works attempt to overcome the challenge of limited data by leveragingthe segmentation foundation model, SAM, prompting it with audio to enhance itsability to segment sounding source objects. While this approach alleviates themodel's burden on understanding visual modality by utilizing pre-trainedknowledge of SAM, it does not address the fundamental challenge of the limiteddataset for learning audio-visual relationships. To address these limitations,we propose \textbf{AV2T-SAM}, a novel framework that bridges audio featureswith the text embedding space of pre-trained text-prompted SAM. Our methodleverages multimodal correspondence learned from rich text-image paireddatasets to enhance audio-visual alignment. Furthermore, we introduce a novelfeature, $\mathbf{\textit{\textbf{f}}_{CLIP} \odot\textit{\textbf{f}}_{CLAP}}$, which emphasizes shared semantics of audio andvisual modalities while filtering irrelevant noise. Experiments on the AVSBenchdataset demonstrate state-of-the-art performance on both datasets of AVSBench.Our approach outperforms existing methods by effectively utilizing pretrainedsegmentation models and cross-modal semantic alignment.</description>
      <author>example@mail.com (Kyungbok Lee, You Zhang, Zhiyao Duan)</author>
      <guid isPermaLink="false">2502.16359v1</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Robust Deterministic Policy Gradient for Disturbance Attenuation and Its Application to Quadrotor Control</title>
      <link>http://arxiv.org/abs/2502.21057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为Robust Deterministic Policy Gradient (RDPG) 的强化学习算法，该算法将H无穷控制问题转化为一个零和动态博弈，并通过深度确定性策略梯度(DPG)及其深度强化学习版本进行训练。为实际应用引入了RDDPG算法，利用深层神经网络架构并结合TD3技术来提高稳定性和学习效率。&lt;h4&gt;背景&lt;/h4&gt;在实际控制系统中，由于系统模型中的不确定性以及外部扰动的存在，识别最优控制策略面临重大挑战。传统的H无穷控制方法虽然广泛应用于设计鲁棒控制器以减轻干扰影响，但往往需要复杂的计算资源和高强度的计算能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于强化学习的方法来解决实际控制系统中由于不确定性和外部干扰导致的设计复杂性问题，并提高控制器的稳定性与实时性能。&lt;h4&gt;方法&lt;/h4&gt;将H无穷控制问题建模为一个两人零和动态博弈，其中一方试图最小化成本而另一方则最大化。采用确定性策略梯度(DPG)及其深度强化学习版本来训练鲁棒控制策略，进而提出了一种名为RDDPG的算法，并在无人机路径跟踪任务中验证了该方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;提出的算法能够在扰动环境下实现精确实时的目标追踪，在对比测试中显示出了比其他控制方法更高的抗干扰性能。&lt;h4&gt;结论&lt;/h4&gt;基于强化学习的RDPG和RDDPG为实际控制系统中的鲁棒控制器设计提供了一种有效且计算效率高的解决方案，尤其适用于需要应对动态变化环境的应用场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Practical control systems pose significant challenges in identifying optimalcontrol policies due to uncertainties in the system model and externaldisturbances. While $H_\infty$ control techniques are commonly used to designrobust controllers that mitigate the effects of disturbances, these methodsoften require complex and computationally intensive calculations. To addressthis issue, this paper proposes a reinforcement learning algorithm calledRobust Deterministic Policy Gradient (RDPG), which formulates the $H_\infty$control problem as a two-player zero-sum dynamic game. In this formulation, oneplayer (the user) aims to minimize the cost, while the other player (theadversary) seeks to maximize it. We then employ deterministic policy gradient(DPG) and its deep reinforcement learning counterpart to train a robust controlpolicy with effective disturbance attenuation. In particular, for practicalimplementation, we introduce an algorithm called robust deep deterministicpolicy gradient (RDDPG), which employs a deep neural network architecture andintegrates techniques from the twin-delayed deep deterministic policy gradient(TD3) to enhance stability and learning efficiency. To evaluate the proposedalgorithm, we implement it on an unmanned aerial vehicle (UAV) tasked withfollowing a predefined path in a disturbance-prone environment. Theexperimental results demonstrate that the proposed method outperforms othercontrol approaches in terms of robustness against disturbances, enablingprecise real-time tracking of moving targets even under severe disturbanceconditions.</description>
      <author>example@mail.com (Taeho Lee, Donghwan Lee)</author>
      <guid isPermaLink="false">2502.21057v1</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating the Robustness of LiDAR Point Cloud Tracking Against Adversarial Attack</title>
      <link>http://arxiv.org/abs/2410.20893v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文研究了基于神经网络的LiDAR点云跟踪模型在对抗攻击下的鲁棒性，重点关注其在白盒和黑盒攻击策略中的脆弱性。&lt;h4&gt;背景&lt;/h4&gt;现有的LiDAR点云跟踪模型往往忽视了抗干扰能力的重要性，而仅仅关注性能提升。然而，在面对对抗攻击、领域转换或数据损坏等问题时，这些模型表现出严重的脆弱性。&lt;h4&gt;目的&lt;/h4&gt;研究如何提高基于神经网络的LiDAR点云跟踪模型在对抗攻击下的鲁棒性，并提出一种新的黑盒攻击策略方法：目标感知扰动生成(TAPG)算法。&lt;h4&gt;方法&lt;/h4&gt;{'白盒攻击': '为各种跟踪范式定制特定损失函数，扩展了现有FGSM、C&amp;W和PGD等方法到点云领域。', '黑盒攻击': '引入TAPG算法，通过稀疏性约束和随机子向量因子化技术提高转移能力。该算法旨在实现高攻击性能的同时保持低感知度。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，先进的跟踪方法在对抗白盒和黑盒攻击时存在显著脆弱性。&lt;h4&gt;结论&lt;/h4&gt;研究强调了增强LiDAR点云跟踪模型鲁棒性的必要性，并提出了一种新的平衡有效性和隐蔽性的TAPG算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we delve into the robustness of neural network-based LiDARpoint cloud tracking models under adversarial attacks, a critical aspect oftenoverlooked in favor of performance enhancement. These models, despiteincorporating advanced architectures like Transformer or Bird's Eye View (BEV),tend to neglect robustness in the face of challenges such as adversarialattacks, domain shifts, or data corruption. We instead focus on the robustnessof the tracking models under the threat of adversarial attacks. We begin byestablishing a unified framework for conducting adversarial attacks within thecontext of 3D object tracking, which allows us to thoroughly investigate bothwhite-box and black-box attack strategies. For white-box attacks, we tailorspecific loss functions to accommodate various tracking paradigms and extendexisting methods such as FGSM, C\&amp;W, and PGD to the point cloud domain. Inaddressing black-box attack scenarios, we introduce a novel transfer-basedapproach, the Target-aware Perturbation Generation (TAPG) algorithm, with thedual objectives of achieving high attack performance and maintaining lowperceptibility. This method employs a heuristic strategy to enforce sparseattack constraints and utilizes random sub-vector factorization to bolstertransferability. Our experimental findings reveal a significant vulnerabilityin advanced tracking methods when subjected to both black-box and white-boxattacks, underscoring the necessity for incorporating robustness againstadversarial attacks into the design of LiDAR point cloud tracking models.Notably, compared to existing methods, the TAPG also strikes an optimal balancebetween the effectiveness of the attack and the concealment of theperturbations.</description>
      <author>example@mail.com (Shengjing Tian, Yinan Han, Xiantong Zhao, Bin Liu, Xiuping Liu)</author>
      <guid isPermaLink="false">2410.20893v2</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-driven Transferable Adversarial Attack on Person Re-Identification with Attribute-aware Textual Inversion</title>
      <link>http://arxiv.org/abs/2502.19697v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了一种新的攻击方法Attribute-aware Prompt Attack (AP-Attack)，该方法利用视觉语言模型(VLM)的图像文本对齐能力，通过对行人属性特定的文字嵌入进行破坏来显式地扰乱行人图像中的细粒度语义特征。&lt;h4&gt;背景&lt;/h4&gt;人员重识别(re-id)模型在安全监控系统中至关重要。然而，现有的基于VLM（视觉-语言模型）的攻击方法由于过于强调整体表示中的判别性语义，缺乏对综合特征破坏的能力。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的属性感知提示攻击(AP-Attack)方法，该方法旨在通过扰动行人图像中特定属性的文字嵌入来增强细粒度语义特征的扰乱效果，并提高对抗样本在不同模型和数据集上的迁移能力。&lt;h4&gt;方法&lt;/h4&gt;设计了文本反转网络以获取个人化的文字描述，这些网络将行人图像映射到表示语义嵌入的伪标记上。训练过程中采用了对比学习方式结合图像与预先定义的文字模板，该模板明确描述了行人的属性特征。&lt;h4&gt;主要发现&lt;/h4&gt;AP-Attack 方法在跨模型和数据集攻击场景中表现出色，其平均Drop Rate比现有方法高出22.9%，展示了极佳的迁移性。&lt;h4&gt;结论&lt;/h4&gt;通过扰乱行人图像中的细粒度语义特征，AP-Attack有效增强了对抗样本的破坏力，并且提高了它们的迁移性能。&lt;h4&gt;翻译&lt;/h4&gt;人员重识别(re-id)模型在安全监控系统中非常重要。最近基于视觉语言模型（VLM）的攻击方法显示出卓越的迁移性，通过攻击VLM中的通用图像和文本特征来探索这些模型的脆弱点。然而，由于过度强调整体表示中的判别性语义，它们缺乏对综合特征的彻底破坏。在这篇论文中，我们引入了属性感知提示攻击（AP-Attack），这是一种新颖的方法，它利用VLM的图像文字对齐能力显式地扰乱行人图像中的细粒度语义特征，通过摧毁特定于属性的文字嵌入来实现这一点。为了获得针对每个个体属性的个性化文本描述，设计了文本反转网络将行人图像映射到表示语义嵌入的伪标记上，并在对比学习方式下进行训练，结合图像和预先定义的好像模板，该模板明确地描述了行人的属性特征。扰动后的良性及对抗性细粒度文本语义使攻击者能够有效地进行全面破坏，从而增强了对抗样本的迁移能力。广泛的实验表明，AP-Attack实现了最先进的迁移性能，在跨模型和数据集的攻击场景中平均Drop Rate比现有方法高出22.9%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Person re-identification (re-id) models are vital in security surveillancesystems, requiring transferable adversarial attacks to explore thevulnerabilities of them. Recently, vision-language models (VLM) based attackshave shown superior transferability by attacking generalized image and textualfeatures of VLM, but they lack comprehensive feature disruption due to theoveremphasis on discriminative semantics in integral representation. In thispaper, we introduce the Attribute-aware Prompt Attack (AP-Attack), a novelmethod that leverages VLM's image-text alignment capability to explicitlydisrupt fine-grained semantic features of pedestrian images by destroyingattribute-specific textual embeddings. To obtain personalized textualdescriptions for individual attributes, textual inversion networks are designedto map pedestrian images to pseudo tokens that represent semantic embeddings,trained in the contrastive learning manner with images and a predefined prompttemplate that explicitly describes the pedestrian attributes. Inverted benignand adversarial fine-grained textual semantics facilitate attacker ineffectively conducting thorough disruptions, enhancing the transferability ofadversarial examples. Extensive experiments show that AP-Attack achievesstate-of-the-art transferability, significantly outperforming previous methodsby 22.9% on mean Drop Rate in cross-model&amp;dataset attack scenarios.</description>
      <author>example@mail.com (Yuan Bian, Min Liu, Yunqi Yi, Xueping Wang, Yaonan Wang)</author>
      <guid isPermaLink="false">2502.19697v2</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation</title>
      <link>http://arxiv.org/abs/2502.18041v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概述&lt;/h4&gt;介绍了用于户外高空Vision-Language Navigation (VLN)的OpenFly平台，该平台包括工具链和大规模基准数据集。&lt;h4&gt;背景&lt;/h4&gt;室内VLN已经得到了广泛研究，但室外高空VLN由于涉及广阔区域的数据收集难度大而研究不足。&lt;h4&gt;目的&lt;/h4&gt;提出一个完整的工具链和大规模的户外高空VLN数据集来解决现有数据缺乏的问题。&lt;h4&gt;方法&lt;/h4&gt;{'自动化工具链': '开发了高度自动化的工具链用于数据收集，包括点云获取、场景语义分割、飞行轨迹创建及指令生成。', '大规模数据集构建': '利用工具链建立了包含100k条轨迹的大规模户外高空VLN数据集，涵盖了多样化的高度和长度以及18个不同场景。', '视觉数据生成': '采用多种渲染引擎和技术（如Unreal Engine、GTA V、Google Earth及3D Gaussian Splatting）生成高质量的视觉数据。', '模型开发': '提出了关键帧感知的VLN模型OpenFly-Agent，输入语言指令、当前观察值和历史关键帧，并直接输出飞行动作。'}&lt;h4&gt;主要发现&lt;/h4&gt;平台及其模型在多项实验中展示了其优越性。&lt;h4&gt;结论&lt;/h4&gt;工具链、数据集及代码将开源以促进相关研究的发展。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language Navigation (VLN)旨在通过利用语言指令和视觉线索来引导环境中的代理，这在具身AI领域扮演着重要角色。虽然室内VLN已经得到了广泛的研究，但室外高空VLN由于涉及广阔区域的数据收集难度大而鲜少有人研究。为解决这一问题，我们提出了一种开放式飞行平台OpenFly，包括一个灵活的工具链和大规模基准数据集。首先，开发了一个高度自动化的工具链用于数据采集，实现了点云获取、场景语义分割、飞行轨迹创建及指令生成等自动化过程。其次，在此基础上建立了一个包含10万条不同高度与长度路线的大规模户外高空VLN数据集，并利用多种渲染引擎（如Unreal Engine, GTA V, Google Earth）和技术（如3D Gaussian Splatting）生成高视觉质量的数据，其中3D GS支持真实到仿真渲染。最后，我们提出了关键帧感知的VLN模型OpenFly-Agent，该模型根据语言指令、当前观察值和历史关键帧输出飞行动作。通过全面分析及实验表明了我们的平台及其模型的优势，并计划开放工具链、数据集以及相关代码以促进进一步的研究进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Navigation (VLN) aims to guide agents through an environmentby leveraging both language instructions and visual cues, playing a pivotalrole in embodied AI. Indoor VLN has been extensively studied, whereas outdooraerial VLN remains underexplored. The potential reason is that outdoor aerialview encompasses vast areas, making data collection more challenging, whichresults in a lack of benchmarks. To address this problem, we propose OpenFly, aplatform comprising a versatile toolchain and large-scale benchmark for aerialVLN. Firstly, we develop a highly automated toolchain for data collection,enabling automatic point cloud acquisition, scene semantic segmentation, flighttrajectory creation, and instruction generation. Secondly, based on thetoolchain, we construct a large-scale aerial VLN dataset with 100ktrajectories, covering diverse heights and lengths across 18 scenes. Thecorresponding visual data are generated using various rendering engines andadvanced techniques, including Unreal Engine, GTA V, Google Earth, and 3DGaussian Splatting (3D GS). All data exhibit high visual quality. Particularly,3D GS supports real-to-sim rendering, further enhancing the realism of thedataset. Thirdly, we propose OpenFly-Agent, a keyframe-aware VLN model, whichtakes language instructions, current observations, and historical keyframes asinput, and outputs flight actions directly. Extensive analyses and experimentsare conducted, showcasing the superiority of our OpenFly platform andOpenFly-Agent. The toolchain, dataset, and codes will be open-sourced.</description>
      <author>example@mail.com (Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li)</author>
      <guid isPermaLink="false">2502.18041v3</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement</title>
      <link>http://arxiv.org/abs/2502.17648v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Transportation Research Part C: Emerging Technologies&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为CalibRefine的全自动、无目标且在线校准框架，该框架可以处理原始LiDAR点云和相机图像，通过一系列阶段实现精确的多传感器校准。&lt;h4&gt;背景&lt;/h4&gt;在诸如自动驾驶汽车、机器人技术及智能交通系统等应用中，准确的多传感器校准至关重要。现有的激光雷达-摄像机校准方法通常依赖于手动放置的目标物、初步参数估计或密集的数据预处理，这限制了它们在实际环境中的可扩展性和适应性。&lt;h4&gt;目的&lt;/h4&gt;旨在提出一种全自动化的校准框架，该框架不依赖人工目标且可以在线完成，并能够直接处理原始激光雷达点云和相机图像数据。&lt;h4&gt;方法&lt;/h4&gt;CalibRefine由四个阶段组成：（1）一个共同特征鉴别器，利用自动检测到的对象的相对位置、外观嵌入以及语义类别生成可靠的激光雷达-摄像机对应关系；（2）基于粗略同构变换的校准；（3）迭代细化，在更多数据帧可用时逐步提高对齐精度；（4）注意力机制改进，通过使用视觉变压器和交叉注意机制解决非平面失真问题。&lt;h4&gt;主要发现&lt;/h4&gt;CalibRefine在两个城市交通数据集上进行了广泛的实验，结果显示它能够以最少的人工干预实现高精度校准结果，优于无目标的现有方法，并与手动调优基线保持竞争性或超越。&lt;h4&gt;结论&lt;/h4&gt;研究强调了如何通过稳健的对象级特征匹配以及迭代和自监督的注意力机制调整，在复杂的真实世界条件下实现一致的传感器融合，而无需地面真相校准矩阵或复杂的预处理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/radar-lab/Lidar_Camera_Automatic_Calibration&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate multi-sensor calibration is essential for deploying robustperception systems in applications such as autonomous driving, robotics, andintelligent transportation. Existing LiDAR-camera calibration methods oftenrely on manually placed targets, preliminary parameter estimates, or intensivedata preprocessing, limiting their scalability and adaptability in real-worldsettings. In this work, we propose a fully automatic, targetless, and onlinecalibration framework, CalibRefine, which directly processes raw LiDAR pointclouds and camera images. Our approach is divided into four stages: (1) aCommon Feature Discriminator that trains on automatically detectedobjects--using relative positions, appearance embeddings, and semanticclasses--to generate reliable LiDAR-camera correspondences, (2) a coarsehomography-based calibration, (3) an iterative refinement to incrementallyimprove alignment as additional data frames become available, and (4) anattention-based refinement that addresses non-planar distortions by leveraginga Vision Transformer and cross-attention mechanisms. Through extensiveexperiments on two urban traffic datasets, we show that CalibRefine delivershigh-precision calibration results with minimal human involvement,outperforming state-of-the-art targetless methods and remaining competitivewith, or surpassing, manually tuned baselines. Our findings highlight howrobust object-level feature matching, together with iterative andself-supervised attention-based adjustments, enables consistent sensor fusionin complex, real-world conditions without requiring ground-truth calibrationmatrices or elaborate data preprocessing.</description>
      <author>example@mail.com (Lei Cheng, Lihao Guo, Tianya Zhang, Tam Bang, Austin Harris, Mustafa Hajij, Mina Sartipi, Siyang Cao)</author>
      <guid isPermaLink="false">2502.17648v3</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model</title>
      <link>http://arxiv.org/abs/2502.16779v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025. Github  page:https://github.com/justacar/Plane-DUSt3R&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Plane-DUSt3R是一种利用3D基础模型DUSt3R进行多视角房间布局估计的新方法。&lt;h4&gt;背景&lt;/h4&gt;由于多视图几何的复杂性，从多视角图像中推断房间布局的研究较少。传统的结构从运动过程中涉及多个步骤（如相机内部和外部参数估计、图像匹配和三角测量）。然而，在3D重建领域，最近出现的3D基础模型改变了传统方法。&lt;h4&gt;目的&lt;/h4&gt;介绍并改进一种基于DUSt3R框架的方法——Plane-DUSt3R，以解决多视角房间布局估计的问题。&lt;h4&gt;方法&lt;/h4&gt;Plane-DUSt3R通过在房间布局数据集（Structure3D）上进行微调，并修改目标函数来估计结构平面。它采用单步后处理步骤和2D检测结果生成一致且简洁的结果。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，Plane-DUSt3R不仅在合成数据集中优于现有最佳方法，还在具有不同图像风格（如卡通）的现实世界数据中表现出鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;Plane-DUSt3R提供了一种简化流程、减少误差累积的端到端解决方案，并能处理多视角图像。此研究拓展了房间布局估计领域的可能性。&lt;h4&gt;翻译&lt;/h4&gt;从多个视角的图片中推断出房间的布局由于涉及到复杂的多视图几何问题，因此研究较少。传统的结构从运动过程需要一系列步骤（例如相机内参和外参估计、图像匹配以及三角测量）。然而，在3D重建领域，最近出现的像DUSt3R这样的3D基础模型改变了这一传统流程，使其向端到端的单步方法转变。为了解决多视角房间布局估计的问题，我们引入了Plane-DUSt3R，这是一种利用3D基础模型DUSt3R的方法。该方法基于DUSt3R框架，并在房间布局数据集（Structure3D）上进行了微调以估计结构平面。它通过单一的后处理步骤和2D检测结果生成一致且简洁的结果。与依赖单视角或全景图象的方法不同，Plane-DUSt3R能够处理多视角图像并提供了一种简化的、端到端的解决方案来简化过程，并减少误差积累。实验结果显示，相较于现有最佳方法，在合成数据集上，该方法表现更优；在不同的真实世界数据集中（例如卡通风格），该方法表现出稳健性和有效性。我们的代码可以在https://github.com/justacar/Plane-DUSt3R中获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Room layout estimation from multiple-perspective images is poorlyinvestigated due to the complexities that emerge from multi-view geometry,which requires muti-step solutions such as camera intrinsic and extrinsicestimation, image matching, and triangulation. However, in 3D reconstruction,the advancement of recent 3D foundation models such as DUSt3R has shifted theparadigm from the traditional multi-step structure-from-motion process to anend-to-end single-step approach. To this end, we introduce Plane-DUSt3R, anovel method for multi-view room layout estimation leveraging the 3D foundationmodel DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes ona room layout dataset (Structure3D) with a modified objective to estimatestructural planes. By generating uniform and parsimonious results, Plane-DUSt3Renables room layout estimation with only a single post-processing step and 2Ddetection results. Unlike previous methods that rely on single-perspective orpanorama image, Plane-DUSt3R extends the setting to handle multiple-perspectiveimages. Moreover, it offers a streamlined, end-to-end solution that simplifiesthe process and reduces error accumulation. Experimental results demonstratethat Plane-DUSt3R not only outperforms state-of-the-art methods on thesynthetic dataset but also proves robust and effective on in the wild data withdifferent image styles such as cartoon. Our code is available at:https://github.com/justacar/Plane-DUSt3R</description>
      <author>example@mail.com (Yaxuan Huang, Xili Dai, Jianan Wang, Xianbiao Qi, Yixing Yuan, Xiangyu Yue)</author>
      <guid isPermaLink="false">2502.16779v3</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Robust Prediction of Frictional Contact Network in Near-Jamming Suspensions Employing Deep Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.18743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络（GNN）的机器学习方法，用于预测颗粒悬浮液中的摩擦接触网络（FCN），特别是在接近拥堵条件下的性能。这种方法在数据驱动模拟训练中表现出色，并且能够准确预测不同参数组合下的FCN。&lt;h4&gt;背景&lt;/h4&gt;细颗粒分散于牛顿流体中的悬浮物粘度在其接近拥挤状态时发散，这主要是由粒子间接触微观结构决定的。这种联系网络是导致固体化的行为的关键。应力传输和网络拓扑对粒子相对运动的限制非常敏感。&lt;h4&gt;目的&lt;/h4&gt;开发一种预测FCN的有效机器学习方法，尤其是在靠近拥堵条件下的情况。&lt;h4&gt;方法&lt;/h4&gt;使用了一种称为Deep Graph Convolutional Network（DeepGCN）的方法，并且展示了在不同参数组合下具有良好的泛化和外推能力。该研究包括从半稀释状态到拥挤状态的广泛相空间，同时系统地改变剪切应力、堆积分数以及滑动和滚动摩擦。&lt;h4&gt;主要发现&lt;/h4&gt;通过训练数据驱动模拟，DeepGCN能够准确预测不同流参数和相空间条件下的FCN。这些结果展示了在材料科学及相关领域创新且可转移的技术途径的潜力。&lt;h4&gt;结论&lt;/h4&gt;这项研究为预测颗粒系统性质提供了新的技术方法，特别是在拥挤条件下，这可能推动材料科学及其相关领域的进展。&lt;h4&gt;翻译&lt;/h4&gt;悬浮液中由细小颗粒分散于牛顿流体中的粘度在接近堆积极限时发散。这种宏观行为受到粒子接触微观结构的支配，通过摩擦接触网络（FCN）来实现。FCN是由机械负载支撑点组成的，在接近拥挤转变时导致刚性出现。应力传递和网络拓扑反过来取决于颗粒相对运动限制的敏感特性。尽管其重要性显而易见，但由于实验和计算障碍的存在，预测FCN特别是靠近拥挤条件下的情况仍然具有挑战性。这项研究提出了一个基于图神经网络（GNN）的成本效益机器学习方法来预测FCN，并且通过使用DeepGCN展示了在不同流参数和相空间条件下准确预测FCN的能力。该研究覆盖了广泛的相空间，从半稀释到拥挤状态以及瞬态到稳定状态，并系统地改变剪切应力、堆积分数及滑动和滚动摩擦等参数。这项研究的结果为颗粒系统的性质预测提供了创新且可转移的技术途径，为进一步发展材料科学及相关领域开辟新的道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The viscosity of the suspension consisting of fine particles dispersed in aNewtonian liquid diverges close to the jamming packing fraction. The contactmicrostructure in suspensions governs this macroscopic behavior in the vicinityof jamming through a frictional contact network (FCN). FCN is composed ofmechanical load-bearing contacts that lead to the emergence of rigidity nearthe jamming transition. The stress transmission and network topology, in turn,depend sensitively on constraints on the relative motion of the particles.Despite their significance, predicting the FCN, especially close to jammingconditions, remains challenging due to experimental and computationalimpediments. This study introduces a cost-effective machine learning approachto predict the FCN using a graph neural network (GNN), which inherentlycaptures hidden features and underlying patterns in dense suspension by mappinginterparticle interactions. Employing a variation of GNN called the Deep GraphConvolutional Network (DeepGCN) trained on data-driven simulations, this studydemonstrates robust generalization and extrapolation capabilities, accuratelypredicting FCNs in systems with divergent flow parameters and phase spaces,despite each being trained exclusively on a single condition. The study coversa wide range of phase space, from semi-dilute to jammed states, spanningtransient to steady states, while systematically varying parameters such asshear stress (${\sigma}_{xy}$), packing fraction(${\phi}$) and sliding androlling friction (${{\mu}_s, {\mu}_r}$). The results of this research pave theway for innovative transferable techniques in predicting the properties ofparticulate systems, offering new avenues for advancement in material scienceand related fields.</description>
      <author>example@mail.com (Armin Aminimajd, Joao Maia, Abhinendra Singh)</author>
      <guid isPermaLink="false">2502.18743v1</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
  <item>
      <title>Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars</title>
      <link>http://arxiv.org/abs/2502.20220v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website: https://tobias-kirschstein.github.io/avat3r/, Video:  https://youtu.be/P3zNVx15gYs&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Avat3r的方法，可以从少量输入图像中生成高质量且可动画化的3D头像。&lt;h4&gt;背景&lt;/h4&gt;传统上，创建逼真的3D头像是一个复杂的过程，需要多视角捕捉设备和昂贵的优化过程。这限制了数字人类替身的应用范围，使其仅限于VFX行业或离线渲染。&lt;h4&gt;目的&lt;/h4&gt;开发一种减少计算需求的方法，使得高质量、可动画化的3D头像可以从少量输入图像中生成。&lt;h4&gt;方法&lt;/h4&gt;{'利用大型重建模型': '使大规模重建模型变得可动画化，并从大量的多视角视频数据集中学习三维人体头部的强大先验知识。', '改进的3D头部重构': '采用来自DUSt3R的位置图和Sapiens的人类基础模型中的泛化特征图来改善3D头部重构。', '实现动画功能': '发现简单的跨注意力到表情代码就足够用于使3D头像可动画化。', '增强鲁棒性': '通过训练时输入不同表情的图像，增强了模型从不一致输入中重建三维头部的能力。'}&lt;h4&gt;主要发现&lt;/h4&gt;Avat3r在少数输入和单个输入场景中的表现优于现有最先进的方法，并展示了广泛的适用性，能够创建来自各种来源（包括智能手机拍摄、单一图片甚至超出领域范围如古董头像）的3D头像。&lt;h4&gt;结论&lt;/h4&gt;通过项目网站https://tobias-kirschstein.github.io/avat3r可查看更多详细信息。该方法在效率和性能上表现出显著优势，为数字人类替身的应用开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的传统创建逼真的3D头像过程需要复杂的多视角捕捉设备以及昂贵的计算资源，在实际应用中受到限制；本文提出了一种名为Avat3r的技术，可以从少量输入图像生成高质量且可动画化的3D头像，并通过训练模型学习泛化特征和不同表情下的鲁棒性重构方法，实现了在效率与性能上的突破。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditionally, creating photo-realistic 3D head avatars requires astudio-level multi-view capture setup and expensive optimization duringtest-time, limiting the use of digital human doubles to the VFX industry oroffline renderings.  To address this shortcoming, we present Avat3r, which regresses ahigh-quality and animatable 3D head avatar from just a few input images, vastlyreducing compute requirements during inference. More specifically, we makeLarge Reconstruction Models animatable and learn a powerful prior over 3D humanheads from a large multi-view video dataset. For better 3D headreconstructions, we employ position maps from DUSt3R and generalized featuremaps from the human foundation model Sapiens. To animate the 3D head, our keydiscovery is that simple cross-attention to an expression code is alreadysufficient. Finally, we increase robustness by feeding input images withdifferent expressions to our model during training, enabling the reconstructionof 3D head avatars from inconsistent inputs, e.g., an imperfect phone capturewith accidental movement, or frames from a monocular video.  We compare Avat3r with current state-of-the-art methods for few-input andsingle-input scenarios, and find that our method has a competitive advantage inboth tasks. Finally, we demonstrate the wide applicability of our proposedmodel, creating 3D head avatars from images of different sources, smartphonecaptures, single images, and even out-of-domain inputs like antique busts.  Project website: https://tobias-kirschstein.github.io/avat3r/</description>
      <author>example@mail.com (Tobias Kirschstein, Javier Romero, Artem Sevastopolsky, Matthias Nießner, Shunsuke Saito)</author>
      <guid isPermaLink="false">2502.20220v1</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</title>
      <link>http://arxiv.org/abs/2502.19645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website: https://openvla-oft.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了用于视觉-语言-动作模型（VLAs）的优化微调策略，提出了一个集成平行解码、连续动作表示等技术的高效微调方案。&lt;h4&gt;背景&lt;/h4&gt;现有的VLAs依赖于预训练的语言和视觉模型，并利用各种机器人数据集展示了强大的任务执行能力。然而，这些模型在面对新环境时需要通过微调来适应，而最佳的微调策略尚未明确。&lt;h4&gt;目的&lt;/h4&gt;探讨不同的动作解码方案、表示方法及学习目标对VLAs性能的影响，提出一种优化微调的方法以改善推理效率和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用OpenVLA作为基准模型，并通过实证分析确定了最佳的动作解码策略、表示方式以及学习目标。提出的OFT框架包括并行解码、动作切块（action chunking）、连续动作表示和基于L1回归的学习目标。&lt;h4&gt;主要发现&lt;/h4&gt;优化后的微调方法显著提高了OpenVLA在LIBERO仿真基准测试中的成功率，同时增加了行动生成的吞吐量；在真实世界评估中，该策略使得OpenVLA能够优于其他VLAs以及其他从头开始训练的模仿学习策略，在平均成功率上提高15%。&lt;h4&gt;结论&lt;/h4&gt;通过引入OFT方法，可以显著改善视觉-语言-动作模型在新环境下的性能和效率，并且提供了一种有效的微调方案。该研究进一步证明了优化微调对于提升这些复杂模型的实际应用效果的重要性。&lt;h4&gt;翻译&lt;/h4&gt;近期的视觉-语言-动作（VLA）模型基于预训练的语言与视觉模型构建，利用多样化的机器人数据集展示出色的任务执行能力、跟随语言指令的能力以及语义泛化。尽管取得了一些成功，但VLAs在面对新机器人设置时仍表现不佳，需要通过微调来获得良好性能。针对这一问题，该研究探讨了关键的VLA适应设计选择，如不同的动作解码方案、表示方法和学习目标，并提出了一个优化微调（OFT）配方。实证分析表明，这种方法提高了推理效率、政策性能以及模型输入输出规范的灵活性。所提出的OpenVLA-OFT在LIBERO仿真基准上达到了新的最佳水平，在四组任务上的平均成功率从76.5%提高到97.1%，同时增加了26倍的动作生成吞吐量。实际评估显示，优化微调方案使得OpenVLA能够成功执行ALOHA双臂机器人上的精细、高频控制任务，并在平均成功率上显著优于其他VLAs和强大的从头开始训练的模仿学习策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent vision-language-action models (VLAs) build upon pretrainedvision-language models and leverage diverse robot datasets to demonstratestrong task execution, language following ability, and semantic generalization.Despite these successes, VLAs struggle with novel robot setups and requirefine-tuning to achieve good performance, yet how to most effectively fine-tunethem is unclear given many possible strategies. In this work, we study key VLAadaptation design choices such as different action decoding schemes, actionrepresentations, and learning objectives for fine-tuning, using OpenVLA as ourrepresentative base model. Our empirical analysis informs an OptimizedFine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, acontinuous action representation, and a simple L1 regression-based learningobjective to altogether improve inference efficiency, policy performance, andflexibility in the model's input-output specifications. We propose OpenVLA-OFT,an instantiation of this recipe, which sets a new state of the art on theLIBERO simulation benchmark, significantly boosting OpenVLA's average successrate across four task suites from 76.5% to 97.1% while increasing actiongeneration throughput by 26$\times$. In real-world evaluations, our fine-tuningrecipe enables OpenVLA to successfully execute dexterous, high-frequencycontrol tasks on a bimanual ALOHA robot and outperform other VLAs ($\pi_0$ andRDT-1B) fine-tuned using their default recipes, as well as strong imitationlearning policies trained from scratch (Diffusion Policy and ACT) by up to 15%(absolute) in average success rate. We release code for OFT and pretrainedmodel checkpoints at https://openvla-oft.github.io/.</description>
      <author>example@mail.com (Moo Jin Kim, Chelsea Finn, Percy Liang)</author>
      <guid isPermaLink="false">2502.19645v1</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>$Δ$-model correction of Foundation Model based on the models own understanding</title>
      <link>http://arxiv.org/abs/2502.21179v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;摘要描述了一种基于Δ-learning的方法，用于改进通用原子间势能模型在特定材料子类中的应用。&lt;h4&gt;背景&lt;/h4&gt;当前的通用势能模型可能需要针对具体材料进行微调或残差修正。CHGNet是一个典型的例子，它能够在全局结构优化设置中准确预测某些氧化物的能量特性。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Δ-learning的方法来改善通用势能模型在特定场景中的表现，并探讨不同聚合方式（如全局、元素分离和原子级别）的效果。&lt;h4&gt;方法&lt;/h4&gt;使用Gaussian Process Regression (GPR)模型作为Δ-model，以CHGNet内部的原子嵌入表示为基础进行修正。这种方法可以为需要精确预测的新材料或环境提供有效的校正方案。&lt;h4&gt;主要发现&lt;/h4&gt;对于铜、银和金表面上硫原子覆盖层的情况，原始CHGNet模型存在不足之处，需要通过基于GPR的Δ-model来进行校正以提高精度。&lt;h4&gt;结论&lt;/h4&gt;通用势能模型在缺乏特定类型原子环境的数据时会表现出误差，这表明了开发更有效的修正方案的重要性。研究还发现其他使用相同训练数据集（如MACE-MP0、SevenNet-0和ORB-v2-only-MPtrj）的通用势能模型也显示出类似的行为。&lt;h4&gt;翻译&lt;/h4&gt;基础材料间的相互作用势能模型可能需要针对具体应用进行微调或残差修正。文中提出了一种基于Δ-learning的方法，通过已嵌入的表示实现这种改进。在全局结构优化设置中使用CHGNet时发现其能够准确描述某些氧化物的能量特性。然而对于金属表面上硫原子覆盖层的情况，则需要利用GPR模型来进行校正以提高预测准确性。研究结果表明了开发更有效的修正方案的重要性，因为其他训练于类似数据集上的通用势能模型也存在相似问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models of interatomic potentials, so called universal potentials,may require fine-tuning or residual corrections when applied to specificsubclasses of materials. In the present work, we demonstrate how suchaugmentation can be accomplished via $\Delta$-learning based on therepresentation already embedded in the universal potentials. The $\Delta$-modelintroduced is a Gaussian Process Regression (GPR) model and various types ofaggregation (global, species-separated, and atomic) of the representationvector are discussed. Employing a specific universal potential, CHGNet [Deng etal., Nat. Mach. Intell. 5, 1031 (2023)], in a global structure optimizationsetting, we find that it correctly describes the energetics of the "8" Cuoxide, which is an ultra-thin oxide film on Cu(111). The universal potentialmodel even predicts a more favorable structure compared to that discussed inrecent DFT-based literature. Moving to sulfur adatom overlayers on Cu(111),Ag(111), and Au(111) the CHGNet model, however, requires corrections. Wedemonstrate that these are efficiently provided via the GPR-based$\Delta$-model formulated on the CHGNet's own internal atomic embeddingrepresentation. The need for corrections is tracked to the scarcity ofmetal-sulfur atomic environments in the materials project database that CHGNetis trained on leading to an overreliance on sulfur-sulfur atomic environments.Other universal potentials trained on the same data, MACE-MP0, SevenNet-0, andORB-v2-only-MPtrj show similar behavior, but with varying degrees of error,demonstrating the general need for augmentation schemes for universal potentialmodels.</description>
      <author>example@mail.com (Mads-Peter Verner Christiansen, Bjørk Hammer)</author>
      <guid isPermaLink="false">2502.21179v1</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>You Only Click Once: Single Point Weakly Supervised 3D Instance Segmentation for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2502.19698v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为YoCo的框架，用于生成高质量的3D伪标签以减少户外LiDAR点云三维实例分割任务中的人工标注工作。&lt;h4&gt;背景&lt;/h4&gt;户外LiDAR点云三维实例分割是自动驾驶中的关键任务，但由于需要大量人工劳动进行标注，训练模型变得非常困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用少量粗略点击注释生成高质量伪标签的方法，以减少标注成本并提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;{'YoCo框架': '1. 利用视觉基础模型结合点云的几何约束来增强伪标签生成；2. 设计了一个基于时空的标签更新模块，利用相邻帧的预测结果，并考虑点云固有的密度变化特性（近处密集、远处稀疏）；3. 提出一个IoU引导增强模块，替换掉置信度低和IoU低的伪标签。', '效果': '在Waymo数据集上的实验表明，YoCo框架具有显著的效果，达到了弱监督方法中的最佳性能，并且超越了完全监督的方法Cylinder3D。此外，YoCo还适用于多种网络，在仅使用少量标注数据的情况下实现了与完全监督方法相当的性能。'}&lt;h4&gt;主要发现&lt;/h4&gt;1. 通过结合视觉基础模型和点云几何约束可以有效生成高质量伪标签；2. 基于时空的标签更新模块能够利用相邻帧的信息提高标签质量。&lt;h4&gt;结论&lt;/h4&gt;YoCo框架在减少标注工作量的同时，提高了户外LiDAR点云三维实例分割任务中的模型性能，并且具有广泛适用性和优秀的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outdoor LiDAR point cloud 3D instance segmentation is a crucial task inautonomous driving. However, it requires laborious human efforts to annotatethe point cloud for training a segmentation model. To address this challenge,we propose a YoCo framework, which generates 3D pseudo labels using minimalcoarse click annotations in the bird's eye view plane. It is a significantchallenge to produce high-quality pseudo labels from sparse annotations. OurYoCo framework first leverages vision foundation models combined with geometricconstraints from point clouds to enhance pseudo label generation. Second, atemporal and spatial-based label updating module is designed to generatereliable updated labels. It leverages predictions from adjacent frames andutilizes the inherent density variation of point clouds (dense near, sparsefar). Finally, to further improve label quality, an IoU-guided enhancementmodule is proposed, replacing pseudo labels with high-confidence and high-IoUpredictions. Experiments on the Waymo dataset demonstrate YoCo's effectivenessand generality, achieving state-of-the-art performance among weakly supervisedmethods and surpassing fully supervised Cylinder3D. Additionally, the YoCo issuitable for various networks, achieving performance comparable to fullysupervised methods with minimal fine-tuning using only 0.8% of the fullylabeled data, significantly reducing annotation costs.</description>
      <author>example@mail.com (Guangfeng Jiang, Jun Liu, Yongxuan Lv, Yuzhi Wu, Xianfei Li, Wenlong Liao, Tao He, Pai Peng)</author>
      <guid isPermaLink="false">2502.19698v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>DV-Matcher: Deformation-based Non-Rigid Point Cloud Matching Guided by Pre-trained Visual Features</title>
      <link>http://arxiv.org/abs/2408.08568v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 21 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为DV-Matcher的创新学习框架，用于估计非刚性可变形点云之间的密集对应关系。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通常需要对点云进行网格化或手动标注才能学习到有效的特征信息。相比之下，基于学习的方法可以直接从无结构化的点云中提取特征。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需额外预处理的框架，用于生成高质量的密集对应关系，并探索如何在几何特征学习过程中引入先验知识以及设计新的变形模块以促进外部对齐。&lt;h4&gt;方法&lt;/h4&gt;1. 通过将来自预训练视觉模型的知识注入到几何特征学习中，增强局部性质的几何特征与全局和语义信息；2. 提出了一种基于变形的模块来促进由所学对应关系诱导的外在对齐。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示该方法在匹配非刚性点云时达到了最先进的水平，无论是在接近等距形状集合还是异构形状集合中，甚至是更具现实性的部分和噪声数据上都表现出色。&lt;h4&gt;结论&lt;/h4&gt;DV-Matcher框架通过结合视觉先验知识和创新的变形模块，在密集对应估计领域开辟了一条新的道路，并展示了其在处理复杂点云数据中的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们提出了一种基于学习的框架DV-Matcher，用于非刚性可变形点云之间的密集对应的估算。该框架直接从无结构化点云中学习，无需网格化或手动标记，并且能够提供高质量的密集对应关系，在点云处理中有实际应用价值。我们的主要贡献在于两点：首先，我们提出了一种方案将预训练视觉模型中的先验知识注入到几何特征学习中，有效地补充了局部性质的几何特征与全局和语义信息；其次，我们提出了一个基于变形的新模块来促进由所学对应关系诱导的外在对齐，有效增强了特征学习。实验结果表明，在匹配非刚性点云时，无论是在接近等距形状集合还是异构形状集合中，甚至是更具现实性的部分和噪声数据上，我们的方法都达到了最先进的水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present DV-Matcher, a novel learning-based framework forestimating dense correspondences between non-rigidly deformable point clouds.Learning directly from unstructured point clouds without meshing or manuallabelling, our framework delivers high-quality dense correspondences, which isof significant practical utility in point cloud processing. Our keycontributions are two-fold: First, we propose a scheme to inject priorknowledge from pre-trained vision models into geometric feature learning, whicheffectively complements the local nature of geometric features with global andsemantic information; Second, we propose a novel deformation-based module topromote the extrinsic alignment induced by the learned correspondences, whicheffectively enhances the feature learning. Experimental results show that ourmethod achieves state-of-the-art results in matching non-rigid point clouds inboth near-isometric and heterogeneous shape collection as well as morerealistic partial and noisy data.</description>
      <author>example@mail.com (Zhangquan Chen, Puhua Jiang, Ruqi Huang)</author>
      <guid isPermaLink="false">2408.08568v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models -- A Panacea for Artificial Intelligence in Pathology?</title>
      <link>http://arxiv.org/abs/2502.21264v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  50 pages, 15 figures and an appendix (study protocol) which is  previously published, see https://doi.org/10.1101/2024.07.04.24309948;  updated authors list format&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文评估了基础模型（FMs）与任务特定（TS）模型在前列腺癌诊断和Gleason分级中的临床表现，发现尽管FMs在数据稀缺情况下有优势，但当有足够的标注训练数据时其性能被TS模型超越。此外，专门的任务培训显著降低了误诊率，并且考虑到可持续性问题，建议结合两种方法以实现稳健且资源高效的AI病理学解决方案。&lt;h4&gt;背景&lt;/h4&gt;人工智能（AI）在病理科的作用从辅助诊断发展到揭示全切片图像中的预测形态模式。基础模型通过自监督预训练被广泛倡导为多种下游任务的通用解决方案，但它们的临床适用性和相对于特定任务学习模型的优势仍存在疑问。&lt;h4&gt;目的&lt;/h4&gt;评估AI在前列腺癌诊断和Gleason分级中具有临床级性能的方法，并比较两个FMs与完全端到端TS模型的表现。&lt;h4&gt;方法&lt;/h4&gt;使用来自15个地点、11个国家的7342名患者超过10万个核心针活检样本进行了大规模验证。将两种基础模型在一个多实例学习框架中与一个完全端到端任务特定模型进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;尽管FMs在数据稀缺情况下有用，但当有足够的标记训练数据时其性能被TS模型超越或低于后者；任务特定培训显著减少了临床重要性的错误分级和形态挑战性情况下的误诊；基础模型消耗的能量最多可达TS模型的35倍，引发了关于可持续性的担忧。&lt;h4&gt;结论&lt;/h4&gt;FMs在快速原型设计和研究中提供了明显优势，但作为适用于临床应用的医疗AI通用解决方案的角色仍不确定。对于高风险临床应用而言，严格的验证以及对特定任务培训的考虑至关重要。建议结合基础模型和端到端学习的优点以实现稳健且资源高效的AI病理学解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了人工智能在前列腺癌诊断及格利森评分中的作用，并通过对比两种基于大规模预训练的基础模型与传统任务特异模型，揭示了关于这两种方法临床适用性的新见解。结果强调，在充足标记数据的情况下，任务特定的训练优于基础模型；同时指出，基础模型较高的能耗问题也应引起重视。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The role of artificial intelligence (AI) in pathology has evolved from aidingdiagnostics to uncovering predictive morphological patterns in whole slideimages (WSIs). Recently, foundation models (FMs) leveraging self-supervisedpre-training have been widely advocated as a universal solution for diversedownstream tasks. However, open questions remain about their clinicalapplicability and generalization advantages over end-to-end learning usingtask-specific (TS) models. Here, we focused on AI with clinical-gradeperformance for prostate cancer diagnosis and Gleason grading. We present thelargest validation of AI for this task, using over 100,000 core needle biopsiesfrom 7,342 patients across 15 sites in 11 countries. We compared two FMs with afully end-to-end TS model in a multiple instance learning framework. Ourfindings challenge assumptions that FMs universally outperform TS models. WhileFMs demonstrated utility in data-scarce scenarios, their performance convergedwith - and was in some cases surpassed by - TS models when sufficient labeledtraining data were available. Notably, extensive task-specific trainingmarkedly reduced clinically significant misgrading, misdiagnosis of challengingmorphologies, and variability across different WSI scanners. Additionally, FMsused up to 35 times more energy than the TS model, raising concerns about theirsustainability. Our results underscore that while FMs offer clear advantagesfor rapid prototyping and research, their role as a universal solution forclinically applicable medical AI remains uncertain. For high-stakes clinicalapplications, rigorous validation and consideration of task-specific trainingremain critically important. We advocate for integrating the strengths of FMsand end-to-end learning to achieve robust and resource-efficient AI pathologysolutions fit for clinical use.</description>
      <author>example@mail.com (Nita Mulliqi, Anders Blilie, Xiaoyi Ji, Kelvin Szolnoky, Henrik Olsson, Sol Erika Boman, Matteo Titus, Geraldine Martinez Gonzalez, Julia Anna Mielcarz, Masi Valkonen, Einar Gudlaugsson, Svein R. Kjosavik, José Asenjo, Marcello Gambacorta, Paolo Libretti, Marcin Braun, Radzislaw Kordek, Roman Łowicki, Kristina Hotakainen, Päivi Väre, Bodil Ginnerup Pedersen, Karina Dalsgaard Sørensen, Benedicte Parm Ulhøi, Pekka Ruusuvuori, Brett Delahunt, Hemamali Samaratunga, Toyonori Tsuzuki, Emilius A. M. Janssen, Lars Egevad, Martin Eklund, Kimmo Kartasalo)</author>
      <guid isPermaLink="false">2502.21264v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Causality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models</title>
      <link>http://arxiv.org/abs/2502.21123v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;确保机器学习系统的可信性至关重要，尤其是在其被嵌入到高风险领域时。&lt;h4&gt;背景&lt;/h4&gt;在机器学习系统变得越来越重要和广泛应用的同时，如何保证这些系统的公平性、隐私性、健壮性、准确性和可解释性成为了研究的重点。&lt;h4&gt;目的&lt;/h4&gt;该论文旨在通过引入因果方法来解决可信机器学习中的多重目标之间的矛盾，并提高其可靠性与解释性。&lt;h4&gt;方法&lt;/h4&gt;通过回顾现有文献中将因果方法应用于机器学习的成功案例，展示如何有效结合这些原则以达成平衡。&lt;h4&gt;主要发现&lt;/h4&gt;指出采用因果推理框架能够帮助更好地管理多个相互竞争的目标，在可信机器学习和基础模型设计方面提供解决方案。&lt;h4&gt;结论&lt;/h4&gt;论文讨论了采纳因果框架面临的挑战、局限性及机会，并倡导在未来的AI系统中使用更加负责任且道德的策略。&lt;h4&gt;翻译&lt;/h4&gt;确保机器学习系统的信任度至关重要，尤其是在它们被广泛应用于高风险领域时。本文提倡将因果方法融入到机器学习中来处理关键原则之间的权衡问题，如公平性、隐私性、鲁棒性、准确性及可解释性。尽管这些目标应理想地同时满足，但现实中往往单独考虑，导致冲突和次优解的产生。通过参考现有的因果推理在机器学习中的应用案例，本文强调了采用因果方法对于平衡多重竞争目标的重要性，并探讨如何实际将因果理论应用于机器学习模型中，以提升其可靠性和可解释性。此外，还讨论了采纳这种框架所面临的挑战、限制和机遇，为更负责任及伦理规范的AI系统开发铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring trustworthiness in machine learning (ML) systems is crucial as theybecome increasingly embedded in high-stakes domains. This paper advocates forintegrating causal methods into machine learning to navigate the trade-offsamong key principles of trustworthy ML, including fairness, privacy,robustness, accuracy, and explainability. While these objectives should ideallybe satisfied simultaneously, they are often addressed in isolation, leading toconflicts and suboptimal solutions. Drawing on existing applications ofcausality in ML that successfully align goals such as fairness and accuracy orprivacy and robustness, this paper argues that a causal approach is essentialfor balancing multiple competing objectives in both trustworthy ML andfoundation models. Beyond highlighting these trade-offs, we examine howcausality can be practically integrated into ML and foundation models, offeringsolutions to enhance their reliability and interpretability. Finally, wediscuss the challenges, limitations, and opportunities in adopting causalframeworks, paving the way for more accountable and ethically sound AI systems.</description>
      <author>example@mail.com (Ruta Binkyte, Ivaxi Sheth, Zhijing Jin, Mohammad Havaei, Bernhard Schölkopf, Mario Fritz)</author>
      <guid isPermaLink="false">2502.21123v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>GraphBridge: Towards Arbitrary Transfer Learning in GNNs</title>
      <link>http://arxiv.org/abs/2502.19252v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, 6 tables, to be published in ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为GraphBridge的框架，旨在解决图神经网络（GNN）在不同任务和数据集之间知识迁移的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的GNN训练方式是针对特定的任务或领域进行的，这导致了跨不同、异构的数据设置的知识转移存在障碍。&lt;h4&gt;目的&lt;/h4&gt;提出一种通用的方法来实现GNN中的跨域和跨任务的知识迁移。&lt;h4&gt;方法&lt;/h4&gt;GraphBridge通过增加预训练模型上的预测头以及输入层与输出层之间的桥梁网络，以保持原模型的固有知识并支持任意维度的输出。为了减少目标领域的源偏差问题，该框架将源模型合并到一个同时训练的目标模型中。&lt;h4&gt;主要发现&lt;/h4&gt;在图转图、节点转节点、图转节点以及图转点云等多种迁移学习场景下进行了广泛的实验验证，并通过16个具有代表性的数据集证明了其在任务和领域无关的图结构数据中的知识转移能力，标志着GNN领域的重大进展。&lt;h4&gt;结论&lt;/h4&gt;GraphBridge框架提供了一种有效的方法来解决跨不同任务和域的知识迁移问题，在多个场景中显示出优越的表现。源代码可在https://github.com/jujulili888/GraphBridge获得。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNNs）通常针对特定领域或特定任务进行训练，这在将所获取知识转移到不同的、异构的数据设置时造成了障碍。本文介绍了GraphBridge框架，一种用于实现不同任务和域之间知识转移的新方法，无需对任务配置或图结构进行修改。具体而言，GraphBridge允许通过添加预测头和连接输入层到输出层的桥梁网络来增强任何预训练的GNN模型。此架构不仅保留了原始模型的内在知识，还支持任意维度的输出。为了解决负向迁移问题，GraphBridge将源模型与同时训练的目标模型合并在一起，在应用于目标领域时减少了源偏置。我们的方法在包括图转图、节点转节点、图转节点以及图转点云在内的多种迁移学习场景中进行了全面评估，并通过代表这些场景的16个数据集上的实验证明了该框架在任务和领域无关的图结构中的知识转移能力，标志着GNN领域的重大进展。代码可在https://github.com/jujulili888/GraphBridge获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are conventionally trained on a per-domain,per-task basis. It creates a significant barrier in transferring the acquiredknowledge to different, heterogeneous data setups. This paper introducesGraphBridge, a novel framework to enable knowledge transfer across disparatetasks and domains in GNNs, circumventing the need for modifications to taskconfigurations or graph structures. Specifically, GraphBridge allows for theaugmentation of any pre-trained GNN with prediction heads and a bridgingnetwork that connects the input to the output layer. This architecture not onlypreserves the intrinsic knowledge of the original model but also supportsoutputs of arbitrary dimensions. To mitigate the negative transfer problem,GraphBridge merges the source model with a concurrently trained model, therebyreducing the source bias when applied to the target domain. Our method isthoroughly evaluated across diverse transfer learning scenarios, includingGraph2Graph, Node2Node, Graph2Node, and graph2point-cloud. Empiricalvalidation, conducted over 16 datasets representative of these scenarios,confirms the framework's capacity for task- and domain-agnostic transferlearning within graph-like data, marking a significant advancement in the fieldof GNNs. Code is available at https://github.com/jujulili888/GraphBridge.</description>
      <author>example@mail.com (Li Ju, Xingyi Yang, Qi Li, Xinchao Wang)</author>
      <guid isPermaLink="false">2502.19252v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare Text Multi-Label Classification</title>
      <link>http://arxiv.org/abs/2502.14189v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;随着收集的医疗文本数据量的不断增加，自动化多标签文本分类（MLTC）面临独特挑战，主要是由于训练所需标记文本的稀缺性和其复杂性。传统机器学习模型通常无法完全捕捉到表达的主题范围。然而，大型语言模型（LLMs）在不同领域的多项自然语言处理任务中展示了显著的效果，这些模型具有出色的计算效率，并且通过提示工程能够适用于无监督学习。因此，这些LLM为医疗叙述的MLTC提供了有效的解决方案。然而，在面对各种标签时，不同的提示可能根据主题的相关性而变化。为了应对这一挑战，提出的QUAD-LLM-MLTC方法利用了四个大型语言模型的优势：GPT-4o、BERT、PEGASUS和BART。该方法在顺序流水线中操作，其中BERT提取关键令牌，PEGASUS增强文本数据，GPT-4o进行分类，而BART提供主题分配概率，从而产生四次0-shot设置的分类结果。这些输出通过集成学习组合，并通过元分类器处理以生成最终的MLTC结果。该方法使用三个标记文本样本进行了评估，与传统和单一模型的方法形成了对比。结果显示，在大多数主题上的F1评分及一致性（F1 和 Micro-F1 分数分别达到78.17% 和 80.16%，标准偏差分别为0.025 和 0.011）上有显著改进。&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的多标签文本分类方法，QUAD-LLM-MLTC，利用多个大型语言模型处理医疗数据的复杂性和多样性，并展示了其在F1评分和一致性上的优越性能。&lt;h4&gt;背景&lt;/h4&gt;自动化多标签文本分类（MLTC）因医疗领域的大量未标记数据而面临挑战。传统机器学习模型难以应对这种复杂性。&lt;h4&gt;目的&lt;/h4&gt;探索使用大型语言模型进行高效的无监督学习，以解决大规模医疗文本的自动分类问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的框架QUAD-LLM-MLTC，该框架利用GPT-4o、BERT、PEGASUS和BART四个大型语言模型来进行零样本设置下的多标签文本分类，并通过集成学习和元分类器处理输出以得到最终结果。&lt;h4&gt;主要发现&lt;/h4&gt;与传统方法相比，使用QUAD-LLM-MLTC的方法在多个主题上显示出更高的F1评分及一致性。这种方法展示出强大的性能并可广泛应用于医疗数据的快速分类。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型的应用为解决复杂文本的数据分类问题提供了创新性的解决方案，并通过集成不同模型的优势，在多标签医学文本分类中实现了高效和扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The escalating volume of collected healthcare textual data presents a uniquechallenge for automated Multi-Label Text Classification (MLTC), which isprimarily due to the scarcity of annotated texts for training and their nuancednature. Traditional machine learning models often fail to fully capture thearray of expressed topics. However, Large Language Models (LLMs) havedemonstrated remarkable effectiveness across numerous Natural LanguageProcessing (NLP) tasks in various domains, which show impressive computationalefficiency and suitability for unsupervised learning through promptengineering. Consequently, these LLMs promise an effective MLTC of medicalnarratives. However, when dealing with various labels, different prompts can berelevant depending on the topic. To address these challenges, the proposedapproach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT,PEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in whichBERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, andBART provides topics' assignment probabilities, which results in fourclassifications, all in a 0-shot setting. The outputs are then combined usingensemble learning and processed through a meta-classifier to produce the finalMLTC result. The approach is evaluated using three samples of annotated texts,which contrast it with traditional and single-model methods. The results showsignificant improvements across the majority of the topics in theclassification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and80.16% with standard deviations of 0.025 and 0.011, respectively). Thisresearch advances MLTC using LLMs and provides an efficient and scalablesolution to rapidly categorize healthcare-related text data without furthertraining.</description>
      <author>example@mail.com (Hajar Sakai, Sarah S. Lam)</author>
      <guid isPermaLink="false">2502.14189v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Doracamom: Joint 3D Detection and Occupancy Prediction with Multi-view 4D Radars and Cameras for Omnidirectional Perception</title>
      <link>http://arxiv.org/abs/2501.15394v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Doracamom的框架，该框架融合了多视角相机和4D雷达的数据，用于实现3D物体检测和语义占用预测任务。通过引入新颖的Coarse Voxel Queries Generator、设计Dual-Branch Temporal Encoder以及Cross-Modal BEV-Voxel Fusion模块，使系统能够在复杂环境感知中表现出色。&lt;h4&gt;背景&lt;/h4&gt;3D目标检测和占位预测在自动驾驶领域非常重要，但现有基于视觉的方法在恶劣条件下效果不佳。整合相机与4D成像雷达可以实现多任务统一感知，但在这一领域的研究仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够集成多视角相机和4D雷达的框架，用于完成3D物体检测和语义占用预测任务。&lt;h4&gt;方法&lt;/h4&gt;引入了Coarse Voxel Queries Generator来初始化查询体素；设计了Dual-Branch Temporal Encoder来利用时间信息，并实现了Cross-Modal BEV-Voxel Fusion模块以融合多模态特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，Doracamom在OmniHD-Scenes、View-of-Delft (VoD)和TJ4DRadSet数据集上均达到了当前最佳性能。&lt;h4&gt;结论&lt;/h4&gt;该框架通过结合多种传感器的数据实现了强大的3D感知能力，并为未来的多模态环境感知系统建立了新的基准。&lt;h4&gt;翻译&lt;/h4&gt;三维物体检测和占用预测在自动驾驶中至关重要，吸引了大量关注。尽管最近基于视觉的方法潜力巨大，但在恶劣条件下仍面临挑战。因此，将相机与下一代4D成像雷达相结合以实现统一的多任务感知非常重要，但该领域的研究仍然有限。本文提出了一种名为Doracamom的框架，它融合了多视角摄像头和4D雷达的数据，用于联合执行3D物体检测和语义占用预测，从而实现了全面的环境感知。特别是引入了一个新的粗体素查询生成器，该生成器将从4D雷达获得的几何先验知识与图像中的语义特征相结合来初始化体素查询，为后续基于Transformer的细化建立了坚实的基础。为了利用时间信息，设计了双分支时间编码器，在鸟瞰图和体素空间中并行处理多模态时序特性，从而能够进行全面的空间-时间表示学习。此外，还提出了一个跨模态BEV-Voxel融合模块，通过注意力机制自适应地融合互补特征，并利用辅助任务来提高特征质量。在OmniHD-Scenes、View-of-Delft (VoD)和TJ4DRadSet数据集上的广泛实验表明，Doracamom在这两项任务中均达到了当前最佳性能，为多模态3D感知建立了新的基准。代码和模型将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-01-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection and occupancy prediction are critical tasks in autonomousdriving, attracting significant attention. Despite the potential of recentvision-based methods, they encounter challenges under adverse conditions. Thus,integrating cameras with next-generation 4D imaging radar to achieve unifiedmulti-task perception is highly significant, though research in this domainremains limited. In this paper, we propose Doracamom, the first framework thatfuses multi-view cameras and 4D radar for joint 3D object detection andsemantic occupancy prediction, enabling comprehensive environmental perception.Specifically, we introduce a novel Coarse Voxel Queries Generator thatintegrates geometric priors from 4D radar with semantic features from images toinitialize voxel queries, establishing a robust foundation for subsequentTransformer-based refinement. To leverage temporal information, we design aDual-Branch Temporal Encoder that processes multi-modal temporal features inparallel across BEV and voxel spaces, enabling comprehensive spatio-temporalrepresentation learning. Furthermore, we propose a Cross-Modal BEV-Voxel Fusionmodule that adaptively fuses complementary features through attentionmechanisms while employing auxiliary tasks to enhance feature quality.Extensive experiments on the OmniHD-Scenes, View-of-Delft (VoD), and TJ4DRadSetdatasets demonstrate that Doracamom achieves state-of-the-art performance inboth tasks, establishing new benchmarks for multi-modal 3D perception. Code andmodels will be publicly available.</description>
      <author>example@mail.com (Lianqing Zheng, Jianan Liu, Runwei Guan, Long Yang, Shouyi Lu, Yuanzhe Li, Xiaokai Bai, Jie Bai, Zhixiong Ma, Hui-Liang Shen, Xichan Zhu)</author>
      <guid isPermaLink="false">2501.15394v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>HybridLinker: Topology-Guided Posterior Sampling for Enhanced Diversity and Validity in 3D Molecular Linker Generation</title>
      <link>http://arxiv.org/abs/2502.17349v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种名为HybridLinker的框架被提出，以解决药物设计中连接子生成问题中的多样性和有效性之间的权衡。&lt;h4&gt;背景&lt;/h4&gt;在药物发现应用（如候选物优化和PROTAC设计）中，分子片段组装成不同的药物候选物时链接器生成至关重要。目前的方法可以分为PC-Free和PC-Aware两类，前者基于它们是否使用3D点云(PC)。PC-Free模型更注重多样性，但因忽视了PC约束导致有效性和合法性较低；而PC-Aware模型通过强制执行严格的PC约束来确保更高的有效性和合法性，却限制了多样性。&lt;h4&gt;目的&lt;/h4&gt;为了在不增加额外训练的情况下克服上述权衡问题，提出了一种名为HybridLinker的框架。&lt;h4&gt;方法&lt;/h4&gt;该框架的核心是LinkerDPS（链接器后验扩散采样），它是一种新的扩散后验采样方法，在PC-Free和PC-Aware空间中操作。通过一种能量启发式的函数将分子拓扑结构与3D点云联系起来，从而允许从预训练的PC-Free模型中提供多样化的键合拓扑作为指导来增强PC-Aware推理。&lt;h4&gt;主要发现&lt;/h4&gt;HybridLinker框架在基础分子设计和应用属性优化任务中显著且一致地超过了基准方法，在提高有效性和多样性方面建立了新的扩散后验采样框架，该框架超越了图像领域，适用于分子和图域。&lt;h4&gt;结论&lt;/h4&gt;通过将PC-Free模型的多样化采样分布转移到PC-Aware分布上，HybridLinker在药物设计应用中的多样性和有效性之间找到了一个很好的平衡点。&lt;h4&gt;翻译&lt;/h4&gt;链接器生成是药物发现应用程序（如候选物优化和PROTAC设计）中的关键问题，其中分子片段被组装成不同的药物候选物。现有的方法根据它们是否使用3D点云(PC)分为PC-Free和PC-Aware两类。PC-Free模型优先考虑多样性但有效性和合法性较低；而PC-Aware模型通过强制执行严格的PC约束来确保更高的有效性和合法性，却限制了多样性。为了克服这些权衡问题且不需额外训练，我们提出了HybridLinker框架，该框架通过从预训练的PC-Free模型中提供多样化的键合拓扑作为指导来增强PC-Aware推理。在核心部分，我们提出了一种新的扩散后验采样方法LinkerDPS，在PC-Free和PC-Aware空间之间操作，并通过一种能量启发式的函数将分子拓扑与3D点云联系起来。HybridLinker框架能够将PC-Free模型的多样化采样分布转移至PC-Aware分布上，从而在基础分子设计和应用属性优化任务中显著且一致地超过了基准方法，在提高有效性和多样性方面建立了新的扩散后验采样框架，该框架超越了图像领域，适用于分子和图域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Linker generation is critical in drug discovery applications such as leadoptimization and PROTAC design, where molecular fragments are assembled intodiverse drug candidates. Existing methods fall into PC-Free and PC-Awarecategories based on their use of 3D point clouds (PC). PC-Free modelsprioritize diversity but suffer from lower validity due to overlooking PCconstraints, while PC-Aware models ensure higher validity but restrictdiversity by enforcing strict PC constraints. To overcome these trade-offswithout additional training, we propose HybridLinker, a framework that enhancesPC-Aware inference by providing diverse bonding topologies from a pretrainedPC-Free model as guidance. At its core, we propose LinkerDPS, the firstdiffusion posterior sampling (DPS) method operating across PC-Free and PC-Awarespaces, bridging molecular topology with 3D point clouds via an energy-inspiredfunction. By transferring the diverse sampling distribution of PC-Free modelsinto the PC-Aware distribution, HybridLinker significantly and consistentlysurpasses baselines, improving both validity and diversity in foundationalmolecular design and applied property optimization tasks, establishing a newDPS framework in the molecular and graph domains beyond imaging.</description>
      <author>example@mail.com (Minyeong Hwang, Ziseok Lee, Kwang-Soo Kim, Kyungsu Kim, Eunho Yang)</author>
      <guid isPermaLink="false">2502.17349v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction</title>
      <link>http://arxiv.org/abs/2502.21186v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR2025. Code would be available at  https://github.com/BaitingLuo/L-MAP.git&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的离线强化学习框架L-MAP，旨在通过学习一组时间扩展的宏观动作来解决高维连续动作空间中的顺序决策问题。&lt;h4&gt;背景&lt;/h4&gt;在具有随机动态的复杂环境中进行顺序决策时，尤其是在需要基于历史数据训练代理以做出决策的情况下，面临计算挑战。这些环境通常包含高维度的动作空间和不确定的状态转换。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来解决传统离线强化学习中遇到的问题，特别是如何有效地处理高维连续动作空间中的随机性问题。&lt;h4&gt;方法&lt;/h4&gt;L-MAP通过状态条件下的向量量化变分自动编码器(VQ-VAE)来减少行动维度，并使用蒙特卡洛树搜索(MCTS)算法在决策过程中考虑环境和行为策略的随机性。此外，还引入了一个独立学习到的先验模型作为潜在转换模型，以实现可能动作的有效采样。&lt;h4&gt;主要发现&lt;/h4&gt;L-MAP在离线强化学习设置中表现优异，在处理复杂和高维的动作空间时显示出低决策延迟，并且能够保持与基于模型的方法相匹配的表现。&lt;h4&gt;结论&lt;/h4&gt;L-MAP提供了一种有效解决具有高度不确定性和动作维度问题的策略规划方法，表明了这种方法在处理具有随机性环境中的顺序决策任务的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential decision-making in high-dimensional continuous action spaces,particularly in stochastic environments, faces significant computationalchallenges. We explore this challenge in the traditional offline RL setting,where an agent must learn how to make decisions based on data collected througha stochastic behavior policy. We present Latent Macro Action Planner (L-MAP),which addresses this challenge by learning a set of temporally extendedmacro-actions through a state-conditional Vector Quantized VariationalAutoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employsa (separate) learned prior model that acts as a latent transition model andallows efficient sampling of plausible actions. During planning, our approachaccounts for stochasticity in both the environment and the behavior policy byusing Monte Carlo tree search (MCTS). In offline RL settings, includingstochastic continuous control tasks, L-MAP efficiently searches over discretelatent actions to yield high expected returns. Empirical results demonstratethat L-MAP maintains low decision latency despite increased actiondimensionality. Notably, across tasks ranging from continuous control withinherently stochastic dynamics to high-dimensional robotic hand manipulation,L-MAP significantly outperforms existing model-based methods and performson-par with strong model-free actor-critic baselines, highlighting theeffectiveness of the proposed approach in planning in complex and stochasticenvironments with high-dimensional action spaces.</description>
      <author>example@mail.com (Baiting Luo, Ava Pettet, Aron Laszka, Abhishek Dubey, Ayan Mukhopadhyay)</author>
      <guid isPermaLink="false">2502.21186v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>GP-GS: Gaussian Processes for Enhanced Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2502.02283v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages,11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Gaussian Processes Gaussian Splatting (GP-GS)的3D重建框架，用于提高稀疏结构从运动(SfM)点云的场景重建质量。&lt;h4&gt;背景&lt;/h4&gt;3D高斯斑点方法是一种高效的逼真新视图合成方法，但其依赖于稀疏的SfM点云，导致了场景重建的质量问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种自适应和不确定性指导的稠密化框架来改进现有的3D重建效果。&lt;h4&gt;方法&lt;/h4&gt;引入了一种多输出高斯过程模型，并提出了一种动态采样与过滤管道，利用基于GP预测的新候选点从输入2D像素和深度图中生成密集点云。&lt;h4&gt;主要发现&lt;/h4&gt;该框架通过不确定性估计指导的稀疏SfM点云稠密化提高了3D重建质量，特别是在几何一致性和稠密性方面表现突出。&lt;h4&gt;结论&lt;/h4&gt;实验结果验证了GP-GS框架的有效性和实用性，在合成数据集和真实世界数据集中均表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯斑点方法作为一种高效的逼真新视图合成方法已经出现。然而，其依赖于稀疏的结构从运动（SfM）点云持续地影响了场景重建的质量。为了应对这些限制，本文提出了一种新的三维重建框架——高斯过程高斯斑点（GP-GS），其中开发了一个多输出的高斯过程模型来实现对稀疏SfM点云的自适应和不确定性指导的稠密化。具体来说，我们提出了一条动态采样和过滤流水线，它通过利用基于GP预测从输入2D像素和深度图中推断新的候选点来自适应地扩展了SfM点云，并且该流程利用不确定性估计来引导高方差预测的修剪工作，确保了几何一致性并使密集点云生成成为可能。这些稠密化的点云提供了高质量的初始3D高斯分布以增强重建性能。在各种规模上的合成和真实世界数据集上进行的一系列实验验证了所提出框架的有效性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zhihaohaoran/GPGS&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting has emerged as an efficient photorealistic novel viewsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)point clouds consistently compromises the scene reconstruction quality. Toaddress these limitations, this paper proposes a novel 3D reconstructionframework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-outputGaussian Process model is developed to achieve adaptive and uncertainty-guideddensification of sparse SfM point clouds. Specifically, we propose a dynamicsampling and filtering pipeline that adaptively expands the SfM point clouds byleveraging GP-based predictions to infer new candidate points from the input 2Dpixels and depth maps. The pipeline utilizes uncertainty estimates to guide thepruning of high-variance predictions, ensuring geometric consistency andenabling the generation of dense point clouds. The densified point cloudsprovide high-quality initial 3D Gaussians to enhance reconstructionperformance. Extensive experiments conducted on synthetic and real-worlddatasets across various scales validate the effectiveness and practicality ofthe proposed framework.</description>
      <author>example@mail.com (Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Liangxiu Han, Peng Wang)</author>
      <guid isPermaLink="false">2502.02283v3</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>A Fused Gromov-Wasserstein Approach to Subgraph Contrastive Learning</title>
      <link>http://arxiv.org/abs/2502.20885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;本文介绍了一种新的自监督图表示学习方法——FOSSIL，用于解决现有对比学习方法在利用结构模式和节点相似性方面的不足。&lt;h4&gt;背景&lt;/h4&gt;自我监督学习已成为处理标注数据稀缺或不可用情况的关键方法。然而，在设计有效的预训练任务以进行自监督图表示学习方面仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合了节点级与子图级对比学习的新方法，旨在更有效地利用图形的结构模式和节点相似性。&lt;h4&gt;方法&lt;/h4&gt;FOSSIL模型通过将标准的节点级别对比损失函数与融合Gromov-Wasserstein距离相结合，可以同时捕捉节点特征和图结构。此外，该方法适用于同构及异构图，并能动态创建视角以生成正负样本对。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在基准图数据集上的测试中，FOSSIL优于或达到了目前最先进的方法的性能水平。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效地改善现有的自监督图表示学习技术，尤其在利用复杂结构模式和节点相似性方面有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;自我监督学习已成为训练深度学习模型的重要手段，尤其是在标注数据稀缺的情况下。尽管图机器学习在各个领域都有巨大的潜力，但设计有效的预训练任务以进行自监督图表示学习仍然具有挑战性。对比学习是图的自我监督学习的一种流行方法，它利用正负对来计算对比损失函数。然而，目前的图对比学习方法往往难以充分使用结构模式和节点相似性。为了解决这些问题，我们提出了一种名为Fused Gromov Wasserstein Subgraph Contrastive Learning（FOSSIL）的新方法。我们的模型集成了节点级和子图级别的对比学习，无缝结合了标准的节点级别对比损失函数与融合Gromov-Wasserstein距离。这种组合使我们的方法能够同时捕捉节点特征和图形结构。重要的是，该方法既适用于同构图也适用于异构图，并能动态创建视角以生成正负样本对。通过在基准图数据集上的广泛实验，我们证明FOSSIL比或与当前最先进的方法性能相当。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning has become a key method for training deep learningmodels when labeled data is scarce or unavailable. While graph machine learningholds great promise across various domains, the design of effective pretexttasks for self-supervised graph representation learning remains challenging.Contrastive learning, a popular approach in graph self-supervised learning,leverages positive and negative pairs to compute a contrastive loss function.However, current graph contrastive learning methods often struggle to fully usestructural patterns and node similarities. To address these issues, we presenta new method called Fused Gromov Wasserstein Subgraph Contrastive Learning(FOSSIL). Our model integrates node-level and subgraph-level contrastivelearning, seamlessly combining a standard node-level contrastive loss with theFused Gromov-Wasserstein distance. This combination helps our method captureboth node features and graph structure together. Importantly, our approachworks well with both homophilic and heterophilic graphs and can dynamicallycreate views for generating positive and negative pairs. Through extensiveexperiments on benchmark graph datasets, we show that FOSSIL outperforms orachieves competitive performance compared to current state-of-the-art methods.</description>
      <author>example@mail.com (Amadou S. Sangare, Nicolas Dunou, Jhony H. Giraldo, Fragkiskos D. Malliaros)</author>
      <guid isPermaLink="false">2502.20885v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
  <item>
      <title>Assessing zero-shot generalisation behaviour in graph-neural-network interatomic potentials</title>
      <link>http://arxiv.org/abs/2502.21317v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了机器学习原子间势能模型（MLIP）在材料化学和分子化学之间的迁移能力。&lt;h4&gt;背景&lt;/h4&gt;随着机器学习原子间势能模型的广泛应用，如何设计适用于多种应用领域的基础性MLIP成为了当前的研究重点。&lt;h4&gt;目的&lt;/h4&gt;评估一种特定于石墨烯氧化物扩展共价网络设计的MLIP（GO-MACE-23）在处理小型独立分子和化学反应时的表现。&lt;h4&gt;方法&lt;/h4&gt;通过将该模型与专门为某一领域训练的状态-of-the-art模型进行直接比较，来量化其零样本学习性能。&lt;h4&gt;主要发现&lt;/h4&gt;提供了图神经网络势能的迁移能力和泛化能力的定量见解。&lt;h4&gt;结论&lt;/h4&gt;这项工作促进了MLIP在化学中的更广泛应用，并为进一步研究这类模型如何在不同领域的应用中发挥作用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;随着机器学习原子间势能（MLIP）模型在化学领域迅速可用性的增长，当前许多研究集中在开发通用且“基础性”的MLIP上。在这个背景下，一个重要的问题是这些模型是否以及在多大程度上可以在不同的应用场景之间转移。在这里，我们评估了一种MLIP模型在材料和分子化学之间的迁移能力。具体来说，我们研究了GO-MACE-23模型，该模型旨在用于石墨烯氧化物的扩展共价网络，并量化了它对小型独立分子和在其直接作用范围之外的化学反应的零样本性能——与专门为某一领域训练的状态-of-the-art模型进行直接比较。我们的工作为图神经网络势能的迁移和泛化能力提供了定量见解，更广泛地说，朝着MLIP在化学中的更广泛应用迈进了一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapidly growing availability of machine-learned interatomicpotential (MLIP) models for chemistry, much current research focuses on thedevelopment of generally applicable and ``foundational'' MLIPs. An importantquestion in this context is whether, and how well, such models can transferfrom one application domain to another. Here, we assess this transferabilityfor an MLIP model at the interface of materials and molecular chemistry.Specifically, we study GO-MACE-23, a model designed for the extended covalentnetwork of graphene oxide, and quantify its zero-shot performance for small,isolated molecules and chemical reactions outside its direct scope--in directcomparison with a state-of-the-art model which has been trained in-domain. Ourwork provides quantitative insight into the transfer and generalisation abilityof graph-neural-network potentials and, more generally, makes a step towardsthe more widespread applicability of MLIPs in chemistry.</description>
      <author>example@mail.com (Chiheb Ben Mahmoud, Zakariya El-Machachi, Krystian A. Gierczak, John L. A. Gardner, Volker L. Deringer)</author>
      <guid isPermaLink="false">2502.21317v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>RuCCoD: Towards Automated ICD Coding in Russian</title>
      <link>http://arxiv.org/abs/2502.21263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本研究探讨了在俄语等生物医学资源有限的语言环境中实现临床编码自动化的可行性。&lt;h4&gt;背景&lt;/h4&gt;当前，许多语言的生物医学资源较为匮乏，特别是在俄语中。这限制了相关领域的自动化进程，如临床编码。&lt;h4&gt;目的&lt;/h4&gt;通过创建新的ICD（国际疾病分类）编码数据集来研究在俄语等资源有限的语言环境中自动进行临床编码的可能性。&lt;h4&gt;方法&lt;/h4&gt;{'数据准备': '构建了一个包含来自电子健康记录（EHRs）的诊断字段的数据集，该数据集包括超过10,000个实体和超过1,500种独特的ICD代码。', '模型测试': '利用这个数据集作为基准，测试了几种最先进的模型，如BERT、LLaMA与LoRA结合使用以及RAG。进行了额外的跨域迁移学习实验（从PubMed摘要到医学诊断）以及术语转换实验（从UMLS概念到ICD编码）。', '应用': '将性能最佳的模型应用于公司内部EHR数据集，该数据集中包含了2017年至2021年的患者历史记录。'}&lt;h4&gt;主要发现&lt;/h4&gt;利用自动化预测代码进行训练后，与医生手动注释的数据相比，在精心准备的测试集上显示出显著提高的准确性。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，在资源有限的语言环境中实现临床编码自动化的潜力巨大，这可以提升这些语境下的医疗效率和数据准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the feasibility of automating clinical coding inRussian, a language with limited biomedical resources. We present a new datasetfor ICD coding, which includes diagnosis fields from electronic health records(EHRs) annotated with over 10,000 entities and more than 1,500 unique ICDcodes. This dataset serves as a benchmark for several state-of-the-art models,including BERT, LLaMA with LoRA, and RAG, with additional experiments examiningtransfer learning across domains (from PubMed abstracts to medical diagnosis)and terminologies (from UMLS concepts to ICD codes). We then apply thebest-performing model to label an in-house EHR dataset containing patienthistories from 2017 to 2021. Our experiments, conducted on a carefully curatedtest set, demonstrate that training with the automated predicted codes leads toa significant improvement in accuracy compared to manually annotated data fromphysicians. We believe our findings offer valuable insights into the potentialfor automating clinical coding in resource-limited languages like Russian,which could enhance clinical efficiency and data accuracy in these contexts.</description>
      <author>example@mail.com (Aleksandr Nesterov, Andrey Sakhovskiy, Ivan Sviridov, Airat Valiev, Vladimir Makharev, Petr Anokhin, Galina Zubkova, Elena Tutubalina)</author>
      <guid isPermaLink="false">2502.21263v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Parameter Efficient Source-free Post-pretraining</title>
      <link>http://arxiv.org/abs/2502.21313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种新的无监督参数高效源无关后预训练方法UpStep，用于在没有源领域数据的情况下将预先训练的模型适应到目标领域。&lt;h4&gt;背景&lt;/h4&gt;随着NLP领域的成功，最佳视觉模型现在达到数十亿参数规模。由于计算和经济原因，在目标分布上调整这些大规模模型变得不可行。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的方法来解决在没有源领域数据的情况下将预先训练的模型适应到新目标领域的问题。&lt;h4&gt;方法&lt;/h4&gt;{'i': '设计了一个自我监督的训练方案，可以在没有任何来源数据的情况下对未标记的目标域进行预训练模型调整。', 'ii': '提出了中心向量正则化（CVR），这是一组辅助操作，最小化灾难性遗忘，并通过在50%的训练迭代中跳过反向传播来降低计算成本。', 'iii': '采用低秩适应方法以参数高效的方式进行模型调整。'}&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够在各种基础架构上（包括监督和非监督训练于Imagenet上的）展示出良好的适应性和泛化能力，将其应用于八个不同的目标领域。&lt;h4&gt;结论&lt;/h4&gt;通过UpStep方法可以有效地在没有源领域数据的情况下将大规模预训练模型调整到新任务中，从而克服了计算成本的限制。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着NLP领域的成功，最佳视觉模型现在达到数十亿参数规模。由于计算和经济原因，在目标分布上调整这些大规模模型变得不可行。为了解决这个问题，我们介绍了一种新的无监督参数高效源无关后预训练方法UpStep，用于在没有源领域数据的情况下将预先训练的模型适应到目标领域：i) 设计了一个自我监督的训练方案，可以在没有任何来源数据的情况下对未标记的目标域进行预训练模型调整。由于这种源无关设置存在灾难性遗忘的风险，ii) 提出了中心向量正则化（CVR），这是一组辅助操作，最小化灾难性遗忘，并通过在50%的训练迭代中跳过反向传播来降低计算成本。最后iii) 采用低秩适应方法以参数高效的方式进行模型调整。我们利用各种一般骨干架构作为基础模型并将其适配到八个不同的目标领域中，展示了我们的方法具有良好的适用性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Following the success in NLP, the best vision models are now in the billionparameter ranges. Adapting these large models to a target distribution hasbecome computationally and economically prohibitive. Addressing this challenge,we introduce UpStep, an Unsupervised Parameter-efficient Source-freepost-pretraining approach, designed to efficiently adapt a base model from asource domain to a target domain: i) we design a self-supervised trainingscheme to adapt a pretrained model on an unlabeled target domain in a settingwhere source domain data is unavailable. Such source-free setting comes withthe risk of catastrophic forgetting, hence, ii) we propose center vectorregularization (CVR), a set of auxiliary operations that minimize catastrophicforgetting and additionally reduces the computational cost by skippingbackpropagation in 50\% of the training iterations. Finally iii) we performthis adaptation process in a parameter-efficient way by adapting the pretrainedmodel through low-rank adaptation methods, resulting in a fraction ofparameters to optimize. We utilize various general backbone architectures, bothsupervised and unsupervised, trained on Imagenet as our base model and adaptthem to a diverse set of eight target domains demonstrating the adaptabilityand generalizability of our proposed approach.</description>
      <author>example@mail.com (Abhishek Jha, Tinne Tuytelaars, Yuki M. Asano)</author>
      <guid isPermaLink="false">2502.21313v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Fast 3D point clouds retrieval for Large-scale 3D Place Recognition</title>
      <link>http://arxiv.org/abs/2502.21067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 1 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个基于Transformer的加速3D点云检索的方法，通过生成一维标识符实现恒定时间内的直接检索。&lt;h4&gt;背景&lt;/h4&gt;在3D点云中寻找最相似的点云是一项具有挑战性的任务。当前方法主要集中在比较描述符以识别相似性，但这个步骤复杂且耗时。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Differentiable Search Index (DSI)的方法来加速3D点云检索过程。&lt;h4&gt;方法&lt;/h4&gt;通过集成视觉Transformer将点云描述符映射到一维标识符，并结合位置和语义编码以适应三维数据。这种方法使得可以直接根据生成的标识符进行恒定时间内的检索。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在公开基准测试中的检索质量和速度方面都优于现有最先进的技术。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为3D点云检索提供了一种高效且准确的新方案。&lt;h4&gt;翻译&lt;/h4&gt;三维点云的检索是一项具有挑战性的任务，旨在从参考点集中检索与给定查询最相似的点云。当前方法集中在通过比较描述符来识别相似性上。由于这一步骤复杂，我们专注于使用可微搜索索引(DSI)，一种最初为文本信息检索设计的基于Transformer的方法，来加速三维点云检索过程。我们的方法生成了一维标识符，该标识符基于点描述符，并使得可以直接在恒定时间内进行检索。为了将DSI适应于3D数据，我们整合了视觉变换器以将描述符映射到这些标识符，同时结合位置和语义编码。我们在公共基准测试的地点识别上评估了此方法，通过与现有最先进的方法比较其点云检索能力和返回的速度及质量来衡量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieval in 3D point clouds is a challenging task that consists inretrieving the most similar point clouds to a given query within a reference of3D points. Current methods focus on comparing descriptors of point clouds inorder to identify similar ones. Due to the complexity of this latter step, herewe focus on the acceleration of the retrieval by adapting the DifferentiableSearch Index (DSI), a transformer-based approach initially designed for textinformation retrieval, for 3D point clouds retrieval. Our approach generates 1Didentifiers based on the point descriptors, enabling direct retrieval inconstant time. To adapt DSI to 3D data, we integrate Vision Transformers to mapdescriptors to these identifiers while incorporating positional and semanticencoding. The approach is evaluated for place recognition on a public benchmarkcomparing its retrieval capabilities against state-of-the-art methods, in termsof quality and speed of returned point clouds.</description>
      <author>example@mail.com (Chahine-Nicolas Zede, Laurent Carrafa, Valérie Gouet-Brunet)</author>
      <guid isPermaLink="false">2502.21067v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>LV-DOT: LiDAR-visual dynamic obstacle detection and tracking for autonomous robot navigation</title>
      <link>http://arxiv.org/abs/2502.20607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;室内自主机器人导航中的动态障碍物感知对于精确导航至关重要。&lt;h4&gt;背景&lt;/h4&gt;尽管在计算机视觉和自动驾驶领域对3D物体检测和跟踪方法进行了深入研究和发展，但这些方法需要昂贵且高精度的传感器设置以及大型神经网络计算资源，使其不适合用于室内机器人。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于车载相机和LiDAR数据的动态障碍物检测与跟踪框架，以实现轻量级且精确的感知。&lt;h4&gt;方法&lt;/h4&gt;{'融合策略': '采用更稳健的数据融合策略，结合了LiDAR和视觉信息，提高了检测精度。使用特征关联和卡尔曼滤波器进行目标追踪，并设计了一种动态障碍物分类算法来可靠地识别移动物体。', '集成检测方法': '基于先前的集合检测方法，该方法整合来自多个低准确度但计算效率高的探测器的结果，以确保在车载计算机上实现实时性能。'}&lt;h4&gt;主要发现&lt;/h4&gt;数据集评估显示，与基准方法相比，所提出的方法具有更好的感知性能；物理实验也证实了这种方法在实际导航中的可行性。&lt;h4&gt;结论&lt;/h4&gt;该研究成功地解决了单一传感器的限制问题，并通过结合LiDAR和视觉信息实现了更加精确且实时的动态障碍物检测与跟踪。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate perception of dynamic obstacles is essential for autonomous robotnavigation in indoor environments. Although sophisticated 3D object detectionand tracking methods have been investigated and developed thoroughly in thefields of computer vision and autonomous driving, their demands on expensiveand high-accuracy sensor setups and substantial computational resources fromlarge neural networks make them unsuitable for indoor robotics. Recently, morelightweight perception algorithms leveraging onboard cameras or LiDAR sensorshave emerged as promising alternatives. However, relying on a single sensorposes significant limitations: cameras have limited fields of view and cansuffer from high noise, whereas LiDAR sensors operate at lower frequencies andlack the richness of visual features. To address this limitation, we propose adynamic obstacle detection and tracking framework that uses both onboard cameraand LiDAR data to enable lightweight and accurate perception. Our proposedmethod expands on our previous ensemble detection approach, which integratesoutputs from multiple low-accuracy but computationally efficient detectors toensure real-time performance on the onboard computer. In this work, we proposea more robust fusion strategy that integrates both LiDAR and visual data toenhance detection accuracy further. We then utilize a tracking module thatadopts feature-based object association and the Kalman filter to track andestimate detected obstacles' states. Besides, a dynamic obstacle classificationalgorithm is designed to robustly identify moving objects. The datasetevaluation demonstrates a better perception performance compared to benchmarkmethods. The physical experiments on a quadcopter robot confirms thefeasibility for real-world navigation.</description>
      <author>example@mail.com (Zhefan Xu, Haoyu Shen, Xinming Han, Hanyu Jin, Kanlong Ye, Kenji Shimada)</author>
      <guid isPermaLink="false">2502.20607v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation</title>
      <link>http://arxiv.org/abs/2502.20984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;SemEval-2025 Task 1旨在根据给定的名词短语在英语和巴西葡萄牙语中可能携带的习惯用法意义对图像进行排名。&lt;h4&gt;背景&lt;/h4&gt;该任务涉及使用生成式大型语言模型（LLMs）和多语言CLIP模型来增强惯用表达的意义表示，以解决基于具有惯用含义的名词短语给图片打分的问题。&lt;h4&gt;目的&lt;/h4&gt;为了提高图片排序的效果，本文提出了一种方法结合使用LLM和多语言CLIP模型，并应用对比学习和数据增强技术对生成的嵌入进行微调。&lt;h4&gt;方法&lt;/h4&gt;大型语言模型用于生成潜在惯用表达的意义，而这些意义随后被编码为图像排名中的表示。然后通过对比学习和技术手段调整后的数据增强来精炼这些嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，通过这种方法提取的多模态表示优于仅基于原始名词短语的方法。然而，微调方法虽然显示出有前景的结果，但不如使用未经微调的嵌入有效。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一个利用大型语言模型和CLIP技术改进惯用表达图像排名的新方案，并通过实验验证了其有效性，尽管还存在进一步优化的空间。&lt;h4&gt;翻译&lt;/h4&gt;SemEval-2025 Task 1专注于根据具有潜在习惯意义的名词短语对图片进行排序。为了解决这个问题，这项工作利用生成式大型语言模型（LLMs）和多语言CLIP模型来增强惯用表达的意义表示。通过这种方式，研究者们在提高图像排名精度方面取得了显著进展，并且他们的源代码可在GitHub上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; SemEval-2025 Task 1 focuses on ranking images based on their alignment with agiven nominal compound that may carry idiomatic meaning in both English andBrazilian Portuguese. To address this challenge, this work uses generativelarge language models (LLMs) and multilingual CLIP models to enhance idiomaticcompound representations. LLMs generate idiomatic meanings for potentiallyidiomatic compounds, enriching their semantic interpretation. These meaningsare then encoded using multilingual CLIP models, serving as representations forimage ranking. Contrastive learning and data augmentation techniques areapplied to fine-tune these embeddings for improved performance. Experimentalresults show that multimodal representations extracted through this methodoutperformed those based solely on the original nominal compounds. Thefine-tuning approach shows promising outcomes but is less effective than usingembeddings without fine-tuning. The source code used in this paper is availableat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.</description>
      <author>example@mail.com (Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang)</author>
      <guid isPermaLink="false">2502.20984v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>JiTTER: Jigsaw Temporal Transformer for Event Reconstruction for Self-Supervised Sound Event Detection</title>
      <link>http://arxiv.org/abs/2502.20857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了JiTTER，一种改进的自监督学习框架，用于提高基于变压器的声音事件检测模型在时间建模方面的性能。&lt;h4&gt;背景&lt;/h4&gt;自我监督学习方法特别是MAT-SED在声音事件检测（SED）中取得了显著效果。然而，这种技术在捕捉瞬态音频事件和保持时间顺序方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自监督框架JiTTER，以增强基于变压器的声音事件检测模型的时间建模能力。&lt;h4&gt;方法&lt;/h4&gt;JiTTER引入了一种分层的随机时间重排重建策略，在块级和帧级随机打乱音频序列，强迫模型重建正确的时序。同时通过在块级重排过程中注入噪声进一步提升特征学习的正则化效果和模型鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明JiTTER相比MAT-SED提升了5.89%的PSDS值，在DESED数据集上表现出更优性能，说明结构化的时序重建任务比简单的掩码预测更有助于自监督学习在声音事件表示学习中的表现。&lt;h4&gt;结论&lt;/h4&gt;研究发现证明了有结构的时间重建任务对于基于自监督学习的声音事件检测模型来说是一种更为有效的预训练范式。&lt;h4&gt;翻译&lt;/h4&gt;声音事件检测（SED）已从自我监督学习（SSL）方法中显著受益，特别是MAT-SED，该方法利用掩码块预测来恢复丢失的音频片段。然而，尽管在捕捉全局依赖性方面有效，掩码块预测却破坏了瞬态声学事件并且缺乏对时间顺序的明确约束，使其不太适合用于精细粒度的事件边界检测。为了克服这些限制，我们提出了JiTTER（拼图时序变压器事件重建），这是一种增强基于变压器的声音事件检测的时间建模能力的SSL框架。JiTTER引入了一种层次化的随机时间重排重构策略，其中音频序列在块级和帧级上随机打乱，强迫模型恢复正确的顺序。这种预训练目标鼓励模型学习全局事件结构以及瞬态细节，从而提高其识别具有锐利起止特征的声音事件的能力。此外，在块重组过程中我们注入噪声，提供了一种细微的扰动机制以进一步正则化特征学习并增强模型鲁棒性。DESED数据集上的实验结果表明JiTTER优于MAT-SED，PSDS值提高了5.89%，这突出了显式时间推理在基于SSL的声音事件检测中的有效性。我们的研究结果表明结构化的时序重建任务比简单的掩码预测更适合声音事件表示学习的自监督预训练范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sound event detection (SED) has significantly benefited from self-supervisedlearning (SSL) approaches, particularly masked audio transformer for SED(MAT-SED), which leverages masked block prediction to reconstruct missing audiosegments. However, while effective in capturing global dependencies, maskedblock prediction disrupts transient sound events and lacks explicit enforcementof temporal order, making it less suitable for fine-grained event boundarydetection. To address these limitations, we propose JiTTER (Jigsaw TemporalTransformer for Event Reconstruction), an SSL framework designed to enhancetemporal modeling in transformer-based SED. JiTTER introduces a hierarchicaltemporal shuffle reconstruction strategy, where audio sequences are randomlyshuffled at both the block-level and frame-level, forcing the model toreconstruct the correct temporal order. This pretraining objective encouragesthe model to learn both global event structures and fine-grained transientdetails, improving its ability to detect events with sharp onset-offsetcharacteristics. Additionally, we incorporate noise injection during blockshuffle, providing a subtle perturbation mechanism that further regularizesfeature learning and enhances model robustness. Experimental results on theDESED dataset demonstrate that JiTTER outperforms MAT-SED, achieving a 5.89%improvement in PSDS, highlighting the effectiveness of explicit temporalreasoning in SSL-based SED. Our findings suggest that structured temporalreconstruction tasks, rather than simple masked prediction, offer a moreeffective pretraining paradigm for sound event representation learning.</description>
      <author>example@mail.com (Hyeonuk Nam, Yong-Hwa Park)</author>
      <guid isPermaLink="false">2502.20857v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>AMPLE: Event-Driven Accelerator for Mixed-Precision Inference of Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.21196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为AMPLE的FPGA加速器，用于改进图神经网络（GNN）在非欧几里得数据上的表现。&lt;h4&gt;背景&lt;/h4&gt;最近，由于其处理非欧几里得数据的性能，图神经网络受到了广泛关注。这些网络因其不规则的记忆访问模式而特别受益于自定义硬件架构，这种模式源于图形结构的稀疏性。&lt;h4&gt;目的&lt;/h4&gt;解决现有FPGA加速器中双缓冲机制的问题，并针对典型图形数据集中节点分布不规则的情况提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;采用事件驱动编程流程的新AMPLE FPGA加速器。开发了混合算术架构，使GNN推理可以以节点级别进行量化。实现了用于优化片外内存访问和最大化节点并行性的预取器。&lt;h4&gt;主要发现&lt;/h4&gt;在引用和社交媒体图数据集上进行了评估，结果表明，在与CPU和GPU对应方相比，平均速度分别提高了243倍和7.2倍。&lt;h4&gt;结论&lt;/h4&gt;采用事件驱动编程流程的AMPLE FPGA加速器显著提升了GNN推理的速度。该方法通过利用混合算术架构、节点级量化以及片外内存访问优化来解决现有FPGA加速器中存在的问题，并在大量图数据集上展示了优秀的性能改进。&lt;h4&gt;翻译&lt;/h4&gt;最近，由于其处理非欧几里得数据的出色表现，图神经网络（GNNs）引起了广泛的关注。这些网络得益于它们不规则的记忆访问模式，这源于图形结构的稀疏性，因此，对定制硬件架构特别有利。然而，现有的FPGA加速器受到双缓冲机制的限制，这种机制未能考虑典型图数据集中的节点分布不规则问题。为了应对这一挑战，我们提出了AMPLE（加速消息传递逻辑引擎），这是一个利用新的事件驱动编程流的新FPGA加速器。我们开发了一种混合算术架构，使GNN推理能够在节点级别进行量化。此外，还实现了用于优化片外内存访问和最大化节点并行性的预取器。在引用和社交媒体图数据集上进行了评估，这些数据集的节点数量从2K到700K不等，结果表明，在与CPU和GPU对应方相比时，平均速度分别提高了243倍和7.2倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have recently gained attention due to theirperformance on non-Euclidean data. The use of custom hardware architecturesproves particularly beneficial for GNNs due to their irregular memory accesspatterns, resulting from the sparse structure of graphs. However, existing FPGAaccelerators are limited by their double buffering mechanism, which doesn'taccount for the irregular node distribution in typical graph datasets. Toaddress this, we introduce \textbf{AMPLE} (Accelerated Message Passing LogicEngine), an FPGA accelerator leveraging a new event-driven programming flow. Wedevelop a mixed-arithmetic architecture, enabling GNN inference to be quantizedat a node-level granularity. Finally, prefetcher for data and instructions isimplemented to optimize off-chip memory access and maximize node parallelism.Evaluation on citation and social media graph datasets ranging from $2$K to$700$K nodes showed a mean speedup of $243\times$ and $7.2\times$ against CPUand GPU counterparts, respectively.</description>
      <author>example@mail.com (Pedro Gimenes, Yiren Zhao, George Constantinides)</author>
      <guid isPermaLink="false">2502.21196v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Incorporating Long-Range Interactions via the Multipole Expansion into Ground and Excited-State Molecular Simulations</title>
      <link>http://arxiv.org/abs/2502.21045v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了FieldMACE，这是基于消息传递原子簇扩展（MACE）架构的一种改进版本，通过引入多极展开来更高效地模拟长程相互作用。&lt;h4&gt;背景&lt;/h4&gt;在分子机器学习势能中，准确捕捉大空间区域内的相互作用是一个重要挑战。&lt;h4&gt;目的&lt;/h4&gt;为了提高对环境和远距离效应的建模效率，特别是在基态和激发态下，本文提出了一种新的架构FieldMACE。&lt;h4&gt;方法&lt;/h4&gt;通过将多极展开集成到MACE架构中，形成一种新框架，称为FieldMACE。&lt;h4&gt;主要发现&lt;/h4&gt;基准评估显示FieldMACE在预测精度、计算效率方面优于先前的架构，并且能够准确模拟非绝热激发态动力学。&lt;h4&gt;结论&lt;/h4&gt;从基础模型中的迁移学习进一步提高了数据利用效率，使FieldMACE成为大规模分子模拟中可扩展、稳健和可转移的框架。&lt;h4&gt;翻译&lt;/h4&gt;模拟长程相互作用一直是分子机器学习势能的重要挑战。本文介绍了一种新的架构FieldMACE，它通过将多极展开整合到消息传递原子簇扩展（MACE）架构中来更高效地建模长程相互作用。FieldMACE能够有效捕捉环境和远距离效应，特别是在基态和激发态下。基准评估表明，与之前的架构相比，FieldMACE在预测准确性、计算效率方面具有优势，并且能准确模拟非绝热激发态动力学。此外，从基础模型中的迁移学习进一步提高了数据利用效率，使得FieldMACE成为一个可扩展的、稳健的以及可转移的大规模分子模拟框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulating long-range interactions remains a significant challenge formolecular machine learning potentials due to the need to accurately captureinteractions over large spatial regions. In this work, we introduce FieldMACE,an extension of the message-passing atomic cluster expansion (MACE)architecture that integrates the multipole expansion to model long-rangeinteractions more efficiently. By incorporating the multipole expansion,FieldMACE effectively captures environmental and long-range effects in bothground and excited states. Benchmark evaluations demonstrate its superiorperformance in predictions and computational efficiency compared to previousarchitectures, as well as its ability to accurately simulate nonadiabaticexcited-state dynamics. Furthermore, transfer learning from foundational modelsenhances data efficiency, making FieldMACE a scalable, robust, and transferableframework for large-scale molecular simulations.</description>
      <author>example@mail.com (Rhyan Barrett, Johannes C. B. Dietschreit, Julia Westermayr)</author>
      <guid isPermaLink="false">2502.21045v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>MESC-3D:Mining Effective Semantic Cues for 3D Reconstruction from a Single Image</title>
      <link>http://arxiv.org/abs/2502.20861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了MESC-3D，一种从单张图像中重建3D形状的新方法。&lt;h4&gt;背景&lt;/h4&gt;目前的方法主要集中在从图像中提取语义信息并将其简单地与3D点云连接起来，而没有进一步探索这种拼接后的语义特征。这些纠缠的语义特征显著阻碍了重建性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够主动挖掘有效语义线索以提高单张图像中的3D重建质量的方法。&lt;h4&gt;方法&lt;/h4&gt;设计了一个有效的语义挖掘模块和一个三维语义先验学习模块，前者建立了点云与图像语义属性之间的联系，后者利用先验知识增强模型的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在重建质量和鲁棒性方面显著优于先前的工作，并且具有强大的泛化能力，在零样本性能上也表现出色。&lt;h4&gt;结论&lt;/h4&gt;MESC-3D通过主动挖掘有效语义线索和使用三维语义先验知识，提高了单张图像中的3D重建的准确性和现实感。&lt;h4&gt;翻译&lt;/h4&gt;从单张图像中重建3D形状在计算机视觉领域扮演着重要角色。许多方法已经被提出并取得了显著的成绩。然而，现有的方法主要集中在提取图像中的语义信息，并简单地将其与3D点云连接起来而没有进一步探索这种拼接后的语义特征。这些纠缠的语义特征极大地阻碍了重建性能。本文提出了一个名为MESC-3D的新方法，它可以主动挖掘有效语义线索以改进单张图像的3D重建效果。具体而言，设计了一个有效的语义挖掘模块来建立点云和图像语义属性之间的联系，并使点云能够自主选择所需信息。此外，为了解决单一图像中的语义信息可能存在的不足问题（如遮挡），受人类利用日常经验中获得的先验知识表示3D对象能力的启发，我们引入了三维语义先验学习模块。此模块集成了对空间结构的语义理解，使模型能够更准确地解释和重建3D对象，并且在复杂3D环境感知方面更加贴近人类的认知。广泛的评估显示，与先前的工作相比，我们的方法在重建质量和鲁棒性方面取得了显著改进。此外，进一步的实验验证了该方法的强大泛化能力和零样本性能上的优越表现。代码可在https://github.com/QINGQINGLE/MESC-3D上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing 3D shapes from a single image plays an important role incomputer vision. Many methods have been proposed and achieve impressiveperformance. However, existing methods mainly focus on extracting semanticinformation from images and then simply concatenating it with 3D point cloudswithout further exploring the concatenated semantics. As a result, theseentangled semantic features significantly hinder the reconstructionperformance. In this paper, we propose a novel single-image 3D reconstructionmethod called Mining Effective Semantic Cues for 3D Reconstruction from aSingle Image (MESC-3D), which can actively mine effective semantic cues fromentangled features. Specifically, we design an Effective Semantic Mining Moduleto establish connections between point clouds and image semantic attributes,enabling the point clouds to autonomously select the necessary information.Furthermore, to address the potential insufficiencies in semantic informationfrom a single image, such as occlusions, inspired by the human ability torepresent 3D objects using prior knowledge drawn from daily experiences, weintroduce a 3D Semantic Prior Learning Module. This module incorporatessemantic understanding of spatial structures, enabling the model to interpretand reconstruct 3D objects with greater accuracy and realism, closely mirroringhuman perception of complex 3D environments. Extensive evaluations show thatour method achieves significant improvements in reconstruction quality androbustness compared to prior works. Additionally, further experiments validatethe strong generalization capabilities and excels in zero-shot preformance onunseen classes. Code is available at https://github.com/QINGQINGLE/MESC-3D.</description>
      <author>example@mail.com (Shaoming Li, Qing Cai, Songqi Kong, Runqing Tan, Heng Tong, Shiji Qiu, Yongguo Jiang, Zhi Liu)</author>
      <guid isPermaLink="false">2502.20861v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models -- A Panacea for Artificial Intelligence in Pathology?</title>
      <link>http://arxiv.org/abs/2502.21264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  50 pages, 15 figures and an appendix (study protocol) which is  previously published, see https://doi.org/10.1101/2024.07.04.24309948&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了基于自监督预训练的大型基础模型（FMs）在前列腺癌诊断和Gleason分级任务中的临床应用效果，发现尽管这些模型在数据稀缺的情况下表现出一定的实用性，但在充分标注的数据集上其性能可能不及针对特定任务训练的模型。&lt;h4&gt;背景&lt;/h4&gt;人工智能在病理学中的角色已从辅助诊断发展到揭示整体切片图像（WSIs）中预测形态模式。近年来，基于自监督预训练的基础模型被广泛推崇为适用于各种下游任务的通用解决方案。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过大规模验证AI系统，在前列腺癌诊断和Gleason分级任务上评估基础模型与特定任务端到端学习模型之间的性能差异。&lt;h4&gt;方法&lt;/h4&gt;研究使用超过10万名患者的7342例核心针活检数据，涵盖全球15个地点的11个国家。采用多实例学习框架对两种基础模型和一个完全端到端训练的任务特定模型进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果挑战了基础模型总是优于任务特定模型的观点。在缺乏标注数据的情况下，基础模型显示出一定优势；然而，在有足够的标记训练数据时，其性能与特有任务模型相匹配或甚至被超越。此外，特定于任务的培训显著减少了临床重要分级错误和难以识别形态的误诊，并降低了不同WSI扫描器间的变异性。&lt;h4&gt;结论&lt;/h4&gt;研究表明，尽管基础模型在快速原型设计和研究中具有明显优势，但它们作为适用于临床应用的医疗AI通用解决方案的角色仍然不确定。对于高风险的应用场景，严格的验证和对特定任务培训的关注仍然是至关重要的。研究者建议将基础模型与端到端学习的优点相结合，以实现适合临床使用的稳健且资源高效的病理学AI解决方案。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要描述了人工智能在病理诊断中的作用从辅助诊断到发现整个切片图像中预测性形态模式的发展历程，并讨论了基于自监督预训练的基础模型是否能成为适用于各种任务的通用解方案。研究结果表明，基础模型虽然在数据稀缺的情况下有其独特优势，但在大量标记训练数据下可能不及特定任务模型的表现。这项工作强调，在高风险临床应用中，必须进行严格的验证以确定最佳解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The role of artificial intelligence (AI) in pathology has evolved from aidingdiagnostics to uncovering predictive morphological patterns in whole slideimages (WSIs). Recently, foundation models (FMs) leveraging self-supervisedpre-training have been widely advocated as a universal solution for diversedownstream tasks. However, open questions remain about their clinicalapplicability and generalization advantages over end-to-end learning usingtask-specific (TS) models. Here, we focused on AI with clinical-gradeperformance for prostate cancer diagnosis and Gleason grading. We present thelargest validation of AI for this task, using over 100,000 core needle biopsiesfrom 7,342 patients across 15 sites in 11 countries. We compared two FMs with afully end-to-end TS model in a multiple instance learning framework. Ourfindings challenge assumptions that FMs universally outperform TS models. WhileFMs demonstrated utility in data-scarce scenarios, their performance convergedwith - and was in some cases surpassed by - TS models when sufficient labeledtraining data were available. Notably, extensive task-specific trainingmarkedly reduced clinically significant misgrading, misdiagnosis of challengingmorphologies, and variability across different WSI scanners. Additionally, FMsused up to 35 times more energy than the TS model, raising concerns about theirsustainability. Our results underscore that while FMs offer clear advantagesfor rapid prototyping and research, their role as a universal solution forclinically applicable medical AI remains uncertain. For high-stakes clinicalapplications, rigorous validation and consideration of task-specific trainingremain critically important. We advocate for integrating the strengths of FMsand end-to-end learning to achieve robust and resource-efficient AI pathologysolutions fit for clinical use.</description>
      <author>example@mail.com (Nita Mulliqi, Anders Blilie, Xiaoyi Ji, Kelvin Szolnoky, Henrik Olsson, Sol Erika Boman, Matteo Titus, Geraldine Martinez Gonzalez, Julia Anna Mielcarz, Masi Valkonen, Einar Gudlaugsson, Svein R. Kjosavik, José Asenjo, Marcello Gambacorta, Paolo Libretti, Marcin Braun, Radzislaw Kordek, Roman Łowicki, Kristina Hotakainen, Päivi Väre, Bodil Ginnerup Pedersen, Karina Dalsgaard Sørensen, Benedicte Parm Ulhøi, Pekka Ruusuvuori, Brett Delahunt, Hemamali Samaratunga, Toyonori Tsuzuki, Emilius A. M. Janssen, Lars Egevad, Martin Eklund, Kimmo Kartasalo)</author>
      <guid isPermaLink="false">2502.21264v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>On the Role of Individual Differences in Current Approaches to Computational Image Aesthetics</title>
      <link>http://arxiv.org/abs/2502.20518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个理论框架，用于解释在图像美学评估（IAA）任务中从通用模型转移到个人化模型的机制，并通过实验证明了不同群体和个体间存在的显著性能差异。&lt;h4&gt;背景&lt;/h4&gt;当前的图像美学评估方法分为两个阶段：第一阶段使用通用图像美学评估（GIAA）模型来估计平均分数，第二阶段利用转移学习的个性化图像美学评估（PIAA）模型来适应用户的主观性。然而，这种从GIAA到PIAA的理论理解仍不充分。&lt;h4&gt;目的&lt;/h4&gt;本文旨在建立一个理论基础，并提出了一种统一模型，该模型可以同时处理个体和群体评估任务。&lt;h4&gt;方法&lt;/h4&gt;本文提出了一个编码个人特征并在分布格式中表示的统一模型。实验通过不同的群体制样进行了验证，包括根据组大小进行子采样和分离的人口统计学变量。&lt;h4&gt;主要发现&lt;/h4&gt;1. 转移学习从GIAA到PIAA涉及外推，反之则是内插；2. 教育水平是影响美学差异的主要因素，其次是摄影艺术经验；3. 在艺术品中观察到了更强的个人主观性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的模型能够同时支持通用和个性化图像美学评估，并且可以提高在不同人口统计学群体中的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;图像美学评估（IAA）是一种复杂任务，由于图像多样性和用户主体性的存在而变得更为复杂。当前的方法将其分为两个阶段：第一阶段使用通用的图像美学评估模型来估计平均分数；第二阶段利用转移学习将GIAA适应为PIAA以融入用户的主观性。然而，缺乏关于在GIAA和PIAA之间进行转移学习时的理论理解，特别是在考虑群体构成、群体规模、个体之间的审美差异以及人口统计学相关性的背景下。本文提出了一个统一模型，该模型使用分布形式编码个人特征来进行个体及群体制评估。我们证明了从GIAA转移到PIAA涉及外推而相反则为内插，后者通常对机器学习更有益。通过对不同构成的群体进行实验（包括按组大小子采样和分离的人口统计学变量）发现，即使对于GIAA来说，性能也表现出显著变化，表明平均分数并不能完全消除个体主观性。性能差异分析以及基尼指数分析显示教育水平是影响审美差异的主要因素，其次是摄影及艺术经验，在艺术品中观察到更强的个人主体性。我们的模型独特地支持了通用和个性化图像美学评估，并提高了不同人口统计学群体中的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image aesthetic assessment (IAA) evaluates image aesthetics, a taskcomplicated by image diversity and user subjectivity. Current approachesaddress this in two stages: Generic IAA (GIAA) models estimate mean aestheticscores, while Personal IAA (PIAA) models adapt GIAA using transfer learning toincorporate user subjectivity. However, a theoretical understanding of transferlearning between GIAA and PIAA, particularly concerning the impact of groupcomposition, group size, aesthetic differences between groups and individuals,and demographic correlations, is lacking. This work establishes a theoreticalfoundation for IAA, proposing a unified model that encodes individualcharacteristics in a distributional format for both individual and groupassessments. We show that transferring from GIAA to PIAA involvesextrapolation, while the reverse involves interpolation, which is generallymore effective for machine learning. Experiments with varying groupcompositions, including sub-sampling by group size and disjoint demographics,reveal significant performance variation even for GIAA, indicating that meanscores do not fully eliminate individual subjectivity. Performance variationsand Gini index analysis reveal education as the primary factor influencingaesthetic differences, followed by photography and art experience, withstronger individual subjectivity observed in artworks than in photos. Our modeluniquely supports both GIAA and PIAA, enhancing generalization acrossdemographics.</description>
      <author>example@mail.com (Li-Wei Chen, Ombretta Strafforello, Anne-Sofie Maerten, Tinne Tuytelaars, Johan Wagemans)</author>
      <guid isPermaLink="false">2502.20518v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Human Beliefs about AI Behavior for Scalable Oversight</title>
      <link>http://arxiv.org/abs/2502.21262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  53 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了如何通过建模人类评估者的信念来改进对AI系统的监督，以解决随着AI能力提升而导致的人类反馈可靠性降低的问题。&lt;h4&gt;背景&lt;/h4&gt;当前的AI系统通常依赖于人类反馈来学习人类的价值观和偏好。然而，随着AI系统的能力增强，这种人类反馈变得越来越不可靠。&lt;h4&gt;目的&lt;/h4&gt;研究如何通过建模人的信念来提高对超出人类能力范围的AI系统的监督效率。&lt;h4&gt;方法&lt;/h4&gt;提出了形式化的模型来描述人对于AI行为的看法，并分析了这些模型在推断人类价值观中的作用。同时引入了一个放松版的人类信念模型覆盖概念，以减少依赖于精确信念模型的需求。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析和实验研究揭示了如何利用基础模型构建覆盖信念模型的潜力，为可扩展监督提供了新方法。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够帮助更好地理解人类反馈，并且提供了一种使用基础AI系统来提高监督效率的新途径。&lt;h4&gt;翻译&lt;/h4&gt;当代的人工智能（AI）对齐工作经常依赖于人类反馈来教导AI系统学习人类的价值观和偏好。然而，随着AI系统的功能增强，这种人类反馈变得越来越不可靠。这导致了一个可扩展监控的问题：如何监管超出人类能力的AI系统？在这项工作中，我们提出通过建模人评估者对于AI行为的看法来更好地解释人的反馈。我们将人类信念模型形式化，并从理论上分析它们在推断人类价值观中的作用。然后描述了这种推理中剩余的不确定性以及这些不确定性消失的情况条件。为了减少对精确信念模型的依赖，我们引入了一个放松版的人类信念模型覆盖概念。最后，我们建议使用基础模型来构建覆盖信念模型，为可扩展监督提供了一种新的潜在方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contemporary work in AI alignment often relies on human feedback to teach AIsystems human preferences and values. Yet as AI systems grow more capable,human feedback becomes increasingly unreliable. This raises the problem ofscalable oversight: How can we supervise AI systems that exceed humancapabilities? In this work, we propose to model the human evaluator's beliefsabout the AI system's behavior to better interpret the human's feedback. Weformalize human belief models and theoretically analyze their role in inferringhuman values. We then characterize the remaining ambiguity in this inferenceand conditions for which the ambiguity disappears. To mitigate reliance onexact belief models, we then introduce the relaxation of human belief modelcovering. Finally, we propose using foundation models to construct coveringbelief models, providing a new potential approach to scalable oversight.</description>
      <author>example@mail.com (Leon Lang, Patrick Forré)</author>
      <guid isPermaLink="false">2502.21262v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Dimension Agnostic Neural Processes</title>
      <link>http://arxiv.org/abs/2502.20661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, Accepted to ICLR 2025 (International Conference  on Learning Representations)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的元学习模型Dimension Agnostic Neural Processes(DANP)，该模型通过引入Dimension Aggregator Block(DAB)和Transformer架构，增强了处理不同维度输入的能力，并在各种回归任务上显示出优越性能。&lt;h4&gt;背景&lt;/h4&gt;传统的Neural Process(NP)方法虽然能够提取跨多种任务的数据共享特征并预测不确定性，但在适应不同输入维度的任务时面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种元学习模型DANP，以克服传统NP模型的局限性，并提升其在回归任务中的适用性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;引入Dimension Aggregator Block(DAB)将输入特征转换为固定维度的空间，同时采用Transformer架构和潜在编码层来学习更具普适性的特征。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证了DANP模型相比于现有NP变体在合成数据集与实际回归任务上的优越性。&lt;h4&gt;结论&lt;/h4&gt;DANP展示了其解决传统NP模型局限性和广泛适应各种回归场景的潜力，具有较高的实用价值。&lt;h4&gt;翻译&lt;/h4&gt;元学习的目标是训练可以使用有限标注数据推广到新任务的模型，通过提取多样任务数据集中的共享特征。此外，在训练和评估期间考虑预测不确定性，这是一个称为不确定感知元学习的概念。神经过程(NP)是一种著名的不确定感知元学习方法，它利用参数化神经网络构建隐式随机过程，使快速适应新任务成为可能。然而，现有的NP方法在处理多样输入维度和学习特征方面存在挑战，限制了它们在回归任务中的广泛应用性。为了克服这些局限并提高NP模型作为通用回归器的实用性，我们引入了Dimension Agnostic Neural Processes(DANP)。DANP采用Dimension Aggregator Block(DAB)，将输入特征转换为固定维度空间，增强模型处理多样数据集的能力；同时利用Transformer架构和潜在编码层，学习可跨多种任务泛化的更广泛特征。通过在各种合成和实际回归任务上的综合实验，我们实证显示了DANP优于先前的NP变体，在克服传统NP模型局限性方面展示出其有效性及其应用于多样化回归场景的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning aims to train models that can generalize to new tasks withlimited labeled data by extracting shared features across diverse taskdatasets. Additionally, it accounts for prediction uncertainty during bothtraining and evaluation, a concept known as uncertainty-aware meta-learning.Neural Process(NP) is a well-known uncertainty-aware meta-learning method thatconstructs implicit stochastic processes using parametric neural networks,enabling rapid adaptation to new tasks. However, existing NP methods facechallenges in accommodating diverse input dimensions and learned features,limiting their broad applicability across regression tasks. To address theselimitations and advance the utility of NP models as general regressors, weintroduce Dimension Agnostic Neural Processes(DANP). DANP incorporatesDimension Aggregator Block(DAB) to transform input features into afixed-dimensional space, enhancing the model's ability to handle diversedatasets. Furthermore, leveraging the Transformer architecture and latentencoding layers, DANP learns a wider range of features that are generalizableacross various tasks. Through comprehensive experimentation on varioussynthetic and practical regression tasks, we empirically show that DANPoutperforms previous NP variations, showcasing its effectiveness in overcomingthe limitations of traditional NP models and its potential for broaderapplicability in diverse regression scenarios.</description>
      <author>example@mail.com (Hyungi Lee, Chaeyun Jang, Dongbok Lee, Juho Lee)</author>
      <guid isPermaLink="false">2502.20661v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Parallel-Learning of Invariant and Tempo-variant Attributes of Single-Lead Cardiac Signals: PLITA</title>
      <link>http://arxiv.org/abs/2502.21162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in The 39th Annual AAAI Conference on Artificial  Intelligence. Main Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的自监督学习方法PLITA，用于捕捉单导联心电图（ECG）信号中的不变和时变属性。&lt;h4&gt;背景&lt;/h4&gt;穿戴式传感设备在未来的数字健康领域中将发挥重要作用。目前的自监督学习方法只能编码不变属性，忽略了反映状态变化的时间变异信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕捉时间不变和时间变异心电图特征的新SSL方法。&lt;h4&gt;方法&lt;/h4&gt;通过强制相邻时间点输入的空间表示更加接近来捕获时变属性。&lt;h4&gt;主要发现&lt;/h4&gt;PLITA在时间变异属性起重要作用的设置中表现出显著更好的性能。&lt;h4&gt;结论&lt;/h4&gt;PLITA是一种有效的自监督学习框架，能够有效处理心电图信号中的不变和时变信息。&lt;h4&gt;翻译&lt;/h4&gt;可穿戴传感设备如Holter监护仪将在未来的数字健康领域扮演关键角色。无监督的学习框架（例如自我监督学习）对于将这些单导联的心电信号映射到预期的临床结果至关重要。这种信号具有时间变异成分，其模式随记录过程而演变，并且还存在不变成分，其模式保持不变。然而，现有的SSL方法只能驱动模型编码不变属性，导致模型忽略反映状态变化的时间变异信息。本文介绍了一种新的SSL方法——并行学习不变和时变属性（PLITA），该方法旨在捕捉这两种类型的心电图特征。通过强制相邻时间点输入的空间表示更加接近来捕获时变属性。我们评估了此方法在学习两种不同类型的特征方面的能力，以及与现有ECG分析的SSL方法相比的性能表现。在时间变异属性起重要作用的情况下，PLITA表现出显著更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wearable sensing devices, such as Holter monitors, will play a crucial rolein the future of digital health. Unsupervised learning frameworks such asSelf-Supervised Learning (SSL) are essential to map these single-leadelectrocardiogram (ECG) signals with their anticipated clinical outcomes. Thesesignals are characterized by a tempo-variant component whose patterns evolvethrough the recording and an invariant component with patterns that remainunchanged. However, existing SSL methods only drive the model to encode theinvariant attributes, leading the model to neglect tempo-variant informationwhich reflects subject-state changes through time. In this paper, we presentParallel-Learning of Invariant and Tempo-variant Attributes (PLITA), a novelSSL method designed for capturing both invariant and tempo-variant ECGattributes. The latter are captured by mandating closer representations inspace for closer inputs on time. We evaluate both the capability of the methodto learn the attributes of these two distinct kinds, as well as PLITA'sperformance compared to existing SSL methods for ECG analysis. PLITA performssignificantly better in the set-ups where tempo-variant attributes play a majorrole.</description>
      <author>example@mail.com (Adtian Atienza, Jakob E. Bardram, Sadasivan Puthusserypady)</author>
      <guid isPermaLink="false">2502.21162v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Continuous Adversarial Text Representation Learning for Affective Recognition</title>
      <link>http://arxiv.org/abs/2502.20613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, The 7th International Conference on Artificial  Intelligence in Information and Communication (ICAIIC 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '提出了一种增强情感感知嵌入的框架，旨在改进基于变压器模型的情感识别能力。', '背景': '预训练语言模型在语义理解方面表现出色，但在捕捉细微的情感信息方面存在困难。', '目的': '通过引入连续的效价唤醒标签系统和动态令牌扰动机制来提高情感敏感性。', '方法': '采用连续的valence-arousal标注体系进行对比学习，并利用基于梯度的方法强调与情感相关的令牌。', '主要发现': '实验结果表明，该框架在情绪分类基准上比现有方法平均提高了15.5%。', '结论': '所提出的框架有效增强了情感表示的学习能力，并能实现精确且上下文相关的情感理解。', '翻译': '摘要原文的中文翻译。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While pre-trained language models excel at semantic understanding, they oftenstruggle to capture nuanced affective information critical for affectiverecognition tasks. To address these limitations, we propose a novel frameworkfor enhancing emotion-aware embeddings in transformer-based models. Ourapproach introduces a continuous valence-arousal labeling system to guidecontrastive learning, which captures subtle and multi-dimensional emotionalnuances more effectively. Furthermore, we employ a dynamic token perturbationmechanism, using gradient-based saliency to focus on sentiment-relevant tokens,improving model sensitivity to emotional cues. The experimental resultsdemonstrate that the proposed framework outperforms existing methods, achievingup to 15.5% improvement in the emotion classification benchmark, highlightingthe importance of employing continuous labels. This improvement demonstratesthat the proposed framework is effective in affective representation learningand enables precise and contextually relevant emotional understanding.</description>
      <author>example@mail.com (Seungah Son, Andrez Saurez, Dongsoo Har)</author>
      <guid isPermaLink="false">2502.20613v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Subtask-Aware Visual Reward Learning from Segmented Demonstrations</title>
      <link>http://arxiv.org/abs/2502.20630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project webpage: https://changyeon.site/reds/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;强化学习（RL）代理在各种机器人任务中展示了其潜力。然而，它们仍然严重依赖于人工设计的奖励函数，并且需要大量的试错以及目标行为信息，在现实世界的应用场景中这些信息往往是不可用的。本文提出了REDs：一种从演示视频片段中学习回报的新框架，该框架利用无动作标记视频进行最小监督的学习。具体而言，REDs使用来自不同来源的动作分段视频并将它们视为真实奖励信号。我们训练一个基于视频片段和相应子任务的密集奖励函数，并通过最小化等价策略不变比较距离来确保与真实奖励信号对齐。此外，我们采用对比学习目标以使视频表示与子任务保持一致，在线交互时可以实现精确的子任务推理。实验表明，REDs在Meta-World中的复杂机器人操作任务以及FurnitureBench中的家具组装等更具挑战性的现实世界任务中显著优于基线方法，并且只需要最小的人工干预。&lt;h4&gt;背景&lt;/h4&gt;强化学习代理已经在多种机器人任务上展示了其潜力，但这些代理依然严重依赖于人工设计的奖励函数。这需要大量的试错过程和对目标行为信息的访问，在许多实际应用场合下这类信息是难以获得的。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的从演示中无监督地学习回报的方法REDs，该方法能够利用来自不同来源的视频演示片段进行训练，并且只需要最小的人工干预。&lt;h4&gt;方法&lt;/h4&gt;1. REDs使用动作标记的视频作为输入并将其分割成子任务。2. 利用这些子任务和它们对应的视频段来学习密集奖励函数。3. 通过优化特定的目标函数（即等价策略不变比较距离）以确保奖励信号与真实信号一致。4. 使用对比学习目标使视频表示与子任务保持一致，从而在在线交互中实现精确的子任务推理。&lt;h4&gt;主要发现&lt;/h4&gt;REDs框架能够在Meta-World中的复杂机器人操作任务以及家具组装等更具挑战性的现实世界任务上表现出色，并且显著优于基线方法。此外，在最小的人工干预下该模型还能够推广到未知的任务和机器人的实例中，展示出其在多变环境下的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;REDs是一种高效的学习框架，它通过利用视频演示片段中的信息来减轻对人工设计奖励函数的依赖，并且能够在多种机器人操作任务上表现出色。此外，REDs显示出良好的泛化能力，这表明它具有广泛的应用潜力和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) agents have demonstrated their potential acrossvarious robotic tasks. However, they still heavily rely on human-engineeredreward functions, requiring extensive trial-and-error and access to targetbehavior information, often unavailable in real-world settings. This paperintroduces REDS: REward learning from Demonstration with Segmentations, a novelreward learning framework that leverages action-free videos with minimalsupervision. Specifically, REDS employs video demonstrations segmented intosubtasks from diverse sources and treats these segments as ground-truthrewards. We train a dense reward function conditioned on video segments andtheir corresponding subtasks to ensure alignment with ground-truth rewardsignals by minimizing the Equivalent-Policy Invariant Comparison distance.Additionally, we employ contrastive learning objectives to align videorepresentations with subtasks, ensuring precise subtask inference during onlineinteractions. Our experiments show that REDS significantly outperforms baselinemethods on complex robotic manipulation tasks in Meta-World and morechallenging real-world tasks, such as furniture assembly in FurnitureBench,with minimal human intervention. Moreover, REDS facilitates generalization tounseen tasks and robot embodiments, highlighting its potential for scalabledeployment in diverse environments.</description>
      <author>example@mail.com (Changyeon Kim, Minho Heo, Doohyun Lee, Jinwoo Shin, Honglak Lee, Joseph J. Lim, Kimin Lee)</author>
      <guid isPermaLink="false">2502.20630v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Dynamically Local-Enhancement Planner for Large-Scale Autonomous Driving</title>
      <link>http://arxiv.org/abs/2502.21134v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了Dynamically Local-Enhancement (DLE) Planner，一种在不永久修改基本驾驶规划器的情况下通过局部驾驶数据动态增强驾驶规划的技术。&lt;h4&gt;背景&lt;/h4&gt;当前自主车辆主要限于特定区域运行，但对更广泛的应用需求日益增长。随着模型规模的扩大，有限的容量成为适应新场景的重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来解决单个庞大模型在处理新情况时效率低下的问题，通过局部驾驶数据动态增强基本驾驶规划器以提高自主驾驶系统的可扩展性而不显著增加规划器的大小。&lt;h4&gt;方法&lt;/h4&gt;引入了位置变化的马尔科夫决策过程(MDP)与图神经网络结合使用的方法，从本地观察数据中提取特定区域的驾驶特征，并利用学习到的特征增强基于强化学习的基本策略。&lt;h4&gt;主要发现&lt;/h4&gt;在多个场景下评估该方法并与适用于所有情况的单一驾驶模型比较后，结果显示本方法在安全性和平均奖励方面都优于基准策略，同时保持较低的规模。&lt;h4&gt;结论&lt;/h4&gt;这种技术有潜力使大规模自主车辆受益，无需大幅扩展设备上的驾驶模型。&lt;h4&gt;翻译&lt;/h4&gt;当前自主车辆主要限于特定区域运行。随着对更广泛应用的需求增加，现有模型在处理新场景时表现出容量限制问题。单个庞大模型难以适应新的情况。本文提出Dynamically Local-Enhancement (DLE) Planner，该方法通过局部驾驶数据动态增强基本驾驶规划器，不需永久修改其本身。通过位置变化的马尔科夫决策过程结合图神经网络从本地观察数据中提取区域特定的驾驶特征，使用这些特征来增强基于强化学习的基本策略。实验结果表明，在安全性和平均奖励方面优于基准模型，并保持较小规模，具备大规模自主车辆应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current autonomous vehicles operate primarily within limited regions, butthere is increasing demand for broader applications. However, as models scale,their limited capacity becomes a significant challenge for adapting to novelscenarios. It is increasingly difficult to improve models for new situationsusing a single monolithic model. To address this issue, we introduce theconcept of dynamically enhancing a basic driving planner with local drivingdata, without permanently modifying the planner itself. This approach, termedthe Dynamically Local-Enhancement (DLE) Planner, aims to improve thescalability of autonomous driving systems without significantly expanding theplanner's size. Our approach introduces a position-varying Markov DecisionProcess formulation coupled with a graph neural network that extractsregion-specific driving features from local observation data. The learnedfeatures describe the local behavior of the surrounding objects, which is thenleveraged to enhance a basic reinforcement learning-based policy. We evaluatedour approach in multiple scenarios and compared it with a one-for-all drivingmodel. The results show that our method outperforms the baseline policy in bothsafety (collision rate) and average reward, while maintaining a lighter scale.This approach has the potential to benefit large-scale autonomous vehicleswithout the need for largely expanding on-device driving models.</description>
      <author>example@mail.com (Nanshan Deng, Weitao Zhou, Bo Zhang, Junze Wen, Kun Jiang, Zhong Cao, Diange Yang)</author>
      <guid isPermaLink="false">2502.21134v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>CuPID: Leveraging Masked Single-Lead ECG Modelling for Enhancing the Representations</title>
      <link>http://arxiv.org/abs/2502.21127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;文章介绍了一种新型的Masked Data Modelling (MDM) 方法CuPID，该方法专门针对单导联ECG数据设计，通过提供频谱图上下文信息来增强现有MDM技术。&lt;h4&gt;背景&lt;/h4&gt;穿戴式传感设备如心电图(ECG)心率监测器将在数字健康领域发挥重要作用。这种持续监控导致了大量的未标注数据，促进了无监督学习框架的发展。&lt;h4&gt;目的&lt;/h4&gt;开发一种适用于单导联ECG的无监督学习方法，克服现有MDM技术在处理不规则心跳间隔时的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种称为Cueing the Predictor Increments the Detailing (CuPID) 的新方法。该方法通过向解码器提供频谱图上下文信息来改善现有的MDM技术。&lt;h4&gt;主要发现&lt;/h4&gt;CuPID 方法在编码器性能上有了显著的提升，特别是在各种不同的配置下。此外，在多个下游任务中，CuPID 表现超过了现有最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;CuPID是一种有效的改进MDM技术的方法，特别适用于处理单导联ECG数据，并且在广泛的配置和应用中优于现有的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;可穿戴传感设备，如心电图(ECG) 心率监测器，在数字健康领域将扮演重要角色。持续的监控导致了大量的未标记数据，促使开发无监督学习框架的需求。尽管Masked Data Modelling (MDM) 技术已广泛使用，但直接应用于单导联ECG 数据的效果不佳，因为解码器在没有上下文信息的情况下难以处理不规则的心跳间隔。本文提出了一种称为Cueing the Predictor Increments the Detailing (CuPID) 的新型MDM方法，专门针对单导联ECG 设计。通过向解码器提供由频谱图派生的上下文，CuPID 增强了现有的MDM 技术，从而激励编码器生成更详细的表示。这极大地影响了编码器在各种不同配置下的性能表现，使得CuPID 在多种下游任务中超越了现有最先进技术的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wearable sensing devices, such as Electrocardiogram (ECG) heart-ratemonitors, will play a crucial role in the future of digital health. Thiscontinuous monitoring leads to massive unlabeled data, incentivizing thedevelopment of unsupervised learning frameworks. While Masked Data Modelling(MDM) techniques have enjoyed wide use, their direct application to single-leadECG data is suboptimal due to the decoder's difficulty handling irregularheartbeat intervals when no contextual information is provided. In this paper,we present Cueing the Predictor Increments the Detailing (CuPID), a novel MDMmethod tailored to single-lead ECGs. CuPID enhances existing MDM techniques bycueing spectrogram-derived context to the decoder, thus incentivizing theencoder to produce more detailed representations. This has a significant impacton the encoder's performance across a wide range of different configurations,leading CuPID to outperform state-of-the-art methods in a variety of downstreamtasks.</description>
      <author>example@mail.com (Adtian Atienza, Gouthamaan Manimaran, Jakob E. Bardram, Sadasivan Puthusserypady)</author>
      <guid isPermaLink="false">2502.21127v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Few-Shot, No Problem: Descriptive Continual Relation Extraction</title>
      <link>http://arxiv.org/abs/2502.20596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于检索的解决方案来解决少样本持续关系抽取问题，该方案通过大语言模型生成关系描述，并利用双编码器检索训练范式增强样本和类别表示学习。&lt;h4&gt;背景&lt;/h4&gt;传统的内存基础方法在面对有限样本时容易过拟合，无法巩固旧知识，在少样本场景中数据稀疏进一步阻碍了有效的隐空间数据增强。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的方法来解决少样本持续关系抽取中的挑战，并保持模型在一系列任务上的鲁棒性能，同时减少灾难性遗忘问题。&lt;h4&gt;方法&lt;/h4&gt;首先使用大语言模型为每个关系生成描述；然后引入双编码器检索训练范式以丰富样本和类别表示学习；最后设计基于检索的预测方法，其中每个样本通过整合关系描述向量和类原型的反向排名融合得分来“检索”最佳匹配的关系。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的广泛实验表明该方法显著地提高了性能，并且在整个顺序任务中保持了鲁棒性，有效地解决了灾难性遗忘问题。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为解决少样本持续关系抽取的挑战提供了一种有效的解决方案，通过利用增强表示来促进模型学习和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot Continual Relation Extraction is a crucial challenge for enabling AIsystems to identify and adapt to evolving relationships in dynamic real-worlddomains. Traditional memory-based approaches often overfit to limited samples,failing to reinforce old knowledge, with the scarcity of data in few-shotscenarios further exacerbating these issues by hindering effective dataaugmentation in the latent space. In this paper, we propose a novelretrieval-based solution, starting with a large language model to generatedescriptions for each relation. From these descriptions, we introduce abi-encoder retrieval training paradigm to enrich both sample and classrepresentation learning. Leveraging these enhanced representations, we design aretrieval-based prediction method where each sample "retrieves" the bestfitting relation via a reciprocal rank fusion score that integrates bothrelation description vectors and class prototypes. Extensive experiments onmultiple datasets demonstrate that our method significantly advances thestate-of-the-art by maintaining robust performance across sequential tasks,effectively addressing catastrophic forgetting.</description>
      <author>example@mail.com (Nguyen Xuan Thanh, Anh Duc Le, Quyen Tran, Thanh-Thien Le, Linh Ngo Van, Thien Huu Nguyen)</author>
      <guid isPermaLink="false">2502.20596v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Best Foot Forward: Robust Foot Reconstruction in-the-wild</title>
      <link>http://arxiv.org/abs/2502.20511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;准确的3D脚部重建对于个性化矫形器、数字医疗和虚拟试穿至关重要。&lt;h4&gt;背景&lt;/h4&gt;现有的方法在处理不完整的扫描数据以及解剖变异时遇到困难，特别是在用户移动受限的情况下（例如自我扫描场景）难以捕捉到像足弓和后跟这样的区域。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的端到端管道来改进结构从运动（SfM）重建过程。&lt;h4&gt;方法&lt;/h4&gt;该方法首先使用SE(3)正则化结合视角预测模块解决扫描对齐的不确定性，然后通过基于注意力机制的网络训练在合成增强点云上的几何补充缺失部分。&lt;h4&gt;主要发现&lt;/h4&gt;该技术实现了同类最佳性能，同时保持了临床验证的解剖学精确度。通过结合合成数据与学习到的几何先验知识，使足部重建能够适应真实世界捕捉条件下的各种情况。&lt;h4&gt;结论&lt;/h4&gt;此方法为基于移动设备的3D扫描在医疗和零售领域的应用开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;准确的三维脚部重建对于个性化矫形器、数字健康护理以及虚拟试穿至关重要。然而，现有的技术难以应对不完整扫描及解剖变异的问题，在自我扫描等情况下尤其困难，因为用户移动受限影响对足弓和后跟等区域的捕捉。我们提出了一种新颖的端到端流程来改进结构从运动重建过程，首先通过SE(3)正则化结合视角预测模块解决扫描对齐问题，再利用基于注意力机制训练在合成增强点云上的网络补充缺失几何部分。此方法实现了同类最佳性能，并保持了临床验证的解剖精确度。借助合成数据和学习到的几何先验知识，在真实世界捕捉条件下实现稳健的脚部重建。这为医疗与零售领域中基于移动设备的3D扫描应用开辟了新的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D foot reconstruction is crucial for personalized orthotics,digital healthcare, and virtual fittings. However, existing methods strugglewith incomplete scans and anatomical variations, particularly in self-scanningscenarios where user mobility is limited, making it difficult to capture areaslike the arch and heel. We present a novel end-to-end pipeline that refinesStructure-from-Motion (SfM) reconstruction. It first resolves scan alignmentambiguities using SE(3) canonicalization with a viewpoint prediction module,then completes missing geometry through an attention-based network trained onsynthetically augmented point clouds. Our approach achieves state-of-the-artperformance on reconstruction metrics while preserving clinically validatedanatomical fidelity. By combining synthetic training data with learnedgeometric priors, we enable robust foot reconstruction under real-world captureconditions, unlocking new opportunities for mobile-based 3D scanning inhealthcare and retail.</description>
      <author>example@mail.com (Kyle Fogarty, Jing Yang, Chayan Kumar Patodi, Aadi Bhanti, Steven Chacko, Cengiz Oztireli, Ujwal Bonde)</author>
      <guid isPermaLink="false">2502.20511v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Information Bottleneck-Guided Heterogeneous Graph Learning for Interpretable Neurodevelopmental Disorder Diagnosis</title>
      <link>http://arxiv.org/abs/2502.20769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的框架I2B-HGNN，用于从神经发育障碍中提取有意义的生物标志物，并进行诊断。&lt;h4&gt;背景&lt;/h4&gt;现有的机器学习模型在提供综合可解释性方面存在挑战，尤其是在处理复杂的数据编码、解码和融合时。这些模型往往难以从成像数据（如fMRI）中抽取有用的生物标记物，也缺乏解释非成像数据重要性的机制。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以诊断神经发育障碍的可解释机器学习框架，该框架能够有效地利用成像与非成像多模态数据并提供清晰的结果解释。&lt;h4&gt;方法&lt;/h4&gt;提出了Interpretable Information Bottleneck Heterogeneous Graph Neural Network (I2B-HGNN)，包括两个关键模块：Information Bottleneck Graph Transformer (IBGraphFormer) 和 Information Bottleneck Heterogeneous Graph Attention Network (IB-HGAN)。IBGraphFormer用于局部模式，通过脑连接图约束的图神经网络进行全局建模并利用信息瓶颈指导的聚类提取生物标记物；IB-HGAN则用于全球多模态互动，使用异构图神经网络实现可解释的多模态融合。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，I2B-HGNN在诊断神经发育障碍方面表现出高精度，并能提供清晰的生物标志物识别和有效的非成像数据分析。&lt;h4&gt;结论&lt;/h4&gt;I2B-HGNN框架是解决当前机器学习模型面临的挑战的有效解决方案，它不仅能够提高诊断准确度，还能通过详细的解释帮助理解疾病特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing interpretable models for diagnosing neurodevelopmental disorders(NDDs) is highly valuable yet challenging, primarily due to the complexity ofencoding, decoding and integrating imaging and non-imaging data. Many existingmachine learning models struggle to provide comprehensive interpretability,often failing to extract meaningful biomarkers from imaging data, such asfunctional magnetic resonance imaging (fMRI), or lacking mechanisms to explainthe significance of non-imaging data. In this paper, we propose theInterpretable Information Bottleneck Heterogeneous Graph Neural Network(I2B-HGNN), a novel framework designed to learn from fine-grained localpatterns to comprehensive global multi-modal interactions. This frameworkcomprises two key modules. The first module, the Information Bottleneck GraphTransformer (IBGraphFormer) for local patterns, integrates global modeling withbrain connectomic-constrained graph neural networks to identify biomarkersthrough information bottleneck-guided pooling. The second module, theInformation Bottleneck Heterogeneous Graph Attention Network (IB-HGAN) forglobal multi-modal interactions, facilitates interpretable multi-modal fusionof imaging and non-imaging data using heterogeneous graph neural networks. Theresults of the experiments demonstrate that I2B-HGNN excels in diagnosing NDDswith high accuracy, providing interpretable biomarker identification andeffective analysis of non-imaging data.</description>
      <author>example@mail.com (Yueyang Li, Lei Chen, Wenhao Dong, Shengyu Gong, Zijian Kang, Boyang Wei, Weiming Zeng, Hongjie Yan, Lingbin Bian, Wai Ting Siok, Nizhuan Wang)</author>
      <guid isPermaLink="false">2502.20769v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Learning for Just-In-Time Software Defect Prediction in Autonomous Driving Systems</title>
      <link>http://arxiv.org/abs/2502.20806v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种使用多模态学习的即时软件缺陷预测（JIT-SDP）方法，以提高自动驾驶软件系统的可靠性和安全性。&lt;h4&gt;背景&lt;/h4&gt;近年来，随着自主驾驶技术的发展，可靠的软件对于确保安全和性能变得至关重要。&lt;h4&gt;目的&lt;/h4&gt;通过利用预训练变换器和组合模块处理多种数据模式来实现即时的软件缺陷预测。&lt;h4&gt;方法&lt;/h4&gt;该模型采用多模态变换器，其中包含针对文本、数值和分类等不同数据模式之间的注意机制。在组合模块中，将基于文本数据和包含分类及数值数据的表格特征的变压器模型输出进行结合以生成预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，相对于现有的深度学习和机器学习模型，在三个开源自动驾驶系统软件项目上该方法显著提高了评估指标的表现。&lt;h4&gt;结论&lt;/h4&gt;通过改善缺陷预测能力，多模态学习在提高自动驾驶软件系统的可靠性和安全性方面具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了利用预训练的变换器及组合模块处理包括代码特征、更改度量和上下文信息等多种数据模式的一种即时软件缺陷预测方法。该模型采用注意机制将不同形式的数据（如文本，数值，分类等）结合在一起，并在GitHub上收集三个开源自动驾驶系统项目的实验中证明其优于现有深度学习与机器学习模型的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the rise of autonomous driving technologies has highlightedthe critical importance of reliable software for ensuring safety andperformance. This paper proposes a novel approach for just-in-time softwaredefect prediction (JIT-SDP) in autonomous driving software systems usingmultimodal learning. The proposed model leverages the multimodal transformersin which the pre-trained transformers and a combining module deal with themultiple data modalities of the software system datasets such as code features,change metrics, and contextual information. The key point for adaptingmultimodal learning is to utilize the attention mechanism between the differentdata modalities such as text, numerical, and categorical. In the combiningmodule, the output of a transformer model on text data and tabular featurescontaining categorical and numerical data are combined to produce thepredictions using the fully connected layers. Experiments conducted on threeopen-source autonomous driving system software projects collected from theGitHub repository (Apollo, Carla, and Donkeycar) demonstrate that the proposedapproach significantly outperforms state-of-the-art deep learning and machinelearning models regarding evaluation metrics. Our findings highlight thepotential of multimodal learning to enhance the reliability and safety ofautonomous driving software through improved defect prediction.</description>
      <author>example@mail.com (Faisal Mohammad, Duksan Ryu)</author>
      <guid isPermaLink="false">2502.20806v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>TimesBERT: A BERT-Style Foundation Model for Time Series Understanding</title>
      <link>http://arxiv.org/abs/2502.21245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列理解包括分类、填充和异常检测等任务，现有的BERT风格架构在这些领域尚未完全解锁。&lt;h4&gt;背景&lt;/h4&gt;时间序列分析在多种场景中非常重要。虽然GPT风格模型已经作为时间序列预测的基础模型被广泛应用，但基于自然语言理解取得重大进展的BERT风格架构还未充分利用于时间序列理解。&lt;h4&gt;目的&lt;/h4&gt;设计一种名为TimesBERT的新方法，旨在学习时间序列中的通用表示形式，并解决多粒度结构的问题。&lt;h4&gt;方法&lt;/h4&gt;受到多元时间序列和多句子文档共享的多粒度结构启发，提出了TimeBERT模型。除了自然地采用掩码建模外，还提出了一种并行的任务——功能令牌预测任务来体现重要的多粒度结构。&lt;h4&gt;主要发现&lt;/h4&gt;TimesBERT在涵盖四个典型下游理解任务的数据集上取得了最先进的性能，并超越了特定任务的模型和语言预训练骨干网络，被定位为时间序列理解的基础模型。&lt;h4&gt;结论&lt;/h4&gt;TimesBERT通过利用多粒度表示形式，在多个领域的时间序列分析中表现出了卓越的能力，展示了其作为通用基础模型在时间序列理解中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;时间序列分析至关重要。除了预测任务之外，许多实际应用包括分类、填充和异常检测，这些都归结为不同的能力术语即本文所说的时间序列理解。虽然GPT风格的模型被定位为基础模型用于时间序列预测，但基于自然语言理解取得重大进展的BERT架构在时间序列理解方面尚未完全解锁。受到多元时间序列与多句子文档共享的多层次结构启发，我们设计了TimesBERT来学习包括时间模式和变量特性在内的通用时间序列表示形式。除了自然适应掩码建模之外，还提出了一种功能令牌预测任务以体现重要的多层次结构。我们的模型在涵盖各种领域的260亿个时间点上进行了预训练，并利用多层次表示，在四个典型下游理解任务中取得了最先进的性能，优于特定任务的模型和语言预训练骨干网络，被定位为时间序列理解的基础模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series analysis is crucial in diverse scenarios. Beyond forecasting,considerable real-world tasks are categorized into classification, imputation,and anomaly detection, underscoring different capabilities termed time seriesunderstanding in this paper. While GPT-style models have been positioned asfoundation models for time series forecasting, the BERT-style architecture,which has made significant advances in natural language understanding, has notbeen fully unlocked for time series understanding, possibly attributed to theundesirable dropout of essential elements of BERT. In this paper, inspired bythe shared multi-granularity structure between multivariate time series andmultisentence documents, we design TimesBERT to learn generic representationsof time series including temporal patterns and variate-centric characteristics.In addition to a natural adaptation of masked modeling, we propose a paralleltask of functional token prediction to embody vital multi-granularitystructures. Our model is pre-trained on 260 billion time points across diversedomains. Leveraging multi-granularity representations, TimesBERT achievesstate-of-the-art performance across four typical downstream understandingtasks, outperforming task-specific models and language pre-trained backbones,positioning it as a versatile foundation model for time series understanding.</description>
      <author>example@mail.com (Haoran Zhang, Yong Liu, Yunzhong Qiu, Haixuan Liu, Zhongyi Pei, Jianmin Wang, Mingsheng Long)</author>
      <guid isPermaLink="false">2502.21245v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Generating Clinically Realistic EHR Data via a Hierarchy- and Semantics-Guided Transformer</title>
      <link>http://arxiv.org/abs/2502.20719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的框架HiSGT，该框架利用层次和语义信息生成高质量的合成电子健康记录（EHRs），提高了合成数据与真实患者记录在统计上的对齐度，并支持下游临床应用。&lt;h4&gt;背景&lt;/h4&gt;现有的生成方法通常将EHRs视为离散医学代码的序列，忽视了临床编码系统的层级组织及其描述所提供的丰富语义信息，导致合成的数据缺乏临床真实性且在实际应用中效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的框架来克服现有生成模型的问题，提高合成EHR的质量和实用性。&lt;h4&gt;方法&lt;/h4&gt;HiSGT通过构建层次图来捕捉医学代码之间的关系，并使用图神经网络导出具有层级意识的嵌入。这些嵌入与从预训练临床语言模型中提取的语义信息相结合，增强了基于Transformer的生成器的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在MIMIC-III和MIMIC-IV数据集上进行的广泛实验表明，HiSGT显著提高了合成EHRs与真实患者记录之间的统计对齐度，并支持慢性病分类等稳健的下游应用。&lt;h4&gt;结论&lt;/h4&gt;通过解决传统基于原始代码生成模型的限制，HiSGT为临床高保真度合成数据生成提供了重要的步骤和通用框架，促进了可解释医学编码表示以及数据分析和隐私保护方面的有价值的应用。&lt;h4&gt;翻译&lt;/h4&gt;生成逼真的合成电子健康记录（EHRs）对加速医疗研究、促进AI模型开发及增强患者隐私具有巨大潜力。然而，现有的生成方法通常将EHR视为离散医疗代码的序列化结构，这种处理方式忽略了临床编码系统内在的层级组织及其描述提供的丰富语义信息。因此，合成的数据在下游临床任务中的应用价值有限。在这篇论文中，我们提出了HiSGT框架，该框架利用层次和语义信息进行生成过程。通过这种方法，HiSGT不仅提高了数据统计上的对齐度，还支持了稳健的下游应用程序（例如慢性病分类）。这项研究代表了一个重要的步骤，即从传统基于原始代码的生成模型转向临床高保真度合成数据生成，并且提供了一种适合解释性医疗编码表示的一般框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating realistic synthetic electronic health records (EHRs) holdstremendous promise for accelerating healthcare research, facilitating AI modeldevelopment and enhancing patient privacy. However, existing generative methodstypically treat EHRs as flat sequences of discrete medical codes. This approachoverlooks two critical aspects: the inherent hierarchical organization ofclinical coding systems and the rich semantic context provided by codedescriptions. Consequently, synthetic patient sequences often lack highclinical fidelity and have limited utility in downstream clinical tasks. Inthis paper, we propose the Hierarchy- and Semantics-Guided Transformer (HiSGT),a novel framework that leverages both hierarchical and semantic information forthe generative process. HiSGT constructs a hierarchical graph to encodeparent-child and sibling relationships among clinical codes and employs a graphneural network to derive hierarchy-aware embeddings. These are then fused withsemantic embeddings extracted from a pre-trained clinical language model (e.g.,ClinicalBERT), enabling the Transformer-based generator to more accuratelymodel the nuanced clinical patterns inherent in real EHRs. Extensiveexperiments on the MIMIC-III and MIMIC-IV datasets demonstrate that HiSGTsignificantly improves the statistical alignment of synthetic data with realpatient records, as well as supports robust downstream applications such aschronic disease classification. By addressing the limitations of conventionalraw code-based generative models, HiSGT represents a significant step towardclinically high-fidelity synthetic data generation and a general paradigmsuitable for interpretable medical code representation, offering valuableapplications in data augmentation and privacy-preserving healthcare analytics.</description>
      <author>example@mail.com (Guanglin Zhou, Sebastiano Barbieri)</author>
      <guid isPermaLink="false">2502.20719v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Global False Negatives On the Fly for Self-supervised Contrastive Learning</title>
      <link>http://arxiv.org/abs/2502.20612v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了GloFND，一种用于自监督对比学习的方法，能够自动识别和排除虚假负样本。&lt;h4&gt;背景&lt;/h4&gt;在自监督对比学习中，通常通过锚图像与整个数据集中的其他样本来构建负对。这种方法可能导致具有相似语义的负对（即虚假负样本）的生成。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法以解决虚假负样本问题，并提高模型训练的效果和效率。&lt;h4&gt;方法&lt;/h4&gt;GloFND是一种基于优化的方法，它在训练过程中为每个锚数据动态学习阈值来识别其虚假负样本。这种方法可以在整个数据集上全局检测虚假负样本，而不是局限于小批量内局部检测。此外，该方法的每轮迭代计算成本与数据集大小无关。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在图像和图像-文本数据上的GloFND方法是有效的。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效解决自监督对比学习中的虚假负样本问题，并且具有较低的计算复杂度。&lt;h4&gt;翻译&lt;/h4&gt;在自我监督对比性学习中，负对通常是通过锚定图片与整个数据集（除去该锚点）中选取的一个样本来构建。然而，这种策略可能导致生成语义相似的负面配对（称为“虚假否定”），从而导致其嵌入物被错误地推开。为解决此问题，我们提出了一种基于优化的方法GloFND，它在训练过程中自动学习每个锚定数据的阈值以识别其虚假否定。与先前用于发现虚假否定的方法相比，我们的方法在整个数据集中全局检测虚假否定，而不是局限于小批量内局部检测。此外，其每轮迭代计算成本保持独立于数据集大小。实验结果表明，在图像和图像-文本数据上提出的该方法是有效的。我们的实现可在https://github.com/vibalcam/GloFND获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In self-supervised contrastive learning, negative pairs are typicallyconstructed using an anchor image and a sample drawn from the entire dataset,excluding the anchor. However, this approach can result in the creation ofnegative pairs with similar semantics, referred to as "false negatives",leading to their embeddings being falsely pushed apart. To address this issue,we introduce GloFND, an optimization-based approach that automatically learnson the fly the threshold for each anchor data to identify its false negativesduring training. In contrast to previous methods for false negative discovery,our approach globally detects false negatives across the entire dataset ratherthan locally within the mini-batch. Moreover, its per-iteration computationcost remains independent of the dataset size. Experimental results on image andimage-text data demonstrate the effectiveness of the proposed method. Ourimplementation is available at https://github.com/vibalcam/GloFND .</description>
      <author>example@mail.com (Vicente Balmaseda, Bokun Wang, Ching-Long Lin, Tianbao Yang)</author>
      <guid isPermaLink="false">2502.20612v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Causality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models</title>
      <link>http://arxiv.org/abs/2502.21123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;确保机器学习系统的可信度至关重要，尤其是在它们被广泛应用于高风险领域时。本文提倡将因果方法集成到机器学习中，以解决公平性、隐私性、健壮性、准确性和可解释性的相互之间常常产生冲突的核心原则之间的权衡。&lt;h4&gt;背景&lt;/h4&gt;随着机器学习系统在关键领域的应用越来越多，确保这些系统的可信度变得至关重要。然而，在实际操作中，诸如公平性、隐私性等重要目标往往被孤立处理，导致解决方案不理想。&lt;h4&gt;目的&lt;/h4&gt;论文旨在通过引入因果方法来解决信任机器学习和基础模型之间的多重竞争目标的平衡问题，并探讨如何将因果推理有效集成到这些系统中以提高其可靠性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;文章回顾了现有文献中关于利用因果关系成功解决如公平性和准确性或隐私性和健壮性的冲突案例，以此证明因果框架在机器学习中的重要性和实用性。&lt;h4&gt;主要发现&lt;/h4&gt;论文强调采用因果分析可以更好地理解和解决不同目标之间的权衡问题，并提出了一些实际方法来实现这一目标。&lt;h4&gt;结论&lt;/h4&gt;尽管存在挑战和局限性，但通过使用因果框架，可以使AI系统更加负责任且伦理上更为可靠。因此，未来的研究应继续探索如何最佳地利用这些工具和技术。&lt;h4&gt;翻译&lt;/h4&gt;确保机器学习系统的可信度至关重要，尤其是在它们被广泛应用于高风险领域时。本文提倡将因果方法集成到机器学习中，以解决公平性、隐私性、健壮性、准确性和可解释性的相互之间常常产生冲突的核心原则之间的权衡。通过回顾文献中的成功案例，文章强调了因果推理在机器学习中的重要角色，并探讨如何将其有效整合进模型当中，从而提升系统的可靠性和透明度。此外，还讨论了采用这一方法所面临的挑战和机遇，指出了未来研究的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring trustworthiness in machine learning (ML) systems is crucial as theybecome increasingly embedded in high-stakes domains. This paper advocates forthe integration of causal methods into machine learning to navigate thetrade-offs among key principles of trustworthy ML, including fairness, privacy,robustness, accuracy, and explainability. While these objectives should ideallybe satisfied simultaneously, they are often addressed in isolation, leading toconflicts and suboptimal solutions. Drawing on existing applications ofcausality in ML that successfully align goals such as fairness and accuracy orprivacy and robustness, this paper argues that a causal approach is essentialfor balancing multiple competing objectives in both trustworthy ML andfoundation models. Beyond highlighting these trade-offs, we examine howcausality can be practically integrated into ML and foundation models, offeringsolutions to enhance their reliability and interpretability. Finally, wediscuss the challenges, limitations, and opportunities in adopting causalframeworks, paving the way for more accountable and ethically sound AI systems.</description>
      <author>example@mail.com (Ruta Binkyte, Ivaxi Sheth, Zhijing Jin, Muhammad Havaei, Bernhardt Schölkopf, Mario Fritz)</author>
      <guid isPermaLink="false">2502.21123v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>A Non-contrast Head CT Foundation Model for Comprehensive Neuro-Trauma Triage</title>
      <link>http://arxiv.org/abs/2502.21106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种用于检测多种神经创伤的3D基础模型，该模型通过利用大规模语言模型进行自动标注，并采用多模态微调技术将神经网络预先训练结果整合进一个综合性的神经创伤检测网络中。&lt;h4&gt;背景&lt;/h4&gt;AI和医学成像的进步为急诊头部CT图像解读提供了变革性潜力，在请求量增加及放射科医生短缺的情况下，这些进步有助于缩短评估时间并提高准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效准确地识别各种神经创伤的3D基础模型。&lt;h4&gt;方法&lt;/h4&gt;使用大规模语言模型自动生成全面的多标签注释，并通过预训练和多模态微调整合出血亚型分割和大脑解剖图谱到一个综合性的神经创伤检测网络中。&lt;h4&gt;主要发现&lt;/h4&gt;在与专家标注对比及与其他方法（如CT-CLIP）比较时，该模型显示出对包括出血、脑中线移位以及脑水肿等在内的多种神经创伤情况的优异分类准确率。通过加入特定于神经系统的特征，使得诊断能力得到了显著增强。&lt;h4&gt;结论&lt;/h4&gt;这项工作推动了医学影像领域基础模型的发展，并为未来AI辅助急诊放射学中的神经创伤诊断提供了基准。&lt;h4&gt;翻译&lt;/h4&gt;近期在人工智能和医疗成像领域的进展为紧急情况下的头部CT图像解读带来了变革性的潜力。这主要是为了减少评估时间并提高准确性，面对日益增长的扫描需求以及全球范围内放射科医生短缺的问题。该研究引入了一个3D基础模型用于检测各种神经创伤发现，并且具有高准确性和效率。通过使用大规模语言模型进行自动标注，生成了全面的多标签注释以识别严重情况。我们的方法包括对出血亚型分割和大脑解剖图谱预先训练神经网络，并将其整合进一个综合性的预训练神经创伤检测网络中，通过多模态微调实现集成。与专家标记对比以及与其他模型（如CT-CLIP）比较的结果表明，在主要的神经创伤发现上具有强大的分类准确性，例如出血和脑中线移位，以及其他不常见但危急的情况，例如脑水肿和动脉高密度。特定于神经系统的特征的整合显著提升了诊断能力，平均AUC为0.861（针对16种神经创伤情况）。这项工作推进了医学成像中的基础模型，并成为未来AI辅助急诊放射学中神经创伤诊断的一个基准点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in AI and medical imaging offer transformative potentialin emergency head CT interpretation for reducing assessment times and improvingaccuracy in the face of an increasing request of such scans and a globalshortage in radiologists. This study introduces a 3D foundation model fordetecting diverse neuro-trauma findings with high accuracy and efficiency.Using large language models (LLMs) for automatic labeling, we generatedcomprehensive multi-label annotations for critical conditions. Our approachinvolved pretraining neural networks for hemorrhage subtype segmentation andbrain anatomy parcellation, which were integrated into a pretrainedcomprehensive neuro-trauma detection network through multimodal fine-tuning.Performance evaluation against expert annotations and comparison with CT-CLIPdemonstrated strong triage accuracy across major neuro-trauma findings, such ashemorrhage and midline shift, as well as less frequent critical conditions suchas cerebral edema and arterial hyperdensity. The integration of neuro-specificfeatures significantly enhanced diagnostic capabilities, achieving an averageAUC of 0.861 for 16 neuro-trauma conditions. This work advances foundationmodels in medical imaging, serving as a benchmark for future AI-assistedneuro-trauma diagnostics in emergency radiology.</description>
      <author>example@mail.com (Youngjin Yoo, Bogdan Georgescu, Yanbo Zhang, Sasa Grbic, Han Liu, Gabriela D. Aldea, Thomas J. Re, Jyotipriya Das, Poikavila Ullaskrishnan, Eva Eibenberger, Andrei Chekkoury, Uttam K. Bodanapally, Savvas Nicolaou, Pina C. Sanelli, Thomas J. Schroeppel, Yvonne W. Lui, Eli Gibson)</author>
      <guid isPermaLink="false">2502.21106v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Are foundation models useful feature extractors for electroencephalography analysis?</title>
      <link>http://arxiv.org/abs/2502.21086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了基础模型在医学时间序列分析中的应用，特别是对于脑电图（EEG）数据的处理。通过一系列任务实验，包括年龄预测、癫痫检测等，研究表明这些基础模型能够提取有意义的时间序列特征，并且超越专业设计的EEG模型而无需领域适应。&lt;h4&gt;背景&lt;/h4&gt;自然语言处理和计算机视觉领域的基础模型取得了巨大成功，但在医疗时间序列分析（特别是脑电图数据）中应用的基础模型研究较少。随着这类任务的数据集越来越有限，探索这些基础模型在医学时间序列中的适用性至关重要。&lt;h4&gt;目的&lt;/h4&gt;评估基础模型在医学时间序列数据分析中的有效性，特别关注EEG信号的处理能力，并对比这些模型与专门设计的EEG模型的表现。&lt;h4&gt;方法&lt;/h4&gt;实验中采用了一系列任务来测试这些基础模型的功能，包括年龄预测、癫痫检测和临床相关的脑电图事件分类等。并通过对比分析验证了这些模型在诊断准确性方面的优势。&lt;h4&gt;主要发现&lt;/h4&gt;1. 基础模型能够提取有意义的EEG特征；2. 即使没有领域适应，基础模型也超过了专门设计的EEG模型的表现；3. 研究表明架构选择（如上下文长度）对诊断准确度有重大影响。&lt;h4&gt;结论&lt;/h4&gt;研究表明基础模型在医学时间序列分析中具有巨大潜力。通过提供通用的时间序列理解能力，这些模型减少了对大规模特定领域数据集的需求，并成为临床实践中的宝贵工具。&lt;h4&gt;翻译&lt;/h4&gt;自然语言处理和计算机视觉领域的基础模型的成功激发了其在一般时间序列分析中的类似应用尝试。尽管这些模型对于多种任务非常有效，但在医疗领域（尤其是具有有限数据的场景）的应用仍然未被充分探索。为了应对这一问题，我们研究了基础模型在涉及脑电图（EEG）的医学时间序列分析中效果，并通过一系列实验如年龄预测、癫痫检测以及临床相关的EEG事件分类，将它们的表现与专用的EEG模型进行了对比。我们的研究表明，基础模型能够提取有意义的时间序列特征，在没有领域适应的情况下也能超越专用模型，并且能够定位任务特异性的生物标记物。此外，我们还展示了诊断准确性很大程度上受到架构选择（例如上下文长度）的影响。总的来说，这项研究揭示了具备通用时间序列理解能力的基础模型消除了对大规模特定领域数据集的依赖性，使其成为临床实践中有价值的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The success of foundation models in natural language processing and computervision has motivated similar approaches for general time series analysis. Whilethese models are effective for a variety of tasks, their applicability inmedical domains with limited data remains largely unexplored. To address this,we investigate the effectiveness of foundation models in medical time seriesanalysis involving electroencephalography (EEG). Through extensive experimentson tasks such as age prediction, seizure detection, and the classification ofclinically relevant EEG events, we compare their diagnostic accuracy with thatof specialised EEG models. Our analysis shows that foundation models extractmeaningful EEG features, outperform specialised models even without domainadaptation, and localise task-specific biomarkers. Moreover, we demonstratethat diagnostic accuracy is substantially influenced by architectural choicessuch as context length. Overall, our study reveals that foundation models withgeneral time series understanding eliminate the dependency on largedomain-specific datasets, making them valuable tools for clinical practice.</description>
      <author>example@mail.com (Özgün Turgut, Felix S. Bott, Markus Ploner, Daniel Rueckert)</author>
      <guid isPermaLink="false">2502.21086v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>SwimVG: Step-wise Multimodal Fusion and Adaption for Visual Grounding</title>
      <link>http://arxiv.org/abs/2502.16786v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SwimVG的分步多模态融合和适应框架，旨在解决视觉接地任务中现有的方法在跨模态对齐不足以及计算成本高的问题。&lt;h4&gt;背景&lt;/h4&gt;当前大多数用于视觉定位的方法依赖于从预训练模型中单独传输视觉或语言知识，并通过堆叠视觉-语言变压器来实现多模态融合。然而这些方法限制了视觉和语言上下文之间的充分互动并带来较高的计算成本。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文提出了一种新的分步多模态融合和适应框架SwimVG。&lt;h4&gt;方法&lt;/h4&gt;该框架提出了逐步多模态提示（Swip）以及跨模态交互适配器（CIA），用于视觉接地任务。Swip通过逐令牌方式提高视觉和语言表示之间的对齐，而CIA在权重级别上促进跨模态融合。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在四个广泛使用的基准测试中，SwimVG表现出优异的能力并显著提高了效率。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法不仅能够有效解决现有方法的不足，而且具有参数高效的特点，并且通过逐步融合浅层到深层的跨模态特征，展示了其在视觉接地任务中的强大能力。&lt;h4&gt;翻译&lt;/h4&gt;视觉定位旨在通过自然语言确定图像区域，这严重依赖于跨模态对齐。现有的大多数方法通过完全微调单模预训练模型来传输视觉/语言知识，并使用简单的视觉-语言变压器堆叠进行多模态融合。然而，这些方法不仅限制了视觉和语言上下文之间的充分互动，还带来了显著的计算成本。因此，为了应对这些问题，我们探索了一种分步多模态融合和适应框架，即SwimVG。具体来说，SwimVG提出了逐步多模态提示（Swip）以及跨模态交互适配器（CIA），用于视觉接地任务，替代冗余的变压器堆叠进行多模态融合。Swip能够以逐令牌的方式分步提高视觉和语言表示之间的对齐。此外，权重级别的CIA通过跨模态互动进一步促进多模态融合。Swip和CIA都是参数高效的模式，并逐步将浅层到深层的跨模态特征融合在一起。实验结果在四个常用的基准测试上表明，SwimVG在效率方面具有显著的优势。我们的代码可以在https://github.com/liuting20/SwimVG获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/liuting20/swimvg&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual grounding aims to ground an image region through natural language,which heavily relies on cross-modal alignment. Most existing methods transfervisual/linguistic knowledge separately by fully fine-tuning uni-modalpre-trained models, followed by a simple stack of visual-language transformersfor multimodal fusion. However, these approaches not only limit adequateinteraction between visual and linguistic contexts, but also incur significantcomputational costs. Therefore, to address these issues, we explore a step-wisemultimodal fusion and adaption framework, namely SwimVG. Specifically, SwimVGproposes step-wise multimodal prompts (Swip) and cross-modal interactiveadapters (CIA) for visual grounding, replacing the cumbersome transformerstacks for multimodal fusion. Swip can improve {the} alignment between thevision and language representations step by step, in a token-level fusionmanner. In addition, weight-level CIA further promotes multimodal fusion bycross-modal interaction. Swip and CIA are both parameter-efficient paradigms,and they fuse the cross-modal features from shallow to deep layers gradually.Experimental results on four widely-used benchmarks demonstrate that SwimVGachieves remarkable abilities and considerable benefits in terms of efficiency.Our code is available at https://github.com/liuting20/SwimVG.</description>
      <author>example@mail.com (Liangtao Shi, Ting Liu, Xiantao Hu, Yue Hu, Quanjun Yin, Richang Hong)</author>
      <guid isPermaLink="false">2502.16786v2</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Accurate 3D Grapevine Structure Extraction from High-Resolution Point Clouds</title>
      <link>http://arxiv.org/abs/2502.20417v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对葡萄藤3D建模的Smart-Tree算法改进版，采用基于图的方法解决传统骨架化算法在复杂结构上的挑战。&lt;h4&gt;背景&lt;/h4&gt;精确的葡萄藤3D建模对精准农业至关重要，特别是对于信息丰富的修剪决策和自动化管理技术。然而，葡萄藤复杂的结构给传统的骨架化算法带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种适用于葡萄藤独特特性的三维建模方法，改善传统Smart-Tree算法在处理复杂结构上的效果。&lt;h4&gt;方法&lt;/h4&gt;提出了基于图的方法来区分骨架化过程中的个体枝条，并通过注释的现实世界点云数据进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;新方法相比原始的Smart-Tree算法，在F1分数上提高了15.8%，表明改进的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究推进了葡萄藤三维建模技术的发展，有可能通过更精确和自动化的农业实践提高葡萄生产的可持续性和盈利能力。&lt;h4&gt;翻译&lt;/h4&gt;准确地对葡萄藤进行3D建模对于精准种植至关重要，特别是在信息丰富的修剪决策以及自动化管理技术方面。然而，由于葡萄藤的复杂结构，传统的骨架化算法面临着重大挑战。本文提出了一种针对Smart-Tree算法改进的方法来应对葡萄藤的独特特点，并使用基于图的方式来解决骨架化的歧义问题。该方法能够区分出每个枝条的骨架结构，这对于精确分析和管理至关重要。我们通过使用注释过的现实世界中葡萄藤点云数据验证了我们的方法的有效性，在F1评分上较原始Smart-Tree算法提高了15.8%。这项研究为3D葡萄藤建模技术的发展做出了贡献，并有可能通过更加准确且自动化的种植实践提高葡萄生产的可持续性和盈利能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D modelling of grapevines is crucial for precision viticulture,particularly for informed pruning decisions and automated managementtechniques. However, the intricate structure of grapevines poses significantchallenges for traditional skeletonization algorithms. This paper presents anadaptation of the Smart-Tree algorithm for 3D grapevine modelling, addressingthe unique characteristics of grapevine structures. We introduce a graph-basedmethod for disambiguating skeletonization. Our method delineates individualcane skeletons, which are crucial for precise analysis and management. Wevalidate our approach using annotated real-world grapevine point clouds,demonstrating improvement of 15.8% in the F1 score compared to the originalSmart-Tree algorithm. This research contributes to advancing 3D grapevinemodelling techniques, potentially enhancing both the sustainability andprofitability of grape production through more precise and automatedviticulture practices</description>
      <author>example@mail.com (Harry Dobbs, Casey Peat, Oliver Batchelor, James Atlas, Richard Green)</author>
      <guid isPermaLink="false">2502.20417v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Can We Simplify Slide-level Fine-tuning of Pathology Foundation Models?</title>
      <link>http://arxiv.org/abs/2502.20823v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的策略SiMLP，通过简单的非线性映射结合均值池化和多层感知机来适应基于切片级别的任务，超越了传统的MIL方法。&lt;h4&gt;背景&lt;/h4&gt;计算病理学中基础模型的出现已经改变了组织病理图像分析的方法，其中全滑动影像（WSI）诊断是核心应用。以往主要采用弱监督微调通过多重实例学习（MIL）来适应基础模型以处理WSIs。&lt;h4&gt;目的&lt;/h4&gt;展示SiMLP策略在多种下游任务中的优越性，并挑战传统的基于MIL的微调范式。&lt;h4&gt;方法&lt;/h4&gt;提出了一种简单非线性映射策略，称为SiMLP，该策略结合均值池化和多层感知机来将基础模型从补丁级别适应到切片级别任务。&lt;h4&gt;主要发现&lt;/h4&gt;1. SiMLP在大规模癌症分类任务中超越了流行MIL方法3.52%，显示强大的少样本分类能力；2. 在肺部肿瘤亚型分类方面，SiMLP表现出显著的鲁棒性和可转移性。3. SiMLP可以不需要复杂的基于MIL的学习过程来适应WSI分析。&lt;h4&gt;结论&lt;/h4&gt;研究结果挑战了传统的基于MIL的微调范式，并表明仅通过任务无关表示策略即可有效调整基础模型以进行WSI分析，为未来数字病理学研究提供了新的视角和方法论。&lt;h4&gt;翻译&lt;/h4&gt;计算病理学中的基础模型出现已经改变了组织病理图像分析的方法，其中全滑动影像（WSI）诊断是核心应用。以往主要采用弱监督微调通过多重实例学习（MIL）来适应基础模型以处理WSIs。然而，在这项工作中我们提出了一种关键的实验发现：一种简单的非线性映射策略结合均值池化和多层感知机，称为SiMLP，可以有效将基于补丁级别的基础模型适配到切片级别任务而不需要复杂MIL基的学习方法。通过广泛的跨多种下游任务实验，我们展示了SiMLP与最新技术相比的优越性能，在大规模癌症分类任务中超越了流行MIL方法3.52%。此外，SiMLP在少样本分类中表现出强大的学习能力，并且仍然与其他预训练于数十万张切片上的切片级别基础模型竞争。最后，SiMLP在肺癌亚型分类方面表现出显著的鲁棒性和可转移性。总的来说，我们的发现挑战了传统的基于MIL的微调范式，表明仅通过任务无关表示策略就可以有效地将基础模型适配到WSI分析中。这些见解为未来数字病理学研究提供了一个独特且具有意义的新视角，并为此类研究铺平了更高效和广泛适用的方法论的道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of foundation models in computational pathology has transformedhistopathological image analysis, with whole slide imaging (WSI) diagnosisbeing a core application. Traditionally, weakly supervised fine-tuning viamultiple instance learning (MIL) has been the primary method for adaptingfoundation models to WSIs. However, in this work we present a key experimentalfinding: a simple nonlinear mapping strategy combining mean pooling and amultilayer perceptron, called SiMLP, can effectively adapt patch-levelfoundation models to slide-level tasks without complex MIL-based learning.Through extensive experiments across diverse downstream tasks, we demonstratethe superior performance of SiMLP with state-of-the-art methods. For instance,on a large-scale pan-cancer classification task, SiMLP surpasses popularMIL-based methods by 3.52%. Furthermore, SiMLP shows strong learning ability infew-shot classification and remaining highly competitive with slide-levelfoundation models pretrained on tens of thousands of slides. Finally, SiMLPexhibits remarkable robustness and transferability in lung cancer subtyping.Overall, our findings challenge the conventional MIL-based fine-tuningparadigm, demonstrating that a task-agnostic representation strategy alone caneffectively adapt foundation models to WSI analysis. These insights offer aunique and meaningful perspective for future research in digital pathology,paving the way for more efficient and broadly applicable methodologies.</description>
      <author>example@mail.com (Jiawen Li, Jiali Hu, Qiehe Sun, Renao Yan, Minxi Ouyang, Tian Guan, Anjia Han, Chao He, Yonghong He)</author>
      <guid isPermaLink="false">2502.20823v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>SemiSAM+: Rethinking Semi-Supervised Medical Image Segmentation in the Era of Foundation Models</title>
      <link>http://arxiv.org/abs/2502.20749v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于提示的基础模型驱动的半监督学习框架SemiSAM+，用于提高医疗图像分割任务中有限标注数据的学习效率。&lt;h4&gt;背景&lt;/h4&gt;深度学习在医学影像分割中的应用通常需要大量的标记数据进行训练，这在临床环境中由于注释成本高而难以实施。半监督学习（SSL）作为一种依赖较少专家标注的方法逐渐受到关注。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于提示的基础模型驱动的半监督框架SemiSAM+，以期通过有限数量的标签实现高效的医学影像分割任务。&lt;h4&gt;方法&lt;/h4&gt;该框架由一个或多个可提示的基础模型和一个特定于任务的学习型模型组成。在给定的新分割任务中，训练过程包括学习型模型与基础模型之间的协作，在此基础上学习型模型生成位置提示并接收来自基础模型的伪标签监督。&lt;h4&gt;主要发现&lt;/h4&gt;SemiSAM+框架在两个公共数据集及一家医院内部临床数据集中展示了显著性能提升，特别是在标注数量极为有限的情况下效果尤为突出。该框架还展现了强大的适应性作为即插即用策略可以轻松应用于不同类型的特定任务和通用模型中。&lt;h4&gt;结论&lt;/h4&gt;论文提出了一种新的半监督学习方法SemiSAM+，它通过结合基础模型的泛化能力和学习型模型的专业能力，在医学图像分割任务中实现了显著性能改进。这种方法为解决有限标签数据集下的高效训练提供了可能路径，并展示了良好的扩展性和通用性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based medical image segmentation typically requires largeamount of labeled data for training, making it less applicable in clinicalsettings due to high annotation cost. Semi-supervised learning (SSL) hasemerged as an appealing strategy due to its less dependence on acquiringabundant annotations from experts compared to fully supervised methods. Beyondexisting model-centric advancements of SSL by designing novel regularizationstrategies, we anticipate a paradigmatic shift due to the emergence ofpromptable segmentation foundation models with universal segmentationcapabilities using positional prompts represented by Segment Anything Model(SAM). In this paper, we present SemiSAM+, a foundation model-driven SSLframework to efficiently learn from limited labeled data for medical imagesegmentation. SemiSAM+ consists of one or multiple promptable foundation modelsas generalist models, and a trainable task-specific segmentation model asspecialist model. For a given new segmentation task, the training is based onthe specialist-generalist collaborative learning procedure, where the trainablespecialist model delivers positional prompts to interact with the frozengeneralist models to acquire pseudo-labels, and then the generalist modeloutput provides the specialist model with informative and efficient supervisionwhich benefits the automatic segmentation and prompt generation in turn.Extensive experiments on two public datasets and one in-house clinical datasetdemonstrate that SemiSAM+ achieves significant performance improvement,especially under extremely limited annotation scenarios, and shows strongefficiency as a plug-and-play strategy that can be easily adapted to differentspecialist and generalist models.</description>
      <author>example@mail.com (Yichi Zhang, Bohao Lv, Le Xue, Wenbo Zhang, Yuchen Liu, Yu Fu, Yuan Cheng, Yuan Qi)</author>
      <guid isPermaLink="false">2502.20749v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding</title>
      <link>http://arxiv.org/abs/2502.20678v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR'25 Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究了弱监督时空视频定位任务，提出了一种新的学习框架STPro。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉语言基础模型虽然具备零样本推理能力，但在执行弱监督时空视频定位任务时缺乏必要的时空定位能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法的不足，设计了一个能够进行时空预测的学习系统。&lt;h4&gt;方法&lt;/h4&gt;引入了Tubelet Referral Grounding (TRG)，并在此基础上提出了一个新颖的进步学习框架STPro。该框架包含两个关键模块：Sub-Action Temporal Curriculum Learning (SA-TCL) 和 Congestion-Guided Spatial Curriculum Learning (CG-SCL)。&lt;h4&gt;主要发现&lt;/h4&gt;通过在三个基准数据集上的实验，证明了所提出的STPro方法的有效性，并且在VidSTG-Declarative和HCSTVG-v1两个数据集中分别取得了比之前最好的结果高出1.0% 和 3.0%的成绩。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为弱监督时空视频定位任务提供了一个新的解决方案，提高了该领域的技术水平。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们研究了使用文本查询而没有任何边界框监督的弱监督时空视频定位（WSTVG）任务。受到近期视觉-语言基础模型进展的启发，我们探索了这些模型在WSTVG中的实用性，利用它们的零样本接地能力。然而，简单地调整这些模型无法满足必要的时空定点功能。为弥补这一差距，我们提出了Tubelet Referral Grounding（TRG），该方法将文本查询与管状体连接起来以实现时空预测。尽管有潜力，但TRG在组合动作理解和密集场景方面仍然存在挑战。为了克服这些问题，我们提出了一种新的渐进式学习框架STPro，具有两个关键模块：Sub-Action Temporal Curriculum Learning（SA-TCL）和Congestion-Guided Spatial Curriculum Learning（CG-SCL）。在三个基准数据集上进行实验后，我们的方法实现了最先进的结果，在VidSTG-Declarative和HCSTVG-v1中分别提高了1.0% 和 3.0%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work we study Weakly Supervised Spatio-Temporal Video Grounding(WSTVG), a challenging task of localizing subjects spatio-temporally in videosusing only textual queries and no bounding box supervision. Inspired by recentadvances in vision-language foundation models, we investigate their utility forWSTVG, leveraging their zero-shot grounding capabilities. However, we find thata simple adaptation lacks essential spatio-temporal grounding abilities. Tobridge this gap, we introduce Tubelet Referral Grounding (TRG), which connectstextual queries to tubelets to enable spatio-temporal predictions. Despite itspromise, TRG struggles with compositional action understanding and dense scenescenarios. To address these limitations, we propose STPro, a novel progressivelearning framework with two key modules: (1) Sub-Action Temporal CurriculumLearning (SA-TCL), which incrementally builds compositional actionunderstanding, and (2) Congestion-Guided Spatial Curriculum Learning (CG-SCL),which adapts the model to complex scenes by spatially increasing taskdifficulty. STPro achieves state-of-the-art results on three benchmarkdatasets, with improvements of 1.0% on VidSTG-Declarative and 3.0% onHCSTVG-v1.</description>
      <author>example@mail.com (Aaryan Garg, Akash Kumar, Yogesh S Rawat)</author>
      <guid isPermaLink="false">2502.20678v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>SciceVPR: Stable Cross-Image Correlation Enhanced Model for Visual Place Recognition</title>
      <link>http://arxiv.org/abs/2502.20676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种名为SciceVPR的稳定跨图相关增强模型，用于视觉位置识别（VPR），旨在生成具有区分性和稳定性全局描述符。&lt;h4&gt;背景&lt;/h4&gt;当前最先进的VPR模型依赖于强大的基础模型DINOv2提取全局特征，并且要么通过探索跨图像相关性来提高性能，要么采用耗时的两阶段重排名策略。但现有工作仅利用了DINOv2的最终输出结果，导致检索效果不稳定。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进方法以克服现有技术中描述符不稳定性问题，并充分利用DINOv2模型提供的特征表示能力，隐式编码有价值的上下文知识。&lt;h4&gt;方法&lt;/h4&gt;SciceVPR通过一个多层特征融合模块捕捉任务相关通道和空间信息；同时利用图像之间不变的相关性作为有价值的知识融入增强自编码器，从而获得对领域转换具有鲁棒性的全局特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，SciceVPR-B变体在多个不同领域条件的数据集上优于现有的单输入一步方法。而SciceVPR-L版本性能与最先进的两步模型相当，在挑战性东京24/7数据集中召回率@1高出3%以上。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够生成稳定的全局描述符，提高视觉位置识别的准确性和鲁棒性，并在多个数据集上优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;Visual Place Recognition（VPR）是机器人和自主系统的一个主要挑战，目标是仅基于图像的视觉特征预测其位置。最先进的模型使用强大的基础模型DINOv2作为骨干网络来提取全局描述符。这些模型要么通过探索跨图相关性提高性能，要么采用耗时的两阶段重排名策略以达到更好的效果。然而，现有的工作仅仅利用了DINOv2的最终输出，并且当前的跨图相关会导致检索结果不稳定。为了生成具有区分性和稳定性的全局描述符，本文提出了名为SciceVPR的增强模型。该模型探索了DINOv2在提供有用特征表示方面的全部潜力，隐式地编码有价值的上下文知识。具体而言，SciceVPR首先利用一个多层特征融合模块捕捉任务相关的通道和空间信息；其次考虑图像批次内的不变相关性作为有价值的知识融入提出的自增强编解码器中。这样，SciceVPR可以获取相对领域转换（例如光照、天气和视角变化）的全局特征具有鲁棒性的特性。实验结果表明，基本版本SciceVPR-B在多种不同条件的数据集上优于现有的单输入一步方法。大型变体SciceVPR-L与最先进的两步模型相当，在挑战性东京24/7数据集中召回率@1高出3%以上。我们的代码将发布于https://github.com/shuimushan/SciceVPR。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Place Recognition (VPR) is a major challenge for robotics andautonomous systems, with the goal of predicting the location of an image basedsolely on its visual features. State-of-the-art (SOTA) models extract globaldescriptors using the powerful foundation model DINOv2 as backbone. Thesemodels either explore the cross-image correlation or propose a time-consumingtwo-stage re-ranking strategy to achieve better performance. However, existingworks only utilize the final output of DINOv2, and the current cross-imagecorrelation causes unstable retrieval results. To produce both discriminativeand constant global descriptors, this paper proposes stable cross-imagecorrelation enhanced model for VPR called SciceVPR. This model explores thefull potential of DINOv2 in providing useful feature representations thatimplicitly encode valuable contextual knowledge. Specifically, SciceVPR firstuses a multi-layer feature fusion module to capture increasingly detailedtask-relevant channel and spatial information from the multi-layer output ofDINOv2. Secondly, SciceVPR considers the invariant correlation between imageswithin a batch as valuable knowledge to be distilled into the proposedself-enhanced encoder. In this way, SciceVPR can acquire fairly robust globalfeatures regardless of domain shifts (e.g., changes in illumination, weatherand viewpoint between pictures taken in the same place). Experimental resultsdemonstrate that the base variant, SciceVPR-B, outperforms SOTA one-stagemethods with single input on multiple datasets with varying domain conditions.The large variant, SciceVPR-L, performs on par with SOTA two-stage models,scoring over 3% higher in Recall@1 compared to existing models on thechallenging Tokyo24/7 dataset. Our code will be released athttps://github.com/shuimushan/SciceVPR.</description>
      <author>example@mail.com (Shanshan Wan, Yingmei Wei, Lai Kang, Tianrui Shen, Haixuan Wang, Yee-Hong Yang)</author>
      <guid isPermaLink="false">2502.20676v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>CoCa-CXR: Contrastive Captioners Learn Strong Temporal Structures for Chest X-Ray Vision-Language Understanding</title>
      <link>http://arxiv.org/abs/2502.20509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;视觉-语言模型在医学图像分析中发挥了重要作用，通过从图像和报告中学到丰富的语义信息来提高图像理解。该研究针对胸部X光片（CXR）的报告处理流程提出了一种新的方法。&lt;h4&gt;背景&lt;/h4&gt;现有努力主要集中在图像与文本表示的一致性上以增强图像理解，但针对CXR报告中常见的时间参照进行图像对之间的语义差异对齐的问题则较少探索。&lt;h4&gt;目的&lt;/h4&gt;提出了两个组件来解决这一问题：一个用于处理CXR报告的流程和CoCa-CXR模型，该模型能够描述图像及其时间进程，并识别配对CXR图像中的局部差异。&lt;h4&gt;方法&lt;/h4&gt;(1) 提出了一种基于大规模语言模型（LLM）的CXR报告处理流水线来提取时态结构。(2) 开发了名为CoCa-CXR的对比性标题生成器，以学习描述图像及其时间变化的方法。该模型包含了一个新颖的区域交叉注意力模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，CoCa-CXR在进展分析和报告生成方面优于先前方法，在MS-CXR-T进展分类上的平均测试准确率达到了65.0%，超过之前的SOTA模型BioViL-T 4.8%。同时在MIMIC-CXR上取得了24.2%的RadGraph F1，与Med-Gemini基础模型相当。&lt;h4&gt;结论&lt;/h4&gt;CoCa-CXR通过新的区域交叉注意力模块和创新性的处理流程，在医学图像的时间进程分析中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型由于能够从图像和报告中学到丰富的语义信息，在医学影像分析领域具有重要作用。以往的研究重点在于更好地对齐图像和文本表示以增强图像理解。然而，尽管胸部X射线（CXR）报告中通常会明确参考之前的影像，但如何将进展描述与成对影像之间的语义差异进行有效对齐仍有待进一步研究。为此，我们提出了两种解决方法：一种用于处理CXR报告的流水线和一个对比性标题生成器CoCa-CXR，用于学习描绘图像及其时间变化的方法。实验表明，该模型在进展分析及报告生成上均优于现有技术，并且在MS-CXR-T进步分类任务中平均测试准确率达到了65.0%，超过之前的SOTA模型BioViL-T 4.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models have proven to be of great benefit for medical imageanalysis since they learn rich semantics from both images and reports. Priorefforts have focused on better alignment of image and text representations toenhance image understanding. However, though explicit reference to a priorimage is common in Chest X-Ray (CXR) reports, aligning progression descriptionswith the semantics differences in image pairs remains under-explored. In thiswork, we propose two components to address this issue. (1) A CXR reportprocessing pipeline to extract temporal structure. It processes reports with alarge language model (LLM) to separate the description and comparison contexts,and extracts fine-grained annotations from reports. (2) A contrastive captionermodel for CXR, namely CoCa-CXR, to learn how to both describe images and theirtemporal progressions. CoCa-CXR incorporates a novel regional cross-attentionmodule to identify local differences between paired CXR images. Extensiveexperiments show the superiority of CoCa-CXR on both progression analysis andreport generation compared to previous methods. Notably, on MS-CXR-Tprogression classification, CoCa-CXR obtains 65.0% average testing accuracy onfive pulmonary conditions, outperforming the previous state-of-the-art (SOTA)model BioViL-T by 4.8%. It also achieves a RadGraph F1 of 24.2% on MIMIC-CXR,which is comparable to the Med-Gemini foundation model.</description>
      <author>example@mail.com (Yixiong Chen, Shawn Xu, Andrew Sellergren, Yossi Matias, Avinatan Hassidim, Shravya Shetty, Daniel Golden, Alan Yuille, Lin Yang)</author>
      <guid isPermaLink="false">2502.20509v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation</title>
      <link>http://arxiv.org/abs/2502.18041v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Vision-Language Navigation (VLN)在机器人导航领域扮演着重要角色，尤其是在具身人工智能中。本文提出了OpenFly平台，旨在解决户外空中VLN数据收集困难的问题，并构建了大规模的数据集和模型。&lt;h4&gt;背景&lt;/h4&gt;室内VLN已经被广泛研究，但室外空中VLN由于涉及的视野广阔、数据采集难度大而较少被探索。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的平台OpenFly来促进户外空中的Vision-Language Navigation（VLN）的发展，包括工具链开发和大规模数据集构建。&lt;h4&gt;方法&lt;/h4&gt;{'开发自动化工具链': '自动收集点云数据、进行场景语义分割、生成飞行轨迹以及创建指令', '构建大型数据集': '使用多种渲染引擎和技术生成包含100k轨迹的大规模数据集，涵盖多样化的高度和长度，并通过3D Gaussian Splatting技术增强数据的真实感。', '提出OpenFly-Agent模型': '基于关键帧的VLN模型，可以接收语言指令、当前观察结果以及历史上的关键帧作为输入，并直接输出飞行动作'}&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的分析和实验展示了OpenFly平台及其核心算法OpenFly-Agent的优越性能。&lt;h4&gt;结论&lt;/h4&gt;开源了工具链、数据集及代码，旨在促进户外空中VLN的研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已包含详细内容，无需额外翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Navigation (VLN) aims to guide agents through an environmentby leveraging both language instructions and visual cues, playing a pivotalrole in embodied AI. Indoor VLN has been extensively studied, whereas outdooraerial VLN remains underexplored. The potential reason is that outdoor aerialview encompasses vast areas, making data collection more challenging, whichresults in a lack of benchmarks. To address this problem, we propose OpenFly, aplatform comprising a versatile toolchain and large-scale benchmark for aerialVLN. Firstly, we develop a highly automated toolchain for data collection,enabling automatic point cloud acquisition, scene semantic segmentation, flighttrajectory creation, and instruction generation. Secondly, based on thetoolchain, we construct a large-scale aerial VLN dataset with 100ktrajectories, covering diverse heights and lengths across 18 scenes. Thecorresponding visual data are generated using various rendering engines andadvanced techniques, including Unreal Engine, GTA V, Google Earth, and 3DGaussian Splatting (3D GS). All data exhibit high visual quality. Particularly,3D GS supports real-to-sim rendering, further enhancing the realism of thedataset. Thirdly, we propose OpenFly-Agent, a keyframe-aware VLN model, whichtakes language instructions, current observations, and historical keyframes asinput, and outputs flight actions directly. Extensive analyses and experimentsare conducted, showcasing the superiority of our OpenFly platform andOpenFly-Agent. The toolchain, dataset, and codes will be open-sourced.</description>
      <author>example@mail.com (Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li)</author>
      <guid isPermaLink="false">2502.18041v2</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN Architectures</title>
      <link>http://arxiv.org/abs/2502.16622v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Upon reflection, the final version of this work does not meet the  author's personal standards for thoroughness and clarity. As a result, the  authors have chosen to withdraw the paper to prevent the dissemination of  work that may not fully reflect the level of quality they strive to maintain&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过合并三个来源创建了一个大型的COVID严重程度数据集，探讨了迁移学习在使用ImageNet和CXR预训练模型以及视觉变换器(ViTs)进行病情预测方面的有效性。&lt;h4&gt;背景&lt;/h4&gt;新冠肺炎大流行导致医疗资源紧张，并引发了关于机器学习如何减轻医生负担并有助于诊断的讨论。胸部X光片（CXRs）用于诊断COVID-19，但很少有研究从CXRs预测患者的病情严重程度。&lt;h4&gt;目的&lt;/h4&gt;探讨迁移学习在基于图像的数据集中的表现以及其对新冠肺炎患者病情预测的贡献。&lt;h4&gt;方法&lt;/h4&gt;本研究使用了ImageNet和CXR预训练模型及视觉变换器(ViTs)，并在此基础上进行了严重程度回归与分类任务的研究。&lt;h4&gt;主要发现&lt;/h4&gt;预训练DenseNet161模型在三种严重程度预测问题上表现最佳，总体准确率为80%，轻度、中度和重度病例的分别准确率分别为77.3%、83.9%和70%。视觉变换器(ViT)在回归任务中的均方绝对误差为0.5676。&lt;h4&gt;结论&lt;/h4&gt;迁移学习方法，特别是预训练模型，可以有效地用于基于图像的数据集来预测新冠肺炎的病情严重程度。&lt;h4&gt;翻译&lt;/h4&gt;摘要提供了英文原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/stwhitfield/covid-severity&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The COVID-19 pandemic strained healthcare resources and prompted discussionabout how machine learning can alleviate physician burdens and contribute todiagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but fewstudies predict the severity of a patient's condition from CXRs. In this study,we produce a large COVID severity dataset by merging three sources andinvestigate the efficacy of transfer learning using ImageNet- andCXR-pretrained models and vision transformers (ViTs) in both severityregression and classification tasks. A pretrained DenseNet161 model performedthe best on the three class severity prediction problem, reaching 80% accuracyoverall and 77.3%, 83.9%, and 70% on mild, moderate and severe cases,respectively. The ViT had the best regression results, with a mean absoluteerror of 0.5676 compared to radiologist-predicted severity scores. Theproject's source code is publicly available.</description>
      <author>example@mail.com (Luis Lara, Lucia Eve Berger, Rajesh Raju)</author>
      <guid isPermaLink="false">2502.16622v3</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning through Enhanced Sufficient Representation: Enriching Source Domain Knowledge with Target Data</title>
      <link>http://arxiv.org/abs/2502.20414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  44 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种新的迁移学习方法——增强充分表示的迁移学习（TESR），旨在解决传统迁移学习方法因模型假设过于严格和源域与目标域相似度要求高而导致的问题。&lt;h4&gt;背景&lt;/h4&gt;随着数据可用性的限制，迁移学习成为了解决这些问题的重要方法。它通过从已建立良好的源领域向不熟悉的靶领域转移知识来实现这一目标。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新的迁移学习方法TESR，旨在克服传统方法的局限性，提高模型在不同任务中的适应性和灵活性。&lt;h4&gt;方法&lt;/h4&gt;首先估计出一个充分和不变的表现形式，然后通过来自靶数据的独立成分增强该表现形式，使其成为针对特定目标领域的充足表示且易于适应其特性。此方法不依赖于跨不同任务的相似模型结构假设。&lt;h4&gt;主要发现&lt;/h4&gt;TESR能够在有限样本环境下有效工作，并在模拟研究和实际应用中验证了它的性能。&lt;h4&gt;结论&lt;/h4&gt;论文展示了TESR作为迁移学习的一种灵活有效的策略，适用于广泛的监督学习问题。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习是一种解决数据可用性限制的重要方法，通过从已建立良好的源领域向不熟悉的靶领域转移知识来实现。然而，传统的方法往往因为过于严格的模型假设和需要高度相似的域模型而面临挑战。本文提出了一种新的方法——增强充分表示的迁移学习（TESR）。该方法首先估计出一个充分不变的表现形式，并通过来自靶数据的独立成分进一步增强它，以适应特定目标领域的特性并确保其充足性。主要优点是不依赖于假设跨任务相似模型结构的存在；例如源域可以使用回归模型而靶域的任务可能是分类。这种灵活性使得TESR能够应用于广泛的监督学习问题中。论文通过理论属性探索和模拟研究以及实际数据应用验证了TESR的性能，证明它在有限样本设置下的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning is an important approach for addressing the challengesposed by limited data availability in various applications. It accomplishesthis by transferring knowledge from well-established source domains to a lessfamiliar target domain. However, traditional transfer learning methods oftenface difficulties due to rigid model assumptions and the need for a high degreeof similarity between source and target domain models. In this paper, weintroduce a novel method for transfer learning called Transfer learning throughEnhanced Sufficient Representation (TESR). Our approach begins by estimating asufficient and invariant representation from the source domains. Thisrepresentation is then enhanced with an independent component derived from thetarget data, ensuring that it is sufficient for the target domain and adaptableto its specific characteristics. A notable advantage of TESR is that it doesnot rely on assuming similar model structures across different tasks. Forexample, the source domain models can be regression models, while the targetdomain task can be classification. This flexibility makes TESR applicable to awide range of supervised learning problems. We explore the theoreticalproperties of TESR and validate its performance through simulation studies andreal-world data applications, demonstrating its effectiveness in finite samplesettings.</description>
      <author>example@mail.com (Yeheng Ge, Xueyu Zhou, Jian Huang)</author>
      <guid isPermaLink="false">2502.20414v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>CurviTrack: Curvilinear Trajectory Tracking for High-speed Chase of a USV</title>
      <link>http://arxiv.org/abs/2502.21303v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于拖曳感知模型与MPC相结合的方法，用于解决海洋环境中异构机器人团队由于需要让自主飞行器着陆充电而导致的时间和能量损失问题。&lt;h4&gt;背景&lt;/h4&gt;在海事应用中使用异构机器人团队会导致时间及能源的浪费，特别是在自主飞行器需要降落以重新充电时。这不仅影响任务效率，还限制了海洋车辆执行复杂机动的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有技术中的预测误差大、预测准确性低以及跟踪性能不足的问题，提高在动态环境下的着陆成功率。&lt;h4&gt;方法&lt;/h4&gt;开发了一种新的拖曳感知模型，并将其与MPC（模型预测控制）结合使用，以实现高速曲线轨迹下的追踪和降落，无需通信即可完成任务。&lt;h4&gt;主要发现&lt;/h4&gt;相比现有技术，该方法降低了40%的预测误差，提高了预测准确性的三倍，并且在跟踪性能上提升了30%，成功着陆率提高到了原来的四倍，尤其是在执行剧烈转弯等传统海上任务难以应对的情况下表现尤为突出。&lt;h4&gt;结论&lt;/h4&gt;通过两个不同实际场景中大小不同的海洋船只测试验证了该方法的有效性，并进一步使用模拟中的统计分析来展示其鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;异构机器人团队在海洋环境中使用时，当海洋车辆必须停止执行任务以让自主飞行器降落进行充电时会遭受时间和能源的惩罚。本文提出了一种解决方案，利用一种新的拖曳感知模型和MPC（模型预测控制）相结合的方法来跟踪并在高速曲线轨迹中实现不依赖通信的着陆。这种方法相比于最先进的技术可以降低40%的预测误差，并提供了预测准确性的三倍提高。因此，在进行剧烈转弯等常规海上任务难以处理的情况下，这导致了30%的追踪性能改进和40%更高的在移动USV上成功的着陆概率。我们在两种不同的现实场景中测试了我们的方法，使用不同大小的海洋船只，并通过模拟中的统计分析进一步证实我们方法的稳健性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3546079&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous robot teams used in marine environments incur time-and-energypenalties when the marine vehicle has to halt the mission to allow theautonomous aerial vehicle to land for recharging. In this paper, we present asolution for this problem using a novel drag-aware model formulation which iscoupled with MPC, and therefore, enables tracking and landing during high-speedcurvilinear trajectories of an USV without any communication. Compared to thestate-of-the-art, our approach yields 40% decrease in prediction errors, andprovides a 3-fold increase in certainty of predictions. Consequently, thisleads to a 30% improvement in tracking performance and 40% higher success inlanding on a moving USV even during aggressive turns that are unfeasible forconventional marine missions. We test our approach in two different real-worldscenarios with marine vessels of two different sizes and further solidify ourresults through statistical analysis in simulation to demonstrate therobustness of our method.</description>
      <author>example@mail.com (Parakh M. Gupta, Ondřej Procházka, Tiago Nascimento, Martin Saska)</author>
      <guid isPermaLink="false">2502.21303v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Back to the Future Cyclopean Stereo: a human perception approach unifying deep and geometric constraints</title>
      <link>http://arxiv.org/abs/2502.21280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合几何模型和学习到的立体视觉特征的方法，用于改善3D表面建模。&lt;h4&gt;背景&lt;/h4&gt;传统的立体视觉方法在处理深度不连续性和遮挡时存在挑战。仅基于数据驱动的方法难以捕捉关键的视觉信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用几何形状描述优势，并通过单目先验模型来增强遮挡和缺乏纹理区域建模的新系统。&lt;h4&gt;方法&lt;/h4&gt;1. 使用类独眼的模型提供分析性3D表面模型，这些模型包含了深度不连续性和遮挡；2. 结合学习到的立体视觉特征；3. 调用单目先验表面模型填补遮挡或缺乏纹理的区域。&lt;h4&gt;主要发现&lt;/h4&gt;该方法结果与现有的数据驱动方法相当，但在视觉质量上有显著提升，证明了三维几何模型的重要性。&lt;h4&gt;结论&lt;/h4&gt;理解并建模三维形状属性对于计算机视觉研究至关重要，并且这种改进可以在虚拟现实和机器人技术中应用，以改善用户体验或减少错误。&lt;h4&gt;翻译&lt;/h4&gt;我们在立体视觉方面进行了创新，通过提供由独眼模型视角下的分析性3D表面模型来明确处理深度不连续性和遮挡问题。结合几何基础与学习到的立体特征使我们的系统能够从两种方法的优势中获益。此外，在数据匹配不足的情况下使用单目先验模型填补遮挡或缺乏纹理区域。我们的结果已达到现有纯数据驱动方法同等水平，但在视觉质量上更胜一筹，突显了3D几何模型捕捉关键视觉信息的重要性。这样的定性改进可能在虚拟现实中找到应用价值，以改善人类体验，并且在机器人技术中减少关键错误方面同样重要。本研究旨在证明理解并建模三维表面的几何属性对计算机视觉研究有益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We innovate in stereo vision by explicitly providing analytical 3D surfacemodels as viewed by a cyclopean eye model that incorporate depthdiscontinuities and occlusions. This geometrical foundation combined withlearned stereo features allows our system to benefit from the strengths of bothapproaches. We also invoke a prior monocular model of surfaces to fill inocclusion regions or texture-less regions where data matching is notsufficient. Our results already are on par with the state-of-the-art purelydata-driven methods and are of much better visual quality, emphasizing theimportance of the 3D geometrical model to capture critical visual information.Such qualitative improvements may find applicability in virtual reality, for abetter human experience, as well as in robotics, for reducing critical errors.Our approach aims to demonstrate that understanding and modeling geometricalproperties of 3D surfaces is beneficial to computer vision research.</description>
      <author>example@mail.com (Sherlon Almeida da Silva, Davi Geiger, Luiz Velho, Moacir Antonelli Ponti)</author>
      <guid isPermaLink="false">2502.21280v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete</title>
      <link>http://arxiv.org/abs/2502.21257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;最近，多模态大型语言模型（MLLM）在各种多模态环境中展示了显著的能力。然而，在机器人场景中的应用，特别是长时程操作任务中表现出重大限制。&lt;h4&gt;背景&lt;/h4&gt;现有的多模态大型语言模型（MLLM）在处理复杂的机器人操控任务时存在明显的不足，特别是在规划能力、可操作性感知和轨迹预测三个方面有缺陷。&lt;h4&gt;目的&lt;/h4&gt;为了增强机器人的核心功能，从抽象到具体的操作，提出了ShareRobot数据集以及基于此的RoboBrain模型，旨在解决现有MLLM在机器人场景中的局限性。&lt;h4&gt;方法&lt;/h4&gt;ShareRobot是一个高质量的数据集，包含任务规划、可操作性识别和末端执行器轨迹等多维度信息。该数据集经过三个标注者的细心校正以确保其多样性和准确性。利用该数据集开发了RoboBrain模型，并采用多层次训练策略以及大量的视频和高分辨率图像进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;通过详尽的实验，证明RoboBrain在多种机器人任务中达到了最先进的性能水平。&lt;h4&gt;结论&lt;/h4&gt;这些成果强调了RoboBrain在提高机器人大脑功能方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Multimodal Large Language Models (MLLMs) have shownremarkable capabilities across various multimodal contexts. However, theirapplication in robotic scenarios, particularly for long-horizon manipulationtasks, reveals significant limitations. These limitations arise from thecurrent MLLMs lacking three essential robotic brain capabilities: PlanningCapability, which involves decomposing complex manipulation instructions intomanageable sub-tasks; Affordance Perception, the ability to recognize andinterpret the affordances of interactive objects; and Trajectory Prediction,the foresight to anticipate the complete manipulation trajectory necessary forsuccessful execution. To enhance the robotic brain's core capabilities fromabstract to concrete, we introduce ShareRobot, a high-quality heterogeneousdataset that labels multi-dimensional information such as task planning, objectaffordance, and end-effector trajectory. ShareRobot's diversity and accuracyhave been meticulously refined by three human annotators. Building on thisdataset, we developed RoboBrain, an MLLM-based model that combines robotic andgeneral multi-modal data, utilizes a multi-stage training strategy, andincorporates long videos and high-resolution images to improve its roboticmanipulation capabilities. Extensive experiments demonstrate that RoboBrainachieves state-of-the-art performance across various robotic tasks,highlighting its potential to advance robotic brain capabilities.</description>
      <author>example@mail.com (Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, Shanghang Zhang)</author>
      <guid isPermaLink="false">2502.21257v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction</title>
      <link>http://arxiv.org/abs/2502.21186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR2025. Code would be available at  \href{https://github.com/BaitingLuo/L-MAP.git}{this https URL}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的离线强化学习框架Latent Macro Action Planner (L-MAP)，该框架通过学习一组时间延长的宏动作，解决了在高维连续行为空间中的序列决策问题。&lt;h4&gt;背景&lt;/h4&gt;在具有随机性的环境和高维行动空间中进行顺序决策面临计算挑战。传统离线增强学习设置下，代理必须基于通过随机行为策略收集的数据来学习如何做决定。&lt;h4&gt;目的&lt;/h4&gt;探索如何利用离线强化学习框架解决具有复杂动作空间的序列决策问题。&lt;h4&gt;方法&lt;/h4&gt;L-MAP 采用状态条件下的向量量化变分自动编码器 (VQ-VAE) 学习一组时间扩展的动作，通过这种方式减少动作维度。同时，它使用一个独立的学习先验模型作为潜在转换模型，并允许高效的可能行动采样。在规划过程中，通过蒙特卡洛树搜索（MCTS）来考虑环境和行为策略中的随机性。&lt;h4&gt;主要发现&lt;/h4&gt;L-MAP 能够有效地在线性时间范围内进行离线强化学习任务中的决策，即使在动作维度增加的情况下也能保持较低的延迟，并且在连续控制到高维机器人手部操作等多种任务上都表现出色。&lt;h4&gt;结论&lt;/h4&gt;通过实验证明 L-MAP 在处理复杂和随机环境下的高维行动空间规划问题时是有效的，并能够与现有的模型方法相媲美，同时表现出了比其他基于模型的方法更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;顺序决策在具有高维连续动作空间的随机环境中面临计算挑战。我们探索了传统的离线强化学习框架，在这种情况下，代理必须根据通过随机行为策略收集的数据来学习如何做出决定。我们提出了 Latent Macro Action Planner (L-MAP)，该方法通过对状态条件下的向量量化变分自动编码器进行训练来降低动作维度，并且采用一个独立的学习先验模型作为潜在转换模型和高效的可能行动采样工具。在规划过程中，通过蒙特卡洛树搜索(MCTS) 来考虑环境及行为策略中的随机性。L-MAP 在离线强化学习设置中包括随机连续控制任务时表现高效，能够在线性时间范围内进行决策，并且即使在动作维度增加的情况下也能保持低延迟。实验证明，在从具有内在随机性的连续控制到高维机器人手部操作的任务上，与现有模型方法相比 L-MAP 显著优于其他方法，并表现出与强大的无模型策略-评估者基准线相媲美的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential decision-making in high-dimensional continuous action spaces,particularly in stochastic environments, faces significant computationalchallenges. We explore this challenge in the traditional offline RL setting,where an agent must learn how to make decisions based on data collected througha stochastic behavior policy. We present \textit{Latent Macro Action Planner}(L-MAP), which addresses this challenge by learning a set of temporallyextended macro-actions through a state-conditional Vector Quantized VariationalAutoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employsa (separate) learned prior model that acts as a latent transition model andallows efficient sampling of plausible actions. During planning, our approachaccounts for stochasticity in both the environment and the behavior policy byusing Monte Carlo tree search (MCTS). In offline RL settings, includingstochastic continuous control tasks, L-MAP efficiently searches over discretelatent actions to yield high expected returns. Empirical results demonstratethat L-MAP maintains low decision latency despite increased actiondimensionality. Notably, across tasks ranging from continuous control withinherently stochastic dynamics to high-dimensional robotic hand manipulation,L-MAP significantly outperforms existing model-based methods and performson-par with strong model-free actor-critic baselines, highlighting theeffectiveness of the proposed approach in planning in complex and stochasticenvironments with high-dimensional action spaces.</description>
      <author>example@mail.com (Baiting Luo, Ava Pettet, Aron Laszka, Abhishek Dubey, Ayan Mukhopadhyay)</author>
      <guid isPermaLink="false">2502.21186v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>A Minor-Testing Approach for Coordinated Motion Planning with Sliding Robots</title>
      <link>http://arxiv.org/abs/2502.21175v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了在无向图上的一种协调移动规划问题的变体，即协作滑动运动规划(CSMP)问题。&lt;h4&gt;背景&lt;/h4&gt;CSMP问题涉及给定一个无向图G、k个机器人R1至Rk放置于G的不同顶点，并且p≤k个不同的目标顶点供机器人R1至Rp使用。该问题是NP困难的，尤其是在全网格中。&lt;h4&gt;目的&lt;/h4&gt;研究CSMP在两个参数（即机器人数量k和时间限制l）下的参数复杂性。&lt;h4&gt;方法&lt;/h4&gt;提出了一个固定参数算法来解决CSMP问题，在第一个结果中根据k参数化；在第二个结果中，为特殊情况下只有一个目标顶点的CSMP提出了一个基于l参数化的固定参数算法，并证明了该特殊情况是NP完全的。&lt;h4&gt;主要发现&lt;/h4&gt;解决方案可以表示为输入图的小标记拓扑子图，这是两个结果的关键新元素。&lt;h4&gt;结论&lt;/h4&gt;通过这两个新的算法和理论发现，作者在解决大规模复杂问题方面取得了进展。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了一种无向图上协调运动规划问题的变体——协作滑动运动规划(CSMP)问题。该问题要求判断是否存在一个序列调度，在这个调度中最多包含l次移动，使得每个有目标顶点的机器人能够到达它的目的地。此外，还提出了解决CSMP参数复杂性的固定参数算法，并证明了在特殊情况下是NP完全的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study a variant of the Coordinated Motion Planning problem on undirectedgraphs, referred to herein as the \textsc{Coordinated Sliding-Motion Planning}(CSMP) problem. In this variant, we are given an undirected graph $G$, $k$robots $R_1,\dots,R_k$ positioned on distinct vertices of $G$, $p\leq k$distinct destination vertices for robots $R_1,\dots,R_p$, and $\ell \in\mathbb{N}$. The problem is to decide if there is a serial schedule of at most$\ell$ moves (i.e., of makespan $\ell$) such that at the end of the scheduleeach robot with a destination reaches it, where a robot's move is a free path(unoccupied by any robots) from its current position to an unoccupied vertex.The problem is known to be NP-hard even on full grids. It has been studied inseveral contexts, including coin movement and reconfiguration problems, withrespect to feasibility, complexity, and approximation. Geometric variants ofthe problem, in which congruent geometric-shape robots (e.g., unitdisk/squares) slide or translate in the Euclidean plane, have also been studiedextensively. We investigate the parameterized complexity of CSMP with respectto two parameters: the number $k$ of robots and the makespan $\ell$. As ourfirst result, we present a fixed-parameter algorithm for CSMP parameterized by$k$. For our second result, we present a fixed-parameter algorithmparameterized by $\ell$ for the special case of CSMP in which only a singlerobot has a destination and the graph is planar, which we prove to beNP-complete. A crucial new ingredient for both of our results is that thesolution admits a succinct representation as a small labeled topological minorof the input graph.</description>
      <author>example@mail.com (Eduard Eiben, Robert Ganian, Iyad Kanj, Ramanujan M. Sridharan)</author>
      <guid isPermaLink="false">2502.21175v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Rare event modeling with self-regularized normalizing flows: what can we learn from a single failure?</title>
      <link>http://arxiv.org/abs/2502.21110v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为CalNF（校准归一化流程）的新框架，用于从有限的数据中进行后验学习。这种方法解决了在处理安全关键系统的罕见故障事件时由于数据稀缺而遇到的问题。&lt;h4&gt;背景&lt;/h4&gt;随着自动驾驶系统和机器人技术的广泛应用，与之相关的安全问题也日益凸显。此类系统的故障往往难以通过现有的方法来建模和调试，因为缺乏足够的失败案例的数据支持。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的框架CalNF，以克服由于罕见故障事件数据不足导致的传统模型训练中的局限性，如过拟合或欠拟合等问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自调节的归一化流程技术（Calibrated Normalizing Flows, CalNF），专门设计用于在数据有限的情况下进行后验学习，并且能够在处理逆问题和罕见故障建模时达到最先进的性能水平。&lt;h4&gt;主要发现&lt;/h4&gt;使用CalNF框架能够成功解析2022年美国西南航空公司调度危机的根本原因，这是一项开创性的案例研究。&lt;h4&gt;结论&lt;/h4&gt;CalNF框架为解决由于数据稀缺而引起的罕见安全关键事件的建模难题提供了一个有效的解决方案，并且已经在实际问题中得到了验证。&lt;h4&gt;翻译&lt;/h4&gt;随着无人驾驶系统和机器人技术在运输等领域的部署增加，相应的安全性关键性故障也有所上升。这些故障难以通过现有的方法进行建模和调试，因为缺乏足够的失败案例的数据支持。为了应对这一挑战，研究人员提出了一种名为CalNF的新框架，它利用了自调节的归一化流程技术，特别适用于从有限数据中进行后验学习，并且已经在处理逆问题和罕见故障事件模型方面取得了最先进的性能表现。通过这种方法的应用，能够首次对2022年美国西南航空公司调度危机的根本原因进行了深入分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Increased deployment of autonomous systems in fields like transportation androbotics have seen a corresponding increase in safety-critical failures. Thesefailures can be difficult to model and debug due to the relative lack of data:compared to tens of thousands of examples from normal operations, we may haveonly seconds of data leading up to the failure. This scarcity makes itchallenging to train generative models of rare failure events, as existingmethods risk either overfitting to noise in the limited failure dataset orunderfitting due to an overly strong prior. We address this challenge withCalNF, or calibrated normalizing flows, a self-regularized framework forposterior learning from limited data. CalNF achieves state-of-the-artperformance on data-limited failure modeling and inverse problems and enables afirst-of-a-kind case study into the root causes of the 2022 Southwest Airlinesscheduling crisis.</description>
      <author>example@mail.com (Charles Dawson, Van Tran, Max Z. Li, Chuchu Fan)</author>
      <guid isPermaLink="false">2502.21110v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Jointly Assigning Processes to Machines and Generating Plans for Autonomous Mobile Robots in a Smart Factory</title>
      <link>http://arxiv.org/abs/2502.21101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ACES是一种用于智能工厂的优化算法，可以同时解决工艺分配和路径规划问题。&lt;h4&gt;背景&lt;/h4&gt;现代智能工厂使用可编程机器进行生产，并通过移动机器人运输材料。目前现有的管理系统是顺序地解决问题，这限制了它们所能实现的最大吞吐量。&lt;h4&gt;目的&lt;/h4&gt;介绍ACES（Anytime Cyclic Embedding Solver），这是一种能够同时优化工艺分配和路径规划问题的解决方案。&lt;h4&gt;方法&lt;/h4&gt;ACES可以同时解决智能工厂中的工艺分配和移动机器人运输路线的问题，从而提高整个系统的生产效率。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验评估表明，ACES能够在现实工业场景中进行扩展，并且相较于现有系统，能够实现更高的吞吐量。&lt;h4&gt;结论&lt;/h4&gt;ACES为智能工厂提供了一种有效的解决方案来优化生产和材料运输流程。&lt;h4&gt;翻译&lt;/h4&gt;摘要：现代智能工厂使用一组可编程机器运行制造程序。通常，材料通过一群移动机器人在这些机器之间运送。为了将制造过程嵌入到智能工厂中，工厂操作员必须a) 将其工艺分配给智能工厂的机器，b) 确定代理如何在机器之间运输材料。一个好的嵌入可以最大化智能工厂的吞吐量；即它输出产品的速度。现有的智能工厂管理系统按顺序解决上述问题，限制了它们所能实现的最大吞吐量。在这篇论文中我们介绍了ACES（Anytime Cyclic Embedding Solver），这是一种首次同时优化工艺分配和路径规划问题的解决方案。我们评估了ACES，并表明它可以扩展到现实工业场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A modern smart factory runs a manufacturing procedure using a collection ofprogrammable machines. Typically, materials are ferried between these machinesusing a team of mobile robots. To embed a manufacturing procedure in a smartfactory, a factory operator must a) assign its processes to the smart factory'smachines and b) determine how agents should carry materials between machines. Agood embedding maximizes the smart factory's throughput; the rate at which itoutputs products. Existing smart factory management systems solve theaforementioned problems sequentially, limiting the throughput that they canachieve. In this paper we introduce ACES, the Anytime Cyclic Embedding Solver,the first solver which jointly optimizes the assignment of processes tomachines and the assignment of paths to agents. We evaluate ACES and show thatit can scale to real industrial scenarios.</description>
      <author>example@mail.com (Christopher Leet, Aidan Sciortino, Sven Koenig)</author>
      <guid isPermaLink="false">2502.21101v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>AuthSim: Towards Authentic and Effective Safety-critical Scenario Generation for Autonomous Driving Tests</title>
      <link>http://arxiv.org/abs/2502.21100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;生成对抗性安全关键场景是测试自动驾驶系统的关键方法，有助于识别潜在弱点并增强系统的鲁棒性和可靠性。然而，现有的方法主要关注不受限制的碰撞场景，导致非玩家角色（NPC）车辆无差别攻击主控车。这些研究忽略了这些场景的真实性和合理性，产生了许多极端、人为构造且不现实的涉及激进NPC车辆的碰撞事件。&lt;h4&gt;背景&lt;/h4&gt;当前的方法在测试自动驾驶系统时过于集中于制造不受限制和过度激进的对抗性情况，导致生成的场景缺乏真实性和理性。&lt;h4&gt;目的&lt;/h4&gt;提出一种三层相对安全区域模型，并开发一个名为AuthSim的平台来产生更真实有效的安全关键场景，以解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入了三层相对安全区域模型和结合强化学习的方法，这个模型可以划分基于危险等级的不同区域并调整NPC车辆进入这些边界区域的概率。同时利用该模型与强化学习相结合构建了一个全面平台AuthSim。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示AuthSim在生成有效的安全关键场景方面比现有方法表现更佳，尤其在平均切入距离和平均碰撞间隔时间上分别提高了5.25%和27.12%，并且效率更高。&lt;h4&gt;结论&lt;/h4&gt;这是首次全面解决自动驾驶系统测试场景的真实性和有效性问题的尝试。AuthSim证明了其在生成真实场景方面的显著优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到，对抗性安全关键场景对测试自动驾驶系统的潜在弱点至关重要，并增强其鲁棒性和可靠性。然而，现有的方法过分关注无约束碰撞情况，导致NPC车辆针对主控车发动毫无选择的攻击。这些方法忽视了情景的真实性和合理性，产生了大量极端且不现实的情况，其中涉及的是激进的NPC行为。为解决这些问题，研究团队提出了一种三层相对安全区域模型，并开发了一个名为AuthSim的平台，该平台利用这个模型与强化学习相结合，以产生更加真实和有效的测试场景。实验表明，相对于现有方法，AuthSim在生成有效且关键的安全场景方面表现出色，尤其是在提高平均切入距离和减少碰撞间隔时间上取得了显著改善。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating adversarial safety-critical scenarios is a pivotal method fortesting autonomous driving systems, as it identifies potential weaknesses andenhances system robustness and reliability. However, existing approachespredominantly emphasize unrestricted collision scenarios, prompting non-playercharacter (NPC) vehicles to attack the ego vehicle indiscriminately. Theseworks overlook these scenarios' authenticity, rationality, and relevance,resulting in numerous extreme, contrived, and largely unrealistic collisionevents involving aggressive NPC vehicles. To rectify this issue, we propose athree-layer relative safety region model, which partitions the area based ondanger levels and increases the likelihood of NPC vehicles entering relativeboundary regions. This model directs NPC vehicles to engage in adversarialactions within relatively safe boundary regions, thereby augmenting thescenarios' authenticity. We introduce AuthSim, a comprehensive platform forgenerating authentic and effective safety-critical scenarios by integrating thethree-layer relative safety region model with reinforcement learning. To ourknowledge, this is the first attempt to address the authenticity andeffectiveness of autonomous driving system test scenarios comprehensively.Extensive experiments demonstrate that AuthSim outperforms existing methods ingenerating effective safety-critical scenarios. Notably, AuthSim achieves a5.25% improvement in average cut-in distance and a 27.12% enhancement inaverage collision interval time, while maintaining higher efficiency ingenerating effective safety-critical scenarios compared to existing methods.This underscores its significant advantage in producing authentic scenariosover current methodologies.</description>
      <author>example@mail.com (Yukuan Yang, Xucheng Lu, Zhili Zhang, Zepeng Wu, Guoqi Li, Lingzhong Meng, Yunzhi Xue)</author>
      <guid isPermaLink="false">2502.21100v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Vibrotactile information coding strategies for a body-worn vest to aid robot-human collaboration</title>
      <link>http://arxiv.org/abs/2502.21056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了通过身体穿戴式的振动触觉背心向操作员传递机器人实时信息的方法。研究旨在探索在高认知负荷条件下，如何有效地利用非视觉和非听觉感知方式来传达关键信息。&lt;h4&gt;背景&lt;/h4&gt;在城市搜索与救援（USAR）场景中，当人类与机器人协同工作时，尤其是在高度复杂的环境中，触觉通信可以为操作员提供重要而不影响其视听能力的信息。这种情况通常伴随着高认知负荷条件下的作业。&lt;h4&gt;目的&lt;/h4&gt;本文的目的是通过不同的振动触觉信息编码策略来探讨如何最好地传达此类信息，并引入了语义触觉的概念，以改善在机器人远程侦察时的情景理解。&lt;h4&gt;方法&lt;/h4&gt;文章介绍了一种新的信息表示技术——语义触觉（Semantic Haptics），该技术利用形状和模式来表示特定事件。这种方法试图使皮肤像屏幕一样工作，旨在提高学习能力和解释准确度。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用形状和图案来代表特定的事件，可以实现更好的可学性和解释准确性。这表明在复杂作业环境中采用触觉信息传递的有效性与优势。&lt;h4&gt;结论&lt;/h4&gt;研究结果证明了利用振动背心进行触觉通信在提高操作员对环境理解方面的潜力，并且语义触觉技术可能为未来机器人辅助搜索和救援任务提供有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;原文摘要的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper explores the use of a body-worn vibrotactile vest to conveyreal-time information from robot to operator. Vibrotactile communication couldbe useful in providing information without compropmising or loading a person'svisual or auditory perception. This paper considers applications in UrbanSearch and Rescue (USAR) scenarios where a human working alongside a robot islikely to be operating in high cognitive load conditions. The focus is onunderstanding how best to convey information considering different vibrotactileinformation coding strategies to enhance scene understanding in scenarios wherea robot might be operating remotely as a scout. In exploring informationrepresentation, this paper introduces Semantic Haptics, using shapes andpatterns to represent certain events as if the skin was a screen, and shows howthese lead to bettter learnability and interpreation accuracy.</description>
      <author>example@mail.com (Adrian Vecina Tercero, Praminda Caleb-Solly)</author>
      <guid isPermaLink="false">2502.21056v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model</title>
      <link>http://arxiv.org/abs/2502.16779v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025. Github  page:https://github.com/justacar/Plane-DUSt3R&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多视角房间布局估计方法Plane-DUSt3R，该方法利用了三维基础模型DUSt3R。&lt;h4&gt;背景&lt;/h4&gt;从多个视角的图像中进行房间布局估计的问题由于多视图几何复杂性而研究较少。传统的结构光运动过程需要多步骤解决方案，比如相机内参和外参估计、图像匹配以及三角测量等。&lt;h4&gt;目的&lt;/h4&gt;为了简化房间布局估计的过程并减少误差累积，本文旨在提出一种新的单步端到端方法来处理这个问题。&lt;h4&gt;方法&lt;/h4&gt;Plane-DUSt3R基于DUSt3R框架，并在房间布局数据集（Structure3D）上进行微调，以修改后的目标函数来估算结构平面。此模型能够仅通过一次后处理步骤和二维检测结果来进行房间布局估计。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与现有最先进的方法相比，Plane-DUSt3R不仅在合成数据集中表现更优，在不同图像风格（如卡通）的真实世界数据中也显示了鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;通过引入Plane-DUSt3R，本文提供了一种高效的多视角房间布局估计解决方案，并展示了其在各种环境下的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;从多个视角的图像进行房间布局估计问题因多视图几何复杂性而研究不足。然而，在三维重建领域，近年来出现了像DUSt3R这样的三维基础模型，改变了传统的多步骤结构光运动流程为单步端到端方法。本文引入了Plane-DUSt3R，一种利用DUSt3R框架进行多视角房间布局估计的新方法。该模型在经过修改的目标函数指导下，在一个房间布局数据集上进行了微调，并且能够通过二维检测结果和单一后处理步骤实现高效精确的结构平面预测。不同于以往依赖单视图或全景图像的方法，Plane-DUSt3R扩展了其能力以适应多视角输入，并提供了一种简化的、端到端的解决方案。实验表明，这种方法在合成数据集中超越了现有最佳方法，在不同样式的实际场景（包括卡通风格）中也展示了强大的性能和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Room layout estimation from multiple-perspective images is poorlyinvestigated due to the complexities that emerge from multi-view geometry,which requires muti-step solutions such as camera intrinsic and extrinsicestimation, image matching, and triangulation. However, in 3D reconstruction,the advancement of recent 3D foundation models such as DUSt3R has shifted theparadigm from the traditional multi-step structure-from-motion process to anend-to-end single-step approach. To this end, we introduce Plane-DUSt3R, anovel method for multi-view room layout estimation leveraging the 3D foundationmodel DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes ona room layout dataset (Structure3D) with a modified objective to estimatestructural planes. By generating uniform and parsimonious results, Plane-DUSt3Renables room layout estimation with only a single post-processing step and 2Ddetection results. Unlike previous methods that rely on single-perspective orpanorama image, Plane-DUSt3R extends the setting to handle multiple-perspectiveimages. Moreover, it offers a streamlined, end-to-end solution that simplifiesthe process and reduces error accumulation. Experimental results demonstratethat Plane-DUSt3R not only outperforms state-of-the-art methods on thesynthetic dataset but also proves robust and effective on in the wild data withdifferent image styles such as cartoon.Our code is available at:https://github.com/justacar/Plane-DUSt3R</description>
      <author>example@mail.com (Yaxuan Huang, Xili Dai, Jianan Wang, Xianbiao Qi, Yixing Yuan, Xiangyu Yue)</author>
      <guid isPermaLink="false">2502.16779v2</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Sixth-Sense: Self-Supervised Learning of Spatial Awareness of Humans from a Planar Lidar</title>
      <link>http://arxiv.org/abs/2502.21029v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种从1D激光雷达数据中检测人类并估计其2D姿态的自监督方法，以解决服务机器人在没有RGB-D摄像头或昂贵3D LiDAR的情况下难以感知周围人的局限性。&lt;h4&gt;背景&lt;/h4&gt;当前的服务机器人主要依赖于RGB-D相机或昂贵的3D LiDAR进行人体定位，但商业上常见的服务机器人通常配备视野狭窄的普通相机或者读数难以解析的一维激光雷达。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有设备的成本和功能限制，论文提出了一个利用1D LiDAR数据检测人类并估计其2D姿态的方法，并使用RGB-D摄像头的数据作为监督信号进行训练。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自监督学习框架，该框架通过将从1DLiDAR获得的原始距离信息与从RGB-D相机获取的人体框和关键点进行配对来实现人体检测和姿态估计任务。模型经过70分钟数据（在两个环境自主收集）训练后能够实现在新环境中基于1DLiDAR的数据进行全向人类检测。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在新的未知环境中从单一线激光雷达信息中准确地进行全方位的人体检测，达到71%的精度和80%的召回率，并且在距离上保持了平均绝对误差为13cm，在方向角度上有44度的估计偏差。&lt;h4&gt;结论&lt;/h4&gt;论文提出的方法证明了一维LiDAR可以作为服务机器人实现自主定位与交互的有效感知工具，显著提高了机器人的环境适应性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Localizing humans is a key prerequisite for any service robot operating inproximity to people. In these scenarios, robots rely on a multitude ofstate-of-the-art detectors usually designed to operate with RGB-D cameras orexpensive 3D LiDARs. However, most commercially available service robots areequipped with cameras with a narrow field of view, making them blind when auser is approaching from other directions, or inexpensive 1D LiDARs whosereadings are difficult to interpret. To address these limitations, we propose aself-supervised approach to detect humans and estimate their 2D pose from 1DLiDAR data, using detections from an RGB-D camera as a supervision source. Ourapproach aims to provide service robots with spatial awareness of nearbyhumans. After training on 70 minutes of data autonomously collected in twoenvironments, our model is capable of detecting humans omnidirectionally from1D LiDAR data in a novel environment, with 71% precision and 80% recall, whileretaining an average absolute error of 13 cm in distance and 44{\deg} inorientation.</description>
      <author>example@mail.com (Simone Arreghini, Nicholas Carlotti, Mirko Nava, Antonio Paolillo, Alessandro Giusti)</author>
      <guid isPermaLink="false">2502.21029v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Nano Drone-based Indoor Crime Scene Analysis</title>
      <link>http://arxiv.org/abs/2502.21019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures, to be submitted to ARSO 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文采用投机原型设计的方法，使用STAIR工具快速回顾文献并识别出犯罪现场分析中尚未受到足够关注的任务，并开发了一种小型无人机进行初步验证。&lt;h4&gt;背景&lt;/h4&gt;机器人技术、人工智能和计算机视觉可以应用于犯罪现场分析，以保护生命、促进正义和防止犯罪。但尚缺乏对可自动化任务的全面概述。&lt;h4&gt;目的&lt;/h4&gt;识别并实现犯罪现场分析中可自动化的任务，并通过原型设计展示其可行性和性能。&lt;h4&gt;方法&lt;/h4&gt;采用STAIR工具进行文献回顾，确定了访问犯罪现场（如通过窗户）、绘制和收集证据以及分析血迹等未受足够关注的任务。接着开发了一种小型无人机原型以执行这些任务。&lt;h4&gt;主要发现&lt;/h4&gt;该无人机在三个特定任务中分别达到了75%、85%和80%的性能，展示了技术应用于犯罪现场分析的可能性。&lt;h4&gt;结论&lt;/h4&gt;此次工作通过初步实验为未来的研究提供指导，并强调了进一步研究的必要性以改善自动化系统对复杂犯罪场景的支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：机器人技术、人工智能（AI）和计算机视觉（CV）可以被用于帮助保护生命、促进正义及防止犯罪，但关于可自动化的任务概述却很缺乏。本文采用投机原型设计的方法：首先利用STAIR工具快速回顾文献并识别出访问犯罪现场通过窗户进入等尚未受到足够关注的任务；其次开发一种小型无人机以实现这些任务，并在室内犯罪场景中进行初步分析。最后报告了所学到的经验教训，为后续的研究提供指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Technologies such as robotics, Artificial Intelligence (AI), and ComputerVision (CV) can be applied to crime scene analysis (CSA) to help protect lives,facilitate justice, and deter crime, but an overview of the tasks that can beautomated has been lacking. Here we follow a speculate prototyping approach:First, the STAIR tool is used to rapidly review the literature and identifytasks that seem to have not received much attention, like accessing crime sitesthrough a window, mapping/gathering evidence, and analyzing blood smears.Secondly, we present a prototype of a small drone that implements these threetasks with 75%, 85%, and 80% performance, to perform a minimal analysis of anindoor crime scene. Lessons learned are reported, toward guiding next work inthe area.</description>
      <author>example@mail.com (Martin Cooney, Sivadinesh Ponrajan, Fernando Alonso-Fernandez)</author>
      <guid isPermaLink="false">2502.21019v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Motion ReTouch: Motion Modification Using Four-Channel Bilateral Control</title>
      <link>http://arxiv.org/abs/2502.20982v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 7 figures, Accepted at ICM2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种名为Motion ReTouch的新方法被提出，该方法可以通过多边控制和动作复制系统结合的方式对通过四通道双边控制获得的运动数据进行事后修改。&lt;h4&gt;背景&lt;/h4&gt;最近的研究表明，在自主机器人操作中使用模仿学习是有效的。特别是利用能够获取位置和力信息的四通道双边控制来进行教学已被证明是有效的。&lt;h4&gt;目的&lt;/h4&gt;为了实现能够轻松执行高速、复杂任务的一次性控制性能，本研究提出了一种新的方法。&lt;h4&gt;方法&lt;/h4&gt;该方法称为Motion ReTouch，它不仅能修改运动的位置数据，还能修改力的信息。这通过多边控制和动作复制系统的结合得以实现。&lt;h4&gt;主要发现&lt;/h4&gt;在使用真实机器人进行的实验中，试验表明测试管转移任务的成功率得到了提高，证明了修改力信息的可能性。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了一种新颖的方法来增强模仿学习的效果，并为实现更高性能的任务执行提供了可能途径。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究已经展示了模仿学习在自主机器人操作中的有用性。特别是使用四通道双边控制进行教学已被证明是有效的，它可以获取位置和力的信息。然而，尚未实现能够轻松一次性完成高速、复杂任务的控制性能。我们提出了一种叫做Motion ReTouch的方法，该方法可以通过事后修改通过四通道双边控制获得的运动数据来提高这一能力。此方法不仅可以修改位置信息，还可以修改力信息。这是通过多边控制和动作复制系统的结合得以实现的。在真实机器人的实验中验证了所提出的这种方法，并且提高了测试管转移任务的成功率，表明了修改力信息的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research has demonstrated the usefulness of imitation learning inautonomous robot operation. In particular, teaching using four-channelbilateral control, which can obtain position and force information, has beenproven effective. However, control performance that can easily executehigh-speed, complex tasks in one go has not yet been achieved. We propose amethod called Motion ReTouch, which retroactively modifies motion data obtainedusing four-channel bilateral control. The proposed method enables modificationof not only position but also force information. This was achieved by thecombination of multilateral control and motion-copying system. The proposedmethod was verified in experiments with a real robot, and the success rate ofthe test tube transfer task was improved, demonstrating the possibility ofmodification force information.</description>
      <author>example@mail.com (Koki Inami, Sho Sakaino, Toshiaki Tsuji)</author>
      <guid isPermaLink="false">2502.20982v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Optimality and Suboptimality of MPPI Control in Stochastic and Deterministic Settings</title>
      <link>http://arxiv.org/abs/2502.20953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, submitted to LCSS with CDC25 option&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了MPPI控制框架在解决最优控制问题中的应用及其性能分析。&lt;h4&gt;背景&lt;/h4&gt;Model Predictive Path Integral (MPPI) 控制方法近年来受到广泛关注，特别是在机器人和强化学习领域。&lt;h4&gt;目的&lt;/h4&gt;旨在使MPPI控制框架更容易被最优控制社区理解，并展示其应用于三类最优控制问题的效果及性能。&lt;h4&gt;方法&lt;/h4&gt;研究了在一般确定性非线性离散时间系统中，MPPI控制与最优解之间的次优性。通过数值例子来说明这一分析结果。&lt;h4&gt;主要发现&lt;/h4&gt;在一个平滑且无约束的条件下，随着不确定性的增加，由MPPI提供的控制输入轨迹和最优控制问题解之间的差距增长是二次级别的。并且指出调整超参数可以调节这种次优性。&lt;h4&gt;结论&lt;/h4&gt;通过数值例子验证了上述分析结果，并展示了如何通过适当调整超参数来减轻MPPI解决方案的次优性。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到，最近Model Predictive Path Integral (MPPI) 控制方法在机器人和强化学习领域获得了大量关注。本论文旨在使该控制框架更容易被最优控制社区理解和应用，同时展示了三种不同类型的最优控制问题以及它们通过MPPI得到的解决方案，并研究了确定性非线性离散时间系统中MPPI的次优性能。主要发现表明，在平滑且无约束条件下，随着不确定性的增加，由MPPI提供的控制输入轨迹和最优解之间的差距呈二次级增长。结果还指出，通过适当调整超参数可以调节这种次优性，并用数值例子展示了这些研究结论的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model predictive path integral (MPPI) control has recently received a lot ofattention, especially in the robotics and reinforcement learning communities.This letter aims to make the MPPI control framework more accessible to theoptimal control community. We present three classes of optimal control problemsand their solutions by MPPI. Further, we investigate the suboptimality of MPPIto general deterministic nonlinear discrete-time systems. Here, suboptimalityis defined as the deviation between the control provided by MPPI and theoptimal solution to the deterministic optimal control problem. Our findings arethat in a smooth and unconstrained setting, the growth of suboptimality in thecontrol input trajectory is second-order with the scaling of uncertainty. Theresults indicate that the suboptimality of the MPPI solution can be modulatedby appropriately tuning the hyperparameters. We illustrate our findings usingnumerical examples.</description>
      <author>example@mail.com (Hannes Homburger, Florian Messerer, Moritz Diehl, Johannes Reuter)</author>
      <guid isPermaLink="false">2502.20953v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping</title>
      <link>http://arxiv.org/abs/2502.20900v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;灵巧抓取在机器人技术中仍然是一个基础且具有挑战性的问题。通用机器人的任务是能够应对各种物体的抓取需求，并适应不同的场景。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，即DexGraspVLA框架，以实现更好的跨域迁移能力及泛化性能，从而解决现有研究依赖于特定假设（如单一物体设置或有限环境）而导致泛化受限的问题。&lt;h4&gt;方法&lt;/h4&gt;DexGraspVLA是一种层次化的框架，它利用预训练的视觉-语言模型作为高层次的任务规划器，并学习一种基于扩散的方法作为低级别的动作控制器。其核心在于能够迭代地将多样性的语言和视觉输入转换为领域不变的表示，从而减少域间偏移并使模仿学习更加有效。&lt;h4&gt;主要发现&lt;/h4&gt;在数千种未见过的对象、光照及背景组合的情况下，在零样本环境中达到了90%以上的成功抓取率，并且实验分析表明内部模型行为的一致性随着环境变化而保持稳定。&lt;h4&gt;结论&lt;/h4&gt;该研究通过设计DexGraspVLA框架，展示了实现灵巧抓取的通用能力的可能性，希望为这一领域的进步提供一步推进。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dexterous grasping remains a fundamental yet challenging problem in robotics.A general-purpose robot must be capable of grasping diverse objects inarbitrary scenarios. However, existing research typically relies on specificassumptions, such as single-object settings or limited environments, leading toconstrained generalization. Our solution is DexGraspVLA, a hierarchicalframework that utilizes a pre-trained Vision-Language model as the high-leveltask planner and learns a diffusion-based policy as the low-level Actioncontroller. The key insight lies in iteratively transforming diverse languageand visual inputs into domain-invariant representations, where imitationlearning can be effectively applied due to the alleviation of domain shift.Thus, it enables robust generalization across a wide range of real-worldscenarios. Notably, our method achieves a 90+% success rate under thousands ofunseen object, lighting, and background combinations in a ``zero-shot''environment. Empirical analysis further confirms the consistency of internalmodel behavior across environmental variations, thereby validating our designand explaining its generalization performance. We hope our work can be a stepforward in achieving general dexterous grasping. Our demo and code can be foundat https://dexgraspvla.github.io/.</description>
      <author>example@mail.com (Yifan Zhong, Xuchuan Huang, Ruochong Li, Ceyao Zhang, Yitao Liang, Yaodong Yang, Yuanpei Chen)</author>
      <guid isPermaLink="false">2502.20900v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical and Modular Network on Non-prehensile Manipulation in General Environments</title>
      <link>http://arxiv.org/abs/2502.20843v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  http://unicorn-hamnet.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种模块化和可重构的架构，以解决机器人在不同几何环境中执行非抓握操作时面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;机器人需要具备非抓取的操作能力来处理不可抓取的对象，如物体倾倒和滚动。然而现有的方法难以适应环境的变化，导致策略难以泛化。&lt;h4&gt;目的&lt;/h4&gt;研究如何让机器人能够根据不同的任务需求自适应地改变其行为模式，以应对复杂多变的几何约束。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的架构设计和接触为基础的对象表示（CORN）的扩展版本来处理环境变化。还开发了一个生成多样环境的算法来训练机器人，并发布了一个包含真实场景数字模型的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的适应性架构能够在完全没有见过的真实世界环境中进行零样本转移，显示出其泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了使用模块化和可重构设计可以解决非抓握操作的通用性和适应性的关键问题。&lt;h4&gt;翻译&lt;/h4&gt;摘要：为了使机器人能够在家用等一般环境运行，它们必须能够执行像倾倒和滚动这样的非抓取行为来处理不可抓取的对象。然而现有的关于非抓取操纵的研究还不能泛化到几何结构多样的环境中去。主要挑战在于适应不同的环境约束：在一个橱柜内，机器人需要避开墙壁和天花板；要将物体提升至阶梯顶部，则必须考虑阶梯的姿态和延伸度。虽然深度强化学习（RL）在非抓握操作方面已经取得了显著的成功，但是考虑到这种变化性为泛化策略带来了挑战，因为这需要从每个新的约束组合中学习不同的策略。为了应对这一挑战，我们提出了一种模块化且可重构的架构，该架构能够根据任务需求自适应地重新配置网络模块。为了捕捉环境中几何结构的变化，我们将基于接触的对象表示（CORN）扩展到环境几何，并提出了一个生成多样环境以训练我们的代理的程序算法。综上所述，所得到的策略可以在完全没有见过的真实世界环境中进行零样本转移，尽管是在模拟器中完成的所有培训。此外，我们还发布了以九个真实场景数字模型为特征的数据集来支持现实领域的非抓握操作研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For robots to operate in general environments like households, they must beable to perform non-prehensile manipulation actions such as toppling androlling to manipulate ungraspable objects. However, prior works onnon-prehensile manipulation cannot yet generalize across environments withdiverse geometries. The main challenge lies in adapting to varyingenvironmental constraints: within a cabinet, the robot must avoid walls andceilings; to lift objects to the top of a step, the robot must account for thestep's pose and extent. While deep reinforcement learning (RL) has demonstratedimpressive success in non-prehensile manipulation, accounting for suchvariability presents a challenge for the generalist policy, as it must learndiverse strategies for each new combination of constraints. To address this, wepropose a modular and reconfigurable architecture that adaptively reconfiguresnetwork modules based on task requirements. To capture the geometricvariability in environments, we extend the contact-based object representation(CORN) to environment geometries, and propose a procedural algorithm forgenerating diverse environments to train our agent. Taken together, theresulting policy can zero-shot transfer to novel real-world environments andobjects despite training entirely within a simulator. We additionally release asimulation-based benchmark featuring nine digital twins of real-world sceneswith 353 objects to facilitate non-prehensile manipulation research inrealistic domains.</description>
      <author>example@mail.com (Yoonyoung Cho, Junhyek Han, Jisu Han, Beomjoon Kim)</author>
      <guid isPermaLink="false">2502.20843v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Learning-Based Leader Localization for Underwater Vehicles With Optical-Acoustic-Pressure Sensor Fusion</title>
      <link>http://arxiv.org/abs/2502.20817v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种三模态传感器融合神经网络方法，用于提高水下多车辆系统中领导者定位的精度和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;水下航行器在探索和监测海洋环境方面发挥着关键作用。多车辆系统的部署因其能够执行协作任务而引起广泛关注，但要在动态复杂的水下环境中精确确定领导者的方位仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;通过整合光学、声学和压力传感器来解决领导者定位的精度问题，并提高其鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;利用深度学习架构将三模态传感器（光学传感器提供高分辨率成像，声学传感器实现远程探测和测距，而压力传感器则感知环境背景信息）的数据融合起来以提取并结合互补特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明提出的三模态方法显著提高了领导者定位的准确性和鲁棒性，优于单模态或双模态的方法。&lt;h4&gt;结论&lt;/h4&gt;这项研究展示了如何通过智能地利用不同类型的传感器数据来提高水下多车辆系统的性能。这种方法有望在未来的海洋科学研究和应用中发挥重要作用。&lt;h4&gt;翻译&lt;/h4&gt;水下航行器已经成为探索和监测水域环境的关键技术，部署多车系统进行协同任务变得越来越受欢迎，但要在一个动态的复杂环境中精确定位领导者则仍然具有挑战性。本文提出了一种新颖的三模态传感器融合神经网络方法，该方法整合了光学、声学和压力传感器来定位领导者。通过利用各个传感模式的独特优势，这种方法提高了定位精度和鲁棒性。实验结果表明，这种三模态方法在准确性与鲁棒性上显著优于单一或双模态的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater vehicles have emerged as a critical technology for exploring andmonitoring aquatic environments. The deployment of multi-vehicle systems hasgained substantial interest due to their capability to perform collaborativetasks with improved efficiency. However, achieving precise localization of aleader underwater vehicle within a multi-vehicle configuration remains asignificant challenge, particularly in dynamic and complex underwaterconditions. To address this issue, this paper presents a novel tri-modal sensorfusion neural network approach that integrates optical, acoustic, and pressuresensors to localize the leader vehicle. The proposed method leverages theunique strengths of each sensor modality to improve localization accuracy androbustness. Specifically, optical sensors provide high-resolution imaging forprecise relative positioning, acoustic sensors enable long-range detection andranging, and pressure sensors offer environmental context awareness. The fusionof these sensor modalities is implemented using a deep learning architecturedesigned to extract and combine complementary features from raw sensor data.The effectiveness of the proposed method is validated through a custom-designedtesting platform. Extensive data collection and experimental evaluationsdemonstrate that the tri-modal approach significantly improves the accuracy androbustness of leader localization, outperforming both single-modal anddual-modal methods.</description>
      <author>example@mail.com (Mingyang Yang, Zeyu Sha, Feitian Zhang)</author>
      <guid isPermaLink="false">2502.20817v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Towards Semantic 3D Hand-Object Interaction Generation via Functional Text Guidance</title>
      <link>http://arxiv.org/abs/2502.20805v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种创新的两阶段框架Functional Grasp Synthesis Net (FGS-Net)，旨在基于功能文本生成手部与物体之间的三维交互姿态。&lt;h4&gt;背景&lt;/h4&gt;手势控制在人机交互中扮演关键角色，但由于抓取动作的复杂性和多样性，现有的技术难以捕捉功能性抓握任务的意义。&lt;h4&gt;目的&lt;/h4&gt;为了实现功能性的三维抓握手部-对象交互（HOI）生成，论文提出了一种新的框架来解决现有方法无法精确模拟功能性抓握的问题。&lt;h4&gt;方法&lt;/h4&gt;该方法包括一个文本引导的3D模型生成器Functional Grasp Generator (FGG)和一个姿态优化策略Functional Grasp Refiner (FGR)，前者基于文本输入生成手部与物体的3D模型，后者利用Object Pose Approximator和能量函数来调整姿态以确保符合人体意图。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够产生精确且高质量的手部-对象交互（HOI）而无需额外的三维标注数据。&lt;h4&gt;结论&lt;/h4&gt;通过实验验证了FGS-Net的有效性，在生成功能性抓握手部-物体交互方面表现出色，具有广泛应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;手部与物体之间的互动是人与环境之间的重要联系。尽管AI和机器人技术已取得重大进展，但捕捉功能抓握任务的语义仍是一个挑战。本文提出了一种新的两阶段框架Functional Grasp Synthesis Net (FGS-Net)，用于基于功能性文本生成3D HOI。该方法结合了基于文本引导的3D模型生成器和姿态优化策略，以确保手部与物体之间的相对位置符合人类意图并保持物理合理性。实验表明，在无需额外三维标注数据的情况下，我们的方法可以实现精确且高质量的手部-对象交互生成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hand-object interaction(HOI) is the fundamental link between human andenvironment, yet its dexterous and complex pose significantly challenges forgesture control. Despite significant advances in AI and robotics, enablingmachines to understand and simulate hand-object interactions, capturing thesemantics of functional grasping tasks remains a considerable challenge. Whileprevious work can generate stable and correct 3D grasps, they are still farfrom achieving functional grasps due to unconsidered grasp semantics. Toaddress this challenge, we propose an innovative two-stage framework,Functional Grasp Synthesis Net (FGS-Net), for generating 3D HOI driven byfunctional text. This framework consists of a text-guided 3D model generator,Functional Grasp Generator (FGG), and a pose optimization strategy, FunctionalGrasp Refiner (FGR). FGG generates 3D models of hands and objects based on textinput, while FGR fine-tunes the poses using Object Pose Approximator and energyfunctions to ensure the relative position between the hand and object alignswith human intent and remains physically plausible. Extensive experimentsdemonstrate that our approach achieves precise and high-quality HOI generationwithout requiring additional 3D annotation data.</description>
      <author>example@mail.com (Yongqi Tian, Xueyu Sun, Haoyuan He, Linji Hao, Ning Ding, Caigui Jiang)</author>
      <guid isPermaLink="false">2502.20805v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Characteristics Analysis of Autonomous Vehicle Pre-crash Scenarios</title>
      <link>http://arxiv.org/abs/2502.20789v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文分析了加州的自动驾驶车辆碰撞报告，并基于修订后的预碰撞场景分类法，提出了一套自动提取预碰撞场景的映射规则。&lt;h4&gt;背景&lt;/h4&gt;近年来，在自动驾驶车辆（AV）的道路测试中发生了数百起事故，强调了提高其可靠性和安全性的重要性。现有的研究主要集中在传统的人类驾驶车辆上的碰撞分析上，缺少专门针对自动驾驶汽车深入碰撞分析的研究。&lt;h4&gt;目的&lt;/h4&gt;通过分析最新的加州自动驾驶车辆碰撞报告和使用修订后的预碰撞场景分类法来识别自动驾驶车辆的预碰撞场景，并提出优化建议以提高其性能。&lt;h4&gt;方法&lt;/h4&gt;提出了用于自动提取24种不同类型预碰撞场景（准确率为98.1%）的映射规则，特别关注了追尾场景和交叉口场景两大关键场景。对这些场景进行了详细的环境因素分析和因果关系分析。&lt;h4&gt;主要发现&lt;/h4&gt;在追尾场景中，交通控制类型、地点类型以及光线等是重要因素；对于可能引发严重碰撞事故的交叉口场景，则识别出了惯常违反规则与期望特定行为为主要原因。&lt;h4&gt;结论&lt;/h4&gt;本文的研究成果可帮助政府机构制定相关法规，指导制造商设计测试场景，并改进控制算法以优化自动驾驶系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;迄今为止，在自动车辆（AV）的道路测试中发生了数百起事故，突显了提高其可靠性和安全性的必要性。基于碰撞前情景分类法对交通事故进行分类，该方法依据车辆动态和运动学特征。在此基础上，特性分析可以识别类似特征下的相似碰撞情况，为更有效地反映一般碰撞模式并提供更具针对性的建议以提升AV性能提供了可能。然而，目前的研究主要集中在传统的人类驾驶车辆上发生的碰撞事件，缺乏专门针对自动驾驶汽车深度碰撞分析的研究内容。本文中，我们分析了最新的加州AV碰撞报告，并使用新修订的预碰撞场景分类法来识别这些碰撞情况下的预碰撞情景类型。我们提出了一套映射规则以自动提取这些AV预碰撞场景，并成功确定了24种不同类型的预碰撞场景（准确率为98.1%），并通过详细的分析获取了两个关键的AV碰撞场景，即追尾场景和交叉口场景。对追尾场景进行关联性分析后发现，显著的环境影响因素包括交通控制类型、地点类型以及光线等；对于可能引发严重事故的交叉口场景，则通过因果关系分析确定出了惯常违反规则与期望特定行为为主要成因。随后，我们制定了优化建议，既考虑了政府监管方面的需求，也针对AV制造商潜在改进进行了探讨。本文的研究成果能够帮助政府部门制定相关法规，并协助制造厂商设计出更有效的测试场景，在各种现实世界情景中识别出控制系统算法的潜在缺陷并加以优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To date, hundreds of crashes have occurred in open road testing of automatedvehicles (AVs), highlighting the need for improving AV reliability and safety.Pre-crash scenario typology classifies crashes based on vehicle dynamics andkinematics features. Building on this, characteristics analysis can identifysimilar features under comparable crashes, offering a more effective reflectionof general crash patterns and providing more targeted recommendations forenhancing AV performance. However, current studies primarily concentrated oncrashes among conventional human-driven vehicles, leaving a gap in researchdedicated to in-depth AV crash analyses. In this paper, we analyzed the latestCalifornia AV collision reports and used the newly revised pre-crash scenariotypology to identify pre-crash scenarios. We proposed a set of mapping rulesfor automatically extracting these AV pre-crash scenarios, successfullyidentifying 24 types with a 98.1% accuracy rate, and obtaining two keyscenarios of AV crashes (i.e., rear-end scenarios and intersection scenarios)through detailed analysis. Association analyses of rear-end scenarios showedthat the significant environmental influencing factors were traffic controltype, location type, light, etc. For intersection scenarios prone to severecrashes with detailed descriptions, we employed causal analyses to obtain thesignificant causal factors: habitual violations and expectations of certainbehavior. Optimization recommendations were then formulated, addressing bothgovernmental oversight and AV manufacturers' potential improvements. Thefindings of this paper could guide government authorities to develop relatedregulations, help manufacturers design AV test scenarios, and identifypotential shortcomings in control algorithms specific to various real-worldscenarios, thereby optimizing AV systems effectively.</description>
      <author>example@mail.com (Yixuan Li, Xuesong Wang, Tianyi Wang, Qian Liu)</author>
      <guid isPermaLink="false">2502.20789v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>CSubBT: A Self-Adjusting Execution Framework for Mobile Manipulation System</title>
      <link>http://arxiv.org/abs/2502.20771v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于行为树的条件子树(CSubBT)框架，用于移动机器人在非结构化环境中执行任务时调整执行策略。&lt;h4&gt;背景&lt;/h4&gt;现代智能技术的发展使得装备有机械臂的移动机器人越来越多地应用于非结构化的环境。这些机器人可以根据感知到的信息规划出完成长期任务所需的行动序列，但实际操作中由于感知信息与实际情况不符导致计划失败的情况十分常见。&lt;h4&gt;目的&lt;/h4&gt;为了提高移动机器人的执行成功率和适应性，提出了一种能够自我调整的执行框架CSubBT。&lt;h4&gt;方法&lt;/h4&gt;CSubBT通过将象征性的动作分解为子动作，并使用行为树来控制这些子动作的执行，解决执行过程中的异常问题。该框架认为常见的异常问题是约束条件未满足的问题，在检测到异常时会指导机器人在约束空间内采样新的行动参数。&lt;h4&gt;主要发现&lt;/h4&gt;CSubBT能够有效应对移动机器人的任务执行过程中遇到的各种异常情况，并通过广泛的仿真和现实世界的实验展示了其鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;提出的CSubBT框架为解决移动机械臂操作中的计划与实际情况不匹配的问题提供了一种有效的解决方案，具有较好的实用性和推广价值。&lt;h4&gt;翻译&lt;/h4&gt;随着现代智能技术的进步，配备有机械臂的移动机器人越来越多地在非结构化的环境中运行。这些机器人可以基于感知信息制定长期任务的动作序列规划。然而，在实践中，由于规划所用的感知信息与实际情况不一致，计划经常失败。本文提出了一种基于行为树（BT）的行为子树（CSubBT），这是一种为具有机械臂的任务的移动机器人的自适应执行而设计的一般框架。CSubBT将象征性动作分解成子动作，并使用BT来控制它们的执行，在过程中处理任何可能的异常情况。当检测到异常时，它会通过在约束空间内采样新的操作参数连续指导机器人完成任务。我们通过广泛的模拟和现实世界环境中的机械臂实验展示了该框架的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the advancements in modern intelligent technologies, mobile robotsequipped with manipulators are increasingly operating in unstructuredenvironments. These robots can plan sequences of actions for long-horizon tasksbased on perceived information. However, in practice, the planned actions oftenfail due to discrepancies between the perceptual information used for planningand the actual conditions. In this paper, we introduce the {\itshapeConditional Subtree} (CSubBT), a general self-adjusting execution framework formobile manipulation tasks based on Behavior Trees (BTs). CSubBT decomposessymbolic action into sub-actions and uses BTs to control their execution,addressing any potential anomalies during the process. CSubBT treats commonanomalies as constraint non-satisfaction problems and continuously guides therobot in performing tasks by sampling new action parameters in the constraintspace when anomalies are detected. We demonstrate the robustness of ourframework through extensive manipulation experiments on different platforms,both in simulation and real-world settings.</description>
      <author>example@mail.com (Huihui Guo, Huizhang Luo, Huilong Pi, Mingxing Duan, Kenli Li, Chubo Liu)</author>
      <guid isPermaLink="false">2502.20771v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>A2DO: Adaptive Anti-Degradation Odometry with Deep Multi-Sensor Fusion for Autonomous Navigation</title>
      <link>http://arxiv.org/abs/2502.20767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6+1pages, 6 figures, accept by ICRA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为A2DO的新系统，该系统通过深度神经网络在挑战性条件下提高了SLAM系统的鲁棒性和定位准确性。&lt;h4&gt;背景&lt;/h4&gt;自主车辆的安全和有效导航需要精确的定位，而同时进行定位与地图构建（SLAM）技术是实现这一目标的关键。然而，在不良天气、光线不足或障碍物等情况下，传感器退化会影响SLAM系统的性能。&lt;h4&gt;目的&lt;/h4&gt;通过结合多传感器数据并利用深度学习技术来提高在各种复杂条件下的自主车辆导航系统精度和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;A2DO是一个端到端的融合了LiDAR与视觉数据的里程计系统，它使用了一个多层次、多尺度特征编码模块，并引入注意力机制以动态减轻传感器退化问题。该模型在广泛覆盖各种退化场景的模拟数据集上进行了预训练，并通过精选的真实世界数据进一步微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，A2DO能够在多种传感器退化条件下保持优越的定位准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了一个具备潜在实用价值的方法来提高自主车辆导航系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;精确的定位对于自主车辆的安全和有效导航至关重要。SLAM是这个领域中的关键技术，但在诸如低光照、恶劣天气或传感器退化等不利条件下，其表现会下降。我们提出了一个基于深度神经网络的新系统A2DO，旨在通过融合LiDAR和视觉数据来提升这些情况下的鲁棒性。该系统采用了多层、多尺度特征编码模块并加入了注意力机制以动态解决传感器退化问题，并且在模拟及真实世界的数据集上进行了广泛的训练与微调。实验结果表明，A2DO能够在各种传感器降级条件下保持卓越的定位准确性和稳定性，展示出其实用实施于自主车辆系统中的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localization is essential for the safe and effective navigation ofautonomous vehicles, and Simultaneous Localization and Mapping (SLAM) is acornerstone technology in this context. However, The performance of the SLAMsystem can deteriorate under challenging conditions such as low light, adverseweather, or obstructions due to sensor degradation. We present A2DO, a novelend-to-end multi-sensor fusion odometry system that enhances robustness inthese scenarios through deep neural networks. A2DO integrates LiDAR and visualdata, employing a multi-layer, multi-scale feature encoding module augmented byan attention mechanism to mitigate sensor degradation dynamically. The systemis pre-trained extensively on simulated datasets covering a broad range ofdegradation scenarios and fine-tuned on a curated set of real-world data,ensuring robust adaptation to complex scenarios. Our experiments demonstratethat A2DO maintains superior localization accuracy and robustness acrossvarious degradation conditions, showcasing its potential for practicalimplementation in autonomous vehicle systems.</description>
      <author>example@mail.com (Hui Lai, Qi Chen, Junping Zhang, Jian Pu)</author>
      <guid isPermaLink="false">2502.20767v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Acquiring Grounded Representations of Words with Situated Interactive Instruction</title>
      <link>http://arxiv.org/abs/2502.20754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种通过混合主动性、情境化的互动与人类指导者交流来获取单词的接地表示的方法。&lt;h4&gt;背景&lt;/h4&gt;研究集中在从感知知识、语义知识和程序知识中获得多样化类型的知识，并学习它们的接地含义。&lt;h4&gt;目的&lt;/h4&gt;允许代理通过请求关于未知概念的指令，控制其学习过程，从而提高学习效率。&lt;h4&gt;方法&lt;/h4&gt;该方法在Soar系统中实现了，并在一个能够操控小型物体的桌面机器人手臂上进行了测试和评估。&lt;h4&gt;主要发现&lt;/h4&gt;交互式学习使得智能体可以高效地获取不同类型的知识并理解和操作未知概念。&lt;h4&gt;结论&lt;/h4&gt;提出的这种方法有效地提高了代理通过与人类互动来学习复杂知识的能力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种从混合主动性和情境化的互动中，通过与人类指导者的交流获得单词的接地表示的方法。该方法关注于多样化类型的知识获取，包括感知、语义和程序性知识，并且旨在学习这些概念的接地含义。交互式的学习允许智能体控制其学习过程，通过请求有关未知概念的指令，从而使其学习过程更加高效。这种方法在Soar系统中实现，并在一个能够操控小型物体的桌面机器人手臂上进行了评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an approach for acquiring grounded representations of words frommixed-initiative, situated interactions with a human instructor. The workfocuses on the acquisition of diverse types of knowledge including perceptual,semantic, and procedural knowledge along with learning grounded meanings.Interactive learning allows the agent to control its learning by requestinginstructions about unknown concepts, making learning efficient. Our approachhas been instantiated in Soar and has been evaluated on a table-top robotic armcapable of manipulating small objects.</description>
      <author>example@mail.com (Shiwali Mohan, Aaron H. Mininger, James R. Kirk, John E. Laird)</author>
      <guid isPermaLink="false">2502.20754v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Indoor Localization for Autonomous Robot Navigation</title>
      <link>http://arxiv.org/abs/2502.20731v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了室内定位系统在自主机器人导航中的应用，研究团队收集了一个数据集并训练模型以测试机器人，并开发了一种A*路径规划算法。经过不同网络结构的测试，机器人的成功转弯率为50%左右。&lt;h4&gt;背景&lt;/h4&gt;随着户外导航技术的发展，在日常生活中的重要性日益增加，室内定位系统（IPS）受到了广泛关注和研究。&lt;h4&gt;目的&lt;/h4&gt;探索利用室内定位系统完成自主机器人在室内的导航任务。&lt;h4&gt;方法&lt;/h4&gt;1. 收集并训练数据集以测试机器人；2. 开发A*路径规划算法使机器人能够使用预测方向自我导航。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验，机器人的成功转弯率为50%左右。&lt;h4&gt;结论&lt;/h4&gt;利用室内定位系统进行自主机器人导航是未来研究的一个有前途的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Indoor positioning systems (IPSs) have gained attention as outdoor navigationbecomes prevalent in everyday life. Research is being actively conducted on howindoor smartphone navigation can be accomplished and improved using receivedsignal strength indication (RSSI) and machine learning (ML). IPSs have more usecases that need further exploration, and we aim to explore using IPSs for theindoor navigation of an autonomous robot. We collected a dataset and trainedmodels to test on a robot. We also developed an A* path-planning algorithm sothat our robot could navigate itself using predicted directions. After testingdifferent network structures, our robot was able to successfully navigatecorners around 50 percent of the time. The findings of this paper indicate thatusing IPSs for autonomous robots is a promising area of future research.</description>
      <author>example@mail.com (Sean Kouma, Rachel Masters)</author>
      <guid isPermaLink="false">2502.20731v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>FSMP: A Frontier-Sampling-Mixed Planner for Fast Autonomous Exploration of Complex and Large 3-D Environments</title>
      <link>http://arxiv.org/abs/2502.20707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13pages, 12 figures, accepted by IEEE Transactions on Instrumentation  and Measurement&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种使用微型空中飞行器（MAVs）快速探索复杂且庞大的3-D环境的系统框架。&lt;h4&gt;背景&lt;/h4&gt;在复杂和大规模的三维环境中进行有效的探索具有挑战性，传统的基于前沿或随机采样的方法难以实现高效和完整的覆盖。&lt;h4&gt;目的&lt;/h4&gt;为了提高探索效率并确保完整性和可靠性，提出了一种结合了基于前沿的方法和基于采样策略的新型框架。&lt;h4&gt;方法&lt;/h4&gt;{'前端检测器': '设计了一个以视野为基础（FOV）的前沿探测器，该探测器保证完成度和正确性', '确定性采样技术': '采用确定性的采样技术来建立和维护基于记录的传感器视场和新检测到的前沿的增量式道路地图。', '路径规划器': '提出了一个两阶段路径规划算法：第一阶段使用惰性评估策略快速计算全局最优探索路线；第二阶段对最佳探索路径进行平滑处理以进一步提高探索效率。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果验证了该方法在模拟和现实环境中的有效性，展示了其在探索效率、计算时间和已探索体积方面的出色表现。&lt;h4&gt;结论&lt;/h4&gt;所提出的快速探索框架为复杂3-D环境下的MAV应用提供了一种高效且可靠的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们提出了一种使用微型空中飞行器（MAVs）的系统性框架，用于快速探索复杂的大型三维环境。该方法的核心见解在于有机地整合基于前沿和采样策略的方法，以实现环境的整体快速探索。设计了一个基于视野（FOV）的前端检测器，确保了完整性和正确性的前提下识别三维地图边界。与随机采样法不同的是，采用确定性采样技术来建立并维护一种增量式道路图，该图依赖于记录下来的传感器视场以及新发现的前沿。利用所构建的道路图，提出了一种两阶段路径规划算法：首阶段快速计算出全局最优探索路线；次阶段则进一步平滑优化此最佳路线以提高效率。文中通过仿真和现实世界中的实验验证了这一方法的有效性，并且比较结果表明该框架在探索效率、计算时间和已探索体积方面表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a systematic framework for fast exploration ofcomplex and large 3-D environments using micro aerial vehicles (MAVs). The keyinsight is the organic integration of the frontier-based and sampling-basedstrategies that can achieve rapid global exploration of the environment.Specifically, a field-of-view-based (FOV) frontier detector with the guaranteeof completeness and soundness is devised for identifying 3-D map frontiers.Different from random sampling-based methods, the deterministic samplingtechnique is employed to build and maintain an incremental road map based onthe recorded sensor FOVs and newly detected frontiers. With the resulting roadmap, we propose a two-stage path planner. First, it quickly computes the globaloptimal exploration path on the road map using the lazy evaluation strategy.Then, the best exploration path is smoothed for further improving theexploration efficiency. We validate the proposed method both in simulation andreal-world experiments. The comparative results demonstrate the promisingperformance of our planner in terms of exploration efficiency, computationaltime, and explored volume.</description>
      <author>example@mail.com (Shiyong Zhang, Xuebo Zhang, Qianli Dong, Ziyu Wang, Haobo Xi, Jing Yuan)</author>
      <guid isPermaLink="false">2502.20707v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>WorldModelBench: Judging Video Generation Models As World Models</title>
      <link>http://arxiv.org/abs/2502.20694v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;视频生成模型在快速发展，能够支持决策应用如机器人和自动驾驶。然而现有的基准测试未能严格评估这些能力。&lt;h4&gt;背景&lt;/h4&gt;当前的基准测试只关注通用视频质量，忽略了世界建模所需的重要因素，例如物理规则遵守情况。&lt;h4&gt;目的&lt;/h4&gt;为了弥补这一差距，我们提出了WorldModelBench，一个旨在评价视频生成模型在应用驱动领域中世界建模能力的基准。&lt;h4&gt;方法&lt;/h4&gt;{'优势1': '通过引入指令跟随和物理规则遵守维度，WorldModelBench能检测细微的违规情况，如违反质量守恒定律的对象尺寸不规则变化的问题，这些问题被之前的基准所忽视。', '优势2': '通过大规模的人类偏好对齐，我们收集了67K人类标签来准确测量14个前沿模型。使用高质量的人类标签进一步微调了一个精确的评判者以自动化评估过程，并实现了比GPT-4o更高的预测世界建模违规平均精度。', '其他贡献': '训练模型与人工注释对齐，最大化来自评判者的奖励可以显著提高世界的建模能力'}&lt;h4&gt;主要发现&lt;/h4&gt;通过引入特定维度来检测细微问题和大规模的人类偏好对齐方法，WorldModelBench能更全面地评估视频生成模型的世界建模能力。&lt;h4&gt;结论&lt;/h4&gt;我们的研究强调了现有基准测试的局限性，并提出了一种改进的方法来准确评价这些模型的能力。该网站可以访问https://worldmodelbench-team.github.io&lt;h4&gt;翻译&lt;/h4&gt;视频生成模型正在迅速发展，将自己定位为支持决策应用（如机器人技术与自动驾驶）的世界模型。然而，当前的评估基准未能严格验证这些声明，只关注通用视频质量，忽略世界建模所需的重要因素，比如物理一致性等。为解决这一问题，我们提出了WorldModelBench，一个旨在测试视频生成模型在应用驱动领域的世界建模能力的新标准。该基准的主要优势在于：1）能够检测细微的世界建模违规情况，如违反了质量守恒定律的对象尺寸不规则变化；2）通过大规模的人类偏好对齐方法进行准确评估，利用67K人类标签来衡量多个前沿模型，并微调评判者以自动化这一过程。结果表明，该方法比GPT-4o更为精确地预测世界建模违规情况。此外，我们展示了训练与人工注释对齐可以显著提高世界的建模能力。有关WorldModelBench的更多信息，请访问https://worldmodelbench-team.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video generation models have rapidly progressed, positioning themselves asvideo world models capable of supporting decision-making applications likerobotics and autonomous driving. However, current benchmarks fail to rigorouslyevaluate these claims, focusing only on general video quality, ignoringimportant factors to world models such as physics adherence. To bridge thisgap, we propose WorldModelBench, a benchmark designed to evaluate the worldmodeling capabilities of video generation models in application-driven domains.WorldModelBench offers two key advantages: (1) Against to nuanced worldmodeling violations: By incorporating instruction-following andphysics-adherence dimensions, WorldModelBench detects subtle violations, suchas irregular changes in object size that breach the mass conservation law -issues overlooked by prior benchmarks. (2) Aligned with large-scale humanpreferences: We crowd-source 67K human labels to accurately measure 14 frontiermodels. Using our high-quality human labels, we further fine-tune an accuratejudger to automate the evaluation procedure, achieving 8.6% higher averageaccuracy in predicting world modeling violations than GPT-4o with 2Bparameters. In addition, we demonstrate that training to align humanannotations by maximizing the rewards from the judger noticeably improve theworld modeling capability. The website is available athttps://worldmodelbench-team.github.io.</description>
      <author>example@mail.com (Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu Yin, Joseph E. Gonzalez, Ion Stoica, Song Han, Yao Lu)</author>
      <guid isPermaLink="false">2502.20694v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>From Safety Standards to Safe Operation with Mobile Robotic Systems Deployment</title>
      <link>http://arxiv.org/abs/2502.20693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper published at "Workshop on Design, Learning, and control for  safe human-robot collaboration at the International Conference on Advanced  Robotics (ICAR)"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文审查了移动机器人在工作场所部署的安全标准和方法，并提出了一个新的风险评估框架来确保建筑工地上的安全使用。&lt;h4&gt;背景&lt;/h4&gt;移动机器人被广泛应用于各种工作环境以提高生产效率，但在拥挤且有危险的环境中与人类工人互动存在安全挑战。&lt;h4&gt;目的&lt;/h4&gt;为解决移动机器人在施工场地部署时的安全问题，提出一套全面的风险评估方法，并通过领域专家验证其有效性。&lt;h4&gt;方法&lt;/h4&gt;首先回顾了现有标准和相关研究文献，然后基于这些信息提出了一个改进的风险评估框架以覆盖未被现有的安全性指南涵盖的情景。&lt;h4&gt;主要发现&lt;/h4&gt;新的风险评估框架可以更好地保护工人免受移动机器人操作的潜在危害，并提供了具体的建议来降低此类风险。&lt;h4&gt;结论&lt;/h4&gt;通过扩展现有安全标准并提出额外的安全措施，该研究有助于更安全地部署和使用移动机器人在建筑工地和其他复杂环境中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobile robotic systems are increasingly used in various work environments tosupport productivity. However, deploying robots in workplaces crowded by humanworkers and interacting with them results in safety challenges and concerns,namely robot-worker collisions and worker distractions in hazardousenvironments. Moreover, the literature on risk assessment as well as thestandard specific to mobile platforms is rather limited. In this context, thispaper first conducts a review of the relevant standards and methodologies andthen proposes a risk assessment for the safe deployment of mobile robots onconstruction sites. The approach extends relevant existing safety standards toencompass uncovered scenarios. Safety recommendations are made based on theframework, after its validation by field experts.</description>
      <author>example@mail.com (Bruno Belzile, Tatiana Wanang-Siyapdjie, Sina Karimi, Rafael Gomes Braga, Ivanka Iordanova, David St-Onge)</author>
      <guid isPermaLink="false">2502.20693v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>The Common Objects Underwater (COU) Dataset for Robust Underwater Object Detection</title>
      <link>http://arxiv.org/abs/2502.20651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了COU数据集，一个包含常见人造物体实例分割图像的水下图像库。&lt;h4&gt;背景&lt;/h4&gt;在水下环境中缺乏用于实例分割的高质量和多样化的数据集。现有的数据集主要关注海洋生物，而忽视了其他类型的水下物体。&lt;h4&gt;目的&lt;/h4&gt;创建一个新的数据集COU，涵盖多种不同环境下的常见人造物品，以促进轻量级实时检测器的研发，特别是针对自主式水下航行器（AUV）的训练需求。&lt;h4&gt;方法&lt;/h4&gt;从多个地点的机器人实地试验中收集图像，并对这些图像进行了详细的实例分割标注。使用三种最先进的模型来评估COU数据集在训练水下目标检测器方面的性能和准确性。&lt;h4&gt;主要发现&lt;/h4&gt;相比于仅基于地面数据训练的目标检测器，使用COU进行训练显著提高了检测器的表现和精确度。&lt;h4&gt;结论&lt;/h4&gt;COU是一个多样化的、高质量的数据集，专门用于改进自主式水下航行器的实时对象检测能力。该数据集将对研究界开放，并采用开源许可证提供。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一个名为COU（水下的常见物体）的数据集，这是一个包含多种水生和海洋环境中常见的人造物体实例分割图像的数据集。COU包含了大约10,000张标注过的分割图像，这些图像是从不同地点进行的多次水下机器人实地试验中收集而来的。该数据集旨在解决缺乏用于水下实例分割的稳健分类覆盖的问题，这对于训练轻量级、实时能力强大的自主式水下航行器（AUV）检测器特别有用。此外，COU解决了对象类别多样性的不足问题，因为常见的水下图像数据集仅关注海洋生物。目前，COU包含了来自封闭水域（泳池）和开放水域（湖泊和海洋）环境的24种不同类别的物体图像，包括海洋垃圾、潜水工具以及AUV等。为了评估COU在训练水下目标检测器方面的效果，我们使用三种最先进的模型来评估其性能和准确性，采用了标准准确率和效率指标相结合的方法。COU训练过的检测器相较于仅基于地面数据训练的检测器表现出明显的优势。我们将在开源许可下提供COU供广泛使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce COU: Common Objects Underwater, an instance-segmented imagedataset of commonly found man-made objects in multiple aquatic and marineenvironments. COU contains approximately 10K segmented images, annotated fromimages collected during a number of underwater robot field trials in diverselocations. COU has been created to address the lack of datasets with robustclass coverage curated for underwater instance segmentation, which isparticularly useful for training light-weight, real-time capable detectors forAutonomous Underwater Vehicles (AUVs). In addition, COU addresses the lack ofdiversity in object classes since the commonly available underwater imagedatasets focus only on marine life. Currently, COU contains images from bothclosed-water (pool) and open-water (lakes and oceans) environments, of 24different classes of objects including marine debris, dive tools, and AUVs. Toassess the efficacy of COU in training underwater object detectors, we usethree state-of-the-art models to evaluate its performance and accuracy, using acombination of standard accuracy and efficiency metrics. The improvedperformance of COU-trained detectors over those solely trained on terrestrialdata demonstrates the clear advantage of training with annotated underwaterimages. We make COU available for broad use under open-source licenses.</description>
      <author>example@mail.com (Rishi Mukherjee, Sakshi Singh, Jack McWilliams, Junaed Sattar)</author>
      <guid isPermaLink="false">2502.20651v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>EDENet: Echo Direction Encoding Network for Place Recognition Based on Ground Penetrating Radar</title>
      <link>http://arxiv.org/abs/2502.20643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于地表穿透雷达(GPR)的定位技术在机器人领域中的应用，并提出了一种新的网络架构EDENet来解决大规模地图中位置识别的问题。&lt;h4&gt;背景&lt;/h4&gt;GPR由于能够探测稳定的地下特征，在机器人领域得到了广泛应用。然而，现有的方法主要集中在小规模的位置识别上，忽略了大规模地图中所面临的挑战。&lt;h4&gt;目的&lt;/h4&gt;研究GPR回波序列与地下场景之间的几何关系，并提出一种新的网络设计来应对大尺度位置识别的难题。&lt;h4&gt;方法&lt;/h4&gt;引入可学习的Gabor滤波器以精确提取方向响应，并结合方向感知注意力机制进行有效的几何编码。还使用了移不变单元和多尺度聚合策略，提高了对介电常数变化的适应性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的EDENet在公共数据集上的实验表明，它不仅在位置识别性能上超越了现有解决方案，还在模型大小和计算效率方面具有优势。&lt;h4&gt;结论&lt;/h4&gt;通过提出创新的方法和技术，有效解决了大规模地图中基于GPR的位置识别问题，并展示了其优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ground penetrating radar (GPR) based localization has gained significantrecognition in robotics due to its ability to detect stable subsurfacefeatures, offering advantages in environments where traditional sensors likecameras and LiDAR may struggle. However, existing methods are primarily focusedon small-scale place recognition (PR), leaving the challenges of PR inlarge-scale maps unaddressed. These challenges include the inherent sparsity ofunderground features and the variability in underground dielectric constants,which complicate robust localization. In this work, we investigate thegeometric relationship between GPR echo sequences and underground scenes,leveraging the robustness of directional features to inform our network design.We introduce learnable Gabor filters for the precise extraction of directionalresponses, coupled with a direction-aware attention mechanism for effectivegeometric encoding. To further enhance performance, we incorporate ashift-invariant unit and a multi-scale aggregation strategy to betteraccommodate variations in di-electric constants. Experiments conducted onpublic datasets demonstrate that our proposed EDENet not only surpassesexisting solutions in terms of PR performance but also offers advantages inmodel size and computational efficiency.</description>
      <author>example@mail.com (Pengyu Zhang, Xieyuanli Chen, Yuwei Chen, Beizhen Bi, Zhuo Xu, Tian Jin, Xiaotao Huang, Liang Shen)</author>
      <guid isPermaLink="false">2502.20643v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    </channel>
</rss>