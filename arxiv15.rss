<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 15 Aug 2025 16:13:18 +0800</lastBuildDate>
    <item>
      <title>HGAurban: Heterogeneous Graph Autoencoding for Urban Spatial-Temporal Learning</title>
      <link>http://arxiv.org/abs/2410.10915v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HGAurban是一种新型异构时空图掩码自编码器，利用生成式自监督学习解决城市感知应用中时空数据的噪声和稀疏性问题，有效提升了区域表示的质量。&lt;h4&gt;背景&lt;/h4&gt;时空图表示在城市感知应用（如交通分析、人类移动行为建模和城市犯罪预测）中起着关键作用，但时空数据的噪声和稀疏性限制了现有神经网络学习有意义区域表示的能力。&lt;h4&gt;目的&lt;/h4&gt;克服时空数据噪声和稀疏性的限制，提出一种鲁棒的城市数据表示方法。&lt;h4&gt;方法&lt;/h4&gt;提出HGAurban框架，包含时空异构图编码器从多源数据提取区域依赖关系，以及掩码自编码器联合处理节点特征和图结构，自动学习跨区域的异构时空模式。&lt;h4&gt;主要发现&lt;/h4&gt;在多个时空挖掘任务上的实验表明，该框架优于最先进的方法，并能鲁棒处理空间和时间维度上的噪声和稀疏性挑战。&lt;h4&gt;结论&lt;/h4&gt;HGAurban框架有效解决了时空数据噪声和稀疏性问题，通过自监督学习显著提高了城市数据表示的质量。&lt;h4&gt;翻译&lt;/h4&gt;时空图表示在城市感知应用中起着关键作用，包括交通分析、人类移动行为建模和城市犯罪预测。然而，一个关键挑战在于时空数据的噪声和稀疏性，这限制了现有神经网络学习时空图中有意义区域表示的能力。为了克服这些限制，我们提出了HGAurban，一种新型异构时空图掩码自编码器，利用生成式自监督学习进行鲁棒的城市数据表示。我们的框架引入了一个时空异构图编码器，从多源数据中提取区域间的依赖关系，实现对多样化空间关系的全面建模。在我们的自监督学习范式中，我们实现了一个掩码自编码器，联合处理节点特征和图结构。这种方法能够自动学习跨区域的异构时空模式，显著改善动态时间相关性的表示。在多个时空挖掘任务上的综合实验表明，我们的框架优于最先进的方法，并能鲁棒处理真实世界城市数据挑战，包括空间和时间维度上的噪声和稀疏性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-10-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial-temporal graph representations play a crucial role in urban sensingapplications, including traffic analysis, human mobility behavior modeling, andcitywide crime prediction. However, a key challenge lies in the noisy andsparse nature of spatial-temporal data, which limits existing neural networks'ability to learn meaningful region representations in the spatial-temporalgraph. To overcome these limitations, we propose HGAurban, a novelheterogeneous spatial-temporal graph masked autoencoder that leveragesgenerative self-supervised learning for robust urban data representation. Ourframework introduces a spatial-temporal heterogeneous graph encoder thatextracts region-wise dependencies from multi-source data, enablingcomprehensive modeling of diverse spatial relationships. Within ourself-supervised learning paradigm, we implement a masked autoencoder thatjointly processes node features and graph structure. This approachautomatically learns heterogeneous spatial-temporal patterns across regions,significantly improving the representation of dynamic temporal correlations.Comprehensive experiments across multiple spatiotemporal mining tasksdemonstrate that our framework outperforms state-of-the-art methods androbustly handles real-world urban data challenges, including noise and sparsityin both spatial and temporal dimensions.</description>
      <author>example@mail.com (Qianru Zhang, Xinyi Gao, Haixin Wang, Dong Huang, Siu-Ming Yiu, Hongzhi Yin)</author>
      <guid isPermaLink="false">2410.10915v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
  <item>
      <title>SimAQ: Mitigating Experimental Artifacts in Soft X-Ray Tomography using Simulated Acquisitions</title>
      <link>http://arxiv.org/abs/2508.10821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种模拟管道，用于生成真实的细胞幻影并应用合成伪影，解决了软X射线断层扫描中实验伪影和数据集有限的问题。通过在合成数据上训练神经网络，实现了在真实SXT断层扫描图像上的有效迁移学习，提供了准确的分割结果，使对噪声断层扫描图像的定量分析成为可能，而不依赖于大型标记数据集或复杂的重建方法。&lt;h4&gt;背景&lt;/h4&gt;软X射线断层扫描(SXT)可以提供全细胞结构的详细洞察，但受到实验伪影(如缺失楔形)和注释数据集有限可用性的阻碍。&lt;h4&gt;目的&lt;/h4&gt;提出一种模拟管道，用于生成真实的细胞幻影并应用合成伪影，生成成对的噪声体积、正弦图和重建图像。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为\method的模拟管道，通过在合成数据上训练神经网络进行验证，并在真实的SXT断层扫描图像上展示有效的少样本和零样本迁移学习能力。&lt;h4&gt;主要发现&lt;/h4&gt;模型能够提供准确的分割结果，使对噪声断层扫描图像的定量分析成为可能，不依赖于大型标记数据集或复杂的重建方法。&lt;h4&gt;结论&lt;/h4&gt;该方法解决了SXT中的实验伪影和数据集有限的问题，通过合成数据训练，实现了在真实数据上的有效迁移学习。&lt;h4&gt;翻译&lt;/h4&gt;软X射线断层扫描(SXT)为全细胞提供了详细的结构洞察，但受到缺失楔形等实验伪影和注释数据集有限可用性的阻碍。我们提出了\method，这是一种模拟管道，可生成真实的细胞幻影并应用合成伪影，以产生成对的噪声体积、正弦图和重建图像。我们通过主要在合成数据上训练神经网络来验证我们的方法，并证明了在真实SXT断层扫描图像上有效的少样本和零样本迁移学习能力。我们的模型提供了准确的分割，使对噪声断层扫描图像的定量分析成为可能，而不依赖于大型标记数据集或复杂的重建方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决软X射线断层扫描(SXT)中的实验伪影问题和标注数据稀缺问题。这些问题很重要，因为SXT是一种能以30-50纳米高分辨率解析完整细胞结构的技术，但'缺失楔形'和各种实验噪声会严重影响重建质量，降低定量和形态学分析的准确性，而获取大量标注数据又非常耗时困难。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考如何利用模拟数据解决真实数据稀缺的问题，设计出SimAQ模拟管道。他们借鉴了现有的SXT重建算法(如滤波反投影FBP)、细胞模拟方法(如椭球体模拟细胞器)、Martínez-Sánchez等人的生物真实合成数据生成、Yao等人的正弦图修复方法以及Liu等人的自监督学习策略，但将这些方法整合并扩展为一个更全面的解决方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过模拟真实的细胞结构和实验条件生成训练数据，结合合成数据和少量真实数据进行混合学习，实现零样本和少样本迁移学习。整体流程包括：1)使用椭球体生成酵母细胞幻影并添加随机变形；2)添加冰裂纹、散射和基准标记等伪影；3)模拟有限角度投影和重建过程；4)使用编码器-解码器架构在混合数据上训练；5)用少量真实数据进行微调适应真实数据分布。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)全面的SimAQ模拟框架，涵盖多种实验伪影；2)混合学习方法，结合合成和真实数据优势；3)实现零样本和少样本迁移学习；4)提供端到端开源解决方案。相比之前工作，SimAQ比Chen等人的纯真实数据方法需要更少标注，比Martínez-Sánchez等人更专注于伪影模拟，比Yao的UsiNet更适合低对比度生物成像，比Liu的方法更好地处理各向异性伪影。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SimAQ通过创建全面的细胞结构和实验伪影模拟管道，结合合成数据和少量真实数据训练，实现了软X射线断层扫描中伪影的有效校正和细胞分割，大幅减少了对大量标注数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Soft X-ray tomography (SXT) provides detailed structural insight into wholecells but is hindered by experimental artifacts such as the missing wedge andby limited availability of annotated datasets. We present \method, a simulationpipeline that generates realistic cellular phantoms and applies syntheticartifacts to produce paired noisy volumes, sinograms, and reconstructions. Wevalidate our approach by training a neural network primarily on synthetic dataand demonstrate effective few-shot and zero-shot transfer learning on real SXTtomograms. Our model delivers accurate segmentations, enabling quantitativeanalysis of noisy tomograms without relying on large labeled datasets orcomplex reconstruction methods.</description>
      <author>example@mail.com (Jacob Egebjerg, Daniel Wüstner)</author>
      <guid isPermaLink="false">2508.10821v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>SemPT: Semantic Prompt Tuning for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2508.10645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为语义提示调优(SemPT)的新型框架，通过利用跨类别的共享属性级知识来解决视觉迁移学习中未见类别的泛化挑战，在各种设置下实现了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;视觉迁移学习对于未见类别是一个活跃且有挑战性的研究领域，存在保留特定类别表示和获取可迁移知识之间的固有冲突。虽然视觉-语言模型(VLMs)提供了有前景的解决方案，但现有的提示调优方法依赖于稀疏类别标签或不同的LLM生成的描述，这分散了知识表示并阻碍了可迁移性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，通过引入语义提示调优(SemPT)框架，利用跨类别的共享属性级知识来处理视觉迁移学习中的泛化挑战。&lt;h4&gt;方法&lt;/h4&gt;SemPT采用两步提示策略，指导LLM提取共享视觉属性并生成属性级描述，捕捉超越标签的可迁移语义线索；应用视觉引导加权减少无关属性噪声并增强文本嵌入；图像嵌入与标签和属性增强文本嵌入联合对齐，平衡已见类别的判别性和对未见类别的可迁移性；根据类别可用性动态选择适当的嵌入方式。&lt;h4&gt;主要发现&lt;/h4&gt;在15个基准数据集上的广泛实验表明，SemPT在各种设置下实现了最先进性能，包括基础到新颖泛化、跨数据集迁移、跨域迁移和少样本学习。&lt;h4&gt;结论&lt;/h4&gt;SemPT有效解决了视觉迁移学习中的挑战，通过利用共享属性级知识显著提高了模型在未见类别上的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;视觉迁移学习对于未见类别呈现一个活跃的研究课题 yet 一个具有挑战性的任务，由于保留特定类别表示和获取可迁移知识之间的固有冲突。视觉-语言模型(VLMs)在大量图像-文本对上预训练提供了一个有前景的解决方案。然而，现有的提示调优方法依赖于稀疏类别标签或不同的LLM生成的描述，这分散了知识表示并阻碍了可迁移性。为了解决这一局限性，我们引入了语义提示调优(SemPT)，一个通过利用跨类别的共享属性级知识处理泛化挑战的新型框架。具体来说，SemPT采用两步提示策略指导LLM提取共享视觉属性并生成属性级描述，捕捉超越标签的可迁移语义线索，同时确保结构连贯性。然后，对属性级描述的嵌入应用视觉引导加权，减少无关属性的噪声并增强文本嵌入。此外，图像嵌入与标签和属性增强的文本嵌入联合对齐，平衡已见类别的判别性和对未见类别的可迁移性。考虑到类别的可用性，我们的推理动态选择已见类别的标准标签嵌入和未见类别的属性增强嵌入，以确保有效适应。在15个基准数据集上的广泛实验表明，SemPT在各种设置下实现了最先进的性能，包括基础到新颖泛化、跨数据集迁移、跨域迁移和少样本学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual transfer learning for unseen categories presents an active researchtopic yet a challenging task, due to the inherent conflict between preservingcategory-specific representations and acquiring transferable knowledge.Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairsoffer a promising solution. However, existing prompt tuning methods rely onsparse category labels or disparate LLM-generated descriptions, which fragmentknowledge representation and hinder transferability. To address thislimitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework thattackles the generalization challenge by leveraging shared attribute-levelknowledge across categories. Specifically, SemPT adopts a two-step promptingstrategy to guide LLM in extracting shared visual attributes and generatingattribute-level descriptions, capturing transferable semantic cues beyondlabels while ensuring coherent structure. Then, visually guided weighting isapplied to the embeddings of attribute-level descriptions to reduce noise fromirrelevant attributes and enhance the text embeddings. Additionally, imageembeddings are jointly aligned with both label and attribute-enhanced textembeddings, balancing discrimination for seen categories and transferability tounseen ones. Considering the availability of category exposure, our inferencedynamically selects between standard label embeddings for seen categories andattribute-enhanced embeddings for unseen ones to ensure effective adaptation.Extensive experiments on 15 benchmark datasets demonstrate that SemPT achievesstate-of-the-art performance across various settings, including base-to-novelgeneralization, cross-dataset transfer, cross-domain transfer, and few-shotlearning.</description>
      <author>example@mail.com (Xiao Shi, Yangjun Ou, Zhenzhong Chen)</author>
      <guid isPermaLink="false">2508.10645v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Deep Equilibrium Model Learning for Large-Scale Channel Estimation with Performance Guarantees</title>
      <link>http://arxiv.org/abs/2508.10546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种无需真实信道标签的无监督学习框架用于大规模信道估计(LCE)，结合广义Stein无偏风险估计(GSURE)和深度平衡(DEQ)模型，实现了与监督方法相当的性能。&lt;h4&gt;背景&lt;/h4&gt;监督深度学习方法在大规模信道估计中显示出潜力，但它们对真实信道标签的依赖严重限制了它们在实际系统中的实用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要真实信道标签的无监督学习框架用于LCE。&lt;h4&gt;方法&lt;/h4&gt;利用广义Stein无偏风险估计(GSURE)作为无监督损失函数，集成深度平衡(DEQ)模型隐式表示无限深度网络，通过直接学习参数化迭代过程的固定点实现。&lt;h4&gt;主要发现&lt;/h4&gt;DEQ架构强制执行可压缩解；DEQ诱导的可压缩性确保通过GSURE优化投影误差足以保证良好的MSE性能；提供了严格的性能保证。&lt;h4&gt;结论&lt;/h4&gt;提出的框架在真实信道不可用时显著优于各种基线方法。&lt;h4&gt;翻译&lt;/h4&gt;监督深度学习方法在大规模信道估计(LCE)中显示出前景，但它们对真实信道标签的依赖严重限制了它们在实际系统中的实用性。在本文中，我们提出了一种用于LCE的无监督学习框架，不需要真实信道标签。所提出的方法利用广义Stein无偏风险估计(GSURE)作为原则性的无监督损失函数，它提供了从压缩噪声测量中投影均方误差(PMSE)的无偏估计。为确保保证的性能，我们集成了一个深度平衡(DEQ)模型，它通过直接学习参数化迭代过程的固定点来隐式表示无限深度网络。我们理论上证明，在温和条件下，所提出的基于GSURE的无监督DEQ学习可以达到与监督方法相当的性能。特别是，我们表明DEQ架构本质上强制执行可压缩解。然后我们证明DEQ诱导的可压缩性确保通过GSURE优化投影误差足以保证良好的MSE性能，从而实现严格的性能保证。广泛的模拟验证了理论发现，并证明当真实信道不可用时，所提出的框架显著优于各种基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Supervised deep learning methods have shown promise for large-scale channelestimation (LCE), but their reliance on ground-truth channel labels greatlylimits their practicality in real-world systems. In this paper, we propose anunsupervised learning framework for LCE that does not require ground-truthchannels. The proposed approach leverages Generalized Stein's Unbiased RiskEstimate (GSURE) as a principled unsupervised loss function, which provides anunbiased estimate of the projected mean-squared error (PMSE) from compressednoisy measurements. To ensure a guaranteed performance, we integrate a deepequilibrium (DEQ) model, which implicitly represents an infinite-depth networkby directly learning the fixed point of a parameterized iterative process. Wetheoretically prove that, under mild conditions, the proposed GSURE-basedunsupervised DEQ learning can achieve oracle-level supervised performance. Inparticular, we show that the DEQ architecture inherently enforces acompressible solution. We then demonstrate that DEQ-induced compressibilityensures that optimizing the projected error via GSURE suffices to guarantee agood MSE performance, enabling a rigorous performance guarantee. Extensivesimulations validate the theoretical findings and demonstrate that the proposedframework significantly outperforms various baselines when ground-truth channelis unavailable.</description>
      <author>example@mail.com (Haotian Tian, Lixiang Lian)</author>
      <guid isPermaLink="false">2508.10546v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>A dataset and model for recognition of audiologically relevant environments for hearing aids: AHEAD-DS and YAMNet+</title>
      <link>http://arxiv.org/abs/2508.10360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了AHEAD-DS数据集和YAMNet+模型，解决了听力辅助设备场景识别中的数据集局限性和边缘设备部署挑战，实现了高效准确的实时场景识别。&lt;h4&gt;背景&lt;/h4&gt;听力辅助设备中的场景识别很重要，但现有数据集缺乏公开性、完整性或听力相关标签，阻碍了机器学习模型的系统比较。同时，将这些模型部署在资源受限的边缘设备上也是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;创建一个标准化的公开数据集用于听力相关环境场景识别，并开发一个适用于边缘设备的声音识别模型，作为基于声音的场景识别的基线模型。&lt;h4&gt;方法&lt;/h4&gt;利用多个开源数据集创建AHEAD-DS数据集，并引入YAMNet+声音识别模型。通过迁移学习使用预训练的YAMNet模型进行优化。将模型部署到Android智能手机上进行实时场景识别测试。&lt;h4&gt;主要发现&lt;/h4&gt;YAMNet+在AHEAD-DS测试集的十四种听力相关环境类别中达到了0.83的平均精度和0.93的准确率。即使在配置适中的Google Pixel 3手机上，模型也能实现实时处理，加载模型的延迟约为50毫秒，每1秒音频的处理时间线性增加约30毫秒。&lt;h4&gt;结论&lt;/h4&gt;AHEAD-DS数据集和YAMNet+模型为听力辅助设备的场景识别提供了有效的解决方案，能够在边缘设备上实现实时、准确的声音场景识别，为未来研究提供了基准。&lt;h4&gt;翻译&lt;/h4&gt;听力辅助设备相关环境的场景识别对助听器很重要；然而，这具有挑战性，部分原因是现有数据集的局限性。数据集通常缺乏公开可访问性、完整性或听力相关的标签，阻碍了机器学习模型的系统比较。将这些模型部署在资源受限的边缘设备上提出了另一个挑战。我们的解决方案是双重的：我们利用几个开源数据集创建了AHEAD-DS，一个为听力相关环境场景识别而设计的数据集，并引入了YAMNet+声音识别模型。AHEAD-DS旨在提供一个标准化的、公开可用的数据集，具有与听力辅助设备相关的统一标签，促进模型比较。YAMNet+设计用于部署在连接到听力设备的智能手机等边缘设备上，如助听器和具有助听功能的无线耳机；作为基于声音的场景识别的基线模型。在AHEAD-DS测试集上，YAMNet+在十四种听力相关环境类别中达到了0.83的平均精度和0.93的准确率。我们发现，使用预训练的YAMNet模型进行迁移学习是必不可少的。我们通过将YAMNet+部署到Android智能手机上，证明了边缘设备上的实时声音场景识别能力。即使在配置适中的Google Pixel 3手机（2018年发布）上，模型处理音频的加载延迟约为50毫秒，每1秒音频的处理时间线性增加约30毫秒。我们的网站和代码https://github.com/Australian-Future-Hearing-Initiative。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene recognition of audiologically relevant environments is important forhearing aids; however, it is challenging, in part because of the limitations ofexisting datasets. Datasets often lack public accessibility, completeness, oraudiologically relevant labels, hindering systematic comparison of machinelearning models. Deploying these models on resource-constrained edge devicespresents another challenge. Our solution is two-fold: we leverage several opensource datasets to create AHEAD-DS, a dataset designed for scene recognition ofaudiologically relevant environments, and introduce YAMNet+, a soundrecognition model. AHEAD-DS aims to provide a standardised, publicly availabledataset with consistent labels relevant to hearing aids, facilitating modelcomparison. YAMNet+ is designed for deployment on edge devices like smartphonesconnected to hearing devices, such as hearing aids and wireless earphones withhearing aid functionality; serving as a baseline model for sound-based scenerecognition. YAMNet+ achieved a mean average precision of 0.83 and accuracy of0.93 on the testing set of AHEAD-DS across fourteen categories ofaudiologically relevant environments. We found that applying transfer learningfrom the pretrained YAMNet model was essential. We demonstrated real-timesound-based scene recognition capabilities on edge devices by deploying YAMNet+to an Android smartphone. Even with a Google Pixel 3 (a phone with modestspecifications, released in 2018), the model processes audio with approximately50ms of latency to load the model, and an approximate linear increase of 30msper 1 second of audio. Our website and codehttps://github.com/Australian-Future-Hearing-Initiative .</description>
      <author>example@mail.com (Henry Zhong, Jörg M. Buchholz, Julian Maclaren, Simon Carlile, Richard Lyon)</author>
      <guid isPermaLink="false">2508.10360v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Explainable AI Technique in Lung Cancer Detection Using Convolutional Neural Networks</title>
      <link>http://arxiv.org/abs/2508.10196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 9 figures, 4 tables. Undergraduate research project report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究开发了一个深度学习框架，用于从胸部CT图像进行自动化肺癌筛查，并集成了可解释性功能。通过评估多种模型架构，发现ResNet152准确率最高，而DenseNet121在各项指标间提供了最佳平衡。使用SHAP方法增强了临床透明度，使这种方法在资源有限的环境中特别有价值。&lt;h4&gt;背景&lt;/h4&gt;肺癌的早期检测对提高生存率至关重要，研究使用胸部CT图像进行自动化肺癌筛查。&lt;h4&gt;目的&lt;/h4&gt;开发一个深度学习框架用于肺癌筛查，集成可解释性功能，提高临床决策透明度。&lt;h4&gt;方法&lt;/h4&gt;使用IQ-OTH/NCCD数据集（包含1197个正常、良性和恶性三类扫描），评估自定义卷积神经网络和三种微调的迁移学习骨干网络（DenseNet121、ResNet152和VGG19）。采用成本敏感学习缓解类别不平衡，通过准确率、精确率、召回率、F1分数和ROC-AUC评估模型，并应用SHAP方法可视化预测证据。&lt;h4&gt;主要发现&lt;/h4&gt;ResNet152达到最高准确率(97.3%)，DenseNet121在精确率、召回率和F1分数方面提供了最佳平衡（分别为92%、90%、91%）。集成可解释性的CNN方法可以提供快速、准确且可解释的支持。&lt;h4&gt;结论&lt;/h4&gt;基于CNN的方法结合可解释性可以为肺癌筛查提供支持，尤其在资源有限的环境中特别有用。&lt;h4&gt;翻译&lt;/h4&gt;肺癌的早期检测对提高生存率至关重要。我们提出了一种深度学习框架，用于从胸部计算机断层扫描（CT）图像进行自动化肺癌筛查，并集成了可解释性功能。我们使用IQ-OTH/NCCD数据集（包含正常、良性和恶性三类共1197个扫描）评估了一个自定义卷积神经网络（CNN）和三种微调的迁移学习骨干网络：DenseNet121、ResNet152和VGG19。模型通过成本敏感学习进行训练以减轻类别不平衡，并通过准确率、精确率、召回率、F1分数和ROC-AUC进行评估。虽然ResNet152达到了最高的准确率（97.3%），但DenseNet121在精确率、召回率和F1分数方面提供了最佳平衡（分别高达92%、90%、91%）。我们进一步应用Shapley Additive Explanations（SHAP）来可视化对预测有贡献的证据，提高了临床透明度。结果表明，结合可解释性的基于CNN的方法可以为肺癌筛查提供快速、准确且可解释的支持，特别是在资源有限的设置中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early detection of lung cancer is critical to improving survival outcomes. Wepresent a deep learning framework for automated lung cancer screening fromchest computed tomography (CT) images with integrated explainability. Using theIQ-OTH/NCCD dataset (1,197 scans across Normal, Benign, and Malignant classes),we evaluate a custom convolutional neural network (CNN) and three fine-tunedtransfer learning backbones: DenseNet121, ResNet152, and VGG19. Models aretrained with cost-sensitive learning to mitigate class imbalance and evaluatedvia accuracy, precision, recall, F1-score, and ROC-AUC. While ResNet152achieved the highest accuracy (97.3%), DenseNet121 provided the best overallbalance in precision, recall, and F1 (up to 92%, 90%, 91%, respectively). Wefurther apply Shapley Additive Explanations (SHAP) to visualize evidencecontributing to predictions, improving clinical transparency. Results indicatethat CNN-based approaches augmented with explainability can provide fast,accurate, and interpretable support for lung cancer screening, particularly inresource-limited settings.</description>
      <author>example@mail.com (Nishan Rai, Sujan Khatri, Devendra Risal)</author>
      <guid isPermaLink="false">2508.10196v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model</title>
      <link>http://arxiv.org/abs/2508.10156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了生成式人工智能(GenAI)模型在农业作物病害诊断中的应用，验证了结合真实图像与合成图像训练模型的可行性及效果。&lt;h4&gt;背景&lt;/h4&gt;生成式人工智能模型的发展为生成高分辨率合成图像提供了新可能，为训练农业计算机视觉模型提供了替代传统图像采集的方法。在作物病害诊断领域，GenAI模型可创建各种疾病的合成图像，减少对资源密集型田间数据采集的依赖。&lt;h4&gt;目的&lt;/h4&gt;研究结合少量真实图像与合成图像是否能提高EfficientNetV2-L模型对西瓜病害分类的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;将训练数据集分为五种处理方式：H0（仅真实图像）、H1（仅合成图像）、H2（真实与合成图像比例为1:1）、H3（真实与合成图像比例为1:10）、H4（H3 + 随机图像以提高变异性）。所有处理都使用自定义的EfficientNetV2-L架构进行训练，应用了增强的微调和迁移学习技术。&lt;h4&gt;主要发现&lt;/h4&gt;在H2、H3和H4处理上训练的模型表现出高精确度、召回率和F1分数指标。加权F1分数从H0的0.65提高到H3-H4的1.00，表明添加少量真实图像与大量合成图像相结合提高了模型性能和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;合成图像单独无法充分替代真实图像；相反，必须以混合方式使用两者，以最大化作物病害分类的模型性能。&lt;h4&gt;翻译&lt;/h4&gt;当前生成式人工智能(GenAI)模型的进步为生成高分辨率合成图像开辟了新的可能性，从而为训练农业计算机视觉模型提供了替代传统图像采集的有前景选择。在作物病害诊断的背景下，GenAI模型正被用于创建各种疾病的合成图像，可能促进模型创建并减少对资源密集型田间数据采集的依赖。然而，关于评估将真实图像与合成图像整合以提高疾病分类性能效果的研究有限。因此，本研究旨在调查结合少量真实图像与合成图像是否能提高EfficientNetV2-L模型对西瓜(Citrullus lanatus)病害分类的预测准确性。训练数据集被分为五种处理方式：H0（仅真实图像）、H1（仅合成图像）、H2（真实与合成图像比例为1:1）、H3（真实与合成图像比例为1:10）和H4（H3 + 随机图像以提高变性和模型泛化能力）。所有处理均使用自定义的EfficientNetV2-L架构进行训练，并应用了增强的微调和迁移学习技术。在H2、H3和H4处理上训练的模型表现出高精确度、召回率和F1分数指标。此外，加权F1分数从H0的0.65提高到H3-H4的1.00，这表明添加少量真实图像与大量合成图像相结合提高了模型性能和泛化能力。总体而言，这验证了发现：合成图像单独无法充分替代真实图像；相反，必须以混合方式使用两者，以最大化作物病害分类的模型性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The current advancements in generative artificial intelligence (GenAI) modelshave paved the way for new possibilities for generating high-resolutionsynthetic images, thereby offering a promising alternative to traditional imageacquisition for training computer vision models in agriculture. In the contextof crop disease diagnosis, GenAI models are being used to create syntheticimages of various diseases, potentially facilitating model creation andreducing the dependency on resource-intensive in-field data collection.However, limited research has been conducted on evaluating the effectiveness ofintegrating real with synthetic images to improve disease classificationperformance. Therefore, this study aims to investigate whether combining alimited number of real images with synthetic images can enhance the predictionaccuracy of an EfficientNetV2-L model for classifying watermelon\textit{(Citrullus lanatus)} diseases. The training dataset was divided intofive treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1real-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images toimprove variability and model generalization). All treatments were trainedusing a custom EfficientNetV2-L architecture with enhanced fine-tuning andtransfer learning techniques. Models trained on H2, H3, and H4 treatmentsdemonstrated high precision, recall, and F1-score metrics. Additionally, theweighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifyingthat the addition of a small number of real images with a considerable volumeof synthetic images improved model performance and generalizability. Overall,this validates the findings that synthetic images alone cannot adequatelysubstitute for real images; instead, both must be used in a hybrid manner tomaximize model performance for crop disease classification.</description>
      <author>example@mail.com (Nitin Rai, Nathan S. Boyd, Gary E. Vallad, Arnold W. Schumann)</author>
      <guid isPermaLink="false">2508.10156v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Unifying equivalences across unsupervised learning, network science, and imaging/network neuroscience</title>
      <link>http://arxiv.org/abs/2508.10045v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过描述不同科学分析方法之间的等价性，促进了科学数据的整合，解决了现代科学领域中数据整合不足导致的循环分析和冗余解释问题。&lt;h4&gt;背景&lt;/h4&gt;现代科学领域面临整合大量数据、分析和结果的挑战，忽视这种整合可能导致循环分析和冗余解释。&lt;h4&gt;目的&lt;/h4&gt;通过描述统一数据集和网络不同分析的等价性，促进科学整合，简化跨学科分析方法的理解和应用。&lt;h4&gt;方法&lt;/h4&gt;描述聚类与降维、网络中心性与动力学、影像学与网络神经科学模型间的等价性；统一无监督学习和网络科学的基础目标；融合优化算法；扩展目标以简化降维方法解释；将连接测量与网络科学中的六种通信测量等同；提供三个半分析性案例阐明分析方法。&lt;h4&gt;主要发现&lt;/h4&gt;统一了无监督学习、网络科学、影像神经科学和网络神经科学中的多种分析方法；开发了abc开放多语言工具箱实现这些分析。&lt;h4&gt;结论&lt;/h4&gt;研究成功统一了跨无监督学习、网络科学、影像神经科学和网络神经科学的多种分析方法，为科学整合提供了新框架。&lt;h4&gt;翻译&lt;/h4&gt;现代科学领域面临整合大量数据、分析和结果的挑战。我们最近表明，忽视这种整合可能导致循环分析和冗余解释。在此，我们通过描述统一数据集和网络不同分析的等价性，促进科学整合。我们描述了聚类和降维、网络中心性和动力学、影像学和网络神经科学中流行模型分析之间的等价性。首先，我们将无监督学习和网络科学中的基础目标等同（从k-means到模块化再到UMAP），融合优化这些目标经典算法，并扩展这些目标以简化流行降维方法的解释。其次，我们将连接幅度和离散度的基本测量与网络科学和网络神经科学中的六种通信、控制和多样性测量等同。第三，我们描述了三个半分析性案例，阐明和简化影像学和网络神经科学中结构和动力学分析的解释。我们在示例脑成像数据上说明了我们的结果，并提供了abc，一个实现我们分析的开源多语言工具箱。总之，我们的研究统一了跨无监督学习、网络科学、影像神经科学和网络神经科学的多种分析方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern scientific fields face the challenge of integrating a wealth of data,analyses, and results. We recently showed that a neglect of this integrationcan lead to circular analyses and redundant explanations. Here, we help advancescientific integration by describing equivalences that unify diverse analysesof datasets and networks. We describe equivalences across analyses ofclustering and dimensionality reduction, network centrality and dynamics, andpopular models in imaging and network neuroscience. First, we equatefoundational objectives across unsupervised learning and network science (fromk means to modularity to UMAP), fuse classic algorithms for optimizing theseobjectives, and extend these objectives to simplify interpretations of populardimensionality reduction methods. Second, we equate basic measures ofconnectional magnitude and dispersion with six measures of communication,control, and diversity in network science and network neuroscience. Third, wedescribe three semi-analytical vignettes that clarify and simplify theinterpretation of structural and dynamical analyses in imaging and networkneuroscience. We illustrate our results on example brain-imaging data andprovide abct, an open multi-language toolbox that implements our analyses.Together, our study unifies diverse analyses across unsupervised learning,network science, imaging neuroscience, and network neuroscience.</description>
      <author>example@mail.com (Mika Rubinov)</author>
      <guid isPermaLink="false">2508.10045v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Hypercomplex Prompt-aware Multimodal Recommendation</title>
      <link>http://arxiv.org/abs/2508.10753v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HPMRec是一种超复杂提示感知多模态推荐框架，通过超复杂嵌入、非线性跨模态交互和提示感知补偿机制解决了现有多模态推荐系统中的三个关键局限性，在四个公共数据集上实现了最先进的推荐性能。&lt;h4&gt;背景&lt;/h4&gt;现代推荐系统在处理信息过载和解决多模态表征学习的固有局限性方面面临关键挑战。&lt;h4&gt;目的&lt;/h4&gt;克服现有多模态推荐方法的三个基本局限性：单一表征能力有限、忽略模态间非线性相关性、无法动态缓解GCN中的过平滑问题。&lt;h4&gt;方法&lt;/h4&gt;提出HPMRec框架，利用超复杂嵌入增强多模态特征表征多样性，采用超复杂乘法建立非线性跨模态交互，引入提示感知补偿机制缓解过平滑问题，并设计自监督学习任务增强表征多样性。&lt;h4&gt;主要发现&lt;/h4&gt;HPMRec通过超复杂嵌入、非线性交互和补偿机制有效解决了现有方法的局限性，在四个公共数据集上实现了最先进的推荐性能。&lt;h4&gt;结论&lt;/h4&gt;HPMRec框架成功解决了多模态推荐系统中的关键问题，提升了推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;现代推荐系统在处理信息过载的同时，解决多模态表征学习的固有局限性方面面临关键挑战。现有方法存在三个基本局限性：(1)通过单一表征表示丰富多模态特征的能力有限；(2)现有的线性模态融合策略忽略了模态间的深层非线性相关性；(3)静态优化方法无法动态缓解图卷积网络(GCN)中的过平滑问题。为克服这些局限性，我们提出了HPMRec，一种新颖的超复杂提示感知多模态推荐框架，它利用多组件形式的超复杂嵌入来增强多模态特征的表征多样性。HPMRec采用超复杂乘法自然地建立非线性跨模态交互，以弥合语义差距，有利于探索跨模态特征。HPMRec还引入了提示感知补偿机制，帮助组件与模态特定特征损失之间的不对齐，该机制从根本上缓解了过平滑问题。它进一步设计了自监督学习任务，以增强表征多样性并使不同模态保持一致。在四个公共数据集上的广泛实验表明，HPMRec实现了最先进的推荐性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern recommender systems face critical challenges in handling informationoverload while addressing the inherent limitations of multimodal representationlearning. Existing methods suffer from three fundamental limitations: (1)restricted ability to represent rich multimodal features through a singlerepresentation, (2) existing linear modality fusion strategies ignore the deepnonlinear correlations between modalities, and (3) static optimization methodsfailing to dynamically mitigate the over-smoothing problem in graphconvolutional network (GCN). To overcome these limitations, we propose HPMRec,a novel Hypercomplex Prompt-aware Multimodal Recommendation framework, whichutilizes hypercomplex embeddings in the form of multi-components to enhance therepresentation diversity of multimodal features. HPMRec adopts the hypercomplexmultiplication to naturally establish nonlinear cross-modality interactions tobridge semantic gaps, which is beneficial to explore the cross-modalityfeatures. HPMRec also introduces the prompt-aware compensation mechanism to aidthe misalignment between components and modality-specific features loss, andthis mechanism fundamentally alleviates the over-smoothing problem. It furtherdesigns self-supervised learning tasks that enhance representation diversityand align different modalities. Extensive experiments on four public datasetsshow that HPMRec achieves state-of-the-art recommendation performance.</description>
      <author>example@mail.com (Zheyu Chen, Jinfeng Xu, Hewei Wang, Shuo Yang, Zitong Wan, Haibo Hu)</author>
      <guid isPermaLink="false">2508.10753v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>SPHENIC: Topology-Informed Multi-View Clustering for Spatial Transcriptomics</title>
      <link>http://arxiv.org/abs/2508.10646v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 6 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SPHENIC是一种新颖的空间持续同调增强邻域集成聚类方法，通过整合不变拓扑特征和空间约束优化模块，解决了现有空间转录组学聚类方法在拓扑学习和空间邻域信息建模方面的局限性，在14个基准数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;空间转录组学聚类通过整合空间位置信息，可以更全面地了解细胞亚群识别。然而，现有方法存在两个主要局限性：拓扑学习易受低质量拓扑信号影响，以及对空间邻域信息的建模不足导致低质量的空间嵌入。&lt;h4&gt;目的&lt;/h4&gt;解决现有空间转录组学聚类方法在拓扑学习和空间邻域信息建模方面的局限性，提高聚类性能。&lt;h4&gt;方法&lt;/h4&gt;提出SPHENIC方法，包括：1) 将不变拓扑特征整合到聚类网络中实现稳定的表示学习；2) 设计空间约束和分布优化模块(SCDOM)，构建高质量空间嵌入，增加细胞与其空间邻域的相似性，减少与非邻域细胞的相似性。&lt;h4&gt;主要发现&lt;/h4&gt;在14个基准空间转录组切片上的实验表明，SPHENIC在空间聚类任务上实现了卓越的性能，比现有最先进的方法高出3.31%-6.54%。&lt;h4&gt;结论&lt;/h4&gt;SPHENIC方法通过整合拓扑特征和空间约束优化，有效解决了现有方法的局限性，在空间转录组学聚类任务上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;通过整合空间位置信息，空间转录组学聚类能够为细胞亚群识别提供更全面的见解。尽管最近有所进展，但现有方法至少有两个局限性：(i) 拓扑学习通常只考虑单个细胞或其交互图的表示；然而，空间转录组资料通常存在噪声，使得这些方法容易受到低质量拓扑信号的影响；(ii) 对空间邻域信息的建模不足导致低质量的空间嵌入。为了解决这些局限性，我们提出了SPHENIC，一种新颖的空间持续同调增强邻域集成聚类方法。具体而言，SPHENIC将不变拓扑特征整合到聚类网络中，以实现稳定的表示学习。此外，为了构建反映真实细胞分布的高质量空间嵌入，我们设计了空间约束和分布优化模块(SCDOM)。该模块增加细胞嵌入与其空间邻域嵌入之间的相似性，减少与非邻域细胞的相似性，从而产生聚类友好的空间嵌入。在14个基准空间转录组切片上的大量实验表明，SPHENIC在空间聚类任务上实现了卓越的性能，比现有最先进的方法高出3.31%-6.54%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; By incorporating spatial location information, spatial-transcriptomicsclustering yields more comprehensive insights into cell subpopulationidentification. Despite recent progress, existing methods have at least twolimitations: (i) topological learning typically considers only representationsof individual cells or their interaction graphs; however, spatialtranscriptomic profiles are often noisy, making these approaches vulnerable tolow-quality topological signals, and (ii) insufficient modeling of spatialneighborhood information leads to low-quality spatial embeddings. To addressthese limitations, we propose SPHENIC, a novel Spatial Persistent HomologyEnhanced Neighborhood Integrative Clustering method. Specifically, SPHENICincorporates invariant topological features into the clustering network toachieve stable representation learning. Additionally, to construct high-qualityspatial embeddings that reflect the true cellular distribution, we design theSpatial Constraint and Distribution Optimization Module (SCDOM). This moduleincreases the similarity between a cell's embedding and those of its spatialneighbors, decreases similarity with non-neighboring cells, and therebyproduces clustering-friendly spatial embeddings. Extensive experiments on 14benchmark spatial transcriptomic slices demonstrate that SPHENIC achievessuperior performance on the spatial clustering task, outperforming existingstate-of-the-art methods by 3.31%-6.54% over the best alternative.</description>
      <author>example@mail.com (Chenkai Guo, Yikai Zhu, Jing Yangum, Renxiang Guan, Por Lip Yee, Guangdun Peng, Dayu Hu)</author>
      <guid isPermaLink="false">2508.10646v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>MirGuard: Towards a Robust Provenance-based Intrusion Detection System Against Graph Manipulation Attacks</title>
      <link>http://arxiv.org/abs/2508.10639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MirGuard的稳健异常检测框架，用于增强溯源入侵检测系统(PIDS)抵抗图操纵攻击的能力。&lt;h4&gt;背景&lt;/h4&gt;基于学习的溯源入侵检测系统(PIDS)已成为主机系统异常检测的重要工具，能够捕捉丰富的上下文和结构信息，并有检测未知攻击的潜力。然而，这些系统容易受到图操纵攻击，攻击者通过操纵图结构来逃避检测，而之前的解决方案未能充分解决这一问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种稳健的异常检测解决方案，增强PIDS抵抗图操纵攻击的能力，提高其安全性和实用性。&lt;h4&gt;方法&lt;/h4&gt;提出MirGuard框架，结合逻辑感知的多视图增强与对比表示学习。该框架引入逻辑感知噪声注入(LNI)生成语义有效的图视图，确保增强保留溯源数据的底层因果语义。这些视图在逻辑保持对比学习框架中使用，鼓励模型学习对良性变换不变但对对抗性变化敏感的表示。&lt;h4&gt;主要发现&lt;/h4&gt;在多个溯源数据集上的全面评估表明，MirGuard在抵抗各种图操纵攻击方面的稳健性显著优于最先进的检测器，同时没有牺牲检测性能和效率。&lt;h4&gt;结论&lt;/h4&gt;MirGuard代表了首个针对增强PIDS抵御图操纵攻击的针对性研究，为现代网络安全挑战提供了稳健有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于学习的溯源入侵检测系统(PIDS)已成为主机系统异常检测的重要工具，因为它们能够捕捉丰富的上下文和结构信息，并有潜力检测未知攻击。然而，最近研究表明这些系统容易受到图操纵攻击，攻击者通过操纵图结构来逃避检测。虽然之前的方法讨论过这类攻击，但没有提供稳健的检测解决方案，限制了PIDS的实际应用。为了应对这一挑战，我们提出MirGuard，一个稳健的异常检测框架，结合了逻辑感知的多视图增强与对比表示学习。MirGuard引入逻辑感知噪声注入(LNI)生成语义有效的图视图，确保所有增强都保留了溯源数据的底层因果语义。这些视图在逻辑保持对比学习框架中使用，鼓励模型学习对良性变换不变但对对抗性变化敏感的表示。在多个溯源数据集上的全面评估表明，MirGuard在抵抗各种图操纵攻击方面的稳健性显著优于最先进的检测器，同时没有牺牲检测性能和效率。我们的工作是首个针对增强PIDS抵御此类对抗威胁的针对性研究，为现代网络安全挑战提供了稳健有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning-based Provenance-based Intrusion Detection Systems (PIDSes) havebecome essential tools for anomaly detection in host systems due to theirability to capture rich contextual and structural information, as well as theirpotential to detect unknown attacks. However, recent studies have shown thatthese systems are vulnerable to graph manipulation attacks, where attackersmanipulate the graph structure to evade detection. While some previousapproaches have discussed this type of attack, none have fully addressed itwith a robust detection solution, limiting the practical applicability ofPIDSes.  To address this challenge, we propose MirGuard, a robust anomaly detectionframework that combines logic-aware multi-view augmentation with contrastiverepresentation learning. Rather than applying arbitrary structuralperturbations, MirGuard introduces Logic-Aware Noise Injection (LNI) togenerate semantically valid graph views, ensuring that all augmentationspreserve the underlying causal semantics of the provenance data. These viewsare then used in a Logic-Preserving Contrastive Learning framework, whichencourages the model to learn representations that are invariant to benigntransformations but sensitive to adversarial inconsistencies. Comprehensiveevaluations on multiple provenance datasets demonstrate that MirGuardsignificantly outperforms state-of-the-art detectors in robustness againstvarious graph manipulation attacks without sacrificing detection performanceand efficiency. Our work represents the first targeted study to enhance PIDSagainst such adversarial threats, providing a robust and effective solution tomodern cybersecurity challenges.</description>
      <author>example@mail.com (Anyuan Sang, Lu Zhou, Li Yang, Junbo Jia, Huipeng Yang, Pengbin Feng, Jianfeng Ma)</author>
      <guid isPermaLink="false">2508.10639v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning</title>
      <link>http://arxiv.org/abs/2508.10298v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SynBrain生成框架，用于模拟视觉语义到神经响应的转化，解决了现有方法难以同时建模生物变性和功能一致性的问题。SynBrain包含BrainVAE模型和语义到神经映射器两个关键组件，实验证明其在视觉到fMRI编码性能上超越现有方法，并能高效适应新主体并合成高质量fMRI信号。&lt;h4&gt;背景&lt;/h4&gt;视觉到神经映射本质上是一对多关系，相同视觉输入在不同试验、情境和主体中会引发不同血流动力学响应。现有确定性方法难以同时建模这种生物变性和编码刺激信息的基本功能一致性。&lt;h4&gt;目的&lt;/h4&gt;提出一个生成框架，能够以概率化和生物学可解释的方式模拟视觉语义到神经响应的转化，同时捕捉生物变性和功能一致性。&lt;h4&gt;方法&lt;/h4&gt;SynBrain框架包含两个关键组件：(i) BrainVAE模型通过概率学习将神经表示建模为连续概率分布，同时通过视觉语义约束保持功能一致性；(ii) 语义到神经映射器作为语义传输通路，将视觉语义投影到神经响应流形，促进高保真fMRI合成。&lt;h4&gt;主要发现&lt;/h4&gt;SynBrain在特定主体的视觉到fMRI编码性能上超越了最先进方法；能够高效适应新主体，使用少量数据；合成的信号能有效提高数据有限情况下的fMRI到图像解码性能；揭示了试验和主体之间的功能一致性，合成的信号捕捉了由生物神经变性的可解释模式。&lt;h4&gt;结论&lt;/h4&gt;SynBrain是一个有效的生成框架，能够模拟视觉语义到神经响应的转化，同时捕捉生物变性和功能一致性，为视觉神经编码研究提供了新的工具。&lt;h4&gt;翻译&lt;/h4&gt;解读视觉刺激如何转化为皮层响应是计算神经科学中的一个基本挑战。这种视觉到神经映射本质上是一对多的关系，因为相同的视觉输入在不同试验、情境和主体中会可靠地引发不同的血流动力学响应。然而，现有的确定性方法难以同时建模这种生物变性和编码刺激信息的基本功能一致性。为了解决这些局限性，我们提出了SynBrain，一个生成框架，能够以概率化和生物学可解释的方式模拟视觉语义到神经响应的转化。SynBrain引入了两个关键组件：(i) BrainVAE模型通过概率学习将神经表示建模为连续概率分布，同时通过视觉语义约束保持功能一致性；(ii) 语义到神经映射器作为语义传输通路，将视觉语义投影到神经响应流形，促进高保真fMRI合成。实验结果表明，SynBrain在特定主体的视觉到fMRI编码性能上超越了最先进的方法。此外，SynBrain能够高效适应新主体，使用少量数据，并合成高质量的fMRI信号，这些信号在提高数据有限的fMRI到图像解码性能方面是有效的。除此之外，SynBrain揭示了试验和主体之间的功能一致性，合成的信号捕捉了由生物神经变性的可解释模式。代码将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deciphering how visual stimuli are transformed into cortical responses is afundamental challenge in computational neuroscience. This visual-to-neuralmapping is inherently a one-to-many relationship, as identical visual inputsreliably evoke variable hemodynamic responses across trials, contexts, andsubjects. However, existing deterministic methods struggle to simultaneouslymodel this biological variability while capturing the underlying functionalconsistency that encodes stimulus information. To address these limitations, wepropose SynBrain, a generative framework that simulates the transformation fromvisual semantics to neural responses in a probabilistic and biologicallyinterpretable manner. SynBrain introduces two key components: (i) BrainVAEmodels neural representations as continuous probability distributions viaprobabilistic learning while maintaining functional consistency through visualsemantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantictransmission pathway, projecting visual semantics into the neural responsemanifold to facilitate high-fidelity fMRI synthesis. Experimental resultsdemonstrate that SynBrain surpasses state-of-the-art methods insubject-specific visual-to-fMRI encoding performance. Furthermore, SynBrainadapts efficiently to new subjects with few-shot data and synthesizeshigh-quality fMRI signals that are effective in improving data-limitedfMRI-to-image decoding performance. Beyond that, SynBrain reveals functionalconsistency across trials and subjects, with synthesized signals capturinginterpretable patterns shaped by biological neural variability. The code willbe made publicly available.</description>
      <author>example@mail.com (Weijian Mai, Jiamin Wu, Yu Zhu, Zhouheng Yao, Dongzhan Zhou, Andrew F. Luo, Qihao Zheng, Wanli Ouyang, Chunfeng Song)</author>
      <guid isPermaLink="false">2508.10298v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation</title>
      <link>http://arxiv.org/abs/2508.10281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个新的时间动作分割(TAS)框架，专门用于识别花样滑冰跳跃动作的类型和时间，该框架结合了三维特性和跳跃动作的语义过程。&lt;h4&gt;背景&lt;/h4&gt;从视频中理解人类动作在多个领域都起着关键作用，包括体育分析。在花样滑冰中，准确识别滑冰者跳跃的类型和时间对于客观评估表现至关重要。然而，由于跳跃过程的细粒度和复杂性，这项任务通常需要专家级知识。现有的TAS方法在应用于花样滑冰时有两个主要限制：标注数据不足，且现有方法没有考虑跳跃动作固有的三维方面和程序结构。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的TAS框架，专门用于花样滑冰跳跃识别，该框架明确结合了跳跃动作的三维特性和语义过程。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的视角不变的、花样滑冰特定的姿态表示学习方法(VIFSS)，结合对比学习作为预训练和动作分类作为微调；构建了FS-Jump3D，这是第一个专门用于花样滑冰跳跃的公开可用3D姿态数据集；引入了一种细粒度标注方案，标记'入场(准备)'和'落地'阶段，使TAS模型能够学习跳跃的程序结构。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在元素级别的TAS上实现了超过92%的F1@50，需要识别跳跃类型和旋转级别；当微调数据有限时，视角不变的对比预训练特别有效，突显了该方法在实际场景中的实用性。&lt;h4&gt;结论&lt;/h4&gt;提出的TAS框架有效解决了花样滑冰跳跃识别中的挑战，通过结合三维特性和语义过程，实现了高精度的跳跃识别，特别是在数据有限的情况下表现良好。&lt;h4&gt;翻译&lt;/h4&gt;从视频中理解人类动作在各个领域都起着关键作用，包括体育分析。在花样滑冰中，准确识别滑冰者跳跃的类型和时间对于客观评估表现至关重要。然而，由于跳跃过程的细粒度和复杂性，这项任务通常需要专家级知识。虽然最近的方法试图使用时间动作分割(TAS)来自动化这项任务，但TAS在应用于花样滑冰时有两个主要限制：标注数据不足，且现有方法没有考虑跳跃动作固有的三维方面和程序结构。在这项工作中，我们提出了一个新的花样滑冰跳跃TAS框架，明确结合了跳跃动作的三维特性和语义过程。首先，我们提出了一种新的视角不变的、花样滑冰特定的姿态表示学习方法(VIFSS)，结合了对比学习作为预训练和动作分类作为微调。对于视角不变的对比预训练，我们构建了FS-Jump3D，这是第一个专门用于花样滑冰跳跃的公开可用3D姿态数据集。其次，我们引入了一种细粒度标注方案，标记'入场(准备)'和'落地'阶段，使TAS模型能够学习跳跃的程序结构。大量实验证明了我们框架的有效性。我们的方法在元素级别的TAS上实现了超过92%的F1@50，需要识别跳跃类型和旋转级别。此外，我们表明当微调数据有限时，视角不变的对比预训练特别有效，突显了我们的方法在实际场景中的实用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决花样滑冰跳跃动作的时间动作分割(TAS)问题，即自动识别视频中跳跃的类型和精确时机。这个问题在现实中很重要，因为当前花样滑冰评判依赖人工识别和记录跳跃类型和时机，需要专家知识和大量时间精力。自动化这一过程可以减轻裁判负担，提高评判效率和客观性，同时为花样滑冰训练提供更精确的反馈。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：2D姿势特征对视角变化敏感，而直接使用3D姿势坐标存在质量瓶颈。他们设计了两阶段学习框架：先用对比学习预训练姿势编码器获取视角不变的表示，然后在花样滑冰特定动作分类任务上微调。借鉴了Pr-VIPE和CV-MIM的对比学习思想、JointFormer作为姿势编码器，以及基于BiGRU的时序建模架构。但自主创新了FS-Jump3D数据集构建和跳跃程序感知的标注策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习既视角不变又适应花样滑冰特定动作的姿势表示，并利用跳跃的程序结构信息。整体流程分为三阶段：1)构建FS-Jump3D数据集和细粒度标注；2)姿势表示学习，包括视角不变的对比预训练和花样滑冰特定的动作分类微调；3)将学习到的VIFSS姿势特征输入TAS模型进行时间动作分割。对比预训练阶段通过随机虚拟相机生成同一姿势的不同视角，微调阶段使用SkatingVerse数据集使模型适应花样滑冰动作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)FS-Jump3D数据集，首个专门针对花样滑冰跳跃的3D姿势数据集；2)细粒度跳跃程序感知标注策略，将跳跃分为进入、跳跃和着陆三阶段；3)VIFSS姿势表示学习框架，结合对比预训练和领域特定微调。相比之前工作，不同之处在于：使用3D姿势而非2D姿势捕捉动作三维特性；考虑跳跃的完整程序结构而非仅跳跃本身；通过姿势嵌入而非直接坐标表示更高级的运动语义；在标注数据有限情况下表现优异。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了VIFSS方法，通过首个花样滑冰3D姿势数据集和跳跃程序感知的细粒度标注策略，结合视角不变的对比预训练和领域特定的动作分类微调，显著提升了花样滑冰跳跃动作时间分割任务的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding human actions from videos plays a critical role across variousdomains, including sports analytics. In figure skating, accurately recognizingthe type and timing of jumps a skater performs is essential for objectiveperformance evaluation. However, this task typically requires expert-levelknowledge due to the fine-grained and complex nature of jump procedures. Whilerecent approaches have attempted to automate this task using Temporal ActionSegmentation (TAS), there are two major limitations to TAS for figure skating:the annotated data is insufficient, and existing methods do not account for theinherent three-dimensional aspects and procedural structure of jump actions. Inthis work, we propose a new TAS framework for figure skating jumps thatexplicitly incorporates both the three-dimensional nature and the semanticprocedure of jump movements. First, we propose a novel View-Invariant, FigureSkating-Specific pose representation learning approach (VIFSS) that combinescontrastive learning as pre-training and action classification as fine-tuning.For view-invariant contrastive pre-training, we construct FS-Jump3D, the firstpublicly available 3D pose dataset specialized for figure skating jumps.Second, we introduce a fine-grained annotation scheme that marks the ``entry(preparation)'' and ``landing'' phases, enabling TAS models to learn theprocedural structure of jumps. Extensive experiments demonstrate theeffectiveness of our framework. Our method achieves over 92% F1@50 onelement-level TAS, which requires recognizing both jump types and rotationlevels. Furthermore, we show that view-invariant contrastive pre-training isparticularly effective when fine-tuning data is limited, highlighting thepracticality of our approach in real-world scenarios.</description>
      <author>example@mail.com (Ryota Tanaka, Tomohiro Suzuki, Keisuke Fujii)</author>
      <guid isPermaLink="false">2508.10281v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization</title>
      <link>http://arxiv.org/abs/2508.09560v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了WeatherPrompt，一种多模态学习范式，通过融合图像嵌入和文本上下文建立天气不变表示，解决无人机视觉地理定位在恶劣天气条件下的性能下降问题。方法包含无需训练的天气推理机制和由文本嵌入驱动的动态门控机制，在多种天气条件下实现了优于现有方法的召回率。&lt;h4&gt;背景&lt;/h4&gt;无人机视觉地理定位在天气扰动（如雨和雾）下面临严重性能下降问题。现有方法有两个固有局限性：1) 严重依赖于有限的天气类别，限制了泛化能力；2) 通过伪天气类别对纠缠的场景-天气特征进行解缠分是不优的。&lt;h4&gt;目的&lt;/h4&gt;提出WeatherPrompt方法，通过融合图像嵌入和文本上下文来建立天气不变表示，解决现有方法在天气变化条件下的地理定位性能下降问题。&lt;h4&gt;方法&lt;/h4&gt;WeatherPrompt是一个多模态学习范式，包含两个关键贡献：1) 无需训练的天气推理机制，利用现成的多模态模型合成多天气文本描述，提高对复杂天气的可扩展性；2) 由文本嵌入驱动的动态门控机制的多模态框架，自适应地重新加权并跨模态融合视觉特征，通过图文对比学习和图文匹配等跨模态目标进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;在多种天气条件下，WeatherPrompt方法实现了与最先进的无人机地理定位方法相竞争的召回率。特别是在夜间条件下，Recall@1提高了13.37%，在雾和雪条件下提高了18.69%。&lt;h4&gt;结论&lt;/h4&gt;WeatherPrompt方法通过多模态学习和天气推理机制，有效解决了无人机视觉地理定位在恶劣天气条件下的性能下降问题，显著提高了不同天气条件下的定位精度。&lt;h4&gt;翻译&lt;/h4&gt;无人机视觉地理定位在天气扰动（如下雨和起雾）下面临严重性能下降，现有方法存在两个固有局限：1) 严重依赖有限的天气类别，限制了泛化能力；2) 通过伪天气类别对纠缠的场景-天气特征进行解缠分是不优的。我们提出了WeatherPrompt，一种多模态学习范式，通过融合图像嵌入和文本上下文建立天气不变表示。我们的框架引入两个关键贡献：首先，一种无需训练的天气推理机制，利用现成的多模态模型合成多天气文本描述，提高对未见或复杂天气的可扩展性，并能反映不同的天气强度。其次，为了更好地解缠场景和天气特征，我们提出了一个由文本嵌入驱动的动态门控机制的多模态框架，能够自适应地重新加权并跨模态融合视觉特征。该框架通过跨模态目标（包括图文对比学习和图文匹配）进一步优化，使不同天气条件下的同一场景在表示空间中更加接近。大量实验验证了，在多种天气条件下，我们的方法与最先进的无人机地理定位方法相比具有竞争力的召回率。值得注意的是，它在夜间条件下将Recall@1提高了13.37%，在雾和雪条件下提高了18.69%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual geo-localization for drones faces critical degradation under weatherperturbations, \eg, rain and fog, where existing methods struggle with twoinherent limitations: 1) Heavy reliance on limited weather categories thatconstrain generalization, and 2) Suboptimal disentanglement of entangledscene-weather features through pseudo weather categories. We presentWeatherPrompt, a multi-modality learning paradigm that establishesweather-invariant representations through fusing the image embedding with thetext context. Our framework introduces two key contributions: First, aTraining-free Weather Reasoning mechanism that employs off-the-shelf largemulti-modality models to synthesize multi-weather textual descriptions throughhuman-like reasoning. It improves the scalability to unseen or complex weather,and could reflect different weather strength. Second, to better disentangle thescene and weather feature, we propose a multi-modality framework with thedynamic gating mechanism driven by the text embedding to adaptively reweightand fuse visual features across modalities. The framework is further optimizedby the cross-modal objectives, including image-text contrastive learning andimage-text matching, which maps the same scene with different weatherconditions closer in the respresentation space. Extensive experiments validatethat, under diverse weather conditions, our method achieves competitive recallrates compared to state-of-the-art drone geo-localization methods. Notably, itimproves Recall@1 by +13.37\% under night conditions and by 18.69\% under fogand snow conditions.</description>
      <author>example@mail.com (Jiahao Wen, Hang Yu, Zhedong Zheng)</author>
      <guid isPermaLink="false">2508.09560v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Fairness in Autoencoders for Node-Level Graph Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.10785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in ECAI-2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为DECAF-GAD的框架，用于解决基于自编码器的图异常检测模型中的公平性问题，能够在减轻偏见的同时保持异常检测性能。&lt;h4&gt;背景&lt;/h4&gt;图异常检测（GAD）在多个领域变得越来越重要，图神经网络的发展显著提高了GAD方法的性能。然而，GAD中的公平性问题尚未得到充分探索，基于GNN的GAD模型可能会继承和放大训练数据中的偏见，导致不公平结果。现有公平性研究大多针对节点分类任务，而非异常检测任务，后者通常使用基于自编码器的结构。&lt;h4&gt;目的&lt;/h4&gt;解决基于自编码器的图异常检测模型中的公平性问题，减轻模型偏见的同时保持异常检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出DECAF-GAD（DisEntangled Counterfactual Adversarial Fair GAD）框架，引入结构因果模型（SCM）将敏感属性与学习到的表示分离，基于此因果框架设计专门的自编码器架构和公平性引导的损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;通过在合成和真实世界数据集上的大量实验，DECAF-GAD不仅实现了具有竞争力的异常检测性能，相比基准GAD方法还显著提高了公平性指标。&lt;h4&gt;结论&lt;/h4&gt;DECAF-GAD框架能够有效减轻图异常检测模型中的偏见问题，同时保持优异的异常检测性能。&lt;h4&gt;翻译&lt;/h4&gt;图异常检测（GAD）已成为各个领域中日益重要的任务。随着图神经网络（GNNs）的快速发展，GAD方法已取得了显著的性能提升。然而，GAD中的公平性考虑在很大程度上仍未被充分探索。实际上，基于GNN的GAD模型可能会继承和放大训练数据中存在的偏见，从而导致不公平的结果。虽然现有工作主要集中在开发公平的GNN上，但大多数方法针对的是节点分类任务，在这些任务中，模型通常依赖简单的层架构而非自编码器结构，而后者是异常检测应用中最广泛使用的架构。为了解决基于自编码器的GAD模型中的公平性问题，我们提出了DECAF-GAD（DisEntangled Counterfactual Adversarial Fair GAD）框架，该框架能够在减轻偏见的同时保持GAD性能。具体而言，我们引入了一个结构因果模型（SCM）来将敏感属性与学习到的表示分离。基于这个因果框架，我们制定了一个专门的自编码器架构以及一个公平性引导的损失函数。通过在合成和真实世界数据集上的大量实验，我们证明了DECAF-GAD不仅实现了具有竞争力的异常检测性能，而且与基准GAD方法相比显著提高了公平性指标。我们的代码可在https://github.com/Tlhey/decaf_code获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph anomaly detection (GAD) has become an increasingly important taskacross various domains. With the rapid development of graph neural networks(GNNs), GAD methods have achieved significant performance improvements.However, fairness considerations in GAD remain largely underexplored. Indeed,GNN-based GAD models can inherit and amplify biases present in training data,potentially leading to unfair outcomes. While existing efforts have focused ondeveloping fair GNNs, most approaches target node classification tasks, wheremodels often rely on simple layer architectures rather than autoencoder-basedstructures, which are the most widely used architecturs for anomaly detection.To address fairness in autoencoder-based GAD models, we propose\textbf{D}is\textbf{E}ntangled \textbf{C}ounterfactual \textbf{A}dversarial\textbf{F}air (DECAF)-GAD, a framework that alleviates bias while preservingGAD performance. Specifically, we introduce a structural causal model (SCM) todisentangle sensitive attributes from learned representations. Based on thiscausal framework, we formulate a specialized autoencoder architecture alongwith a fairness-guided loss function. Through extensive experiments on bothsynthetic and real-world datasets, we demonstrate that DECAF-GAD not onlyachieves competitive anomaly detection performance but also significantlyenhances fairness metrics compared to baseline GAD methods. Our code isavailable at https://github.com/Tlhey/decaf_code.</description>
      <author>example@mail.com (Shouju Wang, Yuchen Song, Sheng'en Li, Dongmian Zou)</author>
      <guid isPermaLink="false">2508.10785v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning</title>
      <link>http://arxiv.org/abs/2508.10747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种稀疏的、目标感知的图神经网络表示方法，用于解决广义规划中的密集图表示问题，有效提高了大规模网格环境中的规划性能。&lt;h4&gt;背景&lt;/h4&gt;使用深度强化学习结合图神经网络进行广义规划在PDDL描述的符号规划领域显示出良好结果，但现有方法通常将规划状态表示为全连接图，导致边缘信息组合爆炸和大规模问题下的稀疏性，特别是在大型网格环境中。&lt;h4&gt;目的&lt;/h4&gt;解决密集表示导致节点级信息稀释、内存需求呈指数增长以及大规模问题学习不可行的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出一种稀疏的、目标感知的GNN表示方法，选择性编码相关的局部关系，并明确整合与目标相关的空间特征；在网格世界内基于PDDL设计新的无人机任务场景，有效模拟真实的任务执行环境。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法能够有效扩展到密集图表示不可行的大型网格尺寸，并显著提高策略泛化和成功率。&lt;h4&gt;结论&lt;/h4&gt;研究结果为解决现实的大规模广义规划任务提供了实际基础。&lt;h4&gt;翻译&lt;/h4&gt;使用深度强化学习结合图神经网络的广义规划已在各种由PDDL描述的符号规划领域显示出良好的结果。然而，现有方法通常将规划状态表示为全连接图，导致边缘信息组合爆炸和问题规模增长时的显著稀疏性，特别是在大型网格环境中尤为明显。这种密集表示导致节点级信息稀释，内存需求呈指数增长，最终使得大规模问题的学习变得不可行。为应对这些挑战，我们提出了一种稀疏的、目标感知的GNN表示，选择性编码相关的局部关系并明确整合与目标相关的空间特征。我们通过在网格世界内基于PDDL设计新的无人机任务场景来验证我们的方法，有效模拟了真实的任务执行环境。实验结果表明，我们的方法能够有效扩展到密集图表示不可行的大型网格尺寸，并显著提高策略泛化和成功率。我们的发现为解决现实的大规模广义规划任务提供了实际基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized planning using deep reinforcement learning (RL) combined withgraph neural networks (GNNs) has shown promising results in various symbolicplanning domains described by PDDL. However, existing approaches typicallyrepresent planning states as fully connected graphs, leading to a combinatorialexplosion in edge information and substantial sparsity as problem scales grow,especially evident in large grid-based environments. This dense representationresults in diluted node-level information, exponentially increases memoryrequirements, and ultimately makes learning infeasible for larger-scaleproblems. To address these challenges, we propose a sparse, goal-aware GNNrepresentation that selectively encodes relevant local relationships andexplicitly integrates spatial features related to the goal. We validate ourapproach by designing novel drone mission scenarios based on PDDL within a gridworld, effectively simulating realistic mission execution environments. Ourexperimental results demonstrate that our method scales effectively to largergrid sizes previously infeasible with dense graph representations andsubstantially improves policy generalization and success rates. Our findingsprovide a practical foundation for addressing realistic, large-scalegeneralized planning tasks.</description>
      <author>example@mail.com (Sangwoo Jeon, Juchul Shin, Gyeong-Tae Kim, YeonJe Cho, Seongwoo Kim)</author>
      <guid isPermaLink="false">2508.10747v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Graph Learning via Logic-Based Weisfeiler-Leman Variants and Tabularization</title>
      <link>http://arxiv.org/abs/2508.10651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于Weisfeiler-Leman算法变体的图分类新方法，通过将图数据表格化后应用表格数据方法，在保持与最先进方法相当准确性的同时提高了效率。&lt;h4&gt;背景&lt;/h4&gt;图分类是机器学习中的重要任务，传统方法如图神经网络和图核方法在准确性和效率方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的图分类方法，能够达到与现有最先进方法相当的准确性，同时在计算效率方面有所提升。&lt;h4&gt;方法&lt;/h4&gt;通过修改底层逻辑框架获得Weisfeiler-Leman算法的变体，将图数据表格化，然后应用表格数据方法进行分类，并在十二个基准数据集上进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法能够达到与最先进的图神经网络和图核方法相当的准确性，同时在时间或内存效率方面更具优势，具体取决于数据集。&lt;h4&gt;结论&lt;/h4&gt;基于Weisfeiler-Leman变体的表格化图分类方法是一种有效且高效的图分类解决方案，同时研究还展示了直接从图数据提取可解释模态逻辑公式的可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖的图分类方法，通过Weisfeiler-Leman算法的变体将图数据表格化，然后应用表格数据方法。我们研究了一类通过修改底层逻辑框架获得的Weisfeiler-Leman变体，并建立了它们表达能力的精确理论表征。随后，我们在十二个涵盖不同领域的基准数据集上测试了两种选定的变体。实验表明，我们的方法能够达到最先进的图神经网络和图核方法的准确性，同时在时间或内存效率方面更具优势，具体取决于数据集。我们还简要讨论了直接从图数据集中提取可解释的模态逻辑公式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel approach for graph classification based on tabularizinggraph data via variants of the Weisfeiler-Leman algorithm and then applyingmethods for tabular data. We investigate a comprehensive class ofWeisfeiler-Leman variants obtained by modifying the underlying logicalframework and establish a precise theoretical characterization of theirexpressive power. We then test two selected variants on twelve benchmarkdatasets that span a range of different domains. The experiments demonstratethat our approach matches the accuracy of state-of-the-art graph neuralnetworks and graph kernels while being more time or memory efficient, dependingon the dataset. We also briefly discuss directly extracting interpretable modallogic formulas from graph datasets.</description>
      <author>example@mail.com (Reijo Jaakkola, Tomi Janhunen, Antti Kuusisto, Magdalena Ortiz, Matias Selin, Mantas Šimkus)</author>
      <guid isPermaLink="false">2508.10651v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>GNN-based Unified Deep Learning</title>
      <link>http://arxiv.org/abs/2508.10583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为'统一学习'的新范式，用于解决医学影像领域中深度学习模型在不同分布下的泛化能力问题。该方法通过将不同架构的模型编码为图表示，在共享图学习空间中实现统一，并通过GNN指导模型优化，实现了跨架构和分布的知识迁移，提高了模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型在医学影像领域往往难以维持泛化能力，特别是在领域断裂场景下，即当分布因成像技术、采集协议、患者群体、人口统计和设备的差异而发生变化时。实践中，每家医院可能需要训练不同参数（学习任务、宽度和深度）的本地模型，例如使用欧几里得架构（MLPs、CNNs）处理规则数据，或使用非欧几里得架构（GNNs）处理不规则数据（如脑连接组）。&lt;h4&gt;目的&lt;/h4&gt;研究目的是解决如何跨数据集一致地训练这些异构模型，同时提高每个模型泛化能力的开放问题。&lt;h4&gt;方法&lt;/h4&gt;研究提出了'统一学习'这一新范式，将每个模型编码为图表示，在共享图学习空间中实现统一。通过解耦单个模型的参数，并通过统一的GNN（uGNN）控制它们，支持在不同架构（MLPs、CNNs、GNNs）和分布之间进行参数共享和知识迁移。&lt;h4&gt;主要发现&lt;/h4&gt;在MorphoMNIST和两个MedMNIST基准（PneumoniaMNIST和BreastMNIST）上的评估表明，统一学习提高了模型在独特分布上训练并在混合分布上测试时的性能，显示出对具有大分布偏移的未见数据的强大鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;统一学习是一种有效的范式，能够提高医学影像领域深度学习模型在不同分布下的泛化能力，通过实现跨架构和分布的知识迁移，增强了模型对未见数据的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型通常难以在医学影像中保持泛化能力，特别是在领域断裂场景下，当分布因不同的成像技术、采集协议、患者群体、人口统计和设备而变化时。在实践中，每家医院可能需要训练不同的模型（学习任务、宽度和深度各不相同）以匹配本地数据。例如，一家医院可能使用欧几里得架构（如MLPs和CNNs）处理表格或网格状图像数据，而另一家医院可能需要非欧几里得架构（如图神经网络GNNs）处理不规则数据（如脑连接组）。如何跨数据集一致地训练这些异构模型，同时提高每个模型的泛化能力，仍然是一个开放问题。我们提出了统一学习，一种新范式，它将每个模型编码为图表示，在共享图学习空间中实现统一。然后，一个GNN指导这些统一模型的优化。通过解耦单个模型的参数并通过统一的GNN（uGNN）控制它们，我们的方法支持在不同架构（MLPs、CNNs、GNNs）和分布之间进行参数共享和知识迁移，提高泛化能力。在MorphoMNIST和两个MedMNIST基准（PneumoniaMNIST和BreastMNIST）上的评估表明，当模型在独特分布上训练并在混合分布上测试时，统一学习提高了性能，显示出对具有大分布偏移的未见数据的强大鲁棒性。代码和基准：https://github.com/basiralab/uGNN&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models often struggle to maintain generalizability in medicalimaging, particularly under domain-fracture scenarios where distribution shiftsarise from varying imaging techniques, acquisition protocols, patientpopulations, demographics, and equipment. In practice, each hospital may needto train distinct models - differing in learning task, width, and depth - tomatch local data. For example, one hospital may use Euclidean architecturessuch as MLPs and CNNs for tabular or grid-like image data, while another mayrequire non-Euclidean architectures such as graph neural networks (GNNs) forirregular data like brain connectomes. How to train such heterogeneous modelscoherently across datasets, while enhancing each model's generalizability,remains an open problem. We propose unified learning, a new paradigm thatencodes each model into a graph representation, enabling unification in ashared graph learning space. A GNN then guides optimization of these unifiedmodels. By decoupling parameters of individual models and controlling themthrough a unified GNN (uGNN), our method supports parameter sharing andknowledge transfer across varying architectures (MLPs, CNNs, GNNs) anddistributions, improving generalizability. Evaluations on MorphoMNIST and twoMedMNIST benchmarks - PneumoniaMNIST and BreastMNIST - show that unifiedlearning boosts performance when models are trained on unique distributions andtested on mixed ones, demonstrating strong robustness to unseen data with largedistribution shifts. Code and benchmarks: https://github.com/basiralab/uGNN</description>
      <author>example@mail.com (Furkan Pala, Islem Rekik)</author>
      <guid isPermaLink="false">2508.10583v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>GraphFedMIG: Tackling Class Imbalance in Federated Graph Learning via Mutual Information-Guided Generation</title>
      <link>http://arxiv.org/abs/2508.10471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GraphFedMIG，一种新的联邦图学习框架，用于解决联邦学习中的统计异质性和类别不平衡问题，特别是针对少数类节点的识别挑战。&lt;h4&gt;背景&lt;/h4&gt;联邦图学习(FGL)允许多个客户端在不共享私有图数据的情况下协作训练图神经网络。FGL面临统计异质性的挑战，即客户端间的非独立同分布数据分布会严重影响模型性能。类别不平衡是一种特别具有破坏性的形式，导致全局模型偏向多数类，无法识别少数但关键的事件。在FGL中，这一问题更加严重，因为少数类节点通常被有偏见的邻域信息包围，阻碍了表达性嵌入的学习。&lt;h4&gt;目的&lt;/h4&gt;解决联邦图学习中由统计异质性和类别不平衡导致的模型性能问题，特别是提高对少数类节点的识别能力。&lt;h4&gt;方法&lt;/h4&gt;提出GraphFedMIG框架，将问题重新构造成联邦生成数据增强任务。采用分层生成对抗网络，每个客户端训练本地生成器来合成高保真特征表示。客户端被分组到聚类中，每个聚类共享一个专用的判别器。设计了一种互信息引导机制来指导这些客户端生成器的演变，通过计算每个客户端的独特信息价值，修正本地生成器参数，确保后续生成专注于产生高价值的少数类特征。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实数据集上进行的广泛实验结果表明，与其它基线相比，所提出的GraphFedMIG具有优越性。&lt;h4&gt;结论&lt;/h4&gt;GraphFedMIG框架有效解决了联邦图学习中的统计异质性和类别不平衡问题，能够更好地识别少数类节点，提高模型性能。&lt;h4&gt;翻译&lt;/h4&gt;联邦图学习(FGL)使多个客户端能够在不共享其私有、分散的图数据的情况下协作训练强大的图神经网络。继承自通用联邦学习，FGL面临着统计异质性的严重挑战，即客户端间的非独立同分布数据分布会严重影响模型性能。其中特别具有破坏性的一种形式是类别不平衡，它导致全局模型偏向多数类，无法识别少数但关键的事件。在FGL中，这一问题更加严重，因为少数类节点通常被有偏见的邻域信息包围，阻碍了表达性嵌入的学习。为应对这一挑战，我们提出了GraphFedMIG，一种新颖的FGL框架，将问题重新构造成联邦生成数据增强任务。GraphFedMIG采用分层生成对抗网络，每个客户端训练本地生成器来合成高保真特征表示。为了提供定制化的监督，客户端被分组到聚类中，每个聚类共享一个专用的判别器。关键的是，该框架设计了一种互信息引导机制来指导这些客户端生成器的演变。通过计算每个客户端的独特信息价值，该机制修正本地生成器参数，确保后续互信息引导的生成专注于产生高价值的少数类特征。我们在四个真实数据集上进行了广泛的实验，结果表明与其它基线相比，所提出的GraphFedMIG具有优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated graph learning (FGL) enables multiple clients to collaborativelytrain powerful graph neural networks without sharing their private,decentralized graph data. Inherited from generic federated learning, FGL iscritically challenged by statistical heterogeneity, where non-IID datadistributions across clients can severely impair model performance. Aparticularly destructive form of this is class imbalance, which causes theglobal model to become biased towards majority classes and fail at identifyingrare but critical events. This issue is exacerbated in FGL, as nodes from aminority class are often surrounded by biased neighborhood information,hindering the learning of expressive embeddings. To grapple with thischallenge, we propose GraphFedMIG, a novel FGL framework that reframes theproblem as a federated generative data augmentation task. GraphFedMIG employs ahierarchical generative adversarial network where each client trains a localgenerator to synthesize high-fidelity feature representations. To providetailored supervision, clients are grouped into clusters, each sharing adedicated discriminator. Crucially, the framework designs a mutualinformation-guided mechanism to steer the evolution of these client generators.By calculating each client's unique informational value, this mechanismcorrects the local generator parameters, ensuring that subsequent rounds ofmutual information-guided generation are focused on producing high-value,minority-class features. We conduct extensive experiments on four real-worlddatasets, and the results demonstrate the superiority of the proposedGraphFedMIG compared with other baselines.</description>
      <author>example@mail.com (Xinrui Li, Qilin Fan, Tianfu Wang, Kaiwen Wei, Ke Yu, Xu Zhang)</author>
      <guid isPermaLink="false">2508.10471v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Influence Maximization in Multi-layer Social Networks Based on Differentiated Graph Embeddings</title>
      <link>http://arxiv.org/abs/2508.10289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Inf-MDE，一种新颖的多层影响力最大化方法，利用差异化图嵌入解决社交网络中识别影响力节点的问题&lt;h4&gt;背景&lt;/h4&gt;现有方法通常忽略本地意见领袖倾向，导致种子节点影响力范围重叠；基于普通图神经网络的方法在消息传递中难以有效聚合影响力特征；当前技术未能充分处理社交网络的多层特性和节点异质性&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在识别影响力节点时的局限性，包括忽略本地意见领袖倾向、无法有效聚合影响力特征、未能处理社交网络的多层特性和节点异质性&lt;h4&gt;方法&lt;/h4&gt;提出Inf-MDE方法，使用多层网络结构建模社交关系，提取自影响力传播子图消除节点嵌入与传播动力学间的表示偏差，并在图神经网络设计中集成自适应局部影响力聚合机制，根据局部上下文和影响力强度动态调整影响力特征聚合&lt;h4&gt;主要发现&lt;/h4&gt;在四个不同的多层社交网络数据集上的实验表明，Inf-MDE显著优于现有的最先进方法&lt;h4&gt;结论&lt;/h4&gt;Inf-MDE通过解决现有方法的局限性，能够更有效地识别社交网络中的影响力节点，为社交网络分析提供了新的解决方案&lt;h4&gt;翻译&lt;/h4&gt;在社交网络分析中识别影响力节点至关重要。现有方法通常忽略本地意见领袖的倾向，导致种子节点的影响力范围重叠。此外，基于普通图神经网络的方法在消息传递过程中难以有效聚合影响力特征，特别是在影响力强度不同的情况下。当前技术也未能充分解决社交网络的多层特性和节点异质性。为解决这些问题，本文提出了Inf-MDE，一种新颖的利用差异化图嵌入的多层影响力最大化方法。Inf-MDE使用多层网络结构建模社交关系。该模型提取自影响力传播子图以消除节点嵌入与传播动力学之间的表示偏差。此外，Inf-MDE在其图神经网络设计中集成了自适应局部影响力聚合机制。该机制根据局部上下文和影响力强度在消息传递过程中动态调整影响力特征聚合，使其能够有效捕获层间传播异质性和层内扩散动力学。在四个不同的多层社交网络数据集上进行的大量实验表明，Inf-MDE显著优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying influential nodes is crucial in social network analysis. Existingmethods often neglect local opinion leader tendencies, resulting in overlappinginfluence ranges for seed nodes. Furthermore, approaches based on vanilla graphneural networks (GNNs) struggle to effectively aggregate influencecharacteristics during message passing, particularly with varying influenceintensities. Current techniques also fail to adequately address the multi-layernature of social networks and node heterogeneity. To address these issues, thispaper proposes Inf-MDE, a novel multi-layer influence maximization methodleveraging differentiated graph embedding. Inf-MDE models social relationshipsusing a multi-layer network structure. The model extracts a self-influencepropagation subgraph to eliminate the representation bias between nodeembeddings and propagation dynamics. Additionally, Inf-MDE incorporates anadaptive local influence aggregation mechanism within its GNN design. Thismechanism dynamically adjusts influence feature aggregation during messagepassing based on local context and influence intensity, enabling it toeffectively capture both inter-layer propagation heterogeneity and intra-layerdiffusion dynamics. Extensive experiments across four distinct multi-layersocial network datasets demonstrate that Inf-MDE significantly outperformsstate-of-the-art methods.</description>
      <author>example@mail.com (Ronghua Lin, Runbin Yao, Yijia Wang, Junjie Lin, Zhengyang Wu, Yong Tang)</author>
      <guid isPermaLink="false">2508.10289v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Less is More: Learning Graph Tasks with Just LLMs</title>
      <link>http://arxiv.org/abs/2508.10115v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了大型语言模型在图推理方面的能力，发现即使小型LLMs也能通过训练解决图任务并实现泛化，无需专门的图编码器。&lt;h4&gt;背景&lt;/h4&gt;对于大型语言模型来说，对图进行推理可以帮助解决许多问题。先前的工作试图通过研究如何将图序列化为文本以及如何结合图神经网络和LLMs来改进LLM的图推理能力，但这些方法的优点尚不明确。&lt;h4&gt;目的&lt;/h4&gt;通过实证研究回答三个研究问题：(1) LLMs是否能在没有专门图编码模型的情况下学习解决基本图任务？(2) LLMs是否能将学到的解决方案推广到未见过的图结构或任务？(3) 竞争性方法在学习图任务方面的优点是什么？&lt;h4&gt;方法&lt;/h4&gt;训练小型LLMs解决图任务，使用指导性的思维链（instructive chain-of-thought）解决方案进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;即使小型LLMs也能学习解决图任务，这种训练可以推广到新的任务和图结构，而无需专门的图编码器。&lt;h4&gt;结论&lt;/h4&gt;研究表明LLMs具有解决图任务的潜力，并通过适当的训练方法实现了泛化能力，为未来在图推理领域应用LLMs提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;对于大型语言模型（LLMs）来说，对图进行推理可以帮助解决许多问题。先前的工作试图通过研究如何将图序列化为文本以及如何结合图神经网络（GNNs）和LLMs来改进LLM的图推理能力。然而，这些方法的优点尚不清楚，因此我们通过实证研究回答了以下研究问题：(1) LLMs是否能够在没有专门的图编码模型的情况下学习解决基本的图任务？(2) LLMs是否能够将学到的解决方案推广到未见过的图结构或任务上？(3) 竞争性方法在学习图任务方面的优点是什么？我们表明，即使小型LLMs也能够通过使用指导性的思维链解决方案进行训练来学习解决图任务，并且这种训练可以推广到新的任务和图结构，而无需专门的图编码器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For large language models (LLMs), reasoning over graphs could help solve manyproblems. Prior work has tried to improve LLM graph reasoning by examining howbest to serialize graphs as text and by combining GNNs and LLMs. However, themerits of such approaches remain unclear, so we empirically answer thefollowing research questions: (1) Can LLMs learn to solve fundamental graphtasks without specialized graph encoding models?, (2) Can LLMs generalizelearned solutions to unseen graph structures or tasks?, and (3) What are themerits of competing approaches to learn graph tasks? We show that even smallLLMs can learn to solve graph tasks by training them with instructivechain-of-thought solutions, and this training generalizes, without specializedgraph encoders, to new tasks and graph structures.</description>
      <author>example@mail.com (Sola Shirai, Kavitha Srinivas, Julian Dolby, Michael Katz, Horst Samulowitz, Shirin Sohrabi)</author>
      <guid isPermaLink="false">2508.10115v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Quantum Mechanics to Organic Liquid Properties via a Universal Force Field</title>
      <link>http://arxiv.org/abs/2508.08575v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究人员开发了ByteFF-Pol，一种基于图神经网络的极化力场，能够基于量子力学数据准确预测宏观性质，克服了计算成本与精度之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;分子动力学模拟是研究凝聚相系统原子级结构和动力学的必要工具，但从从头算计算准确预测宏观属性仍面临重大挑战，通常受限于计算成本和模拟精度之间的权衡。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够基于高水平量子力学数据进行训练的力场，以准确预测宏观性质，弥合微观计算与宏观性质之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出ByteFF-Pol，一种图神经网络(GNN)参数化的可极化力场，完全基于高水平的量子力学数据进行训练，利用物理启发的力场形式和训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;ByteFF-Pol在预测各种小分子液体和电解质的热力学和输运性质方面表现出色，优于最先进的经典和机器学习力场，具有零样本预测能力。&lt;h4&gt;结论&lt;/h4&gt;这一进展在电解质设计和定制溶剂等应用方面具有变革性潜力，代表了数据驱动材料发现的关键一步。&lt;h4&gt;翻译&lt;/h4&gt;分子动力学(MD)模拟是揭示凝聚相系统结构和动力学原子级洞察力的必要工具。然而，从从头算计算准确预测宏观属性仍然是一个重大挑战，通常受限于计算成本和模拟精度之间的权衡。在此，我们提出了ByteFF-Pol，一种图神经网络(GNN)参数化的极化力场，完全基于高水平的量子力学(QM)数据进行训练。利用物理启发的力场形式和训练策略，ByteFF-Pol在预测各种小分子液体和电解质的热力学和输运性质方面表现出色，优于最先进的(SOTA)经典和机器学习力场。ByteFF-Pol的零样本预测能力弥合了微观QM计算与宏观液体性质之间的差距，使得探索以前难以处理的化学空间成为可能。这一进展在电解质设计和定制溶剂等应用方面具有变革性潜力，代表了数据驱动材料发现的关键一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular dynamics (MD) simulations are essential tools for unravelingatomistic insights into the structure and dynamics of condensed-phase systems.However, the universal and accurate prediction of macroscopic properties fromab initio calculations remains a significant challenge, often hindered by thetrade-off between computational cost and simulation accuracy. Here, we presentByteFF-Pol, a graph neural network (GNN)-parameterized polarizable force field,trained exclusively on high-level quantum mechanics (QM) data. Leveragingphysically-motivated force field forms and training strategies, ByteFF-Polexhibits exceptional performance in predicting thermodynamic and transportproperties for a wide range of small-molecule liquids and electrolytes,outperforming state-of-the-art (SOTA) classical and machine learning forcefields. The zero-shot prediction capability of ByteFF-Pol bridges the gapbetween microscopic QM calculations and macroscopic liquid properties, enablingthe exploration of previously intractable chemical spaces. This advancementholds transformative potential for applications such as electrolyte design andcustom-tailored solvent, representing a pivotal step toward data-drivenmaterials discovery.</description>
      <author>example@mail.com (Tianze Zheng, Xingyuan Xu, Zhi Wang, Zhenze Yang, Yuanheng Wang, Xu Han, Zhenliang Mu, Ziqing Zhang, Siyuan Liu, Sheng Gong, Kuang Yu, Wen Yan)</author>
      <guid isPermaLink="false">2508.08575v3</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>M3-Net: A Cost-Effective Graph-Free MLP-Based Model for Traffic Prediction</title>
      <link>http://arxiv.org/abs/2508.08543v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M3-Net的经济高效的基于无图多层感知器的交通预测模型，该模型结合了时间序列和时空嵌入以及具有专家混合机制的MLP-Mixer架构，在多个真实数据集上显示出优越的预测性能和轻量级部署能力。&lt;h4&gt;背景&lt;/h4&gt;交通预测是智能交通系统发展中的基本但关键的任务。主流方法主要依赖于时空图神经网络和时空注意力机制等。&lt;h4&gt;目的&lt;/h4&gt;解决现有深度学习方法面临的挑战，即它们要么依赖于完整的交通网络结构，要么需要复杂的模型设计来捕捉复杂的时空依赖关系，从而实现深度学习模型在大规模数据集上的高效部署和操作。&lt;h4&gt;方法&lt;/h4&gt;提出一种名为M3-Net的基于无图多层感知器的模型，该模型使用时间序列和时空嵌入进行高效特征处理，并引入了一种具有专家混合机制的新型MLP-Mixer架构。&lt;h4&gt;主要发现&lt;/h4&gt;在多个真实数据集上进行的大量实验证明了所提出模型在预测性能和轻量级部署方面的优越性。&lt;h4&gt;结论&lt;/h4&gt;M3-Net模型能够有效解决现有深度学习方法在交通预测中面临的挑战，提供了一种更经济高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;实现准确的交通预测是当前智能交通系统发展中的一个基本但关键的任务。在交通预测方面取得突破的大多数主流方法依赖于时空图神经网络、时空注意力机制等。现有深度学习方法的主要挑战是它们要么依赖于完整的交通网络结构，要么需要复杂的模型设计来捕捉复杂的时空依赖关系。这些限制对于在大规模数据集上高效部署和操作深度学习模型构成了重大挑战。为了解决这些挑战，我们提出了一种经济高效的基于无图多层感知器的模型M3-Net用于交通预测。我们提出的模型不仅使用时间序列和时空嵌入进行高效特征处理，而且首次引入了一种具有专家混合机制的新型MLP-Mixer架构。在多个真实数据集上进行的大量实验证明了所提出模型在预测性能和轻量级部署方面的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving accurate traffic prediction is a fundamental but crucial task inthe development of current intelligent transportation systems.Most of themainstream methods that have made breakthroughs in traffic prediction rely onspatio-temporal graph neural networks, spatio-temporal attention mechanisms,etc. The main challenges of the existing deep learning approaches are that theyeither depend on a complete traffic network structure or require intricatemodel designs to capture complex spatio-temporal dependencies. Theselimitations pose significant challenges for the efficient deployment andoperation of deep learning models on large-scale datasets. To address thesechallenges, we propose a cost-effective graph-free Multilayer Perceptron (MLP)based model M3-Net for traffic prediction. Our proposed model not only employstime series and spatio-temporal embeddings for efficient feature processing butalso first introduces a novel MLP-Mixer architecture with a mixture of experts(MoE) mechanism. Extensive experiments conducted on multiple real datasetsdemonstrate the superiority of the proposed model in terms of predictionperformance and lightweight deployment.</description>
      <author>example@mail.com (Guangyin Jin, Sicong Lai, Xiaoshuai Hao, Mingtao Zhang, Jinlei Zhang)</author>
      <guid isPermaLink="false">2508.08543v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>On Understanding of the Dynamics of Model Capacity in Continual Learning</title>
      <link>http://arxiv.org/abs/2508.08052v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了持续学习中的稳定-塑性困境，引入了持续学习的有效模型容量(CLEMC)概念，通过差分方程建模神经网络、任务数据和优化过程的相互作用，证明了有效容量本质上是非平稳的，并发现当新任务分布与先前任务不同时，神经网络表示新任务的能力会下降。&lt;h4&gt;背景&lt;/h4&gt;稳定-塑性困境与神经网络容量(表示任务的能力)密切相关，是持续学习中的一个基本挑战。&lt;h4&gt;目的&lt;/h4&gt;引入持续学习的有效模型容量(CLEMC)概念，用于描述稳定-塑性平衡点的动态行为。&lt;h4&gt;方法&lt;/h4&gt;开发差分方程来建模神经网络、任务数据和优化过程之间相互作用的演化过程，并利用CLEMC进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;无论神经网络架构或优化方法如何，当新任务分布与先前任务不同时，神经网络表示新任务的能力都会下降；有效容量和稳定-塑性平衡点本质上是非平稳的。&lt;h4&gt;结论&lt;/h4&gt;通过大量实验支持了理论发现，实验涵盖了从小型前馈网络和卷积网络到中等规模的图神经网络和基于Transformer的大型语言模型等多种架构。&lt;h4&gt;翻译&lt;/h4&gt;稳定-塑性困境与神经网络(NN)的容量(其表示任务的能力)密切相关，是持续学习(CL)中的一个基本挑战。在此背景下，我们引入了持续学习的有效模型容量(CLEMC)，用于描述稳定-塑性平衡点的动态行为。我们开发了一个差分方程来建模神经网络、任务数据和优化过程之间相互作用的演化过程。然后我们利用CLEMC证明，有效容量以及稳定-塑性平衡点本质上是非平稳的。我们表明，无论神经网络架构或优化方法如何，当传入任务分布与先前任务不同时，神经网络表示新任务的能力都会下降。我们进行了大量实验来支持我们的理论发现，涵盖了从小型前馈网络和卷积网络到中等规模的图神经网络和基于Transformer的具有数百万参数的大型语言模型等多种架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The stability-plasticity dilemma, closely related to a neural network's (NN)capacity-its ability to represent tasks-is a fundamental challenge in continuallearning (CL). Within this context, we introduce CL's effective model capacity(CLEMC) that characterizes the dynamic behavior of the stability-plasticitybalance point. We develop a difference equation to model the evolution of theinterplay between the NN, task data, and optimization procedure. We thenleverage CLEMC to demonstrate that the effective capacity-and, by extension,the stability-plasticity balance point is inherently non-stationary. We showthat regardless of the NN architecture or optimization method, a NN's abilityto represent new tasks diminishes when incoming task distributions differ fromprevious ones. We conduct extensive experiments to support our theoreticalfindings, spanning a range of architectures-from small feedforward network andconvolutional networks to medium-sized graph neural networks andtransformer-based large language models with millions of parameters.</description>
      <author>example@mail.com (Supriyo Chakraborty, Krishnan Raghavan)</author>
      <guid isPermaLink="false">2508.08052v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering</title>
      <link>http://arxiv.org/abs/2508.10729v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了EgoCross基准测试，用于评估多模态大语言模型在第一人称视频问答中的跨领域泛化能力，突破了现有研究主要局限于日常活动的局限。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在第一人称视频问答领域取得了显著进展，但现有基准测试和研究主要局限于日常活动如烹饪和清洁，而真实世界部署会遇到领域偏移问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个全面的基准测试来评估MLLMs在第一人称视频问答中的跨领域泛化能力，填补现有研究在非日常活动领域的空白。&lt;h4&gt;方法&lt;/h4&gt;构建了EgoCross基准测试，涵盖四个多样且具挑战性的领域（手术、工业、极限运动、动物视角），包含约1000个问答对跨越798个视频片段，涵盖预测、识别、定位和计数四个关键问答任务，并提供开放式和封闭式问答两种格式。&lt;h4&gt;主要发现&lt;/h4&gt;大多数现有的MLLMs，无论是通用型还是第一人称专用型，都难以推广到日常生活以外的领域，突显了当前模型的局限性。&lt;h4&gt;结论&lt;/h4&gt;EgoCross基准测试和相关分析为推进领域自适应、鲁棒的第一人称视频理解提供了基础，数据和代码将在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;最近多模态大语言模型的进展显著推动了第一人称视频问答的前沿。然而，现有的基准测试和研究主要局限于烹饪和清洁等日常活动。相比之下，真实世界的部署不可避免地会遇到领域偏移问题，其中目标领域在视觉风格和语义内容上差异显著。为了弥补这一差距，我们引入了EgoCross，这是一个全面的基准测试，旨在评估MLLMs在第一人称视频问答中的跨领域泛化能力。EgoCross涵盖了四个多样且具有挑战性的领域，包括手术、工业、极限运动和动物视角，代表了现实且影响深远的应用场景。它包含跨越798个视频片段的约1000个问答对，涵盖四个关键问答任务：预测、识别、定位和计数。每个问答对提供开放式和封闭式问答两种格式，以支持细粒度评估。大量实验表明，大多数现有的MLLMs，无论是通用型还是第一人称专用型，都难以推广到日常生活以外的领域，突显了当前模型的局限性。此外，我们进行了几项初步研究，例如微调和强化学习，以探索潜在的改进方向。我们希望EgoCross和我们的相关分析能够成为推进领域自适应、鲁棒的第一人称视频理解的基础。数据和代码将在以下地址发布：https://github.com/MyUniverse0726/EgoCross&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Multimodal Large Language Models (MLLMs) havesignificantly pushed the frontier of egocentric video question answering(EgocentricQA). However, existing benchmarks and studies are mainly limited tocommon daily activities such as cooking and cleaning. In contrast, real-worlddeployment inevitably encounters domain shifts, where target domains differsubstantially in both visual style and semantic content. To bridge this gap, weintroduce \textbf{EgoCross}, a comprehensive benchmark designed to evaluate thecross-domain generalization of MLLMs in EgocentricQA. EgoCross covers fourdiverse and challenging domains, including surgery, industry, extreme sports,and animal perspective, representing realistic and high-impact applicationscenarios. It comprises approximately 1,000 QA pairs across 798 video clips,spanning four key QA tasks: prediction, recognition, localization, andcounting. Each QA pair provides both OpenQA and CloseQA formats to supportfine-grained evaluation. Extensive experiments show that most existing MLLMs,whether general-purpose or egocentric-specialized, struggle to generalize todomains beyond daily life, highlighting the limitations of current models.Furthermore, we conduct several pilot studies, \eg, fine-tuning andreinforcement learning, to explore potential improvements. We hope EgoCross andour accompanying analysis will serve as a foundation for advancingdomain-adaptive, robust egocentric video understanding. Data and codes will bereleased at:\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}</description>
      <author>example@mail.com (Yanjun Li, Yuqian Fu, Tianwen Qian, Qi'ao Xu, Silong Dai, Danda Pani Paudel, Luc Van Gool, Xiaoling Wang)</author>
      <guid isPermaLink="false">2508.10729v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation</title>
      <link>http://arxiv.org/abs/2508.10635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ChatENV是一个创新的交互式视觉语言模型，能够同时处理卫星图像对和现实世界传感器数据，用于环境变化理解和分析。&lt;h4&gt;背景&lt;/h4&gt;理解航空影像中的环境变化对气候适应力、城市规划和生态系统监测至关重要，但当前视觉语言模型存在忽略环境传感器因果信号、依赖单一来源标题易受风格偏见影响、缺乏交互式场景推理能力等问题。&lt;h4&gt;目的&lt;/h4&gt;开发ChatENV，一个能够联合推理卫星图像对和现实世界传感器数据的交互式视觉语言模型，以克服现有模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;创建包含177k图像的数据集，形成跨越62个土地利用类别、197个国家的152k个时间对，并具有丰富的传感器元数据；使用GPT-4o和Gemini 2.0实现风格和语义多样性注释；采用低秩自适应(LoRA)适配器对Qwen-2.5-VL进行微调以支持聊天功能。&lt;h4&gt;主要发现&lt;/h4&gt;ChatENV在时间和'what-if'推理方面表现优异(BERT-F1达到0.903)，与最先进的时间模型相比具有竞争力或表现更好，同时支持交互式场景分析。&lt;h4&gt;结论&lt;/h4&gt;ChatENV是一个强大的工具，可用于基于传感器感知的环境监测，为环境变化理解和分析提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;从航空影像理解环境变化对气候适应力、城市规划和生态系统监测至关重要。然而，当前的视觉语言模型忽略了环境传感器的因果信号，依赖单一来源的标题而易受风格偏见影响，且缺乏交互式场景推理能力。我们提出了ChatENV，这是首个能够同时处理卫星图像对和现实世界传感器数据的交互式视觉语言模型。我们的框架：(i)创建了一个包含177k图像的数据集，形成了跨越62个土地利用类别、197个国家的152k个时间对，并具有丰富的传感器元数据(如温度、PM10、CO)；(ii)使用GPT-4o和Gemini 2.0对数据进行注释，以实现风格和语义多样性；(iii)使用高效的低秩自适应(LoRA)适配器对Qwen-2.5-VL进行微调以用于聊天目的。ChatENV在时间和'what-if'推理方面取得了强劲性能(例如，BERT-F1达到0.903)，与最先进的时间模型相比具有竞争力或表现更好，同时支持交互式场景分析。这使ChatENV成为一个强大的工具，用于基于传感器感知的环境监测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding environmental changes from aerial imagery is vital for climateresilience, urban planning, and ecosystem monitoring. Yet, current visionlanguage models (VLMs) overlook causal signals from environmental sensors, relyon single-source captions prone to stylistic bias, and lack interactivescenario-based reasoning. We present ChatENV, the first interactive VLM thatjointly reasons over satellite image pairs and real-world sensor data. Ourframework: (i) creates a 177k-image dataset forming 152k temporal pairs across62 land-use classes in 197 countries with rich sensor metadata (e.g.,temperature, PM10, CO); (ii) annotates data using GPT- 4o and Gemini 2.0 forstylistic and semantic diversity; and (iii) fine-tunes Qwen-2.5-VL usingefficient Low-Rank Adaptation (LoRA) adapters for chat purposes. ChatENVachieves strong performance in temporal and "what-if" reasoning (e.g., BERT-F10.903) and rivals or outperforms state-of-the-art temporal models, whilesupporting interactive scenario-based analysis. This positions ChatENV as apowerful tool for grounded, sensor-aware environmental monitoring.</description>
      <author>example@mail.com (Hosam Elgendy, Ahmed Sharshar, Ahmed Aboeitta, Mohsen Guizani)</author>
      <guid isPermaLink="false">2508.10635v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Trajectory-aware Shifted State Space Models for Online Video Super-Resolution</title>
      <link>http://arxiv.org/abs/2508.10453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TS-Mamba的在线视频超分辨率方法，结合轨迹感知和低复杂度Mamba模型实现高效空间-时间信息聚合，在三个VSR测试数据集上取得最先进性能，同时降低22.7%计算复杂度。&lt;h4&gt;背景&lt;/h4&gt;在线视频超分辨率(VSR)是许多现实世界视频处理应用的重要技术，旨在基于时间上之前的帧恢复当前高分辨率视频帧。现有方法主要只使用一个相邻前一帧实现时间对齐，限制了视频的长程时间建模。状态空间模型(SSMs)因其线性的计算复杂度和全局感受野被提出，显著提高了计算效率和性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型在线VSR方法，结合长程轨迹建模和低复杂度Mamba模型，实现高效的空间-时间信息聚合。&lt;h4&gt;方法&lt;/h4&gt;TS-Mamba首先在视频中构建轨迹，从前面的帧中选择最相似的token；然后采用由提出的移位SSMs块组成的轨迹感知移位Mamba聚合(TSMA)模块聚合选中的token；移位SSMs块基于Hilbert扫描和相应移位操作设计，用于补偿扫描损失并增强Mamba的空间连续性；同时提出轨迹感知损失函数监督轨迹生成，确保训练时token选择的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在三个广泛使用的VSR测试数据集上的大量实验表明，与六个在线VSR基准模型相比，TS-Mamba在大多数情况下实现了最先进的性能，并且计算复杂度降低了22.7%(以MACs计)。&lt;h4&gt;结论&lt;/h4&gt;TS-Mamba方法通过结合轨迹感知和移位SSMs，有效解决了在线VSR中的长程时间建模问题，同时保持较低计算复杂度，为视频超分辨率领域提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在线视频超分辨率(VSR)是许多现实世界视频处理应用中的重要技术，旨在基于时间上之前的帧来恢复当前的高分辨率视频帧。大多数现有的在线VSR方法仅使用一个相邻的前一帧来实现时间对齐，这限制了视频的长程时间建模。最近，状态空间模型(SSMs)被提出，具有线性的计算复杂度和全局感受野，显著提高了计算效率和性能。在此背景下，本文提出了一种基于轨迹感知移位SSMs(TS-Mamba)的新型在线VSR方法，利用长程轨迹建模和低复杂度Mamba来实现高效的空间-时间信息聚合。具体而言，TS-Mamba首先在视频中构建轨迹，从前面的帧中选择最相似的token。然后，采用由提出的移位SSMs块组成的轨迹感知移位Mamba聚合(TSMA)模块来聚合选中的token。这些移位SSMs块基于Hilbert扫描和相应的移位操作设计，用于补偿扫描损失并增强Mamba的空间连续性。此外，我们还提出了一个轨迹感知损失函数来监督轨迹生成，确保在训练模型时token选择的准确性。在三个广泛使用的VSR测试数据集上的大量实验表明，与六个在线VSR基准模型相比，我们的TS-Mamba在大多数情况下实现了最先进的性能，并且计算复杂度降低了22.7%(以MACs计)。TS-Mamba的源代码将在https://github.com上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Online video super-resolution (VSR) is an important technique for manyreal-world video processing applications, which aims to restore the currenthigh-resolution video frame based on temporally previous frames. Most of theexisting online VSR methods solely employ one neighboring previous frame toachieve temporal alignment, which limits long-range temporal modeling ofvideos. Recently, state space models (SSMs) have been proposed with linearcomputational complexity and a global receptive field, which significantlyimprove computational efficiency and performance. In this context, this paperpresents a novel online VSR method based on Trajectory-aware Shifted SSMs(TS-Mamba), leveraging both long-term trajectory modeling and low-complexityMamba to achieve efficient spatio-temporal information aggregation.Specifically, TS-Mamba first constructs the trajectories within a video toselect the most similar tokens from the previous frames. Then, aTrajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposedshifted SSMs blocks is employed to aggregate the selected tokens. The shiftedSSMs blocks are designed based on Hilbert scannings and corresponding shiftoperations to compensate for scanning losses and strengthen the spatialcontinuity of Mamba. Additionally, we propose a trajectory-aware loss functionto supervise the trajectory generation, ensuring the accuracy of tokenselection when training our model. Extensive experiments on three widely usedVSR test datasets demonstrate that compared with six online VSR benchmarkmodels, our TS-Mamba achieves state-of-the-art performance in most cases andover 22.7\% complexity reduction (in MACs). The source code for TS-Mamba willbe available at https://github.com.</description>
      <author>example@mail.com (Qiang Zhu, Xiandong Meng, Yuxian Jiang, Fan Zhang, David Bull, Shuyuan Zhu, Bing Zeng)</author>
      <guid isPermaLink="false">2508.10453v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Data-Efficient Learning for Generalizable Surgical Video Understanding</title>
      <link>http://arxiv.org/abs/2508.10215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究解决了手术视频分析中的关键挑战，通过开发创新的半监督方法和基准测试模型，减少了对标记数据的依赖，并创建了大型公共数据集，促进了该领域的发展和临床应用。&lt;h4&gt;背景&lt;/h4&gt;外科视频分析技术的进步正在将手术室转变为智能、数据驱动的环境。计算机辅助系统支持完整的手术工作流程，从术前规划到术中指导和术后评估。然而，开发稳健且可推广的手术视频理解模型面临三大挑战：标注稀缺、时空复杂性以及跨手术和机构的领域差异。&lt;h4&gt;目的&lt;/h4&gt;弥合基于深度学习的手术视频分析研究与实际临床部署之间的差距，开发稳健、数据高效且临床可扩展的解决方案。&lt;h4&gt;方法&lt;/h4&gt;通过对先进神经网络架构进行基准测试确定最佳设计；提出新架构和集成高级模块提高性能；开发半监督框架减少对标记数据的依赖；引入DIST、SemiVT-Surge和ENCORE等半监督框架；发布GynSurg和Cataract-1K两个多任务数据集。&lt;h4&gt;主要发现&lt;/h4&gt;新的半监督框架通过利用少量标记数据和动态伪标记增强模型训练，在具有挑战性的手术数据集上取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;这项工作为手术视频分析提供了稳健、数据高效且临床可扩展的解决方案，为可推广的AI系统奠定了基础，这些系统可以显著影响手术护理和培训。&lt;h4&gt;翻译&lt;/h4&gt;外科视频分析技术的进步正在将手术室转变为智能、数据驱动的环境。计算机辅助系统支持完整的手术工作流程，从术前规划到术中指导和术后评估。然而，由于标注稀缺、时空复杂性以及跨手术和机构的领域差异，开发稳健且可推广的手术视频理解模型仍然具有挑战性。这项博士研究旨在弥合基于深度学习的手术视频分析研究与实际临床部署之间的差距。为解决识别手术阶段、动作和事件这一核心挑战，我对最先进的神经网络架构进行了基准测试，以确定每个任务的最有效设计。我通过提出新的架构和集成高级模块进一步提高了性能。鉴于专家标注的高成本和跨手术视频源的领域差异，我专注于减少对标记数据的依赖。我们开发了半监督框架，通过利用大量未标记的手术视频来提高模型在各项任务中的性能。我们引入了新的半监督框架，包括DIST、SemiVT-Surge和ENCORE，这些框架通过利用少量标记数据和通过动态伪标记增强模型训练，在具有挑战性的手术数据集上取得了最先进的结果。为了支持可复现性和推进该领域，我们发布了两个多任务数据集：GynSurg，最大的妇科腹腔镜数据集，以及Cataract-1K，最大的白内障手术视频数据集。这项工作共同为手术视频分析提供了稳健、数据高效且临床可扩展的解决方案，为可推广的AI系统奠定了基础，这些系统可以显著影响手术护理和培训。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advances in surgical video analysis are transforming operating rooms intointelligent, data-driven environments. Computer-assisted systems support fullsurgical workflow, from preoperative planning to intraoperative guidance andpostoperative assessment. However, developing robust and generalizable modelsfor surgical video understanding remains challenging due to (I) annotationscarcity, (II) spatiotemporal complexity, and (III) domain gap acrossprocedures and institutions. This doctoral research aims to bridge the gapbetween deep learning-based surgical video analysis in research and itsreal-world clinical deployment. To address the core challenge of recognizingsurgical phases, actions, and events, critical for analysis, I benchmarkedstate-of-the-art neural network architectures to identify the most effectivedesigns for each task. I further improved performance by proposing novelarchitectures and integrating advanced modules. Given the high cost of expertannotations and the domain gap across surgical video sources, I focused onreducing reliance on labeled data. We developed semi-supervised frameworks thatimprove model performance across tasks by leveraging large amounts of unlabeledsurgical video. We introduced novel semi-supervised frameworks, including DIST,SemiVT-Surge, and ENCORE, that achieved state-of-the-art results on challengingsurgical datasets by leveraging minimal labeled data and enhancing modeltraining through dynamic pseudo-labeling. To support reproducibility andadvance the field, we released two multi-task datasets: GynSurg, the largestgynecologic laparoscopy dataset, and Cataract-1K, the largest cataract surgeryvideo dataset. Together, this work contributes to robust, data-efficient, andclinically scalable solutions for surgical video analysis, laying thefoundation for generalizable AI systems that can meaningfully impact surgicalcare and training.</description>
      <author>example@mail.com (Sahar Nasirihaghighi)</author>
      <guid isPermaLink="false">2508.10215v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>SurgPub-Video: A Comprehensive Surgical Video Dataset for Enhanced Surgical Intelligence in Vision-Language Model</title>
      <link>http://arxiv.org/abs/2508.10054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了SurgPub-Video数据集、SurgLLaVA-Video模型和外科视频级VQA基准三项贡献，解决了现有VLMs在外科场景分析中受限于帧级数据集和缺乏高质量外科视频数据的问题。实验证明SurgLLaVA-Video性能显著优于其他模型。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在外科场景分析中显示出巨大潜力，但现有模型受限于帧级数据集，缺乏带有外科程序知识的高质量视频数据。&lt;h4&gt;目的&lt;/h4&gt;解决现有VLMs在外科场景分析中的局限性，提出三项贡献以促进外科视频理解的发展。&lt;h4&gt;方法&lt;/h4&gt;创建了包含11个专业、超过3000个外科视频和2500万标注帧的SurgPub-Video数据集；开发了基于TinyLLaVA-Video架构构建的SurgLLaVA-Video模型，支持视频级和帧级输入；建立了涵盖11个外科专业的视频级VQA基准。&lt;h4&gt;主要发现&lt;/h4&gt;在提出的基准和三个额外的外科下游任务（动作识别、技能评估和三元组识别）上进行的大量实验表明，仅有三十亿参数的SurgLLaVA-Video显著优于通用外科专用VLM。&lt;h4&gt;结论&lt;/h4&gt;数据集、模型和基准将被发布，以促进外科视频理解的进一步发展。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在外科场景分析中显示出巨大潜力，但现有模型受限于帧级数据集，缺乏带有外科程序知识的高质量视频数据。为解决这些挑战，我们做出了以下贡献：(i) SurgPub-Video，一个包含11个专业、超过3000个外科视频和2500万标注帧的综合数据集，数据来源于同行评审的临床期刊；(ii) SurgLLaVA-Video，一个专门用于外科视频理解的VLM，基于TinyLLaVA-Video架构构建，支持视频级和帧级输入；(iii) 一个外科视频级视觉问答基准，涵盖11个不同的外科专业，如血管科、心脏病学和胸外科。在提出的基准和三个额外的外科下游任务（动作识别、技能评估和三元组识别）上进行的大量实验表明，仅有三十亿参数的SurgLLaVA-Video显著优于通用外科专用VLM。数据集、模型和基准将被发布，以促进外科视频理解的进一步发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have shown significant potential in surgicalscene analysis, yet existing models are limited by frame-level datasets andlack high-quality video data with procedural surgical knowledge. To addressthese challenges, we make the following contributions: (i) SurgPub-Video, acomprehensive dataset of over 3,000 surgical videos and 25 million annotatedframes across 11 specialties, sourced from peer-reviewed clinical journals,(ii) SurgLLaVA-Video, a specialized VLM for surgical video understanding, builtupon the TinyLLaVA-Video architecture that supports both video-level andframe-level inputs, and (iii) a video-level surgical Visual Question Answering(VQA) benchmark, covering diverse 11 surgical specialities, such as vascular,cardiology, and thoracic. Extensive experiments, conducted on the proposedbenchmark and three additional surgical downstream tasks (action recognition,skill assessment, and triplet recognition), show that SurgLLaVA-Videosignificantly outperforms both general-purpose and surgical-specific VLMs withonly three billion parameters. The dataset, model, and benchmark will bereleased to enable further advancements in surgical video understanding.</description>
      <author>example@mail.com (Yaoqian Li, Xikai Yang, Dunyuan Xu, Yang Yu, Litao Zhao, Xiaowei Hu, Jinpeng Li, Pheng-Ann Heng)</author>
      <guid isPermaLink="false">2508.10054v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Re:Verse -- Can Your VLM Read a Manga?</title>
      <link>http://arxiv.org/abs/2508.08508v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV (AISTORY Workshop) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究揭示了当前视觉语言模型在处理连续视觉叙事时，在表面识别与深层叙事推理之间存在关键差距。研究提出了一种新的评估框架，并应用于Re:Zero漫画，发现当前模型缺乏真正的故事级智能，特别是在处理非线性叙事、角色一致性和跨序列因果推理方面存在困难。&lt;h4&gt;背景&lt;/h4&gt;当前视觉语言模型在处理连续视觉叙事时表现出局限性，特别是在时间因果性和跨面板连贯性方面，这些是连贯故事理解的核心要求。现有模型虽然在单幅图像解释方面表现出色，但在叙事理解方面存在系统性失败。&lt;h4&gt;目的&lt;/h4&gt;研究旨在系统地评估视觉语言模型在长篇叙事理解方面的能力，揭示其在时间因果性和跨面板连贯性方面的局限性，并提出一个新的评估框架来系统地描述这些局限性。&lt;h4&gt;方法&lt;/h4&gt;研究引入了一个新的评估框架，结合了细粒度多模态注释、跨模态嵌入分析和检索增强评估。具体方法包括：(i)严格的注释协议，通过对齐的轻小说文本将视觉元素与叙事结构联系起来；(ii)多种推理范式的综合评估，包括直接推理和检索增强生成；(iii)跨模态相似性分析。研究应用了Re:Zero漫画的11个章节，共308个注释面板，通过三个核心评估轴：生成式叙事、上下文化对话基础和时间推理。&lt;h4&gt;主要发现&lt;/h4&gt;当前视觉语言模型缺乏真正的故事级智能，特别是在处理非线性叙事、角色一致性和跨序列因果推理方面存在困难。研究还揭示了当前VLMs联合表示中的基本错位。&lt;h4&gt;结论&lt;/h4&gt;这项工作为评估叙事智能建立了基础和实践方法，同时提供了关于多模态模型在离散视觉叙事中超越基本识别的深层序列理解能力的实用见解。&lt;h4&gt;翻译&lt;/h4&gt;当前视觉语言模型在处理连续视觉叙事时，在表面识别与深层叙事推理之间存在关键差距。通过对漫画叙事理解的全面调查，我们发现，虽然最近的大型多模态模型在单幅图像解释方面表现出色，但它们在时间因果性和跨面板连贯性方面存在系统性失败，而这些是连贯故事理解的核心要求。我们引入了一种新的评估框架，结合了细粒度多模态注释、跨模态嵌入分析和检索增强评估，以系统地描述这些局限性。我们的方法包括(i)通过通过对齐的轻小说文本将视觉元素与叙事结构联系起来的严格注释协议，(ii)多种推理范式的综合评估，包括直接推理和检索增强生成，以及(iii)跨模态相似性分析，揭示了当前VLMs联合表示中的基本错位。将此框架应用于Re:Zero漫画的11个章节，共308个注释面板，我们进行了第一个关于VLMs长篇叙事理解的系统性研究，通过三个核心评估轴：生成式叙事、上下文化对话基础和时间推理。我们的研究结果表明，当前模型缺乏真正的故事级智能，特别是在处理非线性叙事、角色一致性和跨序列因果推理方面存在困难。这项工作为评估叙事智能建立了基础和实践方法，同时提供了关于多模态模型在离散视觉叙事中超越基本识别的深层序列理解能力的实用见解。项目页面：https://re-verse.vercel.app&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current Vision Language Models (VLMs) demonstrate a critical gap betweensurface-level recognition and deep narrative reasoning when processingsequential visual storytelling. Through a comprehensive investigation of manganarrative understanding, we reveal that while recent large multimodal modelsexcel at individual panel interpretation, they systematically fail at temporalcausality and cross-panel cohesion, core requirements for coherent storycomprehension. We introduce a novel evaluation framework that combinesfine-grained multimodal annotation, cross-modal embedding analysis, andretrieval-augmented assessment to systematically characterize theselimitations.  Our methodology includes (i) a rigorous annotation protocol linking visualelements to narrative structure through aligned light novel text, (ii)comprehensive evaluation across multiple reasoning paradigms, including directinference and retrieval-augmented generation, and (iii) cross-modal similarityanalysis revealing fundamental misalignments in current VLMs' jointrepresentations. Applying this framework to Re:Zero manga across 11 chapterswith 308 annotated panels, we conduct the first systematic study of long-formnarrative understanding in VLMs through three core evaluation axes: generativestorytelling, contextual dialogue grounding, and temporal reasoning. Ourfindings demonstrate that current models lack genuine story-level intelligence,struggling particularly with non-linear narratives, character consistency, andcausal inference across extended sequences. This work establishes both thefoundation and practical methodology for evaluating narrative intelligence,while providing actionable insights into the capability of deep sequentialunderstanding of Discrete Visual Narratives beyond basic recognition inMultimodal Models.  Project Page: https://re-verse.vercel.app</description>
      <author>example@mail.com (Aaditya Baranwal, Madhav Kataria, Naitik Agrawal, Yogesh S Rawat, Shruti Vyas)</author>
      <guid isPermaLink="false">2508.08508v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning</title>
      <link>http://arxiv.org/abs/2508.04549v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ACMMM2025 (Dataset track)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对海洋视频理解面临的挑战，提出了一种两阶段的海洋物体导向的视频字幕处理流程，并引入了一个全面的视频理解基准。&lt;h4&gt;背景&lt;/h4&gt;海洋视频理解面临多重挑战，包括海洋物体和环境的动态变化、相机运动以及水下场景的复杂性。现有视频字幕数据集通常专注于通用或以人为中心的领域，难以推广到海洋环境的复杂性，也无法深入了解海洋生物。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频字幕数据集在海洋环境中的局限性，提高海洋视频的理解和分析能力，以及海洋视频生成能力。&lt;h4&gt;方法&lt;/h4&gt;提出一个两阶段的海洋物体导向的视频字幕处理流程，引入一个利用视频、文本和分割掩码三元组的视频理解基准，并采用视频分割技术检测场景变化中的显著物体转换。&lt;h4&gt;主要发现&lt;/h4&gt;视频分割技术对于检测场景变化中的显著物体转换非常有效，能够显著丰富字幕内容的语义。所提出的方法和基准能够改善海洋视频的理解、分析和生成。&lt;h4&gt;结论&lt;/h4&gt;通过提出的两阶段流程和视频理解基准，有效解决了海洋视频理解的挑战，提高了字幕质量和语义丰富度。相关数据集和代码已公开发布。&lt;h4&gt;翻译&lt;/h4&gt;海洋视频由于其物体和环境的动态性、相机运动以及水下场景的复杂性，给视频理解带来了重大挑战。现有的视频字幕数据集通常专注于通用或以人为中心的领域，往往难以推广到海洋环境的复杂性，也无法深入了解海洋生物。为解决这些局限性，我们提出了一个两阶段的海洋物体导向的视频字幕处理流程。我们引入了一个全面的视频理解基准，利用视频、文本和分割掩码的三元组来促进视觉定位和字幕生成，从而改善海洋视频的理解和分析，以及海洋视频生成。此外，我们强调了视频分割在检测场景变化中显著物体转换的有效性，这大大丰富了字幕内容的语义。我们的数据集和代码已在https://msc.hkustvgd.com发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758198&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Marine videos present significant challenges for video understanding due tothe dynamics of marine objects and the surrounding environment, camera motion,and the complexity of underwater scenes. Existing video captioning datasets,typically focused on generic or human-centric domains, often fail to generalizeto the complexities of the marine environment and gain insights about marinelife. To address these limitations, we propose a two-stage marineobject-oriented video captioning pipeline. We introduce a comprehensive videounderstanding benchmark that leverages the triplets of video, text, andsegmentation masks to facilitate visual grounding and captioning, leading toimproved marine video understanding and analysis, and marine video generation.Additionally, we highlight the effectiveness of video splitting in order todetect salient object transitions in scene changes, which significantly enrichthe semantics of captioning content. Our dataset and code have been released athttps://msc.hkustvgd.com.</description>
      <author>example@mail.com (Quang-Trung Truong, Yuk-Kwan Wong, Vo Hoang Kim Tuyen Dang, Rinaldi Gotama, Duc Thanh Nguyen, Sai-Kit Yeung)</author>
      <guid isPermaLink="false">2508.04549v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Stereo Matching with Multi-Baseline Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.10838v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出BaCon-Stereo，一个简单有效的对比学习框架，用于解决自监督立体匹配中遮挡区域的问题。通过多基线输入的教师-学生范式和遮挡感知注意力图，该方法显著提升了遮挡和非遮挡区域的预测性能，并在多个基准测试上超越现有方法。&lt;h4&gt;背景&lt;/h4&gt;当前的自监督立体匹配依赖于光度一致性假设，但在遮挡区域由于对应关系不明确而失效，导致预测不准确。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在遮挡和非遮挡区域都有效工作的自监督立体网络训练框架，提高立体匹配的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;采用多基线输入的教师-学生范式，教师和学生的立体对共享相同参考视图但目标视图不同。利用几何关系，让学生从教师的预测中学习，并通过重新缩放匹配基线进行监督。引入遮挡感知注意力图指导遮挡完成学习，并合成多基线数据集BaCon-20k支持训练。&lt;h4&gt;主要发现&lt;/h4&gt;BaCon-Stereo有效改善了遮挡和非遮挡区域的预测性能，实现了强大的泛化能力和鲁棒性，在多个基准测试上表现优异。&lt;h4&gt;结论&lt;/h4&gt;BaCon-Stereo在KITTI 2015和2012基准测试上超越了最先进的自监督方法，证明了其在解决立体匹配遮挡问题上的有效性。&lt;h4&gt;翻译&lt;/h4&gt;当前的自监督立体匹配依赖于光度一致性假设，由于对应关系不明确，在遮挡区域会失效。为解决这个问题，我们提出了BaCon-Stereo，一个简单有效的对比学习框架，用于在遮挡和非遮挡区域进行自监督立体网络训练。我们采用多基线输入的教师-学生范式，其中输入教师和学生的立体对共享相同的参考视图，但在目标视图上不同。从几何上看，学生在目标视图中被遮挡的区域在教师视图中通常是可见的，使教师更容易在这些区域进行预测。教师预测被重新缩放以匹配学生的基线，然后用于监督学生。我们还引入了一个遮挡感知注意力图，以更好地指导学生学习遮挡完成。为了支持训练，我们合成了一个多基线数据集BaCon-20k。大量实验表明，BaCon-Stereo改善了遮挡和非遮挡区域的预测，实现了强大的泛化能力和鲁棒性，并且在KITTI 2015和2012基准测试上都优于最先进的自监督方法。我们的代码和数据集将在论文接受后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自监督立体匹配中遮挡区域的预测不准确问题。这个问题很重要，因为立体匹配在自动驾驶、增强现实和机器人等领域有广泛应用，而遮挡区域在这些场景中普遍存在。当前自监督方法依赖光度一致性假设，这在遮挡区域会失效，导致网络错误地复制相邻非遮挡区域的视差值，影响整体系统的可靠性和精度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有自监督方法在遮挡区域表现不佳的原因，认为需要额外的目标线索来获得遮挡区域的可靠伪真值。他们借鉴了对比学习中的教师-学生范式(特别是MoCo和BYOL中的动量教师思想)，并基于现有的自监督立体匹配方法(如SsSnet、OASM)进行改进。通过使用多基线配置，教师网络可以在学生被遮挡的区域提供可靠信息，因为那些区域在教师网络中可能是可见的。此外，作者还使用了CARLA模拟器来合成多基线数据，这是计算机视觉中常用的数据合成方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用多基线对比学习框架，通过教师-学生范式提供遮挡区域的可靠监督。具体实现流程：1) 使用CARLA模拟器合成多基线立体图像数据集BaCon-20k；2) 设计教师和学生网络，共享参考图像但使用不同目标图像；3) 训练时，将教师的输出视差重新缩放以匹配学生的基线，作为伪真值监督学生；4) 设计遮挡感知注意力图，为不同区域分配不同监督权重；5) 计算对比损失、光度损失和平滑损失的加权和；6) 学生网络通过反向传播更新，教师网络通过指数移动平均更新参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 多基线对比学习框架，使用教师-学生范式解决遮挡区域问题；2) 遮挡感知注意力图，强调对教师友好但对学生具有挑战性的区域；3) 合成多基线数据集BaCon-20k，支持多基线对比学习；4) 动量教师机制，提供更稳定的监督信号。相比之前工作，不同之处在于：现有方法如SsSnet、OASM主要依赖光度一致性假设，在遮挡区域会失效；而BaCon-Stereo通过多基线配置提供遮挡区域的可靠监督，而非简单排除；与DualNet相比，使用不同目标视图而非相同视图，提供更有效遮挡监督；与NerfStereo相比，不依赖额外第三视图，设计更简洁高效。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，我会怎么说？:&lt;/strong&gt; BaCon-Stereo通过多基线对比学习和遮挡感知监督机制，有效解决了自监督立体匹配中遮挡区域的预测问题，显著提高了在遮挡和非遮挡区域的预测精度，并在多种真实场景和天气条件下表现出强大的泛化能力和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current self-supervised stereo matching relies on the photometric consistencyassumption, which breaks down in occluded regions due to ill-posedcorrespondences. To address this issue, we propose BaCon-Stereo, a simple yeteffective contrastive learning framework for self-supervised stereo networktraining in both non-occluded and occluded regions. We adopt a teacher-studentparadigm with multi-baseline inputs, in which the stereo pairs fed into theteacher and student share the same reference view but differ in target views.Geometrically, regions occluded in the student's target view are often visiblein the teacher's, making it easier for the teacher to predict in these regions.The teacher's prediction is rescaled to match the student's baseline and thenused to supervise the student. We also introduce an occlusion-aware attentionmap to better guide the student in learning occlusion completion. To supporttraining, we synthesize a multi-baseline dataset BaCon-20k. Extensiveexperiments demonstrate that BaCon-Stereo improves prediction in both occludedand non-occluded regions, achieves strong generalization and robustness, andoutperforms state-of-the-art self-supervised methods on both KITTI 2015 and2012 benchmarks. Our code and dataset will be released upon paper acceptance.</description>
      <author>example@mail.com (Peng Xu, Zhiyu Xiang, Jingyun Fu, Tianyu Pu, Kai Wang, Chaojie Ji, Tingming Bai, Eryun Liu)</author>
      <guid isPermaLink="false">2508.10838v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive ECOC: Learning Output Codes for Adversarial Defense</title>
      <link>http://arxiv.org/abs/2508.10491v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于对比学习的自动化码本学习方法，用于改进错误纠正输出码技术，提高多类分类的性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;虽然one-hot编码常用于多类分类，但它并不总是最有效的编码机制。传统错误纠正输出码方法依赖于手动设计或随机生成的码本，这些方法劳动密集且可能产生次优的、数据集无关的结果。&lt;h4&gt;目的&lt;/h4&gt;引入三种基于对比学习的自动化码本学习模型，使码本能够直接且自适应地从数据中学习，提高多类分类的性能。&lt;h4&gt;方法&lt;/h4&gt;提出三种基于对比学习的自动化码本学习模型，这些模型允许码本直接且自适应地从数据中学习，无需人工干预。&lt;h4&gt;主要发现&lt;/h4&gt;在四个数据集上，所提出的模型比两种基线方法显示出对对抗攻击更强的鲁棒性，表明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;自动化码本学习方法比传统方法更有效，能够更好地适应特定数据集，提高多类分类的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;虽然one-hot编码常用于多类分类，但它并不总是最有效的编码机制。错误纠正输出码通过将每个类映射到用作标签的唯一码字来解决多类分类问题。传统ECOC方法依赖于手动设计或随机生成的码本，这些方法劳动密集且可能产生次优的、数据集无关的结果。本文介绍了三种基于对比学习的自动化码本学习模型，使码本能够直接且自适应地从数据中学习。在四个数据集上，我们提出的模型与两种基线方法相比，表现出对对抗攻击更强的鲁棒性。源代码可在https://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although one-hot encoding is commonly used for multiclass classification, itis not always the most effective encoding mechanism. Error Correcting OutputCodes (ECOC) address multiclass classification by mapping each class to aunique codeword used as a label. Traditional ECOC methods rely on manuallydesigned or randomly generated codebooks, which are labor-intensive and mayyield suboptimal, dataset-agnostic results. This paper introduces three modelsfor automated codebook learning based on contrastive learning, allowingcodebooks to be learned directly and adaptively from data. Across fourdatasets, our proposed models demonstrate superior robustness to adversarialattacks compared to two baselines. The source is available athttps://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique.</description>
      <author>example@mail.com (Che-Yu Chou, Hung-Hsuan Chen)</author>
      <guid isPermaLink="false">2508.10491v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation</title>
      <link>http://arxiv.org/abs/2508.10432v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CRISP（对比残差注入和语义提示）方法，用于解决持续视频实例分割中的实例级、类别级和任务级混淆问题，同时保持模型的可塑性和稳定性。&lt;h4&gt;背景&lt;/h4&gt;持续视频实例分割需要模型同时具备吸收新类别的可塑性和保留已学习类别的稳定性，同时保持帧间的时间一致性，这导致了实例级、类别级和任务级的混淆问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来解决持续视频实例分割中的混淆问题，避免灾难性遗忘，并提高分割和分类性能。&lt;h4&gt;方法&lt;/h4&gt;CRISP方法包含三个主要部分：1) 实例级学习：建模实例跟踪并构建实例相关损失；2) 类别级学习：构建自适应残差语义提示学习框架，使用可学习的语义残差提示池和调整性查询-提示匹配机制；3) 任务级学习：引入增量提示的初始化策略，确保任务间在查询空间内的相关性。&lt;h4&gt;主要发现&lt;/h4&gt;在YouTube-VIS-2019和YouTube-VIS-2021数据集上的实验表明，CRISP显著优于现有的持续分割方法，有效避免了灾难性遗忘，并提高了分割和分类性能。&lt;h4&gt;结论&lt;/h4&gt;CRISP方法通过针对实例级、类别级和任务级问题的专门设计，成功解决了持续视频实例分割中的关键挑战，为长期视频实例分割任务提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;持续视频实例分割需要同时具备吸收新类别的可塑性和保留已学习类别的稳定性，同时保持帧间的时间一致性。在这项工作中，我们引入了对比残差注入和语义提示(CRISP)，这是一种专门用于解决持续视频实例分割中实例级、类别级和任务级混淆的早期尝试。对于实例级学习，我们建模实例跟踪并构建实例相关损失，强调与先验查询空间的相关性，同时加强当前任务查询的特异性。对于类别级学习，我们构建了一个自适应残差语义提示(ARSP)学习框架，该框架由类别文本生成可学习的语义残差提示池，并使用调整性查询-提示匹配机制建立当前任务查询与语义残差提示之间的映射关系。同时，引入基于对比学习的语义一致性损失，以在增量训练期间保持对象查询和残差提示之间的语义一致性。对于任务级学习，为确保查询空间内任务间的相关性，我们引入了一种简洁而强大的增量提示初始化策略。在YouTube-VIS-2019和YouTube-VIS-2021数据集上的大量实验表明，CRISP在长期持续视频实例分割任务中显著优于现有的持续分割方法，避免了灾难性遗忘，并有效提高了分割和分类性能。代码可在https://github.com/01upup10/CRISP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual video instance segmentation demands both the plasticity to absorbnew object categories and the stability to retain previously learned ones, allwhile preserving temporal consistency across frames. In this work, we introduceContrastive Residual Injection and Semantic Prompting (CRISP), an earlierattempt tailored to address the instance-wise, category-wise, and task-wiseconfusion in continual video instance segmentation. For instance-wise learning,we model instance tracking and construct instance correlation loss, whichemphasizes the correlation with the prior query space while strengthening thespecificity of the current task query. For category-wise learning, we build anadaptive residual semantic prompt (ARSP) learning framework, which constructs alearnable semantic residual prompt pool generated by category text and uses anadjustive query-prompt matching mechanism to build a mapping relationshipbetween the query of the current task and the semantic residual prompt.Meanwhile, a semantic consistency loss based on the contrastive learning isintroduced to maintain semantic coherence between object queries and residualprompts during incremental training. For task-wise learning, to ensure thecorrelation at the inter-task level within the query space, we introduce aconcise yet powerful initialization strategy for incremental prompts. Extensiveexperiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate thatCRISP significantly outperforms existing continual segmentation methods in thelong-term continual video instance segmentation task, avoiding catastrophicforgetting and effectively improving segmentation and classificationperformance. The code is available at https://github.com/01upup10/CRISP.</description>
      <author>example@mail.com (Baichen Liu, Qi Lyu, Xudong Wang, Jiahua Dong, Lianqing Liu, Zhi Han)</author>
      <guid isPermaLink="false">2508.10432v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance</title>
      <link>http://arxiv.org/abs/2508.10280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高保真图像生成方法，通过整合文本-图像对比约束和结构引导机制，解决了现有文本驱动图像生成方法在语义对齐准确性和结构一致性方面的性能瓶颈。&lt;h4&gt;背景&lt;/h4&gt;现有文本驱动图像生成方法存在性能瓶颈，特别是在语义对齐准确性和结构一致性方面，文本描述与生成图像之间的语义匹配不够精确，且生成图像的结构完整性不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种高保真图像生成方法，提高文本驱动图像生成的语义对齐准确性和结构一致性，同时不增加计算复杂度，生成语义清晰且结构完整的图像。&lt;h4&gt;方法&lt;/h4&gt;1. 引入对比学习模块建立强跨模态对齐约束；2. 使用语义布局图或边缘草图等结构先验指导生成器进行空间级别的结构建模；3. 联合优化对比损失、结构一致性损失和语义保持损失；4. 采用多目标监督机制提高生成内容的语义一致性和可控性。&lt;h4&gt;主要发现&lt;/h4&gt;在COCO-2014数据集上的系统实验表明，所提出方法在CLIP Score、FID和SSIM等指标上表现优越；敏感性分析显示该方法对嵌入维度、文本长度和结构引导强度具有良好的适应性；该方法有效弥合了语义对齐和结构保真度之间的差距而不增加计算复杂度。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了生成语义清晰且结构完整图像的强大能力，为联合文本-图像建模和图像生成提供了可行的技术路径。&lt;h4&gt;翻译&lt;/h4&gt;本文针对现有文本驱动图像生成方法在语义对齐准确性和结构一致性方面的性能瓶颈，提出了一种高保真图像生成方法，通过整合文本-图像对比约束与结构引导机制。该方法引入了对比学习模块，建立了强跨模态对齐约束，以提高文本和图像之间的语义匹配。同时，使用语义布局图或边缘草图等结构先验来指导生成器进行空间级别的结构建模，从而增强生成图像的布局完整性和细节保真度。在整体框架中，模型联合优化对比损失、结构一致性损失和语义保持损失，采用多目标监督机制提高生成内容的语义一致性和可控性。在COCO-2014数据集上进行了系统实验，对嵌入维度、文本长度和结构引导强度进行了敏感性分析。定量指标证实了所提出方法在CLIP Score、FID和SSIM方面的优越性能。结果表明，该方法有效弥合了语义对齐和结构保真度之间的差距，同时不会增加计算复杂度。它展示了生成语义清晰且结构完整图像的强大能力，为联合文本-图像建模和图像生成提供了可行的技术路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the performance bottlenecks of existing text-drivenimage generation methods in terms of semantic alignment accuracy and structuralconsistency. A high-fidelity image generation method is proposed by integratingtext-image contrastive constraints with structural guidance mechanisms. Theapproach introduces a contrastive learning module that builds strongcross-modal alignment constraints to improve semantic matching between text andimage. At the same time, structural priors such as semantic layout maps or edgesketches are used to guide the generator in spatial-level structural modeling.This enhances the layout completeness and detail fidelity of the generatedimages. Within the overall framework, the model jointly optimizes contrastiveloss, structural consistency loss, and semantic preservation loss. Amulti-objective supervision mechanism is adopted to improve the semanticconsistency and controllability of the generated content. Systematicexperiments are conducted on the COCO-2014 dataset. Sensitivity analyses areperformed on embedding dimensions, text length, and structural guidancestrength. Quantitative metrics confirm the superior performance of the proposedmethod in terms of CLIP Score, FID, and SSIM. The results show that the methodeffectively bridges the gap between semantic alignment and structural fidelitywithout increasing computational complexity. It demonstrates a strong abilityto generate semantically clear and structurally complete images, offering aviable technical path for joint text-image modeling and image generation.</description>
      <author>example@mail.com (Danyi Gao)</author>
      <guid isPermaLink="false">2508.10280v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.10567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpaRC-AD是一种创新的端到端摄像头-雷达融合框架，通过稀疏3D特征对齐和多普勒速度估计，解决了纯视觉方法在恶劣天气、遮挡和速度估计方面的局限性。在多个自动驾驶任务上实现了显著的性能提升，包括检测、跟踪、地图构建、运动预测和轨迹规划。&lt;h4&gt;背景&lt;/h4&gt;端到端自动驾驶系统通过统一优化感知、运动预测和规划来提供更强的性能。然而，基于视觉的方法在恶劣天气条件、部分遮挡和精确速度估计方面存在基本限制，这些是在安全敏感场景中的关键挑战，准确的运动理解和长时域轨迹预测对避免碰撞至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决基于视觉方法的局限性，提出一种基于查询的端到端摄像头-雷达融合框架，用于规划导向的自动驾驶。&lt;h4&gt;方法&lt;/h4&gt;通过稀疏3D特征对齐和多普勒速度估计，实现强大的3D场景表示，用于优化智能体锚点、地图多边形和运动建模。&lt;h4&gt;主要发现&lt;/h4&gt;在多个自动驾驶任务上与最先进的仅视觉基线相比实现了显著改进：3D检测(+4.8% mAP)、多目标跟踪(+8.3% AMOTA)、在线地图(+1.8% mAP)、运动预测(-4.0% mADE)和轨迹规划(-0.1m L2和-9% TPC)。在多个具有挑战性的基准测试上实现了空间相干性和时间一致性，证明了基于雷达的融合在安全关键场景中的有效性。&lt;h4&gt;结论&lt;/h4&gt;雷达融合方法可以有效解决视觉方法在恶劣条件下的局限性，在多个自动驾驶任务上实现了性能提升，实现了空间相干性和时间一致性，强调了雷达融合在需要准确运动理解和长时域轨迹预测的安全关键场景中的价值。&lt;h4&gt;翻译&lt;/h4&gt;端到端自动驾驶系统通过统一优化感知、运动预测和规划承诺提供更强的性能。然而，基于视觉的方法在恶劣天气条件、部分遮挡和精确速度估计方面面临基本限制——这些是在安全敏感场景中的关键挑战，准确的运动理解和长时域轨迹预测对避免碰撞至关重要。为解决这些局限性，我们提出了SpaRC-AD，一种基于查询的端到端摄像头-雷达融合框架，用于规划导向的自动驾驶。通过稀疏3D特征对齐和多普勒速度估计，我们实现了强大的3D场景表示，用于优化智能体锚点、地图多边形和运动建模。我们的方法在多个自动驾驶任务上实现了对最先进的仅视觉基线的显著改进，包括3D检测(+4.8% mAP)、多目标跟踪(+8.3% AMOTA)、在线地图(+1.8% mAP)、运动预测(-4.0% mADE)和轨迹规划(-0.1m L2和-9% TPC)。我们在多个具有挑战性的基准测试上实现了空间相干性和时间一致性，包括真实世界的开环nuScenes、长时域T-nuScenes和闭环模拟器Bench2Drive。我们展示了基于雷达的融合在安全关键场景中的有效性，在这些场景中，准确的运动理解和长时域轨迹预测对避免碰撞至关重要。所有实验的源代码可在https://phi-wol.github.io/sparcad/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决纯视觉端到端自动驾驶系统在恶劣天气条件、部分遮挡和精确速度估计方面的局限性。这个问题在现实中非常重要，因为这些条件是自动驾驶面临的常见挑战，准确的运动理解和长时程轨迹预测对于避免碰撞至关重要，直接关系到自动驾驶系统的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了纯视觉方法的局限性，认识到雷达传感器在长距离检测、直接速度测量和天气鲁棒性方面的独特优势。他们借鉴了SparseDrive的稀疏表示范式和SpaRC的稀疏融合设计，将雷达数据特性与规划需求相结合，设计了查询方法来迭代优化地图和代理表示的运动和位置特征。他们利用反射雷达点的空间接近性作为强归纳偏置，构建了一个统一的端到端优化框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过稀疏3D特征对齐和多普勒速度估计实现强3D场景表示，用于优化代理锚点、地图多段线和运动建模。整体流程包括：1)多模态稀疏特征编码处理摄像头和雷达输入；2)统一稀疏融合利用查询方法在模态间交互；3)并行运动规划联合优化增强的空间场景表示。具体实现中，系统处理360度图像和雷达点云，设计检测和地图查询，实现范围自适应聚合和多模态透视特征对齐，最后通过概率轨迹建模生成优化后的驾驶轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个基于雷达的端到端自动驾驶基准；2)扩展的稀疏融合设计同时处理检测、跟踪和规划；3)整体雷达融合提升多个任务性能；4)多基准评估验证方法有效性。相比之前工作，不同之处在于：专注于摄像头-雷达而非摄像头-激光雷达融合；将雷达集成到端到端优化而非仅用于模块化感知；使用稀疏而非密集表示处理雷达数据；采用查询方法迭代优化表示；特别关注长期轨迹预测一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SpaRC-AD通过创新的摄像头-雷达稀疏融合方法，显著提升了端到端自动驾驶系统在恶劣天气、遮挡场景和长距离检测中的性能，特别是在运动理解和轨迹预测方面，为安全关键自动驾驶应用提供了更可靠的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end autonomous driving systems promise stronger performance throughunified optimization of perception, motion forecasting, and planning. However,vision-based approaches face fundamental limitations in adverse weatherconditions, partial occlusions, and precise velocity estimation - criticalchallenges in safety-sensitive scenarios where accurate motion understandingand long-horizon trajectory prediction are essential for collision avoidance.To address these limitations, we propose SpaRC-AD, a query-based end-to-endcamera-radar fusion framework for planning-oriented autonomous driving. Throughsparse 3D feature alignment, and doppler-based velocity estimation, we achievestrong 3D scene representations for refinement of agent anchors, map polylinesand motion modelling. Our method achieves strong improvements over thestate-of-the-art vision-only baselines across multiple autonomous drivingtasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA),online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectoryplanning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporalconsistency on multiple challenging benchmarks, including real-world open-loopnuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. Weshow the effectiveness of radar-based fusion in safety-critical scenarios whereaccurate motion understanding and long-horizon trajectory prediction areessential for collision avoidance. The source code of all experiments isavailable at https://phi-wol.github.io/sparcad/</description>
      <author>example@mail.com (Philipp Wolters, Johannes Gilg, Torben Teepe, Gerhard Rigoll)</author>
      <guid isPermaLink="false">2508.10567v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes</title>
      <link>http://arxiv.org/abs/2508.10427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://turingmotors.github.io/stride-qa/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;STRIDE-QA是一个大规模视觉问答数据集，专为自动驾驶中物理基础的时空推理设计，填补了现有视觉-语言模型在处理动态交通场景方面的不足。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉-语言模型(VLMs)在自动驾驶中应用于支持复杂现实场景的决策制定，但其基于静态网页图像-文本对的训练方式限制了精确时空推理能力，难以理解和预测动态交通场景。&lt;h4&gt;目的&lt;/h4&gt;开发一个支持物理基础推理的大规模视觉问答数据集，特别关注自我中心视角，以提高VLMs在自动驾驶中的时空推理能力。&lt;h4&gt;方法&lt;/h4&gt;构建STRIDE-QA数据集，使用东京100小时的多传感器驾驶数据，包含1600万个问答对和285K帧，通过密集自动生成的注释(3D边界框、分割掩码和多对象轨迹)进行基础支持，设计三种新颖的QA任务支持对象中心和自我中心推理。&lt;h4&gt;主要发现&lt;/h4&gt;现有VLMs在预测一致性方面表现极差(接近零分)，而在STRIDE-QA上微调的VLMs表现出显著性能提升，空间定位成功率达55%，未来运动预测一致性达28%，远高于通用VLMs的接近零分。&lt;h4&gt;结论&lt;/h4&gt;STRIDE-QA为开发更可靠的视觉-语言模型用于安全关键型自主系统奠定了全面基础，显著提升了模型在自动驾驶场景中的时空推理能力。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型(VLMs)已被应用于自动驾驶，以支持复杂现实场景中的决策制定。然而，它们基于静态网页来源的图像-文本对的训练从根本上限制了精确时空推理能力，而这种能力对于理解和预测动态交通场景是必需的。我们通过STRIDE-QA解决了这一关键差距，这是一个大规模视觉问答(VQA)数据集，用于从自我中心视角进行物理基础的推理。该数据集由东京100小时的多传感器驾驶数据构建，捕捉了多样且具有挑战性的条件，是城市驾驶中时空推理的最大VQA数据集，在285K帧上提供了1600万个问答对。通过密集的自动生成的注释(包括3D边界框、分割掩码和多对象轨迹)进行基础支持，该数据集通过三种新颖的QA任务独特地支持了对象中心和自我中心推理，这些任务需要空间定位和时间预测。我们的基准测试表明，现有的VLMs表现不佳，在预测一致性方面接近零分。相比之下，在STRIDE-QA上微调的VLMs表现出显著的性能提升，在空间定位上达到55%的成功率，在未来运动预测上达到28%的一致性，而通用VLMs的得分接近零。因此，STRIDE-QA为开发更可靠的VLMs用于安全关键的自主系统建立了全面的基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉语言模型(VLMs)在自动驾驶领域缺乏精确时空推理能力的问题。这个问题很重要，因为自动驾驶系统需要在复杂多变的交通环境中安全可靠地决策，而缺乏对物体位置、关系和未来运动轨迹的准确理解会导致安全隐患，同时这也是连接大规模多模态预训练与物理人工智能需求之间的关键研究差距。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有VLMs在自动驾驶应用中的局限性，认识到它们大多在静态网络图像上训练，缺乏动态场景理解能力。他们借鉴了多个现有工作的元素：参考nuScenes等数据集使用多传感器数据，采用BEVFusion进行3D检测，使用PubTracker进行目标跟踪，应用SAM 2.1进行语义分割，并基于模板生成QA对。然而，作者将这些元素创新性地整合到一个专门设计用于生成时空问答对的自动化管道中，形成了独特的解决方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模视觉问答数据集，专门用于训练和评估VLMs在自动驾驶场景中的时空推理能力，不仅包含静态空间关系问题，还包含预测物体未来位置和运动的问题。整体流程包括：1)在东京收集100多小时多传感器驾驶数据；2)以1Hz采样关键帧；3)使用BEVFusion进行3D物体检测；4)通过PubTracker进行多目标跟踪；5)提取物体距离、方向和速度等属性；6)应用SAM 2.1生成语义分割掩码；7)过滤不可靠检测；8)基于模板生成QA对。最终形成包含1600万个QA对的数据集，支持三种推理任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)定义三种新颖VQA任务（物体中心空间QA、自中心空间QA和自中心时空QA）；2)创建包含1600万个QA对的大规模数据集；3)提出模块化、可扩展的自动化注释管道；4)提供物理和时空一致的高质量注释。相比之前工作，STRIDE-QA规模更大、任务更多样（同时支持物体中心和自中心推理）、注释更时空一致、数据来源更真实（自收集而非网络来源），并提出了专门的评估指标来衡量时空推理能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出STRIDE-QA这一大规模时空问答数据集，填补了视觉语言模型在自动驾驶领域精确时空推理能力的关键空白，并展示了基于该数据集训练的模型在预测物体未来运动方面的显著性能提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have been applied to autonomous driving tosupport decision-making in complex real-world scenarios. However, theirtraining on static, web-sourced image-text pairs fundamentally limits theprecise spatiotemporal reasoning required to understand and predict dynamictraffic scenes. We address this critical gap with STRIDE-QA, a large-scalevisual question answering (VQA) dataset for physically grounded reasoning froman ego-centric perspective. Constructed from 100 hours of multi-sensor drivingdata in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is thelargest VQA dataset for spatiotemporal reasoning in urban driving, offering 16million QA pairs over 285K frames. Grounded by dense, automatically generatedannotations including 3D bounding boxes, segmentation masks, and multi-objecttracks, the dataset uniquely supports both object-centric and ego-centricreasoning through three novel QA tasks that require spatial localization andtemporal prediction. Our benchmarks demonstrate that existing VLMs strugglesignificantly, achieving near-zero scores on prediction consistency. Incontrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains,achieving 55% success in spatial localization and 28% consistency in futuremotion prediction, compared to near-zero scores from general-purpose VLMs.Therefore, STRIDE-QA establishes a comprehensive foundation for developing morereliable VLMs for safety-critical autonomous systems.</description>
      <author>example@mail.com (Keishi Ishihara, Kento Sasaki, Tsubasa Takahashi, Daiki Shiono, Yu Yamaguchi)</author>
      <guid isPermaLink="false">2508.10427v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Conic Formulations of Transport Metrics for Unbalanced Measure Networks and Hypernetworks</title>
      <link>http://arxiv.org/abs/2508.10888v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了锥形Gromov-Wasserstein (CGW)距离，提出了一种基于半耦合的新公式，扩展了框架以比较更一般的网络和超网络结构，建立了CGW度量的基本性质，包括尺度行为、变分收敛性和比较边界，同时推导了其鲁棒性的定量边界。&lt;h4&gt;背景&lt;/h4&gt;Gromov-Wasserstein (GW)最优传输是一种用于比较不同度量空间上定义的概率密度的方法，已成为分析具有复杂数据结构（如点云集合或网络）的重要工具。然而，传统GW方法存在限制，如只能比较等质量的测度，以及对异常值敏感。&lt;h4&gt;目的&lt;/h4&gt;研究Séjourne、Vialard和Peyré提出的锥形Gromov-Wasserstein (CGW)距离，克服传统GW方法的限制，特别是等质量比较和异常值敏感性问题。&lt;h4&gt;方法&lt;/h4&gt;提出基于半耦合的新公式；将框架扩展到超越度量测度空间设置，以比较更一般的网络和超网络结构；建立CGW度量的基本性质，包括在膨胀下的尺度行为、体积增长约束极限下的变分收敛性；与已建立的最优传输度量进行比较边界；推导定量边界，表征CGW度量对基础测度扰动的鲁棒性；开发简单且可证明收敛的块坐标上升算法进行估计。&lt;h4&gt;主要发现&lt;/h4&gt;CGW度量在膨胀下的尺度行为；在体积增长约束极限下的变分收敛性；与其他最优传输度量的比较边界；CGW度量对基础测度扰动的鲁棒性的定量边界。&lt;h4&gt;结论&lt;/h4&gt;提出了CGW度量的新公式，可以处理更一般的网络和超网络结构；该方法在计算上可行且可扩展，在高维和结构化数据集上进行了实验验证。&lt;h4&gt;翻译&lt;/h4&gt;Gromov-Wasserstein (GW)最优传输变体设计用于比较定义在不同度量空间上的概率密度，已成为分析具有复杂结构数据（如点云集合或网络）的重要工具。为克服某些限制，如仅限于比较等质量测度和对异常值的敏感性，最近文献中引入了几种非平衡或部分传输的GW距离松弛。本文关注Séjourne、Vialard和Peyré提出的锥形Gromov-Wasserstein (CGW)距离。我们提供了基于半耦合的新公式，并将框架扩展到超越度量测度空间设置，以比较更一般的网络和超网络结构。通过这个新公式，我们建立了CGW度量的几个基本性质，包括其在膨胀下的尺度行为、体积增长约束极限下的变分收敛性，以及与已建立的最优传输度量的比较边界。我们进一步推导出定量边界，表征CGW度量对基础测度扰动的鲁棒性。CGW的超网络公式允许简单且可证明收敛的块坐标上升算法进行估计，我们通过在合成和真实世界的高维和结构化数据集上的实验证明了我们方法的计算可行性和可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Gromov-Wasserstein (GW) variant of optimal transport, designed to compareprobability densities defined over distinct metric spaces, has emerged as animportant tool for the analysis of data with complex structure, such asensembles of point clouds or networks. To overcome certain limitations, such asthe restriction to comparisons of measures of equal mass and sensitivity tooutliers, several unbalanced or partial transport relaxations of the GWdistance have been introduced in the recent literature. This paper is concernedwith the Conic Gromov-Wasserstein (CGW) distance introduced byS\'{e}journ\'{e}, Vialard, and Peyr\'{e}. We provide a novel formulation interms of semi-couplings, and extend the framework beyond the metric measurespace setting, to compare more general network and hypernetwork structures.With this new formulation, we establish several fundamental properties of theCGW metric, including its scaling behavior under dilation, variationalconvergence in the limit of volume growth constraints, and comparison boundswith established optimal transport metrics. We further derive quantitativebounds that characterize the robustness of the CGW metric to perturbations inthe underlying measures. The hypernetwork formulation of CGW admits a simpleand provably convergent block coordinate ascent algorithm for its estimation,and we demonstrate the computational tractability and scalability of ourapproach through experiments on synthetic and real-world high-dimensional andstructured datasets.</description>
      <author>example@mail.com (Mary Chriselda Antony Oliver, Emmanuel Hartman, Tom Needham)</author>
      <guid isPermaLink="false">2508.10888v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Natively Trainable Sparse Attention for Hierarchical Point Cloud Datasets</title>
      <link>http://arxiv.org/abs/2508.10758v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过结合Erwin架构和Native Sparse Attention机制，解决了Transformer在处理大型物理系统数据时的二次方扩展问题，提高了模型效率和感受野，同时在三个物理科学数据集上实现了与原始模型相当或更好的性能。&lt;h4&gt;背景&lt;/h4&gt;Transformer模型在处理大型物理系统数据时具有潜力，但注意力机制的二次方扩展问题限制了其应用效率。&lt;h4&gt;目的&lt;/h4&gt;探索将Erwin架构与Native Sparse Attention (NSA)机制相结合，提高Transformer模型处理大规模物理系统的效率和感受野，解决注意力机制的二次复杂度挑战。&lt;h4&gt;方法&lt;/h4&gt;将NSA机制适应为非序列数据，实现Erwin NSA模型，并在三个物理科学数据集（宇宙学模拟、分子动力学和气压建模）上评估，同时重现Erwin论文的实验结果以验证实现。&lt;h4&gt;主要发现&lt;/h4&gt;在三个物理科学数据集上，Erwin NSA模型的性能与原始Erwin模型相当或更好，成功将NSA机制应用于非序列数据。&lt;h4&gt;结论&lt;/h4&gt;通过结合Erwin架构和NSA机制，有效解决了Transformer在处理大型物理系统数据时的二次方扩展问题，提高了模型的效率和感受野，同时保持了或提高了性能。&lt;h4&gt;翻译&lt;/h4&gt;释放Transformer在大型物理系统数据集上的潜力，需要克服注意力机制的二次方扩展问题。本研究探索将Erwin架构与Native Sparse Attention (NSA)机制相结合，以提高Transformer模型处理大规模物理系统的效率和感受野，解决注意力机制二次复杂度的挑战。我们将NSA机制适应为非序列数据，实现了Erwin NSA模型，并在三个物理科学数据集（宇宙学模拟、分子动力学和气压建模）上进行了评估，其性能与原始Erwin模型相当或更好。此外，我们重现了Erwin论文的实验结果以验证其实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unlocking the potential of transformers on datasets of large physical systemsdepends on overcoming the quadratic scaling of the attention mechanism. Thiswork explores combining the Erwin architecture with the Native Sparse Attention(NSA) mechanism to improve the efficiency and receptive field of transformermodels for large-scale physical systems, addressing the challenge of quadraticattention complexity. We adapt the NSA mechanism for non-sequential data,implement the Erwin NSA model, and evaluate it on three datasets from thephysical sciences -- cosmology simulations, molecular dynamics, and airpressure modeling -- achieving performance that matches or exceeds that of theoriginal Erwin model. Additionally, we reproduce the experimental results fromthe Erwin paper to validate their implementation.</description>
      <author>example@mail.com (Nicolas Lapautre, Maria Marchenko, Carlos Miguel Patiño, Xin Zhou)</author>
      <guid isPermaLink="false">2508.10758v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Sparse Point Cloud Data Processing for Privacy-aware Human Action Recognition</title>
      <link>http://arxiv.org/abs/2508.10469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对基于毫米波雷达的人类动作识别系统，评估了三种主要数据处理方法（DBSCAN、匈牙利算法和卡尔曼滤波）及其组合的性能，旨在提高雷达数据的准确性和连续性，同时降低计算成本。&lt;h4&gt;背景&lt;/h4&gt;人类动作识别在医疗保健、健身追踪和辅助生活技术中起着关键作用。传统基于视觉的HAR系统虽然有效，但存在隐私问题。毫米波雷达传感器提供了一种保护隐私的替代方案，但其点云数据稀疏且嘈杂，带来了挑战。文献中广泛使用了三种主要的数据处理方法来提高雷达数据的质量和连续性，但缺乏对这些方法及其组合的综合评估。&lt;h4&gt;目的&lt;/h4&gt;填补毫米波雷达数据处理方法综合评估的空白，通过详细分析三种方法（单独使用、两两组合以及全部组合）在MiliPoint数据集上的性能，评估识别准确性和计算成本，并提出针对性的改进方法以提高准确性。&lt;h4&gt;方法&lt;/h4&gt;使用MiliPoint数据集对三种数据处理方法（DBSCAN、匈牙利算法和卡尔曼滤波）进行详细性能分析。评估包括：单独使用每种方法、所有可能的成对组合以及三种方法的组合，同时评估识别准确性和计算成本。此外，还提出了针对单个方法的改进措施以提高准确性。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果提供了关于每种方法及其组合的优势和权衡的关键见解，这将指导未来基于毫米波雷达的HAR系统的工作。虽然摘要中没有具体说明结果，但研究应该已经确定了哪种方法或组合在准确性和计算效率方面表现最佳。&lt;h4&gt;结论&lt;/h4&gt;通过对毫米波雷达数据处理方法的全面评估，该研究为基于毫米波雷达的HAR系统提供了宝贵的指导，帮助研究人员和开发者选择最适合其应用场景的数据处理方法，平衡准确性和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;人类动作识别(HAR)在医疗保健、健身追踪和辅助生活技术中起着至关重要的作用。虽然传统的基于视觉的HAR系统有效，但它们带来了隐私问题。毫米波雷达传感器提供了一种保护隐私的替代方案，但由于其点云数据的稀疏和嘈杂特性，带来了挑战。在文献中，三种主要的数据处理方法：基于密度的噪声应用空间聚类(DBSCAN)、匈牙利算法和卡尔曼滤波已被广泛用于提高雷达数据的质量和连续性。然而，对这些方法单独使用和组合使用的综合评估仍然缺乏。本文通过使用MiliPoint数据集对这三种方法进行详细的性能分析来填补这一空白。我们单独评估每种方法、所有可能的成对组合以及三种方法的组合，评估识别准确性和计算成本。此外，我们还提出了针对单个方法的改进措施以提高准确性。我们的结果提供了关于每种方法及其组合的优势和权衡的关键见解，指导未来基于毫米波雷达的HAR系统的工作。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决毫米波雷达传感器产生的稀疏点云数据在人体动作识别中的处理挑战。这个问题很重要，因为传统基于视觉的动作识别系统虽然有效，但存在隐私问题；而毫米波雷达虽然能保护隐私，但其数据稀疏且嘈杂，难以直接用于准确识别。随着智能环境和辅助生活技术的发展，在保护隐私的同时准确识别人体动作变得越来越重要，特别是在医疗保健、健身追踪和老年人护理等敏感应用场景中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有研究中已经应用的三种主要数据处理方法：DBSCAN用于聚类和噪声过滤，匈牙利算法用于数据关联，卡尔曼滤波用于轨迹预测。然而，作者发现这些方法缺乏系统性评估，且大多独立使用而非组合。作者借鉴了mID管道等现有工作，但指出其缺乏系统调参且在稀疏输入时性能下降。基于这些观察，作者设计了一个综合框架，不仅单独评估每种方法，还评估了它们的组合（两两组合和三者组合），并针对每种方法进行了参数优化和特定增强。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过组合三种数据处理技术（DBSCAN、匈牙利算法和卡尔曼滤波）来增强毫米波雷达点云数据的质量，提高人体动作识别的准确性和效率，同时保护用户隐私。整体流程包括：1)将原始雷达帧分割成5个连续段；2)移除原点附近的零填充点；3)使用DBSCAN进行噪声减少和聚类识别；4)应用匈牙利算法进行跨段聚类关联；5)使用卡尔曼滤波预测和修正轨迹；6)比较预测轨迹与真实关键点，选择最佳人类聚类；7)输出最终处理后的数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次对三种主要雷达点云预处理技术及其组合进行了全面的性能分析；2)针对每种方法进行了详细的参数调优；3)提出了针对个体方法的特定增强，如DBSCAN中的垂直加权因子；4)评估了所有可能的二元组合和三者组合，同时考虑了识别准确性和计算成本；5)公开了源代码，促进研究可重复性。相比之前的工作，本研究不仅独立评估各种方法，还系统分析了它们的组合；不仅关注准确性，还考虑了计算效率；针对稀疏数据问题提出了特定增强方法，为实际应用提供了更全面的指导。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性评估和增强三种主流雷达点云预处理技术及其组合，为隐私保护的人体动作识别提供了一种高效且准确的数据处理方法，同时为实际应用中的方法选择提供了重要指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human Action Recognition (HAR) plays a crucial role in healthcare, fitnesstracking, and ambient assisted living technologies. While traditional visionbased HAR systems are effective, they pose privacy concerns. mmWave radarsensors offer a privacy preserving alternative but present challenges due tothe sparse and noisy nature of their point cloud data. In the literature, threeprimary data processing methods: Density-Based Spatial Clustering ofApplications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filteringhave been widely used to improve the quality and continuity of radar data.However, a comprehensive evaluation of these methods, both individually and incombination, remains lacking. This paper addresses that gap by conducting adetailed performance analysis of the three methods using the MiliPoint dataset.We evaluate each method individually, all possible pairwise combinations, andthe combination of all three, assessing both recognition accuracy andcomputational cost. Furthermore, we propose targeted enhancements to theindividual methods aimed at improving accuracy. Our results provide crucialinsights into the strengths and trade-offs of each method and theirintegrations, guiding future work on mmWave based HAR systems</description>
      <author>example@mail.com (Maimunatu Tunau, Vincent Gbouna Zakka, Zhuangzhuang Dai)</author>
      <guid isPermaLink="false">2508.10469v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>GPZ: GPU-Accelerated Lossy Compressor for Particle Data</title>
      <link>http://arxiv.org/abs/2508.10305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出GPZ，一种专为GPU大规模粒子数据设计的高性能、有误差边界的有损压缩器，通过四阶段并行流水线和针对性优化，显著提升了压缩效率和吞吐量。&lt;h4&gt;背景&lt;/h4&gt;基于粒子的模拟和点云应用产生大量不规则数据集，对存储、I/O和实时分析构成挑战，传统压缩技术在处理不规则粒子分布和GPU架构限制方面存在困难，导致有限的吞吐量和次优的压缩比。&lt;h4&gt;目的&lt;/h4&gt;设计一种高性能、有误差边界的有损压缩器，专门针对现代GPU上的大规模粒子数据，解决传统压缩技术的局限性。&lt;h4&gt;方法&lt;/h4&gt;GPZ采用新颖的四阶段并行流水线，协同平衡高压缩效率与大规模并行硬件的架构需求；引入针对计算、内存访问和GPU占用的一系列针对性优化，实现接近硬件极限的吞吐量。&lt;h4&gt;主要发现&lt;/h4&gt;GPZ在三种不同GPU架构(工作站、数据中心和边缘)上，使用六个来自不同领域的大规模真实科学数据集进行评估，结果显示GPZ持续且显著优于五种最先进的GPU压缩器，提供高达8倍的更高端到端吞吐量，同时实现更好的压缩比和数据质量。&lt;h4&gt;结论&lt;/h4&gt;GPZ通过创新的架构设计和优化策略，成功解决了大规模粒子数据在GPU上的压缩挑战，为粒子模拟和点云应用提供了高效的压缩解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于粒子的模拟和点云应用产生大量不规则数据集，对存储、I/O和实时分析构成挑战。传统压缩技术在处理不规则粒子分布和GPU架构限制方面存在困难，通常导致有限的吞吐量和次优的压缩比。在本文中，我们提出GPZ，一种专为现代GPU上大规模粒子数据设计的高性能、有误差边界的有损压缩器。GPZ采用新颖的四阶段并行流水线，协同平衡高压缩效率与大规模并行硬件的架构需求。我们引入了一系列针对计算、内存访问和GPU占用的针对性优化，使GPZ能够实现接近硬件极限的吞吐量。我们在三种不同的GPU架构(工作站、数据中心和边缘)上使用来自五个不同领域的六个大规模、真实科学数据集进行了广泛评估。结果表明，GPZ持续且显著优于五种最先进的GPU压缩器，提供高达8倍的更高端到端吞吐量，同时实现更好的压缩比和数据质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Particle-based simulations and point-cloud applications generate massive,irregular datasets that challenge storage, I/O, and real-time analytics.Traditional compression techniques struggle with irregular particledistributions and GPU architectural constraints, often resulting in limitedthroughput and suboptimal compression ratios. In this paper, we present GPZ, ahigh-performance, error-bounded lossy compressor designed specifically forlarge-scale particle data on modern GPUs. GPZ employs a novel four-stageparallel pipeline that synergistically balances high compression efficiencywith the architectural demands of massively parallel hardware. We introduce asuite of targeted optimizations for computation, memory access, and GPUoccupancy that enables GPZ to achieve near-hardware-limit throughput. Weconduct an extensive evaluation on three distinct GPU architectures(workstation, data center, and edge) using six large-scale, real-worldscientific datasets from five distinct domains. The results demonstrate thatGPZ consistently and significantly outperforms five state-of-the-art GPUcompressors, delivering up to 8x higher end-to-end throughput whilesimultaneously achieving superior compression ratios and data quality.</description>
      <author>example@mail.com (Ruoyu Li, Yafan Huang, Longtao Zhang, Zhuoxun Yang, Sheng Di, Jiajun Huang, Jinyang Liu, Jiannan Tian, Xin Liang, Guanpeng Li, Hanqi Guo, Franck Cappello, Kai Zhao)</author>
      <guid isPermaLink="false">2508.10305v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing</title>
      <link>http://arxiv.org/abs/2508.10881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://lg-li.github.io/project/tooncomposer&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ToonComposer是一种生成模型，统一了中间帧生成和上色过程，通过稀疏草图注入机制提供精确控制，仅需少量输入就能高效完成卡通制作，显著减少了人工工作量并提高了灵活性。&lt;h4&gt;背景&lt;/h4&gt;传统卡通和动漫制作包含关键帧绘制、中间帧生成和上色等需要大量人工劳动的环节。尽管AI技术有所进步，但现有方法通常分别处理这些阶段，导致错误累积和伪影问题。例如，中间帧生成方法难以处理大幅度动作，而上色方法需要密集的每帧草图。&lt;h4&gt;目的&lt;/h4&gt;引入ToonComposer模型，将中间帧生成和上色统一到一个后关键帧处理阶段，解决现有方法的局限性，减少人工工作量，提高创作灵活性，为艺术家提供更好的AI辅助工具。&lt;h4&gt;方法&lt;/h4&gt;ToonComposer采用稀疏草图注入机制，通过关键帧草图提供精确控制。同时，使用卡通适配方法结合空间低秩适配器，将现代视频基础模型调整为卡通领域，同时保持其时间先验特性。该模型只需一个草图和一个彩色参考帧就能处理稀疏输入，并支持在任意时间位置使用多个草图实现更精确的运动控制。&lt;h4&gt;主要发现&lt;/h4&gt;研究团队创建了PKBench基准，包含手绘草图以模拟真实用例。评估结果显示，ToonComposer在视觉质量、运动一致性和生产效率方面均优于现有方法，为AI辅助卡通制作提供了更优越的解决方案。&lt;h4&gt;结论&lt;/h4&gt;ToonComposer通过统一处理流程和灵活的输入机制，显著减少了卡通制作的人工工作量，提高了创作灵活性，能够有效赋能艺术家在真实场景中的应用，代表了AI辅助卡通制作的重要进步。&lt;h4&gt;翻译&lt;/h4&gt;传统卡通和动漫制作涉及关键帧绘制、中间帧生成和上色等阶段，这些都需要大量人工劳动。尽管最近AI技术有所进步，但现有方法通常分别处理这些阶段，导致错误累积和伪影。例如，中间帧生成方法难以处理大幅度动作，而上色方法需要密集的每帧草图。为解决这一问题，我们引入了ToonComposer，这是一个生成模型，将中间帧生成和上色统一到一个后关键帧处理阶段。ToonComposer采用稀疏草图注入机制，通过关键帧草图提供精确控制。此外，它使用卡通适配方法结合空间低秩适配器，将现代视频基础模型调整为卡通领域，同时保持其时间先验。仅需一个草图和一个彩色参考帧，ToonComposer就能高效处理稀疏输入，同时支持在任意时间位置使用多个草图实现更精确的运动控制。这种双重能力减少了人工工作量并提高了灵活性，赋能艺术家在真实场景中的应用。为评估我们的模型，我们进一步创建了PKBench基准，包含模拟真实用例的手绘草图。我们的评估表明，ToonComposer在视觉质量、运动一致性和生产效率方面优于现有方法，为AI辅助卡通制作提供了更优越、更灵活的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional cartoon and anime production involves keyframing, inbetweening,and colorization stages, which require intensive manual effort. Despite recentadvances in AI, existing methods often handle these stages separately, leadingto error accumulation and artifacts. For instance, inbetweening approachesstruggle with large motions, while colorization methods require dense per-framesketches. To address this, we introduce ToonComposer, a generative model thatunifies inbetweening and colorization into a single post-keyframing stage.ToonComposer employs a sparse sketch injection mechanism to provide precisecontrol using keyframe sketches. Additionally, it uses a cartoon adaptationmethod with the spatial low-rank adapter to tailor a modern video foundationmodel to the cartoon domain while keeping its temporal prior intact. Requiringas few as a single sketch and a colored reference frame, ToonComposer excelswith sparse inputs, while also supporting multiple sketches at any temporallocation for more precise motion control. This dual capability reduces manualworkload and improves flexibility, empowering artists in real-world scenarios.To evaluate our model, we further created PKBench, a benchmark featuringhuman-drawn sketches that simulate real-world use cases. Our evaluationdemonstrates that ToonComposer outperforms existing methods in visual quality,motion consistency, and production efficiency, offering a superior and moreflexible solution for AI-assisted cartoon production.</description>
      <author>example@mail.com (Lingen Li, Guangzhi Wang, Zhaoyang Zhang, Yaowei Li, Xiaoyu Li, Qi Dou, Jinwei Gu, Tianfan Xue, Ying Shan)</author>
      <guid isPermaLink="false">2508.10881v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>APFL: Analytic Personalized Federated Learning via Dual-Stream Least Squares</title>
      <link>http://arxiv.org/abs/2508.10732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分析个性化联邦学习(APFL)方法，通过双流最小二乘法解决非IID数据问题，实现集体泛化和个体个性化的平衡。&lt;h4&gt;背景&lt;/h4&gt;个性化联邦学习(PFL)旨在通过协作训练为各个客户端提供个性化模型，但现有方法容易受到非IID数据的挑战，这严重阻碍了集体泛化能力并影响个性化效果。&lt;h4&gt;目的&lt;/h4&gt;解决PFL中的非IID问题，提高个性化模型的泛化能力和个性化效果。&lt;h4&gt;方法&lt;/h4&gt;提出APFL方法，使用基础模型作为冻结骨干网络进行特征提取，并开发双流分析模型：共享主流实现所有客户端的全局泛化，专用精炼流实现每个客户端的本地个性化。&lt;h4&gt;主要发现&lt;/h4&gt;APFL具有异构不变性的理想特性，理论上无论数据在其他客户端间如何分布，每个个性化模型都保持不变；实验结果表明，该方法比最先进基线方法在准确性上提高至少1.10%-15.45%。&lt;h4&gt;结论&lt;/h4&gt;APFL方法有效解决了PFL中的非IID问题，通过双流分析模型同时实现了集体泛化和个体个性化，显著提高了个性化模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;个性化联邦学习(PFL)通过协作训练向各个客户端提供个性化模型方面提出了重大挑战。现有的PFL方法通常容易受到非IID数据的挑战，这严重阻碍了集体泛化能力，进而影响了后续的个性化工作。在本文中，为了解决PFL中的非IID问题，我们通过双流最小二乘法提出了一种分析个性化联邦学习(APFL)方法。在我们的APFL中，我们使用基础模型作为冻结骨干网络进行特征提取。在特征提取器之后，我们开发了双流分析模型，以实现集体泛化和个体个性化。具体来说，我们的APFL包含一个共享的主流，用于所有客户端的全局泛化，以及一个专用的精炼流，用于每个客户端的本地个性化。我们的APFL的分析解决方案使其具有异构不变性的理想特性，理论上意味着无论数据在其他所有客户端之间如何异构分布，每个个性化模型都保持不变。各种数据集上的经验结果也验证了我们的APFL优于最先进的基线方法，在准确性方面具有至少1.10%-15.45%的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalized Federated Learning (PFL) has presented a significant challengeto deliver personalized models to individual clients through collaborativetraining. Existing PFL methods are often vulnerable to non-IID data, whichseverely hinders collective generalization and then compromises the subsequentpersonalization efforts. In this paper, to address this non-IID issue in PFL,we propose an Analytic Personalized Federated Learning (APFL) approach viadual-stream least squares. In our APFL, we use a foundation model as a frozenbackbone for feature extraction. Subsequent to the feature extractor, wedevelop dual-stream analytic models to achieve both collective generalizationand individual personalization. Specifically, our APFL incorporates a sharedprimary stream for global generalization across all clients, and a dedicatedrefinement stream for local personalization of each individual client. Theanalytical solutions of our APFL enable its ideal property of heterogeneityinvariance, theoretically meaning that each personalized model remainsidentical regardless of how heterogeneous the data are distributed across allother clients. Empirical results across various datasets also validate thesuperiority of our APFL over state-of-the-art baselines, with advantages of atleast 1.10%-15.45% in accuracy.</description>
      <author>example@mail.com (Kejia Fan, Jianheng Tang, Zhirui Yang, Feijiang Han, Jiaxu Li, Run He, Yajiang Huang, Anfeng Liu, Houbing Herbert Song, Yunhuai Liu, Huiping Zhuang)</author>
      <guid isPermaLink="false">2508.10732v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Towards Agentic AI for Multimodal-Guided Video Object Segmentation</title>
      <link>http://arxiv.org/abs/2508.10572v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Multi-Modal Agent的新型智能体系统，用于解决基于引用的视频对象分割问题，该方法利用大语言模型的推理能力生成动态工作流程，与专门工具交互以识别目标对象，在两个多模态条件VOS任务上明显优于先前方法。&lt;h4&gt;背景&lt;/h4&gt;基于引用的视频对象分割是一个多模态问题，需要根据外部提示产生细粒度分割结果。传统方法需要训练专门模型，计算复杂度高且需要大量人工标注。虽然最近视觉-语言基础模型的进展为免训练方法提供了新方向，但现有方法仍依赖固定流程，缺乏适应任务动态性质的灵活性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法缺乏适应任务动态性质灵活性的问题，提出一种更灵活、自适应的解决方案来处理基于引用的视频对象分割任务。&lt;h4&gt;方法&lt;/h4&gt;提出Multi-Modal Agent智能体系统，利用大语言模型的推理能力为每个输入生成动态工作流程，这种自适应过程与一组为不同模态低级任务设计的专门工具迭代交互，通过多模态提示识别目标对象。&lt;h4&gt;主要发现&lt;/h4&gt;在两个多模态条件VOS任务(RVOS和Ref-AVS)上，多模态智能体方法明显优于先前方法；利用通用基础模型可以实现与全监督、特定任务模型相当的性能。&lt;h4&gt;结论&lt;/h4&gt;多模态智能体方法为基于引用的视频对象分割任务提供了更灵活、自适应的解决方案，其动态工作流程设计比固定流程方法更具优势。&lt;h4&gt;翻译&lt;/h4&gt;基于引用的视频对象分割是一个需要根据外部提示产生细粒度分割结果的多模态问题。传统方法通常涉及训练专门模型，这些模型具有高计算复杂度和人工标注工作量。最近视觉-语言基础模型的进展为免训练方法提供了有希望的方向。一些研究已经探索利用这些通用模型进行细粒度分割，实现了与全监督、特定任务模型相当的性能。然而，现有方法依赖于缺乏适应任务动态性质所需灵活性的固定流程。为解决这一局限，我们提出了Multi-Modal Agent，一种新型智能体系统，旨在以更灵活和自适应的方式解决此任务。具体而言，我们的方法利用大语言模型的推理能力为每个输入生成定制化的动态工作流程。这种自适应过程与一组为不同模态低级任务设计的专门工具迭代交互，以识别由多模态提示描述的目标对象。我们的智能体方法在两个多模态条件VOS任务上明显优于先前方法：RVOS和Ref-AVS。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Referring-based Video Object Segmentation is a multimodal problem thatrequires producing fine-grained segmentation results guided by external cues.Traditional approaches to this task typically involve training specializedmodels, which come with high computational complexity and manual annotationeffort. Recent advances in vision-language foundation models open a promisingdirection toward training-free approaches. Several studies have exploredleveraging these general-purpose models for fine-grained segmentation,achieving performance comparable to that of fully supervised, task-specificmodels. However, existing methods rely on fixed pipelines that lack theflexibility needed to adapt to the dynamic nature of the task. To address thislimitation, we propose Multi-Modal Agent, a novel agentic system designed tosolve this task in a more flexible and adaptive manner. Specifically, ourmethod leverages the reasoning capabilities of large language models (LLMs) togenerate dynamic workflows tailored to each input. This adaptive procedureiteratively interacts with a set of specialized tools designed for low-leveltasks across different modalities to identify the target object described bythe multimodal cues. Our agentic approach demonstrates clear improvements overprior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.</description>
      <author>example@mail.com (Tuyen Tran, Thao Minh Le, Truyen Tran)</author>
      <guid isPermaLink="false">2508.10572v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection</title>
      <link>http://arxiv.org/abs/2508.10568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于SAM模型的遥感变化检测方法，通过微调SAM编码器并结合空间-时间特征增强和多尺度解码器融合技术，以及一种新的交叉熵掩码损失函数，有效解决了遥感变化检测中的类别不平衡问题，在多个数据集上超越了现有最先进的方法。&lt;h4&gt;背景&lt;/h4&gt;基础模型在计算机视觉领域取得了显著成功，它们学习的是通用表示，这些表示可以轻松迁移到训练过程中未见的任务。SAM作为基础模型之一，能够准确分割图像中的物体。&lt;h4&gt;目的&lt;/h4&gt;改进遥感变化检测的性能，使其能够在多尺度上稳健地检测变化，并处理变化检测数据集中的高类别不平衡问题。&lt;h4&gt;方法&lt;/h4&gt;1. 通过微调适应SAM编码器用于遥感变化检测；2. 结合空间-时间特征增强技术；3. 应用多尺度解码器融合技术实现多尺度变化检测；4. 提出一种新的交叉熵掩码损失函数来处理类别不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;1. 该方法在四个变化检测数据集（Levir-CD、WHU-CD、CLCD和S2Looking）上超越了现有的最先进方法；2. 在大型复杂的S2Looking数据集上实现了2.5%的F1分数提升；3. 代码已公开在GitHub上。&lt;h4&gt;结论&lt;/h4&gt;通过结合SAM基础模型的强大表征学习能力与专门设计的空间-时间特征增强、多尺度解码器融合和交叉熵掩码损失函数，该方法有效提升了遥感变化检测的性能，特别是在处理复杂场景和类别不平衡问题时表现突出。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在计算机视觉的各个领域都取得了显著成功。它们学习的是通用表示，这些表示可以轻松迁移到训练过程中未见的任务。SAM就是这样一种基础模型，能够准确分割图像中的物体。我们提出通过微调来适应SAM编码器用于遥感变化检测，并结合空间-时间特征增强和多尺度解码器融合，以在多尺度上稳健地检测变化。此外，我们还提出了一种新颖的交叉熵掩码损失函数来处理变化检测数据集中的高类别不平衡问题。我们的方法在四个变化检测数据集上超越了现有的最先进方法。我们在大型复杂的S2Looking数据集上实现了2.5%的F1分数提升。代码可在以下地址获取：https://github.com/humza909/SAM-CEM-CD&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundational models have achieved significant success in diverse domains ofcomputer vision. They learn general representations that are easilytransferable to tasks not seen during training. One such foundational model isSegment anything model (SAM), which can accurately segment objects in images.We propose adapting the SAM encoder via fine-tuning for remote sensing changedetection (RSCD) along with spatial-temporal feature enhancement (STFE) andmulti-scale decoder fusion (MSDF) to detect changes robustly at multiplescales. Additionally, we propose a novel cross-entropy masking (CEM) loss tohandle high class imbalance in change detection datasets. Our methodoutperforms state-of-the-art (SOTA) methods on four change detection datasets,Levir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.5% F1-score improvement ona large complex S2Looking dataset. The code is available at:https://github.com/humza909/SAM-CEM-CD</description>
      <author>example@mail.com (Humza Naveed, Xina Zeng, Mitch Bryson, Nagita Mehrseresht)</author>
      <guid isPermaLink="false">2508.10568v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Cross-Utterance Speech Contexts for Conformer-Transducer Speech Recognition Systems</title>
      <link>http://arxiv.org/abs/2508.10456v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了四种跨语句语音上下文建模方法，应用于流式和非流式Conformer-Transformer自动语音识别系统，提出高效批处理训练方案，实验结果表明跨语句上下文能显著提高识别准确率。&lt;h4&gt;背景&lt;/h4&gt;语音识别系统需要考虑上下文信息以提高识别准确率，但如何高效整合跨语句语音上下文仍是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;评估四种跨语句语音上下文建模方法在Conformer-Transformer ASR系统中的性能，并提出高效训练方案以减少同步开销。&lt;h4&gt;方法&lt;/h4&gt;研究了四种方法：输入音频特征拼接、跨语句编码器嵌入拼接、跨语句编码器嵌入池化投影，以及首次应用于C-T模型的基于块的方法；提出使用拼接语音语句的批处理训练方案；在四个多语言基准数据集上进行了实验。&lt;h4&gt;主要发现&lt;/h4&gt;最佳上下文C-T系统在四个任务上实现了统计显著的WER/CER降低，分别为0.9%、1.1%、0.51%和0.98%绝对值，性能与先进模型相当，证明跨语句上下文的有效性。&lt;h4&gt;结论&lt;/h4&gt;跨语句语音上下文建模能有效提高ASR系统性能，基于块的方法表现突出，高效批处理方案能在保持上下文顺序的同时减少同步开销，为语音基础模型整合上下文信息提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了四种跨语句语音上下文建模方法，应用于流式和非流式Conformer-Transformer自动语音识别系统：i)输入音频特征拼接；ii)跨语句编码器嵌入拼接；iii)跨语句编码器嵌入池化投影；或iv)一种首次应用于C-T模型的基于块的新方法。提出了用于上下文C-T的高效批处理训练方案，使用拼接的语音语句在每个小批量中，以最小化同步开销，同时保留跨语句语音上下文的顺序顺序。在三个语言的四个基准语音数据集上进行了实验：用于上下文C-T模型预训练的英语GigaSpeech和中文Wenetspeech语料库；以及用于领域微调的英语DementiaBank Pitt和粤语JCCOCC MoCA老年语音数据集。最佳性能的上下文C-T系统在预训练和微调阶段始终优于不使用跨语句语音上下文的相应基线系统，在四个任务上实现了统计上显著的词错误率或字符错误率平均降低，分别为0.9%、1.1%、0.51%和0.98%绝对值。它们与先进模型的性能竞争性突显了将跨语句语音上下文整合到当前语音基础模型中的潜在益处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates four types of cross-utterance speech contextsmodeling approaches for streaming and non-streaming Conformer-Transformer (C-T)ASR systems: i) input audio feature concatenation; ii) cross-utterance Encoderembedding concatenation; iii) cross-utterance Encoder embedding poolingprojection; or iv) a novel chunk-based approach applied to C-T models for thefirst time. An efficient batch-training scheme is proposed for contextual C-Tsthat uses spliced speech utterances within each minibatch to minimize thesynchronization overhead while preserving the sequential order ofcross-utterance speech contexts. Experiments are conducted on four benchmarkspeech datasets across three languages: the English GigaSpeech and MandarinWenetspeech corpora used in contextual C-T models pre-training; and the EnglishDementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets used indomain fine-tuning. The best performing contextual C-T systems consistentlyoutperform their respective baselines using no cross-utterance speech contextsin pre-training and fine-tuning stages with statistically significant averageword error rate (WER) or character error rate (CER) reductions up to 0.9%,1.1%, 0.51%, and 0.98% absolute (6.0%, 5.4%, 2.0%, and 3.4% relative) on thefour tasks respectively. Their performance competitiveness againstWav2vec2.0-Conformer, XLSR-128, and Whisper models highlights the potentialbenefit of incorporating cross-utterance speech contexts into current speechfoundation models.</description>
      <author>example@mail.com (Mingyu Cui, Mengzhe Geng, Jiajun Deng, Chengxi Deng, Jiawen Kang, Shujie Hu, Guinan Li, Tianzi Wang, Zhaoqing Li, Xie Chen, Xunying Liu)</author>
      <guid isPermaLink="false">2508.10456v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design</title>
      <link>http://arxiv.org/abs/2508.10409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AnalogSeeker，一个面向模拟电路设计的开源基础语言模型，旨在整合领域知识并提供设计辅助。研究团队通过创新的语料库收集策略和知识蒸馏方法解决了数据稀缺和知识复杂性问题，并建立了以微调为中心的训练范式。训练后的模型在知识评估基准上达到85.04%的准确率，比原始模型提高15.67个百分点，与主流商业模型竞争，并在实际设计任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;模拟电路设计领域面临数据稀缺和知识复杂性的挑战，缺乏专门针对这一领域的开源基础语言模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个开源基础语言模型AnalogSeeker，用于模拟电路设计，整合领域知识并提供设计辅助，解决该领域数据稀缺和知识复杂性的挑战。&lt;h4&gt;方法&lt;/h4&gt;1. 采用基于模拟电路领域知识框架的语料库收集策略，整理相关教科书形成文本领域语料库；2. 引入粒度领域知识蒸馏方法，通过多智能体框架将非结构化文本中的隐式知识转化为问答数据对；3. 建立以微调为中心的训练范式，实现邻域自约束监督微调算法；4. 训练Qwen2.5-32B-Instruct模型获得AnalogSeeker。&lt;h4&gt;主要发现&lt;/h4&gt;1. AnalogSeeker在AMSBench-TQA评估基准上达到85.04%的准确率，比原始模型提高15.67个百分点；2. AnalogSeeker与主流商业模型具有竞争力；3. AnalogSeeker在下游运算放大器设计任务中显示出有效性。&lt;h4&gt;结论&lt;/h4&gt;AnalogSeeker作为专门针对模拟电路设计的开源基础语言模型，成功整合了领域知识，通过创新训练方法解决了数据稀缺和知识复杂性挑战，为模拟电路设计领域提供了有价值的工具。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了AnalogSeeker，这是一个面向模拟电路设计的开源基础语言模型的尝试，旨在整合领域知识并提供设计辅助。为了克服该领域数据的稀缺性，我们采用了一种基于模拟电路领域知识框架的语料库收集策略。系统整理和清理了相关子领域的高质量、易获取的教科书，形成文本领域语料库。为了解决模拟电路知识的复杂性，我们引入了粒度领域知识蒸馏方法。将原始、未标记的领域语料库分解为典型的粒度学习节点，通过多智能体框架将非结构化文本中嵌入的隐式知识蒸馏为具有详细推理过程的问答数据对，生成用于微调的细粒度可学习数据集。为了解决训练模拟电路基础模型中未探索的挑战，我们通过理论分析和实验验证探索并分享了我们的训练方法。我们最终建立了以微调为中心的训练范式，定制和实现了邻域自约束监督微调算法。这种方法通过约束训练前后模型输出分布之间的扰动幅度来提高训练效果。在实践中，我们训练Qwen2.5-32B-Instruct模型获得AnalogSeeker，在AMSBench-TQA上达到85.04%的准确率，比原始模型提高了15.67个百分点，并与主流商业模型具有竞争力。此外，AnalogSeeker在下游运算放大器设计任务中也显示出有效性。AnalogSeeker已在https://huggingface.co/analogllm/analogseeker开源供研究使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose AnalogSeeker, an effort toward an open-sourcefoundation language model for analog circuit design, with the aim ofintegrating domain knowledge and giving design assistance. To overcome thescarcity of data in this field, we employ a corpus collection strategy based onthe domain knowledge framework of analog circuits. High-quality, accessibletextbooks across relevant subfields are systematically curated and cleaned intoa textual domain corpus. To address the complexity of knowledge of analogcircuits, we introduce a granular domain knowledge distillation method. Raw,unlabeled domain corpus is decomposed into typical, granular learning nodes,where a multi-agent framework distills implicit knowledge embedded inunstructured text into question-answer data pairs with detailed reasoningprocesses, yielding a fine-grained, learnable dataset for fine-tuning. Toaddress the unexplored challenges in training analog circuit foundation models,we explore and share our training methods through both theoretical analysis andexperimental validation. We finally establish a fine-tuning-centric trainingparadigm, customizing and implementing a neighborhood self-constrainedsupervised fine-tuning algorithm. This approach enhances training outcomes byconstraining the perturbation magnitude between the model's outputdistributions before and after training. In practice, we train theQwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04%accuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark,with a 15.67% point improvement over the original model and is competitive withmainstream commercial models. Furthermore, AnalogSeeker also showseffectiveness in the downstream operational amplifier design task. AnalogSeekeris open-sourced at https://huggingface.co/analogllm/analogseeker for researchuse.</description>
      <author>example@mail.com (Zihao Chen, Ji Zhuang, Jinyi Shen, Xiaoyue Ke, Xinyi Yang, Mingjie Zhou, Zhuoyao Du, Xu Yan, Zhouyang Wu, Zhenyu Xu, Jiangli Huang, Li Shang, Xuan Zeng, Fan Yang)</author>
      <guid isPermaLink="false">2508.10409v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models</title>
      <link>http://arxiv.org/abs/2508.10349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, Submitted to INFOCOM2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FlexP-SFL的灵活个性化分割联邦学习方法，使客户端能在有限计算资源条件下进行协作学习同时保持个性化目标&lt;h4&gt;背景&lt;/h4&gt;微调基础模型对个性化下游任务性能至关重要，但客户端数据有限和数据分布异质性阻碍了有效的协作学习&lt;h4&gt;目的&lt;/h4&gt;解决客户端数据有限和数据分布异质性导致的协作问题，提出一种使客户端能够参与协作学习同时保持个性化目标的范式&lt;h4&gt;方法&lt;/h4&gt;提出FlexP-SFL方法，基于分割学习，允许客户端根据资源约束在本地训练模型部分并卸载剩余部分到服务器，同时提出对齐策略提高模型在全局数据上的性能&lt;h4&gt;主要发现&lt;/h4&gt;FlexP-SFL在个性化微调效率和最终准确性方面优于基线模型&lt;h4&gt;结论&lt;/h4&gt;FlexP-SFL是在有限和异构计算资源条件下实现个性化协作学习的有效方法&lt;h4&gt;翻译&lt;/h4&gt;微调基础模型对于在个性化下游任务上获得优异性能至关重要，与使用预训练模型相比更为有效。协作学习可以利用本地客户端的数据集进行微调，但客户端数据有限和数据分布异质性阻碍了有效协作。为应对这一挑战，我们提出了一种灵活的个性化联邦学习范式，使客户端能够在参与协作学习的同时保持个性化目标。考虑到客户端上有限且异构的计算资源，我们引入了灵活的个性化分割联邦学习（FlexP-SFL）。基于分割学习，FlexP-SFL允许每个客户端根据资源约束在本地训练模型的一部分，同时将剩余部分卸载到服务器。此外，我们还提出了一种对齐策略，以提高个性化模型在全局数据上的性能。实验结果表明，FlexP-SFL在个性化微调效率和最终准确性方面优于基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning foundation models is critical for superior performance onpersonalized downstream tasks, compared to using pre-trained models.Collaborative learning can leverage local clients' datasets for fine-tuning,but limited client data and heterogeneous data distributions hinder effectivecollaboration. To address the challenge, we propose a flexible personalizedfederated learning paradigm that enables clients to engage in collaborativelearning while maintaining personalized objectives. Given the limited andheterogeneous computational resources available on clients, we introduce\textbf{flexible personalized split federated learning (FlexP-SFL)}. Based onsplit learning, FlexP-SFL allows each client to train a portion of the modellocally while offloading the rest to a server, according to resourceconstraints. Additionally, we propose an alignment strategy to improvepersonalized model performance on global data. Experimental results show thatFlexP-SFL outperforms baseline models in personalized fine-tuning efficiencyand final accuracy.</description>
      <author>example@mail.com (Tianjun Yuan, Jiaxiang Geng, Pengchao Han, Xianhao Chen, Bing Luo)</author>
      <guid isPermaLink="false">2508.10349v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Improving Learning of New Diseases through Knowledge-Enhanced Initialization for Federated Adapter Tuning</title>
      <link>http://arxiv.org/abs/2508.10299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了联邦知识增强初始化（FedKEI）框架，通过跨客户端和跨任务的知识转移，为医疗联邦学习中的基础模型适配器调整提供有信息的初始值，以帮助医疗机构快速适应新任务或疾病。&lt;h4&gt;背景&lt;/h4&gt;在医疗保健领域，联邦学习被广泛采用以实现隐私保护的协作。随着大型基础模型展现出强大能力，通过成本高效的适配器调整将基础模型应用于联邦学习已成为流行方法。然而，在快速变化的医疗环境中，各机构需要快速适应新疾病。&lt;h4&gt;目的&lt;/h4&gt;开发一种新框架，利用过去的知识来生成适配器调整的初始值，使医疗机构能够更有效地学习新任务或适应新疾病。&lt;h4&gt;方法&lt;/h4&gt;FedKEI首先在服务器端进行全局聚类以跨任务泛化知识，然后优化聚类间和聚类内的权重以个性化知识转移。采用双层优化方案，共同学习客户端间的全局簇内权重，并优化针对每个客户端任务目标的局部跨簇权重。&lt;h4&gt;主要发现&lt;/h4&gt;在三个不同模态的基准数据集（皮肤病学、胸部X光和视网膜OCT）上的实验表明，FedKEI在适应新疾病方面优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;FedKEI框架有效地解决了医疗联邦学习中快速适应新任务的需求，通过知识转移和权重优化提高了性能，为医疗机构的协作学习提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;在医疗保健领域，联邦学习（FL）是一种被广泛采用的框架，使医疗机构能够进行隐私保护的协作。随着大型基础模型（FMs）展现出令人印象深刻的能力，通过成本高效的适配器调整在FL中使用FMs已成为一种流行方法。鉴于快速变化的医疗环境，对于各个客户端来说，在调整适配器的同时借鉴过去的经验来快速适应新任务或疾病至关重要。在这项工作中，我们引入了联邦知识增强初始化（FedKEI），这是一个新框架，利用跨客户端和跨任务的知识转移，为学习新任务的适配器生成有信息的初始值。FedKEI首先在服务器端进行全局聚类过程，跨任务泛化知识，然后优化聚类间的权重（跨簇权重）和每个聚类内的权重（簇内权重），以个性化每个新任务的知识转移。为了促进跨簇权重和簇内权重的更有效学习，我们采用双层优化方案，共同学习客户端间的全局簇内权重，并优化针对每个客户端任务目标的局部跨簇权重。在三个不同模态的基准数据集（包括皮肤病学、胸部X光和视网膜OCT）上的广泛实验证明了FedKEI在适应新疾病方面比最先进方法具有优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In healthcare, federated learning (FL) is a widely adopted framework thatenables privacy-preserving collaboration among medical institutions. With largefoundation models (FMs) demonstrating impressive capabilities, using FMs in FLthrough cost-efficient adapter tuning has become a popular approach. Given therapidly evolving healthcare environment, it is crucial for individual clientsto quickly adapt to new tasks or diseases by tuning adapters while drawing uponpast experiences. In this work, we introduce Federated Knowledge-EnhancedInitialization (FedKEI), a novel framework that leverages cross-client andcross-task transfer from past knowledge to generate informed initializationsfor learning new tasks with adapters. FedKEI begins with a global clusteringprocess at the server to generalize knowledge across tasks, followed by theoptimization of aggregation weights across clusters (inter-cluster weights) andwithin each cluster (intra-cluster weights) to personalize knowledge transferfor each new task. To facilitate more effective learning of the inter- andintra-cluster weights, we adopt a bi-level optimization scheme thatcollaboratively learns the global intra-cluster weights across clients andoptimizes the local inter-cluster weights toward each client's task objective.Extensive experiments on three benchmark datasets of different modalities,including dermatology, chest X-rays, and retinal OCT, demonstrate FedKEI'sadvantage in adapting to new diseases compared to state-of-the-art methods.</description>
      <author>example@mail.com (Danni Peng, Yuan Wang, Kangning Cai, Peiyan Ning, Jiming Xu, Yong Liu, Rick Siow Mong Goh, Qingsong Wei, Huazhu Fu)</author>
      <guid isPermaLink="false">2508.10299v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets</title>
      <link>http://arxiv.org/abs/2508.10256v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是关于裂缝检测领域的综述研究，重点分析了深度学习在该领域的最新发展趋势，包括学习范式转变、泛化能力提升和数据集获取多样化等方面。作者还介绍了一个新的3D裂缝数据集，并进行了基准测试实验。&lt;h4&gt;背景&lt;/h4&gt;裂缝检测在土木基础设施（如路面、建筑物等）的检查中起着关键作用。近年来，深度学习显著推进了这一领域的发展。尽管该领域已有众多技术和综述论文，但新兴趋势正在重塑这一领域。&lt;h4&gt;目的&lt;/h4&gt;这篇综述旨在系统分析裂缝检测领域的最新趋势，突出代表性工作。同时，作者引入了一个使用3D激光扫描收集的新数据集3DCrack，以支持未来研究，并对常用的深度学习方法（包括最近的基础模型）进行了广泛的基准测试实验，建立了基线。&lt;h4&gt;方法&lt;/h4&gt;作者通过系统性综述方法分析了裂缝检测领域的最新趋势。他们收集并标注了一个使用3D激光扫描的新数据集3DCrack，并进行了广泛的基准测试实验，评估了常用的深度学习方法。&lt;h4&gt;主要发现&lt;/h4&gt;作者发现了裂缝检测领域的几个关键趋势：学习范式的转变（从完全监督学习转向半监督、弱监督、无监督、少样本、领域适应和微调基础模型）、泛化能力的提升（从单数据集性能转向跨数据集评估）以及数据集获取的多样化（从RGB图像转向专业传感器数据）。&lt;h4&gt;结论&lt;/h4&gt;该研究为基于深度学习的裂缝检测领域的演变方法和未来方向提供了见解，并建立了一个新的基准数据集和实验结果，支持了未来的研究工作。&lt;h4&gt;翻译&lt;/h4&gt;裂缝检测在土木基础设施（包括路面、建筑物等的检查）中起着至关重要的作用，近年来深度学习显著推进了这一领域的发展。尽管该领域存在众多技术和综述论文，但新兴趋势正在重塑这一领域。这些转变包括学习范式的转变（从完全监督学习转向半监督、弱监督、无监督、少样本、领域适应和微调基础模型）、泛化能力的提升（从单数据集性能转向跨数据集评估）以及数据集获取的多样化（从RGB图像转向专业传感器数据）。在本综述中，我们系统分析了这些趋势并突出了代表性工作。此外，我们介绍了一个使用3D激光扫描收集的新数据集3DCrack，以支持未来研究，并对常用的深度学习方法（包括最近的基础模型）进行了广泛的基准测试实验，建立了基线。我们的发现为基于深度学习的裂缝检测领域的演变方法和未来方向提供了见解。项目页面：https://github.com/nantonzhang/Awesome-Crack-Detection&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决深度学习在裂缝检测领域中的系统性分析问题，特别是三个关键趋势：学习范式的转变、泛化能力的提升和数据集的多样化。这个问题在现实中非常重要，因为裂缝检测对土木基础设施安全至关重要，及时准确的识别可以预防性维护，降低维修成本，防止严重结构故障。传统方法存在劳动密集、耗时和主观错误等问题，而深度学习方法虽提高了性能，但仍面临标注数据需求量大和泛化能力有限等挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者作为综述论文，没有提出新方法，而是通过系统性地分析裂缝检测领域的最新趋势。作者借鉴了大量现有工作，总结了之前的综述论文，并指出它们未全面分析三个关键趋势。作者还借鉴了计算机视觉领域的其他技术，如CLIP和SAM等基础模型，以及PEFT技术。作者引入了新数据集3DCrack，并对常用深度学习方法进行了基准测试实验。整体思路是分类分析、系统综述和实验验证相结合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 作为综述论文，没有单一的核心思想，但整体核心思想是对深度学习裂缝检测领域的最新趋势进行系统性的分类和分析。论文结构包括：背景介绍、学习范式和泛化能力分析、数据集综述、实验和发现、开放挑战和结论。重点分析了七种学习范式（监督、半监督、弱监督、领域适应、少样本、无监督和基础模型），讨论了它们的特点、优缺点和泛化能力，并介绍了各类数据集及其特点。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 系统性分析裂缝检测领域的三个关键趋势；2) 引入新数据集3DCrack，具有更高分辨率和多样化路面条件；3) 对常用深度学习方法进行基准测试，包括基础模型；4) 总结当前挑战和未来机会。相比之前工作，不同之处在于采用层次化结构全面分析三个关键趋势，而非仅按任务类型或流水线组织内容；涵盖最新学习范式如基础模型和PEFT技术；强调跨数据集评估和泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统分析深度学习裂缝检测的最新趋势、引入新数据集并进行基准测试，为裂缝检测领域提供了全面的视角和未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crack detection plays a crucial role in civil infrastructures, includinginspection of pavements, buildings, etc., and deep learning has significantlyadvanced this field in recent years. While numerous technical and review papersexist in this domain, emerging trends are reshaping the landscape. These shiftsinclude transitions in learning paradigms (from fully supervised learning tosemi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptationand fine-tuning foundation models), improvements in generalizability (fromsingle-dataset performance to cross-dataset evaluation), and diversification indataset reacquisition (from RGB images to specialized sensor-based data). Inthis review, we systematically analyze these trends and highlightrepresentative works. Additionally, we introduce a new dataset collected with3D laser scans, 3DCrack, to support future research and conduct extensivebenchmarking experiments to establish baselines for commonly used deep learningmethodologies, including recent foundation models. Our findings provideinsights into the evolving methodologies and future directions in deeplearning-based crack detection. Project page:https://github.com/nantonzhang/Awesome-Crack-Detection</description>
      <author>example@mail.com (Xinan Zhang, Haolin Wang, Yung-An Hsieh, Zhongyu Yang, Anthony Yezzi, Yi-Chang Tsai)</author>
      <guid isPermaLink="false">2508.10256v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Meta-Metrics and Best Practices for System-Level Inference Performance Benchmarking</title>
      <link>http://arxiv.org/abs/2508.10251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了FMwork，一种针对基础模型（如大语言模型）推理性能基准测试的全面且系统化的方法，通过元指标、参数选择和战略性成本-性能评估三个关键组件，能够在不牺牲准确性的情况下显著提高实验效率。&lt;h4&gt;背景&lt;/h4&gt;基准测试基础模型（如大语言模型）的推理性能（速度）需要理解硬件和软件组件之间复杂的交互作用，但评估每一种可能的测试配置是不切实际、不可行且不必要的。&lt;h4&gt;目的&lt;/h4&gt;解决基础模型推理性能基准测试中面临的实验配置空间过大问题，提供一种高效且准确的测试方法。&lt;h4&gt;方法&lt;/h4&gt;提出FMwork框架，包含三个关键组成部分：1)元指标，考虑基准测试所花费的时间和资源以及结果的相对准确性；2)参数选择策略；3)战略性成本-性能评估方法。该框架通过元指标将完整实验空间表征为可管理的部分。&lt;h4&gt;主要发现&lt;/h4&gt;使用FMwork框架，与真实情况相比，运行实验扫描可提高24倍（加速和/或资源节省）。即使将实验输出大小从1024减少到128个标记，对于使用Llama 3.1 8B模型的评估仍可获得2.7倍的增益，同时保持96.6%的准确性。&lt;h4&gt;结论&lt;/h4&gt;FMwork提供了一种高效的基础模型推理性能基准测试方法，通过智能选择实验参数和评估策略，能够在大幅减少实验工作量的同时保持高准确性。&lt;h4&gt;翻译&lt;/h4&gt;基准测试基础模型（如大语言模型）的推理性能（速度）需要导航广泛的实验领域，以理解硬件和软件组件之间复杂的相互作用。然而，评估每一种可能的测试配置是不切实际、不可行且不必要的。为了应对这一挑战，我们引入了FMwork，这是一种创建受控测试环境的全面且系统化的方法，能够准确反映和表征性能。FMwork包含一组基准测试最佳实践，具有三个关键组成部分：1)元指标，2)参数选择，和3)战略性成本-性能评估。元指标考虑了基准测试所花费的时间和资源，以及与更大量测量结果相比的相对准确性，代表了完整的实验空间。FMwork将元指标付诸实践，并为参数选择和成本-性能分析提供高效策略。使用该框架，我们展示了与真实情况相比，运行实验扫描可提高24倍（加速和/或资源节省）。即使已经将实验子集作为参考点（使用批量大小的2的幂），将实验输出大小从1024减少到128个标记，对于使用Llama 3.1 8B模型的评估仍可获得2.7倍的增益，同时保持96.6%的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Benchmarking inference performance (speed) of Foundation Models such as LargeLanguage Models (LLM) involves navigating a vast experimental landscape tounderstand the complex interactions between hardware and software components.However, evaluating every possible test configuration is impractical,unfeasible and unnecessary. To address this challenge, we introduce FMwork, acomprehensive and methodical approach to creating a controlled testingenvironment that accurately reflects and characterizes performance. FMworkcomprises a set of benchmkaring best practices with three key components: 1)meta-metrics, 2) parameter selection, and 3) strategic cost-performanceevaluation. Meta-metrics account for time and resources spent on benchmarkingand the relative accuracy of the results compared to a larger body ofmeasurements, representing the complete experimental space. FMworkoperationalizes the meta-metrics and provides efficient strategies forparameter selection and cost-performance analysis. Using the framework, we showup to 24x improvement (speedup and/or resource savings) running sweeps ofexperiments compared to the ground truth. Even already considering a subset ofexperiments as reference point (using the power of two for batch sizes),reducing experimental output size from 1024 to 128 tokens yields another 2.7xgain while keeping 96.6% accuracy for an evaluation using Llama 3.1 8B model.</description>
      <author>example@mail.com (Shweta Salaria, Zhuoran Liu, Nelson Mimura Gonzalez)</author>
      <guid isPermaLink="false">2508.10251v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics</title>
      <link>http://arxiv.org/abs/2508.10232v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究人员开发了CellSymphony，一个多模态框架，整合了Xenium空间转录组学数据和组织学图像，实现了亚细胞分辨率的复杂肿瘤组织分析，准确识别细胞类型并发现不同癌症的微环境生态位。&lt;h4&gt;背景&lt;/h4&gt;Xenium是一种新型空间转录组学平台，可对复杂肿瘤组织进行亚细胞分辨率分析。尽管组织学图像富含形态信息，但提取稳健的细胞级特征并将其与空间转录组学数据整合仍是关键挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个灵活的多模态框架，整合Xenium转录组学数据和形态学信息，克服细胞特征提取和数据整合的挑战。&lt;h4&gt;方法&lt;/h4&gt;介绍CellSymphony，一个利用来自Xenium转录组学和组织学图像的基础模型衍生嵌入的多模态框架，在真实单细胞分辨率水平上学习融合空间基因表达和形态上下文的联合表示。&lt;h4&gt;主要发现&lt;/h4&gt;CellSymphony实现了准确的细胞类型注释，并在三种癌症类型中发现了不同的微环境生态位。&lt;h4&gt;结论&lt;/h4&gt;这项工作强调了基础模型和多模态融合在解析复杂组织生态系统中细胞的生理和表型编排方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Xenium是一种新的空间转录组学平台，能够对复杂肿瘤组织进行亚细胞分辨率的分析。尽管组织学图像中富含形态信息，但提取稳健的细胞级特征并将其与空间转录组学数据整合仍然是一个关键挑战。我们介绍了CellSymphony，一个灵活的多模态框架，利用来自Xenium转录组学和组织学图像的基础模型衍生嵌入，在真实的单细胞分辨率水平上。通过学习融合空间基因表达和形态上下文的联合表示，CellSymphony实现了准确的细胞类型注释，并在三种癌症类型中发现了不同的微环境生态位。这项工作强调了基础模型和多模态融合在解析复杂组织生态系统中细胞的生理和表型编排方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Xenium, a new spatial transcriptomics platform, enablessubcellular-resolution profiling of complex tumor tissues. Despite the richmorphological information in histology images, extracting robust cell-levelfeatures and integrating them with spatial transcriptomics data remains acritical challenge. We introduce CellSymphony, a flexible multimodal frameworkthat leverages foundation model-derived embeddings from both Xeniumtranscriptomic profiles and histology images at true single-cell resolution. Bylearning joint representations that fuse spatial gene expression withmorphological context, CellSymphony achieves accurate cell type annotation anduncovers distinct microenvironmental niches across three cancer types. Thiswork highlights the potential of foundation models and multimodal fusion fordeciphering the physiological and phenotypic orchestration of cells withincomplex tissue ecosystems.</description>
      <author>example@mail.com (Paul H. Acosta, Pingjun Chen, Simon P. Castillo, Maria Esther Salvatierra, Yinyin Yuan, Xiaoxi Pan)</author>
      <guid isPermaLink="false">2508.10232v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs</title>
      <link>http://arxiv.org/abs/2508.10180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为For-Value的数据估值框架，用于量化单个训练样本对大型语言模型和视觉语言模型的影响，该方法仅通过前向传递即可高效计算影响分数，避免了传统方法中昂贵的梯度计算。&lt;h4&gt;背景&lt;/h4&gt;量化单个训练样本的影响对于提高大型语言模型和视觉语言模型的透明度和责任感至关重要。然而，现有的数据估值方法通常依赖于Hessian信息或模型重新训练，对于十亿参数模型来说计算成本过高，难以实际应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展且高效的数据估值方法，能够准确量化训练样本对大型语言模型和视觉语言模型的影响，同时降低计算复杂度。&lt;h4&gt;方法&lt;/h4&gt;作者提出了For-Value，一种仅前向传递的数据估值框架。该方法利用现代基础模型的丰富表示，通过基于单个前向传递的简单闭式表达式计算影响分数，消除了对昂贵梯度计算的需求。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明，For-Value通过捕捉训练和验证样本在隐藏表示和预测误差中的对齐，能够准确估计每个样本的影响。大量实验证明，For-Value在识别有影响力的微调示例和有效检测错误标记数据方面匹配或优于基于梯度的基线方法。&lt;h4&gt;结论&lt;/h4&gt;For-Value为大型语言模型和视觉语言模型提供了一种高效且准确的数据估值方法，通过仅使用前向传递即可实现影响估计，大大降低了计算复杂度，同时保持了与更复杂方法相当或更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;量化单个训练样本的影响对于提高大型语言模型和视觉语言模型的透明度和责任感至关重要。然而，现有的数据估值方法通常依赖于Hessian信息或模型重新训练，对于十亿参数模型来说计算成本过高。在这项工作中，我们引入了For-Value，一种仅前向传递的数据估值框架，能够为大型语言模型和视觉语言模型提供可扩展且高效的影响估计。通过利用现代基础模型的丰富表示，For-Value仅基于单个前向传递的简单闭式表达式计算影响分数，从而消除了昂贵梯度计算的需求。我们的理论分析表明，For-Value通过捕捉训练和验证样本在隐藏表示和预测误差中的对齐，准确估计了每个样本的影响。大量实验表明，For-Value在识别有影响力的微调示例和有效检测错误标记数据方面匹配或优于基于梯度的基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantifying the influence of individual training samples is essential forenhancing the transparency and accountability of large language models (LLMs)and vision-language models (VLMs). However, existing data valuation methodsoften rely on Hessian information or model retraining, making themcomputationally prohibitive for billion-parameter models. In this work, weintroduce For-Value, a forward-only data valuation framework that enablesscalable and efficient influence estimation for both LLMs and VLMs. Byleveraging the rich representations of modern foundation models, For-Valuecomputes influence scores using a simple closed-form expression based solely ona single forward pass, thereby eliminating the need for costly gradientcomputations. Our theoretical analysis demonstrates that For-Value accuratelyestimates per-sample influence by capturing alignment in hidden representationsand prediction errors between training and validation samples. Extensiveexperiments show that For-Value matches or outperforms gradient-based baselinesin identifying impactful fine-tuning examples and effectively detectingmislabeled data.</description>
      <author>example@mail.com (Wenlong Deng, Jiaming Zhang, Qi Zeng, Christos Thrampoulidis, Boying Gong, Xiaoxiao Li)</author>
      <guid isPermaLink="false">2508.10180v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Pre-trained Transformer-models using chronic invasive electrophysiology for symptom decoding without patient-individual training</title>
      <link>http://arxiv.org/abs/2508.10160v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于预训练大规模基础模型的神经解码方法，用于病理和生理状态估计，实现患者个体化的闭环神经调节治疗。&lt;h4&gt;背景&lt;/h4&gt;神经解码技术可以用于患者个体化的闭环神经调节治疗，但通常需要患者个体化训练。&lt;h4&gt;目的&lt;/h4&gt;探索预训练的大规模基础模型在无需患者个体化训练的情况下进行状态估计的潜力。&lt;h4&gt;方法&lt;/h4&gt;在超过24天的慢性深度脑刺激记录数据上训练基础模型，采用30分钟的扩展上下文窗口，并提出了针对神经电生理数据优化的预训练损失函数，纠正了常见掩码自编码器损失函数由于1/f幂律导致的频率偏差。&lt;h4&gt;主要发现&lt;/h4&gt;使用留一法交叉验证成功解码了帕金森病症状，无需患者个体化训练。&lt;h4&gt;结论&lt;/h4&gt;预训练的大规模基础模型可以有效用于神经解码，无需患者个体化训练，为患者个体化的闭环神经调节治疗提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;神经解码病理和生理状态可以实现对患者个体化的闭环神经调节治疗。最近，预训练大规模基础模型的进展提供了无需患者个体化训练即可进行通用状态估计的可能性。我们展示了一个在超过24天的慢性深度脑刺激记录上训练的基础模型。遵循长时间尺度的症状波动，我们突显了30分钟的扩展上下文窗口。我们提出了针对神经电生理数据优化的预训练损失函数，纠正了常见掩码自编码器损失函数由于1/f幂律导致的频率偏差。我们在一个下游任务中展示了使用留一法交叉验证解码帕金森病症状，无需患者个体化训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural decoding of pathological and physiological states can enablepatient-individualized closed-loop neuromodulation therapy. Recent advances inpre-trained large-scale foundation models offer the potential for generalizedstate estimation without patient-individual training. Here we present afoundation model trained on chronic longitudinal deep brain stimulationrecordings spanning over 24 days. Adhering to long time-scale symptomfluctuations, we highlight the extended context window of 30 minutes. Wepresent an optimized pre-training loss function for neural electrophysiologicaldata that corrects for the frequency bias of common masked auto-encoder lossfunctions due to the 1-over-f power law. We show in a downstream task thedecoding of Parkinson's disease symptoms with leave-one-subject-outcross-validation without patient-individual training.</description>
      <author>example@mail.com (Timon Merk, Saeed Salehi, Richard M. Koehler, Qiming Cui, Maria Olaru, Amelia Hahn, Nicole R. Provenza, Simon Little, Reza Abbasi-Asl, Phil A. Starr, Wolf-Julian Neumann)</author>
      <guid isPermaLink="false">2508.10160v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model</title>
      <link>http://arxiv.org/abs/2508.10110v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多模态学习方法，用于人脸识别系统中的形变攻击检测，能够提供文本描述并进行零样本评估。&lt;h4&gt;背景&lt;/h4&gt;形变攻击检测已成为人脸识别系统确保可靠验证场景的重要组成部分。&lt;h4&gt;目的&lt;/h4&gt;提出一种多模态学习方法，为形变攻击检测提供文本描述。&lt;h4&gt;方法&lt;/h4&gt;使用对比语言-图像预训练模型进行零样本评估，分析十种不同文本提示（包括长短文本），在公开面部生物特征数据集开发的面部形变数据集上进行实验，评估五种不同形变生成技术（三种不同媒介）。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架通过零样本评估不仅能实现可推广的形变攻击检测，还能预测最相关的文本片段。&lt;h4&gt;结论&lt;/h4&gt;多模态学习方法在形变攻击检测中具有良好的性能，能够提供文本描述并实现零样本评估。&lt;h4&gt;翻译&lt;/h4&gt;形变攻击检测已成为人脸识别系统确保可靠验证场景的重要组成部分。在本文中，我们提出了一种多模态学习方法，可以为形变攻击检测提供文本描述。我们首先展示了使用对比语言-图像预训练模型对所提出框架进行零样本评估，不仅可以实现可推广的形变攻击检测，还可以预测最相关的文本片段。我们对十种不同的文本提示进行了广泛分析，包括长短文本提示。这些提示是根据人类可理解的文本片段设计的。在公开可用的面部生物特征数据集开发的面部形变数据集上进行了大量实验。我们评估了最先进的预训练神经网络与所提出的框架，在五种不同的形变生成技术（三种不同媒介）的零样本评估中的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-93694-4_14&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Morphing attack detection has become an essential component of facerecognition systems for ensuring a reliable verification scenario. In thispaper, we present a multimodal learning approach that can provide a textualdescription of morphing attack detection. We first show that zero-shotevaluation of the proposed framework using Contrastive Language-ImagePretraining (CLIP) can yield not only generalizable morphing attack detection,but also predict the most relevant text snippet. We present an extensiveanalysis of ten different textual prompts that include both short and longtextual prompts. These prompts are engineered by considering the humanunderstandable textual snippet. Extensive experiments were performed on a facemorphing dataset that was developed using a publicly available face biometricdataset. We present an evaluation of SOTA pre-trained neural networks togetherwith the proposed framework in the zero-shot evaluation of five differentmorphing generation techniques that are captured in three different mediums.</description>
      <author>example@mail.com (Sushrut Patwardhan, Raghavendra Ramachandra, Sushma Venkatesh)</author>
      <guid isPermaLink="false">2508.10110v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>DINOv3</title>
      <link>http://arxiv.org/abs/2508.10104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DINOv3是一个通过自监督学习实现的视觉基础模型，通过扩展数据集和模型规模、引入Gram锚定方法以及应用后处理策略，在各种视觉任务上取得了卓越性能，无需微调就超过了专业化的最先进技术。&lt;h4&gt;背景&lt;/h4&gt;自监督学习有望消除人工数据标注的需求，使模型能够轻松扩展到大规模数据集和更大的架构。这种训练范式可以从各种来源学习视觉表示，使用单一算法处理从自然图像到航空图像等多种数据类型。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的视觉基础模型，实现自监督学习的愿景，在各种视觉任务上达到最先进性能，同时保持对不同数据类型和任务的适用性。&lt;h4&gt;方法&lt;/h4&gt;通过精心准备数据、设计和优化来扩展数据集和模型规模；引入Gram锚定方法解决密集特征图在长期训练中的退化问题；应用后处理策略增强模型在分辨率、模型大小和与文本对齐方面的灵活性。&lt;h4&gt;主要发现&lt;/h4&gt;DINOv3产生了高质量的密集特征，在各种视觉任务上取得了卓越的性能，显著超过了之前的自监督和弱监督基础模型。该模型无需微调就在广泛的设置中优于专业化的最先进技术。&lt;h4&gt;结论&lt;/h4&gt;DINOv3是一个通用的视觉基础模型，代表了实现自监督学习愿景的重要里程碑。通过提供可扩展的解决方案，DINOv3视觉模型套件旨在推进各种任务和数据上的最先进技术，满足不同的资源限制和部署场景需求。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习有望消除对人工数据标注的需求，使模型能够轻松扩展到大规模数据集和更大的架构。由于不针对特定任务或领域，这种训练范式有可能从各种来源（从自然图像到航空图像）学习视觉表示——使用单一算法。本技术报告介绍了DINOv3，这是通过利用简单而有效的策略实现这一愿景的重要里程碑。首先，我们通过精心准备数据、设计和优化，利用扩展数据集和模型规模的优势。其次，我们引入了一种称为Gram锚定的新方法，它有效地解决了在长期训练过程中密集特征图退化的已知但尚未解决的问题。最后，我们应用后处理策略，进一步增强我们的模型在分辨率、模型大小和与文本对齐方面的灵活性。因此，我们提出了一个通用的视觉基础模型，在广泛的设置中优于专业化的最先进技术，无需微调。DINOv3产生高质量的密集特征，在各种视觉任务上取得了卓越的性能，显著超过了之前的自监督和弱监督基础模型。我们还分享了DINOv3视觉模型套件，旨在通过为各种资源限制和部署场景提供可扩展的解决方案，在广泛的任务和数据上推进最先进的技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning holds the promise of eliminating the need for manualdata annotation, enabling models to scale effortlessly to massive datasets andlarger architectures. By not being tailored to specific tasks or domains, thistraining paradigm has the potential to learn visual representations fromdiverse sources, ranging from natural to aerial images -- using a singlealgorithm. This technical report introduces DINOv3, a major milestone towardrealizing this vision by leveraging simple yet effective strategies. First, weleverage the benefit of scaling both dataset and model size by careful datapreparation, design, and optimization. Second, we introduce a new method calledGram anchoring, which effectively addresses the known yet unsolved issue ofdense feature maps degrading during long training schedules. Finally, we applypost-hoc strategies that further enhance our models' flexibility with respectto resolution, model size, and alignment with text. As a result, we present aversatile vision foundation model that outperforms the specialized state of theart across a broad range of settings, without fine-tuning. DINOv3 produceshigh-quality dense features that achieve outstanding performance on variousvision tasks, significantly surpassing previous self- and weakly-supervisedfoundation models. We also share the DINOv3 suite of vision models, designed toadvance the state of the art on a wide spectrum of tasks and data by providingscalable solutions for diverse resource constraints and deployment scenarios.</description>
      <author>example@mail.com (Oriane Siméoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothée Darcet, Théo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Hervé Jégou, Patrick Labatut, Piotr Bojanowski)</author>
      <guid isPermaLink="false">2508.10104v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation</title>
      <link>http://arxiv.org/abs/2508.09626v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAD-Splat的新型3D航空场景语义分割方法，通过引入高斯点丢弃模块和高置信度伪标签生成流程，有效解决了传统方法难以处理的语义模糊问题，提高了分割准确性和一致性。&lt;h4&gt;背景&lt;/h4&gt;传统方法难以处理航空图像中由尺度变化和结构遮挡引起的语义模糊问题，这限制了分割的准确性和一致性。&lt;h4&gt;目的&lt;/h4&gt;解决3D航空场景语义分割中的语义模糊问题，提高分割准确性和一致性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为SAD-Splat的新型3D-AVS-SS方法，引入了一个高斯点丢弃模块，该模块结合了语义置信度估计和基于Hard Concrete分布的可学习稀疏机制，有效消除了冗余和语义模糊的高斯点；同时集成了一个高置信度伪标签生成流程，利用2D基础模型在真实标签有限时增强监督。&lt;h4&gt;主要发现&lt;/h4&gt;SAD-Splat在分割准确性和表示紧凑性之间取得了良好的平衡，为3D航空场景理解提供了一种高效且可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明SAD-Splat在分割准确性和表示紧凑性方面表现优秀，为3D航空场景理解提供了高效可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在3D航空场景语义分割任务中，传统方法难以解决航空图像中由尺度变化和结构遮挡引起的语义模糊问题。这限制了它们的分割准确性和一致性。为了应对这些挑战，我们提出了一种名为SAD-Splat的新型3D航空场景语义分割方法。我们的方法引入了一个高斯点丢弃模块，该模块将语义置信度估计与基于Hard Concrete分布的可学习稀疏机制相结合。该模块有效消除了冗余和语义模糊的高斯点，提高了分割性能和表示紧凑性。此外，SAD-Splat集成了一个高置信度伪标签生成流程，它利用2D基础模型在真实标签有限时增强监督，从而进一步提高分割准确性。为了推进该领域的研究，我们引入了一个具有挑战性的基准数据集：3D航空语义，其中包含具有稀疏注释的多样化真实世界航空场景。实验结果表明，SAD-Splat在分割准确性和表示紧凑性之间取得了良好的平衡。它为3D航空场景理解提供了一种高效且可扩展的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D航空视角场景语义分割（3D-AVS-SS）中的两个核心挑战：一是航空图像中因尺度变化和结构遮挡导致的语义歧义问题，二是3D高斯溅射框架下模型在语义模糊区域产生冗余高斯点的问题。这个问题在现实中非常重要，因为3D-AVS-SS在土地使用监测、城市规划和灾害响应等多种遥感应用中起着关键作用，而当前方法难以处理航空图像特有的挑战，限制了分割的准确性和一致性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3D-AVS-SS任务中的语义歧义和高斯点冗余问题，发现2D基础模型直接应用于航空图像会产生噪声和不一致性。他们借鉴了3D高斯溅射（3DGS）框架、特征3DGS的语义嵌入方法、SAM和GeoRSCLIP用于生成置信度图，以及Hard Concrete分布实现可微分二值丢弃。基于这些现有工作，作者创新性地设计了语义感知丢弃模块结合语义置信度估计和可学习稀疏机制，以及高置信度伪标签生成管道，解决了现有方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自适应修剪冗余高斯点解决语义歧义和结构冗余问题，同时利用高置信度伪标签增强监督信号，实现更准确、更紧凑的3D航空场景语义分割。整体流程分为三阶段：1）预处理阶段使用SAM和GeoRSCLIP生成置信度图和可靠伪标签；2）训练阶段联合优化语义特征学习和高斯点丢弃，定期根据综合丢弃概率修剪高斯点；3）推理阶段使用优化后的3D高斯表示进行语义渲染和预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）语义感知的高斯点丢弃机制，结合语义置信度和可学习稀疏机制自适应修剪冗余点；2）高置信度伪标签生成管道，通过多指标过滤确保监督可靠性；3）构建了3D航空语义（3D-AS）基准数据集。相比之前工作，SAD-Splat不仅显式建模3D结构提供更好的多视图一致性，还通过自适应修剪解决了高斯点过饱和问题，使用高置信度监督减轻语义混淆，在复杂场景中表现更稳定且模型更紧凑（高斯点数量减少82%）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAD-Splat通过语义感知的高斯点丢弃机制和高置信度伪标签生成方法，有效解决了3D航空视角场景语义分割中的语义歧义和冗余表示问题，实现了更准确、更紧凑的3D场景理解，并为此领域提供了新的基准数据集。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),traditional methods struggle to address semantic ambiguity caused by scalevariations and structural occlusions in aerial images. This limits theirsegmentation accuracy and consistency. To tackle these challenges, we propose anovel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussianpoint drop module, which integrates semantic confidence estimation with alearnable sparsity mechanism based on the Hard Concrete distribution. Thismodule effectively eliminates redundant and semantically ambiguous Gaussianpoints, enhancing both segmentation performance and representation compactness.Furthermore, SAD-Splat incorporates a high-confidence pseudo-label generationpipeline. It leverages 2D foundation models to enhance supervision whenground-truth labels are limited, thereby further improving segmentationaccuracy. To advance research in this domain, we introduce a challengingbenchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diversereal-world aerial scenes with sparse annotations. Experimental resultsdemonstrate that SAD-Splat achieves an excellent balance between segmentationaccuracy and representation compactness. It offers an efficient and scalablesolution for 3D aerial scene understanding.</description>
      <author>example@mail.com (Xu Tang, Junan Jia, Yijing Wang, Jingjing Ma, Xiangrong Zhang)</author>
      <guid isPermaLink="false">2508.09626v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models</title>
      <link>http://arxiv.org/abs/2508.10770v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了视觉语言模型在空间物理推理方面的能力，发现当前模型表现不佳，主要原因是类人先验偏见和缺乏深度推理。通过监督微调和基于规则的强化学习改进Qwen2.5-VL-7B模型，显著提升了其空间物理推理能力，但模型在新物理场景中的泛化能力仍然有限。&lt;h4&gt;背景&lt;/h4&gt;空间物理推理是理解真实物理世界的基础能力，也是构建强大世界模型的关键步骤。虽然最近的视觉语言模型在多模态数学和纯空间理解等专业领域取得了显著进展，但它们在空间物理推理方面的能力在很大程度上仍未被探索。&lt;h4&gt;目的&lt;/h4&gt;对主流视觉语言模型进行全面诊断分析，揭示它们在空间物理推理任务上的表现不足，并提出改进方法以提升模型在这一关键任务上的能力。&lt;h4&gt;方法&lt;/h4&gt;采用监督微调随后基于规则的强化学习对Qwen2.5-VL-7B模型进行改进。&lt;h4&gt;主要发现&lt;/h4&gt;当前主流视觉语言模型在空间物理推理任务上表现不佳，这种表现不足主要归因于类人先验偏见和缺乏深度推理。通过提出的改进方法，模型的空间物理推理能力得到显著提升，超越了领先的专有模型，但模型在新物理场景中的泛化能力仍然有限。&lt;h4&gt;结论&lt;/h4&gt;尽管通过监督微调和基于规则的强化学习改进了模型的空间物理推理能力，但模型在新物理场景中的泛化能力仍然有限，这突显了在空间物理推理方面需要新方法的紧迫性。&lt;h4&gt;翻译&lt;/h4&gt;空间物理推理是理解真实物理世界的基础能力，是构建强大世界模型的关键步骤。虽然最近的视觉语言模型在多模态数学和纯空间理解等专业领域取得了显著进展，但它们在空间物理推理方面的能力在很大程度上仍未被探索。本文对主流视觉语言模型进行了全面的诊断分析，揭示当前模型在这一关键任务上表现不佳。进一步详细分析表明，这种表现不足主要归因于类人先验偏见和缺乏深度推理。为应对这些挑战，我们应用监督微调随后基于规则的强化学习对Qwen2.5-VL-7B进行改进，显著提升了空间物理推理能力，超越了领先的专有模型。尽管如此，尽管取得了这一成功，模型对新物理场景的泛化能力仍然有限——这突显了在空间物理推理方面需要新方法的紧迫性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉语言模型(VLMs)在空间物理推理能力不足的问题。这个问题很重要，因为空间物理推理是构建稳健世界模型的基础能力，对实现通用人工智能(AGI)至关重要；同时，强大的物理认知能力能直接提升具身AI任务的表现，帮助AI代理在现实世界中有效运作。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先对主流VLMs进行全面诊断分析，识别出模型在空间物理推理上的三种主要错误：视觉感知错误、物理推理错误和因果推理错误。同时发现模型存在类似人类的偏见(难度偏见和高度偏见)。基于这些发现，作者设计了两阶段训练方法：先进行监督微调(SFT)提供高质量思维链示例，再应用基于规则的强化学习(RL)优化模型。这种方法借鉴了多模态数学推理和空间理解领域的研究，但将其应用到空间物理推理这一新领域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过监督微调提供高质量的思维链推理示例作为强化学习的初始化，再使用基于规则的强化学习进一步优化模型，使其能更好地理解和应用空间物理推理规则。整体流程包括：1)数据准备，使用ShapeStacks训练集，其中部分样本蒸馏出思维链推理响应；2)监督微调阶段，使用LoRA方法对Qwen2.5-VL-7B进行40个epoch微调；3)强化学习阶段，使用veRL框架和GRPO算法进行8个epoch全参数微调，总奖励由格式奖励(10%)和答案奖励(90%)组成；4)在ShapeStacks测试集上评估改进后的模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次对VLMs在空间物理推理任务进行全面诊断分析；揭示模型存在类似人类的偏见和缺乏深度推理的问题；提出结合监督微调和强化学习的两阶段训练方法；验证该方法能显著提升模型性能并超过领先专有模型；系统测试模型在2D/3D、动态/静态和高度变化方面的泛化能力。相比之前工作，本文专注于涉及物理规律的空间推理而非纯空间理解或数学推理；不仅评估性能还深入分析推理行为和错误来源；采用SFT+RL组合方法而非单一训练范式；对改进模型进行全面泛化能力测试。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过诊断分析揭示了视觉语言模型在空间物理推理方面的局限性，并提出结合监督微调和强化学习的有效改进方法，显著提升了模型性能，同时指出了当前模型在泛化到新物理场景方面的挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatio-physical reasoning, a foundation capability for understanding the realphysics world, is a critical step towards building robust world models. Whilerecent vision language models (VLMs) have shown remarkable progress inspecialized domains like multimodal mathematics and pure spatial understanding,their capability for spatio-physical reasoning remains largely unexplored. Thispaper provides a comprehensive diagnostic analysis of mainstream VLMs,revealing that current models perform inadequately on this crucial task.Further detailed analysis shows that this underperformance is largelyattributable to biases caused by human-like prior and a lack of deep reasoning.To address these challenges, we apply supervised fine-tuning followed byrule-based reinforcement learning to Qwen2.5-VL-7B, resulting in significantimprovements in spatio-physical reasoning capabilities and surpassing leadingproprietary models. Nevertheless, despite this success, the model'sgeneralization to new physics scenarios remains limited -- underscoring thepressing need for new approaches in spatio-physical reasoning.</description>
      <author>example@mail.com (Tiancheng Han, Yunfei Gao, Yong Li, Wuzhou Yu, Qiaosheng Zhang, Wenqi Shao)</author>
      <guid isPermaLink="false">2508.10770v1</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>SIFThinker: Spatially-Aware Image Focus for Visual Reasoning</title>
      <link>http://arxiv.org/abs/2508.06259v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SIFThinker，一种空间感知的'图像思考'框架，通过交错增强深度的边界框和自然语言实现注意力校正和图像区域聚焦，解决了当前多模态大语言模型在复杂视觉任务中的挑战。&lt;h4&gt;背景&lt;/h4&gt;当前多模态大语言模型在复杂视觉任务(如空间理解、细粒度感知)方面仍面临重大挑战，先前方法虽尝试整合视觉推理，但未能有效利用空间提示进行注意力校正。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够模仿人类视觉感知的框架，通过注意力校正和图像区域聚焦来提升模型在空间理解和细粒度视觉感知方面的能力。&lt;h4&gt;方法&lt;/h4&gt;提出SIFThinker框架，包含两个主要贡献：1)反向扩展-前向推理策略，生成交错图像-思维链用于过程级监督，构建SIF-50K数据集；2)GRPO-SIF强化训练范式，将深度感知的视觉定位集成到统一推理流程中。&lt;h4&gt;主要发现&lt;/h4&gt;SIFThinker在空间理解和细粒度视觉感知任务上优于最先进方法，同时保持强大的通用能力，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过空间感知的思考和注意力校正机制，SIFThinker能够有效提升多模态大语言模型在复杂视觉任务中的表现。&lt;h4&gt;翻译&lt;/h4&gt;当前多模态大语言模型在复杂视觉任务(如空间理解、细粒度感知)方面仍面临重大挑战。先前方法尝试整合视觉推理，然而它们未能利用空间提示进行注意力校正，迭代地完善其对提示相关区域的关注。在本文中，我们介绍了SIFThinker，一种空间感知的'图像思考'框架，模仿人类视觉感知。具体而言，SIFThinker通过交错增强深度的边界框和自然语言，实现注意力校正和图像区域聚焦。我们的贡献有两方面：首先，我们引入反向扩展-前向推理策略，促进交错图像-思维链的生成，用于过程级监督，进而构建SIF-50K数据集。此外，我们提出GRPO-SIF，一种强化训练范式，将深度感知的视觉定位集成到统一的推理流程中，教会模型动态校正和关注提示相关区域。大量实验表明，SIFThinker在空间理解和细粒度视觉感知方面优于最先进的方法，同时保持强大的通用能力，突显了我们方法的有效性。代码：https://github.com/zhangquanchen/SIFThinker。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态大语言模型(MLLMs)在复杂视觉任务（如空间理解、细粒度感知）中的挑战，特别是现有方法未能利用空间提示进行注意力校正来迭代地改进对提示相关区域的关注。这个问题很重要，因为视觉理解是计算机视觉的基本任务，而人类视觉感知是动态的，涉及注意力转换和底层空间感知。当前模型处理图像时往往采用全局统一方式，无法模拟人类如何逐步关注相关区域并考虑3D空间关系，这限制了模型在需要精确空间推理和细粒度感知的任务上的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察人类视觉感知特点（逐步关注区域、考虑3D空间关系、动态校正注意力）来启发方法设计。他们分析了现有方法的局限性：传统方法忽略动态注意力转换，早期分步方法切断推理链连续性，外部工具依赖方法不够内在，空间感知方法关注整个图像而非相关区域。因此，作者设计了具有3D感知的自适应图像聚焦机制，将视觉感知和空间感知整合到统一框架。该方法借鉴了视觉思维链推理、强化学习训练范式、深度估计技术和边界框定位等现有工作，但进行了创新性整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是模仿人类视觉感知过程，通过空间感知的图像聚焦机制使模型在推理过程中动态关注和校正提示相关区域。整体流程分为三个阶段：1)数据生成：构建SIF-50K数据集，使用反向扩展-前向推理策略生成图像-文本交错思维链；2)预热阶段：通过监督微调使模型生成结构化推理链；3)强化学习阶段：基于GRPO框架，使用四个奖励函数（格式奖励、渐进答案准确性奖励、定位奖励、深度一致性奖励）优化模型的空间感知能力，并引入分层交并比(HIoU)评估定位质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)空间感知的'图像思考'框架，首个结合自适应聚焦机制和3D感知的框架；2)反向扩展-前向推理策略，用于构建图像-文本交错思维链；3)GRPO-SIF强化训练范式，整合四个专门奖励函数；4)分层交并比(HIoU)评估边界框质量。相比之前工作的不同：与传统方法相比，SIFThinker模拟人类动态视觉过程；与早期分步方法相比，保持推理链连续性；与外部工具依赖方法相比，内在支持图像-文本交错推理；与空间感知方法相比，专注于提示相关区域；与分离定位和生成的方法相比，整合3D感知作为视觉理解基础。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SIFThinker通过引入空间感知的图像聚焦机制和强化训练范式，首次将3D空间感知与动态视觉定位相结合，显著提升了多模态大语言模型在空间理解和细粒度视觉感知任务中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current multimodal large language models (MLLMs) still face significantchallenges in complex visual tasks (e.g., spatial understanding, fine-grainedperception). Prior methods have tried to incorporate visual reasoning, however,they fail to leverage attention correction with spatial cues to iterativelyrefine their focus on prompt-relevant regions. In this paper, we introduceSIFThinker, a spatially-aware "think-with-images" framework that mimics humanvisual perception. Specifically, SIFThinker enables attention correcting andimage region focusing by interleaving depth-enhanced bounding boxes and naturallanguage. Our contributions are twofold: First, we introduce areverse-expansion-forward-inference strategy that facilitates the generation ofinterleaved image-text chains of thought for process-level supervision, whichin turn leads to the construction of the SIF-50K dataset. Besides, we proposeGRPO-SIF, a reinforced training paradigm that integrates depth-informed visualgrounding into a unified reasoning pipeline, teaching the model to dynamicallycorrect and focus on prompt-relevant regions. Extensive experiments demonstratethat SIFThinker outperforms state-of-the-art methods in spatial understandingand fine-grained visual perception, while maintaining strong generalcapabilities, highlighting the effectiveness of our method. Code:https://github.com/zhangquanchen/SIFThinker.</description>
      <author>example@mail.com (Zhangquan Chen, Ruihui Zhao, Chuwei Luo, Mingze Sun, Xinlei Yu, Yangyang Kang, Ruqi Huang)</author>
      <guid isPermaLink="false">2508.06259v2</guid>
      <pubDate>Fri, 15 Aug 2025 16:13:18 +0800</pubDate>
    </item>
    <item>
      <title>PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces</title>
      <link>http://arxiv.org/abs/2508.09950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种点云监督的本体感知运动强化学习方法，用于提升足式机器人在空间受限的爬行环境中的运动能力，无需依赖外部传感器。&lt;h4&gt;背景&lt;/h4&gt;在爬行空间中，基于外部感知的运动学习方法受限于传感器在低可见度条件下的大噪声和误差；而基于本体感知的运动方法难以穿越爬行空间，因为只能推断地面特征。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在爬行空间中有效导航的足式机器人运动控制方法，减少对外部传感器的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出了一种点云监督的本体感知运动强化学习方法，包括：设计状态估计网络利用历史本体感知数据估计地面和空间特征及碰撞状态；将点云表示在极坐标中并提出处理方法提取特征；设计全面的奖励函数指导机器人在碰撞后穿越爬行空间。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与现有方法相比，该方法在爬行空间中表现出更敏捷的运动能力。&lt;h4&gt;结论&lt;/h4&gt;该研究增强了足式机器人在不需要外部传感器的情况下穿越空间受限环境的能力。&lt;h4&gt;翻译&lt;/h4&gt;在空间受限结构（称为爬行空间）中的足式运动具有挑战性。在爬行空间中，当前基于外部感知的运动学习方法受限于传感器在可能的低可见度条件下的大噪声和误差，而当前基于本体感知的运动学习方法难以穿越爬行空间，因为只能推断地面特征。在本研究中，提出了一种用于爬行空间中足式机器人的点云监督本体感知运动强化学习方法。设计了一个状态估计网络，使用历史本体感知传感器数据估计机器人周围的地面和空间特征以及机器人的碰撞状态。点云在极坐标系中表示，并提出了一种点云处理方法，用于有效提取用于监督状态估计网络学习的地面和空间特征。设计了全面的奖励函数，指导机器人在碰撞后穿越爬行空间。实验表明，与现有方法相比，我们的方法在爬行空间中表现出更敏捷的运动。这项研究增强了足式机器人穿越空间受限环境的能力，而无需外部传感器。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决腿式机器人在空间受限环境（如低天花板隧道和小洞穴）中的运动控制问题。在现实应用中，这个问题很重要，因为地震救援、矿难救援等场景需要机器人在这些危险且人难以进入的环境中作业。现有方法要么依赖外部传感器（在低可见度条件下性能下降），要么无法处理天花板等空间结构，限制了机器人在这些环境中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于外部感知的方法在低可见度条件下的局限性，以及基于本体感知的方法无法处理空间受限环境的问题。然后设计了点云监督的本体感受强化学习框架（PPL），借鉴了强化学习在腿式机器人中的应用、点云处理技术和课程学习等方法，但创新性地将点云表示为极坐标形式，使MLP能够高效处理点云数据，并设计了专门的状态估计网络和奖励函数来处理爬行空间的特殊挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用点云数据作为监督信息，训练本体感知网络来估计机器人的周围环境特征和碰撞状态，使机器人能够在不依赖外部传感器的情况下穿越爬行空间。整体流程包括：1）网络架构设计（Actor、Critic和PPL-Net）；2）点云预处理（转换为极坐标表示）；3）状态估计（估计速度、碰撞状态和环境特征）；4）训练过程（使用课程学习和领域随机化）；5）部署与测试（在模拟器和真实机器人上验证）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）点云监督的本体感受强化学习框架；2）极坐标表示的点云处理方法；3）全面的状态估计网络（PPL-Net）；4）智能奖励设计（包括碰撞惩罚和碰撞后速度奖励）。相比之前的工作，不同之处在于：不依赖外部传感器（在低可见度条件下仍然有效）；不仅考虑地面特征，还考虑空间特征（如天花板位置）；使用极坐标表示点云，简化了处理流程；设计了专门的奖励函数来处理爬行空间的特殊挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于点云监督的本体感受强化学习方法，使腿式机器人能够在不依赖外部传感器的情况下，高效穿越空间受限的爬行环境，包括低照明和烟雾等低可见度条件。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The legged locomotion in spatially constrained structures (called crawlspaces) is challenging. In crawl spaces, current exteroceptive locomotionlearning methods are limited by large noises and errors of the sensors inpossible low visibility conditions, and current proprioceptive locomotionlearning methods are difficult in traversing crawl spaces because only groundfeatures are inferred. In this study, a point cloud supervised proprioceptivelocomotion reinforcement learning method for legged robots in crawl spaces isproposed. A state estimation network is designed to estimate the robot'ssurrounding ground and spatial features as well as the robot's collision statesusing historical proprioceptive sensor data. The point cloud is represented inpolar coordinate frame and a point cloud processing method is proposed toefficiently extract the ground and spatial features that are used to supervisethe state estimation network learning. Comprehensive reward functions thatguide the robot to traverse through crawl spaces after collisions are designed.Experiments demonstrate that, compared to existing methods, our method exhibitsmore agile locomotion in crawl spaces. This study enhances the ability oflegged robots to traverse spatially constrained environments without requiringexteroceptive sensors.</description>
      <author>example@mail.com (Bida Ma, Nuo Xu, Chenkun Qi, Xin Liu, Yule Mo, Jinkai Wang, Chunpeng Lu)</author>
      <guid isPermaLink="false">2508.09950v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
  <item>
      <title>RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians</title>
      <link>http://arxiv.org/abs/2508.09830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 Highlight. Shenxing and Jinxi are co-first authors. Code  and data are available at: https://github.com/vLAR-group/RayletDF&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RayletDF的可泛化方法，用于从原始点云或通过3DGS从RGB图像预估计的3D高斯函数进行3D表面重建。&lt;h4&gt;背景&lt;/h4&gt;现有的基于坐标的方法在渲染显式表面时通常计算量大，限制了3D表面重建的效率。&lt;h4&gt;目的&lt;/h4&gt;开发一种新技术，能够直接从查询射线预测表面点，从而高效地重建3D表面。&lt;h4&gt;方法&lt;/h4&gt;RayletDF方法引入了射线距离场技术，包含三个关键模块：射线特征提取器、射线距离场预测器和多射线混合器，这些组件协同工作提取几何特征、预测距离并重建精确表面点。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在多个公共真实世界数据集上表现出优越的表面重建性能，具有出色的泛化能力，能够通过单次前向传递成功重建未见过的数据集的3D表面。&lt;h4&gt;结论&lt;/h4&gt;RayletDF为3D表面重建提供了一种高效且泛化能力强的解决方案，适用于多种场景和数据集。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种从原始点云或通过3DGS从RGB图像预估计的3D高斯函数进行3D表面重建的可泛化方法。与现有的在渲染显式表面时通常计算量大的基于坐标的方法不同，我们提出的方法RayletDF引入了一种称为射线距离场的新技术，旨在直接从查询射线预测表面点。我们的管道包含三个关键模块：射线特征提取器、射线距离场预测器和多射线混合器。这些组件协同工作以提取细粒度的局部几何特征，预测射线距离，并聚合多个预测以重建精确的表面点。我们在多个公共真实世界数据集上广泛评估了我们的方法，展示了在从点云或3D高斯函数进行表面重建方面的优越性能。最值得注意的是，我们的方法具有出色的泛化能力，能够在测试中通过单次前向传递成功重建未见过的数据集的3D表面。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D表面重建的泛化问题。现有方法通常需要在每个场景单独训练，难以泛化到新场景，且计算密集或难以捕捉精细表面细节。这个问题在现实世界中很重要，因为增强现实、机器人导航和数字孪生等应用需要快速适应新环境而不重新训练模型，同时保持高质量的表面重建能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有3D重建方法的局限性：基于坐标的方法计算密集，3D高斯溅射在深度渲染方面表现不佳，而基于光线的方法局限于物体级表面且缺乏泛化能力。受基于光线方法启发，作者提出了'raylet'概念（光线的单位线段），专注于表面局部几何模式。方法借鉴了光线表示和稀疏卷积网络，但创新性地设计了raylet距离场和多raylet混合器，通过学习局部表面特征实现泛化重建。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学习局部表面的raylet距离表示实现可泛化的3D表面重建，避免传统方法中的密集采样问题。整体流程包括三个模块：1) Raylet特征提取器：使用稀疏卷积网络提取场景特征，为每个查询raylet收集附近点的位置和相对信息；2) Raylet距离场预测器：通过MLP预测raylet起点到表面的有符号距离和置信度分数；3) 多raylet混合器：沿每条光线采样多个raylet，使用置信度加权混合多个预测，得到精确的表面点位置。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 引入raylet距离场概念，专注于表面局部几何模式；2) 设计三个协同工作的模块实现高效重建；3) 同时支持点云和3D高斯作为输入；4) 实现了跨数据集的强泛化能力。相比之前工作，RayletDF不使用基于坐标的表示避免了密集采样问题，不局限于物体级重建，不需要每个场景单独训练，通过多raylet混合提高了鲁棒性，在未见过的数据集上表现显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RayletDF通过创新性地引入raylet距离场和多raylet混合机制，实现了从点云或3D高斯表示的高效、准确且高度可泛化的3D表面重建，显著优于现有方法在跨数据集场景重建中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present a generalizable method for 3D surfacereconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS fromRGB images. Unlike existing coordinate-based methods which are oftencomputationally intensive when rendering explicit surfaces, our proposedmethod, named RayletDF, introduces a new technique called raylet distancefield, which aims to directly predict surface points from query rays. Ourpipeline consists of three key modules: a raylet feature extractor, a rayletdistance field predictor, and a multi-raylet blender. These components worktogether to extract fine-grained local geometric features, predict rayletdistances, and aggregate multiple predictions to reconstruct precise surfacepoints. We extensively evaluate our method on multiple public real-worlddatasets, demonstrating superior performance in surface reconstruction frompoint clouds or 3D Gaussians. Most notably, our method achieves exceptionalgeneralization ability, successfully recovering 3D surfaces in a single-forwardpass across unseen datasets in testing.</description>
      <author>example@mail.com (Shenxing Wei, Jinxi Li, Yafei Yang, Siyuan Zhou, Bo Yang)</author>
      <guid isPermaLink="false">2508.09830v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios</title>
      <link>http://arxiv.org/abs/2508.09470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CitySeg，一个用于城市规模点云语义分割的基础模型，结合文本模态实现开放词汇分割和零样本推理。该模型通过创新的数据预处理、网络架构和训练策略，解决了现有模型因数据规模有限和域差距导致的泛化能力不足问题，在多个基准测试上取得最先进性能。&lt;h4&gt;背景&lt;/h4&gt;语义分割是无人机感知系统的关键技术，能实现不依赖视觉信息的3D点分类和全面3D理解。然而，现有模型受限于3D数据规模有限和数据集间域差距，导致泛化能力下降。&lt;h4&gt;目的&lt;/h4&gt;解决城市规模点云语义分割面临的两大挑战：多领域间数据分布不均匀导致的泛化能力有限，以及不同数据集间的语义标签不一致问题。&lt;h4&gt;方法&lt;/h4&gt;1) 提出CitySeg基础模型结合文本模态；2) 自定义数据预处理规则缓解多领域数据分布不均；3) 设计局部-全局交叉注意力网络增强点网络感知能力；4) 引入分层分类策略解决语义标签差异；5) 建立分层图整合数据标签并用图编码器建模类别关系；6) 提出两阶段训练策略并采用hinge损失增加特征可分性。&lt;h4&gt;主要发现&lt;/h4&gt;CitySeg在九个闭集基准测试上实现最先进性能，显著优于现有方法。首次实现不依赖视觉信息的城市规模点云场景下的零样本泛化。&lt;h4&gt;结论&lt;/h4&gt;CitySeg通过结合文本模态和创新的网络架构与训练策略，成功解决了城市规模点云语义分割中的泛化能力问题，实现了更广泛的场景适应性和零样本学习能力。&lt;h4&gt;翻译&lt;/h4&gt;城市规模点云的语义分割是无人机感知系统的关键技术，它能够在不依赖任何视觉信息的情况下实现3D点的分类，从而获得全面的3D理解。然而，现有模型常常受限于3D数据的有限规模以及数据集之间的域差距，这导致泛化能力下降。为了应对这些挑战，我们提出了CitySeg，这是一个用于城市规模点云语义分割的基础模型，它结合了文本模态来实现开放词汇分割和零样本推理。具体而言，为了缓解多领域间数据分布不均匀的问题，我们自定义了数据预处理规则，并提出了一种局部-全局交叉注意力网络，以增强无人机场景下点网络的感知能力。为了解决不同数据集间的语义标签差异，我们引入了一种分层分类策略。根据数据标注规则建立的分层图整合了数据标签，并使用图编码器建模类别间的分层关系。此外，我们提出了一个两阶段训练策略，并采用hinge损失来增加子类别的特征可分性。实验结果表明，我们提出的CitySeg在九个闭集基准测试上实现了最先进的性能，显著优于现有方法。更重要的是，CitySeg首次实现了不依赖视觉信息的城市规模点云场景下的零样本泛化。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决城市规模场景下的3D点云语义分割问题，特别是开放词汇语义分割。这个问题很重要，因为它是无人机(UAV)感知系统的关键技术，能够对3D点进行分类而不依赖视觉信息，实现全面3D理解，对无人机在交通、物流和应急救援等领域的应用至关重要。现有方法受限于3D数据规模和不同数据集间的领域差距，泛化能力有限，且缺乏零样本推理能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有工作识别出城市规模3D语义分割的两个主要挑战：多源数据分布差异和标注不一致。他们借鉴了RandLA-Net、KPConv等点云处理网络的思想，以及视觉语言模型(如CLIP)的开放词汇理念，但针对城市规模场景进行了改进。作者创新性地设计了局部-全局交叉注意力网络增强点网络感知能力，提出层级分类策略解决标注差异，并设计两阶段训练策略提高性能。整体架构结合点云处理和文本编码，通过层级图编码器实现点云与文本的语义对齐。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合文本模态实现开放词汇分割和零样本推理，解决城市规模场景下多源点云数据的分布差异和标注不一致问题。整体流程包括：1)数据预处理标准化多域数据；2)点云处理使用局部-全局交叉注意力模块融合特征；3)文本处理构建层级图并编码类别关系；4)两阶段训练先捕获粗粒度语义再细化细粒度识别；5)支持零样本推理和增量学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)CitySeg基础模型，首个针对城市规模的3D开放词汇语义分割模型；2)局部-全局交叉注意力模块解决多源数据分布差异；3)层级分类策略处理标注不一致；4)两阶段训练策略提高性能。相比之前工作，不同之处在于：直接对齐点云与文本而非依赖2D图像中介；专门解决城市规模多源数据挑战；支持零样本推理；具备更强的跨域泛化能力，避免在单个数据集上过拟合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CitySeg通过引入局部-全局交叉注意力网络和层级分类策略，解决了城市规模场景下多源点云数据的分布差异和标注不一致问题，实现了首个支持开放词汇分割和零样本推理的3D语义分割基础模型，显著提升了城市规模3D场景理解的性能和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation of city-scale point clouds is a critical technology forUnmanned Aerial Vehicle (UAV) perception systems, enabling the classificationof 3D points without relying on any visual information to achieve comprehensive3D understanding. However, existing models are frequently constrained by thelimited scale of 3D data and the domain gap between datasets, which lead toreduced generalization capability. To address these challenges, we proposeCitySeg, a foundation model for city-scale point cloud semantic segmentationthat incorporates text modality to achieve open vocabulary segmentation andzero-shot inference. Specifically, in order to mitigate the issue ofnon-uniform data distribution across multiple domains, we customize the datapreprocessing rules, and propose a local-global cross-attention network toenhance the perception capabilities of point networks in UAV scenarios. Toresolve semantic label discrepancies across datasets, we introduce ahierarchical classification strategy. A hierarchical graph establishedaccording to the data annotation rules consolidates the data labels, and thegraph encoder is used to model the hierarchical relationships betweencategories. In addition, we propose a two-stage training strategy and employhinge loss to increase the feature separability of subcategories. Experimentalresults demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)performance on nine closed-set benchmarks, significantly outperforming existingapproaches. Moreover, for the first time, CitySeg enables zero-shotgeneralization in city-scale point cloud scenarios without relying on visualinformation.</description>
      <author>example@mail.com (Jialei Xu, Zizhuang Wei, Weikang You, Linyun Li, Weijian Sun)</author>
      <guid isPermaLink="false">2508.09470v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.09404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM Multimedia 2025 (Dataset Track) Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Waymo-3DSkelMo，这是首个提供高质量、时间连贯的3D骨骼运动并包含明确交互语义的大规模数据集，用于自动驾驶中的细粒度行人交互理解。&lt;h4&gt;背景&lt;/h4&gt;现有数据集大多依赖于从单目RGB视频帧估计3D姿态，这种方法存在遮挡和缺乏时间连续性的问题，导致产生不真实和低质量的人体运动。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D运动数据集的质量问题，提供高质量、时间连贯的3D骨骼运动数据，用于自动驾驶系统中的细粒度行人交互理解。&lt;h4&gt;方法&lt;/h4&gt;利用3D人体形状和运动先验来增强从原始LiDAR点云中提取的3D姿态序列的质量，数据集基于Waymo感知数据集构建。&lt;h4&gt;主要发现&lt;/h4&gt;数据集涵盖超过800个真实驾驶场景，总时长超过14,000秒；每个场景平均有27个智能体参与交互，最大场景包含多达250个智能体；建立了不同行人密度下的3D姿态预测基准；证明了该数据集作为未来复杂城市环境中细粒度人类行为理解研究的基础资源的价值。&lt;h4&gt;结论&lt;/h4&gt;Waymo-3DSkelMo数据集解决了现有数据集的质量问题，为自动驾驶领域提供了宝贵的资源，有助于提高系统对城市环境中人类交互行为的理解能力。&lt;h4&gt;翻译&lt;/h4&gt;大规模高质量多人交互3D运动数据集对自动驾驶中的数据驱动模型至关重要，使其能够在动态城市环境中实现细粒度行人交互理解。然而，现有数据集大多依赖于从单目RGB视频帧估计3D姿态，这些方法存在遮挡和缺乏时间连续性的问题，从而产生不真实和低质量的人体运动。在本文中，我们介绍了Waymo-3DSkelMo，这是首个提供高质量、时间连贯的3D骨骼运动并包含明确交互语义的大规模数据集，源自Waymo感知数据集。我们的关键见解是利用3D人体形状和运动先验来增强从原始LiDAR点云中提取的3D姿态序列的质量。该数据集涵盖超过800个真实驾驶场景中的14,000多秒，包括每个场景中平均27个智能体之间的丰富交互（最大场景中有多达250个智能体）。此外，我们在不同行人密度下建立了3D姿态预测基准，结果表明其作为未来复杂城市环境中细粒度人类行为理解研究基础资源的价值。数据集和代码将在https://github.com/GuangxunZhu/Waymo-3DSkelMo上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶领域缺乏高质量大规模多人3D骨骼运动数据集的问题，特别是精细的行人交互建模。这个问题很重要，因为准确理解行人和其他交通参与者之间的交互对自动驾驶车辆的安全决策至关重要，不准确或粗糙的建模可能导致对行人未来轨迹的错误预测，从而引发安全隐患。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法从RGB图像估计3D姿态的局限性（如遮挡和时间不连续），以及直接从LiDAR点云估计3D姿态的噪声问题。他们选择了Waymo开放数据集作为基础，设计了四阶段处理流程：点云提取与融合、3D人体网格恢复、时空对齐和神经运动场增强。该方法借鉴了SMPL人体模型、LiDAR-HMR网格恢复技术、Neural Motion Field (NeMF)运动增强和Frenet frame方向校正等现有工作，但创新性地将它们结合成一个完整流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用3D人体形状和运动先验来增强从原始LiDAR点云提取的3D姿态序列质量，生成高质量、时间连贯的3D骨骼运动数据集。整体流程包括：1)从Waymo LiDAR数据中提取并融合点云；2)使用基于SMPL的人体形状先验进行3D人体网格恢复；3)通过时空对齐提高运动连贯性，包括插值缺失帧和采用Frenet frame处理方向问题；4)利用预训练的Neural Motion Field作为运动先验增强运动质量；5)将优化后的SMPL参数转换为骨骼运动表示。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出结合人体形状先验和运动先验的新处理流程；2)发布首个自动驾驶领域的大规模连续抗遮挡3D骨骼运动数据集；3)提供丰富的多智能体交互场景(800+场景，平均27个智能体/场景，最多250个)；4)建立不同行人密度下的3D姿态预测基准。相比之前的工作，不同之处在于：数据规模更大(14,000秒vs最多4,000秒)，交互更丰富(平均27个智能体vs最多6.8个)，标注更密集(243万帧级标注vs约1万手动标注)，且不依赖RGB图像估计，避免了遮挡和时间连续性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Waymo-3DSkelMo通过结合人体形状先验和神经运动场，从Waymo原始LiDAR数据生成了首个大规模、高质量、时间连贯的3D骨骼运动数据集，为自动驾驶中的精细行人交互建模提供了宝贵的资源。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale high-quality 3D motion datasets with multi-person interactionsare crucial for data-driven models in autonomous driving to achievefine-grained pedestrian interaction understanding in dynamic urbanenvironments. However, existing datasets mostly rely on estimating 3D posesfrom monocular RGB video frames, which suffer from occlusion and lack oftemporal continuity, thus resulting in unrealistic and low-quality humanmotion. In this paper, we introduce Waymo-3DSkelMo, the first large-scaledataset providing high-quality, temporally coherent 3D skeletal motions withexplicit interaction semantics, derived from the Waymo Perception dataset. Ourkey insight is to utilize 3D human body shape and motion priors to enhance thequality of the 3D pose sequences extracted from the raw LiDRA point clouds. Thedataset covers over 14,000 seconds across more than 800 real driving scenarios,including rich interactions among an average of 27 agents per scene (with up to250 agents in the largest scene). Furthermore, we establish 3D pose forecastingbenchmarks under varying pedestrian densities, and the results demonstrate itsvalue as a foundational resource for future research on fine-grained humanbehavior understanding in complex urban environments. The dataset and code willbe available at https://github.com/GuangxunZhu/Waymo-3DSkelMo</description>
      <author>example@mail.com (Guangxun Zhu, Shiyu Fan, Hang Dai, Edmond S. L. Ho)</author>
      <guid isPermaLink="false">2508.09404v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>GeoVLA: Empowering 3D Representations in Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2508.09071v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The project is visible at https://linsun449.github.io/GeoVLA/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GeoVLA是一种新型VLA框架，通过整合3D信息提升机器人操作性能，在模拟和真实世界环境中表现出色。&lt;h4&gt;背景&lt;/h4&gt;当前VLA模型主要依赖2D视觉输入，忽略了3D物理世界中的丰富几何信息，限制了机器人的空间感知能力和适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效集成3D信息的VLA框架，提升机器人的空间感知能力和适应性。&lt;h4&gt;方法&lt;/h4&gt;GeoVLA使用视觉语言模型处理图像和语言指令提取嵌入，将深度图转换为点云并通过点嵌入网络生成3D几何嵌入，然后通过3D增强动作专家处理这些嵌入生成精确动作序列。&lt;h4&gt;主要发现&lt;/h4&gt;GeoVLA在LIBERO和ManiSkill2模拟基准测试中取得最先进结果，在需要高度适应性、尺度感知和视角不变性的真实世界任务中表现出显著鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;GeoVLA通过整合3D几何信息有效提升了机器人的操作性能和鲁棒性，为机器人操作提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-行动(VLA)模型已成为一种有前景的方法，使机器人能够遵循语言指令并预测相应动作。然而，当前VLA模型主要依赖2D视觉输入，忽略了3D物理世界中的丰富几何信息，这限制了它们的空间感知能力和适应性。在本文中，我们提出了GeoVLA，一种新型VLA框架，可有效整合3D信息以推进机器人操作。它使用视觉语言模型(VLM)处理图像和语言指令，提取融合的视觉语言嵌入。同时，它将深度图转换为点云，并采用定制的点编码器（称为点嵌入网络）独立生成3D几何嵌入。然后将这些生成的嵌入连接起来，并通过我们提出的空间感知动作专家（称为3D增强动作专家）进行处理，该专家结合来自不同传感器模态的信息以生成精确的动作序列。通过在模拟和真实世界环境中的大量实验，GeoVLA表现出卓越的性能和鲁棒性。它在LIBERO和ManiSkill2模拟基准测试中取得了最先进的结果，并在需要高度适应性、尺度感知和视角不变性的真实世界任务中表现出显著的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 当前视觉-语言-动作（VLA）模型主要依赖2D视觉输入，忽视了3D物理世界中丰富的几何信息，这限制了机器人的空间感知和适应能力。这个问题很重要，因为真实世界是三维的，机器人需要理解3D空间关系才能进行精确操作；2D视觉缺乏深度信息导致机器人在处理高度变化、尺寸感知和视角变化等任务时表现不佳；现有方法在集成3D信息时往往破坏视觉编码器与语言模型之间的对齐，需要大量额外训练数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者希望保留预训练视觉-语言模型的知识，同时有效整合3D信息，避免破坏现有对齐。他们借鉴了视觉-语言模型架构（如Prismatic-7B）、扩散模型用于动作生成、混合专家架构处理多模态信息以及点云处理技术。创新性地设计了点嵌入网络（PEN）独立处理3D几何信息，以及3D增强的动作专家（3DAE）整合视觉和几何信息，并采用静态路由策略平衡不同模态贡献。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过并行处理视觉和几何信息，再通过专门设计的模块有效融合这两种模态，增强机器人在3D空间中的感知和操作能力。整体流程：1)双路径并行处理 - 视觉-语言路径用VLM处理图像和语言指令，几何路径将深度图转换为点云并用PEN处理；2)PEN通过双路径架构处理点云，选择末端执行器对应的标记作为锚点；3)将视觉-语言特征和3D几何特征连接；4)用3DAE处理融合特征，采用扩散模型生成动作序列；5)输出精确动作序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)双路径架构设计，分别处理视觉-语言和3D几何信息；2)点嵌入网络（PEN）专门提取与末端执行器相关的几何特征；3)3D增强的动作专家（3DAE）采用混合专家架构处理多模态信息；4)端到端训练整个系统。不同之处：相比纯2D VLA模型，GeoVLA明确集成3D几何信息，增强空间感知；相比修改视觉主干网络的3D-VLA模型，GeoVLA不破坏视觉编码器与语言模型的对齐，不需要额外大规模数据；相比注入3D信息的动作专家模型，GeoVLA采用端到端训练，允许模型适应新模态。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GeoVLA通过创新的点嵌入网络和3D增强的动作专家，实现了视觉-语言-动作模型中2D视觉与3D几何信息的有效融合，显著提升了机器人在复杂3D环境中的空间感知能力和操作精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have emerged as a promising approach forenabling robots to follow language instructions and predict correspondingactions. However, current VLA models mainly rely on 2D visual inputs,neglecting the rich geometric information in the 3D physical world, whichlimits their spatial awareness and adaptability. In this paper, we presentGeoVLA, a novel VLA framework that effectively integrates 3D information toadvance robotic manipulation. It uses a vision-language model (VLM) to processimages and language instructions,extracting fused vision-language embeddings.In parallel, it converts depth maps into point clouds and employs a customizedpoint encoder, called Point Embedding Network, to generate 3D geometricembeddings independently. These produced embeddings are then concatenated andprocessed by our proposed spatial-aware action expert, called 3D-enhancedAction Expert, which combines information from different sensor modalities toproduce precise action sequences. Through extensive experiments in bothsimulation and real-world environments, GeoVLA demonstrates superiorperformance and robustness. It achieves state-of-the-art results in the LIBEROand ManiSkill2 simulation benchmarks and shows remarkable robustness inreal-world tasks requiring height adaptability, scale awareness and viewpointinvariance.</description>
      <author>example@mail.com (Lin Sun, Bin Xie, Yingfei Liu, Hao Shi, Tiancai Wang, Jiale Cao)</author>
      <guid isPermaLink="false">2508.09071v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Prototype-Guided Diffusion: Visual Conditioning without External Memory</title>
      <link>http://arxiv.org/abs/2508.09922v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出原型扩散模型(PDM)，将原型学习直接整合到扩散过程中，实现高效且自适应的视觉条件化图像生成，无需外部内存存储，同时保持高质量输出。&lt;h4&gt;背景&lt;/h4&gt;扩散模型已成为高质量图像生成的领先框架，但计算密集。潜在空间模型如Stable Diffusion减轻了部分计算负担但牺牲了细节。检索增强扩散模型(RDM)通过外部检索提高效率，但需要昂贵基础设施、依赖静态模型且缺乏适应性。&lt;h4&gt;目的&lt;/h4&gt;解决现有扩散模型计算密集、存储需求高、依赖静态模型且缺乏适应性的问题，开发一种高效且可扩展的图像生成方法。&lt;h4&gt;方法&lt;/h4&gt;提出原型扩散模型(PDM)，通过对比学习从清洁图像特征构建动态紧凑视觉原型集合，这些原型引导去噪步骤，通过将噪声表示与语义相关视觉模式对齐实现高效生成。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明PDM在保持高生成质量的同时，显著减少了计算和存储开销，为基于检索的条件化提供了可扩展的替代方案。&lt;h4&gt;结论&lt;/h4&gt;PDM通过整合原型学习到扩散过程中，实现了高效且自适应的视觉条件化图像生成，无需外部内存，为扩散模型的实际应用提供了更可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型已成为高质量图像生成的领先框架，提供稳定的训练和跨多个领域的强大性能。然而，它们仍然计算密集，特别是在去噪迭代过程中。像Stable Diffusion这样的潜在空间模型通过在压缩表示上操作减轻了部分成本，但牺牲了精细细节。更接近的方法如检索增强扩散模型(RDM)通过从大型外部存储库中检索相似示例来条件化去噪，从而解决效率问题。虽然有效，但这些方法引入了缺点：它们需要昂贵的存储和检索基础设施，依赖静态的视觉-语言模型（如CLIP）进行相似度匹配，并且在训练过程中缺乏适应性。我们提出了原型扩散模型(PDM)，一种将原型学习直接整合到扩散过程中的方法，用于高效和自适应的视觉条件化 - 无需外部内存。PDM不检索参考样本，而是通过对比学习从清洁图像特征构建动态紧凑视觉原型集合。这些原型通过将噪声表示与语义相关的视觉模式对齐来指导去噪步骤，实现具有强语义基础的高效生成。实验表明，PDM在保持高生成质量的同时减少了计算和存储开销，为扩散模型中的基于检索的条件化提供了可扩展的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models have emerged as a leading framework for high-quality imagegeneration, offering stable training and strong performance across diversedomains. However, they remain computationally intensive, particularly duringthe iterative denoising process. Latent-space models like Stable Diffusionalleviate some of this cost by operating in compressed representations, thoughat the expense of fine-grained detail. More recent approaches such asRetrieval-Augmented Diffusion Models (RDM) address efficiency by conditioningdenoising on similar examples retrieved from large external memory banks. Whileeffective, these methods introduce drawbacks: they require costly storage andretrieval infrastructure, depend on static vision-language models like CLIP forsimilarity, and lack adaptability during training. We propose the PrototypeDiffusion Model (PDM), a method that integrates prototype learning directlyinto the diffusion process for efficient and adaptive visual conditioning -without external memory. Instead of retrieving reference samples, PDMconstructs a dynamic set of compact visual prototypes from clean image featuresusing contrastive learning. These prototypes guide the denoising steps byaligning noisy representations with semantically relevant visual patterns,enabling efficient generation with strong semantic grounding. Experiments showthat PDM maintains high generation quality while reducing computational andstorage overhead, offering a scalable alternative to retrieval-basedconditioning in diffusion models.</description>
      <author>example@mail.com (Bilal Faye, Hanane Azzag, Mustapha Lebbah)</author>
      <guid isPermaLink="false">2508.09922v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation</title>
      <link>http://arxiv.org/abs/2508.09860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 tables, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VIPCGRL的新型深度强化学习框架，整合文本、关卡和草图三种模态，通过四元对比学习训练的共享嵌入空间和基于嵌入相似性的辅助奖励对齐策略，提高了AI系统的类人特性，使其在协作内容创作中更好地理解和满足人类设计者的意图。&lt;h4&gt;背景&lt;/h4&gt;与人类对齐的AI是共创力的关键组成部分，它使模型能够准确解释人类意图并生成符合设计目标的可控输出。这在通过强化学习进行程序内容生成（PCGRL）的领域中尤为重要，因为PCGRL旨在作为人类设计师的工具。然而，现有系统往往缺乏人类中心行为，限制了AI驱动生成工具在实际设计工作流程中的实用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型深度强化学习框架，解决现有系统在人类中心行为方面的不足，提高AI驱动生成工具在实际设计工作流程中的实用性，使其更好地理解和满足人类设计者的意图。&lt;h4&gt;方法&lt;/h4&gt;提出VIPCGRL（Vision-Instruction PCGRL）框架，整合三种模态（文本、关卡和草图）来扩展控制模态并增强类人特性。引入通过跨模态和人类-AI风格之间的四元对比学习训练的共享嵌入空间，并使用基于嵌入相似性的辅助奖励来对齐策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，VIPCGRL在类人特性方面优于现有基线，这一结论得到了定量指标和人工评估的双重验证。代码和数据集将在发表后提供。&lt;h4&gt;结论&lt;/h4&gt;VIPCGRL框架通过整合多模态输入和创新的训练方法，成功提高了AI系统的类人特性，使其在协作内容创作中更具实用性，为AI驱动的设计工具提供了新的发展方向。&lt;h4&gt;翻译&lt;/h4&gt;与人类对齐的AI是共创力的关键组成部分，它使模型能够准确解释人类意图并在协作内容创作中生成符合设计目标的可控输出。这一方向在通过强化学习进行程序内容生成（PCGRL）的领域尤为相关，因为PCGRL旨在作为人类设计师的工具。然而，现有系统往往缺乏人类中心行为，限制了AI驱动生成工具在实际设计工作流程中的实用性。在本文中，我们提出了VIPCGRL（Vision-Instruction PCGRL），一种新型深度强化学习框架，它整合了文本、关卡和草图三种模态，以扩展控制模态并增强类人特性。我们引入了一个通过跨模态和人类-AI风格之间的四元对比学习训练的共享嵌入空间，并使用基于嵌入相似性的辅助奖励来对齐策略。实验结果表明，VIPCGRL在类人特性方面优于现有基线，这一结论得到了定量指标和人工评估的双重验证。代码和数据集将在发表后提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human-aligned AI is a critical component of co-creativity, as it enablesmodels to accurately interpret human intent and generate controllable outputsthat align with design goals in collaborative content creation. This directionis especially relevant in procedural content generation via reinforcementlearning (PCGRL), which is intended to serve as a tool for human designers.However, existing systems often fall short of exhibiting human-centeredbehavior, limiting the practical utility of AI-driven generation tools inreal-world design workflows. In this paper, we propose VIPCGRL(Vision-Instruction PCGRL), a novel deep reinforcement learning framework thatincorporates three modalities-text, level, and sketches-to extend controlmodality and enhance human-likeness. We introduce a shared embedding spacetrained via quadruple contrastive learning across modalities and human-AIstyles, and align the policy using an auxiliary reward based on embeddingsimilarity. Experimental results show that VIPCGRL outperforms existingbaselines in human-likeness, as validated by both quantitative metrics andhuman evaluations. The code and dataset will be available upon publication.</description>
      <author>example@mail.com (In-Chang Baek, Seoyoung Lee, Sung-Hyun Kim, Geumhwan Hwang, KyungJoong Kim)</author>
      <guid isPermaLink="false">2508.09860v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization</title>
      <link>http://arxiv.org/abs/2508.09560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 4figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出WeatherPrompt多模态学习方法，通过融合图像嵌入和文本上下文解决无人机视觉地理定位在恶劣天气条件下的性能下降问题，无需训练的天气推理机制和动态门控机制是两大创新点。&lt;h4&gt;背景&lt;/h4&gt;无人机视觉地理定位在雨、雾等天气扰动条件下面临严重性能下降，现有方法存在两个固有局限：严重依赖有限的天气类别限制了泛化能力，以及通过伪天气类别对纠缠的场景-天气特征进行解缠时效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在各种天气条件下（包括复杂和未见过的天气）保持高性能的无人机视觉地理定位方法，解决场景和天气特征解缠问题。&lt;h4&gt;方法&lt;/h4&gt;WeatherPrompt采用多模态学习范式，包含：1) 无需训练的天气推理机制，利用大模型合成多天气文本描述；2) 由文本嵌入驱动的动态门控机制，自适应重新加权并跨模态融合视觉特征；3) 通过图像-文本对比学习和图像-文本匹配等跨模态目标优化框架。&lt;h4&gt;主要发现&lt;/h4&gt;在多种天气条件下，该方法与最先进的无人机地理定位方法相比具有竞争力的召回率。特别在夜间条件下，Recall@1提高13.37%，在雾和雪条件下提高18.69%。&lt;h4&gt;结论&lt;/h4&gt;WeatherPrompt通过多模态学习和创新的天气推理机制，有效解决了无人机视觉地理定位在恶劣天气条件下的性能下降问题，特别是在夜间和恶劣天气条件下表现出显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;无人机视觉地理定位在雨、雾等天气扰动条件下面临严重性能下降，现有方法存在两个固有局限：1) 严重依赖有限的天气类别，限制了泛化能力；2) 通过伪天气类别对纠缠的场景-天气特征进行解缠时效果不佳。我们提出了WeatherPrompt，一种通过融合图像嵌入和文本上下文来建立天气不变表示的多模态学习范式。我们的框架引入了两个关键贡献：首先，一种无需训练的天气推理机制，使用现成的多模态大模型通过类人推理合成多天气文本描述，提高了对未见或复杂天气的可扩展性，并能反映不同的天气强度。其次，为了更好地解缠场景和天气特征，我们提出了一种由文本嵌入驱动的动态门控机制的多模态框架，自适应地重新加权并跨模态融合视觉特征。该框架通过跨模态目标（包括图像-文本对比学习和图像-文本匹配）进一步优化，使不同天气条件下的同一场景在表示空间中更接近。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual geo-localization for drones faces critical degradation under weatherperturbations, \eg, rain and fog, where existing methods struggle with twoinherent limitations: 1) Heavy reliance on limited weather categories thatconstrain generalization, and 2) Suboptimal disentanglement of entangledscene-weather features through pseudo weather categories. We presentWeatherPrompt, a multi-modality learning paradigm that establishesweather-invariant representations through fusing the image embedding with thetext context. Our framework introduces two key contributions: First, aTraining-free Weather Reasoning mechanism that employs off-the-shelf largemulti-modality models to synthesize multi-weather textual descriptions throughhuman-like reasoning. It improves the scalability to unseen or complex weather,and could reflect different weather strength. Second, to better disentangle thescene and weather feature, we propose a multi-modality framework with thedynamic gating mechanism driven by the text embedding to adaptively reweightand fuse visual features across modalities. The framework is further optimizedby the cross-modal objectives, including image-text contrastive learning andimage-text matching, which maps the same scene with different weatherconditions closer in the respresentation space. Extensive experiments validatethat, under diverse weather conditions, our method achieves competitive recallrates compared to state-of-the-art drone geo-localization methods. Notably, itimproves Recall@1 by +13.37\% under night conditions and by 18.69\% under fogand snow conditions.</description>
      <author>example@mail.com (Jiahao Wen, Hang Yu, Zhedong Zheng)</author>
      <guid isPermaLink="false">2508.09560v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>A Unified Contrastive-Generative Framework for Time Series Classification</title>
      <link>http://arxiv.org/abs/2508.09451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为CoGenT的新型自监督学习框架，结合了对比学习和生成方法的优点，用于多变量时间序列分析，解决了两种方法各自的局限性，并在多个数据集上显示出显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列的自监督学习主要包括两种方法：对比方法和生成方法。对比方法在实例区分方面表现出色，生成方法则擅长建模数据分布。虽然这些方法各自有效，但它们的互补潜力尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;探索对比学习和生成方法在多变量时间序列自监督学习中的互补潜力，提出一种统一这两种范式的框架，以克服各自的方法局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了CoGenT（Contrastive Generative Time series framework），这是第一个通过联合对比-生成优化统一对比方法和生成方法的自监督学习框架。该方法通过混合目标函数，同时优化对比学习和生成学习。&lt;h4&gt;主要发现&lt;/h4&gt;在六个不同的时间序列数据集上评估CoGenT，结果显示了持续的改进，与独立的SimCLR和MAE相比，F1分数分别提高了高达59.2%和14.27%。分析表明，混合目标保留了判别能力，同时获得了生成鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;CoGenT为时间域中的混合自监督学习奠定了基础，通过统一对比学习和生成方法，克服了各自的局限性，并显著提升了性能。代码将很快发布。&lt;h4&gt;翻译&lt;/h4&gt;多变量时间序列的自监督学习主要包括两种范式：在实例区分方面表现出色的对比方法和建模数据分布的生成方法。虽然这些方法各自有效，但它们的互补潜力尚未被探索。我们提出了CoGenT（对比生成时间序列框架），这是第一个通过联合对比-生成优化统一这些范式的框架。CoGenT解决了两种方法的基本局限性：它克服了对比学习对时序数据高类内相似性的敏感性，同时减少了生成方法对大型数据集的依赖。我们在六个不同的时间序列数据集上评估了CoGenT。结果显示了持续的改进，与独立的SimCLR和MAE相比，F1分数分别提高了高达59.2%和14.27%。我们的分析表明，混合目标保留了判别能力，同时获得了生成鲁棒性。这些发现为时间域中的混合自监督学习奠定了基础。我们将很快发布代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) for multivariate time series mainly includestwo paradigms: contrastive methods that excel at instance discrimination andgenerative approaches that model data distributions. While effectiveindividually, their complementary potential remains unexplored. We propose aContrastive Generative Time series framework (CoGenT), the first framework tounify these paradigms through joint contrastive-generative optimization. CoGenTaddresses fundamental limitations of both approaches: it overcomes contrastivelearning's sensitivity to high intra-class similarity in temporal data whilereducing generative methods' dependence on large datasets. We evaluate CoGenTon six diverse time series datasets. The results show consistent improvements,with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,respectively. Our analysis reveals that the hybrid objective preservesdiscriminative power while acquiring generative robustness. These findingsestablish a foundation for hybrid SSL in temporal domains. We will release thecode shortly.</description>
      <author>example@mail.com (Ziyu Liu, Azadeh Alavi, Minyi Li, Xiang Zhang)</author>
      <guid isPermaLink="false">2508.09451v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data</title>
      <link>http://arxiv.org/abs/2508.08173v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE VIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CD-TVD框架，结合对比学习和改进的扩散模型，实现从有限高分辨率数据的3D超分辨率，减少了对大规模数据集的依赖。&lt;h4&gt;背景&lt;/h4&gt;大规模科学模拟需要大量资源生成高分辨率时变数据，而现有超分辨率方法依赖大量高分辨率训练数据，限制了其在多样化模拟场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型框架，能够从有限时间步的高分辨率数据实现准确的3D超分辨率，减少对大规模训练数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出CD-TVD框架，结合对比学习和改进的扩散模型；在历史模拟数据上预训练，学习高低分辨率样本的退化模式和特征；使用局部注意力机制的改进扩散模型，仅通过一个高分辨率时间步进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在流体和大气模拟数据集上的实验表明，CD-TVD能够提供准确且资源高效的3D超分辨率结果。&lt;h4&gt;结论&lt;/h4&gt;CD-TVD显著减少了大规模科学模拟中对高分辨率数据集的依赖，同时保持了恢复细粒度细节的能力，代表了科学模拟数据增强领域的重要进展。&lt;h4&gt;翻译&lt;/h4&gt;大规模科学模拟需要大量资源来生成高分辨率时变数据。虽然超分辨率是一种有效的后处理策略来降低成本，但现有方法依赖于大量高分辨率训练数据，限制了它们在不同模拟场景中的适用性。为了解决这一限制，我们提出了CD-TVD，一个结合对比学习和改进的基于扩散的超分辨率模型的新型框架，从有限时间步的高分辨率数据实现准确的3D超分辨率。在历史模拟数据上的预训练期间，对比编码器和扩散超分辨率模块学习高分辨率和低分辨率样本的退化模式和详细特征。在训练阶段，使用局部注意力机制的改进扩散模型仅通过一个新生成的高分辨率时间步进行微调，利用编码器学习到的退化知识。这种设计最小化了对大规模高分辨率数据集的依赖，同时保持了恢复细粒度细节的能力。在流体和大气模拟数据集上的实验结果证实，CD-TVD提供了准确且资源高效的3D超分辨率，标志着大规模科学模拟数据增强的重要进展。代码可在https://github.com/Xin-Gao-private/CD-TVD获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D超分辨率任务中高分辨率时间变化数据稀缺的问题。这个问题非常重要，因为大规模科学模拟需要巨大计算资源生成高分辨率数据，直接进行高分辨率模拟面临计算成本指数增长和数据存储需求激增的瓶颈，而传统低精度计算又无法捕捉微观结构的演化细节或关键状态中的突变特征。现有超分辨率方法依赖大量高分辨率训练数据，限制了它们在不同模拟场景中的适用性，且科学数据具有多物理场耦合、非线性时空演化和严格守恒定律等独特特性，使得简单数据增强技术难以应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有超分辨率方法在科学模拟中的局限性，然后借鉴了对比学习思想来建模高分辨率和低分辨率数据间的退化模式，同时采用扩散模型作为超分辨率基础架构，因为扩散模型能很好地处理复杂模式和细微纹理。作者还引入了局部注意力机制减少计算成本，并在对比编码模块训练中使用对抗学习。这些现有技术的创新组合形成了CD-TVD方法，特别设计了两阶段训练流程：先用历史模拟数据预训练，再用少量高分辨率样本针对新场景微调。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将高分辨率和低分辨率数据间的退化过程建模为对比学习任务，结合对比学习和改进的扩散模型，实现从有限高分辨率时间步数据的准确3D超分辨率。整体流程分为两阶段：1)预训练阶段使用历史模拟数据同时训练对比编码模块(学习退化模式)和扩散超分辨率模块(学习细节恢复)，通过对抗训练联合优化；2)微调阶段冻结对比编码模块，保留先验知识，使用少量高分辨率样本(基于熵选择最具代表性的时间步)微调扩散超分辨率模块。推理时，微调后的模型能准确重建所有低分辨率时间步的高分辨率版本。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将HR-LR退化过程明确建模为对比学习任务，提取强判别性退化特征；2)设计局部注意力增强的扩散架构，减轻计算负担同时恢复细粒度细节；3)利用预训练的通用退化模式，只需一个HR时间步即可重建所有后续LR时间步。相比之前工作，CD-TVD在数据效率上显著提升(仅需少量HR数据而非大量成对数据)，架构上创新结合对比学习和扩散模型而非主要使用GAN，训练策略上采用两阶段流程而非端到端训练，且能更好地处理科学数据的独特特性，在各种模拟数据集上表现出更强的泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CD-TVD通过结合对比学习和改进的扩散模型，实现了从有限高分辨率时间步数据的准确3D超分辨率，显著减少了对大规模高分辨率数据的依赖，为资源受限的科学模拟提供了高效的数据增强解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale scientific simulations require significant resources to generatehigh-resolution time-varying data (TVD). While super-resolution is an efficientpost-processing strategy to reduce costs, existing methods rely on a largeamount of HR training data, limiting their applicability to diverse simulationscenarios. To address this constraint, we proposed CD-TVD, a novel frameworkthat combines contrastive learning and an improved diffusion-basedsuper-resolution model to achieve accurate 3D super-resolution from limitedtime-step high-resolution data. During pre-training on historical simulationdata, the contrastive encoder and diffusion superresolution modules learndegradation patterns and detailed features of high-resolution andlow-resolution samples. In the training phase, the improved diffusion modelwith a local attention mechanism is fine-tuned using only one newly generatedhigh-resolution timestep, leveraging the degradation knowledge learned by theencoder. This design minimizes the reliance on large-scale high-resolutiondatasets while maintaining the capability to recover fine-grained details.Experimental results on fluid and atmospheric simulation datasets confirm thatCD-TVD delivers accurate and resource-efficient 3D super-resolution, marking asignificant advancement in data augmentation for large-scale scientificsimulations. The code is available athttps://github.com/Xin-Gao-private/CD-TVD.</description>
      <author>example@mail.com (Chongke Bi, Xin Gao, Jiangkang Deng, Guan Li, Jun Han)</author>
      <guid isPermaLink="false">2508.08173v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering</title>
      <link>http://arxiv.org/abs/2508.09180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了scAGC，一种用于单细胞RNA测序数据聚类的新方法，通过学习自适应细胞图和对比引导来提高细胞类型注释的准确性。&lt;h4&gt;背景&lt;/h4&gt;单细胞RNA测序数据分析中，准确的细胞类型注释是关键步骤，但传统方法面临高维度和大量零元素的挑战，而现有图神经网络方法依赖静态图结构，难以处理噪声和长尾分布问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理scRNA-seq数据特性的聚类方法，解决传统方法面临的统计和计算挑战，提高细胞类型注释的准确性。&lt;h4&gt;方法&lt;/h4&gt;scAGC方法结合了拓扑自适应图自编码器、可区分的Gumbel-Softmax采样策略、零膨胀负二项式损失函数和对比学习目标，以端到端方式同时优化特征表示和细胞图结构。&lt;h4&gt;主要发现&lt;/h4&gt;scAGC在9个真实scRNA-seq数据集上的实验结果表明，它持续优于其他最先进的方法，在9个和7个数据集上分别获得了最佳NMI和ARI分数。&lt;h4&gt;结论&lt;/h4&gt;scAGC通过自适应图结构和对比学习有效解决了scRNA-seq数据聚类中的挑战，提供了更准确和稳定的细胞类型注释结果。&lt;h4&gt;翻译&lt;/h4&gt;准确的细胞类型注释是分析单细胞RNA测序数据的关键步骤，它为细胞异质性提供了有价值的见解。然而，由于scRNA-seq数据的高维度和零元素普遍存在，传统聚类方法面临着显著的统计和计算挑战。虽然一些先进的方法使用图神经网络来建模细胞间关系，但它们通常依赖于对噪声敏感的静态图结构，并且无法捕捉单细胞群体中固有的长尾分布。为解决这些局限性，我们提出了scAGC，一种学习自适应细胞图并带有对比引导的单细胞聚类方法。我们的方法以端到端方式同时优化特征表示和细胞图。具体来说，我们引入了一种拓扑自适应图自编码器，它利用可区分的Gumbel-Softmax采样策略在训练过程中动态优化图结构。这种自适应机制通过促进更平衡的邻域结构来缓解长尾度分布问题。为了建模scRNA-seq数据的离散、过度离散和零膨胀特性，我们整合了零膨胀负二项式损失以实现稳健的特征重建。此外，还加入了对比学习目标来规范图学习过程，防止图拓扑的突然变化，确保稳定性和提高收敛性。在9个真实scRNA-seq数据集上的综合实验表明，scAGC持续优于其他最先进的方法，在9个和7个数据集上分别获得了最佳NMI和ARI分数。我们的代码可在匿名Github上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate cell type annotation is a crucial step in analyzing single-cell RNAsequencing (scRNA-seq) data, which provides valuable insights into cellularheterogeneity. However, due to the high dimensionality and prevalence of zeroelements in scRNA-seq data, traditional clustering methods face significantstatistical and computational challenges. While some advanced methods use graphneural networks to model cell-cell relationships, they often depend on staticgraph structures that are sensitive to noise and fail to capture thelong-tailed distribution inherent in single-cell populations.To address theselimitations, we propose scAGC, a single-cell clustering method that learnsadaptive cell graphs with contrastive guidance. Our approach optimizes featurerepresentations and cell graphs simultaneously in an end-to-end manner.Specifically, we introduce a topology-adaptive graph autoencoder that leveragesa differentiable Gumbel-Softmax sampling strategy to dynamically refine thegraph structure during training. This adaptive mechanism mitigates the problemof a long-tailed degree distribution by promoting a more balanced neighborhoodstructure. To model the discrete, over-dispersed, and zero-inflated nature ofscRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss forrobust feature reconstruction. Furthermore, a contrastive learning objective isincorporated to regularize the graph learning process and prevent abruptchanges in the graph topology, ensuring stability and enhancing convergence.Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGCconsistently outperforms other state-of-the-art methods, yielding the best NMIand ARI scores on 9 and 7 datasets, respectively.Our code is available atAnonymous Github.</description>
      <author>example@mail.com (Huifa Li, Jie Fu, Xinlin Zhuang, Haolin Yang, Xinpeng Ling, Tong Cheng, Haochen xue, Imran Razzak, Zhili Chen)</author>
      <guid isPermaLink="false">2508.09180v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Mixture-of-Experts for Incremental Graph Learning</title>
      <link>http://arxiv.org/abs/2508.09974v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图增量学习是一种使模型能够适应不断增加的图和数据的学习范式，但常规方法会遭遇灾难性遗忘问题。本文提出动态混合专家(DyMoE)方法，添加新专家网络并设计定制正则化损失，同时采用稀疏MoE减少计算成本，在类别增量学习中实现4.92%的相对准确率提升。&lt;h4&gt;背景&lt;/h4&gt;图增量学习旨在使训练好的模型随时间适应不断增加的图和数据，无需在完整数据集上重新训练。常规图机器学习方法在增量学习环境中会遭遇灾难性遗忘问题，之前学习知识被新知识覆盖。先前方法未考虑不同时间点获取的知识对学习新任务有不同贡献。&lt;h4&gt;目的&lt;/h4&gt;解决图增量学习中的灾难性遗忘问题，区分哪些旧知识可以迁移到新任务学习，哪些可能偏离新数据分布并产生负面影响，使模型能够有效适应不断增长的数据。&lt;h4&gt;方法&lt;/h4&gt;提出动态混合专家(DyMoE)方法：1) DyMoE GNN层添加新专家网络专门建模传入数据块；2) 设计定制正则化损失利用数据序列信息，使现有专家保持解决旧任务能力同时帮助新专家学习；3) 引入稀疏MoE方法，仅让top-k最相关专家进行预测，减少计算时间。&lt;h4&gt;主要发现&lt;/h4&gt;不同时间点获取的知识对学习新任务贡献不同；一些先前模式可转移到新数据学习，其他可能偏离新数据分布并产生负面影响；稀疏MoE方法显著减少计算成本。&lt;h4&gt;结论&lt;/h4&gt;DyMoE模型在类别增量学习方面比最佳基线提高4.92%相对准确率，显示了模型的卓越能力和实用性。&lt;h4&gt;翻译&lt;/h4&gt;图增量学习是一种学习范式，旨在使训练好的模型能够随时间适应不断增加的图和数据，而无需在完整数据集上重新训练。然而，常规的图机器学习方法在增量学习环境中会遭遇灾难性遗忘问题，即之前学习的知识会被新知识覆盖。先前的方法将之前训练的模型视为不可分割的单位，使用技术在学习新知识的同时保持旧行为。但这些方法没有考虑不同时间点获取的知识对学习新任务有不同的贡献。一些先前的模式可以转移到新数据学习中，而其他可能偏离新数据分布并产生负面影响。为解决此问题，我们提出了一种动态混合专家(DyMoE)方法用于增量学习。具体来说，DyMoE GNN层添加了新的专家网络，专门用于建模传入的数据块。我们设计了定制的正则化损失，利用数据序列信息，使现有专家能够保持解决旧任务的能力，同时帮助新专家有效学习新数据。随着数据块数量随时间增长，完整混合专家模型(MoE)的计算成本增加。为解决此问题，我们引入了稀疏MoE方法，只有top-k最相关的专家进行预测，显著减少计算时间。我们的模型在类别增量学习方面比最佳基线提高了4.92%的相对准确率，显示了模型的卓越能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph incremental learning is a learning paradigm that aims to adapt trainedmodels to continuously incremented graphs and data over time without the needfor retraining on the full dataset. However, regular graph machine learningmethods suffer from catastrophic forgetting when applied to incrementallearning settings, where previously learned knowledge is overridden by newknowledge. Previous approaches have tried to address this by treating thepreviously trained model as an inseparable unit and using techniques tomaintain old behaviors while learning new knowledge. These approaches, however,do not account for the fact that previously acquired knowledge at differenttimestamps contributes differently to learning new tasks. Some prior patternscan be transferred to help learn new data, while others may deviate from thenew data distribution and be detrimental. To address this, we propose a dynamicmixture-of-experts (DyMoE) approach for incremental learning. Specifically, aDyMoE GNN layer adds new expert networks specialized in modeling the incomingdata blocks. We design a customized regularization loss that utilizes datasequence information so existing experts can maintain their ability to solveold tasks while helping the new expert learn the new data effectively. As thenumber of data blocks grows over time, the computational cost of the fullmixture-of-experts (MoE) model increases. To address this, we introduce asparse MoE approach, where only the top-$k$ most relevant experts makepredictions, significantly reducing the computation time. Our model achieved4.92\% relative accuracy increase compared to the best baselines on classincremental learning, showing the model's exceptional power.</description>
      <author>example@mail.com (Lecheng Kong, Theodore Vasiloudis, Seongjun Yun, Han Xie, Xiang Song)</author>
      <guid isPermaLink="false">2508.09974v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment</title>
      <link>http://arxiv.org/abs/2508.09843v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络的全方向图像质量评估方法，通过建模视口间的结构关系来增强对空间失真非均匀性的感知，实验证明该方法显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;当前的全方向图像质量评估方法难以评估局部非均匀失真，原因是它们对空间质量变化建模不足，且无法有效捕捉局部细节和全局上下文特征。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于图神经网络的OIQA框架，明确建模视口之间的结构关系，增强对空间失真非均匀性的感知。&lt;h4&gt;方法&lt;/h4&gt;使用斐波那契球采样生成具有良好拓扑结构的视口，将每个视口表示为图节点，使用多阶段特征提取网络推导高维节点表示，集成图注意力网络(GAT)建模相邻视口间的细粒度局部失真变化，使用图变换器捕获远距离区域间的长程质量相互作用。&lt;h4&gt;主要发现&lt;/h4&gt;在两个具有复杂空间失真的大规模OIQA数据库上进行的大量实验表明，该方法显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;该方法有效且具有强大的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;当前的全方向图像质量评估(OIQA)方法由于对空间质量变化建模不足且无法有效捕捉局部细节和全局上下文特征，难以评估局部非均匀失真。为此，我们提出了一种基于图神经网络的OIQA框架，通过明确建模视口之间的结构关系来增强对空间失真非均匀性的感知。我们的方法采用斐波那契球采样生成具有良好拓扑结构的视口，并将每个视口表示为图节点。然后，多阶段特征提取网络推导出高维节点表示。为了全面捕捉空间依赖关系，我们集成了图注意力网络(GAT)来建模相邻视口间的细粒度局部失真变化，以及一个图变换器来捕获远距离区域间的长程质量相互作用。在两个具有复杂空间失真的大规模OIQA数据库上进行的大量实验表明，我们的方法显著优于现有方法，证实了其有效性和强大的泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决全景图像质量评估方法难以评估局部不均匀失真的问题。这个问题在现实中很重要，因为360度全景图像在虚拟现实、增强现实和元宇宙等应用中越来越普及，这些图像在采集、拼接、压缩过程中容易产生复杂的空间失真，而现有方法无法有效捕捉这些非均匀分布的质量变化，影响了用户体验和系统性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到全景图像的复杂空间结构给质量评估带来挑战，特别是非均匀失真难以评估。他们借鉴了图神经网络的思想，设计了一个结合图注意力网络和图变换器的框架。具体来说，他们采用了斐波那契球面采样来均匀分布视口，使用Swin Transformer提取特征，并构建图结构来建模视口之间的关系。这种方法融合了计算机视觉和图神经网络领域的现有技术，但进行了创新性组合以适应全景图像质量评估的特殊需求。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将全景图像视为图结构，其中每个节点代表一个从球面均匀采样的视口，通过图神经网络同时建模局部和全局的空间依赖关系。整体流程包括：1)使用斐波那契球面采样提取空间分布均匀的视口；2)用Swin Transformer提取每个视口的多尺度特征；3)添加球形位置编码；4)基于Haversine距离构建视口邻接图；5)用图注意力网络建模局部质量依赖；6)用图变换器捕获长程质量依赖；7)融合特征预测整体质量分数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)采用斐波那契球面采样获得空间均匀的视口；2)构建球形视口邻接图并应用GAT建模局部交互；3)设计图变换器架构捕获跨区域的全局质量依赖；4)联合建模局部失真敏感性和全局结构一致性。相比之前工作，传统方法直接应用2D评估方法到全景图像投影，忽略了人类视觉系统的视口感知机制；基于整个图像的方法受极地区域拉伸失真影响；基于视口的方法虽然模拟人类观看行为，但对非均匀失真建模不足。本文通过图神经网络同时建模局部和全局空间依赖，有效解决了这些问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于层次图注意力网络的全景图像无参考质量评估方法，通过联合建模局部和全局的空间依赖关系，有效解决了全景图像中非均匀失真的评估问题，显著提升了预测精度和跨数据集泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current Omnidirectional Image Quality Assessment (OIQA) methods struggle toevaluate locally non-uniform distortions due to inadequate modeling of spatialvariations in quality and ineffective feature representation capturing bothlocal details and global context. To address this, we propose a graph neuralnetwork-based OIQA framework that explicitly models structural relationshipsbetween viewports to enhance perception of spatial distortion non-uniformity.Our approach employs Fibonacci sphere sampling to generate viewports withwell-structured topology, representing each as a graph node. Multi-stagefeature extraction networks then derive high-dimensional node representation.To holistically capture spatial dependencies, we integrate a Graph AttentionNetwork (GAT) modeling fine-grained local distortion variations among adjacentviewports, and a graph transformer capturing long-range quality interactionsacross distant regions. Extensive experiments on two large-scale OIQA databaseswith complex spatial distortions demonstrate that our method significantlyoutperforms existing approaches, confirming its effectiveness and stronggeneralization capability.</description>
      <author>example@mail.com (Hao Yang, Xu Zhang, Jiaqi Ma, Linwei Zhu, Yun Zhang, Huan Zhang)</author>
      <guid isPermaLink="false">2508.09843v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Explainable Ensemble Learning for Graph-Based Malware Detection</title>
      <link>http://arxiv.org/abs/2508.09801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图的恶意软件检测和解释的堆叠集成框架，通过多样化GNN基础学习器和基于注意力的元学习器提高分类性能并提供可解释性。&lt;h4&gt;背景&lt;/h4&gt;现代计算环境中的恶意软件检测需要准确、可解释且能抵抗规避技术的模型。图神经网络在基于图的程序表示建模方面显示出潜力，但单一模型方法可能存在泛化能力有限和缺乏可解释性的问题，特别是在高风险安全应用中。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于图的恶意软件检测和解释框架，以提高分类性能并提供对恶意软件行为的可解释解释。&lt;h4&gt;方法&lt;/h4&gt;动态提取PE文件的CFG并使用两步嵌入策略编码基本块；使用具有不同消息传递机制的多样化GNN基础学习器捕获互补行为特征；通过基于注意力的多层感知器元学习器聚合预测输出并量化各基础模型的贡献；引入集成感知的后解释技术，利用GNN解释器生成的边级重要性分数和学习到的注意力权重产生可解释的解释。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该框架提高了恶意软件分类性能，同时提供了对恶意软件行为的有意义的解释。&lt;h4&gt;结论&lt;/h4&gt;所提出的堆叠集成框架在提高分类性能的同时，能够提供对恶意软件行为的可解释性，适用于高风险安全应用。&lt;h4&gt;翻译&lt;/h4&gt;现代计算环境中的恶意软件检测需要不仅准确而且可解释且能抵抗规避技术的模型。图神经网络通过建模基于图的程序表示（如控制流图CFG）中的丰富结构依赖关系在该领域显示出潜力。然而，单一模型方法可能存在泛化能力有限和缺乏可解释性的问题，特别是在高风险安全应用中。在本文中，我们提出了一种用于基于图的恶意软件检测和解释的新型堆叠集成框架。我们的方法从可移植可执行文件（PE）动态提取CFG，并通过两步嵌入策略对其基本块进行编码。使用一组具有不同消息传递机制的多样化GNN基础学习器来捕获互补的行为特征。它们的预测输出由一个元学习器聚合，该元学习器实现为基于注意力的多层感知器，既能分类恶意软件实例又能量化每个基础模型的贡献。为了增强可解释性，我们引入了一种集成感知的后解释技术，利用GNN解释器生成的边级重要性分数，并使用学习到的注意力权重融合它们。这产生了与最终集成决策一致的可解释的、模型不可知的解释。实验结果表明，我们的框架提高了分类性能，同时提供了对恶意软件行为的有意义的解释。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Malware detection in modern computing environments demands models that arenot only accurate but also interpretable and robust to evasive techniques.Graph neural networks (GNNs) have shown promise in this domain by modeling richstructural dependencies in graph-based program representations such as controlflow graphs (CFGs). However, single-model approaches may suffer from limitedgeneralization and lack interpretability, especially in high-stakes securityapplications. In this paper, we propose a novel stacking ensemble framework forgraph-based malware detection and explanation. Our method dynamically extractsCFGs from portable executable (PE) files and encodes their basic blocks througha two-step embedding strategy. A set of diverse GNN base learners, each with adistinct message-passing mechanism, is used to capture complementary behavioralfeatures. Their prediction outputs are aggregated by a meta-learner implementedas an attention-based multilayer perceptron, which both classifies malwareinstances and quantifies the contribution of each base model. To enhanceexplainability, we introduce an ensemble-aware post-hoc explanation techniquethat leverages edge-level importance scores generated by a GNN explainer andfuses them using the learned attention weights. This produces interpretable,model-agnostic explanations aligned with the final ensemble decision.Experimental results demonstrate that our framework improves classificationperformance while providing insightful interpretations of malware behavior.</description>
      <author>example@mail.com (Hossein Shokouhinejad, Roozbeh Razavi-Far, Griffin Higgins, Ali A Ghorbani)</author>
      <guid isPermaLink="false">2508.09801v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring</title>
      <link>http://arxiv.org/abs/2508.09527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的、可解释的图神经网络框架，用于预测性业务流程监控(PBPM)，解决了现有模型在架构、时间和语义方面的不足。&lt;h4&gt;背景&lt;/h4&gt;预测性业务流程监控(PBPM)旨在基于历史事件日志预测正在进行案例中的未来事件。尽管图神经网络(GNN)非常适合捕捉流程数据中的结构依赖关系，但现有的基于GNN的PBPM模型仍然发展不完善。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的、可解释的GNN框架，通过三个关键方面提升PBPM技术的现状：比较局部与全局建模方法、引入时间衰减注意力机制、嵌入转换类型语义。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一个包含多级可解释性模块的GNN架构，包括：1)比较基于前缀的图卷积网络(GCNs)和完整轨迹图注意力网络(GATs)；2)引入新的时间衰减注意力机制，构建动态的、以预测为中心的窗口；3)将转换类型语义嵌入边特征，以支持对结构模糊轨迹的细粒度推理。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在五个基准测试上取得了具有竞争力的Top-k准确率和DL分数，无需针对每个数据集进行调整。通过解决架构、时间和语义方面的差距，该工作为PBPM中的下一个事件预测提供了稳健、通用且可解释的解决方案。&lt;h4&gt;结论&lt;/h4&gt;该研究通过创新的GNN框架，解决了现有PBPM模型在处理局部与全局建模、时间相关性和转换语义方面的局限性，为业务流程预测提供了一个更强大、更通用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;预测性业务流程监控(PBPM)旨在基于历史事件日志预测正在进行案例中的未来事件。尽管图神经网络(GNN)非常适合捕捉流程数据中的结构依赖关系，但现有的基于GNN的PBPM模型仍然发展不完善。大多数模型要么依赖于短前缀子图，要么使用忽略时间相关性和转换语义的全局架构。我们提出了一个统一的、可解释的GNN框架，在三个关键方面提升了最先进的技术水平。首先，我们比较了基于前缀的图卷积网络(GCNs)和完整轨迹图注意力网络(GATs)，以量化局部建模和全局建模之间的性能差距。其次，我们引入了一种新颖的时间衰减注意力机制，构建动态的、以预测为中心的窗口，强调时间相关的历史信息并抑制噪声。第三，我们将转换类型语义嵌入边特征，以便对结构模糊的轨迹进行细粒度推理。我们的架构包含多级可解释性模块，提供对注意力行为的多样化可视化。在五个基准测试上评估，所提出的模型取得了具有竞争力的Top-k准确率和DL分数，无需针对每个数据集进行调整。通过解决架构、时间和语义方面的差距，这项工作为PBPM中的下一个事件预测提供了一个稳健、通用且可解释的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predictive Business Process Monitoring (PBPM) aims to forecast future eventsin ongoing cases based on historical event logs. While Graph Neural Networks(GNNs) are well suited to capture structural dependencies in process data,existing GNN-based PBPM models remain underdeveloped. Most rely either on shortprefix subgraphs or global architectures that overlook temporal relevance andtransition semantics. We propose a unified, interpretable GNN framework thatadvances the state of the art along three key axes. First, we compareprefix-based Graph Convolutional Networks(GCNs) and full trace Graph AttentionNetworks(GATs) to quantify the performance gap between localized and globalmodeling. Second, we introduce a novel time decay attention mechanism thatconstructs dynamic, prediction-centered windows, emphasizing temporallyrelevant history and suppressing noise. Third, we embed transition typesemantics into edge features to enable fine grained reasoning over structurallyambiguous traces. Our architecture includes multilevel interpretabilitymodules, offering diverse visualizations of attention behavior. Evaluated onfive benchmarks, the proposed models achieve competitive Top-k accuracy and DLscores without per-dataset tuning. By addressing architectural, temporal, andsemantic gaps, this work presents a robust, generalizable, and explainablesolution for next event prediction in PBPM.</description>
      <author>example@mail.com (Fang Wang, Ernesto Damiani)</author>
      <guid isPermaLink="false">2508.09527v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery</title>
      <link>http://arxiv.org/abs/2508.09401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对分布式后端服务系统的无监督异常检测方法，通过构建动态图和应用图卷积提取结构特征，使用Transformer建模时间行为，并通过联合嵌入机制整合特征，实现端到端的异常检测。&lt;h4&gt;背景&lt;/h4&gt;分布式后端服务系统面临复杂结构依赖、多样化行为演变和缺少标记数据等实际挑战，使得异常检测变得困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需标记数据的有效异常检测方法，能够捕捉系统中的异常传播路径和动态行为序列。&lt;h4&gt;方法&lt;/h4&gt;构建基于服务调用关系的动态图，应用图卷积提取高阶结构表示；使用Transformer建模节点时间行为；通过可学习的联合嵌入机制整合结构和行为特征；应用非线性映射计算异常分数，实现端到端检测。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在多个关键指标上优于现有模型，在捕捉异常传播路径和建模动态行为序列方面表现出更强的表达能力和稳定性，对图深度、序列长度和数据扰动具有良好的适应性。&lt;h4&gt;结论&lt;/h4&gt;该方法具有实际部署的高潜力，能够有效解决分布式后端服务系统中的异常检测问题，无需标记数据，适用于实际场景。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种针对分布式后端服务系统的无监督异常检测方法，解决了复杂结构依赖、多样化行为演变和缺少标记数据等实际挑战。该方法基于服务调用关系构建动态图，应用图卷积从多跳拓扑结构中提取高阶结构表示。使用Transformer建模每个节点的时间行为，捕捉长期依赖和局部波动。在特征融合阶段，通过可学习的联合嵌入机制将结构表示和行为表示整合为统一的异常向量。然后应用非线性映射计算异常分数，实现无需监督的端到端检测过程。在真实云监控数据上的实验包括对不同图深度、序列长度和数据扰动的敏感性分析。结果表明，该方法在几个关键指标上优于现有模型，在捕捉异常传播路径和建模动态行为序列方面表现出更强的表达能力和稳定性，具有实际部署的高潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study proposes an unsupervised anomaly detection method for distributedbackend service systems, addressing practical challenges such as complexstructural dependencies, diverse behavioral evolution, and the absence oflabeled data. The method constructs a dynamic graph based on service invocationrelationships and applies graph convolution to extract high-order structuralrepresentations from multi-hop topologies. A Transformer is used to model thetemporal behavior of each node, capturing long-term dependencies and localfluctuations. During the feature fusion stage, a learnable joint embeddingmechanism integrates structural and behavioral representations into a unifiedanomaly vector. A nonlinear mapping is then applied to compute anomaly scores,enabling an end-to-end detection process without supervision. Experiments onreal-world cloud monitoring data include sensitivity analyses across differentgraph depths, sequence lengths, and data perturbations. Results show that theproposed method outperforms existing models on several key metrics,demonstrating stronger expressiveness and stability in capturing anomalypropagation paths and modeling dynamic behavior sequences, with high potentialfor practical deployment.</description>
      <author>example@mail.com (Yun Zi, Ming Gong, Zhihao Xue, Yujun Zou, Nia Qi, Yingnan Deng)</author>
      <guid isPermaLink="false">2508.09401v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Exact Verification of Graph Neural Networks with Incremental Constraint Solving</title>
      <link>http://arxiv.org/abs/2508.09320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GNNev的精确验证方法，用于计算图神经网络在属性和结构扰动下的鲁棒性保证，支持三种聚合函数(sum、max和mean)，并在多个数据集上验证了其有效性和优越性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络越来越多地应用于高风险领域如欺诈检测和医疗保健，但容易受到对抗性攻击。虽然已有一些技术提供对抗性鲁棒性保证，但支持消息传递GNNs中常用聚合函数的方法仍然缺乏。&lt;h4&gt;目的&lt;/h4&gt;开发一种精确的验证方法，用于计算图神经网络在属性和结构扰动(包括边的添加或删除)下的保证，这些扰动受到预算限制的约束，特别关注节点分类任务。&lt;h4&gt;方法&lt;/h4&gt;该方法使用约束求解和边界紧缩，迭代解决一系列松弛的约束满足问题，并利用求解器的增量求解能力来提高效率。实现了GNNev求解器，支持三种聚合函数(sum、max和mean)，其中后两种是首次在此类方法中考虑。&lt;h4&gt;主要发现&lt;/h4&gt;在Cora和CiteSeer标准基准数据集以及Amazon和Yelp真实世界欺诈数据集上的实验评估，证明了GNNev的可用性和有效性，以及在sum聚合节点分类任务上比现有精确验证工具具有更好的性能。&lt;h4&gt;结论&lt;/h4&gt;GNNev为消息传递神经网络提供了一个有效的验证框架，支持多种聚合函数，能够在高风险应用中提供对抗性鲁棒性保证。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)越来越多地应用于高风险应用中，如欺诈检测或医疗保健，但容易受到对抗性攻击。已经提出了一些技术来提供对抗性鲁棒性保证，但对于消息传递GNNs中常用的聚合函数的支持仍然缺乏。在本文中，我们开发了一种精确的(可靠且完整的)GNN验证方法，用于计算在属性和结构扰动下的保证，这些扰动涉及边的添加或删除，并受到预算限制的约束。专注于节点分类任务，我们的方法使用约束求解和边界紧缩，迭代解决一系列松弛的约束满足问题，同时依赖求解器的增量求解能力来提高效率。我们实现了GNNev，这是一个用于消息传递神经网络的多功能求解器，支持三种聚合函数：sum、max和mean，其中后两种是首次在此类方法中考虑。在两个标准基准(Cora和CiteSeer)和两个真实世界的欺诈数据集(Amazon和Yelp)上对GNNev进行的广泛实验评估，证明了其可用性和有效性，以及在sum聚合节点分类任务上比现有精确验证工具具有更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are increasingly employed in high-stakesapplications, such as fraud detection or healthcare, but are susceptible toadversarial attacks. A number of techniques have been proposed to provideadversarial robustness guarantees, but support for commonly used aggregationfunctions in message-passing GNNs is still lacking. In this paper, we developan exact (sound and complete) verification method for GNNs to computeguarantees against attribute and structural perturbations that involve edgeaddition or deletion, subject to budget constraints. Focusing on nodeclassification tasks, our method employs constraint solving with boundtightening, and iteratively solves a sequence of relaxed constraintsatisfaction problems while relying on incremental solving capabilities ofsolvers to improve efficiency. We implement GNNev, a versatile solver formessage-passing neural networks, which supports three aggregation functions,sum, max and mean, with the latter two considered here for the first time.Extensive experimental evaluation of GNNev on two standard benchmarks (Cora andCiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates itsusability and effectiveness, as well as superior performance compared toexisting {exact verification} tools on sum-aggregated node classificationtasks.</description>
      <author>example@mail.com (Minghao Liu, Chia-Hsuan Lu, Marta Kwiatkowska)</author>
      <guid isPermaLink="false">2508.09320v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Over-Squashing in GNNs and Causal Inference of Rewiring Strategies</title>
      <link>http://arxiv.org/abs/2508.09265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种评估图神经网络中过压缩问题的方法，并分析了重连接技术对缓解这一问题的效果，发现重连接在图分类任务中通常有益，但在节点分类任务中可能增加过压缩问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在推荐系统、材料设计和药物重定位等领域表现出最先进的性能。然而，消息传递GNNs存在过压缩问题，即来自远程节点的长程信息被指数级压缩，这限制了其表达能力。重连接技术可以缓解这一瓶颈，但由于缺乏直接的过压缩经验指标，其实际影响尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;提出一种严格的、以拓扑为中心的方法来评估节点对之间的过压缩问题，并研究重连接策略如何影响过压缩问题，以及这种影响如何转化为性能提升。&lt;h4&gt;方法&lt;/h4&gt;使用节点对之间相互敏感性的衰减率来评估过压缩问题，并将这些成对评估扩展到四个图级统计量(普遍性、强度、变异性、极端性)。将这些指标与图内因果设计相结合，量化了重连接策略如何影响不同图和节点分类基准测试中的过压缩问题。&lt;h4&gt;主要发现&lt;/h4&gt;大多数图分类数据集都受到过压缩问题的影响（但程度各异）；重连接有效地缓解了这一问题，尽管缓解程度及其转化为性能提升的程度因数据集和方法而异；在节点分类数据集中，过压缩问题不太明显，重连接往往会增加过压缩，且性能变化与过压缩变化无关；当过压缩既显著又得到适度纠正时，重连接最有益；过于激进的重连接或应用于最小过压缩图的重连接不太可能有所帮助，甚至可能损害性能。&lt;h4&gt;结论&lt;/h4&gt;论文提出的即插即用诊断工具可以让从业者在任何训练之前确定重连接是否可能有效。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已在推荐系统、材料设计和药物重定位等广泛领域展现出最先进的性能。然而，消息传递GNNs遭受过压缩问题——来自远程节点的长程信息被指数级压缩——这限制了表达能力。重连接技术可以缓解这一瓶颈；但由于缺乏直接的过压缩经验指标，其实际影响尚不清楚。我们提出了一种严格的、以拓扑为中心的方法，使用节点对之间相互敏感性的衰减率来评估它们之间的过压缩问题。然后，我们将这些成对评估扩展到四个图级统计量（普遍性、强度、变异性、极端性）。将这些指标与图内因果设计相结合，我们量化了重连接策略如何影响不同图和节点分类基准测试中的过压缩问题。我们广泛的实证分析显示，大多数图分类数据集都受到过压缩问题的影响（但程度各异），重连接有效地缓解了这一问题——尽管缓解程度及其转化为性能提升的程度因数据集和方法而异。我们还发现，在节点分类数据集中，过压缩问题不太明显，重连接往往会增加过压缩，且性能变化与过压缩变化无关。这些发现表明，当过压缩既显著又得到适度纠正时，重连接最有益；而过于激进的重连接，或应用于最小过压缩图的重连接，不太可能有所帮助，甚至可能损害性能。我们的即插即用诊断工具让从业者在任何训练之前就能确定重连接是否可能有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have exhibited state-of-the-art performanceacross wide-range of domains such as recommender systems, material design, anddrug repurposing. Yet message-passing GNNs suffer from over-squashing --exponential compression of long-range information from distant nodes -- whichlimits expressivity. Rewiring techniques can ease this bottleneck; but theirpractical impacts are unclear due to the lack of a direct empiricalover-squashing metric. We propose a rigorous, topology-focused method forassessing over-squashing between node pairs using the decay rate of theirmutual sensitivity. We then extend these pairwise assessments to fourgraph-level statistics (prevalence, intensity, variability, extremity).Coupling these metrics with a within-graph causal design, we quantify howrewiring strategies affect over-squashing on diverse graph- andnode-classification benchmarks. Our extensive empirical analyses show that mostgraph classification datasets suffer from over-squashing (but to variousextents), and rewiring effectively mitigates it -- though the degree ofmitigation, and its translation into performance gains, varies by dataset andmethod. We also found that over-squashing is less notable in nodeclassification datasets, where rewiring often increases over-squashing, andperformance variations are uncorrelated with over-squashing changes. Thesefindings suggest that rewiring is most beneficial when over-squashing is bothsubstantial and corrected with restraint -- while overly aggressive rewiring,or rewiring applied to minimally over-squashed graphs, is unlikely to help andmay even harm performance. Our plug-and-play diagnostic tool lets practitionersdecide -- before any training -- whether rewiring is likely to pay off.</description>
      <author>example@mail.com (Danial Saber, Amirali Salehi-Abari)</author>
      <guid isPermaLink="false">2508.09265v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Blockchain Network Analysis using Quantum Inspired Graph Neural Networks &amp; Ensemble Models</title>
      <link>http://arxiv.org/abs/2508.09237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合量子启发图神经网络与集成模型的新方法，用于区块链网络中的反洗钱交易检测，通过引入规范多项式分解层增强了处理复杂数据结构的能力，达到了74.8%的F2分数，展示了量子启发技术在金融安全领域的潜力。&lt;h4&gt;背景&lt;/h4&gt;金融科技领域快速发展，在区块链网络中检测非法交易仍然是一个重大挑战，需要强大且创新的解决方案。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门用于区块链网络分析以反洗钱(AML)的系统，提高欺诈交易的检测能力。&lt;h4&gt;方法&lt;/h4&gt;结合量子启发图神经网络(QI-GNN)与集成模型(使用QBoost或随机森林分类器等经典模型)，并在图神经网络框架中引入规范多项式(CP)分解层，以增强处理和分析复杂数据结构的能力。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在检测欺诈交易方面达到了74.8%的F2分数，展示了量子启发技术结合CP层结构改进的有效性。&lt;h4&gt;结论&lt;/h4&gt;量子启发技术结合CP层的结构改进，在复杂网络分析中具有匹配甚至超越传统方法的潜力，倡导在金融领域更广泛地采用和探索量子启发算法以有效打击欺诈。&lt;h4&gt;翻译&lt;/h4&gt;在快速发展的金融科技领域，在区块链网络中检测非法交易仍然是一个关键挑战，需要强大且创新的解决方案。本研究通过结合量子启发图神经网络(QI-GNN)与使用QBoost或随机森林分类器等经典模型的集成模型的灵活性，提出了一种新方法。该系统专门为反洗钱(AML)工作中的区块链网络分析而定制。我们设计该系统的方法包含一个新组件，即在图神经网络框架中的规范多项式(CP)分解层，增强了其高效处理和分析复杂数据结构的能力。我们的技术方法已经过与经典机器学习实现的严格评估，在检测欺诈交易方面达到了74.8%的F2分数。这些结果突显了量子启发技术的潜力，辅以CP层的结构改进，不仅能够在复杂网络分析中匹配，而且可能超过传统方法，用于金融安全。研究结果倡导在金融部门更广泛地采用和进一步探索量子启发算法，以有效打击欺诈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the rapidly evolving domain of financial technology, the detection ofillicit transactions within blockchain networks remains a critical challenge,necessitating robust and innovative solutions. This work proposes a novelapproach by combining Quantum Inspired Graph Neural Networks (QI-GNN) withflexibility of choice of an Ensemble Model using QBoost or a classic model suchas Random Forrest Classifier. This system is tailored specifically forblockchain network analysis in anti-money laundering (AML) efforts. Ourmethodology to design this system incorporates a novel component, a CanonicalPolyadic (CP) decomposition layer within the graph neural network framework,enhancing its capability to process and analyze complex data structuresefficiently. Our technical approach has undergone rigorous evaluation againstclassical machine learning implementations, achieving an F2 score of 74.8% indetecting fraudulent transactions. These results highlight the potential ofquantum-inspired techniques, supplemented by the structural advancements of theCP layer, to not only match but potentially exceed traditional methods incomplex network analysis for financial security. The findings advocate for abroader adoption and further exploration of quantum-inspired algorithms withinthe financial sector to effectively combat fraud.</description>
      <author>example@mail.com (Luigi D'Amico, Daniel De Rosso, Ninad Dixit, Raul Salles de Padua, Samuel Palmer, Samuel Mugel, Román Orús, Holger Eble, Ali Abedi)</author>
      <guid isPermaLink="false">2508.09237v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor Search</title>
      <link>http://arxiv.org/abs/2508.08744v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at SIGMOD 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为Tagore的GPU加速库，用于高效构建基于图的近似最近邻搜索索引，解决了传统图索引方法开销大的问题。&lt;h4&gt;背景&lt;/h4&gt;高维向量空间中的近似最近邻搜索有广泛应用，基于图的索引因其高精度和效率而受到关注，但随着数据量增长和动态调整需求增加，索引开销不断升级。&lt;h4&gt;目的&lt;/h4&gt;开发一个GPU加速的图索引构建库，能够高效处理基于细化的图索引构建，如NSG和Vamana，同时减少索引开销。&lt;h4&gt;方法&lt;/h4&gt;引入GNN-Descent算法进行k-近邻图初始化；提出CFS通用计算过程支持多种k-NN图剪枝策略；设计两个广义GPU内核并行处理邻居关系中的复杂依赖；实现异步GPU-CPU-磁盘索引框架和集群感知缓存机制。&lt;h4&gt;主要发现&lt;/h4&gt;在7个真实数据集上测试，Tagore在保持索引质量的同时实现了1.32倍至112.79倍的加速效果。&lt;h4&gt;结论&lt;/h4&gt;Tagore是一个高效的GPU加速库，能够显著提高图索引构建速度，解决了大规模数据集下图索引开销大的挑战。&lt;h4&gt;翻译&lt;/h4&gt;高维向量空间中的近似最近邻搜索(ANNS)有广泛的真实世界应用。虽然已提出多种方法来高效处理ANNS，但基于图的索引因其高精度和效率而受到广泛关注。然而，基于图的索引开销仍然很大。随着数据量的指数级增长和对动态索引调整需求的增加，这种开销不断升级，构成了关键挑战。在本文中，我们介绍了Tagore，一个由GPU加速的用于图索引的快速库，具有构建基于细化的图索引(如NSG和Vamana)的强大功能。我们首先介绍了GNN-Descent，一种针对GPU的高效k-近邻(k-NN)图初始化算法。GNN-Descent通过两阶段下降过程加速相似性比较，并实现高度并行的邻居更新。接下来，为了支持各种k-NN图剪枝策略，我们提出了一个称为CFS的通用计算过程，并设计了两个广义GPU内核，用于并行处理邻居关系中的复杂依赖。对于超过GPU内存容量的大规模数据集，我们提出了一个异步GPU-CPU-磁盘索引框架，具有集群感知缓存机制，以最小化磁盘的I/O压力。在7个真实数据集上的广泛实验表明，Tagore在保持索引质量的同时实现了1.32倍至112.79倍的加速。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Approximate nearest neighbor search (ANNS) in high-dimensional vector spaceshas a wide range of real-world applications. Numerous methods have beenproposed to handle ANNS efficiently, while graph-based indexes have gainedprominence due to their high accuracy and efficiency. However, the indexingoverhead of graph-based indexes remains substantial. With exponential growth indata volume and increasing demands for dynamic index adjustments, this overheadcontinues to escalate, posing a critical challenge. In this paper, we introduceTagore, a fast library accelerated by GPUs for graph indexing, which haspowerful capabilities of constructing refinement-based graph indexes such asNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm forefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds upthe similarity comparison by a two-phase descent procedure and enables highlyparallelized neighbor updates. Next, aiming to support various k-NN graphpruning strategies, we formulate a universal computing procedure termed CFS anddevise two generalized GPU kernels for parallel processing complex dependenciesin neighbor relationships. For large-scale datasets exceeding GPU memorycapacity, we propose an asynchronous GPU-CPU-disk indexing framework with acluster-aware caching mechanism to minimize the I/O pressure on the disk.Extensive experiments on 7 real-world datasets exhibit that Tagore achieves1.32x-112.79x speedup while maintaining the index quality.</description>
      <author>example@mail.com (Zhonggen Li, Xiangyu Ke, Yifan Zhu, Bocheng Yu, Baihua Zheng, Yunjun Gao)</author>
      <guid isPermaLink="false">2508.08744v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Quantum Mechanics to Organic Liquid Properties via a Universal Force Field</title>
      <link>http://arxiv.org/abs/2508.08575v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ByteFF-Pol的新型可极化力场，基于图神经网络参数化，完全使用高级量子力学数据训练，能够准确预测小分子液体和电解质的热力学和传输性质。&lt;h4&gt;背景&lt;/h4&gt;分子动力学模拟是理解凝聚相系统结构和动力学的必要工具，但基于从头算计算预测宏观属性仍然面临挑战，通常受计算成本和模拟精度之间的权衡限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的力场方法，能够更准确、更高效地连接微观量子力学计算和宏观液体性质。&lt;h4&gt;方法&lt;/h4&gt;提出了ByteFF-Pol，一种基于图神经网络参数化的可极化力场，完全基于高级量子力学数据进行训练，利用物理启发的力场形式和训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;ByteFF-Pol在预测小分子液体和电解质的热力学和传输性质方面表现出色，优于最先进的经典和机器学习力场，具有零样本预测能力。&lt;h4&gt;结论&lt;/h4&gt;这一进展在电解质设计和定制溶剂等方面具有变革性潜力，代表了数据驱动材料发现的关键一步。&lt;h4&gt;翻译&lt;/h4&gt;分子动力学（MD）模拟是揭示凝聚相系统结构和动力学原子级洞察力的基本工具。然而，从头算计算普遍准确地预测宏观属性仍然是一个重大挑战，通常受计算成本和模拟精度之间的权衡阻碍。在此，我们提出了ByteFF-Pol，这是一种基于图神经网络（GNN）参数化的可极化力场，完全基于高级量子力学（QM）数据进行训练。利用物理启发的力场形式和训练策略，ByteFF-Pol在预测各种小分子液体和电解质的热力学和传输性质方面表现出色，优于最先进的（SOTA）经典和机器学习力场。ByteFF-Pol的零样本预测能力连接了微观量子力学计算和宏观液体性质，使先前难以处理的化学空间的探索成为可能。这一进展在电解质设计和定制溶剂等应用方面具有变革性潜力，代表了数据驱动材料发现的关键一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular dynamics (MD) simulations are essential tools for unravelingatomistic insights into the structure and dynamics of condensed-phase systems.However, the universal and accurate prediction of macroscopic properties fromab initio calculations remains a significant challenge, often hindered by thetrade-off between computational cost and simulation accuracy. Here, we presentByteFF-Pol, a graph neural network (GNN)-parameterized polarizable force field,trained exclusively on high-level quantum mechanics (QM) data. Leveragingphysically-motivated force field forms and training strategies, ByteFF-Polexhibits exceptional performance in predicting thermodynamic and transportproperties for a wide range of small-molecule liquids and electrolytes,outperforming state-of-the-art (SOTA) classical and machine learning forcefields. The zero-shot prediction capability of ByteFF-Pol bridges the gapbetween microscopic QM calculations and macroscopic liquid properties, enablingthe exploration of previously intractable chemical spaces. This advancementholds transformative potential for applications such as electrolyte design andcustom-tailored solvent, representing a pivotal step toward data-drivenmaterials discovery.</description>
      <author>example@mail.com (Tianze Zheng, Xingyuan Xu, Zhi Wang, Zhenze Yang, Yuanheng Wang, Xu Han, Zhenliang Mu, Ziqing Zhang, Siyuan Liu, Sheng Gong, Kuang Yu, Wen Yan)</author>
      <guid isPermaLink="false">2508.08575v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation</title>
      <link>http://arxiv.org/abs/2508.09626v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures, AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAD-Splat的新型3D航空场景语义分割方法，通过高斯点丢弃模块和高置信度伪标签生成流程解决了传统方法在处理尺度变化和结构遮挡时的语义模糊问题，提高了分割准确性和表示紧凑性。&lt;h4&gt;背景&lt;/h4&gt;在3D航空场景语义分割任务中，传统方法难以处理航空图像中由尺度变化和结构遮挡引起的语义模糊问题，这限制了它们的分割准确性和一致性。&lt;h4&gt;目的&lt;/h4&gt;解决3D航空场景语义分割中的语义模糊问题，提高分割准确性和一致性。&lt;h4&gt;方法&lt;/h4&gt;提出SAD-Splat方法，包含：1)高斯点丢弃模块，结合语义置信度估计和基于Hard Concrete分布的可学习稀疏机制；2)高置信度伪标签生成流程，利用2D基础模型在真实标签有限时增强监督。&lt;h4&gt;主要发现&lt;/h4&gt;SAD-Splat有效消除了冗余和语义模糊的高斯点，提高了分割性能和表示紧凑性；同时引入了3D航空语义(3D-AS)基准数据集，包含具有稀疏注释的多样化真实世界航空场景。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，SAD-Splat在分割准确性和表示紧凑性之间取得了良好的平衡，为3D航空场景理解提供了高效且可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在3D航空场景语义分割(3D-AVS-SS)任务中，传统方法难以处理航空图像中由尺度变化和结构遮挡引起的语义模糊问题。这限制了它们的分割准确性和一致性。为了应对这些挑战，我们提出了一种名为SAD-Splat的新型3D-AVS-SS方法。我们的方法引入了一个高斯点丢弃模块，该模块将语义置信度估计与基于Hard Concrete分布的可学习稀疏机制相结合。该模块有效消除了冗余和语义模糊的高斯点，提高了分割性能和表示紧凑性。此外，SAD-Splat集成了一个高置信度伪标签生成流程。它利用2D基础模型在真实标签有限时增强监督，从而进一步提高分割准确性。为了推进该领域的研究，我们引入了一个具有挑战性的基准数据集：3D航空语义(3D-AS)，其中包含具有稀疏注释的多样化真实世界航空场景。实验结果表明，SAD-Splat在分割准确性和表示紧凑性之间取得了优异的平衡。它为3D航空场景理解提供了高效且可扩展的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D航空场景语义分割中的语义歧义问题。传统方法难以处理航空图像中因尺度变化和结构遮挡导致的语义模糊，这限制了分割的准确性和一致性。这个问题在现实中很重要，因为3D航空场景语义分割在土地使用监测、城市规划和灾害响应等多种遥感应用中起着关键作用，准确的语义分割可以帮助理解复杂航空场景并支持各种决策过程。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法在3D航空场景语义分割中的局限性，发现了两个主要挑战：3D重建过程中的冗余高斯点问题和语义分割步骤中的噪声监督问题。针对这些挑战，作者设计了SAD-Splat方法，引入了高斯点丢弃模块和高置信度伪标签生成管道。该方法借鉴了现有的3D高斯溅射(3DGS)框架，并利用了2D基础模型(如SAM和GeoRSCLIP)的知识提取能力，同时采用了知识蒸馏技术和Hard Concrete分布来实现可学习的稀疏机制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过语义感知的高斯点丢弃机制消除冗余和语义模糊的高斯点，同时利用高置信度伪标签生成管道增强监督，在保证分割性能的同时提高表示的紧凑性。整体流程包括：预处理阶段利用SAM和GeoRSCLIP生成高置信度伪标签和语义置信图；训练阶段联合重建语义特征，为每个高斯点学习语义置信度和可学习丢弃率，并定期执行丢弃操作；最后结合语义特征损失、RGB重建损失和L0正则化损失进行优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：语义感知的高斯点丢弃机制(结合语义置信度估计和基于Hard Concrete分布的可学习稀疏机制)、高置信度伪标签生成管道(利用2D基础模型生成可靠伪标签增强监督)、以及新的基准数据集3D-AS(包含各种真实世界航空场景)。相比之前的工作，该方法更有效地处理了航空图像特有的尺度变化和结构遮挡问题，通过丢弃冗余高斯点提高了表示的紧凑性，同时提供了更好的空间一致性和3D结构理解，在有限标注数据的情况下表现更佳。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAD-Splat通过引入语义感知的高斯点丢弃机制和高置信度伪标签生成方法，有效解决了3D航空场景语义分割中的语义歧义和结构冗余问题，同时提高了模型的表示紧凑性和分割准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),traditional methods struggle to address semantic ambiguity caused by scalevariations and structural occlusions in aerial images. This limits theirsegmentation accuracy and consistency. To tackle these challenges, we propose anovel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussianpoint drop module, which integrates semantic confidence estimation with alearnable sparsity mechanism based on the Hard Concrete distribution. Thismodule effectively eliminates redundant and semantically ambiguous Gaussianpoints, enhancing both segmentation performance and representation compactness.Furthermore, SAD-Splat incorporates a high-confidence pseudo-label generationpipeline. It leverages 2D foundation models to enhance supervision whenground-truth labels are limited, thereby further improving segmentationaccuracy. To advance research in this domain, we introduce a challengingbenchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diversereal-world aerial scenes with sparse annotations. Experimental resultsdemonstrate that SAD-Splat achieves an excellent balance between segmentationaccuracy and representation compactness. It offers an efficient and scalablesolution for 3D aerial scene understanding.</description>
      <author>example@mail.com (Xu Tang, Junan Jia, Yijing Wang, Jingjing Ma, Xiangrong Zhang)</author>
      <guid isPermaLink="false">2508.09626v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation</title>
      <link>http://arxiv.org/abs/2508.09987v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了GPT-4o生成合成图像的优势，并提出了Echo-4o-Image数据集和Echo-4o模型，同时开发了两个新的评估基准，证明了合成数据在补充罕见场景和提供可控监督方面的价值。&lt;h4&gt;背景&lt;/h4&gt;GPT-4o因其强大的图像生成能力而受到广泛关注，但开源模型仍落后于它。已有研究探索从GPT-4o中蒸馏图像数据来增强开源模型，但存在一个关键问题：既然现实世界图像数据已是高质量数据来源，为何还需使用GPT-4o生成的合成数据？&lt;h4&gt;目的&lt;/h4&gt;识别合成图像的关键优势，构建基于GPT-4o的合成数据集，提出新的评估基准，并验证合成数据对图像生成模型的提升效果。&lt;h4&gt;方法&lt;/h4&gt;识别出合成图像的两个关键优势：1) 可补充现实数据集中罕见场景，如超现实主义幻想或多参考图像生成；2) 提供干净可控的监督，避免现实数据中的背景噪声和文本-图像错位。基于此，构建了18万规模的Echo-4o-Image合成数据集，微调Bagel模型获得Echo-4o，并提出两个新评估基准：GenEval++(增加指令复杂度)和Imagine-Bench(评估想象内容的理解和生成)。&lt;h4&gt;主要发现&lt;/h4&gt;Echo-4o在标准基准上表现优异；将Echo-4o-Image应用于其他基础模型(如OmniGen2、BLIP3-o)时，在多个指标上带来了一致的性能提升，证明了该数据集的强可转移性。&lt;h4&gt;结论&lt;/h4&gt;合成图像数据在补充罕见场景和提供干净可控监督方面具有独特优势，Echo-4o-Image有效解决了现实世界数据覆盖的盲点，Echo-4o模型在多个基准测试中表现良好且具有很好的可转移性。&lt;h4&gt;翻译&lt;/h4&gt;最近，GPT-4因其强大的图像生成能力而受到广泛关注，但开源模型仍然落后。几项研究已经探索从GPT-4中蒸馏图像数据来增强开源模型，取得了显著进展。然而，一个关键问题仍然存在：鉴于现实世界图像数据已经构成高质量数据的自然来源，为什么我们应该使用GPT-4生成的合成数据？在这项工作中，我们确定了合成图像的两个关键优势。首先，它们可以补充现实世界数据集中的罕见场景，如超现实主义幻想或多参考图像生成，这些场景在用户查询中经常出现。其次，它们提供干净和可控的监督。现实世界数据通常包含复杂的背景噪声和文本描述与图像内容之间的固有错位，而合成图像则提供纯背景和长尾监督信号，促进更准确的文本到图像对齐。基于这些见解，我们引入了Echo-4o-Image，这是一个由GPT-4生成的18万规模的合成数据集，利用合成图像数据的力量来解决现实世界覆盖中的盲点。使用这个数据集，我们微调了统一的多模态生成基线Bagel以获得Echo-4o。此外，我们提出了两个新的评估基准，以更准确和具有挑战性地评估图像生成能力：GenEval++通过增加指令复杂度来减轻分数饱和，而Imagine-Bench专注于评估想象内容的理解和生成。Echo-4o在标准基准上表现出强大的性能。此外，将Echo-4o-Image应用于其他基础模型(如OmniGen2、BLIP3-o)在多个指标上带来了一致的性能提升，突显了该数据集的强可转移性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, GPT-4o has garnered significant attention for its strongperformance in image generation, yet open-source models still lag behind.Several studies have explored distilling image data from GPT-4o to enhanceopen-source models, achieving notable progress. However, a key questionremains: given that real-world image datasets already constitute a naturalsource of high-quality data, why should we use GPT-4o-generated synthetic data?In this work, we identify two key advantages of synthetic images. First, theycan complement rare scenarios in real-world datasets, such as surreal fantasyor multi-reference image generation, which frequently occur in user queries.Second, they provide clean and controllable supervision. Real-world data oftencontains complex background noise and inherent misalignment between textdescriptions and image content, whereas synthetic images offer pure backgroundsand long-tailed supervision signals, facilitating more accurate text-to-imagealignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scalesynthetic dataset generated by GPT-4o, harnessing the power of synthetic imagedata to address blind spots in real-world coverage. Using this dataset, wefine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.In addition, we propose two new evaluation benchmarks for a more accurate andchallenging assessment of image generation capabilities: GenEval++, whichincreases instruction complexity to mitigate score saturation, andImagine-Bench, which focuses on evaluating both the understanding andgeneration of imaginative content. Echo-4o demonstrates strong performanceacross standard benchmarks. Moreover, applying Echo-4o-Image to otherfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gainsacross multiple metrics, highlighting the datasets strong transferability.</description>
      <author>example@mail.com (Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, Conghui He, Weijia Li)</author>
      <guid isPermaLink="false">2508.09987v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation</title>
      <link>http://arxiv.org/abs/2508.09977v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  GitHub Repo:  https://github.com/heshuting555/Awesome-3DGS-Applications&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于3D高斯溅射(3DGS)应用的综述，全面介绍了其在各种下游任务中的应用进展，包括分割、编辑和生成等，并提供了常用数据集和评估方法的总结。&lt;h4&gt;背景&lt;/h4&gt;3D高斯溅射(3DGS)最近作为一种强大的神经辐射场(NeRF)替代方案出现，用于3D场景表示，提供高保真度的照片级真实感渲染和实时性能。&lt;h4&gt;目的&lt;/h4&gt;提供对3DGS应用最新进展的全面概述，支持需要几何和语义理解的各种下游应用。&lt;h4&gt;方法&lt;/h4&gt;介绍支持3DGS应用中语义理解和控制的2D基础模型；回顾基于NeRF的方法及其对3DGS对应方法的启示；将3DGS应用分为分割、编辑、生成和其他功能任务；总结代表性方法、监督策略和学习范式；强调共享设计原则和新兴趋势；总结常用数据集和评估协议；提供方法比较分析；维护资源库。&lt;h4&gt;主要发现&lt;/h4&gt;3DGS明确且紧凑的性质使其能够支持广泛的下游应用；在不同应用类别中存在共享的设计原则。&lt;h4&gt;结论&lt;/h4&gt;维护了一个持续更新的资源库，网址为https://github.com/heshuting555/Awesome-3DGS-Applications，以支持正在进行的研究和开发。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯溅射(3DGS)最近作为一种强大的神经辐射场(NeRF)替代方案出现，用于3D场景表示，提供高保真度的照片级真实感渲染和实时性能。除了新颖视图合成外，3DGS明确且紧凑的性质使其能够支持需要几何和语义理解的广泛下游应用。本调查提供了3DGS应用最新进展的全面概述。它首先介绍支持3DGS应用中语义理解和控制的2D基础模型，然后回顾基于NeRF的方法及其对3DGS对应方法的启示。我们将3DGS应用分为分割、编辑、生成和其他功能任务。对于每个类别，我们总结了代表性方法、监督策略和学习范式，突出了共享的设计原则和新兴趋势。同时还总结了常用的数据集和评估协议，以及最近方法在公共基准上的比较分析。为了支持正在进行的研究和开发，我们维护了一个不断更新的论文、代码和资源库，网址为https://github.com/heshuting555/Awesome-3DGS-Applications。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是3D高斯溅射(3DGS)在下游应用领域的系统性梳理和分类问题。虽然已有一些关于3DGS的综述，但它们主要关注全局分类、实时渲染流水线或压缩策略，而对3DGS驱动的下游应用缺乏深入分析。这个问题在现实中很重要，因为3DGS的显式和紧凑特性使其能够支持需要几何和语义理解的广泛应用，包括虚拟现实、机器人、自主导航和城市映射等领域，通过系统分析这些应用可以促进3DGS技术的发展和应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作为综述论文，作者不是设计新方法而是系统梳理现有研究。作者首先介绍3DGS基本概念和应用背景，然后讨论支持3DGS的2D基础模型(如DINO、CLIP、SAM)和相关NeRF研究，接着按任务类型(分割、编辑、生成)分类分析应用，最后总结数据集、评估协议和未来方向。作者借鉴了大量现有研究成果，特别是2D基础模型和NeRF方法，为3DGS提供概念连续性和技术基础。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这篇综述论文的核心思想是对3DGS在分割、编辑和生成等下游应用领域的最新进展进行系统性分类、比较和分析。整体流程包括：1)引言介绍背景和贡献；2)背景介绍问题表述、分类和相关技术；3)详细讨论三大类应用任务；4)总结评估协议和性能比较；5)讨论挑战和未来方向；6)结论总结关键见解。这种结构化方式帮助读者全面理解3DGS应用现状和发展趋势。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)专注性 - 首个专门针对3DGS下游应用的综述；2)系统性 - 对三大应用方向进行系统分类；3)全面性 - 总结代表性方法、监督策略和学习范式；4)实用性 - 提供数据集、评估协议和性能比较；5)资源支持 - 维护持续更新的资源库。相比之前工作，这篇论文更深入探讨3DGS在高层次视觉和图形任务中的潜力，提供更全面系统的视角，而非仅关注渲染效率或压缩技术。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次系统性地综述了3D高斯溅射在分割、编辑和生成等下游应用领域的最新进展，为研究人员提供了全面的技术分析和未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternativeto Neural Radiance Fields (NeRF) for 3D scene representation, offeringhigh-fidelity photorealistic rendering with real-time performance. Beyond novelview synthesis, the explicit and compact nature of 3DGS enables a wide range ofdownstream applications that require geometric and semantic understanding. Thissurvey provides a comprehensive overview of recent progress in 3DGSapplications. It first introduces 2D foundation models that support semanticunderstanding and control in 3DGS applications, followed by a review ofNeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGSapplications into segmentation, editing, generation, and other functionaltasks. For each, we summarize representative methods, supervision strategies,and learning paradigms, highlighting shared design principles and emergingtrends. Commonly used datasets and evaluation protocols are also summarized,along with comparative analyses of recent methods across public benchmarks. Tosupport ongoing research and development, a continually updated repository ofpapers, code, and resources is maintained athttps://github.com/heshuting555/Awesome-3DGS-Applications.</description>
      <author>example@mail.com (Shuting He, Peilin Ji, Yitong Yang, Changshuo Wang, Jiayi Ji, Yinglin Wang, Henghui Ding)</author>
      <guid isPermaLink="false">2508.09977v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification</title>
      <link>http://arxiv.org/abs/2508.09967v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Meta-Optimized Classifier (MOC)的新方法，用于提高组织病理学视觉语言基础模型在整张切片图像分类任务中的性能，特别是在少样本学习场景下。&lt;h4&gt;背景&lt;/h4&gt;组织病理学视觉语言基础模型通过零样本适应解决了整张切片图像分类的数据稀缺问题，但仍被传统多实例学习方法超越。现有少样本方法虽能提高诊断准确性，但依赖传统分类器设计，对数据稀缺存在脆弱性。&lt;h4&gt;目的&lt;/h4&gt;解决现有少样本方法对数据稀缺的脆弱性问题，提高基于视觉语言基础模型的整张切片图像分类性能，特别是在临床部署中训练数据严重有限的情况下。&lt;h4&gt;方法&lt;/h4&gt;提出Meta-Optimized Classifier (MOC)，包含两个核心组件：(1)一个元学习器，能从候选分类器混合中自动优化分类器配置；(2)一个分类器库，包含多样化候选分类器，实现全面病理解释。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，MOC在多个少样本基准测试中优于先前方法。在TCGA-NSCLC基准测试中，MOC比最先进的少样本VLFM方法提高10.4%的AUC，在1-shot条件下增益高达26.25%。&lt;h4&gt;结论&lt;/h4&gt;MOC为临床部署提供了关键进展，特别是在诊断训练数据严重有限的情况下，代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;组织病理学视觉语言基础模型的最新进展已显示出通过零样本适应解决整张切片图像分类数据稀缺问题的前景。然而，这些方法仍然被在大型数据集上训练的传统多实例学习方法所超越，促使近期努力通过少样本学习范式增强基于VLFM的WSI分类。虽然现有少样本方法通过有限的标注提高了诊断准确性，但它们对传统分类器设计的依赖引入了对数据稀缺的关键脆弱性。为解决这个问题，我们提出了一种元优化分类器，包含两个核心组件：(1)一个元学习器，能从候选分类器混合中自动优化分类器配置；(2)一个分类器库，包含多样化的候选分类器，能够实现全面的病理解释。大量实验证明，MOC在多个少样本基准测试中优于先前方法。值得注意的是，在TCGA-NSCLC基准测试中，MOC比最先进的少样本VLFM方法提高了10.4%的AUC，在1-shot条件下增益高达26.25%，为诊断训练数据严重有限的临床部署提供了关键进展。代码可在https://github.com/xmed-lab/MOC获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in histopathology vision-language foundation models (VLFMs)have shown promise in addressing data scarcity for whole slide image (WSI)classification via zero-shot adaptation. However, these methods remainoutperformed by conventional multiple instance learning (MIL) approachestrained on large datasets, motivating recent efforts to enhance VLFM-based WSIclassification through fewshot learning paradigms. While existing few-shotmethods improve diagnostic accuracy with limited annotations, their reliance onconventional classifier designs introduces critical vulnerabilities to datascarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)comprising two core components: (1) a meta-learner that automatically optimizesa classifier configuration from a mixture of candidate classifiers and (2) aclassifier bank housing diverse candidate classifiers to enable a holisticpathological interpretation. Extensive experiments demonstrate that MOCoutperforms prior arts in multiple few-shot benchmarks. Notably, on theTCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-artfew-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,offering a critical advancement for clinical deployments where diagnostictraining data is severely limited. Code is available athttps://github.com/xmed-lab/MOC.</description>
      <author>example@mail.com (Tianqi Xiang, Yi Li, Qixiang Zhang, Xiaomeng Li)</author>
      <guid isPermaLink="false">2508.09967v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Multi-head committees enable direct uncertainty prediction for atomistic foundation models</title>
      <link>http://arxiv.org/abs/2508.09907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures in main article + supporting information&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用MACE模型及其多头机制实现了神经网络势能的委员会预测，通过预测的标准差来估计模型的不确定性。该方法在多个数据集上表现出良好的不确定性估计能力，并成功应用于基础模型，通过主动学习将训练集缩减到原来的5%，同时保持了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;机器学习势能已成为原子级材料建模的标准工具。尽管模型变得越来越通用，但在主动学习和稳健误差分析方面，如何高效预测不确定性仍是一个开放的挑战。&lt;h4&gt;目的&lt;/h4&gt;研究旨在实现一种能够有效预测不确定性的机器学习势能模型，以支持主动学习和稳健误差分析。&lt;h4&gt;方法&lt;/h4&gt;研究利用MACE及其多头机制实现了一个用于消息传递架构的委员会神经网络势能模型。委员会由多个连接到相同原子环境描述符的输出模块组成。预测的标准差作为模型不确定性的估计。研究者将这一概念应用于基础模型MACE-MP-0，仅训练新添加的输出头，同时保持模型其余部分固定。&lt;h4&gt;主要发现&lt;/h4&gt;1. 力预测的不确定性与真实误差在多个数据集上表现出良好的相关性；2. 将委员会概念应用于基础模型MACE-MP-0；3. 通过主动学习将基础模型的训练集缩减到原来的5%；4. 在缩减的训练集上训练的多头委员会模型能够提供可靠的不确定性估计，而不会显著降低预测准确性。&lt;h4&gt;结论&lt;/h4&gt;通过利用MACE的多头机制实现的委员会神经网络势能模型可以有效预测不确定性，支持主动学习流程，并能在大幅缩减训练集的情况下保持预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;机器学习势能已成为原子级材料建模的标准工具。虽然模型继续变得更加通用，但一个开放的挑战是如何为主动学习和稳健误差分析高效预测不确定性。在这项工作中，我们利用MACE及其多头机制实现了用于消息传递架构的委员会神经网络势能模型，其中委员会由多个连接到相同原子环境描述符的输出模块组成。与传统独立网络的委员会一样，预测的标准差可作为模型不确定性的估计。我们在多个自建模型数据集上表明，力预测的不确定性与真实误差有良好的相关性。随后，我们将这一概念应用于基础模型，特别是MACE-MP-0，在其中我们仅训练新添加的输出头，同时保持模型的其余部分固定。我们在主动学习工作流程中使用这种方法，将基础模型的训练集压缩到其原始大小的5%。在缩减训练集上训练的基础模型多头委员会能够提供可靠的不确定性估计，而不会显著降低预测准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning potentials have become a standard tool for atomisticmaterials modelling. While models continue to become more generalisable, anopen challenge relates to efficient uncertainty predictions for active learningand robust error analysis. In this work, we utilise MACE and its multi-headmechanism to implement a committee neural network potential for message-passingarchitectures, where the committee comprises multiple output modules attachedto the same atomic environment descriptors. As with traditional committees ofindependent networks, the standard deviation of the predictions functions as anestimate of the model's uncertainty. We show for a range of datasets incustom-build models that the uncertainty of the force predictions correlateswell with the true errors. We subsequently apply this concept to foundationmodels, specifically MACE-MP-0, where we train only the newly attached outputheads while keeping the remaining part of the model fixed. We use this approachin an active learning workflow to condense the training set of the foundationmodel to just 5\% of its original size. The foundation model multi-headcommittee trained on the condensed training set enables reliable uncertaintyestimation without any substantial decrease in prediction accuracy.</description>
      <author>example@mail.com (Hubert Beck, Pavol Simko, Lars L. Schaaf, Ondrej Marsalek, Christoph Schran)</author>
      <guid isPermaLink="false">2508.09907v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?</title>
      <link>http://arxiv.org/abs/2508.09888v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了现代人工神经网络在田间尺度预测土壤建模任务中的性能，发现现代ANN架构在大多数任务上优于传统机器学习方法，其中TabPFN表现最佳且具有稳健性。&lt;h4&gt;背景&lt;/h4&gt;在土壤计量学领域，表格机器学习是预测土壤性质的主要方法，但田间尺度PSM任务通常受限于小样本量和土壤光谱中的高特征样本比率，这对传统深度学习方法构成挑战。&lt;h4&gt;目的&lt;/h4&gt;评估现代人工神经网络架构在田间尺度PSM任务上的适用性，挑战古典机器学习算法作为默认选择的观点。&lt;h4&gt;方法&lt;/h4&gt;引入全面的基准测试，评估包括多层感知器模型、注意力Transformer变体、检索增强方法和上下文学习基础模型在内的31种先进ANN架构，在31个田间和农场规模数据集上测试三种关键土壤属性。&lt;h4&gt;主要发现&lt;/h4&gt;现代ANN在大多数任务上始终优于传统方法，深度学习已足够成熟以克服古典机器学习的长期主导地位，TabPFN表现最佳且具有稳健性。&lt;h4&gt;结论&lt;/h4&gt;建议在田间尺度PSM中采用现代ANN，并将TabPFN作为每位土壤计量学家工具包中的新默认选择。&lt;h4&gt;翻译&lt;/h4&gt;在土壤计量学领域，表格机器学习是从远程和近端土壤传感数据预测土壤性质的主要方法，构成了数字土壤图的核心组成部分。在田间尺度上，这种预测土壤建模(PSM)任务通常受限于土壤光谱中的小训练样本量和高特征样本比率。传统上，这些条件已被证明对传统深度学习方法构成挑战。经典机器学习算法，特别是像随机森林这样的树模型和偏最小二乘回归等线性模型，长期以来一直是田间尺度PSM的默认选择。用于表格数据的人工神经网络(ANN)的最新进展挑战了这一观点，但它们在田间尺度PSM中的适用性尚未得到证实。我们引入了一个全面的基准测试，评估了最先进的ANN架构，包括最新的多层感知器(MLP)模型(TabM, RealMLP)、基于注意力的Transformer变体(FT-Transformer, ExcelFormer, T2G-Former, AMFormer)、检索增强方法(TabR, ModernNCA)以及上下文学习基础模型(TabPFN)。我们的评估涵盖31个田间和农场规模的数据集，包含30到460个样本，以及三种关键的土壤性质：土壤有机物或土壤有机碳、pH值和粘土含量。我们的结果显示，现代ANN在大多数任务上始终优于传统方法，证明了深度学习已经足够成熟，可以克服古典机器学习在PSM上的长期主导地位。值得注意的是，TabPFN提供了最强的整体性能，显示出在不同条件下的稳健性。因此，我们建议在田间尺度PSM中采用现代ANN，并提议TabPFN作为每位土壤计量学家工具包中的新默认选择。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the field of pedometrics, tabular machine learning is the predominantmethod for predicting soil properties from remote and proximal soil sensingdata, forming a central component of digital soil mapping. At the field-scale,this predictive soil modeling (PSM) task is typically constrained by smalltraining sample sizes and high feature-to-sample ratios in soil spectroscopy.Traditionally, these conditions have proven challenging for conventional deeplearning methods. Classical machine learning algorithms, particularlytree-based models like Random Forest and linear models such as Partial LeastSquares Regression, have long been the default choice for field-scale PSM.Recent advances in artificial neural networks (ANN) for tabular data challengethis view, yet their suitability for field-scale PSM has not been proven. Weintroduce a comprehensive benchmark that evaluates state-of-the-art ANNarchitectures, including the latest multilayer perceptron (MLP)-based models(TabM, RealMLP), attention-based transformer variants (FT-Transformer,ExcelFormer, T2G-Former, AMFormer), retrieval-augmented approaches (TabR,ModernNCA), and an in-context learning foundation model (TabPFN). Ourevaluation encompasses 31 field- and farm-scale datasets containing 30 to 460samples and three critical soil properties: soil organic matter or soil organiccarbon, pH, and clay content. Our results reveal that modern ANNs consistentlyoutperform classical methods on the majority of tasks, demonstrating that deeplearning has matured sufficiently to overcome the long-standing dominance ofclassical machine learning for PSM. Notably, TabPFN delivers the strongestoverall performance, showing robustness across varying conditions. We thereforerecommend the adoption of modern ANNs for field-scale PSM and propose TabPFN asthe new default choice in the toolkit of every pedometrician.</description>
      <author>example@mail.com (Viacheslav Barkov, Jonas Schmidinger, Robin Gebbers, Martin Atzmueller)</author>
      <guid isPermaLink="false">2508.09888v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Speed Always Wins: A Survey on Efficient Architectures for Large Language Models</title>
      <link>http://arxiv.org/abs/2508.09834v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Survey, 82 pages, GitHub:  https://github.com/weigao266/Awesome-Efficient-Arch&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了创新的大型语言模型(LLM)架构，旨在解决传统Transformer模型的局限性并提高效率，涵盖了从语言建模到多模态应用的多种技术方法。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)在语言理解、生成和推理方面取得了显著成果，推动了多模态模型的能力边界。Transformer模型作为现代LLMs的基础，提供了具有优秀扩展性的强基线，但传统Transformer架构需要大量计算，对大规模训练和实际部署构成显著障碍。&lt;h4&gt;目的&lt;/h4&gt;系统性地检查创新的LLM架构，解决Transformer的固有局限性，提高模型效率，并为开发可扩展、资源感知的基础模型提供更广泛的视角。&lt;h4&gt;方法&lt;/h4&gt;从语言建模开始，涵盖线性序列建模方法和稀疏序列建模方法的背景和技术细节，讨论高效的全注意力变体，探索稀疏混合专家模型，研究结合上述技术的混合模型架构，以及新兴的扩散LLMs，同时讨论这些技术在其他模态中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;通过将近期研究分组到不同类别，展示了现代高效LLM架构的蓝图，为未来研究提供了方向。&lt;h4&gt;结论&lt;/h4&gt;希望这篇综述能够激励未来朝着更高效、多功能的AI系统的研究，推动资源感知的基础模型发展。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)在语言理解、生成和推理方面取得了令人印象深刻的结果，并推动了多模态模型的能力边界。Transformer模型作为现代LLMs的基础，提供了具有优秀扩展性的强基线。然而，传统Transformer架构需要大量计算，并对大规模训练和实际部署构成显著障碍。在本综述中，我们系统地检查了创新的LLM架构，解决了Transformer的固有局限性并提高了效率。从语言建模开始，本综述涵盖了线性序列建模方法和稀疏序列建模方法的背景和技术细节，高效的全注意力变体，稀疏混合专家模型，结合上述技术的混合模型架构，以及新兴的扩散LLMs。此外，我们讨论了这些技术在其他模态中的应用，并考虑了它们对开发可扩展、资源感知的基础模型的更广泛影响。通过将近期研究分组到上述类别，本综述展示了现代高效LLM架构的蓝图，我们希望这能够帮助激励未来朝着更高效、多功能的AI系统的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have delivered impressive results in languageunderstanding, generation, reasoning, and pushes the ability boundary ofmultimodal models. Transformer models, as the foundation of modern LLMs, offera strong baseline with excellent scaling properties. However, the traditionaltransformer architecture requires substantial computations and posessignificant obstacles for large-scale training and practical deployment. Inthis survey, we offer a systematic examination of innovative LLM architecturesthat address the inherent limitations of transformers and boost the efficiency.Starting from language modeling, this survey covers the background andtechnical details of linear and sparse sequence modeling methods, efficientfull attention variants, sparse mixture-of-experts, hybrid model architecturesincorporating the above techniques, and emerging diffusion LLMs. Additionally,we discuss applications of these techniques to other modalities and considertheir wider implications for developing scalable, resource-aware foundationmodels. By grouping recent studies into the above category, this surveypresents a blueprint of modern efficient LLM architectures, and we hope thiscould help motivate future research toward more efficient, versatile AIsystems.</description>
      <author>example@mail.com (Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng)</author>
      <guid isPermaLink="false">2508.09834v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>BeatFM: Improving Beat Tracking with Pre-trained Music Foundation Model</title>
      <link>http://arxiv.org/abs/2508.09790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by ICME2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了BeatFM，一种基于预训练音乐基础模型的节拍跟踪方法，通过多维度语义聚合模块提高了在不同音乐风格中的节拍跟踪性能，实验证明其达到了最先进水平。&lt;h4&gt;背景&lt;/h4&gt;当前节拍跟踪方法面临标注数据稀缺的挑战，这限制了它们在不同音乐风格中的泛化能力，以及准确捕捉复杂节奏结构的能力。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些挑战，研究者提出了一种新的节拍跟踪范式BeatFM，通过引入预训练的音乐基础模型，并利用其丰富的语义知识来提高节拍跟踪性能。&lt;h4&gt;方法&lt;/h4&gt;1. 提出BeatFM，一种新的节拍跟踪范式；2. 引入预训练的音乐基础模型；3. 设计了一个即插即用的多维度语义聚合模块，包含三个并行子模块：时域语义聚合子模块、频域语义聚合子模块和通道域语义聚合子模块。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该方法在多个基准数据集的节拍和下拍跟踪方面取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;预训练在多样化音乐数据集上的音乐基础模型使模型能够对音乐有稳健的理解，从而有效解决了节拍跟踪中的挑战。&lt;h4&gt;翻译&lt;/h4&gt;节拍跟踪是音乐信息检索中广泛研究的主题。然而，由于标注数据的稀缺，当前的节拍跟踪方法面临着挑战，这限制了它们在不同音乐风格中的泛化能力以及准确捕捉复杂节奏结构的能力。为了克服这些挑战，我们提出了一种新颖的节拍跟踪范式BeatFM，它引入了预训练的音乐基础模型，并利用其丰富的语义知识来提高节拍跟踪性能。在多样化音乐数据集上进行预训练使音乐基础模型能够对音乐有稳健的理解，从而有效地解决了这些挑战。为了进一步使其适应节拍跟踪，我们设计了一个即插即用的多维度语义聚合模块，它由三个并行子模块组成，分别专注于时域、频域和通道域的语义聚合。大量实验证明，我们的方法在多个基准数据集的节拍和下拍跟踪方面取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Beat tracking is a widely researched topic in music information retrieval.However, current beat tracking methods face challenges due to the scarcity oflabeled data, which limits their ability to generalize across diverse musicalstyles and accurately capture complex rhythmic structures. To overcome thesechallenges, we propose a novel beat tracking paradigm BeatFM, which introducesa pre-trained music foundation model and leverages its rich semantic knowledgeto improve beat tracking performance. Pre-training on diverse music datasetsendows music foundation models with a robust understanding of music, therebyeffectively addressing these challenges. To further adapt it for beat tracking,we design a plug-and-play multi-dimensional semantic aggregation module, whichis composed of three parallel sub-modules, each focusing on semanticaggregation in the temporal, frequency, and channel domains, respectively.Extensive experiments demonstrate that our method achieves state-of-the-artperformance in beat and downbeat tracking across multiple benchmark datasets.</description>
      <author>example@mail.com (Ganghui Ru, Jieying Wang, Jiahao Zhao, Yulun Wu, Yi Yu, Nannan Jiang, Wei Wang, Wei Li)</author>
      <guid isPermaLink="false">2508.09790v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>HingeNet: A Harmonic-Aware Fine-Tuning Approach for Beat Tracking</title>
      <link>http://arxiv.org/abs/2508.09788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by ICME2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HingeNet是一种专门为节拍跟踪任务设计的参数高效微调方法，通过轻量级铰链式网络结构和谐波感知机制，有效解决了预训练基础模型在有限标注数据下的应用问题，并在基准测试中实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;微调预训练的基础模型在音乐信息检索方面取得了显著进展，但这些模型在节拍跟踪任务上的应用尚未被探索，有限的标注数据使得传统微调方法无效。&lt;h4&gt;目的&lt;/h4&gt;解决预训练基础模型在节拍跟踪任务中的应用挑战，提出一种参数高效的微调方法。&lt;h4&gt;方法&lt;/h4&gt;提出了HingeNet，一种轻量级且可分离的铰链式网络，使用预训练基础模型的中间特征表示作为输入，并在微调过程中引入谐波感知机制以更好地捕获和强调音乐信号中的谐波结构。&lt;h4&gt;主要发现&lt;/h4&gt;HingeNet具有广泛的通用性，能够有效集成各种预训练基础模型；在基准数据集上的实验表明，HingeNet在节拍和下拍跟踪方面取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;HingeNet通过创新的网络设计和谐波感知机制，解决了预训练基础模型在节拍跟踪任务中的应用问题，实现了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;微调预训练的基础模型在音乐信息检索方面取得了显著进展。然而，由于有限的标注数据使得传统微调方法无效，将这些模型应用于节拍跟踪任务仍然是一个未探索的领域。为了应对这一挑战，我们提出了HingeNet，一种专门为节拍跟踪任务设计的新型通用参数高效微调方法。HingeNet是一个轻量级且可分离的网络，视觉上类似铰链，通过使用预训练基础模型的中间特征表示作为输入，与预训练基础模型紧密接口。这种独特的架构赋予了HingeNet广泛的通用性，能够有效集成各种预训练基础模型。此外，考虑到谐波在节拍跟踪中的重要性，我们在微调过程中引入了谐波感知机制，以更好地捕获和强调音乐信号中的谐波结构。在基准数据集上的实验证明，HingeNet在节拍和下拍跟踪方面取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning pre-trained foundation models has made significant progress inmusic information retrieval. However, applying these models to beat trackingtasks remains unexplored as the limited annotated data renders conventionalfine-tuning methods ineffective. To address this challenge, we proposeHingeNet, a novel and general parameter-efficient fine-tuning methodspecifically designed for beat tracking tasks. HingeNet is a lightweight andseparable network, visually resembling a hinge, designed to tightly interfacewith pre-trained foundation models by using their intermediate featurerepresentations as input. This unique architecture grants HingeNet broadgeneralizability, enabling effective integration with various pre-trainedfoundation models. Furthermore, considering the significance of harmonics inbeat tracking, we introduce harmonic-aware mechanism during the fine-tuningprocess to better capture and emphasize the harmonic structures in musicalsignals. Experiments on benchmark datasets demonstrate that HingeNet achievesstate-of-the-art performance in beat and downbeat tracking</description>
      <author>example@mail.com (Ganghui Ru, Jieying Wang, Jiahao Zhao, Yulun Wu, Yi Yu, Nannan Jiang, Wei Wang, Wei Li)</author>
      <guid isPermaLink="false">2508.09788v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors</title>
      <link>http://arxiv.org/abs/2508.09667v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GSFixer是一个改进从稀疏视图重建的3D高斯溅射表示质量的新框架，通过参考引导的视频恢复模型解决3D重建中的伪影问题，实验证明其性能优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;从稀疏视图使用3D高斯溅射重建3D场景是一个不适定问题，由于信息不足常导致明显伪影。现有方法利用生成先验补全信息，但难以生成与输入观察保持一致的内容。&lt;h4&gt;目的&lt;/h4&gt;提出GSFixer框架，提高从稀疏输入重建的3D高斯溅射表示质量，解决3D重建中的伪影问题。&lt;h4&gt;方法&lt;/h4&gt;核心是基于DiT的视频扩散模型构建的参考引导视频恢复模型，在成对伪影3D高斯溅射渲染和干净帧上训练。模型将输入稀疏视图作为参考，集成从视觉几何基础模型提取的2D语义特征和3D几何特征，增强语义一致性和3D一致性。同时提出DL3DV-Res基准用于评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明GSFixer在3D高斯溅射伪影恢复和稀疏视图3D重建方面优于当前最先进方法。&lt;h4&gt;结论&lt;/h4&gt;GSFixer有效解决了3D高斯溅射重建中的伪影问题，提高了从稀疏输入重建的3D场景质量，并能生成与输入观察保持一致的内容。&lt;h4&gt;翻译&lt;/h4&gt;从稀疏视图使用3D高斯溅射重建3D场景是一个不适定问题，由于信息不足，通常会导致明显的伪影。虽然最近的方法试图利用生成先验来补全欠约束区域的信息，但它们难以生成与输入观察保持一致的内容。为应对这一挑战，我们提出了GSFixer，一个旨在提高从稀疏输入重建的3D高斯溅射表示质量的新框架。我们方法的核心是基于参考引导的视频恢复模型，构建在基于DiT的视频扩散模型上，该模型在成对的伪影3D高斯溅射渲染和干净帧上训练，并带有基于参考的附加条件。将输入稀疏视图作为参考，我们的模型集成了从视觉几何基础模型中提取的参考视图的2D语义特征和3D几何特征，在修复伪影新视图时增强了语义一致性和3D一致性。此外，考虑到缺乏适合的3D高斯溅射伪影恢复评估基准，我们提出了DL3DV-Res，其中包含使用低质量3D高斯溅射渲染的伪影帧。大量实验证明，我们的GSFixer在3D高斯溅射伪影恢复和稀疏视图3D重建方面优于当前最先进的方法。项目页面：https://github.com/GVCLab/GSFixer。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决使用稀疏视图进行3D高斯泼溅重建时产生的明显伪影问题。这个问题在现实中非常重要，因为获取密集的多视图数据成本高、耗时长，而3D重建在虚拟现实、自动驾驶和机器人等众多领域有广泛应用价值。稀疏视图重建能力使得这些技术能够在资源有限的环境下实现高质量3D场景重建。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3DGS在稀疏视图下的局限性，然后借鉴了ReconFusion等将扩散模型引入3D重建的工作思路。作者还参考了视频扩散模型（如CogVideoX）的成功应用，以及视觉几何基础模型（如VGGT和DINOv2）的特征提取能力。通过整合这些现有技术，作者创新性地提出同时利用参考视图的2D语义特征和3D几何特征来指导修复过程，解决了现有方法难以保持生成内容与输入观察一致性的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用参考视图的2D语义特征和3D几何特征来引导视频扩散模型修复伪影，同时保持语义一致性和3D一致性。整体流程包括：1)从稀疏视图构建初始3DGS；2)使用DINOv2和VGGT提取参考视图的2D语义和3D几何特征；3)采用参考引导的轨迹采样策略在参考视图间生成新视角；4)将伪影视图和参考特征输入视频扩散模型进行修复；5)将修复后的视图加入训练集，通过重建损失和生成损失迭代优化3DGS；6)输出高质量3D表示和新视图合成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)参考引导的视频修复模型，基于DiT架构并注入参考信息；2)双条件信号融合，同时使用2D语义特征和3D几何特征；3)参考引导的轨迹采样策略，平衡修复质量和视角多样性；4)提出DL3DV-Res基准数据集。相比之前的工作，GSFixer的主要不同在于：使用双条件信号而非单一条件，基于视频扩散模型而非仅图像扩散，采用创新的轨迹策略，同时适用于伪影修复和稀疏视图重建任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GSFixer通过引入参考引导的双条件视频扩散模型和创新的轨迹采样策略，显著提升了3D高斯泼溅在稀疏视图场景下的重建质量和新视图合成的一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse viewsis an ill-posed problem due to insufficient information, often resulting innoticeable artifacts. While recent approaches have sought to leveragegenerative priors to complete information for under-constrained regions, theystruggle to generate content that remains consistent with input observations.To address this challenge, we propose GSFixer, a novel framework designed toimprove the quality of 3DGS representations reconstructed from sparse inputs.The core of our approach is the reference-guided video restoration model, builtupon a DiT-based video diffusion model trained on paired artifact 3DGS rendersand clean frames with additional reference-based conditions. Considering theinput sparse views as references, our model integrates both 2D semanticfeatures and 3D geometric features of reference views extracted from the visualgeometry foundation model, enhancing the semantic coherence and 3D consistencywhen fixing artifact novel views. Furthermore, considering the lack of suitablebenchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res whichcontains artifact frames rendered using low-quality 3DGS. Extensive experimentsdemonstrate our GSFixer outperforms current state-of-the-art methods in 3DGSartifact restoration and sparse-view 3D reconstruction. Project page:https://github.com/GVCLab/GSFixer.</description>
      <author>example@mail.com (Xingyilang Yin, Qi Zhang, Jiahao Chang, Ying Feng, Qingnan Fan, Xi Yang, Chi-Man Pun, Huaqi Zhang, Xiaodong Cun)</author>
      <guid isPermaLink="false">2508.09667v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges</title>
      <link>http://arxiv.org/abs/2508.09561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages. 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇调查论文分析了世界模型如何赋能边缘代理AI系统(EGI)，探讨了其架构基础、在多种边缘场景中的应用、与基础模型和数字孪生的协同作用，并指出了未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;边缘通用智能(EGI)代表了边缘计算的演进，其中分布式代理能够在多样化动态环境中自主感知、推理和行动。世界模型作为核心组件，能够主动预测、想象未来轨迹、在不确定性下推理并规划多步骤行动。尽管世界模型在机器人和游戏领域已有应用，但在无线边缘与EGI的集成仍不充分。&lt;h4&gt;目的&lt;/h4&gt;这篇调查旨在填补世界模型在无线边缘与EGI集成方面的研究空白，提供世界模型如何赋能边缘代理AI系统的全面分析，并为实现下一代智能、自主边缘系统提供概念基础和实用路线图。&lt;h4&gt;方法&lt;/h4&gt;文章首先检查世界模型的架构基础，包括潜在表示学习、动态建模和基于想象的规划。然后展示这些核心能力在车联网、无人机网络、物联网系统和网络功能虚拟化等EGI场景中的应用，探讨它们与基础模型和数字孪生的协同作用，最后分析开放挑战和未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;世界模型作为主动的内部模拟器，能够增强边缘系统在延迟、能源和隐私约束下的优化能力；它们与基础模型和数字孪生协同作用，可作为EGI的认知支柱；当前面临安全保证、高效训练和受限部署等开放挑战。&lt;h4&gt;结论&lt;/h4&gt;这篇调查为世界模型在边缘通用智能中的应用提供了全面分析，包括架构基础、应用场景、协同作用和未来挑战，为实现下一代智能、自主边缘系统提供了概念基础和实用路线图。&lt;h4&gt;翻译&lt;/h4&gt;边缘通用智能(EGI)代表了边缘计算的变革性演进，其中分布式代理能够在多样化、动态的环境中自主感知、推理和行动。这一愿景的核心是世界模型，它们作为主动的内部模拟器，不仅能预测，还能主动想象未来轨迹、在不确定性下推理、并有远见地规划多步骤行动。这种主动性使代理能够预期潜在结果并在真实世界交互前优化决策。尽管机器人和游戏领域的前期工作已经展示了世界模型的潜力，但它们在无线边缘与EGI的集成仍未得到充分探索。这篇调查通过提供世界模型如何赋能边缘代理AI系统的全面分析来填补这一空白。我们首先检查世界模型的架构基础，包括潜在表示学习、动态建模和基于想象的规划。基于这些核心能力，我们说明了它们在车联网、无人机网络、物联网系统和网络功能虚拟化等EGI场景中的主动应用，从而展示了它们如何在延迟、能源和隐私约束下增强优化。然后我们探讨了它们与基础模型和数字孪生的协同作用，将世界模型定位为EGI的认知支柱。最后，我们强调了开放挑战，如安全保证、高效训练和受限部署，并概述了未来研究方向。这篇调查为实现下一代智能、自主边缘系统提供了概念基础和实用路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Edge General Intelligence (EGI) represents a transformative evolution of edgecomputing, where distributed agents possess the capability to perceive, reason,and act autonomously across diverse, dynamic environments. Central to thisvision are world models, which act as proactive internal simulators that notonly predict but also actively imagine future trajectories, reason underuncertainty, and plan multi-step actions with foresight. This proactive natureallows agents to anticipate potential outcomes and optimize decisions ahead ofreal-world interactions. While prior works in robotics and gaming haveshowcased the potential of world models, their integration into the wirelessedge for EGI remains underexplored. This survey bridges this gap by offering acomprehensive analysis of how world models can empower agentic artificialintelligence (AI) systems at the edge. We first examine the architecturalfoundations of world models, including latent representation learning, dynamicsmodeling, and imagination-based planning. Building on these core capabilities,we illustrate their proactive applications across EGI scenarios such asvehicular networks, unmanned aerial vehicle (UAV) networks, the Internet ofThings (IoT) systems, and network functions virtualization, therebyhighlighting how they can enhance optimization under latency, energy, andprivacy constraints. We then explore their synergy with foundation models anddigital twins, positioning world models as the cognitive backbone of EGI.Finally, we highlight open challenges, such as safety guarantees, efficienttraining, and constrained deployment, and outline future research directions.This survey provides both a conceptual foundation and a practical roadmap forrealizing the next generation of intelligent, autonomous edge systems.</description>
      <author>example@mail.com (Changyuan Zhao, Guangyuan Liu, Ruichen Zhang, Yinqiu Liu, Jiacheng Wang, Jiawen Kang, Dusit Niyato, Zan Li, Xuemin, Shen, Zhu Han, Sumei Sun, Chau Yuen, Dong In Kim)</author>
      <guid isPermaLink="false">2508.09561v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks</title>
      <link>http://arxiv.org/abs/2508.09532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分层联邦微调框架，用于车联网系统中资源感知和移动弹性学习，通过LoRA和UCB-DUAL算法优化了精度与效率的权衡。&lt;h4&gt;背景&lt;/h4&gt;联邦微调是使基础模型适应边缘环境中多样化下游任务的有前途方法，但在车联网系统中，由于客户端移动性、异构资源和间歇性连接，实现高效低延迟的多任务适应特别具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一个协调路边单元和车辆的分层联邦微调框架，以支持动态车联网场景下的资源感知和移动弹性学习。&lt;h4&gt;方法&lt;/h4&gt;利用低秩适应(LoRA)引入去中心化、能耗感知的秩适应机制，将其表述为约束多臂老虎机问题；开发UCB-DUAL算法使每个任务在能源预算下进行自适应探索，实现次线性遗憾。&lt;h4&gt;主要发现&lt;/h4&gt;在基于真实轨迹构建的大规模车联网模拟器上进行的实验表明，该方法在所有基线中实现了最佳精度-效率权衡，将延迟减少了24%以上，将平均精度提高了2.5%以上。&lt;h4&gt;结论&lt;/h4&gt;该分层联邦微调框架能够有效处理车联网系统中的资源约束和移动性问题，同时保持高精度和低延迟。&lt;h4&gt;翻译&lt;/h4&gt;联邦微调已成为一种有前景的方法，用于在边缘环境中使基础模型适应多样化的下游任务。在车联网系统中，由于客户端移动性、异构资源和间歇性连接，实现高效低延迟的多任务适应特别具有挑战性。本文提出了一种分层联邦微调框架，协调路边单元和车辆，以支持动态车联网场景下的资源感知和移动弹性学习。利用低秩适应(LoRA)，我们引入了一种去中心化、能耗感知的秩适应机制，将其表述为约束多臂老虎机问题。开发了一种新的UCB-DUAL算法，使每个任务在能源预算下能够进行自适应探索，实现了可证明的次线性遗憾。为了评估我们的方法，我们构建了一个基于真实轨迹的大规模车联网模拟器，捕捉动态参与、路边单元切换和通信变化性。大量实验表明，我们的方法在所有基线中实现了最佳精度-效率权衡，将延迟减少了24%以上，并将平均精度提高了2.5%以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated fine-tuning has emerged as a promising approach for adaptingfoundation models (FMs) to diverse downstream tasks in edge environments. InInternet of Vehicles (IoV) systems, enabling efficient and low-latencymulti-task adaptation is particularly challenging due to client mobility,heterogeneous resources, and intermittent connectivity. This paper proposes ahierarchical federated fine-tuning framework that coordinates roadside units(RSUs) and vehicles to support resource-aware and mobility-resilient learningacross dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), weintroduce a decentralized, energy-aware rank adaptation mechanism formulated asa constrained multi-armed bandit problem. A novel UCB-DUAL algorithm isdeveloped to enable adaptive exploration under per-task energy budgets,achieving provable sublinear regret. To evaluate our method, we construct alarge-scale IoV simulator based on real-world trajectories, capturing dynamicparticipation, RSU handoffs, and communication variability. Extensiveexperiments show that our approach achieves the best accuracy-efficiencytrade-off among all baselines, reducing latency by over 24\% and improvingaverage accuracy by more than 2.5\%.</description>
      <author>example@mail.com (Bokeng Zheng, Jianqiang Zhong, Jiayi Liu, Xiaoxi Zhang)</author>
      <guid isPermaLink="false">2508.09532v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Large-Small Model Collaborative Framework for Federated Continual Learning</title>
      <link>http://arxiv.org/abs/2508.09489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种联邦持续学习框架，通过轻量级本地模型作为动态桥梁，解决基础模型在本地下游任务上表现不佳以及学习新任务时忘记先验知识的问题。&lt;h4&gt;背景&lt;/h4&gt;持续学习对于基础模型是一个重要但尚未充分探索的挑战，特别是在联邦持续学习中，客户端面临严格的数据和通信限制。基础模型虽然具有强大的泛化能力，但无法有效利用私有本地数据，且由于参数量大和模型复杂度高，难以在不忘记先验知识的情况下学习新任务。&lt;h4&gt;目的&lt;/h4&gt;弥合小型模型和基础模型之间的差距，提出一种协作框架使基础模型能够有效利用本地数据并持续学习。&lt;h4&gt;方法&lt;/h4&gt;提出FCL中的第一个协作框架，其中轻量级本地模型作为动态桥梁，不断适应新任务同时增强大型模型的效用。包含两个新组件：小模型持续微调(防止小型模型随时间遗忘)和逐一蒸馏(在服务器上执行异构本地知识的个性化融合)。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果证明了该框架的优越性能，即使在使用异构小型模型的客户端情况下也能取得良好效果。&lt;h4&gt;结论&lt;/h4&gt;该协作框架有效解决了联邦持续学习中基础模型面临的挑战，使基础模型能够利用本地数据并持续学习而不忘记先验知识。&lt;h4&gt;翻译&lt;/h4&gt;基础模型(FMs)的持续学习(CL)是一个重要但尚未充分探索的挑战，特别是在联邦持续学习(FCL)中，每个客户端在严格的数据和通信约束下，从私有、演化的任务流中学习。尽管基础模型具有强大的泛化能力，但它们在本地下游任务上通常表现不佳，因为它们无法利用私有本地数据。此外，使基础模型学习新任务而不忘记先验知识本质上是一个具有挑战性的问题，主要由于其巨大的参数量和高模型复杂度。相比之下，小型模型可以在资源受限条件下进行本地训练，并受益于更成熟的持续学习技术。为了弥合小型模型和基础模型之间的差距，我们提出了FCL中的第一个协作框架，其中轻量级本地模型作为动态桥梁，不断适应新任务同时增强大型模型的效用。还包括两个新组件：小模型持续微调用于防止小型模型随时间遗忘；逐一蒸馏在服务器上执行异构本地知识的个性化融合。实验结果证明了其优越的性能，即使客户端使用异构的小型模型也是如此。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual learning (CL) for Foundation Models (FMs) is an essential yetunderexplored challenge, especially in Federated Continual Learning (FCL),where each client learns from a private, evolving task stream under strict dataand communication constraints. Despite their powerful generalization abilities,FMs often exhibit suboptimal performance on local downstream tasks, as they areunable to utilize private local data. Furthermore, enabling FMs to learn newtasks without forgetting prior knowledge is inherently a challenging problem,primarily due to their immense parameter count and high model complexity. Incontrast, small models can be trained locally under resource-constrainedconditions and benefit from more mature CL techniques. To bridge the gapbetween small models and FMs, we propose the first collaborative framework inFCL, where lightweight local models act as a dynamic bridge, continuallyadapting to new tasks while enhancing the utility of the large model. Two novelcomponents are also included: Small Model Continual Fine-tuning is forpreventing small models from temporal forgetting; One-by-One Distillationperforms personalized fusion of heterogeneous local knowledge on the server.Experimental results demonstrate its superior performance, even when clientsutilize heterogeneous small models.</description>
      <author>example@mail.com (Hao Yu, Xin Yang, Boyang Fan, Xuemei Cao, Hanlin Gu, Lixin Fan, Qiang Yang)</author>
      <guid isPermaLink="false">2508.09489v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models</title>
      <link>http://arxiv.org/abs/2508.09471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EGGS-PTP是一种基于扩展图引导的结构化剪枝方法，可以有效解决大语言模型部署中的计算和内存挑战。&lt;h4&gt;背景&lt;/h4&gt;随着大型语言模型(LLMs)被更广泛采用和规模扩大，部署这些大规模基础模型面临的计算和内存挑战日益严重。&lt;h4&gt;目的&lt;/h4&gt;开发更高效的模型变体，以应对大语言模型部署中的计算和内存挑战。&lt;h4&gt;方法&lt;/h4&gt;提出EGGS-PTP（基于扩展图引导的结构化剪枝方法），利用图理论指导N:M结构化剪枝的设计，有效减少模型大小和计算需求，并通过引入扩展图概念确保剪枝网络中的信息流，保留基本模型功能。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的数值实验表明，EGGS-PTP不仅通过结构化稀疏性实现了显著的加速和内存节省，而且在各种LLM上的准确性方面优于现有的结构化剪枝技术。&lt;h4&gt;结论&lt;/h4&gt;EGGS-PTP是一种有效的方法，可以在减少模型大小和计算需求的同时保持或提高模型性能。&lt;h4&gt;翻译&lt;/h4&gt;随着大型语言模型(LLMs)被更广泛采用和规模扩大，部署这些大规模基础模型所涉及的计算和内存挑战日益严峻。这凸显了开发更高效模型变体的迫切需求。面对这一挑战，本研究提出了EGGS-PTP：一种基于扩展图引导的结构化剪枝方法。该方法利用图理论指导N:M结构化剪枝的设计，有效减少模型大小和计算需求。通过引入扩展图概念，EGGS-PTP确保剪枝网络中的信息流动，保留基本模型功能。大量数值实验表明，EGGS-PTP不仅通过结构化稀疏性实现了显著的加速和内存节省，而且在各种LLM的准确性方面优于现有的结构化剪枝技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As Large Language Models (LLMs) become more widely adopted and scale up insize, the computational and memory challenges involved in deploying thesemassive foundation models have grown increasingly severe. This underscores theurgent need to develop more efficient model variants. Faced with thischallenge, the present work introduces EGGS-PTP: an Expander-Graph GuidedStructured Post-training Pruning method. The proposed approach leverages graphtheory to guide the design of N:M structured pruning, effectively reducingmodel size and computational demands. By incorporating concepts from expandergraphs, EGGS-PTP ensures information flow within the pruned network, preservingessential model functionality. Extensive numerical experiments demonstrate thatEGGS-PTP not only achieves significant acceleration and memory savings due tostructured sparsity but also outperforms existing structured pruning techniquesin terms of accuracy across various LLMs.</description>
      <author>example@mail.com (Omar Bazarbachi, Zijun Sun, Yanning Shen)</author>
      <guid isPermaLink="false">2508.09471v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss</title>
      <link>http://arxiv.org/abs/2508.09453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HyperKD的新型知识蒸馏框架，用于解决基础模型在高光谱遥感领域的应用挑战，实现了不同类型光谱数据之间的逆向知识转移，显著提高了模型在高光谱图像上的表现。&lt;h4&gt;背景&lt;/h4&gt;基础模型在大规模无标签数据集上的预训练已成为创建可适应和可重用架构的有效方法，可用于各种下游任务包括卫星观测。然而，将这些模型直接应用于高光谱遥感仍面临挑战，因为存在固有的光谱差异和可用观测数据的稀缺性。&lt;h4&gt;目的&lt;/h4&gt;提出HyperKD框架，使教师模型学习的表示能够转移到学生模型中，有效地在高光谱图像上开发基础模型，弥合光谱域差距，使预训练基础模型能够有效用于地理空间应用。&lt;h4&gt;方法&lt;/h4&gt;HyperKD基于掩码自编码器，实现了不同类型光谱数据之间的逆向知识转移，由更简单的教师模型指导。该方法通过引入基于特征的策略解决逆向域适应问题，包括基于光谱范围的通道对齐、空间特征引导的掩码，以及为高光谱图像量身定制的增强损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，HyperKD显著提高了MAE中的表示学习，提高了重建保真度，并在下游任务上表现更加稳健，包括土地覆盖分类、作物类型识别和土壤有机碳预测。&lt;h4&gt;结论&lt;/h4&gt;知识蒸馏框架在高光谱图像的遥感分析中具有巨大潜力，HyperKD为有效利用预训练基础模型解决高光谱遥感问题提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在大规模无标签数据集上的激增已成为创建可适应和可重用架构的有效方法，这些架构可以被利用于各种使用卫星观测的下游任务。然而，由于固有的光谱差异和可用观测数据的稀缺性，它们在高光谱遥感的直接应用仍然具有挑战性。在这项工作中，我们提出了HyperKD，一种新颖的知识蒸馏框架，使教师模型学习的表示能够转移到学生模型中，以便在高光谱图像上有效开发基础模型。与使用复杂教师指导简单学生的典型知识蒸馏框架不同，HyperKD实现了不同类型光谱数据之间的逆向知识转移，由更简单的教师模型指导。基于掩码自编码器，HyperKD将Prithvi基础模型的知识蒸馏到专门为EnMAP高光谱图像设计的学生模型中。HyperKD通过引入基于特征的策略解决光谱差距的逆向域适应问题，包括基于光谱范围的通道对齐、空间特征引导的掩码，以及为高光谱图像量身定制的增强损失函数。HyperKD弥合了巨大的光谱域差距，使预训练基础模型能够有效用于地理空间应用。大量实验表明，HyperKD显著提高了MAE中的表示学习，提高了重建保真度，并在土地覆盖分类、作物类型识别和土壤有机碳预测等下游任务上表现更加稳健，证明了知识蒸馏框架在高光谱图像遥感分析中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of foundation models, pretrained on large-scale unlabeleddatasets, has emerged as an effective approach in creating adaptable andreusable architectures that can be leveraged for various downstream tasks usingsatellite observations. However, their direct application to hyperspectralremote sensing remains challenging due to inherent spectral disparities and thescarcity of available observations. In this work, we present HyperKD, a novelknowledge distillation framework that enables transferring learnedrepresentations from a teacher model into a student model for effectivedevelopment of a foundation model on hyperspectral images. Unlike typicalknowledge distillation frameworks, which use a complex teacher to guide asimpler student, HyperKD enables an inverse form of knowledge transfer acrossdifferent types of spectral data, guided by a simpler teacher model. Buildingupon a Masked Autoencoder, HyperKD distills knowledge from the Prithvifoundational model into a student tailored for EnMAP hyperspectral imagery.HyperKD addresses the inverse domain adaptation problem with spectral gaps byintroducing a feature-based strategy that includes spectral range-based channelalignment, spatial feature-guided masking, and an enhanced loss functiontailored for hyperspectral images. HyperKD bridges the substantial spectraldomain gap, enabling the effective use of pretrained foundation models forgeospatial applications. Extensive experiments show that HyperKD significantlyimproves representation learning in MAEs, leading to enhanced reconstructionfidelity and more robust performance on downstream tasks such as land coverclassification, crop type identification, and soil organic carbon prediction,underpinning the potential of knowledge distillation frameworks in remotesensing analytics with hyperspectral imagery.</description>
      <author>example@mail.com (Abdul Matin, Tanjim Bin Faruk, Shrideep Pallickara, Sangmi Lee Pallickara)</author>
      <guid isPermaLink="false">2508.09453v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata</title>
      <link>http://arxiv.org/abs/2508.09415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the ICCV'25 Workshop on Vision Foundation Models and  Generative AI for Accessibility: Challenges and Opportunities&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为RampNet的两阶段管道，用于大规模路缘坡道检测数据集的构建和模型性能提升。研究团队通过自动转换政府提供的路缘坡道位置数据生成了超过21万张标注的Google街景全景图像，并基于此数据集训练了一个改进的ConvNeXt V2检测模型，达到了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;路缘坡道对城市无障碍设施至关重要，但在图像中稳健地检测它们仍然是一个开放问题，主要是由于缺乏大规模、高质量的数据集。先前的工作尝试通过众包或手动标记的数据来提高数据可用性，但这些努力在质量或规模上往往不尽如人意。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在构建一个大规模、高质量的路缘坡道检测数据集，并开发一个高性能的检测模型，以提高城市无障碍设施的可及性。&lt;h4&gt;方法&lt;/h4&gt;研究采用两阶段管道RampNet：第一阶段通过自动转换政府提供的路缘坡道位置数据，生成了超过21万张标注的Google街景全景图像数据集；第二阶段使用生成的数据集训练一个改进的ConvNeXt V2模型进行路缘坡道检测；评估阶段将生成的数据集和检测模型与手动标记的全景图像进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;生成的数据集达到了94.0%的精确度和92.5%的召回率；检测模型达到了0.9236的平均精度(AP)，显著超过了先前的工作；该工作贡献了首个大规模、高质量的路缘坡道检测数据集、基准和模型。&lt;h4&gt;结论&lt;/h4&gt;RampNet两阶段管道成功解决了路缘坡道检测中大规模高质量数据集缺乏的问题，通过自动转换政府数据生成了大规模标注数据集，并训练出高性能检测模型，为城市无障碍设施评估提供了新工具。&lt;h4&gt;翻译&lt;/h4&gt;路缘坡道对城市无障碍设施至关重要，但在图像中稳健地检测它们仍然是一个开放问题，主要是由于缺乏大规模、高质量的数据集。虽然先前的工作试图通过众包或手动标记的数据来提高数据可用性，但这些努力在质量或规模上往往不尽如人意。在本文中，我们介绍并评估了一个名为RampNet的两阶段管道，以扩展路缘坡道检测数据集并提高模型性能。在第一阶段，我们通过自动转换政府提供的路缘坡道位置数据到全景图像中的像素坐标，生成了一个包含超过21万张标注的Google街景(GSV)全景图像的数据集。在第二阶段，我们从生成的数据集训练一个路缘坡道检测模型(改进的ConvNeXt V2)，达到了最先进的性能。为了评估我们管道的两个阶段，我们与手动标记的全景图像进行了比较。我们生成的数据集达到了94.0%的精确度和92.5%的召回率，我们的检测模型达到了0.9236的平均精度(AP)——远远超过了先前的工作。我们的工作贡献了首个大规模、高质量的路缘坡道检测数据集、基准和模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Curb ramps are critical for urban accessibility, but robustly detecting themin images remains an open problem due to the lack of large-scale, high-qualitydatasets. While prior work has attempted to improve data availability withcrowdsourced or manually labeled data, these efforts often fall short in eitherquality or scale. In this paper, we introduce and evaluate a two-stage pipelinecalled RampNet to scale curb ramp detection datasets and improve modelperformance. In Stage 1, we generate a dataset of more than 210,000 annotatedGoogle Street View (GSV) panoramas by auto-translating government-provided curbramp location data to pixel coordinates in panoramic images. In Stage 2, wetrain a curb ramp detection model (modified ConvNeXt V2) from the generateddataset, achieving state-of-the-art performance. To evaluate both stages of ourpipeline, we compare to manually labeled panoramas. Our generated datasetachieves 94.0% precision and 92.5% recall, and our detection model reaches0.9236 AP -- far exceeding prior work. Our work contributes the firstlarge-scale, high-quality curb ramp detection dataset, benchmark, and model.</description>
      <author>example@mail.com (John S. O'Meara, Jared Hwang, Zeyu Wang, Michael Saugstad, Jon E. Froehlich)</author>
      <guid isPermaLink="false">2508.09415v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders</title>
      <link>http://arxiv.org/abs/2508.09363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了领域受限稀疏自编码器在医学文本中的应用，发现这种方法比广泛领域训练的SAEs能更好地捕捉领域特定特征，提高模型的可解释性和重构保真度。&lt;h4&gt;背景&lt;/h4&gt;传统稀疏自编码器在广泛数据分布上训练，使用固定潜在预算只捕获高频通用模式，导致重构误差中存在显著的线性'暗物质'，并产生碎片化或相互吸收的潜在特征，使解释复杂化。&lt;h4&gt;目的&lt;/h4&gt;研究将SAE训练限制在特定领域(医学文本)是否能改善模型的性能和可解释性，以及如何缓解广泛领域SAEs的关键局限性。&lt;h4&gt;方法&lt;/h4&gt;使用195k临床问答示例在Gemma-2模型的第20层激活上训练JumpReLU稀疏自编码器，并与广泛领域训练的SAEs进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;领域受限的SAEs比广泛领域的SAEs能解释多达20%的方差，实现更高的损失恢复，减少线性残差误差；学习到的特征与临床上有意义的概念对齐，而非频繁但信息量少的标记；这些SAEs捕获相关的线性结构，留下更小、更纯粹的非线性残差。&lt;h4&gt;结论&lt;/h4&gt;领域限制缓解了广泛领域SAEs的关键局限性，实现了更完整和可解释的潜在分解，暗示该领域可能需要质疑用于通用SAEs的'基础模型'扩展。&lt;h4&gt;翻译&lt;/h4&gt;稀疏自编码器将大语言模型的激活分解为揭示机制结构的潜在特征。传统稀疏自编码器在广泛的数据分布上训练，迫使固定的潜在预算只能捕获高频、通用模式。这通常导致重构误差中存在显著的线性'暗物质'，并产生碎片化或相互吸收的潜在特征，使解释复杂化。我们证明，将SAE训练限制在明确定义的领域(医学文本)可以将容量重新分配到领域特定特征，提高重构保真度和可解释性。使用195k临床问答示例在Gemma-2模型的第20层激活上训练JumpReLU SAEs，我们发现领域受限的SAEs比广泛领域的SAEs能解释多达20%的方差，实现更高的损失恢复，并减少线性残差误差。自动和人工评估确认，学习到的特征与临床上有意义的概念(如'味觉感觉'或'传染性单核细胞增多症')对齐，而不是频繁但信息量少的标记。这些领域特定的SAEs捕获相关的线性结构，留下更小、更纯粹的非线性残差。我们得出结论，领域限制缓解了广泛领域SAEs的关键局限性，实现了更完整和可解释的潜在分解，并暗示该领域可能需要质疑用于通用SAEs的'基础模型'扩展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparse autoencoders (SAEs) decompose large language model (LLM) activationsinto latent features that reveal mechanistic structure. Conventional SAEs trainon broad data distributions, forcing a fixed latent budget to capture onlyhigh-frequency, generic patterns. This often results in significant linear``dark matter'' in reconstruction error and produces latents that fragment orabsorb each other, complicating interpretation. We show that restricting SAEtraining to a well-defined domain (medical text) reallocates capacity todomain-specific features, improving both reconstruction fidelity andinterpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2models using 195k clinical QA examples, we find that domain-confined SAEsexplain up to 20\% more variance, achieve higher loss recovery, and reducelinear residual error compared to broad-domain SAEs. Automated and humanevaluations confirm that learned features align with clinically meaningfulconcepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), ratherthan frequent but uninformative tokens. These domain-specific SAEs capturerelevant linear structure, leaving a smaller, more purely nonlinear residual.We conclude that domain-confinement mitigates key limitations of broad-domainSAEs, enabling more complete and interpretable latent decompositions, andsuggesting the field may need to question ``foundation-model'' scaling forgeneral-purpose SAEs.</description>
      <author>example@mail.com (Charles O'Neill, Mudith Jayasekara, Max Kirkby)</author>
      <guid isPermaLink="false">2508.09363v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Aryabhata: An exam-focused language model for JEE Math</title>
      <link>http://arxiv.org/abs/2508.08665v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Aryabhata 1.0，一个针对印度联合入学考试(JEE)优化的70亿参数数学推理模型，通过合并开源模型、监督微调和强化学习等方法，在多个基准测试上表现出色，并提供教育友好的逐步推理。&lt;h4&gt;背景&lt;/h4&gt;尽管大型语言模型迅速发展，但当前模型通常仍不适合教育用途。印度联合入学考试(JEE)是重要的学术考试，需要专门的数学推理能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对JEE考试优化的高效准确的数学推理模型，提供教育友好的逐步推理，并作为开源模型发布促进相关研究。&lt;h4&gt;方法&lt;/h4&gt;合并强大的开源推理模型，使用经过最佳拒绝采样验证的思维链痕迹进行监督微调和课程学习，应用可验证奖励的强化学习(A2C目标与组相对优势估计)，实施自适应组大小调整和温度缩放等探索策略。&lt;h4&gt;主要发现&lt;/h4&gt;Aryabhata 1.0在分布内(JEE Main 2025)和分布外(MATH, GSM8K)基准测试上均优于现有模型，在准确性和效率方面表现出色，并提供教育有用的逐步推理。&lt;h4&gt;结论&lt;/h4&gt;Aryabhata作为面向考试的开源小型语言模型基础模型发布，PhysicsWallahAI正在积极训练未来模型以进一步改善学生学习成果。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Aryabhata 1.0，这是一个针对印度联合入学考试(JEE)优化的紧凑型70亿参数数学推理模型。尽管大型语言模型迅速发展，但当前模型通常仍不适合教育用途。Aryabhata 1.0是通过合并强大的开源权重推理模型构建的，随后使用通过最佳拒绝采样验证的思维链痕迹进行课程学习和监督微调。为了进一步提高性能，我们应用了使用A2C目标和组相对优势估计的可验证奖励强化学习，以及自适应组大小调整和温度缩放等新的探索策略。在分布内和分布外基准测试上评估，Aryabhata在准确性和效率方面均优于现有模型，同时提供教育有用的逐步推理。我们将Aryabhata作为基础模型发布，以推进面向考试的开源小型语言模型。这是我们首次开放发布供社区反馈；PhysicsWallahAI正在积极训练未来模型，以进一步改善学生的学习成果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Aryabhata 1.0, a compact 7B parameter math reasoning modeloptimized for the Indian academic exam, the Joint Entrance Examination (JEE).Despite rapid progress in large language models (LLMs), current models oftenremain unsuitable for educational use. Aryabhata 1.0 is built by merging strongopen-weight reasoning models, followed by supervised fine-tuning (SFT) withcurriculum learning on verified chain-of-thought (CoT) traces curated throughbest-of-$n$ rejection sampling. To further boost performance, we applyreinforcement learning with verifiable rewards (RLVR) using A2C objective withgroup-relative advantage estimation along with novel exploration strategiessuch as Adaptive Group Resizing and Temperature Scaling. Evaluated on bothin-distribution (JEE Main 2025) and out-of-distribution (MATH, GSM8K)benchmarks, Aryabhata outperforms existing models in accuracy and efficiency,while offering pedagogically useful step-by-step reasoning. We releaseAryabhata as a foundation model to advance exam-centric, open-source smalllanguage models. This marks our first open release for community feedback(https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0); PW is actively trainingfuture models to further improve learning outcomes for students.</description>
      <author>example@mail.com (Ritvik Rastogi, Sachin Dharashivkar, Sandeep Varma)</author>
      <guid isPermaLink="false">2508.08665v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Towards Scalable Training for Handwritten Mathematical Expression Recognition</title>
      <link>http://arxiv.org/abs/2508.09220v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种解决手写数学表达式识别(HMER)领域数据稀缺问题的新方法，通过开发可扩展数据引擎生成LaTeX序列，构建了包含8000万实例的最大公式数据集Tex80M，并提出了首个大规模训练的HMER模型TexTeller，在几乎所有基准测试中达到最先进性能。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型通过大规模数据集训练取得显著性能提升，但HMER领域因数据稀缺而发展受限，主要是由于手动标注过程困难和昂贵。&lt;h4&gt;目的&lt;/h4&gt;解决HMER领域数据稀缺问题，通过结合有限手写公式与大规模LaTeX渲染公式来提升模型性能。&lt;h4&gt;方法&lt;/h4&gt;开发可扩展数据引擎生成复杂一致的LaTeX序列；构建包含8000万高质量训练实例的Tex80M数据集；提出TexTeller模型，通过混合训练Tex80M和小规模HME数据集进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;大规模训练数据集和改进的流程使TexTeller在几乎所有基准测试中都达到了最先进(SOTA)性能。&lt;h4&gt;结论&lt;/h4&gt;将开放发布完整模型、整个数据集和完整代码库，促进基于其贡献的进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;大型基础模型通过大规模数据集的可扩展训练已取得显著的性能提升。然而，手写数学表达式识别(HMER)领域因数据稀缺而受到阻碍，这主要是由于手动标注过程困难和昂贵。为了弥补这一差距，我们提出了一种新方法，通过开发可扩展数据引擎将有限的手写公式与大规模LaTeX渲染的公式结合起来，生成复杂且一致的LaTeX序列。利用该引擎，我们构建了迄今为止最大的公式数据集，称为Tex80M，包含超过8000万个高质量训练实例。然后我们提出了TexTeller，这是首个大规模训练的HMER模型，通过将Tex80M与相对较小的HME数据集混合训练。广泛的训练数据集和我们改进的流程使TexTeller在几乎所有基准测试中都达到了最先进的(SOTA)性能。为了推动该领域发展，我们将开放发布完整模型、整个数据集和完整代码库，促进基于我们贡献的进一步研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large foundation models have achieved significant performance gains throughscalable training on massive datasets. However, the field of\textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarilydue to the arduous and costly process of manual annotation. To bridge this gap,we propose a novel method integrating limited handwritten formulas withlarge-scale LaTeX-rendered formulas by developing a scalable data engine togenerate complex and consistent LaTeX sequences. With this engine, we built thelargest formula dataset to date, termed \texttt{Tex80M}, comprising over 80million high-quality training instances. Then we propose \texttt{TexTeller},the first HMER model trained at scale, by mix-training \texttt{Tex80M} with arelatively small HME dataset. The expansive training dataset and our refinedpipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA)performance across nearly all benchmarks. To advance the field, we will openlyrelease our complete model, entire dataset, and full codebase, enabling furtherresearch building upon our contributions.</description>
      <author>example@mail.com (Haoyang Li, Jiaqing Li, Jialun Cao, Zongyuan Yang, Yongping Xiong)</author>
      <guid isPermaLink="false">2508.09220v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>VGGSounder: Audio-Visual Evaluations for Foundation Models</title>
      <link>http://arxiv.org/abs/2508.08237v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the IEEE/CVF International Conference on Computer  Vision (ICCV) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VGGSounder，一个改进的视听评估数据集，解决了原VGGSound数据集的标注不完整、类别重叠和模态未对齐等问题，为评估视听基础模型提供了更可靠的工具。&lt;h4&gt;背景&lt;/h4&gt;视听基础模型的兴起凸显了可靠评估其多模态理解能力的重要性，VGGSound数据集常被用作视听分类的基准评估工具。&lt;h4&gt;目的&lt;/h4&gt;解决VGGSound数据集的局限性，创建一个更可靠的评估工具来准确评估视听基础模型的多模态理解能力。&lt;h4&gt;方法&lt;/h4&gt;创建VGGSounder，一个全面重新注释的多标签测试集，扩展VGGSound数据集，并引入模态混淆指标来分析模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;VGGSound数据集存在标注不完整、类别部分重叠和模态未对齐等局限性，导致视听能力评估失真；通过分析添加另一输入模态时的性能下降，揭示了模型的局限性。&lt;h4&gt;结论&lt;/h4&gt;VGGSounder通过详细模态注释和设计，能够提供更精确的模态特定性能分析，为评估视听基础模型提供了更可靠的基准。&lt;h4&gt;翻译&lt;/h4&gt;视听基础模型的出现凸显了可靠评估其多模态理解能力的重要性。VGGSound数据集通常被用作视听分类的评估基准。然而，我们的分析发现了VGGSound的几个局限性，包括标注不完整、类别部分重叠和模态未对齐。这些问题导致了对视听能力的扭曲评估。为了解决这些局限性，我们引入了VGGSounder，这是一个全面重新注释的多标签测试集，扩展了VGGSound，并专门设计用于评估视听基础模型。VGGSounder具有详细的模态注释，能够精确分析特定模态的性能。此外，我们通过使用新的模态混淆指标分析添加另一个输入模态时的性能下降，揭示了模型的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of audio-visual foundation models underscores the importance ofreliably assessing their multi-modal understanding. The VGGSound dataset iscommonly used as a benchmark for evaluation audio-visual classification.However, our analysis identifies several limitations of VGGSound, includingincomplete labelling, partially overlapping classes, and misaligned modalities.These lead to distorted evaluations of auditory and visual capabilities. Toaddress these limitations, we introduce VGGSounder, a comprehensivelyre-annotated, multi-label test set that extends VGGSound and is specificallydesigned to evaluate audio-visual foundation models. VGGSounder featuresdetailed modality annotations, enabling precise analyses of modality-specificperformance. Furthermore, we reveal model limitations by analysing performancedegradation when adding another input modality with our new modality confusionmetric.</description>
      <author>example@mail.com (Daniil Zverev, Thaddäus Wiedemer, Ameya Prabhu, Matthias Bethge, Wieland Brendel, A. Sophia Koepke)</author>
      <guid isPermaLink="false">2508.08237v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>MolmoAct: Action Reasoning Models that can Reason in Space</title>
      <link>http://arxiv.org/abs/2508.07917v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Appendix include. Code, Data and Weights:  https://allenai.org/blog/molmoact&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了行动推理模型(ARMs)，一种通过结构化三阶段管道整合感知、规划和控制的机器人基础模型。MolmoAct作为代表模型，在多个基准测试中展现出优异性能，并首次发布了包含10,000多个高质量机器人轨迹的MolmoAct数据集。&lt;h4&gt;背景&lt;/h4&gt;推理是目标行动的核心，但大多数机器人基础模型直接将感知和指令映射到控制，这限制了适应性、泛化和语义基础。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够通过结构化推理将感知转变为目标行动的机器人基础模型，提高机器人的适应性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;MolmoAct模型采用三阶段处理：1)将观察和指令编码为深度感知令牌；2)生成中级空间计划作为可编辑的轨迹痕迹；3)预测精确的低级行动，使行为可解释和可操控。&lt;h4&gt;主要发现&lt;/h4&gt;MolmoAct-7B-D在SimplerEnv视觉匹配任务上达到70.5%零样本准确率；在LIBERO上达到86.6%平均成功率；真实世界微调中比Pi-0-FAST提高10%(单臂)和22.7%(双臂)；分布外泛化上提高23.3%；使用MolmoAct数据集训练比基础模型提高5.5%性能。&lt;h4&gt;结论&lt;/h4&gt;通过发布模型权重、训练代码和数据集，MolmoAct成为最先进的机器人基础模型，并为构建通过结构化推理将感知转变为目标行动的ARMs提供了开放蓝图。&lt;h4&gt;翻译&lt;/h4&gt;推理是目标行动的核心，然而大多数机器人基础模型直接将感知和指令映射到控制，这限制了适应性、泛化和语义基础。我们引入了行动推理模型(ARMs)，一类通过结构化三阶段管道整合感知、规划和控制的机器人基础模型。我们的模型MolmoAct将观察和指令编码为深度感知令牌，生成中级空间计划作为可编辑的轨迹痕迹，并预测精确的低级行动，使行为可解释和可操控。MolmoAct-7B-D在模拟和真实世界环境中表现出色：在SimplerEnv视觉匹配任务上达到70.5%的零样本准确率，优于闭源Pi-0和GR00T N1；在LIBERO上达到86.6%的平均成功率，比ThinkAct在长距离任务上额外提高6.3%；在真实世界微调中，比Pi-0-FAST额外提高10%（单臂）和22.7%（双臂）任务进展；在分布外泛化上比基线额外提高23.3%；在开放指令跟随和轨迹操控方面获得最高的人类偏好分数。此外，我们首次发布了MolmoAct数据集——包含超过10,000个跨多样场景和任务的高质量机器人轨迹的中期训练机器人数据集。使用此数据集训练比基础模型平均提高5.5%的总体性能。我们发布了所有模型权重、训练代码、收集的数据集和行动推理数据集，将MolmoAct建立为最先进的机器人基础模型，以及构建通过结构化推理将感知转变为目标行动的ARMs的开放蓝图。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人基础模型直接将感知和指令映射到控制的问题，这限制了机器人的适应性、泛化能力和语义理解。这个问题很重要，因为机器人需要像人类一样能够推理而不仅仅是执行任务，才能在复杂环境中灵活应对各种情况。当前的视觉-语言-动作模型(VLA)虽然取得进展，但仍存在脆弱、不透明、难以跨任务泛化等局限性，无法满足实际应用需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从人类行为中获得启发，人类在行动前会潜意识地权衡上下文、目标和约束。他们认为机器人也需要学习推理而非简单映射。设计上借鉴了语言模型从蛮力扩展向结构化学习转变的趋势，构建中间表示来支持推理。具体借鉴了Molmo多模态模型作为视觉-语言骨干，RT-1等工作的动作离散化方法，以及链式思维(CoT)在语言模型中的应用。但作者创新性地将推理从语言空间转移到空间本身，设计了三阶段推理管道。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结构化的三阶段推理管道整合感知、规划和控制，让模型'在空间中推理'而非仅通过语言推理。整体流程为：1)深度感知标记生成：将观察和指令编码成深度感知标记，重建3D环境；2)视觉推理轨迹生成：生成中级空间计划作为可编辑的轨迹痕迹；3)动作预测：基于前两步结果预测精确的低级动作。训练分为预训练、中间训练和后训练三个阶段，分别在不同数据集上进行训练和微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)结构化的三阶段推理管道；2)在空间而非仅通过语言进行推理；3)引入深度感知标记增强3D理解；4)视觉推理轨迹作为可解释中间表示；5)通过视觉轨迹实现交互式引导；6)发布MolmoAct数据集；7)完全开源模型、代码和数据。相比之前工作，不同之处在于：不直接映射感知到控制，而是通过中间推理步骤；不使用语言作为推理媒介，直接在空间推理；提供比语言引导更精确的视觉轨迹引导；使用更少数据实现更好性能；提供完整开源方案而非封闭模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MolmoAct引入了一种新型的动作推理模型，通过结构化的空间推理管道整合感知、规划和控制，实现了可解释、可引导的机器人行为，并在多个基准测试中取得了最先进的结果，同时完全开源了模型、代码和数据集。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning is central to purposeful action, yet most robotic foundation modelsmap perception and instructions directly to control, which limits adaptability,generalization, and semantic grounding. We introduce Action Reasoning Models(ARMs), a class of robotic foundation models that integrate perception,planning, and control through a structured three-stage pipeline. Our model,MolmoAct, encodes observations and instructions into depth-aware perceptiontokens, generates mid-level spatial plans as editable trajectory traces, andpredicts precise low-level actions, enabling explainable and steerablebehavior. MolmoAct-7B-D achieves strong performance across simulation andreal-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matchingtasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success onLIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;and in real-world fine-tuning, an additional 10% (single-arm) and an additional22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselinesby an additional 23.3% on out-of-distribution generalization and achieves tophuman-preference scores for open-ended instruction following and trajectorysteering. Furthermore, we release, for the first time, the MolmoAct Dataset --a mid-training robot dataset comprising over 10,000 high quality robottrajectories across diverse scenarios and tasks. Training with this datasetyields an average 5.5% improvement in general performance over the base model.We release all model weights, training code, our collected dataset, and ouraction reasoning dataset, establishing MolmoAct as both a state-of-the-artrobotics foundation model and an open blueprint for building ARMs thattransform perception into purposeful action through structured reasoning.Blogpost: https://allenai.org/blog/molmoact</description>
      <author>example@mail.com (Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna)</author>
      <guid isPermaLink="false">2508.07917v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Episodic Memory Representation for Long-form Video Understanding</title>
      <link>http://arxiv.org/abs/2508.09486v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Video-EM是一种受人类情景记忆启发的无需训练框架，将关键帧建模为时序有序的情景事件而非孤立实体，结合链式思维方法识别最小但信息丰富的帧子集，在减少帧数的同时提高了Video-LLMs对长视频的理解和问答能力。&lt;h4&gt;背景&lt;/h4&gt;Video-LLMs在视频理解方面表现出色，但因上下文窗口限制难以处理长视频。现有方法通过关键帧检索简化问题，但忽略了时空关系且可能产生冗余关键帧，稀释了重要线索。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，引入Video-EM框架促进稳健且上下文基础的推理，准确捕捉视频中的时空关系和叙事连续性。&lt;h4&gt;方法&lt;/h4&gt;将关键帧明确建模为时序有序的情景事件，捕捉空间关系和时序动态；利用链式思维(CoT)与LLMs迭代识别最小但信息丰富的情景子集，使Video-LLMs能高效准确回答问题。&lt;h4&gt;主要发现&lt;/h4&gt;在Video-MME、EgoSchema、HourVideo和LVBench基准测试上，Video-EM相比基线实现了4-9%的性能提升，同时使用更少的帧。&lt;h4&gt;结论&lt;/h4&gt;Video-EM有效解决了长视频理解中的关键挑战，通过模拟人类情景记忆方式更好地捕捉视频时空关系，在减少帧数的同时提高了性能。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型(Vedio-LLMs)在通用视频理解方面表现出色，但由于上下文窗口限制，难以处理长视频。因此，最近的方法专注于关键帧检索，将冗长的视频压缩为一小组信息丰富的帧。尽管这些方法实用，但它们将问题简化为静态文本图像匹配，忽略了捕捉场景转换和上下文连续性至关重要的时空关系，并且可能产生信息有限且冗余的关键帧，稀释了准确视频问答所需的重要线索。为解决这些限制，我们引入了Video-EM，这是一个受人类情景记忆原理启发的无需训练的框架，旨在促进稳健且上下文基础的推理。Video-EM不将关键帧视为孤立的视觉实体，而是明确地将它们建模为时序有序的情景事件，捕捉准确重建底层叙事所需的空间关系和时序动态。此外，该框架利用LLMs的链式思维(CoT)迭代识别最小但信息丰富的情景子集，使Video-LLMs能够高效准确地回答问题。在Video-MME、EgoSchema、HourVideo和LVBench基准上的广泛评估证实了Video-EM的优越性，相比相应基线实现了4-9%的性能提升，同时使用更少的帧。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (Video-LLMs) excel at general video understandingbut struggle with long-form videos due to context window limits. Consequently,recent approaches focus on keyframe retrieval, condensing lengthy videos into asmall set of informative frames. Despite their practicality, these methodssimplify the problem to static text image matching, overlooking spatio temporalrelationships crucial for capturing scene transitions and contextualcontinuity, and may yield redundant keyframes with limited information,diluting salient cues essential for accurate video question answering. Toaddress these limitations, we introduce Video-EM, a training free frameworkinspired by the principles of human episodic memory, designed to facilitaterobust and contextually grounded reasoning. Rather than treating keyframes asisolated visual entities, Video-EM explicitly models them as temporally orderedepisodic events, capturing both spatial relationships and temporal dynamicsnecessary for accurately reconstructing the underlying narrative. Furthermore,the framework leverages chain of thought (CoT) thinking with LLMs toiteratively identify a minimal yet highly informative subset of episodicmemories, enabling efficient and accurate question answering by Video-LLMs.Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBenchbenchmarks confirm the superiority of Video-EM, which achieves highlycompetitive results with performance gains of 4-9 percent over respectivebaselines while utilizing fewer frames.</description>
      <author>example@mail.com (Yun Wang, Long Zhang, Jingren Liu, Jiaqi Yan, Zhanjie Zhang, Jiahao Zheng, Xun Yang, Dapeng Wu, Xiangyu Chen, Xuelong Li)</author>
      <guid isPermaLink="false">2508.09486v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment</title>
      <link>http://arxiv.org/abs/2508.09399v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于联邦学习的风险评估框架，解决跨机构金融风险分析中的数据隐私和协作建模挑战。通过特征注意力和时间建模结构，实现了无需共享原始数据的跨机构联合建模和风险识别。实验表明该方法在所有评估指标上均优于传统方法和现有联邦学习变体。&lt;h4&gt;背景&lt;/h4&gt;跨机构金融风险分析面临数据隐私和协作建模的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于联邦学习的风险评估框架，实现无需共享原始数据的跨机构联合建模和风险识别。&lt;h4&gt;方法&lt;/h4&gt;采用特征注意力和时间建模结构；分布式优化策略；各金融机构训练本地子模型；使用差分隐私和噪声注入保护模型参数；中央服务器聚合参数生成全局模型；全局模型用于系统性风险识别。&lt;h4&gt;主要发现&lt;/h4&gt;所提模型在通信效率、模型准确性、系统性风险检测和跨市场泛化能力等评估指标上均优于传统集中式方法和现有联邦学习变体；在敏感金融环境中展现出强大的建模能力和实用价值。&lt;h4&gt;结论&lt;/h4&gt;该方法增强了风险识别的范围和效率，同时保留了数据主权；为智能金融风险分析提供了安全高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了跨机构金融风险分析中的数据隐私和协作建模挑战。它提出了一种基于联邦学习的风险评估框架。在不共享原始数据的情况下，该方法使多个机构能够进行联合建模和风险识别。这是通过整合特征注意力和时间建模结构实现的。具体来说，该模型采用分布式优化策略。每个金融机构训练一个本地子模型。模型参数在上传前使用差分隐私和噪声注入进行保护。然后中央服务器聚合这些参数以生成全局模型。该全局模型用于系统性风险识别。为验证所提方法的有效性，进行了多项实验。这些实验评估了通信效率、模型准确性、系统性风险检测和跨市场泛化能力。结果表明，在所有评估指标上，所提出的模型均优于传统集中式方法和现有联邦学习变体。它在敏感金融环境中展现出强大的建模能力和实用价值。该方法在保留数据主权的同时增强了风险识别的范围和效率。它为智能金融风险分析提供了安全高效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenges of data privacy and collaborativemodeling in cross-institution financial risk analysis. It proposes a riskassessment framework based on federated learning. Without sharing raw data, themethod enables joint modeling and risk identification across multipleinstitutions. This is achieved by incorporating a feature attention mechanismand temporal modeling structure. Specifically, the model adopts a distributedoptimization strategy. Each financial institution trains a local sub-model. Themodel parameters are protected using differential privacy and noise injectionbefore being uploaded. A central server then aggregates these parameters togenerate a global model. This global model is used for systemic riskidentification. To validate the effectiveness of the proposed method, multipleexperiments are conducted. These evaluate communication efficiency, modelaccuracy, systemic risk detection, and cross-market generalization. The resultsshow that the proposed model outperforms both traditional centralized methodsand existing federated learning variants across all evaluation metrics. Itdemonstrates strong modeling capabilities and practical value in sensitivefinancial environments. The method enhances the scope and efficiency of riskidentification while preserving data sovereignty. It offers a secure andefficient solution for intelligent financial risk analysis.</description>
      <author>example@mail.com (Yue Yao, Zhen Xu, Youzhu Liu, Kunyuan Ma, Yuxiu Lin, Mohan Jiang)</author>
      <guid isPermaLink="false">2508.09399v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Online Data Generation for MIMO-OFDM Channel Denoising: Transfer Learning vs. Meta Learning</title>
      <link>http://arxiv.org/abs/2508.09751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种标准兼容的在线训练数据生成策略，用于MIMO-OFDM系统中的自适应信道去噪，通过数据辅助信道估计和两种学习方法(迁移学习和元学习)有效减少了信道估计误差&lt;h4&gt;背景&lt;/h4&gt;信道去噪是减轻多输入多输出正交频分复用(MIMO-OFDM)系统中信道估计误差的一种实用有效技术，但将去噪技术适应变化的信道条件通常需要先验知识或产生大量训练开销&lt;h4&gt;目的&lt;/h4&gt;解决信道去噪技术适应变化信道条件时面临的挑战，提出一种标准兼容的策略，用于生成在线训练数据，实现在线自适应信道去噪&lt;h4&gt;方法&lt;/h4&gt;利用数据辅助信道估计获得的高质量信道估计作为真实信道的替代品；利用特定时频邻域内的相邻检测数据符号作为虚拟参考信号，并分析推导该邻域的最佳大小；基于此策略设计了两种信道去噪方法：基于迁移学习(微调预训练神经网络)和基于元学习(快速适应新信道环境)&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法能有效适应动态信道条件，与传统技术相比显著减少了信道估计误差&lt;h4&gt;结论&lt;/h4&gt;提出的在线训练数据生成策略和两种信道去噪方法能够有效解决信道估计问题，提高MIMO-OFDM系统性能&lt;h4&gt;翻译&lt;/h4&gt;信道去噪是一种实用有效的技术，用于减轻多输入多输出正交频分复用(MIMO-OFDM)系统中的信道估计误差。然而，将去噪技术适应变化的信道条件通常需要先验知识或产生大量训练开销。为解决这些挑战，我们提出了一种标准兼容的策略，用于生成在线训练数据，实现在线自适应信道去噪。关键思想是利用通过数据辅助信道估计获得的高质量信道估计作为不可用真实信道的实际替代品。我们的数据辅助方法利用特定时频邻域内的相邻检测数据符号作为虚拟参考信号，并分析推导了该邻域的最佳大小，以最小化所得估计的均方误差。利用所提出的策略，我们设计了两种信道去噪方法，一种是基于迁移学习，微调预训练的去噪神经网络；另一种是基于元学习，以最小的更新快速适应新的信道环境。仿真结果表明，提出的方法能有效适应动态信道条件，与传统技术相比显著减少了信道估计误差&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Channel denoising is a practical and effective technique for mitigatingchannel estimation errors in multiple-input multiple-output orthogonalfrequency-division multiplexing (MIMO-OFDM) systems. However, adaptingdenoising techniques to varying channel conditions typically requires priorknowledge or incurs significant training overhead. To address these challenges,we propose a standard-compatible strategy for generating online training datathat enables online adaptive channel denoising. The key idea is to leveragehigh-quality channel estimates obtained via data-aided channel estimation aspractical substitutes for unavailable ground-truth channels. Our data-aidedmethod exploits adjacent detected data symbols within a specific time-frequencyneighborhood as virtual reference signals, and we analytically derive theoptimal size of this neighborhood to minimize the mean squared error of theresulting estimates. By leveraging the proposed strategy, we devise two channeldenoising approaches, one based on transfer learning, which fine-tunes apre-trained denoising neural network, and the other based on meta learning,which rapidly adapts to new channel environments with minimal updates.Simulation results demonstrate that the proposed methods effectively adapt todynamic channel conditions and significantly reduce channel estimation errorscompared to conventional techniques.</description>
      <author>example@mail.com (Sungyoung Ha, Ikbeom Lee, Seunghyeon Jeon, Yo-Seb Jeon)</author>
      <guid isPermaLink="false">2508.09751v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Reinforcement learning in densely recurrent biological networks</title>
      <link>http://arxiv.org/abs/2508.09618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为ENOMAD的混合优化框架，结合进化搜索和直接搜索，用于训练连续动作空间中的循环神经网络，在线虫神经连接组上的觅食任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;在连续动作空间中训练高度循环神经网络面临技术挑战：基于梯度的方法容易遇到梯度爆炸或消失问题，而纯进化搜索在高维权重空间中收敛缓慢。&lt;h4&gt;目的&lt;/h4&gt;引入一种混合的、无导数的优化框架，通过结合全局进化探索和局部直接搜索开发来实现强化学习。&lt;h4&gt;方法&lt;/h4&gt;提出名为ENOMAD（基于网格自适应直接搜索的进化非线性优化）的方法，在线虫完全映射的神经连接组中的觅食任务套件上进行基准测试。该方法利用生物学衍生的权重先验，改进而非重建生物体的原生电路。引入了两种算法变体：一种是对多个权重进行小分布式调整，另一种是对有限数量的权重进行较大变化。&lt;h4&gt;主要发现&lt;/h4&gt;两种ENOMAD变体都显著优于未经训练的连接组性能（可解释为迁移学习例子）和现有训练策略。&lt;h4&gt;结论&lt;/h4&gt;将进化搜索与非线性优化相结合，为专门化自然循环网络执行特定任务提供了一种高效且基于生物学的方法。&lt;h4&gt;翻译&lt;/h4&gt;在连续动作空间中训练高度循环网络是一个技术挑战：基于梯度的方法容易遇到梯度爆炸或消失问题，而纯进化搜索在高维权重空间中收敛缓慢。我们引入了一种混合的、无导数的优化框架，通过结合全局进化探索和局部直接搜索开发来实现强化学习。该方法称为ENOMAD（基于网格自适应直接搜索的进化非线性优化），在线虫完全映射的神经连接组中的觅食任务套件上进行基准测试。关键是，ENOMAD利用生物学衍生的权重先验，让它改进而非重建生物体的原生电路。介绍了该方法的两种算法变体，导致对多个权重进行小分布式调整，或对有限数量的权重进行较大变化。两种变体都显著优于未经训练的连接组性能（可解释为迁移学习的例子）和现有训练策略。这些发现表明，将进化搜索与非线性优化相结合，为专门化自然循环网络执行特定任务提供了一种高效且基于生物学的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training highly recurrent networks in continuous action spaces is a technicalchallenge: gradient-based methods suffer from exploding or vanishing gradients,while purely evolutionary searches converge slowly in high-dimensional weightspaces. We introduce a hybrid, derivative-free optimization framework thatimplements reinforcement learning by coupling global evolutionary explorationwith local direct search exploitation. The method, termed ENOMAD (EvolutionaryNonlinear Optimization with Mesh Adaptive Direct search), is benchmarked on asuite of food-foraging tasks instantiated in the fully mapped neural connectomeof the nematode \emph{Caenorhabditis elegans}. Crucially, ENOMAD leveragesbiologically derived weight priors, letting it refine--rather than rebuild--theorganism's native circuitry. Two algorithmic variants of the method areintroduced, which lead to either small distributed adjustments of many weights,or larger changes on a limited number of weights. Both variants significantlyexceed the performance of the untrained connectome (in what can be interpretedas an example of transfer learning) and of existing training strategies. Thesefindings demonstrate that integrating evolutionary search with nonlinearoptimization provides an efficient, biologically grounded strategy forspecializing natural recurrent networks towards a specified set of tasks.</description>
      <author>example@mail.com (Miles Walter Churchland, Jordi Garcia-Ojalvo)</author>
      <guid isPermaLink="false">2508.09618v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.09477v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于异常检测的通用AI生成图像检测器，通过无监督学习实现无需访问特定AI生成图像即可有效检测多种生成模型产生的AI生成图像。&lt;h4&gt;背景&lt;/h4&gt;随着AI生成模型的快速发展，AI生成图像(AII)的视觉质量越来越接近自然图像，引发安全担忧。传统AII检测器通常使用自然图像和特定生成模型产生的AI图像进行训练，导致对来自未见过的生成模型的AI图像检测性能有限。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的AI生成图像检测器，能够有效检测来自各种生成模型(包括未见过的模型)的AI生成图像，解决传统检测方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于异常检测视角的检测器，使用预训练的CLIP编码器作为特征提取器，设计类似正规流的无监督模型。使用代理图像(通过对自然图像应用光谱修改操作获得)而非AI图像进行训练，通过最小化代理图像的可能性进行训练，可选择性地结合最大化自然图像的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明该方法对各种图像生成器产生的AI生成图像有效，具有良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;基于异常检测的方法能够有效检测来自不同生成模型的AI生成图像，包括未见过的模型，比传统方法具有更好的泛化性能。&lt;h4&gt;翻译&lt;/h4&gt;随着AI生成模型的快速发展，AI生成图像(AII)的视觉质量越来越接近自然图像，这不可避免地引发了安全担忧。大多数AII检测器通常采用传统图像分类流程，使用自然图像和AI生成图像(由生成模型生成)，这可能导致对来自未见过的生成模型的AI图像检测性能有限。为解决这一问题，我们从异常检测的角度提出了一个通用的AI生成图像检测器。我们的判别器不需要访问任何AI图像，并通过无监督学习学习可泛化的表示。具体来说，我们使用预训练的CLIP编码器作为特征提取器，并设计了一种类似正规流的无监督模型。训练时使用代理图像(例如通过对自然图像应用光谱修改操作获得的图像)，而不是AI图像。我们的模型通过最小化代理图像的可能性进行训练，可选择性地结合最大化自然图像的可能性。大量实验证明我们的方法对各种图像生成器产生的AI图像有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of AI generative models, the visual quality ofAI-generated images (AIIs) has become increasingly close to natural images,which inevitably raises security concerns. Most AII detectors often employ theconventional image classification pipeline with natural images and AIIs(generated by a generative model), which can result in limited detectionperformance for AIIs from unseen generative models. To solve this, we proposeda universal AI-generated image detector from the perspective of anomalydetection. Our discriminator does not need to access any AIIs and learn ageneralizable representation with unsupervised learning. Specifically, we usethe pre-trained CLIP encoder as the feature extractor and design a normalizingflow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained byapplying a spectral modification operation on natural images, are used fortraining. Our models are trained by minimizing the likelihood of proxy images,optionally combined with maximizing the likelihood of natural images. Extensiveexperiments demonstrate the effectiveness of our method on AIIs produced byvarious image generators.</description>
      <author>example@mail.com (Zhipeng Yuan, Kai Wang, Weize Quan, Dong-Ming Yan, Tieru Wu)</author>
      <guid isPermaLink="false">2508.09477v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation</title>
      <link>http://arxiv.org/abs/2508.09462v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为细粒度聚类和拒绝网络(FGCRN)的新型开放集故障诊断模型，解决了多模式过程中健康状态样本多聚类分布导致的决策边界构建困难问题。&lt;h4&gt;背景&lt;/h4&gt;在多模式过程中，同一种健康状态的样本通常表现出多种聚类分布，这使得为该状态构建紧凑且准确的决策边界变得困难。同时，可靠的故障诊断系统不仅要准确分类已知健康状态，还要能有效识别未知故障。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确分类已知健康状态并有效识别未知故障的开放集故障诊断模型，解决多模式过程中健康状态样本多聚类分布带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出FGCRN模型，结合多尺度深度卷积、双向门控循环单元和时序注意力机制捕获判别性特征；设计基于距离的损失函数增强类内紧凑性；通过无监督学习构建细粒度特征表示揭示健康状态内在结构；利用极值理论建模样本特征与细粒度表示间的距离以识别未知故障。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明了FGCRN方法的优越性能，表明该模型能有效解决多模式过程中健康状态样本多聚类分布带来的挑战。&lt;h4&gt;结论&lt;/h4&gt;FGCRN模型通过细粒度聚类和拒绝策略，实现了对已知健康状态的准确分类和对未知故障的有效识别，为多模式过程的故障诊断提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;一个可靠的故障诊断系统不仅要准确分类已知的健康状态，还要有效识别未知故障。在多模式过程中，属于同一健康状态的样本通常表现出多种聚类分布，这使得为该状态构建紧凑且准确的决策边界变得困难。为了应对这一挑战，提出了一种名为细粒度聚类和拒绝网络(FGCRN)的新型开放集故障诊断模型。它结合了多尺度深度卷积、双向门控循环单元和时序注意力机制来捕获判别性特征。设计了一种基于距离的损失函数来增强类内紧凑性。通过无监督学习构建细粒度特征表示，以揭示每个健康状态的内在结构。采用极值理论来建模样本特征与其相应细粒度表示之间的距离，从而有效识别未知故障。大量实验证明了所提出方法的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A reliable fault diagnosis system should not only accurately classify knownhealth states but also effectively identify unknown faults. In multimodeprocesses, samples belonging to the same health state often show multiplecluster distributions, making it difficult to construct compact and accuratedecision boundaries for that state. To address this challenge, a novel open-setfault diagnosis model named fine-grained clustering and rejection network(FGCRN) is proposed. It combines multiscale depthwise convolution,bidirectional gated recurrent unit and temporal attention mechanism to capturediscriminative features. A distance-based loss function is designed to enhancethe intra-class compactness. Fine-grained feature representations areconstructed through unsupervised learning to uncover the intrinsic structuresof each health state. Extreme value theory is employed to model the distancebetween sample features and their corresponding fine-grained representations,enabling effective identification of unknown faults. Extensive experimentsdemonstrate the superior performance of the proposed method.</description>
      <author>example@mail.com (Guangqiang Li, M. Amine Atoui, Xiangshun Li)</author>
      <guid isPermaLink="false">2508.09462v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled Data due to Annotation Challenges</title>
      <link>http://arxiv.org/abs/2508.09022v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10pages,5figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DPGNet的双路径引导网络，用于解决深度伪造检测中的标注挑战，通过利用未标记数据并弥合不同生成模型间的域差距，实现了比现有方法高6.3%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;现有的深度伪造检测方法严重依赖标记数据，但随着AI生成内容越来越逼真，人工标注也变得困难且不可靠。同时，AI生成的面部图像与真实图像分布相似，导致传统无监督学习方法性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决深度伪造检测中的两个关键挑战：弥合不同生成模型面部间的域差距，以及有效利用未标记的图像样本来缓解标注难题。&lt;h4&gt;方法&lt;/h4&gt;提出双路径引导网络(DPGNet)，包含两个核心模块：文本引导的跨域对齐(使用可学习提示统一视觉和文本嵌入到域不变特征空间)和课程驱动的伪标签生成(动态利用信息量更大的未标记样本)。此外，采用跨域知识蒸馏防止灾难性遗忘。&lt;h4&gt;主要发现&lt;/h4&gt;在11个流行数据集上的实验表明，DPGNet比现有最先进方法高出6.3%的性能，验证了其在利用未标记数据解决深度伪造标注挑战方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;DPGNet通过有效利用未标记数据，能够应对深度伪造日益真实性带来的标注挑战，为深度伪造检测提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;现有的深度伪造检测方法严重依赖标记的训练数据。然而，随着AI生成内容变得越来越逼真，即使是人工标注者也难以区分深度伪造和真实图像。这使得标注过程既耗时又不可靠。具体来说，对于能够有效利用来自在线社交网络的大规模未标记数据的方法需求日益增长。与典型的无监督学习任务(类别明显不同)不同，AI生成的面部图像紧密模仿真实图像分布并具有强相似性，导致传统策略性能下降。在本文中，我们引入双路径引导网络(DPGNet)，以解决两个关键挑战：(1)弥合不同生成模型面部之间的域差距，(2)利用未标记的图像样本。该方法具有两个核心模块：文本引导的跨域对齐，使用可学习提示将视觉和文本嵌入统一到域不变特征空间；课程驱动的伪标签生成，动态利用信息量更大的未标记样本。为防止灾难性遗忘，我们还通过跨域知识蒸馏促进域之间的桥接。在11个流行数据集上的大量实验表明，DPGNet比最先进的方法高出6.3%，突显了其利用未标记数据解决深度伪造日益真实性带来的标注挑战的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing deepfake detection methods heavily depend on labeled training data.However, as AI-generated content becomes increasingly realistic, even\textbf{human annotators struggle to distinguish} between deepfakes andauthentic images. This makes the labeling process both time-consuming and lessreliable. Specifically, there is a growing demand for approaches that caneffectively utilize large-scale unlabeled data from online social networks.Unlike typical unsupervised learning tasks, where categories are distinct,AI-generated faces closely mimic real image distributions and share strongsimilarities, causing performance drop in conventional strategies. In thispaper, we introduce the Dual-Path Guidance Network (DPGNet), to tackle two keychallenges: (1) bridging the domain gap between faces from different generationmodels, and (2) utilizing unlabeled image samples. The method features two coremodules: text-guided cross-domain alignment, which uses learnable prompts tounify visual and textual embeddings into a domain-invariant feature space, andcurriculum-driven pseudo label generation, which dynamically exploit moreinformative unlabeled samples. To prevent catastrophic forgetting, we alsofacilitate bridging between domains via cross-domain knowledge distillation.Extensive experiments on \textbf{11 popular datasets}, show that DPGNetoutperforms SoTA approaches by \textbf{6.3\%}, highlighting its effectivenessin leveraging unlabeled data to address the annotation challenges posed by theincreasing realism of deepfakes.</description>
      <author>example@mail.com (Zhiqiang Yang, Renshuai Tao, Xiaolong Zheng, Guodong Yang, Chunjie Zhang)</author>
      <guid isPermaLink="false">2508.09022v2</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring</title>
      <link>http://arxiv.org/abs/2508.09187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于呼吸分析作为健康监测工具的综述研究，全面考察了接触式和非接触式方法，重点强调了机器学习和深度学习技术的最新应用。&lt;h4&gt;背景&lt;/h4&gt;呼吸分析已成为健康监测的关键工具，能提供呼吸功能、疾病检测和持续健康评估的见解。传统接触式方法虽然可靠，但在舒适性和实用性方面存在挑战，特别是对于长期监测。&lt;h4&gt;目的&lt;/h4&gt;全面考察接触式和非接触式呼吸分析方法，强调机器学习和深度学习技术的最新进展，分析非接触式方法的准确性，探讨各种应用场景，并总结关键挑战和新兴趋势。&lt;h4&gt;方法&lt;/h4&gt;分析包括Wi-Fi信道状态信息和声学传感在内的非接触式方法，研究从单用户呼吸率检测到多用户场景、用户识别和呼吸疾病检测的广泛应用。详细介绍了数据预处理、特征提取和分类技术，并提供不同方法的机器学习/深度学习模型比较。&lt;h4&gt;主要发现&lt;/h4&gt;非接触式方法能提供准确、非侵入性的呼吸监测；详细比较了适合不同方法的机器学习/深度学习模型；讨论了数据集稀缺、多用户干扰和数据隐私等挑战；指出可解释AI、联邦学习、迁移学习和混合建模等新兴趋势。&lt;h4&gt;结论&lt;/h4&gt;通过综合当前方法并确定开放研究方向，该综述提供了指导呼吸分析未来创新的全面框架，旨在将先进技术能力与实际医疗保健应用相结合。&lt;h4&gt;翻译&lt;/h4&gt;呼吸分析已成为健康监测的关键工具，为呼吸功能、疾病检测和持续健康评估提供了见解。虽然传统的接触式方法可靠，但它们在舒适性和实用性方面常常带来挑战，特别是对于长期监测。本综述全面考察了接触式和非接触式方法，重点强调了应用于呼吸分析的机器学习和深度学习技术的最新进展。分析了包括Wi-Fi信道状态信息和声学传感在内的非接触式方法，探讨了它们提供准确、非侵入性呼吸监测的能力。我们研究了从单用户呼吸率检测到多用户场景、用户识别和呼吸疾病检测的广泛应用。此外，本综述详细介绍了必要的数据预处理、特征提取和分类技术，提供了适合每种方法的机器学习/深度学习模型的比较见解。还讨论了数据集稀缺、多用户干扰和数据隐私等关键挑战，以及可解释AI、联邦学习、迁移学习和混合建模等新兴趋势。通过综合当前方法并确定开放研究方向，本综述提供了一个全面的框架，以指导呼吸分析的未来创新，将先进的技术能力与实际的医疗保健应用相结合。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.smhl.2025.100579&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Breath analysis has emerged as a critical tool in health monitoring, offeringinsights into respiratory function, disease detection, and continuous healthassessment. While traditional contact-based methods are reliable, they oftenpose challenges in comfort and practicality, particularly for long-termmonitoring. This survey comprehensively examines contact-based and contactlessapproaches, emphasizing recent advances in machine learning and deep learningtechniques applied to breath analysis. Contactless methods, including Wi-FiChannel State Information and acoustic sensing, are analyzed for their abilityto provide accurate, noninvasive respiratory monitoring. We explore a broadrange of applications, from single-user respiratory rate detection tomulti-user scenarios, user identification, and respiratory disease detection.Furthermore, this survey details essential data preprocessing, featureextraction, and classification techniques, offering comparative insights intomachine learning/deep learning models suited to each approach. Key challengeslike dataset scarcity, multi-user interference, and data privacy are alsodiscussed, along with emerging trends like Explainable AI, federated learning,transfer learning, and hybrid modeling. By synthesizing current methodologiesand identifying open research directions, this survey offers a comprehensiveframework to guide future innovations in breath analysis, bridging advancedtechnological capabilities with practical healthcare applications.</description>
      <author>example@mail.com (Almustapha A. Wakili, Babajide J. Asaju, Woosub Jung)</author>
      <guid isPermaLink="false">2508.09187v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection</title>
      <link>http://arxiv.org/abs/2508.09913v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于音频-视觉语音表征学习的人脸伪造视频检测方法，通过利用音频和视觉语音元素间的协同效应，在无需伪造视频参与训练的情况下实现了跨数据集泛化和鲁棒性的显著提升。&lt;h4&gt;背景&lt;/h4&gt;人脸伪造视频检测是数字取证领域的一大挑战，特别是在对未见数据集和常见干扰的泛化能力方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;解决人脸伪造视频检测中的泛化问题，特别是提高对未见数据集和常见干扰的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;通过自监督的掩码预测任务在真实视频上学习音频-视觉语音表征，同时编码局部和全局语义信息，然后将学习到的模型直接迁移到伪造检测任务。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该方法在跨数据集泛化能力和鲁棒性方面优于现有最先进方法，且模型训练过程中无需使用任何伪造视频。&lt;h4&gt;结论&lt;/h4&gt;基于音频-视觉语音表征的学习方法能有效提升人脸伪造视频检测的泛化能力和鲁棒性，为该领域提供了新的解决思路。&lt;h4&gt;翻译&lt;/h4&gt;人脸伪造视频的检测仍然是数字取证领域的一项艰巨挑战，特别是在对未见数据集和常见干扰的泛化方面。本文通过利用音频和视觉语音元素之间的协同效应，采用了一种新颖的音频-视觉语音表征学习方法来解决这一问题。我们的研究动机是发现富含语音内容的音频信号能提供精确反映面部运动的信息。为此，我们首先通过自监督的掩码预测任务在真实视频上学习精确的音频-视觉语音表征，同时编码局部和全局语义信息。然后将得到的模型直接迁移到伪造检测任务。大量实验表明，在跨数据集泛化能力和鲁棒性方面，我们的方法优于最先进的方法，且模型训练过程中无需使用任何伪造视频。代码可在https://github.com/Eleven4AI/SpeechForensics获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detection of face forgery videos remains a formidable challenge in the fieldof digital forensics, especially the generalization to unseen datasets andcommon perturbations. In this paper, we tackle this issue by leveraging thesynergy between audio and visual speech elements, embarking on a novel approachthrough audio-visual speech representation learning. Our work is motivated bythe finding that audio signals, enriched with speech content, can provideprecise information effectively reflecting facial movements. To this end, wefirst learn precise audio-visual speech representations on real videos via aself-supervised masked prediction task, which encodes both local and globalsemantic information simultaneously. Then, the derived model is directlytransferred to the forgery detection task. Extensive experiments demonstratethat our method outperforms the state-of-the-art methods in terms ofcross-dataset generalization and robustness, without the participation of anyfake video in model training. Code is available athttps://github.com/Eleven4AI/SpeechForensics.</description>
      <author>example@mail.com (Yachao Liang, Min Yu, Gang Li, Jianguo Jiang, Boquan Li, Feng Yu, Ning Zhang, Xiang Meng, Weiqing Huang)</author>
      <guid isPermaLink="false">2508.09913v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>PaCo-FR: Patch-Pixel Aligned End-to-End Codebook Learning for Facial Representation Pre-training</title>
      <link>http://arxiv.org/abs/2508.09691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PaCo-FR的无监督面部表征预训练框架，结合掩码图像建模和块-像素对齐技术，解决了现有方法在捕捉面部特征、保留空间结构和利用有限标记数据方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;面部表征预训练对面部识别、表情分析和虚拟现实等任务至关重要，但现有方法面临三个关键挑战：无法捕捉独特面部特征和细粒度语义、忽略面部解剖学固有的空间结构、低效利用有限的标记数据。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的三个关键挑战，开发一种高效的面部表征预训练框架，减少对昂贵标注数据集的依赖。&lt;h4&gt;方法&lt;/h4&gt;PaCo-FR框架包含三个创新组件：(1)结构化掩码策略，保留面部区域的空间连贯性；(2)基于块的代码本，使用多个候选令牌增强特征区分能力；(3)空间一致性约束，保留面部组件间的几何关系。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用200万张未标记图像进行预训练，PaCo-FR在多个面部分析任务上取得最先进性能，尤其在姿势变化、遮挡和不同光照条件场景中表现显著提升。&lt;h4&gt;结论&lt;/h4&gt;PaCo-FR推动了面部表征学习发展，提供了可扩展、高效的解决方案，减少了对昂贵标注数据集的依赖，促进了更有效的面部分析系统。&lt;h4&gt;翻译&lt;/h4&gt;面部表征预训练对于面部识别、表情分析和虚拟现实等任务至关重要。然而，现有方法面临三个关键挑战：(1)无法捕捉独特的面部特征和细粒度语义，(2)忽略了面部解剖学固有的空间结构，(3)低效地利用有限的标记数据。为克服这些问题，我们引入了PaCo-FR，这是一种结合掩码图像建模和块-像素对齐的无监督框架。我们的方法整合了三个创新组件：(1)结构化掩码策略，通过与语义上有意义的面部区域保持一致来保留空间连贯性，(2)基于块的代码本，使用多个候选令牌增强特征区分能力，(3)空间一致性约束，保留面部组件之间的几何关系。仅使用200万张未标记图像进行预训练，PaCo-FR在多个面部分析任务上取得了最先进的性能。我们的方法在不同姿势、遮挡和光照条件的场景中显示出显著改进。我们认为这项工作推动了面部表征学习的发展，并提供了一种可扩展、高效的解决方案，减少了对昂贵标注数据集的依赖，促进了更有效的面部分析系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Facial representation pre-training is crucial for tasks like facialrecognition, expression analysis, and virtual reality. However, existingmethods face three key challenges: (1) failing to capture distinct facialfeatures and fine-grained semantics, (2) ignoring the spatial structureinherent to facial anatomy, and (3) inefficiently utilizing limited labeleddata. To overcome these, we introduce PaCo-FR, an unsupervised framework thatcombines masked image modeling with patch-pixel alignment. Our approachintegrates three innovative components: (1) a structured masking strategy thatpreserves spatial coherence by aligning with semantically meaningful facialregions, (2) a novel patch-based codebook that enhances feature discriminationwith multiple candidate tokens, and (3) spatial consistency constraints thatpreserve geometric relationships between facial components. PaCo-FR achievesstate-of-the-art performance across several facial analysis tasks with just 2million unlabeled images for pre-training. Our method demonstrates significantimprovements, particularly in scenarios with varying poses, occlusions, andlighting conditions. We believe this work advances facial representationlearning and offers a scalable, efficient solution that reduces reliance onexpensive annotated datasets, driving more effective facial analysis systems.</description>
      <author>example@mail.com (Yin Xie, Zhichao Chen, Xiaoze Yu, Yongle Zhao, Xiang An, Kaicheng Yang, Zimin Ran, Jia Guo, Ziyong Feng, Jiankang Deng)</author>
      <guid isPermaLink="false">2508.09691v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning</title>
      <link>http://arxiv.org/abs/2508.09281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一个新颖的、可解释的框架，用于通过基于模式的KCs来自动发现知识组件。这些模式是学生代码中重复出现的结构模式，捕捉了学生必须掌握的特定编程模式和语言结构。&lt;h4&gt;背景&lt;/h4&gt;在计算机科学教育中，个性化学习依赖于准确建模学生已掌握和需要学习的内容。知识组件(KCs)为此提供了基础，但自动从学生代码中提取KCs具有挑战性，因为发现的KCs可解释性不足，编程问题的开放性，以及学生解决方案之间的结构差异显著，编程概念之间的复杂交互。&lt;h4&gt;目的&lt;/h4&gt;开发一个自动化、可扩展且可解释的框架，用于识别细粒度的代码模式和算法结构，这对学生学习至关重要。&lt;h4&gt;方法&lt;/h4&gt;训练一个变分自编码器，在基于注意力的可解释代码表示模型的指导下，从学生代码中生成重要的代表性模式。该模型能识别学生代码中重要的正确和错误模式实现。然后将这些模式聚类形成基于模式的KCs。使用学习曲线分析和深度知识追踪(DKT)两种基于认知科学的方法评估KCs。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明有意义的学习轨迹，以及在DKT预测性能上与传统KT方法相比有显著改进。&lt;h4&gt;结论&lt;/h4&gt;这项工作通过提供自动化、可扩展且可解释的框架来识别细粒度的代码模式和算法结构，推进了计算机科学教育中的知识建模，这对学生学习至关重要。&lt;h4&gt;翻译&lt;/h4&gt;计算机科学教育中有效的个性化学习依赖于准确建模学生已掌握和需要学习的内容。虽然知识组件(KCs)为此提供了基础，但由于发现的KCs可解释性不足、编程问题的开放性以及学生解决方案之间的结构差异显著和编程概念之间的复杂交互，自动从学生代码中提取KCs本质上是具有挑战性的。在这项工作中，我们提出了一种新颖的、可解释的框架，用于通过基于模式的KCs来自动发现知识组件：学生代码中重复出现的结构模式，捕捉了学生必须掌握的特定编程模式和语言结构。为此，我们训练了一个变分自编码器，在可解释的、基于注意力的代码表示模型的指导下，从学生代码中生成重要的代表性模式，该模型能识别学生代码中重要的正确和错误模式实现。然后将这些模式聚类形成基于模式的KCs。我们使用两种基于认知科学的方法评估我们的KCs：学习曲线分析和深度知识追踪(DKT)。实验结果表明有意义的学习轨迹，以及与传统KT方法相比，DKT预测性能有显著改进。这项工作通过提供自动化、可扩展且可解释的框架来识别细粒度的代码模式和算法结构（对学生学习至关重要），推进了计算机科学教育中的知识建模。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective personalized learning in computer science education depends onaccurately modeling what students know and what they need to learn. WhileKnowledge Components (KCs) provide a foundation for such modeling, automated KCextraction from student code is inherently challenging due to insufficientexplainability of discovered KCs and the open-endedness of programming problemswith significant structural variability across student solutions and complexinteractions among programming concepts. In this work, we propose a novel,explainable framework for automated KC discovery through pattern-based KCs:recurring structural patterns within student code that capture the specificprogramming patterns and language constructs that students must master. Towardthis, we train a Variational Autoencoder to generate important representativepatterns from student code guided by an explainable, attention-based coderepresentation model that identifies important correct and incorrect patternimplementations from student code. These patterns are then clustered to formpattern-based KCs. We evaluate our KCs using two well-established methodsinformed by Cognitive Science: learning curve analysis and Deep KnowledgeTracing (DKT). Experimental results demonstrate meaningful learningtrajectories and significant improvements in DKT predictive performance overtraditional KT methods. This work advances knowledge modeling in CS educationby providing an automated, scalable, and explainable framework for identifyinggranular code patterns and algorithmic constructs, essential for studentlearning.</description>
      <author>example@mail.com (Muntasir Hoq, Griffin Pitts, Andrew Lan, Peter Brusilovsky, Bita Akram)</author>
      <guid isPermaLink="false">2508.09281v1</guid>
      <pubDate>Thu, 14 Aug 2025 15:34:18 +0800</pubDate>
    </item>
    <item>
      <title>Chartwin: a Case Study on Channel Charting-aided Localization in Dynamic Digital Network Twins</title>
      <link>http://arxiv.org/abs/2508.09055v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Chartwin，一种将面向定位的信道图表与动态数字网络孪生(DNTs)集成的案例研究，展示了半监督信道图表在构建空间一致无线电地图方面的显著性能。&lt;h4&gt;背景&lt;/h4&gt;无线通信系统可以从空间一致的无线信道表征中显著受益，以高效执行各种通信任务。&lt;h4&gt;目的&lt;/h4&gt;实现局部和全局一致的无线电地图，通过信道图表技术提高无线通信系统的性能。&lt;h4&gt;方法&lt;/h4&gt;提出Chartwin，将面向定位的信道图表与动态数字网络孪生(DNTs)进行集成。&lt;h4&gt;主要发现&lt;/h4&gt;半监督信道图表在构建扩展城市环境的空间一致图表方面具有显著性能，静态DNT的定位误差约为4.5米，动态DNT的定位误差约为6米。&lt;h4&gt;结论&lt;/h4&gt;DNT辅助的信道图表和定位技术具有良好性能，有助于提高无线通信系统的空间一致性和定位精度。&lt;h4&gt;翻译&lt;/h4&gt;无线通信系统可以从空间一致的无线信道表征中显著受益，以高效执行各种通信任务。为此，信道图表已被引入作为一种有效的无监督学习技术，以实现局部和全局一致的无线电地图。在本信函中，我们提出了Chartwin，这是一个关于将面向定位的信道图表与动态数字网络孪生(DNTs)集成的案例研究。数值结果表明，半监督信道图表在构建所考虑的扩展城市环境的空间一致图表方面具有显著性能。所考虑的方法对于静态DNT导致约4.5米的定位误差，对于动态DNT导致约6米的定位误差，促进了DNT辅助的信道图表和定位。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wireless communication systems can significantly benefit from theavailability of spatially consistent representations of the wireless channel toefficiently perform a wide range of communication tasks. Towards this purpose,channel charting has been introduced as an effective unsupervised learningtechnique to achieve both locally and globally consistent radio maps. In thisletter, we propose Chartwin, a case study on the integration oflocalization-oriented channel charting with dynamic Digital Network Twins(DNTs). Numerical results showcase the significant performance ofsemi-supervised channel charting in constructing a spatially consistent chartof the considered extended urban environment. The considered method results in$\approx$ 4.5 m localization error for the static DNT and $\approx$ 6 m in thedynamic DNT, fostering DNT-aided channel charting and localization.</description>
      <author>example@mail.com (Lorenzo Cazzella, Francesco Linsalata, Mahdi Maleki, Damiano Badini, Matteo Matteucci, Umberto Spagnolini)</author>
      <guid isPermaLink="false">2508.09055v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
  <item>
      <title>When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled Data due to Annotation Challenges</title>
      <link>http://arxiv.org/abs/2508.09022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10pages,5figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为双路径引导网络(DPGNet)的新型深度伪造检测方法，解决了现有方法对标记数据的依赖问题，通过文本引导的跨域对齐和课程驱动的伪标签生成有效利用未标记数据，在多个数据集上实现了比现有方法高6.3%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;现有的深度伪造检测方法严重依赖标记的训练数据，但随着AI生成内容变得越来越逼真，即使是人工标注者也难以区分深度伪造和真实图像，这使得标注过程既耗时又不可靠。市场对能够有效利用来自在线社交网络的大规模未标记数据的方法需求日益增长。&lt;h4&gt;目的&lt;/h4&gt;解决深度伪造检测中的两个关键挑战：(1)弥合不同生成模型面部之间的域差距，(2)利用未标记的图像样本来克服标注困难。&lt;h4&gt;方法&lt;/h4&gt;提出双路径引导网络(DPGNet)，包含两个核心模块：文本引导的跨域对齐(使用可学习的提示将视觉和文本嵌入统一到域不变特征空间)和课程驱动的伪标签生成(动态利用信息量更大的未标记样本)。同时通过跨域知识蒸馏来防止灾难性遗忘。&lt;h4&gt;主要发现&lt;/h4&gt;在11个流行数据集上的大量实验表明，DPGNet比现有最先进的方法高出6.3%的性能，突显了其在利用未标记数据解决深度伪造日益真实性带来的标注挑战方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;DPGNet能够有效利用未标记数据解决深度伪造检测中的标注挑战，通过处理不同生成模型域差距和智能选择未标记样本，显著提高了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;现有的深度伪造检测方法严重依赖标记的训练数据。然而，随着AI生成内容变得越来越真实，即使是人工标注者也难以区分深度伪造和真实图像。这使得标注过程既耗时又不可靠。具体而言，市场对能够有效利用来自在线社交网络的大规模未标记数据的方法需求日益增长。与典型的无监督学习任务不同(其中类别是明确的)，AI生成的面部图像与真实图像分布非常相似，并具有强相似性，导致传统策略性能下降。在本文中，我们引入了双路径引导网络(DPGNet)，以解决两个关键挑战：(1)弥合不同生成模型面部之间的域差距，(2)利用未标记的图像样本。该方法具有两个核心模块：文本引导的跨域对齐，使用可学习的提示将视觉和文本嵌入统一到域不变特征空间；以及课程驱动的伪标签生成，动态利用信息量更大的未标记样本。为了防止灾难性遗忘，我们还通过跨域知识蒸馏促进域之间的桥接。在11个流行数据集上的大量实验表明，DPGNet比最先进的方法高出6.3%，突显了其在利用未标记数据解决深度伪造日益真实性带来的标注挑战方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing deepfake detection methods heavily depend on labeled training data.However, as AI-generated content becomes increasingly realistic, even\textbf{human annotators struggle to distinguish} between deepfakes andauthentic images. This makes the labeling process both time-consuming and lessreliable. Specifically, there is a growing demand for approaches that caneffectively utilize large-scale unlabeled data from online social networks.Unlike typical unsupervised learning tasks, where categories are distinct,AI-generated faces closely mimic real image distributions and share strongsimilarities, causing performance drop in conventional strategies. In thispaper, we introduce the Dual-Path Guidance Network (DPGNet), to tackle two keychallenges: (1) bridging the domain gap between faces from different generationmodels, and (2) utilizing unlabeled image samples. The method features two coremodules: text-guided cross-domain alignment, which uses learnable prompts tounify visual and textual embeddings into a domain-invariant feature space, andcurriculum-driven pseudo label generation, which dynamically exploit moreinformative unlabeled samples. To prevent catastrophic forgetting, we alsofacilitate bridging between domains via cross-domain knowledge distillation.Extensive experiments on \textbf{11 popular datasets}, show that DPGNetoutperforms SoTA approaches by \textbf{6.3\%}, highlighting its effectivenessin leveraging unlabeled data to address the annotation challenges posed by theincreasing realism of deepfakes.</description>
      <author>example@mail.com (Zhiqiang Yang, Renshuai Tao, Xiaolong Zheng, Guodong Yang, Chunjie Zhang)</author>
      <guid isPermaLink="false">2508.09022v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Empirical Bayes for Data Integration</title>
      <link>http://arxiv.org/abs/2508.08336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了经验贝叶斯方法在数据整合中的应用，特别是在迁移学习的背景下。研究关注的是在只能访问不完整数据的情况下学习结构（如特征选择）的问题。&lt;h4&gt;背景&lt;/h4&gt;研究者希望学习数据结构，但只能访问先前研究的不完整数据，如摘要、估计或相关特征列表。&lt;h4&gt;目的&lt;/h4&gt;开发一种计算框架，用于经验贝叶斯方法进行数据整合，并比较其与完全贝叶斯方法在变量选择和收敛速率方面的表现。&lt;h4&gt;方法&lt;/h4&gt;讨论完全贝叶斯与经验贝叶斯的差异，为经验贝叶斯开发计算框架，并通过高维回归示例进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;经验贝叶斯在较弱条件下（稀疏性和beta最小假设）实现了一致的变量选择，具有更快的收敛速率；完全贝叶斯推断具有出色特性；经验贝叶斯数据整合在实践中提供适度的但有意义的改进。&lt;h4&gt;结论&lt;/h4&gt;经验贝叶斯方法在处理不完整数据的数据整合和迁移学习方面具有优势，能够提供实用的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;我们讨论了经验贝叶斯在数据整合中的应用，即在迁移学习的意义上。我们的主要兴趣在于那些希望学习结构（如特征选择）但只能访问来自先前研究的不完整数据的场景，如摘要、估计或相关特征列表。我们讨论了完全贝叶斯与经验贝叶斯之间的差异，并为后者开发了计算框架。我们讨论了经验贝叶斯如何在比完全贝叶斯和其他标准标准更弱的条件下（稀疏性和beta最小假设）实现一致的变量选择，以及如何实现更快的收敛速率。我们的高维回归示例表明，完全贝叶斯推断具有出色的特性，而使用经验贝叶斯进行数据整合在实践中可以提供适度的但有意义的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We discuss the use of empirical Bayes for data integration, in the sense oftransfer learning. Our main interest is in settings where one wishes to learnstructure (e.g. feature selection) and one only has access to incomplete datafrom previous studies, such as summaries, estimates or lists of relevantfeatures. We discuss differences between full Bayes and empirical Bayes, anddevelop a computational framework for the latter. We discuss how empiricalBayes attains consistent variable selection under weaker conditions (sparsityand betamin assumptions) than full Bayes and other standard criteria do, andhow it attains faster convergence rates. Our high-dimensional regressionexamples show that fully Bayesian inference enjoys excellent properties, andthat data integration with empirical Bayes can offer moderate yet meaningfulimprovements in practice.</description>
      <author>example@mail.com (Paul Rognon-Vael, David Rossell)</author>
      <guid isPermaLink="false">2508.08336v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models</title>
      <link>http://arxiv.org/abs/2508.05685v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently under review. Code will be released upon acceptance&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Domain-guided Fine-tuning (DogFit)方法，用于解决扩散模型在较小目标领域迁移学习中的挑战。该方法通过在训练损失中注入领域感知的引导偏置，实现了无需额外计算开销的可控图像生成，同时支持高效的保真度-多样性权衡。&lt;h4&gt;背景&lt;/h4&gt;将扩散模型迁移到较小的目标领域具有挑战性，因为简单的微调模型通常会导致泛化能力差。现有的测试时引导方法虽然可以通过在图像保真度和样本多样性之间进行权衡来改善结果，但需要高昂的计算成本，通常在采样期间需要双重前向传播。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的扩散迁移学习引导机制，能够在不增加额外计算开销的情况下保持对生成图像的可控性，同时支持高效的保真度-多样性权衡。&lt;h4&gt;方法&lt;/h4&gt;DogFit方法将领域感知的引导偏置注入到训练损失中，内化了引导行为。作者通过轻量级条件机制将引导强度值编码为额外的模型输入，以支持推理时的可控性。此外，作者还研究了训练期间引导偏置的最佳位置和时间，提出了两种简单的调度策略：late-start和cut-off。&lt;h4&gt;主要发现&lt;/h4&gt;在微调过程中，无条件源模型比目标模型提供了更强的边际估计；提出的DogFit方法在六种不同目标领域的实验中，能够在FID和FDDINOV2指标上优于先前的引导方法，同时采样所需的计算量减少多达2倍。&lt;h4&gt;结论&lt;/h4&gt;DogFit是一种有效的扩散模型迁移学习方法，它通过领域感知的引导偏置和调度策略，在不增加计算成本的情况下实现了更好的生成质量和可控性。&lt;h4&gt;翻译&lt;/h4&gt;将扩散模型迁移到较小的目标领域具有挑战性，因为简单地微调模型通常会导致泛化能力差。测试时引导方法通过在图像保真度和样本多样性之间进行权衡来缓解这一问题，但这种好处带来了高昂的计算成本，通常在采样期间需要双重前向传播。我们提出了Domain-guided Fine-tuning (DogFit)方法，这是一种用于扩散迁移学习的有效引导机制，可以在不增加额外计算开销的情况下保持可控性。DogFit将领域感知的引导偏置注入到训练损失中，有效地在微调过程中内化了引导行为。领域感知设计源于我们的观察：在微调过程中，无条件源模型比目标模型提供了更强的边际估计。为了在推理时支持高效的可控保真度-多样性权衡，我们通过轻量级条件机制将引导强度值编码为额外的模型输入。我们进一步研究了训练期间引导偏置的最佳位置和时间，并提出了两种简单的调度策略，即late-start和cut-off，这些策略提高了生成质量和训练稳定性。在DiT和SiT主干网络上的六种不同目标领域的实验表明，DogFit在迁移学习中可以优于先前的引导方法，在FID和FDDINOV2指标方面表现更好，同时采样所需的TFLOPS减少多达2倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning of diffusion models to smaller target domains ischallenging, as naively fine-tuning the model often results in poorgeneralization. Test-time guidance methods help mitigate this by offeringcontrollable improvements in image fidelity through a trade-off with samplediversity. However, this benefit comes at a high computational cost, typicallyrequiring dual forward passes during sampling. We propose the Domain-guidedFine-tuning (DogFit) method, an effective guidance mechanism for diffusiontransfer learning that maintains controllability without incurring additionalcomputational overhead. DogFit injects a domain-aware guidance offset into thetraining loss, effectively internalizing the guided behavior during thefine-tuning process. The domain-aware design is motivated by our observationthat during fine-tuning, the unconditional source model offers a strongermarginal estimate than the target model. To support efficient controllablefidelity-diversity trade-offs at inference, we encode the guidance strengthvalue as an additional model input through a lightweight conditioningmechanism. We further investigate the optimal placement and timing of theguidance offset during training and propose two simple scheduling strategies,i.e., late-start and cut-off, which improve generation quality and trainingstability. Experiments on DiT and SiT backbones across six diverse targetdomains show that DogFit can outperform prior guidance methods in transferlearning in terms of FID and FDDINOV2 while requiring up to 2x fewer samplingTFLOPS.</description>
      <author>example@mail.com (Yara Bahram, Mohammadhadi Shateri, Eric Granger)</author>
      <guid isPermaLink="false">2508.05685v2</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>KFFocus: Highlighting Keyframes for Enhanced Video Understanding</title>
      <link>http://arxiv.org/abs/2508.08989v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;KFFocus是一种新的视频压缩方法，能够有效识别关键帧并保留重要信息，同时提高计算效率和准确性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型和多模态LLMs在图像和视频模态中表现出色，但视频LLMs在处理长视频序列时面临巨大的计算需求。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频LLMs压缩策略中忽略关键信息不均匀分布的问题，实现高效的视频标记压缩并保留重要信息内容。&lt;h4&gt;方法&lt;/h4&gt;KFFocus方法用基于经典视频压缩原理的改进方法替代均匀采样，根据时间冗余识别关键帧；根据帧的上下文相关性分配不同压缩比例；引入时空建模模块编码视频帧间的时间关系和每帧内的空间结构。&lt;h4&gt;主要发现&lt;/h4&gt;在广泛认可的视频理解基准测试上，特别是在长视频场景中，KFFocus显著优于现有方法，实现了显著的计算效率和准确性提升。&lt;h4&gt;结论&lt;/h4&gt;KFFocus能够有效解决视频LLMs面临的计算效率和关键信息保留问题，为视频理解任务提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;最近，随着大型语言模型的出现，多模态LLMs在图像和视频模态中表现出色。尽管视频理解能力有所提升，但长视频序列的巨大计算需求导致当前的视频LLMs(Vid-LLMs)在帧间级别(如均匀采样视频帧)和帧内级别(如将每帧的所有视觉标记压缩为有限数量)都采用压缩策略。然而，这种方法往往忽略了关键信息在帧间的不均匀时间分布，可能导致包含重要时间和语义细节的关键帧被遗漏。为解决这些挑战，我们提出了KFFocus，这是一种旨在高效压缩视频标记并强调视频帧中包含信息丰富的上下文的方法。我们用基于经典视频压缩原理的改进方法替代均匀采样，根据时间冗余识别和捕获关键帧。通过根据帧的上下文相关性分配不同的压缩比例，KFFocus在保留信息内容细节的同时有效减少了标记冗余。此外，我们引入了一个时空建模模块，编码视频帧之间的时间关系和每帧内的空间结构，从而为Vid-LLMs提供对时空动态的细致理解。在广泛认可的视频理解基准测试上的大量实验，特别是在长视频场景中，表明KFFocus显著优于现有方法，实现了显著的计算效率和准确性提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, with the emergence of large language models, multimodal LLMs havedemonstrated exceptional capabilities in image and video modalities. Despiteadvancements in video comprehension, the substantial computational demands oflong video sequences lead current video LLMs (Vid-LLMs) to employ compressionstrategies at both the inter-frame level (e.g., uniform sampling of videoframes) and intra-frame level (e.g., condensing all visual tokens of each frameinto a limited number). However, this approach often neglects the uneventemporal distribution of critical information across frames, risking theomission of keyframes that contain essential temporal and semantic details. Totackle these challenges, we propose KFFocus, a method designed to efficientlycompress video tokens and emphasize the informative context present withinvideo frames. We substitute uniform sampling with a refined approach inspiredby classic video compression principles to identify and capture keyframes basedon their temporal redundancy. By assigning varying condensation ratios toframes based on their contextual relevance, KFFocus efficiently reduces tokenredundancy while preserving informative content details. Additionally, weintroduce a spatiotemporal modeling module that encodes both the temporalrelationships between video frames and the spatial structure within each frame,thus providing Vid-LLMs with a nuanced understanding of spatial-temporaldynamics. Extensive experiments on widely recognized video understandingbenchmarks, especially long video scenarios, demonstrate that KFFocussignificantly outperforms existing methods, achieving substantial computationalefficiency and enhanced accuracy.</description>
      <author>example@mail.com (Ming Nie, Chunwei Wang, Hang Xu, Li Zhang)</author>
      <guid isPermaLink="false">2508.08989v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>UniSTFormer: Unified Spatio-Temporal Lightweight Transformer for Efficient Skeleton-Based Action Recognition</title>
      <link>http://arxiv.org/abs/2508.08944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种轻量级的基于骨架的动作识别方法，通过统一的空间-时间建模和多尺度池化融合，显著降低了参数和计算复杂度，同时保持了良好的识别性能。&lt;h4&gt;背景&lt;/h4&gt;现有基于骨架的动作识别方法使用Transformer架构取得了显著进展，但这些方法通常依赖复杂的模块组合和重型设计，导致参数数量增加、计算成本高和可扩展性有限。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的空间-时间轻量级Transformer框架，减少参数数量和计算成本，同时保持良好的识别性能。&lt;h4&gt;方法&lt;/h4&gt;提出一个统一的空间-时间轻量级Transformer框架，将空间和时间建模集成在单个注意力模块中，消除单独时间建模块的需求；引入简化的多尺度池化融合模块，结合局部和全局池化路径，增强模型捕获细粒度局部运动和全局运动模式的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的广泛实验表明，该轻量级模型在准确性和效率之间取得了优越的平衡，与最先进的基于Transformer的基线相比，参数复杂性减少了58%以上，计算成本降低了60%以上，同时保持了具有竞争力的识别性能。&lt;h4&gt;结论&lt;/h4&gt;提出的统一空间-时间轻量级Transformer框架和多尺度池化融合模块能够有效减少计算复杂度，同时保持良好的动作识别性能。&lt;h4&gt;翻译&lt;/h4&gt;基于骨架的动作识别已通过Transformer架构取得了显著进展。然而，现有方法通常依赖复杂的模块组合和重型设计，导致参数数量增加、计算成本高昂和可扩展性有限。在本文中，我们提出了一个统一的空间-时间轻量级Transformer框架，将空间和时间建模集成在单个注意力模块中，消除了单独时间建模块的需求。这种方法在保留空间建模过程中的时间感知能力的同时减少了冗余计算。此外，我们引入了一个简化的多尺度池化融合模块，结合局部和全局池化路径，以增强模型捕获细粒度局部运动和全局运动模式的能力。在基准数据集上的广泛实验表明，我们的轻量级模型在准确性和效率之间实现了优越的平衡，与最先进的基于Transformer的基线相比，参数复杂性减少了58%以上，计算成本降低了60%以上，同时保持了具有竞争力的识别性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Skeleton-based action recognition (SAR) has achieved impressive progress withtransformer architectures. However, existing methods often rely on complexmodule compositions and heavy designs, leading to increased parameter counts,high computational costs, and limited scalability. In this paper, we propose aunified spatio-temporal lightweight transformer framework that integratesspatial and temporal modeling within a single attention module, eliminating theneed for separate temporal modeling blocks. This approach reduces redundantcomputations while preserving temporal awareness within the spatial modelingprocess. Furthermore, we introduce a simplified multi-scale pooling fusionmodule that combines local and global pooling pathways to enhance the model'sability to capture fine-grained local movements and overarching global motionpatterns. Extensive experiments on benchmark datasets demonstrate that ourlightweight model achieves a superior balance between accuracy and efficiency,reducing parameter complexity by over 58% and lowering computational cost byover 60% compared to state-of-the-art transformer-based baselines, whilemaintaining competitive recognition performance.</description>
      <author>example@mail.com (Wenhan Wu, Zhishuai Guo, Chen Chen, Aidong Lu)</author>
      <guid isPermaLink="false">2508.08944v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Re:Verse -- Can Your VLM Read a Manga?</title>
      <link>http://arxiv.org/abs/2508.08508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究揭示了当前视觉语言模型在处理连续视觉叙事时，表面识别与深层叙事推理之间存在关键差距，特别是在时间因果关系和跨面板连贯性方面存在系统性缺陷。&lt;h4&gt;背景&lt;/h4&gt;尽管最近的大型多模态模型在单幅图像解释方面表现出色，但在连贯故事理解所需的核心要素上表现不佳。&lt;h4&gt;目的&lt;/h4&gt;引入一个新的评估框架，系统性地表征视觉语言模型在叙事理解方面的局限性，并评估它们在长篇叙事理解方面的能力。&lt;h4&gt;方法&lt;/h4&gt;结合细粒度多模态注释、跨模态嵌入分析和检索增强评估，包括严格的注释协议、多种推理范式下的全面评估以及跨模态相似性分析，并应用此框架对《Re:Zero》漫画的11章（308个注释面板）进行研究。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型缺乏真正的故事级智能，特别在处理非线性叙事、角色一致性和跨越长序列的因果推理方面存在困难。&lt;h4&gt;结论&lt;/h4&gt;这项工作为评估叙事智能奠定了基础和实践方法，同时提供了对多模态模型中深度序列理解能力的实用见解。&lt;h4&gt;翻译&lt;/h4&gt;当前视觉语言模型在处理连续视觉叙事时，表现出表面识别与深层叙事推理之间的关键差距。通过对漫画叙事理解的全面调查，我们揭示尽管最近的大型多模态模型在单幅图像解释方面表现出色，但它们在时间因果关系和跨面板连贯性方面存在系统性缺陷，而这些是连贯故事理解的核心要求。我们引入了一个新的评估框架，结合细粒度多模态注释、跨模态嵌入分析和检索增强评估，以系统性地表征这些局限性。我们的方法包括：(i)一个严格的注释协议，通过对齐的轻小说文本将视觉元素与叙事结构联系起来；(ii)在多种推理范式下的全面评估，包括直接推理和检索增强生成；(iii)跨模态相似性分析，揭示了当前VLMs联合表示中的基本错位。将此框架应用于《Re:Zero》漫画的11章（308个注释面板），我们首次对VLMs中的长篇叙事理解进行了系统性研究，通过三个核心评估轴：生成式叙事、上下文对话定位和时间推理。我们的研究结果表明，当前模型缺乏真正的故事级智能，特别是在处理非线性叙事、角色一致性和跨越长序列的因果推理方面存在困难。这项工作为评估叙事智能奠定了基础和实践方法，同时提供了对多模态模型中超越基本识别的深度序列理解能力的实用见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current Vision Language Models (VLMs) demonstrate a critical gap betweensurface-level recognition and deep narrative reasoning when processingsequential visual storytelling. Through a comprehensive investigation of manganarrative understanding, we reveal that while recent large multimodal modelsexcel at individual panel interpretation, they systematically fail at temporalcausality and cross-panel cohesion, core requirements for coherent storycomprehension. We introduce a novel evaluation framework that combinesfine-grained multimodal annotation, cross-modal embedding analysis, andretrieval-augmented assessment to systematically characterize theselimitations.  Our methodology includes (i) a rigorous annotation protocol linking visualelements to narrative structure through aligned light novel text, (ii)comprehensive evaluation across multiple reasoning paradigms, including directinference and retrieval-augmented generation, and (iii) cross-modal similarityanalysis revealing fundamental misalignments in current VLMs' jointrepresentations. Applying this framework to Re:Zero manga across 11 chapterswith 308 annotated panels, we conduct the first systematic study of long-formnarrative understanding in VLMs through three core evaluation axes: generativestorytelling, contextual dialogue grounding, and temporal reasoning. Ourfindings demonstrate that current models lack genuine story-level intelligence,struggling particularly with non-linear narratives, character consistency, andcausal inference across extended sequences. This work establishes both thefoundation and practical methodology for evaluating narrative intelligence,while providing actionable insights into the capability of deep sequentialunderstanding of Discrete Visual Narratives beyond basic recognition inMultimodal Models.</description>
      <author>example@mail.com (Aaditya Baranwal, Madhav Kataria, Naitik Agrawal, Yogesh S Rawat, Shruti Vyas)</author>
      <guid isPermaLink="false">2508.08508v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Audio-Visual Speech Enhancement: Architectural Design and Deployment Strategies</title>
      <link>http://arxiv.org/abs/2508.08468v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种基于AI的视听语音增强系统，并比较了不同部署架构的性能表现。&lt;h4&gt;背景&lt;/h4&gt;随着通信技术的发展，语音增强技术在各种应用场景中变得越来越重要，特别是在嘈杂环境下的语音通信。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的视听语音增强系统，并分析不同部署架构在性能、延迟和计算开销方面的差异，为实际应用提供部署指南。&lt;h4&gt;方法&lt;/h4&gt;系统采用卷积神经网络进行频谱特征提取，使用长短期记忆网络进行时间建模，通过音频和视觉提示的多模态融合实现语音增强。研究比较了云部署、边缘辅助部署和独立设备部署三种架构，并在以太网、Wi-Fi、4G和5G等不同网络条件下进行了实验。&lt;h4&gt;主要发现&lt;/h4&gt;云部署方案虽然能实现最高的语音增强质量，但延迟较高；边缘辅助架构在语音质量和延迟之间取得了最佳平衡，能够满足5G和Wi-Fi 6条件下的实时要求。&lt;h4&gt;结论&lt;/h4&gt;研究结果为在不同应用场景中选择和优化视听语音增强系统部署架构提供了实用指导，包括辅助听力设备、远程呈现和工业通信等领域。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种新的基于人工智能的视听语音增强系统，并对不同部署架构的性能进行了比较分析。所提出的AVSE系统采用卷积神经网络进行频谱特征提取，使用长短期记忆网络进行时间建模，通过音频和视觉提示的多模态融合实现稳健的语音增强。研究调查了多种部署场景，包括基于云的、边缘辅助的和独立设备的实现。从语音质量改进、延迟和计算开销等方面评估了它们的性能。在各种网络条件下进行了实际实验，包括以太网、Wi-Fi、4G和5G，以分析处理延迟、通信延迟和感知语音质量之间的权衡。结果表明，虽然云部署实现了最高的增强质量，但边缘辅助架构在延迟和可理解性之间提供了最佳平衡，能够满足5G和Wi-Fi 6条件下的实时要求。这些发现为在不同应用中选择和优化AVSE部署架构提供了实用指导，包括辅助听力设备、远程呈现和工业通信。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a new AI-based Audio-Visual Speech Enhancement (AVSE)system and presents a comparative performance analysis of different deploymentarchitectures. The proposed AVSE system employs convolutional neural networks(CNNs) for spectral feature extraction and long short-term memory (LSTM)networks for temporal modeling, enabling robust speech enhancement throughmultimodal fusion of audio and visual cues. Multiple deployment scenarios areinvestigated, including cloud-based, edge-assisted, and standalone deviceimplementations. Their performance is evaluated in terms of speech qualityimprovement, latency, and computational overhead. Real-world experiments areconducted across various network conditions, including Ethernet, Wi-Fi, 4G, and5G, to analyze the trade-offs between processing delay, communication latency,and perceptual speech quality. The results show that while cloud deploymentachieves the highest enhancement quality, edge-assisted architectures offer thebest balance between latency and intelligibility, meeting real-timerequirements under 5G and Wi-Fi 6 conditions. These findings provide practicalguidelines for selecting and optimizing AVSE deployment architectures indiverse applications, including assistive hearing devices, telepresence, andindustrial communications.</description>
      <author>example@mail.com (Anis Hamadouche, Haifeng Luo, Mathini Sellathurai, Tharm Ratnarajah)</author>
      <guid isPermaLink="false">2508.08468v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Whole-Body Coordination for Dynamic Object Grasping with Legged Manipulators</title>
      <link>http://arxiv.org/abs/2508.08328v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出DQ-Bench基准和DQ-Net网络，解决了四足机器人动态抓取问题，实现了在动态环境中的高效物体抓取。&lt;h4&gt;背景&lt;/h4&gt;带机械臂的四足机器人通过全身协调控制，在不规则、动态环境中具有强大移动性和适应性，但现有研究主要关注静态物体抓取，忽视了动态目标带来的挑战，限制了在物流分拣和人机协作等动态场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;解决四足机器人在动态环境中抓取动态目标的问题，提高机器人在动态场景中的适用性。&lt;h4&gt;方法&lt;/h4&gt;引入DQ-Bench基准系统评估不同条件下的动态抓取能力；提出DQ-Net教师-学生框架，教师网络利用特权信息建模目标特性并提供指导，学生网络仅使用有限感知数据进行双视点时间建模实现闭环动作输出。&lt;h4&gt;主要发现&lt;/h4&gt;在DQ-Bench上的大量实验表明，DQ-Net在多种任务设置中实现了稳健的动态物体抓取，在成功率和响应速度方面显著优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;DQ-Bench和DQ-Net为四足机器人在动态环境中的抓取任务提供了有效解决方案，显著提高了机器人在动态场景中的适用性。&lt;h4&gt;翻译&lt;/h4&gt;带机械臂的四足机器人通过全身协调控制，在不规则、动态环境中提供了强大的移动性和抓取适应性。然而，现有研究主要关注静态物体抓取，忽视了动态目标带来的挑战，从而限制了在物流分拣和人机协作等动态场景中的应用。为此，我们引入了DQ-Bench，一个新的基准，系统性地评估不同物体运动、速度、高度、物体类型和地形复杂度下的动态抓取能力，并配有全面的评估指标。基于此基准，我们提出了DQ-Net，一个紧凑的教师-学生框架，设计用于从有限的感知线索中推断抓取配置。在训练过程中，教师网络利用特权信息全面建模目标的静态几何特性和动态运动特征，并集成抓取融合模块为运动规划提供稳健指导。同时，我们设计了一个轻量级的学生网络，仅使用目标掩码、深度图和本体感受状态进行双视点时间建模，实现闭环动作输出而不依赖特权数据。在DQ-Bench上的大量实验表明，DQ-Net在多种任务设置中实现了稳健的动态物体抓取，在成功率和响应速度方面显著优于基线方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决四足机器人在动态环境中抓取移动物体的问题。这个问题在现实中非常重要，因为真实世界中的物体通常是运动的，而非静止的，动态抓取是物流分拣、人机协作等应用场景的核心需求。现有研究主要集中在静态物体抓取上，限制了机器人在实际动态环境中的应用能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有研究的局限性，然后设计了DQ-Bench基准测试平台来系统评估动态抓取性能。基于此，他们提出了DQ-Net教师-学生框架：教师网络利用特权信息学习高质量策略，学生网络仅使用可感知输入模仿教师行为。作者还设计了抓取融合模块(GFM)通过注意力机制动态选择最优抓取姿势。方法借鉴了强化学习、知识蒸馏、Transformer架构和注意力机制等技术，但将其创新性地应用于四足机器人的动态抓取任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用教师-学生框架解决动态抓取问题，通过抓取融合模块动态选择最优抓取姿势，并采用双视角视觉编码增强对物体动态的建模。整体流程分为三部分：1)教师网络使用特权信息和GFM训练，生成高质量抓取策略；2)学生网络仅使用双视角视觉输入和本体感受状态，通过双流Transformer学习模仿教师行为；3)低层控制器将高层命令转换为可执行的关节角度控制信号，实现全身协调控制。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)DQ-Bench基准测试平台，首个专门针对四足机器人动态抓取的评估框架；2)DQ-Net教师-学生框架，结合抓取融合模块和轻量级学生网络；3)抓取融合模块(GFM)，通过注意力机制动态选择最优抓取姿势；4)双视角时序建模，使用双流Transformer捕获全局语义和细粒度动态。相比之前工作，该方法从静态扩展到动态抓取，从依赖高质量视觉输入扩展到使用有限感知输入，通过记忆库提高计算效率，并提出了专门的评估指标全面评估性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DQ-Bench和DQ-Net共同解决了四足机器人在动态环境中抓取移动物体的挑战，通过教师-学生框架和抓取融合模块实现了高效、鲁棒的全身协调控制，显著提高了动态抓取的成功率和响应速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quadrupedal robots with manipulators offer strong mobility and adaptabilityfor grasping in unstructured, dynamic environments through coordinatedwhole-body control. However, existing research has predominantly focused onstatic-object grasping, neglecting the challenges posed by dynamic targets andthus limiting applicability in dynamic scenarios such as logistics sorting andhuman-robot collaboration. To address this, we introduce DQ-Bench, a newbenchmark that systematically evaluates dynamic grasping across varying objectmotions, velocities, heights, object types, and terrain complexities, alongwith comprehensive evaluation metrics. Building upon this benchmark, we proposeDQ-Net, a compact teacher-student framework designed to infer graspconfigurations from limited perceptual cues. During training, the teachernetwork leverages privileged information to holistically model both the staticgeometric properties and dynamic motion characteristics of the target, andintegrates a grasp fusion module to deliver robust guidance for motionplanning. Concurrently, we design a lightweight student network that performsdual-viewpoint temporal modeling using only the target mask, depth map, andproprioceptive state, enabling closed-loop action outputs without reliance onprivileged data. Extensive experiments on DQ-Bench demonstrate that DQ-Netachieves robust dynamic objects grasping across multiple task settings,substantially outperforming baseline methods in both success rate andresponsiveness.</description>
      <author>example@mail.com (Qiwei Liang, Boyang Cai, Rongyi He, Hui Li, Tao Teng, Haihan Duan, Changxin Huang, Runhao Zeng)</author>
      <guid isPermaLink="false">2508.08328v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Dynamic Scenes in Ego Centric 4D Point Clouds</title>
      <link>http://arxiv.org/abs/2508.07251v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了EgoDynamic4D，一个用于从自我中心视角理解动态4D场景的新型QA基准数据集，包含RGB-D视频、相机姿态、实例掩码和4D边界框，并配有927K个问答对和思维链推理。作者还提出了一个统一的时空推理框架，用于处理这些任务。&lt;h4&gt;背景&lt;/h4&gt;从自我中心视角理解动态4D场景（随时间变化的3D空间结构）对人机交互、自主导航和具身智能至关重要。现有自我中心数据集虽包含动态场景，但缺乏统一的4D注释和针对细粒度时空推理的任务驱动评估协议，特别是关于物体和人体运动及其相互作用的方面。&lt;h4&gt;目的&lt;/h4&gt;解决现有自我中心动态场景数据集缺乏统一4D注释和任务驱动评估协议的问题，特别是针对细粒度时空推理，包括物体和人体运动及其相互作用。&lt;h4&gt;方法&lt;/h4&gt;提出EgoDynamic4D基准数据集，包含RGB-D视频、相机姿态、全局唯一实例掩码和4D边界框，构建927K个问答对并配有思维链推理。设计12个动态QA任务，包括代理运动、人机交互等，并配有细粒度指标。提出端到端时空推理框架，统一动态和静态场景信息，使用实例感知特征编码、时间和相机编码，以及空间自适应下采样将4D场景压缩为令牌序列。&lt;h4&gt;主要发现&lt;/h4&gt;在EgoDynamic4D上的实验表明，所提出的方法持续优于基线方法，验证了多模态时间建模在自我中心动态场景理解中的有效性。&lt;h4&gt;结论&lt;/h4&gt;EgoDynamic4D基准数据集和相关方法为从自我中心视角理解动态4D场景提供了新资源和解决方案，特别是在细粒度时空推理方面，有助于推动人机交互、自主导航和具身智能领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;从自我中心视角理解动态4D场景——即随时间变化的3D空间结构的变化——对人机交互、自主导航和具身智能至关重要。虽然现有的自我中心数据集包含动态场景，但它们缺乏统一的4D注释和针对细粒度时空推理的任务驱动评估协议，特别是关于物体和人体的运动及其相互作用。为了解决这一差距，我们介绍了EgoDynamic4D，这是一个关于高度动态场景的新型QA基准，包含RGB-D视频、相机姿态、全局唯一实例掩码和4D边界框。我们构建了927K个问答对，并配有显式的思维链(CoT)，可实现可验证的、逐步的时空推理。我们设计了12个动态QA任务，包括代理运动、人机交互、轨迹预测、关系理解和时间因果推理，并配有细粒度、多维度的指标。为了解决这些任务，我们提出了一个端到端的时空推理框架，该框架统一了动态和静态场景信息，使用实例感知特征编码、时间和相机编码，以及空间自适应下采样，将大型4D场景压缩为LLMs可管理的令牌序列。在EgoDynamic4D上的实验表明，我们的方法持续优于基线方法，验证了多模态时间建模在自我中心动态场景理解中的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从第一人称视角理解动态4D场景（3D空间+时间维度）的问题。现有第一人称数据集缺乏统一的4D标注和任务驱动的评估协议，无法进行细粒度的时空推理，特别是关于物体和人类的运动及其相互作用。这个问题在现实中非常重要，因为机器人感知、增强现实和自动驾驶等应用需要高效准确的第一人称场景理解，以赋能下一代具身智能体。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有数据集的局限性，然后整合了ADT和THUD++两个数据集，创建了统一的、多模态的数据集。他们设计了12个动态QA任务，涵盖场景描述、瞬时动力学和持续动力学。方法设计上借鉴了ConceptFusion和3DLLM的视觉编码技术，使用八叉树进行空间下采样，傅里叶基进行时间编码，以及注意力机制进行相机姿态压缩。这些现有工作为作者提供了基础，但作者在此基础上进行了创新，使其更适合4D动态场景的理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是统一动态和静态场景信息，通过实例感知特征编码、时间和相机编码以及空间自适应下采样，将大型4D场景压缩为LLM可处理的令牌序列。整体流程分为三步：1)实例和时间戳增强的点级特征提取，包括视觉特征、唯一实例嵌入和时间戳；2)特征融合，使用八叉树下采样和傅里叶时间编码；3)投影为LLM令牌，结合相机嵌入和LoRA微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)EgoDynamic4D基准测试，首个专门用于高度动态4D场景理解的第一人称QA基准，包含927K个QA对和12种任务；2)端到端时空推理框架，提出全局唯一实例嵌入、体素时间戳编码和相机嵌入三个新组件；3)多阶段QA数据构建管道。相比之前工作，该研究提供了完整的4D标注而非部分或静态标注，专注于时空推理而非仅图生成，并支持直接QA任务而非仅表示构建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了EgoDynamic4D首个专门用于高度动态4D场景理解的第一人称QA基准测试，以及一个端到端的时空推理框架，实现了对第一人称视角下物体和人类运动及其相互作用的细粒度理解和推理。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding dynamic 4D scenes from an egocentric perspective-modelingchanges in 3D spatial structure over time-is crucial for human-machineinteraction, autonomous navigation, and embodied intelligence. While existingegocentric datasets contain dynamic scenes, they lack unified 4D annotationsand task-driven evaluation protocols for fine-grained spatio-temporalreasoning, especially on motion of objects and human, together with theirinteractions. To address this gap, we introduce EgoDynamic4D, a novel QAbenchmark on highly dynamic scenes, comprising RGB-D video, camera poses,globally unique instance masks, and 4D bounding boxes. We construct 927K QApairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,step-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks coveringagent motion, human-object interaction, trajectory prediction, relationunderstanding, and temporal-causal reasoning, with fine-grained,multidimensional metrics. To tackle these tasks, we propose an end-to-endspatio-temporal reasoning framework that unifies dynamic and static sceneinformation, using instance-aware feature encoding, time and camera encoding,and spatially adaptive down-sampling to compress large 4D scenes into tokensequences manageable by LLMs. Experiments on EgoDynamic4D show that our methodconsistently outperforms baselines, validating the effectiveness of multimodaltemporal modeling for egocentric dynamic scene understanding.</description>
      <author>example@mail.com (Junsheng Huang, Shengyu Hao, Bocheng Hu, Gaoang Wang)</author>
      <guid isPermaLink="false">2508.07251v2</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>GRAVITY: A Controversial Graph Representation Learning for Vertex Classification</title>
      <link>http://arxiv.org/abs/2508.08954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GRAVITY框架，一种基于图的表示学习方法，通过顶点相互作用拓扑实现准确的顶点分类。&lt;h4&gt;背景&lt;/h4&gt;在寻求准确的顶点分类过程中，需要开发新的方法来提高分类性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于图的表示学习方法，用于顶点分类任务，提高分类准确性。&lt;h4&gt;方法&lt;/h4&gt;GRAVITY受物理系统启发，将每个顶点建模为通过学习到的相互作用施加影响，这些相互作用由结构邻近性和属性相似性形成。这些相互作用诱导一个潜在势场，顶点在其中向能量有效的位置移动，围绕类别一致的吸引子聚集。与传统消息传递方案不同，GRAVITY自适应调制每个顶点的感受野，实现由上下文驱动的动态聚合。&lt;h4&gt;主要发现&lt;/h4&gt;GRAVITY通过场驱动的组织使类别边界更加清晰，促进潜在簇内的语义一致性，在直推和归纳顶点分类任务中均表现出色。&lt;h4&gt;结论&lt;/h4&gt;在真实世界基准测试上的实验表明，GRAVITY能够产生具有竞争力的嵌入，在直推和归纳顶点分类任务中均表现出色。&lt;h4&gt;翻译&lt;/h4&gt;在寻求准确的顶点分类过程中，我们引入了GRAVITY(基于图的表示学习通过顶点相互作用拓扑)，这是一种受物理系统启发的框架，在物理系统中，物体在吸引力作用下自组织。GRAVITY将每个顶点建模为通过学习到的相互作用施加影响，这些相互作用由结构邻近性和属性相似性形成。这些相互作用诱导一个潜在势场，顶点在其中向能量有效的位置移动，围绕类别一致的吸引子聚集，并与无关组保持距离。与传统具有静态邻域的消息传递方案不同，GRAVITY基于学习到的力函数自适应地调制每个顶点的感受野，实现由上下文驱动的动态聚合。这种场驱动的组织使类别边界更加清晰，促进潜在簇内的语义一致性。在真实世界基准测试上的实验表明，GRAVITY能够产生具有竞争力的嵌入，在直推和归纳顶点分类任务中均表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the quest of accurate vertex classification, we introduce GRAVITY(Graph-based Representation leArning via Vertices Interaction TopologY), aframework inspired by physical systems where objects self-organize underattractive forces. GRAVITY models each vertex as exerting influence throughlearned interactions shaped by structural proximity and attribute similarity.These interactions induce a latent potential field in which vertices movetoward energy efficient positions, coalescing around class-consistentattractors and distancing themselves from unrelated groups. Unlike traditionalmessage-passing schemes with static neighborhoods, GRAVITY adaptively modulatesthe receptive field of each vertex based on a learned force function, enablingdynamic aggregation driven by context. This field-driven organization sharpensclass boundaries and promotes semantic coherence within latent clusters.Experiments on real-world benchmarks show that GRAVITY yields competitiveembeddings, excelling in both transductive and inductive vertex classificationtasks.</description>
      <author>example@mail.com (Etienne Gael Tajeuna, Jean Marie Tshimula)</author>
      <guid isPermaLink="false">2508.08954v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Learning Generalizable and Efficient Image Watermarking via Hierarchical Two-Stage Optimization</title>
      <link>http://arxiv.org/abs/2508.08667v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为分层水印学习（HiWL）的新方法，通过两阶段优化解决了现有深度水印技术在不可见性、鲁棒性和广泛适用性方面难以同时满足的问题。该方法在实验中表现出色，水印提取准确率比现有方法提高7.6%，同时保持极低延迟（8秒处理10万张图像）。&lt;h4&gt;背景&lt;/h4&gt;深度图像水印技术能够在载体图像中实现不可见水印嵌入和可靠提取，对图像资产版权保护有效。然而，现有方法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有水印方法难以同时满足三个关键标准（不可见性、鲁棒性和广泛适用性）的问题。&lt;h4&gt;方法&lt;/h4&gt;提出分层水印学习（HiWL）两阶段优化方法：第一阶段使用分布对齐学习建立具有视觉一致性和信息不变性约束的公共潜在空间；第二阶段采用广义水印表示学习在RGB空间中建立分离水印与图像内容的解纠缠策略，并对相同消息对应的RGB水印大幅波动进行强惩罚。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明HiWL方法有效，水印提取准确率比现有方法提高7.6%，同时保持极低延迟（8秒处理10万张图像）。&lt;h4&gt;结论&lt;/h4&gt;HiWL能够有效学习可推广的潜在空间水印表示，同时保持广泛适用性，解决了现有水印技术的局限性。&lt;h4&gt;翻译&lt;/h4&gt;深度图像水印技术，即在载体图像中实现不可见水印嵌入和可靠提取，已被证明对图像资产版权保护有效。然而，现有方法在同时满足可推广水印的三个基本标准方面存在局限性：1）不可见性（水印的不可见隐藏），2）鲁棒性（在不同条件下可靠的水印恢复），以及3）广泛适用性（水印处理过程中的低延迟）。为解决这些限制，我们提出了一种分层水印学习（HiWL），这是一种两阶段优化，使水印模型能够同时实现这三个标准。在第一阶段，设计了分布对齐学习来建立具有两个约束的公共潜在空间：1）水印图像和非水印图像之间的视觉一致性，以及2）水印潜在表示之间的信息不变性。这样，包括水印消息（二进制码）和载体图像（RGB像素）在内的多模态输入可以得到良好表示，从而确保水印的不可见性和水印处理过程中的鲁棒性。第二阶段采用广义水印表示学习来建立分离RGB空间中水印与图像内容的解纠缠策略。特别是，它对相同消息对应的分离RGB水印的大幅波动进行强惩罚。因此，HiWL在保持广泛适用性的同时，有效学习了可推广的潜在空间水印表示。大量实验证明了所提出方法的有效性。特别是，它在水印提取方面比现有方法提高了7.6%的准确率，同时保持极低的延迟（8秒处理10万张图像）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep image watermarking, which refers to enable imperceptible watermarkembedding and reliable extraction in cover images, has shown to be effectivefor copyright protection of image assets. However, existing methods facelimitations in simultaneously satisfying three essential criteria forgeneralizable watermarking: 1) invisibility (imperceptible hide of watermarks),2) robustness (reliable watermark recovery under diverse conditions), and 3)broad applicability (low latency in watermarking process). To address theselimitations, we propose a Hierarchical Watermark Learning (HiWL), a two-stageoptimization that enable a watermarking model to simultaneously achieve threecriteria. In the first stage, distribution alignment learning is designed toestablish a common latent space with two constraints: 1) visual consistencybetween watermarked and non-watermarked images, and 2) information invarianceacross watermark latent representations. In this way, multi-modal inputsincluding watermark message (binary codes) and cover images (RGB pixels) can bewell represented, ensuring the invisibility of watermarks and robustness inwatermarking process thereby. The second stage employs generalized watermarkrepresentation learning to establish a disentanglement policy for separatingwatermarks from image content in RGB space. In particular, it stronglypenalizes substantial fluctuations in separated RGB watermarks corresponding toidentical messages. Consequently, HiWL effectively learns generalizablelatent-space watermark representations while maintaining broad applicability.Extensive experiments demonstrate the effectiveness of proposed method. Inparticular, it achieves 7.6\% higher accuracy in watermark extraction thanexisting methods, while maintaining extremely low latency (100K imagesprocessed in 8s).</description>
      <author>example@mail.com (Ke Liu, Xuanhan Wang, Qilong Zhang, Lianli Gao, Jingkuan Song)</author>
      <guid isPermaLink="false">2508.08667v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>$\text{M}^{2}$LLM: Multi-view Molecular Representation Learning with Large Language Models</title>
      <link>http://arxiv.org/abs/2508.08657v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了M²LLM多视图框架，通过整合分子结构、任务和规则三个视角，利用大型语言模型生成丰富的分子表示，实现了在分子属性预测任务上的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;准确的分子属性预测在化学、材料科学和药物发现领域至关重要。现有的分子表示方法如指纹和图神经网络虽能从分子结构中提取特征并取得先进结果，但往往忽视了数十年积累的语义和上下文知识。&lt;h4&gt;目的&lt;/h4&gt;解决现有分子表示方法忽视语义和上下文知识的问题，探索大型语言模型在生成丰富分子表示方面的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出M²LLM多视图框架，整合三个视角：分子结构视图、分子任务视图和分子规则视图，并通过动态融合这些视图来适应不同任务需求。&lt;h4&gt;主要发现&lt;/h4&gt;M²LLM在多个基准测试的分类和回归任务上取得了最先进的性能。通过利用大型语言模型的两个核心功能（编码能力生成分子嵌入和高级推理过程整理分子特征），实现了卓越的性能表现。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型能够通过多视角推理生成丰富的分子表示，M²LLM框架有效整合了分子结构、任务和规则视图，提高了分子属性预测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;准确的分子属性预测是一个关键挑战，在化学、材料科学和药物发现中有广泛应用。分子表示方法，包括指纹和图神经网络(GNNs)，通过有效从分子结构中提取特征，取得了最先进的结果。然而，这些方法往往忽视了数十年来积累的语义和上下文知识。大型语言模型(LLMs)的最新进展展示了在科学领域跨领域的显著推理能力和先验知识，这使我们假设当被引导从多个视角进行推理时，LLM可以生成丰富的分子表示。为解决这些差距，我们提出了M²LLM，一个多视图框架，整合了三个视角：分子结构视图、分子任务视图和分子规则视图。这些视图动态融合以适应任务需求，实验证明M²LLM在多个基准测试的分类和回归任务上取得了最先进的性能。此外，我们证明通过利用两个核心功能，从LLM衍生的表示实现了卓越的性能：通过编码能力生成分子嵌入，以及通过高级推理过程整理分子特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate molecular property prediction is a critical challenge withwide-ranging applications in chemistry, materials science, and drug discovery.Molecular representation methods, including fingerprints and graph neuralnetworks (GNNs), achieve state-of-the-art results by effectively derivingfeatures from molecular structures. However, these methods often overlookdecades of accumulated semantic and contextual knowledge. Recent advancementsin large language models (LLMs) demonstrate remarkable reasoning abilities andprior knowledge across scientific domains, leading us to hypothesize that LLMscan generate rich molecular representations when guided to reason in multipleperspectives. To address these gaps, we propose $\text{M}^{2}$LLM, a multi-viewframework that integrates three perspectives: the molecular structure view, themolecular task view, and the molecular rules view. These views are fuseddynamically to adapt to task requirements, and experiments demonstrate that$\text{M}^{2}$LLM achieves state-of-the-art performance on multiple benchmarksacross classification and regression tasks. Moreover, we demonstrate thatrepresentation derived from LLM achieves exceptional performance by leveragingtwo core functionalities: the generation of molecular embeddings through theirencoding capabilities and the curation of molecular features through advancedreasoning processes.</description>
      <author>example@mail.com (Jiaxin Ju, Yizhen Zheng, Huan Yee Koh, Can Wang, Shirui Pan)</author>
      <guid isPermaLink="false">2508.08657v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Think as Cardiac Sonographers: Marrying SAM with Left Ventricular Indicators Measurements According to Clinical Guidelines</title>
      <link>http://arxiv.org/abs/2508.08566v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AutoSAME框架，结合分割任意模型(SAM)的强大视觉理解能力，同时进行左心室分割和关键点定位，实现与临床指南一致的LV指标测量。&lt;h4&gt;背景&lt;/h4&gt;左心室指标测量对心血管疾病诊断很重要，但现有算法因训练数据集小难以捕捉通用视觉表示。视觉基础模型虽有丰富知识，但如SAM等模型适合分割但不擅长识别关键解剖点。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时进行分割和关键点定位的框架，模仿心脏超声操作者操作，实现与临床指南一致的LV指标测量。&lt;h4&gt;方法&lt;/h4&gt;提出AutoSAME框架，引入过滤交叉分支注意力(FCBA)利用分割特征增强关键点热图回归，以及空间引导提示对齐(SGPA)根据LV空间属性自动生成提示嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;在超声心动图数据集上的大量实验证明了各设计元素的有效性，AutoSAME在LV分割、关键点定位和指标测量方面表现出优越性能。&lt;h4&gt;结论&lt;/h4&gt;AutoSAME框架成功模仿了心脏超声操作者的工作方式，实现了与临床指南一致的LV指标测量，代码将在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;左心室(LV)指标测量遵循临床超声心动图指南对诊断心血管疾病至关重要。尽管现有算法已探索自动化LV量化，但由于通常训练数据集较小，它们难以捕捉通用视觉表示。因此，有必要引入具有丰富知识的视觉基础模型(VFM)。然而，以分割任意模型(SAM)为代表的VFM通常适合分割但无法识别关键解剖点，这对LV指标测量至关重要。在本文中，我们提出了一个名为AutoSAME的新框架，结合SAM的强大视觉理解能力，同时进行分割和关键点定位任务。因此，该框架模仿了心脏超声操作者的操作，实现了与临床指南一致的LV指标测量。我们在AutoSAME中进一步提出了过滤交叉分支注意力(FCBA)，利用分割中的相对全面特征从频域角度增强关键点的热图回归(HR)，优化了后者学习的视觉表示。此外，我们提出了空间引导提示对齐(SGPA)，根据LV的空间属性自动生成提示嵌入，从而通过先验空间知识提高密集预测的准确性。在超声心动图数据集上的大量实验证明了每个设计的效率和AutoSAME在LV分割、关键点定位和指标测量方面的优越性。代码将在https://github.com/QC-LIU-1997/AutoSAME上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Left ventricular (LV) indicator measurements following clinicalechocardiog-raphy guidelines are important for diagnosing cardiovasculardisease. Alt-hough existing algorithms have explored automated LVquantification, they can struggle to capture generic visual representations dueto the normally small training datasets. Therefore, it is necessary tointroduce vision founda-tional models (VFM) with abundant knowledge. However,VFMs represented by the segment anything model (SAM) are usually suitable forsegmentation but incapable of identifying key anatomical points, which arecritical in LV indicator measurements. In this paper, we propose a novelframework named AutoSAME, combining the powerful visual understanding of SAMwith seg-mentation and landmark localization tasks simultaneously.Consequently, the framework mimics the operation of cardiac sonographers,achieving LV indi-cator measurements consistent with clinical guidelines. Wefurther present fil-tered cross-branch attention (FCBA) in AutoSAME, whichleverages relatively comprehensive features in the segmentation to enhance theheatmap regression (HR) of key points from the frequency domain perspective,optimizing the vis-ual representation learned by the latter. Moreover, wepropose spatial-guided prompt alignment (SGPA) to automatically generate promptembeddings guid-ed by spatial properties of LV, thereby improving the accuracyof dense pre-dictions by prior spatial knowledge. The extensive experiments onan echocar-diography dataset demonstrate the efficiency of each design and thesuperiori-ty of our AutoSAME in LV segmentation, landmark localization, andindicator measurements. The code will be available athttps://github.com/QC-LIU-1997/AutoSAME.</description>
      <author>example@mail.com (Tuo Liu, Qinghan Yang, Yu Zhang, Rongjun Ge, Yang Chen, Guangquan Zhou)</author>
      <guid isPermaLink="false">2508.08566v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>CObL: Toward Zero-Shot Ordinal Layering without User Prompting</title>
      <link>http://arxiv.org/abs/2508.08498v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025: Project page with demo, datasets, and code:  https://vision.seas.harvard.edu/cobl/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为Concurrent Object Layers (CObL)的扩散架构，能够并行生成物体层堆栈来表示场景中的物体及其空间关系，包括横向和深度方向的遮挡关系。&lt;h4&gt;背景&lt;/h4&gt;视觉系统受益于将像素分组为物体并理解它们的空间关系，而现有的模态物体分割和无监督物体中心表示学习模型存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕获场景中物体遮挡关系的场景表示方法，并构建相应的架构来推断这种表示。&lt;h4&gt;方法&lt;/h4&gt;提出CObL架构，使用Stable Diffusion作为自然物体的先验，通过推理时指导确保推断的层能够复合回输入图像，并使用合成的多物体桌面场景图像进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;CObL能够零样本泛化到具有不同数量新物体的真实世界桌面照片；无需用户提示且无需预先知道物体数量即可重建多个被遮挡物体；不限于训练时的世界，能够处理未知场景。&lt;h4&gt;结论&lt;/h4&gt;CObL有效地解决了场景表示中的物体分组和空间关系理解问题，为视觉场景理解提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;视觉受益于将像素分组为物体并理解它们的空间关系，包括横向和深度方向。我们通过包含遮挡排序的'物体层'堆栈的场景表示来捕获这一点，每层包含一个被隔离和模态完成的物体。为了从图像推断这种表示，我们引入了一种名为Concurrent Object Layers (CObL)的基于扩散的架构。CObL并行生成物体层堆栈，使用Stable Diffusion作为自然物体的先验，并使用推理时指导确保推断的层能够复合回输入图像。我们使用数千个合成的多物体桌面场景图像训练CObL，发现它能够零样本泛化到具有不同数量新物体的真实世界桌面照片。与最近的模态物体分割模型相比，CObL可以在没有用户提示且不知道物体数量的情况下重建多个被遮挡物体。与之前无监督物体中心表示学习模型不同，CObL不限于其训练时的世界。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何在没有用户提示的情况下，将图像自动分解为按遮挡顺序排列的'对象层'堆叠的问题。每个对象层包含一个完整补全的对象（包括被遮挡部分），这些层按顺序组合后能重构原始图像。这个问题在计算机视觉中非常重要，因为它模拟了人类视觉系统对场景的理解能力，对于增强现实、机器人导航、图像编辑和场景理解等多种应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到缺乏带标注的训练数据是重大挑战，因此设计了一个创新的合成训练数据管道。他们借鉴了多项现有工作：使用Stable Diffusion作为基础模型；采用类似视频生成中并发生成多个帧的思路，但应用于对象层；使用交叉层注意力权重连接UNet；采用类似score distillation sampling的先验分数匹配损失；以及合成到真实的迁移策略。作者将这些技术组合创新，设计了能够并行生成多个对象层的架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用多个并行的Stable Diffusion UNet同时生成多个对象层，通过可学习的交叉层注意力连接这些UNet使各层之间通信，并使用推理时指导确保生成的层能组合回原始图像。整体流程包括：1) 创建合成训练数据，结合3D建模和文本到图像生成；2) 设计模型架构，使用N个冻结的Stable Diffusion UNet副本通过交叉层注意力连接；3) 训练模型，优化输入条件和交叉层注意力的参数；4) 推理过程，使用DDIM采样结合组合损失和先验分数匹配损失进行指导；5) 应用非微分更新操作如排列、擦除和排序。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 零样本分层排序，自动完成多个被遮挡对象的分解；2) 并行生成所有对象层而非顺序生成；3) 组合指导确保生成的层能重构原始图像；4) 创新的合成训练数据管道；5) 先验分数匹配损失确保生成内容在自然图像分布内。相比之前工作，CObL不需要用户提示或输入掩码，能同时处理多个对象，并且不局限于训练时见过的对象类别。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CObL通过创新的扩散模型架构和组合指导策略，实现了无需用户提示的零样本图像分层排序，能够自动将图像分解为按遮挡顺序排列的完整对象层堆叠。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision benefits from grouping pixels into objects and understanding theirspatial relationships, both laterally and in depth. We capture this with ascene representation comprising an occlusion-ordered stack of "object layers,"each containing an isolated and amodally-completed object. To infer thisrepresentation from an image, we introduce a diffusion-based architecture namedConcurrent Object Layers (CObL). CObL generates a stack of object layers inparallel, using Stable Diffusion as a prior for natural objects andinference-time guidance to ensure the inferred layers composite back to theinput image. We train CObL using a few thousand synthetically-generated imagesof multi-object tabletop scenes, and we find that it zero-shot generalizes tophotographs of real-world tabletops with varying numbers of novel objects. Incontrast to recent models for amodal object completion, CObL reconstructsmultiple occluded objects without user prompting and without knowing the numberof objects beforehand. Unlike previous models for unsupervised object-centricrepresentation learning, CObL is not limited to the world it was trained in.</description>
      <author>example@mail.com (Aneel Damaraju, Dean Hazineh, Todd Zickler)</author>
      <guid isPermaLink="false">2508.08498v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>SHeRL-FL: When Representation Learning Meets Split Learning in Hierarchical Federated Learning</title>
      <link>http://arxiv.org/abs/2508.08339v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SHeRL-FL是一种新的联邦学习方法，通过整合分割学习和分层模型聚合，结合中间层的表示学习，显著降低了协调复杂性和通信开销，在多个图像分类和分割任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;联邦学习(FL)是一种有前景的方法，通过在不共享原始数据的情况下进行协作模型训练，解决大规模网络的可扩展性和延迟问题。然而，现有FL框架往往忽略边缘客户端的计算异构性和资源有限设备的训练负担，且面临高通信成本和复杂模型聚合问题。之前结合分割学习(SL)和分层FL(HierFL)的方法虽减少了设备端计算，但引入了跨层协调的复杂性。&lt;h4&gt;目的&lt;/h4&gt;解决现有联邦学习框架中的协调复杂性和通信开销问题，特别是在处理大规模模型和资源有限设备时。&lt;h4&gt;方法&lt;/h4&gt;提出SHeRL-FL，整合分割学习和分层模型聚合，在中间层引入表示学习。允许客户端和边缘服务器独立于云计算训练目标，从而降低协调复杂性和通信开销。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10、CIFAR-100和HAM10000数据集上使用AlexNet、ResNet-18和ResNet-50的图像分类任务，以及在ISIC-2018数据集上使用ResNet-50-based U-Net的图像分割任务实验表明，SHeRL-FL比集中式FL和HierFL减少90%以上的数据传输，比SplitFed减少50%，并改进了分层分割学习方法。&lt;h4&gt;结论&lt;/h4&gt;SHeRL-FL有效解决了联邦学习中的通信成本和协调复杂性问题，显著减少了数据传输量，同时保持了良好的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习(FL)是一种有前景的方法，通过在不共享原始数据的情况下进行协作模型训练，解决大规模网络的可扩展性和延迟问题。然而，现有的FL框架往往忽略了边缘客户端的计算异构性以及对资源有限设备的日益增长训练负担。然而，FL面临高通信成本和复杂模型聚合的问题，特别是对于大型模型而言。之前的工作结合了分割学习(SL)和分层FL(HierFL)来减少设备端计算并提高可扩展性，但这引入了跨层协调的训练复杂性。为解决这些问题，我们提出了SHeRL-FL，它集成了SL和分层模型聚合并在中间层结合了表示学习。通过允许客户端和边缘服务器独立于云计算训练目标，SHeRL-FL显著降低了协调复杂性和通信开销。为评估SHeRL-FL的有效性和效率，我们在CIFAR-10、CIFAR-100和HAM10000上使用AlexNet、ResNet-18和ResNet-50进行了图像分类任务的实验，在IID和非IID设置下进行。此外，我们使用基于ResNet-50的U-Net在ISIC-2018数据集上评估了图像分割任务。实验结果表明，与集中式FL和HierFL相比，SHeRL-FL减少了90%以上的数据传输，与作为FL和SL混合体的SplitFed相比减少了50%，并进一步改进了分层分割学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated learning (FL) is a promising approach for addressing scalabilityand latency issues in large-scale networks by enabling collaborative modeltraining without requiring the sharing of raw data. However, existing FLframeworks often overlook the computational heterogeneity of edge clients andthe growing training burden on resource-limited devices. However, FL suffersfrom high communication costs and complex model aggregation, especially withlarge models. Previous works combine split learning (SL) and hierarchical FL(HierFL) to reduce device-side computation and improve scalability, but thisintroduces training complexity due to coordination across tiers. To addressthese issues, we propose SHeRL-FL, which integrates SL and hierarchical modelaggregation and incorporates representation learning at intermediate layers. Byallowing clients and edge servers to compute training objectives independentlyof the cloud, SHeRL-FL significantly reduces both coordination complexity andcommunication overhead. To evaluate the effectiveness and efficiency ofSHeRL-FL, we performed experiments on image classification tasks usingCIFAR-10, CIFAR-100, and HAM10000 with AlexNet, ResNet-18, and ResNet-50 inboth IID and non-IID settings. In addition, we evaluate performance on imagesegmentation tasks using the ISIC-2018 dataset with a ResNet-50-based U-Net.Experimental results demonstrate that SHeRL-FL reduces data transmission byover 90\% compared to centralized FL and HierFL, and by 50\% compared toSplitFed, which is a hybrid of FL and SL, and further improves hierarchicalsplit learning methods.</description>
      <author>example@mail.com (Dung T. Tran, Nguyen B. Ha, Van-Dinh Nguyen, Kok-Seng Wong)</author>
      <guid isPermaLink="false">2508.08339v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>ImageDDI: Image-enhanced Molecular Motif Sequence Representation for Drug-Drug Interaction Prediction</title>
      <link>http://arxiv.org/abs/2508.08338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted By Information Fusion&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ImageDDI的图像增强分子基序序列表示框架，用于药物-药物相互作用预测，该方法同时考虑药物的全局和局部结构，通过自适应特征融合整合分子视觉信息，实验证明其性能优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;多药使用可能产生不良健康影响，包括意外副作用和相互作用，因此准确识别和预测药物-药物相互作用在深度学习领域被视为关键任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的药物-药物相互作用预测方法，能够同时考虑药物的全局和局部结构，克服现有方法中基于功能基序的表示学习瓶颈。&lt;h4&gt;方法&lt;/h4&gt;ImageDDI框架将分子标记化为功能基序，将药物对的基序组合成单一序列并使用基于transformer的编码器进行嵌入。通过利用药物对之间的关联，使用全局分子图像信息增强分子的空间表示。采用自适应特征融合将分子视觉信息整合到功能基序序列中，动态调整特征表示的融合过程以增强泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在常用数据集上的实验结果表明，ImageDDI优于最先进的方法。在2D和3D图像增强场景中都取得了具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;ImageDDI是一种有效的药物-药物相互作用预测方法，能够同时考虑药物的全局和局部结构，并通过自适应特征融合整合分子视觉信息，提高了预测性能。&lt;h4&gt;翻译&lt;/h4&gt;为了减轻同时使用多种药物的潜在不良健康影响，包括意外副作用和相互作用，准确识别和预测药物-药物相互作用被视为深度学习领域的关键任务。尽管现有方法已显示出有希望的性能，但它们受限于基于功能基序的表示学习瓶颈，因为药物-药物相互作用本质上是由基序相互作用而非整体药物结构引起的。在本文中，我们提出了一种用于药物-药物相互作用预测的图像增强分子基序序列表示框架，称为ImageDDI，它从全局和局部结构表示药物对。具体来说，ImageDDI将分子标记化为功能基序。为了有效表示药物对，它们的基序被组合成单一序列，并使用基于transformer的编码器进行嵌入，从局部结构表示开始。通过利用药物对之间的关联，ImageDDI进一步使用全局分子图像信息(如纹理、阴影、颜色和平面空间关系)增强分子的空间表示。为了将分子视觉信息整合到功能基序序列中，ImageDDI采用自适应特征融合，通过动态调整特征表示的融合过程来增强ImageDDI的泛化能力。在常用数据集上的实验结果表明，ImageDDI优于最先进的方法。此外，大量实验表明，与其他模型相比，ImageDDI在2D和3D图像增强场景中都取得了具有竞争力的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To mitigate the potential adverse health effects of simultaneous multi-druguse, including unexpected side effects and interactions, accurately identifyingand predicting drug-drug interactions (DDIs) is considered a crucial task inthe field of deep learning. Although existing methods have demonstratedpromising performance, they suffer from the bottleneck of limited functionalmotif-based representation learning, as DDIs are fundamentally caused by motifinteractions rather than the overall drug structures. In this paper, we proposean Image-enhanced molecular motif sequence representation framework for\textbf{DDI} prediction, called ImageDDI, which represents a pair of drugs fromboth global and local structures. Specifically, ImageDDI tokenizes moleculesinto functional motifs. To effectively represent a drug pair, their motifs arecombined into a single sequence and embedded using a transformer-based encoder,starting from the local structure representation. By leveraging theassociations between drug pairs, ImageDDI further enhances the spatialrepresentation of molecules using global molecular image information (e.g.texture, shadow, color, and planar spatial relationships). To integratemolecular visual information into functional motif sequence, ImageDDI employsAdaptive Feature Fusion, enhancing the generalization of ImageDDI bydynamically adapting the fusion process of feature representations.Experimental results on widely used datasets demonstrate that ImageDDIoutperforms state-of-the-art methods. Moreover, extensive experiments show thatImageDDI achieved competitive performance in both 2D and 3D image-enhancedscenarios compared to other models.</description>
      <author>example@mail.com (Yuqin He, Tengfei Ma, Chaoyi Li, Pengsen Ma, Hongxin Xiang, Jianmin Wang, Yiping Liu, Bosheng Song, Xiangxiang Zeng)</author>
      <guid isPermaLink="false">2508.08338v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>HSA-Net: Hierarchical and Structure-Aware Framework for Efficient and Scalable Molecular Language Modeling</title>
      <link>http://arxiv.org/abs/2508.08334v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为HSA-Net的分层和结构感知网络框架，用于解决图神经网络在分子表示学习中的过平滑问题，通过结合交叉注意力和Mamba的优势，实现了更有效的特征投影和融合。&lt;h4&gt;背景&lt;/h4&gt;分子表示学习是分子字幕生成和分子属性预测等下游任务的基础，主要依赖图神经网络。然而，GNN存在过平滑问题，即节点特征在深层GNN层中会崩溃。现有的基于交叉注意力的特征投影方法在处理深层特征时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决GNN的过平滑问题，改进深层特征的处理能力，并解决Mamba和交叉注意力之间的全局-局部权衡问题。&lt;h4&gt;方法&lt;/h4&gt;提出HSA-Net框架，包含两个主要模块：1)分层自适应投影器(HAP)，动态选择交叉注意力投影器处理浅层特征和结构感知的图Mamba投影器处理深层特征；2)源感知融合(SAF)模块，根据特征特点灵活选择融合专家进行多级特征的有效合并。&lt;h4&gt;主要发现&lt;/h4&gt;Mamba能够保留来自深层的全局拓扑信息，但忽略了浅层的细粒度细节，而交叉注意力则相反，两者存在全局-局部权衡关系。HSA-Net能够有效处理这种权衡，生成高质量的多级特征。&lt;h4&gt;结论&lt;/h4&gt;大量实验证明，HSA-Net框架在定量和定性上都优于当前最先进的方法，为分子表示学习提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;分子表示学习是分子字幕生成和分子属性预测等下游任务的基础，严重依赖图神经网络。然而，GNN存在过平滑问题，即节点特征在深层GNN层中会崩溃。虽然现有的基于交叉注意力的特征投影方法已被引入以缓解此问题，但它们在处理深层特征时仍然表现不佳。这促使我们探索使用Mamba作为替代投影器，因为它能够处理复杂序列。然而，我们观察到虽然Mamba擅长保留来自深层的全局拓扑信息，但它忽略了浅层的细粒度细节。Mamba和交叉注意力的能力存在全局-局部权衡。为解决这一关键的全局-局部权衡，我们提出了分层和结构感知网络(HSA-Net)，这是一个包含两个模块的新框架，可实现分层特征投影和融合。首先，引入分层自适应投影器(HAP)模块来处理来自不同图层的特征，它学会动态切换浅层的交叉注意力投影器和深层的结构感知图Mamba投影器，产生高质量的多级特征。其次，为了自适应地合并这些多级特征，我们设计了源感知融合(SAF)模块，它根据聚合特征的特点灵活选择融合专家，确保精确有效的最终表示融合。大量实验证明，我们的HSA-Net框架在定量和定性上都优于当前最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular representation learning, a cornerstone for downstream tasks likemolecular captioning and molecular property prediction, heavily relies on GraphNeural Networks (GNN). However, GNN suffers from the over-smoothing problem,where node-level features collapse in deep GNN layers. While existing featureprojection methods with cross-attention have been introduced to mitigate thisissue, they still perform poorly in deep features. This motivated ourexploration of using Mamba as an alternative projector for its ability tohandle complex sequences. However, we observe that while Mamba excels atpreserving global topological information from deep layers, it neglectsfine-grained details in shallow layers. The capabilities of Mamba andcross-attention exhibit a global-local trade-off. To resolve this criticalglobal-local trade-off, we propose Hierarchical and Structure-Aware Network(HSA-Net), a novel framework with two modules that enables a hierarchicalfeature projection and fusion. Firstly, a Hierarchical Adaptive Projector (HAP)module is introduced to process features from different graph layers. It learnsto dynamically switch between a cross-attention projector for shallow layersand a structure-aware Graph-Mamba projector for deep layers, producinghigh-quality, multi-level features. Secondly, to adaptively merge thesemulti-level features, we design a Source-Aware Fusion (SAF) module, whichflexibly selects fusion experts based on the characteristics of the aggregationfeatures, ensuring a precise and effective final representation fusion.Extensive experiments demonstrate that our HSA-Net framework quantitatively andqualitatively outperforms current state-of-the-art (SOTA) methods.</description>
      <author>example@mail.com (Zihang Shao, Wentao Lei, Lei Wang, Wencai Ye, Li Liu)</author>
      <guid isPermaLink="false">2508.08334v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Efficient Semantic Segmentation: Learning Offsets for Better Spatial and Class Feature Alignment</title>
      <link>http://arxiv.org/abs/2508.08811v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025. Project page:  https://github.com/HVision-NKU/OffSeg&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种耦合的双分支偏移学习范式来解决语义分割在资源受限设备上的部署问题，通过学习特征和类别偏移来动态优化类别表示和图像特征，并在多个数据集上实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;语义分割对需要像素级场景理解的视觉系统至关重要，但在资源受限设备上部署需要高效架构。现有方法通过轻量级设计实现实时推理，但存在固有局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有语义分割方法中像素级分类范式导致的类别表示与图像特征不匹配问题，提高在资源受限设备上的分割性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种耦合的双分支偏移学习范式，明确学习特征和类别偏移来动态优化类别表示和空间图像特征，并基于此范式构建了高效语义分割网络OffSeg。&lt;h4&gt;主要发现&lt;/h4&gt;像素级分类范式导致了一个极具挑战性的假设：同一类别的图像像素特征在不同图像中不应变化。实验表明，所提出的偏移学习范式可以显著提升现有方法的性能。&lt;h4&gt;结论&lt;/h4&gt;偏移学习范式可以应用于现有方法而无需架构更改，在四个数据集上实现了一致的性能提升且仅需极少的额外参数。例如，在ADE20K数据集上将SegFormer-B0等模型的mIoU提高了约2%，仅需0.1-0.2M额外参数。&lt;h4&gt;翻译&lt;/h4&gt;语义分割对需要像素级场景理解的视觉系统至关重要，但在资源受限设备上部署需要高效架构。尽管现有方法通过轻量级设计实现实时推理，但我们揭示了它们的固有局限性：像素级分类范式导致的类别表示与图像特征不匹配。通过实验分析，我们发现这种范式导致高效场景下的一个极具挑战性的假设：同一类别的图像像素特征在不同图像中不应变化。为解决这一困境，我们提出了一种耦合的双分支偏移学习范式，明确学习特征和类别偏移以动态优化类别表示和空间图像特征。基于所提出的范式，我们构建了一个高效的语义分割网络OffSeg。值得注意的是，偏移学习范式可以应用于现有方法而无需额外的架构更改。在ADE20K、Cityscapes、COCO-Stuff-164K和Pascal Context四个数据集上的广泛实验展示了一致的改进且参数可忽略不计。例如，在ADE20K数据集上，我们提出的偏移学习范式分别将SegFormer-B0、SegNeXt-T和Mask2Former-Tiny的mIoU提高了2.7%、1.9%和2.6%，仅需0.1-0.2M额外参数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation is fundamental to vision systems requiring pixel-levelscene understanding, yet deploying it on resource-constrained devices demandsefficient architectures. Although existing methods achieve real-time inferencethrough lightweight designs, we reveal their inherent limitation: misalignmentbetween class representations and image features caused by a per-pixelclassification paradigm. With experimental analysis, we find that this paradigmresults in a highly challenging assumption for efficient scenarios: Image pixelfeatures should not vary for the same category in different images. To addressthis dilemma, we propose a coupled dual-branch offset learning paradigm thatexplicitly learns feature and class offsets to dynamically refine both classrepresentations and spatial image features. Based on the proposed paradigm, weconstruct an efficient semantic segmentation network, OffSeg. Notably, theoffset learning paradigm can be adopted to existing methods with no additionalarchitectural changes. Extensive experiments on four datasets, includingADE20K, Cityscapes, COCO-Stuff-164K, and Pascal Context, demonstrate consistentimprovements with negligible parameters. For instance, on the ADE20K dataset,our proposed offset learning paradigm improves SegFormer-B0, SegNeXt-T, andMask2Former-Tiny by 2.7%, 1.9%, and 2.6% mIoU, respectively, with only 0.1-0.2Madditional parameters required.</description>
      <author>example@mail.com (Shi-Chen Zhang, Yunheng Li, Yu-Huan Wu, Qibin Hou, Ming-Ming Cheng)</author>
      <guid isPermaLink="false">2508.08811v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>OpenCUA: Open Foundations for Computer-Use Agents</title>
      <link>http://arxiv.org/abs/2508.09123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OpenCUA是一个综合的开源框架，用于扩展计算机使用代理(CUA)数据和基础模型，包含注释基础设施、大规模数据集和可扩展的演示转换管道，其端到端代理模型在基准测试中表现出色，超过了现有的开源模型和OpenAI的CUA。&lt;h4&gt;背景&lt;/h4&gt;Vision-language models已展现出作为计算机使用代理的能力，能自动化各种计算机任务。随着这些系统商业潜力增长，最强大的CUA系统细节仍封闭。这些代理将越来越多地代表人们进行数字交互和重要决策，研究社区需要开放框架来研究其能力、局限性和风险。&lt;h4&gt;目的&lt;/h4&gt;弥合研究社区对开放CUA框架的需求，提出OpenCUA，一个用于扩展CUA数据和基础模型的综合开源框架。&lt;h4&gt;方法&lt;/h4&gt;包含三个主要部分：(1)注释基础设施，无缝捕获人类计算机使用演示；(2)AgentNet，首个大规模计算机使用任务数据集，涵盖3个操作系统和200多个应用程序和网站；(3)可扩展管道，将演示转换为具有反思性长链思维推理的状态-动作对，随数据扩展保持性能提升。&lt;h4&gt;主要发现&lt;/h4&gt;端到端代理模型在CUA基准测试中表现优异。OpenCUA-32B在OSWorld-Verified上平均成功率达34.8%，在开源模型中建立新最先进水平，超过OpenAI的CUA(GPT-4o)。方法跨领域泛化能力强，从增加测试时间计算中显著受益。&lt;h4&gt;结论&lt;/h4&gt;发布注释工具、数据集、代码和模型，为CUA研究建立开放基础，促进对这类系统的能力、局限性和风险的进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型已展现出作为计算机使用代理(CUAs)的能力，能够自动化各种计算机任务。随着其商业潜力的增长，最强大的CUA系统的关键细节仍然封闭。由于这些代理将越来越多地代表我们调解数字交互并执行重要决策，研究社区需要访问开放的CUA框架来研究其能力、局限性和风险。为弥合这一差距，我们提出OpenCUA，一个用于扩展CUA数据和基础模型的综合开源框架。我们的框架包括：(1)一个注释基础设施，能够无缝捕获人类计算机使用演示；(2)AgentNet，首个大规模计算机使用任务数据集，涵盖3个操作系统和200多个应用程序和网站；(3)一个可扩展的管道，将演示转换为具有反思性长链思维推理的状态-动作对，随着数据扩展保持稳健的性能提升。我们的端到端代理模型在CUA基准测试中表现出强大的性能。特别是，OpenCUA-32B在OSWorld-Verified上平均成功率达到34.8%，在开源模型中建立了新的最先进水平(SOTA)，超过了OpenAI的CUA(GPT-4o)。进一步分析证实，我们的方法在跨领域泛化能力良好，并且从增加的测试时间计算中显著受益。我们发布注释工具、数据集、代码和模型，为CUA研究建立开放基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models have demonstrated impressive capabilities ascomputer-use agents (CUAs) capable of automating diverse computer tasks. Astheir commercial potential grows, critical details of the most capable CUAsystems remain closed. As these agents will increasingly mediate digitalinteractions and execute consequential decisions on our behalf, the researchcommunity needs access to open CUA frameworks to study their capabilities,limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensiveopen-source framework for scaling CUA data and foundation models. Our frameworkconsists of: (1) an annotation infrastructure that seamlessly captures humancomputer-use demonstrations; (2) AgentNet, the first large-scale computer-usetask dataset spanning 3 operating systems and 200+ applications and websites;(3) a scalable pipeline that transforms demonstrations into state-action pairswith reflective long Chain-of-Thought reasoning that sustain robust performancegains as data scales. Our end-to-end agent models demonstrate strongperformance across CUA benchmarks. In particular, OpenCUA-32B achieves anaverage success rate of 34.8% on OSWorld-Verified, establishing a newstate-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA(GPT-4o). Further analysis confirms that our approach generalizes well acrossdomains and benefits significantly from increased test-time computation. Werelease our annotation tool, datasets, code, and models to build openfoundations for further CUA research.</description>
      <author>example@mail.com (Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, Tao Yu)</author>
      <guid isPermaLink="false">2508.09123v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation</title>
      <link>http://arxiv.org/abs/2508.08939v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ACM Multimedia Workshops&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探索了一种纯零样本的人脸变形攻击检测方法，通过CLIP模型设计和聚合多个文本提示来提高检测性能，无需额外训练或微调。&lt;h4&gt;背景&lt;/h4&gt;人脸变形攻击检测(MAD)是人脸识别安全的关键挑战，攻击者通过将多个人的身份信息插值到一张人脸图像中，生成可被验证为属于多个身份的样本。&lt;h4&gt;目的&lt;/h4&gt;探索一种纯零样本人脸变形攻击检测方法，利用CLIP基础模型的多模态知识，无需额外训练或微调。&lt;h4&gt;方法&lt;/h4&gt;利用CLIP模型不进行额外训练或微调，专注于设计和聚合每个类别的多个文本提示，通过聚合多样化提示的嵌入来对齐模型的内部表示与MAD任务。&lt;h4&gt;主要发现&lt;/h4&gt;提示聚合显著提高了零样本检测性能，能够捕获更丰富、更多样化的真实样本或攻击样本的线索。&lt;h4&gt;结论&lt;/h4&gt;通过高效的提示工程利用基础模型内置的多模态知识是一种有效的人脸变形攻击检测方法。&lt;h4&gt;翻译&lt;/h4&gt;人脸变形攻击检测(MAD)是人脸识别安全中的一个关键挑战，攻击者可以通过将两个或多个人的身份信息插值到一张人脸图像中，生成可以被人脸识别系统验证为属于多个身份的样本。虽然多模态基础模型(如CLIP)通过联合建模图像和文本提供了强大的零样本能力，但大多数先前的工作依赖于针对特定下游任务的微调，忽略了它们直接、可部署的潜力。这项工作探索了一种纯零样本的MAD方法，利用CLIP而不进行任何额外的训练或微调，而是专注于每个类别设计和聚合多个文本提示。通过聚合多样化提示的嵌入，我们可以更好地将模型的内部表示与MAD任务对齐，捕获更丰富、更多样化的真实样本或攻击样本的线索。我们的结果表明，提示聚合显著提高了零样本检测性能，证明了通过高效的提示工程利用基础模型内置的多模态知识是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3728425.3759909&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Face Morphing Attack Detection (MAD) is a critical challenge in facerecognition security, where attackers can fool systems by interpolating theidentity information of two or more individuals into a single face image,resulting in samples that can be verified as belonging to multiple identitiesby face recognition systems. While multimodal foundation models (FMs) like CLIPoffer strong zero-shot capabilities by jointly modeling images and text, mostprior works on FMs for biometric recognition have relied on fine-tuning forspecific downstream tasks, neglecting their potential for direct, generalizabledeployment. This work explores a pure zero-shot approach to MAD by leveragingCLIP without any additional training or fine-tuning, focusing instead on thedesign and aggregation of multiple textual prompts per class. By aggregatingthe embeddings of diverse prompts, we better align the model's internalrepresentations with the MAD task, capturing richer and more varied cuesindicative of bona-fide or attack samples. Our results show that promptaggregation substantially improves zero-shot detection performance,demonstrating the effectiveness of exploiting foundation models' built-inmultimodal knowledge through efficient prompt engineering.</description>
      <author>example@mail.com (Eduarda Caldeira, Fadi Boutros, Naser Damer)</author>
      <guid isPermaLink="false">2508.08939v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance</title>
      <link>http://arxiv.org/abs/2508.08774v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个记忆增强型AR代理的概念框架，通过四个相互连接的模块来支持个性化任务辅助，解决当前AR系统在处理复杂多步骤场景时的局限性。&lt;h4&gt;背景&lt;/h4&gt;增强现实系统越来越多地集成基础模型如多模态大语言模型，以提供更上下文感知和自适应的用户体验。当前AR代理能有效支持即时任务，但在需要理解和利用用户长期经验和偏好的复杂多步骤场景中存在困难，这源于它们无法在时空上下文中捕获、保留和推理历史用户交互。&lt;h4&gt;目的&lt;/h4&gt;提出一个记忆增强型AR代理的概念框架，使其能够通过随时间学习和适应用户特定经验来提供个性化的任务辅助。&lt;h4&gt;方法&lt;/h4&gt;框架包含四个相互连接的模块：(1)感知模块用于多模态传感器处理，(2)记忆模块用于持久化的时空经验存储，(3)时空推理模块用于综合过去和现在的上下文，(4)执行器模块用于有效的AR通信。同时提供了实施路线图、未来评估策略、潜在目标应用和使用案例。&lt;h4&gt;主要发现&lt;/h4&gt;当前AR系统在处理复杂多步骤场景时存在局限性，主要源于无法有效利用用户的历史交互数据。记忆增强型框架可以解决这些局限性，提供更智能的AR体验。&lt;h4&gt;结论&lt;/h4&gt;这项工作旨在激励未来研究，开发更智能的AR系统，这些系统能够有效地将用户的交互历史与自适应的、上下文感知的任务辅助联系起来。&lt;h4&gt;翻译&lt;/h4&gt;增强现实(AR)系统越来越多地集成基础模型，如多模态大语言模型(MLLMs)，以提供更上下文感知和自适应的用户体验。这种集成促使了AR代理的发展，以在真实环境中支持智能的、目标导向的交互。虽然当前的AR代理能有效支持即时任务，但在处理需要理解和利用用户长期经验和偏好的复杂多步骤场景时存在困难。这种局限性源于它们无法在时空上下文中捕获、保留和推理历史用户交互。为解决这些挑战，我们提出了一个记忆增强型AR代理的概念框架，该框架能够通过随时间学习和适应用户特定经验来提供个性化的任务辅助。我们的框架由四个相互连接的模块组成：(1)多模态传感器处理的感知模块，(2)持久化时空经验存储的记忆模块，(3)综合过去和现在上下文的时空推理模块，以及(4)有效AR通信的执行器模块。我们还提出了实施路线图、未来评估策略、潜在目标应用和使用案例，以展示我们框架在不同领域的实际适用性。我们希望这项工作能够激励未来研究，开发更智能的AR系统，能够有效地将用户的交互历史与自适应的、上下文感知的任务辅助联系起来。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是当前AR系统无法处理需要理解和利用用户长期经验和偏好的复杂多步骤任务。这个问题很重要，因为随着AR系统越来越多地集成基础模型，它们应该能够提供真正个性化的辅助，而不是仅支持即时任务。这种局限性阻碍了AR系统在需要个性化经验的场景中的应用，如重现用户的烹饪习惯或基于用户偏好组织物品。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了当前AR代理在处理复杂任务时的局限性，然后提出了一个记忆增强型AR代理的概念框架。他们借鉴了多模态场景图生成技术，使用场景图作为统一表示；参考了记忆增强型代理的研究，特别是在具身代理和Web代理中的应用；利用了基础模型在常识推理和多模态理解方面的能力；并整合了现有AR辅助系统的研究成果。作者通过模块化设计，将框架分为感知、记忆、时空推理和执行器四个相互连接的模块。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过存储和检索用户的长期经验，使AR系统能够提供个性化的任务辅助，并利用场景图作为统一表示来整合多模态感知信息。整体流程分为两个阶段：记录阶段，用户使用AR眼镜记录日常活动，这些记录被转换为结构化记忆并存储；回忆阶段，感知模块处理多模态输入生成场景图，记忆模块组织存储的记忆，时空推理模块进行任务意图推断、进度跟踪和行动规划，最后执行器模块通过适当方式提供个性化指导。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出记忆增强型AR代理的概念框架；设计四个相互连接的模块架构；采用场景图作为统一表示；强调时空推理能力；设计记录和回忆两阶段交互模式。相比之前的工作，该框架专注于长期用户经验而非短期交互；提供基于用户工作流程的个性化辅助而非通用指导；支持复杂多步骤任务；存储程序格式的经验而非简单语言匹配；能在嘈杂环境下通过上下文对齐解决不确定性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出并论证了一个记忆增强型AR代理的概念框架，通过整合用户的长期时空经验，实现了真正个性化的任务辅助，突破了当前AR系统在复杂多步骤场景中的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Augmented Reality (AR) systems are increasingly integrating foundationmodels, such as Multimodal Large Language Models (MLLMs), to provide morecontext-aware and adaptive user experiences. This integration has led to thedevelopment of AR agents to support intelligent, goal-directed interactions inreal-world environments. While current AR agents effectively support immediatetasks, they struggle with complex multi-step scenarios that requireunderstanding and leveraging user's long-term experiences and preferences. Thislimitation stems from their inability to capture, retain, and reason overhistorical user interactions in spatiotemporal contexts. To address thesechallenges, we propose a conceptual framework for memory-augmented AR agentsthat can provide personalized task assistance by learning from and adapting touser-specific experiences over time. Our framework consists of fourinterconnected modules: (1) Perception Module for multimodal sensor processing,(2) Memory Module for persistent spatiotemporal experience storage, (3)Spatiotemporal Reasoning Module for synthesizing past and present contexts, and(4) Actuator Module for effective AR communication. We further present animplementation roadmap, a future evaluation strategy, a potential targetapplication and use cases to demonstrate the practical applicability of ourframework across diverse domains. We aim for this work to motivate futureresearch toward developing more intelligent AR systems that can effectivelybridge user's interaction history with adaptive, context-aware task assistance.</description>
      <author>example@mail.com (Dongwook Choi, Taeyoon Kwon, Dongil Yang, Hyojun Kim, Jinyoung Yeo)</author>
      <guid isPermaLink="false">2508.08774v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Aryabhata: An exam-focused language model for JEE Math</title>
      <link>http://arxiv.org/abs/2508.08665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了Aryabhata 1.0，一个针对印度JEE考试优化的7B参数数学推理模型，通过合并开源模型、监督微调和强化学习等方法，在多项基准测试中表现出色，并作为基础模型开源发布。&lt;h4&gt;背景&lt;/h4&gt;尽管大型语言模型发展迅速，但现有模型通常不适合教育用途，特别是在应对特定学术考试方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;创建一个专门针对印度学术考试联合入学考试(JEE)优化的数学推理模型，提供教育友好的逐步推理，并作为基础模型开源以促进相关研究。&lt;h4&gt;方法&lt;/h4&gt;通过合并强大的开源权重推理模型，然后使用课程学习进行监督微调，基于验证链式思维轨迹进行训练；进一步应用带有可验证奖励的强化学习，使用A2C目标和组相对优势估计，结合自适应组调整和温度缩放等探索策略进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;Aryabhata 1.0在JEE Main 2025(分布内)和MATH、GSM8K(分布外)等基准测试中，在准确性和效率方面均优于现有模型，并能提供教育有用的逐步推理过程。&lt;h4&gt;结论&lt;/h4&gt;Aryabhata 1.0作为以考试为中心的开源小型语言模型基础模型发布，标志着向教育领域适用的大型语言模型迈出了重要一步，研究团队将持续改进模型以提升学生学习成果。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Aryabhata 1.0，这是一个紧凑的7B参数数学推理模型，专门为印度学术考试联合入学考试(JEE)优化。尽管大型语言模型进展迅速，但当前模型通常仍不适合教育用途。Aryabhata 1.0通过合并强大的开源权重推理模型构建，然后使用课程学习对通过最佳n拒绝采样筛选的验证链式思维轨迹进行监督微调(SFT)。为进一步提升性能，我们应用了带有可验证奖励的强化学习(RLVR)，使用A2C目标和组相对优势估计，以及自适应组调整和温度缩放等新的探索策略。在分布内(JEE Main 2025)和分布外(MATH, GSM8K)基准测试中评估，Aryabhata在准确性和效率上都优于现有模型，同时提供教育有用的逐步推理。我们将Aryabhata作为基础模型发布，以推进以考试为中心的开源小型语言模型。这是我们首次公开发布以获取社区反馈；PW正在积极训练未来的模型以进一步改善学生的学习成果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present $\textbf{Aryabhata 1.0}$, a compact 7B parameter math reasoningmodel optimized for the Indian academic exam, the Joint Entrance Examination(JEE). Despite rapid progress in large language models (LLMs), current modelsoften remain unsuitable for educational use. Aryabhata 1.0 is built by mergingstrong open-weight reasoning models, followed by supervised fine-tuning (SFT)with curriculum learning on verified chain-of-thought (CoT) traces curatedthrough best-of-$n$ rejection sampling. To further boost performance, we applyreinforcement learning with verifiable rewards (RLVR) using A2C objective withgroup-relative advantage estimation alongwith novel exploration strategies suchas $\textit{Adaptive Group Resizing}$ and $\textit{Temperature Scaling}$.Evaluated on both in-distribution (JEE Main 2025) and out-of-distribution(MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy andefficiency, while offering pedagogically useful step-by-step reasoning. Werelease Aryabhata as a foundation model to advance exam-centric, open-sourcesmall language models. This marks our first open release for community feedback($\href{https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0}{Aryabhata\ 1.0\on\ Hugging\ Face}$); PW is actively training future models to further improvelearning outcomes for students.</description>
      <author>example@mail.com (Ritvik Rastogi, Sachin Dharashivkar, Sandeep Varma)</author>
      <guid isPermaLink="false">2508.08665v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>DeepFleet: Multi-Agent Foundation Models for Mobile Robots</title>
      <link>http://arxiv.org/abs/2508.08574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 10 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeepFleet是一套支持大规模移动机器人舰队协调和规划的基础模型套件，包含四种不同架构，基于全球亚马逊仓库中数十万台机器人的数据训练。&lt;h4&gt;背景&lt;/h4&gt;全球亚马逊仓库中有数十万台机器人，需要有效的协调和规划系统。&lt;h4&gt;目的&lt;/h4&gt;设计支持大规模移动机器人舰队协调和规划的基础模型。&lt;h4&gt;方法&lt;/h4&gt;DeepFleet包含四种架构：机器人中心模型(自回归决策变换器)、机器人地面模型(交叉注意力变换器)、图像地面模型(卷积编码多通道图像)和图形地面模型(时间注意力与图神经网络结合)。&lt;h4&gt;主要发现&lt;/h4&gt;机器人和图形地面模型使用异步状态更新和局部交互结构，显示出最大潜力；当模型规模扩大时，能有效利用更大的仓库运营数据集。&lt;h4&gt;结论&lt;/h4&gt;不同设计选择对预测任务性能有不同影响，机器人和图形地面模型表现最佳。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了DeepFleet，一套旨在支持大规模移动机器人舰队协调和规划的基础模型套件。这些模型基于全球亚马逊仓库中数十万台机器人的舰队运动数据进行训练，包括机器人位置、目标和交互。DeepFleet包含四种架构，每种体现了不同的归纳偏置，共同探索多智能体基础模型设计空间的关键点：机器人中心模型是在单个机器人邻域上运行的自回归决策变换器；机器人地面模型使用机器人和仓库地面之间的交叉注意力的变换器；图像地面模型将卷积编码应用于整个舰队的多通道图像表示；图形地面模型将时间注意力与图神经网络结合，用于空间关系。在本文中，我们描述了这些模型并展示了我们对这些设计选择对预测任务性能影响的评估。我们发现，使用异步机器人状态更新并融入机器人交互局部结构的机器人和图形地面模型显示出最大的潜力。我们还展示了实验，表明当模型规模扩大时，这两种模型能够有效地利用更大的仓库运营数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce DeepFleet, a suite of foundation models designed to supportcoordination and planning for large-scale mobile robot fleets. These models aretrained on fleet movement data, including robot positions, goals, andinteractions, from hundreds of thousands of robots in Amazon warehousesworldwide. DeepFleet consists of four architectures that each embody a distinctinductive bias and collectively explore key points in the design space formulti-agent foundation models: the robot-centric (RC) model is anautoregressive decision transformer operating on neighborhoods of individualrobots; the robot-floor (RF) model uses a transformer with cross-attentionbetween robots and the warehouse floor; the image-floor (IF) model appliesconvolutional encoding to a multi-channel image representation of the fullfleet; and the graph-floor (GF) model combines temporal attention with graphneural networks for spatial relationships. In this paper, we describe thesemodels and present our evaluation of the impact of these design choices onprediction task performance. We find that the robot-centric and graph-floormodels, which both use asynchronous robot state updates and incorporate thelocalized structure of robot interactions, show the most promise. We alsopresent experiments that show that these two models can make effective use oflarger warehouses operation datasets as the models are scaled up.</description>
      <author>example@mail.com (Ameya Agaskar, Sriram Siva, William Pickering, Kyle O'Brien, Charles Kekeh, Ang Li, Brianna Gallo Sarker, Alicia Chua, Mayur Nemade, Charun Thattai, Jiaming Di, Isaac Iyengar, Ramya Dharoor, Dino Kirouani, Jimmy Erskine, Tamir Hegazy, Scott Niekum, Usman A. Khan, Federico Pecora, Joseph W. Durham)</author>
      <guid isPermaLink="false">2508.08574v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Chi-Geometry: A Library for Benchmarking Chirality Prediction of GNNs</title>
      <link>http://arxiv.org/abs/2508.09097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages total: 9 pages main text, 4 pages references, 8 pages  appendices. 4 figures and 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Chi-Geometry是一个用于生成图数据的库，专门用于测试和基准测试图神经网络(GNN)预测手性的能力。它可以生成具有特定几何和拓扑特征的合成图样本，每个图包含一个标记为R或S的手性中心，其他节点标记为N/A。这些样本组合成数据集，用于评估GNN作为节点分类任务预测手性的能力。&lt;h4&gt;背景&lt;/h4&gt;手性预测在化学和材料科学中是一个重要问题，图神经网络被用于预测分子结构中的手性中心。然而，缺乏专门用于测试和基准测试GNN手性预测能力的工具和数据集。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成具有特定几何和拓扑特征的合成图样本的库，用于更可解释和减少混淆因素的GNN手性预测基准测试，从而指导设计具有更好预测性能的新型GNN架构。&lt;h4&gt;方法&lt;/h4&gt;Chi-Geometry库生成合成图样本，具有用户指定的几何和拓扑特征以隔离特定类型的样本，以及随机化的节点位置和种类以最小化外部相关性。每个生成的图包含一个标记为R或S的手性中心，其他节点标记为N/A。&lt;h4&gt;主要发现&lt;/h4&gt;使用Chi-Geometry生成的数据集有效基准测试了各种最先进的GNN架构。基于结果设计了两种新型GNN架构：第一种建立全连接，能准确预测所有具有挑战性的配置中的手性，但计算成本随节点数量二次增长；第二种通过引入虚拟节点避免全连接，恢复了计算成本的线性缩放，同时保持相当的准确性。&lt;h4&gt;结论&lt;/h4&gt;Chi-Geometry库提供了更可解释和减少混淆因素的GNN手性预测基准测试，能够指导设计新型GNN架构。通过引入虚拟节点的优化方法，可以在保持准确性的同时显著降低计算成本。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了Chi-Geometry一个用于生成图数据以测试和基准测试GNN预测手性能力的库。Chi-Geometry生成具有以下特征的合成图样本：(i)用户指定的几何和拓扑特征，以隔离特定类型的样本，以及(ii)随机化的节点位置和种类，以最小化外部相关性。每个生成的图包含一个标记为R或S的手性中心，而所有其他节点标记为N/A（非手性）。然后将生成的样本组合成一个连贯的数据集，可用于评估GNN预测手性的能力，作为节点分类任务。Chi-Geometry使得对图样本中手性预测的GNN进行更可解释和减少混淆因素的基准测试，可以指导设计具有更好预测性能的新型GNN架构。我们通过使用Chi-Geometry生成合成数据集来基准测试各种最先进的GNN架构，说明了Chi-Geometry的有效性。这些基准测试的结果指导了我们设计两种新型GNN架构。第一种GNN架构在图中建立全连接，能够准确预测先前测试的最先进模型失败的所有具有挑战性的配置中的手性，但训练和推理的计算成本随图节点数量二次增长。第二种GNN架构通过在原始图结构中引入虚拟节点避免全连接，这恢复了训练和推理计算成本随图节点数量的线性缩放，同时仍确保与最先进的GNN架构相比在检测手性方面具有竞争性的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Chi-Geometry - a library that generates graph data for testingand benchmarking GNNs' ability to predict chirality. Chi-Geometry generatessynthetic graph samples with (i) user-specified geometric and topologicaltraits to isolate certain types of samples and (ii) randomized node positionsand species to minimize extraneous correlations. Each generated graph containsexactly one chiral center labeled either R or S, while all other nodes arelabeled N/A (non-chiral). The generated samples are then combined into acohesive dataset that can be used to assess a GNN's ability to predictchirality as a node classification task. Chi-Geometry allows more interpretableand less confounding benchmarking of GNNs for prediction of chirality in thegraph samples which can guide the design of new GNN architectures with improvedpredictive performance. We illustrate Chi-Geometry's efficacy by using it togenerate synthetic datasets for benchmarking various state-of-the-art (SOTA)GNN architectures. The conclusions of these benchmarking results guided ourdesign of two new GNN architectures. The first GNN architecture establishedall-to-all connections in the graph to accurately predict chirality across allchallenging configurations where previously tested SOTA models failed, but at acomputational cost (both for training and inference) that grows quadraticallywith the number of graph nodes. The second GNN architecture avoids all-to-allconnections by introducing a virtual node in the original graph structure ofthe data, which restores the linear scaling of training and inferencecomputational cost with respect to the number of nodes in the graph, whilestill ensuring competitive accuracy in detecting chirality with respect to SOTAGNN architectures.</description>
      <author>example@mail.com (Rylie Weaver, Massamiliano Lupo Pasini)</author>
      <guid isPermaLink="false">2508.09097v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Meta-learning optimizes predictions of missing links in real-world networks</title>
      <link>http://arxiv.org/abs/2508.09069v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, 5 tables, 7 appendices&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统性地比较了多种算法在预测关系网络中缺失链接方面的性能，发现没有一种算法在所有类型网络上都表现最佳，但模型堆叠方法具有良好的可扩展性和竞争力。作者还提出了一种元学习算法，可根据网络特性选择最优算法，显著提高了预测性能。&lt;h4&gt;背景&lt;/h4&gt;关系数据在现实世界应用中无处不在，如社交网络分析和生物建模，但观测到的网络数据往往不完整。预测缺失链接是网络分析中的关键挑战。&lt;h4&gt;目的&lt;/h4&gt;确定哪种算法最适合预测缺失链接，以及最佳算法选择是否依赖于输入网络的特性。&lt;h4&gt;方法&lt;/h4&gt;使用包含550个真实世界网络的大型多样化基准测试集，在AUC和Top-k两个准确度指标下，比较四种堆叠算法、42种拓扑链接预测器（包括两种新算法）和两种图神经网络算法。&lt;h4&gt;主要发现&lt;/h4&gt;没有一种算法在所有输入网络上都表现最佳；所有算法在大多数社交网络上表现良好，但只有少数算法在经济和生物网络上表现良好；使用随机森林的模型堆叠方法既高度可扩展，在AUC上超越图神经网络，或在Top-k准确度上与图神经网络具有竞争力；算法性能强烈依赖于网络特性如度分布、三角形密度和度同配性。&lt;h4&gt;结论&lt;/h4&gt;作者引入了一种元学习算法，通过利用网络特性间的变异性，为每个网络选择最佳算法来优化链接预测，该方法优于所有最先进算法并可扩展到大型网络。&lt;h4&gt;翻译&lt;/h4&gt;关系数据在现实世界数据应用中无处不在，例如在社交网络分析或生物建模中，但网络几乎总是不完整观测的。在没有节点属性的网络这种困难情况下，预测缺失链接的最先进技术使用模型堆叠或神经网络技术。目前尚不清楚哪种方法最佳，以及最佳算法选择是否或如何依赖于输入网络的特性。我们使用一个大型、结构多样化的550个真实世界网络基准测试集，在两个标准准确度指标（AUC和Top-k）下，系统地回答了这些问题，比较了四种堆叠算法与42种顶级拓扑链接预测器（其中两种是我们在此引入的），以及两种图神经网络算法。我们表明没有一种算法在所有输入网络上都是最好的，所有算法在大多数社交网络上表现良好，只有少数算法在经济和生物网络上表现良好。总体而言，使用随机森林的模型堆叠方法既高度可扩展，在AUC上超越图神经网络，或在Top-k准确度上与图神经网络具有竞争力。但是，算法性能强烈依赖于网络特性如度分布、三角形密度和度同配性。我们引入了一种元学习算法，利用这种变异性通过选择最佳算法来优化单个网络的链接预测，我们证明该方法优于所有最先进的算法并可扩展到大型网络。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relational data are ubiquitous in real-world data applications, e.g., insocial network analysis or biological modeling, but networks are nearly alwaysincompletely observed. The state-of-the-art for predicting missing links in thehard case of a network without node attributes uses model stacking or neuralnetwork techniques. It remains unknown which approach is best, and whether orhow the best choice of algorithm depends on the input network'scharacteristics. We answer these questions systematically using a large,structurally diverse benchmark of 550 real-world networks under two standardaccuracy measures (AUC and Top-k), comparing four stacking algorithms with 42topological link predictors, two of which we introduce here, and two graphneural network algorithms. We show that no algorithm is best across all inputnetworks, all algorithms perform well on most social networks, and few performwell on economic and biological networks. Overall, model stacking with a randomforest is both highly scalable and surpasses on AUC or is competitive withgraph neural networks on Top-k accuracy. But, algorithm performance dependsstrongly on network characteristics like the degree distribution, triangledensity, and degree assortativity. We introduce a meta-learning algorithm thatexploits this variability to optimize link predictions for individual networksby selecting the best algorithm to apply, which we show outperforms allstate-of-the-art algorithms and scales to large networks.</description>
      <author>example@mail.com (Bisman Singh, Lucy Van Kleunen, Aaron Clauset)</author>
      <guid isPermaLink="false">2508.09069v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Differentiated Information Mining: A Semi-supervised Learning Framework for GNNs</title>
      <link>http://arxiv.org/abs/2508.08769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 5 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DiFac的差异化因子一致性半监督框架，用于增强图神经网络在半监督学习中的性能，通过从单一信息源派生差异化因子并强制它们保持一致性，减轻伪标签确认偏差和训练崩溃问题。&lt;h4&gt;背景&lt;/h4&gt;在半监督学习中，使用互不独立的决策因素进行交叉验证被认为是减轻伪标签确认偏差和训练崩溃的有效策略，但在实践中获取这样的因素具有挑战性，因为额外的有效信息源本质上稀缺，且无法保证与原始信息源的独立性。&lt;h4&gt;目的&lt;/h4&gt;解决半监督学习中获取互不独立决策因素的挑战，提高图神经网络在低标签情况下的鲁棒性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出DiFac框架，在预训练阶段学习提取差异化因子，在训练阶段迭代移除具有冲突因子的样本，并根据最短杆原理对伪标签排序选择顶级候选样本。同时利用大型多模态语言模型引入潜在文本知识作为辅助决策因素，并设计问责评分机制减轻辅助因素引入的错误判断。&lt;h4&gt;主要发现&lt;/h4&gt;DiFac框架在多个基准数据集上实验表明，在低标签情况下持续提高了鲁棒性和泛化能力，优于其他基线方法。&lt;h4&gt;结论&lt;/h4&gt;DiFac框架通过从单一信息源派生差异化因子并强制它们保持一致性，有效解决了半监督学习中获取互不独立决策因素的挑战，提高了图神经网络在低标签情况下的性能。&lt;h4&gt;翻译&lt;/h4&gt;在利用未标记数据增强图神经网络性能的半监督学习中，为交叉验证引入相互独立的决策因素被认为是减轻伪标签确认偏差和训练崩溃的有效策略。然而，在实践中获得这样的因素具有挑战性：额外的有效信息源本质上稀缺，即使有这样的信息源，它们也无法保证与原始信息源的独立性。为应对这一挑战，本文提出了一种差异化因子一致性半监督框架（DiFac），该框架从单一信息源派生出差异化因子并强制它们保持一致性。在预训练阶段，模型学习提取这些因子；在训练阶段，它迭代地移除具有冲突因子的样本，并根据最短杆原理对伪标签进行排序，选择顶级候选样本以减少基于置信度或集成方法中常见的过度自信。我们的框架还可以整合额外的信息源。在这项工作中，我们利用大型多模态语言模型引入潜在文本知识作为辅助决策因素，并设计了一种问责评分机制来减轻这些辅助因素引入的额外错误判断。在多个基准数据集上的实验表明，DiFac在低标签情况下持续提高了鲁棒性和泛化能力，优于其他基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In semi-supervised learning (SSL) for enhancing the performance of graphneural networks (GNNs) with unlabeled data, introducing mutually independentdecision factors for cross-validation is regarded as an effective strategy toalleviate pseudo-label confirmation bias and training collapse. However,obtaining such factors is challenging in practice: additional and validinformation sources are inherently scarce, and even when such sources areavailable, their independence from the original source cannot be guaranteed. Toaddress this challenge, In this paper we propose a Differentiated FactorConsistency Semi-supervised Framework (DiFac), which derives differentiatedfactors from a single information source and enforces their consistency. Duringpre-training, the model learns to extract these factors; in training, ititeratively removes samples with conflicting factors and ranks pseudo-labelsbased on the shortest stave principle, selecting the top candidate samples toreduce overconfidence commonly observed in confidence-based or ensemble-basedmethods. Our framework can also incorporate additional information sources. Inthis work, we leverage the large multimodal language model to introduce latenttextual knowledge as auxiliary decision factors, and we design a accountabilityscoring mechanism to mitigate additional erroneous judgments introduced bythese auxiliary factors. Experiments on multiple benchmark datasets demonstratethat DiFac consistently improves robustness and generalization in low-labelregimes, outperforming other baseline methods.</description>
      <author>example@mail.com (Long Wang, Kai Liu)</author>
      <guid isPermaLink="false">2508.08769v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor Search</title>
      <link>http://arxiv.org/abs/2508.08744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at SIGMOD 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了Tagore，一个GPU加速的图索引库，用于高效处理高维向量空间中的近似最近邻搜索问题。该库能够构建基于细化的图索引，并通过多种技术创新实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;高维向量空间中的近似最近邻搜索(ANNS)在现实世界中有广泛应用。图索引方法因其高精度和效率而受到关注，但其索引开销仍然很大。随着数据量的指数级增长和动态索引调整需求的增加，这一问题变得更加严重。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的GPU加速图索引库，以解决图索引方法的开销问题，特别是在处理大规模数据集时。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了GNN-Descent算法，一种GPU特定的算法，用于高效的k-近邻(k-NN)图初始化；2. 提出了CFS通用计算过程，支持各种k-NN图剪枝策略；3. 设计了两个广义GPU内核，用于并行处理邻居关系中的复杂依赖；4. 提出了异步GPU-CPU-磁盘索引框架，具有集群感知的缓存机制，以最小化磁盘I/O压力。&lt;h4&gt;主要发现&lt;/h4&gt;在7个真实世界数据集上的实验表明，Tagore在保持索引质量的同时实现了1.32x-112.79x的加速比。&lt;h4&gt;结论&lt;/h4&gt;Tagore通过GPU加速和多种技术创新，有效解决了图索引方法的开销问题，特别是在处理大规模数据集时，为高维向量空间中的近似最近邻搜索提供了高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;高维向量空间中的近似最近邻搜索(ANNS)在现实世界中有广泛应用。虽然已有许多方法被提出以有效处理ANNS，但基于图的索引因其高精度和效率而日益突出。然而，基于图的索引的开销仍然很大。随着数据量的指数级增长和动态索引调整需求的增加，这一开销持续攀升，构成了一个关键挑战。在本文中，我们介绍了Tagore，一个由GPU加速的快速图索引库，它具有构建基于细化的图索引（如NSG和Vamana）的强大功能。我们首先介绍了GNN-Descent，一种用于高效k-近邻(k-NN)图初始化的GPU特定算法。GNN-Descent通过两阶段下降过程加速相似性比较，并实现高度并行的邻居更新。接下来，为了支持各种k-NN图剪枝策略，我们提出了一个称为CFS的通用计算过程，并设计了两个广义GPU内核，用于并行处理邻居关系中的复杂依赖。对于超过GPU内存容量的大规模数据集，我们提出了一个具有集群感知缓存机制的异步GPU-CPU-磁盘索引框架，以最小化磁盘的I/O压力。在7个真实世界数据集上的广泛实验表明，Tagore在保持索引质量的同时实现了1.32x-112.79x的加速比。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Approximate nearest neighbor search (ANNS) in high-dimensional vector spaceshas a wide range of real-world applications. Numerous methods have beenproposed to handle ANNS efficiently, while graph-based indexes have gainedprominence due to their high accuracy and efficiency. However, the indexingoverhead of graph-based indexes remains substantial. With exponential growth indata volume and increasing demands for dynamic index adjustments, this overheadcontinues to escalate, posing a critical challenge. In this paper, we introduceTagore, a fast library accelerated by GPUs for graph indexing, which haspowerful capabilities of constructing refinement-based graph indexes such asNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm forefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds upthe similarity comparison by a two-phase descent procedure and enables highlyparallelized neighbor updates. Next, aiming to support various k-NN graphpruning strategies, we formulate a universal computing procedure termed CFS anddevise two generalized GPU kernels for parallel processing complex dependenciesin neighbor relationships. For large-scale datasets exceeding GPU memorycapacity, we propose an asynchronous GPU-CPU-disk indexing framework with acluster-aware caching mechanism to minimize the I/O pressure on the disk.Extensive experiments on 7 real-world datasets exhibit that Tagore achieves1.32x-112.79x speedup while maintaining the index quality.</description>
      <author>example@mail.com (Zhonggen Li, Xiangyu Ke, Yifan Zhu, Bocheng Yu, Baihua Zheng, Yunjun Gao)</author>
      <guid isPermaLink="false">2508.08744v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Node-Destroyer Model with Large Neighborhood Search for Solving the Capacitated Vehicle Routing Problem</title>
      <link>http://arxiv.org/abs/2508.08659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种迭代学习混合优化求解器，用于增强元启发式算法在解决带容量车辆路径问题中的性能，通过结合机器学习模型和图神经网络来指导搜索过程，显著提高了算法效率和解决方案质量。&lt;h4&gt;背景&lt;/h4&gt;元启发式算法在解决带容量车辆路径问题时面临性能挑战，需要更有效的优化方法来处理大规模实例。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合优化求解器，通过迭代学习机制和机器学习方法增强元启发式算法在CVRP问题上的性能，同时保持算法的可扩展性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种迭代混合机制，集成了节点销毁模型(Node-Destroyer Model)，该模型利用图神经网络(GNNs)识别和选择客户节点，以指导大型邻域搜索(LNS)算子在元启发式框架中的操作，利用问题解决方案的图结构属性来指导节点移除的战略选择。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的混合机制能够改进基线元启发式算法的性能，不仅提高了标准CVRP基准的解决方案质量，而且在处理多达30,000个客户节点的超大实例上也证明了其可扩展性，无需针对不同大小的问题实例重新训练模型。&lt;h4&gt;结论&lt;/h4&gt;该混合优化求解器为解决带容量车辆路径问题提供了一种有效的方法，通过结合机器学习和元启发式算法的优势，实现了更高质量的解决方案和更好的可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;在这项研究中，我们提出了一种迭代学习混合优化求解器，旨在增强元启发式算法在解决带容量车辆路径问题(CVRP)中的性能。迭代混合机制集成了所提出的节点销毁模型(Node-Destroyer Model)，这是一个利用图神经网络(GNNs)的机器学习混合模型，用于识别和选择客户节点，以指导元启发式优化框架中的大型邻域搜索(LNS)算子。该模型利用了可以表示为图的问题和解决方案的结构属性，来指导关于节点移除的战略选择。所提出的方法降低了操作复杂性，并缩小了优化过程中涉及的搜索空间。该混合方法专门应用于CVRP，不需要针对不同大小的不同问题实例重新训练。所提出的混合机制能够改进基线元启发式算法的性能。我们的方法不仅提高了标准CVRP基准的解决方案质量，而且在处理多达30,000个客户节点的非常大规模的实例上也证明了其可扩展性。在基准数据集上的实验评估表明，所提出的混合机制能够改进不同的基线算法，在相似设置下获得更高质量的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this research, we propose an iterative learning hybrid optimization solverdeveloped to strengthen the performance of metaheuristic algorithms in solvingthe Capacitated Vehicle Routing Problem (CVRP). The iterative hybrid mechanismintegrates the proposed Node-Destroyer Model, a machine learning hybrid modelthat utilized Graph Neural Networks (GNNs) such identifies and selects customernodes to guide the Large Neighborhood Search (LNS) operator within themetaheuristic optimization frameworks. This model leverages the structuralproperties of the problem and solution that can be represented as a graph, toguide strategic selections concerning node removal. The proposed approachreduces operational complexity and scales down the search space involved in theoptimization process. The hybrid approach is applied specifically to the CVRPand does not require retraining across problem instances of different sizes.The proposed hybrid mechanism is able to improve the performance of baselinemetaheuristic algorithms. Our approach not only enhances the solution qualityfor standard CVRP benchmarks but also proves scalability on very large-scaleinstances with up to 30,000 customer nodes. Experimental evaluations onbenchmark datasets show that the proposed hybrid mechanism is capable ofimproving different baseline algorithms, achieving better quality of solutionsunder similar settings.</description>
      <author>example@mail.com (Bachtiar Herdianto, Romain Billot, Flavien Lucas, Marc Sevaux, Daniele Vigo)</author>
      <guid isPermaLink="false">2508.08659v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Agentic Graph Neural Networks for Wireless Communications and Networking Towards Edge General Intelligence: A Survey</title>
      <link>http://arxiv.org/abs/2508.08620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了图神经网络(GNNs)在无线通信和网络中的应用，并提出采用智能体人工智能(AI)来组织和整合GNNs，实现面向边缘通用智能的场景和任务感知实现。&lt;h4&gt;背景&lt;/h4&gt;通信技术的快速发展推动通信网络向高维资源利用和多功能集成方向发展，这种复杂性为设计满足移动应用服务质量(QoS)和时间敏感性需求的通信网络带来挑战。&lt;h4&gt;目的&lt;/h4&gt;全面了解GNNs的能力，通过智能体AI组织和整合GNNs，实现场景和任务感知的实现；回顾GNNs在无线通信和网络中的最新应用，特别关注图表示与网络拓扑之间以及神经网络架构与无线任务之间的对齐。&lt;h4&gt;方法&lt;/h4&gt;基于突出的神经网络架构概述GNNs，介绍智能体GNNs概念；总结和比较GNN在传统系统和新兴技术(物理层、MAC层、网络层设计、ISAC、RIS、无小区网络架构)中的应用；提出基于大型语言模型(LLM)的智能问答代理框架，利用调查作为本地知识库。&lt;h4&gt;主要发现&lt;/h4&gt;GNNs已成为复杂通信网络的基本深度学习模型，增强网络拓扑特征提取，提高可扩展性并促进分布式计算；但大多数现有GNNs遵循传统被动学习框架，可能无法满足多样化无线系统需求；智能体AI组织和整合GNNs可实现场景和任务感知实现。&lt;h4&gt;结论&lt;/h4&gt;通过全面回顾GNNs在无线通信和网络中的应用，提出基于大型语言模型的智能问答代理框架，利用调查作为知识库，实现针对无线通信研究定制的GNN相关响应。&lt;h4&gt;翻译&lt;/h4&gt;通信技术的快速发展推动通信网络向高维资源利用和多功能集成方向发展。这种日益增长的复杂性为设计通信网络以满足移动应用在动态环境中对服务质量(QoS)和时间敏感性的不断增长需求带来了重大挑战。图神经网络(GNNs)已成为复杂通信网络的基本深度学习(DL)模型。GNNs不仅增强了网络拓扑特征提取，还提高了可扩展性并促进分布式计算。然而，大多数现有的GNNs遵循传统的被动学习框架，可能无法满足日益多样化的无线系统的需求。该调查提出采用智能体人工智能(AI)来组织和整合GNNs，实现面向边缘通用智能的场景和任务感知实现。为了全面了解GNNs的能力，作者全面回顾了GNNs在无线通信和网络中的最新应用。特别是，作者关注图表示与网络拓扑之间以及神经网络架构与无线任务之间的对齐。首先，作者基于突出的神经网络架构概述了GNNs，然后介绍了智能体GNNs的概念。接着，总结了和比较了GNN在传统系统和新兴技术中的应用，包括物理层、MAC层和网络层设计、集成传感和通信(ISAC)、可重构智能表面(RIS)和无小区网络架构。此外，作者还提出了一个大型语言模型(LLM)框架作为智能问答代理，利用本调查作为本地知识库，实现针对无线通信研究定制的GNN相关响应。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement of communication technologies has driven the evolutionof communication networks towards both high-dimensional resource utilizationand multifunctional integration. This evolving complexity poses significantchallenges in designing communication networks to satisfy the growingquality-of-service and time sensitivity of mobile applications in dynamicenvironments. Graph neural networks (GNNs) have emerged as fundamental deeplearning (DL) models for complex communication networks. GNNs not only augmentthe extraction of features over network topologies but also enhance scalabilityand facilitate distributed computation. However, most existing GNNs follow atraditional passive learning framework, which may fail to meet the needs ofincreasingly diverse wireless systems. This survey proposes the employment ofagentic artificial intelligence (AI) to organize and integrate GNNs, enablingscenario- and task-aware implementation towards edge general intelligence. Tocomprehend the full capability of GNNs, we holistically review recentapplications of GNNs in wireless communications and networking. Specifically,we focus on the alignment between graph representations and network topologies,and between neural architectures and wireless tasks. We first provide anoverview of GNNs based on prominent neural architectures, followed by theconcept of agentic GNNs. Then, we summarize and compare GNN applications forconventional systems and emerging technologies, including physical, MAC, andnetwork layer designs, integrated sensing and communication (ISAC),reconfigurable intelligent surface (RIS) and cell-free network architecture. Wefurther propose a large language model (LLM) framework as an intelligentquestion-answering agent, leveraging this survey as a local knowledge base toenable GNN-related responses tailored to wireless communication research.</description>
      <author>example@mail.com (Yang Lu, Shengli Zhang, Chang Liu, Ruichen Zhang, Bo Ai, Dusit Niyato, Wei Ni, Xianbin Wang, Abbas Jamalipour)</author>
      <guid isPermaLink="false">2508.08620v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Quantum Mechanics to Organic Liquid Properties via a Universal Force Field</title>
      <link>http://arxiv.org/abs/2508.08575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ByteFF-Pol，一种基于图神经网络参数化的可极化力场，能够准确预测宏观性质，解决了分子动力学模拟中计算成本与准确性之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;分子动力学模拟是理解凝聚相系统结构和动力学的关键工具，但从量子力学计算准确预测宏观属性仍面临挑战，常受计算成本与模拟精度之间的权衡限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够基于高阶量子力学数据训练的力场，实现对宏观性质的准确预测，同时保持合理的计算成本。&lt;h4&gt;方法&lt;/h4&gt;提出ByteFF-Pol，一种基于图神经网络(GNN)参数化的可极化力场，完全基于高阶量子力学(QM)数据进行训练，采用物理动机的力场形式和训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;ByteFF-Pol在预测多种小分子液体和电解质的宏观热力学和输运性质方面表现出色，性能优于最先进的经典和机器学习力场。其零样本预测能力成功弥合了微观量子力学计算与宏观液体性质之间的差距。&lt;h4&gt;结论&lt;/h4&gt;ByteFF-Pol的进步在电解质设计和定制溶剂等应用领域具有变革性潜力，代表了数据驱动材料发现的关键一步，使探索以前难以处理的化学空间成为可能。&lt;h4&gt;翻译&lt;/h4&gt;分子动力学(MD)模拟是揭示凝聚相系统结构和动力学的原子级见解的必要工具。然而，从头计算(ab initio)普遍准确地预测宏观属性仍然是一个重大挑战，通常受到计算成本与模拟精度之间权衡的阻碍。在此，我们提出ByteFF-Pol，一种基于图神经网络(GNN)参数化的可极化力场，完全基于高阶量子力学(QM)数据进行训练。利用物理动机的力场形式和训练策略，ByteFF-Pol在预测多种小分子液体和电解质的热力学和输运性质方面表现出色，性能优于最先进的(SOTA)经典和机器学习力场。ByteFF-Pol的零样本预测能力弥合了微观QM计算与宏观液体性质之间的差距，使探索以前难以处理的化学空间成为可能。这一进展在电解质设计和定制溶剂等应用方面具有变革性潜力，代表了数据驱动材料发现的关键一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular dynamics (MD) simulations are essential tools for unravelingatomistic insights into the structure and dynamics of condensed-phase systems.However, the universal and accurate prediction of macroscopic properties fromab initio calculations remains a significant challenge, often hindered by thetrade-off between computational cost and simulation accuracy. Here, we presentByteFF-Pol, a graph neural network (GNN)-parameterized polarizable force field,trained exclusively on high-level quantum mechanics (QM) data. Leveragingphysically-motivated force field forms and training strategies, ByteFF-Polexhibits exceptional performance in predicting thermodynamic and transportproperties for a wide range of small-molecule liquids and electrolytes,outperforming state-of-the-art (SOTA) classical and machine learning forcefields. The zero-shot prediction capability of ByteFF-Pol bridges the gapbetween microscopic QM calculations and macroscopic liquid properties, enablingthe exploration of previously intractable chemical spaces. This advancementholds transformative potential for applications such as electrolyte design andcustom-tailored solvent, representing a pivotal step toward data-drivenmaterials discovery.</description>
      <author>example@mail.com (Tianze Zheng, Xingyuan Xu, Zhi Wang, Xu Han, Zhenliang Mu, Ziqing Zhang, Sheng Gong, Kuang Yu, Wen Yan)</author>
      <guid isPermaLink="false">2508.08575v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>UQGNN: Uncertainty Quantification of Graph Neural Networks for Multivariate Spatiotemporal Prediction</title>
      <link>http://arxiv.org/abs/2508.08551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures, SIGSPATIAL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为UQGNN的新型图神经网络，用于多变量时空预测，能够同时预测期望均值和相关的不确定性，并在多个真实数据集上表现出优于现有基线的性能。&lt;h4&gt;背景&lt;/h4&gt;时空预测在城市规划、交通优化、灾害响应和疫情控制等众多实际应用中扮演着关键角色。现有大多数模型是确定性的，仅预测期望均值而不量化不确定性，导致结果可能不可靠。虽然已有概率模型引入不确定性量化，但通常只关注单一现象，忽略了异构城市现象间的内在相关性。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有研究的不足，作者提出了UQGNN模型，用于多变量时空预测，旨在同时捕获复杂时空交互模式并量化预测的不确定性。&lt;h4&gt;方法&lt;/h4&gt;UQGNN引入两个关键创新：(i) 交互感知时空嵌入模块，整合多变量扩散图卷积网络和交互感知时间卷积网络，以捕获复杂时空交互模式；(ii) 多变量概率预测模块，用于估计期望均值和相关的不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;在深圳、纽约市和芝加哥四个真实世界多变量时空数据集上的实验表明，UQGNN在预测准确性和不确定性量化方面均优于最先进的基线。例如，在深圳数据集上，UQGNN在预测准确性和不确定性量化方面均实现了5%的提升。&lt;h4&gt;结论&lt;/h4&gt;UQGNN通过结合交互感知的时空嵌入和多变量概率预测，有效解决了多变量时空预测中的不确定性量化问题，并在多个真实数据集上证明了其优越性。&lt;h4&gt;翻译&lt;/h4&gt;时空预测在众多实际应用中扮演着关键角色，如城市规划、交通优化、灾害响应和疫情控制。近年来，研究人员通过开发先进的深度学习模型在时空预测方面取得了显著进展。然而，大多数现有模型是确定性的，即仅预测期望均值而不量化不确定性，可能导致不可靠和不准确的结果。虽然最近的研究已经引入概率模型来量化不确定性，但它们通常只关注单一现象（如出租车、自行车、犯罪或交通事故），从而忽略了异构城市现象之间的内在相关性。为了解决这一研究空白，我们提出了一种名为UQGNN的新型图神经网络，用于多变量时空预测的不确定性量化。UQGNN引入了两个关键创新：(i) 交互感知时空嵌入模块，它整合了多变量扩散图卷积网络和交互感知时间卷积网络，以有效捕获复杂的时空交互模式；(ii) 多变量概率预测模块，用于估计期望均值和相关的不确定性。在深圳、纽约市和芝加哥四个真实世界多变量时空数据集上的大量实验表明，UQGNN在预测准确性和不确定性量化方面都始终优于最先进的基线。例如，在深圳数据集上，UQGNN在预测准确性和不确定性量化方面都实现了5%的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatiotemporal prediction plays a critical role in numerous real-worldapplications such as urban planning, transportation optimization, disasterresponse, and pandemic control. In recent years, researchers have madesignificant progress by developing advanced deep learning models forspatiotemporal prediction. However, most existing models are deterministic,i.e., predicting only the expected mean values without quantifying uncertainty,leading to potentially unreliable and inaccurate outcomes. While recent studieshave introduced probabilistic models to quantify uncertainty, they typicallyfocus on a single phenomenon (e.g., taxi, bike, crime, or traffic crashes),thereby neglecting the inherent correlations among heterogeneous urbanphenomena. To address the research gap, we propose a novel Graph Neural Networkwith Uncertainty Quantification, termed UQGNN for multivariate spatiotemporalprediction. UQGNN introduces two key innovations: (i) an Interaction-awareSpatiotemporal Embedding Module that integrates a multivariate diffusion graphconvolutional network and an interaction-aware temporal convolutional networkto effectively capture complex spatial and temporal interaction patterns, and(ii) a multivariate probabilistic prediction module designed to estimate bothexpected mean values and associated uncertainties. Extensive experiments onfour real-world multivariate spatiotemporal datasets from Shenzhen, New YorkCity, and Chicago demonstrate that UQGNN consistently outperformsstate-of-the-art baselines in both prediction accuracy and uncertaintyquantification. For example, on the Shenzhen dataset, UQGNN achieves a 5%improvement in both prediction accuracy and uncertainty quantification.</description>
      <author>example@mail.com (Dahai Yu, Dingyi Zhuang, Lin Jiang, Rongchao Xu, Xinyue Ye, Yuheng Bu, Shenhao Wang, Guang Wang)</author>
      <guid isPermaLink="false">2508.08551v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>M3-Net: A Cost-Effective Graph-Free MLP-Based Model for Traffic Prediction</title>
      <link>http://arxiv.org/abs/2508.08543v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M3-Net的经济高效的无图多层感知器模型，用于交通预测任务，该模型在预测性能和轻量级部署方面表现出色。&lt;h4&gt;背景&lt;/h4&gt;准确交通预测是智能交通系统开发的基础任务，主流方法依赖时空图神经网络和注意力机制，但这些方法要么需要完整交通网络结构，要么需要复杂模型设计来捕捉时空依赖关系，限制了在大规模数据集上的高效部署。&lt;h4&gt;目的&lt;/h4&gt;解决现有深度学习方法在交通预测中的局限性，提出一种经济高效且无需图结构的模型。&lt;h4&gt;方法&lt;/h4&gt;提出M3-Net模型，使用时间序列和时空嵌入进行高效特征处理，并首次引入具有专家混合机制的新型MLP-Mixer架构。&lt;h4&gt;主要发现&lt;/h4&gt;在多个真实数据集上的广泛实验表明，M3-Net在预测性能和轻量级部署方面具有优越性。&lt;h4&gt;结论&lt;/h4&gt;M3-Net模型有效解决了现有深度学习方法在交通预测中的局限性，提供了更好的预测性能和部署效率。&lt;h4&gt;翻译&lt;/h4&gt;实现准确的交通预测是当前智能交通系统开发中的一项基础但关键的任务。在交通预测方面取得突破的大多数主流方法依赖于时空图神经网络、时空注意力机制等。现有深度学习方法的主要挑战在于它们要么依赖于完整的交通网络结构，要么需要复杂的模型设计来捕捉复杂的时空依赖关系。这些限制对深度学习模型在大规模数据集上的高效部署和运营提出了重大挑战。为应对这些挑战，我们提出了一种经济高效的无图多层感知器模型M3-Net用于交通预测。我们提出的模型不仅使用时间序列和时空嵌入进行高效特征处理，而且首次引入了一种具有专家混合机制的新型MLP-Mixer架构。在多个真实数据集上进行的广泛实验证明了所提出模型在预测性能和轻量级部署方面的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving accurate traffic prediction is a fundamental but crucial task inthe development of current intelligent transportation systems.Most of themainstream methods that have made breakthroughs in traffic prediction rely onspatio-temporal graph neural networks, spatio-temporal attention mechanisms,etc. The main challenges of the existing deep learning approaches are that theyeither depend on a complete traffic network structure or require intricatemodel designs to capture complex spatio-temporal dependencies. Theselimitations pose significant challenges for the efficient deployment andoperation of deep learning models on large-scale datasets. To address thesechallenges, we propose a cost-effective graph-free Multilayer Perceptron (MLP)based model M3-Net for traffic prediction. Our proposed model not only employstime series and spatio-temporal embeddings for efficient feature processing butalso first introduces a novel MLP-Mixer architecture with a mixture of experts(MoE) mechanism. Extensive experiments conducted on multiple real datasetsdemonstrate the superiority of the proposed model in terms of predictionperformance and lightweight deployment.</description>
      <author>example@mail.com (Guangyin Jin, Sicong Lai, Xiaoshuai Hao, Mingtao Zhang, Jinlei Zhang)</author>
      <guid isPermaLink="false">2508.08543v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Discrete Diffusion-Based Model-Level Explanation of Heterogeneous GNNs with Node Features</title>
      <link>http://arxiv.org/abs/2508.08458v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DiGNNExplainer的模型级别解释方法，用于解释异构图神经网络(HGNNs)的预测结果，通过离散去噪扩散技术生成具有真实节点特征的异构图，从而提供更真实和可信的解释。&lt;h4&gt;背景&lt;/h4&gt;许多现实世界的数据集（如引用网络、社交网络和分子结构）自然地表示为异构图，其中节点属于不同类型并具有附加特征。在这些图上的节点分类任务对假新闻检测、企业风险评估和分子性质预测等应用很有用。&lt;h4&gt;目的&lt;/h4&gt;解决现有异构图神经网络解释方法缺乏对实际节点特征支持、无法生成真实可信解释的问题，提供一种能够生成真实节点特征并忠实反映模型决策过程的解释方法。&lt;h4&gt;方法&lt;/h4&gt;提出DiGNNExplainer，一种模型级别的解释方法，通过离散去噪扩散技术合成具有真实节点特征的异构图。在离散空间中使用扩散模型生成真实的离散特征（如词袋特征），突破了之前方法仅限于连续空间的限制。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的评估表明，DiGNNExplainer生成的解释既真实又忠实于模型的决策过程，性能优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;DiGNNExplainer能够有效解决异构图神经网络解释中存在的问题，为理解模型决策提供了更可靠和真实的解释工具。&lt;h4&gt;翻译&lt;/h4&gt;许多现实世界的数据集，如引用网络、社交网络和分子结构，自然地表示为异构图，其中节点属于不同类型并具有附加特征。例如，在引用网络中，代表'论文'或'作者'的节点可能包含关键词或隶属关系等属性。在这些图上的一个关键机器学习任务是节点分类，这对假新闻检测、企业风险评估和分子性质预测等应用很有用。尽管异构图神经网络(HGNNs)在这些情境中表现良好，但它们的预测仍然不透明。现有的事后解释方法除了节点类型的一hot编码外，缺乏对实际节点特征的支持，并且往往无法生成真实、可信的解释。为了解决这些差距，我们提出了DiGNNExplainer，这是一种模型级别的解释方法，通过离散去噪扩散合成具有真实节点特征的异构图。具体来说，我们在离散空间中使用扩散模型生成真实的离散特征（例如，词袋特征），而之前的方法仅限于连续空间。我们在多个数据集上评估了我们的方法，并表明DiGNNExplainer生成的解释既真实又忠实于模型的决策过程，优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many real-world datasets, such as citation networks, social networks, andmolecular structures, are naturally represented as heterogeneous graphs, wherenodes belong to different types and have additional features. For example, in acitation network, nodes representing "Paper" or "Author" may include attributeslike keywords or affiliations. A critical machine learning task on these graphsis node classification, which is useful for applications such as fake newsdetection, corporate risk assessment, and molecular property prediction.Although Heterogeneous Graph Neural Networks (HGNNs) perform well in thesecontexts, their predictions remain opaque. Existing post-hoc explanationmethods lack support for actual node features beyond one-hot encoding of nodetype and often fail to generate realistic, faithful explanations. To addressthese gaps, we propose DiGNNExplainer, a model-level explanation approach thatsynthesizes heterogeneous graphs with realistic node features via discretedenoising diffusion. In particular, we generate realistic discrete features(e.g., bag-of-words features) using diffusion models within a discrete space,whereas previous approaches are limited to continuous spaces. We evaluate ourapproach on multiple datasets and show that DiGNNExplainer producesexplanations that are realistic and faithful to the model's decision-making,outperforming state-of-the-art methods.</description>
      <author>example@mail.com (Pallabee Das, Stefan Heindorf)</author>
      <guid isPermaLink="false">2508.08458v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>ScamDetect: Towards a Robust, Agnostic Framework to Uncover Threats in Smart Contracts</title>
      <link>http://arxiv.org/abs/2508.07094v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了ScamDetect框架，这是一个未来2.5年内发展的智能合约恶意软件检测系统，旨在解决现有检测方法面临的混淆技术和平台异质性问题。&lt;h4&gt;背景&lt;/h4&gt;智能合约通过可编程、无需信任的交易改变了去中心化金融，但其广泛采用也吸引了持续且复杂的威胁，如网络钓鱼活动和合约级别漏洞利用。传统基于交易的威胁检测方法会暴露敏感用户数据，引发隐私和安全问题。&lt;h4&gt;目的&lt;/h4&gt;开发ScamDetect，一个强大、模块化且与平台无关的智能合约恶意软件检测框架，为去中心化生态系统提供主动、可扩展的安全性。&lt;h4&gt;方法&lt;/h4&gt;静态字节码分析作为主动缓解策略，在恶意合约执行前识别威胁。PhishingHook作为首个基于机器学习的框架，通过静态字节码和操作码分析实现约90%的检测准确率。ScamDetect将分两个阶段发展：首先通过控制流图的图神经网络分析处理混淆的EVM字节码；其次将检测能力扩展到WASM等新兴运行时。&lt;h4&gt;主要发现&lt;/h4&gt;现有方法面临两个紧迫挑战：日益复杂的字节码混淆技术旨在逃避静态分析，以及区块链环境的异质性需要与平台无关的解决方案。&lt;h4&gt;结论&lt;/h4&gt;ScamDetect旨在解决当前智能合约安全检测的关键挑战，特别是混淆代码检测和多平台兼容性问题，为未来去中心化生态系统提供更强大的安全保障。&lt;h4&gt;翻译&lt;/h4&gt;智能合约通过可编程、无需信任的交易改变了去中心化金融。然而，它们的广泛采用和日益增长的经济重要性吸引了持续且复杂的威胁，如网络钓鱼活动和合约级别漏洞利用。传统的基于交易的威胁检测方法通常会暴露敏感的用户数据和交互，引发隐私和安全问题。作为回应，静态字节码分析已成为一种主动缓解策略，可在恶意合约执行有害操作之前识别它们。基于这种方法，我们引入了PhishingHook，这是第一个通过静态字节码和操作码分析检测智能合约中网络钓鱼活动的基于机器学习的框架，实现了约90%的检测准确率。然而，仍有两个紧迫的挑战：(1)日益复杂的字节码混淆技术的使用旨在逃避静态分析，以及(2)区块链环境的异质性需要与平台无关的解决方案。本文提出了ScamDetect（智能合约无关恶意软件检测器）的愿景，这是一个强大、模块化且与平台无关的智能合约恶意软件检测框架。在接下来的2.5年里，ScamDetect将分两个阶段发展：首先，通过控制流图的图神经网络分析处理混淆的以太坊虚拟机字节码，利用GNN捕获操作码序列之外的复杂结构模式的能力；其次，将检测能力扩展到WASM等新兴运行时。ScamDetect旨在为未来去中心化生态系统实现主动、可扩展的安全性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/DSN-S65789.2025.00068&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Smart contracts have transformed decentralized finance by enablingprogrammable, trustless transactions. However, their widespread adoption andgrowing financial significance have attracted persistent and sophisticatedthreats, such as phishing campaigns and contract-level exploits. Traditionaltransaction-based threat detection methods often expose sensitive user data andinteractions, raising privacy and security concerns. In response, staticbytecode analysis has emerged as a proactive mitigation strategy, identifyingmalicious contracts before they execute harmful actions. Building on thisapproach, we introduced PhishingHook, the first machine-learning-basedframework for detecting phishing activities in smart contracts via staticbytecode and opcode analysis, achieving approximately 90% detection accuracy.Nevertheless, two pressing challenges remain: (1) the increasing use ofsophisticated bytecode obfuscation techniques designed to evade staticanalysis, and (2) the heterogeneity of blockchain environments requiringplatform-agnostic solutions. This paper presents a vision for ScamDetect (SmartContract Agnostic Malware Detector), a robust, modular, and platform-agnosticframework for smart contract malware detection. Over the next 2.5 years,ScamDetect will evolve in two stages: first, by tackling obfuscated EthereumVirtual Machine (EVM) bytecode through graph neural network (GNN) analysis ofcontrol flow graphs (CFGs), leveraging GNNs' ability to capture complexstructural patterns beyond opcode sequences; and second, by generalizingdetection capabilities to emerging runtimes such as WASM. ScamDetect aims toenable proactive, scalable security for the future of decentralized ecosystems.</description>
      <author>example@mail.com (Pasquale De Rosa, Pascal Felber, Valerio Schiavoni)</author>
      <guid isPermaLink="false">2508.07094v2</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring</title>
      <link>http://arxiv.org/abs/2508.07552v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于特征图收敛分数的独立评估方法，通过双粒度动态加权评分系统和CLIP-based特征图质量评估网络，有效解决了自动驾驶端到端模型中中间功能模块缺乏明确监督信号的问题，提高了特征图质量和模型性能。&lt;h4&gt;背景&lt;/h4&gt;端到端模型正成为自动驾驶感知和规划的主流方法，但缺乏对中间功能模块的明确监督信号，导致操作机制不透明且可解释性有限，使传统方法难以独立评估和训练这些模块。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于特征图收敛分数的独立评估方法，构建双粒度动态加权评分系统，并开发CLIP-based特征图质量评估网络，以实现对功能模块生成特征图质量的全面评估。&lt;h4&gt;方法&lt;/h4&gt;基于特征图-真实表示相似性的评估框架，构建双粒度动态加权评分系统形成统一的特征图质量分数指标，开发结合特征-真实编码器和质量分数预测头的CLIP-FMQE-Net，实现对功能模块生成特征图的实时质量分析。&lt;h4&gt;主要发现&lt;/h4&gt;在NuScenes数据集上的实验表明，将评估模块整合到训练中可提高3D目标检测性能，NDS指标提升3.89%，验证了该方法在增强特征表示质量和整体模型性能方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能有效评估自动驾驶端到端模型中功能模块生成的特征图质量，通过整合评估模块可显著提高模型的整体性能。&lt;h4&gt;翻译&lt;/h4&gt;端到端模型正在成为自动驾驶感知和规划的主流。然而，由于缺乏对中间功能模块的明确监督信号，导致操作机制不透明且可解释性有限，使传统方法难以独立评估和训练这些模块。本研究开创性地解决了这一问题，基于特征图-真实表示相似性评估框架，提出了基于特征图收敛分数的独立评估方法。构建了双粒度动态加权评分系统，制定了统一的定量指标——特征图质量分数，以实现对功能模块生成的特征图质量的全面评估。进一步开发了基于CLIP的特征图质量评估网络，结合特征-真实编码器和质量分数预测头，能够对功能模块生成的特征图进行实时质量分析。在NuScenes数据集上的实验结果表明，将评估模块整合到训练中提高了3D目标检测性能，NDS指标提升了3.89%。这些结果验证了我们的方法在增强特征表示质量和整体模型性能方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶端到端模型中中间功能模块缺乏显式监督信号的问题，导致操作机制不透明且难以独立评估。这个问题在现实中很重要，因为自动驾驶系统需要高可靠性和可解释性，而传统评估方法无法有效优化功能模块，影响模型开发效率和系统安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到多模块学习(MML)将感知算法划分为专门功能模块的优势，但发现缺乏统一的评估标准。他们借鉴了特征图-真实表示相似性评估框架[19]、对比语言-图像预训练模型(CLIP)[13]、多模块学习[4]和BEVFormer[9]等现有工作，在此基础上设计了基于特征图质量评分(FMQS)的动态评估机制和双粒度动态加权评分系统(DG-DWSS)，并开发了CLIP-FMQE-Net实现闭环评估。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过特征图质量评分实现功能模块的独立评估，结合特征图语义一致性和任务性能反馈建立多维评估模型。整体流程包括：1)特征图质量评估系统，包含特征图编码器、真实值文本编码器和FMQS预测头；2)双粒度成熟度评分机制，包括宏观粒度指标(NDS归一化)和微观粒度指标(CS-CosSim相似性)；3)CLIP-FMQE-Net实现特征-真实值对齐和FMQS预测；4)将评估模块集成到训练过程中，将FMQS作为辅助损失实现自适应优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出特征图质量评分(FMQS)和双粒度动态加权评分系统；2)设计基于CLIP的特征图质量评估网络(CLI-FMQE-Net)；3)将评估模块集成到训练过程中。相比之前工作，不同之处在于：1)相比FMCE-Net只能评估单个功能模块，本文能独立评估多个级联模块；2)相比基于特征图-真实表示相似性的评估方法，本文将评估指标集成到训练中可直接指导优化；3)采用双粒度评估方法，同时从全局和局部角度评估模块成熟度；4)利用CLIP跨模态对齐能力提高评分准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于特征图质量评分的自动驾驶模型解耦功能评估方法，通过双粒度评分系统和CLIP-based评估网络实现了对功能模块训练成熟度的独立评估和优化，显著提高了3D目标检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end models are emerging as the mainstream in autonomous drivingperception and planning. However, the lack of explicit supervision signals forintermediate functional modules leads to opaque operational mechanisms andlimited interpretability, making it challenging for traditional methods toindependently evaluate and train these modules. Pioneering in the issue, thisstudy builds upon the feature map-truth representation similarity-basedevaluation framework and proposes an independent evaluation method based onFeature Map Convergence Score (FMCS). A Dual-Granularity Dynamic WeightedScoring System (DG-DWSS) is constructed, formulating a unified quantitativemetric - Feature Map Quality Score - to enable comprehensive evaluation of thequality of feature maps generated by functional modules. A CLIP-based FeatureMap Quality Evaluation Network (CLIP-FMQE-Net) is further developed, combiningfeature-truth encoders and quality score prediction heads to enable real-timequality analysis of feature maps generated by functional modules. Experimentalresults on the NuScenes dataset demonstrate that integrating our evaluationmodule into the training improves 3D object detection performance, achieving a3.89 percent gain in NDS. These results verify the effectiveness of our methodin enhancing feature representation quality and overall model performance.</description>
      <author>example@mail.com (Ludan Zhang, Sihan Wang, Yuqi Dai, Shuofei Qiao, Qinyue Luo, Lei He)</author>
      <guid isPermaLink="false">2508.07552v2</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision</title>
      <link>http://arxiv.org/abs/2508.09087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3rd Workshop in Data Engineering in Medical Imaging (DEMI),  MICCAI-2025 Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于重加权对比学习框架的属性不可知去偏方法，用于自动化青光眼筛查，通过自适应地关注困难示例来减少模型在不同人口统计子组间的偏见。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型(VLMs)在图像文本检索和零样本分类等多模态任务上取得了显著成功，但即使训练过程中没有明确的受保护属性，也可能表现出人口统计偏见。青光眼是不可逆失明的主要原因，且不成比例地影响服务不足的人群，因此自动化青光眼筛查具有重要的临床意义。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动化青光眼筛查方法，从视网膜眼底图像中检测青光眼，同时解决VLMs在医疗应用中可能存在的人口统计偏见问题，提高模型在不同人口统计子组间的公平性表现。&lt;h4&gt;方法&lt;/h4&gt;提出一种属性不可知的去偏方法，包含三个步骤：(i)通过无监督聚类图像-图像嵌入来推断代理子组；(ii)计算CLIP风格的多模态损失和SimCLR风格的图像对对比损失之间的梯度相似性权重；(iii)在联合的、top-k加权目标中应用这些权重，以提高表现较差的聚类权重。这种无标签方法自适应地针对最困难的示例，从而减少子组差异。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在哈佛FairVLMed青光眼子集上进行了评估，通过均衡赔率距离(EOD)、均衡子组AUC(ESAUC)和组内AUC等指标，证明了模型在推断的人口统计子组间具有公平的性能表现。&lt;h4&gt;结论&lt;/h4&gt;这种无标签的属性不可知方法能够有效减少模型在不同子组间的偏见，通过自适应地关注困难示例，提高了自动化青光眼筛查任务中模型在人口统计子组间的公平性。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型(VLMs)在图像文本检索和零样本分类等多模态任务上取得了显著成功，但即使在没有明确受保护属性的训练过程中，它们也可能表现出人口统计偏见。在这项工作中，我们专注于从视网膜眼底图像进行自动化青光眼筛查，这是一个关键应用，因为青光眼是不可逆失明的主要原因，且不成比例地影响服务不足的人群。基于一种基于重加权的对比学习框架，我们引入了一种属性不可知的去偏方法，该方法(i)通过无监督聚类图像-图像嵌入来推断代理子组，(ii)计算CLIP风格的多模态损失和SimCLR风格的图像对对比损失之间的梯度相似性权重，以及(iii)在联合的、top-k加权目标中应用这些权重，以提高表现较差的聚类权重。这种无标签方法自适应地针对最困难的示例，从而减少子组差异。我们在哈佛FairVLMed青光学子集上评估了我们的方法，通过报告均衡赔率距离(EOD)、均衡子组AUC(ESAUC)和组内AUC，证明了在推断的人口统计子组间的公平性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have achieved remarkable success on multimodaltasks such as image-text retrieval and zero-shot classification, yet they canexhibit demographic biases even when explicit protected attributes are absentduring training. In this work, we focus on automated glaucoma screening fromretinal fundus images, a critical application given that glaucoma is a leadingcause of irreversible blindness and disproportionately affects underservedpopulations. Building on a reweighting-based contrastive learning framework, weintroduce an attribute-agnostic debiasing method that (i) infers proxysubgroups via unsupervised clustering of image-image embeddings, (ii) computesgradient-similarity weights between the CLIP-style multimodal loss and aSimCLR-style image-pair contrastive loss, and (iii) applies these weights in ajoint, top-$k$ weighted objective to upweight underperforming clusters. Thislabel-free approach adaptively targets the hardest examples, thereby reducingsubgroup disparities. We evaluate our method on the Harvard FairVLMed glaucomasubset, reporting Equalized Odds Distance (EOD), Equalized Subgroup AUC (ESAUC), and Groupwise AUC to demonstrate equitable performance across inferreddemographic subgroups.</description>
      <author>example@mail.com (Ahsan Habib Akash, Greg Murray, Annahita Amireskandari, Joel Palko, Carol Laxson, Binod Bhattarai, Prashnna Gyawali)</author>
      <guid isPermaLink="false">2508.09087v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Masked Clustering Prediction for Unsupervised Point Cloud Pre-training</title>
      <link>http://arxiv.org/abs/2508.08910v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3D point cloud pretraining method. 8 pages in the main manuscript&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MaskClu是一种创新的ViTs预训练方法，通过结合掩码点建模和聚类学习，以及全局对比学习机制，有效提升了3D点云理解的性能。&lt;h4&gt;背景&lt;/h4&gt;Vision transformers (ViTs) 已被广泛应用于3D点云理解，掩码自编码是主要的预训练范式。然而，通过标准ViTs从点云学习密集且信息丰富的语义特征仍然是一个未被充分探索的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出MaskClu，一种用于3D点云上ViTs的新型无监督预训练方法，以解决从点云学习密集语义特征的挑战。&lt;h4&gt;方法&lt;/h4&gt;MaskClu结合了基于掩码的点建模和基于聚类的学习，设计为从掩码点云中重建聚类分配和聚类中心，鼓励模型捕获密集语义信息。同时引入全局对比学习机制，通过对同一点云的不同掩码视图进行对比，增强实例级别的特征学习。&lt;h4&gt;主要发现&lt;/h4&gt;通过联合优化密集语义重建和实例级对比学习这两个互补目标，MaskClu使ViTs能够从3D点云中学习更丰富和语义上有意义的表示。&lt;h4&gt;结论&lt;/h4&gt;通过部分分割、语义分割、目标检测和分类等多个3D任务验证了方法的有效性，MaskClu取得了新的竞争性结果。代码和模型将在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;视觉transformers (ViTs) 最近已被广泛应用于3D点云理解，掩码自编码作为主要的预训练范式。然而，通过标准ViTs从点云学习密集且信息丰富的语义特征的挑战仍然探索不足。我们提出了MaskClu，这是一种用于3D点云上ViTs的新型无监督预训练方法，它将基于掩码的点建模与基于聚类的学习相结合。MaskClu设计为从掩码点云中重建聚类分配和聚类中心，从而鼓励模型捕获密集的语义信息。此外，我们引入了一种全局对比学习机制，通过对同一点云的不同掩码视图进行对比，增强实例级别的特征学习。通过联合优化这些互补目标，即密集语义重建和实例级对比学习，MaskClu使ViTs能够从3D点云中学习更丰富和语义上有意义的表示。我们通过多个3D任务验证了我们方法的有效性，包括部分分割、语义分割、目标检测和分类，其中MaskClu取得了新的竞争性结果。代码和模型将在以下地址发布：https://github.com/Amazingren/maskclu。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从3D点云中学习密集且信息丰富的语义特征的问题。这一问题在现实中很重要，因为点云理解广泛应用于自动驾驶、机器人、虚拟现实等领域，而无监督预训练可以减少对大量标注数据的依赖，学习密集语义特征对于部分分割、语义分割、目标检测等下游任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有方法存在局限性：对比学习方法难以探索对象间相似性，聚类方法存在模糊组分配问题，而掩码自编码方法强调空间关系而非语义信息。因此，作者思考将掩码建模和聚类学习的优势结合，同时考虑局部和全局形状信息的重要性。他们借鉴了掩码自编码的掩码重建思想、聚类学习的伪标签指导、对比学习的视图对比机制以及图卷积网络处理点云空间关系的思路，设计了MaskClu方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将掩码自编码与基于聚类的表示学习相结合，通过重建聚类分配和聚类中心来学习密集语义特征，同时加入全局对比学习机制增强实例级特征学习。整体流程包括：1)点云分割为点块并嵌入；2)生成两个随机掩码视图；3)使用共享编码器处理视图；4)解码器重建完整特征；5)构建几何-语义图并进行MinCut聚类；6)重建聚类中心和分配；7)通过对比不同掩码视图进行全局对比学习；8)联合优化分配损失、聚类中心损失和对比损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的预训练框架结合掩码建模和聚类学习；2)聚类重建目标预测聚类分配和中心；3)几何-语义图构建同时利用几何和语义线索；4)实例级对比策略增强表示判别性；5)联合优化互补目标。相比之前工作，不同于Point-MAE的点级重建，它专注于更高层次语义结构；区别于PointClustering仅使用坐标，它利用几何和特征线索；解决了传统多任务学习中姿势感知与不变特征的冲突问题，能同时捕获局部和全局形状信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MaskClu通过创新性地结合掩码点建模与聚类预测，实现了从无标注3D点云中学习密集语义特征，显著提升了多种下游3D视觉任务的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision transformers (ViTs) have recently been widely applied to 3D pointcloud understanding, with masked autoencoding as the predominant pre-trainingparadigm. However, the challenge of learning dense and informative semanticfeatures from point clouds via standard ViTs remains underexplored. We proposeMaskClu, a novel unsupervised pre-training method for ViTs on 3D point cloudsthat integrates masked point modeling with clustering-based learning. MaskCluis designed to reconstruct both cluster assignments and cluster centers frommasked point clouds, thus encouraging the model to capture dense semanticinformation. Additionally, we introduce a global contrastive learning mechanismthat enhances instance-level feature learning by contrasting different maskedviews of the same point cloud. By jointly optimizing these complementaryobjectives, i.e., dense semantic reconstruction, and instance-level contrastivelearning. MaskClu enables ViTs to learn richer and more semantically meaningfulrepresentations from 3D point clouds. We validate the effectiveness of ourmethod via multiple 3D tasks, including part segmentation, semanticsegmentation, object detection, and classification, where MaskClu sets newcompetitive results. The code and models will be releasedat:https://github.com/Amazingren/maskclu.</description>
      <author>example@mail.com (Bin Ren, Xiaoshui Huang, Mengyuan Liu, Hong Liu, Fabio Poiesi, Nicu Sebe, Guofeng Mei)</author>
      <guid isPermaLink="false">2508.08910v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>GeoVLA: Empowering 3D Representations in Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2508.09071v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The project is visible at https://linsun449.github.io/GeoVLA/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了GeoVLA，一个新的Vision-Language-Action框架，通过整合3D信息来提升机器人的操作能力。该框架结合了视觉语言模型和3D几何信息处理，在模拟和真实环境实验中表现出优越的性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language-Action (VLA)模型已成为使机器人能够遵循语言指令并预测相应操作的有前景的方法。然而，当前的VLA模型主要依赖2D视觉输入，忽略了3D物理世界中丰富的几何信息，这限制了它们的空间感知能力和适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效整合3D信息以推进机器人操作的VLA框架，解决当前模型在空间感知和适应性方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;GeoVLA框架结合了视觉语言模型(VLM)处理图像和语言指令，提取融合的视觉语言嵌入；同时将深度图转换为点云，并使用定制的点编码器(点嵌入网络)独立生成3D几何嵌入。然后将这些嵌入连接起来，由提出的空间感知动作专家(3D增强动作专家)处理，该专家结合来自不同传感器模态的信息以生成精确的动作序列。&lt;h4&gt;主要发现&lt;/h4&gt;GeoVLA在模拟和真实环境实验中表现出优越的性能和鲁棒性。在LIBERO和ManiSkill2模拟基准测试中取得了最先进的结果，在需要高度适应性、尺度感知和视角不变性的真实世界任务中显示出显著的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过整合3D几何信息，GeoVLA框架显著提升了机器人的空间感知能力和适应性，为机器人操作提供了一个更强大、更鲁棒的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-动作(VLA)模型已成为一种有前景的方法，使机器人能够遵循语言指令并预测相应的动作。然而，当前的VLA模型主要依赖2D视觉输入，忽略了3D物理世界中丰富的几何信息，这限制了它们的空间感知能力和适应性。在本文中，我们提出了GeoVLA，一个新颖的VLA框架，能够有效整合3D信息以推进机器人操作。它使用视觉语言模型(VLM)处理图像和语言指令，提取融合的视觉语言嵌入。同时，它将深度图转换为点云，并采用一个定制的点编码器，称为点嵌入网络，独立生成3D几何嵌入。然后将这些生成的嵌入连接起来，由我们提出的空间感知动作专家处理，称为3D增强动作专家，它结合来自不同传感器模态的信息以产生精确的动作序列。通过在模拟和真实环境中的大量实验，GeoVLA展示了优越的性能和鲁棒性。它在LIBERO和ManiSkill2模拟基准测试中取得了最先进的结果，并在需要高度适应性、尺度感知和视角不变性的真实世界任务中显示出显著的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前视觉-语言-行动(VLA)模型过度依赖2D视觉输入而忽视3D几何信息的问题。这个问题很重要，因为3D几何信息能提供准确的深度线索、增强空间理解和视点变化鲁棒性，对机器人在现实世界中进行精确物理操作至关重要，特别是在需要处理不同高度、尺度和视点的任务时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有VLA模型在2D输入上的局限性，考察了3D感知在VLA中的应用尝试，发现要么破坏视觉编码器和LLM的对齐，要么无法适应新模态。作者设计了双路径架构分别处理2D和3D信息，借鉴了VLM、扩散模型和MoE架构的思想，但创新性地设计了PEN和3DAE模块来解决3D信息整合问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是双模态并行处理，同时利用2D视觉语言信息和3D几何信息。整体流程：1)视觉语言路径用VLM处理图像和语言提取特征；2)几何路径将深度图转为点云，用PEN提取3D特征；3)将两种特征连接后输入3DAE；4)3DAE通过MoE架构和扩散模型生成精确动作序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)双路径架构保留预训练知识同时增强空间感知；2)PEN双路径设计捕捉末端执行器周围的3D结构；3)3DAE的MoE架构和静态路由策略有效融合多模态信息。不同之处：不同于纯2D模型、直接修改视觉编码器的方法、冻结行动专家注入3D特征的方法以及动态路由MoE，GeoVLA是端到端训练的，能更好适应点云模态。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GeoVLA通过专门设计的点嵌入网络和3D增强行动专家，成功将3D几何信息整合到视觉-语言-行动模型中，显著提升了机器人在空间感知和适应性方面的性能，实现了模拟和现实世界任务中的最先进表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have emerged as a promising approach forenabling robots to follow language instructions and predict correspondingactions.However, current VLA models mainly rely on 2D visual inputs, neglectingthe rich geometric information in the 3D physical world, which limits theirspatial awareness and adaptability. In this paper, we present GeoVLA, a novelVLA framework that effectively integrates 3D information to advance roboticmanipulation. It uses a vision-language model (VLM) to process images andlanguage instructions,extracting fused vision-language embeddings. In parallel,it converts depth maps into point clouds and employs a customized pointencoder, called Point Embedding Network, to generate 3D geometric embeddingsindependently. These produced embeddings are then concatenated and processed byour proposed spatial-aware action expert, called 3D-enhanced Action Expert,which combines information from different sensor modalities to produce preciseaction sequences. Through extensive experiments in both simulation andreal-world environments, GeoVLA demonstrates superior performance androbustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2simulation benchmarks and shows remarkable robustness in real-world tasksrequiring height adaptability, scale awareness and viewpoint invariance.</description>
      <author>example@mail.com (Lin Sun, Bin Xie, Yingfei Liu, Hao Shi, Tiancai Wang, Jiale Cao)</author>
      <guid isPermaLink="false">2508.09071v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Long and Short Range Flows for Point Cloud Filtering</title>
      <link>http://arxiv.org/abs/2508.08542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HybridPF的点云去噪方法，结合短程和长程过滤轨迹，通过两个并行模块分别处理短程分数和长程流，并设计联合损失函数进行端到端训练。同时提出了动态图卷积解码器改进推理过程，实验表明该方法达到最先进结果且推理速度更快。&lt;h4&gt;背景&lt;/h4&gt;点云采集过程容易出错并引入噪声伪影，需要过滤/去噪。然而，现有的过滤方法通常存在点聚类或保留噪声的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的点云去噪方法HybridPF，结合短程和长程过滤轨迹，解决现有方法的点聚类和保留噪声问题。&lt;h4&gt;方法&lt;/h4&gt;1. 提出HybridPF方法，同时考虑短程和长程过滤轨迹；2. 设计两个并行模块：ShortModule和LongModule，每个模块包含编码器-解码器对，分别处理短程分数和长程流；3. 将噪声点视为高噪声变体块和干净块之间的中间状态；4. 设计联合损失函数，以端到端方式同时训练两个模块；5. 提出动态图卷积解码器改进推理过程。&lt;h4&gt;主要发现&lt;/h4&gt;1. 长程信息可以引导短程分数更紧密地与干净点对齐；2. 由长程特征引导的短程分数能产生具有良好点分布且收敛到干净表面的过滤点云；3. 分数模型通常能更快收敛到干净表面；4. 当前基于位移的方法存在解码器架构的局限性。&lt;h4&gt;结论&lt;/h4&gt;HybridPF方法通过结合短程和长程过滤轨迹，解决了现有方法的点聚类和保留噪声问题，达到了最先进的结果，同时实现了更快的推理速度。&lt;h4&gt;翻译&lt;/h4&gt;点云采集过程容易出错并引入噪声伪影，需要进行过滤/去噪。最近的过滤方法通常存在点聚类或保留噪声的问题。在本文中，我们提出了混合点云过滤方法，在去除噪声时同时考虑短程和长程过滤轨迹。众所周知，短程分数可以提供必要的位移，将噪声点移动到底层干净表面。相比之下，长程速度流近似恒定位移，从高噪声变体块指向相应的干净表面。在这里，噪声块被视为高噪声变体和干净块之间的中间状态。我们的直觉是，来自速度流模型的长程信息可以引导短程分数更紧密地与干净点对齐。反过来，分数模型通常能更快收敛到干净表面。具体来说，我们设计了两个并行模块：ShortModule和LongModule，每个模块都包含一个编码器-解码器对，分别处理短程分数和长程流。我们发现，由长程特征引导的短程分数可以产生具有良好点分布且收敛到干净表面的过滤点云。我们设计了一个联合损失函数，以端到端方式同时训练两个模块。最后，我们确定了当前基于位移方法的一个关键弱点，即解码器架构的局限性，并提出了一种动态图卷积解码器来改进推理过程。全面的实验表明，我们的HybridPF实现了最先进的结果，同时实现了更快的推理速度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云滤波/去噪问题。点云捕获过程容易引入噪声和伪影，而现有滤波方法通常存在点云聚类或噪声保留的问题。这个问题在现实中非常重要，因为点云是3D视觉、建模和图形任务中广泛使用的数据形式，滤波/去噪是点云预处理的基本任务，直接影响后续的网格重建、配准、3D建模和场景理解等任务的质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有方法的局限性：基于分数匹配的方法需要大量迭代且点会聚类，基于直线流动的方法可能过冲或欠冲干净表面，且解码器架构存在局限。作者的关键洞察是长程流动信息可以指导短程随机流动。方法设计上借鉴了分数匹配的短程流和直线流动的长程流思想，同时创新性地将两者结合，并使用动态图卷积改进了解码器架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是同时考虑短程和长程信息来去除点云噪声，其中长程流动信息指导短程随机流动。整体实现流程包括：1) 设计两个并行模块(ShortModule和LongModule)，每个模块采用编码器-解码器结构；2) LongModule训练推断长程常量流，只在训练期间使用；3) ShortModule推断短程分数，条件基于LongModule编码器的特征；4) 使用联合损失函数同时训练两个模块；5) 推理时应用迭代滤波过程，使用ShortModule处理噪声点。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 混合点云滤波方法(HybridPF)，同时考虑短程和长程信息；2) 联合训练方案，简化了模型训练；3) 动态图卷积解码器，考虑了高维特征空间中的拓扑信息。相比之前的工作，混合方法解决了分数匹配方法的点云聚类问题和直线流动方法的过冲/欠冲问题，同时动态图卷积解码器提供了比全连接层更好的滤波结果。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种混合长程和短程流动的点云滤波方法，结合了分数匹配和直线流动的优势，并通过动态图卷积解码器改进了位移估计，实现了更高质量的点云去噪效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud capture processes are error-prone and introduce noisy artifactsthat necessitate filtering/denoising. Recent filtering methods often sufferfrom point clustering or noise retaining issues. In this paper, we proposeHybrid Point Cloud Filtering ($\textbf{HybridPF}$) that considers bothshort-range and long-range filtering trajectories when removing noise. It iswell established that short range scores, given by $\nabla_{x}\log p(x_t)$, mayprovide the necessary displacements to move noisy points to the underlyingclean surface. By contrast, long range velocity flows approximate constantdisplacements directed from a high noise variant patch $x_0$ towards thecorresponding clean surface $x_1$. Here, noisy patches $x_t$ are viewed asintermediate states between the high noise variant and the clean patches. Ourintuition is that long range information from velocity flow models can guidethe short range scores to align more closely with the clean points. In turn,score models generally provide a quicker convergence to the clean surface.Specifically, we devise two parallel modules, the ShortModule and LongModule,each consisting of an Encoder-Decoder pair to respectively account forshort-range scores and long-range flows. We find that short-range scores,guided by long-range features, yield filtered point clouds with good pointdistributions and convergence near the clean surface. We design a joint lossfunction to simultaneously train the ShortModule and LongModule, in anend-to-end manner. Finally, we identify a key weakness in current displacementbased methods, limitations on the decoder architecture, and propose a dynamicgraph convolutional decoder to improve the inference process. Comprehensiveexperiments demonstrate that our HybridPF achieves state-of-the-art resultswhile enabling faster inference speed.</description>
      <author>example@mail.com (Dasith de Silva Edirimuni, Xuequan Lu, Ajmal Saeed Mian, Lei Wei, Gang Li, Scott Schaefer, Ying He)</author>
      <guid isPermaLink="false">2508.08542v1</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>3D Human Mesh Estimation from Single View RGBD</title>
      <link>http://arxiv.org/abs/2508.08178v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M$^3$（Masked Mesh Modeling）的方法，用于从单个RGBD视图进行准确的3D人体网格估计。该方法通过利用现有的动作捕捉数据集克服了数据稀缺问题，使用掩码自编码器完成部分网格，有效地恢复了3D人体网格模型中不可见部分，实现了比现有方法更准确的性能。&lt;h4&gt;背景&lt;/h4&gt;尽管从RGB图像和RGBD相机进行3D人体网格估计已取得显著进展，但RGBD相机提供的额外深度数据仍未被充分利用。现有的完全监督方法需要RGBD图像和3D网格标签对的数据集，但收集这样的数据集成本高且具有挑战性，导致现有数据集小，且姿势和形状多样性有限。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，用于从单个RGBD视图进行准确的3D人体网格估计，利用RGBD相机的经济性和广泛性进行实际应用。&lt;h4&gt;方法&lt;/h4&gt;利用现有的动作捕捉(MoCap)数据集，首先从MoCap数据集中的人体模型获取完整的3D网格，然后通过投影到虚拟相机创建它们的部分、单视图版本，模拟RGBD相机从单一视点提供的深度数据。训练一个掩码自编码器来完成部分、单视图网格。在推理过程中，将传感器传来的深度值与模板人体网格的顶点匹配，创建部分、单视图网格，从而恢复完整的3D人体网格。&lt;h4&gt;主要发现&lt;/h4&gt;在SURREAL和CAPE数据集上分别实现了16.8毫米和22.0毫米的每顶点误差(PVE)，优于使用全身点云作为输入的现有方法。在BEHAVE数据集上获得了具有竞争力的70.9 PVE，比最近发表的基于RGB的方法高出18.4毫米，突显了深度数据的有用性。&lt;h4&gt;结论&lt;/h4&gt;M$^3$方法有效地从单个RGBD视图恢复了完整的3D人体网格，证明了深度数据在3D人体网格估计中的价值。代码将被发布。&lt;h4&gt;翻译&lt;/h4&gt;尽管从RGB图像和RGBD相机进行3D人体网格估计已取得显著进展；提供额外深度数据的RGBD相机仍未被充分利用。在本文中，我们提出了一种从单个RGBD视图进行准确3D人体网格估计的方法，利用RGBD相机的经济性和广泛采用性进行实际应用。这个问题的完全监督方法需要包含RGBD图像和3D网格标签对的数据集。然而，收集这样的数据集成本高昂且具有挑战性，因此现有数据集小，且姿势和形状多样性有限。为了克服这种数据稀缺问题，我们利用现有的动作捕捉(MoCap)数据集。我们首先从MoCap数据集中的人体模型获取完整的3D网格，并通过投影到虚拟相机创建它们的部分、单视图版本。这模拟了从单一视点由RGBD相机提供的深度数据。然后，我们训练一个掩码自编码器来完成部分、单视图网格。在推理过程中，我们的方法（我们将其命名为M$^3$，代表'Masked Mesh Modeling'）将传感器传来的深度值与模板人体网格的顶点匹配，从而创建部分、单视图网格。我们有效地恢复了3D人体网格模型中不可见部分，得到完整的人体网格。M$^3$在SURREAL和CAPE数据集上分别实现了16.8毫米和22.0毫米的每顶点误差(PVE)，优于使用全身点云作为输入的现有方法。我们在BEHAVE数据集上获得了具有竞争力的70.9 PVE，比最近发表的基于RGB的方法高出18.4毫米，突显了深度数据的有用性。代码将被发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从单视角RGBD图像估计3D人体网格模型的问题。这个问题重要是因为RGBD相机越来越便宜普及，而准确估计3D人体网格在计算机图形学、医疗保健、体育和AR/VR等领域有广泛应用。相比仅使用RGB图像的方法，RGBD提供额外深度信息有助于更准确重建；相比需要完整3D扫描的方法，单视角RGBD更易于获取且成本更低。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先确定了两个主要挑战：从部分可见数据完成人体网格和缺乏成对的RGBD-3D网格训练数据。针对第一个挑战，借鉴了掩码图像建模(Masked Image Modeling)的思想，使用基于Transformer的掩码自编码器完成部分网格；针对第二个挑战，利用现有的动作捕捉(MoCap)数据集，通过虚拟相机投影模拟RGBD深度数据。整体设计结合了DensePose(用于UV映射提取)和ViT-MAE(掩码自编码器架构)等现有工作，但进行了创新整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用掩码自编码器来完成部分人体网格，利用MoCap数据解决训练数据稀缺问题，并通过UV映射建立RGB像素与3D网格顶点间的对应关系。整体流程：1)输入RGBD图像；2)用Densepose提取UV映射；3)结合深度数据和UV映射生成3D点云；4)将3D点与模板网格顶点匹配生成部分网格；5)用掩码自编码器完成部分网格；6)输出完整3D人体网格。训练采用两阶段：先在MoCap数据上训练，再在真实RGBD数据上微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次系统研究单视角RGBD人体网格估计；2)提出无需成对RGBD-3D网格数据的训练方法；3)设计基于Transformer的掩码自编码器(M3)；4)通过UV映射结合RGB和深度数据；5)在多个数据集上实现最先进性能。相比之前工作的不同：与仅用RGB的方法相比，利用额外深度信息提高准确性；与需要完整3D扫描的方法相比，只需单视角数据；与需要成对数据的方法相比，利用现有MoCap数据；与优化循环方法相比，效率更高；与使用序列数据的方法相比，只需单帧数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的基于掩码自编码器的单视角RGBD人体网格估计方法，通过利用动作捕捉数据集解决了训练数据稀缺问题，并在多个数据集上实现了最先进的性能，为现实世界应用提供了高效准确的3D人体重建解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant progress in 3D human mesh estimation from RGB images;RGBD cameras, offering additional depth data, remain underutilized. In thispaper, we present a method for accurate 3D human mesh estimation from a singleRGBD view, leveraging the affordability and widespread adoption of RGBD camerasfor real-world applications. A fully supervised approach for this problem,requires a dataset with RGBD image and 3D mesh label pairs. However, collectingsuch a dataset is costly and challenging, hence, existing datasets are small,and limited in pose and shape diversity. To overcome this data scarcity, weleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3Dmeshes from the body models found in MoCap datasets, and create partial,single-view versions of them by projection to a virtual camera. This simulatesthe depth data provided by an RGBD camera from a single viewpoint. Then, wetrain a masked autoencoder to complete the partial, single-view mesh. Duringinference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',matches the depth values coming from the sensor to vertices of a template humanmesh, which creates a partial, single-view mesh. We effectively recover partsof the 3D human body mesh model that are not visible, resulting in a full bodymesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREALand CAPE datasets, respectively; outperforming existing methods that usefull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVEdataset, outperforming a recently published RGB based method by 18.4 mm,highlighting the usefulness of depth data. Code will be released.</description>
      <author>example@mail.com (Ozhan Suat, Bedirhan Uguz, Batuhan Karagoz, Muhammed Can Keles, Emre Akbas)</author>
      <guid isPermaLink="false">2508.08178v2</guid>
      <pubDate>Wed, 13 Aug 2025 14:51:00 +0800</pubDate>
    </item>
    <item>
      <title>Open Scene Graphs for Open-World Object-Goal Navigation</title>
      <link>http://arxiv.org/abs/2508.04678v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In IJRR Special Issue: Foundation Models and Neuro-symbolic AI for  Robotics. Journal extension to arXiv:2407.02473&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OSG Navigator是一种用于开放世界目标物体导航的模块化系统，结合基础模型和开放场景图表示，实现了零样本适应新环境和泛化到多样化目标、环境和机器人形态的能力。&lt;h4&gt;背景&lt;/h4&gt;如何构建用于开放世界语义导航的通用机器人系统，例如在陌生环境中搜索自然语言指定的目标物体。基础模型虽然能提供丰富的语义知识，但在大规模组织和维护空间信息方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够零样本适应新环境类型、在目标物体导航任务上取得最先进性能并能够泛化到多样化目标、环境和机器人形态的机器人导航系统。&lt;h4&gt;方法&lt;/h4&gt;OSG Navigator是一个模块化系统，由基础模型组成。其核心是开放场景图表示，作为系统的空间记忆。系统使用OSG模式(模板)来分层组织空间信息，这些模式可以从简单语义标签自动生成，如'家'或'超市'。&lt;h4&gt;主要发现&lt;/h4&gt;在Fetch和Spot机器人的模拟和真实世界实验中，OSG Navigator在ObjectNav基准测试上实现了最先进性能，并能零样本泛化到多样化的目标、环境和机器人形态。&lt;h4&gt;结论&lt;/h4&gt;OSG Navigator通过结合基础模型和开放场景图表示，有效解决了开放世界语义导航中的挑战，为构建通用机器人导航系统提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;我们如何构建用于开放世界语义导航的通用机器人系统，例如在陌生环境中搜索自然语言指定的目标物体？为应对这一挑战，我们引入了OSG Navigator，这是一个由基础模型组成的模块化系统，用于开放世界目标物体导航(ObjectNav)。基础模型提供了关于世界的丰富语义知识，但在大规模组织和维护空间信息方面存在困难。OSG Navigator的关键是开放场景图表示，它作为OSG Navigator的空间记忆。它使用OSG模式(模板)分层组织空间信息，每个模式描述一类环境的常见结构。OSG模式可以从给定环境的简单语义标签自动生成，例如'家'或'超市'。它们使OSG Navigator能够零样本适应新环境类型。我们在模拟和真实世界中使用Fetch和Spot机器人进行了实验，表明OSG Navigator在ObjectNav基准测试上实现了最先进性能，并能零样本泛化到多样化的目标、环境和机器人形态。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何构建通用机器人系统用于开放世界的语义导航问题，特别是在未知环境中根据自然语言描述寻找特定目标物体。这个问题在现实中非常重要，因为它能让机器人在陌生环境中执行实用任务，如在药店找药、超市购物或家中寻找物品，而不需要预先了解环境地图或接受特定训练。研究上，它解决了传统导航系统难以泛化到新环境、新目标和不同机器人平台的局限性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到基础模型（如LLMs和VFMs）拥有丰富的语义知识但难以有效组织空间信息，因此提出开放场景图（OSG）作为结构化空间记忆。他们借鉴了现有工作中的基础模型（GPT-3.5、GroundingDINO、ViNT等）、场景图表示、语义导航任务和概率拓扑映射框架，但创新性地将这些元素组合成一个能够处理开放世界导航挑战的系统。设计思路是分层组织空间信息，通过OSG schemas实现环境适应，并将高级语义推理与低级导航控制分离。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用开放场景图（OSG）作为结构化的环境表示，结合基础模型的语义知识实现零样本泛化。整体流程分为三部分：1）映射阶段：从图像中提取语义信息，估计机器人状态，并构建/更新OSG；2）推理与控制阶段：基于OSG提出搜索子目标，规划路径，并执行导航命令；3）可选的OSG Schema生成：使用LLMs从简单环境标签自动生成环境结构模板。系统通过分层抽象（Objects、Places、Connectors、Region Abstractions）组织空间信息，实现从粗粒度到细粒度的环境理解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）开放场景图（OSG）表示，支持多种抽象层次和开放世界环境；2）零样本泛化能力，能适应新环境类型、开放词汇目标和不同机器人平台；3）模块化基础模型架构，完全由基础模型组成；4）自动OSG Schema生成，只需简单环境标签。相比之前工作，传统系统需要特定环境的手动工程和训练，而OSG Navigator能处理三个维度的泛化（目标、环境、机器人），且使用纯语义信息（不依赖精确几何）实现强大性能，同时支持新环境类型的零样本适应。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OSG Navigator通过结合基础模型和开放场景图表示，实现了在开放世界中零样本泛化的物体目标导航，能够适应多样化的目标、环境和机器人平台，无需特定环境的手动工程或预先训练。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1177/02783649251369549&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; How can we build general-purpose robot systems for open-world semanticnavigation, e.g., searching a novel environment for a target object specifiedin natural language? To tackle this challenge, we introduce OSG Navigator, amodular system composed of foundation models, for open-world Object-GoalNavigation (ObjectNav). Foundation models provide enormous semantic knowledgeabout the world, but struggle to organise and maintain spatial informationeffectively at scale. Key to OSG Navigator is the Open Scene Graphrepresentation, which acts as spatial memory for OSG Navigator. It organisesspatial information hierarchically using OSG schemas, which are templates, eachdescribing the common structure of a class of environments. OSG schemas can beautomatically generated from simple semantic labels of a given environment,e.g., "home" or "supermarket". They enable OSG Navigator to adapt zero-shot tonew environment types. We conducted experiments using both Fetch and Spotrobots in simulation and in the real world, showing that OSG Navigator achievesstate-of-the-art performance on ObjectNav benchmarks and generalises zero-shotover diverse goals, environments, and robot embodiments.</description>
      <author>example@mail.com (Joel Loo, Zhanxin Wu, David Hsu)</author>
      <guid isPermaLink="false">2508.04678v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
  <item>
      <title>3D Human Mesh Estimation from Single View RGBD</title>
      <link>http://arxiv.org/abs/2508.08178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种从单RGBD视图进行准确3D人体网格估计的方法，称为M$^3$（Masked Mesh Modeling）。该方法利用动作捕捉数据集克服了RGBD-3D网格配对数据集稀缺的问题，通过掩码自编码器完成部分人体网格，有效恢复不可见的身体部分。实验证明该方法在多个数据集上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;尽管从RGB图像进行3D人体网格估计已取得显著进展，但提供额外深度数据的RGBD相机仍未得到充分利用。现有的完全监督方法需要包含RGBD图像和3D网格标签对的数据集，但收集这样的数据集成本高且具有挑战性，导致现有数据集小且多样性有限。&lt;h4&gt;目的&lt;/h4&gt;提出一种从单RGBD视图进行准确3D人体网格估计的方法，利用RGBD相机的经济性和广泛应用性进行实际应用。&lt;h4&gt;方法&lt;/h4&gt;利用现有的动作捕捉(MoCap)数据集获取完整3D网格，通过投影到虚拟相机创建部分单视图版本模拟RGBD深度数据。训练掩码自编码器完成部分单视图网格。在推理过程中，M$^3$方法将传感器深度值与模板人体网格顶点匹配创建部分网格，有效恢复不可见的身体部分获得完整网格。&lt;h4&gt;主要发现&lt;/h4&gt;M$^3$在SURREAL和CAPE数据集上分别实现了16.8毫米和22.0毫米的每顶点误差(PVE)，优于使用全身点云作为输入的现有方法。在BEHAVE数据集上获得70.9的竞争性PVE，比最近发布的基于RGB的方法好18.4毫米，突显了深度数据的有用性。&lt;h4&gt;结论&lt;/h4&gt;该方法有效利用RGBD深度数据进行3D人体网格估计，在多个数据集上优于现有方法，代码将被发布。&lt;h4&gt;翻译&lt;/h4&gt;尽管从RGB图像进行3D人体网格估计已取得显著进展；提供额外深度数据的RGBD相机仍未得到充分利用。在本文中，我们提出了一种从单RGBD视图进行准确3D人体网格估计的方法，利用RGBD相机的经济性和广泛应用性进行实际应用。这个问题的完全监督方法需要包含RGBD图像和3D网格标签对的数据集。然而，收集这样的数据集成本高昂且具有挑战性，因此现有数据集小，且在姿势和形状多样性方面有限。为克服这种数据稀缺问题，我们利用现有的动作捕捉(MoCap)数据集。我们首先从MoCap数据集中的身体模型获取完整的3D网格，并通过投影到虚拟相机创建它们的单视图部分版本。这模拟了RGBD相机从单一视点提供的深度数据。然后，我们训练一个掩码自编码器来完成部分、单视图网格。在推理过程中，我们的方法（我们称之为M$^3$，代表'Masked Mesh Modeling'）将传感器传来的深度值与模板人体网格的顶点匹配，创建部分、单视图网格。我们有效地恢复了3D人体身体网格模型中不可见的部分，得到完整身体网格。M$^3$在SURREAL和CAPE数据集上分别实现了16.8毫米和22.0毫米的每顶点误差(PVE)，优于使用全身点云作为输入的现有方法。我们在BEHAVE数据集上获得了70.9的竞争性PVE，比最近发布的基于RGB的方法好18.4毫米，突显了深度数据的有用性。代码将被发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从单视角RGBD图像中估计完整3D人体网格模型的问题。这个问题很重要，因为RGBD相机越来越便宜和普及，而3D人体网格估计在计算机图形学、医疗保健、体育和AR/VR等领域有广泛应用。现有方法要么缺乏深度信息导致3D重建不准确，要么需要昂贵的设备设置或耗时的优化过程。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先确定了两个主要挑战：从部分可见数据完成人体网格，以及缺乏成对的RGBD-网格训练数据。针对这些挑战，作者借鉴了掩码图像建模的思想，设计了基于transformer的掩码自编码器来恢复完整网格；同时利用现有的MoCap数据集，通过投影网格到虚拟相机来模拟单视角深度数据。作者借鉴了DensePose进行UV映射提取，以及ViT-MAE的架构思想，但针对3D网格进行了专门设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用掩码自编码器学习从部分可见的人体网格恢复完整网格的能力，并利用MoCap数据集避免对成对RGBD-网格数据的依赖。整体流程：1)使用DensePose提取RGB图像的UV映射；2)结合深度图和UV映射生成带UV值的3D点云；3)通过UV对应关系匹配点云与模板网格顶点，生成部分网格；4)将部分网格输入掩码自编码器，完成不可见部分并去噪可见部分，输出完整3D人体网格。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出不依赖成对RGBD-网格数据的训练方法，利用MoCap数据集创建虚拟单视角深度数据；2)设计了专门的掩码网格建模(M3)方法，基于transformer的掩码自编码器；3)使用UV映射建立2D图像、深度数据和3D网格之间的对应关系。相比之前工作，不同之处在于：利用深度信息提高了3D重建准确性；不需要昂贵设备或耗时的优化过程；采用单次前向传播而非迭代优化；在多个数据集上表现更好；不依赖特定硬件，适用性更广。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于掩码自编码器的单视角RGBD人体网格估计方法，通过利用MoCap数据集和UV映射技术，实现了从部分可见数据中准确恢复完整3D人体网格，为实际应用提供了高效且准确的人体重建解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant progress in 3D human mesh estimation from RGB images;RGBD cameras, offering additional depth data, remain underutilized. In thispaper, we present a method for accurate 3D human mesh estimation from a singleRGBD view, leveraging the affordability and widespread adoption of RGBD camerasfor real-world applications. A fully supervised approach for this problem,requires a dataset with RGBD image and 3D mesh label pairs. However, collectingsuch a dataset is costly and challenging, hence, existing datasets are small,and limited in pose and shape diversity. To overcome this data scarcity, weleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3Dmeshes from the body models found in MoCap datasets, and create partial,single-view versions of them by projection to a virtual camera. This simulatesthe depth data provided by an RGBD camera from a single viewpoint. Then, wetrain a masked autoencoder to complete the partial, single-view mesh. Duringinference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',matches the depth values coming from the sensor to vertices of a template humanmesh, which creates a partial, single-view mesh. We effectively recover partsof the 3D human body mesh model that are not visible, resulting in a full bodymesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREALand CAPE datasets, respectively; outperforming existing methods that usefull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVEdataset, outperforming a recently published RGB based method by 18.4 mm,highlighting the usefulness of depth data. Code will be released.</description>
      <author>example@mail.com (Ozhan Suat, Bedirhan Uguz, Batuhan Karagoz, Muhammed Can Keles, Emre Akbas)</author>
      <guid isPermaLink="false">2508.08178v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking</title>
      <link>http://arxiv.org/abs/2508.08117v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了GRASPTrack，一种结合单目深度估计和实例分割的新型深度感知多目标跟踪框架，用于解决遮挡和深度模糊问题。&lt;h4&gt;背景&lt;/h4&gt;单目视频中的多目标跟踪面临遮挡和深度模糊的根本性挑战，传统的基于检测的跟踪方法因缺乏几何感知能力而难以解决这些问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理遮挡和深度模糊问题的多目标跟踪框架，提高在复杂场景中的跟踪鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;将单目深度估计和实例分割集成到标准基于检测的跟踪流程中，生成高保真三维点云实现几何推理；将点云体素化实现精确的空间关联；引入深度自适应噪声补偿动态调整卡尔曼滤波器；提出深度增强的观察中心动量将运动方向一致性从图像平面扩展到三维空间。&lt;h4&gt;主要发现&lt;/h4&gt;在MOT17、MOT20和DanceTrack基准测试上的实验表明，该方法取得了具有竞争力的性能，显著提高了在频繁遮挡和复杂运动模式的复杂场景中的跟踪鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;GRASPTrack通过整合深度感知和三维几何推理，有效解决了单目视频多目标跟踪中的遮挡和深度模糊问题，在复杂场景中表现出优越的性能。&lt;h4&gt;翻译&lt;/h4&gt;单目视频中的多目标跟踪在根本上受到遮挡和深度模糊的挑战，这些问题由于缺乏几何感知能力，传统的基于检测的跟踪方法难以解决。为了克服这些局限性，我们引入了GRASPTrack，一种新型深度感知多目标跟踪框架，它将单目深度估计和实例分割集成到标准基于检测的跟踪流程中，从二维检测生成高保真三维点云，从而实现显式的三维几何推理。然后，这些三维点云被体素化，以实现精确和鲁棒的基于体素的3D交并比用于空间关联。为了进一步增强跟踪鲁棒性，我们的方法结合了深度自适应噪声补偿，它根据遮挡严重程度动态调整卡尔曼滤波器过程噪声，以获得更可靠的状态估计。此外，我们提出了深度增强的观察中心动量，它将运动方向一致性从图像平面扩展到三维空间，以改善基于运动的关联线索，特别是对于具有复杂轨迹的物体。在MOT17、MOT20和DanceTrack基准上的大量实验表明，我们的方法取得了具有竞争力的性能，显著提高了在频繁遮挡和复杂运动模式的复杂场景中的跟踪鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目视频中的多目标跟踪问题，特别是遮挡和深度模糊带来的挑战。这个问题在现实中非常重要，因为在自动驾驶、机器人导航和体育分析等应用中，准确跟踪多个物体是至关重要的。遮挡和深度模糊是导致跟踪失败的主要原因，解决这些问题可以提高跟踪系统在复杂场景中的鲁棒性和准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统基于检测的跟踪方法(TBD)的局限性，特别是在处理遮挡和深度模糊方面的不足。他们借鉴了现有的单目深度估计和实例分割技术，将它们集成到跟踪流程中。作者还参考了OC-SORT等现有跟踪方法，并在此基础上进行了改进，如扩展了状态向量以包含深度信息，并改进了运动一致性建模。此外，作者还借鉴了3D点云处理和体素化技术，以实现更精确的空间关联。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D几何推理集成到多目标跟踪流程中，利用单目深度估计和实例分割生成高保真的3D点云表示，从而更好地处理遮挡和深度模糊问题。整体实现流程包括：1)使用单目深度估计模型生成深度图；2)使用实例分割模型生成物体掩码；3)使用掩码引导投影将2D检测转换为3D点云；4)将3D点云转换为体素表示；5)计算基于体素的3D IoU用于物体关联；6)使用扩展的卡尔曼滤波器进行状态预测，并根据遮挡程度动态调整噪声参数；7)在3D空间中建模运动一致性以改进关联。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)深度感知的MOT框架：集成单目深度估计和实例分割生成高保真3D点云；2)基于体素的3D IoU：用于更精确的空间关联，特别是在遮挡场景中；3)DANC：根据遮挡严重程度动态调整卡尔曼滤波过程噪声；4)DOCM：将运动方向一致性从2D扩展到3D空间。相比之前的工作，不同之处在于：不再依赖2D边界框，而是使用精确的3D点云表示；不再使用固定的过程噪声，而是根据遮挡情况动态调整；不再仅考虑2D运动，而是在3D空间中建模运动一致性；使用掩码引导投影，减少背景和遮挡物体的噪声影响。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GRASPTrack通过集成3D几何推理、自适应噪声补偿和深度增强的运动建模，显著提高了单目视频多目标跟踪在遮挡和深度模糊场景中的鲁棒性和准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-object tracking (MOT) in monocular videos is fundamentally challengedby occlusions and depth ambiguity, issues that conventionaltracking-by-detection (TBD) methods struggle to resolve owing to a lack ofgeometric awareness. To address these limitations, we introduce GRASPTrack, anovel depth-aware MOT framework that integrates monocular depth estimation andinstance segmentation into a standard TBD pipeline to generate high-fidelity 3Dpoint clouds from 2D detections, thereby enabling explicit 3D geometricreasoning. These 3D point clouds are then voxelized to enable a precise androbust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. Tofurther enhance tracking robustness, our approach incorporates Depth-awareAdaptive Noise Compensation, which dynamically adjusts the Kalman filterprocess noise based on occlusion severity for more reliable state estimation.Additionally, we propose a Depth-enhanced Observation-Centric Momentum, whichextends the motion direction consistency from the image plane into 3D space toimprove motion-based association cues, particularly for objects with complextrajectories. Extensive experiments on the MOT17, MOT20, and DanceTrackbenchmarks demonstrate that our method achieves competitive performance,significantly improving tracking robustness in complex scenes with frequentocclusions and intricate motion patterns.</description>
      <author>example@mail.com (Xudong Han, Pengcheng Fang, Yueying Tian, Jianhui Yu, Xiaohao Cai, Daniel Roggen, Philip Birch)</author>
      <guid isPermaLink="false">2508.08117v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Biases in Surgical Operating Rooms with Geometry</title>
      <link>http://arxiv.org/abs/2508.08028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Extended Abstract, presented at the MICCAI'25 workshop on  Collaborative Intelligence and Autonomy in Image-guided Surgery&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;深度神经网络在手术室人员识别任务中容易受到虚假相关性的影响，如手术衣标准化导致的视觉特征偏差。研究表明，几何表示方法比基于RGB的方法在现实临床环境中表现更稳定，准确率高12%。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络倾向于学习数据集中的虚假相关性，而非有意义的预测特征。在手术室环境中，手术衣和罩袍的标准化掩盖了稳健的识别特征，为手术室人员建模任务引入了模型偏差。&lt;h4&gt;目的&lt;/h4&gt;开发能够准确识别手术室人员个性化工作流程特征（如手术技能水平或团队协调能力）的智能辅助系统，避免因视觉特征标准化导致的识别偏差。&lt;h4&gt;方法&lt;/h4&gt;使用基于梯度的显著性分析方法研究两个公开手术室数据集，揭示CNN模型如何受到视觉捷径的影响。通过将人员编码为3D点云序列，将身份相关的形状和运动模式与基于外观的混淆因素分离。&lt;h4&gt;主要发现&lt;/h4&gt;RGB和几何方法在具有明显模拟伪影的数据集上性能相当，但在视觉多样性降低的现实临床环境中，RGB模型的准确性下降了12%，表明几何表示能够捕获更有意义的生物特征。&lt;h4&gt;结论&lt;/h4&gt;几何表示方法为开发手术室中稳健的人员建模提供了有效途径，能够更好地捕捉有意义的生物特征，减少因视觉标准化导致的识别偏差。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络容易学习虚假相关性，利用数据集特有的特征而非有意义的特征进行预测。在手术室中，这表现为手术衣和罩袍的标准化掩盖了稳健的识别特征，为建模手术室人员的任务引入了模型偏差。通过对两个公开手术室数据集进行基于梯度的显著性分析，我们揭示CNN模型会受到此类捷径的影响，专注于偶然的视觉线索，如手术服下的鞋子、独特的眼镜或其他角色特定标识。避免此类偏差对于下一代手术室智能辅助系统至关重要，这些系统应能准确识别个性化的工作流程特征，如手术技能水平或与其他工作人员的协调能力。我们通过将人员编码为3D点云序列来解决此问题，将身份相关的形状和运动模式与基于外观的混淆因素分离。我们的实验表明，虽然在具有明显模拟伪影的数据集上RGB和几何方法性能相当，但在视觉多样性降低的现实临床环境中，RGB模型的准确性下降了12%。这一性能差距证实了几何表示能够捕获更有意义的生物特征，为开发手术室中建模人类的稳健方法提供了途径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决手术操作室中深度神经网络模型因医护人员穿着标准化手术服而导致的偏见问题。当模型无法依靠可靠的身份识别特征时，会错误地依赖偶然的视觉线索（如鞋子或眼镜）进行人员识别。这个问题在现实中非常重要，因为手术室智能辅助系统需要准确识别个人化的工作流程特征（如手术技能水平或团队协调能力），而这些系统若依赖数据集特定的伪相关而非有意义的特征，在新临床环境中的表现会显著下降，影响医疗安全和手术质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过梯度显著度分析（使用GradCAM）揭示了CNN模型如何依赖于偶然的视觉线索，而非真正的身份特征。他们提出转向几何表示学习，将人员编码为3D点云序列，从而捕获与身份相关的形状和运动模式。作者借鉴了现有工作，包括使用LiDARGait框架处理点云表示、采用弱监督方法分割个体、使用三元组损失和在线困难负样本挖掘进行训练，并基于已有观点：几何特征在外观变化时保持不变。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用3D点云序列而非RGB图像来表示手术室人员，捕获不受标准化手术服影响的几何特征（如身高、步态和运动模式）。整体流程包括：1）收集模拟和真实临床数据集；2）使用弱监督方法从3D场景中分割个体；3）将个体表示为3D点云序列；4）使用修改后的LiDARGait框架，采用三元组损失进行训练；5）通过人员重新识别指标（如mAP、CMC@3）进行评估，并进行四折交叉验证。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）识别并解决了手术室环境中的特定偏见问题；2）提出使用3D点云序列表示手术室人员；3）通过对比RGB和几何表示，证明几何表示在真实临床环境中的优越性；4）进行详细显著度分析揭示模型偏见来源。相比之前工作，本文专注于解决标准化环境下的偏见问题，强调几何特征而非外观特征的重要性，专门针对手术室环境进行定制，并通过详细分析揭示问题本质而非仅提供解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过使用3D点云序列表示手术室人员，有效解决了标准化手术服导致的模型偏见问题，显著提高了在真实临床环境中的人员识别准确性和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks are prone to learning spurious correlations, exploitingdataset-specific artifacts rather than meaningful features for prediction. Insurgical operating rooms (OR), these manifest through the standardization ofsmocks and gowns that obscure robust identifying landmarks, introducing modelbias for tasks related to modeling OR personnel. Through gradient-basedsaliency analysis on two public OR datasets, we reveal that CNN models succumbto such shortcuts, fixating on incidental visual cues such as footwear beneathsurgical gowns, distinctive eyewear, or other role-specific identifiers.Avoiding such biases is essential for the next generation of intelligentassistance systems in the OR, which should accurately recognize personalizedworkflow traits, such as surgical skill level or coordination with other staffmembers. We address this problem by encoding personnel as 3D point cloudsequences, disentangling identity-relevant shape and motion patterns fromappearance-based confounders. Our experiments demonstrate that while RGB andgeometric methods achieve comparable performance on datasets with apparentsimulation artifacts, RGB models suffer a 12% accuracy drop in realisticclinical settings with decreased visual diversity due to standardizations. Thisperformance gap confirms that geometric representations capture more meaningfulbiometric features, providing an avenue to developing robust methods ofmodeling humans in the OR.</description>
      <author>example@mail.com (Tony Danjun Wang, Tobias Czempiel, Nassir Navab, Lennart Bastian)</author>
      <guid isPermaLink="false">2508.08028v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy</title>
      <link>http://arxiv.org/abs/2508.07611v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种端到端的人形机器人导航策略，可直接从原始激光雷达点云生成电机命令，实现了复杂动态环境中的安全导航，并通过模拟到现实的转移验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;人形机器人在非结构化、以人为中心的环境中部署需要超越简单运动的导航能力，包括强大感知、可证明安全性和社会感知行为。当前强化学习方法受限于缺乏环境意识的控制器或无法感知复杂3D障碍物的视觉系统。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在复杂动态场景中进行鲁棒导航的人形机器人系统，具备环境感知能力、安全性保证和社会感知行为。&lt;h4&gt;方法&lt;/h4&gt;提出端到端运动策略直接映射激光雷达点云到电机命令；将控制问题表述为约束马尔可夫决策过程；将控制屏障函数原理转化为成本函数；引入基于人机交互研究的舒适导向奖励机制。&lt;h4&gt;主要发现&lt;/h4&gt;所提框架成功从模拟转移到物理人形机器人；机器人能够展示敏捷且安全的导航，有效避开静态和动态3D障碍物。&lt;h4&gt;结论&lt;/h4&gt;结合端到端策略、安全保证和舒适性奖励，实现了人形机器人在复杂环境中的安全、社会感知导航。&lt;h4&gt;翻译&lt;/h4&gt;人形机器人在非结构化、以人为中心的环境中的部署需要超越简单运动的导航能力，包括强大的感知能力、可证明的安全性和社会感知行为。当前的强化学习方法通常受到缺乏环境意识的盲目控制器或无法感知复杂3D障碍物的视觉系统的限制。在这项工作中，我们提出了一个端到端的运动策略，直接将原始时空激光雷达点云映射到电机命令，实现复杂动态场景中的鲁棒导航。我们将控制问题表述为约束马尔可夫决策过程，以正式分离安全与任务目标。我们的关键贡献是一种新颖的方法，将控制屏障函数的原理转化为CMDP中的成本，允许无模型的惩罚近端策略优化在训练过程中强制执行安全约束。此外，我们引入了一系列基于人机交互研究的舒适导向奖励，促进平滑、可预测且干扰性较小的运动。我们通过成功将框架从模拟转移到物理人形机器人上，证明了我们框架的有效性，该机器人展示了围绕静态和动态3D障碍物的敏捷且安全的导航。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决人形机器人在复杂、以人为中心的环境中安全、舒适移动的问题。当前方法要么是盲目控制器缺乏环境感知能力，要么是基于视觉的系统无法感知复杂的3D障碍物。这个问题非常重要，因为人形机器人的最终目标是能够在人类日常环境中无缝共存和协作，这要求它们能够安全、高效地导航复杂空间，并且其行为要符合人类社会的舒适度和接受度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前方法的局限性：盲目控制器无法在有障碍物的环境中导航，而基于深度相机的方法对光照敏感且会丢失3D信息。作者选择LiDAR传感器作为感知方案，因为它具有光照不变性且能提供直接的3D信息。在安全方面，作者指出通过碰撞惩罚来设计奖励函数的方法往往是脆弱且难以调整的。作者借鉴了现有工作，包括使用强化学习开发控制器、将控制问题表述为约束马尔可夫决策过程(CMDP)，以及受控制屏障函数(CBF)理论的启发，但创新性地将其转化为模型自由RL算法的成本函数，最终设计了一个综合的端到端框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接处理原始时空LiDAR点云到电机命令的端到端映射，将控制问题表述为约束马尔可夫决策过程(CMDP)以分离安全与任务目标，将控制屏障函数(CBF)原则转化为CMDP中的成本，以及引入基于人机交互研究的舒适导向奖励。整体流程包括：1)使用LiDAR获取环境3D点云；2)提取64维特征向量；3)结合本体感受信息和命令历史；4)通过GRU和MLP处理数据并输出动作；5)使用LDCBF定义安全边界；6)设计任务导向和舒适导向奖励及安全成本函数；7)使用P3O算法训练策略；8)部署到实际机器人。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)LiDAR驱动的端到端策略，直接处理3D点云克服了盲目和2D视觉方法的局限；2)基于约束强化学习的原则性安全框架，将CBF原理转化为模型自由RL算法的成本；3)舒适导向的奖励结构，基于人机交互研究产生社会感知的动作；4)成功在物理人形机器人上的部署。相比之前工作，不同之处在于：能处理非地面级别障碍物，不受光照影响；提供更可靠的安全保证而非简单的奖励塑造；同时考虑安全性和舒适性，使机器人行为更符合人类期望。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于LiDAR感知和约束强化学习的端到端框架，使人形机器人能够在复杂3D环境中实现安全、舒适的导航，并成功在实际机器人上验证了该方法的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of humanoid robots in unstructured, human-centric environmentsrequires navigation capabilities that extend beyond simple locomotion toinclude robust perception, provable safety, and socially aware behavior.Current reinforcement learning approaches are often limited by blindcontrollers that lack environmental awareness or by vision-based systems thatfail to perceive complex 3D obstacles. In this work, we present an end-to-endlocomotion policy that directly maps raw, spatio-temporal LiDAR point clouds tomotor commands, enabling robust navigation in cluttered dynamic scenes. Weformulate the control problem as a Constrained Markov Decision Process (CMDP)to formally separate safety from task objectives. Our key contribution is anovel methodology that translates the principles of Control Barrier Functions(CBFs) into costs within the CMDP, allowing a model-free Penalized ProximalPolicy Optimization (P3O) to enforce safety constraints during training.Furthermore, we introduce a set of comfort-oriented rewards, grounded inhuman-robot interaction research, to promote motions that are smooth,predictable, and less intrusive. We demonstrate the efficacy of our frameworkthrough a successful sim-to-real transfer to a physical humanoid robot, whichexhibits agile and safe navigation around both static and dynamic 3D obstacles.</description>
      <author>example@mail.com (Zifan Wang, Xun Yang, Jianzhuang Zhao, Jiaming Zhou, Teli Ma, Ziyao Gao, Arash Ajoudani, Junwei Liang)</author>
      <guid isPermaLink="false">2508.07611v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Dynamic Scenes in Ego Centric 4D Point Clouds</title>
      <link>http://arxiv.org/abs/2508.07251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为EgoDynamic4D的新型QA基准，用于理解和推理以自我为中心的动态4D场景，包含丰富的标注数据、多种任务设计和一个有效的端到端推理框架，实验证明所提方法在动态场景理解任务上优于基线方法。&lt;h4&gt;背景&lt;/h4&gt;理解以自我为中心视角的动态4D场景（随时间变化的3D空间结构）对人机交互、自主导航和具身智能至关重要，但现有的以自我为中心数据集缺乏统一的4D标注和针对细粒度时空推理的任务驱动评估协议，特别是关于物体和人类的运动及其相互作用。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集的不足，引入EgoDynamic4D，一个针对高度动态场景的新型QA基准，包含RGB-D视频、相机姿态、全局唯一实例掩码和4D边界框，以支持细粒度的时空推理。&lt;h4&gt;方法&lt;/h4&gt;构建927K个配有思维链的QA对，设计12个动态QA任务包括智能体运动、人机交互、轨迹预测等，并提出一个端到端的时空推理框架，使用实例感知特征编码、时间和相机编码以及空间自适应下采样，将大型4D场景压缩为语言模型可处理的标记序列。&lt;h4&gt;主要发现&lt;/h4&gt;在EgoDynamic4D上的实验表明，所提出的方法始终优于基线方法，验证了多模态时间建模对以自我为中心的动态场景理解的有效性。&lt;h4&gt;结论&lt;/h4&gt;EgoDynamic4D基准为理解和推理动态4D场景提供了新的资源和方法，多模态时间建模在以自我为中心的动态场景理解中具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;理解从自我中心视角出发的动态四维场景即建模三维空间结构随时间的变化，对于人机交互、自主导航和具身智能至关重要。虽然现有的自我中心数据集包含动态场景，但它们缺乏统一的四维标注和针对细粒度时空推理的任务驱动评估协议，特别是关于物体和人类的运动及其相互作用。为解决这一差距，我们引入了EgoDynamic4D，一个针对高度动态场景的新型QA基准，包含RGB-D视频、相机姿态、全局唯一实例掩码和四维边界框。我们构建了927K个QA对，并配有明确的思维链，可实现可验证的、逐步的时空推理。我们设计了12个动态QA任务，包括智能体运动、人机交互、轨迹预测、关系理解和时序因果推理，具有细粒度、多维度的指标。为解决这些任务，我们提出了一个端到端的时空推理框架，统一处理动态和静态场景信息，使用实例感知特征编码、时间和相机编码以及空间自适应下采样，将大型四维场景压缩为语言模型可管理的标记序列。在EgoDynamic4D上的实验表明，我们的方法始终优于基线方法，验证了多模态时间建模对自我中心动态场景理解的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从第一人称视角理解动态4D场景（3D空间随时间变化）的问题。这个问题在现实中很重要，因为人机交互、自主导航和具身智能等领域需要准确理解周围环境的变化。现有的第一人称数据集缺乏统一的4D标注和任务驱动的评估协议，特别是对物体和人类运动及其相互作用的细粒度时空推理，限制了相关技术的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有数据集的局限性，包括不完整的4D标注、有限的时序推理评估和不完整的多模态评估。然后，他们创建了EgoDynamic4D基准，整合了ADT（真实世界）和THUD++（合成场景）两个数据集，并设计了端到端的时空推理框架。他们借鉴了现有工作如CLIP视觉编码器、LLaVA-3D等3D LLM，以及Video-CoT和SpatialCoT的思维链方法，但将这些技术扩展到4D动态场景理解中，并进行了创新改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将动态4D点云转换为LLM可处理的令牌序列，通过实例感知特征编码区分不同对象，使用时间和相机编码捕捉动态变化和视角变化，并通过空间自适应下采样压缩大型4D场景数据。整体流程包括：1)实例和时间增强的点级特征提取；2)特征融合，包括动态下采样、时间编码和特征集成；3)相机嵌入，压缩相机姿态序列；4)将融合特征投影到LLM嵌入空间并进行推理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)EgoDynamic4D基准，包含927K个QA对和思维链，涵盖12种任务类型；2)统一的多模态数据集，提供完整的4D标注；3)端到端时空推理框架，使用实例感知特征编码、时间和相机编码；4)思维链监督增强推理能力。相比之前工作，本文提供了完整的4D标注而非稀疏标注，专门针对动态场景而非静态场景，集成了时间和相机信息，并提供了细粒度的多维评估指标和思维链评估。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了EgoDynamic4D基准和一个端到端的时空推理框架，通过统一的4D标注和思维链监督，显著提升了从第一人称视角理解动态场景的能力，为具身AI和机器人感知等领域提供了新的技术基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding dynamic 4D scenes from an egocentric perspective-modelingchanges in 3D spatial structure over time-is crucial for human-machineinteraction, autonomous navigation, and embodied intelligence. While existingegocentric datasets contain dynamic scenes, they lack unified 4D annotationsand task-driven evaluation protocols for fine-grained spatio-temporalreasoning, especially on motion of objects and human, together with theirinteractions. To address this gap, we introduce EgoDynamic4D, a novel QAbenchmark on highly dynamic scenes, comprising RGB-D video, camera poses,globally unique instance masks, and 4D bounding boxes. We construct 927K QApairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,step-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks coveringagent motion, human-object interaction, trajectory prediction, relationunderstanding, and temporal-causal reasoning, with fine-grained,multidimensional metrics. To tackle these tasks, we propose an end-to-endspatio-temporal reasoning framework that unifies dynamic and static sceneinformation, using instance-aware feature encoding, time and camera encoding,and spatially adaptive down-sampling to compress large 4D scenes into tokensequences manageable by LLMs. Experiments on EgoDynamic4D show that our methodconsistently outperforms baselines, validating the effectiveness of multimodaltemporal modeling for egocentric dynamic scene understanding.</description>
      <author>example@mail.com (Junsheng Huang, Shengyu Hao, Bocheng Hu, Gaoang Wang)</author>
      <guid isPermaLink="false">2508.07251v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree</title>
      <link>http://arxiv.org/abs/2508.07083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TeSO是一种创新的3D表示方法，通过结合Surfel和纹理贴图在八叉树结构中，解决了现有3D表示方法的局限性，实现了高质量渲染和高效压缩的平衡&lt;h4&gt;背景&lt;/h4&gt;3D视觉内容流技术是新兴的3D远程呈现和AR/VR应用的关键技术。该技术的基础是一种通用的3D表示方法，能够同时产生高质量渲染和高效压缩。现有的3D表示方法（点云、网格和3D高斯）在渲染质量、表面定义和可压缩性方面各有局限&lt;h4&gt;目的&lt;/h4&gt;提出一种新的3D表示方法，解决现有方法的局限性，创建一种能够同时提供高质量渲染和高效压缩的3D表示方法&lt;h4&gt;方法&lt;/h4&gt;提出了纹理化Surfel八叉树（TeSO），该方法基于点云构建，将3D场景表示为在八叉树上组织的立方体边界Surfel，每个Surfel关联一个纹理贴图。通过在八叉树的较粗层级用大Surfel近似平滑表面，减少表示3D场景所需的基元数量，同时保留高频纹理细节。还提出了一种压缩方案，利用八叉树结构高效编码几何和纹理信息&lt;h4&gt;主要发现&lt;/h4&gt;提出的纹理化Surfel八叉树结合压缩方案，与多个基于点云和3D高斯的基线相比，能够在较低的比特率下实现更高的渲染质量&lt;h4&gt;结论&lt;/h4&gt;TeSO是一种有前景的3D表示方法，能够平衡渲染质量和压缩效率&lt;h4&gt;翻译&lt;/h4&gt;3D视觉内容流是新兴的3D远程呈现和AR/VR应用的关键技术。该技术的基础是一种通用的3D表示方法，能够同时产生高质量渲染并可以高效压缩。现有的3D表示方法如点云、网格和3D高斯在渲染质量、表面定义和可压缩性方面各有局限。在本文中，我们提出了纹理化Surfel八叉树（TeSO），一种基于点云构建但解决了上述局限性的新型3D表示方法。它将3D场景表示为在八叉树上组织的立方体边界Surfel，其中每个Surfel进一步关联一个纹理贴图。通过在八叉树的较粗层级用大Surfel近似平滑表面，减少了表示3D场景所需的基元数量，同时通过附加到每个Surfel的纹理图保留了高频纹理细节。我们进一步提出了一种压缩方案，利用八叉树结构高效编码几何和纹理信息。与多个基于点云和3D高斯的基线相比，所提出的纹理化Surfel八叉树结合压缩方案在较低的比特率下实现了更高的渲染质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D视觉内容流媒体中的表示和压缩问题，特别是针对新兴的3D远程呈现和AR/VR应用需要同时提供高质量渲染和高效压缩的需求。这个问题很重要，因为它直接影响3D远程通信技术的实用性和用户体验，如远程AR教育、3D视频会议和沉浸式游戏等应用需要实时传输大量3D数据，而现有方法（点云、网格、3D高斯）在渲染质量、表面定义和压缩性方面各有局限。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D表示方法的优缺点：点云灵活但缺乏显式表面表示；网格有表面定义但不够紧凑；3D高斯渲染质量好但无法表示高频纹理。作者观察到3D场景中几何和纹理复杂性可能不同，平滑区域可用较少基元表示，但仍需保留纹理细节。基于这些观察，作者设计了TeSO方法，借鉴了八叉树结构、点云渲染中的splatting概念、网格纹理映射、学习熵模型和3D高斯溅射的混合机制等现有工作，并将它们创新地结合在一起。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D场景表示为由八叉树组织的立方体边界纹理表面元素（surfel），每个surfel包含几何属性（位置、法线、半径）和一个纹理贴图，允许在平滑区域使用较大surfel减少基元数量，同时通过纹理贴图保留高频细节。整体流程包括：1)从点云构建surfel八叉树；2)为每个surfel生成纹理贴图；3)使用光线追踪和软区域混合进行高质量渲染；4)使用学习熵模型压缩几何，用标准视频编解码器或点云方法压缩纹理；5)解压缩并从任意视角渲染场景。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出Textured Surfel Octree新表示方法，结合点云和网格优点；2)设计GPU加速的构建算法，100万点可在0.3秒内完成；3)提出专门的压缩方案，使用学习卷积熵模型；4)设计高质量渲染器，支持6自由度实时渲染。相比之前工作：与点云方法相比，TeSO提供显式表面表示，无渲染空洞，不需解码端重建；与网格方法相比，无需顶点连接信息，更紧凑灵活；与3D高斯方法相比，能减少平滑区域基元，更好表示高频纹理；与现有渲染方法相比，不需重型神经网络，对相机设置更鲁棒，支持实时渲染。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TeSO提出了一种创新的3D场景表示方法，通过结合纹理表面元素和八叉树结构，实现了高质量的实时渲染和高效的压缩，为3D远程呈现和AR/VR应用提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D visual content streaming is a key technology for emerging 3D telepresenceand AR/VR applications. One fundamental element underlying the technology is aversatile 3D representation that is capable of producing high-quality rendersand can be efficiently compressed at the same time. Existing 3D representationslike point clouds, meshes and 3D Gaussians each have limitations in terms ofrendering quality, surface definition, and compressibility. In this paper, wepresent the Textured Surfel Octree (TeSO), a novel 3D representation that isbuilt from point clouds but addresses the aforementioned limitations. Itrepresents a 3D scene as cube-bounded surfels organized on an octree, whereeach surfel is further associated with a texture patch. By approximating asmooth surface with a large surfel at a coarser level of the octree, it reducesthe number of primitives required to represent the 3D scene, and yet retainsthe high-frequency texture details through the texture map attached to eachsurfel. We further propose a compression scheme to encode the geometry andtexture efficiently, leveraging the octree structure. The proposed texturedsurfel octree combined with the compression scheme achieves higher renderingquality at lower bit-rates compared to multiple point cloud and 3DGaussian-based baselines.</description>
      <author>example@mail.com (Yueyu Hu, Ran Gong, Tingyu Fan, Yao Wang)</author>
      <guid isPermaLink="false">2508.07083v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View</title>
      <link>http://arxiv.org/abs/2508.06968v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究首次评估了鱼眼镜头的3D高斯散射方法在真实广角图像上的性能，提出了基于深度的初始化策略解决强失真问题，并证明了这些方法从稀疏失真图像进行广角3D重建的可行性。&lt;h4&gt;背景&lt;/h4&gt;鱼眼镜头能提供超过180度的广阔视野但导致严重图像失真，现有3D高斯散射方法通常针对普通相机设计，在处理鱼眼图像的极端失真时面临挑战，且基于SfM的初始化在强失真条件下常常失败。&lt;h4&gt;目的&lt;/h4&gt;评估鱼眼基础的3D高斯散射方法在真实广角图像上的性能，分析处理极端失真的能力，研究不同视野角度下的性能权衡，提出新的初始化策略，展示鱼眼3DGS方法的实际应用价值。&lt;h4&gt;方法&lt;/h4&gt;使用200度鱼眼相机拍摄室内外场景，评估Fisheye-GS和3DGUT两种方法，在不同视野角度(200度、160度和120度)下测试性能，提出基于深度的初始化策略利用仅2-3张鱼眼图像通过UniK3D预测生成密集点云进行重建。&lt;h4&gt;主要发现&lt;/h4&gt;Fisheye-GS在160度视野时表现更好，3DGUT在所有设置下保持稳定并在200度视野下维持高质量；基于深度的UniK3D策略能生成与SfM相当的密集点云，即使在雾、眩光或天空等困难场景中也能有效工作。&lt;h4&gt;结论&lt;/h4&gt;鱼眼基础的3D高斯散射方法在处理广角图像和极端失真方面具有实际可行性，通过适当调整视野角度和使用基于深度的初始化策略，即使在具有挑战性的条件下也能实现高质量的3D重建。&lt;h4&gt;翻译&lt;/h4&gt;我们首次对基于鱼眼的3D高斯散射方法(Fisheye-GS和3DGUT)进行了评估，研究对象是视野超过180度的真实图像。我们的研究涵盖了使用200度鱼眼相机拍摄的室内和室外场景，并分析了每种方法如何处理现实世界中的极端失真。我们在不同视野(200度、160度和120度)下评估性能，以研究周边失真与空间覆盖之间的权衡。Fisheye-GS从视野减少中受益，特别是在160度时，而3DGUT在所有设置下保持稳定，并在完整的200度视野下保持高感知质量。为了解决SfM初始化的局限性(在强失真条件下常常失败)，我们还提出了一种基于深度的策略，使用仅每场景2-3张鱼眼图像的UniK3D预测。尽管UniK3D未在真实鱼眼数据上训练，但它能生成密集点云，即使在有雾、眩光或天空的困难场景中，也能实现与SfM相当的重建质量。我们的结果突显了鱼眼基础3DGS方法从稀疏和高度失真的图像输入中进行广角3D重建的实际可行性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要评估鱼眼相机上的3D高斯溅射方法在视野超过180度的真实图像上的表现。这个问题很重要，因为鱼眼相机能用更少图像覆盖更大场景，对自动驾驶、机器人感知和虚拟现实等领域至关重要，但其非线性投影和强畸变给3D重建带来挑战，而现有方法大多针对窄视野透视相机设计，缺乏对鱼眼图像的系统评估。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3D高斯溅射在鱼眼相机上的应用潜力但缺乏系统评估，于是借鉴了现有的Fisheye-GS和3DGUT两种鱼眼适配方法。针对传统SfM初始化在强畸变下不可靠的问题，作者探索了使用单目深度估计作为替代方案，特别是评估了UniK3D预测的有效性。实验设计包括使用四个真实场景，评估不同视野角度下的性能，并提出基于深度估计的初始化策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是评估两种鱼眼适配的3D高斯溅射方法在超宽视野真实图像上的表现，并提出基于深度估计的初始化策略替代传统SfM。整体流程包括：1)数据采集和相机校准；2)评估Fisheye-GS和3DGUT两种方法；3)研究不同视野角度(200°、160°、120°)对重建质量的影响；4)使用UniK3D从少量鱼眼图像生成深度图并构建密集点云；5)比较不同初始化策略和视野设置下的重建质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次在超宽视野真实图像上评估鱼眼适配的3D高斯溅射方法；首次实证分析UniK3D在真实鱼眼数据上的表现；展示仅用2-3张图像的单目深度可产生与SfM相当的重建质量；分析视野角度与重建质量的关系；将单目点云与COLMAP坐标系对齐。相比之前工作，本文专注于鱼眼相机这种超宽视野设备，在真实超宽视野图像而非合成数据上评估，并提出了替代SfM的初始化策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次系统评估了鱼眼适配的3D高斯溅射方法在超宽视野真实图像上的表现，并提出了一种基于深度估计的初始化策略，使得仅用少量鱼眼图像就能实现高质量的3D重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the first evaluation of fisheye-based 3D Gaussian Splattingmethods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180degree. Our study covers both indoor and outdoor scenes captured with 200degree fisheye cameras and analyzes how each method handles extreme distortionin real world settings. We evaluate performance under varying fields of view(200 degree, 160 degree, and 120 degree) to study the tradeoff betweenperipheral distortion and spatial coverage. Fisheye-GS benefits from field ofview (FoV) reduction, particularly at 160 degree, while 3DGUT remains stableacross all settings and maintains high perceptual quality at the full 200degree view. To address the limitations of SfM-based initialization, whichoften fails under strong distortion, we also propose a depth-based strategyusing UniK3D predictions from only 2-3 fisheye images per scene. AlthoughUniK3D is not trained on real fisheye data, it produces dense point clouds thatenable reconstruction quality on par with SfM, even in difficult scenes withfog, glare, or sky. Our results highlight the practical viability offisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse anddistortion-heavy image inputs.</description>
      <author>example@mail.com (Ulas Gunes, Matias Turkulainen, Juho Kannala, Esa Rahtu)</author>
      <guid isPermaLink="false">2508.06968v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Robust-Sub-Gaussian Model Predictive Control for Safe Ultrasound-Image-Guided Robotic Spinal Surgery</title>
      <link>http://arxiv.org/abs/2508.06744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种处理高维感官反馈安全关键控制的新方法，特别是在机器人手术等应用领域。通过引入次高斯噪声模型和开发新的不确定性传播技术以及模型预测控制框架，能够在复杂图像引导机器人手术任务中提供安全保证。&lt;h4&gt;背景&lt;/h4&gt;在高维感官反馈（如图像、点云）进行安全关键控制面临重大挑战，特别是在自动驾驶和机器人手术等领域。控制依赖于从高维数据估计的低维状态，但估计误差通常遵循复杂且未知的分布，标准概率模型无法捕捉这些分布，使得形式化安全保证变得困难。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的方法来表征一般的估计误差，开发不确定性传播技术，以及在提出的噪声假设下为线性系统提供闭环安全保证的模型预测控制框架。&lt;h4&gt;方法&lt;/h4&gt;1) 引入使用有界均值的次高斯噪声表征估计误差；2) 开发结合鲁棒集合方法和次高斯方差代理传播的不确定性传播技术；3) 开发模型预测控制框架为线性系统提供安全保证；4) 将方法应用于超声图像引导的机器人脊柱手术流程；5) 开发整合真实人体解剖结构、机器人动力学、超声仿真以及呼吸运动和钻孔力数据的仿真环境。&lt;h4&gt;主要发现&lt;/h4&gt;1) 次高斯噪声模型可有效表征估计误差；2) 结合鲁棒集合方法和次高斯方差代理的不确定性传播技术是有效的；3) 模型预测控制框架能在噪声假设下为线性系统提供安全保证；4) 仿真评估结果表明该方法能解决复杂图像引导机器人手术任务同时确保安全。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在解决复杂图像引导机器人手术任务同时确保安全方面显示出潜力。&lt;h4&gt;翻译&lt;/h4&gt;在高维感官反馈（如图像、点云等光学数据）中进行安全关键控制在自动驾驶和机器人手术等领域带来重大挑战。控制可以依赖于从高维数据估计出的低维状态，但估计误差通常遵循复杂且未知的分布，标准概率模型无法捕捉这些分布，使得形式化安全保证变得困难。在这项工作中，我们引入了一种使用有界均值的次高斯噪声来表征这些一般估计误差的新方法。我们开发了一种在线性系统中传播所提出噪声表征的不确定性新技术，结合了鲁棒的集合方法和次高斯方差代理的传播。我们进一步开发了一种模型预测控制框架，为线性系统在提出的噪声假设下提供闭环安全保证。我们将这种方法应用于基于超声图像引导的机器人脊柱手术流程，该流程包含基于深度学习的语义分割、基于图像的配准、高层基于优化的规划和低层机器人控制。为了验证该流程，我们开发了一个真实的仿真环境，整合了真实人体解剖结构、机器人动力学、高效的超声仿真以及呼吸运动和钻孔力的体内数据。仿真中的评估结果表明了我们的方法在解决复杂图像引导机器人手术任务同时确保安全方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safety-critical control using high-dimensional sensory feedback from opticaldata (e.g., images, point clouds) poses significant challenges in domains likeautonomous driving and robotic surgery. Control can rely on low-dimensionalstates estimated from high-dimensional data. However, the estimation errorsoften follow complex, unknown distributions that standard probabilistic modelsfail to capture, making formal safety guarantees challenging. In this work, weintroduce a novel characterization of these general estimation errors usingsub-Gaussian noise with bounded mean. We develop a new technique foruncertainty propagation of proposed noise characterization in linear systems,which combines robust set-based methods with the propagation of sub-Gaussianvariance proxies. We further develop a Model Predictive Control (MPC) frameworkthat provides closed-loop safety guarantees for linear systems under theproposed noise assumption. We apply this MPC approach in anultrasound-image-guided robotic spinal surgery pipeline, which containsdeep-learning-based semantic segmentation, image-based registration, high-leveloptimization-based planning, and low-level robotic control. To validate thepipeline, we developed a realistic simulation environment integrating realhuman anatomy, robot dynamics, efficient ultrasound simulation, as well asin-vivo data of breathing motion and drilling force. Evaluation results insimulation demonstrate the potential of our approach for solving compleximage-guided robotic surgery task while ensuring safety.</description>
      <author>example@mail.com (Yunke Ao, Manish Prajapat, Yarden As, Yassine Taoudi-Benchekroun, Fabio Carrillo, Hooman Esfandiari, Benjamin F. Grewe, Andreas Krause, Philipp Fürnstahl)</author>
      <guid isPermaLink="false">2508.06744v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Fourier Optics and Deep Learning Methods for Fast 3D Reconstruction in Digital Holography</title>
      <link>http://arxiv.org/abs/2508.06703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种高效快速的流水线框架，用于计算机生成全息术(CGH)的合成，通过结合初始点云和MRI数据，利用非凸傅里叶光学优化算法生成全息图，并通过中值滤波提高图像质量。&lt;h4&gt;背景&lt;/h4&gt;计算机生成全息术(CGH)是一种通过数字全息图调制用户定义波形的有前景的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效快速的流水线框架，用于从初始点云和MRI数据合成CGH。&lt;h4&gt;方法&lt;/h4&gt;将输入数据重建为体积对象，然后使用交替投影、随机梯度下降和拟牛顿方法进行非凸傅里叶光学优化，生成仅相位全息图(POH)和复全息图(CH)，并通过二维中值滤波去除伪影和散斑噪声。&lt;h4&gt;主要发现&lt;/h4&gt;使用二维中值滤波可以去除优化过程中的伪影和散斑噪声，从而提高MSE、RMSE和PSNR等性能指标。&lt;h4&gt;结论&lt;/h4&gt;提出的框架和方法能够有效地生成计算机生成全息图，且通过后处理技术可以进一步提高图像质量。&lt;h4&gt;翻译&lt;/h4&gt;计算机生成全息术(CGH)是一种通过数字全息图调制用户定义波形的有前景的方法。该研究提出了一种高效快速的流水线框架，用于使用初始点云和MRI数据合成CGH。这些输入数据被重建为体积对象，然后输入到非凸傅里叶光学优化算法中，使用交替投影、随机梯度下降和拟牛顿方法生成仅相位全息图(POH)和复全息图(CH)。分析了这些算法在MSE、RMSE和PSNR指标下的重建性能，并与HoloNet深度学习CGH进行了比较。研究表明，使用二维中值滤波去除优化过程中的伪影和散斑噪声可以改善性能指标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computer-generated holography (CGH) is a promising method that modulatesuser-defined waveforms with digital holograms. An efficient and fast pipelineframework is proposed to synthesize CGH using initial point cloud and MRI data.This input data is reconstructed into volumetric objects that are then inputinto non-convex Fourier optics optimization algorithms for phase-only hologram(POH) and complex-hologram (CH) generation using alternating projection, SGD,and quasi-Netwton methods. Comparison of reconstruction performance of thesealgorithms as measured by MSE, RMSE, and PSNR is analyzed as well as to HoloNetdeep learning CGH. Performance metrics are shown to be improved by using 2Dmedian filtering to remove artifacts and speckled noise during optimization.</description>
      <author>example@mail.com (Justin London)</author>
      <guid isPermaLink="false">2508.06703v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data</title>
      <link>http://arxiv.org/abs/2508.08173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Time-varying data visualization, deep learning, super-resolution,  diffusion model&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CD-TVD的新框架，结合对比学习和改进的扩散模型，实现从有限高分辨率数据中进行准确的3D超分辨率，减少了对大规模数据集的依赖。&lt;h4&gt;背景&lt;/h4&gt;大规模科学模拟需要大量资源生成高分辨率时变数据，现有超分辨率方法依赖大量高分辨率训练数据，限制了其在多样模拟场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从有限时间步长高分辨率数据实现准确3D超分辨率的方法，减少对大规模数据集的依赖。&lt;h4&gt;方法&lt;/h4&gt;CD-TVD框架结合对比学习和改进的扩散模型，通过历史数据预训练学习退化模式和特征，然后利用局部注意力机制和仅一个高分辨率时间步长进行微调，保持细节恢复能力。&lt;h4&gt;主要发现&lt;/h4&gt;在流体和大气模拟数据集上的实验证实，CD-TVD提供了准确且资源高效的3D超分辨率，是大规模科学模拟数据增强的重要进展。&lt;h4&gt;结论&lt;/h4&gt;CD-TVD框架显著减少了对大规模高分辨率数据集的依赖，同时保持了恢复细粒度细节的能力，为科学模拟提供了高效的数据增强解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大规模科学模拟需要大量资源来生成高分辨率时变数据。虽然超分辨率是一种有效的后处理策略来降低成本，但现有方法依赖于大量高分辨率训练数据，限制了它们在不同模拟场景中的适用性。为了解决这一限制，我们提出了CD-TVD，一种结合对比学习和改进的基于扩散的超分辨率模型的新框架，从有限时间步长的高分辨率数据实现准确的3D超分辨率。在历史模拟数据上的预训练期间，对比编码器和扩散超分辨率模块学习高分辨率和低分辨率样本的退化模式和详细特征。在训练阶段，使用仅一个新生成的高分辨率时间步长来微调具有局部注意力机制的改进扩散模型，利用编码器学到的退化知识。这种设计最小化了对大规模高分辨率数据集的依赖，同时保持恢复细粒度细节的能力。在流体和大气模拟数据集上的实验结果证实，CD-TVD提供了准确且资源高效的3D超分辨率，标志着大规模科学模拟数据增强的重大进展。代码可在https://github.com/Xin-Gao-private/CD-TVD获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决科学模拟中高分辨率时间变化数据获取成本高昂的问题。在现实研究中，大规模科学模拟需要大量资源生成高分辨率数据，直接进行高分辨率模拟面临计算成本指数增长和数据存储需求激增的瓶颈。虽然低精度计算提高了效率，但往往无法捕捉微观结构的演化细节或临界状态的突变特征，导致预测精度显著下降。超分辨率技术本可通过建立从低精度数据到高分辨率空间的映射关系，用有限计算资源恢复关键物理场中的精细结构，但现有方法依赖大量高分辨率训练数据，而获取这些数据成本极高，例如一个高分辨率案例在CFD模拟中通常需要几周的GPU集群计算，严重限制了超分辨率技术在科学模拟中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到科学模拟中高分辨率数据获取困难和成本高昂的问题，意识到现有超分辨率方法因依赖大量训练数据而应用受限。他们借鉴了对比学习和扩散模型两种技术：对比学习用于学习高分辨率和低分辨率数据之间的退化模式，扩散模型用于捕获精细细节。作者设计了一个两阶段流程：预训练阶段使用历史模拟数据训练对比编码模块和扩散超分辨率模块，共同学习退化模式和特征；微调阶段冻结对比编码模块，只使用少量高分辨率样本微调扩散超分辨率模块。此外，作者还借鉴了局部注意力机制来减少传统扩散模型在高维3D数据上的计算负担，并使用对抗训练提高模型性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将高分辨率和低分辨率数据之间的退化过程建模为对比学习任务，从历史数据中提取判别性退化特征，同时使用扩散模型进行超分辨率处理，通过局部注意力机制减少计算成本，并利用预训练学习到的通用退化模式，使模型只需少量高分辨率时间步就能重建所有后续低分辨率时间步。整体实现流程分为两个阶段：1)预训练阶段：使用历史模拟数据同时训练对比编码模块和扩散超分辨率模块，前者通过对比高分辨率、低分辨率和超分辨率数据学习退化模式，后者使用对抗训练捕获精细细节并通过局部注意力机制减少计算成本；2)微调阶段：冻结对比编码模块保留已学习的先验知识，使用少量高分辨率样本微调扩散超分辨率模块，弥补新数据集中缺失的高频细节，实现最小高分辨率数据下的精确超分辨率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将高分辨率和低分辨率数据之间的退化过程显式建模为对比学习任务，从历史数据中提取判别性退化特征，实现跨场景的3D超分辨率泛化；2)将局部注意力机制集成到扩散模型中，减轻传统扩散方法的计算和内存负担，同时能恢复高分辨率结构；3)利用预训练学习的通用退化模式，只需新数据集中的少量高分辨率时间步就能重建所有后续低分辨率时间步。相比之前的工作，CD-TVD显著减少了对高分辨率数据的需求，只需少量高分辨率样本而非大量数据；使用扩散模型避免了GAN方法的训练不稳定问题；相比纯CNN方法能更好地捕获复杂退化模式和精细细节；与其他需要多个高分辨率快照的方法不同，CD-TVD只需一个高分辨率时间步；结合了对比学习和扩散模型的优势，效果优于单独使用其中一种方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CD-TVD通过结合对比学习和扩散模型，实现了在只有少量高分辨率时间数据的情况下，高效准确地重建3D科学模拟中的高分辨率时间变化数据，显著降低了科学模拟对计算资源的需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale scientific simulations require significant resources to generatehigh-resolution time-varying data (TVD). While super-resolution is an efficientpost-processing strategy to reduce costs, existing methods rely on a largeamount of HR training data, limiting their applicability to diverse simulationscenarios. To address this constraint, we proposed CD-TVD, a novel frameworkthat combines contrastive learning and an improved diffusion-basedsuper-resolution model to achieve accurate 3D super-resolution from limitedtime-step high-resolution data. During pre-training on historical simulationdata, the contrastive encoder and diffusion superresolution modules learndegradation patterns and detailed features of high-resolution andlow-resolution samples. In the training phase, the improved diffusion modelwith a local attention mechanism is fine-tuned using only one newly generatedhigh-resolution timestep, leveraging the degradation knowledge learned by theencoder. This design minimizes the reliance on large-scale high-resolutiondatasets while maintaining the capability to recover fine-grained details.Experimental results on fluid and atmospheric simulation datasets confirm thatCD-TVD delivers accurate and resource-efficient 3D super-resolution, marking asignificant advancement in data augmentation for large-scale scientificsimulations. The code is available athttps://github.com/Xin-Gao-private/CD-TVD.</description>
      <author>example@mail.com (Chongke Bi, Xin Gao, Jiangkang Deng, Guan)</author>
      <guid isPermaLink="false">2508.08173v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks</title>
      <link>http://arxiv.org/abs/2508.08127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了BlindGuard，一种用于保护基于大语言模型的多智能体系统(MAS)的无监督防御方法，能够有效检测恶意智能体而不需要攻击特定标签或先验知识。&lt;h4&gt;背景&lt;/h4&gt;基于大语言模型的多智能体系统面临传播漏洞的安全威胁，恶意智能体可通过智能体间消息交互扭曲集体决策。现有监督防御方法在实际应用中可能不实用，因为它们严重依赖标记的恶意智能体进行训练。&lt;h4&gt;目的&lt;/h4&gt;开发一种实用且可泛化的MAS防御方法，不需要任何攻击特定标签或恶意行为先验知识的无监督防御方法。&lt;h4&gt;方法&lt;/h4&gt;提出BlindGuard，建立分层智能体编码器捕获每个智能体的个体、邻域和全局交互模式；设计基于损坏的检测器，包含定向噪声注入和对比学习，仅使用正常智能体行为进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;BlindGuard能有效检测多种攻击类型（提示注入、记忆中毒和工具攻击），在各种通信模式的MAS中有效工作，且与监督基线相比具有更好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;BlindGuard是一种有效的无监督防御方法，不需要攻击特定标签或恶意行为的先验知识，能够检测多种攻击类型并在不同通信模式的MAS中保持良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;基于大语言模型的多智能体系统(MAS)的安全性受到传播漏洞的严重威胁，恶意智能体可以通过智能体间消息交互扭曲集体决策。虽然现有的监督防御方法显示出有希望的性能，但由于它们严重依赖标记的恶意智能体来训练监督恶意检测模型，可能在现实场景中不实用。为了实现实用且可泛化的MAS防御，在本文中，我们提出了BlindGuard，一种无监督防御方法，无需任何攻击特定标签或恶意行为先验知识即可学习。为此，我们建立了分层智能体编码器来捕获每个智能体的个体、邻域和全局交互模式，为恶意智能体检测提供全面理解。同时，我们设计了基于损坏的检测器，包含定向噪声注入和对比学习，允许仅使用正常智能体行为进行有效的检测模型训练。大量实验表明，BlindGuard能够有效检测具有各种通信模式的MAS中的不同攻击类型（即提示注入、记忆中毒和工具攻击），同时保持比监督基线更好的泛化能力。代码可在以下网址获取：https://github.com/MR9812/BlindGuard。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The security of LLM-based multi-agent systems (MAS) is critically threatenedby propagation vulnerability, where malicious agents can distort collectivedecision-making through inter-agent message interactions. While existingsupervised defense methods demonstrate promising performance, they may beimpractical in real-world scenarios due to their heavy reliance on labeledmalicious agents to train a supervised malicious detection model. To enablepractical and generalizable MAS defenses, in this paper, we propose BlindGuard,an unsupervised defense method that learns without requiring anyattack-specific labels or prior knowledge of malicious behaviors. To this end,we establish a hierarchical agent encoder to capture individual, neighborhood,and global interaction patterns of each agent, providing a comprehensiveunderstanding for malicious agent detection. Meanwhile, we design acorruption-guided detector that consists of directional noise injection andcontrastive learning, allowing effective detection model training solely onnormal agent behaviors. Extensive experiments show that BlindGuard effectivelydetects diverse attack types (i.e., prompt injection, memory poisoning, andtool attack) across MAS with various communication patterns while maintainingsuperior generalizability compared to supervised baselines. The code isavailable at: https://github.com/MR9812/BlindGuard.</description>
      <author>example@mail.com (Rui Miao, Yixin Liu, Yili Wang, Xu Shen, Yue Tan, Yiwei Dai, Shirui Pan, Xin Wang)</author>
      <guid isPermaLink="false">2508.08127v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Selective Contrastive Learning for Weakly Supervised Affordance Grounding</title>
      <link>http://arxiv.org/abs/2508.07877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种改进的弱监督功能定位方法，通过选择性原型和像素对比目标解决现有模型过度依赖分类而非功能相关部分的问题，实现了在部分和对象级别自适应学习功能相关线索。&lt;h4&gt;背景&lt;/h4&gt;弱监督功能定位(WSAG)试图模仿人类从第三方演示中学习的方式，无需像素级标注即可理解功能部分。现有方法通常使用跨图像共享分类器和部分发现过程的蒸馏策略，但由于功能相关部分不易区分，模型往往关注与功能无关的常见类别特定模式。&lt;h4&gt;目的&lt;/h4&gt;解决现有WSAG方法过度依赖分类而非功能相关部分的问题，通过引入新型学习目标在不同粒度信息下自适应地学习功能相关线索。&lt;h4&gt;方法&lt;/h4&gt;首先利用CLIP在第一人称和第三人称图像中找到与动作相关的对象；然后通过交叉参考互补视角中发现的物体，挖掘每个视角中的精确部分级功能线索；最后通过持续学习区分功能相关区域与功能无关的背景上下文，使激活从无关区域转向有意义的功能线索。&lt;h4&gt;主要发现&lt;/h4&gt;现有WSAG方法主要依赖分类，关注与功能无关的常见类别特定模式；通过选择性原型和像素对比目标，可以在部分和对象级别自适应地学习功能相关线索；通过交叉参考互补视角，可以挖掘精确的部分级功能线索。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明提出的方法是有效的，代码已在github.com/hynnsk/SelectiveCL上公开。&lt;h4&gt;翻译&lt;/h4&gt;促进实体与物体的交互需要准确识别能够支持特定动作的部分。弱监督功能定位(WASG)试图模仿人类从第三方演示中学习的方式，人类无需像素级标注就能直观理解功能部分。为此，通常使用跨图像的共享分类器和结合部分发现过程的蒸馏策略来进行定位学习。然而，由于功能相关部分并不总是容易区分，模型主要依赖分类，往往关注与功能无关的常见类别特定模式。为解决这一局限，我们通过引入选择性原型和像素对比目标，超越了孤立的部分级学习，这些目标能够根据可用信息的粒度，在部分和对象级别自适应地学习功能相关线索。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Facilitating an entity's interaction with objects requires accuratelyidentifying parts that afford specific actions. Weakly supervised affordancegrounding (WSAG) seeks to imitate human learning from third-persondemonstrations, where humans intuitively grasp functional parts without needingpixel-level annotations. To achieve this, grounding is typically learned usinga shared classifier across images from different perspectives, along withdistillation strategies incorporating part discovery process. However, sinceaffordance-relevant parts are not always easily distinguishable, modelsprimarily rely on classification, often focusing on common class-specificpatterns that are unrelated to affordance. To address this limitation, we movebeyond isolated part-level learning by introducing selective prototypical andpixel contrastive objectives that adaptively learn affordance-relevant cues atboth the part and object levels, depending on the granularity of the availableinformation. Initially, we find the action-associated objects in bothegocentric (object-focused) and exocentric (third-person example) images byleveraging CLIP. Then, by cross-referencing the discovered objects ofcomplementary views, we excavate the precise part-level affordance clues ineach perspective. By consistently learning to distinguish affordance-relevantregions from affordance-irrelevant background context, our approach effectivelyshifts activation from irrelevant areas toward meaningful affordance cues.Experimental results demonstrate the effectiveness of our method. Codes areavailable at github.com/hynnsk/SelectiveCL.</description>
      <author>example@mail.com (WonJun Moon, Hyun Seok Seong, Jae-Pil Heo)</author>
      <guid isPermaLink="false">2508.07877v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.07788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ALDEN是一种创新的解剖学感知低剂量CT去噪方法，通过结合预训练视觉模型的语义特征、对抗学习和对比学习，有效解决了现有方法忽略解剖学语义的问题，在去噪的同时保留解剖结构，减少过度平滑问题。&lt;h4&gt;背景&lt;/h4&gt;低剂量CT(LDCT)旨在减少辐射暴露并提高诊断效果，已有许多基于深度学习的去噪方法被开发出来，但大多数现有方法忽略了人体组织的解剖学语义，可能导致次优的去噪效果。&lt;h4&gt;目的&lt;/h4&gt;解决现有LDCT去噪方法忽略解剖学语义的问题，提出一种能够保留解剖学语义的LDCT去噪方法。&lt;h4&gt;方法&lt;/h4&gt;提出ALDEN方法，集成了预训练视觉模型(PVMs)的语义特征，并结合对抗学习和对比学习；引入解剖学感知的判别器，通过交叉注意力机制从参考正常剂量CT动态融合分层语义特征；提出语义引导的对比学习模块，通过对比来自LDCT、去噪CT和NDCT的PVM衍生特征来强制解剖一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在两个LDCT去噪数据集上的实验表明，ALDEN达到了最先进的性能，提供了解剖学保存的优越性，显著减少了先前工作中的过度平滑问题；在包含117个解剖结构的下游多器官分割任务上的验证证实了模型保持解剖意识的能力。&lt;h4&gt;结论&lt;/h4&gt;ALDEN是一种有效的LDCT去噪方法，能够在去噪的同时保留解剖学语义，在多个任务上表现优越，证明了其在临床应用中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;为了减少辐射暴露并提高低剂量计算机断层扫描的诊断效果，已经开发了许多基于深度学习的去噪方法来减轻噪声和伪影。然而，这些方法中的大多数忽略了人体组织的解剖学语义，这可能导致次优的去噪效果。为了解决这个问题，我们提出了ALDEN，一种具有解剖学意识的LDCT去噪方法，它将预训练视觉模型的语义特征与对抗学习和对比学习相结合。具体来说，我们引入了一个具有解剖学意识的判别器，通过交叉注意力机制从参考正常剂量CT动态融合分层语义特征，使判别器能够进行组织特定的真实性评估。此外，我们提出了一个语义引导的对比学习模块，通过对比来自LDCT、去噪CT和NDCT的PVM衍生特征来强制解剖一致性，通过正对保留组织特定模式，并通过双重负对抑制伪影。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决低剂量CT（LDCT）图像去噪的问题。低剂量CT虽然能减少患者辐射暴露，但会产生更多噪声和伪影，降低图像质量，影响医生诊断。传统去噪方法往往忽略人体组织的解剖语义信息，导致过度平滑，掩盖小组织和细微病变特征。解决这个问题对提高诊断准确性、保护患者健康（特别是需要频繁CT检查的患者）以及改善下游任务（如器官分割）的表现都具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有LDCT去噪方法的局限性，特别是它们忽略了解剖语义信息。他们意识到需要从像素级约束转向细粒度的解剖感知去噪。考虑到传统分割网络需要大量精确的解剖标注，他们发现预训练视觉模型（PVMs）具有强大的语义理解能力，可以在不需要明确监督的情况下生成语义特征。该方法借鉴了GAN框架、对抗学习、对比学习以及注意力机制等现有技术，但创新性地将它们与PVMs结合，专门针对医学图像去噪任务进行了优化设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将预训练视觉模型（PVMs）与对抗学习和对比学习相结合，实现解剖感知的低剂量CT去噪。整体流程包括：1) 使用生成器网络（ESAU-Net）将低剂量CT作为输入，输出去噪后的CT图像；2) 通过解剖感知判别器（AAD）融合参考正常剂量CT的语义特征，实现细粒度的纹理恢复；3) 利用语义引导对比学习（SCL）模块，通过正样本对对齐去噪图像与正常剂量CT的解剖特征，同时使用双负样本对抑制噪声和纠正解剖错位；4) 结合像素保真度损失、对抗损失和对比学习损失进行模型优化。最终输出高质量的去噪CT图像，在减少噪声的同时保留重要解剖结构。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次将预训练视觉模型（PVMs）整合到LDCT去噪中；2) 提出解剖感知判别器（AAD），通过交叉注意力机制动态融合层次化语义特征；3) 设计语义引导对比学习（SCL）模块，采用双负样本策略同时处理噪声和解剖错位问题。相比之前的工作，ALDEN最大的不同是明确利用解剖语义信息指导去噪过程，而传统方法通常不考虑这一点。此外，ALDEN能够平衡保真度指标和感知质量，避免过度平滑问题，并在下游任务中表现出更好的解剖感知能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ALDEN通过整合预训练视觉模型与对抗学习和对比学习，实现了对低剂量CT图像的解剖感知去噪，显著提高了图像质量并保留了重要的解剖结构细节。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To reduce radiation exposure and improve the diagnostic efficacy of low-dosecomputed tomography (LDCT), numerous deep learning-based denoising methods havebeen developed to mitigate noise and artifacts. However, most of theseapproaches ignore the anatomical semantics of human tissues, which maypotentially result in suboptimal denoising outcomes. To address this problem,we propose ALDEN, an anatomy-aware LDCT denoising method that integratessemantic features of pretrained vision models (PVMs) with adversarial andcontrastive learning. Specifically, we introduce an anatomy-aware discriminatorthat dynamically fuses hierarchical semantic features from referencenormal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specificrealism evaluation in the discriminator. In addition, we propose asemantic-guided contrastive learning module that enforces anatomicalconsistency by contrasting PVM-derived features from LDCT, denoised CT andNDCT, preserving tissue-specific patterns through positive pairs andsuppressing artifacts via dual negative pairs. Extensive experiments conductedon two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-artperformance, offering superior anatomy preservation and substantially reducingover-smoothing issue of previous work. Further validation on a downstreammulti-organ segmentation task (encompassing 117 anatomical structures) affirmsthe model's ability to maintain anatomical awareness.</description>
      <author>example@mail.com (Runze Wang, Zeli Chen, Zhiyun Song, Wei Fang, Jiajin Zhang, Danyang Tu, Yuxing Tang, Minfeng Xu, Xianghua Ye, Le Lu, Dakai Jin)</author>
      <guid isPermaLink="false">2508.07788v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion</title>
      <link>http://arxiv.org/abs/2508.07755v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025 workshop (AI4CC)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为对比反转(Contrastive Inversion)的新方法，通过比较输入图像而不依赖额外信息来识别共同概念，实现了在概念表示和编辑方面的平衡高性能。&lt;h4&gt;背景&lt;/h4&gt;当前对定制化图像生成的需求增加，需要从小图像集中有效提取共同概念的技术。现有方法通常依赖文本提示或空间掩码等额外指导，但手动提供的指导可能导致辅助特征分离不完整，降低生成质量。&lt;h4&gt;目的&lt;/h4&gt;提出一种不依赖额外信息的方法来识别图像集中的共同概念，通过比较输入图像来提取共同目标概念，提高概念表示和编辑的性能。&lt;h4&gt;方法&lt;/h4&gt;提出对比反转方法，通过对比学习训练目标标记和图像特定的辅助文本标记，提取良好解耦的目标真实语义；然后应用解耦的交叉注意力微调来提高概念保真度，避免过拟合。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果和分析表明，该方法在概念表示和编辑方面实现了平衡的高性能，优于现有技术。&lt;h4&gt;结论&lt;/h4&gt;对比反转方法能够有效提取图像集中的共同概念，不需要额外的指导信息，在概念表示和编辑方面表现优异。&lt;h4&gt;翻译&lt;/h4&gt;最近对定制化图像生成的需求增加，需要从小图像集中有效提取共同概念的技术。现有方法通常依赖额外的指导，如文本提示或空间掩码，来捕获共同目标概念。不幸的是，依赖手动提供的指导可能导致辅助特征分离不完整，从而降低生成质量。在本文中，我们提出了对比反转，一种新颖的方法，通过比较输入图像而不依赖额外信息来识别共同概念。我们通过对比学习训练目标标记以及图像特定的辅助文本标记，提取良好解耦的目标真实语义。然后我们应用解耦的交叉注意力微调来提高概念保真度而不过拟合。实验结果和分析证明，我们的方法在概念表示和编辑方面实现了平衡的高性能，优于现有技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent demand for customized image generation raises a need fortechniques that effectively extract the common concept from small sets ofimages. Existing methods typically rely on additional guidance, such as textprompts or spatial masks, to capture the common target concept. Unfortunately,relying on manually provided guidance can lead to incomplete separation ofauxiliary features, which degrades generation quality.In this paper, we proposeContrastive Inversion, a novel approach that identifies the common concept bycomparing the input images without relying on additional information. We trainthe target token along with the image-wise auxiliary text tokens viacontrastive learning, which extracts the well-disentangled true semantics ofthe target. Then we apply disentangled cross-attention fine-tuning to improveconcept fidelity without overfitting. Experimental results and analysisdemonstrate that our method achieves a balanced, high-level performance in bothconcept representation and editing, outperforming existing techniques.</description>
      <author>example@mail.com (Minseo Kim, Minchan Kwon, Dongyeun Lee, Yunho Jeon, Junmo Kim)</author>
      <guid isPermaLink="false">2508.07755v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Symmetry-Aware Transformer Training for Automated Planning</title>
      <link>http://arxiv.org/abs/2508.07743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种对称性感知的transformer训练方法，解决了transformer在自动化规划领域应用中的外推问题，特别是在处理从简单到困难规划任务时的局限性。&lt;h4&gt;背景&lt;/h4&gt;Transformer模型在许多领域表现出色，但在自动化规划领域的应用有限。先前的工作如PlanGPT（一种最先进的仅解码器transformer）在从简单到困难的规划问题外推方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的对比学习目标，使transformer具有对称性感知能力，从而弥补其归纳偏置的不足；结合架构改进，展示transformer可以针对规划生成或启发式预测进行高效训练。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的对比学习目标，使transformer能够识别和处理规划任务中的对称性问题；结合架构改进，对transformer进行对称性感知训练。&lt;h4&gt;主要发现&lt;/h4&gt;在多个规划领域的结果表明，对称性感知训练有效且高效地解决了PlanGPT的局限性，使transformer能够更好地处理规划任务。&lt;h4&gt;结论&lt;/h4&gt;对称性感知训练使transformer能够有效应对规划任务中的对称性问题，解决了从简单到困难规划任务的外推挑战。&lt;h4&gt;翻译&lt;/h4&gt;尽管Transformer在许多场景中表现出色，它们在自动化规划领域的应用却有限。像PlanGPT这样的先前工作（一种最先进的仅解码器Transformer）在从简单到困难的规划问题外推方面存在困难。这一问题源于问题的对称性：规划任务可以用任意变量名表示，这些变量名除了作为标识符外没有其他含义。这导致了等价表示的组合爆炸，纯Transformer无法从中高效学习。我们提出了一种新颖的对比学习目标，使Transformer具有对称性感知能力，从而弥补其归纳偏置的不足。结合架构改进，我们展示Transformer可以针对规划生成或启发式预测进行高效训练。我们在多个规划领域的结果表明，我们的对称性感知训练有效且高效地解决了PlanGPT的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While transformers excel in many settings, their application in the field ofautomated planning is limited. Prior work like PlanGPT, a state-of-the-artdecoder-only transformer, struggles with extrapolation from easy to hardplanning problems. This in turn stems from problem symmetries: planning taskscan be represented with arbitrary variable names that carry no meaning beyondbeing identifiers. This causes a combinatorial explosion of equivalentrepresentations that pure transformers cannot efficiently learn from. Wepropose a novel contrastive learning objective to make transformerssymmetry-aware and thereby compensate for their lack of inductive bias.Combining this with architectural improvements, we show that transformers canbe efficiently trained for either plan-generation or heuristic-prediction. Ourresults across multiple planning domains demonstrate that our symmetry-awaretraining effectively and efficiently addresses the limitations of PlanGPT.</description>
      <author>example@mail.com (Markus Fritzsche, Elliot Gestrin, Jendrik Seipp)</author>
      <guid isPermaLink="false">2508.07743v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning</title>
      <link>http://arxiv.org/abs/2508.07607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/OPPO-Mente-Lab/X2Edit&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了X2Edit数据集和基于FLUX.1的任务感知MoE-LoRA训练方法，用于改进任意指令图像编辑性能。&lt;h4&gt;背景&lt;/h4&gt;现有的开源数据集在任意指令图像编辑方面表现不佳，同时缺乏与社区主流生成模型兼容的即插即用编辑模块。&lt;h4&gt;目的&lt;/h4&gt;引入覆盖14种不同编辑任务的X2Edit数据集，并设计能够与社区图像生成模型无缝集成的任务感知MoE-LoRA训练方法。&lt;h4&gt;方法&lt;/h4&gt;使用行业领先的综合图像生成模型和专家模型构建数据，利用VLM设计编辑指令并实施评分机制过滤数据；基于FLUX.1设计任务感知的MoE-LoRA训练（仅使用全模型8%参数），并利用扩散模型内部表示引入对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;构建了370万高质量且类别均衡的数据集；模型编辑性能在众多优秀模型中具有竞争力；构建的数据集比现有开源数据集具有显著优势。&lt;h4&gt;结论&lt;/h4&gt;X2Edit项目包括开源代码、检查点和数据集，已在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;现有的任意指令图像编辑开源数据集仍不理想，而与社区主流生成模型兼容的即插即用编辑模块明显缺失。本文首先介绍了X2Edit数据集，这是一个涵盖14种不同编辑任务的综合性数据集，包括主体驱动的生成。我们使用行业领先的综合图像生成模型和专家模型来构建数据。同时，我们利用VLM设计合理的编辑指令，并实施各种评分机制来过滤数据。因此，我们构建了370万高质量且类别均衡的数据。其次，为了更好地与社区图像生成模型无缝集成，我们基于FLUX.1设计了任务感知的MoE-LoRA训练，仅使用全模型8%的参数。为了进一步提高最终性能，我们利用扩散模型的内部表示，基于图像编辑类型定义正/负样本来引入对比学习。大量实验表明，该模型的编辑性能在众多优秀模型中具有竞争力。此外，构建的数据集比现有的开源数据集具有显著优势。X2Edit的开源代码、检查点和数据集可在以下链接找到：https://github.com/OPPO-Mente-Lab/X2Edit。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing open-source datasets for arbitrary-instruction image editing remainsuboptimal, while a plug-and-play editing module compatible withcommunity-prevalent generative models is notably absent. In this paper, wefirst introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverseediting tasks, including subject-driven generation. We utilize theindustry-leading unified image generation models and expert models to constructthe data. Meanwhile, we design reasonable editing instructions with the VLM andimplement various scoring mechanisms to filter the data. As a result, weconstruct 3.7 million high-quality data with balanced categories. Second, tobetter integrate seamlessly with community image generation models, we designtask-aware MoE-LoRA training based on FLUX.1, with only 8\% of the parametersof the full model. To further improve the final performance, we utilize theinternal representations of the diffusion model and define positive/negativesamples based on image editing types to introduce contrastive learning.Extensive experiments demonstrate that the model's editing performance iscompetitive among many excellent models. Additionally, the constructed datasetexhibits substantial advantages over existing open-source datasets. Theopen-source code, checkpoints, and datasets for X2Edit can be found at thefollowing link: https://github.com/OPPO-Mente-Lab/X2Edit.</description>
      <author>example@mail.com (Jian Ma, Xujie Zhu, Zihao Pan, Qirong Peng, Xu Guo, Chen Chen, Haonan Lu)</author>
      <guid isPermaLink="false">2508.07607v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.07539v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种处理病理图像中域偏移的方法，专注于整张幻灯片图像(WSIs)内部的偏移，如患者特征和组织厚度，而非医院间的偏移。&lt;h4&gt;背景&lt;/h4&gt;传统方法依赖多医院数据进行域适应，但数据收集的挑战使这种方法不切实际。&lt;h4&gt;目的&lt;/h4&gt;开发一种域泛化方法，能够捕获和利用医院内部的域偏移，解决病理图像分析中的域适应问题。&lt;h4&gt;方法&lt;/h4&gt;通过聚类来自非肿瘤区域的WSI级别特征并将这些聚类视为域，应用对比学习来减少不同聚类WSI对之间的特征差距。提出两阶段对比学习方法，包括WSI级别和补丁级别对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;通过捕获WSI内部的域偏移特征并应用对比学习，可以有效减少不同域之间的特征差距，提高病理图像分析的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法不需要多医院数据，仅通过医院内部的域偏移信息就能实现有效的域泛化，解决了传统方法中数据收集的挑战。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们通过关注整张幻灯片图像(WSIs)内部的偏移（如患者特征和组织厚度）而非医院之间的偏移，来解决病理图像中的域偏移问题。传统方法依赖多医院数据，但数据收集的挑战常常使这种方法不切实际。因此，所提出的域泛化方法通过聚类来自非肿瘤区域的WSI级别特征并将这些聚类视为域，来捕获和利用医院内部的域偏移。为了减轻域偏移，我们应用对比学习来减少不同聚类WSI对之间的特征差距。所提出的方法引入了两阶段对比学习方法，包括WSI级别和补丁级别对比学习，以有效减少这些差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we address domain shifts in pathological images by focusing onshifts within whole slide images~(WSIs), such as patient characteristics andtissue thickness, rather than shifts between hospitals. Traditional approachesrely on multi-hospital data, but data collection challenges often make thisimpractical. Therefore, the proposed domain generalization method captures andleverages intra-hospital domain shifts by clustering WSI-level features fromnon-tumor regions and treating these clusters as domains. To mitigate domainshift, we apply contrastive learning to reduce feature gaps between WSI pairsfrom different clusters. The proposed method introduces a two-stage contrastivelearning approach WSI-level and patch-level contrastive learning to minimizethese gaps effectively.</description>
      <author>example@mail.com (Yuki Shigeyasu, Shota Harada, Akihiko Yoshizawa, Kazuhiro Terada, Naoki Nakazima, Mariyo Kurata, Hiroyuki Abe, Tetsuo Ushiku, Ryoma Bise)</author>
      <guid isPermaLink="false">2508.07539v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.07205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by COLING2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对谣言检测中的数据稀缺和不平衡问题，提出了基于图监督对比学习的异常检测框架AD-GSCL，并通过大规模数据集验证了其在各种条件下的优越性。&lt;h4&gt;背景&lt;/h4&gt;当前基于传播结构学习的谣言检测方法主要将谣言检测视为有限标记数据上的平衡分类任务，但真实社交媒体数据呈现出不平衡分布，谣言只是海量常规帖子中的少数。&lt;h4&gt;目的&lt;/h4&gt;解决谣言检测中的数据稀缺和不平衡问题，提高真实不平衡数据分布下的谣言检测效果。&lt;h4&gt;方法&lt;/h4&gt;构建了两个来自微博和Twitter的大规模对话数据集，分析了领域分布差异，提出了AD-GSCL框架，启发式地将未标记数据视为非谣言，并适应图对比学习用于谣言检测。&lt;h4&gt;主要发现&lt;/h4&gt;谣言和非谣言分布有明显差异，非谣言多集中在娱乐领域，而谣言则集中在新闻领域，表明谣言检测符合异常检测范式。&lt;h4&gt;结论&lt;/h4&gt;AD-GSCL框架在平衡、不平衡和少样本条件下均表现出优越性，为具有不平衡数据分布的真实世界谣言检测提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;当前基于传播结构学习的谣言检测方法主要将谣言检测视为有限标记数据上的平衡分类任务。然而，真实社交媒体数据呈现出不平衡分布，谣言只是海量常规帖子中的少数。为解决数据稀缺和不平衡问题，我们从微博和Twitter构建了两个大规模对话数据集并分析了领域分布。我们发现谣言和非谣言分布有明显差异，非谣言多集中在娱乐领域而谣言集中在新闻领域，表明谣言检测符合异常检测范式。相应地，我们提出了带图监督对比学习的异常检测框架AD-GSCL。它启发式地将未标记数据视为非谣言并适应图对比学习用于谣言检测。大量实验证明了AD-GSCL在平衡、不平衡和少样本条件下的优越性。我们的发现为具有不平衡数据分布的真实世界谣言检测提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current rumor detection methods based on propagation structure learningpredominately treat rumor detection as a class-balanced classification task onlimited labeled data. However, real-world social media data exhibits animbalanced distribution with a minority of rumors among massive regular posts.To address the data scarcity and imbalance issues, we construct two large-scaleconversation datasets from Weibo and Twitter and analyze the domaindistributions. We find obvious differences between rumor and non-rumordistributions, with non-rumors mostly in entertainment domains while rumorsconcentrate in news, indicating the conformity of rumor detection to an anomalydetection paradigm. Correspondingly, we propose the Anomaly Detection frameworkwith Graph Supervised Contrastive Learning (AD-GSCL). It heuristically treatsunlabeled data as non-rumors and adapts graph contrastive learning for rumordetection. Extensive experiments demonstrate AD-GSCL's superiority underclass-balanced, imbalanced, and few-shot conditions. Our findings providevaluable insights for real-world rumor detection featuring imbalanced datadistributions.</description>
      <author>example@mail.com (Chaoqun Cui, Caiyan Jia)</author>
      <guid isPermaLink="false">2508.07205v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection</title>
      <link>http://arxiv.org/abs/2508.07201v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by AAAI2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为谣言自适应图对比学习(RAGCL)的方法，用于社交媒体谣言检测。通过分析真实数据集，作者发现谣言传播树(RPTs)实际上呈现宽结构而非深结构，大多数节点是浅层1级回复。基于这一发现，RAGCL方法采用基于节点中心性的自适应视图增强，遵循三条原则：免除根节点、保留深层回复节点和保留深层部分的低级节点。通过节点丢弃、属性掩码和边丢弃技术生成视图，利用图对比目标学习鲁棒的谣言表示。实验证明该方法在四个基准数据集上优于现有最先进方法。&lt;h4&gt;背景&lt;/h4&gt;社交媒体上的谣言检测变得越来越重要。大多数现有的基于图的模型假设谣言传播树(RPTs)具有深层结构，并沿着分支学习序列立场特征。&lt;h4&gt;目的&lt;/h4&gt;为了专注于密集子结构的学习，提出谣言自适应图对比学习(RAGCL)方法，通过基于节点中心性的自适应视图增强来提高谣言检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出RAGCL方法，包含三条RPT增强原则：1)免除根节点，2)保留深层回复节点，3)保留深层部分的低级节点。使用基于中心性重要性分数的概率进行节点丢弃、属性掩码和边丢弃来生成视图，并通过图对比目标学习鲁棒的谣言表示。&lt;h4&gt;主要发现&lt;/h4&gt;通过真实世界数据集的统计分析发现，谣言传播树(RPTs)实际上呈现宽结构，大多数节点是浅层1级回复，而非传统模型假设的深层结构。&lt;h4&gt;结论&lt;/h4&gt;作者的工作揭示了RPTs的宽结构特性，并通过原则性的自适应增强为谣言检测贡献了一种有效的图对比学习方法。所提出的原则和增强技术可能有益于涉及树状图的其他应用。&lt;h4&gt;翻译&lt;/h4&gt;社交媒体上的谣言检测变得越来越重要。大多数现有的基于图的模型假设谣言传播树(RPTs)具有深层结构，并沿着分支学习序列立场特征。然而，通过对真实世界数据集的统计分析，我们发现RPTs呈现宽结构，大多数节点是浅层1级回复。为了专注于密集子结构的学习，我们提出了谣言自适应图对比学习(RAGCL)方法，该方法由基于节点中心性的自适应视图增强引导。我们总结了RPT增强的三条原则：1)免除根节点，2)保留深层回复节点，3)保留深层部分的低级节点。我们使用基于中心性重要性分数的概率进行节点丢弃、属性掩码和边丢弃来生成视图。然后，图对比目标学习鲁棒的谣言表示。在四个基准数据集上的广泛实验表明，RAGCL优于最先进的方法。我们的工作揭示了RPTs的宽结构特性，并通过原则性的自适应增强为谣言检测贡献了一种有效的图对比学习方法。所提出的原则和增强技术可能有益于涉及树状图的其他应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1609/aaai.v38i1.27757&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rumor detection on social media has become increasingly important. Mostexisting graph-based models presume rumor propagation trees (RPTs) have deepstructures and learn sequential stance features along branches. However,through statistical analysis on real-world datasets, we find RPTs exhibit widestructures, with most nodes being shallow 1-level replies. To focus learning onintensive substructures, we propose Rumor Adaptive Graph Contrastive Learning(RAGCL) method with adaptive view augmentation guided by node centralities. Wesummarize three principles for RPT augmentation: 1) exempt root nodes, 2)retain deep reply nodes, 3) preserve lower-level nodes in deep sections. Weemploy node dropping, attribute masking and edge dropping with probabilitiesfrom centrality-based importance scores to generate views. A graph contrastiveobjective then learns robust rumor representations. Extensive experiments onfour benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods.Our work reveals the wide-structure nature of RPTs and contributes an effectivegraph contrastive learning approach tailored for rumor detection throughprincipled adaptive augmentation. The proposed principles and augmentationtechniques can potentially benefit other applications involving tree-structuredgraphs.</description>
      <author>example@mail.com (Chaoqun Cui, Caiyan Jia)</author>
      <guid isPermaLink="false">2508.07201v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations</title>
      <link>http://arxiv.org/abs/2508.07016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于时间滞后交叉相关性的序列预测框架（TLCCSP），通过整合时间滞后交叉相关序列提高预测准确性。该框架使用序列移动动态时间规整（SSDTW）算法捕捉滞后相关性，并采用基于对比学习的编码器（CLE）近似SSDTW距离。实验证明该方法在多个数据集上显著降低了预测误差，同时大幅减少了计算时间。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测在天气、金融和房地产等领域至关重要，准确的预测支持决策制定和风险缓解。虽然最近的深度学习模型提高了预测能力，但它们常常忽略了相关序列之间的时间滞后交叉相关性，这对于捕捉复杂的时间关系至关重要。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有模型忽略时间滞后交叉相关性的问题，作者提出了TLCCSP框架，通过有效整合时间滞后交叉相关序列来提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;TLCCSP框架采用序列移动动态时间规整（SSDTW）算法来捕捉滞后相关性，并使用基于对比学习的编码器（CLE）来有效近似SSDTW距离。&lt;h4&gt;主要发现&lt;/h4&gt;在天气数据集上，SSDTW降低MSE 16.01%，CLE进一步降低17.88%；在股票数据集上，SSDTW降低MSE 9.95%，CLE降低6.13%；在房地产数据集上，SSDTW降低MSE 21.29%，CLE降低8.62%。此外，对比学习方法将SSDTW计算时间减少约99%，提高了框架的扩展性和实时适用性。&lt;h4&gt;结论&lt;/h4&gt;TLCCSP框架在天气、金融和房地产时间序列数据集上的实验结果证明了其有效性，能够显著提高预测准确性并减少计算时间。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测在天气、金融和房地产预测等各个领域至关重要，因为准确的预测支持明智的决策制定和风险缓解。虽然最近的深度学习模型提高了预测能力，但它们常常忽略了相关序列之间时间滞后交叉相关性，这对于捕捉复杂的时间关系至关重要。为此，我们提出了基于时间滞后交叉相关性的序列预测框架（TLCCSP），通过有效整合时间滞后交叉相关序列来提高预测准确性。TLCCSP采用序列移动动态时间规整（SSDTW）算法来捕捉滞后相关性，并使用基于对比学习的编码器来有效近似SSDTW距离。在天气、金融和房地产时间序列数据集上的实验结果证明了我们框架的有效性。在天气数据集上，SSDTW与单序列方法相比均方误差降低了16.01%，而对比学习编码器进一步将均方误差降低了17.88%。在股票数据集上，SSDTW实现了9.95%的均方误差降低，对比学习编码器将其降低了6.13%。对于房地产数据集，SSDTW和对比学习编码器分别将均方误差降低了21.29%和8.62%。此外，对比学习方法将SSDTW计算时间减少了约99%，确保了跨多个时间序列预测任务的扩展性和实时适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting is critical across various domains, such as weather,finance and real estate forecasting, as accurate forecasts support informeddecision-making and risk mitigation. While recent deep learning models haveimproved predictive capabilities, they often overlook time-laggedcross-correlations between related sequences, which are crucial for capturingcomplex temporal relationships. To address this, we propose the Time-LaggedCross-Correlations-based Sequence Prediction framework (TLCCSP), which enhancesforecasting accuracy by effectively integrating time-lagged cross-correlatedsequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW)algorithm to capture lagged correlations and a contrastive learning-basedencoder to efficiently approximate SSDTW distances.  Experimental results on weather, finance and real estate time series datasetsdemonstrate the effectiveness of our framework. On the weather dataset, SSDTWreduces mean squared error (MSE) by 16.01% compared with single-sequencemethods, while the contrastive learning encoder (CLE) further decreases MSE by17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLEreduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by21.29% and 8.62%, respectively. Additionally, the contrastive learning approachdecreases SSDTW computational time by approximately 99%, ensuring scalabilityand real-time applicability across multiple time series forecasting tasks.</description>
      <author>example@mail.com (Jianfei Wu, Wenmian Yang, Bingning Liu, Weijia Jia)</author>
      <guid isPermaLink="false">2508.07016v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>BiXSE: Improving Dense Retrieval via Probabilistic Graded Relevance Distillation</title>
      <link>http://arxiv.org/abs/2508.06781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 5 figures, accepted at COLM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BiXSE的简单有效的点式训练方法，用于优化基于大型语言模型生成的分级相关性分数的二进制交叉熵，实现了在密集检索任务上的优异性能。&lt;h4&gt;背景&lt;/h4&gt;神经句子嵌入模型通常使用二元相关性标签，将查询-文档对简单分类为相关或不相关。然而，现实世界中的相关性实际上是一个连续谱系，而非简单的二元分类。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够利用细粒度分级相关性标签的训练方法，以提高密集检索模型的性能，同时减少标注和计算成本。&lt;h4&gt;方法&lt;/h4&gt;BiXSE方法将大型语言模型生成的分级相关性分数解释为概率目标，使得每个查询的单个标记查询-文档对就能提供细粒度的监督。通过利用批次内负样本，BiXSE实现了性能提升并降低了计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试(MMTEB, BEIR, TREC-DL)上，BiXSE始终优于基于softmax的对比学习方法(InfoNCE)，并且在LLM监督数据上训练时，能够匹配或超过强成的对排序基线性能。&lt;h4&gt;结论&lt;/h4&gt;BiXSE为训练密集检索模型提供了一种强大、可扩展的替代方案，随着分级相关性监督变得越来越容易获取，这种方法将变得更加实用。&lt;h4&gt;翻译&lt;/h4&gt;用于密集检索的神经句子嵌入模型通常依赖于二元相关性标签，将查询-文档对视为相关或不相关。然而，现实世界中的相关性往往是一个连续谱系，大型语言模型(LLMs)的最新进展使得生成细粒度分级相关性标签的扩展成为可能。在这项工作中，我们提出了BiXSE，这是一种简单有效的点式训练方法，可优化基于LLM生成的分级相关性分数的二进制交叉熵(BCE)。BiXSE将这些分数解释为概率目标，使得每个查询的单个标记查询-文档对能够提供细粒度的监督。与需要每个查询多个注释比较的成对或列表损失不同，BiXSE通过利用批次内负样本，在减少注释和计算成本的同时实现了强大的性能。在句子嵌入(MMTEB)和检索基准(BEIR, TREC-DL)上的大量实验表明，BiXSE始终优于基于softmax的对比学习(InfoNCE)，并且在LLM监督数据上训练时，匹配或超过了强成的对排序基线。随着分级相关性监督变得越来越容易获取，BiXSE为训练密集检索模型提供了一种强大、可扩展的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural sentence embedding models for dense retrieval typically rely on binaryrelevance labels, treating query-document pairs as either relevant orirrelevant. However, real-world relevance often exists on a continuum, andrecent advances in large language models (LLMs) have made it feasible to scalethe generation of fine-grained graded relevance labels. In this work, wepropose BiXSE, a simple and effective pointwise training method that optimizesbinary cross-entropy (BCE) over LLM-generated graded relevance scores. BiXSEinterprets these scores as probabilistic targets, enabling granular supervisionfrom a single labeled query-document pair per query. Unlike pairwise orlistwise losses that require multiple annotated comparisons per query, BiXSEachieves strong performance with reduced annotation and compute costs byleveraging in-batch negatives. Extensive experiments across sentence embedding(MMTEB) and retrieval benchmarks (BEIR, TREC-DL) show that BiXSE consistentlyoutperforms softmax-based contrastive learning (InfoNCE), and matches orexceeds strong pairwise ranking baselines when trained on LLM-supervised data.BiXSE offers a robust, scalable alternative for training dense retrieval modelsas graded relevance supervision becomes increasingly accessible.</description>
      <author>example@mail.com (Christos Tsirigotis, Vaibhav Adlakha, Joao Monteiro, Aaron Courville, Perouz Taslakian)</author>
      <guid isPermaLink="false">2508.06781v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations</title>
      <link>http://arxiv.org/abs/2508.05097v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为'MultiCheck'的细粒度多模态事实验证统一框架，用于应对多模态虚假信息的挑战&lt;h4&gt;背景&lt;/h4&gt;多模态虚假信息（文本和图像结合的虚假信息）的增长率很高，这对主要依赖文本证据的事实核查系统构成了重大挑战&lt;h4&gt;目的&lt;/h4&gt;设计一个能够对结构化文本和视觉信号进行推理的事实验证框架&lt;h4&gt;方法&lt;/h4&gt;架构结合文本和图像的专用编码器，以及使用元素级交互捕获跨模态关系的融合模块，分类头预测主张真实性，并通过对比学习目标在共享潜在空间中实现主张-证据对的语义对齐&lt;h4&gt;主要发现&lt;/h4&gt;在Factify 2数据集上评估，获得0.84的加权F1分数，显著优于基线&lt;h4&gt;结论&lt;/h4&gt;显式多模态推理有效，该方法在复杂现实场景中具有可扩展和可解释的事实核查潜力&lt;h4&gt;翻译&lt;/h4&gt;多模态虚假信息（主张由文本和图像共同支持）的增长率日益提高，这对主要依赖文本证据的事实核查系统构成了重大挑战。在这项工作中，我们提出了一个名为'MultiCheck'的细粒度多模态事实验证统一框架，旨在对结构化的文本和视觉信号进行推理。我们的架构结合了文本和图像的专用编码器，以及一个使用元素级交互捕获跨模态关系的融合模块。然后，分类头预测主张的真实性，并通过对比学习目标在共享潜在空间中鼓励主张-证据对的语义对齐。我们在Factify 2数据集上评估了我们的方法，获得了0.84的加权F1分数，显著优于基线。这些结果突显了显式多模态推理的有效性，并展示了我们的方法在复杂现实场景中可扩展和可解释的事实核查潜力&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing rate of multimodal misinformation, where claims are supported byboth text and images, poses significant challenges to fact-checking systemsthat rely primarily on textual evidence. In this work, we have proposed aunified framework for fine-grained multimodal fact verification called"MultiCheck", designed to reason over structured textual and visual signals.Our architecture combines dedicated encoders for text and images with a fusionmodule that captures cross-modal relationships using element-wise interactions.A classification head then predicts the veracity of a claim, supported by acontrastive learning objective that encourages semantic alignment betweenclaim-evidence pairs in a shared latent space. We evaluate our approach on theFactify 2 dataset, achieving a weighted F1 score of 0.84, substantiallyoutperforming the baseline. These results highlight the effectiveness ofexplicit multimodal reasoning and demonstrate the potential of our approach forscalable and interpretable fact-checking in complex, real-world scenarios.</description>
      <author>example@mail.com (Aditya Kishore, Gaurav Kumar, Jasabanta Patro)</author>
      <guid isPermaLink="false">2508.05097v2</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos</title>
      <link>http://arxiv.org/abs/2508.06570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in ACL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个新的视频数据集ImpliHateVid和一个两阶段对比学习框架，用于检测视频中的隐式仇恨言论。&lt;h4&gt;背景&lt;/h4&gt;现有研究主要关注基于文本和图像的仇恨言论检测，而基于视频的方法仍然探索不足。&lt;h4&gt;目的&lt;/h4&gt;创建一个专门用于检测视频中隐式仇恨言论的数据集，并提出一个有效的多模态对比学习框架来检测视频中的仇恨内容。&lt;h4&gt;方法&lt;/h4&gt;1. 创建了包含2009个视频的数据集ImpliHateVid，包括509个隐式仇恨视频、500个显式仇恨视频和1000个非仇恨视频。2. 提出一个两阶段对比学习框架：第一阶段使用对比损失训练音频、文本和图像的模态特定编码器；第二阶段使用对比学习训练交叉编码器来优化多模态表示。3. 整合情感、情绪和基于标题的特征来增强隐式仇恨检测。&lt;h4&gt;主要发现&lt;/h4&gt;提出的多模态对比学习方法在两个数据集（ImpliHateVid和HateMM）上证明了其在检测视频中的仇恨内容方面的有效性，并且他们的数据集具有重要意义。&lt;h4&gt;结论&lt;/h4&gt;论文提出的新数据集和两阶段对比学习框架对视频中的隐式仇恨言论检测有重要贡献。&lt;h4&gt;翻译&lt;/h4&gt;现有研究主要关注基于文本和图像的仇恨言论检测，而基于视频的方法仍然探索不足。在这项工作中，我们引入了一个新的数据集ImpliHateVid，专门为视频中隐式仇恨言论检测而策划。ImpliHateVid包含2009个视频，包括509个隐式仇恨视频、500个显式仇恨视频和1000个非仇恨视频，使其成为首批专门用于隐式仇恨检测的大规模视频数据集之一。我们还提出了一个新颖的两阶段对比学习框架，用于检测视频中的仇恨言论。在第一阶段，我们通过连接三个编码器的特征，使用对比损失训练音频、文本和图像的模态特定编码器。在第二阶段，我们使用对比学习训练交叉编码器，以优化多模态表示。此外，我们整合了情感、情绪和基于标题的特征，以增强隐式仇恨检测。我们在两个数据集上评估了我们的方法：用于隐式仇恨言论检测的ImpliHateVid和用于视频中一般仇恨言论检测的另一个数据集HateMM，证明了提出的多模态对比学习在检测视频中仇恨内容方面的有效性以及我们数据集的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.18653/v1/2025.acl-long.842&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The existing research has primarily focused on text and image-based hatespeech detection, video-based approaches remain underexplored. In this work, weintroduce a novel dataset, ImpliHateVid, specifically curated for implicit hatespeech detection in videos. ImpliHateVid consists of 2,009 videos comprising509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,making it one of the first large-scale video datasets dedicated to implicithate detection. We also propose a novel two-stage contrastive learningframework for hate speech detection in videos. In the first stage, we trainmodality-specific encoders for audio, text, and image using contrastive loss byconcatenating features from the three encoders. In the second stage, we traincross-encoders using contrastive learning to refine multimodal representations.Additionally, we incorporate sentiment, emotion, and caption-based features toenhance implicit hate detection. We evaluate our method on two datasets,ImpliHateVid for implicit hate speech detection and another dataset for generalhate speech detection in videos, HateMM dataset, demonstrating theeffectiveness of the proposed multimodal contrastive learning for hatefulcontent detection in videos and the significance of our dataset.</description>
      <author>example@mail.com (Mohammad Zia Ur Rehman, Anukriti Bhatnagar, Omkar Kabde, Shubhi Bansal, Nagendra Kumar)</author>
      <guid isPermaLink="false">2508.06570v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>SAGOnline: Segment Any Gaussians Online</title>
      <link>http://arxiv.org/abs/2508.08219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了SAGOnline框架，一种轻量级的零样本方法，用于高斯场景的实时3D分割。通过结合视频基础模型和GPU加速的3D掩码生成算法，解决了当前3D分割方法计算成本高、3D空间推理有限以及无法同时跟踪多个对象的问题。&lt;h4&gt;背景&lt;/h4&gt;3D高斯溅射已成为显式3D场景表示的强大范式，但实现高效且一致的3D分割仍然具有挑战性。当前方法存在 prohibitive 计算成本、有限的3D空间推理能力以及无法同时跟踪多个对象的局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一个轻量级且零样本的框架，用于高斯场景的实时3D分割，解决现有方法的计算成本高、3D空间推理有限以及无法同时跟踪多个对象的问题。&lt;h4&gt;方法&lt;/h4&gt;SAGOnline框架包含两个关键创新：(1)解耦策略，集成了视频基础模型(如SAM2)用于在合成视图间进行视图一致的2D掩码传播；(2)GPU加速的3D掩码生成和高斯级实例标记算法，为3D基元分配唯一标识符，实现跨视图的无损多目标跟踪和分割。&lt;h4&gt;主要发现&lt;/h4&gt;SAGOnline在NVOS(92.7% mIoU)和Spin-NeRF(95.2% mIoU)基准测试上实现了最先进的性能，推理速度比Feature3DGS、OmniSeg3D-gs和SA3D快15-1500倍(每帧27毫秒)。定性结果表明在复杂场景中具有强大的多目标分割和跟踪能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作实现了实时渲染和3D场景理解，为实际AR/VR和机器人应用铺平了道路。贡献包括：(i)高斯场景中3D分割的轻量级零样本框架；(ii)高斯基元的显式标记，实现同时分割和跟踪；(iii)2D视频基础模型向3D领域的有效适应。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯溅射已成为显式3D场景表示的强大范式，但实现高效且一致的3D分割仍然具有挑战性。当前方法存在 prohibitive 计算成本、有限的3D空间推理能力以及无法同时跟踪多个对象的局限性。我们提出了'在线分割任意高斯'(SAGOnline)，这是一个轻量级的零样本框架，用于高斯场景的实时3D分割，通过两个关键创新解决了这些限制：(1)解耦策略，集成了视频基础模型(如SAM2)用于在合成视图间进行视图一致的2D掩码传播；(2)GPU加速的3D掩码生成和高斯级实例标记算法，为3D基元分配唯一标识符，实现跨视图的无损多目标跟踪和分割。SAGOnline在NVOS(92.7% mIoU)和Spin-NeRF(95.2% mIoU)基准测试上实现了最先进的性能，推理速度比Feature3DGS、OmniSeg3D-gs和SA3D快15-1500倍(每帧27毫秒)。定性结果表明在复杂场景中具有强大的多目标分割和跟踪能力。我们的贡献包括：(i)高斯场景中3D分割的轻量级零样本框架；(ii)高斯基元的显式标记，实现同时分割和跟踪；(iii)2D视频基础模型向3D领域的有效适应。这项工作实现了实时渲染和3D场景理解，为实际AR/VR和机器人应用铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D高斯场景中的高效一致3D分割问题。当前方法面临三个关键挑战：计算成本过高、难以获取全局3D分割结果、无法同时进行多目标分割和跟踪。这个问题在AR/VR应用、机器人操作和自主系统中至关重要，因为这些应用需要实时3D场景解析，而现有方法要么计算代价太大，要么无法处理多目标场景，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前3D分割方法的局限性，包括基于蒸馏、对比学习和直接操作的方法的不足。他们提出将3D分割任务解耦为一致的2D分割和3D高斯级分割两个阶段，利用视频基础模型进行跨视角一致的2D掩码传播，并设计GPU加速的3D掩码生成算法。该方法借鉴了3D高斯溅射的显式表示方法、SAM 2作为视频基础模型、逆光栅化技术以及类似U-Net的掩码细化网络架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D分割解耦为两个互补阶段：利用视频基础模型进行跨视角一致的2D掩码传播，以及通过GPU加速的逆投影投票算法将2D掩码转换为3D高斯标签。整体流程分为两个阶段：第一阶段(预热初始化)使用SAM 2生成跨视角一致的2D掩码，通过逆投影投票为高斯基元分配实例标签；第二阶段(加速分割)利用分割后的3D高斯表示实时渲染新视图，并通过轻量级后处理网络优化分割结果，保持实时性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：轻量级零样本框架实现无需训练的3D分割；显式的高斯基元标记支持同时分割和跟踪；有效将2D视频基础模型适应到3D领域。相比之前工作，SAGOnline计算效率更高(比SA3D快15-1500倍，27ms/帧)，支持多目标处理(为每个高斯基元分配唯一标识符)，能生成全局3D掩码而非仅2D掩码，并实现实时性能(37 FPS)，同时零训练时间而其他方法需要20-40分钟额外训练。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAGOnline提出了一种轻量级零样本框架，通过解耦2D视频基础模型与3D高斯分割，实现了实时、多视角一致的3D场景分割与多目标跟踪，显著提高了计算效率并支持实际AR/VR和机器人应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit3D scene representation, yet achieving efficient and consistent 3D segmentationremains challenging. Current methods suffer from prohibitive computationalcosts, limited 3D spatial reasoning, and an inability to track multiple objectssimultaneously. We present Segment Any Gaussians Online (SAGOnline), alightweight and zero-shot framework for real-time 3D segmentation in Gaussianscenes that addresses these limitations through two key innovations: (1) adecoupled strategy that integrates video foundation models (e.g., SAM2) forview-consistent 2D mask propagation across synthesized views; and (2) aGPU-accelerated 3D mask generation and Gaussian-level instance labelingalgorithm that assigns unique identifiers to 3D primitives, enabling losslessmulti-object tracking and segmentation across views. SAGOnline achievesstate-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 timesin inference speed (27 ms/frame). Qualitative results demonstrate robustmulti-object segmentation and tracking in complex scenes. Our contributionsinclude: (i) a lightweight and zero-shot framework for 3D segmentation inGaussian scenes, (ii) explicit labeling of Gaussian primitives enablingsimultaneous segmentation and tracking, and (iii) the effective adaptation of2D video foundation models to the 3D domain. This work allows real-timerendering and 3D scene understanding, paving the way for practical AR/VR androbotic applications.</description>
      <author>example@mail.com (Wentao Sun, Quanyun Wu, Hanqing Xu, Kyle Gao, Zhengsen Xu, Yiping Chen, Dedong Zhang, Lingfei Ma, John S. Zelek, Jonathan Li)</author>
      <guid isPermaLink="false">2508.08219v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking</title>
      <link>http://arxiv.org/abs/2508.07968v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Full Research Paper, presented at MICCAI'25 Workshop on Collaborative  Intelligence and Autonomy in Image-guided Surgery&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TrackOR框架，用于手术室中长期多人跟踪和重新识别，利用3D几何特征实现高性能跟踪，支持离线恢复过程，为手术团队提供个性化智能支持。&lt;h4&gt;背景&lt;/h4&gt;为手术团队提供智能支持是自动化手术场景理解的关键前沿，长期目标是改善患者预后。为所有工作人员开发个性化智能需要维持长时间手术中每个人的位置状态，这仍然存在许多计算挑战。&lt;h4&gt;目的&lt;/h4&gt;开发TrackOR框架，以解决手术室中长期多人跟踪和重新识别的问题，从而实现为手术团队提供个性化智能支持。&lt;h4&gt;方法&lt;/h4&gt;TrackOR使用3D几何特征来实现最先进的在线跟踪性能，同时支持有效的离线恢复过程来创建可用于分析的轨迹数据。&lt;h4&gt;主要发现&lt;/h4&gt;通过利用3D几何信息，持久的身份跟踪成为可能，这为手术室中所需的更细致的以人员为中心的分析提供了关键转变，从而能够实现个性化智能系统。&lt;h4&gt;结论&lt;/h4&gt;这项新功能开辟了各种应用，包括提出的时序路径印记，可将原始跟踪数据转化为可操作的见解，以提高团队效率和安全性，最终提供个性化支持。&lt;h4&gt;翻译&lt;/h4&gt;为手术团队提供智能支持是自动化手术场景理解的关键前沿，长期目标是改善患者预后。为所有工作人员开发个性化智能需要维持长时间手术中每个人的位置状态，这仍然存在许多计算挑战。我们提出了TrackOR，一个用于解决手术室中长期多人跟踪和重新识别问题的框架。TrackOR使用3D几何特征实现了最先进的在线跟踪性能（比最强的基线高11%的关联准确率），同时支持有效的离线恢复过程来创建可用于分析的轨迹数据。我们的工作表明，通过利用3D几何信息，持久的身份跟踪成为可能，这为手术室中所需的更细致的以人员为中心的分析提供了关键转变，从而能够实现个性化智能系统。这项新功能开辟了各种应用，包括我们提出的时序路径印记，可将原始跟踪数据转化为可操作的见解，以提高团队效率和安全性，最终提供个性化支持。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决手术室中的长期多人跟踪和再识别问题。这个问题很重要，因为手术是高风险的团队协作过程，而现有方法无法处理医护人员频繁离开和重新进入场景的情况，导致无法进行长期分析。实现个性化智能手术系统需要能够持续跟踪每个医护人员，了解他们的独特习惯和工作模式，从而提高手术安全性和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到手术室环境对多目标跟踪的挑战，包括严重遮挡、拥挤环境和医护人员穿着难以区分的制服。他们发现现有方法基本上都是身份不可知的，只能进行短期分析。作者借鉴了现有的多目标跟踪基础方法、3D人体姿态估计技术和点云处理方法，但创新性地提出了使用3D几何签名而非视觉特征来实现身份跟踪，解决了基于外观的方法在手术室环境中失效的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用3D几何签名来实现手术室中的长期多人跟踪和再识别，不依赖于容易混淆的视觉特征。整体流程包括：1)从多视图RGB数据检测3D人体姿态；2)从分割的3D点云中提取ReID特征；3)通过计算成本矩阵并使用匈牙利算法进行数据关联；4)执行离线全局轨迹恢复来纠正碎片化；5)生成时间路径印记将轨迹转化为可操作的见解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出了TrackOR框架，利用3D几何签名实现长期身份跟踪；2)结合3D姿态和ReID进行在线跟踪，单独使用ReID进行离线恢复；3)提出时间路径印记用于长期工作流程分析。相比之前的工作，TrackOR不依赖视觉特征，在医护人员离开和重新进入场景时仍能保持身份一致性，而传统2D跟踪器受限于视野，标准3D跟踪器缺乏长期再识别能力，现有OR方法则基本是身份不可知的。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TrackOR通过利用3D几何签名实现了手术室中的长期多人跟踪和再识别，为个性化智能手术系统的发展铺平了道路，使医护人员工作流程的细粒度分析和个性化支持成为可能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Providing intelligent support to surgical teams is a key frontier inautomated surgical scene understanding, with the long-term goal of improvingpatient outcomes. Developing personalized intelligence for all staff membersrequires maintaining a consistent state of who is located where for longsurgical procedures, which still poses numerous computational challenges. Wepropose TrackOR, a framework for tackling long-term multi-person tracking andre-identification in the operating room. TrackOR uses 3D geometric signaturesto achieve state-of-the-art online tracking performance (+11% AssociationAccuracy over the strongest baseline), while also enabling an effective offlinerecovery process to create analysis-ready trajectories. Our work shows that byleveraging 3D geometric information, persistent identity tracking becomesattainable, enabling a critical shift towards the more granular, staff-centricanalyses required for personalized intelligent systems in the operating room.This new capability opens up various applications, including our proposedtemporal pathway imprints that translate raw tracking data into actionableinsights for improving team efficiency and safety and ultimately providingpersonalized support.</description>
      <author>example@mail.com (Tony Danjun Wang, Christian Heiliger, Nassir Navab, Lennart Bastian)</author>
      <guid isPermaLink="false">2508.07968v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts</title>
      <link>http://arxiv.org/abs/2508.07842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages,8 figures. Submitted to AAAI'26&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DETACH是一种通过生物启发的双流解耦进行跨领域学习的长时程任务框架，包含环境学习模块和技能学习模块，能够有效解决人类-场景交互中长时程任务的跨领域学习挑战。&lt;h4&gt;背景&lt;/h4&gt;人类-场景交互中的长时程任务是复杂的多步骤任务，需要连续规划、顺序决策和跨领域的扩展执行才能实现最终目标。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法无法推广到新环境和技能组合的问题，实现跨领域的长时程任务学习与执行。&lt;h4&gt;方法&lt;/h4&gt;DETACH受大脑的'位置-内容'双通路机制启发，包含两个核心模块：环境学习模块用于空间理解，捕捉物体功能、空间关系和场景语义，通过完全的环境-自我解耦实现跨领域迁移；技能学习模块用于任务执行，处理包括自由度和运动模式在内的自我状态信息，通过独立的运动模式编码实现跨技能迁移。&lt;h4&gt;主要发现&lt;/h4&gt;在HSI场景中的各种长时程任务上进行了大量实验，与现有方法相比，DETACH可以实现平均子任务成功率提高23%，平均执行效率提高29%。&lt;h4&gt;结论&lt;/h4&gt;DETACH通过双流解耦的方法有效解决了长时程任务在跨领域学习中的挑战，显著提高了任务成功率和执行效率。&lt;h4&gt;翻译&lt;/h4&gt;人类-场景交互(HSI)中的长时程(LH)任务是复杂的多步骤任务，需要连续规划、顺序决策和跨领域的扩展执行才能实现最终目标。然而，现有方法严重依赖于通过连接预训练的子任务来进行技能链式操作，环境观察和自我状态紧密耦合，缺乏推广到新环境和技能组合的能力，无法完成跨领域的各种长时程任务。为了解决这个问题，本文提出了DETACH，一种通过生物启发的双流解耦进行跨领域学习的长时程任务框架。受大脑的'位置-内容'双通路机制启发，DETACH包含两个核心模块：i)环境学习模块用于空间理解，捕捉物体功能、空间关系和场景语义，通过完全的环境-自我解耦实现跨领域迁移；ii)技能学习模块用于任务执行，处理包括自由度和运动模式在内的自我状态信息，通过独立的运动模式编码实现跨技能迁移。我们在HSI场景中的各种长时程任务上进行了大量实验。与现有方法相比，DETACH可以实现平均子任务成功率提高23%，平均执行效率提高29%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决人场景交互(HSI)中长时程(LH)任务在跨领域环境下的泛化问题。现有方法在环境变化时无法有效分离环境与自我状态的影响，遇到新技能时即使运动模式相似也需重新训练整个网络。这个问题在现实中非常重要，因为机器人、医疗干预和智能家居系统等应用需要在不同环境中灵活适应，现有方法的性能差距严重阻碍了这些系统在现实世界中的部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受神经科学中'背侧-腹侧流'假说启发，该假说认为大脑有专门处理物体识别的'what'通路和处理空间运动控制的'where-how'通路。作者借鉴了这一双通路机制，但将其应用于功能解纠缠而非视觉模态分离。设计上采用了双流架构：环境编码器处理空间信息，自我编码器处理身体状态信息，并设计了多策略自适应融合机制和渐进式训练协议。作者也参考了现有的解纠缠学习方法，但这些方法主要关注静态因子分离，不适合动态交互和生成式适应的需求。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过解耦环境感知和自我状态表示，实现长时程任务在跨领域环境下的有效泛化。整体流程包括：1)观察空间重构模型，将统一观察空间分离为环境感知和自我状态；2)解纠缠双编码器，环境编码器用多尺度CNN处理空间信息，自我编码器用双向LSTM处理身体状态；3)多策略自适应融合机制，结合交叉注意力、门控融合和专家混合三种策略；4)渐进式训练协议，包括独立预训练、融合层优化和端到端联合优化三个阶段。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)DETACH解纠缠架构，第一个基于生物认知原则的HSI具身AI控制框架；2)专门的双流编码器，分别增强环境迁移能力和技能重用能力；3)多策略自适应融合机制，根据任务特性动态选择最优融合策略；4)渐进式训练协议，确保各模块专门特性。相比之前工作，DETACH实现了功能解纠缠而非模态分离，同时解决了环境迁移和技能迁移双重挑战，使用多策略自适应融合而非固定融合策略，并通过渐进式训练避免灾难性遗忘。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DETACH提出了一种受生物启发的双流解纠缠框架，通过分离环境感知和自我状态表示，实现了长时程任务在跨领域环境下的高效泛化和技能重用，显著提升了人场景交互系统的适应性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complexmulti-step tasks that require continuous planning, sequential decision-making,and extended execution across domains to achieve the final goal. However,existing methods heavily rely on skill chaining by concatenating pre-trainedsubtasks, with environment observations and self-state tightly coupled, lackingthe ability to generalize to new combinations of environments and skills,failing to complete various LH tasks across domains. To solve this problem,this paper presents DETACH, a cross-domain learning framework for LH tasks viabiologically inspired dual-stream disentanglement. Inspired by the brain's"where-what" dual pathway mechanism, DETACH comprises two core modules: i) anenvironment learning module for spatial understanding, which captures objectfunctions, spatial relationships, and scene semantics, achieving cross-domaintransfer through complete environment-self disentanglement; ii) a skilllearning module for task execution, which processes self-state informationincluding joint degrees of freedom and motor patterns, enabling cross-skilltransfer through independent motor pattern encoding. We conducted extensiveexperiments on various LH tasks in HSI scenes. Compared with existing methods,DETACH can achieve an average subtasks success rate improvement of 23% andaverage execution efficiency improvement of 29%.</description>
      <author>example@mail.com (Yutong Shen, Hangxu Liu, Penghui Liu, Ruizhe Xia, Tianyi Yao, Yitong Sun, Tongtong Feng)</author>
      <guid isPermaLink="false">2508.07842v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models</title>
      <link>http://arxiv.org/abs/2508.07714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种半自动化流程，结合先进的目标检测器和大型语言模型，以最小化人工努力构建多类门检测数据集，解决了相关公开数据集稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;在建筑平面图中准确检测和分类不同类型的门对于建筑合规检查和室内场景理解等多个应用至关重要。然而，专门用于细粒度多类门检测的公开数据集仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;提出一个半自动化流程，利用最先进的对象检测器和大型语言模型以最少的人工努力构建多类门检测数据集。&lt;h4&gt;方法&lt;/h4&gt;首先使用深度目标检测模型将门作为统一类别进行检测；接着，大型语言模型基于每个检测到的实例的视觉和上下文特征对其进行分类；最后，人机交互阶段确保高质量的标签和边界框。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著降低了标注成本，同时产生适合用于分析平面图的神经网络模型基准测试的数据集。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了结合深度学习和多模态推理在复杂现实领域高效构建数据集的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在建筑平面图中准确检测和分类不同类型的门对于多个应用（如建筑合规检查和室内场景理解）至关重要。尽管如此，专门为细粒度多类门检测设计的公开可用数据集仍然稀少。在这项工作中，我们提出了一个半自动化流程，利用最先进的对象检测器和大型语言模型，以最小化人工努力构建多类门检测数据集。门首先使用深度目标检测模型作为统一类别被检测出来。接着，大型语言模型根据每个检测到的实例的视觉和上下文特征对其进行分类。最后，人机交互阶段确保高质量的标签和边界框。我们的方法显著降低了标注成本，同时产生了一个适合用于平面图分析中神经网络模型基准测试的数据集。这项工作展示了结合深度学习和多模态推理在复杂现实领域高效构建数据集的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate detection and classification of diverse door types in floor plansdrawings is critical for multiple applications, such as building compliancechecking, and indoor scene understanding. Despite their importance, publiclyavailable datasets specifically designed for fine-grained multi-class doordetection remain scarce. In this work, we present a semi-automated pipelinethat leverages a state-of-the-art object detector and a large language model(LLM) to construct a multi-class door detection dataset with minimal manualeffort. Doors are first detected as a unified category using a deep objectdetection model. Next, an LLM classifies each detected instance based on itsvisual and contextual features. Finally, a human-in-the-loop stage ensureshigh-quality labels and bounding boxes. Our method significantly reducesannotation cost while producing a dataset suitable for benchmarking neuralmodels in floor plan analysis. This work demonstrates the potential ofcombining deep learning and multimodal reasoning for efficient datasetconstruction in complex real-world domains.</description>
      <author>example@mail.com (Licheng Zhang, Bach Le, Naveed Akhtar, Tuan Ngo)</author>
      <guid isPermaLink="false">2508.07714v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images</title>
      <link>http://arxiv.org/abs/2508.06546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted in ICCV 25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅使用多视图RGB图像进行3D语义场景图估计的新方法，通过改进特征聚合和利用相邻节点信息，克服了从预测深度图重建的伪点几何噪声问题，并减少了多视图图像特征中的背景噪声。&lt;h4&gt;背景&lt;/h4&gt;现代3D语义场景图估计方法通常依赖于真实的3D标注数据来准确预测目标对象、谓词和关系。然而，在没有给定3D真实表示的情况下，仅使用多视图RGB图像进行场景图估计是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种仅使用多视图RGB图像的3D语义场景图估计方法，克服从预测深度图重建的伪点几何噪声问题，减少多视图图像特征中的背景噪声，提高场景图估计的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1. 获取语义掩码来指导特征聚合，过滤背景特征；2. 设计一种新颖的方法来合并相邻节点信息，增强场景图估计的鲁棒性；3. 利用从训练统计数据中计算出的显式统计先验，基于节点的单跳邻域来改进节点和边预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法优于当前仅使用多视图图像作为初始输入的现有方法。&lt;h4&gt;结论&lt;/h4&gt;通过结合语义掩码引导的特征聚合和相邻节点信息，以及利用统计先验进行预测优化，可以有效地在没有3D真实标注的情况下进行准确的3D语义场景图估计。&lt;h4&gt;翻译&lt;/h4&gt;现代3D语义场景图估计方法利用真实的3D标注来准确预测目标对象、谓词和关系。在没有给定3D真实表示的情况下，我们探索仅利用多视图RGB图像来解决这一任务。为了获得准确的场景图估计所需的鲁棒特征，我们必须克服从预测深度图重建的噪声伪点几何，并减少多视图图像特征中存在的背景噪声。关键是通过相邻关系来丰富节点和边特征，加入准确的语义和空间信息。我们获取语义掩码来指导特征聚合以过滤背景特征，并设计了一种新颖的方法来合并相邻节点信息，以增强我们场景图估计的鲁棒性。此外，我们利用从训练统计数据计算出的显式统计先验，基于它们的单跳邻域来改进节点和边预测。我们的实验表明，我们的方法优于当前仅使用多视图图像作为初始输入的方法。我们的项目页面可在 https://qixun1.github.io/projects/SCRSSG 获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在没有真实3D标注的情况下，仅使用多视角RGB图像生成3D语义场景图的问题。这个问题很重要，因为3D场景图是增强高级场景理解的关键表示，在图像描述、检索、编辑和医疗应用中扮演重要角色。现有方法大多依赖昂贵的3D LiDAR点云，而仅使用RGB图像的方法面临特征噪声和背景干扰的挑战，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法依赖边界框提案会导致背景噪声和部分对象特征提取不完整的问题。他们借鉴了Segment Anything Model(SAM)用于生成精确语义掩码，参考了JointSSG的可学习几何门设计，受ResNet残差连接启发引入邻近特征融合机制，并借鉴了2D场景图方法KERNS和Schemata的统计先验思想，但将这些技术整合并创新应用于3D场景图生成任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过掩码特征初始化减少背景干扰，设计残差空间邻居图神经网络增强节点特征，并利用统计先验知识进行置信度重评分。整体流程：1)输入多视角RGB图像；2)使用RGB-D SLAM和深度估计器重建3D点云；3)通过SAM生成语义掩码并初始化节点特征；4)使用RSN-GNN进行消息传递和特征增强；5)通过CR模块基于统计先验细化预测；6)输出最终3D场景图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)掩码特征初始化(MFI)使用语义掩码而非边界框聚合特征，减少背景噪声；2)残差空间邻居图神经网络(RSN-GNN)将高度激活的邻近特征集成到目标节点；3)置信度重评分(CR)模块利用统计先验知识细化预测。相比之前工作，不依赖实体检测器的边界框，能提取完整对象特征；结合视觉和几何线索处理低置信度预测；在处理类别不平衡特别是低尾部类别上表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于统计置信度重评分的创新框架，通过掩码特征初始化、残差空间邻居图神经网络和置信度重评分三个模块，显著提高了仅使用多视角RGB图像进行3D语义场景图生成的鲁棒性和准确性，特别是在处理低置信度和类别不平衡场景时表现优异。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern 3D semantic scene graph estimation methods utilize ground truth 3Dannotations to accurately predict target objects, predicates, andrelationships. In the absence of given 3D ground truth representations, weexplore leveraging only multi-view RGB images to tackle this task. To attainrobust features for accurate scene graph estimation, we must overcome the noisyreconstructed pseudo point-based geometry from predicted depth maps and reducethe amount of background noise present in multi-view image features. The key isto enrich node and edge features with accurate semantic and spatial informationand through neighboring relations. We obtain semantic masks to guide featureaggregation to filter background features and design a novel method toincorporate neighboring node information to aid robustness of our scene graphestimates. Furthermore, we leverage on explicit statistical priors calculatedfrom the training summary statistics to refine node and edge predictions basedon their one-hop neighborhood. Our experiments show that our method outperformscurrent methods purely using multi-view images as the initial input. Ourproject page is available at https://qixun1.github.io/projects/SCRSSG.</description>
      <author>example@mail.com (Qi Xun Yeo, Yanyan Li, Gim Hee Lee)</author>
      <guid isPermaLink="false">2508.06546v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Robot-Assisted Surgery with Hierarchical Class-Incremental Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2508.01713v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted at MICCAI AMAI 2025 workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种增强的TOPICS+方法，专门针对机器人手术场景的鲁棒分割问题，通过引入Dice损失处理类别不平衡、分层伪标记和定制标签分类法，解决了静态分割模型在动态手术环境中的局限性。&lt;h4&gt;背景&lt;/h4&gt;机器人辅助手术依赖于准确和实时的场景理解来安全引导手术器械，然而在静态数据集上训练的分割模型在部署到动态和不断演变的手术环境时面临关键限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够持续适应新类别且避免灾难性遗忘的分割模型，以应对手术环境中的动态变化。&lt;h4&gt;方法&lt;/h4&gt;基于Taxonomy-Oriented Poincaré-regularized Incremental Class Segmentation (TOPICS)方法提出增强版本TOPICS+，在分层损失公式中纳入Dice损失处理强类别不平衡，引入分层伪标记，设计针对机器人手术环境的定制标签分类法，并提出了六个新的CISS基准。&lt;h4&gt;主要发现&lt;/h4&gt;TOPICS+方法在机器人手术场景的分割任务中表现良好，所提出的六个CISS基准能够模拟手术环境中的真实类别增量设置，在Syn-Mediverse合成数据集上包含144多个类别的标签集。&lt;h4&gt;结论&lt;/h4&gt;TOPICS+方法能够有效处理手术环境中的分割挑战，代码和训练模型已公开提供。&lt;h4&gt;翻译&lt;/h4&gt;机器人辅助手术依赖于准确和实时的场景理解来安全引导手术器械。然而，在静态数据集上训练的分割模型在部署到这些动态和不断演变的手术环境时面临关键限制。增量类语义分割(CISS)允许模型持续适应新类别，同时避免对先前知识的灾难性遗忘，无需在先前数据上训练。在这项工作中，我们建立在最近引入的面向分类学的庞加莱正则化增量类分割(TOPICS)方法的基础上，提出了一个增强版本，称为TOPICS+，专门针对手术场景的鲁棒分割。具体而言，我们将Dice损失纳入分层损失公式中以处理强类别不平衡，引入分层伪标记，并设计针对机器人手术环境的定制标签分类法。我们还提出了六个专为机器人手术环境设计的新型CISS基准，包括多个增量步骤和几个语义类别，以模拟手术环境中的真实类别增量设置。此外，我们在Syn-Mediverse合成数据集上引入了一个包含144多个类别的 refined 标签集，在线托管作为评估基准。我们在http://topics.cs.uni-freiburg.de公开提供代码和训练模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人辅助手术中的动态场景理解问题，特别是类增量语义分割(CISS)在手术环境中的应用。这个问题非常重要，因为机器人辅助手术正在快速增长并扩展到多个医疗领域，能够显著减少失血、输血率、住院时间和并发症。手术环境的复杂性和多变性需要系统能够不断适应新的器械、组织和注释协议，而传统方法存在'灾难性遗忘'问题，无法在不重放历史数据的情况下持续学习新知识，这在受隐私法规限制的医疗环境中尤其关键。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于现有的TOPICS方法进行改进，该方法利用超几何空间表示层次化类别关系来缓解灾难性遗忘。作者借鉴了持续学习原则、超几何空间中的树状结构建模以及知识蒸馏在医学图像分类中的应用。针对手术领域的特殊挑战，作者整合了Dice损失处理类别不平衡，引入层次化伪标记应对多样化背景，并设计了专门的标签分类法。作者还创建了六个针对机器人手术环境的新型CISS基准，扩展了Syn-Mediverse数据集到144多个类别，以更真实地模拟手术环境中的增量学习场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将类别组织成树状结构并在超几何空间中编码层次关系，通过层次化Dice损失处理类别不平衡，使用层次化伪标记保持准确性，并设计定制标签分类法防止遗忘。整体流程包括：1)使用三个手术数据集并创建CISS设置；2)基于DeepLabV3和ResNet-101构建模型；3)在超几何空间中显式编码类层次结构；4)结合层次化Dice损失、稀缺性和关系正则化损失；5)在增量步骤中使用层次化伪标记；6)在六个不同CISS设置中评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出TOPICS+专门针对机器人辅助手术的无重放CISS方法；2)整合Dice损失处理手术场景中的严重类别不平衡；3)引入层次化伪标记应对多样化背景；4)设计定制标签分类法防止遗忘；5)创建六个针对机器人手术环境的新型CISS基准；6)扩展Syn-Mediverse数据集到144多个类别。相比之前工作，本文专门针对手术场景优化，使用更精细的层次化损失和伪标记技术，在更大规模数据集上评估，并创建了更真实的手术环境基准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了TOPICS+，一种基于层次化超几何空间表示的无重放类增量语义分割方法，通过集成Dice损失、层次化伪标记和定制标签分类法，有效解决了机器人辅助手术场景中的动态环境理解问题，并在六个新提出的基准测试上展示了卓越的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot-assisted surgeries rely on accurate and real-time scene understandingto safely guide surgical instruments. However, segmentation models trained onstatic datasets face key limitations when deployed in these dynamic andevolving surgical environments. Class-incremental semantic segmentation (CISS)allows models to continually adapt to new classes while avoiding catastrophicforgetting of prior knowledge, without training on previous data. In this work,we build upon the recently introduced Taxonomy-Oriented Poincar\'e-regularizedIncremental Class Segmentation (TOPICS) approach and propose an enhancedvariant, termed TOPICS+, specifically tailored for robust segmentation ofsurgical scenes. Concretely, we incorporate the Dice loss into the hierarchicalloss formulation to handle strong class imbalances, introduce hierarchicalpseudo-labeling, and design tailored label taxonomies for robotic surgeryenvironments. We also propose six novel CISS benchmarks designed for roboticsurgery environments including multiple incremental steps and several semanticcategories to emulate realistic class-incremental settings in surgicalenvironments. In addition, we introduce a refined set of labels with more than144 classes on the Syn-Mediverse synthetic dataset, hosted online as anevaluation benchmark. We make the code and trained models publicly available athttp://topics.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Julia Hindel, Ema Mekic, Enamundram Naga Karthik, Rohit Mohan, Daniele Cattaneo, Maria Kalweit, Abhinav Valada)</author>
      <guid isPermaLink="false">2508.01713v2</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility</title>
      <link>http://arxiv.org/abs/2508.07989v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures, 2 tables. Accepted at CV4A11y, ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了多模态大语言模型作为视障人士辅助技术的潜力及其面临的挑战，特别指出了模型在感知运动方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在为视障和视力受损(BVI)人群提供辅助技术方面展现出巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;识别并分析多模态大语言模型在现实应用中存在的一个关键失败模式，提高对其局限性的认识并推动改进。&lt;h4&gt;方法&lt;/h4&gt;作为立场性论文，通过提出'自动扶梯问题'作为典型案例，分析模型在感知连续运动方面的缺陷，而非开发新模型。&lt;h4&gt;主要发现&lt;/h4&gt;发现最先进的模型存在'隐式运动失明'问题，无法感知自动扶梯的行进方向；这源于视频理解中的帧采样范式将视频视为静态图像序列，难以处理连续的低信号运动。&lt;h4&gt;结论&lt;/h4&gt;需要从纯语义识别向稳健的物理感知转变，并开发新的以人为本的基准测试，优先考虑安全性、可靠性和用户在动态环境中的真实需求。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)作为视障和视力受损(BVI)人群的辅助技术具有巨大潜力。然而，我们识别出一种关键的失败模式，它削弱了模型在现实应用中的可信度。我们引入了自动扶梯问题——最先进模型无法感知自动扶梯行进方向——作为我们称为隐式运动失明的更深层次局限性的典型例子。这种失明源于视频理解中的主流帧采样范式，该范式将视频视为静态图像的离散序列，从根本上难以感知连续的低信号运动。作为立场性论文，我们的贡献不是新模型，而是：(I)正式阐述这一盲点，(II)分析其对用户信任的影响，以及(III)发出行动呼吁。我们倡导从纯语义识别向稳健的物理感知转变，并敦促开发新的以人为本的基准测试，优先考虑安全性、可靠性和用户在动态环境中的真实需求。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决AI系统中的'隐式运动盲区'问题，特别是多模态大语言模型无法感知自动扶梯运动方向的现象。这个问题对盲人和视障人士(BVI)社区至关重要，因为运动方向判断是安全导航的关键信息。当前AI系统连这种基本运动都无法判断，不仅影响功能性，更严重损害用户信任，阻碍视障人士获得真正的独立导航能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察先进AI模型在自动扶梯方向判断任务上的失败，将其视为典型案例来揭示更广泛的运动盲区问题。他们分析了人类视觉系统感知连续光流与AI系统基于帧采样的根本差异。作者借鉴了经典计算机视觉中的光流概念，参考了现有视频理解基准测试的局限性，并考察了当前AI辅助技术的应用情况。作为立场论文，作者没有提出完整解决方案，而是识别问题、分析影响并提出未来研究方向。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是当前AI系统的运动盲区源于其视频处理的帧采样范式，它将视频视为离散静态图像序列，无法捕捉连续运动。解决方案需要从'语义识别'转向'物理感知'。整体实现流程包括：1)开发混合架构，结合传统光流技术与现代MLLM；2)探索事件相机等新型传感器；3)研究物理信息学习，将物理定律纳入模型训练；4)创建以人为本的基准测试，优先考虑安全性、可靠性和用户信任。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次正式识别并命名'隐式运动盲区'问题；2)深入分析人类视觉与AI系统的根本差异；3)将技术问题与用户体验和信任危机联系起来；4)呼吁研究范式从'语义识别'转向'物理感知'。相比之前工作，这篇论文不是提出新模型或算法，而是识别被忽视的根本问题；将技术问题与用户体验全面关联；呼吁研究范式转变而非简单技术改进；强调与最终用户合作的重要性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文识别并正式化了AI辅助技术中的'隐式运动盲区'问题，揭示了当前视频理解范式的根本局限性，并呼吁研究社区从语义识别转向物理感知，以开发真正可靠、值得信赖的辅助技术。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) hold immense promise as assistivetechnologies for the blind and visually impaired (BVI) community. However, weidentify a critical failure mode that undermines their trustworthiness inreal-world applications. We introduce the Escalator Problem -- the inability ofstate-of-the-art models to perceive an escalator's direction of travel -- as acanonical example of a deeper limitation we term Implicit Motion Blindness.This blindness stems from the dominant frame-sampling paradigm in videounderstanding, which, by treating videos as discrete sequences of staticimages, fundamentally struggles to perceive continuous, low-signal motion. As aposition paper, our contribution is not a new model but rather to: (I) formallyarticulate this blind spot, (II) analyze its implications for user trust, and(III) issue a call to action. We advocate for a paradigm shift from purelysemantic recognition towards robust physical perception and urge thedevelopment of new, human-centered benchmarks that prioritize safety,reliability, and the genuine needs of users in dynamic environments.</description>
      <author>example@mail.com (Xiantao Zhang)</author>
      <guid isPermaLink="false">2508.07989v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding</title>
      <link>http://arxiv.org/abs/2508.07683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了TAR-TVG框架，通过在推理过程中引入时间戳锚点来强制执行显式监督，解决了现有强化学习方法无法明确约束推理过程的问题。同时，他们开发了一个三阶段的自蒸馏训练策略，使模型能够生成稳健的锚点，同时保持推理质量。实验表明，该模型在实现最先进性能的同时，产生了可解释、可验证的推理链，并逐步完善时间估计。&lt;h4&gt;背景&lt;/h4&gt;时序视频定位(TVG)旨在精确定位与自然语言查询相对应的视频片段，这对长篇视频理解至关重要。尽管现有的强化学习方法鼓励模型在预测前生成推理链，但它们未能明确约束推理过程，无法确保最终时间预测的质量。&lt;h4&gt;目的&lt;/h4&gt;解决现有强化学习方法在时序视频定位中无法明确约束推理过程的问题，确保推理质量，并提高最终时间预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了TAR-TVG框架，在推理过程中引入时间戳锚点作为中间验证点，并要求每个推理步骤产生越来越精确的时间估计。同时，开发了一个三阶段的自蒸馏训练策略：(1)初始GRPO训练收集包含多个时间戳锚点的3万条高质量推理轨迹；(2)在蒸馏数据上进行监督微调；(3)在SFT增强的模型上进行最终的GRPO优化。&lt;h4&gt;主要发现&lt;/h4&gt;模型实现了最先进的性能；产生了可解释、可验证的推理链；推理过程中包含逐步完善的时间估计；三阶段训练策略使模型能够生成稳健的锚点，同时保持推理质量。&lt;h4&gt;结论&lt;/h4&gt;TAR-TVG框架通过引入时间戳锚点和三阶段训练策略，有效解决了现有方法在时序视频定位中推理质量控制的问题，实现了更准确的视频片段定位，并提供了可解释的推理过程。&lt;h4&gt;翻译&lt;/h4&gt;时序视频定位旨在精确定位与自然语言查询相对应的视频片段，这对长篇视频理解至关重要。尽管现有的强化学习方法鼓励模型在预测前生成推理链，但它们未能明确约束推理过程，无法确保最终时间预测的质量。为解决这一局限，我们提出了时间戳锚点约束的时序视频推理(TAR-TVG)，一种新颖的框架，它在推理过程中引入时间戳锚点，强制对思考内容进行显式监督。这些锚点作为中间验证点。更重要的是，我们要求每个推理步骤产生越来越精确的时间估计，从而确保推理过程对最终预测有实质性贡献。为解决模型中低概率锚点生成的挑战，我们开发了一种高效的自蒸馏训练策略：(1)初始GRPO训练收集3万条包含多个时间戳锚点的高质量推理轨迹；(2)在蒸馏数据上进行监督微调；(3)在SFT增强的模型上进行最终GRPO优化。这一三阶段训练策略使模型能够生成稳健的锚点，同时保持推理质量。实验表明，我们的模型在实现最先进性能的同时，产生了可解释、可验证的推理链，并逐步完善时间估计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal Video Grounding (TVG) aims to precisely localize video segmentscorresponding to natural language queries, which is a critical capability forlong-form video understanding. Although existing reinforcement learningapproaches encourage models to generate reasoning chains before predictions,they fail to explicitly constrain the reasoning process to ensure the qualityof the final temporal predictions. To address this limitation, we proposeTimestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG),a novel framework that introduces timestamp anchors within the reasoningprocess to enforce explicit supervision to the thought content. These anchorsserve as intermediate verification points. More importantly, we require eachreasoning step to produce increasingly accurate temporal estimations, therebyensuring that the reasoning process contributes meaningfully to the finalprediction. To address the challenge of low-probability anchor generation inmodels (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillationtraining strategy: (1) initial GRPO training to collect 30K high-qualityreasoning traces containing multiple timestamp anchors, (2) supervisedfine-tuning (SFT) on distilled data, and (3) final GRPO optimization on theSFT-enhanced model. This three-stage training strategy enables robust anchorgeneration while maintaining reasoning quality. Experiments show that our modelachieves state-of-the-art performance while producing interpretable, verifiablereasoning chains with progressively refined temporal estimations.</description>
      <author>example@mail.com (Chaohong Guo, Xun Mo, Yongwei Nie, Xuemiao Xu, Chao Xu, Fei Yu, Chengjiang Long)</author>
      <guid isPermaLink="false">2508.07683v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents</title>
      <link>http://arxiv.org/abs/2508.07642v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 5 Figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SkillNav，一个模块化框架，将结构化、基于技能的推理引入到基于Transformer的VLN智能体中。该方法将导航分解为可解释的原子技能，由专门智能体处理，并通过新颖的零样本VLM路由器动态选择最合适的智能体。SkillNav在R2R基准测试上实现了最先进性能，并在GSA-R2R基准测试上表现出强大的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;视觉与语言导航(VLN)在让智能体解释自然语言指令并在复杂3D环境中导航方面提出了重大挑战。尽管最近的进展是由大规模预训练和数据增强推动的，但当前方法仍然难以推广到未见过的场景，特别是当需要复杂的时空推理时。&lt;h4&gt;目的&lt;/h4&gt;提出SkillNav框架，将结构化、基于技能的推理引入到基于Transformer的VLN智能体中，以提高泛化能力，特别是在复杂时空推理场景中。&lt;h4&gt;方法&lt;/h4&gt;将导航分解为一组可解释的原子技能（例如，垂直移动、区域和区域识别、停止和暂停），每个技能由专门的智能体处理。然后，引入了一种新颖的零样本视觉语言模型(VLM)路由器，它通过对齐子目标和视觉观察以及历史动作，在每个时间步动态选择最合适的智能体。&lt;h4&gt;主要发现&lt;/h4&gt;SkillNav在R2R基准测试上实现了新的最先进性能，并且对包含新指令风格和未见环境的GSA-R2R基准测试表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SkillNav通过模块化设计和基于技能的推理，有效解决了VLN中的泛化问题，特别是在复杂时空推理方面，为未来研究提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;视觉与语言导航(VLN)在让智能体解释自然语言指令并在复杂3D环境中导航方面提出了重大挑战。尽管最近的进展是由大规模预训练和数据增强推动的，但当前方法仍然难以推广到未见过的场景，特别是当需要复杂的时空推理时。在这项工作中，我们提出了SkillNav，一个模块化框架，将结构化、基于技能的推理引入到基于Transformer的VLN智能体中。我们的方法将导航分解为一组可解释的原子技能（例如，垂直移动、区域和区域识别、停止和暂停），每个技能由专门的智能体处理。然后，我们引入了一种新颖的零样本视觉语言模型(VLM)路由器，它通过对齐子目标和视觉观察以及历史动作，在每个时间步动态选择最合适的智能体。SkillNav在R2R基准测试上实现了新的最先进性能，并表现出对包含新指令风格和未见环境的GSA-R2R基准测试的强大泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决Vision-and-Language Navigation (VLN)任务中的泛化能力不足问题，特别是当代理需要处理复杂时空推理、未见过的环境和新型指令时的挑战。这个问题在现实中非常重要，因为它关系到AI代理能否真正理解和执行人类在复杂环境中的导航指令，对于机器人技术、虚拟助手、自动驾驶等应用领域具有重要意义。在研究中，解决这一问题将推动具身AI的发展，使代理能够更好地适应多样化的现实世界场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到导航任务具有内在的组成性，可以分解为一系列可解释的原子技能。他们借鉴了Transformer-based VLN代理架构(如DUET)和技能专家混合系统的思想，但创新性地将其应用于VLN领域。作者参考了NavNuances数据集中的技能分类，并扩展了'停止和暂停'和'时间顺序规划'两个新技能。在路由器设计中，他们受到了LLM-based规划系统的启发，并利用现有的VLM模型(如GPT-4o和Qwen2.5-VL)来实现零样本推理。整体设计思路是将复杂的导航任务分解为专门的技能代理，并通过智能路由器动态选择最合适的代理来执行特定任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将复杂的导航任务分解为一系列可解释的原子技能(如垂直移动、区域识别、停止和暂停等)，每个技能由专门的代理处理，并使用基于VLM的路由器动态选择最合适的代理。整体实现流程包括：1)技能分类和数据合成，为每个技能创建专门的合成数据集；2)代理训练，每个技能代理经过两阶段训练；3)导航流程，首先使用时间重排序模块将指令分解为结构化的子目标，然后通过子目标定位确定当前需要执行的子目标，接着通过技能路由选择最合适的代理，最后由选定的代理执行相应的导航动作，这个过程循环进行直到导航完成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)SkillNav模块化框架，将导航分解为原子可重用技能；2)VLM-based路由器，动态选择最合适的技能代理；3)扩展技能分类，增加'停止和暂停'和'时间顺序规划'技能；4)技能特定数据合成方法；5)时间重排序模块。与之前工作的不同在于：SkillNav采用模块化而非端到端设计，强调技能的灵活重组而非简单的专家选择，具有更好的泛化能力和可解释性，特别是在处理新型指令和未知环境时表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SkillNav通过将导航任务分解为可重用的原子技能并使用基于VLM的路由器动态选择最合适的技能代理，显著提升了视觉语言导航代理在未知环境和新型指令上的泛化能力和可解释性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-and-Language Navigation (VLN) poses significant challenges in enablingagents to interpret natural language instructions and navigate complex 3Denvironments. While recent progress has been driven by large-scale pre-trainingand data augmentation, current methods still struggle to generalize to unseenscenarios, particularly when complex spatial and temporal reasoning isrequired. In this work, we propose SkillNav, a modular framework thatintroduces structured, skill-based reasoning into Transformer-based VLN agents.Our method decomposes navigation into a set of interpretable atomic skills(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), eachhandled by a specialized agent. We then introduce a novel zero-shotVision-Language Model (VLM)-based router, which dynamically selects the mostsuitable agent at each time step by aligning sub-goals with visual observationsand historical actions. SkillNav achieves a new state-of-the-art performance onthe R2R benchmark and demonstrates strong generalization to the GSA-R2Rbenchmark that includes novel instruction styles and unseen environments.</description>
      <author>example@mail.com (Tianyi Ma, Yue Zhang, Zehao Wang, Parisa Kordjamshidi)</author>
      <guid isPermaLink="false">2508.07642v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>FineBadminton: A Multi-Level Dataset for Fine-Grained Badminton Video Understanding</title>
      <link>http://arxiv.org/abs/2508.07554v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FineBadminton数据集和FBBench基准，用于解决复杂高速运动（如羽毛球）的细粒度分析挑战，并提出了优化的基线方法。&lt;h4&gt;背景&lt;/h4&gt;复杂高速运动（如羽毛球）的细粒度分析对多模态大语言模型(MLLMs)构成重大挑战，尽管MLLMs在通用视频理解方面取得了显著进展。这种困难主要源于缺乏足够丰富且领域特定的注释数据集。&lt;h4&gt;目的&lt;/h4&gt;为了弥补数据集的不足，引入FineBadminton数据集和FBBench基准，促进细粒度视频理解研究并推动MLLMs在体育智能领域的发展。&lt;h4&gt;方法&lt;/h4&gt;1. 构建FineBadminton数据集，采用多级语义注释层次结构（基础动作、战术语义和决策评估）；2. 开创新的注释流程，协同结合MLLM生成的提案和人工细化；3. 提出FBBench基准用于评估；4. 设计优化的基线方法，包括以击球为中心的关键帧选择和坐标引导的视觉信息浓缩。&lt;h4&gt;主要发现&lt;/h4&gt;FBBench上的结果表明，尽管当前的MLLMs在深度体育视频分析方面仍面临重大挑战，但提出的策略仍然取得了显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;FineBadminton和FBBench共同提供了一个关键的生态系统，以促进细粒度视频理解研究并推动MLLMs在体育智能领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;复杂高速运动（如羽毛球）的细粒度分析对多模态大语言模型(MLLMs)提出了重大挑战，尽管它们在通用视频理解方面取得了显著进展。这种困难主要源于缺乏足够丰富且领域特定的注释数据集。为了弥补这一差距，我们引入了FineBadminton，这是一个新颖的大规模数据集，具有独特的多级语义注释层次结构（基础动作、战术语义和决策评估），用于全面的羽毛球理解。FineBadminton的构建由创新的注释流程驱动，该流程协同结合了MLLM生成的提案和人工细化。我们还提出了FBBench，这是一个从FineBadminton衍生的具有挑战性的基准，用于严格评估MLLMs在细微时空推理和战术理解方面的能力。FineBadminton和FBBench共同提供了一个关键的生态系统，以促进细粒度视频理解研究并推动MLLMs在体育智能领域的发展。此外，我们提出了一种优化的基线方法，结合了以击球为中心的关键帧选择，专注于关键时刻，以及坐标引导的浓缩，用于提炼显著视觉信息。FBBench上的结果表明，尽管当前的MLLMs在深度体育视频分析方面仍面临重大挑战，但我们提出的策略仍然取得了显著的性能提升。项目主页可在提供的网址访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758218&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-grained analysis of complex and high-speed sports like badmintonpresents a significant challenge for Multimodal Large Language Models (MLLMs),despite their notable advancements in general video understanding. Thisdifficulty arises primarily from the scarcity of datasets with sufficientlyrich and domain-specific annotations. To bridge this gap, we introduceFineBadminton, a novel and large-scale dataset featuring a unique multi-levelsemantic annotation hierarchy (Foundational Actions, Tactical Semantics, andDecision Evaluation) for comprehensive badminton understanding. Theconstruction of FineBadminton is powered by an innovative annotation pipelinethat synergistically combines MLLM-generated proposals with human refinement.We also present FBBench, a challenging benchmark derived from FineBadminton, torigorously evaluate MLLMs on nuanced spatio-temporal reasoning and tacticalcomprehension. Together, FineBadminton and FBBench provide a crucial ecosystemto catalyze research in fine-grained video understanding and advance thedevelopment of MLLMs in sports intelligence. Furthermore, we propose anoptimized baseline approach incorporating Hit-Centric Keyframe Selection tofocus on pivotal moments and Coordinate-Guided Condensation to distill salientvisual information. The results on FBBench reveal that while current MLLMsstill face significant challenges in deep sports video analysis, our proposedstrategies nonetheless achieve substantial performance gains. The projecthomepage is available at https://finebadminton.github.io/FineBadminton/.</description>
      <author>example@mail.com (Xusheng He, Wei Liu, Shanshan Ma, Qian Liu, Chenghao Ma, Jianlong Wu)</author>
      <guid isPermaLink="false">2508.07554v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding</title>
      <link>http://arxiv.org/abs/2508.06869v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages,3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为视觉-字幕集成(VSI)的多模态关键帧搜索方法，解决了长视频理解中多模态大语言模型面临的数据规模挑战，通过整合字幕、时间戳和场景边界信息，显著提高了关键帧检索的准确率。&lt;h4&gt;背景&lt;/h4&gt;长视频理解对多模态大语言模型(MLLMs)是一个重大挑战，主要原因在于数据规模巨大。目前广泛采用的关键帧检索策略受到文本查询与视觉内容之间弱多模态对齐的限制，无法捕获精确推理所需的复杂时间语义信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效整合视频视觉信息与文本信息的多模态关键帧搜索方法，提高长视频理解中的关键帧检索准确率。&lt;h4&gt;方法&lt;/h4&gt;提出视觉-字幕集成(VSI)方法，通过双流搜索机制分别捕获视频帧的视觉信息和互补的文本信息，包括视频搜索流和字幕匹配流，并通过两个搜索流的交互提高关键帧搜索准确性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，VSI在LongVideoBench的文本相关子集上实现了40.00%的关键帧定位准确率，在下游长视频问答任务上实现了68.48%的准确率，分别比竞争基线高出20.35%和15.79%。此外，VSI在LongVideoBench的中长视频问答任务上达到了最先进水平(SOTA)。&lt;h4&gt;结论&lt;/h4&gt;VSI方法通过整合视觉和文本信息，有效解决了长视频理解中的关键帧检索问题，展示了其鲁棒性和通用性，为多模态长视频理解提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;长视频理解对多模态大语言模型(MLLMs)提出了重大挑战，主要原因是数据规模巨大。使这一计算上可处理的广泛采用的关键策略是关键帧检索，旨在识别与给定文本查询最相关的稀疏视频帧集。然而，这种方法的有效性受到文本查询与视觉内容之间弱多模态对齐的限制，无法捕获精确推理所需的复杂时间语义信息。为此，我们提出了视觉-字幕集成(VSI)，一种多模态关键帧搜索方法，将字幕、时间戳和场景边界整合到统一的多模态搜索过程中。所提出的方法通过视频搜索流和字幕匹配流的双流搜索机制分别捕获视频帧的视觉信息和互补的文本信息，并通过两个搜索流的交互提高关键帧搜索准确性。实验结果表明，VSI在LongVideoBench的文本相关子集上实现了40.00%的关键帧定位准确率，在下游长视频问答任务上实现了68.48%的准确率，分别超过了竞争基线20.35%和15.79%。此外，在LongVideoBench上，VSI在中长视频问答任务中达到了最先进水平(SOTA)，证明了所提出多模态搜索策略的鲁棒性和通用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long video understanding presents a significant challenge to multimodal largelanguage models (MLLMs) primarily due to the immense data scale. A critical andwidely adopted strategy for making this task computationally tractable iskeyframe retrieval, which seeks to identify a sparse set of video frames thatare most salient to a given textual query. However, the efficacy of thisapproach is hindered by weak multimodal alignment between textual queries andvisual content and fails to capture the complex temporal semantic informationrequired for precise reasoning. To address this, we propose Visual-SubtitleIntegeration(VSI), a multimodal keyframe search method that integratessubtitles, timestamps, and scene boundaries into a unified multimodal searchprocess. The proposed method captures the visual information of video frames aswell as the complementary textual information through a dual-stream searchmechanism by Video Search Stream as well as Subtitle Match Stream,respectively, and improves the keyframe search accuracy through the interactionof the two search streams. Experimental results show that VSI achieve 40.00%key frame localization accuracy on the text-relevant subset of LongVideoBenchand 68.48% accuracy on downstream long Video-QA tasks, surpassing competitivebaselines by 20.35% and 15.79%, respectively. Furthermore, on theLongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QAtasks, demonstrating the robustness and generalizability of the proposedmultimodal search strategy.</description>
      <author>example@mail.com (Jianxiang He, Shaoguang Wang, Weiyu Guo, Meisheng Hong, Jungang Li, Yijie Xu, Ziyang Chen, Hui Xiong)</author>
      <guid isPermaLink="false">2508.06869v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding</title>
      <link>http://arxiv.org/abs/2508.04369v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为时间采样策略优化(TSPO)的方法，通过强化学习解决了多模态大语言模型处理长视频输入的挑战，在多个长视频理解基准测试中实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在视觉-语言任务中已取得显著进展，但在处理长视频输入时仍面临挑战。这些挑战源于模型的上下文限制和训练成本，需要在将视频输入模型前进行稀疏帧采样。然而，由于稀疏帧采样在视频多模态大语言模型中的无监督和非可微分性质，构建可训练的采样方法仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型处理长视频输入的挑战，提出时间采样策略优化方法，通过强化学习提升模型对长视频的理解能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种可训练的事件感知时间代理，捕捉事件-查询相关性以执行概率性关键帧选择；提出了TSPO强化学习范式，将关键帧选择和语言生成建模为联合决策过程；提出了双风格长视频训练数据构建管道，平衡全面的时间理解和关键段定位；集成了基于规则的回答准确性和时间定位奖励机制来优化时间采样策略。&lt;h4&gt;主要发现&lt;/h4&gt;TSPO在多个长视频理解基准测试中实现了最先进的性能，并且在不同前沿视频多模态大语言模型上显示了可转移能力。&lt;h4&gt;结论&lt;/h4&gt;TSPO有效地解决了多模态大语言模型处理长视频输入的挑战，通过强化学习优化了时间采样策略，提升了模型对长视频的理解能力。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在视觉-语言任务中已取得显著进展，但在处理长视频输入时仍面临挑战。这种限制源于模型的上下文限制和训练成本，需要在将视频输入模型前进行稀疏帧采样。然而，由于稀疏帧采样在视频多模态大语言模型中的无监督和非可微分性质，构建可训练的采样方法仍然具有挑战性。为了解决这些问题，我们提出了时间采样策略优化，通过强化学习推进模型对长视频的理解。具体来说，我们首先提出了一种可训练的事件感知时间代理，捕捉事件-查询相关性以执行概率性关键帧选择。然后，我们提出了TSPO强化学习范式，将关键帧选择和语言生成建模为联合决策过程，实现时间采样策略的端到端分组相对优化。此外，我们提出了双风格长视频训练数据构建管道，平衡全面的时间理解和关键段定位。最后，我们整合了基于规则的回答准确性和时间定位奖励机制来优化时间采样策略。全面的实验表明，我们的TSPO在多个长视频理解基准测试中实现了最先进的性能，并显示出在不同前沿视频多模态大语言模型上的可转移能力。我们的代码可在https://github.com/Hui-design/TSPO获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have demonstrated significantprogress in vision-language tasks, yet they still face challenges whenprocessing long-duration video inputs. The limitation arises from MLLMs'context limit and training costs, necessitating sparse frame sampling beforefeeding videos into MLLMs. However, building a trainable sampling methodremains challenging due to the unsupervised and non-differentiable nature ofsparse frame sampling in Video-MLLMs. To address these problems, we proposeTemporal Sampling Policy Optimization (TSPO), advancing MLLMs' long-formvideo-language understanding via reinforcement learning. Specifically, we firstpropose a trainable event-aware temporal agent, which captures event-querycorrelation for performing probabilistic keyframe selection. Then, we proposethe TSPO reinforcement learning paradigm, which models keyframe selection andlanguage generation as a joint decision-making process, enabling end-to-endgroup relative optimization for the temporal sampling policy. Furthermore, wepropose a dual-style long video training data construction pipeline, balancingcomprehensive temporal understanding and key segment localization. Finally, weincorporate rule-based answering accuracy and temporal locating rewardmechanisms to optimize the temporal sampling policy. Comprehensive experimentsshow that our TSPO achieves state-of-the-art performance across multiple longvideo understanding benchmarks, and shows transferable ability across differentcutting-edge Video-MLLMs. Our code is available athttps://github.com/Hui-design/TSPO</description>
      <author>example@mail.com (Canhui Tang, Zifan Han, Hongbo Sun, Sanping Zhou, Xuchong Zhang, Xin Wei, Ye Yuan, Huayu Zhang, Jinglin Xu, Hao Sun)</author>
      <guid isPermaLink="false">2508.04369v3</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning</title>
      <link>http://arxiv.org/abs/2508.08186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submitted to IEEE Transactions on Pattern Analysis and Machine  Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为KARMA的高效语义分割框架，用于土木基础设施结构缺陷检测。该框架通过组合一维函数而非传统卷积来建模复杂缺陷模式，具有参数效率高、性能优越的特点，适合实时检测系统。&lt;h4&gt;背景&lt;/h4&gt;土木基础设施的结构缺陷语义分割面临三大挑战：缺陷外观变化大、成像条件恶劣、类别严重不平衡。现有深度学习方法虽有效，但通常需要数百万参数，使其在实时检测系统中不实用。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的语义分割框架，能够在保持竞争力的性能的同时显著减少参数量，使其适合实时基础设施检测系统。&lt;h4&gt;方法&lt;/h4&gt;KARMA框架包含三个技术创新：1) 参数高效的Tiny Kolmogorov-Arnold Network（TiKAN）模块，利用低阶分解进行基于KAN的特征变换；2) 优化的特征金字塔结构，使用可分离卷积进行多尺度缺陷分析；3) 静态-动态原型机制，增强不平衡类别的特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;在基准基础设施检测数据集上的实验表明，KARMA与最先进方法相比具有竞争性或更好的平均IoU性能，同时使用显著更少的参数（0.959M vs. 31.04M，减少了97%）。KARMA以0.264 GFLOPS的速度运行，保持适合实时部署的推理速度。&lt;h4&gt;结论&lt;/h4&gt;KARMA框架通过创新的架构设计，成功解决了传统深度学习模型在实时基础设施检测中的参数效率问题，为自动化基础设施检测系统提供了实用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;土木基础设施中结构缺陷的语义分割由于缺陷外观变化大、成像条件恶劣和严重的类别不平衡而仍然具有挑战性。当前的深度学习方法尽管有效，但通常需要数百万参数，使它们在实时检测系统中不切实际。我们引入了KARMA（Kolmogorov-Arnold表示映射架构），这是一种高效的语义分割框架，通过组合一维函数而非传统卷积来建模复杂的缺陷模式。KARMA具有三个技术创新：（1）参数高效的Tiny Kolmogorov-Arnold Network（TiKAN）模块，利用低阶分解进行基于KAN的特征变换；（2）优化的特征金字塔结构，使用可分离卷积进行多尺度缺陷分析；（3）静态-动态原型机制，增强不平衡类别的特征表示。在基准基础设施检测数据集上的大量实验表明，与最先进的方法相比，KARMA实现了竞争性或更好的平均IoU性能，同时使用显著更少的参数（0.959M vs. 31.04M，减少了97%）。KARMA以0.264 GFLOPS的速度运行，保持适合实时部署的推理速度，使实用的自动化基础设施检测系统成为可能，同时不妥协准确性。源代码可通过以下URL访问：https://github.com/faeyelab/karma。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation of structural defects in civil infrastructure remainschallenging due to variable defect appearances, harsh imaging conditions, andsignificant class imbalance. Current deep learning methods, despite theireffectiveness, typically require millions of parameters, rendering themimpractical for real-time inspection systems. We introduce KARMA(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficientsemantic segmentation framework that models complex defect patterns throughcompositions of one-dimensional functions rather than conventionalconvolutions. KARMA features three technical innovations: (1) aparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraginglow-rank factorization for KAN-based feature transformation; (2) an optimizedfeature pyramid structure with separable convolutions for multi-scale defectanalysis; and (3) a static-dynamic prototype mechanism that enhances featurerepresentation for imbalanced classes. Extensive experiments on benchmarkinfrastructure inspection datasets demonstrate that KARMA achieves competitiveor superior mean IoU performance compared to state-of-the-art approaches, whileusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).Operating at 0.264 GFLOPS, KARMA maintains inference speeds suitable forreal-time deployment, enabling practical automated infrastructure inspectionsystems without compromising accuracy. The source code can be accessed at thefollowing URL: https://github.com/faeyelab/karma.</description>
      <author>example@mail.com (Md Meftahul Ferdaus, Mahdi Abdelguerfi, Elias Ioup, Steven Sloan, Kendall N. Niles, Ken Pathak)</author>
      <guid isPermaLink="false">2508.08186v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0</title>
      <link>http://arxiv.org/abs/2508.08110v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of Interspeech 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了自监督语音表征学习模型架构对表征中学习到的语言信息的影响，特别比较了HuBERT和wav2vec 2.0两种模型在训练目标和迭代伪标签精细化方面的差异。&lt;h4&gt;背景&lt;/h4&gt;自监督语音表征学习模型因其多功能性和在下游任务上的性能而被广泛使用，但模型架构对表征中学习到的语言信息的影响尚未得到充分研究。&lt;h4&gt;目的&lt;/h4&gt;研究HuBERT和wav2vec 2.0两种模型，并比较它们架构上的两个差异：训练目标和通过多次训练迭代进行的迭代伪标签精细化。&lt;h4&gt;方法&lt;/h4&gt;分析两种自监督语音表征学习模型的隐藏表征与词身份、音素身份和说话人身份的典型相关性差异。&lt;h4&gt;主要发现&lt;/h4&gt;隐藏表征与词身份、音素身份和说话人身份的典型相关性差异是由训练迭代解释的，而不是训练目标。&lt;h4&gt;结论&lt;/h4&gt;建议未来的研究调查迭代精细化在自监督语音表征中编码语言信息有效性的原因。&lt;h4&gt;翻译&lt;/h4&gt;自监督语音表征学习模型现在因其多功能性和在下游任务上的性能而被广泛使用，但模型架构对其表征中学习的语言信息的影响仍未得到充分研究。本研究调查了两种这样的模型，HuBERT和wav2vec 2.0，并最小程度地比较了它们架构上的两个差异：训练目标和通过多次训练迭代的迭代伪标签精细化。我们发现，隐藏表征与词身份、音素身份和说话人身份的典型相关性差异是由训练迭代解释的，而不是训练目标。我们建议未来的研究调查迭代精细化在自监督语音表征中编码语言信息有效性的原因。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised models for speech representation learning now see widespreaduse for their versatility and performance on downstream tasks, but the effectof model architecture on the linguistic information learned in theirrepresentations remains under-studied. This study investigates two such models,HuBERT and wav2vec 2.0, and minimally compares two of their architecturaldifferences: training objective and iterative pseudo-label refinement throughmultiple training iterations. We find that differences in canonical correlationof hidden representations to word identity, phoneme identity, and speakeridentity are explained by training iteration, not training objective. Wesuggest that future work investigate the reason for the effectiveness ofiterative refinement in encoding linguistic information in self-supervisedspeech representations.</description>
      <author>example@mail.com (Robin Huo, Ewan Dunbar)</author>
      <guid isPermaLink="false">2508.08110v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning</title>
      <link>http://arxiv.org/abs/2508.08031v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对联邦自监督学习的不可感知且有效的后门攻击方法IPBA，解决了现有方法在隐蔽性和实用性方面的局限性&lt;h4&gt;背景&lt;/h4&gt;联邦自监督学习结合了分布式建模和无标签表示学习的优势，具有可扩展性和隐私保护潜力，但容易受到后门攻击&lt;h4&gt;目的&lt;/h4&gt;开发一种不可感知且有效的后门攻击方法，满足实际部署中的隐蔽性和实用性要求&lt;h4&gt;方法&lt;/h4&gt;IPBA通过解耦后门和增强样本的特征分布，引入Sliced-Wasserstein距离减轻后门样本的分布外特性，优化触发器生成过程&lt;h4&gt;主要发现&lt;/h4&gt;现有不可感知触发器在FSSL中面临有限的迁移性、与增强样本的特征纠缠以及分布外特性等挑战；IPBA在多个FSSL场景和数据集上显著优于现有方法&lt;h4&gt;结论&lt;/h4&gt;IPBA作为一种针对FSSL的后门攻击方法，在性能和隐蔽性上表现出色，且在各种防御机制下具有强大的鲁棒性&lt;h4&gt;翻译&lt;/h4&gt;联邦自监督学习结合了分布式建模和无标签表示学习的优势，是一种具有可扩展性和隐私保护潜力的前沿范式。尽管FSSL受到越来越多的关注，但研究表明它容易受到后门攻击。现有方法通常依赖于视觉上明显的触发器，难以满足实际部署中的隐蔽性和实用性要求。本文提出了一种针对FSSL的不可感知且有效的后门攻击方法IPBA。我们的实证研究表明，现有不可感知触发器在FSSL中面临一系列挑战，特别是有限的迁移性、与增强样本的特征纠缠以及分布外特性。这些问题共同削弱了传统后门攻击在FSSL中的有效性和隐蔽性。为了克服这些挑战，IPBA解耦了后门和增强样本的特征分布，并引入Sliced-Wasserstein距离来减轻后门样本的分布外特性，从而优化触发器生成过程。我们在多个FSSL场景和数据集上的实验结果表明，IPBA在性能上显著优于现有的后门攻击方法，并且在各种防御机制下表现出强大的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated self-supervised learning (FSSL) combines the advantages ofdecentralized modeling and unlabeled representation learning, serving as acutting-edge paradigm with strong potential for scalability and privacypreservation. Although FSSL has garnered increasing attention, researchindicates that it remains vulnerable to backdoor attacks. Existing methodsgenerally rely on visually obvious triggers, which makes it difficult to meetthe requirements for stealth and practicality in real-world deployment. In thispaper, we propose an imperceptible and effective backdoor attack method againstFSSL, called IPBA. Our empirical study reveals that existing imperceptibletriggers face a series of challenges in FSSL, particularly limitedtransferability, feature entanglement with augmented samples, andout-of-distribution properties. These issues collectively undermine theeffectiveness and stealthiness of traditional backdoor attacks in FSSL. Toovercome these challenges, IPBA decouples the feature distributions of backdoorand augmented samples, and introduces Sliced-Wasserstein distance to mitigatethe out-of-distribution properties of backdoor samples, thereby optimizing thetrigger generation process. Our experimental results on several FSSL scenariosand datasets show that IPBA significantly outperforms existing backdoor attackmethods in performance and exhibits strong robustness under various defensemechanisms.</description>
      <author>example@mail.com (Jiayao Wang, Yang Song, Zhendong Zhao, Jiale Zhang, Qilin Wu, Junwu Zhu, Dongfang Zhao)</author>
      <guid isPermaLink="false">2508.08031v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Mining the Social Fabric: Unveiling Communities for Fake News Detection in Short Videos</title>
      <link>http://arxiv.org/abs/2508.07992v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DugFND的双社区图方法，用于检测短视频中的虚假新闻。该方法通过建模上传者社区和事件驱动社区，结合异构图注意力网络和基于预训练的节点表示学习，显著提高了虚假新闻检测的性能。&lt;h4&gt;背景&lt;/h4&gt;短视频平台已成为信息分享的主要媒介，但其快速的内容生成和算法放大也导致虚假新闻广泛传播。检测短视频中的虚假新闻具有挑战性，因为它们是多模态的，且单个视频的上下文有限。现有方法主要分析内容信号（视觉、文本和音频），但常常忽略视频、上传者和事件之间的隐含关系。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法忽略视频、上传者和事件之间隐含关系的问题，提出一种新的方法来增强现有的视频分类器，提高短视频虚假新闻检测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出DugFND（用于虚假新闻检测的双社区图），通过建模两种关键社区模式来增强现有的视频分类器：1) 上传者社区：具有共同兴趣或相似内容创作模式的上传者聚集在一起；2) 事件驱动社区：与相同或语义相似公共事件相关的视频形成局部集群。构建一个连接上传者、视频和事件节点的异构图，设计了一个时间感知的异构图注意力网络来实现有效的消息传递。通过基于预训练的进一步改进节点表示学习阶段，DugFND可以应用于任何预训练的分类器。&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集上的实验表明，DugFND方法取得了显著的性能提升，证明了双社区建模对短视频虚假新闻检测的价值。&lt;h4&gt;结论&lt;/h4&gt;双社区建模对于短视频虚假新闻检测是有价值的，DugFND方法能有效提高虚假新闻检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;短视频平台已成为信息分享的主要媒介，但其快速的内容生成和算法放大也使得虚假新闻广泛传播。由于短视频的多模态性质和单个视频的有限上下文，检测短视频中的虚假新闻具有挑战性。虽然最近的方法专注于分析内容信号-视觉、文本和音频-但它们常常忽略了视频、上传者和事件之间的隐含关系。为了解决这一差距，我们提出了DugFND（用于虚假新闻检测的双社区图），一种通过建模两种关键社区模式来增强现有视频分类器的新方法：(1) 上传者社区，具有共同兴趣或相似内容创作模式的上传者聚集在一起；(2) 事件驱动社区，与相同或语义相似公共事件相关的视频形成局部集群。我们构建了一个连接上传者、视频和事件节点的异构图，并设计了一个时间感知的异构图注意力网络以实现有效的消息传递。基于预训练的进一步改进了节点表示学习。DugFND可以应用于任何预训练的分类器。公共数据集上的实验表明，我们的方法取得了显著的性能提升，证明了双社区建模对短视频虚假新闻检测的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Short video platforms have become a major medium for information sharing, buttheir rapid content generation and algorithmic amplification also enable thewidespread dissemination of fake news. Detecting misinformation in short videosis challenging due to their multi-modal nature and the limited context ofindividual videos. While recent methods focus on analyzing contentsignals-visual, textual, and audio-they often overlook implicit relationshipsamong videos, uploaders, and events. To address this gap, we propose DugFND(Dual-community graph for fake news detection), a novel method that enhancesexisting video classifiers by modeling two key community patterns: (1) uploadercommunities, where uploaders with shared interests or similar content creationpatterns group together, and (2) event-driven communities, where videos relatedto the same or semantically similar public events form localized clusters. Weconstruct a heterogeneous graph connecting uploader, video, and event nodes,and design a time-aware heterogeneous graph attention network to enableeffective message passing. A reconstruction-based pretraining phase furtherimproves node representation learning. DugFND can be applied to any pre-trainedclassifier. Experiments on public datasets show that our method achievessignificant performance gains, demonstrating the value of dual-communitymodeling for fake news detection in short videos.</description>
      <author>example@mail.com (Haisong Gong, Bolan Su, Xinrong Zhang, Jing Li, Qiang Liu, Shu Wu, Liang Wang)</author>
      <guid isPermaLink="false">2508.07992v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images</title>
      <link>http://arxiv.org/abs/2508.07847v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Deep Space Weather Model (Deep SWM)的新方法，用于太阳耀斑预测。该方法基于多个深度状态空间模型处理多通道太阳图像和长程时空依赖关系，并包含稀疏掩码自编码器这一创新预训练策略。同时建立了FlareBench基准数据集，验证了该方法在性能和可靠性上超越基线方法和人类专家。&lt;h4&gt;背景&lt;/h4&gt;准确的太阳耀斑预测对减轻关键基础设施潜在破坏至关重要，但太阳耀斑预测仍面临重大挑战。现有基于启发式物理特征的方法缺乏对太阳图像的表示学习能力，而端到端学习方法难以建模太阳图像中的长程时间依赖关系。&lt;h4&gt;目的&lt;/h4&gt;开发能够处理多通道太阳图像和长程时空依赖关系的太阳耀斑预测方法，并建立新的基准数据集验证该方法的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出Deep Space Weather Model (Deep SWM)，基于多个深度状态空间模型处理十通道太阳图像和长程时空依赖关系。采用稀疏掩码自编码器作为预训练策略，使用两阶段掩码方法保留太阳黑斑等关键区域同时压缩空间信息。建立FlareBench基准数据集，覆盖完整的11年太阳活动周期。&lt;h4&gt;主要发现&lt;/h4&gt;Deep SWM在标准指标上的性能和可靠性方面优于基线方法，甚至超过了人类专家的表现。&lt;h4&gt;结论&lt;/h4&gt;Deep SWM有效结合了深度状态空间模型和稀疏掩码自编码器的优势，成功处理了太阳图像中的长程时空依赖关系，在太阳耀斑预测任务上取得优异性能。FlareBench基准为该领域研究提供了新的评估标准。&lt;h4&gt;翻译&lt;/h4&gt;准确的太阳耀斑预测对于减轻对关键基础设施的潜在破坏至关重要，而预测太阳耀斑仍然是一个重大挑战。现有的基于启发式物理特征的方法通常缺乏对太阳图像的表示学习能力。另一方面，端到端学习方法难以建模太阳图像中的长程时间依赖关系。在本研究中，我们提出了Deep Space Weather Model (Deep SWM)，它基于多个深度状态空间模型，用于处理十通道太阳图像和长程时空依赖关系。Deep SWM还具有稀疏掩码自编码器，这是一种新的预训练策略，采用两阶段掩码方法来保留太阳黑斑等关键区域，同时压缩空间信息。此外，我们建立了FlareBench，一个新的太阳耀斑预测公共基准，覆盖完整的11年太阳活动周期，以验证我们的方法。我们的方法在性能和可靠性方面的标准指标上超越了基线方法，甚至超过了人类专家的表现。项目页面可以在https://keio-smilab25.github.io/DeepSWM找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate, reliable solar flare prediction is crucial for mitigating potentialdisruptions to critical infrastructure, while predicting solar flares remains asignificant challenge. Existing methods based on heuristic physical featuresoften lack representation learning from solar images. On the other hand,end-to-end learning approaches struggle to model long-range temporaldependencies in solar images. In this study, we propose Deep Space WeatherModel (Deep SWM), which is based on multiple deep state space models forhandling both ten-channel solar images and long-range spatio-temporaldependencies. Deep SWM also features a sparse masked autoencoder, a novelpretraining strategy that employs a two-phase masking approach to preservecrucial regions such as sunspots while compressing spatial information.Furthermore, we built FlareBench, a new public benchmark for solar flareprediction covering a full 11-year solar activity cycle, to validate ourmethod. Our method outperformed baseline methods and even human expertperformance on standard metrics in terms of performance and reliability. Theproject page can be found at https://keio-smilab25.github.io/DeepSWM.</description>
      <author>example@mail.com (Shunya Nagashima, Komei Sugiura)</author>
      <guid isPermaLink="false">2508.07847v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Topological Feature Compression for Molecular Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.07807v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种新的图神经网络架构，结合压缩的高阶拓扑信号和标准分子特征，在保持计算效率和可解释性的同时实现了优异的预测性能。&lt;h4&gt;背景&lt;/h4&gt;分子表征学习虽有进展，但在平衡预测准确性、可解释性和计算效率的同时提取通用化学见解仍是重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能捕获全局几何信息同时保持计算效率和人类可解释结构的图神经网络架构。&lt;h4&gt;方法&lt;/h4&gt;引入一种结合压缩高阶拓扑信号与标准分子特征的新型图神经网络架构，在保持计算可处理性和可解释结构的同时捕获全局几何信息。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在从小分子到复杂数据材料集的一系列基准测试中表现出优越性能，使用参数高效架构，在几乎所有基准测试中准确性和鲁棒性均达到最佳结果。&lt;h4&gt;结论&lt;/h4&gt;所提出的图神经网络架构成功地将高阶拓扑信息与标准分子特征结合，实现了多数据集上的高性能，同时保持了计算效率和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;分子表征学习的最新进展为许多化学信息学和生物信息学任务产生了高效的分子编码。然而，在平衡预测准确性、可解释性和计算效率的同时提取通用化学见解仍然是一个重大挑战。在这项工作中，我们引入了一种新颖的图神经网络(GNN)架构，该架构结合了压缩的高阶拓扑信号和标准分子特征。我们的方法在保持计算效率和人类可解释结构的同时捕获了全局几何信息。我们在从小分子数据集到复杂数据材料集的一系列基准测试中评估了我们的模型，并展示了使用参数高效架构的优越性能。我们在几乎所有基准测试中实现了最佳的性能和鲁棒性结果。我们将所有代码开源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in molecular representation learning have produced highlyeffective encodings of molecules for numerous cheminformatics andbioinformatics tasks. However, extracting general chemical insight whilebalancing predictive accuracy, interpretability, and computational efficiencyremains a major challenge. In this work, we introduce a novel Graph NeuralNetwork (GNN) architecture that combines compressed higher-order topologicalsignals with standard molecular features. Our approach captures globalgeometric information while preserving computational tractability andhuman-interpretable structure. We evaluate our model across a range ofbenchmarks, from small-molecule datasets to complex material datasets, anddemonstrate superior performance using a parameter-efficient architecture. Weachieve the best performing results in both accuracy and robustness acrossalmost all benchmarks. We open source all code \footnote{All code and resultscan be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.</description>
      <author>example@mail.com (Rahul Khorana)</author>
      <guid isPermaLink="false">2508.07807v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation</title>
      <link>http://arxiv.org/abs/2508.07649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为DiMuST的社会增强兴趣点(POI)推荐模型，基于多空间-时间转换图的解耦表示学习，解决了现有方法中空间和时间转换单独建模导致的表示不一致问题。&lt;h4&gt;背景&lt;/h4&gt;下一个兴趣点(POI)推荐是商业智能领域的研究热点，用户的空间-时间转换和社会关系在其中起着关键作用。然而，大多数现有工作将空间和时间转换分开建模，导致相同空间-时间关键节点的表示不一致。&lt;h4&gt;目的&lt;/h4&gt;解决现有POI推荐方法中空间和时间转换单独建模导致的表示不一致问题，减少融合过程中的冗余信息，降低模型不确定性，提高模型可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出DiMuST模型，采用创新的解耦变分多层图自编码器(DAE)，首先使用多层空间-时间图策略解耦共享和私有分布，然后通过专家乘积(PoE)机制融合共享特征，通过对比约束去噪私有特征，有效捕获POI的空间-时间转换表示，同时保持其空间-时间关系的内在相关性。&lt;h4&gt;主要发现&lt;/h4&gt;在两个具有挑战性的数据集上进行的实验表明，DiMuST在多个评估指标上显著优于现有的推荐方法，证明了其有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;DiMuST通过解耦表示学习和多空间-时间图策略，成功解决了POI推荐中空间和时间转换建模不一致的问题，提高了推荐性能和模型可解释性，为POI推荐领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;下一个兴趣点(POI)推荐是商业智能领域的研究热点，用户的空间-时间转换和社会关系在其中起着关键作用。然而，大多数现有工作将空间和时间转换分开建模，导致相同空间-时间关键节点的表示不一致。这种不一致在融合过程中引入冗余信息，增加模型不确定性，降低可解释性。为解决这个问题，我们提出了DiMuST，一种基于多空间-时间转换图解耦表示学习的社会增强POI推荐模型。该模型采用一种新的解耦变分多层图自编码器(DAE)，首先使用多层空间-时间图策略解耦共享和私有分布，然后通过专家乘积(PoE)机制融合共享特征，通过对比约束去噪私有特征。该模型有效捕获了POI的空间-时间转换表示，同时保留了其空间-时间关系的内在相关性。在两个具有挑战性的数据集上的实验表明，我们的DiMuST在多个指标上显著优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next Point-of-Interest (POI) recommendation is a research hotspot in businessintelligence, where users' spatial-temporal transitions and socialrelationships play key roles. However, most existing works model spatial andtemporal transitions separately, leading to misaligned representations of thesame spatial-temporal key nodes. This misalignment introduces redundantinformation during fusion, increasing model uncertainty and reducinginterpretability. To address this issue, we propose DiMuST, a socially enhancedPOI recommendation model based on disentangled representation learning overmultiplex spatial-temporal transition graphs. The model employs a novelDisentangled variational multiplex graph Auto-Encoder (DAE), which firstdisentangles shared and private distributions using a multiplexspatial-temporal graph strategy. It then fuses the shared features via aProduct of Experts (PoE) mechanism and denoises the private features throughcontrastive constraints. The model effectively captures the spatial-temporaltransition representations of POIs while preserving the intrinsic correlationof their spatial-temporal relationships. Experiments on two challengingdatasets demonstrate that our DiMuST significantly outperforms existing methodsacross multiple metrics.</description>
      <author>example@mail.com (Jie Li, Haoye Dong, Zhengyang Wu, Zetao Zheng, Mingrong Lin)</author>
      <guid isPermaLink="false">2508.07649v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>UniFlow: Unifying Speech Front-End Tasks via Continuous Generative Modeling</title>
      <link>http://arxiv.org/abs/2508.07558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  extended version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为UniFlow的统一框架，该框架使用连续生成模型在共享潜在空间中处理多样化的语音前端任务，展示了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;生成模型在图像、视频和音频领域取得了显著成功，但语音前端任务（如语音增强、目标说话人提取、声学回声消除和语言查询源分离）仍然使用分散的、特定于任务的解决方案，导致冗余工程努力、不一致性能和有限扩展性。&lt;h4&gt;目的&lt;/h4&gt;引入UniFlow统一框架，采用连续生成模型在共享潜在空间中处理多样化语音前端任务，解决现有方法的碎片化问题。&lt;h4&gt;方法&lt;/h4&gt;UniFlow使用波形变分自编码器学习原始音频的紧凑潜在表示，结合扩散Transformer预测潜在更新；使用任务ID索引的可学习条件嵌入实现最大参数共享同时保持任务特定适应性；研究和比较了三种生成目标：去噪扩散、流匹配和均值流。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共基准上验证UniFlow，结果显示其性能优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;UniFlow的统一潜在公式和条件设计使其易于扩展到新任务，为构建和扩展生成式语音处理管道提供了集成基础，作者将开源代码库以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;生成建模最近在图像、视频和音频领域取得了显著成功，展示了统一表示学习的强大能力。然而，语音前端任务，如语音增强、目标说话人提取、声学回声消除和语言查询源分离，仍然主要由分散的、特定于任务的解决方案处理。这种碎片化导致了冗余的工程努力、不一致的性能和有限的扩展性。为了解决这一差距，我们引入了UniFlow，一个统一框架，它采用连续生成模型在共享潜在空间中处理多样化的语音前端任务。具体来说，UniFlow使用波形变分自编码器学习原始音频的紧凑潜在表示，并结合一个预测潜在更新的扩散Transformer。为了在训练期间区分语音处理任务，使用了由任务ID索引的可学习条件嵌入，以实现最大的参数共享同时保持任务特定的适应性。为了平衡模型性能和计算效率，我们在潜在域内研究和比较了三种生成目标：去噪扩散、流匹配和均值流。我们在多个公共基准上验证了UniFlow，展示了优于最先进基线的一致性提升。UniFlow的统一潜在公式和条件设计使其易于扩展到新任务，为构建和扩展生成式语音处理管道提供了集成基础。为了促进未来研究，我们将开源我们的代码库。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling has recently achieved remarkable success across image,video, and audio domains, demonstrating powerful capabilities for unifiedrepresentation learning. Yet speech front-end tasks such as speech enhancement(SE), target speaker extraction (TSE), acoustic echo cancellation (AEC), andlanguage-queried source separation (LASS) remain largely tackled by disparate,task-specific solutions. This fragmentation leads to redundant engineeringeffort, inconsistent performance, and limited extensibility. To address thisgap, we introduce UniFlow, a unified framework that employs continuousgenerative modeling to tackle diverse speech front-end tasks in a shared latentspace. Specifically, UniFlow utilizes a waveform variational autoencoder (VAE)to learn a compact latent representation of raw audio, coupled with a DiffusionTransformer (DiT) that predicts latent updates. To differentiate the speechprocessing task during the training, learnable condition embeddings indexed bya task ID are employed to enable maximal parameter sharing while preservingtask-specific adaptability. To balance model performance and computationalefficiency, we investigate and compare three generative objectives: denoisingdiffusion, flow matching, and mean flow within the latent domain. We validateUniFlow on multiple public benchmarks, demonstrating consistent gains overstate-of-the-art baselines. UniFlow's unified latent formulation andconditional design make it readily extensible to new tasks, providing anintegrated foundation for building and scaling generative speech processingpipelines. To foster future research, we will open-source our codebase.</description>
      <author>example@mail.com (Ziqian Wang, Zikai Liu, Yike Zhu, Xingchen Li, Boyi Kang, Jixun Yao, Xianjun Xia, Chuanzeng Huang, Lei Xie)</author>
      <guid isPermaLink="false">2508.07558v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>FairDRL-ST: Disentangled Representation Learning for Fair Spatio-Temporal Mobility Prediction</title>
      <link>http://arxiv.org/abs/2508.07518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a Research Paper (short) at ACM SIGSPATIAL 2025. This  arXiv version is the full version of the paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于解耦表示学习的新框架FairDRL-ST，用于解决时空预测中的公平性问题，特别是在移动性需求预测方面，通过无监督方式实现公平性并最小化性能损失。&lt;h4&gt;背景&lt;/h4&gt;深度时空神经网络在城市计算中的应用日益广泛，直接影响公共交通、紧急服务和交通管理系统等关键城市基础设施的用户。虽然许多方法关注提高准确性，但公平性近期受到重视，因为时空应用中的有偏见预测可能不成比例地损害特定人口统计或地理群体，加剧社会经济不平等，影响AI在公共服务中的道德部署。&lt;h4&gt;目的&lt;/h4&gt;开发一个名为FairDRL-ST的新框架，解决时空预测中的公平性问题，特别关注移动性需求预测。&lt;h4&gt;方法&lt;/h4&gt;基于解耦表示学习的框架，利用对抗学习和解耦表示学习技术，学习分离包含敏感信息的属性。与现有通过监督学习强制实现公平性的方法不同，该框架采用无监督方式实现公平性，避免了过度补偿和性能下降。&lt;h4&gt;主要发现&lt;/h4&gt;将框架应用于真实世界城市移动性数据集，结果表明该方法能够缩小公平差距，同时与最先进的公平感知方法相比提供具有竞争力的预测性能。&lt;h4&gt;结论&lt;/h4&gt;FairDRL-ST框架能够在保持预测性能的同时有效实现公平性，无监督方法比监督方法在处理公平性问题时更为有效。&lt;h4&gt;翻译&lt;/h4&gt;随着深度时空神经网络在城市计算环境中日益广泛应用，此类方法的部署可以直接影响关键城市基础设施用户，如公共交通、紧急服务和交通管理系统。虽然许多时空方法专注于提高准确性，但公平性最近受到关注，因为越来越多证据表明时空应用中的有偏见预测可能会不成比例地损害某些人口统计或地理群体，从而加剧现有的社会经济不平等，并削弱人工智能在公共服务中的道德部署。在本文中，我们提出了一种名为FairDRL-ST的新框架，基于解耦表示学习，解决时空预测中的公平性问题，特别关注移动性需求预测。通过利用对抗学习和解耦表示学习，我们的框架学习分离包含敏感信息的属性。与现有通过监督学习强制实现公平性可能导致过度补偿和性能下降的方法不同，我们的框架以无监督方式实现公平性，且性能损失最小。我们将框架应用于真实世界城市移动性数据集，证明了其缩小公平差距的能力，同时与最先进的公平感知方法相比提供具有竞争力的预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As deep spatio-temporal neural networks are increasingly utilised in urbancomputing contexts, the deployment of such methods can have a direct impact onusers of critical urban infrastructure, such as public transport, emergencyservices, and traffic management systems. While many spatio-temporal methodsfocus on improving accuracy, fairness has recently gained attention due togrowing evidence that biased predictions in spatio-temporal applications candisproportionately disadvantage certain demographic or geographic groups,thereby reinforcing existing socioeconomic inequalities and undermining theethical deployment of AI in public services. In this paper, we propose a novelframework, FairDRL-ST, based on disentangled representation learning, toaddress fairness concerns in spatio-temporal prediction, with a particularfocus on mobility demand forecasting. By leveraging adversarial learning anddisentangled representation learning, our framework learns to separateattributes that contain sensitive information. Unlike existing methods thatenforce fairness through supervised learning, which may lead toovercompensation and degraded performance, our framework achieves fairness inan unsupervised manner with minimal performance loss. We apply our framework toreal-world urban mobility datasets and demonstrate its ability to closefairness gaps while delivering competitive predictive performance compared tostate-of-the-art fairness-aware methods.</description>
      <author>example@mail.com (Sichen Zhao, Wei Shao, Jeffrey Chan, Ziqi Xu, Flora Salim)</author>
      <guid isPermaLink="false">2508.07518v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification</title>
      <link>http://arxiv.org/abs/2508.07465v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为MOTGNN的新型多组学整合框架，用于二元疾病分类，通过结合XGBoost、图神经网络和深度前馈网络，实现了高准确率和可解释性。&lt;h4&gt;背景&lt;/h4&gt;整合多组学数据（如DNA甲基化、mRNA表达和microRNA表达）可以全面了解疾病的生物学机制，但高维度和组学层之间的复杂交互给预测建模带来挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新颖且可解释的框架，用于二元疾病分类，解决多组学数据整合中的高维度和复杂交互问题。&lt;h4&gt;方法&lt;/h4&gt;MOTGNN框架使用XGBoost进行组学特定的监督图构建，采用模态特定的图神经网络进行分层表示学习，并通过深度前馈网络实现跨组学整合。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界疾病数据集上，MOTGNN在准确率、ROC-AUC和F1分数方面比最先进的基线方法高出5-10%；在严重类别不平衡情况下保持稳健（F1分数87.2% vs 33.4%）；通过稀疏图保持计算效率；提供内置可解释性，揭示顶级生物标志物和各组学模态的相对贡献。&lt;h4&gt;结论&lt;/h4&gt;MOTGNN在多组学疾病建模中显著提高了预测准确性和可解释性，具有很大的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;整合多组学数据，如DNA甲基化、mRNA表达和microRNA（miRNA）表达，可以全面了解疾病背后的生物学机制。然而，组学数据的高维度和组学层之间的复杂交互给预测建模带来了主要挑战。我们提出了多组学整合与树生成图神经网络（MOTGNN），这是一种新颖且可解释的二元疾病分类框架。MOTGNN采用eXtreme Gradient Boosting（XGBoost）进行组学特定的监督图构建，然后使用模态特定的图神经网络进行分层表示学习，最后使用深度前馈网络进行跨组学整合。在三个真实世界疾病数据集上，MOTGNN在准确率、ROC-AUC和F1分数方面比最先进的基线方法高出5-10%，并且在严重类别不平衡的情况下保持稳健（例如在不平衡数据上F1分数为87.2% vs 33.4%）。模型通过稀疏图（每个节点2.1-2.8条边）保持计算效率，并提供内置可解释性，揭示顶级生物标志物和每个组学模态的相对贡献。这些结果突显了MOTGNN在提高多组学疾病建模预测准确性和可解释性方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating multi-omics data, such as DNA methylation, mRNA expression, andmicroRNA (miRNA) expression, offers a comprehensive view of the biologicalmechanisms underlying disease. However, the high dimensionality and complexinteractions among omics layers present major challenges for predictivemodeling. We propose Multi-Omics integration with Tree-generated Graph NeuralNetwork (MOTGNN), a novel and interpretable framework for binary diseaseclassification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to performomics-specific supervised graph construction, followed by modality-specificGraph Neural Networks (GNNs) for hierarchical representation learning, and adeep feedforward network for cross-omics integration. On three real-worlddisease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% inaccuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance(e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintainscomputational efficiency through sparse graphs (2.1-2.8 edges per node) andprovides built-in interpretability, revealing both top-ranked biomarkers andthe relative contributions of each omics modality. These results highlightMOTGNN's potential to improve both predictive accuracy and interpretability inmulti-omics disease modeling.</description>
      <author>example@mail.com (Tiantian Yang, Zhiqian Chen)</author>
      <guid isPermaLink="false">2508.07465v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Stackelberg Coupling of Online Representation Learning and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2508.07452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SCORER的新框架，通过将感知和控制网络之间的交互建模为Stackelberg博弈，改进了深度强化学习中的表示学习和策略学习，提高了样本效率和最终性能，而无需复杂的辅助目标或架构。&lt;h4&gt;背景&lt;/h4&gt;深度强化学习中的端到端学习是基础，但面对稀疏奖励信号时，学习有效特征具有挑战性。最近研究趋势是添加复杂辅助目标或完全解耦表示学习和策略学习过程，但这增加了设计复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一种替代方案，既不是完全解耦也不是简单端到端学习，而是通过原则性的博弈论动态结构来组织和控制感知与控制网络之间的交互，从而提高性能。&lt;h4&gt;方法&lt;/h4&gt;引入Stackelberg耦合表示和强化学习(SCORER)框架，将感知网络(领导者)和控制网络(跟随者)之间的交互建模为Stackelberg博弈。感知网络战略性地学习特征以有利于控制网络，控制网络的目标是最小化其Bellman误差。使用双时间尺度算法近似博弈均衡。&lt;h4&gt;主要发现&lt;/h4&gt;在标准DQN变体和基准任务上应用SCORER，提高了样本效率和最终性能。结果表明，可以通过对感知-控制动态的原则性算法设计实现性能提升，无需复杂辅助目标或架构。&lt;h4&gt;结论&lt;/h4&gt;通过原则性地设计感知-控制动态的算法，可以实现显著的性能提升，而无需依赖复杂的辅助目标或架构。&lt;h4&gt;翻译&lt;/h4&gt;端到端的表示学习和策略学习的集成仍然是深度强化学习(RL)的基石。然而，为了应对从稀疏奖励信号中学习有效特征的挑战，最近的趋势已转向添加复杂的辅助目标或完全解耦这两个过程，但这通常以增加设计复杂性为代价。本文提出了一种替代完全解耦和简单端到端学习的方法，认为通过使用原则性的博弈论动态结构来组织和控制不同感知和控制网络之间的交互，可以显著提高性能。我们通过引入Stackelberg耦合表示和强化学习(SCORER)框架来形式化这种动态，该框架将感知和控制之间的交互建模为Stackelberg博弈。感知网络(领导者)战略性地学习特征以有利于控制网络(跟随者)，而控制网络的目标是最小化其Bellman误差。我们使用实用的双时间尺度算法来近似博弈的均衡。在基准任务上的标准DQN变体应用中，SCORER提高了样本效率和最终性能。我们的结果表明，可以通过对感知-控制动态的原则性算法设计实现性能提升，而无需复杂的辅助目标或架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrated, end-to-end learning of representations and policies remains acornerstone of deep reinforcement learning (RL). However, to address thechallenge of learning effective features from a sparse reward signal, recenttrends have shifted towards adding complex auxiliary objectives or fullydecoupling the two processes, often at the cost of increased design complexity.This work proposes an alternative to both decoupling and naive end-to-endlearning, arguing that performance can be significantly improved by structuringthe interaction between distinct perception and control networks with aprincipled, game-theoretic dynamic. We formalize this dynamic by introducingthe Stackelberg Coupled Representation and Reinforcement Learning (SCORER)framework, which models the interaction between perception and control as aStackelberg game. The perception network (leader) strategically learns featuresto benefit the control network (follower), whose own objective is to minimizeits Bellman error. We approximate the game's equilibrium with a practicaltwo-timescale algorithm. Applied to standard DQN variants on benchmark tasks,SCORER improves sample efficiency and final performance. Our results show thatperformance gains can be achieved through principled algorithmic design of theperception-control dynamic, without requiring complex auxiliary objectives orarchitectures.</description>
      <author>example@mail.com (Fernando Martinez, Tao Li, Yingdong Lu, Juntao Chen)</author>
      <guid isPermaLink="false">2508.07452v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Statistical Inference for Autoencoder-based Anomaly Detection after Representation Learning-based Domain Adaptation</title>
      <link>http://arxiv.org/abs/2508.07049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了STAND-DA框架，用于在领域适应后进行统计严谨的自编码器异常检测，通过选择性推断计算有效p值并控制假阳性率。&lt;h4&gt;背景&lt;/h4&gt;异常检测在多个领域至关重要，但应用于数据有限的目标域时性能会下降。领域适应可通过从数据丰富的源域转移知识来解决这个问题，但适应过程会引入不确定性，使难以得出统计有效的结论。&lt;h4&gt;目的&lt;/h4&gt;开发一个统计严谨的异常检测框架，能够在领域适应后提供有效的统计推断，控制假阳性率。&lt;h4&gt;方法&lt;/h4&gt;STAND-DA建立在选择性推断框架上，计算检测到的异常的有效p值，将假阳性率控制在预定义水平α以下。开发GPU加速的SI实现以提高可扩展性和运行时性能，使SI适用于现代大规模深度架构。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实世界数据集上的广泛实验验证了STAND-DA方法的计算效率和理论结果，证明其在领域适应后能提供统计严谨的异常检测。&lt;h4&gt;结论&lt;/h4&gt;STAND-DA成功解决了深度学习模型应用选择性推断的计算挑战，为领域适应后的异常检测提供了统计严谨的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;异常检测在广泛领域中发挥着重要作用，但当应用于数据有限的目标域时，其性能可能会下降。领域适应通过从数据丰富的相关源域转移知识提供了解决方案。然而，这种适应过程会引入额外的不确定性，使得难以从异常检测结果中得出统计上有效的结论。在本文中，我们提出了STAND-DA——一个用于在基于表示学习的领域适应之后进行统计严谨的自编码器异常检测的新框架。建立在选择性推断框架之上，STAND-DA计算检测到的异常的有效p值，并将假阳性率严格控制在预定义的水平以下。为解决将选择性推断应用于深度学习模型的计算挑战，我们开发了GPU加速的实现，显著提高了可扩展性和运行时性能。这一进展使选择性推断在现代大规模深度架构中实际可行。在合成和真实世界数据集上的广泛实验验证了所提出的STAND-DA方法的计算效率和理论结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection (AD) plays a vital role across a wide range of domains, butits performance might deteriorate when applied to target domains with limiteddata. Domain Adaptation (DA) offers a solution by transferring knowledge from arelated source domain with abundant data. However, this adaptation process canintroduce additional uncertainty, making it difficult to draw statisticallyvalid conclusions from AD results. In this paper, we propose STAND-DA -- anovel framework for statistically rigorous Autoencoder-based AD afterRepresentation Learning-based DA. Built on the Selective Inference (SI)framework, STAND-DA computes valid $p$-values for detected anomalies andrigorously controls the false positive rate below a pre-specified level$\alpha$ (e.g., 0.05). To address the computational challenges of applying SIto deep learning models, we develop the GPU-accelerated SI implementation,significantly enhancing both scalability and runtime performance. Thisadvancement makes SI practically feasible for modern, large-scale deeparchitectures. Extensive experiments on synthetic and real-world datasetsvalidate the theoretical results and computational efficiency of the proposedSTAND-DA method.</description>
      <author>example@mail.com (Tran Tuan Kiet, Nguyen Thang Loi, Vo Nguyen Le Duy)</author>
      <guid isPermaLink="false">2508.07049v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting</title>
      <link>http://arxiv.org/abs/2508.06915v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了QuiZSF框架，将检索增强生成(RAG)与时间序列预训练模型(TSPMs)结合，以增强零样本时间序列预测能力，在数据稀缺场景下表现优异。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测对支持流数据应用至关重要，而零样本预测在数据稀缺场景(如领域转移或极端条件)下极具价值但传统模型难以处理。虽然TSPMs在ZSF中表现良好，但缺乏动态整合外部知识的机制。&lt;h4&gt;目的&lt;/h4&gt;将RAG引入TSPMs以增强零样本时间序列预测，开发一个轻量级且模块化的框架，耦合高效检索、表示学习和模型适应。&lt;h4&gt;方法&lt;/h4&gt;提出QuiZSF框架，包含分层树结构的ChronoRAG Base(CRB)用于时间序列存储和检索，多粒度序列交互学习器(MSIL)提取关系特征，以及双分支模型协作器(MCC)对齐检索知识与两类TSPMs。&lt;h4&gt;主要发现&lt;/h4&gt;与当代基线相比，QuiZSF以非LLM型和LLM型TSPMs为基础时，分别在75%和87.5%的预测设置中排名第一，同时保持内存和推理时间的高效率。&lt;h4&gt;结论&lt;/h4&gt;QuiZSF成功结合RAG和TSPMs优势，显著提升零样本时间序列预测性能，在保持高效的同时在大多数预测设置中超越现有方法。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测已成为支持各种流数据应用的重要技术。零样本时间序列预测(ZSF)在数据稀缺场景(如领域转移或极端条件预测)下特别有价值，但传统模型难以处理这一问题。虽然时间序列预训练模型(TSPMs)在ZSF中表现出色，但它们通常缺乏动态整合外部知识的机制。幸运的是，新兴的检索增强生成(RAG)为按需注入此类知识提供了有前景的路径，但它们很少与TSPMs集成。为了结合两者的优势，我们将RAG引入TSPMs以增强零样本时间序列预测。在本文中，我们提出了QuiZSF(快速零样本时间序列预测器)，这是一个轻量级且模块化的框架，将高效检索与表示学习和模型适应相结合用于ZSF。具体而言，我们构建了分层树结构的ChronoRAG Base(CRB)用于可扩展的时间序列存储和领域感知检索，引入了多粒度序列交互学习器(MSIL)来提取细粒度和粗粒度关系特征，并开发了双分支模型协作器(MCC)，将检索到的知识与两种TSPMs对齐：非LLM型和LLM型。与当代基线相比，QuiZSF分别以非LLM型和LLM型TSPMs作为基础模型，在75%和87.5%的预测设置中排名第一，同时在内存和推理时间方面保持高效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting has become increasingly important to empower diverseapplications with streaming data. Zero-shot time-series forecasting (ZSF),particularly valuable in data-scarce scenarios, such as domain transfer orforecasting under extreme conditions, is difficult for traditional models todeal with. While time series pre-trained models (TSPMs) have demonstratedstrong performance in ZSF, they often lack mechanisms to dynamicallyincorporate external knowledge. Fortunately, emerging retrieval-augmentedgeneration (RAG) offers a promising path for injecting such knowledge ondemand, yet they are rarely integrated with TSPMs. To leverage the strengths ofboth worlds, we introduce RAG into TSPMs to enhance zero-shot time seriesforecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time SeriesForecaster), a lightweight and modular framework that couples efficientretrieval with representation learning and model adaptation for ZSF.Specifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB)for scalable time-series storage and domain-aware retrieval, introduce aMulti-grained Series Interaction Learner (MSIL) to extract fine- andcoarse-grained relational features, and develop a dual-branch Model CooperationCoherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLMbased and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLMbased and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and87.5% of prediction settings, while maintaining high efficiency in memory andinference time.</description>
      <author>example@mail.com (Shichao Ma, Zhengyang Zhou, Qihe Huang, Binwu Wang, Kuo Yang, Huan Li, Yang Wang)</author>
      <guid isPermaLink="false">2508.06915v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos</title>
      <link>http://arxiv.org/abs/2508.06902v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对短视频情绪分析(VEA)的挑战，提出了大规模数据集eMotions和新型网络架构AV-CANet。研究解决了短视频多模态复杂性带来的语义差距和音频-视觉表达不一致等问题，并通过实验验证了方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;短视频已成为在线信息获取和分享的重要部分，其多模态复杂性给视频分析带来新挑战。现有研究主要关注有明显情绪线索的视频，而短视频内容多样性大、语义差距明显，且音频-视觉共同表达普遍存在，导致情绪分析困难。&lt;h4&gt;目的&lt;/h4&gt;创建大规模短视频情绪数据集eMotions，并开发有效的短视频情绪分析方法AV-CANet，解决短视频情绪分析中的语义差距和音频-视觉表达不一致等问题。&lt;h4&gt;方法&lt;/h4&gt;构建包含27,996个视频的eMotions数据集，采用多阶段标注流程确保质量；提出AV-CANet网络架构，利用视频transformer捕获语义表示，引入局部-全局融合模块处理音频-视觉特征关联，并设计EP-CE损失函数进行全局优化。&lt;h4&gt;主要发现&lt;/h4&gt;短视频内容多样性导致更明显的语义差距，音频-视觉共同表达的普遍性造成局部偏差和集体信息差距；在三个eMotions相关数据集和四个公共VEA数据集上的实验验证了AV-CANet的有效性。&lt;h4&gt;结论&lt;/h4&gt;AV-CANet能有效处理短视频情绪分析中的挑战，为未来研究提供新见解；消融研究确认了方法关键组件的重要性；数据集和代码将在Github上公开共享。&lt;h4&gt;翻译&lt;/h4&gt;短视频已成为我们日常在线获取和分享信息的重要部分。其多模态复杂性给视频分析带来了新挑战，突显了社区对视频情绪分析(VEA)的需求。鉴于短视频情绪数据的有限可用性，我们引入了eMotions，这是一个包含27,996个具有完整标注视频的大规模数据集。为确保质量和减少主观偏见，我们强调更好的人员配置并提出了多阶段标注流程。此外，通过有针对性的采样，我们提供了类别平衡和测试导向的变体以满足不同需求。虽然已有大量研究关注有明显情绪线索的视频(如面部表情)，但分析短视频中的情绪仍然是一项具有挑战性的任务。挑战源于内容多样性更广，这引入了更明显的语义差距，并使情绪相关特征的表示学习复杂化。此外，短视频中音频-视觉共同表达的普遍性导致了由情绪表达不一致引起的局部偏差和集体信息差距。为解决这一问题，我们提出了AV-CANet，这是一种利用视频transformer捕获语义相关表示的端到端音频-视觉融合网络。我们进一步引入了局部-全局融合模块，旨在逐步捕获音频-视觉特征的关联。此外，构建了EP-CE损失函数，使用三极惩罚进行全局优化引导。在三个eMotions相关数据集和四个公共VEA数据集上的广泛实验证明了我们提出的AV-CANet的有效性，同时为未来研究提供了广泛的见解。此外，我们进行了消融研究以检查我们方法的关键组件。数据集和代码将在Github上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Short-form videos (SVs) have become a vital part of our online routine foracquiring and sharing information. Their multimodal complexity poses newchallenges for video analysis, highlighting the need for video emotion analysis(VEA) within the community. Given the limited availability of SVs emotion data,we introduce eMotions, a large-scale dataset consisting of 27,996 videos withfull-scale annotations. To ensure quality and reduce subjective bias, weemphasize better personnel allocation and propose a multi-stage annotationprocedure. Additionally, we provide the category-balanced and test-orientedvariants through targeted sampling to meet diverse needs. While there have beensignificant studies on videos with clear emotional cues (e.g., facialexpressions), analyzing emotions in SVs remains a challenging task. Thechallenge arises from the broader content diversity, which introduces moredistinct semantic gaps and complicates the representations learning ofemotion-related features. Furthermore, the prevalence of audio-visualco-expressions in SVs leads to the local biases and collective information gapscaused by the inconsistencies in emotional expressions. To tackle this, wepropose AV-CANet, an end-to-end audio-visual fusion network that leveragesvideo transformer to capture semantically relevant representations. We furtherintroduce the Local-Global Fusion Module designed to progressively capture thecorrelations of audio-visual features. Besides, EP-CE Loss is constructed toglobally steer optimizations with tripolar penalties. Extensive experimentsacross three eMotions-related datasets and four public VEA datasets demonstratethe effectiveness of our proposed AV-CANet, while providing broad insights forfuture research. Moreover, we conduct ablation studies to examine the criticalcomponents of our method. Dataset and code will be made available at Github.</description>
      <author>example@mail.com (Xuecheng Wu, Dingkang Yang, Danlei Huang, Xinyi Yin, Yifan Wang, Jia Zhang, Jiayu Nie, Liangyu Fu, Yang Liu, Junxiao Xue, Hadi Amirpour, Wei Zhou)</author>
      <guid isPermaLink="false">2508.06902v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>A Joint Sparse Self-Representation Learning Method for Multiview Clustering</title>
      <link>http://arxiv.org/abs/2508.06857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的联合稀疏自表示学习模型用于多视图聚类，通过引入基数约束而非图拉普拉斯正则化来提取视图特定的局部信息，并开发了具有全局收敛性的交替二次惩罚方法来解决算法收敛问题。实验结果表明，与八种最先进算法相比，所提出的方法在六个标准数据集上表现更优越。&lt;h4&gt;背景&lt;/h4&gt;多视图聚类旨在利用不同视图间的一致性和互补信息对样本进行分组。作为多视图聚类的基本技术，子空间聚类已受到广泛关注。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型的联合稀疏自表示学习模型用于多视图聚类，解决基于增广拉格朗日法的交替最小化算法在非凸、非光滑模型中不能保证收敛的问题，提高方法的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出一种联合稀疏自表示学习模型，通过引入基数约束而非图拉普拉斯正则化来提取视图特定的局部信息。在每个视图下，基数约束限制自表示阶段使用的样本，以提取可靠的局部和全局结构信息，同时低秩约束有助于在合并过程中揭示共识亲和矩阵中的全局相干结构。为解决算法收敛问题，开发了具有全局收敛性的交替二次惩罚方法，通过闭式解迭代求解两个子问题。&lt;h4&gt;主要发现&lt;/h4&gt;在六个标准数据集上的经验结果表明，与八种最先进算法相比，所提出的模型和方法具有优越性。这表明基于基数约束的局部信息提取和交替二次惩罚方法能有效提高多视图聚类的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的联合稀疏自表示学习模型和交替二次惩罚方法能够有效处理多视图聚类问题，通过提取视图特定的局部信息和保证算法的全局收敛性，显著提高了聚类性能，为多视图聚类研究提供了新的思路和方法。&lt;h4&gt;翻译&lt;/h4&gt;多视图聚类旨在利用各种视图间的一致性和互补信息对样本进行分组。作为多视图聚类的基本技术，子空间聚类已引起广泛关注。在本文中，我们提出了一种用于多视图聚类的新型联合稀疏自表示学习模型，其特点是引入基数约束而非图拉普拉斯正则化来提取视图特定的局部信息。具体而言，在每个视图下，基数约束直接限制自表示阶段使用的样本，以提取可靠的局部和全局结构信息，同时低秩约束有助于在合并过程中揭示共识亲和矩阵中的全局相干结构。伴随的挑战是，基于增广拉格朗日法的交替最小化算法不能保证直接应用于非凸、非光滑模型时的收敛性，从而导致泛化能力差。为解决这一问题，我们开发了一种具有全局收敛性的交替二次惩罚方法，通过闭式解迭代求解两个子问题。在六个标准数据集上的经验结果表明，与八种最先进算法相比，我们的模型和方法具有优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multiview clustering (MC) aims to group samples using consistent andcomplementary information across various views. The subspace clustering, as afundamental technique of MC, has attracted significant attention. In thispaper, we propose a novel joint sparse self-representation learning model forMC, where a featured difference is the extraction of view-specific localinformation by introducing cardinality (i.e., $\ell_0$-norm) constraintsinstead of Graph-Laplacian regularization. Specifically, under each view,cardinality constraints directly restrict the samples used in theself-representation stage to extract reliable local and global structureinformation, while the low-rank constraint aids in revealing a global coherentstructure in the consensus affinity matrix during merging. The attendantchallenge is that Augmented Lagrange Method (ALM)-based alternatingminimization algorithms cannot guarantee convergence when applied directly toour nonconvex, nonsmooth model, thus resulting in poor generalization ability.To address it, we develop an alternating quadratic penalty (AQP) method withglobal convergence, where two subproblems are iteratively solved by closed-formsolutions. Empirical results on six standard datasets demonstrate thesuperiority of our model and AQP method, compared to eight state-of-the-artalgorithms.</description>
      <author>example@mail.com (Mengxue Jia, Zhihua Allen-Zhao, You Zhao, Sanyang Liu)</author>
      <guid isPermaLink="false">2508.06857v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Geometry-Aware Spiking Graph Neural Network</title>
      <link>http://arxiv.org/abs/2508.06793v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种几何感知的脉冲图神经网络(GSG)，结合了脉冲神经网络的能效性和图神经网络在复杂图结构建模方面的优势，通过在黎曼流形上进行自适应表示学习，解决了现有方法在欧几里得空间中无法有效建模层次结构和循环等复杂图结构的问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在建模图结构数据方面表现出色，而脉冲神经网络(SNNs)通过稀疏、事件驱动的计算提供高能效。然而，现有的脉冲图神经网络主要在欧几里得空间运行，依赖固定的几何假设，限制了它们对复杂图结构如层次结构和循环的建模能力。&lt;h4&gt;目的&lt;/h4&gt;克服现有脉冲图神经网络的局限性，提出一种能够统一脉冲神经动力学与黎曼流形上自适应表示学习的几何感知脉冲图神经网络。&lt;h4&gt;方法&lt;/h4&gt;提出GSG，包含三个关键组件：1)黎曼嵌入层，将节点特征投影到常曲率流形池中，捕获非欧几里得结构；2)流形脉冲层，通过几何一致的邻域聚合和基于曲率的注意，在弯曲空间中建模膜电位演化和脉冲行为；3)流形学习目标，通过在测地距离上联合优化的分类和链接预测损失，实现实例级别的几何适应。所有模块使用黎曼SGD训练，无需通过时间反向传播。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上，GSG相比欧几里得SNN和基于流形的GNN实现了更高的准确性、鲁棒性和能效。&lt;h4&gt;结论&lt;/h4&gt;GSG建立了用于曲率感知、能效图学习的新范式，为处理复杂图结构提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在建模图结构数据方面表现出色，而脉冲神经网络(SNNs)通过稀疏、事件驱动的计算提供高能效。然而，现有的脉冲图神经网络主要在欧几里得空间运行，依赖固定的几何假设，限制了它们对层次结构和循环等复杂图结构的建模能力。为克服这些局限性，我们提出了GSG，一种新颖的几何感知脉冲图神经网络，统一了基于脉冲的神经动力学与黎曼流形上的自适应表示学习。GSG具有三个关键组件：一个将节点特征投影到常曲率流形池中的黎曼嵌入层，捕获非欧几里得结构；一个通过几何一致的邻域聚合和基于曲率的注意，在弯曲空间中建模膜电位演化和脉冲行为的流形脉冲层；以及一个通过在测地距离上联合优化的分类和链接预测损失实现实例级别几何适应的流形学习目标。所有模块都使用黎曼SGD进行训练，无需通过时间反向传播。在多个基准上的广泛实验表明，GSG相比欧几里得SNN和基于流形的GNN实现了更高的准确性、鲁棒性和能效，为曲率感知、能效的图学习建立了新范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated impressive capabilities inmodeling graph-structured data, while Spiking Neural Networks (SNNs) offer highenergy efficiency through sparse, event-driven computation. However, existingspiking GNNs predominantly operate in Euclidean space and rely on fixedgeometric assumptions, limiting their capacity to model complex graphstructures such as hierarchies and cycles. To overcome these limitations, wepropose \method{}, a novel Geometry-Aware Spiking Graph Neural Network thatunifies spike-based neural dynamics with adaptive representation learning onRiemannian manifolds. \method{} features three key components: a RiemannianEmbedding Layer that projects node features into a pool of constant-curvaturemanifolds, capturing non-Euclidean structures; a Manifold Spiking Layer thatmodels membrane potential evolution and spiking behavior in curved spaces viageometry-consistent neighbor aggregation and curvature-based attention; and aManifold Learning Objective that enables instance-wise geometry adaptationthrough jointly optimized classification and link prediction losses definedover geodesic distances. All modules are trained using Riemannian SGD,eliminating the need for backpropagation through time. Extensive experiments onmultiple benchmarks show that GSG achieves superior accuracy, robustness, andenergy efficiency compared to both Euclidean SNNs and manifold-based GNNs,establishing a new paradigm for curvature-aware, energy-efficient graphlearning.</description>
      <author>example@mail.com (Bowen Zhang, Genan Dai, Hu Huang, Long Lan)</author>
      <guid isPermaLink="false">2508.06793v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>In-Context Reinforcement Learning via Communicative World Models</title>
      <link>http://arxiv.org/abs/2508.06659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为CORAL的框架，通过通信表示来增强强化学习代理的泛化能力，使其能够在不更新参数的情况下适应新任务。&lt;h4&gt;背景&lt;/h4&gt;强化学习代理通常难以在不更新参数的情况下泛化到新任务和环境中，主要原因是它们学到的表示策略过度拟合了训练环境的特定特征。&lt;h4&gt;目的&lt;/h4&gt;提高代理的上下文强化学习能力(ICRL)，将其表述为两个代理的涌现通信问题。&lt;h4&gt;方法&lt;/h4&gt;引入CORAL框架，通过解耦潜在表示学习和控制来学习可迁移的通信上下文；信息代理(IA)作为世界模型在各种任务上预训练，不追求最大化任务奖励，而是构建世界模型并将理解提炼为简洁消息；通过因果影响损失形成涌现通信协议；部署时，预训练的IA作为新控制代理(CA)的固定上下文化器，CA通过解释通信上下文学习解决任务。&lt;h4&gt;主要发现&lt;/h4&gt;该方法使CA能够实现显著的样本效率提升，在完全未见过的稀疏奖励环境中，借助预训练的IA成功实现零样本适应。&lt;h4&gt;结论&lt;/h4&gt;验证了学习可迁移通信表示的有效性，为强化学习代理的泛化能力提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;强化学习(RL)代理通常难以在不更新其参数的情况下泛化到新任务和环境中，主要是因为它们学到的表示和策略过度拟合了其训练环境的特定特征。为了提高代理的上下文强化学习(ICRL)能力，这项工作将ICRL表述为一个双代理的涌现通信问题，并引入了CORAL（用于自适应RL的通信表示）框架，该框架通过解耦潜在表示学习和控制来学习可迁移的通信上下文。在CORAL中，信息代理(IA)在多样化的任务分布上预训练为世界模型。其目标不是最大化任务奖励，而是构建世界模型并将其理解提炼为简洁消息。涌现通信协议由一种新颖的因果影响损失(Causal Influence Loss)塑造，该损失衡量消息对下一个动作的影响。在部署期间，先前训练的IA作为新控制代理(CA)的固定上下文化器，CA通过解释提供的通信上下文来学习解决任务。我们的实验证明，这种方法使CA能够在样本效率方面取得显著提升，并借助预训练的IA在完全未见过的稀疏奖励环境中成功执行零样本适应，从而验证了学习可迁移通信表示的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) agents often struggle to generalize to new tasksand contexts without updating their parameters, mainly because their learnedrepresentations and policies are overfit to the specifics of their trainingenvironments. To boost agents' in-context RL (ICRL) ability, this workformulates ICRL as a two-agent emergent communication problem and introducesCORAL (Communicative Representation for Adaptive RL), a framework that learns atransferable communicative context by decoupling latent representation learningfrom control. In CORAL, an Information Agent (IA) is pre-trained as a worldmodel on a diverse distribution of tasks. Its objective is not to maximize taskreward, but to build a world model and distill its understanding into concisemessages. The emergent communication protocol is shaped by a novel CausalInfluence Loss, which measures the effect that the message has on the nextaction. During deployment, the previously trained IA serves as a fixedcontextualizer for a new Control Agent (CA), which learns to solve tasks byinterpreting the provided communicative context. Our experiments demonstratethat this approach enables the CA to achieve significant gains in sampleefficiency and successfully perform zero-shot adaptation with the help ofpre-trained IA in entirely unseen sparse-reward environments, validating theefficacy of learning a transferable communicative representation.</description>
      <author>example@mail.com (Fernando Martinez-Lopez, Tao Li, Yingdong Lu, Juntao Chen)</author>
      <guid isPermaLink="false">2508.06659v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning</title>
      <link>http://arxiv.org/abs/2508.06588v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对图数据向量量化中的代码本崩溃问题，提出了RGVQ框架，通过整合图拓扑和特征相似性作为正则化信号，有效提高了代码本利用率和标记多样性，显著提升了图标记表示的表达能力和可转移性。&lt;h4&gt;背景&lt;/h4&gt;向量量化(VQ)已成为学习图结构数据离散表示的有前景方法，但代码本崩溃这一基本挑战在图领域尚未得到充分探索，限制了图标记的表达能力和泛化能力。即使在视觉或语言领域提出的缓解策略，在图数据上应用VQ时仍然会出现代码本崩溃问题。&lt;h4&gt;目的&lt;/h4&gt;首次实证研究图数据上VQ的代码本崩溃问题，理解图VQ特别容易崩溃的原因，并提出解决方案来增强代码本利用率和促进标记多样性，最终提高图标记表示的表达能力和可转移性。&lt;h4&gt;方法&lt;/h4&gt;提出RGVQ框架，整合图拓扑和特征相似性作为显式正则化信号；通过Gumbel-Softmax重参数化引入软分配，确保所有码字接收梯度更新；结合结构感知对比正则化来惩罚相似节点对之间的标记共分配。&lt;h4&gt;主要发现&lt;/h4&gt;代码本崩溃在图数据上应用VQ时持续发生；图VQ容易崩溃的两个关键因素是：图特征和结构模式中的冗余导致的早期分配不平衡，以及确定性VQ中的自我强化优化循环；RGVQ显著改善了代码本利用，并一致提升了多个下游任务中最先进图VQ骨干模型的性能。&lt;h4&gt;结论&lt;/h4&gt;RGVQ框架通过解决代码本崩溃问题，实现了更具表达能力和可转移性的图标记表示，为图数据上的向量量化提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;向量量化(VQ)最近已成为学习图结构数据离散表示的一种有前景的方法。然而，一个基本挑战，即代码本崩溃，在图领域仍未得到充分探索，这显著限制了图标记的表达能力和泛化能力。在本文中，我们进行了首次实证研究，表明即使在视觉或语言领域提出的缓解策略，当将VQ应用于图数据时，代码本崩溃仍然持续发生。为了理解为什么图VQ特别容易崩溃，我们提供了理论分析并确定了两个关键因素：图特征和结构模式中的冗余导致的早期分配不平衡，以及确定性VQ中的自我强化优化循环。为了解决这些问题，我们提出了RGVQ，一个新颖的框架，它整合图拓扑和特征相似性作为显式正则化信号，以增强代码本利用率和促进标记多样性。RGVQ通过Gumbel-Softmax重参数化引入软分配，确保所有码字接收梯度更新。此外，RGVQ集成了结构感知对比正则化来惩罚相似节点对之间的标记共分配。大量实验表明，RGVQ显著提高了代码本利用率，并一致提升了多个下游任务中最先进图VQ骨干模型的性能，实现了更具表达能力和可转移性的图标记表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vector Quantization (VQ) has recently emerged as a promising approach forlearning discrete representations of graph-structured data. However, afundamental challenge, i.e., codebook collapse, remains underexplored in thegraph domain, significantly limiting the expressiveness and generalization ofgraph tokens.In this paper, we present the first empirical study showing thatcodebook collapse consistently occurs when applying VQ to graph data, even withmitigation strategies proposed in vision or language domains. To understand whygraph VQ is particularly vulnerable to collapse, we provide a theoreticalanalysis and identify two key factors: early assignment imbalances caused byredundancy in graph features and structural patterns, and self-reinforcingoptimization loops in deterministic VQ. To address these issues, we proposeRGVQ, a novel framework that integrates graph topology and feature similarityas explicit regularization signals to enhance codebook utilization and promotetoken diversity. RGVQ introduces soft assignments via Gumbel-Softmaxreparameterization, ensuring that all codewords receive gradient updates. Inaddition, RGVQ incorporates a structure-aware contrastive regularization topenalize the token co-assignments among similar node pairs. Extensiveexperiments demonstrate that RGVQ substantially improves codebook utilizationand consistently boosts the performance of state-of-the-art graph VQ backbonesacross multiple downstream tasks, enabling more expressive and transferablegraph token representations.</description>
      <author>example@mail.com (Zian Zhai, Fan Li, Xingyu Tan, Xiaoyang Wang, Wenjie Zhang)</author>
      <guid isPermaLink="false">2508.06588v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Omni Geometry Representation Learning vs Large Language Models for Geospatial Entity Resolution</title>
      <link>http://arxiv.org/abs/2508.06584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Omni模型，一种具有全几何编码器的地理空间实体解析模型，能够处理多种几何形状，并在测试中表现出色&lt;h4&gt;背景&lt;/h4&gt;地理空间数据库的开发、集成和维护依赖于高效准确的实体解析匹配程序，但具有多样几何形状的实体解析被忽视，现有方法将复杂几何形状简化为点导致信息丢失&lt;h4&gt;目的&lt;/h4&gt;开发能够无缝嵌入异构几何形状到神经网络框架的统一技术，解决现有方法无法保留复杂几何形状空间信息的问题&lt;h4&gt;方法&lt;/h4&gt;提出Omni模型，包含能够嵌入点、线、折线、多边形和多边形几何形状的全几何编码器，并通过属性亲和力机制利用transformer预训练语言模型处理文本属性&lt;h4&gt;主要发现&lt;/h4&gt;Omni相比现有方法实现了高达12%的F1值提升；大型语言模型在地理空间实体解析任务中展现出有竞争力的性能&lt;h4&gt;结论&lt;/h4&gt;Omni模型能有效处理复杂几何形状的地理空间实体解析，大型语言模型在这一领域具有应用潜力&lt;h4&gt;翻译&lt;/h4&gt;地理空间数据库的开发、集成和维护在很大程度上依赖于地理空间实体解析的高效准确的匹配程序。虽然兴趣点的解析已被广泛研究，但具有多样几何形状的实体解析在很大程度上被忽视。这部分是由于缺乏将异构几何形状无缝嵌入到神经网络框架中的统一技术。现有的神经方法将复杂几何形状简化为单个点，导致大量空间信息丢失。为解决这一限制，我们提出了Omni，一种具有全几何编码器的地理空间ER模型。该编码器能够嵌入点、线、折线、多边形和多边形几何形状，使模型能够捕获被比较场所的复杂地理空间细微差别。此外，Omni通过属性亲和力机制利用基于transformer的预训练语言模型处理场所记录的各个文本属性。该模型在现有的仅有点数据集和一个新的多样几何形状地理空间ER数据集上得到了严格测试。Omni相比现有方法实现了高达12%(F1)的改进。此外，我们测试了大型语言模型进行地理空间解析的潜力，实验了提示策略和学习场景，比较了基于预训练语言模型的方法与LLMs的结果。结果表明LLMs具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development, integration, and maintenance of geospatial databases relyheavily on efficient and accurate matching procedures of Geospatial EntityResolution (ER). While resolution of points-of-interest (POIs) has been widelyaddressed, resolution of entities with diverse geometries has been largelyoverlooked. This is partly due to the lack of a uniform technique for embeddingheterogeneous geometries seamlessly into a neural network framework. Existingneural approaches simplify complex geometries to a single point, resulting insignificant loss of spatial information. To address this limitation, we proposeOmni, a geospatial ER model featuring an omni-geometry encoder. This encoder iscapable of embedding point, line, polyline, polygon, and multi-polygongeometries, enabling the model to capture the complex geospatial intricacies ofthe places being compared. Furthermore, Omni leverages transformer-basedpre-trained language models over individual textual attributes of place recordsin an Attribute Affinity mechanism. The model is rigorously tested on existingpoint-only datasets and a new diverse-geometry geospatial ER dataset. Omniproduces up to 12% (F1) improvement over existing methods.  Furthermore, we test the potential of Large Language Models (LLMs) to conductgeospatial ER, experimenting with prompting strategies and learning scenarios,comparing the results of pre-trained language model-based methods with LLMs.Results indicate that LLMs show competitive results.</description>
      <author>example@mail.com (Kalana Wijegunarathna, Kristin Stock, Christopher B. Jones)</author>
      <guid isPermaLink="false">2508.06584v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>VGGSounder: Audio-Visual Evaluations for Foundation Models</title>
      <link>http://arxiv.org/abs/2508.08237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the IEEE/CVF International Conference on Computer  Vision (ICCV) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对视听基础模型评估中的数据集局限性问题，提出了一个全面重新注释的多标签测试集VGGSounder，解决了原数据集标签不完整、类别重叠和模态未对齐等问题，能够更准确地评估模型的多模态理解能力。&lt;h4&gt;背景&lt;/h4&gt;随着视听基础模型的兴起，可靠评估其多模态理解能力变得尤为重要。VGGSounder数据集常被用作视听分类评估的基准。&lt;h4&gt;目的&lt;/h4&gt;解决VGGSounder数据集的局限性，包括标签不完整、类别部分重叠和模态未对齐等问题，提供更准确的视听基础模型评估方法。&lt;h4&gt;方法&lt;/h4&gt;创建了一个全面重新注释的多标签测试集VGGSounder，扩展了VGGSound，专门用于评估视听基础模型。该数据集包含详细的模态注释，能够进行精确的模态特定性能分析。同时，通过新的模态混淆指标分析添加另一个输入模态时的性能下降，揭示模型局限性。&lt;h4&gt;主要发现&lt;/h4&gt;原VGGSounder数据集存在标签不完整、类别部分重叠和模态未对齐等局限性，导致视听能力的扭曲评估。新的VGGSounder数据集通过详细模态注释能够精确分析模态特定性能，并通过模态混淆指标揭示了模型的局限性。&lt;h4&gt;结论&lt;/h4&gt;新的VGGSounder数据集解决了原数据集的局限性，能够更准确地评估视听基础模型的多模态理解能力，为视听模型评估提供了更可靠的基准。&lt;h4&gt;翻译&lt;/h4&gt;视听基础模型的兴起凸显了可靠评估其多模态理解能力的重要性。VGGSounder数据集常被用作视听分类评估的基准。然而，我们的分析发现了VGGSounder的几个局限性，包括标签不完整、类别部分重叠和模态未对齐。这些导致了视听能力的扭曲评估。为解决这些局限性，我们引入了VGGSounder，这是一个全面重新注释的多标签测试集，扩展了VGGSound，并专门设计用于评估视听基础模型。VGGSounder具有详细的模态注释，能够进行精确的模态特定性能分析。此外，我们通过使用新的模态混淆指标分析添加另一个输入模态时的性能下降，揭示了模型的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of audio-visual foundation models underscores the importance ofreliably assessing their multi-modal understanding. The VGGSounder dataset iscommonly used as a benchmark for evaluation audio-visual classification.However, our analysis identifies several limitations of VGGSounder, includingincomplete labelling, partially overlapping classes, and misaligned modalities.These lead to distorted evaluations of auditory and visual capabilities. Toaddress these limitations, we introduce VGGSounder, a comprehensivelyre-annotated, multi-label test set that extends VGGSound and is specificallydesigned to evaluate audio-visual foundation models. VGGSounder featuresdetailed modality annotations, enabling precise analyses of modality-specificperformance. Furthermore, we reveal model limitations by analysing performancedegradation when adding another input modality with our new modality confusionmetric.</description>
      <author>example@mail.com (Daniil Zverev, Thaddäus Wiedemer, Ameya Prabhu, Matthias Bethge, Wieland Brendel, A. Sophia Koepke)</author>
      <guid isPermaLink="false">2508.08237v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>RedDino: A foundation model for red blood cell analysis</title>
      <link>http://arxiv.org/abs/2508.08180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RedDino是一个专为红细胞图像分析设计的自监督基础模型，在红细胞形状分类方面表现优于现有最先进模型，具有强大的特征表示能力和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;红细胞对人类健康至关重要，其精确形态分析对诊断血液疾病很重要，但针对红细胞分析的全面AI解决方案仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门用于红细胞图像分析的基础模型，以解决计算血液学中的关键挑战。&lt;h4&gt;方法&lt;/h4&gt;RedDino使用DINOv2自学习框架的红细胞特定适配，在一个包含125万张来自不同获取方式和来源的红细胞图像的精心策划数据集上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;广泛评估显示RedDino在红细胞形状分类方面优于现有最先进模型；通过线性探测和最近邻分类评估，确认了其强大的特征表示能力和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;RedDino通过捕捉细微的形态特征，解决了计算血液学中的关键挑战，推进了可靠诊断工具的发展。源代码和预训练模型可在GitHub和Hugging Face上获取。&lt;h4&gt;翻译&lt;/h4&gt;红细胞(RBCs)对人类健康至关重要，其精确形态分析对诊断血液疾病很重要。尽管基础模型在医学诊断中有前景，但针对红细胞分析的全面AI解决方案仍然稀缺。我们提出了RedDino，一个专为红细胞图像分析设计的自监督基础模型。RedDino使用了DINOv2自学习框架的红细胞特定适配，并在一个包含125万张来自不同获取方式和来源的红细胞图像的精心策划数据集上进行训练。广泛评估显示RedDino在红细胞形状分类方面优于现有最先进模型。通过线性探测和最近邻分类评估，我们确认了其强大的特征表示能力和泛化能力。我们的主要贡献是：(1)一个专为红细胞分析定制的基础模型，(2)探索DINOv2配置用于红细胞建模的消融研究，以及(3)对泛化性能的详细评估。RedDino通过捕捉细微的形态特征，解决了计算血液学中的关键挑战，推进了可靠诊断工具的发展。RedDino的源代码和预训练模型可在https://github.com/Snarci/RedDino获取，预训练模型可从我们的Hugging Face集合https://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc下载。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Red blood cells (RBCs) are essential to human health, and their precisemorphological analysis is important for diagnosing hematological disorders.Despite the promise of foundation models in medical diagnostics, comprehensiveAI solutions for RBC analysis remain scarce. We present RedDino, aself-supervised foundation model designed for RBC image analysis. RedDino usesan RBC-specific adaptation of the DINOv2 self-supervised learning framework andis trained on a curated dataset of 1.25 million RBC images from diverseacquisition modalities and sources. Extensive evaluations show that RedDinooutperforms existing state-of-the-art models on RBC shape classification.Through assessments including linear probing and nearest neighborclassification, we confirm its strong feature representations andgeneralization ability. Our main contributions are: (1) a foundation modeltailored for RBC analysis, (2) ablation studies exploring DINOv2 configurationsfor RBC modeling, and (3) a detailed evaluation of generalization performance.RedDino addresses key challenges in computational hematology by capturingnuanced morphological features, advancing the development of reliablediagnostic tools. The source code and pretrained models for RedDino areavailable at https://github.com/Snarci/RedDino, and the pretrained models canbe downloaded from our Hugging Face collection athttps://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc</description>
      <author>example@mail.com (Luca Zedda, Andrea Loddo, Cecilia Di Ruberto, Carsten Marr)</author>
      <guid isPermaLink="false">2508.08180v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Hyperspectral Imaging</title>
      <link>http://arxiv.org/abs/2508.08107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述全面介绍了高光谱成像技术，包括其物理原理、传感器架构、数据处理方法、应用领域以及面临的挑战和未来发展方向&lt;h4&gt;背景&lt;/h4&gt;高光谱成像是一种先进的传感模式，能够同时捕获空间和光谱信息，实现对材料、化学和生物特性的非侵入式、无标记分析&lt;h4&gt;目的&lt;/h4&gt;提供高光谱成像技术的全面概述，从基础原理到实际应用，并讨论面临的挑战和未来发展方向&lt;h4&gt;方法&lt;/h4&gt;介绍高光谱成像的数据获取、校准和校正技术，总结常见数据结构和分析方法，包括降维、分类、光谱解混和人工智能驱动的技术，以及计算成像、物理信息建模、跨模态融合和自监督学习等新兴解决方案&lt;h4&gt;主要发现&lt;/h4&gt;高光谱成像在地球观测、精准农业、生物医学、工业检测、文化遗产和安全等领域有广泛应用，能够揭示亚视觉特征用于高级监测和决策；面临硬件权衡、获取变异性和高维数据复杂性等挑战&lt;h4&gt;结论&lt;/h4&gt;高光谱成像正朝着可扩展、实时和嵌入式系统发展，由传感器小型化、自监督学习和基础模型驱动，有望成为跨学科平台，在科学、技术和社会领域实现变革性应用&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像(HSI)是一种先进的传感模式，能够同时捕获空间和光谱信息，实现对材料、化学和生物特性的非侵入式、无标记分析。本综述全面介绍了HSI，从基本物理原理和传感器架构到数据获取、校准和校正的关键步骤。我们总结了常见数据结构，并强调了经典和现代分析方法，包括降维、分类、光谱解混以及人工智能驱动的技术（如深度学习）。我们还讨论了在地球观测、精准农业、生物医学、工业检测、文化遗产和安全等领域的代表性应用，强调了HSI揭示亚视觉特征以实现高级监测、诊断和决策制定的能力。我们审视了持久的挑战，如硬件权衡、获取变异性以及高维数据的复杂性，同时探讨了新兴解决方案，包括计算成像、物理信息建模、跨模态融合和自监督学习。我们进一步强调了数据集共享、可重复性和元数据文档记录的最佳实践，以支持透明度和重用。展望未来，我们探索了未来朝着可扩展、实时和嵌入式HSI系统发展的方向，这一趋势由传感器小型化、自监督学习和基础模型驱动。随着HSI演变为通用、跨学科平台，它在科学、技术和社会的变革性应用方面具有巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) is an advanced sensing modality thatsimultaneously captures spatial and spectral information, enablingnon-invasive, label-free analysis of material, chemical, and biologicalproperties. This Primer presents a comprehensive overview of HSI, from theunderlying physical principles and sensor architectures to key steps in dataacquisition, calibration, and correction. We summarize common data structuresand highlight classical and modern analysis methods, including dimensionalityreduction, classification, spectral unmixing, and AI-driven techniques such asdeep learning. Representative applications across Earth observation, precisionagriculture, biomedicine, industrial inspection, cultural heritage, andsecurity are also discussed, emphasizing HSI's ability to uncover sub-visualfeatures for advanced monitoring, diagnostics, and decision-making. Persistentchallenges, such as hardware trade-offs, acquisition variability, and thecomplexity of high-dimensional data, are examined alongside emerging solutions,including computational imaging, physics-informed modeling, cross-modal fusion,and self-supervised learning. Best practices for dataset sharing,reproducibility, and metadata documentation are further highlighted to supporttransparency and reuse. Looking ahead, we explore future directions towardscalable, real-time, and embedded HSI systems, driven by sensorminiaturization, self-supervised learning, and foundation models. As HSIevolves into a general-purpose, cross-disciplinary platform, it holds promisefor transformative applications in science, technology, and society.</description>
      <author>example@mail.com (Danfeng Hong, Chenyu Li, Naoto Yokoya, Bing Zhang, Xiuping Jia, Antonio Plaza, Paolo Gamba, Jon Atli Benediktsson, Jocelyn Chanussot)</author>
      <guid isPermaLink="false">2508.08107v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.07996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了ProGraD方法，通过可学习群体提示和轻量级GroupContext Transformer，有效利用视觉基础模型进行群体活动检测，在多群体场景中表现尤为突出。&lt;h4&gt;背景&lt;/h4&gt;群体活动检测(GAD)涉及在视频中识别社会群体及其集体行为。视觉基础模型(VFMs)如DinoV2虽然提供了优秀特征，但主要在以物体为中心的数据上预训练，在建模群体动力学方面探索不足。简单替换CNN主干网络为VFMs收益有限，表明需要结构化的群体感知推理。&lt;h4&gt;目的&lt;/h4&gt;弥合视觉基础模型与群体活动检测之间的差距，开发一种方法使VFMs能够更好地捕捉群体动态和社会配置。&lt;h4&gt;方法&lt;/h4&gt;提出提示驱动的群体活动检测(ProGraD)，包括：1)可学习的群体提示，引导VFM注意力朝向社会配置；2)轻量级的两层GroupContext Transformer，用于推断参与者-群体关联和集体行为。&lt;h4&gt;主要发现&lt;/h4&gt;在Cafe和Social-CAD两个基准测试上超越了最先进方法；在复杂多群体场景中特别有效，仅使用1000万可训练参数，获得了6.5%(Group mAP@1.0)和8.2%(Group mAP@0.5)的提升；ProGraD产生可解释的注意力图，为参与者-群体推理提供见解。&lt;h4&gt;结论&lt;/h4&gt;ProGraD是一种有效的群体活动检测方法，能够充分利用视觉基础模型的优势，特别是在复杂的多群体场景中表现突出。代码和模型将被发布。&lt;h4&gt;翻译&lt;/h4&gt;群体活动检测(GAD)涉及在视频中识别社会群体及其集体行为。视觉基础模型(VFMs)，如DinoV2，提供了优秀的特征，但主要在以物体为中心的数据上进行预训练，在建模群体动力学方面探索不足。虽然它们是高度特定于GAD架构的有前途的替代方案，但这些架构需要完全微调，我们初步调查发现，简单地将这些方法中使用的CNN主干网络替换为VFMs带来的收益很小，这强调了在VFM之上进行结构化的、群体感知推理的必要性。我们提出了提示驱动的群体活动检测(ProGraD)——一种通过以下方式弥合这一差距的方法：1)可学习的群体提示，引导VFM注意力朝向社会配置；2)轻量级的两层GroupContext Transformer，用于推断参与者-群体关联和集体行为。我们在两个最近的GAD基准测试上评估了我们的方法：Cafe(具有多个并发社会群体)和Social-CAD(专注于单群体互动)。虽然我们在两种设置中都超越了最先进的方法，但我们的方法在复杂的多群体场景中特别有效，仅使用1000万可训练参数，我们获得了6.5%(Group mAP@1.0)和8.2%(Group mAP@0.5)的提升。此外，我们的实验表明ProGraD产生可解释的注意力图，为参与者-群体推理提供了见解。代码和模型将被发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Group Activity Detection (GAD) involves recognizing social groups and theircollective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,offer excellent features, but are pretrained primarily on object-centric dataand remain underexplored for modeling group dynamics. While they are apromising alternative to highly task-specific GAD architectures that requirefull fine-tuning, our initial investigation reveals that simply swapping CNNbackbones used in these methods with VFMs brings little gain, underscoring theneed for structured, group-aware reasoning on top.  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a methodthat bridges this gap through 1) learnable group prompts to guide the VFMattention toward social configurations, and 2) a lightweight two-layerGroupContext Transformer that infers actor-group associations and collectivebehavior. We evaluate our approach on two recent GAD benchmarks: Cafe, whichfeatures multiple concurrent social groups, and Social-CAD, which focuses onsingle-group interactions. While we surpass state-of-the-art in both settings,our method is especially effective in complex multi-group scenarios, where weyield a gain of 6.5\% (Group mAP\@1.0) and 8.2\% (Group mAP\@0.5) using only10M trainable parameters. Furthermore, our experiments reveal that ProGraDproduces interpretable attention maps, offering insights into actor-groupreasoning. Code and models will be released.</description>
      <author>example@mail.com (Thinesh Thiyakesan Ponbagavathi, Chengzheng Yang, Alina Roitberg)</author>
      <guid isPermaLink="false">2508.07996v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>MolmoAct: Action Reasoning Models that can Reason in Space</title>
      <link>http://arxiv.org/abs/2508.07917v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Appendix on Blogpost: https://allenai.org/blog/molmoact&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了行动推理模型(ARMs)，特别是MolmoAct模型，通过结构化的三阶段管道整合感知、规划和控制，在多种任务中表现出色，并发布了首个机器人行动推理数据集。&lt;h4&gt;背景&lt;/h4&gt;推理是目的性行动的核心，但大多数机器人基础模型直接将感知和指令映射到控制，这限制了适应性、泛化和语义基础。&lt;h4&gt;目的&lt;/h4&gt;开发一类整合感知、规划和控制的视觉-语言-动作模型，通过结构化推理提高机器人的适应性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;MolmoAct模型通过三阶段管道工作：将观察和指令编码为深度感知标记，生成中级空间计划作为可编辑轨迹轨迹，预测精确低级行动，使行为可解释和可引导。&lt;h4&gt;主要发现&lt;/h4&gt;MolmoAct-7B-D在SimplerEnv视觉匹配任务上达到70.5%的零样本准确率；在LIBERO上平均86.6%的成功率；真实世界微调中比基线提高10%-22.7%；分布外泛化提高23.3%；使用MolmoAct数据集训练性能提高5.5%。&lt;h4&gt;结论&lt;/h4&gt;MolmoAct作为尖端的机器人基础模型和构建ARMs的开蓝图，通过结构化推理将感知转化为目的性行动，所有模型权重、代码和数据集已公开。&lt;h4&gt;翻译&lt;/h4&gt;推理是目的性行动的核心，然而大多数机器人基础模型直接将感知和指令映射到控制，这限制了适应性、泛化和语义基础。我们引入了行动推理模型(ARMs)，这是一类通过结构化三阶段管道整合感知、规划和控制的视觉-语言-动作模型。我们的模型MolmoAct将观察和指令编码为深度感知标记，生成中级空间计划作为可编辑轨迹轨迹，并预测精确的低级行动，使行为可解释和可引导。MolmoAct-7B-D在模拟和真实世界环境中表现出色：在SimplerEnv视觉匹配任务上达到70.5%的零样本准确率，优于闭源Pi-0和GR00T N1；在LIBERO上平均86.6%的成功率，比ThinkAct在长距离任务上额外提高6.3%；在真实世界微调中，比Pi-0-FAST额外提高10%（单臂）和22.7%（双臂）任务进展；在分布外泛化上比基线额外提高23.3%；在开放式指令跟随和轨迹引导方面获得最高的人类偏好分数。此外，我们首次发布了MolmoAct数据集，包含10,000多条跨多样化场景和任务的高质量机器人轨迹。使用此数据集训练比基础模型平均提高5.5%的性能。我们发布所有模型权重、训练代码、收集的数据集和行动推理数据集，使MolmoAct成为尖端的机器人基础模型和构建通过结构化推理将感知转化为目的性行动的ARMs的开蓝图。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是机器人基础模型直接将感知和指令映射到控制的问题，这限制了机器人的适应性、泛化能力和语义基础。这个问题在现实中很重要，因为机器人需要像人类一样学会推理，而不仅仅是机械地执行指令；在研究中很重要，因为当前的VLA模型仍然脆弱且不透明，难以跨任务、场景或形态迁移，且缺乏对决策过程的解释性。机器人需要精细的、具身化的交互数据，而这些数据成本高昂、模糊且难以规模化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认为机器人必须学会推理，就像人类在行动前会潜意识地权衡上下文、目标和约束。他们注意到语言模型已开始从蛮力扩展转向结构化学习，构建支持推理、抽象和控制的中间表示。作者借鉴了Molmo多模态开放语言模型的视觉-语言主干，采用了链式思维(CoT)推理方法但扩展到空间推理，利用VQVAE进行深度感知表示学习，并受到多模态链式思维(MCoT)的启发。作者改进了动作令牌的初始化方式，使其更好地保持动作空间的几何特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是'在空间中推理'，教导模型明确推理深度和具体的、与图像对齐的运动草图，然后再发出低级动作。整体实现流程包括三阶段推理管道：1)深度感知令牌生成：使用深度估计器和VQVAE将深度图量化为令牌序列；2)视觉推理轨迹生成：提取机器人夹爪坐标并构建包含当前点、最终点和中间点的轨迹；3)动作令牌生成：基于前两个阶段的输出预测离散化的动作。训练流程分为预训练、中间训练和后训练三个阶段，分别在混合数据、MolmoAct数据集和特定任务数据上进行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)行动推理模型(ARMs)架构，整合感知、规划和控制；2)深度感知令牌，显式整合深度信息；3)视觉推理轨迹，提供精确的运动规划；4)可引导的行为，通过编辑轨迹线实现直接动作引导；5)完全开放模型和数据。相比之前的工作，MolmoAct引入了显式的中间推理阶段而非直接映射；专注于空间推理而非纯语言推理；用更少数据实现了更好性能；提供了决策过程的可解释视图；支持双模态控制（语言和视觉轨迹）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MolmoAct引入了行动推理模型(ARMs)，通过结构化的三阶段推理管道实现了机器人在空间中的推理能力，使机器人能够产生可解释和可引导的行为，同时在各种模拟和现实世界任务中实现了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning is central to purposeful action, yet most robotic foundation modelsmap perception and instructions directly to control, which limits adaptability,generalization, and semantic grounding. We introduce Action Reasoning Models(ARMs), a class of vision-language-action models that integrate perception,planning, and control through a structured three-stage pipeline. Our model,MolmoAct, encodes observations and instructions into depth-aware perceptiontokens, generates mid-level spatial plans as editable trajectory traces, andpredicts precise low-level actions, enabling explainable and steerablebehavior. MolmoAct-7B-D achieves strong performance across simulation andreal-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matchingtasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success onLIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;and in real-world fine-tuning, an additional 10% (single-arm) and an additional22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselinesby an additional 23.3% on out-of-distribution generalization and achieves tophuman-preference scores for open-ended instruction following and trajectorysteering. Furthermore, we release, for the first time, the MolmoAct Dataset --a mid-training robot dataset comprising over 10,000 high quality robottrajectories across diverse scenarios and tasks. Training with this datasetyields an average 5.5% improvement in general performance over the base model.We release all model weights, training code, our collected dataset, and ouraction reasoning dataset, establishing MolmoAct as both a state-of-the-artrobotics foundation model and an open blueprint for building ARMs thattransform perception into purposeful action through structured reasoning.Blogpost: https://allenai.org/blog/molmoact</description>
      <author>example@mail.com (Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna)</author>
      <guid isPermaLink="false">2508.07917v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Multi-agent systems for chemical engineering: A review and perspective</title>
      <link>http://arxiv.org/abs/2508.07880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述调查了大型语言模型驱动的多智能体系统在化学工程领域的最新进展，尽管早期研究显示出有希望的结果，但仍面临多个科学挑战，包括架构设计、数据集成、模型开发和安全性等方面。作为一个新兴但快速发展的领域，多智能体系统为重新思考化学工程工作流程提供了机遇。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型驱动的多智能体系统是一项新兴但快速发展的技术，具有通过将复杂工作流程分解为具有专业知识和工具的协作智能体团队来变革化学工程的潜力。&lt;h4&gt;目的&lt;/h4&gt;这篇综述调查了化学工程中多智能体系统的最先进技术。&lt;h4&gt;方法&lt;/h4&gt;通过将复杂工作流程分解为具有专业知识和工具的协作智能体团队来构建和评估多智能体系统。&lt;h4&gt;主要发现&lt;/h4&gt;早期研究显示出有希望的结果，但仍然存在多个科学挑战，包括定制架构的设计、异构数据模态的集成、具有领域特定模态的基础模型的开发，以及确保透明度、安全性和环境影响的策略。&lt;h4&gt;结论&lt;/h4&gt;作为一个年轻但快速发展的领域，多智能体系统为重新思考化学工程工作流程提供了令人兴奋的机会。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型驱动的多智能体系统是一项新兴但快速发展的技术，具有通过将复杂工作流程分解为具有专业知识和工具的协作智能体团队来变革化学工程的潜力。这篇综述调查了化学工程中多智能体系统的最先进技术。虽然早期研究显示出有希望的结果，但仍然存在科学挑战，包括定制架构的设计、异构数据模态的集成、具有领域特定模态的基础模型的开发，以及确保透明度、安全性和环境影响的策略。作为一个年轻但快速发展的领域，多智能体系统为重新思考化学工程工作流程提供了令人兴奋的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language model (LLM)-based multi-agent systems (MASs) are a recent butrapidly evolving technology with the potential to transform chemicalengineering by decomposing complex workflows into teams of collaborative agentswith specialized knowledge and tools. This review surveys the state-of-the-artof MAS within chemical engineering. While early studies demonstrate promisingresults, scientific challenges remain, including the design of tailoredarchitectures, integration of heterogeneous data modalities, development offoundation models with domain-specific modalities, and strategies for ensuringtransparency, safety, and environmental impact. As a young but fast-movingfield, MASs offer exciting opportunities to rethink chemical engineeringworkflows.</description>
      <author>example@mail.com (Sophia Rupprecht, Qinghe Gao, Tanuj Karia, Artur M. Schweidtmann)</author>
      <guid isPermaLink="false">2508.07880v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP</title>
      <link>http://arxiv.org/abs/2508.07819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 1 reference, 3 figures, icassp 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种架构协同设计框架，解决了预训练视觉-语言模型在零样本异常检测中面临的适应差距问题，通过整合卷积低秩适应适配器和动态融合网关，显著提升了模型在密集感知任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;预训练的视觉-语言模型在应用于零样本异常检测时存在显著的适应差距，主要源于模型缺乏密集预测所需的局部归纳偏置，以及对不灵活的特征融合范式的依赖。&lt;h4&gt;目的&lt;/h4&gt;通过架构协同设计框架解决预训练视觉-语言模型在零样本异常检测中的适应性问题，共同优化特征表示和跨模态融合。&lt;h4&gt;方法&lt;/h4&gt;整合参数高效的卷积低秩适应(Conv-LoRA)适配器为细粒度表示注入局部归纳偏置，并引入动态融合网关(DFG)，利用视觉上下文自适应调制文本提示，实现强大的双向融合。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的工业和医学基准测试中进行了广泛的实验，结果表明该方法具有优越的准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这种协同的架构设计对于将基础模型稳健地适应密集感知任务是至关重要的。&lt;h4&gt;翻译&lt;/h4&gt;预训练的视觉-语言模型在应用于零样本异常检测时面临显著的适应差距，这源于它们缺乏密集预测的局部归纳偏置以及对不灵活特征融合范式的依赖。我们通过架构协同设计框架解决了这些局限性，该框架共同优化了特征表示和跨模态融合。我们的方法整合了参数高效的卷积低秩适应(Conv-LoRA)适配器，为细粒度表示注入局部归纳偏置，并引入了动态融合网关(DFG)，该网关利用视觉上下文自适应调制文本提示，实现强大的双向融合。在多样化的工业和医学基准测试中的大量实验证明了其优越的准确性和鲁棒性，验证了这种协同的协同设计对于将基础模型稳健地适应密集感知任务是至关重要的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained Vision-Language Models (VLMs) face a significant adaptation gapwhen applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack oflocal inductive biases for dense prediction and their reliance on inflexiblefeature fusion paradigms. We address these limitations through an ArchitecturalCo-Design framework that jointly refines feature representation and cross-modalfusion. Our method integrates a parameter-efficient Convolutional Low-RankAdaptation (Conv-LoRA) adapter to inject local inductive biases forfine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) thatleverages visual context to adaptively modulate text prompts, enabling apowerful bidirectional fusion. Extensive experiments on diverse industrial andmedical benchmarks demonstrate superior accuracy and robustness, validatingthat this synergistic co-design is critical for robustly adapting foundationmodels to dense perception tasks.</description>
      <author>example@mail.com (Ke Ma, Jun Long, Hongxiao Fei, Liujie Hua, Yueyi Luo)</author>
      <guid isPermaLink="false">2508.07819v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>An Experimental Reservoir-Augmented Foundation Model: 6G O-RAN Case Study</title>
      <link>http://arxiv.org/abs/2508.07778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RA-MAT的时间序列基础模型，用于解决6G O-RAN中超高维、非平稳时间序列数据的分析挑战，实现了高效、低延迟的KPI预测。&lt;h4&gt;背景&lt;/h4&gt;下一代开放无线接入网络(O-RAN)持续传输大量关键性能指标(KPIs)和原始同相/正交(IQ)样本，产生超高维、非平稳的时间序列数据，这些数据超出了传统Transformer架构的处理能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种满足6G O-RAN测试严格延迟、能效和可扩展性要求的时间序列基础模型，实现实时基础级分析。&lt;h4&gt;方法&lt;/h4&gt;引入基于水库增强的掩码自编码Transformer(RA-MAT)，结合回声状态网络(ESN)计算与掩码自编码技术，通过固定随机初始化的ESN将时间块投影为动态嵌入，将二次自注意力转换为线性操作，使用30%随机掩码块重建进行自监督学习，最后通过浅层任务头微调实现低足迹适应。&lt;h4&gt;主要发现&lt;/h4&gt;在O-RAN KPI案例研究中，RA-MAT在多个连续和离散KPI上实现了低于0.06的均方误差(MSE)，展示了优异的性能。&lt;h4&gt;结论&lt;/h4&gt;RA-MAT为未来6G网络实现实时基础级分析提供了实用途径，满足了高维时间序列分析的效率和可扩展性需求。&lt;h4&gt;翻译&lt;/h4&gt;下一代开放无线接入网络(O-RAN)持续传输大量关键性能指标(KPIs)和原始同相/正交(IQ)样本，产生超高维、非平稳的时间序列数据，这些数据超出了传统Transformer架构的处理能力。我们引入了一种基于水库增强的掩码自编码Transformer(RA-MAT)。这种时间序列基础模型使用回声状态网络(ESN)计算结合掩码自编码，以满足6G O-RAN测试对延迟、能效和可扩展性的严格要求。一个固定随机初始化的ESN无需通过时间反向传播，即可将每个时间块快速投影到丰富的动态嵌入中，将二次自注意力瓶颈转换为轻量级线性操作。这些嵌入驱动一个块级掩码自编码器，该自编码器重建30%随机掩码的块，强制编码器从未标记数据中捕获局部动力学和长程结构。在自监督预训练后，RA-MAT使用浅层任务头进行微调，同时保持水库和大部分Transformer层冻结，实现对各种下游任务(如O-RAN KPI预测)的低足迹适应。在全面的O-RAN KPI案例研究中，RA-MAT在几个连续和离散KPI上实现了低于0.06的均方误差(MSE)。这项工作将RA-MAT定位为未来6G网络实现实时基础级分析的实际途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next-generation open radio access networks (O-RAN) continuously stream tensof key performance indicators (KPIs) together with raw in-phase/quadrature (IQ)samples, yielding ultra-high-dimensional, non-stationary time series thatoverwhelm conventional transformer architectures. We introduce areservoir-augmented masked autoencoding transformer (RA-MAT). This time seriesfoundation model employs echo state network (ESN) computing with maskedautoencoding to satisfy the stringent latency, energy efficiency, andscalability requirements of 6G O-RAN testing. A fixed, randomly initialized ESNrapidly projects each temporal patch into a rich dynamical embedding withoutbackpropagation through time, converting the quadratic self-attentionbottleneck into a lightweight linear operation. These embeddings drive apatch-wise masked autoencoder that reconstructs 30% randomly masked patches,compelling the encoder to capture both local dynamics and long-range structurefrom unlabeled data. After self-supervised pre-training, RA-MAT is fine-tunedwith a shallow task head while keeping the reservoir and most transformerlayers frozen, enabling low-footprint adaptation to diverse downstream taskssuch as O-RAN KPI forecasting. In a comprehensive O-RAN KPI case study, RA-MATachieved sub-0.06 mean squared error (MSE) on several continuous and discreteKPIs. This work positions RA-MAT as a practical pathway toward real-time,foundation-level analytics in future 6G networks.</description>
      <author>example@mail.com (Farhad Rezazadeh, Raymond Zhao, Jiongyu Dai, Amir Ashtari Gargari, Hatim Chergui, Lingjia Liu)</author>
      <guid isPermaLink="false">2508.07778v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild</title>
      <link>http://arxiv.org/abs/2508.07759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为CAV-SAM的新方法，通过将参考-目标图像对的对应关系表示为伪视频，利用SAM2的交互式视频对象分割能力，轻量级地将视觉模型适应到下游任务，避免了传统元学习方法的高昂成本。&lt;h4&gt;背景&lt;/h4&gt;大型视觉模型如SAM在应用于野外下游任务时存在显著局限性。现有的参考分割方法主要依赖元学习，但元学习需要大量元训练过程，带来巨大的数据和计算成本。&lt;h4&gt;目的&lt;/h4&gt;提出一种轻量级方法来适应视觉模型到下游任务，避免元学习方法带来的高成本问题。&lt;h4&gt;方法&lt;/h4&gt;将参考-目标图像对之间的固有对应关系表示为伪视频，利用具有交互式视频对象分割能力的SAM2。CAV-SAM包含两个关键模块：基于扩散的语义转换(DBST)模块使用扩散模型构建语义转换序列；测试时几何对齐(TTGA)模块通过测试时微调对齐序列中的几何变化。&lt;h4&gt;主要发现&lt;/h4&gt;在广泛使用的数据集上评估CAV-SAM，其分割性能比最先进方法提高了超过5%。&lt;h4&gt;结论&lt;/h4&gt;CAV-SAM是一种轻量级方法，能够有效适应视觉模型到下游任务，实现已在补充材料中提供。&lt;h4&gt;翻译&lt;/h4&gt;像Segment Anything Model(SAM)这样的大型视觉模型在应用于野外下游任务时表现出显著的局限性。因此，参考分割(reference segmentation)作为一种有前景的新方向出现，它利用参考图像及其对应的掩码为模型注入新知识。然而，现有的参考分割方法主要依赖元学习，这仍然需要大量的元训练过程并带来巨大的数据和计算成本。在本研究中，我们提出了一种新颖的方法，将参考-目标图像对之间的固有对应关系表示为伪视频。这一视角使得具有交互式视频对象分割(iVOS)能力的SAM2最新版本能够以轻量级方式适应下游任务。我们将这种方法命名为Correspondence As Video for SAM (CAV-SAM)。CAV-SAM包含两个关键模块：基于扩散的语义转换(DBST)模块使用扩散模型构建语义转换序列，而测试时几何对齐(TTGA)模块通过测试时微调对齐此序列中的几何变化。我们在广泛使用的数据集上评估了CAV-SAM，其分割性能比最先进方法提高了超过5%。实现已在补充材料中提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large vision models like the Segment Anything Model (SAM) exhibit significantlimitations when applied to downstream tasks in the wild. Consequently,reference segmentation, which leverages reference images and theircorresponding masks to impart novel knowledge to the model, emerges as apromising new direction for adapting vision models. However, existing referencesegmentation approaches predominantly rely on meta-learning, which stillnecessitates an extensive meta-training process and brings massive data andcomputational cost. In this study, we propose a novel approach by representingthe inherent correspondence between reference-target image pairs as a pseudovideo. This perspective allows the latest version of SAM, known as SAM2, whichis equipped with interactive video object segmentation (iVOS) capabilities, tobe adapted to downstream tasks in a lightweight manner. We term this approachCorrespondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules:the Diffusion-Based Semantic Transition (DBST) module employs a diffusion modelto construct a semantic transformation sequence, while the Test-Time GeometricAlignment (TTGA) module aligns the geometric changes within this sequencethrough test-time fine-tuning. We evaluated CAVSAM on widely-used datasets,achieving segmentation performance improvements exceeding 5% over SOTA methods.Implementation is provided in the supplementary materials.</description>
      <author>example@mail.com (Haoran Wang, Zekun Li, Jian Zhang, Lei Qi, Yinghuan Shi)</author>
      <guid isPermaLink="false">2508.07759v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification</title>
      <link>http://arxiv.org/abs/2508.07577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了LayerNorm在Vision Transformers中的微调动力学，特别是在数据稀缺和领域迁移情况下。研究发现LayerNorm参数在微调后的变化可以反映源域和目标域之间的过渡，并提出了基于微调移位比率(FSR)的重缩放机制和循环框架来增强LayerNorm微调效果。&lt;h4&gt;背景&lt;/h4&gt;LayerNorm在Vision Transformers (ViTs)中至关重要，但在数据稀缺和领域迁移情况下的微调动力学尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究LayerNorm参数在微调后的变化如何反映源域和目标域之间的过渡，并提高LayerNorm微调的效果。&lt;h4&gt;方法&lt;/h4&gt;提出了微调移位比率(FSR)来量化目标训练样本对目标域的准确代表程度；设计了一个使用与FSR负相关的标量λ的重缩放机制来对齐学习的LayerNorm shifts与理想shifts；结合循环框架进一步增强LayerNorm微调。&lt;h4&gt;主要发现&lt;/h4&gt;OOD任务比ID任务产生更低的FSR和更高的λ，特别是在数据稀缺时，表明目标训练样本代表性不足；在病理数据上微调的ViTs行为更像ID设置，倾向于保守的LayerNorm更新。&lt;h4&gt;结论&lt;/h4&gt;研究结果揭示了迁移学习中LayerNorm未被充分探索的动力学，并为LayerNorm微调提供了实用策略。&lt;h4&gt;翻译&lt;/h4&gt;LayerNorm在Vision Transformers (ViTs)中至关重要，然而在数据稀缺和领域迁移情况下的微调动力学仍未被充分探索。本文表明，微调后LayerNorm参数的变化反映了源域和目标域之间的过渡；其有效性取决于目标训练样本对目标域的准确代表程度，由我们提出的微调移位比率(FSR)量化。基于此，我们提出了一种简单而有效的重缩放机制，使用与FSR负相关的标量λ，将学习的LayerNorm shifts与在完全代表性数据下实现的理想shifts对齐，并结合一个循环框架进一步增强LayerNorm微调。在自然图像和病理图像、ID和OOD设置以及各种目标训练样本条件下的广泛实验验证了我们的框架。值得注意的是，与ID情况相比，OOD任务往往产生更低的FSR和更高的λ，特别是在数据稀缺的情况下，表明目标训练样本代表性不足。此外，在病理数据上微调的ViTs行为更接近ID设置，倾向于保守的LayerNorm更新。我们的研究结果揭示了迁移学习中LayerNorm未被充分探索的动力学，并为LayerNorm微调提供了实用策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LayerNorm is pivotal in Vision Transformers (ViTs), yet its fine-tuningdynamics under data scarcity and domain shifts remain underexplored. This papershows that shifts in LayerNorm parameters after fine-tuning (LayerNorm shifts)are indicative of the transitions between source and target domains; itsefficacy is contingent upon the degree to which the target training samplesaccurately represent the target domain, as quantified by our proposedFine-tuning Shift Ratio ($FSR$). Building on this, we propose a simple yeteffective rescaling mechanism using a scalar $\lambda$ that is negativelycorrelated to $FSR$ to align learned LayerNorm shifts with those ideal shiftsachieved under fully representative data, combined with a cyclic framework thatfurther enhances the LayerNorm fine-tuning. Extensive experiments acrossnatural and pathological images, in both in-distribution (ID) andout-of-distribution (OOD) settings, and various target training sample regimesvalidate our framework. Notably, OOD tasks tend to yield lower $FSR$ and higher$\lambda$ in comparison to ID cases, especially with scarce data, indicatingunder-represented target training samples. Moreover, ViTFs fine-tuned onpathological data behave more like ID settings, favoring conservative LayerNormupdates. Our findings illuminate the underexplored dynamics of LayerNorm intransfer learning and provide practical strategies for LayerNorm fine-tuning.</description>
      <author>example@mail.com (Zhaorui Tan, Tan Pan, Kaizhu Huang, Weimiao Yu, Kai Yao, Chen Jiang, Qiufeng Wang, Anh Nguyen, Xin Guo, Yuan Cheng, Xi Yang)</author>
      <guid isPermaLink="false">2508.07577v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Mega-Satellite Networks with Generative Semantic Communication: A Networking Perspective</title>
      <link>http://arxiv.org/abs/2508.07573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted paper to be published in IEEE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了将生成式语义通信(GSC)集成到巨型卫星星座中的网络视角，提出GSC赋能的卫星网络架构和关键使能技术，构建离散时间图模型，开发模型部署和路由方案，并评估性能。&lt;h4&gt;背景&lt;/h4&gt;直接卫星到设备通信的进步使巨型卫星星座成为6G无线通信的基石，实现全球无缝连接，但频谱稀缺和容量限制仍是支持多媒体应用大量数据需求的挑战。&lt;h4&gt;目的&lt;/h4&gt;从网络视角研究将生成式语义通信(GSC)集成到巨型卫星星座中，提出GSC赋能的卫星网络架构并确定关键使能技术。&lt;h4&gt;方法&lt;/h4&gt;构建离散时间图模型来建模语义编码器/解码器、知识库和资源变化；开发语义编码器和解码器的模型部署以及GSC兼容的路由方案；进行性能评估。&lt;h4&gt;主要发现&lt;/h4&gt;GSC由AI生成基础模型支持，实现了从传输原始数据到交换语义意义的范式转变，可减少带宽消耗并增强多媒体内容的关键语义特征，为克服传统卫星通信系统局限提供解决方案。&lt;h4&gt;结论&lt;/h4&gt;提出GSC赋能的卫星网络架构和关键使能技术，构建离散时间图模型，开发模型部署和路由方案，并提出未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;直接卫星到设备通信的进步使巨型卫星星座成为6G无线通信的基石，即使在偏远和服务不足的地区也能实现无缝全球连接。然而，由香农经典信息理论造成的频谱稀缺和容量限制，仍然是支持多媒体丰富无线应用大量数据需求的重大挑战。由基于人工智能的生成基础模型支持的生成式语义通信(GSC)，代表了一种从传输原始数据到交换语义意义的范式转变。GSC不仅可以减少带宽消耗，还可以增强多媒体内容中的关键语义特征，从而为克服传统卫星通信系统的局限性提供了有希望的解决方案。本文从网络视角研究了将GSC集成到巨型卫星星座中。我们提出了GSC赋能的卫星网络架构，并确定了关键的使能技术，重点关注GSC赋能的网络建模和GSC感知的网络策略。我们构建了一个离散时间图，用于建模巨型卫星网络中的语义编码器和解码器、不同的知识库和资源变化。基于此框架，我们开发了语义编码器和解码器的模型部署和GSC兼容的路由方案，然后进行了性能评估。最后，我们概述了推进GSC赋能卫星网络的未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advance of direct satellite-to-device communication has positionedmega-satellite constellations as a cornerstone of 6G wireless communication,enabling seamless global connectivity even in remote and underserved areas.However, spectrum scarcity and capacity constraints imposed by the Shannon'sclassical information theory remain significant challenges for supporting themassive data demands of multimedia-rich wireless applications. GenerativeSemantic Communication (GSC), powered by artificial intelligence-basedgenerative foundation models, represents a paradigm shift from transmitting rawdata to exchanging semantic meaning. GSC can not only reduce bandwidthconsumption, but also enhance key semantic features in multimedia content,thereby offering a promising solution to overcome the limitations oftraditional satellite communication systems. This article investigates theintegration of GSC into mega-satellite constellations from a networkingperspective. We propose a GSC-empowered satellite networking architecture andidentify key enabling technologies, focusing on GSC-empowered network modelingand GSC-aware networking strategies. We construct a discrete temporal graph tomodel semantic encoders and decoders, distinct knowledge bases, and resourcevariations in mega-satellite networks. Based on this framework, we developmodel deployment for semantic encoders and decoders and GSC-compatible routingschemes, and then present performance evaluations. Finally, we outline futureresearch directions for advancing GSC-empowered satellite networks.</description>
      <author>example@mail.com (Binquan Guo, Wanting Yang, Zehui Xiong, Zhou Zhang, Baosheng Li, Zhu Han, Rahim Tafazolli, Tony Q. S. Quek)</author>
      <guid isPermaLink="false">2508.07573v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems</title>
      <link>http://arxiv.org/abs/2508.07407v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对自进化AI代理系统技术进行了全面综述，提出统一概念框架，分析各种自进化技术，探讨特定领域应用策略，并讨论评估、安全与伦理问题。&lt;h4&gt;背景&lt;/h4&gt;大语言模型的进步引发了人们对解决复杂现实世界任务的AI代理的兴趣，但现有系统大多依赖手动配置且部署后保持静态，难以适应动态变化的环境。&lt;h4&gt;目的&lt;/h4&gt;探索基于交互数据和环境反馈自动增强代理系统的进化技术，为自进化AI代理奠定基础，连接基础模型的静态能力和终身代理系统所需的持续适应性。&lt;h4&gt;方法&lt;/h4&gt;引入统一概念框架抽象自进化代理系统设计背后的反馈循环，突出四个关键组件(系统输入、代理系统、环境和优化器)；基于该框架系统审查各种自进化技术；调查生物医学、编程和金融等特定领域的进化策略；讨论评估、安全性和伦理考虑。&lt;h4&gt;主要发现&lt;/h4&gt;自进化代理系统可通过反馈循环实现持续适应；统一框架有助于理解和比较不同策略；特定领域需要结合领域约束的优化目标；评估、安全和伦理对确保系统有效性和可靠性至关重要。&lt;h4&gt;结论&lt;/h4&gt;为研究人员和实践者提供对自进化AI代理的系统理解，为开发更适应性、自主性和终身代理系统奠定基础。&lt;h4&gt;翻译&lt;/h4&gt;近期大型语言模型的进步引发了人们对能够解决复杂现实世界任务的AI代理日益增长的关注。然而，大多数现有代理系统依赖于手动配置的方案，且部署后保持静态，限制了它们适应动态和不断变化环境的能力。为此，近期研究探索了代理进化技术，旨在基于交互数据和环境反馈自动增强代理系统。这一新兴方向为自进化AI代理奠定了基础，连接了基础模型的静态能力和终身代理系统所需的持续适应性。在本综述中，我们对自进化代理系统的现有技术进行了全面回顾。具体而言，我们首先介绍了一个统一的概念框架，该框架抽象了自进化代理系统设计背后的反馈循环。该框架突出了四个关键组件：系统输入、代理系统、环境和优化器，作为理解和比较不同策略的基础。基于此框架，我们系统地审查了针对代理系统不同组件的各种自进化技术。我们还调查了针对生物医学、编程和金融等专门领域开发的特定领域进化策略，其中优化目标与领域约束紧密耦合。此外，我们专门讨论了自进化代理系统的评估、安全性和伦理考虑，这对于确保其有效性和可靠性至关重要。本综述旨在为研究人员和实践者提供对自进化AI代理的系统理解，为开发更适应性、自主性和终身代理系统奠定基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in large language models have sparked growing interest in AIagents capable of solving complex, real-world tasks. However, most existingagent systems rely on manually crafted configurations that remain static afterdeployment, limiting their ability to adapt to dynamic and evolvingenvironments. To this end, recent research has explored agent evolutiontechniques that aim to automatically enhance agent systems based on interactiondata and environmental feedback. This emerging direction lays the foundationfor self-evolving AI agents, which bridge the static capabilities of foundationmodels with the continuous adaptability required by lifelong agentic systems.In this survey, we provide a comprehensive review of existing techniques forself-evolving agentic systems. Specifically, we first introduce a unifiedconceptual framework that abstracts the feedback loop underlying the design ofself-evolving agentic systems. The framework highlights four key components:System Inputs, Agent System, Environment, and Optimisers, serving as afoundation for understanding and comparing different strategies. Based on thisframework, we systematically review a wide range of self-evolving techniquesthat target different components of the agent system. We also investigatedomain-specific evolution strategies developed for specialised fields such asbiomedicine, programming, and finance, where optimisation objectives aretightly coupled with domain constraints. In addition, we provide a dedicateddiscussion on the evaluation, safety, and ethical considerations forself-evolving agentic systems, which are critical to ensuring theireffectiveness and reliability. This survey aims to provide researchers andpractitioners with a systematic understanding of self-evolving AI agents,laying the foundation for the development of more adaptive, autonomous, andlifelong agentic systems.</description>
      <author>example@mail.com (Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, Zhaochun Ren, Nikos Aletras, Xi Wang, Han Zhou, Zaiqiao Meng)</author>
      <guid isPermaLink="false">2508.07407v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack</title>
      <link>http://arxiv.org/abs/2508.07402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对参数高效微调(PEFT)方法在图像伪造检测与定位(IFDL)任务中易受对抗攻击的问题，提出了ForensicsSAM框架，通过注入伪造专家和对抗专家，以及设计轻量级对抗检测器，显著提高了模型对对抗攻击的鲁棒性，同时保持了先进的IFDL性能。&lt;h4&gt;背景&lt;/h4&gt;参数高效微调(PEFT)已成为适应大型视觉基础模型(如SAM和LLaVA)用于下游任务(如图像伪造检测和定位)的流行策略。然而，现有的PEFT方法容易受到对抗攻击的影响。&lt;h4&gt;目的&lt;/h4&gt;解决PEFT方法在IFDL任务中的对抗攻击脆弱性问题，提高模型对对抗攻击的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出ForensicsSAM框架，包含三个关键设计：(1)向transformer块注入伪造专家，增强捕获伪造伪影的能力；(2)设计轻量级对抗检测器，学习捕获RGB域中的结构化、任务特定伪影；(3)向全局注意力层和MLP模块注入对抗专家，纠正对抗噪声引起的特征偏移。&lt;h4&gt;主要发现&lt;/h4&gt;可以通过上游模型生成高度可转移的对抗图像，无需访问下游模型或训练数据，这会显著降低IFDL性能。ForensicsSAM能够有效抵抗各种对抗攻击方法，同时保持先进的IFDL性能。&lt;h4&gt;结论&lt;/h4&gt;ForensicsSAM通过集成对抗鲁棒性设计，解决了PEFT方法在IFDL任务中的对抗攻击脆弱性问题，为视觉基础模型的安全应用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;参数高效微调(PEFT)已成为适应大型视觉基础模型(如Segment Anything Model和LLaVA)用于下游任务(如图像伪造检测和定位)的流行策略。然而，现有的基于PEFT的方法忽视了它们对对抗攻击的脆弱性。在本文中，我们展示了可以通过上游模型专门生成高度可转移的对抗图像，无需访问下游模型或训练数据，这会显著降低IFDL性能。为解决此问题，我们提出了ForensicsSAM，一个具有内置对抗鲁棒性的统一IFDL框架。我们的设计由三个关键思想指导：(1)为弥补冻结图像编码器中缺乏伪造相关知识的问题，我们向每个transformer块注入伪造专家，增强其捕获伪造伪影的能力。这些伪造专家始终处于激活状态，并在任何输入图像间共享。(2)为检测对抗图像，我们设计了一个轻量级对抗检测器，学习捕获RGB域中的结构化、任务特定伪影，实现对各种攻击方法的可靠区分。(3)为抵抗对抗攻击，我们向全局注意力层和MLP模块注入对抗专家，逐步纠正由对抗噪声引起的特征偏移。这些对抗专家由对抗检测器自适应激活，从而避免对干净图像造成不必要干扰。在多个基准上的广泛实验表明，ForensicsSAM对各种对抗攻击方法具有更强的抵抗力，同时在图像级伪造检测和像素级伪造定位方面取得了最先进的性能。资源可在https://github.com/siriusPRX/ForensicsSAM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning (PEFT) has emerged as a popular strategy foradapting large vision foundation models, such as the Segment Anything Model(SAM) and LLaVA, to downstream tasks like image forgery detection andlocalization (IFDL). However, existing PEFT-based approaches overlook theirvulnerability to adversarial attacks. In this paper, we show that highlytransferable adversarial images can be crafted solely via the upstream model,without accessing the downstream model or training data, significantlydegrading the IFDL performance. To address this, we propose ForensicsSAM, aunified IFDL framework with built-in adversarial robustness. Our design isguided by three key ideas: (1) To compensate for the lack of forgery-relevantknowledge in the frozen image encoder, we inject forgery experts into eachtransformer block to enhance its ability to capture forgery artifacts. Theseforgery experts are always activated and shared across any input images. (2) Todetect adversarial images, we design an light-weight adversary detector thatlearns to capture structured, task-specific artifact in RGB domain, enablingreliable discrimination across various attack methods. (3) To resistadversarial attacks, we inject adversary experts into the global attentionlayers and MLP modules to progressively correct feature shifts induced byadversarial noise. These adversary experts are adaptively activated by theadversary detector, thereby avoiding unnecessary interference with cleanimages. Extensive experiments across multiple benchmarks demonstrate thatForensicsSAM achieves superior resistance to various adversarial attackmethods, while also delivering state-of-the-art performance in image-levelforgery detection and pixel-level forgery localization. The resource isavailable at https://github.com/siriusPRX/ForensicsSAM.</description>
      <author>example@mail.com (Rongxuan Peng, Shunquan Tan, Chenqi Kong, Anwei Luo, Alex C. Kot, Jiwu Huang)</author>
      <guid isPermaLink="false">2508.07402v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control</title>
      <link>http://arxiv.org/abs/2508.07387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅使用RGB相机在未知环境中进行导航的新方法，通过将估计的深度作为上下文输入到学习的碰撞模型中，而非直接用于碰撞检测，从而提高了在杂乱环境中的导航成功率。&lt;h4&gt;背景&lt;/h4&gt;仅使用RGB相机在未知环境中导航具有挑战性，因为缺乏深度信息无法进行可靠的碰撞检测。现有使用视觉基础模型估计深度的方法在杂乱环境中过于嘈杂，不适合零样本导航。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖嘈杂估计深度直接进行碰撞检测的替代方法，提高在高度杂乱环境中的导航成功率。&lt;h4&gt;方法&lt;/h4&gt;提出将估计的深度作为上下文输入到学习的碰撞模型中，该模型预测给定控制序列下机器人可以预期的最小障碍物清除距离分布。使用感知风险的MPC规划器最小化估计的碰撞风险。通过联合学习管道同时训练碰撞模型和风险度量，使用安全和unsafe轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;联合训练确保了碰撞模型的最佳方差，提高了在高度杂乱环境中的导航能力。真实世界实验显示，与NoMaD和ROS堆栈相比，成功率分别提高了9倍和7倍。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过创新地使用估计深度和联合学习策略，显著提高了仅使用RGB相机在未知杂乱环境中的导航性能。&lt;h4&gt;翻译&lt;/h4&gt;使用单个RGB相机在未知环境中导航具有挑战性，因为缺乏深度信息会阻碍可靠的碰撞检测。虽然一些方法使用估计的深度来构建碰撞地图，但我们发现视觉基础模型估计的深度对于在杂乱环境中的零样本导航来说过于嘈杂。我们提出了一种替代方法：不使用嘈杂的估计深度进行直接碰撞检测，而是将其作为丰富的上下文输入到学习的碰撞模型中。该模型预测机器人对于给定控制序列可以预期的最小障碍物清除距离分布。在推理时，这些预测会通知一个感知风险的MPC规划器，该规划器最小化估计的碰撞风险。我们的联合学习管道使用安全和unsafe轨迹共同训练碰撞模型和风险度量。关键是，我们的联合训练确保了碰撞模型的最佳方差，从而提高了在高度杂乱环境中的导航能力。因此，真实世界的实验表明，与NoMaD和ROS堆栈相比，成功率分别提高了9倍和7倍。消融研究进一步验证了他们设计选择的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决单目RGB相机在未知环境中导航的挑战，特别是缺乏深度信息导致的可靠碰撞检测问题。这个问题很重要，因为单目相机对轻型机器人平台（如空中或紧凑地面机器人）非常适用，这些平台受限于尺寸、重量和功耗难以使用LiDAR。仅视觉设置降低了硬件复杂性和能源使用，支持更长时间的任务和成本敏感环境中的广泛部署，但缺乏深度感知使得安全碰撞检测变得困难。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，发现直接使用估计深度构建碰撞地图的方法在杂乱环境中表现不佳。他们提出将嘈杂的估计深度作为上下文输入到学习的碰撞模型中，而非直接用于碰撞检测。作者借鉴了视觉基础模型（如DepthAnything）进行深度预测，借鉴了MPC框架进行规划，借鉴了PointNet++进行点云特征提取，借鉴了基于采样的优化方法解决轨迹优化问题，并借鉴了MMD作为风险度量概念。他们的创新在于重新解释了估计深度的使用方式，设计了联合学习管道，并通过下游任务监督改进了方差正则化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：将单目RGB图像和候选控制序列作为输入，学习一个概率碰撞模型来预测沿轨迹的最小障碍物清除分布，使用这些预测估计碰撞风险，并通过风险感知的MPC规划器最小化风险。整体流程包括：1)使用预训练深度估计器从RGB图像生成深度图并转换为点云；2)使用PointNet++提取点云特征；3)将特征与初始状态连接并通过MLP预测障碍物清除的均值和方差；4)生成样本并计算约束违反和MMD风险；5)使用采样优化器解决轨迹优化问题生成控制序列；6)以递归方式应用MPC反馈循环。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)重新解释估计深度作为信息丰富的输入而非真实深度的代理；2)联合学习管道共同训练碰撞模型和风险度量；3)通过下游任务监督实现任务感知方差正则化；4)风险感知MPC框架利用预测分布估计碰撞风险；5)学习最优MMD核参数。相比之前工作的不同：与端到端方法不同，作者的方法显式结合了安全约束；与世界模型方法不同，作者的方法建模了预测不确定性和碰撞风险；与使用估计深度构建地图的方法不同，作者的方法不直接依赖嘈杂深度估计；大多数单目导航方法只在简单环境中测试，而作者的方法在复杂杂乱环境中展示了可靠导航。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种新颖的单目视觉导航方法，通过学习概率碰撞模型和风险感知模型预测控制，有效解决了嘈杂深度估计下的可靠碰撞检测问题，显著提高了杂乱环境中的导航成功率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Navigating unknown environments with a single RGB camera is challenging, asthe lack of depth information prevents reliable collision-checking. While somemethods use estimated depth to build collision maps, we found that depthestimates from vision foundation models are too noisy for zero-shot navigationin cluttered environments.  We propose an alternative approach: instead of using noisy estimated depthfor direct collision-checking, we use it as a rich context input to a learnedcollision model. This model predicts the distribution of minimum obstacleclearance that the robot can expect for a given control sequence. At inference,these predictions inform a risk-aware MPC planner that minimizes estimatedcollision risk. Our joint learning pipeline co-trains the collision model andrisk metric using both safe and unsafe trajectories. Crucially, ourjoint-training ensures optimal variance in our collision model that improvesnavigation in highly cluttered environments. Consequently, real-worldexperiments show 9x and 7x improvements in success rates over NoMaD and the ROSstack, respectively. Ablation studies further validate the effectiveness of ourdesign choices.</description>
      <author>example@mail.com (Basant Sharma, Prajyot Jadhav, Pranjal Paul, K. Madhava Krishna, Arun Kumar Singh)</author>
      <guid isPermaLink="false">2508.07387v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance</title>
      <link>http://arxiv.org/abs/2508.07375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TurnGuide的新方法，用于解决全双工语音语言模型在对话能力方面面临的挑战。该方法通过模拟人类的对话规划过程，动态将助手语音分割为对话回合，并在语音输出前生成回合级别的文本指导，从而显著提高了端到端FD-SLMs的对话能力。&lt;h4&gt;背景&lt;/h4&gt;全双工语音语言模型(FD-SLMs)是专门设计的基础模型，旨在通过建模复杂的对话动态（如打断、反馈和重叠语音）来实现自然、实时的口语交互。端到端FD-SLMs利用真实世界的双通道对话数据来捕捉精细的双人对话模式，以实现类人交互。然而，由于长时间的语音序列和有限的高质量口语对话数据，这些模型的对话能力往往比纯文本对话差。&lt;h4&gt;目的&lt;/h4&gt;解决全双工语音语言模型在对话能力方面面临的挑战，特别是由于长时间语音序列和有限高质量口语对话数据导致的对话能力下降问题，以及文本引导语音生成在整合到双通道音频流时面临的时机和长度问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为TurnGuide的、受规划启发的新方法。该方法通过模拟人类的对话规划过程，动态地将助手语音分割为对话回合，并在语音输出前生成回合级别的文本指导，从而有效解决了插入时机和长度的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;TurnGuide方法显著提高了端到端FD-SLMs的对话能力，使模型能够生成语义上有意义且连贯的语音，同时保持了自然的对话流程。&lt;h4&gt;结论&lt;/h4&gt;TurnGuide方法有效地解决了全双工语音语言模型面临的挑战，通过模拟人类的对话规划过程，提高了模型的对话能力，使其能够生成更自然、更有意义的语音交互。&lt;h4&gt;翻译&lt;/h4&gt;全双工语音语言模型(FD-SLMs)是专门设计的基础模型，通过建模复杂的对话动态（如打断、反馈和重叠语音）来实现自然、实时的口语交互。端到端(e2e)FD-SLMs利用真实世界的双通道对话数据来捕捉精细的双人对话模式，以实现类人交互。然而，由于长时间的语音序列和有限的高质量口语对话数据，这些模型的对话能力往往比纯文本对话差。虽然文本引导的语音生成可以缓解这些问题，但在将文本引导整合到双通道音频流时，它面临着时机和长度问题，破坏了自然交互所需的时间精确对齐。为解决这些挑战，我们提出了TurnGuide，一种受规划启发的新方法，它通过动态将助手语音分割为对话回合并在语音输出前生成回合级别的文本指导，模拟人类的对话规划过程，从而有效解决了插入时机和长度的挑战。大量实验证明我们的方法显著提高了端到端FD-SLMs的对话能力，使它们能够生成语义上有意义且连贯的语音，同时保持自然的对话流程。演示可在https://dreamtheater123.github.io/TurnGuide-Demo/查看，代码将在https://github.com/dreamtheater123/TurnGuide提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Full-Duplex Speech Language Models (FD-SLMs) are specialized foundationmodels designed to enable natural, real-time spoken interactions by modelingcomplex conversational dynamics such as interruptions, backchannels, andoverlapping speech, and End-to-end (e2e) FD-SLMs leverage real-worlddouble-channel conversational data to capture nuanced two-speaker dialoguepatterns for human-like interactions. However, they face a critical challenge-- their conversational abilities often degrade compared to pure-textconversation due to prolonged speech sequences and limited high-quality spokendialogue data. While text-guided speech generation could mitigate these issues,it suffers from timing and length issues when integrating textual guidance intodouble-channel audio streams, disrupting the precise time alignment essentialfor natural interactions. To address these challenges, we propose TurnGuide, anovel planning-inspired approach that mimics human conversational planning bydynamically segmenting assistant speech into dialogue turns and generatingturn-level text guidance before speech output, which effectively resolves bothinsertion timing and length challenges. Extensive experiments demonstrate ourapproach significantly improves e2e FD-SLMs' conversational abilities, enablingthem to generate semantically meaningful and coherent speech while maintainingnatural conversational flow. Demos are available athttps://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available athttps://github.com/dreamtheater123/TurnGuide.</description>
      <author>example@mail.com (Wenqian Cui, Lei Zhu, Xiaohui Li, Zhihan Guo, Haoli Bai, Lu Hou, Irwin King)</author>
      <guid isPermaLink="false">2508.07375v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Gradient Surgery for Safe LLM Fine-Tuning</title>
      <link>http://arxiv.org/abs/2508.07172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SafeGrad是一种新型防御方法，通过梯度手术解决微调即服务中的安全对齐问题，能够在保持任务性能的同时增强模型安全性。&lt;h4&gt;背景&lt;/h4&gt;微调即服务(Fine-tuning-as-a-Service)引入了一个关键漏洞，用户微调数据集中的少量恶意示例可能损害大型语言模型(LLMs)的安全对齐。&lt;h4&gt;目的&lt;/h4&gt;解决现有安全微调解决方案对有害比例敏感的问题，提高在高有害比例情况下的防御效果。&lt;h4&gt;方法&lt;/h4&gt;SafeGrad采用梯度手术技术，当检测到冲突时，通过将用户任务梯度投影到对齐梯度的正交平面上来消除有害成分；同时使用KL散度对齐损失学习基础模型的分布式安全配置文件。&lt;h4&gt;主要发现&lt;/h4&gt;现有安全微调解决方案的性能会随着有害比例的增加而急剧下降，这是因为用户任务更新与安全目标之间存在冲突的梯度。&lt;h4&gt;结论&lt;/h4&gt;SafeGrad在各种LLMs和数据集上提供了最先进的防御，即使在有害比例较高的情况下也能保持强大的安全性，而不损害任务保真度。&lt;h4&gt;翻译&lt;/h4&gt;微调即服务引入了一个关键漏洞，即用户微调数据集中的少量恶意示例可能损害大型语言模型的安全对齐。虽然公认的安全微调范式将其视为一个多目标优化问题，平衡用户任务性能与安全对齐，但我们发现现有解决方案对有害比例极其敏感，随着有害比例的增加，防御效果会急剧下降。我们诊断出这种失败源于冲突的梯度，其中用户任务更新直接损害了安全目标。为此，我们提出了SafeGrad，一种采用梯度手术的新方法。当检测到冲突时，SafeGrad通过将用户任务梯度投影到对齐梯度的正交平面上来消除其有害成分，使模型能够学习用户的任务而不牺牲安全性。为了进一步增强鲁棒性和数据效率，我们采用了KL散度对齐损失，学习良好对齐基础模型的丰富、分布式安全配置文件。大量实验表明，SafeGrad在各种LLMs和数据集上提供了最先进的防御，即使在有害比例较高的情况下也能保持强大的安全性而不损害任务保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning-as-a-Service introduces a critical vulnerability where a fewmalicious examples mixed into the user's fine-tuning dataset can compromise thesafety alignment of Large Language Models (LLMs). While a recognized paradigmframes safe fine-tuning as a multi-objective optimization problem balancinguser task performance with safety alignment, we find existing solutions arecritically sensitive to the harmful ratio, with defenses degrading sharply asharmful ratio increases. We diagnose that this failure stems from conflictinggradients, where the user-task update directly undermines the safety objective.To resolve this, we propose SafeGrad, a novel method that employs gradientsurgery. When a conflict is detected, SafeGrad nullifies the harmful componentof the user-task gradient by projecting it onto the orthogonal plane of thealignment gradient, allowing the model to learn the user's task withoutsacrificing safety. To further enhance robustness and data efficiency, weemploy a KL-divergence alignment loss that learns the rich, distributionalsafety profile of the well-aligned foundation model. Extensive experiments showthat SafeGrad provides state-of-the-art defense across various LLMs anddatasets, maintaining robust safety even at high harmful ratios withoutcompromising task fidelity.</description>
      <author>example@mail.com (Biao Yi, Jiahao Li, Baolei Zhang, Lihai Nie, Tong Li, Tiansheng Huang, Zheli Liu)</author>
      <guid isPermaLink="false">2508.07172v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications</title>
      <link>http://arxiv.org/abs/2508.07165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了PRISM，一种基于大规模多序列MRI预训练的基础模型，解决了MRI序列异质性导致的深度学习模型泛化能力差的问题，在44个下游任务中39项排名第一，显著提升了AI在放射学中的应用潜力。&lt;h4&gt;背景&lt;/h4&gt;多序列磁共振成像(MRI)具有显著多样性，能可视化不同组织类型，但MRI序列间的内在异质性对深度学习模型的泛化能力构成重大挑战，影响了模型在不同采集参数下的表现，限制了其临床应用。&lt;h4&gt;目的&lt;/h4&gt;开发PRISM基础模型，通过大规模多序列MRI预训练，解决MRI序列异质性导致的模型泛化能力差的问题，提升模型在临床应用中的表现。&lt;h4&gt;方法&lt;/h4&gt;收集64个数据集(336,476个体积MRI扫描)构建最大多器官多序列MRI预训练语料库；提出新预训练范式，解耦解剖不变特征与序列特定变化；建立包含44个下游任务的基准测试，在32个公共数据集和5个私人队列上评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;PRISM在非预训练模型和现有基础模型上表现更优，在44个下游基准测试中39项排名第一，具有统计学显著改进；能够学习在多样化MRI协议下采集的未见数据上的鲁棒且可泛化的表示。&lt;h4&gt;结论&lt;/h4&gt;PRISM为多序列MRI分析提供了可扩展框架，增强了AI在放射学中的转化潜力，在多样化成像协议上提供一致性能，强化了临床适用性。&lt;h4&gt;翻译&lt;/h4&gt;多序列磁共振成像(MRI)提供了显著的多样性，能够清晰可视化不同组织类型。然而，MRI序列之间的内在异质性对深度学习模型的泛化能力构成了重大挑战。这些挑战在面对不同采集参数时损害了模型性能，从而严重限制了它们的临床应用。在本研究中，我们提出了PRISM，一种基于大规模多序列MRI预训练的基础模型。我们从公共和私人来源收集了总共64个数据集，涵盖广泛的全身解剖结构，扫描跨越多种MRI序列。其中，336,476个体积MRI扫描来自34个数据集(8个公共和26个私人)，被精心筛选构建迄今为止最大的多器官多序列MRI预训练语料库。我们提出了一种新的预训练范式，将MRI中解剖不变特征与序列特定变化解耦，同时保留高级语义表示。我们建立了一个包含44个下游任务的基准测试，包括疾病诊断、图像分割、配准、进展预测和报告生成。这些任务在32个公共数据集和5个私人队列上进行了评估。PRISM始终优于非预训练模型和现有基础模型，在44个下游基准测试中的39项中取得排名第一的结果，具有统计学显著改进。这些结果强调了PRISM在多样化MRI协议下采集的未见数据上学习鲁棒且可泛化表示的能力。PRISM为多序列MRI分析提供了可扩展框架，从而增强了AI在放射学中的转化潜力。它在多样化的成像协议上提供一致的性能，强化了其临床适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多序列磁共振成像（MRI）中的内在异质性对深度学习模型泛化能力的挑战。这个问题在现实中非常重要，因为MRI是临床诊断的基础工具，能够无创、无辐射地可视化软组织并提供复杂的解剖和病理细节。然而，不同MRI序列之间的差异导致AI模型难以在不同采集参数下保持稳定性能，严重限制了AI在放射学领域的临床应用和实用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有医学基础模型在MRI应用中的局限性，包括解剖覆盖有限、多序列信息利用不足和下游任务验证范围窄等问题。他们借鉴了自然图像和语言领域的基础模型思想，以及现有的医学基础模型如Med3D和BrainSegFounder，设计了PRISM模型。核心思路是创建一个能够学习通用表示的基础模型，通过解耦解剖不变特征与序列特定变化，使模型能够适应各种临床应用。作者采用了Swin Transformer作为骨干网络，并创新性地结合了多种自监督学习任务，包括掩码图像重建、跨序列翻译、元数据预测和解剖不变对比学习。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过在大规模多序列MRI数据上预训练一个基础模型（PRISM），解耦解剖不变特征和序列特定特征，使模型能够捕捉跨序列的共享解剖模式，同时保留序列特定的对比度变化，从而提高模型对不同扫描仪协议和采集参数的鲁棒性。整体实现流程包括：1）收集并预处理64个数据集，构建包含336,476个体积MRI扫描的预训练语料库；2）采用Swin Transformer骨干网络和双分支解缠模块进行特征提取；3）通过掩码图像重建、跨序列翻译、元数据预测和解剖不变对比学习四种自监督任务进行预训练；4）在44个下游任务上评估模型性能，包括疾病诊断、图像分割、跨序列配准等。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）构建了迄今为止最大的多器官多序列MRI预训练语料库；2）提出新的预训练范式，解耦解剖不变特征和序列特定变化；3）结合像素级掩码图像重建和图像到图像翻译，保持不同对比度条件下的结构保真度；4）将元数据预测与对比学习相结合，增强语义表示学习；5）在44个下游任务上进行全面验证。相比之前的工作，PRISM的不同之处在于：它覆盖了更广泛的解剖区域，充分利用了多序列信息，对真实世界的异质性具有更强鲁棒性，适用于更广泛的临床应用，并且训练数据规模远大于现有MRI基础模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PRISM通过在大规模多序列MRI数据上进行预训练并采用创新的解耦表示学习方法，实现了在多种临床应用中具有卓越泛化能力和鲁棒性的MRI基础模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkableversatility, enabling the distinct visualization of different tissue types.Nevertheless, the inherent heterogeneity among MRI sequences poses significantchallenges to the generalization capability of deep learning models. Thesechallenges undermine model performance when faced with varying acquisitionparameters, thereby severely restricting their clinical utility. In this study,we present PRISM, a foundation model PRe-trained with large-scalemultI-Sequence MRI. We collected a total of 64 datasets from both public andprivate sources, encompassing a wide range of whole-body anatomical structures,with scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRIscans from 34 datasets (8 public and 26 private) were curated to construct thelargest multi-organ multi-sequence MRI pretraining corpus to date. We propose anovel pretraining paradigm that disentangles anatomically invariant featuresfrom sequence-specific variations in MRI, while preserving high-level semanticrepresentations. We established a benchmark comprising 44 downstream tasks,including disease diagnosis, image segmentation, registration, progressionprediction, and report generation. These tasks were evaluated on 32 publicdatasets and 5 private cohorts. PRISM consistently outperformed bothnon-pretrained models and existing foundation models, achieving first-rankresults in 39 out of 44 downstream benchmarks with statistical significanceimprovements. These results underscore its ability to learn robust andgeneralizable representations across unseen data acquired under diverse MRIprotocols. PRISM provides a scalable framework for multi-sequence MRI analysis,thereby enhancing the translational potential of AI in radiology. It deliversconsistent performance across diverse imaging protocols, reinforcing itsclinical applicability.</description>
      <author>example@mail.com (Zelin Qiu, Xi Wang, Zhuoyao Xie, Juan Zhou, Yu Wang, Lingjie Yang, Xinrui Jiang, Juyoung Bae, Moo Hyun Son, Qiang Ye, Dexuan Chen, Rui Zhang, Tao Li, Neeraj Ramesh Mahboobani, Varut Vardhanabhuti, Xiaohui Duan, Yinghua Zhao, Hao Chen)</author>
      <guid isPermaLink="false">2508.07165v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>UniMove: A Unified Model for Multi-city Human Mobility Prediction</title>
      <link>http://arxiv.org/abs/2508.06986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by SIGSPATIAL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出UniMove，一个统一的多城市人类移动性预测模型，通过通用空间表示和异构移动模式建模，解决了现有方法需为每个城市单独训练的问题，实现10.2%以上的预测准确率提升。&lt;h4&gt;背景&lt;/h4&gt;人类移动性预测对城市规划、交通优化和个性化服务至关重要，但人类移动性的内在随机性、非均匀时间间隔和复杂模式，以及不同城市结构、基础设施和人口密度带来的异质性，给建模带来重大挑战。现有解决方案通常需要为每个城市单独训练模型。&lt;h4&gt;目的&lt;/h4&gt;提出UniMove统一模型，解决两个挑战：(1)构建通用空间表示以实现城市间有效标记共享；(2)建模来自不同城市特征的异构移动模式。&lt;h4&gt;方法&lt;/h4&gt;采用轨迹-位置双塔架构，包括位置塔用于通用空间编码和轨迹塔用于顺序移动性建模，并设计MoE Transformer块自适应选择专家处理不同移动模式，支持多城市数据联合训练和相互数据增强。&lt;h4&gt;主要发现&lt;/h4&gt;在多个不同城市数据集上的实验表明，UniMove通过多城市数据联合训练和相互数据增强，显著提高了移动性预测准确性，提升超过10.2%。&lt;h4&gt;结论&lt;/h4&gt;UniMove代表了朝着实现具有统一架构的人类移动性真正基础模型迈出的关键一步，相关实现已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;人类移动性预测对城市规划、交通优化和个性化服务至关重要。然而，人类移动性的内在随机性、非均匀时间间隔和复杂模式，加上不同城市结构、基础设施和人口密度引入的异质性，给建模带来了重大挑战。现有解决方案通常需要为每个城市训练单独的模型，因为它们具有不同的空间表示和地理覆盖范围。在本文中，我们提出了UniMove，一个用于多城市人类移动性预测的统一模型，解决了两个挑战：(1)构建通用的空间表示，以便在城市间有效共享标记；(2)建模来自不同城市特征的异构移动模式。我们提出了一个轨迹-位置双塔架构，包括一个用于通用空间编码的位置塔和一个用于顺序移动性建模的轨迹塔。我们还设计了MoE Transformer块，以自适应地选择专家来处理不同的移动模式。在多个不同城市数据集上的大量实验表明，UniMove真正体现了统一模型的本质。通过多城市数据的联合训练和相互数据增强，它显著提高了移动性预测的准确性，提高了10.2%以上。UniMove代表了朝着实现具有统一架构的人类移动性真正基础模型迈出的关键一步。我们在https://github.com/tsinghua-fib-lab/UniMove/上发布了该模型的实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human mobility prediction is vital for urban planning, transportationoptimization, and personalized services. However, the inherent randomness,non-uniform time intervals, and complex patterns of human mobility, compoundedby the heterogeneity introduced by varying city structures, infrastructure, andpopulation densities, present significant challenges in modeling. Existingsolutions often require training separate models for each city due to distinctspatial representations and geographic coverage. In this paper, we proposeUniMove, a unified model for multi-city human mobility prediction, addressingtwo challenges: (1) constructing universal spatial representations foreffective token sharing across cities, and (2) modeling heterogeneous mobilitypatterns from varying city characteristics. We propose a trajectory-locationdual-tower architecture, with a location tower for universal spatial encodingand a trajectory tower for sequential mobility modeling. We also design MoETransformer blocks to adaptively select experts to handle diverse movementpatterns. Extensive experiments across multiple datasets from diverse citiesdemonstrate that UniMove truly embodies the essence of a unified model. Byenabling joint training on multi-city data with mutual data enhancement, itsignificantly improves mobility prediction accuracy by over 10.2\%. UniMoverepresents a key advancement toward realizing a true foundational model with aunified architecture for human mobility. We release the implementation athttps://github.com/tsinghua-fib-lab/UniMove/.</description>
      <author>example@mail.com (Chonghua Han, Yuan Yuan, Yukun Liu, Jingtao Ding, Jie Feng, Yong Li)</author>
      <guid isPermaLink="false">2508.06986v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing</title>
      <link>http://arxiv.org/abs/2508.06937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: vaynexie.github.io/CannyEdit/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CannyEdit是一种创新的无需训练框架，通过选择性Canny控制和双提示引导解决了现有文本到图像编辑方法在平衡文本遵循度、上下文保真度和编辑无缝性方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;文本到图像(T2I)模型的发展使得基于基础模型生成先验的无需训练区域图像编辑成为可能，但现有方法难以平衡编辑区域的文本遵循度、未编辑区域的上下文保真度以及编辑的无缝集成。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的无需训练框架，实现更精确的图像编辑，同时保持文本遵循度、上下文保真度和编辑无缝性的平衡。&lt;h4&gt;方法&lt;/h4&gt;CannyEdit框架包含两个关键创新：(1)选择性Canny控制：在用户指定的可编辑区域屏蔽Canny ControlNet的结构指导，同时保留未编辑区域的源图像细节；(2)双提示引导：结合局部提示进行特定对象编辑和全局目标提示以保持场景交互一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在实际图像编辑任务中，CannyEdit优于先前方法如KV-Edit，在文本遵循度和上下文保真度的平衡上实现了2.93%到10.49%的改进。用户研究表明，只有49.2%的普通用户和42.0%的AIGC专家能将CannyEdit的结果识别为AI编辑，而竞争对手方法这一比例为76.08%到89.09%。&lt;h4&gt;结论&lt;/h4&gt;CannyEdit通过其创新方法成功解决了现有文本到图像编辑面临的挑战，在保持文本遵循度、上下文保真度和编辑无缝性方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;最近的文本到图像(T2I)模型进展使得通过利用基础模型的生成先验实现无需训练的区域图像编辑成为可能。然而，现有方法难以平衡编辑区域的文本遵循度、未编辑区域的上下文保真度以及编辑的无缝集成。我们引入了CannyEdit，一种新颖的无需训练框架，通过两个关键创新解决这些挑战：(1)选择性Canny控制，在用户指定的可编辑区域屏蔽Canny ControlNet的结构指导，同时通过反转阶段ControlNet信息保留严格保留未编辑区域的源图像细节。这能够在不损害上下文完整性的情况下实现精确的文本驱动编辑。(2)双提示引导，结合用于特定对象编辑的局部提示和用于保持场景交互一致性的全局目标提示。在实际图像编辑任务（添加、替换、删除）中，CannyEdit优于先前的方法如KV-Edit，在文本遵循度和上下文保真度的平衡上实现了2.93%到10.49%的改进。在编辑无缝性方面，用户研究表明，当与未编辑的真实图像配对时，只有49.2%的普通用户和42.0%的AIGC专家能够识别CannyEdit的结果为AI编辑，而竞争对手方法这一比例为76.08%到89.09%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in text-to-image (T2I) models have enabled training-freeregional image editing by leveraging the generative priors of foundationmodels. However, existing methods struggle to balance text adherence in editedregions, context fidelity in unedited areas, and seamless integration of edits.We introduce CannyEdit, a novel training-free framework that addresses thesechallenges through two key innovations: (1) Selective Canny Control, whichmasks the structural guidance of Canny ControlNet in user-specified editableregions while strictly preserving details of the source images in uneditedareas via inversion-phase ControlNet information retention. This enablesprecise, text-driven edits without compromising contextual integrity. (2)Dual-Prompt Guidance, which combines local prompts for object-specific editswith a global target prompt to maintain coherent scene interactions. Onreal-world image editing tasks (addition, replacement, removal), CannyEditoutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percentimprovement in the balance of text adherence and context fidelity. In terms ofediting seamlessness, user studies reveal only 49.2 percent of general usersand 42.0 percent of AIGC experts identified CannyEdit's results as AI-editedwhen paired with real images without edits, versus 76.08 to 89.09 percent forcompetitor methods.</description>
      <author>example@mail.com (Weiyan Xie, Han Gao, Didan Deng, Kaican Li, April Hua Liu, Yongxiang Huang, Nevin L. Zhang)</author>
      <guid isPermaLink="false">2508.06937v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification</title>
      <link>http://arxiv.org/abs/2508.06908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了MMReID-Bench，这是专为行人重识别设计的首个多任务多模态基准。研究表明多模态大语言模型在行人ReID方面具有显著能力，但在处理某些模态（特别是热成像和红外数据）时存在局限性。&lt;h4&gt;背景&lt;/h4&gt;传统行人ReID模型受限于单模态能力，在处理多模态数据（如RGB、热成像、红外、草图图像、文本描述等）时泛化能力较差。多模态大语言模型(MLLMs)的出现为解决这一问题提供了新途径，但现有方法未能充分利用MLLMs的推理、指令遵循和跨模态理解能力。&lt;h4&gt;目的&lt;/h4&gt;引入MMReID-Bench，填补现有方法的不足，促进开发更强大且可泛化的多模态基础模型用于行人ReID。&lt;h4&gt;方法&lt;/h4&gt;提出MMReID-Bench，包含20,710个多模态查询和图库图像，涵盖10种不同的行人ReID任务。通过全面实验评估MLLMs在行人ReID任务中的能力。&lt;h4&gt;主要发现&lt;/h4&gt;多模态大语言模型在提供有效且多样化的行人ReID方面具有显著能力，但在处理热成像和红外数据等某些模态时存在局限性。&lt;h4&gt;结论&lt;/h4&gt;MMReID-Bench作为首个专为行人ReID设计的多任务多模态基准，有望促进社区开发更强大且可泛化的多模态基础模型。&lt;h4&gt;翻译&lt;/h4&gt;行人重识别(ReID)旨在在图库图像中检索感兴趣人员的图像，在医疗康复、异常行为检测和公共安全等领域有广泛应用。然而，传统行人ReID模型受限于单模态能力，在处理多模态数据（如RGB、热成像、红外、草图图像、文本描述等）时泛化能力较差。最近，多模态大语言模型(MLLMs)的出现为解决这个问题提供了有希望的途径。尽管如此，现有方法仅将MLLMs视为特征提取器或字幕生成器，未能充分利用其推理、指令遵循和跨模态理解能力。为了弥补这一差距，我们引入了MMReID-Bench，这是第一个专为行人ReID设计的多任务多模态基准。MMReID-Bench包含20,710个多模态查询和图库图像，涵盖10种不同的行人ReID任务。全面实验表明MLLMs在提供有效且多样化的行人ReID方面具有显著能力。然而，它们在处理某些模态（特别是热成像和红外数据）时也存在局限性。我们希望MMReID-Bench能够促进社区开发更强大且可泛化的多模态基础模型用于行人ReID。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Person re-identification (ReID) aims to retrieve the images of an interestedperson in the gallery images, with wide applications in medical rehabilitation,abnormal behavior detection, and public security. However, traditional personReID models suffer from uni-modal capability, leading to poor generalizationability in multi-modal data, such as RGB, thermal, infrared, sketch images,textual descriptions, etc. Recently, the emergence of multi-modal largelanguage models (MLLMs) shows a promising avenue for addressing this problem.Despite this potential, existing methods merely regard MLLMs as featureextractors or caption generators, which do not fully unleash their reasoning,instruction-following, and cross-modal understanding capabilities. To bridgethis gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmarkspecifically designed for person ReID. The MMReID-Bench includes 20,710multi-modal queries and gallery images covering 10 different person ReID tasks.Comprehensive experiments demonstrate the remarkable capabilities of MLLMs indelivering effective and versatile person ReID. Nevertheless, they also havelimitations in handling a few modalities, particularly thermal and infrareddata. We hope MMReID-Bench can facilitate the community to develop more robustand generalizable multimodal foundation models for person ReID.</description>
      <author>example@mail.com (Jinhao Li, Zijian Chen, Lirong Deng, Changbo Wang, Guangtao Zhai)</author>
      <guid isPermaLink="false">2508.06908v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Large Model Driven Solar Activity AI Forecaster: A Scalable Dual Data-Model Framework</title>
      <link>http://arxiv.org/abs/2508.06892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了'太阳活动AI预测器'，一个基于基础模型的可扩展双数据-模型驱动框架，通过整合专家知识和多模态太阳数据，在OODA范式中实现了自主太阳耀斑预测，其性能在多源数据泛化、预测准确性和操作效率方面优于或匹配人类预报员。&lt;h4&gt;背景&lt;/h4&gt;太阳活动驱动空间天气，影响地球磁层和技术基础设施，使准确的太阳耀斑预测至关重要。当前空间天气模型未充分利用多模态太阳数据，缺乏专家知识的迭代增强，且过度依赖人类预报员。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够自主复制人类预测任务并提供可量化输出的系统，以提高太阳耀斑预测的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出'太阳活动AI预测器'，一个基于基础模型构建的可扩展双数据-模型驱动框架，集成专家知识，在OODA范式中实现。包含三个模块：情境感知模块（整合多模态观测生成太阳态势感知图）、深度分析工具（表征关键太阳特征）和耀斑预测模块（预测强耀斑）。&lt;h4&gt;主要发现&lt;/h4&gt;模型可在几分钟内执行，在多源数据的泛化能力、预测准确性和操作效率方面，优于或匹配人类预报员。&lt;h4&gt;结论&lt;/h4&gt;建立了基于AI的空间天气预测新范式，证明了AI提高预测准确性和效率的潜力，为自主运行预测系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;太阳活动驱动空间天气，影响地球磁层和技术基础设施，这使得准确的太阳耀斑预测至关重要。当前空间天气模型未充分利用多模态太阳数据，缺乏专家知识的迭代增强，并在观察-定向-决策-行动（OODA）范式下过度依赖人类预报员。我们在此提出'太阳活动AI预测器'，一个基于基础模型构建的可扩展双数据-模型驱动框架，集成专家知识，自主复制人类预测任务并提供可量化输出。它在OODA范式中实现，包含三个模块：通过整合多模态观测生成每日太阳态势感知图的情境感知模块；表征关键太阳特征（活动区、冕洞、日珥）的深度分析工具；以及预测整个太阳圆盘和活动区强耀斑的耀斑预测模块。模型在几分钟内执行，在多源数据泛化、预测准确性和操作效率方面优于或匹配人类预报员。这项工作建立了基于AI的空间天气预测新范式，证明了AI提高预测准确性和效率的潜力，为自主运行预测系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solar activity drives space weather, affecting Earth's magnetosphere andtechnological infrastructure, which makes accurate solar flare forecastingcritical. Current space weather models under-utilize multi-modal solar data,lack iterative enhancement via expert knowledge, and rely heavily on humanforecasters under the Observation-Orientation-Decision-Action (OODA) paradigm.Here we present the "Solar Activity AI Forecaster", a scalable dual data-modeldriven framework built on foundational models, integrating expert knowledge toautonomously replicate human forecasting tasks with quantifiable outputs. It isimplemented in the OODA paradigm and comprises three modules: a SituationalPerception Module that generates daily solar situation awareness maps byintegrating multi-modal observations; In-Depth Analysis Tools that characterizekey solar features (active regions, coronal holes, filaments); and a FlarePrediction Module that forecasts strong flares for the full solar disk andactive regions. Executed within a few minutes, the model outperforms or matcheshuman forecasters in generalization across multi-source data, forecastaccuracy, and operational efficiency. This work establishes a new paradigm forAI-based space weather forecasting, demonstrating AI's potential to enhanceforecast accuracy and efficiency, and paving the way for autonomous operationalforecasting systems.</description>
      <author>example@mail.com (Jingjing Wang, Pengyu Liang, Tingyu Wang, Ming Li, Yanmei Cui, Siwei Liu, Xin Huang, Xiang Li, Minghui Zhang, Yunshi Zeng, Zhu Cao, Jiekang Feng, Qinghua Hu, Bingxian Luo, Bing Cao)</author>
      <guid isPermaLink="false">2508.06892v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Analysis of Solow-Swan model with nonlocal fractional derivative operator</title>
      <link>http://arxiv.org/abs/2508.06883v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过引入分数微积分扩展了传统的Solow-Swan经济增长模型，以包含记忆效应，使模型能够反映现实经济系统中过去状态对当前资本变化率的影响，并对两种模型下的资本动态进行了比较分析。&lt;h4&gt;背景&lt;/h4&gt;Solow-Swan方程是现代经济增长理论发展的基础模型，对理解资本积累和产出的长期行为提供了关键见解，但传统模型依赖整数阶导数，可能无法完全捕捉现实经济系统中常见的记忆和遗传特性。&lt;h4&gt;目的&lt;/h4&gt;扩展经典Solow-Swan框架，通过引入记忆效应使模型能够反映过去经济状态对当前资本变化率的影响，并比较分析经典与分数阶形式下的资本动态。&lt;h4&gt;方法&lt;/h4&gt;使用分数微积分将记忆效应融入传统Solow-Swan模型，创建分数阶模型，并对经典和分数阶形式下的资本动态进行比较分析。&lt;h4&gt;主要发现&lt;/h4&gt;分数阶模型能够捕捉到传统模型无法反映的记忆效应，即过去经济状态对当前资本变化率的影响。&lt;h4&gt;结论&lt;/h4&gt;通过分数微积分扩展的Solow-Swan模型能够更准确地描述现实经济系统中的动态行为，特别是那些表现出记忆和遗传特性的系统。&lt;h4&gt;翻译&lt;/h4&gt;Solow-Swan方程是现代经济增长理论发展中的基础模型。它对资本积累和产出的长期行为提供了关键见解。自其创立以来，该模型已成为理解宏观经济动态的基石，并激发了大量的后续研究。然而，传统Solow-Swan模型的表述依赖于整数阶导数，这可能无法完全捕捉现实经济系统中常见的记忆和遗传特性。在本文中，我们通过使用分数微积分将记忆效应融入，扩展了经典的Solow-Swan框架。分数阶模型考虑了过去状态对当前资本变化率的影响，这是标准模型未涵盖的特征。我们提出了对经典和分数阶形式的Solow-Swan方程下资本动态的比较分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Solow-Swan equation is a foundational model in the evolution of moderneconomic growth theory. It offers key insights into the long-term behaviour ofcapital accumulation and output. Since its inception, the model has served as acornerstone for understanding macroeconomic dynamics and has inspired a vastbody of subsequent research. However, traditional formulations of theSolow-Swan model rely on integer-order derivatives, which may not fully capturethe memory and hereditary properties often observed in real-world economicsystems. In this paper, we extend the classical Solow-Swan framework byincorporating memory effects through the use of fractional calculus. Thefractional model accounts for the influence of past states on the present rateof capital change, a feature not accommodated in the standard model. We presenta comparative analysis of the capital dynamics under both the classical andfractional-order formulations of the Solow-Swan equation.</description>
      <author>example@mail.com (MO Aibinu, KJ Duffy, S Moyo)</author>
      <guid isPermaLink="false">2508.06883v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>On Understanding of the Dynamics of Model Capacity in Continual Learning</title>
      <link>http://arxiv.org/abs/2508.08052v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了持续学习中的稳定性-塑性困境问题，引入了持续学习的有效模型容量(CLEMC)概念，揭示了神经网络有效容量的非平稳性特征。&lt;h4&gt;背景&lt;/h4&gt;稳定性-塑性困境是与神经网络能力相关的持续学习中的一个基本挑战，该能力指神经网络表示任务的能力。&lt;h4&gt;目的&lt;/h4&gt;引入持续学习的有效模型容量(CLEMC)，用于描述稳定性-塑性平衡点的动态行为。&lt;h4&gt;方法&lt;/h4&gt;开发差分方程建模神经网络、任务数据和优化过程的相互作用演变；利用CLEMC分析有效容量的特性；在多种架构网络(前馈网络、卷积网络、图神经网络、大型语言模型)上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;无论神经网络架构或优化方法如何，当传入任务分布与之前不同时，神经网络表示新任务的能力会减弱；有效容量及稳定性-塑性平衡点本质上是非平稳的。&lt;h4&gt;结论&lt;/h4&gt;理论发现得到广泛实验支持，这些实验覆盖了从小型到大型多种神经网络架构。&lt;h4&gt;翻译&lt;/h4&gt;稳定性-塑性困境与神经网络(NN)能力-其表示任务的能力-密切相关，是持续学习(CL)中的一个基本挑战。在此背景下，我们引入了持续学习的有效模型容量(CLEMC)，用于描述稳定性-塑性平衡点的动态行为。我们开发了一个差分方程来建模神经网络、任务数据和优化过程之间相互作用的演变。然后我们利用CLEMC证明，有效容量以及稳定性-塑性平衡点本质上是非平稳的。我们表明，无论神经网络架构或优化方法如何，当传入的任务分布与先前不同时，神经网络表示新任务的能力会减弱。我们进行了广泛的实验来支持我们的理论发现，涵盖了从小的前馈网络和卷积网络到中等规模的图神经网络和基于Transformer的具有数百万参数的大型语言模型等多种架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The stability-plasticity dilemma, closely related to a neural network's (NN)capacity-its ability to represent tasks-is a fundamental challenge in continuallearning (CL). Within this context, we introduce CL's effective model capacity(CLEMC) that characterizes the dynamic behavior of the stability-plasticitybalance point. We develop a difference equation to model the evolution of theinterplay between the NN, task data, and optimization procedure. We thenleverage CLEMC to demonstrate that the effective capacity-and, by extension,the stability-plasticity balance point is inherently non-stationary. We showthat regardless of the NN architecture or optimization method, a NN's abilityto represent new tasks diminishes when incoming task distributions differ fromprevious ones. We conduct extensive experiments to support our theoreticalfindings, spanning a range of architectures-from small feedforward network andconvolutional networks to medium-sized graph neural networks andtransformer-based large language models with millions of parameters.</description>
      <author>example@mail.com (Supriyo Chakraborty, Krishnan Raghavan)</author>
      <guid isPermaLink="false">2508.08052v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP</title>
      <link>http://arxiv.org/abs/2508.08005v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一个基于学习的框架，结合传统机器学习和图神经网络来解决最大团问题(MCP)的算法选择问题，通过实验证明双通道架构在算法选择中的有效性。&lt;h4&gt;背景&lt;/h4&gt;现有研究表明，没有单一的最大团算法能在所有实例上表现最佳，这突显了根据实例特征选择合适算法的重要性。然而，针对最大团问题的算法选择研究较少。&lt;h4&gt;目的&lt;/h4&gt;填补最大团问题算法选择研究的空白，开发一种能够根据图实例特征选择最合适算法的方法。&lt;h4&gt;方法&lt;/h4&gt;提出一个基于学习的框架，结合传统机器学习和图神经网络。通过在多样化的图实例上运行四种精确MCP算法构建标记数据集，并提取每个图的结构和全局统计特征。评估了四种传统分类器(SVM、随机森林、决策树和KNN)，并基于这些发现开发了名为GAT-MLP的双通道模型，该模型结合了用于局部结构编码的图注意力网络(GAT)和用于全局特征建模的多层感知机(MLP)。&lt;h4&gt;主要发现&lt;/h4&gt;随机森林(RF)在各种数据集变体和指标上表现一致且强劲，是可靠的基线；连通性和拓扑结构是算法性能的强预测指标；GAT-MLP模型在所有指标上表现出强劲且一致的性能。&lt;h4&gt;结论&lt;/h4&gt;双通道架构在组合算法选择中有效，图神经网络在该领域具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;广泛的实验和先前的研究表明，没有任何单一的最大团算法能在所有实例上始终表现最佳，这突显了根据实例特征选择合适算法的重要性。通过对相关研究的广泛分析，发现针对最大团问题(MCP)的算法选择研究工作较少。在本工作中，我们提出了一个基于学习的框架，结合传统机器学习和图神经网络来解决这一空白。我们通过在多样化的图实例上运行四种精确的MCP算法构建了一个标记数据集，并提取了每个图的结构和全局统计特征。我们首先评估了四种传统分类器：支持向量机(SVM)、随机森林(RF)、决策树(DT)和K近邻(KNN)，在多个数据集变体上的表现。实验结果表明，RF在指标和数据集变体中始终表现出强劲性能，使其成为一个可靠的基线。此外，特征重要性分析表明，连通性和拓扑结构是算法性能的强预测指标。基于这些发现，我们开发了一个名为GAT-MLP的双通道模型，该模型结合了用于局部结构编码的图注意力网络(GAT)和用于全局特征建模的多层感知机(MLP)。GAT-MLP模型在所有指标上表现出强劲且一致的性能。我们的结果突显了双通道架构的有效性以及图神经网络在组合算法选择中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extensive experiments and prior studies show that no single maximum cliquealgorithm consistently performs best across all instances, highlighting theimportance of selecting suitable algorithms based on instance features. Throughan extensive analysis of relevant studies, it is found that there is a lack ofresearch work concerning algorithm selection oriented toward the Maximum CliqueProblem (MCP). In this work, we propose a learning-based framework thatintegrates both traditional machine learning and graph neural networks toaddress this gap. We construct a labeled dataset by running four exact MCPalgorithms on a diverse collection of graph instances, accompanied bystructural and global statistical features extracted from each graph. We firstevaluate four conventional classifiers: Support Vector Machine (SVM), RandomForest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multipledataset variants. Experimental results show that RF consistently shows strongperformance across metrics and dataset variants, making it a reliable baseline.In addition, feature importance analysis indicates that connectivity andtopological structure are strong predictors of algorithm performance. Buildingon these findings, we develop a dual-channel model named GAT-MLP, whichcombines a Graph Attention Network (GAT) for local structural encoding with aMultilayer Perceptron (MLP) for global feature modeling. The GAT-MLP modelshows strong and consistent performance across all metrics. Our resultshighlight the effectiveness of dual-channel architectures and the promise ofgraph neural networks in combinatorial algorithm selection.</description>
      <author>example@mail.com (Xiang Li, Shanshan Wang, Chenglong Xiao)</author>
      <guid isPermaLink="false">2508.08005v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>TLV-HGNN: Thinking Like a Vertex for Memory-efficient HGNN Inference</title>
      <link>http://arxiv.org/abs/2508.07796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 9 figures, accepted by ICCD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TVL-HGNN的可重构硬件加速器，通过创新的语义完整执行范式和顶点分组技术解决了异构图神经网络推理中邻居聚合阶段的内存低效问题，实现了显著的性能提升和能耗降低。&lt;h4&gt;背景&lt;/h4&gt;异构图神经网络(HGNNs)在处理异构图数据方面表现出色，广泛应用于关键领域。在HGNN推理中，邻居聚合阶段是性能的主要决定因素，但存在两个主要的内存低效问题：按语义执行范式导致内存扩展，以及聚合过程中的冗余内存访问。&lt;h4&gt;目的&lt;/h4&gt;消除按语义的中间存储和冗余目标顶点访问，设计优化的硬件加速器，减少对共享邻居的冗余访问，从而提高HGNN推理的性能和能效。&lt;h4&gt;方法&lt;/h4&gt;提出从顶点视角出发的语义完整执行范式，设计TVL-HGNN硬件加速器，以及实现基于跨语义邻域重叠的顶点分组技术并进行硬件实现。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，TVL-HGNN比NVIDIA A100 GPU平均加速7.85倍，比最先进的HGNN加速器HiHGNN平均加速1.41倍，同时能耗降低98.79%(相比A100 GPU)和32.61%(相比HiHGNN)。&lt;h4&gt;结论&lt;/h4&gt;TVL-HGNN通过创新的执行范式和硬件设计显著提高了HGNN推理的性能和能效，解决了传统方法中的内存低效问题。&lt;h4&gt;翻译&lt;/h4&gt;异构图神经网络(HGNNs)在处理异构图数据方面表现出色，并广泛应用于关键领域。在HGNN推理中，邻居聚合阶段是性能的主要决定因素，但该阶段存在两个主要的内存低效问题。首先，常用的按语义执行范式在语义融合前为每个语义存储中间聚合结果，导致大量内存扩展。其次，聚合过程会产生大量冗余内存访问，包括跨语义重复加载目标顶点特征和由于跨语义邻域重叠导致的重复访问共享邻居。这些低效性严重限制了可扩展性并降低了HGNN推理性能。在本工作中，我们首先提出一种从顶点视角出发的语义完整执行范式，消除了按语义的中间存储和冗余目标顶点访问。基于该范式，我们设计了TVL-HGNN，这是一个为高效聚合而优化的可重构硬件加速器。此外，我们引入了一种基于跨语义邻域重叠的顶点分组技术，并进行了硬件实现，以减少对共享邻居的冗余访问。实验结果表明，TVL-HGNN比NVIDIA A100 GPU和最先进的HGNN加速器HiHGNN分别实现了平均7.85倍和1.41倍的加速，同时能耗降低了98.79%和32.61%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous graph neural networks (HGNNs) excel at processing heterogeneousgraph data and are widely applied in critical domains. In HGNN inference, theneighbor aggregation stage is the primary performance determinant, yet itsuffers from two major sources of memory inefficiency. First, the commonlyadopted per-semantic execution paradigm stores intermediate aggregation resultsfor each semantic prior to semantic fusion, causing substantial memoryexpansion. Second, the aggregation process incurs extensive redundant memoryaccesses, including repeated loading of target vertex features across semanticsand repeated accesses to shared neighbors due to cross-semantic neighborhoodoverlap. These inefficiencies severely limit scalability and reduce HGNNinference performance.  In this work, we first propose a semantics-complete execution paradigm from avertex perspective that eliminates per-semantic intermediate storage andredundant target vertex accesses. Building on this paradigm, we designTVL-HGNN, a reconfigurable hardware accelerator optimized for efficientaggregation. In addition, we introduce a vertex grouping technique based oncross-semantic neighborhood overlap, with hardware implementation, to reduceredundant accesses to shared neighbors. Experimental results demonstrate thatTVL-HGNN achieves average speedups of 7.85x and 1.41x over the NVIDIA A100 GPUand the state-of-the-art HGNN accelerator HiHGNN, respectively, while reducingenergy consumption by 98.79% and 32.61%.</description>
      <author>example@mail.com (Dengke Han, Duo Wang, Mingyu Yan, Xiaochun Ye, Dongrui Fan)</author>
      <guid isPermaLink="false">2508.07796v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Spatial Correlations between Earth Observations in Global Atmospheric State Estimation by using Adaptive Graph Structure Learning</title>
      <link>http://arxiv.org/abs/2508.07659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于时空图神经网络的方法，通过自适应边采样解决大气状态估计中的空间相关性问题，提高了天气预报准确性。&lt;h4&gt;背景&lt;/h4&gt;数值天气预报(NWP)系统是天气预报的基础，通过分析前期大气状态和新获取的地球观测数据来预测未来大气状态。然而，周围气象背景和观测位置的变化使得大气状态和观测之间的空间相关性随时间动态变化，增加了预测难度。&lt;h4&gt;目的&lt;/h4&gt;发现地球观测和大气状态之间的空间相关性，以提高全球大气状态估计的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;采用具有结构学习的时空图神经网络(STGNNs)处理动态变化的空间相关性，并通过自适应确定节点度数并考虑NWP网格点和观测之间的空间距离来调节边采样，解决结构学习中的结构信息损失和过平滑问题。&lt;h4&gt;主要发现&lt;/h4&gt;使用东亚地区的实际数据验证，即使在大气变化程度高的区域，所提方法也优于现有有结构学习和无结构学习的STGNN模型。&lt;h4&gt;结论&lt;/h4&gt;自适应边采样的时空图神经网络能有效处理动态变化的空间相关性，提高大气状态估计和天气预报的准确性。&lt;h4&gt;翻译&lt;/h4&gt;本研究旨在发现地球观测和大气状态之间的空间相关性，以提高全球大气状态估计的预测准确性，这些估计通常使用传统的数值天气预报(NWP)系统进行，是天气预报的开始。NWP系统通过分析前期大气状态和新获取的地球观测数据来预测固定位置(称为NWP网格点)的未来大气状态。因此，周围气象背景和观测位置的变化使得大气状态和观测之间的空间相关性随时间变化。为了处理这种动态变化的空间相关性，我们采用了具有结构学习的时空图神经网络(STGNNs)。然而，结构学习存在一个固有局限性，即通过生成过多边可能导致结构信息损失和过平滑问题。为了解决这个问题，我们通过自适应确定节点度数并考虑NWP网格点和观测之间的空间距离来调节边采样。我们使用东亚地区的实际大气状态和观测数据验证了所提方法的有效性。即使在大气变化程度高的区域，所提方法也优于现有有结构学习和无结构学习的STGNN模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study aims to discover spatial correlations between Earth observationsand atmospheric states to improve the forecasting accuracy of globalatmospheric state estimation, which are usually conducted using conventionalnumerical weather prediction (NWP) systems and is the beginning of weatherforecasting. NWP systems predict future atmospheric states at fixed locations,which are called NWP grid points, by analyzing previous atmospheric states andnewly acquired Earth observations without fixed locations. Thus, surroundingmeteorological context and the changing locations of the observations makespatial correlations between atmospheric states and observations over time. Tohandle complicated spatial correlations, which change dynamically, we employspatiotemporal graph neural networks (STGNNs) with structure learning. However,structure learning has an inherent limitation that this can cause structuralinformation loss and over-smoothing problem by generating excessive edges. Tosolve this problem, we regulate edge sampling by adaptively determining nodedegrees and considering the spatial distances between NWP grid points andobservations. We validated the effectiveness of the proposed method by usingreal-world atmospheric state and observation data from East Asia. Even in areaswith high atmospheric variability, the proposed method outperformed existingSTGNN models with and without structure learning.</description>
      <author>example@mail.com (Hyeon-Ju Jeon, Jeon-Ho Kang, In-Hyuk Kwon, O-Joun Lee)</author>
      <guid isPermaLink="false">2508.07659v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction</title>
      <link>http://arxiv.org/abs/2508.07624v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图的后处理流程，利用物体间的空间关系来修正检测异常，显著提高了物体检测系统的性能和可靠性。&lt;h4&gt;背景&lt;/h4&gt;在许多现实世界的静态环境应用中，物体的空间布局在不同实例中保持一致。然而，最先进的物体检测模型往往无法利用这种空间先验知识，导致预测不一致、漏检或误分类，特别是在杂乱或遮挡场景中。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于图的后处理流程，明确建模物体间的空间关系，以修正第一人称视角帧中的检测异常。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNN)在手动标注数据上训练，模型能够识别无效的物体类别标签，并根据邻域上下文预测修正后的类别标签。该方法既可以作为独立的异常检测和修正框架，也可以作为标准物体检测器(如YOLOv7和RT-DETR)的后处理模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，整合这种空间推理显著提高了检测性能，mAP@50提升了最高达4%。&lt;h4&gt;结论&lt;/h4&gt;这种方法强调了利用环境空间结构提高物体检测系统可靠性的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在许多涉及静态环境的现实世界应用中，物体的空间布局在不同实例中保持一致。然而，最先进的物体检测模型往往无法利用这种空间先验知识，导致预测不一致、漏检或误分类，特别是在杂乱或遮挡场景中。在这项工作中，我们提出了一种基于图的后处理流程，明确建模物体间的空间关系，以修正第一人称视角帧中的检测异常。使用在手动标注数据上训练的图神经网络(GNN)，我们的模型能够识别无效的物体类别标签，并根据其邻域上下文预测修正后的类别标签。我们将我们的方法评估为独立的异常检测和修正框架，以及标准物体检测器(如YOLOv7和RT-DETR)的后处理模块。实验证明，整合这种空间推理显著提高了检测性能，mAP_50提升了最高达4%。这种方法强调了利用环境空间结构提高物体检测系统可靠性的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In many real-world applications involving static environments, the spatiallayout of objects remains consistent across instances. However,state-of-the-art object detection models often fail to leverage this spatialprior, resulting in inconsistent predictions, missed detections, ormisclassifications, particularly in cluttered or occluded scenes. In this work,we propose a graph-based post-processing pipeline that explicitly models thespatial relationships between objects to correct detection anomalies inegocentric frames. Using a graph neural network (GNN) trained on manuallyannotated data, our model identifies invalid object class labels and predictscorrected class labels based on their neighbourhood context. We evaluate ourapproach both as a standalone anomaly detection and correction framework and asa post-processing module for standard object detectors such as YOLOv7 andRT-DETR. Experiments demonstrate that incorporating this spatial reasoningsignificantly improves detection performance, with mAP@50 gains of up to 4%.This method highlights the potential of leveraging the environment's spatialstructure to improve reliability in object detection systems.</description>
      <author>example@mail.com (Vishakha Lall, Yisi Liu)</author>
      <guid isPermaLink="false">2508.07624v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Neuro-Symbolic Acceleration of MILP Motion Planning with Temporal Logic and Chance Constraints</title>
      <link>http://arxiv.org/abs/2508.07515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种神经符号方法来解决自主系统中的复杂运动规划问题，通过机器学习技术引导MILP求解器的符号搜索，显著提高了计算效率和解决方案质量，比现有方法平均提升约20%。&lt;h4&gt;背景&lt;/h4&gt;自主系统需要解决日益复杂、时效性强且具有不确定性的任务中的运动规划问题。这些问题通常涉及高层任务规范，如时序逻辑或机会约束，需要解决大规模混合整数线性规划(MILP)问题。然而，现有的基于MILP的规划方法计算成本高且可扩展性有限，阻碍了它们的实时应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种神经符号方法来加速基于MILP的运动规划，利用机器学习技术引导求解器的符号搜索。&lt;h4&gt;方法&lt;/h4&gt;专注于两类代表性的规划问题：具有信号时序逻辑(STL)规范的问题和通过形式化预测编程(CPP)表述机会约束的问题。展示了如何基于图神经网络的学习方法引导传统符号MILP求解器解决具有挑战性的规划问题，包括分支变量选择和求解器参数配置。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验，表明神经符号搜索技术带来了可扩展性的提升。该方法取得了显著的改进，在关键指标上（包括运行时间和解决方案质量）比最先进的求解器平均提高了约20%的性能。&lt;h4&gt;结论&lt;/h4&gt;神经符号方法可以有效解决自主系统中的复杂运动规划问题，通过机器学习引导MILP求解器可以显著提高计算效率和解决方案质量。&lt;h4&gt;翻译&lt;/h4&gt;自主系统必须解决日益复杂、时效性强且具有不确定性的任务中的运动规划问题。这些问题通常涉及高层任务规范，如时序逻辑或机会约束，需要解决大规模混合整数线性规划(MILP)问题。然而，现有的基于MILP的规划方法计算成本高且可扩展性有限，阻碍了它们的实时应用。我们提出使用神经符号方法来加速基于MILP的运动规划，利用机器学习技术引导求解器的符号搜索。专注于两类代表性的规划问题，即具有信号时序逻辑(STL)规范的问题和通过形式化预测编程(CPP)表述机会约束的问题。我们展示了如何基于图神经网络的学习方法引导传统符号MILP求解器解决具有挑战性的规划问题，包括分支变量选择和求解器参数配置。通过大量实验，我们表明神经符号搜索技术带来了可扩展性的提升。我们的方法取得了显著的改进，在关键指标上（包括运行时间和解决方案质量）比最先进的求解器平均提高了约20%的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous systems must solve motion planning problems subject toincreasingly complex, time-sensitive, and uncertain missions. These problemsoften involve high-level task specifications, such as temporal logic or chanceconstraints, which require solving large-scale Mixed-Integer Linear Programs(MILPs). However, existing MILP-based planning methods suffer from highcomputational cost and limited scalability, hindering their real-timeapplicability. We propose to use a neuro-symbolic approach to accelerateMILP-based motion planning by leveraging machine learning techniques to guidethe solver's symbolic search. Focusing on two representative classes ofplanning problems, namely, those with Signal Temporal Logic (STL)specifications and those with chance constraints formulated via ConformalPredictive Programming (CPP). We demonstrate how graph neural network-basedlearning methods can guide traditional symbolic MILP solvers in solvingchallenging planning problems, including branching variable selection andsolver parameter configuration. Through extensive experiments, we show thatneuro-symbolic search techniques yield scalability gains. Our approach yieldssubstantial improvements, achieving an average performance gain of about 20%over state-of-the-art solver across key metrics, including runtime and solutionquality.</description>
      <author>example@mail.com (Junyang Cai, Weimin Huang, Jyotirmoy V. Deshmukh, Lars Lindemann, Bistra Dilkina)</author>
      <guid isPermaLink="false">2508.07515v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering</title>
      <link>http://arxiv.org/abs/2508.07486v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Mo2oM的框架，用于将单体软件系统转换为重叠的微服务架构，允许组件以概率方式属于多个微服务，从而提高系统的模块化、可维护性和部署灵活性。&lt;h4&gt;背景&lt;/h4&gt;现代软件系统正从单体架构转向微服务架构以提高可扩展性、可维护性和部署灵活性。然而，现有微服务提取方法通常依赖硬聚类，将每个组件分配到单一微服务，这往往增加服务间耦合并降低服务内聚。&lt;h4&gt;目的&lt;/h4&gt;提出Mo2oM框架，将微服务提取制定为软聚类问题，允许组件以概率方式属于多个微服务，以减少服务间耦合并提高服务内聚。&lt;h4&gt;方法&lt;/h4&gt;Mo2oM结合深度语义嵌入和从方法调用图中提取的结构依赖关系，以捕获功能性和架构性关系，然后使用基于图神经网络的软聚类算法生成最终的微服务集合。&lt;h4&gt;主要发现&lt;/h4&gt;在四个开源单体基准测试上评估显示，Mo2oM在结构模块化方面提高40.97%，在服务间调用百分比（通信开销）方面提高58%，在接口数量方面提高26.16%，在非极端分布（服务大小平衡）方面提高38.96%。&lt;h4&gt;结论&lt;/h4&gt;Mo2oM框架通过允许组件以概率方式属于多个微服务，有效提高了微服务架构的模块化、通信效率、接口数量平衡和服务大小平衡，显著优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;现代软件系统正越来越多地从单体架构转向微服务架构，以提高可扩展性、可维护性和部署灵活性。现有的微服务提取方法通常依赖硬聚类，将每个软件组件分配到单个微服务。这种方法通常会增加服务间耦合并降低服务内聚。我们提出了Mo2oM（单体到重叠微服务）框架，将微服务提取制定为软聚类问题，允许组件以概率方式属于多个微服务。这种方法受到专家驱动的分解启发，实践者有意地在服务间复制某些软件组件以减少通信开销。Mo2oM结合了深度语义嵌入和从方法调用图中提取的结构依赖关系，以捕获功能性和架构性关系。然后使用基于图神经网络的软聚类算法生成最终的微服务集合。我们在四个开源单体基准测试上评估了Mo2oM，并与八个最先进的基线方法进行比较。我们的结果表明，Mo2oM在所有基准测试中，结构模块化（平衡内聚和耦合）方面提高了40.97%，服务间调用百分比（通信开销）方面提高了58%，接口数量（模块化和解耦）方面提高了26.16%，非极端分布（服务大小平衡）方面提高了38.96%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern software systems are increasingly shifting from monolithicarchitectures to microservices to enhance scalability, maintainability, anddeployment flexibility. Existing microservice extraction methods typically relyon hard clustering, assigning each software component to a single microservice.This approach often increases inter-service coupling and reduces intra-servicecohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), aframework that formulates microservice extraction as a soft clustering problem,allowing components to belong probabilistically to multiple microservices. Thisapproach is inspired by expert-driven decompositions, where practitionersintentionally replicate certain software components across services to reducecommunication overhead. Mo2oM combines deep semantic embeddings with structuraldependencies extracted from methodcall graphs to capture both functional andarchitectural relationships. A graph neural network-based soft clusteringalgorithm then generates the final set of microservices. We evaluate Mo2oM onfour open-source monolithic benchmarks and compare it against eightstate-of-the-art baselines. Our results demonstrate that Mo2oM achievesimprovements of up to 40.97% in structural modularity (balancing cohesion andcoupling), 58% in inter-service call percentage (communication overhead),26.16% in interface number (modularity and decoupling), and 38.96% innon-extreme distribution (service size balance) across all benchmarks.</description>
      <author>example@mail.com (Morteza Ziabakhsh, Kiyan Rezaee, Sadegh Eskandari, Seyed Amir Hossein Tabatabaei, Mohammad M. Ghassemi)</author>
      <guid isPermaLink="false">2508.07486v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures</title>
      <link>http://arxiv.org/abs/2508.07423v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD thesis, Chapters 8 and 9 include results from work performed in  collaboration with Anthony Correia&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了为CERN的LHCb实验开发的基于图神经网络的管道，用于带电粒子轨迹重建，该管道完全在GPU上实现并嵌入到第一级触发器中，同时也在FPGA架构上进行了加速，并与经典算法进行了性能比较。&lt;h4&gt;背景&lt;/h4&gt;粒子物理需要更高精度测试亚原子模型，对撞机实验探测器升级导致更多碰撞和复杂相互作用，数据量大幅增加，CERN产生的数据量巨大，需要实时过滤和选择。&lt;h4&gt;目的&lt;/h4&gt;理解如何在高频率数据处理环境中高效部署机器学习模型，最大化吞吐量并最小化能耗，应对严格的高频率数据速率挑战。&lt;h4&gt;方法&lt;/h4&gt;开发基于图神经网络的管道用于LHCb实验中的带电粒子轨迹重建，完全在GPU上实现端到端嵌入到第一级触发器中，与经典跟踪算法比较性能，并在FPGA架构上加速，比较GPU和FPGA实现的功耗和处理速度。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体主要发现。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及具体结论。&lt;h4&gt;翻译&lt;/h4&gt;随着粒子物理界需要越来越高的精度来测试我们对亚原子世界的当前模型，更大的数据集是必要的。随着世界各地对撞机实验探测器的升级计划，特别是CERN的大型强子对撞机，预计会有更多的碰撞和更复杂的相互作用。这直接意味着产生的数据量增加，以及处理这些数据所需的计算资源相应增加。在CERN，产生的数据量是巨大的。这就是为什么数据必须在永久存储前进行实时过滤和选择。然后，这些数据可用于进行物理分析，以扩展我们对宇宙的当前理解并改进物理的标准模型。这种实时过滤，称为触发，涉及复杂的处理，频率高达40 MHz。本论文有助于理解如何在这样的环境中高效部署机器学习模型，以最大化吞吐量并最小化能耗。为了应对严格的高频率数据速率带来的挑战，不可避免地需要用于此类任务的现代硬件和当代算法。在本工作中，我展示了我们为CERN的LHCb实验中带电粒子轨迹重建而开发的基于图神经网络的管道。该管道完全在GPU上实现，端到端地嵌入到LHCb的第一级触发器中。其性能与LHCb当前使用的经典跟踪算法进行了比较。该管道还在FPGA架构上进行了加速，并在功耗和处理速度方面将其性能与GPU实现进行了比较。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the particle physics community needs higher and higher precisions in orderto test our current model of the subatomic world, larger and larger datasetsare necessary. With upgrades scheduled for the detectors of colliding-beamexperiments around the world, and specifically at the Large Hadron Collider atCERN, more collisions and more complex interactions are expected. This directlyimplies an increase in data produced and consequently in the computationalresources needed to process them. At CERN, the amount of data produced isgargantuan. This is why the data have to be heavily filtered and selected inreal time before being permanently stored. This data can then be used toperform physics analyses, in order to expand our current understanding of theuniverse and improve the Standard Model of physics. This real-time filtering,known as triggering, involves complex processing happening often at frequenciesas high as 40 MHz. This thesis contributes to understanding how machinelearning models can be efficiently deployed in such environments, in order tomaximize throughput and minimize energy consumption. Inevitably, modernhardware designed for such tasks and contemporary algorithms are needed inorder to meet the challenges posed by the stringent, high-frequency data rates.In this work, I present our graph neural network-based pipeline, developed forcharged particle track reconstruction at the LHCb experiment at CERN. Thepipeline was implemented end-to-end inside LHCb's first-level trigger, entirelyon GPUs. Its performance was compared against the classical tracking algorithmscurrently in production at LHCb. The pipeline was also accelerated on the FPGAarchitecture, and its performance in terms of power consumption and processingspeed was compared against the GPU implementation.</description>
      <author>example@mail.com (Fotis I. Giasemis)</author>
      <guid isPermaLink="false">2508.07423v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging GNN to Enhance MEF Method in Predicting ENSO</title>
      <link>http://arxiv.org/abs/2508.07410v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于图分析的新框架来改进ENSO预测，通过构建集合成员之间的相似性图并使用社区检测方法选择最优子集，提高了预测技能，提供了对集合行为的新见解，并适用于其他预测模型。&lt;h4&gt;背景&lt;/h4&gt;ENSO的长期可靠预测一直是气候科学中的长期挑战。之前开发的MEF模型使用了80个集合预测，通过两个独立的深度学习模块：3D卷积神经网络和时间序列模块，但未对单个集合成员进行单独加权或测试，这可能限制了模型对高性能但分散预测的优化使用。&lt;h4&gt;目的&lt;/h4&gt;改进现有的预测框架，优化使用高性能但分散的预测，提高ENSO长期预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于图分析的新框架，构建一个无向图，顶点是集合输出，边权重通过RMSE和相关性测量相似性，使用社区检测方法识别并聚类结构相似且准确的预测，从中获得20个成员的优化子集，通过平均这个优化子集获得最终预测。&lt;h4&gt;主要发现&lt;/h4&gt;该方法通过去除噪声和强调集合一致性提高了预测技能；基于图的选择显示顶级预测者具有稳健的统计特征，提供了新的集合行为见解；GNN方法在复合长期情况下产生更稳定和一致的输出；该方法是模型不可知的，可以应用于其他具有巨大集合输出的预测模型。&lt;h4&gt;结论&lt;/h4&gt;基于图的集合选择方法改进了ENSO预测的准确性，并可以推广到其他预测模型。&lt;h4&gt;翻译&lt;/h4&gt;厄尔尼诺-南方振荡(ENSO)的可靠长期预测一直是气候科学中的一个长期挑战。先前开发的多模态ENSO预测(MEF)模型使用两个独立深度学习模块的80个集合预测：一个3D卷积神经网络(3D-CNN)和一个时间序列模块。在他们的方法中，两个模块的输出使用一种加权策略组合，其中一个根据全球性能优先于另一个。然而，没有对单个集合成员进行单独加权或测试，这可能限制了模型优化使用高性能但分散的预测。在本研究中，我们提出了一个更好的框架，使用基于图的分析直接建模所有80个集合成员之间的相似性。通过构建一个无向图，其顶点是集合输出，边上的权重通过RMSE和相关性测量相似性，我们识别并聚类结构相似且准确的预测。从中我们使用社区检测方法获得了一个由20个成员组成的优化子集。然后通过平均这个优化子集获得最终预测。这种方法通过去除噪声和强调集合一致性提高了预测技能。有趣的是，我们的基于图的选择显示顶级预测者具有稳健的统计特征，提供了新的集合行为见解。此外，我们观察到，虽然基于GNN的方法在每种情况下并不总是优于基线MEF，但它产生更稳定和一致的输出，特别是在复合长期情况下。该方法也是模型不可知的，表明它可以直接应用于其他具有巨大集合输出的预测模型，如统计、物理或混合模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable long-lead forecasting of the El Nino Southern Oscillation (ENSO)remains a long-standing challenge in climate science. The previously developedMultimodal ENSO Forecast (MEF) model uses 80 ensemble predictions by twoindependent deep learning modules: a 3D Convolutional Neural Network (3D-CNN)and a time-series module. In their approach, outputs of the two modules arecombined using a weighting strategy wherein one is prioritized over the otheras a function of global performance. Separate weighting or testing ofindividual ensemble members did not occur, however, which may have limited themodel to optimize the use of high-performing but spread-out forecasts. In thisstudy, we propose a better framework that employs graph-based analysis todirectly model similarity between all 80 members of the ensemble. Byconstructing an undirected graph whose vertices are ensemble outputs and whoseweights on edges measure similarity (via RMSE and correlation), we identify andcluster structurally similar and accurate predictions. From which we obtain anoptimized subset of 20 members using community detection methods. The finalprediction is then obtained by averaging this optimized subset. This methodimproves the forecast skill through noise removal and emphasis on ensemblecoherence. Interestingly, our graph-based selection shows robust statisticalcharacteristics among top performers, offering new ensemble behavior insights.In addition, we observe that while the GNN-based approach does not alwaysoutperform the baseline MEF under every scenario, it produces more stable andconsistent outputs, particularly in compound long-lead situations. The approachis model-agnostic too, suggesting that it can be applied directly to otherforecasting models with gargantuan ensemble outputs, such as statistical,physical, or hybrid models.</description>
      <author>example@mail.com (Saghar Ganji, Mohammad Naisipour)</author>
      <guid isPermaLink="false">2508.07410v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.07122v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于时空图神经网络的性能预测算法，用于解决具有多层服务调用结构的分布式后端系统中性能波动的预测挑战。&lt;h4&gt;背景&lt;/h4&gt;分布式后端系统面临性能波动预测的挑战，这些系统具有多层服务调用结构，使得性能预测变得复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测分布式后端系统性能波动的算法，帮助系统管理者更好地进行性能优化和资源分配。&lt;h4&gt;方法&lt;/h4&gt;将不同时间片的系统状态抽象为图结构序列，整合服务节点的运行时特征与服务间的调用关系构建统一时空建模框架；使用图卷积网络提取服务拓扑的高阶依赖信息；采用门控循环网络捕获性能指标的动态演化；引入时间编码机制增强对非平稳时间序列的表示能力；通过端到端方式训练架构实现高精度回归。&lt;h4&gt;主要发现&lt;/h4&gt;在大规模公共集群数据集上的实验表明，该方法在MAE、RMSE和R2等关键指标上优于现有代表性方法；在不同负载强度和结构复杂度下保持强鲁棒性；设计的多维度实验（包括时间窗口和并发负载水平变化）全面验证了模型的预测性能和稳定性。&lt;h4&gt;结论&lt;/h4&gt;该模型在后端服务性能管理任务中具有实际应用潜力，能够有效支持分布式系统的性能优化决策。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于时空图神经网络的性能预测算法，以应对具有多层服务调用结构的分布式后端系统中性能波动的预测挑战。该方法将不同时间片的系统状态抽象为图结构序列，整合服务节点的运行时特征与服务间的调用关系，构建统一的时空建模框架。模型首先应用图卷积网络从服务拓扑中提取高阶依赖信息，然后使用门控循环网络捕获性能指标随时间的动态演化。同时引入时间编码机制增强模型对非平稳时间序列的表示能力。该架构以端到端方式训练，优化多层嵌套结构以实现未来服务性能指标的高精度回归。为验证所提方法的有效性，使用了大规模公共集群数据集，并设计了一系列多维度实验，包括时间窗口和并发负载水平的变化。这些实验全面评估了模型的预测性能和稳定性。实验结果表明，所提模型在MAE、RMSE和R2等关键指标上优于现有代表性方法，并在不同负载强度和结构复杂度下保持强鲁棒性。这些结果证明了该模型在后端服务性能管理任务中的实际应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a spatiotemporal graph neural network-based performanceprediction algorithm to address the challenge of forecasting performancefluctuations in distributed backend systems with multi-level service callstructures. The method abstracts system states at different time slices into asequence of graph structures. It integrates the runtime features of servicenodes with the invocation relationships among services to construct a unifiedspatiotemporal modeling framework. The model first applies a graphconvolutional network to extract high-order dependency information from theservice topology. Then it uses a gated recurrent network to capture the dynamicevolution of performance metrics over time. A time encoding mechanism is alsointroduced to enhance the model's ability to represent non-stationary temporalsequences. The architecture is trained in an end-to-end manner, optimizing themulti-layer nested structure to achieve high-precision regression of futureservice performance metrics. To validate the effectiveness of the proposedmethod, a large-scale public cluster dataset is used. A series ofmulti-dimensional experiments are designed, including variations in timewindows and concurrent load levels. These experiments comprehensively evaluatethe model's predictive performance and stability. The experimental results showthat the proposed model outperforms existing representative methods across keymetrics such as MAE, RMSE, and R2. It maintains strong robustness under varyingload intensities and structural complexities. These results demonstrate themodel's practical potential for backend service performance management tasks.</description>
      <author>example@mail.com (Zhihao Xue, Yun Zi, Nia Qi, Ming Gong, Yujun Zou)</author>
      <guid isPermaLink="false">2508.07122v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context</title>
      <link>http://arxiv.org/abs/2508.07117v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 3 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了LOGIC，一种用于图神经网络(GNN)预测解释的轻量级事后框架，利用大型语言模型生成忠实且可解释的解释，通过将GNN节点嵌入投影到LLM嵌入空间并构建混合提示，使LLM能够推理GNN内部表示并生成自然语言解释和简洁解释子图。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为处理结构化数据(包括文本属性图)的强大工具，常见于引文网络、社交平台和知识图谱等领域。然而，GNN本身不具有内在可解释性，现有解释方法在生成可解释的细粒度理由方面存在困难，特别是当节点属性包含丰富的自然语言时。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够为GNN预测生成忠实且可解释的解释的方法，提高解释质量使其更符合人类推理方式，并在保真度和稀疏性之间取得良好平衡。&lt;h4&gt;方法&lt;/h4&gt;提出LOGIC框架，将GNN节点嵌入投影到大型语言模型嵌入空间，构建混合提示交错软提示与图结构文本输入，使LLM能够推理GNN内部表示并生成自然语言解释和简洁解释子图。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实文本属性图数据集上的实验表明，LOGIC在保真度和稀疏性之间取得有利平衡，同时显著提高了以人为中心的指标如洞察力。&lt;h4&gt;结论&lt;/h4&gt;LOGIC为图神经网络提供了一种新的解释方法，通过利用大型语言模型能力生成更符合人类推理方式的解释，为基于LLM的图学习可解释性开辟了新方向。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为处理结构化数据的强大工具，包括文本属性图，这些图在引文网络、社交平台和知识图谱等领域很常见。GNN本身不具有内在可解释性，因此提出了许多解释方法。然而，现有的解释方法在生成可解释的细粒度理由方面往往存在困难，特别是当节点属性包含丰富的自然语言时。在这项工作中，我们介绍了LOGIC，一种轻量级的事后框架，使用大型语言模型(LLM)为GNN预测生成忠实且可解释的解释。LOGIC将GNN节点嵌入投影到LLM嵌入空间，并构建混合提示，将软提示与来自图结构的文本输入交错。这使得LLM能够推理GNN的内部表示，并生成自然语言解释以及简洁的解释子图。我们在四个真实的TAG数据集上的实验表明，LOGIC在保真度和稀疏性之间取得了有利的平衡，同时显著提高了以人为中心的指标，如洞察力。LOGIC通过将GNN内部与人类推理对齐，为基于LLM的图学习可解释性设定了新方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as powerful tools for learning overstructured data, including text-attributed graphs, which are common in domainssuch as citation networks, social platforms, and knowledge graphs. GNNs are notinherently interpretable and thus, many explanation methods have been proposed.However, existing explanation methods often struggle to generate interpretable,fine-grained rationales, especially when node attributes include rich naturallanguage. In this work, we introduce LOGIC, a lightweight, post-hoc frameworkthat uses large language models (LLMs) to generate faithful and interpretableexplanations for GNN predictions. LOGIC projects GNN node embeddings into theLLM embedding space and constructs hybrid prompts that interleave soft promptswith textual inputs from the graph structure. This enables the LLM to reasonabout GNN internal representations and produce natural language explanationsalong with concise explanation subgraphs. Our experiments across fourreal-world TAG datasets demonstrate that LOGIC achieves a favorable trade-offbetween fidelity and sparsity, while significantly improving human-centricmetrics such as insightfulness. LOGIC sets a new direction for LLM-basedexplainability in graph learning by aligning GNN internals with humanreasoning.</description>
      <author>example@mail.com (Peyman Baghershahi, Gregoire Fournier, Pranav Nyati, Sourav Medya)</author>
      <guid isPermaLink="false">2508.07117v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation</title>
      <link>http://arxiv.org/abs/2508.07106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为BrainATCL的无监督、非参数化自适应时间序列脑连接学习框架，用于捕捉功能性磁共振成像(fMRI)数据中的长程时间依赖性。该方法在功能链接预测和年龄估计任务中表现出优越的性能和强大的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;功能性磁共振成像(fMRI)是一种广泛用于研究人类大脑活动的成像技术。即使在个体休息时，大脑各区域的fMRI信号也会以高度结构化的方式短暂同步和去同步，形成功能连接动力学。这些动力学可能与行为和神经精神疾病相关，但传统的图神经网络(GNN)难以捕捉动态fMRI数据中的长程时间依赖性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉动态fMRI数据中长程时间依赖性的方法，用于功能链接预测和年龄估计，从而更好地理解大脑功能连接动力学及其与行为和疾病的关系。&lt;h4&gt;方法&lt;/h4&gt;作者提出了BrainATCL框架，该方法根据新增边的速率动态调整每个快照的回溯窗口，使用GINE-Mamba2主干编码图序列来学习来自1000名参与者静息态fMRI数据的动态功能连接的空间-时间表示。此外，还融入了脑结构和功能信息提示的边属性（左右半球身份和子网络成员身份），以增强模型捕捉生物学上有意义的拓扑模式的能力。&lt;h4&gt;主要发现&lt;/h4&gt;BrainATCL在功能链接预测和年龄估计两个任务上均表现出优越的性能和强大的泛化能力，包括在跨会话预测场景中。该方法能够有效捕捉动态fMRI数据中的长程时间依赖性，并识别出生物学上有意义的脑区连接模式。&lt;h4&gt;结论&lt;/h4&gt;BrainATCL框架为研究大脑功能连接动力学提供了一种有效工具，能够捕捉传统方法难以处理的复杂时间依赖关系。该方法的优越性能和泛化能力表明其在脑功能研究和临床应用中的潜力，特别是在理解神经精神疾病和大脑老化过程方面。&lt;h4&gt;翻译&lt;/h4&gt;功能性磁共振成像(fMRI)是一种广泛用于研究人类大脑活动的成像技术。即使在个体休息时，大脑各区域的fMRI信号也会以高度结构化的方式短暂同步和去同步。这些功能连接动力学可能与行为和神经精神疾病有关。为了建模这些动力学，时间序列脑连接表示是必要的，因为它们反映了脑区之间不断变化的相互作用，并提供了关于瞬时神经状态和网络重构的见解。然而，传统的图神经网络(GNN)往往难以捕捉动态fMRI数据中的长程时间依赖性。为了应对这一挑战，我们提出了BrainATCL，这是一种用于自适应时间序列脑连接学习的无监督、非参数化框架，能够实现功能链接预测和年龄估计。我们的方法根据新增边的速率动态调整每个快照的回溯窗口。随后使用GINE-Mamba2主干对图序列进行编码，学习来自人类连接组计划中1000名参与者的静息态fMRI数据的动态功能连接的空间-时间表示。为了进一步提高空间建模，我们融入了脑结构和功能信息提示的边属性，即脑区的左右半球身份和子网络成员身份，使模型能够捕捉生物学上有意义的拓扑模式。我们在两个任务上评估了BrainATCL：功能链接预测和年龄估计。实验结果展示了优越的性能和强大的泛化能力，包括在跨会话预测场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widelyused to study human brain activity. fMRI signals in areas across the braintransiently synchronise and desynchronise their activity in a highly structuredmanner, even when an individual is at rest. These functional connectivitydynamics may be related to behaviour and neuropsychiatric disease. To modelthese dynamics, temporal brain connectivity representations are essential, asthey reflect evolving interactions between brain regions and provide insightinto transient neural states and network reconfigurations. However,conventional graph neural networks (GNNs) often struggle to capture long-rangetemporal dependencies in dynamic fMRI data. To address this challenge, wepropose BrainATCL, an unsupervised, nonparametric framework for adaptivetemporal brain connectivity learning, enabling functional link prediction andage estimation. Our method dynamically adjusts the lookback window for eachsnapshot based on the rate of newly added edges. Graph sequences aresubsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporalrepresentations of dynamic functional connectivity in resting-state fMRI dataof 1,000 participants from the Human Connectome Project. To further improvespatial modeling, we incorporate brain structure and function-informed edgeattributes, i.e., the left/right hemispheric identity and subnetwork membershipof brain regions, enabling the model to capture biologically meaningfultopological patterns. We evaluate our BrainATCL on two tasks: functional linkprediction and age estimation. The experimental results demonstrate superiorperformance and strong generalization, including in cross-session predictionscenarios.</description>
      <author>example@mail.com (Yiran Huang, Amirhossein Nouranizadeh, Christine Ahrends, Mengjia Xu)</author>
      <guid isPermaLink="false">2508.07106v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>ScamDetect: Towards a Robust, Agnostic Framework to Uncover Threats in Smart Contracts</title>
      <link>http://arxiv.org/abs/2508.07094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ScamDetec框架，一个用于智能合约恶意软件检测的稳健、模块化且平台无关的解决方案，旨在应对日益复杂的字节码混淆技术和区块链环境异构性带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;智能合约通过可编程、无需信任的交易改变了去中心化金融，但其广泛采用和增长的经济重要性吸引了持续而复杂的威胁，如网络钓鱼活动和合约级别漏洞利用。传统基于交易的威胁检测方法会暴露敏感用户数据，引发隐私和安全问题。&lt;h4&gt;目的&lt;/h4&gt;开发ScamDetec框架，解决两个紧迫挑战：日益复杂的字节码混淆技术用于规避静态分析，以及区块链环境异构性需要平台无关的解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出分阶段发展策略：第一阶段使用图神经网络分析控制流图来处理混淆的以太坊虚拟机字节码；第二阶段将检测能力泛化到新兴运行时如WASM。前期工作PhishingHook通过静态字节码和操作码分析实现了约90%的检测准确率。&lt;h4&gt;主要发现&lt;/h4&gt;静态字节码分析可作为主动缓解策略，在恶意合约执行有害操作前识别它们；PhishingHook框架实现了约90%的智能合约网络钓鱼活动检测准确率。&lt;h4&gt;结论&lt;/h4&gt;ScamDetec旨在为去中心化生态系统的未来提供主动、可扩展的安全保障。&lt;h4&gt;翻译&lt;/h4&gt;智能合约通过可编程、无需信任的交易改变了去中心化金融。然而，其广泛采用和日益增长的经济重要性吸引了持续而复杂的威胁，如网络钓鱼活动和合约级别漏洞利用。传统基于交易的威胁检测方法通常会暴露敏感的用户数据和交互，引发隐私和安全问题。作为回应，静态字节码分析已成为一种主动缓解策略，在恶意合约执行有害操作之前识别它们。基于这种方法，我们引入了PhishingHook，第一个基于机器学习的框架，通过静态字节码和操作码分析检测智能合约中的网络钓鱼活动，实现了约90%的检测准确率。然而，两个紧迫挑战仍然存在：(1)日益复杂的字节码混淆技术用于规避静态分析，(2)区块链环境的异构性需要平台无关的解决方案。本文提出了ScamDetec（智能合约通用恶意软件检测器）的愿景，这是一个稳健、模块化且平台无关的智能合约恶意软件检测框架。在未来2.5年内，ScamDetec将分两个阶段发展：首先，通过图神经网络分析控制流图来处理混淆的以太坊虚拟机字节码，利用GNN捕获操作码序列之外的复杂结构模式的能力；其次，将检测能力泛化到新兴运行时如WASM。ScamDetec旨在为去中心化生态系统的未来提供主动、可扩展的安全性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/DSN-S65789.2025.00068&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Smart contracts have transformed decentralized finance by enablingprogrammable, trustless transactions. However, their widespread adoption andgrowing financial significance have attracted persistent and sophisticatedthreats, such as phishing campaigns and contract-level exploits. Traditionaltransaction-based threat detection methods often expose sensitive user data andinteractions, raising privacy and security concerns. In response, staticbytecode analysis has emerged as a proactive mitigation strategy, identifyingmalicious contracts before they execute harmful actions.Building on thisapproach, we introduced PhishingHook, the first machine-learning-basedframework for detecting phishing activities in smart contracts via staticbytecode and opcode analysis, achieving approximately 90% detection accuracy.Nevertheless, two pressing challenges remain: (1) the increasing use ofsophisticated bytecode obfuscation techniques designed to evade staticanalysis, and (2) the heterogeneity of blockchain environments requiringplatform-agnostic solutions.This paper presents a vision for ScamDetect (SmartContract Agnostic Malware Detector), a robust, modular, and platform-agnosticframework for smart contract malware detection. Over the next 2.5 years,ScamDetect will evolve in two stages: first, by tackling obfuscated EthereumVirtual Machine (EVM) bytecode through graph neural network (GNN) analysis ofcontrol flow graphs (CFGs), leveraging GNNs' ability to capture complexstructural patterns beyond opcode sequences; and second, by generalizingdetection capabilities to emerging runtimes such as WASM. ScamDetect aims toenable proactive, scalable security for the future of decentralized ecosystems.</description>
      <author>example@mail.com (Pasquale De Rosa, Pascal Felber, Valerio Schiavoni)</author>
      <guid isPermaLink="false">2508.07094v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.07028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Manuscript under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FOCUS-Med是一种创新的息肉分割方法，通过结合图卷积网络、自注意力机制和多尺度融合策略，有效解决了内镜图像中息肉分割的挑战，在五个关键指标上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;准确的内镜图像息肉分割对早期结直肠癌检测至关重要，但这一任务具有挑战性，因为息肉与周围黏膜对比度低、存在镜面高光、边界不清晰。&lt;h4&gt;目的&lt;/h4&gt;解决内镜图像中息肉分割的挑战，提高分割准确性，从而辅助早期结直肠癌检测。&lt;h4&gt;方法&lt;/h4&gt;提出FOCUS-Med方法，整合双图卷积网络（Dual-GCN）模块捕获空间和拓扑结构依赖，采用位置融合的独立自注意力机制加强全局上下文集成，引入可训练的加权快速归一化融合策略处理编码器-解码器层间的语义差距，并首次使用大型语言模型提供分割质量的定性评估。&lt;h4&gt;主要发现&lt;/h4&gt;FOCUS-Med能够更好地保留边界并描绘息肉的复杂形状，在五个关键指标上实现了最先进的性能，证明了其在AI辅助结肠镜检查中的有效性和临床潜力。&lt;h4&gt;结论&lt;/h4&gt;FOCUS-Med是一种有效的息肉分割方法，具有临床应用潜力，可以辅助结肠镜检查中的早期结直肠癌检测。&lt;h4&gt;翻译&lt;/h4&gt;准确的内镜图像息肉分割对早期结直肠癌检测至关重要。然而，由于与周围黏膜对比度低、镜面高光和边界不清晰，这一任务仍然具有挑战性。为应对这些挑战，我们提出了FOCUS-Med，即内镜医学成像中具有注意力上下文感知的息肉分割的空间和结构图融合。FOCUS-Med集成了双图卷积网络（Dual-GCN）模块来捕获上下文空间和拓扑结构依赖关系。这种基于图的表示使模型能够利用拓扑线索和空间连接来更好地区分息肉和背景组织，而这些信息在原始图像强度中通常被掩盖。它增强了模型保留边界和描绘息肉典型复杂形状的能力。此外，采用位置融合的独立自注意力机制来加强全局上下文集成。为了弥合编码器-解码器层之间的语义差距，我们纳入了可训练的加权快速归一化融合策略以实现有效的多尺度聚合。值得注意的是，我们首次引入使用大型语言模型（LLM）来提供分割质量的详细定性评估。在公共基准上的大量实验表明，FOCUS-Med在五个关键指标上实现了最先进的性能，强调了其在AI辅助结肠镜检查中的有效性和临床潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate endoscopic image segmentation on the polyps is critical for earlycolorectal cancer detection. However, this task remains challenging due to lowcontrast with surrounding mucosa, specular highlights, and indistinctboundaries. To address these challenges, we propose FOCUS-Med, which stands forFusion of spatial and structural graph with attentional context-aware polypsegmentation in endoscopic medical imaging. FOCUS-Med integrates a Dual GraphConvolutional Network (Dual-GCN) module to capture contextual spatial andtopological structural dependencies. This graph-based representation enablesthe model to better distinguish polyps from background tissues by leveragingtopological cues and spatial connectivity, which are often obscured in rawimage intensities. It enhances the model's ability to preserve boundaries anddelineate complex shapes typical of polyps. In addition, a location-fusedstand-alone self-attention is employed to strengthen global contextintegration. To bridge the semantic gap between encoder-decoder layers, weincorporate a trainable weighted fast normalized fusion strategy for efficientmulti-scale aggregation. Notably, we are the first to introduce the use of aLarge Language Model (LLM) to provide detailed qualitative evaluations ofsegmentation quality. Extensive experiments on public benchmarks demonstratethat FOCUS-Med achieves state-of-the-art performance across five key metrics,underscoring its effectiveness and clinical potential for AI-assistedcolonoscopy.</description>
      <author>example@mail.com (Juntong Fan, Shuyi Fan, Debesh Jha, Changsheng Fang, Tieyong Zeng, Hengyong Yu, Dayang Wang)</author>
      <guid isPermaLink="false">2508.07028v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Blending Sequential Embeddings, Graphs, and Engineered Features: 4th Place Solution in RecSys Challenge 2025</title>
      <link>http://arxiv.org/abs/2508.06970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文描述了'ambitious'团队在RecSysChallenge 2025比赛中获得的第四名解决方案，该方案专注于通用行为建模，通过整合多种技术生成对多种下游任务有效的用户嵌入。&lt;h4&gt;背景&lt;/h4&gt;RecSysChallenge 2025由Synerise和ACM RecSys组织，专注于通用行为建模领域的研究与竞赛。&lt;h4&gt;目的&lt;/h4&gt;比赛目标是生成能够在六个不同下游任务中有效的用户嵌入，而本文旨在提出一种能够实现这一目标的解决方案。&lt;h4&gt;方法&lt;/h4&gt;解决方案整合了四个主要组件：序列编码器捕捉用户兴趣的时序演变；图神经网络增强模型泛化能力；深度交叉网络建模高阶特征交互；以及针对性能关键的特征工程方法。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及主要研究发现。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及结论。&lt;h4&gt;翻译&lt;/h4&gt;本文描述了'ambitious'团队在由Synerise和ACM RecSys组织的RecSysChallenge 2025比赛中获得的第四名解决方案，该比赛专注于通用行为建模。比赛目标是为六个不同的下游任务生成有效的用户嵌入。我们的解决方案整合了以下技术：(1)序列编码器，用于捕捉用户兴趣的时序演变；(2)图神经网络，用于增强泛化能力；(3)深度交叉网络，用于建模高阶特征交互；(4)性能关键特征工程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3758126.3758131&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper describes the 4th-place solution by team ambitious for the RecSysChallenge 2025, organized by Synerise and ACM RecSys, which focused onuniversal behavioral modeling. The challenge objective was to generate userembeddings effective across six diverse downstream tasks. Our solutionintegrates (1) a sequential encoder to capture the temporal evolution of userinterests, (2) a graph neural network to enhance generalization, (3) a deepcross network to model high-order feature interactions, and (4)performance-critical feature engineering.</description>
      <author>example@mail.com (Sergei Makeev, Alexandr Andreev, Vladimir Baikalov, Vladislav Tytskiy, Aleksei Krasilnikov, Kirill Khrylchenko)</author>
      <guid isPermaLink="false">2508.06970v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Distribution Shift in Graph-Based Android Malware Classification via Function Metadata and LLM Embeddings</title>
      <link>http://arxiv.org/abs/2508.06734v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 3 figures, 7 tables, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种语义增强框架，通过添加上下文特征来改进基于图的恶意软件分类器，解决了现有方法在处理未见过的恶意软件变种时的泛化能力不足问题。&lt;h4&gt;背景&lt;/h4&gt;基于图的恶意软件分类器在标准Android数据集上可达到94%以上准确率，但在评估同一家族中未见过的恶意软件变种时，准确率会下降高达45%，表明现有方法无法捕捉更深层次的语义模式。&lt;h4&gt;目的&lt;/h4&gt;开发一个强大的语义增强框架，提高恶意软件分类器在分布变化情况下的泛化能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出语义增强框架，通过添加函数级元数据和来自大型语言模型的代码嵌入等上下文特征来增强函数调用图，设计用于处理特征可用性不一致的现实约束，支持语义信号的灵活集成。&lt;h4&gt;主要发现&lt;/h4&gt;引入了MalNet-Tiny-Common和MalNet-Tiny-Distinct两个新基准测试数据集；实验表明该方法在分布变化情况下将分类性能提高高达8%；与基于适应的方法结合使用时能持续增强鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为在不断变化的威胁环境中构建弹性恶意软件检测系统提供了实用途径。&lt;h4&gt;翻译&lt;/h4&gt;基于图的恶意软件分类器可以在标准Android数据集上实现超过94%的准确率，然而我们发现，当在来自同一家族但之前未见过的恶意软件变种上进行评估时，它们的准确率会下降高达45%——这种情况通常应该表现出强大的泛化能力。这突显了现有方法的一个关键局限性：模型架构及其仅基于结构的表示往往无法捕捉更深层次的语义模式。在这项工作中，我们提出了一个强大的语义增强框架，通过添加上下文特征（包括函数级元数据和可用的来自大型语言模型的代码嵌入）来增强函数调用图。该框架设计用于在特征可用性不一致的现实约束下运行，并支持语义信号的灵活集成。为了评估在真实领域和时间变化下的泛化能力，我们引入了两个新的基准测试：MalNet-Tiny-Common和MalNet-Tiny-Distinct，它们使用恶意软件家族分区构建，以模拟跨家族泛化和不断变化的威胁行为。在多个图神经网络骨干网络上的实验表明，我们的方法在分布变化的情况下将分类性能提高了高达8%，并且与基于适应的方法集成时能持续增强鲁棒性。这些结果为在不断变化的威胁环境中构建弹性恶意软件检测系统提供了实用途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-based malware classifiers can achieve over 94% accuracy on standardAndroid datasets, yet we find they suffer accuracy drops of up to 45% whenevaluated on previously unseen malware variants from the same family - ascenario where strong generalization would typically be expected. Thishighlights a key limitation in existing approaches: both the modelarchitectures and their structure-only representations often fail to capturedeeper semantic patterns. In this work, we propose a robust semantic enrichmentframework that enhances function call graphs with contextual features,including function-level metadata and, when available, code embeddings derivedfrom large language models. The framework is designed to operate underreal-world constraints where feature availability is inconsistent, and supportsflexible integration of semantic signals. To evaluate generalization underrealistic domain and temporal shifts, we introduce two new benchmarks:MalNet-Tiny-Common and MalNet-Tiny-Distinct, constructed using malware familypartitioning to simulate cross-family generalization and evolving threatbehavior. Experiments across multiple graph neural network backbones show thatour method improves classification performance by up to 8% under distributionshift and consistently enhances robustness when integrated withadaptation-based methods. These results offer a practical path toward buildingresilient malware detection systems in evolving threat environments.</description>
      <author>example@mail.com (Ngoc N. Tran, Anwar Said, Waseem Abbas, Tyler Derr, Xenofon D. Koutsoukos)</author>
      <guid isPermaLink="false">2508.06734v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks</title>
      <link>http://arxiv.org/abs/2508.06663v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究将Kolmogorov-Arnold Networks (KANs)集成到图神经网络中，提高了节点分类准确率，并通过知识融合方法显著提升了模型性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在图结构数据上表现出色，但依赖图连接性限制了其可扩展性和效率。KANs是一种具有可学习单变量函数的架构，提供强大的非线性表达能力和高效的推理能力。&lt;h4&gt;目的&lt;/h4&gt;将KANs集成到GNN架构中以提高表达能力和推理效率，并探索通过知识融合进一步提升模型性能的可能性。&lt;h4&gt;方法&lt;/h4&gt;将KANs集成到三种流行的GNN架构（GAT、SGC和APPNP）中，分别得到KGAT、KSGC和KAPPNP三种新模型。采用多教师知识融合框架，将多个基于KAN的GNN知识蒸馏到一个与图无关的KAN学生模型中。&lt;h4&gt;主要发现&lt;/h4&gt;提出的模型提高了节点分类准确率，知识融合方法显著提升了学生模型的性能。&lt;h4&gt;结论&lt;/h4&gt;KANs在增强GNN表达能力和实现高效、与图无关的推理方面具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在图结构数据上表现出色，但它们对图连接性的依赖往往限制了其可扩展性和效率。Kolmogorov-Arnold Networks（KANs）是一种最近的架构，具有可学习的单变量函数，提供强大的非线性表达能力和高效的推理能力。在这项工作中，我们将KANs集成到三种流行的GNN架构中——GAT、SGC和APPNP，从而得到三种新模型：KGAT、KSGC和KAPPNP。我们进一步采用了一个多教师知识融合框架，其中多个基于KAN的GNN知识被蒸馏到一个与图无关的KAN学生模型中。在基准数据集上的实验表明，提出的模型提高了节点分类准确率，知识融合方法显著提升了学生模型的性能。我们的研究结果强调了KANs在增强GNN表达能力和实现高效、与图无关的推理方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown strong performance ongraph-structured data, but their reliance on graph connectivity often limitsscalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recentarchitecture with learnable univariate functions, offer strong nonlinearexpressiveness and efficient inference. In this work, we integrate KANs intothree popular GNN architectures-GAT, SGC, and APPNP-resulting in three newmodels: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledgeamalgamation framework, where knowledge from multiple KAN-based GNNs isdistilled into a graph-independent KAN student model. Experiments on benchmarkdatasets show that the proposed models improve node classification accuracy,and the knowledge amalgamation approach significantly boosts student modelperformance. Our findings highlight the potential of KANs for enhancing GNNexpressiveness and for enabling efficient, graph-free inference.</description>
      <author>example@mail.com (Yuan-Hung Chao, Chia-Hsun Lu, Chih-Ya Shen)</author>
      <guid isPermaLink="false">2508.06663v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Hypergraph Neural Network with State Space Models for Node Classification</title>
      <link>http://arxiv.org/abs/2508.06587v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的超图神经网络与状态空间模型相结合的模型（HGMN），有效解决了传统图神经网络忽略基于角色的特征的问题，通过超图构建技术和可学习机制整合了基于角色和邻接的表示，在节点分类任务上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;图神经网络近年来在图结构数据节点分类任务中备受关注，但传统GNN主要关注节点间邻接关系，忽略了基于角色的关键特征；现有捕获基于角色特征的方法多为无监督，在下游任务中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效整合基于角色的表示到GNN和状态空间模型中的方法，解决传统GNN的局限性，提升节点分类性能。&lt;h4&gt;方法&lt;/h4&gt;提出HGMN模型，利用超图构建技术建模高阶关系，通过可学习的mamba transformer机制结合基于角色和邻接的表示，采用基于节点度和邻域级别的超图构建方法加强相似角色节点间连接，包含超图卷积层捕获复杂依赖，并集成残差网络缓解过平滑问题。&lt;h4&gt;主要发现&lt;/h4&gt;在一个新数据集和四个基准数据集上的实验证明HGMN优于最先进GNN方法，在节点分类任务上实现了显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;HGMN能有效嵌入基于角色的特征和邻接信息，提供丰富的节点表示，成为各种基于图的学习应用的通用且强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;近年来，图神经网络在图结构数据的节点分类任务中获得了显著关注。然而，传统GNN主要关注节点之间的邻接关系，往往忽略了对于学习更丰富的节点表示至关重要的基于角色的丰富特征。现有的捕获基于角色的特征的方法大多是无监督的，并且在下游任务中无法实现最佳性能。为解决这些限制，我们提出了一种新颖的超图神经网络与状态空间模型相结合的模型，有效地将基于角色的表示整合到GNN和状态空间模型中。该模型利用超图构建技术来建模高阶关系，并通过可学习的mamba transformer机制结合基于角色和基于邻接的表示。通过利用两种不同的基于节点度和邻域级别的超图构建方法，它加强了具有相似角色的节点之间的连接，增强了模型的表示能力。此外，超图卷积层的包含使模型能够捕获超图结构内的复杂依赖关系。为了缓解深度GNN中固有的过平滑问题，我们集成了残差网络，确保了更好的层间稳定性和特征传播。在一个新引入的数据集和四个基准数据集上进行的广泛实验证明了该模型的优越性。与最先进的GNN方法相比，该模型在节点分类任务上取得了显著的性能提升。这些结果突显了该模型通过有效嵌入基于角色的特征和邻接信息来提供丰富节点表示的能力，使其成为各种基于图的学习应用的通用且强大的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, graph neural networks (GNNs) have gained significantattention for node classification tasks on graph-structured data. However,traditional GNNs primarily focus on adjacency relationships between nodes,often overlooking the rich role-based characteristics that are crucial forlearning more expressive node representations. Existing methods for capturingrole-based features are largely unsupervised and fail to achieve optimalperformance in downstream tasks. To address these limitations, we propose anovel hypergraph neural network with state space model (HGMN) that effectivelyintegrates role-aware representations into GNNs and the state space model. HGMNutilizes hypergraph construction techniques to model higher-order relationshipsand combines role-based and adjacency-based representations through a learnablemamba transformer mechanism. By leveraging two distinct hypergraph constructionmethods-based on node degree and neighborhood levels, it strengthens theconnections among nodes with similar roles, enhancing the model'srepresentational power. Additionally, the inclusion of hypergraph convolutionlayers enables the model to capture complex dependencies within hypergraphstructures. To mitigate the over-smoothing problem inherent in deep GNNs, weincorporate a residual network, ensuring improved stability and better featurepropagation across layers. Extensive experiments conducted on one newlyintroduced dataset and four benchmark datasets demonstrate the superiority ofHGMN. The model achieves significant performance improvements on nodeclassification tasks compared to state-of-the-art GNN methods. These resultshighlight HGMN's ability to provide enriched node representations byeffectively embedding role-based features alongside adjacency information,making it a versatile and powerful tool for a variety of graph-based learningapplications.</description>
      <author>example@mail.com (A. Quadir, M. Tanveer)</author>
      <guid isPermaLink="false">2508.06587v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion</title>
      <link>http://arxiv.org/abs/2508.08216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'Individual Tangent Space Alignment (ITSA)'的新型预对齐策略，用于增强跨主体泛化能力，以支持基于音乐的个性化运动康复干预。&lt;h4&gt;背景&lt;/h4&gt;个性化音乐干预可以通过动态调整听觉刺激来支持运动康复，提供外部节拍线索，调节情感状态，稳定步态模式。通用脑机接口(BCIs)有望使这些干预措施能够适应不同个体。然而，脑电图(EEG)信号的主体间变异性，加上运动引起的伪影和运动计划差异，阻碍了BCIs的泛化能力，导致校准过程冗长。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的预对齐策略(ITSA)，以提高跨主体泛化能力，解决BCIs在个体间差异导致的泛化困难和校准时间长的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了'Individual Tangent Space Alignment (ITSA)'，一种新颖的预对齐策略，包含主体特定的重新中心化、分布匹配和监督旋转对齐。使用混合架构，融合了正则化公共空间模式(RCSP)和黎曼几何，采用并行和顺序配置。使用留一主体交叉验证评估性能。&lt;h4&gt;主要发现&lt;/h4&gt;ITSA在主体和条件下显示出显著的性能改进。并行融合方法相比顺序融合方法显示出最大的改进。系统在不同数据条件和电极配置下保持了稳健的性能。&lt;h4&gt;结论&lt;/h4&gt;ITSA策略有效地增强了跨主体泛化能力，特别是在并行融合配置下，能够适应不同的数据条件和电极配置，为基于音乐的个性化运动康复干预提供了更好的支持。&lt;h4&gt;翻译&lt;/h4&gt;个性化音乐干预提供了一种强大的支持运动康复的手段，通过动态调整听觉刺激来提供外部节拍线索，调节情感状态，并稳定步态模式。因此，通用脑机接口(BCIs)有望使这些干预措施能够适应不同个体。然而，脑电图信号的主体间变异性，再加上运动引起的伪影和运动计划差异，阻碍了BCIs的泛化能力，并导致冗长的校准过程。我们提出了'Individual Tangent Space Alignment (ITSA)'，一种新颖的预对齐策略，包含主体特定的重新中心化、分布匹配和监督旋转对齐，以增强跨主体泛化。我们的混合架构并行和顺序地融合了正则化公共空间模式(RCSP)和黎曼几何，提高了类可分性，同时保持了协方差矩阵的几何结构，以实现稳健的统计计算。使用留一主体交叉验证，'ITSA'在主体和条件下显示出显著的性能改进。并行融合方法相比其顺序对应方法显示出最大的改进，并且在变化的数据条件和电极配置下保持了稳健的性能。代码将在发表时公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalised music-based interventions offer a powerful means of supportingmotor rehabilitation by dynamically tailoring auditory stimuli to provideexternal timekeeping cues, modulate affective states, and stabilise gaitpatterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise foradapting these interventions across individuals. However, inter-subjectvariability in EEG signals, further compounded by movement-induced artefactsand motor planning differences, hinders the generalisability of BCIs andresults in lengthy calibration processes. We propose Individual Tangent SpaceAlignment (ITSA), a novel pre-alignment strategy incorporating subject-specificrecentering, distribution matching, and supervised rotational alignment toenhance cross-subject generalisation. Our hybrid architecture fuses RegularisedCommon Spatial Patterns (RCSP) with Riemannian geometry in parallel andsequential configurations, improving class separability while maintaining thegeometric structure of covariance matrices for robust statistical computation.Using leave-one-subject-out cross-validation, `ITSA' demonstrates significantperformance improvements across subjects and conditions. The parallel fusionapproach shows the greatest enhancement over its sequential counterpart, withrobust performance maintained across varying data conditions and electrodeconfigurations. The code will be made publicly available at the time ofpublication.</description>
      <author>example@mail.com (Nicole Lai-Tan, Xiao Gu, Marios G. Philiastides, Fani Deligianni)</author>
      <guid isPermaLink="false">2508.08216v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks</title>
      <link>http://arxiv.org/abs/2508.08125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published In Proceedings of the 2024 Joint International Conference  on Computational Linguistics, Language Resources and Evaluation (LREC-COLING  2024). Official version: https://aclanthology.org/2024.lrec-main.374/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个新的捷克语数据集，用于基于方面的情感分析(ABSA)，包含3100条手动标注的餐厅评论，支持更复杂的任务，并遵循SemEval-2016数据集格式。&lt;h4&gt;背景&lt;/h4&gt;之前有一个捷克语数据集，但只包含基本ABSA任务的独立标签，如方面术语提取或方面极性检测，缺乏支持更复杂任务的统一标注格式。&lt;h4&gt;目的&lt;/h4&gt;创建一个新的捷克语数据集，支持更复杂的ABSA任务，如目标-方面类别检测，并采用统一的标注格式将情感元素链接在一起。&lt;h4&gt;方法&lt;/h4&gt;数据集构建于之前的捷克语数据集，包含3100条手动标注的餐厅评论，两名训练有素的标注员完成标注，标注者间一致性约为90%，还提供2400万条未标注评论适用于无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;使用各种基于Transformer的模型实现了强大的单语基线结果，并提供了有价值的错误分析。&lt;h4&gt;结论&lt;/h4&gt;代码和数据集可供非商业研究目的免费使用，促进了跨语言比较。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们介绍了一个用于基于方面的情感分析(ABSA)的新型捷克语数据集，该数据集包含3100条来自餐厅领域的手动标注评论。该数据集建立在较早的捷克语数据集之上，该数据集仅包含基本ABSA任务（如方面术语提取或方面极性检测）的独立标签。与前身不同，我们的新数据集专门设计用于更复杂的任务，例如目标-方面类别检测。这些高级任务需要统一的标注格式，将情感元素（标签）无缝连接在一起。我们的数据集遵循著名的SemEval-2016数据集的格式。这一设计选择使得在跨语言场景中能够轻松应用和评估，最终促进与其他语言中对应数据集的跨语言比较。标注过程涉及两名训练有素的标注员，获得了约90%的惊人标注者间一致性。此外，我们还提供了2400万条未标注的评论，适用于无监督学习。我们展示了使用各种基于Transformer的模型实现的强大单语基线结果，并提供有价值的错误分析作为补充贡献。我们的代码和数据集可供非商业研究目的免费使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce a novel Czech dataset for aspect-based sentimentanalysis (ABSA), which consists of 3.1K manually annotated reviews from therestaurant domain. The dataset is built upon the older Czech dataset, whichcontained only separate labels for the basic ABSA tasks such as aspect termextraction or aspect polarity detection. Unlike its predecessor, our newdataset is specifically designed for more complex tasks, e.g.target-aspect-category detection. These advanced tasks require a unifiedannotation format, seamlessly linking sentiment elements (labels) together. Ourdataset follows the format of the well-known SemEval-2016 datasets. This designchoice allows effortless application and evaluation in cross-lingual scenarios,ultimately fostering cross-language comparisons with equivalent counterpartdatasets in other languages. The annotation process engaged two trainedannotators, yielding an impressive inter-annotator agreement rate ofapproximately 90%. Additionally, we provide 24M reviews without annotationssuitable for unsupervised learning. We present robust monolingual baselineresults achieved with various Transformer-based models and insightful erroranalysis to supplement our contributions. Our code and dataset are freelyavailable for non-commercial research purposes.</description>
      <author>example@mail.com (Jakub Šmíd, Pavel Přibáň, Ondřej Pražák, Pavel Král)</author>
      <guid isPermaLink="false">2508.08125v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations</title>
      <link>http://arxiv.org/abs/2508.08061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于迁移学习的预测性过程监控技术，使缺乏足够事件数据的组织也能实施PPM进行有效决策支持。该技术在两个实际用例中进行了验证，实验表明业务流程知识可以在组织内部和组织间转移，实现目标环境中的有效预测监控。&lt;h4&gt;背景&lt;/h4&gt;事件日志反映了组织信息系统中映射的业务流程行为。预测性过程监控(PPM)通过创建过程相关预测将数据转化为价值，为过程运行时的主动干预提供洞察。然而，现有PPM技术需要大量事件数据或其他相关资源，这些资源可能不易获得，限制了部分组织应用PPM的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于迁移学习的PPM技术，使没有合适事件数据或其他相关资源的组织也能实施PPM，从而实现有效的决策支持。&lt;h4&gt;方法&lt;/h4&gt;该技术在两个实际用例中得到实例化，并基于这些用例，使用IT服务管理流程的事件日志进行了数值实验，包括组织内部和组织间的设置。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，一个业务流程的知识可以转移到同一组织或不同组织中的相似业务流程，以实现目标环境中的有效PPM。通过所提出的技术，组织可以在组织内部和组织间环境中从迁移学习中受益，其中预训练模型等资源可以在组织内部和组织边界之间转移。&lt;h4&gt;结论&lt;/h4&gt;基于迁移学习的PPM技术使缺乏足够数据资源的组织也能实现有效的过程监控和决策支持，知识可以在相似业务流程和组织间转移应用，扩大了PPM的适用范围。&lt;h4&gt;翻译&lt;/h4&gt;事件日志反映了在组织信息系统中映射的业务流程行为。预测性过程监控(PPM)通过创建与过程相关的预测，将数据转化为价值，为过程运行时的主动干预提供洞察。现有的PPM技术需要足够的事件数据或其他相关资源，这些资源可能不容易获得，阻止一些组织利用PPM。本文提出的基于迁移学习的PPM技术使没有合适事件数据或其他相关资源的组织能够实施PPM以实现有效的决策支持。该技术在两个实际用例中得到实例化，基于这些用例，使用IT服务管理流程的事件日志在组织内部和组织间设置中进行了数值实验。实验结果表明，一个业务流程的知识可以转移到同一组织或不同组织中的相似业务流程，以实现目标环境中的有效PPM。通过所提出的技术，组织可以在组织内部和组织间环境中从迁移学习中受益，其中预训练模型等资源可以在组织内部和组织边界之间转移。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event logs reflect the behavior of business processes that are mapped inorganizational information systems. Predictive process monitoring (PPM)transforms these data into value by creating process-related predictions thatprovide the insights required for proactive interventions at process runtime.Existing PPM techniques require sufficient amounts of event data or otherrelevant resources that might not be readily available, preventing someorganizations from utilizing PPM. The transfer learning-based PPM techniquepresented in this paper allows organizations without suitable event data orother relevant resources to implement PPM for effective decision support. Thetechnique is instantiated in two real-life use cases, based on which numericalexperiments are performed using event logs for IT service management processesin an intra- and inter-organizational setting. The results of the experimentssuggest that knowledge of one business process can be transferred to a similarbusiness process in the same or a different organization to enable effectivePPM in the target context. With the proposed technique, organizations canbenefit from transfer learning in an intra- and inter-organizational setting,where resources like pre-trained models are transferred within and acrossorganizational boundaries.</description>
      <author>example@mail.com (Sven Weinzierl, Sandra Zilker, Annina Liessmann, Martin Käppel, Weixin Wang, Martin Matzner)</author>
      <guid isPermaLink="false">2508.08061v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Informed Multimodal Bearing Fault Classification under Variable Operating Conditions using Transfer Learning</title>
      <link>http://arxiv.org/abs/2508.07536v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种物理信息多模态卷积神经网络，用于轴承故障分类，整合了振动和电机电流信号，并采用后期融合架构。模型包含基于物理特性的损失函数，以惩罚不合理的预测。实验表明该方法优于传统基线，并评估了三种迁移学习策略以提高在未见工况下的性能。&lt;h4&gt;背景&lt;/h4&gt;准确且可解释的轴承故障分类对确保旋转机械的可靠性至关重要，特别是在可变工况下，域偏移会显著降低模型性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种物理信息多模态神经网络，提高轴承故障分类的准确性、鲁棒性和可解释性，特别是在可变工况下的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种物理信息多模态卷积神经网络，采用后期融合架构，整合振动和电机电流信号，并配有专门的基于物理的特征提取分支。模型包含基于轴承几何形状和轴速推导的特征故障频率（外圈通过频率和内圈通过频率）的物理信息损失函数。评估了三种迁移学习策略（目标特定微调、逐层适应策略和混合特征重用）以解决在未见工况下的性能下降问题。&lt;h4&gt;主要发现&lt;/h4&gt;物理信息方法持续优于非物理信息基线，实现更高准确率、减少错误分类并提高鲁棒性；逐层适应策略提供最佳泛化能力；物理信息建模与逐层适应策略结合可进一步提升性能；在KAIST数据集上验证了框架的跨数据集适用性，准确率高达98%；统计假设检验验证了分类性能的显著改善。&lt;h4&gt;结论&lt;/h4&gt;将领域知识与数据驱动学习相结合的框架能够实现鲁棒、可解释和可泛化的故障诊断，适用于实际工业应用，在轴承故障分类任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;准确且可解释的轴承故障分类对于确保旋转机械的可靠性至关重要，特别是在可变工况下，域偏移会显著降低模型性能。本研究提出了一种物理信息多模态卷积神经网络，采用后期融合架构，整合了振动和电机电流信号，并配有专门的基于物理的特征提取分支。该模型包含一种新颖的物理信息损失函数，根据轴承几何形状和轴速推导出的特征轴承故障频率对物理上不可能的预测进行惩罚。在帕德博恩大学数据集上的综合实验表明，所提出的物理信息方法持续优于非物理信息基线，在多个数据分割中实现了更高的准确性、减少了错误分类并提高了鲁棒性。为解决在未见工况下的性能下降问题，评估了三种迁移学习策略。结果显示，逐层适应策略提供了最佳泛化能力，与物理信息建模结合时还有额外的性能提升。在KAIST轴承数据集上的验证确认了该框架的跨数据集适用性，准确率高达98%。统计假设检验进一步验证了分类性能的显著改善。所提出的框架展示了将领域知识与数据驱动学习相结合以实现鲁棒、可解释和可泛化的故障诊断的潜力，适用于实际工业应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and interpretable bearing fault classification is critical forensuring the reliability of rotating machinery, particularly under variableoperating conditions where domain shifts can significantly degrade modelperformance. This study proposes a physics-informed multimodal convolutionalneural network (CNN) with a late fusion architecture, integrating vibration andmotor current signals alongside a dedicated physics-based feature extractionbranch. The model incorporates a novel physics-informed loss function thatpenalizes physically implausible predictions based on characteristic bearingfault frequencies - Ball Pass Frequency Outer (BPFO) and Ball Pass FrequencyInner (BPFI) - derived from bearing geometry and shaft speed. Comprehensiveexperiments on the Paderborn University dataset demonstrate that the proposedphysics-informed approach consistently outperforms a non-physics-informedbaseline, achieving higher accuracy, reduced false classifications, andimproved robustness across multiple data splits. To address performancedegradation under unseen operating conditions, three transfer learning (TL)strategies - Target-Specific Fine-Tuning (TSFT), Layer-Wise Adaptation Strategy(LAS), and Hybrid Feature Reuse (HFR) - are evaluated. Results show that LASyields the best generalization, with additional performance gains when combinedwith physics-informed modeling. Validation on the KAIST bearing datasetconfirms the framework's cross-dataset applicability, achieving up to 98percent accuracy. Statistical hypothesis testing further verifies significantimprovements (p &lt; 0.01) in classification performance. The proposed frameworkdemonstrates the potential of integrating domain knowledge with data-drivenlearning to achieve robust, interpretable, and generalizable fault diagnosisfor real-world industrial applications.</description>
      <author>example@mail.com (Tasfiq E. Alam, Md Manjurul Ahsan, Shivakumar Raman)</author>
      <guid isPermaLink="false">2508.07536v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation</title>
      <link>http://arxiv.org/abs/2508.07295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一个名为CCFQA的新基准，用于评估多模态大型语言模型在跨语言和跨模态事实准确性方面的能力，特别是在语音处理方面。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在多语言世界中日益普及，确保无幻觉的事实准确性变得至关重要。然而，现有的多模态大型语言模型评估基准主要关注文本或视觉模态，且以英语为主，在处理多语言输入特别是语音时存在评估差距。&lt;h4&gt;目的&lt;/h4&gt;填补多语言语音输入评估的空白，提出一个能够系统评估多模态大型语言模型跨语言和跨模态事实准确性能力的新型基准。&lt;h4&gt;方法&lt;/h4&gt;开发了CCFQA基准，包含8种语言的并行语音文本事实问题；提出少样本迁移学习策略，将LLMs在英语问答能力迁移到多语言口语问答任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明当前MLLMs在CCFQA基准上仍面临重大挑战；使用仅5次训练的少样本迁移学习策略，可以与GPT-4o-mini-Audio实现具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;发布CCFQA作为基础研究资源，促进具有更强大可靠语音理解能力的MLLMs发展，相关代码和数据集已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;随着大型语言模型在多语言世界中日益普及，确保无幻觉的事实准确性变得至关重要。然而，现有的用于评估多模态大型语言模型可靠性的基准主要关注文本或视觉模态，并以英语为主，这在处理多语言输入时（特别是语音）造成了评估差距。为了弥补这一差距，我们提出了一个新颖的跨语言和跨模态事实准确性基准CCFQA。具体来说，CCFQA基准包含8种语言的并行语音文本事实问题，旨在系统评估MLLMs的跨语言和跨模态事实准确性能力。我们的实验结果表明，当前的MLLMs在CCFQA基准上仍面临重大挑战。此外，我们提出了一种少样本迁移学习策略，有效地将LLMs在英语中的问答能力迁移到多语言口语问答任务，仅使用5次训练就实现了与GPT-4o-mini-Audio相竞争的性能。我们发布CCFQA作为基础研究资源，以促进具有更强大可靠语音理解能力的MLLMs的发展。我们的代码和数据集可在https://github.com/yxduir/ccfqa获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As Large Language Models (LLMs) are increasingly popularized in themultilingual world, ensuring hallucination-free factuality becomes markedlycrucial. However, existing benchmarks for evaluating the reliability ofMultimodal Large Language Models (MLLMs) predominantly focus on textual orvisual modalities with a primary emphasis on English, which creates a gap inevaluation when processing multilingual input, especially in speech. To bridgethis gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal\textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQAbenchmark contains parallel speech-text factual questions across 8 languages,designed to systematically evaluate MLLMs' cross-lingual and cross-modalfactuality capabilities. Our experimental results demonstrate that currentMLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, wepropose a few-shot transfer learning strategy that effectively transfers theQuestion Answering (QA) capabilities of LLMs in English to multilingual SpokenQuestion Answering (SQA) tasks, achieving competitive performance withGPT-4o-mini-Audio using just 5-shot training. We release CCFQA as afoundational research resource to promote the development of MLLMs with morerobust and reliable speech understanding capabilities. Our code and dataset areavailable at https://github.com/yxduir/ccfqa.</description>
      <author>example@mail.com (Yexing Du, Kaiyuan Liu, Youcheng Pan, Zheng Chu, Bo Yang, Xiaocheng Feng, Yang Xiang, Ming Liu)</author>
      <guid isPermaLink="false">2508.07295v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Reconstruction of Solar EUV Irradiance Using CaII K Images and SOHO/SEM Data with Bayesian Deep Learning and Uncertainty Quantification</title>
      <link>http://arxiv.org/abs/2508.07065v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为SEMNet的贝叶斯深度学习模型，用于重建太阳极紫外(EUV)辐照度的长期数据，填补了1995年之前EUV连续测量数据的空白。通过使用CaII K图像作为代理数据，研究人员成功重建了1950-1960年和1998-2014年期间的EUV通量数据，为理解太阳对地球气候的长期影响提供了重要依据。&lt;h4&gt;背景&lt;/h4&gt;太阳极紫外(EUV)辐照度对地球电离层、热层和中层的加热起着关键作用，影响不同时间尺度的大气动力学。虽然已有大量研究关注太阳瞬态事件导致的短期EUV变化，但对多个太阳周期内EUV通量长期演变的研究较少。此外，连续的EUV通量测量数据自1995年起才有，早期数据存在显著空白。&lt;h4&gt;目的&lt;/h4&gt;填补太阳EUV辐照度长期数据的空白，特别是1995年之前的数据，以便更好地理解太阳对地球气候在较长时间尺度上的影响。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为SEMNet的贝叶斯深度学习模型。首先使用精密太阳光度望远镜的CaII K图像构建1998年至2014年期间的SOHO/SEM EUV通量测量来验证该方法。然后通过迁移学习扩展SEMNet，使用科代卡纳尔太阳天文台的CaII K图像重建1950年至1960年期间的太阳EUV辐照度。&lt;h4&gt;主要发现&lt;/h4&gt;SEMNet模型能够提供可靠的EUV通量预测和不确定性边界，证明了CaII K图像可以作为长期EUV通量的稳健代理数据。&lt;h4&gt;结论&lt;/h4&gt;通过使用SEMNet模型和CaII K图像作为代理数据，成功重建了长期太阳EUV辐照度数据，为研究太阳对地球气候的长期影响提供了重要数据支持。&lt;h4&gt;翻译&lt;/h4&gt;太阳极紫外(EUV)辐照度在加热地球电离层、热层和中层方面起着关键作用，影响不同时间尺度的大气动力学。虽然已经投入大量精力研究太阳瞬态事件导致的短期EUV变化，但很少有工作探索多个太阳周期内EUV通量的长期演变。连续的EUV通量测量数据自1995年起才有，早期数据存在显著空白。在本研究中，我们提出了一种名为SEMNet的贝叶斯深度学习模型来填补这些空白。我们通过将SEMNet应用于使用精密太阳光度望远镜的CaII K图像构建1998年至2014年期间的SOHO/SEM EUV通量测量来验证我们的方法。然后我们通过迁移学习扩展SEMNet，使用科代卡纳尔太阳天文台的CaII K图像重建1950年至1960年期间的太阳EUV辐照度。实验结果表明，SEMNet提供了可靠的预测和不确定性边界，证明了CaII K图像作为长期EUV通量的稳健代理的可行性。这些发现有助于更好地理解太阳对地球气候在较长时间尺度上的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solar extreme ultraviolet (EUV) irradiance plays a crucial role in heatingthe Earth's ionosphere, thermosphere, and mesosphere, affecting atmosphericdynamics over varying time scales. Although significant effort has been spentstudying short-term EUV variations from solar transient events, there is littlework to explore the long-term evolution of the EUV flux over multiple solarcycles. Continuous EUV flux measurements have only been available since 1995,leaving significant gaps in earlier data. In this study, we propose a Bayesiandeep learning model, named SEMNet, to fill the gaps. We validate our approachby applying SEMNet to construct SOHO/SEM EUV flux measurements in the periodbetween 1998 and 2014 using CaII K images from the Precision Solar PhotometricTelescope. We then extend SEMNet through transfer learning to reconstruct solarEUV irradiance in the period between 1950 and 1960 using CaII K images from theKodaikanal Solar Observatory. Experimental results show that SEMNet providesreliable predictions along with uncertainty bounds, demonstrating thefeasibility of CaII K images as a robust proxy for long-term EUV fluxes. Thesefindings contribute to a better understanding of solar influences on Earth'sclimate over extended periods.</description>
      <author>example@mail.com (Haodi Jiang, Qin Li, Jason T. L. Wang, Haimin Wang, Serena Criscuoli)</author>
      <guid isPermaLink="false">2508.07065v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2508.06784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了模式感知非线性Tucker自编码器(MA-NTAE)，用于解决高阶张量在自监督学习中的挑战，有效克服了传统方法和现有张量网络的局限性。&lt;h4&gt;背景&lt;/h4&gt;高维数据，特别是高阶张量，在自监督学习中构成重大挑战。基于MLP的自编码器依赖扁平化操作，加剧维度灾难，导致模型规模过大、计算开销高且难以优化深层特征捕获。现有张量网络虽通过张量分解减轻计算负担，但在学习非线性关系方面能力有限。&lt;h4&gt;目的&lt;/h4&gt;克服现有自编码器和张量网络方法的局限性，开发一种能够有效处理高阶张量的自监督学习方法。&lt;h4&gt;方法&lt;/h4&gt;引入模式感知非线性Tucker自编码器(MA-NTAE)，将经典Tucker分解推广到非线性框架，采用'选择-展开'策略，通过递归的展开-编码-折叠操作实现高阶张量的灵活按模式编码，有效整合张量结构先验。&lt;h4&gt;主要发现&lt;/h4&gt;MA-NTAE的计算复杂度随张量阶数呈线性增长，随模式维度呈比例增长。实验表明，MA-NTAE在压缩和聚类任务上优于标准自编码器和当前张量网络，且对更高阶、更高维的张量，这种优势更加明显。&lt;h4&gt;结论&lt;/h4&gt;MA-NTAE是处理高阶张量数据的有效方法，特别是在自监督学习任务中，能够平衡计算效率与模型性能。&lt;h4&gt;翻译&lt;/h4&gt;高维数据，特别是高阶张量形式，在自监督学习中构成了重大挑战。虽然基于MLP的自编码器(AE)被广泛使用，但它们对扁平化操作的依赖加剧了维度灾难，导致模型规模过大、计算开销高，且难以优化深层特征捕获。虽然现有的张量网络通过张量分解技术减轻了计算负担，但大多数在学习非线性关系方面能力有限。为了克服这些局限性，我们引入了模式感知非线性Tucker自编码器(MA-NTAE)。MA-NTAE将经典Tucker分解推广到非线性框架，并采用'选择-展开'策略，通过递归的展开-编码-折叠操作实现高阶张量的灵活按模式编码，有效整合了张量结构先验。值得注意的是，MA-NTAE的计算复杂度随张量阶数呈线性增长，随模式维度呈比例增长。大量实验表明，MA-NTAE在压缩和聚类任务上优于标准自编码器和当前张量网络，对于更高阶、更高维的张量，这种优势变得更加明显。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-dimensional data, particularly in the form of high-order tensors,presents a major challenge in self-supervised learning. While MLP-basedautoencoders (AE) are commonly employed, their dependence on flatteningoperations exacerbates the curse of dimensionality, leading to excessivelylarge model sizes, high computational overhead, and challenging optimizationfor deep structural feature capture. Although existing tensor networksalleviate computational burdens through tensor decomposition techniques, mostexhibit limited capability in learning non-linear relationships. To overcomethese limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder(MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linearframework and employs a Pick-and-Unfold strategy, facilitating flexibleper-mode encoding of high-order tensors via recursive unfold-encode-foldoperations, effectively integrating tensor structural priors. Notably, MA-NTAEexhibits linear growth in computational complexity with tensor order andproportional growth with mode dimensions. Extensive experiments demonstrateMA-NTAE's performance advantages over standard AE and current tensor networksin compression and clustering tasks, which become increasingly pronounced forhigher-order, higher-dimensional tensors.</description>
      <author>example@mail.com (Junjing Zheng, Chengliang Song, Weidong Jiang, Xinyu Zhang)</author>
      <guid isPermaLink="false">2508.06784v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised learning for inverse problems in computed tomography</title>
      <link>http://arxiv.org/abs/2508.05321v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种无监督深度学习方法用于CT图像重建，通过在深度学习框架中整合前向和后向投影层，实现了在不依赖真实图像的情况下从投影数据重建图像。该方法在2DeteCT数据集上表现出优于传统FBP和ML重建技术的性能，同时显著减少了重建时间。&lt;h4&gt;背景&lt;/h4&gt;研究利用深度神经网络训练与传统迭代重建方法之间的内在相似性，开发了一种创新的CT图像重建方法。&lt;h4&gt;目的&lt;/h4&gt;实现在不依赖真实图像的情况下从投影数据重建图像，并开发一种适用于实时医学成像的替代方法。&lt;h4&gt;方法&lt;/h4&gt;在深度学习框架中整合前向和后向投影层，并在二维2DeteCT数据集上评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的滤波反投影(FBP)和最大似然(ML)重建技术相比，该方法在均方误差(MSE)和结构相似性指数(SSIM)方面表现更优，同时显著减少了重建时间。&lt;h4&gt;结论&lt;/h4&gt;该方法是一种有前景的实时医学成像替代方案，未来工作将重点扩展到三维重建并提高投影几何的适应性。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种用于计算机断层扫描(CT)图像重建的无监督深度学习方法，利用深度神经网络训练与传统迭代重建方法之间的内在相似性。通过在深度学习框架中整合前向和后向投影层，我们证明了在不依赖真实图像的情况下从投影数据重建图像的可行性。我们的方法在二维2DeteCT数据集上进行了评估，与传统的滤波反投影(FBP)和最大似然(ML)重建技术相比，在均方误差(MSE)和结构相似性指数(SSIM)方面表现出优越的性能。此外，我们的方法显著减少了重建时间，使其成为实时医学成像应用的一种有前途的替代方案。未来的工作将重点扩展该方法到三维重建并提高投影几何的适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents an unsupervised deep learning approach for computedtomography (CT) image reconstruction, leveraging the inherent similaritiesbetween deep neural network training and conventional iterative reconstructionmethods. By incorporating forward and backward projection layers within thedeep learning framework, we demonstrate the feasibility of reconstructingimages from projection data without relying on ground-truth images. Our methodis evaluated on the two-dimensional 2DeteCT dataset, showcasing superiorperformance in terms of mean squared error (MSE) and structural similarityindex (SSIM) compared to traditional filtered backprojection (FBP) and maximumlikelihood (ML) reconstruction techniques. Additionally, our approachsignificantly reduces reconstruction time, making it a promising alternativefor real-time medical imaging applications. Future work will focus on extendingthis methodology to three-dimensional reconstructions and enhancing theadaptability of the projection geometry.</description>
      <author>example@mail.com (Laura Hellwege, Johann Christopher Engster, Moritz Schaar, Thorsten M. Buzug, Maik Stille)</author>
      <guid isPermaLink="false">2508.05321v2</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Investigating the Impact of Large-Scale Pre-training on Nutritional Content Estimation from 2D Images</title>
      <link>http://arxiv.org/abs/2508.03996v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了大规模预训练数据集对仅使用2D图像进行营养成分估算的深度学习模型性能的影响，发现数据集的特性（规模、领域相关性和质量）对迁移学习效果至关重要。&lt;h4&gt;背景&lt;/h4&gt;从食物图像中估算营养成分对健康和饮食监测具有重要意义，但仅依靠2D图像具有挑战性，因为食物呈现方式、光照条件的变化以及缺乏深度信息导致难以推断体积和质量。此外，最先进的方法依赖于专有数据集进行大规模预训练，这影响了该领域的可重复性。&lt;h4&gt;目的&lt;/h4&gt;研究大规模预训练数据集对仅使用2D图像进行营养成分估算的深度学习模型性能的影响。&lt;h4&gt;方法&lt;/h4&gt;作者对在ImageNet和COYO两个大型公共数据集上预训练的Vision Transformer模型进行微调和评估，并将其性能与在私有JFT-300M数据集上预训练的最先进方法以及基线CNN模型（InceptionV2和ResNet-50）进行比较。实验在Nutrition5k数据集上进行，这是一个包含高精度营养注释的真实世界食物盘的大规模集合。&lt;h4&gt;主要发现&lt;/h4&gt;在JFT-300M上预训练的模型显著优于在公共数据集上预训练的模型；在大型COYO数据集上预训练的模型对于这个特定的回归任务表现比在ImageNet上预训练的模型更差，这与最初的假设相反；数据集的特性（规模、领域相关性和质量）对有效迁移学习至关重要。&lt;h4&gt;结论&lt;/h4&gt;分析提供了定量证据，突显了预训练数据集特性（包括规模、领域相关性和质量）在2D营养成分估算的有效迁移学习中的关键作用。&lt;h4&gt;翻译&lt;/h4&gt;从食物图像中估算营养成分是一项具有重大健康和饮食监测意义的任务。仅依靠2D图像时，这具有挑战性，因为食物呈现方式的多样性、光照条件的变化以及缺乏深度信息导致难以推断体积和质量。此外，该领域的可重复性受到最先进方法对大规模预训练专有数据集依赖的阻碍。在本文中，我们研究了大规模预训练数据集对仅使用2D图像进行营养成分估算的深度学习模型性能的影响。我们对在两个大型公共数据集ImageNet和COYO上预训练的Vision Transformer模型进行微调和评估，将其性能与在私有JFT-300M数据集上预训练的最先进方法以及基线CNN模型（InceptionV2和ResNet-50）进行比较。我们在Nutrition5k数据集上进行了广泛实验，这是一个包含高精度营养注释的真实世界食物盘的大规模集合。我们使用平均绝对误差和平均绝对百分比误差进行的评估显示，在JFT-300M上预训练的模型显著优于在公共数据集上预训练的模型。出乎意料的是，对于这个特定的回归任务，在大型COYO数据集上预训练的模型表现比在ImageNet上预训练的模型更差，这与我们最初的假设相矛盾。我们的分析提供了定量证据，突显了预训练数据集特性（包括规模、领域相关性和质量）在2D营养成分估算的有效迁移学习中的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the nutritional content of food from images is a critical taskwith significant implications for health and dietary monitoring. This ischallenging, especially when relying solely on 2D images, due to thevariability in food presentation, lighting, and the inherent difficulty ininferring volume and mass without depth information. Furthermore,reproducibility in this domain is hampered by the reliance of state-of-the-artmethods on proprietary datasets for large-scale pre-training. In this paper, weinvestigate the impact of large-scale pre-training datasets on the performanceof deep learning models for nutritional estimation using only 2D images. Wefine-tune and evaluate Vision Transformer (ViT) models pre-trained on two largepublic datasets, ImageNet and COYO, comparing their performance againstbaseline CNN models (InceptionV2 and ResNet-50) and a state-of-the-art methodpre-trained on the proprietary JFT-300M dataset. We conduct extensiveexperiments on the Nutrition5k dataset, a large-scale collection of real-worldfood plates with high-precision nutritional annotations. Our evaluation usingMean Absolute Error (MAE) and Mean Absolute Percentage Error (MAE%) revealsthat models pre-trained on JFT-300M significantly outperform those pre-trainedon public datasets. Unexpectedly, the model pre-trained on the massive COYOdataset performs worse than the model pre-trained on ImageNet for this specificregression task, refuting our initial hypothesis. Our analysis providesquantitative evidence highlighting the critical role of pre-training datasetcharacteristics, including scale, domain relevance, and curation quality, foreffective transfer learning in 2D nutritional estimation.</description>
      <author>example@mail.com (Michele Andrade, Guilherme A. L. Silva, Valéria Santos, Gladston Moreira, Eduardo Luz)</author>
      <guid isPermaLink="false">2508.03996v2</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.07838v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CBDES MoE的分层解耦专家混合架构，用于解决自动驾驶中的鸟瞰图感知系统问题。该系统通过整合多个结构异构的专家网络和轻量级自注意力路由器门控机制，实现了动态专家路径选择和稀疏、输入感知的高效推理。实验表明，该方法在3D目标检测中优于固定单专家基线模型。&lt;h4&gt;背景&lt;/h4&gt;基于多传感器特征融合的鸟瞰图(BEV)感知系统已成为端到端自动驾驶的基础。然而，现有的多模态BEV方法通常存在输入适应性有限、建模能力受限和泛化能力欠佳等问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态BEV方法中存在的输入适应性有限、建模能力受限和泛化能力欠佳等挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为CBDES MoE的分层解耦专家混合架构。该架构在功能模块级别整合了多个结构异构的专家网络，并采用轻量级自注意力路由器(SAR)门控机制，实现动态专家路径选择和稀疏、输入感知的高效推理。这是首个在自动驾驶领域功能模块粒度构建的模块化专家混合框架。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界的nuScenes数据集上进行的大量评估表明，CBDES MoE在3D目标检测中始终优于固定的单专家基线模型。与最强的单专家模型相比，CBDES MoE在mAP上提高了1.6个百分点，在NDS上提升了4.1个百分点。&lt;h4&gt;结论&lt;/h4&gt;CBDES MoE方法的有效性和实际优势得到了验证，表明所提出的架构能够解决现有多模态BEV方法的局限性，提高自动驾驶系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;基于多传感器特征融合的鸟瞰图(BEV)感知系统已成为端到端自动驾驶的基本基石。然而，现有的多模态BEV方法普遍存在输入适应性有限、建模能力受限和泛化能力欠佳等问题。为解决这些挑战，我们在功能模块级别提出了一种分层解耦的专家混合架构，称为计算脑发育系统专家混合(CBDES MoE)。CBDES MoE整合了多个结构异构的专家网络和轻量级自注意力路由器(SAR)门控机制，实现了动态专家路径选择和稀疏、输入感知的高效推理。据我们所知，这是自动驾驶领域首个在功能模块粒度构建的模块化专家混合框架。在真实世界nuScenes数据集上的大量评估表明，CBDES MoE在3D目标检测中始终优于固定的单专家基线模型。与最强的单专家模型相比，CBDES MoE在mAP上实现了1.6个百分点的增长，在NDS上提升了4.1个百分点，证明了所提出方法的有效性和实际优势。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有多模态BEV感知方法存在的输入适应性有限、建模能力受限和泛化能力不足的问题。在现实中，自动驾驶系统需要在各种复杂动态环境下（如不同光照、天气条件、摄像头视角）可靠工作，而单一固定骨干网络难以捕捉多样化的场景信息，导致性能下降，这对安全关键的自动驾驶系统尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，然后借鉴了Mixture-of-Experts(MoE)范式在自然语言处理中的成功应用，将其引入到自动驾驶BEV感知领域。作者设计了异构专家集合（Swin Transformer、ResNet、ConvNeXt和PVT）和轻量级自注意力路由器(SAR)，并引入负载平衡正则化。该方法借鉴了BEVFusion等多模态融合框架的基本架构，以及动态卷积和可变形注意力的设计思想，但将其扩展到了更粗粒度的架构层面。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多个结构异构的专家网络和动态路由机制，根据输入条件自动选择最适合的专家进行处理，从而增强模型对不同环境的适应性。整体流程：1)接收多摄像头图像和LiDAR输入；2)四个异构专家网络分别提取图像特征；3)自注意力路由器分析输入并生成专家路由概率；4)根据路由概率选择和融合专家特征；5)将特征投影到BEV空间；6)与其他模态特征融合；7)传递给下游任务处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个在自动驾驶领域构建在功能模块粒度的模块化MoE框架；2)整合四种结构异构专家网络的设计；3)轻量级自注意力路由机制；4)负载平衡正则化防止专家坍塌。相比之前工作，CBDES MoE不采用固定单骨干网络，而是实现动态专家选择；不仅限于细粒度模块调整，而是实现粗粒度架构多样性；专门针对自动驾驶多模态BEV感知任务设计，解决了异构专家组合和跨模态一致性挑战；专注于感知系统而非仅规划决策层的MoE应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CBDES MoE通过引入异构专家网络和自注意力路由机制，实现了自动驾驶BEV感知系统中动态、高效的任务特定专家选择，显著提升了3D目标检测性能和环境适应性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bird's Eye View (BEV) perception systems based on multi-sensor feature fusionhave become a fundamental cornerstone for end-to-end autonomous driving.However, existing multi-modal BEV methods commonly suffer from limited inputadaptability, constrained modeling capacity, and suboptimal generalization. Toaddress these challenges, we propose a hierarchically decoupledMixture-of-Experts architecture at the functional module level, termedComputing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoEintegrates multiple structurally heterogeneous expert networks with alightweight Self-Attention Router (SAR) gating mechanism, enabling dynamicexpert path selection and sparse, input-aware efficient inference. To the bestof our knowledge, this is the first modular Mixture-of-Experts frameworkconstructed at the functional module granularity within the autonomous drivingdomain. Extensive evaluations on the real-world nuScenes dataset demonstratethat CBDES MoE consistently outperforms fixed single-expert baselines in 3Dobject detection. Compared to the strongest single-expert model, CBDES MoEachieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS,demonstrating the effectiveness and practical advantages of the proposedapproach.</description>
      <author>example@mail.com (Qi Xiang, Kunsong Shi, Zhigui Lin, Lei He)</author>
      <guid isPermaLink="false">2508.07838v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring</title>
      <link>http://arxiv.org/abs/2508.07552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于特征图收敛分数的独立评估方法，通过构建双粒度动态加权评分系统和CLIP-based特征图质量评估网络，实现了对自动驾驶端到端模型中功能模块生成的特征图质量的全面评估，并在NuScenes数据集上验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;端到端模型正在成为自动驾驶感知和规划的主流方法，但缺乏对中间功能模块的明确监督信号，导致操作机制不透明，可解释性有限，使传统方法难以独立评估和训练这些模块。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于特征图收敛分数的独立评估方法，构建双粒度动态加权评分系统，形成统一的定量指标-特征图质量分数，并开发CLIP-based特征图质量评估网络，以实现对功能模块生成的特征图质量的实时质量分析。&lt;h4&gt;方法&lt;/h4&gt;基于特征图-真实表示相似性的评估框架，构建双粒度动态加权评分系统，形成特征图质量分数这一统一定量指标，开发结合特征-真实编码器和质量分数预测头的CLIP-based特征图质量评估网络。&lt;h4&gt;主要发现&lt;/h4&gt;在NuScenes数据集上的实验表明，将评估模块整合到训练中可以提高3D目标检测性能，NDS指标提升了3.89%，验证了该方法在提高特征表示质量和整体模型性能方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效评估功能模块生成的特征图质量，这种评估方法可以提升整体模型性能并增强特征表示质量。&lt;h4&gt;翻译&lt;/h4&gt;端到端模型正在成为自动驾驶感知和规划的主流。然而，缺乏对中间功能模块的明确监督信号导致操作机制不透明和可解释性有限，使传统方法难以独立评估和训练这些模块。在该问题上，本研究基于特征图-真实表示相似性评估框架，提出了基于特征图收敛分数的独立评估方法。构建了双粒度动态加权评分系统，形成了统一的定量指标-特征图质量分数，以实现对功能模块生成的特征图质量的全面评估。进一步开发了基于CLIP的特征图质量评估网络，结合特征-真实编码器和质量分数预测头，实现对功能模块生成的特征图质量的实时分析。在NuScenes数据集上的实验结果表明，将我们的评估模块整合到训练中提高了3D目标检测性能，NDS指标提升了3.89%。这些结果验证了我们的方法在提高特征表示质量和整体模型性能方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶端到端模型中中间功能模块缺乏独立评估和优化的问题。由于端到端模型的'黑盒'特性，内部特征缺乏显式监督信号，使得传统方法难以独立评估和训练这些模块。这个问题在现实中至关重要，因为自动驾驶系统的安全性和可靠性直接依赖于其感知和规划模块的性能。无法有效评估这些模块就难以提高系统整体性能和安全性，也限制了模型调试效率和可解释性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，特别是端到端模型中中间特征缺乏显式监督的问题。他们借鉴了多模块学习(MML)的思想，将感知算法划分为多个专门功能模块，并参考了特征图-真实表示相似性评估框架[19]，但认识到这种方法尚未集成到模型训练中。基于这些观察，作者设计了基于特征图质量评分(FMQS)的独立评估方法，构建了双粒度动态加权评分系统，并开发了基于CLIP的特征图质量评估网络，实现了对特征图质量的实时分析。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过特征图质量评分(FMQS)实现对自动驾驶模型中功能模块的独立评估和优化，认为特征图质量与模型最终性能之间存在密切关联。整体流程：1)构建双粒度动态加权评分系统，从宏观(模型性能)和微观(特征图相似性)两个维度评估特征图质量；2)开发基于CLIP的特征图质量评估网络，包括特征图编码器、真实文本编码器和FMQS预测头；3)将评估网络集成到模型训练过程中，将预测的FMQS作为辅助损失项，与原始任务损失结合共同优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出特征图质量评分(FMQS)和双粒度动态加权评分系统；2)设计基于CLIP的特征图质量评估网络；3)将评估模块有效集成到训练过程中。相比之前工作：1)与FMCE-Net[18]相比，能评估多个级联功能模块；2)与[19]相比，将评估指标集成到训练中可直接贡献于优化；3)结合特征图语义一致性和任务性能反馈，建立多维度评估模型；4)通过双粒度评估机制同时考虑全局模型水平和局部特征图水平。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于特征图质量评分的解耦功能评估方法，通过双粒度动态加权评分系统和CLIP-based评估网络，实现了对自动驾驶模型中间功能模块的独立评估和优化，显著提升了模型性能和训练效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end models are emerging as the mainstream in autonomous drivingperception and planning. However, the lack of explicit supervision signals forintermediate functional modules leads to opaque operational mechanisms andlimited interpretability, making it challenging for traditional methods toindependently evaluate and train these modules. Pioneering in the issue, thisstudy builds upon the feature map-truth representation similarity-basedevaluation framework and proposes an independent evaluation method based onFeature Map Convergence Score (FMCS). A Dual-Granularity Dynamic WeightedScoring System (DG-DWSS) is constructed, formulating a unified quantitativemetric - Feature Map Quality Score - to enable comprehensive evaluation of thequality of feature maps generated by functional modules. A CLIP-based FeatureMap Quality Evaluation Network (CLIP-FMQE-Net) is further developed, combiningfeature-truth encoders and quality score prediction heads to enable real-timequality analysis of feature maps generated by functional modules. Experimentalresults on the NuScenes dataset demonstrate that integrating our evaluationmodule into the training improves 3D object detection performance, achieving a3.89 percent gain in NDS. These results verify the effectiveness of our methodin enhancing feature representation quality and overall model performance.</description>
      <author>example@mail.com (Ludan Zhang, Sihan Wang, Yuqi Dai, Shuofei Qiao, Lei He)</author>
      <guid isPermaLink="false">2508.07552v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Toward Patient-specific Partial Point Cloud to Surface Completion for Pre- to Intra-operative Registration in Image-guided Liver Interventions</title>
      <link>http://arxiv.org/abs/2505.19518v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种患者特定的点云补全方法，利用VN-OccNet从不完整的术中点云生成完整肝脏表面，以解决图像引导手术中术中数据缺乏表面下信息导致的配准挑战。&lt;h4&gt;背景&lt;/h4&gt;在图像引导手术中捕获的术中数据缺乏表面下的信息，而关键感兴趣区域（如血管和肿瘤）位于表面下。图像到物理配准能够融合术前信息和术中数据，但由于术中点云的部分可见性，配准过程面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种患者特定的点云补全方法来辅助配准过程，解决术中点云部分可见性导致的配准困难问题。&lt;h4&gt;方法&lt;/h4&gt;利用VN-OccNet从不完整的术中点云生成完整的肝脏表面。网络以患者特定的方式进行训练，使用来自术前模型的模拟形变来训练模型。首先分析VN-OccNet的旋转等变性特性，然后将补全的术中表面集成到Go-ICP配准算法中。&lt;h4&gt;主要发现&lt;/h4&gt;VN-OccNet具有有效的旋转等变性和从不完整的术中表面恢复完整表面的能力。将补全的术中表面集成到配准算法中能够改善初始刚性配准结果。&lt;h4&gt;结论&lt;/h4&gt;患者特定的点云补全方法在缓解术中部分可见性带来的挑战方面具有前景。VN-OccNet的旋转等变性和表面生成能力在开发针对术中点云变化的稳健配准框架方面具有强大前景。&lt;h4&gt;翻译&lt;/h4&gt;在图像引导手术期间捕获的术中数据缺乏表面下信息，而关键感兴趣区域（如血管和肿瘤）位于表面下。图像到物理配准能够融合术前信息和术中数据，通常表示为点云。然而，由于术中点云的部分可见性，这种配准过程面临挑战。在本研究中，我们提出了一种患者特定的点云补全方法来辅助配准过程。具体而言，我们利用VN-OccNet从不完整的术中点云生成完整的肝脏表面。该网络以患者特定的方式进行训练，其中使用来自术前模型的模拟形变来训练模型。首先，我们对VN-OccNet的旋转等变性特性及其从不完整的术中表面恢复完整表面的有效性进行了深入分析。接下来，我们将补全的术中表面集成到Go-ICP配准算法中，以展示其在改善初始刚性配准结果方面的效用。我们的结果突显了这种患者特定的补全方法在缓解术中部分可见性带来的挑战方面的前景。VN-OccNet的旋转等变性和表面生成能力在开发针对术中点云变化的稳健配准框架方面具有强大前景。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决图像引导肝脏手术中术中数据部分可见性的问题。由于相机视角限制和遮挡，术中获取的点云数据不完整，导致术前CT/MRI图像与术中数据的配准精度不足。这个问题很重要，因为肝脏中的血管和肿瘤等关键区域位于器官表面下方，准确的配准对帮助外科医生精确定位这些区域、提高手术安全性和成功率至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法（如Jia和Foti等人的工作）的局限性，指出它们需要刚性初始化或手动识别对应点。作者选择使用VN-OccNet作为基础，因为它具有旋转等变性和生成水密网格的能力，适合处理不同方向的术中数据。作者借鉴了点云补全、表面重建和配准领域的现有技术，但创新性地采用了患者特定训练策略，通过模拟患者肝脏变形来生成训练数据，使网络能够学习患者特定的几何特征和变形模式。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过患者特定的点云补全来提高术前到术中配准的准确性。整体流程包括：1) 使用患者特定的术前肝脏模型模拟变形生成训练数据；2) 训练VN-OccNet网络，输入部分术中点云，输出占用概率；3) 使用多分辨率等值面提取和Marching Cubes算法从预测的占用点生成完整表面网格；4) 从生成的网格中提取顶点作为完整目标点云；5) 使用Go-ICP算法将完整目标点云与术前源点云进行配准。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 患者特定训练策略，使网络专注于特定患者的几何和变形特征；2) 利用VN-OccNet的旋转等变性，处理不同方向的术中数据；3) 生成水密网格而非点云，提供均匀表面采样；4) 直接将补全表面用于配准，显著提高精度。相比之前工作，此方法不需要刚性配准初始化（如Jia的方法），也不需要手动识别对应点（如Foti的方法），且专门针对肝脏手术中的配准挑战进行了优化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种患者特定的点云补全方法，利用VN-OccNet的旋转等变性和网格生成能力，显著提高了图像引导肝脏手术中术前到术中配准的准确性，有效解决了术中部分可见性带来的挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intra-operative data captured during image-guided surgery lacks sub-surfaceinformation, where key regions of interest, such as vessels and tumors, reside.Image-to-physical registration enables the fusion of pre-operative informationand intra-operative data, typically represented as a point cloud. However, thisregistration process struggles due to partial visibility of the intra-operativepoint cloud. In this research, we propose a patient-specific point cloudcompletion approach to assist with the registration process. Specifically, weleverage VN-OccNet to generate a complete liver surface from a partialintra-operative point cloud. The network is trained in a patient-specificmanner, where simulated deformations from the pre-operative model are used totrain the model. First, we conduct an in-depth analysis of VN-OccNet'srotation-equivariant property and its effectiveness in recovering completesurfaces from partial intra-operative surfaces. Next, we integrate thecompleted intra-operative surface into the Go-ICP registration algorithm todemonstrate its utility in improving initial rigid registration outcomes. Ourresults highlight the promise of this patient-specific completion approach inmitigating the challenges posed by partial intra-operative visibility. Therotation equivariant and surface generation capabilities of VN-OccNet holdstrong promise for developing robust registration frameworks for variations ofthe intra-operative point cloud.</description>
      <author>example@mail.com (Nakul Poudel, Zixin Yang, Kelly Merrell, Richard Simon, Cristian A. Linte)</author>
      <guid isPermaLink="false">2505.19518v2</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Learning an Implicit Physics Model for Image-based Fluid Simulation</title>
      <link>http://arxiv.org/abs/2508.08254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种从单张静态图像生成具有物理一致性的4D场景（包含运动和3D几何）的新方法，特别针对自然流体图像。通过物理信息神经网络和基于物理原理的损失函数，该方法能够生成更符合物理规律的动画效果。&lt;h4&gt;背景&lt;/h4&gt;人类具有从单张静态图像想象4D场景（包括运动和3D几何）的非凡能力，这基于我们对类似场景的积累观察和对物理的直观理解。然而，现有方法通常使用简化的2D运动估计器来使图像动起来，导致运动预测常常违背物理原理，产生不真实的动画效果。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在复制人类从单张静态图像想象4D场景的能力，特别是在自然流体图像方面，解决现有方法生成的动画不符合物理原理的问题。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种新颖的方法，使用物理信息神经网络为每个表面点预测运动，并通过基于基本物理原理（包括纳维-斯托克斯方程）导出的损失项进行指导。为了捕捉外观，作者从输入图像及其估计的深度预测基于特征的3D高斯分布，然后使用预测的运动对这些高斯分布进行动画处理，并从任何期望的摄像机视角进行渲染。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果突显了该方法在产生物理合理动画方面的有效性，显示出与现有方法相比显著的性能改进。&lt;h4&gt;结论&lt;/h4&gt;该研究成功开发了一种从单张静态图像生成物理一致4D场景的新方法，特别是在自然流体图像方面，通过物理信息神经网络和基于物理原理的损失函数，解决了现有方法生成的动画不符合物理原理的问题，实验结果表明该方法具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;人类拥有从单张静态图像想象包含运动和3D几何的4D场景的非凡能力。这种能力源于我们对类似场景的积累观察和对物理的直观理解。在本文中，我们旨在在神经网络中复制这种能力，特别关注自然流体图像。现有方法通常使用简化的2D运动估计器来使图像动起来，导致运动预测常常违背物理原理，产生不真实的动画。我们的方法引入了一种从单张图像生成具有物理一致动画的4D场景的新方法。我们提出使用物理信息神经网络为每个表面点预测运动，由基本物理原理（包括纳维-斯托克斯方程）导出的损失项进行指导。为了捕捉外观，我们从输入图像及其估计的深度预测基于特征的3D高斯分布，然后使用预测的运动对这些高斯分布进行动画处理，并从任何期望的摄像机视角进行渲染。实验结果突显了我们的方法在产生物理合理动画方面的有效性，显示出与现有方法相比显著的性能改进。我们的项目页面是https://physfluid.github.io/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从单张静态流体图像生成符合物理规律的动画视频的问题。这个问题很重要，因为人类能从静止图像想象出动态场景，但现有计算机方法生成的动画往往不符合物理原理，导致结果不真实。解决这个问题可以应用于电影特效、游戏、虚拟现实等领域，生成更真实的流体效果，同时为计算机视觉和图形学提供新研究方向。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了人类从静止图像想象动态场景的能力，指出现有方法使用简单2D运动估计器导致动画不物理真实。他们设计结合数据驱动和物理原理的方法，使用物理信息神经网络预测流体运动。作者借鉴了3D高斯表示法来捕捉场景外观，参考了物理信息神经网络概念和Navier-Stokes方程等流体力学原理，并使用单目深度图帮助将2D图像转换为3D表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合数据驱动和物理原理，通过物理信息神经网络预测流体的3D速度场，然后使用3D高斯表示法生成符合物理规律的动画视频。整体流程：1)输入单张流体图像和相机轨迹；2)将图像转换为3D高斯表示（通过分层深度图像和特征提取）；3)使用物理信息神经网络预测3D速度场（结合图像特征和物理损失函数）；4)动画模块（用预测速度移动3D高斯中心并渲染视频帧，处理可能出现的空洞问题）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出物理信息神经网络动力学，结合数据驱动和物理原理；2)使用3D高斯表示法捕捉和动画化流体外观；3)直接从单张图像估计3D速度场而非仅2D运动；4)设计物理损失函数确保动画符合流体力学原理。不同之处：不同于纯数据驱动方法，我们结合物理约束；不同于传统流体模拟，我们可从单张图像推断；优于其他基于学习的方法，我们考虑物理规律使动画更真实；能处理图像编辑任务并生成符合物理规律的结果。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合物理信息神经网络和3D高斯表示的新方法，能够从单张静态流体图像生成符合物理规律的动画视频，并在质量和真实性上超越了现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans possess an exceptional ability to imagine 4D scenes, encompassing bothmotion and 3D geometry, from a single still image. This ability is rooted inour accumulated observations of similar scenes and an intuitive understandingof physics. In this paper, we aim to replicate this capacity in neuralnetworks, specifically focusing on natural fluid imagery. Existing methods forthis task typically employ simplistic 2D motion estimators to animate theimage, leading to motion predictions that often defy physical principles,resulting in unrealistic animations. Our approach introduces a novel method forgenerating 4D scenes with physics-consistent animation from a single image. Wepropose the use of a physics-informed neural network that predicts motion foreach surface point, guided by a loss term derived from fundamental physicalprinciples, including the Navier-Stokes equations. To capture appearance, wepredict feature-based 3D Gaussians from the input image and its estimateddepth, which are then animated using the predicted motions and rendered fromany desired camera perspective. Experimental results highlight theeffectiveness of our method in producing physically plausible animations,showcasing significant performance improvements over existing methods. Ourproject page is https://physfluid.github.io/ .</description>
      <author>example@mail.com (Emily Yue-Ting Jia, Jiageng Mao, Zhiyuan Gao, Yajie Zhao, Yue Wang)</author>
      <guid isPermaLink="false">2508.08254v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Model Probabilistic Framework for Seismic Risk Assessment and Retrofit Planning of Electric Power Networks</title>
      <link>http://arxiv.org/abs/2508.07376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种用于电力系统地震风险评估和加固规划的多模型概率框架，通过整合地震危险性表征、组件级损伤分析、系统级级联影响评估和启发式优化方法，有效提高了电力网络的地震弹性。&lt;h4&gt;背景&lt;/h4&gt;电力网络是关键的生命线基础设施，地震期间的中断可能导致严重的级联故障，并严重阻碍灾后恢复。&lt;h4&gt;目的&lt;/h4&gt;提高电力网络的地震弹性，以经济有效且系统感知的方式识别和加强脆弱组件。&lt;h4&gt;方法&lt;/h4&gt;提出了一种多模型概率框架，包括：(1)区域地震危险性表征；(2)组件级损伤分析；(3)系统级级联影响评估；(4)启发式优化加固规划。使用蒙特卡洛模拟传播不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够捕获级联故障、识别关键组件并生成有效的加固策略，证明了其作为可扩展数据驱动决策支持工具的潜力。&lt;h4&gt;结论&lt;/h4&gt;该框架有望成为一种可扩展的、数据驱动的决策支持工具，用于提高电力基础设施的地震弹性。&lt;h4&gt;翻译&lt;/h4&gt;电力网络是关键的生命线，地震期间的中断可能导致严重的级联故障并显著阻碍灾后恢复。提高其地震弹性需要以经济有效且系统感知的方式识别和加强脆弱组件。然而，现有研究往往忽视电力网络在地震荷载下的系统性行为。常见局限包括忽视网络范围相互依赖关系的孤立组件分析、假设二元状态或损伤独立性的过度简化损伤模型，以及排除电气运行约束。这些简化可能导致不准确的风险估计和低效的加固决策。本研究提出了一种电力系统地震风险评估和加固规划的多模型概率框架。该方法整合：(1)区域地震危险性表征，结合地面运动预测和空间相关模型；(2)使用易损性函数和多状态损伤-功能映射的组件级损伤分析；(3)基于图的孤岛检测和约束最优潮流分析的系统级级联影响评估；(4)通过启发式优化进行加固规划，在预算约束下最小化预期年度功能损失。使用蒙特卡洛模拟在整个框架中传播不确定性。该方法在IEEE 24节点可靠性测试系统上得到展示，展示了其捕获级联故障、识别关键组件和生成有效加固策略的能力。结果强调了该框架作为提高电力基础设施地震弹性的可扩展数据驱动决策支持工具的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electric power networks are critical lifelines, and their disruption duringearthquakes can lead to severe cascading failures and significantly hinderpost-disaster recovery. Enhancing their seismic resilience requires identifyingand strengthening vulnerable components in a cost-effective and system-awaremanner. However, existing studies often overlook the systemic behavior of powernetworks under seismic loading. Common limitations include isolated componentanalyses that neglect network-wide interdependencies, oversimplified damagemodels assuming binary states or damage independence, and the exclusion ofelectrical operational constraints. These simplifications can result ininaccurate risk estimates and inefficient retrofit decisions. This studyproposes a multi-model probabilistic framework for seismic risk assessment andretrofit planning of electric power systems. The approach integrates: (1)regional seismic hazard characterization with ground motion prediction andspatial correlation models; (2) component-level damage analysis using fragilityfunctions and multi-state damage-functionality mappings; (3) system-levelcascading impact evaluation through graph-based island detection andconstrained optimal power flow analysis; and (4) retrofit planning viaheuristic optimization to minimize expected annual functionality loss (EAFL)under budget constraints. Uncertainty is propagated throughout the frameworkusing Monte Carlo simulation. The methodology is demonstrated on the IEEE24-bus Reliability Test System, showcasing its ability to capture cascadingfailures, identify critical components, and generate effective retrofitstrategies. Results underscore the potential of the framework as a scalable,data-informed decision-support tool for enhancing the seismic resilience ofpower infrastructure.</description>
      <author>example@mail.com (Huangbin Liang, Beatriz Moya, Francisco Chinesta, Eleni Chatzi)</author>
      <guid isPermaLink="false">2508.07376v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Conical Intersections Shed Light on Hot Carrier Cooling in Quantum Dots</title>
      <link>http://arxiv.org/abs/2508.07322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过实验观测胶体半导体纳米晶体中电子激发态的振动相干性，揭示了热载流子超快动力学机制，并证明了锥形交叉级联框架在不同配体钝化量子点中的普遍适用性。&lt;h4&gt;背景&lt;/h4&gt;先前研究表明，胺钝化量子点中的振动相干性是在通过电子激发态之间锥形交叉级联弛豫过程中产生的，这种观测为研究热载流子超快动力学提供了窗口。&lt;h4&gt;目的&lt;/h4&gt;将锥形交叉级联框架应用于表面结合羧酸酯配体的量子点，证明该框架的普遍性，并研究配体对弛豫动力学的影响机制。&lt;h4&gt;方法&lt;/h4&gt;使用宽带多维光谱学观测振动相干性的频率，建立涉及锥形交叉级联的模型来解释观测结果，并比较不同配体（胺钝化、羧酸酯钝化、乙酸盐和甲酸盐配体）对弛豫动力学的影响。&lt;h4&gt;主要发现&lt;/h4&gt;锥形交叉级联模型能准确重现观测到的振动相干性频率；配体影响弛豫动力学的机制涉及核心与配体之间的电子或振动耦合；与胺钝化量子点相比，羧酸酯钝化量子点中电子耦合机制较弱；截短配体烷基链会改变模型的预测行为。&lt;h4&gt;结论&lt;/h4&gt;锥形交叉级联框架具有普遍适用性，可应用于不同配体钝化的量子点系统；配体类型（特别是烷基链长度）会影响热载流子弛豫动力学。&lt;h4&gt;翻译&lt;/h4&gt;实验观测胶体半导体纳米晶体中电子激发态的振动相干性，为研究热载流子超快动力学提供了一个窗口。在先前的工作中，我们已经证明，在胺钝化的量子点中，这些相干性是在通过电子激发态之间的锥形交叉级联弛豫过程中产生的。在此，我们将该框架应用于带有表面结合羧酸酯配体的量子点，证明了其普遍性。一个涉及类似锥形交叉级联的模型准确重现了通过宽带多维光谱学观测到的振动相干性频率。配体对弛豫动力学的影响归因于两个不同的机制，涉及核心与配体之间的电子或振动耦合。与先前研究的胺钝化量子点相比，在羧酸酯钝化的量子点中，电子耦合机制不那么显著。此外，乙酸盐和甲酸盐配体的比较表明，截短配体烷基链会改变模型的预测行为。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Experimental observations of vibronic coherences in electronically excitedcolloidal semiconductor nanocrystals offer a window into the ultrafast dynamicsof hot carrier cooling. In previous work, we showed that, in amine-passivatedquantum dots (QDs), these coherences arise during relaxation through a cascadeof conical intersections between electronically excited states. Here, wedemonstrate the generality of this framework by application to QDs withsurface-bound carboxylate ligands. A model involving a similar cascade ofconical intersections accurately reproduces the frequencies of vibroniccoherences observed with broadband multidimensional spectroscopy. The impact ofligands on the relaxation dynamics is attributed to two distinct mechanismsinvolving either electronic or vibrational coupling between the core andligands. Compared to the amine-passivated QDs studied previously, theelectronic coupling mechanism is less prominent in carboxylate-passivated QDs.Furthermore, comparison of acetate and formate ligands reveals that truncatingthe ligand alkyl chains alters the relaxation behavior predicted by the model.</description>
      <author>example@mail.com (Caitlin V. Hetherington, Nila Mohan T. M., Shanu A. Shameem, Warren F. Beck, Benjamin G. Levine)</author>
      <guid isPermaLink="false">2508.07322v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion</title>
      <link>http://arxiv.org/abs/2508.07162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoopDiff的新型接触一致解耦扩散框架，用于3D人-物体交互预测。该方法通过两个不同分支分别建模人体和物体运动，使用接触点作为共享锚点桥接分支间的运动生成，并引入人驱动的交互模块增强一致性。&lt;h4&gt;背景&lt;/h4&gt;3D人-物体交互预测旨在根据历史上下文预测人类及其操作物体的未来运动。具有关节结构的人和刚性物体因不同物理特性表现出不同运动模式，但现有研究大多忽略这种区别，尝试在单一模型中同时捕捉两者的动力学。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够分别建模人体和物体不同运动模式的框架，提高3D人-物体交互预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出CoopDiff框架，包含两个分支解耦人体和物体运动建模，使用接触点作为共享锚点桥接分支，人体分支预测结构化运动，物体分支处理刚体运动，并通过一致性约束连接分支，同时引入人驱动的交互模块指导物体运动建模。&lt;h4&gt;主要发现&lt;/h4&gt;在BEHAVE和Human-object Interaction数据集上的实验表明，CoopDiff方法优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;通过解耦人体和物体运动建模并使用接触点作为共享锚点，CoopDiff框架能更准确预测3D人-物体交互，同时保持运动一致性。&lt;h4&gt;翻译&lt;/h4&gt;3D人-物体交互预测旨在根据历史上下文预测人类及其操作物体的未来运动。通常，具有关节结构的人和刚性物体由于其不同的内在物理特性而表现出不同的运动模式。然而，大多数现有工作忽略了这种区别，试图在单一的预测模型中捕捉人和物体的动力学。在这项工作中，我们提出了一种新颖的接触一致解耦扩散框架CoopDiff，它采用两个不同的分支来解耦人体和物体运动建模，以人-物体接触点作为共享锚点来桥接不同分支间的运动生成。人体动力学分支旨在预测高度结构化的人体运动，而物体动力学分支专注于具有刚体平移和旋转的物体运动。这两个分支通过一系列具有一致性约束的共享接触点连接，以实现连贯的人-物体运动预测。为进一步增强人-物体一致性和预测可靠性，我们提出了一个人驱动的交互模块来指导物体运动建模。在BEHAVE和Human-object Interaction数据集上的大量实验表明，我们的CoopDiff优于最先进的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D人-物交互（HOI）预测问题，即根据历史信息预测未来人类和他们所操作物体的运动。这个问题在现实中非常重要，因为它有广泛的应用价值，包括机器人技术、动画制作、增强现实和具身AI等领域。同时，现有方法大多忽略了人类和物体在物理特性上的本质区别，导致预测结果不够精确和真实。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到人类和物体在交互中表现出完全不同的动态模式：人类是高度关节化的，表现出多样化的运动；而物体通常是刚性的，主要表现为平移和旋转。现有方法使用单一预测模型同时处理两者，导致预测不准确。作者借鉴了扩散模型（如InterDiff）的成功应用，但认识到其局限性，进而提出将人类和物体的动态建模分离，同时保持它们之间的一致性。此外，还借鉴了Transformer架构、SMPL模型和ControlNet等技术来构建整个框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1) 使用双分支扩散模型分别建模人类和物体的动态；2) 以接触点作为共享锚点连接两个分支，确保运动一致性；3) 引入人类驱动的交互模块，将人类动态作为条件控制注入物体动态建模。整体流程包括：数据表示（人体姿势、物体姿势和接触点）、人类动态分支预测、物体动态分支预测、接触一致性约束、人类驱动的交互模块，以及三阶段模型训练策略。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 联系一致的解耦扩散框架，首次分离建模人类和物体动态；2) 人类驱动的交互模块，强调人类在交互中的主导作用；3) 将接触点作为共享锚点连接两个分支。相比之前工作，不同之处在于：大多数现有方法使用单一模型处理两者动态，而CoopDiff使用双分支分离建模；之前方法将接触点仅用于后处理，而CoopDiff将其作为连接分支的桥梁；现有方法平等对待人类和物体动态，而CoopDiff强调人类的主导角色。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种新颖的联系一致的解耦扩散框架CoopDiff，通过分离建模人类和物体的不同动态模式并利用接触点保持一致性，实现了更准确、更真实的3D人-物交互预测。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D human-object interaction (HOI) anticipation aims to predict the futuremotion of humans and their manipulated objects, conditioned on the historicalcontext. Generally, the articulated humans and rigid objects exhibit differentmotion patterns, due to their distinct intrinsic physical properties. However,this distinction is ignored by most of the existing works, which intend tocapture the dynamics of both humans and objects within a single predictionmodel. In this work, we propose a novel contact-consistent decoupled diffusionframework CoopDiff, which employs two distinct branches to decouple human andobject motion modeling, with the human-object contact points as shared anchorsto bridge the motion generation across branches. The human dynamics branch isaimed to predict highly structured human motion, while the object dynamicsbranch focuses on the object motion with rigid translations and rotations.These two branches are bridged by a series of shared contact points withconsistency constraint for coherent human-object motion prediction. To furtherenhance human-object consistency and prediction reliability, we propose ahuman-driven interaction module to guide object motion modeling. Extensiveexperiments on the BEHAVE and Human-object Interaction datasets demonstratethat our CoopDiff outperforms state-of-the-art methods.</description>
      <author>example@mail.com (Xiaotong Lin, Tianming Liang, Jian-Fang Hu, Kun-Yu Lin, Yulei Kang, Chunwei Tian, Jianhuang Lai, Wei-Shi Zheng)</author>
      <guid isPermaLink="false">2508.07162v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting</title>
      <link>http://arxiv.org/abs/2508.07089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ForeSight的新型联合检测和预测框架，用于自动驾驶车辆的基于视觉的3D感知。该框架通过多任务流式和双向学习方法，使检测和预测能够共享查询内存并无缝传播信息，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;传统方法将检测和预测视为独立的顺序任务，限制了它们利用时间线索的能力。基于跟踪的方法需要显式的对象关联，容易导致错误传播，且难以有效扩展到多帧序列。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时进行检测和预测的框架，克服传统方法的局限性，提高自动驾驶车辆3D感知的性能，减少错误传播，并有效处理多帧序列。&lt;h4&gt;方法&lt;/h4&gt;ForeSight采用多任务流式和双向学习架构，包括预测感知的检测变换器和流式预测变换器。预测感知的检测变换器通过集成来自多假设预测内存队列的轨迹预测来增强空间推理；流式预测变换器使用过去的预测和精细化的检测来提高时间一致性。该框架消除了显式对象关联的需要，采用无跟踪模型设计。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验表明，ForeSight实现了最先进的性能，EPA达到54.9%，比之前的方法高出9.3%，同时在多视图检测和预测模型中获得了最佳的mAP和minADE。&lt;h4&gt;结论&lt;/h4&gt;ForeSight通过将检测和预测统一在一个框架内，并采用创新的内存共享和信息传播机制，显著提高了自动驾驶车辆的3D感知能力。该方法的成功证明了联合处理检测和预测任务的有效性，为未来自动驾驶感知系统的发展提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了ForeSight，这是一种用于自动驾驶车辆中基于视觉的3D感知的新型联合检测和预测框架。传统方法将检测和预测视为独立的顺序任务，限制了它们利用时间线索的能力。ForeSight通过多任务流式和双向学习方法解决了这一限制，使检测和预测能够共享查询内存并无缝传播信息。预测感知的检测变换器通过集成来自多假设预测内存队列的轨迹预测来增强空间推理，而流式预测变换器使用过去的预测和精细化的检测来提高时间一致性。与基于跟踪的方法不同，ForeSight消除了显式对象关联的需要，采用一种无跟踪模型，有效扩展到多帧序列。在nuScenes数据集上的实验表明，ForeSight实现了最先进的性能，EPA达到54.9%，比之前的方法高出9.3%，同时在多视图检测和预测模型中获得了最佳的mAP和minADE。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统中物体检测和轨迹预测的整合问题。传统方法将这两个任务视为独立顺序任务，限制了利用时间线索的能力，并且依赖跟踪技术会引入错误传播问题。这个问题在现实中非常重要，因为自动驾驶车辆需要准确理解动态驾驶环境，特别是在处理遮挡或部分可见物体时，这些情况可能带来关键安全风险。准确的物体检测和轨迹预测对自动驾驶系统的安全性和可靠性至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者重新思考了传统自动驾驶系统架构，提出不应将运动预测仅用于规划后丢弃，而应将其反馈到检测和预测任务中。他们借鉴了现有工作中的多个方面：基于稀疏查询的物体检测方法（如DETR3D、PETR）、时域融合方法（如BEVDet4D、StreamPETR）以及运动预测方法。但ForeSight的创新在于将这些方法整合为一个统一框架，并引入了双向查询传播机制，实现检测和预测之间的闭环反馈。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双向查询传播和联合内存系统，实现检测和预测之间的闭环反馈，消除对显式对象关联的依赖，减少错误传播，并使用流式处理提高计算效率。整体流程包括：1)从多视角图像提取场景特征，可选地从高清地图编码道路上下文；2)通过联合流式内存队列管理过去和未来的查询；3)初始化检测查询并使用预测感知检测Transformer融合信息；4)基于检测结果初始化预测查询，并通过联合流式预测Transformer生成未来轨迹；5)使用多任务目标函数优化模型，并将预测查询推入内存供未来使用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双向查询传播机制，实现预测信息反馈回检测任务；2)无跟踪的流式预测方法，消除跟踪瓶颈；3)联合内存系统，使过去检测和预测作为当前任务的先验；4)预测感知检测，将轨迹预测集成到空间推理中。相比之前工作，ForeSight将检测和预测统一为联合框架而非独立任务，消除了显式对象关联的需要，通过流式处理提高计算效率，并首次实现双向查询传播形成闭环反馈。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ForeSight通过引入双向查询传播和联合内存系统，实现了自动驾驶系统中物体检测和轨迹预测的高效整合，显著提高了在复杂场景中的感知准确性和预测可靠性，同时消除了传统跟踪方法的瓶颈和错误传播问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce ForeSight, a novel joint detection and forecasting framework forvision-based 3D perception in autonomous vehicles. Traditional approaches treatdetection and forecasting as separate sequential tasks, limiting their abilityto leverage temporal cues. ForeSight addresses this limitation with amulti-task streaming and bidirectional learning approach, allowing detectionand forecasting to share query memory and propagate information seamlessly. Theforecast-aware detection transformer enhances spatial reasoning by integratingtrajectory predictions from a multiple hypothesis forecast memory queue, whilethe streaming forecast transformer improves temporal consistency using pastforecasts and refined detections. Unlike tracking-based methods, ForeSighteliminates the need for explicit object association, reducing error propagationwith a tracking-free model that efficiently scales across multi-framesequences. Experiments on the nuScenes dataset show that ForeSight achievesstate-of-the-art performance, achieving an EPA of 54.9%, surpassing previousmethods by 9.3%, while also attaining the best mAP and minADE among multi-viewdetection and forecasting models.</description>
      <author>example@mail.com (Sandro Papais, Letian Wang, Brian Cheong, Steven L. Waslander)</author>
      <guid isPermaLink="false">2508.07089v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>Historical Prediction Attention Mechanism based Trajectory Forecasting for Proactive Work Zone Safety in a Digital Twin Environment</title>
      <link>http://arxiv.org/abs/2508.06544v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于基础设施的主动式工作区安全预警系统，利用数字孪生环境整合实时多传感器数据、高精度地图和历史预测注意力机制的轨迹预测模型，可有效预测车辆轨迹并提前预警潜在冲突。&lt;h4&gt;背景&lt;/h4&gt;工作区车辆冲突可能导致严重事故，传统安全系统可能无法准确预测车辆轨迹并提供及时预警。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够预测车辆轨迹并提前预警潜在冲突的系统，以预防工作区相关事故。&lt;h4&gt;方法&lt;/h4&gt;使用数字孪生环境整合实时多传感器数据和高精度地图；采用基于历史预测注意力机制的轨迹预测模型；结合SUMO和CARLA模拟器的联合仿真环境；使用Lanelet2 HD地图和历史预测网络模型；通过车辆边界框和概率冲突建模进行预警生成。&lt;h4&gt;主要发现&lt;/h4&gt;基于基础设施的HPNet模型在工作区数据集上表现优异，最小联合最终位移误差为0.3228米，最小联合平均位移误差为0.1327米，低于Argoverse和Interaction数据集的基准；主动安全预警应用程序能够有效发出潜在车辆冲突的警报。&lt;h4&gt;结论&lt;/h4&gt;基于基础设施的HPNet模型在预测工作区车辆轨迹方面表现优越，结合预警系统可有效预防工作区相关事故。&lt;h4&gt;翻译&lt;/h4&gt;主动安全系统旨在通过预测车辆之间的潜在冲突并实现早期干预来预防工作区相关事故，从而缓解风险。本研究提出了一种基于基础设施的主动式工作区安全预警系统，该系统利用数字孪生环境，整合实时多传感器数据、详细高精度地图以及基于历史预测注意力机制的轨迹预测模型。结合城市交通移动性仿真和CARLA学习行动模拟器的联合仿真环境，以及Lanelet2高精度地图和历史预测网络模型，我们证明了在高速公路工作区中车辆交互的有效轨迹预测和早期预警生成。为评估预测轨迹的准确性，我们使用两个标准指标：联合平均位移误差和联合最终位移误差。具体而言，基于基础设施的HPNet模型在联合仿真环境生成的工作区数据集上表现出优越的性能，最小联合最终位移误差为0.3228米，最小联合平均位移误差为0.1327米，低于Argoverse数据集和Interaction数据集的基准。此外，我们的主动安全预警应用程序利用车辆边界框和概率冲突建模，展示了其发出潜在车辆冲突警报的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Proactive safety systems aim to mitigate risks by anticipating potentialconflicts between vehicles and enabling early intervention to prevent workzone-related crashes. This study presents an infrastructure-enabled proactivework zone safety warning system that leverages a Digital Twin environment,integrating real-time multi-sensor data, detailed High-Definition (HD) maps,and a historical prediction attention mechanism-based trajectory predictionmodel. Using a co-simulation environment that combines Simulation of UrbanMObility (SUMO) and CAR Learning to Act (CARLA) simulators, along with Lanelet2HD maps and the Historical Prediction Network (HPNet) model, we demonstrateeffective trajectory prediction and early warning generation for vehicleinteractions in freeway work zones. To evaluate the accuracy of predictedtrajectories, we use two standard metrics: Joint Average Displacement Error(ADE) and Joint Final Displacement Error (FDE). Specifically, theinfrastructure-enabled HPNet model demonstrates superior performance on thework-zone datasets generated from the co-simulation environment, achieving aminimum Joint FDE of 0.3228 meters and a minimum Joint ADE of 0.1327 meters,lower than the benchmarks on the Argoverse (minJointFDE: 1.0986 m, minJointADE:0.7612 m) and Interaction (minJointFDE: 0.8231 m, minJointADE: 0.2548 m)datasets. In addition, our proactive safety warning generation application,utilizing vehicle bounding boxes and probabilistic conflict modeling,demonstrates its capability to issue alerts for potential vehicle conflicts.</description>
      <author>example@mail.com (Minhaj Uddin Ahmad, Mizanur Rahman, Alican Sevim, David Bodoh, Sakib Khan, Li Zhao, Nathan Huynh, Eren Erman Ozguven)</author>
      <guid isPermaLink="false">2508.06544v1</guid>
      <pubDate>Tue, 12 Aug 2025 16:21:21 +0800</pubDate>
    </item>
    <item>
      <title>OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing</title>
      <link>http://arxiv.org/abs/2508.04361v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了OmniPlay基准测试，用于评估通用基础模型在动态、交互式世界中的智能表现，特别是它们在多模态融合和推理方面的能力。研究发现了当前全模态模型在高保真记忆任务与需要推理和战略规划的任务之间的性能差异，以及模态冲突对模型性能的影响。&lt;h4&gt;背景&lt;/h4&gt;现有的通用基础模型（如Gemini和GPT-4o）展示了令人印象深刻的多模态能力，但现有的评估方法无法测试它们在动态、交互式世界中的智能。静态基准测试缺乏主动性，而交互式基准测试则存在严重的模态瓶颈，通常忽略关键的听觉和时间线索。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一评估差距，研究引入了OmniPlay，一个诊断基准，旨在评估代理模型在完整感官范围内的融合和推理能力。&lt;h4&gt;方法&lt;/h4&gt;OmniPlay基于模态相互依赖的核心理念，包含五个游戏环境，创造协同和冲突的场景，迫使代理执行真正的跨模态推理。研究对六个领先的全模态模型进行了全面评估。&lt;h4&gt;主要发现&lt;/h4&gt;1. 六个领先的全模态模型表现出关键的二元性：它们在高保真记忆任务中表现出超人性能，但在需要稳健推理和战略规划的挑战中存在系统性失败。2. 这种脆弱性源于脆弱的融合机制，导致在模态冲突下性能灾难性下降。3. 研究发现了一个反直觉的'少即是多'悖论，即移除感官信息可以悖论性地提高性能。&lt;h4&gt;结论&lt;/h4&gt;实现稳健AGI的路径需要超越扩展规模的研究重点，明确解决协同融合问题。研究平台可在https://github.com/fuqingbie/omni-game-benchmark获取。&lt;h4&gt;翻译&lt;/h4&gt;虽然像Gemini和GPT-4o这样的通用基础模型展示了令人印象深刻的多模态能力，但现有的评估无法测试它们在动态、交互式世界中的智能。静态基准测试缺乏主动性，而交互式基准测试则存在严重的模态瓶颈，通常忽略关键的听觉和时间线索。为了弥合这一评估差距，我们引入了OmniPlay，一个诊断基准，旨在评估代理模型在整个感官范围内的融合和推理能力。基于模态相互依赖的核心理念，OmniPlay包含一套五个游戏环境，系统地创造协同和冲突的场景，迫使代理执行真正的跨模态推理。我们对六个领先的全模态模型的全面评估揭示了一个关键的二元性：它们在高保真记忆任务中表现出超人性能，但在需要稳健推理和战略规划的挑战中存在系统性失败。我们证明这种脆弱性源于脆弱的融合机制，导致在模态冲突下性能灾难性下降，并发现了一个反直觉的'少即是多'悖论，即移除感官信息可以悖论性地提高性能。我们的研究结果表明，实现稳健AGI的路径需要超越扩展规模的研究重点，明确解决协同融合问题。我们的平台可在https://github.com/fuqingbie/omni-game-benchmark进行匿名评审。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While generalist foundation models like Gemini and GPT-4o demonstrateimpressive multi-modal competence, existing evaluations fail to test theirintelligence in dynamic, interactive worlds. Static benchmarks lack agency,while interactive benchmarks suffer from a severe modal bottleneck, typicallyignoring crucial auditory and temporal cues. To bridge this evaluation chasm,we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,but to probe the fusion and reasoning capabilities of agentic models across thefull sensory spectrum. Built on a core philosophy of modality interdependence,OmniPlay comprises a suite of five game environments that systematically createscenarios of both synergy and conflict, forcing agents to perform genuinecross-modal reasoning. Our comprehensive evaluation of six leading omni-modalmodels reveals a critical dichotomy: they exhibit superhuman performance onhigh-fidelity memory tasks but suffer from systemic failures in challengesrequiring robust reasoning and strategic planning. We demonstrate that thisfragility stems from brittle fusion mechanisms, which lead to catastrophicperformance degradation under modality conflict and uncover a counter-intuitive"less is more" paradox, where removing sensory information can paradoxicallyimprove performance. Our findings suggest that the path toward robust AGIrequires a research focus beyond scaling to explicitly address synergisticfusion. Our platform is available for anonymous review athttps://github.com/fuqingbie/omni-game-benchmark.</description>
      <author>example@mail.com (Fuqing Bie, Shiyu Huang, Xijia Tao, Zhiqin Fang, Leyi Pan, Junzhe Chen, Min Ren, Liuyu Xiang, Zhaofeng He)</author>
      <guid isPermaLink="false">2508.04361v2</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
  <item>
      <title>Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning</title>
      <link>http://arxiv.org/abs/2508.06199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究对预训练神经网络在化学和小分子药物设计中的应用进行了迄今为止最广泛的比较评估，发现几乎所有神经网络模型相比基线ECFP分子指纹方法没有显著改进，只有基于分子指纹的CLAMP模型表现明显优于其他模型。&lt;h4&gt;背景&lt;/h4&gt;预训练神经网络在化学和小分子药物设计领域引起了广泛关注，这些模型的嵌入表示被广泛用于分子性质预测、虚拟筛选和分子化学中的小数据学习。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在对各种预训练神经网络模型进行最广泛的比较评估，以了解它们在实际应用中的性能表现。&lt;h4&gt;方法&lt;/h4&gt;研究在公平比较框架下评估了25个不同模态、架构和预训练策略的模型，并使用了专门的分层贝叶斯统计测试模型进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;几乎所有神经网络模型相比基线ECFP分子指纹方法显示出可忽略或没有改进，只有基于分子指纹的CLAMP模型在统计上显著优于其他模型。&lt;h4&gt;结论&lt;/h4&gt;这些发现引发了人们对现有研究评估严谨性的担忧，研究讨论了可能的原因，提出了解决方案，并给出了实用建议。&lt;h4&gt;翻译&lt;/h4&gt;预训练神经网络在化学和小分子药物设计中引起了广泛关注。这些模型的嵌入表示被广泛用于分子性质预测、虚拟筛选和分子化学中的小数据学习。本研究迄今为止对这类模型进行了最广泛的比较，评估了25个模型在25个数据集上的表现。在公平比较框架下，我们评估了涵盖各种模态、架构和预训练策略的模型。使用专门的分层贝叶斯统计测试模型，我们得出了一个令人惊讶的结果：几乎所有神经网络模型相比基线ECFP分子指纹显示出可忽略或没有改进。只有同样基于分子指纹的CLAMP模型在统计上显著优于其他模型。这些发现引发了对现有研究评估严谨性的担忧。我们讨论了潜在原因，提出了解决方案，并给出了实用建议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pretrained neural networks have attracted significant interest in chemistryand small molecule drug design. Embeddings from these models are widely usedfor molecular property prediction, virtual screening, and small data learningin molecular chemistry. This study presents the most extensive comparison ofsuch models to date, evaluating 25 models across 25 datasets. Under a faircomparison framework, we assess models spanning various modalities,architectures, and pretraining strategies. Using a dedicated hierarchicalBayesian statistical testing model, we arrive at a surprising result: nearlyall neural models show negligible or no improvement over the baseline ECFPmolecular fingerprint. Only the CLAMP model, which is also based on molecularfingerprints, performs statistically significantly better than thealternatives. These findings raise concerns about the evaluation rigor inexisting studies. We discuss potential causes, propose solutions, and offerpractical recommendations.</description>
      <author>example@mail.com (Mateusz Praski, Jakub Adamczyk, Wojciech Czech)</author>
      <guid isPermaLink="false">2508.06199v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events</title>
      <link>http://arxiv.org/abs/2508.06122v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 6 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究应用表征学习算法处理卫星图像，并通过各种天气事件分类评估学习到的潜在空间表现，发现卷积自编码器(CAE)在大多数任务中表现最佳，且高分辨率数据集对深度学习算法有益，但潜在空间维度小于128会导致误报率显著增加。&lt;h4&gt;背景&lt;/h4&gt;卫星图像分析在天气事件监测中具有重要价值，但如何有效提取和利用卫星图像中的表征信息仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;评估不同表征学习算法在卫星图像天气事件分类中的表现，并探索影响分类效果的关键因素。&lt;h4&gt;方法&lt;/h4&gt;研究了三种表征学习算法：主成分分析(PCA)、卷积自编码器(CAE)和预训练残差网络(PT)，通过分类任务评估潜在空间质量，并测试了数据分辨率和潜在空间大小对分类效果的影响。&lt;h4&gt;主要发现&lt;/h4&gt;1) CAE学习的潜在空间在所有分类任务中表现最佳；2) PCA虽高命中率但误报率也高；3) PT在识别热带气旋方面表现突出，其他任务较差；4) 高分辨率数据集对深度学习算法有益；5) 潜在空间维度小于128会导致误报率显著增加；6) CAE学习到的表征与物理属性缺乏直接联系。&lt;h4&gt;结论&lt;/h4&gt;开发物理信息的CAE版本可能是提高卫星图像天气事件分类准确性的有前景方向。&lt;h4&gt;翻译&lt;/h4&gt;本研究将表征学习算法应用于卫星图像，并通过各种天气事件的分类评估学习到的潜在空间。研究的算法包括经典线性变换即主成分分析(PCA)、最先进的深度学习方法即卷积自编码器(CAE)，以及在大型图像数据集上预训练的残差网络(PT)。实验结果表明，CAE学习到的潜在空间在所有分类任务中持续显示更高的威胁分数。PCA的分类产生了高命中率但也产生了高误报率。此外，PT在识别热带气旋方面表现异常出色，但在其他任务中表现较差。进一步的实验表明，从更高分辨率数据集中学习到的表征在深度学习算法(CAE和PT)的所有分类任务中都更优。我们还发现，较小的潜在空间大小对分类任务的命中率影响较小，但潜在空间维度小于128会导致显著更高的误报率。尽管CAE能够有效且高效地学习潜在空间，但学习到的表征与物理属性缺乏直接联系。因此，开发物理信息的CAE版本可能是当前工作的有前景的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.22541/essoar.168394729.95734739/v1&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study applied representation learning algorithms to satellite images andevaluated the learned latent spaces with classifications of various weatherevents. The algorithms investigated include the classical lineartransformation, i.e., principal component analysis (PCA), state-of-the-art deeplearning method, i.e., convolutional autoencoder (CAE), and a residual networkpre-trained with large image datasets (PT). The experiment results indicatedthat the latent space learned by CAE consistently showed higher threat scoresfor all classification tasks. The classifications with PCA yielded high hitrates but also high false-alarm rates. In addition, the PT performedexceptionally well at recognizing tropical cyclones but was inferior in othertasks. Further experiments suggested that representations learned fromhigher-resolution datasets are superior in all classification tasks fordeep-learning algorithms, i.e., CAE and PT. We also found that smaller latentspace sizes had minor impact on the classification task's hit rate. Still, alatent space dimension smaller than 128 caused a significantly higher falsealarm rate. Though the CAE can learn latent spaces effectively and efficiently,the interpretation of the learned representation lacks direct connections tophysical attributions. Therefore, developing a physics-informed version of CAEcan be a promising outlook for the current work.</description>
      <author>example@mail.com (Ting-Shuo Yo, Shih-Hao Su, Chien-Ming Wu, Wei-Ting Chen, Jung-Lien Chu, Chiao-Wei Chang, Hung-Chi Kuo)</author>
      <guid isPermaLink="false">2508.06122v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Dual prototype attentive graph network for cross-market recommendation</title>
      <link>http://arxiv.org/abs/2508.05969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICONIP 2025 (Oral)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DGRE的新型跨市场推荐方法，通过同时考虑市场特定和市场共享的洞察力，提高了跨市场推荐系统的泛化能力和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;跨市场推荐系统旨在利用成熟市场的历史数据来促进新兴市场中的跨国产品，但现有方法往往忽视了不同市场用户间潜在共享的偏好，主要专注于建模每个市场内的特定偏好。&lt;h4&gt;目的&lt;/h4&gt;整合市场特定和市场共享的洞察力，增强跨市场推荐系统的泛化能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出DGRE方法，基于图表示学习利用物品和用户原型捕获市场特定和市场共享的洞察。通过聚类不同市场用户创建市场共享用户档案，同时聚合市场内物品特征构建物品侧原型，提供市场特定洞察。&lt;h4&gt;主要发现&lt;/h4&gt;在建模中同时考虑市场特定和市场共享方面可以改进跨市场推荐系统的泛化能力和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;考虑市场特定和市场共享两个方面的建模能有效提高跨市场推荐系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;跨市场推荐系统旨在利用成熟市场的历史数据来促进新兴市场中的跨国产品。然而，现有方法往往忽视了不同市场用户之间潜在共享的偏好，主要专注于建模每个市场内的特定偏好。在本文中，我们认为整合市场特定和市场共享的洞察力可以增强CMRS的泛化能力和鲁棒性。我们提出了一种名为DGRE的新方法来解决这个问题。DGRE利用基于图表示学习的物品和用户原型来捕获市场特定和市场共享的洞察力。具体来说，DGRE通过聚类来自不同市场的用户来识别行为相似性并创建市场共享的用户档案，从而整合市场共享原型；同时，它通过聚合每个市场内的物品特征来构建物品侧原型，提供有价值的市场特定洞察。我们在真实的跨市场数据集上进行了广泛的实验来验证DGRE的有效性，结果表明在建模中同时考虑市场特定和市场共享方面可以改进CMRS的泛化能力和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-market recommender systems (CMRS) aim to utilize historical data frommature markets to promote multinational products in emerging markets. However,existing CMRS approaches often overlook the potential for shared preferencesamong users in different markets, focusing primarily on modeling specificpreferences within each market. In this paper, we argue that incorporating bothmarket-specific and market-shared insights can enhance the generalizability androbustness of CMRS. We propose a novel approach called Dual Prototype AttentiveGraph Network for Cross-Market Recommendation (DGRE) to address this. DGREleverages prototypes based on graph representation learning from both items andusers to capture market-specific and market-shared insights. Specifically, DGREincorporates market-shared prototypes by clustering users from various marketsto identify behavioural similarities and create market-shared user profiles.Additionally, it constructs item-side prototypes by aggregating item featureswithin each market, providing valuable market-specific insights. We conductextensive experiments to validate the effectiveness of DGRE on a real-worldcross-market dataset, and the results show that considering bothmarket-specific and market-sharing aspects in modelling can improve thegeneralization and robustness of CMRS.</description>
      <author>example@mail.com (Li Fan, Menglin Kong, Yang Xiang, Chong Zhang, Chengtao Ji)</author>
      <guid isPermaLink="false">2508.05969v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Algorithm for Estimating Intrinsic Geometry</title>
      <link>http://arxiv.org/abs/2508.06355v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种量子算法，用于估计点云的局部内在维数和局部标量曲率，这些是几何数据分析中的关键量。该算法在处理具有成对几何距离的数据集时，能够高效地输出给定点的几何特性估计，相比经典算法实现了指数级加速，并且比现有量子算法在扩散映射方面也有显著改进。&lt;h4&gt;背景&lt;/h4&gt;高维数据集通常围绕低维流形聚类，但常常受到严重噪声的影响，这掩盖了下游学习任务所必需的内在几何结构。这种噪声干扰使得准确理解和分析数据的内在几何特性变得困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确估计数据内在几何特性的量子算法，特别是局部内在维数和局部标量曲率，这些量对于降维、特征提取和异常检测等核心任务至关重要，从而为各种数据驱动和数据辅助应用提供支持。&lt;h4&gt;方法&lt;/h4&gt;提出一种量子算法，该算法接受具有成对几何距离的数据集作为输入，输出给定点的局部维数和曲率的估计。该方法利用量子计算的优势，能够更高效地处理高维几何数据分析问题。&lt;h4&gt;主要发现&lt;/h4&gt;该量子算法相比其经典对应算法实现了指数级加速优势。此外，作为推论，研究将主要技术扩展到扩散映射，取得了比现有量子算法更显著的指数级改进。这表明量子计算在几何数据分析方面具有巨大潜力。&lt;h4&gt;结论&lt;/h4&gt;这项研究标志着几何数据分析中高效量子应用的又一步进展，超越了传统的拓扑摘要方法，走向更精确的几何推断。它为量子增强的流形学习开辟了一条新颖的可扩展路径，有望在未来数据科学和机器学习中发挥重要作用。&lt;h4&gt;翻译&lt;/h4&gt;高维数据集通常围绕低维流形聚类，但也常常受到严重噪声的影响，掩盖了下游学习任务所必需的内在几何结构。我们提出了一种量子算法，用于估计点云的内在几何结构，特别是其局部内在维数和局部标量曲率。这些量对于降维、特征提取和异常检测至关重要，而这些任务是各种数据驱动和数据辅助应用的核心。在这项工作中，我们提出了一种量子算法，该算法接受具有成对几何距离的数据集，并输出给定点的局部维数和曲率的估计。我们证明这种量子算法比其经典对应算法具有指数级加速优势，并且作为推论，进一步将我们的主要技术扩展到扩散映射，取得了比现有量子算法更显著的指数级改进。我们的工作标志着几何数据分析中高效量子应用的又一步进展，超越了拓扑摘要，走向精确的几何推断，并为量子增强的流形学习开辟了一条新颖的可扩展路径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何利用量子算法高效估计点云数据的内在几何特性，特别是局部内在维度和局部标量曲率。这个问题在现实中很重要，因为高维数据通常围绕低维流形聚类但受噪声影响，准确估计这些几何特性对于降维、特征提取、异常检测等数据驱动任务至关重要，能帮助有效去噪、减少存储计算成本，同时发现数据中的异常结构。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从经典几何数据分析算法出发，特别是基于[9]中提出的经典估计器，并将其翻译到量子设置中。他们借鉴了扩散几何方法[18]从原始距离估计测地距离，利用块编码/量子奇异值变换框架[22-24]处理矩阵运算，并采用了高效的量子状态准备技术[32]。作者还参考了量子PCA技术[34,35]和已有的量子拓扑数据分析成果，将这些技术整合应用于几何数据分析这一相对未被探索的领域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用量子计算的并行性和状态变换能力，通过块编码框架将经典几何算法转化为量子算法，实现指数级加速。整体流程包括：1)输入数据点和成对距离；2)构建核矩阵的块编码；3)估计测地距离；4)为给定点找最近邻；5)计算局部内在维度；6)估计采样密度；7)构建测地球并计算体积；8)拟合二次曲线；9)估计局部标量曲率。整个流程充分利用了量子状态准备和量子奇异值变换等技术处理大规模高维数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出估计点云内在几何特性的量子算法；2)证明相比经典算法具有指数级加速；3)将块编码框架应用于几何数据分析；4)技术扩展到扩散地图实现指数级改进；5)提供量子状态准备技术的有效应用。相比之前工作，该算法不需要访问某些矩阵的oracle，只需成对距离的经典值，复杂度从O(N³)或O(N² log³ N)降低到O(polylog(N, m))，专注于几何特性而非拓扑特性，为量子几何数据分析开辟了新路径。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种量子算法，能够以指数级速度超越经典方法估计高维点云数据的局部内在维度和曲率，为量子增强的流形学习开辟了一条新的可扩展路径。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-dimensional datasets typically cluster around lower-dimensionalmanifolds but are also often marred by severe noise, obscuring the intrinsicgeometry essential for downstream learning tasks. We present a quantumalgorithm for estimating the intrinsic geometry of a point cloud --specifically its local intrinsic dimension and local scalar curvature. Thesequantities are crucial for dimensionality reduction, feature extraction, andanomaly detection -- tasks that are central to a wide range of data-driven anddata-assisted applications. In this work, we propose a quantum algorithm whichtakes a dataset with pairwise geometric distance, output the estimation oflocal dimension and curvature at a given point. We demonstrate that thisquantum algorithm achieves an exponential speedup over its classicalcounterpart, and, as a corollary, further extend our main technique todiffusion maps, yielding exponential improvements even over existing quantumalgorithms. Our work marks another step toward efficient quantum applicationsin geometrical data analysis, moving beyond topological summaries towardprecise geometric inference and opening a novel, scalable path toquantum-enhanced manifold learning.</description>
      <author>example@mail.com (Nhat A. Nghiem, Tuan K. Do, Tzu-Chieh Wei, Trung V. Phan)</author>
      <guid isPermaLink="false">2508.06355v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.06203v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AnomalyMoE是一种基于专家混合(MoE)架构的新型通用异常检测框架，通过分层设计和专门的专家网络实现了比现有专业化方法更优越的性能。&lt;h4&gt;背景&lt;/h4&gt;异常检测是许多领域和模态中的关键任务，但现有方法通常高度专业化，限制了它们的泛化能力。这些专业化模型在部署到指定环境之外时表现有限。&lt;h4&gt;目的&lt;/h4&gt;克服现有异常检测方法的局限性，提出一个能够处理多种类型异常的通用框架。&lt;h4&gt;方法&lt;/h4&gt;AnomalyMoE将异常检测问题分解为三个语义层次：局部结构异常、组件级语义异常和全局逻辑异常，并在相应级别使用专门的专家网络。同时引入专家信息排斥(EIR)模块促进专家多样性和专家选择平衡(ESB)模块确保充分利用所有专家。&lt;h4&gt;主要发现&lt;/h4&gt;在8个涵盖工业成像、3D点云、医学成像、视频监控和逻辑异常检测的数据集上，AnomalyMoE建立了新的最先进性能，显著优于各自领域中的专业化方法。&lt;h4&gt;结论&lt;/h4&gt;AnomalyMoE通过分层设计和专门的专家网络，实现了单个模型能够同时理解和检测广泛异常的能力，是一种有效的通用异常检测解决方案。&lt;h4&gt;翻译&lt;/h4&gt;异常检测是众多领域和模态中的关键任务，但现有方法通常高度专业化，限制了它们的泛化能力。这些针对特定异常类型（如纹理缺陷或逻辑错误）定制的专业化模型，在部署到指定环境之外时通常表现有限。为克服这一局限，我们提出了AnomalyMoE，一种基于专家混合(MoE)架构的新型通用异常检测框架。我们的核心见解是将复杂的异常检测问题分解为三个不同的语义层次：局部结构异常、组件级语义异常和全局逻辑异常。AnomalyMoE相应地在补丁、组件和全局级别使用三个专门的专家网络，专门用于重建特征和识别指定语义级别的偏差。这种分层设计使单个模型能够同时理解和检测广泛的异常。此外，我们引入了专家信息排斥(EIR)模块以促进专家多样性，以及专家选择平衡(ESB)模块以确保充分利用所有专家。在8个涵盖工业成像、3D点云、医学成像、视频监控和逻辑异常检测的具有挑战性的数据集上的实验表明，AnomalyMoE建立了新的最先进性能，显著优于各自领域中的专业化方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection is a critical task across numerous domains and modalities,yet existing methods are often highly specialized, limiting theirgeneralizability. These specialized models, tailored for specific anomaly typeslike textural defects or logical errors, typically exhibit limited performancewhen deployed outside their designated contexts. To overcome this limitation,we propose AnomalyMoE, a novel and universal anomaly detection framework basedon a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose thecomplex anomaly detection problem into three distinct semantic hierarchies:local structural anomalies, component-level semantic anomalies, and globallogical anomalies. AnomalyMoE correspondingly employs three dedicated expertnetworks at the patch, component, and global levels, and is specialized inreconstructing features and identifying deviations at its designated semanticlevel. This hierarchical design allows a single model to concurrentlyunderstand and detect a wide spectrum of anomalies. Furthermore, we introducean Expert Information Repulsion (EIR) module to promote expert diversity and anExpert Selection Balancing (ESB) module to ensure the comprehensive utilizationof all experts. Experiments on 8 challenging datasets spanning industrialimaging, 3D point clouds, medical imaging, video surveillance, and logicalanomaly detection demonstrate that AnomalyMoE establishes new state-of-the-artperformance, significantly outperforming specialized methods in theirrespective domains.</description>
      <author>example@mail.com (Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Wei Ge, Ming Tang, Jinqiao Wang)</author>
      <guid isPermaLink="false">2508.06203v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Neural Radio Radiance Field for Localized Statistical Channel Modelling</title>
      <link>http://arxiv.org/abs/2508.06054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MM-LSCM，一种用于下一代网络优化的自监督多模态神经辐射场框架，用于局部统计信道建模。该框架整合RSRP数据和LiDAR点云信息，提高空间感知和预测准确性，采用自监督训练方法无需昂贵标记数据，实验表明其在信道重建精度和噪声鲁棒性上显著优于传统方法。&lt;h4&gt;背景&lt;/h4&gt;传统LSCM方法仅依赖RSRP数据，限制了对影响信号传播的环境结构的建模能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更准确建模环境结构对信号传播影响的LSCM方法，提高信道重建精度和对噪声的鲁棒性，用于下一代网络优化。&lt;h4&gt;方法&lt;/h4&gt;提出MM-LSCM框架，采用双分支神经网络架构整合RSRP数据和LiDAR点云信息，利用基于体积渲染的多模态合成对齐无线电传播与环境障碍物，采用自监督训练方法无需标记数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MM-LSCM在信道重建精度和对噪声的鲁棒性方面显著优于传统方法。&lt;h4&gt;结论&lt;/h4&gt;MM-LSCM是实际无线网络优化的一种有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了MM-LSCM，一种用于下一代网络优化的自监督多模态神经辐射场框架，用于局部统计信道建模。传统LSCM方法仅依赖RSRP数据，限制了其对影响信号传播的环境结构的建模能力。为解决这一问题，我们提出了一种双分支神经网络架构，整合RSRP数据和LiDAR点云信息，提高了空间感知和预测准确性。MM-LSCM利用基于体积渲染的多模态合成来对齐无线电传播与环境障碍物，并采用自监督训练方法，无需昂贵的标记数据。实验结果表明，MM-LSCM在信道重建精度和对噪声的鲁棒性方面显著优于传统方法，使其成为实际无线网络优化的一种有前景的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决传统局部统计信道建模方法仅依赖RSRP数据而缺乏环境感知能力的问题。这个问题在现实中很重要，因为随着新一代无线网络对无缝连接的需求增长，优化网络参数变得复杂，而传统方法要么成本高（如路测），要么缺乏环境细节（如统计模型），要么计算昂贵（如射线追踪），难以准确预测未探索区域的信道特性，影响网络优化效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统LSCM方法的局限性，然后考虑如何将环境信息融入信道建模。他们借鉴了NeRF在室内信道建模的潜力，以及多模态感知信息（特别是LiDAR点云与RSRP测量同时收集）的最新进展。方法设计上，他们创建了双分支神经网络架构分别处理点云数据和无线电信号，引入'停止概率'作为相干权重对齐无线电传播与环境障碍，并采用自监督训练避免昂贵的标记数据需求。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态融合将无线电RSRP数据与环境LiDAR点云数据结合，增强空间感知能力。整体流程包括：1) 环境特征设计，基于空间体素化处理3D点云识别障碍物；2) 多模态神经无线电辐射场网络，采用双分支架构分别建模命中概率和方向相关信号；3) 基于体积渲染的多模态合成，沿基站发出的射线查询网络获取信号和概率；4) 自监督训练，结合无线电模态损失和环境监督损失优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 专门针对LSCM的多模态神经无线电辐射网络；2) '停止概率'作为隐式相干权重对齐无线电传播与环境障碍；3) 自监督训练方法利用点云先验深度信息。相比之前工作，不同之处在于整合了RSRP和LiDAR点云信息，适用于更广泛场景而非仅限室内，利用环境数据监督提高预测能力，且在噪声环境下表现更鲁棒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了MM-LSCM，一种自监督多模态神经无线电辐射场框架，通过整合RSRP数据和LiDAR点云信息，显著提升了局部统计信道建模的准确性和鲁棒性，为下一代网络优化提供了有效解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents MM-LSCM, a self-supervised multi-modal neural radioradiance field framework for localized statistical channel modeling (LSCM) fornext-generation network optimization. Traditional LSCM methods rely solely onRSRP data, limiting their ability to model environmental structures that affectsignal propagation. To address this, we propose a dual-branch neuralarchitecture that integrates RSRP data and LiDAR point cloud information,enhancing spatial awareness and predictive accuracy. MM-LSCM leveragesvolume-rendering-based multi-modal synthesis to align radio propagation withenvironmental obstacles and employs a self-supervised training approach,eliminating the need for costly labeled data. Experimental results demonstratethat MM-LSCM significantly outperforms conventional methods in channelreconstruction accuracy and robustness to noise, making it a promising solutionfor real-world wireless network optimization.</description>
      <author>example@mail.com (Yiheng Wang, Shutao Zhang, Ye Xue, Tsung-Hui Chang)</author>
      <guid isPermaLink="false">2508.06054v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Neural Field Representations of Mobile Computational Photography</title>
      <link>http://arxiv.org/abs/2508.05907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了如何使用精心设计的神经场模型从移动摄影数据中直接实现深度估计、分层分离和图像拼接等功能，无需复杂预处理或标记数据。&lt;h4&gt;背景&lt;/h4&gt;移动成像在过去二十年经历了深刻变革，现代手机配备了多种成像技术和非视觉传感器，结合板载处理芯片成为多功能的口袋式计算成像平台。同时，神经场技术能够重建复杂场景而不需要显式的数据表示。&lt;h4&gt;目的&lt;/h4&gt;展示精心设计的神经场模型如何紧凑地表示复杂的几何和光照效果，并实现直接从野外移动摄影数据中应用深度估计、分层分离和图像拼接等功能。&lt;h4&gt;方法&lt;/h4&gt;使用精心设计、自正则化的神经场模型，通过随机梯度下降解决具有挑战性的逆问题，直接适配智能手机的原始测量数据。&lt;h4&gt;主要发现&lt;/h4&gt;这些方法优于最先进的方法，不依赖复杂的预处理步骤、标记的真实数据或机器学习先验。&lt;h4&gt;结论&lt;/h4&gt;神经场模型能够有效表示复杂的几何和光照效果，从移动摄影数据中直接应用各种图像处理任务是可行的，且不需要传统方法所需的预处理和标记数据。&lt;h4&gt;翻译&lt;/h4&gt;在过去的二十年里，移动成像经历了深刻的变革，手机迅速在普及性上超越了所有其他形式的数字摄影。如今的手机配备了各种成像技术——激光测距、多焦摄像头阵列和分像素传感器——以及非视觉传感器，如陀螺仪、加速度计和磁力计。结合用于图像和信号处理的板载集成芯片，使手机成为多功能的口袋式计算成像平台。与此同时，近年来我们见证了神经场如何通过小型神经网络训练将连续的空间输入坐标映射到输出信号，从而能够在没有显式数据表示（如像素阵列或点云）的情况下重建复杂场景。在本论文中，我展示了精心设计的神经场模型如何紧凑地表示复杂的几何和光照效果。实现了直接从收集的野外移动摄影数据中进行深度估计、分层分离和图像拼接等功能。这些方法优于最先进的方法，而不依赖复杂的预处理步骤、标记的真实数据或机器学习先验。相反，它们利用构建良好、自正则化的模型，通过随机梯度下降解决具有挑战性的逆问题，直接适配智能手机的原始测量数据。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何利用神经场模型从移动设备采集的野外摄影数据中直接实现深度估计、层分离和图像拼接等应用问题。这个问题很重要，因为现代手机已成为多功能的口袋式计算成像平台，集成了多种传感器和图像处理芯片，能够产生大量高质量数据；同时，神经场技术能紧凑表示复杂几何和光照效果，为计算机视觉研究提供更丰富、多样化的数据集和问题空间，推动增强现实、物体理解等实际应用的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了移动计算摄影的现状和神经场技术的潜力，认识到手机摄影产生的数据量巨大但存储处理存在挑战。他借鉴了现有的神经辐射场(NeRF)和神经曲面等技术，这些技术通过小型神经网络将连续空间坐标映射到输出信号。作者设计了专门的神经场模型适应移动摄影特点，如使用密集神经场跟踪帧间像素运动估计深度，并通过最小化光度重投影损失实现端到端优化。整个设计过程基于对成像原理、传感器特性和计算挑战的深入理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用神经场模型紧凑表示复杂场景的几何和光照信息，从移动设备数据中直接实现视觉任务。整体流程包括：1) 使用智能手机采集长曝光序列的RAW图像、相机内参和陀螺仪数据；2) 设计神经RGB-D场景拟合模型，包含深度和姿态的显式几何投影；3) 通过最小化光度重投影损失，将模型拟合到数据中联合估计深度和相机姿态；4) 利用估计信息实现深度估计、层分离和图像拼接等应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 端到端的神经RGB-D场景拟合方法，从不稳定长时间曝光摄影中提取高保真深度和姿态估计，无需深度初始化；2) 开发智能手机数据采集应用，获取RAW图像和传感器数据；3) 改进神经场模型，使用多分辨率哈希网格编码提高训练速度和重建质量；4) 全面实验验证，优于现有方法并与高精度扫描对比。相比之前工作，不同之处在于无需依赖LiDAR等初始深度测量，使用更紧凑的前向投影模型，且应用范围更广，不仅限于深度估计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 作者提出了一种基于神经场的端到端方法，能够从移动设备采集的野外摄影数据中直接实现高质量深度估计、层分离和图像拼接，无需复杂预处理或标记数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over the past two decades, mobile imaging has experienced a profoundtransformation, with cell phones rapidly eclipsing all other forms of digitalphotography in popularity. Today's cell phones are equipped with a diverserange of imaging technologies - laser depth ranging, multi-focal camera arrays,and split-pixel sensors - alongside non-visual sensors such as gyroscopes,accelerometers, and magnetometers. This, combined with on-board integratedchips for image and signal processing, makes the cell phone a versatilepocket-sized computational imaging platform. Parallel to this, we have seen inrecent years how neural fields - small neural networks trained to mapcontinuous spatial input coordinates to output signals - enable thereconstruction of complex scenes without explicit data representations such aspixel arrays or point clouds. In this thesis, I demonstrate how carefullydesigned neural field models can compactly represent complex geometry andlighting effects. Enabling applications such as depth estimation, layerseparation, and image stitching directly from collected in-the-wild mobilephotography data. These methods outperform state-of-the-art approaches withoutrelying on complex pre-processing steps, labeled ground truth data, or machinelearning priors. Instead, they leverage well-constructed, self-regularizedmodels that tackle challenging inverse problems through stochastic gradientdescent, fitting directly to raw measurements from a smartphone.</description>
      <author>example@mail.com (Ilya Chugunov)</author>
      <guid isPermaLink="false">2508.05907v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Situationally-aware Path Planning Exploiting 3D Scene Graphs</title>
      <link>http://arxiv.org/abs/2508.06283v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;S-Path是一种情境感知的路径规划器，利用室内3D场景图的度量-语义结构显著提高规划效率，实现平均5.7倍的规划时间减少，同时保持路径最优性，在复杂场景中表现更优。&lt;h4&gt;背景&lt;/h4&gt;3D场景图整合了度量信息和语义信息，但其结构在提高路径规划效率和可解释性方面未被充分利用。&lt;h4&gt;目的&lt;/h4&gt;提出S-Path，一个情境感知的路径规划器，利用室内3D场景图的度量-语义结构来显著提高规划效率。&lt;h4&gt;方法&lt;/h4&gt;S-Path采用两阶段过程：首先在从场景图派生的语义图上进行搜索，产生人类可理解的高层次路径并识别相关规划区域，将问题分解为更小、独立的子问题并行解决；引入重新规划机制，在路径不可行时重用已解决子问题的信息更新语义启发式，优先重用以提高未来规划效率。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界和模拟环境中的广泛实验表明，S-Path实现了平均5.7倍的规划时间减少，保持与经典基于采样的规划器相当的路径最优性，并在复杂场景中优于经典方法。&lt;h4&gt;结论&lt;/h4&gt;S-Path是针对由室内3D场景图表示的环境的高效且可解释的路径规划器。&lt;h4&gt;翻译&lt;/h4&gt;3D场景图整合了度量和语义信息，但它们的结构在提高路径规划效率和可解释性方面仍未得到充分利用。在这项工作中，我们提出了S-Path，一个情境感知的路径规划器，它利用室内3D场景图的度量-语义结构来显著提高规划效率。S-Path遵循两阶段过程：它首先在从场景图派生的语义图上进行搜索，产生人类可理解的高层次路径。这还确定了规划的相关区域，随后允许将问题分解为更小、独立的子问题，可以并行解决。我们还引入了一种重新规划机制，在路径不可行的情况下，重用先前已解决子问题的信息来更新语义启发式，并优先重用以进一步提高未来规划尝试的效率。在真实世界和模拟环境中的广泛实验表明，S-Path实现了平均5.7倍的规划时间减少，同时保持与经典基于采样的规划器相当的路径最优性，并在复杂场景中超越它们，使其成为由室内3D场景图表示的环境的高效且可解释的路径规划器。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决传统室内机器人导航路径规划器的效率问题。传统规划器依赖密集采样配置空间，计算成本随环境复杂度快速增长，限制了效率和实用性。这个问题在现实中很重要，因为实时应用（如服务机器人、自动驾驶）需要快速响应，大型复杂环境中传统方法计算资源需求过高，且缺乏语义理解能力，难以与人类直观交互。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到3D场景图集成了度量和语义信息，但结构未被充分利用于路径规划。他们设计了两阶段过程：先在语义图上搜索人类可理解的高层次路径，再分解问题为可并行解决的子问题。借鉴了语义感知路径规划和3D场景图在任务规划中的应用，但将其扩展到低级路径规划，并特别参考了Ray等人的工作，同时专注于效率提升和并行处理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用3D场景图的层次结构（房间、门、墙等）将全局规划分解为更小、独立的子问题，通过并行计算和重用已解决的子问题提高效率。流程：1)输入3D场景图和度量网格；2)环境设置，生成语义图和轮廓；3)语义规划，执行A*搜索得到高层次路径；4)子问题生成，将路径分解为独立子问题；5)几何规划，并行解决各子问题；6)缝合结果形成最终路径；7)必要时重规划，重用已解决子问题。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)充分利用3D场景图的层次结构提高效率和可解释性；2)问题分解与并行处理，显著减少规划时间；3)高效重规划机制，重用子问题信息；4)生成人类可解释的基于语义概念的路径。相比之前工作：传统采样方法效率低；其他语义方法依赖扁平表示，扩展性差；现有场景图应用主要关注任务规划而非路径规划；相比Ray等人工作，S-Path直接利用语义层次结构，专注效率与并行性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; S-Path通过利用3D场景图的层次结构将路径规划分解为可并行子问题，将室内机器人导航的规划效率提高了5.7倍，同时保持路径质量并提供人类可解释的结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Scene Graphs integrate both metric and semantic information, yet theirstructure remains underutilized for improving path planning efficiency andinterpretability. In this work, we present S-Path, a situationally-aware pathplanner that leverages the metric-semantic structure of indoor 3D Scene Graphsto significantly enhance planning efficiency. S-Path follows a two-stageprocess: it first performs a search over a semantic graph derived from thescene graph to yield a human-understandable high-level path. This alsoidentifies relevant regions for planning, which later allows the decompositionof the problem into smaller, independent subproblems that can be solved inparallel. We also introduce a replanning mechanism that, in the event of aninfeasible path, reuses information from previously solved subproblems toupdate semantic heuristics and prioritize reuse to further improve theefficiency of future planning attempts. Extensive experiments on bothreal-world and simulated environments show that S-Path achieves averagereductions of 5.7x in planning time while maintaining comparable pathoptimality to classical sampling-based planners and surpassing them in complexscenarios, making it an efficient and interpretable path planner forenvironments represented by indoor 3D Scene Graphs.</description>
      <author>example@mail.com (Saad Ejaz, Marco Giberna, Muhammad Shaheer, Jose Andres Millan-Romera, Ali Tourani, Paul Kremer, Holger Voos, Jose Luis Sanchez-Lopez)</author>
      <guid isPermaLink="false">2508.06283v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>SIFThinker: Spatially-Aware Image Focus for Visual Reasoning</title>
      <link>http://arxiv.org/abs/2508.06259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了SIFThinker，一种空间感知的'图像思维'框架，通过交替使用深度增强的边界框和自然语言来实现注意力校正和图像区域聚焦，解决了现有多模态大语言模型在复杂视觉任务中的局限性。&lt;h4&gt;背景&lt;/h4&gt;当前的多模态大语言模型在复杂视觉任务（如空间理解、细粒度感知）方面仍面临重大挑战。先前的方法尝试整合视觉推理，但未能利用空间线索进行注意力校正，从而迭代地优化其对提示相关区域的关注。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够模拟人类视觉感知的空间感知框架，通过注意力校正和图像区域聚焦来提高模型在空间理解和细粒度视觉感知方面的能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出了SIFThinker框架，包括两个主要贡献：一是引入反向扩展-前向推理策略，生成交互式的图像-思维链用于过程级监督，从而构建了SIF-50K数据集；二是提出GRPO-SIF训练范式，将深度感知的视觉定位集成到统一的推理流程中，教导模型动态校正和关注提示相关区域。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，SIFThinker在空间理解和细粒度视觉感知方面优于最先进的方法，同时保持强大的通用能力，突显了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;SIFThinker通过空间感知的'图像思维'框架和创新的训练方法，成功解决了多模态大语言模型在复杂视觉任务中的局限性，特别是在空间理解和细粒度感知方面取得了显著进展。&lt;h4&gt;翻译&lt;/h4&gt;当前的多模态大语言模型在复杂的视觉任务（例如，空间理解、细粒度感知）方面仍面临重大挑战。先前的方法尝试整合视觉推理，然而它们未能利用空间线索进行注意力校正，从而迭代地优化其对提示相关区域的关注。在本文中，我们介绍了SIFThinker，一种空间感知的'图像思维'框架，模拟人类视觉感知。具体而言，SIFThinker通过交替使用深度增强的边界框和自然语言来实现注意力校正和图像区域聚焦。我们的贡献有两方面：首先，我们引入了反向扩展-前向推理策略，促进了用于过程级监督的交互式图像-思维链的生成，进而促进了SIF-50K数据集的构建。此外，我们提出了GRPO-SIF，一种将深度感知的视觉定位集成到统一推理流程中的强化训练范式，教导模型动态校正和关注提示相关区域。大量实验表明，SIFThinker在空间理解和细粒度视觉感知方面优于最先进的方法，同时保持强大的通用能力，突显了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current multimodal large language models (MLLMs) still face significantchallenges in complex visual tasks (e.g., spatial understanding, fine-grainedperception). Prior methods have tried to incorporate visual reasoning, however,they fail to leverage attention correction with spatial cues to iterativelyrefine their focus on prompt-relevant regions. In this paper, we introduceSIFThinker, a spatially-aware "think-with-images" framework that mimics humanvisual perception. Specifically, SIFThinker enables attention correcting andimage region focusing by interleaving depth-enhanced bounding boxes and naturallanguage. Our contributions are twofold: First, we introduce areverse-expansion-forward-inference strategy that facilitates the generation ofinterleaved image-text chains of thought for process-level supervision, whichin turn leads to the construction of the SIF-50K dataset. Besides, we proposeGRPO-SIF, a reinforced training paradigm that integrates depth-informed visualgrounding into a unified reasoning pipeline, teaching the model to dynamicallycorrect and focus on prompt-relevant regions. Extensive experiments demonstratethat SIFThinker outperforms state-of-the-art methods in spatial understandingand fine-grained visual perception, while maintaining strong generalcapabilities, highlighting the effectiveness of our method.</description>
      <author>example@mail.com (Zhangquan Chen, Ruihui Zhao, Chuwei Luo, Mingze Sun, Xinlei Yu, Yangyang Kang, Ruqi Huang)</author>
      <guid isPermaLink="false">2508.06259v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines</title>
      <link>http://arxiv.org/abs/2508.06226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了GeoLaux基准测试，用于评估多模态大语言模型(MLLMs)在几何问题解决方面的长步骤推理和辅助线构造能力，通过13个主流模型的实验发现了模型在长步骤推理、证明问题处理和辅助线意识方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;几何问题解决需要模型掌握图表理解、逻辑推理、知识应用、数值计算和辅助线构造等多方面能力，这对多模态大语言模型构成重大挑战。然而现有评估基准忽略了辅助线构造，缺乏细粒度过程评估，不足以评估长步骤推理能力。&lt;h4&gt;目的&lt;/h4&gt;填补现有评估基准的空白，提出一个全面评估MLLMs几何问题解决能力特别是长步骤推理和辅助线构造能力的基准测试。&lt;h4&gt;方法&lt;/h4&gt;构建包含2,186个几何问题的数据集(平均6.51个推理步骤，最多24步，41.8%需辅助线构造)，设计五维度评估策略(答案正确性、过程正确性、过程质量、辅助线影响和错误原因)，对13个主流MLLMs进行实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;1)模型在长步骤推理中性能明显下降(九个模型性能下降超50%)；2)与计算问题相比，MLLMs解决证明问题时倾向于走捷径；3)模型缺乏辅助线意识，增强此能力对几何推理改进特别有益。&lt;h4&gt;结论&lt;/h4&gt;GeoLaux作为评估MLLMs长步骤几何推理和辅助线能力的基准，同时也是能力提升的指导。数据集和代码将随补充材料公开发布。&lt;h4&gt;翻译&lt;/h4&gt;几何问题解决需要模型掌握图表理解、逻辑推理、知识应用、数值计算和辅助线构造。这对多模态大语言模型构成重大挑战。然而现有评估基准忽略了辅助线构造，缺乏细粒度过程评估，不足以评估长步骤推理能力。为填补空白，我们提出GeoLaux基准测试，包含2,186个几何问题，既有计算题也有证明题，平均需6.51个推理步骤(最多24步)，其中41.8%需辅助线构造。基于此数据集，我们设计五维度评估策略，评估13个主流MLLMs后得出：模型在长步骤推理中性能明显下降；解决证明问题时倾向于走捷径；缺乏辅助线意识，增强此能力特别有益于几何推理改进。这些发现确立了GeoLaux作为评估基准和指导。数据集和代码将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometry problem solving (GPS) requires models to master diagramcomprehension, logical reasoning, knowledge application, numerical computation,and auxiliary line construction. This presents a significant challenge forMultimodal Large Language Models (MLLMs). However, existing benchmarks forevaluating MLLM geometry skills overlook auxiliary line construction and lackfine-grained process evaluation, making them insufficient for assessing MLLMs'long-step reasoning abilities. To bridge these gaps, we present the GeoLauxbenchmark, comprising 2,186 geometry problems, incorporating both calculationand proving questions. Notably, the problems require an average of 6.51reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliaryline construction. Building on the dataset, we design a novel five-dimensionalevaluation strategy assessing answer correctness, process correctness, processquality, auxiliary line impact, and error causes. Extensive experiments on 13leading MLLMs (including thinking models and non-thinking models) yield threepivotal findings: First, models exhibit substantial performance degradation inextended reasoning steps (nine models demonstrate over 50% performance drop).Second, compared to calculation problems, MLLMs tend to take shortcuts whensolving proving problems. Third, models lack auxiliary line awareness, andenhancing this capability proves particularly beneficial for overall geometryreasoning improvement. These findings establish GeoLaux as both a benchmark forevaluating MLLMs' long-step geometric reasoning with auxiliary lines and aguide for capability advancement. Our dataset and code are included insupplementary materials and will be released.</description>
      <author>example@mail.com (Yumeng Fu, Jiayin Zhu, Lingling Zhang, Bo Zhao, Shaoxuan Ma, Yushun Zhang, Yanrui Wu, Wenjun Wu)</author>
      <guid isPermaLink="false">2508.06226v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments</title>
      <link>http://arxiv.org/abs/2508.05852v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种视觉-语言框架，通过自然语言描述驾驶员视线变化，使用少样本和零样本学习在单张RGB图像上进行驾驶员视觉注意力预测。&lt;h4&gt;背景&lt;/h4&gt;驾驶员视觉注意力预测是自动驾驶和人机交互研究中的关键任务。大多数先前的研究集中在估计单一时间点的注意力分配，通常使用静态RGB图像如驾驶场景图片。&lt;h4&gt;目的&lt;/h4&gt;提出一种视觉-语言框架，通过自然语言建模驾驶员视线的变化，提高注意力预测的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;从BDD-A数据集收集并完善高质量标题，使用人在回路反馈；微调LLaVA模型使视觉感知与以注意力为中心的场景理解保持一致；整合底层线索和自上而下的上下文；使用基于语言的注视行为描述；在不同训练机制下评估性能；引入特定领域指标进行语义对齐和响应多样性评估。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的模型在注意力转移检测和可解释性方面优于通用视觉语言模型；这是首次尝试用自然语言生成驾驶员视觉注意力分配和转移预测的研究之一。&lt;h4&gt;结论&lt;/h4&gt;该方法为自动驾驶中的可解释AI提供了新方向，并为行为预测、人机协作和多智能体协调等下游任务奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;驾驶员视觉注意力预测是自动驾驶和人机交互研究中的一个关键任务。大多数先前的研究集中在估计单一时间点的注意力分配，通常使用静态RGB图像如驾驶场景图片。在这项工作中，我们提出了一种视觉-语言框架，通过自然语言建模驾驶员视线的动态变化，在单张RGB图像上使用少样本和零样本学习。我们使用人在回路反馈从BDD-A数据集中收集并完善高质量标题，然后微调LLaVA以使视觉感知与以注意力为中心的场景理解保持一致。我们的方法整合了底层线索和自上而下的上下文（例如，路线语义、风险预期），实现了基于语言的注视行为描述。我们在不同的训练机制（少样本和单样本）下评估性能，并引入了特定领域的指标进行语义对齐和响应多样性评估。结果表明，我们的微调模型在注意力转移检测和可解释性方面优于通用视觉语言模型。据我们所知，这是首次尝试用自然语言生成驾驶员视觉注意力分配和转移预测的研究之一，为自动驾驶中的可解释AI提供了新方向。我们的方法为行为预测、人机协作和多智能体协调等下游任务提供了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driver visual attention prediction is a critical task in autonomous drivingand human-computer interaction (HCI) research. Most prior studies focus onestimating attention allocation at a single moment in time, typically usingstatic RGB images such as driving scene pictures. In this work, we propose avision-language framework that models the changing landscape of drivers' gazethrough natural language, using few-shot and zero-shot learning on single RGBimages. We curate and refine high-quality captions from the BDD-A dataset usinghuman-in-the-loop feedback, then fine-tune LLaVA to align visual perceptionwith attention-centric scene understanding. Our approach integrates bothlow-level cues and top-down context (e.g., route semantics, risk anticipation),enabling language-based descriptions of gaze behavior. We evaluate performanceacross training regimes (few shot, and one-shot) and introduce domain-specificmetrics for semantic alignment and response diversity. Results show that ourfine-tuned model outperforms general-purpose VLMs in attention shift detectionand interpretability. To our knowledge, this is among the first attempts togenerate driver visual attention allocation and shifting predictions in naturallanguage, offering a new direction for explainable AI in autonomous driving.Our approach provides a foundation for downstream tasks such as behaviorforecasting, human-AI teaming, and multi-agent coordination.</description>
      <author>example@mail.com (Kaiser Hamid, Khandakar Ashrafi Akbar, Nade Liang)</author>
      <guid isPermaLink="false">2508.05852v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging transfer learning for accurate estimation of ionic migration barriers in solids</title>
      <link>http://arxiv.org/abs/2508.06436v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络的迁移学习方法，用于准确预测材料中的离子迁移势垒，显著提高了预测精度，为电池等应用中的材料发现提供了有力工具。&lt;h4&gt;背景&lt;/h4&gt;离子迁移率决定电池、燃料电池和电化学传感器等应用的性能表现，它与迁移势垒呈指数关系，而迁移势垒难以测量和计算。以往方法依赖不精确描述符，缺乏可推广的预测模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效准确地预测不同材料中迁移势垒的方法，促进高性能离子导体的发现。&lt;h4&gt;方法&lt;/h4&gt;构建基于图神经网络的架构，采用迁移学习策略：先在七种不同体性质上预训练模型(MPT)，然后修改模型以分类结构中的不同迁移路径，最后在619个手动整理的密度泛函理论计算数据集上进行微调(FT)。&lt;h4&gt;主要发现&lt;/h4&gt;最佳性能的FT模型(MODEL-3)在测试集上达到0.703的R²得分和0.261 eV的平均绝对误差，显著优于传统方法；能区分结构中的不同迁移路径，具有优秀的跨成分和化学性质的泛化能力；作为分类器，在识别'良好'离子导体方面达到80%准确率和82.8%精确率。&lt;h4&gt;结论&lt;/h4&gt;有效利用迁移学习策略和架构修改可进行快速准确的迁移势垒预测，对电池材料发现和数据稀缺材料性质预测具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;离子迁移率决定了电池、燃料电池和电化学传感器等应用的性能表现，并与迁移势垒呈指数相关，而迁移势垒是一个难以测量/计算的量。以往识别高离子迁移率材料的方法依赖于不精确的描述符，因为缺乏可推广的模型来预测迁移势垒。本研究提出了一种基于图神经网络的架构，利用迁移学习原理来高效准确地预测不同材料中的迁移势垒。我们在七种不同体性质上预训练模型，修改模型以分类结构中的不同迁移路径，并在包含619个迁移势垒数据点(使用密度泛函理论计算)的数据集上进行微调。我们的最佳性能模型(MODEL-3)在预测准确性上比传统方法有显著提高，测试集上R²得分为0.703，平均绝对误差为0.261 eV。MODEL-3能区分结构中的不同迁移路径，并在嵌入物成分和化学性质方面表现出优秀的泛化能力。作为分类器，它在识别'良好'离子导体方面达到80%准确率和82.8%精确率。因此，我们的工作展示了有效利用迁移学习策略进行快速准确迁移势垒预测的方法，对电池材料发现和其他数据稀缺材料性质预测具有重要价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ionic mobility determines the rate performance of several applications, suchas batteries, fuel cells, and electrochemical sensors and is exponentiallydependent on the migration barrier ($E_m$), a difficult to measure/calculatequantity. Previous approaches to identify materials with high ionic mobilityhave relied on imprecise descriptors given the lack of generalizable models topredict $E_m$. Here, we present a graph neural network based architecture thatleverages principles of transfer learning to efficiently and accurately predict$E_m$ across a diverse set of materials. We use a model pre-trainedsimultaneously on seven distinct bulk properties (labeled MPT), modify the MPTmodel to classify different migration pathways in a structure, and fine-tune(FT) on a manually-curated literature-derived dataset of 619 $E_m$ data pointscalculated with density functional theory. Importantly, our best-performing FTmodel (labeled MODEL-3) demonstrates substantial improvements in predictionaccuracy compared to classical machine learning methods, graph models trainedfrom scratch, and a universal machine learned interatomic potential, with aR$^2$ score of 0.703 and a mean absolute error of 0.261 eV on the test set.Notably, MODEL-3 is able to distinguish different migration pathways within astructure and also demonstrates excellent ability to generalize acrossintercalant compositions and chemistries. As a classifier, MODEL-3 exhibits80\% accuracy and 82.8\% precision in identifying materials that are `good'ionic conductors (i.e., structures with $E_m &lt;$0.65~eV). Thus, our workdemonstrates the effective use of FT strategies and architectural modificationsnecessary for making swift and accurate $E_m$ predictions, which will be usefulfor materials discovery in batteries and for predicting other data-scarcematerial properties.</description>
      <author>example@mail.com (Reshma Devi, Keith T. Butler, Gopalakrishnan Sai Gautam)</author>
      <guid isPermaLink="false">2508.06436v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors</title>
      <link>http://arxiv.org/abs/2508.06257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为GTMancer的图变换器框架，用于多组学癌症亚型分类，通过对比学习和双重注意力机制有效整合多组学数据，在七个真实世界癌症数据集上展现出优于现有最先进算法的性能。&lt;h4&gt;背景&lt;/h4&gt;整合多组学数据可通过数据驱动分析理解复杂生物过程，特别是癌症；图神经网络在利用生物数据关系结构方面表现出色；现有方法常忽略异构组学间的复杂耦合，限制了它们解决癌症亚型异质性的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在癌症亚型分类中的局限性，提出一种能够更好整合多组学数据并捕捉癌症亚型异质性的框架。&lt;h4&gt;方法&lt;/h4&gt;提出GTMancer框架，基于GNN优化问题并扩展到复杂多组学数据；利用对比学习将多组学数据嵌入统一语义空间；在统一空间中展开多图优化问题；引入双重注意力系数集合捕获多组学数据内部和之间的结构图先验，使全局组学信息指导个体组学表示的精炼。&lt;h4&gt;主要发现&lt;/h4&gt;在七个真实世界癌症数据集上的实验表明，GTMancer优于现有的最先进算法。&lt;h4&gt;结论&lt;/h4&gt;GTMancer框架有效解决了多组学整合和癌症亚型分类中的关键挑战，为精准肿瘤学提供了新工具。&lt;h4&gt;翻译&lt;/h4&gt;通过数据驱动分析整合多组学数据可以全面理解各种疾病特别是癌症背后的复杂生物过程。图神经网络最近展现出了利用生物数据中关系结构的显著能力，推动了多组学整合在癌症亚型分类中的进展。现有方法常常忽略异构组学之间的复杂耦合，限制了它们解决对精准肿瘤学至关重要的细微癌症亚型异质性的能力。为解决这些局限性，我们提出一个名为GTMancer（用于多组学癌症亚型分类的图变换器）的框架。该框架建立在GNN优化问题基础上，并将其应用扩展到复杂多组学数据。具体而言，我们的方法利用对比学习将多组学数据嵌入统一的语义空间。我们在该统一空间中展开多图优化问题，并引入双重注意力系数集合来捕获多组学数据内部和之间的结构图先验。这种方法使全局组学信息能够指导个体组学表示的精炼。在七个真实世界癌症数据集上的实证实验表明，GTMancer优于现有的最先进算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating multi-omics datasets through data-driven analysis offers acomprehensive understanding of the complex biological processes underlyingvarious diseases, particularly cancer. Graph Neural Networks (GNNs) haverecently demonstrated remarkable ability to exploit relational structures inbiological data, enabling advances in multi-omics integration for cancersubtype classification. Existing approaches often neglect the intricatecoupling between heterogeneous omics, limiting their capacity to resolve subtlecancer subtype heterogeneity critical for precision oncology. To address theselimitations, we propose a framework named Graph Transformer for Multi-omicsCancer Subtype Classification (GTMancer). This framework builds upon the GNNoptimization problem and extends its application to complex multi-omics data.Specifically, our method leverages contrastive learning to embed multi-omicsdata into a unified semantic space. We unroll the multiplex graph optimizationproblem in that unified space and introduce dual sets of attention coefficientsto capture structural graph priors both within and among multi-omics data. Thisapproach enables global omics information to guide the refining of therepresentations of individual omics. Empirical experiments on seven real-worldcancer datasets demonstrate that GTMancer outperforms existing state-of-the-artalgorithms.</description>
      <author>example@mail.com (Jielong Lu, Zhihao Wu, Jiajun Yu, Jiajun Bu, Haishuai Wang)</author>
      <guid isPermaLink="false">2508.06257v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor</title>
      <link>http://arxiv.org/abs/2508.06177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at 28th RoboCup International Symposium, Salvador, Brasil&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种创新的机器人定位框架，利用地板特性和图卷积网络实现高精度定位&lt;h4&gt;背景&lt;/h4&gt;机器人导航中的精确定位是一个基本挑战，传统方法如基于Lidar或QR码的系统在复杂环境中存在可扩展性和适应性限制&lt;h4&gt;目的&lt;/h4&gt;提出一种创新的定位框架，利用地板特性和基于图的表示以及图卷积网络(GCNs)来提高机器人定位的准确性和效率&lt;h4&gt;方法&lt;/h4&gt;使用图来表示地板特征，而不是比较单个图像特征，实现更准确（误差0.64厘米）和更高效的机器人定位&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在每一帧中成功解决机器人被绑架的问题，无需复杂的过滤过程&lt;h4&gt;结论&lt;/h4&gt;这些进步为机器人在各种环境中的导航开辟了新的可能性&lt;h4&gt;翻译&lt;/h4&gt;准确的定位代表了机器人导航中的一个基本挑战。传统方法，如基于Lidar或QR码的系统，在复杂环境中存在固有的可扩展性和适应性限制。在这项工作中，我们提出了一种创新的定位框架，利用地板特性，采用基于图的表示和图卷积网络(GCNs)。我们的方法使用图来表示地板特征，这有助于比比较单个图像特征更准确（0.64厘米误差）和更高效地定位机器人。此外，这种方法能够在每一帧中成功解决机器人被绑架的问题，而无需复杂的过滤过程。这些进步为机器人在各种环境中的导航开辟了新的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localization represents a fundamental challenge in  robotic navigation. Traditional methodologies, such as Lidar or QR-code basedsystems, suffer from inherent scalability and adaptability con straints,particularly in complex environments. In this work, we propose  an innovative localization framework that harnesses flooring characteris ticsby employing graph-based representations and Graph Convolutional  Networks (GCNs). Our method uses graphs to represent floor features,  which helps localize the robot more accurately (0.64cm error) and more  efficiently than comparing individual image features. Additionally, this  approach successfully addresses the kidnapped robot problem in every  frame without requiring complex filtering processes. These advancements  open up new possibilities for robotic navigation in diverse environments.</description>
      <author>example@mail.com (Dominik Brämer, Diana Kleingarn, Oliver Urbann)</author>
      <guid isPermaLink="false">2508.06177v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification</title>
      <link>http://arxiv.org/abs/2508.06118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于集成方法的图表示方法，用于功能磁共振成像数据的脑状态分类。通过多个基础机器学习模型构建图，边的权重反映认知状态间后验概率差异，在七个认知任务中实现了97.07%到99.74%的高分类准确率，优于传统相关图方法。&lt;h4&gt;背景&lt;/h4&gt;基于神经影像数据理解和分类人类认知脑状态是神经科学中最具挑战性的问题之一，主要由于信号具有高维度和内在噪声特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于集成方法的图表示方法，用于功能磁共振成像(fMRI)数据的二进制脑状态分类任务。&lt;h4&gt;方法&lt;/h4&gt;通过利用多个基础机器学习模型构建图，每条边的权重反映两个认知状态间后验概率差异（取值范围[-1, 1]），编码对给定状态的置信度。将方法应用于人类连接组计划的七个认知任务，仅使用图的平均入射边权重作为特征，并与基于经典相关的图在图神经网络分类任务中进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;在所有实验中，集成图获得了最高的分类准确率。结果表明集成图传递了更丰富的拓扑信息并增强了脑状态区分能力。&lt;h4&gt;结论&lt;/h4&gt;该方法保留了fMRI图表示的边级可解释性，可适应多类和回归任务，并可扩展到其他神经成像模态和病理状态分类。&lt;h4&gt;翻译&lt;/h4&gt;理解和分类基于神经影像数据的人类认知脑状态仍然是神经科学中最重要且最具挑战性的问题之一，这归因于信号的高维度和内在噪声。在这项工作中，我们提出了一种基于集成方法的图表示方法，用于功能磁共振成像(fMRI)数据的二进制脑状态分类任务。我们的方法通过利用多个基础机器学习模型构建图：每条边的权重反映了两个认知状态之间后验概率的差异，产生范围在[-1, 1]内的值，编码了对给定状态的置信度。我们将这种方法应用于人类连接组计划(HCP 1200受试者发布)中的七个认知任务，包括工作记忆、赌博、运动活动、语言、社会认知、关系处理和情绪处理。仅使用图的平均入射边权重作为特征，一个简单的逻辑回归分类器实现了97.07%到99.74%的平均准确率。我们还在图神经网络(GNN)的分类任务中将我们的集成图与基于经典相关的图进行了比较。在所有实验中，集成图获得了最高的分类准确率。这些结果表明集成图传递了更丰富的拓扑信息并增强了脑状态区分能力。我们的方法保留了fMRI图表示的边级可解释性，可适应多类和回归任务，并可扩展到其他神经成像模态和病理状态分类。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and classifying human cognitive brain states based onneuroimaging data remains one of the foremost and most challenging problems inneuroscience, owing to the high dimensionality and intrinsic noise of thesignals. In this work, we propose an ensemble-based graph representation methodof functional magnetic resonance imaging (fMRI) data for the task of binarybrain-state classification. Our method builds the graph by leveraging multiplebase machine-learning models: each edge weight reflects the difference inposterior probabilities between two cognitive states, yielding values in therange [-1, 1] that encode confidence in a given state. We applied this approachto seven cognitive tasks from the Human Connectome Project (HCP 1200 SubjectRelease), including working memory, gambling, motor activity, language, socialcognition, relational processing, and emotion processing. Using only the meanincident edge weights of the graphs as features, a simple logistic-regressionclassifier achieved average accuracies from 97.07% to 99.74%. We also comparedour ensemble graphs with classical correlation-based graphs in a classificationtask with a graph neural network (GNN). In all experiments, the highestclassification accuracy was obtained with ensemble graphs. These resultsdemonstrate that ensemble graphs convey richer topological information andenhance brain-state discrimination. Our approach preserves edge-levelinterpretability of the fMRI graph representation, is adaptable to multiclassand regression tasks, and can be extended to other neuroimaging modalities andpathological-state classification.</description>
      <author>example@mail.com (Daniil Vlasenko, Vadim Ushakov, Alexey Zaikin, Denis Zakharov)</author>
      <guid isPermaLink="false">2508.06118v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2</title>
      <link>http://arxiv.org/abs/2508.06091v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文解决了图神经网络逻辑表达能力的一个开放性问题，证明了aggregate-combine-readout GNNs的逻辑表达能力严格超过了C2逻辑。&lt;h4&gt;背景&lt;/h4&gt;近年来，研究者们对通过逻辑语言理解图神经网络(GNNs)的表达能力越来越感兴趣。Barceló等人(2020)的开创性研究表明，分级模态逻辑(或C2逻辑的守护片段)刻画了aggregate-combine GNNs的逻辑表达能力。他们将'全C2是否刻画了aggregate-combine-readout GNNs的逻辑表达能力'作为一个'具有挑战性的开放问题'提出，尽管有多次尝试，这个问题仍未解决。&lt;h4&gt;目的&lt;/h4&gt;解决Barceló等人提出的开放性问题，即确定全C2逻辑是否刻画了aggregate-combine-readout GNNs的逻辑表达能力。&lt;h4&gt;方法&lt;/h4&gt;通过证明的方法，展示了aggregate-combine-readout GNNs的逻辑表达能力严格超过了C2逻辑。这一结果在无向图和有向图上都成立。&lt;h4&gt;主要发现&lt;/h4&gt;aggregate-combine-readout GNNs的逻辑表达能力严格超过了C2逻辑，无论是在无向图还是有向图上都是如此。&lt;h4&gt;结论&lt;/h4&gt;解决了GNNs逻辑表达能力领域的一个重要开放性问题，同时也为无穷逻辑的表达能力提供了纯逻辑上的见解。&lt;h4&gt;翻译&lt;/h4&gt;近年来，人们越来越有兴趣通过逻辑语言来理解图神经网络(GNNs)的表达能力。这项研究由Barceló等人(2020)的一个有影响力的结果开创，他们证明了分级模态逻辑(或C2逻辑的守护片段)刻画了aggregate-combine GNNs的逻辑表达能力。他们留下了一个'具有挑战性的开放问题'，即全C2是否刻画了aggregate-combine-readout GNNs的逻辑表达能力。尽管有多次尝试，这个问题仍未得到解决。在本文中，我们通过证明aggregate-combine-readout GNNs的逻辑表达能力严格超过C2，解决了上述开放问题。这一结果在无向图和有向图上都成立。除了对GNNs的启示外，我们的工作还为无穷逻辑的表达能力提供了纯逻辑上的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, there has been growing interest in understanding theexpressive power of graph neural networks (GNNs) by relating them to logicallanguages. This research has been been initialised by an influential result ofBarcel\'o et al. (2020), who showed that the graded modal logic (or a guardedfragment of the logic C2), characterises the logical expressiveness ofaggregate-combine GNNs. As a ``challenging open problem'' they left thequestion whether full C2 characterises the logical expressiveness ofaggregate-combine-readout GNNs. This question has remained unresolved despiteseveral attempts. In this paper, we solve the above open problem by provingthat the logical expressiveness of aggregate-combine-readout GNNs strictlyexceeds that of C2. This result holds over both undirected and directed graphs.Beyond its implications for GNNs, our work also leads to purely logicalinsights on the expressive power of infinitary logics.</description>
      <author>example@mail.com (Stan P Hauke, Przemysław Andrzej Wałęga)</author>
      <guid isPermaLink="false">2508.06091v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>ProvX: Generating Counterfactual-Driven Attack Explanations for Provenance-Based Detection</title>
      <link>http://arxiv.org/abs/2508.06073v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ProvX是一种基于溯源图的GNN安全模型解释框架，通过反事实解释逻辑和连续优化方法定位关键图结构，提高了解释的精确性和稳定性，实验表明其能有效增强模型对抗对抗性攻击的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;基于溯源图的入侵检测系统用于防御高级持续性威胁，图神经网络(GNN)在检测这些威胁方面表现出色，但其黑盒性质限制了广泛应用。&lt;h4&gt;目的&lt;/h4&gt;提出ProvX框架，解决GNN安全模型缺乏可验证解释的问题，为安全分析师提供模型预测的证据和解释。&lt;h4&gt;方法&lt;/h4&gt;ProvX引入反事实解释逻辑，寻找图中被预测为恶意的最小结构子集；创新地将离散搜索问题转化为连续优化任务；采用分阶段固化策略提高解释精确性和稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;ProvX能定位与真实世界攻击高度相关的关键图结构，平均解释必要性达51.59%，优于当前最先进的解释器；其解释结果可指导模型优化，增强对抗对抗性攻击的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;ProvX有效解决了GNN安全模型的可解释性问题，并通过闭环检测-解释-反馈框架提升了模型性能，为安全分析提供了有价值的工具。&lt;h4&gt;翻译&lt;/h4&gt;基于溯源图的入侵检测系统被部署在主机上以防御日益严重的高级持续性威胁。使用图神经网络检测这些威胁已成为研究重点并表现出色。然而，GNN安全模型的广泛应用受到其固有黑盒性质的限制，因为它们无法为安全分析师提供模型预测的可验证解释或关于模型判断与现实世界攻击关系的任何证据。为解决这一挑战，我们提出了ProvX，一个用于解释基于溯源图的GNN安全模型的有效解释框架。ProvX引入反事实解释逻辑，寻找图中被预测为恶意的最小结构子集，当扰动时可以颠覆模型的原始预测。我们创新地将寻找关键子图的离散搜索问题转化为由预测翻转和距离最小化双重目标引导的连续优化任务。此外，还采用分阶段固化策略提高解释的精确性和稳定性。我们在权威数据集上对ProvX进行了广泛评估。实验结果表明ProvX能定位与真实世界攻击高度相关的关键图结构，平均解释必要性达51.59%，这些指标优于当前最先进的解释器。此外，我们探索并提供了闭环检测-解释-反馈增强框架的初步验证，实验证明ProvX的解释结果可以指导模型优化，有效增强其对对抗性攻击的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Provenance graph-based intrusion detection systems are deployed on hosts todefend against increasingly severe Advanced Persistent Threat. Using GraphNeural Networks to detect these threats has become a research focus and hasdemonstrated exceptional performance. However, the widespread adoption ofGNN-based security models is limited by their inherent black-box nature, asthey fail to provide security analysts with any verifiable explanations formodel predictions or any evidence regarding the model's judgment in relation toreal-world attacks. To address this challenge, we propose ProvX, an effectiveexplanation framework for exlaining GNN-based security models on provenancegraphs. ProvX introduces counterfactual explanation logic, seeking the minimalstructural subset within a graph predicted as malicious that, when perturbed,can subvert the model's original prediction. We innovatively transform thediscrete search problem of finding this critical subgraph into a continuousoptimization task guided by a dual objective of prediction flipping anddistance minimization. Furthermore, a Staged Solidification strategy isincorporated to enhance the precision and stability of the explanations. Weconducted extensive evaluations of ProvX on authoritative datasets. Theexperimental results demonstrate that ProvX can locate critical graphstructures that are highly relevant to real-world attacks and achieves anaverage explanation necessity of 51.59\%, with these metrics outperformingcurrent SOTA explainers. Furthermore, we explore and provide a preliminaryvalidation of a closed-loop Detection-Explanation-Feedback enhancementframework, demonstrating through experiments that the explanation results fromProvX can guide model optimization, effectively enhancing its robustnessagainst adversarial attacks.</description>
      <author>example@mail.com (Weiheng Wu, Wei Qiao, Teng Li, Yebo Feng, Zhuo Ma, Jianfeng Ma, Yang Liu)</author>
      <guid isPermaLink="false">2508.06073v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity</title>
      <link>http://arxiv.org/abs/2508.06034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted tp CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自适应异构图神经网络（AHGNN）来解决异构图建模中的异质性挑战，通过考虑跳数和元路径特定的异质性分布，并使用从粗到细的注意力机制整合不同语义空间的信息。&lt;h4&gt;背景&lt;/h4&gt;异构图在实际场景中很常见且通常表现出异质性，但大多数现有研究要么只关注异质性，要么只关注异质性，忽略了实际应用中异质性HG的普遍存在，导致性能下降。&lt;h4&gt;目的&lt;/h4&gt;识别建模异质性HG的两个主要挑战，并提出一种能有效处理这些挑战的自适应异构图神经网络（AHGNN）。&lt;h4&gt;方法&lt;/h4&gt;AHGNN采用异质性感知卷积，考虑跳数和元路径特定的异质性分布，并使用从粗到细的注意力机制整合来自不同语义空间的消息，过滤噪声并强调信息信号。&lt;h4&gt;主要发现&lt;/h4&gt;在七个真实世界图和二十个基线的实验中，AHGNN表现出优越的性能，特别是在高异质性的情况下。&lt;h4&gt;结论&lt;/h4&gt;通过考虑异质性分布和语义信息的多样性，AHGNN能有效处理异质性HG，在高异质性情况下表现出色。&lt;h4&gt;翻译&lt;/h4&gt;异构图（HGs）在实际场景中很常见，并且通常表现出异质性。然而，大多数现有研究要么只关注异质性，要么只关注异质性，忽略了实际应用中异质性HG的普遍存在。这种忽视导致了它们的性能下降。在这项工作中，我们首先确定了建模异质性HG的两个主要挑战：（1）跳数和元路径之间异质性分布的变化；（2）不同元路径间语义信息的复杂且通常由异质性驱动的多样性。然后，我们提出了自适应异构图神经网络（AHGNN）来解决这些挑战。AHGNN采用了一种异质性感知卷积，考虑了跳数和元路径特定的异质性分布。然后，它使用从粗到细的注意力机制整合来自不同语义空间的消息，过滤掉噪声并强调信息信号。在七个真实世界图和二十个基线的实验中，证明了AHGNN的优越性能，特别是在高异质性的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous graphs (HGs) are common in real-world scenarios and oftenexhibit heterophily. However, most existing studies focus on eitherheterogeneity or heterophily in isolation, overlooking the prevalence ofheterophilic HGs in practical applications. Such ignorance leads to theirperformance degradation. In this work, we first identify two main challenges inmodeling heterophily HGs: (1) varying heterophily distributions across hops andmeta-paths; (2) the intricate and often heterophily-driven diversity ofsemantic information across different meta-paths. Then, we propose the AdaptiveHeterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNNemploys a heterophily-aware convolution that accounts for heterophilydistributions specific to both hops and meta-paths. It then integrates messagesfrom diverse semantic spaces using a coarse-to-fine attention mechanism, whichfilters out noise and emphasizes informative signals. Experiments on sevenreal-world graphs and twenty baselines demonstrate the superior performance ofAHGNN, particularly in high-heterophily situations.</description>
      <author>example@mail.com (Qin Chen, Guojie Song)</author>
      <guid isPermaLink="false">2508.06034v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Functional Connectivity Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.05786v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 5 figures, 24 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种受脑成像多模态分析启发的图神经网络框架，通过结合结构信息和基于持久图同调的功能连接性模块，捕获网络的局部和全局特征，形成功能连接性图神经网络。实验证明该方法在图级分类任务中优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;现实世界的网络通常受益于捕捉局部和全局交互。受脑成像多模态分析的启发，其中结构连接性和功能连接性提供了网络组织的互补视角。&lt;h4&gt;目的&lt;/h4&gt;提出一个图神经网络框架，将脑成像多模态分析方法推广到其他领域。&lt;h4&gt;方法&lt;/h4&gt;引入了一种基于持久图同调的功能连接性模块，用于捕获全局拓扑特征。与结构信息相结合，形成了一种称为功能连接性图神经网络的多模态架构。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与现有方法相比，该方法取得了持续的性能提升。&lt;h4&gt;结论&lt;/h4&gt;受大脑启发的表示在不同网络图级分类中具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;现实世界的网络通常受益于捕捉局部和全局交互。受脑成像多模态分析的启发，其中结构连接性和功能连接性提供了网络组织的互补视角，我们提出了一个图神经网络框架，将这种方法推广到其他领域。我们的方法引入了一种基于持久图同调的功能连接性模块，用于捕获全局拓扑特征。与结构信息相结合，这形成了一种称为功能连接性图神经网络的多模态架构。实验表明，与现有方法相比，该方法取得了持续的性能提升，证明了受大脑启发的表示在不同网络图级分类中的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world networks often benefit from capturing both local and globalinteractions. Inspired by multi-modal analysis in brain imaging, wherestructural and functional connectivity offer complementary views of networkorganization, we propose a graph neural network framework that generalizes thisapproach to other domains. Our method introduces a functional connectivityblock based on persistent graph homology to capture global topologicalfeatures. Combined with structural information, this forms a multi-modalarchitecture called Functional Connectivity Graph Neural Networks. Experimentsshow consistent performance gains over existing methods, demonstrating thevalue of brain-inspired representations for graph-level classification acrossdiverse networks.</description>
      <author>example@mail.com (Yang Li, Luopeiwen Yi, Tananun Songdechakraiwut)</author>
      <guid isPermaLink="false">2508.05786v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics</title>
      <link>http://arxiv.org/abs/2508.05724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了一种将物理定律表示和分析为加权知识图的新框架，通过构建物理方程数据库、开发增强图表示方法和训练图注意力网络，实现了高效的物理方程链接预测，并发现了物理学的宏观结构和跨领域关系。&lt;h4&gt;背景&lt;/h4&gt;物理定律传统上以方程形式表示，但缺乏系统化的知识组织方式。研究团队构建了一个包含659个不同物理方程的数据库，经过严格的语义清洗解决符号歧义，最终形成400个高级物理方程的语料库。&lt;h4&gt;目的&lt;/h4&gt;开发一种增强的图表示方法，其中物理概念和方程都是节点，通过加权的方程间桥接连接；使用客观定义的权重（变量重叠的标准化度量、物理信息重要性评分和计量学数据）；训练图注意力网络进行链接预测。&lt;h4&gt;方法&lt;/h4&gt;构建物理方程数据库并进行语义清洗；开发增强的图表示方法，物理概念和方程作为节点，通过加权的方程间桥接连接；使用变量重叠、物理信息重要性和计量学数据定义客观权重；训练图注意力网络进行链接预测。&lt;h4&gt;主要发现&lt;/h4&gt;模型自主发现了物理学的已知宏观结构，识别出电磁学和统计力学之间的强概念轴；识别出作为多个物理领域间关键桥梁的中心枢纽方程；模型生成稳定的、计算得出的跨领域关系假设，识别已知原理并建议新的数学类比供进一步理论研究。&lt;h4&gt;结论&lt;/h4&gt;该框架可以生成数百个此类假设， enabling 创建专门的数据集用于针对特定物理学子领域的定向分析；代码和数据可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;这项工作引入了一种将物理定律表示和分析为加权知识图的新框架。我们构建了一个包含659个不同物理方程的数据库，经过严格的语义清洗解决符号歧义，最终形成400个高级物理方程的语料库。我们开发了一种增强的图表示方法，其中物理概念和方程都是节点，通过加权的方程间桥接连接。这些权重使用变量重叠的标准化度量、物理信息重要性评分和计量学数据进行客观定义。我们训练了一个图注意力网络(GAT)进行链接预测，在五次独立运行中测试AUC达到0.9742 +/- 0.0018，显著优于经典启发式方法（最佳基线AUC：0.9487）和已建立的GNN架构如图神经网络（AUC：0.9504，p = 0.029）。统计检验确认了所有比较的显著性（p &lt; 0.05），比最佳基线提高了2.7%。我们的分析揭示了三个关键发现：(i) 模型自主发现了物理学的已知宏观结构，识别出电磁学和统计力学之间的强概念轴。(ii) 它识别出作为多个物理领域间关键桥梁的中心枢纽方程。(iii) 模型生成稳定的、计算得出的跨领域关系假设，识别已知原理并建议新的数学类比供进一步理论研究。该框架可以生成数百个此类假设， enabling 创建专门的数据集用于针对特定物理学子领域的定向分析。代码和数据可在https://github.com/kingelanci/graphysics获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work introduces a novel framework for representing and analyzingphysical laws as a weighted knowledge graph. We constructed a database of 659distinct physical equations, subjected to rigorous semantic cleaning to resolvenotational ambiguities, resulting in a corpus of 400 advanced physicsequations. We developed an enhanced graph representation where both physicalconcepts and equations are nodes, connected by weighted inter-equation bridges.These weights are objectively defined using normalized metrics for variableoverlap, physics-informed importance scores, and bibliometric data. A GraphAttention Network (GAT) was trained for link prediction, achieving a test AUCof 0.9742 +/- 0.0018 across five independent runs, significantly outperformingboth classical heuristics (best baseline AUC: 0.9487) and established GNNarchitectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testingconfirmed significance of all comparisons (p &lt; 0.05), with 2.7% improvementover the best baseline. Our analysis reveals three key findings: (i) The modelautonomously rediscovers the known macroscopic structure of physics,identifying strong conceptual axes between Electromagnetism and StatisticalMechanics. (ii) It identifies central hub equations that serve as criticalbridges between multiple physical domains. (iii) The model generates stable,computationally-derived hypotheses for cross-domain relationships, identifyingboth known principles and suggesting novel mathematical analogies for furthertheoretical investigation. The framework can generate hundreds of suchhypotheses, enabling the creation of specialized datasets for targeted analysisof specific physics subfields. Code and data available athttps://github.com/kingelanci/graphysics</description>
      <author>example@mail.com (Massimiliano Romiti)</author>
      <guid isPermaLink="false">2508.05724v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation</title>
      <link>http://arxiv.org/abs/2508.06452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TRUST，一种新型无监督域适应方法，利用语言模态的鲁棒性来指导视觉模型适应复杂域偏移。&lt;h4&gt;背景&lt;/h4&gt;现有无监督域适应方法在处理经典域偏移（如合成到真实）时表现出色，但在处理复杂域偏移（如地理偏移）时效果不佳，因为这些情况下不同域之间的背景和物体外观存在显著差异。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理复杂域偏移的无监督域适应方法，利用语言模态的鲁棒性来提高视觉模型在目标域上的性能。&lt;h4&gt;方法&lt;/h4&gt;TRUST方法通过从图像标题生成伪标签，并使用归一化的CLIP相似度分数估计这些伪标签的不确定性；然后利用估计的不确定性重新加权分类损失，减轻低质量标题带来的错误标签影响；此外，还提出多模态软对比学习损失，对齐视觉和语言特征空间，利用标题引导视觉模型在目标图像上的对比训练。&lt;h4&gt;主要发现&lt;/h4&gt;语言模态可以帮助适应过程，对复杂域偏移表现出更强的鲁棒性；提出的多模态软对比学习损失避免了需要确定正负样本对的问题，这在无监督域适应设置中非常关键。&lt;h4&gt;结论&lt;/h4&gt;TRUST方法在经典域偏移（DomainNet）和复杂域偏移（GeoNet）上均超越了先前方法，设立了新的最先进水平。&lt;h4&gt;翻译&lt;/h4&gt;最近的无监督域适应方法在处理经典域偏移（如合成到真实）方面已显示出巨大成功，但在复杂偏移（如地理偏移）下仍然表现不佳，在这种情况下，不同域之间的背景和物体外观存在显著差异。先前的工作表明，语言模态可以在适应过程中提供帮助，对这类复杂偏移表现出更强的鲁棒性。在本文中，我们引入了TRUST，一种新型无监督域适应方法，它利用语言模态的鲁棒性来指导视觉模型的适应。TRUST从图像标题为目标样本生成伪标签，并引入了一种新的不确定性估计策略，使用归一化的CLIP相似度分数来估计生成伪标签的不确定性。然后使用这种估计的不确定性对分类损失进行重新加权，减轻从低质量标题获得的错误伪标签的不利影响。为了进一步增强视觉模型的鲁棒性，我们提出了一种多模态软对比学习损失，通过利用标题来引导视觉模型在目标图像上的对比训练，从而对齐视觉和语言特征空间。在我们的对比损失中，每对图像同时作为正样本对和负样本对，其特征表示根据其标题的相似度比例被吸引和排斥。这种解决方案避免了难以确定正负样本对的需要，这在无监督域适应设置中至关重要。我们的方法超越了先前的方法，在经典域偏移（DomainNet）和复杂域偏移（GeoNet）上设立了新的最先进水平。代码将在接受后提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent unsupervised domain adaptation (UDA) methods have shown great successin addressing classical domain shifts (e.g., synthetic-to-real), but they stillsuffer under complex shifts (e.g. geographical shift), where both thebackground and object appearances differ significantly across domains. Priorworks showed that the language modality can help in the adaptation process,exhibiting more robustness to such complex shifts. In this paper, we introduceTRUST, a novel UDA approach that exploits the robustness of the languagemodality to guide the adaptation of a vision model. TRUST generatespseudo-labels for target samples from their captions and introduces a noveluncertainty estimation strategy that uses normalised CLIP similarity scores toestimate the uncertainty of the generated pseudo-labels. Such estimateduncertainty is then used to reweight the classification loss, mitigating theadverse effects of wrong pseudo-labels obtained from low-quality captions. Tofurther increase the robustness of the vision model, we propose a multimodalsoft-contrastive learning loss that aligns the vision and language featurespaces, by leveraging captions to guide the contrastive training of the visionmodel on target images. In our contrastive loss, each pair of images acts asboth a positive and a negative pair and their feature representations areattracted and repulsed with a strength proportional to the similarity of theircaptions. This solution avoids the need for hardly determining positive andnegative pairs, which is critical in the UDA setting. Our approach outperformsprevious methods, setting the new state-of-the-art on classical (DomainNet) andcomplex (GeoNet) domain shifts. The code will be available upon acceptance.</description>
      <author>example@mail.com (Mattia Litrico, Mario Valerio Giuffrida, Sebastiano Battiato, Devis Tuia)</author>
      <guid isPermaLink="false">2508.06452v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment</title>
      <link>http://arxiv.org/abs/2508.06434v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CLIPin，一个统一的非对比插件，可无缝集成到CLIP架构中，改进多模态语义对齐并提供更强监督，增强对齐鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;大规模自然图像-文本数据集（特别是网络自动收集的）常因弱监督存在松散语义对齐问题；医学数据集虽有高跨模态相关性但内容多样性低，这些特性阻碍了CLIP模型学习鲁棒且可推广的表示能力。&lt;h4&gt;目的&lt;/h4&gt;开发CLIPin以改进多模态语义对齐，提供更强监督并增强对齐鲁棒性，解决现有数据集的语义对齐挑战。&lt;h4&gt;方法&lt;/h4&gt;设计两个共享预投影器分别用于图像和文本模态，促进对比学习和非对比学习以参数折衷方式集成；CLIPin作为即插即用组件兼容各种对比框架。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化下游任务上的广泛实验证明CLIPin作为即插即用组件具有有效性和通用性，可兼容各种对比框架。&lt;h4&gt;结论&lt;/h4&gt;CLIPin成功解决了大规模自然图像-文本数据集和医学数据集的语义对齐问题，提供了一种改进多模态表示学习的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;大规模自然图像-文本数据集，特别是那些从网络自动收集的数据集，通常因弱监督而存在松散的语义对齐问题，而医学数据集往往具有较高的跨模态相关性但较低的内容多样性。这些特性对对比语言-图像预训练（CLIP）构成了共同挑战：它们阻碍了模型学习鲁棒且可推广的表示能力。在这项工作中，我们提出了CLIPin，一个统一的非对比插件，可以无缝集成到CLIP架构中，改进多模态语义对齐，提供更强的监督并增强对齐鲁棒性。此外，分别为图像和文本模态设计了两个共享预投影器，以促进对比学习和非对比学习以参数折衷方式集成。在多样化下游任务上的广泛实验证明了CLIPin作为即插即用组件的有效性和通用性，兼容各种对比框架。代码可在https://github.com/T6Yang/CLIPin获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale natural image-text datasets, especially those automaticallycollected from the web, often suffer from loose semantic alignment due to weaksupervision, while medical datasets tend to have high cross-modal correlationbut low content diversity. These properties pose a common challenge forcontrastive language-image pretraining (CLIP): they hinder the model's abilityto learn robust and generalizable representations. In this work, we proposeCLIPin, a unified non-contrastive plug-in that can be seamlessly integratedinto CLIP-style architectures to improve multimodal semantic alignment,providing stronger supervision and enhancing alignment robustness. Furthermore,two shared pre-projectors are designed for image and text modalitiesrespectively to facilitate the integration of contrastive and non-contrastivelearning in a parameter-compromise manner. Extensive experiments on diversedownstream tasks demonstrate the effectiveness and generality of CLIPin as aplug-and-play component compatible with various contrastive frameworks. Code isavailable at https://github.com/T6Yang/CLIPin.</description>
      <author>example@mail.com (Shengzhu Yang, Jiawei Du, Shuai Lu, Weihang Zhang, Ningli Wang, Huiqi Li)</author>
      <guid isPermaLink="false">2508.06434v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Item Graph Enhancement for Multimodal Recommendation</title>
      <link>http://arxiv.org/abs/2508.06154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种改进的多模态推荐系统，解决了现有语义图中的语义缺陷问题，包括物品间协作信号建模不足和原始模态特征噪声导致的结构扭曲。通过注入协作信号、设计个性化嵌入扰动机制和双重表示对齐机制，提高了模型对噪声的鲁棒性，并在多个基准数据集上验证了有效性。&lt;h4&gt;背景&lt;/h4&gt;多模态推荐系统通过利用物品的多模态信息提高了性能，受到越来越多的关注。先前的方法通常从原始模态特征构建模态特定的物品-物品语义图，并与用户-物品交互图一起作为补充结构，以增强用户偏好学习。&lt;h4&gt;目的&lt;/h4&gt;解决现有语义图中的语义缺陷问题，包括物品间协作信号建模不足和原始模态特征噪声导致的结构扭曲，从而提高多模态推荐系统的性能。&lt;h4&gt;方法&lt;/h4&gt;1) 从交互图中提取协作信号并注入到模态特定的物品语义图中以增强语义建模；2) 设计基于模的个性化嵌入扰动机制，通过对比学习学习对噪声鲁棒的表示；3) 提出双重表示对齐机制，先使用行为表示作为锚点对齐多个语义表示，再对齐行为表示与融合语义。&lt;h4&gt;主要发现&lt;/h4&gt;现有语义图存在两个主要语义缺陷：物品间协作信号建模不足和原始模态特征噪声导致的结构扭曲，这些缺陷会损害系统性能。通过注入协作信号、设计个性化嵌入扰动机制和双重表示对齐机制，可以有效解决这些问题。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架通过增强语义建模、提高对噪声的鲁棒性和确保表示一致性，有效解决了多模态推荐系统中语义图的语义缺陷问题，在多个基准数据集上验证了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;多模态推荐系统因利用物品的多模态信息提高性能而受到越来越多的关注。先前的方法通常从原始模态特征构建模态特定的物品-物品语义图，并将它们与用户-物品交互图一起作为补充结构，以增强用户偏好学习。然而，这些语义图存在语义缺陷，包括(1)物品间协作信号建模不足，以及(2)原始模态特征中的噪声引入的结构扭曲，最终损害性能。为解决这些问题，我们首先从交互图中提取协作信号，并将它们注入到每个模态特定的物品语义图中，以增强语义建模。然后，我们设计了一种基于模的个性化嵌入扰动机制，将具有模引导的个性化强度的扰动注入嵌入中，以生成对比视图。这使得模型能够通过对比学习学习对噪声鲁棒的表示，从而减少语义图中结构噪声的影响。此外，我们提出了一种双重表示对齐机制，首先使用行为表示作为锚点，通过设计的基于锚点的InfoNCE损失对齐多个语义表示，然后通过标准InfoNCE将对齐行为表示与融合语义，以确保表示一致性。在四个基准数据集上的大量实验验证了我们框架的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal recommendation systems have attracted increasing attention fortheir improved performance by leveraging items' multimodal information. Priormethods often build modality-specific item-item semantic graphs from rawmodality features and use them as supplementary structures alongside theuser-item interaction graph to enhance user preference learning. However, thesesemantic graphs suffer from semantic deficiencies, including (1) insufficientmodeling of collaborative signals among items and (2) structural distortionsintroduced by noise in raw modality features, ultimately compromisingperformance. To address these issues, we first extract collaborative signalsfrom the interaction graph and infuse them into each modality-specific itemsemantic graph to enhance semantic modeling. Then, we design a modulus-basedpersonalized embedding perturbation mechanism that injects perturbations withmodulus-guided personalized intensity into embeddings to generate contrastiveviews. This enables the model to learn noise-robust representations throughcontrastive learning, thereby reducing the effect of structural noise insemantic graphs. Besides, we propose a dual representation alignment mechanismthat first aligns multiple semantic representations via a designed Anchor-basedInfoNCE loss using behavior representations as anchors, and then alignsbehavior representations with the fused semantics by standard InfoNCE, toensure representation consistency. Extensive experiments on four benchmarkdatasets validate the effectiveness of our framework.</description>
      <author>example@mail.com (Xiaoxiong Zhang, Xin Zhou, Zhiwei Zeng, Dusit Niyato, Zhiqi Shen)</author>
      <guid isPermaLink="false">2508.06154v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>IOCC: Aligning Semantic and Cluster Centers for Few-shot Short Text Clustering</title>
      <link>http://arxiv.org/abs/2508.06126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;IOCC是一种解决短文本聚类中语义中心识别困难的小样本对比学习方法，通过IEOT和CACL两个模块的协作，实现了聚类中心和语义中心的对齐，显著提升了聚类效果。&lt;h4&gt;背景&lt;/h4&gt;在聚类任务中，将特征空间结构化为清晰、良好分离的分布至关重要。然而，由于短文本表示的表达能力有限，传统方法难以真正识别出能够捕捉每个类别潜在语义的聚类中心，导致表示在次优方向上被优化。&lt;h4&gt;目的&lt;/h4&gt;提出IOCC，一种新颖的小样本对比学习方法，旨在实现聚类中心和语义中心之间的对齐。&lt;h4&gt;方法&lt;/h4&gt;IOCC包含两个关键模块：1)交互增强最优传输(IEOT)：将个体样本之间的语义交互整合到传统的最优传输问题中，并生成伪标签；2)中心感知对比学习(CACL)：基于这些伪标签，聚合高置信度样本来构建近似语义中心的伪中心，然后将文本表示优化到其对应的伪中心。随着训练进行，两个模块协作逐渐缩小聚类中心和语义中心之间的差距。&lt;h4&gt;主要发现&lt;/h4&gt;在八个基准数据集上的大量实验表明，IOCC优于先前的方法，在具有挑战性的生物医学数据集上实现了高达7.34%的改进，并且在聚类稳定性和效率方面也表现出色。&lt;h4&gt;结论&lt;/h4&gt;通过IOCC方法，模型能够学习到高质量的分布，从而提高聚类性能。&lt;h4&gt;翻译&lt;/h4&gt;在聚类任务中，将特征空间结构化为清晰、良好分离的分布是至关重要的。然而，由于短文本表示的表达能力有限，传统方法难以真正识别出能够捕捉每个类别潜在语义的聚类中心，导致表示在次优方向上被优化。为解决这一问题，我们提出了IOCC，一种新颖的小样本对比学习方法，旨在实现聚类中心和语义中心之间的对齐。IOCC包含两个关键模块：交互增强最优传输(IEOT)和中心感知对比学习(CACL)。具体而言，IEOT将个体样本之间的语义交互整合到传统的最优传输问题中，并生成伪标签。基于这些伪标签，我们聚合高置信度样本来构建近似语义中心的伪中心。随后，CACL将文本表示优化到其对应的伪中心。随着训练的进行，两个模块之间的协作逐渐缩小了聚类中心和语义中心之间的差距。因此，模型将学习到高质量的分布，提高聚类性能。在八个基准数据集上的大量实验表明，IOCC优于先前的方法，在具有挑战性的生物医学数据集上实现了高达7.34%的改进，并且在聚类稳定性和效率方面也表现出色。代码可在以下地址获取：https://anonymous.4open.science/r/IOCC-C438。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In clustering tasks, it is essential to structure the feature space intoclear, well-separated distributions. However, because short textrepresentations have limited expressiveness, conventional methods struggle toidentify cluster centers that truly capture each category's underlyingsemantics, causing the representations to be optimized in suboptimaldirections. To address this issue, we propose IOCC, a novel few-shotcontrastive learning method that achieves alignment between the cluster centersand the semantic centers. IOCC consists of two key modules:Interaction-enhanced Optimal Transport (IEOT) and Center-aware ContrastiveLearning (CACL). Specifically, IEOT incorporates semantic interactions betweenindividual samples into the conventional optimal transport problem, andgenerate pseudo-labels. Based on these pseudo-labels, we aggregatehigh-confidence samples to construct pseudo-centers that approximate thesemantic centers. Next, CACL optimizes text representations toward theircorresponding pseudo-centers. As training progresses, the collaboration betweenthe two modules gradually reduces the gap between cluster centers and semanticcenters. Therefore, the model will learn a high-quality distribution, improvingclustering performance. Extensive experiments on eight benchmark datasets showthat IOCC outperforms previous methods, achieving up to 7.34\% improvement onchallenging Biomedical dataset and also excelling in clustering stability andefficiency. The code is available at:https://anonymous.4open.science/r/IOCC-C438.</description>
      <author>example@mail.com (Jixuan Yin, Zhihao Yao, Wenshuai Huo, Xinmiao Yu, Xiaocheng Feng, Bo Li)</author>
      <guid isPermaLink="false">2508.06126v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2508.06115v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为SynSeg的新型弱监督方法，用于解决开放词汇场景下的语义分割挑战，通过多类别对比学习和特征协同结构实现了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;开放词汇场景下的语义分割面临语义类别范围广、粒度多样的挑战，现有弱监督方法依赖特定类别监督且不适合对比学习的特征构建，导致语义错位和性能不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为SynSeg的新型弱监督方法，以解决开放词汇场景下语义分割的挑战。&lt;h4&gt;方法&lt;/h4&gt;SynSeg采用多类别对比学习(MCCL)作为更强训练信号，结合名为特征协同结构(FSS)的新特征重建框架；MCCL策略稳健结合类别内和类别间的对齐与分离，使模型学习同一图像中不同类别间的相关性；FSS通过先验融合和语义激活图增强重建判别性特征，避免视觉编码器引入的前景偏差。&lt;h4&gt;主要发现&lt;/h4&gt;SynSeg有效提高了弱监督下的语义定位和判别能力；在基准测试中，该方法性能优于最先进方法，在VOC上高4.5%，在Context上高8.9%，在Object上高2.6%，在City上高2.0%。&lt;h4&gt;结论&lt;/h4&gt;SynSeg是一种有效的弱监督方法，能够解决开放词汇场景下语义分割的挑战，并在多个基准测试上取得最先进性能。&lt;h4&gt;翻译&lt;/h4&gt;开放词汇场景下的语义分割由于语义类别的广泛性和多样性而面临重大挑战。现有的弱监督方法通常依赖于特定类别的监督和不适用的对比学习特征构建方法，导致语义错位和性能不佳。在这项工作中，我们提出了一种新颖的弱监督方法SynSeg来解决这些挑战。SynSeg将多类别对比学习(MCCL)作为更强的训练信号，并采用名为特征协同结构(FSS)的新特征重建框架。具体来说，MCCL策略稳健地结合了类别内和类别间的对齐与分离，使模型能够学习同一图像中不同类别之间的相关性知识。此外，FSS通过先验融合和语义激活图增强为对比学习重建判别性特征，有效避免了视觉编码器引入的前景偏差。总体而言，SynSeg在弱监督下有效提高了语义定位和判别能力。在基准测试上的广泛实验表明，我们的方法优于最先进的性能。例如，SynSeg在VOC上比SOTA基线高4.5%，在Context上高8.9%，在Object上高2.6%，在City上高2.0%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation in open-vocabulary scenarios presents significantchallenges due to the wide range and granularity of semantic categories.Existing weakly-supervised methods often rely on category-specific supervisionand ill-suited feature construction methods for contrastive learning, leadingto semantic misalignment and poor performance. In this work, we propose a novelweakly-supervised approach, SynSeg, to address the challenges. SynSeg performsMulti-Category Contrastive Learning (MCCL) as a stronger training signal with anew feature reconstruction framework named Feature Synergy Structure (FSS).Specifically, MCCL strategy robustly combines both intra- and inter-categoryalignment and separation in order to make the model learn the knowledge ofcorrelations from different categories within the same image. Moreover, FSSreconstructs discriminative features for contrastive learning through priorfusion and semantic-activation-map enhancement, effectively avoiding theforeground bias introduced by the visual encoder. In general, SynSegeffectively improves the abilities in semantic localization and discriminationunder weak supervision. Extensive experiments on benchmarks demonstrate thatour method outperforms state-of-the-art (SOTA) performance. For instance,SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% onContext, 2.6\% on Object and 2.0\% on City.</description>
      <author>example@mail.com (Weichen Zhang, Kebin Liu, Fan Dang, Zhui Zhu, Xikai Sun, Yunhao Liu)</author>
      <guid isPermaLink="false">2508.06115v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration</title>
      <link>http://arxiv.org/abs/2508.06189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MA-CBP框架，一种基于多智能体异步协作的犯罪行为预测方法，通过处理实时视频流进行犯罪活动预警。&lt;h4&gt;背景&lt;/h4&gt;城市化加速导致公共场所犯罪行为对社会安全构成日益严重威胁；传统基于特征识别的异常检测方法难以捕获高级行为语义；基于大型语言模型的生成方法无法满足实时要求。&lt;h4&gt;目的&lt;/h4&gt;解决传统方法和生成方法的局限性，开发一种能够有效预测犯罪行为的实时框架。&lt;h4&gt;方法&lt;/h4&gt;提出MA-CBP框架，将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，融合相邻图像帧进行长短期上下文联合推理；构建高质量犯罪行为数据集，提供多尺度语言监督（帧级、摘要级和事件级语义注释）。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在多个数据集上实现了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;MA-CBP为城市公共安全场景中的风险预警提供了有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着城市化进程的加速，公共场所的犯罪行为对社会安全构成日益严重的威胁。基于特征识别的传统异常检测方法难以从历史信息中捕获高级行为语义，而基于大型语言模型的生成方法通常无法满足实时要求。为应对这些挑战，我们提出了MA-CBP，一种基于多智能体异步协作的犯罪行为预测框架。该框架将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，并融合相邻图像帧对长短期上下文进行联合推理。所产生的行为决策包括事件主体、地点和原因等关键要素，能够对潜在的犯罪活动进行预警。此外，我们构建了一个高质量的犯罪行为数据集，提供了多尺度语言监督，包括帧级、摘要级和事件级语义注释。实验结果表明，我们的方法在多个数据集上实现了优越的性能，为城市公共安全场景中的风险预警提供了有前景的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the acceleration of urbanization, criminal behavior in public scenesposes an increasingly serious threat to social security. Traditional anomalydetection methods based on feature recognition struggle to capture high-levelbehavioral semantics from historical information, while generative approachesbased on Large Language Models (LLMs) often fail to meet real-timerequirements. To address these challenges, we propose MA-CBP, a criminalbehavior prediction framework based on multi-agent asynchronous collaboration.This framework transforms real-time video streams into frame-level semanticdescriptions, constructs causally consistent historical summaries, and fusesadjacent image frames to perform joint reasoning over long- and short-termcontexts. The resulting behavioral decisions include key elements such as eventsubjects, locations, and causes, enabling early warning of potential criminalactivity. In addition, we construct a high-quality criminal behavior datasetthat provides multi-scale language supervision, including frame-level,summary-level, and event-level semantic annotations. Experimental resultsdemonstrate that our method achieves superior performance on multiple datasetsand offers a promising solution for risk warning in urban public safetyscenarios.</description>
      <author>example@mail.com (Cheng Liu, Daou Zhang, Tingxu Liu, Yuhan Wang, Jinyang Chen, Yuexuan Li, Xinying Xiao, Chenbo Xin, Ziru Wang, Weichao Wu)</author>
      <guid isPermaLink="false">2508.06189v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</title>
      <link>http://arxiv.org/abs/2508.06471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍GLM-4.5，一个开源的混合专家大型语言模型，具有355B总参数和32B激活参数，采用混合推理方法，在多个任务上表现优异，总体排名第3。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的发展，特别是混合专家模型(MoE)在提升模型性能方面的潜力。&lt;h4&gt;目的&lt;/h4&gt;开发一个高性能的开源大型语言模型，支持混合推理，并在代理、推理和编程任务上取得优异成绩。&lt;h4&gt;方法&lt;/h4&gt;构建一个混合专家模型(MoE)，使用23T tokens进行多阶段训练，并通过专家模型迭代和强化学习进行全面的后期训练。&lt;h4&gt;主要发现&lt;/h4&gt;GLM-4.5在TAU-Bench上得分为70.1%，在AIME 24上得分为91.0%，在SWE-bench Verified上得分为64.2%。尽管参数比一些竞争对手少，但在所有评估模型中总体排名第3，在代理基准测试中排名第2。&lt;h4&gt;结论&lt;/h4&gt;GLM-4.5是一个高性能的开源大型语言模型，通过混合推理方法和全面训练，在多个任务上取得了优异的成绩，为研究和应用提供了有价值的资源。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了GLM-4.5，一个开源的混合专家大型语言模型，具有355B总参数和32B激活参数，采用混合推理方法，支持思考模式和直接响应模式。通过23T tokens的多阶段训练和包含专家模型迭代和强化学习的全面后训练，GLM-4.5在代理、推理和编程任务上表现出色，在TAU-Bench上得分为70.1%，在AIME 24上得分为91.0%，在SWE-bench Verified上得分为64.2%。尽管参数比一些竞争对手少，但GLM-4.5在所有评估模型中总体排名第3，在代理基准测试中排名第2。我们发布了GLM-4.5(355B参数)和紧凑版本GLM-4.5-Air(106B参数)，以推动推理和代理AI系统的研究。代码、模型和更多信息可在https://github.com/zai-org/GLM-4.5获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large languagemodel with 355B total parameters and 32B activated parameters, featuring ahybrid reasoning method that supports both thinking and direct response modes.Through multi-stage training on 23T tokens and comprehensive post-training withexpert model iteration and reinforcement learning, GLM-4.5 achieves strongperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% onTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewerparameters than several competitors, GLM-4.5 ranks 3rd overall among allevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355Bparameters) and a compact version, GLM-4.5-Air (106B parameters), to advanceresearch in reasoning and agentic AI systems. Code, models, and moreinformation are available at https://github.com/zai-org/GLM-4.5.</description>
      <author>example@mail.com (GLM-4. 5 Team, :, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang)</author>
      <guid isPermaLink="false">2508.06471v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Deepfake Detection that Generalizes Across Benchmarks</title>
      <link>http://arxiv.org/abs/2508.06248v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LNCLIP-DF是一种仅微调CLIP模型0.03%参数的深度伪造检测方法，通过L2归一化和潜在空间增强实现超球面特征流形，在13个基准数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;深度伪造检测器对未见过的操纵技术的泛化能力仍然是一个实际部署的挑战。尽管许多方法通过引入显著的架构复杂性来适应基础模型，但这项工作证明可以通过参数高效的适应实现稳健泛化。&lt;h4&gt;目的&lt;/h4&gt;解决深度伪造检测器对未见过的操纵技术的泛化挑战，提出一种计算效率高且可复现的方法，证明通过针对预训练CLIP模型进行目标性、最小化的修改可以实现最先进的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了LNCLIP-DF方法，仅微调层归一化参数（占总参数的0.03%），通过使用L2归一化和潜在空间增强来强制执行超球面特征流形，在2019年至2025年间的13个基准数据集上进行了广泛评估。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在来自同一源视频的成对真实-伪造数据上进行训练对于减轻快捷学习并提高泛化能力至关重要；2) 学术数据集上的检测难度并未随时间严格增加，在较旧、多样化的数据集上训练的模型显示出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了一种计算效率高且可复现的方法，证明通过针对预训练CLIP模型进行目标性、最小化的修改可以实现最先进的泛化能力，代码将在接受后公开发布。&lt;h4&gt;翻译&lt;/h4&gt;深度伪造检测器对未见操纵技术的泛化能力仍是实际部署的挑战。尽管许多方法通过引入显著架构复杂性来适应基础模型，但本研究证明，通过对预训练的CLIP视觉编码器进行参数高效适应，可实现稳健泛化。所提出的方法LNCLIP-DF仅微调层归一化参数（占总数的0.03%），并通过使用L2归一化和潜在空间增强强制执行超球面特征流形来增强泛化能力。我们在2019年至2025年的13个基准数据集上进行了广泛评估。所提出的方法实现了最先进的性能，在平均跨数据集AUROC上优于更复杂、更近期的方法。我们的分析为该领域得出两个主要发现：1) 在来自同一源视频的成对真实-伪造数据上进行训练对于减轻快捷学习并提高泛化能力至关重要；2) 学术数据集上的检测难度并未随时间严格增加，在较旧、多样化数据集上训练的模型显示出强大的泛化能力。这项工作提供了一种计算效率高且可复现的方法，证明通过针对预训练CLIP模型进行目标性、最小化的修改可实现最先进的泛化能力。代码将在接受后公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The generalization of deepfake detectors to unseen manipulation techniquesremains a challenge for practical deployment. Although many approaches adaptfoundation models by introducing significant architectural complexity, thiswork demonstrates that robust generalization is achievable through aparameter-efficient adaptation of a pre-trained CLIP vision encoder. Theproposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters(0.03% of the total) and enhances generalization by enforcing a hypersphericalfeature manifold using L2 normalization and latent space augmentations.  We conducted an extensive evaluation on 13 benchmark datasets spanning from2019 to 2025. The proposed method achieves state-of-the-art performance,outperforming more complex, recent approaches in average cross-dataset AUROC.Our analysis yields two primary findings for the field: 1) training on pairedreal-fake data from the same source video is essential for mitigating shortcutlearning and improving generalization, and 2) detection difficulty on academicdatasets has not strictly increased over time, with models trained on older,diverse datasets showing strong generalization capabilities.  This work delivers a computationally efficient and reproducible method,proving that state-of-the-art generalization is attainable by making targeted,minimal changes to a pre-trained CLIP model. The code will be made publiclyavailable upon acceptance.</description>
      <author>example@mail.com (Andrii Yermakov, Jan Cech, Jiri Matas, Mario Fritz)</author>
      <guid isPermaLink="false">2508.06248v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures</title>
      <link>http://arxiv.org/abs/2508.06127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages,recived by ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Vertex-Refining Simplicial Complex Attack (VeSCA)的新方法，用于评估Segment Anything Model (SAM)的可转移漏洞。VeSCA仅使用SAM的编码器生成可转移对抗样本，通过参数化单纯复数表征共享脆弱区域，并通过迭代顶点细化识别对抗有效区域。实验显示，VeSCA在三个下游模型类别和五个领域特定数据集上比最先进方法提高12.7%的性能。&lt;h4&gt;背景&lt;/h4&gt;Segment Anything Model (SAM)具有零样本能力，改变了交互式分割领域，但其固有漏洞可能导致许多下游应用失败。之前的对抗攻击方法在跨领域探索共同弱点方面不足，导致可转移性有限。&lt;h4&gt;目的&lt;/h4&gt;评估SAM的可转移漏洞，开发一种能够生成高度可转移对抗样本的新方法，以揭示这些漏洞对下游应用的潜在风险。&lt;h4&gt;方法&lt;/h4&gt;提出Vertex-Refining Simplicial Complex Attack (VeSCA)，通过以下步骤实现：(1)仅使用SAM的编码器生成对抗样本；(2)通过参数化单纯复数明确表征SAM和下游模型之间的共享脆弱区域；(3)通过迭代顶点细化识别对抗有效区域内的复数；(4)引入轻量级域自适应策略桥接域差异；(5)通过随机单纯复数采样生成一致可转移的对抗样本。&lt;h4&gt;主要发现&lt;/h4&gt;VeSCA在三个下游模型类别和五个领域特定数据集上比最先进方法提高了12.7%的性能，证明了其生成可转移对抗样本的有效性。研究还强调了SAM漏洞对下游模型的显著风险。&lt;h4&gt;结论&lt;/h4&gt;SAM的漏洞对下游模型构成重大风险，开发更强大的基础模型变得至关重要。本研究为评估基础模型的脆弱性提供了新方法，并强调了提高模型鲁棒性的紧迫性。&lt;h4&gt;翻译&lt;/h4&gt;虽然Segment Anything Model (SAM)通过零样本能力改变了交互式分割，但其固有漏洞呈现单点风险，可能导致许多下游应用失败。因此，主动评估这些可转移漏洞至关重要。针对SAM的先前对抗攻击往往因跨领域共同弱点的探索不足而表现出有限的转移性。为解决这一问题，我们提出了Vertex-Refining Simplicial Complex Attack (VeSCA)，一种仅使用SAM编码器来生成可转移对抗样本的新方法。具体而言，它通过参数化单纯复数明确表征SAM与下游模型之间的共享脆弱区域来实现这一点。我们的目标是通过迭代顶点细化在对抗有效区域内识别此类复数。引入了轻量级域自适应策略，在单纯复数初始化期间使用最少参考数据来桥接域差异。最终，VeSCA通过随机单纯复数采样生成一致可转移的对抗样本。大量实验证明，VeSCA在五个领域特定数据集上的三个下游模型类别中，比最先进方法提高了12.7%的性能。我们的研究结果进一步强调了SAM漏洞对下游模型的风险，并强调了开发更强大基础模型的紧迫性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While the Segment Anything Model (SAM) transforms interactive segmentationwith zero-shot abilities, its inherent vulnerabilities present a single-pointrisk, potentially leading to the failure of numerous downstream applications.Proactively evaluating these transferable vulnerabilities is thus imperative.Prior adversarial attacks on SAM often present limited transferability due toinsufficient exploration of common weakness across domains. To address this, wepropose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method thatleverages only the encoder of SAM for generating transferable adversarialexamples. Specifically, it achieves this by explicitly characterizing theshared vulnerable regions between SAM and downstream models through aparametric simplicial complex. Our goal is to identify such complexes withinadversarially potent regions by iterative vertex-wise refinement. A lightweightdomain re-adaptation strategy is introduced to bridge domain divergence usingminimal reference data during the initialization of simplicial complex.Ultimately, VeSCA generates consistently transferable adversarial examplesthrough random simplicial complex sampling. Extensive experiments demonstratethat VeSCA achieves performance improved by 12.7% compared to state-of-the-artmethods across three downstream model categories across five domain-specificdatasets. Our findings further highlight the downstream model risks posed bySAM's vulnerabilities and emphasize the urgency of developing more robustfoundation models.</description>
      <author>example@mail.com (Yi Qin, Rui Wang, Tao Huang, Tong Xiao, Liping Jing)</author>
      <guid isPermaLink="false">2508.06127v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges</title>
      <link>http://arxiv.org/abs/2508.06111v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages and appendices&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SKATE是一种创新的评估框架，通过让大型语言模型相互生成和解决可验证任务来进行评估。该框架完全自动化、无需数据且可扩展，不需要人工输入或领域专业知识，能够实现客观且开放式的模型评估。&lt;h4&gt;背景&lt;/h4&gt;评估基础模型的能力和风险至关重要，但当前方法需要大量领域专业知识，限制了它们的可扩展性，随着模型快速发展，需要更有效的评估方法。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的评估框架SKATE，解决现有评估方法的局限性，创建一种可扩展、客观且开放式的评估方法，以跟上大型语言模型的快速发展。&lt;h4&gt;方法&lt;/h4&gt;在SKATE框架中，大型语言模型通过为彼此生成和解决可验证任务来竞争，将评估视为一种游戏，模型既是任务设置者又是解决者。研究引入了LLM设置的代码输出预测(COP)挑战作为可验证和可扩展的测试框架，并使用基于TrueSkill的排名系统评估六个前沿LLM。&lt;h4&gt;主要发现&lt;/h4&gt;较弱的模型可以可靠地区分和评分更强的模型；基于LLM的系统能够表现出自我偏好行为，生成与其自身能力相匹配的问题；SKATE自动揭示模型之间的细粒度能力差异。&lt;h4&gt;结论&lt;/h4&gt;研究成果朝着通用、可扩展的评估框架迈出了重要一步，这种评估框架能够跟上大型语言模型的进步。&lt;h4&gt;翻译&lt;/h4&gt;评估基础模型的能力和风险至关重要，然而当前方法需要大量领域专业知识，限制了它们随着这些模型快速演变而发展的可扩展性。我们引入了SKATE：一种新颖的评估框架，其中大型语言模型通过为彼此生成和解决可验证任务来竞争。我们的核心洞见是将评估视为一种游戏：模型既是任务设置者又是解决者，被激励创建能够突出自身优势同时暴露他人弱点的问题。SKATE提供了几个关键优势，平衡了可扩展性、开放性和客观性。它完全自动化、无需数据且可扩展，不需要人工输入或领域专业知识。通过使用可验证任务而非LLM裁判，评分是客观的。与领域有限的程序生成基准测试（如国际象棋或空间推理）不同，让LLM创造性地提出挑战能够实现开放且可扩展的评估。作为概念证明，我们引入了LLM设置的代码输出预测(COP)挑战作为一个可验证且可扩展的框架来测试我们的方法。使用基于TrueSkill的排名系统，我们评估了六个前沿LLM并发现：(1)较弱的模型可以可靠地区分和评分更强的模型，(2)基于LLM的系统能够表现出自我偏好行为，生成与其自身能力相匹配的问题，(3)SKATE自动揭示了模型之间的细粒度能力差异。我们的研究成果朝着通用、可扩展的评估框架迈出了重要一步，这种框架能够跟上大型语言模型的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluating the capabilities and risks of foundation models is paramount, yetcurrent methods demand extensive domain expertise, hindering their scalabilityas these models rapidly evolve. We introduce SKATE: a novel evaluationframework in which large language models (LLMs) compete by generating andsolving verifiable tasks for one another. Our core insight is to treatevaluation as a game: models act as both task-setters and solvers, incentivizedto create questions which highlight their own strengths while exposing others'weaknesses. SKATE offers several key advantages, balancing scalability,open-endedness, and objectivity. It is fully automated, data-free, andscalable, requiring no human input or domain expertise. By using verifiabletasks rather than LLM judges, scoring is objective. Unlike domain-limitedprogrammatically-generated benchmarks (e.g. chess-playing or spatialreasoning), having LLMs creatively pose challenges enables open-ended andscalable evaluation. As a proof of concept, we introduce LLM-setcode-output-prediction (COP) challenges as a verifiable and extensibleframework in which to test our approach. Using a TrueSkill-based rankingsystem, we evaluate six frontier LLMs and find that: (1) weaker models canreliably differentiate and score stronger ones, (2) LLM-based systems arecapable of self-preferencing behavior, generating questions that align withtheir own capabilities, and (3) SKATE automatically surfaces fine-grainedcapability differences between models. Our findings are an important steptowards general, scalable evaluation frameworks which can keep pace with LLMprogress.</description>
      <author>example@mail.com (Dewi S. W. Gould, Bruno Mlodozeniec, Samuel F. Brown)</author>
      <guid isPermaLink="false">2508.06111v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline</title>
      <link>http://arxiv.org/abs/2508.06094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://conlangcrafter.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了ConlangCrafter，一个利用大型语言模型作为计算创造力辅助工具的端到端人工语言创建系统。&lt;h4&gt;背景&lt;/h4&gt;人工语言如世界语和昆雅语在艺术、哲学和国际交流中扮演多种角色，同时大型基础模型已彻底改变了文本、图像等领域的创造性生成。&lt;h4&gt;目的&lt;/h4&gt;利用现代大型语言模型作为端到端人工语言创建的计算创造力辅助工具。&lt;h4&gt;方法&lt;/h4&gt;介绍了ConlangCrafter，一个多跳流水线，将语言设计分解为音系学、形态学、句法学、词汇生成和翻译等模块化阶段。在每个阶段，利用LLMs的元语言推理能力，注入随机性以鼓励多样性，并利用自我完善反馈来鼓励语言描述的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;ConlangCrafter能够在无需人类语言学专业知识的情况下，产生连贯且多样化的人工语言。&lt;h4&gt;结论&lt;/h4&gt;ConlangCrafter在衡量连贯性和类型学多样性的指标上表现出色，证明了其作为人工语言创建工具的有效性。&lt;h4&gt;翻译&lt;/h4&gt;本研究利用现代大型语言模型作为端到端人工语言创建的计算创造力辅助工具，提出了ConlangCrafter，一个将语言设计分解为音系学、形态学、句法学、词汇生成和翻译等模块化阶段的多跳流水线。在每个阶段，我们的方法利用了LLMs的元语言推理能力，注入随机性以鼓励多样性，并利用自我完善反馈来鼓励新兴语言描述的一致性。我们在衡量连贯性和类型学多样性的指标上评估了ConlangCrafter，展示了其无需人类语言学专业知识就能产生连贯且多样化的人工语言的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Constructed languages (conlangs) such as Esperanto and Quenya have playeddiverse roles in art, philosophy, and international communication. Meanwhile,large-scale foundation models have revolutionized creative generation in text,images, and beyond. In this work, we leverage modern LLMs as computationalcreativity aids for end-to-end conlang creation. We introduce ConlangCrafter, amulti-hop pipeline that decomposes language design into modular stages --phonology, morphology, syntax, lexicon generation, and translation. At eachstage, our method leverages LLMs' meta-linguistic reasoning capabilities,injecting randomness to encourage diversity and leveraging self-refinementfeedback to encourage consistency in the emerging language description. Weevaluate ConlangCrafter on metrics measuring coherence and typologicaldiversity, demonstrating its ability to produce coherent and varied conlangswithout human linguistic expertise.</description>
      <author>example@mail.com (Morris Alper, Moran Yanuka, Raja Giryes, Gašper Beguš)</author>
      <guid isPermaLink="false">2508.06094v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?</title>
      <link>http://arxiv.org/abs/2508.06057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IGARSS 2025!&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了卫星光谱图像作为AGI发展中的重要模态，指出其尚未得到应有的关注，同时提出了需要更全面的基准来评估地球观测模型，并提出了一套应包含在基准中的任务。&lt;h4&gt;背景&lt;/h4&gt;通用人工智能(AGI)正接近现实，研究人员正在处理多种模态的数据，包括文本、图像、视频和音频。然而，卫星光谱图像作为一种有价值的模态尚未得到充分研究，尽管它在帮助AGI理解自然世界方面具有巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;论证地球观测数据对智能模型的有用性，回顾现有基准测试的局限性，并强调需要更全面的基准来评估地球观测模型。&lt;h4&gt;方法&lt;/h4&gt;通过回顾现有基准测试，分析它们在评估基础模型泛化能力方面的局限性，并提出一套应包含在基准中的全面任务。&lt;h4&gt;主要发现&lt;/h4&gt;现有基准测试在评估基础模型在地球观测领域的泛化能力方面存在局限性，需要开发更全面的基准来有效评估模型理解和处理地球观测数据的能力。&lt;h4&gt;结论&lt;/h4&gt;地球观测数据对于AGI的发展至关重要，需要开发更全面的基准测试来评估模型在这一领域的性能，并提出了一套应包含在基准中的任务。&lt;h4&gt;翻译&lt;/h4&gt;通用人工智能(AGI)比以往任何时候都更接近现实，引发了研究界收集和处理各种模态（包括文本、图像、视频和音频）的广泛热情。尽管最近有相关努力，但卫星光谱图像作为额外的模态尚未得到应有的关注。这个领域存在独特的挑战，但在推进AGI理解自然世界的能力方面也具有巨大潜力。在本文中，我们首先论证了地球观测数据对智能模型的有用性，然后回顾了现有的基准测试，并指出了它们在评估基础模型在该领域的泛化能力方面的局限性。本文强调需要更全面的基准来评估地球观测模型。为此，我们提出了一套基准应该包含的全面任务，以有效评估模型理解和处理地球观测数据的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial General Intelligence (AGI) is closer than ever to becoming areality, sparking widespread enthusiasm in the research community to collectand work with various modalities, including text, image, video, and audio.Despite recent efforts, satellite spectral imagery, as an additional modality,has yet to receive the attention it deserves. This area presents uniquechallenges, but also holds great promise in advancing the capabilities of AGIin understanding the natural world. In this paper, we argue why EarthObservation data is useful for an intelligent model, and then we reviewexisting benchmarks and highlight their limitations in evaluating thegeneralization ability of foundation models in this domain. This paperemphasizes the need for a more comprehensive benchmark to evaluate earthobservation models. To facilitate this, we propose a comprehensive set of tasksthat a benchmark should encompass to effectively assess a model's ability tounderstand and interact with Earth observation data.</description>
      <author>example@mail.com (Mojtaba Valipour, Kelly Zheng, James Lowman, Spencer Szabados, Mike Gartner, Bobby Braswell)</author>
      <guid isPermaLink="false">2508.06057v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>When a Paper Has 1000 Authors: Rethinking Citation Metrics in the Era of LLMs</title>
      <link>http://arxiv.org/abs/2508.06004v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了SBCI指标，用于在大规模合作论文中评估个体研究者的贡献，解决了传统引用指标无法有效区分大规模论文中个体贡献者的问题。&lt;h4&gt;背景&lt;/h4&gt;作者级别引用指标是衡量学术影响力的实用工具，但在大型语言模型和基础模型领域，论文合作规模激增（如Gemini有1361名作者，19个月内被引用4600次），传统指标如总引用数和h-index无法有效区分个体贡献。&lt;h4&gt;目的&lt;/h4&gt;研究如何在大规模语言模型论文的数千名合作者中识别出突出的研究人员，这对学术招聘和资金决策等场景尤为重要。&lt;h4&gt;方法&lt;/h4&gt;引入了一种新的引用指标SBCI，通过平衡大规模和小规模出版物中的贡献来解决这一挑战，分析了其理论特性，并在合成的出版物数据集上评估了其行为。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的SBCI指标在大规模合作时代为个体学术影响提供了更稳健和更具区分度的评估。&lt;h4&gt;结论&lt;/h4&gt;SBCI指标能够更好地评估大规模合作环境下的个体学术贡献，特别是在大型语言模型和基础模型领域。&lt;h4&gt;翻译&lt;/h4&gt;作者级别的引用指标在复杂的研究生态系统中提供了实用、可解释且可扩展的学术影响力信号，已被广泛用作招聘决策的代理指标。然而，过去五年大型语言模型和基础模型领域出现了大规模出版物，论文合作者从数百到数千不等，并在几个月内获得数万次引用。例如，Gemini有1361名作者，在19个月内被引用约4600次。在这种情况下，传统指标无法有效区分个体贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Author-level citation metrics provide a practical, interpretable, andscalable signal of scholarly influence in a complex research ecosystem. It hasbeen widely used as a proxy in hiring decisions. However, the past five yearshave seen the rapid emergence of large-scale publications in the field of largelanguage models and foundation models, with papers featuring hundreds tothousands of co-authors and receiving tens of thousands of citations withinmonths. For example, Gemini has 1361 authors and has been cited around 4600times in 19 months. In such cases, traditional metrics, such as total citationcount and the $h$-index, fail to meaningfully distinguish individualcontributions. Therefore, we propose the following research question: How canone identify standout researchers among thousands of co-authors in large-scaleLLM papers? This question is particularly important in scenarios such asacademic hiring and funding decisions. In this paper, we introduce a novelcitation metric designed to address this challenge by balancing contributionsacross large-scale and small-scale publications. We propose the SBCI index,analyze its theoretical properties, and evaluate its behavior on syntheticpublication datasets. Our results demonstrate that the proposed metric providesa more robust and discriminative assessment of individual scholarly impact inthe era of large-scale collaborations.</description>
      <author>example@mail.com (Weihang Guo, Zhao Song, Jiahao Zhang)</author>
      <guid isPermaLink="false">2508.06004v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models</title>
      <link>http://arxiv.org/abs/2508.05880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了大型语言模型(LLMs)如何通过认知维度进行情感推理，超越了传统的表面情感任务。研究引入了一个名为CoRE的大规模基准测试，用于评估LLMs在情感推理过程中使用的内部认知结构。&lt;h4&gt;背景&lt;/h4&gt;情感计算已成为人工智能系统全面发展的重要研究领域。过去的研究主要在监督方式下评估或训练LLMs，使用离散情感标签来处理情感相关任务，但这些评估通常局限于标准和表面的情感识别任务。&lt;h4&gt;目的&lt;/h4&gt;超越表面的情感任务，研究LLMs如何通过认知维度进行情感推理；检查LLMs在处理情感刺激时是否能产生连贯和合理的认知推理；引入CoRE基准测试评估LLMs的内部认知结构；回答模型是否更依赖特定认知维度、哪些认知维度对特定情感重要、以及LLMs中情感表征是否可通过认知维度解释三个问题。&lt;h4&gt;方法&lt;/h4&gt;基于认知评估理论，检查LLMs在处理情感刺激时的认知推理能力；引入CoRE大规模基准测试；进行大量评估实验和分析。&lt;h4&gt;主要发现&lt;/h4&gt;不同LLMs展现出多样化的推理模式；研究提供了新的基准测试和工具，可供进一步研究LLMs的情感认知能力。&lt;h4&gt;结论&lt;/h4&gt;研究揭示了不同LLMs在情感认知推理方面的多样性模式；CoRE基准测试和代码将公开可用，为未来研究提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;情感计算已被确立为一个关键研究领域，以推动人工智能(AI)系统的整体发展。基础模型--特别是大型语言模型(LLMs)--在过去的几项工作中已被评估、训练或指令调优，以成为更好的情感预测器或生成器。然而，大多数研究以监督方式处理情感相关任务，使用与刺激物(如文本、图像、视频、音频)相关的离散情感标签来评估或训练LLMs的能力。评估研究尤其局限于标准和表面的情感相关任务，如识别引发或表达的情感。在本文中，我们超越表面情感任务，研究LLMs如何通过认知维度进行情感推理。借鉴认知评估理论，我们检查LLMs在处理情感刺激时是否能产生连贯和合理的认知推理。我们引入了一个关于情感认知推理的大规模基准测试--CoRE--用于评估LLMs用于情感推理的内部认知结构。通过大量的评估实验和分析，我们试图回答：(a)模型是否更可能隐式依赖特定的认知评估维度？(b)哪些认知维度对于表征特定情感很重要？以及(c)LLMs中不同情感类别的内部表征是否可以通过认知评估维度来解释？我们的结果和分析揭示了不同LLMs之间的多样化推理模式。我们的基准测试和代码将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Affective Computing has been established as a crucial field of inquiry toadvance the holistic development of Artificial Intelligence (AI) systems.Foundation models -- especially Large Language Models (LLMs) -- have beenevaluated, trained, or instruction-tuned in several past works, to becomebetter predictors or generators of emotion. Most of these studies, however,approach emotion-related tasks in a supervised manner, assessing or trainingthe capabilities of LLMs using discrete emotion labels associated with stimuli(e.g., text, images, video, audio). Evaluation studies, in particular, haveoften been limited to standard and superficial emotion-related tasks, such asthe recognition of evoked or expressed emotions. In this paper, we move beyondsurface-level emotion tasks to investigate how LLMs reason about emotionsthrough cognitive dimensions. Drawing from cognitive appraisal theory, weexamine whether LLMs produce coherent and plausible cognitive reasoning whenreasoning about emotionally charged stimuli. We introduce a large-scalebenchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internalcognitive structures implicitly used by LLMs for emotional reasoning. Through aplethora of evaluation experiments and analysis, we seek to answer: (a) Aremodels more likely to implicitly rely on specific cognitive appraisaldimensions?, (b) What cognitive dimensions are important for characterizingspecific emotions?, and, (c) Can the internal representations of differentemotion categories in LLMs be interpreted through cognitive appraisaldimensions? Our results and analyses reveal diverse reasoning patterns acrossdifferent LLMs. Our benchmark and code will be made publicly available.</description>
      <author>example@mail.com (Sree Bhattacharyya, Lucas Craig, Tharun Dilliraj, Jia Li, James Z. Wang)</author>
      <guid isPermaLink="false">2508.05880v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction</title>
      <link>http://arxiv.org/abs/2508.05838v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the Proceedings of the 2025 3rd International Conference  on Robotics, Control and Vision Engineering (RCVE'25). 6 pages, 3 figures, 1  table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的方法，将视觉基础模型与强化学习相结合，以增强在模拟环境中的物体交互能力。&lt;h4&gt;背景&lt;/h4&gt;在模拟环境中提高智能体的物体交互能力是机器人领域的重要挑战。&lt;h4&gt;目的&lt;/h4&gt;通过结合先进的视觉模型与强化学习算法，提高智能体在模拟环境中感知和与物体交互的能力。&lt;h4&gt;方法&lt;/h4&gt;结合Segment Anything Model (SAM)和YOLOv5与Proximal Policy Optimization (PPO)智能体，在AI2-THOR模拟环境中运行，并在四种不同的室内厨房环境中进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;与没有高级感知能力的基线智能体相比，物体交互成功率和导航效率显著提高，平均累积奖励增加68%，物体交互成功率提高52.5%，导航效率提高33%。&lt;h4&gt;结论&lt;/h4&gt;将基础模型与强化学习集成对于复杂的机器人任务具有潜力，为更复杂、更强大的自主智能体铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新颖的方法，将视觉基础模型与强化学习相结合，以增强在模拟环境中的物体交互能力。通过将Segment Anything Model (SAM)和YOLOv5与在AI2-THOR模拟环境中运行的Proximal Policy Optimization (PPO)智能体相结合，我们使智能体能够更有效地感知和与物体交互。我们在四种不同的室内厨房环境中进行的综合实验表明，与没有高级感知能力的基线智能体相比，物体交互成功率和导航效率有显著提高。结果显示平均累积奖励增加了68%，物体交互成功率提高了52.5%，导航效率提高了33%。这些发现强调了将基础模型与强化学习集成用于复杂机器人任务的潜力，为更复杂、更强大的自主智能体铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3747393.3747399&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel approach that integrates vision foundation modelswith reinforcement learning to enhance object interaction capabilities insimulated environments. By combining the Segment Anything Model (SAM) andYOLOv5 with a Proximal Policy Optimization (PPO) agent operating in theAI2-THOR simulation environment, we enable the agent to perceive and interactwith objects more effectively. Our comprehensive experiments, conducted acrossfour diverse indoor kitchen settings, demonstrate significant improvements inobject interaction success rates and navigation efficiency compared to abaseline agent without advanced perception. The results show a 68% increase inaverage cumulative reward, a 52.5% improvement in object interaction successrate, and a 33% increase in navigation efficiency. These findings highlight thepotential of integrating foundation models with reinforcement learning forcomplex robotic tasks, paving the way for more sophisticated and capableautonomous agents.</description>
      <author>example@mail.com (Ahmad Farooq, Kamran Iqbal)</author>
      <guid isPermaLink="false">2508.05838v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios</title>
      <link>http://arxiv.org/abs/2508.05829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TSMS-SAM2框架，通过多时间尺度视频采样增强和记忆分割剪枝机制，解决了外科视频中快速物体运动和记忆冗余的挑战，实现了更高效准确的可提示视频对象分割和跟踪。&lt;h4&gt;背景&lt;/h4&gt;随着Segment Anything Model 2等基础模型的出现，可提示的视频对象分割和跟踪取得了显著进展，但在外科视频分析中的应用仍面临复杂动态运动和记忆冗余导致的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出TSMS-SAM2框架，增强可提示视频对象分割和跟踪在外科视频中的应用效果，解决快速物体运动和记忆冗余问题。&lt;h4&gt;方法&lt;/h4&gt;TSMS-SAM2引入两种关键策略：多时间尺度视频采样增强提高对运动变化的鲁棒性，以及记忆分割和剪枝机制组织和过滤过去帧特征以实现更高效准确的分割。&lt;h4&gt;主要发现&lt;/h4&gt;在EndoVis2017和EndoVis2018数据集上评估，TSMS-SAM2分别获得95.24和86.73的最高平均Dice分数，优于之前的基于SAM和任务特定方法，消融研究证实了多时间尺度增强和记忆分割的有效性。&lt;h4&gt;结论&lt;/h4&gt;TSMS-SAM2框架在复杂外科场景中具有进行鲁棒、高效分割的潜力。&lt;h4&gt;翻译&lt;/h4&gt;可提示的视频对象分割和跟踪随着Segment Anything Model 2等基础模型的出现已取得显著进展；然而，由于其复杂的动态运动和阻碍有效学习的冗余记忆，它们在外科视频分析中的应用仍然具有挑战性。在这项工作中，我们提出了TSMS-SAM2，一个新颖的框架，通过解决SAM2中快速物体运动和记忆冗余的挑战，增强了外科视频中的可提示视频对象分割和跟踪。TSMS-SAM2引入了两个关键策略：多时间尺度视频采样增强，以提高对运动变化的鲁棒性；以及记忆分割和剪枝机制，组织和过滤过去帧特征以实现更高效和准确的分割。在EndoVis2017和EndoVis2018数据集上评估，TSMS-SAM2分别获得了95.24和86.73的最高平均Dice分数，优于之前的基于SAM和任务特定的方法。大量的消融研究证实了多时间尺度增强和记忆分割的有效性，突显了该框架在复杂外科场景中进行鲁棒、高效分割的潜力。我们的源代码将在https://github.com/apple1986/TSMS-SAM2上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Promptable video object segmentation and tracking (VOST) has seen significantadvances with the emergence of foundation models like Segment Anything Model 2(SAM2); however, their application in surgical video analysis remainschallenging due to complex motion dynamics and the redundancy of memory thatimpedes effective learning. In this work, we propose TSMS-SAM2, a novelframework that enhances promptable VOST in surgical videos by addressingchallenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2introduces two key strategies: multi-temporal-scale video sampling augmentationto improve robustness against motion variability, and a memory splitting andpruning mechanism that organizes and filters past frame features for moreefficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,respectively, outperforming prior SAM-based and task-specific methods.Extensive ablation studies confirm the effectiveness of multiscale temporalaugmentation and memory splitting, highlighting the framework's potential forrobust, efficient segmentation in complex surgical scenarios. Our source codewill be available at https://github.com/apple1986/TSMS-SAM2.</description>
      <author>example@mail.com (Guoping Xu, Hua-Chieh Shao, You Zhang)</author>
      <guid isPermaLink="false">2508.05829v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>CF3: Compact and Fast 3D Feature Fields</title>
      <link>http://arxiv.org/abs/2508.05254v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CF3的自上而下管道，用于构建紧凑快速的3D高斯特征场，通过多视图2D特征融合和自适应稀疏化方法，显著减少了高斯数量同时保持了几何细节。&lt;h4&gt;背景&lt;/h4&gt;3D Gaussian Splatting(3DGS)开始整合来自2D基础模型的信息，但大多数方法采用自下而上的优化过程，将原始2D特征视为真实值，导致计算成本增加。&lt;h4&gt;目的&lt;/h4&gt;提出一种自上而下的管道，构建紧凑且快速的3D高斯特征场，以降低计算成本并提高效率。&lt;h4&gt;方法&lt;/h4&gt;1) 使用预训练的高斯快速加权融合多视图2D特征；2) 直接在提升的特征上训练每个高斯的自动编码器，而非在2D域中训练；3) 引入自适应稀疏化方法，在优化特征场高斯属性的同时剪枝和合并冗余高斯。&lt;h4&gt;主要发现&lt;/h4&gt;自动编码器更好地与特征分布对齐，与Feature-3DGS相比，使用少至5%的高斯实现了具有竞争力的3D特征场。&lt;h4&gt;结论&lt;/h4&gt;CF3方法能够在保持几何细节的同时，构建高效的3D特征场表示。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯喷溅(3DGS)开始整合来自2D基础模型的丰富信息。然而，大多数方法依赖于自下而上的优化过程，将原始2D特征视为真实值，导致计算成本增加。我们提出了一种自上而下的管道，用于构建紧凑且快速的3D高斯特征场，即CF3。我们首先使用预训练的高斯对多视图2D特征进行快速加权融合。这种方法允许直接在提升的特征上训练每个高斯的自动编码器，而不是在2D域中训练自动编码器。因此，自动编码器更好地与特征分布对齐。更重要的是，我们引入了一种自适应稀疏化方法，在剪枝和合并冗余高斯的同时优化特征场的高斯属性，构建了保留几何细节的高效表示。与Feature-3DGS相比，我们的方法使用少至5%的高斯实现了具有竞争力的3D特征场。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何构建紧凑且快速的3D特征场问题。现有方法将2D基础模型的丰富信息整合到3D高斯溅射(3DGS)中时，需要大量高斯元和计算资源，导致存储和效率问题。这个问题在现实中很重要，因为高效紧凑的3D特征场可以实现实时语义理解、开放词汇查询等任务，使这些方法能在更大规模场景和实际应用中部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析现有方法如Feature-3DGS和LangSplat存在联合优化颜色和特征导致冗余、直接嵌入高维特征带来高成本的问题。因此提出自顶向下的管道，首先使用预训练3DGS进行快速多视角特征融合，然后训练每个高斯元的自编码器压缩特征，最后通过自适应稀疏化减少冗余高斯元。作者借鉴了FiT3D和CONDENSE的3D感知训练思想，LightGaussian的剪枝方法，以及高斯混合约简中的矩匹配方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过特征提升、压缩和稀疏化构建紧凑3D特征场，将特征直接存储在RGB通道替代颜色信息。整体流程分三步：1)特征提升：用预训练3DGS加权融合多视角2D特征，过滤噪声；2)特征压缩：训练每个高斯元的自编码器将高维特征压缩到3维；3)自适应稀疏化：优化高斯元属性，剪枝低价值高斯元，合并相邻语义相似的高斯元，使用马氏距离衡量重叠程度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)自顶向下的3D特征场构建方法；2)每个高斯元的自编码器直接在3D提升特征上训练；3)自适应稀疏化方法减少冗余高斯元；4)将压缩特征存储在RGB通道中与3DGS兼容。相比之前工作：CF3不联合优化颜色和特征，仅用5%高斯元实现相似性能；在3D域训练自编码器而非2D域，特征更一致；明确考虑为颜色优化的高斯元对特征表达的冗余性并针对性优化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CF3提出了一种自顶向下的3D特征场构建方法，通过特征提升、每个高斯元的自编码器压缩和自适应稀疏化，实现了使用仅5%高斯元的紧凑且快速的3D特征表示，同时保持与现有方法相当的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2Dfoundation models. However, most approaches rely on a bottom-up optimizationprocess that treats raw 2D features as ground truth, incurring increasedcomputational costs. We propose a top-down pipeline for constructing compactand fast 3D Gaussian feature fields, namely, CF3. We first perform a fastweighted fusion of multi-view 2D features with pre-trained Gaussians. Thisapproach enables training a per-Gaussian autoencoder directly on the liftedfeatures, instead of training autoencoders in the 2D domain. As a result, theautoencoder better aligns with the feature distribution. More importantly, weintroduce an adaptive sparsification method that optimizes the Gaussianattributes of the feature field while pruning and merging the redundantGaussians, constructing an efficient representation with preserved geometricdetails. Our approach achieves a competitive 3D feature field using as littleas 5% of the Gaussians compared to Feature-3DGS.</description>
      <author>example@mail.com (Hyunjoon Lee, Joonkyu Min, Jaesik Park)</author>
      <guid isPermaLink="false">2508.05254v2</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>V*: An Efficient Motion Planning Algorithm for Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2508.06404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了V*，一种基于图的自动驾驶车辆运动规划器，能够在结构化环境中生成时间最优、无碰撞且满足动态和运动学约束的轨迹。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆在结构化环境中导航需要能够生成时间最优、无碰撞且满足动态和运动学约束的轨迹规划器。&lt;h4&gt;目的&lt;/h4&gt;开发一种名为V*的基于图的运动规划器，将速度和方向作为显式状态变量在离散化的空间-时间-速度格中表示。&lt;h4&gt;方法&lt;/h4&gt;通过搜索扩展过程中的动态图生成将运动维度直接整合到图中；使用六边形离散化策略管理高维搜索；为速度感知运动规划提供形式化数学证明；开发运动学自行车模型中瞬态转向动力学的数学公式；结合几何剪枝策略消除导致不可行转向配置的扩展。&lt;h4&gt;主要发现&lt;/h4&gt;V*能够评估动态允许的机动操作，确保轨迹物理可实现；在包含移动障碍物的复杂动态环境中能有效避免冲突、主动让行；能够生成具有时间推理能力的安全、高效轨迹，用于等待行为和动态协调。&lt;h4&gt;结论&lt;/h4&gt;V*是一种有效的自动驾驶车辆导航规划器，能够在复杂动态环境中生成安全、高效的轨迹。&lt;h4&gt;翻译&lt;/h4&gt;在结构化环境中自动驾驶车辆导航需要能够生成时间最优、无碰撞且满足动态和运动学约束的轨迹规划器。我们介绍了V*，一种基于图的运动规划器，它在离散化的空间-时间-速度格中将速度和方向表示为显式状态变量。与将空间搜索与动态可行性解耦或依赖后处理平滑的传统方法不同，V*通过搜索扩展过程中的动态图生成将运动维度直接整合到图中。为管理高维搜索的复杂性，我们采用六边形离散化策略，并提供形式化数学证明，建立速度感知运动规划在约束转向转换下的最优路径点间距和最小节点冗余。我们开发了运动学自行车模型中瞬态转向动力学的数学公式，模拟指数行为的转向角收敛，并推导收敛率参数的关系。这一理论基础结合几何剪枝策略，消除了导致不可行转向配置的扩展，使V*能够评估动态允许的机动操作，确保每条轨迹无需进一步细化即可物理实现。我们在包含移动障碍物的杂乱和动态环境中通过模拟研究进一步证明了V*的性能，展示了它避免冲突、主动让行以及生成具有时间推理能力的安全、高效轨迹的能力，用于等待行为和动态协调。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous vehicle navigation in structured environments requires plannerscapable of generating time-optimal, collision-free trajectories that satisfydynamic and kinematic constraints. We introduce V*, a graph-based motionplanner that represents speed and direction as explicit state variables withina discretised space-time-velocity lattice. Unlike traditional methods thatdecouple spatial search from dynamic feasibility or rely on post-hoc smoothing,V* integrates both motion dimensions directly into graph construction throughdynamic graph generation during search expansion. To manage the complexity ofhigh-dimensional search, we employ a hexagonal discretisation strategy andprovide formal mathematical proofs establishing optimal waypoint spacing andminimal node redundancy under constrained heading transitions forvelocity-aware motion planning. We develop a mathematical formulation fortransient steering dynamics in the kinematic bicycle model, modelling steeringangle convergence with exponential behaviour, and deriving the relationship forconvergence rate parameters. This theoretical foundation, combined withgeometric pruning strategies that eliminate expansions leading to infeasiblesteering configurations, enables V* to evaluate dynamically admissiblemanoeuvres, ensuring each trajectory is physically realisable without furtherrefinement. We further demonstrate V*'s performance in simulation studies withcluttered and dynamic environments involving moving obstacles, showing itsability to avoid conflicts, yield proactively, and generate safe, efficienttrajectories with temporal reasoning capabilities for waiting behaviours anddynamic coordination.</description>
      <author>example@mail.com (Abdullah Zareh Andaryan, Michael G. H. Bell, Mohsen Ramezani, Glenn Geers)</author>
      <guid isPermaLink="false">2508.06404v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Aligning Effective Tokens with Video Anomaly in Large Language Models</title>
      <link>http://arxiv.org/abs/2508.06350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为VA-GPT的新型多模态大语言模型，专门用于总结和定位各种视频中的异常事件。&lt;h4&gt;背景&lt;/h4&gt;理解视频中的异常事件是一项重要且具有挑战性的任务，已在广泛应用领域受到关注。然而，当前的视频理解多模态大语言模型虽然能分析一般视频，但由于异常事件在空间和时间上的稀疏性以及冗余信息，往往难以有效处理异常情况。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉和分析与异常事件相关的空间和时间信息，从而提供更准确响应和交互的视频异常事件理解模型。&lt;h4&gt;方法&lt;/h4&gt;提出VA-GPT模型，通过两个关键模块对齐视觉编码器和大型语言模型之间的有效令牌：空间有效令牌选择(SETS)和时间有效令牌生成(TETG)。此外，还构建了一个用于微调视频异常感知多模态大语言模型的指令遵循数据集，并基于XD-Violence数据集引入了跨领域评估基准。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在多个基准测试上优于现有的最先进方法，能够更有效地处理视频中的异常事件。&lt;h4&gt;结论&lt;/h4&gt;VA-GPT模型通过有效利用视觉语言模型和大型语言模型的表示和泛化能力，解决了当前视频理解模型在处理异常事件时面临的挑战，为视频异常事件的理解提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;理解视频中的异常事件是一项重要且具有挑战性的任务，已在广泛的应用领域引起了显著关注。尽管当前的视频理解多模态大语言模型能够分析一般视频，但由于异常事件在空间和时间上的稀疏性以及冗余信息，它们往往难以处理异常情况。为了解决这些挑战，我们利用视觉语言模型和大型语言模型的表示和泛化能力，提出了VA-GPT，一种专为总结和定位各种视频中异常事件而设计的新型多模态大语言模型。我们的方法通过两个关键提出的模块有效地对齐视觉编码器和大型语言模型之间的有效令牌：空间有效令牌选择(SETS)和时间有效令牌生成(TETG)。这些模块使我们的模型能够有效地捕捉和分析与异常事件相关的空间和时间信息，从而提供更准确的响应和交互。此外，我们还构建了一个专门用于微调视频异常感知多模态大语言模型的指令遵循数据集，并基于XD-Violence数据集引入了跨领域评估基准。我们提出的方法在各种基准上都优于现有的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding abnormal events in videos is a vital and challenging task thathas garnered significant attention in a wide range of applications. Althoughcurrent video understanding Multi-modal Large Language Models (MLLMs) arecapable of analyzing general videos, they often struggle to handle anomaliesdue to the spatial and temporal sparsity of abnormal events, where theredundant information always leads to suboptimal outcomes. To address thesechallenges, exploiting the representation and generalization capabilities ofVison Language Models (VLMs) and Large Language Models (LLMs), we proposeVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events invarious videos. Our approach efficiently aligns effective tokens between visualencoders and LLMs through two key proposed modules: Spatial Effective TokenSelection (SETS) and Temporal Effective Token Generation (TETG). These modulesenable our model to effectively capture and analyze both spatial and temporalinformation associated with abnormal events, resulting in more accurateresponses and interactions. Furthermore, we construct an instruction-followingdataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce across-domain evaluation benchmark based on XD-Violence dataset. Our proposedmethod outperforms existing state-of-the-art methods on various benchmarks.</description>
      <author>example@mail.com (Yingxian Chen, Jiahui Liu, Ruifan Di, Yanwei Li, Chirui Chang, Shizhen Zhao, Wilton W. T. Fok, Xiaojuan Qi, Yik-Chung Wu)</author>
      <guid isPermaLink="false">2508.06350v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback</title>
      <link>http://arxiv.org/abs/2508.06292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 7 Tables, 6 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型多输出尖峰神经元模型，结合了线性SSM状态转换和非线性反馈机制，通过重置机制克服不稳定性，实现了与现有基准相当的性能。&lt;h4&gt;背景&lt;/h4&gt;神经形态计算是一种新兴技术，可实现低延迟和低能耗的信号处理。尖峰神经网络(SNNs)是其关键算法工具，使用有状态的神经元并通过尖峰编码信息。深度状态空间模型(SSMs)也使用有状态构建块，最近在时间建模任务中取得竞争性性能，但通常采用高精度激活函数且无重置机制。&lt;h4&gt;目的&lt;/h4&gt;结合SNNs和深度SSM模型的优势，提出一种新型多输出尖峰神经元模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种结合线性、通用SSM状态转换和非线性反馈机制的新型多输出尖峰神经元模型，通过重置机制实现非线性反馈。该模型明确区分了尖峰函数、重置条件和重置动作。在关键词发现、基于事件的视觉和顺序模式识别等任务上进行了实验。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的模型在SNN文献中实现了与现有基准相当的性能。重置机制可以克服不稳定性，即使在线性部分神经元动力学不稳定的情况下也能实现学习，超越了最近深度SSM模型中严格 enforced 的线性动力学稳定性。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型结合了SNNs和深度SSMs的优势，通过重置机制实现了更灵活的学习能力，能够处理不稳定的线性动力学。&lt;h4&gt;翻译&lt;/h4&gt;神经形态计算是一种新兴技术，可实现低延迟和低能耗的信号处理。神经形态计算中的关键算法工具是尖峰神经网络(SNNs)。SNNs是受生物启发的神经网络，使用有状态的神经元，并通过尖峰对信息进行编码和解码，实现低比特数据处理。类似于SNNs，深度状态空间模型(SSMs)也使用有状态的构建块。然而，最近在各种时间建模任务中取得竞争性性能的深度SSMs通常设计为高精度激活函数且无重置机制。为了结合SNNs和最近深度SSM模型的优势，我们提出了一种新型多输出尖峰神经元模型，该模型将线性的通用SSM状态转换通过重置机制与非线性反馈相结合。与现有的SNN神经元模型相比，我们提出的模型明确区分了尖峰函数、重置条件和重置动作。在关键词发现任务、基于事件的视觉任务和顺序模式识别任务等各种任务上的实验结果表明，我们提出的模型在SNN文献中实现了与现有基准相当的性能。我们的结果说明了所提出的重置机制如何能够克服不稳定性，即使在线性部分神经元动力学不稳定的情况下也能实现学习，使我们能够超越最近深度SSM模型中严格 enforced 的线性动力学稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/JSTSP.2025.3595030&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neuromorphic computing is an emerging technology enabling low-latency andenergy-efficient signal processing. A key algorithmic tool in neuromorphiccomputing is spiking neural networks (SNNs). SNNs are biologically inspiredneural networks which utilize stateful neurons, and provide low-bit dataprocessing by encoding and decoding information using spikes. Similar to SNNs,deep state-space models (SSMs) utilize stateful building blocks. However, deepSSMs, which recently achieved competitive performance in various temporalmodeling tasks, are typically designed with high-precision activation functionsand no reset mechanisms. To bridge the gains offered by SNNs and the recentdeep SSM models, we propose a novel multiple-output spiking neuron model thatcombines a linear, general SSM state transition with a non-linear feedbackmechanism through reset. Compared to the existing neuron models for SNNs, ourproposed model clearly conceptualizes the differences between the spikingfunction, the reset condition and the reset action. The experimental results onvarious tasks, i.e., a keyword spotting task, an event-based vision task and asequential pattern recognition task, show that our proposed model achievesperformance comparable to existing benchmarks in the SNN literature. Ourresults illustrate how the proposed reset mechanism can overcome instabilityand enable learning even when the linear part of neuron dynamics is unstable,allowing us to go beyond the strictly enforced stability of linear dynamics inrecent deep SSM models.</description>
      <author>example@mail.com (Sanja Karilanova, Subhrakanti Dey, Ayça Özçelikkale)</author>
      <guid isPermaLink="false">2508.06292v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology</title>
      <link>http://arxiv.org/abs/2508.06066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究为深度时间模型提供了首个非平凡、架构感知的泛化边界和原则性评估方法，发现时间依赖性可在固定信息预算下增强学习，同时揭示了理论与实践之间的差距。&lt;h4&gt;背景&lt;/h4&gt;深度时间架构如时间卷积网络在序列数据上表现出强大的预测性能，但其泛化能力的理论理解仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提供深度时间模型的首个非平凡、架构感知的泛化边界，并建立一种原则性的评估方法。&lt;h4&gt;方法&lt;/h4&gt;对于指数β混合序列，推导出边界为O(R√(Dpn log N/N))，其中D是网络深度，p是核大小，n是输入维度，R是权重范数；提出延迟反馈阻塞机制将相关样本转化为有效独立样本；引入公平比较方法固定有效样本大小以分离时间结构与信息内容的影响。&lt;h4&gt;主要发现&lt;/h4&gt;延迟反馈阻塞机制实现√D缩放而非指数缩放，意味着加倍深度大约需要四倍训练数据；强相关序列比弱相关序列表现出约76%更小的泛化差距；弱依赖遵循N_eff^(-1.21)缩放，强依赖遵循N_eff^(-0.89)，都比预测的N^(-0.5)更陡峭。&lt;h4&gt;结论&lt;/h4&gt;时间依赖性可以在固定信息预算下增强学习，同时理论与实践之间存在差距，这为未来研究提供了方向。&lt;h4&gt;翻译&lt;/h4&gt;深度时间架构如时间卷积网络在序列数据上实现了强大的预测性能，但其泛化能力的理论理解仍然有限。我们通过提供深度时间模型的首个非平凡、架构感知的泛化边界和一种原则性评估方法来填补这一空白。对于指数β混合序列，我们推导出边界为O(R√(Dpn log N/N))，其中D是网络深度，p是核大小，n是输入维度，R是权重范数。我们的延迟反馈阻塞机制将相关样本转化为有效独立样本，同时仅丢弃O(1/log N)的数据，实现了√D缩放而非指数缩放，意味着加倍深度大约需要四倍训练数据。我们还引入了一种公平比较方法，固定有效样本大小以分离时间结构的影响与信息内容的影响。在N_eff=2,000的情况下，强相关序列(ρ=0.8)比弱相关序列(ρ=0.2)表现出约76%更小的泛化差距，挑战了依赖性纯粹有害的直觉。然而，收敛率与理论预测不符：弱依赖遵循N_eff^(-1.21)缩放，强依赖遵循N_eff^(-0.89)，都比预测的N^(-0.5)更陡峭。这些发现揭示了时间依赖性可以在固定信息预算下增强学习，同时突显了理论与实践之间的差距，这激励了未来的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep temporal architectures such as Temporal Convolutional Networks (TCNs)achieve strong predictive performance on sequential data, yet theoreticalunderstanding of their generalization remains limited. We address this gap byproviding both the first non-vacuous, architecture-aware generalization boundsfor deep temporal models and a principled evaluation methodology.  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is networkdepth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Ourdelayed-feedback blocking mechanism transforms dependent samples intoeffectively independent ones while discarding only $O(1/\log N)$ of the data,yielding $\sqrt{D}$ scaling instead of exponential, implying that doublingdepth requires approximately quadrupling the training data.  We also introduce a fair-comparison methodology that fixes the effectivesample size to isolate the effect of temporal structure from informationcontent. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weaklydependent ones ($\rho=0.2$), challenging the intuition that dependence ispurely detrimental. Yet convergence rates diverge from theory: weakdependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependenciesfollow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.These findings reveal that temporal dependence can enhance learning under fixedinformation budgets, while highlighting gaps between theory and practice thatmotivate future research.</description>
      <author>example@mail.com (Barak Gahtan, Alex M. Bronstein)</author>
      <guid isPermaLink="false">2508.06066v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Accelerating Quantum Monte Carlo Calculations with Set-Equivariant Architectures and Transfer Learning</title>
      <link>http://arxiv.org/abs/2508.06441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了如何使用transformer架构显著加速或绕过变分量子蒙特卡洛计算中的可观测量评估步骤，特别是在处理磁化强度幂次等耗时算子时。通过多种复杂度递增的示例，从经典伊辛模型到具有长程相互作用的量子系统，包括回归和分类任务，证明了该方法的有效性。此外，研究还展示了如何利用迁移学习通过重用不同系统和较小系统规模的知识来降低训练成本。&lt;h4&gt;背景&lt;/h4&gt;机器学习方法（ML）扩展了变分量子蒙特卡洛（QMC）计算的准确性和应用范围，特别是在探索自旋系统表现出的各种量子现象方面。然而，QMC的可扩展性仍然受到其他瓶颈的限制，特别是与基于随机偏差的实际可观测量评估相关的问题。&lt;h4&gt;目的&lt;/h4&gt;展示如何使用transformer架构来显著加速甚至绕过QMC中基于随机偏差的可观测量评估步骤，特别是对于耗时的算子，如磁化强度的幂次。&lt;h4&gt;方法&lt;/h4&gt;通过一系列复杂度递增的例子来说明这一过程，从经典伊辛模型到具有长程相互作用的量子系统，包括回归（预测可观测量）和分类（检测相变）任务。此外，还探讨了如何利用迁移学习通过重用不同系统和较小系统规模的知识来降低训练成本。&lt;h4&gt;主要发现&lt;/h4&gt;transformer架构可以显著加速QMC中的可观测量评估，甚至可以绕过某些计算步骤；该方法适用于从经典到量子的多种系统；迁移学习可以进一步优化计算效率，通过重用已有知识减少训练成本。&lt;h4&gt;结论&lt;/h4&gt;transformer架构在QMC计算中具有显著的加速潜力，特别是在处理复杂算子时，而迁移学习可以进一步优化计算效率，为解决QMC可扩展性问题提供了新的思路。&lt;h4&gt;翻译&lt;/h4&gt;机器学习方法（ML）极大地扩展了变分量子蒙特卡洛（QMC）计算的准确性和应用范围，特别是在探索自旋系统表现出的各种量子现象时。然而，QMC的可扩展性仍然受到其他几个瓶颈的限制，特别是与基于随机偏差的实际可观测量评估相关的问题，这是该方法的核心所在。在本文中，我们展示了如何使用transformer架构来显著加速甚至绕过这一步骤，特别是对于磁化强度幂次等耗时算子。我们通过一系列复杂度递增的例子来说明这一过程，从经典伊辛模型到具有长程相互作用的量子系统，包括回归（预测可观测量）和分类（检测相变）。此外，我们还展示了如何利用迁移学习通过重用不同系统和较小系统规模的知识来降低训练成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine-learning (ML) ans\"atze have greatly expanded the accuracy and reachof variational quantum Monte Carlo (QMC) calculations, in particular whenexploring the manifold quantum phenomena exhibited by spin systems. However,the scalability of QMC is still compromised by several other bottlenecks, andspecifically those related to the actual evaluation of observables based onrandom deviates that lies at the core of the approach. Here we show how theset-transformer architecture can be used to dramatically accelerate or evenbypass that step, especially for time-consuming operators such as powers of themagnetization. We illustrate the procedure with a range of examples ofincreasing complexity, from the classical Ising model to quantum systems withlong-range interactions, and comprising both regressions (to predictobservables) and classifications (to detect phase transitions). Moreover, weshow how transfer learning can be leveraged to reduce the training cost byreusing knowledge from different systems and smaller system sizes.</description>
      <author>example@mail.com (Manuel Gallego, Sebastián Roca-Jerat, David Zueco, Jesús Carrete)</author>
      <guid isPermaLink="false">2508.06441v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation</title>
      <link>http://arxiv.org/abs/2508.06429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于GAN的半监督学习框架，专为医学影像中标记数据稀少的场景设计，通过结合生成器、判别器和分类器，在极少量标记数据条件下实现了显著优于现有方法的分类性能。&lt;h4&gt;背景&lt;/h4&gt;深度学习已彻底改变医学影像领域，但其效果受到标记训练数据不足的严重限制，尤其是在医学影像应用中标注成本高昂的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于GAN的半监督学习框架，专门为标记数据稀少的场景设计，在每类5到50个标记样本的设置下进行评估。&lt;h4&gt;方法&lt;/h4&gt;整合三个专门神经网络（类别条件图像转换的生成器、真实性和分类评估的判别器、专用分类器）在三阶段训练框架中工作，交替进行监督训练和利用未标记图像的无监督学习，采用基于集成方法的伪标记，结合判别器和分类器的置信度加权预测，并通过指数移动平均实现时间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在十一个MedMNIST数据集上的评估表明，该方法比六种最先进的基于GAN的半监督方法取得统计学显著改进，特别是在5-shot设置中表现尤为突出，且在所有评估设置（5、10、20和50 shot/类）中均保持优越性。&lt;h4&gt;结论&lt;/h4&gt;该方法为标记成本过高的医学影像应用提供了实用解决方案，即使使用最少的标记数据也能实现强大的分类性能，代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;深度学习彻底改变了医学影像，但其效果因标记训练数据不足而受到严重限制。本文介绍了一种基于GAN的新型半监督学习框架，专为标记数据稀少的场景设计，在每类5到50个标记样本的设置下进行了评估。我们的方法整合了三个专门神经网络——用于类别条件图像转换的生成器、用于真实性和分类评估的判别器，以及一个专门的分类器——在一个三阶段训练框架内。该方法在有限标记数据上的监督训练和通过图像到图像转换（而非从噪声生成）利用大量未标记图像的无监督学习之间交替进行。我们采用基于集成方法的伪标记，结合判别器和分类器的置信度加权预测，并通过指数移动平均实现时间一致性，从而能够可靠地估计未标记数据的标签。在十一个MedMNIST数据集上的全面评估表明，我们的方法比六种最先进的基于GAN的半监督方法取得了统计学上的显著改进，特别是在标记数据最稀缺的5-shot设置中表现尤为突出。该框架在所有评估设置（每类5、10、20和50个样本）中都保持着其优越性。我们的方法为标记成本过高的医学影像应用提供了实用的解决方案，即使使用最少的标记数据也能实现强大的分类性能。代码可在https://github.com/GuidoManni/SPARSE获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has revolutionized medical imaging, but its effectiveness isseverely limited by insufficient labeled training data. This paper introduces anovel GAN-based semi-supervised learning framework specifically designed forlow labeled-data regimes, evaluated across settings with 5 to 50 labeledsamples per class. Our approach integrates three specialized neural networks --a generator for class-conditioned image translation, a discriminator forauthenticity assessment and classification, and a dedicated classifier --within a three-phase training framework. The method alternates betweensupervised training on limited labeled data and unsupervised learning thatleverages abundant unlabeled images through image-to-image translation ratherthan generation from noise. We employ ensemble-based pseudo-labeling thatcombines confidence-weighted predictions from the discriminator and classifierwith temporal consistency through exponential moving averaging, enablingreliable label estimation for unlabeled data. Comprehensive evaluation acrosseleven MedMNIST datasets demonstrates that our approach achieves statisticallysignificant improvements over six state-of-the-art GAN-based semi-supervisedmethods, with particularly strong performance in the extreme 5-shot settingwhere the scarcity of labeled data is most challenging. The framework maintainsits superiority across all evaluated settings (5, 10, 20, and 50 shots perclass). Our approach offers a practical solution for medical imagingapplications where annotation costs are prohibitive, enabling robustclassification performance even with minimal labeled data. Code is available athttps://github.com/GuidoManni/SPARSE.</description>
      <author>example@mail.com (Guido Manni, Clemente Lauretti, Loredana Zollo, Paolo Soda)</author>
      <guid isPermaLink="false">2508.06429v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?</title>
      <link>http://arxiv.org/abs/2508.06327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICONIP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于扩散模型的合成数据生成方法，用于解决心脏磁共振成像中的域偏移问题。该方法生成的合成数据保持了空间和结构保真度，与源域相似且兼容分割掩码。通过域泛化和域适应两种策略，该方法显著提高了分割模型在未见目标域上的性能，减轻了对迁移学习或在线训练的需求，特别适用于数据稀缺的场景。&lt;h4&gt;背景&lt;/h4&gt;磁共振成像（包括心脏磁共振）容易因成像设备和采集协议的不同而产生域偏移问题，这限制了训练好的AI模型在真实场景中的应用，因为模型在未见过的域上性能会下降。&lt;h4&gt;目的&lt;/h4&gt;解决心脏磁共振成像中的域偏移问题，提高AI模型在真实场景中的泛化能力，特别是在数据稀缺的情况下。&lt;h4&gt;方法&lt;/h4&gt;提出一种在源域上训练的扩散模型（DM），生成与给定参考相似的心脏磁共振图像。通过域泛化（在合成的源域数据上训练域不变的分割模型）和域适应（使用DM将目标域数据向源域迁移）两种策略评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;两种策略都比仅使用真实数据训练分割模型在未见目标域上的分割性能有显著提高（基于表面度量，Welch's t-test, p &lt; 0.01）。&lt;h4&gt;结论&lt;/h4&gt;提出的方法减轻了为解决心脏磁共振图像分析中的域偏移问题而进行迁移学习或在线训练的需要，特别是在数据稀缺的情况下特别有用。&lt;h4&gt;翻译&lt;/h4&gt;磁共振成像（包括心脏磁共振）容易因成像设备和采集协议的不同而产生域偏移。这一挑战限制了训练好的AI模型在真实场景中的应用，因为在未见过的域上性能会下降。传统解决方案通过临时图像增强或额外的在线训练/迁移学习来增加数据集大小，但存在一些局限性。合成数据是一个有前途的替代方案，但解剖/结构一致性约束限制了生成模型在创建图像-标签对方面的有效性。为此，我们提出了一种在源域上训练的扩散模型（DM），能够生成与给定参考相似的心脏磁共振图像。合成的数据保持了空间和结构保真度，确保与源域的相似性和与分割掩码的兼容性。我们在多中心心脏磁共振分割任务中评估了我们的生成方法，使用了2D nnU-Net、3D nnU-Net和普通U-Net分割网络。我们探索了域泛化（在合成的源域数据上训练域不变的分割模型）和域适应（使用DM将目标域数据向源域迁移）两种策略。与仅使用真实数据训练分割模型相比，两种策略在未见目标域上的分割性能都有显著提高（基于表面度量，Welch's t-test, p &lt; 0.01）。提出的方法减轻了为解决心脏磁共振图像分析中的域偏移问题而进行迁移学习或在线训练的需要，特别适用于数据稀缺的场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Magnetic resonance (MR) imaging, including cardiac MR, is prone to domainshift due to variations in imaging devices and acquisition protocols. Thischallenge limits the deployment of trained AI models in real-world scenarios,where performance degrades on unseen domains. Traditional solutions involveincreasing the size of the dataset through ad-hoc image augmentation oradditional online training/transfer learning, which have several limitations.Synthetic data offers a promising alternative, but anatomical/structuralconsistency constraints limit the effectiveness of generative models increating image-label pairs. To address this, we propose a diffusion model (DM)trained on a source domain that generates synthetic cardiac MR images thatresemble a given reference. The synthetic data maintains spatial and structuralfidelity, ensuring similarity to the source domain and compatibility with thesegmentation mask. We assess the utility of our generative approach inmulti-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net andvanilla U-Net segmentation networks. We explore domain generalisation, where,domain-invariant segmentation models are trained on synthetic source domaindata, and domain adaptation, where, we shift target domain data towards thesource domain using the DM. Both strategies significantly improved segmentationperformance on data from an unseen target domain, in terms of surface-basedmetrics (Welch's t-test, p &lt; 0.01), compared to training segmentation models onreal data alone. The proposed method ameliorates the need for transfer learningor online training to address domain shift challenges in cardiac MR imageanalysis, especially useful in data-scarce settings.</description>
      <author>example@mail.com (Xin Ci Wong, Duygu Sarikaya, Kieran Zucker, Marc De Kamps, Nishant Ravikumar)</author>
      <guid isPermaLink="false">2508.06327v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models</title>
      <link>http://arxiv.org/abs/2508.05685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently under review. Code will be released upon acceptance&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Domain-guided Fine-tuning (DogFit)方法，这是一种用于扩散模型迁移学习的有效引导机制，能够在保持可控性的同时不增加额外的计算开销。DogFit通过将领域感知的引导偏置注入训练损失中，在微调过程中内部化引导行为，并通过轻量级条件机制将引导强度编码为额外模型输入。实验表明，DogFit在多个指标上优于之前的引导方法，同时计算效率更高。&lt;h4&gt;背景&lt;/h4&gt;将扩散模型迁移到较小的目标域具有挑战性，因为简单地微调模型通常会导致泛化能力差。测试时引导方法通过在图像保真度和样本多样性之间进行权衡，有助于缓解这一问题，但这些方法通常需要双向前向传递，计算成本高。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的引导机制，用于扩散模型的迁移学习，能够在保持可控性的同时不增加额外的计算开销。&lt;h4&gt;方法&lt;/h4&gt;提出了Domain-guided Fine-tuning (DogFit)方法，将领域感知的引导偏置注入训练损失中，在微调过程中内部化引导行为。通过轻量级条件机制将引导强度值编码为额外模型输入。研究了训练过程中引导偏置的最佳位置和时间，提出了两种简单的调度策略：late-start和cut-off。&lt;h4&gt;主要发现&lt;/h4&gt;在DiT和SiT骨干网络上，在六个不同的目标域上进行的实验表明，DogFit在FID和FDDINOV2指标上优于之前的引导方法，同时采样所需的TFLOPS减少高达2倍。&lt;h4&gt;结论&lt;/h4&gt;DogFit是一种有效的扩散迁移学习引导机制，能够在保持可控性的同时不增加额外的计算开销，实现更高效的保真度-多样性权衡。&lt;h4&gt;翻译&lt;/h4&gt;将扩散模型迁移到较小的目标域具有挑战性，因为简单地微调模型通常会导致泛化能力差。测试时引导方法通过在图像保真度和样本多样性之间进行权衡，有助于缓解这一问题，但这些方法通常需要双向前向传递，计算成本高。我们提出了Domain-guided Fine-tuning (DogFit)方法，这是一种用于扩散迁移学习的有效引导机制，能够在保持可控性的同时不增加额外的计算开销。DogFit将领域感知的引导偏置注入训练损失中，在微调过程中内部化引导行为。领域感知设计基于我们的观察：在微调过程中，无条件源模型比目标模型提供更强的边际估计。为了在推理时支持高效的保真度-多样性权衡，我们通过轻量级条件机制将引导强度值编码为额外模型输入。我们进一步研究了训练过程中引导偏置的最佳位置和时间，并提出了两种简单的调度策略，即late-start和cut-off，这些策略提高了生成质量和训练稳定性。在DiT和SiT骨干网络上，在六个不同的目标域上进行的实验表明，DogFit在FID和FDDINOV2指标上优于之前的引导方法，同时采样所需的TFLOPS减少高达2倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning of diffusion models to smaller target domains ischallenging, as naively fine-tuning the model often results in poorgeneralization. Test-time guidance methods help mitigate this by offeringcontrollable improvements in image fidelity through a trade-off with samplediversity. However, this benefit comes at a high computational cost, typicallyrequiring dual forward passes during sampling. We propose the Domain-guidedFine-tuning (DogFit) method, an effective guidance mechanism for diffusiontransfer learning that maintains controllability without incurring additionalcomputational overhead. DogFit injects a domain-aware guidance offset into thetraining loss, effectively internalizing the guided behavior during thefine-tuning process. The domain-aware design is motivated by our observationthat during fine-tuning, the unconditional source model offers a strongermarginal estimate than the target model. To support efficient controllablefidelity-diversity trade-offs at inference, we encode the guidance strengthvalue as an additional model input through a lightweight conditioningmechanism. We further investigate the optimal placement and timing of theguidance offset during training and propose two simple scheduling strategies,i.e., late-start and cut-off, which improve generation quality and trainingstability. Experiments on DiT and SiT backbones across six diverse targetdomains show that DogFit can outperform prior guidance methods in transferlearning in terms of FID and FDDINOV2 while requiring up to 2x fewer samplingTFLOPS.</description>
      <author>example@mail.com (Yara Bahram, Mohammadhadi Shateri, Eric Granger)</author>
      <guid isPermaLink="false">2508.05685v1</guid>
      <pubDate>Mon, 11 Aug 2025 14:47:11 +0800</pubDate>
    </item>
    <item>
      <title>GAP: Gaussianize Any Point Clouds with Text Guidance</title>
      <link>http://arxiv.org/abs/2508.05631v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025. Project page: https://weiqi-zhang.github.io/GAP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GAP的新方法，能够将原始点云转换为具有文本引导的高保真3D高斯表示，解决了从无颜色3D点云直接生成高斯表示的挑战。&lt;h4&gt;背景&lt;/h4&gt;3D Gaussian Splatting在快速高质量渲染方面显示出优势，点云是广泛使用且易于获取的3D表示形式，连接点云和高斯表示之间的差距变得越来越重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，能够将无颜色的原始点云转换为具有文本引导的高保真3D高斯表示。&lt;h4&gt;方法&lt;/h4&gt;设计了一个多视图优化框架，利用深度感知图像扩散模型合成不同视角下的一致外观；引入表面锚定机制约束高斯表示在3D形状表面上；采用基于扩散的修复策略补全难以观察的区域。&lt;h4&gt;主要发现&lt;/h4&gt;GAP方法能够有效地从各种复杂度的点云（从合成点云到真实世界扫描，甚至大规模场景）生成高质量的3D高斯表示。&lt;h4&gt;结论&lt;/h4&gt;GAP成功解决了从无颜色3D点云直接生成高斯表示的挑战，为点云到高斯表示的转换提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯飞溅已展现出在实现快速高质量渲染方面的优势。由于点云是一种广泛使用且易于获取的3D表示形式，连接点云与高斯表示之间的差距变得越来越重要。最近的研究探索了如何将着色点转换为高斯表示，但直接从无颜色3D点云生成高斯表示仍然是一个未解决的挑战。在本文中，我们提出GAP，一种新颖的方法，能够将原始点云转换为具有文本引导的高保真3D高斯表示。我们的核心思想是设计一个多视图优化框架，利用深度感知图像扩散模型在不同视点间合成一致的外观。为确保几何准确性，我们引入了一种表面锚定机制，在优化过程中有效约束高斯表示位于3D形状的表面上。此外，GAP采用基于扩散的修复策略，专门针对难以观察的区域进行补全。我们在从合成点云到具有挑战性的真实世界扫描，甚至大规模场景的不同复杂度级别上，对GAP在点到高斯生成任务上进行了评估。项目页面：https://weiqi-zhang.github.io/GAP。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从原始点云（无颜色信息）生成高质量3D高斯表示的问题。这个问题很重要，因为点云是3D计算机视觉中广泛使用且易于获取的表示形式，而3D高斯溅射(3DGS)能提供快速高质量的渲染能力。目前已有方法只能处理有颜色的点云，无法直接从无颜色点云生成高斯，限制了点云在3D内容创作中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到点云到高斯转换的重要性及现有方法的局限性，然后提出利用文本指导来生成外观同时保持几何准确性。他们设计了多视角优化框架，结合深度感知图像扩散模型和表面锚定机制。该方法借鉴了3DGS作为基础表示技术，利用深度感知扩散模型进行外观生成，采用UDF学习点云几何表示，并参考了2DGS和现有的点云到高斯转换方法，但解决了它们的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合点云几何信息和文本到图像扩散模型的外观指导，将原始点云转换为高质量3D高斯。整体流程包括：1)高斯初始化，将点云坐标作为高斯中心并利用法线初始化旋转；2)多视角修复和更新，使用深度感知扩散模型生成一致外观；3)高斯优化，通过表面锚定机制确保几何准确性；4)基于扩散的高斯修复，处理难以观察的区域，利用可见高斯的空间关系完成未见区域。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个从无颜色原始点云生成高质量3D高斯的框架；2)多视角优化框架确保外观一致性；3)表面锚定机制提高几何准确性；4)基于扩散的高斯修复策略处理难以观察区域。相比之前工作，GAP不需要RGB点云输入，能生成更多样化高质量外观，避免UV参数化问题，直接在3D空间优化高斯，更好地保持几何细节和外观一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GAP首次实现了从无颜色原始点云通过文本指导生成高质量、高保真的3D高斯表示，bridging了广泛使用的点云数据与高质量3D高斯表示之间的差距。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has demonstrated its advantages in achievingfast and high-quality rendering. As point clouds serve as a widely-used andeasily accessible form of 3D representation, bridging the gap between pointclouds and Gaussians becomes increasingly important. Recent studies haveexplored how to convert the colored points into Gaussians, but directlygenerating Gaussians from colorless 3D point clouds remains an unsolvedchallenge. In this paper, we propose GAP, a novel approach that gaussianizesraw point clouds into high-fidelity 3D Gaussians with text guidance. Our keyidea is to design a multi-view optimization framework that leverages adepth-aware image diffusion model to synthesize consistent appearances acrossdifferent viewpoints. To ensure geometric accuracy, we introduce asurface-anchoring mechanism that effectively constrains Gaussians to lie on thesurfaces of 3D shapes during optimization. Furthermore, GAP incorporates adiffuse-based inpainting strategy that specifically targets at completinghard-to-observe regions. We evaluate GAP on the Point-to-Gaussian generationtask across varying complexity levels, from synthetic point clouds tochallenging real-world scans, and even large-scale scenes. Project Page:https://weiqi-zhang.github.io/GAP.</description>
      <author>example@mail.com (Weiqi Zhang, Junsheng Zhou, Haotian Geng, Wenyuan Zhang, Yu-Shen Liu)</author>
      <guid isPermaLink="false">2508.05631v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
  <item>
      <title>Point cloud segmentation for 3D Clothed Human Layering</title>
      <link>http://arxiv.org/abs/2508.05531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'clothed human layering'的3D点云分割新范式，用于解决3D布料建模中的语义信息缺失问题，能够同时处理多个重叠的服装层。&lt;h4&gt;背景&lt;/h4&gt;3D布料建模和模拟在时尚、娱乐和动画等领域对创建虚拟化身至关重要，但高质量结果的实现面临挑战。3D扫描虽然准确但缺乏语义信息，而现有的3D形状分割方法主要面向场景理解而非建模。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的3D点云分割范式，使每个3D点可以同时关联到不同的层，从而估计底层身体部分和被上层服装遮挡的不可见区域。&lt;h4&gt;方法&lt;/h4&gt;创建模拟逼真3D扫描的合成数据集，包含服装层的真实标签；提出并评估了处理3D服装分层的不同神经网络设置，考虑了从粗粒度到细粒度的每层服装识别。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，在服装领域引入适当的分割策略，无论是在合成数据集还是真实世界扫描数据集上，都能带来显著益处。&lt;h4&gt;结论&lt;/h4&gt;提出的'clothed human layering'分割范式解决了传统方法只能提供不相交集合的问题，能够有效处理多个重叠的服装层，为3D布料建模提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;3D布料建模和模拟在时尚、娱乐和动画等多个领域的虚拟化身创建中至关重要。由于着装人体的巨大变异性，特别是逼真皱纹的生成，实现高质量结果具有挑战性。3D扫描提供更准确的真实世界物体表示，但缺乏可以通过可靠的语义重建管道推断的语义信息。为此，形状分割在识别语义形状部分中起着关键作用。然而，当前的3D形状分割方法是为场景理解和解释而设计的，只有少量工作致力于建模。在着装人体建模的背景下，分割是完全语义形状部分重建的初步步骤，即底层身体和相关服装。这些部分代表具有强重叠的多个层，与提供不相交集合的标准分割方法形成对比。在这项工作中，我们提出了一种新的3D点云分割范式，其中每个3D点可以同时关联到不同的层。这样，我们可以估计底层身体部分和不可见的着装区域，即被上层着装遮挡的布料部分。我们将这种分割范式命名为'clothed human layering'。我们创建了一个新的合成数据集，模拟非常逼真的3D扫描，并包含相关服装层的真实标签。我们提出并评估了不同的神经网络设置来处理3D服装分层。我们考虑了从粗粒度到细粒度的每层服装识别。我们的实验证明，在服装领域引入适当的分割策略，无论是在合成数据集还是真实世界扫描数据集上，都能带来益处。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D穿衣人体点云的分层分割问题。传统分割方法只能提供互不重叠的分割结果，而穿衣人体建模需要处理身体和服装之间的重叠关系。这个问题在时尚设计、虚拟试衣、动画制作等领域非常重要，因为它能帮助更准确地重建3D人体模型，区分底层身体和上层服装，包括那些被遮挡但仍存在的服装部分。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统3D分割方法是为场景理解设计的，不适用于穿衣人体建模。他们提出'穿衣人体分层'的新概念，允许每个点同时属于多个层次。作者借鉴了现有的点云分割网络如PointNet++、DGCNN和Point Transformer作为特征提取器，并参考了CLOTH3D数据集和SMPL人体模型。他们创建了新的合成数据集模拟真实扫描，并设计了五种不同的策略来处理多层次分割，包括粗略和细粒度分割，以及隐式和显式重叠处理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是一种新的3D点云分割范式，称为'穿衣人体分层'，其中每个3D点可以同时关联到不同的层次（如身体、裤子、T恤等），即使只有部分层次是可见的。整体流程包括：1)创建模拟真实扫描的合成数据集，包含多层次的标签；2)使用PointNet++、DGCNN或Point Transformer作为特征提取器；3)设计五种不同的分割策略处理多层次问题；4)训练和评估模型，使用IoU等指标；5)在真实数据上验证方法的泛化能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'穿衣人体分层'新分割范式，允许点属于多个层次；2)创建包含真实重叠区域标签的新合成数据集；3)设计五种多层次分割策略；4)扩展现有点云分割网络处理多维度标签。相比之前工作，不同之处在于：传统方法提供互斥分割，每个点只属于一个类别；而本文方法处理非互斥分割，能显式识别和处理服装重叠区域，包括可见和不可见部分，更适合穿衣人体建模需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了'穿衣人体分层'的新3D点云分割范式，通过允许每个点同时属于多个层次，有效解决了传统分割方法无法处理穿衣人体建模中服装重叠区域的问题，并创建了相应的数据集和评估方法来支持这一创新。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Cloth modeling and simulation is essential for avatars creation in severalfields, such as fashion, entertainment, and animation. Achieving high-qualityresults is challenging due to the large variability of clothed body especiallyin the generation of realistic wrinkles. 3D scan acquisitions provide moreaccuracy in the representation of real-world objects but lack semanticinformation that can be inferred with a reliable semantic reconstructionpipeline. To this aim, shape segmentation plays a crucial role in identifyingthe semantic shape parts. However, current 3D shape segmentation methods aredesigned for scene understanding and interpretation and only few work isdevoted to modeling. In the context of clothed body modeling the segmentationis a preliminary step for fully semantic shape parts reconstruction namely theunderlying body and the involved garments. These parts represent several layerswith strong overlap in contrast with standard segmentation methods that providedisjoint sets. In this work we propose a new 3D point cloud segmentationparadigm where each 3D point can be simultaneously associated to differentlayers. In this fashion we can estimate the underlying body parts and theunseen clothed regions, i.e., the part of a cloth occluded by the clothed-layerabove. We name this segmentation paradigm clothed human layering. We create anew synthetic dataset that simulates very realistic 3D scans with the groundtruth of the involved clothing layers. We propose and evaluate different neuralnetwork settings to deal with 3D clothing layering. We considered both coarseand fine grained per-layer garment identification. Our experiments demonstratesthe benefit in introducing proper strategies for the segmentation on thegarment domain on both the synthetic and real-world scan datasets.</description>
      <author>example@mail.com (Davide Garavaso, Federico Masi, Pietro Musoni, Umberto Castellani)</author>
      <guid isPermaLink="false">2508.05531v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Symmetry Understanding of 3D Shapes via Chirality Disentanglement</title>
      <link>http://arxiv.org/abs/2508.05505v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于Diff3F框架的无监督手性特征提取方法，用于从2D基础模型中提取手性感知信息，解决形状分析中无法区分左右对称部分的问题。&lt;h4&gt;背景&lt;/h4&gt;手性信息（区分左右的信息）在计算机视觉的各种数据模式中普遍存在。虽然手性在图像领域已被广泛研究，但在形状分析（如点云和网格）中的探索仍然不足。许多形状顶点描述符虽然具有良好特性，但无法区分左右对称部分。&lt;h4&gt;目的&lt;/h4&gt;考虑到手性信息在不同形状分析问题中的普遍性以及当前形状描述符中缺乏手性感知特征，开发手性特征提取器变得必要和紧迫。&lt;h4&gt;方法&lt;/h4&gt;基于Diff3F框架，提出了一种无监督的手性特征提取流程，从2D基础模型中提取手性感知信息来装饰形状顶点。&lt;h4&gt;主要发现&lt;/h4&gt;通过定量和定性实验评估了提取的手性特征，在下游任务包括左右解纠缠、形状匹配和部分分割中证明了其有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;开发的手性特征提取器能够有效区分形状中的左右对称部分，为形状分析提供了新的特征表示方法，具有重要的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;手性信息（即能够区分左右的信息）在计算机视觉的各种数据模式中普遍存在，包括图像、视频、点云和网格。虽然手性在图像领域已被广泛研究，但在形状分析（如点云和网格）中的探索仍然不足。尽管许多形状顶点描述符具有吸引人的特性（例如对刚体变换的鲁棒性），但它们通常无法区分左右对称部分。考虑到手性信息在不同形状分析问题中的普遍性以及当前形状描述符中缺乏手性感知特征，开发手性特征提取器变得必要和紧迫。基于最近的Diff3F框架，我们提出了一种无监督的手性特征提取流程，从2D基础模型中提取手性感知信息来装饰形状顶点。我们通过各种数据集的定量和定性实验评估了提取的手性特征。在下游任务（包括左右解纠缠、形状匹配和部分分割）中的结果证明了它们的有效性和实用性。项目页面：https://wei-kang-wang.github.io/chirality/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D形状分析中无法区分左右对称部分的问题。这个问题很重要，因为在形状匹配、变形检测和对称平面分析等任务中，能够区分左右对称部分的能力至关重要。当前的方法虽然能处理几何特征，但往往无法区分左右，导致匹配不准确和理解不完整，限制了3D形状分析在机器人、医学成像和增强现实等领域的应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到对称性和手性是同一枚硬币的两面：对称性强调相似性，手性强调差异性。他们注意到2D图像领域的手性研究已较成熟，但3D形状分析中缺乏类似方法。作者基于Diff3F框架，该框架利用2D基础模型从多视角图像中提取特征。他们借鉴了图像领域的手性概念和Zhang等人的工作，通过水平翻转图像创建手性图像对，并利用这些图像对训练网络区分左右。整个方法是无监督的，不需要左右标注的监督信号。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过水平翻转图像创建手性图像对，并从这些图像对中提取特征来训练网络区分形状的左右部分。整体流程包括：1) 输入未纹理化的3D网格；2) 从多个视角渲染并生成纹理图像；3) 创建水平翻转的图像对；4) 使用预训练的DINO-V2和StableDiffusion模型提取特征；5) 将特征投影回3D顶点；6) 训练手性网络将特征对转换为手性特征；7) 使用多种损失函数确保特征能够准确区分左右。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次在形状分析领域引入无监督的手性特征提取方法；2) 提供能够区分左右对称部分的顶点描述符；3) 展示手性特征在形状匹配和分割等任务中的有效性；4) 证明方法在部分形状和各向异性形状上的鲁棒性。相比之前的工作，本文关注手性而非对称性，解决了现有方法无法区分左右的问题，且能与现有顶点描述符结合增强其手性感知能力，无需监督信号即可工作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种无监督方法，通过解离2D基础模型中的手性信息，为3D形状提供能够区分左右的手性感知特征，显著提升了形状匹配和分割等任务的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chirality information (i.e. information that allows distinguishing left fromright) is ubiquitous for various data modes in computer vision, includingimages, videos, point clouds, and meshes. While chirality has been extensivelystudied in the image domain, its exploration in shape analysis (such as pointclouds and meshes) remains underdeveloped. Although many shape vertexdescriptors have shown appealing properties (e.g. robustness to rigid-bodytransformations), they are often not able to disambiguate between left andright symmetric parts. Considering the ubiquity of chirality information indifferent shape analysis problems and the lack of chirality-aware featureswithin current shape descriptors, developing a chirality feature extractorbecomes necessary and urgent. Based on the recent Diff3F framework, we proposean unsupervised chirality feature extraction pipeline to decorate shapevertices with chirality-aware information, extracted from 2D foundation models.We evaluated the extracted chirality features through quantitative andqualitative experiments across diverse datasets. Results from downstream tasksincluding left-right disentanglement, shape matching, and part segmentationdemonstrate their effectiveness and practical utility. Project page:https://wei-kang-wang.github.io/chirality/</description>
      <author>example@mail.com (Weikang Wang, Tobias Weißberg, Nafie El Amrani, Florian Bernard)</author>
      <guid isPermaLink="false">2508.05505v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Learning Geometric-Aware Quadrature Rules for Functional Minimization</title>
      <link>http://arxiv.org/abs/2508.05445v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为QuadrANN的图神经网络架构，用于在非均匀点云上进行精确的数值积分，解决了现代无网格机器学习求解器在变分原理下处理偏微分方程时面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;现代基于变分原理的无网格机器学习求解器在非均匀点云上进行精确数值积分是一个挑战。标准的蒙特卡洛方法无法处理非均匀点云，而现代神经网络架构可以处理排列不变的输入。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够学习从点云底层几何结构直接获取最优求积权重的图神经网络架构。&lt;h4&gt;方法&lt;/h4&gt;QuadrANN采用深度消息传递方案，初始层编码绝对和相对位置以及显式局部密度度量的丰富局部几何特征，后续层则整合全局上下文向量，生成对排列不变且能适应局部点密度和整体域形状的数据驱动求积规则。&lt;h4&gt;主要发现&lt;/h4&gt;在一系列具有挑战性的测试案例中，QuadrANN通过将点云扭曲以在积分函数呈现某些奇异性的关键区域变得更密集，与标准准蒙特卡洛方法相比，减少了积分估计的方差。&lt;h4&gt;结论&lt;/h4&gt;QuadrANN在关键区域增强的稳定性对于能量泛函的优化至关重要，能够改进基于变分原理的深度学习求解器。&lt;h4&gt;翻译&lt;/h4&gt;在非均匀点云上进行精确的数值积分是现代使用变分原理的无网格机器学习求解器求解偏微分方程(PDEs)的挑战。虽然标准蒙特卡洛(MC)方法无法处理非均匀点云，但现代神经网络架构可以处理排列不变的输入，为任何点云创建求积规则。在这项工作中，我们引入了QuadrANN，这是一种图神经网络(GNN)架构，旨在直接从点云的底层几何结构学习最优求积权重。该模型的设计利用了深度消息传递方案，其中初始层从绝对和相对位置以及显式局部密度度量中编码丰富的局部几何特征。相比之下，后续层整合了全局上下文向量。这些架构选择使得QuadrANN能够生成数据驱动的求积规则，该规则对排列不变并能适应局部点密度和整体域形状。我们在一系列具有挑战性的测试案例上测试了我们的方法，包括在凸和非凸域上的积分以及估计热方程和Fokker-Planck方程的解。在所有测试中，与标准准蒙特卡洛方法相比，QuadrANN通过将点云扭曲以在积分函数呈现某些奇异性的关键区域变得更密集，从而减少了积分估计的方差。这对于当前域关键区域的增强稳定性对于能量泛函的优化至关重要，从而改进了基于变分原理的深度学习求解器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate numerical integration over non-uniform point clouds is a challengefor modern mesh-free machine learning solvers for partial differentialequations (PDEs) using variational principles. While standard Monte Carlo (MC)methods are not capable of handling a non-uniform point cloud, modern neuralnetwork architectures can deal with permutation-invariant inputs, creatingquadrature rules for any point cloud. In this work, we introduce QuadrANN, aGraph Neural Network (GNN) architecture designed to learn optimal quadratureweights directly from the underlying geometry of point clouds. The design ofthe model exploits a deep message-passing scheme where the initial layerencodes rich local geometric features from absolute and relative positions aswell as an explicit local density measure. In contrast, the following layersincorporate a global context vector. These architectural choices allow theQuadrANN to generate a data-driven quadrature rule that ispermutation-invariant and adaptive to both local point density and the overalldomain shape. We test our methodology on a series of challenging test cases,including integration on convex and non-convex domains and estimating thesolution of the Heat and Fokker-Planck equations. Across all the tests,QuadrANN reduces the variance of the integral estimation compared to standardQuasi-Monte Carlo methods by warping the point clouds to be more dense incritical areas where the integrands present certain singularities. Thisenhanced stability in critical areas of the domain at hand is critical for theoptimization of energy functionals, leading to improved deep learning-basedvariational solvers.</description>
      <author>example@mail.com (Costas Smaragdakis)</author>
      <guid isPermaLink="false">2508.05445v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding</title>
      <link>http://arxiv.org/abs/2508.05269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了B4DL基准测试，专门用于4D LiDAR理解的多模态大语言模型训练与评估，并提出首个可直接处理原始4D LiDAR的MLLM模型，为动态户外环境中的时空推理提供统一解决方案。&lt;h4&gt;背景&lt;/h4&gt;理解动态户外环境需捕捉复杂物体交互及时间演变，基于LiDAR的4D点云提供精确空间几何和丰富时间线索，是表示真实世界场景的理想选择。然而，由于缺乏高质量特定模态注释和处理高维组成的MLLM架构，4D LiDAR在MLLMs中应用不足。&lt;h4&gt;目的&lt;/h4&gt;解决4D LiDAR在MLLMs应用中面临的挑战，包括缺乏高质量特定模态注释和处理高维数据的MLLM架构。&lt;h4&gt;方法&lt;/h4&gt;1. 引入B4DL基准测试，专门用于4D LiDAR理解的MLLM训练与评估；2. 提出可扩展的数据生成管道；3. 开发可直接处理原始4D LiDAR并与语言理解结合的MLLM模型。&lt;h4&gt;主要发现&lt;/h4&gt;B4DL基准测试可有效训练评估MLLMs在4D LiDAR理解能力；提出的数据生成管道可生成高质量4D LiDAR数据；开发的MLLM模型可直接处理原始4D LiDAR并与语言理解结合；模型为动态户外环境中的时空推理提供统一解决方案。&lt;h4&gt;结论&lt;/h4&gt;通过B4DL基准测试、数据生成管道和MLLM模型的结合，为4D LiDAR在MLLMs中的应用提供了全面解决方案，解决了缺乏高质量注释和合适架构的问题。&lt;h4&gt;翻译&lt;/h4&gt;理解动态户外环境需要捕捉复杂物体的交互及其随时间的演变。基于LiDAR的4D点云提供了精确的空间几何和丰富的时间线索，使其成为表示真实世界场景的理想选择。然而，尽管4D LiDAR有潜力，但由于缺乏高质量、特定模态的注释以及缺乏能够处理其高维组成的MLLM架构，它在MLLMs的背景下仍然探索不足。为了解决这些挑战，我们引入了B4DL，一个专门为在4D LiDAR理解上训练和评估MLLMs而设计的新基准测试。此外，我们提出了一个可扩展的数据生成管道和一个MLLM模型，首次直接通过将原始4D LiDAR与语言理解相结合来处理它。结合我们的数据集和基准测试，我们的模型为动态户外环境中的时空推理提供了统一的解决方案。我们在https://mmb4dl.github.io/mmb4dl/上提供了渲染的4D LiDAR视频、生成的数据集和不同场景上的推理输出。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决4D LiDAR（随时间变化的三维点云数据）在多模态大语言模型(MLLMs)中的应用不足问题。这个问题很重要，因为理解动态户外环境需要捕捉复杂物体交互及其随时间的演变，而4D LiDAR能提供精确的空间几何和丰富的时间线索，是表示现实世界场景的理想方式。解决这一问题将使AI系统能更好地理解和推理动态现实世界环境，对自动驾驶、机器人等领域具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到4D LiDAR在MLLMs中的应用存在两大挑战：缺乏高质量特定模态的标注数据和现有MLLM架构无法处理高维4D数据。为此，他们设计了三部分解决方案：创建B4DL基准测试、开发数据生成管道、构建专门的MLLM模型。作者借鉴了现有工作如CLIP模型进行跨模态对齐，参考了LiDARCLIP处理点云的方法，使用LoRA模块进行高效训练，并吸收了视频理解模型的时间建模思路，但将其扩展到3D空间场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个专门用于4D LiDAR理解的综合框架，使MLLM能够直接处理高维时空数据并进行推理。整体流程包括：1)数据生成管道，利用多视图图像和GPT-4o从LiDAR生成高质量QA对；2)模型架构，包含LiDAR点云编码器、LiDAR对齐器和元令牌三个关键组件；3)两阶段训练策略，先学习静态3D空间特征，再扩展到4D时空理解；4)定义六种评估任务，涵盖从简单存在性检测到复杂时空推理的多种能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门针对4D LiDAR的B4DL基准测试；2)可扩展的数据生成管道；3)包含178.4k个QA对的B4DL数据集；4)能直接处理原始4D LiDAR的B4DL模型；5)阶段式训练策略。相比之前工作，B4DL不仅处理静态3D场景，还能同时建模空间和时间维度；不仅通过图像作为中介，还能直接处理原始LiDAR数据；不仅提供物体位置信息，还整合了传感器元数据和物体运动轨迹；不仅评估简单问答，还包含复杂的时空理解和综合推理任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了B4DL，一个包含数据集、基准测试和模型的综合框架，使多模态大语言模型能够首次直接处理4D LiDAR数据，实现时空理解并推理动态户外环境中的物体交互和演变。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3755074&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding dynamic outdoor environments requires capturing complex objectinteractions and their evolution over time. LiDAR-based 4D point clouds provideprecise spatial geometry and rich temporal cues, making them ideal forrepresenting real-world scenes. However, despite their potential, 4D LiDARremains underexplored in the context of Multimodal Large Language Models(MLLMs) due to the absence of high-quality, modality-specific annotations andthe lack of MLLM architectures capable of processing its high-dimensionalcomposition. To address these challenges, we introduce B4DL, a new benchmarkspecifically designed for training and evaluating MLLMs on 4D LiDARunderstanding. In addition, we propose a scalable data generation pipeline andan MLLM model that, for the first time, directly processes raw 4D LiDAR bybridging it with language understanding. Combined with our dataset andbenchmark, our model offers a unified solution for spatio-temporal reasoning indynamic outdoor environments. We provide rendered 4D LiDAR videos, generateddataset, and inference outputs on diverse scenarios at:https://mmb4dl.github.io/mmb4dl/</description>
      <author>example@mail.com (Changho Choi, Youngwoo Shin, Gyojin Han, Dong-Jae Lee, Junmo Kim)</author>
      <guid isPermaLink="false">2508.05269v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning Based Dynamic Environment Reconstruction for Vehicular ISAC Scenarios</title>
      <link>http://arxiv.org/abs/2508.05226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于深度学习的ISAC框架，用于智能交通系统中的车辆环境重建，通过多阶段网络实现高质量动态环境重建，减少对传统传感器的依赖，降低成本。&lt;h4&gt;背景&lt;/h4&gt;ISAC技术在智能交通系统中扮演关键角色，通过重用无线信号使车辆感知和重建周围环境，可减少或消除对LiDAR或雷达等额外传感器的需求。然而，现有ISAC重建方法往往缺乏足够的精度和时间一致性来跟踪动态场景，限制了现实世界应用。&lt;h4&gt;目的&lt;/h4&gt;解决现有ISAC重建方法在动态场景跟踪方面的不足，提出一种基于深度学习的框架，利用ISAC通道进行车辆环境重建，提高重建质量和实时性。&lt;h4&gt;方法&lt;/h4&gt;建立基于真实城市街道场景多模态测量的联合通道环境数据集；开发多阶段深度学习网络，包括：场景解码器识别环境上下文（如建筑物、树木）；聚类中心解码器预测粗略空间布局；点云解码器恢复周围环境的细粒度几何和结构。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法实现了高质量的动态环境重建，查姆弗距离为0.29，F Score@1%为0.87。复杂度分析证明了该方法在实时场景中的效率和实际适用性。&lt;h4&gt;结论&lt;/h4&gt;这项工作为基于ISAC的低成本环境重建提供了有效途径，适用于未来智能交通系统，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;集成感知与通信（ISAC）技术通过重用无线信号使车辆能够感知和重建周围环境，在未来智能交通系统中发挥着关键作用，从而减少甚至消除对LiDAR或雷达等额外传感器的需求。然而，现有的基于ISAC的重建方法通常缺乏足够的精度和时间一致性来跟踪动态场景，限制了其在现实世界中的适用性。为解决这一局限，我们提出了一种基于深度学习的框架，利用ISAC通道进行车辆环境重建。我们首先基于真实城市街道场景的多模态测量建立了联合通道环境数据集。然后，开发了多阶段深度学习网络来重建环境。具体而言，场景解码器识别建筑物、树木等环境上下文；聚类中心解码器通过定位主要散射中心来预测粗略空间布局；点云解码器恢复周围环境的细粒度几何和结构。实验结果表明，提出的方法实现了高质量的动态环境重建，查姆弗距离为0.29，F Score@1%为0.87。此外，复杂度分析证明了该方法在实时场景中的效率和实际适用性。这项工作为未来智能交通系统基于ISAC的低成本环境重建提供了途径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有ISAC（集成感知与通信）环境重建方法在动态场景中缺乏足够准确性和时间一致性的问题。这个问题很重要，因为ISAC技术对未来智能交通系统至关重要，能让车辆通过重用无线信号感知周围环境，减少对昂贵LiDAR或雷达传感器的依赖。然而，车辆行驶时环境快速变化、观测角度不断变化，这对感知精度和时间一致性提出了更高要求，限制了ISAC技术在自动驾驶等实际场景中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了无线通信信道包含周围环境丰富信息但具有高维抽象特性的挑战。受人类视觉系统感知场景过程的启发（先识别物体数量，再定位中心，最后细化形状），作者设计了多阶段重建框架。作者借鉴了现有深度学习处理通道特征的研究，如使用Vision Transformer处理多通道时空特征、利用Transformer提取通道变化影响等。在此基础上，作者建立了基于真实世界测量的数据集，并设计了MSCR-Net网络，采用渐进式训练策略依次实现场景分类、聚类中心定位和点云生成。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用ISAC通道数据驱动的深度学习方法，通过多阶段重建框架从稀疏通道参数中恢复完整3D环境点云。整体流程包括：1)数据采集：使用车载ISAC和LiDAR测量平台同步获取通道数据和点云数据；2)数据预处理：提取通道参数并处理点云；3)数据集构建：将通道数据和点云数据配对并标注场景类别；4)模型训练：分三阶段训练MSCR-Net，分别预测场景类别、聚类中心位置和点云细节；5)环境重建：输入通道参数，输出重建的3D环境点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出基于深度学习的动态环境重建方法，由ISAC通道数据驱动；2)构建基于真实世界城市街道场景多模态测量的联合信道-环境数据集；3)开发多阶段通道到重建网络(MSCR-Net)，包含场景解码器、聚类中心解码器和点云解码器。相比之前工作，本文使用真实道路场景数据而非模拟数据，能够处理高动态交通场景，重建质量更高(Chamfer Distance为0.29，F-Score@1%为0.87)，且在复杂环境中表现出更好的稳定性和准确性，解决了传统方法在动态场景下性能下降的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于深度学习的动态环境重建框架，通过利用ISAC通道数据，实现了高效、准确的车辆周围3D环境重建，为未来低成本智能交通系统提供了新途径。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrated Sensing and Communication (ISAC) technology plays a critical rolein future intelligent transportation systems, by enabling vehicles to perceiveand reconstruct the surrounding environment through reuse of wireless signals,thereby reducing or even eliminating the need for additional sensors such asLiDAR or radar. However, existing ISAC based reconstruction methods often lackthe ability to track dynamic scenes with sufficient accuracy and temporalconsistency, limiting the real world applicability. To address this limitation,we propose a deep learning based framework for vehicular environmentreconstruction by using ISAC channels. We first establish a joint channelenvironment dataset based on multi modal measurements from real world urbanstreet scenarios. Then, a multistage deep learning network is developed toreconstruct the environment. Specifically, a scene decoder identifies theenvironmental context such as buildings, trees and so on; a cluster centerdecoder predicts coarse spatial layouts by localizing dominant scatteringcenters; a point cloud decoder recovers fine grained geometry and structure ofsurrounding environments. Experimental results demonstrate that the proposedmethod achieves high-quality dynamic environment reconstruction with a ChamferDistance of 0.29 and F Score@1% of 0.87. In addition, complexity analysisdemonstrates the efficiency and practical applicability of the method in realtime scenarios. This work provides a pathway toward low cost environmentreconstruction based on ISAC for future intelligent transportation.</description>
      <author>example@mail.com (Junzhe Song, Ruisi He, Mi Yang, Zhengyu Zhang, Bingcheng Liu, Jiahui Han, Haoxiang Zhang, Bo Ai)</author>
      <guid isPermaLink="false">2508.05226v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Refining Gaussian Splatting: A Volumetric Densification Approach</title>
      <link>http://arxiv.org/abs/2508.05187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种改进的3D高斯溅射中的点原素管理方法，利用惯性体积指导细化过程，并研究了不同点云初始化方法的效果，在多种场景下提升了重建质量。&lt;h4&gt;背景&lt;/h4&gt;在3D高斯溅射(3DGS)中实现高质量的新视角合成通常依赖于有效的点原素管理。底层的自适应密度控制(ADC)过程通过自动化增密和修剪解决了这个问题。&lt;h4&gt;目的&lt;/h4&gt;解决传统3DGS增密策略的关键缺陷，提高3D场景重建的质量。&lt;h4&gt;方法&lt;/h4&gt;引入了一种新的密度控制方法，利用与每个高斯函数相关的惯性体积来指导细化过程；同时研究了传统运动结构(SfM)和深度图像匹配(DIM)方法对点云初始化的影响。&lt;h4&gt;主要发现&lt;/h4&gt;在Mip-NeRF 360数据集上的大量实验评估表明，该方法在重建质量上超越了原始3DGS，在不同场景下都取得了有希望的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过改进密度控制和点云初始化，有效地提高了3D高斯溅射的重建质量。&lt;h4&gt;翻译&lt;/h4&gt;在3D高斯溅射(3DGS)中实现高质量的新视角合成通常依赖于有效的点原素管理。底层的自适应密度控制(ADC)过程通过自动化增密和修剪解决了这个问题。然而，传统的3DGS增密策略存在关键缺陷。为了解决这个问题，在本文中我们介绍了一种新的密度控制方法，它利用与每个高斯函数相关的惯性体积来指导细化过程。此外，我们还研究了传统运动结构(SfM)和深度图像匹配(DIM)方法对点云初始化的影响。在Mip-NeRF 360数据集上的大量实验评估表明，我们的方法在重建质量上超越了3DGS，在各种场景下都取得了有希望的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D高斯飞溅(3DGS)方法中自适应密度控制机制在处理低纹理区域时的不足问题，导致这些区域出现模糊和细节缺失。这个问题很重要，因为高质量的3D场景重建和新视角合成是AR/VR/MR、机器人技术和电影制作等应用的基础，而3DGS作为一种高效实时渲染方法，如果无法有效处理低纹理区域，会限制其实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出原始3DGS方法的ADC机制在处理低纹理区域时的局限性。他们借鉴了传统的结构化运动(SfM)和深度图像匹配(DIM)方法用于点云初始化，参考了3DGS原始论文中的梯度分裂和克隆机制，但发现这些方法在处理低纹理区域时不足。基于这些观察，作者设计了一种基于体积的密度控制方法，利用每个高斯函数的惯性体积来指导细化过程，并研究了不同初始化策略对结果的影响。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过评估和分裂大体积的高斯椭球来增加点密度，解决低纹理区域的细节缺失问题。具体实现流程：1)使用SfM或DIM初始化点云并转换为3D高斯基元；2)训练阶段优化高斯属性，每100次迭代应用体积密度控制机制；3)计算每个高斯椭球体积，超过阈值(0.03)则进行分裂；4)分裂过程中考虑条件数调整新高斯大小保持形状特征；5)使用飞溅光栅化技术渲染最终图像。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于体积的密度控制机制，直接评估椭球体积指导细化；2)椭球体积评估与分裂策略，只处理大体积椭球；3)条件数调整分裂过程，保持形状特征；4)系统分析不同初始化策略(SfM和DIM)的影响。相比之前工作，本文专门针对低纹理区域的密度不足问题，采用体积驱动而非梯度或误差驱动的密度控制，并确保分裂后高斯保持原始形状特征，同时提供了初始化方法选择的指导。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于体积的3D高斯飞溅密度控制方法，通过评估和分裂大体积椭球来有效解决低纹理区域的细节缺失问题，显著提升了场景重建的视觉质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.24132/CSRN.2025-6&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS)often depends on effective point primitive management. The underlying AdaptiveDensity Control (ADC) process addresses this issue by automating densificationand pruning. Yet, the vanilla 3DGS densification strategy shows keyshortcomings. To address this issue, in this paper we introduce a novel densitycontrol method, which exploits the volumes of inertia associated to eachGaussian function to guide the refinement process. Furthermore, we study theeffect of both traditional Structure from Motion (SfM) and Deep Image Matching(DIM) methods for point cloud initialization. Extensive experimentalevaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses3DGS in reconstruction quality, delivering encouraging performance acrossdiverse scenes.</description>
      <author>example@mail.com (Mohamed Abdul Gafoor, Marius Preda, Titus Zaharia)</author>
      <guid isPermaLink="false">2508.05187v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction</title>
      <link>http://arxiv.org/abs/2508.05153v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 3 figures, 1 table. Submitted to IEEE Robotics and  Automation Letters&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为特征条件双臂价值网络(FCBV-Net)的新方法，用于提升机器人在衣物平滑操作中的类别级别策略泛化能力。该方法通过在3D点云上操作，将双臂动作价值预测基于预训练的冻结密集几何特征，从而增强对衣物类别内变化的鲁棒性。实验表明，该方法在模拟环境中表现出优越的类别级别泛化能力。&lt;h4&gt;背景&lt;/h4&gt;机器人衣物操作（如双臂平滑）由于高维度、复杂动力学和类别内变化而面临显著挑战。当前方法存在两种局限：要么使用特定实例的视觉特征导致过拟合，要么尽管实现了类别级别感知泛化，但无法预测协同双臂动作的价值。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够增强衣物平滑任务中类别级别策略泛化的方法，解决当前方法在处理类别内变化和预测双臂动作价值方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了特征条件双臂价值网络(FCBV-Net)，该方法在3D点云上操作，将双臂动作价值预测基于预训练、冻结的密集几何特征，确保对衣物类别内变化的鲁棒性，并使用可训练的下游组件学习特定任务策略，将几何理解与双臂动作价值学习解耦。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟GarmentLab实验中使用CLOTH3D数据集，FCBV-Net展现出优越的类别级别泛化能力：与未见过的衣物相比，效率仅下降11.5%（步骤80），而基于2D图像的基线下降了96.2%；达到89%的最终覆盖率，超过使用相同每点几何特征但固定基元的3D对应基线（83%覆盖率）。&lt;h4&gt;结论&lt;/h4&gt;几何理解与双臂动作价值学习的解耦能够实现更好的类别级别泛化，FCBV-Net方法在机器人衣物操作任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;对于机器人衣物操作（如双臂平滑）的类别级别泛化仍然是一个重大挑战，这源于高维度、复杂动力学和类别内变化。当前方法常常面临困境，要么使用特定实例的并发学习视觉特征导致过拟合，要么尽管实现了类别级别感知泛化，但无法预测协同双臂动作的价值。我们提出了特征条件双臂价值网络(FCBV-Net)，它在3D点云上运行，专门增强衣物平滑任务的类别级别策略泛化。FCBV-Net将双臂动作价值预测基于预训练的、冻结的密集几何特征，确保对衣物类别内变化的鲁棒性。然后，可训练的下游组件使用这些静态特征学习特定任务策略。在CLOTH3D数据集的模拟GarmentLab实验中，FCBV-Net展示了优越的类别级别泛化能力。与未见过的衣物相比，它仅表现出11.5%的效率下降（步骤80），而基于2D图像的基线下降了96.2%，并实现了89%的最终覆盖率，超过了使用相同每点几何特征但固定基元的3D对应基线的83%覆盖率。这些结果表明，几何理解与双臂动作价值学习的解耦能够实现更好的类别级别泛化。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人衣物熨平的类别级泛化问题，即让机器人能够对同一类别但未见过的衣物实例进行高效的双手协同熨平操作。这个问题在现实和研究中非常重要，因为衣物具有高变形性、复杂动力学和近乎无限的配置空间，且同类衣物在形状、大小和材料上可能有显著差异，这使得稳健的操纵变得困难。在辅助护理和自动穿衣等应用中，能够处理各种衣物类型的机器人系统具有实用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性来设计解决方案：直接学习动作值函数的方法可能过拟合，而有良好类别级泛化的方法不直接预测特定双手动作的结果质量。作者的核心假设是，将动作值预测建立在稳健的预训练每点几何特征上可以显著提高泛化能力。该方法借鉴了现有工作：使用PointNet++实现密集几何特征提取（来自[4]），采用自监督对比学习进行特征预训练，并受[5]启发使用自我监督学习信号作为奖励函数。作者结合了人工标注数据和通过自我监督交互收集的数据进行训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将几何理解与任务特定的动作值学习解耦，使用预训练的、冻结的密集几何特征作为基础，在上面学习任务特定的策略，使系统能够预测协同结果并泛化到未见过的衣物几何形状。整体流程包括：1) 使用预训练和冻结的PointNet++骨干网络从输入点云中提取每点密集几何特征；2) 通过ValueDecoderPN++网络预测初始抓取质量，使用原始选择头确定操作原语，采样候选抓取点并构建描述符，最后使用最终的双手价值预测头输出条件动作值；3) 训练可训练组件端到端，结合人类标注数据和自我监督交互数据，使用奖励函数和多个损失函数进行优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) FCBV-Net架构，使用预训练的冻结每点几何特征条件化动作价值预测；2) 解耦策略，将几何理解与任务特定的动作值学习分离；3) 直接预测协同双手动作的结果质量。相比不同工作：与2D图像基线相比，FCBV-Net使用3D点云和预训练几何特征，避免过拟合，在未见衣物上效率下降仅11.5%（对比基线的96.2%）；与3D对应基线相比，使用相同特征但学习多原语策略而非固定原语，实现更高覆盖率（89%对比83%）；与传统方法相比，不依赖手工规则，能直接预测动作价值。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FCBV-Net通过将几何理解与任务特定的动作值学习解耦，使用预训练的冻结3D特征作为基础，实现了对未见衣物类别的高效双手熨平操作，显著提高了类别级泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Category-level generalization for robotic garment manipulation, such asbimanual smoothing, remains a significant hurdle due to high dimensionality,complex dynamics, and intra-category variations. Current approaches oftenstruggle, either overfitting with concurrently learned visual features for aspecific instance or, despite category-level perceptual generalization, failingto predict the value of synergistic bimanual actions. We propose theFeature-Conditioned Bimanual Value Network (FCBV-Net), operating on 3D pointclouds to specifically enhance category-level policy generalization for garmentsmoothing. FCBV-Net conditions bimanual action value prediction on pre-trained,frozen dense geometric features, ensuring robustness to intra-category garmentvariations. Trainable downstream components then learn a task-specific policyusing these static features. In simulated GarmentLab experiments with theCLOTH3D dataset, FCBV-Net demonstrated superior category-level generalization.It exhibited only an 11.5% efficiency drop (Steps80) on unseen garmentscompared to 96.2% for a 2D image-based baseline, and achieved 89% finalcoverage, outperforming an 83% coverage from a 3D correspondence-based baselinethat uses identical per-point geometric features but a fixed primitive. Theseresults highlight that the decoupling of geometric understanding from bimanualaction value learning enables better category-level generalization.</description>
      <author>example@mail.com (Mohammed Daba, Jing Qiu)</author>
      <guid isPermaLink="false">2508.05153v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework</title>
      <link>http://arxiv.org/abs/2508.04962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in IEEE Transactions on Circuits and Systems for  Video Technology&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了HOW-Seg，首个用于开放世界点云语义分割的人机交互框架。该方法通过在查询数据上直接构建类别原型，避免支持数据和查询数据间的类内分布偏移，利用稀疏人类标注指导分割，并通过分层原型消歧机制和密集条件随场优化标签分配。&lt;h4&gt;背景&lt;/h4&gt;开放世界点云语义分割旨在预测真实场景中基础类别和新类别的点标签，但现有方法依赖资源密集型的离线增量学习或密集标注的支持数据，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提出一种更高效、更实用的开放世界点云语义分割方法，减少对密集标注数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;HOW-Seg框架包含：1)在查询数据上直接构建类别原型；2)利用稀疏人类标注作为指导；3)引入分层原型消歧机制 refine 模糊原型；4)使用密集条件随场优化标签分配；5)通过迭代人类反馈动态改进预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，使用稀疏标注(如每个新类别单击一次)，HOW-Seg在5次设置下匹配或超越了最先进的广义少样本分割方法。当使用高级骨干网络和更密集标注时，HOW-Seg在S3DIS上达到85.27% mIoU，在ScanNetv2上达到66.37% mIoU，显著优于其他替代方法。&lt;h4&gt;结论&lt;/h4&gt;HOW-Seg通过人机交互实现了高效的开放世界点云语义分割，减少了对密集标注数据的依赖，同时达到了与现有方法相当或更好的性能，显著提升了实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;开放世界点云语义分割(OW-Seg)旨在预测真实场景中基础类别和新类别的点标签。然而，现有方法依赖于资源密集型的离线增量学习或密集标注的支持数据，限制了它们的实用性。为解决这些限制，我们提出了HOW-Seg，这是首个用于OW-Seg的人机交互框架。具体来说，我们在查询数据上直接构建类别原型(基本的分割单元)，避免了支持数据和查询数据之间的类内分布偏移引起的问题。通过利用稀疏的人类标注作为指导，HOW-Seg能够实现对基础类别和新类别的基于原型的分割。考虑到初始原型的粒度不足，我们引入了分层原型消歧机制来 refine 模糊的原型，这些原型对应于不同类别的标注。为进一步增强上下文感知能力，我们在精炼的原型上应用密集条件随场(CRF)以优化其标签分配。通过迭代人类反馈，HOW-Seg动态改进其预测，实现对基础类别和新类别的高质量分割。实验证明，使用稀疏标注(如每个新类别单击一次)，HOW-Seg在5次设置下匹配或超越了最先进的广义少样本分割(GFS-Seg)方法。当使用高级骨干网络(如分层Transformer)和更密集的标注(如每个子场景10次点击)时，HOW-Seg在S3DIS上达到85.27% mIoU，在ScanNetv2上达到66.37% mIoU，显著优于其他替代方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决开放世界点云语义分割问题，即让分割系统能同时处理已知的基础类别和未知的新兴类别。这个问题在现实中非常重要，因为实际应用场景如自动驾驶、机器人导航等往往是动态开放的，新类别会不断出现，而传统封闭世界方法无法处理这种情况。现有方法依赖资源密集的离线学习或预收集的标注数据，限制了实用性，且无法处理样本间的分布偏移问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法(IL和GFS-Seg)的局限性，发现它们依赖额外标注数据且存在跨样本分布偏移问题。作者借鉴了原型理论，提出直接在查询样本特征空间构建类别原型，避免分布偏移。方法分为四个模块：原型初始化(使用K-Means聚类)、原型消歧(解决模糊原型)、原型标签分配(使用CRF优化)和基于原型的分割。作者还参考了少样本分割中的原型思想，但扩展到开放世界场景并引入人机交互机制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过人机交互机制在查询样本特征空间中构建无偏类别原型，避免跨样本分布偏移。整体流程：1)使用预训练网络提取点特征和封闭世界预测；2)通过K-Means聚类构建初始原型；3)原型消歧模块处理含多类点的模糊原型；4)基于稀疏标注和CRF优化原型标签分配；5)将标签从原型传播到点生成开放世界预测。支持迭代改进，随着更多标注提供，分割质量逐步提高。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个用于开放世界点云分割的人机交互框架，无需额外标注数据；2)在查询样本特征空间动态构建无偏类别原型；3)设计交互式原型消歧机制和基于CRF的标签分配模块；4)通过迭代人类反馈动态改进预测。相比之前工作，HOW-Seg不依赖离线增量学习或预收集支持样本，避免计算密集微调；在查询样本特征空间构建原型解决分布偏移问题；支持迭代改进提升分割质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HOW-Seg提出了一种创新的人机交互框架，通过在查询样本特征空间中构建无偏类别原型并利用稀疏标注迭代优化，实现了高效且高质量的开放世界点云语义分割，显著优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TCSVT.2025.3596238&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-world point cloud semantic segmentation (OW-Seg) aims to predict pointlabels of both base and novel classes in real-world scenarios. However,existing methods rely on resource-intensive offline incremental learning ordensely annotated support data, limiting their practicality. To address theselimitations, we propose HOW-Seg, the first human-in-the-loop framework forOW-Seg. Specifically, we construct class prototypes, the fundamentalsegmentation units, directly on the query data, avoiding the prototype biascaused by intra-class distribution shifts between the support and query data.By leveraging sparse human annotations as guidance, HOW-Seg enablesprototype-based segmentation for both base and novel classes. Considering thelack of granularity of initial prototypes, we introduce a hierarchicalprototype disambiguation mechanism to refine ambiguous prototypes, whichcorrespond to annotations of different classes. To further enrich contextualawareness, we employ a dense conditional random field (CRF) upon the refinedprototypes to optimize their label assignments. Through iterative humanfeedback, HOW-Seg dynamically improves its predictions, achieving high-qualitysegmentation for both base and novel classes. Experiments demonstrate that withsparse annotations (e.g., one-novel-class-one-click), HOW-Seg matches orsurpasses the state-of-the-art generalized few-shot segmentation (GFS-Seg)method under the 5-shot setting. When using advanced backbones (e.g.,Stratified Transformer) and denser annotations (e.g., 10 clicks per sub-scene),HOW-Seg achieves 85.27% mIoU on S3DIS and 66.37% mIoU on ScanNetv2,significantly outperforming alternatives.</description>
      <author>example@mail.com (Peng Zhang, Songru Yang, Jinsheng Sun, Weiqing Li, Zhiyong Su)</author>
      <guid isPermaLink="false">2508.04962v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control</title>
      <link>http://arxiv.org/abs/2508.05342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Journal under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Graph-Fused Vision-Language-Action (GF-VLA)框架使双臂机器人能够直接从RGB和深度人类演示中执行任务级推理和执行，实现了高准确性和成功率，解决了传统低级轨迹模仿无法泛化的问题。&lt;h4&gt;背景&lt;/h4&gt;从人类视频中教机器人灵巧技能具有挑战性，因为现有方法依赖低级轨迹模仿，无法在不同物体类型、空间布局和机械臂配置之间泛化。&lt;h4&gt;目的&lt;/h4&gt;提出GF-VLA框架，使双臂机器人系统能够直接从RGB和深度人类演示中执行任务级推理和执行。&lt;h4&gt;方法&lt;/h4&gt;GF-VLA提取基于香农信息论的线索识别关键手和物体，编码为时序场景图捕捉交互，与语言条件transformer融合生成行为树和运动命令，并引入跨手选择策略提高双臂执行效率。&lt;h4&gt;主要发现&lt;/h4&gt;信息论场景表示实现95%图准确性和93%子任务分割；机器人执行策略实现94%抓取成功率、89%放置准确性和90%总体任务成功率，展示强大泛化能力和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;GF-VLA有效解决了从人类视频中教机器人灵巧技能的泛化问题，实现了高准确性和成功率，在多样化和语义变化场景中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;从人类视频中教机器人灵巧技能仍然具有挑战性，因为依赖于低级轨迹模仿，无法在不同物体类型、空间布局和机械臂配置之间泛化。我们提出Graph-Fused Vision-Language-Action (GF-VLA)框架，使双臂机器人系统能够直接从RGB和深度人类演示中执行任务级推理和执行。GF-VLA首先提取基于香农信息论的线索，以识别与任务最相关的手和物体，然后将这些线索编码为时序场景图，捕捉手-物体和物体-物体之间的交互。这些图与语言条件transformer融合，生成层次化行为树和可解释的笛卡尔运动命令。为了提高双臂设置中的执行效率，我们引入了跨手选择策略，推断最优夹爪分配，无需显式几何推理。我们在四个涉及符号形状构建和空间泛化的结构化双臂积木组装任务上评估GF-VLA。实验结果表明，信息论场景表示实现了超过95%的图准确性和93%的子任务分割，支持LLM规划器生成可靠且人类可读的任务策略。当由双臂机器人执行时，这些策略在堆叠、字母构建和几何重新配置场景中实现了94%的抓取成功率、89%的放置准确性和90%的总体任务成功率，展示了在多样化和语义变化方面的强大泛化能力和鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人从人类视频中学习灵巧技能时的泛化问题。现有方法依赖低级轨迹模仿，无法跨对象类型、空间布局和机械手配置进行泛化。这个问题很重要，因为它限制了机器人在非结构化环境中的实用性，而开发能在动态环境中稳健操作的机器人系统是具身人工智能的核心挑战，有效操作需要机器人能感知复杂场景、推理物理交互并可靠执行多步骤任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有VLA模型在物理交互建模方面的不足，然后设计了一种结合信息论场景图与VLA推理的统一框架。他们借鉴了信息论中的熵和互信息概念来量化不确定性并识别关键交互，扩展了场景图表示以包含动态物理交互，并基于现有LLaMA 2模型构建了特定推理机制。此外，他们还融入了思维链和自我验证提示，以及从有限演示数据中学习的迁移学习策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用信息论方法从人类演示视频中提取与任务最相关的手和对象线索，编码为时间有序场景图，然后与语言调节的transformer融合生成层次化行为树和可解释运动命令。整体流程包括：1)视频处理识别手和对象；2)应用熵和互信息构建场景图；3)检测手-对象和对象-对象交互；4)实现双臂选择策略；5)使用统一双头架构进行VLA推理；6)结合思维链进行语义规划；7)执行时进行自我验证和局部重新规划。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于信息论的时空结构化场景图，显式编码动态物理交互；2)首次系统整合结构化物理交互建模与VLA推理的GF-VLA框架；3)思维链与VLA模型的集成，提供明确子目标分解；4)无需显式几何推理的跨手选择策略。相比之前工作，传统方法依赖固定校准且对噪声敏感，现有VLA模型难以建模物理关系，策略学习方法需大量数据且泛化差，而GF-VLA通过结构化场景图与VLA结合实现了更好的泛化、鲁棒性和可解释性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的图融合视觉-语言-动作框架，通过信息论场景图和思维链推理，使双臂机器人能够从人类演示中学习可泛化、可解释的操作策略，显著提高了在复杂任务中的执行性能和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Teaching robots dexterous skills from human videos remains challenging due tothe reliance on low-level trajectory imitation, which fails to generalizeacross object types, spatial layouts, and manipulator configurations. Wepropose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enablesdual-arm robotic systems to perform task-level reasoning and execution directlyfrom RGB and Depth human demonstrations. GF-VLA first extractsShannon-information-based cues to identify hands and objects with the highesttask relevance, then encodes these cues into temporally ordered scene graphsthat capture both hand-object and object-object interactions. These graphs arefused with a language-conditioned transformer that generates hierarchicalbehavior trees and interpretable Cartesian motion commands. To improveexecution efficiency in bimanual settings, we further introduce a cross-handselection policy that infers optimal gripper assignment without explicitgeometric reasoning. We evaluate GF-VLA on four structured dual-arm blockassembly tasks involving symbolic shape construction and spatialgeneralization. Experimental results show that the information-theoretic scenerepresentation achieves over 95 percent graph accuracy and 93 percent subtasksegmentation, supporting the LLM planner in generating reliable andhuman-readable task policies. When executed by the dual-arm robot, thesepolicies yield 94 percent grasp success, 89 percent placement accuracy, and 90percent overall task success across stacking, letter-building, and geometricreconfiguration scenarios, demonstrating strong generalization and robustnessacross diverse spatial and semantic variations.</description>
      <author>example@mail.com (Shunlei Li, Longsen Gao, Jin Wang, Chang Che, Xi Xiao, Jiuwen Cao, Yingbai Hu, Hamid Reza Karimi)</author>
      <guid isPermaLink="false">2508.05342v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>EndoMatcher: Generalizable Endoscopic Image Matcher via Multi-Domain Pre-training for Robot-Assisted Surgery</title>
      <link>http://arxiv.org/abs/2508.05205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EndoMatcher的可泛化内窥镜图像匹配方法，通过大规模多领域数据预训练解决困难视觉条件和数据稀缺问题，实现了在未见过的器官和成像条件下的零样本匹配，显著提高了匹配准确率。&lt;h4&gt;背景&lt;/h4&gt;内窥镜图像中的可泛化密集特征匹配对机器人辅助任务（包括3D重建、导航和手术场景理解）至关重要，但由于困难的视觉条件（如弱纹理、大视角变化）和标注数据稀缺，这仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理困难视觉条件并克服数据稀缺问题的可泛化内窥镜图像匹配方法。&lt;h4&gt;方法&lt;/h4&gt;提出EndoMatcher，采用双分支Vision Transformer提取多尺度特征，通过双交互块增强对应关系学习；构建包含约120万真实和合成图像对的Endo-Mix6多领域数据集；采用渐进式多目标训练策略促进跨领域的平衡学习和表示质量提升。&lt;h4&gt;主要发现&lt;/h4&gt;EndoMatcher在零样本匹配实验中，在Hamlyn和Bladder数据集上的内点匹配数量分别比最先进方法增加了140.69%和201.43%，在Gastro-Matching数据集上的匹配方向预测准确率提高了9.40%。&lt;h4&gt;结论&lt;/h4&gt;EndoMatcher能够在具有挑战性的内窥镜条件下实现密集且准确的匹配，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;内窥镜图像中的可泛化密集特征匹配对于机器人辅助任务至关重要，包括3D重建、导航和手术场景理解。然而，由于困难的视觉条件（如弱纹理、大视角变化）和标注数据稀缺，这仍然是一个挑战。为解决这些挑战，我们提出了EndoMatcher，一种通过大规模多领域数据预训练的可泛化内窥镜图像匹配器。为解决困难的视觉条件，EndoMatcher采用双分支Vision Transformer提取多尺度特征，通过双交互块增强，以实现稳健的对应关系学习。为克服数据稀缺并提高领域多样性，我们构建了Endo-Mix6，这是第一个用于内窥镜匹配的多领域数据集。Endo-Mix6包含约120万跨六个领域的真实和合成图像对，使用运动恢复结构和模拟变换生成对应关系标签。Endo-Mix6的多样性和规模带来了训练稳定性的新挑战，由于数据集大小的显著差异、分布偏移和误差不平衡。为解决这些问题，采用渐进式多目标训练策略促进平衡学习并提高跨领域的表示质量。这使得EndoMatcher能够以零样本方式泛化到未见过的器官和成像条件。大量的零样本匹配实验表明，EndoMatcher在Hamlyn和Bladder数据集上分别将内点匹配数量比最先进方法增加了140.69%和201.43%，并在Gastro-Matching数据集上将匹配方向预测准确率(MDPA)提高了9.40%，在具有挑战性的内窥镜条件下实现了密集且准确的匹配。代码已在https://github.com/Beryl2000/EndoMatcher公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalizable dense feature matching in endoscopic images is crucial forrobot-assisted tasks, including 3D reconstruction, navigation, and surgicalscene understanding. Yet, it remains a challenge due to difficult visualconditions (e.g., weak textures, large viewpoint variations) and a scarcity ofannotated data. To address these challenges, we propose EndoMatcher, ageneralizable endoscopic image matcher via large-scale, multi-domain datapre-training. To address difficult visual conditions, EndoMatcher employs atwo-branch Vision Transformer to extract multi-scale features, enhanced by dualinteraction blocks for robust correspondence learning. To overcome datascarcity and improve domain diversity, we construct Endo-Mix6, the firstmulti-domain dataset for endoscopic matching. Endo-Mix6 consists ofapproximately 1.2M real and synthetic image pairs across six domains, withcorrespondence labels generated using Structure-from-Motion and simulatedtransformations. The diversity and scale of Endo-Mix6 introduce new challengesin training stability due to significant variations in dataset sizes,distribution shifts, and error imbalance. To address them, a progressivemulti-objective training strategy is employed to promote balanced learning andimprove representation quality across domains. This enables EndoMatcher togeneralize across unseen organs and imaging conditions in a zero-shot fashion.Extensive zero-shot matching experiments demonstrate that EndoMatcher increasesthe number of inlier matches by 140.69% and 201.43% on the Hamlyn and Bladderdatasets over state-of-the-art methods, respectively, and improves the MatchingDirection Prediction Accuracy (MDPA) by 9.40% on the Gastro-Matching dataset,achieving dense and accurate matching under challenging endoscopic conditions.The code is publicly available at https://github.com/Beryl2000/EndoMatcher.</description>
      <author>example@mail.com (Bingyu Yang, Qingyao Tian, Yimeng Geng, Huai Liao, Xinyan Huang, Jiebo Luo, Hongbin Liu)</author>
      <guid isPermaLink="false">2508.05205v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2508.05064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了将语言指导与3D高斯飞溅技术相结合的最新研究进展，详细介绍了理论基础、集成策略和实际应用案例，并指出了当前面临的挑战和未来发展方向。&lt;h4&gt;背景&lt;/h4&gt;高斯飞溅技术已成为实时3D场景表示的革命性技术，相比神经辐射场(NeRF)更高效且表现力强，已在场景重建、机器人技术和交互式内容创作等领域取得进展。最近，将大型语言模型和语言嵌入集成到高斯飞溅管道中，为文本条件生成、编辑和语义场景理解开辟了新可能性。&lt;h4&gt;目的&lt;/h4&gt;提供对语言引导与3D高斯飞溅这一新兴交叉领域的全面概述，填补现有研究的空白，为未来研究方向提供指导。&lt;h4&gt;方法&lt;/h4&gt;对当前将语言指导与3D高斯飞溅相结合的研究工作进行结构化回顾，分析理论基础、集成策略和实际用例，并评估现有技术的局限性。&lt;h4&gt;主要发现&lt;/h4&gt;语言引导的3D高斯飞溅技术在多个应用领域展现出潜力，但面临计算瓶颈、泛化能力有限以及语义注释的3D高斯数据稀缺等关键限制。&lt;h4&gt;结论&lt;/h4&gt;语言引导的3D高斯飞溅技术代表了3D场景理解和生成的重要发展方向，解决现有局限性并进一步推进这一领域将有助于实现更先进的3D场景理解和交互应用。&lt;h4&gt;翻译&lt;/h4&gt;高斯飞溅技术已迅速发展为实时3D场景表示的革命性技术，为神经辐射场(NeRF)提供了一种高效且表现力强的替代方案。它能够以高保真度渲染复杂场景，推动了场景重建、机器人技术和交互式内容创作等领域的进步。最近，将大型语言模型(LLMs)和语言嵌入集成到高斯飞溅管道中，为文本条件生成、编辑和语义场景理解开辟了新的可能性。尽管取得了这些进展，但缺乏对这一新兴交叉领域的全面概述。本综述对当前将语言指导与3D高斯飞溅相结合的研究工作进行了结构化回顾，详细介绍了理论基础、集成策略和实际用例。我们强调了计算瓶颈、泛化能力和语义注释的3D高斯数据稀缺等关键局限性，并概述了使用高斯飞溅推进语言引导3D场景理解的开放挑战和未来方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将语言嵌入技术与3D高斯泼溅技术相结合，以增强3D场景的语义理解能力。这个问题很重要，因为随着机器人、自主导航和虚拟现实等领域的发展，系统需要更好地理解和解释3D环境，而语言嵌入与3D场景理解的结合可以使AI系统通过自然语言与3D环境进行更智能的交互，支持语义场景理解、对象定位、场景编辑等复杂任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先回顾了3D场景表示技术从早期神经隐式表示到3D高斯泼溅的发展历程，然后考察了语言模型从词嵌入到大型语言模型的演进。他们认识到虽然2D视觉语言模型已很成熟，但扩展到3D仍面临挑战。作者整合了现有工作，借鉴了3D高斯泼溅的实时渲染优势、大型语言模型的语义理解能力以及视觉语言模型的多模态融合策略，构建了一个统一的框架来实现语言引导的3D场景理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将语言嵌入与3D高斯泼溅相结合，创建能够理解3D场景语义的统一框架。整体流程包括：1)使用运动结构恢复技术初始化稀疏3D高斯；2)提取和压缩语言特征；3)将语言特征嵌入到3D高斯中；4)联合优化几何、外观和语义参数；5)支持文本查询、语义分割、场景编辑等下游应用。这种方法使系统能够通过自然语言理解和操作3D场景，实现了从纯几何表示到语义丰富理解的转变。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次对语言嵌入与3D高斯泼溅结合领域进行全面综述；提出静态和动态场景的分类框架；引入多尺度语义理解；开发高效语言特征表示方法；提出训练免费方法提高泛化能力。相比之前工作，这篇论文范围更广（涵盖静态和动态场景），方法更多样（从每个高斯嵌入到基于场的嵌入），应用更丰富（从机器人到虚拟现实），理论基础更扎实，并更清晰地指出了当前方法的局限性和未来方向。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次系统性地综述了将语言嵌入与3D高斯泼溅技术相结合的方法，为通过自然语言理解、查询和交互3D场景提供了全面框架，并展示了其在多个领域的应用前景。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gaussian Splatting has rapidly emerged as a transformative technique forreal-time 3D scene representation, offering a highly efficient and expressivealternative to Neural Radiance Fields (NeRF). Its ability to render complexscenes with high fidelity has enabled progress across domains such as scenereconstruction, robotics, and interactive content creation. More recently, theintegration of Large Language Models (LLMs) and language embeddings intoGaussian Splatting pipelines has opened new possibilities for text-conditionedgeneration, editing, and semantic scene understanding. Despite these advances,a comprehensive overview of this emerging intersection has been lacking. Thissurvey presents a structured review of current research efforts that combinelanguage guidance with 3D Gaussian Splatting, detailing theoreticalfoundations, integration strategies, and real-world use cases. We highlight keylimitations such as computational bottlenecks, generalizability, and thescarcity of semantically annotated 3D Gaussian data and outline open challengesand future directions for advancing language-guided 3D scene understandingusing Gaussian Splatting.</description>
      <author>example@mail.com (Mahmoud Chick Zaouali, Todd Charter, Yehor Karpichev, Brandon Haworth, Homayoun Najjjaran)</author>
      <guid isPermaLink="false">2508.05064v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Clustering Wind data at 1 AU to contextualize magnetic reconnection in the solar wind</title>
      <link>http://arxiv.org/abs/2508.05579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 19 figures, accepted for publication in Astronomy &amp;  Astrophysics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究使用无监督学习技术对太阳风数据进行聚类，发现磁重联事件主要发生在五种不同的太阳风条件下，其中大多数与慢速太阳风相关。这种方法有助于理解太阳风条件如何影响磁重联的发生。&lt;h4&gt;背景&lt;/h4&gt;磁重联事件在太阳风中频繁观测到。理解太阳风中的模式和结构对于将观测到的磁重联事件放入背景中至关重要，因为它们的发生率和特性可能受到太阳风条件的影响。&lt;h4&gt;目的&lt;/h4&gt;采用无监督学习技术如自组织映射和K-Means对1天文单位处的太阳风数据进行聚类和解释，以改进对导致太阳风中磁重联条件的理解。&lt;h4&gt;方法&lt;/h4&gt;收集Wind航天器测量的磁场数据、质子密度、质子温度和太阳风速度数据。对数据进行预处理后，训练自组织映射将高维数据可视化到低维空间，并应用K-Means聚类识别太阳风数据中的不同簇。&lt;h4&gt;主要发现&lt;/h4&gt;分析显示重联事件分布在五个不同的簇中：慢速太阳风、压缩慢速风、高阿尔芬风、压缩快速风和抛射物。大多数重联事件与慢速太阳风相关，其次是高阿尔芬风、压缩慢速风和压缩快速风，一小部分与抛射物相关。压缩慢速风和快速风以及抛射物是与太阳风瞬态相关的簇。&lt;h4&gt;结论&lt;/h4&gt;使用自组织映射和K-Means的无监督学习方法能够基于太阳风的瞬态产生可物理解释的太阳风簇，并允许对太阳风中磁重联外流的发生进行背景化解释。&lt;h4&gt;翻译&lt;/h4&gt;背景。磁重联事件在太阳风中频繁观测到。理解太阳风中的模式和结构对于将观测到的磁重联事件放入背景中至关重要，因为它们的发生率和特性可能受到太阳风条件的影响。目的。我们采用无监督学习技术如自组织映射和K-Means对1天文单位处的太阳风数据进行聚类和解释，以改进对导致太阳风中磁重联条件的理解。方法。我们收集了Wind航天器测量的磁场数据以及质子密度、质子温度和太阳风速度测量数据。在预处理数据后，我们训练了一个自组织映射将高维数据可视化到低维空间，并应用K-Means聚类识别太阳风数据中的不同簇。结果。我们的分析显示重联事件分布在五个不同的簇中：a)慢速太阳风，b)压缩慢速风，c)高阿尔芬风，d)压缩快速风，e)抛射物。压缩慢速风和快速风以及抛射物是与太阳风瞬态相关的簇，如流相互作用区域和日冕物质抛射。大多数重联事件与慢速太阳风相关，其次是高阿尔芬风、压缩慢速风和压缩快速风，一小部分重联事件与抛射物相关。结论。使用自组织映射和K-Means的无监督学习方法能够基于太阳风的瞬态产生可物理解释的太阳风簇，并允许对太阳风中磁重联外流的发生进行背景化解释。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Context. Magnetic reconnection events are frequently observed in the solarwind. Understanding the patterns and structures within the solar wind iscrucial to put observed magnetic reconnection events into context, since theiroccurrence rate and properties are likely influenced by solar wind conditions.  Aims. We employed unsupervised learning techniques such as self-organizingmaps (SOM) and K-Means to cluster and interpret solar wind data at 1 AU for animproved understanding of the conditions that lead to magnetic reconnection inthe solar wind.  Methods. We collected magnetic field data and proton density, protontemperature, and solar wind speed measurements taken by the Wind spacecraft.After preprocessing the data, we trained a SOM to visualize thehigh-dimensional data in a lower-dimensional space and applied K-Meansclustering to identify distinct clusters within the solar wind data.  Results. Our analysis revealed that the reconnection events are distributedacross five different clusters: a) slow solar wind, b) compressed slow wind, c)highly Alfv\'enic wind, d) compressed fast wind, and e) ejecta. Compressed slowand fast wind and ejecta are clusters associated with solar wind transientssuch as stream interaction regions and interplanetary coronal mass ejections.The majority of the reconnection events are associated with the slow solarwind, followed by the highly Alfv\'enic wind, compressed slow wind, andcompressed fast wind, and a small fraction of the reconnection events areassociated with ejecta.  Conclusions. Unsupervised learning approaches with SOM and K-Means lead tophysically interpretable solar wind clusters based on their transients andallow for the contextualization of magnetic reconnection exhausts' occurrencein the solar wind.</description>
      <author>example@mail.com (Francesco Carella, Giovanni Lapenta, Alessandro Bemporad, Stefan Eriksson, Maria Elena Innocenti, Sophia Köhne, Jasmina Magdalenic)</author>
      <guid isPermaLink="false">2508.05579v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting</title>
      <link>http://arxiv.org/abs/2508.05323v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于文本反转的视觉语言模型个性化方法，用于开放词汇目标检测，仅需少量样本即可扩展模型词汇表，同时保持原始模型的零样本能力和计算效率。&lt;h4&gt;背景&lt;/h4&gt;大型预训练视觉语言模型在目标检测任务上表现优异且具有零样本能力，但针对特定目标优化时仍需微调，而传统微调会丧失原始模型的自然语言查询和零样本能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，使视觉语言模型能够学习新或改进现有的token来准确检测新颖或细粒度对象，同时保留原始模型的零样本能力和基准性能。&lt;h4&gt;方法&lt;/h4&gt;受文本反转在文本到图像扩散模型中成功的启发，提出了一种开放词汇目标检测的类似方法，通过学习token嵌入来扩展模型词汇表，保持原始权重冻结，仅更新token嵌入维度。&lt;h4&gt;主要发现&lt;/h4&gt;该方法通过广泛的定量和定性实验证明，能够匹配或优于存在遗忘问题的基线方法，同时保持原始模型的零样本域转移能力（如从真实照片训练后检测对象草图）。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在保持原始模型性能和能力的同时，仅需少量样本即可扩展视觉语言模型的词汇表，且计算需求远低于全模型微调。&lt;h4&gt;翻译&lt;/h4&gt;大型预训练视觉语言模型(VLMs)在多个目标检测基准测试上已达到最先进性能并具有强大的零样本能力，但为了在特定目标上获得最佳性能，某种形式的微调仍然是必要的。虽然初始的VLM权重允许优秀的少样本迁移学习，但这通常会导致原始自然语言查询和零样本能力的丧失。受文本反转(TI)在个性化文本到图像扩散模型中成功的启发，我们提出了开放词汇目标检测的类似方法。TI允许通过学习新的或改进现有的token来扩展VLM词汇表，以便从仅三个样本中准确检测新颖或细粒度的对象。学习的token与原始VLM权重完全兼容，同时保持权重冻结，保留了原始模型的基准性能，并利用其现有能力如零样本域转移（例如，仅在真实照片上训练后检测对象的草图）。存储和梯度计算仅限于token嵌入维度，比全模型微调需要少得多的计算量。我们通过广泛的定量和定性实验评估了该方法是否匹配或优于存在遗忘问题的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in large pre-trained vision language models (VLMs) hasreached state-of-the-art performance on several object detection benchmarks andboasts strong zero-shot capabilities, but for optimal performance on specifictargets some form of finetuning is still necessary. While the initial VLMweights allow for great few-shot transfer learning, this usually involves theloss of the original natural language querying and zero-shot capabilities.Inspired by the success of Textual Inversion (TI) in personalizingtext-to-image diffusion models, we propose a similar formulation foropen-vocabulary object detection. TI allows extending the VLM vocabulary bylearning new or improving existing tokens to accurately detect novel orfine-grained objects from as little as three examples. The learned tokens arecompletely compatible with the original VLM weights while keeping them frozen,retaining the original model's benchmark performance, and leveraging itsexisting capabilities such as zero-shot domain transfer (e.g., detecting asketch of an object after training only on real photos). The storage andgradient calculations are limited to the token embedding dimension, requiringsignificantly less compute than full-model fine-tuning. We evaluated whetherthe method matches or outperforms the baseline methods that suffer fromforgetting in a wide variety of quantitative and qualitative experiments.</description>
      <author>example@mail.com (Frank Ruis, Gertjan Burghouts, Hugo Kuijf)</author>
      <guid isPermaLink="false">2508.05323v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised learning for inverse problems in computed tomography</title>
      <link>http://arxiv.org/abs/2508.05321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种用于CT图像重建的无监督深度学习方法，通过在深度学习框架中融入前向和后向投影层，实现了不依赖真实图像的图像重建。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络训练与传统的迭代重建方法之间存在固有相似性，这为开发新型CT图像重建方法提供了理论基础。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖于真实图像(ground-truth images)的从投影数据重建图像的方法，并提高重建质量和效率。&lt;h4&gt;方法&lt;/h4&gt;在深度学习框架中融入前向和后向投影层，构建无监督深度学习模型，使用2DeteCT数据集进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;与传统滤波反投影(FBP)和最大似然(ML)重建技术相比，该方法在均方误差(MSE)和结构相似性指数(SSIM)方面表现更优，同时显著减少了重建时间。&lt;h4&gt;结论&lt;/h4&gt;该方法是一种有前景的替代方案，可用于实时医学成像应用。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种用于CT图像重建的无监督深度学习方法，利用深度神经网络训练与传统迭代重建方法之间的固有相似性。通过在深度学习框架中融入前向和后向投影层，我们证明了在不依赖真实图像的情况下从投影数据重建图像的可行性。我们的方法在二维2DeteCT数据集上进行了评估，与传统滤波反投影(FBP)和最大似然(ML)重建技术相比，在均方误差(MSE)和结构相似性指数(SSIM)方面表现出优越性能。此外，我们的方法显著减少了重建时间，使其成为实时医学成像应用的有前景的替代方案。未来的工作将聚焦于将此方法扩展到三维重建，并提高投影几何的适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents an unsupervised deep learning approach for computedtomography (CT) image reconstruction, leveraging the inherent similaritiesbetween deep neural network training and conventional iterative reconstructionmethods. By incorporating forward and backward projection layers within thedeep learning framework, we demonstrate the feasibility of reconstructingimages from projection data without relying on ground-truth images. Our methodis evaluated on the two-dimensional 2DeteCT dataset, showcasing superiorperformance in terms of mean squared error (MSE) and structural similarityindex (SSIM) compared to traditional filtered backprojection (FBP) and maximumlikelihood (ML) reconstruction techniques. Additionally, our approachsignificantly reduces reconstruction time, making it a promising alternativefor real-time medical imaging applications. Future work will focus on extendingthis methodology to three-dimensional reconstructions and enhancing theadaptability of the projection geometry.</description>
      <author>example@mail.com (Laura Hellwege, Johann Christopher Engster, Moritz Schaar, Thorsten M. Buzug, Maik Stille)</author>
      <guid isPermaLink="false">2508.05321v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Segmenting the Complex and Irregular in Two-Phase Flows: A Real-World Empirical Study with SAM2</title>
      <link>http://arxiv.org/abs/2508.05227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于现代视觉基础模型的气泡分割方法，解决了多相流中非球形气泡分割的难题。&lt;h4&gt;背景&lt;/h4&gt;多相流中气体气泡分割是许多工业环境中的关键挑战，从冶金处理到船舶减阻。传统方法和基于学习的方法假设气泡接近球形，限制了它们在气泡变形、合并或破碎场景中的有效性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确分割高度非凸、不规则气泡结构的方法，特别是在空气润滑系统中形成的无定形和拓扑多样的气泡斑块。&lt;h4&gt;方法&lt;/h4&gt;将气泡分割任务作为迁移学习问题，使用微调的Segment Anything Model SAM v2.1模型。&lt;h4&gt;主要发现&lt;/h4&gt;微调的SAM v2.1模型可以使用少至100张标注图像准确分割高度非凸、不规则的气泡结构。&lt;h4&gt;结论&lt;/h4&gt;现代视觉基础模型在处理复杂气泡分割问题上展现出巨大潜力，为工业应用提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在多相流中分割气体气泡是许多工业环境中的关键但尚未解决的挑战，从冶金处理到船舶减阻。传统方法以及最近基于学习的方法都假设气泡接近球形形状，限制了它们在气泡发生变形、合并或破碎区域的有效性。这种复杂性在空气润滑系统中尤为明显，合并的气泡形成无定形和拓扑多样的斑块。在本工作中，我们通过现代视觉基础模型的视角重新审视这个问题。我们将任务作为迁移学习问题，并首次证明，微调的Segment Anything Model SAM v2.1可以使用少至100张标注图像准确分割高度非凸、不规则的气泡结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Segmenting gas bubbles in multiphase flows is a critical yet unsolvedchallenge in numerous industrial settings, from metallurgical processing tomaritime drag reduction. Traditional approaches-and most recent learning-basedmethods-assume near-spherical shapes, limiting their effectiveness in regimeswhere bubbles undergo deformation, coalescence, or breakup. This complexity isparticularly evident in air lubrication systems, where coalesced bubbles formamorphous and topologically diverse patches. In this work, we revisit theproblem through the lens of modern vision foundation models. We cast the taskas a transfer learning problem and demonstrate, for the first time, that afine-tuned Segment Anything Model SAM v2.1 can accurately segment highlynon-convex, irregular bubble structures using as few as 100 annotated images.</description>
      <author>example@mail.com (Semanur Küçük, Cosimo Della Santina, Angeliki Laskari)</author>
      <guid isPermaLink="false">2508.05227v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Sensitivity of Stability: Theoretical &amp; Empirical Analysis of Replicability for Adaptive Data Selection in Transfer Learning</title>
      <link>http://arxiv.org/abs/2508.04901v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 Pages, 5 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了迁移学习中自适应数据选择策略对结果可复制性的影响，提出了选择敏感性作为衡量策略对数据扰动响应的度量，发现高度自适应策略性能更好但可复制性差，而源域预训练可缓解这一问题。&lt;h4&gt;背景&lt;/h4&gt;迁移学习的广泛应用已彻底改变机器学习，使预训练模型能高效适应新领域。然而，使用动态优先级排序训练样本的自适应数据选择策略时，这些适应的可靠性仍不明确。&lt;h4&gt;目的&lt;/h4&gt;对迁移学习中的可复制性进行全面理论和经验分析，引入数学框架量化适应有效性和结果一致性之间的权衡。&lt;h4&gt;方法&lt;/h4&gt;引入选择敏感性(Δ_Q)度量，证明可复制性失败概率与选择敏感性呈二次方增长而与样本量呈指数级下降。在MultiNLI语料库上使用六种自适应选择策略（从均匀采样到基于梯度选择）进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;高度自适应策略（如基于梯度和课程学习）实现卓越任务性能但可复制性失败率高；较不自适应方法将失败率保持在7%以下；源域预训练可降低失败率高达30%同时保持性能增益。&lt;h4&gt;结论&lt;/h4&gt;研究结果为从业者提供了权衡性能和可复制性的指导原则，强调了现代迁移学习系统中需要考虑可复制性的设计。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习的广泛应用已彻底改变机器学习，使预训练模型能够高效适应新领域。然而，当使用动态优先级排序训练样本的自适应数据选择策略时，这些适应的可靠性仍不清楚。我们提出了对迁移学习中可复制性的全面理论和经验分析，引入了一个数学框架来量化适应有效性和结果一致性之间的基本权衡。我们的主要贡献是形式化了选择敏感性(Δ_Q)，这是一个捕捉自适应选择策略如何响应训练数据扰动的度量。我们证明可复制性失败概率——即两个独立训练运行产生性能差异超过阈值的可能性——与选择敏感性呈二次方增长，而与样本量呈指数级下降。在MultiNLI语料库上使用六种自适应选择策略（从均匀采样到基于梯度选择）进行的广泛实验表明，这种理论关系在实践中精确成立。我们的结果显示，像基于梯度和课程学习这样的高度自适应策略实现了卓越的任务性能，但可复制性失败率高，而较不自适应的方法将失败率保持在7%以下。重要的是，我们表明源域预训练提供了一个强大的缓解机制，在保持性能增益的同时将失败率降低高达30%。这些发现为从业者提供了指导原则，以权衡性能和可复制性，并突显了现代迁移学习系统中需要考虑可复制性设计的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The widespread adoption of transfer learning has revolutionized machinelearning by enabling efficient adaptation of pre-trained models to new domains.However, the reliability of these adaptations remains poorly understood,particularly when using adaptive data selection strategies that dynamicallyprioritize training examples. We present a comprehensive theoretical andempirical analysis of replicability in transfer learning, introducing amathematical framework that quantifies the fundamental trade-off betweenadaptation effectiveness and result consistency. Our key contribution is theformalization of selection sensitivity ($\Delta_Q$), a measure that captureshow adaptive selection strategies respond to perturbations in training data. Weprove that replicability failure probability: the likelihood that twoindependent training runs produce models differing in performance by more thana threshold, increases quadratically with selection sensitivity whiledecreasing exponentially with sample size. Through extensive experiments on theMultiNLI corpus using six adaptive selection strategies - ranging from uniformsampling to gradient-based selection - we demonstrate that this theoreticalrelationship holds precisely in practice. Our results reveal that highlyadaptive strategies like gradient-based and curriculum learning achievesuperior task performance but suffer from high replicability failure rates,while less adaptive approaches maintain failure rates below 7%. Crucially, weshow that source domain pretraining provides a powerful mitigation mechanism,reducing failure rates by up to 30% while preserving performance gains. Thesefindings establish principled guidelines for practitioners to navigate theperformance-replicability trade-off and highlight the need forreplicability-aware design in modern transfer learning systems.</description>
      <author>example@mail.com (Prabhav Singh, Jessica Sorrell)</author>
      <guid isPermaLink="false">2508.04901v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Discrepancy-Aware Contrastive Adaptation in Medical Time Series Analysis</title>
      <link>http://arxiv.org/abs/2508.05572v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文针对医学时间序列疾病诊断中的两个关键挑战提出了创新解决方案。首先，通过引入外部数据和AE-GAN提取先验知识来解决标注成本高导致的过拟合问题；其次，提出LMCF框架，通过多视图对比学习和多注意力头机制自适应学习疾病特征。实验证明该方法在三个目标数据集上均优于其他基线方法，对心肌梗死、阿尔茨海默病和帕金森病等疾病的诊断有显著影响。&lt;h4&gt;背景&lt;/h4&gt;医学时间序列疾病诊断面临两个主要挑战：一是医学数据标注成本高，导致在标记有限、单一中心数据集上训练的模型容易过拟合；二是现有研究通常依赖手动设计的多样化正负样本对进行对比学习，这些方法复杂且缺乏通用性，无法自适应地捕捉不同疾病条件下的特异性特征。&lt;h4&gt;目的&lt;/h4&gt;解决医学时间序列疾病诊断中的两个关键挑战：一是解决高标注成本导致的过拟合问题；二是改进对比学习方法，使其能够自适应地捕获不同疾病条件下的特异性特征。&lt;h4&gt;方法&lt;/h4&gt;提出两种创新方法：一是引入相关任务的外部数据，利用AE-GAN提取先验知识，为下游任务提供有价值的参考；二是提出LMCF（可学习的多视图对比框架），整合多头注意力机制，通过视图间和视图内对比学习策略自适应地从不同视图学习表示。此外，使用预训练的AE-GAN重建目标数据中的差异作为疾病概率，并将其整合到对比学习过程中。&lt;h4&gt;主要发现&lt;/h4&gt;在三个目标数据集上的实验表明，该方法始终优于其他基线方法，对心肌梗死、阿尔茨海默病和帕金森病等疾病的诊断应用具有显著影响。&lt;h4&gt;结论&lt;/h4&gt;提出的创新方法有效解决了医学时间序列疾病诊断中的两个关键挑战，通过引入外部数据和AE-GAN提取先验知识解决了过拟合问题，通过LMCF框架改进了对比学习方法，使其能够自适应地捕获疾病特异性特征。该方法在多种疾病诊断任务中表现出色，具有重要的医疗应用价值。&lt;h4&gt;翻译&lt;/h4&gt;在医学时间序列疾病诊断中，确定了两个关键挑战。首先，医学数据的高标注成本导致在标记有限、单一中心数据集上训练的模型出现过拟合。为解决此问题，我们提出整合来自相关任务的外部数据，并利用AE-GAN提取先验知识，为下游任务提供有价值的参考。其次，许多现有研究采用对比学习来为诊断任务推导出更通用的医学序列表示，通常依赖手动设计的多样化正负样本对。然而，这些方法复杂，缺乏通用性，无法自适应地捕捉不同疾病条件下的疾病特异性特征。为克服这一局限，我们引入LMCF（可学习的多视图对比框架），这是一个整合多头注意力机制并通过视图间和视图内对比学习策略自适应地从不同视图学习表示的框架。此外，使用预训练的AE-GAN重建目标数据中的差异作为疾病概率，然后将其整合到对比学习过程中。在三个目标数据集上的实验证明，我们的方法始终优于其他基线方法，突显了其对心肌梗死、阿尔茨海默病和帕金森病等医疗应用诊断的显著影响。我们在xxxxx发布了源代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In medical time series disease diagnosis, two key challenges are identified.First, the high annotation cost of medical data leads to overfitting in modelstrained on label-limited, single-center datasets. To address this, we proposeincorporating external data from related tasks and leveraging AE-GAN to extractprior knowledge, providing valuable references for downstream tasks. Second,many existing studies employ contrastive learning to derive more generalizedmedical sequence representations for diagnostic tasks, usually relying onmanually designed diverse positive and negative sample pairs. However, theseapproaches are complex, lack generalizability, and fail to adaptively capturedisease-specific features across different conditions. To overcome this, weintroduce LMCF (Learnable Multi-views Contrastive Framework), a framework thatintegrates a multi-head attention mechanism and adaptively learnsrepresentations from different views through inter-view and intra-viewcontrastive learning strategies. Additionally, the pre-trained AE-GAN is usedto reconstruct discrepancies in the target data as disease probabilities, whichare then integrated into the contrastive learning process. Experiments on threetarget datasets demonstrate that our method consistently outperforms otherseven baselines, highlighting its significant impact on healthcare applicationssuch as the diagnosis of myocardial infarction, Alzheimer's disease, andParkinson's disease. We release the source code at xxxxx.</description>
      <author>example@mail.com (Yifan Wang, Hongfeng Ai, Ruiqi Li, Maowei Jiang, Ruiyuan Kang, Jiahua Dong, Cheng Jiang, Chenzhong Li)</author>
      <guid isPermaLink="false">2508.05572v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Revealing Latent Information: A Physics-inspired Self-supervised Pre-training Framework for Noisy and Sparse Events</title>
      <link>http://arxiv.org/abs/2508.05507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种自监督预训练框架，用于充分提取事件相机数据中的潜在信息，包括边缘信息和纹理线索。该框架包含三个阶段：差分引导的掩码建模、主干固定特征转换和聚焦目标对比学习。实验证明该框架在多种下游任务上表现优异。&lt;h4&gt;背景&lt;/h4&gt;事件相机是一种新型的神经形态视觉传感器，具有高时间分辨率和宽动态范围，能够在具有挑战性的场景中提供准确的视觉表示。然而，事件数据本质上是稀疏和有噪声的，主要反映亮度变化，这使有效的特征提取变得复杂。&lt;h4&gt;目的&lt;/h4&gt;为了解决事件数据特征提取的困难，本研究旨在提出一个自监督预训练框架，以充分揭示事件数据中的潜在信息，包括边缘信息和纹理线索，从而在各种下游任务中实现更好的性能。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一个三阶段的自监督预训练框架：1)差分引导的掩码建模，受事件物理采样过程启发，重建时间强度差异图以提取增强信息；2)主干固定特征转换，对比事件和图像特征而不更新主干，以保留从掩码建模中学到的表示；3)聚焦目标对比学习，更新整个模型，通过关注高价值区域来提高语义判别能力。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该框架具有鲁棒性，并且在各种下游任务上持续优于最先进的方法，包括目标识别、语义分割和光流估计。&lt;h4&gt;结论&lt;/h4&gt;该自监督预训练框架有效地解决了事件数据特征提取的挑战，通过充分利用事件数据中的潜在信息，显著提升了在多种视觉任务上的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;事件相机是一种新型的神经形态视觉传感器，以高时间分辨率和宽动态范围记录数据，在具有挑战性的场景中为准确的视觉表示提供了新的可能性。然而，事件数据本质上是稀疏和有噪声的，主要反映亮度变化，这使有效的特征提取变得复杂。为此，我们提出了一个自监督预训练框架，以充分揭示事件数据中的潜在信息，包括边缘信息和纹理线索。我们的框架包含三个阶段：受事件物理采样过程启发的差分引导掩码建模，重建时间强度差异图，从原始事件数据中提取增强信息；主干固定特征转换，对比事件和图像特征而不更新主干，以保留从掩码建模中学到的表示并稳定它们对对比学习的影响；聚焦目标对比学习，更新整个模型，通过关注高价值区域来提高语义判别能力。大量实验表明，我们的框架具有鲁棒性，并且在各种下游任务上持续优于最先进的方法，包括目标识别、语义分割和光流估计。代码和数据集可在https://github.com/BIT-Vision/EventPretrain获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event camera, a novel neuromorphic vision sensor, records data with hightemporal resolution and wide dynamic range, offering new possibilities foraccurate visual representation in challenging scenarios. However, event data isinherently sparse and noisy, mainly reflecting brightness changes, whichcomplicates effective feature extraction. To address this, we propose aself-supervised pre-training framework to fully reveal latent information inevent data, including edge information and texture cues. Our framework consistsof three stages: Difference-guided Masked Modeling, inspired by the eventphysical sampling process, reconstructs temporal intensity difference maps toextract enhanced information from raw event data. Backbone-fixed FeatureTransition contrasts event and image features without updating the backbone topreserve representations learned from masked modeling and stabilizing theireffect on contrastive learning. Focus-aimed Contrastive Learning updates theentire model to improve semantic discrimination by focusing on high-valueregions. Extensive experiments show our framework is robust and consistentlyoutperforms state-of-the-art methods on various downstream tasks, includingobject recognition, semantic segmentation, and optical flow estimation. Thecode and dataset are available at https://github.com/BIT-Vision/EventPretrain.</description>
      <author>example@mail.com (Lin Zhu, Ruonan Liu, Xiao Wang, Lizhi Wang, Hua Huang)</author>
      <guid isPermaLink="false">2508.05507v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding</title>
      <link>http://arxiv.org/abs/2508.05244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出RegionMed-CLIP，一种区域感知的多模态对比学习框架，通过整合局部病理信号和整体语义表示来解决医学图像理解中的数据稀缺和过度依赖全局特征的问题。&lt;h4&gt;背景&lt;/h4&gt;医学图像理解在自动化诊断和临床决策支持中至关重要，但面临两个主要挑战：高质量标注医学数据有限，以及过度依赖全局图像特征而忽略细微但重要的病理区域。&lt;h4&gt;目的&lt;/h4&gt;解决医学图像理解中的数据稀缺和特征提取问题，通过引入区域感知的多模态学习方法提高医学图像分析的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;开发了RegionMed-CLIP框架，包含创新的感兴趣区域(ROI)处理器，自适应整合细粒度区域特征与全局上下文；采用渐进式训练策略增强分层多模态对齐；构建了包含广泛区域标注和多级临床描述的MedRegion-500k医学图像-文本语料库。&lt;h4&gt;主要发现&lt;/h4&gt;在图像-文本检索、零样本分类和视觉问答任务上的实验表明，RegionMed-CLIP显著超越现有最先进的视觉语言模型，证明了区域感知对比预训练的关键重要性。&lt;h4&gt;结论&lt;/h4&gt;RegionMed-CLIP为推进多模态医学图像理解提供了强大基础，通过区域感知方法解决了医学图像分析中的关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;医学图像理解在实现自动化诊断和数据驱动的临床决策支持方面起着至关重要的作用。然而，其进展受到两个主要挑战的阻碍：高质量标注医学数据的有限可用性以及对全局图像特征的过度依赖，这些特征常常忽略了细微但临床上有意义的病理区域。为了解决这些问题，我们引入了RegionMed-CLIP，一种区域感知的多模态对比学习框架，明确整合了局部病理信号和整体语义表示。我们方法的核心是一种创新的感兴趣区域(ROI)处理器，自适应地将细粒度区域特征与全局上下文集成，并由增强分层多模态对齐的渐进式训练策略支持。为了实现大规模区域级表示学习，我们构建了MedRegion-500k，一个包含广泛区域标注和多级临床描述的综合医学图像-文本语料库。在图像-文本检索、零样本分类和视觉问答任务上的大量实验表明，RegionMed-CLIP一致性地以较大优势超越最先进的视觉语言模型。我们的结果突显了区域感知对比预训练的关键重要性，并将RegionMed-CLIP定位为推进多模态医学图像理解的强大基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical image understanding plays a crucial role in enabling automateddiagnosis and data-driven clinical decision support. However, its progress isimpeded by two primary challenges: the limited availability of high-qualityannotated medical data and an overreliance on global image features, whichoften miss subtle but clinically significant pathological regions. To addressthese issues, we introduce RegionMed-CLIP, a region-aware multimodalcontrastive learning framework that explicitly incorporates localizedpathological signals along with holistic semantic representations. The core ofour method is an innovative region-of-interest (ROI) processor that adaptivelyintegrates fine-grained regional features with the global context, supported bya progressive training strategy that enhances hierarchical multimodalalignment. To enable large-scale region-level representation learning, weconstruct MedRegion-500k, a comprehensive medical image-text corpus thatfeatures extensive regional annotations and multilevel clinical descriptions.Extensive experiments on image-text retrieval, zero-shot classification, andvisual question answering tasks demonstrate that RegionMed-CLIP consistentlyexceeds state-of-the-art vision language models by a wide margin. Our resultshighlight the critical importance of region-aware contrastive pre-training andposition RegionMed-CLIP as a robust foundation for advancing multimodal medicalimage understanding.</description>
      <author>example@mail.com (Tianchen Fang, Guiru Liu)</author>
      <guid isPermaLink="false">2508.05244v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Latent Expression Generation for Referring Image Segmentation and Grounding</title>
      <link>http://arxiv.org/abs/2508.05123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的视觉定位框架，通过生成多个潜在表达式来弥补丰富视觉细节与稀疏文本提示之间的不匹配，从而提高目标对象定位的准确性。&lt;h4&gt;背景&lt;/h4&gt;视觉定位任务（如指代表达分割和指代表达理解）旨在根据给定文本描述定位图像中的目标对象。目标对象可通过多种方式描述，反映颜色、位置等不同属性。然而，大多数现有方法仅依赖单一文本输入，仅能捕捉视觉领域可用信息的一小部分。&lt;h4&gt;目的&lt;/h4&gt;解决丰富视觉细节与稀疏文本提示之间的不匹配问题，防止相似对象的错误识别。&lt;h4&gt;方法&lt;/h4&gt;提出一种新颖的视觉定位框架，通过从单一文本输入生成多个潜在表达式，并融入原始描述中缺失的补充视觉细节。引入主题分发器和视觉概念注入器模块，将共享主题和不同属性概念嵌入到潜在表示中，从而捕捉独特且目标特定的视觉线索。还提出一种正边距对比学习策略，将所有潜在表达式与原始文本对齐，同时保留细微变化。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法不仅在多个基准测试上优于最先进的RIS和REC方法，而且在广义指代表达分割基准上也取得了出色的性能。&lt;h4&gt;结论&lt;/h4&gt;通过生成多个潜在表达式并融入补充视觉细节，该方法有效解决了视觉定位任务中文本描述与视觉信息不匹配的问题，显著提高了目标对象定位的准确性。&lt;h4&gt;翻译&lt;/h4&gt;视觉定位任务，如指代表达分割和指代表达理解，旨在根据给定的文本描述定位目标对象。图像中的目标对象可以通过多种方式描述，反映颜色、位置等多种属性。然而，大多数现有方法仅依赖单一文本输入，仅能捕捉视觉领域可用信息的一小部分。这种丰富视觉细节与稀疏文本提示之间的不匹配可能导致相似对象的错误识别。为此，我们提出了一种新颖的视觉定位框架，通过从单一文本输入生成多个潜在表达式，并融入原始描述中缺失的补充视觉细节。具体来说，我们引入了主题分发器和视觉概念注入器模块，将共享主题和不同属性概念嵌入到潜在表示中，从而捕捉独特且目标特定的视觉线索。我们还提出了一种正边距对比学习策略，将所有潜在表达式与原始文本对齐，同时保留细微变化。实验结果表明，我们的方法不仅在多个基准测试上优于最先进的RIS和REC方法，而且在广义指代表达分割基准上也取得了出色的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual grounding tasks, such as referring image segmentation (RIS) andreferring expression comprehension (REC), aim to localize a target object basedon a given textual description. The target object in an image can be describedin multiple ways, reflecting diverse attributes such as color, position, andmore. However, most existing methods rely on a single textual input, whichcaptures only a fraction of the rich information available in the visualdomain. This mismatch between rich visual details and sparse textual cues canlead to the misidentification of similar objects. To address this, we propose anovel visual grounding framework that leverages multiple latent expressionsgenerated from a single textual input by incorporating complementary visualdetails absent from the original description. Specifically, we introducesubject distributor and visual concept injector modules to embed bothshared-subject and distinct-attributes concepts into the latentrepresentations, thereby capturing unique and target-specific visual cues. Wealso propose a positive-margin contrastive learning strategy to align alllatent expressions with the original text while preserving subtle variations.Experimental results show that our method not only outperforms state-of-the-artRIS and REC approaches on multiple benchmarks but also achieves outstandingperformance on the generalized referring expression segmentation (GRES)benchmark.</description>
      <author>example@mail.com (Seonghoon Yu, Joonbeom Hong, Joonseok Lee, Jeany Son)</author>
      <guid isPermaLink="false">2508.05123v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations</title>
      <link>http://arxiv.org/abs/2508.05097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为'MultiCheck'的细粒度多模态事实验证统一框架，用于处理文本和图像结合的虚假信息核查问题。&lt;h4&gt;背景&lt;/h4&gt;多模态虚假信息（文本和图像结合）的增长给主要依赖文本证据的事实核查系统带来了显著挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够处理结构化文本和视觉信号的统一框架，用于细粒度多模态事实验证。&lt;h4&gt;方法&lt;/h4&gt;结合文本和图像的专用编码器，使用元素间交互的融合模块捕获跨模态关系，分类头预测主张真实性，并通过对比学习目标在共享潜在空间中实现主张-证据对的语义对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在Factify 2数据集上评估，加权F1得分为0.84，显著优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;明确的多模态推理是有效的，该方法在复杂现实场景中具有可扩展和可解释的事实核查潜力。&lt;h4&gt;翻译&lt;/h4&gt;多模态虚假信息的增长率（其中主张同时得到文本和图像的支持）给主要依赖文本证据的事实核查系统带来了重大挑战。在这项工作中，我们提出了一个名为'MultiCheck'的细粒度多模态事实验证统一框架，旨在对结构化的文本和视觉信号进行推理。我们的架构结合了文本和图像的专用编码器，以及一个使用元素间交互捕获跨模态关系的融合模块。然后，分类头预测主张的真实性，并由对比学习目标支持，该目标鼓励在共享潜在空间中主张-证据对的语义对齐。我们在Factify 2数据集上评估了我们的方法，实现了0.84的加权F1分数，显著优于基线。这些结果突显了明确多模态推理的有效性，并证明了我们的方法在复杂现实场景中可扩展和可解释的事实核查潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing rate of multimodal misinformation, where claims are supported byboth text and images, poses significant challenges to fact-checking systemsthat rely primarily on textual evidence. In this work, we have proposed aunified framework for fine-grained multimodal fact verification called"MultiCheck", designed to reason over structured textual and visual signals.Our architecture combines dedicated encoders for text and images with a fusionmodule that captures cross-modal relationships using element-wise interactions.A classification head then predicts the veracity of a claim, supported by acontrastive learning objective that encourages semantic alignment betweenclaim-evidence pairs in a shared latent space. We evaluate our approach on theFactify 2 dataset, achieving a weighted F1 score of 0.84, substantiallyoutperforming the baseline. These results highlight the effectiveness ofexplicit multimodal reasoning and demonstrate the potential of our approach forscalable and interpretable fact-checking in complex, real-world scenarios.</description>
      <author>example@mail.com (Aditya Kishore, Gaurav Kumar, Jasabanta Patro)</author>
      <guid isPermaLink="false">2508.05097v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>MetaDiT: Enabling Fine-grained Constraints in High-degree-of Freedom Metasurface Design</title>
      <link>http://arxiv.org/abs/2508.05076v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MetaDiT是一种新型的高保真超表面设计框架，解决了现有方法的两个关键限制：仅能生成部分设计参数和依赖低分辨率光谱目标，实现了更精确的光谱响应控制。&lt;h4&gt;背景&lt;/h4&gt;超表面是由纳米结构组成的超薄工程材料，能够以自然材料无法实现的方式操控光。近期进展利用计算优化、机器学习和深度学习来自动化设计，但现有方法存在两个基本限制：通常将模型限制为只生成部分设计参数，且依赖于严重下采样的光谱目标，这降低了所产生结构的新颖性和准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种生成模型，能够探索大型、无约束的设计空间，同时精确捕捉材料参数与其高分辨率光谱响应之间复杂的物理关系。&lt;h4&gt;方法&lt;/h4&gt;提出了MetaDiT框架，利用通过对比学习预训练的稳健光谱编码器，为基于扩散变换器的主干网络提供强大的条件指导。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明MetaDiT在光谱准确性方面优于现有基线，并通过大量的消融研究进一步验证了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;MetaDiT解决了现有超表面设计方法的局限性，代码和模型将被开源以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;超表面是由纳米结构组成的超薄工程材料，能够以自然材料无法实现的方式操控光。最近的进展利用计算优化、机器学习和深度学习来自动化它们的设计。然而，现有方法表现出两个基本限制：(1)它们通常将模型限制为只生成部分设计参数，(2)它们依赖于严重下采样的光谱目标，这降低了所产生结构的新颖性和准确性。核心挑战在于开发一种生成模型，能够探索大型、无约束的设计空间，同时精确捕捉材料参数与其高分辨率光谱响应之间复杂的物理关系。在本文中，我们介绍了MetaDiT，一个用于高保真超表面设计的新框架，解决了这些限制。我们的方法利用通过对比学习预训练的稳健光谱编码器，为基于扩散变换器的主干网络提供强大的条件指导。实验证明MetaDiT在光谱准确性方面优于现有基线，我们通过大量的消融研究进一步验证了我们的方法。我们的代码和模型将被开源，以促进未来的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Metasurfaces are ultrathin, engineered materials composed of nanostructuresthat manipulate light in ways unattainable by natural materials. Recentadvances have leveraged computational optimization, machine learning, and deeplearning to automate their design. However, existing approaches exhibit twofundamental limitations: (1) they often restrict the model to generating only asubset of design parameters, and (2) they rely on heavily downsampled spectraltargets, which compromises both the novelty and accuracy of the resultingstructures. The core challenge lies in developing a generative model capable ofexploring a large, unconstrained design space while precisely capturing theintricate physical relationships between material parameters and theirhigh-resolution spectral responses. In this paper, we introduce MetaDiT, anovel framework for high-fidelity metasurface design that addresses theselimitations. Our approach leverages a robust spectrum encoder pretrained withcontrastive learning, providing strong conditional guidance to a DiffusionTransformer-based backbone. Experiments demonstrate that MetaDiT outperformsexisting baselines in spectral accuracy, we further validate our method throughextensive ablation studies. Our code and model weights will be open-sourced tofacilitate future research.</description>
      <author>example@mail.com (Hao Li, Andrey Bogdanov)</author>
      <guid isPermaLink="false">2508.05076v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework</title>
      <link>http://arxiv.org/abs/2508.04816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 Pages, 2 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoMAD是一种轻量级无参数框架，通过统一多个自监督Vision Transformers的知识创建紧凑学生网络，在ImageNet-1K上达到75.4% Top-1准确率，在多个密集预测任务中建立新SOTA。&lt;h4&gt;背景&lt;/h4&gt;现有自监督学习范式如对比学习和掩码图像建模通常独立预训练，忽略互补见解，且产生的大型模型在资源受限环境下不实用。&lt;h4&gt;目的&lt;/h4&gt;克服独立预训和模型过大问题，引入统一框架整合多个先进自监督Vision Transformers知识，创建轻量级学生网络。&lt;h4&gt;方法&lt;/h4&gt;从MAE、MoCo v3和iBOT三个教师模型中提取知识，使用非对称掩码（学生见25%图像块，教师见不同掩码），通过线性适配器和层归一化对齐嵌入，用联合共识门控融合教师知识，双重KL散度训练学生。&lt;h4&gt;主要发现&lt;/h4&gt;ImageNet-1K上ViT-Tiny达75.4% Top-1准确率（提高0.4%），ADE20K上47.3% mIoU，MS-COCO上44.5% box平均精度和40.5% mask平均精度。&lt;h4&gt;结论&lt;/h4&gt;CoMAD在紧凑型自监督蒸馏领域建立了新的最先进水平。&lt;h4&gt;翻译&lt;/h4&gt;许多自监督学习范式，如对比学习和掩码图像建模，从未标记数据中学习强大的表示，但通常独立预训练，忽略了互补的见解，并产生了在实际资源受限部署中不实用的大型模型。为克服这些挑战，我们引入了共识导向的掩码蒸馏（CoMAD），这是一个轻量级、无参数的框架，将多个当前最先进的自监督Vision Transformers的知识统一到一个紧凑的学生网络中。CoMAD从三个预训练的ViT-Base教师模型中提取知识：MAE、MoCo v3和iBOT，每个模型都提供不同的语义和上下文先验。而不是简单地平均教师输出，我们应用非对称掩码：学生只能看到25%的图像块，而每个教师接收逐渐变轻的独特掩码，迫使学生更丰富的上下文中插值缺失的特征。教师嵌入通过线性适配器和层归一化与学生的空间对齐，然后通过我们的联合共识门控融合，该门控通过结合余弦相似度和教师间一致性来加权每个标记。学生使用双重层次的KL散度在可见标记和重构的特征图上进行训练，同时捕捉局部和全局结构。在ImageNet-1K上，CoMAD的ViT-Tiny实现了75.4%的Top-1准确率，比之前的最先进方法提高了0.4%。在密集预测迁移中，在ADE20K上达到47.3%的mIoU，在MS-COCO上达到44.5%的box平均精度和40.5%的mask平均精度，在紧凑型自监督蒸馏中建立了新的最先进水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Numerous self-supervised learning paradigms, such as contrastive learning andmasked image modeling, learn powerful representations from unlabeled data butare typically pretrained in isolation, overlooking complementary insights andyielding large models that are impractical for resource-constrained deployment.To overcome these challenges, we introduce Consensus-oriented MaskedDistillation (CoMAD), a lightweight, parameter-free framework that unifiesknowledge from multiple current state-of-the-art self-supervised VisionTransformers into a compact student network. CoMAD distills from threepretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinctsemantic and contextual priors. Rather than naively averaging teacher outputs,we apply asymmetric masking: the student sees only 25 percent of patches whileeach teacher receives a progressively lighter, unique mask, forcing the studentto interpolate missing features under richer contexts. Teacher embeddings arealigned to the student's space via a linear adapter and layer normalization,then fused through our joint consensus gating, which weights each token bycombining cosine affinity with inter-teacher agreement. The student is trainedwith dual-level KL divergence on visible tokens and reconstructed feature maps,capturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tinyachieves 75.4 percent Top-1, an increment of 0.4 percent over the previousstate-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoUon ADE20K, and 44.5 percent box average precision and 40.5 percent mask averageprecision on MS-COCO, establishing a new state-of-the-art in compact SSLdistillation.</description>
      <author>example@mail.com (Sriram Mandalika, Lalitha V)</author>
      <guid isPermaLink="false">2508.04816v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>SSEmb: A Joint Structural and Semantic Embedding Framework for Mathematical Formula Retrieval</title>
      <link>http://arxiv.org/abs/2508.04162v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SSEmb是一种新型嵌入框架，能够同时捕捉数学公式的结构和语义特征，在ARQMath-3公式检索任务中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;公式检索是数学信息检索领域中的一个重要课题。&lt;h4&gt;目的&lt;/h4&gt;提出SSEmb框架，以有效捕获数学公式的结构特征和语义特征，提高公式检索性能。&lt;h4&gt;方法&lt;/h4&gt;结构上使用图对比学习编码表示为操作符图的公式，并通过替换策略引入新型图数据增强方法；语义上使用Sentence-BERT编码公式周围的文本；最后通过加权方案分别计算并融合结构与语义相似性。&lt;h4&gt;主要发现&lt;/h4&gt;在ARQMath-3公式检索任务中，SSEmb在P'@10和nDCG'@10指标上比现有基于嵌入的方法提高5个百分点以上；SSEmb提升了其他所有方法的性能，与Approach0结合时达到最先进水平。&lt;h4&gt;结论&lt;/h4&gt;SSEmb是一种有效的数学公式嵌入框架，通过同时考虑结构和语义特征，显著提升了公式检索性能。&lt;h4&gt;翻译&lt;/h4&gt;公式检索是数学信息检索中的一个重要课题。我们提出了SSEmb，一种能够捕捉数学公式结构和语义特征的新型嵌入框架。结构上，我们采用图对比学习来编码表示为操作符图的公式。为了增强这些公式图的结构多样性同时保持其数学有效性，我们通过替换策略引入了一种新型的图数据增强方法。语义上，我们使用Sentence-BERT来编码公式周围的文本。最后，对于每个查询及其候选，分别计算结构和语义相似性，然后通过加权方案进行融合。在ARQMath-3公式检索任务中，SSEmb在P'@10和nDCG'@10上比现有的基于嵌入的方法提高了5个百分点以上。此外，SSEmb提升了其他所有方法的性能，与Approach0结合时实现了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Formula retrieval is an important topic in Mathematical InformationRetrieval. We propose SSEmb, a novel embedding framework capable of capturingboth structural and semantic features of mathematical formulas. Structurally,we employ Graph Contrastive Learning to encode formulas represented as OperatorGraphs. To enhance structural diversity while preserving mathematical validityof these formula graphs, we introduce a novel graph data augmentation approachthrough a substitution strategy. Semantically, we utilize Sentence-BERT toencode the surrounding text of formulas. Finally, for each query and itscandidates, structural and semantic similarities are calculated separately andthen fused through a weighted scheme. In the ARQMath-3 formula retrieval task,SSEmb outperforms existing embedding-based methods by over 5 percentage pointson P'@10 and nDCG'@10. Furthermore, SSEmb enhances the performance of all runsof other methods and achieves state-of-the-art results when combined withApproach0.</description>
      <author>example@mail.com (Ruyin Li, Xiaoyu Chen)</author>
      <guid isPermaLink="false">2508.04162v2</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Deformable Attention Graph Representation Learning for Histopathology Whole Slide Image Analysis</title>
      <link>http://arxiv.org/abs/2508.05382v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种具有可变形注意力的新型图神经网络框架，用于病理图像分析，通过动态加权有向图和可学习空间偏移增强空间依赖关系的捕捉能力。&lt;h4&gt;背景&lt;/h4&gt;全切片图像和感兴趣区域的准确分类是计算病理学的挑战；主流多实例学习方法难以捕捉组织结构间的空间依赖；现有图神经网络依赖静态图拓扑，忽略组织块实际空间位置；传统注意力机制缺乏特异性，难以专注于结构相关区域。&lt;h4&gt;目的&lt;/h4&gt;开发一种具有可变形注意力的新型GNN框架，用于提升病理图像分析中空间结构的捕捉能力。&lt;h4&gt;方法&lt;/h4&gt;构建基于块特征的动态加权有向图，每个节点通过注意力加权边从邻居聚合上下文信息；结合由每个块实际坐标提供的可学习空间偏移，使模型能自适应关注形态学相关区域；增强上下文场同时保持空间特异性。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在四个基准数据集(TCGA-COAD, BRACS, 胃肠上皮化分级和肠ROI分类)上实现了最先进性能；可变形注意力能有效捕捉WSIs和ROIs中的复杂空间结构。&lt;h4&gt;结论&lt;/h4&gt;可变形注意力机制在病理图像分析中具有显著优势，能有效捕捉复杂空间结构，提升分类性能。&lt;h4&gt;翻译&lt;/h4&gt;全切片图像(WSIs)和感兴趣区域(ROIs)的准确分类是计算病理学的基本挑战。虽然主流方法通常采用多实例学习(MIL)，但它们难以捕捉组织结构之间的空间依赖关系。图神经网络(GNNs)已成为建模实例间关系的解决方案，但大多数依赖于静态图拓扑，忽略了组织块的实际空间位置。此外，传统注意力机制缺乏特异性，限制了它们专注于结构相关区域的能力。在这项工作中，我们提出了一种用于病理图像分析的新型GNN框架，具有可变形注意力。我们基于块特征构建动态加权有向图，其中每个节点通过注意力加权边从其邻居聚合上下文信息。具体来说，我们整合了由每个块实际坐标提供信息的可学习空间偏移，使模型能够自适应地关注幻片形态学相关区域。这种设计显著增强了上下文场，同时保留了空间特异性。我们的框架在四个基准数据集(TCGA-COAD, BRACS, 胃肠上皮化分级和肠ROI分类)上实现了最先进的性能，证明了可变形注意力在捕获WSIs和ROIs中复杂空间结构方面的强大能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate classification of Whole Slide Images (WSIs) and Regions of Interest(ROIs) is a fundamental challenge in computational pathology. While mainstreamapproaches often adopt Multiple Instance Learning (MIL), they struggle tocapture the spatial dependencies among tissue structures. Graph Neural Networks(GNNs) have emerged as a solution to model inter-instance relationships, yetmost rely on static graph topologies and overlook the physical spatialpositions of tissue patches. Moreover, conventional attention mechanisms lackspecificity, limiting their ability to focus on structurally relevant regions.In this work, we propose a novel GNN framework with deformable attention forpathology image analysis. We construct a dynamic weighted directed graph basedon patch features, where each node aggregates contextual information from itsneighbors via attention-weighted edges. Specifically, we incorporate learnablespatial offsets informed by the real coordinates of each patch, enabling themodel to adaptively attend to morphologically relevant regions across theslide. This design significantly enhances the contextual field while preservingspatial specificity. Our framework achieves state-of-the-art performance onfour benchmark datasets (TCGA-COAD, BRACS, gastric intestinal metaplasiagrading, and intestinal ROI classification), demonstrating the power ofdeformable attention in capturing complex spatial structures in WSIs and ROIs.</description>
      <author>example@mail.com (Mingxi Fu, Xitong Ling, Yuxuan Chen, Jiawen Li, fanglei fu, Huaitian Yuan, Tian Guan, Yonghong He, Lianghui Zhu)</author>
      <guid isPermaLink="false">2508.05382v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Learning to See and Act: Task-Aware View Planning for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2508.05186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 9 figures, project page: https://hcplab-sysu.github.io/TAVP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了任务感知视角规划（TAVP）框架，通过结合主动视角规划和任务特定表征学习，解决了现有视觉-语言-动作模型在多任务机器人操作中因静态视角和共享视觉编码器导致的3D感知受限和任务干扰问题，显著提升了动作预测的鲁棒性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现有的多任务机器人操作视觉-语言-动作模型通常依赖静态视角和共享视觉编码器，这限制了3D感知能力并导致任务间干扰，从而阻碍了模型的鲁棒性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架来克服现有VLA模型的局限性，通过主动获取信息丰富的视角和解耦不同任务的特征，提升机器人操作任务的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了任务感知视角规划（TAVP）框架，包含两个主要组件：1) 结合伪环境的高效探索策略，主动获取信息丰富的视角；2) 专家混合（MoE）视觉编码器，解耦不同任务的特征，提高表征保真度和任务泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;TAVP通过学习任务感知的视觉表征方式，生成了更完整和判别性的视觉表示，在各种操作挑战中显著提升了动作预测性能。在RLBench任务上的广泛实验表明，TAVP模型优于最先进的固定视角方法。&lt;h4&gt;结论&lt;/h4&gt;TAVP框架有效解决了现有VLA模型在多任务机器人操作中的局限性，通过主动视角规划和任务特定表征学习，显著提升了模型的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;最近用于多任务机器人操作的视觉-语言-动作（VLA）模型通常依赖静态视角和共享视觉编码器，这限制了3D感知并导致任务干扰，阻碍了鲁棒性和泛化能力。在这项工作中，我们提出了任务感知视角规划（TAVP）框架，通过将主动视角规划与任务特定表征学习相结合来克服这些挑战。TAVP采用了一种高效的探索策略，由新型伪环境加速，主动获取信息丰富的视角。此外，我们引入了专家混合（MoE）视觉编码器来解耦不同任务的特征，提高了表征保真度和任务泛化能力。通过学习以任务感知的方式观察世界，TAVP生成更完整和判别性的视觉表征，在各种操作挑战中显示出显著增强的动作预测。在RLBench任务上的大量实验表明，我们提出的TAVP模型实现了优于最先进固定视角方法的性能。视觉结果和代码可在以下网址获取：https://hcplab-sysu.github.io/TAVP。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有多任务机器人操作模型依赖静态视角和共享视觉编码器的问题，这限制了3D感知能力并导致任务干扰，影响模型鲁棒性和泛化能力。这个问题很重要，因为在复杂或动态场景中，固定视角常导致目标物体或机械臂被遮挡，造成场景理解不完整；同时，共享编码器在处理视觉和语义差异大的任务时会产生干扰，限制了机器人在各种操作环境中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有VLA模型的局限性，认识到需要主动探索信息量丰富的视角和处理多任务干扰的方法。他们设计了任务感知视角规划框架，结合高效的探索策略和专家混合视觉编码器。方法借鉴了现有工作：使用强化学习优化探索策略，参考RVT-2的粗预测阶段生成感兴趣区域，采用类似RVT-2的多视图Transformer架构但加入了TaskMoE模块，并升级了ARP的自回归动作序列模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是让机器人能根据任务需求主动选择最优观察视角，获取更完整的视觉表示，并使用任务感知的专家混合模块处理多任务场景中的特征冲突。整体流程包括：1)接收语言指令和RGB-D图像输入；2)重建3D点云；3)生成感兴趣区域；4)双分支处理（一个分支进行粗定位，另一个通过MVEP预测最佳相机参数）；5)从预测视角渲染图像；6)使用TaskMoE进行任务特定特征提取；7)预测最终机器人动作。训练分为固定视角训练、探索策略优化和整个模型微调三个阶段。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多视角探索策略(MVEP)，使机器人能主动探索最优观察视角；2)任务感知专家混合(TaskMoE)，根据任务和场景信息动态选择专家；3)创新的训练策略，使用伪环境加速训练。相比之前工作，TAVP采用动态多视角而非固定视角，使用任务特定专家而非共享编码器，通过伪环境而非真实环境加速强化学习训练，且探索策略基于任务需求而非随机选择。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的任务感知视角规划框架通过主动探索最优观察视角和任务特定的专家混合编码，显著提升了机器人在复杂多任务操作场景中的感知能力和动作预测准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent vision-language-action (VLA) models for multi-task roboticmanipulation commonly rely on static viewpoints and shared visual encoders,which limit 3D perception and cause task interference, hindering robustness andgeneralization. In this work, we propose Task-Aware View Planning (TAVP), aframework designed to overcome these challenges by integrating active viewplanning with task-specific representation learning. TAVP employs an efficientexploration policy, accelerated by a novel pseudo-environment, to activelyacquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE)visual encoder to disentangle features across different tasks, boosting bothrepresentation fidelity and task generalization. By learning to see the worldin a task-aware way, TAVP generates more complete and discriminative visualrepresentations, demonstrating significantly enhanced action prediction acrossa wide array of manipulation challenges. Extensive experiments on RLBench tasksshow that our proposed TAVP model achieves superior performance overstate-of-the-art fixed-view approaches. Visual results and code are providedat: https://hcplab-sysu.github.io/TAVP.</description>
      <author>example@mail.com (Yongjie Bai, Zhouxia Wang, Yang Liu, Weixing Chen, Ziliang Chen, Mingtong Dai, Yongsen Zheng, Lingbo Liu, Guanbin Li, Liang Lin)</author>
      <guid isPermaLink="false">2508.05186v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>TANGO: Graph Neural Dynamics via Learned Energy and Tangential Flows</title>
      <link>http://arxiv.org/abs/2508.05070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TANGO是一种受动力系统启发的图表示学习框架，通过学习能量景观及其下降动力学控制节点特征演化。核心是学习节点嵌入上的李雅普诺夫函数，其梯度保证收敛和稳定性。同时引入切向分量，在保持能量值的同时演化特征。这种正交流分解形成灵活图动力学，能在平坦或病态能量区域有效传播信号。TANGO减轻过压缩问题，兼容多种图神经网络架构，并在多种基准测试中表现优异。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在处理图数据时面临信号传播效率低下的问题，特别是在平坦或病态能量区域，以及过压缩问题。现有方法缺乏有效机制处理这些挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种灵活有效的图表示学习框架，通过能量景观控制节点特征演化，保证模型收敛性和稳定性，在各种能量区域实现有效信号传播，减轻过压缩问题，并兼容不同图神经网络架构。&lt;h4&gt;方法&lt;/h4&gt;1) 引入TANGO框架，通过学习能量景观及其下降动力学控制节点特征演化；2) 设计节点嵌入上的可学习李雅普诺夫函数，其梯度定义能量减少方向；3) 通过消息传递学习切向分量，保持能量值同时演化特征；4) 将能量梯度下降和切向演化分解为正交流；5) 使方法兼容不同图神经网络骨干架构。&lt;h4&gt;主要发现&lt;/h4&gt;1) TANGO能有效减轻过压缩问题；2) 方法兼容不同图神经网络架构；3) 在多样化节点和图分类及回归基准测试中取得强大性能；4) 联合学习的能量函数和切向流对图神经网络有效。&lt;h4&gt;结论&lt;/h4&gt;TANGO通过联合学习的能量函数和切向流，为图神经网络提供了灵活有效的表示学习框架。动力系统启发的能量景观和下降动力学解决了图神经网络中的信号传播问题，特别是在平坦或病态能量区域。TANGO的通用性和有效性使其在各种图学习任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了TANGO——一种受动力系统启发的图表示学习框架，它通过学习到的能量景观及其相关的下降动力学来控制节点特征的演化。我们方法的核心是节点嵌入上的一个可学习李雅普诺夫函数，其梯度定义了一个能量减少的方向，保证了收敛和稳定性。为了在保持基于能量的动力学优势的同时增强灵活性，我们通过消息传递引入了一个新颖的切向分量，它在保持能量值的同时演化特征。这种将能量梯度下降和切向演化分解为正交流的形式，产生了灵活的图动力学，使得即使在图学习中常见的平坦或病态能量区域也能有效传播信号。我们的方法减轻了过压缩问题，并且兼容不同的图神经网络骨干。从经验上看，TANGO在多样化的节点和图分类及回归基准测试中取得了强大性能，证明了联合学习的能量函数和切向流对图神经网络的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce TANGO -- a dynamical systems inspired framework for graphrepresentation learning that governs node feature evolution through a learnedenergy landscape and its associated descent dynamics. At the core of ourapproach is a learnable Lyapunov function over node embeddings, whose gradientdefines an energy-reducing direction that guarantees convergence and stability.To enhance flexibility while preserving the benefits of energy-based dynamics,we incorporate a novel tangential component, learned via message passing, thatevolves features while maintaining the energy value. This decomposition intoorthogonal flows of energy gradient descent and tangential evolution yields aflexible form of graph dynamics, and enables effective signal propagation evenin flat or ill-conditioned energy regions, that often appear in graph learning.Our method mitigates oversquashing and is compatible with different graphneural network backbones. Empirically, TANGO achieves strong performance acrossa diverse set of node and graph classification and regression benchmarks,demonstrating the effectiveness of jointly learned energy functions andtangential flows for graph neural networks.</description>
      <author>example@mail.com (Moshe Eliasof, Eldad Haber, Carola-Bibiane Schönlieb)</author>
      <guid isPermaLink="false">2508.05070v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.05008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为多模态因果驱动表征学习(MCDRL)的新框架，将因果推理与视觉语言模型结合，解决医学图像分割中的域泛化问题。MCDRL通过两步实现：首先利用CLIP的跨模态能力识别候选病变区域并构建混杂因素字典；其次训练因果干预网络消除域特定变化的影响，同时保留关键解剖结构信息。实验证明该方法优于竞争方法，具有更高的分割精度和更强的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)如CLIP在各种计算机视觉任务中表现出强大的零样本能力，但在医学影像领域的应用面临挑战。医学数据具有高度可变性和复杂性，医学图像常表现出显著的域偏移，由设备差异、程序伪影和成像模式等混杂因素引起，导致模型在未见过的域上泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;提出多模态因果驱动表征学习(MCDRL)框架，将因果推理与VLM结合，解决医学图像分割中的域泛化问题。&lt;h4&gt;方法&lt;/h4&gt;MCDRL分两步实现：首先，利用CLIP的跨模态能力识别候选病变区域，并通过文本提示构建混杂因素字典，专门用于表示域特定变化；其次，训练一个因果干预网络，利用该字典识别并消除这些域特定变化的影响，同时保留对分割任务至关重要的解剖结构信息。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，MCDRL始终优于竞争方法，产生优越的分割精度，并表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MCDRL框架有效解决了医学图像分割中的域泛化问题，通过因果推理消除了域特定变化的影响，同时保留了重要的解剖结构信息。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLMs)，如CLIP，已在各种计算机视觉任务中展现出卓越的零样本能力。然而，由于其数据的高可变性和复杂性，将其应用于医学影像仍然具有挑战性。具体而言，医学图像常常表现出显著的域偏移，这种偏移由各种混杂因素引起，包括设备差异、程序伪影和成像模式，这导致模型在应用于未见过的域时泛化能力差。为解决这一局限，我们提出了多模态因果驱动表征学习(MCDRL)，这是一个将因果推理与VLM相结合的新颖框架，用于解决医学图像分割中的域泛化问题。MCDRL分两步实现：首先，它利用CLIP的跨模态能力识别候选病变区域，并通过专门设计用于表示域特定变化的文本提示构建混杂因素字典；其次，它训练一个因果干预网络，利用该字典识别并消除这些域特定变化的影响，同时保留对分割任务至关重要的解剖结构信息。大量实验表明，MCDRL始终优于竞争方法，产生优越的分割精度，并表现出强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkablezero-shot capabilities in various computer vision tasks. However, theirapplication to medical imaging remains challenging due to the high variabilityand complexity of medical data. Specifically, medical images often exhibitsignificant domain shifts caused by various confounders, including equipmentdifferences, procedure artifacts, and imaging modes, which can lead to poorgeneralization when models are applied to unseen domains. To address thislimitation, we propose Multimodal Causal-Driven Representation Learning(MCDRL), a novel framework that integrates causal inference with the VLM totackle domain generalization in medical image segmentation. MCDRL isimplemented in two steps: first, it leverages CLIP's cross-modal capabilitiesto identify candidate lesion regions and construct a confounder dictionarythrough text prompts, specifically designed to represent domain-specificvariations; second, it trains a causal intervention network that utilizes thisdictionary to identify and eliminate the influence of these domain-specificvariations while preserving the anatomical structural information critical forsegmentation tasks. Extensive experiments demonstrate that MCDRL consistentlyoutperforms competing methods, yielding superior segmentation accuracy andexhibiting robust generalizability.</description>
      <author>example@mail.com (Xusheng Liang, Lihua Zhou, Nianxin Li, Miao Xu, Ziyang Song, Dong Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo, Zhen Lei)</author>
      <guid isPermaLink="false">2508.05008v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>AdvDINO: Domain-Adversarial Self-Supervised Representation Learning for Spatial Proteomics</title>
      <link>http://arxiv.org/abs/2508.04955v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AdvDINO，一种域对抗自监督学习框架，用于解决医学图像中的域偏移问题，特别是在多通道多重免疫荧光(mIF)图像中的应用。&lt;h4&gt;背景&lt;/h4&gt;自监督学习(SSL)是一种无需人工标注即可学习视觉表征的强大方法。然而，标准SSL方法对域偏移（数据源间的系统性差异）的鲁棒性尚不确定，这在批效应可能掩盖真实生物信号的生物医学成像中构成特别严峻的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够减少域偏移影响的自监督学习方法，以便在生物医学成像等领域学习更鲁棒和生物学上有意义的表征。&lt;h4&gt;方法&lt;/h4&gt;提出了AdvDINO，一种域对抗自监督学习框架，将梯度反转层整合到DINOv2架构中，以促进域不变特征学习。该方法应用于来自非小细胞肺癌患者的六通道多重免疫荧光(mIF)全切片图像。&lt;h4&gt;主要发现&lt;/h4&gt;AdvDINO能够减轻切片特异性偏差，学习比非对抗基线更鲁棒和生物学上有意义的表征。在超过546万个mIF图像切片上，模型发现了具有不同蛋白质谱特征和预后意义的表型聚类，并在基于注意力的多实例学习中改善了生存预测。&lt;h4&gt;结论&lt;/h4&gt;AdvDINO虽然在mIF数据上进行了演示，但可广泛应用于其他成像领域，包括放射学、遥感和自动驾驶，在这些领域中，域偏移和有限的标注数据阻碍了模型的泛化性和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习（SSL）已成为一种无需人工标注即可学习视觉表征的强大方法。然而，标准SSL方法对域偏移（数据源间的系统性差异）的鲁棒性仍然不确定，这在批效应可能掩盖真实生物信号的生物医学成像中构成了特别严峻的挑战。我们提出了AdvDINO，一种域对抗自监督学习框架，它将梯度反转层整合到DINOv2架构中，以促进域不变特征学习。应用于来自非小细胞肺癌患者的六通道多重免疫荧光（mIF）全切片图像的真实队列中，AdvDINO减轻了切片特异性偏差，学习了比非对抗基线更鲁棒和生物学上有意义的表征。在超过546万个mIF图像切片上，模型发现了具有不同蛋白质谱特征和预后意义的表型聚类，并在基于注意力的多实例学习中改善了生存预测。虽然在mIF数据上进行了演示，但AdvDINO可广泛应用于其他成像领域——包括放射学、遥感和自动驾驶——在这些领域中，域偏移和有限的标注数据阻碍了模型的泛化性和可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has emerged as a powerful approach forlearning visual representations without manual annotations. However, therobustness of standard SSL methods to domain shift -- systematic differencesacross data sources -- remains uncertain, posing an especially criticalchallenge in biomedical imaging where batch effects can obscure true biologicalsignals. We present AdvDINO, a domain-adversarial self-supervised learningframework that integrates a gradient reversal layer into the DINOv2architecture to promote domain-invariant feature learning. Applied to areal-world cohort of six-channel multiplex immunofluorescence (mIF) whole slideimages from non-small cell lung cancer patients, AdvDINO mitigatesslide-specific biases to learn more robust and biologically meaningfulrepresentations than non-adversarial baselines. Across $&gt;5.46$ million mIFimage tiles, the model uncovers phenotype clusters with distinct proteomicprofiles and prognostic significance, and improves survival prediction inattention-based multiple instance learning. While demonstrated on mIF data,AdvDINO is broadly applicable to other imaging domains -- including radiology,remote sensing, and autonomous driving -- where domain shift and limitedannotated data hinder model generalization and interpretability.</description>
      <author>example@mail.com (Stella Su, Marc Harary, Scott J. Rodig, William Lotter)</author>
      <guid isPermaLink="false">2508.04955v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Understanding protein function with a multimodal retrieval-augmented foundation model</title>
      <link>http://arxiv.org/abs/2508.04724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PoET-2是一个多模态、检索增强的蛋白质基础模型，通过结合家族特异性进化约束和结构条件，学习蛋白质序列的生成分布，在零样本变异效应预测和序列-功能关系学习中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;蛋白质语言模型通过学习数亿个自然蛋白质序列获得了蛋白质理解和设计能力，但研究表明扩展模型虽能改善结构预测，却不能提升突变理解和表示质量，用于蛋白质功能预测。&lt;h4&gt;目的&lt;/h4&gt;引入PoET-2模型，结合家族特异性进化约束的上下文学习和可选结构条件，学习蛋白质序列的生成分布，以提升蛋白质功能预测和突变理解能力。&lt;h4&gt;方法&lt;/h4&gt;PoET-2使用层次化transformer编码器，采用双解码器架构，包含因果和掩码语言建模目标，允许在完全生成和双向表示学习模式下运行，结合检索增强与多模态、以家族为中心的建模方法。&lt;h4&gt;主要发现&lt;/h4&gt;PoET-2在零样本变异效应预测上达到最先进性能，特别擅长评分多重突变和indel突变；在监督设置中，其嵌入在学习序列-功能关系方面优于先前方法，尤其在小数据集上表现突出。&lt;h4&gt;结论&lt;/h4&gt;结合检索增强与多模态、以家族为中心的建模方法对推进蛋白质基础模型具有显著益处。&lt;h4&gt;翻译&lt;/h4&gt;蛋白质语言模型(PLMs)学习自然蛋白质序列的概率分布。通过学习数亿个自然蛋白质序列，蛋白质理解和设计能力逐渐显现。最近的研究表明，扩展这些模型可以改善结构预测，但似乎不能改善突变理解和表示质量，用于蛋白质功能预测。我们引入PoET-2，一个多模态、检索增强的蛋白质基础模型，它结合了家族特异性进化约束的上下文学习，并可选择性地添加结构条件，以学习蛋白质序列的生成分布。PoET-2使用一个层次化的transformer编码器，对序列上下文顺序等变，并采用双解码器架构，包含因果和掩码语言建模目标，允许PoET-2在完全生成和双向表示学习模式下运行。PoET-2在零样本变异效应预测上达到了最先进的性能，在评分多重突变和具有挑战性的indel突变方面表现出色。在监督设置中，PoET-2嵌入在学习序列-功能关系方面优于先前方法，特别是在小数据集上。这项工作强调了结合检索增强与多模态、以家族为中心的建模方法对于推进蛋白质基础模型的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein language models (PLMs) learn probability distributions over naturalprotein sequences. By learning from hundreds of millions of natural proteinsequences, protein understanding and design capabilities emerge. Recent workshave shown that scaling these models improves structure prediction, but doesnot seem to improve mutation understanding and representation quality forprotein function prediction. We introduce PoET-2, a multimodal,retrieval-augmented protein foundation model that incorporates in-contextlearning of family-specific evolutionary constraints with optional structureconditioning to learn generative distributions over protein sequences. PoET-2uses a hierarchical transformer encoder that is equivariant to sequence contextordering and a dual decoder architecture with both causal and masked languagemodeling objectives, allowing PoET-2 to operate in both fully generative andbidirectional representation learning modes. PoET-2 achieves state-of-the-artperformance on zero-shot variant effect prediction, excelling at scoringvariants with multiple mutations and challenging indel mutations. In supervisedsettings, PoET-2 embeddings outperform previous methods for learningsequence-function relationships, especially with small datasets. This workhighlights the benefits of combining retrieval augmentation with multimodal,family-centric modeling for advancing protein foundation models.</description>
      <author>example@mail.com (Timothy Fei Truong Jr, Tristan Bepler)</author>
      <guid isPermaLink="false">2508.04724v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>When Deepfake Detection Meets Graph Neural Network:a Unified and Lightweight Learning Framework</title>
      <link>http://arxiv.org/abs/2508.05526v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种轻量级的空域-频谱-时域图神经网络框架SSTGNN，用于检测AI生成和操纵的视频，该方法在保持高性能的同时大幅减少了模型参数。&lt;h4&gt;背景&lt;/h4&gt;生成式视频模型的激增使得检测AI生成和操纵的视频成为紧迫挑战。现有检测方法往往无法泛化到多种操纵类型，且通常需要大型模型才能表现良好。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级视频检测框架，能够有效检测多种类型的AI生成和操纵视频，并减少模型参数需求。&lt;h4&gt;方法&lt;/h4&gt;提出SSTGNN框架，将视频表示为结构化图，实现空域、时域和频谱信息的联合推理，整合可学习频谱滤波器和时域差分建模到基于图的架构中。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化基准数据集上，SSTGNN在域内和跨域设置中表现优异，对未见过操纵具有强鲁棒性，且参数量比最先进模型少42.4倍。&lt;h4&gt;结论&lt;/h4&gt;SSTGNN是一种高效、轻量级的视频检测框架，能有效检测多种AI生成和操纵视频，适合实际部署应用。&lt;h4&gt;翻译&lt;/h4&gt;生成式视频模型的激增使得检测AI生成和操纵的视频成为紧迫的挑战。现有的检测方法往往无法泛化到多种操纵类型，因为它们依赖于孤立的空域、时域或频谱信息，并且通常需要大型模型才能表现良好。本文介绍了SSTGNN，一种轻量级的空域-频谱-时域图神经网络框架，该框架将视频表示为结构化图，能够对空间不一致性、时间伪影和频谱失真进行联合推理。SSTGNN将可学习的频谱滤波器和时域差分建模整合到基于图的架构中，能够更有效地捕捉微妙的操纵痕迹。在多样化基准数据集上的大量实验表明，SSTGNN不仅在域内和跨域设置中取得了优异的性能，而且对未见的操纵也具有很强的鲁棒性。值得注意的是，SSTGNN以比最先进模型少42.4倍的参数实现了这些结果，使其非常轻量级且可扩展，适合实际部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of generative video models has made detecting AI-generatedand manipulated videos an urgent challenge. Existing detection approaches oftenfail to generalize across diverse manipulation types due to their reliance onisolated spatial, temporal, or spectral information, and typically requirelarge models to perform well. This paper introduces SSTGNN, a lightweightSpatial-Spectral-Temporal Graph Neural Network framework that represents videosas structured graphs, enabling joint reasoning over spatial inconsistencies,temporal artifacts, and spectral distortions. SSTGNN incorporates learnablespectral filters and temporal differential modeling into a graph-basedarchitecture, capturing subtle manipulation traces more effectively. Extensiveexperiments on diverse benchmark datasets demonstrate that SSTGNN not onlyachieves superior performance in both in-domain and cross-domain settings, butalso offers strong robustness against unseen manipulations. Remarkably, SSTGNNaccomplishes these results with up to 42.4$\times$ fewer parameters thanstate-of-the-art models, making it highly lightweight and scalable forreal-world deployment.</description>
      <author>example@mail.com (Haoyu Liu, Chaoyu Gong, Mengke He, Jiate Li, Kai Han, Siqiang Luo)</author>
      <guid isPermaLink="false">2508.05526v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Models Transform Organic Synthesis From Reaction Prediction to Automation</title>
      <link>http://arxiv.org/abs/2508.05427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型语言模型正在改变有机合成领域，可作为实验室伙伴提出合成路线、预测反应结果并指导机器人实验。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型经过数百万个已报道反应的训练，开始重塑化学家规划和进行有机合成反应的方式。&lt;h4&gt;目的&lt;/h4&gt;调查大型语言模型从推测工具转变为实用实验室伙伴的里程碑。&lt;h4&gt;方法&lt;/h4&gt;将大型语言模型与图神经网络、量子计算和实时光谱学相结合。&lt;h4&gt;主要发现&lt;/h4&gt;这种方法可以缩短发现周期，支持更环保、数据驱动的化学；但也存在数据集偏见、推理不透明和安全问题等局限性。&lt;h4&gt;结论&lt;/h4&gt;通过开放基准、联邦学习和可解释界面等社区倡议，人工智能和自动化支持的快速、可靠和包容性分子创新正在成为可能。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)开始重塑化学家规划和进行有机合成反应的方式。这些基于文本的模型经过数百万个已报道反应的训练，可以提出合成路线、预测反应结果，甚至指导在没有人工监督的情况下执行实验的机器人。我们调查了将大型语言模型从推测工具转变为实用实验室伙伴的里程碑。我们展示了如何将大型语言模型与图神经网络、量子计算和实时光谱学相结合，以缩短发现周期并支持更环保、数据驱动的化学。我们讨论了局限性，包括有偏见的数据集、不透明的推理以及需要防止意外危害的安全机制。最后，我们概述了社区倡议，如开放基准、联邦学习和可解释的界面，旨在促进民主化访问同时保持人类的控制权。这些进展为人工智能和自动化支持的快速、可靠和包容性分子创新铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) are beginning to reshape how chemists plan andrun reactions in organic synthesis. Trained on millions of reportedtransformations, these text-based models can propose synthetic routes, forecastreaction outcomes and even instruct robots that execute experiments withouthuman supervision. Here we survey the milestones that turned LLMs fromspeculative tools into practical lab partners. We show how coupling LLMs withgraph neural networks, quantum calculations and real-time spectroscopy shrinksdiscovery cycles and supports greener, data-driven chemistry. We discusslimitations, including biased datasets, opaque reasoning and the need forsafety gates that prevent unintentional hazards. Finally, we outline communityinitiatives open benchmarks, federated learning and explainable interfaces thataim to democratize access while keeping humans firmly in control. Theseadvances chart a path towards rapid, reliable and inclusive molecularinnovation powered by artificial intelligence and automation.</description>
      <author>example@mail.com (Kartar Kumar Lohana Tharwani, Rajesh Kumar, Sumita, Numan Ahmed, Yong Tang)</author>
      <guid isPermaLink="false">2508.05427v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Pixels: Medical Image Quality Assessment with Implicit Neural Representations</title>
      <link>http://arxiv.org/abs/2508.05168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in 16th Machine Learning in Medical Imaging (MLMI 2025)  workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于隐式神经表示的医学图像质量评估方法，解决了传统方法中的信息损失和高内存需求问题，通过开发深度权重空间网络、图神经网络和关系注意变换器，在保持相似性能的同时减少了参数数量。&lt;h4&gt;背景&lt;/h4&gt;伪影在医学成像中构成重大挑战，影响诊断准确性和下游分析。虽然基于图像的伪影检测方法可能有效，但它们通常依赖于预处理方法，这些方法可能导致信息损失和高内存需求的医学图像，从而限制了分类模型的扩展性。&lt;h4&gt;目的&lt;/h4&gt;提出使用隐式神经表示进行图像质量评估，解决传统方法中的信息损失和高内存需求问题。&lt;h4&gt;方法&lt;/h4&gt;开发了深度权重空间网络、图神经网络和关系注意变换器，这些网络在隐式神经表示上运行以实现图像质量评估。隐式神经表示提供医学图像的紧凑且连续的表示，自然处理分辨率和图像大小的变化，同时减少内存开销。&lt;h4&gt;主要发现&lt;/h4&gt;在ACDC数据集上使用合成的伪影模式评估了该方法，证明了其在评估图像质量方面的有效性，同时用更少的参数实现了相似的性能。&lt;h4&gt;结论&lt;/h4&gt;隐式神经表示提供了一种有效的方法来评估医学图像质量，同时减少内存需求和提高可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;伪影在医学成像中构成重大挑战，影响诊断准确性和下游分析。虽然基于图像的伪影检测方法可能有效，但它们通常依赖于预处理方法，这些方法可能导致信息损失和高内存需求的医学图像，从而限制了分类模型的扩展性。在这项工作中，我们提出使用隐式神经表示进行图像质量评估。隐式神经表示提供医学图像的紧凑且连续的表示，自然处理分辨率和图像大小的变化，同时减少内存开销。我们开发了在隐式神经表示上运行的深度权重空间网络、图神经网络和关系注意变换器，以实现图像质量评估。我们的方法在ACDC数据集上使用合成的伪影模式进行了评估，证明了其在评估图像质量方面的有效性，同时用更少的参数实现了相似的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artifacts pose a significant challenge in medical imaging, impactingdiagnostic accuracy and downstream analysis. While image-based approaches fordetecting artifacts can be effective, they often rely on preprocessing methodsthat can lead to information loss and high-memory-demand medical images,thereby limiting the scalability of classification models. In this work, wepropose the use of implicit neural representations (INRs) for image qualityassessment. INRs provide a compact and continuous representation of medicalimages, naturally handling variations in resolution and image size whilereducing memory overhead. We develop deep weight space networks, graph neuralnetworks, and relational attention transformers that operate on INRs to achieveimage quality assessment. Our method is evaluated on the ACDC dataset withsynthetically generated artifact patterns, demonstrating its effectiveness inassessing image quality while achieving similar performance with fewerparameters.</description>
      <author>example@mail.com (Caner Özer, Patryk Rygiel, Bram de Wilde, İlkay Öksüz, Jelmer M. Wolterink)</author>
      <guid isPermaLink="false">2508.05168v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Event Log Repair</title>
      <link>http://arxiv.org/abs/2508.05145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种异构图神经网络模型，用于重建过程挖掘中事件日志的缺失信息。该模型能够处理包含不完整事件的轨迹，并返回这些事件中缺失的所有属性。&lt;h4&gt;背景&lt;/h4&gt;事件日志质量在过程挖掘分析中至关重要，但现实世界中的事件日志经常因数据获取困难而存在信息缺失问题。标准方法要么需要过程模型来填充缺失值，要么使用机器学习/深度学习模型从相似案例中恢复缺失值。&lt;h4&gt;目的&lt;/h4&gt;开发一种异构图神经网络模型，能够重建包含不完整事件的轨迹中缺失的所有事件属性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种异构图神经网络模型，该模型能够处理包含不完整事件的轨迹，并返回这些事件中缺失的所有属性。在两个人工合成日志和四个真实事件日志上，针对不同类型的缺失值，将该方法与利用自编码器的最先进方法进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;与现有的最先进的无模型方法相比，该提出的方法在重建所有不同的事件属性方面表现出非常好的性能，而不仅仅是修复部分事件属性。&lt;h4&gt;结论&lt;/h4&gt;异构图神经网络模型为过程挖掘中事件日志的缺失信息重建提供了有效解决方案，能够更全面地恢复事件属性。&lt;h4&gt;翻译&lt;/h4&gt;在过程挖掘中应用任何形式的分析时，事件日志的质量至关重要。在现实世界的事件日志中，数据获取可能很复杂（例如，由于手动活动的执行和相关记录，或者由于无法为每个事件收集其所有属性的问题），并且经常会导致记录的事件缺少某些信息。轨迹（或日志）重建问题的标准方法要么需要可用的过程模型，通过利用不同的推理技术来填充缺失值，要么采用机器学习/深度学习模型通过从相似案例中学习来恢复缺失值。近年来，出现了一种新型深度学习模型，能够处理编码为图的输入数据，即图神经网络。图神经网络模型，尤其是异构图神经网络模型，具有优势，能够更好地表示过程挖掘中执行轨迹等复杂多模态序列，从而实现更具表现力和语义丰富的编码。在这项工作中，我们专注于开发一种异构图神经网络模型，该模型在给定包含某些不完整事件的轨迹时，将返回这些事件中缺失的全部属性集。我们在两个人工合成日志和四个真实事件日志上，针对不同类型的缺失值，将我们的工作与利用自编码器的最先进方法进行了评估。与主要关注修复部分事件属性的最先进无模型方法不同，所提出的方法在重建所有不同事件属性方面表现出非常好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The quality of event logs in Process Mining is crucial when applying any formof analysis to them. In real-world event logs, the acquisition of data can benon-trivial (e.g., due to the execution of manual activities and related manualrecording or to issues in collecting, for each event, all its attributes), andoften may end up with events recorded with some missing information. Standardapproaches to the problem of trace (or log) reconstruction either require theavailability of a process model that is used to fill missing values byleveraging different reasoning techniques or employ a Machine Learning/DeepLearning model to restore the missing values by learning from similar cases. Inrecent years, a new type of Deep Learning model that is capable of handlinginput data encoded as graphs has emerged, namely Graph Neural Networks. GraphNeural Network models, and even more so Heterogeneous Graph Neural Networks,offer the advantage of working with a more natural representation of complexmulti-modal sequences like the execution traces in Process Mining, allowing formore expressive and semantically rich encodings.  In this work, we focus on the development of a Heterogeneous Graph NeuralNetwork model that, given a trace containing some incomplete events, willreturn the full set of attributes missing from those events. We evaluate ourwork against a state-of-the-art approach leveraging autoencoders on twosynthetic logs and four real event logs, on different types of missing values.Different from state-of-the-art model-free approaches, which mainly focus onrepairing a subset of event attributes, the proposed approach shows very goodperformance in reconstructing all different event attributes.</description>
      <author>example@mail.com (Sebastiano Dissegna, Chiara Di Francescomarino, Massimiliano Ronzani)</author>
      <guid isPermaLink="false">2508.05145v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality</title>
      <link>http://arxiv.org/abs/2508.05025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了增强现实(AR)系统中的认知隧道效应对情境意识(SA)的影响，特别关注AR指导的心肺复苏(CPR)场景。研究开发了Magic Leap 2上的AR应用，结合眼动追踪分析提出FixGraphPool模型，有效预测用户的情境意识水平。&lt;h4&gt;背景&lt;/h4&gt;增强现实系统通过实时指导提高任务表现，但可能导致认知隧道效应，即过度关注虚拟内容而损害安全关键场景中的情境意识。在CPR过程中，救援人员需要在有效按压和警惕不可预测危险之间取得平衡。&lt;h4&gt;目的&lt;/h4&gt;研究AR指导CPR中的情境意识，开发提供实时CPR反馈的AR应用，评估用户在模拟意外事件中的情境意识表现，并利用眼动数据构建预测模型。&lt;h4&gt;方法&lt;/h4&gt;在Magic Leap 2上开发AR应用，覆盖实时CPR反馈(按压深度和速率)；进行用户研究模拟意外事件；通过观察和冻结探针事件问卷收集情境意识指标；使用眼动追踪分析注视模式；提出FixGraphPool图神经网络模型，将注视事件构建为时空图。&lt;h4&gt;主要发现&lt;/h4&gt;高水平情境意识与更大的扫视幅度和速度相关；在虚拟内容上的注视比例和频率降低与更好的情境意识相关；FixGraphPool模型达到83.0%准确率(F1=81.0%)，优于传统机器学习和时间序列模型。&lt;h4&gt;结论&lt;/h4&gt;眼动追踪在AR情境意识建模中具有潜力；研究结果有助于设计确保用户安全和情境意识的AR系统，通过优化注意力分配来减轻认知隧道效应。&lt;h4&gt;翻译&lt;/h4&gt;增强现实(AR)系统虽然通过实时指导提高任务表现，但存在引发认知隧道效应的风险——即过度关注虚拟内容，损害安全关键场景中的情境意识(SA)。本文研究AR指导的心肺复苏(CPR)中的情境意识，救援人员需要在有效按压与警惕不可预测危险(如患者呕吐)之间取得平衡。我们在Magic Leap 2上开发了覆盖实时CPR反馈(按压深度和速率)的AR应用，并进行了用户研究，通过模拟意外事件(如出血)评估情境意识，情境意识指标通过观察和冻结探针事件期间的管理问卷收集。眼动分析显示，更高水平的情境意识与更大的扫视幅度和速度相关，与在虚拟内容上的注视比例和频率降低相关。为预测情境意识，我们提出FixGraphPool，一种将注视事件(注视、扫视)构建为时空图的图神经网络，有效捕捉动态注意力模式。我们的模型达到83.0%准确率(F1=81.0%)，通过利用眼动数据中编码的领域知识和时空信息，优于基于特征的机器学习和最先进的时间序列模型。这些发现展示了眼动追踪在AR中情境意识建模的潜力，并强调了其在设计确保用户安全和情境意识的AR系统方面的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Augmented Reality (AR) systems, while enhancing task performance throughreal-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus onvirtual content that compromises situational awareness (SA) in safety-criticalscenarios. This paper investigates SA in AR-guided cardiopulmonaryresuscitation (CPR), where responders must balance effective compressions withvigilance to unpredictable hazards (e.g., patient vomiting). We developed an ARapp on a Magic Leap 2 that overlays real-time CPR feedback (compression depthand rate) and conducted a user study with simulated unexpected incidents (e.g.,bleeding) to evaluate SA, in which SA metrics were collected via observationand questionnaires administered during freeze-probe events. Eye trackinganalysis revealed that higher SA levels were associated with greater saccadicamplitude and velocity, and with reduced proportion and frequency of fixationson virtual content. To predict SA, we propose FixGraphPool, a graph neuralnetwork that structures gaze events (fixations, saccades) into spatiotemporalgraphs, effectively capturing dynamic attentional patterns. Our model achieved83.0% accuracy (F1=81.0%), outperforming feature-based machine learning andstate-of-the-art time-series models by leveraging domain knowledge andspatial-temporal information encoded in ET data. These findings demonstrate thepotential of eye tracking for SA modeling in AR and highlight its utility indesigning AR systems that ensure user safety and situational awareness.</description>
      <author>example@mail.com (Zhehan Qu, Tianyi Hu, Christian Fronk, Maria Gorlatova)</author>
      <guid isPermaLink="false">2508.05025v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)</title>
      <link>http://arxiv.org/abs/2508.04894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了大型语言模型与图结构数据整合后的脆弱性，发现它们容易受到对抗性攻击，并提出了新的防御框架GALGUARD。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)越来越多地与图结构数据整合用于节点分类等任务，这一领域传统上由图神经网络(GNNs)主导。虽然这种整合利用了丰富的关系信息来提高任务性能，但其对对抗性攻击的鲁棒性尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;探索图感知的大型语言模型的脆弱性，研究这些模型在面对针对基于图的模型的对抗性攻击时的表现。&lt;h4&gt;方法&lt;/h4&gt;利用现有的针对基于图的模型的对抗性攻击方法，包括投毒(训练时攻击)和规避(测试时攻击)，在LLAGA和GRAPHPROMPTER两个代表性模型上进行测试。还发现了LLAGA的一个新攻击面，并进行了系统分析研究图编码设计选择如何影响攻击成功率。&lt;h4&gt;主要发现&lt;/h4&gt;(1) LLAGA中的节点序列模板增加了其脆弱性；(2) GRAPHPROMPTER中使用的GNN编码器表现出更强的鲁棒性；(3) 两种方法都容易受到不可感知的特征扰动攻击。&lt;h4&gt;结论&lt;/h4&gt;研究揭示了图感知大型语言模型的脆弱性，并提出了一个端到端的防御框架GALGUARD，该框架结合了基于LLM的特征校正模块来减轻特征级别的扰动，以及适应的GNN防御来保护结构攻击。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)越来越多地与图结构数据整合，用于节点分类等任务，这一领域传统上由图神经网络(GNNs)主导。虽然这种整合利用了丰富的关系信息来提高任务性能，但其对对抗性攻击的鲁棒性尚未被探索。我们通过利用现有的针对基于图的模型的对抗性攻击方法，包括投毒(训练时攻击)和规避(测试时攻击)，在两个代表性模型LLAGA(Chen et al. 2024)和GRAPHPROMPTER(Liu et al. 2024)上进行了首次探索。此外，我们还发现了LLAGA的一个新攻击面，攻击者可以将恶意节点作为占位符注入到节点序列模板中，严重降低其性能。我们的系统分析揭示了图编码中的某些设计选择可以增强攻击成功率，具体发现包括：(1) LLAGA中的节点序列模板增加了其脆弱性；(2) GRAPHPROMPTER中使用的GNN编码器表现出更强的鲁棒性；(3) 两种方法都容易受到不可感知的特征扰动攻击。最后，我们提出了一个端到端的防御框架GALGUARD，该框架结合了基于LLM的特征校正模块来减轻特征级别的扰动，并采用适应的GNN防御来保护结构攻击。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) are increasingly integrated withgraph-structured data for tasks like node classification, a domaintraditionally dominated by Graph Neural Networks (GNNs). While this integrationleverages rich relational information to improve task performance, theirrobustness against adversarial attacks remains unexplored. We take the firststep to explore the vulnerabilities of graph-aware LLMs by leveraging existingadversarial attack methods tailored for graph-based models, including those forpoisoning (training-time attacks) and evasion (test-time attacks), on tworepresentative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al.2024). Additionally, we discover a new attack surface for LLAGA where anattacker can inject malicious nodes as placeholders into the node sequencetemplate to severely degrade its performance. Our systematic analysis revealsthat certain design choices in graph encoding can enhance attack success, withspecific findings that: (1) the node sequence template in LLAGA increases itsvulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greaterrobustness; and (3) both approaches remain susceptible to imperceptible featureperturbation attacks. Finally, we propose an end-to-end defense frameworkGALGUARD, that combines an LLM-based feature correction module to mitigatefeature-level perturbations and adapted GNN defenses to protect againststructural attacks.</description>
      <author>example@mail.com (Iyiola E. Olatunji, Franziska Boenisch, Jing Xu, Adam Dziedzic)</author>
      <guid isPermaLink="false">2508.04894v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Probing and Enhancing the Robustness of GNN-based QEC Decoders with Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2508.03783v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种使用强化学习代理系统探测图神经网络(GNN)解码器漏洞的新框架，应用于量子纠错领域，并通过对抗性训练显著提高了解码器的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)已成为一种强大的数据驱动方法用于量子纠错解码，能够直接从综合症数据中学习复杂的噪声特征。然而，这些解码器对微妙、对抗性扰动的鲁棒性仍然是一个关键开放问题。&lt;h4&gt;目的&lt;/h4&gt;引入一个新框架，使用强化学习(RL)代理系统地探测GNN解码器的漏洞，提高量子纠错解码器的可靠性。&lt;h4&gt;方法&lt;/h4&gt;将RL代理训练为对手，目标是找到导致解码器错误分类的最小综合症修改。将此框架应用于在Google Quantum AI的实验表面码数据上训练的图注意力网络(GAT)解码器，并通过对抗性训练增强解码器鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;RL代理能够成功识别特定的、关键漏洞，以最少的位翻转实现高攻击成功率。通过在RL代理生成的对抗样本上重新训练模型，解码器的鲁棒性得到显著增强。&lt;h4&gt;结论&lt;/h4&gt;这种自动化漏洞发现和针对性再训练的迭代过程，为开发更可靠和更强大的神经网络解码器用于容错量子计算提供了一种有前途的方法论。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为一种强大的数据驱动方法用于量子纠错(QEC)解码，能够直接从综合症数据中学习复杂的噪声特征。然而，这些解码器对微妙、对抗性扰动的鲁棒性仍然是一个关键开放问题。本研究引入了一种新框架，使用强化学习(RL)代理系统地探测GNN解码器的漏洞。RL代理被训练为对手，目标是找到导致解码器错误分类的最小综合症修改。我们将此框架应用于在Google Quantum AI的实验表面码数据上训练的图注意力网络(GAT)解码器。我们的结果表明，RL代理能够成功识别特定的、关键漏洞，以最少的位翻转实现高攻击成功率。此外，我们展示了通过对抗性训练可以显著提高解码器的鲁棒性，其中模型在RL代理生成的对抗样本上重新训练。这种自动化漏洞发现和针对性再训练的迭代过程，为开发更可靠和更强大的神经网络解码器用于容错量子计算提供了一种有前途的方法论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as a powerful, data-driven approachfor Quantum Error Correction (QEC) decoding, capable of learning complex noisecharacteristics directly from syndrome data. However, the robustness of thesedecoders against subtle, adversarial perturbations remains a critical openquestion. This work introduces a novel framework to systematically probe thevulnerabilities of a GNN decoder using a reinforcement learning (RL) agent. TheRL agent is trained as an adversary with the goal of finding minimal syndromemodifications that cause the decoder to misclassify. We apply this framework toa Graph Attention Network (GAT) decoder trained on experimental surface codedata from Google Quantum AI. Our results show that the RL agent cansuccessfully identify specific, critical vulnerabilities, achieving a highattack success rate with a minimal number of bit flips. Furthermore, wedemonstrate that the decoder's robustness can be significantly enhanced throughadversarial training, where the model is retrained on the adversarial examplesgenerated by the RL agent. This iterative process of automated vulnerabilitydiscovery and targeted retraining presents a promising methodology fordeveloping more reliable and robust neural network decoders for fault-tolerantquantum computing.</description>
      <author>example@mail.com (Ryota Ikeda)</author>
      <guid isPermaLink="false">2508.03783v2</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation</title>
      <link>http://arxiv.org/abs/2508.05508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种通用的模块化评估框架，用于独立于任务领域地评估代理任务完成情况。该框架模拟人类评估方式，通过分解任务并验证每一步，显著提高了与人类评估的一致性。&lt;h4&gt;背景&lt;/h4&gt;随着基础模型作为代理在各领域的广泛应用，迫切需要强大的评估框架。当前方法如LLM-as-a-Judge仅关注最终输出，忽略了驱动代理决策的逐步推理过程；而现有的Agent-as-a-Judge系统通常局限于狭窄的特定领域场景。&lt;h4&gt;目的&lt;/h4&gt;解决现有评估方法的局限性，提出一个通用的、模块化的框架，用于独立于任务领域地评估代理任务完成情况。&lt;h4&gt;方法&lt;/h4&gt;提出一个模拟人类评估的通用模块化框架，将任务分解为子任务，并使用代理的输出和推理等可用信息验证每一步。各模块针对评估过程的不同方面贡献评估结果，最终聚合输出产生关于任务完成的裁决。&lt;h4&gt;主要发现&lt;/h4&gt;在GAIA和BigCodeBench两个基准测试上评估Magentic-One Actor Agent时，提出的Judge Agent预测任务成功的准确性与人类评估更接近，相比基于GPT-4o的LLM-as-a-Judge基线，分别提高了4.76%和10.52%的对齐准确率。&lt;h4&gt;结论&lt;/h4&gt;所提出的通用评估框架具有巨大潜力，能够有效评估代理任务完成情况，且不受任务领域限制。&lt;h4&gt;翻译&lt;/h4&gt;随着基础模型作为代理在各个领域的日益普及，迫切需要一个强大的评估框架。当前的方法如LLM-as-a-Judge只关注最终输出，忽略了驱动代理决策的逐步推理过程。同时，现有的Agent-as-a-Judge系统（其中一个代理评估另一个代理的任务完成情况）通常是为狭窄的、特定领域的场景设计的。为了解决这一差距，我们提出了一个通用的、模块化的框架，用于独立于任务领域地评估代理任务完成情况。该框架通过将任务分解为子任务并使用可用信息（如代理的输出和推理）验证每一步来模拟人类评估方式。每个模块对评估过程的一个特定方面做出贡献，并将它们的输出聚合起来，产生关于任务完成的最终裁决。我们通过在GAIA和BigCodeBench两个基准测试上评估Magentic-One Actor Agent来验证我们的框架。我们的Judge Agent预测任务成功的准确性与人类评估更接近，相比基于GPT-4o的LLM-as-a-Judge基线，分别提高了4.76%和10.52%的对齐准确率。这证明了我们提出的通用评估框架的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing adoption of foundation models as agents across diverse domainsnecessitates a robust evaluation framework. Current methods, such asLLM-as-a-Judge, focus only on final outputs, overlooking the step-by-stepreasoning that drives agentic decision-making. Meanwhile, existingAgent-as-a-Judge systems, where one agent evaluates another's task completion,are typically designed for narrow, domain-specific settings. To address thisgap, we propose a generalizable, modular framework for evaluating agent taskcompletion independent of the task domain. The framework emulates human-likeevaluation by decomposing tasks into sub-tasks and validating each step usingavailable information, such as the agent's output and reasoning. Each modulecontributes to a specific aspect of the evaluation process, and their outputsare aggregated to produce a final verdict on task completion. We validate ourframework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIAand BigCodeBench. Our Judge Agent predicts task success with closer agreementto human evaluations, achieving 4.76% and 10.52% higher alignment accuracy,respectively, compared to the GPT-4o based LLM-as-a-Judge baseline. Thisdemonstrates the potential of our proposed general-purpose evaluationframework.</description>
      <author>example@mail.com (Roshita Bhonsle, Rishav Dutta, Sneha Vavilapalli, Harsh Seth, Abubakarr Jaye, Yapei Chang, Mukund Rungta, Emmanuel Aboah Boateng, Sadid Hasan, Ehi Nosakhare, Soundar Srinivasan)</author>
      <guid isPermaLink="false">2508.05508v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>SMOL-MapSeg: Show Me One Label</title>
      <link>http://arxiv.org/abs/2508.05501v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种OND知识提示方法，创建了SMOL-MapSeg模型，解决了基础模型在历史地图语义分割中的局限性，实现了对用户指定概念的准确分割，并能适应未见过的类别。&lt;h4&gt;背景&lt;/h4&gt;历史地图对研究地球表面变化具有重要价值。UNet等深度学习模型已被用于从历史地图中提取信息。虽然预训练基础模型在多个领域表现出色，但在历史地图上表现不佳，因为历史地图缺乏现代图像中模式与概念之间的一致性联系。&lt;h4&gt;目的&lt;/h4&gt;解决基础模型在历史地图上应用受限的问题，使模型能够准确分割历史地图中的概念，并具备适应未见过的类别的灵活性。&lt;h4&gt;方法&lt;/h4&gt;提出OND（On-Need Declarative）知识提示方法，通过引入明确提示指导模型识别哪些模式对应哪些概念。将SAM基础模型的提示编码器替换为OND提示机制，并在历史地图上进行微调，创建SMOL-MapSeg模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SMOL-MapSeg能准确分割由OND知识定义的类别，通过少量样本微调可适应未见过的类别，且在平均分割性能上优于基于UNet的基线模型。&lt;h4&gt;结论&lt;/h4&gt;OND知识提示方法有效解决了基础模型在历史地图上的应用问题，使模型能够根据用户需求灵活分割历史地图中的概念，并在未见过的类别上表现出良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;历史地图对于研究地球表面变化具有重要价值。随着深度学习的兴起，UNet等模型已被用于通过语义分割从这些地图中提取信息。最近，预训练的基础模型在自动驾驶、医学成像和工业检测等领域表现出色。然而，它们在历史地图上表现不佳。这些模型是在现代或领域特定图像上训练的，其中模式可以通过常识或专家知识与预定义概念相关联。历史地图缺乏这种一致性——相似的概念可能以截然不同的形状和样式出现。为解决这一问题，我们提出了按需声明（OND）知识提示方法，通过引入明确提示来指导模型识别哪些模式对应哪些概念。这使用户能够在推理时（按需推理）指定目标概念和模式。我们通过将基础模型SAM的提示编码器替换为OND提示机制并在历史地图上微调来实现这一点。得到的模型称为SMOL-MapSeg（Show Me One Label）。实验表明，SMOL-MapSeg能够准确分割由OND知识定义的类别。它还能通过少量样本微调适应未见过的类别。此外，它在平均分割性能上优于基于UNet的基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Historical maps are valuable for studying changes to the Earth's surface.With the rise of deep learning, models like UNet have been used to extractinformation from these maps through semantic segmentation. Recently,pre-trained foundation models have shown strong performance across domains suchas autonomous driving, medical imaging, and industrial inspection. However,they struggle with historical maps. These models are trained on modern ordomain-specific images, where patterns can be tied to predefined conceptsthrough common sense or expert knowledge. Historical maps lack such consistency-- similar concepts can appear in vastly different shapes and styles. Toaddress this, we propose On-Need Declarative (OND) knowledge-based prompting,which introduces explicit prompts to guide the model on what patternscorrespond to which concepts. This allows users to specify the target conceptand pattern during inference (on-need inference). We implement this byreplacing the prompt encoder of the foundation model SAM with our OND promptingmechanism and fine-tune it on historical maps. The resulting model is calledSMOL-MapSeg (Show Me One Label). Experiments show that SMOL-MapSeg canaccurately segment classes defined by OND knowledge. It can also adapt tounseen classes through few-shot fine-tuning. Additionally, it outperforms aUNet-based baseline in average segmentation performance.</description>
      <author>example@mail.com (Yunshuang Yuan, Frank Thiemann, Thorsten Dahms, Monika Sester)</author>
      <guid isPermaLink="false">2508.05501v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction</title>
      <link>http://arxiv.org/abs/2508.05294v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述论文关注基础模型（包括大型语言模型、视觉语言模型、视觉语言行动模型和大型行为模型）如何推动机器人自主性和人机界面的发展，特别聚焦于代理应用和架构，包括GPT风格接口工具和AI作为协调者、规划者等复杂系统，并提出分类法和比较分析。&lt;h4&gt;背景&lt;/h4&gt;基础模型（包括大型语言模型和视觉语言模型）最近为机器人自主性和人机界面带来了新方法，同时视觉语言行动模型或大型行为模型正在提高机器人系统的灵活性和能力。&lt;h4&gt;目的&lt;/h4&gt;关注那些朝着代理应用和架构发展的工作，探索GPT风格接口工具的初步尝试，以及AI作为协调者、规划者、感知行为者或通用接口的更复杂系统，并提出一个分类模型集成方法的分类法和比较分析。&lt;h4&gt;方法&lt;/h4&gt;通过综述同行评审研究，同时关注社区驱动项目、ROS包和工业框架，提出一个分类模型集成方法的分类法，并对当今文献中不同解决方案中代理的作用进行了比较分析。&lt;h4&gt;主要发现&lt;/h4&gt;代理架构使机器人能够基于自然语言指令进行推理、调用API、规划任务序列或协助操作和诊断；社区驱动项目、ROS包和工业框架展示了新兴趋势；模型集成方法可以分类，不同解决方案中代理的角色可以比较分析。&lt;h4&gt;结论&lt;/h4&gt;基础模型正在显著提升机器人系统的能力和灵活性，代理架构在机器人自主性和人机交互中扮演着越来越重要的角色，通过分类法和比较分析可以更好地理解和评估这些方法。&lt;h4&gt;翻译&lt;/h4&gt;基础模型，包括大型语言模型和视觉语言模型，最近为机器人自主性和人机界面带来了新方法。同时，视觉语言行动模型或大型行为模型正在提高机器人系统的灵活性和能力。这篇综述论文关注那些朝着代理应用和架构发展的工作。这包括探索GPT风格接口工具的初步尝试，以及AI作为协调者、规划者、感知行为者或通用接口的更复杂系统。这些代理架构使机器人能够基于自然语言指令进行推理、调用API、规划任务序列或协助操作和诊断。除了同行评审的研究，由于该领域发展迅速，作者还强调了社区驱动项目、ROS包和工业框架，这些展示了新兴趋势。作者提出了一个分类模型集成方法的分类法，并对当今文献中不同解决方案中代理的作用进行了比较分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, including large language models (LLMs) and vision-languagemodels (VLMs), have recently enabled novel approaches to robot autonomy andhuman-robot interfaces. In parallel, vision-language-action models (VLAs) orlarge behavior models (BLMs) are increasing the dexterity and capabilities ofrobotic systems. This survey paper focuses on those words advancing towardsagentic applications and architectures. This includes initial efforts exploringGPT-style interfaces to tooling, as well as more complex system where AI agentsare coordinators, planners, perception actors, or generalist interfaces. Suchagentic architectures allow robots to reason over natural languageinstructions, invoke APIs, plan task sequences, or assist in operations anddiagnostics. In addition to peer-reviewed research, due to the fast-evolvingnature of the field, we highlight and include community-driven projects, ROSpackages, and industrial frameworks that show emerging trends. We propose ataxonomy for classifying model integration approaches and present a comparativeanalysis of the role that agents play in different solutions in today'sliterature.</description>
      <author>example@mail.com (Sahar Salimpour, Lei Fu, Farhad Keramat, Leonardo Militano, Giovanni Toffetti, Harry Edelman, Jorge Peña Queralta)</author>
      <guid isPermaLink="false">2508.05294v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>FlowState: Sampling Rate Invariant Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2508.05287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently under review at AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FlowState是一种新型时间序列基础模型(TSFM)架构，通过基于状态空间模型的编码器和功能基础解码器，解决了现有TSFMs在泛化能力、适应性和计算效率方面的挑战，实现了连续时间建模和动态时间尺度调整。&lt;h4&gt;背景&lt;/h4&gt;基础模型(FMs)已成功应用于自然语言处理，但尚未有效转化为时间序列预测领域。现有的时间序列基础模型通常基于变体transformer，存在泛化能力有限、对不同采样率适应性差和计算效率低等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够泛化不同上下文和目标长度、适应不同采样率且计算高效的时间序列基础模型架构。&lt;h4&gt;方法&lt;/h4&gt;提出FlowState架构，包含两个关键创新：基于状态空间模型(SSM)的编码器和功能基础解码器。这种设计实现了连续时间建模和动态时间尺度调整，使模型能够适应所有可能的时间分辨率并动态调整预测范围。同时提出了一种有效的预训练策略以提高鲁棒性和加速训练。&lt;h4&gt;主要发现&lt;/h4&gt;FlowState优于所有其他模型，在GIFT-ZS和Chronos-ZS基准测试中达到最先进水平。尽管模型尺寸最小，但性能最佳。消融研究证实了其组件的有效性，且模型能够在线适应不同输入采样率。&lt;h4&gt;结论&lt;/h4&gt;FlowState通过状态空间模型和功能基础解码器的设计，解决了现有TSFMs的关键局限性，实现了更好的泛化能力、适应性和计算效率，为时间序列预测领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基础模型(FMs)已经改变了自然语言处理，但它们尚未成功转化为时间序列预测。现有的时间序列基础模型(TSFMs)通常基于变体transformer，难以在不同上下文和目标长度间泛化，缺乏对不同采样率的适应性，且计算效率低下。我们引入了FlowState，一种新型TSFM架构，通过两个关键创新解决了这些挑战：基于状态空间模型(SSM)的编码器和功能基础解码器。这种设计实现了连续时间建模和动态时间尺度调整，使FlowState能够在所有可能的时间分辨率上泛化，并动态调整预测范围。与其他需要所有可能采样率训练数据来记忆每个尺度模式的最先进TSFMs相比，FlowState能够根据输入规模调整其内部动态，从而实现更小的模型、更少的数据需求和更高的效率。我们进一步提出了一种有效的预训练策略，提高了鲁棒性并加速了训练。尽管是最小的模型，FlowState优于所有其他模型，并在GIFT-ZS和Chronos-ZS基准测试中达到了最先进的水平。消融研究证实了其组件的有效性，我们展示了它在线适应不同输入采样率的独特能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) have transformed natural language processing, buttheir success has not yet translated to time series forecasting. Existing timeseries foundation models (TSFMs), often based on transformer variants, strugglewith generalization across varying context and target lengths, lackadaptability to different sampling rates, and are computationally inefficient.We introduce FlowState, a novel TSFM architecture that addresses thesechallenges through two key innovations: a state space model (SSM) based encoderand a functional basis decoder. This design enables continuous-time modelingand dynamic time-scale adjustment, allowing FlowState to inherently generalizeacross all possible temporal resolutions, and dynamically adjust theforecasting horizons. In contrast to other state-of-the-art TSFMs, whichrequire training data across all possible sampling rates to memorize patternsat each scale, FlowState inherently adapts its internal dynamics to the inputscale, enabling smaller models, reduced data requirements, and improvedefficiency. We further propose an efficient pretraining strategy that improvesrobustness and accelerates training. Despite being the smallest model,FlowState outperforms all other models and is state-of-the-art for the GIFT-ZSand the Chronos-ZS benchmarks. Ablation studies confirm the effectiveness ofits components, and we demonstrate its unique ability to adapt online tovarying input sampling rates.</description>
      <author>example@mail.com (Lars Graf, Thomas Ortner, Stanisław Woźniak, Angeliki Pantazi)</author>
      <guid isPermaLink="false">2508.05287v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>CF3: Compact and Fast 3D Feature Fields</title>
      <link>http://arxiv.org/abs/2508.05254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CF3是一种自上而下的3D高斯特征场构建方法，通过多视图2D特征快速融合和自适应稀疏化，实现了使用少量高斯(仅5%)的紧凑高效3D表示&lt;h4&gt;背景&lt;/h4&gt;3D Gaussian Splatting开始融入2D基础模型信息，但现有方法依赖自下而上优化过程，将原始2D特征视为真实值，导致计算成本增加&lt;h4&gt;目的&lt;/h4&gt;提出一种自上而下的流程，构建紧凑快速的3D高斯特征场(CF3)，减少计算成本同时保持性能&lt;h4&gt;方法&lt;/h4&gt;1) 使用预训练高斯快速加权融合多视图2D特征；2) 在提升特征上直接训练每个高斯的自动编码器；3) 引入自适应稀疏化方法，优化特征场高斯属性同时修剪和合并冗余高斯&lt;h4&gt;主要发现&lt;/h4&gt;自动编码器更好地与特征分布对齐；与Feature-3DGS相比，仅使用5%的高斯数量就能实现具有竞争力的3D特征场&lt;h4&gt;结论&lt;/h4&gt;CF3方法通过优化特征表示和减少高斯数量，实现了高效且保留几何细节的3D特征场&lt;h4&gt;翻译&lt;/h4&gt;3D高斯飞溅(3DGS)已经开始融入2D基础模型的丰富信息。然而，大多数方法依赖自下而上的优化过程，将原始2D特征视为真实值，导致计算成本增加。我们提出了一种自上而下的流程，用于构建紧凑快速的3D高斯特征场，即CF3。我们首先使用预训练高斯快速加权融合多视图2D特征。这种方法能够在提升的特征上直接训练每个高斯的自动编码器，而不是在2D域中训练自动编码器。因此，自动编码器更好地与特征分布对齐。更重要的是，我们引入了一种自适应稀疏化方法，在修剪和合并冗余高斯的同时优化特征场的高斯属性，构建了一种保留几何细节的高效表示。与Feature-3DGS相比，我们的方法仅使用5%的高斯数量就能实现具有竞争力的3D特征场。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何构建紧凑且快速的3D特征场的问题。这个问题在现实中很重要，因为现有的3D特征场方法（如Feature-3DGS）需要大量的存储空间（如528MB）和计算资源，导致渲染速度慢（如64.2 FPS），无法满足实时应用需求。此外，大多数方法依赖自底向上的优化过程，计算成本高，且直接从2D基础模型提取的特征往往缺乏多视图一致性，影响3D特征场的质量和实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用自顶向下的流程设计CF3方法，首先通过预训练的3DGS进行特征提升，然后进行特征压缩和自适应稀疏化。他们借鉴了现有工作中的多个方面：基于3DGS框架利用其高效渲染能力；借鉴FiT3D和CONDENSE的3D感知训练思想进行特征融合；使用自动编码器进行特征压缩但改进了训练方式，直接在3D提升的特征上训练；借鉴LightGaussian的修剪策略并引入新的合并方法。整体上，CF3是对现有技术的创新性整合和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; CF3的核心思想是通过三个主要步骤构建紧凑且快速的3D特征场：1)特征提升：使用预训练的3DGS将多视图2D特征融合到3D空间，通过加权组合实现视图一致性，并过滤掉不准确的特征；2)特征压缩：使用针对每个高斯的自动编码器将高维特征压缩到低维嵌入空间，直接在提升的特征上训练，使压缩后的特征兼容现有3DGS渲染器；3)自适应稀疏化：优化高斯属性，修剪贡献度低的高斯，合并具有相同语义信息的相邻高斯，通过马氏距离判断重叠程度，使用矩匹配计算合并后属性。整体流程从预训练的3DGS开始，经过这三个步骤，最终得到高效的3D特征场。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)自顶向下的流程，避免直接优化特征的高计算成本；2)针对每个高斯的自动编码器，直接在3D提升的特征上训练，更好地与特征分布对齐；3)自适应稀疏化方法，在优化特征场的同时修剪和合并冗余高斯，保留几何细节；4)视图一致性的特征提升，解决2D特征的多视图不一致问题。相比之前的工作，CF3使用自顶向下的流程而非自底向上优化；自动编码器在3D域而非2D域训练；引入专门针对特征场的稀疏化方法；实现了更高的压缩率（使用不到5%的高斯）和更快的渲染速度（如147 FPS）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种自顶向下的方法，通过特征提升、特征压缩和自适应稀疏化三个步骤，构建了紧凑且快速的3D特征场，使用不到5%的高斯数量实现了与现有方法相当的性能，显著提高了存储效率和渲染速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2Dfoundation models. However, most approaches rely on a bottom-up optimizationprocess that treats raw 2D features as ground truth, incurring increasedcomputational costs. We propose a top-down pipeline for constructing compactand fast 3D Gaussian feature fields, namely, CF3. We first perform a fastweighted fusion of multi-view 2D features with pre-trained Gaussians. Thisapproach enables training a per-Gaussian autoencoder directly on the liftedfeatures, instead of training autoencoders in the 2D domain. As a result, theautoencoder better aligns with the feature distribution. More importantly, weintroduce an adaptive sparsification method that optimizes the Gaussianattributes of the feature field while pruning and merging the redundantGaussians, constructing an efficient representation with preserved geometricdetails. Our approach achieves a competitive 3D feature field using as littleas 5% of the Gaussians compared to Feature-3DGS.</description>
      <author>example@mail.com (Hyunjoon Lee, Joonkyu Min, Jaesik Park)</author>
      <guid isPermaLink="false">2508.05254v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology Foundation Models</title>
      <link>http://arxiv.org/abs/2508.05084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 Tables, 11 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AdaFusion，一种提示引导的推理框架，通过动态整合多个病理学基础模型(PFMs)的互补知识，解决了PFMs因预训练背景不透明导致的潜在偏差问题。&lt;h4&gt;背景&lt;/h4&gt;病理学基础模型(PFMs)通过在大量未标注的组织病理学图像数据集上进行自监督预训练，展示了强大的表示能力。然而，这些模型多样且不透明的预训练背景引入了潜在偏差，阻碍了下游应用中的泛化能力和透明度。&lt;h4&gt;目的&lt;/h4&gt;提出一个新颖的提示引导推理框架AdaFusion，动态整合多个PFMs的互补知识，以增强模型性能和可解释性。&lt;h4&gt;方法&lt;/h4&gt;AdaFusion压缩和对齐来自不同模型的瓦片级特征，并使用轻量级注意力机制基于组织表型上下文自适应地融合这些特征。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界基准测试（治疗反应预测、肿瘤分级和空间基因表达推断）上，AdaFusion在分类和回归任务中始终超越单个PFMs，同时提供关于每个模型生物语义专业化的可解释性见解。&lt;h4&gt;结论&lt;/h4&gt;AdaFusion能够桥接异构PFMs，实现增强的性能和模型特定归纳偏差的可解释性，为病理学AI应用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;病理学基础模型(PFMs)通过在大量未标注的组织病理学图像数据集上进行自监督预训练，展示了强大的表示能力。然而，这些模型多样且不透明的预训练背景（由数据相关因素和结构/训练因素共同塑造）引入了潜在偏差，阻碍了下游应用中的泛化能力和透明度。在本文中，我们提出了AdaFusion，一种新颖的提示引导推理框架，据我们所知，这是首批动态整合多个PFMs互补知识的方法之一。我们的方法压缩和对齐来自不同模型的瓦片级特征，并使用轻量级注意力机制基于组织表型上下文自适应地融合这些特征。我们在三个真实世界基准测试上评估AdaFusion，涵盖治疗反应预测、肿瘤分级和空间基因表达推断。我们的方法在分类和回归任务中始终超越单个PFMs，同时提供关于每个模型生物语义专业化的可解释性见解。这些结果突显了AdaFusion桥接异构PFMs的能力，实现了增强的性能和模型特定归纳偏差的可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pathology foundation models (PFMs) have demonstrated strong representationalcapabilities through self-supervised pre-training on large-scale, unannotatedhistopathology image datasets. However, their diverse yet opaque pretrainingcontexts, shaped by both data-related and structural/training factors,introduce latent biases that hinder generalisability and transparency indownstream applications. In this paper, we propose AdaFusion, a novelprompt-guided inference framework that, to our knowledge, is among the veryfirst to dynamically integrate complementary knowledge from multiple PFMs. Ourmethod compresses and aligns tile-level features from diverse models andemploys a lightweight attention mechanism to adaptively fuse them based ontissue phenotype context. We evaluate AdaFusion on three real-world benchmarksspanning treatment response prediction, tumour grading, and spatial geneexpression inference. Our approach consistently surpasses individual PFMsacross both classification and regression tasks, while offering interpretableinsights into each model's biosemantic specialisation. These results highlightAdaFusion's ability to bridge heterogeneous PFMs, achieving both enhancedperformance and interpretability of model-specific inductive biases.</description>
      <author>example@mail.com (Yuxiang Xiao, Yang Hu, Bin Li, Tianyang Zhang, Zexi Li, Huazhu Fu, Jens Rittscher, Kaixiang Yang)</author>
      <guid isPermaLink="false">2508.05084v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion</title>
      <link>http://arxiv.org/abs/2508.04984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE TIP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种利用深度基础模型增强深度补全鲁棒性的新框架，通过双空间传播和可学习校正模块，在无需大规模训练的情况下显著提升了模型在分布外场景的性能。&lt;h4&gt;背景&lt;/h4&gt;深度补全是计算机视觉中的关键挑战，现有基于学习的模型依赖于有限的数据，导致在分布外场景中性能下降，而深度基础模型在大规模训练下展示了单目深度估计的卓越鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够提升深度补全模型鲁棒性的方法，使其在分布外场景中表现优异，同时避免依赖大规模训练数据。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的深度补全框架，利用深度基础模型从RGB图像中提取环境线索（包括结构和语义上下文）来引导稀疏深度信息的传播；设计了双空间传播方法，在3D和2D空间传播稀疏深度，无需可学习参数，以保持几何结构和局部一致性；引入了可学习校正模块，逐步调整深度预测以优化复杂结构。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在16个不同数据集上的广泛评估中表现优异，在分布外场景中显著优于现有的最先进深度补全方法，证明了其在实际应用中的有效性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过利用深度基础模型和创新的传播策略，所提出的深度补全框架能够在不依赖大规模训练的情况下，显著提升模型在分布外场景的性能，为深度补全领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;深度补全是计算机视觉中的一个关键挑战，旨在从稀疏深度图（通常配有RGB图像）重建密集深度图。现有的基于学习的模型依赖于精心准备但有限的数据，导致在分布外（OOD）场景中性能显著下降。最近的基础模型通过大规模训练在单目深度估计中展示了卓越的鲁棒性，利用这样的模型来增强深度补全模型的鲁棒性是一个有前景的解决方案。在这项工作中，我们提出了一个新的深度补全框架，利用深度基础模型获得显著的鲁棒性，而无需大规模训练。具体来说，我们利用深度基础模型从RGB图像中提取环境线索，包括结构和语义上下文，以引导稀疏深度信息向缺失区域的传播。我们进一步设计了一个双空间传播方法，无需任何可学习参数，在3D和2D空间有效传播稀疏深度，以保持几何结构和局部一致性。为了优化复杂结构，我们引入了一个可学习校正模块，逐步调整深度预测向真实深度靠近。我们在NYUv2和KITTI数据集上训练我们的模型作为分布内数据集，并在其他16个数据集上广泛评估该框架。我们的框架在OOD场景中表现卓越，并优于现有的最先进深度补全方法。我们的模型发布在https://github.com/shenglunch/PSD。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决深度补全在分布外场景下的性能下降问题。深度补全旨在从稀疏深度图重建密集深度图，这对自动驾驶、机器人导航、3D重建等应用至关重要，因为这些领域需要准确的深度信息来感知环境。现有方法在训练数据集上表现良好，但在新场景中性能显著下降，限制了实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了深度补全在OOD场景下降的原因，认为受RGB图像外观特征和稀疏深度特征两个因素影响。他们借鉴了单目深度估计的基础模型（如Depth Anything、MiDaS）在大规模训练中展现的鲁棒性，以及现有的深度传播方法（如CSPN）。作者创新性地将基础模型生成的深度图作为空间结构线索而非直接使用其预测结果，设计了双空间传播方法结合3D几何结构和2D局部一致性，并引入校正模块修正基础模型可能引入的失真。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练深度基础模型的鲁棒性增强深度补全在OOD场景的性能，通过双空间传播结合稀疏深度信息和基础模型提供的结构/语义线索重建密集深度图。流程包括：1)输入稀疏深度图和RGB图像；2)使用基础模型生成空间结构点云；3)在3D空间基于空间结构传播深度，在2D空间基于语义上下文传播深度；4)通过校正模块修正初始深度预测中的失真；5)输出最终密集深度图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次系统性地将深度基础模型的能力转移到深度补全任务中；2)设计双空间传播方法在3D和2D空间传播稀疏深度；3)引入可学习校正模块修正基础模型可能引入的失真；4)在16个OOD数据集上展示了出色的零样本泛化能力。相比之前的工作，本文方法无需大规模训练数据就能获得出色的OOD性能，不同于传统深度补全方法局限于特定数据集，也不同于其他OOD方法依赖大规模合成数据或仅在2D空间传播。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于深度基础模型的双空间稀疏深度传播框架，显著提升了深度补全在分布外场景下的鲁棒性和泛化能力，无需大规模训练数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Depth completion is a pivotal challenge in computer vision, aiming atreconstructing the dense depth map from a sparse one, typically with a pairedRGB image. Existing learning based models rely on carefully prepared butlimited data, leading to significant performance degradation inout-of-distribution (OOD) scenarios. Recent foundation models have demonstratedexceptional robustness in monocular depth estimation through large-scaletraining, and using such models to enhance the robustness of depth completionmodels is a promising solution. In this work, we propose a novel depthcompletion framework that leverages depth foundation models to attainremarkable robustness without large-scale training. Specifically, we leverage adepth foundation model to extract environmental cues, including structural andsemantic context, from RGB images to guide the propagation of sparse depthinformation into missing regions. We further design a dual-space propagationapproach, without any learnable parameters, to effectively propagates sparsedepth in both 3D and 2D spaces to maintain geometric structure and localconsistency. To refine the intricate structure, we introduce a learnablecorrection module to progressively adjust the depth prediction towards the realdepth. We train our model on the NYUv2 and KITTI datasets as in-distributiondatasets and extensively evaluate the framework on 16 other datasets. Ourframework performs remarkably well in the OOD scenarios and outperformsexisting state-of-the-art depth completion methods. Our models are released inhttps://github.com/shenglunch/PSD.</description>
      <author>example@mail.com (Shenglun Chen, Xinzhu Ma, Hong Zhang, Haojie Li, Zhihui Wang)</author>
      <guid isPermaLink="false">2508.04984v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>MENDR: Manifold Explainable Neural Data Representations</title>
      <link>http://arxiv.org/abs/2508.04956v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为MENDR的EEG基础模型，基于黎曼流形变换器架构和滤波器组方法，学习EEG信号的对称正定矩阵嵌入，具有更好的可解释性和临床适用性。&lt;h4&gt;背景&lt;/h4&gt;当前EEG基础模型在下游任务中表现优异，但缺乏预训练动态的透明度，且对EEG信息在嵌入中的保留情况了解有限。现有方法主要在时域操作，忽略了数字信号处理领域的进展，如小波表示等特征提取方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个透明的、可解释的EEG基础模型，确保预训练、下游微调和学习表示的可解释性，同时整合数字信号处理领域的先进技术，如小波变换。&lt;h4&gt;方法&lt;/h4&gt;提出了MENDR（Manifold Explainable Neural Data Representations），一种基于滤波器组的EEG基础模型，构建在新型黎曼流形变换器架构上。该模型学习EEG信号的对称正定矩阵嵌入，并在包含超过4,000小时EEG数据的大型语料库上预训练，通过离散小波包变换分解为多分辨率系数。&lt;h4&gt;主要发现&lt;/h4&gt;MENDR通过将对称正定嵌入可视化为几何椭球体显著提高了可解释性，并支持从学习到的嵌入准确重建EEG信号。在多个临床EEG任务上的评估表明，MENDR以少得多的参数实现了接近最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;MENDR为高效、可解释和临床适用的EEG分析提供了潜力，解决了现有EEG基础模型在透明度和可解释性方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;脑电图（EEG）信号的基础模型最近在学习EEG的通用表示方面取得了成功，在各种下游任务中优于专用模型。然而，这些模型中的许多在预训练动态方面缺乏透明度，并且对其嵌入中保留了多少EEG信息提供了有限的见解。为了成功的临床整合，EEG基础模型必须确保预训练、下游微调和学习表示的可解释性的透明度。当前方法主要在时域操作，忽略了数字信号处理的进展，这些进展能够提取确定性和可追踪的特征，例如基于小波的表示。我们提出了MENDR（流形可解释神经数据表示），这是一种基于滤波器组的EEG基础模型，构建在新型黎曼流形变换器架构上，以解决这些问题。MENDR学习EEG信号的对称正定矩阵嵌入，并在一个大型语料库上预训练，该语料库包含超过4,000小时的EEG数据，通过离散小波包变换分解为多分辨率系数。MENDR通过将对称正定嵌入可视化为几何椭球体显著提高了可解释性，并支持从学习到的嵌入准确重建EEG信号。在多个临床EEG任务上的评估表明，MENDR以少得多的参数实现了接近最先进的性能，强调了其在高效、可解释和临床适用的EEG分析方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for electroencephalography (EEG) signals have recentlydemonstrated success in learning generalized representations of EEGs,outperforming specialized models in various downstream tasks. However, many ofthese models lack transparency in their pretraining dynamics and offer limitedinsight into how well EEG information is preserved within their embeddings. Forsuccessful clinical integration, EEG foundation models must ensure transparencyin pretraining, downstream fine-tuning, and the interpretability of learnedrepresentations. Current approaches primarily operate in the temporal domain,overlooking advancements in digital signal processing that enable theextraction of deterministic and traceable features, such as wavelet-basedrepresentations. We propose MENDR (Manifold Explainable Neural DataRepresentations), a filter bank-based EEG foundation model built on a novelRiemannian Manifold Transformer architecture to resolve these issues. MENDRlearns symmetric positive definite matrix embeddings of EEG signals and ispretrained on a large corpus comprising over 4,000 hours of EEG data,decomposed via discrete wavelet packet transforms into multi-resolutioncoefficients. MENDR significantly enhances interpretability by visualizingsymmetric positive definite embeddings as geometric ellipsoids and supportsaccurate reconstruction of EEG signals from learned embeddings. Evaluationsacross multiple clinical EEG tasks demonstrate that MENDR achieves nearstate-of-the-art performance with substantially fewer parameters, underscoringits potential for efficient, interpretable, and clinically applicable EEGanalysis.</description>
      <author>example@mail.com (Matthew Chen, Micky Nnamdi, Justin Shao, Andrew Hornback, Hongyun Huang, Ben Tamo, Yishan Zhong, Benoit Marteau, Wenqi Shi, May Dongmei Wang)</author>
      <guid isPermaLink="false">2508.04956v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Retrieval-Augmented Water Level Forecasting for Everglades</title>
      <link>http://arxiv.org/abs/2508.04888v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入检索增强预测（RAF）框架到水文学领域，通过检索历史上类似的多变量水文事件来提高水位预测准确性，无需模型重新训练或微调。&lt;h4&gt;背景&lt;/h4&gt;准确的水位预测对管理佛罗里达大沼泽地等生态系统至关重要，尽管深度学习在通用领域预测中取得成功，但在水文学应用仍不充分，且难以在多样化数据集间推广。&lt;h4&gt;目的&lt;/h4&gt;解决深度学习模型在水文学应用中的推广问题，提出RAF框架以提高水位预测的准确性和适应性。&lt;h4&gt;方法&lt;/h4&gt;维护过去观察的外部档案，RAF识别并整合历史数据中的相关模式来增强上下文意识和预测准确性，探索比较基于相似性和基于互信息的RAF方法。&lt;h4&gt;主要发现&lt;/h4&gt;在佛罗里达大沼泽地真实世界数据上的全面评估表明，RAF框架显著提高了水位预测准确性。&lt;h4&gt;结论&lt;/h4&gt;RAF方法在水文环境中具有巨大潜力，为生态系统管理专家采用自适应AI方法铺平道路，代码和数据已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;准确的水位预测对于管理像大沼泽地这样的生态系统至关重要，这是一个对减轻洪水、管理干旱、水资源规划和生物多样性保护至关重要的亚热带湿地。虽然深度学习，特别是时间序列基础模型最近在通用领域预测中显示出成功，但它们在水文学领域的应用仍探索不足。此外，由于缺乏有效的适应机制，它们通常难以在多样化的未见数据集和领域之间推广。为了解决这一差距，我们将检索增强预测（RAF）引入水文学领域，提出了一个在预测前检索历史上类似的多变量水文事件来丰富模型输入的框架。通过维护过去观察的外部档案，RAF识别并整合历史数据中的相关模式，从而增强上下文意识和预测准确性，而无需模型进行任务特定的重新训练或微调。此外，我们探索并比较了基于相似性和基于互信息的RAF方法。我们在佛罗里达大沼泽地的真实世界数据上进行了全面评估，证明RAF框架在水文预测准确性方面取得了显著改进。这项研究突显了RAF方法在水文环境中的潜力，并为生态系统管理领域的专家采用自适应AI方法铺平了道路。代码和数据可在https://github.com/rahuul2992000/WaterRAF获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate water level forecasting is crucial for managing ecosystems such asthe Everglades, a subtropical wetland vital for flood mitigation, droughtmanagement, water resource planning, and biodiversity conservation. Whilerecent advances in deep learning, particularly time series foundation models,have demonstrated success in general-domain forecasting, their application inhydrology remains underexplored. Furthermore, they often struggle to generalizeacross diverse unseen datasets and domains, due to the lack of effectivemechanisms for adaptation. To address this gap, we introduceRetrieval-Augmented Forecasting (RAF) into the hydrology domain, proposing aframework that retrieves historically analogous multivariate hydrologicalepisodes to enrich the model input before forecasting. By maintaining anexternal archive of past observations, RAF identifies and incorporates relevantpatterns from historical data, thereby enhancing contextual awareness andpredictive accuracy without requiring the model for task-specific retraining orfine-tuning. Furthermore, we explore and compare both similarity-based andmutual information-based RAF methods. We conduct a comprehensive evaluation onreal-world data from the Everglades, demonstrating that the RAF frameworkyields substantial improvements in water level forecasting accuracy. This studyhighlights the potential of RAF approaches in environmental hydrology and pavesthe way for broader adoption of adaptive AI methods by domain experts inecosystem management. The code and data are available athttps://github.com/rahuul2992000/WaterRAF.</description>
      <author>example@mail.com (Rahuul Rangaraj, Jimeng Shi, Rajendra Paudel, Giri Narasimhan, Yanzhao Wu)</author>
      <guid isPermaLink="false">2508.04888v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>PriceFM: Foundation Model for Probabilistic Electricity Price Forecasting</title>
      <link>http://arxiv.org/abs/2508.04875v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了PriceFM模型，一个基于图的时空基础模型，用于捕捉欧洲电力市场的空间相互依赖性，并构建了涵盖24个欧洲国家（38个区域）的全面数据集，时间跨度从2022年到2025年。&lt;h4&gt;背景&lt;/h4&gt;欧洲电力市场日益整合和物理互联，使电力价格预测面临独特挑战。虽然深度学习和基础模型在时间序列预测方面取得了进展，但大多数现有方法无法捕捉电力市场固有的复杂空间相互依赖性和不确定性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在捕捉电力市场复杂空间相互依赖性和不确定性方面的局限性，提高欧洲电力价格预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;构建了涵盖24个欧洲国家（38个区域）的全面数据集，时间跨度从2022年1月1日到2025年1月1日；提出了PriceFM模型，一个集成了基于图归纳偏置的时空基础模型，用于多区域、多时间步长和多分位数的概率电力价格预测。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验和消融研究证实了模型的有效性，PriceFM持续优于竞争基线，强调了空间背景在电力市场中的重要性。&lt;h4&gt;结论&lt;/h4&gt;PriceFM模型能够有效捕捉欧洲电力市场的空间相互依赖性，提高了电力价格预测的准确性。研究团队公开了数据集和代码（https://github.com/runyao-yu/PriceFM）。&lt;h4&gt;翻译&lt;/h4&gt;欧洲电力价格预测面临着独特的挑战，这是由于该大陆日益整合和物理互联的电力市场。尽管深度学习和基础模型的最新进展已在通用时间序列预测方面取得了实质性改进，但大多数现有方法无法捕捉电力市场固有的复杂空间相互依赖性和不确定性。在本文中，我们通过引入一个涵盖24个欧洲国家（38个区域）的全面且最新的数据集来解决这些局限性，时间跨度从2022年1月1日到2025年1月1日。在此基础上，我们提出了PriceFM，一个时空基础模型，它集成了基于图的归纳偏置来捕捉相互关联的电力市场之间的空间相互依赖性。该模型专为多区域、多时间步长和多分位数概率电力价格预测而设计。大量的实验和消融研究证实了模型的有效性，持续优于竞争基线，并突显了空间背景在电力市场中的重要性。数据集和代码可以在https://github.com/runyao-yu/PriceFM找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electricity price forecasting in Europe presents unique challenges due to thecontinent's increasingly integrated and physically interconnected power market.While recent advances in deep learning and foundation models have led tosubstantial improvements in general time series forecasting, most existingapproaches fail to capture the complex spatial interdependencies anduncertainty inherent in electricity markets. In this paper, we address theselimitations by introducing a comprehensive and up-to-date dataset across 24European countries (38 regions), spanning from 2022-01-01 to 2025-01-01.Building on this groundwork, we propose PriceFM, a spatiotemporal foundationmodel that integrates graph-based inductive biases to capture spatialinterdependencies across interconnected electricity markets. The model isdesigned for multi-region, multi-timestep, and multi-quantile probabilisticelectricity price forecasting. Extensive experiments and ablation studiesconfirm the model's effectiveness, consistently outperforming competitivebaselines and highlighting the importance of spatial context in electricitymarkets. The dataset and code can be found athttps://github.com/runyao-yu/PriceFM.</description>
      <author>example@mail.com (Runyao Yu, Chenhui Gu, Jochen Stiasny, Qingsong Wen, Wasim Sarwar Dilov, Lianlian Qi, Jochen L. Cremer)</author>
      <guid isPermaLink="false">2508.04875v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM</title>
      <link>http://arxiv.org/abs/2508.04795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in the 2025 IEEE Automatic Speech Recognition and  Understanding Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索了在对话转录流水线中添加元数据标签以丰富转录内容的方法，通过结合冻结的音频基础模型和语言模型实现说话者特征推断。&lt;h4&gt;背景&lt;/h4&gt;在对话转录流水线中，大型语言模型(LLMs)经常用于后处理以改善语法、标点和可读性。&lt;h4&gt;目的&lt;/h4&gt;探索一个互补的后处理步骤：通过添加说话者特征（如年龄、性别和情感）的元数据标签来丰富转录的对话，其中一些标签是对整个对话的全局标签，一些是随时间变化的标签。&lt;h4&gt;方法&lt;/h4&gt;将冻结的音频基础模型（如Whisper或WavLM）与冻结的LLAMA语言模型相结合，无需对任一模型进行任务特定的微调；使用轻量级、高效的连接器来桥接音频和语言表示。&lt;h4&gt;主要发现&lt;/h4&gt;冻结的LLAMA模型可以直接比较x向量，在某些场景下实现了8.8%的等错误率；在保持模块化和速度的同时，在说话者画像任务上实现了有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;通过结合冻结的音频基础模型和语言模型，可以有效地为转录的对话添加元数据标签，而无需对模型进行特定任务的微调，同时保持良好的性能和效率。&lt;h4&gt;翻译&lt;/h4&gt;在对话转录流水线中，大型语言模型(LLMs)经常用于后处理以改善语法、标点和可读性。我们探索了一个互补的后处理步骤：通过添加说话者特征（如年龄、性别和情感）的元数据标签来丰富转录的对话。其中一些标签是对整个对话的全局标签，一些是随时间变化的标签。我们的方法将冻结的音频基础模型（如Whisper或WavLM）与冻结的LLAMA语言模型相结合，来推断这些说话者属性，无需对任一模型进行任务特定的微调。使用轻量级、高效的连接器来桥接音频和语言表示，我们在保持模块化和速度的同时，在说话者画像任务上实现了有竞争力的性能。此外，我们证明了冻结的LLAMA模型可以直接比较x向量，在某些场景下实现了8.8%的等错误率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In dialogue transcription pipelines, Large Language Models (LLMs) arefrequently employed in post-processing to improve grammar, punctuation, andreadability. We explore a complementary post-processing step: enrichingtranscribed dialogues by adding metadata tags for speaker characteristics suchas age, gender, and emotion. Some of the tags are global to the entiredialogue, while some are time-variant. Our approach couples frozen audiofoundation models, such as Whisper or WavLM, with a frozen LLAMA language modelto infer these speaker attributes, without requiring task-specific fine-tuningof either model. Using lightweight, efficient connectors to bridge audio andlanguage representations, we achieve competitive performance on speakerprofiling tasks while preserving modularity and speed. Additionally, wedemonstrate that a frozen LLAMA model can compare x-vectors directly, achievingan Equal Error Rate of 8.8% in some scenarios.</description>
      <author>example@mail.com (Thomas Thebaud, Yen-Ju Lu, Matthew Wiesner, Peter Viechnicki, Najim Dehak)</author>
      <guid isPermaLink="false">2508.04795v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>WiFo-CF: Wireless Foundation Model for CSI Feedback</title>
      <link>http://arxiv.org/abs/2508.04068v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WiFo-CF是一种创新的无线基础模型，通过多用户多速率自监督预训练和共享与路由专家混合架构，解决了传统CSI反馈方案在系统配置灵活性方面的限制，实现了对异构配置的适应。&lt;h4&gt;背景&lt;/h4&gt;现有的基于深度学习的信道状态信息反馈方案表现出强大的压缩能力，但通常受限于固定系统配置，限制了它们的泛化和灵活性。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够适应异构配置(如变化的信道维度、反馈速率和数据分布)的CSI反馈方案，提高模型的通用性和灵活性。&lt;h4&gt;方法&lt;/h4&gt;提出了WiFo-CF，一种专门为CSI反馈设计的无线基础模型，通过两个关键创新实现：(1)多用户、多速率自监督预训练策略；(2)共享和路由专家混合(S-R MoE)架构。同时创建了第一个支持大规模预训练的异构信道反馈数据集。&lt;h4&gt;主要发现&lt;/h4&gt;该数据集的多样化模式使模型在模拟和真实场景中的分布内和分布外数据上都能实现卓越性能。此外，学习的表示有效地促进了基于CSI的室内定位等下游任务的适应。&lt;h4&gt;结论&lt;/h4&gt;WiFo-CF具有出色的可扩展性和实际部署潜力，能够有效处理各种无线通信场景中的CSI反馈需求。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的信道状态信息反馈方案展现出强大的压缩能力，但通常受限于固定系统配置，限制了它们的泛化和灵活性。为应对这一挑战，本文提出了WiFo-CF，一种专为CSI反馈设计的新型无线基础模型，通过其关键创新独特地适应异构配置，如变化的信道维度、反馈速率和数据分布：(1)多用户、多速率自监督预训练策略；(2)共享和路由专家混合(S-R MoE)架构。支持WiFo-CF大规模预训练的是首个异构信道反馈数据集，其多样化模式使模型在模拟和真实场景中的分布内和分布外数据上都能实现卓越性能。此外，学习到的表示有效地促进了基于CSI的室内定位等下游任务的适应，验证了WiFo-CF的可扩展性和部署潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based channel state information (CSI) feedback schemesdemonstrate strong compression capabilities but are typically constrained tofixed system configurations, limiting their generalization and flexibility. Toaddress this challenge, WiFo-CF, a novel wireless foundation model tailored forCSI feedback, is proposed, uniquely accommodating heterogeneous configurationssuch as varying channel dimensions, feedback rates, and data distributionswithin a unified framework through its key innovations: (1) a multi-user,multi-rate self-supervised pre-training strategy; and (2) a Mixture of Sharedand Routed Expert (S-R MoE) architecture. Supporting the large-scalepre-training of WiFo-CF is the first heterogeneous channel feedback dataset,whose diverse patterns enable the model to achieve superior performance on bothin-distribution and out-of-distribution data across simulated and real-worldscenarios. Furthermore, the learned representations effectively facilitateadaptation to downstream tasks such as CSI-based indoor localization,validating WiFo-CF's scalability and deployment potential.</description>
      <author>example@mail.com (Xuanyu Liu, Shijian Gao, Boxun Liu, Xiang Cheng, Liuqing Yang)</author>
      <guid isPermaLink="false">2508.04068v2</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety</title>
      <link>http://arxiv.org/abs/2508.05527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the Computer Vision in Advertising and Marketing (CVAM)  workshop at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了多模态大语言模型(MLLMs)在品牌安全分类任务中的能力，这是一个内容审核的关键子领域。研究团队创建了新的多模态多语言数据集，并比较了Gemini、GPT和Llama等模型与专业人类审核员的性能。&lt;h4&gt;背景&lt;/h4&gt;在线视频内容量呈指数级增长，导致不安全视频审核需求超出人类处理能力，带来运营和心理健康挑战。尽管MLLMs在视频理解任务中表现出色，但在需要同时理解视觉和文本线索的多模态内容审核领域应用仍不充分。&lt;h4&gt;目的&lt;/h4&gt;评估MLLMs在品牌安全分类方面的能力，这是保护广告完整性的关键内容审核环节。&lt;h4&gt;方法&lt;/h4&gt;创建了一个新的多模态多语言数据集，由专业评审人员在多个风险类别中仔细标记。通过详细比较分析，评估了Gemini、GPT和Llama等MLLMs在多模态品牌安全任务中的有效性和成本效率。&lt;h4&gt;主要发现&lt;/h4&gt;MLLMs在多模态品牌安全任务中表现出色，研究还揭示了这些模型的局限性和失败案例，并比较了它们与专业人类审核员的准确性和成本效率。&lt;h4&gt;结论&lt;/h4&gt;研究团队公开发布了所创建的数据集，以促进未来对有效和负责任的品牌安全及内容审核的研究。&lt;h4&gt;翻译&lt;/h4&gt;随着在线视频内容的指数级增长，对不安全视频进行审核的需求已经超出了人类能力范围，带来了运营和心理健康方面的挑战。虽然最近的研究证明了多模态大语言模型(MLLMs)在各种视频理解任务中的优势，但它们在需要同时理解视觉和文本线索的多模态内容审核领域的应用仍然相对未被探索。在这项工作中，我们评估了MLLMs在品牌安全分类方面的能力，这是内容审核的一个关键子集，对于保护广告完整性至关重要。为此，我们引入了一个新颖的、多模态和多语言的数据集，由专业评审人员在多个风险类别中仔细标记。通过详细的比较分析，我们展示了Gemini、GPT和Llama等MLLMs在多模态品牌安全方面的有效性，并将其准确性和成本效率与专业人类评审人员进行比较。此外，我们进行了深入讨论，揭示了MLLMs的局限性和失败案例。我们正在随论文一起发布我们的数据集，以促进对未来有效和负责任的品牌安全和内容审核的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the volume of video content online grows exponentially, the demand formoderation of unsafe videos has surpassed human capabilities, posing bothoperational and mental health challenges. While recent studies demonstrated themerits of Multimodal Large Language Models (MLLMs) in various videounderstanding tasks, their application to multimodal content moderation, adomain that requires nuanced understanding of both visual and textual cues,remains relatively underexplored. In this work, we benchmark the capabilitiesof MLLMs in brand safety classification, a critical subset of contentmoderation for safe-guarding advertising integrity. To this end, we introduce anovel, multimodal and multilingual dataset, meticulously labeled byprofessional reviewers in a multitude of risk categories. Through a detailedcomparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini,GPT, and Llama in multimodal brand safety, and evaluate their accuracy and costefficiency compared to professional human reviewers. Furthermore, we present anin-depth discussion shedding light on limitations of MLLMs and failure cases.We are releasing our dataset alongside this paper to facilitate future researchon effective and responsible brand safety and content moderation.</description>
      <author>example@mail.com (Adi Levi, Or Levi, Sardhendu Mishra, Jonathan Morra)</author>
      <guid isPermaLink="false">2508.05527v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding</title>
      <link>http://arxiv.org/abs/2508.04369v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为时间采样策略优化(TSPO)的方法，通过强化学习提升多模态大语言模型(MLLMs)对长视频的理解能力，解决了现有方法在处理长视频时面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在视觉语言任务中已取得显著进展，但在处理长视频输入时仍面临上下文限制和训练成本的挑战，需要在输入前进行稀疏帧采样。现有方法如均匀采样或关键帧搜索可能错过关键事件或受限于预训练模型的能力。&lt;h4&gt;目的&lt;/h4&gt;解决MLLMs处理长视频时的局限性，通过提出TSPO方法优化时间采样策略，提升模型对长视频语言理解的能力。&lt;h4&gt;方法&lt;/h4&gt;提出可训练的事件感知时间代理捕捉事件查询相关性；设计TSPO强化学习范式将关键帧选择和语言生成作为联合决策过程；构建包含全面时间数据和视频Needle-in-a-Haystack数据的长视频训练数据管道；整合基于规则的回答准确性和时间定位奖励机制。&lt;h4&gt;主要发现&lt;/h4&gt;TSPO在多个长视频理解基准测试中取得了最先进的性能，并且在不同前沿视频MLLMs中显示出可迁移的能力。&lt;h4&gt;结论&lt;/h4&gt;TSPO方法有效解决了MLLMs处理长视频输入的挑战，通过强化学习优化时间采样策略，显著提升了模型对长视频的理解能力，代码已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)在视觉语言任务中已显示出显著进展，但在处理长视频输入时仍面临挑战。这种限制源于MLLMs的上下文限制和训练成本，需要在将视频输入MLLMs之前进行稀疏帧采样。现有的视频MLLMs采用无需训练的均匀采样或关键帧搜索，这可能会错过关键事件或受限于预训练模型的事件理解能力。同时，由于稀疏帧采样的无监督和不可微分特性，构建基于训练的方法仍然具有挑战性。为解决这些问题，我们提出了时间采样策略优化(TSPO)，通过强化学习提升MLLMs对长视频语言理解的能力。具体而言，我们首先提出了一种可训练的事件感知时间代理，它捕捉事件查询相关性以执行概率性关键帧选择。然后，我们提出了TSPO强化学习范式，将关键帧选择和语言生成建模为联合决策过程，实现基于规则的高效奖励的端到端分组相对优化。此外，为了TSPO的训练，我们提出了包含全面时间数据和视频Needle-in-a-Haystack数据的长视频训练数据构建流程。最后，我们整合了基于规则的回答准确性和时间定位奖励机制来优化时间采样策略。全面的实验表明，我们的TSPO在多个长视频理解基准测试中取得了最先进的性能，并在不同的前沿视频MLLMs中显示出可迁移的能力。我们的代码可在https://github.com/Hui-design/TSPO获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have demonstrated significantprogress in vision-language tasks, yet they still face challenges whenprocessing long-duration video inputs. The limitation arises from MLLMs'context limit and training costs, necessitating sparse frame sampling beforefeeding videos into MLLMs. Existing video MLLMs adopt training-free uniformsampling or keyframe search, which may miss critical events or be constrainedby the pre-trained models' event understanding capabilities. Meanwhile,building a training-based method remains challenging due to the unsupervisedand non-differentiable nature of sparse frame sampling. To address theseproblems, we propose Temporal Sampling Policy Optimization (TSPO), advancingMLLMs' long-form video-language understanding via reinforcement learning.Specifically, we first propose a trainable event-aware temporal agent, whichcaptures event-query correlation for performing probabilistic keyframeselection. Then, we propose the TSPO reinforcement learning paradigm, whichmodels keyframe selection and language generation as a joint decision-makingprocess, enabling end-to-end group relative optimization with efficientrule-based rewards. Furthermore, for the TSPO's training, we propose a longvideo training data construction pipeline with comprehensive temporal data andvideo Needle-in-a-Haystack data. Finally, we incorporate rule-based answeringaccuracy and temporal locating reward mechanisms to optimize the temporalsampling policy. Comprehensive experiments show that our TSPO achievesstate-of-the-art performance across multiple long video understandingbenchmarks, and shows transferable ability across different cutting-edgeVideo-MLLMs. Our code is available at https://github.com/Hui-design/TSPO</description>
      <author>example@mail.com (Canhui Tang, Zifan Han, Hongbo Sun, Sanping Zhou, Xuchong Zhang, Xin Wei, Ye Yuan, Jinglin Xu, Hao Sun)</author>
      <guid isPermaLink="false">2508.04369v2</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>LuKAN: A Kolmogorov-Arnold Network Framework for 3D Human Motion Prediction</title>
      <link>http://arxiv.org/abs/2508.04847v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LuKAN的3D人体运动预测模型，基于Kolmogorov-Arnold Networks和Lucas多项式激活函数，通过离散小波变换和时间依赖学习器实现了高效且准确的预测。&lt;h4&gt;背景&lt;/h4&gt;3D人体运动预测的目标是基于历史运动数据预测人体的未来3D姿态。现有方法往往难以在预测准确性和计算效率之间取得平衡。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在保持预测准确性的同时提高计算效率的3D人体运动预测模型。&lt;h4&gt;方法&lt;/h4&gt;提出LuKAN模型，包含以下步骤：1)使用离散小波变换编码输入运动序列的时间信息；2)使用空间投影层捕捉关节间依赖关系；3)采用由Lucas多项式参数化的KAN层作为时间依赖学习器进行高效函数逼近；4)通过逆离散小波变换在时间域重建运动序列。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准数据集上的大量实验表明，与强基线方法相比，LuKAN模型具有竞争力，定量和定性评估都证明了其优越性能。&lt;h4&gt;结论&lt;/h4&gt;LuKAN模型的紧凑架构结合Lucas多项式的线性递归特性，确保了计算效率，同时保持了较高的预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;3D人体运动预测的目标是基于历史运动数据预测人体的未来3D姿态。现有方法往往难以在预测准确性和计算效率之间取得平衡。在本文中，我们提出了LuKAN，这是一种基于Kolmogorov-Arnold Networks和Lucas多项式激活函数的有效模型。我们的模型首先应用离散小波变换来编码输入运动序列中的时间信息。然后，使用空间投影层来捕捉关节间的依赖关系，确保人体结构的连贯性。LuKAN的核心是时间依赖学习器，它采用由Lucas多项式参数化的KAN层进行高效函数逼近。这些多项式提供了计算效率并增强处理振荡行为的能力。最后，逆离散小波变换在时间域重建运动序列，生成时间上连贯的预测。在三个基准数据集上的大量实验表明，与强基线方法相比，我们的模型具有竞争力，这得到了定量和定性评估的证明。此外，其紧凑的架构结合Lucas多项式的线性递归特性，确保了计算效率。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D人体运动预测中预测准确性和计算效率难以平衡的问题。这个问题在现实中非常重要，因为3D人体运动预测在人机交互、动画制作、自动驾驶等领域有广泛应用，这些应用既需要高精度的预测结果，又需要模型在资源受限环境中高效运行。现有的方法各有局限：循环神经网络难以处理长期序列，图卷积网络容易出现过度平滑，Transformer计算复杂度高，而多层感知机虽然计算开销小但使用固定激活函数且需要深度架构来建模复杂关系。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，然后注意到Kolmogorov-Arnold Networks(KANs)作为多层感知机的替代品表现出优越性能。作者借鉴了现有工作中使用频域编码的方法，但认为离散余弦变换(DCT)在捕获局部运动模式方面受限。基于这些分析，作者设计了LuKAN模型，主要借鉴了KANs架构、离散小波变换(DWT)用于时域编码、空间投影层捕捉关节关系，并创新性地使用Lucas多项式作为激活函数。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于Kolmogorov-Arnold Networks的模型，结合离散小波变换和Lucas多项式激活函数，在保持计算效率的同时提高预测准确性。整体实现流程包括：1)接收历史3D人体运动序列；2)应用离散小波变换编码时域信息，将轨迹分解为低频和高频分量；3)通过空间投影层捕捉关节间依赖关系；4)使用时序依赖学习器(含KAN层和Lucas多项式)建模时序模式；5)应用第二个空间投影和逆离散小波变换将数据映射回时间域；6)生成预测的3D姿势序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用离散小波变换而非离散余弦变换编码时域信息，能更好地捕捉局部运动模式；2)使用Kolmogorov-Arnold Networks替代传统多层感知机，提供更大灵活性；3)创新性地使用Lucas多项式作为激活函数，计算效率高且性能优越；4)设计紧凑架构，在保持高性能的同时减少参数数量和计算成本。相比之前的工作，LuKAN保留高频细节而非通过扩散过程修改运动信号，使用可学习的1D函数自适应建模复杂时序依赖，并能同时捕获粗粒度和细粒度的运动模式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LuKAN通过结合Kolmogorov-Arnold Networks和离散小波变换，实现了在保持计算效率的同时提高3D人体运动预测准确性的创新方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of 3D human motion prediction is to forecast future 3D poses of thehuman body based on historical motion data. Existing methods often facelimitations in achieving a balance between prediction accuracy andcomputational efficiency. In this paper, we present LuKAN, an effective modelbased on Kolmogorov-Arnold Networks (KANs) with Lucas polynomial activations.Our model first applies the discrete wavelet transform to encode temporalinformation in the input motion sequence. Then, a spatial projection layer isused to capture inter-joint dependencies, ensuring structural consistency ofthe human body. At the core of LuKAN is the Temporal Dependency Learner, whichemploys a KAN layer parameterized by Lucas polynomials for efficient functionapproximation. These polynomials provide computational efficiency and anenhanced capability to handle oscillatory behaviors. Finally, the inversediscrete wavelet transform reconstructs motion sequences in the time domain,generating temporally coherent predictions. Extensive experiments on threebenchmark datasets demonstrate the competitive performance of our modelcompared to strong baselines, as evidenced by both quantitative and qualitativeevaluations. Moreover, its compact architecture coupled with the linearrecurrence of Lucas polynomials, ensures computational efficiency.</description>
      <author>example@mail.com (Md Zahidul Hasan, A. Ben Hamza, Nizar Bouguila)</author>
      <guid isPermaLink="false">2508.04847v1</guid>
      <pubDate>Fri, 08 Aug 2025 14:45:58 +0800</pubDate>
    </item>
    <item>
      <title>Occupancy Learning with Spatiotemporal Memory</title>
      <link>http://arxiv.org/abs/2508.04705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV2025. Project website:  https://matthew-leng.github.io/stocc&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ST-Occ是一种场景级的占用表示学习框架，通过时空记忆和记忆注意力两个核心设计，有效学习具有时间一致性的时空特征，解决了跨帧聚合3D占用信息的挑战。&lt;h4&gt;背景&lt;/h4&gt;3D occupancy成为自动驾驶中很有前景的环境感知表示方法，可以精细建模周围环境，但存在高处理成本和体素不确定性、动态性等挑战。&lt;h4&gt;目的&lt;/h4&gt;解决高效跨多个输入帧聚合3D占用信息的挑战，提高3D占用预测任务中的时空表示质量。&lt;h4&gt;方法&lt;/h4&gt;提出ST-Occ框架，包含时空记忆（捕捉全面历史信息并高效存储）和记忆注意力（将当前占用表示条件化到时空记忆上，具有不确定性和动态感知模型）两个核心设计。&lt;h4&gt;主要发现&lt;/h4&gt;通过利用多帧输入之间的时间依赖性，显著提高了3D占用预测任务中学习的时空表示。&lt;h4&gt;结论&lt;/h4&gt;ST-Occ比最先进的方法高出3 mIoU，并将时间不一致性降低了29%，表明该方法在3D占用预测任务中具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;三维占用成为自动驾驶中很有前景的感知表示方法，可以精细建模周围环境。然而，由于高处理成本和体素的不确定性和动态性，很难高效地跨多个输入帧聚合三维占用信息。为解决这一问题，我们提出了ST-Occ，一种场景级的占用表示学习框架，能够有效学习具有时间一致性的时空特征。ST-Occ包含两个核心设计：时空记忆，捕捉全面的历史信息并通过场景级表示高效存储；以及记忆注意力，将当前占用表示条件化到时空记忆上，具有不确定性和动态感知模型。我们的方法通过利用多帧输入之间的时间依赖性，显著提高了为3D占用预测任务学习的时空表示。实验表明，我们的方法比最先进的方法高出3 mIoU，并将时间不一致性降低了29%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何高效地聚合3D占用表示在多个输入帧上的时间信息，以改善自动驾驶中的环境感知。这个问题在现实中很重要，因为自动驾驶需要准确理解周围环境，而3D占用表示能提供比传统BEV表示更详细的几何和语义信息；时间信息的整合可以提高感知系统的鲁棒性，特别是在遮挡、光照变化等挑战性场景中；现有的时间融合方法在处理3D占用表示时面临效率和有效性挑战，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有时间融合方法在3D占用表示上的局限性，包括效率、不确定性和动态性问题。他们借鉴了BEVFormer（时间自注意力）和BEVDet（历史特征融合）等基于BEV的时间建模方法，以及PasCo的不确定性感知思路。在此基础上，创新性地提出了'统一时间建模'范式，设计了场景中心坐标下的时空记忆模块来高效存储历史信息，并通过记忆注意力模块实现不确定性和动态感知的融合，解决了现有方法的三个主要挑战：效率、不确定性和动态性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用场景中心坐标下的时空记忆高效存储和利用历史信息，并通过记忆注意力模块将当前帧占用表示与历史信息融合，同时处理不确定性和动态性问题。整体流程：1)输入多视图图像，通过占用编码器提取自车中心的占用表示；2)从时空记忆中提取相关历史信息；3)使用记忆注意力模块融合当前表示与历史信息，生成融合表示；4)更新时空记忆，存储融合表示和时间属性；5)使用融合表示进行占用预测。时空记忆存储历史类别激活、平均对数方差和占用流等属性，记忆注意力通过MLP预测不确定性并使用占用流补偿动态实例运动。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)统一时间建模范式，在场景中心坐标下构建统一记忆，大幅减少内存计算需求；2)时空记忆模块，存储全面历史信息；3)记忆注意力模块，实现不确定性和动态感知的融合；4)时间一致性评估指标mSTCV。相比之前工作不同：1)效率更高，训练内存比递归方法少10%，比堆叠方法少40%，推理FPS达8.65；2)性能更好，Occ3D基准测试上高出最先进方法3 mIoU；3)时间一致性降低29%；4)能更有效地利用长期时间依赖(2.8倍于FB-OCC)；5)明确处理不确定性和动态性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ST-Occ提出了一种基于时空记忆的场景级别3D占用表示学习方法，通过统一的时间建模范式，实现了高效、鲁棒且具有不确定性和动态感知能力的占用预测，显著提升了自动驾驶环境感知的性能和时间一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D occupancy becomes a promising perception representation for autonomousdriving to model the surrounding environment at a fine-grained scale. However,it remains challenging to efficiently aggregate 3D occupancy over time acrossmultiple input frames due to the high processing cost and the uncertainty anddynamics of voxels. To address this issue, we propose ST-Occ, a scene-leveloccupancy representation learning framework that effectively learns thespatiotemporal feature with temporal consistency. ST-Occ consists of two coredesigns: a spatiotemporal memory that captures comprehensive historicalinformation and stores it efficiently through a scene-level representation anda memory attention that conditions the current occupancy representation on thespatiotemporal memory with a model of uncertainty and dynamic awareness. Ourmethod significantly enhances the spatiotemporal representation learned for 3Doccupancy prediction tasks by exploiting the temporal dependency betweenmulti-frame inputs. Experiments show that our approach outperforms thestate-of-the-art methods by a margin of 3 mIoU and reduces the temporalinconsistency by 29%.</description>
      <author>example@mail.com (Ziyang Leng, Jiawei Yang, Wenlong Yi, Bolei Zhou)</author>
      <guid isPermaLink="false">2508.04705v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
  <item>
      <title>BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.04702v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出BEVCon，一个简单而有效的对比学习框架，用于提高自动驾驶中的鸟瞰图（BEV）感知能力。通过引入两个对比学习模块，该方法改进了BEV特征和图像骨干网络，在nuScenes数据集上实现了最先进基线最高2.4%的mAP提升。&lt;h4&gt;背景&lt;/h4&gt;鸟瞰图（BEV）感知提供了周围环境自上而下的视图表示，对3D目标检测、分割和轨迹预测等任务至关重要。以往的研究主要集中在增强BEV编码器和任务特定的头部，而忽略了BEV模型中表征学习的潜力。&lt;h4&gt;目的&lt;/h4&gt;探索表征学习在BEV感知模型中的潜力，通过对比学习框架来提高BEV感知性能。&lt;h4&gt;方法&lt;/h4&gt;BEVCon引入了两个对比学习模块：1）实例特征对比模块，用于优化BEV特征；2）视角对比模块，增强图像骨干网络。基于检测损失的密集对比学习改进了BEV编码器和骨干网络的特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的大量实验表明，BEVCons实现了持续的性能提升，比最先进的基线最高提升2.4%的mAP。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了表征学习在BEV感知中的关键作用，并为传统的任务特定优化提供了互补的途径。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了BEVCon，一个简单而有效的对比学习框架，旨在提高自动驾驶中的鸟瞰图（BEV）感知。BEV感知提供了周围环境自上而下的视图表示，使其对3D目标检测、分割和轨迹预测任务至关重要。虽然先前的工作主要集中在增强BEV编码器和任务特定的头部，但我们解决了BEV模型中表征学习的未被充分探索的潜力。BEVCon引入了两个对比学习模块：一个用于优化BEV特征的实例特征对比模块和一个增强图像骨干网络的视角对比模块。在检测损失基础上设计的密集对比学习导致BEV编码器和骨干网络的改进特征表示。在nuScenes数据集上的大量实验证明，BEVCons实现了持续的性能提升，比最先进的基线最高提升2.4%的mAP。我们的结果强调了表征学习在BEV感知中的关键作用，并为传统的任务特定优化提供了互补的途径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决BEV（鸟瞰图）感知模型中的表示学习不足问题。当前研究主要集中在优化BEV编码器和任务特定头，而对表示学习的潜力探索较少。这个问题在现实中非常重要，因为BEV感知是自动驾驶和机器人系统的关键组件，提供统一的环境表示用于3D目标检测、分割和轨迹预测等任务。改进表示学习可以在不增加额外数据或标注的情况下，充分利用现有训练数据中的信息，提升模型性能，这对实际应用中数据有限和硬件受限的场景尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先进行了初步实验，发现传统的对比学习方法（如MoCo v2）在BEV感知任务中效果不佳，原因包括驾驶数据集样本多样性不足，以及图像级对比方法不适合关注特定对象的需求。基于这些发现，作者设计了专门的BEVCon框架，包含两个对比学习模块：实例特征对比模块和透视视图对比模块。作者借鉴了SimCLR等对比学习的基本思想，但针对BEV感知的特殊挑战进行了定制化设计，如使用区域感知池化解决标注框重叠问题，以及多层级特征对比增强监督信号。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入两个专门的对比学习模块，增强BEV感知模型中的表示学习，同时与检测任务结合实现联合训练。整体流程：1) 输入多摄像头图像并生成对比样本对；2) 使用EMA更新的图像主干网络提取特征；3) 通过视图变换模块将图像特征转换为BEV表示；4) 实例特征对比模块对同一实例在不同增强视图中的BEV特征进行对比；5) 透视视图对比模块对图像中与标注相关的区域特征进行对比；6) 结合对比学习损失和检测损失进行联合优化训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 首次将对比学习引入BEV检测模型增强表示学习；2) 设计两个专门的对比学习模块；3) 创建统一框架实现对比学习与检测任务的联合优化；4) 证明方法在各种BEV架构上的通用性。相比之前工作，不同之处在于：专注于表示学习而非架构优化；解决传统对比学习方法在BEV中的样本多样性和对比层次问题；不需要额外数据或标注；通过实例级和区域级对比提供更精细的监督信号。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BEVCon通过两个专门的对比学习模块，在不增加额外数据或标注的情况下，显著提升了BEV感知模型的表示学习能力和检测性能，且可广泛应用于各种BEV感知架构。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present BEVCon, a simple yet effective contrastive learning frameworkdesigned to improve Bird's Eye View (BEV) perception in autonomous driving. BEVperception offers a top-down-view representation of the surroundingenvironment, making it crucial for 3D object detection, segmentation, andtrajectory prediction tasks. While prior work has primarily focused onenhancing BEV encoders and task-specific heads, we address the underexploredpotential of representation learning in BEV models. BEVCon introduces twocontrastive learning modules: an instance feature contrast module for refiningBEV features and a perspective view contrast module that enhances the imagebackbone. The dense contrastive learning designed on top of detection lossesleads to improved feature representations across both the BEV encoder and thebackbone. Extensive experiments on the nuScenes dataset demonstrate that BEVConachieves consistent performance gains, achieving up to +2.4% mAP improvementover state-of-the-art baselines. Our results highlight the critical role ofrepresentation learning in BEV perception and offer a complementary avenue toconventional task-specific optimizations.</description>
      <author>example@mail.com (Ziyang Leng, Jiawei Yang, Zhicheng Ren, Bolei Zhou)</author>
      <guid isPermaLink="false">2508.04702v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning Robust Intervention Representations with Delta Embeddings</title>
      <link>http://arxiv.org/abs/2508.04492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;因果表征学习是提高模型泛化能力和鲁棒性的重要方法。本文提出关注干预表征而非仅关注场景变量表征，能有效提高分布外鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;因果表征学习在过去几年引起了显著研究兴趣，因为它可以改善模型的泛化能力和鲁棒性。干预图像对的因果表征具有这样的特性：只有与受干预/动作影响的场景元素相对应的变量才会从开始状态到结束状态发生变化。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探索干预表征在提高模型分布外(OOD)鲁棒性方面的作用，提出一种不需要额外监督的因果表征学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出干预可以用一个对视觉场景不变且在影响的因果变量方面稀疏的'因果Delta嵌入'(Causal Delta Embedding)来表示。基于此，提出一个能够从图像对中学习因果表征的框架，不需要额外的监督。&lt;h4&gt;主要发现&lt;/h4&gt;在Causal Triplet挑战中的实验表明，因果Delta嵌入在OOD设置中非常有效，在合成和真实世界基准中都显著超过了基线性能。&lt;h4&gt;结论&lt;/h4&gt;关注干预表征是提高模型分布外鲁棒性的有效策略，为因果表征学习提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;因果表征学习在过去几年引起了显著的研究兴趣，作为提高模型泛化能力和鲁棒性的一种手段。干预图像对的因果表征具有这样的特性：只有与受干预/动作影响的场景元素相对应的变量才会从开始状态到结束状态发生变化。尽管该领域的大多数工作都集中在识别和表示因果模型下的场景变量，而较少关注干预本身的表征。在这项工作中，我们表明，提高分布外(OOD)鲁棒性的有效策略是关注潜在空间中的干预表征。具体来说，我们提出干预可以用一个对视觉场景不变且在影响的因果变量方面稀疏的'因果Delta嵌入'来表示。利用这一见解，我们提出了一个能够从图像对中学习因果表征的框架，不需要任何额外的监督。在Causal Triplet挑战中的实验表明，因果Delta嵌入在OOD设置中非常有效，在合成和真实世界基准中都显著超过了基线性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal representation learning has attracted significant research interestduring the past few years, as a means for improving model generalization androbustness. Causal representations of interventional image pairs, have theproperty that only variables corresponding to scene elements affected by theintervention / action are changed between the start state and the end state.While most work in this area has focused on identifying and representing thevariables of the scene under a causal model, fewer efforts have focused onrepresentations of the interventions themselves. In this work, we show that aneffective strategy for improving out of distribution (OOD) robustness is tofocus on the representation of interventions in the latent space. Specifically,we propose that an intervention can be represented by a Causal Delta Embeddingthat is invariant to the visual scene and sparse in terms of the causalvariables it affects. Leveraging this insight, we propose a framework that iscapable of learning causal representations from image pairs, without anyadditional supervision. Experiments in the Causal Triplet challenge demonstratethat Causal Delta Embeddings are highly effective in OOD settings,significantly exceeding baseline performance in both synthetic and real-worldbenchmarks.</description>
      <author>example@mail.com (Panagiotis Alimisis, Christos Diou)</author>
      <guid isPermaLink="false">2508.04492v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Graph Representation Learning with Massive Unlabeled Data for Rumor Detection</title>
      <link>http://arxiv.org/abs/2508.04252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用大规模未标记主题数据和通用图自监督学习方法提高谣言检测的泛化能力，解决了现有方法难以获得大规模标记数据集和面对新事件性能下降的问题。&lt;h4&gt;背景&lt;/h4&gt;社交媒体发展导致谣言迅速传播，对社会和经济造成很大危害。基于谣言传播结构学习的方法相比其他方法特别有效。&lt;h4&gt;目的&lt;/h4&gt;解决现有谣言检测方法中难以获得大规模标记数据集导致的泛化能力低和性能下降问题，提高模型在各种主题上的语义学习能力。&lt;h4&gt;方法&lt;/h4&gt;使用三种典型图自监督方法（InfoGraph、JOAO和GraphMAE）在两种训练策略下进行验证；收集覆盖十年多种主题的谣言数据集缓解未标记主题数据与谣言数据之间的时间和主题差异；利用社交媒体平台抓取的大规模未标记主题数据结合声明传播结构。&lt;h4&gt;主要发现&lt;/h4&gt;通用图自监督学习方法优于专门为谣言检测任务设计的方法，在少样本条件下表现良好，展示了借助大量未标记主题数据集的更好泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过大规模未标记主题数据和通用图自监督学习方法可以有效提高谣言检测的泛化能力，特别是在面对新事件时。&lt;h4&gt;翻译&lt;/h4&gt;随着社交媒体的发展，谣言迅速传播，对社会和经济造成极大危害。因此，开发了许多有效的谣言检测方法，其中基于谣言传播结构学习的方法相比其他方法尤为有效。然而，现有方法仍存在许多问题，包括难以获得大规模标记的谣言数据集，这导致泛化能力低，并且在新的热点事件或新兴事件上性能下降，因为谣言具有时效性。为了解决上述问题，本研究利用从微博和Twitter等社交媒体平台抓取的大规模未标记主题数据，结合声明传播结构，提高图表示学习模型在各种主题上的语义学习能力。我们使用三种典型的图自监督方法（InfoGraph、JOAO和GraphMAE）在两种常用的训练策略下，验证通用图半监督方法在谣言检测任务中的性能。此外，为缓解未标记主题数据与谣言数据之间的时间和主题差异，我们还收集了来自微博辟谣平台的覆盖十年（从2022年前十年）多种主题的谣言数据集。实验表明，这些通用图自监督学习方法优于之前专门为谣言检测任务设计的方法，并且在少样本条件下表现良好，展示了借助我们大量未标记主题数据集的更好泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the development of social media, rumors spread quickly, cause great harmto society and economy. Thereby, many effective rumor detection methods havebeen developed, among which the rumor propagation structure learning basedmethods are particularly effective compared to other methods. However, theexisting methods still suffer from many issues including the difficulty toobtain large-scale labeled rumor datasets, which leads to the lowgeneralization ability and the performance degeneration on new events sincerumors are time-critical and usually appear with hot topics or newly emergentevents. In order to solve the above problems, in this study, we usedlarge-scale unlabeled topic datasets crawled from the social media platformWeibo and Twitter with claim propagation structure to improve the semanticlearning ability of a graph reprentation learing model on various topics. Weuse three typical graph self-supervised methods, InfoGraph, JOAO and GraphMAEin two commonly used training strategies, to verify the performance of generalgraph semi-supervised methods in rumor detection tasks. In addition, foralleviating the time and topic difference between unlabeled topic data andrumor data, we also collected a rumor dataset covering a variety of topics overa decade (10-year ago from 2022) from the Weibo rumor-refuting platform. Ourexperiments show that these general graph self-supervised learning methodsoutperform previous methods specifically designed for rumor detection tasks andachieve good performance under few-shot conditions, demonstrating the bettergeneralization ability with the help of our massive unlabeled topic dataset.</description>
      <author>example@mail.com (Chaoqun Cui, Caiyan Jia)</author>
      <guid isPermaLink="false">2508.04252v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Adversarial Fair Multi-View Clustering</title>
      <link>http://arxiv.org/abs/2508.04071v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种对抗性公平多视图聚类（AFMVC）框架，将公平性学习集成到表示学习过程中，通过对抗性训练去除敏感属性信息，确保聚类分配不受敏感属性影响。&lt;h4&gt;背景&lt;/h4&gt;聚类分析是数据挖掘和机器学习中的基本问题，多视图聚类因其能够整合多个视图的互补信息而受到关注。然而，现有方法主要关注聚类性能，而忽略了公平性这一以人为中心应用中的关键问题。&lt;h4&gt;目的&lt;/h4&gt;解决多视图聚类中的公平性问题，提出一种不依赖敏感属性与底层聚类结构对齐假设的方法，实现公平且有效的聚类。&lt;h4&gt;方法&lt;/h4&gt;提出对抗性公平多视图聚类（AFMVC）框架，采用对抗性训练从学习特征中去除敏感属性信息，并通过KL散度将视图特定聚类分配与公平不变共识分布对齐，以保持聚类一致性。&lt;h4&gt;主要发现&lt;/h4&gt;通过KL散度对齐视图特定聚类分配与公平不变共识分布可以在不显著损害公平性的情况下保持聚类一致性，为框架提供了理论保证。&lt;h4&gt;结论&lt;/h4&gt;AFMVC在具有公平性约束的数据集上实现了卓越的公平性和具有竞争力的聚类性能，优于现有的多视图聚类和公平感知聚类方法。&lt;h4&gt;翻译&lt;/h4&gt;聚类分析是数据挖掘和机器学习中的一个基本问题。近年来，多视图聚类由于其能够整合来自多个视图的互补信息而受到越来越多的关注。然而，现有方法主要关注聚类性能，而公平性-以人为中心应用中的关键关注点-在很大程度上被忽视了。尽管最近的研究探索了多视图聚类中的群体公平性，但大多数方法对聚类分配施加显式正则化，依赖于敏感属性和底层聚类结构之间的对齐。然而，这种假设在实践中往往不成立，并且可能降低聚类性能。在本文中，我们提出了一种对抗性公平多视图聚类（AFMVC）框架，将公平性学习集成到表示学习过程中。具体而言，我们的方法采用对抗性训练从根本上从学习到的特征中去除敏感属性信息，确保结果聚类分配不受其影响。此外，我们通过KL散度将视图特定的聚类分配与公平不变的共识分布对齐，理论上证明了这保持了聚类一致性而不会显著损害公平性，从而为我们的框架提供了额外的理论保证。在具有公平性约束的数据集上进行的大量实验表明，与现有的多视图聚类和公平感知聚类方法相比，AFMVC实现了卓越的公平性和具有竞争力的聚类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cluster analysis is a fundamental problem in data mining and machinelearning. In recent years, multi-view clustering has attracted increasingattention due to its ability to integrate complementary information frommultiple views. However, existing methods primarily focus on clusteringperformance, while fairness-a critical concern in human-centeredapplications-has been largely overlooked. Although recent studies have exploredgroup fairness in multi-view clustering, most methods impose explicitregularization on cluster assignments, relying on the alignment betweensensitive attributes and the underlying cluster structure. However, thisassumption often fails in practice and can degrade clustering performance. Inthis paper, we propose an adversarial fair multi-view clustering (AFMVC)framework that integrates fairness learning into the representation learningprocess. Specifically, our method employs adversarial training to fundamentallyremove sensitive attribute information from learned features, ensuring that theresulting cluster assignments are unaffected by it. Furthermore, wetheoretically prove that aligning view-specific clustering assignments with afairness-invariant consensus distribution via KL divergence preservesclustering consistency without significantly compromising fairness, therebyproviding additional theoretical guarantees for our framework. Extensiveexperiments on data sets with fairness constraints demonstrate that AFMVCachieves superior fairness and competitive clustering performance compared toexisting multi-view clustering and fairness-aware clustering methods.</description>
      <author>example@mail.com (Mudi Jiang, Jiahui Zhou, Lianyu Hu, Xinying Liu, Zengyou He, Zhikui Chen)</author>
      <guid isPermaLink="false">2508.04071v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>FeDaL: Federated Dataset Learning for Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2508.04045v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, scaling FL to time series foundation models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为联邦数据集学习(FeDaL)的新方法，用于解决时间序列基础模型(TSFMs)中的数据集异质性问题，通过联邦学习范式学习数据集无关的时间表示，并添加领域偏差消除和全局偏差消除两种机制来提高模型泛化能力。&lt;h4&gt;背景&lt;/h4&gt;数据集异质性引入了显著的领域偏差，从根本上损害了时间序列基础模型(TSFMs)的泛化能力，而这一挑战尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;重新思考使用联邦学习范式发展TSFMs，解决异构时间序列数据的问题，提高模型的跨数据集泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出联邦数据集学习(FeDaL)方法，利用联邦学习的分布式架构将异构时间序列数据集分解为共享的通用知识和保留的个性化知识，并添加领域偏差消除(DBE)和全局偏差消除(GBE)两种互补机制来减轻局部和全局偏差。&lt;h4&gt;主要发现&lt;/h4&gt;在跨越8个任务的8个真实世界数据集上评估了FeDaL的跨数据集泛化能力，包括表示学习和下游时间序列分析，并与54个基线方法进行比较；分析了联邦扩展行为，显示了数据量、客户端数量和加入率在去中心化情况下如何影响模型性能。&lt;h4&gt;结论&lt;/h4&gt;FeDaL方法能有效处理时间序列数据中的异构性问题，通过联邦学习架构和偏差消除机制显著提高了模型泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;数据集异质性引入了显著的领域偏差，从根本上损害了时间序列基础模型(TSFMs)的泛化能力，然而这一挑战仍然探索不足。本文使用联邦学习范式重新思考了TSFMs的发展。我们提出了一种新型的联邦数据集学习(FeDaL)方法，通过学习数据集无关的时间表示来处理异构时间序列。具体而言，联邦学习的分布式架构自然地将异构TS数据集分解为共享的通用知识和保留的个性化知识。此外，基于TSFM架构，FeDaL通过添加两种互补机制明确减轻了局部和全局偏差：领域偏差消除(DBE)和全局偏差消除(GBE)。FeDaL的跨数据集泛化能力已在跨越8个任务的8个真实世界数据集上进行了广泛评估，包括表示学习和下游时间序列分析，并与54个基线方法进行了比较。我们进一步分析了联邦扩展行为，展示了在去中心化情况下，数据量、客户端数量和加入率如何影响模型性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dataset-wise heterogeneity introduces significant domain biases thatfundamentally degrade generalization on Time Series Foundation Models (TSFMs),yet this challenge remains underexplored. This paper rethink the development ofTSFMs using the paradigm of federated learning. We propose a novel FederatedDataset Learning (FeDaL) approach to tackle heterogeneous time series bylearning dataset-agnostic temporal representations. Specifically, thedistributed architecture of federated learning is a nature solution todecompose heterogeneous TS datasets into shared generalized knowledge andpreserved personalized knowledge. Moreover, based on the TSFM architecture,FeDaL explicitly mitigates both local and global biases by adding twocomplementary mechanisms: Domain Bias Elimination (DBE) and Global BiasElimination (GBE). FeDaL`s cross-dataset generalization has been extensivelyevaluated in real-world datasets spanning eight tasks, including bothrepresentation learning and downstream time series analysis, against 54baselines. We further analyze federated scaling behavior, showing how datavolume, client count, and join rate affect model performance underdecentralization.</description>
      <author>example@mail.com (Shengchao Chen, Guodong Long, Jing Jiang)</author>
      <guid isPermaLink="false">2508.04045v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval</title>
      <link>http://arxiv.org/abs/2508.04028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种双提示学习框架DCAR，用于解决图像文本检索任务中区分细粒度属性和相似子类别的挑战，通过联合优化属性和类别特征实现了精确的图像文本匹配。&lt;h4&gt;背景&lt;/h4&gt;提示学习在适应预训练视觉语言模型到各种下游任务(如图像分类)方面表现出色，但在图像文本检索任务中的应用更具挑战性。&lt;h4&gt;目的&lt;/h4&gt;实现精确的图像文本匹配，提高CLIP模型在下游ITR任务上的性能，特别是解决区分细粒度属性和相似子类别的问题。&lt;h4&gt;方法&lt;/h4&gt;提出双提示学习与联合类别-属性重加权(DCAR)框架，从语义和视觉维度动态调整提示向量：(1)在属性层面，基于文本-图像互信息相关性动态更新属性描述权重；(2)在类别层面，引入多角度负样本并采用类别匹配加权来学习子类别区分。&lt;h4&gt;主要发现&lt;/h4&gt;挑战在于下游数据中细粒度属性和相似子类别的区分；通过构建FDRD数据集(包含1,500+细粒度类别和230,000+图像-标题对)验证了方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;DCAR在FDRD数据集上的实验表明，该方法在现有基线上达到了最先进的性能，能有效解决下游ITR任务中的挑战。&lt;h4&gt;翻译&lt;/h4&gt;最近，提示学习在将预训练的视觉语言模型(VLMs)适应到各种下游任务(如图像分类)方面表现出显著成功。然而，将其应用于下游的图像文本检索(ITR)任务更具挑战性。我们发现挑战在于区分下游数据的细粒度属性和相似子类别。为了应对这一挑战，我们提出了双提示学习与联合类别-属性重加权(DCAR)，这是一个新颖的双提示学习框架，用于实现精确的图像文本匹配。该框架从语义和视觉维度动态调整提示向量，以提高CLIP在下游ITR任务上的性能。基于提示范式，DCAR联合优化属性和类别特征以增强细粒度表示学习。具体而言，(1)在属性层面，它基于文本-图像互信息相关性动态更新属性描述的权重；(2)在类别层面，它引入多角度的负样本并采用类别匹配加权来学习子类别区分。为了验证我们的方法，我们构建了细粒度类别描述检索数据集(FDRD)，它作为下游数据领域中ITR的有挑战性的基准。它覆盖了超过1,500个下游细粒度类别和230,000个图像-标题对，并具有详细的属性注释。在FDRD上的大量实验表明，DCAR在现有基线上达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, prompt learning has demonstrated remarkable success in adaptingpre-trained Vision-Language Models (VLMs) to various downstream tasks such asimage classification. However, its application to the downstream Image-TextRetrieval (ITR) task is more challenging. We find that the challenge lies indiscriminating both fine-grained attributes and similar subcategories of thedownstream data. To address this challenge, we propose Dual prompt Learningwith Joint Category-Attribute Reweighting (DCAR), a novel dual-prompt learningframework to achieve precise image-text matching. The framework dynamicallyadjusts prompt vectors from both semantic and visual dimensions to improve theperformance of CLIP on the downstream ITR task. Based on the prompt paradigm,DCAR jointly optimizes attribute and class features to enhance fine-grainedrepresentation learning. Specifically, (1) at the attribute level, itdynamically updates the weights of attribute descriptions based on text-imagemutual information correlation; (2) at the category level, it introducesnegative samples from multiple perspectives with category-matching weighting tolearn subcategory distinctions. To validate our method, we construct theFine-class Described Retrieval Dataset (FDRD), which serves as a challengingbenchmark for ITR in downstream data domains. It covers over 1,500 downstreamfine categories and 230,000 image-caption pairs with detailed attributeannotations. Extensive experiments on FDRD demonstrate that DCAR achievesstate-of-the-art performance over existing baselines.</description>
      <author>example@mail.com (Yifan Wang, Tao Wang, Chenwei Tang, Caiyang Yu, Zhengqing Zang, Mengmi Zhang, Shudong Huang, Jiancheng Lv)</author>
      <guid isPermaLink="false">2508.04028v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification</title>
      <link>http://arxiv.org/abs/2508.03967v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了RAVID，首个利用视觉检索增强生成(RAG)的AI生成图像检测框架。通过动态检索相关图像并融合视觉语言模型，RAVID显著提高了检测的准确性和鲁棒性，在UniversalFakeDetect基准测试上达到93.85%的平均准确率。&lt;h4&gt;背景&lt;/h4&gt;现有RAG方法主要集中在文本领域，对视觉知识探索不足；同时，现有检测方法在泛化能力和鲁棒性方面存在困难，通常依赖低级伪影和模型特定特征，限制了适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效检测AI生成图像的框架，解决现有方法在泛化能力和鲁棒性方面的局限性，探索视觉检索增强生成在图像检测领域的应用。&lt;h4&gt;方法&lt;/h4&gt;RAVID使用微调的CLIP图像编码器(RAVID CLIP)并通过添加类别相关提示改进表示学习；集成视觉语言模型(VLM)来融合检索到的图像与查询；给定查询图像时，RAVID生成嵌入并检索相关图像，将这些图像与查询结合形成增强输入供VLM处理。&lt;h4&gt;主要发现&lt;/h4&gt;在UniversalFakeDetect基准测试上(涵盖19个生成模型)，RAVID达到93.85%的平均准确率；在退化条件下，RAVID的平均准确率为80.27%，优于最先进的模型C2P-CLIP(63.44%)；在高斯模糊和JPEG压缩场景中均有显著改进。&lt;h4&gt;结论&lt;/h4&gt;RAVID成功将视觉检索增强生成技术应用于AI生成图像检测，通过动态检索相关图像并利用视觉语言模型融合，有效解决了现有方法在泛化能力和鲁棒性方面的局限。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们介绍了RAVID，这是第一个利用视觉检索增强生成(RAG)的AI生成图像检测框架。虽然RAG方法在减轻基础模型的事实性错误方面显示出潜力，但它们主要集中在文本领域，导致视觉知识探索不足。与此同时，现有的检测方法在泛化能力和鲁棒性方面存在困难，通常依赖于低级伪影和模型特定特征，限制了它们的适应性。为了解决这个问题，RAVID动态检索相关图像以增强检测。我们的方法使用微调的CLIP图像编码器RAVID CLIP，并通过添加类别相关提示来改进表示学习。我们进一步集成了视觉语言模型(VLM)来融合检索到的图像与查询，丰富输入并提高准确性。给定查询图像，RAVID使用RAVID CLIP生成嵌入，从数据库中检索最相关的图像，并将这些图像与查询图像结合，形成VLM的增强输入。在涵盖19个生成模型的UniversalFakeDetect基准测试上，RAVID以93.85%的平均准确率实现了最先进的性能。在鲁棒性方面，RAVID也优于传统方法，即使在图像退化(如高斯模糊和JPEG压缩)的情况下也能保持高准确率。具体而言，在退化条件下，RAVID的平均准确率为80.27%，而最先进的模型C2P-CLIP为63.44%，这表明在高斯模糊和JPEG压缩场景中都有一致的改进。代码将在接受后公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce RAVID, the first framework for AI-generated imagedetection that leverages visual retrieval-augmented generation (RAG). While RAGmethods have shown promise in mitigating factual inaccuracies in foundationmodels, they have primarily focused on text, leaving visual knowledgeunderexplored. Meanwhile, existing detection methods, which struggle withgeneralization and robustness, often rely on low-level artifacts andmodel-specific features, limiting their adaptability. To address this, RAVIDdynamically retrieves relevant images to enhance detection. Our approachutilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced withcategory-related prompts to improve representation learning. We furtherintegrate a vision-language model (VLM) to fuse retrieved images with thequery, enriching the input and improving accuracy. Given a query image, RAVIDgenerates an embedding using RAVID CLIP, retrieves the most relevant imagesfrom a database, and combines these with the query image to form an enrichedinput for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on theUniversalFakeDetect benchmark, which covers 19 generative models, show thatRAVID achieves state-of-the-art performance with an average accuracy of 93.85%.RAVID also outperforms traditional methods in terms of robustness, maintaininghigh accuracy even under image degradations such as Gaussian blur and JPEGcompression. Specifically, RAVID achieves an average accuracy of 80.27% underdegradation conditions, compared to 63.44% for the state-of-the-art modelC2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEGcompression scenarios. The code will be publicly available upon acceptance.</description>
      <author>example@mail.com (Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdelmalik Taleb-Ahmed, Abdenour Hadid)</author>
      <guid isPermaLink="false">2508.03967v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>CoughViT: A Self-Supervised Vision Transformer for Cough Audio Representation Learning</title>
      <link>http://arxiv.org/abs/2508.03764v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ISWC&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoughViT的新型预训练框架，用于学习通用咳嗽声音表示，以解决呼吸声音诊断中的标签和数据稀缺问题，提高诊断性能。&lt;h4&gt;背景&lt;/h4&gt;医生通常在诊断过程中评估呼吸声音以了解患者气道状况。近年来，基于AI的呼吸声音诊断系统在呼吸疾病检测中显示出成功，代表了早期和可及诊断的重要进展。&lt;h4&gt;目的&lt;/h4&gt;提出CoughViT框架，用于学习通用咳嗽声音表示，以增强有限数据任务中的诊断性能。&lt;h4&gt;方法&lt;/h4&gt;采用掩码数据建模以自监督学习方式训练特征编码器，以解决标签稀缺问题。在三个诊断重要的咳嗽分类任务上评估与其他预训练策略的比较。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，CoughViT的表示在增强下游任务性能方面匹配或超过了当前最先进的监督音频表示。&lt;h4&gt;结论&lt;/h4&gt;CoughViT框架能够有效解决呼吸声音诊断中的数据稀缺问题，提高诊断性能。&lt;h4&gt;翻译&lt;/h4&gt;医生通常在诊断过程中评估呼吸声音，为了解患者气道状况提供见解。近年来，基于AI的呼吸声音诊断系统在呼吸疾病检测中显示出成功。这些系统代表了早期和可及诊断的重要进展，对于及时治疗至关重要。然而，标签和数据稀缺仍然是关键挑战，特别是对于COVID-19以外的疾病，限制了诊断性能和可靠评估。在本文中，我们提出了CoughViT，一种新颖的预训练框架，用于学习通用咳嗽声音表示，以增强有限数据任务中的诊断性能。为了解决标签稀缺问题，我们采用掩码数据建模以自监督学习方式训练特征编码器。我们在三个诊断重要的咳嗽分类任务上评估了我们的方法与其他预训练策略的比较。实验结果表明，我们的表示在增强下游任务性能方面匹配或超过了当前最先进的监督音频表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physicians routinely assess respiratory sounds during the diagnostic process,providing insight into the condition of a patient's airways. In recent years,AI-based diagnostic systems operating on respiratory sounds, have demonstratedsuccess in respiratory disease detection. These systems represent a crucialadvancement in early and accessible diagnosis which is essential for timelytreatment. However, label and data scarcity remain key challenges, especiallyfor conditions beyond COVID-19, limiting diagnostic performance and reliableevaluation. In this paper, we propose CoughViT, a novel pre-training frameworkfor learning general-purpose cough sound representations, to enhance diagnosticperformance in tasks with limited data. To address label scarcity, we employmasked data modelling to train a feature encoder in a self-supervised learningmanner. We evaluate our approach against other pre-training strategies on threediagnostically important cough classification tasks. Experimental results showthat our representations match or exceed current state-of-the-art supervisedaudio representations in enhancing performance on downstream tasks.</description>
      <author>example@mail.com (Justin Luong, Hao Xue, Flora D. Salim)</author>
      <guid isPermaLink="false">2508.03764v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Entity Representation Learning Through Onsite-Offsite Graph for Pinterest Ads</title>
      <link>http://arxiv.org/abs/2508.02609v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络和知识图谱嵌入的广告排名模型改进方法，通过构建大规模异构图和创新的TransRA模型，结合大型ID嵌入表技术和基于注意力机制的KGE微调方法，显著提升了广告系统的点击率和转化率预测性能，并在Pinterest的实际应用中取得了显著效果。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已广泛应用于工业推荐系统，如GraphSage、TwHIM和LiGNN等模型。这些模型基于用户在平台上的活动构建图，以有效学习节点嵌入。然而，除了用户的线上活动外，他们的线下转化数据对广告模型捕捉购物兴趣同样重要。&lt;h4&gt;目的&lt;/h4&gt;为了更好地利用线下转化数据并探索线上与线下活动之间的联系，本研究旨在构建一个结合用户线上广告互动和线下转化活动的大规模异构图，并开发一种有效的方法将图嵌入整合到广告排名模型中。&lt;h4&gt;方法&lt;/h4&gt;研究构建了一个基于用户线上广告互动和线下转化活动的大规模异构图，并提出了TransRA（带锚点的TransR）这一新型知识图谱嵌入模型。为了解决广告排名模型难以直接整合知识图谱嵌入的问题，采用了大型ID嵌入表技术，并在广告排名模型中创新了一种基于注意力机制的KGE微调方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过所提出的方法，研究在点击率(CTR)和转化率(CVR)预测模型中观察到了显著的AUC提升。该框架已在Pinterest的广告参与模型中部署，带来了2.69%的CTR提升和1.34%的CPC降低。&lt;h4&gt;结论&lt;/h4&gt;研究提出的技术可以被其他大规模工业模型采用，有效提升广告系统的性能表现。通过结合线上和线下用户活动数据，以及创新的图嵌入和微调方法，能够显著改善广告排名模型的预测能力。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNN)已广泛应用于工业推荐系统，如GraphSage、TwHIM和LiGNN等模型。在这些工作中，图是基于用户在平台上的活动构建的，并开发了各种图模型来有效学习节点嵌入。除了用户的线上活动外，他们的线下转化数据对广告模型捕捉购物兴趣至关重要。为了更好地利用线下转化数据并探索线上与线下活动之间的联系，我们基于用户的线上广告互动和选择性的线下转化活动构建了一个大规模异构图。此外，我们引入了TransRA（带锚点的TransR），这是一种新型的知识图谱嵌入(KGE)模型，能够更有效地将图嵌入整合到广告排名模型中。然而，我们的广告排名模型最初难以直接整合知识图谱嵌入，并且在离线实验中只观察到适度的提升。为了应对这一挑战，我们采用了大型ID嵌入表技术，并在广告排名模型中创新了一种基于注意力机制的KGE微调方法。结果，我们在点击率(CTR)和转化率(CVR)预测模型中观察到了显著的AUC提升。此外，该框架已在Pinterest的广告参与模型中部署，并带来了2.69%的CTR提升和1.34%的CPC降低。我们相信本文提出的技术可以被其他大规模工业模型所利用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNN) have been extensively applied to industryrecommendation systems, as seen in models like GraphSage\cite{GraphSage},TwHIM\cite{TwHIM}, LiGNN\cite{LiGNN} etc. In these works, graphs wereconstructed based on users' activities on the platforms, and various graphmodels were developed to effectively learn node embeddings. In addition tousers' onsite activities, their offsite conversions are crucial for Ads modelsto capture their shopping interest. To better leverage offsite conversion dataand explore the connection between onsite and offsite activities, weconstructed a large-scale heterogeneous graph based on users' onsite adinteractions and opt-in offsite conversion activities. Furthermore, weintroduced TransRA (TransR\cite{TransR} with Anchors), a novel Knowledge GraphEmbedding (KGE) model, to more efficiently integrate graph embeddings into Adsranking models. However, our Ads ranking models initially struggled to directlyincorporate Knowledge Graph Embeddings (KGE), and only modest gains wereobserved during offline experiments. To address this challenge, we employed theLarge ID Embedding Table technique and innovated an attention based KGEfinetuning approach within the Ads ranking models. As a result, we observed asignificant AUC lift in Click-Through Rate (CTR) and Conversion Rate (CVR)prediction models. Moreover, this framework has been deployed in Pinterest'sAds Engagement Model and contributed to $2.69\%$ CTR lift and $1.34\%$ CPCreduction. We believe the techniques presented in this paper can be leveragedby other large-scale industrial models.</description>
      <author>example@mail.com (Jiayin Jin, Zhimeng Pan, Yang Tang, Jiarui Feng, Kungang Li, Chongyuan Xiang, Jiacheng Li, Runze Su, Siping Ji, Han Sun, Ling Leng, Prathibha Deshikachar)</author>
      <guid isPermaLink="false">2508.02609v2</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Composed Object Retrieval: Object-level Retrieval via Composed Expressions</title>
      <link>http://arxiv.org/abs/2508.04424v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了组合对象检索(COR)这一新任务，超越了传统图像级别检索，实现了对象级别的精确性，并构建了大规模数据集COR127K和统一模型CORE，显著提升了多模态系统中基于用户意图的细粒度视觉内容检索能力。&lt;h4&gt;背景&lt;/h4&gt;基于用户意图检索细粒度视觉内容在多模态系统中仍然是一个挑战。现有的组合图像检索(CIR)方法虽然结合了参考图像与检索文本，但仅限于图像级别的匹配，无法定位特定对象。&lt;h4&gt;目的&lt;/h4&gt;提出组合对象检索(COR)这一全新任务，实现对象级别的精确检索，能够基于组合表达式（结合参考对象和检索文本）检索和分割目标对象，并构建相应的大规模数据集和有效模型。&lt;h4&gt;方法&lt;/h4&gt;构建了COR127K大规模基准数据集，包含408个类别中的127,166个检索三元组；提出了CORE统一端到端模型，集成了参考区域编码、自适应视觉-文本交互和区域级对比学习技术。&lt;h4&gt;主要发现&lt;/h4&gt;CORE模型在基础类别和新颖类别上都显著优于现有模型，为这一具有挑战性的任务建立了简单有效的基线，同时为细粒度多模态检索研究开辟了新方向。&lt;h4&gt;结论&lt;/h4&gt;组合对象检索(COR)任务通过实现对象级别的精确检索，解决了传统方法无法定位特定对象的局限性，为多模态系统中的细粒度视觉内容检索提供了新思路和有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于用户意图检索细粒度视觉内容在多模态系统中仍然是一个挑战。虽然当前组合图像检索(CIR)方法结合了参考图像与检索文本，但它们仅限于图像级别匹配，无法定位特定对象。为此，我们提出了组合对象检索(COR)，这是一个全新的任务，超越了图像级别检索，实现了对象级别的精确性，能够基于组合表达式（结合参考对象和检索文本）检索和分割目标对象。COR在检索灵活性方面提出了重大挑战，要求系统能够识别满足组合表达式的任意对象，同时避免在同一场景中语义相似但不相关的负面对象。我们构建了COR127K，这是第一个大规模COR基准数据集，包含408个类别中的127,166个检索三元组，具有各种语义变换。我们还提出了CORE，一个统一的端到端模型，集成了参考区域编码、自适应视觉-文本交互和区域级对比学习。大量实验表明，CORE在基础类别和新颖类别上都显著优于现有模型，为这一具有挑战性的任务建立了简单有效的基线，同时为细粒度多模态检索研究开辟了新方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieving fine-grained visual content based on user intent remains achallenge in multi-modal systems. Although current Composed Image Retrieval(CIR) methods combine reference images with retrieval texts, they areconstrained to image-level matching and cannot localize specific objects. Tothis end, we propose Composed Object Retrieval (COR), a brand-new task thatgoes beyond image-level retrieval to achieve object-level precision, allowingthe retrieval and segmentation of target objects based on composed expressionscombining reference objects and retrieval texts. COR presents significantchallenges in retrieval flexibility, which requires systems to identifyarbitrary objects satisfying composed expressions while avoiding semanticallysimilar but irrelevant negative objects within the same scene. We constructCOR127K, the first large-scale COR benchmark that contains 127,166 retrievaltriplets with various semantic transformations in 408 categories. We alsopresent CORE, a unified end-to-end model that integrates reference regionencoding, adaptive visual-textual interaction, and region-level contrastivelearning. Extensive experiments demonstrate that CORE significantly outperformsexisting models in both base and novel categories, establishing a simple andeffective baseline for this challenging task while opening new directions forfine-grained multi-modal retrieval research.</description>
      <author>example@mail.com (Tong Wang, Guanyu Yang, Nian Liu, Zongyan Han, Jinxing Zhou, Salman Khan, Fahad Shahbaz Khan)</author>
      <guid isPermaLink="false">2508.04424v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification</title>
      <link>http://arxiv.org/abs/2508.04308v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17th International Conference on Computational Collective  Intelligence 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为WSS-CL的两阶段高效机器遗忘方法用于图像分类，通过权重显著性关注关键模型参数，在logit空间进行高效遗忘，并在特征空间中通过对比学习最大化遗忘与保留数据的距离，实验证明该方法显著提高了遗忘效果且性能损失可忽略。&lt;h4&gt;背景&lt;/h4&gt;机器遗忘（从已训练模型中高效删除特定数据的影响）仍是一个具有挑战性的问题。当前主要关注数据中心或权重策略的机器学习方法在实现精确遗忘、保持稳定性和确保跨领域适用性方面经常遇到挑战。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的两阶段高效机器遗忘方法用于图像分类，利用权重显著性将遗忘过程集中在关键模型参数上，缩小与'精确'遗忘之间的性能差距。&lt;h4&gt;方法&lt;/h4&gt;提出WSS-CL方法，包含两个阶段：1）遗忘阶段：最大化输出logits与聚合伪标签之间的KL散度，在logit空间实现高效遗忘；2）对抗微调阶段：以自监督方式引入对比学习，通过缩放特征表示最大化遗忘和保留数据样本在特征空间中的距离，其中遗忘样本和配对的增强样本作为正对，保留样本作为负对。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估表明，与最先进的方法相比，所提出的方法在遗忘效果上有显著提高，且性能损失可忽略不计，表明该方法在监督和自监督设置中具有可用性。&lt;h4&gt;结论&lt;/h4&gt;WSS-CL方法通过权重显著性关注关键参数，结合两阶段处理策略，显著提高了机器遗忘的效率，同时保持了模型性能，适用于多种学习场景。&lt;h4&gt;翻译&lt;/h4&gt;机器遗忘，即从已训练模型中高效删除特定数据的影响，仍然是一个具有挑战性的问题。当前主要关注数据中心或权重策略的机器学习方法在实现精确遗忘、保持稳定性和确保跨领域适用性方面经常遇到挑战。在这项工作中，我们引入了一种新的用于图像分类的两阶段高效机器遗忘方法，基于权重显著性，利用权重显著性将遗忘过程集中在关键模型参数上。我们的方法被称为权重显著性软引导对比学习用于高效机器遗忘图像分类（WSS-CL），显著缩小了与'精确'遗忘之间的性能差距。首先，遗忘阶段最大化输出logits与聚合伪标签之间的KL散度，在logit空间中实现高效遗忘。接下来，对抗微调阶段以自监督方式引入对比学习。通过使用缩放的特征表示，它最大化了遗忘和保留数据样本在特征空间中的距离，其中遗忘样本和配对的增强样本作为正对，而保留样本在对比损失计算中作为负对。实验评估表明，与最先进的方法相比，我们提出的方法在遗忘效果上有显著提高，且性能损失可忽略不计，表明其在监督和自监督设置中的可用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine unlearning, the efficient deletion of the impact of specific data ina trained model, remains a challenging problem. Current machine unlearningapproaches that focus primarily on data-centric or weight-based strategiesfrequently encounter challenges in achieving precise unlearning, maintainingstability, and ensuring applicability across diverse domains. In this work, weintroduce a new two-phase efficient machine unlearning method for imageclassification, in terms of weight saliency, leveraging weight saliency tofocus the unlearning process on critical model parameters. Our method is calledweight saliency soft-guided contrastive learning for efficient machineunlearning image classification (WSS-CL), which significantly narrows theperformance gap with "exact" unlearning. First, the forgetting stage maximizeskullback-leibler divergence between output logits and aggregated pseudo-labelsfor efficient forgetting in logit space. Next, the adversarial fine-tuningstage introduces contrastive learning in a self-supervised manner. By usingscaled feature representations, it maximizes the distance between the forgottenand retained data samples in the feature space, with the forgotten and thepaired augmented samples acting as positive pairs, while the retained samplesact as negative pairs in the contrastive loss computation. Experimentalevaluations reveal that our proposed method yields much-improved unlearningefficacy with negligible performance loss compared to state-of-the-artapproaches, indicative of its usability in supervised and self-supervisedsettings.</description>
      <author>example@mail.com (Thang Duc Tran, Thai Hoang Le)</author>
      <guid isPermaLink="false">2508.04308v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>SSEmb: A Joint Structural and Semantic Embedding Framework for Mathematical Formula Retrieval</title>
      <link>http://arxiv.org/abs/2508.04162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SSEmb，一种能够捕捉数学公式结构和语义特征的新型嵌入框架，在ARQMath-3公式检索任务中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;公式检索是数学信息检索领域的一个重要话题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕捉数学公式结构和语义特征的新型嵌入框架，以提高公式检索性能。&lt;h4&gt;方法&lt;/h4&gt;1) 结构上：使用图对比学习编码表示为操作符图的公式，并通过替换策略引入图数据增强方法；2) 语义上：使用Sentence-BERT编码公式周围的文本；3) 对每个查询和候选公式，分别计算结构相似性和语义相似性，并通过加权方案进行融合。&lt;h4&gt;主要发现&lt;/h4&gt;在ARQMath-3公式检索任务中，SSEmb在P'@10和nDCG'@10指标上比现有的基于嵌入的方法高出5个百分点以上；SSEmb提高了其他方法所有运行的性能，并与Approach0结合时达到了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;SSEmb是一种有效的数学公式嵌入框架，能够同时捕捉公式结构和语义特征，显著提高了公式检索性能。&lt;h4&gt;翻译&lt;/h4&gt;公式检索是数学信息检索领域的一个重要话题。我们提出了SSEmb，一种能够捕捉数学公式结构和语义特征的新型嵌入框架。结构上，我们使用图对比学习来编码表示为操作符图的公式。为了增强结构多样性同时保持这些公式图的有效性，我们引入了一种通过替换策略实现的新的图数据增强方法。语义上，我们使用Sentence-BERT来编码公式周围的文本。最后，对于每个查询及其候选公式，分别计算结构相似性和语义相似性，然后通过加权方案进行融合。在ARQMath-3公式检索任务中，SSEmb在P'@10和nDCG'@10指标上比现有的基于嵌入的方法高出5个百分点以上。此外，SSEmb提高了其他方法所有运行的性能，并与Approach0结合时达到了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Formula retrieval is an important topic in Mathematical InformationRetrieval. We propose SSEmb, a novel embedding framework capable of capturingboth structural and semantic features of mathematical formulas. Structurally,we employ Graph Contrastive Learning to encode formulas represented as OperatorGraphs. To enhance structural diversity while preserving mathematical validityof these formula graphs, we introduce a novel graph data augmentation approachthrough a substitution strategy. Semantically, we utilize Sentence-BERT toencode the surrounding text of formulas. Finally, for each query and itscandidates, structural and semantic similarities are calculated separately andthen fused through a weighted scheme. In the ARQMath-3 formula retrieval task,SSEmb outperforms existing embedding-based methods by over 5 percentage pointson P'@10 and nDCG'@10. Furthermore, SSEmb enhances the performance of all runsof other methods and achieves state-of-the-art results when combined withApproach0.</description>
      <author>example@mail.com (Ruyin Li, Xiaoyu Chen)</author>
      <guid isPermaLink="false">2508.04162v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Search and Recommendation through Latent Cross Reasoning</title>
      <link>http://arxiv.org/abs/2508.04152v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种潜在的交叉推理框架，通过识别和利用有用的搜索行为信号来改进推荐系统性能。&lt;h4&gt;背景&lt;/h4&gt;搜索和推荐是现代在线平台的基础组成部分，但有效利用搜索行为改进推荐具有挑战性。用户搜索历史常包含嘈杂或不相关信号，现有方法通常联合或单独编码搜索和推荐历史，未明确识别真正有用的搜索行为。&lt;h4&gt;目的&lt;/h4&gt;设计一个框架来识别真正有用的搜索行为信号，利用这些信号改善推荐性能。&lt;h4&gt;方法&lt;/h4&gt;设计了一个潜在的交叉推理框架，首先编码用户的搜索和推荐历史以捕获全局兴趣，然后迭代推理搜索行为提取有益信号。使用对比学习使潜在推理状态与目标项保持一致，并引入强化学习直接优化排序性能。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准测试上，该方法与强基线相比显示出一致的改进，证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;推理在增强感知推荐的搜索中起着重要作用，所提出的框架能有效识别和利用有用的搜索信号。&lt;h4&gt;翻译&lt;/h4&gt;搜索和推荐(S&amp;R)是现代在线平台的基本组成部分，然而有效利用搜索行为来改进推荐仍然是一个具有挑战性的问题。用户搜索历史通常包含嘈杂或不相关的信号，这些信号甚至可能降低推荐性能，而现有方法通常联合或单独编码S&amp;R历史，而没有明确识别哪些搜索行为真正有用。受人类决策过程的启发，人们首先识别推荐意图，然后推理相关证据，我们设计了一个潜在的交叉推理框架，首先编码用户的S&amp;R历史以捕获全局兴趣，然后迭代地推理搜索行为以提取对推荐有益的信号。采用对比学习使潜在推理状态与目标项保持一致，并进一步引入强化学习直接优化排序性能。在公共基准上的大量实验证明了与强基线相比的一致性改进，验证了推理在增强感知推荐的搜索中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Search and recommendation (S&amp;R) are fundamental components of modern onlineplatforms, yet effectively leveraging search behaviors to improverecommendation remains a challenging problem. User search histories oftencontain noisy or irrelevant signals that can even degrade recommendationperformance, while existing approaches typically encode S&amp;R histories eitherjointly or separately without explicitly identifying which search behaviors aretruly useful. Inspired by the human decision-making process, where one firstidentifies recommendation intent and then reasons about relevant evidence, wedesign a latent cross reasoning framework that first encodes user S&amp;R historiesto capture global interests and then iteratively reasons over search behaviorsto extract signals beneficial for recommendation. Contrastive learning isemployed to align latent reasoning states with target items, and reinforcementlearning is further introduced to directly optimize ranking performance.Extensive experiments on public benchmarks demonstrate consistent improvementsover strong baselines, validating the importance of reasoning in enhancingsearch-aware recommendation.</description>
      <author>example@mail.com (Teng Shi, Weicong Qin, Weijie Yu, Xiao Zhang, Ming He, Jianping Fan, Jun Xu)</author>
      <guid isPermaLink="false">2508.04152v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Decoupled Contrastive Learning for Federated Learning</title>
      <link>http://arxiv.org/abs/2508.04005v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DCFL的新框架，解决了联邦学习中对比学习因有限样本违反渐近假设的问题，通过解耦对比损失实现了在数据量有限情况下的有效学习。&lt;h4&gt;背景&lt;/h4&gt;联邦学习是一种分布式机器学习范式，允许多个参与者通过交换模型更新而非原始数据来训练共享模型。然而，由于客户端间的数据异构性，其性能相比集中式方法会降低。&lt;h4&gt;目的&lt;/h4&gt;解决对比学习在联邦学习环境中的适用性问题，特别是在每个客户端只有少量数据的情况下。&lt;h4&gt;方法&lt;/h4&gt;引入联邦学习解耦对比学习(DCFL)框架，将现有对比损失解耦为对齐和均匀性两个目标，实现独立校准吸引力和排斥力，无需依赖渐近假设。&lt;h4&gt;主要发现&lt;/h4&gt;DCFL实现了正样本间更强的对齐和负样本间更好的均匀性；在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准基准测试上，持续优于现有最先进的联邦学习方法。&lt;h4&gt;结论&lt;/h4&gt;DCFL提供了一种适合联邦学习环境的对比学习方法，特别适用于客户端数据量有限的情况。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习是一种分布式机器学习范式，允许多个参与者通过交换模型更新而非原始数据来训练共享模型。然而，由于客户端间的数据异构性，其性能相比集中式方法会降低。虽然对比学习已成为缓解这一问题的有前景方法，但我们的理论分析揭示了一个根本性冲突：其在联邦学习有限样本情况下的渐近假设（无限数量的负样本）被违反。为解决这个问题，我们引入了联邦学习解耦对比学习(DCFL)，一种将现有对比损失解耦为两个目标的新框架。将损失解耦为对齐和均匀性组件，使得能够独立校准吸引力和排斥力，而不依赖于渐近假设。这种策略提供了一种适合联邦学习环境的对比学习方法，特别是在每个客户端只有少量数据的情况下。我们的实验结果表明，与现有对比学习方法相比，DCFL实现了正样本之间更强的对齐和负样本之间更好的均匀性。此外，在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准基准上的实验结果表明，DCFL持续优于最先进的联邦学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated learning is a distributed machine learning paradigm that allowsmultiple participants to train a shared model by exchanging model updatesinstead of their raw data. However, its performance is degraded compared tocentralized approaches due to data heterogeneity across clients. Whilecontrastive learning has emerged as a promising approach to mitigate this, ourtheoretical analysis reveals a fundamental conflict: its asymptotic assumptionsof an infinite number of negative samples are violated in finite-sample regimeof federated learning. To address this issue, we introduce DecoupledContrastive Learning for Federated Learning (DCFL), a novel framework thatdecouples the existing contrastive loss into two objectives. Decoupling theloss into its alignment and uniformity components enables the independentcalibration of the attraction and repulsion forces without relying on theasymptotic assumptions. This strategy provides a contrastive learning methodsuitable for federated learning environments where each client has a smallamount of data. Our experimental results show that DCFL achieves strongeralignment between positive samples and greater uniformity between negativesamples compared to existing contrastive learning methods. Furthermore,experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, andTiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-artfederated learning methods.</description>
      <author>example@mail.com (Hyungbin Kim, Incheol Baek, Yon Dohn Chung)</author>
      <guid isPermaLink="false">2508.04005v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework</title>
      <link>http://arxiv.org/abs/2508.03989v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了PrivCLIP，一种动态、用户可控的少样本隐私保护感知框架，允许用户通过分类活动为敏感、非敏感或中性来指定隐私偏好，使用多模态对比学习对齐IMU传感器数据与自然语言描述，并通过数据转换实现隐私保护。&lt;h4&gt;背景&lt;/h4&gt;在配备IMU传感器的现代设备中，用户可控隐私很重要，因为这些设备持续收集可能暴露敏感行为的时间序列数据。现有隐私保护方法多依赖静态标签或大量私有训练数据，缺乏适应性和用户自主性。&lt;h4&gt;目的&lt;/h4&gt;开发一种动态、用户可控、少样本的隐私保护感知框架，使用户能够灵活指定和修改隐私偏好，同时保持数据效用。&lt;h4&gt;方法&lt;/h4&gt;PrivCLIP采用多模态对比学习将IMU传感器数据与自然语言活动描述对齐在共享嵌入空间，实现敏感活动少样本检测。检测到敏感活动时，使用语言引导的活动清理器和IMU-GPT运动生成模块将数据转换为符合隐私要求的版本。&lt;h4&gt;主要发现&lt;/h4&gt;在多个人体活动识别数据集上评估表明，PrivCLIP在隐私保护和数据效用方面显著优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;PrivCLIP提供了一种有效的隐私保护解决方案，既尊重用户隐私偏好变化，又保持数据实用性，解决了现有方法在适应性和用户自主性方面的局限。&lt;h4&gt;翻译&lt;/h4&gt;用户可控的隐私在现代感知系统中很重要，因为隐私偏好可能因人而异，并可能随时间演变。这在配备惯性测量单元传感器的设备中尤其相关，这些设备持续收集丰富的时间序列数据，可能会无意中暴露敏感的用户行为。虽然先前的工作已经提出了传感器数据的隐私保护方法，但大多数依赖于静态、预定义的隐私标签或需要大量私有训练数据，限制了它们的适应性和用户自主性。在这项工作中，我们介绍了PrivCLIP，一种动态、用户可控、少样本的隐私保护感知框架。PrivCLIP允许用户通过将活动分类为敏感、非敏感或中性来指定和修改其隐私偏好。利用多模态对比学习方法，PrivCLIP将IMU传感器数据与自然语言活动描述在共享嵌入空间中对齐，实现敏感活动的少样本检测。当检测到隐私敏感活动时，系统使用语言引导的活动清理器和运动生成模块将原始数据转换为语义上类似于非敏感活动的符合隐私要求的版本。我们在多个人体活动识别数据集上评估了PrivCLIP，并证明它在隐私保护和数据效用方面都显著优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; User-controllable privacy is important in modern sensing systems, as privacypreferences can vary significantly from person to person and may evolve overtime. This is especially relevant in devices equipped with Inertial MeasurementUnit (IMU) sensors, such as smartphones and wearables, which continuouslycollect rich time-series data that can inadvertently expose sensitive userbehaviors. While prior work has proposed privacy-preserving methods for sensordata, most rely on static, predefined privacy labels or require largequantities of private training data, limiting their adaptability and useragency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,few-shot privacy-preserving sensing framework. PrivCLIP allows users to specifyand modify their privacy preferences by categorizing activities as sensitive(black-listed), non-sensitive (white-listed), or neutral (gray-listed).Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMUsensor data with natural language activity descriptions in a shared embeddingspace, enabling few-shot detection of sensitive activities. When aprivacy-sensitive activity is identified, the system uses a language-guidedactivity sanitizer and a motion generation module (IMU-GPT) to transform theoriginal data into a privacy-compliant version that semantically resembles anon-sensitive activity. We evaluate PrivCLIP on multiple human activityrecognition datasets and demonstrate that it significantly outperforms baselinemethods in terms of both privacy protection and data utility.</description>
      <author>example@mail.com (Ajesh Koyatan Chathoth, Shuhao Yu, Stephen Lee)</author>
      <guid isPermaLink="false">2508.03989v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Marco-Voice Technical Report</title>
      <link>http://arxiv.org/abs/2508.02038v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report. Our code and dataset are publicly available at  https://github.com/AIDC-AI/Marco-Voice and  https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS respectively&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一个名为Marco-Voice的多功能语音合成系统，集成了声音克隆和情感控制功能，通过创新的解耦机制和情感嵌入方法实现了高质量、自然的语音生成。&lt;h4&gt;背景&lt;/h4&gt;在语音合成领域，长期以来存在如何生成高度表达性、可控且自然语音的挑战，同时需要在不同语言和情感背景下保持说话人身份的真实性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够解决语音合成中长期挑战的系统，实现高度表达性、可控性和自然语音生成，同时保持说话人身份的真实性。&lt;h4&gt;方法&lt;/h4&gt;研究引入了有效的说话人-情感解耦机制，采用批内对比学习实现说话人身份和情感风格的独立操作，并开发了旋转情感嵌入集成方法以实现平滑的情感控制。同时构建了CSEMOTIONS数据集，包含6位专业说话人的10小时普通话语音，涵盖7种情感类别。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，Marco-Voice系统在客观和主观指标上都有显著改进，在语音清晰度和情感丰富度方面表现具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;Marco-Voice代表了表达性神经语音合成领域的重大进展，代码和数据集已公开，可供研究人员使用。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文提出了一个多功能语音合成系统，将声音克隆和情感控制语音合成集成在一个统一框架内。这项工作的目标是解决在实现高度表达性、可控性和自然语音生成方面的长期挑战，同时在不同语言和情感背景下真实保留说话人身份。我们的方法引入了有效的说话人-情感解耦机制，采用批内对比学习，能够独立操作说话人身份和情感风格，以及旋转情感嵌入集成方法用于平滑的情感控制。为了支持全面的训练和评估，我们构建了CSEMOTIONS，这是一个高质量的情感语音数据集，包含来自六位专业说话人的10小时普通话语音，涵盖七种情感类别。大量实验表明，我们的系统Marco-Voice在客观和主观指标上都取得了显著改进。进行了全面评估和分析，结果显示Marco-Voice在语音清晰度和情感丰富度方面具有竞争力的性能，代表了表达性神经语音合成领域的重大进展。我们的代码和数据集分别在GitHub和Hugging Face上公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a multifunctional speech synthesis system that integratesvoice cloning and emotion control speech synthesis within a unified framework.The goal of this work is to address longstanding challenges in achieving highlyexpressive, controllable, and natural speech generation that faithfullypreserves speaker identity across diverse linguistic and emotional contexts.Our approach introduces an effective speaker-emotion disentanglement mechanismwith in-batch contrastive learning, enabling independent manipulation ofspeaker identity and eemotional style, as well as rotational emotionalembedding integration method for smooth emotion control. To supportcomprehensive training and evaluation, we construct CSEMOTIONS, a high-qualityemotional speech dataset containing 10 hours of Mandarin speech from sixprofessional speakers across seven emotional categories. Extensive experimentsdemonstrate that our system, Marco-Voice, achieves substantial improvements inboth objective and subjective metrics. Comprehensive evaluations and analysiswere conducted, results show that MarcoVoice delivers competitive performancein terms of speech clarity and emotional richness, representing a substantialadvance in the field of expressive neural speech synthesis. Our code anddataset are publicly available at https://github.com/AIDC-AI/Marco-Voice andhttps://huggingface.co/datasets/AIDC-AI/CSEMOTIONS respectively.</description>
      <author>example@mail.com (Fengping Tian, Chenyang Lyu, Xuanfan Ni, Haoqin Sun, Qingjuan Li, Zhiqiang Qian, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, Kaifu Zhang)</author>
      <guid isPermaLink="false">2508.02038v2</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Context-Adaptive Multi-Prompt Embedding with Large Language Models for Vision-Language Alignment</title>
      <link>http://arxiv.org/abs/2508.02762v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  COLM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Context-Adaptive Multi-Prompt Embedding的新方法，用于增强视觉语言对比学习中的语义表示。通过引入多个结构化提示和自适应令牌，结合预训练大语言模型和特殊的损失函数，该方法在图像-文本和视频-文本检索任务中取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;标准CLIP风格模型依赖于单一的文本嵌入，这在捕捉输入文本的多样语义方面存在局限性，需要一种能够丰富语义表示的新方法。&lt;h4&gt;目的&lt;/h4&gt;增强视觉语言对比学习中的语义表示，使文本表示与视觉特征实现更丰富的语义对齐，提高检索任务的性能。&lt;h4&gt;方法&lt;/h4&gt;引入多个结构化提示，每个提示包含不同的自适应令牌以捕捉文本的不同语义方面；使用预训练大语言模型作为文本编码器，在单个前向传播中联合处理所有提示；将提示嵌入组合成统一文本表示；引入多样性正则化损失和否定感知损失以促进语义多样性和表示质量。&lt;h4&gt;主要发现&lt;/h4&gt;Context-Adaptive Multi-Prompt Embedding方法在图像-文本和视频-文本检索基准测试中取得了一致的性能改进。&lt;h4&gt;结论&lt;/h4&gt;通过多提示结构和特殊的损失函数设计，Context-Adaptive Multi-Prompt Embedding能够有效增强视觉语言对比学习中的语义表示能力，提升检索性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了上下文自适应多提示嵌入，一种新颖的用于增强视觉语言对比学习中语义表示的方法。与依赖单一文本嵌入的标准CLIP风格模型不同，我们的方法引入了多个结构化提示，每个提示包含一个不同的自适应令牌，能够捕捉输入文本的多样语义方面。我们在CLIP框架内使用预训练大语言模型作为文本编码器，在单个前向传播中联合处理所有提示。将得到的提示嵌入组合成统一的文本表示，使文本表示与视觉特征实现更丰富的语义对齐。为进一步促进语义多样性和表示质量，我们引入了多样性正则化损失和否定感知损失，鼓励提示之间的专业化并提高对比判别能力。我们的方法在图像-文本和视频-文本检索基准测试中取得了持续的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose Context-Adaptive Multi-Prompt Embedding, a novel approach toenrich semantic representations in vision-language contrastive learning. Unlikestandard CLIP-style models that rely on a single text embedding, our methodintroduces multiple structured prompts, each containing a distinct adaptivetoken that captures diverse semantic aspects of the input text. We leverage apretrained LLM as the text encoder within the CLIP framework, processing allprompts jointly in a single forward pass. The resulting prompt embeddings arecombined into a unified text representation, enabling semantically richeralignment with visual features. To further promote semantic diversity andrepresentation quality, we incorporate a diversity regularization loss and anegation-aware loss, encouraging specialization across prompts and improvingcontrastive discrimination. Our method achieves consistent improvements on bothimage-text and video-text retrieval benchmarks.</description>
      <author>example@mail.com (Dahun Kim, Anelia Angelova)</author>
      <guid isPermaLink="false">2508.02762v2</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning Pivoting Manipulation with Force and Vision Feedback Using Optimization-based Demonstrations</title>
      <link>http://arxiv.org/abs/2508.01082v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合基于模型和基于学习方法的框架，用于学习闭环枢轴操作。该方法利用接触隐式轨迹优化和演示引导的深度强化学习，实现样本高效学习，并通过特权训练策略实现仿真到现实迁移，使机器人仅依靠本体感受、视觉和力传感就能执行枢轴操作。&lt;h4&gt;背景&lt;/h4&gt;非抓取操作由于物体、环境和机器人之间复杂的接触交互而具有挑战性。基于模型的方法能在接触约束下高效生成轨迹，但对模型不准确敏感且需要特权信息；而基于学习的方法对建模误差更具鲁棒性，但需要大量数据。&lt;h4&gt;目的&lt;/h4&gt;结合基于模型和基于学习的方法的优势，提出一个用于学习闭环枢轴操作的框架，实现样本高效学习，并开发一种使机器人仅依靠本体感受、视觉和力传感就能执行枢轴操作的仿真到现实迁移方法。&lt;h4&gt;方法&lt;/h4&gt;利用计算高效的接触隐式轨迹优化(CITO)设计演示引导的深度强化学习(RL)，实现样本高效学习；提出使用特权训练策略的仿真到现实迁移方法，使机器人能够仅使用本体感受、视觉和力传感执行枢轴操作。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在多个枢轴任务上进行了评估，证明能够成功实现仿真到现实的迁移，机器人无需访问特权信息即可执行枢轴操作。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法成功结合了基于模型和基于学习的方法的优势，解决了非抓取操作的挑战，实现了高效的样本学习和仿真到现实迁移。&lt;h4&gt;翻译&lt;/h4&gt;非抓取操作由于物体、环境和机器人之间复杂的接触交互而具有挑战性。基于模型的方法可以在接触约束下高效生成机器人和物体的复杂轨迹。然而，它们往往对模型不准确敏感，且需要访问特权信息(如物体质量、大小、姿态)，使其不太适合处理新物体。相比之下，基于学习的方法通常对建模误差更具鲁棒性，但需要大量数据。在本文中，我们结合这两种方法，提出了一个用于学习闭环枢轴操作的框架。通过利用计算高效的接触隐式轨迹优化(CITO)，我们设计了演示引导的深度强化学习(RL)，实现了样本高效学习。我们还提出了使用特权训练策略的仿真到现实迁移方法，使机器人能够仅依靠本体感受、视觉和力传感执行枢轴操作，无需访问特权信息。我们的方法在多个枢轴任务上进行了评估，证明能够成功实现仿真到现实迁移。我们方法的概述和硬件实验可在 https://youtu.be/akjGDgfwLbM?si=QVw6ExoPy2VsU2g6 查看。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Non-prehensile manipulation is challenging due to complex contactinteractions between objects, the environment, and robots. Model-basedapproaches can efficiently generate complex trajectories of robots and objectsunder contact constraints. However, they tend to be sensitive to modelinaccuracies and require access to privileged information (e.g., object mass,size, pose), making them less suitable for novel objects. In contrast,learning-based approaches are typically more robust to modeling errors butrequire large amounts of data. In this paper, we bridge these two approaches topropose a framework for learning closed-loop pivoting manipulation. Byleveraging computationally efficient Contact-Implicit Trajectory Optimization(CITO), we design demonstration-guided deep Reinforcement Learning (RL),leading to sample-efficient learning. We also present a sim-to-real transferapproach using a privileged training strategy, enabling the robot to performpivoting manipulation using only proprioception, vision, and force sensingwithout access to privileged information. Our method is evaluated on severalpivoting tasks, demonstrating that it can successfully perform sim-to-realtransfer. The overview of our method and the hardware experiments are shown athttps://youtu.be/akjGDgfwLbM?si=QVw6ExoPy2VsU2g6</description>
      <author>example@mail.com (Yuki Shirai, Kei Ota, Devesh K. Jha, Diego Romeres)</author>
      <guid isPermaLink="false">2508.01082v2</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>TAlignDiff: Automatic Tooth Alignment assisted by Diffusion-based Transformation Learning</title>
      <link>http://arxiv.org/abs/2508.04565v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为TAlignDiff的自动牙齿对齐新方法，基于扩散变换学习，结合点云回归网络和扩散变换矩阵去噪模块，解决了传统方法无法捕捉变换矩阵分布特征的问题。&lt;h4&gt;背景&lt;/h4&gt;正畸治疗依赖于牙齿对齐，这会影响咬合功能、面部美学和患者的生活质量。当前深度学习方法主要通过点对点几何约束预测变换矩阵，但这些矩阵与人类口腔解剖结构相关的分布特征难以被传统方法捕捉。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的自动牙齿对齐方法，能够更好地捕捉变换矩阵的分布特征，提高正畸治疗的效果。&lt;h4&gt;方法&lt;/h4&gt;TAlignDiff方法包含两个主要组件：基于点云的回归网络(PRN)和基于扩散的变换矩阵去噪模块(DTMD)。几何约束损失监督PRN学习点云级别的对齐，DTMD作为辅助模块从临床数据中学习变换矩阵的潜在分布。两者集成在一个统一框架中，实现几何约束和扩散细化之间的双向反馈。&lt;h4&gt;主要发现&lt;/h4&gt;大量的消融和比较实验证明了TAlignDiff方法的有效性和优越性，该方法在正畸治疗中具有应用潜力。&lt;h4&gt;结论&lt;/h4&gt;基于扩散变换学习的TAlignDiff方法能够更好地捕捉变换矩阵的分布特征，为正畸治疗提供更有效的自动牙齿对齐解决方案。&lt;h4&gt;翻译&lt;/h4&gt;正畸治疗依赖于牙齿对齐，这显著影响咬合功能、面部美学和患者的生活质量。当前的深度学习方法主要通过对施加点对点几何约束来预测变换矩阵进行牙齿对齐。然而，这些矩阵可能与人类口腔的解剖结构有关，并具有特定的分布特征，而先前工作中的确定性点对点几何约束无法捕捉这些特征。为此，我们引入了一种名为TAlignDiff的新自动牙齿对齐方法，该方法基于扩散变换学习支持。TAlignDiff包含两个主要组件：一个主要的基于点云的回归网络(PRN)和一个基于扩散的变换矩阵去噪模块(DTMD)。几何约束损失监督PRN学习点云级别的对齐。DTMD作为辅助模块，从临床数据中学习变换矩阵的潜在分布。我们将基于点云的变换回归和基于扩散的变换建模集成到一个统一框架中，允许几何约束和扩散细化之间的双向反馈。大量的消融和比较实验证明了我们方法的有效性和优越性，突显了其在正畸治疗中的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动牙齿排列问题，即如何通过深度学习预测牙齿的变换矩阵实现牙齿自动对齐。这个问题在现实中非常重要，因为牙齿排列是正畸治疗的核心，直接影响咬合功能、面部美观和患者生活质量；传统正畸规划依赖医生经验，耗时且主观；自动排列可提供清晰的治疗结果可视化，帮助医生制定准确计划，提高正畸护理效率和精度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析当前深度学习方法通过点对点几何约束预测变换矩阵的局限性，指出这些方法无法捕捉变换矩阵与口腔解剖结构相关的特定分布特征。作者借鉴了Lei等人提出的TADPM扩散模型框架，但发现其直接从高维几何特征回归变换矩阵导致计算复杂且需要大数据集。因此，作者设计了TAlignDiff，先用点云回归网络预测初始变换矩阵，再用轻量级扩散模型建模变换矩阵的潜在分布，通过比较预测和真实矩阵的噪声估计来优化结果，降低了输入维度并提高对小数据集的适应性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合显式几何约束和隐式分布建模，通过点云回归网络(PRN)和基于扩散的变换矩阵去噪模块(DTMD)双向协同工作。整体流程为：1)输入未对齐的3D牙齿点云；2)PRN使用PointNet编码器提取全局和局部特征，回归预测变换矩阵；3)应用几何约束损失(点重建损失和质心偏移损失)确保几何一致性；4)DTMD通过扩散过程学习变换矩阵分布，对比去噪损失优化预测；5)联合优化所有损失函数，分阶段训练模型；6)输出对齐后的牙齿点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的框架设计，整合几何约束变换回归与扩散辅助分布学习；2)双向反馈机制，通过比较预测和真实矩阵的噪声估计迭代优化；3)轻量级扩散模型，专注于变换矩阵而非原始几何数据。相比之前工作，TAlignDiff不同于TADPM直接从高维特征回归变换矩阵，而是先预测后细化，降低计算复杂度；区别于传统几何约束方法仅关注点云重建，忽略了变换矩阵的分布特征；优于其他深度学习方法仅关注几何特征提取，同时考虑几何一致性和分布建模。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TAlignDiff通过结合几何约束的变换回归与扩散辅助的分布学习，提出了一种更准确、更可靠的自动牙齿排列方法，显著提高了正畸治疗规划的效率和精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Orthodontic treatment hinges on tooth alignment, which significantly affectsocclusal function, facial aesthetics, and patients' quality of life. Currentdeep learning approaches predominantly concentrate on predicting transformationmatrices through imposing point-to-point geometric constraints for toothalignment. Nevertheless, these matrices are likely associated with theanatomical structure of the human oral cavity and possess particulardistribution characteristics that the deterministic point-to-point geometricconstraints in prior work fail to capture. To address this, we introduce a newautomatic tooth alignment method named TAlignDiff, which is supported bydiffusion-based transformation learning. TAlignDiff comprises two maincomponents: a primary point cloud-based regression network (PRN) and adiffusion-based transformation matrix denoising module (DTMD).Geometry-constrained losses supervise PRN learning for point cloud-levelalignment. DTMD, as an auxiliary module, learns the latent distribution oftransformation matrices from clinical data. We integrate point cloud-basedtransformation regression and diffusion-based transformation modeling into aunified framework, allowing bidirectional feedback between geometricconstraints and diffusion refinement. Extensive ablation and comparativeexperiments demonstrate the effectiveness and superiority of our method,highlighting its potential in orthodontic treatment.</description>
      <author>example@mail.com (Yunbi Liu, Enqi Tang, Shiyu Li, Lei Ma, Juncheng Li, Shu Lou, Yongchu Pan, Qingshan Liu)</author>
      <guid isPermaLink="false">2508.04565v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space</title>
      <link>http://arxiv.org/abs/2508.04286v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 15 figures, and will be published in IEEE TVCG&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PKSS-Align的鲁棒点云配准方法，能够在Pre-Kendall形状空间上测量点云间的相似性，无需点到点或点到平面的度量。该方法能够处理相似变换、非均匀密度、随机噪声点和缺陷部分等多种影响因素，通过简单的并行加速实现高效配准，实验证明其性能优于现有最先进方法。&lt;h4&gt;背景&lt;/h4&gt;点云配准是3D视觉和计算机图形学领域的一个经典话题。现有的点云配准方法通常对相似变换（平移、缩放和旋转）、噪声点和不完整的几何结构敏感。特别是，点云的非均匀尺度和缺陷部分增加了配准任务陷入局部最优的概率。&lt;h4&gt;目的&lt;/h4&gt;提出一种鲁棒点云配准方法PKSS-Align，能够处理各种影响因素，包括相似变换、非均匀密度、随机噪声点和缺陷部分。&lt;h4&gt;方法&lt;/h4&gt;在Pre-Kendall形状空间（PKSS）上测量基于形状特征的点云相似性，这是一种基于形状测量的方案，不需要点到点或点到平面的度量。所采用的测量可以被视为流形度量，对欧几里得坐标系中的各种表示具有鲁棒性。得益于这种测量，可以同时为具有上述影响的点云直接生成变换矩阵。该方法不需要数据训练和复杂的特征编码。&lt;h4&gt;主要发现&lt;/h4&gt;通过简单的并行加速，该方法在实际应用中实现了效率和可行性的显著提高。实验证明，该方法优于相关的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;PKSS-Align是一种鲁棒、高效且实用的点云配准方法，能够处理各种复杂情况下的点云配准问题。&lt;h4&gt;翻译&lt;/h4&gt;点云配准是3D视觉和计算机图形学领域的一个经典话题。通常，配准的实现通常对相似变换（平移、缩放和旋转）、噪声点和不完整的几何结构敏感。特别是，点云的非均匀尺度和缺陷部分增加了配准任务陷入局部最优的概率。在本文中，我们提出了一种鲁棒点云配准方法PKSS-Align，可以处理各种影响，包括相似变换、非均匀密度、随机噪声点和缺陷部分。提出的方法在Pre-Kendall形状空间（PKSS）上测量基于形状特征的点云相似性，这是一种基于形状测量的方案，不需要点到点或点到平面的度量。所采用的测量可以被视为流形度量，对欧几里得坐标系中的各种表示具有鲁棒性。得益于这种测量，可以同时为具有上述影响的点云直接生成变换矩阵。提出的方法不需要数据训练和复杂的特征编码。基于简单的并行加速，它可以在实践中实现效率和可行性的显著提高。实验表明，我们的方法优于相关的最先进方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云配准中的鲁棒性问题，包括相似变换（平移、缩放、旋转）、非均匀密度、随机噪声点和缺陷部分的影响。这个问题在现实中非常重要，因为点云配准是3D视觉和计算机图形学的核心课题，广泛应用于同步定位与地图构建(SLAM)、自动驾驶系统、3D重建等领域。在实际扫描过程中，这些因素会导致传统配准方法容易陷入局部最优解，特别是在处理具有非均匀尺度和缺陷部分的点云时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云配准技术的局限性：基于距离的方法（如ICP）对初始姿态敏感；基于几何特征的方法（如NDT）性能受特征质量影响且效率低；基于深度编码的方法（如PointNetLK）受限于训练数据。作者借鉴了之前工作KSS-ICP中的Kendall形状空间理论，但指出了其两个局限性：计算效率低和对不同尺度、缺陷部分的点云配准受限。为解决这些问题，作者设计了PKSS-Align方法，在Pre-Kendall形状空间上实现，包括PKSS映射、形状测量和全局搜索三个核心组件。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在Pre-Kendall形状空间上测量点云间的形状相似性，不依赖点对点或点对平面度量，而是使用流形度量来处理各种变换和噪声影响。整体流程分为三步：首先通过PKSS映射减少平移和尺度影响，包括重采样和异常值剔除；然后使用PKSS形状测量描述点云对齐状态，基于分区结构和代表样本考虑外部轮廓和内部几何特征；最后在SO(3)空间中进行并行加速的全局搜索，找到最优变换矩阵，同时处理不同尺度和缺陷部分。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三：PKSS-based映射减少平移和尺度影响；PKSS-based形状测量提供鲁棒流形度量，克服了Hausdorff度量的局限；全局搜索方案在SO(3)中并行加速，避免局部最优。相比之前工作（尤其是KSS-ICP），PKSS-Align计算效率更高，能同时处理不同尺度、非均匀密度、噪声点和缺陷部分，不需要复杂特征训练，且形状测量更准确鲁棒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PKSS-Align通过在Pre-Kendall形状空间上进行鲁棒形状测量和全局搜索，实现了对受相似变换、非均匀密度、噪声点和缺陷影响的点云的高效精确配准，无需复杂特征训练。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is a classical topic in the field of 3D Vision andComputer Graphics. Generally, the implementation of registration is typicallysensitive to similarity transformations (translation, scaling, and rotation),noisy points, and incomplete geometric structures. Especially, the non-uniformscales and defective parts of point clouds increase probability of struck localoptima in registration task. In this paper, we propose a robust point cloudregistration PKSS-Align that can handle various influences, includingsimilarity transformations, non-uniform densities, random noisy points, anddefective parts. The proposed method measures shape feature-based similaritybetween point clouds on the Pre-Kendall shape space (PKSS),\textcolor{black}{which is a shape measurement-based scheme and doesn't requirepoint-to-point or point-to-plane metric.} The employed measurement can beregarded as the manifold metric that is robust to various representations inthe Euclidean coordinate system. Benefited from the measurement, thetransformation matrix can be directly generated for point clouds with mentionedinfluences at the same time. The proposed method does not require data trainingand complex feature encoding. Based on a simple parallel acceleration, it canachieve significant improvement for efficiency and feasibility in practice.Experiments demonstrate that our method outperforms the relevantstate-of-the-art methods.</description>
      <author>example@mail.com (Chenlei Lv, Hui Huang)</author>
      <guid isPermaLink="false">2508.04286v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2508.04236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PIS3R的图像拼接解决方案，基于深度3D重建概念，能够有效处理具有大视差的图像。该方法通过视觉几何基础Transformer获取相机参数和3D场景重建，然后将点云投影到参考视图实现像素对齐，最后使用点条件图像扩散模块处理伪影，生成高质量的拼接结果。&lt;h4&gt;背景&lt;/h4&gt;图像拼接旨在将两个从不同视点拍摄的图像对齐为一个无缝的、更宽的图像。然而，当3D场景包含深度变化且相机基线较大时，会出现明显的视差现象，即场景元素在不同视图中的相对位置有很大差异。大多数现有的拼接方法难以有效处理具有大视差的图像。&lt;h4&gt;目的&lt;/h4&gt;解决具有大视差的图像拼接问题，提出一种对大视差鲁棒的图像拼接解决方案，保留3D摄影测量背景下所有像素的几何完整性，使其可直接应用于下游3D视觉任务。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为PIS3R的图像拼接解决方案，基于新颖的深度3D重建概念。首先，使用视觉几何基础Transformer处理两个具有大视差的输入图像，获取内参、外参以及密集的3D场景重建。然后，使用恢复的相机参数将重建的密集点云重新投影到指定的参考视图上，实现逐像素对齐并生成初始拼接图像。最后，提出一个点条件图像扩散模块来处理初始拼接中可能出现的孔洞或噪声等伪影，以获得精细化的结果。&lt;h4&gt;主要发现&lt;/h4&gt;与现有方法相比，该解决方案对大视差具有很好的容忍度，能够保留3D摄影测量背景下所有像素的几何完整性，可直接应用于下游3D视觉任务（如SfM）。实验结果表明，该算法对具有大视差的图像提供了准确的拼接结果，并且在定性和定量上都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;PIS3R是一种有效的图像拼接方法，能够处理具有大视差的图像。该方法通过深度3D重建实现了精确的像素对齐，并能够处理拼接过程中的伪影问题。实验验证了该方法的有效性和优越性。&lt;h4&gt;翻译&lt;/h4&gt;图像拼接旨在将两个从不同视点拍摄的图像对齐为一个无缝的、更宽的图像。然而，当3D场景包含深度变化且相机基线显著时，会出现明显的视差现象，即场景元素在不同视图中的相对位置有很大差异。大多数现有的拼接方法难以有效处理具有大视差的图像。为了应对这一挑战，在本文中，我们提出了一种名为PIS3R的图像拼接解决方案，基于新颖的深度3D重建概念，对大视差具有鲁棒性。首先，我们将视觉几何基础Transformer应用于两个具有大视差的输入图像，以获取内参和外参参数，以及密集的3D场景重建。随后，我们使用恢复的相机参数将重建的密集点云重新投影到指定的参考视图上，实现逐像素对齐并生成初始拼接图像。最后，为了进一步处理初始拼接中可能出现的孔洞或噪声等伪影，我们提出一个点条件图像扩散模块以获得精细化的结果。与现有方法相比，我们的解决方案对大视差具有很好的容忍度，并提供的结果完全保留了3D摄影测量背景下所有像素的几何完整性，使其可直接应用于下游3D视觉任务，如SfM。实验结果表明，所提出的算法对具有大视差的图像提供了准确的拼接结果，并且在定性和定量上都优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是图像拼接中处理极大视差（very large parallax）的问题。当图像场景有深度变化且相机基线较大时，传统拼接方法难以有效处理，导致场景元素在不同视图中的相对位置差异很大。这个问题在现实中非常重要，因为图像拼接技术在自动驾驶、医学成像、监控和虚拟现实等多个领域都有广泛应用，而且现有方法大多只关注视觉上的无缝融合，忽略了拼接结果是否保持了底层3D投影几何，这对于支持下游3D视觉任务（如SfM、SLAM）至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有图像拼接方法的局限性来设计新方法。他们发现传统自适应变形方法在极大视差条件下表现不佳，视频拼接方法不适用于静态图像，现有方法忽略了3D几何一致性。作者借鉴了现有的深度3D重建技术，特别是VGGT（Visual Geometry Grounded Transformer）模型，该模型能同时估计相机参数和密集3D场景重建。基于此，他们设计了一个三阶段流程：深度3D重建、点云重投影和基于扩散的拼接优化，从而解决了极大视差图像拼接问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; PIS3R方法的核心思想是基于深度3D重建而非传统特征匹配来实现图像拼接。通过深度3D重建获取场景精确几何表示，然后通过重投影将不同视角图像对齐到同一视图中。整体流程包括三步：1)使用VGGT模型进行深度3D重建，获取点云和相机参数；2)将重建点云投影到参考视图，实现像素级对齐生成初始拼接图像；3)使用点条件图像扩散模块优化结果，处理空洞和噪声等伪影，生成最终精细拼接图像。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出基于深度3D重建的新图像拼接框架；2)明确保留3D几何一致性，支持下游3D视觉任务；3)首次将深度3D重建技术应用于图像拼接；4)开发点条件图像扩散模块优化重投影结果。相比之前工作，PIS3R不依赖传统特征匹配和单应性变换，不使用自适应变形或接缝切割，不仅关注视觉质量，还强调保持3D几何一致性，能有效处理极大视差条件下的图像拼接，而现有方法在此条件下表现不佳。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PIS3R通过基于深度3D重建的创新方法，解决了图像拼接中处理极大视差的挑战，同时保持了3D几何一致性，使拼接结果不仅视觉质量高，还能直接支持下游3D视觉任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image stitching aim to align two images taken from different viewpoints intoone seamless, wider image. However, when the 3D scene contains depth variationsand the camera baseline is significant, noticeable parallax occurs-meaning therelative positions of scene elements differ substantially between views. Mostexisting stitching methods struggle to handle such images with large parallaxeffectively. To address this challenge, in this paper, we propose an imagestitching solution called PIS3R that is robust to very large parallax based onthe novel concept of deep 3D reconstruction. First, we apply visual geometrygrounded transformer to two input images with very large parallax to obtainboth intrinsic and extrinsic parameters, as well as the dense 3D scenereconstruction. Subsequently, we reproject reconstructed dense point cloud ontoa designated reference view using the recovered camera parameters, achievingpixel-wise alignment and generating an initial stitched image. Finally, tofurther address potential artifacts such as holes or noise in the initialstitching, we propose a point-conditioned image diffusion module to obtain therefined result.Compared with existing methods, our solution is very largeparallax tolerant and also provides results that fully preserve the geometricintegrity of all pixels in the 3D photogrammetric context, enabling directapplicability to downstream 3D vision tasks such as SfM. Experimental resultsdemonstrate that the proposed algorithm provides accurate stitching results forimages with very large parallax, and outperforms the existing methodsqualitatively and quantitatively.</description>
      <author>example@mail.com (Muhua Zhu, Xinhao Jin, Chengbo Wang, Yongcong Zhang, Yifei Xue, Tie Ji, Yizhen Lao)</author>
      <guid isPermaLink="false">2508.04236v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation</title>
      <link>http://arxiv.org/abs/2508.04033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE/RSJ International Conference on Intelligent Robots  and Systems (IROS), 2025. 8 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合单目摄像头图像和2D雷达点云数据的NLoS行人定位框架，用于解决城市环境中路边停车造成的盲点问题，提高对突然出现行人的检测能力。&lt;h4&gt;背景&lt;/h4&gt;城市环境中路边停车造成的NLoS盲点对道路安全构成重大挑战；现有方法主要依赖预定义空间信息或简单假设，限制了泛化能力；停放的车辆是动态的，卫星地图等预定义信息无法准确反映实时路况。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖预定义空间信息的NLoS行人定位框架，提高在动态城市环境中对突然出现行人的检测能力。&lt;h4&gt;方法&lt;/h4&gt;通过图像分割检测停放车辆，估计深度推断空间特征，并利用2D雷达点云数据精炼信息实现精确空间推理。&lt;h4&gt;主要发现&lt;/h4&gt;在真实城市道路环境中的实验评估表明，该方法增强了早期行人检测，有助于提高道路安全性。&lt;h4&gt;结论&lt;/h4&gt;结合视觉和雷达数据的方法能够更准确地处理由路边停车造成的NLoS盲点问题，特别是在行人突然出现的场景中。&lt;h4&gt;翻译&lt;/h4&gt;由于路边停车导致的城市环境中存在非视距盲点，特别是行人突然出现的情况，对道路安全构成了重大挑战。毫米波技术利用衍射和反射来观察NLoS区域，最近的研究已证明其检测被遮挡物体的潜力。然而，现有方法主要依赖预定义的空间信息或假设简单的墙壁反射，从而限制了它们的泛化能力和实际适用性。当行人在停放的车辆之间突然出现时，这些停放的车辆充当临时空间障碍物，形成特殊挑战。此外，由于停放的车辆是动态的，可能会随时间重新定位，从卫星地图或其他预定义来源获得的空间信息可能无法准确反映实时道路状况，导致传感器解释错误。为解决这一限制，我们提出了一种结合单目摄像头图像和2D雷达点云数据的NLoS行人定位框架。所提出的方法首先通过图像分割检测停放的车辆，估计深度以推断近似空间特征，然后使用2D雷达点云数据进一步精炼信息，以实现精确的空间推理。在真实城市道路环境中进行的实验评估表明，所提出的方法增强了早期行人检测，有助于提高道路安全。补充材料可在https://hiyeun.github.io/NLoS/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决城市环境中路边停放车辆造成的非视线(NLoS)盲区问题，特别是行人突然从这些停放的车辆之间出现时的检测困难。这个问题在现实中非常重要，因为这些盲区可能导致驾驶员无法及时发现行人，尤其是儿童突然冲出马路的情况，反应时间不足可能导致严重事故。现有方法主要依赖预定义的空间信息或假设简单的墙壁反射，限制了它们在真实世界场景中的适用性，且停放车辆是动态的，预定义的空间信息可能无法准确反映实时路况。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的局限性，包括依赖预定义空间信息和假设简单反射表面。然后寻找互补传感器：毫米波雷达可以观察NLoS区域但数据稀疏，相机图像包含丰富环境特征但距离估计受光照影响。作者设计融合方法，利用单目相机进行车辆分割和深度估计，再使用雷达点云精炼位置信息。该方法借鉴了现有工作，如使用Depth Anything V2进行深度估计，YOLOv8进行图像分割，DBSCAN进行点云聚类，以及射线追踪技术分析反射路径。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是融合相机图像和2D雷达点云数据来推断空间信息，并利用这些信息分析雷达反射路径，实现NLoS行人的早期检测。整体流程分为四步：1)车辆推断：从相机图像检测车辆并估计深度，再用雷达数据精炼位置；2)空间信息推断：结合两种数据推断车辆空间状态，使用帧平均稳定信息；3)目标定位：通过射线追踪估计反射路径定位NLoS目标；4)最终位置确定：应用DBSCAN去除噪声，过滤幽灵反射，计算聚类质心作为预测位置。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新的基于图像的雷达点云解释管道，用于定位停车辆间突然出现的NLoS行人；2)使用从距离测量不精确的图像中提取深度信息来解释稀疏雷达点云的空间推断方法；3)在真实世界户外道路环境中进行实际验证。相比之前工作，本文不依赖预定义空间信息而是实时推断，处理更复杂的场景(多车辆之间的区域)，结合相机和雷达的多模态优势，并在实际道路环境中而非受控环境中验证。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种融合单目相机和2D雷达点云数据的新方法，能够在真实道路环境中实时推断停车辆之间的空间信息，并通过分析雷达反射路径实现非视线行人的早期检测，从而提高道路安全性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The presence of Non-Line-of-Sight (NLoS) blind spots resulting from roadsideparking in urban environments poses a significant challenge to road safety,particularly due to the sudden emergence of pedestrians. mmWave technologyleverages diffraction and reflection to observe NLoS regions, and recentstudies have demonstrated its potential for detecting obscured objects.However, existing approaches predominantly rely on predefined spatialinformation or assume simple wall reflections, thereby limiting theirgeneralizability and practical applicability. A particular challenge arises inscenarios where pedestrians suddenly appear from between parked vehicles, asthese parked vehicles act as temporary spatial obstructions. Furthermore, sinceparked vehicles are dynamic and may relocate over time, spatial informationobtained from satellite maps or other predefined sources may not accuratelyreflect real-time road conditions, leading to erroneous sensor interpretations.To address this limitation, we propose an NLoS pedestrian localizationframework that integrates monocular camera image with 2D radar point cloud(PCD) data. The proposed method initially detects parked vehicles through imagesegmentation, estimates depth to infer approximate spatial characteristics, andsubsequently refines this information using 2D radar PCD to achieve precisespatial inference. Experimental evaluations conducted in real-world urban roadenvironments demonstrate that the proposed approach enhances early pedestriandetection and contributes to improved road safety. Supplementary materials areavailable at https://hiyeun.github.io/NLoS/.</description>
      <author>example@mail.com (Hee-Yeun Kim, Byeonggyu Park, Byonghyok Choi, Hansang Cho, Byungkwan Kim, Soomok Lee, Mingu Jeon, Seung-Woo Seo, Seong-Woo Kim)</author>
      <guid isPermaLink="false">2508.04033v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Point-Based Shape Representation Generation with a Correspondence-Preserving Diffusion Model</title>
      <link>http://arxiv.org/abs/2508.03925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新型的扩散模型，用于生成具有点对应关系的基于点的形状表示，特别是在医学影像中海马体形状的生成方面取得了显著成果。&lt;h4&gt;背景&lt;/h4&gt;传统统计形状模型广泛考虑了点对应关系，但当前深度学习方法忽略了这一点，专注于无序点云。现有深度生成模型无法生成具有点对应关系的形状。&lt;h4&gt;目的&lt;/h4&gt;制定一种能够生成真实基于点形状表示的扩散模型，同时保留训练数据中存在的点对应关系。&lt;h4&gt;方法&lt;/h4&gt;使用基于点的形状表示数据，这些数据具有从开放获取影像研究系列3(OASIS-3)导出的对应关系，开发了一种对应关系保留的扩散模型。&lt;h4&gt;主要发现&lt;/h4&gt;该模型能够生成高度真实的基于点的海马体形状表示，优于现有方法。此外，该模型可用于条件生成健康和阿尔茨海默病受试者的形状，以及通过反事实生成预测疾病进展的形态变化。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的对应关系保留扩散模型在医学形状生成领域具有应用潜力，特别是在神经退行性疾病研究中。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种扩散模型，旨在生成具有对应关系的基于点的形状表示。传统统计形状模型已经广泛考虑了点对应关系，但当前的深度学习方法没有考虑这一点，而是专注于无序点云。当前用于点云的深度生成模型并不能解决生成具有点对应关系的形状的问题。这项工作旨在制定一个扩散模型，能够生成真实的基于点的形状表示，并保留训练数据中存在的点对应关系。使用从开放获取影像研究系列3(OASIS-3)导出的具有对应关系的形状表示数据，我们证明我们的对应关系保留模型相比现有方法能够生成高度真实的基于点的海马体形状表示。我们进一步展示了我们的生成模型在下游任务中的应用，如健康和AD受试者的条件生成，以及通过反事实生成预测疾病进展的形态变化。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何生成具有点对应关系的基于点的形状表示问题。在生物医学研究中，特别是神经科学领域，解剖形状分析对于理解发育、衰老和疾病进展非常重要。例如，阿尔茨海默病会导致海马体萎缩，而基于点的形状表示能精确量化健康人与患者间的局部形态变化，这对疾病诊断和研究至关重要。点对应关系能确保不同形状间的相同解剖位置可比，这是当前深度生成方法无法做到的。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从传统统计形状模型获得灵感，这些模型已考虑点对应关系，但被当前深度学习方法忽略。他们借鉴了PointNet架构，使用共享线性权重而非卷积层，但保留了点的顺序以维持对应关系。还参考了transformer中的位置嵌入概念，设计了对应关系嵌入来编码点的解剖位置信息。此外，他们采用了扩散模型这一强大的生成框架，并引入掩码自注意力机制来建模点间的空间关系，综合这些创新设计出了新方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计一个能保持点对应关系的扩散模型，生成具有解剖意义的形状表示。整体流程分两部分：前向过程逐渐向原始点云添加噪声，使其逐渐变模糊；生成过程则反向操作，学习从噪声中恢复出有意义的形状。具体实现上，使用类似U-Net的架构，通过共享线性权重的RFT块处理点数据，在中间层注入对应关系嵌入确保点的解剖位置一致，并在瓶颈处用掩码自注意力建模点间空间关系。训练时，网络学习预测添加的噪声，通过最小化预测噪声与实际噪声的差异来优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将点对应关系引入深度生成模型，专门针对生物医学形状设计；2)提出对应关系嵌入机制，确保生成形状保持解剖对应关系；3)设计特殊网络架构，使用共享线性权重和掩码自注意力；4)在生物医学形态分析中成功应用扩散模型。相比之前工作，不同在于：传统点云生成方法(Luo等人,2021; Zeng等人,2022)将点视为独立采样，忽略了对应关系；PCA方法虽能生成形状但缺乏多样性和细节；本文是首个专门设计用于保持点对应关系的生成模型，对生物医学形态分析尤为重要。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的对应关系保留扩散模型，首次实现了深度学习生成具有解剖对应关系的基于点的形状表示，为生物医学形态分析提供了强大的新工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a diffusion model designed to generate point-based shaperepresentations with correspondences. Traditional statistical shape models haveconsidered point correspondences extensively, but current deep learning methodsdo not take them into account, focusing on unordered point clouds instead.Current deep generative models for point clouds do not address generatingshapes with point correspondences between generated shapes. This work aims toformulate a diffusion model that is capable of generating realistic point-basedshape representations, which preserve point correspondences that are present inthe training data. Using shape representation data with correspondences derivedfrom Open Access Series of Imaging Studies 3 (OASIS-3), we demonstrate that ourcorrespondence-preserving model effectively generates point-based hippocampalshape representations that are highly realistic compared to existing methods.We further demonstrate the applications of our generative model by downstreamtasks, such as conditional generation of healthy and AD subjects and predictingmorphological changes of disease progression by counterfactual generation.</description>
      <author>example@mail.com (Shen Zhu, Yinzhu Jin, Ifrah Zawar, P. Thomas Fletcher)</author>
      <guid isPermaLink="false">2508.03925v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT Reconstruction</title>
      <link>http://arxiv.org/abs/2508.02408v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GR-Gaussian的基于图的3D高斯飞溅框架，用于解决CT重建中的针状伪影问题，提高稀疏视图条件下的重建准确性。&lt;h4&gt;背景&lt;/h4&gt;3D高斯飞溅(3DGS)已成为CT重建的一种有前景的方法，但现有方法依赖于视图内点的平均梯度幅度，这通常在稀疏视图条件下导致严重的针状伪影。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在稀疏视图条件下产生针状伪影的问题，提高稀疏视图条件下的重建准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了GR-Gaussian框架，包含两个关键创新：1)去噪点云初始化策略，减少初始化误差并加速收敛；2)像素图感知梯度策略，使用基于图的密度差异改进梯度计算，提高分割准确性和密度表示。&lt;h4&gt;主要发现&lt;/h4&gt;在X-3D和真实数据集上的实验验证了GR-Gaussian的有效性，实现了PSNR提升0.67 dB和0.92 dB，SSIM增益为0.011和0.021。&lt;h4&gt;结论&lt;/h4&gt;GR-Gaussian在具有挑战性的稀疏视图条件下适用于准确的CT重建。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯飞溅(3DGS)已成为CT重建的一种有前景的方法。然而，现有方法依赖于视图内点的平均梯度幅度，这通常在稀疏视图条件下导致严重的针状伪影。为应对这一挑战，我们提出了GR-Gaussian，一种基于图的3D高斯飞溅框架，可抑制针状伪影并提高稀疏视图条件下的重建准确性。我们的框架引入了两个关键创新：1)一种去噪点云初始化策略，可减少初始化误差并加速收敛；2)一种像素图感知梯度策略，使用基于图的密度差异改进梯度计算，提高分割准确性和密度表示。在X-3D和真实数据集上的实验验证了GR-Gaussian的有效性，实现了PSNR提升0.67 dB和0.92 dB，SSIM增益为0.011和0.021。这些结果突显了GR-Gaussian在具有挑战性的稀疏视图条件下进行准确CT重建的适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决稀疏视角CT重建中的针状伪影问题。这个问题在现实中非常重要，因为在医学成像和工业检测等领域，减少X射线曝光剂量对患者或检测对象至关重要，这自然会导致采集的投影数据稀疏。稀疏视角条件下的CT重建是一个高度不适定的问题，传统方法难以获得高质量重建结果，而现有方法要么需要大量标注数据，要么计算成本极高，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3D高斯飞溅方法在稀疏视角CT重建中产生针状伪影的原因，发现这些伪影源于保留了具有小梯度的大高斯核，且缺乏对点间关系的考虑。基于这一分析，作者引入图结构来建模点之间的关系，设计了两个关键创新：去噪点云初始化策略和像素图感知梯度策略。该方法借鉴了3D高斯飞溅技术、基于NeRF的方法、传统CT重建方法以及图神经网络的思想，特别是利用KNN方法构建图结构，但针对CT重建的特殊需求进行了创新性改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入图结构来建模点之间的关系，解决稀疏视角CT重建中的针状伪影问题。整体实现流程包括：1) 使用增强的FDK方法生成初始体积；2) 应用去噪点云初始化策略对点云进行高斯滤波，减少噪声和伪影；3) 使用KNN算法构建图结构，顶点表示高斯核位置，边连接邻近高斯核；4) 训练优化阶段包括投影光栅化计算光度损失、体素化进行3D正则化、应用像素图感知梯度策略增强梯度计算；5) 根据增强后的梯度决定是否分裂或克隆高斯核；6) 使用多种损失函数进行优化；7) 通过体素化生成最终的密度体积。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 基于图的辐射高斯表示，考虑点与点之间的空间关系；2) 去噪点云初始化策略，使用高斯滤波减少初始化误差；3) 像素图感知梯度策略，利用密度差异增强梯度计算；4) 图拉普拉斯正则化，鼓励相邻高斯核之间的密度平滑。相比之前的工作，不同之处在于：与传统3D高斯飞溅方法相比，引入了图结构和密度差异来细化梯度计算；与基于NeRF的方法相比，显著提高了渲染效率；与传统CT重建方法相比，在稀疏视角条件下能产生更高质量的重建结果且计算效率更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GR-Gaussian通过引入图结构建模点间关系，结合去噪初始化和像素图感知梯度策略，有效解决了稀疏视角CT重建中的针状伪影问题，显著提高了重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has emerged as a promising approach for CTreconstruction. However, existing methods rely on the average gradientmagnitude of points within the view, often leading to severe needle-likeartifacts under sparse-view conditions. To address this challenge, we proposeGR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppressesneedle-like artifacts and improves reconstruction accuracy under sparse-viewconditions. Our framework introduces two key innovations: (1) a Denoised PointCloud Initialization Strategy that reduces initialization errors andaccelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy thatrefines gradient computation using graph-based density differences, improvingsplitting accuracy and density representation. Experiments on X-3D andreal-world datasets validate the effectiveness of GR-Gaussian, achieving PSNRimprovements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. Theseresults highlight the applicability of GR-Gaussian for accurate CTreconstruction under challenging sparse-view conditions.</description>
      <author>example@mail.com (Yikuang Yuluo, Yue Ma, Kuan Shen, Tongtong Jin, Wang Liao, Yangpu Ma, Fuquan Wang)</author>
      <guid isPermaLink="false">2508.02408v2</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>GraphProp: Training the Graph Foundation Models using Graph Properties</title>
      <link>http://arxiv.org/abs/2508.04594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了GraphProp，一种强调结构泛化的图基础模型训练方法，通过预测图不变量和利用结构GFM的表示作为位置编码来提高图模型在跨域任务中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;图基础模型需要在图分类等图级任务中具有强大的泛化能力，有效的GFM训练需要捕获跨不同域的一致信息。&lt;h4&gt;目的&lt;/h4&gt;解决传统GFMs缺乏跨域结构泛化能力的问题，提高图模型在跨域任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;GraphProp包含两个训练阶段：首先通过预测图不变量训练结构GFM，捕获抽象结构信息；然后使用结构GFM的表示作为位置编码训练全面GFM，利用特定域的节点属性和图标签改善跨域节点特征泛化。&lt;h4&gt;主要发现&lt;/h4&gt;图结构比节点特征和图标签提供更多跨域一致的信息；传统GFMs主要关注节点特征转移而缺乏结构泛化能力；GraphProp在监督学习和少样本学习中表现优异，尤其在处理无节点属性的图时。&lt;h4&gt;结论&lt;/h4&gt;GraphProp通过强调结构泛化，显著提高了图基础模型在跨域任务中的泛化能力，特别是在处理没有节点属性的图时。&lt;h4&gt;翻译&lt;/h4&gt;这项工作专注于训练图基础模型，使其在图分类等图级任务中具有强大的泛化能力。有效的GFM训练需要捕获跨不同域的一致信息。我们发现与节点特征和图标签相比，图结构提供了更多跨域一致的信息。然而，传统的GFMs主要关注将不同域的节点特征转移到统一的表示空间，但往往缺乏跨域的结构泛化能力。为此，我们引入了GraphProp，它强调结构泛化。GraphProp的训练过程包含两个主要阶段。首先，我们通过预测图不变量来训练结构GFM。由于图不变量仅依赖于抽象结构，不依赖于特定的标记或绘制，这种结构GFM能够捕获抽象结构信息，并提供可在不同域之间比较的判别性图表示。在第二阶段，我们使用结构GFM提供的表示作为位置编码来训练全面的GFM。这一阶段利用特定域的节点属性和图标签来进一步改善跨域节点特征泛化。我们的实验证明，GraphProp在监督学习和少样本学习中显著优于竞争对手，特别是在处理没有节点属性的图时。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work focuses on training graph foundation models (GFMs) that have stronggeneralization ability in graph-level tasks such as graph classification.Effective GFM training requires capturing information consistent acrossdifferent domains. We discover that graph structures provide more consistentcross-domain information compared to node features and graph labels. However,traditional GFMs primarily focus on transferring node features from variousdomains into a unified representation space but often lack structuralcross-domain generalization. To address this, we introduce GraphProp, whichemphasizes structural generalization. The training process of GraphPropconsists of two main phases. First, we train a structural GFM by predictinggraph invariants. Since graph invariants are properties of graphs that dependonly on the abstract structure, not on particular labellings or drawings of thegraph, this structural GFM has a strong ability to capture the abstractstructural information and provide discriminative graph representationscomparable across diverse domains. In the second phase, we use therepresentations given by the structural GFM as positional encodings to train acomprehensive GFM. This phase utilizes domain-specific node attributes andgraph labels to further improve cross-domain node feature generalization. Ourexperiments demonstrate that GraphProp significantly outperforms thecompetitors in supervised learning and few-shot learning, especially inhandling graphs without node attributes.</description>
      <author>example@mail.com (Ziheng Sun, Qi Feng, Lehao Lin, Chris Ding, Jicong Fan)</author>
      <guid isPermaLink="false">2508.04594v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use</title>
      <link>http://arxiv.org/abs/2508.04482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACL 2025 (Oral)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文对操作系统代理（OS Agents）进行了全面综述，OS Agents是基于多模态大语言模型的AI助手，能在操作系统环境中通过图形用户界面等接口自动化完成任务。论文介绍了其基础组成部分、构建方法、评估标准，并讨论了当前挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;创造像钢铁侠中的J.A.R.V.I.S一样全能的AI助手一直是人们的梦想。随着多模态大语言模型的发展，基于(M)LLMs的OS Agents通过在操作系统环境中操作计算设备来自动化任务，已取得显著进展。&lt;h4&gt;目的&lt;/h4&gt;整合OS Agent领域的研究现状，为学术研究和工业发展提供指导，通过系统性地介绍基础知识、构建方法、评估标准和未来方向，帮助研究人员和开发者更好地理解和应用这一技术。&lt;h4&gt;方法&lt;/h4&gt;采用系统综述方法，首先阐明OS Agent的基础知识，包括环境、观察空间和行动空间等关键组成部分，以及理解、规划和基础能力等核心功能；然后探讨构建OS Agent的方法论，专注于领域特定的基础模型和代理框架；最后评估OS Agent在各种任务中的评估协议和基准。&lt;h4&gt;主要发现&lt;/h4&gt;OS Agent领域已取得显著进展，但仍面临安全和隐私、个性化和自我进化等挑战。开源GitHub资源为这一领域的进一步创新提供了支持。&lt;h4&gt;结论&lt;/h4&gt;OS Agent代表了AI助手发展的重要方向，将多模态大语言模型与操作系统环境相结合，能实现更复杂的任务自动化。未来研究应关注安全性、隐私保护、个性化和自我进化等方面。&lt;h4&gt;翻译&lt;/h4&gt;创造像钢铁侠中的J.A.R.V.I.S一样全能的人工智能助手的梦想一直吸引着人们的想象力。随着多模态大语言模型的发展，这一梦想正逐渐成为现实，因为基于(M)LLMs的代理使用计算设备在操作系统提供的环境和界面中操作来自动化任务，已经取得了显著进展。本文对这些先进的代理进行了全面综述，并将其命名为操作系统代理（OS Agents）。我们首先阐明OS Agent的基础知识，探讨其关键组成部分，包括环境、观察空间和行动空间，并概述了理解、规划和基础等基本能力。然后我们研究了构建OS Agent的方法论，专注于领域特定的基础模型和代理框架。对评估协议和基准的详细回顾强调了OS Agent如何在各种任务中被评估。最后，我们讨论了当前挑战并确定了未来研究的有前途方向，包括安全和隐私、个性化和自我进化。本综述旨在整合OS Agent领域的研究现状，为学术研究和工业发展提供见解。维护了一个开源GitHub仓库作为动态资源，以促进该领域的进一步创新。我们提供了论文的9页版本，已被ACL 2025接受，为该领域提供了简洁的概述。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The dream to create AI assistants as capable and versatile as the fictionalJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolutionof (multi-modal) large language models ((M)LLMs), this dream is closer toreality, as (M)LLM-based Agents using computing devices (e.g., computers andmobile phones) by operating within the environments and interfaces (e.g.,Graphical User Interface (GUI)) provided by operating systems (OS) to automatetasks have significantly advanced. This paper presents a comprehensive surveyof these advanced agents, designated as OS Agents. We begin by elucidating thefundamentals of OS Agents, exploring their key components including theenvironment, observation space, and action space, and outlining essentialcapabilities such as understanding, planning, and grounding. We then examinemethodologies for constructing OS Agents, focusing on domain-specificfoundation models and agent frameworks. A detailed review of evaluationprotocols and benchmarks highlights how OS Agents are assessed across diversetasks. Finally, we discuss current challenges and identify promising directionsfor future research, including safety and privacy, personalization andself-evolution. This survey aims to consolidate the state of OS Agentsresearch, providing insights to guide both academic inquiry and industrialdevelopment. An open-source GitHub repository is maintained as a dynamicresource to foster further innovation in this field. We present a 9-pageversion of our work, accepted by ACL 2025, to provide a concise overview to thedomain.</description>
      <author>example@mail.com (Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, Yuhuai Li, Shengze Xu, Shenzhi Wang, Xinchen Xu, Shuofei Qiao, Zhaokai Wang, Kun Kuang, Tieyong Zeng, Liang Wang, Jiwei Li, Yuchen Eleanor Jiang, Wangchunshu Zhou, Guoyin Wang, Keting Yin, Zhou Zhao, Hongxia Yang, Fan Wu, Shengyu Zhang, Fei Wu)</author>
      <guid isPermaLink="false">2508.04482v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions</title>
      <link>http://arxiv.org/abs/2508.04470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了名为FedHiP的异构不变性个性化联邦学习方案，通过分析性解决方案避免基于梯度的更新，解决了联邦学习中数据异构性问题，实现了理想的异构不变性特性。&lt;h4&gt;背景&lt;/h4&gt;个性化联邦学习(PFL)已成为一种普遍范式，通过协作训练同时适应每个客户端的本地应用来提供个性化模型。然而，现有PFL方法通常面临客户端间普遍存在的数据异构性(非IID数据)这一重大挑战，这严重阻碍了收敛并降低了性能。&lt;h4&gt;目的&lt;/h4&gt;从根本上解决基于梯度的更新对非IID数据的固有敏感性问题，弥补研究空白，提出一个能够处理数据异构性的个性化联邦学习方案。&lt;h4&gt;方法&lt;/h4&gt;研究提出了FedHiP方案，包含三个阶段：分析性本地训练、分析性全局聚合和分析性本地个性化。该方法利用自监督预训练的趋势，将基础模型作为冻结主干进行无梯度特征提取，并进一步开发了分析分类器进行无梯度训练。&lt;h4&gt;主要发现&lt;/h4&gt;FedHiP方案的理想特性是异构不变性，意味着无论其他客户端的数据分布如何非IID，每个个性化模型保持相同。在基准数据集上的广泛实验验证了FedHiP方案的优越性，在准确性上比最先进的基线方法高出至少5.79%-20.97%。&lt;h4&gt;结论&lt;/h4&gt;通过避免基于梯度的更新，FedHiP方案从根本上解决了联邦学习中数据异构性问题，实现了理想的异构不变性，显著提高了个性化模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;最近，个性化联邦学习(PFL)已成为一种普遍范式，通过协作训练同时适应每个客户端的本地应用来提供个性化模型。现有PFL方法通常面临一个重大挑战，即客户端间普遍存在的数据异构性(非IID数据)，这严重阻碍了收敛并降低了性能。我们确定根本问题在于长期以来对基于梯度的更新的依赖，这些更新本质上对非IID数据敏感。为了从根本上解决这个问题并弥补研究空白，在本文中，我们提出了一个异构不变性个性化联邦学习方案，名为FedHiP，通过分析性(即闭式)解决方案避免基于梯度的更新。具体来说，我们利用自监督预训练的趋势，利用基础模型作为冻结主干进行无梯度特征提取。在特征提取器之后，我们进一步开发了分析分类器进行无梯度训练。为了支持集体泛化和个体个性化，我们的FedHiP方案包含三个阶段：分析性本地训练、分析性全局聚合和分析性本地个性化。FedHiP方案的分析性解决方案使其具有异构不变性的理想特性，这意味着无论其他客户端的数据分布如何非IID，每个个性化模型保持相同。在基准数据集上的广泛实验验证了我们的FedHiP方案的优越性，在准确性上比最先进的基线方法高出至少5.79%-20.97%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lately, Personalized Federated Learning (PFL) has emerged as a prevalentparadigm to deliver personalized models by collaboratively training whilesimultaneously adapting to each client's local applications. Existing PFLmethods typically face a significant challenge due to the ubiquitous dataheterogeneity (i.e., non-IID data) across clients, which severely hindersconvergence and degrades performance. We identify that the root issue lies inthe long-standing reliance on gradient-based updates, which are inherentlysensitive to non-IID data. To fundamentally address this issue and bridge theresearch gap, in this paper, we propose a Heterogeneity-invariant PersonalizedFederated learning scheme, named FedHiP, through analytical (i.e., closed-form)solutions to avoid gradient-based updates. Specifically, we exploit the trendof self-supervised pre-training, leveraging a foundation model as a frozenbackbone for gradient-free feature extraction. Following the feature extractor,we further develop an analytic classifier for gradient-free training. Tosupport both collective generalization and individual personalization, ourFedHiP scheme incorporates three phases: analytic local training, analyticglobal aggregation, and analytic local personalization. The closed-formsolutions of our FedHiP scheme enable its ideal property of heterogeneityinvariance, meaning that each personalized model remains identical regardlessof how non-IID the data are distributed across all other clients. Extensiveexperiments on benchmark datasets validate the superiority of our FedHiPscheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%in accuracy.</description>
      <author>example@mail.com (Jianheng Tang, Zhirui Yang, Jingchao Wang, Kejia Fan, Jinfeng Xu, Huiping Zhuang, Anfeng Liu, Houbing Herbert Song, Leye Wang, Yunhuai Liu)</author>
      <guid isPermaLink="false">2508.04470v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>TotalRegistrator: Towards a Lightweight Foundation Model for CT Image Registration</title>
      <link>http://arxiv.org/abs/2508.04450v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TotalRegistrator是一个基于UNet架构和场分解策略的多器官图像配准框架，能够在不同解剖区域间实现高效配准，通过大规模数据集训练和多种外部数据集验证，证明了该方法在多器官配准任务中的优越性和强大的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;图像配准是临床实践中分析纵向和多阶段CT图像的基本技术，但现有方法大多针对单器官应用，限制了它们在其他解剖区域的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时对多个解剖区域进行配准的图像配准框架，提高方法的泛化能力，使其能够应用于不同的解剖区域。&lt;h4&gt;方法&lt;/h4&gt;提出了TotalRegistrator，使用标准UNet架构和新颖场分解策略的图像配准框架；构建了一个包含695个全身配对CT扫描的大规模纵向数据集；将TotalRegistrator与经典迭代算法和基础模型进行基准测试；在三个外部数据集上评估模型的鲁棒性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在内部数据集上，所提出的方法在多器官腹部配准中通常优于基线方法，但在肺部配准性能上略有下降；在分布外数据集上，尽管没有针对这些任务进行微调，但与领先的单一器官模型相比取得了具有竞争力的结果，证明了其强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;TotalRegistrator是一个有效的多器官图像配准框架，具有强大的泛化能力，源代码将在GitHub上公开提供。&lt;h4&gt;翻译&lt;/h4&gt;图像配准是临床实践中分析纵向和多阶段CT图像的基本技术。然而，大多数现有方法针对单器官应用定制，限制了它们在其他解剖区域的泛化能力。这项工作提出了TotalRegistrator，一个图像配准框架，能够使用标准UNet架构和新颖的场分解策略同时对多个解剖区域进行配准。该模型轻量级，训练仅需11GB GPU内存。为了训练和评估我们的方法，我们构建了一个包含695个全身配对CT扫描的大规模纵向数据集，这些扫描来自在不同时间点获取的个体患者。我们将TotalRegistrator与通用的经典迭代算法和最近的图像配准基础模型进行了基准测试。为了进一步评估鲁棒性和泛化能力，我们在三个外部数据集上评估了我们的模型：来自Learn2Reg挑战赛的公开胸部和腹部数据集，以及合作医院的私人多阶段腹部数据集。内部数据集上的实验结果表明，所提出的方法在多器官腹部配准中通常优于基线方法，但在肺部配准性能上略有下降。在分布外数据集上，尽管没有针对这些任务进行微调，但与领先的单一器官模型相比取得了具有竞争力的结果，证明了其强大的泛化能力。源代码将在以下公开提供：https://github.com/DIAGNijmegen/oncology_image_registration.git。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有CT图像配准方法大多针对单一器官设计，难以泛化到其他解剖区域的问题。这个问题在临床实践中很重要，因为癌症治疗中常见全身成像，需要一个通用配准模型来简化临床流程，减少对特定任务模型的需求，这些特定模型通常需要大量资源训练维护，且难以处理跨越多个解剖区域的工作流程。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先指出传统配准方法计算密集且需要参数调整，难以应用于临床环境；然后指出现有深度学习方法虽然直接预测变形场，但大多局限于单一器官。作者借鉴了uniGradICON等基础模型的思想，但发现其依赖异构数据集且计算资源需求大。因此，作者提出使用单一精心策划的纵向数据集从头训练，采用区域特定的场分解策略，借鉴了UNet架构、多级级联架构和无监督学习框架（如VoxelMorph）的思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是场分解策略：将全身视图分为胸部、腹部和骨骼区域，每个区域分配独立注册块，最后由整体块融合产生连贯变形场。实现流程包括：1)收集695对全身CT扫描数据；2)预处理图像并重采样；3)构建基于UNet的注册块架构；4)按顺序训练各块(仿射→骨骼→胸部→腹部→全身)；5)结合互信息损失、Dice损失和正则化训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)区域特定的场分解策略；2)轻量级可在11GB GPU上运行的模型；3)包含695对扫描的大规模纵向数据集，其中104例由专家标注。相比之前工作，该方法使用单一数据集而非多样化公共数据集，采用场分解而非单一变形场，训练时独立训练各块减少内存需求，并增加了骨骼注册块扩展解剖覆盖范围。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TotalRegistrator提出了一种基于场分解策略的轻量级通用CT图像配准模型，能在标准GPU上高效训练，实现对多个解剖区域的准确配准，同时保持与最先进方法相当的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image registration is a fundamental technique in the analysis of longitudinaland multi-phase CT images within clinical practice. However, most existingmethods are tailored for single-organ applications, limiting theirgeneralizability to other anatomical regions. This work presentsTotalRegistrator, an image registration framework capable of aligning multipleanatomical regions simultaneously using a standard UNet architecture and anovel field decomposition strategy. The model is lightweight, requiring only11GB of GPU memory for training. To train and evaluate our method, weconstructed a large-scale longitudinal dataset comprising 695 whole-body(thorax-abdomen-pelvic) paired CT scans from individual patients acquired atdifferent time points. We benchmarked TotalRegistrator against a genericclassical iterative algorithm and a recent foundation model for imageregistration. To further assess robustness and generalizability, we evaluatedour model on three external datasets: the public thoracic and abdominaldatasets from the Learn2Reg challenge, and a private multiphase abdominaldataset from a collaborating hospital. Experimental results on the in-housedataset show that the proposed approach generally surpasses baseline methods inmulti-organ abdominal registration, with a slight drop in lung alignmentperformance. On out-of-distribution datasets, it achieved competitive resultscompared to leading single-organ models, despite not being fine-tuned for thosetasks, demonstrating strong generalizability. The source code will be publiclyavailable at: https://github.com/DIAGNijmegen/oncology_image_registration.git.</description>
      <author>example@mail.com (Xuan Loc Pham, Gwendolyn Vuurberg, Marjan Doppen, Joey Roosen, Tip Stille, Thi Quynh Ha, Thuy Duong Quach, Quoc Vu Dang, Manh Ha Luu, Ewoud J. Smit, Hong Son Mai, Mattias Heinrich, Bram van Ginneken, Mathias Prokop, Alessa Hering)</author>
      <guid isPermaLink="false">2508.04450v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Foundation Models for Mitotic Figure Classification</title>
      <link>http://arxiv.org/abs/2508.04441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基础模型在有丝分裂图像分类任务中的应用，通过低秩适应(LoRA)技术调整模型，发现仅使用10%的训练数据就能接近100%数据可用性时的性能水平，并在跨域泛化方面表现出色。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型性能随数据数量和多样性增加而提升，但在病理学等医学影像领域，特定任务的标记图像数据通常有限。自监督学习技术可以利用大量未标记数据训练基础模型，提供语义丰富的特征向量，解决数据有限问题。&lt;h4&gt;目的&lt;/h4&gt;研究基础模型在有丝分裂图像分类任务中的应用，有丝分裂计数是某些肿瘤的独立预后标志物，也是肿瘤分级系统的一部分。&lt;h4&gt;方法&lt;/h4&gt;研究多个当前基础模型的数据扩展规律，评估模型对未见肿瘤域的鲁棒性，使用注意力机制的LoRA调整模型，并与端到端训练的基线模型（CNN和Vision Transformers）进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;LoRA调整的基础模型性能优于标准线性调整的模型；仅使用10%的训练数据，性能接近100%数据可用性时的水平；最新的基础模型使用LoRA调整几乎消除了在未见肿瘤域上的性能差距；传统架构的完整微调仍具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;基础模型，特别是使用LoRA调整的模型，在有限数据条件下表现优异，且在跨域泛化方面也有良好表现。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型的性能已知随数据数量和多样性的增加而提升。在病理学，如同许多其他医学影像领域，特定任务的标记图像通常可用性有限。自监督学习技术使得能够利用大量未标记数据训练大规模神经网络，即基础模型，这些模型可以通过提供语义丰富的特征向量来解决有限数据问题，这些特征向量可以很好地泛化到新任务，只需最少的训练努力，从而提高模型性能和鲁棒性。在这项工作中，我们研究了基础模型在有丝分裂图像分类任务中的应用。有丝分裂计数可以从这一分类任务中推导出来，它是某些肿瘤的独立预后标志物，也是某些肿瘤分级系统的一部分。特别是，我们研究了多个当前基础模型的数据扩展规律，并评估它们对未见肿瘤域的鲁棒性。除了常用的线性探测范式外，我们还使用注意力机制的低秩适应(LoRA)来调整模型。我们将所有模型与端到端训练的基线模型（CNN和Vision Transformers）进行比较。我们的结果表明，LoRA调整的基础模型比使用标准线性调整的模型提供更优越的性能，仅使用10%的训练数据就能达到接近100%数据可用性时的性能水平。此外，最新基础模型的LoRA调整在评估未见肿瘤域时几乎消除了域外性能差距。然而，传统架构的完整微调仍然具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The performance of deep learning models is known to scale with data quantityand diversity. In pathology, as in many other medical imaging domains, theavailability of labeled images for a specific task is often limited.Self-supervised learning techniques have enabled the use of vast amounts ofunlabeled data to train large-scale neural networks, i.e., foundation models,that can address the limited data problem by providing semantically richfeature vectors that can generalize well to new tasks with minimal trainingeffort increasing model performance and robustness. In this work, weinvestigate the use of foundation models for mitotic figure classification. Themitotic count, which can be derived from this classification task, is anindependent prognostic marker for specific tumors and part of certain tumorgrading systems. In particular, we investigate the data scaling laws onmultiple current foundation models and evaluate their robustness to unseentumor domains. Next to the commonly used linear probing paradigm, we also adaptthe models using low-rank adaptation (LoRA) of their attention mechanisms. Wecompare all models against end-to-end-trained baselines, both CNNs and VisionTransformers. Our results demonstrate that LoRA-adapted foundation modelsprovide superior performance to those adapted with standard linear probing,reaching performance levels close to 100% data availability with only 10% oftraining data. Furthermore, LoRA-adaptation of the most recent foundationmodels almost closes the out-of-domain performance gap when evaluated on unseentumor domains. However, full fine-tuning of traditional architectures stillyields competitive performance.</description>
      <author>example@mail.com (Jonas Ammeling, Jonathan Ganz, Emely Rosbach, Ludwig Lausser, Christof A. Bertram, Katharina Breininger, Marc Aubreville)</author>
      <guid isPermaLink="false">2508.04441v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>VisionTS++: Cross-Modal Time Series Foundation Model with Continual Pre-trained Visual Backbones</title>
      <link>http://arxiv.org/abs/2508.04379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近期研究表明，在图像上预训练的视觉模型可通过将时间序列预测重新表述为图像重建任务，在时间序列预测中表现良好，但面临三个关键挑战：数据模态差异、多元预测差异和概率预测差异。为解决这些问题，研究者提出VisionTS++，一种基于视觉模型的时间序列基础模型，包含三项创新：基于视觉模型的过滤机制、彩色多元转换方法和多分位数预测方法。实验证明，该方法在多个基准测试中取得了最先进的结果，MSE降低比专用模型高6%-44%，在12个概率预测设置中的9个中排名第一。&lt;h4&gt;背景&lt;/h4&gt;近期研究表明，在图像上预训练的视觉模型可以通过将时间序列预测重新表述为图像重建任务，在时间序列预测中表现良好，表明它们可能成为通用时间序列基础模型的潜力。&lt;h4&gt;目的&lt;/h4&gt;弥合视觉模型到时间序列跨模态迁移的三个关键差距：数据模态差异、多元预测差异和概率预测差异，并提出一种基于视觉模型的时间序列基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出VisionTS++，一种基于视觉模型的时间序列基础模型，包含三项创新：(1)基于视觉模型的过滤机制识别高质量时间序列数据；(2)彩色多元转换方法将多元时间序列转换为多子图RGB图像；(3)多分位数预测方法使用并行重建头生成不同分位数水平的预测。&lt;h4&gt;主要发现&lt;/h4&gt;在分布内和分布外时间序列预测基准上取得了最先进(SOTA)的结果，在MSE降低方面比专用时间序列基础模型高出6%-44%，在12个概率预测设置中的9个中排名第一。&lt;h4&gt;结论&lt;/h4&gt;该工作为跨模态知识转移建立了新范式，推动了通用时间序列基础模型的发展。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，在图像上预训练的视觉模型可以通过将预测重新表述为图像重建任务，在时间序列预测中表现良好，表明它们作为通用时间序列基础模型的潜力。然而，由于三个关键差异，从视觉到时间序列的有效跨模态转移仍然具有挑战性：(1)结构化、有界的图像数据与无界、异构的时间序列之间的数据模态差距；(2)标准RGB三通道视觉模型与需要建模具有任意数量变量的时间序列之间的多元预测差距；(3)大多数视觉模型的确定性输出格式与需要不确定性感知的概率预测要求之间的概率预测差距。为了弥合这些差距，我们提出了VisionTS++，一种基于视觉模型的时间序列基础模型，在大规模时间序列数据集上进行持续预训练，包括三项创新：(1)基于视觉模型的过滤机制，用于识别高质量时间序列数据，从而减轻模态差距并提高预训练稳定性；(2)彩色多元转换方法，将多元时间序列转换为多子图RGB图像，捕捉复杂的变量间依赖关系；(3)多分位数预测方法，使用并行重建头生成不同分位数水平的预测，从而更灵活地近似任意输出分布，无需限制性的先验分布假设。在分布内和分布外时间序列预测基准上评估，该模型取得了最先进的结果，在MSE降低方面比专用时间序列基础模型高出6%-44%，在12个概率预测设置中的9个中排名第一。我们的工作为跨模态知识转移建立了新范式，推动了通用时间序列基础模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have revealed that vision models pre-trained on images canperform well in time series forecasting by reformulating forecasting as animage reconstruction task, suggesting their potential as universal time seriesfoundation models. However, effective cross-modal transfer from vision to timeseries remains challenging due to three key discrepancies: (1) data-modalitygap between structured, bounded image data and unbounded, heterogeneous timeseries; (2) multivariate-forecasting gap between standard RGBthree-channel-based vision models and the need to model time series witharbitrary numbers of variates; and (3) probabilistic-forecasting gap betweenthe deterministic output formats of most vision models and the requirement foruncertainty-aware probabilistic predictions. To bridge these gaps, we proposeVisionTS++, a vision-model-based TSFM that performs continual pre-training onlarge-scale time series datasets, including 3 innovations: (1) avision-model-based filtering mechanism to identify high-quality time seriesdata, thereby mitigating modality gap and improving pre-training stability, (2)a colorized multivariate conversion method that transforms multivariate timeseries into multi-subfigure RGB images, capturing complex inter-variatedependencies; and (3) a multi-quantile forecasting approach using parallelreconstruction heads to generate forecasts of different quantile levels, thusmore flexibly approximating arbitrary output distributions without restrictiveprior distributional assumptions. Evaluated on both in-distribution andout-of-distribution TSF benchmarks, \model achieves SOTA results, outperformingspecialized TSFMs by 6%-44% in MSE reduction and ranking first in 9 out of 12probabilistic forecasting settings. Our work establishes a new paradigm forcross-modal knowledge transfer, advancing the development of universal TSFMs.</description>
      <author>example@mail.com (Lefei Shen, Mouxiang Chen, Xu Liu, Han Fu, Xiaoxue Ren, Jianling Sun, Zhuo Li, Chenghao Liu)</author>
      <guid isPermaLink="false">2508.04379v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing</title>
      <link>http://arxiv.org/abs/2508.04361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了OmniPlay评估基准，用于测试多模态模型在动态交互环境中的智能表现。通过五个游戏环境评估六种领先模型，发现它们在记忆任务上表现出色但在推理和规划方面存在系统性失败，揭示了模型融合机制的脆弱性和'少即是多'的悖论现象。&lt;h4&gt;背景&lt;/h4&gt;现有通用基础模型(如Gemini和GPT-4o)展示了多模态能力，但现有评估无法测试它们在动态、交互式世界中的智能。静态基准缺乏自主性，交互式基准存在模态瓶颈，通常忽略关键听觉和时间线索。&lt;h4&gt;目的&lt;/h4&gt;为了弥合评估差距，引入OmniPlay这一诊断基准，旨在评估和探索代理模型在完整感官谱系中的融合和推理能力。&lt;h4&gt;方法&lt;/h4&gt;OmniPlay建立在模态相互依赖的核心理念上，包含五个游戏环境，系统性地创造协同和冲突场景，迫使代理执行真正的跨模态推理。对六种领先全模态模型进行了全面评估。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现模型在高保真记忆任务上表现出超人性能，但在需要稳健推理和战略规划的挑战中存在系统性失败。这种脆弱性源于脆弱的融合机制，导致在模态冲突下性能灾难性下降，并发现'少即是多'的悖论现象。&lt;h4&gt;结论&lt;/h4&gt;实现稳健AGI的路径需要超越规模扩展的研究重点，明确解决协同融合问题。&lt;h4&gt;翻译&lt;/h4&gt;虽然像Gemini和GPT-4o这样的通用基础模型展示了令人印象深刻的多模态能力，但现有的评估无法测试它们在动态、交互式世界中的智能。静态基准缺乏自主性，而交互式基准则存在严重的模态瓶颈，通常忽略关键的听觉和时间线索。为了弥合这一评估差距，我们引入了OmniPlay，这是一个诊断基准，不仅用于评估，还旨在探索代理模型在完整感官谱系中的融合和推理能力。OmniPlay建立在模态相互依赖的核心理念上，包含一套五个游戏环境，这些环境系统性地创造协同和冲突场景，迫使代理执行真正的跨模态推理。我们对六种领先的全模态模型的全面评估揭示了一个关键的两极分化：它们在高保真记忆任务上表现出超人性能，但在需要稳健推理和战略规划的挑战中存在系统性失败。我们证明这种脆弱性源于脆弱的融合机制，这导致在模态冲突下性能灾难性下降，并发现了一个反直觉的'少即是多'悖论，即移除感官信息反而可能提高性能。我们的研究结果表明，实现稳健AGI的路径需要超越规模扩展的研究重点，明确解决协同融合问题。我们的平台可在https://github.com/fuqingbie/omni-game-benchmark上匿名查看。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While generalist foundation models like Gemini and GPT-4o demonstrateimpressive multi-modal competence, existing evaluations fail to test theirintelligence in dynamic, interactive worlds. Static benchmarks lack agency,while interactive benchmarks suffer from a severe modal bottleneck, typicallyignoring crucial auditory and temporal cues. To bridge this evaluation chasm,we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,but to probe the fusion and reasoning capabilities of agentic models across thefull sensory spectrum. Built on a core philosophy of modality interdependence,OmniPlay comprises a suite of five game environments that systematically createscenarios of both synergy and conflict, forcing agents to perform genuinecross-modal reasoning. Our comprehensive evaluation of six leading omni-modalmodels reveals a critical dichotomy: they exhibit superhuman performance onhigh-fidelity memory tasks but suffer from systemic failures in challengesrequiring robust reasoning and strategic planning. We demonstrate that thisfragility stems from brittle fusion mechanisms, which lead to catastrophicperformance degradation under modality conflict and uncover a counter-intuitive"less is more" paradox, where removing sensory information can paradoxicallyimprove performance. Our findings suggest that the path toward robust AGIrequires a research focus beyond scaling to explicitly address synergisticfusion. Our platform is available for anonymous review athttps://github.com/fuqingbie/omni-game-benchmark.</description>
      <author>example@mail.com (Fuqing Bie, Shiyu Huang, Xijia Tao, Zhiqin Fang, Leyi Pan, Junzhe Chen, Min Ren, Liuyu Xiang, Zhaofeng He)</author>
      <guid isPermaLink="false">2508.04361v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Chain of Questions: Guiding Multimodal Curiosity in Language Models</title>
      <link>http://arxiv.org/abs/2508.04350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了问题链(CoQ)框架，一种好奇心驱动的推理方法，使多模态语言模型能够动态生成针对性问题并选择性地激活相关感官模态，从而提高推理能力和准确性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的推理能力通过思维链和逐步解释方法得到显著提升，但这些改进尚未完全过渡到多模态环境中，模型需要主动决定在复杂现实环境中使用哪些感官模态。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法使多模态语言模型能够动态生成关于周围环境的针对性问题，引导模型选择性地激活相关模态，收集准确推理和响应生成所需的关键信息。&lt;h4&gt;方法&lt;/h4&gt;提出问题链(CoQ)框架，并通过整合WebGPT、ScienceQA、AVSD和ScanQA数据集构建新的多模态基准数据集进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;CoQ方法提高了基础模型有效识别和整合相关感官信息的能力，导致准确性提高、推理过程可解释性增强以及与多样化多模态任务的对齐。&lt;h4&gt;结论&lt;/h4&gt;CoQ框架能够改善多模态语言模型的推理能力，使其能够更好地处理复杂的现实世界环境。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型中的推理能力已通过思维链和明确的逐步解释方法得到显著提升。然而，这些改进尚未完全过渡到多模态环境中，在多模态环境中，模型必须主动决定在与复杂的现实世界环境互动时使用哪些感官模态，如视觉、音频或空间感知。在本文中，我们引入了问题链(CoQ)框架，这是一种好奇心驱动的推理方法，它鼓励多模态语言模型动态生成关于其周围环境的针对性问题。这些生成的问题引导模型选择性地激活相关模态，从而收集准确推理和响应生成所需的关键信息。我们在一个新颖的多模态基准数据集上评估了我们的框架，该数据集是通过整合WebGPT、ScienceQA、AVSD和ScanQA数据集构建的。实验结果表明，我们的CoQ方法提高了基础模型有效识别和整合相关感官信息的能力，从而提高了准确性、推理过程的可解释性以及与多样化多模态任务的对齐。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning capabilities in large language models (LLMs) have substantiallyadvanced through methods such as chain-of-thought and explicit step-by-stepexplanations. However, these improvements have not yet fully transitioned tomultimodal contexts, where models must proactively decide which sensorymodalities such as vision, audio, or spatial perception to engage wheninteracting with complex real-world environments. In this paper, we introducethe Chain of Questions (CoQ) framework, a curiosity-driven reasoning approachthat encourages multimodal language models to dynamically generate targetedquestions regarding their surroundings. These generated questions guide themodel to selectively activate relevant modalities, thereby gathering criticalinformation necessary for accurate reasoning and response generation. Weevaluate our framework on a novel multimodal benchmark dataset, assembled byintegrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental resultsdemonstrate that our CoQ method improves a foundation model's ability toeffectively identify and integrate pertinent sensory information. This leads toimproved accuracy, interpretability, and alignment of the reasoning processwith diverse multimodal tasks.</description>
      <author>example@mail.com (Nima Iji, Kia Dashtipour)</author>
      <guid isPermaLink="false">2508.04350v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks</title>
      <link>http://arxiv.org/abs/2508.04316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于掩码自编码器的DAS信号识别基础模型MAEPD，通过视觉提示调优(VPT)方法，在仅微调0.322%参数的情况下实现了96.94%的分类准确率，超越了传统方法并减少了45%的训练时间。&lt;h4&gt;背景&lt;/h4&gt;分布式声学传感(DAS)技术在多个领域有广泛应用，但由于异构传感环境导致的数据分布差异给数据驱动的人工智能模型带来挑战，限制了跨域泛化能力，并面临标记训练数据短缺的问题。&lt;h4&gt;目的&lt;/h4&gt;解决DAS数据分布差异导致的模型泛化能力有限和标记数据短缺问题，开发一个通用的DAS信号识别基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出MAEPD模型，在包含635,860个样本的多类型DAS数据集上进行自监督掩码重建预训练，捕获DAS信号的深层语义特征；采用视觉提示调优(VPT)进行下游任务，冻结预训练骨干参数，只微调插入到Transformer编码器层中的少量可学习视觉提示向量。&lt;h4&gt;主要发现&lt;/h4&gt;在室内步态识别任务中，VPT-Deep方法仅微调0.322%参数就达到96.94%分类准确率，比传统完全微调方法高0.61%，同时训练时间减少45%；模型在管道泄漏检测中也表现出强大性能。&lt;h4&gt;结论&lt;/h4&gt;MAEPD作为基础模型具有通用性、高效性和可扩展性，为解决DAS领域信号识别模型泛化能力有限的问题提供了新范式。&lt;h4&gt;翻译&lt;/h4&gt;分布式声学传感(DAS)技术在各个领域发现越来越多的应用。然而，由于异构传感环境导致的数据分布差异给数据驱动的人工智能(AI)模型带来挑战，限制了跨域泛化能力并面临标记训练数据短缺的问题。为解决这些问题，本研究提出了一种基于掩码自编码器的DAS信号识别基础模型，名为MAEPD。MAEPD模型在包含635,860个样本的数据集上进行预训练，涵盖DAS步态时空信号、用于周界安全的2D GASF图像、用于管道泄漏的2D时频图像，以及包括鲸鱼发声和地震活动的开放数据集信号，使用自监督掩码重建任务来捕获DAS信号的深层语义特征。采用视觉提示调优(VPT)进行下游识别任务。该方法冻结预训练骨干参数，只微调插入到Transformer编码器层中的一小组可学习视觉提示向量。在NVIDIA GeForce RTX 4080 Super平台上使用室内步态识别作为下游任务验证MAEPD。VPT-Deep方法仅微调0.322%的参数就实现了96.94%的分类准确率，超越了传统的完全微调(FFT)方法0.61%，并将训练时间减少了45%。该模型在管道泄漏检测中也表现出强大的性能，证实了MAEPD作为基础模型的通用性、高效性和可扩展性。这种方法为解决DAS领域信号识别模型泛化能力有限的问题提供了新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distributed Acoustic Sensing (DAS) technology finds growing applicationsacross various domains. However, data distribution disparities due toheterogeneous sensing environments pose challenges for data-driven artificialintelligence (AI) models, limiting cross-domain generalization and facing ashortage of labeled training data. To address these issues, this study proposesa foundational model for DAS signal recognition based on a Masked Autoencoder,named MAEPD. The MAEPD model is pretrained on a dataset of 635,860 samples,encompassing DAS gait spatiotemporal signals, 2D GASF images for perimetersecurity, 2D time-frequency images for pipeline leakage, and open-datasetsignals including whale vocalizations and seismic activities, using aself-supervised mask reconstruction task to capture deep semantic features ofDAS signals. Visual Prompt Tuning (VPT) is employed for downstream recognitiontasks. This method freezes the pretrained backbone parameters and fine-tunesonly a small set of learnable visual prompt vectors inserted into theTransformer encoder layers. Experiments on the NVIDIA GeForce RTX 4080 Superplatform validate MAEPD using indoor gait recognition as a downstream task. TheVPT-Deep approach achieves a classification accuracy of 96.94% with just 0.322%of parameters fine-tuned, surpassing the traditional Full Fine Tuning (FFT)method by 0.61% and reducing training time by 45%. The model also exhibitsrobust performance in pipeline leakage detection, confirming the generality,efficiency, and scalability of MAEPD as a foundational model. This approachoffers a novel paradigm for addressing the limited generalization of signalrecognition models in the DAS domain.</description>
      <author>example@mail.com (Kun Gui, Hongliang Ren, Shang Shi, Jin Lu, Changqiu Yu, Quanjun Cao, Guomin Gu, Qi Xuan)</author>
      <guid isPermaLink="false">2508.04316v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Edge2Prompt: Modality-Agnostic Model for Out-of-Distribution Liver Segmentation</title>
      <link>http://arxiv.org/abs/2508.04305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Edge2Prompt是一种新颖的模态无关肝脏分割管道，结合经典边缘检测与基础模型，能有效处理分布内和分布外数据，在数据稀缺场景下表现优异。&lt;h4&gt;背景&lt;/h4&gt;肝脏分割对于肿瘤切除或移植等手术的术前规划至关重要，但由于模态特定工具和数据稀缺，在临床工作流程中实施面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种模态无关的肝脏分割方法，能够推广到分布外(OOD)数据，解决临床应用中的数据稀缺问题。&lt;h4&gt;方法&lt;/h4&gt;Edge2Prompt管道首先从输入图像中提取模态无关的边缘图，然后通过U-Net处理这些边缘图生成基于logit的提示，这些提示引导Segment Anything Model 2(SAM-2)生成2D肝脏分割，最后将2D分割重建为3D体积。&lt;h4&gt;主要发现&lt;/h4&gt;在多模态CHAOS数据集上评估，Edge2Prompt在分布内训练和测试时与经典分割方法具有竞争力；在数据稀缺场景下表现更优；在OOD任务上达到86.4%的平均Dice分数，比U-Net基线高27.4%，比其他自提示方法高9.1%。&lt;h4&gt;结论&lt;/h4&gt;Edge2Prompt成功桥接了经典模型和基础模型，实现了临床适应性高、数据效率高的肝脏分割方法。&lt;h4&gt;翻译&lt;/h4&gt;肝脏分割对于肿瘤切除或移植等手术的术前规划至关重要，但由于模态特定工具和数据稀缺，在临床工作流程中实施面临挑战。我们提出了Edge2Prompt，一种新颖的模态无关肝脏分割管道，能够推广到分布外(OOD)数据。我们的方法结合了经典边缘检测和基础模型。首先从输入图像中提取模态无关的边缘图，然后通过U-Net处理生成基于logit的提示。这些提示引导Segment Anything Model 2(SAM-2)生成2D肝脏分割，随后可重建为3D体积。在多模态CHAOS数据集上的评估显示，Edge2Prompt在分布内训练和测试时与经典分割方法具有竞争力，并且在数据稀缺场景下由于SAM-2模块而优于其他方法。此外，它在OOD任务上达到86.4%的平均Dice分数，比U-Net基线高27.4%，比其他自提示方法高9.1%，证明了其有效性。这项工作将经典模型和基础模型桥接起来，实现了临床适应性高、数据效率高的分割。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Liver segmentation is essential for preoperative planning in interventionslike tumor resection or transplantation, but implementation in clinicalworkflows faces challenges due to modality-specific tools and data scarcity. Wepropose Edge2Prompt, a novel pipeline for modality-agnostic liver segmentationthat generalizes to out-of-distribution (OOD) data. Our method integratesclassical edge detection with foundation models. Modality-agnostic edge mapsare first extracted from input images, then processed by a U-Net to generatelogit-based prompts. These prompts condition the Segment Anything Model 2(SAM-2) to generate 2D liver segmentations, which can then be reconstructedinto 3D volumes. Evaluated on the multi-modal CHAOS dataset, Edge2Promptachieves competitive results compared to classical segmentation methods whentrained and tested in-distribution (ID), and outperforms them in data-scarcescenarios due to the SAM-2 module. Furthermore, it achieves a mean Dice Scoreof 86.4% on OOD tasks, outperforming U-Net baselines by 27.4% and otherself-prompting methods by 9.1%, demonstrating its effectiveness. This workbridges classical and foundation models for clinically adaptable,data-efficient segmentation.</description>
      <author>example@mail.com (Nathan Hollet, Oumeymah Cherkaoui, Philippe C. Cattin, Sidaty El hadramy)</author>
      <guid isPermaLink="false">2508.04305v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>What Holds Back Open-Vocabulary Segmentation?</title>
      <link>http://arxiv.org/abs/2508.04211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at ICCV 25 Workshop: What is Next in  Multimodal Foundation Models?&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对开放词汇分割模型的性能瓶颈问题，提出了新的oracle组件来识别和解耦这些瓶颈，并通过实验提供了重要发现，为未来研究指明了方向。&lt;h4&gt;背景&lt;/h4&gt;标准分割设置无法训练出识别训练分类之外概念的模型。开放词汇方法通过语言-图像预训练有望解决这一问题，但性能已停滞近两年。&lt;h4&gt;目的&lt;/h4&gt;识别并解决开放词汇分割模型中的性能瓶颈，提升模型对训练分类外概念的识别能力。&lt;h4&gt;方法&lt;/h4&gt;提出新的oracle组件，利用真实信息来识别和解耦导致开放词汇模型性能停滞的瓶颈。&lt;h4&gt;主要发现&lt;/h4&gt;验证实验提供了关于开放词汇模型失败原因的重要经验发现，并提出了有前途的未来研究方向。&lt;h4&gt;结论&lt;/h4&gt;通过识别和解耦瓶颈，该研究为开放词汇分割模型的未来发展提供了新思路和方向。&lt;h4&gt;翻译&lt;/h4&gt;标准的分割设置无法提供能够识别训练分类之外概念的模型。开放词汇方法承诺通过使用数十亿图像-标题对进行语言-图像预训练来弥补这一差距。不幸的是，我们观察到由于几个瓶颈，这一承诺未能实现，导致性能停滞了近两年。本文提出了新的oracle组件，利用真实信息来识别和解耦这些瓶颈。所呈现的验证实验提供了重要的经验发现，这些发现对开放词汇模型的失败提供了更深入的见解，并提出了突出的方法来解锁未来的研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Standard segmentation setups are unable to deliver models that can recognizeconcepts outside the training taxonomy. Open-vocabulary approaches promise toclose this gap through language-image pretraining on billions of image-captionpairs. Unfortunately, we observe that the promise is not delivered due toseveral bottlenecks that have caused the performance to plateau for almost twoyears. This paper proposes novel oracle components that identify and decouplethese bottlenecks by taking advantage of the groundtruth information. Thepresented validation experiments deliver important empirical findings thatprovide a deeper insight into the failures of open-vocabulary models andsuggest prominent approaches to unlock the future research.</description>
      <author>example@mail.com (Josip Šarić, Ivan Martinović, Matej Kristan, Siniša Šegvić)</author>
      <guid isPermaLink="false">2508.04211v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork</title>
      <link>http://arxiv.org/abs/2508.04163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合基于知识和数据驱动方法的架构，用于解决AI代理在辅助角色中临时团队协作的问题。该架构使每个代理能够通过非单调逻辑推理，利用先验常识知识、快速学习和修订的模型来预测其他代理行为，以及基于基础模型中类似情况的通用知识来预测未来目标。&lt;h4&gt;背景&lt;/h4&gt;AI代理在辅助角色中经常需要与其他代理（人类、AI系统）协作，而无需事先协调。目前最先进的方法通常采用数据驱动的方法，需要大量标记的先前观测数据集，缺乏透明度，并且难以快速修改现有知识以应对变化。随着代理数量的增加，决策的复杂性使得有效协作变得困难。&lt;h4&gt;目的&lt;/h4&gt;提倡利用基于知识和数据驱动方法的互补优势来进行临时团队推理和学习。&lt;h4&gt;方法&lt;/h4&gt;提出一种架构，使每个临时代理能够通过非单调逻辑推理确定其行动：(a)使用先验常识领域特定知识；(b)使用快速学习和修订的模型来预测其他代理的行为；(c)基于现有基础模型中类似情况的通用知识来预测抽象的未来目标。在VirtualHome（真实的基于物理的3D模拟环境）中实验评估该架构的能力。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中没有明确提及具体发现，但暗示了所提出架构在解决临时团队协作问题上的有效性。&lt;h4&gt;结论&lt;/h4&gt;摘要中没有明确结论，但暗示了所提出的基于知识和数据驱动相结合的方法对于解决临时团队协作问题有潜力。&lt;h4&gt;翻译&lt;/h4&gt;部署在辅助角色中的AI代理通常必须与其他代理（人类、AI系统）协作，而无需事先协调。此类临时团队中被认为最先进的方法通常采用数据驱动的方法，需要大量标记的先前观测数据集，缺乏透明度，并且难以快速修改现有知识以应对变化。随着代理数量的增加，决策的复杂性使得有效协作变得困难。本文提倡利用基于知识和数据驱动方法的互补优势来进行临时团队推理和学习。对于任何给定目标，我们的架构使每个临时代理能够通过非单调逻辑推理确定其行动：(a)先验常识领域特定知识；(b)快速学习和修订以预测其他代理行为的模型；(c)基于现有基础模型中类似情况的通用知识预测抽象未来目标。我们在VirtualHome（一个真实的基于物理的3D模拟环境）中实验评估了我们架构的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI agents deployed in assistive roles often have to collaborate with otheragents (humans, AI systems) without prior coordination. Methods consideredstate of the art for such ad hoc teamwork often pursue a data-driven approachthat needs a large labeled dataset of prior observations, lacks transparency,and makes it difficult to rapidly revise existing knowledge in response tochanges. As the number of agents increases, the complexity of decision-makingmakes it difficult to collaborate effectively. This paper advocates leveragingthe complementary strengths of knowledge-based and data-driven methods forreasoning and learning for ad hoc teamwork. For any given goal, ourarchitecture enables each ad hoc agent to determine its actions throughnon-monotonic logical reasoning with: (a) prior commonsense domain-specificknowledge; (b) models learned and revised rapidly to predict the behavior ofother agents; and (c) anticipated abstract future goals based on genericknowledge of similar situations in an existing foundation model. Weexperimentally evaluate our architecture's capabilities in VirtualHome, arealistic physics-based 3D simulation environment.</description>
      <author>example@mail.com (Hasra Dodampegama, Mohan Sridharan)</author>
      <guid isPermaLink="false">2508.04163v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>STARE: Predicting Decision Making Based on Spatio-Temporal Eye Movements</title>
      <link>http://arxiv.org/abs/2508.04148v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为STARE的深度学习架构，用于从原始注视点或眼睛在决策环境图像上的时间序列预测各种消费者选择行为。&lt;h4&gt;背景&lt;/h4&gt;目前还没有可用于从原始注视点或眼睛在决策环境图像上的时间序列预测各种消费者选择行为的基础模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种深度学习架构，能够从眼睛运动的时间序列数据有效预测消费者选择行为。&lt;h4&gt;方法&lt;/h4&gt;提出名为STARE的架构，使用新的标记化策略将眼动坐标映射到预定义的兴趣区域，使数据可用于基于T5架构的Chronos时间序列基础模型，并添加共同注意力和/或交叉注意力机制捕捉眼动特征。与多种最先进方法在多个数据集上进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体研究结果，只提到了与现有方法的比较研究。&lt;h4&gt;结论&lt;/h4&gt;这是开发和测试基于眼运动神经生理学的视觉注意力动力学的深度学习架构的第一步。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种深度学习架构，用于从决策环境图像的原始注视点或眼睛注视的时间序列预测各种消费者选择行为，目前此类基础模型尚不存在。该架构名为STARE（眼动时空注意力表示），采用新的标记化策略，将眼动时间序列的x和y像素坐标映射到预定义的连续兴趣区域。这种标记化使时空眼动数据可用于Chronos（一种基于T5架构的时间序列基础模型），并添加了共同注意力和/或交叉注意力来捕捉眼动的方向性和/或双眼间的影响。我们在多个数据集上将STARE与几种最先进的方法进行比较，目的是从眼动预测消费者选择行为。因此，我们迈出了开发和测试基于眼动神经生理学的视觉注意力动力学的深度学习架构的第一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The present work proposes a Deep Learning architecture for the prediction ofvarious consumer choice behaviors from time series of raw gaze or eye fixationson images of the decision environment, for which currently no foundationalmodels are available. The architecture, called STARE (Spatio-Temporal AttentionRepresentation for Eye Tracking), uses a new tokenization strategy, whichinvolves mapping the x- and y- pixel coordinates of eye-movement time series onpredefined, contiguous Regions of Interest. That tokenization makes thespatio-temporal eye-movement data available to the Chronos, a time-seriesfoundation model based on the T5 architecture, to which co-attention and/orcross-attention is added to capture directional and/or interocular influencesof eye movements. We compare STARE with several state-of-the art alternativeson multiple datasets with the purpose of predicting consumer choice behaviorsfrom eye movements. We thus make a first step towards developing and testing DLarchitectures that represent visual attention dynamics rooted in theneurophysiology of eye movements.</description>
      <author>example@mail.com (Moshe Unger, Alexander Tuzhilin, Michel Wedel)</author>
      <guid isPermaLink="false">2508.04148v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Neuro-MoBRE: Exploring Multi-subject Multi-task Intracranial Decoding via Explicit Heterogeneity Resolving</title>
      <link>http://arxiv.org/abs/2508.04128v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Neural Mixture of Brain Regional Experts (Neuro-MoBRE)的通用神经生理解码框架，旨在解决神经生理建模中普遍存在的数据异质性挑战。该框架结合脑区-时间嵌入机制和专家混合方法，有效处理来自不同脑区和任务的神经信号，并在多种任务上展现出优越的性能和强大的零样本解码泛化能力。&lt;h4&gt;背景&lt;/h4&gt;神经生理学解码对推进脑机接口(BCI)技术发展至关重要，已从深度学习进步中显著受益。然而，现有解码方法主要局限于单任务场景和单个受试者，限制了其应用性和泛化性。创建大规模神经生理基础模型的努力虽显示出前景，但由于受试者和任务间普遍存在的数据异质性，仍面临重大挑战。简单地增加模型参数和数据集大小而不明确解决异质性，无法复制自然语言处理中的扩展成功。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用解码框架，明确设计用于处理神经生理建模中普遍存在的数据异质性问题，提高解码方法的泛化能力和适用范围。&lt;h4&gt;方法&lt;/h4&gt;引入Neuro-MoBRE框架，包含：1)脑区-时间嵌入机制结合专家混合方法，将不同脑区的神经信号分配给专门区域专家；2)区域掩码自编码预训练策略，增强受试者间表征一致性；3)任务解缠结的信息聚合方法，有效处理特定任务的神经常态变化。&lt;h4&gt;主要发现&lt;/h4&gt;在来自11个受试者跨越五种不同任务（包括复杂语言解码和癫痫发作诊断）的颅内记录评估中，Neuro-MoBRE超越了先前方法，并在未见过的受试者上表现出强大的零样本解码泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Neuro-MoBRE框架通过明确处理神经生理数据中的异质性，显著提高了神经生理解码的性能和泛化能力，为脑机接口技术的发展提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;神经生理学解码对推进脑机接口(BCI)技术发展至关重要，并已从最近的深度学习进步中显著受益。然而，现有的解码方法主要仍局限于单任务场景和单个受试者，限制了它们更广泛的应用性和泛化性。创建大规模神经生理基础模型的努力显示出前景，但由于受试者和解码任务之间普遍存在的数据异质性，仍面临重大挑战。简单地增加模型参数和数据集大小而不明确解决这种异质性，无法复制在自然语言处理中看到的扩展成功。在此，我们引入了神经脑区专家混合模型(Neural Mixture of Brain Regional Experts, Neuro-MoBRE)，这是一种通用解码框架，明确设计用于处理神经生理建模中普遍存在的数据异质性。Neuro-MoBRE结合了脑区-时间嵌入机制和专家混合方法，将来自不同脑区的神经信号分配给统一的嵌入基础上的专门区域专家，从而明确解决结构和功能异质性。此外，我们的区域掩码自编码预训练策略进一步增强了受试者之间的表征一致性，辅以针对有效处理特定任务神经变异而设计的任务解缠结信息聚合方法。在来自11个受试者跨越五种不同任务的颅内记录评估中，包括复杂语言解码和癫痫发作诊断，结果表明Neuro-MoBRE超越了先前的方法，并在未见过的受试者上表现出强大的零样本解码泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neurophysiological decoding, fundamental to advancing brain-computerinterface (BCI) technologies, has significantly benefited from recent advancesin deep learning. However, existing decoding approaches largely remainconstrained to single-task scenarios and individual subjects, limiting theirbroader applicability and generalizability. Efforts towards creatinglarge-scale neurophysiological foundation models have shown promise, butcontinue to struggle with significant challenges due to pervasive dataheterogeneity across subjects and decoding tasks. Simply increasing modelparameters and dataset size without explicitly addressing this heterogeneityfails to replicate the scaling successes seen in natural language processing.Here, we introduce the Neural Mixture of Brain Regional Experts (Neuro-MoBRE),a general-purpose decoding framework explicitly designed to manage theubiquitous data heterogeneity in neurophysiological modeling. Neuro-MoBREincorporates a brain-regional-temporal embedding mechanism combined with amixture-of-experts approach, assigning neural signals from distinct brainregions to specialized regional experts on a unified embedding basis, thusexplicitly resolving both structural and functional heterogeneity.Additionally, our region-masked autoencoding pre-training strategy furtherenhances representational consistency among subjects, complemented by atask-disentangled information aggregation method tailored to effectively handletask-specific neural variations. Evaluations conducted on intracranialrecordings from 11 subjects across five diverse tasks, including complexlanguage decoding and epileptic seizure diagnosis, demonstrate that Neuro-MoBREsurpasses prior art and exhibits robust generalization for zero-shot decodingon unseen subjects.</description>
      <author>example@mail.com (Di Wu, Yifei Jia, Siyuan Li, Shiqi Zhao, Jie Yang, Mohamad Sawan)</author>
      <guid isPermaLink="false">2508.04128v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>WiFo-CF: Wireless Foundation Model for CSI Feedback</title>
      <link>http://arxiv.org/abs/2508.04068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为WiFo-CF的新型无线基础模型，专为CSI反馈设计，能够适应异构配置，通过多用户多速率自监督预训练策略和S-R MoE架构实现，在模拟和真实场景中表现优异，并能有效促进下游任务适应。&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的CSI反馈方案具有强大压缩能力，但通常受限于固定系统配置，限制了泛化能力和灵活性。&lt;h4&gt;目的&lt;/h4&gt;解决传统CSI反馈方案的局限性，提出一种能够适应异构配置(如变化的信道维度、反馈速率和数据分布)的统一框架。&lt;h4&gt;方法&lt;/h4&gt;WiFo-CF采用两大关键创新：(1)多用户、多速率自监督预训练策略；(2)共享和路由专家混合(S-R MoE)架构。同时利用首个异构信道反馈数据集支持大规模预训练。&lt;h4&gt;主要发现&lt;/h4&gt;WiFo-CF在模拟和真实场景中的分布内和分布外数据上实现了卓越性能，学习到的表示有效促进了基于CSI的室内定位等下游任务的适应。&lt;h4&gt;结论&lt;/h4&gt;WiFo-CF通过创新架构和预训练策略解决了传统CSI反馈方案的局限性，提供了更好的泛化能力和灵活性，验证了其可扩展性和部署潜力。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的信道状态信息(CSI)反馈方案展示了强大的压缩能力，但通常受限于固定系统配置，限制了它们的泛化和灵活性。为应对这一挑战，我们提出了WiFo-CF，一种专为CSI反馈设计的新型无线基础模型，通过其关键创新独特地适应异构配置，如变化的信道维度、反馈速率和数据分布，在一个统一框架内：(1)多用户、多速率自监督预训练策略；(2)共享和路由专家混合(S-R MoE)架构。支持WiFo-CF大规模预训练的是首个异构信道反馈数据集，其多样化的模式使模型能够在模拟和真实场景中的分布内和分布外数据上实现卓越性能。此外，学习到的表示有效促进了基于CSI的室内定位等下游任务的适应，验证了WiFo-CF的可扩展性和部署潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based channel state information (CSI) feedback schemesdemonstrate strong compression capabilities but are typically constrained tofixed system configurations, limiting their generalization and flexibility. Toaddress this challenge, WiFo-CF, a novel wireless foundation model tailored forCSI feedback, is proposed, uniquely accommodating heterogeneous configurationssuch as varying channel dimensions, feedback rates, and data distributionswithin a unified framework through its key innovations: (1) a multi-user,multi-rate self-supervised pre-training strategy; and (2) a Mixture of Sharedand Routed Expert (S-R MoE) architecture. Supporting the large-scalepre-training of WiFo-CF is the first heterogeneous channel feedback dataset,whose diverse patterns enable the model to achieve superior performance on bothin-distribution and out-of-distribution data across simulated and real-worldscenarios. Furthermore, the learned representations effectively facilitateadaptation to downstream tasks such as CSI-based indoor localization,validating WiFo-CF's scalability and deployment potential.</description>
      <author>example@mail.com (Liu Xuanyu, Gao Shijian, Liu Boxun, Cheng Xiang, Yang Liuqing)</author>
      <guid isPermaLink="false">2508.04068v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>VeriGUI: Verifiable Long-Chain GUI Dataset</title>
      <link>http://arxiv.org/abs/2508.04026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了VeriGUI，一个新型的可验证长链GUI数据集，用于促进在真实计算机环境中运行的通用GUI智能体的开发和评估。&lt;h4&gt;背景&lt;/h4&gt;最近的研究致力于构建能够执行复杂图形用户界面（GUI）任务的自主智能体，这些研究有潜力彻底改变人机交互。然而，现有工作主要集中在短期交互，并且仅依赖结果验证，限制了它们在真实世界GUI应用中的可扩展性。&lt;h4&gt;目的&lt;/h4&gt;创建一个专门设计用于开发和评估能够处理长期任务分解和执行的通用GUI智能体的数据集。&lt;h4&gt;方法&lt;/h4&gt;VeriGUI数据集强调两个关键维度：1）长链复杂性，任务被分解为跨越数百步的相互依赖的子任务序列，设计允许任何子任务作为有效起点；2）子任务级别可验证性，支持在每个子任务内的多样化探索策略，同时确保每个子任务级别的目标保持可验证和一致。数据集包含桌面和Web上的GUI任务轨迹，由人类专家注释。&lt;h4&gt;主要发现&lt;/h4&gt;使用不同基础模型的多种智能体在VeriGUI上的广泛实验显示处理长期任务时存在显著性能差距，突显了GUI智能体需要更强大的规划和决策能力。&lt;h4&gt;结论&lt;/h4&gt;GUI智能体需要发展更强大的规划和决策能力来处理长期任务，而VeriGUI数据集为评估和开发这些智能体提供了有价值的资源。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究致力于构建能够执行复杂图形用户界面（GUI）任务的自主智能体，有潜力彻底改变人机交互。尽管结果令人鼓舞，但现有工作主要集中在短期交互，并仅依赖结果验证，从而限制了它们在需要长期任务分解和执行的真实世界GUI应用中的可扩展性。在这项工作中，我们介绍了VeriGUI，一个新型的可验证长链GUI数据集，旨在促进在真实计算机环境中运行的通用GUI智能体的开发和评估。我们的数据集强调两个关键维度：（1）长链复杂性，任务被分解为跨越数百步的相互依赖的子任务序列，明确设计允许任何子任务作为有效起点；（2）子任务级别可验证性，使每个子任务内能够进行多样化的探索策略，同时确保每个子任务级别的目标保持可验证和一致。该数据集包含桌面和Web上的GUI任务轨迹，由人类专家注释。在VeriGUI上使用不同基础模型的多种智能体的广泛实验揭示了处理长期任务时的显著性能差距，突显了GUI智能体需要更强大的规划和决策能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have delved into constructing autonomous agents capable ofperforming complex Graphical User Interface (GUI)-based computer tasks, withthe potential to revolutionize human-computer interaction. Despite encouragingresults, existing efforts mainly focus on short-term interactions and rely onoutcome-only verification, thereby limiting their scalability in real-world GUIapplications that demand long-horizon task decomposition and execution. In thiswork, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designedto facilitate the development and evaluation of generalist GUI agents operatingin realistic computer environments. Our dataset emphasizes two criticaldimensions: (1) long-chain complexity, with tasks decomposed into a sequence ofinterdependent subtasks spanning hundreds of steps, explicitly designed toallow any subtask to serve as a valid starting point; and (2) subtask-levelverifiability, which enables diverse exploration strategies within eachsubtask, while ensuring that each subtask-level goal remains verifiable andconsistent. The dataset consists of GUI task trajectories across both desktopand web, annotated by human experts. Extensive experiments on VeriGUI usingvarious agents with different foundation models reveal significant performancegaps in handling long-horizon tasks, highlighting the need for more robustplanning and decision-making capabilities in GUI agents.</description>
      <author>example@mail.com (Shunyu Liu, Minghao Liu, Huichi Zhou, Zhenyu Cui, Yang Zhou, Yuhao Zhou, Wendong Fan, Ge Zhang, Jiajun Shi, Weihao Xuan, Jiaxing Huang, Shuang Luo, Fang Wu, Heli Qi, Qingcheng Zeng, Ziqi Ren, Jialiang Gao, Jindi Lv, Junjie Wang, Aosong Feng, Heng Zhou, Wangchunshu Zhou, Zhenfei Yin, Wenlong Zhang, Guohao Li, Wenhao Yu, Irene Li, Lei Ma, Lei Bai, Qunshu Lin, Mingli Song, Dacheng Tao)</author>
      <guid isPermaLink="false">2508.04026v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models</title>
      <link>http://arxiv.org/abs/2508.03998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种社交机器人共同主持人系统，通过分析多模态会议数据为人类主持人提供微妙提示，帮助解决团体会议中的挑战。该系统使用代理概念瓶颈模型(CBM)进行推理，确保决策过程的透明度和可信度。&lt;h4&gt;背景&lt;/h4&gt;成功的团体会议需要促进个人目标设定和执行，同时加强团体内的社会关系。主持人需要能够察觉参与者的脱离、个人目标设定困难以及人际交往问题等需要干预的微妙动态。然而，主持人面临的挑战和认知负荷创造了对具身技术的需求，这种技术应能解读社会交流并意识到个体需求，而非仅依赖'黑盒'模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种社交机器人共同主持人，能够分析多模态会议数据，并向人类主持人提供微妙提示，帮助主持人在团体会议中更好地识别需要干预的情况。&lt;h4&gt;方法&lt;/h4&gt;机器人使用代理概念瓶颈模型(CBM)进行推理，该模型基于人类可解释的概念（如参与者的参与度和情感）做出决策。核心贡献是一种迁移学习框架，将基础模型(FM)的广泛社会理解提炼到专门的、透明的CBM中。&lt;h4&gt;主要发现&lt;/h4&gt;概念驱动的系统在预测干预需求方面明显优于直接零样本FMs，并支持实时推理修正。该模型能够泛化到不同团体，并成功将资深主持人的专业知识迁移以提高新手的性能。&lt;h4&gt;结论&lt;/h4&gt;通过将专家的认知模型转移到可解释的机器人伙伴中，这项工作为增强复杂社会领域中人类能力提供了强大的蓝图。&lt;h4&gt;翻译&lt;/h4&gt;成功的团体会议，如团体行为改变项目、工作会议和其他社会背景中实施的会议，必须促进个人目标设定和执行，同时加强团体内的社会关系。因此，理想的主持人必须对脱离参与、个人目标设定和执行困难以及人际交往问题等需要干预的微妙动态保持敏感。主持人面临的挑战和认知负荷创造了对具身技术的关键需求，这种技术能够解读社会交流，同时意识到团体中个体的需求，并提供超越仅识别社会线索的强大但'黑盒'基础模型(FMs)的透明建议。我们通过一个社交机器人共同主持人来解决这一重要需求，该机器人分析多模态会议数据并向主持人提供微妙提示。机器人的推理由代理概念瓶颈模型(CBM)提供动力，该模型基于人类可解释的概念（如参与者的参与度和情感）做出决策，确保透明度和可信度。我们的核心贡献是一种迁移学习框架，将FM的广泛社会理解提炼到我们专门的、透明的CBM中。这种概念驱动的系统在预测干预需求方面明显优于直接零样本FMs，并支持对其推理的实时人工修正。关键性地，我们展示了稳健的知识迁移：该模型能够泛化到不同团体，并成功将资深人类主持人的专业知识迁移以提高新手的性能。通过将专家的认知模型转移到可解释的机器人伙伴中，我们的工作为增强复杂社会领域中人类能力提供了强大的蓝图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Successful group meetings, such as those implemented in groupbehavioral-change programs, work meetings, and other social contexts, mustpromote individual goal setting and execution while strengthening the socialrelationships within the group. Consequently, an ideal facilitator must besensitive to the subtle dynamics of disengagement, difficulties with individualgoal setting and execution, and interpersonal difficulties that signal a needfor intervention. The challenges and cognitive load experienced by facilitatorscreate a critical gap for an embodied technology that can interpret socialexchanges while remaining aware of the needs of the individuals in the groupand providing transparent recommendations that go beyond powerful but "blackbox" foundation models (FMs) that identify social cues. We address thisimportant demand with a social robot co-facilitator that analyzes multimodalmeeting data and provides discreet cues to the facilitator. The robot'sreasoning is powered by an agentic concept bottleneck model (CBM), which makesdecisions based on human-interpretable concepts like participant engagement andsentiments, ensuring transparency and trustworthiness. Our core contribution isa transfer learning framework that distills the broad social understanding ofan FM into our specialized and transparent CBM. This concept-driven systemsignificantly outperforms direct zero-shot FMs in predicting the need forintervention and enables real-time human correction of its reasoning.Critically, we demonstrate robust knowledge transfer: the model generalizesacross different groups and successfully transfers the expertise of seniorhuman facilitators to improve the performance of novices. By transferring anexpert's cognitive model into an interpretable robotic partner, our workprovides a powerful blueprint for augmenting human capabilities in complexsocial domains.</description>
      <author>example@mail.com (Xinyu Zhao, Zhen Tan, Maya Enisman, Minjae Seo, Marta R. Durantini, Dolores Albarracin, Tianlong Chen)</author>
      <guid isPermaLink="false">2508.03998v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation</title>
      <link>http://arxiv.org/abs/2508.03820v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  64 Pages, 9 Algorithms, 22 Theorems, 10 Lemmas, 2 Figures, 3 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Bernoulli-LoRA，一个统一并扩展现有LoRA方法的新理论框架，引入概率伯努利机制选择更新矩阵，分析了多种变体并证明其收敛性，通过实验验证了理论发现和方法有效性。&lt;h4&gt;背景&lt;/h4&gt;参数高效微调(PEFT)是适应大型基础模型到特定任务的关键方法，随着模型规模指数级增长而变得尤为重要。在PEFT方法中，LoRA因其有效性和简单性而突出，但现有方法缺乏充分的理论理解。&lt;h4&gt;目的&lt;/h4&gt;引入Bernoulli-LoRA，一个新的理论框架，统一并扩展现有的LoRA方法，同时保持理论上的可处理性。&lt;h4&gt;方法&lt;/h4&gt;引入一个概率性的伯努利机制来选择要更新的矩阵，这种方法包含了并推广了各种现有的更新策略，同时保持了理论上的可处理性。&lt;h4&gt;主要发现&lt;/h4&gt;在非凸优化文献的标准假设下，分析了框架的多个变体，为每个变体建立了收敛保证。此外，还将分析扩展到凸非光滑函数，为常数步长和自适应步长提供了收敛率。&lt;h4&gt;结论&lt;/h4&gt;通过各种任务的广泛实验验证了理论发现，并证明了该方法的有效性。这项工作是开发有理论基础且在实践中有效的PEFT方法的重要一步。&lt;h4&gt;翻译&lt;/h4&gt;参数高效微调(PEFT)已成为将大型基础模型适应到特定任务的关键方法，尤其随着模型规模继续呈指数级增长时。在PEFT方法中，低秩适应(LoRA)因其有效性和简单性而脱颖而出，将适应表示为两个低秩矩阵的乘积。尽管大量实证研究证明了LoRA的实际效用，但对这类方法的理论理解仍然有限。最近关于RAC-LoRA的研究(arXiv:2410.08305)为严格分析迈出了初步步伐。在这项工作中，我们介绍了Bernoulli-LoRA，这是一个统一并扩展现有LoRA方法的新理论框架。我们的方法引入了一个概率性的伯努利机制来选择要更新的矩阵。这种方法包含了并推广了各种现有的更新策略，同时保持了理论上的可处理性。在非凸优化文献的标准假设下，我们分析了框架的几种变体：Bernoulli-LoRA-GD、Bernoulli-LoRA-SGD、Bernoulli-LoRA-PAGE、Bernoulli-LoRA-MVR、Bernoulli-LoRA-QGD、Bernoulli-LoRA-MARINA和Bernoulli-LoRA-EF21，为每个变体建立了收敛保证。此外，我们将分析扩展到凸非光滑函数，为常数步长和自适应(Polyak-type)步长提供了收敛率。通过各种任务的广泛实验，我们验证了理论发现并证明了我们方法的有效性。这项工作是开发有理论基础且在实践中有效的PEFT方法的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach foradapting large foundational models to specific tasks, particularly as modelsizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,expressing adaptations as a product of two low-rank matrices. While extensiveempirical studies demonstrate LoRA's practical utility, theoreticalunderstanding of such methods remains limited. Recent work on RAC-LoRA(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,we introduce Bernoulli-LoRA, a novel theoretical framework that unifies andextends existing LoRA approaches. Our method introduces a probabilisticBernoulli mechanism for selecting which matrix to update. This approachencompasses and generalizes various existing update strategies whilemaintaining theoretical tractability. Under standard assumptions fromnon-convex optimization literature, we analyze several variants of ourframework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, andBernoulli-LoRA-EF21, establishing convergence guarantees for each variant.Additionally, we extend our analysis to convex non-smooth functions, providingconvergence rates for both constant and adaptive (Polyak-type) stepsizes.Through extensive experiments on various tasks, we validate our theoreticalfindings and demonstrate the practical efficacy of our approach. This work is astep toward developing theoretically grounded yet practically effective PEFTmethods.</description>
      <author>example@mail.com (Igor Sokolov, Abdurakhmon Sadiev, Yury Demidovich, Fawaz S Al-Qahtani, Peter Richtárik)</author>
      <guid isPermaLink="false">2508.03820v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons</title>
      <link>http://arxiv.org/abs/2508.03785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 7 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了SoilNet，一种多模态多任务模型，用于解决土壤层分类这一具有挑战性的问题。该方法通过整合图像数据和地理时空元数据，采用结构化模块化管道，能够处理复杂的层次标签结构，对土壤健康监测具有重要意义。&lt;h4&gt;背景&lt;/h4&gt;基础模型虽在多领域取得进展，但经验科学中的某些问题如土壤层分类仍难以受益。土壤层分类因其多模态、多任务特性和复杂的层次结构标签分类法而具有挑战性。准确的土壤层分类对监测土壤健康至关重要，直接影响农业生产力、粮食安全、生态系统稳定性和气候韧性。&lt;h4&gt;目的&lt;/h4&gt;开发一种多模态多任务模型来解决土壤层分类问题，特别是处理其复杂的层次结构标签特性。&lt;h4&gt;方法&lt;/h4&gt;提出了SoilNet模型，通过结构化模块化管道解决问题：1)整合图像数据和地理时空元数据；2)预测深度标记将土壤剖面分割成层候选；3)为每个segment提取层特定形态特征；4)基于多模态连接特征向量预测层标签；5)利用基于图的标签表示处理土壤层间的复杂层次关系。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界土壤剖面数据集上验证了所提方法的有效性，能够处理标签数量大、不平衡且结构复杂的层次分类问题。&lt;h4&gt;结论&lt;/h4&gt;SoilNet为土壤层分类提供了有效解决方案，所有代码和实验已开源在GitHub仓库中。&lt;h4&gt;翻译&lt;/h4&gt;尽管基础模型最近的进展已在许多领域提高了最先进水平，但经验科学中的一些问题尚未能从这一进展中受益。例如，土壤层分类仍然具有挑战性，因为它具有多模态、多任务特性以及复杂的层次结构标签分类法。准确的土壤层分类对监测土壤健康至关重要，直接影响农业生产力、粮食安全、生态系统稳定性和气候韧性。在这项工作中，我们提出了SoilNet——一种多模态多任务模型，通过结构化模块化管道解决这个问题。我们的方法整合图像数据和地理时空元数据，首先预测深度标记，将土壤剖面分割成层候选。每个segment由一组特定于层的形态特征表征。最后，基于多模态连接的特征向量预测层标签，利用基于图的标签表示来考虑土壤层之间的复杂层次关系。我们的方法旨在解决复杂的层次分类问题，其中可能的标签数量非常大、不平衡且结构复杂。我们在真实世界土壤剖面数据集上证明了我们方法的有效性。所有代码和实验都可以在我们的GitHub仓库中找到：https://github.com/calgo-lab/BGR/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While recent advances in foundation models have improved the state of the artin many domains, some problems in empirical sciences could not benefit fromthis progress yet. Soil horizon classification, for instance, remainschallenging because of its multimodal and multitask characteristics and acomplex hierarchically structured label taxonomy. Accurate classification ofsoil horizons is crucial for monitoring soil health, which directly impactsagricultural productivity, food security, ecosystem stability and climateresilience. In this work, we propose $\textit{SoilNet}$ - a multimodalmultitask model to tackle this problem through a structured modularizedpipeline. Our approach integrates image data and geotemporal metadata to firstpredict depth markers, segmenting the soil profile into horizon candidates.Each segment is characterized by a set of horizon-specific morphologicalfeatures. Finally, horizon labels are predicted based on the multimodalconcatenated feature vector, leveraging a graph-based label representation toaccount for the complex hierarchical relationships among soil horizons. Ourmethod is designed to address complex hierarchical classification, where thenumber of possible labels is very large, imbalanced and non-triviallystructured. We demonstrate the effectiveness of our approach on a real-worldsoil profile dataset. All code and experiments can be found in our repository:https://github.com/calgo-lab/BGR/</description>
      <author>example@mail.com (Teodor Chiaburu, Vipin Singh, Frank Haußer, Felix Bießmann)</author>
      <guid isPermaLink="false">2508.03785v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2508.04059v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了O-Bench，首个专门针对遮挡感知的视觉问答基准测试，通过分层合成方法创建了1,365张图像和4,588个问答对，评估显示当前多模态大语言模型在遮挡感知方面与人类表现存在显著差距，并识别出三种典型失败模式。&lt;h4&gt;背景&lt;/h4&gt;遮挡感知是人类水平空间理解的关键基础，它整合了视觉识别和推理的挑战。尽管多模态大语言模型已显示出显著能力，但它们在遮挡感知方面的表现尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;引入O-Bench，第一个专门为遮挡感知设计的视觉问答基准测试，以解决当前多模态大语言模型在遮挡感知方面表现不足的问题。&lt;h4&gt;方法&lt;/h4&gt;基于SA-1B数据集，通过新颖的分层合成方法构建1,365张具有语义连贯遮挡场景的图像，并采用可靠、半自动的工作流程标注了涵盖五个定制任务的4,588个问答对，随后对22个代表性多模态大语言模型进行了广泛评估并与人类基线进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;当前多模态大语言模型与人类之间存在显著性能差距，这种差距不能通过模型扩展或思考过程充分弥补。研究识别出三种典型失败模式：过于保守的偏见、脆弱的格式塔预测以及在定量任务上的挣扎。&lt;h4&gt;结论&lt;/h4&gt;O-Bench不仅可以为遮挡感知提供重要的评估工具，还能启发多模态大语言模型的发展以实现更好的视觉智能，该基准测试将在论文发表后公开提供。&lt;h4&gt;翻译&lt;/h4&gt;遮挡感知，作为人类水平空间理解的关键基础，体现了整合视觉识别和推理的挑战。尽管多模态大语言模型已显示出显著能力，但它们在遮挡感知方面的表现尚未被充分探索。为解决这一差距，我们引入了O-Bench，这是第一个专门为遮挡感知设计的视觉问答基准测试。基于SA-1B，我们通过一种新颖的分层合成方法构建了1,365张具有语义连贯遮挡场景的图像。在此基础上，我们采用可靠、半自动的工作流程，总共标注了涵盖五个定制任务的4,588个问答对。我们对22个代表性多模态大语言模型进行的广泛评估，与人类基线相比，揭示了当前多模态大语言模型与人类之间的显著性能差距，我们发现这种差距不能通过模型扩展或思考过程充分弥补。我们进一步识别出三种典型的失败模式，包括过于保守的偏见、脆弱的格式塔预测以及在定量任务上的挣扎。我们相信O-Bench不仅可以为遮挡感知提供重要的评估工具，还能启发多模态大语言模型的发展以实现更好的视觉智能。我们的基准测试将在论文发表后公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Occlusion perception, a critical foundation for human-level spatialunderstanding, embodies the challenge of integrating visual recognition andreasoning. Though multimodal large language models (MLLMs) have demonstratedremarkable capabilities, their performance on occlusion perception remainsunder-explored. To address this gap, we introduce O-Bench, the first visualquestion answering (VQA) benchmark specifically designed for occlusionperception. Based on SA-1B, we construct 1,365 images featuring semanticallycoherent occlusion scenarios through a novel layered synthesis approach. Uponthis foundation, we annotate 4,588 question-answer pairs in total across fivetailored tasks, employing a reliable, semi-automatic workflow. Our extensiveevaluation of 22 representative MLLMs against the human baseline reveals asignificant performance gap between current MLLMs and humans, which, we find,cannot be sufficiently bridged by model scaling or thinking process. We furtheridentify three typical failure patterns, including an overly conservative bias,a fragile gestalt prediction, and a struggle with quantitative tasks. Webelieve O-Bench can not only provide a vital evaluation tool for occlusionperception, but also inspire the development of MLLMs for better visualintelligence. Our benchmark will be made publicly available upon paperpublication.</description>
      <author>example@mail.com (Zhaochen Liu, Kaiwen Gao, Shuyi Liang, Bin Xiao, Limeng Qiao, Lin Ma, Tingting Jiang)</author>
      <guid isPermaLink="false">2508.04059v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2508.03060v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CHARM的新型互补学习框架，用于模态不可知语义分割，旨在实现跨模态内容的协同调和而非同质化，从而保留各模态的独特优势并实现互补融合。&lt;h4&gt;背景&lt;/h4&gt;现有的模态不可知语义分割方法通常依赖于显式特征对齐来实现模态同质化，这会稀释各模态的独特优势并破坏它们固有的互补性。&lt;h4&gt;目的&lt;/h4&gt;实现协同调和而非同质化，通过隐式对齐内容同时保留模态特定优势，从而实现跨模态互补，达到真正的多样性和谐。&lt;h4&gt;方法&lt;/h4&gt;CHARM框架包含两个主要组件：1）互感单元（MPU），通过基于窗口的跨模态交互实现隐式对齐，使各模态互为查询和上下文；2）双路径优化策略，将训练解耦为互补融合学习的协作学习策略（CoL）和保护模态特定优化的个体增强策略（InE）。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集和骨干网络上的实验表明，CHARM始终优于基线方法，并且在脆弱模态上有显著提升。&lt;h4&gt;结论&lt;/h4&gt;这项工作将重点从模型同质化转向调和，实现了跨模态互补，达到真正的多样性和谐。&lt;h4&gt;翻译&lt;/h4&gt;模态不可知语义分割（MaSS）旨在实现跨任意输入模态组合的鲁棒场景理解。现有方法通常依赖于显式特征对齐来实现模态同质化，这稀释了各模态的独特优势并破坏了它们固有的互补性。为了实现协同调和而非同质化，我们提出了CHARM，一种新型互补学习框架，通过两个组件隐式对齐内容同时保留模态特定优势：（1）互感单元（MPU），通过基于窗口的跨模态交互实现隐式对齐，使各模态互为查询和上下文，发现模态交互对应关系；（2）双路径优化策略，将训练解耦为互补融合学习的协作学习策略（CoL）和保护模态特定优化的个体增强策略（InE）。在多个数据集和骨干网络上的实验表明，CHARM始终优于基线方法，并且在脆弱模态上有显著提升。这项工作将重点从模型同质化转向调和，实现了跨模态互补，达到真正的多样性和谐。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决模态无关语义分割（MaSS）中的模态同质化问题。现有方法通过显式特征对齐实现模态一致性，但这会稀释各模态的独特优势，破坏它们固有的互补性。这个问题在现实中很重要，因为不同传感器（如RGB、LiDAR、深度相机等）在不同环境条件下各有优势（如雨天RGB可能退化而LiDAR仍有效），当某些传感器不可用或降级时，系统需要利用可用模态组合保持鲁棒性能，而现有方法无法充分利用模态间的互补信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有MaSS方法的局限性：显式特征对齐虽然能处理稳健模态组合，但会抑制脆弱模态特征并减弱模态间互补性。他们提出从'同质化'转向'和谐化'的范式，强调模态间的互补合作而非一致性。设计上借鉴了多模态语义分割领域的特征融合和注意力机制，特别是改进了Transformer中的自注意力机制以支持跨模态交互（MPU模块），并受特征对齐思想启发设计了双路径优化策略（CoL和InE），但将它们解耦并协同工作，实现模态互补与个体增强的平衡。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是实现'协作和谐化'而非'同质化'，通过互感单元（MPU）实现模态间隐式对齐，同时采用双路径优化策略平衡协作学习和个体增强。整体流程为：1)共享权重编码器独立提取各模态特征；2)双路径处理：协作学习路径（CoL）通过鲁棒性评估加权融合并经MPU跨模态交互，个体增强路径（InE）为脆弱模态提供保护性学习空间；3)两条路径特征输入分割头并行预测；4)使用交叉熵损失进行联合优化；5)MPU通过窗口式跨模态注意力使各模态同时作为查询和上下文，实现隐式对齐。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有：1)从同质化到和谐化的范式转变，保留各模态独特优势同时实现互补；2)互感单元（MPU）实现隐式模态对齐，无需显式约束；3)双路径优化策略（CoL促进模态互补，InE保护脆弱模态）。相比之前工作：区别于显式对齐方法（如MAGIC、Any2Seg）不使用KL散度等约束强制特征一致性；区别于传统多模态分割方法不依赖预设主-从模态关系；区别于模态丢弃策略不仅模拟缺失场景还主动增强脆弱模态表现。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CHARM通过引入互感单元和双路径优化策略，实现了跨模态的协作和谐化而非同质化，在保留各模态独特优势的同时显著提升了模态无关语义分割的性能，特别是在脆弱模态组合上的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modality-agnostic Semantic Segmentation (MaSS) aims to achieve robust sceneunderstanding across arbitrary combinations of input modality. Existing methodstypically rely on explicit feature alignment to achieve modal homogenization,which dilutes the distinctive strengths of each modality and destroys theirinherent complementarity. To achieve cooperative harmonization rather thanhomogenization, we propose CHARM, a novel complementary learning frameworkdesigned to implicitly align content while preserving modality-specificadvantages through two components: (1) Mutual Perception Unit (MPU), enablingimplicit alignment through window-based cross-modal interaction, wheremodalities serve as both queries and contexts for each other to discovermodality-interactive correspondences; (2) A dual-path optimization strategythat decouples training into Collaborative Learning Strategy (CoL) forcomplementary fusion learning and Individual Enhancement Strategy (InE) forprotected modality-specific optimization. Experiments across multiple datasetsand backbones indicate that CHARM consistently outperform the baselines, withsignificant increment on the fragile modalities. This work shifts the focusfrom model homogenization to harmonization, enabling cross-modalcomplementarity for true harmony in diversity.</description>
      <author>example@mail.com (Lekang Wen, Jing Xiao, Liang Liao, Jiajun Chen, Mi Wang)</author>
      <guid isPermaLink="false">2508.03060v2</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>A Large Language Model Powered Integrated Circuit Footprint Geometry Understanding</title>
      <link>http://arxiv.org/abs/2508.03725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LLM4-IC8K的新型框架，用于从IC机械图纸自动进行封装几何标记。该方法将IC机械图纸视为图像，利用大型语言模型进行结构化几何解释，通过两阶段训练和专门的数据集支持，解决了当前大型多模态模型在几何感知方面的不准确问题。&lt;h4&gt;背景&lt;/h4&gt;集成电路的印刷电路板引脚几何形状标记对于定义组件与PCB布局之间的物理接口至关重要。然而，由于引脚绘制结构化和抽象图表标注的存在，自动解析和精确的几何建模仍然极具挑战性。目前尚无直接从IC机械图纸自动进行封装几何标记的方法。&lt;h4&gt;目的&lt;/h4&gt;研究大型多模态模型在解决IC引脚几何理解方面的视觉感知性能，解决当前LMMs在几何感知不准确的问题，并提出一个有效的框架来自动处理IC封装几何标记。&lt;h4&gt;方法&lt;/h4&gt;提出LLM4-IC8K框架，将IC机械图纸视为图像，利用LLMs进行结构化几何解释。模仿工程师的逐步推理方法，解决三个子任务：感知引脚数量、计算每个引脚的中心坐标、估计单个引脚的尺寸。采用两阶段框架：首先在合成的IC引脚图上训练LMMs学习基本几何推理，然后在真实数据图纸上进行微调。引入了ICGeo8K数据集，包含8,608个标记样本。&lt;h4&gt;主要发现&lt;/h4&gt;当前LMMs在几何感知方面存在严重的不准确性，这阻碍了它们在解决引脚几何标记问题上的性能。通过实验证明，提出的模型在提出的基准测试中优于最先进的LMMs。&lt;h4&gt;结论&lt;/h4&gt;LLM4-IC8K框架能够有效解决IC封装几何标记问题。通过两阶段训练和专门的数据集支持，该方法提高了模型在实际场景中的鲁棒性和准确性。&lt;h4&gt;翻译&lt;/h4&gt;印刷电路板集成电路的引脚几何形状标记对于定义组件与PCB布局之间的物理接口至关重要，需要卓越的视觉感知能力。然而，由于引脚绘制结构化和抽象图表标注的存在，自动解析和精确的引脚几何建模仍然极具挑战性。尽管其重要性，但目前尚无直接从IC机械图纸自动进行封装几何标记的方法。在本文中，我们首先研究了大型多模态模型在解决IC引脚几何理解时的视觉感知性能。我们的发现表明，当前的LMMs在几何感知方面存在严重的不准确性，这阻碍了它们在解决引脚几何标记问题上的性能。为解决这些局限性，我们提出了LLM4-IC8K，一种将IC机械图纸视为图像并利用LLMs进行结构化几何解释的新型框架。为模仿工程师使用的逐步推理方法，LLM4-IC8K解决了三个子任务：感知引脚数量、计算每个引脚的中心坐标和估计单个引脚的尺寸。我们提出了一个两阶段框架，首先在合成的IC引脚图上训练LMMs学习基本几何推理，然后在真实数据图纸上进行微调，以提高实际场景中的鲁棒性和准确性。为此，我们引入了ICGeo8K，一个包含8,608个标记样本的多模态数据集，包括4,138个手工制作的IC引脚样本和4,470个合成生成的样本。大量实验证明，我们的模型在提出的基准测试中优于最先进的LMMs。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Printed-Circuit-board (PCB) footprint geometry labeling of integratedcircuits (IC) is essential in defining the physical interface betweencomponents and the PCB layout, requiring exceptional visual perceptionproficiency. However, due to the unstructured footprint drawing and abstractdiagram annotations, automated parsing and accurate footprint geometry modelingremain highly challenging. Despite its importance, no methods currently existfor automated package geometry labeling directly from IC mechanical drawings.In this paper, we first investigate the visual perception performance of LargeMultimodal Models (LMMs) when solving IC footprint geometry understanding. Ourfindings reveal that current LMMs severely suffer from inaccurate geometricperception, which hinders their performance in solving the footprint geometrylabeling problem. To address these limitations, we propose LLM4-IC8K, a novelframework that treats IC mechanical drawings as images and leverages LLMs forstructured geometric interpretation. To mimic the step-by-step reasoningapproach used by human engineers, LLM4-IC8K addresses three sub-tasks:perceiving the number of pins, computing the center coordinates of each pin,and estimating the dimensions of individual pins. We present a two-stageframework that first trains LMMs on synthetically generated IC footprintdiagrams to learn fundamental geometric reasoning and then fine-tunes them onreal-world datasheet drawings to enhance robustness and accuracy in practicalscenarios. To support this, we introduce ICGeo8K, a multi-modal dataset with8,608 labeled samples, including 4138 hand-crafted IC footprint samples and4470 synthetically generated samples. Extensive experiments demonstrate thatour model outperforms state-of-the-art LMMs on the proposed benchmark.</description>
      <author>example@mail.com (Yida Wang, Taiting Lu, Runze Liu, Lanqing Yang, Yifan Yang, Zhe Chen, Yuehai Wang, Yixin Liu, Kaiyuan Lin, Xiaomeng Chen, Dian Ding, Yijie Li, Yi-Chao Chen, Yincheng Jin, Mahanth Gowda)</author>
      <guid isPermaLink="false">2508.03725v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning</title>
      <link>http://arxiv.org/abs/2508.04549v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ACMMM2025 (Dataset track)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种面向海洋对象的两阶段视频字幕生成流程，解决了海洋视频理解中的挑战，并发布了相关数据集和代码。&lt;h4&gt;背景&lt;/h4&gt;海洋视频因海洋物体动态变化、相机运动和水下场景复杂性而给视频理解带来显著挑战。现有视频字幕数据集通常专注于通用或以人为中心的领域，难以推广到海洋环境的复杂性中，也无法有效分析海洋生物。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频字幕方法在海洋环境中的局限性，提出专门针对海洋视频理解的解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出了一种全面的视频理解基准，利用视频、文本和分割掩码的三元组来促进视觉定位和字幕生成。同时采用视频分割技术检测场景变化中的显著物体转换，以丰富字幕内容的语义。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法改善了海洋视频理解和分析能力，促进了海洋视频生成的发展，视频分割技术对检测场景变化中的显著物体转换非常有效。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的基准和方法论，有效提升了海洋视频理解的质量，相关数据集和代码已在https://msc.hkustvgd.com上发布。&lt;h4&gt;翻译&lt;/h4&gt;海洋视频由于海洋物体和周围环境的动态变化、相机运动以及水下场景的复杂性，给视频理解带来了重大挑战。现有的视频字幕数据集通常专注于通用或以人为中心的领域，往往无法推广到海洋环境的复杂性中，也无法获得关于海洋生活的洞察。为解决这些局限性，我们提出了一种两阶段的面向海洋对象的视频字幕生成流程。我们引入了一个全面的视频理解基准，利用视频、文本和分割掩码的三元组来促进视觉定位和字幕生成，从而改善海洋视频理解和分析，以及海洋视频生成。此外，我们强调了视频分割在检测场景变化中显著物体转换方面的有效性，这显著丰富了字幕内容的语义。我们的数据集和代码已在https://msc.hkustvgd.com上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758198&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Marine videos present significant challenges for video understanding due tothe dynamics of marine objects and the surrounding environment, camera motion,and the complexity of underwater scenes. Existing video captioning datasets,typically focused on generic or human-centric domains, often fail to generalizeto the complexities of the marine environment and gain insights about marinelife. To address these limitations, we propose a two-stage marineobject-oriented video captioning pipeline. We introduce a comprehensive videounderstanding benchmark that leverages the triplets of video, text, andsegmentation masks to facilitate visual grounding and captioning, leading toimproved marine video understanding and analysis, and marine video generation.Additionally, we highlight the effectiveness of video splitting in order todetect salient object transitions in scene changes, which significantly enrichthe semantics of captioning content. Our dataset and code have been released athttps://msc.hkustvgd.com.</description>
      <author>example@mail.com (Quang-Trung Truong, Yuk-Kwan Wong, Vo Hoang Kim Tuyen Dang, Rinaldi Gotama, Duc Thanh Nguyen, Sai-Kit Yeung)</author>
      <guid isPermaLink="false">2508.04549v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Quantum and Classical Sequential Models for Urban Telecommunication Forecasting</title>
      <link>http://arxiv.org/abs/2508.04488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了经典和量子启发的序列模型在预测米兰电信数据集中的单变量SMS活动时间序列的性能，比较了LSTM和四种量子模型在不同序列长度下的表现，发现量子增强并非普遍有利，其效果依赖于具体任务和架构设计。&lt;h4&gt;背景&lt;/h4&gt;研究使用米兰电信活动数据集，但由于数据完整性限制，仅关注每个空间网格单元的SMS-in信号。时间序列预测是电信网络管理和资源分配的重要任务。&lt;h4&gt;目的&lt;/h4&gt;评估不同序列模型（包括经典LSTM和量子启发模型）在预测SMS活动方面的性能，并研究序列长度对模型表现的影响，以及量子增强的有效性条件。&lt;h4&gt;方法&lt;/h4&gt;比较了五种模型：LSTM（基线）、Quantum LSTM (QLSTM)、Quantum Adaptive Self-Attention (QASA)、Quantum Receptance Weighted Key-Value (QRWKV)和Quantum Fast Weight Programmers (QFWP)。实验在不同输入序列长度（4, 8, 12, 16, 32和64）下进行，所有模型都基于历史值预测下一个10分钟的SMS-in值。&lt;h4&gt;主要发现&lt;/h4&gt;不同模型对序列长度表现出不同的敏感性，量子增强并非在所有情况下都优于经典模型，量子模块的有效性高度依赖于特定任务和架构设计，反映了模型大小、参数化策略和时间建模能力之间的权衡关系。&lt;h4&gt;结论&lt;/h4&gt;量子增强序列模型的效果不是普遍的，而是与具体应用场景和模型架构密切相关。在选择量子增强模型时，需要考虑任务特性、序列长度需求和模型复杂度之间的平衡。&lt;h4&gt;翻译&lt;/h4&gt;在本研究中，我们评估了经典和量子启发的序列模型在预测米兰电信活动数据集中的单变量传入短信活动（SMS-in）时间序列的性能。由于数据完整性限制，我们仅专注于每个空间网格单元的SMS-in信号。我们在不同的输入序列长度（4、8、12、16、32和64）下比较了五种模型：LSTM（基线）、量子LSTM（QLSTM）、量子自适应自注意力（QASA）、量子感受加权键值（QRWKV）和量子快速权重编程器（QFWP）。所有模型都经过训练，仅基于给定序列窗口内的历史值来预测下一个10分钟的SMS-in值。我们的研究结果表明，不同模型对序列长度表现出不同的敏感性，这表明量子增强并非普遍有利。相反，量子模块的有效性高度依赖于特定任务和架构设计，反映了模型大小、参数化策略和时间建模能力之间的固有权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we evaluate the performance of classical and quantum-inspiredsequential models in forecasting univariate time series of incoming SMSactivity (SMS-in) using the Milan Telecommunication Activity Dataset. Due todata completeness limitations, we focus exclusively on the SMS-in signal foreach spatial grid cell. We compare five models, LSTM (baseline), Quantum LSTM(QLSTM), Quantum Adaptive Self-Attention (QASA), Quantum Receptance WeightedKey-Value (QRWKV), and Quantum Fast Weight Programmers (QFWP), under varyinginput sequence lengths (4, 8, 12, 16, 32 and 64). All models are trained topredict the next 10-minute SMS-in value based solely on historical valueswithin a given sequence window. Our findings indicate that different modelsexhibit varying sensitivities to sequence length, suggesting that quantumenhancements are not universally advantageous. Rather, the effectiveness ofquantum modules is highly dependent on the specific task and architecturaldesign, reflecting inherent trade-offs among model size, parameterizationstrategies, and temporal modeling capabilities.</description>
      <author>example@mail.com (Chi-Sheng Chen, Samuel Yen-Chi Chen, Yun-Cheng Tsai)</author>
      <guid isPermaLink="false">2508.04488v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning</title>
      <link>http://arxiv.org/abs/2508.04416v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VITAL框架，通过工具增强学习提升多模态大语言模型的长视频推理能力，解决了现有方法中跨模态交互有限和幻觉增加的问题。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)的视频推理能力对视频问答和时间定位等下游任务至关重要，但现有基于文本的思维链推理方法存在跨模态交互有限和幻觉增加的问题，特别是在处理较长视频时。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频推理方法中的跨模态交互有限和幻觉增加问题，提升模型在长视频场景下的推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出Video Intelligence via Tool-Augmented Learning (VITAL)框架，使用视觉工具箱按需密集采样视频帧并生成多模态思维链；构建MTVR-CoT-72k和MTVR-RL-110k两个多任务数据集；提出Difficulty-aware Group Relative Policy Optimization算法(DGRPO)缓解多任务强化学习中的难度不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;时间定位和问答任务对视频理解具有相互促进作用。&lt;h4&gt;结论&lt;/h4&gt;在11个视频理解基准测试上，VITAL展现出先进的推理能力，在视频问答和时间定位任务上优于现有方法，特别是在长视频场景中。所有代码、数据和模型权重将公开提供。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)的视频推理能力对于视频问答和时间定位等下游任务至关重要。虽然最近的方法已经探索了基于文本的思维链(CoT)推理用于MLLMs，但这些方法通常受到跨模态交互有限和幻觉增加的限制，特别是在处理较长视频或推理链时。为了解决这些挑战，我们提出了通过工具增强学习的视频智能(VITAL)，这是一种新颖的端到端智能体视频推理框架。通过视觉工具箱，模型可以按需密集采样新的视频帧，并生成多模态思维链以进行精确的长视频推理。我们观察到时间定位和问答对视频理解任务有相互促进作用。因此，我们构建了两个高质量的多任务视频推理数据集MTVR-CoT-72k用于监督微调，MTVR-RL-110k用于强化学习。此外，我们提出了一个难度感知的组相对策略优化算法(DGRPO)来缓解多任务强化学习中的难度不平衡问题。在11个具有挑战性的视频理解基准上的广泛实验证明了VITAL的先进推理能力，在视频问答和时间定位任务上优于现有方法，特别是在长视频场景中。所有代码、数据和模型权重都将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The video reasoning ability of multimodal large language models (MLLMs) iscrucial for downstream tasks like video question answering and temporalgrounding. While recent approaches have explored text-based chain-of-thought(CoT) reasoning for MLLMs, these methods often suffer from limited cross-modalinteraction and increased hallucination, especially with longer videos orreasoning chains. To address these challenges, we propose Video Intelligencevia Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoningframework. With a visual toolbox, the model can densely sample new video frameson demand and generate multimodal CoT for precise long video reasoning. Weobserve that temporal grounding and question answering are mutually beneficialfor video understanding tasks. Therefore, we construct two high-qualitymulti-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning andMTVR-RL-110k for reinforcement learning. Moreover, we propose aDifficulty-aware Group Relative Policy Optimization algorithm (DGRPO) tomitigate difficulty imbalance in multi-task reinforcement learning. Extensiveexperiments on 11 challenging video understanding benchmarks demonstrate theadvanced reasoning ability of VITAL, outperforming existing methods in videoquestion answering and temporal grounding tasks, especially in long videoscenarios. All code, data and model weight will be made publicly available.</description>
      <author>example@mail.com (Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, Yansong Tang)</author>
      <guid isPermaLink="false">2508.04416v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding</title>
      <link>http://arxiv.org/abs/2508.04369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为时间采样策略优化(TSPO)的新方法，通过强化学习提升多模态大语言模型(MLLMs)对长时间视频的理解能力。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在视觉语言任务中已显示出显著进展，但在处理长时间视频输入时仍面临挑战。这种限制源于MLLMs的上下文限制和训练成本，需要在将视频输入MLLMs之前进行稀疏帧采样。现有的视频MLLMs采用无训练的均匀采样或关键帧搜索，可能会错过关键事件或受预训练模型事件理解能力的限制。同时，由于稀疏帧采样的无监督和非可微分性质，构建基于训练的方法仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频MLLMs在处理长时间视频时的局限性，提出一种通过强化学习提升MLLMs长时间视频语言理解能力的方法。&lt;h4&gt;方法&lt;/h4&gt;提出时间采样策略优化(TSPO)方法：1. 提出可训练的事件感知时间智能体，捕捉事件-查询相关性以执行概率性关键帧选择；2. 提出TSPO强化学习范式，将关键帧选择和语言生成建模为联合决策过程，实现基于规则奖励的高效端到端组相对优化；3. 为TSPO训练提出包含全面时间数据和视频Needle-in-a-Haystack数据的长时间视频训练数据构建管道；4. 整合基于规则的回答准确性和时间定位奖励机制来优化时间采样策略。&lt;h4&gt;主要发现&lt;/h4&gt;全面的实验表明，TSPO在多个长时间视频理解基准测试中取得了最先进的性能，并且在不同的前沿视频-MLLMs中显示出可转移的能力。&lt;h4&gt;结论&lt;/h4&gt;TSPO方法有效解决了MLLMs处理长时间视频输入的挑战，通过强化学习和事件感知的时间采样策略，显著提升了模型对长时间视频的理解能力，并且该方法具有很好的通用性和可转移性。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)在视觉语言任务中已显示出显著进展，但在处理长时间视频输入时仍面临挑战。这种限制源于MLLMs的上下文限制和训练成本，需要在将视频输入MLLMs之前进行稀疏帧采样。现有的视频MLLMs采用无训练的均匀采样或关键帧搜索，可能会错过关键事件或受预训练模型事件理解能力的限制。同时，由于稀疏帧采样的无监督和非可微分性质，构建基于训练的方法仍然具有挑战性。为解决这些问题，我们提出时间采样策略优化(TSPO)，通过强化学习提升MLLMs的长时间视频语言理解能力。具体而言，我们首先提出一种可训练的事件感知时间智能体，它捕捉事件-查询相关性以执行概率性关键帧选择。然后，我们提出TSPO强化学习范式，将关键帧选择和语言生成建模为联合决策过程，实现基于规则奖励的高效端到端组相对优化。此外，为TSPO的训练，我们提出了一种包含全面时间数据和视频Needle-in-a-Haystack数据的长时间视频训练数据构建管道。最后，我们整合基于规则的回答准确性和时间定位奖励机制来优化时间采样策略。全面的实验表明，我们的TSPO在多个长时间视频理解基准测试中取得了最先进的性能，并且在不同的前沿视频-MLLMs中显示出可转移的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have demonstrated significantprogress in vision-language tasks, yet they still face challenges whenprocessing long-duration video inputs. The limitation arises from MLLMs'context limit and training costs, necessitating sparse frame sampling beforefeeding videos into MLLMs. Existing video MLLMs adopt training-free uniformsampling or keyframe search, which may miss critical events or be constrainedby the pre-trained models' event understanding capabilities. Meanwhile,building a training-based method remains challenging due to the unsupervisedand non-differentiable nature of sparse frame sampling. To address theseproblems, we propose Temporal Sampling Policy Optimization (TSPO), advancingMLLMs' long-form video-language understanding via reinforcement learning.Specifically, we first propose a trainable event-aware temporal agent, whichcaptures event-query correlation for performing probabilistic keyframeselection. Then, we propose the TSPO reinforcement learning paradigm, whichmodels keyframe selection and language generation as a joint decision-makingprocess, enabling end-to-end group relative optimization with efficientrule-based rewards. Furthermore, for the TSPO's training, we propose a longvideo training data construction pipeline with comprehensive temporal data andvideo Needle-in-a-Haystack data. Finally, we incorporate rule-based answeringaccuracy and temporal locating reward mechanisms to optimize the temporalsampling policy. Comprehensive experiments show that our TSPO achievesstate-of-the-art performance across multiple long video understandingbenchmarks, and shows transferable ability across different cutting-edgeVideo-MLLMs.</description>
      <author>example@mail.com (Canhui Tang, Zifan Han, Hongbo Sun, Sanping Zhou, Xuchong Zhang, Xin Wei, Ye Yuan, Jinglin Xu, Hao Sun)</author>
      <guid isPermaLink="false">2508.04369v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning</title>
      <link>http://arxiv.org/abs/2508.04043v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了VisualTrans，这是第一个专门为视觉转换推理(VTR)在真实人类-物体交互场景设计的全面基准测试，包含12种操作任务和472个高质量问答对，通过6种子任务类型评估三个推理维度。&lt;h4&gt;背景&lt;/h4&gt;视觉转换推理是智能体理解动态场景、建模因果关系和预测未来状态的重要认知能力，但现有基准测试存在模拟到现实的差距、任务复杂度有限和推理覆盖不完整等问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有基准测试的局限性，为真实世界场景中的视觉转换推理提供一个全面、高质量的评估工具。&lt;h4&gt;方法&lt;/h4&gt;构建了一个基于第一人称操作视频的可扩展数据流程，整合任务选择、图像对提取、自动化元数据注释和结构化问题生成，并通过人工验证确保质量和可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;最先进的视觉语言模型在静态空间任务中表现良好，但在动态、多步骤推理场景（如中间状态识别和转换序列规划）中存在明显不足，揭示了时间建模和因果推理的基本弱点。&lt;h4&gt;结论&lt;/h4&gt;研究结果为开发更强大和可推广的视觉转换推理系统提供了明确方向，相关数据集和代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;视觉转换推理是一种重要的认知能力，它使智能体能够理解动态场景、建模因果关系并预测未来状态，从而指导行动并为高级智能系统奠定基础。然而，现有的基准测试存在模拟到现实的差距、任务复杂度有限和推理覆盖不完整等问题，限制了它们在真实世界场景中的实际应用。为了解决这些局限性，我们引入了VisualTrans，这是第一个专门为真实人类-物体交互场景中的视觉转换推理设计的全面基准测试。VisualTrans包含12种语义多样的操作任务，并通过6种明确定义的子任务类型系统评估三个基本推理维度：空间、程序和定量。该基准测试包含472个高质量问答对，格式包括多项选择、开放式计数和目标列举。我们引入了一个可扩展的数据构建流程，基于第一人称操作视频，整合了任务选择、图像对提取、使用大型多模态模型的自动化元数据注释和结构化问题生成。人工验证确保了最终基准测试的高质量和可解释性。对各种最先进的视觉语言模型的评估显示，它们在静态空间任务中表现出色。然而，在动态、多步骤推理场景中，特别是在中间状态识别和转换序列规划方面，它们显示出明显的不足。这些发现揭示了时间建模和因果推理的基本弱点，为未来研究指明了明确方向，旨在开发更强大和可推广的视觉转换推理系统。数据集和代码可在https://github.com/WangYipu2002/VisualTrans获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual transformation reasoning (VTR) is a vital cognitive capability thatempowers intelligent agents to understand dynamic scenes, model causalrelationships, and predict future states, and thereby guiding actions andlaying the foundation for advanced intelligent systems. However, existingbenchmarks suffer from a sim-to-real gap, limited task complexity, andincomplete reasoning coverage, limiting their practical use in real-worldscenarios. To address these limitations, we introduce VisualTrans, the firstcomprehensive benchmark specifically designed for VTR in real-worldhuman-object interaction scenarios. VisualTrans encompasses 12 semanticallydiverse manipulation tasks and systematically evaluates three essentialreasoning dimensions - spatial, procedural, and quantitative - through 6well-defined subtask types. The benchmark features 472 high-qualityquestion-answer pairs in various formats, including multiple-choice, open-endedcounting, and target enumeration. We introduce a scalable data constructionpipeline built upon first-person manipulation videos, which integrates taskselection, image pair extraction, automated metadata annotation with largemultimodal models, and structured question generation. Human verificationensures the final benchmark is both high-quality and interpretable. Evaluationsof various state-of-the-art vision-language models show strong performance instatic spatial tasks. However, they reveal notable shortcomings in dynamic,multi-step reasoning scenarios, particularly in areas like intermediate staterecognition and transformation sequence planning. These findings highlightfundamental weaknesses in temporal modeling and causal reasoning, providingclear directions for future research aimed at developing more capable andgeneralizable VTR systems. The dataset and code are available athttps://github.com/WangYipu2002/VisualTrans.</description>
      <author>example@mail.com (Yuheng Ji, Yipu Wang, Yuyang Liu, Xiaoshuai Hao, Yue Liu, Yuting Zhao, Huaihai Lyu, Xiaolong Zheng)</author>
      <guid isPermaLink="false">2508.04043v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>From Waveforms to Pixels: A Survey on Audio-Visual Segmentation</title>
      <link>http://arxiv.org/abs/2508.03724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是对视听分割领域的全面综述，涵盖了问题定义、数据集、评估指标和方法进展，分析了各种架构和训练范式，并指出了当前挑战和未来方向。&lt;h4&gt;背景&lt;/h4&gt;视听分割是多模态感知领域的重要研究方向，旨在通过结合视觉和音频信息来识别和分割视频中产生声音的物体，实现细粒度的物体级理解。&lt;h4&gt;目的&lt;/h4&gt;提供视听分割领域的全面概述，分析现有方法和训练范式，比较不同方法的性能，并指出当前挑战和未来发展方向。&lt;h4&gt;方法&lt;/h4&gt;综述分析了多种方法，包括单模态和多模态编码架构、视听融合策略、解码器设计，以及从完全监督到弱监督和无监督的训练范式。通过标准基准对各种方法进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;不同架构选择、融合策略和训练范式对AVS性能有显著影响。当前方法面临时间建模有限、视觉模态偏向、复杂环境鲁棒性不足和高计算需求等挑战。&lt;h4&gt;结论&lt;/h4&gt;未来研究方向包括改进时间推理和多模态融合、利用基础模型提升泛化和少样本学习能力、减少对标记数据的依赖，以及融入更高级别的推理以实现更智能的AVS系统。&lt;h4&gt;翻译&lt;/h4&gt;视听分割旨在通过利用视觉和音频两种模态来识别和分割视频中产生声音的物体。它已成为多模态感知领域的重要研究方向，能够实现细粒度的物体级理解。在本综述中，我们提供了AVS领域的全面概述，涵盖其问题定义、基准数据集、评估指标和方法进展。我们分析了多种方法，包括单模态和多模态编码架构、视听融合的关键策略以及各种解码器设计。此外，我们探讨了主要的训练范式，从完全监督学习到弱监督和无监督方法。值得注意的是，我们在标准基准上对AVS方法进行了广泛比较，突出了不同架构选择、融合策略和训练范式对性能的影响。最后，我们概述了当前面临的挑战，如时间建模有限、对视觉模态的偏向、复杂环境中缺乏鲁棒性以及高计算需求，并提出了有前途的未来方向，包括改进时间推理和多模态融合、利用基础模型实现更好的泛化和少样本学习、通过自监督和弱监督学习减少对标记数据的依赖，以及融入更高级别的推理以实现更智能的AVS系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-Visual Segmentation (AVS) aims to identify and segment sound-producingobjects in videos by leveraging both visual and audio modalities. It hasemerged as a significant research area in multimodal perception, enablingfine-grained object-level understanding. In this survey, we present acomprehensive overview of the AVS field, covering its problem formulation,benchmark datasets, evaluation metrics, and the progression of methodologies.We analyze a wide range of approaches, including architectures for unimodal andmultimodal encoding, key strategies for audio-visual fusion, and variousdecoder designs. Furthermore, we examine major training paradigms, from fullysupervised learning to weakly supervised and training-free methods. Notably, weprovide an extensive comparison of AVS methods across standard benchmarks,highlighting the impact of different architectural choices, fusion strategies,and training paradigms on performance. Finally, we outline the currentchallenges, such as limited temporal modeling, modality bias toward vision,lack of robustness in complex environments, and high computational demands, andpropose promising future directions, including improving temporal reasoning andmultimodal fusion, leveraging foundation models for better generalization andfew-shot learning, reducing reliance on labeled data through selfand weaklysupervised learning, and incorporating higher-level reasoning for moreintelligent AVS systems.</description>
      <author>example@mail.com (Jia Li, Yapeng Tian)</author>
      <guid isPermaLink="false">2508.03724v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Perch 2.0: The Bittern Lesson for Bioacoustics</title>
      <link>http://arxiv.org/abs/2508.04665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Perch 2.0是一个扩展到多类群的生物声学预训练模型，通过自蒸馏和新的训练标准实现了最先进的性能，并在海洋迁移学习任务上表现出色，尽管几乎没有海洋训练数据。&lt;h4&gt;背景&lt;/h4&gt;生物声学领域需要高性能的预训练模型来处理多种物种的声音数据，而现有的模型大多专注于特定类群（如鸟类）。&lt;h4&gt;目的&lt;/h4&gt;扩展Perch模型的训练范围，从单一类群（鸟类）扩展到多类群数据集，提高模型在生物声学任务中的通用性和性能。&lt;h4&gt;方法&lt;/h4&gt;使用自蒸馏训练方法，结合原型学习分类器和新的源预测训练标准，对大型多类群数据集进行监督训练。&lt;h4&gt;主要发现&lt;/h4&gt;Perch 2.0在BirdSet和BEANS基准测试中取得了最先进的性能；尽管几乎没有海洋训练数据，它在海洋迁移学习任务上仍优于专业海洋模型；细粒度物种分类可能是生物声学中特别稳健的预训练任务。&lt;h4&gt;结论&lt;/h4&gt;Perch 2.0通过扩展训练数据和改进训练方法，成功提升了模型在多种生物声学任务中的性能，展示了多类群预训练在生物声学领域的潜力。&lt;h4&gt;翻译&lt;/h4&gt;Perch是一个用于生物声学的高性能预训练模型。它以监督方式训练，可以为数千种发声物种提供即用型分类分数，同时也为迁移学习提供了强大的嵌入表示。在这个新版本Perch 2.0中，我们将训练范围从 exclusively 鸟类扩展到一个大型多类群数据集。该模型使用自蒸馏训练，结合了原型学习分类器和一种新的源预测训练标准。Perch 2.0在BirdSet和BEANS基准测试中获得了最先进的性能。尽管几乎没有海洋训练数据，它在海洋迁移学习任务上仍优于专业海洋模型。我们提出了假设，解释为什么细粒度物种分类是生物声学中特别稳健的预训练任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Perch is a performant pre-trained model for bioacoustics. It was trained insupervised fashion, providing both off-the-shelf classification scores forthousands of vocalizing species as well as strong embeddings for transferlearning. In this new release, Perch 2.0, we expand from training exclusivelyon avian species to a large multi-taxa dataset. The model is trained withself-distillation using a prototype-learning classifier as well as a newsource-prediction training criterion. Perch 2.0 obtains state-of-the-artperformance on the BirdSet and BEANS benchmarks. It also outperformsspecialized marine models on marine transfer learning tasks, despite havingalmost no marine training data. We present hypotheses as to why fine-grainedspecies classification is a particularly robust pre-training task forbioacoustics.</description>
      <author>example@mail.com (Bart van Merriënboer, Vincent Dumoulin, Jenny Hamer, Lauren Harrell, Andrea Burns, Tom Denton)</author>
      <guid isPermaLink="false">2508.04665v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Cluster-specific ranking and variable importance for Scottish regional deprivation via vine mixtures</title>
      <link>http://arxiv.org/abs/2508.04533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种使用藤混合模型对苏格兰区域进行社会经济剥夺聚类的方法，通过分析21个连续指标发现社会经济因素特别是收入和就业率是剥夺的主要驱动力。&lt;h4&gt;背景&lt;/h4&gt;社会经济剥夺是公共健康的关键决定因素，苏格兰政府通过苏格兰多重剥夺指数(SIMD)强调了这一点。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于多重剥夺指标对苏格兰区域进行聚类的方法，使用藤混合模型来捕捉指标间的复杂关系。&lt;h4&gt;方法&lt;/h4&gt;使用藤 copulas 捕捉指标间的尾部依赖和非对称关系；从拟合的藤混合模型中获得每个区域属于各聚类的后验概率；通过排序区域构建聚类驱动的剥夺排名；采用留一变量法评估变量重要性；分析苏格兰格拉斯哥及其周边1964个区域的21个连续指标。&lt;h4&gt;主要发现&lt;/h4&gt;社会经济措施，特别是收入和就业率，是剥夺的主要驱动力；某些健康和犯罪相关指标影响较小；变量重要性的方法和识别聚类的拟合藤结构分析结果一致。&lt;h4&gt;结论&lt;/h4&gt;藤混合模型能有效捕捉指标间的复杂关系；社会经济因素在剥夺评估中比健康和犯罪因素更为重要。&lt;h4&gt;翻译&lt;/h4&gt;社会经济剥夺是公共健康的关键决定因素，正如苏格兰政府的苏格兰多重剥夺指数(SIMD)所强调的那样。我们提出了一种使用藤混合模型基于多重剥夺指标对苏格兰区域进行聚类的方法。该框架利用藤 copulas 的灵活性来捕捉指标间的尾部依赖和非对称关系。从拟合的藤混合模型中，我们获得每个区域属于各聚类的后验概率。这允许通过根据区域属于最剥夺聚类的概率进行排序来构建由聚类驱动的剥夺排名。为了评估这种无监督学习设置中的变量重要性，我们采用留一变量法，通过重新拟合每个变量缺失的模型并计算贝叶斯信息准则的变化。我们对苏格兰格拉斯哥及其周边1964个区域的21个连续指标的分析表明，社会经济措施，特别是收入和就业率，是剥夺的主要驱动力，而某些健康和犯罪相关指标影响较小。这些发现在变量重要性的方法和识别聚类的拟合藤结构分析中是一致的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Socioeconomic deprivation is a key determinant of public health, ashighlighted by the Scottish Government's Scottish Index of Multiple Deprivation(SIMD). We propose an approach for clustering Scottish zones based on multipledeprivation indicators using vine mixture models. This framework uses theflexibility of vine copulas to capture tail dependent and asymmetricrelationships among the indicators. From the fitted vine mixture model, weobtain posterior probabilities for each zone's membership in clusters. Thisallows the construction of a cluster-driven deprivation ranking by sortingzones according to their probability of belonging to the most deprived cluster.To assess variable importance in this unsupervised learning setting, we adopt aleave-one-variable-out procedure by refitting the model without each variableand calculating the resulting change in the Bayesian information criterion. Ouranalysis of 21 continuous indicators across 1964 zones in Glasgow and thesurrounding areas in Scotland shows that socioeconomic measures, particularlyincome and employment rates, are major drivers of deprivation, while certainhealth- and crime-related indicators appear less influential. These findingsare consistent across the approach of variable importance and the analysis ofthe fitted vine structures of the identified clusters.</description>
      <author>example@mail.com (Özge Şahin, Ozan Evkaya, Ariane Hanebeck)</author>
      <guid isPermaLink="false">2508.04533v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Investigating the Impact of Large-Scale Pre-training on Nutritional Content Estimation from 2D Images</title>
      <link>http://arxiv.org/abs/2508.03996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了大规模预训练数据集对仅使用2D图像的深度学习模型在食物营养成分估计任务上的性能影响，发现专有数据集预训练的模型表现优于公共数据集预训练的模型，而数据集规模并非唯一决定因素。&lt;h4&gt;背景&lt;/h4&gt;从图像估计食物营养成分对健康和饮食监测至关重要，但仅使用2D图像面临食物呈现方式、光照变化以及缺乏深度信息推断体积质量的挑战。此外，该领域的可重复性受到最先进方法依赖专有数据集进行大规模预训练的限制。&lt;h4&gt;目的&lt;/h4&gt;研究大规模预训练数据集特征如何影响仅使用2D图像的深度学习模型在营养估计任务上的性能表现。&lt;h4&gt;方法&lt;/h4&gt;微调和评估在ImageNet和COYO两个大型公共数据集上预训练的Vision Transformer模型，并与在JFT-300M专有数据集上预训练的最先进方法及基线CNN模型（InceptionV2和ResNet-50）进行比较。在具有高精度营养注释的Nutrition5k数据集上进行实验，使用平均绝对误差和平均绝对百分比误差进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;在JFT-300M上预训练的模型显著优于在公共数据集上预训练的模型。出乎意料的是，在大型COYO数据集上预训练的模型表现不如在ImageNet上预训练的模型，这与初始假设相矛盾。&lt;h4&gt;结论&lt;/h4&gt;分析提供了定量证据，强调了预训练数据集特征（包括规模、领域相关性和策划质量）在2D营养估计有效迁移学习中的关键作用，表明数据集的质量和领域相关性比单纯的规模更为重要。&lt;h4&gt;翻译&lt;/h4&gt;从图像估计食物的营养成分是一项具有重大健康和饮食监测意义的关键任务。这具有挑战性，特别是仅依靠2D图像时，由于食物呈现方式的多样性、光照的变化，以及在没有深度信息的情况下推断体积和质量的固有困难。此外，该领域的可重复性受到最先进方法依赖专有数据集进行大规模预训练的阻碍。在本文中，我们研究了大规模预训练数据集对仅使用2D图像的深度学习模型在营养估计任务上的性能影响。我们微调和评估了在两个大型公共数据集ImageNet和COYO上预训练的Vision Transformer模型，将其性能与基线CNN模型（InceptionV2和ResNet-50）以及在专有JFT-300M数据集上预训练的最先进方法进行比较。我们在Nutrition5k数据集上进行了大量实验，这是一个具有高精度营养注释的真实世界餐盘的大规模收集。我们使用平均绝对误差和平均绝对百分比误差进行的评估显示，在JFT-300M上预训练的模型显著优于在公共数据集上预训练的模型。出乎意料的是，在此特定回归任务中，在大型COYO数据集上预训练的模型表现不如在ImageNet上预训练的模型，这与我们的初始假设相矛盾。我们的分析提供了定量证据，强调了预训练数据集特征（包括规模、领域相关性和策划质量）在2D营养估计有效迁移学习中的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the nutritional content of food from images is a critical taskwith significant implications for health and dietary monitoring. This ischallenging, especially when relying solely on 2D images, due to thevariability in food presentation, lighting, and the inherent difficulty ininferring volume and mass without depth information. Furthermore,reproducibility in this domain is hampered by the reliance of state-of-the-artmethods on proprietary datasets for large-scale pre-training. In this paper, weinvestigate the impact of large-scale pre-training datasets on the performanceof deep learning models for nutritional estimation using only 2D images. Wefine-tune and evaluate Vision Transformer (ViT) models pre-trained on two largepublic datasets, ImageNet and COYO, comparing their performance againstbaseline CNN models (InceptionV2 and ResNet-50) and a state-of-the-art methodpre-trained on the proprietary JFT-300M dataset. We conduct extensiveexperiments on the Nutrition5k dataset, a large-scale collection of real-worldfood plates with high-precision nutritional annotations. Our evaluation usingMean Absolute Error (MAE) and Mean Absolute Percentage Error (MAE%) revealsthat models pre-trained on JFT-300M significantly outperform those pre-trainedon public datasets. Unexpectedly, the model pre-trained on the massive COYOdataset performs worse than the model pre-trained on ImageNet for this specificregression task, refuting our initial hypothesis. Our analysis providesquantitative evidence highlighting the critical role of pre-training datasetcharacteristics, including scale, domain relevance, and curation quality, foreffective transfer learning in 2D nutritional estimation.</description>
      <author>example@mail.com (Michele Andrade, Guilherme A. L. Silva, Valéria Santos, Gladston Moreira, Eduardo Luz)</author>
      <guid isPermaLink="false">2508.03996v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>SocialPulse: An On-Smartwatch System for Detecting Real-World Social Interactions</title>
      <link>http://arxiv.org/abs/2508.03980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种实时手表系统，能够检测面对面和虚拟社交互动，在真实世界环境中实现了73.18%的检测准确率，为理解日常社交体验提供了新工具。&lt;h4&gt;背景&lt;/h4&gt;社交互动是日常生活的基础，对幸福感至关重要。新兴技术为不显眼监测行为提供了机会，但自动检测社交互动（特别是通过可穿戴设备）仍研究不足。现有系统通常局限于受控环境，仅限面对面互动，且依赖严格假设，降低了捕捉多样化现实世界互动的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够检测面对面和虚拟互动的实时、手表系统，解决现有系统的局限性，提高在真实世界环境中捕捉社交互动的能力。&lt;h4&gt;方法&lt;/h4&gt;利用迁移学习检测前景语音(FS)，并基于FS和耳语等对话线索推断互动边界。在真实世界环境中评估系统，涉及11名参与者，总共38天（平均=3.45天，标准差=2.73）。&lt;h4&gt;主要发现&lt;/h4&gt;系统实现了73.18%的互动检测准确率。对6名参与者的后续调查显示，检测互动的召回率达到完美水平（100%）。&lt;h4&gt;结论&lt;/h4&gt;初步研究结果展示了该系统捕捉日常生活中互动的潜力，为针对社交焦虑的个性化干预等应用提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;社交互动是日常生活的基本组成部分，对幸福感起着关键作用。随着新兴技术提供了不显眼监测行为的机会，人们越来越有兴趣利用这些技术更好地理解社交体验。然而，自动检测互动（特别是通过可穿戴设备）仍然研究不足。现有系统通常局限于受控环境，仅限于面对面互动，并依赖严格的假设，如固定时间窗口内存在两个说话者。这些限制降低了它们捕捉多样化现实世界互动的泛化能力。为了解决这些挑战，我们开发了一个实时、手表系统，能够检测面对面和虚拟互动。该系统利用迁移学习检测前景语音（FS），并根据FS和耳语等对话线索推断互动边界。在涉及11名参与者总共38天（平均=3.45天，标准差=2.73）的真实世界评估中，系统实现了73.18%的互动检测准确率。对6名参与者的后续调查显示，检测互动的召回率达到完美。这些初步研究结果证明了我们的系统捕捉日常生活中互动的潜力，为针对社交焦虑的个性化干预等应用提供了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3714394.3754435&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social interactions are a fundamental part of daily life and play a criticalrole in well-being. As emerging technologies offer opportunities tounobtrusively monitor behavior, there is growing interest in using them tobetter understand social experiences. However, automatically detectinginteractions, particularly via wearable devices, remains underexplored.Existing systems are often limited to controlled environments, constrained toin-person interactions, and rely on rigid assumptions such as the presence oftwo speakers within a fixed time window. These limitations reduce theirgeneralizability to capture diverse real-world interactions. To address thesechallenges, we developed a real-time, on-watch system capable of detecting bothin-person and virtual interactions. The system leverages transfer learning todetect foreground speech (FS) and infers interaction boundaries based upon FSand conversational cues like whispering. In a real-world evaluation involving11 participants over a total of 38 days (Mean = 3.45 days, SD = 2.73), thesystem achieved an interaction detection accuracy of 73.18%. Follow-up with sixparticipants indicated perfect recall for detecting interactions. Thesepreliminary findings demonstrate the potential of our system to captureinteractions in daily life, providing a foundation for applications such aspersonalized interventions targeting social anxiety.</description>
      <author>example@mail.com (Md Sabbir Ahmed, Arafat Rahman, Mark Rucker, Laura E. Barnes)</author>
      <guid isPermaLink="false">2508.03980v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data</title>
      <link>http://arxiv.org/abs/2508.03921v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了主动学习和迁移学习结合在跨领域时间序列异常检测中的效果，发现不应用聚类时效果最佳，主动学习确实能提高性能但改进速度较慢，且性能提升与所选点数量呈线性关系。&lt;h4&gt;背景&lt;/h4&gt;研究关注跨领域时间序列数据中的异常检测问题，探索主动学习与迁移学习结合的有效性。&lt;h4&gt;目的&lt;/h4&gt;评估主动学习和迁移学习结合在跨领域时间序列数据异常检测中的有效性，并分析聚类与主动学习之间的交互作用。&lt;h4&gt;方法&lt;/h4&gt;研究使用了聚类和主动学习的结合方法，并在多个数据集上评估了迁移学习与主动学习结合的性能上限，使用了改进的实验设计，确保采样和测试池使用不同的数据样本。&lt;h4&gt;主要发现&lt;/h4&gt;1) 聚类和主动学习之间存在交互作用，通常不应用聚类时能达到最佳性能；2) 使用主动学习添加新样本能提高模型性能，但改进速度比文献报道的慢；3) 迁移学习与主动学习结合的性能最初会提高，但随着更多目标点被选入训练，性能最终开始下降；4) 性能下降表明主动学习在排序数据点方面表现良好，将不太有用的点推向选择过程末尾。&lt;h4&gt;结论&lt;/h4&gt;综合结果表明，主动学习是有效的，但模型性能的提升与所选点的数量呈线性平坦函数关系，而非文献中可能暗示的更快速改进。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了结合主动学习和迁移学习在跨领域时间序列数据异常检测中的有效性。我们的结果表明聚类和主动学习之间存在交互作用，通常情况下，不应用聚类（即使用单个聚类）时能达到最佳性能。此外，我们发现使用主动学习向训练集添加新样本确实能提高模型性能，但改进速度一般比文献报道的要慢。我们将这种差异归因于改进的实验设计，其中使用了不同的数据样本进行采样和测试。最后，我们在多个数据集上评估了迁移学习与主动学习结合的性能上限，发现性能最初会提高，但随着更多目标点被选入训练，性能最终开始下降。这种性能下降可能表明主动学习过程在排序数据点方面做得很好，将不太有用的点推向选择过程的末尾，而这种下降发生在这些不太有用的点最终被添加时。综合来看，我们的结果表明主动学习是有效的，但模型性能的提升与所选点的数量呈线性平坦函数关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper examines the effectiveness of combining active learning andtransfer learning for anomaly detection in cross-domain time-series data. Ourresults indicate that there is an interaction between clustering and activelearning and in general the best performance is achieved using a single cluster(in other words when clustering is not applied). Also, we find that adding newsamples to the training set using active learning does improve modelperformance but that in general, the rate of improvement is slower than theresults reported in the literature suggest. We attribute this difference to animproved experimental design where distinct data samples are used for thesampling and testing pools. Finally, we assess the ceiling performance oftransfer learning in combination with active learning across several datasetsand find that performance does initially improve but eventually begins to tailoff as more target points are selected for inclusion in training. This tail-offin performance may indicate that the active learning process is doing a goodjob of sequencing data points for selection, pushing the less useful pointstowards the end of the selection process and that this tail-off occurs whenthese less useful points are eventually added. Taken together our resultsindicate that active learning is effective but that the improvement in modelperformance follows a linear flat function concerning the number of pointsselected and labelled.</description>
      <author>example@mail.com (John D. Kelleher, Matthew Nicholson, Rahul Agrahari, Clare Conran)</author>
      <guid isPermaLink="false">2508.03921v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning</title>
      <link>http://arxiv.org/abs/2508.03863v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to be presented at IEEE PIMRC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于众包用户关键性能指标和监管数据的时空频谱需求预测框架，实现了比传统ITU模型更准确的预测结果和更好的跨区域泛化能力。&lt;h4&gt;背景&lt;/h4&gt;准确的频谱需求预测对现代无线通信网络的频谱分配、监管规划和可持续发展至关重要，支持国际电信联盟建立公平的频谱分配政策和改进拍卖机制，以满足5G、6G和物联网等新兴技术的需求。&lt;h4&gt;目的&lt;/h4&gt;开发一个有效的时空预测框架，利用众包用户关键性能指标和监管数据来建模和预测频谱需求，为政策制定者和监管机构提供更可靠的频谱管理工具。&lt;h4&gt;方法&lt;/h4&gt;结合高级特征工程、全面相关性分析和迁移学习技术，利用细粒度、数据驱动的洞察来考虑频谱利用的空间和时间变化，避免传统ITU模型中任意输入和不切实际假设的限制。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法实现了卓越的预测精度和跨区域泛化能力，与ITU估计作为基准的比较评估证明了该框架能够提供更现实和可操作的预测结果。&lt;h4&gt;结论&lt;/h4&gt;该研究验证了所提出方法的有效性，其有望成为监管机构和政策制定者加强频谱管理和规划的稳健方法。&lt;h4&gt;翻译&lt;/h4&gt;准确的频谱需求预测对于现代无线通信网络中的明智频谱分配、有效的监管规划和促进可持续发展至关重要。它支持国际电信联盟领导的政府工作，建立公平的频谱分配政策，改进拍卖机制，并满足先进5G、未来6G和物联网等新兴技术的需求。本文提出了一种有效的时空预测框架，利用众包用户关键性能指标和监管数据来建模和预测频谱需求。所提出的方法通过结合高级特征工程、全面相关性分析和迁移学习技术，实现了卓越的预测精度和跨区域泛化能力。与经常受限于任意输入和不切实际假设的传统ITU模型不同，这种方法利用细粒度、数据驱动的洞察来考虑频谱利用的空间和时间变化。与作为基准的ITU估计进行的比较评估强调了我们的框架提供更现实和可操作预测的能力。实验结果验证了我们的方法的有效性，突显其作为政策制定者和监管机构加强频谱管理和规划的稳健方法的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate spectrum demand prediction is crucial for informed spectrumallocation, effective regulatory planning, and fostering sustainable growth inmodern wireless communication networks. It supports governmental efforts,particularly those led by the international telecommunication union (ITU), toestablish fair spectrum allocation policies, improve auction mechanisms, andmeet the requirements of emerging technologies such as advanced 5G, forthcoming6G, and the internet of things (IoT). This paper presents an effectivespatio-temporal prediction framework that leverages crowdsourced user-side keyperformance indicators (KPIs) and regulatory datasets to model and forecastspectrum demand. The proposed methodology achieves superior prediction accuracyand cross-regional generalizability by incorporating advanced featureengineering, comprehensive correlation analysis, and transfer learningtechniques. Unlike traditional ITU models, which are often constrained byarbitrary inputs and unrealistic assumptions, this approach exploits granular,data-driven insights to account for spatial and temporal variations in spectrumutilization. Comparative evaluations against ITU estimates, as the benchmark,underscore our framework's capability to deliver more realistic and actionablepredictions. Experimental results validate the efficacy of our methodology,highlighting its potential as a robust approach for policymakers and regulatorybodies to enhance spectrum management and planning.</description>
      <author>example@mail.com (Amin Farajzadeh, Hongzhao Zheng, Sarah Dumoulin, Trevor Ha, Halim Yanikomeroglu, Amir Ghasemi)</author>
      <guid isPermaLink="false">2508.03863v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training</title>
      <link>http://arxiv.org/abs/2508.03742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过提升视觉语义密度来改善视觉-语言预训练在医疗诊断中性能的方法，有效解决了低信噪比医学图像与高信噪比报告之间的语义密度差距问题。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言预训练在发展多功能和通用医疗诊断能力方面有很大潜力，但将低信噪比医学图像与高信噪比报告对齐时存在语义密度差距，导致视觉对齐偏差。&lt;h4&gt;目的&lt;/h4&gt;提升视觉语义密度以改善医学图像与报告之间的对齐效果，增强医疗诊断能力。&lt;h4&gt;方法&lt;/h4&gt;一方面，通过疾病级别的视觉对比学习增强视觉语义，加强模型区分正常和异常样本的能力；另一方面，引入解剖正常性建模方法，利用VQ-VAE在潜在空间重建正常视觉嵌入，通过分布偏移放大异常信号。&lt;h4&gt;主要发现&lt;/h4&gt;增强的视觉表示有效捕捉诊断相关语义，促进与诊断报告的高效准确对齐；在多个CT数据集上实验，达到最先进的零样本性能；在15个器官54种疾病中平均AUC达84.9%，显著超越现有方法；展示了优秀的迁移学习能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过提升视觉语义密度，有效解决了医学图像与报告之间的语义密度差距问题，显著提高了医疗诊断的性能。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言预训练在发展多功能和通用医疗诊断能力方面有很大潜力。然而，将低信噪比的医学图像与高信噪比的报告进行对齐时存在语义密度差距，导致视觉对齐偏差。在本文中，我们提出通过提升视觉语义密度来改善对齐效果。一方面，我们通过疾病级别的视觉对比学习来增强视觉语义，加强模型区分每个解剖结构正常和异常样本的能力。另一方面，我们引入解剖正常性建模方法为每个解剖结构建模正常样本的分布，利用VQ-VAE在潜在空间中重建正常的视觉嵌入。这个过程通过利用异常样本中的分布偏移来放大异常信号，增强模型对异常属性的感知和辨别能力。增强的视觉表示有效地捕捉了诊断相关的语义，促进了与诊断报告更高效和准确的对齐。我们在两个胸部CT数据集CT-RATE和Rad-ChestCT以及一个腹部CT数据集MedVL-CT69K上进行了大量实验，全面评估了在胸部和腹部CT场景多个任务中的诊断性能，达到了最先进的零样本性能。值得注意的是，我们的方法在15个器官的54种疾病中平均AUC达到84.9%，显著超越现有方法。此外，我们展示了预训练模型的优秀迁移学习能力。代码可在https://github.com/alibaba-damo-academy/ViSD-Boost获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language pre-training (VLP) has great potential for developingmultifunctional and general medical diagnostic capabilities. However, aligningmedical images with a low signal-to-noise ratio (SNR) to reports with a highSNR presents a semantic density gap, leading to visual alignment bias. In thispaper, we propose boosting vision semantic density to improve alignmenteffectiveness. On one hand, we enhance visual semantics through disease-levelvision contrastive learning, which strengthens the model's ability todifferentiate between normal and abnormal samples for each anatomicalstructure. On the other hand, we introduce an anatomical normality modelingmethod to model the distribution of normal samples for each anatomy, leveragingVQ-VAE for reconstructing normal vision embeddings in the latent space. Thisprocess amplifies abnormal signals by leveraging distribution shifts inabnormal samples, enhancing the model's perception and discrimination ofabnormal attributes. The enhanced visual representation effectively capturesthe diagnostic-relevant semantics, facilitating more efficient and accuratealignment with the diagnostic report. We conduct extensive experiments on twochest CT datasets, CT-RATE and Rad-ChestCT, and an abdominal CT dataset,MedVL-CT69K, and comprehensively evaluate the diagnosis performance acrossmultiple tasks in the chest and abdominal CT scenarios, achievingstate-of-the-art zero-shot performance. Notably, our method achieved an averageAUC of 84.9% across 54 diseases in 15 organs, significantly surpassing existingmethods. Additionally, we demonstrate the superior transfer learningcapabilities of our pre-trained model. Code is available athttps://github.com/alibaba-damo-academy/ViSD-Boost.</description>
      <author>example@mail.com (Weiwei Cao, Jianpeng Zhang, Zhongyi Shui, Sinuo Wang, Zeli Chen, Xi Li, Le Lu, Xianghua Ye, Tingbo Liang, Qi Zhang, Ling Zhang)</author>
      <guid isPermaLink="false">2508.03742v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation</title>
      <link>http://arxiv.org/abs/2508.04645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by KDD 2025 Research Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络在链接预测任务中的应用挑战，并提出了一种基于预训练的解决方案。通过研究节点级和边级信息的可迁移性，采用后期融合策略和专家混合框架，有效解决了LP任务中的监督有限、初始化敏感和泛化能力差等问题，在保持高性能的同时大幅降低了计算开销。&lt;h4&gt;背景&lt;/h4&gt;链接预测(LP)是图机器学习中的关键任务。尽管图神经网络(GNNs)近年来显著提升了LP性能，但现有方法面临三大挑战：稀疏连接提供的监督有限、对初始化敏感以及在分布偏移下泛化能力差。LP与节点分类不同，它本质上是一个需要集成节点级和边级信息的成对任务。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决LP任务中的关键挑战，探索预训练作为解决方案，研究节点级和边级模块的可迁移性，并提出有效的方法来组合这些模块，同时处理预训练数据的多样性，避免负迁移，并实现快速适应。&lt;h4&gt;方法&lt;/h4&gt;作者提出了多种创新方法：1)首次系统性研究不同模块的可迁移性；2)提出后期融合策略有效组合节点级和边级信息的输出；3)引入专家混合(MoE)框架处理预训练数据多样性，避免负迁移；4)开发参数高效微调策略实现快速适应。这些方法共同构成了一个完整的预训练-微调框架。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在两个领域的16个数据集上均有效，在低资源链接预测任务上取得了最先进的性能，与端到端训练方法相比获得了具有竞争力的结果，同时计算开销降低了10,000倍以上。&lt;h4&gt;结论&lt;/h4&gt;预训练方法可以有效解决链接预测任务中的关键挑战，通过后期融合策略和专家混合框架，在保持高性能的同时大幅降低计算开销，为链接预测任务提供了一种高效可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;链接预测(LP)是图机器学习中的一个关键任务。虽然图神经网络(GNNs)最近显著提升了LP性能，但现有方法面临关键挑战，包括来自稀疏连接的有限监督、对初始化的敏感性以及在分布偏移下的泛化能力差。我们探索预训练作为解决这些挑战的方案。与节点分类不同，LP本质上是一个成对任务，需要集成节点级和边级信息。在这项工作中，我们进行了首次关于这些不同模块可迁移性的系统性研究，并提出了一种后期融合策略，以有效组合它们的输出以改善性能。为了处理预训练数据的多样性和避免负迁移，我们引入了一个专家混合(MoE)框架，该框架在不同的专家中捕获不同的模式，促进预训练模型在多样化下游数据集上的无缝应用。为了快速适应，我们开发了一种参数高效的微调策略，使预训练模型能够以最小的计算开销适应未见过的数据集。在两个领域的16个数据集上的实验证明了我们方法的有效性，在低资源链接预测上取得了最先进的性能，与端到端训练的方法相比获得了具有竞争力的结果，计算开销降低了10,000倍以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link Prediction (LP) is a critical task in graph machine learning. WhileGraph Neural Networks (GNNs) have significantly advanced LP performancerecently, existing methods face key challenges including limited supervisionfrom sparse connectivity, sensitivity to initialization, and poorgeneralization under distribution shifts. We explore pretraining as a solutionto address these challenges. Unlike node classification, LP is inherently apairwise task, which requires the integration of both node- and edge-levelinformation. In this work, we present the first systematic study on thetransferability of these distinct modules and propose a late fusion strategy toeffectively combine their outputs for improved performance. To handle thediversity of pretraining data and avoid negative transfer, we introduce aMixture-of-Experts (MoE) framework that captures distinct patterns in separateexperts, facilitating seamless application of the pretrained model on diversedownstream datasets. For fast adaptation, we develop a parameter-efficienttuning strategy that allows the pretrained model to adapt to unseen datasetswith minimal computational overhead. Experiments on 16 datasets across twodomains demonstrate the effectiveness of our approach, achievingstate-of-the-art performance on low-resource link prediction while obtainingcompetitive results compared to end-to-end trained methods, with over 10,000xlower computational overhead.</description>
      <author>example@mail.com (Yu Song, Zhigang Hua, Harry Shomer, Yan Xie, Jingzhe Liu, Bo Long, Hui Liu)</author>
      <guid isPermaLink="false">2508.04645v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape</title>
      <link>http://arxiv.org/abs/2508.04542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 9 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过分析超过5000个身份盗窃和欺诈案例，构建了身份生态系统图模型，开发了隐私风险预测框架，用于评估个人数据泄露风险及连锁反应。&lt;h4&gt;背景&lt;/h4&gt;个人和组织难以在没有基本了解相关隐私风险的情况下保护个人信息。身份盗窃和欺诈事件频发，但缺乏对哪些个人数据易受攻击、暴露频率及后果的系统理解。&lt;h4&gt;目的&lt;/h4&gt;识别哪些类型的个人数据被暴露、暴露频率如何、暴露后果是什么，并开发一种方法来预测当某些个人信息被泄露时其他信息可能被泄露的概率。&lt;h4&gt;方法&lt;/h4&gt;分析超过5000个身份盗窃和欺诈的实证案例；构建身份生态系统图，其中节点代表可识别个人信息的属性，边代表它们之间的实证披露关系；利用图结构和图论、图神经网络开发隐私风险预测框架。&lt;h4&gt;主要发现&lt;/h4&gt;研究确定了哪些类型的个人数据最容易暴露、暴露的频率以及暴露的后果；开发的隐私风险预测框架能够有效预测当某些个人信息被泄露时其他信息可能被泄露的概率。&lt;h4&gt;结论&lt;/h4&gt;身份生态系统图和隐私风险预测框架为理解和预测个人信息泄露风险提供了有效工具，能够回答给定身份属性的披露是否可能导致另一个属性披露的核心问题。&lt;h4&gt;翻译&lt;/h4&gt;个人和组织在没有基本了解相关隐私风险的情况下难以保护个人信息。通过分析超过5000个身份盗窃和欺诈的实证案例，本研究确定了哪些类型的个人数据被暴露、暴露频率如何以及暴露的后果是什么。我们构建了一个身份生态系统图——一个基于图的基础模型，其中节点代表可识别个人信息的属性，边代表它们之间的实证披露关系（例如，一个PII属性由于另一个属性的暴露而被暴露的概率）。利用这种图结构，我们开发了一个隐私风险预测框架，使用图论和图神经网络来估计当某些PII属性被泄露时进一步泄露的可能性。结果表明，我们的方法有效地回答了核心问题：给定身份属性的披露是否可能导致另一个属性的披露？&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; It is difficult for individuals and organizations to protect personalinformation without a fundamental understanding of relative privacy risks. Byanalyzing over 5,000 empirical identity theft and fraud cases, this researchidentifies which types of personal data are exposed, how frequently exposuresoccur, and what the consequences of those exposures are. We construct anIdentity Ecosystem graph--a foundational, graph-based model in which nodesrepresent personally identifiable information (PII) attributes and edgesrepresent empirical disclosure relationships between them (e.g., theprobability that one PII attribute is exposed due to the exposure of another).Leveraging this graph structure, we develop a privacy risk prediction frameworkthat uses graph theory and graph neural networks to estimate the likelihood offurther disclosures when certain PII attributes are compromised. The resultsshow that our approach effectively answers the core question: Can thedisclosure of a given identity attribute possibly lead to the disclosure ofanother attribute?</description>
      <author>example@mail.com (Haoran Niu, K. Suzanne Barber)</author>
      <guid isPermaLink="false">2508.04542v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Reliable and Real-Time Highway Trajectory Planning via Hybrid Learning-Optimization Frameworks</title>
      <link>http://arxiv.org/abs/2508.04436v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种混合轨迹规划框架，结合基于学习的方法适应性和基于优化方法的正式安全保证，用于解决自动驾驶高速公路驾驶中的碰撞风险问题。该框架采用两层架构，实现了高成功率和实时性能。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶高速公路驾驶面临环境快速变化和反应时间有限的挑战，导致较高的碰撞风险，需要可靠高效的轨迹规划方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在复杂现实世界紧急场景中生成平滑、无碰撞轨迹的轨迹规划方法，同时保证实时性能。&lt;h4&gt;方法&lt;/h4&gt;提出混合轨迹规划框架，采用两层架构：上层使用图神经网络(GNN)预测纵向速度曲线，下层将路径优化表述为混合整数二次规划(MIQP)问题，引入离散化车辆几何的线性近似降低计算复杂度，并执行严格时空不重叠约束确保碰撞避免。&lt;h4&gt;主要发现&lt;/h4&gt;该规划器在复杂紧急场景中生成高度平滑、无碰撞轨迹，成功率超过97%，平均规划时间为54毫秒，确认了其实时能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的混合轨迹规划框架成功结合了学习方法的适应性和优化方法的安全保证，能够在复杂紧急场景中高效生成安全轨迹，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶高速公路驾驶由于环境快速变化和反应时间有限而呈现高碰撞风险，需要可靠高效的轨迹规划。本文提出了一种混合轨迹规划框架，整合了基于学习方法的适应性和基于优化方法的正式安全保证。该框架采用两层架构：上层使用在真实高速公路数据上训练的图神经网络(GNN)预测类人的纵向速度曲线，下层利用表述为混合整数二次规划(MIQP)问题的路径优化。主要贡献是下层的路径优化模型，该模型引入了离散化车辆几何的线性近似，显著降低了计算复杂度，同时强制执行严格的时空不重叠约束，在整个规划范围内正式保证碰撞避免。实验结果表明，该规划器在复杂的现实世界紧急场景中生成高度平滑、无碰撞的轨迹，成功率超过97%，平均规划时间为54毫秒，从而确认了其实时能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous highway driving presents a high collision risk due tofast-changing environments and limited reaction time, necessitating reliableand efficient trajectory planning. This paper proposes a hybrid trajectoryplanning framework that integrates the adaptability of learning-based methodswith the formal safety guarantees of optimization-based approaches. Theframework features a two-layer architecture: an upper layer employing a graphneural network (GNN) trained on real-world highway data to predict human-likelongitudinal velocity profiles, and a lower layer utilizing path optimizationformulated as a mixed-integer quadratic programming (MIQP) problem. The primarycontribution is the lower-layer path optimization model, which introduces alinear approximation of discretized vehicle geometry to substantially reducecomputational complexity, while enforcing strict spatiotemporal non-overlappingconstraints to formally guarantee collision avoidance throughout the planninghorizon. Experimental results demonstrate that the planner generates highlysmooth, collision-free trajectories in complex real-world emergency scenarios,achieving success rates exceeding 97% with average planning times of 54 ms,thereby confirming real-time capability.</description>
      <author>example@mail.com (Yujia Lu, Chong Wei, Lu Ma)</author>
      <guid isPermaLink="false">2508.04436v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>ProtoN: Prototype Node Graph Neural Network for Unconstrained Multi-Impression Ear Recognition</title>
      <link>http://arxiv.org/abs/2508.04381v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了ProtoN少样本学习框架，通过基于图的方法联合处理同一身份的多个耳部印象，显著提高了耳部生物识别的准确率，在有限数据条件下达到了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;耳部生物识别是一种稳定且非接触式的身份识别方式，但其有效性受到标注数据稀缺和类内变化大的限制。现有方法通常单独处理单个印象，限制了捕捉一致性和区分性表示的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够联合处理多个印象、捕捉一致性和区分性表示的少样本学习框架，以提高耳部生物识别的准确性，克服数据稀缺和类内变化的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出ProtoN框架，使用基于图的方法将每个印象表示为类特定图中的一个节点，同时包含一个可学习的原型节点。通过原型图神经网络层处理该图，采用双路径消息传递机制优化表示，并使用跨图原型对齐策略和混合损失函数增强区分能力。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准耳部数据集上的实验表明，ProtoN实现了最先进的性能，Rank-1识别准确率高达99.60%，等错误率低至0.025，证明了在有限数据条件下少样本耳部识别的有效性。&lt;h4&gt;结论&lt;/h4&gt;ProtoN框架通过联合处理多个印象和基于图的方法，有效解决了耳部生物识别中的数据稀缺和类内变化问题，为少样本耳部识别提供了一种有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;耳部生物识别为身份识别提供了一种稳定且非接触式的模态，但其有效性仍受限于标注数据的稀缺和显著的类内变化。现有方法通常单独从单个印象中提取身份特征，限制了它们捕捉一致性和区分性表示的能力。为了克服这些局限性，提出了一种名为ProtoN的少样本学习框架，使用基于图的方法联合处理同一身份的多个印象。每个印象在类特定图中表示为一个节点，同时包含一个可学习的原型节点，该节点编码身份级信息。该图由原型图神经网络层处理，该层专门设计通过双路径消息传递机制来优化印象和原型表示。为了进一步增强区分能力，PGNN包含跨图原型对齐策略，通过强制类内紧凑性同时保持类间区分性来提高类可分性。此外，采用混合损失函数来平衡周期性和全局分类目标，从而改善嵌入空间的总体结构。在五个基准耳部数据集上的大量实验表明，ProtoN实现了最先进的性能，Rank-1识别准确率高达99.60%，等错误率低至0.025，显示了在有限数据条件下少样本耳部识别的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ear biometrics offer a stable and contactless modality for identityrecognition, yet their effectiveness remains limited by the scarcity ofannotated data and significant intra-class variability. Existing methodstypically extract identity features from individual impressions in isolation,restricting their ability to capture consistent and discriminativerepresentations. To overcome these limitations, a few-shot learning framework,ProtoN, is proposed to jointly process multiple impressions of an identityusing a graph-based approach. Each impression is represented as a node in aclass-specific graph, alongside a learnable prototype node that encodesidentity-level information. This graph is processed by a Prototype Graph NeuralNetwork (PGNN) layer, specifically designed to refine both impression andprototype representations through a dual-path message-passing mechanism. Tofurther enhance discriminative power, the PGNN incorporates a cross-graphprototype alignment strategy that improves class separability by enforcingintra-class compactness while maintaining inter-class distinction.Additionally, a hybrid loss function is employed to balance episodic and globalclassification objectives, thereby improving the overall structure of theembedding space. Extensive experiments on five benchmark ear datasetsdemonstrate that ProtoN achieves state-of-the-art performance, with Rank-1identification accuracy of up to 99.60% and an Equal Error Rate (EER) as low as0.025, showing the effectiveness for few-shot ear recognition under limiteddata conditions.</description>
      <author>example@mail.com (Santhoshkumar Peddi, Sadhvik Bathini, Arun Balasubramanian, Monalisa Sarma, Debasis Samanta)</author>
      <guid isPermaLink="false">2508.04381v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities</title>
      <link>http://arxiv.org/abs/2508.04235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为CASCAD的新型电路感知SAT求解框架，通过利用图神经网络计算的门级条件概率直接利用电路级信息，显著提高了SAT求解器效率。在逻辑等价性检查基准测试中，CASCAD相比最先进的基于CNF的方法将求解时间减少了最多10倍。&lt;h4&gt;背景&lt;/h4&gt;电路可满足性(CSAT)在电子设计自动化中起着关键作用。解决CSAT问题的标准工作流程是将电路转换为合取范式(CNF)，并使用基于冲突驱动子句学习(CDCL)的通用SAT求解器。然而，这个过程固有地丢弃了丰富的结构和功能信息，导致求解器性能次优。&lt;h4&gt;目的&lt;/h4&gt;解决传统CSAT求解方法中丢弃电路结构和功能信息的问题，提高SAT求解器的效率。&lt;h4&gt;方法&lt;/h4&gt;作者提出了CASCAD，一种新颖的电路感知SAT求解框架，直接利用通过图神经网络(GNN)计算的电路级条件概率。通过显式建模门级条件概率，CASCAD动态引导两个关键的CDCL启发式方法——变量相位选择和子句管理，从而提高求解器效率，并采用概率引导的子句过滤策略。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的实际逻辑等价性检查(LEC)基准测试中，CASCAD相比最先进的基于CNF的方法将求解时间减少了最多10倍，并通过概率引导的子句过滤策略额外实现了23.5%的运行时间减少。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了在SAT求解器中保留电路级结构洞察力的重要性，为未来提高SAT求解效率和EDA工具设计提供了坚实的基础。&lt;h4&gt;翻译&lt;/h4&gt;电路可满足性(CSAT)在电子设计自动化中起着关键作用。解决CSAT问题的标准工作流程将电路转换为合取范式(CNF)，并采用由冲突驱动子句学习(CDCL)提供支持的通用SAT求解器。然而，这个过程固有地丢弃了丰富的结构和功能信息，导致求解器性能次优。为了解决这一限制，我们引入了CASCAD，一种新颖的电路感知SAT求解框架，直接利用通过图神经网络(GNN)计算的电路级条件概率。通过显式建模门级条件概率，CASCAD动态引导两个关键的CDCL启发式方法——变量相位选择和子句管理，显著提高了解决器效率。在具有挑战性的实际逻辑等价性检查(LEC)基准测试上的广泛评估表明，CASCAD相比最先进的基于CNF的方法将求解时间减少了最多10倍，并通过我们的概率引导子句过滤策略额外实现了23.5%的运行时间减少。我们的结果强调了在SAT求解器中保留电路级结构洞察力的重要性，为未来提高SAT求解效率和EDA工具设计提供了坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Circuit Satisfiability (CSAT) plays a pivotal role in Electronic DesignAutomation. The standard workflow for solving CSAT problems converts circuitsinto Conjunctive Normal Form (CNF) and employs generic SAT solvers powered byConflict-Driven Clause Learning (CDCL). However, this process inherentlydiscards rich structural and functional information, leading to suboptimalsolver performance. To address this limitation, we introduce CASCAD, a novelcircuit-aware SAT solving framework that directly leverages circuit-levelconditional probabilities computed via Graph Neural Networks (GNNs). Byexplicitly modeling gate-level conditional probabilities, CASCAD dynamicallyguides two critical CDCL heuristics -- variable phase selection and clausemanagementto significantly enhance solver efficiency. Extensive evaluations onchallenging real-world Logical Equivalence Checking (LEC) benchmarksdemonstrate that CASCAD reduces solving times by up to 10x compared tostate-of-the-art CNF-based approaches, achieving an additional 23.5% runtimereduction via our probability-guided clause filtering strategy. Our resultsunderscore the importance of preserving circuit-level structural insightswithin SAT solvers, providing a robust foundation for future improvements inSAT-solving efficiency and EDA tool design.</description>
      <author>example@mail.com (Jiaying Zhu, Ziyang Zheng, Zhengyuan Shi, Yalun Cai, Qiang Xu)</author>
      <guid isPermaLink="false">2508.04235v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>The Ubiquitous Sparse Matrix-Matrix Products</title>
      <link>http://arxiv.org/abs/2508.04077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;关于稀疏矩阵运算及其在多个领域应用的统一处理研究&lt;h4&gt;背景&lt;/h4&gt;稀疏矩阵与另一个矩阵（密集或稀疏）的乘法是数据科学应用中的基础运算，广泛应用于图算法、稀疏连接神经网络、图神经网络、聚类以及生物测序数据的许多对多比较等领域&lt;h4&gt;目的&lt;/h4&gt;提供稀疏矩阵运算及其丰富应用空间的统一处理方法&lt;h4&gt;方法&lt;/h4&gt;研究在任意代数半环（其中标量运算被用户定义函数重载）和更一般的异构代数（其中输入矩阵的定义域可以不同）上的稀疏矩阵乘法运算&lt;h4&gt;主要发现&lt;/h4&gt;稀疏矩阵乘法可以在不同代数结构上进行，包括具有特定属性的用户定义函数重载的代数半环和具有不同定义域的异构代数&lt;h4&gt;结论&lt;/h4&gt;稀疏矩阵运算及其统一处理方法适用于机器学习、计算生物学和化学、图算法和科学计算等多个领域&lt;h4&gt;翻译&lt;/h4&gt;稀疏矩阵与另一个（密集或稀疏）矩阵的乘法是一种基本运算，它捕捉了许多数据科学应用的计算模式，包括但不限于图算法、稀疏连接神经网络、图神经网络、聚类以及生物测序数据的许多对多比较。在许多应用场景中，矩阵乘法发生在任意代数半环上，其中标量运算被具有特定属性的用户定义函数重载，或者在更一般的异构代数上，甚至输入矩阵的定义域也可以不同。在这里，我们提供了稀疏矩阵运算及其丰富应用空间的统一处理方法，包括机器学习、计算生物学和化学、图算法和科学计算。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multiplication of a sparse matrix with another (dense or sparse) matrix is afundamental operation that captures the computational patterns of many datascience applications, including but not limited to graph algorithms, sparselyconnected neural networks, graph neural networks, clustering, and many-to-manycomparisons of biological sequencing data.  In many application scenarios, the matrix multiplication takes places on anarbitrary algebraic semiring where the scalar operations are overloaded withuser-defined functions with certain properties or a more general heterogenousalgebra where even the domains of the input matrices can be different. Here, weprovide a unifying treatment of the sparse matrix-matrix operation and its richapplication space including machine learning, computational biology andchemistry, graph algorithms, and scientific computing.</description>
      <author>example@mail.com (Aydın Buluç)</author>
      <guid isPermaLink="false">2508.04077v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Probing and Enhancing the Robustness of GNN-based QEC Decoders with Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2508.03783v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 3 figures, Affiliation updated to match user registration&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种使用强化学习代理系统探测图神经网络解码器脆弱性的新框架，并通过对抗训练显著提高了解码器的鲁棒性，为开发更可靠的量子计算解码器提供了新方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为量子错误解码的一种强大数据驱动方法，能直接从综合症数据中学习复杂噪声特征，但这些解码器对细微对抗性扰动的鲁棒性仍是关键问题。&lt;h4&gt;目的&lt;/h4&gt;引入新框架，使用强化学习代理作为对手，系统探测GNN解码器的脆弱性，寻找能导致解码器错误分类的最小综合症修改。&lt;h4&gt;方法&lt;/h4&gt;将框架应用于在谷歌量子AI实验表面码数据上训练的图注意力网络解码器，训练强化学习代理寻找最小综合症修改以造成解码错误。&lt;h4&gt;主要发现&lt;/h4&gt;强化学习代理能成功识别特定关键脆弱性，以最少比特翻转实现高攻击成功率；通过对抗训练（用生成的对抗性例子重新训练模型）可显著提高解码器鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;自动化的脆弱性发现和有针对性重新训练的迭代过程，为开发容错量子计算中的更可靠鲁棒神经网络解码器提供了有前途的方法。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为量子错误校正(QEC)解码的一种强大数据驱动方法，能够直接从综合症数据中学习复杂的噪声特征。然而，这些解码器对细微的、对抗性扰动的鲁棒性仍然是一个关键开放问题。这项工作引入了一种新框架，使用强化学习(RL)代理系统性地探测GNN解码器的脆弱性。RL代理被训练为对手，目标是寻找导致解码器错误分类的最小综合症修改。我们将该框架应用于在谷歌量子AI的实验表面码数据上训练的图注意力网络(GAT)解码器。结果表明，RL代理能够成功识别特定的、关键的脆弱性，以最少的比特翻转次数实现高攻击成功率。此外，我们证明通过对抗训练（使用RL生成的对抗性例子重新训练模型）可以显著提高解码器的鲁棒性。这种自动化的脆弱性发现和有针对性的重新训练的迭代过程，为开发用于容错量子计算的更可靠和更鲁棒的神经网络解码器提供了有前途的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as a powerful, data-driven approachfor Quantum Error Correction (QEC) decoding, capable of learning complex noisecharacteristics directly from syndrome data. However, the robustness of thesedecoders against subtle, adversarial perturbations remains a critical openquestion. This work introduces a novel framework to systematically probe thevulnerabilities of a GNN decoder using a reinforcement learning (RL) agent. TheRL agent is trained as an adversary with the goal of finding minimal syndromemodifications that cause the decoder to misclassify. We apply this framework toa Graph Attention Network (GAT) decoder trained on experimental surface codedata from Google Quantum AI. Our results show that the RL agent cansuccessfully identify specific, critical vulnerabilities, achieving a highattack success rate with a minimal number of bit flips. Furthermore, wedemonstrate that the decoder's robustness can be significantly enhanced throughadversarial training, where the model is retrained on the adversarial examplesgenerated by the RL agent. This iterative process of automated vulnerabilitydiscovery and targeted retraining presents a promising methodology fordeveloping more reliable and robust neural network decoders for fault-tolerantquantum computing.</description>
      <author>example@mail.com (Ryota Ikeda)</author>
      <guid isPermaLink="false">2508.03783v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Do GNN-based QEC Decoders Require Classical Knowledge? Evaluating the Efficacy of Knowledge Distillation from MWPM</title>
      <link>http://arxiv.org/abs/2508.03782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure, 1 table. Affiliation updated to match user  registration&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究测试了知识蒸馏是否能提高量子错误校正解码器中图神经网络(GNN)的性能。研究发现，尽管知识蒸馏模型的测试准确率与基线模型相当，但其训练时间显著增加，表明现代GNN架构能够直接从硬件数据学习错误相关性，无需理论模型指导。&lt;h4&gt;背景&lt;/h4&gt;量子错误校正(QEC)解码器的性能对于实现实用量子计算机至关重要。近年来，图神经网络(GNNs)已成为一种有前景的方法，但其训练方法尚未成熟。&lt;h4&gt;目的&lt;/h4&gt;测试知识蒸馏(将经典算法如最小权完美匹配(MWPM)的理论知识转移到GNNs)是否能有效提高GNN解码器性能的假设。&lt;h4&gt;方法&lt;/h4&gt;比较两个基于图注意力网络(GAT)架构的模型，该架构将时间信息作为节点特征。第一个是纯数据驱动模型(基线)，仅使用真实标签训练；第二个则基于MWPM的理论错误概率融入了知识蒸馏损失。使用Google的公开实验数据进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;知识蒸馏模型的最终测试准确率与基线几乎相同，但其训练损失收敛更慢，训练时间增加了约5倍。&lt;h4&gt;结论&lt;/h4&gt;现代GNN架构具有很高的能力，能够直接从真实硬件数据中高效学习复杂的错误相关性，而无需近似理论模型的指导。&lt;h4&gt;翻译&lt;/h4&gt;量子错误校正(QEC)解码器的性能对于实现实用量子计算机至关重要。近年来，图神经网络(GNNs)已成为一种有前景的方法，但其训练方法尚未成熟。通常预期，将经典算法(如最小权完美匹配MWPM)的理论知识转移到GNNs(一种称为知识蒸馏的技术)可以有效地提高性能。在这项工作中，我们通过严格比较两个基于图注意力网络(GAT)架构的模型来测试这一假设，该架构将时间信息作为节点特征。第一个是纯数据驱动的模型(基线)，仅使用真实标签进行训练，而第二个则基于MWPM的理论错误概率融入了知识蒸馏损失。使用Google的公开实验数据，我们的评估显示，尽管知识蒸馏模型的最终测试准确率与基线几乎相同，但其训练损失收敛更慢，训练时间增加了约5倍。这一结果表明，现代GNN架构具有很高的能力，能够直接从真实硬件数据中高效学习复杂的错误相关性，而无需近似理论模型的指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The performance of decoders in Quantum Error Correction (QEC) is key torealizing practical quantum computers. In recent years, Graph Neural Networks(GNNs) have emerged as a promising approach, but their training methodologiesare not yet well-established. It is generally expected that transferringtheoretical knowledge from classical algorithms like Minimum Weight PerfectMatching (MWPM) to GNNs, a technique known as knowledge distillation, caneffectively improve performance. In this work, we test this hypothesis byrigorously comparing two models based on a Graph Attention Network (GAT)architecture that incorporates temporal information as node features. The firstis a purely data-driven model (baseline) trained only on ground-truth labels,while the second incorporates a knowledge distillation loss based on thetheoretical error probabilities from MWPM. Using public experimental data fromGoogle, our evaluation reveals that while the final test accuracy of theknowledge distillation model was nearly identical to the baseline, its trainingloss converged more slowly, and the training time increased by a factor ofapproximately five. This result suggests that modern GNN architectures possessa high capacity to efficiently learn complex error correlations directly fromreal hardware data, without guidance from approximate theoretical models.</description>
      <author>example@mail.com (Ryota Ikeda)</author>
      <guid isPermaLink="false">2508.03782v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Drone Detection with Event Cameras</title>
      <link>http://arxiv.org/abs/2508.04564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文调查了事件视觉作为解决无人机检测挑战的解决方案。事件相机通过消除运动模糊和在极端光照条件下保持一致检测来克服传统相机的限制。论文涵盖了从数据表示到使用尖峰神经网络的先进处理流程的最先进技术，并讨论了实时跟踪、轨迹预测和通过螺旋桨签名分析进行唯一识别等更复杂的任务，表明事件视觉为下一代反无人机系统提供了强大基础。&lt;h4&gt;背景&lt;/h4&gt;无人机扩散带来了显著的安全挑战。传统监控系统，特别是基于帧的相机，难以可靠检测这些目标，原因包括无人机体积小、高度灵活、运动模糊以及在挑战性光照条件下性能差。&lt;h4&gt;目的&lt;/h4&gt;调查事件视觉这一新兴领域作为无人机检测问题的稳健解决方案，展示该技术如何为下一代可靠、低延迟和高效的反无人机系统提供基础。&lt;h4&gt;方法&lt;/h4&gt;回顾基于事件的无人机检测的最先进技术，从数据表示方法到使用尖峰神经网络的高级处理流程。讨论扩展到实时跟踪、轨迹预测和通过螺旋桨签名分析进行唯一识别等更复杂的任务。&lt;h4&gt;主要发现&lt;/h4&gt;事件相机几乎消除了运动模糊，使在极端光照条件下保持一致的检测成为可能。它们的稀疏、异步输出抑制静态背景，实现对运动线索的低延迟关注。&lt;h4&gt;结论&lt;/h4&gt;通过检查当前方法论、可用数据集和技术的独特优势，论文证明了事件视觉为可靠、低延迟和高效的反无人机系统提供了强大基础。&lt;h4&gt;翻译&lt;/h4&gt;无人机的扩散带来了显著的安全和挑战。传统的监控系统，特别是传统的基于帧的相机，由于无人机体积小、高度灵活以及由此产生的运动模糊和在挑战性光照条件下性能差，难以可靠地检测这些目标。本文调查了事件视觉这一新兴领域作为这些问题的稳健解决方案。事件相机几乎消除了运动模糊，使在极端光照条件下保持一致的检测成为可能。它们稀疏、异步的输出抑制静态背景，实现对运动线索的低延迟关注。我们回顾了基于事件的无人机检测的最先进技术，从数据表示方法到使用尖峰神经网络的先进处理流程。讨论超越了简单的检测，涵盖了实时跟踪、轨迹预测和通过螺旋桨签名分析进行唯一识别等更复杂的任务。通过检查当前的方法论、可用的数据集和技术的独特优势，这项工作证明了事件视觉为下一代可靠、低延迟和高效的反无人机系统提供了强大的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The diffusion of drones presents significant security and safety challenges.Traditional surveillance systems, particularly conventional frame-basedcameras, struggle to reliably detect these targets due to their small size,high agility, and the resulting motion blur and poor performance in challenginglighting conditions. This paper surveys the emerging field of event-basedvision as a robust solution to these problems. Event cameras virtuallyeliminate motion blur and enable consistent detection in extreme lighting.Their sparse, asynchronous output suppresses static backgrounds, enablinglow-latency focus on motion cues. We review the state-of-the-art in event-baseddrone detection, from data representation methods to advanced processingpipelines using spiking neural networks. The discussion extends beyond simpledetection to cover more sophisticated tasks such as real-time tracking,trajectory forecasting, and unique identification through propeller signatureanalysis. By examining current methodologies, available datasets, and thedistinct advantages of the technology, this work demonstrates that event-basedvision provides a powerful foundation for the next generation of reliable,low-latency, and efficient counter-UAV systems.</description>
      <author>example@mail.com (Gabriele Magrini, Lorenzo Berlincioni, Luca Cultrera, Federico Becattini, Pietro Pala)</author>
      <guid isPermaLink="false">2508.04564v1</guid>
      <pubDate>Thu, 07 Aug 2025 14:44:01 +0800</pubDate>
    </item>
    <item>
      <title>Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis</title>
      <link>http://arxiv.org/abs/2508.00381v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种创新的焊接缺陷检测方法，结合了自适应神经网络架构和可解释AI技术，通过专家验证和可信AI原则，确保了检测系统的可靠性和透明度，解决了传统检测方法的局限性，提高了在关键环境中的安全性和可靠性。&lt;h4&gt;背景&lt;/h4&gt;焊接缺陷检测对油气行业管道系统的安全性和可靠性至关重要，特别是在具有挑战性的海洋和近海环境中。传统无损检测方法常无法检测到细微或内部缺陷，导致潜在故障和昂贵停机时间。现有基于神经网络的缺陷分类方法依赖任意选择的预训练架构，缺乏可解释性，引发部署安全担忧。&lt;h4&gt;目的&lt;/h4&gt;解决传统检测方法的局限性，提高焊接缺陷检测的准确性和可靠性，增强系统的可解释性和透明度，建立可信的自动化决策系统。&lt;h4&gt;方法&lt;/h4&gt;提出'Adapt-WeldNet'自适应框架，系统评估各种预训练架构、迁移学习策略和自适应优化器；开发缺陷检测可解释性分析(DDIA)框架，使用Grad-CAM和LIME等可解释AI技术，结合ASNT NDE II级专业人士的领域评估，采用人机循环方法并遵循可信AI原则。&lt;h4&gt;主要发现&lt;/h4&gt;Adapt-WeldNet能优化缺陷检测并提供可操作的见解；DDIA框架增强系统透明度；结合专家验证确保检测系统的可靠性、公平性和问责制；提高性能和可解释性，增强信任度。&lt;h4&gt;结论&lt;/h4&gt;通过提高性能和可解释性，这项工作增强了焊接缺陷检测系统的信任度、安全性和可靠性，支持海洋和近海环境中的关键操作。&lt;h4&gt;翻译&lt;/h4&gt;焊接缺陷检测对确保油气行业管道系统的安全性和可靠性至关重要，特别是在具有挑战性的海洋和近海环境中。传统无损检测方法常常无法检测到细微或内部缺陷，导致潜在故障和昂贵的停机时间。此外，现有的基于神经网络的缺陷分类方法通常依赖于任意选择的预训练架构，缺乏可解释性，引发了部署安全担忧。为应对这些挑战，本文介绍了'Adapt-WeldNet'，这是一种用于焊接缺陷检测的自适应框架，系统评估各种预训练架构、迁移学习策略和自适应优化器，以确定性能最佳模型和超参数，优化缺陷检测并提供可操作的见解。此外，还提出了新颖的缺陷检测可解释性分析(DDIA)框架，以增强系统透明度。DDIA采用可解释AI技术，如Grad-CAM和LIME，并结合经过认证的ASNT NDE II级专业人士验证的领域特定评估。采用人机循环方法并遵循可信AI原则，DDIA确保了缺陷检测系统的可靠性、公平性和问责制，通过专家验证培养对自动化决策的信心。通过提高性能和可解释性，这项工作增强了焊接缺陷检测系统的信任度、安全性和可靠性，支持海洋和近海环境中的关键操作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Weld defect detection is crucial for ensuring the safety and reliability ofpiping systems in the oil and gas industry, especially in challenging marineand offshore environments. Traditional non-destructive testing (NDT) methodsoften fail to detect subtle or internal defects, leading to potential failuresand costly downtime. Furthermore, existing neural network-based approaches fordefect classification frequently rely on arbitrarily selected pretrainedarchitectures and lack interpretability, raising safety concerns fordeployment. To address these challenges, this paper introduces``Adapt-WeldNet", an adaptive framework for welding defect detection thatsystematically evaluates various pre-trained architectures, transfer learningstrategies, and adaptive optimizers to identify the best-performing model andhyperparameters, optimizing defect detection and providing actionable insights.Additionally, a novel Defect Detection Interpretability Analysis (DDIA)framework is proposed to enhance system transparency. DDIA employs ExplainableAI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specificevaluations validated by certified ASNT NDE Level II professionals.Incorporating a Human-in-the-Loop (HITL) approach and aligning with theprinciples of Trustworthy AI, DDIA ensures the reliability, fairness, andaccountability of the defect detection system, fostering confidence inautomated decisions through expert validation. By improving both performanceand interpretability, this work enhances trust, safety, and reliability inwelding defect detection systems, supporting critical operations in offshoreand marine environments.</description>
      <author>example@mail.com (Kamal Basha S, Athira Nambiar)</author>
      <guid isPermaLink="false">2508.00381v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
  <item>
      <title>Correspondence-Free Fast and Robust Spherical Point Pattern Registration</title>
      <link>http://arxiv.org/abs/2508.02339v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种具有线性时间复杂度的球面模式旋转估计算法，通过将球面模式表示为离散3D点集，重新表述为球面点集对齐问题，并引入了三种新算法。&lt;h4&gt;背景&lt;/h4&gt;现有球面旋转估计方法基于球面函数交叉相关最大化，计算复杂度大于O(n³)，且在异常值污染下缺乏充分评估。&lt;h4&gt;目的&lt;/h4&gt;开发一种具有线性时间复杂度O(n)的球面模式间旋转估计算法，提高计算效率和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;将球面模式显式表示为单位球面上的离散3D点集，将旋转估计重新表述为球面点集对齐问题（3D单位向量的Wahba问题），并提出了三种新算法：SPMC、FRS和SPMC+FRS混合方法。&lt;h4&gt;主要发现&lt;/h4&gt;在S²域和无对应关系设置下，所提算法比当前最先进方法快10倍以上，在存在异常值的情况下准确度高10倍以上，通过'鲁棒向量对齐数据集'上的广泛模拟得到验证。&lt;h4&gt;结论&lt;/h4&gt;该方法已成功应用于点云配准和球面图像旋转估计两个实际任务中。&lt;h4&gt;翻译&lt;/h4&gt;现有球面模式间的旋转估计方法通常依赖于两个球面函数之间的球面交叉相关最大化。然而，这些方法在旋转空间离散化上的计算复杂度大于立方级，并且在大量异常值污染下缺乏广泛评估。为此，我们提出了一种具有线性时间复杂度的球面模式间旋转估计算法。与现有的基于球面函数的方法不同，我们将球面模式显式表示为单位球面上的离散3D点集，将旋转估计重新表述为球面点集对齐问题（即3D单位向量的Wahba问题）。基于我们的几何表述，球面模式对齐算法自然地与3D单位向量的Wahba问题框架相契合。具体而言，我们引入了三种新算法：(1) 基于相关的球面模式匹配，(2) 快速旋转搜索，以及(3) 结合前两种方法优势的混合方法。我们的实验表明，在球面域和无对应关系设置下，我们的算法比当前解决Wahba问题的最先进方法快10倍以上，且在存在异常值的情况下准确度高10倍以上。我们通过在新的球面模式数据集'鲁棒向量对齐数据集'上的广泛模拟验证了我们的方法。此外，我们将我们的方法适应到两个实际任务：(i) 点云配准和(ii) 球面图像的旋转估计。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决球面点模式之间的旋转估计问题，即在单位球面上找到两个点集之间的最优旋转矩阵，使一个点集旋转后能与另一个点集对齐。这个问题在现实中非常重要，因为它与Wahba问题和正交Procrustes问题密切相关，广泛应用于姿态估计、点云配准、图像拼接、3D重建和机器人等领域。现有方法计算复杂度高（大于O(n^3)），难以处理实时应用，且在高旋转误差和异常值情况下表现不佳，限制了这些技术的实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于球面互相关方法的局限性，特别是高计算复杂度和对大旋转误差的不适应性。然后，他们将球面模式重新表示为单位球面上的离散3D点集，将问题转化为球面点集对齐问题（即3D单位向量的Wahba问题）。作者借鉴了现有工作中的Wahba问题理论基础、球面坐标系统、直方图表示和交叉相关等概念，但创新性地提出了三种新算法（SPMC、FRS和SPMC+FRS）和2D直方图表示方法，利用均值方向和北方向来对齐点集，并通过1D圆形交叉相关来估计旋转。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将球面上的点集转换为2D直方图表示，利用均值方向和北方向对齐点集，使用1D圆形交叉相关来估计旋转，并通过迭代优化提高准确性。整体实现流程（以SPMC算法为例）：1) 计算每个点集的旋转矩阵，使它们的均值方向与北极方向对齐；2) 使用旋转矩阵将点集旋转到与北极对齐的位置；3) 从对齐的点集计算二元直方图；4) 在固定直方图和移动直方图之间执行1D圆形交叉相关，找到最优偏移量；5) 计算所需的旋转矩阵；6) 计算最优旋转矩阵。FRS算法则是迭代优化方法，每一步都计算中间旋转矩阵并更新源点集，直到达到收敛。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出三种新算法（SPMC、FRS和SPMC+FRS）；2) 实现线性时间复杂度O(n)，而现有方法大于O(n^3)；3) 创建'Robust Vector Alignment Dataset'数据集；4) 提出'Centroid Aware Spherical Embedding'方法；5) 提出球面图像转换为球面点云的新方法。相比之前的工作，本文方法在计算效率上显著提升，能够处理高达90%的异常值情况，适用于整个SO(3)旋转空间，并成功扩展到点云配准和球面图像旋转估计等实际应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种计算复杂度为O(n)的球面点模式旋转估计方法，相比现有方法在速度和准确性上都有显著提升，并能有效处理噪声和异常值情况，同时扩展到了点云配准和球面图像旋转估计等实际应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing methods for rotation estimation between two spherical($\mathbb{S}^2$) patterns typically rely on spherical cross-correlationmaximization between two spherical function. However, these approaches exhibitcomputational complexities greater than cubic $O(n^3)$ with respect to rotationspace discretization and lack extensive evaluation under significant outliercontamination. To this end, we propose a rotation estimation algorithm betweentwo spherical patterns with linear time complexity $O(n)$. Unlike existingspherical-function-based methods, we explicitly represent spherical patterns asdiscrete 3D point sets on the unit sphere, reformulating rotation estimation asa spherical point-set alignment (i.e., Wahba problem for 3D unit vectors).Given the geometric nature of our formulation, our spherical pattern alignmentalgorithm naturally aligns with the Wahba problem framework for 3D unitvectors. Specifically, we introduce three novel algorithms: (1) SPMC (SphericalPattern Matching by Correlation), (2) FRS (Fast Rotation Search), and (3) ahybrid approach (SPMC+FRS) that combines the advantages of the previous twomethods. Our experiments demonstrate that in the $\mathbb{S}^2$ domain and incorrespondence-free settings, our algorithms are over 10x faster and over 10xmore accurate than current state-of-the-art methods for the Wahba problem withoutliers. We validate our approach through extensive simulations on a newdataset of spherical patterns, the ``Robust Vector Alignment Dataset."Furthermore, we adapt our methods to two real-world tasks: (i) Point CloudRegistration (PCR) and (ii) rotation estimation for spherical images.</description>
      <author>example@mail.com (Anik Sarker, Alan T. Asbeck)</author>
      <guid isPermaLink="false">2508.02339v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>evTransFER: A Transfer Learning Framework for Event-based Facial Expression Recognition</title>
      <link>http://arxiv.org/abs/2508.03609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为evTransFER的基于迁移学习的框架和架构，用于基于事件相机的面部表情识别，实现了93.6%的识别率，比现有方法提高了25.9个百分点以上。&lt;h4&gt;背景&lt;/h4&gt;事件相机是一种受生物启发的视觉传感器，能够异步捕捉像素强度变化，具有微秒级延迟、高时间分辨率和高动态范围，为场景的时空动态信息提供了有价值的输入。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于迁移学习的框架和架构，用于基于事件相机的面部表情识别。&lt;h4&gt;方法&lt;/h4&gt;设计了一个特征提取器编码面部时空动态，通过在面部重建问题上训练对抗生成方法并将编码器权重迁移到表情识别系统；提出结合LSTM的架构捕获长期表情动态；引入新的基于事件的TIE表示方法。&lt;h4&gt;主要发现&lt;/h4&gt;迁移学习方法显著提高了面部表情识别能力；在e-CK+数据库上达到93.6%的识别率；与最先进方法相比准确率提高25.9个百分点以上。&lt;h4&gt;结论&lt;/h4&gt;提出的evTransFER框架在基于事件的面部表情识别任务中取得了显著成果，性能明显优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;事件相机是受生物启发的视觉传感器，可异步捕捉像素强度变化，具有微秒级延迟、高时间分辨率和高动态范围，为场景的时空动态信息提供了有价值的输入。在本文工作中，我们提出了evTransFER，一个基于迁移学习的框架和架构，用于基于事件相机的面部表情识别。主要贡献是一个特征提取器，用于编码面部的时空动态，通过在一个不同问题（面部重建）上训练对抗生成方法，然后将训练好的编码器权重迁移到面部表情识别系统。我们表明，与从头开始训练网络相比，这种提出的迁移学习方法大大提高了识别面部表情的能力。此外，我们提出了一种结合LSTM的架构，用于捕获长期的面部表情动态，并引入了一种新的基于事件的表示方法，称为TIE，这些都进一步提高了结果。我们在基于事件的面部表情数据库e-CK+上评估了提出的框架，并将其与最先进的方法进行比较。结果表明，提出的evTransFER框架在e-CK+数据库上实现了93.6%的识别率，与类似问题中最先进的性能相比，显著提高了准确性（25.9个百分点或更多）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event-based cameras are bio-inspired vision sensors that asynchronouslycapture per-pixel intensity changes with microsecond latency, high temporalresolution, and high dynamic range, providing valuable information about thespatio-temporal dynamics of the scene. In the present work, we proposeevTransFER, a transfer learning-based framework and architecture for faceexpression recognition using event-based cameras. The main contribution is afeature extractor designed to encode the spatio-temporal dynamics of faces,built by training an adversarial generative method on a different problem(facial reconstruction) and then transferring the trained encoder weights tothe face expression recognition system. We show that this proposed transferlearning method greatly improves the ability to recognize facial expressionscompared to training a network from scratch. In addition, we propose anarchitecture that incorporates an LSTM to capture longer-term facial expressiondynamics, and we introduce a new event-based representation, referred to asTIE, both of which further improve the results. We evaluate the proposedframework on the event-based facial expression database e-CK+ and compare it tostate-of-the-art methods. The results show that the proposed frameworkevTransFER achieves a 93.6\% recognition rate on the e-CK+ database,significantly improving the accuracy (25.9\% points or more) when compared tostate-of-the-art performance for similar problems.</description>
      <author>example@mail.com (Rodrigo Verschae, Ignacio Bugueno-Cordova)</author>
      <guid isPermaLink="false">2508.03609v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Predicting EGFR Mutation in LUAD from Histopathological Whole-Slide Images Using Pretrained Foundation Model and Transfer Learning: An Indian Cohort Study</title>
      <link>http://arxiv.org/abs/2508.01352v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 4 figures and 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于深度学习的框架，用于从H&amp;E染色全幻灯图像预测EGFR突变状态，结合视觉变换器病理基础模型和基于注意力的多实例学习架构，在印度队列和TCGA数据集上均表现出色。&lt;h4&gt;背景&lt;/h4&gt;肺腺癌是非小细胞肺癌的一种亚型，约46%的LUAD患者存在EGFR基因突变，这类患者可使用特定酪氨酸激酶抑制剂治疗。东南亚人群EGFR突变发生率显著高于高加索人(39-64% vs 7-22%)。H&amp;E染色全幻灯成像常规用于癌症分期和分型筛查，AI模型在癌症检测和分类方面显示出良好前景。&lt;h4&gt;目的&lt;/h4&gt;开发一个深度学习框架来预测EGFR突变状态，以辅助临床决策。&lt;h4&gt;方法&lt;/h4&gt;构建了一个基于视觉变换器病理基础模型和基于注意力的多实例学习架构的深度学习框架。使用印度队列(170个WSI)的数据进行训练，并在两个独立数据集上评估：内部测试集(印度队列中的30个WSI)和外部测试集(TCGA的86个WSI)。&lt;h4&gt;主要发现&lt;/h4&gt;模型在两个数据集上均表现出一致的性能，内部测试集AUC为0.933(±0.010)，外部测试集AUC为0.965(±0.015)。该框架可在小数据集上高效训练，与先前研究相比实现了优越性能，不受训练领域限制。&lt;h4&gt;结论&lt;/h4&gt;研究证明了使用常规病理幻灯片准确预测EGFR突变状态的可行性，特别是在资源有限条件下，使用基础模型和基于注意力的多实例学习方法。&lt;h4&gt;翻译&lt;/h4&gt;肺腺癌(LUAD)是非小细胞肺癌(NSCLC)的一种亚型。EGFR基因突变的LUAD约占LUAD病例的46%。携带EGFR突变的患者可以用特异性酪氨酸激酶抑制剂(TKIs)治疗。因此，预测EGFR突变状态有助于临床决策。H&amp;E染色全幻灯成像(WSI)是常规进行的癌症分期和分型筛查程序，特别是对东南亚人群，其突变发生率明显高于高加索人(39-64%比7-22%)。最近AI模型的进展在癌症检测和分类方面显示出有希望的结果。在本研究中，我们提出了一种基于视觉变换器(ViT)病理基础模型和基于注意力的多实例学习(ABMIL)架构的深度学习(DL)框架，用于从H&amp;E WSI预测EGFR突变状态。开发的管道使用印度队列(170个WSI)的数据进行训练，并在两个独立数据集上进行评估：内部测试集(印度队列中的30个WSI)和来自TCGA的外部测试集(86个WSI)。模型在两个数据集上均表现出一致的性能，内部和外部测试集的AUC分别为0.933(±0.010)和0.965(±0.015)。该框架可以在小数据集上高效训练，与先前研究相比实现了优越的性能，无论训练领域如何。当前研究证明了使用常规病理幻灯片准确预测EGFR突变状态的可行性，特别是在资源有限的条件下，使用基础模型和基于注意力的多实例学习方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lung adenocarcinoma (LUAD) is a subtype of non-small cell lung cancer(NSCLC). LUAD with mutation in the EGFR gene accounts for approximately 46% ofLUAD cases. Patients carrying EGFR mutations can be treated with specifictyrosine kinase inhibitors (TKIs). Hence, predicting EGFR mutation status canhelp in clinical decision making. H&amp;E-stained whole slide imaging (WSI) is aroutinely performed screening procedure for cancer staging and subtyping,especially affecting the Southeast Asian populations with significantly higherincidence of the mutation when compared to Caucasians (39-64% vs 7-22%). Recentprogress in AI models has shown promising results in cancer detection andclassification. In this study, we propose a deep learning (DL) framework builton vision transformers (ViT) based pathology foundation model andattention-based multiple instance learning (ABMIL) architecture to predict EGFRmutation status from H&amp;E WSI. The developed pipeline was trained using datafrom an Indian cohort (170 WSI) and evaluated across two independent datasets:Internal test (30 WSI from Indian cohort) set, and an external test set fromTCGA (86 WSI). The model shows consistent performance across both datasets,with AUCs of 0.933 (+/-0.010), and 0.965 (+/-0.015) for the internal andexternal test sets respectively. This proposed framework can be efficientlytrained on small datasets, achieving superior performance as compared toseveral prior studies irrespective of training domain. The current studydemonstrates the feasibility of accurately predicting EGFR mutation statususing routine pathology slides, particularly in resource-limited settings usingfoundation models and attention-based multiple instance learning.</description>
      <author>example@mail.com (Sagar Singh Gwal, Rajan, Suyash Devgan, Shraddhanjali Satapathy, Abhishek Goyal, Nuruddin Mohammad Iqbal, Vivaan Jain, Prabhat Singh Mallik, Deepali Jain, Ishaan Gupta)</author>
      <guid isPermaLink="false">2508.01352v2</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>MPCA-based Domain Adaptation for Transfer Learning in Ultrasonic Guided Waves</title>
      <link>http://arxiv.org/abs/2508.02726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于多线性主成分分析(MPCA)的新型迁移学习框架，用于解决超声导波结合机器学习在结构健康监测中面临的数据稀缺和泛化能力有限的问题。&lt;h4&gt;背景&lt;/h4&gt;超声导波是薄壁结构健康监测中有前景的诊断工具，与机器学习结合可实现实时监测。然而，基于超声导波的机器学习方法大规模部署受数据稀缺限制，且在不同材料和传感器配置间泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;解决基于超声导波的机器学习方法在数据稀缺和跨材料/传感器配置泛化能力有限的问题，开发有效的域适应迁移学习框架。&lt;h4&gt;方法&lt;/h4&gt;首先训练用于回归的卷积神经网络实现平板结构损伤定位，然后结合多线性主成分分析和微调技术使CNN适应不同平板。通过对源域和目标域联合应用MPCA提取共享特征，实现无需预先假设维度的有效域适应。最后在12个涉及不同复合材料和传感器阵列的案例中测试该方法。&lt;h4&gt;主要发现&lt;/h4&gt;统计指标显示MPCA应用前后域对齐情况显著改善，与标准迁移学习技术相比，所提方法大幅降低了定位误差，在多种复合材料和传感器配置的案例中表现出色。&lt;h4&gt;结论&lt;/h4&gt;提出的方法是一种鲁棒、数据高效且基于统计的迁移学习框架，适用于超声导波为基础的结构健康监测应用。&lt;h4&gt;翻译&lt;/h4&gt;超声导波代表了一种用于薄壁结构健康监测的有前景的诊断工具，它们与机器学习算法的结合日益增多，以实现实时监测能力。然而，基于超声导波的机器学习方法的大规模部署受到数据稀缺和跨不同材料及传感器配置泛化能力有限的约束。为解决这些局限性，这项工作提出了一种基于多线性主成分分析的新型迁移学习框架。首先，训练一个用于回归的卷积神经网络来执行平板结构的损伤定位。然后，结合多线性和微调使CNN能够应用于不同的平板。通过对源域和目标域联合应用多线性，该方法提取共享的潜在特征，实现了无需预先假设维度的有效域适应。在多线性之后，微调使预训练的CNN能够适应新域，而无需大量训练数据。所提出的多线性基础迁移学习方法在12个涉及不同复合材料和传感器阵列的案例研究中进行了测试。使用统计指标评估了多线性前后的域对齐情况，结果表明与标准迁移学习技术相比，定位误差显著降低。因此，所提出的方法成为一种适用于超声导波为基础的健康监测的鲁棒、数据高效和基于统计的迁移学习框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ultrasonic Guided Waves (UGWs) represent a promising diagnostic tool forStructural Health Monitoring (SHM) in thin-walled structures, and theirintegration with machine learning (ML) algorithms is increasingly being adoptedto enable real-time monitoring capabilities. However, the large-scaledeployment of UGW-based ML methods is constrained by data scarcity and limitedgeneralisation across different materials and sensor configurations. To addressthese limitations, this work proposes a novel transfer learning (TL) frameworkbased on Multilinear Principal Component Analysis (MPCA). First, aConvolutional Neural Network (CNN) for regression is trained to perform damagelocalisation for a plated structure. Then, MPCA and fine-tuning are combined tohave the CNN work for a different plate. By jointly applying MPCA to the sourceand target domains, the method extracts shared latent features, enablingeffective domain adaptation without requiring prior assumptions aboutdimensionality. Following MPCA, fine-tuning enables adapting the pre-trainedCNN to a new domain without the need for a large training dataset. The proposedMPCA-based TL method was tested against 12 case studies involving differentcomposite materials and sensor arrays. Statistical metrics were used to assessdomains alignment both before and after MPCA, and the results demonstrate asubstantial reduction in localisation error compared to standard TL techniques.Hence, the proposed approach emerges as a robust, data-efficient, andstatistically based TL framework for UGW-based SHM.</description>
      <author>example@mail.com (Lucio Pinello, Francesco Cadini, Luca Lomazzi)</author>
      <guid isPermaLink="false">2508.02726v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Transfer Learning Methods on Real-World Data Streams: A Case Study in Financial Fraud Detection</title>
      <link>http://arxiv.org/abs/2508.02702v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 7 figures, submitted to ECML PKDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种数据操作框架，用于模拟随时间变化的数据可用性场景，支持迁移学习算法在动态环境下的更真实评估和比较。该框架通过重新采样数据集创建多个领域，并应用真实的领域转换引入领域间变异性，从而模拟大量现实的实验变体。&lt;h4&gt;背景&lt;/h4&gt;当目标领域的数据有限时，迁移学习方法可用于在相关数据丰富的领域开发模型后部署到目标领域。然而，这些方法通常基于特定的静态假设设计，与许多实际应用中数据和标签可用性随时间变化的情况不符。现有评估方法在静态假设下进行，导致对模型实际性能的不切实际期望。&lt;h4&gt;目的&lt;/h4&gt;为了支持迁移学习算法和模型更真实、实际的评估和比较，提出一个数据操作框架，该框架能够模拟随时间变化的数据可用性场景，创建多个领域，并通过应用真实的领域转换引入领域间变异性。&lt;h4&gt;方法&lt;/h4&gt;提出一个数据操作框架，具有三种能力：(1)模拟随时间变化的数据可用性场景，(2)通过重新采样给定数据集创建多个领域，(3)通过应用真实的领域转换（如创建时间协变量和概念偏移）引入领域间变异性。&lt;h4&gt;主要发现&lt;/h4&gt;通过在专有卡支付数据集和公开的银行账户欺诈(BAF)数据集上进行案例研究，证明了框架的有用性。该框架通过提供随时间和现实数据可用性场景下评估迁移学习方法的能力，促进了对模型和算法行为的理解。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架通过模拟大量现实的实验变体，提供了关于算法在动态环境中潜在行为的更多信息，支持对迁移学习算法更真实、实际的评估和比较，有助于在实际环境中为新领域部署模型时做出更好的决策。&lt;h4&gt;翻译&lt;/h4&gt;当目标领域的数据有限时，可以使用迁移学习(TL)方法在相关数据丰富的领域开发模型，然后在目标领域部署。然而，这些迁移学习方法通常基于对可用标记和未标记目标数据数量的特定、静态假设设计。这与许多实际应用形成对比，因为在实际应用中，数据和相应标签的可用性会随时间变化。由于迁移学习方法的评估通常也在相同静态数据可用性假设下进行，这会导致对其在实际环境中性能的不切实际的期望。为了支持迁移学习算法和模型更真实、实际的评估和比较，我们提出了一种数据操作框架，该框架(1)模拟随时间变化的数据可用性场景，(2)通过重新采样给定数据集创建多个领域，(3)通过应用真实的领域转换（例如创建各种可能的时间协变量和概念偏移）引入领域间变异性。这些能力使能够模拟大量现实的实验变体，进而提供有关算法在动态环境中部署时潜在行为的更多信息。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When the available data for a target domain is limited, transfer learning(TL) methods can be used to develop models on related data-rich domains, beforedeploying them on the target domain. However, these TL methods are typicallydesigned with specific, static assumptions on the amount of available labeledand unlabeled target data. This is in contrast with many real worldapplications, where the availability of data and corresponding labels variesover time. Since the evaluation of the TL methods is typically also performedunder the same static data availability assumptions, this would lead tounrealistic expectations concerning their performance in real world settings.To support a more realistic evaluation and comparison of TL algorithms andmodels, we propose a data manipulation framework that (1) simulates varyingdata availability scenarios over time, (2) creates multiple domains throughresampling of a given dataset and (3) introduces inter-domain variability byapplying realistic domain transformations, e.g., creating a variety ofpotentially time-dependent covariate and concept shifts. These capabilitiesenable simulation of a large number of realistic variants of the experiments,in turn providing more information about the potential behavior of algorithmswhen deployed in dynamic settings. We demonstrate the usefulness of theproposed framework by performing a case study on a proprietary real-world suiteof card payment datasets. Given the confidential nature of the case study, wealso illustrate the use of the framework on the publicly available Bank AccountFraud (BAF) dataset. By providing a methodology for evaluating TL methods overtime and in realistic data availability scenarios, our framework facilitatesunderstanding of the behavior of models and algorithms. This leads to betterdecision making when deploying models for new domains in real-worldenvironments.</description>
      <author>example@mail.com (Ricardo Ribeiro Pereira, Jacopo Bono, Hugo Ferreira, Pedro Ribeiro, Carlos Soares, Pedro Bizarro)</author>
      <guid isPermaLink="false">2508.02702v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation</title>
      <link>http://arxiv.org/abs/2507.21455v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in the Proceedings of the International Conference on  Learning Representations (ICLR 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为'自监督数据集蒸馏'的方法，通过创新参数化、预增强处理和轻量级网络建模技术，有效提取真实数据集信息，产生具有更好跨架构泛化能力的压缩数据集，在多个实验中表现出优越性。&lt;h4&gt;背景&lt;/h4&gt;大型深度模型的训练需要更大的数据集，但数据集规模的快速增长带来了显著的训练成本挑战，甚至导致计算费用过高。&lt;h4&gt;目的&lt;/h4&gt;旨在将图像及其自监督训练的表示蒸馏成一个压缩的数据集，有效提取真实数据集的丰富信息，产生具有更好跨架构泛化能力的蒸馏数据集。&lt;h4&gt;方法&lt;/h4&gt;提出了三种新技术：1) 通过不同的低维基对图像和表示进行创新参数化；2) 通过预增强处理解决数据增强随机性引起的不稳定性问题；3) 利用轻量级网络建模同一图像增强视图表示之间的连接，产生更紧凑的蒸馏对。&lt;h4&gt;主要发现&lt;/h4&gt;在各种数据集上进行的广泛实验验证了该方法在蒸馏效率、跨架构泛化和迁移学习性能方面的优越性。&lt;h4&gt;结论&lt;/h4&gt;自监督数据集蒸馏是一种有效的方法，可以提取真实数据集的丰富信息，所提出的技术能够更忠实和紧凑地保留原始数据集的关键特征，在多个方面表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;虽然大型数据集对于训练大型深度模型至关重要，但数据集规模的快速增长带来了显著的训练成本挑战，甚至导致计算费用过高。数据集蒸馏最近成为一种流行技术，通过学习高度紧凑的代表样本集来减少数据集大小，其中用这些样本训练的理想模型应与使用完整数据集训练的模型具有相当的性能。然而，大多数现有的数据集蒸馏工作都集中在监督数据集上，而我们则旨在将图像及其自监督训练的表示蒸馏成一个压缩集。这一过程被称为自监督数据集蒸馏，有效提取了真实数据集的丰富信息，产生了具有增强跨架构泛化能力的压缩集。特别是，为了更忠实和紧凑地保留原始数据集的关键特征，我们提出了几种新技术：1) 我们通过不同的低维基对图像和表示进行创新参数化，实验表明基的选择起着关键作用；2) 我们通过预增强处理解决了自监督学习中关键组件但被先前自监督数据集蒸馏工作低估的数据增强随机性引起的不稳定性问题；3) 我们进一步利用轻量级网络建模同一图像增强视图表示之间的连接，产生更紧凑的蒸馏对。在各种数据集上进行的广泛实验验证了我们的方法在蒸馏效率、跨架构泛化和迁移学习性能方面的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although larger datasets are crucial for training large deep models, therapid growth of dataset size has brought a significant challenge in terms ofconsiderable training costs, which even results in prohibitive computationalexpenses. Dataset Distillation becomes a popular technique recently to reducethe dataset size via learning a highly compact set of representative exemplars,where the model trained with these exemplars ideally should have comparableperformance with respect to the one trained with the full dataset. While mostof existing works upon dataset distillation focus on supervised datasets, weinstead aim to distill images and their self-supervisedly trainedrepresentations into a distilled set. This procedure, named as Self-SupervisedDataset Distillation, effectively extracts rich information from real datasets,yielding the distilled sets with enhanced cross-architecture generalizability.Particularly, in order to preserve the key characteristics of original datasetmore faithfully and compactly, several novel techniques are proposed: 1) weintroduce an innovative parameterization upon images and representations viadistinct low-dimensional bases, where the base selection for parameterizationis experimentally shown to play a crucial role; 2) we tackle the instabilityinduced by the randomness of data augmentation -- a key component inself-supervised learning but being underestimated in the prior work ofself-supervised dataset distillation -- by utilizing predeterminedaugmentations; 3) we further leverage a lightweight network to model theconnections among the representations of augmented views from the same image,leading to more compact pairs of distillation. Extensive experiments conductedon various datasets validate the superiority of our approach in terms ofdistillation efficiency, cross-architecture generalization, and transferlearning performance.</description>
      <author>example@mail.com (Sheng-Feng Yu, Jia-Jiun Yao, Wei-Chen Chiu)</author>
      <guid isPermaLink="false">2507.21455v2</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Minimal Convolutional RNNs Accelerate Spatiotemporal Learning</title>
      <link>http://arxiv.org/abs/2508.03614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICANN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MinConvLSTM和MinConvGRU两种新型时空模型，结合了卷积网络的空间特征和最小可并行RNN的训练效率，实现了完全并行训练并保留了局部空间建模能力，在训练速度和预测精度上都优于传统方法。&lt;h4&gt;背景&lt;/h4&gt;传统ConvRNN模型在教师强制训练过程中需要顺序隐藏状态更新，这是一个主要瓶颈，限制了训练效率。&lt;h4&gt;目的&lt;/h4&gt;开发结合卷积网络空间归纳偏置和最小可并行RNN训练效率的时空模型，消除顺序更新的需求，提高训练效率。&lt;h4&gt;方法&lt;/h4&gt;将MinLSTM和MinGRU的对数域前缀和公式扩展到卷积架构，实现完全并行训练；在MinConvLSTM中融入受xLSTM启发的指数门控机制，简化对数域计算。&lt;h4&gt;主要发现&lt;/h4&gt;在Navier-Stokes动力学和实际地势数据两个时空预测任务上，新模型在训练速度上显著优于标准ConvLSTMs和ConvGRUs，同时实现了更低的预测误差，即使在闭环自回归模式下也是如此。&lt;h4&gt;结论&lt;/h4&gt;最小化的循环结构与卷积输入相结合为时空序列建模提供了高效且有吸引力的选择，弥合了循环简单性和空间复杂性之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了MinConvLSTM和MinConvGRU，两种结合了卷积循环网络空间归纳偏置与最小可并行化RNN训练效率的新型时空模型。我们的方法将MinLSTM和MinGRU的对数域前缀和公式扩展到卷积架构，实现了完全并行训练，同时保留了局部空间建模能力。这消除了教师强制训练过程中顺序隐藏状态更新的需求——这是传统ConvRNN模型的主要瓶颈。此外，我们在MinConvLSTM中融入了受xLSTM架构启发的指数门控机制，进一步简化了对数域计算。我们的模型结构最小化且计算效率高，参数数量减少，可扩展性提高。我们在两个时空预测任务上评估了我们的模型：Navier-Stokes动力学和实际地势数据。在训练速度方面，我们的架构显著优于标准的ConvLSTMs和ConvGRUs。此外，我们的模型在两个领域中都实现了更低的预测误差，即使在闭环自回归模式下也是如此。这些发现表明，当与卷积输入聚合相结合时，最小化的循环结构为时空序列建模提供了一个高效且有吸引力的选择，弥合了循环简单性和空间复杂性之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce MinConvLSTM and MinConvGRU, two novel spatiotemporal models thatcombine the spatial inductive biases of convolutional recurrent networks withthe training efficiency of minimal, parallelizable RNNs. Our approach extendsthe log-domain prefix-sum formulation of MinLSTM and MinGRU to convolutionalarchitectures, enabling fully parallel training while retaining localizedspatial modeling. This eliminates the need for sequential hidden state updatesduring teacher forcing - a major bottleneck in conventional ConvRNN models. Inaddition, we incorporate an exponential gating mechanism inspired by the xLSTMarchitecture into the MinConvLSTM, which further simplifies the log-domaincomputation. Our models are structurally minimal and computationally efficient,with reduced parameter count and improved scalability. We evaluate our modelson two spatiotemporal forecasting tasks: Navier-Stokes dynamics and real-worldgeopotential data. In terms of training speed, our architectures significantlyoutperform standard ConvLSTMs and ConvGRUs. Moreover, our models also achievelower prediction errors in both domains, even in closed-loop autoregressivemode. These findings demonstrate that minimal recurrent structures, whencombined with convolutional input aggregation, offer a compelling and efficientalternative for spatiotemporal sequence modeling, bridging the gap betweenrecurrent simplicity and spatial complexity.</description>
      <author>example@mail.com (Coşku Can Horuz, Sebastian Otte, Martin V. Butz, Matthias Karlbauer)</author>
      <guid isPermaLink="false">2508.03614v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland Waterways</title>
      <link>http://arxiv.org/abs/2508.03672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Inland-LOAM，一种专门用于内河水道环境的激光雷达SLAM框架，通过改进特征提取和水表面平面约束解决垂直漂移问题，并实现从3D点云到2D语义地图的转换，提供实时导航参数计算和海岸线提取功能。&lt;h4&gt;背景&lt;/h4&gt;准确的地理空间信息对安全、自主的内河运输至关重要，但现有航道图缺乏实时细节，传统激光雷达SLAM在水道环境中表现不佳，导致垂直漂移和非语义地图问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门针对内河水道环境的激光雷达SLAM框架，提高定位精度，生成语义地图，并提取海岸线，为自主内河运输提供可靠的地理空间信息。&lt;h4&gt;方法&lt;/h4&gt;提出Inland-LOAM框架，采用改进的特征提取技术，应用水表面平面约束减轻垂直漂移；使用基于体素的几何分析将3D点云转换为结构化2D语义地图；开发自动模块提取海岸线并导出为IENC兼容的轻量级格式。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上评估显示，Inland-LOAM比现有最先进方法实现更高定位精度；生成的语义地图和海岸线与真实世界条件一致，为增强态势感知提供可靠数据。&lt;h4&gt;结论&lt;/h4&gt;Inland-LOAM有效解决了内河水道环境中的SLAM挑战，为自主内河运输提供了可靠的地理空间信息支持，代码和数据集将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;准确的地理空间信息对安全、自主的内河运输至关重要，因为现有航道图(IENC)缺乏实时细节，而传统激光雷达SLAM在水道环境中表现不佳。这些挑战导致垂直漂移和非语义地图，阻碍了自主导航。本文介绍了Inland-LOAM，一种用于水道的激光雷达SLAM框架。它使用改进的特征提取和水表面平面约束来减轻垂直漂移。新的管道使用基于体素的几何分析将3D点云转换为结构化的2D语义地图，能够实时计算桥梁净空高度等导航参数。自动模块提取海岸线并将其导出为轻量级、IENC兼容的格式。在真实世界数据集上的评估显示，Inland-LOAM比最先进方法实现了更高的定位精度。生成的语义地图和海岸线与真实世界条件一致，为增强态势感知提供了可靠数据。代码和数据集将公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决内陆水道环境中的高精度定位和语义地图构建问题。传统电子导航图缺乏实时细节，而现有LiDAR SLAM方法难以处理内陆水道的动态岸线和反光水面等特殊环境，导致定位漂移和地图缺乏语义信息。这个问题很重要，因为内陆水道运输是欧洲关键货运网络，但导航存在安全风险，自动化船只需要准确的环境信息来确保安全和提高效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了内陆水道环境的特殊挑战，包括动态岸线和缺乏结构化几何特征等。然后评估了现有LiDAR SLAM方法如LOAM和LeGO-LOAM的局限性，针对这些不足设计了专门改进。方法确实借鉴了现有工作，如基于LOAM的里程计框架、概率占据格子概念、RANSAC平面拟合和图优化技术，但针对水道环境进行了特别优化，如使用水面作为全局约束和改进的特征提取方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用水面作为全局平面约束来减少垂直漂移，并通过几何分析将3D点云转换为结构化的语义地图。整体流程包括：1)预处理和平面拟合，过滤船只自身点并拟合水面平面；2)改进的特征提取，使用方差方法区分平滑和纹理表面；3)两阶段LiDAR里程计和映射，结合水面约束进行联合优化；4)将点云转换为体素网格并构建2D语义地图，分类为水平平面、垂直平面等结构类别；5)提取岸线和桥梁特征，生成IENC兼容格式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)改进的特征提取方法，使用方差量化局部变化，更有效区分表面特征；2)将水面作为全局平面约束的联合优化，显著减少垂直漂移；3)基于体素的结构语义映射，通过几何分析生成包含导航意义的分类地图；4)自动地图转换模块，提取岸线并导出为IENC兼容格式。相比之前工作，不同之处在于专门针对内陆水道环境设计，使用水面而非地面作为约束，生成结构语义地图而非简单点云，并直接与导航标准兼容。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Inland-LOAM通过专门为内陆水道环境设计的改进特征提取、水面约束优化和基于体素的语义映射方法，实现了高精度定位和结构化语义地图生成，为自主水面航行器提供了可靠且最新的地理空间信息。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate geospatial information is crucial for safe, autonomous InlandWaterway Transport (IWT), as existing charts (IENC) lack real-time detail andconventional LiDAR SLAM fails in waterway environments. These challenges leadto vertical drift and non-semantic maps, hindering autonomous navigation.  This paper introduces Inland-LOAM, a LiDAR SLAM framework for waterways. Ituses an improved feature extraction and a water surface planar constraint tomitigate vertical drift. A novel pipeline transforms 3D point clouds intostructured 2D semantic maps using voxel-based geometric analysis, enablingreal-time computation of navigational parameters like bridge clearances. Anautomated module extracts shorelines and exports them into a lightweight,IENC-compatible format.  Evaluations on a real-world dataset show Inland-LOAM achieves superiorlocalization accuracy over state-of-the-art methods. The generated semanticmaps and shorelines align with real-world conditions, providing reliable datafor enhanced situational awareness. The code and dataset will be publiclyavailable</description>
      <author>example@mail.com (Zhongbi Luo, Yunjia Wang, Jan Swevers, Peter Slaets, Herman Bruyninckx)</author>
      <guid isPermaLink="false">2508.03672v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation</title>
      <link>http://arxiv.org/abs/2508.03526v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages,5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了CollaBot框架，用于机器人协作操作大型物体。该框架包括场景分割、协作抓取规划和无碰撞轨迹生成，能够在不同数量机器人和各种任务中实现协作操作。&lt;h4&gt;背景&lt;/h4&gt;机器人学研究的一个重要课题是机器人系统如何与物理世界交互。传统操作任务主要针对小型物体，但在工厂或家庭环境中，经常需要移动大型物体（如桌子），这些任务通常需要多机器人系统协作完成。先前的研究缺乏能够扩展到任意大小机器人并推广到各种任务的框架。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的协作操作框架，能够处理不同数量机器人和各种任务的大型物体操作。&lt;h4&gt;方法&lt;/h4&gt;1. 使用SEEM进行场景分割和目标物体的点云提取；2. 提出协作抓取框架，将任务分解为局部抓取姿态生成和全局协作；3. 设计两阶段规划模块，生成无碰撞轨迹以完成任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在不同数量机器人和各种物体、任务条件下，该框架的成功率达到52%，证明了所提出框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;CollaBot框架是一个通用的同时协作操作框架，能够有效处理多机器人协作操作大型物体的任务。&lt;h4&gt;翻译&lt;/h4&gt;机器人学中的一个中心研究课题是如何利用该系统与物理世界交互。传统的操作任务主要关注小型物体。然而，在工厂或家庭环境中，经常需要移动大型物体，例如移动桌子。这些任务通常需要多机器人系统协同工作。先前的研究缺乏能够扩展到任意大小机器人并推广到各种任务的框架。在这项工作中，我们提出了CollaBot，一个用于同时协作操作的通用框架。首先，我们使用SEEM进行场景分割和目标物体的点云提取。然后，我们提出了一个协作抓取框架，将任务分解为局部抓取姿态生成和全局协作。最后，我们设计了一个两阶段规划模块，可以生成无碰撞轨迹来完成任务。实验表明，在不同数量机器人和各种物体、任务条件下，成功率达到52%，表明了所提出框架的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A central research topic in robotics is how to use this system to interactwith the physical world. Traditional manipulation tasks primarily focus onsmall objects. However, in factory or home environments, there is often a needfor the movement of large objects, such as moving tables. These tasks typicallyrequire multi-robot systems to work collaboratively. Previous research lacks aframework that can scale to arbitrary sizes of robots and generalize to variouskinds of tasks. In this work, we propose CollaBot, a generalist framework forsimultaneous collaborative manipulation. First, we use SEEM for scenesegmentation and point cloud extraction of the target object. Then, we proposea collaborative grasping framework, which decomposes the task into local grasppose generation and global collaboration. Finally, we design a 2-stage planningmodule that can generate collision-free trajectories to achieve this task.Experiments show a success rate of 52% across different numbers of robots,objects, and tasks, indicating the effectiveness of the proposed framework.</description>
      <author>example@mail.com (Kun Song, Shentao Ma, Gaoming Chen, Ninglong Jin, Guangbao Zhao, Mingyu Ding, Zhenhua Xiong, Jia Pan)</author>
      <guid isPermaLink="false">2508.03526v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>A Closed-Loop Multi-Agent Framework for Aerodynamics-Aware Automotive Styling Design</title>
      <link>http://arxiv.org/abs/2508.03370v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大型语言模型的多智能体框架，用于自动化汽车外部设计工作流程，平衡美学与空气动力学性能，加速开发周期。&lt;h4&gt;背景&lt;/h4&gt;汽车外部设计的核心挑战是在主观美学和客观空气动力学性能之间取得平衡，同时显著加速开发周期。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的、由大型语言模型驱动的多智能体框架，自动化从模糊需求到3D概念模型性能验证的端到端工作流程。&lt;h4&gt;方法&lt;/h4&gt;工作流程分为两个阶段：概念生成阶段（智能体协作解释模糊设计需求，生成概念草图，使用扩散模型生成逼真渲染图）和性能验证阶段（将渲染图转换为3D点云，基于轻量级代理模型的阻力预测智能体提供阻力系数和压力场的即时预测，替代耗时的CFD模拟）。&lt;h4&gt;主要发现&lt;/h4&gt;将创意生成与快速工程验证循环无缝集成到一个统一的自动化系统中。&lt;h4&gt;结论&lt;/h4&gt;为在设计的最早阶段高效平衡创意探索与工程约束提供了新的范式。&lt;h4&gt;翻译&lt;/h4&gt;汽车外部设计的核心挑战在于平衡主观美学与客观空气动力学性能，同时显著加速开发周期。为此，我们提出了一种新颖的、由大型语言模型驱动的多智能体框架，自动化了从模糊需求到3D概念模型性能验证的端到端工作流程。工作流程分为两个阶段：概念生成和性能验证。在第一阶段，智能体协作解释模糊设计需求，生成概念草图，并使用扩散模型生成逼真渲染图。在第二阶段，将渲染图转换为3D点云，基于轻量级代理模型的阻力预测智能体提供阻力系数和压力场的即时预测，替代耗时的CFD模拟。这项工作的主要贡献是将创意生成与快速工程验证循环无缝集成到一个统一的自动化系统中，为在设计的最早阶段高效平衡创意探索与工程约束提供了新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The core challenge in automotive exterior design is balancing subjectiveaesthetics with objective aerodynamic performance while dramaticallyaccelerating the development cycle. To address this, we propose a novel,LLM-driven multi-agent framework that automates the end-to-end workflow fromambiguous requirements to 3D concept model performance validation. The workflowis structured in two stages: conceptual generation and performance validation.In the first stage, agents collaborate to interpret fuzzy design requirements,generate concept sketches, and produce photorealistic renderings usingdiffusion models. In the second stage, the renderings are converted to 3D pointclouds, where a Drag Prediction Agent, built upon a lightweight surrogatemodel, provides near-instantaneous predictions of the drag coefficient andpressure fields, replacing time-consuming CFD simulations. The primarycontribution of this work is the seamless integration of creative generationwith a rapid engineering validation loop within a unified, automated system,which provides a new paradigm for efficiently balancing creative explorationwith engineering constraints in the earliest stages of design.</description>
      <author>example@mail.com (Xinyu Jin, Shengmao Yan, Qingtao Wang, Shisong Deng, Yanzhen Jiang, Shuangyao Zhao)</author>
      <guid isPermaLink="false">2508.03370v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Advancing Precision in Multi-Point Cloud Fusion Environments</title>
      <link>http://arxiv.org/abs/2508.03179v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accpeted for publication in Communications in Computer and  Information Science, Springer&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究专注于视觉工业检测领域，通过评估点云技术和多点云匹配方法，提出了一种新的合成数据集和CloudCompare插件，以提高自动化检测系统的准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;视觉工业检测是现代制造业中的重要环节，而点云技术作为一种三维数据处理方法，在工业检测中具有广泛应用。然而，点云数据的处理和比较仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过改进点云处理方法，提高工业视觉检测的准确性和效率，同时提供标准化的评估工具。&lt;h4&gt;方法&lt;/h4&gt;研究采用了点云评估和多点云匹配方法，引入了合成数据集用于定量评估，并开发了CloudCompare插件用于点云合并和表面缺陷可视化。&lt;h4&gt;主要发现&lt;/h4&gt;研究提出了一个合成数据集用于评估配准方法和距离度量，以及一个新颖的CloudCompare插件，能够有效合并多个点云并可视化表面缺陷。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的数据集和工具，研究显著提高了自动化工业检测系统的准确性和效率，为视觉工业检测领域提供了有价值的贡献。&lt;h4&gt;翻译&lt;/h4&gt;这项研究专注于通过评估点云和多点云匹配方法进行视觉工业检测。我们还引入了一个合成数据集，用于定量评估配准方法和各种点云比较的距离度量。此外，我们提出了一种新颖的CloudCompare插件，用于合并多个点云和可视化表面缺陷，从而提高自动化检测系统的准确性和效率。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多点云融合环境中的精度问题，特别是改进点云配准方法和开发相关工具。这个问题在现实中非常重要，因为制造业快速发展需要更高效的质量控制，而目前人工检测过程缓慢、不可靠且成本高。自动化检测系统可以更快、更一致地工作，从长远来看需要更少投资，帮助企业提高制造质量标准。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云获取和配准方法，包括激光和相机-based方法，特别关注了ICP算法及其变体Global ICP和Pose Graph，以及深度学习方法。他们借鉴了现有工作，但发现传统Pose Graph方法使用点对平面ICP不估计法向量，可能导致错误对齐。因此，他们设计了一种改进方法，使用Generalized ICP考虑表面法线信息，确定平面间距离而非点对平面距离，从而提高配准精度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是改进Pose Graph配准方法，通过考虑点云表面法线信息来提高配准精度，确保只有具有相似法线方向的点被对齐。整体流程包括：1)使用结构光传感器获取物体部分点云；2)预处理去除背景和噪声；3)利用运动学信息进行初始对齐；4)使用改进的Pose Graph方法合并多个部分点云；5)将合并点云与地面真相网格比较进行距离估计；6)可视化距离图以识别缺陷。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)改进的Pose Graph方法(Refined Pose Graph)，考虑表面法线信息提高配准精度；2)参数调查，研究配准参数的选择和相互关系；3)开发专门的CloudCompare插件用于多视图点云配准。相比之前工作，不同之处在于传统方法使用点对平面ICP可能错误对齐，而改进方法使用Generalized ICP考虑法线信息；开发了专门的插件提供用户友好界面；并确定了算法参数间的关系。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过改进Pose Graph配准方法并开发专门的CloudCompare插件，显著提高了多点云融合环境的精度，为工业检测提供了一种更准确、更高效的自动化解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This research focuses on visual industrial inspection by evaluating pointclouds and multi-point cloud matching methods. We also introduce a syntheticdataset for quantitative evaluation of registration method and various distancemetrics for point cloud comparison. Additionally, we present a novelCloudCompare plugin for merging multiple point clouds and visualizing surfacedefects, enhancing the accuracy and efficiency of automated inspection systems.</description>
      <author>example@mail.com (Ulugbek Alibekov, Vanessa Staderini, Philipp Schneider, Doris Antensteiner)</author>
      <guid isPermaLink="false">2508.03179v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Medical Point Cloud Shape Learning: Registration, Reconstruction and Variation</title>
      <link>http://arxiv.org/abs/2508.03057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提供了基于学习的医学点云形状分析的全面系统综述，重点关注配准、重建和变异建模三个基本任务，回顾了2021-2025年的文献，总结了方法、数据集、评估指标，并讨论了临床应用、挑战和未来方向。&lt;h4&gt;背景&lt;/h4&gt;点云已成为3D医学成像的重要表示方式，是传统体素或网格方法的紧凑且保留表面的替代方案。深度学习的进步使得直接从点云数据中提取、建模和分析解剖形状成为可能。&lt;h4&gt;目的&lt;/h4&gt;提供基于学习的医学点云形状分析的全面和系统综述，总结代表性方法、数据集和评估指标，强调临床应用和医学领域的独特挑战，并指出未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;回顾2021年至2025年的最新文献，总结代表性方法、数据集和评估指标，分析临床应用和医学领域的独特挑战，并探讨当前局限性和未来方向。&lt;h4&gt;主要发现&lt;/h4&gt;主要趋势包括混合表示的集成、大规模自监督模型和生成技术的应用；当前局限性包括数据稀缺、患者间变异性以及临床部署对可解释性和稳健解决方案的需求。&lt;h4&gt;结论&lt;/h4&gt;点云在医学成像中具有广阔应用前景，但需要解决数据稀缺、变异性大等问题，并开发可解释、稳健的解决方案以满足临床需求，未来应继续推进基于点云的形状学习在医学成像中的应用。&lt;h4&gt;翻译&lt;/h4&gt;点云已成为3D医学成像中日益重要的表示方式，提供了传统基于体素或网格方法的紧凑且保留表面的替代方案。深度学习的最新进展使得直接从点云数据中提取、建模和分析解剖形状成为可能。本文提供了基于学习的医学点云形状分析的全面和系统综述，重点关注三个基本任务：配准、重建和变异建模。我们回顾了2021年至2025年的最新文献，总结了代表性方法、数据集和评估指标，并强调了临床应用和医学领域的独特挑战。主要趋势包括混合表示的集成、大规模自监督模型和生成技术。我们还讨论了当前局限性，如数据稀缺、患者间变异性以及临床部署对可解释性和稳健解决方案的需求。最后，概述了推进基于点云的形状学习在医学成像中的未来方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决医学点云形状学习的系统性综述需求，特别关注配准、重建和变化建模三个核心任务。这个问题很重要，因为点云已成为3D医学成像的重要表示形式，深度学习的进步使得直接从点云数据中提取和分析解剖形状成为可能，但医学领域面临数据稀缺、解剖变异性、跨模态不一致性和临床需求等独特挑战，且缺乏对医学点云形状学习的统一比较概述。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作为综述论文，作者遵循PRISMA指南，系统检索了2021-2025年Web of Science和Scopus数据库中的相关文献，使用关键词筛选后纳入35篇研究。作者借鉴了现有文献收集方法，并参考了之前的综述工作，指出之前的综述要么集中在通用点云处理，要么强调基于图像的学习流程，而对医学领域的解剖先验和临床约束关注有限，因此需要专门针对医学点云形状学习的系统性综述。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提供对医学点云形状学习的全面和系统概述，特别关注配准、重建和变化建模三个基本任务。整体流程包括：1) 文献收集：遵循PRISMA指南系统检索筛选；2) 内容组织：将方法分为三大类及其子任务；3) 分析比较：分析比较各种任务的最先进方法；4) 讨论挑战：讨论相关挑战和未解决问题；5) 展望未来：确定趋势并提出未来方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次全面覆盖医学成像中的形状学习应用；系统回顾过去五年的医学点云研究；总结最先进方法及其模型、数据集和损失函数；讨论挑战并建议未来方向。相比之前工作，这篇综述专注于医学点云而非通用点云处理；关注三个特定任务而非泛泛点云处理；覆盖最新研究(2021-2025)；强调解剖先验和临床约束；提供专门的医学数据集和评估指标概述。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提供了对医学点云形状学习的全面系统性综述，重点关注配准、重建和变化三个核心任务，为研究人员提供了宝贵的参考框架和未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds have become an increasingly important representation for 3Dmedical imaging, offering a compact, surface-preserving alternative totraditional voxel or mesh-based approaches. Recent advances in deep learninghave enabled rapid progress in extracting, modeling, and analyzing anatomicalshapes directly from point cloud data. This paper provides a comprehensive andsystematic survey of learning-based shape analysis for medical point clouds,focusing on three fundamental tasks: registration, reconstruction, andvariation modeling. We review recent literature from 2021 to 2025, summarizerepresentative methods, datasets, and evaluation metrics, and highlightclinical applications and unique challenges in the medical domain. Key trendsinclude the integration of hybrid representations, large-scale self-supervisedmodels, and generative techniques. We also discuss current limitations, such asdata scarcity, inter-patient variability, and the need for interpretable androbust solutions for clinical deployment. Finally, future directions areoutlined for advancing point cloud-based shape learning in medical imaging.</description>
      <author>example@mail.com (Tongxu Zhang, Zhiming Liang, Bei Wang)</author>
      <guid isPermaLink="false">2508.03057v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>MIDAR: Mimicking LiDAR Detection for Traffic Applications with a Lightweight Plug-and-Play Model</title>
      <link>http://arxiv.org/abs/2508.02858v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MIDAR的LiDAR检测模仿模型，解决了现有自动驾驶仿真工具在感知建模和可扩展性之间的矛盾，能够从微观交通模拟器中生成接近真实的LiDAR检测数据，为协同感知研究提供了有效解决方案。&lt;h4&gt;背景&lt;/h4&gt;随着自动驾驶技术发展，利用多车协同感知数据增强交通应用的研究日益增多。由于大规模真实AV部署不切实际，仿真成为主要研究方法。基于游戏引擎的模拟器(如CARLA)能生成高保真传感器数据但在多AV场景下可扩展性差，而微观交通模拟器(如SUMO)虽高效扩展却缺乏感知建模能力。&lt;h4&gt;目的&lt;/h4&gt;弥合高保真模拟器与高效可扩展模拟器之间的差距，开发一种能够利用微观交通模拟器已有车辆级特征来近似真实LiDAR检测的模型。&lt;h4&gt;方法&lt;/h4&gt;提出MIDAR模型，基于周围车辆的空间布局和尺寸从理想LiDAR检测结果预测真正例和假负例；构建改进的多视线路径图编码车辆间遮挡关系；采用GRU增强的APPNP架构将特征从自车和遮挡车辆传播到预测目标。&lt;h4&gt;主要发现&lt;/h4&gt;MIDAR在nuScenes AD数据集上达到0.909的AUC，成功近似主流3D LiDAR检测模型CenterPoint的检测结果；两种基于协同感知的交通应用验证了此类真实检测建模的必要性，尤其对需要准确个体车辆观测的任务。&lt;h4&gt;结论&lt;/h4&gt;MIDAR可无缝集成到交通模拟器和轨迹数据集中，为自动驾驶协同感知研究提供了有效工具，将在发表后开源。&lt;h4&gt;翻译&lt;/h4&gt;随着自动驾驶(AD)技术的进步，越来越多的研究开始关注利用多辆自动驾驶汽车(AV)收集的协同感知(CP)数据来增强交通应用。由于大规模真实世界AV部署的不切实际性，仿真已成为大多数研究的主要方法。虽然基于游戏引擎的模拟器(如CARLA)能生成高保真的原始传感器数据(如LiDAR点云)并可用于产生真实的检测结果，但在多AV场景下面临可扩展性挑战。相比之下，微观交通模拟器(如SUMO)能高效扩展但缺乏感知建模能力。为了弥合这一差距，我们提出了MIDAR，一种模仿LiDAR检测的模型，利用微观交通模拟器中现成的车辆级特征来近似真实的LiDAR检测。具体而言，MIDAR基于周围车辆的空间布局和尺寸，从理想的LiDAR检测结果预测真正例(TPs)和假负例(FNs)。构建了一个改进的多视线路径(RM-LoS)图来编码车辆之间的遮挡关系，在此基础上，MIDAR采用GRU增强的APPNP架构，将特征从自车和遮挡车辆传播到预测目标。在nuScenes AD数据集上，MIDAR在近似CenterPoint(主流3D LiDAR检测模型)生成的检测结果方面达到了0.909的AUC。两种基于CP的交通应用进一步验证了此类真实检测建模的必要性，特别是对需要准确个体车辆观测的任务(如位置、速度、车道索引)。如应用所示，MIDAR可以无缝集成到交通模拟器和轨迹数据集中，将在发表后开源。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶和交通应用研究中高保真传感器数据生成与大规模仿真效率之间的平衡问题。高保真模拟器(如CARLA)能生成真实传感器数据但难以扩展到多车场景，而微观交通模拟器(如SUMO)虽高效却缺乏真实感知建模能力。这个问题很重要，因为随着自动驾驶技术发展，合作感知(CP)研究需要既高效又真实的仿真环境来评估交通应用，但现有方法在仿真精度和效率之间难以取得平衡。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有模拟器的优缺点，发现高保真模拟器缺乏可扩展性而微观模拟器缺乏感知建模能力。他们借鉴了LoS图概念用于表示车辆间的遮挡关系，以及APPNP架构用于图节点分类。在此基础上，作者设计了改进的多跳LoS图(RM-LoS)来更好地捕捉部分遮挡影响，并引入GRU增强APPNP架构实现更灵活的信息混合。这种方法基于车辆级特征预测检测结果，而非生成原始点云数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建改进的多跳视线图(RM-LoS)编码车辆遮挡关系，使用图神经网络预测LiDAR检测结果。流程包括：1)构建RM-LoS图，节点表示车辆，边表示视线关系；2)用MLP将节点特征转换为嵌入；3)通过APPNP算法传播特征；4)集成GRU实现灵活信息混合；5)用线性解码器预测车辆是否被检测到；6)使用nuScenes数据集和CenterPoint检测结果训练模型，输出TPs和FNs预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首创的LiDAR检测模拟模型，平衡仿真效率和检测真实性；2)改进的多跳视线图(RM-LoS)，更好捕捉部分遮挡；3)GRU增强的APPNP架构，实现灵活信息混合；4)轻量级即插即用模型，可集成到多种平台。相比之前工作，MIDAR不同于高保真模拟器的高计算需求，也不同于简化模型的随机假设，而是基于实际车辆布局和尺寸特征预测检测结果，在保持高效的同时实现了高精度(AUC=0.909)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MIDAR提出了一种轻量级即插即用的LiDAR检测模拟模型，通过改进的多跳视线图和GRU增强的图神经网络架构，在微观交通模拟器中高效生成接近真实的LiDAR检测结果，解决了高保真仿真与大规模应用之间的效率与真实性权衡问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As autonomous driving (AD) technology advances, increasing research hasfocused on leveraging cooperative perception (CP) data collected from multipleAVs to enhance traffic applications. Due to the impracticality of large-scalereal-world AV deployments, simulation has become the primary approach in moststudies. While game-engine-based simulators like CARLA generate high-fidelityraw sensor data (e.g., LiDAR point clouds) which can be used to producerealistic detection outputs, they face scalability challenges in multi-AVscenarios. In contrast, microscopic traffic simulators such as SUMO scaleefficiently but lack perception modeling capabilities. To bridge this gap, wepropose MIDAR, a LiDAR detection mimicking model that approximates realisticLiDAR detections using vehicle-level features readily available frommicroscopic traffic simulators. Specifically, MIDAR predicts true positives(TPs) and false negatives (FNs) from ideal LiDAR detection results based on thespatial layouts and dimensions of surrounding vehicles. A Refined Multi-hopLine-of-Sight (RM-LoS) graph is constructed to encode the occlusionrelationships among vehicles, upon which MIDAR employs a GRU-enhanced APPNParchitecture to propagate features from the ego AV and occluding vehicles tothe prediction target. MIDAR achieves an AUC of 0.909 in approximating thedetection results generated by CenterPoint, a mainstream 3D LiDAR detectionmodel, on the nuScenes AD dataset. Two CP-based traffic applications furthervalidate the necessity of such realistic detection modeling, particularly fortasks requiring accurate individual vehicle observations (e.g., position,speed, lane index). As demonstrated in the applications, MIDAR can beseamlessly integrated into traffic simulators and trajectory datasets and willbe open-sourced upon publication.</description>
      <author>example@mail.com (Tianheng Zhu, Yiheng Feng)</author>
      <guid isPermaLink="false">2508.02858v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Cropping outperforms dropout as an augmentation strategy for training self-supervised text embeddings</title>
      <link>http://arxiv.org/abs/2508.03453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了两种数据增强策略在文本嵌入对比学习中的效果，发现裁剪增强方法优于dropout方法，并表明自监督微调在特定领域内可快速生成高质量文本嵌入。&lt;h4&gt;背景&lt;/h4&gt;文本嵌入在多种NLP应用中扮演重要角色，当前最佳嵌入模型通过监督微调预训练语言模型获得，而计算机视觉领域已成功应用自监督训练方法。&lt;h4&gt;目的&lt;/h4&gt;系统比较文本嵌入对比学习中两种数据增强策略的效果，评估嵌入质量，并探索自监督微调在文本嵌入生成中的应用价值。&lt;h4&gt;方法&lt;/h4&gt;在MTEB基准和领域内评估中比较裁剪和dropout两种增强方法，研究表示质量与transformer层的关系，以及仅微调最后几层的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;裁剪增强明显优于dropout方法；领域外数据上自监督方法质量低于监督SOTA；领域内数据上短暂自监督微调可产生高质量嵌入；表示质量随transformer层深入而提高；仅微调最后几层即可达到相似质量。&lt;h4&gt;结论&lt;/h4&gt;自监督微调，特别是使用裁剪增强，可在特定领域快速生成高质量文本嵌入，微调transformer最后几层就足以获得高质量表示。&lt;h4&gt;翻译&lt;/h4&gt;文本嵌入，即整个文本的向量表示，在许多NLP应用中扮演着重要角色，例如检索增强生成、情感分析、聚类或可视化文本集合以进行数据探索。目前，表现最佳的嵌入模型是通过精心筛选的文本对进行大规模监督微调，从预训练语言模型中衍生出来的。在这里，我们系统地比较了在文本嵌入对比学习中生成正样本对的两种最著名的数据增强策略。我们在MTEB和额外的领域内评估中评估了嵌入质量，并表明裁剪增强方法明显优于基于dropout的方法。我们发现，在领域外数据上，产生的嵌入质量低于监督SOTA模型，但对于领域内数据，自监督微调在非常短暂的微调后就能产生高质量的文本嵌入，有时仅略低于监督SOTA。最后，我们表明表示质量随着最后transformer层的提高而增加，并且仅微调那些最后几层就足以达到相似的嵌入质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text embeddings, i.e. vector representations of entire texts, play animportant role in many NLP applications, such as retrieval-augmentedgeneration, sentiment analysis, clustering, or visualizing collections of textsfor data exploration. Currently, top-performing embedding models are derivedfrom pre-trained language models via extensive supervised fine-tuning usingcurated text pairs. This contrasts with computer vision, where self-supervisedtraining based on data augmentations has demonstrated remarkable success. Herewe systematically compare the two most well-known augmentation strategies forpositive pair generation in contrastive learning of text embeddings. We assessembedding quality on MTEB and additional in-domain evaluations and show thatcropping augmentation strongly outperforms the dropout-based approach. We findthat on out-of-domain data, the quality of resulting embeddings is below thesupervised SOTA models, but for in-domain data, self-supervised fine-tuningproduces high-quality text embeddings after very short fine-tuning, sometimesonly marginally below the supervised SOTA. Finally, we show that representationquality increases towards the last transformer layers, which undergo thelargest change during fine-tuning; and that fine-tuning only those last layersis sufficient to reach similar embedding quality.</description>
      <author>example@mail.com (Rita González-Márquez, Philipp Berens, Dmitry Kobak)</author>
      <guid isPermaLink="false">2508.03453v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>AlignCAT: Visual-Linguistic Alignment of Category and Attributefor Weakly Supervised Visual Grounding</title>
      <link>http://arxiv.org/abs/2508.03201v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AlignCAT的新型弱监督视觉定位框架，通过粗粒度和细粒度对齐模块解决现有方法在跨模态推理方面的不足，有效区分文本表达中的细微语义差异。&lt;h4&gt;背景&lt;/h4&gt;弱监督视觉定位旨在基于文本描述定位图像中的对象，但现有方法缺乏强大的跨模态推理能力，难以区分文本表达中的细微语义差异，这种局限性源于基于类别和基于属性的歧义性。&lt;h4&gt;目的&lt;/h4&gt;解决现有弱监督视觉定位方法中跨模态推理不足的问题，提高区分文本表达中细微语义差异的能力。&lt;h4&gt;方法&lt;/h4&gt;引入AlignCAT，一种基于查询的语义匹配框架，包含粗粒度对齐模块（利用类别信息和全局上下文减轻类别不一致对象的干扰）和细粒度对齐模块（利用描述性信息和词级文本特征实现属性一致性），通过充分利用语言线索过滤未对齐的视觉查询，提高对比学习效率。&lt;h4&gt;主要发现&lt;/h4&gt;在RefCOCO、RefCOCO+和RefCOCOg三个视觉定位基准测试上进行了大量实验，验证了AlignCAT在两种视觉定位任务上优于现有弱监督方法。&lt;h4&gt;结论&lt;/h4&gt;AlignCAT通过有效的跨模态对齐方法解决了现有方法的局限性，提出的粗粒度和细粒度对齐模块共同提高了视觉-语言对齐效果，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;弱监督视觉定位旨在基于文本描述定位图像中的对象。尽管已有显著进展，但现有方法缺乏强大的跨模态推理能力，难以区分文本表达中的细微语义差异，这种局限性源于基于类别和基于属性的歧义性。为解决这些挑战，我们引入AlignCAT，一种新颖的基于查询的语义匹配框架，用于弱监督视觉定位。为增强视觉-语言对齐，我们提出一个粗粒度对齐模块，利用类别信息和全局上下文，有效减轻来自类别不一致对象的干扰。随后，一个细粒度对齐模块利用描述性信息并捕获词级文本特征，以实现属性一致性。通过充分利用语言线索，我们提出的AlignCAT逐步过滤掉未对齐的视觉查询，提高对比学习效率。在RefCOCO、RefCOCO+和RefCOCOg三个视觉定位基准测试上的大量实验验证了AlignCAT在两种视觉定位任务上优于现有弱监督方法。我们的代码可在以下网址获取：https://github.com/I2-Multimedia-Lab/AlignCAT。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Weakly supervised visual grounding (VG) aims to locate objects in imagesbased on text descriptions. Despite significant progress, existing methods lackstrong cross-modal reasoning to distinguish subtle semantic differences in textexpressions due to category-based and attribute-based ambiguity. To addressthese challenges, we introduce AlignCAT, a novel query-based semantic matchingframework for weakly supervised VG. To enhance visual-linguistic alignment, wepropose a coarse-grained alignment module that utilizes category informationand global context, effectively mitigating interference fromcategory-inconsistent objects. Subsequently, a fine-grained alignment moduleleverages descriptive information and captures word-level text features toachieve attribute consistency. By exploiting linguistic cues to their fullestextent, our proposed AlignCAT progressively filters out misaligned visualqueries and enhances contrastive learning efficiency. Extensive experiments onthree VG benchmarks, namely RefCOCO, RefCOCO+, and RefCOCOg, verify thesuperiority of AlignCAT against existing weakly supervised methods on two VGtasks. Our code is available at: https://github.com/I2-Multimedia-Lab/AlignCAT.</description>
      <author>example@mail.com (Yidan Wang, Chenyi Zhuang, Wutao Liu, Pan Gao, Nicu Sebe)</author>
      <guid isPermaLink="false">2508.03201v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>HiTeC: Hierarchical Contrastive Learning on Text-Attributed Hypergraph with Semantic-Aware Augmentation</title>
      <link>http://arxiv.org/abs/2508.03104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 18 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出HiTeC，一个针对文本属性超图的两阶段分层对比学习框架，解决了现有方法在处理文本信息超图时的三个主要限制，并实现了可扩展且有效的自监督学习。&lt;h4&gt;背景&lt;/h4&gt;对比学习已成为无监督超图学习的主导范式，但现实世界中的超图节点通常与丰富的文本信息相关联，这在先前的工作中被忽视了。&lt;h4&gt;目的&lt;/h4&gt;填补现有对比学习方法在文本属性超图上应用的研究空白，提出一个可扩展且有效的自监督学习框架。&lt;h4&gt;方法&lt;/h4&gt;HiTeC采用两阶段设计：第一阶段使用结构感知的对比目标预训练文本编码器；第二阶段引入提示增强的文本增强和语义感知的超边丢弃两种增强策略；此外还提出了多尺度对比损失，通过子图级别对比更好地捕获长程依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;通过将文本编码器预训练与超图对比学习解耦，两阶段设计在不损害表示质量的情况下提高了可扩展性。&lt;h4&gt;结论&lt;/h4&gt;大量实验证实了HiTeC在文本属性超图上的有效性和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;对比学习已成为无监督超图学习的主导范式，使模型能够在没有昂贵标签的情况下进行有效训练。然而，现实世界中的超图节点通常与丰富的文本信息相关联，这在先前的工作中被忽视了。将现有的基于对比学习的方法直接应用于文本属性超图会导致三个关键限制：(1)常用的与图无关的文本编码器忽视了文本内容与超图拓扑之间的相关性，导致次优的表示。(2)它们依赖于随机数据增强，引入噪声并削弱了对比目标。(3)主要关注节点和超边级别的对比信号，限制了捕获长程依赖关系的能力。虽然HyperBERT开创了TAHGs上的对比学习，但其协同训练范式可扩展性差。为了填补这一研究空白，我们引入了HiTeC，一个具有语义感知增强的两阶段分层对比学习框架，用于在TAHGs上进行可扩展且有效的自监督学习。在第一阶段，我们使用结构感知的对比目标预训练文本编码器，以克服传统方法的与图无关的性质。在第二阶段，我们引入两种语义感知增强策略，包括提示增强的文本增强和语义感知的超边丢弃，以促进信息视图的生成。此外，我们还提出了一个多尺度对比损失，通过基于s-walk的子图级别对比扩展了现有目标，以更好地捕获长程依赖关系。通过将文本编码器预训练与超图对比学习解耦，这种两阶段设计在不损害表示质量的情况下提高了可扩展性。大量实验证实了HiTeC的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning (CL) has become a dominant paradigm for self-supervisedhypergraph learning, enabling effective training without costly labels.However, node entities in real-world hypergraphs are often associated with richtextual information, which is overlooked in prior works. Directly applyingexisting CL-based methods to such text-attributed hypergraphs (TAHGs) leads tothree key limitations: (1) The common use of graph-agnostic text encodersoverlooks the correlations between textual content and hypergraph topology,resulting in suboptimal representations. (2) Their reliance on random dataaugmentations introduces noise and weakens the contrastive objective. (3) Theprimary focus on node- and hyperedge-level contrastive signals limits theability to capture long-range dependencies, which is essential for expressiverepresentation learning. Although HyperBERT pioneers CL on TAHGs, itsco-training paradigm suffers from poor scalability. To fill the research gap,we introduce HiTeC, a two-stage hierarchical contrastive learning frameworkwith semantic-aware augmentation for scalable and effective self-supervisedlearning on TAHGs. In the first stage, we pre-train the text encoder with astructure-aware contrastive objective to overcome the graph-agnostic nature ofconventional methods. In the second stage, we introduce two semantic-awareaugmentation strategies, including prompt-enhanced text augmentation andsemantic-aware hyperedge drop, to facilitate informative view generation.Furthermore, we propose a multi-scale contrastive loss that extends existingobjectives with an $s$-walk-based subgraph-level contrast to better capturelong-range dependencies. By decoupling text encoder pretraining from hypergraphcontrastive learning, this two-stage design enhances scalability withoutcompromising representation quality. Extensive experiments confirm theeffectiveness of HiTeC.</description>
      <author>example@mail.com (Mengting Pan, Fan Li, Xiaoyang Wang, Wenjie Zhang, Xuemin Lin)</author>
      <guid isPermaLink="false">2508.03104v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning</title>
      <link>http://arxiv.org/abs/2508.03102v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了Causal CLIP Adapter (CCA)框架，通过独立成分分析(ICA)解耦CLIP的视觉特征，并利用CLIP的跨模态对齐能力，在少样本学习中取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;Few-shot learning通常需要使用有限的标记数据有效适应模型。然而，大多数现有的少样本学习方法依赖于纠缠表示，要求模型仅使用有限的监督隐式地恢复解耦过程以获得解耦表示，这阻碍了有效的适应。&lt;h4&gt;目的&lt;/h4&gt;提出一种新框架，明确解耦从CLIP提取的视觉特征，减少可训练参数数量，减轻过拟合，并在少样本学习中提高分类准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1. 使用无监督独立成分分析(ICA)明确解耦从CLIP提取的视觉特征；2. 通过微调基于CLIP的文本分类器单向增强CLIP的跨模态对齐；3. 通过交叉注意力机制双向增强CLIP的跨模态对齐；4. 将单模态和跨模态分类输出线性组合以提高分类准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在11个基准数据集上的广泛实验表明，该方法在少样本性能和对分布偏移的鲁棒性方面始终优于最先进的方法，同时保持计算效率。&lt;h4&gt;结论&lt;/h4&gt;CCA框架通过显式解耦视觉特征和增强CLIP的跨模态对齐能力，有效解决了少样本学习中纠缠表示的问题，减少了可训练参数，减轻了过拟合，并在多个基准测试中取得了优异的性能。&lt;h4&gt;翻译&lt;/h4&gt;小样本学习通常需要使用有限的标记数据有效适应模型。然而，大多数现有的小样本学习方法依赖于纠缠表示，要求模型仅使用有限的监督隐式地恢复解耦过程以获得解耦表示，这阻碍了有效的适应。最近的理论研究表明，多模态对比学习方法如CLIP可以线性变换地解耦潜在表示。鉴于此，我们提出了因果CLIP适配器(CCA)，这是一个新颖的框架，使用无监督独立成分分析(ICA)明确解耦从CLIP提取的视觉特征。这消除了从标记数据学习解耦过程的需要，从而减少了可训练参数的数量并减轻了过拟合。更进一步，虽然ICA可以获得视觉解耦表示，但它也可能破坏CLIP的内部和跨模态对齐。为了对抗这一点，CCA进一步利用CLIP固有的跨模态对齐，通过两种方式增强它：单向地，通过微调基于CLIP的文本分类器；双向地，通过交叉注意力机制，通过相互丰富视觉和文本表示。单模态和跨模态分类输出可以有效地线性组合以提高分类准确性。在11个基准数据集上的广泛实验表明，我们的方法在少样本性能和对分布偏移的鲁棒性方面始终优于最先进的方法，同时保持计算效率。代码将在https://github.com/tianjiao-j/CCA上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot learning (FSL) often requires effective adaptation of models usinglimited labeled data. However, most existing FSL methods rely on entangledrepresentations, requiring the model to implicitly recover the unmixing processto obtain disentangled representations using only limited supervision, whichhinders effective adaptation. Recent theoretical studies show that multimodalcontrastive learning methods, such as CLIP, can disentangle latentrepresentations up to linear transformations. In light of this, we propose theCausal CLIP Adapter (CCA), a novel framework that explicitly disentanglesvisual features extracted from CLIP using unsupervised Independent ComponentAnalysis (ICA). This removes the need to learn the unmixing process from thelabeled data, thereby reducing the number of trainable parameters andmitigating overfitting. Taking a step further, while ICA can obtain visualdisentangled representations, it may also disrupt CLIP's intra- and inter-modalalignment. To counteract this, CCA further leverages CLIP's inherentcross-modal alignment by enhancing it in two ways: unidirectionally, throughfine-tuning a CLIP-based text classifier, and bidirectionally, via across-attention mechanism that enriches visual and textual representationsthrough mutual interaction. Both unimodal and cross-modal classificationoutputs can be effectively combined linearly to improve classificationaccuracy. Extensive experiments on 11 benchmark datasets demonstrate that ourmethod consistently outperforms state-of-the-art approaches in terms offew-shot performance and robustness to distributional shifts, while maintainingcomputational efficiency. Code will be available athttps://github.com/tianjiao-j/CCA.</description>
      <author>example@mail.com (Tianjiao Jiang, Zhen Zhang, Yuhang Liu, Javen Qinfeng Shi)</author>
      <guid isPermaLink="false">2508.03102v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Cross-Bag Augmentation for Multiple Instance Learning-based Whole Slide Image Classification</title>
      <link>http://arxiv.org/abs/2508.03081v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种对比跨袋增强($C^2Aug$)方法，用于解决基于多实例学习的全切片图像分类中伪袋增强方法多样性受限的问题，并通过引入袋级和组级对比学习框架提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;当前基于多实例学习(MIL)的全切片图像(WSI)分类中的伪袋增强方法从有限数量的袋中采样实例，导致多样性受限。&lt;h4&gt;目的&lt;/h4&gt;增加伪袋的多样性，解决引入新实例后关键实例数量增加导致的问题，并提高模型在肿瘤面积小的测试切片上的性能。&lt;h4&gt;方法&lt;/h4&gt;提出对比跨袋增强($C^2Aug$)方法从同一类的所有袋中采样实例增加伪袋多样性，并引入袋级和组级对比学习框架增强具有不同语义含义的特征的区分能力。&lt;h4&gt;主要发现&lt;/h4&gt;引入新实例到伪袋中会增加关键实例的数量，导致包含少量关键实例的伪袋出现次数减少，这限制了模型在肿瘤面积小的测试切片上的性能。&lt;h4&gt;结论&lt;/h4&gt;$C^2Aug$在多个评估指标上一致优于现有的最先进方法。&lt;h4&gt;翻译&lt;/h4&gt;最近的基于多实例学习(MIL)的全切片图像(WSI)分类中的伪袋增强方法从有限数量的袋中采样实例，导致多样性受限。为解决这一问题，我们提出了对比跨袋增强($C^2Aug$)方法，从同一类的所有袋中采样实例以增加伪袋的多样性。然而，将新实例引入伪袋会增加关键实例(如肿瘤实例)的数量。这种增加导致包含少量关键实例的伪袋出现次数减少，从而限制了模型性能，特别是在肿瘤面积小的测试切片上。为解决这一问题，我们引入了袋级和组级对比学习框架，以增强具有不同语义含义的特征的区分能力，从而提高模型性能。实验结果表明，$C^2Aug$在多个评估指标上一致优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent pseudo-bag augmentation methods for Multiple Instance Learning(MIL)-based Whole Slide Image (WSI) classification sample instances from alimited number of bags, resulting in constrained diversity. To address thisissue, we propose Contrastive Cross-Bag Augmentation ($C^2Aug$) to sampleinstances from all bags with the same class to increase the diversity ofpseudo-bags. However, introducing new instances into the pseudo-bag increasesthe number of critical instances (e.g., tumor instances). This increase resultsin a reduced occurrence of pseudo-bags containing few critical instances,thereby limiting model performance, particularly on test slides with smalltumor areas. To address this, we introduce a bag-level and group-levelcontrastive learning framework to enhance the discrimination of features withdistinct semantic meanings, thereby improving model performance. Experimentalresults demonstrate that $C^2Aug$ consistently outperforms state-of-the-artapproaches across multiple evaluation metrics.</description>
      <author>example@mail.com (Bo Zhang, Xu Xinan, Shuo Yan, Yu Bai, Zheng Zhang, Wufan Wang, Wendong Wang)</author>
      <guid isPermaLink="false">2508.03081v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>SecoustiCodec: Cross-Modal Aligned Streaming Single-Codecbook Speech Codec</title>
      <link>http://arxiv.org/abs/2508.02849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SecoustiCodec，一种跨模态对齐的低比特率流式语音编解码器，解决了现有方法在语义编码中的多个挑战，包括残余语言外信息、语义完整性不足、重构能力有限和缺乏流式支持。&lt;h4&gt;背景&lt;/h4&gt;语音编解码器在统一语音和文本语言模型中扮演着重要角色，但现有方法在语义编码方面面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有语音编解码方法在语义编码方面的挑战，包括残余的语言外信息（如音色、情感）、语义完整性不足、重构能力有限和缺乏流式支持。&lt;h4&gt;方法&lt;/h4&gt;提出SecoustiCodec，在单一码本空间中解耦语义和语言外信息；引入语言外编码确保语义完整性和重构保真度；提出基于VAE和FSQ的语义专用高效量化方法；提出基于对比学习的语义解耦方法，对齐文本和语音在联合多模态帧级空间；提出声学约束的多阶段优化策略确保稳健稳定的收敛。&lt;h4&gt;主要发现&lt;/h4&gt;SecoustiCodec在0.27/1 kbps比特率下分别实现了1.77/2.58的SOTA重构质量（PESQ）。&lt;h4&gt;结论&lt;/h4&gt;SecoustiCodec成功解决了现有语音编解码方法的多个挑战，实现了高质量重构能力并支持流式处理，已开源演示、代码和模型权重。&lt;h4&gt;翻译&lt;/h4&gt;语音编解码器在统一语音和文本语言模型中扮演着关键桥梁的角色。现有的编解码方法在语义编码方面面临几个挑战，例如残余的语言外信息（如音色、情感）、语义完整性不足、重构能力有限以及缺乏流式支持。为了解决这些挑战，我们提出了SecoustiCodec，一种跨模态对齐的低比特率流式语音编解码器，在单一码本空间中解耦语义和语言外信息。为确保语义完整性和重构保真度，引入了语言外编码来桥接语义和声学编码之间的信息鸿沟。提出了基于VAE（变分自编码器）和FSQ（有限标量量化）的语义专用高效量化方法。这种方法缓解了令牌的长尾分布问题，同时保持高码本利用率。提出了基于对比学习的语义解耦方法，在联合多模态帧级空间中对齐文本和语音，有效从语义编码中移除语言外信息。提出了声学约束的多阶段优化策略，确保稳健和稳定的收敛。图~\ref{fig:pesq_kbps_below_2kbps}显示SecoustiCodec在0.27/1 kbps下分别实现了1.77/2.58的SOTA（最先进）重构质量（PESQ）。SecoustiCodec的代码和模型权重将在同行评审完成后开源。我们已经开源了SecoustiCodec的演示、代码和模型权重。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech codecs serve as a crucial bridge in unifying speech and text languagemodels. Existing codec methods face several challenges in semantic encoding,such as residual paralinguistic information (e.g., timbre, emotion),insufficient semantic completeness, limited reconstruction capability, and lackof support for streaming. To address these challenges, we proposeSecoustiCodec, a cross-modal aligned low-bitrate streaming speech codec thatdisentangles semantic and paralinguistic information in a single-codebookspace. To ensure semantic completeness and reconstruction fidelity,paralinguistic encoding is introduced to bridge the information gap betweensemantic and acoustic encoding. A semantic-only efficient quantization methodbased on VAE (Variational Autoencoder) and FSQ (Finite Scalar Quantization) isproposed. This approach alleviates the long-tail distribution problem of tokenswhile maintaining high codebook utilization. A semantic disentanglement methodbased on contrastive learning is proposed, which aligns text and speech in ajoint multimodal frame-level space, effectively removing paralinguisticinformation from semantic encoding. An acoustic-constrained multi-stageoptimization strategy is proposed to ensure robust and stable convergence.Figure~\ref{fig:pesq_kbps_below_2kbps} shows SecoustiCodec achieves SOTA(state-of-the-art) reconstruction quality (PESQ) of 1.77/2.58 at 0.27/1 kbps.The code and model weights for SecoustiCodec will be open-sourced upon thecompletion of the peer-review process. We've open-sourced SecoustiCodec's demo,code, and model weights.</description>
      <author>example@mail.com (Chunyu Qiang, Haoyu Wang, Cheng Gong, Tianrui Wang, Ruibo Fu, Tao Wang, Ruilong Chen, Jiangyan Yi, Zhengqi Wen, Chen Zhang, Longbiao Wang, Jianwu Dang, Jianhua Tao)</author>
      <guid isPermaLink="false">2508.02849v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Context-Adaptive Multi-Prompt LLM Embedding for Vision-Language Alignment</title>
      <link>http://arxiv.org/abs/2508.02762v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出Context-Adaptive Multi-Prompt Embedding方法，通过多个结构化提示增强视觉语言对比学习中的语义表示，相比标准CLIP模型的单一文本嵌入，能捕捉输入文本的多样语义方面。&lt;h4&gt;背景&lt;/h4&gt;标准CLIP风格模型依赖单一文本嵌入来表示视觉语言数据，可能无法充分捕捉文本的丰富语义信息。&lt;h4&gt;目的&lt;/h4&gt;增强视觉语言对比学习中的语义表示，实现文本与视觉特征之间更丰富的语义对齐，提高检索性能。&lt;h4&gt;方法&lt;/h4&gt;引入多个结构化提示，每个提示包含不同的自适应标记来捕捉输入文本的不同语义方面；所有提示在一个前向传播中联合处理；提示嵌入被组合成统一的文本表示；加入多样性正则化损失和否定感知损失来促进语义多样性和表示质量。&lt;h4&gt;主要发现&lt;/h4&gt;通过多个提示的组合和专门的损失函数，能够实现语义更丰富的表示，提高对比判别能力，在图像-文本和视频-文本检索任务中取得一致改进。&lt;h4&gt;结论&lt;/h4&gt;Context-Adaptive Multi-Prompt Embedding方法能有效增强视觉语言对比学习中的语义表示，在多种检索任务中展现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出上下文自适应多提示嵌入，这是一种在视觉语言对比学习中丰富语义表示的新方法。与依赖单一文本嵌入的标准CLIP风格模型不同，我们的方法引入了多个结构化提示，每个提示包含不同的自适应标记，捕捉输入文本的多样语义方面。我们在单个前向传播中联合处理所有提示。将提示组合成统一的文本表示，实现与视觉特征的更丰富语义对齐。为进一步促进语义多样性和表示质量，我们纳入多样性正则化损失和否定感知损失，鼓励提示专业化并提高对比判别能力。我们的方法在图像-文本和视频-文本检索基准测试中取得了一致的改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose Context-Adaptive Multi-Prompt Embedding, a novel approach toenrich semantic representations in vision-language contrastive learning. Unlikestandard CLIP-style models that rely on a single text embedding, our methodintroduces multiple structured prompts, each containing a distinct adaptivetoken that captures diverse semantic aspects of the input text. We process allprompts jointly in a single forward pass. The resulting prompt embeddings arecombined into a unified text representation, enabling semantically richeralignment with visual features. To further promote semantic diversity andrepresentation quality, we incorporate a diversity regularization loss and anegation-aware loss, encouraging specialization across prompts and improvingcontrastive discrimination. Our method achieves consistent improvements on bothimage-text and video-text retrieval benchmarks.</description>
      <author>example@mail.com (Dahun Kim, Anelia Angelova)</author>
      <guid isPermaLink="false">2508.02762v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>ECGTwin: Personalized ECG Generation Using Controllable Diffusion Model</title>
      <link>http://arxiv.org/abs/2508.02720v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出ECGTwin，一个两阶段框架用于个性化心电图生成，解决了在没有真实标签情况下提取个体特征和注入各种心脏条件两个基本挑战，能够生成高质量、多样化的ECG信号并保留个体特定特征。&lt;h4&gt;背景&lt;/h4&gt;个性化心电图生成可将传统医疗转变为更准确的个体化范式，同时保留群体水平ECG合成的优势。但面临两个基本挑战：无真实标签时提取个体特征，以及注入多种心脏条件时不混淆生成模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架解决个性化ECG生成的两个基本挑战，生成高质量、多样化的ECG信号并保留个体特定特征，增强下游应用中的ECG自动诊断能力。&lt;h4&gt;方法&lt;/h4&gt;ECGTwin采用两阶段框架：第一阶段是通过对比学习训练的个体基础提取器，从参考ECG中捕获个人特征；第二阶段是将个体特征与目标心脏条件通过新颖的AdaX条件注入器整合到基于扩散的生成过程中，通过两个专用途径注入信号。&lt;h4&gt;主要发现&lt;/h4&gt;定性和定量实验表明，ECGTwin能通过细粒度生成可控性生成高质量、多样化的ECG信号，同时保留个体特定特征。此外，该模型显示出增强下游ECG自动诊断的潜力。&lt;h4&gt;结论&lt;/h4&gt;ECGTwin成功解决了个性化ECG生成中的基本挑战，为精确个性化医疗解决方案提供了可能性，能够生成高质量的个性化ECG信号并应用于实际医疗诊断。&lt;h4&gt;翻译&lt;/h4&gt;个性化心电图(ECG)生成是模拟特定条件下患者心电图数字孪生的过程。它有可能将传统医疗保健转变为更准确的个体化范式，同时保留传统群体水平ECG合成的主要优势。然而，这一有前景的任务提出了两个基本挑战：在没有真实标签的情况下提取个体特征，以及在混淆生成模型的情况下注入各种类型的心脏条件。在本文中，我们提出了ECGTwin，一个两阶段框架，旨在解决这些挑战。在第一阶段，通过对比学习训练的个体基础提取器稳健地捕获来自参考ECG的个人特征。在第二阶段，提取的个体特征与目标心脏条件通过我们新颖的AdaX条件注入器整合到基于扩散的生成过程中，该注入器通过两个专用且专门的途径注入这些信号。定性和定量实验都证明，我们的模型不仅能够通过提供细粒度的生成可控性来生成高质量、多样化的ECG信号，还能保留个体特定特征。此外，ECGTwin显示出增强下游应用中ECG自动诊断的潜力，确认了精确个性化医疗解决方案的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalized electrocardiogram (ECG) generation is to simulate a patient'sECG digital twins tailored to specific conditions. It has the potential totransform traditional healthcare into a more accurate individualized paradigm,while preserving the key benefits of conventional population-level ECGsynthesis. However, this promising task presents two fundamental challenges:extracting individual features without ground truth and injecting various typesof conditions without confusing generative model. In this paper, we presentECGTwin, a two-stage framework designed to address these challenges. In thefirst stage, an Individual Base Extractor trained via contrastive learningrobustly captures personal features from a reference ECG. In the second stage,the extracted individual features, along with a target cardiac condition, areintegrated into the diffusion-based generation process through our novel AdaXCondition Injector, which injects these signals via two dedicated andspecialized pathways. Both qualitative and quantitative experiments havedemonstrated that our model can not only generate ECG signals of high fidelityand diversity by offering a fine-grained generation controllability, but alsopreserving individual-specific features. Furthermore, ECGTwin shows thepotential to enhance ECG auto-diagnosis in downstream application, confirmingthe possibility of precise personalized healthcare solutions.</description>
      <author>example@mail.com (Yongfan Lai, Bo Liu, Xinyan Guan, Qinghao Zhao, Hongyan Li, Shenda Hong)</author>
      <guid isPermaLink="false">2508.02720v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>VITA: Variational Pretraining of Transformers for Climate-Robust Crop Yield Forecasting</title>
      <link>http://arxiv.org/abs/2508.03589v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种名为VITA的变分预训练框架，解决了农业产量预测中AI模型在偏离历史趋势时表现不佳的问题。该模型通过利用丰富的天气数据作为代理目标进行预训练，可以在仅使用基本天气统计数据的情况下进行微调，并在美国玉米带的763个县中实现了最先进的玉米和大豆产量预测性能，特别是在极端天气年份表现突出。&lt;h4&gt;背景&lt;/h4&gt;准确的作物产量预测对全球粮食安全至关重要。然而，当前的AI模型在产量偏离历史趋势时表现不佳。这一问题源于关键的数据挑战，包括丰富的预训练天气数据集与可用于微调的有限数据之间的主要不对称性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服数据限制、在各种条件下（特别是极端天气条件下）准确预测作物产量的AI模型，使其在数据稀缺地区更加实用。&lt;h4&gt;方法&lt;/h4&gt;研究团队引入了VITA（用于非对称数据的变分推理Transformer）变分预训练框架。该框架不依赖输入重建，而是在预训练期间使用详细的天气变量作为代理目标，并通过自监督特征掩码学习预测丰富的大气状态。这使得模型在部署时只需使用基本天气统计数据即可进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在美国玉米带的763个县中，VITA在预测玉米和大豆产量方面实现了最先进的性能。2. 在正常条件下，VITA持续提供优越的性能。3. 在极端天气年份，VITA的优势尤为明显，具有统计学上的显著改善。4. VITA使用更少的数据就能优于先前的框架（如GNN-RNN），使其在现实世界应用中更加实用，特别是在数据稀缺地区。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了领域感知的AI设计如何能够克服数据限制，并在不断变化的气候中支持有弹性的农业预测。VITA模型通过有效处理天气数据的不对称性，为作物产量预测提供了更准确、更可靠的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的作物产量预测对全球粮食安全至关重要。然而，当产量偏离历史趋势时，当前的AI模型普遍表现不佳。这一问题源于关键的数据挑战，包括丰富的预训练天气数据集与可用于微调的有限数据之间的主要不对称性。我们引入了VITA（用于非对称数据的变分推理Transformer），这是一个解决这种不对称性的变分预训练框架。VITA不依赖输入重建，而是在预训练期间使用详细的天气变量作为代理目标，并通过自监督特征掩码学习预测丰富的大气状态。这使得模型在部署时只需使用基本天气统计数据即可进行微调。应用于美国玉米带的763个县，VITA在所有评估场景中实现了预测玉米和大豆产量的最先进性能。虽然在正常条件下它持续提供优越的性能，但其优势在极端天气年份尤为明显，具有统计学上的显著改善。重要的是，VITA使用更少的数据就能优于先前的框架（如GNN-RNN），使其在现实世界应用中更加实用——特别是在数据稀缺地区。这项工作展示了领域感知的AI设计如何能够克服数据限制，并在不断变化的气候中支持有弹性的农业预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate crop yield forecasting is essential for global food security.However, current AI models systematically underperform when yields deviate fromhistorical trends. This issue arises from key data challenges, including amajor asymmetry between rich pretraining weather datasets and the limited dataavailable for fine-tuning. We introduce VITA (Variational Inference Transformerfor Asymmetric data), a variational pretraining framework that addresses thisasymmetry. Instead of relying on input reconstruction, VITA uses detailedweather variables as proxy targets during pretraining and learns to predictrich atmospheric states through self-supervised feature masking. This allowsthe model to be fine-tuned using only basic weather statistics duringdeployment. Applied to 763 counties in the U.S. Corn Belt, VITA achievesstate-of-the-art performance in predicting corn and soybean yields across allevaluation scenarios. While it consistently delivers superior performance undernormal conditions, its advantages are particularly pronounced during extremeweather years, with statistically significant improvements (paired t-test, $p\approx 0.01$). Importantly, VITA outperforms prior frameworks like GNN-RNNusing less data, making it more practical for real-world use--particularly indata-scarce regions. This work highlights how domain-aware AI design canovercome data limitations and support resilient agricultural forecasting in achanging climate.</description>
      <author>example@mail.com (Adib Hasan, Mardavij Roozbehani, Munther Dahleh)</author>
      <guid isPermaLink="false">2508.03589v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>GaitAdapt: Continual Learning for Evolving Gait Recognition</title>
      <link>http://arxiv.org/abs/2508.03375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GaitAdapter的无需回放的步态识别持续学习方法，通过GPAK模块和EDSN方法有效解决了模型在新数据集上重新训练时难以保留先前知识的问题，显著提高了步态识别的判别能力。&lt;h4&gt;背景&lt;/h4&gt;当前步态识别方法通常需要在遇到新数据集时重新训练，但重新训练的模型难以保留之前数据集的知识，导致在早期测试集上性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;提出一个持续的步态识别任务GaitAdapt，支持步态识别能力的逐步增强，并根据各种评估场景进行系统分类。同时提出GaitAdapter，一种无需回放的步态识别持续学习方法。&lt;h4&gt;方法&lt;/h4&gt;GaitAdapter集成了GaitPartition Adaptive Knowledge (GPAK)模块，使用图神经网络从当前数据中聚合常见的步态模式到一个由图向量构建的存储库中，用于提高新任务中步态特征的判别能力。还引入了一种基于负样本的欧几里得距离稳定性方法(EDSN)，确保来自不同类别的新增步态样本在之前的和当前的步态任务中保持相似的相对空间分布。&lt;h4&gt;主要发现&lt;/h4&gt;广泛评估表明，GaitAdapter有效保留了从不同任务中获取的步态知识，与替代方法相比表现出明显优越的判别能力。&lt;h4&gt;结论&lt;/h4&gt;GaitAdapter方法解决了步态识别中持续学习的知识保留问题，提高了模型在多任务环境下的性能。&lt;h4&gt;翻译&lt;/h4&gt;当前步态识别方法通常需要在遇到新数据集时重新训练。然而，重新训练的模型经常难以保留来自之前数据集的知识，导致在早期测试集上性能显著下降。为了解决这些挑战，我们提出了一个持续的步态识别任务，称为GaitAdapt，它支持随着时间的推移逐步增强步态识别能力，并根据各种评估场景进行系统分类。此外，我们提出了GaitAdapter，一种用于步态识别的无回放持续学习方法。这种方法集成了GaitPartition Adaptive Knowledge (GPAK)模块，采用图神经网络从当前数据中聚合常见的步态模式到一个由图向量构建的存储库中。随后，使用该存储库来提高新任务中步态特征的判别能力，从而增强模型有效识别步态模式的能力。我们还引入了一种基于负样本的欧几里得距离稳定性方法(EDSN)，确保来自不同类别的新增步态样本在之前的和当前的步态任务中保持相似的相对空间分布，从而减轻任务变化对原始领域特征判别性的影响。广泛的评估表明，GaitAdapter有效保留了从不同任务中获取的步态知识，与替代方法相比表现出明显优越的判别能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current gait recognition methodologies generally necessitate retraining whenencountering new datasets. Nevertheless, retrained models frequently encounterdifficulties in preserving knowledge from previous datasets, leading to asignificant decline in performance on earlier test sets. To tackle thesechallenges, we present a continual gait recognition task, termed GaitAdapt,which supports the progressive enhancement of gait recognition capabilitiesover time and is systematically categorized according to various evaluationscenarios. Additionally, we propose GaitAdapter, a non-replay continuallearning approach for gait recognition. This approach integrates theGaitPartition Adaptive Knowledge (GPAK) module, employing graph neural networksto aggregate common gait patterns from current data into a repositoryconstructed from graph vectors. Subsequently, this repository is used toimprove the discriminability of gait features in new tasks, thereby enhancingthe model's ability to effectively recognize gait patterns. We also introduce aEuclidean Distance Stability Method (EDSN) based on negative pairs, whichensures that newly added gait samples from different classes maintain similarrelative spatial distributions across both previous and current gait tasks,thereby alleviating the impact of task changes on the distinguishability oforiginal domain features. Extensive evaluations demonstrate that GaitAdaptereffectively retains gait knowledge acquired from diverse tasks, exhibitingmarkedly superior discriminative capability compared to alternative methods.</description>
      <author>example@mail.com (Jingjie Wang, Shunli Zhang, Xiang Wei, Senmao Tian)</author>
      <guid isPermaLink="false">2508.03375v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Online Continual Graph Learning</title>
      <link>http://arxiv.org/abs/2508.03283v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了图上在线持续学习的通用公式，建立了基准测试，为系统评估提供了明确定义的设置。&lt;h4&gt;背景&lt;/h4&gt;持续学习旨在增量学习新任务同时避免灾难性遗忘；在线持续学习关注从连续数据流中高效学习，这些数据流具有变化的分布。尽管有研究探索使用图神经网络进行持续学习，但只有少数关注流式设置，而现实中的图数据通常会随时间演变，需要及时和在线的预测。&lt;h4&gt;目的&lt;/h4&gt;提出图上在线持续学习的通用公式，强调在图拓扑上进行批处理的效率要求，并为系统模型评估提供明确定义的设置。&lt;h4&gt;方法&lt;/h4&gt;引入一组基准测试，并报告持续学习文献中几种方法在所提设置下的性能表现。&lt;h4&gt;主要发现&lt;/h4&gt;当前方法与标准的在线持续学习设置不太一致，部分原因是缺乏对图上在线持续学习的明确定义。&lt;h4&gt;结论&lt;/h4&gt;通过提出的通用公式和基准测试，为图上的在线持续学习研究提供了更清晰的框架和评估标准。&lt;h4&gt;翻译&lt;/h4&gt;持续学习的目标是在避免灾难性遗忘的同时增量学习新任务。在线持续学习特别关注从连续数据流中高效学习，这些数据流具有变化的分布。虽然最近有研究探索使用图神经网络进行持续学习，但只有少数研究关注流式设置。然而，现实中的图数据通常会随时间演变，往往需要及时和在线的预测。然而，当前方法与标准的在线持续学习设置不太一致，部分原因是缺乏对图上在线持续学习的明确定义。在这项工作中，我们提出了图上在线持续学习的通用公式，强调在图拓扑上进行批处理的效率要求，并为系统模型评估提供明确定义的设置。最后，我们引入一组基准测试，并报告了持续学习文献中几种方法在我们设置下的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The aim of Continual Learning (CL) is to learn new tasks incrementally whileavoiding catastrophic forgetting. Online Continual Learning (OCL) specificallyfocuses on learning efficiently from a continuous stream of data with shiftingdistribution. While recent studies explore Continual Learning on graphsexploiting Graph Neural Networks (GNNs), only few of them focus on a streamingsetting. Yet, many real-world graphs evolve over time, often requiring timelyand online predictions. Current approaches, however, are not well aligned withthe standard OCL setting, partly due to the lack of a clear definition ofonline Continual Learning on graphs. In this work, we propose a generalformulation for online Continual Learning on graphs, emphasizing the efficiencyrequirements on batch processing over the graph topology, and providing awell-defined setting for systematic model evaluation. Finally, we introduce aset of benchmarks and report the performance of several methods in the CLliterature, adapted to our setting.</description>
      <author>example@mail.com (Giovanni Donghi, Luca Pasa, Daniele Zambon, Cesare Alippi, Nicolò Navarin)</author>
      <guid isPermaLink="false">2508.03283v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Embedding Models on Hyper-relational Knowledge Graph</title>
      <link>http://arxiv.org/abs/2508.03280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究质疑了超关系知识图谱嵌入(HKGE)模型的优越性来源，通过将超关系知识图谱转换为传统知识图谱格式测试经典KGE模型，发现部分经典模型性能相当。分析表明分解方法改变了原始HKG拓扑且无法完全保留信息，且当前HKGE模型在捕捉长距离依赖或整合主三元组和限定词信息方面存在不足。为此，作者提出了FormerGNN框架，通过限定词整合器保留原始拓扑，基于GNN的图编码器捕捉长距离依赖，并改进信息整合方法。实验证明FormerGNN优于现有HKGE模型。&lt;h4&gt;背景&lt;/h4&gt;超关系知识图谱(HKGs)是传统知识图谱(KGs)的扩展，通过额外的限定词(qualifiers)更好地表示现实世界事实。研究人员尝试通过设计额外的限定词处理模块来适应经典知识图谱嵌入(KGE)模型以应用于HKGs。&lt;h4&gt;目的&lt;/h4&gt;确定HKGE模型的优越性能是来自于其基础KGE模型还是专门设计的扩展模块，并探索改进HKGE模型的方法。&lt;h4&gt;方法&lt;/h4&gt;作者采用三种分解方法将超关系知识图谱转换为传统知识图谱格式，评估了多个经典KGE模型在HKGs上的性能。基于发现的问题，提出了FormerGNN框架，包含限定词整合器保留原始HKG拓扑，基于GNN的图编码器捕捉长距离依赖，以及改进的主三元组和限定词信息整合方法。&lt;h4&gt;主要发现&lt;/h4&gt;1)一些经典KGE模型在HKGs上可以达到与HKGE模型相当的性能；2)分解方法改变了原始HKG拓扑且无法完全保留HKG信息；3)当前HKGE模型在捕捉图的长期依赖或整合主三元组和限定词信息方面存在不足，部分原因是信息压缩问题。&lt;h4&gt;结论&lt;/h4&gt;FormerGNN框架通过保留原始HKG拓扑、捕捉长距离依赖和改进信息整合方法，能够有效提升HKGE模型的性能，优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;最近，超关系知识图谱(HKGs)被提出作为传统知识图谱(KGs)的扩展，通过额外的限定词更好地表示现实世界事实。因此，研究人员尝试通过设计额外的限定词处理模块来适应经典知识图谱嵌入(KGE)模型以应用于HKGs。然而，目前尚不清楚超关系KGE(HKGE)模型的优越性能是来自于其基础KGE模型还是专门设计的扩展模块。因此，在本文中，我们使用三种分解方法将HKG数据转换为KG格式，然后评估了几个经典KGE模型在HKGs上的性能。我们的结果表明，一些KGE模型能够达到与HKGE模型相当的性能。进一步分析发现，分解方法改变了原始HKG拓扑且无法完全保留HKG信息。此外，我们观察到当前HKGE模型在捕捉图的长期依赖方面不足，或由于信息压缩问题而难以整合主三元组和限定词信息。为进一步验证我们的发现并为未来HKGE研究提供潜在方向，我们提出了FormerGNN框架。该框架采用限定词整合器保留原始HKG拓扑，基于GNN的图编码器捕捉图的长距离依赖，然后采用改进的方法整合主三元组和限定词信息以缓解压缩问题。我们的实验结果表明FormerGNN优于现有HKGE模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Hyper-relational Knowledge Graphs (HKGs) have been proposed as anextension of traditional Knowledge Graphs (KGs) to better represent real-worldfacts with additional qualifiers. As a result, researchers have attempted toadapt classical Knowledge Graph Embedding (KGE) models for HKGs by designingextra qualifier processing modules. However, it remains unclear whether thesuperior performance of Hyper-relational KGE (HKGE) models arises from theirbase KGE model or the specially designed extension module. Hence, in thispaper, we data-wise convert HKGs to KG format using three decomposition methodsand then evaluate the performance of several classical KGE models on HKGs. Ourresults show that some KGE models achieve performance comparable to that ofHKGE models. Upon further analysis, we find that the decomposition methodsalter the original HKG topology and fail to fully preserve HKG information.Moreover, we observe that current HKGE models are either insufficient incapturing the graph's long-range dependency or struggle to integratemain-triple and qualifier information due to the information compression issue.To further justify our findings and offer a potential direction for future HKGEresearch, we propose the FormerGNN framework. This framework employs aqualifier integrator to preserve the original HKG topology, and a GNN-basedgraph encoder to capture the graph's long-range dependencies, followed by animproved approach for integrating main-triple and qualifier information tomitigate compression issues. Our experimental results demonstrate thatFormerGNN outperforms existing HKGE models.</description>
      <author>example@mail.com (Yubo Wang, Shimin Di, Zhili Wang, Haoyang Li, Fei Teng, Hao Xin, Lei Chen)</author>
      <guid isPermaLink="false">2508.03280v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>COFFEE: A Shadow-Resilient Real-Time Pose Estimator for Unknown Tumbling Asteroids using Sparse Neural Networks</title>
      <link>http://arxiv.org/abs/2508.03132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  in Proc. 75th Int. Astronautical Congress (IAC-24), Milan, Italy,  Oct. 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为COFFEE的实时姿态估计框架，用于解决太空天体精确状态估计中的挑战，特别是处理高不透明自投射阴影导致的偏差问题。&lt;h4&gt;背景&lt;/h4&gt;太空中的未知天体精确状态估计是一个关键挑战，应用包括空间碎片跟踪和小天体形状估计。现有方法在实时性和准确性之间存在权衡，且都不太能处理天体自投射阴影带来的偏差问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种实时、准确且无偏差的姿态估计方法，能够处理天体自投射阴影导致的偏差问题，适用于航天级硬件环境。&lt;h4&gt;方法&lt;/h4&gt;提出COFFEE框架，利用太阳跟踪传感器提供的太阳相位角先验信息，通过将显著轮廓与其投射的阴影关联来检测特征稀疏集，然后联合训练稀疏神经网络和基于注意力的图神经网络特征匹配模型，提供连续帧之间的对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;当天体旋转时，自投射阴影会导致姿态估计产生很大偏差，特别是对于经历混沌翻滚运动的天体；所提出的COFFEE框架无偏差，比传统方法更准确，比其他深度学习方法快一个数量级。&lt;h4&gt;结论&lt;/h4&gt;COFFEE框架解决了太空天体姿态估计中的实时性、准确性和鲁棒性问题，特别适用于处理高不透明自投射阴影带来的挑战，为航天任务提供了可靠的天体状态估计解决方案。&lt;h4&gt;翻译&lt;/h4&gt;太空中的未知天体精确状态估计是一个关键挑战，应用范围从空间碎片跟踪到小天体形状估计。实现这一能力的一个必要前提是在连续的图像流中找到并跟踪特征。现有方法如SIFT、ORB和AKAZE实现了实时但不准确姿态估计，而现代深度学习方法在提供更高质量特征的同时需要更多计算资源，而这些资源在航天级硬件上可能不可用。此外，传统方法和数据驱动方法对目标物体上的高不透明自投射阴影都不鲁棒。我们表明，当目标天体旋转时，这些阴影可能导致姿态估计产生很大偏差。对于这些天体，实时姿态估计算法中的偏差可能会误导航天器的状态估计器并导致任务失败，特别是当天体经历混沌翻滚运动时。我们提出了COFFEE（天体遮挡快速特征提取器），这是一种为小行星设计的实时姿态估计框架，旨在利用航天器上常用的太阳跟踪传感器提供的太阳相位角先验信息。通过将显著轮廓与其投射的阴影关联，检测一组对阴影运动不变的特征稀疏集。然后联合训练稀疏神经网络和基于注意力的图神经网络特征匹配模型，以提供连续帧之间的对应关系集合。结果表明，所提出的姿态估计管道无偏差，比传统姿态估计管道更准确，在合成数据以及旋转小行星Apophis的渲染上比其他最先进的深度学习管道快一个数量级。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决对未知太空小行星（尤其是具有混沌翻滚运动的小行星）进行实时、准确的姿态估计问题。这个问题在现实中非常重要，因为准确估计小行星姿态是航天器与小行星轨道同步、最终接近和着陆的关键前提。传统方法实时但不准确，深度学习方法准确但计算资源需求高，且小行星表面的高度均匀性和自影会导致姿态估计产生很大偏差，可能误导航天器状态估计器，导致任务失败，特别是对于像Apophis这样进行混沌翻滚运动的小行星。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了太空小行星姿态估计的特殊挑战，认识到小行星上的高对比度边缘实际上是巨石和陨石坑投射的自影。他们提出利用航天器上常见的太阳跟踪传感器提供的太阳相位角先验信息，来区分几何特征和投射阴影。方法设计包括关键点检测（利用太阳光线方向信息）、特征描述（使用稀疏子流形CNN）、特征匹配（基于注意力的图神经网络）和姿态估计（5点算法和RANSAC）。作者借鉴了传统特征检测器的基本思想，但针对太空环境进行了改进；使用了稀疏卷积神经网络处理稀疏关键点；借鉴了Lightglue架构用于特征匹配；并使用了计算机视觉中的经典姿态估计方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用太阳相位角信息来区分小行星表面的真实几何特征和由这些特征投射的阴影，通过分析太阳光线的投影，找到不会随着小行星旋转而移动的几何特征。整体流程包括：1)关键点检测：利用太阳光线方向计算消失点，沿指向消失点的光线应用边缘滤波，保留负边缘并编码阴影大小；2)特征描述：使用稀疏子流形CNN将关键点转换为256维特征向量；3)特征匹配：结合图神经网络和注意力机制进行自注意力和交叉注意力处理；4)姿态估计：使用5点算法和RANSAC从对应关键点恢复相对姿态，并利用测距传感器获取深度信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)针对太空环境的特征检测，利用太阳相位角区分几何特征和阴影；2)使用稀疏子流形CNN处理关键点而非整个图像；3)结合图神经网络和注意力机制进行特征匹配；4)实现对自影变化的鲁棒性，消除传统方法中的姿态估计偏差。相比传统方法，COFFEE更准确且对阴影变化鲁棒；相比深度学习方法，计算效率更高且专门针对太空小行星场景设计；相比其他考虑光照条件的方法，明确将太阳相位角作为先验信息整合到状态估计管道中。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; COFFEE通过利用太阳相位角信息结合稀疏神经网络，实现了对未知翻滚小行星实时、准确且对阴影变化具有鲁棒性的姿态估计，解决了太空任务中小行星轨道同步和接近的关键挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The accurate state estimation of unknown bodies in space is a criticalchallenge with applications ranging from the tracking of space debris to theshape estimation of small bodies. A necessary enabler to this capability is tofind and track features on a continuous stream of images. Existing methods,such as SIFT, ORB and AKAZE, achieve real-time but inaccurate pose estimates,whereas modern deep learning methods yield higher quality features at the costof more demanding computational resources which might not be available onspace-qualified hardware. Additionally, both classical and data-driven methodsare not robust to the highly opaque self-cast shadows on the object ofinterest. We show that, as the target body rotates, these shadows may lead tolarge biases in the resulting pose estimates. For these objects, a bias in thereal-time pose estimation algorithm may mislead the spacecraft's stateestimator and cause a mission failure, especially if the body undergoes achaotic tumbling motion. We present COFFEE, the Celestial Occlusion FastFEature Extractor, a real-time pose estimation framework for asteroids designedto leverage prior information on the sun phase angle given by sun-trackingsensors commonly available onboard spacecraft. By associating salient contoursto their projected shadows, a sparse set of features are detected, invariant tothe motion of the shadows. A Sparse Neural Network followed by anattention-based Graph Neural Network feature matching model are then jointlytrained to provide a set of correspondences between successive frames. Theresulting pose estimation pipeline is found to be bias-free, more accurate thanclassical pose estimation pipelines and an order of magnitude faster than otherstate-of-the-art deep learning pipelines on synthetic data as well as onrenderings of the tumbling asteroid Apophis.</description>
      <author>example@mail.com (Arion Zimmermann, Soon-Jo Chung, Fred Hadaegh)</author>
      <guid isPermaLink="false">2508.03132v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>GEDAN: Learning the Edit Costs for Graph Edit Distance</title>
      <link>http://arxiv.org/abs/2508.03111v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新颖的图神经网络框架，通过监督和无监督训练来近似图编辑距离(GED)，该方法集成了广义加性模型以学习上下文感知的编辑成本，显著提高了适应性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;图编辑距离(GED)是将一个图转换为另一个图的最小成本变换，是衡量图之间差异的广泛采用的度量标准。然而，GED的计算是NP难的，这促使了各种近似方法的发展，包括基于神经网络的方法。大多数现有方法假设单位成本编辑操作，这在现实应用中不太现实。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决现有基于神经网络的GED近似方法中单位成本编辑操作的不现实约束问题，提出一种能够学习灵活且可解释的上下文感知编辑成本的图神经网络框架。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种新颖的图神经网络框架，使用监督和无监督训练来近似GED。在无监督设置中，它采用仅基于梯度的自组织机制，无需真实距离即可进行优化。架构的核心组件是广义加性模型的集成，这允许灵活且可解释地学习上下文感知的编辑成本。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法与最先进的参考方法取得了相似的结果，但显著提高了适应性和可解释性。学习到的成本函数为复杂的图结构提供了见解。&lt;h4&gt;结论&lt;/h4&gt;该方法通过学习上下文感知的编辑成本，不仅实现了与现有方法相当的性能，还提高了模型的适应性和可解释性，使其在分子分析和结构模式发现等领域特别有价值。&lt;h4&gt;翻译&lt;/h4&gt;图编辑距离(GED)被定义为将一个图转换为另一个图的最小成本变换，是衡量图之间差异的广泛采用的度量标准。GED的主要问题是其计算是NP难的，这反过来又促使了各种近似方法的发展，包括基于神经网络(NN)的方法。大多数这些基于神经网络的模型通过假设单位成本编辑操作来简化GED问题，这在现实应用中是一个相当不现实的约束。在这项工作中，我们提出了一种新颖的图神经网络框架，使用监督和无监督训练来近似GED。在无监督设置中，它采用仅基于梯度的自组织机制，使无需真实距离即可进行优化。此外，我们架构的一个核心组件是广义加性模型的集成，这允许灵活且可解释地学习上下文感知的编辑成本。实验结果表明，所提出的方法与最先进的参考方法取得了相似的结果，但显著提高了适应性和可解释性。也就是说，学习到的成本函数为复杂的图结构提供了见解，使其在分子分析和结构模式发现等领域特别有价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Edit Distance (GED) is defined as the minimum cost transformation ofone graph into another and is a widely adopted metric for measuring thedissimilarity between graphs. The major problem of GED is that its computationis NP-hard, which has in turn led to the development of various approximationmethods, including approaches based on neural networks (NN). Most of theseNN-based models simplify the problem of GED by assuming unit-cost editoperations, a rather unrealistic constraint in real-world applications. In thiswork, we present a novel Graph Neural Network framework that approximates GEDusing both supervised and unsupervised training. In the unsupervised setting,it employs a gradient-only self-organizing mechanism that enables optimizationwithout ground-truth distances. Moreover, a core component of our architectureis the integration of a Generalized Additive Model, which allows the flexibleand interpretable learning of context-aware edit costs. Experimental resultsshow that the proposed method achieves similar results as state-of-the-artreference methods, yet significantly improves both adaptability andinterpretability. That is, the learned cost function offers insights intocomplex graph structures, making it particularly valuable in domains such asmolecular analysis and structural pattern discovery.</description>
      <author>example@mail.com (Francesco Leonardi, Markus Orsi, Jean-Louis Reymond, Kaspar Riesen)</author>
      <guid isPermaLink="false">2508.03111v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Considering Spatial Structure of the Road Network in Pavement Deterioration Modeling</title>
      <link>http://arxiv.org/abs/2508.02749v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过图神经网络将道路网络的空间依赖性纳入路面退化建模，以提高预测性能&lt;h4&gt;背景&lt;/h4&gt;路面退化建模对于提供道路网络未来状态信息和确定预防性维护或修复需求非常重要&lt;h4&gt;目的&lt;/h4&gt;探索考虑道路网络的空间结构是否能提高退化模型的预测性能&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNN)将道路网络的空间依赖性纳入路面退化建模，数据来自德克萨斯州交通部维护的路面管理信息系统(PMIS)，包含超过50万个观测值&lt;h4&gt;主要发现&lt;/h4&gt;比较结果表明，当考虑空间关系时，路面退化预测模型的性能更好&lt;h4&gt;结论&lt;/h4&gt;将空间关系考虑在内的路面退化预测模型表现更好，证明了在建模中考虑道路网络空间结构的重要性&lt;h4&gt;翻译&lt;/h4&gt;路面退化建模对于提供道路网络未来状态信息和确定预防性维护或修复需求非常重要。本研究通过图神经网络(GNN)将道路网络的空间依赖性纳入路面退化建模。使用图神经网络进行路面性能建模的关键动机是能够轻松直接地利用网络中的丰富结构信息。本文探讨了考虑道路网络的空间结构是否能提高退化模型的预测性能。本研究使用的数据集来自德克萨斯州交通部维护的路面管理信息系统(PMIS)，包含超过50万个观测值的有大路面状况数据集。有前景的比较结果表明，当考虑空间关系时，路面退化预测模型的性能更好&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1177/03611981231188373&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pavement deterioration modeling is important in providing informationregarding the future state of the road network and in determining the needs ofpreventive maintenance or rehabilitation treatments. This research incorporatedspatial dependence of road network into pavement deterioration modeling througha graph neural network (GNN). The key motivation of using a GNN for pavementperformance modeling is the ability to easily and directly exploit the richstructural information in the network. This paper explored if consideringspatial structure of the road network will improve the prediction performanceof the deterioration models. The data used in this research comprises a largepavement condition data set with more than a half million observations takenfrom the Pavement Management Information System (PMIS) maintained by the TexasDepartment of Transportation. The promising comparison results indicates thatpavement deterioration prediction models perform better when spatialrelationship is considered.</description>
      <author>example@mail.com (Lu Gao, Ke Yu, Pan Lu)</author>
      <guid isPermaLink="false">2508.02749v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes</title>
      <link>http://arxiv.org/abs/2508.03484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SmartGen，一个基于大型语言模型(LLM)的框架，用于生成情境感知的用户行为数据，以支持智能家居模型在行为漂移情况下的持续适应。该框架包含四个关键组件：时间和语义感知分割模块、语义感知序列压缩、图引导序列合成和两阶段异常过滤器。实验表明，SmartGen显著提高了异常检测和行为预测任务的性能。&lt;h4&gt;背景&lt;/h4&gt;随着智能家居的普及，智能模型被广泛用于异常检测和行为预测等任务。这些模型通常在静态数据集上训练，对由季节变化、生活方式转变或日常习惯演变引起的行为漂移较为脆弱。然而，重新收集行为数据进行再训练往往不切实际，因为数据收集缓慢、成本高且存在隐私问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于LLM的框架SmartGen，用于合成情境感知的用户行为数据，以支持智能家居模型在行为漂移情况下的持续适应。&lt;h4&gt;方法&lt;/h4&gt;SmartGen包含四个关键组件：1)时间和语义感知分割模块，在双时间跨度约束下将长行为序列分割为语义连贯的子序列；2)语义感知序列压缩，通过在潜在空间中对行为映射进行聚类来减少输入长度同时保留代表性语义；3)图引导序列合成，构建行为关系图并将频繁转换编码为提示，指导LLM生成与情境变化一致的数据；4)两阶段异常过滤器，识别并移除不合理或语义不一致的输出。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界数据集上的实验表明，SmartGen在行为漂移情况下显著提高了模型性能，异常检测任务平均提高85.43%，行为预测任务平均提高70.51%。&lt;h4&gt;结论&lt;/h4&gt;SmartGen是一个有效的框架，能够通过合成情境感知的用户行为数据来支持智能家居模型在行为漂移情况下的持续适应，显著提升了模型在异常检测和行为预测任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;随着智能家居变得越来越普遍，智能模型被广泛用于异常检测和行为预测等任务。这些模型通常在静态数据集上训练，对由季节变化、生活方式转变或日常习惯演变引起的行为漂移较为脆弱。然而，由于数据收集缓慢、成本高和隐私问题，重新收集行为数据进行再训练往往不切实际。在本文中，我们提出了SmartGen，一个基于LLM的框架，用于合成情境感知的用户行为数据，以支持下游智能家居模型的持续适应。SmartGen包含四个关键组件：时间和语义感知分割模块、语义感知序列压缩、图引导序列合成和两阶段异常过滤器。在三个真实世界数据集上的实验表明，SmartGen在行为漂移情况下显著提高了异常检测和行为预测任务的性能，异常检测平均提高85.43%，行为预测平均提高70.51%。代码可在https://github.com/horizonsinzqs/SmartGen获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As smart homes become increasingly prevalent, intelligent models are widelyused for tasks such as anomaly detection and behavior prediction. These modelsare typically trained on static datasets, making them brittle to behavioraldrift caused by seasonal changes, lifestyle shifts, or evolving routines.However, collecting new behavior data for retraining is often impractical dueto its slow pace, high cost, and privacy concerns. In this paper, we proposeSmartGen, an LLM-based framework that synthesizes context-aware user behaviordata to support continual adaptation of downstream smart home models. SmartGenconsists of four key components. First, we design a Time and Semantic-awareSplit module to divide long behavior sequences into manageable, semanticallycoherent subsequences under dual time-span constraints. Second, we proposeSemantic-aware Sequence Compression to reduce input length while preservingrepresentative semantics by clustering behavior mapping in latent space. Third,we introduce Graph-guided Sequence Synthesis, which constructs a behaviorrelationship graph and encodes frequent transitions into prompts, guiding theLLM to generate data aligned with contextual changes while retaining corebehavior patterns. Finally, we design a Two-stage Outlier Filter to identifyand remove implausible or semantically inconsistent outputs, aiming to improvethe factual coherence and behavioral validity of the generated sequences.Experiments on three real-world datasets demonstrate that SmartGensignificantly enhances model performance on anomaly detection and behaviorprediction tasks under behavioral drift, with anomaly detection improving by85.43% and behavior prediction by 70.51% on average. The code is available athttps://github.com/horizonsinzqs/SmartGen.</description>
      <author>example@mail.com (Zhiyao Xu, Dan Zhao, Qingsong Zou, Qing Li, Yong Jiang, Yuhang Wang, Jingyu Xiao)</author>
      <guid isPermaLink="false">2508.03484v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio</title>
      <link>http://arxiv.org/abs/2508.02944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page at https://byteaigc.github.io/X-Actor/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;X-Actor是一种新颖的音频驱动肖像动画框架，可以从单个参考图像和输入音频生成逼真、富有情感表现力的说话头部视频。&lt;h4&gt;背景&lt;/h4&gt;先前的方法强调在有限说话场景中的口型同步和短距离视觉保真度，无法实现长形式的、情感丰富的肖像表演。&lt;h4&gt;目的&lt;/h4&gt;开发能够实现演员质量、长形式的肖像表演系统，捕捉与言语节奏和内容流畅连贯的细微动态情感变化。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段解耦生成管道：首先使用音频条件自回归扩散模型在长时间上下文窗口内预测表情丰富但身份无关的面部运动潜在标记；然后通过基于扩散的视频合成模块将这些动作转换为高保真视频动画。&lt;h4&gt;主要发现&lt;/h4&gt;在紧凑的面部运动潜在空间中操作，与视觉和身份线索解耦，通过扩散强制训练范式有效捕捉音频和面部动力学之间的长程相关性，实现无限长度的情感丰富动作预测而不会出现误差累积。&lt;h4&gt;结论&lt;/h4&gt;X-Actor产生了引人入胜的电影风格表演，超越了标准的说话头部动画，在长范围、音频驱动的情感肖像表演方面取得了最先进的结果。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了X-Actor，一个新颖的音频驱动肖像动画框架，可以从单个参考图像和输入音频片段生成逼真、富有情感表现力的说话头部视频。与之前强调在有限说话场景中口型同步和短距离视觉保真度的方法不同，X-Actor能够实现演员质量的长形式肖像表演，捕捉与言语节奏和内容流畅连贯的细微动态情感变化。我们方法的核心是一种两阶段解耦生成管道：一个音频条件自回归扩散模型，在长时间上下文窗口内预测表情丰富但身份无关的面部运动潜在标记；随后是一个基于扩散的视频合成模块，将这些动作转换为高保真视频动画。在紧凑的面部运动潜在空间中操作，与视觉和身份线索解耦，我们的自回归扩散模型通过扩散强制训练范式有效捕捉音频和面部动力学之间的长程相关性，实现无限长度的情感丰富动作预测而不会出现误差累积。大量实验表明X-Actor产生了引人入胜的电影风格表演，超越了标准的说话头部动画，在长范围、音频驱动的情感肖像表演方面取得了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present X-Actor, a novel audio-driven portrait animation framework thatgenerates lifelike, emotionally expressive talking head videos from a singlereference image and an input audio clip. Unlike prior methods that emphasizelip synchronization and short-range visual fidelity in constrained speakingscenarios, X-Actor enables actor-quality, long-form portrait performancecapturing nuanced, dynamically evolving emotions that flow coherently with therhythm and content of speech. Central to our approach is a two-stage decoupledgeneration pipeline: an audio-conditioned autoregressive diffusion model thatpredicts expressive yet identity-agnostic facial motion latent tokens within along temporal context window, followed by a diffusion-based video synthesismodule that translates these motions into high-fidelity video animations. Byoperating in a compact facial motion latent space decoupled from visual andidentity cues, our autoregressive diffusion model effectively captureslong-range correlations between audio and facial dynamics through adiffusion-forcing training paradigm, enabling infinite-length emotionally-richmotion prediction without error accumulation. Extensive experiments demonstratethat X-Actor produces compelling, cinematic-style performances that go beyondstandard talking head animations and achieves state-of-the-art results inlong-range, audio-driven emotional portrait acting.</description>
      <author>example@mail.com (Chenxu Zhang, Zenan Li, Hongyi Xu, You Xie, Xiaochen Zhao, Tianpei Gu, Guoxian Song, Xin Chen, Chao Liang, Jianwen Jiang, Linjie Luo)</author>
      <guid isPermaLink="false">2508.02944v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition</title>
      <link>http://arxiv.org/abs/2508.03695v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025; First two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Trokens的新方法，通过将轨迹点转换为语义感知的关系标记，结合自适应采样策略和运动建模框架，有效结合了运动信息和外观特征，在六个少样本动作识别基准测试上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;视频理解需要有效建模运动和外观信息，特别是在少样本动作识别任务中。尽管最近的点跟踪技术已显示出对少样本动作识别的改进，但仍然存在两个基本挑战：选择有信息量的跟踪点和有效建模它们的运动模式。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法，解决少样本动作识别中的两个基本挑战：选择有信息量的跟踪点和有效建模运动模式，从而提高动作识别的性能。&lt;h4&gt;方法&lt;/h4&gt;Trokens方法包括两个主要部分：1) 语义感知的采样策略，根据对象尺度和语义相关性自适应分布跟踪点；2) 运动建模框架，通过方向位移直方图(HoD)捕获轨迹内动态，并通过轨迹间关系建模复杂动作模式。该方法将这些轨迹标记与语义特征结合，用运动信息增强外观特征。&lt;h4&gt;主要发现&lt;/h4&gt;Trokens方法在六个不同的少样本动作识别基准测试上取得了最先进的性能，包括Something-Something-V2（完整和小型分割）、Kinetics、UCF101、HMDB51和FineGym。&lt;h4&gt;结论&lt;/h4&gt;通过将轨迹点转换为语义感知的关系标记，并采用自适应采样和运动建模策略，Trokens方法有效解决了少样本动作识别中的关键挑战，显著提升了性能。&lt;h4&gt;翻译&lt;/h4&gt;视频理解需要对运动和外观信息进行有效建模，特别是在少样本动作识别方面。尽管最近在点跟踪方面的进展已被证明可以改善少样本动作识别，但两个基本挑战仍然存在：选择信息丰富的跟踪点和有效建模它们的运动模式。我们提出了Trokens，一种新颖的方法，将轨迹点转换为用于动作识别的语义感知关系标记。首先，我们引入了一种语义感知的采样策略，根据对象尺度和语义相关性自适应分布跟踪点。其次，我们开发了一个运动建模框架，通过方向位移直方图(HoD)捕获轨迹内动态，并通过轨迹间关系建模复杂动作模式。我们的方法将这些轨迹标记与语义特征有效结合，用运动信息增强外观特征，在六个不同的少样本动作识别基准测试上实现了最先进的性能：Something-Something-V2（完整和小型分割）、Kinetics、UCF101、HMDB51和FineGym。项目页面请见https://trokens-iccv25.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video understanding requires effective modeling of both motion and appearanceinformation, particularly for few-shot action recognition. While recentadvances in point tracking have been shown to improve few-shot actionrecognition, two fundamental challenges persist: selecting informative pointsto track and effectively modeling their motion patterns. We present Trokens, anovel approach that transforms trajectory points into semantic-aware relationaltokens for action recognition. First, we introduce a semantic-aware samplingstrategy to adaptively distribute tracking points based on object scale andsemantic relevance. Second, we develop a motion modeling framework thatcaptures both intra-trajectory dynamics through the Histogram of OrientedDisplacements (HoD) and inter-trajectory relationships to model complex actionpatterns. Our approach effectively combines these trajectory tokens withsemantic features to enhance appearance features with motion information,achieving state-of-the-art performance across six diverse few-shot actionrecognition benchmarks: Something-Something-V2 (both full and small splits),Kinetics, UCF101, HMDB51, and FineGym. For project page seehttps://trokens-iccv25.github.io</description>
      <author>example@mail.com (Pulkit Kumar, Shuaiyi Huang, Matthew Walmer, Sai Saketh Rambhatla, Abhinav Shrivastava)</author>
      <guid isPermaLink="false">2508.03695v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations</title>
      <link>http://arxiv.org/abs/2508.03420v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM 2025. 11 pages, 4 figures. Code:  https://github.com/wangbing1416/MISDER&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MISDER的新型虚假信息检测框架，该框架考虑了社会环境的动态变化，通过学习每个时期的社会环境表征并预测未来表征来提高虚假信息检测的准确性。&lt;h4&gt;背景&lt;/h4&gt;虚假信息在各类社交媒体平台上的广泛传播引起了学术界和工业界的关注，自动区分虚假信息（MD）已成为一个活跃的研究领域。&lt;h4&gt;目的&lt;/h4&gt;解决主流虚假信息检测方法中静态学习范式的局限性，考虑新闻真实性在动态变化的社会环境中的波动性。&lt;h4&gt;方法&lt;/h4&gt;提出MISDER框架，学习每个时期的社会环境表征，并使用时间模型预测未来时期的表征。具体包括三种变体：MISDER-LSTM（使用LSTM模型）、MISDER-ODE（使用连续动力学方程）和MISDER-PT（使用预训练动力学系统）。&lt;h4&gt;主要发现&lt;/h4&gt;在两个流行数据集上的实验结果表明，所提出的MISDER模型比各种MD基线方法更有效，证明了考虑社会环境动态变化对虚假信息检测的重要性。&lt;h4&gt;结论&lt;/h4&gt;通过引入动态环境表征，MISDER框架能够更好地捕捉虚假信息在动态社会环境中的变化特性，提高了虚假信息检测的准确性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;虚假信息在各类社交媒体平台上的广泛传播由于其有害效应而引起了学术界和工业界的显著关注。因此，自动区分虚假信息（被称为虚假信息检测MD）已成为一个日益活跃的研究课题。主流方法将MD构建为静态学习范式，学习新闻文章的内容、链接和传播与相应的人工真实性标签之间的映射。然而，在现实场景中，静态假设经常被违反，因为新闻文章的真实性可能在动态演变的社会环境中波动。为了解决这个问题，我们提出了一个新框架，即带有动态环境表征的虚假信息检测（MISDER）。MISDER的基本思想在于为每个时期学习一个社会环境表征，并使用时间模型来预测未来时期的表征。在这项工作中，我们将时间模型指定为LSTM模型、连续动力学方程和预训练动力学系统，分别提出了MISDER的三种变体，即MISDER-LSTM、MISDER-ODE和MISDER-PT。为了评估MISDER的性能，我们在两个流行数据集上将其与各种MD基线方法进行了比较，实验结果可以证明我们提出的模型的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of misinformation across diverse social media platforms hasdrawn significant attention from both academic and industrial communities dueto its detrimental effects. Accordingly, automatically distinguishingmisinformation, dubbed as Misinformation Detection (MD), has become anincreasingly active research topic. The mainstream methods formulate MD as astatic learning paradigm, which learns the mapping between the content, links,and propagation of news articles and the corresponding manual veracity labels.However, the static assumption is often violated, since in real-worldscenarios, the veracity of news articles may vacillate within the dynamicallyevolving social environment. To tackle this problem, we propose a novelframework, namely Misinformation detection with Dynamic EnvironmentalRepresentations (MISDER). The basic idea of MISDER lies in learning a socialenvironmental representation for each period and employing a temporal model topredict the representation for future periods. In this work, we specify thetemporal model as the LSTM model, continuous dynamics equation, and pre-traineddynamics system, suggesting three variants of MISDER, namely MISDER-LSTM,MISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER,we compare it to various MD baselines across 2 prevalent datasets, and theexperimental results can indicate the effectiveness of our proposed model.</description>
      <author>example@mail.com (Bing Wang, Ximing Li, Yiming Wang, Changchun Li, Jiaxu Cui, Renchu Guan, Bo Yang)</author>
      <guid isPermaLink="false">2508.03420v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Full-History Graphs with Edge-Type Decoupled Networks for Temporal Reasoning</title>
      <link>http://arxiv.org/abs/2508.03251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  European Conference of Artificial Intelligence 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种全历史图表示方法和边类型解耦网络(ETDNet)，用于建模实体间随时间演化的交互关系，在驾驶员意图预测和比特币欺诈检测任务上超越了现有基线方法。&lt;h4&gt;背景&lt;/h4&gt;在许多现实世界任务中，建模实体间随时间演化的交互关系至关重要，如预测驾驶员操作需要跟踪相邻车辆的相对运动，检测金融欺诈需要追踪资金在网络中的传播路径。这些任务不同于经典时间序列预测，需要推理谁与谁交互以及何时交互。&lt;h4&gt;目的&lt;/h4&gt;提出一种时间图表示方法，使关系及其演化都显式化，以解决需要推理实体间交互关系及其随时间变化的问题。&lt;h4&gt;方法&lt;/h4&gt;提出全历史图，为每个实体在每个时间步创建节点，并分离两种边集：时间内步边捕获单个帧内的关系，时间间步边连接实体在连续步骤中的自身。设计边类型解耦网络，包含图注意力模块、多头时间注意力模块和融合模块。&lt;h4&gt;主要发现&lt;/h4&gt;在Waymo驾驶员意图预测和Elliptic++比特币欺诈检测任务上，ETDNet超越强基线：将Waymo联合准确率提升至75.6%(对比74.1%)，将Elliptic++非法类别F1值提升至88.1%(对比60.4%)。&lt;h4&gt;结论&lt;/h4&gt;这些性能提升证明了在单一图中将结构关系和时间关系表示为不同边的好处。&lt;h4&gt;翻译&lt;/h4&gt;建模实体间不断演化的交互关系在许多现实世界任务中至关重要。例如，预测交通中的驾驶员操作需要跟踪相邻车辆在连续帧中如何加速、刹车和变道。同样，检测金融欺诈依赖于追踪资金通过连续交易在网络中传播的路径。与经典时间序列预测不同，这些场景需要推理谁与谁交互以及何时交互，这需要一种时间图表示方法，使关系及其演化都显式化。现有的时间图方法通常使用快照图来编码时间演化。我们引入了一种全历史图，为每个实体在每个时间步实例化一个节点，并分离两种边集：(i)时间内步边捕获单个帧内的关系，(ii)时间间步边将实体连接到其连续步骤中的自身。为了在此图上进行学习，我们设计了边类型解耦网络(ETDNet)，包含并行模块：图注意力模块沿时间内步边聚合信息，多头时间注意力模块关注实体的时间间步历史，融合模块在每一层后组合这两种消息。在驾驶员意图预测(Waymo)和比特币欺诈检测(Elliptic++)上评估，ETDNet一致超越强基线，将Waymo联合准确率提升至75.6%(对比74.1%)，将Elliptic++非法类别F1提升至88.1%(对比60.4%)。这些增益证明了在单一图中将结构关系和时间关系表示为不同边的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling evolving interactions among entities is critical in many real-worldtasks. For example, predicting driver maneuvers in traffic requires trackinghow neighboring vehicles accelerate, brake, and change lanes relative to oneanother over consecutive frames. Likewise, detecting financial fraud hinges onfollowing the flow of funds through successive transactions as they propagatethrough the network. Unlike classic time-series forecasting, these settingsdemand reasoning over who interacts with whom and when, calling for atemporal-graph representation that makes both the relations and their evolutionexplicit. Existing temporal-graph methods typically use snapshot graphs toencode temporal evolution. We introduce a full-history graph that instantiatesone node for every entity at every time step and separates two edge sets: (i)intra-time-step edges that capture relations within a single frame and (ii)inter-time-step edges that connect an entity to itself at consecutive steps. Tolearn on this graph we design an Edge-Type Decoupled Network (ETDNet) withparallel modules: a graph-attention module aggregates information alongintra-time-step edges, a multi-head temporal-attention module attends over anentity's inter-time-step history, and a fusion module combines the two messagesafter every layer. Evaluated on driver-intention prediction (Waymo) and Bitcoinfraud detection (Elliptic++), ETDNet consistently surpasses strong baselines,lifting Waymo joint accuracy to 75.6\% (vs. 74.1\%) and raising Elliptic++illicit-class F1 to 88.1\% (vs. 60.4\%). These gains demonstrate the benefit ofrepresenting structural and temporal relations as distinct edges in a singlegraph.</description>
      <author>example@mail.com (Osama Mohammed, Jiaxin Pan, Mojtaba Nayyeri, Daniel Hernández, Steffen Staab)</author>
      <guid isPermaLink="false">2508.03251v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering</title>
      <link>http://arxiv.org/abs/2508.03039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VideoForest框架，通过以人为中心的分层推理解决跨视频问答的挑战，利用人物级特征作为视频间的自然桥梁，无需端到端训练即可实现有效的跨视频理解。&lt;h4&gt;背景&lt;/h4&gt;跨视频问答面临显著挑战，超越了传统的单视频理解，特别是在建立视频流之间的有意义连接和管理多源信息检索的复杂性方面。&lt;h4&gt;目的&lt;/h4&gt;引入VideoForest框架解决跨视频理解挑战，并开发CrossVideoQA基准数据集专门用于以人为中心的跨视频分析。&lt;h4&gt;方法&lt;/h4&gt;VideoForest整合三个关键创新：1)以人为中心特征提取机制，使用ReID和跟踪算法建立多视频源间的时空关系；2)多粒度生成树结构，围绕人物级轨迹分层组织视觉内容；3)多智能体推理框架，有效遍历分层结构回答复杂跨视频查询。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明VideoForest在跨视频推理任务中表现优越：人物识别准确率71.93%，行为分析83.75%，总结和推理51.67%，显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;通过以人物级特征统一多个视频流，VideoForest建立了跨视频理解的新范式，能够在保持计算效率的同时实现分布式视觉信息的高级推理。&lt;h4&gt;翻译&lt;/h4&gt;跨视频问答提出了超越传统单视频理解的重大挑战，特别是在建立视频流之间的有意义连接和管理多源信息检索的复杂性方面。我们引入了VideoForest，这是一个新颖的框架，通过以人为中心的分层推理来解决这些挑战。我们的方法利用人物级特征作为视频之间的自然桥梁点，实现了有效的跨视频理解，而无需端到端训练。VideoForest整合了三个关键创新：1)采用ReID和跟踪算法的以人为中心特征提取机制，在多个视频源之间建立强大的时空关系；2)多粒度生成树结构，围绕人物级轨迹分层组织视觉内容；3)多智能体推理框架，有效遍历此分层结构以回答复杂的跨视频查询。为了评估我们的方法，我们开发了CrossVideoQA，这是一个专门设计用于以人为中心的跨视频分析的综合基准数据集。实验结果表明VideoForest在跨视频推理任务中具有卓越性能，在人物识别方面达到71.93%的准确率，在行为分析方面达到83.75%，在总结和推理方面达到51.67%，显著优于现有方法。我们的工作通过以人物级特征统一多个视频流，为跨视频理解建立了新范式，能够在保持计算效率的同时实现对分布式视觉信息的高级推理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3754573&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-video question answering presents significant challenges beyondtraditional single-video understanding, particularly in establishing meaningfulconnections across video streams and managing the complexity of multi-sourceinformation retrieval. We introduce VideoForest, a novel framework thataddresses these challenges through person-anchored hierarchical reasoning. Ourapproach leverages person-level features as natural bridge points betweenvideos, enabling effective cross-video understanding without requiringend-to-end training. VideoForest integrates three key innovations: 1) ahuman-anchored feature extraction mechanism that employs ReID and trackingalgorithms to establish robust spatiotemporal relationships across multiplevideo sources; 2) a multi-granularity spanning tree structure thathierarchically organizes visual content around person-level trajectories; and3) a multi-agent reasoning framework that efficiently traverses thishierarchical structure to answer complex cross-video queries. To evaluate ourapproach, we develop CrossVideoQA, a comprehensive benchmark datasetspecifically designed for person-centric cross-video analysis. Experimentalresults demonstrate VideoForest's superior performance in cross-video reasoningtasks, achieving 71.93% accuracy in person recognition, 83.75% in behavioranalysis, and 51.67% in summarization and reasoning, significantlyoutperforming existing methods. Our work establishes a new paradigm forcross-video understanding by unifying multiple video streams throughperson-level features, enabling sophisticated reasoning across distributedvisual information while maintaining computational efficiency.</description>
      <author>example@mail.com (Yiran Meng, Junhong Ye, Wei Zhou, Guanghui Yue, Xudong Mao, Ruomei Wang, Baoquan Zhao)</author>
      <guid isPermaLink="false">2508.03039v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Long Video Question Answering with Scene-Localized Frame Grouping</title>
      <link>http://arxiv.org/abs/2508.03009v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SLFG的新方法，通过场景帧重组机制增强多模态大语言模型在长视频理解中的表现，无需修改原始模型架构即可实现即插即用。&lt;h4&gt;背景&lt;/h4&gt;当前多模态大语言模型在长视频理解方面表现不佳，主要受限于资源无法处理所有视频帧。现有评估方法专注于识别特定帧而非整体场景理解，与实际应用需求不符。&lt;h4&gt;目的&lt;/h4&gt;提出新的视频问答任务场景SceneQA，强调基于场景的细节感知和推理能力，并开发LVSQA数据集以支持更公平的模型评估。&lt;h4&gt;方法&lt;/h4&gt;受人类认知启发，SLFG方法将单个帧组合成语义连贯的场景帧，利用场景定位和动态帧重组机制增强模型理解能力，无需修改原始架构。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SLFG方法在多个长视频基准测试中表现优异，代码和数据集将在指定网址发布。&lt;h4&gt;结论&lt;/h4&gt;SLFG方法有效解决了长视频理解中的资源限制问题，通过场景重组提高了模型效率，同时保持了良好的性能。&lt;h4&gt;翻译&lt;/h4&gt;当前多模态大语言模型通常在长视频理解中表现不佳，主要由于资源限制导致无法处理所有视频帧及其相关信息。高效提取相关信息成为一项挑战。现有框架和评估任务专注于从大量无关帧中识别包含核心对象的特定帧，这与现实世界应用的实际需求不符。为解决这一问题，我们在视频问答任务中提出了一种新场景SceneQA，强调基于场景的细节感知和推理能力。我们开发了LVSQA数据集来支持SceneQA任务，该数据集基于从LVBench中精心挑选的视频构建，包含新的问答对集合，以促进对MLLMs在长视频中场景感知能力的更公平评估。受人类认知启发，我们引入了一种名为SLFG的新方法。SLFG的核心思想是将单个帧组合成语义连贯的场景帧。通过利用场景定位方法和动态帧重组机制，SLFG显著增强了现有MLLMs对长视频的理解能力。SLFG无需修改原始模型架构，并具有出色的即插即用可用性。实验结果表明，该方法在多个长视频基准测试中表现异常出色。代码和数据集将在http://www.slfg.pkuzwh.cn发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态大语言模型在长视频理解中表现不佳的问题。当前模型因资源限制无法处理视频所有帧，现有方法侧重于从大量帧中识别特定对象帧，这与现实应用需求不符。这个问题很重要，因为长视频理解对于视频内容分析、自动摘要、智能监控等应用至关重要，且现有方法无法满足对场景细节感知和推理能力的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从人类认知过程中获取灵感，指出人类处理长视频时不是逐帧分析，而是浏览视频并对场景转换保持敏感，然后专注于相关场景内的细节。基于这一认知，作者认为模型分析的基本单元应从单个帧转向语义连贯的场景。作者借鉴了现有工作中的视频帧采样、分组技术、大语言模型生成场景表示、语义相似度计算等方法，但将这些技术组合成一个专注于场景级别理解的新框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将单个帧组合成语义连贯的场景帧，通过场景定位方法和动态帧重组机制增强模型对长视频的理解能力。整体流程包括四个阶段：1）帧分组描述：密集采样视频帧并分组，用多模态大语言模型提取视觉描述；2）场景生成：用大语言模型将视觉描述抽象为场景级表示；3）场景定位：计算问题场景与各帧组场景的语义相似度；4）帧重组：根据相似度分数调整帧组结构，合并高相关性组，扩展关键片段时间窗口。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）提出SceneQA任务场景，强调基于场景的细节感知和推理能力；2）构建LVSQA数据集，支持对MLLMs长视频场景感知能力的评估；3）提出SLFG方法，将帧组织成语义连贯的场景单元，无需修改原始模型架构。相比之前工作，本文实现了分析单元从单个帧到语义连贯场景的转变，评估方法从识别特定帧转向理解整个场景，通过场景级信息压缩提升了处理效率，并增强了方法的通用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了基于场景定位的帧分组方法(SLFG)，通过将视频帧组织成语义连贯的场景单元，显著提升了多模态大语言模型对长视频的理解能力，并构建了新的评估基准LVSQA以促进长视频理解研究。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current Multimodal Large Language Models (MLLMs) often perform poorly in longvideo understanding, primarily due to resource limitations that prevent themfrom processing all video frames and their associated information. Efficientlyextracting relevant information becomes a challenging task. Existing frameworksand evaluation tasks focus on identifying specific frames containing coreobjects from a large number of irrelevant frames, which does not align with thepractical needs of real-world applications. To address this issue, we propose anew scenario under the video question-answering task, SceneQA, which emphasizesscene-based detail perception and reasoning abilities. And we develop the LVSQAdataset to support the SceneQA task, which is built upon carefully selectedvideos from LVBench and contains a new collection of question-answer pairs topromote a more fair evaluation of MLLMs' scene perception abilities in longvideos. Inspired by human cognition, we introduce a novel method called SLFG.The core idea of SLFG is to combine individual frames into semanticallycoherent scene frames. By leveraging scene localization methods and dynamicframe reassembly mechanisms, SLFG significantly enhances the understandingcapabilities of existing MLLMs in long videos. SLFG requires no modification tothe original model architecture and boasts excellent plug-and-play usability.Experimental results show that this method performs exceptionally well inseveral long video benchmark tests. Code and dataset will be released athttp://www.slfg.pkuzwh.cn.</description>
      <author>example@mail.com (Xuyi Yang, Wenhao Zhang, Hongbo Jin, Lin Liu, Hongbo Xu, Yongwei Nie, Fei Yu, Fei Ma)</author>
      <guid isPermaLink="false">2508.03009v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion</title>
      <link>http://arxiv.org/abs/2508.03252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RSDNet的单阶段全稀疏3D目标检测网络，通过可分离的潜在框架改进了去噪扩散概率模型，实现了高效且鲁棒的3D目标检测。&lt;h4&gt;背景&lt;/h4&gt;DDPMs在鲁棒的3D目标检测任务中显示出成功，但现有方法通常依赖于3D框的分数匹配或预训练的扩散先验，且需要在推理时进行多步迭代，限制了效率。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法效率低下的问题，提出一种单阶段、全稀疏的3D目标检测网络，实现高效且鲁棒的检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出RSDNet，通过轻量级去噪网络在潜在特征空间学习去噪过程；重构DDPMs的加噪和去噪机制，构建多类型和多级别的噪声样本和目标；引入语义-几何条件引导感知物体边界和形状；设计可分离的去噪网络实现单步检测。&lt;h4&gt;主要发现&lt;/h4&gt;RSDNet能在多级扰动下有效理解场景分布，实现鲁棒可靠的检测；在全稀疏检测流程中表现良好；可分离的去噪网络设计提高了检测效率。&lt;h4&gt;结论&lt;/h4&gt;在公共基准上的大量实验表明，RSDNet能够超越现有方法，实现最先进的检测性能。&lt;h4&gt;翻译&lt;/h4&gt;去噪扩散概率模型已在鲁棒的3D目标检测任务中显示出成功。现有方法通常依赖于3D框的分数匹配或预训练的扩散先验。然而，它们通常需要在推理时进行多步迭代，这限制了效率。为此，我们提出了一种具有DDPMs可分离潜在框架的鲁棒单阶段全稀疏3D目标检测网络，命名为RSDNet。具体来说，RSDNet通过轻量级去噪网络在潜在特征空间中学习去噪过程。这使得RSDNet能够在多级扰动下有效理解场景分布，实现鲁棒可靠的检测。同时，我们重构了DDPMs的加噪和去噪机制，使DLF能够构建多类型和多级别的噪声样本和目标，增强RSDNet对多种扰动的鲁棒性。此外，引入了语义-几何条件引导以感知物体边界和形状，缓解了稀疏表示中的中心特征缺失问题，使RSDNet能够在全稀疏检测流程中运行。而且，DLF的可分离去噪网络设计使RSDNet能够在推理时进行单步检测，进一步提高检测效率。在公共基准上的大量实验表明，RSDNet能够超越现有方法，实现最先进的检测性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D目标检测中的多步推理效率和噪声鲁棒性问题。现有基于DDPMs的3D目标检测方法需要多步迭代推理，限制了检测效率，同时难以应对点云数据中存在的多种扰动(如点级随机噪声和全局几何失真)。这个问题在自动驾驶、AR/VR和机器人等实时应用中至关重要，因为这些应用需要高效且可靠的3D目标检测，而实际场景中的传感器数据常常受到各种噪声影响。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了DDPMs的鲁棒性来源，发现其鲁棒性来自训练阶段构建的噪声样本和目标，而非推理过程本身。然后重新思考了DDPMs的加噪和解噪机制，提出'样本拟合'概念，将复杂的分布匹配问题简化为简单的样本拟合问题。基于这些洞察，设计了可分离的潜在框架(DLF)。作者借鉴了DDPMs的基本原理、单阶段全稀疏3D检测管道(如CenterPoint、PillarNeXt)以及去噪自编码器(DAEs)的设计，将其作为轻量级去噪网络。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计一个可分离的潜在框架(DLF)，将去噪网络作为辅助分支，在潜在特征空间中指导主干网络学习多类型和多级别的去噪过程，训练阶段构建多类型噪声样本和目标以增强鲁棒性，推理阶段分离去噪网络实现单步检测。整体流程包括：1)全稀疏管道(FSP)使用3D和2D稀疏主干提取特征；2)噪声构建模块(NCM)构建多类型噪声样本；3)语义-几何条件层(SGCL)嵌入对象边界和形状的先验知识；4)去噪U-Net(DUNet)包括3DDU和2DDU两个轻量级网络指导去噪学习。训练时计算扩散损失和任务损失，推理时只需单步即可完成检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)可分离的潜在框架(DLF)，实现单步推理同时保持多噪声鲁棒性；2)多类型和多级别的噪声建模，使模型能应对点级随机噪声和全局几何失真等多种扰动；3)语义-几何条件引导，缓解下采样导致的中心特征缺失问题；4)基于DLF的RSDNet实现单步全稀疏检测。与传统DDPMs方法不同，DLF不是通过迭代去噪估计边界框分数，而是在潜在特征空间学习去噪过程；与只能处理高斯噪声的传统方法不同，DLF能处理多种扰动；与现有全稀疏检测方法相比，RSDNet特别强调了鲁棒性而不仅仅是效率和准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种可分离的潜在框架(DLF)，通过在潜在特征空间中学习多类型和多级别的去噪过程，实现了单步推理的鲁棒3D目标检测，同时保持了对多种扰动的强鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust3D object detection tasks. Existing methods often rely on the score matchingfrom 3D boxes or pre-trained diffusion priors. However, they typically requiremulti-step iterations in inference, which limits efficiency. To address this,we propose a \textbf{R}obust single-stage fully \textbf{S}parse 3D object\textbf{D}etection \textbf{Net}work with a Detachable Latent Framework (DLF) ofDDPMs, named RSDNet. Specifically, RSDNet learns the denoising process inlatent feature spaces through lightweight denoising networks like multi-leveldenoising autoencoders (DAEs). This enables RSDNet to effectively understandscene distributions under multi-level perturbations, achieving robust andreliable detection. Meanwhile, we reformulate the noising and denoisingmechanisms of DDPMs, enabling DLF to construct multi-type and multi-level noisesamples and targets, enhancing RSDNet robustness to multiple perturbations.Furthermore, a semantic-geometric conditional guidance is introduced toperceive the object boundaries and shapes, alleviating the center featuremissing problem in sparse representations, enabling RSDNet to perform in afully sparse detection pipeline. Moreover, the detachable denoising networkdesign of DLF enables RSDNet to perform single-step detection in inference,further enhancing detection efficiency. Extensive experiments on publicbenchmarks show that RSDNet can outperform existing methods, achievingstate-of-the-art detection.</description>
      <author>example@mail.com (Wentao Qu, Guofeng Mei, Jing Wang, Yujiao Wu, Xiaoshui Huang, Liang Xiao)</author>
      <guid isPermaLink="false">2508.03252v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Model Semantics in Representation Learning</title>
      <link>http://arxiv.org/abs/2508.03649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了结构约束如何影响不同架构间内部表示的兼容性，发现结构规律性使表示几何结构在架构变化下更稳定，某些归纳偏差能提高跨模型特征的互操作性。&lt;h4&gt;背景&lt;/h4&gt;深度网络学习到的内部表示通常对架构特定选择敏感，引发了对学习结构在不同模型间稳定性、对齐性和可转移性的疑问。&lt;h4&gt;目的&lt;/h4&gt;研究结构约束（如线性整形算子和校正路径）如何影响不同架构间内部表示的兼容性。&lt;h4&gt;方法&lt;/h4&gt;基于对结构化变换和收敛的先前研究见解，开发测量和分析不同架构网络间表示对齐的框架，结合理论分析、经验探针和受控转移实验。&lt;h4&gt;主要发现&lt;/h4&gt;结构规律性诱导的表示几何结构在架构变化下更加稳定，某些形式的归纳偏差不仅支持模型内的泛化，还提高了跨模型学习特征的互操作性。&lt;h4&gt;结论&lt;/h4&gt;表示可转移性对模型蒸馏、模块化学习和鲁棒学习系统的原则性设计具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;深度网络学习到的内部表示通常对架构特定选择敏感，引发了对学习结构在不同模型间稳定性、对齐性和可转移性的问题。在本文中，我们研究结构约束（如线性整形算子和校正路径）如何影响不同架构间内部表示的兼容性。基于对结构化变换和收敛的先前研究的见解，我们开发了一个框架，用于测量和分析具有不同但相关架构先验的网络之间的表示对齐。通过结合理论见解、经验探针和受控的转移实验，我们证明结构规律性诱导的表示几何结构在架构变化下更加稳定。这表明某些形式的归纳偏差不仅支持模型内的泛化，还提高了跨模型学习特征的互操作性。最后，我们讨论了表示可转移性对模型蒸馏、模块化学习和鲁棒学习系统原则性设计的意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The internal representations learned by deep networks are often sensitive toarchitecture-specific choices, raising questions about the stability,alignment, and transferability of learned structure across models. In thispaper, we investigate how structural constraints--such as linear shapingoperators and corrective paths--affect the compatibility of internalrepresentations across different architectures. Building on the insights fromprior studies on structured transformations and convergence, we develop aframework for measuring and analyzing representational alignment acrossnetworks with distinct but related architectural priors. Through a combinationof theoretical insights, empirical probes, and controlled transfer experiments,we demonstrate that structural regularities induce representational geometrythat is more stable under architectural variation. This suggests that certainforms of inductive bias not only support generalization within a model, butalso improve the interoperability of learned features across models. Weconclude with a discussion on the implications of representationaltransferability for model distillation, modular learning, and the principleddesign of robust learning systems.</description>
      <author>example@mail.com (Saleh Nikooroo, Thomas Engel)</author>
      <guid isPermaLink="false">2508.03649v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-Efficient Single Collaborative Branch for Recommendation</title>
      <link>http://arxiv.org/abs/2508.03518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于权重共享的新型推荐系统CoBraR，通过减少参数数量并提高准确性以外的方面，在不损害准确性的情况下提升推荐性能。&lt;h4&gt;背景&lt;/h4&gt;推荐系统通常依赖于用户和物品在联合嵌入空间中的表示以及相似度度量来计算相关性得分。在现代推荐系统中，获取用户和物品表示的模块由两个独立且分离的神经网络组成。&lt;h4&gt;目的&lt;/h4&gt;受多模态表示学习中权重共享方法的启发，提出一种利用用户和物品神经网络模块之间权重共享的新型推荐系统，以获取共享嵌入空间中的潜在表示。&lt;h4&gt;方法&lt;/h4&gt;提出的框架由一个单一的推荐协作分支(CoBraR)组成，该框架通过在用户和物品神经网络模块之间实现权重共享来减少参数数量并提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过在电子商务和电影推荐方面的定量实验，发现CoBraR能够减少参数数量，在不损害准确性的情况下提高准确性以外的方面。&lt;h4&gt;结论&lt;/h4&gt;CoBraR有潜力被应用于和扩展到实际场景中。&lt;h4&gt;翻译&lt;/h4&gt;推荐系统(RS)通常依赖于用户和物品在联合嵌入空间中的表示以及相似度度量来计算相关性得分。在现代RS中，获取用户和物品表示的模块由两个独立且分离的神经网络(NN)组成。在多模态表示学习中，权重共享已被证明在减少同一物品多个模态之间的距离方面是有效的。受这些方法的启发，我们提出了一种新型RS，它利用用户和物品神经网络模块之间的权重共享，以获取共享嵌入空间中的潜在表示。所提出的框架由一个单一的推荐协作分支(CoBraR)组成。我们通过电子商务和电影推荐的定量实验来评估CoBraR。我们的实验表明，通过减少参数数量并提高准确性以外的方面而不损害准确性，CoBraR有潜力被应用于和扩展到实际场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3705328.3759302&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recommender Systems (RS) often rely on representations of users and items ina joint embedding space and on a similarity metric to compute relevance scores.In modern RS, the modules to obtain user and item representations consist oftwo distinct and separate neural networks (NN). In multimodal representationlearning, weight sharing has been proven effective in reducing the distancebetween multiple modalities of a same item. Inspired by these approaches, wepropose a novel RS that leverages weight sharing between the user and item NNmodules used to obtain the latent representations in the shared embeddingspace. The proposed framework consists of a single Collaborative Branch forRecommendation (CoBraR). We evaluate CoBraR by means of quantitativeexperiments on e-commerce and movie recommendation. Our experiments show thatby reducing the number of parameters and improving beyond-accuracy aspectswithout compromising accuracy, CoBraR has the potential to be applied andextended for real-world scenarios.</description>
      <author>example@mail.com (Marta Moscati, Shah Nawaz, Markus Schedl)</author>
      <guid isPermaLink="false">2508.03518v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Imputation Drives Cross-Domain Alignment for EEG Classification</title>
      <link>http://arxiv.org/abs/2508.03437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACMMM 2025 poster&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;IMAC是一种创新的通道依赖掩码和插值自监督框架，通过将跨域EEG数据偏移对齐表述为时空序列插值任务，解决了因异构电极配置、采集协议和硬件差异导致的数据分布偏移问题，在多个EEG数据集上实现了最先进的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;脑电图(EEG)信号分类面临重大挑战，由于不同领域间的异构电极配置、采集协议和硬件差异导致数据分布偏移，影响了分类性能。&lt;h4&gt;目的&lt;/h4&gt;开发IMAC框架，将跨域EEG数据偏移的对齐表述为时空序列插值任务，以解决异构电极配置带来的数据分布偏移问题。&lt;h4&gt;方法&lt;/h4&gt;IMAC采用3D到2D位置统一映射策略标准化不同电极布局，建立统一空间表示；引入时空信号对齐，构建通道依赖掩码和重建任务，模拟跨域变化；采用解耦结构分别建模EEG信号的时间和空间信息，降低计算复杂度并提高灵活性。&lt;h4&gt;主要发现&lt;/h4&gt;在10个公开EEG数据集上全面评估，IMAC在跨subject和跨中心验证场景中实现最先进分类准确率；在模拟和真实世界分布偏移下表现出强大鲁棒性，完整性得分比基线方法高35%，同时保持一致的分类准确率。&lt;h4&gt;结论&lt;/h4&gt;IMAC能有效处理跨域EEG数据分布偏移问题，通过创新的插值和解耦结构实现高性能信号分类，展现出强大的鲁棒性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)信号分类由于跨领域异构电极配置、采集协议和硬件差异导致的数据分布偏移而面临重大挑战。本文介绍了IMAC，一种新颖的通道依赖掩码和插值自监督框架，将跨域EEG数据偏移的对齐表述为时空序列插值任务。为解决跨域场景中的异构电极配置问题，IMAC首先使用3D到2D位置统一映射策略标准化不同电极布局，建立统一空间表示。与以往基于掩码的自监督表示学习方法不同，IMAC引入了时空信号对齐。这包括构建通道依赖掩码和重建任务，表述为低到高分辨率的EEG空间插值问题。因此，该方法模拟了通道缺失和时间不稳定性等跨域变化，使模型能够在推理过程中利用提出的插值器进行稳健信号对齐。此外，IMAC采用解耦结构，分别建模EEG信号的时间和空间信息，降低计算复杂度同时提高灵活性和适应性。在10个公开可用的EEG数据集上的全面评估表明IMAC的卓越性能，在跨subject和跨中心验证场景中均实现了最先进的分类准确率。值得注意的是，IMAC在模拟和真实世界分布偏移下表现出强大的鲁棒性，完整性得分比基线方法高出35%，同时保持一致的分类准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3755582&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalogram (EEG) signal classification faces significant challengesdue to data distribution shifts caused by heterogeneous electrodeconfigurations, acquisition protocols, and hardware discrepancies acrossdomains. This paper introduces IMAC, a novel channel-dependent mask andimputation self-supervised framework that formulates the alignment ofcross-domain EEG data shifts as a spatial time series imputation task. Toaddress heterogeneous electrode configurations in cross-domain scenarios, IMACfirst standardizes different electrode layouts using a 3D-to-2D positionalunification mapping strategy, establishing unified spatial representations.Unlike previous mask-based self-supervised representation learning methods,IMAC introduces spatio-temporal signal alignment. This involves constructing achannel-dependent mask and reconstruction task framed as a low-to-highresolution EEG spatial imputation problem. Consequently, this approachsimulates cross-domain variations such as channel omissions and temporalinstabilities, thus enabling the model to leverage the proposed imputer forrobust signal alignment during inference. Furthermore, IMAC incorporates adisentangled structure that separately models the temporal and spatialinformation of the EEG signals separately, reducing computational complexitywhile enhancing flexibility and adaptability. Comprehensive evaluations across10 publicly available EEG datasets demonstrate IMAC's superior performance,achieving state-of-the-art classification accuracy in both cross-subject andcross-center validation scenarios. Notably, IMAC shows strong robustness underboth simulated and real-world distribution shifts, surpassing baseline methodsby up to $35$\% in integrity scores while maintaining consistent classificationaccuracy.</description>
      <author>example@mail.com (Hongjun Liu, Chao Yao, Yalan Zhang, Xiaokun wang, Xiaojuan Ban)</author>
      <guid isPermaLink="false">2508.03437v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN</title>
      <link>http://arxiv.org/abs/2508.03415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is currently under review for publication in an IEEE  Transactions. If accepted, the copyright will be transferred to IEEE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Fd-CycleGAN，一个图像到图像的翻译框架，通过增强潜在表示学习来近似真实数据分布。该方法基于CycleGAN进行改进，整合了局部邻域编码和频率感知监督，在多个数据集上展示了优于基线方法的性能。&lt;h4&gt;背景&lt;/h4&gt;图像到图像翻译是计算机视觉的重要任务，CycleGAN是该领域的基础框架，但在捕获细粒度局部语义和保持结构连贯性方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;增强潜在表示学习以近似真实数据分布，提高图像翻译的感知质量、收敛速度和模式多样性，特别是在低数据情况下。&lt;h4&gt;方法&lt;/h4&gt;提出Fd-CycleGAN框架，集成局部邻域编码和频率感知监督；使用基于分布的损失指标包括KL/JS散度和基于对数的相似性度量；在空间和频率域上量化真实和生成图像分布的对齐情况；在Horse2Zebra、Monet2Photo和合成的Strike-off数据集上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;Fd-CycleGAN相比基线方法具有更好的感知质量、更快的收敛速度和改进的模式多样性；在低数据情况下表现尤为突出；通过有效捕获局部和全局分布特征，实现了视觉上更连贯和语义上更一致的翻译；频率引导的潜在学习显著提高了图像翻译任务的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;频率引导的潜在学习可以显著提高图像翻译任务的泛化能力，有希望应用于文档恢复、艺术风格迁移和医学图像合成等领域；与基于扩散的生成模型相比，这种轻量级对抗性方法在训练效率和输出质量方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;Fd-CycleGAN: 一种增强潜在表示学习的图像到图像翻译框架&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents Fd-CycleGAN, an image-to-image (I2I) translationframework that enhances latent representation learning to approximate real datadistributions. Building upon the foundation of CycleGAN, our approachintegrates Local Neighborhood Encoding (LNE) and frequency-aware supervision tocapture fine-grained local pixel semantics while preserving structuralcoherence from the source domain. We employ distribution-based loss metrics,including KL/JS divergence and log-based similarity measures, to explicitlyquantify the alignment between real and generated image distributions in bothspatial and frequency domains. To validate the efficacy of Fd-CycleGAN, weconduct experiments on diverse datasets -- Horse2Zebra, Monet2Photo, and asynthetically augmented Strike-off dataset. Compared to baseline CycleGAN andother state-of-the-art methods, our approach demonstrates superior perceptualquality, faster convergence, and improved mode diversity, particularly inlow-data regimes. By effectively capturing local and global distributioncharacteristics, Fd-CycleGAN achieves more visually coherent and semanticallyconsistent translations. Our results suggest that frequency-guided latentlearning significantly improves generalization in image translation tasks, withpromising applications in document restoration, artistic style transfer, andmedical image synthesis. We also provide comparative insights withdiffusion-based generative models, highlighting the advantages of ourlightweight adversarial approach in terms of training efficiency andqualitative output.</description>
      <author>example@mail.com (Shivangi Nigam, Adarsh Prasad Behera, Shekhar Verma, P. Nagabhushan)</author>
      <guid isPermaLink="false">2508.03415v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>CIVQLLIE: Causal Intervention with Vector Quantization for Low-Light Image Enhancement</title>
      <link>http://arxiv.org/abs/2508.03338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出CIVQLLIE框架，通过离散表示学习和因果推理解决夜间低光图像增强问题，克服了现有方法在极暗条件下表现不佳和依赖简化假设的局限性。&lt;h4&gt;背景&lt;/h4&gt;夜间场景拍摄的图像存在严重可见度降低问题，阻碍有效内容感知。当前低光图像增强方法面临挑战：数据驱动的端到端映射网络缺乏可解释性或依赖不可靠先验指导，在极暗条件下表现不佳；而基于物理的方法依赖简化假设，往往在复杂现实场景中失效。&lt;h4&gt;目的&lt;/h4&gt;解决现有低光图像增强方法的局限性，提出一种利用离散表示学习和因果推理的新型框架CIVQLLIE。&lt;h4&gt;方法&lt;/h4&gt;通过向量量化将连续图像特征映射到从大规模高质量图像学习到的离散视觉标记码本，该码本作为可靠先验编码标准化亮度和颜色模式。针对分布偏移问题，提出多级因果干预：1)像素级因果干预模块对齐低级特征与码本期望的分布；2)特征感知因果干预机制结合低频选择性注意力门控识别和增强受光照退化影响最大的通道；3)高频细节重建模块利用匹配码本中的结构信息重建精细细节。&lt;h4&gt;主要发现&lt;/h4&gt;直接将向量量化应用于低光图像会因退化输入与学习码本间的分布偏移而失败，所提出的多级因果干预方法能有效纠正这些偏移。&lt;h4&gt;结论&lt;/h4&gt;CIVQLLIE框架通过离散表示学习和因果推理有效解决了低光图像增强问题，能够在复杂现实场景中提升图像质量，增强内容感知能力。&lt;h4&gt;翻译&lt;/h4&gt;夜间场景拍摄的图像存在严重可见度降低问题，阻碍有效内容感知。当前低光图像增强方法面临重大挑战：数据驱动的端到端映射网络缺乏可解释性或依赖不可靠的先验指导，在极暗条件下表现不佳，而基于物理的方法依赖简化的假设，往往在复杂现实场景中失效。为解决这些局限性，我们提出CIVQLLIE，一种利用离散表示学习和因果推理力量的新框架。我们通过向量量化实现这一点，它将连续图像特征映射到从大规模高质量图像学习到的离散视觉标记码本。该码本作为可靠先验，编码独立于退化的标准化亮度和颜色模式。然而，由于退化输入与学习码本之间的分布偏移，直接将VQ应用于低光图像会失败。因此，我们提出多级因果干预方法来系统纠正这些偏移。首先，在编码过程中，我们的像素级因果干预模块干预以使低级特征与码本期望的亮度和颜色分布对齐。其次，具有低频选择性注意力门控的特征感知因果干预机制识别和增强受光照退化影响最大的通道，促进准确的码本标记匹配，同时通过灵活的特征级干预增强编码器的泛化性能。最后，在解码过程中，高频细节重建模块利用匹配码本表示中保留的结构信息，使用可变形卷积技术重建精细细节。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Images captured in nighttime scenes suffer from severely reduced visibility,hindering effective content perception. Current low-light image enhancement(LLIE) methods face significant challenges: data-driven end-to-end mappingnetworks lack interpretability or rely on unreliable prior guidance, strugglingunder extremely dark conditions, while physics-based methods depend onsimplified assumptions that often fail in complex real-world scenarios. Toaddress these limitations, we propose CIVQLLIE, a novel framework thatleverages the power of discrete representation learning through causalreasoning. We achieve this through Vector Quantization (VQ), which mapscontinuous image features to a discrete codebook of visual tokens learned fromlarge-scale high-quality images. This codebook serves as a reliable prior,encoding standardized brightness and color patterns that are independent ofdegradation. However, direct application of VQ to low-light images fails due todistribution shifts between degraded inputs and the learned codebook.Therefore, we propose a multi-level causal intervention approach tosystematically correct these shifts. First, during encoding, our Pixel-levelCausal Intervention (PCI) module intervenes to align low-level features withthe brightness and color distributions expected by the codebook. Second, aFeature-aware Causal Intervention (FCI) mechanism with Low-frequency SelectiveAttention Gating (LSAG) identifies and enhances channels most affected byillumination degradation, facilitating accurate codebook token matching whileenhancing the encoder's generalization performance through flexiblefeature-level intervention. Finally, during decoding, the High-frequency DetailReconstruction Module (HDRM) leverages structural information preserved in thematched codebook representations to reconstruct fine details using deformableconvolution techniques.</description>
      <author>example@mail.com (Tongshun Zhang, Pingping Liu, Zhe Zhang, Qiuzhan Zhou)</author>
      <guid isPermaLink="false">2508.03338v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>BaroPoser: Real-time Human Motion Tracking from IMUs and Barometers in Everyday Devices</title>
      <link>http://arxiv.org/abs/2508.03313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了BaroPoser，一种结合智能手机和智能手表记录的IMU和气压数据来实时估计人体姿态和全局位移的方法。通过利用气压数据估计传感器高度变化，该方法提高了人体姿态估计精度并支持非平坦地形上的运动追踪，同时提出的局部大腿坐标系有助于更好地学习姿态表示。实验证明该方法在相同硬件配置下优于仅使用IMU的最先进方法。&lt;h4&gt;背景&lt;/h4&gt;近年来，使用智能手机和智能手表等日常设备的IMU来追踪人体运动变得越来越流行。然而，由于传感器测量的稀疏性以及缺乏在不平地形上捕捉人体运动的数据集，现有方法在姿态估计准确性方面常常遇到困难，并且通常仅限于恢复平坦地形上的运动。&lt;h4&gt;目的&lt;/h4&gt;提出BaroPoser，第一个结合智能手机和智能手表记录的IMU和气压数据来实时估计人体姿态和全局位移的方法。&lt;h4&gt;方法&lt;/h4&gt;利用气压读数估计传感器高度变化，为提高人体姿态估计精度和预测非平坦地形上的全局位移提供线索；提出一个局部大腿坐标系，用于分离局部和全局运动输入，以便更好地进行姿态表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准数据集和真实世界记录上的评估表明，该方法在相同硬件配置下优于仅使用IMU的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;BaroPoser方法通过结合IMU和气压数据，解决了现有方法在不平地形上姿态估计精度不足的问题，扩展了IMU在复杂地形下人体运动追踪的应用能力。&lt;h4&gt;翻译&lt;/h4&gt;近年来，使用智能手机和智能手表等日常设备的IMU来追踪人体运动越来越受欢迎。然而，由于传感器测量的稀疏性以及缺乏在不平地形上捕捉人体运动的数据集，现有方法在姿态估计准确性方面常常遇到困难，并且通常仅限于恢复平坦地形上的运动。为此，我们提出了BaroPoser，这是第一个结合智能手机和智能手表记录的IMU和气压数据来实时估计人体姿态和全局位移的方法。通过利用气压读数，我们估计传感器高度变化，这为提高人体姿态估计精度和预测非平坦地形上的全局位移提供了有价值的线索。此外，我们提出了一个局部大腿坐标系，用于分离局部和全局运动输入，以便更好地进行姿态表示学习。我们在公共基准数据集和真实世界记录上评估了我们的方法。定量和定性结果表明，我们的方法优于使用相同硬件配置的仅使用IMU的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746059.3747731&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, tracking human motion using IMUs from everyday devices suchas smartphones and smartwatches has gained increasing popularity. However, dueto the sparsity of sensor measurements and the lack of datasets capturing humanmotion over uneven terrain, existing methods often struggle with poseestimation accuracy and are typically limited to recovering movements on flatterrain only. To this end, we present BaroPoser, the first method that combinesIMU and barometric data recorded by a smartphone and a smartwatch to estimatehuman pose and global translation in real time. By leveraging barometricreadings, we estimate sensor height changes, which provide valuable cues forboth improving the accuracy of human pose estimation and predicting globaltranslation on non-flat terrain. Furthermore, we propose a local thighcoordinate frame to disentangle local and global motion input for better poserepresentation learning. We evaluate our method on both public benchmarkdatasets and real-world recordings. Quantitative and qualitative resultsdemonstrate that our approach outperforms the state-of-the-art (SOTA) methodsthat use IMUs only with the same hardware configuration.</description>
      <author>example@mail.com (Libo Zhang, Xinyu Yi, Feng Xu)</author>
      <guid isPermaLink="false">2508.03313v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Dual-disentangle Framework for Diversified Sequential Recommendation</title>
      <link>http://arxiv.org/abs/2508.03172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种模型无关的双重解耦框架(DDSRec)用于多样化顺序推荐，通过解耦用户兴趣和意图建模，平衡准确性和多样性。&lt;h4&gt;背景&lt;/h4&gt;顺序推荐预测用户随时间的偏好并取得了显著成功，但随着用户交互序列长度增加以及用户兴趣和意图的复杂交织，多样性面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种模型无关的双重解耦框架用于多样化顺序推荐(DDSRec)，以解决顺序推荐中的多样性挑战。&lt;h4&gt;方法&lt;/h4&gt;该框架通过在交互建模和表示学习中采用解耦视角来改进用户兴趣和意图建模，从而平衡顺序推荐中的准确性和多样性。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共数据集上的大量实验证明了DDSRec在顺序推荐的准确性和多样性方面的有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;DDSRec框架能够有效解决顺序推荐中面临的多样性挑战，提高推荐的准确性和多样性。&lt;h4&gt;翻译&lt;/h4&gt;顺序推荐预测用户随时间的偏好并取得了显著成功。然而，用户交互序列的不断增长以及不断演变的用户兴趣和意图的复杂交织给多样性带来了重大挑战。为解决这些问题，我们提出了一种模型无关的双重解耦框架用于多样化顺序推荐(DDSRec)。该框架通过在交互建模和表示学习中采用解耦视角来完善用户兴趣和意图建模，从而平衡顺序推荐中的准确性和多样性。在多个公共数据集上的大量实验证明了DDSRec在顺序推荐的准确性和多样性方面的有效性和优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential recommendation predicts user preferences over time and hasachieved remarkable success. However, the growing length of user interactionsequences and the complex entanglement of evolving user interests andintentions introduce significant challenges to diversity. To address these, wepropose a model-agnostic Dual-disentangle framework for Diversified SequentialRecommendation (DDSRec). The framework refines user interest and intentionmodeling by adopting disentangling perspectives in interaction modeling andrepresentation learning, thereby balancing accuracy and diversity in sequentialrecommendations. Extensive experiments on multiple public datasets demonstratethe effectiveness and superiority of DDSRec in terms of accuracy and diversityfor sequential recommendations.</description>
      <author>example@mail.com (Haoran Zhang, Jingtong Liu, Jiangzhou Deng, Junpeng Guo)</author>
      <guid isPermaLink="false">2508.03172v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Pseudo-label Induced Subspace Representation Learning for Robust Out-of-Distribution Detection</title>
      <link>http://arxiv.org/abs/2508.03108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于伪标签诱导子空间表示的OOD检测框架，通过结合交叉熵分类损失和子空间距离正则化损失来增强ID-OOD可分离性，实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;OOD检测是稳健人工智能的核心，旨在识别训练集之外的新分布样本。&lt;h4&gt;目的&lt;/h4&gt;开发一种在更宽松和自然假设下工作的OOD检测方法，解决现有方法对特征空间限制性假设的依赖问题。&lt;h4&gt;方法&lt;/h4&gt;提出基于伪标签诱导的子空间表示的OOD检测框架，并引入结合交叉熵ID分类损失和子空间距离正则化损失的学习准则。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架在更宽松和自然的假设下工作，能够有效增强ID和OOD样本之间的可分离性。&lt;h4&gt;结论&lt;/h4&gt;实验结果验证了所提出的OOD检测框架的有效性，表明该方法在识别训练集外新分布样本方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;分布外(OOD)检测是稳健人工智能(AI)的核心，旨在识别来自训练集之外的新分布的样本。最近的方法利用特征表示作为OOD检测的区分性特征。然而，大多数现有方法依赖于特征空间的限制性假设，这限制了训练集内(ID)和分布外(OOD)样本之间的可分离性。在这项工作中，我们提出了一种基于伪标签诱导的子空间表示的新型OOD检测框架，与现有的基于特征的技术相比，它在更宽松和自然的假设下工作。此外，我们引入了一个简单而有效的学习准则，将基于交叉熵的ID分类损失与基于子空间距离的正则化损失相结合，以增强ID-OOD的可分离性。大量的实验验证了我们框架的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Out-of-distribution (OOD) detection lies at the heart of robust artificialintelligence (AI), aiming to identify samples from novel distributions beyondthe training set. Recent approaches have exploited feature representations asdistinguishing signatures for OOD detection. However, most existing methodsrely on restrictive assumptions on the feature space that limit theseparability between in-distribution (ID) and OOD samples. In this work, wepropose a novel OOD detection framework based on a pseudo-label-inducedsubspace representation, that works under more relaxed and natural assumptionscompared to existing feature-based techniques. In addition, we introduce asimple yet effective learning criterion that integrates a cross-entropy-basedID classification loss with a subspace distance-based regularization loss toenhance ID-OOD separability. Extensive experiments validate the effectivenessof our framework.</description>
      <author>example@mail.com (Tarhib Al Azad, Faizul Rakib Sayem, Shahana Ibrahim)</author>
      <guid isPermaLink="false">2508.03108v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Elucidating the Role of Feature Normalization in IJEPA</title>
      <link>http://arxiv.org/abs/2508.02829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了在图像联合嵌入预测架构(IJEPA)中，层归一化(LN)对特征处理的影响，并提出用DynTanh激活替代LN，以保留视觉标记的自然能量层次结构，从而提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;在标准的IJEPA架构中，教师编码器输出的特征在作为学生编码器和预测器的蒸馏目标之前会进行层归一化处理。这种归一化被认为可能破坏视觉标记的自然能量层次结构。&lt;h4&gt;目的&lt;/h4&gt;研究的目的是探索特征归一化对IJEPA模型性能的影响，并提出一种改进方法，以更好地保留视觉标记的自然能量层次结构，提高模型的自监督视觉表征学习能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出用DynTanh激活替换特征层归一化(LN)，以更好地保留标记能量并允许高能量标记对预测损失做出更大贡献。他们比较了使用LN和DynTanh激活的IJEPA模型在ImageNet线性探针准确率和NYU Depth V2单目深度估计任务上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;特征LN破坏了视觉标记的自然能量层次结构，阻止模型优先考虑语义丰富的区域；使用特征LN训练的IJEPA模型在损失图中表现出显著的棋盘状伪影；使用DynTanh激活替代LN可以修复损失图中的棋盘状伪影并使IJEPA表现出更长尾的损失分布；使用DynTanh激活将ViT-Small在ImageNet线性探针准确率从38%提高到42.7%；在NYU Depth V2单目深度估计任务中，RMSE降低了0.08。&lt;h4&gt;结论&lt;/h4&gt;保留自然标记能量对于有效的自监督视觉表征学习至关重要。用DynTanh激活替代特征层归一化是一种简单而有效的改进方法，可以显著提高IJEPA模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;在标准的图像联合嵌入预测架构(IJEPA)中，教师编码器输出的特征在作为学生编码器和预测器的蒸馏目标之前会被层归一化(LN)处理。我们提出这种特征归一化会破坏视觉标记的自然能量层次结构，其中高能量标记(具有较大L2范数的标记)编码了语义重要的图像区域。LN强制所有特征具有相同的L2范数，有效地平衡了它们的能量，并阻止了模型优先考虑语义丰富的区域。我们发现，使用特征LN训练的IJEPA模型在损失图中表现出显著的棋盘状伪影。我们提出用DynTanh激活替换特征LN，因为后者更好地保留了标记能量，并允许高能量标记对预测损失做出更大贡献。研究表明，使用特征DynTanh训练的IJEPA表现出更长尾的损失分布，并修复了损失图中的棋盘状伪影。我们的实证结果显示，这种简单的修改将ViT-Small在ImageNet线性探针准确率从38%提高到42.7%，并在NYU Depth V2单目深度估计中将RMSE降低了0.08。这些结果表明，保留自然标记能量对于有效的自监督视觉表征学习至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the standard image joint embedding predictive architecture (IJEPA),features at the output of the teacher encoder are layer normalized (LN) beforeserving as a distillation target for the student encoder and predictor. Wepropose that this feature normalization disrupts the natural energy hierarchyof visual tokens, where high-energy tokens (those with larger L2 norms) encodesemantically important image regions. LN forces all features to have identicalL2 norms, effectively equalizing their energies and preventing the model fromprioritizing semantically rich regions. We find that IJEPA models trained withfeature LN exhibit loss maps with significant checkerboard-like artifacts. Wepropose that feature LN be replaced with a DynTanh activation as the latterbetter preserves token energies and allows high-energy tokens to greatercontribute to the prediction loss. We show that IJEPA trained with featureDynTanh exhibits a longer-tailed loss distribution and fixes the checkerboardartifacts in the loss map. Our empirical results show that our simplemodification improves ImageNet linear probe accuracy from 38% to 42.7% forViT-Small and reduces RMSE by 0.08 on NYU Depth V2 monocular depth estimation.These results suggest that preserving natural token energies is crucial foreffective self-supervised visual representation learning.</description>
      <author>example@mail.com (Adam Colton)</author>
      <guid isPermaLink="false">2508.02829v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy</title>
      <link>http://arxiv.org/abs/2508.03596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 (Highlight); Project Page:  https://cuhk-aim-group.github.io/MetaScope/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出MetaScope，一种专为金属透镜内窥镜设计的光学驱动神经网络，通过光学信息强度调整和色差校正技术，解决了金属透镜内窥镜领域的挑战，在分割、恢复和泛化能力上均表现出色。&lt;h4&gt;背景&lt;/h4&gt;微型内窥镜技术已发展，但传统相机使用的凸透镜受限于毫米级厚度，阻碍了微型临床应用。基于金属透镜（微米级）的超微型成像成为有前景的解决方案，但由于金属透镜的物理特性差异，数据获取和算法研究存在巨大差距。&lt;h4&gt;目的&lt;/h4&gt;弥合金属透镜内窥镜领域未被探索的差距，推动新型金属透镜内窥镜的发展。&lt;h4&gt;方法&lt;/h4&gt;建立金属透镜内窥镜数据集并进行初步光学模拟；提出MetaScope网络，包含光学信息强度调整(OIA)和光学信息色差校正(OCC)两个创新设计；采用梯度引导的蒸馏方法增强联合学习，自适应转移知识。&lt;h4&gt;主要发现&lt;/h4&gt;MetaScope在金属透镜分割和恢复方面优于最先进的方法，在真实生物医学场景中实现了令人印象深刻的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MetaScope代表了金属透镜内窥镜领域的重要进展，解决了该领域的关键挑战，展示了在临床应用中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;微型内窥镜的进步已在人体内实现了准确的视觉感知。现有研究仍局限于使用凸透镜的传统相机，其毫米级厚度的物理限制对微型临床应用构成严重阻碍。最近，随着超表面光学技术的出现，基于金属透镜（微米级）的超微型成像受到广泛关注，成为有前景的解决方案。然而，由于金属透镜的物理差异，在数据获取和算法研究方面存在巨大差距。鉴于此，我们旨在弥合这一未被探索的差距，推动新型金属透镜内窥镜的发展。首先，我们建立金属透镜内窥镜数据集并进行初步光学模拟，确定了两个符合强光学先验的衍生光学问题。其次，我们提出MetaScope，一种专为金属透镜内窥镜设计的新型光学驱动神经网络。MetaScope包含两个新颖设计：光学信息强度调整(OIA)，通过学习光学嵌入来校正强度衰减；光学信息色差校正(OCC)，通过学习由学习到的点扩散函数(PSF)分布提供信息的空间变形来减轻色差。为了增强联合学习，我们进一步部署了梯度引导的蒸馏方法，自适应地将知识从基础模型转移过来。大量实验表明，MetaScope不仅在金属透镜分割和恢复方面优于最先进的方法，而且在真实的生物医学场景中实现了令人印象深刻的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Miniaturized endoscopy has advanced accurate visual perception within thehuman body. Prevailing research remains limited to conventional camerasemploying convex lenses, where the physical constraints with millimetre-scalethickness impose serious impediments on the micro-level clinical. Recently,with the emergence of meta-optics, ultra-micro imaging based on metalenses(micron-scale) has garnered great attention, serving as a promising solution.However, due to the physical difference of metalens, there is a large gap indata acquisition and algorithm research. In light of this, we aim to bridgethis unexplored gap, advancing the novel metalens endoscopy. First, weestablish datasets for metalens endoscopy and conduct preliminary opticalsimulation, identifying two derived optical issues that physically adhere tostrong optical priors. Second, we propose MetaScope, a novel optics-drivenneural network tailored for metalens endoscopy driven by physical optics.MetaScope comprises two novel designs: Optics-informed Intensity Adjustment(OIA), rectifying intensity decay by learning optical embeddings, andOptics-informed Chromatic Correction (OCC), mitigating chromatic aberration bylearning spatial deformations informed by learned Point Spread Function (PSF)distributions. To enhance joint learning, we further deploy a gradient-guideddistillation to transfer knowledge from the foundational model adaptively.Extensive experiments demonstrate that MetaScope not only outperformsstate-of-the-art methods in both metalens segmentation and restoration but alsoachieves impressive generalized ability in real biomedical scenes.</description>
      <author>example@mail.com (Wuyang Li, Wentao Pan, Xiaoyuan Liu, Zhendong Luo, Chenxin Li, Hengyu Liu, Din Ping Tsai, Mu Ku Chen, Yixuan Yuan)</author>
      <guid isPermaLink="false">2508.03596v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks</title>
      <link>http://arxiv.org/abs/2508.03566v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAM2-UNeXT的先进框架，通过整合辅助DINOv2编码器扩展SAM2的表示能力，采用双分辨率策略和密集粘合层实现更准确的分割，同时简化架构设计，无需复杂解码器。在四个基准测试中展示了优越性能。&lt;h4&gt;背景&lt;/h4&gt;最近的研究已强调将SegmentAnything Model (SAM) 适应各种下游任务的潜力，但构建更强大和可推广的编码器以进一步增强性能仍是一个开放挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个先进框架SAM2-UNeXT，基于SAM2-UNet核心原则，通过整合辅助DINOv2编码器扩展SAM2的表示能力，实现更准确的分割，同时简化架构设计。&lt;h4&gt;方法&lt;/h4&gt;提出SAM2-UNeXT框架，整合辅助DINOv2编码器到SAM2中，采用双分辨率策略和密集粘合层，实现更准确的分割，简化架构设计，无需复杂解码器。&lt;h4&gt;主要发现&lt;/h4&gt;在二值图像分割、伪装物体检测、海洋动物分割和遥感显著性检测四个基准测试上进行的广泛实验，证明了所提出方法的优越性能。&lt;h4&gt;结论&lt;/h4&gt;SAM2-UNeXT框架通过整合辅助DINOv2编码器及采用双分辨率策略和密集粘合层，实现了更准确的分割性能，同时简化了架构设计。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究已经强调了将SegmentAnything Model (SAM) 适应各种下游任务的潜力。然而，构建一个更强大和可推广的编码器以进一步增强性能仍然是一个开放的挑战。在这项工作中，我们提出了SAM2-UNeXT，一个基于SAM2-UNet核心原则构建的先进框架，通过整合辅助DINOv2编码器扩展了SAM2的表示能力。通过采用双分辨率策略和密集粘合层，我们的方法实现了更准确的分割，同时采用简单架构，减少了对复杂解码器设计的需求。在二值图像分割、伪装物体检测、海洋动物分割和遥感显著性检测四个基准测试上进行的广泛实验，证明了我们提出方法的优越性能。代码可在https://github.com/WZH0120/SAM2-UNeXT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have highlighted the potential of adapting the SegmentAnything Model (SAM) for various downstream tasks. However, constructing a morepowerful and generalizable encoder to further enhance performance remains anopen challenge. In this work, we propose SAM2-UNeXT, an advanced framework thatbuilds upon the core principles of SAM2-UNet while extending therepresentational capacity of SAM2 through the integration of an auxiliaryDINOv2 encoder. By incorporating a dual-resolution strategy and a dense gluelayer, our approach enables more accurate segmentation with a simplearchitecture, relaxing the need for complex decoder designs. Extensiveexperiments conducted on four benchmarks, including dichotomous imagesegmentation, camouflaged object detection, marine animal segmentation, andremote sensing saliency detection, demonstrate the superior performance of ourproposed method. The code is available athttps://github.com/WZH0120/SAM2-UNeXT.</description>
      <author>example@mail.com (Xinyu Xiong, Zihuang Wu, Lei Zhang, Lei Lu, Ming Li, Guanbin Li)</author>
      <guid isPermaLink="false">2508.03566v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models</title>
      <link>http://arxiv.org/abs/2508.03533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了EmbedGrad框架，通过基于梯度的微调优化文本提示嵌入，解决了现有提示工程和参数适应方法的局限性，实现了任务适应的新范式。&lt;h4&gt;背景&lt;/h4&gt;有效地将强大的预训练基础模型适应到各种任务中仍然是AI部署中的一个关键挑战。当前方法主要有两种：离散优化文本提示或通过额外参数连续适应，但前者缺乏精确性，后者增加复杂性和降低可解释性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来解决现有提示工程和参数适应方法的局限性，实现更精确、高效且可解释的任务适应。&lt;h4&gt;方法&lt;/h4&gt;EmbedGrad是一种通过基于梯度的微调来优化文本提示嵌入的新框架。该方法将训练与部署解耦：优化过程中，标记示例引导精确的嵌入调整；推理过程中，只有优化后的嵌入与用户查询集成，实现了文本空间中不可能实现的精细校准。&lt;h4&gt;主要发现&lt;/h4&gt;在数学推理、情感分析和因果判断任务上的全面评估显示了EmbedGrad的有效性。优化推理提示使Qwen2.5-Math-1.5B在数学问题上的准确率从14.74%提高到58.96%。在模型规模(0.5B-14B)和所有任务上都观察到一致的改进，小型模型在复杂问题上获得了特别显著的提升。&lt;h4&gt;结论&lt;/h4&gt;通过桥接提示工程和参数效率，无需架构更改，EmbedGrad建立了嵌入微调作为任务适应的一个强大新范式。&lt;h4&gt;翻译&lt;/h4&gt;有效地将强大的预训练基础模型适应到各种任务仍然是AI部署中的一个关键挑战。当前方法主要遵循两种范式：通过提示工程进行文本提示的离散优化，或通过额外的可训练参数进行连续适应。两种方法都存在局限性——离散方法缺乏精细的精确度，而基于参数的技术增加了复杂性和降低了可解释性。为了解决这些限制，我们提出了EmbedGrad，一种通过基于梯度的微调来优化文本提示嵌入的新框架。我们的方法独特地将训练与部署解耦：在优化过程中，标记示例引导精确的嵌入调整同时保持语义意义；在推理过程中，只有优化后的嵌入与用户查询集成。这实现了在文本空间中不可能实现的精细校准，例如增强"请逐步推理"等提示的推理能力。在数学推理、情感分析和因果判断任务上的全面评估证明了EmbedGrad的有效性：为Qwen2.5-Math-1.5B优化这个推理提示使数学问题的准确率从14.74%提高到58.96%。在模型规模(0.5B-14B)和所有任务上都观察到一致的改进，小型模型在因果判断等复杂问题上获得了特别显著的提升。通过桥接提示工程和参数效率而无需架构更改，我们的工作建立了嵌入微调作为任务适应的一个强大新范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effectively adapting powerful pretrained foundation models to diverse tasksremains a key challenge in AI deployment. Current approaches primarily followtwo paradigms:discrete optimization of text prompts through prompt engineering,or continuous adaptation via additional trainable parameters. Both exhibitlimitations-discrete methods lack refinement precision while parameter-basedtechniques increase complexity and reduce interpretability. To address theseconstraints, we propose EmbedGrad, a novel framework that optimizes text promptembeddings through gradient-based refinement. Our approach uniquely decouplestraining from deployment:during optimization,labeled examples guide preciseembedding adjustments while preserving semantic meaning; during inference, onlyoptimized embeddings integrate with user queries. This enables fine-grainedcalibration impossible in text space, such as enhancing the reasoningcapability of prompts like please reason step by step. Comprehensiveevaluations across mathematical reasoning, sentiment analysis, and causaljudgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoningprompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\% to 58.96\% onmathematical problems. Consistent improvements were observed across modelscales (0.5B-14B) and all tasks, with particularly significant gains forsmaller models on complex problems like causal judgment. By bridging promptengineering and parameter efficiency without architectural changes, our workestablishes embedding refinement as a powerful new paradigm for taskadaptation.</description>
      <author>example@mail.com (Xiaoming Hou, Jiquan Zhang, Zibin Lin, DaCheng Tao, Shengli Zhang)</author>
      <guid isPermaLink="false">2508.03533v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Mosaicing of Histo-Pathology Image Fragments using Visual Foundation Models</title>
      <link>http://arxiv.org/abs/2508.03524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为SemanticStitcher的新型组织病理学图像拼接方法，使用视觉病理学基础模型的潜在特征表示来识别不同组织片段中的相邻区域，并通过鲁棒姿态估计将多个片段拼接成全载玻片图像(WMS)。&lt;h4&gt;背景&lt;/h4&gt;在组织病理学中，组织样本通常大于标准显微镜载玻片，需要拼接多个片段来处理整个结构如肿瘤。自动化拼接是扩展分析的前提，但面临多种挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服传统方法限制的新型组织病理学图像拼接技术，用于重建人工全载玻片图像(WMS)。&lt;h4&gt;方法&lt;/h4&gt;SemanticStitcher利用视觉病理学基础模型推导的潜在特征表示来识别不同组织片段中的相邻区域，基于大量语义匹配候选者进行鲁棒姿态估计，将多个片段拼接成马赛克形成完整的WMS。&lt;h4&gt;主要发现&lt;/h4&gt;在三个不同的组织病理学数据集上的实验表明，SemanticStitcher能够产生鲁棒的WMS拼接，并在正确的边界匹配方面始终优于现有最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;SemanticStitcher有效解决了传统组织病理学图像拼接中的挑战，能够克服组织丢失、形态畸变、染色不一致等问题，为组织病理学分析提供了可靠的图像拼接解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在组织病理学中，组织样本通常比标准显微镜载玻片大，因此需要拼接多个片段来处理整个结构如肿瘤。自动化拼接是扩展分析的前提，但由于制备过程中可能发生组织丢失、形态学畸变不均匀、染色不一致、载玻片上错位导致的缺失区域或组织边缘破碎等问题，这一过程具有挑战性。这限制了使用边界形状匹配算法的最先进拼接方法重建人工全载玻片图像(WMS)的能力。在此，我们引入了SemanticStitcher，它使用视觉病理学基础模型推导的潜在特征表示来识别不同片段中的相邻区域。基于大量语义匹配候选者的鲁棒姿态估计，将多个片段拼接成马赛克形成WMS。在三个不同组织病理学数据集上的实验表明，SemanticStitcher能够产生鲁棒的WMS拼接，并在正确的边界匹配方面始终优于现有最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In histopathology, tissue samples are often larger than a standard microscopeslide, making stitching of multiple fragments necessary to process entirestructures such as tumors. Automated stitching is a prerequisite for scalinganalysis, but is challenging due to possible tissue loss during preparation,inhomogeneous morphological distortion, staining inconsistencies, missingregions due to misalignment on the slide, or frayed tissue edges. This limitsstate-of-the-art stitching methods using boundary shape matching algorithms toreconstruct artificial whole mount slides (WMS). Here, we introduceSemanticStitcher using latent feature representations derived from a visualhistopathology foundation model to identify neighboring areas in differentfragments. Robust pose estimation based on a large number of semantic matchingcandidates derives a mosaic of multiple fragments to form the WMS. Experimentson three different histopathology datasets demonstrate that SemanticStitcheryields robust WMS mosaicing and consistently outperforms the state of the artin correct boundary matches.</description>
      <author>example@mail.com (Stefan Brandstätter, Maximilian Köller, Philipp Seeböck, Alissa Blessing, Felicitas Oberndorfer, Svitlana Pochepnia, Helmut Prosch, Georg Langs)</author>
      <guid isPermaLink="false">2508.03524v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting for Cross-domain Few-shot Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.03511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MAUP（多中心自适应不确定性感知提示）的无训练跨领域少样本医学图像分割方法，通过将自然图像预训练的Segment Anything Model适应到医学图像分割任务中，实现了在三个医学数据集上的精确分割结果，无需额外训练。&lt;h4&gt;背景&lt;/h4&gt;当前跨领域少样本医学图像分割模型在其他医学源域上的重度训练依赖限制了模型的通用性和部署便利性。随着大型视觉模型的发展，特别是自然图像领域的Segment Anything Model的出现，为解决这个问题提供了新思路。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需训练的CD-FSMIS模型，利用MAUP策略将自然图像预训练的SAM模型适应到跨领域少样本医学图像分割任务中，避免对其他源医学域的重度训练，提高模型的通用性和部署便利性。&lt;h4&gt;方法&lt;/h4&gt;MAUP策略包含三个关键创新：(1)基于K-means聚类的多中心提示生成，实现全面的空间覆盖；(2)不确定性感知的提示选择，专注于困难区域；(3)自适应提示优化，能根据目标区域复杂度动态调整。&lt;h4&gt;主要发现&lt;/h4&gt;使用预训练的DINOv2特征编码器，MAUP在三个医学数据集上实现了精确的分割结果，与传统的CD-FSMIS模型和无训练的FSMIS模型相比，无需任何额外训练就取得了良好效果。&lt;h4&gt;结论&lt;/h4&gt;MAUP策略成功解决了传统CD-FSMIS模型需要重度训练其他源医学域的问题，通过将自然图像预训练的SAM模型适应到医学图像分割任务中，实现了高效准确的跨领域少样本医学图像分割，提高了模型的通用性和部署便利性。&lt;h4&gt;翻译&lt;/h4&gt;跨领域少样本医学图像分割(CD-FSMIS)是利用其他领域的知识来分割标注有限的医学图像的一种潜在解决方案。当前CD-FSMIS模型的显著性能依赖于在其他医学源域上的重度训练过程，这降低了模型的通用性和部署便利性。随着大型自然图像视觉模型的发展，我们提出了一种无需训练的CD-FSMIS模型，引入了多中心自适应不确定性感知提示(MAUP)策略，将自然图像训练的基础模型Segment Anything Model适应到CD-FSMIS任务中。具体而言，MAUP包含三个关键创新：(1)基于K-means聚类的多中心提示生成，实现全面的空间覆盖；(2)不确定性感知的提示选择，专注于困难区域；(3)自适应提示优化，能根据目标区域复杂度动态调整。使用预训练的DINOv2特征编码器，MAUP在三个医学数据集上实现了精确的分割结果，与几个传统的CD-FSMIS模型和无训练的FSMIS模型相比，无需任何额外训练。源代码可在以下网址获取：https://github.com/YazhouZhu19/MAUP。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-domain Few-shot Medical Image Segmentation (CD-FSMIS) is a potentialsolution for segmenting medical images with limited annotation using knowledgefrom other domains. The significant performance of current CD-FSMIS modelsrelies on the heavily training procedure over other source medical domains,which degrades the universality and ease of model deployment. With thedevelopment of large visual models of natural images, we propose atraining-free CD-FSMIS model that introduces the Multi-center AdaptiveUncertainty-aware Prompting (MAUP) strategy for adapting the foundation modelSegment Anything Model (SAM), which is trained with natural images, into theCD-FSMIS task. To be specific, MAUP consists of three key innovations: (1)K-means clustering based multi-center prompts generation for comprehensivespatial coverage, (2) uncertainty-aware prompts selection that focuses on thechallenging regions, and (3) adaptive prompt optimization that can dynamicallyadjust according to the target region complexity. With the pre-trained DINOv2feature encoder, MAUP achieves precise segmentation results across threemedical datasets without any additional training compared with severalconventional CD-FSMIS models and training-free FSMIS model. The source code isavailable at: https://github.com/YazhouZhu19/MAUP.</description>
      <author>example@mail.com (Yazhou Zhu, Haofeng Zhang)</author>
      <guid isPermaLink="false">2508.03511v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>ParticleSAM: Small Particle Segmentation for Material Quality Monitoring in Recycling Processes</title>
      <link>http://arxiv.org/abs/2508.03490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures. Accepted for presentation at EUSIPCO 2025,  September 8-12, 2025. List of accepted papers available at  http://cmsworkshops.com/EUSIPCO2025/papers/accepted_papers.php&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对建筑行业资源消耗问题，提出了一种名为ParticleSAM的视觉分割方法，用于建筑材料颗粒的质量监测，并创建了相应的数据集作为基准。&lt;h4&gt;背景&lt;/h4&gt;建筑行业是资源消耗的主要部门，回收建筑材料具有高再利用潜力，但骨料质量监测通常仍采用人工方法进行。基于视觉的机器学习方法可能提供更高效的解决方案，但现有分割方法不适用于包含数百个小颗粒的图像。&lt;h4&gt;目的&lt;/h4&gt;开发一种适用于小而密集物体图像的分割方法，特别是建筑材料颗粒图像，以实现自动化的材料质量控制。&lt;h4&gt;方法&lt;/h4&gt;提出了ParticleSAM，一种针对小而密集物体的分割基础模型适配；创建了新的密集多颗粒数据集，通过自动数据生成和标记流程从孤立的颗粒图像模拟而来；进行了定量和定性实验验证方法优势。&lt;h4&gt;主要发现&lt;/h4&gt;ParticleSAM在处理小颗粒分割任务上优于原始SAM方法；创建的数据集可作为视觉材料质量控制自动化的基准；该方法在建筑以外的领域也具有应用潜力。&lt;h4&gt;结论&lt;/h4&gt;ParticleSAM为建筑材料质量监测提供了一种更快、更高效的解决方案，通过自动化数据生成和标记流程创建的数据集为该领域提供了基准，方法具有广泛的潜在应用价值。&lt;h4&gt;翻译&lt;/h4&gt;建筑行业在资源消耗方面代表了一个主要部门。回收建筑材料具有高再利用潜力，但骨料的质量监测通常仍采用人工方法进行。基于视觉的机器学习方法可以为这一问题提供更快、更高效的解决方案，但现有的分割方法在设计上不适用于包含数百个小颗粒的图像。在本文中，我们提出了ParticleSAM，这是一种分割基础模型针对建筑材料颗粒等常见的小而密集物体图像的适配。此外，我们借助自动数据生成和标记流程，从孤立的颗粒图像创建了一个新的密集多颗粒数据集。该数据集作为视觉材料质量控制自动化的基准，而我们的分割方法在需要小颗粒分割的建筑以外的应用领域也具有潜在价值。我们的实验结果通过定量和定性实验与原始SAM方法进行比较，验证了我们方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The construction industry represents a major sector in terms of resourceconsumption. Recycled construction material has high reuse potential, butquality monitoring of the aggregates is typically still performed with manualmethods. Vision-based machine learning methods could offer a faster and moreefficient solution to this problem, but existing segmentation methods are bydesign not directly applicable to images with hundreds of small particles. Inthis paper, we propose ParticleSAM, an adaptation of the segmentationfoundation model to images with small and dense objects such as the ones oftenencountered in construction material particles. Moreover, we create a new densemulti-particle dataset simulated from isolated particle images with theassistance of an automated data generation and labeling pipeline. This datasetserves as a benchmark for visual material quality control automation while oursegmentation approach has the potential to be valuable in application areasbeyond construction where small-particle segmentation is needed. Ourexperimental results validate the advantages of our method by comparing to theoriginal SAM method both in quantitative and qualitative experiments.</description>
      <author>example@mail.com (Yu Zhou, Pelle Thielmann, Ayush Chamoli, Bruno Mirbach, Didier Stricker, Jason Rambach)</author>
      <guid isPermaLink="false">2508.03490v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning with Foundation Models for Medical Image Analysis</title>
      <link>http://arxiv.org/abs/2508.03441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 6 figures, 10 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了MedCAL-Bench，首个基于基础模型的医学图像分析冷启动主动学习(CSAL)基准，评估了14个基础模型和7种CSAL策略在7个数据集上的表现，覆盖分类和分割任务。&lt;h4&gt;背景&lt;/h4&gt;在医学图像分析中，冷启动主动学习(CSAL)旨在没有先验知识的情况下选择有信息量的样本进行标注，以提高标注效率和模型性能。现有方法主要依赖自监督学习进行特征提取，但这种方法效率低下且特征表示不足。&lt;h4&gt;目的&lt;/h4&gt;填补预训练基础模型在冷启动主动学习任务中的研究空白，并建立一个全面的基准来比较不同基础模型和CSAL策略在医学图像分析中的性能。&lt;h4&gt;方法&lt;/h4&gt;构建MedCAL-Bench基准，评估14个基础模型和7种CSAL策略在7个数据集上的表现，覆盖不同标注预算，包括分类和分割任务，来自多种医学模态。该基准首次同时评估特征提取和样本选择两个阶段。&lt;h4&gt;主要发现&lt;/h4&gt;1) 大多数基础模型是CSAL的有效特征提取器，DINO系列在分割任务中表现最佳；2) 这些基础模型在分割任务中的性能差异较大，而在分类任务中差异较小；3) 在不同数据集上应考虑不同的样本选择策略，ALPS在分割任务中表现最佳，而RepDiv在分类任务中领先。&lt;h4&gt;结论&lt;/h4&gt;基础模型在冷启动主动学习中具有巨大潜力，但不同任务和数据集需要选择合适的基础模型和样本选择策略。MedCAL-Bench为这一领域提供了重要的基准和参考。&lt;h4&gt;翻译&lt;/h4&gt;Cold-Start Active Learning (CSAL)旨在在没有先验知识的情况下选择有信息量的样本进行标注，这对于在有限的标注预算下提高医学图像分析中的标注效率和模型性能很重要。大多数现有的CSAL方法依赖在目标数据集上进行自监督学习(SSL)进行特征提取，这种方法效率低下且受限于特征表示不足。最近，预训练基础模型(FMs)已显示出强大的特征提取能力，并有潜力用于更好的CSAL。然而，这种范式很少被研究，缺乏在CSAL任务中比较FMs的基准。为此，我们提出了MedCAL-Bench，这是第一个基于FM的医学图像分析CSAL基准。我们在7个数据集上评估了14个FMs和7种CSAL策略，涵盖不同标注预算，包括来自多种医学模态的分类和分割任务。它也是首个同时评估特征提取和样本选择两个阶段的CSAL基准。我们的实验结果表明：1) 大多数FMs是CSAL的有效特征提取器，DINO系列在分割任务中表现最佳；2) 这些FMs在分割任务中的性能差异较大，而在分类任务中差异较小；3) 在不同数据集上的CSAL应考虑不同的样本选择策略，ALPS在分割任务中表现最佳，而RepDiv在分类任务中领先。代码可在https://github.com/HiLab-git/MedCAL-Bench获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cold-Start Active Learning (CSAL) aims to select informative samples forannotation without prior knowledge, which is important for improving annotationefficiency and model performance under a limited annotation budget in medicalimage analysis. Most existing CSAL methods rely on Self-Supervised Learning(SSL) on the target dataset for feature extraction, which is inefficient andlimited by insufficient feature representation. Recently, pre-trainedFoundation Models (FMs) have shown powerful feature extraction ability with apotential for better CSAL. However, this paradigm has been rarely investigated,with a lack of benchmarks for comparison of FMs in CSAL tasks. To this end, wepropose MedCAL-Bench, the first systematic FM-based CSAL benchmark for medicalimage analysis. We evaluate 14 FMs and 7 CSAL strategies across 7 datasetsunder different annotation budgets, covering classification and segmentationtasks from diverse medical modalities. It is also the first CSAL benchmark thatevaluates both the feature extraction and sample selection stages. Ourexperimental results reveal that: 1) Most FMs are effective feature extractorsfor CSAL, with DINO family performing the best in segmentation; 2) Theperformance differences of these FMs are large in segmentation tasks, whilesmall for classification; 3) Different sample selection strategies should beconsidered in CSAL on different datasets, with Active Learning by ProcessingSurprisal (ALPS) performing the best in segmentation while RepDiv leading forclassification. The code is available athttps://github.com/HiLab-git/MedCAL-Bench.</description>
      <author>example@mail.com (Ning Zhu, Xiaochuan Ma, Shaoting Zhang, Guotai Wang)</author>
      <guid isPermaLink="false">2508.03441v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology Report Generation</title>
      <link>http://arxiv.org/abs/2508.03426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于大规模多模态医学知识图谱的X光医学报告生成方法，解决了现有方法中存在的幻觉和疾病诊断能力弱等问题。&lt;h4&gt;背景&lt;/h4&gt;X光医学报告生成是人工智能在医疗领域的重要应用，基于大型基础模型的支持，医学报告生成的质量已显著提高，但仍存在幻觉和疾病诊断能力弱等挑战。&lt;h4&gt;目的&lt;/h4&gt;构建一个大规模多模态医学知识图谱，并设计一个有效的X光报告生成框架，提高生成质量和疾病诊断能力。&lt;h4&gt;方法&lt;/h4&gt;1) 构建基于真实医学报告的大规模多模态医学知识图谱(M3KG)，包含2477个实体、3种关系、37424个三元组和6943个疾病感知视觉标记；2) 对知识图谱进行采样获得多粒度语义图，使用R-GCN编码器进行特征提取；3) 采用Swin-Transformer提取X光图像视觉特征，通过交叉注意力与知识交互；4) 将视觉标记输入Q-former，使用交叉注意力检索疾病感知视觉标记；5) 采用大型语言模型将语义知识图谱、输入X光图像和疾病感知视觉标记映射为语言描述。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的大量实验充分验证了所提出的知识图谱和X光报告生成框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法能够有效解决X光医学报告生成中的问题，提高生成质量和诊断能力，源代码将在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;X光医学报告生成是人工智能在医疗领域的重要应用之一。在大型基础模型的支持下，医学报告生成的质量已显著提高。然而，诸如幻觉和疾病诊断能力弱等挑战仍然存在。在本文中，我们首先基于真实医学报告使用GPT-4o构建了一个大规模多模态医学知识图谱（称为M3KG）。它包含2477个实体、3种关系、37424个三元组和6943个针对CheXpert Plus数据集的疾病感知视觉标记。然后，我们对知识图谱进行采样以获得多粒度语义图，并使用R-GCN编码器进行特征提取。对于输入的X光图像，我们采用Swin-Transformer提取视觉特征，并通过交叉注意力与知识交互。视觉标记被输入Q-former，并使用另一个交叉注意力检索疾病感知视觉标记。最后，我们采用大型语言模型将语义知识图谱、输入X光图像和疾病感知视觉标记映射为语言描述。在多个数据集上的大量实验充分验证了我们提出的知识图谱和X光报告生成框架的有效性。本文的源代码将在https://github.com/Event-AHU/Medical_Image_Analysis上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; X-ray medical report generation is one of the important applications ofartificial intelligence in healthcare. With the support of large foundationmodels, the quality of medical report generation has significantly improved.However, challenges such as hallucination and weak disease diagnosticcapability still persist. In this paper, we first construct a large-scalemulti-modal medical knowledge graph (termed M3KG) based on the ground truthmedical report using the GPT-4o. It contains 2477 entities, 3 kinds ofrelations, 37424 triples, and 6943 disease-aware vision tokens for the CheXpertPlus dataset. Then, we sample it to obtain multi-granularity semantic graphsand use an R-GCN encoder for feature extraction. For the input X-ray image, weadopt the Swin-Transformer to extract the vision features and interact with theknowledge using cross-attention. The vision tokens are fed into a Q-former andretrieved the disease-aware vision tokens using another cross-attention.Finally, we adopt the large language model to map the semantic knowledge graph,input X-ray image, and disease-aware vision tokens into language descriptions.Extensive experiments on multiple datasets fully validated the effectiveness ofour proposed knowledge graph and X-ray report generation framework. The sourcecode of this paper will be released onhttps://github.com/Event-AHU/Medical_Image_Analysis.</description>
      <author>example@mail.com (Futian Wang, Yuhan Qiao, Xiao Wang, Fuling Wang, Yuxiang Zhang, Dengdi Sun)</author>
      <guid isPermaLink="false">2508.03426v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models</title>
      <link>http://arxiv.org/abs/2508.03356v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages (main document) + 12 pages (appendix), 3 figures (main) + 12  figures (appendix), 5 tables (main) + 6 tables (appendix), submitted to AAAI  2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FedPromo是一种新型框架，通过两阶段过程和知识蒸馏技术，使大规模基础模型能够在资源有限的客户端设备上有效适应新领域，同时保持隐私保护和计算效率。&lt;h4&gt;背景&lt;/h4&gt;联邦学习(Federated Learning)是一种在分散数据上训练深度学习模型的成熟范式。然而，随着模型规模的增长，传统联邦学习方法通常需要客户端设备大量计算资源，这可能不切实际。&lt;h4&gt;目的&lt;/h4&gt;引入FedPromo框架，使存储在中央服务器上的大规模基础模型能够有效适应远程客户端遇到的新领域，同时减少计算开销并保持隐私保护。&lt;h4&gt;方法&lt;/h4&gt;FedPromo通过两阶段过程实现：首先，服务器端知识蒸馏将大规模基础模型(如transformer)的表示与紧凑模型(如CNN)的表示对齐；然后，紧凑模型编码器部署到客户端设备，在本地学习可训练分类器。这些分类器随后被聚合并无缝传输回基础模型，促进个性化适应而无需直接访问用户数据。通过新颖的正则化策略，框架实现分散式多领域学习，平衡性能、隐私和资源效率。&lt;h4&gt;主要发现&lt;/h4&gt;在五个图像分类基准上的大量实验表明，FedPromo在资源有限的客户端情况下优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;FedPromo提供了一种有效的方法，使大规模基础模型能够在联邦学习环境中适应新领域，同时减少计算开销并保持隐私保护。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习(Federated Learning)是一种在分散数据上训练深度学习模型的成熟范式。然而，随着模型规模的增长，传统的联邦学习方法通常需要客户端设备大量计算资源，这可能不切实际。我们引入了FedPromo，一种新型框架，使存储在中央服务器上的大规模基础模型能够有效适应远程客户端遇到的新领域。FedPromo不是直接在客户端设备上训练大模型，而是通过联邦学习优化轻量级代理模型，显著减少计算开销同时保持隐私。我们的方法遵循两阶段过程：首先，服务器端知识蒸馏将大规模基础模型(如transformer)的表示与紧凑模型(如CNN)的表示对齐。然后，紧凑模型编码器部署到客户端设备，在本地学习可训练分类器。这些分类器随后被聚合并无缝传输回基础模型，促进个性化适应而无需直接访问用户数据。通过新颖的正则化策略，我们的框架能够实现分散式多领域学习，平衡性能、隐私和资源效率。在五个图像分类基准上的大量实验表明，FedPromo在资源有限的客户端情况下优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Learning (FL) is an established paradigm for training deep learningmodels on decentralized data. However, as the size of the models grows,conventional FL approaches often require significant computational resources onclient devices, which may not be feasible. We introduce FedPromo, a novelframework that enables efficient adaptation of large-scale foundation modelsstored on a central server to new domains encountered only by remote clients.Instead of directly training the large model on client devices, FedPromooptimizes lightweight proxy models via FL, significantly reducing computationaloverhead while maintaining privacy. Our method follows a two-stage process:first, server-side knowledge distillation aligns the representations of alarge-scale foundation model (e.g., a transformer) with those of a compactcounterpart (e.g., a CNN). Then, the compact model encoder is deployed toclient devices, where trainable classifiers are learned locally. Theseclassifiers are subsequently aggregated and seamlessly transferred back to thefoundation model, facilitating personalized adaptation without requiring directaccess to user data. Through novel regularization strategies, our frameworkenables decentralized multi-domain learning, balancing performance, privacy,and resource efficiency. Extensive experiments on five image classificationbenchmarks demonstrate that FedPromo outperforms existing methods whileassuming limited-resource clients.</description>
      <author>example@mail.com (Matteo Caligiuri, Francesco Barbato, Donald Shenaj, Umberto Michieli, Pietro Zanuttigh)</author>
      <guid isPermaLink="false">2508.03356v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools</title>
      <link>http://arxiv.org/abs/2508.03284v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出ToolVQA数据集和ToolEngine数据生成管道，旨在提高大型基础模型在真实世界多模态工具使用场景中的问题解决能力，特别是在需要多步推理的复杂任务中。&lt;h4&gt;背景&lt;/h4&gt;现有研究在工具增强的视觉问答方面已显示出强大性能，但最近的基准测试显示，在需要多步推理的真实世界工具使用能力方面存在显著差距，特别是在功能多样的多模态环境中。&lt;h4&gt;目的&lt;/h4&gt;介绍ToolVQA，一个大规模多模态数据集，包含23K个实例，旨在弥合大型基础模型在真实世界工具使用能力方面的差距。&lt;h4&gt;方法&lt;/h4&gt;提出ToolEngine，一种新颖的数据生成管道，采用带有动态上下文示例匹配机制的深度优先搜索来模拟类人工具使用推理，构建了包含10种多模态工具跨越7个不同任务领域的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在ToolVQA上微调的70亿参数大型基础模型不仅在测试集上取得了令人印象深刻的性能，而且在各种分布外数据集上超越了大型闭源模型GPT-3.5-turbo，显示出对真实世界工具使用场景的强大泛化能力。&lt;h4&gt;结论&lt;/h4&gt;ToolVQA数据集和ToolEngine管道能够有效提高大型基础模型在真实世界多模态工具使用场景中的表现和泛化能力，特别是在需要多步推理的复杂任务中。&lt;h4&gt;翻译&lt;/h4&gt;将外部工具集成到大型基础模型中已成为提高其问题解决能力的一种有前景的方法。虽然现有研究已在工具增强的视觉问答方面展示了强大性能，但最近的基准测试揭示了在真实世界工具使用能力方面的显著差距，特别是在功能多样的多模态环境中需要多步推理。在这项工作中，我们介绍了ToolVQA，一个包含23K个实例的大规模多模态数据集，旨在弥合这一差距。与依赖合成场景和简化查询的先前数据集不同，ToolVQA具有真实世界的视觉上下文和具有挑战性的隐式多步推理任务，更好地与真实用户交互保持一致。为构建此数据集，我们提出了ToolEngine，一种新颖的数据生成管道，它采用带有动态上下文示例匹配机制的深度优先搜索来模拟类人工具使用推理。ToolVQA涵盖7个不同任务领域的10种多模态工具，每个实例平均推理长度为2.78步。在ToolVQA上微调的70亿参数大型基础模型不仅在测试集上取得了令人印象深刻的性能，还在各种分布外数据集上超越了大型闭源模型GPT-3.5-turbo，显示出对真实世界工具使用场景的强大泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating external tools into Large Foundation Models (LFMs) has emerged asa promising approach to enhance their problem-solving capabilities. Whileexisting studies have demonstrated strong performance in tool-augmented VisualQuestion Answering (VQA), recent benchmarks reveal significant gaps inreal-world tool-use proficiency, particularly in functionally diversemultimodal settings requiring multi-step reasoning. In this work, we introduceToolVQA, a large-scale multimodal dataset comprising 23K instances, designed tobridge this gap. Unlike previous datasets that rely on synthetic scenarios andsimplified queries, ToolVQA features real-world visual contexts and challengingimplicit multi-step reasoning tasks, better aligning with real userinteractions. To construct this dataset, we propose ToolEngine, a novel datageneration pipeline that employs Depth-First Search (DFS) with a dynamicin-context example matching mechanism to simulate human-like tool-usereasoning. ToolVQA encompasses 10 multimodal tools across 7 diverse taskdomains, with an average inference length of 2.78 reasoning steps per instance.The fine-tuned 7B LFMs on ToolVQA not only achieve impressive performance onour test set but also surpass the large close-sourced model GPT-3.5-turbo onvarious out-of-distribution (OOD) datasets, demonstrating stronggeneralizability to real-world tool-use scenarios.</description>
      <author>example@mail.com (Shaofeng Yin, Ting Lei, Yang Liu)</author>
      <guid isPermaLink="false">2508.03284v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Zero-shot Shape Classification of Nanoparticles in SEM Images using Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.03235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了一种基于视觉基础模型的零样本分类流程，用于扫描电子显微镜图像中纳米颗粒形态的高效准确表征，无需大量标记数据和参数微调。&lt;h4&gt;背景&lt;/h4&gt;在纳米材料合成中，准确高效地表征纳米颗粒形态对确保产品质量和加速开发至关重要，但传统深度学习方法受限于大量标记数据需求和计算密集型训练。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需大量标记数据和参数微调的纳米颗粒形态分类方法，使其对研究及工业环境中的纳米颗粒从业者更易访问和使用。&lt;h4&gt;方法&lt;/h4&gt;结合Segment Anything Model (SAM)用于目标分割和DINOv2用于特征嵌入，与轻量级分类器构建零样本分类流程，应用于三个形态多样化的纳米颗粒数据集。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在纳米颗粒形状分类上优于微调的YOLOv11和ChatGPT o4-mini-high基线，对小数据集、细微形态变化和域转移具有鲁棒性，且DINOv2特征的PCA聚类指标可用于评估化学合成进展。&lt;h4&gt;结论&lt;/h4&gt;基础模型在自动化显微镜图像分析方面具有巨大潜力，为纳米颗粒研究中的传统深度学习流程提供了更高效、更用户友好的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;准确高效地表征扫描电子显微镜(SEM)图像中的纳米颗粒形态对于确保纳米材料合成产品质量和加速开发至关重要。然而，传统的用于形状分类的深度学习方法需要大量标记的数据集和计算密集型训练，限制了其在研究和工业环境中典型纳米颗粒从业者中的应用。在本研究中，我们引入了一种零样本分类流程，利用两个视觉基础模型：Segment Anything Model (SAM)用于目标分割，DINOv2用于特征嵌入。将这些模型与轻量级分类器结合，我们在三个形态多样化的纳米颗粒数据集上实现了高精度的形状分类，无需大量参数微调。我们的方法优于微调的YOLOv11和ChatGPT o4-mini-high基线，展示了对小数据集、细微形态变化和从自然图像到科学成像的域转移的鲁棒性。讨论了DINOv2特征的PCA图上的定量聚类指标，作为评估化学合成进展的一种手段。这项工作强调了基础模型在推进自动化显微镜图像分析方面的潜力，为纳米颗粒研究中的传统深度学习流程提供了一种更高效、更易于用户使用的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and efficient characterization of nanoparticle morphology inScanning Electron Microscopy (SEM) images is critical for ensuring productquality in nanomaterial synthesis and accelerating development. However,conventional deep learning methods for shape classification require extensivelabeled datasets and computationally demanding training, limiting theiraccessibility to the typical nanoparticle practitioner in research andindustrial settings. In this study, we introduce a zero-shot classificationpipeline that leverages two vision foundation models: the Segment AnythingModel (SAM) for object segmentation and DINOv2 for feature embedding. Bycombining these models with a lightweight classifier, we achieve high-precisionshape classification across three morphologically diverse nanoparticle datasets- without the need for extensive parameter fine-tuning. Our methodologyoutperforms a fine-tuned YOLOv11 and ChatGPT o4-mini-high baselines,demonstrating robustness to small datasets, subtle morphological variations,and domain shifts from natural to scientific imaging. Quantitative clusteringmetrics on PCA plots of the DINOv2 features are discussed as a means ofassessing the progress of the chemical synthesis. This work highlights thepotential of foundation models to advance automated microscopy image analysis,offering an alternative to traditional deep learning pipelines in nanoparticleresearch which is both more efficient and more accessible to the user.</description>
      <author>example@mail.com (Freida Barnatan, Emunah Goldstein, Einav Kalimian, Orchen Madar, Avi Huri, David Zitoun, Ya'akov Mandelbaum, Moshe Amitay)</author>
      <guid isPermaLink="false">2508.03235v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2508.03118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出H3R混合框架，通过集成体积潜在融合与基于注意力的特征聚合，解决了3D重建中多视图对应建模的挑战，实现了比现有方法快2倍的收敛速度，并在多个基准测试上取得最先进性能。&lt;h4&gt;背景&lt;/h4&gt;尽管前馈3D高斯喷溅技术最近有所进展，通用3D重建仍然具有挑战性，特别是在多视图对应建模方面。现有方法面临基本权衡：显式方法具有几何精度但在模糊区域表现不佳，隐式方法提供鲁棒性但收敛速度慢。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提高通用性，加快收敛速度，并解决语义表示与空间重建需求之间的不匹配问题。&lt;h4&gt;方法&lt;/h4&gt;提出H3R混合框架，集成体积潜在融合与基于注意力的特征聚合。框架包含两个互补组件：1) 高效的潜在体积，通过极线约束强制几何一致性；2) 相机感知Transformer，利用普吕克坐标进行自适应对应细化。同时探索空间对齐的基础模型（如SD-VAE）比语义对齐的模型（如DINOv2）表现更好。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法提高了通用性，收敛速度比现有方法快2倍；空间对齐的基础模型比语义对齐的模型表现更好；支持可变数量和高分辨率输入视图；展示了强大的跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;在多个基准测试上取得了最先进的性能，在RealEstate10K、ACID和DTU数据集上分别有0.59 dB、1.06 dB和0.22 dB的显著PSNR改进。&lt;h4&gt;翻译&lt;/h4&gt;尽管前馈3D高斯喷溅技术最近有所进展，通用3D重建仍然具有挑战性，特别是在多视图对应建模方面。现有方法面临基本权衡：显式方法实现几何精度但在模糊区域表现不佳，而隐式方法提供鲁棒性但收敛缓慢。我们提出H3R，一个通过集成体积潜在融合与基于注意力的特征聚合来解决这一局限性的混合框架。我们的框架包含两个互补组件：一个通过极线约束强制几何一致性的高效潜在体积，以及一个利用普吕克坐标进行自适应对应细化的相机感知Transformer。通过集成这两种范式，我们的方法提高了通用性，同时收敛速度比现有方法快2倍。此外，我们表明空间对齐的基础模型（如SD-VAE）明显优于语义对齐的模型（如DINOv2），解决了语义表示与空间重建需求之间的不匹配问题。我们的方法支持可变数量和高分辨率输入视图，同时展示了强大的跨数据集泛化能力。大量实验表明，我们的方法在多个基准测试上取得了最先进的性能，在RealEstate10K、ACID和DTU数据集上分别有0.59 dB、1.06 dB和0.22 dB的显著PSNR改进。代码可在https://github.com/JiaHeng-DLUT/H3R获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决通用化3D重建中的多视角对应关系建模挑战。现有方法在显式方法(几何精确但处理模糊区域能力弱)和隐式方法(鲁棒性强但收敛慢)之间存在权衡。这个问题很重要，因为3D重建在虚拟现实、自动驾驶等领域有广泛应用，而传统方法需要针对每个场景进行昂贵训练，限制了实用性。通用化3D重建方法不需要场景特定优化，能快速应用于各种场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法在显式和隐式对应关系建模间的权衡问题，然后设计混合框架结合两者优势。方法借鉴了多视角立体中的平面扫描立体技术构建潜在体积，采用Transformer架构进行跨视角对应关系聚合，并使用3D高斯溅射作为场景表示。此外，作者系统评估不同视觉基础模型，发现空间对齐模型(如SD-VAE)比语义对齐模型更适合3D重建。为适应实际应用，还开发了H3R-α(处理多视图)和H3R-β(高分辨率重建)两个扩展版本。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合显式和隐式多视角对应关系建模的优点，通过混合框架解决两者间的权衡。具体包括：使用显式体积潜在融合捕获几何精确对应关系，基于相机的Transformer处理模糊场景，以及利用空间对齐视觉表示增强空间保真度。整体流程分为五步：1)视觉编码从上下文视图提取特征；2)极线匹配使用几何约束对齐特征并构建潜在体积；3)相机嵌入集成参数并通过多视角注意力细化特征；4)高斯解码将特征转换为像素对齐的高斯参数；5)实时渲染实现新视角合成。多视角重建还支持可变输入视图和目标视图集成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)混合多视角对应关系建模，统一显式体积融合和隐式Transformer，解决方法间权衡；2)空间对齐的视觉表示，证明SD-VAE等模型比DINOv2等语义对齐模型更适合3D重建；3)全面的性能改进，解决三个核心挑战并实现最先进性能。相比之前工作，不同之处在于：结合了单一方法通常不具备的显式和隐式优势；采用更合适的视觉表示；支持可变输入视图和高分辨率重建；在多个基准测试上实现显著性能提升(PSNR提高0.22-1.06 dB)且训练速度快2倍。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; H3R通过结合显式体积融合和隐式基于相机的Transformer，并采用空间对齐的视觉表示，解决了通用化3D重建中多视角对应关系建模的权衡问题，实现了更高质量、更快收敛且具有强大泛化能力的3D重建方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable3D reconstruction remains challenging, particularly in multi-viewcorrespondence modeling. Existing approaches face a fundamental trade-off:explicit methods achieve geometric precision but struggle with ambiguousregions, while implicit methods provide robustness but suffer from slowconvergence. We present H3R, a hybrid framework that addresses this limitationby integrating volumetric latent fusion with attention-based featureaggregation. Our framework consists of two complementary components: anefficient latent volume that enforces geometric consistency through epipolarconstraints, and a camera-aware Transformer that leverages Pl\"uckercoordinates for adaptive correspondence refinement. By integrating bothparadigms, our approach enhances generalization while converging 2$\times$faster than existing methods. Furthermore, we show that spatial-alignedfoundation models (e.g., SD-VAE) substantially outperform semantic-alignedmodels (e.g., DINOv2), resolving the mismatch between semantic representationsand spatial reconstruction requirements. Our method supports variable-numberand high-resolution input views while demonstrating robust cross-datasetgeneralization. Extensive experiments show that our method achievesstate-of-the-art performance across multiple benchmarks, with significant PSNRimprovements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, andDTU datasets, respectively. Code is available athttps://github.com/JiaHeng-DLUT/H3R.</description>
      <author>example@mail.com (Heng Jia, Linchao Zhu, Na Zhao)</author>
      <guid isPermaLink="false">2508.03118v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping</title>
      <link>http://arxiv.org/abs/2508.03099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Point2Act是一种直接检索与上下文描述任务相关的3D动作点的方法，利用多模态大语言模型(MLLMs)。该方法绕过高维特征，而是高效地注入轻量级2D点级指导，专门针对特定任务的动作。多视图聚合有效补偿了几何歧义和语义不确定性带来的错位。完整流程在20秒内生成空间响应，促进实际操作任务。&lt;h4&gt;背景&lt;/h4&gt;基础模型为通用机器人提供了可能性，使其能够在未知环境中执行零样本任务，遵循自然语言描述。然而，从大规模图像和语言数据集获得的语义虽然在2D图像中提供上下文理解，但其丰富而细微的特征只能推断出模糊的2D区域，难以找到精确的3D动作位置。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接检索与上下文描述任务相关的3D动作点的方法，解决从2D语义理解到精确3D动作定位的挑战，使机器人能够在未知环境中执行实际操作任务。&lt;h4&gt;方法&lt;/h4&gt;提出3D相关性场，绕过高维特征，而是高效地注入针对特定任务动作的轻量级2D点级指导。使用多视图聚合有效补偿了几何歧义（如遮挡）或语言描述中固有的语义不确定性带来的错位。完整流程包括捕获、MLLM查询、3D重建和抓取姿态提取。&lt;h4&gt;主要发现&lt;/h4&gt;输出区域高度局部化，能够推理细粒度的3D空间上下文，可以直接转移到场景即时重建中的物理动作明确位置。完整流程在20秒内生成空间响应，促进实际操作任务。&lt;h4&gt;结论&lt;/h4&gt;Point2Act成功解决了从2D语义理解到精确3D动作定位的挑战，通过轻量级点级指导和多视图聚合，实现了快速的空间响应生成，为机器人在未知环境中执行实际操作任务提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Point2Act，它直接检索与上下文描述任务相关的3D动作点，利用多模态大语言模型(MLLMs)。基础模型为通用机器人打开了可能性，使其能够在未知环境中执行零样本任务，遵循自然语言描述。虽然从大规模图像和语言数据集获得的语义在2D图像中提供上下文理解，但其丰富而细微的特征只能推断出模糊的2D区域，难以找到精确的3D动作位置。我们提出的3D相关性场绕过高维特征，而是高效地注入针对特定任务动作的轻量级2D点级指导。多视图聚合有效补偿了几何歧义（如遮挡）或语言描述中固有的语义不确定性带来的错位。输出区域高度局部化，能够推理细粒度的3D空间上下文，可以直接转移到场景即时重建中的物理动作明确位置。我们的完整流程，包括捕获、MLLM查询、3D重建和抓取姿态提取，在20秒内生成空间响应，促进实际操作任务。项目页面：https://sangminkim-99.github.io/point2act/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人如何根据自然语言描述在3D场景中精确定位并执行抓取动作的问题，特别是在零样本和上下文感知的场景下。这个问题很重要，因为它能让机器人理解人类自然语言指令，处理新环境中的新任务，并根据任务需求选择合适的抓取位置，而不仅仅是稳定抓取，这对机器人在现实世界中的应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法（如LERF和F3RM）使用高维特征进行3D场景表示存在计算效率低和定位不精确的问题。他们提出使用MLLM直接预测2D点，然后蒸馏成3D相关性场的方法，避免了高维特征的计算负担。该方法借鉴了多视角图像3D重建（如NeRF）、MLLM语言理解（如Molmo）和现有抓取姿态生成方法（如AnyGrasp），并通过多视角聚合处理遮挡和视角变化问题，最后采用流水线处理优化整个流程减少延迟。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用MLLM直接预测多视角图像中的2D相关点，然后蒸馏成一个轻量级的3D相关性场，避免高维特征的计算负担，同时提供精确的空间定位。整体流程包括：1) 从多视角捕获场景图像；2) 将图像和指令输入MLLM获取2D相关点；3) 使用NeRF重建3D场景并学习相关性分数；4) 将多视角2D点预测蒸馏成3D相关性场；5) 将相关性场转换为点云并生成抓取姿态候选；6) 根据相关性场选择最符合指令的抓取姿态。整个流程采用流水线设计，可在约20秒内完成。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 使用MLLM直接预测2D点作为中间表示，降低计算复杂度；2) 提出轻量级3D相关性场编码单通道相关性信息；3) 通过多视角聚合处理遮挡和视角变化；4) 设计高效流水线系统实现20秒内完成全流程；5) 支持零样本上下文感知抓取，处理多种语言查询。相比之前工作，Point2Act避免了高维特征的计算负担，提高了定位精度和鲁棒性，不需要大量机器人演示数据训练，能处理更复杂的组合描述和上下文差异。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Point2Act通过将多模态大语言模型的2D点预测蒸馏成高效的3D相关性场，实现了零样本上下文感知的机器人抓取，在保持高精度的同时显著提高了计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose Point2Act, which directly retrieves the 3D action point relevantfor a contextually described task, leveraging Multimodal Large Language Models(MLLMs). Foundation models opened the possibility for generalist robots thatcan perform a zero-shot task following natural language descriptions within anunseen environment. While the semantics obtained from large-scale image andlanguage datasets provide contextual understanding in 2D images, the rich yetnuanced features deduce blurry 2D regions and struggle to find precise 3Dlocations for actions. Our proposed 3D relevancy fields bypass thehigh-dimensional features and instead efficiently imbue lightweight 2Dpoint-level guidance tailored to the task-specific action. The multi-viewaggregation effectively compensates for misalignments due to geometricambiguities, such as occlusion, or semantic uncertainties inherent in thelanguage descriptions. The output region is highly localized, reasoningfine-grained 3D spatial context that can directly transfer to an explicitposition for physical action at the on-the-fly reconstruction of the scene. Ourfull-stack pipeline, which includes capturing, MLLM querying, 3Dreconstruction, and grasp pose extraction, generates spatially groundedresponses in under 20 seconds, facilitating practical manipulation tasks.Project page: https://sangminkim-99.github.io/point2act/</description>
      <author>example@mail.com (Sang Min Kim, Hyeongjun Heo, Junho Kim, Yonghyeon Lee, Young Min Kim)</author>
      <guid isPermaLink="false">2508.03099v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2508.03007v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为多粒度特征校准(MGFC)的新框架，用于增强领域泛化语义分割任务中的模型泛化能力，通过层次化和粒度感知的特征校准，有效将视觉基础模型的泛化能力转移到领域特定任务中。&lt;h4&gt;背景&lt;/h4&gt;领域泛化语义分割(DGSS)旨在提高模型在未见领域上的泛化能力，训练过程中无法访问目标数据。近期DGSS进展越来越多地利用视觉基础模型(VFMs)并通过参数高效微调策略，但大多数方法专注于全局特征微调，忽略了跨特征层级的层次化适应，这对精确密集预测至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一个新框架，对视觉基础模型特征进行从粗到细的对齐，增强在领域变化下的鲁棒性，解决现有方法忽略层次化适应的问题。&lt;h4&gt;方法&lt;/h4&gt;提出多粒度特征校准(MGFC)框架，首先校准粗粒度特征捕获全局上下文语义和场景级结构，然后通过提高类别级特征判别能力细化中等粒度特征，最后通过高频空间细节增强校准细粒度特征，实现层次化和粒度感知的特征校准。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的广泛实验表明，该方法优于最先进的DGSS方法，多粒度适应对于领域泛化的语义分割任务是有效的。&lt;h4&gt;结论&lt;/h4&gt;多粒度特征校准(MGFC)框架能够有效解决DGSS任务中的领域泛化问题，层次化和粒度感知的特征校准对于视觉基础模型在语义分割任务中的有效迁移至关重要。&lt;h4&gt;翻译&lt;/h4&gt;领域泛化语义分割(DGSS)旨在提高模型在未见领域的泛化能力，且在训练过程中无法访问目标数据。近期DGSS进展越来越多地利用视觉基础模型(VFMs)并通过参数高效的微调策略。然而，大多数现有方法专注于全局特征微调，而忽略了跨特征层级的层次化适应，这对精确的密集预测至关重要。本文提出多粒度特征校准(MGFC)，一种新框架，对VFM特征进行从粗到细的对齐以增强在领域变化下的鲁棒性。具体而言，MGFC首先校准粗粒度特征以捕获全局上下文语义和场景级结构。然后，通过提高类别级特征判别能力来细化中等粒度特征。最后，通过高频空间细节增强来校准细粒度特征。通过层次化和粒度感知的校准，MGFC有效地将VFMs的泛化能力转移到DGSS的领域特定任务中。在基准数据集上的广泛实验表明，我们的方法优于最先进的DGSS方法，突显了多粒度适应对领域泛化语义分割任务的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain Generalized Semantic Segmentation (DGSS) aims to improve thegeneralization ability of models across unseen domains without access to targetdata during training. Recent advances in DGSS have increasingly exploitedvision foundation models (VFMs) via parameter-efficient fine-tuning strategies.However, most existing approaches concentrate on global feature fine-tuning,while overlooking hierarchical adaptation across feature levels, which iscrucial for precise dense prediction. In this paper, we proposeMulti-Granularity Feature Calibration (MGFC), a novel framework that performscoarse-to-fine alignment of VFM features to enhance robustness under domainshifts. Specifically, MGFC first calibrates coarse-grained features to captureglobal contextual semantics and scene-level structure. Then, it refinesmedium-grained features by promoting category-level feature discriminability.Finally, fine-grained features are calibrated through high-frequency spatialdetail enhancement. By performing hierarchical and granularity-awarecalibration, MGFC effectively transfers the generalization strengths of VFMs tothe domain-specific task of DGSS. Extensive experiments on benchmark datasetsdemonstrate that our method outperforms state-of-the-art DGSS approaches,highlighting the effectiveness of multi-granularity adaptation for the semanticsegmentation task of domain generalization.</description>
      <author>example@mail.com (Xinhui Li, Xiaojie Guo)</author>
      <guid isPermaLink="false">2508.03007v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Real-time speech enhancement in noise for throat microphone using neural audio codec as foundation model</title>
      <link>http://arxiv.org/abs/2508.02974v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了一个使用喉部麦克风捕获语音的实时语音增强演示系统，解决了喉部麦克风带宽受限的问题，并提供了完整的处理流程。&lt;h4&gt;背景&lt;/h4&gt;喉部麦克风记录皮肤振动，能自然衰减外部噪声，但代价是音频带宽减少，这限制了语音质量。&lt;h4&gt;目的&lt;/h4&gt;展示在嘈杂环境中使用体导麦克风捕获语音的完整处理流程，并解决喉部麦克风带宽受限的问题。&lt;h4&gt;方法&lt;/h4&gt;微调Kyutai的Mimi（一种支持实时推理的神经音频编解码器）在Vibravox数据集上，该数据集包含空气传导和喉部麦克风的配对录音。&lt;h4&gt;主要发现&lt;/h4&gt;与最先进的模型相比，所提出的增强策略展现出优越的性能。&lt;h4&gt;结论&lt;/h4&gt;该实时语音增强系统有效解决了喉部麦克风带宽受限的问题，并通过交互式界面提供了良好的用户体验。&lt;h4&gt;翻译&lt;/h4&gt;我们展示了一个使用喉部麦克风捕获语音的实时语音增强演示。这个演示旨在展示完整流程，从录制到基于深度学习的后处理，用于在嘈杂环境中使用体导麦克风捕获的语音。喉部麦克风记录皮肤振动，自然衰减外部噪声，但这种稳健性是以减少音频带宽为代价的。为应对这一挑战，我们在Vibravox数据集上微调了Kyutai的Mimi——一种支持实时推理的神经音频编解码器，该数据集包含空气传导和喉部麦克风的配对录音。我们将这种增强策略与最先进的模型进行比较，展示了其优越性能。推理在交互式界面中运行，允许用户切换增强、可视化频谱图和监控处理延迟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a real-time speech enhancement demo using speech captured with athroat microphone. This demo aims to showcase the complete pipeline, fromrecording to deep learning-based post-processing, for speech captured in noisyenvironments with a body-conducted microphone. The throat microphone recordsskin vibrations, which naturally attenuate external noise, but this robustnesscomes at the cost of reduced audio bandwidth. To address this challenge, wefine-tune Kyutai's Mimi--a neural audio codec supporting real-timeinference--on Vibravox, a dataset containing paired air-conducted and throatmicrophone recordings. We compare this enhancement strategy againststate-of-the-art models and demonstrate its superior performance. The inferenceruns in an interactive interface that allows users to toggle enhancement,visualize spectrograms, and monitor processing latency.</description>
      <author>example@mail.com (Julien Hauret, Thomas Joubaud, Éric Bavu)</author>
      <guid isPermaLink="false">2508.02974v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow</title>
      <link>http://arxiv.org/abs/2508.02959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 12 figures, under review for AAAI2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Polymath的自优化代理，具有动态层次工作流程，能够解决现实世界的动态问题，无需标记数据即可优化工作流程。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型通过代理工作流程解决复杂任务，但通过文本接口手动构建通用代理系统存在可扩展性和效率限制。现有基于代码的工作流程优化方法依赖标记数据，无法有效处理标记数据不可用的现实世界动态问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需标记数据即可自动生成和优化工作流程的代理系统，以解决现实世界的动态问题。&lt;h4&gt;方法&lt;/h4&gt;Polymath结合任务流图的灵活性和代码表示工作流程的表现力，采用多网格启发的图优化与自反思引导的进化算法相结合的优化方法，实现无标记数据的工作流程优化。&lt;h4&gt;主要发现&lt;/h4&gt;在编码、数学和多轮问答任务的六个基准数据集上，Polymath比最先进的基线平均提高了8.1%的性能。&lt;h4&gt;结论&lt;/h4&gt;Polymath通过动态层次工作流程和无标记数据优化方法，有效解决了现有代理系统在处理现实世界动态问题时的局限性。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型通过执行由详细指令和结构化操作组成的代理工作流程，在解决复杂任务方面表现出色。然而，通过文本接口将基础模型手动嵌入到代理系统(如Chain-of-Thought、Self-Reflection和ReACT)中构建通用代理，限制了可扩展性和效率。最近，许多研究人员试图通过基于代码的表示来自动生成和优化这些工作流程。然而，现有方法通常依赖标记数据集来训练和优化工作流程，使得它们在解决标记数据不可用的现实世界动态问题时效果不佳且不够灵活。为解决这一挑战，我们引入了Polymath，这是一个具有动态层次工作流程的自优化代理，利用任务流图的灵活性和代码表示工作流程的表现力来解决各种现实世界的动态问题。所提出的优化方法将多网格启发的图优化与自反思引导的进化算法相结合，以在没有标记数据的情况下优化工作流程。在编码、数学和多轮问答任务的六个基准数据集上的实验结果表明，Polymath比最先进的基线平均提高了8.1%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) excel at solving complex tasks by executingagentic workflows composed of detailed instructions and structured operations.Yet, building general-purpose agents by manually embedding foundation modelsinto agentic systems such as Chain-of-Thought, Self-Reflection, and ReACTthrough text interfaces limits scalability and efficiency. Recently, manyresearchers have sought to automate the generation and optimization of theseworkflows through code-based representations. However, existing methods oftenrely on labeled datasets to train and optimize workflows, making themineffective and inflexible for solving real-world, dynamic problems wherelabeled data is unavailable. To address this challenge, we introduce Polymath,a self-optimizing agent with dynamic hierarchical workflow that leverages theflexibility of task flow graphs and the expressiveness of code-representedworkflows to solve a wide range of real-world, dynamic problems. The proposedoptimization methodology integrates multi-grid-inspired graph optimization witha self-reflection-guided evolutionary algorithm to refine workflows withoutlabeled data. Experimental results on six benchmark datasets across coding,math, and multi-turn QA tasks show that Polymath achieves 8.1% averageimprovement over state-of-the-art baselines.</description>
      <author>example@mail.com (Chia-Tung Ho, Jing Gong, Xufeng Yao, Yunsheng Bai, Abhishek B Akkur, Haoxing Ren)</author>
      <guid isPermaLink="false">2508.02959v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Seemingly Simple Planning Problems are Computationally Challenging: The Countdown Game</title>
      <link>http://arxiv.org/abs/2508.02900v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于Countdown游戏的规划基准测试，解决了现有基准测试无法有效衡量基础模型和代理规划能力的问题。Countdown游戏要求玩家通过算术运算从输入数字形成目标数字，该问题具有直观的自然语言描述、计算复杂度高(NP完全)、实例空间丰富等特点，能有效防止记忆问题。评估显示该基准测试对现有LLM辅助规划方法极具挑战性。&lt;h4&gt;背景&lt;/h4&gt;当前基础模型和代理无法制定长期计划是一个关键局限，而现有规划基准测试存在严重不足：要么专注于难以形式化和验证的松散定义任务(如旅行规划)，要么利用国际规划竞赛中专门用于测试现有自动规划器弱点的现有问题。&lt;h4&gt;目的&lt;/h4&gt;创建一种以Countdown游戏为中心的规划基准测试，该游戏要求玩家通过算术运算从输入数字列表中形成目标数字，以有效评估基础模型和代理的规划能力。&lt;h4&gt;方法&lt;/h4&gt;提出Countdown规划基准测试并进行广泛理论分析，建立计算复杂性结果；展示实例生成程序优于公共基准测试的优势；使用程序生成的实例评估各种现有LLM辅助规划方法。&lt;h4&gt;主要发现&lt;/h4&gt;Countdown问题领域允许对每个问题实例进行直观的、自然语言的描述；该问题在计算上具有挑战性(NP完全)；实例空间足够丰富，无需担心记忆问题；与24点游戏等其他领域不同，所提出的动态基准测试对现有基于LLM的方法仍然极具挑战性。&lt;h4&gt;结论&lt;/h4&gt;Countdown基准测试满足了理想规划能力评估基准的多个要求，是一个有效的规划能力评估工具，能有效区分现有方法的性能。&lt;h4&gt;翻译&lt;/h4&gt;普遍认为，无法制定长期计划是当前基础模型和代理的关键局限之一。然而，现有的规划基准测试远远不足以真正衡量它们的规划能力。大多数现有基准测试要么专注于难以形式化和验证的松散定义任务(如旅行规划)，要么最终利用国际规划竞赛中现有的领域和问题，而这些问题专门设计用于测试和挑战现有自动规划器的弱点。为解决这些不足，我们提出了一种创建以名为Countdown的游戏为中心的规划基准测试的程序，在该游戏中，玩家需要通过算术运算从输入数字列表中形成一个目标数字。我们讨论了这个问题如何满足与理想的规划能力评估基准相关的许多期望特性。具体来说，该领域允许对每个问题实例进行直观的、自然语言的描述，它在计算上具有挑战性(NP完全)，并且实例空间足够丰富，我们不必担心记忆问题。我们进行了广泛的理论分析，建立了计算复杂性结果，并展示了我们的实例生成程序优于公共基准测试的优势。我们使用我们的程序生成的实例评估了各种现有的LLM辅助规划方法。我们的结果表明，与其他领域(如24点游戏，Countdown的特殊情况)不同，我们提出的动态基准测试对现有的基于LLM的方法仍然极具挑战性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There is a broad consensus that the inability to form long-term plans is oneof the key limitations of current foundational models and agents. However, theexisting planning benchmarks remain woefully inadequate to truly measure theirplanning capabilities. Most existing benchmarks either focus on loosely definedtasks like travel planning or end up leveraging existing domains and problemsfrom international planning competitions. While the former tasks are hard toformalize and verify, the latter were specifically designed to test andchallenge the weaknesses of existing automated planners. To address theseshortcomings, we propose a procedure for creating a planning benchmark centeredaround the game called Countdown, where a player is expected to form a targetnumber from a list of input numbers through arithmetic operations. We discusshow this problem meets many of the desiderata associated with an idealbenchmark for planning capabilities evaluation. Specifically, the domain allowsfor an intuitive, natural language description for each problem instance, it iscomputationally challenging (NP-complete), and the instance space is richenough that we do not have to worry about memorization. We perform an extensivetheoretical analysis, establishing the computational complexity result anddemonstrate the advantage of our instance generation procedure over publicbenchmarks. We evaluate a variety of existing LLM-assisted planning methods oninstances generated using our procedure. Our results show that, unlike otherdomains like 24 Game (a special case of Countdown), our proposed dynamicbenchmark remains extremely challenging for existing LLM-based approaches.</description>
      <author>example@mail.com (Michael Katz, Harsha Kokel, Sarath Sreedharan)</author>
      <guid isPermaLink="false">2508.02900v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>CauKer: classification time series foundation models can be pretrained on synthetic data only</title>
      <link>http://arxiv.org/abs/2508.02879v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了CauKer，一种新型算法，用于生成多样且因果一致的人工时间序列，以实现时间序列基础模型(TSFMs)的高效样本预训练。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)因其强大的零样本能力和广泛的应用而受到关注，但通常需要在大规模、精心收集的真实序列上进行计算成本高昂的预训练。&lt;h4&gt;目的&lt;/h4&gt;提出一种算法，允许时间序列基础模型以更高效的样本方式进行预训练，减少对大规模真实数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;CauKer算法结合高斯过程(GP)核组合与结构因果模型(SCM)，生成具有真实趋势、季节性和非线性交互的多样因果一致人工时间序列数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，CauKer生成的数据集在数据集规模(10K至10M样本)和模型容量(1M至783M参数)方面显示出明确的缩放定律，而真实世界数据集则表现出不规则的缩放行为。&lt;h4&gt;结论&lt;/h4&gt;CauKer为时间序列基础模型提供了一种样本高效的预训练方法，通过生成高质量的人工时间序列数据，减少了对大规模真实数据的依赖。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)最近因其强大的零样本能力和广泛的应用而受到广泛关注。这类模型通常需要在大规模、精心收集的真实序列上进行计算成本高昂的预训练。为了实现TSFMs的样本高效预训练，我们提出了CauKer，一种新颖的算法，旨在生成具有真实趋势、季节性和非线性交互的多样、因果一致的人工时间序列。CauKer将高斯过程(GP)核组合与结构因果模型(SCM)相结合，为具有不同架构和遵循不同预训练方法的最先进分类TSFMs生成数据，以实现样本高效预训练。此外，我们的实验揭示，与表现出不规则缩放行为的真实世界数据集不同，CauKer生成的数据集在数据集规模(10K至10M样本)和模型容量(1M至783M参数)方面显示出明确的缩放定律。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series foundation models (TSFMs) have recently gained significantattention due to their strong zero-shot capabilities and widespread real-worldapplications. Such models typically require a computationally costlypretraining on large-scale, carefully curated collections of real-worldsequences. To allow for a sample-efficient pretraining of TSFMs, we proposeCauKer, a novel algorithm designed to generate diverse, causally coherentsynthetic time series with realistic trends, seasonality, and nonlinearinteractions. CauKer combines Gaussian Process (GP) kernel composition withStructural Causal Models (SCM) to produce data for sample-efficient pretrainingof state-of-the-art classification TSFMs having different architectures andfollowing different pretraining approaches. Additionally, our experimentsreveal that CauKer-generated datasets exhibit clear scaling laws for bothdataset size (10K to 10M samples) and model capacity (1M to 783M parameters),unlike real-world datasets, which display irregular scaling behavior.</description>
      <author>example@mail.com (Shifeng Xie, Vasilii Feofanov, Marius Alonso, Ambroise Odonnat, Jianfeng Zhang, Themis Palpanas, Ievgen Redko)</author>
      <guid isPermaLink="false">2508.02879v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows</title>
      <link>http://arxiv.org/abs/2508.02866v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper under peer-reviewed evaluation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PROV-AGENT，一种专为AI代理工作流设计的溯源模型，扩展了W3C PROV并利用模型上下文协议(MCP)来集成代理交互到端到端工作流溯源中。该系统解决了现有技术在捕获以代理为中心的元数据方面的不足，提供了近实时、开源的代理溯源解决方案，并在多种环境中进行了评估。&lt;h4&gt;背景&lt;/h4&gt;基础模型（如大型语言模型）正越来越多地被用作复杂、大规模工作流中AI代理的核心组件，这些工作流跨越联邦和异构环境。在代理工作流中，自主代理规划任务、与人类和同行互动，并塑造科学成果，这使得透明度、可追溯性、可重复性和可靠性变得至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决AI代理可能产生的幻觉或推理错误及其在工作流中传播的问题，通过引入细粒度溯源来链接代理决策、端到端上下文和下游影响。&lt;h4&gt;方法&lt;/h4&gt;介绍PROV-AGENT，一种扩展W3C PROV并利用模型上下文协议(MCP)的溯源模型，将代理交互集成到端到端工作流溯源中。&lt;h4&gt;主要发现&lt;/h4&gt;现有溯源技术无法捕获和关联以代理为中心的元数据（提示、响应和决策）与工作流的其余部分；PROV-AGENT提供了近实时、开源的代理溯源系统，并在边缘、云和HPC环境中展示了支持关键溯源查询和代理可靠性分析的能力。&lt;h4&gt;结论&lt;/h4&gt;PROV-AGENT解决了现有技术在捕获代理元数据方面的不足，提供了在多种环境中工作的系统，支持关键溯源查询和代理可靠性分析，有助于提高AI代理工作流的透明度和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;基础模型，如大型语言模型，正越来越多地被用作复杂、大规模工作流中AI代理的核心组件，这些工作流跨越联邦和异构环境。在代理工作流中，自主代理规划任务、与人类和同行互动，并塑造科学成果。这使得透明度、可追溯性、可重复性和可靠性变得至关重要。然而，基于AI的代理可能产生幻觉或推理错误，并且它们的决策可能会在工作流中传播错误，特别是当一个代理的输出成为另一个代理的输入时。因此，细粒度溯源对于链接代理决策、它们的端到端上下文和下游影响至关重要。虽然溯源技术长期以来支持可重复性和工作流数据理解，但它们无法捕获并将以代理为中心的元数据（提示、响应和决策）与工作流的其余部分关联起来。在本文中，我们介绍了PROV-AGENT，一种扩展W3C PROV并利用模型上下文协议(MCP)将代理交互集成到端到端工作流溯源中的溯源模型。我们的贡献包括：(1)专为代理工作流定制的溯源模型，(2)用于捕获代理溯源的近实时、开源系统，以及(3)跨越边缘、云和HPC环境的跨设施评估，展示了支持关键溯源查询和代理可靠性分析的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, such as Large Language Models (LLMs), are increasinglyused as core components of AI agents in complex, large-scale workflows acrossfederated and heterogeneous environments. In agentic workflows, autonomousagents plan tasks, interact with humans and peers, and shape scientificoutcomes. This makes transparency, traceability, reproducibility, andreliability essential. However, AI-based agents can hallucinate or reasonincorrectly, and their decisions may propagate errors through the workflow,especially when one agent's output feeds into another's input. Therefore,fine-grained provenance is essential to link agent decisions, their end-to-endcontext, and downstream impacts. While provenance techniques have longsupported reproducibility and workflow data understanding, they fail to captureand relate agent-centric metadata (prompts, responses, and decisions) with therest of the workflow. In this paper, we introduce PROV-AGENT, a provenancemodel that extends W3C PROV and leverages the Model Context Protocol (MCP) tointegrate agent interactions into end-to-end workflow provenance. Ourcontributions include: (1) a provenance model tailored for agentic workflows,(2) a near real-time, open-source system for capturing agentic provenance, and(3) a cross-facility evaluation spanning edge, cloud, and HPC environments,demonstrating support for critical provenance queries and agent reliabilityanalysis.</description>
      <author>example@mail.com (Renan Souza, Amal Gueroudji, Stephen DeWitt, Daniel Rosendo, Tirthankar Ghosal, Robert Ross, Prasanna Balaprakash, Rafael Ferreira da Silva)</author>
      <guid isPermaLink="false">2508.02866v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation</title>
      <link>http://arxiv.org/abs/2508.02808v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ICARE的可解释评估框架，用于评估自动生成的放射学报告。该框架利用大型语言模型代理和动态多选题回答方法，通过让两个代理基于不同报告相互提问来评估报告的临床准确性和一致性。研究表明，这种方法与专家判断更为一致，且具有良好的可解释性。&lt;h4&gt;背景&lt;/h4&gt;放射学成像在诊断、治疗计划和临床决策中至关重要。视觉-语言基础模型推动了自动放射学报告生成的发展，但安全部署需要可靠的临床评估。现有评估方法往往依赖表面相似度或作为黑盒操作，缺乏可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一种可解释且基于临床的放射学报告生成评估框架，解决现有评估指标的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入ICARE（可解释且基于临床的代理报告评估）框架，利用两个大型语言模型代理分别基于真实报告或生成报告生成临床问题并相互提问。答案的一致性作为临床精确度和召回率的可解释指标，通过将分数与问题-答案对关联实现透明评估。&lt;h4&gt;主要发现&lt;/h4&gt;临床研究表明ICARE与专家判断的一致性显著高于先前指标；扰动分析证实了其对临床内容的敏感性和可重复性；模型比较揭示了可解释的错误模式。&lt;h4&gt;结论&lt;/h4&gt;ICARE提供了一种更可靠、更可解释的放射学报告生成评估方法，能够更好地与临床专家判断保持一致。&lt;h4&gt;翻译&lt;/h4&gt;放射学成像在诊断、治疗计划和临床决策中至关重要。视觉-语言基础模型激发了人们对自动放射学报告生成(RRG)的兴趣，但安全部署需要对生成的报告进行可靠的临床评估。现有指标通常依赖于表面相似度或表现为黑盒，缺乏可解释性。我们引入ICARE（可解释且基于临床的代理报告评估），这是一个利用大型语言模型代理和动态多选题回答(MCQA)的可解释评估框架。两个代理分别基于真实报告或生成报告，生成有临床意义的问题并相互提问。答案上的一致性捕捉了发现结果的保留性和一致性，作为临床精确度和召回率的可解释代理。通过将分数与问题-答案对关联，ICARE实现了透明且可解释的评估。临床研究表明，ICARE与专家判断的一致性显著高于先前的指标。扰动分析证实了对临床内容的敏感性和可重复性，而模型比较则揭示了可解释的错误模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radiological imaging is central to diagnosis, treatment planning, andclinical decision-making. Vision-language foundation models have spurredinterest in automated radiology report generation (RRG), but safe deploymentrequires reliable clinical evaluation of generated reports. Existing metricsoften rely on surface-level similarity or behave as black boxes, lackinginterpretability. We introduce ICARE (Interpretable and Clinically-groundedAgent-based Report Evaluation), an interpretable evaluation frameworkleveraging large language model agents and dynamic multiple-choice questionanswering (MCQA). Two agents, each with either the ground-truth or generatedreport, generate clinically meaningful questions and quiz each other. Agreementon answers captures preservation and consistency of findings, serving asinterpretable proxies for clinical precision and recall. By linking scores toquestion-answer pairs, ICARE enables transparent, and interpretable assessment.Clinician studies show ICARE aligns significantly more with expert judgmentthan prior metrics. Perturbation analyses confirm sensitivity to clinicalcontent and reproducibility, while model comparisons reveal interpretable errorpatterns.</description>
      <author>example@mail.com (Radhika Dua, Young Joon, Kwon, Siddhant Dogra, Daniel Freedman, Diana Ruan, Motaz Nashawaty, Danielle Rigau, Daniel Alexander Alber, Kang Zhang, Kyunghyun Cho, Eric Karl Oermann)</author>
      <guid isPermaLink="false">2508.02808v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision</title>
      <link>http://arxiv.org/abs/2508.03177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对大型视觉-语言模型在风格化图像中出现的幻觉问题，提出了SAVER机制，通过基于标记级视觉注意力模式的动态调整，有效减轻了风格化图像引发的幻觉现象。&lt;h4&gt;背景&lt;/h4&gt;大型视觉-语言模型在理解复杂的视觉-文本上下文方面取得了重大突破，但幻觉问题仍然限制了它们的实际应用能力。现有方法主要关注照片图像中的幻觉问题，忽视了风格化图像在游戏场景理解、艺术教育和医疗分析等关键场景中的潜在风险。&lt;h4&gt;目的&lt;/h4&gt;研究风格化图像对LVLMs幻觉现象的影响，并提出有效的缓解方法。&lt;h4&gt;方法&lt;/h4&gt;首先构建了一个包含照片图像及其对应风格化版本的标注数据集，然后对13个先进的LVLMs进行判别性和生成性任务的比较测试。基于研究结果，提出了Style-Aware Visual Early Revision (SAVER)机制，该机制基于标记级别的视觉注意力模式动态调整LVLMs的最终输出，利用早期层的反馈来减轻风格化图像引起的幻觉。&lt;h4&gt;主要发现&lt;/h4&gt;风格化图像比其照片对应图像更容易引发幻觉现象。SAVER机制在各种模型、数据集和任务中实现了最先进的幻觉缓解性能。&lt;h4&gt;结论&lt;/h4&gt;SAVER作为一种新颖的幻觉缓解机制，能够有效处理风格化图像引发的幻觉问题，提高了LVLMs在关键场景中的可靠性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉-语言模型最近在理解复杂的视觉-文本上下文方面取得了重大突破。然而，幻觉问题仍然限制了它们的实际应用能力。尽管先前的缓解方法能有效减少照片图像中的幻觉，但它们 largely 忽视了风格化图像带来的潜在风险，而这些图像在游戏场景理解、艺术教育和医疗分析等关键场景中扮演着重要角色。在本工作中，我们首先构建了一个包含照片图像及其对应风格化版本的标注数据集。然后，通过在收集的数据集上对13个先进的LVLMs进行判别性和生成性任务的比较测试。我们的研究结果表明，风格化图像比其照片对应图像更容易引发幻觉。为了解决这个问题，我们提出了Style-Aware Visual Early Revision，一种新颖的机制，它基于标记级别的视觉注意力模式动态调整LVLMs的最终输出，利用早期层的反馈来减轻风格化图像引起的幻觉。大量实验表明，SAVER在各种模型、数据集和任务中实现了最先进的幻觉缓解性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Vision-Language Models (LVLMs) recently achieve significantbreakthroughs in understanding complex visual-textual contexts. However,hallucination issues still limit their real-world applicability. Althoughprevious mitigation methods effectively reduce hallucinations in photographicimages, they largely overlook the potential risks posed by stylized images,which play crucial roles in critical scenarios such as game sceneunderstanding, art education, and medical analysis. In this work, we firstconstruct a dataset comprising photographic images and their correspondingstylized versions with carefully annotated caption labels. We then conducthead-to-head comparisons on both discriminative and generative tasks bybenchmarking 13 advanced LVLMs on the collected datasets. Our findings revealthat stylized images tend to induce significantly more hallucinations thantheir photographic counterparts. To address this issue, we propose Style-AwareVisual Early Revision SAVER, a novel mechanism that dynamically adjusts LVLMs'final outputs based on the token-level visual attention patterns, leveragingearly-layer feedback to mitigate hallucinations caused by stylized images.Extensive experiments demonstrate that SAVER achieves state-of-the-artperformance in hallucination mitigation across various models, datasets, andtasks.</description>
      <author>example@mail.com (Zhaoxu Li, Chenqi Kong, Yi Yu, Qiangqiang Wu, Xinghao Jiang, Ngai-Man Cheung, Bihan Wen, Alex Kot, Xudong Jiang)</author>
      <guid isPermaLink="false">2508.03177v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary Constructions</title>
      <link>http://arxiv.org/abs/2508.03173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了Geoint-R1多模态推理框架和Geoint基准测试，解决了现有模型在形式几何推理中的困难，特别是在动态构建和验证辅助几何元素方面。Geoint-R1能够从文本描述和视觉图表生成可形式验证的几何解决方案，并在测试中表现优异。&lt;h4&gt;背景&lt;/h4&gt;数学几何推理对科学发现和教育发展至关重要，需要精确的逻辑和严格的正式验证。尽管多模态大语言模型(MLLMs)的最新进展改善了推理任务，但现有模型在形式几何推理方面通常表现不佳，特别是在动态构建和验证辅助几何元素时。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型在形式几何推理方面的挑战，特别是动态构建和验证辅助几何元素方面的困难，开发能够生成可形式验证几何解决方案的框架。&lt;h4&gt;方法&lt;/h4&gt;引入Geoint-R1多模态推理框架，整合辅助元素构建、通过Lean4表示的形式推理和交互式可视化；提出Geoint基准测试，包含1,885个跨平面几何、空间几何和立体几何等不同主题的严格注释几何问题，每个问题包括结构化文本注释、精确Lean4代码和专家验证的详细解决方案步骤。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，Geoint-R1显著优于现有的多模态和特定数学推理模型，特别是在需要明确辅助元素构建的挑战性问题上。&lt;h4&gt;结论&lt;/h4&gt;Geoint-R1框架和Geoint基准为形式几何推理提供了有效的解决方案和评估工具，推动了该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;数学几何推理对于科学发现和教育发展至关重要，需要精确的逻辑和严格的正式验证。虽然多模态大语言模型(MLLMs)的最新进展已经改善了推理任务，但现有模型通常在形式几何推理方面表现不佳，特别是在动态构建和验证辅助几何元素时。为了解决这些挑战，我们引入了Geoint-R1，这是一个多模态推理框架，旨在从文本描述和视觉图表中生成可形式验证的几何解决方案。Geoint-R1独特地集成了辅助元素构建、通过Lean4表示的形式推理和交互式可视化。为了系统评估和推进形式几何推理，我们提出了Geoint基准，包含1,885个跨平面几何、空间几何和立体几何等不同主题的严格注释几何问题。每个问题包括结构化文本注释、用于辅助构造的精确Lean4代码以及专家验证的详细解决方案步骤。大量实验表明，Geoint-R1显著优于现有的多模态和特定数学推理模型，特别是在需要明确辅助元素构建的挑战性问题上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mathematical geometric reasoning is essential for scientific discovery andeducational development, requiring precise logic and rigorous formalverification. While recent advances in Multimodal Large Language Models (MLLMs)have improved reasoning tasks, existing models typically struggle with formalgeometric reasoning, particularly when dynamically constructing and verifyingauxiliary geometric elements. To address these challenges, we introduceGeoint-R1, a multimodal reasoning framework designed to generate formallyverifiable geometric solutions from textual descriptions and visual diagrams.Geoint-R1 uniquely integrates auxiliary elements construction, formal reasoningrepresented via Lean4, and interactive visualization. To systematicallyevaluate and advance formal geometric reasoning, we propose the Geointbenchmark, comprising 1,885 rigorously annotated geometry problems acrossdiverse topics such as plane, spatial, and solid geometry. Each problemincludes structured textual annotations, precise Lean4 code for auxiliaryconstructions, and detailed solution steps verified by experts. Extensiveexperiments demonstrate that Geoint-R1 significantly surpasses existingmultimodal and math-specific reasoning models, particularly on challengingproblems requiring explicit auxiliary element constructions.</description>
      <author>example@mail.com (Jingxuan Wei, Caijun Jia, Qi Chen, Honghao He, Linzhuang Sun, Conghui He, Lijun Wu, Bihui Yu, Cheng Tan)</author>
      <guid isPermaLink="false">2508.03173v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2508.03060v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CHARM框架，一种新颖的互补学习框架，用于实现跨模态的语义分割。CHARM通过隐式对齐内容同时保留各模态的独特优势，避免了传统方法中模态同质化的问题。框架包含相互感知单元(MPU)和双路径优化策略两个核心组件，在多个数据集和骨干网络上均表现出色，特别是在脆弱模态上有显著提升。&lt;h4&gt;背景&lt;/h4&gt;现有的模态无关语义分割方法通常依赖显式特征对齐来实现模态同质化，这种方法削弱了各模态的独特优势并破坏了它们内在的互补性。&lt;h4&gt;目的&lt;/h4&gt;实现跨模态的协同和谐化而非同质化，保留各模态的特定优势，同时实现内容层面的隐式对齐。&lt;h4&gt;方法&lt;/h4&gt;提出CHARM框架，包含两个核心组件：(1)相互感知单元(MPU)，通过基于窗口的跨模态交互实现隐式对齐，使各模态互为查询和上下文；(2)双路径优化策略，将训练解耦为互补融合学习的协作学习策略(CoL)和保护模态特定优化的个体增强策略(InE)。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集和骨干网络上的实验表明，CHARM框架始终优于基线方法，特别是在脆弱模态上有显著提升。&lt;h4&gt;结论&lt;/h4&gt;本研究将重点从模型同质化转向和谐化，实现了真正的跨模态互补，为多样化模态的真正和谐提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;模态无关的语义分割(MaSS)旨在实现跨越任意输入模态组合的鲁棒场景理解。现有方法通常依赖显式特征对齐来实现模态同质化，这削弱了各模态的独特优势并破坏了它们内在的互补性。为了实现协同和谐化而非同质化，我们提出了CHARM，一种新颖的互补学习框架，通过两个组件设计来隐式对齐内容同时保留模态特定优势：(1)相互感知单元(MPU)，通过基于窗口的跨模态交互实现隐式对齐，使各模态互为查询和上下文，以发现模态交互对应关系；(2)双路径优化策略，将训练解耦为互补融合学习的协作学习策略(CoL)和保护模态特定优化的个体增强策略(InE)。在多个数据集和骨干网络上的实验表明，CHARM始终优于基线方法，在脆弱模态上有显著提升。本研究将重点从模型同质化转向和谐化，实现了真正的跨模态互补，实现了多样化中的真正和谐。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决模态无关语义分割（MaSS）中现有方法通过显式特征对齐实现模态同质化的问题，这种方法会稀释各模态的独特优势并破坏其互补性。这个问题在现实中非常重要，因为不同传感器模态（如RGB、LiDAR、深度等）在恶劣环境（如雨天、夜间）下各有优势和局限性，保留各模态独特特性并实现有效互补对于自动驾驶、监控等安全关键应用中的鲁棒场景理解至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有MaSS方法的局限性：显式特征对齐导致模态同质化，削弱了各模态独特优势。基于两个设计原则（模态应相互理解协调；应积极强化互补特征而非抑制），作者设计了CHARM框架，包含相互感知单元（MPU）实现隐式对齐，以及双路径优化策略平衡协作与个体增强。作者借鉴了现有多模态分割工作（如Swin Transformer的窗口注意力设计）和MaSS方法（如MAGIC、Any2Seg），但针对其局限性提出了新的调和范式而非简单同质化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是实现'协同调和'而非'同质化'，保留模态独特性同时实现互补融合。整体流程包括：1)共享编码器提取各模态特征；2)双路径处理（协作学习CoL和个体增强InE）；3)CoL路径通过鲁棒性加权融合和MPU实现模态互补；4)InE路径通过脆弱模态偏向采样和MPU增强个体模态；5)双路径输出通过分割头预测并计算损失；6)推理时类似CoL路径但直接使用模态特征作为上下文。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)从'同质化'到'调和'的范式转变；2)相互感知单元（MPU）实现隐式对齐；3)双路径优化策略（CoL和InE）；4)鲁棒性引导的模态处理。相比之前工作，CHARM不使用显式对齐约束强制特征一致性，而是通过MPU实现隐式对齐；不倾向于将辅助模态向主模态对齐，而是实现真正的模态间互补；采用双路径优化而非单一路径，在保持协作学习的同时增强个体模态；在脆弱模态组合上性能显著提升（Last-1 mIoU提高超过28%）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出CHARM框架，通过相互感知单元和双路径优化策略实现了跨模态的协同调和而非简单同质化，显著提升了模态无关语义分割在任意模态组合下的性能，特别是在脆弱模态上的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modality-agnostic Semantic Segmentation (MaSS) aims to achieve robust sceneunderstanding across arbitrary combinations of input modality. Existing methodstypically rely on explicit feature alignment to achieve modal homogenization,which dilutes the distinctive strengths of each modality and destroys theirinherent complementarity. To achieve cooperative harmonization rather thanhomogenization, we propose CHARM, a novel complementary learning frameworkdesigned to implicitly align content while preserving modality-specificadvantages through two components: (1) Mutual Perception Unit (MPU), enablingimplicit alignment through window-based cross-modal interaction, wheremodalities serve as both queries and contexts for each other to discovermodality-interactive correspondences; (2) A dual-path optimization strategythat decouples training into Collaborative Learning Strategy (CoL) forcomplementary fusion learning and Individual Enhancement Strategy (InE) forprotected modality-specific optimization. Experiments across multiple datasetsand backbones indicate that CHARM consistently outperform the baselines, withsignificant increment on the fragile modalities. This work shifts the focusfrom model homogenization to harmonization, enabling cross-modalcomplementarity for true harmony in diversity.</description>
      <author>example@mail.com (Lekang Wen, Jing Xiao, Liang Liao, Jiajun Chen, Mi Wang)</author>
      <guid isPermaLink="false">2508.03060v1</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Taking Language Embedded 3D Gaussian Splatting into the Wild</title>
      <link>http://arxiv.org/abs/2507.19830v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Visit our project page at  https://yuzewang1998.github.io/takinglangsplatw/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的框架，用于从不受约束的照片集合中进行开放词汇场景理解，扩展了语言嵌入3D高斯溅射技术，并通过多外观CLIP特征和语言特征不确定性图指导优化过程。作者还提出了临时不确定性感知自动编码器、多外观语言场3DGS表示和后集成策略，并引入了PT-OVS基准数据集进行评估。实验结果表明该方法优于现有方法，支持多种应用。&lt;h4&gt;背景&lt;/h4&gt;利用大规模互联网照片集合进行3D重建的最新进展使得全球地标和历史遗迹的沉浸式虚拟探索成为可能。然而，建筑风格和结构知识的沉浸式理解很少受到关注，主要局限于浏览静态文本图像对，对建筑组件3D结构的沉浸式理解研究不足。&lt;h4&gt;目的&lt;/h4&gt;从3D野外重建技术中汲取灵感，利用不受约束的照片集合创建一种沉浸式方法，用于理解建筑组件的3D结构，开发能够从不受约束照片集合中进行开放词汇场景理解的方法。&lt;h4&gt;方法&lt;/h4&gt;1) 从重建的辐射场中渲染与不受约束图像相同视点的多个外观图像；2) 提取多外观CLIP特征和两种语言特征不确定性图（临时不确定性和外观不确定性）；3) 提出临时不确定性感知自动编码器、多外观语言场3DGS表示和后集成策略；4) 引入PT-OVS基准数据集用于评估不受约束照片集合上的开放词汇分割性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，提出的方法优于现有方法，能够实现准确的开放词汇分割，并支持开放词汇查询的交互式漫游、建筑风格模式识别和3D场景编辑等应用。&lt;h4&gt;结论&lt;/h4&gt;通过扩展语言嵌入3D高斯溅射技术并引入新框架和组件，成功实现了从不受约束照片集合中进行开放词汇场景理解，为建筑组件的3D结构理解提供了新的沉浸式方法。&lt;h4&gt;翻译&lt;/h4&gt;利用大规模互联网照片集合进行3D重建的最新进展使得全球地标和历史遗迹的沉浸式虚拟探索成为可能。然而，很少有人关注建筑风格和结构知识的沉浸式理解，这部分内容主要局限于浏览静态文本图像对。因此，我们能否从3D野外重建技术中汲取灵感，利用不受约束的照片集合创建一种沉浸式方法，用于理解建筑组件的3D结构？为此，我们扩展了语言嵌入3D高斯溅射技术，并提出了一个用于从不受约束照片集合中进行开放词汇场景理解的新框架。具体来说，我们首先从重建的辐射场中渲染与不受约束图像相同视点的多个外观图像，然后提取多外观CLIP特征和两种类型的语言特征不确定性图——临时不确定性和外观不确定性——这些特征来自多外观特征，用于指导后续的优化过程。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从不受约束的照片集合(如互联网收集的照片)中实现开放词汇的场景理解，特别是对建筑结构和风格的沉浸式理解。这个问题在现实中很重要，因为它能让人们以更直观、沉浸式的方式探索历史建筑和地标的结构、风格和历史背景，而不仅仅是浏览静态的文本-图像对。在研究中，这是一个挑战性问题，因为不受约束的照片集合包含各种条件下的图像(不同年份、光照、视角)和临时遮挡物，会导致多视图语言特征不一致，影响3D场景理解的准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：现有3D开放词汇场景理解方法依赖高质量受控拍摄的照片集合，而不受约束照片集合中的外观变化和临时遮挡物会导致多视图CLIP特征不一致；同时CLIP特征的非加性性质使得直接适应野外辐射场重建方法困难。作者借鉴了现有工作，包括3D高斯飞溅(3DGS)技术用于场景表示、野外3D场景重建方法(如WE-GS)处理不受约束照片集合、以及开放词汇场景理解方法(如LangSplat)结合自然语言与3D表示。基于这些，作者设计了一个新框架，通过多外观CLIP特征和不确定性图来处理外观变化和临时遮挡物问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用多外观CLIP特征和不确定性图来处理不受约束照片集合中的外观变化和临时遮挡物，实现开放词汇场景理解。具体通过渲染多外观图像并提取对应的CLIP特征，计算外观不确定性和瞬时不确定性图来指导优化过程。整体流程包括：1)多外观像素级语言特征提取：渲染多外观图像并提取CLIP特征，计算不确定性图；2)瞬时不确定性感知自动编码器：压缩CLIP特征到低维表示；3)多外观语言嵌入3DGS：为每个3D高斯分配多个语言特征并优化；4)后融合和开放词汇查询：融合多外观CLIP特征，应用背景过滤和加权融合，执行开放词汇分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多外观CLIP特征增强策略，通过渲染多外观图像增强语言特征鲁棒性；2)两种不确定性图(外观和瞬时不确定性图)量化语义不确定性；3)瞬时不确定性感知自动编码器有效压缩语言特征；4)多外观语言嵌入3DGS学习多外观CLIP特征；5)后融合策略支持开放词汇查询；6)PT-OVS基准数据集评估不受约束照片集合上的开放词汇分割性能。相比之前工作，不同之处在于：之前的开放词汇3D场景理解方法依赖高质量受控拍摄的照片集合，而野外3D场景重建方法主要关注辐射场重建而非语言场；本文首次将语言嵌入3D高斯飞溅扩展到不受约束照片集合，实现了开放词汇场景理解。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一个从不受约束照片集合中构建语言嵌入3D高斯飞溅的框架，实现了对建筑结构和风格的沉浸式开放词汇理解，并引入了新的基准数据集PT-OVS进行评估。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in leveraging large-scale Internet photo collections for 3Dreconstruction have enabled immersive virtual exploration of landmarks andhistoric sites worldwide. However, little attention has been given to theimmersive understanding of architectural styles and structural knowledge, whichremains largely confined to browsing static text-image pairs. Therefore, can wedraw inspiration from 3D in-the-wild reconstruction techniques and useunconstrained photo collections to create an immersive approach forunderstanding the 3D structure of architectural components? To this end, weextend language embedded 3D Gaussian splatting (3DGS) and propose a novelframework for open-vocabulary scene understanding from unconstrained photocollections. Specifically, we first render multiple appearance images from thesame viewpoint as the unconstrained image with the reconstructed radiancefield, then extract multi-appearance CLIP features and two types of languagefeature uncertainty maps-transient and appearance uncertainty-derived from themulti-appearance features to guide the subsequent optimization process. Next,we propose a transient uncertainty-aware autoencoder, a multi-appearancelanguage field 3DGS representation, and a post-ensemble strategy to effectivelycompress, learn, and fuse language features from multiple appearances. Finally,to quantitatively evaluate our method, we introduce PT-OVS, a new benchmarkdataset for assessing open-vocabulary segmentation performance on unconstrainedphoto collections. Experimental results show that our method outperformsexisting methods, delivering accurate open-vocabulary segmentation and enablingapplications such as interactive roaming with open-vocabulary queries,architectural style pattern recognition, and 3D scene editing.</description>
      <author>example@mail.com (Yuze Wang, Yue Qi)</author>
      <guid isPermaLink="false">2507.19830v2</guid>
      <pubDate>Wed, 06 Aug 2025 14:52:23 +0800</pubDate>
    </item>
    <item>
      <title>Entity Representation Learning Through Onsite-Offsite Graph for Pinterset Ads</title>
      <link>http://arxiv.org/abs/2508.02609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用图神经网络和知识图谱嵌入技术改进广告推荐系统，构建了基于用户线上广告互动和线下转化活动的大规模异构图，提出TransRA模型和基于注意力的KGE微调方法，显著提高了CTR和CVR预测模型性能，并在Pinterest广告系统中实现了2.69%的CTR提升和1.34%的CPC降低。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已广泛应用于工业推荐系统，如GraphSage、TwHIM和LiGNN等模型，这些模型基于用户平台活动构建图结构学习节点嵌入。除线上活动外，用户的线下转化数据对捕捉购物兴趣同样重要。&lt;h4&gt;目的&lt;/h4&gt;更好地利用线下转化数据，探索线上和线下活动间的联系，并将知识图谱嵌入有效整合到广告排序模型中，提高点击率和转化率预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;1) 构建基于用户线上广告互动和线下转化活动的大规模异构图；2) 引入TransRA(带锚点的TransR)模型作为新型知识图谱嵌入方法；3) 采用大型ID嵌入表技术；4) 创新基于注意力的KGE微调方法应用于广告排序模型。&lt;h4&gt;主要发现&lt;/h4&gt;1) 广告排序模型最初难以直接整合知识图谱嵌入，仅观察到适度提升；2) 通过新技术显著提高了CTR和CVR预测模型的AUC；3) 在Pinterest广告系统中部署后，实现了2.69%的CTR提升和1.34%的CPC降低。&lt;h4&gt;结论&lt;/h4&gt;提出的技术可有效应用于大规模工业广告推荐系统，显著提升广告效果，具有广泛的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已广泛应用于工业推荐系统，如GraphSage、TwHIM、LiGNN等模型。在这些工作中，图是基于用户在平台上的活动构建的，各种图模型被开发用于有效学习节点嵌入。除了用户的线上活动外，他们的线下转化对广告模型捕捉购物兴趣也至关重要。为了更好地利用线下转化数据并探索线上和线下活动之间的联系，我们基于用户的线上广告互动和选择参与的线下转化活动构建了一个大规模异构图。此外，我们引入了TransRA(带锚点的TransR)，这是一种新的知识图谱嵌入模型，可以更有效地将图嵌入整合到广告排序模型中。然而，我们的广告排序模型最初难以直接整合知识图谱嵌入，离线实验中只观察到适度的提升。为了解决这一挑战，我们采用了大型ID嵌入表技术，并在广告排序模型中创新了一种基于注意力的KGE微调方法。结果，我们在点击率和转化率预测模型中观察到了显著的AUC提升。此外，该框架已在Pinterest的广告参与模型中部署，并为CTR提升2.69%和CPC降低1.34%做出了贡献。我们相信本文提出的技术可以被其他大规模工业模型所利用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNN) have been extensively applied to industryrecommendation systems, as seen in models like GraphSage\cite{GraphSage},TwHIM\cite{TwHIM}, LiGNN\cite{LiGNN} etc. In these works, graphs wereconstructed based on users' activities on the platforms, and various graphmodels were developed to effectively learn node embeddings. In addition tousers' onsite activities, their offsite conversions are crucial for Ads modelsto capture their shopping interest. To better leverage offsite conversion dataand explore the connection between onsite and offsite activities, weconstructed a large-scale heterogeneous graph based on users' onsite adinteractions and opt-in offsite conversion activities. Furthermore, weintroduced TransRA (TransR\cite{TransR} with Anchors), a novel Knowledge GraphEmbedding (KGE) model, to more efficiently integrate graph embeddings into Adsranking models. However, our Ads ranking models initially struggled to directlyincorporate Knowledge Graph Embeddings (KGE), and only modest gains wereobserved during offline experiments. To address this challenge, we employed theLarge ID Embedding Table technique and innovated an attention based KGEfinetuning approach within the Ads ranking models. As a result, we observed asignificant AUC lift in Click-Through Rate (CTR) and Conversion Rate (CVR)prediction models. Moreover, this framework has been deployed in Pinterest'sAds Engagement Model and contributed to $2.69\%$ CTR lift and $1.34\%$ CPCreduction. We believe the techniques presented in this paper can be leveragedby other large-scale industrial models.</description>
      <author>example@mail.com (Jiayin Jin, Zhimeng Pan, Yang Tang, Jiarui Feng, Kungang Li, Chongyuan Xiang, Jiacheng Li, Runze Su, Siping Ji, Han Sun, Ling Leng, Prathibha Deshikachar)</author>
      <guid isPermaLink="false">2508.02609v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
  <item>
      <title>Adaptive Riemannian Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.02600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了自适应黎曼图神经网络(ARGNN)框架，学习连续且各向异性的黎曼度量张量场，使图神经网络能够适应图数据中复杂的几何异质性，包括树状层次结构和密集社区等不同局部曲率的结构。&lt;h4&gt;背景&lt;/h4&gt;图数据通常表现出复杂的几何异质性，其中具有不同局部曲率的结构（如树状层次结构和密集社区）共存于单个网络中。现有的几何图神经网络将图嵌入到单一固定曲率流形或离散乘积空间中，难以捕捉这种多样性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自适应处理图数据中复杂几何异质性的图神经网络框架，使模型能够根据节点的局部特性确定最优几何表示，从而更好地捕捉图的结构多样性。&lt;h4&gt;方法&lt;/h4&gt;提出自适应黎曼图神经网络(ARGNN)，学习一个连续且各向异性的黎曼度量张量场在图上。核心创新是节点级度量张量的高效参数化，采用可学习的对角形式，在保持计算可处理性的同时捕捉方向几何信息。同时，集成了受里奇流启发的正则化，确保几何规律性和稳定训练。&lt;h4&gt;主要发现&lt;/h4&gt;ARGNN在同类和异类基准数据集上表现出优越性能，能够自适应地捕捉多样化结构。学习到的几何为底层图结构提供了可解释的见解，并 empirically corroborate 理论分析。理论上建立了ARGNN的严格几何演化收敛保证，并提供了统一先前固定或混合曲率GNNs的连续泛化。&lt;h4&gt;结论&lt;/h4&gt;自适应黎曼图神经网络(ARGNN)通过学习连续且各向异性的黎曼度量张量场，有效解决了图数据中复杂几何异质性的建模问题，为图神经网络提供了一种灵活且强大的表示学习方法，具有理论和实验上的优势。&lt;h4&gt;翻译&lt;/h4&gt;图数据通常表现出复杂的几何异质性，其中具有不同局部曲率的结构（如树状层次结构和密集社区）共存于单个网络中。现有的几何图神经网络将图嵌入到单一固定曲率流形或离散乘积空间中，难以捕捉这种多样性。我们引入了自适应黎曼图神经网络(ARGNN)，一种新颖的框架，它在图上学习连续且各向异性的黎曼度量张量场。它允许每个节点确定其最优局部几何，使模型能够流畅地适应图的结构景观。我们的核心创新是节点级度量张量的高效参数化，专门化为可学习的对角形式，在保持计算可处理性的同时捕捉方向几何信息。为确保几何规律性和稳定训练，我们集成了受里奇流启发的正则化，平滑学习到的流形。理论上，我们建立了ARGNN的严格几何演化收敛保证，并提供了统一先前固定或混合曲率GNNs的连续泛化。实验上，我们的方法在同类和异类基准数据集上表现出优越性能，能够自适应地捕捉多样化结构。此外，学习到的几何为底层图结构提供了可解释的见解，并 empirically corroborate 我们的理论分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph data often exhibits complex geometric heterogeneity, where structureswith varying local curvature, such as tree-like hierarchies and densecommunities, coexist within a single network. Existing geometric GNNs, whichembed graphs into single fixed-curvature manifolds or discrete product spaces,struggle to capture this diversity. We introduce Adaptive Riemannian GraphNeural Networks (ARGNN), a novel framework that learns a continuous andanisotropic Riemannian metric tensor field over the graph. It allows each nodeto determine its optimal local geometry, enabling the model to fluidly adapt tothe graph's structural landscape. Our core innovation is an efficientparameterization of the node-wise metric tensor, specializing to a learnablediagonal form that captures directional geometric information while maintainingcomputational tractability. To ensure geometric regularity and stable training,we integrate a Ricci flow-inspired regularization that smooths the learnedmanifold. Theoretically, we establish the rigorous geometric evolutionconvergence guarantee for ARGNN and provide a continuous generalization thatunifies prior fixed or mixed-curvature GNNs. Empirically, our methoddemonstrates superior performance on both homophilic and heterophilic benchmarkdatasets with the ability to capture diverse structures adaptively. Moreover,the learned geometries both offer interpretable insights into the underlyinggraph structure and empirically corroborate our theoretical analysis.</description>
      <author>example@mail.com (Xudong Wang, Tongxin Li, Chris Ding, Jicong Fan)</author>
      <guid isPermaLink="false">2508.02600v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction</title>
      <link>http://arxiv.org/abs/2508.02532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为上下文图Transformer (CGT)的混合神经架构，结合图神经网络和Transformer技术，专门用于处理技术、工程文档中的细粒度语法和实体关系问题。该模型在保持参数效率的同时，在检索增强生成管道中表现出色，准确率比GPT-2提高24.7%，参数减少62.4%。&lt;h4&gt;背景&lt;/h4&gt;标准基于Transformer的语言模型虽然对普通文本处理能力强大，但在处理复杂技术、工程文档中的细粒度语法和实体关系方面存在明显局限性。技术领域通常需要具有更强上下文感知和结构感知能力的专业语言模型，而非通用大型模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理技术文档中细粒度语法和实体关系的专业语言模型，同时保持参数效率，适用于特定领域的问答任务。&lt;h4&gt;方法&lt;/h4&gt;提出上下文图Transformer (CGT)，一种混合神经架构，使用顺序、skip-gram和语义相似性边在输入token上构建动态图，通过GATv2Conv层处理局部结构学习，然后将增强的嵌入传递给Transformer编码器以捕获全局依赖。模型采用两阶段训练方法：首先在普通文本上预训练，然后在特定领域的手册上进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在检索增强生成(RAG)管道中集成后，CGT在准确率上比GPT-2高24.7%，同时参数减少了62.4%。这种提升来自于CGT能够联合建模结构token交互和长距离语义连贯性的能力。该模型能够很好地适应技术语言，实现更好的基础、实体跟踪和检索增强响应。&lt;h4&gt;结论&lt;/h4&gt;CGT为技术文档处理提供了一种参数高效的解决方案，通过结合图神经网络和Transformer的优势，能够有效捕获技术文档中的细粒度语法和实体关系，优于通用大型语言模型。&lt;h4&gt;翻译&lt;/h4&gt;标准基于Transformer的语言模型虽然对普通文本很强大，但在处理复杂技术、工程文档中的细粒度语法和实体关系方面往往存在困难。为解决这一问题，我们提出了上下文图Transformer (CGT)，这是一种混合神经架构，结合了图神经网络(GNNs)和Transformer，用于特定领域的问答。CGT使用顺序、skip-gram和语义相似性边在输入token上构建动态图，通过GATv2Conv层处理局部结构学习。然后，这些增强的嵌入被传递给Transformer编码器以捕获全局依赖。与通用大型模型不同，技术领域通常需要具有更强上下文感知和结构感知能力的专业语言模型。CGT为这类用例提供了参数高效的解决方案。集成到检索增强生成(RAG)管道中后，CGT优于GPT-2和BERT等基线模型，准确率比GPT-2高24.7%，同时参数减少62.4%。这种提升来自于CGT能够联合建模结构token交互和长距离语义连贯性的能力。该模型采用两阶段方法从头开始训练：首先在普通文本上预训练，然后在特定领域的手册上进行微调。这突显了CGT对技术语言的适应性，能够在实际应用中实现更好的基础、实体跟踪和检索增强响应。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Standard transformer-based language models, while powerful for general text,often struggle with the fine-grained syntax and entity relationships in complextechnical, engineering documents. To address this, we propose the ContextualGraph Transformer (CGT), a hybrid neural architecture that combines GraphNeural Networks (GNNs) and Transformers for domain-specific question answering.CGT constructs a dynamic graph over input tokens using sequential, skip-gram,and semantic similarity edges, which is processed by GATv2Conv layers for localstructure learning. These enriched embeddings are then passed to a Transformerencoder to capture global dependencies. Unlike generic large models, technicaldomains often require specialized language models with strongercontextualization and structure awareness. CGT offers a parameter-efficientsolution for such use cases. Integrated into a Retrieval-Augmented Generation(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems fromCGTs ability to jointly model structural token interactions and long-rangesemantic coherence. The model is trained from scratch using a two-phaseapproach: pretraining on general text followed by fine-tuning ondomain-specific manuals. This highlights CGTs adaptability to technicallanguage, enabling better grounding, entity tracking, and retrieval-augmentedresponses in real-world applications.</description>
      <author>example@mail.com (Karan Reddy, Mayukha Pal)</author>
      <guid isPermaLink="false">2508.02532v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>On Effectiveness of Graph Neural Network Architectures for Network Digital Twins (NDTs)</title>
      <link>http://arxiv.org/abs/2508.02373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于人工智能的网络数字孪生（AI-NDT）解决方案，利用多层知识图谱架构和图神经网络来预测直接影响用户体验的网络指标。研究评估了四种图神经网络架构，发现GraphTransformer性能最佳，同时其他架构在需要较短训练时间的场景中也能提供可接受的结果。&lt;h4&gt;背景&lt;/h4&gt;未来网络如6G需要支持大量多样化的互联设备和应用，每种都有特定需求。传统网络管理方法已不足够，自动化解决方案变得必要。然而，网络自动化框架容易出错，通常采用需要训练的机器学习技术来优化网络。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于人工智能的网络数字孪生系统，用于在不影响真实网络和用户的情况下模拟、测试和训练AI模型，并预测直接影响用户体验的网络指标。&lt;h4&gt;方法&lt;/h4&gt;采用多层知识图谱架构和图神经网络技术，对四种最突出的图神经网络架构进行评估，并使用RIPE Atlas的公开可用测量数据训练数字孪生系统。&lt;h4&gt;主要发现&lt;/h4&gt;在评估的四种架构中，GraphTransformer表现最佳；在需要较短训练时间的场景中，其他架构可能更适合，同时也能提供可接受的结果；研究结果接近实际应用中的预期效果。&lt;h4&gt;结论&lt;/h4&gt;这项工作预示了主动网络管理可能成为常见实践，提供了一种可扩展且准确的解决方案，符合下一代网络的需求。&lt;h4&gt;翻译&lt;/h4&gt;未来的网络，如6G，将需要支持大量多样的互联设备和应用，每种都有其特定的需求。虽然传统的网络管理方法将足够，但自动化解决方案正变得必不可少。然而，网络自动化框架容易出错，并且通常采用基于机器学习的技术，这些技术需要训练来学习如何优化网络。从这个意义上说，网络数字孪生是一种有用的工具，它允许在不影响真实网络和用户的情况下模拟、测试和训练AI模型。本文提出了一种基于人工智能的网络数字孪生（AI-NDT），它利用多层知识图谱架构和图神经网络来预测直接影响用户体验的网络指标。对四种最突出的图神经网络架构进行了评估，以评估它们在开发网络数字孪生方面的有效性。我们在RIPE Atlas的公开可用测量数据上训练了数字孪生，因此获得了接近实际应用预期结果的结果。结果表明，在评估的四种架构中，GraphTransformer表现最佳。然而，在较短训练时间更重要的场景中，其他架构可能更适合，同时也能提供可接受的结果。这项工作的结果表明，主动网络管理可能成为常见做法，提供了一种符合下一代网络需求的可扩展且准确的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Future networks, such as 6G, will need to support a vast and diverse range ofinterconnected devices and applications, each with its own set of requirements.While traditional network management approaches will suffice, an automatedsolutions are becoming a must. However, network automation frameworks are proneto errors, and often they employ ML-based techniques that require training tolearn how the network can be optimized. In this sense, network digital twinsare a useful tool that allows for the simulation, testing, and training of AImodels without affecting the real-world networks and users. This paper presentsan AI-based Network Digital Twin (AI-NDT) that leverages a multi-layeredknowledge graph architecture and graph neural networks to predict networkmetrics that directly affect the quality of experience of users. An evaluationof the four most prominent Graph Neural Networks (GNN) architectures wasconducted to assess their effectiveness in developing network digital twins. Wetrained the digital twin on publicly available measurement data from RIPEAtlas, therefore obtaining results close to what is expected in real-worldapplications. The results show that among the four architectures evaluated,GraphTransformer presents the best performance. However, other architecturesmight fit better in scenarios where shorter training time is important, whilealso delivering acceptable results. The results of this work are indicative ofwhat might become common practice for proactive network management, offering ascalable and accurate solution aligned with the requirements of thenext-generation networks.</description>
      <author>example@mail.com (Iulisloi Zacarias, Oussama Ben Taarit, Admela Jukan)</author>
      <guid isPermaLink="false">2508.02373v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Skeleton-Guided Learning for Shortest Path Search</title>
      <link>http://arxiv.org/abs/2508.02270v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通用的基于学习的框架，用于在通用图上进行最短路径搜索，无需特定领域特征。该框架通过构建骨架图和骨架图神经网络(SGNN)来学习节点嵌入并预测距离，结合LSearch算法实现高效搜索，并通过分层训练策略扩展到大型图，形成HLSearch方法。&lt;h4&gt;背景&lt;/h4&gt;最短路径搜索是图应用的核心操作，但现有方法存在局限性：经典算法(如Dijkstra和A*)在复杂图中效率低下；基于索引的技术需要大量预处理和存储；基于学习的方法通常局限于空间图并依赖特定领域特征，限制了通用性。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的基于学习的框架，用于在通用图上进行最短路径搜索，不依赖特定领域特征。&lt;h4&gt;方法&lt;/h4&gt;构建骨架图捕获多级距离和跳数信息；使用骨架图神经网络(SGNN)学习节点嵌入并预测节点对间的距离和跳长；开发LSearch算法利用模型驱动剪枝减少搜索空间；引入分层训练策略处理大图，形成HLSearch方法在图分区间进行高效搜索。&lt;h4&gt;主要发现&lt;/h4&gt;在五个不同真实图上的实验表明，该框架在各种图类型上实现了强大的性能，为基于学习的最短路径搜索提供了灵活且有效的解决方案。&lt;h4&gt;结论&lt;/h4&gt;该框架为基于学习的最短路径搜索提供了灵活且有效的解决方案，适用于多种图类型。&lt;h4&gt;翻译&lt;/h4&gt;最短路径搜索是基于图的应用中的核心操作，然而现有方法面临重要的局限性。经典算法如Dijkstra和A*随着图变得更复杂而变得效率低下，而基于索引的技术通常需要大量的预处理和存储。最近的基于学习的方法通常专注于空间图，并依赖于特定于上下文的特征如地理坐标，限制了它们的通用适用性。我们提出了一种通用的基于学习的框架，用于在通用图上进行最短路径搜索，不需要特定领域的特征。我们方法的核心是构建一个骨架图，以紧凑的形式捕获多级距离和跳数信息。一个骨架图神经网络(SGNN)在该结构上操作，学习节点嵌入并预测节点对之间的距离和跳长。这些预测支持LSearch，一种使用模型驱动的剪枝来减少搜索空间同时保持准确性的引导搜索算法。为了处理更大的图，我们引入了一种分层训练策略，将图划分为具有单独训练的SGNN的子图。这种结构支持HLSearch，我们方法的扩展，用于在图分区之间进行高效的路径搜索。在五个不同的真实图上的实验表明，我们的框架在图类型方面实现了强大的性能，为基于学习的最短路径搜索提供了一个灵活且有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Shortest path search is a core operation in graph-based applications, yetexisting methods face important limitations. Classical algorithms such asDijkstra's and A* become inefficient as graphs grow more complex, whileindex-based techniques often require substantial preprocessing and storage.Recent learning-based approaches typically focus on spatial graphs and rely oncontext-specific features like geographic coordinates, limiting their generalapplicability. We propose a versatile learning-based framework for shortestpath search on generic graphs, without requiring domain-specific features. Atthe core of our approach is the construction of a skeleton graph that capturesmulti-level distance and hop information in a compact form. A Skeleton GraphNeural Network (SGNN) operates on this structure to learn node embeddings andpredict distances and hop lengths between node pairs. These predictions supportLSearch, a guided search algorithm that uses model-driven pruning to reduce thesearch space while preserving accuracy. To handle larger graphs, we introduce ahierarchical training strategy that partitions the graph into subgraphs withindividually trained SGNNs. This structure enables HLSearch, an extension ofour method for efficient path search across graph partitions. Experiments onfive diverse real-world graphs demonstrate that our framework achieves strongperformance across graph types, offering a flexible and effective solution forlearning-based shortest path search.</description>
      <author>example@mail.com (Tiantian Liu, Xiao Li, Huan Li, Hua Lu, Christian S. Jensen, Jianliang Xu)</author>
      <guid isPermaLink="false">2508.02270v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>SpikeSTAG: Spatial-Temporal Forecasting via GNN-SNN Collaboration</title>
      <link>http://arxiv.org/abs/2508.02069v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了一种新的脉冲神经网络架构，首次将图结构学习与基于脉冲的时间处理无缝集成，用于多元时间序列预测，显著提高了模型在长期序列数据集上的准确性。&lt;h4&gt;背景&lt;/h4&gt;脉冲神经网络(SNNs)受生物神经元脉冲行为启发，是捕捉时间数据复杂性的独特方法，但其多元时间序列预测中的空间建模潜力尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;填补SNNs在多元时间序列预测中空间建模研究的空白，引入一种新的SNN架构，将图结构学习与基于脉冲的时间处理相结合。&lt;h4&gt;方法&lt;/h4&gt;1) 嵌入时间特征和自适应矩阵，消除预定义图结构需求；2) 通过观察(OBS)块学习序列特征；3) 利用脉冲SAGE层分层聚合邻域信息，实现多跳特征提取且无需浮点运算；4) 提出双路径脉冲融合(DSF)块，通过脉冲门控机制整合空间图特征和时间动态，结合LSTM处理序列与脉冲自注意力输出。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该模型在所有数据集上超越最先进的基于SNN的iSpikformer，在长期预测中优于传统时间模型，为高效时空建模建立了新范式。&lt;h4&gt;结论&lt;/h4&gt;该模型为多元时间序列预测中的空间建模提供了新方法，通过脉冲神经网络和图结构学习的结合，有效提高了长期序列数据集的模型准确性。&lt;h4&gt;翻译&lt;/h4&gt;脉冲神经网络(SNNs)受生物神经元的脉冲行为启发，为捕捉时间数据的复杂性提供了一种独特方法。然而，SNNs在多元时间序列预测中的空间建模潜力在很大程度上仍未被探索。为了弥补这一空白，我们引入了一种全新的SNN架构，这是首批将图结构学习与基于脉冲的时间处理无缝集成用于多元时间序列预测的架构之一。具体来说，我们首先嵌入时间特征和一个自适应矩阵，消除了对预定义图结构的需求。然后，我们通过观察(OBS)块进一步学习序列特征。在此基础上，我们的多尺度脉冲聚合(MSSA)通过脉冲SAGE层分层聚合邻域信息，实现多跳特征提取，同时消除了对浮点运算的需求。最后，我们提出双路径脉冲融合(DSF)块，通过脉冲门控机制整合空间图特征和时间动态，将LSTM处理的序列与脉冲自注意力输出相结合，有效提高了长期序列数据集的模型准确性。实验表明，我们的模型在所有数据集上都超越了最先进的基于SNN的iSpikformer，并在长期预测中优于传统时间模型，从而为高效的时空建模建立了新范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spiking neural networks (SNNs), inspired by the spiking behavior ofbiological neurons, offer a distinctive approach for capturing the complexitiesof temporal data. However, their potential for spatial modeling in multivariatetime-series forecasting remains largely unexplored. To bridge this gap, weintroduce a brand new SNN architecture, which is among the first to seamlesslyintegrate graph structural learning with spike-based temporal processing formultivariate time-series forecasting. Specifically, we first embed timefeatures and an adaptive matrix, eliminating the need for predefined graphstructures. We then further learn sequence features through the Observation(OBS) Block. Building upon this, our Multi-Scale Spike Aggregation (MSSA)hierarchically aggregates neighborhood information through spiking SAGE layers,enabling multi-hop feature extraction while eliminating the need forfloating-point operations. Finally, we propose a Dual-Path Spike Fusion (DSF)Block to integrate spatial graph features and temporal dynamics via aspike-gated mechanism, combining LSTM-processed sequences with spikingself-attention outputs, effectively improve the model accuracy of long sequencedatasets. Experiments show that our model surpasses the state-of-the-artSNN-based iSpikformer on all datasets and outperforms traditional temporalmodels at long horizons, thereby establishing a new paradigm for efficientspatial-temporal modeling.</description>
      <author>example@mail.com (Bang Hu, Changze Lv, Mingjie Li, Yunpeng Liu, Xiaoqing Zheng, Fengzhe Zhang, Wei cao, Fan Zhang)</author>
      <guid isPermaLink="false">2508.02069v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Graph Unlearning via Embedding Reconstruction -- A Range-Null Space Decomposition Approach</title>
      <link>http://arxiv.org/abs/2508.02044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的图神经网络节点遗忘方法，通过嵌入重建和范围-零空间分解实现高效节点遗忘，实验证明该方法达到最先进性能。&lt;h4&gt;背景&lt;/h4&gt;图遗忘是专门为图神经网络设计的，用于处理广泛且多样的图结构遗忘请求，但这一领域在很大程度上尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;避免重新训练的开销，并实现遗忘的模型效用，解决现有方法在节点遗忘方面的挑战。&lt;h4&gt;方法&lt;/h4&gt;通过嵌入重建来反转GNN中的聚合过程，并采用范围-零空间分解进行节点交互学习。&lt;h4&gt;主要发现&lt;/h4&gt;在多个代表性数据集上的实验结果证明了所提出方法的SOTA性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的节点遗忘方法有效解决了现有方法在处理节点遗忘时的局限性，达到了最先进的性能水平。&lt;h4&gt;翻译&lt;/h4&gt;图遗忘专门针对图神经网络设计，用于处理广泛且多样的图结构遗忘请求，但这一领域在很大程度上尚未被探索。GIF（图影响函数）在部分边遗忘方面有效，但在处理更具挑战性的节点遗忘时面临挑战。为了避免重新训练的开销并实现遗忘的模型效用，我们提出了一种新颖的节点遗忘方法，通过嵌入重建来反转GNN中的聚合过程，并采用范围-零空间分解进行节点交互学习。在多个代表性数据集上的实验结果证明了我们提出方法的SOTA性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph unlearning is tailored for GNNs to handle widespread and various graphstructure unlearning requests, which remain largely unexplored. The GIF (graphinfluence function) achieves validity under partial edge unlearning, but faceschallenges in dealing with more disturbing node unlearning. To avoid theoverhead of retraining and realize the model utility of unlearning, we proposeda novel node unlearning method to reverse the process of aggregation in GNN byembedding reconstruction and to adopt Range-Null Space Decomposition for thenodes' interaction learning. Experimental results on multiple representativedatasets demonstrate the SOTA performance of our proposed approach.</description>
      <author>example@mail.com (Hang Yin, Zipeng Liu, Xiaoyong Peng, Liyao Xiang)</author>
      <guid isPermaLink="false">2508.02044v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Flow-Aware GNN for Transmission Network Reconfiguration via Substation Breaker Optimization</title>
      <link>http://arxiv.org/abs/2508.01951v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OptiGridML，一个用于电力网格离散拓扑优化的机器学习框架，通过两阶段神经网络架构替代传统混合整数规划方法，实现了电力出口优化和计算效率的大幅提升。&lt;h4&gt;背景&lt;/h4&gt;电力网格拓扑优化涉及选择变电站断路器配置以最大化跨区域电力出口，传统方法将其表述为混合整数规划问题，但对于大型网络来说，这类问题是NP-hard且计算上难以处理的。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的机器学习方法来解决电力网格离散拓扑优化问题，克服传统混合整数规划方法在大规模网络中的计算瓶颈。&lt;h4&gt;方法&lt;/h4&gt;OptiGridML采用两阶段神经网络架构：1)线图神经网络(LGNN)近似给定网络拓扑的直流电力流；2)异构GNN(HeteroGNN)在结构和物理约束下预测断路器状态；并通过物理信息一致性损失连接这些组件，强制执行基尔霍夫定律。&lt;h4&gt;主要发现&lt;/h4&gt;在具有多达1,000个断路器的合成网络上，OptiGridML实现了比基线拓扑高18%的电力出口改进，同时将推理时间从小时级别减少到毫秒级别。&lt;h4&gt;结论&lt;/h4&gt;结构化、感知流的图神经网络在加速物理网络系统中的组合优化方面具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了OptiGridML，一个用于电力网格离散拓扑优化的机器学习框架。该任务涉及选择变电站断路器配置以最大化跨区域电力出口，这通常被表述为混合整数规划问题，对于大型网络来说是NP-hard且计算上难以处理的。OptiGridML用两阶段神经网络架构替代了重复的MIP求解：一个近似给定网络拓扑直流电力流的线图神经网络(LGNN)，和一个在结构和物理约束下预测断路器状态的异构GNN(HeteroGNN)。物理信息一致性损失通过在预测流上强制执行基尔霍夫定律连接这些组件。在具有多达1,000个断路器的合成网络上的实验表明，OptiGridML实现了比基线拓扑高18%的电力出口改进，同时将推理时间从小时减少到毫秒。这些结果展示了结构化、感知流的GNN在加速物理网络系统中的组合优化的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces OptiGridML, a machine learning framework for discretetopology optimization in power grids. The task involves selecting substationbreaker configurations that maximize cross-region power exports, a problemtypically formulated as a mixed-integer program (MIP) that is NP-hard andcomputationally intractable for large networks. OptiGridML replaces repeatedMIP solves with a two-stage neural architecture: a line-graph neural network(LGNN) that approximates DC power flows for a given network topology, and aheterogeneous GNN (HeteroGNN) that predicts breaker states under structural andphysical constraints. A physics-informed consistency loss connects thesecomponents by enforcing Kirchhoff's law on predicted flows. Experiments onsynthetic networks with up to 1,000 breakers show that OptiGridML achievespower export improvements of up to 18% over baseline topologies, while reducinginference time from hours to milliseconds. These results demonstrate thepotential of structured, flow-aware GNNs for accelerating combinatorialoptimization in physical networked systems.</description>
      <author>example@mail.com (Dekang Meng, Rabab Haider, Pascal van Hentenryck)</author>
      <guid isPermaLink="false">2508.01951v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>From Binary to Continuous: Stochastic Re-Weighting for Robust Graph Explanation</title>
      <link>http://arxiv.org/abs/2508.01925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的迭代解释框架，用于解决图神经网络(GNNs)预测解释中的挑战。通过将模型训练数据分布与解释过程中的加权图分布对齐，该方法提高了解释的鲁棒性，并在多个基准数据集上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在多种图相关学习任务中表现出色，但解释其预测仍然是一个挑战。现有方法主要在加权图上优化软边缘掩码来突出重要子结构，但这些图与GNN训练时使用的无权图存在差异，导致分布偏移，产生不可靠的梯度和低质量的解释，特别是在生成小型稀疏子图时。&lt;h4&gt;目的&lt;/h4&gt;解决训练图和解释图之间的分布不匹配问题，提高GNN预测解释的鲁棒性和质量，同时保持方法的灵活性和通用性。&lt;h4&gt;方法&lt;/h4&gt;提出一种迭代解释框架，交替进行两个阶段：解释子图识别和模型适应。从较大的解释子图开始，在该子图上进行软掩码优化是可靠的。基于此子图，为解释性和非解释性边分配重要性感知的边权重，并在这些加权图上重新训练GNN。这个过程使用逐渐变小的子图重复进行，形成迭代细化过程。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在多个基准数据集上使用不同的GNN骨干网络和解释方法时，一致提高了解释质量。该方法可以灵活地集成到不同的架构中，证明了其通用性和有效性。&lt;h4&gt;结论&lt;/h4&gt;通过迭代对齐训练数据分布和解释过程中的加权图分布，该方法有效解决了GNN解释中的分布偏移问题，提高了解释的鲁棒性和质量，为GNN的解释提供了新的思路和方法。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在广泛的图相关学习任务中取得了显著性能。然而，解释它们的预测仍然是一个具有挑战性的问题，特别是在训练期间使用的图与解释过程中遇到的图之间存在不匹配的情况下。大多数现有方法在加权图上优化软边缘掩码以突出重要子结构，但这些图与GNN训练所用的无权图不同。这种分布偏移导致不可靠的梯度和退化的解释质量，特别是在生成小型稀疏子图时。为了解决这个问题，我们提出了一种新颖的迭代解释框架，通过将模型的训练数据分布与解释过程中出现的加权图分布对齐来提高解释的鲁棒性。我们的方法在两个阶段之间交替：解释子图识别和模型适应。它从一个相对较大的解释子图开始，在该子图上软掩码优化是可靠的。基于此子图，我们为解释性和非解释性边分配重要性感知的边权重，并在这些加权图上重新训练GNN。这个过程使用逐渐变小的子图重复进行，形成迭代细化过程。我们使用不同的GNN骨干网络和解释方法在多个基准数据集上评估了我们的方法。实验结果表明，我们的方法一致提高了解释质量，并且可以灵活地集成到不同的架构中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved remarkable performance in a widerange of graph-related learning tasks. However, explaining their predictionsremains a challenging problem, especially due to the mismatch between thegraphs used during training and those encountered during explanation. Mostexisting methods optimize soft edge masks on weighted graphs to highlightimportant substructures, but these graphs differ from the unweighted graphs onwhich GNNs are trained. This distributional shift leads to unreliable gradientsand degraded explanation quality, especially when generating small, sparsesubgraphs. To address this issue, we propose a novel iterative explanationframework which improves explanation robustness by aligning the model'straining data distribution with the weighted graph distribution appeared duringexplanation. Our method alternates between two phases: explanation subgraphidentification and model adaptation. It begins with a relatively largeexplanation subgraph where soft mask optimization is reliable. Based on thissubgraph, we assign importance-aware edge weights to explanatory andnon-explanatory edges, and retrain the GNN on these weighted graphs. Thisprocess is repeated with progressively smaller subgraphs, forming an iterativerefinement procedure. We evaluate our method on multiple benchmark datasetsusing different GNN backbones and explanation methods. Experimental resultsshow that our method consistently improves explanation quality and can beflexibly integrated with different architectures.</description>
      <author>example@mail.com (Zhuomin Chen, Jingchao Ni, Hojat Allah Salehi, Xu Zheng, Dongsheng Luo)</author>
      <guid isPermaLink="false">2508.01925v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Pi-SAGE: Permutation-invariant surface-aware graph encoder for binding affinity prediction</title>
      <link>http://arxiv.org/abs/2508.01924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Pi-SAGE，一种新型的置换不变表面感知图编码器，通过明确结合蛋白质表面的局部、上下文感知的化学性质，提高了全原子图神经网络在预测蛋白质结合亲和力变化方面的能力。&lt;h4&gt;背景&lt;/h4&gt;当前预测蛋白质结合亲和力变化的最先进模型（如GearBind）都是基于蛋白质结构衍生的全原子几何模型。虽然表面性质可以从蛋白质结构中隐式学习，但明确了解蛋白质表面可能改进预测能力。&lt;h4&gt;目的&lt;/h4&gt;引入Pi-SAGE模型，通过明确利用蛋白质表面特征，提高预测蛋白质复合物之间结合亲和力变化的准确性。&lt;h4&gt;方法&lt;/h4&gt;首先训练Pi-SAGE从蛋白质结构创建表面码本并为表面残基分配标记；然后使用领域自适应的Pi-SAGE表面特征增强GearBind模型的节点特征，在SKEMPI数据集上进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;明确结合残基的局部、上下文感知的化学性质，显著增强了全原子图神经网络在建模野生型和突变体蛋白质间结合亲和力变化方面的预测能力。&lt;h4&gt;结论&lt;/h4&gt;通过明确整合蛋白质表面的局部、上下文感知化学性质，可以有效改进基于结构的模型预测蛋白质结合亲和力变化的能力。&lt;h4&gt;翻译&lt;/h4&gt;蛋白质表面指纹编码了控制蛋白质相互作用的化学和几何特征，可用于预测两个蛋白质复合物之间结合亲和力的变化。当前预测结合亲和力变化的最先进模型（如GearBind）都是基于蛋白质结构衍生的全原子几何模型。虽然表面性质可以从蛋白质结构中隐式学习，但我们假设明确了解蛋白质表面可以改进基于结构的模型预测结合亲和力变化的能力。为此，我们引入了Pi-SAGE，一种新型的置换不变表面感知图编码器。我们首先训练Pi-SAGE直接从结构创建蛋白质表面码本，并为每个表面暴露的残基分配一个标记。接下来，我们使用领域自适应的Pi-SAGE中的表面特征增强GearBind模型的节点特征，以在SKEMPI数据集上预测结合亲和力的变化。我们表明，明确地结合残基的局部、上下文感知的化学性质增强了全原子图神经网络在建模野生型和突变体蛋白质之间结合亲和力变化方面的预测能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein surface fingerprint encodes chemical and geometric features thatgovern protein-protein interactions and can be used to predict changes inbinding affinity between two protein complexes. Current state-of-the-art modelsfor predicting binding affinity change, such as GearBind, are all-atom basedgeometric models derived from protein structures. Although surface propertiescan be implicitly learned from the protein structure, we hypothesize thatexplicit knowledge of protein surfaces can improve a structure-based model'sability to predict changes in binding affinity. To this end, we introducePi-SAGE, a novel Permutation-Invariant Surface-Aware Graph Encoder. We firsttrain Pi-SAGE to create a protein surface codebook directly from the structureand assign a token for each surface-exposed residue. Next, we augment the nodefeatures of the GearBind model with surface features from domain-adaptedPi-SAGE to predict binding affinity change on the SKEMPI dataset. We show thatexplicitly incorporating local, context-aware chemical properties of residuesenhances the predictive power of all-atom graph neural networks in modelingbinding affinity changes between wild-type and mutant proteins.</description>
      <author>example@mail.com (Sharmi Banerjee, Mostafa Karimi, Melih Yilmaz, Tommi Jaakkola, Bella Dubrov, Shang Shang, Ron Benson)</author>
      <guid isPermaLink="false">2508.01924v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning Unified System Representations for Microservice Tail Latency Prediction</title>
      <link>http://arxiv.org/abs/2508.01635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为USRFNet的深度学习网络，用于解决微服务架构中性能监控和资源管理的挑战。该网络通过分离和建模流量端与资源端特征，结合图神经网络和gMLP模块，实现了对窗口级P95延迟的高精度预测，在真实微服务基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;微服务架构已成为构建可扩展云原生应用的标准，但其分布式特性在性能监控和资源管理方面带来了重大挑战。传统方法依赖每个请求的延迟指标，这些指标对瞬时噪声高度敏感，无法反映复杂并发工作负载的整体行为。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在处理异构数据和整合互补模态方面的不足，开发一种能够准确预测窗口级P95延迟的系统，以捕捉系统整体趋势和用户感知的性能退化。&lt;h4&gt;方法&lt;/h4&gt;提出USRFNet，一种深度学习网络，使用图神经网络(GNNs)捕获服务交互和请求传播模式，同时使用gMLP模块独立建模集群资源动态。这些表示被融合为统一的系统嵌入，用于预测窗口级P95延迟。&lt;h4&gt;主要发现&lt;/h4&gt;现有方法在处理异构数据（流量端特征传播和资源端局部瓶颈）和整合互补模态方面存在不足。USRFNet通过明确分离和建模流量端与资源端特征，实现了比现有基线更准确的预测性能。&lt;h4&gt;结论&lt;/h4&gt;USRFNet有效解决了微服务架构中性能监控的挑战，在大规模压力测试条件下展现出显著的预测准确性提升，为复杂分布式系统的性能优化提供了有力工具。&lt;h4&gt;翻译&lt;/h4&gt;微服务架构已成为构建可扩展云原生应用的事实标准，但其分布式特性在性能监控和资源管理方面带来了重大挑战。传统方法通常依赖每个请求的延迟指标，这些指标对瞬时噪声高度敏感，无法反映复杂并发工作负载的整体行为。相比之下，窗口级P95尾部延迟提供了稳定且有意义的信号，既能捕捉系统级趋势，又能反映用户感知的性能退化。我们确定了现有方法的两个关键缺陷：(i) 对异构数据处理不足，其中流量端特征在服务依赖关系间传播，资源端信号反映局部瓶颈；(ii) 缺乏有原则的架构设计，无法有效区分和整合这些互补模态。为解决这些挑战，我们提出了USRFNet，一种深度学习网络，明确分离并建模流量端和资源端特征。USRFNet使用图神经网络捕获服务交互和请求传播模式，同时使用gMLP模块独立建模集群资源动态。这些表示随后被融合为统一的系统嵌入，以高精度预测窗口级P95延迟。我们在大规模压力测试条件下的真实微服务基准上评估了USRFNet，展示了与最先进基线相比预测准确性的显著提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Microservice architectures have become the de facto standard for buildingscalable cloud-native applications, yet their distributed nature introducessignificant challenges in performance monitoring and resource management.Traditional approaches often rely on per-request latency metrics, which arehighly sensitive to transient noise and fail to reflect the holistic behaviorof complex, concurrent workloads. In contrast, window-level P95 tail latencyprovides a stable and meaningful signal that captures both system-wide trendsand user-perceived performance degradation. We identify two key shortcomings inexisting methods: (i) inadequate handling of heterogeneous data, wheretraffic-side features propagate across service dependencies and resource-sidesignals reflect localized bottlenecks, and (ii) the lack of principledarchitectural designs that effectively distinguish and integrate thesecomplementary modalities. To address these challenges, we propose USRFNet, adeep learning network that explicitly separates and models traffic-side andresource-side features. USRFNet employs GNNs to capture service interactionsand request propagation patterns, while gMLP modules independently modelcluster resource dynamics. These representations are then fused into a unifiedsystem embedding to predict window-level P95 latency with high accuracy. Weevaluate USRFNet on real-world microservice benchmarks under large-scale stresstesting conditions, demonstrating substantial improvements in predictionaccuracy over state-of-the-art baselines.</description>
      <author>example@mail.com (Wenzhuo Qian, Hailiang Zhao, Tianlv Chen, Jiayi Chen, Ziqi Wang, Kingsum Chow, Shuiguang Deng)</author>
      <guid isPermaLink="false">2508.01635v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning for the Elementary Shortest Path Problem</title>
      <link>http://arxiv.org/abs/2508.01557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于概率方法和无监督图神经网络的解决方案，用于解决存在负成本环时的基本最短路径问题(ESPP)，该方法在实验中表现出色并具有良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;基本最短路径问题(ESPP)旨在寻找从起点s到终点t的最小成本路径，且每个顶点最多访问一次。当图中存在负成本环时，该问题变为NP难问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效求解存在负成本环的ESPP问题的方法，特别是寻找近似最优解。&lt;h4&gt;方法&lt;/h4&gt;提出一种概率方法，使用无监督图神经网络联合学习节点价值估计和边选择概率，通过代理损失函数实现，该损失函数可减少负成本环并保证算法一致性。在推理阶段，使用解码算法将学习到的边概率转换为基本路径。&lt;h4&gt;主要发现&lt;/h4&gt;在最多100个节点的图上进行的实验表明，该方法优于无监督基线和经典启发式算法，并且在未见过的合成图上跨尺寸和跨拓扑泛化方面表现出高性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于图神经网络的概率方法为解决NP难的基本最短路径问题提供了一种有效途径，特别是在处理负成本环时，具有良好的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;基本最短路径问题(ESPP)寻求从s到t的最小成本路径，且每个顶点最多访问一次。负成本环的存在使问题变得NP难。我们提出了一种寻找近似最优ESPP的概率方法，该方法通过一个无监督图神经网络实现，该网络通过代理损失函数联合学习节点价值估计和边选择概率。该损失函数通过同时减少负成本环和嵌入所需的算法一致性，为找到近似最优ESPP解提供了高概率保证。在推理时，解码算法将学习到的边概率转换为基本路径。在最多100个节点的图上的实验表明，所提出的方法优于无监督基线和经典启发式算法，并且在未见过的合成图上跨尺寸和跨拓扑泛化方面表现出高性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Elementary Shortest-Path Problem(ESPP) seeks a minimum cost path from sto t that visits each vertex at most once. The presence of negative-cost cyclesrenders the problem NP-hard. We present a probabilistic method for findingnear-optimal ESPP, enabled by an unsupervised graph neural network that jointlylearns node value estimates and edge-selection probabilities via a surrogateloss function. The loss provides a high probability certificate of findingnear-optimal ESPP solutions by simultaneously reducing negative-cost cycles andembedding the desired algorithmic alignment. At inference time, a decodingalgorithm transforms the learned edge probabilities into an elementary path.Experiments on graphs of up to 100 nodes show that the proposed methodsurpasses both unsupervised baselines and classical heuristics, whileexhibiting high performance in cross-size and cross-topology generalization onunseen synthetic graphs.</description>
      <author>example@mail.com (Jingyi Chen, Xinyuan Zhang, Xinwu Qian)</author>
      <guid isPermaLink="false">2508.01557v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Sensitivity to $HH\to b\bar{b} γγ$ with Graph Neural Networks and XGBoost</title>
      <link>http://arxiv.org/abs/2508.01449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了使用先进机器学习技术提高双希格斯玻色子搜索的灵敏度，比较了XGBoost和GNN两种模型，发现GNN模型显著提高了双希格斯玻色子产生截面的上限约束。&lt;h4&gt;背景&lt;/h4&gt;在希格斯玻色子物理研究中，双希格斯玻色子搜索对于理解希格斯自耦合至关重要，但传统方法在HH → b̄bγγ衰变通道中的灵敏度有限。&lt;h4&gt;目的&lt;/h4&gt;提高双希格斯玻色子搜索的灵敏度，特别是在HH → b̄bγγ衰变通道中，以改善对双希格斯玻色子产生截面上限的约束和对希格斯玻色子自耦合(κλ)的测量精度。&lt;h4&gt;方法&lt;/h4&gt;实现了两种机器学习模型：基于树的XGBoost分类器和基于几何的图神经网络分类器(GNN)，并在√s = 13.6 TeV条件下进行性能比较。&lt;h4&gt;主要发现&lt;/h4&gt;几何模型(GNN)优于传统的XGBoost分类器，将双希格斯玻色子产生截面的95%置信水平上限提高了28%，与最新的ATLAS实验结果相比，在上限和希格斯玻色子自耦合(κλ)约束方面均有显著改进。&lt;h4&gt;结论&lt;/h4&gt;基于几何的图神经网络模型在双希格斯玻色子搜索中表现优异，能有效提高实验数据的分析灵敏度，为粒子物理研究提供更精确的测量结果。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们探讨了使用先进的机器学习技术来提高在13.6 TeV条件下，双希格斯玻色子在HH → b̄bγγ衰变通道中的搜索灵敏度。实现了并比较了两种机器学习模型：使用XGBoost的基于树的分类器和使用几何方法的图神经网络分类器。我们证明几何模型优于传统的XGBoost分类器，将双希格斯玻色子产生截面的95%置信水平上限提高了28%。我们的结果与最新的ATLAS实验结果进行了比较，显示出在上限和希格斯玻色子自耦合约束方面的显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we explore the use of advanced machine learning (ML)techniques to enhance the sensitivity of double Higgs boson searches in the \(HH \to b\bar{b}\gamma\gamma \) decay channel at $\sqrt{s} = $ 13.6 TeV. Two MLmodels are implemented and compared: a tree-based classifier using XGBoost, anda geometrical-based graph neural network classifier (GNN). We show that thegeometrical model outperform the traditional XGBoost classifier improving theexpected 95\% CL upper limit on the double Higgs boson production cross-sectionby 28\%. Our results are compared to the latest ATLAS experiment results,showing significant improvement of both upper limit and Higgs bosonself-coupling ($\kappa_{\lambda}$) constraints.</description>
      <author>example@mail.com (Mohamed Belfkir, Mohamed Amin Loualidi, Salah Nasri)</author>
      <guid isPermaLink="false">2508.01449v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A graph neural network based on feature network for identifying influential nodes</title>
      <link>http://arxiv.org/abs/2508.01278v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为FNGCN的新型图卷积网络框架，用于解决复杂网络中重要节点识别问题，通过特征网络表示局部中心性间关系，实验证明其性能优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;识别复杂网络中的重要节点非常重要，在电子商务、计算机信息系统等领域有广泛应用。现有方法要么只考虑网络结构的一个方面，要么使用计算成本高的全局中心性作为节点特征，且没有考虑不同中心性之间的关系。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提出一种更准确识别复杂网络中重要节点的框架。&lt;h4&gt;方法&lt;/h4&gt;提出基于特征网络的图卷积网络框架FNGCN，利用特征网络表示局部中心性间的复杂关系，确定最适合的局部中心性。开发了浅层和深层两种FNGCN模型，使用SIR模型获得的真实数据在多个真实网络中与最先进方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，两种FNGCN模型比比较的方法能更准确地识别重要节点，证明所提框架在识别复杂网络重要节点方面有效。&lt;h4&gt;结论&lt;/h4&gt;FNGCN框架通过考虑局部中心性之间的关系，解决了现有方法的局限性，能够更准确地识别复杂网络中的重要节点。&lt;h4&gt;翻译&lt;/h4&gt;识别复杂网络中的重要节点非常重要，并且在实践中有很多应用。例如，在电子商务网络中找到重要节点可以为商家提供购买意愿强的客户；在计算机信息系统中识别重要节点可以帮助定位导致系统崩溃的组件；识别这些网络中的重要节点可以加速网络中的信息流动。因此，人们已经做了大量努力来解决识别重要节点的问题。然而，之前的研究要么只考虑网络结构的一个方面，要么使用计算成本高的全局中心性作为节点特征来识别重要节点，且现有方法没有考虑不同中心性之间的关系。为了解决这些问题，我们提出了一个基于特征网络的图卷积网络框架，简称为FNGCN。此外，为了排除噪声和减少冗余，FNGCN利用特征网络来表示局部中心性之间的复杂关系，基于此确定最适合的局部中心性。通过将浅层GCN和深层GCN纳入FNGCN框架，开发了两种FNGCN。使用广泛使用的易感染-感染-恢复模型获得的真实数据，在几个真实网络中将这两种FNGCN与最先进的方法进行比较。实验结果表明，这两种FNGCN比比较的方法能更准确地识别重要节点，这表明所提出的框架在识别复杂网络中的重要节点方面是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying influential nodes in complex networks is of great importance, andhas many applications in practice. For example, finding influential nodes ine-commerce network can provide merchants with customers with strong purchaseintent; identifying influential nodes in computer information system can helplocating the components that cause the system break down and identifyinginfluential nodes in these networks can accelerate the flow of information innetworks. Thus, a lot of efforts have been made on the problem of indentifyinginfluential nodes. However, previous efforts either consider only one aspect ofthe network structure, or using global centralities with high time consuming asnode features to identify influential nodes, and the existing methods do notconsider the relationships between different centralities. To solve theseproblems, we propose a Graph Convolutional Network Framework based on FeatureNetwork, abbreviated as FNGCN (graph convolutional network is abbreviated asGCN in the following text). Further, to exclude noises and reduce redundency,FNGCN utilizes feature network to represent the complicated relationships amongthe local centralities, based on which the most suitable local centralities aredetermined. By taking a shallow GCN and a deep GCN into the FNGCN framework,two FNGCNs are developed. With ground truth obtained from the widely usedSusceptible Infected Recovered (SIR) model, the two FNGCNs are compared withthe state-of-art methods on several real-world networks. Experimental resultsshow that the two FNGCNs can identify the influential nodes more accuratelythan the compared methods, indicating that the proposed framework is effectivein identifying influential nodes in complex networks.</description>
      <author>example@mail.com (Yanmei Hu, Siyuan Yin, Yihang Wu, Xue Yue, Yue Liu)</author>
      <guid isPermaLink="false">2508.01278v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>RelMap: Reliable Spatiotemporal Sensor Data Visualization via Imputative Spatial Interpolation</title>
      <link>http://arxiv.org/abs/2508.01240v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 14 figures, paper accepted to IEEE VIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种新的空间插值流程，结合图神经网络和特殊的编码技术，能够产生可靠的结果并有效传达不确定性信息。&lt;h4&gt;背景&lt;/h4&gt;准确可靠地可视化时空传感器数据（如环境参数和气象条件）对于决策制定至关重要。然而，传统空间插值方法由于传感器覆盖有限且不规则，往往无法产生可靠的插值结果。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的空间插值流程，实现可靠的插值结果，并生成一种包含不确定性信息的新型热图表示。&lt;h4&gt;方法&lt;/h4&gt;利用图神经网络(GNNs)的插值参考数据提高可视化可靠性和时间分辨率；集成主邻域聚合(PNA)和地理位置编码(GPE)使模型有效学习时空依赖关系；提出一种基于插值热图的外部静态可视化技术，有效传达插值图中各种来源的不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;通过一系列用例、真实世界数据集的广泛评估和用户研究，证明了模型在数据插值方面的优越性能；参考数据改善了插值结果；可视化设计在传达不确定性方面有效。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效解决传统空间插值方法的局限性，提供更可靠的数据可视化和不确定性表达。&lt;h4&gt;翻译&lt;/h4&gt;准确可靠地可视化时空传感器数据（如环境参数和气象条件）对于明智的决策制定至关重要。然而，由于传感器覆盖有限且不规则，传统的空间插值方法往往无法产生可靠的插值结果。本文介绍了一种新颖的空间插值流程，能够实现可靠的插值结果，并生成一种编码了不确定性信息的新型热图表示。我们利用图神经网络(GNNs)的插值参考数据来提高可视化可靠性和时间分辨率。通过集成主邻域聚合(PNA)和地理位置编码(GPE)，我们的模型能够有效学习时空依赖关系。此外，我们提出了一种基于插值热图的外部静态可视化技术，能够有效传达插值图中各种来源产生的不确定性。通过一系列用例、真实世界数据集的广泛评估和用户研究，我们证明了模型在数据插值方面的优越性能，参考数据对插值的改进效果，以及我们的可视化设计在传达不确定性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and reliable visualization of spatiotemporal sensor data such asenvironmental parameters and meteorological conditions is crucial for informeddecision-making. Traditional spatial interpolation methods, however, often fallshort of producing reliable interpolation results due to the limited andirregular sensor coverage. This paper introduces a novel spatial interpolationpipeline that achieves reliable interpolation results and produces a novelheatmap representation with uncertainty information encoded. We leverageimputation reference data from Graph Neural Networks (GNNs) to enhancevisualization reliability and temporal resolution. By integrating PrincipalNeighborhood Aggregation (PNA) and Geographical Positional Encoding (GPE), ourmodel effectively learns the spatiotemporal dependencies. Furthermore, wepropose an extrinsic, static visualization technique for interpolation-basedheatmaps that effectively communicates the uncertainties arising from varioussources in the interpolated map. Through a set of use cases, extensiveevaluations on real-world datasets, and user studies, we demonstrate ourmodel's superior performance for data imputation, the improvements to theinterpolant with reference data, and the effectiveness of our visualizationdesign in communicating uncertainties.</description>
      <author>example@mail.com (Juntong Chen, Huayuan Ye, He Zhu, Siwei Fu, Changbo Wang, Chenhui Li)</author>
      <guid isPermaLink="false">2508.01240v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Oldie but Goodie: Re-illuminating Label Propagation on Graphs with Partially Observed Features</title>
      <link>http://arxiv.org/abs/2508.01209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GOODIE是一种结合标签传播和特征传播的混合框架，通过GNN解码器、结构-特征注意力和伪标签对比学习，有效解决了现实世界中图数据特征缺失的问题。&lt;h4&gt;背景&lt;/h4&gt;在现实世界的图中，经常遇到节点特征缺失的情况，如敏感信息缺失。直接使用图神经网络会导致下游任务次优结果，而当只有少量特征可用时，现有的基于GNN的方法表现甚至不如传统基于结构的模型。&lt;h4&gt;目的&lt;/h4&gt;提出一个新框架，进一步挖掘经典标签传播的潜力，特别是在只有部分特征可用时，有效利用特征传播。&lt;h4&gt;方法&lt;/h4&gt;GOODIE框架采用混合方法从标签传播分支和特征传播分支获取嵌入：设计基于GNN的解码器使两分支输出对齐；利用结构-特征注意力自动捕获结构和特征信息重要性；采用伪标签对比学习区分不同正样本贡献；最终输出未标记节点的预测。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验证明，GOODIE模型不仅只在少量特征可用时表现优异，而且在特征丰富的情况下也优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;GOODIE模型能有效处理图数据中的节点特征缺失问题，特别是在特征有限的情况下表现突出。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界的图中，我们经常遇到特征缺失的情况，其中少数或大部分节点特征（如敏感信息）缺失。在这种情况下，直接使用图神经网络会导致下游任务（如节点分类）的次优结果。尽管已经出现了一些基于GNN的方法试图缓解这种缺失情况，但当只有少量特征可用时，它们的表现反而不如传统的基于结构的模型。为此，我们提出了一个新框架，进一步挖掘经典标签传播的潜力，利用特征传播，特别是在只有部分特征可用的情况下。现在称为GOODIE，它采用混合方法从标签传播分支和特征传播分支获取嵌入。为此，我们首先设计了一个基于GNN的解码器，使标签传播分支能够输出与特征传播分支对齐的隐藏嵌入。然后，GOODIE通过新设计的结构-特征注意力自动捕获结构和特征信息的重要性。接着是一种新的伪标签对比学习，它区分来自标签传播分支的伪标签中每对正样本的贡献，GOODIE输出未标记节点的最终预测。通过大量实验，我们证明所提出的GOODIE模型不仅在只有少量特征可用时优于现有的最先进方法，而且在特征丰富的情况下也表现更好。GOODIE的源代码可在以下网址获取：https://github.com/SukwonYun/GOODIE。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world graphs, we often encounter missing feature situations where afew or the majority of node features, e.g., sensitive information, are missed.In such scenarios, directly utilizing Graph Neural Networks (GNNs) would yieldsub-optimal results in downstream tasks such as node classification. Despitethe emergence of a few GNN-based methods attempting to mitigate its missingsituation, when only a few features are available, they rather perform worsethan traditional structure-based models. To this end, we propose a novelframework that further illuminates the potential of classical Label Propagation(Oldie), taking advantage of Feature Propagation, especially when only apartial feature is available. Now called by GOODIE, it takes a hybrid approachto obtain embeddings from the Label Propagation branch and Feature Propagationbranch. To do so, we first design a GNN-based decoder that enables the LabelPropagation branch to output hidden embeddings that align with those of the FPbranch. Then, GOODIE automatically captures the significance of structure andfeature information thanks to the newly designed Structure-Feature Attention.Followed by a novel Pseudo-Label contrastive learning that differentiates thecontribution of each positive pair within pseudo-labels originating from the LPbranch, GOODIE outputs the final prediction for the unlabeled nodes. Throughextensive experiments, we demonstrate that our proposed model, GOODIE,outperforms the existing state-of-the-art methods not only when only a fewfeatures are available but also in abundantly available situations. Source codeof GOODIE is available at: https://github.com/SukwonYun/GOODIE.</description>
      <author>example@mail.com (Sukwon Yun, Xin Liu, Yunhak Oh, Junseok Lee, Tianlong Chen, Tsuyoshi Murata, Chanyoung Park)</author>
      <guid isPermaLink="false">2508.01209v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>BSL: A Unified and Generalizable Multitask Learning Platform for Virtual Drug Discovery from Design to Synthesis</title>
      <link>http://arxiv.org/abs/2508.01195v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了Baishenglai(BSL)平台，一个深度学习增强的开放平台，专为虚拟药物发现设计。该平台整合了七个核心任务，采用生成模型和图神经网络等先进技术，在多个基准数据集上达到最先进性能，并对分布外数据有良好泛化能力。实验证明BSL能有效发现具有生物活性的化合物，是加速药物发现的有力工具。&lt;h4&gt;背景&lt;/h4&gt;药物发现对保障人类健康、延长寿命和应对重大疾病挑战具有重要意义。近年来，人工智能在生物信息学和药理学关键任务中展现出显著优势，但现有计算平台存在工作流程碎片化、效率低下、缺乏算法创新以及对分布外数据泛化能力差等问题，阻碍了药物发现的进展。&lt;h4&gt;目的&lt;/h4&gt;开发一个全面、高效且算法创新的平台，解决现有药物发现计算平台的局限性，提高虚拟药物发现的效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出Baishenglai(BSL)平台，这是一个深度学习增强的开放平台。在一个统一和模块化的框架内整合七个核心任务，采用生成模型和图神经网络等先进技术，并强调对OOD分子结构泛化的评估机制。&lt;h4&gt;主要发现&lt;/h4&gt;1) BSL在多个基准数据集上达到了最先进的性能；2) 与现有平台和基线方法的比较实验表明，BSL为虚拟药物发现提供了全面、可扩展且有效的解决方案；3) BSL成功发现了GluN1/GluN3A NMDA受体的新型调节剂，识别出三种具有明确生物活性的化合物，证明了其实用价值。&lt;h4&gt;结论&lt;/h4&gt;BSL是一个有前途的综合平台，既具有算法创新性，又能提供高精度预测，为加速生物医学研究和药物发现提供了有力工具。&lt;h4&gt;翻译&lt;/h4&gt;药物发现对于保障人类健康、延长寿命以及应对重大疾病挑战具有重大的社会意义。近年来，由于其高效的数据处理和数据表示能力，人工智能在生物信息学和药理学的关键任务中展现出了显著优势。然而，大多数现有计算平台仅涵盖部分核心任务，导致工作流程碎片化和效率低下。此外，它们通常缺乏算法创新，对分布外(OOD)数据的泛化能力差，这严重阻碍了药物发现的进展。为解决这些局限性，我们提出了Baishenglai(BSL)，这是一个深度学习增强的开放平台，专为虚拟药物发现设计。BSL在一个统一和模块化的框架内整合了七个核心任务，融入了生成模型和图神经网络等先进技术。除了在多个基准数据集上达到最先进的性能外，该平台还强调关注对OOD分子结构泛化的评估机制。与现有平台和基线方法的比较实验表明，BSL为虚拟药物发现提供了一个全面、可扩展且有效的解决方案，既提供了算法创新，又为现实世界的研究提供了高精度预测。此外，BSL通过发现GluN1/GluN3A NMDA受体的新型调节剂证明了其实用性，在体外电生理学实验中成功识别出三种具有明确生物活性的化合物。这些结果突显了BSL作为加速生物医学研究和药物发现的有前途且全面的平台。平台网址为https://www.baishenglai.net。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug discovery is of great social significance in safeguarding human health,prolonging life, and addressing the challenges of major diseases. In recentyears, artificial intelligence has demonstrated remarkable advantages in keytasks across bioinformatics and pharmacology, owing to its efficient dataprocessing and data representation capabilities. However, most existingcomputational platforms cover only a subset of core tasks, leading tofragmented workflows and low efficiency. In addition, they often lackalgorithmic innovation and show poor generalization to out-of-distribution(OOD) data, which greatly hinders the progress of drug discovery. To addressthese limitations, we propose Baishenglai (BSL), a deep learning-enhanced,open-access platform designed for virtual drug discovery. BSL integrates sevencore tasks within a unified and modular framework, incorporating advancedtechnologies such as generative models and graph neural networks. In additionto achieving state-of-the-art (SOTA) performance on multiple benchmarkdatasets, the platform emphasizes evaluation mechanisms that focus ongeneralization to OOD molecular structures. Comparative experiments withexisting platforms and baseline methods demonstrate that BSL provides acomprehensive, scalable, and effective solution for virtual drug discovery,offering both algorithmic innovation and high-precision prediction forreal-world pharmaceutical research. In addition, BSL demonstrated its practicalutility by discovering novel modulators of the GluN1/GluN3A NMDA receptor,successfully identifying three compounds with clear bioactivity in in-vitroelectrophysiological assays. These results highlight BSL as a promising andcomprehensive platform for accelerating biomedical research and drug discovery.The platform is accessible at https://www.baishenglai.net.</description>
      <author>example@mail.com (Kun Li, Zhennan Wu, Yida Xiong, Hongzhi Zhang, Longtao Hu, Zhonglie Liu, Junqi Zeng, Wenjie Wu, Mukun Chen, Jiameng Chen, Wenbin Hu)</author>
      <guid isPermaLink="false">2508.01195v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Explaining GNN Explanations with Edge Gradients</title>
      <link>http://arxiv.org/abs/2508.01048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络(GNN)解释方法的理论基础，建立了不同解释方法之间的联系，并在输入级和分层设置下进行了分析。&lt;h4&gt;背景&lt;/h4&gt;近年来，图神经网络在图结构数据上取得显著成功，促使了大量解释GNN预测方法的出现。然而，GNN可解释性的最先进方法仍在不断变化，不同比较方法结果不一，许多解释方法在处理复杂GNN架构和任务时存在困难。&lt;h4&gt;目的&lt;/h4&gt;对竞争性的GNN解释方法进行更仔细的理论分析，建立不同解释方法之间的理论联系。&lt;h4&gt;方法&lt;/h4&gt;在两种不同设置下研究GNN解释：输入级解释(产生输入图的解释子图)和分层解释(产生计算图的解释子图)。建立基于扰动的方法和基于梯度的方法之间的理论联系，并分析其他最近提出的方法。&lt;h4&gt;主要发现&lt;/h4&gt;1) 建立了基于扰动和基于梯度的方法之间的第一个理论联系；2) 在输入级，证明了GNNExplainer在某些条件下可以通过基于边梯度符号的简单启发式方法近似；3) 在分层设置中，指出边梯度等价于线性GNN的遮挡搜索；4) 通过实验展示了理论结果在实际数据上的表现。&lt;h4&gt;结论&lt;/h4&gt;该工作提供了对GNN解释方法更深入的理论理解，连接了不同的解释方法，并展示了理论结果在实际中的应用。&lt;h4&gt;翻译&lt;/h4&gt;近年来，图神经网络在图结构数据上的显著成功促使了大量解释GNN预测方法的涌现。然而，GNN可解释性的最先进方法仍在不断变化。不同的比较方法对不同方法的结果不一，许多解释方法在处理更复杂的GNN架构和任务时存在困难。这迫切需要对竞争性的GNN解释方法进行更仔细的理论分析。在本工作中，我们更仔细地研究了两种不同设置下的GNN解释：输入级解释，产生输入图的解释子图；以及分层解释，产生计算图的解释子图。我们建立了流行的基于扰动的方法和经典的基于梯度的方法之间的第一个理论联系，并指出了与其他最近提出的方法之间的联系。在输入级，我们证明了在某些条件下，GNNExplainer可以通过基于边梯度符号的简单启发式方法来近似。在分层设置中，我们指出边梯度等价于线性GNN的遮挡搜索。最后，我们通过在合成和真实数据集上的实验展示了我们的理论结果如何在实际中体现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3711896.3736947&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the remarkable success of graph neural networks (GNNs) ongraph-structured data has prompted a surge of methods for explaining GNNpredictions. However, the state-of-the-art for GNN explainability remains influx. Different comparisons find mixed results for different methods, with manyexplainers struggling on more complex GNN architectures and tasks. Thispresents an urgent need for a more careful theoretical analysis of competingGNN explanation methods. In this work we take a closer look at GNN explanationsin two different settings: input-level explanations, which produce explanatorysubgraphs of the input graph, and layerwise explanations, which produceexplanatory subgraphs of the computation graph. We establish the firsttheoretical connections between the popular perturbation-based and classicalgradient-based methods, as well as point out connections between other recentlyproposed methods. At the input level, we demonstrate conditions under whichGNNExplainer can be approximated by a simple heuristic based on the sign of theedge gradients. In the layerwise setting, we point out that edge gradients areequivalent to occlusion search for linear GNNs. Finally, we demonstrate how ourtheoretical results manifest in practice with experiments on both synthetic andreal datasets.</description>
      <author>example@mail.com (Jesse He, Akbar Rafiey, Gal Mishne, Yusu Wang)</author>
      <guid isPermaLink="false">2508.01048v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions</title>
      <link>http://arxiv.org/abs/2508.02329v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了CLIP-IN框架，通过两种核心创新增强了CLIP模型的细粒度视觉感知能力，包括利用指令编辑数据集作为困难负图像-文本对来源，以及整合长描述性字幕并使用旋转位置编码捕获丰富语义上下文。&lt;h4&gt;背景&lt;/h4&gt;尽管视觉语言模型（如CLIP）在视觉和语言对齐方面取得了成功，但它们在详细、细粒度的视觉理解方面仍面临关键挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新框架来增强CLIP模型的细粒度感知能力，提升其对细微视觉差异的理解。&lt;h4&gt;方法&lt;/h4&gt;首先利用指令编辑数据集作为困难负图像-文本对来源，结合对称困难负对比损失使模型能区分细微的视觉-语义差异；其次整合长描述性字幕，使用旋转位置编码捕获标准CLIP经常错过的丰富语义上下文。&lt;h4&gt;主要发现&lt;/h4&gt;CLIP-IN在MMVP基准和各种细粒度视觉识别任务上取得显著提升，同时不损害在更广泛分类和检索任务上的零样本性能；将CLIP-IN的视觉表示整合到多模态大语言模型中，显著减少了视觉幻觉并提高了推理能力。&lt;h4&gt;结论&lt;/h4&gt;将目标化的指令对比学习与全面的描述信息相结合，对于提升视觉语言模型的细粒度理解具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;尽管视觉语言模型（如CLIP）在视觉和语言对齐方面取得了成功，但它们在详细、细粒度的视觉理解方面仍面临关键挑战。我们提出了CLIP-IN，一个通过两项核心创新增强CLIP细粒度感知能力的新框架。首先，我们利用原本用于图像操作的指令编辑数据集作为困难负图像-文本对的独特来源。结合对称困难负对比损失，这使模型能够有效区分细微的视觉-语义差异。其次，CLIP-IN整合了长描述性字幕，使用旋转位置编码来捕获标准CLIP经常错过的丰富语义上下文。我们的实验表明，CLIP-IN在MMVP基准和各种细粒度视觉识别任务上取得了显著提升，同时不损害在更广泛的分类和检索任务上的零样本鲁棒性能。关键的是，将CLIP-IN的视觉表示整合到多模态大语言模型中显著减少了视觉幻觉并增强了推理能力。这项工作强调了将目标化的指令对比学习与全面的描述信息相结合以提升VLM细粒度理解的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the success of Vision-Language Models (VLMs) like CLIP in aligningvision and language, their proficiency in detailed, fine-grained visualcomprehension remains a key challenge. We present CLIP-IN, a novel frameworkthat bolsters CLIP's fine-grained perception through two core innovations.Firstly, we leverage instruction-editing datasets, originally designed forimage manipulation, as a unique source of hard negative image-text pairs.Coupled with a symmetric hard negative contrastive loss, this enables the modelto effectively distinguish subtle visual-semantic differences. Secondly,CLIP-IN incorporates long descriptive captions, utilizing rotary positionalencodings to capture rich semantic context often missed by standard CLIP. Ourexperiments demonstrate that CLIP-IN achieves substantial gains on the MMVPbenchmark and various fine-grained visual recognition tasks, withoutcompromising robust zero-shot performance on broader classification andretrieval tasks. Critically, integrating CLIP-IN's visual representations intoMultimodal Large Language Models significantly reduces visual hallucinationsand enhances reasoning abilities. This work underscores the considerablepotential of synergizing targeted, instruction-based contrastive learning withcomprehensive descriptive information to elevate the fine-grained understandingof VLMs.</description>
      <author>example@mail.com (Ziteng Wang, Siqi Yang, Limeng Qiao, Lin Ma)</author>
      <guid isPermaLink="false">2508.02329v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image Classification and Segmentation</title>
      <link>http://arxiv.org/abs/2508.02265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in ECAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Hermes是一种创新的半监督双阈值对比学习策略，用于超声图像分类和分割，解决了传统方法中伪标签不准确和任务间信息利用不足的问题。&lt;h4&gt;背景&lt;/h4&gt;基于置信度的伪标签选择通常产生过于自信但不正确的预测，这是由于模型早期的误导性和学习过程中对不准确伪标签的过拟合，严重降低了半监督对比学习的性能。此外，分割和分类任务被独立处理，亲和力无法被充分探索。&lt;h4&gt;目的&lt;/h4&gt;解决现有半监督对比学习在超声图像处理中的局限性，提出一种能够有效利用任务间信息的新方法。&lt;h4&gt;方法&lt;/h4&gt;结合对比学习和半监督学习的优势，让伪标签通过提供额外指导来辅助对比学习；开发任务间注意力和显著性模块促进分割和分类任务间的信息共享；设计任务间一致性学习策略对齐两个任务中的肿瘤特征，避免负迁移以减少特征差异；收集了SZ-TUS甲状腺超声图像数据集解决数据集缺乏问题。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共超声数据集和一个私有数据集上的大量实验表明，Hermes在各种半监督设置下始终优于多种最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;Hermes通过有效解决伪标签不准确和任务间信息利用不足的问题，显著提升了半监督超声图像分类和分割的性能。&lt;h4&gt;翻译&lt;/h4&gt;基于置信度的伪标签选择通常会产生过于自信但不正确的预测，这是由于模型早期的误导性和学习过程中对不准确伪标签的过拟合，这严重降低了半监督对比学习的性能。此外，分割和分类任务被独立处理，亲和力无法被充分探索。为解决这些问题，我们提出了一种用于超声图像分类和分割的新型半监督双阈值对比学习策略，名为Hermes。该策略结合了对比学习和半监督学习的优势，其中伪标签通过提供额外指导来辅助对比学习。具体而言，还开发了任务间注意力和显著性模块，以促进分割和分类任务之间的信息共享。此外，还设计了任务间一致性学习策略，对齐两个任务中的肿瘤特征，避免负迁移以减少特征差异。为解决公开可用超声数据集缺乏的问题，我们收集了SZ-TUS数据集，一个甲状腺超声图像数据集。在两个公共超声数据集和一个私有数据集上的大量实验表明，Hermes在各种半监督设置下始终优于多种最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Confidence-based pseudo-label selection usually generates overly confidentyet incorrect predictions, due to the early misleadingness of model andoverfitting inaccurate pseudo-labels in the learning process, which heavilydegrades the performance of semi-supervised contrastive learning. Moreover,segmentation and classification tasks are treated independently and theaffinity fails to be fully explored. To address these issues, we propose anovel semi-supervised dual-threshold contrastive learning strategy forultrasound image classification and segmentation, named Hermes. This strategycombines the strengths of contrastive learning with semi-supervised learning,where the pseudo-labels assist contrastive learning by providing additionalguidance. Specifically, an inter-task attention and saliency module is alsodeveloped to facilitate information sharing between the segmentation andclassification tasks. Furthermore, an inter-task consistency learning strategyis designed to align tumor features across both tasks, avoiding negativetransfer for reducing features discrepancy. To solve the lack of publiclyavailable ultrasound datasets, we have collected the SZ-TUS dataset, a thyroidultrasound image dataset. Extensive experiments on two public ultrasounddatasets and one private dataset demonstrate that Hermes consistentlyoutperforms several state-of-the-art methods across various semi-supervisedsettings.</description>
      <author>example@mail.com (Peng Zhang, Zhihui Lai, Heng Kong)</author>
      <guid isPermaLink="false">2508.02265v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Marco-Voice Technical Report</title>
      <link>http://arxiv.org/abs/2508.02038v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个多功能语音合成系统Marco-Voice，集成了声音克隆和情感控制功能，在一个统一框架内实现高度表达性、可控性和自然的语音生成。&lt;h4&gt;背景&lt;/h4&gt;长期以来，在实现高度表达性、可控性和自然的语音生成方面存在挑战，特别是在不同语言和情感背景下保持说话人身份的一致性。&lt;h4&gt;目的&lt;/h4&gt;解决在高度表达性、可控性和自然语音生成方面的长期挑战，使生成的语音能在不同语言和情感背景下忠实保留说话人身份。&lt;h4&gt;方法&lt;/h4&gt;引入了有效的说话人-情感解耦机制，使用批次内对比学习实现说话人身份和情感风格的独立操作；开发了旋转情感嵌入集成方法用于平滑情感控制；构建了CSEMOTIONS数据集，包含6位专业说话人的10小时普通话语音，涵盖7种情感类别。&lt;h4&gt;主要发现&lt;/h4&gt;提出的系统Marco-Voice在客观和主观指标上都有显著改进；在语音清晰度和情感丰富度方面具有竞争力；代表了表达性神经语音合成领域的重大进展。&lt;h4&gt;结论&lt;/h4&gt;Marco-Voice系统能够在保持说话人身份的同时，实现高度表达性、可控性和自然的语音生成，是表达性神经语音合成领域的重要突破。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文提出了一个多功能语音合成系统，该系统在一个统一的框架内集成了声音克隆和情感控制语音合成。这项工作的目标是解决在实现高度表达性、可控性和自然的语音生成方面的长期挑战，使生成的语音能在不同语言和情感背景下忠实保留说话人身份。我们的方法引入了一种有效的说话人-情感解耦机制，使用批次内对比学习，能够独立操作说话人身份和情感风格，以及用于平滑情感控制的旋转情感嵌入集成方法。为了支持全面的训练和评估，我们构建了CSEMOTIONS，一个高质量的情感语音数据集，包含来自六位专业说话人的10小时普通话语音，涵盖七种情感类别。大量实验表明，我们的系统Marco-Voice在客观和主观指标上都取得了显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a multifunctional speech synthesis system that integratesvoice cloning and emotion control speech synthesis within a unified framework.The goal of this work is to address longstanding challenges in achieving highlyexpressive, controllable, and natural speech generation that faithfullypreserves speaker identity across diverse linguistic and emotional contexts.Our approach introduces an effective speaker-emotion disentanglement mechanismwith in-batch contrastive learning, enabling independent manipulation ofspeaker identity and eemotional style, as well as rotational emotionalembedding integration method for smooth emotion control. To supportcomprehensive training and evaluation, we construct CSEMOTIONS, a high-qualityemotional speech dataset containing 10 hours of Mandarin speech from sixprofessional speakers across seven emotional categories. Extensive experimentsdemonstrate that our system, Marco-Voice, achieves substantial improvements inboth objective and subjective metrics. Comprehensive evaluations and analysiswere conducted, results show that MarcoVoice delivers competitive performancein terms of speech clarity and emotional richness, representing a substantialadvance in the field of expressive neural speech synthesis.</description>
      <author>example@mail.com (Fengping Tian, Chenyang Lyu, Xuanfan Ni, Haoqin Sun, Qingjuan Li, Zhiqiang Qian, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, Kaifu Zhang)</author>
      <guid isPermaLink="false">2508.02038v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised YOLO: Leveraging Contrastive Learning for Label-Efficient Object Detection</title>
      <link>http://arxiv.org/abs/2508.01966v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了对比自监督学习(SSL)如何减少单阶段目标检测器对大规模标注数据的依赖，通过SimCLR框架预训练YOLO骨干网络，在有限标注数据场景下提升检测性能。&lt;h4&gt;背景&lt;/h4&gt;单阶段目标检测器如YOLO系列在实时视觉应用中取得最先进性能，但训练过程严重依赖大规模标注数据集，限制了其在标注数据有限场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;研究对比自监督学习作为减少对标注数据依赖的方法，通过在未标注图像上预训练YOLO检测器骨干网络，提高模型在标注数据有限情况下的性能。&lt;h4&gt;方法&lt;/h4&gt;提出一种简单有效的流程：将YOLO卷积骨干网络作为编码器，使用全局池化和投影头，利用COCO未标注数据集(12万张图像)的增强图像优化对比损失，然后在标注有限的自行车骑行者检测任务上微调预训练骨干网络。&lt;h4&gt;主要发现&lt;/h4&gt;SSL预训练带来更高的mAP，加快收敛速度，改善精确率-召回率性能，特别是在低标注数据条件下；SimCLR预训练的YOLOv8达到0.7663的mAP@50:95，超过监督学习对应模型，尽管预训练过程中未使用任何标注数据。&lt;h4&gt;结论&lt;/h4&gt;这些发现为将对比SSL应用于单阶段检测器建立了强有力基准，突出了未标注数据作为标注高效目标检测的可扩展资源的潜力。&lt;h4&gt;翻译&lt;/h4&gt;单阶段目标检测器如YOLO系列在实时视觉应用中取得了最先进的性能，但在训练过程中仍然严重依赖大规模标注数据集。在这项工作中，我们提出了对对比自监督学习(SSL)的系统性研究，作为一种通过使用SimCLR框架在未标注图像上预训练YOLOv5和YOLOv8骨干网络来减少这种依赖的方法。我们的方法引入了一个简单而有效的流程，将YOLO的卷积骨干网络作为编码器，采用全局池化和投影头，并使用COCO未标注数据集(12万张图像)的增强图像优化对比损失。然后在标注有限的自行车骑行者检测任务上对预训练骨干网络进行微调。实验结果表明，SSL预训练 consistently带来更高的mAP，更快的收敛速度，以及改善的精确率-召回率性能，特别是在低标注数据条件下。例如，我们的SimCLR预训练的YOLOv8达到了0.7663的mAP@50:95，超过了其监督学习对应模型，尽管在预训练过程中没有使用任何标注数据。这些发现为将对比SSL应用于单阶段检测器建立了强有力的基准，并突出了未标注数据作为标注高效目标检测的可扩展资源的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One-stage object detectors such as the YOLO family achieve state-of-the-artperformance in real-time vision applications but remain heavily reliant onlarge-scale labeled datasets for training. In this work, we present asystematic study of contrastive self-supervised learning (SSL) as a means toreduce this dependency by pretraining YOLOv5 and YOLOv8 backbones on unlabeledimages using the SimCLR framework. Our approach introduces a simple yeteffective pipeline that adapts YOLO's convolutional backbones as encoders,employs global pooling and projection heads, and optimizes a contrastive lossusing augmentations of the COCO unlabeled dataset (120k images). The pretrainedbackbones are then fine-tuned on a cyclist detection task with limited labeleddata. Experimental results show that SSL pretraining leads to consistentlyhigher mAP, faster convergence, and improved precision-recall performance,especially in low-label regimes. For example, our SimCLR-pretrained YOLOv8achieves a mAP@50:95 of 0.7663, outperforming its supervised counterpartdespite using no annotations during pretraining. These findings establish astrong baseline for applying contrastive SSL to one-stage detectors andhighlight the potential of unlabeled data as a scalable resource forlabel-efficient object detection.</description>
      <author>example@mail.com (Manikanta Kotthapalli, Reshma Bhatia, Nainsi Jain)</author>
      <guid isPermaLink="false">2508.01966v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Multi-Task Learning with Solvent-Aware Augmentation for Drug Discovery</title>
      <link>http://arxiv.org/abs/2508.01799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型预训练方法，通过整合不同溶剂条件下生成的配体构象系综作为增强输入，显著提升了蛋白质-配体相互作用预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确的蛋白质-配体相互作用预测对计算机辅助药物发现至关重要，但现有方法无法捕捉溶剂依赖的构象变化，且缺乏联合学习多个相关任务的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，开发一种能够同时学习结构灵活性和环境背景的预训练方法。&lt;h4&gt;方法&lt;/h4&gt;引入预训练方法，将不同溶剂条件下生成的配体构象系综作为增强输入。训练过程整合了分子重建以捕获局部几何结构、原子间距离预测以建模空间关系、以及对比学习以构建溶剂不变的分子表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法带来显著改进：结合亲和力预测提高3.7%、在PoseBusters Astex对接基准测试中达到82%成功率、虚拟筛选中曲线下面积为97.1%。框架支持溶剂感知的多任务建模，并在基准测试中产生一致结果。案例研究展示亚埃级对接精度，均方根偏差为0.157埃。&lt;h4&gt;结论&lt;/h4&gt;该框架为结合机制提供了原子级见解，并推动了基于结构的药物设计。&lt;h4&gt;翻译&lt;/h4&gt;准确的蛋白质-配体相互作用预测对计算机辅助药物发现至关重要。然而，现有方法通常无法捕捉溶剂依赖的构象变化，并且缺乏联合学习多个相关任务的能力。为解决这些局限性，我们引入了一种预训练方法，该方法将不同溶剂条件下生成的配体构象系综作为增强输入。这种设计使模型能够以统一方式学习结构灵活性和环境背景。训练过程整合了分子重建以捕获局部几何结构、原子间距离预测以建模空间关系、以及对比学习以构建溶剂不变的分子表示。这些组件共同带来了显著改进，包括结合亲和力预测提高3.7%、在PoseBusters Astex对接基准测试中达到82%的成功率、以及在虚拟筛选中97.1%的曲线下面积。该框架支持溶剂感知的多任务建模，并在基准测试中产生一致的结果。案例研究进一步展示了亚埃级对接精度，均方根偏差为0.157埃，为结合机制提供了原子级见解，并推动了基于结构的药物设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of protein-ligand interactions is essential forcomputer-aided drug discovery. However, existing methods often fail to capturesolvent-dependent conformational changes and lack the ability to jointly learnmultiple related tasks. To address these limitations, we introduce apre-training method that incorporates ligand conformational ensembles generatedunder diverse solvent conditions as augmented input. This design enables themodel to learn both structural flexibility and environmental context in aunified manner. The training process integrates molecular reconstruction tocapture local geometry, interatomic distance prediction to model spatialrelationships, and contrastive learning to build solvent-invariant molecularrepresentations. Together, these components lead to significant improvements,including a 3.7% gain in binding affinity prediction, an 82% success rate onthe PoseBusters Astex docking benchmarks, and an area under the curve of 97.1%in virtual screening. The framework supports solvent-aware, multi-task modelingand produces consistent results across benchmarks. A case study furtherdemonstrates sub-angstrom docking accuracy with a root-mean-square deviation of0.157 angstroms, offering atomic-level insight into binding mechanisms andadvancing structure-based drug design.</description>
      <author>example@mail.com (Jing Lan, Hexiao Ding, Hongzhao Chen, Yufeng Jiang, Ng Nga Chun, Gerald W. Y. Cheng, Zongxi Li, Jing Cai, Liang-ting Lin, Jung Sun Yoo)</author>
      <guid isPermaLink="false">2508.01799v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>CLASS: Contrastive Learning via Action Sequence Supervision for Robot Manipulation</title>
      <link>http://arxiv.org/abs/2508.01600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in Proceedings of the Conference on Robot Learning (CoRL)  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CLASS(Contrastive Learning via Action Sequence Supervision)的新方法，用于解决行为克隆在异构数据集上的泛化问题，特别是在视觉变化显著的情况下。&lt;h4&gt;背景&lt;/h4&gt;行为克隆在机器人操作领域因表现力强的模型、动作序列建模和大规模演示数据而取得显著进展，但在应用于异构数据集(如不同相机姿态或物体外观导致的视觉变化)时性能下降。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法使行为克隆能够从异构演示数据中学习共享结构，提高在视觉变化环境中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出CLASS方法，使用监督式对比学习从演示中学习行为表征，利用动态时间规整识别相似动作序列提供的弱监督，并优化具有相似度加权正对的软InfoNCE损失。&lt;h4&gt;主要发现&lt;/h4&gt;CLASS在5个模拟基准和3个真实世界任务上实现了具有竞争力的结果；对于在显著视觉变化下的下游策略学习，使用CLASS预训练的扩散策略取得了平均75%的成功率，而所有其他基线方法无法竞争性地执行。&lt;h4&gt;结论&lt;/h4&gt;CLASS方法有效解决了行为克隆在异构数据集上的泛化问题，特别是在视觉变化显著的情况下表现优异。&lt;h4&gt;翻译&lt;/h4&gt;最近的 Behavior Cloning (BC) 进展在机器人操作领域取得了强大性能，这得益于表现力强的模型、动作序列建模和大规模演示数据。然而，当应用于异构数据集(如不同相机姿态或物体外观导致的视觉变化)时，BC面临重大挑战，尽管有大规模学习的好处，性能仍然下降。这是因为BC倾向于过度拟合单个演示而不是捕获共享结构，限制了泛化能力。为解决这一问题，我们引入了通过动作序列监督进行对比学习(Contrastive Learning via Action Sequence Supervision, CLASS)，一种使用监督式对比学习从演示中学习行为表征的方法。CLASS利用通过动态时间规整(DTW)识别的相似动作序列提供的弱监督，并优化具有相似度加权正对的软InfoNCE损失。我们在5个模拟基准和3个真实世界任务上评估CLASS，仅使用基于检索的控制和表征就实现了具有竞争力的结果。最值得注意的是，对于在显著视觉变化下的下游策略学习，使用CLASS预训练的扩散策略取得了平均75%的成功率，而所有其他基线方法无法竞争性地执行。项目网页：https://class-robot.github.io。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Behavior Cloning (BC) have led to strong performance inrobotic manipulation, driven by expressive models, sequence modeling ofactions, and large-scale demonstration data. However, BC faces significantchallenges when applied to heterogeneous datasets, such as visual shift withdifferent camera poses or object appearances, where performance degradesdespite the benefits of learning at scale. This stems from BC's tendency tooverfit individual demonstrations rather than capture shared structure,limiting generalization. To address this, we introduce Contrastive Learning viaAction Sequence Supervision (CLASS), a method for learning behavioralrepresentations from demonstrations using supervised contrastive learning.CLASS leverages weak supervision from similar action sequences identified viaDynamic Time Warping (DTW) and optimizes a soft InfoNCE loss withsimilarity-weighted positive pairs. We evaluate CLASS on 5 simulationbenchmarks and 3 real-world tasks to achieve competitive results usingretrieval-based control with representations only. Most notably, for downstreampolicy learning under significant visual shifts, Diffusion Policy with CLASSpre-training achieves an average success rate of 75%, while all other baselinemethods fail to perform competitively. Project webpage:https://class-robot.github.io.</description>
      <author>example@mail.com (Sung-Wook Lee, Xuhui Kang, Brandon Yang, Yen-Ling Kuo)</author>
      <guid isPermaLink="false">2508.01600v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>LetheViT: Selective Machine Unlearning for Vision Transformers via Attention-Guided Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.01569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LetheViT的对比遗忘方法，专门用于解决Vision Transformers(ViTs)中的随机数据遗忘问题。通过选择性掩码实验揭示ViTs核心特征，LetheViT能够有效引导模型忘记特定数据样本的影响，同时保留其一般识别能力，达到隐私保护与模型效能的平衡。&lt;h4&gt;背景&lt;/h4&gt;Vision Transformers在计算机视觉任务中表现出色，但随着GDPR和CCPA等隐私法规的实施，用户有权撤回其数据，这要求不仅删除数据，还需从训练模型中完全移除数据影响。精确遗忘方法计算成本过高，而近似方法提供了更实用的解决方案。&lt;h4&gt;目的&lt;/h4&gt;解决ViTs中随机数据遗忘的特别挑战场景，使模型能够忘记特定样本同时保留其他样本，即使在同一类别中也是如此，同时平衡隐私合规性与模型效能。&lt;h4&gt;方法&lt;/h4&gt;通过选择性掩码实验揭示ViTs的核心特征，提出LetheViT对比遗忘方法。该方法使用掩码图像输入生成正logits，原始图像输入生成负logits，引导模型忘记特定细节同时保留一般类别轮廓。&lt;h4&gt;主要发现&lt;/h4&gt;1) 当ViTs的高注意力区域被掩码时，模型保留识别能力但显著削弱记忆能力；2) 基于这一发现提出的LetheViT方法能有效平衡隐私保护与模型性能；3) LetheViT在随机数据遗忘场景中达到了最先进性能。&lt;h4&gt;结论&lt;/h4&gt;LetheViT作为一种专为ViTs设计的对比遗忘方法，通过选择性掩码和对比学习策略，成功解决了随机数据遗忘问题，实现了隐私合规与模型效能的有效平衡，为满足隐私法规要求提供了实用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视觉变换器(ViTs)凭借其卓越性能彻底改变了计算机视觉任务。然而，GDPR和CCPA等隐私法规的引入给它们带来了新的挑战。这些法律授予用户撤回其数据的权利，不仅需要删除数据，还需要从训练好的模型中完全移除其影响。机器遗忘成为一种关键解决方案，其中精确遗忘在计算上过于昂贵，而近似方法提供了更实用的方法。这项工作解决了ViTs中随机数据遗忘的特别具有挑战性的场景，模型必须忘记特定样本同时保留其他样本，即使在同一类别中也是如此。我们首先通过选择性掩码实验揭示了ViTs的核心特征：当高注意力区域被掩码时，模型保留其识别能力但显著削弱其记忆能力。基于上述见解，我们提出了LetheViT，一种专为ViTs设计的对比遗忘方法。LetheViT使用掩码图像输入生成正logits，原始图像输入生成负logits，引导模型忘记特定细节同时保留一般的类别轮廓。实验结果表明，LetheViT达到了最先进的性能，有效平衡了隐私合规与模型效能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Transformers (ViTs) have revolutionized computer vision tasks withtheir exceptional performance. However, the introduction of privacy regulationssuch as GDPR and CCPA has brought new challenges to them. These laws grantusers the right to withdraw their data, necessitating not only the deletion ofdata but also the complete removal of its influence from trained models.Machine unlearning emerges as a critical solution, with exact unlearning beingcomputationally prohibitive and approximate methods offering a more practicalapproach. This work addresses the particularly challenging scenario of randomdata forgetting in ViTs, where the model must forget specific samples whileretaining others, even within the same class. We first reveal the corecharacteristics of ViTs through selective masking experiments: whenhigh-attention areas are masked, the model retains its recognition capabilitybut significantly weakens its memorization ability. Based on the aboveinsights, we propose LetheViT, a contrastive unlearning method tailored forViTs. LetheViT uses masked image inputs to generate positive logits andoriginal image inputs to generate negative logits, guiding the model to forgetspecific details while retaining the general cl category outlines. Experimentalresults demonstrate that LetheViT achieves state-of-the-art performance,effectively balancing privacy compliance with model efficacy.</description>
      <author>example@mail.com (Yujia Tong, Tian Zhang, Jingling Yuan, Yuze Wang, Chuang Hu)</author>
      <guid isPermaLink="false">2508.01569v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection</title>
      <link>http://arxiv.org/abs/2508.01248v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NS-Net的新型检测框架，通过NULL-Space投影解耦CLIP视觉特征中的语义信息，并结合对比学习捕获真实与生成图像间的分布差异，同时设计了Patch Selection策略保留精细痕迹，显著提升了AI生成图像检测的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;生成模型如GANs和扩散模型的快速发展能够创建高度逼真的图像，引发安全敏感领域被滥用的担忧。现有检测器在已知生成设置下表现良好，但难以推广到未知生成模型，尤其当真实与伪造图像语义内容紧密对齐时。&lt;h4&gt;目的&lt;/h4&gt;重新审视CLIP特征在AI生成图像检测中的应用，解决CLIP视觉特征中嵌入的高层次语义信息阻碍有效区分的问题。&lt;h4&gt;方法&lt;/h4&gt;提出NS-Net检测框架，利用NULL-Space投影将CLIP视觉特征中的语义信息解耦，使用对比学习捕获真实和生成图像间的内在分布差异，并设计Patch Selection策略减轻全局图像结构引起的语义偏差以保留精细人工痕迹。&lt;h4&gt;主要发现&lt;/h4&gt;CLIP特征中的高层次语义信息阻碍了有效区分真实和AI生成图像；NS-Net在包含40种不同生成模型的开世界基准测试中表现优异，检测准确率比现有最先进方法提高7.4%。&lt;h4&gt;结论&lt;/h4&gt;NS-Net在GAN和基于扩散的图像生成技术方面展现出强大的泛化能力，为AI生成图像检测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;生成模型（如GANs和扩散模型）的快速发展促进了高度逼真图像的创建，引发了在安全敏感领域被滥用的日益增长的担忧。虽然现有检测器在已知的生成设置下表现良好，但它们往往难以推广到未知的生成模型，特别是当真实和伪造图像之间的语义内容紧密对齐时。在本文中，我们重新审视了CLIP特征在AI生成图像检测中的应用，并发现了一个关键局限：CLIP视觉特征中嵌入的高层次语义信息阻碍了有效区分。为解决这一问题，我们提出了NS-Net，一种新的检测框架，利用NULL-Space投影将CLIP视觉特征中的语义信息解耦，然后通过对比学习捕获真实和生成图像之间的内在分布差异。此外，我们设计了一种Patch Selection策略，通过减轻全局图像结构引起的语义偏差来保留精细的人工痕迹。在对包含40种不同生成模型生成图像的开世界基准进行的广泛实验中，NS-Net优于现有最先进的方法，检测准确率提高了7.4%，从而证明了它在基于GAN和基于扩散的图像生成技术方面具有强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid progress of generative models, such as GANs and diffusion models,has facilitated the creation of highly realistic images, raising growingconcerns over their misuse in security-sensitive domains. While existingdetectors perform well under known generative settings, they often fail togeneralize to unknown generative models, especially when semantic contentbetween real and fake images is closely aligned. In this paper, we revisit theuse of CLIP features for AI-generated image detection and uncover a criticallimitation: the high-level semantic information embedded in CLIP's visualfeatures hinders effective discrimination. To address this, we propose NS-Net,a novel detection framework that leverages NULL-Space projection to decouplesemantic information from CLIP's visual features, followed by contrastivelearning to capture intrinsic distributional differences between real andgenerated images. Furthermore, we design a Patch Selection strategy to preservefine-grained artifacts by mitigating semantic bias caused by global imagestructures. Extensive experiments on an open-world benchmark comprising imagesgenerated by 40 diverse generative models show that NS-Net outperforms existingstate-of-the-art methods, achieving a 7.4\% improvement in detection accuracy,thereby demonstrating strong generalization across both GAN- anddiffusion-based image generation techniques.</description>
      <author>example@mail.com (Jiazhen Yan, Fan Wang, Weiwei Jiang, Ziqiang Li, Zhangjie Fu)</author>
      <guid isPermaLink="false">2508.01248v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>CM$^3$: Calibrating Multimodal Recommendation</title>
      <link>http://arxiv.org/abs/2508.01226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Working Paper: https://github.com/enoche/CM3&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对多模态推荐系统中的对齐和均匀性原则，提出了一种校准的均匀性损失函数和球形贝塞尔方法，解决了现有模型过度强调均匀性而忽视对齐的问题，提升了推荐性能。&lt;h4&gt;背景&lt;/h4&gt;在对比学习中，对齐和均匀性是基本原则。在推荐系统中，优化贝叶斯个性化排序(BPR)损失有助于实现对齐和均匀性目标，其中对齐旨在拉近交互用户和项目的表示，均匀性要求用户和项目嵌入在单位超球面上均匀分布。&lt;h4&gt;目的&lt;/h4&gt;重新审视多模态推荐系统中的对齐和均匀性属性，挑战通过均匀性损失对项目进行公平处理的传统假设，提出使具有相似多模态属性的项目在超球面流形上收敛到相近表示的更细致方法。&lt;h4&gt;方法&lt;/h4&gt;利用项目多模态数据之间的相似性校准均匀性分布，增强不相似实体间的排斥力；引入球形贝塞尔方法融合多模态特征，确保融合后的特征被约束在相同的超球面流形上；进行理论分析阐明校准的均匀性损失与常规均匀性函数的关系。&lt;h4&gt;主要发现&lt;/h4&gt;现有多模态推荐模型倾向于优先考虑均匀性而损害对齐；通过整合MLLM提取的特征，所提方法可实现NDCG@20性能高达5.4%的提升；校准的均匀性损失能在嵌入空间中产生更强的排斥力。&lt;h4&gt;结论&lt;/h4&gt;通过校准均匀性损失和引入球形贝塞尔方法，所提方法在多模态推荐系统中优于竞争基线，实现了更好的推荐性能，源代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;对齐和均匀性是对比学习领域的基本原则。在推荐系统中，先前的工作已经证明优化贝叶斯个性化排序(BPR)损失有助于实现对齐和均匀性的目标。具体来说，对齐旨在拉近交互用户和项目的表示，而均匀性要求用户和项目嵌入在单位超球面上均匀分布。本研究重新审视了多模态推荐系统背景下的对齐和均匀性属性，揭示了现有模型倾向于优先考虑均匀性而损害对齐的倾向。我们的假设挑战了通过均匀性损失对项目进行公平处理的传统假设，提出了一种更细致的方法，即具有相似多模态属性的项目在超球面流形上收敛到相近的表示。具体来说，我们利用项目多模态数据之间的相似性来校准它们的均匀性分布，从而在嵌入空间中诱导不相似实体之间更明显的排斥力。理论分析阐明了这种校准的均匀性损失与常规均匀性函数之间的关系。此外，为了增强多模态特征的融合，我们引入了一种球形贝塞尔方法，旨在融合任意数量的模态，同时确保融合后的特征被约束在相同的超球面流形上。在五个真实世界数据集上进行经验评估，证实了我们的方法优于竞争基线。我们还表明，通过整合MLLM提取的特征，所提出的方法可以实现NDCG@20性能高达5.4%的提升。源代码可在以下网址获取：https://github.com/enoche/CM3。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Alignment and uniformity are fundamental principles within the domain ofcontrastive learning. In recommender systems, prior work has established thatoptimizing the Bayesian Personalized Ranking (BPR) loss contributes to theobjectives of alignment and uniformity. Specifically, alignment aims to drawtogether the representations of interacting users and items, while uniformitymandates a uniform distribution of user and item embeddings across a unithypersphere. This study revisits the alignment and uniformity properties withinthe context of multimodal recommender systems, revealing a proclivity amongextant models to prioritize uniformity to the detriment of alignment. Ourhypothesis challenges the conventional assumption of equitable item treatmentthrough a uniformity loss, proposing a more nuanced approach wherein items withsimilar multimodal attributes converge toward proximal representations withinthe hyperspheric manifold. Specifically, we leverage the inherent similaritybetween items' multimodal data to calibrate their uniformity distribution,thereby inducing a more pronounced repulsive force between dissimilar entitieswithin the embedding space. A theoretical analysis elucidates the relationshipbetween this calibrated uniformity loss and the conventional uniformityfunction. Moreover, to enhance the fusion of multimodal features, we introducea Spherical B\'ezier method designed to integrate an arbitrary number ofmodalities while ensuring that the resulting fused features are constrained tothe same hyperspherical manifold. Empirical evaluations conducted on fivereal-world datasets substantiate the superiority of our approach over competingbaselines. We also shown that the proposed methods can achieve up to a 5.4%increase in NDCG@20 performance via the integration of MLLM-extracted features.Source code is available at: https://github.com/enoche/CM3.</description>
      <author>example@mail.com (Xin Zhou, Yongjie Wang, Zhiqi Shen)</author>
      <guid isPermaLink="false">2508.01226v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning Pivoting Manipulation with Force and Vision Feedback Using Optimization-based Demonstrations</title>
      <link>http://arxiv.org/abs/2508.01082v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合模型方法和学习方法的框架，用于学习闭环摆动操作，通过接触隐式轨迹优化和演示引导的深度强化学习实现样本高效学习，并采用特权训练策略实现仿真到现实迁移。&lt;h4&gt;背景&lt;/h4&gt;非抓取操作具有挑战性，因为物体、环境和机器人之间复杂的接触相互作用使得操作困难。&lt;h4&gt;目的&lt;/h4&gt;结合基于模型和基于学习方法的优势，提出一种框架，使机器人能够仅使用本体感觉、视觉和力传感执行摆动操作，无需特权信息。&lt;h4&gt;方法&lt;/h4&gt;利用计算高效的接触隐式轨迹优化（CITO）设计演示引导的深度强化学习（RL），采用特权训练策略实现仿真到现实迁移。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在多个摆动任务上成功实现了仿真到现实迁移，机器人能够仅使用本体感觉、视觉和力传感执行摆动操作，无需特权信息。&lt;h4&gt;结论&lt;/h4&gt;结合模型方法和学习方法可以克服各自的局限性，实现样本高效的学习和成功的仿真到现实迁移。&lt;h4&gt;翻译&lt;/h4&gt;非抓取操作由于物体、环境和机器人之间复杂的接触相互作用而具有挑战性。基于模型的方法可以在接触约束下高效生成机器人和物体的复杂轨迹，但它们往往对模型不准确敏感，并且需要访问特权信息（如物体质量、大小、姿态），使其不太适合新物体。相比之下，基于学习的方法通常对建模错误更鲁棒，但需要大量数据。在本文中，我们结合这两种方法，提出了一种学习闭环摆动操作的框架。通过利用计算高效的接触隐式轨迹优化（CITO），我们设计了演示引导的深度强化学习（RL），实现了样本高效学习。我们还提出了使用特权训练策略的仿真到现实迁移方法，使机器人能够仅使用本体感觉、视觉和力传感执行摆动操作，无需访问特权信息。我们的方法在几个摆动任务上进行了评估，证明它能够成功实现仿真到现实迁移。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Non-prehensile manipulation is challenging due to complex contactinteractions between objects, the environment, and robots. Model-basedapproaches can efficiently generate complex trajectories of robots and objectsunder contact constraints. However, they tend to be sensitive to modelinaccuracies and require access to privileged information (e.g., object mass,size, pose), making them less suitable for novel objects. In contrast,learning-based approaches are typically more robust to modeling errors butrequire large amounts of data. In this paper, we bridge these two approaches topropose a framework for learning closed-loop pivoting manipulation. Byleveraging computationally efficient Contact-Implicit Trajectory Optimization(CITO), we design demonstration-guided deep Reinforcement Learning (RL),leading to sample-efficient learning. We also present a sim-to-real transferapproach using a privileged training strategy, enabling the robot to performpivoting manipulation using only proprioception, vision, and force sensingwithout access to privileged information. Our method is evaluated on severalpivoting tasks, demonstrating that it can successfully perform sim-to-realtransfer.</description>
      <author>example@mail.com (Yuki Shirai, Kei Ota, Devesh K. Jha, Diego Romeres)</author>
      <guid isPermaLink="false">2508.01082v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Graph Embedding in the Graph Fractional Fourier Transform Domain</title>
      <link>http://arxiv.org/abs/2508.02383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图分数傅里叶变换的广义分数滤波嵌入方法(GEFRFE)，通过将现有方法扩展到分数域来增强嵌入信息量，并在多个基准数据集上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;谱图嵌入在图表示学习中起着关键作用，它通过从图谱信息生成低维向量表示来实现。然而，传统谱嵌入方法的嵌入空间往往表现出有限的表达能力，无法在可替代的变换域中充分捕获潜在的结构特征。&lt;h4&gt;目的&lt;/h4&gt;解决传统谱嵌入方法表达能力有限的问题，使其能够在可替代的变换域中充分捕获潜在的结构特征。&lt;h4&gt;方法&lt;/h4&gt;使用图分数傅里叶变换将现有的广义频率滤波嵌入(GEFFE)扩展到分数域，产生GEFRFE；利用图分数域滤波和来自分数化图拉普拉斯矩阵的特征向量分量的非线性组合；引入两种并行策略动态确定分数阶：基于搜索的优化和基于ResNet18的自适应学习。&lt;h4&gt;主要发现&lt;/h4&gt;在六个基准数据集上的实验表明，GEFRFE捕获了更丰富的结构特征并显著提高了分类性能，同时保持了与GEFFE方法相当的计算复杂度。&lt;h4&gt;结论&lt;/h4&gt;GEFRFE方法通过扩展到分数域有效增强了嵌入的表达能力，提高了图表示学习的效果。&lt;h4&gt;翻译&lt;/h4&gt;谱图嵌入通过从图谱信息生成低维向量表示，在图表示学习中起着关键作用。然而，传统谱嵌入方法的嵌入空间往往表现出有限的表达能力，无法在可替代的变换域中充分捕获潜在的结构特征。为解决这个问题，我们使用图分数傅里叶变换将现有的最先进广义频率滤波嵌入(GEFFE)扩展到分数域，产生了广义分数滤波嵌入(GEFRFE)，该方法通过图分数域增强了嵌入信息量。GEFRFE利用图分数域滤波和来自分数化图拉普拉斯矩阵导出的特征向量分量的非线性组合。为了动态确定分数阶，引入了两种并行策略：基于搜索的优化和基于ResNet18的自适应学习。在六个基准数据集上的大量实验表明，GEFRFE捕获了更丰富的结构特征并显著提高了分类性能。值得注意的是，所提出的方法保持了与GEFFE方法相当的计算复杂度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spectral graph embedding plays a critical role in graph representationlearning by generating low-dimensional vector representations from graphspectral information. However, the embedding space of traditional spectralembedding methods often exhibit limited expressiveness, failing to exhaustivelycapture latent structural features across alternative transform domains. Toaddress this issue, we use the graph fractional Fourier transform to extend theexisting state-of-the-art generalized frequency filtering embedding (GEFFE)into fractional domains, giving birth to the generalized fractional filteringembedding (GEFRFE), which enhances embedding informativeness via the graphfractional domain. The GEFRFE leverages graph fractional domain filtering and anonlinear composition of eigenvector components derived from a fractionalizedgraph Laplacian. To dynamically determine the fractional order, two parallelstrategies are introduced: search-based optimization and a ResNet18-basedadaptive learning. Extensive experiments on six benchmark datasets demonstratethat the GEFRFE captures richer structural features and significantly enhanceclassification performance. Notably, the proposed method retains computationalcomplexity comparable to GEFFE approaches.</description>
      <author>example@mail.com (Changjie Sheng, Zhichao Zhang, Wei Yao)</author>
      <guid isPermaLink="false">2508.02383v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Whole-body Representation Learning For Competing Preclinical Disease Risk Assessment</title>
      <link>http://arxiv.org/abs/2508.02307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种全身自监督表示学习方法，用于竞争风险建模下的临床前疾病风险评估，在多种疾病预测中优于传统放射组学方法，并展示了其在临床工作流程中的转化潜力。&lt;h4&gt;背景&lt;/h4&gt;可靠的临床前疾病风险评估对将公共医疗从反应性治疗转变为主动识别和预防至关重要，但现有基于图像的风险预测算法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种全身自监督表示学习方法，用于临床前疾病风险评估，以克服现有方法仅考虑单一疾病和依赖手工制作特征的不足。&lt;h4&gt;方法&lt;/h4&gt;采用全身自监督表示学习方法在竞争风险建模框架下进行临床前疾病风险评估，并结合心脏MRI提高心血管疾病亚组的预测能力。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在心血管疾病、2型糖尿病、慢性阻塞性肺病和慢性肾病等多种疾病预测中优于全身放射组学；与心脏MRI结合后，进一步提高了缺血性心脏病、高血压疾病和中风等心血管疾病亚组的预测准确性。&lt;h4&gt;结论&lt;/h4&gt;全身表示作为独立筛查方式和临床工作流程中多模态框架的一部分，在早期个性化风险分层方面具有重要转化潜力，相关代码已在GitHub平台公开。&lt;h4&gt;翻译&lt;/h4&gt;可靠的临床前疾病风险评估对于将公共医疗从反应性治疗转变为主动识别和预防至关重要。然而，基于图像的风险预测算法通常一次只考虑一种情况，并依赖于通过分割工具获得的手工制作特征。我们提出了一种在竞争风险建模下的临床前疾病风险评估的全局自监督表示学习方法。这种方法在多种疾病（包括心血管疾病、2型糖尿病、慢性阻塞性肺病和慢性肾病）中优于全身放射组学。模拟临床前筛查场景并与心脏MRI结合后，进一步提高了心血管疾病亚组（缺血性心脏病、高血压疾病和中风）的预测能力。结果表明，全身表示作为独立筛查方式和临床工作流程中多模态框架的一部分，在早期个性化风险分层方面具有转化潜力。代码可在https://github.com/yayapa/WBRLforCR/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何进行更准确的全身性疾病风险预测问题，特别是在疾病尚未出现症状的'临床前'阶段。这个问题很重要，因为心血管疾病、2型糖尿病等慢性疾病每年导致超过2600万人死亡，这些疾病经常共存，而传统算法通常单独处理，无法捕捉疾病间的相互作用。早期风险识别可以将医疗从被动治疗转向主动预防，而全身MRI在一次筛查中提供多器官视图，但这种方法在风险预测中尚未得到充分利用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有风险预测方法的局限性，特别是无法处理竞争风险和依赖手工特征的问题。他们使用UK Biobank数据集作为基础，借鉴了Zhang等人在心脏MRI表示学习中的MAE架构，以及Bai等人的心脏特征提取方法。在竞争风险建模方面，他们采用了现有的DSM、NFG和DeepHit模型。作者设计了一个基于MAE的自监督学习框架，从全身MRI中学习表示，然后将其输入竞争风险模型中进行疾病风险预测，并评估了单独使用全身表示以及与心脏特征结合的多模态方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用自监督学习从全身MRI中学习有意义的表示，这些表示能够捕捉全身多个器官和系统之间的相互作用，并在竞争风险框架下使用它们，同时考虑多种疾病之间的相互关系。整体流程包括：1)从UK Biobank获取全身MRI和心脏MRI数据并确定疾病组；2)开发基于MAE的自监督学习模型训练全身表示；3)提取心脏结构和功能特征；4)使用竞争风险模型(DSM、NFG、DeepHit)进行风险预测；5)使用时间依赖一致性指数评估不同模态和模型的表现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)开发了专门针对全身MRI的自监督表示学习方法；2)在单一框架中同时评估多种疾病风险，考虑疾病间的竞争关系；3)提出将全身MRI筛查整合到临床工作流程中的方法；4)证明全身表示可作为独立筛查工具，也可与器官特异性成像结合。相比之前的工作，这篇论文不依赖手工分割和特征提取，而是使用端到端的表示学习；同时考虑多种疾病风险而非单一疾病；将自监督学习应用于全身MRI，并结合表示学习和竞争风险建模，这些都是医学影像分析中的新方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文开发了一种基于全身自监督表示学习的竞争风险预测方法，能够在疾病出现症状前准确评估多种疾病风险，为早期预防和个性化医疗提供了新工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable preclinical disease risk assessment is essential to move publichealthcare from reactive treatment to proactive identification and prevention.However, image-based risk prediction algorithms often consider one condition ata time and depend on hand-crafted features obtained through segmentation tools.We propose a whole-body self-supervised representation learning method for thepreclinical disease risk assessment under a competing risk modeling. Thisapproach outperforms whole-body radiomics in multiple diseases, includingcardiovascular disease (CVD), type 2 diabetes (T2D), chronic obstructivepulmonary disease (COPD), and chronic kidney disease (CKD). Simulating apreclinical screening scenario and subsequently combining with cardiac MRI, itsharpens further the prediction for CVD subgroups: ischemic heart disease(IHD), hypertensive diseases (HD), and stroke. The results indicate thetranslational potential of whole-body representations as a standalone screeningmodality and as part of a multi-modal framework within clinical workflows forearly personalized risk stratification. The code is available athttps://github.com/yayapa/WBRLforCR/</description>
      <author>example@mail.com (Dmitrii Seletkov, Sophie Starck, Ayhan Can Erdur, Yundi Zhang, Daniel Rueckert, Rickmer Braren)</author>
      <guid isPermaLink="false">2508.02307v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2508.02190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FedVLA，第一个联邦Vision-language-action学习框架，旨在解决VLA模型训练中的隐私安全问题，同时保持模型性能。&lt;h4&gt;背景&lt;/h4&gt;Vision-language-action (VLA)模型通过使机器人能够解释语言指令来执行任务，显著推进了机器人操作。然而，训练这些模型通常依赖于大规模用户特定数据，引发隐私和安全问题，限制了其广泛采用。&lt;h4&gt;目的&lt;/h4&gt;提出FedVLA框架，实现分布式模型训练，保护数据隐私而不损害性能，促进VLA模型的广泛采用。&lt;h4&gt;方法&lt;/h4&gt;整合任务感知表征学习、自适应专家选择和专家驱动的联邦聚合；引入指令导向场景解析机制，基于任务指令分解和增强对象级特征；设计双门控专家混合(DGMoE)机制，使输入标记和自感知专家自适应决定激活；在联邦服务器上提出专家驱动聚合策略，由激活专家引导模型聚合。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的仿真和真实机器人实验证明了提案的有效性；DGMoE与其基础版本相比显著提高了计算效率；FedVLA实现了与集中式训练相当的任务成功率，同时有效保护数据隐私。&lt;h4&gt;结论&lt;/h4&gt;FedVLA框架能够在保护数据隐私的同时有效训练VLA模型，有望解决VLA模型广泛采用中的隐私问题。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-行动(VLA)模型通过使机器人能够解释语言指令来执行任务，显著推进了机器人操作。然而，训练这些模型通常依赖于大规模用户特定数据，引发隐私和安全问题，进而限制了它们的广泛采用。为此，我们提出了FedVLA，第一个联邦VLA学习框架，能够实现分布式模型训练，在保护数据隐私的同时不损害性能。我们的框架整合了任务感知表征学习、自适应专家选择和专家驱动的联邦聚合，实现了VLA模型的高效且隐私保护的训练。具体来说，我们引入了指令导向场景解析机制，基于任务指令分解和增强对象级特征，提高上下文理解能力。为了有效学习多样化的任务模式，我们设计了双门控专家混合(DGMoE)机制，其中不仅输入标记，而且自感知专家也能自适应决定其激活。最后，我们在联邦服务器上提出了专家驱动聚合策略，模型聚合由激活专家引导，确保有效的跨客户端知识转移。广泛的仿真和真实机器人实验证明了我们提案的有效性。值得注意的是，与基础版本相比，DGMoE显著提高了计算效率，而FedVLA实现了与集中式训练相当的任务成功率，同时有效保护数据隐私。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉-语言-动作模型训练中的隐私安全问题以及联邦学习环境下机器人操作任务的异构性问题。这些问题很重要，因为训练VLA模型需要大量用户特定数据，会引发隐私泄露风险，限制了技术的广泛应用；同时，传统联邦学习方法难以处理多模态环境下的任务异构性，而现有混合专家模型又缺乏对任务复杂性的适应性，导致在资源受限的客户端上效率低下。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到VLA模型训练中的隐私泄露风险和联邦学习中的任务异构性问题，然后借鉴了联邦学习的基本框架保护数据隐私，基于现有VLA模型架构如RoboFlamingo和RT-2，扩展了混合专家模型架构并增加了自适应选择机制。创新性地设计了指令导向场景解析模块增强特征提取，提出双重门控混合专家机制实现自适应知识路由，以及专家驱动聚合策略处理任务异构性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在保护数据隐私的前提下，通过联邦学习训练VLA模型，利用双重门控混合专家机制提高计算效率和模型性能，基于专家相似性进行模型聚合处理任务异构性。整体流程是：客户端使用IOSP模块处理输入图像，通过DGMoE机制自适应选择专家处理输入并记录专家选择统计信息；服务器接收客户端的模型更新和专家选择统计信息，使用EDA策略基于专家相似性计算聚合权重并更新全局模型；重复迭代直到模型收敛。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) FedVLA框架，首个用于VLA训练的隐私保护联邦学习框架；2) 双重门控混合专家(DGMoE)机制，引入自我感知专家实现双向选择；3) 专家驱动聚合(EDA)策略，基于专家相似性动态分配权重；4) 指令导向场景解析(IOSP)模块，提高上下文理解能力。相比之前的工作，FedVLA能够处理任务异构性，自适应选择专家而非固定数量，并专注于多模态环境下的隐私保护训练，而非单模态任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了FedVLA框架，结合双重门控混合专家机制，在保护用户数据隐私的同时，通过自适应专家选择和基于专家相似性的智能聚合，实现了与集中式训练相当的机器人操作性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language-action (VLA) models have significantly advanced roboticmanipulation by enabling robots to interpret language instructions for taskexecution. However, training these models often relies on large-scaleuser-specific data, raising concerns about privacy and security, which in turnlimits their broader adoption. To address this, we propose FedVLA, the firstfederated VLA learning framework, enabling distributed model training thatpreserves data privacy without compromising performance. Our frameworkintegrates task-aware representation learning, adaptive expert selection, andexpert-driven federated aggregation, enabling efficient and privacy-preservingtraining of VLA models. Specifically, we introduce an Instruction OrientedScene-Parsing mechanism, which decomposes and enhances object-level featuresbased on task instructions, improving contextual understanding. To effectivelylearn diverse task patterns, we design a Dual Gating Mixture-of-Experts (DGMoE)mechanism, where not only input tokens but also self-aware experts adaptivelydecide their activation. Finally, we propose an Expert-Driven Aggregationstrategy at the federated server, where model aggregation is guided byactivated experts, ensuring effective cross-client knowledge transfer.Extensivesimulations and real-world robotic experiments demonstrate the effectiveness ofour proposals. Notably, DGMoE significantly improves computational efficiencycompared to its vanilla counterpart, while FedVLA achieves task success ratescomparable to centralized training, effectively preserving data privacy.</description>
      <author>example@mail.com (Cui Miao, Tao Chang, Meihan Wu, Hongbin Xu, Chun Li, Ming Li, Xiaodong Wang)</author>
      <guid isPermaLink="false">2508.02190v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2508.02172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures, accepted by MM'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GaussianCross，一种新的跨模态自监督3D表示学习架构，通过整合3D高斯溅射技术解决3D场景理解中的模型坍塌和结构信息不足问题，实现了优越的性能和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;3D场景理解中，信息丰富且鲁棒的点表示的重要性已被广泛认可。尽管现有的自监督预训练方法显示出有前景的性能，但由于点区分难度不足，模型坍塌和结构信息不足的问题仍然普遍存在，导致不可靠的表达和次优性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的跨模态自监督3D表示学习架构，以解决当前3D场景理解中的模型坍塌和结构信息不足问题。&lt;h4&gt;方法&lt;/h4&gt;提出了GaussianCross，整合前馈3D高斯溅射技术，将尺度不一致的3D点云转换为统一的立方体归一化高斯表示，并加入三属性自适应蒸馏溅射模块构建3D特征场，促进外观、几何和语义线索的协同特征捕获。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试中，GaussianCross显示出显著的参数和数据效率，通过线性 probing（使用少于0.1%的参数）和有限数据训练（仅使用1%的场景）实现了优越性能；在ScanNet200的语义和实例分割任务上，完整微调将mIoU提高了9.3%，AP50提高了6.1%。&lt;h4&gt;结论&lt;/h4&gt;GaussianCross是一种有效的3D表示学习方法，能够解决现有方法中的模型坍塌和结构信息不足问题，并提供强大的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;信息丰富且鲁棒的点表示在3D场景理解中的重要性已被广泛认可。尽管现有的自监督预训练对应方法显示出有前景的性能，但由于点区分难度不足，模型坍塌和结构信息不足仍然普遍存在，导致不可靠的表达和次优性能。在本文中，我们提出了GaussianCross，一种新的跨模态自监督3D表示学习架构，整合前馈3D高斯溅射技术以解决当前挑战。GaussianCross无缝地将尺度不一致的3D点云转换为统一的立方体归一化高斯表示而不丢失细节，实现稳定且可泛化的预训练。随后，集成了一个三属性自适应蒸馏溅射模块来构建3D特征场，促进外观、几何和语义线索的协同特征捕获，以保持跨模态一致性。为了验证GaussianCross，我们在各种基准上进行了广泛评估，包括ScanNet、ScanNet200和S3DIS。特别是，GaussianCross显示出显著的参数和数据效率，通过线性 probing（使用少于0.1%的参数）和有限数据训练（仅使用1%的场景）与最先进的方法相比实现了优越性能。此外，GaussianCross展示了强大的泛化能力，在ScanNet200的语义和实例分割任务上，通过完整微调将mIoU提高了9.3%，AP50提高了6.1%，支持我们方法的有效性。代码、权重和可视化可在https://rayyoh.github.io/GaussianCross/公开获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景理解中自监督表示学习的挑战，特别是模型崩溃和结构信息不足的问题。这个问题在研究中很重要，因为3D数据稀缺且具有复杂空间结构，限制了有效表示学习策略的设计；在现实中，解决这一问题将减少对大量标注3D数据的依赖，提高3D场景理解的鲁棒性，推动自动驾驶、机器人导航和虚拟现实等领域的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有自监督3D表示学习方法的局限性，包括对比学习方法的模型崩溃问题和神经渲染方法的训练效率低、忽略几何语义关系的问题。他们选择3D高斯溅射(3DGS)作为基础技术，但注意到其泛化能力不足。为此，他们设计了立方体归一化高斯初始化解决尺度不一致问题，并引入三属性自适应蒸馏溅射模块同时捕获外观、几何和语义信息。该方法借鉴了Ponder的NeRF预训练范式、GS3的3D高斯溅射技术、PointContrast的对比学习思想，以及视觉基础模型的知识蒸馏技术，但避免了这些方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D点云转换为结构化的3D高斯表示，实现尺度一致的场景表示，通过多模态自监督学习同时捕获外观、几何和语义信息，并利用视觉基础模型的知识蒸馏增强语义感知能力。整体流程包括：1)立方体归一化高斯初始化，将点云转换到单位立方体空间并体素化；2)三属性自适应蒸馏溅射，预测高斯属性并引入偏移量细化位置；3)多目标渲染与损失计算，同时优化光度重建、几何一致性和语义对齐；4)下游任务评估，包括线性探测、数据效率和完整微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)立方体归一化高斯初始化，解决不同场景间的尺度不一致问题；2)三属性自适应蒸馏溅射模块，同时捕获外观、几何和语义属性；3)跨模态知识蒸馏，利用预训练2D视觉基础模型的知识；4)多目标渲染策略，组合多种损失函数提供全面监督。相比之前的工作，该方法避免了对比学习方法的模型崩溃问题，提高了神经渲染方法的训练效率，扩展了现有3DGS方法仅关注光度重建的局限性，并在有限数据情况下也能取得较好性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GaussianCross通过创新的跨模态自监督学习框架，利用3D高斯溅射技术实现了对3D场景外观、几何和语义信息的联合建模，在多种3D场景理解任务上取得了最先进的性能，同时展现出卓越的参数和数据效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The significance of informative and robust point representations has beenwidely acknowledged for 3D scene understanding. Despite existingself-supervised pre-training counterparts demonstrating promising performance,the model collapse and structural information deficiency remain prevalent dueto insufficient point discrimination difficulty, yielding unreliableexpressions and suboptimal performance. In this paper, we presentGaussianCross, a novel cross-modal self-supervised 3D representation learningarchitecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniquesto address current challenges. GaussianCross seamlessly convertsscale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussianrepresentation without missing details, enabling stable and generalizablepre-training. Subsequently, a tri-attribute adaptive distillation splattingmodule is incorporated to construct a 3D feature field, facilitating synergeticfeature capturing of appearance, geometry, and semantic cues to maintaincross-modal consistency. To validate GaussianCross, we perform extensiveevaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. Inparticular, GaussianCross shows a prominent parameter and data efficiency,achieving superior performance through linear probing (&lt;0.1% parameters) andlimited data training (1% of scenes) compared to state-of-the-art methods.Furthermore, GaussianCross demonstrates strong generalization capabilities,improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ onScanNet200 semantic and instance segmentation tasks, respectively, supportingthe effectiveness of our approach. The code, weights, and visualizations arepublicly available at\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.</description>
      <author>example@mail.com (Lei Yao, Yi Wang, Yi Zhang, Moyun Liu, Lap-Pui Chau)</author>
      <guid isPermaLink="false">2508.02172v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Structure Maintained Representation Learning Neural Network for Causal Inference</title>
      <link>http://arxiv.org/abs/2508.01865v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结构保持表征学习方法，通过引入结构保持器来提高表征学习和对抗网络在估计个体处理效应时的预测准确性。&lt;h4&gt;背景&lt;/h4&gt;因果推断的最新发展使研究兴趣从估计平均处理效应转向个体处理效应。&lt;h4&gt;目的&lt;/h4&gt;提高表征学习和对抗网络在估计个体处理效应时的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;在表征层末端训练一个判别器以平衡表征平衡性和信息损失，提出的方法最小化了处理估计误差的上界，通过考虑学习表征空间与原始协变量特征空间之间的相关性来解决分布平衡与信息损失之间的权衡。&lt;h4&gt;主要发现&lt;/h4&gt;提出的结构保持表征学习算法在模拟和真实世界观察数据上的实验中优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;该算法在MIMIC-III数据库的真实电子健康记录数据上得到了验证。&lt;h4&gt;翻译&lt;/h4&gt;最近因果推断的发展极大地改变了研究兴趣，从估计平均处理效应转向个体处理效应。在本文中，我们通过引入结构保持器来提高表征学习和对抗网络在估计个体处理效应时的预测准确性，该结构保持器保持了基线协变量及其在高维空间中对应表征之间的相关性。我们在表征层末端训练一个判别器来平衡表征平衡性和信息损失。我们表明，所提出的判别器最小化了处理估计误差的上界。通过考虑学习表征空间与原始协变量特征空间之间的相关性，我们可以解决分布平衡与信息损失之间的权衡。我们使用模拟和真实世界观察数据进行了广泛的实验，表明我们提出的结构保持表征学习算法优于最先进的方法。我们还在MIMIC-III数据库的真实电子健康记录数据上展示了该算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent developments in causal inference have greatly shifted the interestfrom estimating the average treatment effect to the individual treatmenteffect. In this article, we improve the predictive accuracy of representationlearning and adversarial networks in estimating individual treatment effects byintroducing a structure keeper which maintains the correlation between thebaseline covariates and their corresponding representations in the highdimensional space. We train a discriminator at the end of representation layersto trade off representation balance and information loss. We show that theproposed discriminator minimizes an upper bound of the treatment estimationerror. We can address the tradeoff between distribution balance and informationloss by considering the correlations between the learned representation spaceand the original covariate feature space. We conduct extensive experiments withsimulated and real-world observational data to show that our proposed StructureMaintained Representation Learning (SMRL) algorithm outperformsstate-of-the-art methods. We also demonstrate the algorithms on real electronichealth record data from the MIMIC-III database.</description>
      <author>example@mail.com (Yang Sun, Wenbin Lu, Yi-Hui Zhou)</author>
      <guid isPermaLink="false">2508.01865v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>OmniEvent: Unified Event Representation Learning</title>
      <link>http://arxiv.org/abs/2508.01842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了OmniEvent，第一个统一的事件表征学习框架，能够在多样化任务上实现最先进性能，完全消除了对特定任务设计的需求。&lt;h4&gt;背景&lt;/h4&gt;事件相机因其超高的动态范围和时间分辨率在计算机视觉中日益流行，但事件网络严重依赖特定任务的设计，因为数据分布是非结构化的，且存在时空不均匀性，这使得难以重用现有架构来完成新任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的事件表征学习框架，能够在多样化任务上实现最先进性能，完全消除对特定任务设计的需求。&lt;h4&gt;方法&lt;/h4&gt;提出解耦-增强-融合范式，在空间和时间域上独立进行局部特征聚合和增强以避免不均匀性问题；应用空间填充曲线实现大感受野同时提高内存和计算效率；使用注意力机制融合各域特征学习时空相互作用；输出网格形状张量使标准视觉模型能处理事件数据无需架构改变。&lt;h4&gt;主要发现&lt;/h4&gt;使用统一框架和相似超参数，OmniEvent在3个代表性任务和10个数据集上，比特定任务的SOTA方法性能提高高达68.2%。&lt;h4&gt;结论&lt;/h4&gt;OmniEvent解决了事件相机数据处理的时空不均匀性问题，提供了一种通用方法，无需针对不同任务进行专门设计，代码将在https://github.com/Wickyan/OmniEvent上提供。&lt;h4&gt;翻译&lt;/h4&gt;事件相机由于其超高的动态范围和时间分辨率，在计算机视觉中获得了越来越多的关注。然而，由于事件数据的非结构化分布和时空(S-T)不均匀性，事件网络严重依赖特定任务的设计，这使得难以重用现有架构来完成新任务。我们提出了OmniEvent，这是第一个统一的事件表征学习框架，能够在多样化任务上实现最先进性能，完全消除对特定任务设计的需求。与以往将事件数据视为具有手动调整时空缩放权重的3D点云的方法不同，OmniEvent提出了解耦-增强-融合范式，在空间和时间域上独立进行局部特征聚合和增强，以避免不均匀性问题。应用空间填充曲线能够实现大感受野，同时提高内存和计算效率。然后通过注意力机制融合各个域的特征，学习时空相互作用。OmniEvent的输出是一个网格形状的张量，使标准视觉模型能够处理事件数据而无需改变架构。使用统一的框架和相似的超参数，OmniEvent在3个代表性任务和10个数据集上，比特定任务的SOTA方法性能提高高达68.2%(图1)。代码将在https://github.com/Wickyan/OmniEvent上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras have gained increasing popularity in computer vision due totheir ultra-high dynamic range and temporal resolution. However, event networksheavily rely on task-specific designs due to the unstructured data distributionand spatial-temporal (S-T) inhomogeneity, making it hard to reuse existingarchitectures for new tasks. We propose OmniEvent, the first unified eventrepresentation learning framework that achieves SOTA performance across diversetasks, fully removing the need of task-specific designs. Unlike previousmethods that treat event data as 3D point clouds with manually tuned S-Tscaling weights, OmniEvent proposes a decouple-enhance-fuse paradigm, where thelocal feature aggregation and enhancement is done independently on the spatialand temporal domains to avoid inhomogeneity issues. Space-filling curves areapplied to enable large receptive fields while improving memory and computeefficiency. The features from individual domains are then fused by attention tolearn S-T interactions. The output of OmniEvent is a grid-shaped tensor, whichenables standard vision models to process event data without architecturechange. With a unified framework and similar hyper-parameters, OmniEventout-performs (tasks-specific) SOTA by up to 68.2% across 3 representative tasksand 10 datasets (Fig.1). Code will be ready inhttps://github.com/Wickyan/OmniEvent .</description>
      <author>example@mail.com (Weiqi Yan, Chenlu Lin, Youbiao Wang, Zhipeng Cai, Xiuhong Lin, Yangyang Shi, Weiquan Liu, Yu Zang)</author>
      <guid isPermaLink="false">2508.01842v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>ModFus-DM: Explore the Representation in Modulated Signal Diffusion Generated Models</title>
      <link>http://arxiv.org/abs/2508.01719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ModFus-DM的新型无监督AMC框架，通过扩散模型进行调制特征融合，解决了现有方法在大规模标记信号需求、非固定信号长度、分布偏移和标记信号有限等方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;自动调制分类(AMC)在军事和民用无线通信系统中都至关重要，但现有基于深度学习的方法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在有限标记信号条件下处理非固定信号长度、分布偏移等挑战的AMC方法。&lt;h4&gt;方法&lt;/h4&gt;提出调制驱动特征融合的扩散模型(ModFus-DM)，包括调制信号扩散生成模型(MSDGM)和扩散感知特征融合(DAFFus)模块，通过渐进去噪过程捕获信号结构和语义信息，并自适应聚合多尺度扩散特征。&lt;h4&gt;主要发现&lt;/h4&gt;在RML2016.10A、RML2016.10B、RML2018.01A和RML2022数据集上的实验表明，ModFus-DM在有限标记设置、分布偏移、可变长度信号识别和信道衰减等场景下显著优于现有方法，在24类识别任务中，当信噪比≥12dB且每类只有10个标记信号时，准确率达到88.27%以上。&lt;h4&gt;结论&lt;/h4&gt;ModFus-DM通过利用扩散模型的生成能力，有效解决了AMC中的关键挑战，为实际应用提供了更稳健的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;自动调制分类(AMC)对于军事和民用应用中的无线通信系统至关重要。然而，现有的基于深度学习的AMC方法通常需要大量标记信号，并且难以处理非固定信号长度、分布偏移和标记信号有限等问题。为应对这些挑战，我们提出了通过扩散模型进行调制驱动的特征融合(ModFus-DM)，这是一种新颖的无监督AMC框架，它利用扩散模型的生成能力进行稳健的调制表征学习。我们设计了一个调制信号扩散生成模型(MSDGM)，通过渐进去噪过程隐式捕获结构和语义信息。此外，我们提出了扩散感知特征融合(DAFFus)模块，该模块自适应地聚合多尺度扩散特征以增强判别性表征。在RML2016.10A、RML2016.10B、RML2018.01A和RML2022数据集上的广泛实验表明，ModFus-DM在各种具有挑战性的场景下显著优于现有方法，例如有限标记设置、分布偏移、可变长度信号识别和信道衰减场景。值得注意的是，在24类识别任务中，当信噪比≥12dB且每类只有10个标记信号时，ModFus-DM的准确率超过88.27%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic modulation classification (AMC) is essential for wirelesscommunication systems in both military and civilian applications. However,existing deep learning-based AMC methods often require large labeled signalsand struggle with non-fixed signal lengths, distribution shifts, and limitedlabeled signals. To address these challenges, we propose a modulation-drivenfeature fusion via diffusion model (ModFus-DM), a novel unsupervised AMCframework that leverages the generative capacity of diffusion models for robustmodulation representation learning. We design a modulated signal diffusiongeneration model (MSDGM) to implicitly capture structural and semanticinformation through a progressive denoising process. Additionally, we proposethe diffusion-aware feature fusion (DAFFus) module, which adaptively aggregatesmulti-scale diffusion features to enhance discriminative representation.Extensive experiments on RML2016.10A, RML2016.10B, RML2018.01A and RML2022datasets demonstrate that ModFus-DM significantly outperforms existing methodsin various challenging scenarios, such as limited-label settings, distributionshifts, variable-length signal recognition and channel fading scenarios.Notably, ModFus-DM achieves over 88.27% accuracy in 24-type recognition tasksat SNR $\geq $ 12dB with only 10 labeled signals per type.</description>
      <author>example@mail.com (Haoyue Tan, Yu Li, Zhenxi Zhang, Xiaoran Shi, Feng Zhou)</author>
      <guid isPermaLink="false">2508.01719v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition</title>
      <link>http://arxiv.org/abs/2508.01644v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in ACM Multimedia 2025. 10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DRKF(解耦表示与知识融合)的多模态情感识别方法，通过优化表示学习和知识融合两个主要模块解决了模态异质性和情感线索不一致性的挑战，在多个数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态情感识别(MER)旨在通过整合和分析多种模态的信息来识别情感状态，但模态异质性和情感线索不一致性是阻碍性能提升的关键挑战。&lt;h4&gt;目的&lt;/h4&gt;解决多模态情感识别中存在的模态异质性和情感线索不一致性问题，提高情感识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出DRKF方法，包含两个主要模块：1)优化表示学习(ORL)模块，使用对比互信息估计方法和渐进模态增强来解耦任务相关的共享表示和模态特定特征；2)知识融合(KF)模块，包含基于轻量级自注意力的融合编码器(FE)和情感判别子模块(ED)，用于识别主导模态、整合情感信息并处理情感不一致情况。&lt;h4&gt;主要发现&lt;/h4&gt;DRKF在IEMOCAP、MELD和M3ED数据集上实现了最先进的(SOTA)性能，有效解决了多模态情感识别中的关键挑战。&lt;h4&gt;结论&lt;/h4&gt;DRKF方法通过解耦表示和知识融合有效处理了模态异质性和情感不一致性问题，即使在不正确选择主导模态的情况下仍能保持准确预测，源代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;多模态情感识别(MER)旨在通过整合和分析多种模态的信息来识别情感状态。然而，固有的模态异质性和情感线索不一致性仍然是阻碍性能的关键挑战。为解决这些问题，我们为MER提出了一种解耦表示与知识融合(DRKF)方法。DRKF包含两个主要模块：优化表示学习(ORL)模块和知识融合(KF)模块。ORL采用对比互信息估计方法与渐进模态增强来解耦任务相关的共享表示和模态特定特征，同时减轻模态异质性。KF包含一个基于轻量级自注意力的融合编码器(FE)，该编码器识别主导模态并整合其他模态的情感信息以增强融合表示。为处理情感不一致条件下错误选择主导模态的潜在问题，我们引入了情感判别子模块(ED)，强制融合表示保留情感不一致性的判别线索。这确保即使FE选择了不适当的主导模态，情感分类子模块(EC)仍能通过利用保留的不一致性信息做出准确预测。实验表明，DRKF在IEMOCAP、MELD和M3ED上达到了最先进的性能。源代码已在https://github.com/PANPANKK/DRKF公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3754758&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal emotion recognition (MER) aims to identify emotional states byintegrating and analyzing information from multiple modalities. However,inherent modality heterogeneity and inconsistencies in emotional cues remainkey challenges that hinder performance. To address these issues, we propose aDecoupled Representations with Knowledge Fusion (DRKF) method for MER. DRKFconsists of two main modules: an Optimized Representation Learning (ORL) Moduleand a Knowledge Fusion (KF) Module. ORL employs a contrastive mutualinformation estimation method with progressive modality augmentation todecouple task-relevant shared representations and modality-specific featureswhile mitigating modality heterogeneity. KF includes a lightweightself-attention-based Fusion Encoder (FE) that identifies the dominant modalityand integrates emotional information from other modalities to enhance the fusedrepresentation. To handle potential errors from incorrect dominant modalityselection under emotionally inconsistent conditions, we introduce an EmotionDiscrimination Submodule (ED), which enforces the fused representation toretain discriminative cues of emotional inconsistency. This ensures that evenif the FE selects an inappropriate dominant modality, the EmotionClassification Submodule (EC) can still make accurate predictions by leveragingpreserved inconsistency information. Experiments show that DRKF achievesstate-of-the-art (SOTA) performance on IEMOCAP, MELD, and M3ED. The source codeis publicly available at https://github.com/PANPANKK/DRKF.</description>
      <author>example@mail.com (Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, Daibing Yao)</author>
      <guid isPermaLink="false">2508.01644v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>MiraGe: Multimodal Discriminative Representation Learning for Generalizable AI-Generated Image Detection</title>
      <link>http://arxiv.org/abs/2508.01525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACMMM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MiraGe的AI生成图像检测方法，通过学习生成器不变特征，实现了对已知和未知生成器的高效检测。&lt;h4&gt;背景&lt;/h4&gt;生成式模型的快速发展使得需要能够区分真实图像和AI生成图像的鲁棒检测器，现有方法在已知生成器上表现良好，但在面对新兴或未见过的生成模型时性能下降。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效区分真实图像和AI生成图像的检测方法，特别是在面对未知生成器时仍能保持高性能。&lt;h4&gt;方法&lt;/h4&gt;提出MiraGe方法，学习生成器不变特征，通过最小化类内变化和最大化类间分离来增强特征判别能力，并应用多模态提示学习将原则提炼到CLIP中，利用文本嵌入作为语义锚点。&lt;h4&gt;主要发现&lt;/h4&gt;MiraGe在多个基准测试上取得了最先进的性能，即使在对像Sora这样的未见过的生成器时也能保持鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;MiraGe通过学习生成器不变特征和利用多模态提示学习，显著提高了AI生成图像检测的泛化能力和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;最近的生成模型进展凸显了对能够区分真实图像和AI生成图像的鲁棒检测器的需求。虽然现有方法在已知生成器上表现良好，但当用新兴或未见的生成模型测试时，其性能往往会下降，这是由于重叠的特征嵌入阻碍了准确的跨生成器分类。在本文中，我们提出了用于通用AI生成图像检测的多模态判别表示学习(MiraGe)，这是一种旨在学习生成器不变特征的方法。受最小化类内变化和最大化类间分离的理论见解启发，MiraGe紧密对齐同一类别内的特征，同时最大化类别之间的分离，增强特征判别能力。此外，我们将多模态提示学习应用于将这些原则进一步提炼到CLIP中，利用文本嵌入作为语义锚点进行有效的判别表示学习，从而提高泛化能力。在多个基准测试上的综合实验表明，MiraGe实现了最先进的性能，即使对像Sora这样的未见过的生成器也能保持鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in generative models have highlighted the need for robustdetectors capable of distinguishing real images from AI-generated images. Whileexisting methods perform well on known generators, their performance oftendeclines when tested with newly emerging or unseen generative models due tooverlapping feature embeddings that hinder accurate cross-generatorclassification. In this paper, we propose Multimodal DiscriminativeRepresentation Learning for Generalizable AI-generated Image Detection(MiraGe), a method designed to learn generator-invariant features. Motivated bytheoretical insights on intra-class variation minimization and inter-classseparation, MiraGe tightly aligns features within the same class whilemaximizing separation between classes, enhancing feature discriminability.Moreover, we apply multimodal prompt learning to further refine theseprinciples into CLIP, leveraging text embeddings as semantic anchors foreffective discriminative representation learning, thereby improvinggeneralizability. Comprehensive experiments across multiple benchmarks showthat MiraGe achieves state-of-the-art performance, maintaining robustness evenagainst unseen generators like Sora.</description>
      <author>example@mail.com (Kuo Shi, Jie Lu, Shanshan Ye, Guangquan Zhang, Zhen Fang)</author>
      <guid isPermaLink="false">2508.01525v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians</title>
      <link>http://arxiv.org/abs/2508.01464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Can3Tok是首个3D场景级别的变分自编码器，能够将大量高斯基元编码为低维潜在嵌入，有效捕获输入的语义和空间信息，并通过通用数据处理流程解决了3D场景中的尺度不一致问题。&lt;h4&gt;背景&lt;/h4&gt;3D生成已取得显著进展，但仍主要停留在物体级别；前馈3D场景级别生成很少被探索，因为缺乏能够扩展3D场景级别数据潜在表示学习的模型；与物体级别生成模型不同，3D场景级别生成面临无边界的场景和不同场景间尺度不一致的挑战，使得统一的潜在表示学习变得极其困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种3D场景级别的变分自编码器(VAE)，能够将大量高斯基元编码为低维潜在嵌入，同时解决3D场景数据中的尺度不一致性问题。&lt;h4&gt;方法&lt;/h4&gt;提出Can3Tok，第一个3D场景级别的变分自编码器，设计了一种通用的3D场景数据处理流程来解决尺度不一致问题，使用3D高斯溅射(3DGS)表示3D场景。&lt;h4&gt;主要发现&lt;/h4&gt;在DL3DV-10K数据集上验证，只有Can3Tok成功推广到新的3D场景，而其他方法在训练几百个场景输入时无法收敛，并且在推理时表现出零泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Can3Tok能够有效捕获输入的语义和空间信息，展示了图像到3DGS和文本到3DGS生成的应用，证明了其促进下游生成任务的能力。&lt;h4&gt;翻译&lt;/h4&gt;三维生成已取得显著进展，然而，它仍然主要停留在物体级别。由于缺乏能够扩展三维场景级别数据潜在表示学习的模型，前馈三维场景级别生成很少被探索。与在有限规范空间中使用良好标记的三维数据进行训练的物体级别生成模型不同，使用三维高斯溅射表示的三维场景级别生成是无边界的，并且在不同场景之间存在尺度不一致性，这使得用于生成目的的统一潜在表示学习变得极其困难。在本文中，我们引入了Can3Tok，这是第一个三维场景级别的变分自编码器(VAE)，能够将大量高斯基元编码为低维潜在嵌入，有效捕获输入的语义和空间信息。除了模型设计外，我们还提出了一个通用的三维场景数据处理流程来解决尺度不一致问题。我们在最近的三维场景级别数据集DL3DV-10K上验证了我们的方法，发现只有Can3Tok成功推广到新的三维场景，而相比之下，其他方法在训练几百个场景输入时无法收敛，并且在推理时表现出零泛化能力。最后，我们展示了图像到三维高斯溅射和文本到三维高斯溅射的生成作为应用，以证明其促进下游生成任务的能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景级别的生成和表示学习问题。当前3D生成技术主要集中在物体级别，而完整的场景级别3D生成很少被探索，缺乏能在大规模3D场景数据上进行潜在表示学习的模型。这个问题很重要，因为它能推动AR/VR等沉浸式应用的发展，为游戏、电影制作等领域提供更高效的3D内容创建工具，而现有方法要么训练时间过长，要么缺乏3D一致性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有3D生成方法的局限性，识别出两个主要挑战：3DGS数据的高度无结构性和不同场景间的尺度不一致性。他们借鉴了变分自编码器(VAE)的思想、PerceiverIO的交叉注意力架构、图像处理中的归一化技术，以及文本分割模型(Segment Anything)来过滤噪声。设计上，作者创建了Can3Tok架构，结合了交叉注意力处理无结构数据、3DGS归一化解决尺度问题，以及语义过滤提高质量，这些都是对现有技术的创新应用和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将无结构的3D高斯溅射(3DGS)转换为结构化的3D标记，通过VAE框架学习低维潜在空间表示，同时捕捉语义和空间信息。整体流程包括：1)输入处理(归一化和语义过滤)；2)编码器(傅里叶位置编码、交叉注意力、自注意力)；3)潜在空间采样(使用VAE重参数化)；4)解码器(线性层、自注意力、多层感知器映射回3D空间)；5)训练目标(重建损失和KL散度)；6)应用(文本到3DGS和图像到3DGS生成)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Can3Tok架构，首个场景级别3DGS VAE，使用交叉注意力处理无结构数据；2)3DGS数据归一化方法，解决尺度不一致问题；3)语义感知过滤机制，提高重建质量；4)结构化潜在空间，同时保留语义和空间关系。相比之前工作，Can3Tok能处理数千个场景(其他方法仅能处理数百个)，在测试新场景时表现良好，而其他方法完全失效；它专门针对高度无结构的3DGS数据设计，而非传统点云；专注于整个场景而非单个物体的表示和生成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Can3Tok首次实现了场景级别3D高斯溅射的标准化3D标记化和潜在建模，解决了大规模3D场景表示学习的尺度不一致问题，并成功应用于文本到3DGS和图像到3DGS的生成任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D generation has made significant progress, however, it still largelyremains at the object-level. Feedforward 3D scene-level generation has beenrarely explored due to the lack of models capable of scaling-up latentrepresentation learning on 3D scene-level data. Unlike object-level generativemodels, which are trained on well-labeled 3D data in a bounded canonical space,scene-level generations with 3D scenes represented by 3D Gaussian Splatting(3DGS) are unbounded and exhibit scale inconsistency across different scenes,making unified latent representation learning for generative purposes extremelychallenging. In this paper, we introduce Can3Tok, the first 3D scene-levelvariational autoencoder (VAE) capable of encoding a large number of Gaussianprimitives into a low-dimensional latent embedding, which effectively capturesboth semantic and spatial information of the inputs. Beyond model design, wepropose a general pipeline for 3D scene data processing to address scaleinconsistency issue. We validate our method on the recent scene-level 3Ddataset DL3DV-10K, where we found that only Can3Tok successfully generalizes tonovel 3D scenes, while compared methods fail to converge on even a few hundredscene inputs during training and exhibit zero generalization ability duringinference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation asour applications to demonstrate its ability to facilitate downstream generationtasks.</description>
      <author>example@mail.com (Quankai Gao, Iliyan Georgiev, Tuanfeng Y. Wang, Krishna Kumar Singh, Ulrich Neumann, Jae Shin Yoon)</author>
      <guid isPermaLink="false">2508.01464v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Capturing More: Learning Multi-Domain Representations for Robust Online Handwriting Verification</title>
      <link>http://arxiv.org/abs/2508.01427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SPECTRUM模型，一个时间-频率协同模型，用于在线手写验证(OHV)，通过多领域表示学习提高验证性能，包含三个核心组件：多尺度交互器、自门控融合模块和多域距离验证器。&lt;h4&gt;背景&lt;/h4&gt;在线手写验证领域需要更有效的特征表示方法来区分真实和伪造的手写，传统的基于时间的方法可能不够充分，而多领域学习可能未被充分利用。&lt;h4&gt;目的&lt;/h4&gt;提出SPECTRUM模型，解锁多领域表示学习在在线手写验证中的未开发潜力，提高真实与伪造手写之间的区分度。&lt;h4&gt;方法&lt;/h4&gt;SPECTRUM包含三个核心组件：(1)多尺度交互器：通过双模态序列交互和多尺度聚合精细结合时间和频率特征；(2)自门控融合模块：通过自驱动平衡动态整合全局时间和频率特征；(3)多域距离验证器：利用时间和频率表示来提高真实和伪造手写之间的区分度。&lt;h4&gt;主要发现&lt;/h4&gt;SPECTRUM在OHV任务上表现优于现有方法；时间-频率多领域学习是有效的；整合多种手写生物特征可以增强手写表示的区分能力；多领域学习在OHV中有效，并为特征和生物识别领域的多领域方法研究铺平了道路。&lt;h4&gt;结论&lt;/h4&gt;SPECTRUM模型通过时间-频率多领域协同学习显著提高了在线手写验证的性能，验证了多领域学习的有效性，并为未来研究提供了方向。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们提出了SPECTRUM，一个时间-频率协同模型，该模型解锁了多领域表示学习在在线手写验证中的未开发潜力。SPECTRUM包含三个核心组件：(1) 一个多尺度交互器，通过双模态序列交互和多尺度聚合精细结合时间和频率特征；(2) 一个自门控融合模块，通过自驱动平衡动态整合全局时间和频率特征。这两个组件协同工作，实现从微观到宏观的光谱-时间整合。(3) 一个基于多域距离的验证器随后利用时间和频率表示来提高真实和伪造手写之间的区分度，超越了传统仅基于时间的方法。大量实验证明了SPECTRUM相对于现有OHV方法的优越性能，强调了时间-频率多领域学习的有效性。此外，我们揭示整合多种手写生物特征可以从根本上增强手写表示的区分能力并促进验证。这些发现不仅验证了多领域学习在OHV中的有效性，也为未来在特征和生物识别领域的多领域方法研究铺平了道路。代码可在公开链接获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose SPECTRUM, a temporal-frequency synergistic modelthat unlocks the untapped potential of multi-domain representation learning foronline handwriting verification (OHV). SPECTRUM comprises three corecomponents: (1) a multi-scale interactor that finely combines temporal andfrequency features through dual-modal sequence interaction and multi-scaleaggregation, (2) a self-gated fusion module that dynamically integrates globaltemporal and frequency features via self-driven balancing. These two componentswork synergistically to achieve micro-to-macro spectral-temporal integration.(3) A multi-domain distance-based verifier then utilizes both temporal andfrequency representations to improve discrimination between genuine and forgedhandwriting, surpassing conventional temporal-only approaches. Extensiveexperiments demonstrate SPECTRUM's superior performance over existing OHVmethods, underscoring the effectiveness of temporal-frequency multi-domainlearning. Furthermore, we reveal that incorporating multiple handwrittenbiometrics fundamentally enhances the discriminative power of handwritingrepresentations and facilitates verification. These findings not only validatethe efficacy of multi-domain learning in OHV but also pave the way for futureresearch in multi-domain approaches across both feature and biometric domains.Code is publicly available at https://github.com/NiceRingNode/SPECTRUM.</description>
      <author>example@mail.com (Peirong Zhang, Kai Ding, Lianwen Jin)</author>
      <guid isPermaLink="false">2508.01427v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>CoCoLIT: ControlNet-Conditioned Latent Image Translation for MRI to Amyloid PET Synthesis</title>
      <link>http://arxiv.org/abs/2508.01292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoCoLIT的扩散模型框架，用于从结构MRI合成淀粉样蛋白PET扫描图像，为阿尔茨海默病的大规模筛查提供了一种经济有效的方法。&lt;h4&gt;背景&lt;/h4&gt;淀粉样蛋白PET扫描是阿尔茨海默病筛查的重要工具，但不如结构MRI广泛可用且成本高。虽然MRI不直接检测淀粉样蛋白病理，但它可能编码与淀粉样蛋白沉积相关的信息。现有的MRI到PET翻译方法面临3D神经影像数据高维度和结构复杂性的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从MRI合成高质量淀粉样蛋白PET图像的方法，用于大规模阿尔茨海默病筛查，同时提高合成质量和分类准确性。&lt;h4&gt;方法&lt;/h4&gt;提出CoCoLIT（ControlNet-Conditioned Latent Image Translation），一种基于扩散的潜在生成框架，包含三个主要创新：加权图像空间损失（WISL）改善潜在表示学习和合成质量；对潜在平均稳定化（LAS）的理论和实证分析；基于ControlNet的条件化方法用于MRI到PET翻译。&lt;h4&gt;主要发现&lt;/h4&gt;在公开数据集上评估，CoCoLIT在图像相关和淀粉样蛋白相关指标上都显著优于最先进的方法。在淀粉样蛋白阳性分类中，CoCoLIT比第二好的方法在内部分别集上提高了10.5%，在外部分别集上提高了23.7%。&lt;h4&gt;结论&lt;/h4&gt;CoCoLIT提供了一种有效且经济的方法，可以从MRI合成高质量的淀粉样蛋白PET图像，有助于阿尔茨海默病的大规模筛查。&lt;h4&gt;翻译&lt;/h4&gt;论文的代码和模型可在https://github.com/brAIn-science/CoCoLIT获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从广泛可用的结构MRI图像合成淀粉样蛋白PET图像的问题。这个问题很重要，因为淀粉样蛋白PET是早期阿尔茨海默病诊断的关键工具，但成本高、获取有限且有辐射暴露。而MRI更经济、无创且广泛使用，但对早期AD诊断效果较差。从MRI合成PET可以 enabling大规模、经济高效的AD筛查，特别是在资源有限地区。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有GAN方法存在训练不稳定和模式崩溃问题，而扩散模型虽能生成高质量图像但计算需求大。作者借鉴了潜在扩散模型(LDMs)、ControlNet调节机制和潜在平均稳定化(LAS)技术，设计了一个结合三个创新的框架：加权图像空间损失(WISL)、LAS的理论分析以及ControlNet条件用于MRI-PET转换。作者在低维潜在空间操作以简化学习任务，并利用ControlNet实现精确条件控制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过扩散模型在潜在空间进行MRI到PET的转换，结合ControlNet实现精确条件控制，并使用加权图像空间损失和潜在平均稳定化技术提高合成质量。流程包括：(1)独立训练MRI和PET的VAE进行表示学习；(2)训练LDM学习潜在分布；(3)添加ControlNet模块学习条件分布；(4)引入WISL损失提高合成质量；(5)在推理中使用LAS技术，通过平均多个潜在样本提高效率而不损失质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：(1)加权图像空间损失(WISL)，使用时间步长相关加权优先考虑不同频率特征的合成；(2)提供潜在平均稳定化(LAS)的理论分析和实证验证，证明其在充分训练模型中的有效性；(3)首次成功将ControlNet应用于MRI到PET转换。相比之前工作，CoCoLIT在潜在空间操作而非图像空间，减少计算需求；使用WISL而非简单图像损失，更好地与扩散过程保持一致；通过LAS提高推理效率；在图像质量和临床指标上均显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CoCoLIT通过创新的ControlNet条件潜在图像转换框架，结合加权图像空间损失和潜在平均稳定化技术，实现了从MRI到淀粉样蛋白PET的高质量合成，显著提高了阿尔茨海默病早期筛查的准确性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthesizing amyloid PET scans from the more widely available and accessiblestructural MRI modality offers a promising, cost-effective approach forlarge-scale Alzheimer's Disease (AD) screening. This is motivated by evidencethat, while MRI does not directly detect amyloid pathology, it may nonethelessencode information correlated with amyloid deposition that can be uncoveredthrough advanced modeling. However, the high dimensionality and structuralcomplexity of 3D neuroimaging data pose significant challenges for existingMRI-to-PET translation methods. Modeling the cross-modality relationship in alower-dimensional latent space can simplify the learning task and enable moreeffective translation. As such, we present CoCoLIT (ControlNet-ConditionedLatent Image Translation), a diffusion-based latent generative framework thatincorporates three main innovations: (1) a novel Weighted Image Space Loss(WISL) that improves latent representation learning and synthesis quality; (2)a theoretical and empirical analysis of Latent Average Stabilization (LAS), anexisting technique used in similar generative models to enhance inferenceconsistency; and (3) the introduction of ControlNet-based conditioning forMRI-to-PET translation. We evaluate CoCoLIT's performance on publicly availabledatasets and find that our model significantly outperforms state-of-the-artmethods on both image-based and amyloid-related metrics. Notably, inamyloid-positivity classification, CoCoLIT outperforms the second-best methodwith improvements of +10.5% on the internal dataset and +23.7% on the externaldataset. The code and models of our approach are available athttps://github.com/brAIn-science/CoCoLIT.</description>
      <author>example@mail.com (Alec Sargood, Lemuel Puglisi, James H. Cole, Neil P. Oxtoby, Daniele Ravì, Daniel C. Alexander)</author>
      <guid isPermaLink="false">2508.01292v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models for Bioacoustics -- a Comparative Review</title>
      <link>http://arxiv.org/abs/2508.01277v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对大规模预训练的生物声学基础模型进行了全面综述，并系统研究了它们在多个生物声学分类任务上的可迁移性。研究评估了不同模型在各种基准测试上的表现，并提供了模型选择和适应新任务的有价值指导。&lt;h4&gt;背景&lt;/h4&gt;自动生物声学分析对生物多样性监测和保护至关重要，需要先进的深度学习模型来适应各种生物声学任务。&lt;h4&gt;目的&lt;/h4&gt;对大规模预训练的生物声学基础模型进行全面综述，系统研究这些模型在多个生物声学分类任务上的可迁移性。&lt;h4&gt;方法&lt;/h4&gt;概述生物声学表示学习包括主要预训练数据来源和基准；通过分析模型架构、预训练方案和训练范式等设计决策回顾生物声学基础模型；在BEANS和BirdSet基准的分类任务上评估选定模型；比较在线性和注意力探测策略下学习表示的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;BirdMAE在大规模鸟类歌曲数据上以自监督目标训练，在BirdSet基准上表现最佳；在BEANS上，BEATs_NLM表现稍好；基于Transformer的模型需要注意力探测来提取其表示的全部性能；ConvNext_BS和Perch模型在BirdSet的被动声学监测分类任务中保持竞争力；训练新的线性分类器比不进一步训练评估模型有明显优势；在BEANS上，BEATs基线模型在使用注意力探测评估时优于鸟类特定模型。&lt;h4&gt;结论&lt;/h4&gt;这些发现为从业者选择适当的模型并通过探测将其适应新的生物声学分类任务提供了有价值的指导。&lt;h4&gt;翻译&lt;/h4&gt;自动生物声学分析对生物多样性监测和保护至关重要，需要能够适应各种生物声学任务的高级深度学习模型。本文对大规模预训练的生物声学基础模型进行了全面综述，并系统研究了它们在多个生物声学分类任务上的可迁移性。我们概述了生物声学表示学习，包括主要的预训练数据来源和基准。在此基础上，我们通过深入分析模型架构、预训练方案和训练范式等设计决策来回顾生物声学基础模型。此外，我们在BEANS和BirdSet基准的分类任务上评估了选定的基础模型，比较了在线性和注意力探测策略下学习表示的泛化能力。我们全面的实验分析表明，在大规模鸟类歌曲数据上以自监督目标训练的BirdMAE在BirdSet基准上取得了最佳性能。在BEANS上，NatureLM-audio大型音频模型的提取编码器BEATs_NLM表现稍好。两种基于Transformer的模型都需要注意力探测来提取其表示的全部性能。在大规模鸟类歌曲数据上监督训练的ConvNext_BS和Perch模型在BirdSet的线性探测设置中对于被动声学监测分类任务仍然具有竞争力。训练新的线性分类器比在不进一步训练的情况下评估这些模型有明显优势。而在BEANS上，当使用注意力探测评估时，在AudioSet上以自监督训练的基线模型BEATs优于鸟类特定模型。这些发现为从业者选择适当的模型并通过探测将其适应新的生物声学分类任务提供了有价值的指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated bioacoustic analysis is essential for biodiversity monitoring andconservation, requiring advanced deep learning models that can adapt to diversebioacoustic tasks. This article presents a comprehensive review of large-scalepretrained bioacoustic foundation models and systematically investigates theirtransferability across multiple bioacoustic classification tasks. We overviewbioacoustic representation learning including major pretraining data sourcesand benchmarks. On this basis, we review bioacoustic foundation models bythoroughly analysing design decisions such as model architecture, pretrainingscheme, and training paradigm. Additionally, we evaluate selected foundationmodels on classification tasks from the BEANS and BirdSet benchmarks, comparingthe generalisability of learned representations under both linear and attentiveprobing strategies. Our comprehensive experimental analysis reveals thatBirdMAE, trained on large-scale bird song data with a self-supervisedobjective, achieves the best performance on the BirdSet benchmark. On BEANS,BEATs$_{NLM}$, the extracted encoder of the NatureLM-audio large audio model,is slightly better. Both transformer-based models require attentive probing toextract the full performance of their representations. ConvNext$_{BS}$ andPerch models trained with supervision on large-scale bird song data remaincompetitive for passive acoustic monitoring classification tasks of BirdSet inlinear probing settings. Training a new linear classifier has clear advantagesover evaluating these models without further training. While on BEANS, thebaseline model BEATs trained with self-supervision on AudioSet outperformsbird-specific models when evaluated with attentive probing. These findingsprovide valuable guidance for practitioners selecting appropriate models toadapt them to new bioacoustic classification tasks via probing.</description>
      <author>example@mail.com (Raphael Schwinger, Paria Vali Zadeh, Lukas Rauch, Mats Kurz, Tom Hauschild, Sam Lapp, Sven Tomforde)</author>
      <guid isPermaLink="false">2508.01277v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2508.01251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Soft Separation and Distillation (SSD)的新方法，用于解决联邦无监督学习中的全局均匀性问题。该方法通过鼓励客户端表示向不同方向扩展来减少模型聚合时的干扰，从而提高全局均匀性同时保持局部表示的表现力。&lt;h4&gt;背景&lt;/h4&gt;联邦无监督学习(FUL)旨在在联邦和自监督设置下学习表现力强的表示。现有方法在实现客户端(局部)均匀性方面表现良好，但由于非独立同分布数据分布和FUL的去中心化特性，在聚合后无法实现客户端间(全局)均匀性。&lt;h4&gt;目的&lt;/h4&gt;解决FUL中局部均匀性良好但全局均匀性不足的问题，提出一种能够保持客户端间均匀性的新方法。&lt;h4&gt;方法&lt;/h4&gt;提出Soft Separation and Distillation (SSD)方法，通过鼓励客户端表示向不同方向扩展来保持客户端间的均匀性。此外，引入了投影蒸馏模块来解决损失优化与表示质量之间的差异。&lt;h4&gt;主要发现&lt;/h4&gt;在跨设施和跨设备的联邦设置下评估SSD，展示了在各种训练场景中表示质量和任务性能的一致性改进。&lt;h4&gt;结论&lt;/h4&gt;强调了客户端间均匀性在FUL中的重要性，并将SSD确立为解决这一挑战的有效方案。&lt;h4&gt;翻译&lt;/h4&gt;联邦无监督学习(FUL)旨在在联邦和自监督设置下学习表现力强的表示。FUL中学习到的表示质量通常由均匀性决定，即表示在嵌入空间中分布的均匀程度。然而，现有解决方案在实现客户端(局部)均匀性方面表现良好，但由于非独立同分布数据分布和FUL的去中心化特性，在聚合后无法实现客户端间(全局)均匀性。为了解决这个问题，我们提出了Soft Separation and Distillation (SSD)，一种新颖的方法，通过鼓励客户端表示向不同方向扩展来保持客户端间均匀性。这种设计减少了客户端模型聚合时的干扰，从而提高了全局均匀性，同时保持了局部表示的表现力。我们通过引入投影蒸馏模块进一步增强了这一效果，以解决损失优化与表示质量之间的差异。我们在跨设施和跨设备的联邦设置下评估了SSD，展示了在各种训练场景中表示质量和任务性能的一致性改进。我们的结果强调了客户端间均匀性在FUL中的重要性，并将SSD确立为解决这一挑战的有效方案。项目页面：https://ssd-uniformity.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Unsupervised Learning (FUL) aims to learn expressiverepresentations in federated and self-supervised settings. The quality ofrepresentations learned in FUL is usually determined by uniformity, a measureof how uniformly representations are distributed in the embedding space.However, existing solutions perform well in achieving intra-client (local)uniformity for local models while failing to achieve inter-client (global)uniformity after aggregation due to non-IID data distributions and thedecentralized nature of FUL. To address this issue, we propose Soft Separationand Distillation (SSD), a novel approach that preserves inter-client uniformityby encouraging client representations to spread toward different directions.This design reduces interference during client model aggregation, therebyimproving global uniformity while preserving local representationexpressiveness. We further enhance this effect by introducing a projectordistillation module to address the discrepancy between loss optimization andrepresentation quality. We evaluate SSD in both cross-silo and cross-devicefederated settings, demonstrating consistent improvements in representationquality and task performance across various training scenarios. Our resultshighlight the importance of inter-client uniformity in FUL and establish SSD asan effective solution to this challenge. Project page:https://ssd-uniformity.github.io/</description>
      <author>example@mail.com (Hung-Chieh Fang, Hsuan-Tien Lin, Irwin King, Yifei Zhang)</author>
      <guid isPermaLink="false">2508.01251v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning</title>
      <link>http://arxiv.org/abs/2508.01184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法，通过学习可供性感知的3D表示和采用分阶段推理策略来解决Embodied AI中物体操作学习的问题，实验证明该方法在可供性定位和分类方面都有更好的性能。&lt;h4&gt;背景&lt;/h4&gt;Embodied AI的一个核心问题是像人类一样通过观察学习物体操作。这需要通过观察（如图像）来定位3D物体的可供性区域并理解其功能。&lt;h4&gt;目的&lt;/h4&gt;解决先前方法将可供性定位和分类任务分开处理导致预测不一致的问题，以及无法预测完整可供性区域和在固定尺度下操作难以处理尺度变化的可供性的问题。&lt;h4&gt;方法&lt;/h4&gt;开发跨模态3D表示通过高效融合和多尺度几何特征传播，能够在适当区域尺度上推断完整潜在可供性区域；采用两阶段预测机制有效耦合定位和分类任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明了所提出方法的有效性，在可供性定位和分类方面都显示出改进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过结合跨模态3D表示和两阶段预测机制，所提出的方法解决了先前方法在处理可供性任务时面临的挑战，实现了更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;Embodied AI的一个核心问题是像人类一样通过观察学习物体操作。为了实现这一点，通过观察如图像等来定位3D物体的可供性区域（3D可供性定位）并理解其功能（可供性分类）非常重要。先前的尝试通常分别处理这两个任务，由于缺乏对它们之间依赖关系的适当建模，导致预测不一致。此外，这些方法通常只定位图像中描绘的部分可供性区域，无法预测完整的潜在可供性区域，并且在固定尺度下操作，难以处理相对于整个物体尺度显著变化的可供性。为了解决这些问题，我们提出了一种新方法，学习可供性感知的3D表示，并利用定位和分类任务之间的依赖关系采用分阶段推理策略。具体来说，我们首先通过高效融合和多尺度几何特征传播开发跨模态3D表示，使得能够在适当的区域尺度上推断完整的潜在可供性区域。此外，我们采用简单的两阶段预测机制，有效耦合定位和分类，以更好地理解可供性。实验证明了我们方法的有效性，显示在可供性定位和分类方面都有性能提升。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决具身AI中的核心问题：如何从观察中学习物体操作，具体包括3D物体可供性区域定位（通过图像等观察定位3D物体的交互区域）和可供性分类（理解这些区域的功能）。这个问题在现实中非常重要，因为它使机器人能够像人类一样理解物体的交互方式和功能，对于机器人操作、人机交互、智能家居等领域至关重要，是实现智能体与物理世界有效交互的基础。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的三大局限性：1）将定位和分类任务分开处理导致预测不一致；2）只能定位图像中显示的不完整可供性区域，无法预测完整区域；3）在固定尺度上操作，难以处理尺度变化大的可供性。基于这些分析，作者借鉴了跨模态学习、图神经网络和多尺度特征提取等现有技术，但创新性地将它们组合起来，设计了一个能同时处理两个任务、预测完整区域并适应多尺度的方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习一个可供性感知的3D表示，并采用分阶段推理策略利用定位和分类任务间的依赖关系。整体流程分为四步：1）多模态特征提取：从图像提取上下文感知特征，从点云提取多尺度几何特征；2）跨模态融合：通过多头交叉注意力融合两个模态特征；3）传播和选择：用图神经网络传播特征并选择相关尺度；4）可供性预测：先预测定位概率掩码，再融合全局和局部特征预测类别。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）新的跨模态多尺度可供性表示，提高区域覆盖率；2）简单有效的级联策略，耦合定位和分类任务；3）几何特征传播模块，利用几何相似性传播特征；4）多尺度特征选择，适应不同大小的可供性区域。相比之前工作，本文同时处理两个任务并利用其依赖关系，预测完整而非部分可供性区域，使用多尺度特征处理不同尺度需求，并利用几何相似性提高定位准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于多尺度跨模态表示学习的创新方法，通过融合图像和点云信息并利用几何特征传播，实现了更准确的3D物体可供性区域定位和分类，显著提升了机器从观察中学习物体操作的能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A core problem of Embodied AI is to learn object manipulation fromobservation, as humans do. To achieve this, it is important to localize 3Dobject affordance areas through observation such as images (3D affordancegrounding) and understand their functionalities (affordance classification).Previous attempts usually tackle these two tasks separately, leading toinconsistent predictions due to lacking proper modeling of their dependency. Inaddition, these methods typically only ground the incomplete affordance areasdepicted in images, failing to predict the full potential affordance areas, andoperate at a fixed scale, resulting in difficulty in coping with affordancessignificantly varying in scale with respect to the whole object. To addressthese issues, we propose a novel approach that learns an affordance-aware 3Drepresentation and employs a stage-wise inference strategy leveraging thedependency between grounding and classification tasks. Specifically, we firstdevelop a cross-modal 3D representation through efficient fusion andmulti-scale geometric feature propagation, enabling inference of full potentialaffordance areas at a suitable regional scale. Moreover, we adopt a simpletwo-stage prediction mechanism, effectively coupling grounding andclassification for better affordance understanding. Experiments demonstrate theeffectiveness of our method, showing improved performance in both affordancegrounding and classification.</description>
      <author>example@mail.com (Xinhang Wan, Dongqiang Gou, Xinwang Liu, En Zhu, Xuming He)</author>
      <guid isPermaLink="false">2508.01184v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Interaction Augmentation Network for Robust Multimodal Sentiment Analysis</title>
      <link>http://arxiv.org/abs/2508.01168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图的新型框架，用于解决多模态情感分析中因模态不完整性导致的挑战，通过利用模态内部和跨模态的交互作用，使模型能够从不完整样本中获取缺失语义。&lt;h4&gt;背景&lt;/h4&gt;真实场景中不可避免的模态不完整性对多模态情感分析构成重大挑战。现有方法通过重建或联合表示学习来恢复缺失语义，但往往忽略了模态内部和跨模态的复杂依赖关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用模态内部和跨模态交互作用的框架，使不完整样本能够从互补部分获取缺失语义，从而实现更稳健的多模态情感分析。&lt;h4&gt;方法&lt;/h4&gt;1) 设计可学习超图建模模态内部时间依赖关系，利用各模态内上下文信息；2) 采用基于注意力机制的有向图探索跨模态相关性，捕捉模态间互补信息；3) 整合完美样本知识监督交互过程，引导模型学习可靠稳健的联合表示。&lt;h4&gt;主要发现&lt;/h4&gt;在MOSI和MOSEI数据集上的大量实验证明了该方法的有效性，能够更好地处理模态不完整性问题并提升多模态情感分析的稳健性。&lt;h4&gt;结论&lt;/h4&gt;所提出的图框架能够有效处理模态不完整性，通过建模复杂的模态内部和跨模态交互关系，使模型能够从不完整数据中学习更全面、更鲁棒的表示。&lt;h4&gt;翻译&lt;/h4&gt;真实场景中不可避免的模态不完整性对多模态情感分析(MSA)构成了重大挑战。虽然现有方法通过调整重建或联合表示学习策略来恢复缺失的语义，但它们往往忽略了模态内部和跨模态的复杂依赖关系。因此，它们无法充分利用可用模态来捕捉互补语义。为此，本文提出了一种基于图的新型框架，利用模态内部和跨模态的交互作用，使不完整的样本能够从互补部分中获取缺失语义，从而实现稳健的MSA。具体来说，我们首先设计了一个可学习的超图来建模模态内部的时间依赖关系，以利用每个模态内的上下文信息。然后，采用有向图基于注意力机制探索跨模态相关性，捕捉不同模态间的互补信息。最后，将完美样本中的知识整合起来监督我们的交互过程，引导模型学习可靠且稳健的联合表示。在MOSI和MOSEI数据集上的大量实验证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The inevitable modality imperfection in real-world scenarios posessignificant challenges for Multimodal Sentiment Analysis (MSA). While existingmethods tailor reconstruction or joint representation learning strategies torestore missing semantics, they often overlook complex dependencies within andacross modalities. Consequently, they fail to fully leverage availablemodalities to capture complementary semantics. To this end, this paper proposesa novel graph-based framework to exploit both intra- and inter-modalityinteractions, enabling imperfect samples to derive missing semantics fromcomplementary parts for robust MSA. Specifically, we first devise a learnablehypergraph to model intra-modality temporal dependencies to exploit contextualinformation within each modality. Then, a directed graph is employed to exploreinter-modality correlations based on attention mechanism, capturingcomplementary information across different modalities. Finally, the knowledgefrom perfect samples is integrated to supervise our interaction processes,guiding the model toward learning reliable and robust joint representations.Extensive experiments on MOSI and MOSEI datasets demonstrate the effectivenessof our method.</description>
      <author>example@mail.com (Hu Zhangfeng, Shi mengxin)</author>
      <guid isPermaLink="false">2508.01168v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Dataset Condensation with Color Compensation</title>
      <link>http://arxiv.org/abs/2508.01139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了DC3框架，通过颜色补偿解决数据集压缩中的性能与保真度权衡问题，利用潜在扩散模型增强图像颜色多样性，实验证明其优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;数据集压缩面临在极端压缩下平衡性能和保真度的固有挑战。现有方法存在两个瓶颈：图像级选择方法（如Coreset Selection、Dataset Quantization）压缩效率低，像素级优化（如Dataset Distillation）因过参数化导致语义失真。&lt;h4&gt;目的&lt;/h4&gt;解决数据集压缩中的性能与保真度权衡问题，提高压缩图像的色彩度以增强表示学习。&lt;h4&gt;方法&lt;/h4&gt;提出DC3（带颜色补偿的数据集压缩框架），采用校准的选择策略，并利用潜在扩散模型增强图像颜色多样性而非创建全新图像。&lt;h4&gt;主要发现&lt;/h4&gt;颜色在数据集压缩中扮演双重角色（信息载体和基本语义表示单元）；提高压缩图像色彩度有利于表示学习；DC3在多个基准测试中优于最先进方法。&lt;h4&gt;结论&lt;/h4&gt;DC3框架有效解决了数据集压缩中的性能与保真度权衡问题；使用压缩数据集微调预训练扩散模型是可行的，不会导致模型崩溃或退化问题。&lt;h4&gt;翻译&lt;/h4&gt;数据集压缩总是面临一个固有的权衡：在极端压缩下平衡性能和保真度。现有方法面临两个瓶颈：图像级选择方法（Coreset Selection、Dataset Quantization）在压缩效率方面表现不佳，而像素级优化（Dataset Distillation）由于过参数化导致语义失真。通过经验观察，我们发现数据集压缩中的一个关键问题是忽视了颜色的双重作用：作为信息载体和基本语义表示单元。我们认为提高压缩图像的色彩度有利于表示学习。受此启发，我们提出了DC3：一种带颜色补偿的数据集压缩框架。经过校准的选择策略后，DC3利用潜在扩散模型增强图像的颜色多样性，而不是创建全新的图像。大量实验证明了DC3的优越性能和泛化能力，在多个基准测试中优于最先进的方法。据我们所知，除了关注下游任务外，DC3是第一个使用压缩数据集微调预训练扩散模型的研究。FID结果表明，使用我们的高质量数据集训练网络是可行的，不会出现模型崩溃或其他退化问题。代码和生成数据即将发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dataset condensation always faces a constitutive trade-off: balancingperformance and fidelity under extreme compression. Existing methods strugglewith two bottlenecks: image-level selection methods (Coreset Selection, DatasetQuantization) suffer from inefficiency condensation, while pixel-leveloptimization (Dataset Distillation) introduces semantic distortion due toover-parameterization. With empirical observations, we find that a criticalproblem in dataset condensation is the oversight of color's dual role as aninformation carrier and a basic semantic representation unit. We argue thatimproving the colorfulness of condensed images is beneficial for representationlearning. Motivated by this, we propose DC3: a Dataset Condensation frameworkwith Color Compensation. After a calibrated selection strategy, DC3 utilizesthe latent diffusion model to enhance the color diversity of an image ratherthan creating a brand-new one. Extensive experiments demonstrate the superiorperformance and generalization of DC3 that outperforms SOTA methods acrossmultiple benchmarks. To the best of our knowledge, besides focusing ondownstream tasks, DC3 is the first research to fine-tune pre-trained diffusionmodels with condensed datasets. The FID results prove that training networkswith our high-quality datasets is feasible without model collapse or otherdegradation issues. Code and generated data will be released soon.</description>
      <author>example@mail.com (Huyu Wu, Duo Su, Junjie Hou, Guang Li)</author>
      <guid isPermaLink="false">2508.01139v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Mobile U-ViT: Revisiting large kernel and U-shaped ViT for efficient medical image segmentation</title>
      <link>http://arxiv.org/abs/2508.01064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACM Multimedia 2025. Code:  https://github.com/FengheTan9/Mobile-U-ViT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Mobile U-shaped Vision Transformer (Mobile U-ViT)的轻量级移动模型，专门用于医疗图像分割，在资源受限设备上实现了高效且高性能的医疗图像分析。&lt;h4&gt;背景&lt;/h4&gt;临床实践中医疗图像分析需要在资源受限的移动设备上高效执行，但现有针对自然图像优化的移动模型在医疗任务上表现不佳，因为自然图像和医疗领域之间存在显著的信息密度差距。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级、通用且高性能的网络，结合计算效率和医疗成像特有的架构优势，解决医疗图像在移动设备上的分析挑战。&lt;h4&gt;方法&lt;/h4&gt;提出Mobile U-ViT模型，采用ConvUtr作为分层块嵌入，引入大核局部-全局-局部(LGL)块促进信息交换，使用浅层轻量级transformer瓶颈进行长距离建模，并采用具有下采样跳跃连接的级联解码器进行密集预测。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在八个涵盖多种成像模态的公共2D和3D数据集上实现了最先进性能，包括在四个未见数据集上的零样本测试，证明了其强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Mobile U-ViT成为移动医疗图像分析的一种高效、强大且具有泛化能力的解决方案，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;在临床实践中，医疗图像分析通常需要在资源受限的移动设备上高效执行。然而，现有的移动模型主要针对自然图像优化，在医疗任务上表现不佳，因为自然图像和医疗领域之间存在显著的信息密度差距。开发结合计算效率与医疗成像特定架构优势的轻量级、通用且高性能网络仍然是一个挑战。为此，我们提出了一种名为Mobile U-shaped Vision Transformer (Mobile U-ViT)的移动模型，专为医疗图像分割而设计。具体而言，我们采用新提出的ConvUtr作为分层块嵌入，这是一种具有倒置瓶颈融合的参数高效大核CNN。该设计具有类似transformer的表示学习能力，同时更轻更快。为了实现高效的局部-全局信息交换，我们引入了一种新颖的大核局部-全局-局部(LGL)块，有效平衡了医疗图像的低信息密度和高层次语义差异。最后，我们采用浅层轻量级transformer瓶颈进行长距离建模，并使用具有下采样跳跃连接的级联解码器进行密集预测。尽管计算需求减少，我们的医疗优化架构在涵盖多种成像模态的八个公共2D和3D数据集上实现了最先进性能，包括在四个未见数据集上的零样本测试。这些结果确立了它作为移动医疗图像分析的一种高效且强大且具有泛化能力的解决方案。代码可在https://github.com/FengheTan9/Mobile-U-ViT获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何在资源受限的移动设备上高效执行医学图像分割任务，同时保持高准确性的问题。这个问题在现实中非常重要，因为临床实践中医学图像分析常需要在移动设备上实时执行，而现有为自然图像优化的移动模型在医学任务上表现不佳。医学图像分割对提高诊断效率至关重要，能为医生提供客观精确的参考，尤其在床旁成像干预和实时诊断等场景中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了医学图像的两个关键特性：稀疏局部信息和模糊边界高噪声水平。他们借鉴了CNN的高效性和Transformer的全局建模能力，结合深度可分离卷积(DSConv)等轻量级技术。针对医学图像特点，作者设计了ConvUtr块扩大感受野，并创建了LKLGL模块促进局部和全局信息交换。整个方法是基于对现有医学分割模型的批判性分析，结合CNN和Transformer优势，并针对医学图像特殊特性进行的专门优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合CNN的高效性和Transformer的全局建模能力，同时针对医学图像的特殊特性（稀疏局部信息、模糊边界和高噪声）进行专门优化，设计一个轻量级但高效的医学图像分割网络。整体流程包括：1)输入图像经过ConvUtr块进行特征提取；2)通过LKLGL块进行局部和全局信息交互；3)使用轻量级Transformer进行长程依赖建模；4)最后通过级联解码器生成分割结果，解码过程中使用下采样跳跃连接融合不同层次特征，过滤冗余信息，突出边界信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)ConvUtr块 - 一种轻量级分层补丁嵌入，结合大核CNN和Transformer学习模式；2)LKLGL块 - 促进局部和全局信息交换的新型模块；3)级联解码器与下采样跳跃连接 - 特殊解码器结构，过滤冗余信息；4)针对医学图像特性的整体架构优化。相比之前工作，Mobile U-ViT专门针对医学图像特性优化，而非简单应用自然图像模型；巧妙结合CNN和Transformer优势；引入新组件解决特定挑战；整体设计更轻量，适合移动设备部署。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Mobile U-ViT通过创新的ConvUtr块和LKLGL模块，成功地将Transformer的全局建模能力与CNN的计算效率相结合，为医学图像分割提供了一个既轻量又高效的解决方案，特别适合在资源受限的移动设备上部署。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In clinical practice, medical image analysis often requires efficientexecution on resource-constrained mobile devices. However, existing mobilemodels-primarily optimized for natural images-tend to perform poorly on medicaltasks due to the significant information density gap between natural andmedical domains. Combining computational efficiency with medicalimaging-specific architectural advantages remains a challenge when developinglightweight, universal, and high-performing networks. To address this, wepropose a mobile model called Mobile U-shaped Vision Transformer (Mobile U-ViT)tailored for medical image segmentation. Specifically, we employ the newlypurposed ConvUtr as a hierarchical patch embedding, featuring aparameter-efficient large-kernel CNN with inverted bottleneck fusion. Thisdesign exhibits transformer-like representation learning capacity while beinglighter and faster. To enable efficient local-global information exchange, weintroduce a novel Large-kernel Local-Global-Local (LGL) block that effectivelybalances the low information density and high-level semantic discrepancy ofmedical images. Finally, we incorporate a shallow and lightweight transformerbottleneck for long-range modeling and employ a cascaded decoder withdownsample skip connections for dense prediction. Despite its reducedcomputational demands, our medical-optimized architecture achievesstate-of-the-art performance across eight public 2D and 3D datasets coveringdiverse imaging modalities, including zero-shot testing on four unseendatasets. These results establish it as an efficient yet powerful andgeneralization solution for mobile medical image analysis. Code is available athttps://github.com/FengheTan9/Mobile-U-ViT.</description>
      <author>example@mail.com (Fenghe Tang, Bingkun Nian, Jianrui Ding, Wenxin Ma, Quan Quan, Chengqi Dong, Jie Yang, Wei Liu, S. Kevin Zhou)</author>
      <guid isPermaLink="false">2508.01064v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>v-PuNNs: van der Put Neural Networks for Transparent Ultrametric Representation Learning</title>
      <link>http://arxiv.org/abs/2508.01010v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了van der Put神经网络(v-PuNNs)，一种基于p-adic数的新型深度学习架构，专门用于处理严格分层数据，在多个基准测试中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;传统深度学习模型将数据嵌入在欧几里得空间中，不适合处理如分类单元、词义或文件系统等严格分层的对象。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够精确表示和处理分层数据的深度学习架构，提供可解释且高效的模型。&lt;h4&gt;方法&lt;/h4&gt;提出van der Put神经网络(v-PuNNs)，其神经元是p-adic球的特征函数；在透明超度量表示学习(TURL)原则下，每个权重都是p-adic数；开发值自适应扰动优化(VAPO)方法解决离散空间中的梯度消失问题。&lt;h4&gt;主要发现&lt;/h4&gt;深度K的v-PuNN可普遍表示任何K级树；在WordNet名词、GO分子功能和NCBI哺乳动物分类等任务上取得最先进性能；学习到的度量是完全超度量的，具有特定的分形和信息论特性。&lt;h4&gt;结论&lt;/h4&gt;v-PuNNs连接了数论和深度学习，为分层数据提供了精确、可解释和高效的模型，并已扩展到量子系统和表格数据生成等应用。&lt;h4&gt;翻译&lt;/h4&gt;传统深度学习模型将数据嵌入在欧几里得空间中，对于严格分层的对象如分类单元、词义或文件系统来说，这不是一个好的拟合。我们引入了van der Put神经网络(v-PuNNs)，这是第一个神经元是p-adic球中的特征函数的架构。在我们的透明超度量表示学习(TURL)原则下，每个权重本身就是一个p-adic数，给出精确的子树语义。一个新的有限层次逼近定理表明，具有特定数量神经元的深度K的v-PuNN可以普遍表示任何K级树。由于梯度在离散空间中消失，我们提出了值自适应扰动优化(VAPO)，包括一个快速确定性变体和一个基于矩的变体。在三个标准基准测试中，我们的仅CPU实现设置了新的最先进水平：WordNet名词在短时间内达到高准确率；GO分子功能高准确率；NCBI哺乳动物与真实分类距离高度相关。学习到的度量是完全超度量的，并分析了其分形和信息论特性。除了分类外，作者还为量子系统推导了结构不变量和表格数据的可控生成代码。因此，v-PuNNs连接了数论和深度学习，为分层数据提供了精确、可解释和高效的模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional deep learning models embed data in Euclidean space$\mathbb{R}^d$, a poor fit for strictly hierarchical objects such as taxa, wordsenses, or file systems. We introduce van der Put Neural Networks (v-PuNNs),the first architecture whose neurons are characteristic functions of p-adicballs in $\mathbb{Z}_p$. Under our Transparent Ultrametric RepresentationLearning (TURL) principle every weight is itself a p-adic number, giving exactsubtree semantics. A new Finite Hierarchical Approximation Theorem shows that adepth-K v-PuNN with $\sum_{j=0}^{K-1}p^{\,j}$ neurons universally representsany K-level tree. Because gradients vanish in this discrete space, we proposeValuation-Adaptive Perturbation Optimization (VAPO), with a fast deterministicvariant (HiPaN-DS) and a moment-based one (HiPaN / Adam-VAPO). On threecanonical benchmarks our CPU-only implementation sets new state-of-the-art:WordNet nouns (52,427 leaves) 99.96% leaf accuracy in 16 min; GOmolecular-function 96.9% leaf / 100% root in 50 s; NCBI Mammalia Spearman $\rho= -0.96$ with true taxonomic distance. The learned metric is perfectlyultrametric (zero triangle violations), and its fractal andinformation-theoretic properties are analyzed. Beyond classification we derivestructural invariants for quantum systems (HiPaQ) and controllable generativecodes for tabular data (Tab-HiPaN). v-PuNNs therefore bridge number theory anddeep learning, offering exact, interpretable, and efficient models forhierarchical data.</description>
      <author>example@mail.com (Gnankan Landry Regis N'guessan)</author>
      <guid isPermaLink="false">2508.01010v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles</title>
      <link>http://arxiv.org/abs/2508.00969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MORPHEUS是一个统一的基于Transformer的预训练框架，能够将组织病理学和多组学数据编码到共享的潜在空间中，实现了跨模态学习和任意组学生成功能。&lt;h4&gt;背景&lt;/h4&gt;自监督学习在计算病理学中取得重大进展，使模型能从H&amp;E染色癌症组织中学习丰富表征。然而，仅靠组织病理学不足以进行分子表征和理解临床结果，重要信息包含在转录组学、甲基化组学或基因组学等高维组学谱中。&lt;h4&gt;目的&lt;/h4&gt;引入MORPHEUS框架，整合组织病理学和多组学数据，实现跨模态学习和预测功能。&lt;h4&gt;方法&lt;/h4&gt;MORPHEUS应用掩码建模目标于随机选择的组学部分，鼓励模型学习有生物学意义的跨模态关系。同一预训练网络可单独或组合使用不同模态，并支持从一种或多种组学推断其他组学数据。&lt;h4&gt;主要发现&lt;/h4&gt;在大型泛癌队列上预训练后，MORPHEUS在各种模态组合和任务中始终优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;MORPHEUS是肿瘤学中开发多模态基础模型的有前景框架，其代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习通过使模型能够从苏木精-伊红(H&amp;E)染色的癌症组织中学习丰富的表征，推动了计算病理学的主要进展。然而，仅靠组织病理学往往不足以进行分子表征和理解临床结果，因为重要信息包含在转录组学、甲基化组学或基因组学等高维组学谱中。在这项工作中，我们引入了MORPHEUS，一个统一的基于Transformer的预训练框架，将组织病理学和多组学数据编码到共享的潜在空间中。其核心是MORPHEUS依赖于应用于随机选择的组学部分的掩码建模目标，鼓励模型学习有生物学意义的跨模态关系。相同的预训练网络可以单独应用于组织病理学或与任何组学模态子集结合应用，无缝适应可用输入。此外，MORPHEUS支持任意到任意组学生成，能够从一个或多个组学谱推断任何模态子集，包括仅从H&amp;E推断。在大型泛癌队列上预训练后，MORPHEUS在各种模态组合和任务中始终优于最先进的方法，将自己定位为肿瘤学中开发多模态基础模型的有前景框架。代码可在以下网址获取：https://github.com/Lucas-rbnt/MORPHEUS&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning has driven major advances in computational pathologyby enabling models to learn rich representations from hematoxylin and eosin(H&amp;E)-stained cancer tissue. However, histopathology alone often falls shortfor molecular characterization and understanding clinical outcomes, asimportant information is contained in high-dimensional omics profiles liketranscriptomics, methylomics, or genomics. In this work, we introduce MORPHEUS,a unified transformer-based pre-training framework that encodes bothhistopathology and multi-omics data into a shared latent space. At its core,MORPHEUS relies on a masked modeling objective applied to randomly selectedomics portions, encouraging the model to learn biologically meaningfulcross-modal relationships. The same pre-trained network can be applied tohistopathology alone or in combination with any subset of omics modalities,seamlessly adapting to the available inputs. Additionally, MORPHEUS enablesany-to-any omics generation, enabling one or more omics profiles to be inferredfrom any subset of modalities, including H&amp;E alone. Pre-trained on a largepan-cancer cohort, MORPHEUS consistently outperforms state-of-the-art methodsacross diverse modality combinations and tasks, positioning itself as apromising framework for developing multimodal foundation models in oncology.The code is available at: https://github.com/Lucas-rbnt/MORPHEUS</description>
      <author>example@mail.com (Lucas Robinet, Ahmad Berjaoui, Elizabeth Cohen-Jonathan Moyal)</author>
      <guid isPermaLink="false">2508.00969v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning Unified User Quantized Tokenizers for User Representation</title>
      <link>http://arxiv.org/abs/2508.00956v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为U²QT的新型框架，用于多源用户表示学习，通过整合跨领域知识转移和异构领域的早期融合，解决了现有方法在统一表示框架、数据压缩的扩展性和存储问题以及跨任务泛化灵活性方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;多源用户表示学习在为网络平台（如支付宝）提供个性化服务方面起着关键作用。先前的研究采用后期融合策略组合异构数据源，但存在三个主要局限：缺乏统一的表示框架、数据压缩中的扩展性和存储问题，以及不灵活的跨任务泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有多源用户表示学习方法中的三个关键局限：缺乏统一表示框架、数据压缩的扩展性和存储问题、以及不灵活的跨任务泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出U²QT框架，采用两阶段架构：首先，因果Q-Former将领域特定特征投影到共享的因果表示空间，保留模态间依赖关系；其次，多视图RQ-VAE通过共享和特定于源的码本将因果嵌入离散化为紧凑的标记，实现高效存储同时保持语义连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，U²QT在各种下游任务中具有优势，在未来的行为预测和推荐任务中优于特定任务的基线，同时在存储和计算方面实现了效率提升。&lt;h4&gt;结论&lt;/h4&gt;统一的标记化框架能够与语言模型无缝集成，并支持工业规模的应用。&lt;h4&gt;翻译&lt;/h4&gt;多源用户表示学习在网络平台（例如支付宝）上实现个性化服务方面起着关键作用。虽然先前的研究采用了后期融合策略来组合异构数据源，但它们存在三个主要局限：缺乏统一的表示框架、数据压缩中的扩展性和存储问题，以及不灵活的跨任务泛化能力。为了解决这些挑战，我们提出了U²QT，一种将跨领域知识转移与异构领域早期融合相结合的新框架。我们的框架采用两阶段架构：首先，因果Q-Former将领域特定特征投影到共享的因果表示空间，以保留模态间依赖关系；其次，多视图RQ-VAE通过共享和特定于源的码本将因果嵌入离散化为紧凑的标记，实现高效存储同时保持语义连贯性。实验结果展示了U²QT在各种下游任务中的优势，在未来的行为预测和推荐任务中优于特定任务的基线，同时在存储和计算方面实现了效率提升。统一的标记化框架能够与语言模型无缝集成，并支持工业规模的应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-source user representation learning plays a critical role in enablingpersonalized services on web platforms (e.g., Alipay). While prior works haveadopted late-fusion strategies to combine heterogeneous data sources, theysuffer from three key limitations: lack of unified representation frameworks,scalability and storage issues in data compression, and inflexible cross-taskgeneralization. To address these challenges, we propose U^2QT (Unified UserQuantized Tokenizers), a novel framework that integrates cross-domain knowledgetransfer with early fusion of heterogeneous domains. Our framework employs atwo-stage architecture: first, a causal Q-Former projects domain-specificfeatures into a shared causal representation space to preserve inter-modalitydependencies; second, a multi-view RQ-VAE discretizes causal embeddings intocompact tokens through shared and source-specific codebooks, enabling efficientstorage while maintaining semantic coherence. Experimental results showcaseU^2QT's advantages across diverse downstream tasks, outperforming task-specificbaselines in future behavior prediction and recommendation tasks whileachieving efficiency gains in storage and computation. The unified tokenizationframework enables seamless integration with language models and supportsindustrial-scale applications.</description>
      <author>example@mail.com (Chuan He, Yang Chen, Wuliang Huang, Tianyi Zheng, Jianhu Chen, Bin Dou, Yice Luo, Yun Zhu, Baokun Wang, Yongchao Liu, Xing Fu, Yu Cheng, Chuntao Hong, Weiqiang Wang, Xin-Wei Yao)</author>
      <guid isPermaLink="false">2508.00956v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model</title>
      <link>http://arxiv.org/abs/2508.00955v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的多模态嵌入框架，通过分层嵌入提示模板和自觉困难负样本采样两个组件，成功将多模态大语言模型的生成性质适应到判别表示学习中，无需大规模对比预训练即可达到最先进性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型已成为通用嵌入任务的解决方案，但将其生成性质适应判别表示学习仍面临挑战。大规模对比预训练存在计算成本高和无法利用MLLM固有指令跟随能力等关键低效率问题。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提出一个高效框架用于通用多模态嵌入，解决多模态大语言模型在判别表示学习中的适应问题。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含两个协同组件的框架：1) 分层嵌入提示模板，采用两级指令架构强制模型生成判别性表示；2) 自觉困难负样本采样，利用模型自身理解高效挖掘挑战性负样本并过滤潜在假负样本。&lt;h4&gt;主要发现&lt;/h4&gt;1) 分层提示实现了与对比训练基线相当的零样本性能；2) 在MMEB基准测试上，通过简单的批次内负样本基线提升了4.8个百分点；3) 结合自觉困难负样本采样，无需对比预训练就达到了最先进性能。&lt;h4&gt;结论&lt;/h4&gt;该工作为将多模态大语言模型适应通用嵌入任务提供了有效且高效的途径，显著减少了训练时间。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型已成为通用嵌入任务的解决方案，但将其生成性质适应判别表示学习仍然是一个重大挑战。大规模对比预训练主导范式存在关键低效率问题，包括难以承受的计算成本和无法利用多模态大语言模型的固有指令跟随能力。为了克服这些限制，我们提出了一个用于通用多模态嵌入的高效框架，通过聚焦两个协同组件弥合这一差距。首先，我们的分层嵌入提示模板采用两级指令架构，强制模型生成判别性表示。基于这一坚实基础，我们的第二个组件——自觉困难负样本采样，通过利用模型自身的理解重新定义了微调过程，高效挖掘挑战性负样本同时主动过滤潜在假负样本。我们的全面实验表明，我们的分层提示实现了与对比训练基线相当的零样本性能，并通过在MMEB基准测试上将简单的批次内负样本基线提升4.8个百分点来增强微调过程。我们通过自觉困难负样本采样进一步提升了性能，无需对比预训练就达到了最先进性能。我们的工作为将多模态大语言模型适应通用嵌入任务提供了一条有效且高效的途径，显著减少了训练时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have emerged as a promising solutionfor universal embedding tasks, yet adapting their generative nature fordiscriminative representation learning remains a significant challenge. Thedominant paradigm of large-scale contrastive pre-training suffers from criticalinefficiencies, including prohibitive computational costs and a failure toleverage the intrinsic, instruction-following capabilities of MLLMs. Toovercome these limitations, we propose an efficient framework for universalmultimodal embeddings, which bridges this gap by centering on two synergisticcomponents. First, our hierarchical embedding prompt template employs atwo-level instruction architecture that forces the model to producediscriminative representations. Building on this strong foundation, our secondcomponent, self-aware hard negative sampling, redefines the fine-tuning processby leveraging the model's own understanding to efficiently mine challengingnegatives while actively filtering out potential false negatives. Ourcomprehensive experiments show that our hierarchical prompt achieves zero-shotperformance competitive with contrastively trained baselines and enhances thefine-tuning process by lifting a simple in-batch negative baseline by 4.8points on the MMEB benchmark. We further boost the performance via ourself-aware hard negative sampling, achieving the state-of-the-art performancewithout the contrative pre-training. Our work presents an effective andefficient pathway to adapt MLLMs for universal embedding tasks, significantlyreducing training time.</description>
      <author>example@mail.com (Yeong-Joon Ju, Seong-Whan Lee)</author>
      <guid isPermaLink="false">2508.00955v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A Moment Matching-Based Method for Sparse and Noisy Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2508.02187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于矩匹配的点云配准框架，在稀疏点和重噪声条件下实现了比传统方法更高的准确性和鲁棒性，并成功应用于4D雷达SLAM系统。&lt;h4&gt;背景&lt;/h4&gt;点云配准是机器人感知任务中的关键步骤，如同时定位与建图（SLAM）。在稀疏点和重噪声条件下，传统配准方法如迭代最近点（ICP）和正态分布变换（NDT）难以实现鲁棒且准确的配准。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理稀疏点和重噪声条件的鲁棒点云配准方法，提高SLAM系统在挑战性环境中的性能。&lt;h4&gt;方法&lt;/h4&gt;将点云视为从源帧和目标帧中观察到的同一分布的独立同分布样本，通过匹配广义高斯径向基矩来估计两帧之间的刚体变换，无需显式点对点对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;合成和真实数据集实验表明，该方法比现有方法具有更高的准确性和鲁棒性；集成到4D雷达SLAM系统后显著提高了定位性能，结果与基于激光雷达的系统相当。&lt;h4&gt;结论&lt;/h4&gt;矩匹配技术在稀疏和噪声场景中的鲁棒点云配准具有潜力，可为机器人感知任务提供有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;点云配准是机器人感知任务中的关键步骤，如同时定位与建图（SLAM）。在稀疏点和重噪声条件下，点云配准特别具有挑战性。传统配准方法，如迭代最近点（ICP）和正态分布变换（NDT），通常难以在这些条件下实现鲁棒且准确的配准。在本文中，我们提出了一种基于矩匹配的配准框架。特别是，点云被视为从源帧和目标帧中观察到的同一分布的独立同分布样本。然后，我们匹配从点云计算出的广义高斯径向基矩来估计两帧之间的刚体变换。此外，该方法不需要点云之间的显式点对点对应关系。我们进一步证明了所提出方法的一致性。在合成和真实世界数据集上的实验表明，我们的方法比现有方法实现了更高的准确性和鲁棒性。此外，我们将我们的框架集成到4D雷达SLAM系统中。所提出的方法显著提高了定位性能，并实现了与基于激光雷达的系统相当的结果。这些发现证明了矩匹配技术在稀疏和噪声场景中鲁棒点云配准的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准在稀疏点和噪声条件下的挑战问题。这个问题很重要，因为点云配准是机器人感知和自动驾驶中的关键步骤，而实际传感器数据常常是稀疏和含噪声的，特别是在使用4D雷达等传感器时。解决这一问题可以显著改善定位性能，使基于雷达的系统能够达到与基于LiDAR的系统相当的效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法如ICP和NDT在稀疏和噪声条件下的局限性，然后提出不依赖点对点对应关系或局部密度估计的配准框架。他们将点云视为从同一分布中抽取的样本，通过匹配广义矩来估计变换。作者借鉴了统计学中的矩匹配方法、广义矩估计理论、机器学习中的径向基函数、数值优化中的BFGS方法以及GPU并行计算技术，但将这些元素创新性地组合应用于点云配准问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云视为从同一环境对象分布中抽取的独立同分布样本，通过计算和匹配点云的广义矩(特别是使用高斯径向基函数构造的矩)来估计两个视图之间的刚性变换，不依赖于显式的点对点对应关系。整体流程包括：1)根据点云密度自适应选择高斯RBF核的中心点；2)计算目标点云的各阶矩；3)初始化变换参数；4)计算变换后源点云的矩并与目标点云矩比较；5)使用BFGS方法优化变换参数以最小化矩差异；6)迭代直至收敛并输出最终变换参数。整个流程通过CUDA并行计算实现加速。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出基于矩匹配的全新配准框架，不依赖点对点对应关系；2)首次证明了算法的统计一致性，提供了对噪声的稳健保证；3)设计了自适应核函数选择策略，根据点云密度选择中心点；4)实现了CUDA并行化加速。相比传统方法，它不依赖点对应关系和局部统计估计；相比基于特征的方法，它不依赖局部特征提取且计算效率更高；相比基于学习的方法，它不需要大规模训练数据且泛化能力更强；相比其他矩匹配方法，它专门针对点云设计并提供理论保证。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于矩匹配的点云配准方法，通过匹配高斯径向基函数构造的广义矩，实现了在稀疏和噪声条件下的稳健配准，无需显式的点对点对应关系，并在理论和实验上证明了其优越性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is a key step in robotic perception tasks, such asSimultaneous Localization and Mapping (SLAM). It is especially challenging inconditions with sparse points and heavy noise. Traditional registrationmethods, such as Iterative Closest Point (ICP) and Normal DistributionsTransform (NDT), often have difficulties in achieving a robust and accuratealignment under these conditions. In this paper, we propose a registrationframework based on moment matching. In particular, the point clouds areregarded as i.i.d. samples drawn from the same distribution observed in thesource and target frames. We then match the generalized Gaussian Radial Basismoments calculated from the point clouds to estimate the rigid transformationbetween two frames. Moreover, such method does not require explicitpoint-to-point correspondences among the point clouds. We further show theconsistency of the proposed method. Experiments on synthetic and real-worlddatasets show that our approach achieves higher accuracy and robustness thanexisting methods. In addition, we integrate our framework into a 4D Radar SLAMsystem. The proposed method significantly improves the localization performanceand achieves results comparable to LiDAR-based systems. These findingsdemonstrate the potential of moment matching technique for robust point cloudregistration in sparse and noisy scenarios.</description>
      <author>example@mail.com (Xingyi Li, Han Zhang, Ziliang Wang, Yukai Yang, Weidong Chen)</author>
      <guid isPermaLink="false">2508.02187v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Free-MoRef: Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference</title>
      <link>http://arxiv.org/abs/2508.02134v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  published in ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Free-MoRef的无训练方法，用于解决视频多模态大型语言模型在长视频理解中的上下文长度限制问题，通过分割和融合长视觉token序列，实现了高效的长视频理解。&lt;h4&gt;背景&lt;/h4&gt;Video-MLLM在视频理解任务中取得了显著进展，但受限于底层LLM的上下文长度，现有模型在长视频场景中表现不佳。常见解决方案如token压缩和流推理技术会牺牲特征粒度或推理效率。&lt;h4&gt;目的&lt;/h4&gt;高效实现对更长帧输入的全面理解，在不牺牲特征粒度和推理效率的情况下处理长视频场景。&lt;h4&gt;方法&lt;/h4&gt;受MoE启发，提出Free-MoRef方法：1)将视觉token重构为几个短序列作为多参考；2)引入MoRef-attention并行收集多参考块线索；3)在LLM阴影层后添加参考融合步骤，组合关键token形成最终混合推理序列。&lt;h4&gt;主要发现&lt;/h4&gt;在VideoMME、MLVU、LongVideoBench等数据集上，Free-MoRef能够在不压缩的情况下处理2-8倍更长的输入帧，在单个A100 GPU上保持即时响应，性能甚至超过专门训练的长视频MLLM。&lt;h4&gt;结论&lt;/h4&gt;Free-MoRef通过分割和融合长视觉token序列，在更低的计算成本下实现了显著性能提升，为长视频理解提供了高效有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视频多模态大型语言模型(Video-MLLM)在视频理解任务中取得了显著进展。然而，受限于底层LLM的上下文长度限制，现有Video-MLLM通常在长视频场景中表现不佳。为了理解扩展的输入帧，常见的解决方案包括token压缩和流推理技术，这些方法牺牲了特征粒度或推理效率。不同地，为了高效实现对更长帧输入的全面理解，我们从MoE中汲取灵感，提出了一种无训练方法Free-MoRef，该方法在一个推理过程中即时复用Video-MLLM的上下文感知能力。具体而言，Free-MoRef将视觉token重构为几个短序列作为多参考。随后，我们引入MoRef-attention，并行从多参考块中收集线索以总结统一的查询激活。在LLM的阴影层后，推导出一个参考融合步骤，组合来自并行块的关键token构成最终混合推理序列，这补偿了MoRef-attention中被忽略的跨参考视觉交互。通过分割和融合长视觉token序列，Free-MoRef在推理复用上下文长度方面以更低的计算成本实现了改进的性能，展现出强大的效率和有效性。在VideoMME、MLVU、LongVideoBench上的实验表明，Free-MoRef能够在不进行压缩的情况下，在单个A100 GPU上完全感知2倍到8倍更长的输入帧，同时保持即时响应，从而带来显著的性能提升，甚至超过了专门训练的长视频MLLM。代码可在https://github.com/wkfdb/Free-MoRef获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Multimodal Large Language Models~(Video-MLLM) have achieved remarkableadvancements in video understanding tasks. However, constrained by the contextlength limitation in the underlying LLMs, existing Video-MLLMs typicallyexhibit suboptimal performance on long video scenarios. To understand extendedinput frames, common solutions span token compression and streaming inferencetechniques, which sacrifice feature granularity or inference efficiency.Differently, to efficiently achieve comprehensive understanding of longer frameinputs, we draw ideas from MoE and propose a training-free approach\textbf{Free-MoRef}, which instantly multiplexes the context perceptioncapabilities of Video-MLLMs within one inference pass. Specifically, Free-MoRefreconstructs the vision tokens into several short sequences asmulti-references. Subsequently, we introduce MoRef-attention, which gathersclues from the multi-reference chunks in parallel to summarize unified queryactivations. After the shadow layers in LLMs, a reference fusion step isderived to compose a final mixed reasoning sequence with key tokens fromparallel chunks, which compensates the cross-reference vision interactions thatare neglected in MoRef-attention. By splitting and fusing the long vision tokensequences, Free-MoRef achieves improved performance under much lower computingcosts in reasoning multiplexed context length, demonstrating strong efficiencyand effectiveness. Experiments on VideoMME, MLVU, LongVideoBench show thatFree-MoRef achieves full perception of 2$\times$ to 8$\times$ longer inputframes without compression on a single A100 GPU while keeping instantresponses, thereby bringing significant performance gains, even surpassingdedicatedly trained long-video-MLLMs. Codes are available athttps://github.com/wkfdb/Free-MoRef</description>
      <author>example@mail.com (Kuo Wang, Quanlong Zheng, Junlin Xie, Yanhao Zhang, Jinguo Luo, Haonan Lu, Liang Lin, Fan Zhou, Guanbin Li)</author>
      <guid isPermaLink="false">2508.02134v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding</title>
      <link>http://arxiv.org/abs/2508.01875v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为StreamAgent的实时流视频理解方法，通过预测未来任务相关信息和优化记忆机制，提升了响应准确性和实时效率。&lt;h4&gt;背景&lt;/h4&gt;实时流视频理解在自动驾驶和智能监控等领域带来了挑战，需要基于动态演化的视觉内容进行持续感知、主动决策和响应式交互，这超越了传统离线视频处理的范畴。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法缺乏任务驱动规划和未来预测的问题，本文提出StreamAgent以实现主动和目标驱动的响应。&lt;h4&gt;方法&lt;/h4&gt;StreamAgent通过预期代理整合问题语义和历史观察，预测关键事件的时间进展，并将当前观察与预期未来证据对齐，同时调整感知动作。此外，设计了流式KV缓存记忆机制，构建分层记忆结构实现高效语义检索，减少存储开销。&lt;h4&gt;主要发现&lt;/h4&gt;在流式和长视频理解任务上的实验表明，该方法在响应准确性和实时效率方面优于现有方法，具有现实世界流式场景的实用价值。&lt;h4&gt;结论&lt;/h4&gt;StreamAgent通过预测未来时空区间和优化记忆机制，有效提升了实时流视频理解中的响应准确性和实时效率。&lt;h4&gt;翻译&lt;/h4&gt;实时流视频理解在自动驾驶和智能监控等领域带来了超越传统离线视频处理的挑战，需要基于动态演化的视觉内容进行持续感知、主动决策和响应式交互。然而，现有方法依赖于交替的感知-反应或异步触发，缺乏任务驱动的规划和未来预测，这限制了它们在演化视频流中的实时响应性和主动决策能力。为此，我们提出了StreamAgent，它可以预测包含未来任务相关信息的时空区间，以实现主动和目标驱动的响应。具体来说，我们通过提示预期代理来整合问题语义和历史观察，以预测关键事件的时间进展，将当前观察与预期的未来证据对齐，并随后调整感知动作（例如，关注任务相关区域或在后续帧中持续跟踪）。为了实现高效推理，我们设计了一个流式KV缓存记忆机制，它构建了一个分层记忆结构，用于选择性回忆相关标记，实现高效的语义检索，同时减少了传统KV缓存中存储所有标记的开销。在流式和长视频理解任务上的广泛实验表明，我们的方法在响应准确性和实时效率方面优于现有方法，突显了其在现实世界流式场景中的实际价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time streaming video understanding in domains such as autonomous drivingand intelligent surveillance poses challenges beyond conventional offline videoprocessing, requiring continuous perception, proactive decision making, andresponsive interaction based on dynamically evolving visual content. However,existing methods rely on alternating perception-reaction or asynchronoustriggers, lacking task-driven planning and future anticipation, which limitstheir real-time responsiveness and proactive decision making in evolving videostreams. To this end, we propose a StreamAgent that anticipates the temporalintervals and spatial regions expected to contain future task-relevantinformation to enable proactive and goal-driven responses. Specifically, weintegrate question semantics and historical observations through prompting theanticipatory agent to anticipate the temporal progression of key events, aligncurrent observations with the expected future evidence, and subsequently adjustthe perception action (e.g., attending to task-relevant regions or continuouslytracking in subsequent frames). To enable efficient inference, we design astreaming KV-cache memory mechanism that constructs a hierarchical memorystructure for selective recall of relevant tokens, enabling efficient semanticretrieval while reducing the overhead of storing all tokens in the traditionalKV-cache. Extensive experiments on streaming and long video understanding tasksdemonstrate that our method outperforms existing methods in response accuracyand real-time efficiency, highlighting its practical value for real-worldstreaming scenarios.</description>
      <author>example@mail.com (Haolin Yang, Feilong Tang, Linxiao Zhao, Xiang An, Ming Hu, Huifa Li, Xinlin Zhuang, Boqian Wang, Yifan Lu, Xiaofeng Zhang, Abdalla Swikir, Junjun He, Zongyuan Ge, Imran Razzak)</author>
      <guid isPermaLink="false">2508.01875v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>T-GRAG: A Dynamic GraphRAG Framework for Resolving Temporal Conflicts and Redundancy in Knowledge Retrieval</title>
      <link>http://arxiv.org/abs/2508.01680v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为T-GRAG（Temporal GraphRAG）的新框架，解决了现有GraphRAG方法在处理知识时间动态性方面的局限性，通过建模知识随时间的演变来提高检索增强生成的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在自然语言生成方面表现出色，但在知识密集型任务中受到内部知识过时或不完整的限制。检索增强生成（RAG）通过整合外部检索解决了这一问题，而GraphRAG通过结构化知识图谱和多跳推理进一步提高了性能。然而，现有的GraphRAG方法大多忽略了知识的时间动态性，导致时间模糊性、时间不敏感的检索和语义冗余等问题。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有GraphRAG方法在处理知识时间动态性方面的局限性，作者提出了T-GRAG，一个动态的、时间感知的检索增强生成框架，用于建模知识随时间的演变。&lt;h4&gt;方法&lt;/h4&gt;T-GRAG包含五个关键组件：1）时间知识图谱生成器，创建带时间戳的、演化的图结构；2）时间查询分解机制，将复杂的时间查询分解为可管理的子查询；3）三层交互检索器，在时间子图上逐步过滤和优化检索；4）源文本提取器，减少噪声；5）基于LLM的生成器，合成具有上下文和时间准确性的响应。作者还引入了Time-LongQA基准数据集，基于真实世界公司年报，用于测试在演化知识上的时间推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，在时间约束下，T-GRAG在检索准确性和响应相关性方面显著优于之前的RAG和GraphRAG基线，突显了为健壮的长文本问答建模知识演化的必要性。&lt;h4&gt;结论&lt;/h4&gt;T-GRAG通过考虑知识的时间动态性，有效解决了现有GraphRAG方法在处理时间相关知识时的局限性，显著提高了检索增强生成的性能。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在自然语言生成方面已展现出强大的性能，但由于内部知识过时或不完整，在知识密集型任务中仍然存在局限。检索增强生成通过整合外部检索解决了这一问题，而GraphRAG通过结构化知识图谱和多跳推理进一步增强了性能。然而，现有的GraphRAG方法大多忽略了知识的时间动态性，导致时间模糊性、时间不敏感的检索和语义冗余等问题。为了克服这些局限性，我们提出了T-GRAG，这是一个动态的、时间感知的RAG框架，用于建模知识随时间的演变。T-GRAG包含五个关键组件：时间知识图谱生成器，创建带时间戳的、演化的图结构；时间查询分解机制，将复杂的时间查询分解为可管理的子查询；三层交互检索器，在时间子图上逐步过滤和优化检索；源文本提取器，减少噪声；基于LLM的生成器，合成具有上下文和时间准确性的响应。我们还引入了Time-LongQA，这是一个基于真实世界公司年报的新颖基准数据集，用于测试在演化知识上的时间推理能力。大量实验表明，在时间约束下，T-GRAG在检索准确性和响应相关性方面显著优于之前的RAG和GraphRAG基线，突显了为健壮的长文本问答建模知识演化的必要性。我们的代码已公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated strong performance in naturallanguage generation but remain limited in knowle-  dge-intensive tasks due to outdated or incomplete internal knowledge.Retrieval-Augmented Generation (RAG) addresses this by incorporating externalretrieval, with GraphRAG further enhancing performance through structuredknowledge graphs and multi-hop reasoning. However, existing GraphRAG methodslargely ignore the temporal dynamics of knowledge, leading to issues such astemporal ambiguity, time-insensitive retrieval, and semantic redundancy. Toovercome these limitations, we propose Temporal GraphRAG (T-GRAG), a dynamic,temporally-aware RAG framework that models the evolution of knowledge overtime. T-GRAG consists of five key components: (1) a Temporal Knowledge GraphGenerator that creates time-stamped, evolving graph structures; (2) a TemporalQuery Decomposition mechanism that breaks complex temporal queries intomanageable sub-queries; (3) a Three-layer Interactive Retriever thatprogressively filters and refines retrieval across temporal subgraphs; (4) aSource Text Extractor to mitigate noise; and (5) a LLM-based Generator thatsynthesizes contextually and temporally accurate responses. We also introduceTime-LongQA, a novel benchmark dataset based on real-world corporate annualreports, designed to test temporal reasoning across evolving knowledge.Extensive experiments show that T-GRAG significantly outperforms prior RAG andGraphRAG baselines in both retrieval accuracy and response relevance undertemporal constraints, highlighting the necessity of modeling knowledgeevolution for robust long-text question answering. Our code is publiclyavailable on the T-GRAG</description>
      <author>example@mail.com (Dong Li, Yichen Niu, Ying Ai, Xiang Zou, Biqing Qi, Jianxing Liu)</author>
      <guid isPermaLink="false">2508.01680v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>E-VRAG: Enhancing Long Video Understanding with Resource-Efficient Retrieval Augmented Generation</title>
      <link>http://arxiv.org/abs/2508.01546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;E-VRAG是一种新颖高效的视频检索增强生成(RAG)框架，通过帧预过滤、轻量级VLM评分、全局统计分布的帧检索策略和多视角问答方案，显著降低了计算成本并提高了视频理解的准确性。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言模型(VLMs)通过利用跨模态推理能力在视频理解方面取得了实质性进展。然而，它们的有效性受到上下文窗口的限制以及处理包含数千帧的长视频所需的高计算成本的限制。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频RAG方法在检索效率和准确性之间难以平衡的问题，特别是在处理多样化和复杂视频内容时。&lt;h4&gt;方法&lt;/h4&gt;提出E-VRAG框架，包含：1)基于分层查询分解的帧预过滤方法；2)使用轻量级VLM进行帧评分；3)利用帧间分数全局统计分布的帧检索策略；4)多视角问答方案增强VLM对长视频上下文的理解能力。&lt;h4&gt;主要发现&lt;/h4&gt;E-VRAG在四个公共基准测试上实现了约70%的计算成本降低，同时比基线方法获得更高的准确性，且无需额外训练。&lt;h4&gt;结论&lt;/h4&gt;E-VRAG在提高视频RAG任务的效率和准确性方面是有效的。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型(VLMs)通过利用跨模态推理能力在视频理解方面取得了实质性进展。然而，它们的有效性受到上下文窗口的限制以及处理包含数千帧的长视频所需的高计算成本的限制。检索增强生成(RAG)通过仅选择最相关的帧作为输入来应对这一挑战，从而降低了计算负担。然而，现有的视频RAG方法难以平衡检索效率和准确性，特别是在处理多样化和复杂的视频内容时。为解决这些局限性，我们提出了E-VRAG，一种用于视频理解的新型高效视频RAG框架。我们首先应用基于分层查询分解的帧预过滤方法来消除无关帧，从而在数据层面降低计算成本。然后，我们使用轻量级VLM进行帧评分，进一步在模型层面降低计算成本。此外，我们提出了一种利用帧间分数全局统计分布的帧检索策略，以减轻使用轻量级VLM可能导致的性能下降。最后，我们为检索到的帧引入了多视角问答方案，增强了VLM从长视频上下文中提取和理解信息的能力。在四个公共基准上的实验表明，E-VRAG实现了比基线方法约70%的计算成本降低和更高的准确性，且无需额外训练。这些结果证明了E-VRAG在提高视频RAG任务的效率和准确性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have enabled substantial progress in videounderstanding by leveraging cross-modal reasoning capabilities. However, theireffectiveness is limited by the restricted context window and the highcomputational cost required to process long videos with thousands of frames.Retrieval-augmented generation (RAG) addresses this challenge by selecting onlythe most relevant frames as input, thereby reducing the computational burden.Nevertheless, existing video RAG methods struggle to balance retrievalefficiency and accuracy, particularly when handling diverse and complex videocontent. To address these limitations, we propose E-VRAG, a novel and efficientvideo RAG framework for video understanding. We first apply a framepre-filtering method based on hierarchical query decomposition to eliminateirrelevant frames, reducing computational costs at the data level. We thenemploy a lightweight VLM for frame scoring, further reducing computationalcosts at the model level. Additionally, we propose a frame retrieval strategythat leverages the global statistical distribution of inter-frame scores tomitigate the potential performance degradation from using a lightweight VLM.Finally, we introduce a multi-view question answering scheme for the retrievedframes, enhancing the VLM's capability to extract and comprehend informationfrom long video contexts. Experiments on four public benchmarks show thatE-VRAG achieves about 70% reduction in computational cost and higher accuracycompared to baseline methods, all without additional training. These resultsdemonstrate the effectiveness of E-VRAG in improving both efficiency andaccuracy for video RAG tasks.</description>
      <author>example@mail.com (Zeyu Xu, Junkang Zhang, Qiang Wang, Yi Liu)</author>
      <guid isPermaLink="false">2508.01546v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>ReasonAct: Progressive Training for Fine-Grained Video Reasoning in Small Models</title>
      <link>http://arxiv.org/abs/2508.01533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ReasonAct方法，通过三阶段训练过程增强小模型在视频理解中的细粒度时间推理能力，并在多个数据集上取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;近期多模态模型在视觉-语言任务上取得了进展，但小规模模型在视频理解所需的细粒度时间推理方面仍然存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法（ReasonAct）来增强小模型中的视频推理能力，使其能够更好地处理视频理解中的时间推理问题。&lt;h4&gt;方法&lt;/h4&gt;采用三阶段训练过程：首先仅使用文本进行基础推理训练，然后在视频上进行微调，最后使用时间感知的强化学习进行优化；基于时间组相对策略优化（T-GRPO）构建，融入时间一致性建模；提出生物力学启发的子动作分解机制，为组成动作阶段提供渐进式奖励。&lt;h4&gt;主要发现&lt;/h4&gt;在HMDB51、UCF-101和Kinetics-400数据集上，30亿参数模型分别达到67.2%、94.1%和78.9%的准确率，比基线分别提高了17.9、15.8和12.3个百分点；渐进式训练方法使小模型能够实现竞争性的视频推理性能，同时保持计算效率。&lt;h4&gt;结论&lt;/h4&gt;所提出的ReasonAct方法使小规模模型能够在视频理解任务上取得显著改进，通过三阶段训练和特定优化技术有效解决了小模型在时间推理方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;尽管最近的多模态模型在视觉-语言任务上取得了进展，但小规模变体在视频理解所需的细粒度时间推理方面仍然存在困难。我们引入了ReasonAct，一种通过三阶段训练过程增强小模型视频推理的方法：首先仅使用文本推理建立基础，然后在视频上进行微调，最后使用时间感知的强化学习进行优化。我们在时间组相对策略优化（T-GRPO）的基础上构建，通过在策略优化中融入时间一致性建模。我们还提出了一个生物力学启发的子动作分解机制，为组成动作阶段提供渐进式奖励。在HMDB51、UCF-101和Kinetics-400上的实验表明，我们的30亿参数模型分别实现了67.2%、94.1%和78.9%的准确率，比基线分别提高了17.9、15.8和12.3个百分点。消融研究验证了我们的渐进式训练方法使小模型能够实现竞争性的视频推理性能，同时保持计算效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While recent multimodal models have shown progress in vision-language tasks,small-scale variants still struggle with the fine-grained temporal reasoningrequired for video understanding. We introduce ReasonAct, a method thatenhances video reasoning in smaller models through a three-stage trainingprocess: first building a foundation with text-only reasoning, then fine-tuningon video, and finally refining with temporal-aware reinforcement learning. Webuild upon Temporal Group Relative Policy Optimization (T-GRPO) byincorporating temporal consistency modeling into policy optimization. We alsopropose a biomechanically-motivated sub-action decomposition mechanism thatprovides graduated rewards for constituent action phases. Through experimentson HMDB51, UCF-101, and Kinetics-400, our 3B-parameter model achieves 67.2%,94.1%, and 78.9% accuracy respectively, demonstrating improvements of 17.9,15.8, and 12.3 points over baselines. Ablation studies validate that ourprogressive training methodology enables smaller models to achieve competitivevideo reasoning performance while maintaining computational efficiency.</description>
      <author>example@mail.com (Jiaxin Liu, Zhaolu Kang)</author>
      <guid isPermaLink="false">2508.01533v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Frequency-Constrained Learning for Long-Term Forecasting</title>
      <link>http://arxiv.org/abs/2508.01508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种简单而有效的方法，通过频谱初始化和频率约束优化来增强深度学习模型的长期预测能力，特别关注捕捉时间序列中的周期性模式。&lt;h4&gt;背景&lt;/h4&gt;现实世界的时间序列通常表现出由物理规律、人类习惯或季节性周期引起的强周期性结构。然而，现代深度预测模型由于频谱偏差和缺乏频率感知的归纳先验，往往无法捕捉这些重复出现的模式。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够明确建模周期性的方法，通过注入频谱先验来增强深度时间模型的长期预测能力，使其能够更准确地捕捉时间序列中的重复模式。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种通过频谱初始化和频率约束优化来明确建模周期性的方法。具体包括：通过快速傅里叶变换(FFT)引导的坐标下降法提取主导低频分量；用这些分量初始化正弦嵌入；采用双速学习计划来训练期间保留有意义的频率结构。该方法与模型无关，可以无缝集成到现有的基于Transformer的架构中。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的现实世界基准上的广泛实验展示了持续的性能提升，特别是在长期预测方面。此外，在合成数据上，该方法能准确恢复真实频率，验证了其在捕捉潜在周期性模式方面的可解释性和有效性。&lt;h4&gt;结论&lt;/h4&gt;将频谱先验注入深度时间模型可以带来稳健和可解释的长期预测性能提升。该方法通过明确建模周期性，有效解决了现代深度预测模型在捕捉重复模式方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的许多时间序列表现出由物理规律、人类习惯或季节性周期引起的强周期性结构。然而，现代深度预测模型往往由于频谱偏差和缺乏频率感知的归纳先验而无法捕捉这些重复出现的模式。受此差距的启发，我们提出了一种简单而有效的方法，通过频谱初始化和频率约束优化来明确建模周期性，从而增强长期预测能力。具体而言，我们通过快速傅里叶变换(FFT)引导的坐标下降法提取主导低频分量，用这些分量初始化正弦嵌入，并采用双速学习计划来训练期间保留有意义的频率结构。我们的方法与模型无关，可以无缝集成到现有的基于Transformer的架构中。在多样化的现实世界基准上的广泛实验展示了持续的性能提升——特别是在长期预测方面——突显了将频谱先验注入深度时间模型对稳健和可解释的长期预测的好处。此外，在合成数据上，我们的方法能准确恢复真实频率，进一步验证了其在捕捉潜在周期性模式方面的可解释性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many real-world time series exhibit strong periodic structures arising fromphysical laws, human routines, or seasonal cycles. However, modern deepforecasting models often fail to capture these recurring patterns due tospectral bias and a lack of frequency-aware inductive priors. Motivated by thisgap, we propose a simple yet effective method that enhances long-termforecasting by explicitly modeling periodicity through spectral initializationand frequency-constrained optimization. Specifically, we extract dominantlow-frequency components via Fast Fourier Transform (FFT)-guided coordinatedescent, initialize sinusoidal embeddings with these components, and employ atwo-speed learning schedule to preserve meaningful frequency structure duringtraining. Our approach is model-agnostic and integrates seamlessly intoexisting Transformer-based architectures. Extensive experiments across diversereal-world benchmarks demonstrate consistent performance gains--particularly atlong horizons--highlighting the benefits of injecting spectral priors into deeptemporal models for robust and interpretable long-range forecasting. Moreover,on synthetic data, our method accurately recovers ground-truth frequencies,further validating its interpretability and effectiveness in capturing latentperiodic patterns.</description>
      <author>example@mail.com (Menglin Kong, Vincent Zhihao Zheng, Lijun Sun)</author>
      <guid isPermaLink="false">2508.01508v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>WinkTPG: An Execution Framework for Multi-Agent Path Finding Using Temporal Reasoning</title>
      <link>http://arxiv.org/abs/2508.01495v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为WinkTPG的多智能体路径规划执行框架，能够高效地为大量智能体生成无碰撞的可行速度曲线。&lt;h4&gt;背景&lt;/h4&gt;为大量智能体规划无碰撞路径是一个具有众多实际应用前景的挑战性问题。尽管多智能体路径查找(MAPF)的最新进展显示出 promising 的进展，但标准MAPF算法依赖于简化的运动动力学模型，导致智能体无法直接遵循生成的MAPF计划。&lt;h4&gt;目的&lt;/h4&gt;弥合MAPF算法生成的计划与实际智能体运动之间的差距，提出一种能够将MAPF计划转化为运动动力学可行计划的方法，同时考虑不确定性并保持无碰撞特性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了两种方法：运动动力学时序计划图规划(kTPG)，一种多智能体速度优化算法，能够高效地将MAPF计划转化为运动动力学可行的计划；以及基于窗口的kTPG(WinkTPG)，一种MAPF执行框架，使用基于窗口的机制增量式地优化MAPF计划，在执行过程中动态整合智能体信息以减少不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，WinkTPG可以在1秒内为多达1,000个智能体生成速度曲线，并且相比现有的MAPF执行方法，解决方案质量提高了高达51.7%。&lt;h4&gt;结论&lt;/h4&gt;WinkTPG是一种高效的多智能体路径规划执行框架，能够解决大规模智能体路径规划问题，显著提高解决方案质量。&lt;h4&gt;翻译&lt;/h4&gt;为大量智能体规划无碰撞路径是一个具有众多实际应用前景的挑战性问题。尽管多智能体路径查找(MAPF)的最新进展显示出 promising 的进展，但标准MAPF算法依赖于简化的运动动力学模型，这导致智能体无法直接遵循生成的MAPF计划。为了弥合这一差距，我们提出了运动动力学时序计划图规划(kTPG)，这是一种多智能体速度优化算法，能够高效地将MAPF计划转化为运动动力学可行的计划，同时考虑不确定性并保持无碰撞特性。基于kTPG，我们提出了基于窗口的kTPG(WinkTPG)，这是一种MAPF执行框架，使用基于窗口的机制增量式地优化MAPF计划，在执行过程中动态整合智能体信息以减少不确定性。实验表明，WinkTPG可以在1秒内为多达1,000个智能体生成速度曲线，并且相比现有的MAPF执行方法，解决方案质量提高了高达51.7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Planning collision-free paths for a large group of agents is a challengingproblem with numerous real-world applications. While recent advances inMulti-Agent Path Finding (MAPF) have shown promising progress, standard MAPFalgorithms rely on simplified kinodynamic models, preventing agents fromdirectly following the generated MAPF plan. To bridge this gap, we proposekinodynamic Temporal Plan Graph Planning (kTPG), a multi-agent speedoptimization algorithm that efficiently refines a MAPF plan into akinodynamically feasible plan while accounting for uncertainties and preservingcollision-freeness. Building on kTPG, we propose Windowed kTPG (WinkTPG), aMAPF execution framework that incrementally refines MAPF plans using awindow-based mechanism, dynamically incorporating agent information duringexecution to reduce uncertainty. Experiments show that WinkTPG can generatespeed profiles for up to 1,000 agents in 1 second and improves solution qualityby up to 51.7% over existing MAPF execution methods.</description>
      <author>example@mail.com (Jingtian Yan, Stephen F. Smith, Jiaoyang Li)</author>
      <guid isPermaLink="false">2508.01495v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Modeling high and low extremes with a novel dynamic spatio-temporal model</title>
      <link>http://arxiv.org/abs/2508.01481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新型动态时空模型，能够有效捕捉环境系统中的高值和低值极端事件，解决了现有模型在处理极端情况时的局限性。&lt;h4&gt;背景&lt;/h4&gt;近年来，极端环境事件（如严重风暴、干旱、热浪、山洪和物种突然崩溃）在地球-大气动态系统中变得更加普遍。&lt;h4&gt;目的&lt;/h4&gt;为了充分理解极端环境事件的潜在机制并增强相关决策的科学性，需要开发能够容纳极端情况的灵活模型。&lt;h4&gt;方法&lt;/h4&gt;引入了一类新的动态时空模型，使用具有不同尾部指数的重尾和轻尾分布混合来捕捉高值和低值极端情况，该框架能够灵活识别时空上的极端依赖性和独立性，并进行不确定性量化，同时支持缺失数据预测。&lt;h4&gt;主要发现&lt;/h4&gt;现有动态时空统计模型在假设高斯误差分布时难以捕捉极端情况，而当前空间极端模型大多假设时间独立性且仅关注多个位置的联合上尾。新模型通过混合分布有效解决了这些问题。&lt;h4&gt;结论&lt;/h4&gt;该新型动态时空模型能够更好地捕捉环境系统中的极端事件，通过美国中部每小时颗粒物再分析数据集验证了其有效性，为理解和应对极端环境事件提供了更好的工具。&lt;h4&gt;翻译&lt;/h4&gt;近年来，极端环境事件如严重风暴、干旱、热浪、山洪和物种突然崩溃在地球-大气动态系统中变得更加普遍。为了充分理解潜在机制并增强知情决策，需要一个能够容纳极端情况的灵活模型。现有的动态时空统计模型在假设高斯误差分布时，在捕捉极端情况方面存在局限性，而当前的空间极端模型大多假设时间独立性，并专注于两个或多个位置的联合上尾。在此，我们引入了一类新的动态时空模型，使用具有不同尾部指数的重尾和轻尾分布混合来捕捉高值和低值极端情况。我们的框架灵活地识别时空上的极端依赖性和独立性，进行不确定性量化，并支持缺失数据预测，与其他动态时空模型类似。我们使用美国中部每小时颗粒物再分析数据集的大数据集证明了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extreme environmental events such as severe storms, drought, heat waves,flash floods, and abrupt species collapse have become more prevalent in theearth-atmosphere dynamic system in recent years. In order to fully understandthe underlying mechanisms and enhance informed decision-making, a flexiblemodel capable of accommodating extremes is necessary. Existing dynamicspatio-temporal statistical models exhibit limitations in capturing extremeswhen assuming Gaussian error distributions, whereas the current models forspatial extremes mostly assume temporal independence and are focused on jointupper tails at two or more locations. Here, we introduce a new class of dynamicspatio-temporal models that capture both high and low extremes using a mixtureof heavy- and light-tailed distributions with varying tail indices. Ourframework flexibly identifies extremal dependence and independence in bothspace and time with uncertainty quantification and supports missing dataprediction, as in other dynamic spatio-temporal models. We demonstrate itseffectiveness using a large reanalysis dataset of hourly particulate matter inthe Central United States.</description>
      <author>example@mail.com (Myungsoo Yoo, Likun Zhang, Christopher K. Wikle, Thomas Opitz)</author>
      <guid isPermaLink="false">2508.01481v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Toward Using Machine Learning as a Shape Quality Metric for Liver Point Cloud Generation</title>
      <link>http://arxiv.org/abs/2508.02482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究使用机器学习方法评估生成肝脏形状的质量，作为专家评估的替代方案，提供可解释的反馈。&lt;h4&gt;背景&lt;/h4&gt;3D医学形状生成模型（如扩散模型）在合成多样化且解剖学合理的结构方面显示出潜力，但由于缺乏真实数据，质量评估具有挑战性。现有评估指标通常测量训练集和生成集之间的分布距离，而医学领域需要对每个生成的形状进行个体级别的质量评估，这需要大量的人工专家审查。&lt;h4&gt;目的&lt;/h4&gt;研究使用传统机器学习方法和PointNet作为替代的、可解释的方法来评估生成肝脏形状的质量。&lt;h4&gt;方法&lt;/h4&gt;从生成肝脏形状的表面采样点云，提取手工制作的几何特征，并训练一组监督机器学习和PointNet模型将肝脏形状分类为好或坏。然后训练这些模型作为代理判别器来评估生成模型产生的合成肝脏形状的质量。&lt;h4&gt;主要发现&lt;/h4&gt;基于机器学习的形状分类器不仅提供可解释的反馈，而且与专家评估相比还能提供补充性的见解。&lt;h4&gt;结论&lt;/h4&gt;机器学习分类器可以作为3D器官形状生成中的轻量级、任务相关的质量指标，支持医学形状建模中更透明和临床一致的评估协议。&lt;h4&gt;翻译&lt;/h4&gt;虽然3D医学形状生成模型（如扩散模型）在合成多样化且解剖学合理的结构方面显示出前景，但缺乏真实数据使得质量评估具有挑战性。现有的评估指标通常测量训练集和生成集之间的分布距离，而医学领域需要对每个生成的形状进行个体级别的质量评估，这需要大量的人工专家审查。在本文中，我们研究了使用传统机器学习方法和PointNet作为替代的、可解释的方法来评估生成肝脏形状的质量。我们从生成肝脏形状的表面采样点云，提取手工制作的几何特征，并训练一组监督机器学习和PointNet模型将肝脏形状分类为好或坏。然后训练这些模型作为代理判别器来评估生成模型产生的合成肝脏形状的质量。我们的结果表明，基于机器学习的形状分类器不仅提供可解释的反馈，而且与专家评估相比还能提供补充性的见解。这表明机器学习分类器可以作为3D器官形状生成中的轻量级、任务相关的质量指标，支持医学形状建模中更透明和临床一致的评估协议。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D医学生成肝脏形状的质量评估问题。由于缺乏真实值（ground truth），现有评估方法主要测量分布距离，而医学领域需要在个体层面评估每个生成形状，这需要劳动密集型的专家审查。这个问题很重要，因为高质量肝脏形状模型对手术规划、医学教育和研究至关重要，专家评估耗时且成本高，限制了生成模型的开发和迭代速度，自动化质量评估能加速医学形状建模流程并确保生成模型的临床适用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别专家评估是肝脏生成工作中的瓶颈，然后寻找机器学习替代方案。他们设计了两种方法：传统机器学习方法（提取几何特征后使用分类器）和深度学习方法（使用PointNet直接处理点云）。作者借鉴了现有工作：使用了现有的机器学习算法（如随机森林、SVM等）；应用了PointNet和PointNet++等点云处理经典架构；使用了计算机视觉和医学图像分析中常用的基本几何特征；采用了机器学习领域的标准评估方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用机器学习分类器作为肝脏形状质量的代理评估工具，替代或辅助专家评估。整体流程包括：1)数据准备（获取肝脏对象并由专家分类为'好'/'坏'）；2)点云创建（在每个肝脏表面采样20,000个点）；3)特征提取（传统ML方法提取14维几何特征）；4)模型训练（传统ML方法和PointNet/PointNet++分别训练）；5)模型评估（在测试集和生成集上评估）；6)可解释性分析（使用SHAP分析理解决策依据）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将机器学习分类器应用于3D医学生成形状的质量评估；提供了可解释的质量评估方法（通过SHAP分析）；展示了机器学习评估与专家评估的互补性；提出了轻量级、任务相关的质量指标。相比之前的工作，该方法专注于个体层面的质量评估而非分布距离；能学习比纯几何指标更复杂的形状特征；可以自动化评估过程而非依赖专家；传统ML方法提供了可解释的决策依据而非黑盒预测。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了一种基于机器学习的可解释方法，用于评估3D生成肝脏形状的质量，能够替代部分专家评估工作并提供与专家判断互补的见解，加速了医学形状生成模型的开发和评估流程。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While 3D medical shape generative models such as diffusion models have shownpromise in synthesizing diverse and anatomically plausible structures, theabsence of ground truth makes quality evaluation challenging. Existingevaluation metrics commonly measure distributional distances between trainingand generated sets, while the medical field requires assessing quality at theindividual level for each generated shape, which demands labor-intensive expertreview.  In this paper, we investigate the use of classical machine learning (ML)methods and PointNet as an alternative, interpretable approach for assessingthe quality of generated liver shapes. We sample point clouds from the surfacesof the generated liver shapes, extract handcrafted geometric features, andtrain a group of supervised ML and PointNet models to classify liver shapes asgood or bad. These trained models are then used as proxy discriminators toassess the quality of synthetic liver shapes produced by generative models.  Our results show that ML-based shape classifiers provide not onlyinterpretable feedback but also complementary insights compared to expertevaluation. This suggests that ML classifiers can serve as lightweight,task-relevant quality metrics in 3D organ shape generation, supporting moretransparent and clinically aligned evaluation protocols in medical shapemodeling.</description>
      <author>example@mail.com (Khoa Tuan Nguyen, Gaeun Oh, Ho-min Park, Francesca Tozzi, Wouter Willaert, Joris Vankerschaver, Niki Rashidian, Wesley De Neve)</author>
      <guid isPermaLink="false">2508.02482v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT Reconstruction</title>
      <link>http://arxiv.org/abs/2508.02408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GR-Gaussian是一种基于图的3D高斯飞溅框架，通过两个关键创新策略有效解决了稀疏视图CT重建中的针状伪影问题，提高了重建准确性。&lt;h4&gt;背景&lt;/h4&gt;3D高斯飞溅(3DGS)已成为CT重建的一种有前景的方法，但现有方法依赖于视图内点的平均梯度幅度，这在稀疏视图条件下常导致严重的针状伪影。&lt;h4&gt;目的&lt;/h4&gt;解决稀疏视图条件下的针状伪影问题，提高重建准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了GR-Gaussian框架，包含两个关键创新：(1)去噪点云初始化策略，减少初始化误差并加速收敛；(2)像素图感知梯度策略，使用基于图的密度差异改进梯度计算，提高分割精度和密度表示。&lt;h4&gt;主要发现&lt;/h4&gt;在X-3D和真实世界数据集上的实验验证了GR-Gaussian的有效性，实现了PSNR提高0.67 dB和0.92 dB，SSIM提高0.011和0.021。&lt;h4&gt;结论&lt;/h4&gt;GR-Gaussian适用于在具有挑战性的稀疏视图条件下进行准确的CT重建。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯飞溅(3DGS)已成为一种有前景的CT重建方法。然而，现有方法依赖于视图内点的平均梯度幅度，常导致稀疏视图条件下出现严重的针状伪影。为应对这一挑战，我们提出了GR-Gaussian，一种基于图的3D高斯飞溅框架，可在稀疏视图条件下抑制针状伪影并提高重建准确性。我们的框架引入了两个关键创新：(1)去噪点云初始化策略，减少初始化误差并加速收敛；(2)像素图感知梯度策略，利用基于图的密度差异改进梯度计算，提高分割精度和密度表示。在X-3D和真实世界数据集上的实验验证了GR-Gaussian的有效性，实现了PSNR提高0.67 dB和0.92 dB，SSIM提高0.011和0.021。这些结果突显了GR-Gaussian在具有挑战性的稀疏视图条件下进行准确CT重建的适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决稀疏视角CT重建中的针状伪影问题。这个问题在现实中非常重要，因为在医学CT等领域减少X射线投影角度可以降低患者辐射风险，在电子断层扫描中受限于旋转角度，而传统方法在这些条件下会产生严重伪影，影响诊断准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析发现现有3D高斯飞溅方法在CT重建中产生针状伪影的原因是保留了小梯度的大高斯核，缺乏对点间关系的考虑。因此，他们引入图结构建模点间关系，并借鉴了3D高斯飞溅技术、图神经网络思想，同时改进了传统FDK方法和像素感知梯度概念，设计了去噪点云初始化和像素图感知梯度策略两个核心创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图结构表示对象点间关系，通过密度差异改进梯度计算，并在初始化阶段就去噪。流程包括：1)初始化阶段：用改进FDK生成初始体积，高斯滤波去噪，随机采样点位置，用KNN构建图结构；2)训练阶段：稀疏视角投影光栅化计算损失，应用像素图感知梯度策略优化高斯核分裂，使用包含光度损失、正则化的总损失函数进行优化；3)重建阶段：优化后的图结构辐射高斯表示进行体素化获取最终密度体积。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)图结构辐射高斯表示；2)去噪点云初始化策略；3)像素图感知梯度策略。相比传统方法，GR-Gaussian能抑制条纹伪影；相比迭代优化方法，更高效且保留更多细节；相比深度学习方法，无需大量标记数据；相比NeRF方法，渲染速度更快；特别相比现有3DGS方法，有效解决了稀疏视角CT中的针状伪影问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了GR-Gaussian框架，通过图结构和改进的初始化与梯度策略，有效解决了稀疏视角CT重建中的针状伪影问题，显著提高了重建质量和准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has emerged as a promising approach for CTreconstruction. However, existing methods rely on the average gradientmagnitude of points within the view, often leading to severe needle-likeartifacts under sparse-view conditions. To address this challenge, we proposeGR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppressesneedle-like artifacts and improves reconstruction accuracy under sparse-viewconditions. Our framework introduces two key innovations: (1) a Denoised PointCloud Initialization Strategy that reduces initialization errors andaccelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy thatrefines gradient computation using graph-based density differences, improvingsplitting accuracy and density representation. Experiments on X-3D andreal-world datasets validate the effectiveness of GR-Gaussian, achieving PSNRimprovements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. Theseresults highlight the applicability of GR-Gaussian for accurate CTreconstruction under challenging sparse-view conditions.</description>
      <author>example@mail.com (Yikuang Yuluo, Yue Ma, Kuan Shen, Tongtong Jin, Wang Liao, Yangpu Ma, Fuquan Wang)</author>
      <guid isPermaLink="false">2508.02408v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at T-Junctions Utilizing Road Layout Extraction via Camera</title>
      <link>http://arxiv.org/abs/2508.02348v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新框架，通过相机推断的道路布局来解释雷达点云，实现非视距区域行人的定位。&lt;h4&gt;背景&lt;/h4&gt;在城市环境中，非视距区域的行人定位对自动驾驶系统构成重大挑战。毫米波雷达容易受多径反射影响而失真，相机图像则缺乏深度感知能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新框架，通过从相机推断的道路布局来解释雷达点云，实现NLoS区域行人的定位。&lt;h4&gt;方法&lt;/h4&gt;利用相机的视觉信息来解释2D雷达点云，实现空间场景重建。&lt;h4&gt;主要发现&lt;/h4&gt;通过在真实车辆上安装的雷达-相机系统进行实验验证，使用在户外NLoS驾驶环境中收集的数据集评估定位性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在NLoS行人定位中具有实际适用性。&lt;h4&gt;翻译&lt;/h4&gt;在城市环境中，非视距区域的行人定位对自动驾驶系统构成重大挑战。虽然毫米波雷达已显示出在这种场景中检测物体的潜力，但2D雷达点云数据容易受多径反射引起的失真影响，使得准确的空间推断变得困难。此外，尽管相机图像能提供高分辨率的视觉信息，但它们缺乏深度感知能力，无法直接观察到非视距区域的物体。在本文中，我们提出了一种新框架，该框架通过从相机推断的道路布局来解释雷达点云，以实现非视距行人的定位。所提出的方法利用相机的视觉信息来解释2D雷达点云，从而实现空间场景重建。通过在真实车辆上安装的雷达-相机系统进行的实验验证了所提出方法的有效性。使用在户外非视距驾驶环境中收集的数据集评估了定位性能，证明了该方法的实际适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在城市环境中非视距(NLoS)区域内行人定位的问题。这个问题在现实中非常重要，因为根据美国交通部数据，2022年交叉口交通事故导致12,036人死亡。传统的传感器只能检测可见区域(LoS)的物体，无法被建筑物或围栏遮挡的行人，这对自动驾驶系统构成重大安全风险。现有的V2X通信解决方案又缺乏基础设施支持且成本高昂。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到需要能反射和衍射信号的传感器如毫米波雷达来解决NLoS检测问题。他们意识到单独使用雷达点云存在稀疏、噪声大和多路径反射导致位置失真等挑战；而相机虽提供高分辨率视觉信息但缺乏深度感知。因此，他们设计了一种融合方法，结合雷达的精确距离测量和相机的视觉信息。该方法借鉴了现有的相机BEV变换技术、多路径反射模型和雷达-相机融合技术，但进行了创新改进以适应NLoS场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是融合毫米波雷达和相机图像的优势，利用相机提供的道路布局信息来解释雷达点云数据，从而推断遮挡空间的结构并定位NLoS行人。整体流程分为两部分：1)空间配置推断：从相机提取道路布局，对齐雷达与相机数据，对静态点进行分类和射线追踪，重建空间环境；2)NLoS行人定位：对动态点进行射线追踪，通过过滤和聚类处理噪声，使用DBSCAN算法估计行人实际位置。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出毫米波雷达与相机融合的新框架用于T型路口NLoS行人定位；2)利用相机图像解释2D雷达点云实现准确空间推理；3)在真实世界室外NLoS环境中验证方法有效性。相比之前工作，本文方法不仅适用于室外复杂场景，还能估计反射器位置并利用多反射生成的雷达点，而之前的方法或仅适用于室内环境，或无法重建空间结构，或不考虑多反射情况，导致定位准确性受限。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的毫米波雷达与相机融合方法，通过利用相机推断的道路布局解释雷达点云，实现了在T型路口非视距区域内行人的准确定位，显著提高了自动驾驶系统在复杂城市环境中的感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urbanenvironments poses a significant challenge for autonomous driving systems.While mmWave radar has demonstrated potential for detecting objects in suchscenarios, the 2D radar point cloud (PCD) data is susceptible to distortionscaused by multipath reflections, making accurate spatial inference difficult.Additionally, although camera images provide high-resolution visualinformation, they lack depth perception and cannot directly observe objects inNLoS regions. In this paper, we propose a novel framework that interprets radarPCD through road layout inferred from camera for localization of NLoSpedestrians. The proposed method leverages visual information from the camerato interpret 2D radar PCD, enabling spatial scene reconstruction. Theeffectiveness of the proposed approach is validated through experimentsconducted using a radar-camera system mounted on a real vehicle. Thelocalization performance is evaluated using a dataset collected in outdoor NLoSdriving environments, demonstrating the practical applicability of the method.</description>
      <author>example@mail.com (Byeonggyu Park, Hee-Yeun Kim, Byonghyok Choi, Hansang Cho, Byungkwan Kim, Soomok Lee, Mingu Jeon, Seong-Woo Kim)</author>
      <guid isPermaLink="false">2508.02348v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>AID4AD: Aerial Image Data for Automated Driving Perception</title>
      <link>http://arxiv.org/abs/2508.02140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出AID4AD数据集，将高分辨率航空图像精确对齐到nuScenes数据集的局部坐标系，展示了航空图像在自动驾驶车辆感知任务中的实用价值，实验证明其可提高地图构建准确度和轨迹预测性能。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆需要准确的环境感知和地图信息，但高清地图在某些场景下可能不可用、过时或维护成本高，需要寻找替代或补充的环境信息源。&lt;h4&gt;目的&lt;/h4&gt;开发一种将航空图像与地面车辆数据集精确对齐的方法，创建公开可用的数据集(AID4AD)，并验证航空图像在自动驾驶感知任务中的实用价值。&lt;h4&gt;方法&lt;/h4&gt;使用基于SLAM的点云地图将航空图像与nuScenes局部坐标系对齐，提出对齐工作流程校正定位和投影失真，通过手动质量控制识别高质量对齐作为地面真相，并在在线地图构建和运动预测两个任务中评估数据集。&lt;h4&gt;主要发现&lt;/h4&gt;航空图像作为补充输入可提高地图构建过程；作为结构化环境表示可替代高清地图；使地图构建准确度提高15-23%；使轨迹预测性能提高2%。&lt;h4&gt;结论&lt;/h4&gt;航空图像是自动驾驶系统中一种可扩展和适应性强的环境上下文来源，特别是在高清地图不可用、过时或维护成本高的场景中具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;本研究研究了将空间对齐的航空图像集成到自动驾驶车辆(AVs)的感知任务中。作为核心贡献，我们提出了AID4AD，一个公开可用的数据集，通过将高分辨率航空图像精确对齐到nuScenes数据集的局部坐标系来增强该数据集。对齐是使用nuScenes提供的基于SLAM的点云地图执行的，建立了航空数据与nuScenes局部坐标系之间的直接联系。为确保空间保真度，我们提出了一种对齐工作流程，校正定位和投影失真。手动质量控制过程进一步通过识别一组高质量对齐来完善数据集，我们将其作为地面真相发布，以支持未来关于自动注册的研究。我们在两个代表性任务中展示了AID4AD的实际价值：在在线地图构建中，航空图像作为补充输入改善了地图构建过程；在运动预测中，它作为结构化环境表示替代了高清地图。实验表明，航空图像使地图构建准确度提高了15-23%，轨迹预测性能提高了2%。这些结果突显了航空图像作为自动驾驶系统中可扩展和适应性强的环境上下文来源的潜力，特别是在高清地图不可用、过时或维护成本高的场景中。AID4AD以及评估代码和预训练模型已公开发布，以促进该方向的进一步研究：https://github.com/DriverlessMobility/AID4AD。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何将航空影像有效集成到自动驾驶汽车感知系统中的问题，作为高清地图的替代或补充方案。这个问题很重要，因为当前自动驾驶系统依赖的高清地图维护成本高、更新困难，特别是在动态变化的城市环境中，而航空影像提供了一种可扩展、近乎实时更新的环境上下文来源，可用于高清地图不可用或过时的场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到现有数据集如SatforHDMap和OpenSatMap在空间对齐和时间匹配方面的局限性，缺乏公开的精确对齐航空影像数据集。他们选择nuScenes作为基础数据集，因为它在规模、传感器覆盖和定位精度间提供了最佳平衡。方法设计上，他们开发了专门的航空影像对齐工作流程，使用基于SLAM的点云地图解决空间不匹配问题，并借鉴了现有的地图构建和运动预测算法，但解决了之前工作的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是航空影像可作为结构化、可扩展的环境参考源，通过精确空间对齐可补充或替代高清地图。整体流程包括：1)收集与nuScenes时期匹配的高分辨率航空影像；2)开发对齐工作流程，计算偏移网格解决空间差异；3)使用互信息最大化自动估计偏移量；4)应用预处理提高结构一致性；5)进行手动质量控制；6)生成车辆为中心的影像裁剪；7)将偏移量聚合到空间网格并插值；8)在地图构建和运动预测任务中评估效果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)AID4AD数据集-首个公开精确对齐的航空影像数据集；2)对齐工作流程-使用SLAM点云地图解决定位和投影畸变；3)应用评估-证明航空影像在地图构建提升15-23%精度，运动预测提升2%性能。相比之前工作，AID4AD实现了更高空间对齐精度(0.16m vs 1.46m/2.80m)，更高分辨率(0.15m/像素)，更好的时间匹配，并提供地面真实对齐数据，解决了现有数据集的空间不匹配和时间不一致问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AID4AD数据集通过将高分辨率航空影像与nuScenes精确对齐，为自动驾驶感知提供了一种可扩展、适应性强的环境上下文来源，实验证明其在地图构建和运动预测任务中显著提升了性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work investigates the integration of spatially aligned aerial imageryinto perception tasks for automated vehicles (AVs). As a central contribution,we present AID4AD, a publicly available dataset that augments the nuScenesdataset with high-resolution aerial imagery precisely aligned to its localcoordinate system. The alignment is performed using SLAM-based point cloud mapsprovided by nuScenes, establishing a direct link between aerial data andnuScenes local coordinate system. To ensure spatial fidelity, we propose analignment workflow that corrects for localization and projection distortions. Amanual quality control process further refines the dataset by identifying a setof high-quality alignments, which we publish as ground truth to support futureresearch on automated registration. We demonstrate the practical value ofAID4AD in two representative tasks: in online map construction, aerial imageryserves as a complementary input that improves the mapping process; in motionprediction, it functions as a structured environmental representation thatreplaces high-definition maps. Experiments show that aerial imagery leads to a15-23% improvement in map construction accuracy and a 2% gain in trajectoryprediction performance. These results highlight the potential of aerial imageryas a scalable and adaptable source of environmental context in automatedvehicle systems, particularly in scenarios where high-definition maps areunavailable, outdated, or costly to maintain. AID4AD, along with evaluationcode and pretrained models, is publicly released to foster further research inthis direction: https://github.com/DriverlessMobility/AID4AD.</description>
      <author>example@mail.com (Daniel Lengerer, Mathias Pechinger, Klaus Bogenberger, Carsten Markgraf)</author>
      <guid isPermaLink="false">2508.02140v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>On-the-Fly Object-aware Representative Point Selection in Point Cloud</title>
      <link>http://arxiv.org/abs/2508.01980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种用于点云降采样的代表性点选择框架，能够在自动驾驶车辆应用中有效存储和处理大量点云数据，同时保留关键物体相关信息并过滤无关背景点。&lt;h4&gt;背景&lt;/h4&gt;点云对物体建模和自动驾驶车辆的驾驶任务辅助至关重要。然而，自动驾驶车辆产生的大量数据给存储、带宽和处理成本带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种点云降采样方法，能够在减少数据量的同时保留关键物体相关信息，有效过滤无关背景点，解决自动驾驶车辆数据处理中的存储、带宽和处理成本问题。&lt;h4&gt;方法&lt;/h4&gt;该方法包含两个步骤：1) 物体存在检测：引入无监督的基于密度峰值的分类器和监督的朴素贝叶斯分类器来处理不同场景；2) 采样预算分配：提出一种选择物体相关点同时保持高物体信息保留率的策略。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI和nuScenes数据集上的大量实验表明，该方法在不同采样率下，在效率和效果方面均持续优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;作为一种与模型无关的解决方案，该方法能够与各种下游模型无缝集成，成为自动驾驶应用中3D点云降采样工具包中有价值且可扩展的补充。&lt;h4&gt;翻译&lt;/h4&gt;点云对于物体建模至关重要，并在辅助自动驾驶车辆(AVs)的驾驶任务中发挥关键作用。然而，自动驾驶车辆产生的大量数据给存储、带宽和处理成本带来了挑战。为了应对这些挑战，我们提出了一种用于点云降采样的代表性点选择框架，该方法在保留关键物体相关信息的同时，有效过滤了无关的背景点。我们的方法包括两个步骤：(1)物体存在检测，我们引入了一种无监督的基于密度峰值的分类器和一种监督的朴素贝叶斯分类器来处理不同场景；(2)采样预算分配，我们提出了一种选择物体相关点同时保持高物体信息保留率的策略。在KITTI和nuScenes数据集上的大量实验表明，我们的方法在不同采样率下，在效率和效果方面均持续优于最先进的基线方法。作为一种与模型无关的解决方案，我们的方法能够与各种下游模型无缝集成，使其成为自动驾驶应用中3D点云降采样工具包中有价值且可扩展的补充。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶车辆生成的点云数据量过大带来的存储、带宽和处理成本挑战。每小时约5TB的数据量超过了5G网络的处理能力，且高性能GPU处理成本高昂。这个问题在现实中很重要，因为它直接影响自动驾驶系统的实时性能和部署成本，同时限制了大规模数据处理和模型训练的可行性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析点云中物体点比背景点更重要的特性，设计了两阶段方法：物体存在检测和采样预算分配。借鉴了密度峰值聚类思想用于物体检测，以及朴素贝叶斯分类器进行监督学习。同时结合了随机采样和FPS（最远点采样）的优点，但做了改进以提高效率。作者特别关注了方法的泛化能力，使其能适用于不同的下游检测模型，而非仅针对特定模型优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是物体感知的点云降采样，优先保留物体相关的点，过滤掉不相关的背景点。整体流程分为两步：1)物体存在检测：使用无监督的密度峰值分类器或监督的朴素贝叶斯分类器将点分为物体点和背景点；2)采样预算分配：为物体点和背景点分配不同采样比例（如7:3或8:2），对物体点使用更精细的采样策略，对背景点使用高效随机采样。这种方法在保持物体检测精度的同时，显著减少了数据量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)物体感知的点云降采样框架，明确区分物体点和背景点；2)两阶段处理方法，结合无监督和监督两种物体检测方案；3)基于统计特征的物体检测，无需深度学习特征提取；4)分层过滤策略，平衡效率和效果。相比之前工作，本方法泛化能力强（可与各种下游模型集成），计算效率更高（比FPS快约50倍），且在低采样率下对小物体检测效果更好，解决了现有方法要么效率低、要么泛化能力差的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种高效且通用的物体感知点云降采样方法，通过两阶段处理在保持物体检测精度的同时显著减少了点云数据量，解决了自动驾驶系统中大规模点云数据的存储、传输和处理挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds are essential for object modeling and play a critical role inassisting driving tasks for autonomous vehicles (AVs). However, the significantvolume of data generated by AVs creates challenges for storage, bandwidth, andprocessing cost. To tackle these challenges, we propose a representative pointselection framework for point cloud downsampling, which preserves criticalobject-related information while effectively filtering out irrelevantbackground points. Our method involves two steps: (1) Object PresenceDetection, where we introduce an unsupervised density peak-based classifier anda supervised Na\"ive Bayes classifier to handle diverse scenarios, and (2)Sampling Budget Allocation, where we propose a strategy that selectsobject-relevant points while maintaining a high retention rate of objectinformation. Extensive experiments on the KITTI and nuScenes datasetsdemonstrate that our method consistently outperforms state-of-the-art baselinesin both efficiency and effectiveness across varying sampling rates. As amodel-agnostic solution, our approach integrates seamlessly with diversedownstream models, making it a valuable and scalable addition to the 3D pointcloud downsampling toolkit for AV applications.</description>
      <author>example@mail.com (Xiaoyu Zhang, Ziwei Wang, Hai Dong, Zhifeng Bao, Jiajun Liu)</author>
      <guid isPermaLink="false">2508.01980v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Rate-distortion Optimized Point Cloud Preprocessing for Geometry-based Point Cloud Compression</title>
      <link>http://arxiv.org/abs/2508.01633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的预处理框架，结合面向压缩的体素化网络和可微的G-PCC代理模型，显著提高了基于几何的点云压缩标准的效率，同时保持其互操作性和计算灵活性，实现了38.84%的平均BD-rate降低。&lt;h4&gt;背景&lt;/h4&gt;基于几何的点云压缩(G-PCC)是由MPEG设计的国际标准，为压缩各种类型的点云提供了通用框架，并确保了应用和设备之间的互操作性。然而，与最近的基于深度学习的点云压缩方法相比，G-PCC的性能较差，尽管它的计算功耗较低。&lt;h4&gt;目的&lt;/h4&gt;为了提高G-PCC的效率而不牺牲其互操作性或计算灵活性，研究人员提出了一种新的预处理框架，旨在将深度学习与传统的G-PCC标准相结合，实现性能提升的同时保持向后兼容性。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种结合压缩导向的体素化网络和可微的G-PCC代理模型的预处理框架。这两个组件在训练阶段进行联合优化。代理模型模仿非可微G-PCC编解码器的率失真行为，实现端到端的梯度传播。多功能的体素化网络使用基于学习的体素化自适应转换输入点云，并通过全局缩放、细粒度剪枝和点级编辑有效操作点云，以实现率失真权衡。在推理阶段，只有轻量级的体素化网络被附加到G-PCC编码器上，不需要修改解码器。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，该方法比G-PCC实现了38.84%的平均BD-rate降低。通过将传统编解码器与深度学习相结合，这项工作为增强传统压缩标准同时保持其向后兼容性提供了实际途径。&lt;h4&gt;结论&lt;/h4&gt;这项研究成功地将深度学习与传统G-PCC标准相结合，通过创新的预处理框架实现了显著的性能提升，同时保持了标准的互操作性和计算灵活性。这种方法为增强现有压缩标准提供了实用途径，特别适合实际部署场景。&lt;h4&gt;翻译&lt;/h4&gt;基于几何的点云压缩(G-PCC)是由MPEG设计的国际标准，为压缩各种类型的点云提供了通用框架，同时确保了应用和设备之间的互操作性。然而，与最近的基于深度学习的点云压缩方法相比，G-PCC的性能较差，尽管它的计算功耗较低。为了提高G-PCC的效率而不牺牲其互操作性或计算灵活性，我们提出了一种新颖的预处理框架，该框架集成了面向压缩的体素化网络和可微的G-PCC代理模型，在训练阶段进行联合优化。代理模型模仿非可微G-PCC编解码器的率失真行为，实现端到端的梯度传播。多功能的体素化网络使用基于学习的体素化自适应转换输入点云，并通过全局缩放、细粒度剪枝和点级编辑有效操作点云，以实现率失真权衡。在推理阶段，只有轻量级的体素化网络被附加到G-PCC编码器上，不需要修改解码器，因此不会给最终用户带来计算开销。广泛的实验表明，该方法比G-PCC实现了38.84%的平均BD-rate降低。通过将传统编解码器与深度学习相结合，这项工作为增强传统压缩标准同时保持其向后兼容性提供了实际途径，使其非常适合实际部署。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决G-PCC（基于几何的点云压缩）国际标准压缩性能不足的问题。在自动驾驶、虚拟现实和3D重建等领域，点云数据应用广泛但体积庞大，需要高效压缩技术。G-PCC虽然具有互操作性和计算效率优势，但相比基于深度学习的压缩方法性能较差。而深度学习方法虽性能好但计算复杂且存在可重现性问题。因此，在不牺牲G-PCC优势的前提下提升其效率，对推动点云技术的实际应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了G-PCC和基于深度学习的PCC方法的优缺点，提出结合两者优势的方案。他们设计了一个预处理框架，整合了面向压缩的体素化网络和可微分G-PCC代理模型。在训练阶段进行联合优化，使代理模型模仿G-PCC的率失真行为。推理阶段只保留轻量级体素化网络附加到G-PCC编码器。该方法借鉴了现有工作：体素化网络采用稀疏张量多尺度表示；代理模型设计参考了图像压缩中的可微分JPEG代理；损失函数结合了率失真优化框架；网络架构借鉴了InceptionResNet等结构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学习型预处理优化点云分布，使其能被G-PCC以最优的率失真权衡进行压缩，同时保持G-PCC的兼容性和效率。实现流程：1)输入原始点云；2)体素化网络进行全局缩放、局部剪枝和点级编辑；3)使用稀疏卷积处理多尺度稀疏张量表示；4)通过分类机制优化节点占用状态；5)可微分代理模型模拟G-PCC编码行为；6)训练阶段联合优化率失真损失；7)推理阶段仅保留轻量级体素化网络附加到G-PCC编码器，解码器保持不变。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)轻量级通用点云体素化网络，通过多种操作优化点云分布；2)可微分G-PCC代理模型实现梯度传播；3)后加载上采样模块减少计算量；4)实用范例增强传统标准而不修改解码端。相比之前工作：与传统G-PCC相比显著提升压缩性能(38.84% BD-rate减少)；与深度学习方法相比避免解码端使用神经网络，解决可重现性问题；与现有预处理相比针对压缩任务联合优化；与图像压缩方法相比专门针对3D点云特性设计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种结合学习型预处理和传统G-PCC框架的混合方法，通过可微分代理模型实现端到端优化，显著提升了点云压缩性能同时保持标准的互操作性和计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometry-based point cloud compression (G-PCC), an international standarddesigned by MPEG, provides a generic framework for compressing diverse types ofpoint clouds while ensuring interoperability across applications and devices.However, G-PCC underperforms compared to recent deep learning-based PCC methodsdespite its lower computational power consumption. To enhance the efficiency ofG-PCC without sacrificing its interoperability or computational flexibility, wepropose a novel preprocessing framework that integrates a compression-orientedvoxelization network with a differentiable G-PCC surrogate model, jointlyoptimized in the training phase. The surrogate model mimics the rate-distortionbehaviour of the non-differentiable G-PCC codec, enabling end-to-end gradientpropagation. The versatile voxelization network adaptively transforms inputpoint clouds using learning-based voxelization and effectively manipulatespoint clouds via global scaling, fine-grained pruning, and point-level editingfor rate-distortion trade-offs. During inference, only the lightweightvoxelization network is appended to the G-PCC encoder, requiring nomodifications to the decoder, thus introducing no computational overhead forend users. Extensive experiments demonstrate a 38.84% average BD-rate reductionover G-PCC. By bridging classical codecs with deep learning, this work offers apractical pathway to enhance legacy compression standards while preservingtheir backward compatibility, making it ideal for real-world deployment.</description>
      <author>example@mail.com (Wanhao Ma, Wei Zhang, Shuai Wan, Fuzheng Yang)</author>
      <guid isPermaLink="false">2508.01633v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor</title>
      <link>http://arxiv.org/abs/2508.01311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  We have provided the code for C3D-AD with checkpoints and BASELINE at  this link: https://github.com/hzzzzzhappy/CL3AD&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为C3D-AD的持续学习框架，用于3D异常检测，通过三个关键模块实现了对多类别点云的通用表示学习，并能处理随时间出现的新类别，在三个公共数据集上取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;3D异常检测在检测高精度工业产品的异常或缺陷方面显示出巨大潜力。然而，现有方法通常以类别特定的方式训练，并且缺乏从新兴类别中学习的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一个持续学习框架，能够学习多类别点云的通用表示，并处理随时间出现的新类别。&lt;h4&gt;方法&lt;/h4&gt;提出了C3D-AD框架，包含三个主要模块：1)引入带有随机特征层的核注意力(KAL)来高效提取多样化产品类型的通用局部特征并规范化特征空间；2)提出带有可学习顾问的核注意力(KAA)机制，在编码器和解码器中从新类别学习信息同时丢弃冗余旧信息；3)设计带有参数扰动的重建(RPP)模块，通过表示重放损失函数保持跨任务的表示一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公共数据集上的广泛实验证明了所提出方法的有效性，在Real3D-AD、Anomaly-ShapeNet和MulSen-AD上分别实现了66.4%、83.1%和63.4%的平均AUROC性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的C3D-AD框架能够有效处理3D点云数据的持续学习异常检测问题，能够在处理新类别的同时保持对旧类别的记忆。&lt;h4&gt;翻译&lt;/h4&gt;3D异常检测在检测高精度工业产品的异常或缺陷方面显示出巨大潜力。然而，现有方法通常以类别特定的方式训练，并且缺乏从新兴类别中学习的能力。在本研究中，我们提出了一个名为持续3D异常检测的持续学习框架，它不仅能学习多类别点云的通用表示，还能处理随时间出现的新类别。具体来说，在特征提取模块中，为了高效地从不同任务的各种产品类型中提取通用局部特征，引入了带有随机特征层的核注意力，它规范化了特征空间。然后，为了正确且持续地重建数据，提出了一个高效的带有可学习顾问的核注意力机制，它在编码器和解码器中从新类别学习信息，同时丢弃冗余的旧信息。最后，为了保持跨任务的表示一致性，设计了带有参数扰动的重建模块，通过设计表示重放损失函数，确保模型记住之前的类别信息并返回类别自适应的表示。在三个公共数据集上的广泛实验证明了所提出方法的有效性，在Real3D-AD、Anomaly-ShapeNet和MulSen-AD上分别实现了66.4%、83.1%和63.4%的平均AUROC性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D异常检测在持续学习环境下的挑战，即当新类别产品不断出现时，现有方法需要完全重新训练的问题。这个问题在工业现实中非常重要，因为生产线会不断更新产品类型，需要能够持续学习并适应新类别的异常检测系统，同时保持对之前学习类别的检测能力，避免灾难性遗忘。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D异常检测方法的局限性，指出它们通常针对特定类别训练且缺乏从新兴类别学习的能力。然后，作者借鉴了持续学习领域的方法，如基于正则化和基于重放的技术，以及3D异常检测中的特征嵌入和重建方法。但由于3D点云数据的高分辨率特性和类别特定模型限制，这些方法无法直接应用于3D场景，因此作者设计了专门针对3D点云的持续学习框架，包括KAL、KAA和RPP三个核心组件。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过三个组件实现持续学习：1) KAL模块提取通用特征并归一化特征空间；2) KAA机制在编码器-解码器中学习新信息同时丢弃冗余旧信息；3) RPP模块保持跨任务表示一致性。整体流程是：首先使用KAL从点云提取特征；然后通过KAA处理特征，学习新类别信息；最后使用RPP重建特征，保持表示一致性；通过比较特征令牌和重建令牌差异计算异常分数进行检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) KAL层归一化特征空间并提取跨组点云的通用非线性结构信息；2) KAA机制具有线性O(n)复杂度，能减少旧知识冗余同时学习新知识；3) RPP模块通过参数扰动保持表示一致性。相比之前工作，C3D-AD首次以类别增量方式解决3D异常检测问题，提供了多类别和持续异常检测能力，避免了传统方法的完全重新训练需求，同时解决了统一模型中的灾难性遗忘问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; C3D-AD首次提出了一种持续学习框架，通过核注意力和可学习顾问机制，使3D异常检测模型能够从新兴类别中学习同时避免灾难性遗忘，实现了高效的多类别持续异常检测。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Anomaly Detection (AD) has shown great potential in detecting anomalies ordefects of high-precision industrial products. However, existing methods aretypically trained in a class-specific manner and also lack the capability oflearning from emerging classes. In this study, we proposed a continual learningframework named Continual 3D Anomaly Detection (C3D-AD), which can not onlylearn generalized representations for multi-class point clouds but also handlenew classes emerging over time.Specifically, in the feature extraction module,to extract generalized local features from diverse product types of differenttasks efficiently, Kernel Attention with random feature Layer (KAL) isintroduced, which normalizes the feature space. Then, to reconstruct datacorrectly and continually, an efficient Kernel Attention with learnable Advisor(KAA) mechanism is proposed, which learns the information from new categorieswhile discarding redundant old information within both the encoder and decoder.Finally, to keep the representation consistency over tasks, a Reconstructionwith Parameter Perturbation (RPP) module is proposed by designing arepresentation rehearsal loss function, which ensures that the model remembersprevious category information and returns category-adaptiverepresentation.Extensive experiments on three public datasets demonstrate theeffectiveness of the proposed method, achieving an average performance of66.4%, 83.1%, and 63.4% AUROC on Real3D-AD, Anomaly-ShapeNet, and MulSen-AD,respectively.</description>
      <author>example@mail.com (Haoquan Lu, Hanzhe Liang, Jie Zhang, Chenxi Hu, Jinbao Wang, Can Gao)</author>
      <guid isPermaLink="false">2508.01311v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>ModelNet40-E: An Uncertainty-Aware Benchmark for Point Cloud Classification</title>
      <link>http://arxiv.org/abs/2508.01269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了ModelNet40-E基准数据集，用于评估点云分类模型在合成激光雷达噪声下的鲁棒性和校准能力。该数据集提供带噪声的点云和点级别的不确定性标注，使研究者能够进行更细致的不确定性建模评估。通过评估三种模型，发现Point Transformer v3在噪声环境下表现出更好的校准能力。&lt;h4&gt;背景&lt;/h4&gt;现有的点云分类模型评估基准缺乏对模型在噪声环境下鲁棒性和校准能力的全面评估，特别是缺乏点级别的不确定性标注。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的基准数据集ModelNet40-E，用于评估点云分类模型在合成激光雷达噪声环境下的鲁棒性和校准能力，并提供点级别的不确定性标注以支持细粒度的不确定性建模评估。&lt;h4&gt;方法&lt;/h4&gt;构建ModelNet40-E基准数据集，包含噪声损坏的点云和通过高斯噪声参数提供的不确定性标注。评估了三种流行的点云分类模型：PointNet、DGCNN和Point Transformer v3，使用分类准确率、校准指标和不确定性感知能力等多个指标在不同噪声水平下进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;所有模型在噪声水平增加时性能都会下降，但Point Transformer v3表现出更好的校准能力，其预测的不确定性更接近底层测量不确定性。&lt;h4&gt;结论&lt;/h4&gt;ModelNet40-E基准数据集为点云分类模型在噪声环境下的评估提供了新的工具，特别是不确定性建模的评估。Point Transformer v3在噪声环境下的校准表现优于其他评估的模型，这表明它在需要可靠不确定性估计的应用中具有优势。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了ModelNet40-E，这是一个新的基准，旨在评估点云分类模型在合成激光雷达类似噪声下的鲁棒性和校准能力。与现有基准不同，ModelNet40-E通过高斯噪声参数提供了噪声损坏的点云和点级别的不确定性标注，使不确定性建模能够进行细粒度评估。我们使用分类准确率、校准指标和不确定性感知能力，在多个噪声水平下评估了三种流行模型：PointNet、DGCNN和Point Transformer v3。虽然所有模型在噪声增加时性能都会下降，但Point Transformer v3表现出更好的校准能力，其预测的不确定性更接近底层测量不确定性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有点云分类基准缺乏真实感的问题，即现有数据集（如ModelNet40）使用干净、理想化的点云，无法反映真实传感器数据中的噪声和不确定性。这一问题在自动驾驶、机器人等安全关键应用中尤为重要，因为这些应用不仅需要模型做出准确预测，还需要评估预测的置信度，避免过于自信但错误的预测导致灾难性后果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有基准的局限性，包括缺乏真实噪声和不确定性标注。他们借鉴了激光雷达测量误差的实证研究（Gschwandtner et al. 2011），设计了包含范围相关噪声、角度相关噪声、系统偏差和随机异常值的激光雷达噪声模拟方法。基于ModelNet40数据集，他们添加了三种严重程度的噪声（轻度、中度、重度），并为每个点提供噪声统计信息（σ和μ），从而创建了一个更贴近真实场景的评估基准。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个包含真实激光雷达样式噪声和不确定性标注的点云分类基准，用于评估模型在噪声环境下的分类性能和不确定性估计能力。整体流程包括：1) 基于原始ModelNet40数据集；2) 添加三种严重程度的激光雷达噪声；3) 为每个点计算噪声参数（包括范围相关噪声、角度相关噪声、系统偏差和异常值）；4) 评估三种模型（PointNet、DGCNN、Point Transformer v3）的性能；5) 使用分类准确率、校准误差和不确定性相关性作为评估指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出ModelNet40-E，一个不确定性感知的点云分类基准；2) 引入真实的激光雷达样式噪声而非随机扰动；3) 提供逐点噪声标注（σ和μ），支持细粒度不确定性评估；4) 全面评估模型在不同噪声水平下的分类准确率、校准和不确定性感知能力。相比之前工作，ModelNet40-E与ModelNet40相比添加了真实噪声和不确定性标注；与ModelNet-C相比使用更真实的激光雷达噪声并提供不确定性标注；评估方面不仅关注准确率，还关注校准和不确定性感知。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了ModelNet40-E，一个包含真实激光雷达噪声和不确定性标注的点云分类基准，用于评估点云分类模型在噪声环境下的鲁棒性和不确定性估计能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce ModelNet40-E, a new benchmark designed to assess the robustnessand calibration of point cloud classification models under synthetic LiDAR-likenoise. Unlike existing benchmarks, ModelNet40-E provides both noise-corruptedpoint clouds and point-wise uncertainty annotations via Gaussian noiseparameters ({\sigma}, {\mu}), enabling fine-grained evaluation of uncertaintymodeling. We evaluate three popular models-PointNet, DGCNN, and PointTransformer v3-across multiple noise levels using classification accuracy,calibration metrics, and uncertainty-awareness. While all models degrade underincreasing noise, Point Transformer v3 demonstrates superior calibration, withpredicted uncertainties more closely aligned with the underlying measurementuncertainty.</description>
      <author>example@mail.com (Pedro Alonso, Tianrui Li, Chongshou Li)</author>
      <guid isPermaLink="false">2508.01269v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Point-wise Diffusion Models for Physical Systems with Shape Variations: Application to Spatio-temporal and Large-scale system</title>
      <link>http://arxiv.org/abs/2508.01230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型的逐点扩散模型，能够独立处理时空点，有效预测具有形状变化的复杂物理系统。该方法在各个时空点上应用前向和后向扩散过程，结合逐点扩散Transformer架构进行去噪，可直接处理网格和点云等数据格式，同时保持几何保真度。&lt;h4&gt;背景&lt;/h4&gt;传统基于图像的扩散模型在处理复杂物理系统预测时存在局限性，特别是在处理非结构化数据格式和实时应用方面。现有方法在训练效率、参数量和预测准确性方面仍有提升空间。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效处理复杂物理系统预测的逐点扩散模型，提高预测精度，减少训练时间和参数量，并实现实时预测能力。&lt;h4&gt;方法&lt;/h4&gt;提出一种逐点扩散模型，在各个时空点上应用前向和后向扩散过程，结合逐点扩散Transformer架构进行去噪。采用去噪扩散隐式模型(DDIM)进行高效确定性采样，仅需5-10步。在三个物理领域验证该方法：2D时空系统(圆柱体流体和OLED液滴冲击测试)和3D大规模系统(汽车外空气动力学)。&lt;h4&gt;主要发现&lt;/h4&gt;1) 相比传统基于图像的扩散模型，训练时间减少94.4%，参数减少89.0%，预测准确率提高28%以上；2) 采用DDIM仅需5-10步采样，实现100-200倍的计算加速，同时不牺牲准确性；3) 与DeepONet和Meshgraphnet等代理模型相比，在所有测试物理系统中表现优越；4) 该方法能够直接处理网格和点云等数据格式，保持几何保真度。&lt;h4&gt;结论&lt;/h4&gt;逐点扩散模型在复杂物理系统预测中表现出优越的性能，具有高效性、准确性和灵活性，适用于实时预测应用。该方法通过减少训练时间、参数量和计算步骤，同时提高预测精度，为物理系统模拟提供了新的有效途径。&lt;h4&gt;翻译&lt;/h4&gt;本研究引入了一种新型的逐点扩散模型，它独立处理时空点以高效预测具有形状变化的复杂物理系统。该方法的主要贡献在于在各个时空点上应用前向和后向扩散过程，并结合逐点扩散Transformer架构进行去噪。与操作在结构化数据表示上的传统基于图像的扩散模型不同，该框架能够直接处理任何数据格式，包括网格和点云，同时保持几何保真度。我们在三个具有复杂几何配置的不同物理领域验证了我们的方法：包括圆柱体流体和OLED液滴冲击测试的2D时空系统，以及用于汽车外空气动力学的3D大规模系统。为了证明我们的逐点方法在实时预测应用中的必要性，我们采用去噪扩散隐式模型(DDIM)进行高效确定性采样，仅需5-10步，相比传统的1000步，在推理过程中实现了100到200倍的计算加速，同时不牺牲准确性。此外，与基于图像的扩散模型相比，我们提出的模型在训练时间上减少了94.4%，参数减少了89.0%，同时预测准确率提高了28%以上。与DeepONet和Meshgraphnet等数据灵活的代理模型的全面比较表明，我们的方法在所有三个物理系统中都表现出一致的优势。为了进一步完善提出的模型，我们研究了两个关键方面：1) 最终物理状态预测或增量变化预测的比较，以及2) 在不同下采样比率(10%-100%)下的计算效率评估。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何高效、准确地模拟具有复杂几何形状变化的物理系统的问题。这个问题在现实中非常重要，因为许多工程应用（如流体动力学、结构力学、气候建模等）需要快速模拟具有不同几何形状的物理系统，而传统数值模拟方法计算成本高，难以实时处理；在研究中，现有机器学习方法在处理不规则几何形状和保持几何保真度方面存在局限，无法满足形状设计过程中需要快速迭代评估不同几何配置性能的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法在处理形状变化物理系统时的局限性进行思考，发现基于图像的方法难以准确捕捉不规则几何形状，而网格和点云方法虽灵活但计算开销大。作者借鉴了扩散模型（如DDPM、DDIM）的基本框架，Transformer架构（如DiT）的思想，以及科学机器学习领域的成果（如Meshgraphnet和DeepONet），同时创新性地将扩散过程从全局图像处理改为点级独立处理，设计了点扩散变换器架构，采用DDIM进行高效确定性采样，并使用位置编码和时间嵌入增强模型的空间和时间感知能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在点级别独立执行扩散过程，而非传统的全局图像级别处理，使模型能够直接处理任何数据格式（包括网格和点云）并保持几何保真度，同时利用DDIM算法实现高效确定性采样。整体实现流程包括：1）数据准备，收集物理系统模拟数据并提取时空点坐标和物理量；2）前向扩散过程，对每个点逐步添加高斯噪声；3）模型训练，使用点扩散变换器架构学习去噪过程；4）后向扩散过程，采用DDIM算法仅需5-10步从噪声恢复原始物理量；5）预测与应用，对未见过的几何形状进行预测；6）模型优化，通过比较不同预测策略和采样比例优化性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）点级扩散框架，首次将扩散模型应用于点级独立处理；2）几何保真度保持，直接在原始几何结构上操作避免信息损失；3）高效确定性采样，将1000步采样减少到5-10步；4）非自回归时间建模，避免误差累积；5）条件注入机制，有效注入物理和几何条件。相比之前的工作，不同之处在于：相比图像扩散模型，不需规则网格表示且精度更高；相比网格图神经网络，消除计算开销和误差累积；相比基于坐标的方法，克服频谱偏差问题；相比其他扩散模型，避免几何信息损失和自然语言歧义影响。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的点扩散模型，通过在点级别独立执行扩散过程，实现了对具有复杂形状变化的物理系统的高效、高精度预测，同时保持了几何保真度和计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces a novel point-wise diffusion model that processesspatio-temporal points independently to efficiently predict complex physicalsystems with shape variations. This methodological contribution lies inapplying forward and backward diffusion processes at individual spatio-temporalpoints, coupled with a point-wise diffusion transformer architecture fordenoising. Unlike conventional image-based diffusion models that operate onstructured data representations, this framework enables direct processing ofany data formats including meshes and point clouds while preserving geometricfidelity. We validate our approach across three distinct physical domains withcomplex geometric configurations: 2D spatio-temporal systems including cylinderfluid flow and OLED drop impact test, and 3D large-scale system for road-carexternal aerodynamics. To justify the necessity of our point-wise approach forreal-time prediction applications, we employ denoising diffusion implicitmodels (DDIM) for efficient deterministic sampling, requiring only 5-10 stepscompared to traditional 1000-step and providing computational speedup of 100 to200 times during inference without compromising accuracy. In addition, ourproposed model achieves superior performance compared to image-based diffusionmodel: reducing training time by 94.4% and requiring 89.0% fewer parameterswhile achieving over 28% improvement in prediction accuracy. Comprehensivecomparisons against data-flexible surrogate models including DeepONet andMeshgraphnet demonstrate consistent superiority of our approach across allthree physical systems. To further refine the proposed model, we investigatetwo key aspects: 1) comparison of final physical states prediction orincremental change prediction, and 2) computational efficiency evaluationacross varying subsampling ratios (10%-100%).</description>
      <author>example@mail.com (Jiyong Kim, Sunwoong Yang, Namwoo Kang)</author>
      <guid isPermaLink="false">2508.01230v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding</title>
      <link>http://arxiv.org/abs/2508.01197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IROS 2025 Accepted Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了3D占用目标定位的新基准和模型，解决了传统边界框表示物体不准确的问题，通过结合自然语言与体素级注释提供了更精确的物体感知方法。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉目标定位任务通常依赖于边界框，但边界框往往无法捕捉细粒度细节，因为边界框内的所有体素并非都被占用，导致物体表示不准确。&lt;h4&gt;目的&lt;/h4&gt;引入具有挑战性的户外场景3D占用目标定位基准，并提出一种端到端模型GroundingOcc，用于更精确的3D占用目标定位。&lt;h4&gt;方法&lt;/h4&gt;提出GroundingOcc模型，通过多模态学习结合视觉、文本和点云特征，从粗到细预测物体位置和占用信息。该模型包含多模态编码器、占用头、定位头，以及增强几何理解的2D目标定位模块和深度估计模块。&lt;h4&gt;主要发现&lt;/h4&gt;在基准上的大量实验表明，提出的方法在3D占用目标定位上优于现有的基线方法。&lt;h4&gt;结论&lt;/h4&gt;通过新的3D占用目标定位基准和GroundingOcc模型，研究提供了比传统目标定位任务更精确的物体感知方法，有助于自动驾驶中的空间感知。&lt;h4&gt;翻译&lt;/h4&gt;视觉目标定位旨在根据自然语言描述识别场景中的物体或区域，对自动驾驶中的空间感知至关重要。然而，现有的视觉目标定位任务通常依赖于边界框，这些边界框往往无法捕捉细粒度细节。边界框内的并非所有体素都被占用，导致物体表示不准确。为了解决这个问题，我们引入了一个具有挑战性的户外场景3D占用目标定位基准。该基准基于nuScenes数据集构建，将自然语言与体素级占用注释相结合，与传统目标定位任务相比，提供了更精确的物体感知。此外，我们提出了GroundingOcc，这是一个专为3D占用目标定位设计的端到端模型，通过多模态学习实现。它结合视觉、文本和点云特征，从粗到细预测物体位置和占用信息。具体而言，GroundingOcc包含一个用于特征提取的多模态编码器，一个用于体素级预测的占用头，以及一个用于精确定位的定位头。此外，2D目标定位模块和深度估计模块增强了几何理解，从而提高了模型性能。在基准上的大量实验表明，我们的方法在3D占用目标定位上优于现有的基线方法。该数据集可在https://github.com/RONINGOD/GroundingOcc获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D占用定位问题，即根据自然语言描述在3D场景中精确定位物体并预测其占用状态。这个问题在自动驾驶领域非常重要，因为传统的边界框定位无法准确表示复杂或不规则形状的物体，而精确的物体表示对于自动驾驶系统做出正确决策至关重要，特别是在处理不规则形状或部分遮挡的物体时。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有视觉定位任务的局限性，特别是边界框表示的不足。他们注意到现有数据集主要关注基于边界框的理解，缺乏细粒度的体素级占用预测。作者借鉴了多模态学习思想，结合视觉、文本和点云特征，设计了GroundingOcc模型。该方法整合了2D视觉定位、3D视觉定位和多模态占用感知等领域的研究成果，但将其扩展到新的3D占用定位任务，并通过添加2D定位模块和深度估计模块来增强几何理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态学习，结合视觉、文本和点云特征，从粗到细地预测物体的位置和占用信息，克服传统边界框表示的局限性。整体流程包括：1)输入多模态数据(多视角图像、点云和自然语言)；2)使用编码器提取各模态特征；3)通过Vision-Language PAN模块增强视觉和文本表示间的交互；4)利用2D定位和深度估计等辅助任务增强几何理解；5)通过体素编码和融合生成最终特征；6)输出3D占用预测和可选的3D边界框。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将占用预测引入3D视觉定位；2)提出新的基准数据集Talk2Occ；3)设计端到端多分支网络GroundingOcc；4)利用2D定位和深度估计提高准确性；5)引入几何监督而非仅依赖语义监督。相比之前工作，本文从边界框定位转向体素级占用定位，结合多模态信息，引入辅助任务增强几何理解，使用几何监督提高模型性能，并提出了专门用于评估3D占用定位任务的新基准数据集。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过提出GroundingOcc模型和Talk2Occ基准数据集，将多模态学习与体素级占用预测相结合，实现了比传统边界框定位更精确的3D物体表示，为自动驾驶中的细粒度场景理解提供了新解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual grounding aims to identify objects or regions in a scene based onnatural language descriptions, essential for spatially aware perception inautonomous driving. However, existing visual grounding tasks typically dependon bounding boxes that often fail to capture fine-grained details. Not allvoxels within a bounding box are occupied, resulting in inaccurate objectrepresentations. To address this, we introduce a benchmark for 3D occupancygrounding in challenging outdoor scenes. Built on the nuScenes dataset, itintegrates natural language with voxel-level occupancy annotations, offeringmore precise object perception compared to the traditional grounding task.Moreover, we propose GroundingOcc, an end-to-end model designed for 3Doccupancy grounding through multi-modal learning. It combines visual, textual,and point cloud features to predict object location and occupancy informationfrom coarse to fine. Specifically, GroundingOcc comprises a multimodal encoderfor feature extraction, an occupancy head for voxel-wise predictions, and agrounding head to refine localization. Additionally, a 2D grounding module anda depth estimation module enhance geometric understanding, thereby boostingmodel performance. Extensive experiments on the benchmark demonstrate that ourmethod outperforms existing baselines on 3D occupancy grounding. The dataset isavailable at https://github.com/RONINGOD/GroundingOcc.</description>
      <author>example@mail.com (Zhan Shi, Song Wang, Junbo Chen, Jianke Zhu)</author>
      <guid isPermaLink="false">2508.01197v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>CP-FREEZER: Latency Attacks against Vehicular Cooperative Perception</title>
      <link>http://arxiv.org/abs/2508.01062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CP-FREEZER的新型延迟攻击，通过V2V消息注入对抗性扰动，显著增加合作感知系统的计算延迟，威胁自动驾驶系统的及时性和可用性。&lt;h4&gt;背景&lt;/h4&gt;合作感知通过交换和组合多个智能体的消息来增强联网和自动驾驶车辆的环境感知能力。先前研究主要关注针对感知准确性的对抗攻击，但对CP系统在及时性方面的鲁棒性研究不足，而及时性是自动驾驶的安全关键要求。&lt;h4&gt;目的&lt;/h4&gt;研究合作感知系统在面对针对及时性(可用性)的攻击时的脆弱性，并开发一种能够最大化CP算法计算延迟的攻击方法。&lt;h4&gt;方法&lt;/h4&gt;提出CP-FREEZER攻击方法，通过V2V消息注入对抗性扰动来增加延迟。该方法解决了点云预处理非可微性、传输延迟导致的受害者输入异步知识等挑战，并使用新型损失函数有效最大化CP管道的执行时间。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明CP-FREEZER将端到端CP延迟增加了90倍以上，在真实车辆测试平台上以100%的成功率将每帧处理时间推至3秒以上，揭示了CP系统可用性的关键威胁。&lt;h4&gt;结论&lt;/h4&gt;合作感知系统在面对可用性攻击时存在严重威胁，突显了开发鲁棒防御措施的迫切需求。&lt;h4&gt;翻译&lt;/h4&gt;合作感知(CP)通过交换和组合多个智能体的消息来增强联网和自动驾驶车辆的环境感知能力。虽然先前工作已经探索了降低感知准确性的对抗完整性攻击，但CP系统针对及时性(或可用性)攻击的鲁棒性仍知之甚少，这对自动驾驶是一项安全关键要求。在本文中，我们提出了CP-FREEZER，这是第一种通过V2V消息注入对抗性扰动来最大化CP算法计算延迟的延迟攻击。我们的攻击解决了几个独特挑战，包括点云预处理的非可微性、由于传输延迟导致的受害者输入异步知识，并使用了一种新的损失函数，有效最大化CP管道的执行时间。大量实验表明，CP-FREEZER将端到端CP延迟增加了90倍以上，在我们的真实车辆测试平台上以100%的成功率将每帧处理时间推至3秒以上。我们的发现揭示了CP系统可用性的关键威胁，突显了开发鲁棒防御措施的迫切需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cooperative perception (CP) enhances situational awareness of connected andautonomous vehicles by exchanging and combining messages from multiple agents.While prior work has explored adversarial integrity attacks that degradeperceptual accuracy, little is known about CP's robustness against attacks ontimeliness (or availability), a safety-critical requirement for autonomousdriving. In this paper, we present CP-FREEZER, the first latency attack thatmaximizes the computation delay of CP algorithms by injecting adversarialperturbation via V2V messages. Our attack resolves several unique challenges,including the non-differentiability of point cloud preprocessing, asynchronousknowledge of the victim's input due to transmission delays, and uses a novelloss function that effectively maximizes the execution time of the CP pipeline.Extensive experiments show that CP-FREEZER increases end-to-end CP latency byover $90\times$, pushing per-frame processing time beyond 3 seconds with a 100%success rate on our real-world vehicle testbed. Our findings reveal a criticalthreat to the availability of CP systems, highlighting the urgent need forrobust defenses.</description>
      <author>example@mail.com (Chenyi Wang, Ruoyu Song, Raymond Muller, Jean-Philippe Monteuuis, Z. Berkay Celik, Jonathan Petit, Ryan Gerdes, Ming Li)</author>
      <guid isPermaLink="false">2508.01062v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving via Online HD Map Diffusion</title>
      <link>http://arxiv.org/abs/2508.01778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DiffSemanticFusion是一种融合框架，用于多模态轨迹预测和规划，结合了基于光栅和基于图表示方法的优点，提高了在线高清地图的稳定性和表现力。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶需要准确理解场景，包括道路几何形状、交通参与者及其语义关系。在线高清地图生成场景中，基于光栅的表示适合视觉模型但缺乏几何精度，而基于图的表示保留了结构细节但没有精确地图时会变得不稳定。&lt;h4&gt;目的&lt;/h4&gt;利用基于光栅和基于图两种表示方法的互补优势，提出DiffSemanticFusion融合框架，用于多模态轨迹预测和规划。&lt;h4&gt;方法&lt;/h4&gt;在语义光栅融合BEV空间上推理，增强地图扩散模块，提高在线高清地图表示的稳定性和表现力。在轨迹预测和面向规划的全栈自动驾驶两个下游任务上验证框架。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和NAVSIM真实自动驾驶基准测试上，性能优于几种最先进方法。在nuScenes预测任务中，与QCNet集成实现5.1%性能提升；在NAVSIM全栈自动驾驶中，NavHard场景实现15%性能提升。地图扩散模块可无缝集成到其他基于向量的方法中提高性能。&lt;h4&gt;结论&lt;/h4&gt;DiffSemanticFusion有效结合了基于光栅和基于图表示方法的优点，提高了在线高清地图的稳定性和表现力，在多个任务和基准测试上取得显著性能提升。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶需要准确理解场景，包括道路几何形状、交通参与者及其语义关系。在线高清地图生成场景中，基于光栅的表示适合视觉模型但缺乏几何精度，而基于图的表示保留了结构细节但没有精确地图时会变得不稳定。为了利用这两种方法的互补优势，我们提出了DiffSemanticFusion——一个用于多模态轨迹预测和规划的融合框架。我们的方法在语义光栅融合的BEV空间上进行推理，并通过增强地图扩散模块提高了在线高清地图表示的稳定性和表现力。我们在两个下游任务上验证了我们的框架：轨迹预测和面向规划的全栈自动驾驶。在nuScenes和NAVSIM真实自动驾驶基准测试上的实验表明，性能优于几种最先进方法。对于nuScenes上的预测任务，我们将DiffSemanticFusion与在线高清地图信息QCNet集成，实现了5.1%的性能提升。在NAVSIM的全栈自动驾驶中，DiffSemanticFusion实现了最先进的结果，在NavHard场景中实现了15%的性能提升。此外，大量的消融和敏感性研究表明，我们的地图扩散模块可以无缝集成到其他基于向量的方法中以提高性能。所有代码和资源可在https://github.com/SunZhigang7/DiffSemanticFusion获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统中在线高清地图(online HD map)在噪声、不完整或错位条件下的鲁棒性问题，以及如何有效融合栅格表示和图表示两种场景表示方法的优势。这个问题在现实中非常重要，因为自动驾驶需要准确理解道路几何、交通参与者及其语义关系，而地图质量问题会显著影响下游任务如运动预测和规划，特别是在动态或未见过的环境中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了自动驾驶中场景表示的两种主要方法(栅格表示和图表示)的优缺点，认识到需要结合两者的优势。他们借鉴了扩散模型在图像生成和轨迹预测中的应用(如MotionDiffuser)，参考了BEV表示方法(如BEVDet)和图神经网络在交通场景表示中的应用(如SemanticFormer)，以及端到端自动驾驶框架(如DiffusionDrive)。在此基础上，作者创新性地设计了在线高清地图扩散模块和语义栅格BEV融合框架，通过扩散模型增强地图表示，并在统一的BEV空间中融合多种表示方法的优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过扩散模型增强在线高清地图的表示，提高其在噪声或不完整条件下的鲁棒性，同时融合栅格表示、图表示和BEV特征表示的优点。整体实现流程包括：1)混合感知模块提取稀疏和密集场景表示；2)在线高清地图扩散模块对稀疏车道向量添加噪声并预测去噪结果；3)语义栅格图像融合模块将不同表示投影到统一空间；4)DiffSemanticFusion架构融合多模态特征并通过扩散模型解码轨迹；5)端到端训练整个模型，优化扩散损失和原始任务损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)在线高清地图扩散模块，首次将扩散模型应用于地图表示增强而非直接轨迹生成；2)语义栅格BEV融合框架，首个统一稀疏场景表示和密集BEV特征的端到端自动驾驶框架；3)多模态融合方法，有效结合栅格、图和BEV特征的互补特性。相比之前工作，本文创新性地使用扩散模型增强地图表示而非直接生成轨迹，首次统一了稀疏和密集表示方法，并将地图视为需要动态优化的表示而非固定输入。实验表明，该方法在nuScenes预测任务上提升5.1%，在NAVSIM NavHard场景上提升15%性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了DiffSemanticFusion，一种结合在线高清地图扩散和语义栅格BEV融合的创新框架，有效解决了自动驾驶中地图不完整和表示方法单一的问题，显著提升了轨迹预测和规划的准确性与鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving requires accurate scene understanding, including roadgeometry, traffic agents, and their semantic relationships. In online HD mapgeneration scenarios, raster-based representations are well-suited to visionmodels but lack geometric precision, while graph-based representations retainstructural detail but become unstable without precise maps. To harness thecomplementary strengths of both, we propose DiffSemanticFusion -- a fusionframework for multimodal trajectory prediction and planning. Our approachreasons over a semantic raster-fused BEV space, enhanced by a map diffusionmodule that improves both the stability and expressiveness of online HD maprepresentations. We validate our framework on two downstream tasks: trajectoryprediction and planning-oriented end-to-end autonomous driving. Experiments onreal-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrateimproved performance over several state-of-the-art methods. For the predictiontask on nuScenes, we integrate DiffSemanticFusion with the online HD mapinformed QCNet, achieving a 5.1\% performance improvement. For end-to-endautonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-artresults, with a 15\% performance gain in NavHard scenarios. In addition,extensive ablation and sensitivity studies show that our map diffusion modulecan be seamlessly integrated into other vector-based approaches to enhanceperformance. All artifacts are available athttps://github.com/SunZhigang7/DiffSemanticFusion.</description>
      <author>example@mail.com (Zhigang Sun, Yiru Wang, Anqing Jiang, Shuo Wang, Yu Gao, Yuwen Heng, Shouyi Zhang, An He, Hao Jiang, Jinhao Chai, Zichong Gu, Wang Jijun, Shichen Tang, Lavdim Halilaj, Juergen Luettin, Hao Sun)</author>
      <guid isPermaLink="false">2508.01778v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing</title>
      <link>http://arxiv.org/abs/2508.01740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AG$^2$aussian的新型框架，利用锚图结构组织语义特征并调节基本高斯，实现了干净准确的实例级高斯选择，在四个应用场景中验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;3D Gaussian Splatting (3DGS)已在不同领域获得广泛应用，对语义感知的3D高斯表示的需求日益增长，以支持场景理解和编辑任务。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中语义特征附加到自由高斯上导致的嘈杂分割和杂乱高斯选择问题，实现干净准确的实例级高斯选择。&lt;h4&gt;方法&lt;/h4&gt;提出AG$^2$aussian框架，利用锚图结构组织语义特征和调节基本高斯，促进紧凑和实例感知的高斯分布分布，支持基于图的传播。&lt;h4&gt;主要发现&lt;/h4&gt;在四个应用场景（基于点击的交互式查询、开放词汇文本驱动查询、物体移除编辑和物理模拟）中验证了该方法的优势，实验和消融研究证实了关键设计的有效性。&lt;h4&gt;结论&lt;/h4&gt;AG$^2$aussian框架在各种应用中显示出优势，能够实现干净准确的实例级高斯选择，为语义感知的3D高斯表示提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;三维高斯散射(3DGS)已在不同应用中见证指数级增长，推动了语义感知的3D高斯表示的迫切需求，以实现场景理解和编辑任务。现有方法通常将语义特征附加到一组自由高斯上，并通过可微分渲染来蒸馏这些特征，导致嘈杂的分割和杂乱的高斯选择。在本文中，我们引入了AG$^2$aussian，一种利用锚图结构来组织语义特征和调节基本高斯的新型框架。我们的锚图结构不仅促进了紧凑和实例感知的高斯分布，而且支持基于图的传播，实现了干净准确的实例级高斯选择。在四个应用（即基于点击的交互式查询、开放词汇文本驱动查询、物体移除编辑和物理模拟）上的广泛验证证明了我们方法的优势及其对各种应用的益处。实验和消融研究进一步评估了我们方法关键设计的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景理解和编辑中的实例级高斯选择不准确问题。现有方法将语义特征直接分配给自由高斯集合，导致分割结果有噪声，选择时包含周围多余高斯而内部高斯被遗漏。这个问题很重要，因为准确的对象选择是3D场景编辑和物理模拟等应用的基础，不准确的选择会严重影响后续应用效果，限制了3D场景理解和编辑的能力发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：自由高斯过度扩展、特征分配歧义性以及忽略3D空间约束。基于这些分析，作者构思出构建锚点图结构来组织语义特征并约束高斯分布。设计过程中借鉴了Scaffold-GS的结构化思想、OpenGaussian的对比学习策略、SAM的实例分割和CLIP的语言特征提取等技术，但创新性地将它们整合到锚点图框架中，形成了一个三阶段流程：锚点-高斯生长、锚点图传播和语言特征附加。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建锚点图结构来组织语义特征并约束高斯分布，每个锚点附着语义特征和一小组高斯，从而实现紧凑且实例感知的高斯分布，并通过图传播提高特征准确性。整体流程分为三阶段：1)锚点-高斯生长：初始化语义锚点和相关高斯，通过体素化定位锚点，优化高斯参数并进行语义对比学习；2)锚点图传播：构建基于体素邻域的锚点图，使用图拉普拉斯传播细化语义特征；3)语言特征附加：通过图聚类定位对象实例，匹配实例图与掩码，附加CLIP语言特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)锚点图结构化的3D高斯表示，将语义特征组织成更高层次结构；2)锚点图特征传播算法，通过图拉普拉斯传播提高特征准确性；3)应用于多种3D场景理解和编辑任务。相比之前工作，不同之处在于：与SAGA等方法相比产生更清洁的语义特征和更精确的查询结果；与OpenGaussian相比显式调节高斯并采用图传播；与Scaffold-GS相比限制高斯在锚点体素内并使用图传播；与SuperGSeg相比特征细化更有效且内存成本更低。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了AG2aussian，一种通过锚点图结构化高斯飞溅的方法，实现了干净准确的实例级3D场景理解和编辑，显著提高了对象相关高斯选择的精确度并支持多种应用场景。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has witnessed exponential adoption acrossdiverse applications, driving a critical need for semantic-aware 3D Gaussianrepresentations to enable scene understanding and editing tasks. Existingapproaches typically attach semantic features to a collection of free Gaussiansand distill the features via differentiable rendering, leading to noisysegmentation and a messy selection of Gaussians. In this paper, we introduceAG$^2$aussian, a novel framework that leverages an anchor-graph structure toorganize semantic features and regulate Gaussian primitives. Our anchor-graphstructure not only promotes compact and instance-aware Gaussian distributions,but also facilitates graph-based propagation, achieving a clean and accurateinstance-level Gaussian selection. Extensive validation across fourapplications, i.e. interactive click-based query, open-vocabulary text-drivenquery, object removal editing, and physics simulation, demonstrates theadvantages of our approach and its benefits to various applications. Theexperiments and ablation studies further evaluate the effectiveness of the keydesigns of our approach.</description>
      <author>example@mail.com (Zhaonan Wang, Manyi Li, Changhe Tu)</author>
      <guid isPermaLink="false">2508.01740v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Robot-Assisted Surgery with Hierarchical Class-Incremental Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2508.01713v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了TOPICS+方法，专门针对机器人辅助手术场景的鲁棒分割问题，通过引入Dice损失、层次伪标记和定制标签分类法，解决了传统分割模型在动态手术环境中的局限性。&lt;h4&gt;背景&lt;/h4&gt;机器人辅助手术依赖于准确和实时的场景理解来安全引导手术器械，但静态数据集上训练的分割模型在部署到动态演变的手术环境中时面临关键限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种增强的TOPICS+方法，专门针对手术场景的鲁棒分割，使模型能够持续适应新类别而避免灾难性遗忘。&lt;h4&gt;方法&lt;/h4&gt;基于TOPICS方法提出增强版本TOPICS+，将Dice损失合并到层次损失公式中处理类别不平衡，引入层次伪标记，为机器人手术环境设计定制标签分类法，提出六个新的CISS基准测试，并在Syn-Mediverse数据集上创建包含144多个类别的标签集。&lt;h4&gt;主要发现&lt;/h4&gt;TOPICS+方法能够有效处理手术环境中的强类别不平衡问题，通过层次伪标记和定制标签分类法提高了分割性能，六个新的CISS基准测试为评估提供了标准。&lt;h4&gt;结论&lt;/h4&gt;TOPICS+方法为机器人辅助手术提供了更有效的场景理解解决方案，通过公开代码和训练模型促进了该领域的研究发展。&lt;h4&gt;翻译&lt;/h4&gt;机器人辅助手术依赖于准确和实时的场景理解来安全地引导手术器械。然而，在静态数据集上训练的分割模型在部署到这些动态和不断演变的手术环境时面临关键限制。类增量语义分割(CISS)允许模型持续适应新类别，同时避免对先前知识的灾难性遗忘，而无需在先前数据上训练。在这项工作中，我们基于最近引入的面向分类学的庞加莱正则化增量类别分割(TOPICS)方法，提出了一个增强版本，称为TOPICS+，专门针对手术场景的鲁棒分割。具体来说，我们将Dice损失合并到层次损失公式中以处理强类别不平衡，引入层次伪标记，并为机器人手术环境设计定制的标签分类法。我们还提出了六个专为机器人手术环境设计的新型CISS基准，包括多个增量步骤和几个语义类别，以模拟手术环境中真实的类增量设置。此外，我们在Syn-Mediverse合成数据集上引入了一个包含144多个类别的 refined 标签集，在线托管作为评估基准。我们在http://topics.cs.uni-freiburg.de公开了代码和训练模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人辅助手术中，当手术环境动态变化时，模型如何持续学习新类别而不忘记已学知识的问题（即类增量语义分割问题）。这个问题非常重要，因为机器人手术在全球快速增长，手术环境复杂多变，医院需要快速适应新设备和患者群体，而传统方法存在'灾难性遗忘'问题，且临床场景因隐私限制无法无限制访问历史数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了机器人手术场景的特殊需求和挑战，借鉴了TOPICS（Taxonomy-Oriented Poincaré-regularized Incremental Class Segmentation）方法作为基础，并针对手术场景进行了改进。作者还参考了双曲空间中树状结构建模的知识表示方法，以及医学图像分类中的持续学习技术。通过分析现有工作的局限性，作者设计了专门针对手术场景的层次化Dice损失、层次化伪标记和定制标签分类法，以处理类别不平衡和背景复杂度多样等问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在双曲空间中构建层次化的类别表示，使模型能够在新旧类别间保持知识连续性，同时处理手术场景中的类别不平衡和背景复杂度问题。整体实现流程包括：1)使用DeepLabV3和ResNet-101构建基础模型；2)在双曲空间中编码类别层次关系；3)应用层次化Dice损失处理类别不平衡；4)使用层次化伪标记维持旧类知识；5)设计适合手术场景的标签分类法；6)在不同数据集和增量设置下训练评估模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)TOPICS+方法，专门针对机器人手术场景优化的不基于回放的类增量分割；2)层次化Dice损失，处理手术中的强烈类别不平衡；3)层次化伪标记，改善不同背景复杂度下的准确性；4)定制标签分类法，防止知识遗忘；5)六个新的CISS基准测试，模拟真实手术环境；6)扩展Syn-Mediverse数据集，创建144+手术环境细分类别。相比之前工作，TOPICS+专门针对手术场景优化，使用双曲空间层次编码，结合了层次化Dice损失和伪标记，提供了更全面的评估基准，并解决了背景偏移等特定问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了TOPICS+方法，使机器人辅助手术系统能够在动态变化的环境中持续学习新的手术工具和组织类型，同时保持对已学知识的记忆，无需访问历史数据，从而提高了手术的精确性和安全性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot-assisted surgeries rely on accurate and real-time scene understandingto safely guide surgical instruments. However, segmentation models trained onstatic datasets face key limitations when deployed in these dynamic andevolving surgical environments. Class-incremental semantic segmentation (CISS)allows models to continually adapt to new classes while avoiding catastrophicforgetting of prior knowledge, without training on previous data. In this work,we build upon the recently introduced Taxonomy-Oriented Poincar\'e-regularizedIncremental Class Segmentation (TOPICS) approach and propose an enhancedvariant, termed TOPICS+, specifically tailored for robust segmentation ofsurgical scenes. Concretely, we incorporate the Dice loss into the hierarchicalloss formulation to handle strong class imbalances, introduce hierarchicalpseudo-labeling, and design tailored label taxonomies for robotic surgeryenvironments. We also propose six novel CISS benchmarks designed for roboticsurgery environments including multiple incremental steps and several semanticcategories to emulate realistic class-incremental settings in surgicalenvironments. In addition, we introduce a refined set of labels with more than144 classes on the Syn-Mediverse synthetic dataset, hosted online as anevaluation benchmark. We make the code and trained models publicly available athttp://topics.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Julia Hindel, Ema Mekic, Enamundram Naga Karthik, Rohit Mohan, Daniele Cattaneo, Maria Kalweit, Abhinav Valada)</author>
      <guid isPermaLink="false">2508.01713v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding</title>
      <link>http://arxiv.org/abs/2508.01150v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IROS2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了OpenGS-Fusion，一种创新的开词汇密集映射框架，结合3D高斯表示和截断符号距离场，实现语义特征的实时无损融合，并通过多模态语言引导的自适应阈值方法显著提高了3D对象分割效果。&lt;h4&gt;背景&lt;/h4&gt;尽管3D场景理解在VR/AR和机器人应用方面取得了显著进展，但现有方法受限于僵化的离线流程，无法根据开放查询提供精确的3D对象级理解。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够提供精确3D对象级理解并支持开放词汇查询交互的框架，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出OpenGS-Fusion框架，结合3D高斯表示与截断符号距离场实现语义特征实时无损融合；引入MLLM-Assisted Adaptive Thresholding方法，通过自适应调整相似度阈值改进3D对象分割。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，该方法在3D对象理解、场景重建质量方面优于现有方法；与固定阈值策略相比，自适应阈值方法使3D mIoU提升了17%；该方法在语言引导的场景交互中表现出色。&lt;h4&gt;结论&lt;/h4&gt;OpenGS-Fusion框架显著提升了3D场景理解和交互能力，为VR/AR和机器人应用提供了更精确的对象级理解工具。&lt;h4&gt;翻译&lt;/h4&gt;最近3D场景理解的进展使得使用开放词汇查询与场景进行交互取得了显著进步，特别是在VR/AR和机器人应用方面。然而，现有方法受限于僵化的离线流程，无法根据开放查询提供精确的3D对象级理解。在本文中，我们提出了OpenGS-Fusion，一种创新的开词汇密集映射框架，改进了语义建模并优化了对象级理解。OpenGS-Fusion将3D高斯表示与截断符号距离场相结合，实现语义特征的实时无损融合。此外，我们引入了一种名为MLLM-Assisted Adaptive Thresholding的新型多模态语言引导方法，通过自适应调整相似度阈值来改进3D对象分割，与固定阈值策略相比，3D mIoU提高了17%。大量实验证明，我们的方法在3D对象理解和场景重建质量方面优于现有方法，同时展示了其在语言引导场景交互中的有效性。代码可在https://young-bit.github.io/opengs-fusion.github.io/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决两个问题：一是传统3D场景理解方法的刚性离线流水线问题，无法支持在线感知；二是现有方法在3D对象级理解上的局限性，无法提供精确的3D对象模型且使用固定阈值导致对象边界划分不准确。这些问题对于VR/AR应用、机器人导航和物体操作等现实应用至关重要，因为这些应用需要通过开放词汇查询与场景实时交互，并获取精确的3D对象信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：点云和体素方法难以高保真重建，NeRF方法效率低下，基于高斯溅射的方法存在离线流水线和3D对象理解不足的问题。在此基础上，作者借鉴了3D高斯溅射的训练效率和显式表示优势，结合截断符号距离场(TSDF)辅助初始化和语义融合，并利用视觉语言模型(如CLIP)提取语义特征。同时，作者创新性地设计了混合场景表示和MLLM辅助的自适应阈值调整机制，解决了现有方法的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用混合场景表示(TSDF-3DGS)同时建模场景的外观、几何和语义特征，并利用多模态大语言模型辅助的自适应阈值调整提高3D对象定位精度。整体流程包括：1)接收RGB-D输入并提取2D语义特征；2)使用基于VDB的TSDF融合更新全局几何；3)在高密度区域初始化高斯原始体并融合语义特征；4)应用场景优化策略提升映射质量；5)通过开放词汇查询和AT-MLLM机制实现精确的3D对象定位。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)OpenGS-Fusion框架，支持在线建模的开放词汇密集映射；2)混合3D高斯场景表示，结合TSDF和3DGS的优势；3)MLLM辅助的自适应阈值调整机制，显著提高对象定位精度。相比之前的工作，OpenGS-Fusion不再依赖离线流水线，支持实时更新；提供精确的3D对象级理解而非仅2D渲染；使用自适应阈值而非固定阈值提高边界划分准确性；同时支持高质量场景重建和对象定位，更适合机器人导航和交互式任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenGS-Fusion通过结合3D高斯溅射与截断符号距离场的混合表示以及多模态大语言模型辅助的自适应阈值调整，实现了支持在线更新的开放词汇3D场景理解，显著提高了3D对象定位精度和场景重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in 3D scene understanding have made significant stridesin enabling interaction with scenes using open-vocabulary queries, particularlyfor VR/AR and robotic applications. Nevertheless, existing methods are hinderedby rigid offline pipelines and the inability to provide precise 3D object-levelunderstanding given open-ended queries. In this paper, we presentOpenGS-Fusion, an innovative open-vocabulary dense mapping framework thatimproves semantic modeling and refines object-level understanding.OpenGS-Fusion combines 3D Gaussian representation with a Truncated SignedDistance Field to facilitate lossless fusion of semantic features on-the-fly.Furthermore, we introduce a novel multimodal language-guided approach namedMLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3Dobjects by adaptively adjusting similarity thresholds, achieving an improvement17\% in 3D mIoU compared to the fixed threshold strategy. Extensive experimentsdemonstrate that our method outperforms existing methods in 3D objectunderstanding and scene reconstruction quality, as well as showcasing itseffectiveness in language-guided scene interaction. The code is available athttps://young-bit.github.io/opengs-fusion.github.io/ .</description>
      <author>example@mail.com (Dianyi Yang, Xihan Wang, Yu Gao, Shiyang Liu, Bohan Ren, Yufeng Yue, Yi Yang)</author>
      <guid isPermaLink="false">2508.01150v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>3D Reconstruction via Incremental Structure From Motion</title>
      <link>http://arxiv.org/abs/2508.01019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures, proceedings in International Bhurban Conference  on Applied Sciences &amp; Technology (IBCAST) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种增量式结构运动（SfM）方法的详细实现，该方法通过逐步整合新视图到三维重建中，能够在稀疏或部分重叠的数据集中恢复场景结构和相机运动。&lt;h4&gt;背景&lt;/h4&gt;从非结构化图像集合中进行精确的三维重建是机器人技术、地图绘制和场景理解等应用中的关键需求。全局式SfM技术依赖完整的图像连接性，对噪声或缺失数据较为敏感。&lt;h4&gt;目的&lt;/h4&gt;展示增量式SfM流程的详细实现，重点关注几何估计的一致性和通过捆集调整进行迭代优化的效果，并评估其在实际应用中的可靠性。&lt;h4&gt;方法&lt;/h4&gt;实现增量式SfM流程，通过逐步整合新视图到重建中，使用捆集调整进行迭代优化，并通过重投影误差和相机轨迹一致性评估重建质量。&lt;h4&gt;主要发现&lt;/h4&gt;增量式SfM能够在稀疏或部分重叠的数据集中恢复场景结构和相机运动，实验验证了其在视觉结构化环境中进行稀疏三维重建的有效性。&lt;h4&gt;结论&lt;/h4&gt;增量式SfM是一种可靠的稀疏三维重建方法，在视觉结构化环境中具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;从非结构化图像集合中进行精确的三维重建是机器人技术、地图绘制和场景理解等应用中的关键需求。虽然全局式结构运动（SfM）技术依赖完整的图像连接性，并且可能对噪声或缺失数据敏感，但增量式SfM提供了更灵活的替代方案。通过逐步将新视图整合到重建中，它使系统能够在稀疏或部分重叠的数据集中恢复场景结构和相机运动。在本文中，我们详细介绍了增量式SfM流程的实现，重点关注几何估计的一致性和通过捆集调整进行迭代优化的效果。我们使用真实数据集演示了该方法，并通过重投影误差和相机轨迹一致性评估了重建质量。结果支持了增量式SfM作为在视觉结构化环境中进行稀疏三维重建的可靠方法的实际效用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从非结构化图像集合中进行准确的3D重建问题。这个问题在现实中非常重要，因为它在机器人导航、地图绘制、场景理解、文化遗产保护和智慧城市发展等多个领域都有广泛应用。相比全局SfM技术，增量式SfM对噪声和缺失数据更具鲁棒性，能够处理稀疏或部分重叠的数据集，这对于实际应用场景中的图像收集尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者重新审视了传统几何方法，强调方法的模块化和可解释性，关注几何估计的一致性和通过捆绑调整进行迭代细化的效果。他们借鉴了多项现有工作：增量SfM的基本框架、捆绑调整优化技术、SIFT特征提取算法、RANSAC几何验证方法、本质矩阵分解和PnP姿态估计算法。作者还与现有的COLMAP工具进行了比较，以验证自己方法的性能和优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过增量式重建从初始图像对开始，逐步添加新图像，而不是一次性处理所有图像，同时通过捆绑调整不断优化相机姿态和3D点位置以保持几何一致性。整体流程包括：1)使用SIFT算法检测图像特征并生成描述符；2)通过暴力匹配和RANSAC进行特征匹配和几何验证；3)选择最佳初始图像对估计相对姿态并三角测量初始3D点；4)使用PnP算法逐步添加新图像并估计其姿态；5)三角测量新的3D点并加入点云；6)通过捆绑调整联合优化所有相机参数和3D点位置。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出了模块化、可解释的增量SfM流水线；2)特别关注了几何估计的一致性和迭代细化效果；3)通过真实数据集和重投影误差分析验证了实用性。相比之前的工作，不同之处在于：相比全局SfM，增量式方法对噪声和缺失数据更鲁棒；相比深度学习方法，重新强调了传统几何方法的透明度和灵活性；相比现有工具如COLMAP，虽然性能相当，但提供了更大的透明度和灵活性，更适合研究和实际应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种模块化、可解释的增量式结构从运动流水线，展示了传统几何方法在从无序图像集进行准确且一致的稀疏3D重建方面的有效性，同时提供了比现有工具更大的透明度和灵活性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D reconstruction from unstructured image collections is a keyrequirement in applications such as robotics, mapping, and scene understanding.While global Structure from Motion (SfM) techniques rely on full imageconnectivity and can be sensitive to noise or missing data, incremental SfMoffers a more flexible alternative. By progressively incorporating new viewsinto the reconstruction, it enables the system to recover scene structure andcamera motion even in sparse or partially overlapping datasets. In this paper,we present a detailed implementation of the incremental SfM pipeline, focusingon the consistency of geometric estimation and the effect of iterativerefinement through bundle adjustment. We demonstrate the approach using a realdataset and assess reconstruction quality through reprojection error and cameratrajectory coherence. The results support the practical utility of incrementalSfM as a reliable method for sparse 3D reconstruction in visually structuredenvironments.</description>
      <author>example@mail.com (Muhammad Zeeshan, Umer Zaki, Syed Ahmed Pasha, Zaar Khizar)</author>
      <guid isPermaLink="false">2508.01019v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Cooperative Perception: A Resource-Efficient Framework for Multi-Drone 3D Scene Reconstruction Using Federated Diffusion and NeRF</title>
      <link>http://arxiv.org/abs/2508.00967v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 3 figures, 1 table, 1 algorithm. Preprint based on NeurIPS  2024 template&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种创新的无人机集群感知系统，通过联邦学习和轻量级模型解决计算限制、低带宽通信和实时场景重建问题&lt;h4&gt;背景&lt;/h4&gt;无人机集群系统面临计算资源有限、通信带宽低以及实时场景重建困难等挑战&lt;h4&gt;目的&lt;/h4&gt;开发一个创新的无人机集群感知系统，解决计算限制、低带宽通信和实时场景重建问题&lt;h4&gt;方法&lt;/h4&gt;使用联邦学习共享扩散模型、采用YOLOv12轻量级语义提取、实施局部NeRF更新、重新设计生成扩散模型用于联合场景重建、添加语义感知压缩协议&lt;h4&gt;主要发现&lt;/h4&gt;该框架能实现高效的多智能体3D/4D场景合成，在保持隐私和可扩展性的同时提升协作场景理解能力&lt;h4&gt;结论&lt;/h4&gt;该提案代表了多智能体AI在自主系统领域的颠覆性进步，具有实际应用潜力&lt;h4&gt;翻译&lt;/h4&gt;该提案引入了一种创新的无人机集群感知系统，旨在解决与计算限制、低带宽通信和实时场景重建相关的问题。该框架通过共享扩散模型的联邦学习、YOLOv12轻量级语义提取和局部NeRF更新，实现了高效的多智能体3D/4D场景合成，同时保持隐私和可扩展性。该框架重新设计了生成扩散模型，用于联合场景重建，并改进了协作场景理解，同时添加了语义感知压缩协议。该方法可以通过模拟和无人机测试平台上的潜在实际部署进行验证，使其成为自主系统多智能体AI的颠覆性进步。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多无人机系统中的'三难困境'：通信瓶颈、计算负担和隐私/可扩展性问题。这个问题在现实中非常重要，因为单个无人机的感知能力受限于物理视角，导致遮挡和视野局限，在复杂场景如城市交通、搜索救援中成为安全瓶颈。现有方法要么丢失重要低级数据，要么通信成本过高，无法满足实时应用需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有合作感知方法的局限性（后期融合丢失信息、早期融合通信成本高、中期融合缺乏可扩展性）来设计方法。他们借鉴了多个领域的现有工作：NeRF用于3D场景表示、扩散模型用于图像生成、联邦学习保护隐私、多终端编码理论提供压缩基础、语义压缩重新定义压缩目标，以及YOLOv12用于轻量级语义提取。这些技术被创新地整合成一个新框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是无人机只交换压缩的语义信息和姿态数据，而非原始数据或特征。使用联邦学习训练共享的生成扩散模型，根据语义提示和姿态生成逼真2D图像，这些图像用于训练或增强局部NeRF模型。整体流程包括：1)请求阶段-目标无人机广播需求；2)数据共享阶段-源无人机发送语义和姿态；3)幻觉阶段-目标无人机生成图像；4)NeRF更新阶段-用生成图像训练局部模型；5)反馈迭代-持续改进模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)生成式扩散与NeRF结合的3D/4D场景合成；2)YOLO增强的语义提取；3)语义感知压缩减少90%以上通信开销；4)在真实无人机平台上的验证。相比之前工作，该方法通信效率更高（&lt;1MB vs &gt;10MB），计算负载更低（源无人机只做轻量级语义提取），隐私保护更好（联邦学习无需共享原始数据），并且整合了多种技术形成一个完整系统。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的资源高效合作感知框架，通过联邦学习和条件扩散模型结合神经辐射场，使无人机群能够在低带宽和计算受限条件下实时构建高保真3D/4D场景，同时保护隐私并确保系统可扩展性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proposal introduces an innovative drone swarm perception system that aimsto solve problems related to computational limitations and low-bandwidthcommunication, and real-time scene reconstruction. The framework enablesefficient multi-agent 3D/4D scene synthesis through federated learning ofshared diffusion model and YOLOv12 lightweight semantic extraction and localNeRF updates while maintaining privacy and scalability. The framework redesignsgenerative diffusion models for joint scene reconstruction, and improvescooperative scene understanding, while adding semantic-aware compressionprotocols. The approach can be validated through simulations and potentialreal-world deployment on drone testbeds, positioning it as a disruptiveadvancement in multi-agent AI for autonomous systems.</description>
      <author>example@mail.com (Massoud Pourmandi)</author>
      <guid isPermaLink="false">2508.00967v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning</title>
      <link>http://arxiv.org/abs/2507.23318v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Vision-Language-Action (VLA)模型在复杂场景理解和动作推理方面显示出巨大潜力，但视觉token长度增加了计算成本。针对自动驾驶场景，本文提出了FastDriveVLA框架，通过MAE风格的像素重建优先考虑前景信息，并设计了对抗性前景-背景重建策略训练ReconPruner剪枝器。该方法在不同剪枝比例下取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;VLA模型在自动驾驶系统中应用广泛，但其视觉token长度导致计算成本高昂。现有视觉token剪枝方法在自动驾驶场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对自动驾驶场景的视觉token剪枝框架，减少VLA模型的计算成本同时保持性能。&lt;h4&gt;方法&lt;/h4&gt;提出FastDriveVLA框架，包含ReconPruner剪枝器，通过MAE风格像素重建和对抗性前景-背景重建策略训练，并引入nuScenes-FG数据集(241K图像-掩码对)。&lt;h4&gt;主要发现&lt;/h4&gt;人类驾驶者关注前景区域，保留前景信息对有效决策至关重要；FastDriveVLA在不同剪枝比例下取得最先进结果。&lt;h4&gt;结论&lt;/h4&gt;FastDriveVLA有效解决了VLA模型在自动驾驶中的计算效率问题，通过优先保留前景信息实现高性能决策。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action (VLA)模型在复杂场景理解和动作推理方面显示出巨大潜力，导致它们在端到端自动驾驶系统中被越来越多地采用。然而，VLA模型的视觉token长度大大增加了计算成本。当前视觉语言模型(VLM)中的视觉token剪枝方法依赖于视觉token相似性或视觉-文本注意力，但在自动驾驶场景中表现不佳。鉴于人类驾驶者在驾驶时会专注于相关的前景区域，我们认为保留包含这种前景信息的视觉token对有效决策至关重要。受此启发，我们提出了FastDriveVLA，一种专为自动驾驶设计的基于重建的视觉token剪枝框架。FastDriveVLA包含一个即插即用的视觉token剪枝器ReconPruner，它通过MAE风格的像素重建优先考虑前景信息。设计了一种新的对抗性前景-背景重建策略来训练VLA模型视觉编码器的ReconPruner。一旦训练完成，ReconPruner可以无缝应用于具有相同视觉编码器的不同VLA模型，无需重新训练。为了训练ReconPruner，我们还引入了一个名为nuScenes-FG的大规模数据集，包含241K带有注释前景区域的图像-掩码对。我们的方法在不同剪裁比例下的nuScenes开环规划基准测试中取得了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have demonstrated significant potentialin complex scene understanding and action reasoning, leading to theirincreasing adoption in end-to-end autonomous driving systems. However, the longvisual tokens of VLA models greatly increase computational costs. Currentvisual token pruning methods in Vision-Language Models (VLM) rely on eithervisual token similarity or visual-text attention, but both have shown poorperformance in autonomous driving scenarios. Given that human driversconcentrate on relevant foreground areas while driving, we assert thatretaining visual tokens containing this foreground information is essential foreffective decision-making. Inspired by this, we propose FastDriveVLA, a novelreconstruction-based vision token pruning framework designed specifically forautonomous driving. FastDriveVLA includes a plug-and-play visual token prunercalled ReconPruner, which prioritizes foreground information through MAE-stylepixel reconstruction. A novel adversarial foreground-background reconstructionstrategy is designed to train ReconPruner for the visual encoder of VLA models.Once trained, ReconPruner can be seamlessly applied to different VLA modelswith the same visual encoder without retraining. To train ReconPruner, we alsointroduce a large-scale dataset called nuScenes-FG, consisting of 241Kimage-mask pairs with annotated foreground regions. Our approach achievesstate-of-the-art results on the nuScenes open-loop planning benchmark acrossdifferent pruning ratios.</description>
      <author>example@mail.com (Jiajun Cao, Qizhe Zhang, Peidong Jia, Xuhui Zhao, Bo Lan, Xiaoan Zhang, Xiaobao Wei, Sixiang Chen, Zhuo Li, Yang Wang, Liyun Li, Xianming Liu, Ming Lu, Shanghang Zhang)</author>
      <guid isPermaLink="false">2507.23318v2</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Variance in Visual Question Answering Benchmarks</title>
      <link>http://arxiv.org/abs/2508.02645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in ICCV 2025 Workshop on What's Next in Multimodal  Foundational Models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了多模态大语言模型在视觉问答任务评估中的性能变异性问题，指出当前评估方法存在局限性，并提出差异感知的评估方法。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型已成为视觉问答的强大工具，能够实现视觉和文本模态间的推理和上下文理解。&lt;h4&gt;目的&lt;/h4&gt;批判性审视当前多模态大语言模型在视觉问答基准测试中评估的局限性，分析导致性能变异的因素，并提出改进的评估方法。&lt;h4&gt;方法&lt;/h4&gt;分析14个广泛使用的VQA基准测试中的性能差异，研究训练种子、框架非确定性、模型规模和扩展指令微调对性能变异性的影响，并探索填空式评估作为替代评估策略。&lt;h4&gt;主要发现&lt;/h4&gt;当前视觉问答基准测试的评估实践存在显著局限性，忽视了随机模型输出、训练种子敏感性和超参数配置等因素导致的性能差异。&lt;h4&gt;结论&lt;/h4&gt;应采用差异感知的评估方法来促进多模态大语言模型更强大和可靠的发展，以提高评估的全面性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型已成为视觉问答的强大工具，能够实现视觉和文本模态间的推理和上下文理解。尽管取得了进步，但在VQA基准测试中对MLLMs的评估通常依赖于点估计，忽视了随机模型输出、训练种子敏感性和超参数配置等因素导致的显著性能差异。本文通过分析14个广泛使用的VQA基准测试中的性能差异，批判性地审视这些问题，这些基准测试涵盖了视觉推理、文本理解和常识推理等多样化任务。我们系统地研究了训练种子、框架非确定性、模型规模和扩展指令微调对性能变异性的影响。此外，我们还探索了填空式评估作为替代评估策略，研究了其在减少随机性和提高基准测试可靠性方面的有效性。研究结果突显了当前评估实践的局限性，并倡导采用差异感知的方法来促进MLLMs更强大和可靠的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have emerged as powerful tools forvisual question answering (VQA), enabling reasoning and contextualunderstanding across visual and textual modalities. Despite their advancements,the evaluation of MLLMs on VQA benchmarks often relies on point estimates,overlooking the significant variance in performance caused by factors such asstochastic model outputs, training seed sensitivity, and hyperparameterconfigurations. This paper critically examines these issues by analyzingvariance across 14 widely used VQA benchmarks, covering diverse tasks such asvisual reasoning, text understanding, and commonsense reasoning. Wesystematically study the impact of training seed, framework non-determinism,model scale, and extended instruction finetuning on performance variability.Additionally, we explore Cloze-style evaluation as an alternate assessmentstrategy, studying its effectiveness in reducing stochasticity and improvingreliability across benchmarks. Our findings highlight the limitations ofcurrent evaluation practices and advocate for variance-aware methodologies tofoster more robust and reliable development of MLLMs.</description>
      <author>example@mail.com (Nikitha SR)</author>
      <guid isPermaLink="false">2508.02645v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.02464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了SAMPO（Segment Anything Model with Preference Optimization），一种新型框架，用于教会视觉基础模型从稀疏视觉交互中推断高层级分类意图，解决了视觉基础模型在提示分割中存在的意图差距问题。&lt;h4&gt;背景&lt;/h4&gt;基础模型如Segment Anything Model (SAM)在提示分割方面表现出色，但存在意图差距：它们只能分割明确提示的对象，无法泛化到用户隐含期望的语义相关实例。这种限制在具有密集同质对象的领域（例如生物医学细胞核分割）中尤为关键，因为稀疏视觉提示通常会导致不完整的结果，而密集标注则因成本过高而不切实际。&lt;h4&gt;目的&lt;/h4&gt;弥合视觉基础模型中的意图差距，使模型能够从稀疏提示中推断用户的隐含意图，实现更完整和准确的分割，特别是在密集同质对象的场景中。&lt;h4&gt;方法&lt;/h4&gt;SAMPO通过偏好优化（preference optimization）来训练视觉基础模型，使其能够从稀疏视觉交互中推断高层级分类意图。与传统的像素级微调不同，SAMPO优化模型以隐式地捕获目标类特征，这种方法不依赖于语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;SAMPO能够在稀疏提示的情况下实现稳健的多目标分割，并在微调过程中表现出卓越的数据效率。在三个医学分割任务上的验证表明，SAMPO达到了最先进的性能：在像PanNuke-T2这样具有挑战性的任务上，仅使用10%的训练数据进行微调，就显著优于在完整100%数据集上训练的所有现有方法，比最佳基线提高了9个百分点以上。&lt;h4&gt;结论&lt;/h4&gt;该研究为视觉基础模型中的意图感知对齐建立了新范式，消除了对辅助提示生成器或语言模型辅助偏好学习的依赖。&lt;h4&gt;翻译&lt;/h4&gt;基础模型如Segment Anything Model (SAM)在提示分割方面表现出色，但存在意图差距：它们只能分割明确提示的对象，无法泛化到用户隐含期望的语义相关实例。这种限制在具有密集同质对象的领域（例如生物医学细胞核分割）中尤为关键，因为稀疏视觉提示通常会导致不完整的结果，而密集标注则因成本过高而不切实际。为了弥合这一差距，我们引入了SAMPO（Segment Anything Model with Preference Optimization），一种新型框架，它教会视觉基础模型从稀疏视觉交互中推断高层级分类意图。与传统的像素级微调不同，SAMPO优化模型以通过偏好优化隐式地捕获目标类特征。这种方法不依赖于语言模型，使得即使在稀疏提示的情况下也能实现稳健的多目标分割，并在微调过程中表现出卓越的数据效率。在三个医学分割任务上的验证表明，SAMPO达到了最先进的性能：在像PanNuke-T2这样具有挑战性的任务上，我们的方法仅使用10%的训练数据进行微调，就显著优于在完整100%数据集上训练的所有现有方法，比最佳基线提高了9个百分点以上。我们的研究为视觉基础模型中的意图感知对齐建立了一种新范式，消除对辅助提示生成器或语言模型辅助偏好学习的依赖。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models like Segment Anything Model (SAM) excel in promptablesegmentation but suffer from an intent gap: they segment only explicitlyprompted objects, failing to generalize to semantically related instancesimplicitly desired by users. This limitation is critical in domains with densehomogeneous objects (e.g., biomedical nuclei segmentation), where sparse visualprompts typically yield incomplete results, rendering dense annotationsimpractical due to prohibitive cost. To bridge this gap, we introduce SAMPO(Segment Anything Model with Preference Optimization), a novel framework thatteaches visual foundation models to infer high-level categorical intent fromsparse visual interactions. Unlike conventional pixel-level fine-tuning, SAMPOoptimizes models to implicitly capture target-class characteristics throughpreference optimization. This approach, which operates without dependency onlanguage models, enables robust multi-object segmentation even under sparseprompting and demonstrates superior data efficiency during fine-tuning.Validated on three medical segmentation tasks, SAMPO achieves state-of-the-artperformance: on challenging tasks like PanNuke-T2, our method, when fine-tunedwith only 10% of the training data, significantly outperforms all existingmethods trained on the full 100% dataset, achieving an improvement of over 9percentage points compared to the best baseline. Our work establishes a newparadigm for intent-aware alignment in visual foundation models, removingdependencies on auxiliary prompt generators or language-model-assistedpreference learning.</description>
      <author>example@mail.com (Yonghuang Wu, Wenwen Zeng, Xuan Xie, Chengqian Zhao, Guoqing Wu, Jinhua Yu)</author>
      <guid isPermaLink="false">2508.02464v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems</title>
      <link>http://arxiv.org/abs/2508.02344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Traffic-R1，一个具有人类推理能力的基础模型，用于交通信号控制系统。该模型通过在模拟环境中使用专家指导的强化大型语言模型进行自我探索和迭代开发，相比传统方法和现有LLM方法具有显著优势，已在实际部署中显示出积极效果。&lt;h4&gt;背景&lt;/h4&gt;交通信号控制对于缓解交通拥堵和维持城市交通流动性至关重要。随着城市交通需求的增长，开发更智能、更高效的交通信号控制系统变得日益重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个具有人类推理能力的基础模型(Traffic-R1)，用于交通信号控制系统，以解决传统方法和现有基于LLM方法的局限性，提高交通信号控制的效率和适应性。&lt;h4&gt;方法&lt;/h4&gt;通过在模拟交通环境中使用专家指导的强化大型语言模型进行自我探索和迭代开发来构建Traffic-R1模型。这种方法结合了强化学习和大型语言模型的优势，使模型能够学习类似人类的交通控制策略。&lt;h4&gt;主要发现&lt;/h4&gt;Traffic-R1相比传统强化学习和近期基于LLM的方法有三个显著优势：1) 实现零样本泛化，能够直接应用于新的道路网络和分布外事件；2) 30亿参数的轻量级架构，支持在移动级芯片上进行实时推理，便于大规模边缘部署；3) 提供可解释的交通信号控制过程，并通过自我迭代和新的同步通信网络促进多路口通信。&lt;h4&gt;结论&lt;/h4&gt;Traffic-R1在基准测试中建立了新的最先进状态，超越了强大的基线和训练密集型强化学习控制器。在实际应用中，该模型已成功管理超过55,000名驾驶员的交通信号，将平均排队时间缩短5%以上，并将操作员工作量减少一半。模型检查点已公开发布，可供进一步研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;交通信号控制对于缓解交通拥堵和维持城市交通流动性至关重要。在本文中，我们介绍了Traffic-R1，一个具有人类推理能力的基础模型，用于交通信号控制系统。我们的模型通过在模拟交通环境中使用专家指导的强化大型语言模型进行自我探索和迭代开发而成。与传统强化学习和近期基于LLM的方法相比，Traffic-R1提供了三个显著优势。首先，Traffic-R1实现了零样本泛化，通过利用其内部交通控制策略和类似人类的推理能力，无需修改即可应用于新的道路网络和分布外事件。其次，其30亿参数的架构足够轻量，可以在移动级芯片上进行实时推理，支持大规模边缘部署。第三，Traffic-R1通过自我迭代和新的同步通信网络，提供可解释的交通信号控制过程，并促进多路口通信。广泛的基准测试表明，Traffic-R1建立了新的最先进状态，超越了强大的基线和训练密集型强化学习控制器。在实际应用中，该模型现在每天为超过55,000名驾驶员管理信号，将平均排队时间缩短5%以上，并将操作员工作量减少一半。我们的模型检查点可在https://huggingface.co/Season998/Traffic-R1获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic signal control (TSC) is vital for mitigating congestion andsustaining urban mobility. In this paper, we introduce Traffic-R1, a foundationmodel with human-like reasoning for TSC systems. Our model is developed throughself-exploration and iteration of reinforced large language models (LLMs) withexpert guidance in a simulated traffic environment. Compared to traditionalreinforcement learning (RL) and recent LLM-based methods, Traffic-R1 offersthree significant advantages. First, Traffic-R1 delivers zero-shotgeneralisation, transferring unchanged to new road networks andout-of-distribution incidents by utilizing its internal traffic controlpolicies and human-like reasoning. Second, its 3B-parameter architecture islightweight enough for real-time inference on mobile-class chips, enablinglarge-scale edge deployment. Third, Traffic-R1 provides an explainable TSCprocess and facilitates multi-intersection communication through itsself-iteration and a new synchronous communication network. Extensivebenchmarks demonstrate that Traffic-R1 sets a new state of the art,outperforming strong baselines and training-intensive RL controllers. Inpractice, the model now manages signals for more than 55,000 drivers daily,shortening average queues by over 5% and halving operator workload. Ourcheckpoint is available at https://huggingface.co/Season998/Traffic-R1.</description>
      <author>example@mail.com (Xingchen Zou, Yuhao Yang, Zheng Chen, Xixuan Hao, Yiqi Chen, Chao Huang, Yuxuan Liang)</author>
      <guid isPermaLink="false">2508.02344v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Qwen-Image Technical Report</title>
      <link>http://arxiv.org/abs/2508.02324v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/QwenLM/Qwen-Image&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了Qwen-Image图像生成基础模型，该模型在复杂文本渲染和精确图像编辑方面取得了显著进展。通过全面的数据管道设计和渐进式训练策略，模型在字母语言和表意语言上都表现出色。同时，通过改进的多任务训练范式和双编码机制，有效提高了图像编辑的一致性和质量。&lt;h4&gt;背景&lt;/h4&gt;图像生成技术在文本渲染和图像编辑方面仍面临挑战，特别是在复杂文本处理和保持编辑一致性方面。现有方法在处理不同语言类型和保持语义一致性方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在复杂文本渲染和精确图像编辑方面取得显著进展的图像生成基础模型，提高模型在多种语言上的表现，并增强图像编辑的一致性和质量。&lt;h4&gt;方法&lt;/h4&gt;1. 设计全面的数据管道，包括大规模数据收集、过滤、标注、合成和平衡2. 采用渐进式训练策略，从简单到复杂逐步提升模型能力3. 引入改进的多任务训练范式，整合T2I、TI2I和I2I任务4. 实施双编码机制，分别获取语义和重建表示5. 对齐Qwen2.5-VL和MMDiT之间的潜在表示&lt;h4&gt;主要发现&lt;/h4&gt;1. Qwen-Image在复杂文本渲染方面取得了显著进展2. 模型在英语等字母语言和中文等表意语言上都表现出色3. 改进的多任务训练范式有效提高了图像编辑的一致性4. 双编码机制使编辑模块能够在语义一致性和视觉保真度之间取得平衡5. 模型在多个基准测试中达到了最先进的性能&lt;h4&gt;结论&lt;/h4&gt;Qwen-Image是一个强大的图像生成基础模型，通过创新的数据处理方法、训练策略和技术架构，在复杂文本渲染和精确图像编辑方面取得了突破性进展，为图像生成领域的发展提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Qwen-Image，这是Qwen系列中的一个图像生成基础模型，在复杂文本渲染和精确图像编辑方面取得了显著进展。为解决复杂文本渲染的挑战，我们设计了一个全面的数据管道，包括大规模数据收集、过滤、标注、合成和平衡。此外，我们采用了一种渐进式训练策略，从非文本到文本渲染开始，从简单到复杂的文本输入逐步演变，最终扩展到段落级描述。这种课程学习方法显著增强了模型的原生文本渲染能力。因此，Qwen-Image不仅在英语等字母语言上表现出色，还在中文等更困难的表意语言上取得了显著进展。为提高图像编辑的一致性，我们引入了改进的多任务训练范式，不仅包括传统的文本到图像(T2I)和文本图像到图像(TI2I)任务，还包括图像到图像(I2I)重建，有效对齐了Qwen2.5-VL和MMDiT之间的潜在表示。此外，我们将原始图像分别输入Qwen2.5-VL和VAE编码器，以获取语义和重建表示。这种双编码机制使编辑模块能够在保持语义一致性和维护视觉保真度之间取得平衡。Qwen-Image达到了最先进的性能，展示了在多个基准测试中图像生成和编辑的强大能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Qwen-Image, an image generation foundation model in the Qwenseries that achieves significant advances in complex text rendering and preciseimage editing. To address the challenges of complex text rendering, we design acomprehensive data pipeline that includes large-scale data collection,filtering, annotation, synthesis, and balancing. Moreover, we adopt aprogressive training strategy that starts with non-text-to-text rendering,evolves from simple to complex textual inputs, and gradually scales up toparagraph-level descriptions. This curriculum learning approach substantiallyenhances the model's native text rendering capabilities. As a result,Qwen-Image not only performs exceptionally well in alphabetic languages such asEnglish, but also achieves remarkable progress on more challenging logographiclanguages like Chinese. To enhance image editing consistency, we introduce animproved multi-task training paradigm that incorporates not only traditionaltext-to-image (T2I) and text-image-to-image (TI2I) tasks but alsoimage-to-image (I2I) reconstruction, effectively aligning the latentrepresentations between Qwen2.5-VL and MMDiT. Furthermore, we separately feedthe original image into Qwen2.5-VL and the VAE encoder to obtain semantic andreconstructive representations, respectively. This dual-encoding mechanismenables the editing module to strike a balance between preserving semanticconsistency and maintaining visual fidelity. Qwen-Image achievesstate-of-the-art performance, demonstrating its strong capabilities in bothimage generation and editing across multiple benchmarks.</description>
      <author>example@mail.com (Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, Zenan Liu)</author>
      <guid isPermaLink="false">2508.02324v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Do Edges Matter? Investigating Edge-Enhanced Pre-Training for Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.02281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, Third International Workshop on Data Engineering  in Medical Imaging (DEMI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了边缘增强预训练对医学图像分割模型性能的影响，并提出了一种元学习策略来优化预训练方法的选择，显著提高了跨模态医学图像分割的总体性能。&lt;h4&gt;背景&lt;/h4&gt;医学图像分割对疾病诊断和治疗规划至关重要，但开发强大的分割模型需要大量计算资源和大型数据集。虽然预训练和微调基础模型可以提高分割性能，但特定图像预处理步骤如何影响不同医学成像模态的分割性能仍不清楚。边缘作为像素强度的急剧变化，被广泛认为是物体边界的重要线索，但在基础模型的预训练中尚未得到系统性研究。&lt;h4&gt;目的&lt;/h4&gt;研究使用计算高效的边缘核（如kirsch）处理数据进行预训练，能在多大程度上提高基础模型的跨模态分割能力。&lt;h4&gt;方法&lt;/h4&gt;在多个医学成像模态上训练两个版本的基础模型：一个使用原始数据，一个使用边缘增强数据，然后在针对特定医学模态选择的原始数据子集上进行微调。在Dermoscopy、Fundus、Mammography、Microscopy、OCT、US和XRay等医学领域进行系统研究。提出一种元学习策略，使用原始图像的标准差和图像熵来选择在边缘增强数据或原始数据上预训练的模型。&lt;h4&gt;主要发现&lt;/h4&gt;使用边缘聚焦预训练在不同模态上显示出提高和降低的分割性能，表明需要选择性应用这种方法。集成元学习层相比仅在边缘增强数据上预训练的模型，在各种医学成像任务中总体分割性能提高了16.42%；相比仅在原始数据上预训练的模型，总体分割性能提高了19.30%。&lt;h4&gt;结论&lt;/h4&gt;边缘增强预训练可以提高基础模型的跨模态分割能力，但需要根据具体情况选择性应用。提出的元学习策略可以有效指导选择适当的预训练方法，显著提高分割性能。&lt;h4&gt;翻译&lt;/h4&gt;医学图像分割对疾病诊断和治疗规划至关重要，然而开发强大的分割模型通常需要大量计算资源和大型数据集。现有研究表明，预训练和微调的基础模型可以提高分割性能。然而，关于特定图像预处理步骤如何影响不同医学成像模态的分割性能的问题仍然存在。特别是，边缘（像素强度的急剧变化）被广泛认为是物体边界的重要线索，但在基础模型的预训练中尚未得到系统性研究。我们通过研究使用计算高效的边缘核（如kirsch）处理数据进行预训练能在多大程度上提高基础模型的跨模态分割能力来填补这一空白。首先在多个医学成像模态上使用原始数据或边缘增强数据训练基础模型的两个版本，然后在针对特定医学模态选择的原始数据子集上进行微调。在使用Dermoscopy、Fundus、Mammography、Microscopy、OCT、US和XRay等医学领域进行系统研究后，我们发现使用边缘聚焦预训练在不同模态上显示出提高和降低的分割性能，表明需要选择性应用这种方法。为指导此类选择性应用，我们提出了一种元学习策略，它使用原始图像的标准差和图像熵来选择在边缘增强数据或原始数据上预训练的模型以获得最佳性能。我们的实验表明，与仅在边缘增强数据上预训练的模型相比，集成这种元学习层在各种医学成像任务中总体分割性能提高了16.42%；与仅在原始数据上预训练的模型相比，提高了19.30%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical image segmentation is crucial for disease diagnosis and treatmentplanning, yet developing robust segmentation models often requires substantialcomputational resources and large datasets. Existing research shows thatpre-trained and finetuned foundation models can boost segmentation performance.However, questions remain about how particular image preprocessing steps mayinfluence segmentation performance across different medical imaging modalities.In particular, edges-abrupt transitions in pixel intensity-are widelyacknowledged as vital cues for object boundaries but have not beensystematically examined in the pre-training of foundation models. We addressthis gap by investigating to which extend pre-training with data processedusing computationally efficient edge kernels, such as kirsch, can improvecross-modality segmentation capabilities of a foundation model. Two versions ofa foundation model are first trained on either raw or edge-enhanced data acrossmultiple medical imaging modalities, then finetuned on selected raw subsetstailored to specific medical modalities. After systematic investigation usingthe medical domains Dermoscopy, Fundus, Mammography, Microscopy, OCT, US, andXRay, we discover both increased and reduced segmentation performance acrossmodalities using edge-focused pre-training, indicating the need for a selectiveapplication of this approach. To guide such selective applications, we proposea meta-learning strategy. It uses standard deviation and image entropy of theraw image to choose between a model pre-trained on edge-enhanced or on raw datafor optimal performance. Our experiments show that integrating thismeta-learning layer yields an overall segmentation performance improvementacross diverse medical imaging tasks by 16.42% compared to models pre-trainedon edge-enhanced data only and 19.30% compared to models pre-trained on rawdata only.</description>
      <author>example@mail.com (Paul Zaha, Lars Böcking, Simeon Allmendinger, Leopold Müller, Niklas Kühl)</author>
      <guid isPermaLink="false">2508.02281v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for Text-to-Image Generation</title>
      <link>http://arxiv.org/abs/2508.02107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的框架，通过语义驱动的LoRA检索和动态聚合，解决了分布式开源LoRA模块在图像生成模型中的有效利用问题，实现了图像生成性能的显著提升。&lt;h4&gt;背景&lt;/h4&gt;尽管FLUX和Stable Diffusion v3等大规模模型在照片级真实感图像生成方面取得了进展，但这些架构的实际部署受到参数微调复杂性的限制。低秩适应（LoRA）虽然能以最小参数开销实现模型定制，但分布式开源LoRA模块的有效利用面临三个关键挑战：稀疏的元数据标注、零样本适应能力的需求，以及多LoRA融合策略的次优融合策略。&lt;h4&gt;目的&lt;/h4&gt;解决分布式开源LoRA模块有效利用面临的三个关键挑战，实现语义驱动的LoRA检索和动态聚合，从而提高图像生成性能，促进基础模型的可扩展和数据高效增强。&lt;h4&gt;方法&lt;/h4&gt;作者引入了一个包含两个关键组件的框架：(1) 基于权重编码的LoRA检索器，在LoRA参数矩阵和文本提示之间建立共享语义空间，消除对原始训练数据的依赖；(2) 细粒度门控融合机制，在网络层和扩散时间步上计算上下文特定的融合权重，以在生成过程中最优地集成多个LoRA模块。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在图像生成性能上取得了显著改进，实现了基础模型的可扩展和数据高效增强。这项工作在社区开发的碎片化LoRA景观与实际部署需求之间建立了关键桥梁。&lt;h4&gt;结论&lt;/h4&gt;通过标准化适配器集成，该方法实现了协作模型演进，解决了LoRA模块在图像生成模型中的有效利用问题。&lt;h4&gt;翻译&lt;/h4&gt;尽管通过FLUX和Stable Diffusion v3等大规模模型在照片级真实感图像生成方面取得了最近进展，但这些架构的实际部署仍然受到其参数微调固有复杂性的限制。虽然低秩适应（LoRA）已经证明在以最小参数开销实现模型定制方面有效，但分布式开源LoRA模块的有效利用面临三个关键挑战：稀疏的元数据标注、零样本适应能力的需求，以及多LoRA融合策略的次优融合策略。为了解决这些限制，我们引入了一个新颖的框架，通过两个关键组件实现语义驱动的LoRA检索和动态聚合：(1) 基于权重编码的LoRA检索器，在LoRA参数矩阵和文本提示之间建立共享语义空间，消除对原始训练数据的依赖；(2) 细粒度门控融合机制，在网络层和扩散时间步上计算上下文特定的融合权重，以在生成过程中最优地集成多个LoRA模块。我们的方法在图像生成性能上取得了显著改进，从而促进基础模型的可扩展和数据高效增强。这项工作在社区开发的碎片化LoRA景观与实际部署需求之间建立了关键桥梁，通过标准化适配器集成实现协作模型演进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite recent advances in photorealistic image generation throughlarge-scale models like FLUX and Stable Diffusion v3, the practical deploymentof these architectures remains constrained by their inherent intractability toparameter fine-tuning. While low-rank adaptation (LoRA) have demonstratedefficacy in enabling model customization with minimal parameter overhead, theeffective utilization of distributed open-source LoRA modules faces threecritical challenges: sparse metadata annotation, the requirement for zero-shotadaptation capabilities, and suboptimal fusion strategies for multi-LoRA fusionstrategies. To address these limitations, we introduce a novel framework thatenables semantic-driven LoRA retrieval and dynamic aggregation through two keycomponents: (1) weight encoding-base LoRA retriever that establishes a sharedsemantic space between LoRA parameter matrices and text prompts, eliminatingdependence on original training data, and (2) fine-grained gated fusionmechanism that computes context-specific fusion weights across network layersand diffusion timesteps to optimally integrate multiple LoRA modules duringgeneration. Our approach achieves significant improvement in image generationperfermance, thereby facilitating scalable and data-efficient enhancement offoundational models. This work establishes a critical bridge between thefragmented landscape of community-developed LoRAs and practical deploymentrequirements, enabling collaborative model evolution through standardizedadapter integration.</description>
      <author>example@mail.com (Zhiwen Li, Zhongjie Duan, Die Chen, Cen Chen, Daoyuan Chen, Yaliang Li, Yingda Chen)</author>
      <guid isPermaLink="false">2508.02107v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2508.02062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference on Robot Learning 2025 (CoRL 2025), 17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为RICL(为上下文学习重新训练)的方法，能够向预训练的视觉-语言-动作(VLA)模型注入上下文学习(ICL)能力，使机器人模型能够通过少量演示学习新任务，无需参数更新。&lt;h4&gt;背景&lt;/h4&gt;多任务'视觉-语言-动作'(VLA)模型作为机器人领域的通用基础模型表现出色，但终端用户缺乏简单的方法来改进这些模型。语言和视觉模型已通过上下文学习(ICL)实现了无需参数微调的任务教学，但通过模仿学习预训练的VLA模型不具备这种自然获得的ICL能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，使预训练的VLA模型能够获得上下文学习(ICL)能力，允许终端用户通过少量演示轻松教授新任务，无需进行参数微调。&lt;h4&gt;方法&lt;/h4&gt;通过特定的微调方法和一个小型机器人演示数据集，对预训练的VLA模型进行重新训练(RICL)，以注入上下文适应能力。RICL能够从用户提供的少量演示(10-20个)中提取最相关部分，并将其注入到VLA的上下文中，利用ICL执行新任务。&lt;h4&gt;主要发现&lt;/h4&gt;将RICL应用于π0-FAST VLA模型后，仅通过每个任务20个演示且无需参数更新，就能显著提高各种新操作任务的性能。当允许对目标任务演示进行参数更新时，RICL微调可以进一步提升性能。&lt;h4&gt;结论&lt;/h4&gt;RICL方法成功地向预训练的VLA模型注入了上下文学习能力，为机器人操作任务提供了一种简单而有效的接口，使用户能够通过少量演示教授新任务，无需复杂的参数调整。&lt;h4&gt;翻译&lt;/h4&gt;多任务'视觉-语言-动作'(VLA)模型最近显示出作为机器人通用基础模型的巨大潜力，能够在全新环境中的新任务上取得非平凡的初始性能。然而，为了使这类模型真正实用，终端用户必须拥有简单的方法来教会它们改进。对于语言和视觉模型，涌现出的上下文学习(ICL)能力已被证明是一种多功能且非常有用的接口，可以通过无需参数微调的方式轻松教授新任务。不幸的是，通过模仿学习目标预训练的VLA模型不会自然获得ICL能力。在本文中，我们证明，通过正确的微调方法和一个小型机器人演示数据集，可以在事后向这样的VLA注入上下文适应性。在为上下文学习重新训练(RICL)后，我们的系统允许终端用户为新的任务提供少量(10-20个)演示。RICL随后将这些演示中最相关的部分提取到VLA上下文中，以利用ICL执行新任务并提高任务性能。我们将RICL应用于向π0-FAST VLA注入ICL，并表明仅通过每个任务20个演示，无需任何参数更新，就能为各种新的操作任务带来大幅的上下文改进。当可以对目标任务演示进行参数更新时，RICL微调可以进一步提高性能。我们随论文一起发布了RICL-π0-FAST的代码和模型权重，首次为新的操作任务提供了一个简单的上下文学习接口。网站：https://ricl-vla.github.io。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-task ``vision-language-action'' (VLA) models have recently demonstratedincreasing promise as generalist foundation models for robotics, achievingnon-trivial performance out of the box on new tasks in new environments.However, for such models to be truly useful, an end user must have easy meansto teach them to improve. For language and vision models, the emergent abilityto perform in-context learning (ICL) has proven to be a versatile and highlyuseful interface to easily teach new tasks with no parameter finetuning.Unfortunately, VLAs pre-trained with imitation learning objectives do notnaturally acquire ICL abilities. In this paper, we demonstrate that, with theright finetuning recipe and a small robot demonstration dataset, it is possibleto inject in-context adaptability post hoc into such a VLA. After retrainingfor in-context learning (RICL), our system permits an end user to provide asmall number (10-20) of demonstrations for a new task. RICL then fetches themost relevant portions of those demonstrations into the VLA context to exploitICL, performing the new task and boosting task performance. We apply RICL toinject ICL into the $\pi_{0}$-FAST VLA, and show that it permits largein-context improvements for a variety of new manipulation tasks with only 20demonstrations per task, without any parameter updates. When parameter updateson the target task demonstrations is possible, RICL finetuning further boostsperformance. We release code and model weights for RICL-$\pi_{0}$-FASTalongside the paper to enable, for the first time, a simple in-context learninginterface for new manipulation tasks. Website: https://ricl-vla.github.io.</description>
      <author>example@mail.com (Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, Insup Lee)</author>
      <guid isPermaLink="false">2508.02062v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Generative Large-Scale Pre-trained Models for Automated Ad Bidding Optimization</title>
      <link>http://arxiv.org/abs/2508.02002v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出GRAD模型，一种结合动作专家混合模块和因果变换器价值估计器的可扩展自动出价基础模型，解决了生成方法面临的分布偏移、探索有限和约束满足等挑战，显著提高了平台收入。&lt;h4&gt;背景&lt;/h4&gt;现代自动出价系统需要平衡整体性能、广告商目标和现实约束，条件生成模型如transformers和diffusers的最新进展为传统基于马尔可夫决策过程的方法提供了替代方案。&lt;h4&gt;目的&lt;/h4&gt;解决生成方法在自动出价系统中面临的离线和在线环境分布偏移、动作空间探索有限以及满足CPM和ROI等约束的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出GRAD（基于生成奖励的自动出价与专家混合模型），结合动作专家混合模块用于多样化出价动作探索，以及因果变换器价值估计器用于约束感知优化。&lt;h4&gt;主要发现&lt;/h4&gt;大量离线和在线实验证明GRAD显著提高了平台收入；在美团的实施导致GMV增加2.18%，ROI增加10.68%。&lt;h4&gt;结论&lt;/h4&gt;GRAD能有效应对现代广告商不断变化和多样化的需求，是解决自动出价系统复杂挑战的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;现代自动出价系统需要在整体性能、多样化的广告商目标和现实约束之间取得平衡，反映了行业动态发展的需求。条件生成模型（如transformers和diffusers）的最新进展使得能够根据广告商偏好直接生成轨迹，这为传统基于马尔可夫决策过程的方法提供了有希望的替代方案。然而，这些生成方法面临重大挑战，如离线和在线环境之间的分布偏移、动作空间的探索有限，以及满足边际千次展示成本(CPM)和投资回报率(ROI)等约束的必要性。为了应对这些挑战，我们提出了GRAD（基于生成奖励的自动出价与专家混合模型），这是一种可扩展的基础模型，它结合了用于多样化出价动作探索的'动作专家混合'模块和用于约束感知优化的'因果变换器价值估计器'。大量的离线和在线实验证明，GRAD显著提高了平台收入，突显了其在应对现代广告商不断变化和多样化需求方面的有效性。此外，GRAD已在美团（全球最大的在线食品配送平台之一）的多个营销场景中实施，导致商品交易总额(GMV)增加2.18%，投资回报率(ROI)增加10.68%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern auto-bidding systems are required to balance overall performance withdiverse advertiser goals and real-world constraints, reflecting the dynamic andevolving needs of the industry. Recent advances in conditional generativemodels, such as transformers and diffusers, have enabled direct trajectorygeneration tailored to advertiser preferences, offering a promising alternativeto traditional Markov Decision Process-based methods. However, these generativemethods face significant challenges, such as the distribution shift betweenoffline and online environments, limited exploration of the action space, andthe necessity to meet constraints like marginal Cost-per-Mille (CPM) and Returnon Investment (ROI). To tackle these challenges, we propose GRAD (GenerativeReward-driven Ad-bidding with Mixture-of-Experts), a scalable foundation modelfor auto-bidding that combines an Action-Mixture-of-Experts module for diversebidding action exploration with the Value Estimator of Causal Transformer forconstraint-aware optimization. Extensive offline and online experimentsdemonstrate that GRAD significantly enhances platform revenue, highlighting itseffectiveness in addressing the evolving and diverse requirements of modernadvertisers. Furthermore, GRAD has been implemented in multiple marketingscenarios at Meituan, one of the world's largest online food deliveryplatforms, leading to a 2.18% increase in Gross Merchandise Value (GMV) and10.68% increase in ROI.</description>
      <author>example@mail.com (Yu Lei, Jiayang Zhao, Yilei Zhao, Zhaoqi Zhang, Linyou Cai, Qianlong Xie, Xingxing Wang)</author>
      <guid isPermaLink="false">2508.02002v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models</title>
      <link>http://arxiv.org/abs/2508.01731v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpectralX是一种创新的参数高效微调框架，通过两阶段训练方法使现有遥感基础模型能够适应处理各种光谱模态，无需大量光谱预训练，从而提高领域泛化性能并能够解释来自新区域或季节的光谱图像。&lt;h4&gt;背景&lt;/h4&gt;遥感基础模型(RSFMs)最近取得了显著突破，但许多模型使用大量光学图像进行预训练，而多光谱/高光谱数据缺乏相应的基础模型。&lt;h4&gt;目的&lt;/h4&gt;探索现有RSFMs是否可以有效适应处理各种光谱模态而无需大量光谱预训练，以利用光谱图像在地球观测中的优势。&lt;h4&gt;方法&lt;/h4&gt;提出SpectralX框架，使用现有RSFMs作为骨干网络，采用两阶段训练方法：第一阶段使用掩码重建任务和专门的超标记器(HyperT)提取属性令牌，开发面向属性的适配器混合(AoMoA)；第二阶段以语义分割为下游任务，插入属性细化适配器(Are-adapter)，通过迭代查询低层语义特征与高层表示，使模型关注任务有益属性。&lt;h4&gt;主要发现&lt;/h4&gt;SpectralX能够显著提高领域泛化性能，并能够解释来自新区域或季节的光谱图像。&lt;h4&gt;结论&lt;/h4&gt;通过两阶段适应过程，SpectralX使现有遥感基础模型能够有效处理各种光谱模态，无需大量光谱预训练。&lt;h4&gt;翻译&lt;/h4&gt;遥感基础模型(RSFMs)的最新进展在该领域取得了重大突破。虽然许多RSFMs使用大量光学图像进行了预训练，但更多多光谱/高光谱数据仍缺乏相应的基础模型。为了利用光谱图像在地球观测中的优势，我们探索了现有RSFMs是否可以有效地适应处理各种光谱模态，而无需大量光谱预训练。为应对这一挑战，我们提出了SpectralX，一种创新的参数高效微调框架，它将现有RSFMs作为骨干网络，同时引入两阶段训练方法来处理各种光谱输入，从而显著提高了领域泛化性能。在第一阶段，我们采用掩码重建任务，并设计了一个专门的超标记器(HyperT)来从空间和光谱维度提取属性令牌。同时，我们开发了一个面向属性的适配器混合(AoMoA)，在逐层微调的同时动态聚合多属性专家知识。在第二阶段，以语义分割作为下游任务，我们在第一阶段框架中插入了一个属性细化适配器(Are-adapter)。通过用高层表示迭代查询低层语义特征，模型学会关注任务有益的属性，从而实现对RSFMs的定制调整。经过这两阶段适应过程，SpectralX能够解释来自新区域或季节的光谱图像。代码将在网站https://github.com/YuxiangZhang-BIT上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Remote Sensing Foundation Models (RSFMs) have led tosignificant breakthroughs in the field. While many RSFMs have been pretrainedwith massive optical imagery, more multispectral/hyperspectral data remain lackof the corresponding foundation models. To leverage the advantages of spectralimagery in earth observation, we explore whether existing RSFMs can beeffectively adapted to process diverse spectral modalities without requiringextensive spectral pretraining. In response to this challenge, we proposedSpectralX, an innovative parameter-efficient fine-tuning framework that adaptexisting RSFMs as backbone while introducing a two-stage training approach tohandle various spectral inputs, thereby significantly improving domaingeneralization performance. In the first stage, we employ amasked-reconstruction task and design a specialized Hyper Tokenizer (HyperT) toextract attribute tokens from both spatial and spectral dimensions.Simultaneously, we develop an Attribute-oriented Mixture of Adapter (AoMoA)that dynamically aggregates multi-attribute expert knowledge while performinglayer-wise fine-tuning. With semantic segmentation as downstream task in thesecond stage, we insert an Attribute-refined Adapter (Are-adapter) into thefirst stage framework. By iteratively querying low-level semantic features withhigh-level representations, the model learns to focus on task-beneficialattributes, enabling customized adjustment of RSFMs. Following this two-phaseadaptation process, SpectralX is capable of interpreting spectral imagery fromnew regions or seasons. The codes will be available from the website:https://github.com/YuxiangZhang-BIT.</description>
      <author>example@mail.com (Yuxiang Zhang, Wei Li, Mengmeng Zhang, Jiawei Han, Ran Tao, Shunlin Liang)</author>
      <guid isPermaLink="false">2508.01731v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>OccamVTS: Distilling Vision Models to 1% Parameters for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2508.01727v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为OccamVTS的知识蒸馏框架，能够从大型视觉模型中仅提取1%的必要参数用于时间序列预测，在保持或提高预测精度的同时大幅减少了模型复杂度。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测是多种应用的基础，近期研究利用大型视觉模型(LVMs)通过视觉表示来捕获时间模式。&lt;h4&gt;目的&lt;/h4&gt;解决大型视觉模型应用于时间序列预测时参数冗余的问题，提取关键预测信息并构建轻量级网络。&lt;h4&gt;方法&lt;/h4&gt;提出OccamVTS知识蒸馏框架，使用预训练LVMs作为教师模型，采用金字塔式特征对齐结合相关性和特征蒸馏，转移有益模式同时过滤语义噪声。&lt;h4&gt;主要发现&lt;/h4&gt;时间序列与低级纹理特征对齐而非高级语义，99%的LVMs参数对时间序列任务是不必要的；通过减少参数可以消除对无关视觉特征的过拟合，提高预测准确性。&lt;h4&gt;结论&lt;/h4&gt;OccamVTS在多个基准数据集上实现了最先进性能，仅使用原始参数的1%，尤其在少样本和零样本场景中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测是多种应用的基础，近期方法利用大型视觉模型(LVMs)通过视觉表示来捕获时间模式。研究表明，虽然视觉模型提高了预测性能，但99%的参数对时间序列任务是不必要的。通过跨模态分析，我们发现时间序列与低级纹理特征对齐而非高级语义，这可能损害预测准确性。我们提出了OccamVTS，一个知识蒸馏框架，只从LVMs中提取必要的1%预测信息到轻量级网络中。使用预训练LVMs作为特权教师，OccamVTS采用金字塔式特征对齐结合相关性和特征蒸馏，转移有益模式同时过滤语义噪声。反直觉的是，这种激进的参数减少通过消除对无关视觉特征的过拟合并保留基本时间模式提高了准确性。在多个基准数据集上的广泛实验表明，OccamVTS仅使用原始参数的1%就持续实现了最先进的性能，尤其在少样本和零样本场景中表现优异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting is fundamental to diverse applications, with recentapproaches leverage large vision models (LVMs) to capture temporal patternsthrough visual representations. We reveal that while vision models enhanceforecasting performance, 99% of their parameters are unnecessary for timeseries tasks. Through cross-modal analysis, we find that time series align withlow-level textural features but not high-level semantics, which can impairforecasting accuracy. We propose OccamVTS, a knowledge distillation frameworkthat extracts only the essential 1% of predictive information from LVMs intolightweight networks. Using pre-trained LVMs as privileged teachers, OccamVTSemploys pyramid-style feature alignment combined with correlation and featuredistillation to transfer beneficial patterns while filtering out semanticnoise. Counterintuitively, this aggressive parameter reduction improvesaccuracy by eliminating overfitting to irrelevant visual features whilepreserving essential temporal patterns. Extensive experiments across multiplebenchmark datasets demonstrate that OccamVTS consistently achievesstate-of-the-art performance with only 1% of the original parameters,particularly excelling in few-shot and zero-shot scenarios.</description>
      <author>example@mail.com (Sisuo Lyu, Siru Zhong, Weilin Ruan, Qingxiang Liu, Qingsong Wen, Hui Xiong, Yuxuan Liang)</author>
      <guid isPermaLink="false">2508.01727v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Towards Zero-Shot Terrain Traversability Estimation: Challenges and Opportunities</title>
      <link>http://arxiv.org/abs/2508.01715v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and presented at the 1st German Robotics Conference (GRC);  March 13-15, 2025, Nuremberg, Germany  https://ras.papercept.net/conferences/conferences/GRC25/program/GRC25_ContentListWeb_3.html#sada_48&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了使用视觉语言模型进行地形可通行性估计的可能性，特别是在非结构化环境中。研究引入了一个人工标注的水域可通行性数据集，并提出了一种整合视觉语言模型的简单管道进行零样本估计。实验结果表明当前基础模型在实际应用中尚不成熟，但为未来研究提供了方向。&lt;h4&gt;背景&lt;/h4&gt;地形可通行性估计对自主机器人在非结构化环境中至关重要，视觉线索和推理在其中起关键作用。虽然视觉语言模型具有零样本估计的潜力，但这一问题本质上是不适定的。&lt;h4&gt;目的&lt;/h4&gt;探索视觉语言模型在零样本地形可通行性估计中的应用潜力，并评估当前基础模型在这一任务上的适用性。&lt;h4&gt;方法&lt;/h4&gt;研究引入了一个包含人类对水域可通行性评分的人工标注数据集，并提出了一个整合视觉语言模型的简单管道进行零样本可通行性估计。通过实验评估了该方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;尽管地形可通行性估计是主观的，但人类评分者之间仍表现出一定的一致性。实验结果喜忧参半，表明当前基础模型在实际部署中尚未成熟。&lt;h4&gt;结论&lt;/h4&gt;当前基础模型尚不适合实际部署，但研究结果为可通行性估计领域的进一步研究提供了有价值的见解和方向。&lt;h4&gt;翻译&lt;/h4&gt;地形可通行性估计对自主机器人至关重要，特别是在非结构化环境中，视觉线索和推理起着关键作用。虽然视觉语言模型为零样本估计提供了可能性，但该问题本质上是不适定的。为了探索这一点，我们引入了一个包含人工标注的水域可通行性评分的小型数据集，揭示尽管估计是主观的，但人工评分者仍表现出一定的一致性。此外，我们提出了一种整合视觉语言模型进行零样本可通行性估计的简单管道。我们的实验结果喜忧参半，表明当前基础模型尚不适合实际部署，但为进一步研究提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Terrain traversability estimation is crucial for autonomous robots,especially in unstructured environments where visual cues and reasoning play akey role. While vision-language models (VLMs) offer potential for zero-shotestimation, the problem remains inherently ill-posed. To explore this, weintroduce a small dataset of human-annotated water traversability ratings,revealing that while estimations are subjective, human raters still show someconsensus. Additionally, we propose a simple pipeline that integrates VLMs forzero-shot traversability estimation. Our experiments reveal mixed results,suggesting that current foundation models are not yet suitable for practicaldeployment but provide valuable insights for further research.</description>
      <author>example@mail.com (Ida Germann, Mark O. Mints, Peer Neubert)</author>
      <guid isPermaLink="false">2508.01715v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe</title>
      <link>http://arxiv.org/abs/2508.01691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Voxlect是一个用于建模世界范围内方言和区域语言的语音基础模型的新型基准测试，涵盖了英语、阿拉伯语、汉语、藏语、印度语言等多种语言的方言评估。&lt;h4&gt;背景&lt;/h4&gt;缺乏针对全球范围内方言和区域语言变体的系统性基准测试，现有语音模型在处理方言差异方面表现不足。&lt;h4&gt;目的&lt;/h4&gt;创建一个全面的基准测试来评估语音基础模型在识别和分类不同方言和区域语言变体方面的性能，并探索其下游应用。&lt;h4&gt;方法&lt;/h4&gt;使用来自30个公开可用的语音语料库的超过200万个训练语句，评估多种语音基础模型在方言分类任务上的表现，测试模型在嘈杂环境下的鲁棒性，并进行错误分析。&lt;h4&gt;主要发现&lt;/h4&gt;语音基础模型在不同方言上的分类性能存在差异，模型在嘈杂条件下的鲁棒性有限，错误分析显示建模结果与地理连续性一致。&lt;h4&gt;结论&lt;/h4&gt;Voxlect为方言和区域语言建模提供了重要基准，可用于增强现有语音识别数据集、分析ASR性能在不同方言中的表现，以及评估语音生成系统。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Voxlect，这是一个用于使用语音基础模型建模全球范围内方言和区域语言的新型基准测试。具体来说，我们报告了在英语、阿拉伯语、普通话和粤语、藏语、印度语言、泰语、西班牙语、法语、德语、巴西葡萄牙语和意大利语的方言和区域语言变体上的全面基准评估。我们的研究使用了来自30个公开可用的语音语料库的超过200万个训练语句，这些语料库都提供了方言信息。我们评估了几种广泛使用的语音基础模型在分类语音方言方面的性能。我们评估了方言模型在嘈杂条件下的鲁棒性，并进行了错误分析，突出了与地理连续性一致的结果。除了基准测试方言分类外，我们还展示了Voxlect启用的几种下游应用。具体来说，我们展示了Voxlect可以应用于用方言信息增强现有的语音识别数据集，从而能够对ASR性能在不同方言变化中的表现进行更详细的分析。Voxlect也被用作评估语音生成系统性能的工具。Voxlect以RAIL系列的许可证公开发布，地址为：https://github.com/tiantiaf0627/voxlect。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Voxlect, a novel benchmark for modeling dialects and regionallanguages worldwide using speech foundation models. Specifically, we reportcomprehensive benchmark evaluations on dialects and regional language varietiesin English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai,Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over2 million training utterances from 30 publicly available speech corpora thatare provided with dialectal information. We evaluate the performance of severalwidely used speech foundation models in classifying speech dialects. We assessthe robustness of the dialectal models under noisy conditions and present anerror analysis that highlights modeling results aligned with geographiccontinuity. In addition to benchmarking dialect classification, we demonstrateseveral downstream applications enabled by Voxlect. Specifically, we show thatVoxlect can be applied to augment existing speech recognition datasets withdialect information, enabling a more detailed analysis of ASR performanceacross dialectal variations. Voxlect is also used as a tool to evaluate theperformance of speech generation systems. Voxlect is publicly available withthe license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.</description>
      <author>example@mail.com (Tiantian Feng, Kevin Huang, Anfeng Xu, Xuan Shi, Thanathai Lertpetchpun, Jihwan Lee, Yoonjeong Lee, Dani Byrd, Shrikanth Narayanan)</author>
      <guid isPermaLink="false">2508.01691v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Innovative tokenisation of structured data for LLM training</title>
      <link>http://arxiv.org/abs/2508.01685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种混合标记化方法，将表格数据转换为适合大语言模型训练的统一序列格式，应用于网络入侵检测系统基础模型的训练。&lt;h4&gt;背景&lt;/h4&gt;机器学习中数据表示是一个基本挑战，特别是当将Transformers和大语言模型等基于序列的架构应用于结构化表格数据时。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的混合标记化方法，将表格数据转换为适合LLM训练的统一序列格式，解决现有方法无法统一编码特征混合或保留表格结构的问题。&lt;h4&gt;方法&lt;/h4&gt;结合预定义的固定标记表示结构元素和低基数分类特征，使用字节对编码(BPE)学习子词词汇表来处理高基数和连续值。&lt;h4&gt;主要发现&lt;/h4&gt;该方法高效处理了超过3100万网络流量，用时不到5小时，实现了6.18:1的显著数据压缩比，生成了超过10亿个标记的计算可管理语料库。&lt;h4&gt;结论&lt;/h4&gt;为在结构化数据上训练基础模型建立了可行且可推广的途径。&lt;h4&gt;翻译&lt;/h4&gt;数据表示在机器学习中仍然是一个基本挑战，特别是当将Transformers和大语言模型等基于序列的架构应用于结构化表格数据时。现有方法通常无法统一编码数值和分类特征的混合，或者保留表格的固有结构。本文引入了一种新的混合标记化方法，旨在将表格数据转换为适合LLM训练的统一序列格式。我们的方法结合了预定义的固定标记来表示结构元素和低基数分类特征，并使用字节对编码(BPE)学习子词词汇表来处理高基数和连续值。我们通过将此技术应用于大规模NetFlow数据集(CIDDS-001)，为网络入侵检测系统基础模型准备语料库，证明了该技术的有效性。评估显示，我们的方法效率很高，在不到五小时内处理了超过3100万网络流量，并实现了6.18:1的显著数据压缩比。这一过程生成了超过10亿个标记的计算可管理语料库，为在结构化数据上训练基础模型建立了可行且可推广的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data representation remains a fundamental challenge in machine learning,particularly when adapting sequence-based architectures like Transformers andLarge Language Models (LLMs) for structured tabular data. Existing methodsoften fail to cohesively encode the mix of numerical and categorical featuresor preserve the inherent structure of tables. This paper introduces a novel,hybrid tokenisation methodology designed to convert tabular data into aunified, sequential format suitable for LLM training. Our approach combinespredefined fixed tokens to represent structural elements and low-cardinalitycategorical features, with a learned subword vocabulary using Byte-PairEncoding (BPE) for high-cardinality and continuous values. We demonstrate theefficacy of this technique by applying it to a large-scale NetFlow dataset(CIDDS-001), preparing a corpus for a Network Intrusion Detection System (NIDS)foundation model. The evaluation shows that our method is highly efficient,processing over 31 million network flows in under five hours and achieving asignificant data compression ratio of 6.18:1. This process resulted in acomputationally manageable corpus of over one billion tokens, establishing aviable and generalisable pathway for training foundation models on structureddata.</description>
      <author>example@mail.com (Kayvan Karim, Hani Ragab Hassen. Hadj Batatia)</author>
      <guid isPermaLink="false">2508.01685v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Rein++: Efficient Generalization and Adaptation for Semantic Segmentation with Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.01667v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Rein++是一种高效的基于视觉基础模型(VFM)的语义分割框架，能够从有限数据中实现优越的泛化，并有效适应多样化的无标记场景。该框架结合了领域泛化解决方案Rein-G和领域适应解决方案Rein-A，通过参数高效的方法微调不到主干网络1%的参数，实现鲁棒泛化，并通过无监督领域适应和语义转移模块减轻领域偏移。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型(VFMs)已在各种计算机视觉任务中取得显著成功，但在语义分割应用中面临两个主要挑战：一是分割数据集通常远小于VFM预训练使用的数据集，存在数据规模差异；二是现实世界分割场景多样，但在预训练中常常代表性不足，存在领域分布偏移问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的VFM分割框架，克服数据规模差异和领域分布偏移的挑战，实现从有限数据中学习可泛化模型，并能够有效适应多样化的无标记目标场景。&lt;h4&gt;方法&lt;/h4&gt;Rein++框架包含两个主要组件：1) Rein-G引入一组可训练的、感知实例的令牌，有效优化VFM特征用于分割任务，采用参数高效方法微调不到1%的主干参数；2) Rein-A在Rein-G基础上，在实例和logit层面执行无监督领域适应，并整合语义转移模块，利用Segment Anything模型的类无关能力增强目标域边界细节。该框架首先在源域学习可泛化模型，然后无需目标域标签即可适应多样化目标域。&lt;h4&gt;主要发现&lt;/h4&gt;综合实验表明，Rein++以高效的训练显著优于最先进方法，证明了其作为VFM的高效、可泛化和适应性强分割解决方案的有效性，即使对于具有数十亿参数的大模型也同样适用。&lt;h4&gt;结论&lt;/h4&gt;Rein++成功解决了VFMs在语义分割应用中的关键挑战，提供了一个实用的框架，使VFMs能够有效处理数据规模有限和领域分布多样的实际场景，为计算机视觉领域特别是语义分割任务提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型(VFMs)已在各种计算机视觉任务中取得显著成功。然而，它们在语义分割中的应用受到两个重大挑战的阻碍：(1)数据规模差异，因为分割数据集通常远小于用于VFM预训练的数据集，以及(2)领域分布偏移，现实世界的分割场景多样，但在预训练中常常代表性不足。为了克服这些局限性，我们提出了Rein++，一种高效的基于VFM的分割框架，展示了从有限数据中优越的泛化能力，并能够有效适应多样化的无标记场景。具体来说，Rein++包含一个领域泛化解决方案Rein-G和一个领域适应解决方案Rein-A。Rein-G引入一组可训练的、感知实例的令牌，有效优化VFM特征用于分割任务。这种参数高效的方法微调了不到主干网络1%的参数，实现鲁棒泛化。基于Rein-G，Rein-A在实例和logit层面执行无监督领域适应，以减轻领域偏移。此外，它整合了语义转移模块，利用分割任何模型的类无关能力增强目标域的边界细节。集成的Rein++流程首先在源域学习可泛化模型，然后无需任何目标标签将其适应到多样化的目标域。综合实验证明，Rein++以高效的训练显著优于最先进方法，强调了其作为VFM的高效、可泛化和适应性强分割解决方案的作用，即使对于具有数十亿参数的大模型也是如此。代码可在https://github.com/wloves/Rein获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Foundation Models(VFMs) have achieved remarkable success in variouscomputer vision tasks. However, their application to semantic segmentation ishindered by two significant challenges: (1) the disparity in data scale, assegmentation datasets are typically much smaller than those used for VFMpre-training, and (2) domain distribution shifts, where real-world segmentationscenarios are diverse and often underrepresented during pre-training. Toovercome these limitations, we present Rein++, an efficient VFM-basedsegmentation framework that demonstrates superior generalization from limiteddata and enables effective adaptation to diverse unlabeled scenarios.Specifically, Rein++ comprises a domain generalization solution Rein-G and adomain adaptation solution Rein-A. Rein-G introduces a set of trainable,instance-aware tokens that effectively refine the VFM's features for thesegmentation task. This parameter-efficient approach fine-tunes less than 1% ofthe backbone's parameters, enabling robust generalization. Building on theRein-G, Rein-A performs unsupervised domain adaptation at both the instance andlogit levels to mitigate domain shifts. In addition, it incorporates a semantictransfer module that leverages the class-agnostic capabilities of the segmentanything model to enhance boundary details in the target domain. The integratedRein++ pipeline first learns a generalizable model on a source domain (e.g.,daytime scenes) and subsequently adapts it to diverse target domains (e.g.,nighttime scenes) without any target labels. Comprehensive experimentsdemonstrate that Rein++ significantly outperforms state-of-the-art methods withefficient training, underscoring its roles an efficient, generalizable, andadaptive segmentation solution for VFMs, even for large models with billions ofparameters. The code is available at https://github.com/wloves/Rein.</description>
      <author>example@mail.com (Zhixiang Wei, Xiaoxiao Ma, Ruishen Yan, Tao Tu, Huaian Chen, Jinjin Zheng, Yi Jin, Enhong Chen)</author>
      <guid isPermaLink="false">2508.01667v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>TCDiff: Triplex Cascaded Diffusion for High-fidelity Multimodal EHRs Generation with Incomplete Clinical Data</title>
      <link>http://arxiv.org/abs/2508.01615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TCDiff（三重级联扩散网络），一种新型电子健康记录生成框架，解决了大规模高质量EHR数据稀缺问题，特别是在中医领域。&lt;h4&gt;背景&lt;/h4&gt;大规模高质量电子健康记录(EHRs)的稀缺性是生物医学研究的主要瓶颈，随着大型基础模型变得越来越需要大量数据，这一问题更加突出。从现有数据集中合成去标识化和高保真数据是一个有前景的解决方案。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理异构多模态EHR数据、捕捉复杂依赖关系并稳健处理数据不完整性的生成框架，特别针对中医领域的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出TCDiff框架，级联三个扩散网络（参考模态扩散、跨模态桥接、目标模态扩散）来学习真实EHR数据特征。同时构建了TCM-SZ1多模态EHR数据集用于基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在各种缺失率下，TCDiff在数据保真度方面平均比最先进的基线高出10%，同时保持了有竞争力的隐私保证。&lt;h4&gt;结论&lt;/h4&gt;TCDiff在现实医疗场景中展现出有效性、鲁棒性和通用性，为解决EHR数据稀缺问题提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;大规模高质量电子健康记录(EHRs)的稀缺性仍然是生物医学研究的主要瓶颈，特别是随着大型基础模型变得越来越需要大量数据时。从现有数据集中合成大量去标识化和高保真数据已成为一个有前景的解决方案。然而，现有方法存在一系列局限性：它们难以建模异构多模态EHR数据的内在特性（如连续、离散和文本模态），难以捕捉它们之间的复杂依赖关系，以及难以稳健地处理普遍存在的数据不完整性。这些挑战在中医(TCM)领域尤为突出。为此，我们提出了TCDiff（三重级联扩散网络），一种新颖的EHR生成框架，通过级联三个扩散网络来学习真实世界EHR数据的特征，形成多阶段生成过程：参考模态扩散、跨模态桥接和目标模态扩散。此外，为了验证我们提出的框架，除了两个公共数据集外，我们还构建并引入了TCM-SZ1，这是一个用于基准测试的新型多模态EHR数据集。实验结果表明，在各种缺失率下，TCDiff在数据保真度方面平均比最先进的基线高出10%，同时保持了有竞争力的隐私保证。这突显了我们的方法在现实医疗场景中的有效性、鲁棒性和通用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The scarcity of large-scale and high-quality electronic health records (EHRs)remains a major bottleneck in biomedical research, especially as largefoundation models become increasingly data-hungry. Synthesizing substantialvolumes of de-identified and high-fidelity data from existing datasets hasemerged as a promising solution. However, existing methods suffer from a seriesof limitations: they struggle to model the intrinsic properties ofheterogeneous multimodal EHR data (e.g., continuous, discrete, and textualmodalities), capture the complex dependencies among them, and robustly handlepervasive data incompleteness. These challenges are particularly acute inTraditional Chinese Medicine (TCM). To this end, we propose TCDiff (TriplexCascaded Diffusion Network), a novel EHR generation framework that cascadesthree diffusion networks to learn the features of real-world EHR data,formatting a multi-stage generative process: Reference Modalities Diffusion,Cross-Modal Bridging, and Target Modality Diffusion. Furthermore, to validateour proposed framework, besides two public datasets, we also construct andintroduce TCM-SZ1, a novel multimodal EHR dataset for benchmarking.Experimental results show that TCDiff consistently outperforms state-of-the-artbaselines by an average of 10% in data fidelity under various missing rate,while maintaining competitive privacy guarantees. This highlights theeffectiveness, robustness, and generalizability of our approach in real-worldhealthcare scenarios.</description>
      <author>example@mail.com (Yandong Yan, Chenxi Li, Yu Huang, Dexuan Xu, Jiaqi Zhu, Zhongyan Chai, Huamin Zhang)</author>
      <guid isPermaLink="false">2508.01615v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Towards Generalizable AI-Generated Image Detection via Image-Adaptive Prompt Learning</title>
      <link>http://arxiv.org/abs/2508.01603v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Image-Adaptive Prompt Learning (IAPL)的新框架，通过两个自适应模块增强AI生成图像检测模型对未知生成器的泛化能力，在两个数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;AI生成图像检测的主要挑战是识别来自未知生成器的假图像。现有方法通常通过部分参数微调预训练基础模型，但这些在有限范围生成器上训练的参数难以泛化到未知来源。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够灵活处理多样化测试图像的新框架，解决现有方法在未知生成器上泛化能力差的问题。&lt;h4&gt;方法&lt;/h4&gt;IAPL框架包含两个自适应模块：1) 条件信息学习器，使用基于CNN的特征提取器学习伪造特定和图像特定条件，通过门控机制传播到可学习标记；2) 置信度驱动的自适应预测，基于单个测试样本优化最浅层可学习标记，选择最高预测置信度的裁剪视图进行最终检测。这些模块使提示能根据输入图像自动调整。&lt;h4&gt;主要发现&lt;/h4&gt;在UniversalFakeDetect和GenImage两个广泛使用的数据集上，IAPL分别实现了95.61%和96.7%的平均准确率，达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;IAPL通过自适应提示学习显著增强了模型对各种伪造图像的适应能力，解决了现有AI生成图像检测方法在未知生成器上泛化能力不足的问题。&lt;h4&gt;翻译&lt;/h4&gt;AI生成图像检测的一个主要挑战是识别来自未知生成器的假图像。现有的最先进方法通常通过部分参数微调来定制预训练的基础模型。然而，这些在有限范围生成器上训练的参数可能无法泛化到未知来源。鉴于此，我们提出了一种名为Image-Adaptive Prompt Learning (IAPL)的新颖框架，增强了处理多样化测试图像的灵活性。它包含两个自适应模块，即条件信息学习器和置信度驱动的自适应预测。前者使用基于CNN的特征提取器学习伪造特定和图像特定的条件，这些条件通过门控机制传播到可学习标记。后者基于单个测试样本优化最浅层的可学习标记，并选择具有最高预测置信度的裁剪视图进行最终检测。这两个模块使输入基础模型的提示能够根据输入图像自动调整，而不是在训练后固定，从而增强模型对各种伪造图像的适应能力。大量实验表明，IAPL实现了最先进的性能，在两个广泛使用的UniversalFakeDetect和GenImage数据集上分别达到95.61%和96.7%的平均准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A major struggle for AI-generated image detection is identifying fake imagesfrom unseen generators. Existing cutting-edge methods typically customizepre-trained foundation models to this task via partial-parameter fine-tuning.However, these parameters trained on a narrow range of generators may fail togeneralize to unknown sources. In light of this, we propose a novel frameworknamed Image-Adaptive Prompt Learning (IAPL), which enhances flexibility inprocessing diverse testing images. It consists of two adaptive modules, i.e.,the Conditional Information Learner and the Confidence-Driven AdaptivePrediction. The former employs CNN-based feature extractors to learnforgery-specific and image-specific conditions, which are then propagated tolearnable tokens via a gated mechanism. The latter optimizes the shallowestlearnable tokens based on a single test sample and selects the cropped viewwith the highest prediction confidence for final detection. These two modulesenable the prompts fed into the foundation model to be automatically adjustedbased on the input image, rather than being fixed after training, therebyenhancing the model's adaptability to various forged images. Extensiveexperiments show that IAPL achieves state-of-the-art performance, with 95.61%and 96.7% mean accuracy on two widely used UniversalFakeDetect and GenImagedatasets, respectively.</description>
      <author>example@mail.com (Yiheng Li, Zichang Tan, Zhen Lei, Xu Zhou, Yang Yang)</author>
      <guid isPermaLink="false">2508.01603v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Set Pivot Learning: Redefining Generalized Segmentation with Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.01582v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文首次引入集合枢轴学习(SPL)概念，这是一种基于视觉基础模型(VFMs)重新定义领域泛化(DG)的范式转变。作者提出了一种新的领域迁移任务定义，并开发了动态提示微调方法，结合动态类感知提示器和提示引导的特征聚焦器，以提高VFMs在特定场景中的性能。&lt;h4&gt;背景&lt;/h4&gt;传统领域泛化(DG)假设在训练过程中目标领域不可访问，但随着视觉基础模型(VFMs)的出现，这些模型在大量多样化数据上训练，使得这一假设变得不明确和过时。&lt;h4&gt;目的&lt;/h4&gt;为了应对这一挑战，作者提出集合枢轴学习(SPL)，这是一种基于VFMs的新领域迁移任务定义，更适合当前研究和应用需求。&lt;h4&gt;方法&lt;/h4&gt;SPL具有两个关键属性：1)动态适应，从静态领域对齐转向灵活的、任务驱动的特征优化，使模型能够随着下游场景而演变；2)VFM为中心的调优，利用预训练知识作为枢轴，同时保留跨领域鲁棒性。基于SPL，提出了一种动态提示微调方法，结合了动态类感知提示器和提示引导的特征聚焦器。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的大量实验表明，该方法有效，并且优于最先进的方法，特别是在通用分割方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;集合枢轴学习和动态提示微调方法能够有效提升视觉基础模型在特定场景中的性能，特别是在通用分割任务上具有优越性。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们首次引入了集合枢轴学习的概念，这是一种基于视觉基础模型重新定义领域泛化的范式转变。传统领域泛化假设在训练过程中目标领域不可访问，但视觉基础模型的出现，这些模型在大量多样化数据上训练，使得这一假设变得不明确和过时。为了应对这一挑战，我们提出了基于VFMs的集合枢轴学习(SPL)，这是一种更适合当前研究和应用需求的领域迁移任务新定义。与传统DG方法不同，SPL优先考虑适应性调整而非严格的领域迁移，确保与不断变化的真实世界条件保持持续对齐。具体来说，SPL具有两个关键属性：(i)动态适应，从静态领域对齐转向灵活的、任务驱动的特征优化，使模型能够随着下游场景而演变；(ii)VFM为中心的调优，利用预训练知识作为枢轴，同时保留跨领域鲁棒性。基于SPL，我们提出了一种动态提示微调方法，结合了动态类感知提示器和提示引导的特征聚焦器，以提高VFMs在目标场景中的性能。在基准数据集上的大量实验表明了我们方法的有效性，突显了其优于最先进方法的优越性，特别是在通用分割方面。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce, for the first time, the concept of Set PivotLearning, a paradigm shift that redefines domain generalization (DG) based onVision Foundation Models (VFMs). Traditional DG assumes that the target domainis inaccessible during training, but the emergence of VFMs, trained on vast anddiverse data, renders this assumption unclear and obsolete. Traditional DGassumes that the target domain is inaccessible during training, but theemergence of VFMs, which are trained on vast and diverse datasets, renders thisassumption unclear and obsolete. To address this challenge, we propose SetPivot Learning (SPL), a new definition of domain migration task based on VFMs,which is more suitable for current research and application requirements.Unlike conventional DG methods, SPL prioritizes adaptive refinement over rigiddomain transfer, ensuring continuous alignment with evolving real-worldconditions. Specifically, SPL features two key attributes: (i) Dynamicadaptation, transitioning from static domain alignment to flexible, task-drivenfeature optimization, enabling models to evolve with downstream scenarios; (ii)VFM-centric tuning, leveraging pretrained knowledge as a pivot to honetask-specific representations while preserving cross-domain robustness.Building on SPL, we propose a Dynamic Prompt Fine-Tuning method, which combinesa Dynamic Class-aware Prompter with a Prompt-guided Feature Focuser, to elevateVFM performance in targeted scenarios. Extensive experiments on benchmarkdatasets show the effectiveness of our method, highlighting its superiorityover state-of-the-art methods, particularly in generalized segmentation.</description>
      <author>example@mail.com (Xinhui Li, Xinyu He, Qiming Hu, Xiaojie Guo)</author>
      <guid isPermaLink="false">2508.01582v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>EfficientGFormer: Multimodal Brain Tumor Segmentation via Pruned Graph-Augmented Transformer</title>
      <link>http://arxiv.org/abs/2508.01465v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了EfficientGFormer，一种结合预训练基础模型、基于图的推理和轻量级效率机制的新型架构，用于鲁棒的3D脑肿瘤分割。&lt;h4&gt;背景&lt;/h4&gt;准确高效的脑肿瘤分割在神经影像学中仍是一个关键挑战，主要由于肿瘤亚区域的异质性和体积推断的高计算成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实现准确且高效脑肿瘤分割的方法，同时保持临床实用性。&lt;h4&gt;方法&lt;/h4&gt;EfficientGFormer使用nnFormer作为模态感知编码器将多模态MRI转换为补丁级嵌入，构建捕捉空间邻接和语义相似性的双边图，采用修剪的边类型感知图注意力网络进行关系推理，并通过蒸馏模块实现知识转移以支持实时部署。&lt;h4&gt;主要发现&lt;/h4&gt;在MSD Task01和BraTS 2021数据集上，EfficientGFormer实现了最先进的准确性，同时显著减少了内存使用和推理时间，性能优于最近的基于transformer和基于图的方法。&lt;h4&gt;结论&lt;/h4&gt;EfficientGFormer为快速准确的体积肿瘤分割提供了临床可行的解决方案，结合了可扩展性、可解释性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;准确高效的脑肿瘤分割由于肿瘤亚区域的异质性和体积推断的高计算成本，在神经影像学中仍然是一个关键挑战。在本文中，我们提出了EfficientGFormer，一种结合预训练基础模型、基于图的推理和轻量级效率机制的新型架构，用于鲁棒的3D脑肿瘤分割。我们的框架利用nnFormer作为模态感知编码器，将多模态MRI体积转换为补丁级嵌入。这些特征被结构化为双边图，同时捕捉空间邻接和语义相似性。修剪的、边类型感知的图注意力网络(GAT)实现了肿瘤亚区域间的高效关系推理，而蒸馏模块将全容量教师模型的知识转移到紧凑的学生模型以实现实时部署。在MSD Task01和BraTS 2021数据集上的实验表明，EfficientGFormer以显著减少的内存和推理时间实现了最先进的准确性，优于最近的基于transformer和基于图的方法。这项工作为快速准确的体积肿瘤分割提供了临床可行的解决方案，结合了可扩展性、可解释性和泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决脑肿瘤分割中准确性和效率的平衡问题。脑肿瘤具有异质性，包含不同亚区域（如增强肿瘤、肿瘤核心和周围水肿），这些区域有不同的强度特征、形态和变异性，使得分割变得困难。同时，现有的3D分割方法计算成本高，难以在实际临床环境中部署。这个问题很重要，因为准确的脑肿瘤分割直接影响诊断、预后和治疗计划，而高效性则决定了该方法能否在临床实践中得到应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到transformer模型在3D分割任务中表现出色但存在计算瓶颈，同时看到图神经网络在建模医学图像关系结构方面有潜力。因此，作者思考如何将两者的优势结合起来。他们借鉴了nnFormer作为模态感知编码器，利用图神经网络进行关系建模，并引入知识蒸馏技术来提高效率。这种方法还受到了视频理解领域结合图建模和基础模型的启发，将其整合到医学成像的结构化和多模态推理中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将预训练的基础模型编码器与图推理和轻量级效率机制相结合，使用双重边图结构捕获空间邻接和语义相似性，通过剪枝的边类型感知图注意力网络实现高效的关系推理，并利用知识蒸馏将全容量教师模型的知识转移到紧凑的学生模型以实现实时部署。整体流程包括：1)使用预训练nnFormer处理多模态MRI数据提取特征；2)构建双重边图，连接空间邻近和语义相似的图像补丁；3)应用剪枝的边类型感知GAT进行关系推理；4)通过知识蒸馏压缩模型；5)使用分割头生成体素级肿瘤预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出EfficientGFormer架构，结合基础模型编码器、图推理和效率模块；2)引入双重边图构建策略，捕获空间连续性和语义相似性；3)设计剪枝的边类型感知GAT，减少计算开销；4)实现知识蒸馏模块，压缩模型同时保持精度；5)在多个数据集上展示最先进性能。相比之前的工作，这种方法不仅结合了transformer和GNN的优势，还通过双重边图和边类型感知注意力更有效地建模空间和语义关系，并通过知识蒸馏实现了模型压缩，使其更适合临床应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; EfficientGFormer通过结合基础模型编码器、双重边图推理和知识蒸馏，实现了准确且高效的脑肿瘤分割，为临床应用提供了兼具高精度和实时性能的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and efficient brain tumor segmentation remains a critical challengein neuroimaging due to the heterogeneous nature of tumor subregions and thehigh computational cost of volumetric inference. In this paper, we proposeEfficientGFormer, a novel architecture that integrates pretrained foundationmodels with graph-based reasoning and lightweight efficiency mechanisms forrobust 3D brain tumor segmentation. Our framework leverages nnFormer as amodality-aware encoder, transforming multi-modal MRI volumes into patch-levelembeddings. These features are structured into a dual-edge graph that capturesboth spatial adjacency and semantic similarity. A pruned, edge-type-aware GraphAttention Network (GAT) enables efficient relational reasoning across tumorsubregions, while a distillation module transfers knowledge from afull-capacity teacher to a compact student model for real-time deployment.Experiments on the MSD Task01 and BraTS 2021 datasets demonstrate thatEfficientGFormer achieves state-of-the-art accuracy with significantly reducedmemory and inference time, outperforming recent transformer-based andgraph-based baselines. This work offers a clinically viable solution for fastand accurate volumetric tumor delineation, combining scalability,interpretability, and generalization.</description>
      <author>example@mail.com (Fatemeh Ziaeetabar)</author>
      <guid isPermaLink="false">2508.01465v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>UniExtreme: A Universal Foundation Model for Extreme Weather Forecasting</title>
      <link>http://arxiv.org/abs/2508.01426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 80 figures, submitted to ACM KDD 2026 conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了UniExtreme，一个通用的极端天气预测基础模型，通过整合自适应频率调制和事件先验增强两个模块，有效解决了现有方法在预测多样化极端天气事件方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;深度学习的进步已促成天气预测基础模型的发展，但这些模型预测极端天气事件的能力仍然有限。现有方法要么专注于一般天气条件，要么专门针对特定类型的极端天气，忽略了多样化极端事件的现实大气模式。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效预测多样化极端天气事件的基础模型，解决现有方法在处理极端天气事件频谱差异和层次驱动因素方面的不足。&lt;h4&gt;方法&lt;/h4&gt;UniExtreme模型包含两个关键模块：(1)自适应频率调制(AFM)模块，通过可学习的Beta分布滤波器和多粒度频谱聚合，捕捉区域性的正常与极端天气之间的频谱差异；(2)事件先验增强(EPA)模块，通过双级记忆融合网络，整合区域特定的极端事件先验，解决极端多样性和复合极端模式问题。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验证明，UniExtreme在极端天气和一般天气预测方面都优于最先进的基线方法，展现出在多样化极端场景下的卓越适应性。&lt;h4&gt;结论&lt;/h4&gt;UniExtreme模型通过整合自适应频率调制和事件先验增强两个关键模块，有效解决了极端天气预测中的频谱差异和层次驱动因素问题，为极端天气预测提供了一个通用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;深度学习的最新进展已经促成了天气预测基础模型(FMs)的发展，但它们预测极端天气事件的能力仍然有限。现有方法要么专注于一般天气条件，要么专门针对特定类型的极端天气，忽略了多样化极端事件的现实大气模式。在这项工作中，我们确定了极端事件的两个关键特征：(1)与正常天气系统的频谱差异，以及(2)多样化极端事件的层次驱动因素和地理融合。基于此，我们提出了UniExtreme，一个通用的极端天气预测基础模型，它整合了(1)一个自适应频率调制(AFM)模块，通过可学习的Beta分布滤波器和多粒度频谱聚合，捕捉区域性的正常与极端天气之间的频谱差异，以及(2)一个事件先验增强(EPA)模块，通过双级记忆融合网络，整合区域特定的极端事件先验，以解决极端多样性和复合极端模式问题。大量实验证明，UniExtreme在极端天气和一般天气预测方面都优于最先进的基线方法，展现出在多样化极端场景下的卓越适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in deep learning have led to the development ofFoundation Models (FMs) for weather forecasting, yet their ability to predictextreme weather events remains limited. Existing approaches either focus ongeneral weather conditions or specialize in specific-type extremes, neglectingthe real-world atmospheric patterns of diversified extreme events. In thiswork, we identify two key characteristics of extreme events: (1) the spectraldisparity against normal weather regimes, and (2) the hierarchical drivers andgeographic blending of diverse extremes. Along this line, we proposeUniExtreme, a universal extreme weather forecasting foundation model thatintegrates (1) an Adaptive Frequency Modulation (AFM) module that capturesregion-wise spectral differences between normal and extreme weather, throughlearnable Beta-distribution filters and multi-granularity spectral aggregation,and (2) an Event Prior Augmentation (EPA) module which incorporatesregion-specific extreme event priors to resolve hierarchical extreme diversityand composite extreme schema, via a dual-level memory fusion network. Extensiveexperiments demonstrate that UniExtreme outperforms state-of-the-art baselinesin both extreme and general weather forecasting, showcasing superioradaptability across diverse extreme scenarios.</description>
      <author>example@mail.com (Hang Ni, Weijia Zhang, Hao Liu)</author>
      <guid isPermaLink="false">2508.01426v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>VLH: Vision-Language-Haptics Foundation Model</title>
      <link>http://arxiv.org/abs/2508.01361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了VLH，一种新型的视觉-语言-触觉基础模型，统一了空中机器人和虚拟现实中的感知、语言和触觉反馈。&lt;h4&gt;背景&lt;/h4&gt;先前的研究将触觉作为次要的、反应性的通道，而VLH将空中力和振动提示作为视觉理解上下文和自然语言命令的直接结果。&lt;h4&gt;目的&lt;/h4&gt;创建一个能够统一视觉、语言和触觉反馈的基础模型，以增强人机交互的表达性和沉浸感。&lt;h4&gt;方法&lt;/h4&gt;平台由8英寸四轴飞行器组成，配备双反向五连杆阵列用于局部触觉驱动，第一人称VR摄像头和自上下的外部视角；使用微调的OpenVLA主干网络，通过在450个多模态场景的自定义数据集上使用LoRA进行适应，输出7维动作向量；采用INT8量化和高性能服务器确保4-5Hz的实时操作。&lt;h4&gt;主要发现&lt;/h4&gt;人机交互实验（90次飞行）中，目标获取成功率为56.7%（平均到达时间21.3秒，姿态误差0.24米），纹理辨别准确率为100%；泛化测试在新任务上的表现：视觉70.0%，运动54.4%，物理40.0%，语义35.0%。&lt;h4&gt;结论&lt;/h4&gt;VLH能够将触觉反馈与感知推理和意图共同发展，推进了表达性强、沉浸式的人机交互。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了VLH，一种新型的视觉-语言-触觉基础模型，统一了空中机器人和虚拟现实中的感知、语言和触觉反馈。与先前将触觉作为次要、反应性通道的研究不同，VLH将空中力和振动提示作为视觉理解上下文和自然语言命令的直接结果。我们的平台包括一个配备双反向五连杆阵列用于局部触觉驱动的8英寸四轴飞行器，一个第一人称VR摄像头，以及一个自上而下的外部视角。视觉输入和语言指令由微调的OpenVLA主干网络处理——通过在450个多模态场景的自定义数据集上使用LoRA进行适应——输出一个7维动作向量（Vx, Vy, Vz, Hx, Hy, Hz, Hv）。INT8量化和高性能服务器确保4-5Hz的实时操作。在人机交互实验（90次飞行）中，VLH在目标获取方面实现了56.7%的成功率（平均到达时间21.3秒，姿态误差0.24米），纹理辨别准确率为100%。泛化测试在新任务上取得了70.0%（视觉）、54.4%（运动）、40.0%（物理）和35.0%（语义）的性能。这些结果证明了VLH能够将触觉反馈与感知推理和意图共同发展，推进了表达性强、沉浸式的人机交互。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在虚拟现实和空中机器人领域中，如何将触觉反馈作为一种生成性、表达性的模态，与视觉感知和自然语言处理无缝集成的问题。这个问题很重要，因为当前触觉技术主要局限于静态、预编程的反馈，无法捕捉触觉动态、上下文敏感的特性，这限制了沉浸式人机交互的发展。触觉作为人类交流的基本方面，能够传达情感、意图和物理互动，具有无与伦比的细微差别，在动态环境如空中机器人中，缺乏上下文感知的触觉反馈系统限制了人机交互的自然性和有效性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者的设计思路是将触觉视为感知和意图的直接结果，而非次要的反应性通道。他们设计了一个配备双倒五杆联动阵列的空中触觉平台，结合机械驱动与感知推理，使用视觉数据估计物体几何和表面特性。作者借鉴了多项现有工作：基于斯坦福AI实验室的OpenVLA模型进行微调；使用低秩适应(LoRA)方案更新权重；参考了RaceVLA和CognitiveDrone等视觉-语言-动作框架的思想，但增加了触觉模态；受到HapticGen等文本到振动模型的启发；参考了VRHapticDrones等利用四旋翼作为触觉反馈代理的系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将触觉反馈视为一种生成性、表达性的模态，与视觉感知和自然语言处理无缝集成，实现上下文感知的实时触觉渲染和交互。整体实现流程包括：1)数据采集：收集真实世界与VR环境的交互数据，创建450个多模态场景的自定义数据集；2)模型训练：基于OpenVLA模型进行微调，使用LoRA方案更新权重；3)系统架构：包括双视觉系统、适应的VLA模型、意图推断和触觉映射模块、空中触觉接口；4)实时操作：使用INT8量化和高性能服务器实现4-5Hz的更新率；5)输出与反馈：模型输出三维速度控制和三维方向力及振动强度，无人机执行飞行控制并产生触觉反馈。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将触觉作为生成性模态，而非仅是反应性通道；2)开发配备双倒五杆联动阵列的空中触觉平台；3)结合第一人称视角和俯视外部视角的双视觉系统；4)适应的VLA模型，输出7维动作向量；5)实时性能优化，实现4-5Hz的操作频率。相比之前工作的不同：传统触觉系统提供静态反馈，VLH提供动态上下文感知反馈；现有VLA框架如RaceVLA专注于导航但忽略触觉，VLH集成了触觉实现多模态交互；VRHapticDrones提供被动触觉表面，VLH主动合成与飞行命令协调的触觉模式；HapticGen等从文本生成触觉，VLH结合视觉、语言和触觉实现全面交互。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VLH开创了一种将触觉反馈作为生成性、表达性模态与视觉感知和自然语言处理无缝集成的新方法，通过创新的空中触觉平台和适应的视觉-语言-动作模型，实现了上下文感知的实时触觉渲染和交互，为沉浸式人机交互树立了新标杆。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present VLH, a novel Visual-Language-Haptic Foundation Model that unifiesperception, language, and tactile feedback in aerial robotics and virtualreality. Unlike prior work that treats haptics as a secondary, reactivechannel, VLH synthesizes mid-air force and vibration cues as a directconsequence of contextual visual understanding and natural language commands.Our platform comprises an 8-inch quadcopter equipped with dual inverse five-barlinkage arrays for localized haptic actuation, an egocentric VR camera, and anexocentric top-down view. Visual inputs and language instructions are processedby a fine-tuned OpenVLA backbone - adapted via LoRA on a bespoke dataset of 450multimodal scenarios - to output a 7-dimensional action vector (Vx, Vy, Vz, Hx,Hy, Hz, Hv). INT8 quantization and a high-performance server ensure real-timeoperation at 4-5 Hz. In human-robot interaction experiments (90 flights), VLHachieved a 56.7% success rate for target acquisition (mean reach time 21.3 s,pose error 0.24 m) and 100% accuracy in texture discrimination. Generalizationtests yielded 70.0% (visual), 54.4% (motion), 40.0% (physical), and 35.0%(semantic) performance on novel tasks. These results demonstrate VLH's abilityto co-evolve haptic feedback with perceptual reasoning and intent, advancingexpressive, immersive human-robot interactions.</description>
      <author>example@mail.com (Luis Francisco Moreno Fuentes, Muhammad Haris Khan, Miguel Altamirano Cabrera, Valerii Serpiva, Dmitri Iarchuk, Yara Mahmoud, Issatay Tokmurziyev, Dzmitry Tsetserukou)</author>
      <guid isPermaLink="false">2508.01361v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Predicting EGFR Mutation in LUAD from Histopathological Whole-Slide Images Using Pretrained Foundation Model and Transfer Learning: An Indian Cohort Study</title>
      <link>http://arxiv.org/abs/2508.01352v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于深度学习的框架，用于从常规病理切片中预测EGFR突变状态，结合视觉变换器基础模型和基于注意力的多实例学习架构，在印度队列和TCGA数据集上均表现出色。&lt;h4&gt;背景&lt;/h4&gt;肺腺癌是非小细胞肺癌的一种亚型，约46%的肺腺癌患者存在EGFR基因突变，这类患者可使用特异性酪氨酸激酶抑制剂治疗。东南亚人群的EGFR突变率显著高于高加索人（39-64%对比7-22%）。H&amp;E染色的全切片成像是癌症分期和亚型的常规筛查程序，近期AI模型在癌症检测和分类方面展现出良好前景。&lt;h4&gt;目的&lt;/h4&gt;开发一种深度学习框架，从H&amp;E全切片成像中预测EGFR突变状态，以辅助临床决策制定。&lt;h4&gt;方法&lt;/h4&gt;构建基于视觉变换器病理基础模型和基于注意力的多实例学习架构的深度学习框架。使用印度队列（170个全切片成像）进行训练，并在两个独立数据集上评估：内部测试集（印度队列中的30个全切片成像）和TCGA的外部测试集（86个全切片成像）。&lt;h4&gt;主要发现&lt;/h4&gt;模型在两个数据集上均表现出一致的性能，内部测试集的AUC为0.933（±0.010），外部测试集的AUC为0.965（±0.015）。该框架可以在小数据集上有效训练，与先前研究相比，无论训练领域如何，都取得了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了使用常规病理切片准确预测EGFR突变状态的可行性，特别是在资源有限的环境中，使用基础模型和基于注意力的多实例学习可有效实现这一目标。&lt;h4&gt;翻译&lt;/h4&gt;肺腺癌是非小细胞肺癌的一种亚型。EGFR基因突变的肺腺癌约占肺腺癌病例的46%。携带EGFR突变的患者可以用特异性酪氨酸激酶抑制剂治疗。因此，预测EGFR突变状态有助于临床决策制定。H&amp;E染色的全切片成像是常规执行的癌症分期和亚型筛查程序，特别是在东南亚人群中，与高加索人相比，该人群的突变发生率显著更高（39-64%对比7-22%）。近期AI模型的进展在癌症检测和分类方面显示出有前景的结果。在本研究中，我们提出了一种深度学习框架，基于视觉变换器的病理基础模型和基于注意力的多实例学习架构，用于从H&amp;E全切片成像中预测EGFR突变状态。开发的管道使用印度队列（170个全切片成像）的数据进行训练，并在两个独立数据集上评估：内部测试集（来自印度队列的30个全切片成像）和来自TCGA的外部测试集（86个全切片成像）。模型在两个数据集上都表现出一致的性能，内部测试集和外部测试集的AUC分别为0.933（±0.010）和0.965（±0.015）。该框架可以在小数据集上有效训练，与先前研究相比，无论训练领域如何，都取得了优越的性能。当前研究证明了使用常规病理切片准确预测EGFR突变状态的可行性，特别是在资源有限的环境中，使用基础模型和基于注意力的多实例学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lung adenocarcinoma (LUAD) is a subtype of non-small cell lung cancer(NSCLC). LUAD with mutation in the EGFR gene accounts for approximately 46% ofLUAD cases. Patients carrying EGFR mutations can be treated with specifictyrosine kinase inhibitors (TKIs). Hence, predicting EGFR mutation status canhelp in clinical decision making. H&amp;E-stained whole slide imaging (WSI) is aroutinely performed screening procedure for cancer staging and subtyping,especially affecting the Southeast Asian populations with significantly higherincidence of the mutation when compared to Caucasians (39-64% vs 7-22%). Recentprogress in AI models has shown promising results in cancer detection andclassification. In this study, we propose a deep learning (DL) framework builton vision transformers (ViT) based pathology foundation model andattention-based multiple instance learning (ABMIL) architecture to predict EGFRmutation status from H&amp;E WSI. The developed pipeline was trained using datafrom an Indian cohort (170 WSI) and evaluated across two independent datasets:Internal test (30 WSI from Indian cohort) set, and an external test set fromTCGA (86 WSI). The model shows consistent performance across both datasets,with AUCs of 0.933 (+/-0.010), and 0.965 (+/-0.015) for the internal andexternal test sets respectively. This proposed framework can be efficientlytrained on small datasets, achieving superior performance as compared toseveral prior studies irrespective of training domain. The current studydemonstrates the feasibility of accurately predicting EGFR mutation statususing routine pathology slides, particularly in resource-limited settings usingfoundation models and attention-based multiple instance learning.</description>
      <author>example@mail.com (Sagar Singh Gwal, Rajan, Suyash Devgan, Shraddhanjali Satapathy, Abhishek Goyal, Nuruddin Mohammad Iqbal, Vivaan Jain, Prabhat Singh Mallik, Deepali Jain, Ishaan Gupta)</author>
      <guid isPermaLink="false">2508.01352v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Advancing the Foundation Model for Music Understanding</title>
      <link>http://arxiv.org/abs/2508.01178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为MuFun的统一基础模型，用于整体音乐理解，挑战了音乐信息检索领域碎片化的现状。&lt;h4&gt;背景&lt;/h4&gt;音乐信息检索(MIR)领域是碎片化的，专业模型在孤立任务上表现优异。&lt;h4&gt;目的&lt;/h4&gt;挑战现有范式，引入一个统一的音乐理解基础模型。&lt;h4&gt;方法&lt;/h4&gt;MuFun模型具有新颖的架构，能够同时处理乐器内容和歌词内容，并在覆盖多种任务(如流派分类、音乐标记和问答)的大规模数据集上进行训练。同时提出了新的多方面音乐理解基准测试MuCUE。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，MuFun模型在MuCUE任务上显著优于现有的音频大语言模型。&lt;h4&gt;结论&lt;/h4&gt;MuFun模型展示了最先进的有效性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;音乐信息检索(MIR)领域是碎片化的，专业模型在孤立任务上表现优异。在这项工作中，我们通过引入一个名为MuFun的统一基础模型来挑战这一范式，用于整体音乐理解。我们的模型具有新颖的架构，能够同时处理乐器内容和歌词内容，并在覆盖多种任务(如流派分类、音乐标记和问答)的大规模数据集上进行训练。为了促进稳健评估，我们还提出了一个名为MuCUE(音乐综合理解评估)的多方面音乐理解新基准。实验表明，我们的模型在MuCUE任务上显著优于现有的音频大语言模型，展示了其最先进的有效性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The field of Music Information Retrieval (MIR) is fragmented, withspecialized models excelling at isolated tasks. In this work, we challenge thisparadigm by introducing a unified foundation model named MuFun for holisticmusic understanding. Our model features a novel architecture that jointlyprocesses instrumental and lyrical content, and is trained on a large-scaledataset covering diverse tasks such as genre classification, music tagging, andquestion answering. To facilitate robust evaluation, we also propose a newbenchmark for multi-faceted music understanding called MuCUE (MusicComprehensive Understanding Evaluation). Experiments show our modelsignificantly outperforms existing audio large language models across the MuCUEtasks, demonstrating its state-of-the-art effectiveness and generalizationability.</description>
      <author>example@mail.com (Yi Jiang, Wei Wang, Xianwen Guo, Huiyun Liu, Hanrui Wang, Youri Xu, Haoqi Gu, Zhongqian Xie, Chuanjiang Luo)</author>
      <guid isPermaLink="false">2508.01178v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Multi-class Image Anomaly Detection for Practical Applications: Requirements and Robust Solutions</title>
      <link>http://arxiv.org/abs/2508.02477v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了多类图像异常检测中类信息使用和检测阈值定义的问题，提出了Hierarchical Coreset (HierCore)框架，该框架能够在不同条件下有效工作，即使在没有类标签的情况下也能保持稳定性能。&lt;h4&gt;背景&lt;/h4&gt;图像异常检测已从单类扩展到多类框架以提高效率，但单模型处理多类时每类检测准确性通常低于特定类别模型。以往研究主要关注缩小性能差距，而类信息如何影响检测阈值定义相对未被研究。&lt;h4&gt;目的&lt;/h4&gt;确定并形式化多类图像异常检测模型在不同条件（训练和评估是否有类标签）下必须满足的要求，并据此提出新框架。&lt;h4&gt;方法&lt;/h4&gt;提出Hierarchical Coreset (HierCore)框架，利用分层存储库估计类决策标准，即使在无类标签情况下也能有效工作。在四种由训练和评估阶段类标签存在与否决定的不同场景下验证方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明HierCore在各种设置下均满足所有要求，保持强大且稳定的性能，显示出在实际多类异常检测任务中的潜力。&lt;h4&gt;结论&lt;/h4&gt;HierCore框架能有效处理多类图像异常检测问题，无论训练和评估阶段是否有类标签，都能保持良好性能，为实际应用提供了有价值的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图像异常检测的最新进展已经将基于无监督学习的模型从单类设置扩展到多类框架，旨在提高训练时间和模型存储效率。当单个模型被训练处理多个类别时，其在每类检测准确性方面通常不如特定类别的模型。因此，以往的研究主要关注缩小这种性能差距。然而，类信息如何使用或未使用仍然是一个相对未被研究的因素，这可能影响多类图像异常检测中检测阈值的定义。这些阈值，无论是特定类别的还是类别无关的，都会显著影响检测结果。在本研究中，我们确定了并形式化了多类图像异常检测模型在不同条件下必须满足的要求，这些条件取决于训练和评估期间是否有类标签可用。然后，我们根据这些标准重新审视了现有方法。为了应对这些挑战，我们提出了Hierarchical Coreset (HierCore)，一个设计用于满足所有定义要求的新颖框架。HierCore即使在没有类标签的情况下也能有效工作，利用分层存储库来估计用于异常检测的类决策标准。我们在四种不同场景下实证验证了现有方法和HierCore的适用性和鲁棒性，这些场景由训练和评估阶段是否存在类标签决定。实验结果表明，HierCore始终满足所有要求，并在所有设置中保持强大且稳定的性能，突显了其在现实世界多类异常检测任务中的实际潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in image anomaly detection have extended unsupervisedlearning-based models from single-class settings to multi-class frameworks,aiming to improve efficiency in training time and model storage. When a singlemodel is trained to handle multiple classes, it often underperforms compared toclass-specific models in terms of per-class detection accuracy. Accordingly,previous studies have primarily focused on narrowing this performance gap.However, the way class information is used, or not used, remains a relativelyunderstudied factor that could influence how detection thresholds are definedin multi-class image anomaly detection. These thresholds, whetherclass-specific or class-agnostic, significantly affect detection outcomes. Inthis study, we identify and formalize the requirements that a multi-class imageanomaly detection model must satisfy under different conditions, depending onwhether class labels are available during training and evaluation. We thenre-examine existing methods under these criteria. To meet these challenges, wepropose Hierarchical Coreset (HierCore), a novel framework designed to satisfyall defined requirements. HierCore operates effectively even without classlabels, leveraging a hierarchical memory bank to estimate class-wise decisioncriteria for anomaly detection. We empirically validate the applicability androbustness of existing methods and HierCore under four distinct scenarios,determined by the presence or absence of class labels in the training andevaluation phases. The experimental results demonstrate that HierCoreconsistently meets all requirements and maintains strong, stable performanceacross all settings, highlighting its practical potential for real-worldmulti-class anomaly detection tasks.</description>
      <author>example@mail.com (Jaehyuk Heo, Pilsung Kang)</author>
      <guid isPermaLink="false">2508.02477v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Efforts in Modeling the Mechanics and Chemistry of Energetic Materials Across Scales</title>
      <link>http://arxiv.org/abs/2508.02141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了含能分子晶体多尺度力学和化学本构定律的最新发展，包括分子动力学模拟工具的开发、连续介质模型的构建、化学分解动力学的识别与校准，以及在中尺度水平上模拟冲击到爆轰转变的研究。&lt;h4&gt;背景&lt;/h4&gt;含能分子晶体的多尺度建模需要将原子尺度的信息传递到连续介质尺度，以准确描述材料的力学和化学行为。&lt;h4&gt;目的&lt;/h4&gt;开发综合的多尺度建模方法，用于模拟含能分子晶体的力学行为和化学分解，特别是冲击到爆轰的转变过程。&lt;h4&gt;方法&lt;/h4&gt;在分子动力学代码中集成特定工具，增强原子模拟的变形跟踪能力，构建非线性超弹性连续介质模型，使用无监督学习算法分析反应分子动力学模拟，并在有限元代码中实现这些模型。&lt;h4&gt;主要发现&lt;/h4&gt;成功构建了TATB的晶体塑性模型，识别和校准了RDX和TATB单晶的化学分解动力学，并将方法扩展到β-HMX和多组分状态方程，能够模拟中尺度水平的冲击到爆轰转变。&lt;h4&gt;结论&lt;/h4&gt;多尺度建模方法为含能分子晶体的研究提供了有力工具，但也需要新的前瞻性实验来验证和进一步发展这些模型。&lt;h4&gt;翻译&lt;/h4&gt;本文致力于构建含能分子晶体的多尺度力学和化学本构定律的最新发展进行了阐述和讨论。特别是，各种工具已被专门整合到分子动力学代码中，以促进随后向连续介质（即有限元模拟代码）的信息传递。原子模拟已增强跟踪特定变形路径以及局部拉格朗日力学度量的能力，使材料流动应力面的计算成为可能。这种机制库允许构建一个包括TATB晶体塑性和孪生在内的全面非线性超弹性连续介质模型。此外，使用无监督学习算法分析反应分子动力学模拟的最新进展，使RDX和TATB单晶的化学分解动力学的识别和校准成为可能。在本工作中，该程序被应用于β-HMX，并扩展到多组分状态方程的校准。这两个要素被实现在有限元代码中，以便在介观尺度上模拟冲击到爆轰的转变，并研究准静态热点中的尺寸效应。最后，这些针对含能材料综合多尺度建模的努力也带来了对新的前瞻性实验的需求，本文对此进行了讨论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent developments dedicated to the building of multiscale mechanical andchemical constitutive laws for energetic molecular crystals are presented anddiscussed. In particular, various tools have been specifically incorporated inmolecular dynamics codes to facilitate the subsequent information transfer tothe continuum, i.e. finite elements simulation codes. Atomistic simulationshave been augmented with the capability to follow specific deformation paths aswell as local Lagrangian mechanical metrics, enabling the computation ofmaterials flow stress surface. This mechanistic library allowed theconstruction of a comprehensive non-linear hyperelastic continuum modelincluding crystal plasticity and twinning for TATB. Besides, recent advances inanalyzing reactive molecular dynamics simulations with unsupervised learningalgorithms has enabled the identification and calibration of chemicaldecomposition kinetics for RDX and TATB single crystal. In the present work,the procedure is applied to $\beta$-HMX and extended with the calibration of amulti-components equation of state. These two ingredients are implemented in afinite-element code in order to model the shock-to-detonation transition at themesoscale level and to study dimensionality effects in quasi-static hotspots.Finally, these dedicated efforts towards a comprehensive multiscale modeling ofexplosives has also given rise to the need for new prospective experiments,discussed throughout the paper.</description>
      <author>example@mail.com (Paul Lafourcade, Nicolas Bruzy, Paul Bouteiller, Jean-Bernard Maillet, Christophe Denoual)</author>
      <guid isPermaLink="false">2508.02141v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A Bayesian approach to model uncertainty in single-cell genomic data</title>
      <link>http://arxiv.org/abs/2508.02061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一种基于变分贝叶斯框架的单细胞基因组数据聚类分析方法，通过贝叶斯高斯混合模型估计细胞与不同簇的概率关联，能够捕捉细胞转换状态，并在神经发生和乳腺癌进展研究中获得了生物学上有意义的见解。&lt;h4&gt;背景&lt;/h4&gt;网络模型为分析单细胞计数数据提供了强大框架，有助于表征细胞身份、疾病机制和发育轨迹。然而，在基因组数据无监督学习中的不确定性建模仍未得到充分探索。传统聚类方法为每个细胞分配单一身份，可能掩盖分化或突变过程中的过渡状态。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉细胞转换状态的不确定性建模方法，用于单细胞基因组数据的聚类和分析，以更准确地揭示发育和疾病过程中的动态细胞身份。&lt;h4&gt;方法&lt;/h4&gt;研究引入了一种变分贝叶斯框架用于聚类和分析单细胞基因组数据，采用贝叶斯高斯混合模型来估计细胞与不同簇的概率关联。此外，提出了使用误分簇率和曲线下面积作为评估单细胞RNA测序数据聚类性能的创新指标。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够捕捉细胞转换状态，在神经发生和乳腺癌进展研究中获得了生物学上有意义的见解。推断的聚类概率可以进行进一步分析，包括差异表达分析和伪时间分析。&lt;h4&gt;结论&lt;/h4&gt;这种方法学进展增强了单细胞数据分析的分辨率，能够对发育和疾病中的动态细胞身份进行更细致的表征，为理解复杂的生物过程提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;网络模型为分析单细胞计数数据提供了强大的框架，有助于表征细胞身份、疾病机制和发育轨迹。然而，在基因组数据无监督学习中的不确定性建模仍未得到充分探索。传统的聚类方法为每个细胞分配单一的身份，可能掩盖分化或突变过程中的过渡状态。本研究引入了一种用于聚类和分析单细胞基因组数据的变分贝叶斯框架，采用贝叶斯高斯混合模型来估计细胞与不同簇的概率关联。这种方法能够捕捉细胞转换状态，在神经发生和乳腺癌进展研究中获得了生物学上有意义的见解。推断的聚类概率可以进行进一步分析，包括差异表达分析和伪时间分析。此外，我们提出使用误分簇率和曲线下面积作为评估单细胞RNA测序数据聚类性能的创新指标，以定量评估整体聚类性能。这种方法学进展增强了单细胞数据分析的分辨率，能够对发育和疾病中的动态细胞身份进行更细致的表征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network models provide a powerful framework for analysing single-cell countdata, facilitating the characterisation of cellular identities, diseasemechanisms, and developmental trajectories. However, uncertainty modeling inunsupervised learning with genomic data remains insufficiently explored.Conventional clustering methods assign a singular identity to each cell,potentially obscuring transitional states during differentiation or mutation.This study introduces a variational Bayesian framework for clustering andanalysing single-cell genomic data, employing a Bayesian Gaussian mixture modelto estimate the probabilistic association of cells with distinct clusters. Thisapproach captures cellular transitions, yielding biologically coherent insightsinto neurogenesis and breast cancer progression. The inferred clusteringprobabilities enable further analyses, including Differential ExpressionAnalysis and pseudotime analysis. Furthermore, we propose utilising themisclustering rate and Area Under the Curve in clustering scRNA-seq data as aninnovative metric to quantitatively evaluate overall clustering performance.This methodological advancement enhances the resolution of single-cell dataanalysis, enabling a more nuanced characterisation of dynamic cellularidentities in development and disease.</description>
      <author>example@mail.com (Shanshan Ren, Thomas E. Bartlett, Lina Gerontogianni, Swati Chandna)</author>
      <guid isPermaLink="false">2508.02061v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning</title>
      <link>http://arxiv.org/abs/2508.02039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种模型回收框架，用于源数据自由的迁移学习，通过识别和重用相关源模型的子集，实现了参数高效的模型训练，使模型即服务提供商能够构建高效的预训练模型库。&lt;h4&gt;背景&lt;/h4&gt;数据隐私问题和获取源数据用于模型训练的困难，使得源数据自由的迁移学习变得必要，在这种设置下只能访问预训练模型而无法获取原始源域数据。&lt;h4&gt;目的&lt;/h4&gt;解决源数据不可用时迁移学习的挑战，提出一种模型回收框架用于参数高效训练，识别相关源模型的子集进行重用。&lt;h4&gt;方法&lt;/h4&gt;提出模型回收框架，能够在白盒和黑盒设置下重用源模型子集，实现参数高效的模型训练。&lt;h4&gt;主要发现&lt;/h4&gt;现有迁移学习方法通常依赖源数据访问，限制了其在源数据不可用场景的直接应用；没有源数据信息的情况下难以高效选择用于迁移的模型；无法完全访问源模型的情况下进行迁移也存在困难。&lt;h4&gt;结论&lt;/h4&gt;该框架使模型即服务提供商能够构建高效的预训练模型库，创造了多源数据自由监督迁移学习的机会。&lt;h4&gt;翻译&lt;/h4&gt;对数据隐私日益增长的担忧以及检索用于模型训练的源数据所面临的其他困难，使得对源数据自由的迁移学习变得必要，在这种学习中，人们只能访问预训练模型，而无法获取原始源域的数据。这种设置带来了许多挑战，因为现有的迁移学习方法通常依赖于对源数据的访问，这限制了它们在源数据不可用场景中的直接应用。此外，实际问题使其变得更加困难，例如在没有源数据信息的情况下高效选择用于迁移的模型，以及在无法完全访问源模型的情况下进行迁移。因此，我们提出了一种模型回收框架，用于模型的参数高效训练，该框架能够识别要在白盒和黑盒设置中重用的相关源模型的子集。因此，我们的框架使得模型即服务提供商能够构建高效的预训练模型库，从而创造了多源数据自由监督迁移学习的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Increasing concerns for data privacy and other difficulties associated withretrieving source data for model training have created the need for source-freetransfer learning, in which one only has access to pre-trained models insteadof data from the original source domains. This setting introduces manychallenges, as many existing transfer learning methods typically rely on accessto source data, which limits their direct applicability to scenarios wheresource data is unavailable. Further, practical concerns make it more difficult,for instance efficiently selecting models for transfer without information onsource data, and transferring without full access to the source models. Somotivated, we propose a model recycling framework for parameter-efficienttraining of models that identifies subsets of related source models to reuse inboth white-box and black-box settings. Consequently, our framework makes itpossible for Model as a Service (MaaS) providers to build libraries ofefficient pre-trained models, thus creating an opportunity for multi-sourcedata-free supervised transfer learning.</description>
      <author>example@mail.com (Sijia Wang, Ricardo Henao)</author>
      <guid isPermaLink="false">2508.02039v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2508.01916v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为邻居距离最小化(NDM)的无监督学习方法，能够在神经网络模型中找到可解释的子空间，这些子空间类似于模型使用的变量，为理解模型内部机制提供了新视角。&lt;h4&gt;背景&lt;/h4&gt;理解神经模型的内部表征是机制可解释性的核心兴趣点。由于表征空间的高维性，它可以编码关于输入的各种方面，但这些不同方面在多大程度上被组织和编码在独立的子空间中尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;探索是否可以纯粹以无监督的方式找到神经网络中编码不同方面的'自然'子空间，并验证这些子空间的可解释性和与模型内部变量的关联性。&lt;h4&gt;方法&lt;/h4&gt;提出邻居距离最小化(NDM)方法，以无监督方式学习非基对齐的子空间，并在GPT-2模型上进行实验，验证该方法在大型模型(20亿参数)上的可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;1) 定性分析显示NDM找到的子空间在许多情况下是可解释的；2) 这些子空间中编码的信息在不同输入间往往共享相同的抽象概念；3) 定量实验表明子空间与GPT-2中的电路变量有强联系；4) 该方法可扩展到20亿模型，能够发现分别介导上下文和参数知识路由的独立子空间。&lt;h4&gt;结论&lt;/h4&gt;NDM方法为理解神经网络模型内部表征和构建计算电路提供了新视角，证明了可以通过看似无关的训练目标在无监督方式下发现具有语义意义的子空间结构。&lt;h4&gt;翻译&lt;/h4&gt;理解神经模型的内部表征是机制可解释性的核心兴趣。由于其高维性，表征空间可以编码关于输入的各个方面。不同方面在多大程度上被组织和编码在独立的子空间中？是否可以纯粹以无监督的方式找到这些'自然'子空间？有些令人惊讶的是，我们确实可以通过一个看似无关的训练目标实现这一点，并找到可解释的子空间。我们的方法，邻居距离最小化(NDM)，以无监督方式学习非基对齐的子空间。定性分析显示子空间在许多情况下是可解释的，并且获得的子空间中编码的信息往往在不同输入间共享相同的抽象概念，使这些子空间类似于模型使用的'变量'。我们还使用GPT-2中的已知电路进行了定量实验；结果显示子空间与电路变量之间存在强联系。我们还提供了证据表明该方法可扩展到20亿模型，通过发现分别介导上下文和参数知识路由的独立子空间。更广泛地看，我们的发现为理解模型内部和构建电路提供了新视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding internal representations of neural models is a core interest ofmechanistic interpretability. Due to its large dimensionality, therepresentation space can encode various aspects about inputs. To what extentare different aspects organized and encoded in separate subspaces? Is itpossible to find these ``natural'' subspaces in a purely unsupervised way?Somewhat surprisingly, we can indeed achieve this and find interpretablesubspaces by a seemingly unrelated training objective. Our method, neighbordistance minimization (NDM), learns non-basis-aligned subspaces in anunsupervised manner. Qualitative analysis shows subspaces are interpretable inmany cases, and encoded information in obtained subspaces tends to share thesame abstract concept across different inputs, making such subspaces similar to``variables'' used by the model. We also conduct quantitative experiments usingknown circuits in GPT-2; results show a strong connection between subspaces andcircuit variables. We also provide evidence showing scalability to 2B models byfinding separate subspaces mediating context and parametric knowledge routing.Viewed more broadly, our findings offer a new perspective on understandingmodel internals and building circuits.</description>
      <author>example@mail.com (Xinting Huang, Michael Hahn)</author>
      <guid isPermaLink="false">2508.01916v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>LoRA-based methods on Unet for transfer learning in Subarachnoid Hematoma Segmentation</title>
      <link>http://arxiv.org/abs/2508.01772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了使用迁移学习方法改善动脉瘤性蛛网膜下腔出血(SAH)的医学图像分割。研究团队基于Unet架构实现了从创伤性脑损伤患者到动脉瘤性SAH患者的迁移学习，并开发了创新的LoRA变体方法。结果显示，LoRA方法显著优于传统Unet微调，且CP-LoRA在性能相当的情况下参数更少。&lt;h4&gt;背景&lt;/h4&gt;动脉瘤性蛛网膜下腔出血是一种危及生命的神经科急症，死亡率超过30%。从相关血肿类型进行迁移学习是一种有潜力但尚未充分探索的方法。虽然Unet架构因其在小数据集上的有效性仍然是医学图像分割的金标准，但LoRA方法在医学成像的卷积神经网络中很少被应用。&lt;h4&gt;目的&lt;/h4&gt;研究目的是探索迁移学习方法在动脉瘤性SAH图像分割中的应用，并评估LoRA及其变体方法与传统微调策略的性能比较。&lt;h4&gt;方法&lt;/h4&gt;研究团队实现了基于Unet的架构，该架构在来自多个机构的124名创伤性脑损伤患者的CT扫描上进行了预训练，然后在密歇根大学健康系统的30名动脉瘤性SAH患者上使用3折交叉验证进行了微调。研究团队开发了基于张量CP分解的CP-LoRA方法，并引入了将权重矩阵分解为幅度和方向分量的DoRA变体(DoRA-C, convDoRA, CP-DoRA)。研究团队将这些方法与现有的LoRA方法(LoRA-C, convLoRA)和在不同模块上的标准微调策略进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;LoRA方法持续优于标准Unet微调。性能因出血量而异，所有方法在大体积出血中显示出更高的准确性。CP-LoRA在性能与现有方法相当的情况下使用了显著更少的参数。更高秩的过度参数化比严格的低秩适应持续产生更好的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究表明血肿类型之间的迁移学习是可行的，并且LoRA方法在动脉瘤性SAH分割中显著优于传统的Unet微调。&lt;h4&gt;翻译&lt;/h4&gt;动脉瘤性蛛网膜下腔出血是一种危及生命的神经科急症，死亡率超过30%。从相关血肿类型进行迁移学习是一种有潜力但尚未充分探索的方法。虽然Unet架构因其在小数据集上的有效性仍然是医学图像分割的金标准，但LoRA方法在医学成像的卷积神经网络中很少被应用。研究团队实现了基于Unet的架构，该架构在来自多个机构的124名创伤性脑损伤患者的CT扫描上进行了预训练，然后在密歇根大学健康系统的30名动脉瘤性SAH患者上使用3折交叉验证进行了微调。研究团队开发了基于张量CP分解的CP-LoRA方法，并引入了将权重矩阵分解为幅度和方向分量的DoRA变体(DoRA-C, convDoRA, CP-DoRA)。研究团队将这些方法与现有的LoRA方法(LoRA-C, convLoRA)和在不同模块上的标准微调策略进行了比较。LoRA方法持续优于标准Unet微调。性能因出血量而异，所有方法在大体积出血中显示出更高的准确性。CP-LoRA在性能与现有方法相当的情况下使用了显著更少的参数。更高秩的过度参数化比严格的低秩适应持续产生更好的性能。该研究表明血肿类型之间的迁移学习是可行的，并且LoRA方法在动脉瘤性SAH分割中显著优于传统的Unet微调。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aneurysmal subarachnoid hemorrhage (SAH) is a life-threatening neurologicalemergency with mortality rates exceeding 30%. Transfer learning from relatedhematoma types represents a potentially valuable but underexplored approach.Although Unet architectures remain the gold standard for medical imagesegmentation due to their effectiveness on limited datasets, Low-RankAdaptation (LoRA) methods for parameter-efficient transfer learning have beenrarely applied to convolutional neural networks in medical imaging contexts. Weimplemented a Unet architecture pre-trained on computed tomography scans from124 traumatic brain injury patients across multiple institutions, thenfine-tuned on 30 aneurysmal SAH patients from the University of Michigan HealthSystem using 3-fold cross-validation. We developed a novel CP-LoRA method basedon tensor CP-decomposition and introduced DoRA variants (DoRA-C, convDoRA,CP-DoRA) that decompose weight matrices into magnitude and directionalcomponents. We compared these approaches against existing LoRA methods (LoRA-C,convLoRA) and standard fine-tuning strategies across different modules on amulti-view Unet model. LoRA-based methods consistently outperformed standardUnet fine-tuning. Performance varied by hemorrhage volume, with all methodsshowing improved accuracy for larger volumes. CP-LoRA achieved comparableperformance to existing methods while using significantly fewer parameters.Over-parameterization with higher ranks consistently yielded better performancethan strictly low-rank adaptations. This study demonstrates that transferlearning between hematoma types is feasible and that LoRA-based methodssignificantly outperform conventional Unet fine-tuning for aneurysmal SAHsegmentation.</description>
      <author>example@mail.com (Cristian Minoccheri, Matthew Hodgman, Haoyuan Ma, Rameez Merchant, Emily Wittrup, Craig Williamson, Kayvan Najarian)</author>
      <guid isPermaLink="false">2508.01772v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Classification of Brain Tumors using Hybrid Deep Learning Models</title>
      <link>http://arxiv.org/abs/2508.01350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究应用迁移学习使用较少训练样本实现强分类性能，比较了EfficientNetV2与EfficientNet、ResNet50在脑肿瘤分类中的表现。&lt;h4&gt;背景&lt;/h4&gt;卷积神经网络(CNNs)已大大改善医学图像解释，但传统CNN通常需要大量计算资源和大型训练数据集。&lt;h4&gt;目的&lt;/h4&gt;解决传统CNN对计算资源和训练数据的高需求限制，通过迁移学习实现使用较少训练样本的强分类性能。&lt;h4&gt;方法&lt;/h4&gt;比较EfficientNetV2与其前代EfficientNet以及ResNet50在将脑肿瘤分为胶质瘤、脑膜瘤和垂体瘤三种类型的分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;EfficientNetV2与其他模型相比提供了更好的分类性能。&lt;h4&gt;结论&lt;/h4&gt;EfficientNetV2的性能提升是以训练时间增加为代价的，这可能是由于模型复杂度更高所致。&lt;h4&gt;翻译&lt;/h4&gt;卷积神经网络(CNNs)的使用极大地改善了医学图像的解释。然而，传统的CNN通常需要大量的计算资源和大型训练数据集。为了解决这些限制，本研究应用迁移学习，使用较少的训练样本实现强分类性能。具体而言，研究比较了EfficientNetV2与其前代EfficientNet以及ResNet50在将脑肿瘤分为三种类型(胶质瘤、脑膜瘤和垂体瘤)方面的分类能力。结果表明，与其他模型相比，EfficientNetV2提供了更优的性能。然而，这种改进是以训练时间增加为代价的，可能是由于模型复杂度更高所致。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of Convolutional Neural Networks (CNNs) has greatly improved theinterpretation of medical images. However, conventional CNNs typically demandextensive computational resources and large training datasets. To address theselimitations, this study applied transfer learning to achieve strongclassification performance using fewer training samples. Specifically, thestudy compared EfficientNetV2 with its predecessor, EfficientNet, and withResNet50 in classifying brain tumors into three types: glioma, meningioma, andpituitary tumors. Results showed that EfficientNetV2 delivered superiorperformance compared to the other models. However, this improvement came at thecost of increased training time, likely due to the model's greater complexity.</description>
      <author>example@mail.com (Neerav Nemchand Gala)</author>
      <guid isPermaLink="false">2508.01350v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Disparity Confidence Estimation into Relative Depth Prior-Guided Unsupervised Stereo Matching</title>
      <link>http://arxiv.org/abs/2508.01275v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的无监督立体匹配学习框架，通过立体置信度估计算法和深度先验引导的损失函数解决传统方法中的立体匹配歧义问题，在KITTI基准测试中实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;无监督立体匹配因不需要昂贵的视差标注而受到广泛关注。典型方法依赖多视图一致性假设，但在处理重复模式和纹理less区域等歧义问题时表现不佳。现有知识转移方法从随机稀疏对应关系学习深度排序，导致3D几何知识利用效率低下并引入噪声。&lt;h4&gt;目的&lt;/h4&gt;解决无监督立体匹配中的立体匹配歧义问题，提高3D几何知识利用效率，减少错误视差估计带来的噪声，提升立体匹配准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含三部分的无监督学习框架：1)即插即用的视差置信度估计算法，检查相邻视差与相对深度的局部相干一致性；2)使用高置信度视差估计构建准密集对应关系促进深度排序学习；3)双重视差平滑损失函数提高视差不连续处的匹配性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在所有无监督立体匹配方法中，于KITTI立体匹配基准测试上实现了最先进的立体匹配精度。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过有效利用3D几何知识并解决立体匹配中的歧义问题，显著提升了无监督立体匹配的性能，为该领域研究提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;无监督立体匹配因其独立于昂贵的视差标注而受到广泛关注。典型的无监督方法依赖于多视图一致性假设来训练网络，但这种方法在处理重复模式和纹理less区域等立体匹配歧义问题时表现不佳。一个可行的解决方案是将3D几何知识从相对深度图转移到立体匹配网络中。然而，现有的知识转移方法从随机构建的稀疏对应关系中学习深度排序信息，这使得3D几何知识的利用效率低下，并引入了错误视差估计带来的噪声。这项工作提出了一个新的无监督学习框架来解决这些挑战，该框架包括一个即插即用的视差置信度估计算法和两个深度先验引导的损失函数。具体来说，首先检查相邻视差及其对应相对深度之间的局部相干一致性以获得视差置信度。然后，仅使用高置信度的视差估计构建准密集对应关系，以促进高效的深度排序学习。最后，提出了双重视差平滑损失函数，以提高视差不连续处的立体匹配性能。实验结果表明，我们的方法在所有无监督立体匹配方法中，于KITTI立体匹配基准测试上实现了最先进的立体匹配精度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决无监督立体匹配方法在存在立体匹配模糊性区域（如重复模式和纹理less区域）表现不佳的问题。这个问题很重要，因为立体匹配在3D几何重建、自动驾驶、增强现实等领域有广泛应用，而获取大规模真实世界视差标注数据非常困难，限制了监督学习的发展。无监督方法可以避免对标注数据的依赖，但面临立体匹配模糊性的挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了无监督立体匹配在多视图一致性假设下的局限性，然后受到基于视觉基础模型的监督立体匹配方法的启发，考虑将单目深度模型的几何约束整合到无监督框架中。作者分析了现有知识转移方法的三大局限性：随机对应引入噪声、稀疏对应导致知识利用效率低、简单发散策略缺乏精确指导。作者借鉴了基于马尔可夫随机场的立体匹配方法中的局部相干性概念，以及SC-DepthV3中的置信深度排序损失，设计了选择性构建准密集对应关系的方法，仅使用可靠的视差估计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过视差-深度一致性投票算法估计视差置信度，使用高置信度的视差估计构建准密集对应关系，并设计两个基于相对深度先验的损失函数。整体流程是：1)使用ViTAStereo网络生成视差图和相对深度图；2)应用DDCV算法计算视差置信度；3)基于置信度选择高置信像素作为参考点；4)建立像素与参考点的对应关系；5)计算LDR损失惩罚不一致的视差对应；6)计算DDS损失增强视差与深度的一致性；7)结合传统损失和新的损失函数进行网络训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)DDCV算法，第一个无监督的相对深度先导视差置信度估计方法；2)LDR损失，基于高置信视差构建准密集对应关系；3)DDS损失，增强视差与深度在平滑性和不连续性上的一致性。相比之前的工作不同之处在于：提高了知识转移效率，通过选择可靠视差抑制噪声，设计了更精确的损失函数，以及提出了无监督的视差置信度估计方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于视差置信度估计和相对深度先导损失函数的无监督立体匹配方法，显著提高了在立体匹配模糊性区域的准确性，并在KITTI立体匹配基准测试上达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised stereo matching has garnered significant attention for itsindependence from costly disparity annotations. Typical unsupervised methodsrely on the multi-view consistency assumption for training networks, whichsuffer considerably from stereo matching ambiguities, such as repetitivepatterns and texture-less regions. A feasible solution lies in transferring 3Dgeometric knowledge from a relative depth map to the stereo matching networks.However, existing knowledge transfer methods learn depth ranking informationfrom randomly built sparse correspondences, which makes inefficient utilizationof 3D geometric knowledge and introduces noise from mistaken disparityestimates. This work proposes a novel unsupervised learning framework toaddress these challenges, which comprises a plug-and-play disparity confidenceestimation algorithm and two depth prior-guided loss functions. Specifically,the local coherence consistency between neighboring disparities and theircorresponding relative depths is first checked to obtain disparity confidence.Afterwards, quasi-dense correspondences are built using only confidentdisparity estimates to facilitate efficient depth ranking learning. Finally, adual disparity smoothness loss is proposed to boost stereo matching performanceat disparity discontinuities. Experimental results demonstrate that our methodachieves state-of-the-art stereo matching accuracy on the KITTI Stereobenchmarks among all unsupervised stereo matching methods.</description>
      <author>example@mail.com (Chuang-Wei Liu, Mingjian Sun, Cairong Zhao, Hanli Wang, Alexander Dvorkovich, Rui Fan)</author>
      <guid isPermaLink="false">2508.01275v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Perspective from a Broader Context: Can Room Style Knowledge Help Visual Floorplan Localization?</title>
      <link>http://arxiv.org/abs/2508.01216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to AAAI 2026. arXiv admin note: text overlap with  arXiv:2507.18881&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用视觉场景上下文增强视觉楼层平面定位的方法，通过无监督学习技术预训练房间判别器，提取房间类型信息，并将其注入到FLoc算法中以提高定位的鲁棒性和准确性。&lt;h4&gt;背景&lt;/h4&gt;建筑物的楼层平面图随时间保持一致且对视觉外观变化具有鲁棒性，因此视觉楼层平面定位受到研究者关注。然而，楼层平面图包含许多重复结构，容易导致定位模糊。&lt;h4&gt;目的&lt;/h4&gt;使用更广泛的视觉场景上下文增强FLoc算法，利用场景布局先验来消除定位不确定性。&lt;h4&gt;方法&lt;/h4&gt;提出带有聚类约束的无监督学习技术，在未标记房间图像上预训练房间判别器，提取隐藏房间类型并区分不同房间类型，将判别器总结的场景上下文信息注入到FLoc算法中。&lt;h4&gt;主要发现&lt;/h4&gt;在两个标准的视觉FLoc基准上进行的实验表明，该方法优于最先进的方法，在鲁棒性和准确性方面取得了显著改进。&lt;h4&gt;结论&lt;/h4&gt;通过注入场景上下文信息，有效利用了房间风格知识来指导明确的视觉楼层平面定位，提高了定位性能。&lt;h4&gt;翻译&lt;/h4&gt;由于建筑物的楼层平面图随时间保持一致且对视觉外观变化具有内在鲁棒性，视觉楼层平面定位已受到研究者的越来越多的关注。然而，作为建筑物布局的紧凑和极简表示，楼层平面图包含许多重复结构（例如走廊和角落），因此容易导致定位模糊。现有方法要么寄希望于匹配楼层平面图中的二维结构线索，要么依赖于3D几何约束的视觉预训练，忽略了视觉图像提供的更丰富的上下文信息。在本文中，我们建议使用更广泛的视觉场景上下文来增强FLoc算法，利用场景布局先验来消除定位不确定性。特别是，我们提出了一种带有聚类约束的无监督学习技术，在自行收集的未标记房间图像上预训练一个房间判别器。这种判别器可以经验性地提取观察图像的隐藏房间类型，并将其与其他房间类型区分开。通过将判别器总结的场景上下文信息注入到FLoc算法中，房间风格知识被有效利用来指导明确的视觉FLoc。我们在两个标准的视觉FLoc基准上进行了充分的比较研究。我们的实验表明，我们的方法优于最先进的方法，在鲁棒性和准确性方面取得了显著改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉平面定位（Visual Floorplan Localization, FLoc）中的歧义性问题。由于平面图作为建筑的紧凑表示包含许多重复结构（如走廊和角落），现有方法容易产生错误的定位结果。这个问题在现实中很重要，因为室内视觉定位广泛应用于3D重建、AR/VR和机器人导航等领域，而平面图是轻量级、易于获取且随时间保持一致的定位辅助工具，能有效解决室内场景缺乏卫星信号的问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到不同类型房间（如卧室、浴室）具有特定视觉风格，这些风格可作为上下文信息解决定位歧义。他们借鉴了F3Loc框架的架构（前端观察模型和后端贝叶斯滤波器），并设计了聚类约束的无监督学习技术来训练房间判别器。这种方法不需要人工标注的房间类别信息，而是利用导航任务的难度信息构建约束矩阵，指导模型学习房间的风格表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用房间风格知识解决平面图定位中的歧义性。整体流程包括：1) 自动收集室内场景RGB图像并过滤掉缺乏房间风格信息的图像；2) 使用聚类约束的无监督学习技术训练房间判别器，提取房间风格特征；3) 将预训练的房间风格编码器转移到视觉FLoc任务中；4) 在F3Loc框架基础上，将房间风格知识注入观察模型，指导定位过程。方法支持单帧、多帧和自适应三种定位模式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次利用房间视觉风格解决平面图定位歧义；2) 提出聚类约束的无监督学习技术训练房间判别器；3) 设计自动化数据收集和处理流程。相比之前的工作，不同之处在于：不依赖2D结构线索或3D几何约束，而是提供场景级别的上下文信息；不要求人工标注的语义信息；能在视觉外观相似情况下也能提供准确定位，而不仅仅是依赖视觉外观显著变化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种利用房间风格知识的视觉平面定位方法，通过无监督学习技术训练房间判别器将场景上下文信息注入定位算法，有效解决了由平面图中重复结构引起的定位歧义问题，显著提高了定位的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Since a building's floorplan remains consistent over time and is inherentlyrobust to changes in visual appearance, visual Floorplan Localization (FLoc)has received increasing attention from researchers. However, as a compact andminimalist representation of the building's layout, floorplans contain manyrepetitive structures (e.g., hallways and corners), thus easily result inambiguous localization. Existing methods either pin their hopes on matching 2Dstructural cues in floorplans or rely on 3D geometry-constrained visualpre-trainings, ignoring the richer contextual information provided by visualimages. In this paper, we suggest using broader visual scene context to empowerFLoc algorithms with scene layout priors to eliminate localization uncertainty.In particular, we propose an unsupervised learning technique with clusteringconstraints to pre-train a room discriminator on self-collected unlabeled roomimages. Such a discriminator can empirically extract the hidden room type ofthe observed image and distinguish it from other room types. By injecting thescene context information summarized by the discriminator into an FLocalgorithm, the room style knowledge is effectively exploited to guide definitevisual FLoc. We conducted sufficient comparative studies on two standard visualFloc benchmarks. Our experiments show that our approach outperformsstate-of-the-art methods and achieves significant improvements in robustnessand accuracy.</description>
      <author>example@mail.com (Bolei Chen, Shengsheng Yan, Yongzheng Cui, Jiaxu Kang, Ping Zhong, Jianxin Wang)</author>
      <guid isPermaLink="false">2508.01216v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Fine-tuning physics-informed neural networks for cavity flows using coordinate transformation</title>
      <link>http://arxiv.org/abs/2508.01122v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 15 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于坐标变换的物理信息神经网络微调方法，用于解决不同形状腔驱动流动问题，通过预训练模型和变形梯度张量提高训练效率。&lt;h4&gt;背景&lt;/h4&gt;物理信息神经网络(PINNs)作为求解偏微分方程的替代方法受到关注，但其训练成本高。使用迁移学习或微调预训练模型是潜在解决方案，但预训练模型与目标几何形状和流动条件不匹配时效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种微调方法，使预训练的PINNs能够适应不同几何形状的腔驱动流动问题，提高训练效率并减少训练成本。&lt;h4&gt;方法&lt;/h4&gt;提出结合坐标变换的PINNs微调方法：公式化逆问题给定域内参考数据和壁面边界条件；使用任意雷诺数和形状预训练的PINN初始化目标DNN；通过变形梯度张量进行坐标变换，将控制方程作为PINNs损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;数值实验表明，对于方形、矩形和剪切变形几何形状的各种腔流动，所提方法比未训练模型提高了训练收敛性；使用与目标几何形状相似的模型预训练可进一步提高训练效率。&lt;h4&gt;结论&lt;/h4&gt;基于坐标变换的PINNs微调方法有效解决了不同几何形状腔驱动流动问题，提高了训练效率，对颅内动脉瘤血流建模等临床应用具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;物理信息神经网络(PINNs)作为一种使用深度神经网络(DNN)求解偏微分方程的替代方法已受到关注。它们的简单性和能力使它们能够解决许多应用的逆问题。尽管PINNs具有多功能性，但降低其训练成本仍然具有挑战性。使用通过迁移学习或微调使用任意数据集预训练的DNN是一种潜在的解决方案。然而，使用与目标不同几何形状和流动条件预训练的模型可能不会产生合适的结果。本文提出了一种结合坐标变换的PINNs微调方法，用于模拟各种形状的腔驱动流动。我们将逆问题公式化，给定域内的参考数据和壁面边界条件。使用任意雷诺数和形状预训练的PINN模型来初始化目标DNN。为了使参考形状与不同目标相协调，使用变形梯度张量进行坐标变换，并将控制方程作为PINNs的损失函数。针对方形、矩形和剪切变形几何形状的各种腔流动的数值示例表明，与未训练模型相比，所提出的微调方法提高了训练收敛性。使用与目标几何形状相似的模型预训练可以进一步提高训练效率。这些发现对于临床应用中的颅内动脉瘤血流建模等实际应用是有用的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physics-informed neural networks (PINNs) have attracted attention as analternative approach to solve partial differential equations using a deepneural network (DNN). Their simplicity and capability allow them to solveinverse problems for many applications. Despite the versatility of PINNs, itremains challenging to reduce their training cost. Using a DNN pre-trained withan arbitrary dataset with transfer learning or fine-tuning is a potentialsolution. However, a pre-trained model using a different geometry and flowcondition than the target may not produce suitable results. This paper proposesa fine-tuning approach for PINNs with coordinate transformation, modellinglid-driven cavity flows with various shapes. We formulate the inverse problem,where the reference data inside the domain and wall boundary conditions aregiven. A pre-trained PINN model with an arbitrary Reynolds number and shape isused to initialize a target DNN. To reconcile the reference shape withdifferent targets, governing equations as a loss of the PINNs are given withcoordinate transformation using a deformation gradient tensor. Numericalexamples for various cavity flows with square, rectangular, and shear deformedgeometries demonstrate that the proposed fine-tuning approach improves thetraining convergence compared with an un-trained model. A pre-trained modelwith a similar geometry to the target further increases training efficiency.These findings are useful for real-world applications such as modellingintra-aneurysmal blood flows in clinical use.</description>
      <author>example@mail.com (Ryuta Takao, Satoshi Ii)</author>
      <guid isPermaLink="false">2508.01122v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing material behavior discovery using embedding-oriented Physically-Guided Neural Networks with Internal Variables</title>
      <link>http://arxiv.org/abs/2508.00959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究改进了物理引导神经网络与内部变量（PGNNIV）框架，通过降阶建模技术解决其在高维数据应用中的可扩展性挑战，并引入了模型重用策略以提高效率。&lt;h4&gt;背景&lt;/h4&gt;物理引导神经网络与内部变量（PGNNIV）是一种仅使用可观测数据进行训练的科学机器学习工具，能够揭示内部状态关系。然而，当应用于高维数据（如细网格空间场或时间演化系统）时，这些模型面临可扩展性挑战。&lt;h4&gt;目的&lt;/h4&gt;提出对PGNNIV框架的增强方法，解决其在高维数据应用中的可扩展性限制，提高计算效率、准确性、噪声容忍度和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;通过降阶建模技术改进PGNNIV框架，引入使用谱分解、POD和预训练自编码器映射的替代原始解码器结构；集成模型重用策略，包括迁移学习和微调技术，以利用先前获得的知识。&lt;h4&gt;主要发现&lt;/h4&gt;增强的PGNNIV框架成功识别了潜在的构成状态方程，同时保持了高预测准确性；提高了对噪声的鲁棒性，减轻了过拟合，并降低了计算需求；模型重用策略显著减少了训练时间，同时保持或提高了模型性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的技术可以根据数据可用性、资源和特定建模目标进行调整，克服各种场景中的可扩展性挑战，使PGNNIV能够更有效地应用于高维数据。&lt;h4&gt;翻译&lt;/h4&gt;物理引导神经网络与内部变量是科学机器学习工具，它们仅使用可观测数据进行训练，并有能力揭示内部状态关系。它们通过规定模型架构和使用损失正则化来整合物理知识，从而使某些特定神经元具有作为内部状态变量的物理意义。尽管有这些潜力，但当这些模型应用于高维数据（如细网格空间场或时间演化系统）时，它们面临可扩展性挑战。在这项工作中，我们提出了一些对PGNNIV框架的增强，通过降阶建模技术解决这些可扩展性限制。具体来说，我们引入了使用谱分解、POD和预训练自编码器映射的替代原始解码器结构。这些替代解码器在计算效率、准确性、噪声容忍度和泛化能力之间提供了不同的权衡，同时显著提高了可扩展性。此外，我们通过迁移学习和微调策略集成模型重用，利用先前获得的知识，支持对新材料或配置的高效适应，同时显著减少训练时间，同时保持或提高模型性能。为了说明这些技术，我们使用了一个由非线性扩散方程控制的代表性案例，仅使用可观测数据。结果表明，增强的PGNNIV框架成功识别了潜在的构成状态方程，同时保持了高预测准确性。它还提高了对噪声的鲁棒性，减轻了过拟合，并降低了计算需求。所提出的技术可以根据数据可用性、资源和特定建模目标进行调整，克服所有场景中的可扩展性挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physically Guided Neural Networks with Internal Variables are SciML toolsthat use only observable data for training and and have the capacity to unravelinternal state relations. They incorporate physical knowledge both byprescribing the model architecture and using loss regularization, thus endowingcertain specific neurons with a physical meaning as internal state variables.Despite their potential, these models face challenges in scalability whenapplied to high-dimensional data such as fine-grid spatial fields ortime-evolving systems. In this work, we propose some enhancements to the PGNNIVframework that address these scalability limitations through reduced-ordermodeling techniques. Specifically, we introduce alternatives to the originaldecoder structure using spectral decomposition, POD, and pretrainedautoencoder-based mappings. These surrogate decoders offer varying trade-offsbetween computational efficiency, accuracy, noise tolerance, andgeneralization, while improving drastically the scalability. Additionally, weintegrate model reuse via transfer learning and fine-tuning strategies toexploit previously acquired knowledge, supporting efficient adaptation to novelmaterials or configurations, and significantly reducing training time whilemaintaining or improving model performance. To illustrate these varioustechniques, we use a representative case governed by the nonlinear diffusionequation, using only observable data. Results demonstrate that the enhancedPGNNIV framework successfully identifies the underlying constitutive stateequations while maintaining high predictive accuracy. It also improvesrobustness to noise, mitigates overfitting, and reduces computational demands.The proposed techniques can be tailored to various scenarios depending on dataavailability, resources, and specific modeling objectives, overcomingscalability challenges in all the scenarios.</description>
      <author>example@mail.com (Rubén Muñoz-Sierra, Manuel Doblaré, Jacobo Ayensa-Jiménez)</author>
      <guid isPermaLink="false">2508.00959v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.02288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种仅基于事件摄像头的立体3D物体检测框架，解决了传统传感器在高速场景下感知间隔的问题，通过双滤波机制和边界框对齐方法提高了检测性能。&lt;h4&gt;背景&lt;/h4&gt;3D物体检测对自主系统至关重要，可精确定位和估计物体尺寸。传统LiDAR和RGB摄像头因固定帧率在高速场景下会产生感知间隔。事件摄像头具有异步特性和高时间分辨率，能连续捕捉运动。但现有结合事件摄像头与传统传感器的方法依赖同步传感器，在快速运动场景下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种仅依赖事件摄像头的立体3D物体检测框架，消除对传统3D传感器的需求，解决高速场景下的感知间隔问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种纯事件摄像头驱动的立体3D物体检测框架；引入双滤波机制提取事件数据中的语义和几何信息；通过将边界框与以物体为中心的信息对齐来增强回归效果。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在动态环境中优于先前的方法；验证了事件摄像头在鲁棒、连续时间3D感知方面的潜力。&lt;h4&gt;结论&lt;/h4&gt;事件摄像头可作为传统传感器的有效替代，特别是在高速场景下；所提出的方法为基于事件的3D物体检测提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;3D物体检测对自主系统至关重要，能够实现精确定位和尺寸估计。虽然LiDAR和RGB摄像头被广泛使用，但它们的固定帧率在高速场景下会产生感知间隔。事件摄像头以其异步特性和高时间分辨率，通过连续捕捉运动提供了解决方案。最近将事件摄像头与传统传感器集成用于连续时间检测的方法，由于其依赖于同步传感器，在快速运动场景下表现不佳。我们提出了一种新颖的立体3D物体检测框架，仅依赖事件摄像头，消除了对传统3D传感器的需求。为了弥补事件数据中语义和几何信息的不足，我们引入了双滤波机制来提取这两类信息。此外，我们通过将边界框与以物体为中心的信息对齐来增强回归效果。实验表明，我们的方法在动态环境中优于先前的方法，展示了事件摄像头在鲁棒、连续时间3D感知方面的潜力。代码可在https://github.com/mickeykang16/Ev-Stereo3D获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是在高速动态场景下3D物体检测的感知延迟问题。传统传感器如LiDAR和RGB相机由于固定帧率限制，在高动态场景下会产生感知间隙，导致检测延迟。这一问题在自动驾驶系统中尤为重要，因为在高速场景下，即使是微小的检测延迟也可能导致严重的安全事故。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有连续时间3D物体检测方法（如Ev-3DOD）的局限性，即它们仍依赖于同步传感器（LiDAR和RGB相机），在快速运动场景下表现不佳。因此，作者提出仅使用事件相机来实现3D物体检测，避免对同步传感器的依赖。方法设计上借鉴了PSMNet的特征提取器设计，参考了Ev-3DOD的连续时间检测框架，并利用了立体视觉和Transformer机制来增强特征对齐。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是仅使用异步事件相机实现连续时间的3D物体检测，通过双重语义-几何滤波机制增强事件数据的语义和几何信息，并利用以物体为中心的ROI对齐来提高回归性能。整体流程包括：1)几何平面扫描体积构建深度估计；2)双重语义-几何滤波器增强语义和几何信息；3)全局3D检测器预测3D边界框；4)以物体为中心的ROI对齐细化边界框预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)第一个完全异步的3D物体检测系统，完全基于事件相机；2)双重语义-几何滤波机制协同增强信息；3)以物体为中心的ROI对齐提高回归准确性；4)连续时间3D检测框架。相比之前的工作（如Ev-3DOD），本文方法无需同步传感器，在高速动态场景下表现更好，时间分辨率更高，且有专门的语义-几何交互机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种仅使用立体事件相机的完全异步3D物体检测方法，通过双重语义-几何滤波和以物体为中心的ROI对齐机制，实现了在高速动态场景下连续时间的精确3D物体检测。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection is essential for autonomous systems, enabling preciselocalization and dimension estimation. While LiDAR and RGB cameras are widelyused, their fixed frame rates create perception gaps in high-speed scenarios.Event cameras, with their asynchronous nature and high temporal resolution,offer a solution by capturing motion continuously. The recent approach, whichintegrates event cameras with conventional sensors for continuous-timedetection, struggles in fast-motion scenarios due to its dependency onsynchronized sensors. We propose a novel stereo 3D object detection frameworkthat relies solely on event cameras, eliminating the need for conventional 3Dsensors. To compensate for the lack of semantic and geometric information inevent data, we introduce a dual filter mechanism that extracts both.Additionally, we enhance regression by aligning bounding boxes withobject-centric information. Experiments show that our method outperforms priorapproaches in dynamic environments, demonstrating the potential of eventcameras for robust, continuous-time 3D perception. The code is available athttps://github.com/mickeykang16/Ev-Stereo3D.</description>
      <author>example@mail.com (Jae-Young Kang, Hoonhee Cho, Kuk-Jin Yoon)</author>
      <guid isPermaLink="false">2508.02288v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive LiDAR Scanning: Harnessing Temporal Cues for Efficient 3D Object Detection via Multi-Modal Fusion</title>
      <link>http://arxiv.org/abs/2508.01562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种预测性历史感知的自适应扫描框架，通过智能选择LiDAR扫描区域，显著降低能耗同时保持3D目标检测性能。&lt;h4&gt;背景&lt;/h4&gt;传统LiDAR传感器进行密集、无状态扫描，忽略了现实场景中的时间连续性，导致感知冗余和高能耗，限制了在资源受限平台上的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来减少LiDAR传感器的能耗，同时保持或提高3D目标检测的性能，使其更适合资源受限平台。&lt;h4&gt;方法&lt;/h4&gt;提出一种包含轻量级预测器网络和可微分掩码生成器网络的框架，预测器网络将历史时空上下文转化为查询嵌入，掩码生成器网络利用Gumbel-Softmax采样生成二进制掩码，识别下一帧的关键ROI，实现自适应扫描。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Lyft基准上的实验表明，该方法将LiDAR能耗降低了65%以上，同时与传统密集LiDAR扫描方法相比，保持了具有竞争力甚至更优的3D目标检测性能。&lt;h4&gt;结论&lt;/h4&gt;通过预测性历史感知的自适应扫描策略，可以在显著降低能耗的同时保持甚至提高3D目标检测性能，为资源受限平台上的多传感器融合应用提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;使用LiDAR和RGB相机的多传感器融合显著增强了3D目标检测任务。然而，传统的LiDAR传感器执行密集、无状态的扫描，忽略了现实场景中的强时间连续性。这导致了大量的感知冗余和过高的能耗，限制了它们在资源受限平台上的实用性。为了解决这种低效率问题，我们提出了一种预测性的、历史感知的自适应扫描框架，基于过去的观察来预测感兴趣区域。我们的方法引入了一个轻量级的预测器网络，将历史空间和时间上下文提炼为精细的查询嵌入。这些嵌入指导一个可微分的掩码生成器网络，利用Gumbel-Softmax采样生成二进制掩码，识别下一帧的关键ROI。我们的方法通过仅在这些ROI内进行密集LiDAR扫描，其他地方稀疏采样，显著减少了不必要的数据采集。在nuScenes和Lyft基准上的实验表明，我们的自适应扫描策略将LiDAR能耗降低了65%以上，同时与传统使用密集LiDAR扫描的LiDAR-相机融合方法相比，保持了具有竞争力甚至更优的3D目标检测性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决传统激光雷达(LiDAR)传感器在3D物体检测中的效率低下问题。传统LiDAR执行密集、无状态的扫描，忽略了场景中的时序连续性，导致大量感知冗余和过度能源消耗。这个问题很重要，因为LiDAR作为主动传感器能耗远高于相机(10-100W vs 1-5W)，高能耗限制了LiDAR在资源受限平台上的实际应用，增加了自动驾驶系统的运行成本和可持续性挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到传统LiDAR扫描是'无状态'的，每次帧都执行均匀角度密集扫描，忽略了真实环境中世界不会每十分之一秒就完全重新排列的事实。基于这一观察，作者设计了预测性、历史感知的自适应扫描框架，利用过去帧预测下一帧的感兴趣区域(ROI)。该方法借鉴了现有的多模态融合技术、QTNet中的运动引导时序模块(MTM)进行查询预测，以及Gumbel-Softmax采样技术生成二进制掩码，同时改进了传统体素化方法使其可微分以支持端到端训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用历史帧信息预测下一帧中的感兴趣区域(ROI)，只在ROI区域进行密集LiDAR扫描，而在非ROI区域进行稀疏扫描，从而减少不必要的数据采集，降低能耗。整体流程包括：1)历史感知查询预测模块使用过去帧信息预测当前帧的物体位置；2)可微分掩码生成器将预测转换为扫描策略；3)自适应执行扫描，ROI区域密集扫描，非ROI区域稀疏采样；4)可微分体素化处理点云数据；5)结合多种损失函数进行模型训练，包括3D检测损失、蒸馏损失、掩码损失和条件风险价值(CVaR)损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)历史驱动的自适应LiDAR范式，从无记忆转向利用过去预测未来ROI；2)两阶段架构结合历史感知查询预测和可微分掩码生成；3)可微分体素化层实现端到端训练；4)条件风险价值(CVaR)损失提高对小物体的检测鲁棒性。相比之前工作，不同之处在于：传统多模态融合方法假设每帧都有密集扫描，不利用时序信息；现有自适应推理主要关注计算资源而非感知资源；传统点云下采样是后处理技术，而本文在捕获前控制扫描密度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种利用历史信息预测感兴趣区域的自适应LiDAR扫描方法，能够在减少65%以上LiDAR能耗的同时，保持或提升3D物体检测性能，为自动驾驶系统提供了一种更高效、更可持续的感知方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-sensor fusion using LiDAR and RGB cameras significantly enhances 3Dobject detection task. However, conventional LiDAR sensors perform dense,stateless scans, ignoring the strong temporal continuity in real-world scenes.This leads to substantial sensing redundancy and excessive power consumption,limiting their practicality on resource-constrained platforms. To address thisinefficiency, we propose a predictive, history-aware adaptive scanningframework that anticipates informative regions of interest (ROI) based on pastobservations. Our approach introduces a lightweight predictor network thatdistills historical spatial and temporal contexts into refined queryembeddings. These embeddings guide a differentiable Mask Generator network,which leverages Gumbel-Softmax sampling to produce binary masks identifyingcritical ROIs for the upcoming frame. Our method significantly reducesunnecessary data acquisition by concentrating dense LiDAR scanning only withinthese ROIs and sparsely sampling elsewhere. Experiments on nuScenes and Lyftbenchmarks demonstrate that our adaptive scanning strategy reduces LiDAR energyconsumption by over 65% while maintaining competitive or even superior 3Dobject detection performance compared to traditional LiDAR-camera fusionmethods with dense LiDAR scanning.</description>
      <author>example@mail.com (Sara Shoouri, Morteza Tavakoli Taba, Hun-Seok Kim)</author>
      <guid isPermaLink="false">2508.01562v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy</title>
      <link>http://arxiv.org/abs/2507.21358v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The manuscript has been accepted by ICONIP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为协作感知器(CoP)的多任务学习框架，通过整合空间占用信息作为辅助任务，显著提升了基于视觉的鸟瞰图3D物体检测性能，在nuScenes测试集上实现了49.5%的mAP和59.2%的NDS的优异成绩。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的鸟瞰图(BEV)3D物体检测在自动驾驶领域取得了显著进展，具有成本效益和丰富的上下文信息。然而，现有方法通常通过折叠提取的物体特征来构建BEV表示，忽略了内在的环境上下文（如道路和人行道），这阻碍了检测器全面感知物理世界特征的能力。&lt;h4&gt;目的&lt;/h4&gt;引入一个多任务学习框架，利用空间占用率作为辅助信息，挖掘3D物体检测和占用预测任务之间一致的结构和概念相似性，弥补空间表示和特征细化方面的差距。&lt;h4&gt;方法&lt;/h4&gt;提出一个管道生成密集占用率真实值（包含局部密度信息LDO）；采用体素高度引导的采样(VHS)策略根据不同物体特性提炼细粒度局部特征；开发全局-局部协作特征融合(CFF)模块无缝整合两个任务间的互补知识，构建更强大的BEV表示。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes基准测试上进行了大量实验，CoP优于现有基于视觉的框架，在测试集上达到49.5%的mAP和59.2%的NDS。&lt;h4&gt;结论&lt;/h4&gt;协作感知器(CoP)框架有效提升了基于视觉的BEV 3D物体检测性能，通过整合空间占用信息作为辅助任务，能够更好地理解环境上下文，从而构建更准确的BEV表示。&lt;h4&gt;翻译&lt;/h4&gt;基于视觉的鸟瞰图(BEV)3D物体检测在自动驾驶领域已取得显著进展，提供了成本效益和丰富的上下文信息。然而，现有方法通常通过折叠提取的物体特征来构建BEV表示，忽略了内在的环境上下文，如道路和人行道。这阻碍了检测器全面感知物理世界特征的能力。为缓解这一问题，我们引入了一个多任务学习框架——协作感知器(CoP)，它利用空间占用率作为辅助信息，挖掘3D物体检测和占用预测任务之间一致的结构和概念相似性，弥补了空间表示和特征细化方面的差距。为此，我们首先提出一个管道来生成包含局部密度信息(LDO)的密集占用率真实值，用于重建详细的环境信息。接着，我们采用体素高度引导的采样(VHS)策略，根据不同的物体特性提炼细粒度的局部特征。此外，我们还开发了一个全局-局部协作特征融合(CFF)模块，无缝整合两个任务之间的互补知识，从而构建更强大的BEV表示。在nuScenes基准测试上的大量实验表明，CoP优于现有的基于视觉的框架，在测试集上实现了49.5%的mAP和59.2%的NDS。代码和补充材料可在以下链接获取：https://github.com/jichengyuan/Collaborative-Perceiver。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于鸟瞰图(BEV)的3D物体检测方法中忽略环境内在上下文信息的问题。现有方法通过压缩多视图特征构建BEV表示，无法全面感知物理世界特征，特别是在识别具有独特或不规则几何形状的物体时表现不佳。这个问题在自动驾驶领域至关重要，因为准确感知周围环境对确保安全至关重要，而现有方法难以处理复杂交通场景，无法满足自动驾驶的安全需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过引入多任务学习框架Collaborative Perceiver (CoP)，利用空间占用作为辅助信息来挖掘3D物体检测和占用预测任务之间的一致结构和概念相似性。作者借鉴了现有BEV-based方法如BEVDet的Lift-Splat-Shoot视图变换，以及3D占用预测方法如SurroundOcc的概念，但改进了它们在处理点云密度不均匀问题上的不足。同时借鉴了FlashOcc的通道到高度(C2H)模块，用于特征转换。作者设计了一个包含局部密度信息的密集占用生成管道、体素高度引导的采样策略和全局-局部协作特征融合模块的创新组合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多任务学习框架将3D物体检测和3D占用预测任务结合起来，利用两个任务之间的互补和一致知识，结合局部密度感知的空间占用信息作为辅助监督，帮助模型更好地理解环境结构。整体流程包括：1)输入多摄像头图像并提取特征；2)使用Lift-Splat-Shoot将2D特征转换为3D体素特征；3)采用体素高度引导的采样提取细粒度局部特征；4)使用全局池化提取高度无关的全局特征；5)通过全局-局部协作特征融合模块整合特征；6)将统一BEV特征转换回体素特征；7)分别执行3D检测和占用预测；8)使用局部密度矩阵优化多任务学习目标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)局部密度感知的空间占用生成(LDO)，考虑点云非均匀分布为每个体素分配局部密度因子；2)体素高度引导的采样(VHS)策略，根据占用体素高度分布进行分层采样；3)全局-局部协作特征融合(CFF)模块，通过自适应参数融合全局和局部特征。相比之前工作，CoP保留了传统方法忽略的不同高度范围内的细粒度信息，考虑了局部密度作为表示细粒度物体结构的关键因素，并通过多任务学习整合了两个任务的互补知识，而不是仅依赖任务特定的局部或全局信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Collaborative Perceiver通过引入局部密度感知的空间占用作为辅助信息，结合体素高度引导的采样和全局-局部协作特征融合，有效提升了基于视觉的3D物体检测性能，特别是在复杂交通场景中识别具有不规则几何形状的物体方面取得了显著进步。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-based bird's-eye-view (BEV) 3D object detection has advancedsignificantly in autonomous driving by offering cost-effectiveness and richcontextual information. However, existing methods often construct BEVrepresentations by collapsing extracted object features, neglecting intrinsicenvironmental contexts, such as roads and pavements. This hinders detectorsfrom comprehensively perceiving the characteristics of the physical world. Toalleviate this, we introduce a multi-task learning framework, CollaborativePerceiver (CoP), that leverages spatial occupancy as auxiliary information tomine consistent structural and conceptual similarities shared between 3D objectdetection and occupancy prediction tasks, bridging gaps in spatialrepresentations and feature refinement. To this end, we first propose apipeline to generate dense occupancy ground truths incorporating local densityinformation (LDO) for reconstructing detailed environmental information. Next,we employ a voxel-height-guided sampling (VHS) strategy to distill fine-grainedlocal features according to distinct object properties. Furthermore, we developa global-local collaborative feature fusion (CFF) module that seamlesslyintegrates complementary knowledge between both tasks, thus composing morerobust BEV representations. Extensive experiments on the nuScenes benchmarkdemonstrate that CoP outperforms existing vision-based frameworks, achieving49.5\% mAP and 59.2\% NDS on the test set. Code and supplementary materials areavailable at this link https://github.com/jichengyuan/Collaborative-Perceiver.</description>
      <author>example@mail.com (Jicheng Yuan, Manh Nguyen Duc, Qian Liu, Manfred Hauswirth, Danh Le Phuoc)</author>
      <guid isPermaLink="false">2507.21358v4</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A survey on proximity monitoring and warning in construction</title>
      <link>http://arxiv.org/abs/2508.00862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究全面回顾了2010-2024年间发表的97篇相关文章，对接近度监测和预警(PMW)进行了文献计量分析和定性审查，指出了当前研究的局限性和未来发展方向。&lt;h4&gt;背景&lt;/h4&gt;各种技术被应用于监测两个施工实体之间的距离，防止撞击事故，从而提高现场安全性。&lt;h4&gt;目的&lt;/h4&gt;全面回顾接近度监测和预警(PMW)的相关研究工作，呈现当前研究状况、局限性和未来方向。&lt;h4&gt;方法&lt;/h4&gt;通过文献计量分析揭示技术路线图和研究网络，从四个角度进行定性审查：影响因素研究、危险等级定义和确定、接近度感知、警报发布和接收。&lt;h4&gt;主要发现&lt;/h4&gt;当前接近度感知存在局限性和挑战，未来研究方向包括端到端的三维物体检测、动态施工场景的实时三维重建和更新、多模态融合。&lt;h4&gt;结论&lt;/h4&gt;本综述呈现了PMW的当前研究状况、局限性和未来方向，指导PMW系统的未来发展。&lt;h4&gt;翻译&lt;/h4&gt;Various technologies have been applied to monitor the proximity between two construction entities, preventing struck-by accidents and thereby enhancing onsite safety. This study comprehensively reviews related efforts dedicated to proximity monitoring and warning (PMW) based on 97 relevant articles published between 2010 and 2024. The bibliometric analysis reveals the technical roadmap over time, as well as the five most influential leaders and the two largest research networks they have established. The qualitative review is then conducted from four perspectives: influencing factor study, hazard level definition and determination, proximity perception, and alarm issuing and receiving. Finally, the limitations and challenges of current proximity perception are discussed, along with corresponding future research directions, including end-to-end three-dimensional (3D) object detection, real-time 3D reconstruction and updating for dynamic construction scenes, and multimodal fusion. This review presents the current research status, limitations, and future directions of PMW, guiding the future development of PMW systems.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Various technologies have been applied to monitor the proximity between twoconstruction entities, preventing struck-by accidents and thereby enhancingonsite safety. This study comprehensively reviews related efforts dedicated toproximity monitoring and warning (PMW) based on 97 relevant articles publishedbetween 2010 and 2024. The bibliometric analysis reveals the technical roadmapover time, as well as the five most influential leaders and the two largestresearch networks they have established. The qualitative review is thenconducted from four perspectives: influencing factor study, hazard leveldefinition and determination, proximity perception, and alarm issuing andreceiving. Finally, the limitations and challenges of current proximityperception are discussed, along with corresponding future research directions,including end-to-end three-dimensional (3D) object detection, real-time 3Dreconstruction and updating for dynamic construction scenes, and multimodalfusion. This review presents the current research status, limitations, andfuture directions of PMW, guiding the future development of PMW systems.</description>
      <author>example@mail.com (Yuexiong Ding, Qiong Liu, Ankang Ji, Xiaowei Luo, Wen Yi, Albert P. C. Chan)</author>
      <guid isPermaLink="false">2508.00862v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Distinct inhibitory connectivity motifs trigger distinct forms of anticipation in the retinal network</title>
      <link>http://arxiv.org/abs/2508.02436v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 Figures, 28 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了视网膜中两种不同的抑制性连接模式（前馈抑制和循环反馈抑制）如何影响视网膜对移动物体的响应，特别是运动预测机制及其速度依赖性差异。&lt;h4&gt;背景&lt;/h4&gt;运动是视觉场景的重要特征，视网膜神经元回路会选择性地传递不同的运动特征。视网膜可以外推移动物体的位置，从而补偿感觉传导延迟，实现实时信号处理。无长突细胞作为视网膜的抑制性中间神经元在这种计算中发挥重要作用，但其精确功能尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;探索两种不同的抑制性连接模式（前馈抑制和循环反馈抑制）对视网膜响应移动物体的影响，以及它们如何实现运动预测。&lt;h4&gt;方法&lt;/h4&gt;通过计算建模方法研究前馈抑制和循环反馈抑制对视网膜运动响应的影响。&lt;h4&gt;主要发现&lt;/h4&gt;两种机制都可以实现运动预测，但机制不同。前馈抑制通过减法抑制截断运动响应并将峰值响应前移；循环反馈耦合通过除法抑制引起不同相位的兴奋性和抑制性波，它们相加并使响应峰值偏移。两种机制的主要区别在于峰值响应如何随移动物体速度变化。使用前馈电路的运动预测随速度增加而单调下降，而循环反馈耦合则诱导出具有最优速度的调谐曲线，在该速度下运动预测最大。&lt;h4&gt;结论&lt;/h4&gt;视网膜中存在多种机制可以实现运动预测，前馈抑制和循环反馈抑制是两种不同的机制，它们在速度依赖性方面表现出显著差异。&lt;h4&gt;翻译&lt;/h4&gt;运动是视觉场景的重要特征，视网膜神经元回路会选择性地传递不同的运动特征。研究表明，视网膜可以外推移动物体的位置，从而补偿感觉传导延迟，实现实时信号处理。无长突细胞作为视网膜的抑制性中间神经元，在这种计算中发挥重要作用，但其精确功能仍不清楚。本文通过计算探索了两种不同的抑制性连接模式对视网膜响应移动物体的影响：前馈抑制和循环反馈抑制。我们证明，两种机制都可以通过不同的方式实现运动预测。前馈抑制通过减法抑制截断运动响应并将峰值响应前移，而循环反馈耦合通过除法抑制引起不同相位的兴奋性和抑制性波，它们相加并使响应峰值偏移。两种机制的主要区别在于峰值响应如何随移动物体速度变化。使用前馈电路的运动预测随速度增加而单调下降，而循环反馈耦合则诱导出具有最优速度的调谐曲线，在该速度下运动预测达到最大。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion is an important feature of visual scenes and retinal neuronal circuitsselectively signal different motion features. It has been shown that the retinacan extrapolate the position of a moving object, thereby compensating sensorytransmission delays and enabling signal processing in real-time. Amacrinecells, the inhibitory interneurons of the retina, play essential roles in suchcomputations although their precise function remain unclear. Here, wecomputationally explore the effect of two different inhibitory connectivitymotifs on the retina's response to moving objects: feed-forward and recurrentfeed-back inhibition. We show that both can account for motion anticipationwith two different mechanisms. Feed-forward inhibition truncates motionresponses and shifts peak responses forward via subtractive inhibition, whereasrecurrent feedback coupling evokes, via divisive inhibition, excitatory andinhibitory waves with different phases that add up and shift the response peak.A key difference between the two mechanisms is how the peak response scaleswith the speed of a moving object. Motion prediction with feedforward circuitsmonotonically decreases with increasing speeds, while recurrent feedbackcoupling induces tuning curves that exhibit a preferred speed for which motionprediction is maximal.</description>
      <author>example@mail.com (S. Ebert, B. Cessac)</author>
      <guid isPermaLink="false">2508.02436v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Text2Lip: Progressive Lip-Synced Talking Face Generation from Text via Viseme-Guided Rendering</title>
      <link>http://arxiv.org/abs/2508.02362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Text2Lip是一种以视素为中心的框架，通过将文本输入转换为结构化视素序列，构建语音-视觉桥梁，实现语义连贯且视觉准确的说话人脸生成，解决了现有音频驱动方法在可扩展性和鲁棒性方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;生成语义连贯且视觉准确的说话人脸需要弥合语言意义和面部发音之间的差距。基于音频驱动的方法虽然普遍，但它们依赖于高质量的成对视听数据，并且将声学映射到唇部运动存在固有歧义，这给可扩展性和鲁棒性带来了重大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有音频驱动方法在可扩展性和鲁棒性方面的问题，提出一种新的可控且灵活的说话人脸生成范式。&lt;h4&gt;方法&lt;/h4&gt;提出Text2Lip框架，将文本输入嵌入到结构化视素序列中构建可解释的语音-视觉桥梁；设计基于课程学习的渐进式视素-音频替换策略，使模型能从真实音频过渡到通过跨模态注意力从增强视素特征重建的伪音频；使用标志引导的渲染器合成具有精确唇部同步的逼真面部视频。&lt;h4&gt;主要发现&lt;/h4&gt;全面评估表明，Text2Lip在语义保真度、视觉真实性和模态鲁棒性方面优于现有方法，能够在有音频和无音频场景中都能鲁棒生成。&lt;h4&gt;结论&lt;/h4&gt;Text2Lip为可控且灵活的说话人脸生成建立了新的范式。&lt;h4&gt;翻译&lt;/h4&gt;生成语义连贯且视觉准确的说话人脸需要弥合语言意义和面部发音之间的差距。虽然基于音频驱动的方法仍然普遍，但它们依赖于高质量的成对视听数据，并且将声学映射到唇部运动存在固有歧义，这给可扩展性和鲁棒性带来了重大挑战。为解决这些问题，我们提出了Text2Lip，一个以视素为中心的框架，通过将文本输入嵌入到结构化的视素序列中，构建了一个可解释的语音-视觉桥梁。这些中间单元作为唇部运动预测的语言学基础先验。此外，我们设计了一种基于课程学习的渐进式视素-音频替换策略，使模型能够逐渐从真实音频过渡到通过跨模态注意力从增强的视素特征重建的伪音频。这使得模型能够在有音频和无音频场景中都能鲁棒生成。最后，一个由标志引导的渲染器合成具有精确唇部同步的逼真面部视频。广泛的评估表明，Text2Lip在语义保真度、视觉真实性和模态鲁棒性方面优于现有方法，为可控且灵活的说话人脸生成建立了新的范式。我们的项目主页是https://plyon1.github.io/Text2Lip/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating semantically coherent and visually accurate talking faces requiresbridging the gap between linguistic meaning and facial articulation. Althoughaudio-driven methods remain prevalent, their reliance on high-quality pairedaudio visual data and the inherent ambiguity in mapping acoustics to lip motionpose significant challenges in terms of scalability and robustness. To addressthese issues, we propose Text2Lip, a viseme-centric framework that constructsan interpretable phonetic-visual bridge by embedding textual input intostructured viseme sequences. These mid-level units serve as a linguisticallygrounded prior for lip motion prediction. Furthermore, we design a progressiveviseme-audio replacement strategy based on curriculum learning, enabling themodel to gradually transition from real audio to pseudo-audio reconstructedfrom enhanced viseme features via cross-modal attention. This allows for robustgeneration in both audio-present and audio-free scenarios. Finally, alandmark-guided renderer synthesizes photorealistic facial videos with accuratelip synchronization. Extensive evaluations show that Text2Lip outperformsexisting approaches in semantic fidelity, visual realism, and modalityrobustness, establishing a new paradigm for controllable and flexible talkingface generation. Our project homepage is https://plyon1.github.io/Text2Lip/.</description>
      <author>example@mail.com (Xu Wang, Shengeng Tang, Fei Wang, Lechao Cheng, Dan Guo, Feng Xue, Richang Hong)</author>
      <guid isPermaLink="false">2508.02362v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A Spatio-temporal Continuous Network for Stochastic 3D Human Motion Prediction</title>
      <link>http://arxiv.org/abs/2508.01585v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;STCN是一种创新的随机和连续人类运动预测方法，通过时空连续网络和锚集机制解决了现有方法的局限性，在多个数据集上表现出色&lt;h4&gt;背景&lt;/h4&gt;随机人类运动预测（HMP）因广泛应用而受到越来越多的关注。尽管生成领域取得了快速进展，但现有方法在学习连续时间动态和预测随机运动序列方面面临挑战。这些方法往往忽视了复杂人类运动中固有的灵活性，并且容易出现模式崩溃。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在学习连续时间动态和预测随机运动序列方面的挑战，解决方法忽视复杂人类运动中固有灵活性的问题，缓解模式崩溃问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为STCN的新方法，用于随机和连续的人类运动预测，包含两个阶段：第一阶段提出时空连续网络生成更平滑的人类运动序列，并创新地将锚集引入随机HMP任务以防止模式崩溃；第二阶段STCN借助锚集获取观测运动序列的高斯混合分布，关注每个锚相关的概率，并采用从每个锚采样多个序列的策略来缓解人类运动中的类内差异。&lt;h4&gt;主要发现&lt;/h4&gt;在两个广泛使用的数据集（Human3.6M和HumanEva-I）上的实验结果表明，该模型在多样性和准确性方面都获得了具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;STCN方法有效解决了随机人类运动预测中的连续时间动态学习和模式崩溃问题，通过引入锚集和采用多序列采样策略，提高了预测的多样性和准确性。&lt;h4&gt;翻译&lt;/h4&gt;随机人类运动预测（HMP）由于其广泛的应用而受到越来越多的关注。尽管生成领域取得了快速进展，但现有方法通常在学习连续时间动态和预测随机运动序列方面面临挑战。它们往往忽视了复杂人类运动中固有的灵活性，并且容易出现模式崩溃。为了缓解这些问题，我们提出了一种名为STCN的新方法，用于随机和连续的人类运动预测，它包含两个阶段。具体来说，在第一阶段，我们提出了一个时空连续网络来生成更平滑的人类运动序列。此外，创新地将锚集引入随机HMP任务以防止模式崩溃，锚集代表了潜在的人类运动模式。在第二阶段，STCN借助锚集获取观测运动序列的高斯混合分布（GMM）。它还关注与每个锚相关的概率，并采用从每个锚采样多个序列的策略来缓解人类运动中的类内差异。在两个广泛使用的数据集（Human3.6M和HumanEva-I）上的实验结果表明，我们的模型在多样性和准确性方面都获得了具有竞争力的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决随机3D人体运动预测中的两个关键问题：一是现有方法难以学习连续时间动态，无法灵活处理不同速度和频率的复杂人体运动；二是容易出现模式崩溃问题，忽略了运动中的多样性和类内差异。这个问题在现实中非常重要，因为人体运动预测在虚拟现实、人机交互、游戏开发等领域有广泛应用，而准确预测多种可能的未来运动对实现自然、安全的人机交互至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有随机人体运动预测方法的局限性，包括VAEs生成模糊运动、GANs容易模式崩溃、扩散模型训练复杂等问题。他们借鉴了神经ODE用于连续建模、VQVAE作为骨干网络、锚集概念用于聚类等现有技术，但创新性地将这些技术组合应用于人体运动预测。作者设计了两阶段方法：第一阶段学习连续表示和锚集，第二阶段预测随机运动序列，整体上通过时空连续网络和锚集机制来解决现有方法的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用时空连续网络捕捉人体运动的连续时间动态，引入锚集表示潜在运动模式防止模式崩溃，并通过高斯混合模型建模每个锚的分布，同时从每个锚采样多个序列来捕捉同一运动模式内的类内差异。整体流程分为两个阶段：第一阶段是人体运动重建，将输入序列编码到潜在空间，使用ODE网络建模连续动态，并通过K-means学习锚集；第二阶段是随机运动预测，将观察序列与锚集匹配，学习每个锚的概率和分布，最后从分布中采样多个序列生成多样化预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 基于ODE的时空连续网络，能捕捉连续时间动态提高预测平滑性；2) 创新引入锚集机制防止模式崩溃；3) 使用高斯混合模型建模每个锚的分布；4) 从每个锚采样多个序列缓解类内差异。相比之前工作，STCN解决了VAEs生成模糊、GANs模式崩溃、扩散模型训练复杂等问题，同时克服了离散时间模型不够灵活的局限，能够生成更平滑、更多样化的人体运动预测。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; STCN通过时空连续网络和锚集机制，有效解决了随机3D人体运动预测中的模式崩溃和连续动态建模问题，显著提高了预测序列的多样性和平滑性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stochastic Human Motion Prediction (HMP) has received increasing attentiondue to its wide applications. Despite the rapid progress in generative fields,existing methods often face challenges in learning continuous temporal dynamicsand predicting stochastic motion sequences. They tend to overlook theflexibility inherent in complex human motions and are prone to mode collapse.To alleviate these issues, we propose a novel method called STCN, forstochastic and continuous human motion prediction, which consists of twostages. Specifically, in the first stage, we propose a spatio-temporalcontinuous network to generate smoother human motion sequences. In addition,the anchor set is innovatively introduced into the stochastic HMP task toprevent mode collapse, which refers to the potential human motion patterns. Inthe second stage, STCN endeavors to acquire the Gaussian mixture distribution(GMM) of observed motion sequences with the aid of the anchor set. It alsofocuses on the probability associated with each anchor, and employs thestrategy of sampling multiple sequences from each anchor to alleviateintra-class differences in human motions. Experimental results on twowidely-used datasets (Human3.6M and HumanEva-I) demonstrate that our modelobtains competitive performance on both diversity and accuracy.</description>
      <author>example@mail.com (Hua Yu, Yaqing Hou, Xu Gui, Shanshan Feng, Dongsheng Zhou, Qiang Zhang)</author>
      <guid isPermaLink="false">2508.01585v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future</title>
      <link>http://arxiv.org/abs/2507.22792v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  45 pages, 21 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述文章提供了基于SAM/SAM2的视频对象分割和跟踪(VOST)方法的全面回顾，从过去、现在和未来三个时间维度分析了相关技术发展，指出了从早期基于内存的架构到SAM2的流式内存和实时分割能力的演变，讨论了最新的创新，并确定了剩余挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;视频对象分割和跟踪(VOST)在计算机视觉中是一个复杂而关键的挑战，需要稳健地整合跨时间动态帧的分割和跟踪功能。传统方法在领域泛化、时间一致性和计算效率方面存在困难。基础模型(如Segment Anything Model及其后续版本SAM2)的出现带来了范式转变，实现了具有强大泛化能力的提示驱动分割。&lt;h4&gt;目的&lt;/h4&gt;这篇综述旨在提供一个及时且结构化的VOST领域概述，通过基础模型的视角，指导研究人员和实践者推进VOST的发展状态。&lt;h4&gt;方法&lt;/h4&gt;综述沿着三个时间维度对SAM/SAM2方法进行了结构化分析：过去(保留和更新历史信息的策略)，现在(从当前帧提取和优化判别特征的方法)，未来(预测对象动态和轨迹估计的机制)。&lt;h4&gt;主要发现&lt;/h4&gt;从早期基于内存的架构发展到SAM2的流式内存和实时分割能力；最近的创新包括感知内存的选择和轨迹引导的提示，旨在提高准确性和效率；基础模型如SAM和SAM2为VOST带来了范式转变，实现了具有强大泛化能力的提示驱动分割。&lt;h4&gt;结论&lt;/h4&gt;剩余挑战包括内存冗余、错误累积和提示效率低下。未来研究有希望的方向包括优化内存使用、减少错误传播、提高提示效率等。&lt;h4&gt;翻译&lt;/h4&gt;视频对象分割和跟踪(VOST)在计算机视觉中提出了一个复杂而至关重要的挑战，需要稳健地整合跨时间动态帧的分割和跟踪。传统方法在领域泛化、时间一致性和计算效率方面一直存在困难。基础模型(如Segment Anything Model及其后续版本SAM2)的出现引入了范式转变，实现了具有强大泛化能力的提示驱动分割。基于这些进展，这篇综述对基于SAM/SAM2的VOST方法进行了全面回顾，沿着过去、现在和未来三个时间维度进行结构化分析。我们检查了保留和更新历史信息(过去)的策略，从当前帧提取和优化判别特征(现在)的方法，以及预测后续帧中对象动态和轨迹估计(未来)的机制。通过这样做，我们突显了从早期基于内存的架构到SAM2的流式内存和实时分割能力的演变。我们还讨论了最近的创新，如感知内存的选择和轨迹引导的提示，这些创新旨在提高准确性和效率。最后，我们确定了剩余的挑战，包括内存冗余、错误累积和提示效率低下，并建议了未来研究的有希望方向。这篇综述提供了该领域及时且结构化的概述，旨在通过基础模型的视角指导研究人员和实践者推进VOST的发展状态。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Object Segmentation and Tracking (VOST) presents a complex yet criticalchallenge in computer vision, requiring robust integration of segmentation andtracking across temporally dynamic frames. Traditional methods have struggledwith domain generalization, temporal consistency, and computational efficiency.The emergence of foundation models like the Segment Anything Model (SAM) andits successor, SAM2, has introduced a paradigm shift, enabling prompt-drivensegmentation with strong generalization capabilities. Building upon theseadvances, this survey provides a comprehensive review of SAM/SAM2-based methodsfor VOST, structured along three temporal dimensions: past, present, andfuture. We examine strategies for retaining and updating historical information(past), approaches for extracting and optimizing discriminative features fromthe current frame (present), and motion prediction and trajectory estimationmechanisms for anticipating object dynamics in subsequent frames (future). Indoing so, we highlight the evolution from early memory-based architectures tothe streaming memory and real-time segmentation capabilities of SAM2. We alsodiscuss recent innovations such as motion-aware memory selection andtrajectory-guided prompting, which aim to enhance both accuracy and efficiency.Finally, we identify remaining challenges including memory redundancy, erroraccumulation, and prompt inefficiency, and suggest promising directions forfuture research. This survey offers a timely and structured overview of thefield, aiming to guide researchers and practitioners in advancing the state ofVOST through the lens of foundation models.</description>
      <author>example@mail.com (Guoping Xu, Jayaram K. Udupa, Yajun Yu, Hua-Chieh Shao, Songlin Zhao, Wei Liu, You Zhang)</author>
      <guid isPermaLink="false">2507.22792v2</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Deep Multi-Task Learning in Connected Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2508.00917v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2303.01788,  arXiv:2304.01168 by other authors&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文是关于多任务学习(MTL)在连接自动驾驶汽车(CAVs)中应用的首次全面综述研究。CAVs需要同时执行多项任务以确保安全导航，而传统方法使用独立模型导致高成本和计算开销。MTL通过单一模型联合学习多个任务，提高了效率和资源利用率。&lt;h4&gt;背景&lt;/h4&gt;CAVs必须同时执行多项任务(如目标检测、语义分割、深度估计、轨迹预测等)以确保在复杂环境中的安全导航。V2X通信使CAVs能够协同驾驶，减轻单个传感器限制。传统方法使用不同模型处理不同任务，导致高部署成本、增加计算开销和实时性能挑战。&lt;h4&gt;目的&lt;/h4&gt;提供首个专注于CAVs背景下MTL的全面综述，探讨MTL在关键功能模块中的应用，讨论现有方法的优缺点，确定研究差距，并提供未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;本文首先概述CAVs和MTL提供基础背景，然后探索MTL在关键功能模块(包括感知、预测、规划、控制和多智能体协作)中的应用，最后讨论现有方法的优缺点并确定未来研究方向。&lt;h4&gt;主要发现&lt;/h4&gt;MTL作为一种有前景的解决方案，可以在单一统一模型中联合学习多个任务，提高了效率和资源利用率。V2X通信使CAVs能够协同驾驶，减轻了单个传感器的限制，减少了遮挡，并提高了长距离感知能力。&lt;h4&gt;结论&lt;/h4&gt;MTL为CAVs系统提供了更高效和资源优化的解决方案，但仍有研究需要开展以进一步改进MTL方法。未来的研究方向应集中在解决现有方法的局限性，填补研究空白，并推进CAV系统的MTL方法论。&lt;h4&gt;翻译&lt;/h4&gt;连接自动驾驶汽车(CAVs)必须同时执行多项任务，如目标检测、语义分割、深度估计、轨迹预测、运动预测和行为预测，以确保在复杂环境中的安全导航。车对万物(V2X)通信使CAVs能够实现协同驾驶，从而减轻单个传感器的限制，减少遮挡，并提高长距离感知能力。传统上，这些任务使用不同的模型来解决，这导致高部署成本、增加的计算开销以及实现实时性能的挑战。多任务学习(MTL)最近出现了一种有前景的解决方案，它可以在单一统一模型中实现多个任务的联合学习。这提高了效率和资源利用率。据我们所知，这是首个专注于CAVs背景下MTL的全面综述。我们从概述CAVs和MTL开始，以提供基础背景。然后，我们探讨了MTL在关键功能模块中的应用，包括感知、预测、规划、控制和多智能体协作。最后，我们讨论了现有方法的优缺点，确定了关键研究差距，并提供了旨在推进CAV系统MTL方法论的未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Connected autonomous vehicles (CAVs) must simultaneously perform multipletasks, such as object detection, semantic segmentation, depth estimation,trajectory prediction, motion prediction, and behaviour prediction, to ensuresafe and reliable navigation in complex environments. Vehicle-to-everything(V2X) communication enables cooperative driving among CAVs, thereby mitigatingthe limitations of individual sensors, reducing occlusions, and improvingperception over long distances. Traditionally, these tasks are addressed usingdistinct models, which leads to high deployment costs, increased computationaloverhead, and challenges in achieving real-time performance. Multi-tasklearning (MTL) has recently emerged as a promising solution that enables thejoint learning of multiple tasks within a single unified model. This offersimproved efficiency and resource utilization. To the best of our knowledge,this survey is the first comprehensive review focused on MTL in the context ofCAVs. We begin with an overview of CAVs and MTL to provide foundationalbackground. We then explore the application of MTL across key functionalmodules, including perception, prediction, planning, control, and multi-agentcollaboration. Finally, we discuss the strengths and limitations of existingmethods, identify key research gaps, and provide directions for future researchaimed at advancing MTL methodologies for CAV systems.</description>
      <author>example@mail.com (Jiayuan Wang, Farhad Pourpanah, Q. M. Jonathan Wu, Ning Zhang)</author>
      <guid isPermaLink="false">2508.00917v1</guid>
      <pubDate>Tue, 05 Aug 2025 15:32:54 +0800</pubDate>
    </item>
    <item>
      <title>Amortized Clustering Assistant Classification of Anomalous Hybrid Floquet Modes in a Periodically Driven non-Hermitian Lattice</title>
      <link>http://arxiv.org/abs/2508.00571v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了Floquet周期驱动与非厄米性相互作用在有限尺寸紧束缚晶格模型中产生的异常Floquet拓扑相现象。作者为双晶格系统设计了复杂驱动协议，发现了支持Floquet π模的两个非平凡拓扑相，并引入无监督学习方法分析系统本征函数在不同能量增益/损失条件下的分布特征。通过构建能动态升级的算法选择器，研究揭示了二维晶格中动态局域化的调控机制，为Floquet模的分类提供了基于机器学习的可行方法论。&lt;h4&gt;背景&lt;/h4&gt;Floquet周期驱动与非厄米性的相互作用可以产生有趣的异常Floquet拓扑相现象。然而，如何有效研究具有复杂驱动协议的非厄米Floquet系统的准能量和本征场仍然是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;研究具有复杂驱动协议的双晶格系统中的Floquet拓扑相，探索系统本征函数在不同能量增益/损失条件下的分布特征，揭示动态局域化的调控机制，并提供一种通过机器学习方法辅助Floquet模分类的可行方法论。&lt;h4&gt;方法&lt;/h4&gt;定义复杂的双晶格系统驱动协议；引入无监督学习方法探索系统本征函数在不同能量增益/损失幅度下的分布特征；利用分摊聚类思想构建能随输入参数增加而动态升级的算法选择器；通过机器学习方法对Floquet模进行分类。&lt;h4&gt;主要发现&lt;/h4&gt;发现了支持Floquet π模的两个非平凡拓扑相；算法选择器的适当应用能够从二维晶格中丰富的波函数分布中有效地揭示动态局域化的调控机制；提供了一种通过机器学习方法辅助Floquet模分类的可行方法论。&lt;h4&gt;结论&lt;/h4&gt;本研究通过结合Floquet拓扑相理论与机器学习方法，有效地揭示了复杂驱动协议下非厄米Floquet系统的动态局域化调控机制，为Floquet模的分类提供了一种新的可行途径。&lt;h4&gt;翻译&lt;/h4&gt;Floquet周期驱动与非厄米性之间的相互作用可以在有限尺寸的紧束缚晶格模型中带来有趣的异常Floquet拓扑相现象。如何有效地研究具有复杂驱动协议的非厄米Floquet系统的准能量和本征场仍然是一个具有挑战性的任务。在这项工作中，我们为双晶格系统定义了一种复杂的驱动协议，并发现了支持Floquet π模的两个非平凡拓扑相。随后，我们引入无监督学习方法来探索系统本征函数在不同能量增益/损失幅度下的分布特征。我们利用分摊聚类的思想构建了一个算法选择器，该选择器可以随着增益/损失作为输入参数的增加而动态升级。选择器的适当应用使我们能够以另一种高效的方式从二维晶格中丰富的波函数分布中揭示动态局域化的调控机制。此外，我们的工作通过机器学习方法提供了一种可行的辅助Floquet模分类的方法论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The interplay between Floquet periodically driving and non-Hermiticity couldbring about intriguing novel phenomena with anomalous Floquet topologicalphases of a finite-size, tight-binding lattice model. How to efficientlyinvestigate on quasi-energy and eigenfield of a non-Hermitian Floquet systemwith complicated driving protocol remains a challenging task. In this work, wedefine a somewhat complex driving protocol for a bipartite lattice system anddiscover two nontrivial topological phases that support Floquet {\pi} mode.Thereafter, we introduce unsupervised learning method in order to exploredistribution features of system eigenfunctions under different magnitude ofsystem energy gain/loss. We utilize the idea of amortized clustering andconstruct an algorithm selector that could dynamically upgrade with increasinggain/loss as input parameter. Proper employment of the selector enables us toreveal the regulation of dynamic localization from abundant possible wavefunction distribution in two-dimension lattice in another efficient way. Inaddition, our work provides a feasible methodology via machine learning methodto assist in classification of Floquet modes.</description>
      <author>example@mail.com (Yifei Xia, Xiumei Wang, Yali Li, Xingping Zhou)</author>
      <guid isPermaLink="false">2508.00571v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
  <item>
      <title>Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning</title>
      <link>http://arxiv.org/abs/2508.00822v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究分析了异构标记点云数据集上的语义分割性能，这些数据集与公共安全应用相关。研究使用NIST点云城市数据集，采用分级模式和KPConv架构，通过IoU指标评估性能。研究发现大型物体分割性能较高，而小型安全关键特征识别率较低，主要受类别不平衡和物体几何区分度有限的影响。研究指出当前方法在检测某些安全特征方面存在局限性，并提出需要标准化注释协议和改进标记技术。&lt;h4&gt;背景&lt;/h4&gt;研究针对公共安全应用中的点云语义分割，包括基于激光雷达扫描的预先事件规划系统。面临的主要挑战是如何统一不同标记的3D数据集。&lt;h4&gt;目的&lt;/h4&gt;研究异构标记点云数据集上的语义分割性能，评估统一不同标记3D数据所面临的挑战，以及点云语义分割在公共安全应用中的局限性。&lt;h4&gt;方法&lt;/h4&gt;使用NIST的点云城市数据集（Enfield和Memphis收藏），采用分级模式和KPConv架构，通过IoU指标评估与安全相关特征的性能。&lt;h4&gt;主要发现&lt;/h4&gt;几何上较大的物体（如楼梯、窗户）实现了较高的分割性能，可能用于导航上下文；较小的安全关键特征识别率较低。性能受到类别不平衡和典型激光雷达扫描中小物体几何区分度有限的影响，表明当前点云方法在检测某些安全相关特征方面存在局限性。&lt;h4&gt;结论&lt;/h4&gt;公共安全可靠的点云语义分割需要标准化的注释协议和改进的标记技术，以解决数据异质性和检测小安全关键元素的问题。&lt;h4&gt;翻译&lt;/h4&gt;本研究分析了与公共安全应用相关的异构标记点云数据集上的语义分割性能，包括从激光雷达扫描中获得的预先事件规划系统。使用NIST的点云城市数据集（Enfield和Memphis收藏），我们研究了统一不同标记的3D数据所面临的挑战。我们的方法采用分级模式和KPConv架构，通过IoU指标评估与安全相关特征的性能。结果表明性能存在变异性：几何上较大的物体（如楼梯、窗户）实现了较高的分割性能，表明可能用于导航上下文，而较小的安全关键特征表现出较低的识别率。性能受到类别不平衡和典型激光雷达扫描中小物体几何区分度有限的影响，表明当前点云方法在检测某些安全相关特征方面存在局限性。确定的主要挑战包括标记数据不足、难以统一不同数据集中的类别标签以及标准化的需求。潜在方向包括自动化标记和多数据集学习策略。我们得出结论，公共安全可靠的点云语义分割需要标准化的注释协议和改进的标记技术，以解决数据异质性和检测小安全关键元素的问题。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决不同标注方式下的点云数据集整合问题，特别是在公共安全应用场景中如何统一NIST点云城市数据集用于3D深度学习。这个问题在现实中非常重要，因为公共安全应用（如紧急响应预案系统）需要可靠的3D点云语义分割来识别安全关键特征，而现有点云数据集通常缺乏针对公共安全应用的专门标注，不同数据集使用不同的标注方法导致难以整合利用，限制了深度学习模型在公共安全领域的应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到公共安全领域缺乏统一的点云数据集和标注标准，意识到不同数据集（Enfield和Memphis）虽然都关注公共安全特征但使用了不同的标注方法。他们借鉴了现有工作，包括使用SemanticKITTI数据集结构作为转换目标格式，采用KPConv（可变形卷积）架构直接处理点云数据，利用Open3D-ML软件管道处理数据，并参考了Point Prompt Training等新兴方法。作者设计了一种统一的标注模式（graded schema）和转换管道来解决数据异质性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的标注模式将不同数据集的异构标注整合为一致的语义类别，开发转换管道将NIST点云城市数据集转换为与SemanticKITTI兼容的格式，并评估不同训练配置下模型的性能。整体流程包括：1) 数据预处理和格式统一，选择Enfield和Memphis数据集并转换为SemanticKITTI格式；2) 开发PCC-SKITTI映射为语义类分配一致标识符；3) 使用Open3D-ML实现KPConv架构；4) 设置三种训练配置（合并数据集、仅Enfield、仅Memphis）；5) 使用IoU和准确率指标评估性能，分析几何特征与性能的关系及类别不平衡的影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 系统分析公共安全导向点云数据集的跨数据集语义分割性能；2) 提出统一异构标注数据集的方法并保留语义意义；3) 提供不同标注方法对模型性能影响的定量见解。相比之前工作，本研究专注于公共安全应用而非通用场景，使用专门为公共安全设计的NIST点云城市数据集，直接解决跨数据集标注不一致问题，分析了几何特征与分割性能的关系，并提出了为公共安全领域点云数据集标准化提供基础的统一框架（PCC-SKITTI）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过开发统一异构标注点云数据集的方法并系统评估跨数据集语义分割性能，为公共安全应用中的3D场景理解提供了重要框架和实用见解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study analyzes semantic segmentation performance across heterogeneouslylabeled point-cloud datasets relevant to public safety applications, includingpre-incident planning systems derived from lidar scans. Using NIST's PointCloud City dataset (Enfield and Memphis collections), we investigate challengesin unifying differently labeled 3D data. Our methodology employs a gradedschema with the KPConv architecture, evaluating performance through IoU metricson safety-relevant features. Results indicate performance variability:geometrically large objects (e.g. stairs, windows) achieve higher segmentationperformance, suggesting potential for navigational context, while smallersafety-critical features exhibit lower recognition rates. Performance isimpacted by class imbalance and the limited geometric distinction of smallerobjects in typical lidar scans, indicating limitations in detecting certainsafety-relevant features using current point-cloud methods. Key identifiedchallenges include insufficient labeled data, difficulties in unifying classlabels across datasets, and the need for standardization. Potential directionsinclude automated labeling and multi-dataset learning strategies. We concludethat reliable point-cloud semantic segmentation for public safety necessitatesstandardized annotation protocols and improved labeling techniques to addressdata heterogeneity and the detection of small, safety-critical elements.</description>
      <author>example@mail.com (Alexander Nikitas Dimopoulos, Joseph Grasso)</author>
      <guid isPermaLink="false">2508.00822v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR</title>
      <link>http://arxiv.org/abs/2508.00744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted at the Embedded Vision Workshop ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Dense Backbone的轻量级主干网络，专为基于点云的3D目标检测设计，能够在保持高检测性能的同时显著降低模型复杂度和计算成本。&lt;h4&gt;背景&lt;/h4&gt;基于LiDAR的3D目标检测技术已取得显著进展，推动了自动驾驶的实现。然而，现有方法大多仍依赖VGG或ResNet作为主干网络，增加了模型复杂度。轻量级主干设计在2D目标检测中研究较多，但在3D目标检测领域研究仍有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合高处理速度、轻量级架构和稳健检测准确性的轻量级主干网络，用于3D目标检测。&lt;h4&gt;方法&lt;/h4&gt;提出Dense Backbone，一种密集层主干网络，适配了多个最先进的3D目标检测器（如PillarNet）。该设计为即插即用，可轻松集成到现有架构中，无需修改其他网络组件。&lt;h4&gt;主要发现&lt;/h4&gt;DensePillarNet在nuScenes测试集上实现了模型参数减少29%，延迟降低28%，而检测精度仅下降2%。使用Dense Backbone的模型在显著降低计算成本的情况下保留了大部分检测能力。&lt;h4&gt;结论&lt;/h4&gt;Dense Backbone是首个专门为点云数据3D目标检测设计的密集层主干网络，能够在保持高检测性能的同时显著减少模型复杂度和计算成本。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的3D目标检测的最新进展显著加速了在现实环境中实现完全自动驾驶的进程。尽管取得了高检测性能，但大多数方法仍依赖于基于VGG或ResNet的主干网络进行特征探索，这增加了模型复杂度。轻量级主干设计在2D目标检测中已被充分研究，但在3D目标检测方面的研究仍然有限。在这项工作中，我们引入了Dense Backbone，一种轻量级主干网络，结合了高处理速度、轻量级架构和稳健检测准确性的优势。我们适配了多个最先进的3D目标检测器（如PillarNet）与我们的主干网络，并表明使用我们的主干网络，这些模型在显著降低计算成本的情况下保留了大部分检测能力。据我们所知，这是首个专门为点云数据3D目标检测设计的密集层主干网络。我们的PillarNet适配版本DensePillarNet在nuScenes测试集上实现了模型参数减少29%，延迟降低28%，而检测精度仅下降2%。此外，Dense Backbone的即插即用设计允许轻松集成到现有架构中，无需修改其他网络组件。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的问题是当前基于LiDAR的3D目标检测模型大多依赖VGG或ResNet等复杂骨干网络，导致模型计算量大、难以在资源有限的自动驾驶车辆上实时运行。这个问题在现实中非常重要，因为自动驾驶车辆需要在边缘设备上实时执行多项任务（如目标检测、路径规划等），而车载计算资源有限，无法部署大型模型；同时将计算卸载到云端又受网络限制且存在安全隐私问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到点云数据本质上是稀疏且缺乏结构的，因此高效的特征重用对于从点云学习表示至关重要。他们认为现有模型检测精度下降的主要原因在于使用了未针对点云数据优化的图像检测骨干网络。作者借鉴了DenseNet的特征重用思想、PeleeNet的双向密集层设计以及VovNet的一次性聚合策略，设计了专门针对点云数据的密集连接骨干网络。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过密集连接和高效的特征重用，在减少模型参数和计算成本的同时保持检测精度。整体实现流程包括：1) Dense Block作为基本构建块，使用一系列卷积层后进行特征连接；2) Transition Layer作为密集块间的中间层，通过点卷积聚合特征并加入平均池化；3) 采用渐进式增长率策略，初始增长率为32，后续每个密集块中翻倍；4) 设计即插即用架构，可无缝集成到现有3D目标检测框架中。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了首个专为LiDAR点云3D目标设计的密集层骨干网络；2) 通过密集连接和一次性聚合实现高效特征重用；3) 设计了完全即插即用的骨干网络，无需修改其他组件；4) 在多种现有模型上验证了有效性。相比之前的工作，不同之处在于：不使用通用图像检测骨干网络（如ResNet），而是专门针对点云稀疏特性设计；与需要修改编码器的其他方法不同，Dense Backbone完全即插即用；通过渐进式增长率策略解决了传统密集网络内存访问成本高的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Dense Backbone是一种专为LiDAR点云3D目标检测设计的轻量级、即插即用的密集层骨干网络，通过高效特征重用显著减少了模型参数和计算成本，同时保持了 competitive的检测精度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in LiDAR-based 3D object detection have significantlyaccelerated progress toward the realization of fully autonomous driving inreal-world environments. Despite achieving high detection performance, most ofthe approaches still rely on a VGG-based or ResNet-based backbone for featureexploration, which increases the model complexity. Lightweight backbone designis well-explored for 2D object detection, but research on 3D object detectionstill remains limited. In this work, we introduce Dense Backbone, a lightweightbackbone that combines the benefits of high processing speed, lightweightarchitecture, and robust detection accuracy. We adapt multiple SoTA 3d objectdetectors, such as PillarNet, with our backbone and show that with ourbackbone, these models retain most of their detection capability at asignificantly reduced computational cost. To our knowledge, this is the firstdense-layer-based backbone tailored specifically for 3D object detection frompoint cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%reduction in model parameters and a 28% reduction in latency with just a 2%drop in detection accuracy on the nuScenes test set. Furthermore, DenseBackbone's plug-and-play design allows straightforward integration intoexisting architectures, requiring no modifications to other network components.</description>
      <author>example@mail.com (Adwait Chandorkar, Hasan Tercan, Tobias Meisen)</author>
      <guid isPermaLink="false">2508.00744v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints</title>
      <link>http://arxiv.org/abs/2508.00558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at the IEEE/CVF International Conference on  Computer Vision (ICCV), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出PhysNAP，一种基于扩散模型的新方法，用于生成与部分点云对齐且物理合理的铰接对象。&lt;h4&gt;背景&lt;/h4&gt;铰接对象是日常生活中重要的可交互对象类型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够与部分点云对齐并提高物理合理性的铰接对象生成方法。&lt;h4&gt;方法&lt;/h4&gt;使用符号距离函数表示部分形状；通过点云对齐损失引导反向扩散过程；施加非穿透性和移动性约束；使方法具有类别感知能力以提高点云对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在PartNet-Mobility数据集上评估显示，PhysNAP能提高约束一致性，并在生成能力方面与无引导基线模型提供权衡。&lt;h4&gt;结论&lt;/h4&gt;PhysNAP是一种有效的扩散模型方法，能够生成与部分点云对齐且物理合理的铰接对象。&lt;h4&gt;翻译&lt;/h4&gt;铰接对象是日常生活中重要的可交互对象类型。在本文中，我们提出PhysNAP，一种基于扩散模型的新方法，用于生成与部分点云对齐并提高其物理合理性的铰接对象。该模型使用符号距离函数表示部分形状。我们使用预测的SDFs计算点云对齐损失来引导反向扩散过程。此外，我们基于部分SDFs施加非穿透性和移动性约束，引导模型生成更物理合理的对象。我们还使扩散方法具有类别感知能力，如果在有类别信息的情况下可进一步提高点云对齐。我们使用PartNet-Mobility数据集评估了使用PhysNAP生成的样本的生成能力和约束一致性。我们还将其与无引导的基线扩散模型进行比较，证明PhysNAP可以提高约束一致性，并在生成能力方面提供权衡。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成符合物理规律且能与部分点云对齐的关节式物体问题。这个问题在虚拟现实应用和机器人技术中非常重要，因为这些应用需要创建日常物品（如抽屉、电器、笔记本电脑等）的数字孪生体。生成高质量的关节式物体对于构建交互式环境、物体识别和机器人操作等任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于NAP扩散模型进行扩展，借鉴了PhysPart的物理约束思想。他们设计了一个训练自由引导框架，在反向扩散过程中添加点云对齐和物理可行性约束。作者考虑了关节式物体的复杂性，设计了多种可微损失函数，并确保这些损失能够调整物体的各种属性，如位置、形状和关节参数。他们还引入了类别条件信息，以提高点云对齐的准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过扩散模型生成关节式物体，同时确保生成的物体符合物理规律（无部分穿透）且能与输入的部分点云对齐。整体流程包括：1) 使用预训练SDF表示部分形状；2) 前向扩散添加噪声，反向扩散逐步重建；3) 在反向扩散最后阶段添加引导损失（点云对齐、穿透损失和移动性损失）；4) 后处理提取网格和确定关节连接；5) 评估生成的物体质量和对齐程度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) PhysNAP方法首次将扩散模型与点云对齐和物理约束结合，无需预知关节结构；2) 设计了基于SDF的可微点云对齐损失；3) 引入非穿透和移动性物理约束；4) 实现类别感知生成。相比之前工作，PhysNAP不需要完整的点云输入（不同于PhysPart），不需要预知关节结构（不同于CAGE），同时生成关节图和形状（不同于MIDGaRD），并考虑了所有部分之间的穿透问题（不同于PhysPart只考虑基物体与单个部分）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PhysNAP是一种创新的扩散模型方法，通过点云对齐和物理可行性约束引导，能够生成符合物理规律且与部分点云对齐的关节式物体，无需预先知道关节图结构。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Articulated objects are an important type of interactable objects in everydayenvironments. In this paper, we propose PhysNAP, a novel diffusion model-basedapproach for generating articulated objects that aligns them with partial pointclouds and improves their physical plausibility. The model represents partshapes by signed distance functions (SDFs). We guide the reverse diffusionprocess using a point cloud alignment loss computed using the predicted SDFs.Additionally, we impose non-penetration and mobility constraints based on thepart SDFs for guiding the model to generate more physically plausible objects.We also make our diffusion approach category-aware to further improve pointcloud alignment if category information is available. We evaluate thegenerative ability and constraint consistency of samples generated with PhysNAPusing the PartNet-Mobility dataset. We also compare it with an unguidedbaseline diffusion model and demonstrate that PhysNAP can improve constraintconsistency and provides a tradeoff with generative ability.</description>
      <author>example@mail.com (Jens U. Kreber, Joerg Stueckler)</author>
      <guid isPermaLink="false">2508.00558v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.00473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种用于3D点云视频异常检测的新型双曲时空变换器HyPCV-Former，通过在洛伦兹双曲空间中嵌入特征和使用双曲多头自注意力机制，有效捕获了事件的分层结构和时空连续性，实验证明其在多个数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;视频异常检测是视频监控中的基本任务，在公共安全和智能监控系统中有广泛应用。之前的方法利用RGB或深度域中的欧几里得表示，但这些嵌入在捕获分层事件结构和时空连续性方面存在固有局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在捕获分层事件结构和时空连续性方面的局限性，提出一种新的用于3D点云视频异常检测的双曲时空变换器。&lt;h4&gt;方法&lt;/h4&gt;提出HyPCV-Former，首先通过点云提取器从点云序列中提取每帧空间特征，然后将这些特征嵌入到洛伦兹双曲空间中。引入双曲多头自注意力(HMHA)机制来建模时间动态，利用洛伦兹内积和曲率感知的softmax在非欧几里得几何下学习时间依赖性。方法在完整的洛伦兹空间内直接执行所有特征变换和异常评分，而不是通过切线空间近似。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，HyPCV-Former在多个异常类别上达到了最先进的性能，在TIMo数据集上比基准方法提高了7%，在DAD数据集上提高了5.6%。&lt;h4&gt;结论&lt;/h4&gt;HyPCV-Former是一种有效的视频异常检测方法，特别是在处理3D点云视频时。代码将在论文接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;视频异常检测是视频监控中的一个基本任务，在公共安全和智能监控系统中有广泛的应用。尽管先前的方法利用RGB或深度域中的欧几里得表示，但这类嵌入本质上在捕获分层事件结构和时空连续性方面存在局限性。为了解决这些局限性，我们提出了HyPCV-Former，一种用于3D点云视频异常检测的新型双曲时空变换器。我们的方法首先通过点云提取器从点云序列中提取每帧空间特征，然后将它们嵌入到洛伦兹双曲空间中，这更好地捕获了事件的潜在分层结构。为了建模时间动态，我们引入了一种双曲多头自注意力(HMHA)机制，该机制利用洛伦兹内积和曲率感知的softmax，在非欧几里得几何下学习时间依赖性。我们的方法在完整的洛伦兹空间内直接执行所有特征变换和异常评分，而不是通过切线空间近似。大量实验证明，HyPCV-Former在多个异常类别上实现了最先进的性能，在TIMo数据集上比基准方法提高了7%，在DAD数据集上提高了5.6%。代码将在论文接受后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云视频中的异常检测问题。这个问题在现实和研究中非常重要，因为视频异常检测是视频监控的基础任务，在公共安全和智能监控系统中有着广泛应用。现有的方法主要在欧几里得空间处理RGB或深度图像，难以有效捕捉异常事件的层次结构和时空连续性。而3D点云能提供准确的3D空间信息，同时保护个人隐私，是一种很有前景的数据表示方式。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3D点云数据的优势，能够提供精确的几何信息同时保护隐私。然后发现现有方法在欧几里得空间处理层次结构数据存在局限，而超几何空间特别适合表示层次结构。作者借鉴了PointNet等点云处理方法用于特征提取，Transformer架构用于处理序列数据，以及超几何学习中的Lorentz模型。创新性地将这些技术结合，设计出完全在超几何空间中操作的模型，而不是像之前工作那样通过切线空间近似。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用超几何空间更好地表示层次事件结构，设计超几何多头自注意力机制建模时间依赖关系，并在完整的超几何空间中执行所有特征变换和异常评分。整体流程分为四个阶段：首先使用PointNet等点云提取器提取每帧空间特征；然后将特征嵌入到Lorentzian超几何空间；接着使用超几何时空Transformer建模时空依赖关系；最后使用Lorentzian内距离作为异常评分，直接在超几何空间中计算。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出超几何时空Transformer用于3D点云视频异常检测；引入超几何多头自注意力机制利用Lorentzian内积和曲率感知softmax；在完整Lorentzian空间中执行所有特征变换和异常评分；首次将超几何几何应用于3D点云视频异常检测。相比之前工作，不同之处在于：使用超几何空间而非欧几里得空间；直接在超几何空间操作而非切线空间近似；处理3D点云数据而非RGB或深度图像；引入专门设计的超几何多头自注意力机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了HyPCV-Former，一种基于超几何时空Transformer的3D点云视频异常检测方法，通过在Lorentzian超几何空间中直接操作特征，有效捕获了层次事件结构，实现了比现有方法更优的异常检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video anomaly detection is a fundamental task in video surveillance, withbroad applications in public safety and intelligent monitoring systems.Although previous methods leverage Euclidean representations in RGB or depthdomains, such embeddings are inherently limited in capturing hierarchical eventstructures and spatio-temporal continuity. To address these limitations, wepropose HyPCV-Former, a novel hyperbolic spatio-temporal transformer foranomaly detection in 3D point cloud videos. Our approach first extractsper-frame spatial features from point cloud sequences via point cloudextractor, and then embeds them into Lorentzian hyperbolic space, which bettercaptures the latent hierarchical structure of events. To model temporaldynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanismthat leverages Lorentzian inner products and curvature-aware softmax to learntemporal dependencies under non-Euclidean geometry. Our method performs allfeature transformations and anomaly scoring directly within full Lorentzianspace rather than via tangent space approximation. Extensive experimentsdemonstrate that HyPCV-Former achieves state-of-the-art performance acrossmultiple anomaly categories, with a 7\% improvement on the TIMo dataset and a5.6\% gain on the DAD dataset compared to benchmarks. The code will be releasedupon paper acceptance.</description>
      <author>example@mail.com (Jiaping Cao, Kangkang Zhou, Juan Du)</author>
      <guid isPermaLink="false">2508.00473v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2508.00259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PointGauss的新颖点云引导框架，用于高斯泼溅表示中的实时多目标分割，以及一个名为DesktopObjects-360的全面3D分割数据集。&lt;h4&gt;背景&lt;/h4&gt;现有方法存在初始化时间长和多视角一致性有限的问题，当前基准测试也有局限性，如只关注单个对象、3D评估不一致、规模小和覆盖不全面。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的3D分割方法，解决现有方法的局限性，并提供一个全面的3D分割数据集。&lt;h4&gt;方法&lt;/h4&gt;提出两个关键创新：(1) 基于点云的高斯基元解码器，可在1分钟内生成3D实例掩码；(2) GPU加速的2D掩码渲染系统，确保多视角一致性。同时提出DesktopObjects-360数据集，具有复杂多对象场景、全局一致的2D注释、大规模训练数据、完整的360度覆盖和3D评估掩码。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与之前的最先进方法相比有显著改进，在多视角mIoU上实现了1.89%到31.78%的性能提升，同时保持卓越的计算效率。&lt;h4&gt;结论&lt;/h4&gt;PointGauss框架有效解决了现有方法在初始化时间和多视角一致性方面的局限性，而DesktopObjects-360数据集解决了当前基准测试的局限性，为3D分割研究提供了更全面的资源。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了PointGauss，一种新颖的点云引导框架，用于高斯泼溅表示中的实时多目标分割。与现有方法相比，这些方法存在初始化时间长和多视角一致性有限的问题，我们的方法通过点云分割驱动的管道直接解析高斯基元，实现了高效的3D分割。关键创新在于两个方面：(1) 基于点云的高斯基元解码器，可在1分钟内生成3D实例掩码，以及(2) GPU加速的2D掩码渲染系统，确保多视角一致性。大量实验表明，与之前的最先进方法相比有显著改进，在多视角mIoU上实现了1.89%到31.78%的性能提升，同时保持卓越的计算效率。为了解决当前基准测试的局限性（单一对象关注、不一致的3D评估、规模小和部分覆盖），我们提出了DesktopObjects-360，这是一个用于辐射场中3D分割的新型综合数据集，具有以下特点：(1) 复杂的多对象场景，(2) 全局一致的2D注释，(3) 大规模训练数据（超过27千个2D掩码），(4) 完整的360度覆盖，以及(5) 3D评估掩码。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D高斯溅射表示中的多目标分割问题，具体包括现有方法初始化时间过长（通常需要32-45分钟）和多视角一致性不足的问题。这个问题在现实中非常重要，因为3D场景分割是实现3D环境理解和交互的关键步骤，对于增强现实、自动驾驶、机器人导航等领域至关重要。现有方法无法满足实时交互需求，限制了这些技术的实际应用，且多目标分割能力对于复杂场景的理解必不可少。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性（3D几何利用不足、架构复杂、依赖2D模型）发现可以直接操作高斯原始体实现3D分割。他们借鉴了点云分割技术（特别是PointTransformerV3作为骨干网络）和交互式分割思想，允许用户通过点击指定分割目标。同时利用了3D高斯溅射的渲染机制，但创新性地将其用于分割而非仅用于渲染。整体设计思路是绕过2D转换步骤，直接在3D空间中进行分割，从而提高效率并保证多视角一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过点云分割模型直接对3D高斯原始体进行分割，充分利用3D几何结构实现快速初始化和多视角一致的分割结果。整体流程分为三部分：1) 提示编码器：将用户点击转换为3D参考点，计算每个高斯原始体的空间相关性权重，并将其与原始属性结合；2) 高斯解码器：通过区域裁剪、自适应批处理、PointTransformerV3网络分类和实例标签分配四阶段处理，生成3D实例标签；3) 溅射投影：将3D标签投影到2D视图生成分割掩码，并应用后处理优化质量。整个流程在1分钟内完成3D实例分割，并能实时渲染多视角一致的分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 点云引导的高斯原始体解码器，直接在3D空间分割，1分钟内生成掩码，比现有方法快200-300倍；2) GPU加速的2D掩码渲染系统，确保多视角一致性；3) DesktopObjects-360数据集，提供复杂多目标场景、全局一致的2D标注、大规模训练数据、完整360°覆盖和3D评估掩码。相比之前工作，PointGauss简化了架构（消除多阶段蒸馏），显著提高了效率（初始化从32-45分钟减至8秒），提升了性能（多视角mIoU提高1.89-31.78%），并专门针对多目标场景设计，能更好处理复杂场景中的遮挡和交互。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointGauss通过直接操作3D高斯原始体实现了快速、多视角一致的多目标分割，显著提高了分割效率和准确性，并提供了新的基准数据集推动领域发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce PointGauss, a novel point cloud-guided framework for real-timemulti-object segmentation in Gaussian Splatting representations. Unlikeexisting methods that suffer from prolonged initialization and limitedmulti-view consistency, our approach achieves efficient 3D segmentation bydirectly parsing Gaussian primitives through a point cloud segmentation-drivenpipeline. The key innovation lies in two aspects: (1) a point cloud-basedGaussian primitive decoder that generates 3D instance masks within 1 minute,and (2) a GPU-accelerated 2D mask rendering system that ensures multi-viewconsistency. Extensive experiments demonstrate significant improvements overprevious state-of-the-art methods, achieving performance gains of 1.89 to31.78% in multi-view mIoU, while maintaining superior computational efficiency.To address the limitations of current benchmarks (single-object focus,inconsistent 3D evaluation, small scale, and partial coverage), we presentDesktopObjects-360, a novel comprehensive dataset for 3D segmentation inradiance fields, featuring: (1) complex multi-object scenes, (2) globallyconsistent 2D annotations, (3) large-scale training data (over 27 thousand 2Dmasks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.</description>
      <author>example@mail.com (Wentao Sun, Hanqing Xu, Quanyun Wu, Dedong Zhang, Yiping Chen, Lingfei Ma, John S. Zelek, Jonathan Li)</author>
      <guid isPermaLink="false">2508.00259v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Robust Model Reconstruction Based on the Topological Understanding of Point Clouds Using Persistent Homology</title>
      <link>http://arxiv.org/abs/2508.00251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于持久同调和Loop细分技术的自动方法，用于从无组织的噪声点云中重建多组件模型，能够有效区分和分离具有共享区域的多个封闭表面，生成高质量最终表面。&lt;h4&gt;背景&lt;/h4&gt;从无组织的点云重建模型是一个重大挑战，特别是当模型由多个组件并由其表面点云表示时。这些模型通常包含噪声的点云，代表具有共享区域的多个封闭表面，使得它们的自动识别和分离本质上很复杂。&lt;h4&gt;目的&lt;/h4&gt;提出一种自动方法，利用持久同调的拓扑理解和代表2-循环，有效区分和分离每个封闭表面，并生成高质量的最终表面实现完整的模型重建。&lt;h4&gt;方法&lt;/h4&gt;使用持久同调提供的拓扑理解，结合持久同调群的代表2-循环来区分表面，采用Loop细分和最小二乘渐进迭代逼近(LSPIA)技术生成高质量最终表面。&lt;h4&gt;主要发现&lt;/h4&gt;该方法对点云中的噪声具有鲁棒性，适合从包含噪声的数据重建模型，实验结果证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了实际应用的潜力，能够有效处理多组件模型的重建问题。&lt;h4&gt;翻译&lt;/h4&gt;从无组织的点云重建模型是一个重大挑战，特别是当模型由多个组件并由其表面点云表示时。这类模型通常涉及表示具有共享区域的多个封闭表面的含噪点云，使得它们的自动识别和分离本质上很复杂。在本文中，我们提出了一种自动方法，利用持久同调提供的拓扑理解以及持久同调群的代表2-循环，有效地区分和分离每个封闭表面。此外，我们采用Loop细分和最小二乘渐进迭代逼近(LSPIA)技术来生成高质量的最终表面，实现完整的模型重建。我们的方法对点云中的噪声具有鲁棒性，使其适合从这类数据重建模型。实验结果证明了我们方法的有效性，并突显了其在实际应用中的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从无组织的点云重建多组件模型的问题，特别是当这些组件的表面点云共享区域且包含噪声时。这个问题在计算机图形学、几何建模和逆向工程中非常重要，因为现实世界中的扫描数据通常包含噪声，而复杂模型（如工业机制、雕塑等）往往由多个组件组成。传统方法在这种情况下难以自动识别和分离每个封闭表面，限制了这些技术在CAD、医学成像、计算机动画和虚拟现实等领域的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统表面重建方法的局限性，包括基于三角剖分的方法对参数敏感、基于平滑先验的方法会过度平滑细节、基于模板的方法缺乏通用性，以及深度学习方法需要大量训练数据。然后，作者借鉴了持续同调用于分析点云拓扑特征的工作，代表性循环作为表面初始近似的方法，以及Loop细化和LSPIA优化技术。通过整合拓扑分析与几何处理技术，作者设计了一种能够自动识别和分离多组件模型封闭表面的方法，同时保持对噪声的鲁棒性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用持续同调的拓扑理解来识别和分离点云中的多个封闭表面，使用代表性2-循环作为初始控制网格，并通过Loop细分和LSPIA优化生成高质量表面。整体流程包括：1)从点云构建alpha过滤并计算持续同调和2-PD；2)聚类2-PD中的点找出显著点；3)为每个显著点计算体积最优循环；4)移除非流形顶点和边；5)为每个2-循环计算邻近点；6)简化网格并作为初始控制网格；7)应用Loop细分和LSPIA优化直到满足停止准则。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用持续同调的拓扑理解来确定重建的封闭表面数量；2)使用持续同调群的有代表性2-循环作为表面重建的初始控制网格；3)采用LSPIA方法生成更好地近似给定点云的重建表面。与传统方法相比，这种方法能够处理多组件共享区域的复杂模型，对噪声具有鲁棒性，能够自动识别和分离每个封闭表面，同时生成高质量的重建表面，解决了传统方法在处理此类复杂场景时的局限性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于持续同调拓扑理解和代表性2-循环的鲁棒模型重建方法，能够从包含噪声的无组织点云中自动识别、分离并高质量重建多组件模型的封闭表面。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing models from unorganized point clouds presents a significantchallenge, especially when the models consist of multiple componentsrepresented by their surface point clouds. Such models often involve pointclouds with noise that represent multiple closed surfaces with shared regions,making their automatic identification and separation inherently complex. Inthis paper, we propose an automatic method that uses the topologicalunderstanding provided by persistent homology, along with representative2-cycles of persistent homology groups, to effectively distinguish and separateeach closed surface. Furthermore, we employ Loop subdivision and least squaresprogressive iterative approximation (LSPIA) techniques to generate high-qualityfinal surfaces and achieve complete model reconstruction. Our method is robustto noise in the point cloud, making it suitable for reconstructing models fromsuch data. Experimental results demonstrate the effectiveness of our approachand highlight its potential for practical applications.</description>
      <author>example@mail.com (Yu Chen, Hongwei Lin)</author>
      <guid isPermaLink="false">2508.00251v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs</title>
      <link>http://arxiv.org/abs/2508.00169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了概率点云(PPC)作为一种新的3D场景表示方法，通过为每个点增加概率属性来封装原始LiDAR测量中的不确定性信息，并展示了其在鲁棒3D目标检测中的优势。&lt;h4&gt;背景&lt;/h4&gt;基于LiDAR的3D传感器提供点云，这是各种场景理解任务中的标准3D表示。然而，现代LiDAR在远距离或低反照率物体等现实场景中面临挑战，导致产生稀疏或有误的点云。这些错误源于原始LiDAR测量的噪声，并传播到下游感知模型中，导致准确性严重下降，因为传统3D处理管道在构建点云时不会保留原始测量中的任何不确定性信息。&lt;h4&gt;目的&lt;/h4&gt;作者旨在解决传统LiDAR点云表示中缺乏不确定性信息的问题，提出一种新的3D场景表示方法，能够提高3D目标检测在具有挑战性场景中的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了概率点云(PPC)，一种新的3D场景表示方法，其中每个点都增加了概率属性，封装原始数据中的测量不确定性(或置信度)。他们还引入了利用PPC进行鲁棒3D目标检测的推理方法，这些方法可作为计算量小的即插即用模块集成到3D推理管道中。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟和真实捕获的实验表明，基于PPC的3D推理方法在具有挑战性的室内和室外场景中优于几种基线方法，这些场景涉及小型、远距离和低反照率物体以及强环境光，包括仅使用LiDAR的模型和相机-LiDAR融合模型。&lt;h4&gt;结论&lt;/h4&gt;概率点云(PPC)作为一种新的3D场景表示方法，通过为点云增加不确定性信息，能够显著提高3D目标检测在具有挑战性场景中的性能，特别是在处理小型、远距离和低反照率物体以及强环境光条件时。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的3D传感器提供点云，这是各种场景理解任务中的标准3D表示。现代LiDAR在远距离或低反照率物体等现实场景中面临关键挑战，产生稀疏或有误的点云。这些错误源于原始LiDAR测量的噪声，并传播到下游感知模型中，可能导致准确性严重损失。这是因为传统3D处理管道在构建点云时不会保留原始测量中的任何不确定性信息。我们提出了概率点云(PPC)，这是一种新的3D场景表示方法，其中每个点都增加了概率属性，封装原始数据中的测量不确定性(或置信度)。我们进一步引入了利用PPC进行鲁棒3D目标检测的推理方法；这些方法具有通用性，可作为计算量小的即插即用模块用于3D推理管道。通过模拟和真实捕获的实验，我们证明了基于PPC的3D推理方法在具有挑战性的室内和室外场景中优于几种基线方法，这些场景涉及小型、远距离和低反照率物体以及强环境光，包括使用LiDAR和相机-LiDAR融合模型的基线方法。我们的项目网页位于https://bhavyagoyal.github.io/ppc。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现代LiDAR在挑战性现实场景（如长距离、低反照率物体、强环境光）下产生稀疏或错误点云的问题。这个问题很重要，因为LiDAR是自动驾驶、监控等应用的核心传感器，而这些应用需要在各种条件下可靠工作。传统3D处理管道在构建点云时忽略了原始测量中的不确定性信息，导致错误传播到下游感知模型，严重影响系统可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到传统LiDAR信号处理仅依靠峰值位置估计点云，忽略了原始SPAD直方图中的丰富信息。他们决定不采用确定性过滤方法，而是设计轻量级置信度特征来编码传感器不确定性。作者借鉴了点云网络中的关键点采样方法，参考了现有点云去噪方法但指出其计算成本高，也参考了3D重建技术但避免了其高带宽需求。设计时特别考虑了在资源受限的传感器上实现的可能性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出概率点云(PPC)，一种新的3D场景表示，其中每个点都增加了概率属性，封装了原始测量中的不确定性。整体流程包括：1)从原始SPAD直方图中提取概率特征；2)创建带概率属性的点云；3)使用两种方法进行鲁棒推理：邻域概率密度(NPD)过滤去除低置信度噪声点，最远概率点采样(FPPS)确保采样点更可靠。这些方法可作为即插即用模块集成到现有3D感知模型中。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)设计轻量级物理置信度特征；2)开发PPC表示传播传感器不确定性；3)设计低计算成本的鲁棒推理方法；4)在模拟和真实数据上验证效果。不同之处在于：PPC保留而非丢弃不确定性信息；避免额外计算密集型去噪步骤；使用紧凑表示而非处理完整直方图；是首个为3D LiDAR设计和传播置信度属性的工作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出概率点云表示和相应的推理方法，显著提高了在挑战性现实场景下3D物体检测的鲁棒性和准确性，同时保持了计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based 3D sensors provide point clouds, a canonical 3D representationused in various scene understanding tasks. Modern LiDARs face key challenges inseveral real-world scenarios, such as long-distance or low-albedo objects,producing sparse or erroneous point clouds. These errors, which are rooted inthe noisy raw LiDAR measurements, get propagated to downstream perceptionmodels, resulting in potentially severe loss of accuracy. This is becauseconventional 3D processing pipelines do not retain any uncertainty informationfrom the raw measurements when constructing point clouds.  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representationwhere each point is augmented with a probability attribute that encapsulatesthe measurement uncertainty (or confidence) in the raw data. We furtherintroduce inference approaches that leverage PPC for robust 3D objectdetection; these methods are versatile and can be used as computationallylightweight drop-in modules in 3D inference pipelines. We demonstrate, via bothsimulations and real captures, that PPC-based 3D inference methods outperformseveral baselines using LiDAR as well as camera-LiDAR fusion models, acrosschallenging indoor and outdoor scenarios involving small, distant, andlow-albedo objects, as well as strong ambient light.  Our project webpage is at https://bhavyagoyal.github.io/ppc .</description>
      <author>example@mail.com (Bhavya Goyal, Felipe Gutierrez-Barragan, Wei Lin, Andreas Velten, Yin Li, Mohit Gupta)</author>
      <guid isPermaLink="false">2508.00169v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation</title>
      <link>http://arxiv.org/abs/2508.00428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE VIS VAST 2025 ACM 2012 CCS - Human-centered computing,  Visualization, Visualization design and evaluation methods&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Sel3DCraft，一个用于Text-to-3D生成的视觉提示工程系统，通过三个关键创新解决了传统方法中的盲目试错问题，将无结构的探索转变为引导式视觉过程。&lt;h4&gt;背景&lt;/h4&gt;Text-to-3D生成已经改变了数字内容创作，但仍受到盲目试错提示过程的限制，导致结果不可预测。虽然视觉提示工程在文本到图像领域已经取得进展，但在3D生成中的应用面临独特挑战，需要多视图一致性和空间理解。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够将无结构的探索转变为引导式视觉过程的系统，以支持设计师在Text-to-3D生成中的创造力，提高生成结果的可预测性。&lt;h4&gt;方法&lt;/h4&gt;Sel3DCraft采用了三个关键创新：1) 双分支结构，结合检索和生成，用于多样化候选探索；2) 多视图混合评分方法，利用MLLMs和创新的高层次指标，以人类专家一致性评估3D模型；3) 提示驱动的可视化分析套件，实现直观的缺陷识别和改进。&lt;h4&gt;主要发现&lt;/h4&gt;大量测试和用户研究表明，Sel3DCraft在支持设计师创造力方面优于其他Text-to-3D系统。&lt;h4&gt;结论&lt;/h4&gt;Sel3DCraft通过创新的视觉提示工程方法，有效解决了Text-to-3D生成中的盲目试错问题，提高了生成结果的可预测性和设计师的工作效率。&lt;h4&gt;翻译&lt;/h4&gt;文本到3D生成已经改变了数字内容创作，但仍受到盲目试错提示过程的限制，导致结果不可预测。虽然视觉提示工程在文本到图像领域已经取得进展，但在3D生成中的应用面临独特挑战，需要多视图一致性和空间理解。我们提出了Sel3DCraft，一个用于T23D的视觉提示工程系统，将无结构的探索转变为引导式视觉过程。我们的方法引入了三个关键创新：结合检索和生成的双分支结构，用于多样化候选探索；利用MLLMs和创新高层次指标的多视图混合评分方法，以人类专家一致性评估3D模型；以及提示驱动的可视化分析套件，实现直观的缺陷识别和改进。大量测试和用户研究表明，Sel3DCraft在支持设计师创造力方面优于其他T23D系统。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决文本到3D生成(T23D)中的提示词工程挑战，当前T23D工具存在盲目试错、用户期望与生成质量差距大、缺乏有效提示词推荐机制等问题。这个问题很重要，因为T23D已改变虚拟现实、游戏等行业的数字内容创作，但现有工具以'黑盒'方式运行，缺乏用户控制，导致专业设计师难以高效创建符合预期的3D模型，限制了技术的实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过与专业设计师进行形成性研究，识别了当前T23D工具的四个关键低效问题，并据此推导出四个设计要求：生成足够图像用于探索、支持3D感知的多视图结果、提供与人类一致的多级评估、推荐提示词迭代改进。方法借鉴了视觉提示工程(如PromptMagician)、创意支持系统(如TaleBrush)、形状探索技术和文本到3D生成技术，但创新性地将这些技术整合应用于3D生成领域，设计了结合检索和生成的双分支结构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将T23D的无结构探索转变为引导式视觉过程，通过结合检索与生成、多视图混合评分和可视化分析工具，使提示词工程更加直观高效。整体流程包括：1)多模态统一视觉表示，通过预训练模型检索和多模态合成生成候选；2)多视图混合级评分函数，结合低级计算指标和基于MLLM的高级语义评估；3)评分引导的提示推荐，通过树图词云和关键词贡献图帮助用户迭代优化提示词；4)用户交互流程，用户浏览候选、评估质量、选择关键词并迭代生成新模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三：1)首次引入T23D视觉提示工程作为新任务，利用MLLMs评估3D模型多视图图像；2)提出跨模态多视图视觉表示方法，包括多视图卫星图、混合级评分热图和提示驱动的树图词云；3)设计统一的检索和生成交互式系统Sel3DCraft。相比之前工作，本文将2D视觉提示工程扩展到3D领域，强调人类在环路的探索体验而非仅关注生成算法，结合多维度评估方法提供直观的缺陷识别和改进，并通过双分支结构提供更丰富的候选选择。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Sel3DCraft通过交互式视觉提示工程方法结合检索与生成的双分支结构、多视图混合评分和可视化分析工具，显著提高了文本到3D生成的效率、质量和用户体验，使专业设计师能够更直观、高效地创建符合预期的3D模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-to-3D (T23D) generation has transformed digital content creation, yetremains bottlenecked by blind trial-and-error prompting processes that yieldunpredictable results. While visual prompt engineering has advanced intext-to-image domains, its application to 3D generation presents uniquechallenges requiring multi-view consistency evaluation and spatialunderstanding. We present Sel3DCraft, a visual prompt engineering system forT23D that transforms unstructured exploration into a guided visual process. Ourapproach introduces three key innovations: a dual-branch structure combiningretrieval and generation for diverse candidate exploration; a multi-view hybridscoring approach that leverages MLLMs with innovative high-level metrics toassess 3D models with human-expert consistency; and a prompt-driven visualanalytics suite that enables intuitive defect identification and refinement.Extensive testing and user studies demonstrate that Sel3DCraft surpasses otherT23D systems in supporting creativity for designers.</description>
      <author>example@mail.com (Nan Xiang, Tianyi Liang, Haiwen Huang, Shiqi Jiang, Hao Huang, Yifei Huang, Liangyu Chen, Changbo Wang, Chenhui Li)</author>
      <guid isPermaLink="false">2508.00428v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Analysing Temporal Reasoning in Description Logics Using Formal Grammars</title>
      <link>http://arxiv.org/abs/2508.00575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is an extended version of a paper appearing at the 28th European  Conference on Artificial Intelligence (ECAI 2025). 20 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文建立了一种时间扩展描述逻辑的片段与特定形式文法（特别是合取文法）之间的对应关系。&lt;h4&gt;背景&lt;/h4&gt;这种描述逻辑（具有LTL算子的k次方操作特性）自提出以来，其模型是否具有终极周期性以及查询回答是否可判定一直是一个未解决的问题。&lt;h4&gt;目的&lt;/h4&gt;建立该描述逻辑与合取文法之间的联系，从而解决该逻辑的模型周期性和查询回答可判定性问题。&lt;h4&gt;方法&lt;/h4&gt;通过建立该描述逻辑与合取文法之间的对应关系，利用已有的合取文法工具和算法来解决该逻辑的相关问题。&lt;h4&gt;主要发现&lt;/h4&gt;发现该描述逻辑不具有模型的终极周期性；证明了该描述逻辑中查询回答的不可判定性；为该描述逻辑的一些新的有趣片段建立了查询回答的可判定性。&lt;h4&gt;结论&lt;/h4&gt;通过建立该描述逻辑与合取文法之间的对应关系，解决了该描述逻辑的一些重要理论问题，并为后续研究提供了新的方法和工具。&lt;h4&gt;翻译&lt;/h4&gt;我们建立了一种时间扩展的EL描述逻辑片段，该逻辑带有LTL算子的k次方操作，与特定形式文法（特别是合取文法，即配备了交运算的上下文无关文法）之间的对应关系。这种联系意味着该描述逻辑不具有模型的终极周期性，并进一步导致该描述逻辑中查询回答的不可判定性，解决了一个自该描述逻辑提出以来一直悬而未决的问题。此外，它还为该描述逻辑的一些新的有趣片段建立了查询回答的可判定性，并为此目的重用了现有的合取文法工具和算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We establish a correspondence between (fragments of)$\mathcal{TEL}^\bigcirc$, a temporal extension of the $\mathcal{EL}$description logic with the LTL operator $\bigcirc^k$, and some specific kindsof formal grammars, in particular, conjunctive grammars (context-free grammarsequipped with the operation of intersection). This connection implies that$\mathcal{TEL}^\bigcirc$ does not possess the property of ultimate periodicityof models, and further leads to undecidability of query answering in$\mathcal{TEL}^\bigcirc$, closing a question left open since the introductionof $\mathcal{TEL}^\bigcirc$. Moreover, it also allows to establish decidabilityof query answering for some new interesting fragments of$\mathcal{TEL}^\bigcirc$, and to reuse for this purpose existing tools andalgorithms for conjunctive grammars.</description>
      <author>example@mail.com (Camille Bourgaux, Anton Gnatenko, Michaël Thomazo)</author>
      <guid isPermaLink="false">2508.00575v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Fine-grained Spatiotemporal Grounding on Egocentric Videos</title>
      <link>http://arxiv.org/abs/2508.00518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对第一人称视角视频中的时空定位问题，提出了EgoMask基准和EgoMask-Train数据集，解决了第一人称视频中目标持续时间短、轨迹稀疏、目标尺寸小和位置变化大等挑战。&lt;h4&gt;背景&lt;/h4&gt;现有研究主要在外视点视频中取得了显著进展，但在第一人称视角视频中的研究相对较少，尽管在增强现实和机器人应用中越来越重要。&lt;h4&gt;目的&lt;/h4&gt;分析第一人称视角和外视点视频之间的差异，解决第一人称视角视频中的关键挑战，为第一人称视频理解提供资源和见解。&lt;h4&gt;方法&lt;/h4&gt;系统分析第一人称视角和外视点视频之间的差异；引入EgoMask，第一个用于第一人称视频中细粒度时空定位的像素级基准；通过提出的自动标注流程构建EgoMask；创建EgoMask-Train大规模训练数据集。&lt;h4&gt;主要发现&lt;/h4&gt;第一人称视角视频中的关键挑战包括：目标持续时间更短、轨迹更稀疏、目标尺寸更小、位置变化更大；最先进的时空定位模型在EgoMask基准上表现不佳；在EgoMask-Train上进行微调可以显著提高性能，同时保持在外视点数据集上的表现。&lt;h4&gt;结论&lt;/h4&gt;该工作为推进第一人称视频理解提供了必要的资源和见解，代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;时空视频定位旨在根据文本查询在视频中定位目标实体。虽然现有研究在外视点视频中取得了显著进展，但第一人称视角设置的研究相对较少，尽管它在增强现实和机器人等应用中的重要性日益增长。在这项工作中，我们对第一人称视角和外视点视频之间的差异进行了系统分析，揭示了关键挑战，如更短的目标持续时间、更稀疏的轨迹、更小的目标尺寸和更大的位置变化。为应对这些挑战，我们引入了EgoMask，这是第一个用于第一人称视频中细粒度时空定位的像素级基准。它由我们提出的自动标注流程构建，该流程标注了短、中、长期视频中的指代表达式和对象掩码。此外，我们创建了EgoMask-Train，一个大规模训练数据集，以促进模型开发。实验表明，最先进的时空定位模型在我们的EgoMask基准上表现不佳，但在EgoMask-Train上进行微调可以显著提高性能，同时保持在外视点数据集上的表现。我们的工作因此为推进第一人称视频理解提供了必要的资源和见解。我们的代码可在https://github.com/LaVi-Lab/EgoMask获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatiotemporal video grounding aims to localize target entities in videosbased on textual queries. While existing research has made significant progressin exocentric videos, the egocentric setting remains relatively underexplored,despite its growing importance in applications such as augmented reality androbotics. In this work, we conduct a systematic analysis of the discrepanciesbetween egocentric and exocentric videos, revealing key challenges such asshorter object durations, sparser trajectories, smaller object sizes, andlarger positional shifts. To address these challenges, we introduce EgoMask,the first pixel-level benchmark for fine-grained spatiotemporal grounding inegocentric videos. It is constructed by our proposed automatic annotationpipeline, which annotates referring expressions and object masks across short-,medium-, and long-term videos. Additionally, we create EgoMask-Train, alarge-scale training dataset to facilitate model development. Experimentsdemonstrate that the state-of-the-art spatiotemporal grounding models performpoorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yieldssignificant improvements, while preserving performance on exocentric datasets.Our work thus provides essential resources and insights for advancingegocentric video understanding. Our code is available athttps://github.com/LaVi-Lab/EgoMask .</description>
      <author>example@mail.com (Shuo Liang, Yiwu Zhong, Zi-Yuan Hu, Yeyao Tao, Liwei Wang)</author>
      <guid isPermaLink="false">2508.00518v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>iSafetyBench: A video-language benchmark for safety in industrial environment</title>
      <link>http://arxiv.org/abs/2508.00399v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to VISION'25 - ICCV 2025 workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队创建了iSafetyBench基准测试，评估视觉语言模型在工业环境中的表现，特别是在识别常规操作和危险场景方面的能力。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)在零样本设置下已能在多种视频理解任务中实现令人印象深刻的泛化能力，但在高风险工业领域（需要识别常规操作和安全关键异常）的能力尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;解决这一差距，引入iSafetyBench，一个专门用于评估模型在工业环境中正常和危险场景下性能的新视频语言基准。&lt;h4&gt;方法&lt;/h4&gt;iSafetyBench包含1,100个来自真实工业场景的视频片段，标注了98个常规和67个危险动作类别的开放词汇、多标签动作标签。每个片段都配有单标签和多标签评估的多项选择题，能够在零样本条件下评估八种最先进的视频语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;尽管这些模型在现有视频基准上表现强劲，但在iSafetyBench上表现不佳，特别是在识别危险活动和多标签场景方面。结果显示了显著的性能差距。&lt;h4&gt;结论&lt;/h4&gt;需要更强大、安全感知的多模态模型用于工业应用，iSafetyBench提供了首个测试平台来推动这一方向的发展。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLMs)的最新进展使其能够在零样本设置下跨多种视频理解任务实现令人印象深刻的泛化能力。然而，它们在高风险工业领域的能力仍然很大程度上未被探索，在这些领域中，识别常规操作和安全关键异常至关重要。为解决这一差距，我们引入了iSafetyBench，这是一个新的视频语言基准，专门设计用于评估模型在工业环境中正常和危险场景下的性能。iSafetyBench包含1,100个来自真实工业场景的视频片段，标注了98个常规和67个危险动作类别的开放词汇、多标签动作标签。每个片段都配有单标签和多标签评估的多项选择题，能够细粒度评估VLMs在标准和安全关键环境中的性能。我们在零样本条件下评估了八种最先进的视频语言模型。尽管它们在现有视频基准上表现强劲，但这些模型在iSafetyBench上表现不佳，特别是在识别危险活动和多标签场景方面。我们的结果显示了显著的性能差距，强调了工业应用需要更强大、安全感知的多模态模型。iSafetyBench提供了首个测试平台来推动这一方向的进展。数据集可在以下网址获取：https://github.com/raiyaan-abdullah/iSafety-Bench。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in vision-language models (VLMs) have enabled impressivegeneralization across diverse video understanding tasks under zero-shotsettings. However, their capabilities in high-stakes industrial domains-whererecognizing both routine operations and safety-critical anomalies isessential-remain largely underexplored. To address this gap, we introduceiSafetyBench, a new video-language benchmark specifically designed to evaluatemodel performance in industrial environments across both normal and hazardousscenarios. iSafetyBench comprises 1,100 video clips sourced from real-worldindustrial settings, annotated with open-vocabulary, multi-label action tagsspanning 98 routine and 67 hazardous action categories. Each clip is pairedwith multiple-choice questions for both single-label and multi-labelevaluation, enabling fine-grained assessment of VLMs in both standard andsafety-critical contexts. We evaluate eight state-of-the-art video-languagemodels under zero-shot conditions. Despite their strong performance on existingvideo benchmarks, these models struggle with iSafetyBench-particularly inrecognizing hazardous activities and in multi-label scenarios. Our resultsreveal significant performance gaps, underscoring the need for more robust,safety-aware multimodal models for industrial applications. iSafetyBenchprovides a first-of-its-kind testbed to drive progress in this direction. Thedataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.</description>
      <author>example@mail.com (Raiyaan Abdullah, Yogesh Singh Rawat, Shruti Vyas)</author>
      <guid isPermaLink="false">2508.00399v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Punching Bag vs. Punching Person: Motion Transferability in Videos</title>
      <link>http://arxiv.org/abs/2508.00085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025 main conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了动作识别模型在不同上下文中传递高级运动概念的能力，特别是面对未见过的变化时的表现。作者提出了一个动作可迁移性框架，包含三个数据集，并评估了13个最先进模型，发现模型在识别新上下文中的高级动作时性能显著下降。&lt;h4&gt;背景&lt;/h4&gt;动作识别模型展现出强大的泛化能力，但它们是否能够在不同上下文（即使在相似分布内）有效地传递高级运动概念尚不清楚。例如，模型能否识别出未见过的'punching person'这样的变化，而不仅仅是基本的'punching'动作。&lt;h4&gt;目的&lt;/h4&gt;探索动作识别模型在不同上下文中传递高级运动概念的能力，特别是面对未见过的变化时的表现，并建立一个评估动作可迁移性的重要基准。&lt;h4&gt;方法&lt;/h4&gt;作者引入了一个动作可迁移性框架，包含三个数据集：Syn-TA（包含3D物体运动的合成数据集）、Kinetics400-TA和Something-Something-v2-TA（两者都改编自自然视频数据集）。作者评估了13个最先进的模型在这些基准上的表现，并进行了详细分析。&lt;h4&gt;主要发现&lt;/h4&gt;1) 多模态模型在细粒度未知动作上比粗粒度动作表现更差；2) 无偏的Syn-TA数据集与真实世界数据集一样具有挑战性，模型在受控环境中表现出更大的性能下降；3) 当空间线索占主导时，较大的模型提高了可迁移性，但在密集的时间推理方面表现不佳，而对物体和背景线索的依赖阻碍了泛化能力。&lt;h4&gt;结论&lt;/h4&gt;这项研究建立了评估动作识别中运动可迁移性的重要基准，并揭示了当前模型在处理新上下文中的高级动作时的局限性。作者还探讨了分离粗粒度和细粒度运动如何改善在时间上有挑战性的数据集中的识别。&lt;h4&gt;翻译&lt;/h4&gt;动作识别模型展现出强大的泛化能力，但它们是否能够在不同上下文（即使在相似分布内）有效地传递高级运动概念？例如，当面对'punching person'这样的未见过的变化时，模型能否识别出广义的'punching'动作？为了探索这一点，我们引入了一个动作可迁移性框架，包含三个数据集：(1) Syn-TA，一个包含3D物体运动的合成数据集；(2) Kinetics400-TA；以及(3) Something-Something-v2-TA，两者都改编自自然视频数据集。我们在这些基准上评估了13个最先进的模型，并观察到模型在识别新上下文中的高级动作时性能显著下降。我们的分析揭示：1) 多模态模型在细粒度未知动作上比粗粒度动作表现更差；2) 无偏的Syn-TA数据集与真实世界数据集一样具有挑战性，模型在受控环境中表现出更大的性能下降；3) 当空间线索占主导时，较大的模型提高了可迁移性，但在密集的时间推理方面表现不佳，而对物体和背景线索的依赖阻碍了泛化能力。我们进一步探讨了分离粗粒度和细粒度运动如何改善在时间上有挑战性的数据集中的识别。我们认为这项研究建立了评估动作识别中运动可迁移性的重要基准。数据集和相关代码：https://github.com/raiyaan-abdullah/Motion-Transfer。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Action recognition models demonstrate strong generalization, but can theyeffectively transfer high-level motion concepts across diverse contexts, evenwithin similar distributions? For example, can a model recognize the broadaction "punching" when presented with an unseen variation such as "punchingperson"? To explore this, we introduce a motion transferability framework withthree datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from naturalvideo datasets. We evaluate 13 state-of-the-art models on these benchmarks andobserve a significant drop in performance when recognizing high-level actionsin novel contexts. Our analysis reveals: 1) Multimodal models struggle morewith fine-grained unknown actions than with coarse ones; 2) The bias-freeSyn-TA proves as challenging as real-world datasets, with models showinggreater performance drops in controlled settings; 3) Larger models improvetransferability when spatial cues dominate but struggle with intensive temporalreasoning, while reliance on object and background cues hinders generalization.We further explore how disentangling coarse and fine motions can improverecognition in temporally challenging datasets. We believe this studyestablishes a crucial benchmark for assessing motion transferability in actionrecognition. Datasets and relevant code:https://github.com/raiyaan-abdullah/Motion-Transfer.</description>
      <author>example@mail.com (Raiyaan Abdullah, Jared Claypoole, Michael Cogswell, Ajay Divakaran, Yogesh Rawat)</author>
      <guid isPermaLink="false">2508.00085v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial, Appearance, and Motion Anomaly</title>
      <link>http://arxiv.org/abs/2507.19924v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025. Project page: https://dejian-lc.github.io/humansam/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HumanSAM是一个新型框架，旨在解决以人为中心的伪造视频检测问题，通过将伪造分为空间、外观和运动异常三种类型，并采用视频理解和空间深度融合的方法，以及基于排序的置信度增强策略。研究团队还构建了第一个公共基准数据集HFV，实验表明HumanSAM在二元和多类伪造分类中表现优异。&lt;h4&gt;背景&lt;/h4&gt;生成式模型（特别是模拟真实人类行为的以人为中心的视频）对人类信息安全和真实性构成重大威胁。尽管二元伪造视频检测取得进展，但对伪造类型的细粒度理解不足，影响可靠性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;提出HumanSAM框架，解决视频生成模型的基本挑战，将人为中心的伪造分为三种常见类型：空间异常、外观异常和运动异常，提高检测的可靠性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;1) 通过融合视频理解和空间深度两个分支生成人类伪造表示；2) 采用基于排序的置信度增强策略，引入三个先验分数学习更强大的表示；3) 构建第一个公共基准数据集HFV，所有类型的伪造都经过半自动仔细注释。&lt;h4&gt;主要发现&lt;/h4&gt;HumanSAM在二元和多类伪造分类任务中与最先进方法相比取得了有希望的结果，表明该方法在处理不同类型伪造视频方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;HumanSAM框架为以人为中心的伪造视频检测提供了新的解决方案，通过细粒度分类和多特征融合方法提高了检测的准确性和可靠性，为实际应用提供了更好的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;许多来自生成模型的合成视频，特别是模拟真实人类行为的以人为中心的视频，对人类信息安全和真实性构成重大威胁。虽然在二元伪造视频检测方面取得了进展，但对伪造类型的细粒度理解不足，这关系到可靠性和可解释性，而这些对于实际应用至关重要。为解决这一局限，我们提出了HumanSAM，一个构建于视频生成模型基本挑战之上的新框架。具体来说，HumanSAM旨在将人为中心的伪造分为生成内容中常见的三种不同类型的伪影：空间异常、外观异常和运动异常。为了更好地捕捉几何、语义和时空一致性的特征，我们提出通过融合视频理解和空间深度两个分支来生成人类伪造表示。在训练过程中，我们还采用基于排序的置信度增强策略，通过引入三个先验分数来学习更强大的表示。为了训练和评估，我们构建了第一个公共基准——以人为中心的伪造视频(HFV)数据集，所有类型的伪造都经过半自动仔细注释。在我们的实验中，HumanSAM在二元和多类伪造分类中与最先进方法相比取得了有希望的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Numerous synthesized videos from generative models, especially human-centricones that simulate realistic human actions, pose significant threats to humaninformation security and authenticity. While progress has been made in binaryforgery video detection, the lack of fine-grained understanding of forgerytypes raises concerns regarding both reliability and interpretability, whichare critical for real-world applications. To address this limitation, wepropose HumanSAM, a new framework that builds upon the fundamental challengesof video generation models. Specifically, HumanSAM aims to classifyhuman-centric forgeries into three distinct types of artifacts commonlyobserved in generated content: spatial, appearance, and motion anomaly. Tobetter capture the features of geometry, semantics and spatiotemporalconsistency, we propose to generate the human forgery representation by fusingtwo branches of video understanding and spatial depth. We also adopt arank-based confidence enhancement strategy during the training process to learnmore robust representation by introducing three prior scores. For training andevaluation, we construct the first public benchmark, the Human-centric ForgeryVideo (HFV) dataset, with all types of forgeries carefully annotatedsemi-automatically. In our experiments, HumanSAM yields promising results incomparison with state-of-the-art methods, both in binary and multi-classforgery classification.</description>
      <author>example@mail.com (Chang Liu, Yunfan Ye, Fan Zhang, Qingyang Zhou, Yuchuan Luo, Zhiping Cai)</author>
      <guid isPermaLink="false">2507.19924v2</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Can Large Pretrained Depth Estimation Models Help With Image Dehazing?</title>
      <link>http://arxiv.org/abs/2508.00698v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to AAAI2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于预训练深度表示的图像去雾方法，通过RGB-D融合模块增强现有去雾架构的适应性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;真实场景中的图像去雾具有挑战性，因为雾霾具有空间变化特性。现有方法虽然在大规模预训练模型上表现出色，但其架构特定设计限制了在不同场景下的适应能力。&lt;h4&gt;目的&lt;/h4&gt;系统研究预训练深度表示在图像去雾任务中的泛化能力，并开发一种适用于多种去雾架构的通用方法。&lt;h4&gt;方法&lt;/h4&gt;分析从数百万张不同图像中学习的预训练深度表示的去雾泛化能力，并基于深度特征在不同雾霾水平下保持一致的发现，提出一个即插即用的RGB-D融合模块，可无缝集成到各种去雾架构中。&lt;h4&gt;主要发现&lt;/h4&gt;学习的深度深度特征在不同雾霾水平下表现出显著的一致性，这为跨场景去雾提供了基础。&lt;h4&gt;结论&lt;/h4&gt;在多个基准测试上的广泛实验验证了该方法的有效性和广泛适用性，表明它能够适应不同准确性和效率要求的场景。&lt;h4&gt;翻译&lt;/h4&gt;图像去雾由于真实场景中雾霾的空间变化特性而仍然是一个具有挑战性的问题。虽然现有方法已经展示了大规模预训练模型在图像去雾方面的潜力，但其架构特定设计限制了在不同准确性和效率要求场景下的适应性。在这项工作中，我们系统地研究了从数百万张不同图像中学习的预训练深度表示在图像去雾中的泛化能力。我们的经验分析表明，学习的深度深度特征在不同雾霾水平下保持显著的一致性。基于这一见解，我们提出一个即插即用的RGB-D融合模块，可以与各种去雾架构无缝集成。在多个基准测试上的广泛实验验证了我们方法的有效性和广泛适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image dehazing remains a challenging problem due to the spatially varyingnature of haze in real-world scenes. While existing methods have demonstratedthe promise of large-scale pretrained models for image dehazing, theirarchitecture-specific designs hinder adaptability across diverse scenarios withdifferent accuracy and efficiency requirements. In this work, we systematicallyinvestigate the generalization capability of pretrained depthrepresentations-learned from millions of diverse images-for image dehazing. Ourempirical analysis reveals that the learned deep depth features maintainremarkable consistency across varying haze levels. Building on this insight, wepropose a plug-and-play RGB-D fusion module that seamlessly integrates withdiverse dehazing architectures. Extensive experiments across multiplebenchmarks validate both the effectiveness and broad applicability of ourapproach.</description>
      <author>example@mail.com (Hongfei Zhang, Kun Zhou, Ruizheng Wu, Jiangbo Lu)</author>
      <guid isPermaLink="false">2508.00698v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text</title>
      <link>http://arxiv.org/abs/2508.00447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究者提出了CLIPTime，一个基于CLIP架构的多模态、多任务框架，能够从图像和文本输入中预测真菌生长的发育阶段和时间戳，有效解决了现有视觉-语言模型在捕捉时间进展方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;理解生物生长的时间动态在微生物学、农业和生物降解研究等多个领域至关重要，但现有的视觉-语言模型如CLIP在联合视觉-文本推理方面表现出色，但在捕捉时间进展方面的效果有限。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从图像和文本输入中预测真菌生长发育阶段和相应时间戳的多模态、多任务框架。&lt;h4&gt;方法&lt;/h4&gt;基于CLIP架构构建，学习联合视觉-文本嵌入，实现时间感知推理；引入带有对齐时间戳和分类阶段标签的合成真菌生长数据集；联合执行分类和回归任务；提出自定义评估指标包括时间准确性和回归误差。&lt;h4&gt;主要发现&lt;/h4&gt;CLIPTime能够有效建模生物进展，产生可解释的、基于时间的输出。&lt;h4&gt;结论&lt;/h4&gt;视觉-语言模型在现实世界生物监测应用中具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;理解生物生长的时间动态在微生物学、农业和生物降解研究等不同领域至关重要。尽管像对比语言图像预训练(CLIP)这样的视觉-语言模型在联合视觉-文本推理方面表现出强大能力，但它们在捕捉时间进展方面的有效性仍然有限。为解决这一问题，我们提出了CLIPTime，一个多模态、多任务框架，旨在从图像和文本输入中预测真菌生长的发育阶段和相应时间戳。基于CLIP架构构建，我们的模型学习联合视觉-文本嵌入，并能够在测试时不需要明确的时间输入的情况下实现时间感知推理。为促进训练和评估，我们引入了一个带有对齐时间戳和分类阶段标签的合成真菌生长数据集。CLIPTime联合执行分类和回归，预测离散生长阶段以及连续时间戳。我们还提出了自定义评估指标，包括时间准确性和回归误差，以评估时间感知预测的精确度。实验结果表明，CLIPTime有效地建模了生物进展，并产生了可解释的、基于时间的输出，突显了视觉-语言模型在现实世界生物监测应用中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the temporal dynamics of biological growth is critical acrossdiverse fields such as microbiology, agriculture, and biodegradation research.Although vision-language models like Contrastive Language Image Pretraining(CLIP) have shown strong capabilities in joint visual-textual reasoning, theireffectiveness in capturing temporal progression remains limited. To addressthis, we propose CLIPTime, a multimodal, multitask framework designed topredict both the developmental stage and the corresponding timestamp of fungalgrowth from image and text inputs. Built upon the CLIP architecture, our modellearns joint visual-textual embeddings and enables time-aware inference withoutrequiring explicit temporal input during testing. To facilitate training andevaluation, we introduce a synthetic fungal growth dataset annotated withaligned timestamps and categorical stage labels. CLIPTime jointly performsclassification and regression, predicting discrete growth stages alongsidecontinuous timestamps. We also propose custom evaluation metrics, includingtemporal accuracy and regression error, to assess the precision of time-awarepredictions. Experimental results demonstrate that CLIPTime effectively modelsbiological progression and produces interpretable, temporally grounded outputs,highlighting the potential of vision-language models in real-world biologicalmonitoring applications.</description>
      <author>example@mail.com (Anju Rani, Daniel Ortiz-Arroyo, Petar Durdevic)</author>
      <guid isPermaLink="false">2508.00447v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment</title>
      <link>http://arxiv.org/abs/2508.00332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出MCSEO方法，通过细粒度的对象-短语对齐增强多模态句子嵌入，解决了图像-字幕对中存在的噪声问题&lt;h4&gt;背景&lt;/h4&gt;多模态句子嵌入模型通常在训练中结合图像-字幕对和文本数据，但这些对常包含冗余或无关信息等噪声&lt;h4&gt;目的&lt;/h4&gt;减轻图像-字幕对中的噪声问题，提高多模态句子嵌入的质量&lt;h4&gt;方法&lt;/h4&gt;MCSEO方法结合传统图像-字幕对齐与细粒度对象-短语对齐，利用现有分割和目标检测模型提取准确对象-短语对，优化针对对象-短语对应的对比学习目标&lt;h4&gt;主要发现&lt;/h4&gt;在不同骨干模型上的语义文本相似性任务中，MCSEO持续超越强大基线方法，证明精确的对象-短语对齐对多模态表示学习至关重要&lt;h4&gt;结论&lt;/h4&gt;精确的对象-短语对齐在多模态表示学习中具有重要意义，MCSEO方法能有效提升多模态句子嵌入性能&lt;h4&gt;翻译&lt;/h4&gt;多模态句子嵌入模型通常在训练过程中除了使用文本数据外，还会利用图像-字幕对。然而，这些对常常包含噪声，包括图像或字幕一侧的冗余或无关信息。为缓解这一问题，我们提出了MCSEO，一种通过结合细粒度的对象-短语对齐和传统的图像-字幕对齐来增强多模态句子嵌入的方法。具体而言，MCSEO利用现有的分割和目标检测模型提取准确的对象-短语对，然后使用这些对来优化针对对象-短语对应的对比学习目标。在不同骨干模型上的语义文本相似性任务上的实验结果表明，MCSEO始终优于强大的基线方法，突显了精确的对象-短语对齐在多模态表示学习中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal sentence embedding models typically leverage image-caption pairsin addition to textual data during training. However, such pairs often containnoise, including redundant or irrelevant information on either the image orcaption side. To mitigate this issue, we propose MCSEO, a method that enhancesmultimodal sentence embeddings by incorporating fine-grained object-phrasealignment alongside traditional image-caption alignment. Specifically, MCSEOutilizes existing segmentation and object detection models to extract accurateobject-phrase pairs, which are then used to optimize a contrastive learningobjective tailored to object-phrase correspondence. Experimental results onsemantic textual similarity (STS) tasks across different backbone modelsdemonstrate that MCSEO consistently outperforms strong baselines, highlightingthe significance of precise object-phrase alignment in multimodalrepresentation learning.</description>
      <author>example@mail.com (Kaiyan Zhao, Zhongtao Miao, Yoshimasa Tsuruoka)</author>
      <guid isPermaLink="false">2508.00332v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Neighbor-Sampling Based Momentum Stochastic Methods for Training Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.00267v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于邻居采样的Adam型随机方法来解决非凸GCN训练问题，利用控制变量技术减少随机误差，并在标准假设下证明了最优收敛速率。数值实验表明，这些方法在节点分类任务上优于经典方法，特别是在大规模图数据集上。&lt;h4&gt;背景&lt;/h4&gt;图卷积网络(GCN)是图表示学习的强大工具，但由于其递归邻域聚合的特性，现有高效训练方法缺乏理论保证或缺少现代深度学习算法中的重要实用元素，如自适应性和动量。&lt;h4&gt;目的&lt;/h4&gt;开发基于邻居采样的Adam型随机方法，解决非凸GCN训练问题，同时提供理论保证和现代深度学习算法的实用优势。&lt;h4&gt;方法&lt;/h4&gt;提出基于邻居采样的Adam型随机方法；利用控制变量技术减少邻居采样引起的随机误差；在标准Adam型方法假设下证明收敛性；在多个基准数据集上进行节点分类任务数值实验。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在标准Adam型方法假设下享有最优收敛速率；在节点分类任务上优于经典基于邻居采样的SGD方法；在大规模图数据集上性能优势尤其明显。&lt;h4&gt;结论&lt;/h4&gt;基于邻居采样的Adam型方法结合控制变量技术可有效解决非凸GCN训练问题，提供理论保证和实际性能优势，特别是在处理大规模图数据时。&lt;h4&gt;翻译&lt;/h4&gt;图卷积网络(GCN)是图表示学习的强大工具。由于GCN采用的递归邻域聚合，现有的高效训练方法要么缺乏理论保证，要么缺少现代深度学习算法中的重要实用元素，如自适应性和动量。在本文中，我们提出了几种基于邻居采样的Adam型随机方法来解决非凸GCN训练问题。我们利用[1]提出的控制变量技术来减少邻居采样引起的随机误差。在Adam型方法的标准假设下，我们证明了我们的方法享有最优收敛速率。此外，我们在几个基准数据集上进行了广泛的节点分类任务数值实验。结果表明，我们的方法优于同样使用控制变量技术的经典基于邻居采样的SGD方法，特别是在大规模图数据集上。我们的代码可在https://github.com/RPI-OPT/CV-ADAM-GNN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph convolutional networks (GCNs) are a powerful tool for graphrepresentation learning. Due to the recursive neighborhood aggregationsemployed by GCNs, efficient training methods suffer from a lack of theoreticalguarantees or are missing important practical elements from modern deeplearning algorithms, such as adaptivity and momentum. In this paper, we presentseveral neighbor-sampling (NS) based Adam-type stochastic methods for solving anonconvex GCN training problem. We utilize the control variate techniqueproposed by [1] to reduce the stochastic error caused by neighbor sampling.Under standard assumptions for Adam-type methods, we show that our methodsenjoy the optimal convergence rate. In addition, we conduct extensive numericalexperiments on node classification tasks with several benchmark datasets. Theresults demonstrate superior performance of our methods over classic NS-basedSGD that also uses the control-variate technique, especially for large-scalegraph datasets. Our code is available at https://github.com/RPI-OPT/CV-ADAM-GNN .</description>
      <author>example@mail.com (Molly Noel, Gabriel Mancino-Ball, Yangyang Xu)</author>
      <guid isPermaLink="false">2508.00267v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Melody-Lyrics Matching with Contrastive Alignment Loss</title>
      <link>http://arxiv.org/abs/2508.00123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures, 3 tables. This work has been submitted to the  IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了旋律-歌词匹配(MLM)任务，探索音乐和歌词之间的关系，通过自监督表示学习框架实现旋律与歌词的匹配，并展示了将旋律与连贯且可唱的歌词匹配的能力。&lt;h4&gt;背景&lt;/h4&gt;音乐和歌词之间的联系超越了语义层面，包括节奏与韵律、音符时长与音节重音、结构对应等概念对关系，这在音乐信息检索领域是一个引人注目但很少被探索的方向。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的旋律-歌词匹配(MLM)任务，旨在从文本源中检索给定符号旋律的潜在歌词，而非从头开始生成歌词。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自监督的表示学习框架，使用对比对齐损失处理旋律和歌词；引入了'音节'(sylphone)这一基于音素身份和元音重音激活的歌词音节级别的新表示方法；无需对齐注释即可利用现有的大量配对旋律和歌词的歌曲。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验结果和直观示例，证明了该方法能够将旋律与连贯且可唱的歌词有效匹配。&lt;h4&gt;结论&lt;/h4&gt;通过开源代码和提供匹配示例，展示了旋律-歌词匹配方法的有效性和实用性，为音乐信息检索领域提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;音乐和歌词之间的联系远超语义联系。两种模态中的概念对，如节奏和韵律、音符时长和音节重音、结构对应关系等，在音乐信息检索领域提出了一个引人注目但很少被探索的方向。在本文中，我们提出了旋律-歌词匹配(MLM)，一个新任务，可以从文本源中检索给定符号旋律的潜在歌词。MLM本质上是利用旋律和歌词之间的关系，而非从头开始生成歌词。我们提出了一个带有对比对齐损失的自监督表示学习框架，用于处理旋律和歌词。这有可能利用现有的大量配对旋律和歌词的歌曲，无需对齐注释。此外，我们引入了'音节'，这是一种基于音素身份和元音重音激活的歌词音节级别的新表示方法。我们通过实验结果和直观示例证明了我们的方法可以将旋律与连贯且可唱的歌词相匹配。我们开源了代码，并在配套网页上提供了匹配示例：https://github.com/changhongw/mlm。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The connection between music and lyrics is far beyond semantic bonds.Conceptual pairs in the two modalities such as rhythm and rhyme, note durationand syllabic stress, and structure correspondence, raise a compelling yetseldom-explored direction in the field of music information retrieval. In thispaper, we present melody-lyrics matching (MLM), a new task which retrievespotential lyrics for a given symbolic melody from text sources. Rather thangenerating lyrics from scratch, MLM essentially exploits the relationshipsbetween melody and lyrics. We propose a self-supervised representation learningframework with contrastive alignment loss for melody and lyrics. This has thepotential to leverage the abundance of existing songs with paired melody andlyrics. No alignment annotations are required. Additionally, we introducesylphone, a novel representation for lyrics at syllable-level activated byphoneme identity and vowel stress. We demonstrate that our method can matchmelody with coherent and singable lyrics with empirical results and intuitiveexamples. We open source code and provide matching examples on the companionwebpage: https://github.com/changhongw/mlm.</description>
      <author>example@mail.com (Changhong Wang, Michel Olvera, Gaël Richard)</author>
      <guid isPermaLink="false">2508.00123v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity</title>
      <link>http://arxiv.org/abs/2508.00043v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究比较了两种地形神经网络约束方法(权重相似性和激活相似性)对模型性能的影响，发现权重相似性约束能产生更鲁棒的表示，提高对噪声和权重扰动的抵抗力，并增强功能定位。&lt;h4&gt;背景&lt;/h4&gt;地形神经网络是能够模拟大脑空间和功能组织的计算模型。神经网络中的地形约束可以通过多种方式实现，可能对网络学习的表示产生不同影响，但这些不同实现方式的影响尚未得到系统性研究。&lt;h4&gt;目的&lt;/h4&gt;比较两种空间约束下的地形卷积神经网络(权重相似性和激活相似性)对模型性能的影响，评估它们在分类准确率、鲁棒性和表示空间组织方面的差异。&lt;h4&gt;方法&lt;/h4&gt;使用两种空间约束训练地形卷积神经网络：权重相似性推动相邻单元发展相似的输入权重，激活相似性强制单元激活相似性。评估这些模型在分类准确率、对权重扰动和输入退化的鲁棒性以及学习表示的空间组织方面的表现。&lt;h4&gt;主要发现&lt;/h4&gt;与激活相似性和标准CNN相比，权重相似性提供了三个主要优势：提高了对噪声的鲁棒性；更高的输入敏感性，反映在更高的激活方差上；更强的功能定位，具有相似激活的单元位于更近的距离。此外，权重相似性还产生了单元方向调谐、对称敏感性和离心率剖面的差异，表明这种空间约束对网络表示几何形状的影响。&lt;h4&gt;结论&lt;/h4&gt;在端到端训练过程中，权重相似性约束比激活相似性或非地形CNN产生更鲁棒的表示。这些发现还表明，基于权重的空间约束可以在生物物理启发的模型中塑造特征学习和功能组织。&lt;h4&gt;翻译&lt;/h4&gt;地形神经网络是能够模拟大脑空间和功能组织的计算模型。神经网络中的地形约束可以通过多种方式实现，可能对网络学习的表示产生不同影响。然而，这些不同实现方式的影响尚未得到系统性研究。为此，我们比较了使用两种空间约束训练的地形卷积神经网络：权重相似性，它推动相邻单元发展相似的输入权重；和激活相似性，它强制单元激活相似性。我们评估了这些模型在分类准确率、对权重扰动和输入退化的鲁棒性以及学习表示的空间组织方面的表现。与激活相似性和标准CNN相比，权重相似性提供了三个主要优势：i) 提高了对噪声的鲁棒性，在权重损坏情况下也显示出更高的准确率；ii) 更高的输入敏感性，反映在更高的激活方差上；iii) 更强的功能定位，具有相似激活的单元位于更近的距离。此外，权重相似性还产生了单元方向调谐、对称敏感性和离心率剖面的差异，表明这种空间约束对网络表示几何形状的影响。我们的研究结果表明，在端到端训练过程中，权重相似性约束比激活相似性或非地形CNN产生更鲁棒的表示。这些发现还表明，基于权重的空间约束可以在生物物理启发的模型中塑造特征学习和功能组织。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Topographic neural networks are computational models that can simulate thespatial and functional organization of the brain. Topographic constraints inneural networks can be implemented in multiple ways, with potentially differentimpacts on the representations learned by the network. The impact of suchdifferent implementations has not been systematically examined. To this end,here we compare topographic convolutional neural networks trained with twospatial constraints: Weight Similarity (WS), which pushes neighboring units todevelop similar incoming weights, and Activation Similarity (AS), whichenforces similarity in unit activations. We evaluate the resulting models onclassification accuracy, robustness to weight perturbations and inputdegradation, and the spatial organization of learned representations. Comparedto both AS and standard CNNs, WS provided three main advantages: i) improvedrobustness to noise, also showing higher accuracy under weight corruption; ii)greater input sensitivity, reflected in higher activation variance; and iii)stronger functional localization, with units showing similar activationspositioned at closer distances. In addition, WS produced differences inorientation tuning, symmetry sensitivity, and eccentricity profiles of units,indicating an influence of this spatial constraint on the representationalgeometry of the network. Our findings suggest that during end-to-end training,WS constraints produce more robust representations than AS or non-topographicCNNs. These findings also suggest that weight-based spatial constraints canshape feature learning and functional organization in biophysical inspiredmodels.</description>
      <author>example@mail.com (Nhut Truong, Uri Hasson)</author>
      <guid isPermaLink="false">2508.00043v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items</title>
      <link>http://arxiv.org/abs/2507.22268v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MMSC的自监督多模态关系物品表征学习框架，用于推断可替代和互补物品，解决了用户行为数据噪声和数据稀疏性问题，显著提升了推荐性能。&lt;h4&gt;背景&lt;/h4&gt;现有方法主要使用图神经网络建模物品关联或利用物品内容信息，但忽略了用户行为数据噪声和长尾分布导致的数据稀疏性问题。&lt;h4&gt;目的&lt;/h4&gt;提出MMSC框架以解决用户行为数据噪声和数据稀疏性问题，提高可替代和互补物品推荐的准确性。&lt;h4&gt;方法&lt;/h4&gt;MMSC包含三个主要组件：多模态物品表征学习模块、自监督行为表征学习模块和层次化表征聚合机制，并利用大语言模型生成增强训练数据进行去噪。&lt;h4&gt;主要发现&lt;/h4&gt;在五个真实世界数据集上实验表明，MMSC在可替代推荐方面比现有基线方法高出26.1%，在互补推荐方面高出39.2%，且能有效建模冷启动物品。&lt;h4&gt;结论&lt;/h4&gt;MMSC框架通过结合多模态表征学习、自监督行为表征学习和层次化表征聚合，有效解决了用户行为数据噪声和数据稀疏性问题，显著提升了推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种新颖的自监督多模态关系物品表征学习框架，旨在推断可替代和互补物品。现有方法主要关注使用图神经网络(GNN)建模从用户行为推断的物品关联，或利用物品内容信息。然而，这些方法经常忽略关键挑战，如嘈杂的用户行为数据以及由于这些行为的长尾分布导致的数据稀疏性。在本文中，我们提出了MMSC，一个自监督多模态关系物品表征学习框架，以应对这些挑战。具体而言，MMSC包含三个主要组件：(1) 多模态物品表征学习模块，利用多模态基础模型并从物品元数据中学习；(2) 自监督行为表征学习模块，对用户行为数据进行去噪并从中学习；(3) 层次化表征聚合机制，在语义和任务两个级别整合物品表征。此外，我们利用大语言模型生成增强训练数据，进一步训练过程中的去噪。我们在五个真实世界数据集上进行了广泛实验，表明MMSC在可替代推荐方面比现有基线方法高出26.1%，在互补推荐方面高出39.2%。此外，我们经验性地证明MMSC在建模冷启动物品方面是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel self-supervised multi-modal relational itemrepresentation learning framework designed to infer substitutable andcomplementary items. Existing approaches primarily focus on modeling item-itemassociations deduced from user behaviors using graph neural networks (GNNs) orleveraging item content information. However, these methods often overlookcritical challenges, such as noisy user behavior data and data sparsity due tothe long-tailed distribution of these behaviors. In this paper, we proposeMMSC, a self-supervised multi-modal relational item representation learningframework to address these challenges. Specifically, MMSC consists of threemain components: (1) a multi-modal item representation learning module thatleverages a multi-modal foundational model and learns from item metadata, (2) aself-supervised behavior-based representation learning module that denoises andlearns from user behavior data, and (3) a hierarchical representationaggregation mechanism that integrates item representations at both the semanticand task levels. Additionally, we leverage LLMs to generate augmented trainingdata, further enhancing the denoising process during training. We conductextensive experiments on five real-world datasets, showing that MMSCoutperforms existing baselines by 26.1% for substitutable recommendation and39.2% for complementary recommendation. In addition, we empirically show thatMMSC is effective in modeling cold-start items.</description>
      <author>example@mail.com (Junting Wang, Chenghuan Guo, Jiao Yang, Yanhui Guo, Yan Gao, Hari Sundaram)</author>
      <guid isPermaLink="false">2507.22268v2</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>LargeMvC-Net: Anchor-based Deep Unfolding Network for Large-scale Multi-view Clustering</title>
      <link>http://arxiv.org/abs/2507.20980v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为LargeMvC-Net的新型深度网络架构，用于解决大规模多视图聚类问题，通过将优化问题展开为专门的模块实现更好的结构和优化可追溯性。&lt;h4&gt;背景&lt;/h4&gt;基于深度锚点的多视图聚类方法利用代表性锚点减少大规模聚类的计算复杂度，但现有方法通常以启发式或任务无关方式整合锚点结构，忽略了基于锚点的聚类的核心结构需求和优化原则。&lt;h4&gt;目的&lt;/h4&gt;重新审视大规模基于锚点的多视图聚类的底层优化问题，并将其迭代解展开为一种新颖的深度网络架构，以弥合现有方法的设计缺陷。&lt;h4&gt;方法&lt;/h4&gt;提出的模型将基于锚点的聚类过程分解为三个模块：RepresentModule（表示学习）、NoiseModule（噪声抑制）和AnchorModule（锚点指标估计）。每个模块通过将原始优化过程的步骤展开到专用网络组件中导出，并提供了一种无监督重构损失来对齐视图与锚点诱导的潜在空间。&lt;h4&gt;主要发现&lt;/h4&gt;在多个大规模多视图基准上的广泛实验表明，LargeMvC-Net在有效性和扩展性方面持续优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;LargeMvC-Net通过优化问题展开的方式提供了更好的结构和优化可追溯性，是一种有效的大规模多视图聚类解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于深度锚点的多视图聚类方法通过利用代表性锚点来减少大规模聚类的计算复杂度，从而提高神经网络的扩展性。尽管它们具有扩展性优势，但现有方法通常以启发式或任务无关的方式整合锚点结构，通过事后图构建或作为消息传递的辅助组件。此类设计忽略了基于锚点的聚类的核心结构需求，忽视了关键的优化原则。为了弥合这一差距，我们重新审视了大规模基于锚点的多视图聚类的底层优化问题，并将其迭代解展开为一种新颖的深度网络架构，称为LargeMvC-Net。所提出的模型将基于锚点的聚类过程分解为三个模块：RepresentModule、NoiseModule和AnchorModule，分别对应表示学习、噪声抑制和锚点指标估计。每个模块都是通过将原始优化过程的步骤展开到专用的网络组件中导出的，提供了结构清晰性和优化可追溯性。此外，一种无监督的重构损失将每个视图与锚点诱导的潜在空间对齐，鼓励跨视图保持一致的聚类结构。在几个大规模多视图基准上的广泛实验表明，LargeMvC-Net在有效性和扩展性方面都持续优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep anchor-based multi-view clustering methods enhance the scalability ofneural networks by utilizing representative anchors to reduce the computationalcomplexity of large-scale clustering. Despite their scalability advantages,existing approaches often incorporate anchor structures in a heuristic ortask-agnostic manner, either through post-hoc graph construction or asauxiliary components for message passing. Such designs overlook the corestructural demands of anchor-based clustering, neglecting key optimizationprinciples. To bridge this gap, we revisit the underlying optimization problemof large-scale anchor-based multi-view clustering and unfold its iterativesolution into a novel deep network architecture, termed LargeMvC-Net. Theproposed model decomposes the anchor-based clustering process into threemodules: RepresentModule, NoiseModule, and AnchorModule, corresponding torepresentation learning, noise suppression, and anchor indicator estimation.Each module is derived by unfolding a step of the original optimizationprocedure into a dedicated network component, providing structural clarity andoptimization traceability. In addition, an unsupervised reconstruction lossaligns each view with the anchor-induced latent space, encouraging consistentclustering structures across views. Extensive experiments on severallarge-scale multi-view benchmarks show that LargeMvC-Net consistentlyoutperforms state-of-the-art methods in terms of both effectiveness andscalability.</description>
      <author>example@mail.com (Shide Du, Chunming Wu, Zihan Fang, Wendi Zhao, Yilin Wu, Changwei Wang, Shiping Wang)</author>
      <guid isPermaLink="false">2507.20980v2</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Three-dimentional reconstruction of complex, dynamic population canopy architecture for crops with a novel point cloud completion model: A case study in Brassica napus rapeseed</title>
      <link>http://arxiv.org/abs/2506.18292v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为CP-PCN的方法，用于油菜作物复杂动态群体冠层架构的三维重建，通过点云补全模型解决冠层内部遮挡问题，实现了高精度的冠层架构描述，提高了产量预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确描述完整冠层架构对于评估作物光合作用和产量性能以及指导理想株型设计至关重要。尽管已有各种传感技术用于单株和冠层的三维重建，但由于复杂冠层架构之间的严重遮挡，这些技术未能获得准确的冠层架构描述。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的方法，用于油菜作物复杂动态群体冠层架构的三维重建，以解决冠层内部遮挡问题，从而更准确地评估作物光合作用和产量性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种点云补全模型CP-PCN（作物群体点云补全网络）；开发了完整的点云生成框架，用于自动标注训练数据集，区分冠层内的表面点和遮挡点；设计了包含多分辨率动态图卷积编码器(MRDG)和点金字塔解码器(PPD)的CP-PCN网络；提出了动态图卷积特征提取器(DGCFE)模块，以捕获整个油菜生长过程中的结构变化。&lt;h4&gt;主要发现&lt;/h4&gt;CP-PCN在四个生长阶段上的查普距离(CD)值达到3.35厘米-4.51厘米，优于基于Transformer的最先进方法(PoinTr)；消融研究证实了MRDG和DGCFE模块的有效性；使用CP-PCN开发的角果效率指数相比使用不完整点云，提高了油菜产量预测的整体准确性11.2%。&lt;h4&gt;结论&lt;/h4&gt;CP-PCN流程有潜力扩展到其他作物，显著推进田间群体冠层架构的定量分析，为作物光合作用评估和产量预测提供了更准确的方法。&lt;h4&gt;翻译&lt;/h4&gt;完整冠层架构的定量描述对于准确评估作物光合作用和产量性能以及指导理想株型设计至关重要。尽管已经开发了各种传感技术用于单株和冠层的三维重建，但由于复杂冠层架构之间的严重遮挡，它们未能获得准确的冠层架构描述。我们提出了一种有效的方法，使用新颖的点云补全模型进行油菜作物复杂动态群体冠层架构的三维重建。开发了完整的点云生成框架，用于通过区分冠层内的表面点和遮挡点来自动标注训练数据集。随后设计了作物群体点云补全网络(CP-PCN)，采用多分辨率动态图卷积编码器(MRDG)和点金字塔解码器(PPD)来预测遮挡点。为了进一步增强特征提取，提出了动态图卷积特征提取器(DGCFE)模块，以捕获整个油菜生长过程中的结构变化。结果表明，CP-PCN在四个生长阶段上的查普距离(CD)值达到3.35厘米-4.51厘米，优于基于Transformer的最先进方法(PoinTr)。消融研究证实了MRDG和DGCFE模块的有效性。此外，验证实验表明，从CP-PCN开发的角果效率指数相比使用不完整点云，提高了油菜产量预测的整体准确性11.2%。CP-PCN流程有潜力扩展到其他作物，显著推进田间群体冠层架构的定量分析。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何准确重建复杂、动态的作物群体冠层三维结构的问题。由于冠层内部严重遮挡，现有三维重建技术无法获取完整的冠层信息，特别是内部被遮挡部分。这个问题很重要，因为完整的冠层结构定量描述对于准确评估作物光合作用和产量性能至关重要，直接影响光分布和截获效率，进而影响作物产量。理想冠层结构设计对于提高作物产量潜力，特别是对于油菜这种生长周期中冠层结构多样性大的作物尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有三维重建技术的局限性：深度相机和激光扫描仅适用于简单目标；LiDAR成本高且存在光吸收问题；多视图成像仍无法获取完整冠层结构。同时，现有点云补全方法主要针对刚性物体设计，难以处理复杂植物冠层。基于这些分析，作者借鉴了生成对抗网络(GAN)框架和图神经网络(GNN)处理点云数据的思想，设计了专门针对作物群体冠层的CP-PCN网络。作者还创新性地引入多分辨率特征提取和动态图卷积策略，以适应不同生长阶段的冠层结构变化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用虚拟-真实集成(VRI)方法生成高质量训练数据集，设计专门针对作物群体冠层的点云补全网络CP-PCN，利用多分辨率动态图卷积编码器(MRDG)和点金字塔解码器(PPD)预测被遮挡点，并通过动态图卷积特征提取器(DGCFE)捕捉生长周期中的结构变化。整体流程包括：1)收集个体植物3D数据并基于种植模式生成完整群体冠层；2)使用遮挡点检测算法区分表面点和被遮挡点；3)训练CP-PCN模型，输入表面点云预测被遮挡点；4)将模型应用于田间采集的不完整点云，补全冠层结构；5)从完整点云提取特征并用于产量估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出虚拟-真实集成(VRI)方法和遮挡点检测算法，生成更真实的训练数据集；2)设计CP-PCN网络，提出多分辨率动态图卷积编码器(MRDG)和动态图卷积特征提取器(DGCFE)；3)提出角果效率指数(SEI)作为产量评估指标。相比之前工作，本文专注于作物群体而非单株冠层重建，考虑了生长过程中的结构动态变化，解决了冠层内部严重遮挡问题，并将完整冠层结构与产量预测相结合，方法可扩展到其他作物，推动了田间群体冠层结构的定量分析。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出创新的点云补全模型和虚拟-真实集成数据生成方法，实现了复杂作物群体冠层结构的高精度三维重建，显著提高了产量预测精度，为作物理想株型设计提供了新的技术手段。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-06-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantitative descriptions of the complete canopy architecture are essentialfor accurately evaluating crop photosynthesis and yield performance to guideideotype design. Although various sensing technologies have been developed forthree-dimensional (3D) reconstruction of individual plants and canopies, theyfailed to obtain an accurate description of canopy architectures due to severeocclusion among complex canopy architectures. We proposed an effective methodfor 3D reconstruction of complex, dynamic population canopy architecture forrapeseed crops with a novel point cloud completion model. A complete pointcloud generation framework was developed for automated annotation of thetraining dataset by distinguishing surface points from occluded points withincanopies. The crop population point cloud completion network (CP-PCN) was thendesigned with a multi-resolution dynamic graph convolutional encoder (MRDG) anda point pyramid decoder (PPD) to predict occluded points. To further enhancefeature extraction, a dynamic graph convolutional feature extractor (DGCFE)module was proposed to capture structural variations over the whole rapeseedgrowth period. The results demonstrated that CP-PCN achieved chamfer distance(CD) values of 3.35 cm -4.51 cm over four growth stages, outperforming thestate-of-the-art transformer-based method (PoinTr). Ablation studies confirmedthe effectiveness of the MRDG and DGCFE modules. Moreover, the validationexperiment demonstrated that the silique efficiency index developed from CP-PCNimproved the overall accuracy of rapeseed yield prediction by 11.2% compared tothat of using incomplete point clouds. The CP-PCN pipeline has the potential tobe extended to other crops, significantly advancing the quantitatively analysisof in-field population canopy architectures.</description>
      <author>example@mail.com (Ziyue Guo, Xin Yang, Yutao Shen, Yang Zhu, Lixi Jiang, Haiyan Cen)</author>
      <guid isPermaLink="false">2506.18292v2</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>GECO: Geometrically Consistent Embedding with Lightspeed Inference</title>
      <link>http://arxiv.org/abs/2508.00746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GECO是一种基于最优传输的自监督视觉框架，能够产生几何一致的特征，有效区分不同部分的几何特性，同时实现高效运行。&lt;h4&gt;背景&lt;/h4&gt;自监督视觉基础模型能够捕捉语义对应关系，但通常缺乏对底层3D几何结构的感知能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法缺乏对底层3D几何结构感知的问题，开发能基于几何区分不同部分的视觉特征表示方法。&lt;h4&gt;方法&lt;/h4&gt;提出基于最优传输的训练框架GECO，超越关键点监督，支持在遮挡和遮挡解除情况下的训练，采用轻量级架构实现高效运行。&lt;h4&gt;主要发现&lt;/h4&gt;GECO比之前的方法快98.2%，在PFPascal、APK和CUB数据集上达到最先进性能，PCK指标分别提高了6.0%、6.2%和4.1%。同时发现PCK指标不足以捕捉几何质量。&lt;h4&gt;结论&lt;/h4&gt;GECO成功实现了几何一致的特征学习，为更几何感知的特征学习提供了新指标和见解。&lt;h4&gt;翻译&lt;/h4&gt;最近的特征学习进展表明，自监督视觉基础模型能够捕捉语义对应关系，但通常缺乏对底层3D几何结构的感知能力。GECO通过产生几何一致的特征来填补这一空白，这些特征能够基于几何语义区分不同部分（如左右眼、前后腿）。我们提出了一种基于最优传输的训练框架，使得监督可以超越关键点，即使在遮挡和遮挡解除的情况下也能工作。凭借轻量级架构，GECO以30fps的速度运行，比之前的方法快98.2%，同时在PFPascal、APK和CUB上取得了最先进的性能，分别将PCK提高了6.0%、6.2%和4.1%。最后，我们表明单独的PCK不足以捕捉几何质量，并为更几何感知的特征学习引入了新的指标和见解。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉基础模型缺乏对底层3D几何认识的问题，特别是难以区分对称部分（如左右眼、前后腿）的几何属性。这个问题很重要，因为缺乏几何意识会导致实际应用中出现严重的人工制品，例如在3D重建中错误重建动物或家具，而现有几何感知方法速度太慢难以实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性：它们依赖基于argmax的分配，无法处理遮挡情况，只提供稀疏监督信号。借鉴了最优传输（特别是Sinkhorn算法）的概念来实现可微的软分配公式，处理部分分配问题并提供更强监督。同时使用了DINOv2作为预训练模型和LoRA适配技术进行微调，这些都是对现有工作的合理借鉴。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用最优传输学习几何一致的特征表示，处理部分分配问题并提供更强监督。流程：1)使用DINOv2-B模型和LoRA适配器；2)计算图像特征间的余弦相似度矩阵；3)增强矩阵添加bin；4)通过可微分OT层计算最优传输计划；5)使用KL正则化软分配；6)应用二元交叉熵损失训练模型预测正确分配。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)基于最优传输的软分配损失函数和轻量级架构；2)不同的最优传输模块边缘分布创建方式，无需可训练参数；3)作为特征编码器而非特征匹配器；4)提出新评估指标PGCK补充传统PCK。相比Geo方法，GECO速度更快(30fps，快98.2%)、模型更小(332MB vs 9GB)、性能更优，且不依赖交叉注意力机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GECO通过基于最优传输的软分配损失函数和轻量级架构，实现了高效且几何一致的视觉特征学习，在保持实时性能的同时显著提升了关键点匹配的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in feature learning have shown that self-supervised visionfoundation models can capture semantic correspondences but often lack awarenessof underlying 3D geometry. GECO addresses this gap by producing geometricallycoherent features that semantically distinguish parts based on geometry (e.g.,left/right eyes, front/back legs). We propose a training framework based onoptimal transport, enabling supervision beyond keypoints, even under occlusionsand disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%faster than prior methods, while achieving state-of-the-art performance onPFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.Finally, we show that PCK alone is insufficient to capture geometric qualityand introduce new metrics and insights for more geometry-aware featurelearning. Link to project page:https://reginehartwig.github.io/publications/geco/</description>
      <author>example@mail.com (Regine Hartwig, Dominik Muhle, Riccardo Marin, Daniel Cremers)</author>
      <guid isPermaLink="false">2508.00746v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems</title>
      <link>http://arxiv.org/abs/2508.00721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FMPlug是一种新型插件框架，用于增强基础流匹配(FM)先验以解决病态逆问题。它利用观测对象与期望对象间的相似性以及生成流的高斯性，通过时间自适应预热策略和强化高斯性正则化，释放了领域不可知基础模型的潜力，并在图像处理任务上超越了现有方法。&lt;h4&gt;背景&lt;/h4&gt;传统解决病态逆问题的方法依赖于领域特定或未训练的先验，存在局限性。&lt;h4&gt;目的&lt;/h4&gt;增强基础流匹配(FM)先验，解决病态逆问题，并释放领域不可知基础模型的真正潜力。&lt;h4&gt;方法&lt;/h4&gt;开发FMPlug插件框架，利用观测对象与期望对象之间的相似性以及生成流的高斯性两个关键洞见；引入时间自适应预热策略和强化高斯性正则化技术。&lt;h4&gt;主要发现&lt;/h4&gt;FMPlug在图像超分辨率和高斯去模糊任务上，以显著优势击败了使用基础FM先验的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;FMPlug通过巧妙利用简单而强大的洞见，结合时间自适应预热策略和强化高斯性正则化，成功释放了领域不可知基础模型的真正潜力，有效解决了病态逆问题。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了FMPlug，一种新颖的插件框架，用于增强基础流匹配(FM)先验以解决病态逆问题。与传统依赖于领域特定或未训练先验的方法不同，FMPlug巧妙地利用了两个简单而强大的洞见：观测对象与期望对象之间的相似性以及生成流的高斯性。通过引入时间自适应预热策略和强化的高斯性正则化，FMPlug释放了领域不可知基础模型的真正潜力。我们的方法在图像超分辨率和高斯去模糊任务上，以显著优势击败了使用基础FM先验的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present FMPlug, a novel plug-in framework that enhances foundationflow-matching (FM) priors for solving ill-posed inverse problems. Unliketraditional approaches that rely on domain-specific or untrained priors, FMPlugsmartly leverages two simple but powerful insights: the similarity betweenobserved and desired objects and the Gaussianity of generative flows. Byintroducing a time-adaptive warm-up strategy and sharp Gaussianityregularization, FMPlug unlocks the true potential of domain-agnostic foundationmodels. Our method beats state-of-the-art methods that use foundation FM priorsby significant margins, on image super-resolution and Gaussian deblurring.</description>
      <author>example@mail.com (Yuxiang Wan, Ryan Devera, Wenjie Zhang, Ju Sun)</author>
      <guid isPermaLink="false">2508.00721v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources</title>
      <link>http://arxiv.org/abs/2508.00627v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;IAMAP是一个用户友好的QGIS插件，解决了深度学习在遥感应用中的三个主要限制：需要大量数据集、计算资源和编程技能。它利用自监督学习策略提供的特征提取器，使非AI专业人员能够高效使用深度学习方法。&lt;h4&gt;背景&lt;/h4&gt;遥感技术随着人工智能方法的快速发展已进入新时代，但深度学习的实施主要局限于专业人士且不实用，因为它需要大量参考数据集、计算资源和编程技能。&lt;h4&gt;目的&lt;/h4&gt;开发IAMAP插件，以一种简单而灵活的方式解决深度学习在遥感应用中的三个主要挑战，使非AI专业人员也能利用这些先进技术。&lt;h4&gt;方法&lt;/h4&gt;IAMAP基于自监督学习策略提供的基础模型，提供界面让用户执行：(i)使用多种深度学习架构提取图像特征；(ii)使用内置算法降维；(iii)对特征或其降维表示进行聚类；(iv)生成特征相似性地图；(v)校准和验证监督机器学习模型进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;通过使非AI专业人员能够利用最新深度学习方法提供的高质量特征，而无需GPU容量或大量参考数据集，IAMAP实现了计算高效且节能的深度学习方法。&lt;h4&gt;结论&lt;/h4&gt;IAMAP推动了计算高效且节能的深度学习方法的普及化，降低了遥感技术中深度学习的应用门槛。&lt;h4&gt;翻译&lt;/h4&gt;遥感技术随着人工智能方法的快速发展已进入新时代。然而，深度学习的实施主要局限于专业人士，且不实用，因为它通常需要：(i)大量的参考数据集用于模型训练和验证；(ii)大量的计算资源；(iii)强大的编程技能。在这里，我们介绍IAMAP，一个用户友好的QGIS插件，以一种简单而灵活的方式解决这三个挑战。IAMAP建立在最新的自监督学习策略进展基础上，这些策略提供了强大的特征提取器，通常被称为基础模型。这些通用模型可以在少样本或零样本场景中可靠使用（即很少或不需要微调）。IAMAP的界面允许用户简化遥感图像分析中的几个关键步骤：(i)使用各种深度学习架构提取图像特征；(ii)使用内置算法降维；(iii)对特征或其降维表示进行聚类；(iv)生成特征相似性地图；(v)校准和验证监督机器学习模型以进行预测。通过使非AI专业人员能够利用最新深度学习方法提供的高质量特征，而无需GPU容量或大量参考数据集，IAMAP有助于实现计算高效且节能的深度学习方法的普及化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote sensing has entered a new era with the rapid development of artificialintelligence approaches. However, the implementation of deep learning haslargely remained restricted to specialists and has been impractical because itoften requires (i) large reference datasets for model training and validation;(ii) substantial computing resources; and (iii) strong coding skills. Here, weintroduce IAMAP, a user-friendly QGIS plugin that addresses these threechallenges in an easy yet flexible way. IAMAP builds on recent advancements inself-supervised learning strategies, which now provide robust featureextractors, often referred to as foundation models. These generalist models canoften be reliably used in few-shot or zero-shot scenarios (i.e., with little tono fine-tuning). IAMAP's interface allows users to streamline several key stepsin remote sensing image analysis: (i) extracting image features using a widerange of deep learning architectures; (ii) reducing dimensionality withbuilt-in algorithms; (iii) performing clustering on features or their reducedrepresentations; (iv) generating feature similarity maps; and (v) calibratingand validating supervised machine learning models for prediction. By enablingnon-AI specialists to leverage the high-quality features provided by recentdeep learning approaches without requiring GPU capacity or extensive referencedatasets, IAMAP contributes to the democratization of computationally efficientand energy-conscious deep learning methods.</description>
      <author>example@mail.com (Paul Tresson, Pierre Le Coz, Hadrien Tulet, Anthony Malkassian, Maxime Réjou Méchain)</author>
      <guid isPermaLink="false">2508.00627v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Wireless Networks for IoT with Large Vision Models: Foundations and Applications</title>
      <link>http://arxiv.org/abs/2508.00583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了大型视觉模型(LVMs)在视觉智能中的基础性作用及其在物联网场景中的应用。作者研究了LVM的功能和核心架构，探索了其在无线通信各层的应用，并提出了一种渐进式微调框架，通过案例研究证明了其在无人机互联网任务中优于传统CNNs。&lt;h4&gt;背景&lt;/h4&gt;大型视觉模型已成为视觉智能的基础性范式，在多样化视觉任务中取得最先进性能。LVMs的最新进展促进了它们在物联网场景中的集成，为视觉辅助的网络优化提供了更好的泛化能力和适应性。&lt;h4&gt;目的&lt;/h4&gt;研究LVM的功能和核心架构，探索其在无线通信中的应用，针对LVM模型大和无线领域重新训练挑战提出渐进式微调框架，并通过案例研究验证框架有效性。&lt;h4&gt;方法&lt;/h4&gt;分析LVM在分类、分割、生成和多模态视觉处理方面的能力；探索LVM在无线通信物理层、网络层和应用层的应用；提出渐进式微调框架逐步调整预训练LVMs以优化多个IoT任务；在低空经济网络中进行案例研究比较与传统CNNs的性能差异。&lt;h4&gt;主要发现&lt;/h4&gt;大型视觉模型在视觉智能领域表现出色且适用性广；LVMs能有效集成到物联网场景提供更好的泛化能力和适应性；所提出的渐进式微调框架能有效适应预训练LVMs用于多任务优化；在波束成形和定位任务中，所提框架优于传统CNNs。&lt;h4&gt;结论&lt;/h4&gt;将大型视觉模型集成到智能无线系统是一个有前景的方向，特别是在处理多任务优化时。渐进式微调框架为在资源受限的无线环境中有效利用LVMs提供了有效方法。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉模型已成为视觉智能的基础性范式，在多样化的视觉任务中取得了最先进的性能。LVMs的最新进展促进了它们在物联网场景中的集成，为视觉辅助的网络优化提供了更好的泛化能力和适应性。在本文中，我们首先研究了LVM的功能和核心架构，强调了它们在分类、分割、生成和多模态视觉处理方面的能力。然后，我们探索了LVM在无线通信中的各种应用，涵盖了物理层、网络层和应用层的代表性任务。此外，鉴于LVM的模型规模大以及无线领域模型重新训练的挑战，我们提出了一种渐进式微调框架，逐步调整预训练的LVMs，以优化多个IoT任务。在低空经济网络中的案例研究证明了所提框架在无人机互联网的波束成形和定位任务中优于传统CNNs，强调了将LVMs集成到智能无线系统中的有前景方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large vision models (LVMs) have emerged as a foundational paradigm in visualintelligence, achieving state-of-the-art performance across diverse visualtasks. Recent advances in LVMs have facilitated their integration into Internetof Things (IoT) scenarios, offering superior generalization and adaptabilityfor vision-assisted network optimization. In this paper, we first investigatethe functionalities and core architectures of LVMs, highlighting theircapabilities across classification, segmentation, generation, and multimodalvisual processing. We then explore a variety of LVM applications in wirelesscommunications, covering representative tasks across the physical layer,network layer, and application layer. Furthermore, given the substantial modelsize of LVMs and the challenges of model retraining in wireless domains, wepropose a progressive fine-tuning framework that incrementally adaptspretrained LVMs for joint optimization of multiple IoT tasks. A case study inlow-altitude economy networks (LAENets) demonstrates the effectiveness of theproposed framework over conventional CNNs in joint beamforming and positioningtasks for Internet of drones, underscoring a promising direction forintegrating LVMs into intelligent wireless systems.</description>
      <author>example@mail.com (Yunting Xu, Jiacheng Wang, Ruichen Zhang, Dusit Niyato, Deepu Rajan, Liang Yu, Haibo Zhou, Abbas Jamalipour, Xianbin Wang)</author>
      <guid isPermaLink="false">2508.00583v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer</title>
      <link>http://arxiv.org/abs/2508.00477v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LAMIC是一种布局感知的多图像合成框架，首次以无需训练的方式将单参考扩散模型扩展到多参考场景。它通过两种即插即用的注意力机制实现了身份保持、背景保留、布局控制和提示跟随等能力，无需任何训练或微调，展示了强大的零样本泛化能力。&lt;h4&gt;背景&lt;/h4&gt;在可控图像合成领域，从多个参考图像生成连贯且一致的图像，同时保持空间布局意识仍然是一个开放的挑战。现有的多参考基线方法在性能上存在局限。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需训练的框架，能够从多个参考图像生成连贯且一致的图像，同时保持空间布局意识，并超越现有多参考基线方法的性能。&lt;h4&gt;方法&lt;/h4&gt;LAMIC基于MMDiT模型，引入了两种即插即用的注意力机制：1）组隔离注意力（GIA）以增强实体解耦；2）区域调制注意力（RMA）以实现布局感知的生成。同时，引入了三个评估指标：包含比率（IN-R）和填充比率（FI-R）用于评估布局控制；背景相似度（BG-S）用于测量背景一致性。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，LAMIC在大多数主要指标上达到了最先进的性能：在所有设置中，它在ID-S、BG-S、IN-R和平均分数方面持续优于现有的多参考基线，并在复杂合成任务中取得了最佳的DPG。这些结果证明了LAMIC在身份保持、背景保留、布局控制和提示跟随方面的优越能力。&lt;h4&gt;结论&lt;/h4&gt;通过继承先进单参考模型的优势并实现向多图像场景的无缝扩展，LAMIC为可控多图像合成建立了一种新的无需训练的范式。随着基础模型的不断发展，LAMIC的性能有望相应提升。该研究的实现可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;在可控图像合成中，从多个参考图像生成具有空间布局意识的连贯且一致的图像仍然是一个开放的挑战。我们提出了LAMIC，一种布局感知的多图像合成框架，首次以无需训练的方式将单参考扩散模型扩展到多参考场景。基于MMDiT模型，LAMIC引入了两种即插即用的注意力机制：1）组隔离注意力（GIA）以增强实体解耦；2）区域调制注意力（RMA）以实现布局感知的生成。为了全面评估模型能力，我们进一步引入了三个指标：1）包含比率（IN-R）和填充比率（FI-R）用于评估布局控制；2）背景相似度（BG-S）用于测量背景一致性。广泛的实验表明，LAMIC在大多数主要指标上达到了最先进的性能：在所有设置中，它在ID-S、BG-S、IN-R和平均分数方面持续优于现有的多参考基线，并在复杂合成任务中取得了最佳的DPG。这些结果证明了LAMIC在身份保持、背景保留、布局控制和提示跟随方面的优越能力，均无需任何训练或微调，展示了强大的零样本泛化能力。通过继承先进单参考模型的优势并实现向多图像场景的无缝扩展，LAMIC为可控多图像合成建立了一种新的无需训练的范式。随着基础模型的不断发展，LAMIC的性能有望相应提升。我们的实现在https://github.com/Suchenl/LAMIC上可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In controllable image synthesis, generating coherent and consistent imagesfrom multiple references with spatial layout awareness remains an openchallenge. We present LAMIC, a Layout-Aware Multi-Image Composition frameworkthat, for the first time, extends single-reference diffusion models tomulti-reference scenarios in a training-free manner. Built upon the MMDiTmodel, LAMIC introduces two plug-and-play attention mechanisms: 1) GroupIsolation Attention (GIA) to enhance entity disentanglement; and 2)Region-Modulated Attention (RMA) to enable layout-aware generation. Tocomprehensively evaluate model capabilities, we further introduce threemetrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layoutcontrol; and 2) Background Similarity (BG-S) for measuring backgroundconsistency. Extensive experiments show that LAMIC achieves state-of-the-artperformance across most major metrics: it consistently outperforms existingmulti-reference baselines in ID-S, BG-S, IN-R and AVG scores across allsettings, and achieves the best DPG in complex composition tasks. These resultsdemonstrate LAMIC's superior abilities in identity keeping, backgroundpreservation, layout control, and prompt-following, all achieved without anytraining or fine-tuning, showcasing strong zero-shot generalization ability. Byinheriting the strengths of advanced single-reference models and enablingseamless extension to multi-image scenarios, LAMIC establishes a newtraining-free paradigm for controllable multi-image composition. As foundationmodels continue to evolve, LAMIC's performance is expected to scaleaccordingly. Our implementation is available at:https://github.com/Suchenl/LAMIC.</description>
      <author>example@mail.com (Yuzhuo Chen, Zehua Ma, Jianhua Wang, Kai Kang, Shunyu Yao, Weiming Zhang)</author>
      <guid isPermaLink="false">2508.00477v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training</title>
      <link>http://arxiv.org/abs/2508.00414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Cognitive Kernel-Pro，一个完全开源且尽可能免费的模块化AI智能体框架，旨在促进先进AI智能体的开发和评估民主化。该框架在GAIA基准测试中取得了最先进的结果，其8B参数模型超越了之前的领先系统。&lt;h4&gt;背景&lt;/h4&gt;通用AI智能体被认为是下一代人工智能的基础框架，具有复杂推理、网络交互、编码和自主研究能力。然而，当前智能体系统要么是闭源的，要么严重依赖各种付费API和专有工具，限制了研究社区的访问性和可复现性。&lt;h4&gt;目的&lt;/h4&gt;开发一个完全开源且尽可能免费的模块化智能体框架，促进先进AI智能体的开发和评估民主化。&lt;h4&gt;方法&lt;/h4&gt;系统性地研究智能体基础模型的高质量训练数据整理，重点关注四个关键领域（网络、文件、代码和通用推理）中查询、轨迹和可验证答案的构建；探索新颖的智能体测试时反思和投票策略，以增强智能体的鲁棒性和性能。&lt;h4&gt;主要发现&lt;/h4&gt;在GAIA上评估Cognitive Kernel-Pro，在开源和免费智能体中取得了最先进的结果；8B参数开源模型超越了之前的领先系统如WebDancer和WebSailor，为可访问的高能力AI智能体建立了新的性能标准。&lt;h4&gt;结论&lt;/h4&gt;Cognitive Kernel-Pro为研究社区提供了一个可访问、高性能的AI智能体框架，促进了AI智能体研究和开发的民主化。&lt;h4&gt;翻译&lt;/h4&gt;通用AI智能体正越来越多地被视为下一代人工智能的基础框架，使复杂推理、网络交互、编码和自主研究成为可能。然而，当前的智能体系统要么是闭源的，要么严重依赖各种付费API和专有工具，这限制了研究社区的访问性和可复现性。在本工作中，我们提出了Cognitive Kernel-Pro，这是一个完全开源且尽可能免费的多模块智能体框架，旨在促进先进AI智能体的开发和评估民主化。在Cognitive Kernel-Pro中，我们系统性地研究了智能体基础模型的高质量训练数据的整理，重点关注四个关键领域（网络、文件、代码和通用推理）中查询、轨迹和可验证答案的构建。此外，我们探索了新颖的智能体测试时反思和投票策略，以增强智能体的鲁棒性和性能。我们在GAIA上评估了Cognitive Kernel-Pro，在开源和免费智能体中取得了最先进的结果。值得注意的是，我们的8B参数开源模型超越了之前的领先系统如WebDancer和WebSailor，为可访问的高能力AI智能体建立了新的性能标准。代码可在https://github.com/Tencent/CognitiveKernel-Pro获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; General AI Agents are increasingly recognized as foundational frameworks forthe next generation of artificial intelligence, enabling complex reasoning, webinteraction, coding, and autonomous research capabilities. However, currentagent systems are either closed-source or heavily reliant on a variety of paidAPIs and proprietary tools, limiting accessibility and reproducibility for theresearch community. In this work, we present \textbf{Cognitive Kernel-Pro}, afully open-source and (to the maximum extent) free multi-module agent frameworkdesigned to democratize the development and evaluation of advanced AI agents.Within Cognitive Kernel-Pro, we systematically investigate the curation ofhigh-quality training data for Agent Foundation Models, focusing on theconstruction of queries, trajectories, and verifiable answers across four keydomains: web, file, code, and general reasoning. Furthermore, we explore novelstrategies for agent test-time reflection and voting to enhance agentrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achievingstate-of-the-art results among open-source and free agents. Notably, our8B-parameter open-source model surpasses previous leading systems such asWebDancer and WebSailor, establishing a new performance standard foraccessible, high-capability AI agents. Code is available athttps://github.com/Tencent/CognitiveKernel-Pro</description>
      <author>example@mail.com (Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, Dong Yu)</author>
      <guid isPermaLink="false">2508.00414v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition</title>
      <link>http://arxiv.org/abs/2508.00391v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Cued-Agent的协作多智能体系统，用于自动Cued Speech识别，解决了传统方法中因数据有限导致的多模态融合性能不佳问题。&lt;h4&gt;背景&lt;/h4&gt;Cued Speech是一种结合唇读和手部编码的视觉沟通系统，用于帮助听力障碍人士交流。传统自动CS识别方法因手部和唇部动作的时间异步性需要复杂模块，但受限于数据不足，导致融合机制训练不充分，性能不佳。&lt;h4&gt;目的&lt;/h4&gt;开发首个用于自动Cued Speech识别的协作多智能体系统，克服数据限制，提高识别性能。&lt;h4&gt;方法&lt;/h4&gt;Cued-Agent整合四个专门子智能体：基于多模态大型语言模型的手部识别智能体、基于预训练Transformer的唇部识别智能体、手部提示解码智能体和自我校正音素到词智能体。同时扩展了现有普通话CS数据集，收集了八名听力障碍者的数据，建立了包含十四名受试者的混合数据集。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，与最先进方法相比，Cued-Agent在正常和听力障碍场景下均表现出色。&lt;h4&gt;结论&lt;/h4&gt;Cued-Agent通过多智能体协作有效解决了数据有限条件下的多模态融合问题，显著提升了自动Cued Speech识别性能。&lt;h4&gt;翻译&lt;/h4&gt;Cued Speech (CS) 是一种结合唇读和手部编码的视觉沟通系统，旨在帮助听力障碍人士进行交流。自动CS识别(ACSR)的目标是通过AI驱动的方法将CS手势和唇部动作转换为文本。传统上，手部和唇部动作的时间异步性需要设计复杂模块以实现有效的多模态融合。然而，受限于数据可用性不足，当前方法在训练这些融合机制时能力不够，导致性能不佳。最近，多智能体系统在处理数据有限条件下的复杂任务方面显示出良好的能力。为此，我们提出了首个用于ACSR的协作多智能体系统，名为Cued-Agent。它整合了四个专门的子智能体：一个基于多模态大型语言模型的手部识别智能体，采用关键帧筛选和CS专家提示策略解码手部动作；一个基于预训练Transformer的唇部识别智能体，从输入视频中提取唇部特征；一个手部提示解码智能体，以无需训练的方式在推理过程中动态整合手部提示和唇部特征；以及一个自我校正音素到词智能体，首次通过语义优化实现音素序列到自然语言句子的后处理和端到端转换。为支持本研究，我们从八名听力障碍的手语使用者那里收集数据，扩展了现有的普通话CS数据集，建立了包含十四名受试者的混合数据集。大量实验表明，与最先进方法相比，我们的Cued-Agent在正常和听力障碍场景下均表现出色。实现可在https://github.com/DennisHgj/Cued-Agent获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cued Speech (CS) is a visual communication system that combines lip-readingwith hand coding to facilitate communication for individuals with hearingimpairments. Automatic CS Recognition (ACSR) aims to convert CS hand gesturesand lip movements into text via AI-driven methods. Traditionally, the temporalasynchrony between hand and lip movements requires the design of complexmodules to facilitate effective multimodal fusion. However, constrained bylimited data availability, current methods demonstrate insufficient capacityfor adequately training these fusion mechanisms, resulting in suboptimalperformance. Recently, multi-agent systems have shown promising capabilities inhandling complex tasks with limited data availability. To this end, we proposethe first collaborative multi-agent system for ACSR, named Cued-Agent. Itintegrates four specialized sub-agents: a Multimodal Large Language Model-basedHand Recognition agent that employs keyframe screening and CS expert promptstrategies to decode hand movements, a pretrained Transformer-based LipRecognition agent that extracts lip features from the input video, a HandPrompt Decoding agent that dynamically integrates hand prompts with lipfeatures during inference in a training-free manner, and a Self-CorrectionPhoneme-to-Word agent that enables post-process and end-to-end conversion fromphoneme sequences to natural language sentences for the first time throughsemantic refinement. To support this study, we expand the existing Mandarin CSdataset by collecting data from eight hearing-impaired cuers, establishing amixed dataset of fourteen subjects. Extensive experiments demonstrate that ourCued-Agent performs superbly in both normal and hearing-impaired scenarioscompared with state-of-the-art methods. The implementation is available athttps://github.com/DennisHgj/Cued-Agent.</description>
      <author>example@mail.com (Guanjie Huang, Danny H. K. Tsang, Shan Yang, Guangzhi Lei, Li Liu)</author>
      <guid isPermaLink="false">2508.00391v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.00383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted (Oral) in MICCAI 2025 COMPAYL Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为MVHybrid的新型混合主干架构，结合状态空间模型与Vision Transformer，用于从组织病理学图像预测空间基因表达，在多项评估中表现优于传统ViT模型。&lt;h4&gt;背景&lt;/h4&gt;空间转录组学可在组织环境中揭示基因表达模式，支持精准肿瘤学应用，但高成本和技术复杂性限制了临床应用。从常规组织病理学图像预测空间基因表达是一种替代方法，但当前基于Vision Transformer的病理学视觉基础模型表现低于临床标准。&lt;h4&gt;目的&lt;/h4&gt;探索超越ViT架构的创新，以更好地捕捉与分子表型相关的低频、细微形态模式，并开发一种新的混合主干架构。&lt;h4&gt;方法&lt;/h4&gt;展示具有负实特征值初始化的状态空间模型表现出强烈的低频偏差；引入MVHybrid混合架构；比较五种不同主干架构；所有模型在相同结直肠癌数据集上使用DINOv2自监督学习预训练；通过随机分割和留一研究法(LOSO)评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;在LOSO评估中，MVHybrid比最佳ViT高57%的相关性；在基因表达预测中，MVHybrid比随机分割显示43%更小的性能下降；MVHybrid在分类、块检索和生存预测等下游任务中表现与ViT相当或更好。&lt;h4&gt;结论&lt;/h4&gt;MVHybrid作为下一代病理学视觉基础模型主干具有广阔前景，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;空间转录组学揭示了组织环境中的基因表达模式，使精准肿瘤学应用如治疗反应预测成为可能，但其高成本和技术复杂性限制了临床应用。从常规组织病理学图像预测空间基因表达（生物标志物）提供了一种实用的替代方案，然而，当前基于Vision Transformer主干架构的病理学视觉基础模型表现低于临床标准。鉴于视觉基础模型已在数百万种不同的全幻灯片图像上进行了训练，我们假设超越ViT的架构创新可能更好地捕捉与分子表型相关的低频、细微形态模式。通过展示具有负实特征值初始化的状态空间模型表现出强烈的低频偏差，我们引入了MVHybrid，这是一种结合状态空间模型与ViT的混合主干架构。我们比较了其他五种不同的病理学视觉基础模型主干架构，所有架构都使用DINOv2自监督学习方法在相同的结直肠癌数据集上进行了预训练。我们使用相同的生物标志物数据集的随机分割和留一研究法(LOSO)设置评估了所有预训练模型。在LOSO评估中，MVHybrid比表现最佳的ViT高57%的相关性，并且在基因表达预测中显示出比随机分割小43%的性能下降，分别证明了其优越的性能和鲁棒性。此外，与ViT相比，MVHybrid在分类、块检索和生存预测等下游任务中显示出相等或更好的性能，显示了其作为下一代病理学视觉基础模型主干的潜力。我们的代码已在GitHub上公开：https://github.com/deepnoid-ai/MVHybrid。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial transcriptomics reveals gene expression patterns within tissuecontext, enabling precision oncology applications such as treatment responseprediction, but its high cost and technical complexity limit clinical adoption.Predicting spatial gene expression (biomarkers) from routine histopathologyimages offers a practical alternative, yet current vision foundation models(VFMs) in pathology based on Vision Transformer (ViT) backbones perform belowclinical standards. Given that VFMs are already trained on millions of diversewhole slide images, we hypothesize that architectural innovations beyond ViTsmay better capture the low-frequency, subtle morphological patterns correlatingwith molecular phenotypes. By demonstrating that state space models initializedwith negative real eigenvalues exhibit strong low-frequency bias, we introduce$MV_{Hybrid}$, a hybrid backbone architecture combining state space models(SSMs) with ViT. We compare five other different backbone architectures forpathology VFMs, all pretrained on identical colorectal cancer datasets usingthe DINOv2 self-supervised learning method. We evaluate all pretrained modelsusing both random split and leave-one-study-out (LOSO) settings of the samebiomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% highercorrelation than the best-performing ViT and shows 43% smaller performancedegradation compared to random split in gene expression prediction,demonstrating superior performance and robustness, respectively. Furthermore,$MV_{Hybrid}$ shows equal or better downstream performance in classification,patch retrieval, and survival prediction tasks compared to that of ViT, showingits promise as a next-generation pathology VFM backbone. Our code is publiclyavailable at: https://github.com/deepnoid-ai/MVHybrid.</description>
      <author>example@mail.com (Won June Cho, Hongjun Yoon, Daeky Jeong, Hyeongyeol Lim, Yosep Chong)</author>
      <guid isPermaLink="false">2508.00383v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer</title>
      <link>http://arxiv.org/abs/2508.00298v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2412.00837&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了AniMer+，一个统一理解不同动态对象的网络框架，特别用于哺乳动物和鸟类的姿态与形状估计。通过高容量的Vision Transformer架构和混合专家设计，结合扩散条件图像生成技术创建大规模合成数据集，解决了多物种3D训练数据稀缺的问题，并在多个基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;在基础模型时代，通过单一网络实现对不同动态对象的统一理解有望增强空间智能。准确估计不同物种动物的姿态和形状对生物研究中的定量分析至关重要。然而，由于先前方法的网络容量有限以及全面的多物种数据集稀缺，这一领域研究不足。&lt;h4&gt;目的&lt;/h4&gt;解决网络容量限制和多物种数据集稀缺的问题，开发一个统一的网络框架，用于重建哺乳动物和鸟类，并提高动物姿态和形状估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了AniMer+，可扩展的AniMer框架的扩展版本；2. 设计了高容量的、家族感知的Vision Transformer (ViT)，采用混合专家(MoE)设计；3. 架构将网络层分为特定分类组件(哺乳动物和鸟类)和共享分类组件；4. 引入基于扩散的条件图像生成管道，创建两个大规模合成数据集：CtrlAni3D(四足动物)和CtrlAVES3D(鸟类)；5. 在41.3k张哺乳动物和12.4k张鸟类图像(真实和合成数据组合)上训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;1. AniMer+在广泛的基准测试中表现优于现有方法，包括具有挑战性的域外Animal Kingdom数据集；2. 消融研究验证了新型网络架构和生成的合成数据集在提高实际应用性能方面的有效性；3. CtrlAVES3D是首个大规模3D注释的鸟类数据集，对于解决单视图深度歧义至关重要。&lt;h4&gt;结论&lt;/h4&gt;AniMer+通过统一网络架构和合成数据生成技术，有效解决了多物种动物姿态和形状估计的挑战，为生物研究和空间智能提供了强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;在基础模型时代，通过单一网络实现对不同动态对象的统一理解有望增强强大的空间智能。此外，准确估计不同物种动物的姿态和形状对生物研究中的定量分析至关重要。然而，由于先前方法的网络容量有限以及全面的多物种数据集稀缺，这一主题仍未得到充分探索。为解决这些限制，我们引入了AniMer+，即可扩展AniMer框架的扩展版本。在本文中，我们专注于统一重建哺乳动物(mammalia)和鸟类(aves)的方法。AniMer+的一个关键创新是其高容量的、家族感知的Vision Transformer (ViT)，采用混合专家(MoE)设计。其架构将网络层分为特定分类组件(针对哺乳动物和鸟类)和共享分类组件，使单一模型能够高效学习独特的和共同的解剖特征。为克服3D训练数据的严重短缺，特别是鸟类数据，我们引入了基于扩散的条件图像生成管道。该管道产生两个大规模合成数据集：用于四足动物的CtrlAni3D和用于鸟类的CtrlAVES3D。值得注意的是，CtrlAVES3D是首个大规模3D注释的鸟类数据集，对于解决单视图深度歧义至关重要。在41.3k张哺乳动物和12.4k张鸟类图像(结合真实和合成数据)的集合上训练后，我们的方法在广泛的基准测试中表现出优于现有方法的性能，包括具有挑战性的域外Animal Kingdom数据集。消融研究验证了我们的新型网络架构和生成的合成数据集在提高实际应用性能方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决统一估计不同动物（特别是哺乳动物和鸟类）的姿态和形状的问题。这个问题在现实中非常重要，因为准确估计跨物种动物姿态和形状对生物研究中的定量分析至关重要，能够帮助研究人员更好地理解动物行为、生物力学和与环境相互作用的关系，为动物福利、农业、生态学和生命科学等多个领域提供重要见解。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先指出现有方法的局限性：网络容量有限和多物种数据集稀缺。他们借鉴了人类网格恢复领域的工作，特别是HMR2.0和HaMeR等使用Vision Transformer骨干网络的方法。作者扩展了他们之前的工作AniMer，引入了专家混合(MoE)设计来处理不同动物分类群。为了解决3D训练数据不足的问题，特别是鸟类数据，作者引入了基于扩散的条件图像生成管道。借鉴的现有工作包括Vision Transformer架构、专家混合设计、ControlNet图像生成、SMAL和AVES参数模型以及监督对比学习技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用高容量的家族感知Vision Transformer网络，结合专家混合(MoE)设计，将网络层分为特定分类群组件（哺乳动物和鸟类）和分类群共享组件，在单一模型中高效学习不同和共同的解剖特征，并利用合成数据解决3D训练数据稀缺问题。整体实现流程包括：1)收集和生成数据（聚合现有数据集并使用ControlNet生成CtrlAni3D和CtrlAVES3D合成数据集）；2)构建模型架构（ViT-MoE编码器、Transformer解码器、预测头和回归头）；3)采用两阶段训练策略；4)使用多种损失函数进行训练；5)在多个基准数据集上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)AniMer+网络架构，首个统一预测不同动物分类群的方法；2)动物家族监督对比学习方案，增强模型区分不同动物家族独特形状的能力；3)合成数据集生成管道，创建了首个带有3D注释的大规模鸟类数据集CtrlAVES3D；4)统一框架，首次在单一网络中同时处理哺乳动物和鸟类的网格恢复。相比之前的工作，AniMer+使用高容量Transformer骨干网络而非传统CNN，通过MoE设计处理不同解剖结构，使用ControlNet生成更高质量合成图像，并首次引入3D注释的大规模鸟类数据集解决深度歧义问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AniMer+通过引入家族感知Transformer和专家混合架构，结合大规模合成数据集，首次实现了在单一网络中统一估计哺乳动物和鸟类的姿态和形状，为动物网格恢复研究提供了新的基础模型范式。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of foundation models, achieving a unified understanding ofdifferent dynamic objects through a single network has the potential to empowerstronger spatial intelligence. Moreover, accurate estimation of animal pose andshape across diverse species is essential for quantitative analysis inbiological research. However, this topic remains underexplored due to thelimited network capacity of previous methods and the scarcity of comprehensivemulti-species datasets. To address these limitations, we introduce AniMer+, anextended version of our scalable AniMer framework. In this paper, we focus on aunified approach for reconstructing mammals (mammalia) and birds (aves). A keyinnovation of AniMer+ is its high-capacity, family-aware Vision Transformer(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecturepartitions network layers into taxa-specific components (for mammalia and aves)and taxa-shared components, enabling efficient learning of both distinct andcommon anatomical features within a single model. To overcome the criticalshortage of 3D training data, especially for birds, we introduce adiffusion-based conditional image generation pipeline. This pipeline producestwo large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D forbirds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset forbirds, which is crucial for resolving single-view depth ambiguities. Trained onan aggregated collection of 41.3k mammalian and 12.4k avian images (combiningreal and synthetic data), our method demonstrates superior performance overexisting approaches across a wide range of benchmarks, including thechallenging out-of-domain Animal Kingdom dataset. Ablation studies confirm theeffectiveness of both our novel network architecture and the generatedsynthetic datasets in enhancing real-world application performance.</description>
      <author>example@mail.com (Jin Lyu, Liang An, Li Lin, Pujin Cheng, Yebin Liu, Xiaoying Tang)</author>
      <guid isPermaLink="false">2508.00298v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models</title>
      <link>http://arxiv.org/abs/2508.00202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures, under review at CAMSAP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在标签噪声环境下提高基础模型鲁棒性的两阶段框架，无需重新训练模型。通过引入几何信息和使用非负核邻域构建，改进了现有的kNN方法，在多个数据集上表现出更好的性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型在大数据集上预训练后，已成为各种下游机器学习任务的基础，特别是在获取完美标记数据成本极高的场景中。然而，当基础模型需要用噪声数据进行微调时，如何确保鲁棒分类成为一个挑战。最近的kNN方法虽然能在严重标签噪声下表现良好，但仅利用了局部几何信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在标签噪声环境下确保鲁棒分类的两阶段框架，无需重新训练模型。通过引入几何信息，进一步提高现有方法的性能。&lt;h4&gt;方法&lt;/h4&gt;提出一个两阶段框架：首先是可靠性估计，其次是可靠性加权推理。对于给定实例，使用非负核邻域构建获取训练数据的局部邻域。作者还提出了几种可靠性估计方法，这些方法随着标签噪声的增加可以减少对距离和局部邻域的依赖。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10和DermaMNIST上的评估表明，所提出的方法在各种噪声条件下都能提高鲁棒性，超越了标准的K-NN方法和最近的自适应邻域基线方法。&lt;h4&gt;结论&lt;/h4&gt;通过引入几何信息并使用非负核邻域构建，提出的方法能够在标签噪声环境下提供更鲁棒的性能，且无需重新训练模型。&lt;h4&gt;翻译&lt;/h4&gt;在大数据集上预训练的基础模型已成为各种下游机器学习任务的基础，特别是在获取完美标记数据成本极高的场景中。在本文中，我们假设一个基础模型必须用噪声数据进行微调，并提出一个两阶段框架，以确保在存在标签噪声的情况下进行鲁棒分类，而无需重新训练模型。最近的工作表明，即使存在严重的标签噪声，使用从基础模型派生的嵌入的简单k最近邻方法也能取得良好的性能。我们的工作动机是这些方法利用了局部几何信息。在本文中，遵循类似的两阶段过程，即可靠性估计后接可靠性加权推理，我们表明通过引入几何信息可以实现改进的性能。对于给定的实例，我们提出的推理使用使用非负核邻域构建获得的训练数据的局部邻域。我们提出了几种可靠性估计方法，这些方法随着标签噪声的增加可以减少对距离和局部邻域的依赖。我们在CIFAR-10和DermaMNIST上的评估表明，我们的方法在各种噪声条件下都能提高鲁棒性，超越了标准的K-NN方法和最近的自适应邻域基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) pretrained on large datasets have become fundamentalfor various downstream machine learning tasks, in particular in scenarios whereobtaining perfectly labeled data is prohibitively expensive. In this paper, weassume an FM has to be fine-tuned with noisy data and present a two-stageframework to ensure robust classification in the presence of label noisewithout model retraining. Recent work has shown that simple k-nearest neighbor(kNN) approaches using an embedding derived from an FM can achieve goodperformance even in the presence of severe label noise. Our work is motivatedby the fact that these methods make use of local geometry. In this paper,following a similar two-stage procedure, reliability estimation followed byreliability-weighted inference, we show that improved performance can beachieved by introducing geometry information. For a given instance, ourproposed inference uses a local neighborhood of training data, obtained usingthe non-negative kernel (NNK) neighborhood construction. We propose severalmethods for reliability estimation that can rely less on distance and localneighborhood as the label noise increases. Our evaluation on CIFAR-10 andDermaMNIST shows that our methods improve robustness across various noiseconditions, surpassing standard K-NN approaches and recentadaptive-neighborhood baselines.</description>
      <author>example@mail.com (Ecem Bozkurt, Antonio Ortega)</author>
      <guid isPermaLink="false">2508.00202v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2507.23523v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出H-RDT（Human to Robotics Diffusion Transformer），一种利用人类操作数据增强机器人操作能力的新方法，采用两阶段训练范式，在模拟和真实世界实验中显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;机器人操作的模仿学习面临大规模高质量演示数据稀缺的挑战，现有机器人基础模型虽在跨形态数据集上预训练以增加数据规模，但因不同机器人形态和动作空间多样性导致统一训练困难。&lt;h4&gt;目的&lt;/h4&gt;利用大规模第一人称视角人类操作数据及其3D手部姿态注释，为机器人操作学习提供丰富的行为先验，解决数据稀缺和跨形态训练问题。&lt;h4&gt;方法&lt;/h4&gt;H-RDT采用两阶段训练范式：1)在大量第一人称视角人类操作数据上预训练；2)在机器人特定数据上跨形态微调，使用模块化动作编码器和解码器。基于20亿参数的扩散变压器架构，使用流匹配建模复杂动作分布。&lt;h4&gt;主要发现&lt;/h4&gt;广泛评估表明H-RDT在模拟和真实世界实验中分别比从头训练提高13.9%和40.5%，优于现有最先进方法包括Pi0和RDT，在单任务、多任务、少样本学习和鲁棒性评估中均表现优异。&lt;h4&gt;结论&lt;/h4&gt;人类操作数据可作为学习双臂机器人操作策略的强大基础，验证了利用人类行为先验增强机器人能力的核心假设。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作的模仿学习面临一个基本挑战：大规模、高质量的机器人演示数据稀缺。最近的机器人基础模型通常在跨形态机器人数据集上进行预训练以增加数据规模，而它们面临着显著的限制，因为不同机器人形态和动作空间的多样性使得统一训练具有挑战性。在本文中，我们提出了H-RDT（Human to Robotics Diffusion Transformer），一种利用人类操作数据增强机器人操作能力的新方法。我们的关键见解是，大规模第一人称视角人类操作视频与配对的3D手部姿态注释提供了丰富的行为先验，这些先验捕获了自然操作策略，并且可以受益于机器人策略学习。我们引入了一种两阶段训练范式：（1）在大量第一人称视角人类操作数据上进行预训练，以及（2）使用模块化动作编码器和解码器在机器人特定数据上进行跨形态微调。基于具有20亿参数的扩散变压器架构，H-RDT使用流匹配来建模复杂的动作分布。涵盖模拟和真实世界实验、单任务和多任务场景以及少样本学习和鲁棒性评估的广泛评估表明，H-RDT优于从头训练和现有的最先进方法，包括Pi0和RDT，在模拟和真实世界实验中分别比从头训练实现了13.9%和40.5%的显著改进。结果验证了我们的核心假设，即人类操作数据可以作为学习双臂机器人操作策略的强大基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation learning for robotic manipulation faces a fundamental challenge:the scarcity of large-scale, high-quality robot demonstration data. Recentrobotic foundation models often pre-train on cross-embodiment robot datasets toincrease data scale, while they face significant limitations as the diversemorphologies and action spaces across different robot embodiments make unifiedtraining challenging. In this paper, we present H-RDT (Human to RoboticsDiffusion Transformer), a novel approach that leverages human manipulation datato enhance robot manipulation capabilities. Our key insight is that large-scaleegocentric human manipulation videos with paired 3D hand pose annotationsprovide rich behavioral priors that capture natural manipulation strategies andcan benefit robotic policy learning. We introduce a two-stage trainingparadigm: (1) pre-training on large-scale egocentric human manipulation data,and (2) cross-embodiment fine-tuning on robot-specific data with modular actionencoders and decoders. Built on a diffusion transformer architecture with 2Bparameters, H-RDT uses flow matching to model complex action distributions.Extensive evaluations encompassing both simulation and real-world experiments,single-task and multitask scenarios, as well as few-shot learning androbustness assessments, demonstrate that H-RDT outperforms training fromscratch and existing state-of-the-art methods, including Pi0 and RDT, achievingsignificant improvements of 13.9% and 40.5% over training from scratch insimulation and real-world experiments, respectively. The results validate ourcore hypothesis that human manipulation data can serve as a powerful foundationfor learning bimanual robotic manipulation policies.</description>
      <author>example@mail.com (Hongzhe Bi, Lingxuan Wu, Tianwei Lin, Hengkai Tan, Zhizhong Su, Hang Su, Jun Zhu)</author>
      <guid isPermaLink="false">2507.23523v2</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data</title>
      <link>http://arxiv.org/abs/2508.00758v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 16 figures, 7 tables, preprint version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种扩散调度去噪自编码器(DDAE)框架，通过整合基于扩散的噪声调度和对比学习来改进表格数据异常检测，在半监督和无监督设置中均取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;表格数据异常检测面临特征交互复杂和异常样本稀少的挑战；传统去噪自编码器使用固定幅度噪声，适应性有限；扩散模型虽有计划噪声和迭代去噪机制，但缺乏明确的重建映射。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合扩散模型噪声调度和对比学习的新框架，提高表格数据异常检测的准确性和适应性。&lt;h4&gt;方法&lt;/h4&gt;提出扩散调度去噪自编码器(DDAE)，将基于扩散的噪声调度机制和对比学习整合到编码过程中，形成一种新的异常检测框架。&lt;h4&gt;主要发现&lt;/h4&gt;在57个ADBench数据集上评估显示，DDAE在半监督设置中优于现有方法，在无监督设置中具有竞争力；与最先进基线相比，PR-AUC提高最多65%(9%)，ROC-AUC提高16%(6%)；较高噪声水平有利于无监督训练，较低噪声配合线性调度在半监督设置中最优。&lt;h4&gt;结论&lt;/h4&gt;合理的噪声策略对表格异常检测至关重要，DDAE框架通过整合扩散模型的噪声调度和对比学习显著提升了异常检测性能。&lt;h4&gt;翻译&lt;/h4&gt;表格数据中的异常检测由于复杂的特征交互和异常样本的稀少性而仍然具有挑战性。去噪自编码器依赖于固定幅度的噪声，限制了其对不同数据分布的适应性。扩散模型引入了计划噪声和迭代去噪，但缺乏明确的重建映射。我们提出了扩散调度去噪自编码器(DDAE)，这是一个将基于扩散的噪声调度和对比学习整合到编码过程中的框架，以改进异常检测。我们在ADBench的57个数据集上评估了DDAE。我们的方法在半监督设置中优于现有方法，并在无监督设置中取得了具有竞争力的结果，将PR-AUC比最先进的自编码器(扩散)模型基线提高了最多65%(9%)，ROC-AUC提高了16%(6%)。我们观察到，较高的噪声水平有利于无监督训练，而较低的噪声配合线性调度在半监督设置中是最优的。这些发现强调了在表格异常检测中，合理的噪声策略的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3711896.3736910&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection in tabular data remains challenging due to complex featureinteractions and the scarcity of anomalous examples. Denoising autoencodersrely on fixed-magnitude noise, limiting adaptability to diverse datadistributions. Diffusion models introduce scheduled noise and iterativedenoising, but lack explicit reconstruction mappings. We propose theDiffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integratesdiffusion-based noise scheduling and contrastive learning into the encodingprocess to improve anomaly detection. We evaluated DDAE on 57 datasets fromADBench. Our method outperforms in semi-supervised settings and achievescompetitive results in unsupervised settings, improving PR-AUC by up to 65%(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)model baselines. We observed that higher noise levels benefit unsupervisedtraining, while lower noise with linear scheduling is optimal insemi-supervised settings. These findings underscore the importance ofprincipled noise strategies in tabular anomaly detection.</description>
      <author>example@mail.com (Timur Sattarov, Marco Schreyer, Damian Borth)</author>
      <guid isPermaLink="false">2508.00758v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction</title>
      <link>http://arxiv.org/abs/2508.00657v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MLHC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了TrajSurv模型，通过学习纵向电子健康记录中的连续潜在轨迹，实现可信的生存预测，并提高模型的可解释性。&lt;h4&gt;背景&lt;/h4&gt;可信的生存预测对临床决策至关重要，纵向电子健康记录提供了独特的预测机会，但准确建模不规则采样数据下的连续临床进展并透明地将其与生存结果联系起来具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从纵向EHR数据中学习连续潜在轨迹的模型，以实现可信且透明的生存预测。&lt;h4&gt;方法&lt;/h4&gt;TrajSurv使用神经控制微分方程从不规则采样数据中提取连续时间潜在状态，形成连续轨迹；通过时间感知对比学习确保轨迹反映临床进展；使用两步解释过程将临床进展与生存结果透明链接。&lt;h4&gt;主要发现&lt;/h4&gt;在MIMIC-III和eICU数据集上评估显示，TrajSurv具有比现有深度学习方法更具竞争力的准确性和优越的透明度。&lt;h4&gt;结论&lt;/h4&gt;TrajSurv模型能够有效处理不规则采样的纵向EHR数据，生成可信的生存预测，同时提供透明的临床进展解释，有助于临床决策。&lt;h4&gt;翻译&lt;/h4&gt;可信的生存预测对临床决策至关重要。纵向电子健康记录(EHRs)为预测提供了独特而强大的机会。然而，准确建模不规则采样临床特征下患者潜在的连续临床进展，并透明地将进展与生存结果联系起来具有挑战性。为应对这些挑战，我们开发了TrajSurv，一个从纵向EHR数据学习连续潜在轨迹以实现可信生存预测的模型。TrajSurv采用神经控制微分方程(NCDE)从不规则采样数据中提取连续时间潜在状态，形成连续潜在轨迹。为确保潜在轨迹反映临床进展，TrajSurv通过时间感知对比学习方法将潜在状态空间与患者状态空间对齐。为透明地连接临床进展与生存结果，TrajSurv在两步分而治之的解释过程中使用潜在轨迹。首先，它使用学习到的向量场解释临床特征变化如何转化为潜在轨迹的演变。其次，它对这些潜在轨迹进行聚类，以识别与不同生存结果相关的关键临床进展模式。在MIMIC-III和eICU两个真实医疗数据集上的评估显示，TrajSurv相比现有深度学习方法具有竞争性的准确性和优越的透明度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trustworthy survival prediction is essential for clinical decision making.Longitudinal electronic health records (EHRs) provide a uniquely powerfulopportunity for the prediction. However, it is challenging to accurately modelthe continuous clinical progression of patients underlying the irregularlysampled clinical features and to transparently link the progression to survivaloutcomes. To address these challenges, we develop TrajSurv, a model that learnscontinuous latent trajectories from longitudinal EHR data for trustworthysurvival prediction. TrajSurv employs a neural controlled differential equation(NCDE) to extract continuous-time latent states from the irregularly sampleddata, forming continuous latent trajectories. To ensure the latent trajectoriesreflect the clinical progression, TrajSurv aligns the latent state space withpatient state space through a time-aware contrastive learning approach. Totransparently link clinical progression to the survival outcome, TrajSurv useslatent trajectories in a two-step divide-and-conquer interpretation process.First, it explains how the changes in clinical features translate into thelatent trajectory's evolution using a learned vector field. Second, it clustersthese latent trajectories to identify key clinical progression patternsassociated with different survival outcomes. Evaluations on two real-worldmedical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy andsuperior transparency over existing deep learning methods.</description>
      <author>example@mail.com (Sihang Zeng, Lucas Jing Liu, Jun Wen, Meliha Yetisgen, Ruth Etzioni, Gang Luo)</author>
      <guid isPermaLink="false">2508.00657v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.00513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ECAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CMUCL，一种用于文本属性图异常检测的新型端到端范式，通过联合训练文本和图编码器并利用跨模态和单模态多尺度一致性来提高异常检测性能，平均精度比次优方法提高11.13%。&lt;h4&gt;背景&lt;/h4&gt;图数据在各种高风险场景中的广泛应用增加了对图异常检测(GAD)的关注。现实世界中的图通常带有以原始文本序列形式存在的节点描述，称为文本属性图(TAGs)。&lt;h4&gt;目的&lt;/h4&gt;解决如何无缝整合原始文本和图拓扑，以释放TAGs中跨模态数据在异常检测方面的巨大潜力这一挑战性问题。&lt;h4&gt;方法&lt;/h4&gt;提出名为CMUCL的新型端到端范式，同时建模文本和图结构的数据，通过利用跨模态和单模态多尺度一致性联合训练文本和图编码器，设计基于不一致性挖掘的异常评分估计器得出节点特定的异常评分。&lt;h4&gt;主要发现&lt;/h4&gt;大量评估表明，CMUCL在文本属性图异常检测方面取得了显著进展，平均精度(AP)比次优方法提高了11.13%。&lt;h4&gt;结论&lt;/h4&gt;CMUCL通过整合文本和图信息，有效提高了文本属性图异常检测的性能，并发布了8个专门针对TAGs异常检测的基准数据集以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;图数据在各种高风险场景中的广泛应用增加了对图异常检测(GAD)的关注。面对现实世界中通常携带以原始文本序列形式存在的节点描述的图(称为文本属性图(TAGs))，现有的图异常检测流程通常涉及浅层嵌入技术将此类文本信息编码为特征，然后依赖图域内的复杂自监督任务来检测异常。然而，这种文本编码过程与图域中的异常检测训练目标分离，难以确保提取的文本特征专注于GAD相关信息，严重限制了检测能力。如何无缝整合原始文本和图拓扑，以释放TAGs中跨模态数据在异常检测方面的巨大潜力，构成了一个具有挑战性的问题。本文提出了一个用于文本属性图异常检测的新型端到端范式，名为CMUCL。我们同时建模来自文本和图结构的数据，并通过利用跨模态和单模态多尺度一致性联合训练文本和图编码器，以发现潜在的异常相关信息。据此，我们设计了一个基于不一致性挖掘的异常评分估计器，以得出节点特定的异常评分。考虑到缺乏专门针对TAGs异常检测的基准数据集，我们发布了8个数据集以促进未来研究。大量评估表明，CMUCL在文本属性图异常检测方面取得了显著进展，平均精度(AP)比次优方法提高了11.13%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The widespread application of graph data in various high-risk scenarios hasincreased attention to graph anomaly detection (GAD). Faced with real-worldgraphs that often carry node descriptions in the form of raw text sequences,termed text-attributed graphs (TAGs), existing graph anomaly detectionpipelines typically involve shallow embedding techniques to encode such textualinformation into features, and then rely on complex self-supervised taskswithin the graph domain to detect anomalies. However, this text encodingprocess is separated from the anomaly detection training objective in the graphdomain, making it difficult to ensure that the extracted textual features focuson GAD-relevant information, seriously constraining the detection capability.How to seamlessly integrate raw text and graph topology to unleash the vastpotential of cross-modal data in TAGs for anomaly detection poses a challengingissue. This paper presents a novel end-to-end paradigm for text-attributedgraph anomaly detection, named CMUCL. We simultaneously model data from bothtext and graph structures, and jointly train text and graph encoders byleveraging cross-modal and uni-modal multi-scale consistency to uncoverpotential anomaly-related information. Accordingly, we design an anomaly scoreestimator based on inconsistency mining to derive node-specific anomaly scores.Considering the lack of benchmark datasets tailored for anomaly detection onTAGs, we release 8 datasets to facilitate future research. Extensiveevaluations show that CMUCL significantly advances in text-attributed graphanomaly detection, delivering an 11.13% increase in average accuracy (AP) overthe suboptimal.</description>
      <author>example@mail.com (Yiming Xu, Xu Hua, Zhen Peng, Bin Shi, Jiarun Chen, Xingbo Fu, Song Wang, Bo Dong)</author>
      <guid isPermaLink="false">2508.00513v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>When Vision-Language Model (VLM) Meets Beam Prediction: A Multimodal Contrastive Learning Framework</title>
      <link>http://arxiv.org/abs/2508.00456v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于视觉-语言模型的对比学习多模态波束预测框架，通过整合多模态数据并采用对比预训练策略，提高了在复杂动态环境下的毫米波波束预测准确性。&lt;h4&gt;背景&lt;/h4&gt;现实传播环境日益复杂和动态，传统依赖实时信道状态信息的方法计算成本高且在复杂环境下难以保持准确性，毫米波波束预测面临巨大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于视觉-语言模型(VLM)的对比学习多模态波束预测框架，提高复杂动态环境下的波束预测性能。&lt;h4&gt;方法&lt;/h4&gt;通过特定模态编码器整合多模态数据，采用对比预训练策略对齐图像和LiDAR特征，使用位置信息作为文本提示引入语言模态。&lt;h4&gt;主要发现&lt;/h4&gt;在DeepSense-6G数据集上实验表明，VLM主干提供了额外的语义基础，与现有方法相比，基于距离的整体准确度分数达到0.9016，平均提高了1.46%。&lt;h4&gt;结论&lt;/h4&gt;VLM驱动的对比学习方法能有效提高毫米波波束预测在复杂动态环境下的准确性和性能。&lt;h4&gt;翻译&lt;/h4&gt;随着实际传播环境变得越来越复杂和动态，毫米波波束预测面临巨大挑战。然而，视觉-语言模型(VLM)强大的跨模态表示能力提供了一种有前景的方法。传统依赖实时信道状态信息(CSI)的方法计算量大，且在这些环境中往往难以保持准确性。本文提出了一种VLM驱动的基于对比学习的多模态波束预测框架，通过特定模态编码器整合多模态数据。为了强制跨模态一致性，我们采用对比预训练策略对齐潜在空间中的图像和LiDAR特征。我们使用位置信息作为文本提示并将其连接到文本编码器以引入语言模态，这进一步提高了跨模态一致性。在DeepSense-6G数据集上的实验表明，我们的VLM主干提供了额外的语义基础。与现有方法相比，0.9016的整体基于距离的准确度分数(DBA-Score)对应于1.46%的平均改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决毫米波通信系统中在复杂动态环境下的波束预测问题。传统方法依赖实时信道状态信息(CSI)，计算成本高且在动态环境中难以保持准确性。这个问题很重要，因为随着实际环境日益复杂，可靠的波束预测是毫米波(特别是车对基础设施V2I通信)高效通信的基础，而实时CSI获取在快速变化环境中开销巨大，需要更高效的替代方案。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有单模态方法(如雷达、LiDAR、GPS)的局限性，它们性能不稳定且对环境变化敏感。然后借鉴了多模态融合的潜力，但发现跨模态对齐仍是挑战。作者创新性地将视觉-语言模型(VLM)引入波束预测，借鉴了对比学习框架来对齐不同模态特征，同时改进了现有方法中直接将位置作为数值输入的做法，转而将其文本化输入文本编码器。整体设计融合了现有技术但提出了新的架构和训练策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉-语言模型的跨模态表征能力，通过对比学习对齐不同模态特征，结合图像、LiDAR和GPS数据提高波束预测准确性。整体流程包括：1)使用模态特定编码器提取各模态特征；2)通过对比预训练对齐图像和LiDAR特征；3)将GPS位置转化为文本提示引入语言模态；4)使用动态门控和跨模态注意力融合多模态特征；5)通过分类器预测最优波束。训练分为对比预训练和微调两个阶段。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将视觉-语言模型应用于毫米波波束预测；2)提出自监督对比预训练策略对齐跨模态特征；3)设计GPS-Text分支将位置信息文本化引入语言模态；4)采用动态门控和跨模态注意力实现自适应特征融合。相比之前工作，不同之处在于：位置信息处理从数值输入转为文本提示；特征对齐从简单拼接转为对比学习；融合机制从固定转为动态自适应；训练流程从端到端转为两阶段(预训练+微调)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于视觉-语言模型和对比学习的多模态框架，通过有效对齐图像、LiDAR和文本特征，显著提高了复杂环境下毫米波波束预测的准确性，同时降低了对实时信道状态信息的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the real propagation environment becomes in creasingly complex anddynamic, millimeter wave beam prediction faces huge challenges. However, thepowerful cross modal representation capability of vision-language model (VLM)provides a promising approach. The traditional methods that rely on real-timechannel state information (CSI) are computationally expensive and often fail tomaintain accuracy in such environments. In this paper, we present a VLM-drivencontrastive learning based multimodal beam prediction framework that integratesmultimodal data via modality-specific encoders. To enforce cross-modalconsistency, we adopt a contrastive pretraining strategy to align image andLiDAR features in the latent space. We use location information as text promptsand connect it to the text encoder to introduce language modality, whichfurther improves cross-modal consistency. Experiments on the DeepSense-6Gdataset show that our VLM backbone provides additional semantic grounding.Compared with existing methods, the overall distance-based accuracy score(DBA-Score) of 0.9016, corresponding to 1.46% average improvement.</description>
      <author>example@mail.com (Ji Wang, Bin Tang, Jian Xiao, Qimei Cui, Xingwang Li, Tony Q. S. Quek)</author>
      <guid isPermaLink="false">2508.00456v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start Item Recommendation</title>
      <link>http://arxiv.org/abs/2508.00452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M^2VAE的多模态多视图变分自编码器模型，用于解决冷启动项目推荐问题，通过处理多模态特征的公共视图和独特视图，以及用户偏好建模，有效提升了新项目的推荐效果。&lt;h4&gt;背景&lt;/h4&gt;冷启动项目推荐是推荐系统中的重大挑战，特别是在引入没有历史交互数据的新项目时。现有方法虽利用多模态内容缓解此问题，但往往忽略了模态固有的多视图结构以及共享特征与模态特定特征的区别。&lt;h4&gt;目的&lt;/h4&gt;开发一种生成模型，能够处理属性和多模态特征中的公共视图和独特视图建模挑战，以及对用户在单类型项目特征上的偏好进行建模。&lt;h4&gt;方法&lt;/h4&gt;提出M^2VAE模型，为项目ID、分类属性和图像特征生成类型特定的潜在变量，使用专家乘积导出公共表示，采用解耦对比损失分离公共视图与独特视图，利用偏好引导的专家混合自适应融合表示，并通过对比学习结合共现信号实现无需预训练。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上的大量实验验证了M^2VAE方法的有效性，能够有效解决冷启动项目推荐问题。&lt;h4&gt;结论&lt;/h4&gt;M^2VAE模型通过多模态多视图的变分自编码方法，成功解决了冷启动项目推荐中的关键挑战，为新项目的推荐提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;冷启动项目推荐是推荐系统中的一个重大挑战，特别是当引入没有任何历史交互数据的新项目时。虽然现有方法利用多模态内容来缓解冷启动问题，但它们常常忽略了模态固有的多视图结构以及共享特征和模态特定特征之间的区别。在本文中，我们提出了多模态多视图变分自编码器(M^2VAE)，这是一种生成模型，它解决了在属性和多模态特征中建模公共视图和独特视图的挑战，以及对用户在单类型项目特征上的偏好建模的问题。具体来说，我们为项目ID、分类属性和图像特征生成类型特定的潜在变量，并使用专家乘积(PoE)导出公共表示。解耦对比损失将公共视图与独特视图分离，同时保持特征信息量。为了建模用户倾向，我们采用偏好引导的专家混合(MoE)来自适应融合表示。我们还通过对比学习结合共现信号，消除了对预训练的需求。在真实世界数据集上的大量实验验证了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cold-start item recommendation is a significant challenge in recommendationsystems, particularly when new items are introduced without any historicalinteraction data. While existing methods leverage multi-modal content toalleviate the cold-start issue, they often neglect the inherent multi-viewstructure of modalities, the distinction between shared and modality-specificfeatures. In this paper, we propose Multi-Modal Multi-View VariationalAutoEncoder (M^2VAE), a generative model that addresses the challenges ofmodeling common and unique views in attribute and multi-modal features, as wellas user preferences over single-typed item features. Specifically, we generatetype-specific latent variables for item IDs, categorical attributes, and imagefeatures, and use Product-of-Experts (PoE) to derive a common representation. Adisentangled contrastive loss decouples the common view from unique views whilepreserving feature informativeness. To model user inclinations, we employ apreference-guided Mixture-of-Experts (MoE) to adaptively fuse representations.We further incorporate co-occurrence signals via contrastive learning,eliminating the need for pretraining. Extensive experiments on real-worlddatasets validate the effectiveness of our approach.</description>
      <author>example@mail.com (Chuan He, Yongchao Liu, Qiang Li, Wenliang Zhong, Chuntao Hong, Xinwei Yao)</author>
      <guid isPermaLink="false">2508.00452v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA</title>
      <link>http://arxiv.org/abs/2508.00719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DAMR框架，结合符号搜索和自适应路径评估，解决现有KGQA方法在适应性和计算效率方面的局限性，实现高效且上下文感知的知识图谱问答。&lt;h4&gt;背景&lt;/h4&gt;知识图谱问答旨在解释自然语言查询并利用知识图谱的关系和语义结构进行推理。现有方法主要分为两类：基于静态路径提取的检索-推理范式和基于大型语言模型的动态路径生成策略，但前者适应性有限，后者计算成本高且难以准确评估路径。&lt;h4&gt;目的&lt;/h4&gt;解决现有KGQA方法的局限性，提出一种高效且上下文感知的知识图谱问答框架。&lt;h4&gt;方法&lt;/h4&gt;提出DAMR框架，使用基于LLM的规划器引导的蒙特卡洛树搜索主干，每步选择top-k相关关系减少搜索空间；引入基于Transformer的评分器通过交叉注意力编码问题和关系序列；包含动态伪路径优化机制，从搜索过程中生成训练信号。&lt;h4&gt;主要发现&lt;/h4&gt;在多个KGQA基准测试上，DAMR显著优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;DAMR框架有效解决了现有KGQA方法在适应性和计算效率方面的问题，实现了更高效准确的知识图谱问答。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱问答(KGQA)旨在解释自然语言查询并通过利用知识图谱的关系和语义结构进行结构化推理来检索准确答案。最近的KGQA方法主要遵循检索-推理范式，依赖GNN或启发式规则进行静态路径提取，或使用大型语言模型(LLM)通过提示进行动态路径生成策略，共同执行检索和推理。然而，前者由于静态路径提取和缺乏上下文优化而适应性有限，后者由于依赖固定评分函数和大量LLM调用导致计算成本高且难以准确评估路径。为解决这些问题，本文提出了基于动态自适应蒙特卡洛树搜索的推理(DAMR)，这是一个结合符号搜索和自适应路径评估的新型框架，用于高效且上下文感知的KGQA。DAMR采用基于LLM的规划器引导的蒙特卡洛树搜索(MCTS)主干，在每一步选择top-k相关关系以减少搜索空间。为提高路径评估准确性，我们引入了一个基于轻量级Transformer的评分器，通过交叉注意力共同编码问题和关系序列，执行上下文感知的合理性估计，使模型能够捕获多跳推理过程中的细粒度语义变化。此外，为缓解高质量监督数据的稀缺性，DAMR纳入了动态伪路径优化机制，定期从搜索过程中探索的部分路径生成训练信号，使评分器能够持续适应推理轨迹的演变分布。在多个KGQA基准上的广泛实验表明，DAMR显著优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge Graph Question Answering (KGQA) aims to interpret natural languagequeries and perform structured reasoning over knowledge graphs by leveragingtheir relational and semantic structures to retrieve accurate answers. RecentKGQA methods primarily follow either retrieve-then-reason paradigm, relying onGNNs or heuristic rules for static paths extraction, or dynamic path generationstrategies that use large language models (LLMs) with prompting to jointlyperform retrieval and reasoning. However, the former suffers from limitedadaptability due to static path extraction and lack of contextual refinement,while the latter incurs high computational costs and struggles with accuratepath evaluation due to reliance on fixed scoring functions and extensive LLMcalls. To address these issues, this paper proposes Dynamically AdaptiveMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic searchwith adaptive path evaluation for efficient and context-aware KGQA. DAMRemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-basedplanner, which selects top-$k$ relevant relations at each step to reduce searchspace. To improve path evaluation accuracy, we introduce a lightweightTransformer-based scorer that performs context-aware plausibility estimation byjointly encoding the question and relation sequence through cross-attention,enabling the model to capture fine-grained semantic shifts during multi-hopreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,DAMR incorporates a dynamic pseudo-path refinement mechanism that periodicallygenerates training signals from partial paths explored during search, allowingthe scorer to continuously adapt to the evolving distribution of reasoningtrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMRsignificantly outperforms state-of-the-art methods.</description>
      <author>example@mail.com (Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siwei Liu)</author>
      <guid isPermaLink="false">2508.00719v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning Network Dismantling without Handcrafted Inputs</title>
      <link>http://arxiv.org/abs/2508.00706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MIND的消息迭代网络拆除器模型，通过引入注意力机制和利用消息迭代配置文件，消除了对手工制作结构特征的需求，实现了在大型真实网络上高效解决网络拆除问题。&lt;h4&gt;背景&lt;/h4&gt;消息传递图神经网络在网络科学问题上取得了突破性进展，但其竞争性能通常依赖于手工制作的特征作为输入，这增加了计算成本并引入了偏差。&lt;h4&gt;目的&lt;/h4&gt;消除对手工制作特征的需求，构建表达性强的消息传递框架，高效解决网络拆除(NP难问题)及其相关的关键节点识别问题。&lt;h4&gt;方法&lt;/h4&gt;引入注意力机制和消息迭代配置文件，采用有效算法生成结构多样化的合成训练网络集，提出MIND模型，仅使用多样化合成网络进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;MIND模型能够在仅使用多样化合成网络训练的情况下，成功推广到包含数百万节点的大型未见真实网络，性能优于现有最先进的网络拆除方法。&lt;h4&gt;结论&lt;/h4&gt;所提出模型的高效率和泛化能力不仅限于网络拆除问题，还可以在一系列复杂网络问题中得到广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;消息传递图神经网络的应用已成为网络科学重要问题的一个突破。然而，其竞争性能通常依赖于使用手工制作的特征作为输入，这增加了计算成本，并对原本纯数据驱动的网络表示引入了偏差。在此，我们通过引入注意力机制和利用消息迭代配置文件，消除了对手工制作特征的需求，同时采用有效的算法方法生成结构多样化的小型合成训练网络集。由此，我们构建了一个表达性强的消息传递框架，并利用它高效解决网络拆除这一NP难问题，该问题几乎等同于关键节点识别，具有重要的实际应用。仅使用多样化的合成网络进行训练，我们提出的模型——MIND：消息迭代网络拆除器——能够推广到包含数百万节点的大型未见真实网络，性能优于最先进的网络拆除方法。所提出模型在效率和泛化能力方面的提升可以在拆除之外的一系列复杂网络问题中得到利用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The application of message-passing Graph Neural Networks has been abreakthrough for important network science problems. However, the competitiveperformance often relies on using handcrafted structural features as inputs,which increases computational cost and introduces bias into the otherwisepurely data-driven network representations. Here, we eliminate the need forhandcrafted features by introducing an attention mechanism and utilizingmessage-iteration profiles, in addition to an effective algorithmic approach togenerate a structurally diverse training set of small synthetic networks.Thereby, we build an expressive message-passing framework and use it toefficiently solve the NP-hard problem of Network Dismantling, virtuallyequivalent to vital node identification, with significant real-worldapplications. Trained solely on diversified synthetic networks, our proposedmodel -- MIND: Message Iteration Network Dismantler -- generalizes to large,unseen real networks with millions of nodes, outperforming state-of-the-artnetwork dismantling methods. Increased efficiency and generalizability of theproposed model can be leveraged beyond dismantling in a range of complexnetwork problems.</description>
      <author>example@mail.com (Haozhe Tian, Pietro Ferraro, Robert Shorten, Mahdi Jalili, Homayoun Hamedmoghadam)</author>
      <guid isPermaLink="false">2508.00706v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data</title>
      <link>http://arxiv.org/abs/2508.00615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于相似性的自构建图模型(SBSCGM)和混合图神经网络架构(HybridGraphMedGNN)，用于预测ICU患者的临界状态。该模型利用多模态电子健康记录数据构建患者相似性图，并通过图神经网络预测患者死亡风险和连续临界评分，实验证明其性能达到最先进水平且具有可解释性。&lt;h4&gt;背景&lt;/h4&gt;准确预测ICU患者的临界状态(如ICU内死亡风险)对于重症监护的早期干预至关重要。然而，传统模型通常将每个患者视为独立个体，难以利用电子健康记录(EHR)中的关系结构。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用电子健康记录中关系结构的模型，以提高ICU患者临界状态预测的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;1. 提出相似性自构建图模型(SBSCGM)，从多模态EHR数据动态构建患者相似性图；2. 开发混合图神经网络架构(HybridGraphMedGNN)，整合图卷积网络(GCN)、GraphSAGE和图注意力网络(GAT)层；3. 使用混合相似度度量实时连接具有相似临床特征的患者；4. 利用局部和全局图模式学习鲁棒的患者表示。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在MIMIC-III数据集的6,000次ICU住院实验中，模型达到AUC-ROC 0.94的最先进性能；2. 优于基线分类器和单类型GNN模型；3. 提高了精确度和召回率；4. 注意力机制为模型预测提供了可解释的见解。&lt;h4&gt;结论&lt;/h4&gt;该框架为重症监护风险预测提供了可扩展和可解释的解决方案，有潜力支持临床医生在现实ICU环境中部署使用。&lt;h4&gt;翻译&lt;/h4&gt;准确预测ICU患者的临界状态(如ICU内死亡风险)对于重症监护的早期干预至关重要。然而，传统模型通常将每个患者视为独立个体，难以利用电子健康记录(EHR)中的关系结构。我们提出了一种基于相似性的自构建图模型(SBSCGM)，该模型从多模态EHR数据动态构建患者相似性图，以及一种混合图神经网络架构(HybridGraphMedGNN)，在该图上操作以预测患者死亡和连续临界评分。SBSCGM使用混合相似度度量(结合基于特征和结构相似度)实时连接具有相似临床特征的患者。HybridGraphMedGNN整合了图卷积网络(GCN)、GraphSAGE和图注意力网络(GAT)层，以学习鲁棒的患者表示，利用局部和全局图模式。在MIMIC-III数据集的6,000次ICU住院实验中，我们的模型达到了最先进的性能(AUC-ROC 0.94)，优于基线分类器和单类型GNN模型。我们还展示了改进的精确度/召回率，并证明注意力机制为模型预测提供了可解释的见解。我们的框架为重症监护风险预测提供了可扩展和可解释的解决方案，有潜力支持临床医生在现实ICU环境中部署使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting the criticalness of ICU patients (such as in-ICUmortality risk) is vital for early intervention in critical care. However,conventional models often treat each patient in isolation and struggle toexploit the relational structure in Electronic Health Records (EHR). We proposea Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically buildsa patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNNarchitecture that operates on this graph to predict patient mortality and acontinuous criticalness score. SBSCGM uses a hybrid similarity measure(combining feature-based and structural similarities) to connect patients withanalogous clinical profiles in real-time. The HybridGraphMedGNN integratesGraph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)layers to learn robust patient representations, leveraging both local andglobal graph patterns. In experiments on 6,000 ICU stays from the MIMIC-IIIdataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)outperforming baseline classifiers and single-type GNN models. We alsodemonstrate improved precision/recall and show that the attention mechanismprovides interpretable insights into model predictions. Our framework offers ascalable and interpretable solution for critical care risk prediction, withpotential to support clinicians in real-world ICU deployment.</description>
      <author>example@mail.com (Mukesh Kumar Sahu, Pinki Roy)</author>
      <guid isPermaLink="false">2508.00615v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides</title>
      <link>http://arxiv.org/abs/2508.00578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 12 figures, and 4 tables (references and SI included)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种基于机器学习的势能面学习方法，用于模拟氢原子转移反应，在生物相关系统中实现了量子化学准确性。&lt;h4&gt;背景&lt;/h4&gt;氢原子转移反应在生物过程中至关重要，如受损蛋白质中的自由基迁移，但其机制尚未完全理解。模拟HAT具有挑战性，因为需要在生物相关尺度上达到量子化学准确性，而经典力场和基于DFT的分子动力学都不适用。&lt;h4&gt;目的&lt;/h4&gt;系统生成肽中的HAT构型构建大型数据集，基准测试图神经网络架构学习HAT势能面的能力，将ML势能整合到大规模胶原蛋白模拟中，计算反应速率，推进对HAT和自由基迁移机制的理解。&lt;h4&gt;方法&lt;/h4&gt;使用半经验方法和DFT系统生成肽中的HAT构型构建数据集，基准测试三种图神经网络架构(SchNet, Allegro, 和 MACE)，分析标度律、模型可转移性和成本-性能权衡，探索结合ML势能和过渡态搜索算法以及主动学习的改进策略。&lt;h4&gt;主要发现&lt;/h4&gt;MACE在能量、力和势垒预测方面始终优于其他模型，在分布外DFT势垒预测上达到1.13千卡/摩尔的平均绝对误差，这种准确性使得能够将ML势能整合到大规模胶原蛋白模拟中。&lt;h4&gt;结论&lt;/h4&gt;该方法可推广到其他生物分子系统，能够在复杂环境中实现化学反应的量子精确模拟。&lt;h4&gt;翻译&lt;/h4&gt;氢原子转移反应在许多生物过程中至关重要，如受损蛋白质中的自由基迁移，但其机制途径尚未完全理解。模拟HAT具有挑战性，因为需要在生物相关尺度上达到量子化学准确性；因此，经典力场和基于DFT的分子动力学都不适用。机器学习势能提供了一种替代方案，能够以接近量子的准确性学习势能面。然而，训练这些模型以在多样化的HAT构型中泛化，特别是在蛋白质的自由基位置，需要量身定制的数据生成和仔细的模型选择。在此，我们使用半经验方法和DFT系统生成肽中的HAT构型来构建大型数据集。我们基准测试了三种图神经网络架构学习HAT势能面的能力，并从能量预测间接预测反应势垒的能力。MACE在能量、力和势垒预测方面始终优于其他模型，在分布外DFT势垒预测上达到1.13千卡/摩尔的平均绝对误差。这种准确性使得能够将ML势能整合到大规模胶原蛋白模拟中，从预测的势垒计算反应速率，推进对HAT和肽中自由基迁移的机制理解。我们分析了标度律、模型可转移性和成本-性能权衡，并概述了通过结合ML势能和过渡态搜索算法以及主动学习来改进的策略。我们的方法可推广到其他生物分子系统，能够在复杂环境中实现化学反应的量子精确模拟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hydrogen atom transfer (HAT) reactions are essential in many biologicalprocesses, such as radical migration in damaged proteins, but their mechanisticpathways remain incompletely understood. Simulating HAT is challenging due tothe need for quantum chemical accuracy at biologically relevant scales; thus,neither classical force fields nor DFT-based molecular dynamics are applicable.Machine-learned potentials offer an alternative, able to learn potential energysurfaces (PESs) with near-quantum accuracy. However, training these models togeneralize across diverse HAT configurations, especially at radical positionsin proteins, requires tailored data generation and careful model selection.Here, we systematically generate HAT configurations in peptides to build largedatasets using semiempirical methods and DFT. We benchmark three graph neuralnetwork architectures (SchNet, Allegro, and MACE) on their ability to learn HATPESs and indirectly predict reaction barriers from energy predictions. MACEconsistently outperforms the others in energy, force, and barrier prediction,achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFTbarrier predictions. This accuracy enables integration of ML potentials intolarge-scale collagen simulations to compute reaction rates from predictedbarriers, advancing mechanistic understanding of HAT and radical migration inpeptides. We analyze scaling laws, model transferability, and cost-performancetrade-offs, and outline strategies for improvement by combining ML potentialswith transition state search algorithms and active learning. Our approach isgeneralizable to other biomolecular systems, enabling quantum-accuratesimulations of chemical reactivity in complex environments.</description>
      <author>example@mail.com (Marlen Neubert, Patrick Reiser, Frauke Gräter, Pascal Friederich)</author>
      <guid isPermaLink="false">2508.00578v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.00507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACM Multimedia 2025 (MM '25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoLL的新型框架，结合大型语言模型和图神经网络，用于文本属性图(TAG)中的异常检测，实现了平均13.37%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;文本属性图(TAGs)结合了复杂的拓扑结构和丰富的文本信息，为图异常检测(GAD)提供了新视角。然而，现有方法主要关注图域内的复杂优化目标，忽视了文本模态的补充价值，且文本特征通常由浅层嵌入技术编码，可能导致与异常相关的语义上下文被遗漏。&lt;h4&gt;目的&lt;/h4&gt;释放文本模态的巨大潜力，解决大型语言模型在TAG异常检测应用中的局限性，并克服LLMs因输入长度限制而难以编码图中固有高阶结构信息的问题。&lt;h4&gt;方法&lt;/h4&gt;提出CoLL框架，结合大型语言模型(LLMs)和图神经网络(GNNs)的互补优势。CoLL采用多LLM协作进行证据增强生成，捕获与异常相关的上下文并提供人类可读的解释；同时集成带有门控机制的GNN，自适应融合文本特征和证据，保留高阶拓扑信息。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明CoLL具有优越性，在AP指标上平均提高了13.37%。&lt;h4&gt;结论&lt;/h4&gt;这项研究为将大型语言模型整合到图异常检测的进展中开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;文本属性图(TAGs)中复杂的拓扑结构与丰富的文本信息的自然结合，为图异常检测(GAD)开辟了新视角。然而，现有的GAD方法主要专注于在图域内设计复杂的优化目标，忽视了文本模态的补充价值，其特征通常由bag-of-words或skip-gram等浅层嵌入技术编码，因此可能与异常相关的语义上下文被遗漏。为了释放文本模态的巨大潜力，大型语言模型(LLMs)由于其强大的语义理解和推理能力，已成为有希望的替代方案。然而，它们在TAG异常检测中的应用仍处于起步阶段，并且由于输入长度限制，它们难以编码图中固有高阶结构信息。为了在TAGs中进行高质量的异常检测，我们提出了CoLL，这是一个结合LLMs和图神经网络(GNNs)以利用它们互补优势的新型框架。CoLL采用多LLM协作进行证据增强生成，以捕获与异常相关的上下文，同时为检测到的异常提供人类可读的解释。此外，CoLL集成了一个带有门控机制的GNN，以自适应地融合文本特征和证据，同时保留高阶拓扑信息。大量实验证明了CoLL的优越性，在AP上平均提高了13.37%。这项研究为将LLMs整合到推进GAD中开辟了新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The natural combination of intricate topological structures and rich textualinformation in text-attributed graphs (TAGs) opens up a novel perspective forgraph anomaly detection (GAD). However, existing GAD methods primarily focus ondesigning complex optimization objectives within the graph domain, overlookingthe complementary value of the textual modality, whose features are oftenencoded by shallow embedding techniques, such as bag-of-words or skip-gram, sothat semantic context related to anomalies may be missed. To unleash theenormous potential of textual modality, large language models (LLMs) haveemerged as promising alternatives due to their strong semantic understandingand reasoning capabilities. Nevertheless, their application to TAG anomalydetection remains nascent, and they struggle to encode high-order structuralinformation inherent in graphs due to input length constraints. Forhigh-quality anomaly detection in TAGs, we propose CoLL, a novel framework thatcombines LLMs and graph neural networks (GNNs) to leverage their complementarystrengths. CoLL employs multi-LLM collaboration for evidence-augmentedgeneration to capture anomaly-relevant contexts while delivering human-readablerationales for detected anomalies. Moreover, CoLL integrates a GNN equippedwith a gating mechanism to adaptively fuse textual features with evidence whilepreserving high-order topological information. Extensive experimentsdemonstrate the superiority of CoLL, achieving an average improvement of 13.37%in AP. This study opens a new avenue for incorporating LLMs in advancing GAD.</description>
      <author>example@mail.com (Yiming Xu, Jiarun Chen, Zhen Peng, Zihan Chen, Qika Lin, Lan Ma, Bin Shi, Bo Dong)</author>
      <guid isPermaLink="false">2508.00507v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool</title>
      <link>http://arxiv.org/abs/2508.00506v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Video supplement demonstrating feature-space exploration and  interactive labelling is available at: https://youtu.be/GZl1ebZJgEA and is  archived at https://doi.org/10.5281/zenodo.16676591&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于无监督学习的遥感图像标记方法，利用卷积神经网络和图神经网络分割和编码Sentinel-2卫星图像，实现地理区域的自动标记&lt;h4&gt;背景&lt;/h4&gt;机器学习在遥感成像中的应用依赖于最新和准确的标签进行模型训练和测试，但遥感图像标记需要大量时间和成本，需要专家分析&lt;h4&gt;目的&lt;/h4&gt;定义一个无监督管道，用于在Sentinel-2卫星图像中寻找和标记具有相似上下文和内容的地理区域&lt;h4&gt;方法&lt;/h4&gt;利用分割技术与卷积神经网络和图神经网络相结合，为图像比较编码更强大的特征空间；将图像分割成基于颜色和空间相似性分组的同质像素区域；使用图神经网络聚合周围片段的信息&lt;h4&gt;主要发现&lt;/h4&gt;该方法减少了标记工具中的异常值；允许用户在细粒度级别进行标记；允许在编码空间内形成图像级别的旋转不变语义关系&lt;h4&gt;结论&lt;/h4&gt;该方法通过去除先前方法的局限性，提供了一种更有效的遥感图像标记方法&lt;h4&gt;翻译&lt;/h4&gt;Machine learning for remote sensing imaging relies on up-to-date and accurate labels for model training and testing. Labelling remote sensing imagery is time and cost intensive, requiring expert analysis. Previous labelling tools rely on pre-labelled data for training in order to label new unseen data. In this work, we define an unsupervised pipeline for finding and labelling geographical areas of similar context and content within Sentinel-2 satellite imagery. Our approach removes limitations of previous methods by utilising segmentation with convolutional and graph neural networks to encode a more robust feature space for image comparison. Unlike previous approaches we segment the image into homogeneous regions of pixels that are grouped based on colour and spatial similarity. Graph neural networks are used to aggregate information about the surrounding segments enabling the feature representation to encode the local neighbourhood whilst preserving its own local information. This reduces outliers in the labelling tool, allows users to label at a granular level, and allows a rotationally invariant semantic relationship at the image level to be formed within the encoding space.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning for remote sensing imaging relies on up-to-date and accuratelabels for model training and testing. Labelling remote sensing imagery is timeand cost intensive, requiring expert analysis. Previous labelling tools rely onpre-labelled data for training in order to label new unseen data. In this work,we define an unsupervised pipeline for finding and labelling geographical areasof similar context and content within Sentinel-2 satellite imagery. Ourapproach removes limitations of previous methods by utilising segmentation withconvolutional and graph neural networks to encode a more robust feature spacefor image comparison. Unlike previous approaches we segment the image intohomogeneous regions of pixels that are grouped based on colour and spatialsimilarity. Graph neural networks are used to aggregate information about thesurrounding segments enabling the feature representation to encode the localneighbourhood whilst preserving its own local information. This reducesoutliers in the labelling tool, allows users to label at a granular level, andallows a rotationally invariant semantic relationship at the image level to beformed within the encoding space.</description>
      <author>example@mail.com (Tulsi Patel, Mark W. Jones, Thomas Redfern)</author>
      <guid isPermaLink="false">2508.00506v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network</title>
      <link>http://arxiv.org/abs/2508.00429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为ReaGAN的新型图神经网络框架，通过智能体规划和检索增强生成技术解决了传统GNN在节点信息不平衡和全局语义关系捕获方面的局限性，实现了在少样本场景下的高性能表现。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)通过在邻居节点之间传播信息在基于图的学习中取得了显著成功，主要通过预定义的聚合机制实现。&lt;h4&gt;目的&lt;/h4&gt;解决传统GNN的两个关键局限性：一是无法处理节点信息不平衡问题，二是预定义消息传递主要利用局部结构相似性而忽略全局语义关系，限制了模型捕获远距离相关信息的能力。&lt;h4&gt;方法&lt;/h4&gt;提出Retrieval-augmented Graph Agentic Network (ReaGAN)，这是一个基于智能体的框架，使每个节点能够自主进行节点级决策。每个节点作为智能体基于内部记忆独立规划下一个动作，实现节点级规划和自适应消息传播。同时，使用检索增强生成(RAG)技术使节点能够访问语义相关内容并在图中建立全局关系。&lt;h4&gt;主要发现&lt;/h4&gt;ReaGAN在少样本上下文设置下使用冻结的LLM主干架构无需微调即可实现有竞争力的性能，展示了智能体规划和局部-全局检索在图学习中的潜力。&lt;h4&gt;结论&lt;/h4&gt;智能体规划和检索增强技术可以有效解决传统图神经网络在处理节点信息不平衡和捕获全局语义关系方面的局限性，为图学习领域提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)通过在邻居节点之间通过预定义的聚合机制传播信息，在基于图的学习中取得了显著成功。然而，这种固定方案通常面临两个关键局限性。首先，它们无法处理节点信息不平衡的问题——一些节点信息丰富，而另一些则保持稀疏。其次，预定义的消息传递主要利用局部结构相似性，而忽略了图中的全局语义关系，限制了模型捕获远距离但相关信息的能力。我们提出了检索增强图智能体网络(ReaGAN)，这是一个基于智能体的框架，使每个节点具备自主的节点级决策能力。每个节点作为一个智能体，基于其内部记忆独立规划下一个行动，实现节点级规划和自适应消息传播。此外，检索增强生成(RAG)使节点能够访问语义相关内容并在图中建立全局关系。ReaGAN在少样本上下文设置下使用冻结的LLM主干架构无需微调即可实现有竞争力的性能，展示了智能体规划和局部-全局检索在图学习中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved remarkable success in graph-basedlearning by propagating information among neighbor nodes via predefinedaggregation mechanisms. However, such fixed schemes often suffer from two keylimitations. First, they cannot handle the imbalance in node informativeness --some nodes are rich in information, while others remain sparse. Second,predefined message passing primarily leverages local structural similaritywhile ignoring global semantic relationships across the graph, limiting themodel's ability to capture distant but relevant information. We proposeRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based frameworkthat empowers each node with autonomous, node-level decision-making. Each nodeacts as an agent that independently plans its next action based on its internalmemory, enabling node-level planning and adaptive message propagation.Additionally, retrieval-augmented generation (RAG) allows nodes to accesssemantically relevant content and build global relationships in the graph.ReaGAN achieves competitive performance under few-shot in-context settingsusing a frozen LLM backbone without fine-tuning, showcasing the potential ofagentic planning and local-global retrieval in graph learning.</description>
      <author>example@mail.com (Minghao Guo, Xi Zhu, Jingyuan Huang, Kai Mei, Yongfeng Zhang)</author>
      <guid isPermaLink="false">2508.00429v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization</title>
      <link>http://arxiv.org/abs/2508.00357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SGPC的新型图神经网络架构，通过结合胞层消息传递、最优传输提升、方差减少扩散和PAC-Bayes谱正则化机制，有效解决了图神经网络中的过平滑问题，并在多个基准测试上取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络中的过平滑问题导致节点特征崩溃，尤其在异质图上问题更严重，相邻节点通常具有不同标签。现有的层神经网络虽部分缓解此问题，但依赖静态或高度参数化的层结构，阻碍了泛化和可扩展性，且无法提供严格的稳定性保证。&lt;h4&gt;目的&lt;/h4&gt;引入SGPC(基于PAC-Bayes校准的层神经网络)方案，创建统一架构，结合多种机制实现稳健的半监督节点分类，同时提供理论性能边界和认证置信区间。&lt;h4&gt;方法&lt;/h4&gt;SGPC结合胞层消息传递与三种机制：基于最优传输的提升、方差减少的扩散和PAC-Bayes谱正则化。通过理论建立性能边界，证明可通过端到端训练实现边界感知目标，计算复杂度为线性。&lt;h4&gt;主要发现&lt;/h4&gt;在九个同质和异质基准测试上，SGPC性能优于最先进的谱和基于层的GNN，同时能为未见过的节点提供认证的置信区间。&lt;h4&gt;结论&lt;/h4&gt;SGPC是一种创新的层神经网络方法，有效解决了GNN中的过平滑问题，在理论保证和实际性能之间取得了良好平衡，具有更好的泛化能力和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络中的过平滑问题导致节点特征崩溃，特别是在相邻节点通常具有不同标签的异质图上。虽然层神经网络部分缓解了这个问题，但它们通常依赖静态或高度参数化的层结构，阻碍了泛化和可扩展性。现有的基于层的方法要么预定义限制映射，要么引入过多复杂性，但未能提供严格的稳定性保证。在本文中，我们引入了一种名为SGPC(基于PAC-Bayes校准的层神经网络)的新方案，这是一种统一架构，结合了胞层消息传递和多种机制，包括基于最优传输的提升、方差减少的扩散和PAC-Bayes谱正则化，用于稳健的半监督节点分类。我们从理论上建立了性能边界，并证明可以通过线性计算复杂度的端到端训练实现边界感知目标。在九个同质和异质基准测试上的实验表明，SGPC优于最先进的谱和基于层的GNN，同时为未见过的节点提供认证的置信区间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinctnode features, particularly on heterophilic graphs where adjacent nodes oftenhave dissimilar labels. Although sheaf neural networks partially mitigate thisproblem, they typically rely on static or heavily parameterized sheafstructures that hinder generalization and scalability. Existing sheaf-basedmodels either predefine restriction maps or introduce excessive complexity, yetfail to provide rigorous stability guarantees. In this paper, we introduce anovel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unifiedarchitecture that combines cellular-sheaf message passing with severalmechanisms, including optimal transport-based lifting, variance-reduceddiffusion, and PAC-Bayes spectral regularization for robust semi-supervisednode classification. We establish performance bounds theoretically anddemonstrate that the resulting bound-aware objective can be achieved viaend-to-end training in linear computational complexity. Experiments on ninehomophilic and heterophilic benchmarks show that SGPC outperformsstate-of-the-art spectral and sheaf-based GNNs while providing certifiedconfidence intervals on unseen nodes.</description>
      <author>example@mail.com (Yoonhyuk Choi, Jiho Choi, Chong-Kwon Kim)</author>
      <guid isPermaLink="false">2508.00357v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition</title>
      <link>http://arxiv.org/abs/2508.00205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的真实性格识别(RPR)方法，通过模拟与表达行为相关的个性化内部认知，使用二维图神经网络(2D-GNN)来准确推断真实性格特质。&lt;h4&gt;背景&lt;/h4&gt;现有的自动真实性格识别方法通常作为外部观察者，基于目标个体的表达行为来推断观察者对他们的性格印象，这种方法与真实性格有显著偏差，导致识别性能不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从目标个体容易获取的外部短音频-视觉行为中有效模拟个性化内部认知，并准确推断真实性格特质的方法。&lt;h4&gt;方法&lt;/h4&gt;1) 模拟个性化内部认知，表示为使个性化网络重现个体特定面部反应的网络权重；2) 将模拟的认知编码为包含二维节点和边特征矩阵的新型图结构；3) 使用二维图神经网络(2D-GNN)从该图中推断真实性格特质；4) 设计端到端策略联合训练认知模拟、图构建和性格识别模块。&lt;h4&gt;主要发现&lt;/h4&gt;真实性格与生成表达行为的人类内部认知之间存在关联，这种关联可用于改进真实性格识别性能。&lt;h4&gt;结论&lt;/h4&gt;通过模拟与真实性格相关的认知并使用2D-GNN进行推断，可以更准确地识别真实性格特质，优于传统的外部观察者方法。&lt;h4&gt;翻译&lt;/h4&gt;自动真实性格识别(RPR)旨在从人们的表达行为中评估真实性格特质。然而，大多数现有解决方案通常作为外部观察者，基于目标个体的表达行为来推断观察者对他们的性格印象，这与他们的真实性格有显著偏差，并持续导致识别性能不佳。受真实性格与生成表达行为的人类内部认知之间关联的启发，我们提出了一种新的RPR方法，能够从目标个体容易获取的外部短音频-视觉行为中有效模拟个性化的内部认知。模拟的个性化认知表示为一组网络权重，这些权重使个性化网络能够重现个体特定的面部反应，进一步编码为包含二维节点和边特征矩阵的新型图结构，并提出了一种新的二维图神经网络(2D-GNN)用于从中推断真实性格特质。为了模拟与真实性格相关的认知，我们设计了一种端到端策略，联合训练认知模拟、2D图构建和性格识别模块。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic real personality recognition (RPR) aims to evaluate human realpersonality traits from their expressive behaviours. However, most existingsolutions generally act as external observers to infer observers' personalityimpressions based on target individuals' expressive behaviours, whichsignificantly deviate from their real personalities and consistently lead toinferior recognition performance. Inspired by the association between realpersonality and human internal cognition underlying the generation ofexpressive behaviours, we propose a novel RPR approach that efficientlysimulates personalised internal cognition from easy-accessible external shortaudio-visual behaviours expressed by the target individual. The simulatedpersonalised cognition, represented as a set of network weights that enforcethe personalised network to reproduce the individual-specific facial reactions,is further encoded as a novel graph containing two-dimensional node and edgefeature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed forinferring real personality traits from it. To simulate real personality-relatedcognition, an end-to-end strategy is designed to jointly train our cognitionsimulation, 2D graph construction, and personality recognition modules.</description>
      <author>example@mail.com (Xiangyu Kong, Hengde Zhu, Haoqin Sun, Zhihao Guo, Jiayan Gu, Xinyi Ni, Wei Zhang, Shizhe Liu, Siyang Song)</author>
      <guid isPermaLink="false">2508.00205v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.00141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了INSPIRE-GNN框架，一种结合强化学习和图神经网络的混合模型，用于优化传感器放置和提高数据稀疏环境下的自行车流量估计&lt;h4&gt;背景&lt;/h4&gt;准确的链路级自行车流量估计对可持续城市交通规划至关重要，但许多城市面临自行车计数传感器覆盖有限导致的数据稀疏性挑战&lt;h4&gt;目的&lt;/h4&gt;开发一种框架来优化传感器放置并提高数据稀疏环境下的链路级自行车流量估计&lt;h4&gt;方法&lt;/h4&gt;INSPIRE-GNN整合了图卷积网络、图注意力网络和基于深度Q网络的强化学习代理，实现数据驱动的传感器位置选择以最大化估计性能&lt;h4&gt;主要发现&lt;/h4&gt;在墨尔本自行车网络（15,933个路段中仅141个有传感器覆盖，99%稀疏性）上应用INSPIRE-GNN，通过战略性地选择额外传感器位置，在50、100、200和500个传感器的部署中显著提高了流量估计准确性，并在均方误差、均方根误差和平均绝对误差等指标上优于传统启发式方法&lt;h4&gt;结论&lt;/h4&gt;该框架为交通规划者提供了可行的见解，可有效扩展传感器网络、优化传感器放置，并最大化自行车数据的流量估计准确性和可靠性，支持知情交通规划决策&lt;h4&gt;翻译&lt;/h4&gt;准确的链路级自行车流量估计对可持续城市交通规划至关重要。然而，许多城市由于自行车计数传感器覆盖有限而面临严重的稀疏数据挑战。为解决这个问题，我们提出了INSPIRE-GNN，一种新颖的强化学习增强混合图神经网络框架，旨在优化传感器放置并提高数据稀疏环境下的链路级自行车流量估计。INSPIRE-GNN将图卷积网络和图注意力网络与基于深度Q网络的强化学习代理相结合，实现了数据驱动的传感器位置战略选择，以最大化估计性能。应用于墨尔本的自行车网络（包含15,933个路段，仅141个路段有传感器覆盖，99%稀疏性），INSPIRE-GNN通过战略性地选择额外传感器位置，在50、100、200和500个传感器的部署中显著提高了流量估计。我们的框架在均方误差、均方根误差和平均绝对误差等关键指标上，优于介数中心性、接近中心性、观察到的自行车活动和随机放置等传统启发式传感器放置方法。此外，我们的实验将INSPIRE-GNN与标准机器学习和深度学习模型在自行车流量估计性能上进行基准测试，证明了其有效性。我们提出的框架为交通规划者提供了可行的见解，以有效扩展传感器网络、优化传感器放置，并最大化自行车数据的流量估计准确性和可靠性，为知情交通规划决策提供支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate link-level bicycling volume estimation is essential for sustainableurban transportation planning. However, many cities face significant challengesof high data sparsity due to limited bicycling count sensor coverage. Toaddress this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimizesensor placement and improve link-level bicycling volume estimation indata-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RLagent, enabling a data-driven strategic selection of sensor locations tomaximize estimation performance. Applied to Melbourne's bicycling network,comprising 15,933 road segments with sensor coverage on only 141 road segments(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volumeestimation by strategically selecting additional sensor locations indeployments of 50, 100, 200 and 500 sensors. Our framework outperformstraditional heuristic methods for sensor placement such as betweennesscentrality, closeness centrality, observed bicycling activity and randomplacement, across key metrics such as Mean Squared Error (MSE), Root MeanSquared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, ourexperiments benchmark INSPIRE-GNN against standard machine learning and deeplearning models in the bicycle volume estimation performance, underscoring itseffectiveness. Our proposed framework provides transport planners actionableinsights to effectively expand sensor networks, optimize sensor placement andmaximize volume estimation accuracy and reliability of bicycling data forinformed transportation planning decisions.</description>
      <author>example@mail.com (Mohit Gupta, Debjit Bhowmick, Rhys Newbury, Meead Saberi, Shirui Pan, Ben Beck)</author>
      <guid isPermaLink="false">2508.00141v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion</title>
      <link>http://arxiv.org/abs/2508.00037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE Transactions on Industrial Informatics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ScaleSTF的可扩展时空Transformer模型，解决了大规模城市网络中预测模型的效能与效率权衡问题，基于物理定律启发设计，具有线性复杂度，在大规模城市系统中表现出色。&lt;h4&gt;背景&lt;/h4&gt;网络化城市系统促进人流、资源和服务的流动，对经济和社会互动至关重要。这些系统涉及具有未知规则的复杂过程，通过传感器时间序列观测。当前预测模型如图神经网络在大规模应用中面临效能与效率的权衡。&lt;h4&gt;目的&lt;/h4&gt;解决预测模型在大规模网络应用中面临的效能与效率权衡挑战，设计一种高性能且计算效率高的模型。&lt;h4&gt;方法&lt;/h4&gt;从物理定律获取灵感，指导基本模型设计，避免架构冗余。提出基于Transformer结构的可解释神经扩散方案，其注意层由低维嵌入诱导，开发出具有线性复杂度的ScaleSTF模型。&lt;h4&gt;主要发现&lt;/h4&gt;ScaleSTF模型在交通流量、太阳能和智能电表等大规模城市系统验证中，展示了最先进的性能和显著的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;研究结果为大规模城市网络的动态预测提供了新的视角和方法。&lt;h4&gt;翻译&lt;/h4&gt;网络化城市系统促进了人流、资源和服务的流动，对经济和社会互动至关重要。这些系统通常涉及具有未知规则的复杂过程，通过基于传感器的时间序列进行观测。为了辅助工业和工程环境中的决策制定，数据驱动的预测模型被用于预测城市系统的时空动态。当前模型如图神经网络已显示出前景，但由于计算需求，它们在效能和效率之间面临权衡。因此，它们在大规模网络中的应用仍需进一步努力。本文通过从物理定律中获取灵感，解决了这一权衡挑战，指导了符合基本原理且避免架构冗余的基本模型设计。通过理解微观和宏观过程，我们提出了一种基于Transformer结构的可解释神经扩散方案，其注意层由低维嵌入诱导。所提出的具有线性复杂度的可扩展时空Transformer（ScaleSTF）在包括交通流量、太阳能和智能电表在内的大规模城市系统中得到了验证，展示了最先进的性能和显著的可扩展性。我们的结果为大规模城市网络的动态预测提供了新的视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TII.2025.3588614&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Networked urban systems facilitate the flow of people, resources, andservices, and are essential for economic and social interactions. These systemsoften involve complex processes with unknown governing rules, observed bysensor-based time series. To aid decision-making in industrial and engineeringcontexts, data-driven predictive models are used to forecast spatiotemporaldynamics of urban systems. Current models such as graph neural networks haveshown promise but face a trade-off between efficacy and efficiency due tocomputational demands. Hence, their applications in large-scale networks stillrequire further efforts. This paper addresses this trade-off challenge bydrawing inspiration from physical laws to inform essential model designs thatalign with fundamental principles and avoid architectural redundancy. Byunderstanding both micro- and macro-processes, we present a principledinterpretable neural diffusion scheme based on Transformer-like structureswhose attention layers are induced by low-dimensional embeddings. The proposedscalable spatiotemporal Transformer (ScaleSTF), with linear complexity, isvalidated on large-scale urban systems including traffic flow, solar power, andsmart meters, showing state-of-the-art performance and remarkable scalability.Our results constitute a fresh perspective on the dynamics prediction inlarge-scale urban networks.</description>
      <author>example@mail.com (Tong Nie, Jian Sun, Wei Ma)</author>
      <guid isPermaLink="false">2508.00037v1</guid>
      <pubDate>Mon, 04 Aug 2025 15:00:56 +0800</pubDate>
    </item>
    <item>
      <title>Formal Bayesian Transfer Learning via the Total Risk Prior</title>
      <link>http://arxiv.org/abs/2507.23768v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在源数据集也有限且与目标数据集不一定对齐的情况下进行迁移学习的新方法，通过贝叶斯框架进行不确定性量化和模型平均，显著提高了预测性能。&lt;h4&gt;背景&lt;/h4&gt;在数据严重受限的分析中，通过辅助数据集增强目标数据集可改进统计程序，但现有迁移学习方法难以处理源数据集有限且与目标数据集不一定对齐的情况。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在源数据集有限且与目标数据集不一定对齐的情况下进行有效迁移学习的方法。&lt;h4&gt;方法&lt;/h4&gt;提出使用基于源参数的条件风险最小化器而非经验损失最小化器作为目标参数的先验均值，构建单个联合先验分布；通过Gibbs采样对指示变量进行模型平均，实现完整的贝叶斯不确定性量化；讨论了特定先验实例在变换坐标系中导致贝叶斯Lasso的实现，以及扩展到中等规模数据集的计算技术。&lt;h4&gt;主要发现&lt;/h4&gt;最近提出的极小化-频率论迁移学习技术可被视为该模型的最大后验概率近似方法；在遗传学应用中，特别是在源数据有限的情况下，与频率论基线相比显示出优越的预测性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够在源数据有限的情况下，通过贝叶斯框架进行不确定性量化和模型平均，提高预测性能，为数据受限环境下的迁移学习提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;在数据严重受限的分析中，通过应用领域内的辅助数据集（源数据集）来增强目标数据集，可以显著改进统计程序。然而，现有的迁移学习方法难以处理源数据集也有限且与目标数据集不一定对齐的情况。典型策略是使用源数据上的经验损失最小化器作为目标参数的先验均值，这使得源参数的估计脱离贝叶斯框架。我们的关键概念贡献是使用基于源参数的条件风险最小化器。这使我们能够为源数据集和目标数据集的所有参数构建单个联合先验分布。因此，我们受益于完整的贝叶斯不确定性量化，并且可以通过Gibbs采样对指示变量进行模型平均，这些指示变量控制每个源数据集的包含。我们展示了特定先验实例如何在变换坐标系中导致贝叶斯Lasso，并讨论了将该方法扩展到中等规模数据集的计算技术。我们还证明了最近提出的极小化-频率论迁移学习技术可以被视为我们模型的最大后验概率近似方法。最后，我们在遗传学应用中展示了相对于频率论基线的优越预测性能，特别是在源数据有限的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In analyses with severe data-limitations, augmenting the target dataset withinformation from ancillary datasets in the application domain, called sourcedatasets, can lead to significantly improved statistical procedures. However,existing methods for this transfer learning struggle to deal with situationswhere the source datasets are also limited and not guaranteed to bewell-aligned with the target dataset. A typical strategy is to use theempirical loss minimizer on the source data as a prior mean for the targetparameters, which places the estimation of source parameters outside of theBayesian formalism. Our key conceptual contribution is to use a risk minimizerconditional on source parameters instead. This allows us to construct a singlejoint prior distribution for all parameters from the source datasets as well asthe target dataset. As a consequence, we benefit from full Bayesian uncertaintyquantification and can perform model averaging via Gibbs sampling overindicator variables governing the inclusion of each source dataset. We show howa particular instantiation of our prior leads to a Bayesian Lasso in atransformed coordinate system and discuss computational techniques to scale ourapproach to moderately sized datasets. We also demonstrate that recentlyproposed minimax-frequentist transfer learning techniques may be viewed as anapproximate Maximum a Posteriori approach to our model. Finally, we demonstratesuperior predictive performance relative to the frequentist baseline on agenetics application, especially when the source data are limited.</description>
      <author>example@mail.com (Nathan Wycoff, Ali Arab, Lisa O. Singh)</author>
      <guid isPermaLink="false">2507.23768v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
  <item>
      <title>Text-to-SQL Task-oriented Dialogue Ontology Construction</title>
      <link>http://arxiv.org/abs/2507.23358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TeQoDO是一种无需监督的面向任务对话本体构建方法，利用LLM的SQL编程能力和对话理论自主构建本体，优于迁移学习方法，且构建的本体在下游任务中表现良好，有助于提高LLM的可解释性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型被广泛用作通用知识源，但它们依赖参数化知识，限制了可解释性和可信度。在面向任务的对话系统中，通常使用由显式本体结构化的外部数据库来确保可解释性和可控性。&lt;h4&gt;目的&lt;/h4&gt;引入TeQoDO：一种文本到SQL的面向任务对话本体构建方法，解决构建本体需要手动标签或监督训练的问题。&lt;h4&gt;方法&lt;/h4&gt;LLM利用其固有的SQL编程能力，结合提示中提供的对话理论，自主从头构建面向任务对话本体，无需监督。&lt;h4&gt;主要发现&lt;/h4&gt;TeQoDO优于迁移学习方法；其构建的本体在下游对话状态跟踪任务上具有竞争力；消融研究证明对话理论的关键作用；TeQoDO能够扩展构建更大的本体，在Wikipedia和ArXiv数据集上进行了验证。&lt;h4&gt;结论&lt;/h4&gt;这被视为朝着更广泛地将本体应用于提高LLM可解释性迈出的一步。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)被广泛用作通用知识源，但它们依赖于参数化知识，这限制了可解释性和可信度。在面向任务的对话(TOD)系统中，这种分离是明确的，使用由显式本体结构化的外部数据库来确保可解释性和可控性。然而，构建这样的本体需要手动标签或监督训练。我们引入TeQoDO：一种文本到SQL的面向任务对话本体构建方法。在这里，LLM利用其固有的SQL编程能力，结合提示中提供的对话理论，自主从头构建TOD本体，无需监督。我们证明TeQoDO优于迁移学习方法，其构建的本体在下游对话状态跟踪任务上具有竞争力。消融研究证明了对话理论的关键作用。TeQoDO还能够扩展以构建更大的本体，我们在Wikipedia和ArXiv数据集上对此进行了研究。我们认为这是朝着更广泛地将本体应用于提高LLM可解释性迈出的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) are widely used as general-purpose knowledgesources, but they rely on parametric knowledge, limiting explainability andtrustworthiness. In task-oriented dialogue (TOD) systems, this separation isexplicit, using an external database structured by an explicit ontology toensure explainability and controllability. However, building such ontologiesrequires manual labels or supervised training. We introduce TeQoDO: aText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLMautonomously builds a TOD ontology from scratch without supervision using itsinherent SQL programming capabilities combined with dialogue theory provided inthe prompt. We show that TeQoDO outperforms transfer learning approaches, andits constructed ontology is competitive on a downstream dialogue state trackingtask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO alsoscales to allow construction of much larger ontologies, which we investigate ona Wikipedia and ArXiv dataset. We view this as a step towards broaderapplication of ontologies to increase LLM explainability.</description>
      <author>example@mail.com (Renato Vukovic, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Hsien-Chin Lin, Shutong Feng, Nurul Lubis, Milica Gasic)</author>
      <guid isPermaLink="false">2507.23358v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Search for $t\bar tt\bar tW$ Production at $\sqrt{s} = 13$ TeV Using a Modified Graph Neural Network at the LHC</title>
      <link>http://arxiv.org/abs/2507.23723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究在大型强子对撞机上同时产生四个顶夸克和一个W玻色子的稀有标准模型过程，提出了一种改进的物理信息神经网络方法，在完全强子衰变通道中有效识别这一过程，克服了巨大本底干扰的挑战。&lt;h4&gt;背景&lt;/h4&gt;在质心能量为13 TeV时，同时产生四个顶夸克和一个W玻色子的过程是标准模型中的稀有过程，其截面为6.6加上2.4减去2.6 ab。在完全强子衰变通道中识别这一过程面临来自tar{t}、tar{t}W、tar{t}Z和三顶夸克产生过程的巨大本底干扰，极具挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的方法来识别大型强子对撞机上同时产生四个顶夸克和一个W玻色子的稀有过程，特别是在完全强子衰变通道中，以提高信号显著性并克服本底干扰。&lt;h4&gt;方法&lt;/h4&gt;提出了一种改进的物理信息神经网络，这是一种混合图神经网络(GNN)，结合了用于粒子级特征的图层、基于多层感知器(MLP)的自定义全局流(包含量子电路)以及交叉注意力融合机制来结合局部和全局表示。使用物理信息损失函数强制执行源自事件衰变动力学的喷注多重数约束。模型在蒙特卡洛模拟上进行训练，并使用基于截面的重加权对事件进行归一化。&lt;h4&gt;主要发现&lt;/h4&gt;与传统方法相比，所提出的GNN模型在信号显著性方面达到0.174，ROC-AUC达到0.974，显著优于BDT(0.138显著性和0.913 ROC)和Xgboost(0.149显著性和0.920 ROC)。该方法为在350逆飞秒平方积分亮度的数据集中精确选择此类事件提供了有效框架。&lt;h4&gt;结论&lt;/h4&gt;这种增强的方法为大型强子对撞机上的精确事件选择提供了框架，利用高维统计学习和物理信息推断来解决高能物理学的基本挑战，与机器学习的发展相一致。所提出的混合图神经网络方法在识别稀有物理过程方面表现出色，克服了传统方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;在质心能量为13 TeV时，同时产生四个顶夸克和一个W玻色子是标准模型中的稀有过程，其截面为6.6加上2.4减去2.6 ab。在完全强子衰变通道中识别这一过程特别具有挑战性，因为存在来自tar{t}、tar{t}W、tar{t}Z和三顶夸克产生过程的巨大本底。本研究引入了一种改进的物理信息神经网络，一种增强事件分类的混合图神经网络(GNN)。所提出的模型结合了用于粒子级特征的图层，一个基于多层感知器(MLP)的自定义全局流(包含量子电路)以及交叉注意力融合，以结合局部和全局表示。物理信息损失函数强制执行源自事件衰变动力学的喷注多重数约束。与传统方法相比，GNN达到了0.174的信号显著性(信号除以信号加本底的平方根)和0.974的ROC-AUC，优于BDT的0.138显著性和0.913的ROC，而Xgboost达到了0.149的显著性和0.920的ROC。分类模型在蒙特卡洛模拟上进行训练，使用基于截面的重加权对事件进行归一化，以反映在350逆飞秒平方积分亮度数据集中的预期贡献。这种增强的方法为大型强子对撞机上的精确事件选择提供了框架，利用高维统计学习和物理信息推断来解决高能物理学的基本挑战，与机器学习的发展相一致。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The simultaneous production of four top quarks in association with a ($W$)boson at $(\sqrt{s} = 13)$ TeV is an rare SM process with anext-to-leading-order (NLO) cross-section of $(6.6^{+2.4}_{-2.6}{ab})$\cite{saiel}. Identifying this process in the fully hadronic decaychannel is particularly challenging due to overwhelming backgrounds from$t\bar{t}, t\bar{t}W, t\bar{t}Z$, and triple-top production processes. Thisstudy introduces a modified physics informed Neural Network, a hybrid graphneural network (GNN) enhancing event classification. The proposed modelintegrates Graph layers for particle-level features, a custom Multi LayerPerceptron(MLP) based global stream with a quantum circuit and cross-attentionfusion to combine local and global representations. Physics-informed Lossfunction enforce jet multiplicity constraints, derived from event decaydynamics. Benchmarked against conventional methods, the GNN achieves a signalsignificance $(S/\sqrt{S+B})$ of $0.174$ and ROC-AUC of 0.974, surpassing BDT'ssignificance of $0.148$ and ROC of $0.913$, while Xgboost achieves asignificance of $0.149$ and ROC of $0.920$. The classification models aretrained on Monte Carlo (MC) simulations, with events normalized usingcross-section-based reweighting to reflect their expected contributions in adataset corresponding to $350\;$fb$^{-1}$ of integrated luminosity. Thisenhanced approach offers a framework for precision event selection at the LHC,leveraging high dimensional statistical learning and physics informed inferenceto tackle fundamental HEP challenges, aligning with ML developments.</description>
      <author>example@mail.com (Syed Haider Ali, Ashfaq Ahmad, Muhammad Saiel, Nadeem Shaukat)</author>
      <guid isPermaLink="false">2507.23723v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>FMIP: Multimodal Flow Matching for Mixed Integer Linear Programming</title>
      <link>http://arxiv.org/abs/2507.23390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A Generative Model based Method for Mixed Integer Linear Programming&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为FMIP的新型多模态流匹配框架，用于解决混合整数线性规划(MILP)问题。该框架能够对MILP混合解空间中的整数和连续变量进行联合分布建模，并通过引导机制提高解决方案质量。&lt;h4&gt;背景&lt;/h4&gt;混合整数线性规划(MILP)是数学优化的基石，能够同时处理整数和连续变量的复杂决策问题。然而，大多数MILP问题是NP完全的，在实际求解中具有挑战性。现有的基于图神经网络(GNN)的启发式方法仅预测整数变量的解，难以捕捉连续变量和整数变量之间的复杂相互作用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来解决现有GNN基方法在MILP问题求解中的局限性，能够更好地捕捉整数和连续变量之间的相互作用，提高解决方案的质量和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了FMIP，一种新颖的多模态流匹配框架，该框架对MILP混合解空间中的整数和连续变量的联合分布进行建模。FMIP集成了一个引导机制，用于在目标函数优化和约束满足条件下指导解的采样。&lt;h4&gt;主要发现&lt;/h4&gt;在七个标准MILP基准上评估FMIP，实验表明FMIP平均比现有的基于GNN的预测基线提高了50.04%的解决方案质量。&lt;h4&gt;结论&lt;/h4&gt;FMIP作为开发基于学习的MILP解决方案策略的有力新方法具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;混合整数线性规划(MILP)是数学优化的基石，能够对涉及整数和连续变量的复杂决策问题进行建模。尽管其通用性很强，但大多数MILP问题是NP完全的，因此在实践中难以求解。现有的基于图神经网络(GNN)的启发式方法旨在通过仅预测给定实例中整数变量的解来减少问题规模，但难以捕捉连续变量和整数变量之间的复杂相互作用，且缺乏足够的表示能力。为了解决这些局限性，我们提出了FMIP，一种新颖的多模态流匹配框架，该框架对MILP混合解空间中整数和连续变量的联合分布进行建模。为了实现更准确和可扩展的启发式方法，FMIP集成了一个引导机制，用于在目标函数优化和约束满足条件下指导解的采样。我们在七个标准MILP基准上评估了FMIP。实验表明，FMIP平均比现有的基于GNN的预测基线提高了50.04%的解决方案质量。这些结果突显了FMIP作为开发基于学习的MILP解决方案策略的有力新方法的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mixed-Integer Linear Programming (MILP) is a cornerstone of mathematicaloptimization, enabling the modeling of complex decision-making problemsinvolving both integer and continuous variables. Despite its versatility, mostMILP problems are NP-complete, making them challenging to solve in practice.Existing graph neural network (GNN)-based heuristics aim to reduce problemscale by predicting only the solutions on integer variables for a giveninstance, struggling to capture the intricate interplay between continuous andinteger variables and lack sufficient representational power. To address theselimitations, we propose FMIP, a novel multimodal flow-matching framework thatmodels the joint distribution over integer and continuous variables in themixed solution space of MILP. To enable more accurate and scalable heuristics,FMIP integrates a guidance mechanism to guide solution sampling under bothobjective function optimization and constraint satisfaction. We evaluate FMIPon seven standard MILP benchmarks. Our experiments show that FMIP improvessolution quality by 50.04% on average over existing GNN-based predictivebaselines. These results highlight FMIP's potential as a powerful new approachfor developing learning based MILP solution strategy.</description>
      <author>example@mail.com (Hongpei Li, Hui Yuan, Han Zhang, Dongdong Ge, Mengdi Wang, Yinyu Ye)</author>
      <guid isPermaLink="false">2507.23390v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Causal-Inspired Multi-Agent Decision-Making via Graph Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2507.23080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究将因果学习与强化学习相结合，通过因果解缠表示学习提取影响自动驾驶决策的因果特征，并融入图神经网络强化学习算法中，优化无信号灯交叉口的车辆行为，实验表明该方法在多个关键指标上优于现有学习方法。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶技术在过去十年取得了显著进展，但现有研究仍难以解决多车辆环境下的无缝交互挑战。&lt;h4&gt;目的&lt;/h4&gt;通过整合因果学习与强化学习方法，识别和提取影响自动驾驶车辆最优决策的因果特征，提高复杂交通场景下的决策能力。&lt;h4&gt;方法&lt;/h4&gt;利用因果解缠表示学习(CDRL)识别和提取因果特征，将这些特征融入基于图神经网络的强化学习算法，优化无信号灯交叉口的车辆行为。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在训练过程中获得最高平均奖励，并在碰撞率和平均累积奖励等关键指标上显著优于其他学习方法。&lt;h4&gt;结论&lt;/h4&gt;该研究为推进多智能体自动驾驶系统提供了有前景的方向，使自动驾驶车辆在复杂交通环境中导航更安全高效。&lt;h4&gt;翻译&lt;/h4&gt;自从自动驾驶技术问世以来，在过去十年中经历了显著的发展。然而，大多数现有研究仍然难以解决多车辆必须无缝交互的环境所带来的挑战。本研究旨在通过利用因果解缠表示学习(CDRL)将因果学习与基于强化学习的方法相结合，以识别和提取影响自动驾驶车辆最优决策的因果特征。然后将这些特征融入基于图神经网络的强化学习算法中，以增强复杂交通场景下的决策能力。通过使用因果特征作为输入，所提出的方法能够在无信号灯交叉口优化车辆行为。实验结果表明，我们提出的方法在训练过程中实现了最高的平均奖励，并且在测试过程中的碰撞率和平均累积奖励等几个关键指标上显著优于其他基于学习的方法。该研究为推进多智能体自动驾驶系统提供了有前景的方向，使自动驾驶车辆在复杂交通环境中导航更安全、更高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Since the advent of autonomous driving technology, it has experiencedremarkable progress over the last decade. However, most existing research stillstruggles to address the challenges posed by environments where multiplevehicles have to interact seamlessly. This study aims to integrate causallearning with reinforcement learning-based methods by leveraging causaldisentanglement representation learning (CDRL) to identify and extract causalfeatures that influence optimal decision-making in autonomous vehicles. Thesefeatures are then incorporated into graph neural network-based reinforcementlearning algorithms to enhance decision-making in complex traffic scenarios. Byusing causal features as inputs, the proposed approach enables the optimizationof vehicle behavior at an unsignalized intersection. Experimental resultsdemonstrate that our proposed method achieves the highest average reward duringtraining and our approach significantly outperforms other learning-basedmethods in several key metrics such as collision rate and average cumulativereward during testing. This study provides a promising direction for advancingmulti-agent autonomous driving systems and make autonomous vehicles' navigationsafer and more efficient in complex traffic environments.</description>
      <author>example@mail.com (Jing Wang, Yan Jin, Fei Ding, Chongfeng Wei)</author>
      <guid isPermaLink="false">2507.23080v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding</title>
      <link>http://arxiv.org/abs/2507.23478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了3D-R1，一个增强三维视觉-语言模型推理能力的基础模型，通过高质量数据集构建、强化学习训练策略和动态视角选择策略，显著提高了三维场景理解的推理和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;大型视觉-语言模型在二维视觉理解任务中取得了显著进展，但当前的三维VLMs由于高质量空间数据有限和视角假设的静态性质，在推理和泛化方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;解决当前3D VLMs在推理和泛化方面的挑战，提高三维场景理解能力。&lt;h4&gt;方法&lt;/h4&gt;构建高质量合成数据集Scene-30K，使用思维链(CoT)和基于Gemini 2.5 Pro的数据引擎；利用RLHF策略(如GRPO)增强推理能力；引入三个奖励函数：感知奖励、语义相似性奖励和格式奖励；实施动态视角选择策略，自适应选择最有信息量的视角。&lt;h4&gt;主要发现&lt;/h4&gt;3D-R1在各种三维场景基准测试中平均提高了10%，有效增强了三维场景理解的推理和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;3D-R1通过综合运用数据集构建、强化学习训练和动态视角选择策略，成功解决了3D VLMs在推理和泛化方面的挑战，显著提高了三维场景理解能力。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉-语言模型在二维视觉理解任务中取得了显著进展，引发了将这些能力扩展到三维场景理解领域的兴趣。然而，由于高质量空间数据的限制和视角假设的静态性质，当前的三维VLMs在推理和泛化方面常常遇到困难。为应对这些挑战，我们提出了3D-R1，这是一个增强三维VLMs推理能力的基础模型。具体而言，我们首先构建了一个高质量的包含思维链(CoT)的合成数据集，名为Scene-30K，利用现有的3D-VL数据集和基于Gemini 2.5 Pro的数据引擎。它作为3D-R1的冷启动初始化数据。此外，我们在强化学习训练过程中利用了RLHF策略，如GRPO，以增强推理能力，并引入了三个奖励函数：感知奖励、语义相似性奖励和格式奖励，以保持检测精度和答案语义精确性。此外，我们还引入了一种动态视角选择策略，能够自适应地选择对三维场景理解最有信息量的视角。大量实验表明，3D-R1在各种三维场景基准测试中平均提高了10%，突显了其在增强三维场景理解推理和泛化方面的有效性。代码：https://github.com/AIGeeksGroup/3D-R1。网站：https://aigeeksgroup.github.io/3D-R1。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large vision-language models (VLMs) have made significant strides in 2Dvisual understanding tasks, sparking interest in extending these capabilitiesto 3D scene understanding. However, current 3D VLMs often struggle with robustreasoning and generalization due to limitations in high-quality spatial dataand the static nature of viewpoint assumptions. To address these challenges, wepropose 3D-R1, a foundation model that enhances the reasoning capabilities of3D VLMs. Specifically, we first construct a high-quality synthetic dataset withCoT, named Scene-30K, leveraging existing 3D-VL datasets and a data enginebased on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1.Moreover, we leverage RLHF policy such as GRPO in the reinforcement learningtraining process to enhance reasoning capabilities and introduce three rewardfunctions: a perception reward, a semantic similarity reward and a formatreward to maintain detection accuracy and answer semantic precision.Furthermore, we introduce a dynamic view selection strategy that adaptivelychooses the most informative perspectives for 3D scene understanding. Extensiveexperiments demonstrate that 3D-R1 delivers an average improvement of 10%across various 3D scene benchmarks, highlighting its effectiveness in enhancingreasoning and generalization in 3D scene understanding. Code:https://github.com/AIGeeksGroup/3D-R1. Website:https://aigeeksgroup.github.io/3D-R1.</description>
      <author>example@mail.com (Ting Huang, Zeyu Zhang, Hao Tang)</author>
      <guid isPermaLink="false">2507.23478v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World Models</title>
      <link>http://arxiv.org/abs/2507.23325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FASTopoWM是一种结合潜在世界模型的新型快速-慢速车道线段拓扑推理框架，通过并行监督历史和新初始化查询，以及引入基于动作潜力的潜在查询和BEV世界模型，显著提高了自动驾驶系统的车道线段检测和中心线感知性能。&lt;h4&gt;背景&lt;/h4&gt;车道线段拓扑推理提供全面的鸟瞰道路场景理解，可作为端到端自动驾驶系统的关键感知模块，但现有方法难以有效利用时间信息提升性能。&lt;h4&gt;目的&lt;/h4&gt;克服现有车道拓扑推理方法的局限性，包括过度依赖历史查询、易受姿态估计失败影响以及时间传播不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出FASTopoWM框架，通过并行监督历史和新初始化查询减少姿态估计失败影响；引入基于动作潜力的潜在查询和BEV世界模型，将状态表示从过去观测传播到当前时间步，提高慢速流程中的时间感知性能。&lt;h4&gt;主要发现&lt;/h4&gt;在OpenLane-V2基准测试上，FASTopoWM在车道线段检测方面mAP达到37.4%(优于最先进方法的33.6%)，在中心线感知方面OLS达到46.3%(优于最先进方法的41.5%)。&lt;h4&gt;结论&lt;/h4&gt;FASTopoWM显著提升了自动驾驶系统的车道线段检测和中心线感知性能，是有效的车道线段拓扑推理框架。&lt;h4&gt;翻译&lt;/h4&gt;车道线段拓扑推理提供了全面的鸟瞰(BEV)道路场景理解，可以作为面向规划的端到端自动驾驶系统中的关键感知模块。现有的车道拓扑推理方法通常无法有效利用时间信息来提升检测和推理性能。最近，基于流的时序传播方法通过在查询和BEV级别整合时序线索展示了有前景的结果。然而，它仍然受限于过度依赖历史查询、容易受姿态估计失败影响以及时间传播不足等问题。为了克服这些限制，我们提出了FASTopoWM，一种结合潜在世界模型的新型快速-慢速车道线段拓扑推理框架。为减少姿态估计失败的影响，这个统一框架实现了历史查询和新初始化查询的并行监督，促进快速和慢速系统之间的相互强化。此外，我们引入了基于动作潜力的潜在查询和BEV世界模型，将状态表示从过去观测传播到当前时间步。这种设计显著提高了慢速流程中的时间感知性能。在OpenLane-V2基准测试上的大量实验表明，FASTopoWM在车道线段检测(37.4%对比33.6%的mAP)和中心线感知(46.3%对比41.5%的OLS)方面均优于最先进的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决车道线段拓扑推理中时间信息利用不足的问题。现有方法在有效利用时间信息增强检测和推理性能方面存在局限，特别是在姿态估计失败时性能显著下降。这个问题在自动驾驶领域非常重要，因为车道拓扑推理作为关键感知模块，为端到端自动驾驶系统提供鸟瞰图道路场景理解，直接关系到自动驾驶系统的可靠性和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于流的时间传播方法的三大局限性：过度依赖历史查询、易受姿态估计失败影响、时间传播不足。受视觉语言模型(VLMs)的启发，作者设计了双路径系统(快速-慢速管道)；借鉴自动驾驶中世界模型的概念，设计了两个潜在世界模型来捕获时间动态；同时引入统一框架实现并行监督，使两个系统相互强化而非完全独立。这些设计都建立在批判性分析现有工作和吸收相关领域创新的基础上。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建快速-慢速双系统架构，结合潜在世界模型增强时间传播能力。快速系统负责单帧感知确保基本功能，慢速系统利用时间信息提升性能；通过统一框架实现两个系统的相互强化；潜在世界模型基于'现在是过去的延续'原则，利用相对姿态将历史状态传播到当前时间步。整体流程分为编码器和解码器：编码器处理多视角图像提取BEV特征，并通过世界模型生成时间感知的流查询和流BEV特征；解码器分为快速和慢速两个分支，共享权重但分别处理不同信息，训练时联合监督，推理时根据姿态估计可靠性选择输出。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：(1)统一的快速-慢速框架，实现并行监督和相互强化；(2)两个潜在世界模型(查询世界模型和BEV世界模型)有效捕获时间动态；(3)改进的时间传播机制，避免传统warping导致的信息丢失。相比之前工作，该方法解决了现有流式方法过度依赖历史查询、姿态估计失败时性能下降、时间传播不足的问题；与其他快速-慢速系统不同，它无需额外模型，通过共享权重实现相互强化；相比现有世界模型方法，它专注于时间推理而非未来预测，更适合当前车道拓扑任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FASTopoWM通过创新的快速-慢双系统架构和潜在世界模型，显著提升了车道拓扑推理在时间感知和姿态估计失败场景下的鲁棒性和性能，实现了自动驾驶系统道路理解的重大突破。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lane segment topology reasoning provides comprehensive bird's-eye view (BEV)road scene understanding, which can serve as a key perception module inplanning-oriented end-to-end autonomous driving systems. Existing lane topologyreasoning methods often fall short in effectively leveraging temporalinformation to enhance detection and reasoning performance. Recently,stream-based temporal propagation method has demonstrated promising results byincorporating temporal cues at both the query and BEV levels. However, itremains limited by over-reliance on historical queries, vulnerability to poseestimation failures, and insufficient temporal propagation. To overcome theselimitations, we propose FASTopoWM, a novel fast-slow lane segment topologyreasoning framework augmented with latent world models. To reduce the impact ofpose estimation failures, this unified framework enables parallel supervisionof both historical and newly initialized queries, facilitating mutualreinforcement between the fast and slow systems. Furthermore, we introducelatent query and BEV world models conditioned on the action latent to propagatethe state representations from past observations to the current timestep. Thisdesign substantially improves the performance of temporal perception within theslow pipeline. Extensive experiments on the OpenLane-V2 benchmark demonstratethat FASTopoWM outperforms state-of-the-art methods in both lane segmentdetection (37.4% v.s. 33.6% on mAP) and centerline perception (46.3% v.s. 41.5%on OLS).</description>
      <author>example@mail.com (Yiming Yang, Hongbin Lin, Yueru Luo, Suzhong Fu, Chao Zheng, Xinrui Yan, Shuqi Mei, Kun Tang, Shuguang Cui, Zhen Li)</author>
      <guid isPermaLink="false">2507.23325v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning</title>
      <link>http://arxiv.org/abs/2507.23318v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FastDriveVLA的新型视觉标记修剪框架，专门针对自动驾驶场景中的Vision-Language-Action模型设计，通过保留关键前景信息来降低计算成本同时保持性能。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language-Action模型在复杂场景理解和动作推理方面表现出色，被越来越多地应用于端到端自动驾驶系统。然而，这些模型的长视觉标记导致计算成本高昂。现有的视觉标记修剪方法在自动驾驶场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效减少VLA模型计算负担同时保持决策性能的视觉标记修剪方法，特别针对自动驾驶场景优化。&lt;h4&gt;方法&lt;/h4&gt;提出FastDriveVLA框架，包含一个名为ReconPruner的即插即用视觉标记修剪器，通过MAE风格像素重建优先保留前景信息。设计对抗性前景-背景重建策略训练ReconPruner，适用于VLA模型的视觉编码器。创建nuScenes-FG数据集，包含24.1万张带前景标注的图像-掩码对。&lt;h4&gt;主要发现&lt;/h4&gt;人类驾驶员驾驶时专注于前景区域，保留这些区域的视觉标记对有效决策至关重要。ReconPruner能够无缝应用于不同VLA模型而无需重新训练。&lt;h4&gt;结论&lt;/h4&gt;FastDriveVLA在不同修剪比例下于nuScenes闭环规划基准测试上取得了最先进的结果，证明其在自动驾驶场景中有效降低了VLA模型的计算成本同时保持了性能。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action模型在复杂场景理解和动作推理方面显示出巨大潜力，导致它们在端到端自动驾驶系统中的采用率不断增加。然而，VLA模型的长视觉标记大大增加了计算成本。当前视觉语言模型中的视觉标记修剪方法依赖于视觉标记相似性或视觉-文本注意力，但两者在自动驾驶场景中都表现不佳。鉴于人类驾驶员驾驶时专注于相关前景区域，我们认为保留包含这些前景信息的视觉标记对有效决策至关重要。受此启发，我们提出了FastDriveVLA，这是一个专门为自动驾驶设计的基于重建的视觉标记修剪框架。FastDriveVLA包括一个即插即用的视觉标记修剪器ReconPruner，它通过MAE风格的像素重建优先考虑前景信息。设计了一种新的对抗性前景-背景重建策略来训练ReconPruner，用于VLA模型的视觉编码器。一旦训练完成，ReconPruner可以无缝应用于具有相同视觉编码器的不同VLA模型而无需重新训练。为了训练ReconPruner，我们还引入了一个名为nuScenes-FG的大规模数据集，包含24.1万张图像-掩码对，并标注了前景区域。我们的方法在不同修剪比例下于nuScenes闭环规划基准测试上取得了最先进的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决Vision-Language-Action (VLA) 模型在自动驾驶应用中的计算效率问题。VLA模型将视觉输入转换为大量视觉标记，大大增加了计算成本和推理延迟，使得在真实场景中部署受限。这个问题在现实中非常重要，因为自动驾驶系统需要实时处理大量视觉数据，计算资源有限，且推理速度对安全性至关重要。提高VLA模型的计算效率可以降低硬件要求，提高实时性，使自动驾驶技术更接近实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到人类驾驶员主要关注前景区域（如道路、车辆、行人等），而背景区域对驾驶决策影响很小。这启发他们思考保留包含前景信息的视觉标记对自动驾驶决策至关重要。作者批判了现有的视觉标记修剪方法（基于相似性和基于注意力）在自动驾驶场景中的局限性。他们设计了基于重建的视觉标记修剪框架FastDriveVLA，其中包括一个即插即用的视觉标记修剪器ReconPruner。作者借鉴了MAE（Masked Autoencoders）的像素重建思想和GAN（生成对抗网络）的对抗训练思想，并使用Grounded-SAM进行前景分割，构建了nuScenes-FG数据集来训练模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：人类驾驶员主要关注前景区域，因此保留包含前景信息的视觉标记对自动驾驶决策至关重要；通过像素重建来训练模型识别和保留重要的前景视觉标记；使用对抗性训练策略确保模型能够区分前景和背景。整体实现流程包括：1) 设计轻量级的ReconPruner，由PrunerLayer和Scorer组成；2) 训练过程中引入可学习的查询标记，通过MAE风格的像素重建和对抗性前景-背景重建策略训练模型；3) 推理时根据显著性分数选择保留重要视觉标记；4) 构建nuScenes-FG数据集提供训练支持。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了基于重建的视觉标记修剪框架FastDriveVLA；2) 设计了即插即用的视觉标记修剪器ReconPruner；3) 引入了对抗性前景-背景重建策略；4) 构建了nuScenes-FG大规模数据集。相比之前的工作，这种方法不依赖于文本-视觉对齐（区别于基于注意力的方法），也不强调标记相似性（区别于基于相似性的方法），更适合自动驾驶场景中前景区域明确的情况，能够更好地保留与驾驶相关的关键信息，在各种修剪比例下都取得了SOTA结果。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了FastDriveVLA，一种基于重建的视觉标记修剪框架，通过即插即用的ReconPruner和对抗性训练策略，显著提高了自动驾驶VLA模型的计算效率，同时保持了甚至超越了原始模型的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models have demonstrated significant potentialin complex scene understanding and action reasoning, leading to theirincreasing adoption in end-to-end autonomous driving systems. However, the longvisual tokens of VLA models greatly increase computational costs. Currentvisual token pruning methods in Vision-Language Models (VLM) rely on eithervisual token similarity or visual-text attention, but both have shown poorperformance in autonomous driving scenarios. Given that human driversconcentrate on relevant foreground areas while driving, we assert thatretaining visual tokens containing this foreground information is essential foreffective decision-making. Inspired by this, we propose FastDriveVLA, a novelreconstruction-based vision token pruning framework designed specifically forautonomous driving. FastDriveVLA includes a plug-and-play visual token prunercalled ReconPruner, which prioritizes foreground information through MAE-stylepixel reconstruction. A novel adversarial foreground-background reconstructionstrategy is designed to train ReconPruner for the visual encoder of VLA models.Once trained, ReconPruner can be seamlessly applied to different VLA modelswith the same visual encoder without retraining. To train ReconPruner, we alsointroduce a large-scale dataset called nuScenes-FG, consisting of 241Kimage-mask pairs with annotated foreground regions. Our approach achievesstate-of-the-art results on the nuScenes closed-loop planning benchmark acrossdifferent pruning ratios.</description>
      <author>example@mail.com (Jiajun Cao, Qizhe Zhang, Peidong Jia, Xuhui Zhao, Bo Lan, Xiaoan Zhang, Xiaobao Wei, Sixiang Chen, Zhuo Li, Yang Wang, Liyun Li, Xianming Liu, Ming Lu, Shanghang Zhang)</author>
      <guid isPermaLink="false">2507.23318v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>AGA: An adaptive group alignment framework for structured medical cross-modal representation learning</title>
      <link>http://arxiv.org/abs/2507.23402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为自适应分组对齐(AGA)的新框架，用于从配对的医学图像和报告中学习结构化语义，解决了当前医学视觉语言预训练中忽略报告结构和难以获取硬负样本的问题。&lt;h4&gt;背景&lt;/h4&gt;学习医学视觉表征是表征学习的有前途方向，但当前医学领域的视觉语言预训练方法常将临床报告简化为单一实体或碎片化标记，忽略了其固有结构。此外，对比学习框架通常依赖大量硬负样本，这在小规模医学数据集上不切实际。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕获医学图像和报告间结构化语义的框架，无需依赖大量硬负样本，同时保留临床报告的固有结构信息。&lt;h4&gt;方法&lt;/h4&gt;提出AGA框架，包含：1)基于稀疏相似矩阵的双向分组机制，计算文本标记和图像块间相似性并形成视觉组和语言组；2)语言分组阈值门和视觉分组阈值门两个阈值门控模块，动态学习分组阈值；3)基于相似度分数计算组表示；4)实例感知组对齐损失，无需外部负样本；5)双向跨模态分组对齐模块增强细粒度对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在公共和私有数据集上的实验表明，该方法在微调和零样本设置下的图像-文本检索和分类任务中取得了强大性能。&lt;h4&gt;结论&lt;/h4&gt;AGA框架有效解决了医学视觉语言预训练中的结构忽略和负样本获取困难问题，通过自适应分组和双向对齐机制实现了更好的医学图像和报告之间的对齐。&lt;h4&gt;翻译&lt;/h4&gt;从配对的图像和报告中学习医学视觉表征是表征学习中的一个有前途的方向。然而，当前医学领域的视觉语言预训练方法通常将临床报告简化为单一实体或碎片化标记，忽略了它们的固有结构。此外，对比学习框架通常依赖于大量硬负样本，这在小规模医学数据集上是不切实际的。为了应对这些挑战，我们提出了自适应分组对齐(AGA)，一个新框架，能够从配对的医学图像和报告中捕获结构化语义。AGA引入了基于稀疏相似矩阵的双向分组机制。对于每个图像-报告对，我们计算文本标记和图像块之间的细粒度相似性。每个标记选择其最匹配的图像块形成视觉组，每个图像块选择其最相关的标记形成语言组。为了实现自适应分组，我们设计了两个阈值门控模块，称为语言分组阈值门和视觉分组阈值门，它们动态学习分组阈值。组表示基于相似度分数计算为加权平均值。为了将每个标记与其组表示对齐，我们引入了实例感知组对齐损失，它在每个图像-文本对内操作，无需外部负样本。最后，应用双向跨模态分组对齐模块，以增强视觉和语言组表示之间的细粒度对齐。在公共和私有数据集上的广泛实验表明，我们的方法在微调和零样本设置下的图像-文本检索和分类任务中取得了强大的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning medical visual representations from paired images and reports is apromising direction in representation learning. However, currentvision-language pretraining methods in the medical domain often simplifyclinical reports into single entities or fragmented tokens, ignoring theirinherent structure. In addition, contrastive learning frameworks typicallydepend on large quantities of hard negative samples, which is impractical forsmall-scale medical datasets. To tackle these challenges, we propose AdaptiveGrouped Alignment (AGA), a new framework that captures structured semanticsfrom paired medical images and reports. AGA introduces a bidirectional groupingmechanism based on a sparse similarity matrix. For each image-report pair, wecompute fine-grained similarities between text tokens and image patches. Eachtoken selects its top-matching patches to form a visual group, and each patchselects its most related tokens to form a language group. To enable adaptivegrouping, we design two threshold gating modules, called Language GroupedThreshold Gate and Vision Grouped Threshold Gate, which learn groupingthresholds dynamically. Group representations are computed as weighted averagesbased on similarity scores. To align each token with its group representation,we introduce an Instance Aware Group Alignment loss that operates within eachimage-text pair, removing the need for external negatives. Finally, aBidirectional Cross-modal Grouped Alignment module is applied to enhancefine-grained alignment between visual and linguistic group representations.Extensive experiments on public and private datasets show that our methodachieves strong performance on image-text retrieval and classification tasksunder both fine-tuning and zero-shot settings.</description>
      <author>example@mail.com (Wei Li, Xun Gong, Jiao Li, Xiaobin Sun)</author>
      <guid isPermaLink="false">2507.23402v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning-Driven Traffic Sign Perception: Multi-Modal Fusion of Text and Vision</title>
      <link>http://arxiv.org/abs/2507.23331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种结合开放词汇检测和跨模态学习的两阶段框架，用于解决交通标志识别中的长尾分布和小目标检测问题，在TT100K数据集上取得了78.4% mAP的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;交通标志识别作为自动驾驶感知系统的核心组成部分，直接影响车辆的环境感知和驾驶安全。&lt;h4&gt;目的&lt;/h4&gt;解决当前交通标志识别技术面临的两个主要挑战：数据集的长尾分布导致传统网络对低频类别识别性能下降，以及真实场景中小目标、多尺度交通标志的特征提取困难。&lt;h4&gt;方法&lt;/h4&gt;提出两阶段框架：1) 交通标志检测采用NanoVerse YOLO模型，集成RepVL-PAN网络和SPD-Conv模块增强小目标和多尺度特征提取；2) 交通标志分类设计TSR-MCL模型，通过对比视觉Transformer的视觉特征和BERT的语义特征学习鲁棒的频率无关表示。&lt;h4&gt;主要发现&lt;/h4&gt;在TT100K数据集上，方法在长尾检测任务中达到78.4% mAP，分类准确率和召回率分别达到91.8%和88.9%，显著优于主流算法。&lt;h4&gt;结论&lt;/h4&gt;该方法有效缓解了数据不平衡引起的类别混淆问题，在复杂开放世界场景中展现出更高的准确性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;交通标志识别作为自动驾驶感知系统的核心组成部分，直接影响车辆的环境感知和驾驶安全。当前技术面临两个重大挑战：首先，交通标志数据集呈现明显的长尾分布，导致传统卷积网络在处理低频和分布外类别时识别性能显著下降；其次，真实场景中的交通标志主要是小目标且尺度变化大，难以提取多尺度特征。为解决这些问题，我们提出了一种结合开放词汇检测和跨模态学习的新型两阶段框架。对于交通标志检测，我们的NanoVerse YOLO模型集成了可重参数化的视觉语言路径聚合网络(RepVL-PAN)和SPD-Conv模块，专门针对小目标和多尺度目标增强特征提取。对于交通标志分类，我们设计了交通标志识别多模态对比学习模型(TSR-MCL)。通过对比视觉Transformer的视觉特征和基于规则的BERT的语义特征，TSR-MCL学习鲁棒的、频率无关的表示，有效缓解了数据不平衡引起的类别混淆。在TT100K数据集上，我们的方法在长尾检测任务的全类别识别中取得了最先进的78.4% mAP。该模型还获得了91.8%的准确率和88.9%的召回率，显著优于主流算法，在复杂开放世界场景中展现出更高的准确性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic sign recognition, as a core component of autonomous drivingperception systems, directly influences vehicle environmental awareness anddriving safety. Current technologies face two significant challenges: first,the traffic sign dataset exhibits a pronounced long-tail distribution,resulting in a substantial decline in recognition performance of traditionalconvolutional networks when processing low-frequency and out-of-distributionclasses; second, traffic signs in real-world scenarios are predominantly smalltargets with significant scale variations, making it difficult to extractmulti-scale features.To overcome these issues, we propose a novel two-stageframework combining open-vocabulary detection and cross-modal learning. Fortraffic sign detection, our NanoVerse YOLO model integrates a reparameterizablevision-language path aggregation network (RepVL-PAN) and an SPD-Conv module tospecifically enhance feature extraction for small, multi-scale targets. Fortraffic sign classification, we designed a Traffic Sign Recognition MultimodalContrastive Learning model (TSR-MCL). By contrasting visual features from aVision Transformer with semantic features from a rule-based BERT, TSR-MCLlearns robust, frequency-independent representations, effectively mitigatingclass confusion caused by data imbalance. On the TT100K dataset, our methodachieves a state-of-the-art 78.4% mAP in the long-tail detection task forall-class recognition. The model also obtains 91.8% accuracy and 88.9% recall,significantly outperforming mainstream algorithms and demonstrating superioraccuracy and generalization in complex, open-world scenarios.</description>
      <author>example@mail.com (Qiang Lu, Waikit Xiu, Xiying Li, Shenyu Hu, Shengbo Sun)</author>
      <guid isPermaLink="false">2507.23331v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding Space</title>
      <link>http://arxiv.org/abs/2507.23188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE TMM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多模态动作检索框架，整合文本、音频、视频和动作四种模态，通过细粒度联合嵌入空间和序列级对比学习方法，显著提升了动作检索性能。&lt;h4&gt;背景&lt;/h4&gt;动作检索对动作获取至关重要，相比动作生成具有更高精确度、真实感、可控性和可编辑性。现有方法利用对比学习构建统一嵌入空间进行文本或视觉模态的动作检索，但缺乏直观友好的交互模式，且常忽略模态的序列表示以提升检索性能。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提出将文本、音频、视频和动作四种模态对齐在细粒度联合嵌入空间中的框架，首次在动作检索中引入音频以增强用户沉浸感和便利性。&lt;h4&gt;方法&lt;/h4&gt;通过序列级对比学习方法实现细粒度空间，捕捉跨模态关键细节以实现更好对齐。通过合成多样化音频记录扩展现有文本-动作数据集，创建两个多模态动作检索数据集进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在多个子任务上优于最先进方法，包括在HumanML3D数据集上文本到动作检索的R@10提高10.16%，视频到动作检索的R@1提高25.43%。四模态框架显著优于三模态对应框架，突显多模态动作检索的潜力。&lt;h4&gt;结论&lt;/h4&gt;多模态动作检索框架通过整合四种模态并在细粒度联合嵌入空间中对齐，显著提升了动作检索性能，为动作获取领域带来新可能性。&lt;h4&gt;翻译&lt;/h4&gt;动作检索对于动作获取至关重要，相比动作生成具有更高的精确度、真实感、可控性和可编辑性。现有方法利用对比学习构建统一嵌入空间进行文本或视觉模态的动作检索，但缺乏更直观和用户友好的交互模式，并且常常忽略了大多数模态的序列表示以提升检索性能。为了解决这些限制，我们提出了一种将文本、音频、视频和动作四种模态对齐在细粒度联合嵌入空间中的框架，首次在动作检索中引入音频以增强用户沉浸感和便利性。这种细粒度空间是通过序列级对比学习方法实现的，捕捉跨模态的关键细节以实现更好的对齐。为了评估我们的框架，我们通过合成但多样化的音频记录扩展现有的文本-动作数据集，创建了两个多模态动作检索数据集。实验结果表明，在多个子任务上，我们的方法优于最先进的方法，包括在HumanML3D数据集上文本到动作检索的R@10提高了10.16%，视频到动作检索的R@1提高了25.43%。此外，我们的结果显示四模态框架显著优于三模态对应框架，突显了多模态动作检索在推进动作获取方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion retrieval is crucial for motion acquisition, offering superiorprecision, realism, controllability, and editability compared to motiongeneration. Existing approaches leverage contrastive learning to construct aunified embedding space for motion retrieval from text or visual modality.However, these methods lack a more intuitive and user-friendly interaction modeand often overlook the sequential representation of most modalities forimproved retrieval performance. To address these limitations, we propose aframework that aligns four modalities -- text, audio, video, and motion --within a fine-grained joint embedding space, incorporating audio for the firsttime in motion retrieval to enhance user immersion and convenience. Thisfine-grained space is achieved through a sequence-level contrastive learningapproach, which captures critical details across modalities for betteralignment. To evaluate our framework, we augment existing text-motion datasetswith synthetic but diverse audio recordings, creating two multi-modal motionretrieval datasets. Experimental results demonstrate superior performance overstate-of-the-art methods across multiple sub-tasks, including an 10.16%improvement in R@10 for text-to-motion retrieval and a 25.43% improvement inR@1 for video-to-motion retrieval on the HumanML3D dataset. Furthermore, ourresults show that our 4-modal framework significantly outperforms its 3-modalcounterpart, underscoring the potential of multi-modal motion retrieval foradvancing motion acquisition.</description>
      <author>example@mail.com (Shiyao Yu, Zi-An Wang, Kangning Yin, Zheng Tian, Mingyuan Zhang, Weixin Si, Shihao Zou)</author>
      <guid isPermaLink="false">2507.23188v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>GCL-GCN: Graphormer and Contrastive Learning Enhanced Attributed Graph Clustering Network</title>
      <link>http://arxiv.org/abs/2507.19095v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The source code for this study is available at  https://github.com/YF-W/GCL-GCN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GCL-GCN的新型深度图聚类模型，通过引入Graphormer模块和对比学习模块，有效解决了现有模型在处理稀疏和异构图数据时的局限性，显著提升了聚类质量和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;带属性图聚类在现代数据分析中具有重要意义，但由于图数据的复杂性和节点属性的异构性，利用图信息进行聚类仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够有效捕获局部依赖性和复杂结构的图聚类模型，以处理稀疏和异构图数据。&lt;h4&gt;方法&lt;/h4&gt;提出GCL-GCN模型，包含创新的Graphormer模块(结合中心性编码和空间关系)和对比学习模块(通过对比学习增强特征判别能力)。&lt;h4&gt;主要发现&lt;/h4&gt;在六个数据集上的实验表明，GCL-GCN在聚类质量和鲁棒性方面优于14种先进方法；在Cora数据集上，ACC、NMI和ARI分别提高了4.94%、13.01%和10.97%。&lt;h4&gt;结论&lt;/h4&gt;GCL-GCN通过有效捕获节点间的全局和局部信息，显著提升了节点表示质量，是一种先进的图聚类方法。&lt;h4&gt;翻译&lt;/h4&gt;带属性图聚类在现代数据分析中具有重要意义。然而，由于图数据的复杂性和节点属性的异构性，利用图信息进行聚类仍然具有挑战性。为此，我们提出了一种新型深度图聚类模型GCL-GCN，专门设计用于解决现有模型在处理稀疏和异构图数据时捕获局部依赖性和复杂结构方面的局限性。GCL-GCN引入了一个创新的Graphormer模块，该模块结合了中心性编码和空间关系，有效捕获节点之间的全局和局部信息，从而增强节点表示的质量。此外，我们还提出了一种新颖的对比学习模块，显著增强了特征表示的判别能力。在预训练阶段，该模块通过对原始特征矩阵进行对比学习来增加特征区分度，确保随后的图卷积和聚类任务具有更可识别的初始表示。在六个数据集上的大量实验结果表明，GCL-GCN在聚类质量和鲁棒性方面优于14种先进方法。特别是在Cora数据集上，与主要对比方法MBN相比，ACC、NMI和ARI分别提高了4.94%、13.01%和10.97%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Attributed graph clustering holds significant importance in modern dataanalysis. However, due to the complexity of graph data and the heterogeneity ofnode attributes, leveraging graph information for clustering remainschallenging. To address this, we propose a novel deep graph clustering model,GCL-GCN, specifically designed to address the limitations of existing models incapturing local dependencies and complex structures when dealing with sparseand heterogeneous graph data. GCL-GCN introduces an innovative Graphormermodule that combines centrality encoding and spatial relationships, effectivelycapturing both global and local information between nodes, thereby enhancingthe quality of node representations. Additionally, we propose a novelcontrastive learning module that significantly enhances the discriminativepower of feature representations. In the pre-training phase, this moduleincreases feature distinction through contrastive learning on the originalfeature matrix, ensuring more identifiable initial representations forsubsequent graph convolution and clustering tasks. Extensive experimentalresults on six datasets demonstrate that GCL-GCN outperforms 14 advancedmethods in terms of clustering quality and robustness. Specifically, on theCora dataset, it improves ACC, NMI, and ARI by 4.94%, 13.01%, and 10.97%,respectively, compared to the primary comparison method MBN.</description>
      <author>example@mail.com (Binxiong Li, Xu Xiang, Xue Li, Quanzhou Lou, Binyu Zhao, Yujie Liu, Huijie Tang, Benhan Yang)</author>
      <guid isPermaLink="false">2507.19095v2</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Gloss: A Hand-Centric Framework for Gloss-Free Sign Language Translation</title>
      <link>http://arxiv.org/abs/2507.23575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BeyondGloss是一种新型的无词汇表手语翻译框架，利用视频大语言模型的时空推理能力，通过生成手部运动的细粒度文本描述、对比对齐模块、特征提炼和对比损失等方法，有效弥合了视觉和语言信息之间的模态差距，在基准测试上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;手语翻译是一个具有挑战性的任务，需要弥合视觉和语言信息之间的模态差距，同时捕捉手部形状和运动的细微变化。现有视频大语言模型难以详细建模长视频。&lt;h4&gt;目的&lt;/h4&gt;介绍BeyondGloss框架，解决现有VideoLLMs的局限性，提高手语翻译的准确性和效果。&lt;h4&gt;方法&lt;/h4&gt;提出生成手部运动的细粒度、时间感知文本描述的方法；通过对比对齐模块将描述与视频特征对齐；从HaMeR中提炼细粒度特征；应用手语视频表示与目标语言嵌入之间的对比损失以减少模态差距。&lt;h4&gt;主要发现&lt;/h4&gt;BeyondGloss在Phoenix14T和CSL-Daily基准测试上取得了最先进的性能，证明了所提出框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;BeyondGloss框架在手语翻译任务上表现出色，作者将在论文被接受后发布代码。&lt;h4&gt;翻译&lt;/h4&gt;手语翻译是一项具有挑战性的任务，需要弥合视觉和语言信息之间的模态差距，同时捕捉手部形状和运动的细微变化。为应对这些挑战，我们介绍了BeyondGloss，一种新型的无词汇表手语翻译框架，利用视频大语言模型的时空推理能力。由于现有VideoLLMs难以详细建模长视频，我们提出了一种生成手部运动的细粒度、时间感知文本描述的新方法。对比对齐模块在预训练期间将这些描述与视频特征对齐，鼓励模型专注于以手为中心的时间动态，更有效地区分手语。此外，我们从HaMeR中提炼细粒度特征，以丰富手部特定表示。我们还应用了手语视频表示与目标语言嵌入之间的对比损失，以在预训练期间减少模态差距。BeyondGloss在Phoenix14T和CSL-Daily基准测试上取得了最先进的性能，证明了所提出框架的有效性。我们将在论文被接受后发布代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign Language Translation (SLT) is a challenging task that requires bridgingthe modality gap between visual and linguistic information while capturingsubtle variations in hand shapes and movements. To address these challenges, weintroduce \textbf{BeyondGloss}, a novel gloss-free SLT framework that leveragesthe spatio-temporal reasoning capabilities of Video Large Language Models(VideoLLMs). Since existing VideoLLMs struggle to model long videos in detail,we propose a novel approach to generate fine-grained, temporally-aware textualdescriptions of hand motion. A contrastive alignment module aligns thesedescriptions with video features during pre-training, encouraging the model tofocus on hand-centric temporal dynamics and distinguish signs more effectively.To further enrich hand-specific representations, we distill fine-grainedfeatures from HaMeR. Additionally, we apply a contrastive loss between signvideo representations and target language embeddings to reduce the modality gapin pre-training. \textbf{BeyondGloss} achieves state-of-the-art performance onthe Phoenix14T and CSL-Daily benchmarks, demonstrating the effectiveness of theproposed framework. We will release the code upon acceptance of the paper.</description>
      <author>example@mail.com (Sobhan Asasi, Mohamed Ilyas Lakhal, Ozge Mercanoglu Sincan, Richard Bowden)</author>
      <guid isPermaLink="false">2507.23575v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>The Effect of Prior Parameters on Standardized Kalman Filter-Based EEG Source Localization</title>
      <link>http://arxiv.org/abs/2507.23450v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发和优化了标准卡尔曼滤波器(SKF)框架内的高斯先验模型，用于同时检测皮层和皮层下活动。通过合成数据测试，发现提高标准化指数并结合平滑技术能显著改善深度定位准确性。&lt;h4&gt;背景&lt;/h4&gt;EEG源定位是神经科学的关键工具，用于解决不适定的逆问题。贝叶斯框架可通过先验知识约束提供解决方案。标准卡尔曼滤波器(SKF)是一种动态贝叶斯方法，结合时间建模与高斯先验结构，通过后标准化步骤解决深度偏差问题。&lt;h4&gt;目的&lt;/h4&gt;开发和优化SKF框架内的高斯先验模型，用于同时检测皮层和皮层下活动，确定在不同噪声水平下重建深部和浅表源的有效先验参数配置，并研究RTS平滑在增强源可分离性方面的作用。&lt;h4&gt;方法&lt;/h4&gt;使用类似体感诱发电位(SEP)的P20/N20组分的合成数据，测试不同先验参数配置下深部和浅表源的重建效果，并应用RTS平滑技术增强源可分离性。&lt;h4&gt;主要发现&lt;/h4&gt;将标准化指数提高到1.25并结合平滑技术，在低噪声水平下显著提高了深度定位的准确性。&lt;h4&gt;结论&lt;/h4&gt;优化后的高斯先验模型和标准化参数配置能有效改善EEG源定位中的深度偏差问题，特别是对于低噪声环境下的深部活动检测。&lt;h4&gt;翻译&lt;/h4&gt;EEG源定位是神经科学中的关键工具，应用范围从癫痫诊断到认知研究。它涉及解决一个不适定的逆问题，除非有先验知识的约束，否则没有唯一解。贝叶斯框架能够整合这些先验知识，通常通过先验模型进行编码。已提出了各种源定位算法，它们在如何整合先验知识方面存在显著差异。一些方法依赖于解剖或功能约束，而其他方法使用统计分布或基于采样的技术。在这种背景下，标准卡尔曼滤波器(SKF)代表了一种动态贝叶斯方法，它将时间建模与高斯先验结构相结合。通过一个后标准化步骤解决了源定位中的深度偏差问题，该步骤使皮层深度之间的敏感性相等，并使深部活动检测成为可能。本研究专注于在SKF框架内开发和优化高斯先验模型，用于同时检测皮层和皮层下活动。使用类似体感诱发电位(SEP)的P20/N20组分的合成数据，来确定在不同噪声水平下重建深部和浅表源的有效先验参数配置。还研究了RTS平滑在增强源可分离性方面的作用。结果表明，将标准化指数提高到1.25并结合平滑技术，在低噪声水平下显著提高了深度定位的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; EEG Source localization is a critical tool in neuroscience, with applicationsranging from epilepsy diagnosis to cognitive research. It involves solving anill-posed inverse problem that lacks a unique solution unless constrained byprior knowledge. The Bayesian framework enables the incorporation of suchknowledge, typically encoded through prior models. Various algorithms have beenproposed for source localization, and they differ significantly in how priorknowledge is incorporated. Some approaches rely on anatomical or functionalconstraints, while others use statistical distributions or sampling-basedtechniques. In this landscape, the Standardized Kalman Filter (SKF) representsa dynamic Bayesian approach that integrates temporal modeling with a Gaussianprior structure. It addresses the depth bias, a common limitation in sourcelocalization, through a post-hoc standardization step that equalizessensitivity across cortical depths and makes deep activity detection feasible.  This study focuses on the development and optimization of Gaussian priormodels within the SKF framework for simultaneous cortical and sub-corticalactivity detection. Synthetic data similar to the P20 / N20 component of thesomatosensory evoked potentials (SEP) was used to identify effective priorparameter configurations for reconstructing both deep and superficial sourcesunder different noise levels. We also investigated the role of RTS smoothing inenhancing source separability. Our results indicate that raising thestandardization exponent to 1.25, along with smoothing, significantly improvesdepth localization accuracy at low noise levels.</description>
      <author>example@mail.com (Dilshanie Prasikala, Joonas Lahtinen, Alexandra Koulouri, Sampsa Pursiainen)</author>
      <guid isPermaLink="false">2507.23450v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>A Deep Dive into Generic Object Tracking: A Survey</title>
      <link>http://arxiv.org/abs/2507.23251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  55 pages, 29 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对通用目标跟踪领域进行了全面综述，涵盖基于Siamese的跟踪器、判别式跟踪器和基于Transformer的方法三种主要范式，特别强调了快速发展的基于Transformer的方法。&lt;h4&gt;背景&lt;/h4&gt;通用目标跟踪在计算机视觉中是一项重要且具有挑战性的任务，主要面临复杂的空间-时间动态特性，特别是在存在遮挡、相似干扰物和外观变化的情况下。&lt;h4&gt;目的&lt;/h4&gt;提供一个对所有三种跟踪类别的全面回顾，特别关注基于Transformer的方法，通过定性和定量比较分析各类方法的核心设计原则、创新点和局限性。&lt;h4&gt;方法&lt;/h4&gt;采用新的分类方法，对代表性方法进行统一的视觉和表格比较，从多视角组织现有跟踪器，并总结主要评估基准。&lt;h4&gt;主要发现&lt;/h4&gt;基于Transformer的跟踪方法因其强大的空间-时间建模能力而取得了快速发展，成为目标跟踪领域的重要进展。&lt;h4&gt;结论&lt;/h4&gt;通过对三类跟踪范式的全面回顾和比较，深入理解了目标跟踪领域的当前状态，并突显了基于Transformer方法的发展趋势。&lt;h4&gt;翻译&lt;/h4&gt;通用目标跟踪在计算机视觉中仍然是一个重要且具有挑战性的任务，由于复杂的空间-时间动态特性，特别是在存在遮挡、相似干扰物和外观变化的情况下。在过去的二十年里，已经引入了多种跟踪范式，包括基于Siamese的跟踪器、判别式跟踪器，以及最近突出的基于Transformer的方法，来解决这些挑战。虽然该领域已有一些综述论文要么集中在单一类别上，要么广泛涵盖多个类别以捕捉进展，但我们的论文对所有三个类别进行了全面回顾，特别强调了快速发展的基于Transformer的方法。我们通过定性和定量比较分析了每种方法的核心设计原则、创新点和局限性。我们的研究引入了一种新的分类方法，并对代表性方法提供了统一的视觉和表格比较。此外，我们从多个视角组织了现有的跟踪器，总结了主要的评估基准，突显了基于Transformer的跟踪方法由于其强大的空间-时间建模能力而取得的快速发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generic object tracking remains an important yet challenging task in computervision due to complex spatio-temporal dynamics, especially in the presence ofocclusions, similar distractors, and appearance variations. Over the past twodecades, a wide range of tracking paradigms, including Siamese-based trackers,discriminative trackers, and, more recently, prominent transformer-basedapproaches, have been introduced to address these challenges. While a fewexisting survey papers in this field have either concentrated on a singlecategory or widely covered multiple ones to capture progress, our paperpresents a comprehensive review of all three categories, with particularemphasis on the rapidly evolving transformer-based methods. We analyze the coredesign principles, innovations, and limitations of each approach through bothqualitative and quantitative comparisons. Our study introduces a novelcategorization and offers a unified visual and tabular comparison ofrepresentative methods. Additionally, we organize existing trackers frommultiple perspectives and summarize the major evaluation benchmarks,highlighting the fast-paced advancements in transformer-based tracking drivenby their robust spatio-temporal modeling capabilities.</description>
      <author>example@mail.com (Fereshteh Aghaee Meibodi, Shadi Alijani, Homayoun Najjaran)</author>
      <guid isPermaLink="false">2507.23251v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection</title>
      <link>http://arxiv.org/abs/2507.23567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了3D-MOOD，第一个端到端的开放场景单目3D物体检测器，解决了现有方法在封闭场景设置中的局限性，通过创新方法实现了在封闭和开放场景下的高性能表现。&lt;h4&gt;背景&lt;/h4&gt;单目3D物体检测在机器人和AR/VR等多种应用中具有重要价值。然而，现有方法仅限于封闭场景设置，即训练和测试集包含相同的场景和/或物体类别。现实世界的应用常常引入新的环境和新颖的物体类别，这些方法难以应对。&lt;h4&gt;目的&lt;/h4&gt;解决开放场景设置下的单目3D物体检测问题，并提出第一个端到端的3D单目开放场景物体检测器（3D-MOOD），以提高在多样化场景和物体类别下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;设计了3D边界框头部，将开放场景的2D检测提升到3D空间，实现2D和3D任务的端到端联合训练；使用几何先验条件处理物体查询，克服3D估计在多样化场景中的泛化问题；设计了规范图像空间，以实现更高效的跨数据集训练。&lt;h4&gt;主要发现&lt;/h4&gt;3D-MOOD在封闭场景（Omni3D）和开放场景（从Omni3D到Argoverse 2、ScanNet）的评估中都取得了新的最先进结果，证明了其在不同场景下的有效性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;3D-MOOD成功解决了开放场景下单目3D物体检测的挑战，通过创新的方法设计，实现了在封闭和开放场景下的高性能表现，为实际应用提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;单目3D物体检测在机器人学和AR/VR等众多应用中具有重要价值。现有方法仅限于封闭场景设置，其中训练和测试集包含相同的场景和/或物体类别。然而，现实世界的应用常常引入新的环境和新颖的物体类别，对这些方法构成了挑战。在本文中，我们解决了开放场景设置下的单目3D物体检测问题，并引入了第一个端到端的3D单目开放场景物体检测器（3D-MOOD）。我们提出通过设计的3D边界框头部将开放场景的2D检测提升到3D空间，使2D和3D任务能够进行端到端的联合训练，从而获得更好的整体性能。我们使用几何先验条件处理物体查询，克服了3D估计在多样化场景中的泛化问题。为了进一步提高性能，我们设计了规范图像空间以实现更高效的跨数据集训练。我们在封闭场景（Omni3D）和开放场景（从Omni3D到Argoverse 2、ScanNet）上评估了3D-MOOD，并取得了新的最先进结果。代码和模型可在royyang0714.github.io/3D-MOOD获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目3D目标检测在开放环境下的挑战，即现有方法只能在训练和测试数据集包含相同场景和物体类别的封闭集设置下工作。这个问题在现实中非常重要，因为真实世界应用（如机器人技术和AR/VR）经常需要面对新的环境和未知物体类别，封闭集方法无法适应这些变化，限制了实际应用效果。开放集检测能使模型识别和定位训练时未见过的物体，这在实际场景中非常实用且必要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出开放集单目3D目标检测的两个主要障碍：跨模态学习困难（3D数据缺乏丰富视觉-语言对）和单目深度估计在不同场景间的泛化挑战。针对这些问题，他们提出利用通用单目深度估计方法的良好泛化能力，设计3D边界框头部将2D检测结果提升到3D空间。作者借鉴了G-DINO作为2D开放集检测器基础，采用检测Transformer架构，并参考了Cube R-CNN和Uni-MODE在统一模型方面的思路，以及UniDepth在相机条件下的深度估计方法，最终形成了3D-MOOD的完整框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过设计的3D边界框头部将开放集2D检测提升到3D空间，实现2D和3D任务的端到端联合训练，利用几何先验增强模型在不同场景间的泛化能力。整体流程为：1)接收单目图像和语言提示；2)提取图像和文本特征；3)通过transformer进行视觉-语言融合；4)生成2D检测结果；5)使用3D边界框头部将2D结果提升为3D边界框；6)利用几何感知查询生成增强模型泛化能力；7)通过辅助深度估计提供场景理解；8)输出包含位置、尺寸和方向的3D检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个端到端的开放集单目3D目标检测器；2)创新的3D边界框头部设计，实现可微分提升；3)几何感知的3D查询生成，利用相机内参和深度估计增强泛化；4)规范图像空间解决图像、相机内参和深度间的歧义。相比之前工作，3D-MOOD最大的不同是实现了真正的开放集3D检测，能够识别未知物体类别；而之前的方法如Cube R-CNN和Uni-MODE只能在封闭集下工作。此外，3D-MOOD可通过可微分参数实现端到端训练，而OVM3D-Det等方法无法使用3D数据进行端到端训练。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 3D-MOOD首次实现了端到端的单目开放集3D目标检测，通过创新的2D到3D提升机制和几何感知查询生成，显著提升了模型在未知场景和未知物体类别上的检测性能，为实际应用中的开放环境视觉理解提供了新解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D object detection is valuable for various applications such asrobotics and AR/VR. Existing methods are confined to closed-set settings, wherethe training and testing sets consist of the same scenes and/or objectcategories. However, real-world applications often introduce new environmentsand novel object categories, posing a challenge to these methods. In thispaper, we address monocular 3D object detection in an open-set setting andintroduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD).We propose to lift the open-set 2D detection into 3D space through our designed3D bounding box head, enabling end-to-end joint training for both 2D and 3Dtasks to yield better overall performance. We condition the object queries withgeometry prior and overcome the generalization for 3D estimation across diversescenes. To further improve performance, we design the canonical image space formore efficient cross-dataset training. We evaluate 3D-MOOD on both closed-setsettings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), andachieve new state-of-the-art results. Code and models are available atroyyang0714.github.io/3D-MOOD.</description>
      <author>example@mail.com (Yung-Hsu Yang, Luigi Piccinelli, Mattia Segu, Siyuan Li, Rui Huang, Yuqian Fu, Marc Pollefeys, Hermann Blum, Zuria Bauer)</author>
      <guid isPermaLink="false">2507.23567v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy</title>
      <link>http://arxiv.org/abs/2507.21358v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The manuscript has been accepted by ICONIP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Collaborative Perceiver (CoP)的多任务学习框架，通过利用空间占用作为辅助信息，改进了基于视觉的鸟瞰图3D目标检测方法。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的鸟瞰图3D目标检测在自动驾驶领域取得了显著进展，具有成本效益和丰富的上下文信息。然而，现有方法通常通过折叠提取的目标特征来构建BEV表示，忽略了道路和人行道等内在环境上下文。&lt;h4&gt;目的&lt;/h4&gt;为了缓解现有方法无法全面感知物理世界特征的问题，作者引入了CoP框架，挖掘3D目标检测和占用预测任务之间的一致结构和概念相似性，弥合空间表示和特征精化的差距。&lt;h4&gt;方法&lt;/h4&gt;1) 提出了一种管道生成包含局部密度信息的密集占用真实值；2) 采用基于体素高度引导的采样策略提炼细粒度局部特征；3) 开发全局-局部协作特征融合模块集成两个任务的互补知识。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes基准上的实验表明，CoP优于现有基于视觉的框架，在测试集上实现了49.5%的平均精度和59.2%的检测分数。&lt;h4&gt;结论&lt;/h4&gt;CoP通过多任务学习和特征融合有效改进了BEV表示，使检测器能够更全面地感知物理世界的特征。&lt;h4&gt;翻译&lt;/h4&gt;基于视觉的鸟瞰图3D目标检测在自动驾驶领域通过提供成本效益和丰富的上下文信息取得了显著进展。然而，现有方法通常通过折叠提取的目标特征来构建BEV表示，忽略了道路和人行道等内在环境上下文。这阻碍了检测器全面感知物理世界的特征。为了缓解这一问题，我们引入了Collaborative Perceiver (CoP)多任务学习框架，利用空间占用作为辅助信息，挖掘3D目标检测和占用预测任务之间共享的一致结构和概念相似性，弥合空间表示和特征精化的差距。为此，我们首先提出了一种管道来生成包含局部密度信息的密集占用真实值，用于重建详细的环境信息。接下来，我们采用基于体素高度引导的采样策略，根据不同的物体属性提炼细粒度的局部特征。此外，我们还开发了一种全局-局部协作特征融合模块，无缝集成两个任务之间的互补知识，从而组成更强大的BEV表示。在nuScenes基准上的大量实验表明，CoP优于现有的基于视觉的框架，在测试集上实现了49.5%的平均精度和59.2%的检测分数。代码和补充材料可在提供的链接获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有视觉3D物体检测方法无法全面感知物理世界特征的问题，特别是在识别具有独特或不规则几何形状物体方面的局限性。这个问题在自动驾驶领域至关重要，因为准确的物体识别和环境理解是确保系统在复杂交通场景中安全运行的基础，同时环境上下文信息（如道路、人行道等）对于导航和决策也必不可少。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D检测方法的不足，然后提出利用多任务学习框架，将3D物体检测和占用预测相结合，挖掘两个任务之间的互补知识。方法借鉴了现有工作中的视图转换技术（如Lift-Splat-Shoot）、体素化方法和注意力机制，但创新性地引入了局部密度感知的密集占用生成管道、体素高度引导的采样策略和全局-局部协作特征融合模块，以解决传统方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多任务协作学习，结合3D物体检测和3D占用预测两个任务的互补知识，利用局部密度信息捕获物体的细粒度结构，并通过全局-局部特征融合构建更鲁棒的BEV表示。整体流程包括：1)输入多视图图像并提取特征；2)通过LSS将2D特征转换为3D体素特征；3)分别提取全局特征和通过VHS策略提取局部特征；4)通过CFF模块融合全局和局部特征；5)将融合后的特征分别送入3D占用预测头和3D物体检测头；6)结合两个任务的损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)局部密度感知的密集占用(LDO)生成管道，从多帧激光雷达数据生成包含局部密度信息的密集占用地面真实值；2)体素高度引导的采样(VHS)策略，根据占用预测的高度分布提取细粒度局部特征；3)全局-局部协作特征融合(CFF)模块，自适应融合全局和局部特征。相比之前的工作，CoP不是单一任务方法，而是通过多任务学习利用互补信息；考虑了非均匀点密度分布，更好地表示物体细粒度结构；避免了传统方法中沿高度维度压缩特征导致的信息丢失问题，在nuScenes基准测试上实现了最先进的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了协作感知器(CoP)，一个通过局部密度感知的空间占用信息和全局-局部特征融合来提升视觉3D物体检测性能的多任务学习框架，在nuScenes基准测试上实现了49.5%的mAP和59.2%的NDS，超过了现有视觉框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-based bird's-eye-view (BEV) 3D object detection has advancedsignificantly in autonomous driving by offering cost-effectiveness and richcontextual information. However, existing methods often construct BEVrepresentations by collapsing extracted object features, neglecting intrinsicenvironmental contexts, such as roads and pavements. This hinders detectorsfrom comprehensively perceiving the characteristics of the physical world. Toalleviate this, we introduce a multi-task learning framework, CollaborativePerceiver (CoP), that leverages spatial occupancy as auxiliary information tomine consistent structural and conceptual similarities shared between 3D objectdetection and occupancy prediction tasks, bridging gaps in spatialrepresentations and feature refinement. To this end, we first propose apipeline to generate dense occupancy ground truths incorporating local densityinformation (LDO) for reconstructing detailed environmental information. Next,we employ a voxel-height-guided sampling (VHS) strategy to distill fine-grainedlocal features according to distinct object properties. Furthermore, we developa global-local collaborative feature fusion (CFF) module that seamlesslyintegrates complementary knowledge between both tasks, thus composing morerobust BEV representations. Extensive experiments on the nuScenes benchmarkdemonstrate that CoP outperforms existing vision-based frameworks, achieving49.5\% mAP and 59.2\% NDS on the test set. Code and supplementary materials areavailable at this link https://github.com/jichengyuan/Collaborative-Perceiver.</description>
      <author>example@mail.com (Jicheng Yuan, Manh Nguyen Duc, Qian Liu, Manfred Hauswirth, Danh Le Phuoc)</author>
      <guid isPermaLink="false">2507.21358v3</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios</title>
      <link>http://arxiv.org/abs/2507.23660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DuLoc的鲁棒且准确的LiDAR定位方法，通过紧密耦合LiDAR-惯性里程计与基于离线地图的定位，并集成动态实时地图，解决了传统方法在长期环境变化中的局限性，在大型变化的户外环境中优于其他最先进的LiDAR定位系统。&lt;h4&gt;背景&lt;/h4&gt;LiDAR定位是自主系统中的关键组成部分，但现有方法在平衡重复性、准确性和环境适应性方面面临持续挑战。传统仅依赖离线地图的点云注册方法在面对长期环境变化时往往表现出有限的鲁棒性，导致在动态现实场景中出现定位漂移和可靠性下降。&lt;h4&gt;目的&lt;/h4&gt;解决现有LiDAR定位方法在平衡重复性、准确性和环境适应性方面的挑战，特别是在处理长期环境变化和动态现实场景中的定位漂移和可靠性下降问题。&lt;h4&gt;方法&lt;/h4&gt;提出DuLoc方法，紧密耦合LiDAR-惯性里程计与基于离线地图的定位，引入恒定速度运动模型减轻现实场景中的异常噪声，开发将先验全局地图与动态实时局部地图无缝集成的LiDAR定位框架，实现无界和变化环境中的鲁棒定位。&lt;h4&gt;主要发现&lt;/h4&gt;在超大型港口进行的实验中，涉及32辆智能引导车(IGV)的2856小时运行数据，结果表明该系统在大型变化的户外环境中优于其他最先进的LiDAR定位系统。&lt;h4&gt;结论&lt;/h4&gt;DuLoc方法通过紧密耦合LiDAR-惯性里程计与离线地图定位，并集成动态实时地图，有效解决了传统方法在长期环境变化和动态场景中的局限性，实现了更准确、更鲁棒的定位性能。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的定位是自主系统中的关键组成部分，然而现有方法在平衡重复性、准确性和环境适应性方面面临持续挑战。传统仅依赖离线地图的点云注册方法在面对长期环境变化时往往表现出有限的鲁棒性，导致在动态现实场景中出现定位漂移和可靠性下降。为解决这些挑战，本文提出了DuLoc，一种鲁棒且准确的定位方法，该方法将LiDAR-惯性里程计与基于离线地图的定位紧密耦合，并采用恒定速度运动模型来减轻现实场景中的异常噪声。具体而言，我们开发了一种基于LiDAR的定位框架，该框架将先验全局地图与动态实时局部地图无缝集成，使得在无界和变化环境中实现鲁棒定位成为可能。本研究在涉及32辆智能引导车(IGV)的2856小时运行数据的超大型港口进行了大量现实实验。结果表明，我们的系统在大型变化的户外环境中优于其他最先进的LiDAR定位系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based localization serves as a critical component in autonomoussystems, yet existing approaches face persistent challenges in balancingrepeatability, accuracy, and environmental adaptability. Traditional pointcloud registration methods relying solely on offline maps often exhibit limitedrobustness against long-term environmental changes, leading to localizationdrift and reliability degradation in dynamic real-world scenarios. To addressthese challenges, this paper proposes DuLoc, a robust and accurate localizationmethod that tightly couples LiDAR-inertial odometry with offline map-basedlocalization, incorporating a constant-velocity motion model to mitigateoutlier noise in real-world scenarios. Specifically, we develop a LiDAR-basedlocalization framework that seamlessly integrates a prior global map withdynamic real-time local maps, enabling robust localization in unbounded andchanging environments. Extensive real-world experiments in ultra unbounded portthat involve 2,856 hours of operational data across 32 Intelligent GuidedVehicles (IGVs) are conducted and reported in this study. The results attaineddemonstrate that our system outperforms other state-of-the-art LiDARlocalization systems in large-scale changing outdoor environments.</description>
      <author>example@mail.com (Haoxuan Jiang, Peicong Qian, Yusen Xie, Xiaocong Li, Ming Liu, Jun Ma)</author>
      <guid isPermaLink="false">2507.23660v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization</title>
      <link>http://arxiv.org/abs/2507.23569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于3D高斯溅射(3DGS)的视觉定位方法，通过结合显式几何模型和隐式特征场，实现了准确且保护隐私的视觉定位。&lt;h4&gt;背景&lt;/h4&gt;视觉定位是在已知环境中估计相机姿态的任务，传统方法可能在准确性和隐私保护方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种既能保持高精度又能保护隐私的视觉定位方法。&lt;h4&gt;方法&lt;/h4&gt;提出高斯溅射特征场(GSFFs)，结合3DGS的密集几何信息和可微分光栅化算法学习鲁棒特征表示；通过对比框架对齐3D尺度感知特征场和2D特征编码器；利用3D结构感知聚类过程正则化表示学习；将特征转换为分割用于隐私保护定位；通过查询图像与GSFFs渲染特征的对齐实现姿态精炼。&lt;h4&gt;主要发现&lt;/h4&gt;在多个真实世界数据集上评估显示，所提出的隐私和非隐私保护定位流程达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;基于3DGS的视觉定位方法在准确性和隐私保护方面都表现出色，是一种有效的视觉定位解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视觉定位是在已知环境中估计相机姿态的任务。在本文中，我们利用基于3D高斯溅射(3DGS)的表示方法进行准确且保护隐私的视觉定位。我们提出了高斯溅射特征场(GSFFs)，这是一种用于视觉定位的场景表示方法，结合了显式几何模型(3DGS)和隐式特征场。我们利用3DGS的密集几何信息和可微分光栅化算法来学习基于3D的鲁棒特征表示。特别是，我们通过对比框架将3D尺度感知特征场和2D特征编码器对齐到共同的嵌入空间。使用3D结构感知聚类过程，我们进一步正则化表示学习，并将特征无缝转换为分割，可用于保护隐私的视觉定位。姿态精炼涉及将查询图像的特征图或分割与从GSFFs场景表示渲染的特征图进行对齐，用于实现定位。在多个真实世界数据集上评估的隐私和非隐私保护定位流程显示出最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉定位任务中的隐私保护问题。视觉定位是估计相机在已知环境中的姿态，是自动驾驶和自主机器人的核心技术。然而，传统基于特征匹配的方法虽然准确，但存在隐私泄露风险，因为图像细节可能从特征描述符中恢复。随着视觉定位系统越来越多地通过云端部署，保护用户上传图像和场景的隐私变得至关重要，这关系到用户数据安全和隐私保护法规的遵守。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的优缺点：基于特征匹配的方法准确但存在隐私问题，SegLoc方法通过分割标签提高隐私但缺乏3D一致性，而NeRF-based方法虽有3D一致性但不提供隐私保护。作者选择3D高斯泼溅(3DGS)作为基础，因其高效渲染特性，结合显式几何模型和隐式特征场，提出高斯泼溅特征场(GSFFs)。借鉴了SegLoc的隐私保护思路和NeRF的多视角一致性思想，但通过显式表示提高了隐私保护能力。设计过程中，作者利用3DGS的密集几何信息和可微分光栅化算法，通过对比框架对齐3D和2D特征，并使用聚类将特征转换为分割标签实现隐私保护。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合3D高斯泼溅与特征场，创建一种新型场景表示方法，通过自监督学习特征表示，使2D图像特征与从3D渲染的特征能够对齐，并利用聚类将特征转换为分割标签实现隐私保护。整体流程包括：1)场景表示：采用高斯不透明场模型，使用三平面网格参数化特征场；2)自监督训练：通过对比损失对齐渲染的3D特征和提取的2D特征，使用原型对比损失改进特征区分度；3)特征定位：从查询图像提取特征，检索初始姿态，通过最小化特征误差细化姿态；4)隐私保护定位：将特征转换为分割标签，移除敏感信息，通过最小化分割图差异细化姿态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)高斯泼溅特征场(GSFFs)：结合显式几何模型和隐式特征场；2)自监督特征学习：通过对比框架和多视角一致性正则化学习特征；3)原型特征正则化：利用3D高斯云空间结构聚类构建特征原型；4)隐私保护转换：将高维特征转换为低维分割标签。相比之前工作的不同：与SegLoc相比，GSFFs提供3D一致的分割标签和更高精度；与基于NeRF的方法相比，GSFFs使用显式表示更适合隐私保护且渲染更快；与其他基于3DGS的方法相比，GSFFs自监督学习特征且直接通过光栅器反向传播姿态误差。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了高斯泼溅特征场(GSFFs)，一种结合3D高斯泼溅与特征场的新型场景表示方法，实现了准确且隐私保护的视觉定位，在多个真实世界数据集上取得了最先进的结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual localization is the task of estimating a camera pose in a knownenvironment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-basedrepresentations for accurate and privacy-preserving visual localization. Wepropose Gaussian Splatting Feature Fields (GSFFs), a scene representation forvisual localization that combines an explicit geometry model (3DGS) with animplicit feature field. We leverage the dense geometric information anddifferentiable rasterization algorithm from 3DGS to learn robust featurerepresentations grounded in 3D. In particular, we align a 3D scale-awarefeature field and a 2D feature encoder in a common embedding space through acontrastive framework. Using a 3D structure-informed clustering procedure, wefurther regularize the representation learning and seamlessly convert thefeatures to segmentations, which can be used for privacy-preserving visuallocalization. Pose refinement, which involves aligning either feature maps orsegmentations from a query image with those rendered from the GSFFs scenerepresentation, is used to achieve localization. The resulting privacy- andnon-privacy-preserving localization pipelines, evaluated on multiple real-worlddatasets, show state-of-the-art performances.</description>
      <author>example@mail.com (Maxime Pietrantoni, Gabriela Csurka, Torsten Sattler)</author>
      <guid isPermaLink="false">2507.23569v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Your Spending Needs Attention: Modeling Financial Habits with Transformers</title>
      <link>http://arxiv.org/abs/2507.23267v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为nuFormer的新方法，将基于Transformer的自监督学习应用于金融交易数据，通过结合文本和结构化属性，并使用端到端微调，在Nubank的推荐系统中实现了性能提升。&lt;h4&gt;背景&lt;/h4&gt;预测模型在金融行业中至关重要，用于风险预测、欺诈检测和个性化推荐。金融机构拥有大量用户数据，但有效利用这些数据具有挑战性。目前大多数生产模型采用传统机器学习方法，而其他领域已有效利用自监督学习从原始数据中学习丰富表示。&lt;h4&gt;目的&lt;/h4&gt;研究基于Transformer的表示学习模型处理交易数据，探索这些在大规模数据上训练的模型是否可以提供理解和理解客户行为的新方法。&lt;h4&gt;方法&lt;/h4&gt;提出一种新方法使自监督学习能够应用于交易数据，通过调整基于Transformer的模型来处理文本和结构化属性。提出了名为nuFormer的方法，包括一种将用户嵌入与现有表格特征集成的端到端微调方法。&lt;h4&gt;主要发现&lt;/h4&gt;在Nubank的大规模推荐问题上显示出改进，这些改进仅通过增强表示学习实现，而非整合新数据源。&lt;h4&gt;结论&lt;/h4&gt;基于Transformer的表示学习模型可以有效地应用于金融交易数据，替代传统的手动特征工程，提供更强大的客户行为理解。&lt;h4&gt;翻译&lt;/h4&gt;预测模型在金融行业中起着关键作用，能够进行风险预测、欺诈检测和个性化推荐，其中核心模型性能的微小变化可能导致数十亿美元的收益或损失。虽然金融机构可以访问大量用户数据（例如银行交易、应用内事件和客户支持日志），但由于其复杂性和规模，有效利用这些数据仍然具有挑战性。因此，在许多金融机构中，大多数生产模型采用传统机器学习方法，将非结构化数据转换为手动设计的表格特征。相反，其他领域（如自然语言处理）已有效利用自监督学习从原始数据中学习丰富表示，无需手动特征提取。在本文中，我们研究使用基于Transformer的表示学习模型处理交易数据，假设这些在大规模数据上训练的模型可以提供理解和理解客户行为的新方法。我们提出了一种新方法，通过调整基于Transformer的模型来处理文本和结构化属性，使自监督学习能够应用于交易数据。我们的方法称为nuFormer，包括一种将用户嵌入与现有表格特征集成的端到端微调方法。我们的实验证明了在Nubank的大规模推荐问题上有所改进。值得注意的是，这些改进仅通过增强表示学习实现，而非整合新数据源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predictive models play a crucial role in the financial industry, enablingrisk prediction, fraud detection, and personalized recommendations, whereslight changes in core model performance can result in billions of dollars inrevenue or losses. While financial institutions have access to enormous amountsof user data (e.g., bank transactions, in-app events, and customer supportlogs), leveraging this data effectively remains challenging due to itscomplexity and scale. Thus, in many financial institutions, most productionmodels follow traditional machine learning (ML) approaches by convertingunstructured data into manually engineered tabular features. Conversely, otherdomains (e.g., natural language processing) have effectively utilizedself-supervised learning (SSL) to learn rich representations from raw data,removing the need for manual feature extraction. In this paper, we investigateusing transformer-based representation learning models for transaction data,hypothesizing that these models, trained on massive data, can provide a noveland powerful approach to understanding customer behavior. We propose a newmethod enabling the use of SSL with transaction data by adaptingtransformer-based models to handle both textual and structured attributes. Ourapproach, denoted nuFormer, includes an end-to-end fine-tuning method thatintegrates user embeddings with existing tabular features. Our experimentsdemonstrate improvements for large-scale recommendation problems at Nubank.Notably, these gains are achieved solely through enhanced representationlearning rather than incorporating new data sources.</description>
      <author>example@mail.com (D. T. Braithwaite, Misael Cavalcanti, R. Austin McEver, Hiroto Udagawa, Daniel Silva, Rohan Ramanath, Felipe Meneses, Arissa Yoshida, Evan Wingert, Matheus Ramos, Brian Zanfelice, Aman Gupta)</author>
      <guid isPermaLink="false">2507.23267v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Balancing Information Preservation and Disentanglement in Self-Supervised Music Representation Learning</title>
      <link>http://arxiv.org/abs/2507.22995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In proceedings of WASPAA 2025. 4 pages, 4 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了一种统一的多视图自监督学习框架，结合了对比和重建目标，用于解耦音乐音频表示。通过广泛评估发现，这两种方法虽然存在权衡，但可以相互补充，最终能够在不损害信息完整性的情况下解耦音乐属性。&lt;h4&gt;背景&lt;/h4&gt;自监督学习(SSL)方法在音乐音频领域取得了进展，能够无需标记数据就能捕获有用的音乐表示。现有方法分为两类：一类通过重建来保留全面细节，另一类通过对比目标来保留语义结构。很少有研究在一个统一的SSL框架中检查这些范式之间的相互作用。&lt;h4&gt;目的&lt;/h4&gt;提出一个多视图SSL框架，用于解耦音乐音频表示，结合对比和重建目标，促进解耦子空间中信息保真度和结构语义。&lt;h4&gt;方法&lt;/h4&gt;设计了一个结合对比和重建目标的架构，在受控环境中对对比策略的设计选择进行了广泛评估，使用音乐音频表示进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;重建和对比策略表现出一致性的权衡，当有效结合时，它们可以相互补充，能够在不损害信息完整性的情况下解耦音乐属性。&lt;h4&gt;结论&lt;/h4&gt;多视图SSL框架能够有效结合对比和重建方法，这种结合可以解耦音乐属性同时保持信息完整性。&lt;h4&gt;翻译&lt;/h4&gt;最近的自监督学习(SSL)方法进展提供了多种策略，可以从音乐音频中捕获有用的表示，而无需标记数据。虽然一些技术通过重建来保留全面细节，但其他技术则通过对比目标来保留语义结构。很少有研究在统一的SSL框架中检查这些范式之间的相互作用。在这项工作中，我们提出了一个多视图SSL框架，用于解耦音乐音频表示，该框架结合了对比和重建目标。该架构旨在促进解耦子空间中因素的信息保真度和结构语义。我们在受控环境中使用音乐音频表示对对比策略的设计选择进行了广泛评估。我们发现，虽然重建和对比策略表现出一致性的权衡，但当有效结合时，它们可以相互补充；这能够在不损害信息完整性的情况下解耦音乐属性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in self-supervised learning (SSL) methods offer a range ofstrategies for capturing useful representations from music audio without theneed for labeled data. While some techniques focus on preserving comprehensivedetails through reconstruction, others favor semantic structure via contrastiveobjectives. Few works examine the interaction between these paradigms in aunified SSL framework. In this work, we propose a multi-view SSL framework fordisentangling music audio representations that combines contrastive andreconstructive objectives. The architecture is designed to promote bothinformation fidelity and structured semantics of factors in disentangledsubspaces. We perform an extensive evaluation on the design choices ofcontrastive strategies using music audio representations in a controlledsetting. We find that while reconstruction and contrastive strategies exhibitconsistent trade-offs, when combined effectively, they complement each other;this enables the disentanglement of music attributes without compromisinginformation integrity.</description>
      <author>example@mail.com (Julia Wilkins, Sivan Ding, Magdalena Fuentes, Juan Pablo Bello)</author>
      <guid isPermaLink="false">2507.22995v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2507.23772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SeqSplatNet框架，用于解决3D功能区域推理中的长期、多物体任务挑战，通过引入序列3D高斯功能区域推理任务和SeqAffordSplat基准测试，成功将功能区域推理从单步交互提升到场景级别的复杂序列任务。&lt;h4&gt;背景&lt;/h4&gt;3D功能区域推理是具身智能体的关键能力，但当前基于3D Gaussian Splatting的方法仅限于单物体、单步交互，无法满足复杂现实应用所需的长期、多物体任务需求。&lt;h4&gt;目的&lt;/h4&gt;弥补现有方法的不足，引入序列3D高斯功能区域推理新任务，并建立SeqAffordSplat基准测试，支持复杂3DGS环境中的长期功能区域理解研究。&lt;h4&gt;方法&lt;/h4&gt;提出SeqSplatNet端到端框架，使用大型语言模型自回归生成交织文本与分割标记，引入条件几何重建预训练策略建立几何先验，设计特征注入机制融合2D视觉基础模型的语义特征到3D解码器。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验证明，该方法在具有挑战性的基准测试上达到新的最先进水平，有效推动功能区域推理从单步交互发展到场景级别的复杂序列任务。&lt;h4&gt;结论&lt;/h4&gt;该方法成功解决了3D功能区域推理中的长期、多物体任务挑战，为具身智能体在复杂环境中的交互提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;3D功能区域推理是将人类指令与3D物体功能区域关联的任务，是具身智能体的关键能力。当前基于3D高斯溅射的方法根本局限于单物体、单步交互，这种范式无法解决复杂现实应用所需的长期、多物体任务。为弥合这一差距，我们引入了序列3D高斯功能区域推理这一新颖任务，并建立了SeqAffordSplat基准测试，包含1800多个场景，支持复杂3DGS环境中的长期功能区域理解研究。随后我们提出了SeqSplatNet，一个端到端框架，直接将指令映射为3D功能区域掩码序列。SeqSplatNet采用大型语言模型自回归生成与特殊分割标记交织的文本，引导条件解码器生成相应的3D掩码。为处理复杂场景几何，我们引入了条件几何重建预训练策略，让模型从已知的几何观测中重建完整的功能区域掩码，从而建立强大的几何先验。此外，为解决语义歧义，我们设计了一个特征注入机制，从2D视觉基础模型中提取丰富的语义特征，并将它们多尺度融合到3D解码器中。大量实验证明，我们的方法在具有挑战性的基准测试上设定了新的最先进水平，有效将功能区域推理从单步交互推进到场景级别的复杂任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前基于3D高斯溅射(3DGS)的功能推理方法只能处理单对象、单步交互的局限性，无法满足现实世界中长视距、多对象的复杂任务需求。这个问题在现实中很重要，因为3D功能推理是机器人操作、增强现实和虚拟现实等应用中具身智能体的核心能力，能够将人类指令与3D对象的功能区域相关联，使智能体能够理解并执行复杂的多步骤任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：点云方法存在稀疏性问题，而3DGS方法仅限于单步交互。他们借鉴了3DGS的高保真表示能力、大型语言模型的层次规划能力以及2D视觉基础模型的语义特征提取能力。设计上，他们创建了SeqSplatNet框架，整合了3DGS编码器、大型语言模型和条件功能解码器，并设计了条件几何重建预训练策略和语义特征注入机制来解决复杂场景中的几何和语义挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将大型语言模型的序列规划能力与3DGS的高保真表示能力统一到端到端架构中，直接将语言指令映射为一系列有序的3D功能掩码。整体流程是：1)接收复杂指令和3DGS场景；2)3DGS编码器提取几何信息，LLM处理指令并自回归生成包含特殊&lt;SEG&gt;标记的序列；3)每个&lt;SEG&gt;标记触发条件功能解码器，结合几何特征和注入的语义特征生成对应的3D功能掩码；4)输出完成复杂指令所需的有序功能掩码序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出场景级顺序3D高斯功能推理新任务；2)创建SeqAffordSplat大规模基准数据集(1800+场景、14000+掩码、8000+指令)；3)设计SeqSplatNet统一框架；4)提出条件几何重建预训练策略；5)实现语义特征注入机制。相比之前工作，不同之处在于：现有点云方法无法提供精细定位，而现有3DGS方法仅限于单步交互，本文首次同时实现了精细功能定位和长视距顺序推理能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SeqAffordSplat通过引入场景级顺序3D高斯功能推理任务和SeqSplatNet框架，首次将长视距序列推理与高保真3D表示相结合，显著提升了具身智能体在复杂环境中理解和执行多步骤指令的能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D affordance reasoning, the task of associating human instructions with thefunctional regions of 3D objects, is a critical capability for embodied agents.Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limitedto single-object, single-step interactions, a paradigm that falls short ofaddressing the long-horizon, multi-object tasks required for complex real-worldapplications. To bridge this gap, we introduce the novel task of Sequential 3DGaussian Affordance Reasoning and establish SeqAffordSplat, a large-scalebenchmark featuring 1800+ scenes to support research on long-horizon affordanceunderstanding in complex 3DGS environments. We then propose SeqSplatNet, anend-to-end framework that directly maps an instruction to a sequence of 3Daffordance masks. SeqSplatNet employs a large language model thatautoregressively generates text interleaved with special segmentation tokens,guiding a conditional decoder to produce the corresponding 3D mask. To handlecomplex scene geometry, we introduce a pre-training strategy, ConditionalGeometric Reconstruction, where the model learns to reconstruct completeaffordance region masks from known geometric observations, thereby building arobust geometric prior. Furthermore, to resolve semantic ambiguities, we designa feature injection mechanism that lifts rich semantic features from 2D VisionFoundation Models (VFM) and fuses them into the 3D decoder at multiple scales.Extensive experiments demonstrate that our method sets a new state-of-the-arton our challenging benchmark, effectively advancing affordance reasoning fromsingle-step interactions to complex, sequential tasks at the scene level.</description>
      <author>example@mail.com (Di Li, Jie Feng, Jiahao Chen, Weisheng Dong, Guanbin Li, Yuhui Zheng, Mingtao Feng, Guangming Shi)</author>
      <guid isPermaLink="false">2507.23772v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>SAMSA: Segment Anything Model Enhanced with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2507.23673v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SAMSA是一种结合RGB基础模型和光谱分析的交互式分割框架，有效解决了高光谱医学成像中的数据限制和硬件差异挑战。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像为医学成像提供丰富光谱信息，但面临数据限制和硬件差异带来的重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的交互式分割框架，结合RGB基础模型与光谱分析，提高高光谱医学图像分割效果。&lt;h4&gt;方法&lt;/h4&gt;SAMSA利用用户点击指导RGB分割和光谱相似度计算，采用独特的光谱特征融合策略，独立于光谱波段数量和分辨率运作。&lt;h4&gt;主要发现&lt;/h4&gt;在神经外科高光谱数据集上达到八十一点零的1点击和九十三点四的5点击DICE分数，在术中猪高光谱数据集上达到八十一点一的1点击和八十九点二的5点击DICE分数。&lt;h4&gt;结论&lt;/h4&gt;SAMSA在小样本和零样本学习场景中表现有效，能够无缝集成不同光谱特性的数据集，为高光谱医学图像分析提供灵活框架。&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像为医学成像提供丰富的光谱信息，但由于数据限制和硬件差异而遇到重大挑战。我们引入了SAMSA，一种结合RGB基础模型与光谱分析的新型交互式分割框架。SAMSA有效利用用户点击来指导RGB分割和光谱相似度计算。该方法通过一种独特的光谱特征融合策略解决了HSI分割的关键限制，该策略独立于光谱波段数量和分辨率。在公开可用数据集上的性能评估显示，在神经外科和术中猪高光谱数据集上分别达到八十一点零的1点击和九十三点四的5点击DICE，以及八十一点一的1点击和八十九点二的5点击DICE。实验结果表明SAMSA在小样本和零样本学习场景以及使用最少训练示例时的有效性。我们的方法能够无缝集成具有不同光谱特性的数据集，为高光谱医学图像分析提供了灵活的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) provides rich spectral information for medicalimaging, yet encounters significant challenges due to data limitations andhardware variations. We introduce SAMSA, a novel interactive segmentationframework that combines an RGB foundation model with spectral analysis. SAMSAefficiently utilizes user clicks to guide both RGB segmentation and spectralsimilarity computations. The method addresses key limitations in HSIsegmentation through a unique spectral feature fusion strategy that operatesindependently of spectral band count and resolution. Performance evaluation onpublicly available datasets has shown 81.0% 1-click and 93.4% 5-click DICE on aneurosurgical and 81.1% 1-click and 89.2% 5-click DICE on an intraoperativeporcine hyperspectral dataset. Experimental results demonstrate SAMSA'seffectiveness in few-shot and zero-shot learning scenarios and using minimaltraining examples. Our approach enables seamless integration of datasets withdifferent spectral characteristics, providing a flexible framework forhyperspectral medical image analysis.</description>
      <author>example@mail.com (Alfie Roddan, Tobias Czempiel, Chi Xu, Daniel S. Elson, Stamatia Giannarou)</author>
      <guid isPermaLink="false">2507.23673v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2507.23523v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了H-RDT（Human to Robotics Diffusion Transformer），一种利用大规模人类操作视频数据增强机器人操作能力的新方法，通过两阶段训练范式和扩散Transformer架构，显著提升了机器人操作性能。&lt;h4&gt;背景&lt;/h4&gt;机器人模仿学习面临大规模高质量演示数据稀缺的挑战，而现有的机器人基础模型在跨形态机器人数据集上预训练时，由于不同机器人形态和动作空间的多样性，统一训练面临重大限制。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用人类操作数据增强机器人操作能力的新方法，解决机器人操作数据稀缺和跨形态训练困难的问题。&lt;h4&gt;方法&lt;/h4&gt;提出H-RDT方法，基于20亿参数的扩散Transformer架构，使用流匹配建模复杂动作分布；采用两阶段训练范式：首先在大规模以自我为中心的人类操作数据上进行预训练，然后在机器人特定数据上进行跨形态微调，使用模块化动作编码器和解码器。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界实验、单任务和多任务场景、少样本学习和鲁棒性评估中，H-RDT优于从头训练和现有最先进方法；与从头训练相比，在模拟和真实世界实验中分别实现了13.9%和40.5%的显著改进。&lt;h4&gt;结论&lt;/h4&gt;人类操作数据可以作为学习双臂机器人操作策略的强大基础，验证了利用人类行为先验增强机器人操作能力的核心假设。&lt;h4&gt;翻译&lt;/h4&gt;机器人模仿学习面临一个基本挑战：大规模、高质量的机器人演示数据稀缺。最近的机器人基础模型通常在跨形态机器人数据集上进行预训练以增加数据规模，但由于不同机器人形态和动作空间的多样性，统一训练面临重大限制。在本文中，我们提出了H-RDT（Human to Robotics Diffusion Transformer），一种利用人类操作数据增强机器人操作能力的新方法。我们的核心见解是，大规模带有配对3D手部姿态注释的以自我为中心的人类操作视频提供了丰富的行为先验，可以捕捉自然操作策略并有益于机器人策略学习。我们引入了两阶段训练范式：（1）在大规模以自我为中心的人类操作数据上进行预训练，（2）在机器人特定数据上进行跨形态微调，使用模块化动作编码器和解码器。基于具有20亿参数的扩散Transformer架构构建，H-RDT使用流匹配来建模复杂的动作分布。在模拟和真实世界实验、单任务和多任务场景、少样本学习和鲁棒性评估的广泛评估中，H-RDT优于从头训练和现有的最先进方法，包括Pi0和RDT，在模拟和真实世界实验中分别实现了比从头训练高13.9%和40.5%的显著改进。结果验证了我们的核心假设：人类操作数据可以作为学习双臂机器人操作策略的强大基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation learning for robotic manipulation faces a fundamental challenge:the scarcity of large-scale, high-quality robot demonstration data. Recentrobotic foundation models often pre-train on cross-embodiment robot datasets toincrease data scale, while they face significant limitations as the diversemorphologies and action spaces across different robot embodiments make unifiedtraining challenging. In this paper, we present H-RDT (Human to RoboticsDiffusion Transformer), a novel approach that leverages human manipulation datato enhance robot manipulation capabilities. Our key insight is that large-scaleegocentric human manipulation videos with paired 3D hand pose annotationsprovide rich behavioral priors that capture natural manipulation strategies andcan benefit robotic policy learning. We introduce a two-stage trainingparadigm: (1) pre-training on large-scale egocentric human manipulation data,and (2) cross-embodiment fine-tuning on robot-specific data with modular actionencoders and decoders. Built on a diffusion transformer architecture with 2Bparameters, H-RDT uses flow matching to model complex action distributions.Extensive evaluations encompassing both simulation and real-world experiments,single-task and multitask scenarios, as well as few-shot learning androbustness assessments, demonstrate that H-RDT outperforms training fromscratch and existing state-of-the-art methods, including Pi0 and RDT, achievingsignificant improvements of 13.9% and 40.5% over training from scratch insimulation and real-world experiments, respectively. The results validate ourcore hypothesis that human manipulation data can serve as a powerful foundationfor learning bimanual robotic manipulation policies.</description>
      <author>example@mail.com (Hongzhe Bi, Lingxuan Wu, Tianwei Lin, Hengkai Tan, Zhizhong Su, Hang Su, Jun Zhu)</author>
      <guid isPermaLink="false">2507.23523v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Towards Affordable Tumor Segmentation and Visualization for 3D Breast MRI Using SAM2</title>
      <link>http://arxiv.org/abs/2507.23272v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in the 28th International Conference on  Medical Image Computing and Computer Assisted Intervention (MICCAI), 2nd Deep  Breast Workshop on AI and Imaging for Diagnostic and Treatment Challenges in  Breast Care (DeepBreath), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了如何将Segment Anything Model 2 (SAM2)模型适应用于低成本、低输入的3D乳腺MRI肿瘤分割，发现使用单一切片边界框注释结合从中心向外传播策略能提供最佳分割效果，为资源受限环境提供了经济实惠的医学图像分析解决方案。&lt;h4&gt;背景&lt;/h4&gt;乳腺MRI提供高分辨率体积成像对肿瘤评估和治疗规划至关重要，但3D扫描的手动解释劳动密集且主观。商业医疗AI产品在低收入和中等收入国家采用有限，主要受制于高昂的许可费用、专有软件和基础设施需求。&lt;h4&gt;目的&lt;/h4&gt;调查Segment Anything Model 2 (SAM2)是否可以适应用于低成本、低输入的3D乳腺MRI肿瘤分割。&lt;h4&gt;方法&lt;/h4&gt;使用单一切片上的单个边界框注释，通过三种不同的切片跟踪策略（从上到下、从下到上、从中心向外）将分割预测传播到整个3D体积，并在大量患者队列中评估这些策略。&lt;h4&gt;主要发现&lt;/h4&gt;1. 从中心向外传播策略提供了最一致和准确的分割结果；2. 尽管SAM2是为通用目的设计的零样本模型，没有针对体积医学数据进行训练，但在最少监督下仍能实现强大的分割性能；3. 研究分析了分割性能与肿瘤大小、位置和形状的关系，确定了关键的失败模式。&lt;h4&gt;结论&lt;/h4&gt;通用基础模型如SAM2可以在最少监督的情况下支持3D医学图像分析，为资源受限环境提供了一种可访问且经济实惠的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;乳腺MRI提供高分辨率体积成像，对肿瘤评估和治疗规划至关重要，但3D扫描的手动解释仍然劳动密集且主观。虽然人工智能工具有望加速医学图像分析，但由于高昂的许可费用、专有软件和基础设施需求，商业医疗AI产品在低收入和中等收入国家的采用仍然有限。在这项工作中，我们研究了Segment Anything Model 2 (SAM2)是否可以适应用于低成本、低输入的3D乳腺MRI肿瘤分割。使用一个切片上的单个边界框注释，我们通过三种不同的切片跟踪策略将分割预测传播到整个3D体积：从上到下、从下到上和从中心向外。我们在大量患者队列中评估了这些策略，发现从中心向外传播提供了最一致和准确的分割结果。尽管SAM2是一个没有针对体积医学数据进行训练的零样本模型，但在最少监督下仍能实现强大的分割性能。我们进一步分析了分割性能与肿瘤大小、位置和形状的关系，确定了关键的失败模式。我们的结果表明，像SAM2这样的通用基础模型可以在最少监督的情况下支持3D医学图像分析，为资源受限环境提供了一种可访问且经济实惠的替代方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何开发低成本、低资源需求的3D乳腺MRI肿瘤分割和可视化方法。这个问题很重要，因为乳腺癌是全球最常见的癌症之一，早期准确检测对改善患者预后至关重要。然而，3D乳腺MRI包含数百张切片，人工解读耗时且主观，而现有AI工具因成本高、需要专业基础设施，在资源有限的医疗环境中难以普及。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到现有AI医疗工具在资源有限地区的应用障碍，以及传统3D肿瘤分割方法需要大量标注数据的局限性。他们借鉴了视频对象跟踪技术，将SAM2这一开源基础模型应用于医学图像领域。SAM2原本是为图像和视频分割设计的，作者创新性地将其用于3D医学数据的切片级处理，并设计了三种不同的传播策略来遍历3D体积。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D乳腺MRI视为2D切片序列，利用SAM2模型的跟踪功能，通过最小人工输入（单个边界框）实现肿瘤分割。流程包括：1)从3D MRI中提取2D切片；2)在一个切片上提供边界框作为初始输入；3)使用SAM2的跟踪功能在相邻切片间传播分割；4)评估三种传播策略（从下到上、从上到下、从中心向外）；5)使用体积Dice系数评估分割效果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)利用SAM2这一零样本基础模型实现最小监督的3D肿瘤分割；2)发现并验证了'从中心向外'的传播策略最有效；3)完全基于开源工具，无需专业基础设施；4)只需单一切片的边界框输入，大幅减少人工标注；5)分析失败模式，提供对模型局限性的深入理解。相比传统方法，这种方法不需要大量标注数据和密集训练，更适合资源有限的医疗环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文成功将SAM2开源模型应用于3D乳腺MRI肿瘤分割，通过最小人工输入实现了准确分割，为资源有限的医疗环境提供了一种可负担且可访问的AI解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Breast MRI provides high-resolution volumetric imaging critical for tumorassessment and treatment planning, yet manual interpretation of 3D scansremains labor-intensive and subjective. While AI-powered tools hold promise foraccelerating medical image analysis, adoption of commercial medical AI productsremains limited in low- and middle-income countries due to high license costs,proprietary software, and infrastructure demands. In this work, we investigatewhether the Segment Anything Model 2 (SAM2) can be adapted for low-cost,minimal-input 3D tumor segmentation in breast MRI. Using a single bounding boxannotation on one slice, we propagate segmentation predictions across the 3Dvolume using three different slice-wise tracking strategies: top-to-bottom,bottom-to-top, and center-outward. We evaluate these strategies across a largecohort of patients and find that center-outward propagation yields the mostconsistent and accurate segmentations. Despite being a zero-shot model nottrained for volumetric medical data, SAM2 achieves strong segmentationperformance under minimal supervision. We further analyze how segmentationperformance relates to tumor size, location, and shape, identifying key failuremodes. Our results suggest that general-purpose foundation models such as SAM2can support 3D medical image analysis with minimal supervision, offering anaccessible and affordable alternative for resource-constrained settings.</description>
      <author>example@mail.com (Solha Kang, Eugene Kim, Joris Vankerschaver, Utku Ozbulak)</author>
      <guid isPermaLink="false">2507.23272v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs</title>
      <link>http://arxiv.org/abs/2507.23227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为TAP-GPT的新型框架，利用大型语言模型进行阿尔茨海默病的早期准确诊断。该框架通过整合多种生物标志物数据，在小样本情况下实现了优于现有方法的诊断性能。&lt;h4&gt;背景&lt;/h4&gt;阿尔茨海默病是一种复杂的神经退行性疾病，其早期准确诊断需要分析包括神经影像、遗传风险因素、认知测试和脑脊液蛋白在内的多种异质性生物标志物，这些数据通常以表格形式呈现。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用结构化生物标志数据进行阿尔茨海默病诊断的框架，解决小样本量下的诊断挑战。&lt;h4&gt;方法&lt;/h4&gt;提出TAP-GPT框架，将TableGPT2(一种商业智能任务开发的多模态表格专用LLM)适应用于AD诊断。通过构建少量样本表格提示和使用qLoRA参数高效适配技术对TableGPT2进行微调，实现AD与认知正常的临床二分类任务。&lt;h4&gt;主要发现&lt;/h4&gt;TAP-GPT框架利用TableGPT2的表格理解能力和LLMs的先验知识，在预测任务上表现优于先进的通用LLMs和专为预测任务开发的表格基础模型(TFM)。这是首次将LLMs应用于表格生物标志数据预测任务。&lt;h4&gt;结论&lt;/h4&gt;该研究为未来生物信息学中的LLM驱动多智能体框架铺平了道路，展示了大型语言模型在医疗诊断特别是神经退行性疾病诊断中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;阿尔茨海默病(AD)是一种复杂的神经退行性疾病，其早期准确诊断需要分析异质性生物标志物(如神经影像、遗传风险因素、认知测试和脑脊液蛋白)，这些数据通常以表格形式呈现。凭借灵活的少样本推理、多模态集成和基于自然语言的可解释性，大型语言模型(LLMs)为结构化生物医学数据的预测提供了前所未有的机会。我们提出了一种名为TAP-GPT(表格阿尔茨海默病预测GPT)的新框架，该框架将TableGPT2(一种最初为商业智能任务开发的多模态表格专用LLM)适应用于使用小样本量的结构化生物标志数据进行AD诊断。我们的方法使用结构化生物医学数据中的上下文学习示例构建少量样本表格提示，并使用参数高效的qLoRA适配对TableGPT2进行微调，用于AD或认知正常(CN)的临床二分类任务。TAP-GPT框架利用TableGPT2的强大表格理解能力和LLMs的编码先验知识，在性能上超越了更先进的通用LLMs和专为预测任务开发的表格基础模型(TFM)。据我们所知，这是首次将LLMs应用于使用表格生物标志数据的预测任务，为未来生物信息学中的LLM驱动多智能体框架铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early and accurate diagnosis of Alzheimer's disease (AD), a complexneurodegenerative disorder, requires analysis of heterogeneous biomarkers(e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinalfluid proteins) typically represented in a tabular format. With flexiblefew-shot reasoning, multimodal integration, and natural-language-basedinterpretability, large language models (LLMs) offer unprecedentedopportunities for prediction with structured biomedical data. We propose anovel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adaptsTableGPT2, a multimodal tabular-specialized LLM originally developed forbusiness intelligence tasks, for AD diagnosis using structured biomarker datawith small sample sizes. Our approach constructs few-shot tabular prompts usingin-context learning examples from structured biomedical data and finetunesTableGPT2 using the parameter-efficient qLoRA adaption for a clinical binaryclassification task of AD or cognitively normal (CN). The TAP-GPT frameworkharnesses the powerful tabular understanding ability of TableGPT2 and theencoded prior knowledge of LLMs to outperform more advanced general-purposeLLMs and a tabular foundation model (TFM) developed for prediction tasks. Toour knowledge, this is the first application of LLMs to the prediction taskusing tabular biomarker data, paving the way for future LLM-driven multi-agentframeworks in biomedical informatics.</description>
      <author>example@mail.com (Sophie Kearney, Shu Yang, Zixuan Wen, Bojian Hou, Duy Duong-Tran, Tianlong Chen, Jason Moore, Marylyn Ritchie, Li Shen)</author>
      <guid isPermaLink="false">2507.23227v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models for Clean Energy Forecasting: A Comprehensive Review</title>
      <link>http://arxiv.org/abs/2507.23147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is currently under review at the journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了基础模型(FMs)在可再生能源预测领域的应用，特别是风能和太阳能预测。论文涵盖了模型架构、预训练策略、微调方法和数据类型，强调了大规模训练模型和领域特定Transformer架构的重要性，并评估了预测准确性方面的最新进展和存在的挑战。&lt;h4&gt;背景&lt;/h4&gt;随着全球能源系统向清洁能源转型，准确的可再生能源发电量和需求预测对有效的电网管理至关重要。基础模型能够快速处理复杂的高维时间序列数据，从而提高预测准确性。&lt;h4&gt;目的&lt;/h4&gt;这篇综述论文旨在概述基础模型在可再生能源预测领域的应用，评估其在预测准确性方面的最新进展，讨论现有挑战和改进领域，并提出未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;论文通过综述现有研究，分析了基础模型的架构、预训练策略、微调方法和数据类型，特别关注了大规模训练模型、领域特定Transformer架构、时空相关性、领域知识嵌入以及可再生能源发电的短暂性和间歇性。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在可再生能源预测中表现出色，特别是在多时间尺度预测协调和不确定性量化方面。论文区分了理论和实践，批判性评估了FMs的优缺点，并确定了长期和多变量时间序列预测中的挑战和改进领域。&lt;h4&gt;结论&lt;/h4&gt;基础模型为可再生能源预测提供了新的可能性，但仍存在理论和实践上的差距。未来研究应关注解决这些差距，进一步提高预测准确性，特别是在长期和多变量时间序列预测方面。&lt;h4&gt;翻译&lt;/h4&gt;随着全球能源系统向清洁能源转型，准确的可再生能源发电和需求预测对有效的电网管理至关重要。基础模型(FMs)可以帮助提高可再生能源发电和需求的预测，因为FMs能够快速处理复杂的高维时间序列数据。这篇综述论文专注于可再生能源预测领域的基础模型，主要关注风能和太阳能。我们概述了可再生能源预测背景下的架构、预训练策略、微调方法和数据类型。我们强调了大规模训练模型、领域特定Transformer架构的作用，这些架构关注时空相关性、领域知识的嵌入，以及可再生能源发电的短暂性和间歇性。我们评估了基于FMs在预测准确性方面的最新进展，如协调多时间尺度的预测和量化可再生能源预测中的不确定性。我们还回顾了长期和多变量时间序列预测中现有的挑战和改进领域。在本综述中，我们建立了在清洁能源预测领域使用FMs时理论与实践之间的区别。此外，它批判性地评估了FMs的优缺点，同时推进了这一新兴而令人兴奋的预测领域的未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As global energy systems transit to clean energy, accurate renewablegeneration and renewable demand forecasting is imperative for effective gridmanagement. Foundation Models (FMs) can help improve forecasting of renewablegeneration and demand because FMs can rapidly process complex, high-dimensionaltime-series data. This review paper focuses on FMs in the realm of renewableenergy forecasting, primarily focusing on wind and solar. We present anoverview of the architectures, pretraining strategies, finetuning methods, andtypes of data used in the context of renewable energy forecasting. We emphasizethe role of models that are trained at a large scale, domain specificTransformer architectures, where attention is paid to spatial temporalcorrelations, the embedding of domain knowledge, and also the brief andintermittent nature of renewable generation. We assess recent FM basedadvancements in forecast accuracy such as reconciling predictions over multipletime scales and quantifying uncertainty in renewable energy forecasting. Wealso review existing challenges and areas of improvement in long-term andmultivariate time series forecasting. In this survey, a distinction betweentheory and practice is established regarding the use of FMs in the clean energyforecasting domain. Additionally, it critically assesses the strengths andweaknesses of FMs while advancing future research direction in this new andexciting area of forecasting.</description>
      <author>example@mail.com (Md Meftahul Ferdaus, Tanmoy Dam, Md Rasel Sarkar, Moslem Uddin, Sreenatha G. Anavatti)</author>
      <guid isPermaLink="false">2507.23147v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance</title>
      <link>http://arxiv.org/abs/2507.23088v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新型的感知代理(Perception Agent)，通过结合语音集成的提示工程大型语言模型、分割任何模型和任意点跟踪基础模型，实现了更自然的人机交互，用于实时术中外科手术辅助。该代理包含记忆库和两种分割未见元素的新机制，能够记忆新元素供未来手术使用，解决了当前AI驱动解决方案的僵化问题。&lt;h4&gt;背景&lt;/h4&gt;外科手术数据科学和机器人解决方案需要自然的人机界面来充分发挥其潜力，但当代AI驱动解决方案本质上仍然僵化，灵活性有限，限制了动态手术环境中的自然人机交互。这些解决方案依赖于大量特定任务预训练、固定物体类别和明确的手动提示。&lt;h4&gt;目的&lt;/h4&gt;引入一种新型感知代理，实现更自然的人机交互，用于实时术中外科手术辅助，克服当前AI驱动解决方案的僵化问题。&lt;h4&gt;方法&lt;/h4&gt;利用语音集成的提示工程大型语言模型、分割任何模型和任意点跟踪基础模型；包含记忆库和两种分割未见元素的新机制；能够通过直观交互分割手术场景中已知和未见的元素；具备记忆新元素供未来手术使用的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集上的定量分析表明，代理性能与劳动强度大得多的手动提示策略相当；在自定义数据集上的定性分析展示了代理在分割新元素（器械、移植物纱布）方面的灵活性。&lt;h4&gt;结论&lt;/h4&gt;通过提供自然的人机交互和克服僵化，感知代理可能使基于AI的实时辅助在动态手术环境中更接近现实，在外科手术中人机共生方面迈出了显著的一步。&lt;h4&gt;翻译&lt;/h4&gt;新兴的外科手术数据科学和机器人解决方案，特别是那些旨在现场提供帮助的解决方案，需要自然的人机界面来充分发挥其在提供自适应和直观帮助方面的潜力。当代的AI驱动解决方案本质上仍然僵化，灵活性有限，限制了动态手术环境中的自然人机交互。这些解决方案严重依赖于大量特定任务的预训练、固定的物体类别和明确的手动提示。本文介绍了一种新型的感知代理，它利用语音集成的提示工程大型语言模型、分割任何模型和任意点跟踪基础模型，以实现更自然的人机交互，用于实时术中外科手术辅助。通过包含记忆库和两种用于分割未见元素的新机制，感知代理能够通过直观的交互来分割手术场景中已知和未见的元素。通过具备记忆新元素以供未来手术使用的能力，这项工作在外科手术中人机共生方面迈出了显著的一步。通过对公共数据集的定量分析，我们展示了代理的性能与劳动强度大得多的手动提示策略相当。在定性方面，我们在自定义数据集中展示了代理在分割新元素（器械、移植物纱布）方面的灵活性。通过提供自然的人机交互和克服僵化，我们的感知代理可能使基于AI的实时辅助在动态手术环境中更接近现实。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决当前AI手术辅助系统的'刚性'问题。现有AI系统缺乏灵活性，依赖大量预训练、固定对象类别和手动提示，无法自然适应动态手术环境。这个问题很重要，因为随着手术越来越复杂，需要能自然集成并适应变化的智能辅助系统。灵活的AI系统可以增强决策制定和程序效率，提高手术精度，减少并发症风险，最终改善患者治疗效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到当前手术AI系统的局限性，特别是它们无法处理新颖手术元素和缺乏自然交互能力。他们设计了一个名为'Perception Agent'的系统，整合语音交互、记忆存储和新型分割机制。作者确实借鉴了现有基础模型如SAM2（分割模型）和CoTracker3（点跟踪模型），以及大语言模型，但将这些技术以创新方式组合，并添加了记忆存储库和新的分割方法，创造出更加灵活的系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个能通过自然语言和手势与外科医生交互的AI系统，实时分割和跟踪手术场景中的已知和新元素，并能'学习'新元素供未来使用。整体流程包括：1) 语音指令转换为文本并提取任务和元素；2) 对于已知元素，从记忆库中检索并注入SAM2进行分割；3) 对于新元素，采用两种方法：以对象为中心的方法通过分析运动模式识别新仪器，或基于参考的方法通过已知对象关系定位新元素；4) 将新元素存入记忆库供未来使用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 记忆存储库，能存储和检索手术元素的记忆；2) 语音集成的大语言模型，理解自然语言指令；3) 以对象为中心的分割方法，通过运动识别新仪器；4) 基于参考的分割方法，通过对象关系定位新元素；5) 人机共生能力，系统可不断学习适应。相比之前工作，不同之处在于：之前的系统只能处理预定义类别，需要手动标记，无法学习新元素；而这个系统能理解自然语言，分割已知和新元素，并能记忆新元素供未来使用，实现了更自然、更灵活的人机交互。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文引入了Perception Agent，一个创新的AI系统，通过整合语音交互、记忆存储和新型分割机制，实现了手术中自然、灵活的人机协作，使AI能够实时分割已知和新元素，并学习记忆新元素以供未来使用，朝着手术中人机共生的方向迈出了重要一步。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Emerging surgical data science and robotics solutions, especially thosedesigned to provide assistance in situ, require natural human-machineinterfaces to fully unlock their potential in providing adaptive and intuitiveaid. Contemporary AI-driven solutions remain inherently rigid, offering limitedflexibility and restricting natural human-machine interaction in dynamicsurgical environments. These solutions rely heavily on extensive task-specificpre-training, fixed object categories, and explicit manual-prompting. This workintroduces a novel Perception Agent that leverages speech-integratedprompt-engineered large language models (LLMs), segment anything model (SAM),and any-point tracking foundation models to enable a more natural human-machineinteraction in real-time intraoperative surgical assistance. Incorporating amemory repository and two novel mechanisms for segmenting unseen elements,Perception Agent offers the flexibility to segment both known and unseenelements in the surgical scene through intuitive interaction. Incorporating theability to memorize novel elements for use in future surgeries, this work takesa marked step towards human-machine symbiosis in surgical procedures. Throughquantitative analysis on a public dataset, we show that the performance of ouragent is on par with considerably more labor-intensive manual-promptingstrategies. Qualitatively, we show the flexibility of our agent in segmentingnovel elements (instruments, phantom grafts, and gauze) in a custom-curateddataset. By offering natural human-machine interaction and overcoming rigidity,our Perception Agent potentially brings AI-based real-time assistance indynamic surgical environments closer to reality.</description>
      <author>example@mail.com (Lalithkumar Seenivasan, Jiru Xu, Roger D. Soberanis Mukul, Hao Ding, Grayson Byrd, Yu-Chun Ku, Jose L. Porras, Masaru Ishii, Mathias Unberath)</author>
      <guid isPermaLink="false">2507.23088v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>A Foundation Model for Material Fracture Prediction</title>
      <link>http://arxiv.org/abs/2507.23077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种数据驱动的断裂预测基础模型，该模型基于Transformer架构，能够跨模拟器、多种材料和不同加载条件进行断裂预测，显著减少了对数据量的需求。&lt;h4&gt;背景&lt;/h4&gt;准确预测材料失效对设计安全可靠的结构至关重要，但断裂行为在多样化条件下难以建模。现有方法中，机器学习模型缺乏泛化能力，而基于物理的模拟器则分散且计算资源密集。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的断裂预测模型，解决现有方法的局限性，实现跨材料、跨模拟器的断裂预测，并减少对大量训练数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;构建基于Transformer的架构，支持结构化和非结构化网格，结合大型语言模型嵌入的文本输入（材料属性、边界条件和求解器设置），实现多模态输入设计，使模型能够灵活适应各种模拟场景。&lt;h4&gt;主要发现&lt;/h4&gt;该模型可用最少数据在多种下游任务上微调，包括失效时间估计和断裂演化建模；能推广至未见材料如钛和混凝土，仅需单个样本；相比标准机器学习方法大幅减少数据需求。&lt;h4&gt;结论&lt;/h4&gt;断裂预测可在单一模型架构下统一，为特定模拟器工作流程提供了可扩展、可扩展的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;准确预测材料何时以及如何失效对于设计在应力下运行的安全、可靠结构、机械系统和工程部件至关重要。然而，断裂行为在现实应用的多样化材料、几何形状和加载条件下仍然难以建模。虽然机器学习方法显示出前景，但大多数模型在狭窄数据集上训练，缺乏鲁棒性，并且难以泛化。同时，基于物理的模拟器提供高保真预测，但分散在各种专门方法中，需要大量高性能计算资源来探索输入空间。为了解决这些限制，我们提出了一个用于断裂预测的数据驱动基础模型，这是一个基于Transformer的架构，可以在模拟器、多种材料（包括塑料粘结炸药、钢、铝、页岩和钨）和不同加载条件下运行。该模型支持结构化和非结构化网格，将它们与大型语言模型嵌入的文本输入相结合，指定材料属性、边界条件和求解器设置。这种多模态输入设计使模型能够灵活适应各种模拟场景，而无需更改模型架构。训练好的模型可以用最少的数据在各种下游任务上进行微调，包括失效时间估计、建模断裂演化和适应组合有限元-离散元方法模拟。它还可以推广到未见过的材料，如钛和混凝土，只需要单个样本，与标准机器学习方法相比，显著减少了数据需求。我们的结果表明，断裂预测可以在单一模型架构下统一，为特定模拟器的工作流程提供了一种可扩展、可扩展的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting when and how materials fail is critical to designingsafe, reliable structures, mechanical systems, and engineered components thatoperate under stress. Yet, fracture behavior remains difficult to model acrossthe diversity of materials, geometries, and loading conditions in real-worldapplications. While machine learning (ML) methods show promise, most models aretrained on narrow datasets, lack robustness, and struggle to generalize.Meanwhile, physics-based simulators offer high-fidelity predictions but arefragmented across specialized methods and require substantial high-performancecomputing resources to explore the input space. To address these limitations,we present a data-driven foundation model for fracture prediction, atransformer-based architecture that operates across simulators, a wide range ofmaterials (including plastic-bonded explosives, steel, aluminum, shale, andtungsten), and diverse loading conditions. The model supports both structuredand unstructured meshes, combining them with large language model embeddings oftextual input decks specifying material properties, boundary conditions, andsolver settings. This multimodal input design enables flexible adaptationacross simulation scenarios without changes to the model architecture. Thetrained model can be fine-tuned with minimal data on diverse downstream tasks,including time-to-failure estimation, modeling fracture evolution, and adaptingto combined finite-discrete element method simulations. It also generalizes tounseen materials such as titanium and concrete, requiring as few as a singlesample, dramatically reducing data needs compared to standard ML. Our resultsshow that fracture prediction can be unified under a single model architecture,offering a scalable, extensible alternative to simulator-specific workflows.</description>
      <author>example@mail.com (Agnese Marcato, Aleksandra Pachalieva, Ryley G. Hill, Kai Gao, Xiaoyu Wang, Esteban Rougier, Zhou Lei, Vinamra Agrawal, Janel Chua, Qinjun Kang, Jeffrey D. Hyman, Abigail Hunter, Nathan DeBardeleben, Earl Lawrence, Hari Viswanathan, Daniel O'Malley, Javier E. Santos)</author>
      <guid isPermaLink="false">2507.23077v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation</title>
      <link>http://arxiv.org/abs/2507.23058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A dissertation submitted to The University of Manchester for the  degree of Bachelor of Science in Artificial Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了两种新的合成数据生成方法MObI和AnydoorMed，分别用于自动驾驶和医学图像分析，通过扩散模型实现高度真实且可控的多模态对象修复。&lt;h4&gt;背景&lt;/h4&gt;安全关键应用需要大量多模态数据进行严格测试，但真实世界数据收集成本高、复杂，合成数据方法需要高度真实性和可控性才能有效。&lt;h4&gt;目的&lt;/h4&gt;开发适用于自动驾驶和医学图像分析的合成数据生成方法，提供高度真实且可控的多模态数据。&lt;h4&gt;方法&lt;/h4&gt;MObI是首个多模态对象修复框架，利用扩散模型在相机和激光雷达数据中生成对象修复；AnydoorMed将此范式扩展到医学成像，专注于乳腺X光扫描的参考引导修复。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型可以轻松适应不同感知模态，实现高度真实且可控的多模态对象修复。&lt;h4&gt;结论&lt;/h4&gt;这些方法为构建高度真实、可控和多模态反事实情景的下一代系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;该研究介绍了自动驾驶和医学图像分析领域中的两种新型合成数据生成方法：MObI和AnydoorMed。MObI是一种首创的多模态对象修复框架，利用扩散模型在感知模态（同时适用于相机和激光雷达）中生成真实且可控的对象修复。给定单个参考RGB图像，MObI能够在指定的3D位置（由边界框引导）将对象无缝插入到现有多模态场景中，同时保持语义一致性和多模态一致性。与仅依赖编辑掩码的传统修复方法不同，此方法使用3D边界框条件确保准确的空间定位和真实的缩放。AnydoorMed将此范式扩展到医学成像领域，专注于乳腺X光扫描的参考引导修复。它利用基于扩散的模型以令人印象深刻的细节保留修复异常，同时保持参考异常的结构完整性，并在语义上将其与周围组织融合。这些方法共同证明，自然图像中参考引导修复的基础模型可以轻松适应不同的感知模态，为能够构建高度真实、可控和多模态反事实情景的下一代系统铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成高质量、可控的多模态反事实数据问题。在自动驾驶领域，需要生成逼真的相机和激光雷达数据来测试感知系统；在医学领域，需要生成真实的异常图像来增强数据集。这个问题很重要，因为安全关键应用需要大量数据进行严格测试，但收集真实世界数据成本高、复杂，且难以覆盖罕见但重要的场景。现有合成数据方法往往缺乏可控性或真实感。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于反事实推理是人类智能核心的理念，认为构建能够生成反事实的系统是人工智能发展的重要方向。他们利用了生成模型的优势，特别是潜在扩散模型能够在学习的潜在空间内捕获底层语义结构。作者借鉴了多项现有工作，包括扩散模型（DDPM、DDIM、LDM）、参考引导修复方法（Paint-by-Example）、多模态数据生成技术和3D条件控制方法，并将这些技术整合并扩展到多模态反事实生成任务中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用参考引导的扩散修复技术在多模态数据中生成逼真的反事实示例，并通过3D边界框条件确保准确的定位和缩放。整体流程包括：1)数据处理和编码（图像、激光雷达或医学图像的预处理和VAE编码）；2)条件编码（场景上下文、参考对象、3D边界框和编辑掩码）；3)多模态生成（在潜在空间中使用扩散模型预测噪声，确保多模态一致性）；4)推理和合成（生成新数据并解码回原始空间）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)MObI是首个用于相机-激光雷达多模态物体修复的框架，使用3D边界框条件确保准确空间定位；2)AnydoorMed将参考引导修复扩展到医学图像领域，能合成逼真的医学异常；3)端到端方法，联合生成多模态数据；4)保持语义一致性和多模态一致性。相比之前的工作，这种方法不依赖编辑掩码或高质量3D资产，能处理部分可见的对象，并且是端到端的解决方案，而非多阶段管道。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了两种创新方法（MObI和AnydoorMed），利用参考引导的扩散修复技术在自动驾驶和医学图像领域生成高质量、可控的多模态反事实数据，解决了安全关键应用中数据收集困难和现有合成方法缺乏可控性或真实感的问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safety-critical applications, such as autonomous driving and medical imageanalysis, require extensive multimodal data for rigorous testing. Syntheticdata methods are gaining prominence due to the cost and complexity of gatheringreal-world data, but they demand a high degree of realism and controllabilityto be useful. This work introduces two novel methods for synthetic datageneration in autonomous driving and medical image analysis, namely MObI andAnydoorMed, respectively. MObI is a first-of-its-kind framework for MultimodalObject Inpainting that leverages a diffusion model to produce realistic andcontrollable object inpaintings across perceptual modalities, demonstratedsimultaneously for camera and lidar. Given a single reference RGB image, MObIenables seamless object insertion into existing multimodal scenes at aspecified 3D location, guided by a bounding box, while maintaining semanticconsistency and multimodal coherence. Unlike traditional inpainting methodsthat rely solely on edit masks, this approach uses 3D bounding box conditioningto ensure accurate spatial positioning and realistic scaling. AnydoorMedextends this paradigm to the medical imaging domain, focusing onreference-guided inpainting for mammography scans. It leverages adiffusion-based model to inpaint anomalies with impressive detail preservation,maintaining the reference anomaly's structural integrity while semanticallyblending it with the surrounding tissue. Together, these methods demonstratethat foundation models for reference-guided inpainting in natural images can bereadily adapted to diverse perceptual modalities, paving the way for the nextgeneration of systems capable of constructing highly realistic, controllableand multimodal counterfactual scenarios.</description>
      <author>example@mail.com (Alexandru Buburuzan)</author>
      <guid isPermaLink="false">2507.23058v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Data Readiness for Scientific AI at Scale</title>
      <link>http://arxiv.org/abs/2507.23018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 1 figure, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了人工智能数据准备原则在大规模科学数据集训练基础模型中的应用，分析了四个代表性领域的典型工作流程，并提出了一个二维数据准备框架。&lt;h4&gt;背景&lt;/h4&gt;科学数据用于训练基础模型时面临数据准备的挑战，需要针对高性能计算环境特定的解决方案。&lt;h4&gt;目的&lt;/h4&gt;识别科学数据预处理中的通用模式和领域特定约束，建立评估科学数据准备就绪程度的框架，指导基础设施开发。&lt;h4&gt;方法&lt;/h4&gt;分析气候、核聚变、生物/健康和材料四个领域的典型工作流程，识别预处理模式和约束，构建二维数据准备框架。&lt;h4&gt;主要发现&lt;/h4&gt;科学数据转换为可扩展AI训练数据存在关键挑战，基于Transformer的生成模型需要特定的数据准备策略。&lt;h4&gt;结论&lt;/h4&gt;提出的二维框架形成了概念成熟度矩阵，可表征科学数据准备情况，指导开发标准化、跨域支持的科学AI基础设施，促进可扩展和可重复的科学AI。&lt;h4&gt;翻译&lt;/h4&gt;本文探讨了人工智能数据准备(DRAI)原则如何应用于用于训练基础模型的大规模科学数据集。我们分析了气候、核聚变、生物/健康和材料四个代表性领域的典型工作流程，以识别通用的预处理模式和领域特定约束。我们提出了一个由数据准备级别(从原始数据到AI就绪数据)和数据处理阶段(从摄取到分片)组成的二维框架，这两个维度都针对高性能计算(HPC)环境定制。该框架概述了将科学数据转换为可扩展AI训练数据的关键挑战，强调基于Transformer的生成模型。这两个维度共同形成了一个概念成熟度矩阵，用于表征科学数据准备情况，并指导基础设施开发朝着标准化、跨域支持方向发展，以实现科学AI的可扩展性和可重复性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3750720.3757282&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper examines how Data Readiness for AI (DRAI) principles apply toleadership-scale scientific datasets used to train foundation models. Weanalyze archetypal workflows across four representative domains - climate,nuclear fusion, bio/health, and materials - to identify common preprocessingpatterns and domain-specific constraints. We introduce a two-dimensionalreadiness framework composed of Data Readiness Levels (raw to AI-ready) andData Processing Stages (ingest to shard), both tailored to high performancecomputing (HPC) environments. This framework outlines key challenges intransforming scientific data for scalable AI training, emphasizingtransformer-based generative models. Together, these dimensions form aconceptual maturity matrix that characterizes scientific data readiness andguides infrastructure development toward standardized, cross-domain support forscalable and reproducible AI for science.</description>
      <author>example@mail.com (Wesley Brewer, Patrick Widener, Valentine Anantharaj, Feiyi Wang, Tom Beck, Arjun Shankar, Sarp Oral)</author>
      <guid isPermaLink="false">2507.23018v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>H2Tune: Federated Foundation Model Fine-Tuning with Hybrid Heterogeneity</title>
      <link>http://arxiv.org/abs/2507.22633v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;H2Tune是一种处理混合异构联邦微调场景的新方法，解决了客户端在模型架构和下游任务上表现出的双重异构性问题。该方法通过三个关键组件有效解决了异构矩阵聚合和多任务知识干扰两个主要挑战，实现了高达15.4%的准确率提升。&lt;h4&gt;背景&lt;/h4&gt;现有的联邦微调方法主要针对基础模型，而混合异构联邦微调（HHFFT）是一个尚未充分探索的场景，其中客户端在模型架构和下游任务上表现出双重异构性。&lt;h4&gt;目的&lt;/h4&gt;解决混合异构联邦微调中的两个主要挑战：异构矩阵聚合和多任务知识干扰，提高联邦微调的效率和效果。&lt;h4&gt;方法&lt;/h4&gt;提出了H2Tune框架，包含三个关键组件：(1)稀疏三重矩阵分解，通过构建秩一致的中介矩阵对齐不同客户端的隐藏维度；(2)关系引导的矩阵层对齐，处理异构层结构和表示能力；(3)交替任务知识解纠缠机制，通过交替优化解耦本地模型参数的共享和特定知识。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析证明了收敛率为O(1/√T)，实验结果表明与最先进的基线方法相比，准确率提高了高达15.4%。&lt;h4&gt;结论&lt;/h4&gt;H2Tune框架有效解决了混合异构联邦微调中的双重异构性问题，为处理不同架构模型和不同任务下的联邦学习提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;与现有的基础模型联邦微调方法不同，混合异构联邦微调（HHFFT）是一个尚未充分探索的场景，其中客户端在模型架构和下游任务上表现出双重异构性。这种混合异构性带来了两个重大挑战：1)异构矩阵聚合，客户端根据任务需求和资源限制采用不同的大规模基础模型，导致LoRA参数聚合时出现维度不匹配；2)多任务知识干扰，本地共享参数同时使用任务共享知识和任务特定知识进行训练，无法确保只有任务共享知识在客户端之间传递。为解决这些挑战，我们提出了H2Tune，一种处理混合异构性的联邦基础模型微调方法。我们的框架H2Tune包含三个关键组件：(i)稀疏三重矩阵分解，通过构建秩一致的中介矩阵对齐不同客户端的隐藏维度，并根据客户端资源进行自适应稀疏化；(ii)关系引导的矩阵层对齐，处理异构层结构和表示能力；(iii)交替任务知识解纠缠机制，通过交替优化解耦本地模型参数的共享和特定知识。理论分析证明了收敛率为O(1/√T)。大量实验表明，我们的方法与最先进的基线方法相比，准确率提高了高达15.4%。我们的代码可在https://anonymous.4open.science/r/H2Tune-1407获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Different from existing federated fine-tuning (FFT) methods for foundationmodels, hybrid heterogeneous federated fine-tuning (HHFFT) is an under-exploredscenario where clients exhibit double heterogeneity in model architectures anddownstream tasks. This hybrid heterogeneity introduces two significantchallenges: 1) heterogeneous matrix aggregation, where clients adopt differentlarge-scale foundation models based on their task requirements and resourcelimitations, leading to dimensional mismatches during LoRA parameteraggregation; and 2) multi-task knowledge interference, where local sharedparameters, trained with both task-shared and task-specific knowledge, cannotensure only task-shared knowledge is transferred between clients. To addressthese challenges, we propose H2Tune, a federated foundation model fine-tuningwith hybrid heterogeneity. Our framework H2Tune consists of three keycomponents: (i) sparsified triple matrix decomposition to align hiddendimensions across clients through constructing rank-consistent middle matrices,with adaptive sparsification based on client resources; (ii) relation-guidedmatrix layer alignment to handle heterogeneous layer structures andrepresentation capabilities; and (iii) alternating task-knowledgedisentanglement mechanism to decouple shared and specific knowledge of localmodel parameters through alternating optimization. Theoretical analysis provesa convergence rate of O(1/\sqrt{T}). Extensive experiments show our methodachieves up to 15.4% accuracy improvement compared to state-of-the-artbaselines. Our code is available athttps://anonymous.4open.science/r/H2Tune-1407.</description>
      <author>example@mail.com (Wei Guo, Siyuan Lu, Yiqi Tong, Zhaojun Hu, Fuzhen Zhuang, Xiao Zhang, Tao Fan, Jin Dong)</author>
      <guid isPermaLink="false">2507.22633v2</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Automated Label Placement on Maps via Large Language Models</title>
      <link>http://arxiv.org/abs/2507.22952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Workshop on AI for Data Editing (AI4DE) at KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的自动标签放置(ALP)范式，将其作为数据编辑问题，并利用大型语言模型(LLMs)进行上下文感知的空间标注。作者创建了名为MAPLE的基准数据集，评估了四种开源LLMs的性能，结果表明在结构化提示和领域特定检索的引导下，LLMs能够执行准确的空间编辑，生成的输出符合专家制图标准。这项工作为AI辅助地图 finishing提供了可扩展的框架，展示了基础模型在结构化数据编辑任务中的潜力。&lt;h4&gt;背景&lt;/h4&gt;标签放置是地图设计的关键方面，作为空间标注直接影响地图的清晰度和可解释性。尽管其重要性，标签放置仍然主要依赖手动操作，难以规模化，因为现有自动化系统难以整合制图惯例、适应上下文或解释标注指令。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的自动标签放置(ALP)范式，将其作为数据编辑问题，并利用大型语言模型(LLMs)进行上下文感知的空间标注。为此，创建了MAPLE基准数据集，用于评估真实世界地图上的ALP性能。&lt;h4&gt;方法&lt;/h4&gt;方法包括检索与每个地标类型相关的标注指南，利用检索增强生成(RAG)将其整合到提示中，并使用指令微调的LLMs生成理想的标签坐标。作者在MAPLE上评估了四种开源LLMs，分析了整体性能以及在不同类型地标上的泛化能力，包括零样本和指令微调性能。&lt;h4&gt;主要发现&lt;/h4&gt;大型语言模型在结构化提示和领域特定检索的引导下，能够学习执行准确的空间编辑，使生成的输出与专家制图标准保持一致。&lt;h4&gt;结论&lt;/h4&gt;该工作为AI辅助地图 finishing提供了可扩展的框架，展示了基础模型在结构化数据编辑任务中的潜力。代码和数据可在https://github.com/HarryShomer/MAPLE获取。&lt;h4&gt;翻译&lt;/h4&gt;标签放置是地图设计的关键方面，作为一种空间标注直接影响清晰度和可解释性。尽管其重要性，标签放置仍然主要依赖手动操作，难以规模化，因为现有自动化系统难以整合制图惯例、适应上下文或解释标注指令。在这项工作中，我们引入了一种用于自动标签放置(ALP)的新范式，将任务表述为数据编辑问题，并利用大型语言模型(LLMs)进行上下文感知的空间标注。为支持这一方向，我们创建了MAPLE，这是第一个已知的用于评估真实世界地图上ALP的基准数据集，包含来自开源数据的不同地标类型和标签放置标注。我们的方法利用检索增强生成(RAG)检索与每个地标类型相关的标注指南，将其整合到提示中，并使用指令微调的LLMs生成理想的标签坐标。我们在MAPLE上评估了四种开源LLMs，分析了整体性能以及在不同类型地标上的泛化能力，包括零样本和指令微调性能。我们的结果表明，大型语言模型在结构化提示和领域特定检索的引导下，能够学习执行准确的空间编辑，使生成的输出与专家制图标准保持一致。总体而言，我们的工作为AI辅助地图 finishing提供了可扩展的框架，展示了基础模型在结构化数据编辑任务中的潜力。代码和数据可在https://github.com/HarryShomer/MAPLE找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Label placement is a critical aspect of map design, serving as a form ofspatial annotation that directly impacts clarity and interpretability. Despiteits importance, label placement remains largely manual and difficult to scale,as existing automated systems struggle to integrate cartographic conventions,adapt to context, or interpret labeling instructions. In this work, weintroduce a new paradigm for automatic label placement (ALP) that formulatesthe task as a data editing problem and leverages large language models (LLMs)for context-aware spatial annotation. To support this direction, we curateMAPLE, the first known benchmarking dataset for evaluating ALP on real-worldmaps, encompassing diverse landmark types and label placement annotations fromopen-source data. Our method retrieves labeling guidelines relevant to eachlandmark type leveraging retrieval-augmented generation (RAG), integrates theminto prompts, and employs instruction-tuned LLMs to generate ideal labelcoordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overallperformance and generalization across different types of landmarks. Thisincludes both zero-shot and instruction-tuned performance. Our resultsdemonstrate that LLMs, when guided by structured prompts and domain-specificretrieval, can learn to perform accurate spatial edits, aligning the generatedoutputs with expert cartographic standards. Overall, our work presents ascalable framework for AI-assisted map finishing and demonstrates the potentialof foundation models in structured data editing tasks. The code and data can befound at https://github.com/HarryShomer/MAPLE.</description>
      <author>example@mail.com (Harry Shomer, Jiejun Xu)</author>
      <guid isPermaLink="false">2507.22952v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>Meta CLIP 2: A Worldwide Scaling Recipe</title>
      <link>http://arxiv.org/abs/2507.22062v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Meta CLIP 2，这是第一个从零开始在全球网络规模图像-文本对上训练的CLIP模型，成功解决了多语言CLIP训练中的两大挑战：处理非英语数据点和克服'多语言诅咒'现象。&lt;h4&gt;背景&lt;/h4&gt;CLIP是一种流行的基础模型，支持从零样本分类、检索到多模态大语言模型的编码器。虽然CLIP成功在英语世界的十亿级图像-文本对上训练，但扩展到全球网络数据仍面临挑战：缺乏处理非英语数据的方法，以及现有多语言CLIP的英语性能比仅英语版本差。&lt;h4&gt;目的&lt;/h4&gt;开发一种训练方法，使CLIP能够从全球网络规模的图像-文本对中学习，同时克服多语言训练中的挑战，实现英语和非英语数据的互利共赢。&lt;h4&gt;方法&lt;/h4&gt;提出Meta CLIP 2，从零开始在全球网络规模图像-文本对上训练CLIP。通过严格的消融研究，采用最小必要的变化解决挑战，提出使英语和非英语世界数据互利共赢的方案。&lt;h4&gt;主要发现&lt;/h4&gt;在零样本ImageNet分类中，Meta CLIP 2 ViT-H/14比仅英语版本高0.8%，比mSigLIP高0.7%；在多语言基准测试上设置新最先进水平：CVQA达57.4%，Babel-ImageNet达50.2%，XM3600图像到文本检索达64.3%。&lt;h4&gt;结论&lt;/h4&gt;Meta CLIP 2成功解决了CLIP模型扩展到全球网络数据训练的关键挑战，特别是在多语言环境下的性能优化，为多模态基础模型的进一步发展提供了重要参考。&lt;h4&gt;翻译&lt;/h4&gt;对比语言-图像预训练(CLIP)是一种流行的基础模型，支持从零样本分类、检索到多模态大语言模型的编码器。尽管CLIP成功在英语世界的十亿级图像-文本对上训练，但进一步扩展训练以从全球网络数据中学习仍面临挑战：(1)缺乏处理非英语数据的方法；(2)现有多语言CLIP的英语性能比仅英语版本差，即'多语言诅咒'现象。我们提出Meta CLIP 2，这是首个从零开始在全球网络规模图像-文本对上训练CLIP的方案。通过严格的消融研究，采用最小必要变化解决上述挑战，提出使英语和非英语数据互利共赢的方案。在零样本ImageNet分类中，Meta CLIP 2 ViT-H/14比仅英语版本高0.8%，比mSigLIP高0.7%，并在多语言基准测试上设置新最先进水平，无系统混淆因素，如CVQA达57.4%，Babel-ImageNet达50.2%，XM3600图像到文本检索达64.3%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,supporting from zero-shot classification, retrieval to encoders for multimodallarge language models (MLLMs). Although CLIP is successfully trained onbillion-scale image-text pairs from the English world, scaling CLIP's trainingfurther to learning from the worldwide web data is still challenging: (1) nocuration method is available to handle data points from non-English world; (2)the English performance from existing multilingual CLIP is worse than itsEnglish-only counterpart, i.e., "curse of multilinguality" that is common inLLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratchon worldwide web-scale image-text pairs. To generalize our findings, we conductrigorous ablations with minimal changes that are necessary to address the abovechallenges and present a recipe enabling mutual benefits from English andnon-English world data. In zero-shot ImageNet classification, Meta CLIP 2ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,and surprisingly sets new state-of-the-art without system-level confoundingfactors (e.g., translation, bespoke architecture changes) on multilingualbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with64.3% on image-to-text retrieval.</description>
      <author>example@mail.com (Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, Xinlei Chen, Zhuang Liu, Saining Xie, Wen-tau Yih, Shang-Wen Li, Hu Xu)</author>
      <guid isPermaLink="false">2507.22062v2</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>MamV2XCalib: V2X-based Target-less Infrastructure Camera Calibration with State Space Model</title>
      <link>http://arxiv.org/abs/2507.23595v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV25 poster&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MamV2XCalib的基于V2X的基础设施摄像头自动标定方法，利用车载激光雷达实现高效、精确的标定，无需特定参考物体或人工干预。&lt;h4&gt;背景&lt;/h4&gt;随着利用路边摄像头辅助自动驾驶车辆感知的协作系统日益普及，大规模精确标定基础设施摄像头已成为关键问题。传统手动标定方法通常耗时、费力，且可能需要封闭道路。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于V2X的基础设施摄像头自动标定方法，减少对人工干预和特定参考物体的依赖，提高标定效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出MamV2XCalib，首个基于V2X并借助车载激光雷达的基础设施摄像头标定方法。引入一种新的无目标激光雷达-摄像头标定方法，结合多尺度特征和4D相关体积估计车载点云与路边图像的关联性。使用Mamba建模时间信息并估计旋转角度，解决V2X场景中标定失败问题。&lt;h4&gt;主要发现&lt;/h4&gt;在V2X-Seq和TUMTraf-V2X真实世界数据集上评估表明，该方法具有有效性和鲁棒性。与之前为单车标定设计的激光雷达-摄像头方法相比，在V2X场景下用更少参数实现了更好且更稳定的标定性能。&lt;h4&gt;结论&lt;/h4&gt;MamV2XCalib是一种创新的V2X-based基础设施摄像头标定解决方案，仅需配备激光雷达的自动驾驶车辆在待标定摄像头附近行驶即可完成标定，无需特定参考物体或人工干预。&lt;h4&gt;翻译&lt;/h4&gt;随着利用路边摄像头辅助自动驾驶车辆感知的协作系统日益普及，大规模精确标定基础设施摄像头已成为关键问题。传统手动标定方法通常耗时、费力，且可能需要封闭道路。本文提出了MamV2XCalib，这是首个基于V2X并借助车载激光雷达的基础设施摄像头标定方法。MamV2XCalib仅需配备激光雷达的自动驾驶车辆在基础设施中待标定的摄像头附近行驶，无需特定参考物体或人工干预。我们还引入了一种新的无目标激光雷达-摄像头标定方法，结合多尺度特征和4D相关体积来估计车载点云与路边图像之间的相关性。我们使用Mamba建模时间信息并估计旋转角度，有效解决了V2X场景中标定失败问题，这些问题由车载数据缺陷（如遮挡）和视点差异大引起。我们在V2X-Seq和TUMTraf-V2X真实世界数据集上评估了MamV2XCalib，证明了我们基于V2X的自动标定方法的有效性和鲁棒性。与之前为单车标定设计的激光雷达-摄像头方法相比，我们的方法在V2X场景下用更少的参数实现了更好且更稳定的标定性能。代码可在https://github.com/zhuyaoye/MamV2XCalib获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决大规模基础设施摄像头的精确标定问题。随着利用路边摄像头辅助自动驾驶车辆感知的协作系统越来越普及，这些摄像头在户外部署容易受到天气和振动影响导致位置偏移，如果不重新标定会显著降低感知系统准确性甚至导致完全失效。高效可扩展的标定方法对稳定的智能交通系统至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统标定方法耗时耗力和现有自动标定方法依赖特定检测或参考图像的不足，然后创新性地提出反转V2X过程，利用车辆感知数据将整个环境作为参考来标定摄像头。方法借鉴了光流估计(RAFT模型)构建4D相关体积，参考了激光雷达-摄像头标定任务中的特征匹配方法，并利用Mamba架构处理时序信息，但针对V2X场景进行了专门改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用车辆移动过程中收集的多帧激光雷达点云和路边摄像头图像数据，通过时空建模估计路边摄像头的旋转偏差，无需特定参考物体。整体流程包括：1)输入处理，将激光雷达点云投影到摄像头平面生成深度图；2)多尺度特征提取，使用三个分支分别提取图像、深度图和上下文特征；3)特征匹配和迭代细化，构建4D相关体积并迭代更新流场；4)使用Mamba架构进行时空信息聚合；5)回归旋转偏差；6)损失函数计算和模型训练；7)多网络协同推理提高标定精度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个基于V2X的无目标基础设施摄像头标定解决方案；2)结合多尺度处理、迭代更新和Mamba架构的新型标定网络；3)在真实数据集上验证了方法有效性。相比传统方法，它完全利用环境数据且无需预准备参考物；相比现有单车辆标定算法，它解决了车辆点云在路边视角下覆盖区域小且不确定的问题，用更少参数实现了更稳定的标定性能；通过时序信息解决了V2X场景中的特殊挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MamV2XCalib首次提出了一种基于V2X的无目标基础设施摄像头标定方法，通过融合车载激光雷达点云与路边摄像头图像，并利用Mamba状态空间模型进行时空建模，实现了高效、自动、鲁棒的路边摄像头旋转偏差标定。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As cooperative systems that leverage roadside cameras to assist autonomousvehicle perception become increasingly widespread, large-scale precisecalibration of infrastructure cameras has become a critical issue. Traditionalmanual calibration methods are often time-consuming, labor-intensive, and mayrequire road closures. This paper proposes MamV2XCalib, the first V2X-basedinfrastructure camera calibration method with the assistance of vehicle-sideLiDAR. MamV2XCalib only requires autonomous vehicles equipped with LiDAR todrive near the cameras to be calibrated in the infrastructure, without the needfor specific reference objects or manual intervention. We also introduce a newtargetless LiDAR-camera calibration method, which combines multi-scale featuresand a 4D correlation volume to estimate the correlation between vehicle-sidepoint clouds and roadside images. We model the temporal information andestimate the rotation angles with Mamba, effectively addressing calibrationfailures in V2X scenarios caused by defects in the vehicle-side data (such asocclusions) and large differences in viewpoint. We evaluate MamV2XCalib on theV2X-Seq and TUMTraf-V2X real-world datasets, demonstrating the effectivenessand robustness of our V2X-based automatic calibration approach. Compared toprevious LiDAR-camera methods designed for calibration on one car, our approachachieves better and more stable calibration performance in V2X scenarios withfewer parameters. The code is available athttps://github.com/zhuyaoye/MamV2XCalib.</description>
      <author>example@mail.com (Yaoye Zhu, Zhe Wang, Yan Wang)</author>
      <guid isPermaLink="false">2507.23595v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>FastPoint: Accelerating 3D Point Cloud Model Inference via Sample Point Distance Prediction</title>
      <link>http://arxiv.org/abs/2507.23480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FastPoint的新型软件加速技术，用于高效处理大型和不规则的3D点云，通过预测距离曲线加速最远点采样和邻域搜索操作，在保持采样质量和模型性能的同时实现了显著的加速效果。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络已经革新了3D点云处理，但高效处理大型和不规则点云仍然面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决在3D点云处理中高效处理大型和不规则点云的问题。&lt;h4&gt;方法&lt;/h4&gt;引入FastPoint，一种基于软件的加速技术，利用最远点采样过程中采样点之间的可预测距离趋势，通过预测距离曲线来高效识别后续样本点，避免穷举计算所有点对距离。&lt;h4&gt;主要发现&lt;/h4&gt;FastPoint显著加速了最远点采样和邻域搜索操作，同时保持采样质量和模型性能；集成到最先进的3D点云模型中，在NVIDIA RTX 3090 GPU上实现了2.55倍的端到端加速，而不牺牲准确性。&lt;h4&gt;结论&lt;/h4&gt;FastPoint是一种有效的加速技术，可以在不牺牲质量的情况下提高3D点云处理的效率。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络已经革新了3D点云处理，但高效处理大型和不规则点云仍然具有挑战性。为解决这个问题，我们引入了FastPoint，一种新颖的软件加速技术，它利用最远点采样过程中采样点之间的可预测距离趋势。通过预测距离曲线，我们可以有效地识别后续样本点，而不需要穷举计算所有点对距离。我们的提议显著加速了最远点采样和邻域搜索操作，同时保持采样质量和模型性能。通过将FastPoint集成到最先进的3D点云模型中，我们在NVIDIA RTX 3090 GPU上实现了2.55倍的端到端加速，而不会牺牲准确性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云模型推理过程中的计算效率问题，特别是最远点采样(FPS)和邻域搜索操作的性能瓶颈。这个问题在现实中非常重要，因为3D点云在机器人、自动驾驶等领域变得越来越重要，用于表示和理解3D场景。随着点云数据量的增长和不规则性的增加，计算挑战变得更加显著，限制了实时应用场景的可行性。现有的硬件加速器开发成本高且复杂，而软件解决方案通常以显著精度损失为代价，限制了实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3D点云模型中的性能瓶颈，发现FPS和邻域搜索操作占用了大部分计算时间。他们观察到FPS算法有两个可预测的趋势：最小距离递减（随着采样进行，采样点间的最小距离平滑递减）和早期结构捕获（初始采样点往往是点云的极值点）。基于这些观察，作者提出可以通过仅使用几个初始FPS迭代来准确估计距离曲线，从而高效识别后续采样点。作者借鉴了现有硬件加速器（如QuickFPS）和软件加速技术（如RandLA-Net、Grid采样）的思想，但通过预测距离曲线的方式避免了精度损失，同时实现了显著的加速。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; FastPoint的核心思想是利用最远点采样(FPS)过程中采样点之间的可预测距离趋势，通过预测距离曲线来高效识别后续采样点，而无需全面计算所有点对距离。整体实现流程包括：1)最小距离预测采样(MDPS)：使用前10%的FPS迭代结果训练MLP预测器预测剩余距离曲线，将曲线分段，基于排除列表进行采样，并实现提前终止；2)无冗余邻域搜索：直接使用排除列表进行球查询，利用排除列表限制k-NN的搜索空间。这种方法不仅加速了FPS本身，还通过预计算的距离信息使后续的邻域搜索操作受益。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)最小距离预测采样(MDPS)：首次利用FPS过程中的可预测距离趋势，通过MLP预测器估计距离曲线，实现采样和距离计算的解耦；2)无冗余邻域搜索：重用排除列表优化后续操作；3)纯软件加速方案：无需专用硬件即可实现高效推理。相比之前的工作，FastPoint保持了与原始FPS相近的采样质量（超过98%），而其他软件方法通常导致显著精度损失；作为纯软件解决方案，它可以在普通GPU上运行，且可与硬件加速方法结合使用，实现更大的加速。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FastPoint通过预测最远点采样过程中的距离曲线，实现了3D点云模型的高效推理，在保持模型准确性的同时实现了2.55倍的端到端加速，为点云处理提供了一种实用且高效的软件解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks have revolutionized 3D point cloud processing, yetefficiently handling large and irregular point clouds remains challenging. Totackle this problem, we introduce FastPoint, a novel software-basedacceleration technique that leverages the predictable distance trend betweensampled points during farthest point sampling. By predicting the distancecurve, we can efficiently identify subsequent sample points withoutexhaustively computing all pairwise distances. Our proposal substantiallyaccelerates farthest point sampling and neighbor search operations whilepreserving sampling quality and model performance. By integrating FastPointinto state-of-the-art 3D point cloud models, we achieve 2.55x end-to-endspeedup on NVIDIA RTX 3090 GPU without sacrificing accuracy.</description>
      <author>example@mail.com (Donghyun Lee, Dawoon Jeong, Jae W. Lee, Hongil Yoon)</author>
      <guid isPermaLink="false">2507.23480v1</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    <item>
      <title>MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors</title>
      <link>http://arxiv.org/abs/2507.21872v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MultiEditor是一种双分支潜在扩散框架，用于共同编辑驾驶场景中的图像和LiDAR点云，通过引入3D高斯溅射作为先验，并设计多级别外观控制机制和深度引导的可变形跨模态条件模块，有效解决了自动驾驶系统中多模态感知的长尾分布问题，提高了对稀有车辆类别的感知能力。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶系统依赖多模态感知数据来理解复杂环境，但真实世界数据的长尾分布阻碍了泛化能力，特别是对于稀有但安全关键的车辆类别。&lt;h4&gt;目的&lt;/h4&gt;解决数据长尾分布导致的泛化问题，提高对稀有但安全关键的车辆类别的感知能力。&lt;h4&gt;方法&lt;/h4&gt;提出MultiEditor双分支潜在扩散框架，引入3D高斯溅射作为目标对象的结构和外观先验，设计多级别外观控制机制（包括像素级粘贴、语义级指导和多分支细化），以及深度引导的可变形跨模态条件模块，实现图像和LiDAR点云的联合编辑。&lt;h4&gt;主要发现&lt;/h4&gt;MultiEditor在视觉和几何保真度、编辑可控性和跨模态一致性方面表现出色；使用MultiEditor生成稀有类别车辆数据显著提高了感知模型在代表性不足类别上的检测准确性。&lt;h4&gt;结论&lt;/h4&gt;MultiEditor能够有效解决自动驾驶系统中多模态感知的长尾分布问题，通过联合编辑图像和LiDAR点云，提高了对稀有车辆类别的感知能力。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶系统严重依赖多模态感知数据来理解复杂环境。然而，真实世界数据的长尾分布阻碍了泛化能力，特别是对于稀有但安全关键的车辆类别。为应对这一挑战，我们提出了MultiEditor，一种双分支潜在扩散框架，专为共同编辑驾驶场景中的图像和LiDAR点云而设计。我们方法的核心是引入3D高斯溅射作为目标对象的结构和外观先验。利用这一先验，我们设计了一个多级别外观控制机制，包括像素级粘贴、语义级指导和多分支细化，以实现跨模态的高保真重建。我们进一步提出了一个深度引导的可变形跨模态条件模块，使用3DGS渲染的深度自适应地实现模态间的相互引导，显著增强了跨模态一致性。大量实验表明，MultiEditor在视觉和几何保真度、编辑可控性和跨模态一致性方面表现出色。此外，使用MultiEditor生成稀有类别车辆数据显著提高了感知模型在代表性不足类别上的检测准确性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶系统中多模态数据（图像和LiDAR点云）编辑的长尾分布问题，特别是罕见但安全关键的车辆类别（如压路机、挖掘机）数据不足的问题。这个问题在现实中很重要，因为数据不平衡导致感知模型在边缘情况下的鲁棒性不足，而自动驾驶系统需要准确识别各种类型的车辆以确保安全性。现有方法大多是单模态编辑，缺乏跨模态一致性，难以精确控制对象姿态和罕见类别编辑。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了自动驾驶场景中多模态数据编辑的挑战，特别是长尾数据分布问题。他们认识到现有单模态方法的局限性，需要一种能同时编辑图像和LiDAR点云并保持一致性的方法。设计上，作者借鉴了潜在扩散模型(LDMs)、3D高斯散射(3DGS)、图像编辑技术(如Paint-by-Example和AnyDoor)、点云处理方法(如RangeLDM)以及跨模态交互机制(如X-Drive)。核心思路是利用3DGS作为统一先验，设计双分支扩散框架，通过多层次外观控制和跨模态条件模块确保一致性和高质量编辑。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用3D高斯散射(3DGS)作为目标对象的结构和外观统一先验，设计双分支潜在扩散框架同时处理图像和LiDAR点云，并通过多层次外观控制和跨模态条件模块确保一致性和高质量。整体流程包括：1)数据准备(从KITTI构建训练数据，处理阴影问题)；2)双分支框架(图像分支使用像素级保留、语义级引导和多分支优化；点云分支针对范围图像特性定制)；3)跨模态条件模块(利用3DGS渲染的深度图建立对齐，通过可变形交叉注意力实现双向条件传递)；4)五阶段训练策略；5)推理过程(给定目标对象和场景掩码，生成编辑后的多模态数据)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个整合3DGS作为统一先验的多模态编辑框架；2)多层次外观控制机制(像素级保留、语义级引导、多分支优化)；3)深度引导的可变形跨模态条件模块；4)支持罕见车辆类型的编辑。相比之前工作的不同：1)实现了图像和LiDAR点云的联合编辑而非单模态；2)使用3DGS而非数据集派生先验；3)通过专门模块确保跨模态一致性；4)提供更高的编辑可控性和罕见类别处理能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MultiEditor通过整合3D高斯散射作为统一先验，实现了自动驾驶场景中图像和LiDAR点云的高保真、一致且可控的联合编辑，显著提升了感知模型在罕见车辆类别上的检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving systems rely heavily on multimodal perception data tounderstand complex environments. However, the long-tailed distribution ofreal-world data hinders generalization, especially for rare but safety-criticalvehicle categories. To address this challenge, we propose MultiEditor, adual-branch latent diffusion framework designed to edit images and LiDAR pointclouds in driving scenarios jointly. At the core of our approach is introducing3D Gaussian Splatting (3DGS) as a structural and appearance prior for targetobjects. Leveraging this prior, we design a multi-level appearance controlmechanism--comprising pixel-level pasting, semantic-level guidance, andmulti-branch refinement--to achieve high-fidelity reconstruction acrossmodalities. We further propose a depth-guided deformable cross-modalitycondition module that adaptively enables mutual guidance between modalitiesusing 3DGS-rendered depth, significantly enhancing cross-modality consistency.Extensive experiments demonstrate that MultiEditor achieves superiorperformance in visual and geometric fidelity, editing controllability, andcross-modality consistency. Furthermore, generating rare-category vehicle datawith MultiEditor substantially enhances the detection accuracy of perceptionmodels on underrepresented classes.</description>
      <author>example@mail.com (Shouyi Lu, Zihan Lin, Chao Lu, Huanran Wang, Guirong Zhuo, Lianqing Zheng)</author>
      <guid isPermaLink="false">2507.21872v3</guid>
      <pubDate>Fri, 01 Aug 2025 14:33:36 +0800</pubDate>
    </item>
    </channel>
</rss>