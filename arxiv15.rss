<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 11 Nov 2025 16:30:28 +0800</lastBuildDate>
    <item>
      <title>How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?</title>
      <link>http://arxiv.org/abs/2511.05449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为gitmerge3D的全局感知图令牌合并方法，可以显著减少3D点云transformer模型中的令牌数量，同时保持竞争性性能，提高计算效率。&lt;h4&gt;背景&lt;/h4&gt;3D点云transformer在语义分割和重建等任务中取得了最先进的结果，但这些模型通常依赖于密集的令牌表示，导致训练和推理过程中计算和内存成本高昂。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D点云transformer模型中令牌冗余问题，减少计算和内存成本，提高模型的可扩展性。&lt;h4&gt;方法&lt;/h4&gt;提出gitmerge3D，一种全局感知的图令牌合并方法，可以减少高达90-95%的令牌数量，同时保持竞争性性能。&lt;h4&gt;主要发现&lt;/h4&gt;令牌存在显著的冗余性，许多当前模型存在过度令牌化和可扩展性不足的问题；减少令牌数量不会导致性能下降，反而可以提高计算效率。&lt;h4&gt;结论&lt;/h4&gt;该研究首次评估了大规模3D transformer模型中的冗余性，为开发更高效的3D基础架构提供了见解；该方法在多个3D视觉任务中验证了计算效率的一致性改进。&lt;h4&gt;翻译&lt;/h4&gt;3D点云transformer的最新进展已在语义分割和重建等任务中取得了最先进的结果。然而，这些模型通常依赖于密集的令牌表示，导致训练和推理过程中计算和内存成本高昂。在这项工作中，我们发现令牌存在显著的冗余性，导致效率低下。我们引入了gitmerge3D，一种全局感知的图令牌合并方法，可以在保持竞争性性能的同时将令牌数量减少高达90-95%。这一发现挑战了更多令牌必然带来更好性能的普遍假设，并指出当前许多模型存在过度令牌化和可扩展性不足的问题。我们在多个3D视觉任务中验证了我们的方法，并展示了计算效率的一致性改进。这项工作是首次评估大规模3D transformer模型中的冗余性，为开发更高效的3D基础架构提供了见解。我们的代码和检查点可在https://gitmerge3d.github.io公开获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云Transformer架构中token冗余的问题。当前模型使用大量token表示点云数据，导致计算和内存成本过高，限制了模型在实际应用中的部署。这个问题很重要，因为它关系到3D视觉技术的效率和可扩展性，影响自动驾驶、机器人、AR/VR等需要处理大规模3D数据的实时应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性实验发现现有3D点云Transformer模型存在大量token冗余，即使减少90-95%的token也能保持性能。他们借鉴了现有的token合并技术（如Token Merging、Token Pruning等），但这些方法主要针对2D图像设计。作者创新性地结合了3D点云的空间结构特性，设计了全局感知的能量分数来评估token重要性，并实现了自适应合并策略，根据不同区域的信息密度动态调整合并比例。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是3D点云Transformer中的token存在大量冗余，只需保留最具空间信息的token即可保持性能；不同区域的信息密度不同，应采用自适应的合并策略。整体流程：1)将点云分区；2)计算每个token的能量分数；3)计算分区平均能量；4)根据能量分数决定合并策略（高能量分区温和合并，低能量分区激进合并）；5)在分区内合并token；6)通过解合并恢复原始结构供后续层使用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次系统揭示3D点云Transformer中90-95%的token是冗余的；2)提出gitmerge3D方法，专门针对3D点云的全局感知图token合并；3)设计基于能量分数的自适应合并策略；4)实现无需重新训练即可大幅减少token，同时支持微调恢复性能。相比之前工作：不同于主要针对2D图像的token减少方法，我们的方法充分利用3D点云的空间特性；支持特征恢复，适合密集分割任务；实现了更高的token减少比例（最高99%），同时保持或超过原有性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文揭示了3D点云Transformer中token的高度冗余性，并提出了一种创新的3D感知token合并方法，能够在减少90-95%token的同时保持competitive性能，显著提高了计算效率和内存利用率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in 3D point cloud transformers have led to state-of-the-artresults in tasks such as semantic segmentation and reconstruction. However,these models typically rely on dense token representations, incurring highcomputational and memory costs during training and inference. In this work, wepresent the finding that tokens are remarkably redundant, leading tosubstantial inefficiency. We introduce gitmerge3D, a globally informed graphtoken merging method that can reduce the token count by up to 90-95% whilemaintaining competitive performance. This finding challenges the prevailingassumption that more tokens inherently yield better performance and highlightsthat many current models are over-tokenized and under-optimized forscalability. We validate our method across multiple 3D vision tasks and showconsistent improvements in computational efficiency. This work is the first toassess redundancy in large-scale 3D transformer models, providing insights intothe development of more efficient 3D foundation architectures. Our code andcheckpoints are publicly available at https://gitmerge3d.github.io</description>
      <author>example@mail.com (Tuan Anh Tran, Duy M. H. Nguyen, Hoai-Chau Tran, Michael Barz, Khoa D. Doan, Roger Wattenhofer, Ngo Anh Vien, Mathias Niepert, Daniel Sonntag, Paul Swoboda)</author>
      <guid isPermaLink="false">2511.05449v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
  <item>
      <title>SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards</title>
      <link>http://arxiv.org/abs/2511.07403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint. Accepted at NeurIPS 2025 Workshops on SPACE in Vision,  Language, and Embodied AI (SpaVLE), Embodied World Models for Decision Making  (EWM), Aligning Reinforcement Learning Experimentalists and Theorists  (ARLET), and Scaling Environments for Agents (SEA)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpatialThinker是一种3D感知的多模态大语言模型，通过强化学习训练，将结构化空间定位与多步推理相结合，有效解决了现有模型在空间理解方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在视觉-语言任务中取得了显著进展，但在空间理解方面仍存在困难。现有的空间MLLM通常依赖于显式的3D输入或特定的架构修改，并受限于大规模数据集或稀疏监督。&lt;h4&gt;目的&lt;/h4&gt;解决现有空间MLLM的局限性，开发一种能够在有限数据下实现稳健3D空间理解的模型，推动MLLMs向人类水平的视觉推理发展。&lt;h4&gt;方法&lt;/h4&gt;SpatialThinker通过构建与任务相关的对象和空间关系的场景图来模拟类人的空间感知，并通过密集空间奖励推理得出答案。主要贡献包括：(1)数据合成管道生成高质量空间VQA数据集STVQA-7K；(2)使用多目标密集空间奖励的在线强化学习强制执行空间定位。&lt;h4&gt;主要发现&lt;/h4&gt;SpatialThinker-7B在空间理解和真实世界VQA基准测试上优于监督微调和稀疏强化学习基线，与稀疏强化学习相比几乎使基模型增益翻倍，并超越了GPT-4o的性能。&lt;h4&gt;结论&lt;/h4&gt;结合空间监督与奖励对齐推理的方法在有限数据下能有效实现稳健的3D空间理解，是推动MLLMs向人类水平视觉推理发展的重要一步。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在视觉-语言任务中已取得显著进展，但它们在空间理解方面仍然存在困难。现有的空间MLLM通常依赖于显式的3D输入或特定的架构修改，并受限于大规模数据集或稀疏监督。为解决这些局限性，我们引入了SpatialThinker，一种通过强化学习训练的3D感知MLLM，将结构化空间定位与多步推理相结合。该模型通过构建与任务相关的对象和空间关系的场景图来模拟类人的空间感知，并通过密集空间奖励推理得出答案。SpatialThinker包含两个关键贡献：(1)生成高质量空间VQA数据集STVQA-7K的数据合成管道；(2)使用多目标密集空间奖励强制执行空间定位的在线强化学习。SpatialThinker-7B在空间理解和真实世界VQA基准测试上优于监督微调和稀疏强化学习基线，与稀疏强化学习相比几乎使基模型增益翻倍，并超越了GPT-4o。这些结果表明，结合空间监督与奖励对齐推理在有限数据下实现稳健3D空间理解的有效性，推动MLLMs向人类水平的视觉推理发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态大语言模型(MLLMs)在空间理解方面的局限性，特别是在3D空间理解上的困难。这个问题非常重要，因为空间推理是人类智能的核心能力，对于机器人操作、导航、增强现实等具身AI任务至关重要，这些任务需要精确的空间意识作为交互决策的基础。现有方法要么需要大量训练数据，要么需要特定架构修改，要么依赖显式的3D输入，限制了MLLMs在现实世界应用中的能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有MLLMs在空间理解上的局限性，借鉴了人类空间认知过程（先构建场景关系再推理）和现有技术（场景图表示、强化学习与可验证奖励）。他们注意到现有RL方法使用简单的最终正确性奖励，对视觉引导推理指导不足，因此设计了更密集的奖励信号。方法设计上，他们将场景图与端到端推理集成（而非作为外部预处理），并采用多目标奖励框架，结合了格式、计数、准确度和空间奖励，通过字典序排序确保模型先满足格式要求，再优化计数和准确度，最后获得空间奖励。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过强化学习训练一个3D感知的MLLM，将结构化的场景图基础与多步空间推理相结合，模拟人类的空间感知过程。整体流程包括：1) 构建问题聚焦的场景子图，捕捉物体、关系和坐标；2) 设计多目标密集奖励（格式、计数、准确度和空间奖励），使用字典序排序；3) 使用GRPO进行在线RL策略优化；4) 构建STVQA-7K数据集（基于人类标注的场景图）；5) 在高分辨率图像上训练模型，更新所有参数（包括视觉编码器）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个将场景图基础与在线RL结合的MLLM，仅需7K样本就实现强大性能；2) 创建STVQA-7K高质量空间VQA数据集及可扩展生成管道；3) 设计密集多目标空间奖励，提供更丰富的学习信号；4) 展示了高质量数据与适当指导相结合的高效学习。相比之前工作，SPATIALTHINKER在数据效率上高出2-3个数量级（仅需7K样本 vs 数百万样本），使用密集而非稀疏奖励，将场景图与端到端推理集成，并在多个基准测试上超越GPT-4o和Claude 3.5 Sonnet等模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SPATIALTHINKER通过结合场景图基础与密集空间奖励的强化学习，仅用7K训练样本就实现了超越现有多模态大语言模型的3D空间理解能力，展示了高质量数据与适当指导相结合的强大学习效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have achieved remarkable progress invision-language tasks, but they continue to struggle with spatialunderstanding. Existing spatial MLLMs often rely on explicit 3D inputs orarchitecture-specific modifications, and remain constrained by large-scaledatasets or sparse supervision. To address these limitations, we introduceSpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatialgrounding with multi-step reasoning. The model simulates human-like spatialperception by constructing a scene graph of task-relevant objects and spatialrelations, and reasoning towards an answer via dense spatial rewards.SpatialThinker consists of two key contributions: (1) a data synthesis pipelinethat generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RLwith a multi-objective dense spatial reward enforcing spatial grounding.SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baselineon spatial understanding and real-world VQA benchmarks, nearly doubling thebase-model gain compared to sparse RL, and surpassing GPT-4o. These resultsshowcase the effectiveness of combining spatial supervision with reward-alignedreasoning in enabling robust 3D spatial understanding with limited data andadvancing MLLMs towards human-level visual reasoning.</description>
      <author>example@mail.com (Hunar Batra, Haoqin Tu, Hardy Chen, Yuanze Lin, Cihang Xie, Ronald Clark)</author>
      <guid isPermaLink="false">2511.07403v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Inference-Time Scaling of Diffusion Models for Infrared Data Generation</title>
      <link>http://arxiv.org/abs/2511.07362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Peer-reviewed workshop paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种利用领域适应的CLIP验证器来改进红外图像生成质量的方法，通过在推理时指导扩散模型，能够在有限数据条件下提高生成效果。&lt;h4&gt;背景&lt;/h4&gt;红外成像使用被动传感器进行基于温度的场景理解，在低能见度条件下优于传统RGB成像。然而，红外应用开发受限于高质量标注数据的稀缺，因为红外标注需要专业知识。合成红外图像生成可提供大规模训练数据，但受限于数据集不足，难以训练基础级生成扩散模型。&lt;h4&gt;目的&lt;/h4&gt;在数据限制条件下，探索一种推理时扩展方法，使用领域适应的基于CLIP的验证器来增强红外图像生成质量。&lt;h4&gt;方法&lt;/h4&gt;采用参数高效技术，在少量红外图像样本上微调FLUX.1-dev（最先进的文本到图像扩散模型）。训练好的验证器在推理过程中用于引导扩散采样，生成更高质量的红外图像，更好地与输入文本提示保持一致。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在生成质量上取得了一致的改进。与无引导的基线样本相比，在KAIST多光谱行人检测基准数据集上将FID分数降低了10%。&lt;h4&gt;结论&lt;/h4&gt;推理时指导为弥合低数据红外设置中的领域差距提供了有希望的方向。&lt;h4&gt;翻译&lt;/h4&gt;红外成像使用被动传感器实现基于温度的场景理解，特别是在传统RGB成像失效的低能见度条件下。然而，为红外应用开发下游视觉模型受到高质量标注数据稀缺的阻碍，因为红外标注需要专业知识。虽然合成红外图像生成有潜力通过提供大规模、多样化的训练数据来加速模型开发，但由于数据集有限，在红外领域训练基础级生成扩散模型一直难以实现。鉴于这些数据限制，我们探索了一种使用领域适应的基于CLIP的验证器的推理时扩展方法，以增强红外图像生成质量。我们使用参数高效技术，在少量红外图像样本上微调FLUX.1-dev（最先进的文本到图像扩散模型），将其适应到红外领域。训练好的验证器随后在推理过程中被使用，以引导扩散采样过程，生成更高质量的红外图像，更好地与输入文本提示保持一致。经验表明，我们发现我们的方法在生成质量上取得了一致的改进，与无引导的基线样本相比，在KAIST多光谱行人检测基准数据集上将FID分数降低了10%。我们的结果表明，推理时指导为弥合低数据红外设置中的领域差距提供了有希望的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infrared imagery enables temperature-based scene understanding using passivesensors, particularly under conditions of low visibility where traditional RGBimaging fails. Yet, developing downstream vision models for infraredapplications is hindered by the scarcity of high-quality annotated data, due tothe specialized expertise required for infrared annotation. While syntheticinfrared image generation has the potential to accelerate model development byproviding large-scale, diverse training data, training foundation-levelgenerative diffusion models in the infrared domain has remained elusive due tolimited datasets. In light of such data constraints, we explore aninference-time scaling approach using a domain-adapted CLIP-based verifier forenhanced infrared image generation quality. We adapt FLUX.1-dev, astate-of-the-art text-to-image diffusion model, to the infrared domain byfinetuning it on a small sample of infrared images using parameter-efficienttechniques. The trained verifier is then employed during inference to guide thediffusion sampling process toward higher quality infrared generations thatbetter align with input text prompts. Empirically, we find that our approachleads to consistent improvements in generation quality, reducing FID scores onthe KAIST Multispectral Pedestrian Detection Benchmark dataset by 10% comparedto unguided baseline samples. Our results suggest that inference-time guidanceoffers a promising direction for bridging the domain gap in low-data infraredsettings.</description>
      <author>example@mail.com (Kai A. Horstmann, Maxim Clouser, Kia Khezeli)</author>
      <guid isPermaLink="false">2511.07362v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving</title>
      <link>http://arxiv.org/abs/2511.07292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PlanT 2.0，一个面向对象的轻量级规划transformer，用于自动驾驶研究。通过系统性扰动模型输入，作者分析了模型失败的根本原因，并在CARLA基准测试中实现了最先进性能。研究揭示了模型存在的偏见和捷径学习问题，并提出了向数据中心开发转变的建议。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶领域最近的优先事项是基准性能和方法创新，而非对模型失败、偏见和捷径学习的深入分析。这导致了对当前失败缺乏深入理解的渐进式改进。&lt;h4&gt;目的&lt;/h4&gt;理解自动驾驶模型失败的根本原因，通过系统性扰动模型输入并观察预测结果，进行深入分析，并提出改进方向。&lt;h4&gt;方法&lt;/h4&gt;引入PlanT 2.0，一个轻量级、面向对象的规划transformer，专为CARLA中的自动驾驶研究设计。对象级表示使受控分析成为可能，因为输入可以轻松扰动。为应对CARLA Leaderboard 2.0的新挑战，对PlanT进行了多项升级。&lt;h4&gt;主要发现&lt;/h4&gt;分析揭示了模型存在的失败案例，包括：由于障碍物多样性低导致缺乏场景理解；刚性专家行为导致可利用的捷径；以及对固定专家轨迹集的过度拟合。&lt;h4&gt;结论&lt;/h4&gt;基于研究发现，作者主张向数据中心开发转变，重点是构建更丰富、更健壮、偏见更少的数据集，以提高自动驾驶模型的鲁棒性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;大多数最新的自动驾驶研究优先考虑基准性能和方法创新，而不是对模型失败、偏见和捷径学习的深入分析。这导致了在没有深入理解当前失败情况下的渐进式改进。虽然查看模型失效的情况很简单，但要理解根本原因却很困难。这促使我们进行系统性研究，通过扰动模型输入并观察预测结果。我们引入了PlanT 2.0，一个轻量级的、面向对象的规划transformer，专为CARLA中的自动驾驶研究而设计。对象级表示使受控分析成为可能，因为输入可以轻松扰动（例如，通过改变位置或添加或删除某些对象），这与基于传感器的模型形成对比。为了应对CARLA Leaderboard 2.0引入的具有挑战性的新场景，我们对PlanT进行了多项升级，在Longest6 v2、Bench2Drive和CARLA验证路线上实现了最先进的性能。我们的分析揭示了有价值的失败案例，如由于障碍物多样性低导致的场景理解不足，刚性专家行为导致的可利用捷径，以及对固定专家轨迹集的过度拟合。基于这些发现，我们主张向数据中心开发转变，重点是构建更丰富、更健壮、偏见更少的数据集。我们在https://github.com/autonomousvision/plant2开源了我们的代码和模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶领域过分关注基准测试性能和方法创新，而对模型失败、偏见和捷径学习缺乏深入分析的问题。这个问题很重要，因为它导致虽然性能有所提升，但对失败的根本原因缺乏理解，可能使自动驾驶系统在实际应用中出现不可预测的故障，同时模型可能存在未被发现的偏见和捷径学习，在真实世界中可能导致严重安全问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到自动驾驶领域缺乏对模型失败原因的深入分析，并指出传感器端到端模型难以进行受控分析。因此选择重新审视PlanT模型，因为它具有物体中心表示，便于进行受控分析。作者借鉴了原始PlanT模型、PDM-Lite专家策略，以及[33]在输出表示上的方法，使用空间等距路径点进行横向规划和传统时间航点进行纵向规划。同时比较了多种航点生成方法，最终选择了简单线性层方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用物体中心表示而非传感器输入，便于进行受控分析和扰动；通过系统性地扰动模型输入并观察预测结果，来揭示模型的偏见、结构缺陷和捷径学习；设计一个轻量级的物体中心规划器，能够高效训练和评估，同时支持深入分析。整体实现流程包括：扩展输入表示（增加五个新物体类别、添加道路布局信息、增加检测范围）；升级输出表示（使用空间等距路径点进行横向规划，使用传统时间航点进行纵向规划，改进航点生成方法）；在多个基准测试上评估性能；通过系统扰动输入分析模型行为并识别各种失败模式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出PlanT 2.0轻量级物体中心规划器，专为CARLA自动驾驶研究设计；2) 设计新的物体表示和输出表示，适应CARLA Leaderboard 2.0挑战；3) 实现受控分析框架，能系统扰动输入并观察模型行为；4) 进行深入模型分析，揭示8种关键问题（如缺乏环境理解、轨迹泛化能力差等）；5) 指出当前数据集局限性并提出改进方向。相比之前的工作，PlanT 2.0在多个基准测试上达到最先进性能，同时提供了更深入的分析；相比端到端传感器模型，其物体中心表示便于受控分析；相比其他规划器，它更注重揭示模型偏见和结构缺陷而非仅追求性能指标。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了PlanT 2.0，一个在CARLA自动驾驶基准测试上实现最先进性能的轻量级物体中心规划器，并通过系统性的输入扰动分析揭示了自动驾驶模型中的关键偏见和结构缺陷，为未来更鲁棒、更少偏见的自动驾驶系统开发提供了重要见解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most recent work in autonomous driving has prioritized benchmark performanceand methodological innovation over in-depth analysis of model failures, biases,and shortcut learning. This has led to incremental improvements without a deepunderstanding of the current failures. While it is straightforward to look atsituations where the model fails, it is hard to understand the underlyingreason. This motivates us to conduct a systematic study, where inputs to themodel are perturbed and the predictions observed. We introduce PlanT 2.0, alightweight, object-centric planning transformer designed for autonomousdriving research in CARLA. The object-level representation enables controlledanalysis, as the input can be easily perturbed (e.g., by changing the locationor adding or removing certain objects), in contrast to sensor-based models. Totackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0,we introduce multiple upgrades to PlanT, achieving state-of-the-art performanceon Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysisexposes insightful failures, such as a lack of scene understanding caused bylow obstacle diversity, rigid expert behaviors leading to exploitableshortcuts, and overfitting to a fixed set of expert trajectories. Based onthese findings, we argue for a shift toward data-centric development, with afocus on richer, more robust, and less biased datasets. We open-source our codeand model at https://github.com/autonomousvision/plant2.</description>
      <author>example@mail.com (Simon Gerstenecker, Andreas Geiger, Katrin Renz)</author>
      <guid isPermaLink="false">2511.07292v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Omni-View: Unlocking How Generation Facilitates Understanding in Unified 3D Model based on Multiview images</title>
      <link>http://arxiv.org/abs/2511.07222v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Omni-View系统，基于多视图图像将统一的多模态理解和生成扩展到3D场景，探索'生成促进理解'的原理。该系统由理解模型、纹理模块和几何模块组成，能够联合建模场景理解、新视图合成和几何估计，实现3D场景理解和生成任务之间的协同交互。&lt;h4&gt;背景&lt;/h4&gt;基于多视图图像的3D场景理解和生成是计算机视觉领域的重要研究方向，但现有方法在整合理解和生成任务方面存在局限。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时处理3D场景理解、新视图合成和几何估计的系统，并通过'生成促进理解'的原理提升整体性能。&lt;h4&gt;方法&lt;/h4&gt;Omni-View系统由理解模型、纹理模块和几何模块组成。纹理模块负责外观合成，利用其空间时间建模能力；几何模块提供显式几何约束。系统采用两阶段训练策略，实现3D场景理解和生成任务的协同交互。&lt;h4&gt;主要发现&lt;/h4&gt;在VSI-Bench基准测试上，Omni-View达到55.4的最先进分数，超越现有的专用3D理解模型，同时在新型视图合成和3D场景生成方面也表现出色。&lt;h4&gt;结论&lt;/h4&gt;Omni-View通过整合理解和生成任务，实现了3D场景处理的协同优化，证明了'生成促进理解'原理的有效性，为3D场景理解和生成提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了Omni-View，它基于多视图图像将统一的多模态理解和生成扩展到3D场景，探索'生成促进理解'的原理。Omni-View由理解模型、纹理模块和几何模块组成，联合建模场景理解、新视图合成和几何估计，实现3D场景理解和生成任务之间的协同交互。通过设计，它利用了负责外观合成的纹理模块的空间时间建模能力，以及专用几何模块提供的显式几何约束，从而丰富了模型对3D场景的整体理解。采用两阶段策略训练后，Omni-View在VSI-Bench基准测试上达到55.4的最先进分数，超越现有的专用3D理解模型，同时在新型视图合成和3D场景生成方面也表现出色。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文旨在解决如何构建一个统一的3D场景理解和生成模型的问题。这个问题很重要，因为3D场景理解是人工智能领域的关键挑战，对机器人导航、自动驾驶和增强现实等应用至关重要。现有的方法通常专注于2D图像或需要明确的3D输入，限制了实际应用场景。同时，探索生成任务如何促进理解能力，对于构建更智能的通用人工智能系统具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于'生成促进理解'的原则进行思考，认为几何估计和新视角合成等生成任务具有内在的几何和时空建模能力，可以增强3D场景理解。他们借鉴了Bagel框架的共享多模态自注意力机制，并在此基础上创新性地将生成模型分解为纹理模块和几何模块两个专门组件。训练上采用两阶段策略：第一阶段同时训练所有模块以增强理解能力，第二阶段冻结理解模型优化生成性能。此外，还借鉴了视频扩散模型、点云表示、SigLIP视觉编码器等技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是'生成促进理解'，即通过几何估计和新视角合成等生成任务来增强3D场景理解能力。整体实现流程包括：1) 架构上分为理解模型、纹理模块和几何模块；2) 两阶段训练策略，第一阶段同时训练所有模块并采用密集到稀疏(D2S)的渐进式训练方法，第二阶段冻结理解模型优化生成性能；3) 推理时，理解模型处理多视角图像执行理解任务，生成模型接收参考图像、相机姿态和提示生成新视角和几何信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次探索'生成促进理解'的3D统一模型；2) 创新性地将生成模型分离为纹理模块和几何模块；3) 提出两阶段训练策略和密集到稀疏(D2S)的渐进式训练方法；4) 利用几何约束增强时空建模能力。相比之前的工作，不同之处在于：不需要明确的3D输入即可实现高性能3D理解；专注于3D场景而非2D图像；同时实现高质量的场景理解和生成；通过几何约束提高生成的一致性和质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Omni-View通过'生成促进理解'的创新理念，首次实现了仅基于多视角图像的高性能统一3D场景理解和生成，为人工智能在3D空间感知和创造能力的发展奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents Omni-View, which extends the unified multimodalunderstanding and generation to 3D scenes based on multiview images, exploringthe principle that "generation facilitates understanding". Consisting ofunderstanding model, texture module, and geometry module, Omni-View jointlymodels scene understanding, novel view synthesis, and geometry estimation,enabling synergistic interaction between 3D scene understanding and generationtasks. By design, it leverages the spatiotemporal modeling capabilities of itstexture module responsible for appearance synthesis, alongside the explicitgeometric constraints provided by its dedicated geometry module, therebyenriching the model's holistic understanding of 3D scenes. Trained with atwo-stage strategy, Omni-View achieves a state-of-the-art score of 55.4 on theVSI-Bench benchmark, outperforming existing specialized 3D understandingmodels, while simultaneously delivering strong performance in both novel viewsynthesis and 3D scene generation.</description>
      <author>example@mail.com (JiaKui Hu, Shanshan Zhao, Qing-Guo Chen, Xuerui Qiu, Jialun Liu, Zhao Xu, Weihua Luo, Kaifu Zhang, Yanye Lu)</author>
      <guid isPermaLink="false">2511.07222v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>TrueCity: Real and Simulated Urban Data for Cross-Domain 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2511.07007v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper accepted for 3DV 2026 (International Conference on 3D  Vision 2026)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了TrueCity数据集，这是首个包含真实世界和模拟点云的城市语义分割基准，用于解决3D语义场景理解中的合成到真实域差距问题。&lt;h4&gt;背景&lt;/h4&gt;3D语义场景理解是3D计算机视觉领域的长期挑战，关键问题是有标注的真实世界数据有限，难以促进可泛化模型的发展。常见做法是模拟新数据，但合成数据虽然具有可扩展性和完美标签，却无法捕捉真实世界的复杂性和传感器噪声，导致合成到真实域的差距。&lt;h4&gt;目的&lt;/h4&gt;引入TrueCity数据集，这是第一个具有厘米级精确标注的真实世界点云、语义3D城市模型和表示同一城市的标注模拟点云的城市语义分割基准，并提出与国际3D城市建模标准一致的分割类别，以便对合成到真实差距进行一致的评估。&lt;h4&gt;方法&lt;/h4&gt;开发TrueCity基准数据集，并在常见基线上进行广泛实验，量化域差距并强调利用合成数据增强真实世界3D场景理解的策略。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验量化了域差距，并确定了利用合成数据增强真实世界3D场景理解的有效策略。&lt;h4&gt;结论&lt;/h4&gt;TrueCity数据集将促进合成到真实差距量化的进一步发展，并能够实现可泛化的数据驱动模型。&lt;h4&gt;翻译&lt;/h4&gt;3D语义场景理解仍然是3D计算机视觉界的一个长期挑战。关键问题之一是有标注的真实世界数据有限，难以促进可泛化模型的发展。解决这个问题的常见做法是模拟新数据。尽管合成数据集具有可扩展性和完美标签，但其设计者精心设计的场景无法捕捉真实世界的复杂性和传感器噪声，导致合成到真实域的差距。此外，没有基准提供用于面向分割的域偏移分析的真实和模拟点云的同步数据。我们引入了TrueCity，这是第一个具有厘米级精确标注的真实世界点云、语义3D城市模型和表示同一城市的标注模拟点云的城市语义分割基准。TrueCity提出了与国际3D城市建模标准一致的分割类别，使合成到真实差距的评估保持一致。我们在常见基线上进行的广泛实验量化了域偏移，并强调了利用合成数据增强真实世界3D场景理解的策略。我们相信TrueCity数据集将促进合成到真实差距量化的进一步发展，并实现可泛化的数据驱动模型。数据、代码和3D模型可在网上获取：https://tum-gis.github.io/TrueCity/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D语义场景理解中高质量标注数据不足以及模拟数据与真实数据之间存在'域差距'的问题。这个问题很重要，因为真实世界的高质量3D数据稀缺限制了可泛化模型的发展，而模拟数据无法完全捕捉真实世界的复杂性和传感器噪声，导致模型在实际应用中表现不佳。城市场景由于扫描距离变化、物体材料多样性和动态事件等因素尤其具有挑战性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出3D语义分割的两个主要障碍：标注数据稀缺和模拟-真实域差距。他们注意到现有数据集类别定义不一致，难以进行统一比较，因此决定与国际标准（CityGML 2.0和OpenDRIVE 1.4）保持一致。作者借鉴了现有工作：利用国际标准定义语义类别，使用CARLA模拟器进行激光扫描模拟，采用现有点云标注流程（如连通分量分析和布料模拟过滤算法），并使用多种点云分割方法作为基线模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个同步的真实和模拟数据集，使研究者能够精确量化分析模拟-真实域差距，并通过设计与国际标准兼容的语义类别确保数据集与现有工作流程的兼容性。整体流程包括：1)在德国Ingolstadt市中心采集高精度真实点云；2)基于真实点云构建符合CityGML标准的语义3D城市模型；3)使用CARLA模拟器生成模拟点云；4)对点云进行标注和分类；5)在不同比例的混合数据上训练和评估多种分割模型，分析域差距影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)TrueCity数据集——首个包含同步真实和模拟点云的城市语义分割基准，提供厘米级精度标注；2)标准化语义类别——提出与国际标准对齐的12个城市场景类别；3)系统化域差距分析——通过不同比例混合训练揭示各类别对域差距的敏感性。相比之前工作，TrueCity的真实和模拟数据针对同一地理位置，提供全面的城市场景类别而非仅关注特定建筑类型，解决了类别定义不一致问题，并提供了更全面的域差距分析。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TrueCity数据集通过提供同步的真实和模拟城市点云以及与国际标准兼容的语义标注，使研究者能够精确量化和分析模拟-真实域差距，从而促进更鲁棒的3D城市场景理解模型的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D semantic scene understanding remains a long-standing challenge in the 3Dcomputer vision community. One of the key issues pertains to limited real-worldannotated data to facilitate generalizable models. The common practice totackle this issue is to simulate new data. Although synthetic datasets offerscalability and perfect labels, their designer-crafted scenes fail to capturereal-world complexity and sensor noise, resulting in a synthetic-to-real domaingap. Moreover, no benchmark provides synchronized real and simulated pointclouds for segmentation-oriented domain shift analysis. We introduce TrueCity,the first urban semantic segmentation benchmark with cm-accurate annotatedreal-world point clouds, semantic 3D city models, and annotated simulated pointclouds representing the same city. TrueCity proposes segmentation classesaligned with international 3D city modeling standards, enabling consistentevaluation of synthetic-to-real gap. Our extensive experiments on commonbaselines quantify domain shift and highlight strategies for exploitingsynthetic data to enhance real-world 3D scene understanding. We are convincedthat the TrueCity dataset will foster further development of sim-to-real gapquantification and enable generalizable data-driven models. The data, code, and3D models are available online: https://tum-gis.github.io/TrueCity/</description>
      <author>example@mail.com (Duc Nguyen, Yan-Ling Lai, Qilin Zhang, Prabin Gyawali, Benedikt Schwab, Olaf Wysocki, Thomas H. Kolbe)</author>
      <guid isPermaLink="false">2511.07007v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Video Dataset for Surgical Phase, Keypoint, and Instrument Recognition in Laparoscopic Surgery (PhaKIR)</title>
      <link>http://arxiv.org/abs/2511.06549v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Surgical Procedure Phase, Keypoint, and Instrument Recognition (PhaKIR)数据集，这是一个包含三个医疗中心八例完整腹腔镜胆囊切除手术视频的多机构数据集，提供手术阶段识别、器械关键点估计和器械实例分割三个任务的帧级标注。&lt;h4&gt;背景&lt;/h4&gt;机器人辅助微创手术(RAMIS)越来越多地依赖计算机视觉方法进行器械识别和手术流程理解，但现有数据集往往存在孤立任务、忽视时间依赖或缺乏多中心变异等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够满足计算机视觉在手术中应用需求的多机构数据集，提供全面的标注信息以支持手术场景理解研究。&lt;h4&gt;方法&lt;/h4&gt;收集三个医疗中心记录的八例完整腹腔镜胆囊切除手术视频，并提供三个相互关联任务的帧级标注：手术阶段识别(485,875帧)、器械关键点估计(19,435帧)和器械实例分割(19,435帧)。&lt;h4&gt;主要发现&lt;/h4&gt;PhaKIR据作者所知是第一个提供阶段标签、器械姿态信息和像素级精确器械分割的多机构数据集，同时能够利用时间上下文，因为提供了完整的手术流程序列。&lt;h4&gt;结论&lt;/h4&gt;该数据集作为PhaKIR挑战赛的基础，在MICCAI 2024内镜视觉挑战赛中被用作基准，验证了数据集的质量和相关价值，现已通过Zenodo平台公开可用。&lt;h4&gt;翻译&lt;/h4&gt;机器人辅助和计算机辅助微创手术(RAMIS)越来越多地依赖计算机视觉方法进行可靠的器械识别和手术流程理解。开发此类系统通常需要大量、标注良好的数据集，但现有资源往往只处理孤立任务，忽视时间依赖性，或缺乏多中心变异。我们提出了手术阶段、关键点和器械识别(PhaKIR)数据集，包含三个医疗中心记录的八例完整腹腔镜胆囊切除手术视频。该数据集提供了三个相互关联任务的帧级标注：手术阶段识别(485,875帧)、器械关键点估计(19,435帧)和器械实例分割(19,435帧)。据我们所知，PhaKIR是第一个联合提供阶段标签、器械姿态信息和像素级精确器械分割的多机构数据集，同时由于完整手术流程序列的可用性，能够利用时间上下文。它作为PhaKIR挑战赛的基础，成为MICCAI 2024内镜视觉(EndoVis)挑战赛的一部分，用于评估手术场景理解方法，从而进一步验证了数据集的质量和相关价值。该数据集可通过Zenodo平台公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic- and computer-assisted minimally invasive surgery (RAMIS) isincreasingly relying on computer vision methods for reliable instrumentrecognition and surgical workflow understanding. Developing such systems oftenrequires large, well-annotated datasets, but existing resources often addressisolated tasks, neglect temporal dependencies, or lack multi-centervariability. We present the Surgical Procedure Phase, Keypoint, and InstrumentRecognition (PhaKIR) dataset, comprising eight complete laparoscopiccholecystectomy videos recorded at three medical centers. The dataset providesframe-level annotations for three interconnected tasks: surgical phaserecognition (485,875 frames), instrument keypoint estimation (19,435 frames),and instrument instance segmentation (19,435 frames). PhaKIR is, to ourknowledge, the first multi-institutional dataset to jointly provide phaselabels, instrument pose information, and pixel-accurate instrumentsegmentations, while also enabling the exploitation of temporal context sincefull surgical procedure sequences are available. It served as the basis for thePhaKIR Challenge as part of the Endoscopic Vision (EndoVis) Challenge at MICCAI2024 to benchmark methods in surgical scene understanding, thereby furthervalidating the dataset's quality and relevance. The dataset is publiclyavailable upon request via the Zenodo platform.</description>
      <author>example@mail.com (Tobias Rueckert, Raphaela Maerkl, David Rauber, Leonard Klausmann, Max Gutbrod, Daniel Rueckert, Hubertus Feussner, Dirk Wilhelm, Christoph Palm)</author>
      <guid isPermaLink="false">2511.06549v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>TimeSense:Making Large Language Models Proficient in Time-Series Analysis</title>
      <link>http://arxiv.org/abs/2511.06344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TimeSense是一个多模态框架，通过平衡文本推理和保留时间感知能力，使大型语言模型能够精通时间序列分析，在多个任务上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;在时间序列领域，越来越多的研究将文本与时间数据结合，利用大型语言模型(LLMs)的推理能力来进行各种下游时间序列理解任务，使单个模型能够灵活执行以前需要每个领域专用模型才能完成的任务。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在训练期间依赖文本标签进行监督所导致的模型偏向文本线索而忽略完整时间特征的问题，防止输出与底层时间序列上下文相矛盾。&lt;h4&gt;方法&lt;/h4&gt;构建EvalTS基准(包含10个跨越三个难度级别的任务)来评估模型；提出TimeSense多模态框架，包含时间感知模块以在模型上下文中重建输入时间序列，并集成基于坐标的位置嵌入来增强时间序列数据的空间理解。&lt;h4&gt;主要发现&lt;/h4&gt;TimeSense在多个任务上实现了最先进的性能，特别是在复杂的多维时间序列推理任务上优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;通过平衡文本推理和保留时间感知能力，TimeSense使大型语言模型能够更有效地进行时间序列分析，解决了现有方法中的文本偏见问题。&lt;h4&gt;翻译&lt;/h4&gt;在时间序列领域，越来越多的工作将文本与时间数据相结合，利用大型语言模型(LLMs)的推理能力来完成各种下游时间序列理解任务。这使得单个模型能够灵活执行以前需要为每个领域开发专用模型才能完成的任务。然而，这些方法通常在训练期间依赖文本标签进行监督，使模型偏向于文本线索，同时可能忽略完整的时间特征。这种偏差可能导致输出与底层时间序列上下文相矛盾。为解决这一问题，我们构建了EvalTS基准，包含跨越三个难度级别的10个任务，从基本的时间模式识别到复杂的现实世界推理，用于在更具挑战性和现实性的场景下评估模型。我们还提出了TimeSense，一个多模态框架，通过平衡文本推理和保留时间感知能力，使LLM精通时间序列分析。TimeSense包含一个时间感知模块，在模型上下文中重建输入时间序列，确保文本推理基于时间序列动态。此外，为了增强对时间序列数据的空间理解，我们明确集成了基于坐标的位置嵌入，为每个时间点提供空间上下文，使模型能够更有效地捕获结构依赖关系。实验结果表明，TimeSense在多个任务上实现了最先进的性能，特别是在复杂的多维时间序列推理任务上优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the time-series domain, an increasing number of works combine text withtemporal data to leverage the reasoning capabilities of large language models(LLMs) for various downstream time-series understanding tasks. This enables asingle model to flexibly perform tasks that previously required specializedmodels for each domain. However, these methods typically rely on text labelsfor supervision during training, biasing the model toward textual cues whilepotentially neglecting the full temporal features. Such a bias can lead tooutputs that contradict the underlying time-series context. To address thisissue, we construct the EvalTS benchmark, comprising 10 tasks across threedifficulty levels, from fundamental temporal pattern recognition to complexreal-world reasoning, to evaluate models under more challenging and realisticscenarios. We also propose TimeSense, a multimodal framework that makes LLMsproficient in time-series analysis by balancing textual reasoning with apreserved temporal sense. TimeSense incorporates a Temporal Sense module thatreconstructs the input time-series within the model's context, ensuring thattextual reasoning is grounded in the time-series dynamics. Moreover, to enhancespatial understanding of time-series data, we explicitly incorporatecoordinate-based positional embeddings, which provide each time point withspatial context and enable the model to capture structural dependencies moreeffectively. Experimental results demonstrate that TimeSense achievesstate-of-the-art performance across multiple tasks, and it particularlyoutperforms existing methods on complex multi-dimensional time-series reasoningtasks.</description>
      <author>example@mail.com (Zhirui Zhang, Changhua Pei, Tianyi Gao, Zhe Xie, Yibo Hao, Zhaoyang Yu, Longlong Xu, Tong Xiao, Jing Han, Dan Pei)</author>
      <guid isPermaLink="false">2511.06344v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>10 Open Challenges Steering the Future of Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2511.05936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AAAI 2026 (Senior Track)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文讨论了视觉-语言-动作(VLA)模型在具身人工智能领域的发展，分析了10个主要里程碑和新兴趋势，旨在加速VLA模型的更广泛接受。&lt;h4&gt;背景&lt;/h4&gt;视觉-语言-动作模型因其遵循自然语言指令的能力，在具身人工智能领域日益普及，这是继大语言模型和视觉语言模型成功之后的自然发展。&lt;h4&gt;目的&lt;/h4&gt;讨论VLA模型发展的10个主要里程碑，探讨新兴趋势，并引起对可能加速VLA模型更广泛接受的研究途径的关注。&lt;h4&gt;方法&lt;/h4&gt;通过分析VLA模型发展的关键里程碑和新兴趋势，系统梳理该领域的研究现状和未来方向。&lt;h4&gt;主要发现&lt;/h4&gt;VLA模型发展的10个主要里程碑包括：多模态能力、推理能力、数据优化、评估方法、跨机器人动作泛化、效率提升、全身协调、安全性、智能体设计以及与人类协调能力。新兴趋势包括：空间理解、建模世界动态、训练后优化和数据合成。&lt;h4&gt;结论&lt;/h4&gt;通过系统讨论VLA模型的发展里程碑和新兴趋势，可以促进相关研究，加速VLA模型的更广泛接受和应用。&lt;h4&gt;翻译&lt;/h4&gt;由于遵循自然语言指令的能力，视觉-语言-动作(VLA)模型在具身人工智能领域日益普及，这继其前身——大语言模型和视觉语言模型——的广泛成功之后。在本文中，我们讨论了VLA模型持续发展中的10个主要里程碑——多模态、推理、数据、评估、跨机器人动作泛化、效率、全身协调、安全、智能体以及与人类的协调。此外，我们还探讨了使用空间理解、建模世界动态、训练后优化和数据合成等新兴趋势——所有这些都旨在达到这些里程碑。通过这些讨论，我们希望引起对可能加速VLA模型更广泛接受的研究途径的关注。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉-语言-行动(VLA)模型面临的十大开放性挑战。这些问题在现实中非常重要，因为VLA模型是实现真正智能机器人的关键，能让机器人理解自然语言指令并在复杂环境中执行任务，推动具身AI从实验室走向实际应用，如家庭服务、灾难救援和工业制造等领域。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析当前VLA模型研究现状，识别出10个主要挑战，并针对每个挑战提出可能的解决方向。他们借鉴了多个现有工作，包括大型语言模型(LLMs)、视觉语言模型(VLMs)、机器人控制、强化学习和模仿学习等领域的成果，还整合了分层规划、世界建模和预测等理论框架，这些方法都是基于现有研究的延伸和创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合视觉感知、语言理解和行动执行，使机器人能够理解自然语言指令并在复杂环境中执行任务。整体流程包括：1)接收多模态输入(视觉、语言、上下文)；2)通过视觉语言模型理解场景和任务，进行高层次规划；3)生成具体机器人动作序列；4)执行动作并接收环境反馈；5)根据反馈调整后续动作。具体实现分为离散动作模型和连续动作模型两种路径，还提出了分层规划框架和推理前行动等高级方法。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：系统性挑战识别、多模态感知扩展、分层规划框架、空间理解增强、通用动作表示、世界动态建模、数据合成方法和安全与保障机制。相比之前工作，这篇论文提供了全面的视角，强调从感知到行动的完整闭环，重视实际应用中的挑战，提出系统性的解决方案框架，并关注多智能体协作和人机交互等更高级的协作模式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文系统性地识别并探讨了视觉-语言-行动模型面临的十大核心挑战，并提出了一系列创新方法框架，为构建更强大、更实用、更安全的具身AI系统提供了全面的研究路线图。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to their ability of follow natural language instructions,vision-language-action (VLA) models are increasingly prevalent in the embodiedAI arena, following the widespread success of their precursors -- LLMs andVLMs. In this paper, we discuss 10 principal milestones in the ongoingdevelopment of VLA models -- multimodality, reasoning, data, evaluation,cross-robot action generalization, efficiency, whole-body coordination, safety,agents, and coordination with humans. Furthermore, we discuss the emergingtrends of using spatial understanding, modeling world dynamics, post training,and data synthesis -- all aiming to reach these milestones. Through thesediscussions, we hope to bring attention to the research avenues that mayaccelerate the development of VLA models into wider acceptability.</description>
      <author>example@mail.com (Soujanya Poria, Navonil Majumder, Chia-Yu Hung, Amir Ali Bagherzadeh, Chuan Li, Kenneth Kwok, Ziwei Wang, Cheston Tan, Jiajun Wu, David Hsu)</author>
      <guid isPermaLink="false">2511.05936v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Open-World 3D Scene Graph Generation for Retrieval-Augmented Reasoning</title>
      <link>http://arxiv.org/abs/2511.05894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于开放世界3D场景图生成的统一框架，结合了视觉语言模型和检索增强推理技术，实现了可泛化和交互式的3D场景理解。该方法包含动态场景图生成模块和检索增强推理管道两个关键组件，在多个基准测试和任务中表现出强大的泛化能力和优越性能。&lt;h4&gt;背景&lt;/h4&gt;在开放世界环境中理解3D场景对视觉和机器人技术构成了根本性挑战，主要受限于封闭词汇监督和静态标注的局限性。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一框架，解决开放世界3D场景理解中的挑战，实现可泛化和交互式的3D场景理解。&lt;h4&gt;方法&lt;/h4&gt;提出了一种带有检索增强推理的开放世界3D场景图生成统一框架，整合视觉语言模型与基于检索的推理，支持多模态探索和语言引导交互。方法包含两个关键组件：(1)动态场景图生成模块，无需固定标签集即可检测对象并推断语义关系；(2)检索增强推理管道，将场景图编码为向量数据库以支持文本/图像条件查询。&lt;h4&gt;主要发现&lt;/h4&gt;在3DSSG和Replica基准上对四个任务（场景问答、视觉定位、实例检索和任务规划）进行的评估中，展示了该方法强大的泛化能力和在不同环境中的优越性能。&lt;h4&gt;结论&lt;/h4&gt;结合开放词汇感知与基于检索的推理对于可扩展的3D场景理解是有效的，研究结果突显了这种组合方法的价值。&lt;h4&gt;翻译&lt;/h4&gt;在开放世界环境中理解3D场景对视觉和机器人技术构成了根本性挑战，主要受限于封闭词汇监督和静态标注的局限性。为解决这一问题，我们提出了一个带有检索增强推理的开放世界3D场景图生成统一框架，实现了可泛化和交互式的3D场景理解。我们的方法将视觉语言模型与基于检索的推理相结合，支持多模态探索和语言引导交互。该框架包含两个关键组件：(1)动态场景图生成模块，无需固定标签集即可检测对象并推断语义关系；(2)检索增强推理管道，将场景图编码为向量数据库以支持文本/图像条件查询。我们在3DSSG和Replica基准上对四个任务（场景问答、视觉定位、实例检索和任务规划）评估了我们的方法，展示了强大的泛化能力和在不同环境中的优越性能。我们的研究结果突显了结合开放词汇感知与基于检索的推理对于可扩展3D场景理解的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决开放世界3D场景理解的问题，即如何让计算机系统在动态、非结构化环境中识别未知物体并理解它们之间的关系。这个问题在现实中非常重要，因为自主导航、增强现实等应用需要系统适应新环境，而传统方法依赖于预定义的物体类别和固定场景，无法处理现实世界中经常出现的新物体和场景变化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统3D场景理解方法的局限性，特别是它们对封闭词汇表和静态注释的依赖。然后，作者借鉴了视觉-语言模型(VLMs)在2D任务中的成功经验，结合检索机制来处理开放世界场景。设计过程中，作者参考了多模态大语言模型(MLLMs)的架构，包括大语言模型骨干、视觉编码器和适配器模块，以及参数高效微调方法。同时，作者也基于3D场景图生成的前期工作进行了改进，消除了对人工注释或固定姿态RGB-D数据的依赖，实现了完全无注释的3D场景图生成。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将视觉-语言模型与检索机制相结合，实现开放世界3D场景理解和推理。通过动态场景图构建环境表示，支持对未见物体的识别和关系推断，并使用检索增强推理来支持多模态交互。整体流程分为三部分：1)开放世界3D场景图生成，包括多帧目标检测、最佳视角选择、物体过滤和语义关系提取；2)检索增强语义推理，将场景图编码为向量数据库，处理用户查询并进行大语言模型推理；3)场景驱动的多模态交互，支持文本问答、视觉定位、实例检索和任务规划四种任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的开放世界3D场景图生成框架，无需固定注释；2)检索增强推理模块，将场景图编码为支持查询的向量数据库；3)多模态交互能力，支持四种场景交互任务。相比之前工作，不同之处在于：1)相比封闭词汇方法，能识别未知物体且不依赖预定义标签；2)相比开放词汇方法，无需人工注释或固定姿态RGB-D数据，泛化能力更强；3)相比多模态大语言模型，专门针对3D场景设计，结合场景图结构和检索推理，在空间理解和任务规划方面表现更好。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合视觉-语言模型和检索增强推理的开放世界3D场景图生成框架，实现了无需人工注释的动态场景理解和多模态交互能力，显著提升了系统在复杂环境中的泛化性和实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding 3D scenes in open-world settings poses fundamental challengesfor vision and robotics, particularly due to the limitations ofclosed-vocabulary supervision and static annotations. To address this, wepropose a unified framework for Open-World 3D Scene Graph Generation withRetrieval-Augmented Reasoning, which enables generalizable and interactive 3Dscene understanding. Our method integrates Vision-Language Models (VLMs) withretrieval-based reasoning to support multimodal exploration and language-guidedinteraction. The framework comprises two key components: (1) a dynamic scenegraph generation module that detects objects and infers semantic relationshipswithout fixed label sets, and (2) a retrieval-augmented reasoning pipeline thatencodes scene graphs into a vector database to support text/image-conditionedqueries. We evaluate our method on 3DSSG and Replica benchmarks across fourtasks-scene question answering, visual grounding, instance retrieval, and taskplanning-demonstrating robust generalization and superior performance indiverse environments. Our results highlight the effectiveness of combiningopen-vocabulary perception with retrieval-based reasoning for scalable 3D sceneunderstanding.</description>
      <author>example@mail.com (Fei Yu, Quan Deng, Shengeng Tang, Yuehua Li, Lechao Cheng)</author>
      <guid isPermaLink="false">2511.05894v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Lite VLA: Efficient Vision-Language-Action Control on CPU-Bound Edge Robots</title>
      <link>http://arxiv.org/abs/2511.05642v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究展示了在移动机器人上部署小型视觉语言模型以实现实时场景理解和推理的可行性，使机器人能够在动态环境中同时进行移动和推理，无需云连接支持。&lt;h4&gt;背景&lt;/h4&gt;在GPS受限环境中运行的自主机器人，其人工智能模型在边缘的部署日益重要，因为本地、资源高效的推理是必不可少的。&lt;h4&gt;目的&lt;/h4&gt;证明在移动机器人上部署小型视觉语言模型（VLM）的可行性，以在严格的计算约束下实现实时场景理解和推理。&lt;h4&gt;方法&lt;/h4&gt;提出一种框架，将紧凑的VLM与多模态感知集成，直接在嵌入式硬件上执行上下文解释，消除对云连接的依赖，使机器人在动态环境中仅使用板载硬件就能同时进行移动和推理。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证了计算效率、任务准确性和系统响应能力之间的平衡，在移动机器人上的实现确认了小型VLM在边缘进行并发推理和移动的首次成功部署之一。&lt;h4&gt;结论&lt;/h4&gt;这项工作为服务机器人、灾难响应和国防行动等应用中的可扩展、有保障的自主性奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;在GPS受限环境中运行的自主机器人，其人工智能模型在边缘的部署日益重要，因为本地、资源高效的推理是必不可少的。这项工作证明了在移动机器人上部署小型视觉语言模型（VLM）的可行性，以在严格的计算约束下实现实时场景理解和推理。与先前分离感知和移动的方法不同，所提出的框架使机器人在动态环境中仅使用板载硬件就能同时进行移动和推理。该系统将紧凑的VLM与多模态感知集成，直接在嵌入式硬件上执行上下文解释，消除对云连接的依赖。实验验证突显了计算效率、任务准确性和系统响应能力之间的平衡。在移动机器人上的实现确认了小型VLM在边缘进行并发推理和移动的首次成功部署之一。这项工作为服务机器人、灾难响应和国防行动等应用中的可扩展、有保障的自主性奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在资源受限的边缘设备上实现高效的视觉-语言-动作控制问题。这个问题在现实中非常重要，因为在GPS受限环境（如灾难区域、地下设施或国防任务）中运行的机器人需要完全自包含的智能，能够本地推理和行动而不依赖外部基础设施。现有的大型多模态模型严重依赖云端计算，在连接、电源和计算能力有限的边缘场景中不实用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到大型模型在边缘设备上的局限性，然后考虑将感知、规划和控制统一在单一推理循环中。他们借鉴了多项现有工作：SmolVLA作为基础架构，LoRA用于参数高效微调，QLoRA结合4位量化技术，SmolVLM作为基础多模态架构，llama-cpp运行时用于CPU推理，以及ROS 2用于机器人控制。设计方法包括使用小型多模态变换器、LoRA微调、4位量化和混合精度设计，最终集成到ROS 2环境中实现端到端控制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在资源受限的边缘设备上通过模型小型化、量化和参数高效微调实现高效的视觉-语言-动作控制，使机器人能够在本地进行实时推理而不依赖云端。整体流程包括：1)通过远程操作收集RGB图像和对应动作并进行时间同步；2)使用LoRA微调预训练的SmolVLM模型；3)应用4位NF4量化和混合精度设计；4)部署到ROS 2环境；5)执行时捕获RGB帧，通过量化模型处理生成语义动作命令，解析并转换为机器人运动命令。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次实现完全基于CPU的视觉-语言-动作推理；2)使用LoRA进行参数高效微调；3)采用4位NF4量化和混合精度设计；4)实现端到端ROS 2管道；5)提出EDGE-VLA-ROADMAP可扩展路线图。相比之前工作，与SmolVLA不同，LiteVLA针对移动机器人且完全基于CPU运行；与其他边缘VLA系统相比，实现了更低的内存占用和更高的推理效率；与大型多模态模型相比，专为边缘设备设计且无需云端支持；技术上采用混合量化策略和llama-cpp运行时。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文成功实现了在资源受限的边缘设备上运行轻量级视觉-语言-动作模型，通过模型量化、参数高效微调和ROS 2集成，使机器人能够在本地进行实时场景理解和推理，为服务机器人、灾难响应和国防应用等场景提供了可扩展的自主解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of artificial intelligence models at the edge is increasinglycritical for autonomous robots operating in GPS-denied environments wherelocal, resource-efficient reasoning is essential. This work demonstrates thefeasibility of deploying small Vision-Language Models (VLMs) on mobile robotsto achieve real-time scene understanding and reasoning under strictcomputational constraints. Unlike prior approaches that separate perceptionfrom mobility, the proposed framework enables simultaneous movement andreasoning in dynamic environments using only on-board hardware. The systemintegrates a compact VLM with multimodal perception to perform contextualinterpretation directly on embedded hardware, eliminating reliance on cloudconnectivity. Experimental validation highlights the balance betweencomputational efficiency, task accuracy, and system responsiveness.Implementation on a mobile robot confirms one of the first successfuldeployments of small VLMs for concurrent reasoning and mobility at the edge.This work establishes a foundation for scalable, assured autonomy inapplications such as service robotics, disaster response, and defenseoperations.</description>
      <author>example@mail.com (Justin Williams, Kishor Datta Gupta, Roy George, Mrinmoy Sarkar)</author>
      <guid isPermaLink="false">2511.05642v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Grounding Foundational Vision Models with 3D Human Poses for Robust Action Recognition</title>
      <link>http://arxiv.org/abs/2511.05622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025 SpaVLE, for code see  https://github.com/nbabey20/groundactrec , 9 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于物理空间的动作识别模型，通过融合V-JEPA2的上下文预测世界动态和CoMotion的抗遮挡人体姿态数据，使具身智能体能够更有效地理解和与周围世界交互。&lt;h4&gt;背景&lt;/h4&gt;当前的动作识别模型主要依赖RGB视频，学习的是模式与动作标签之间的表面关联，难以捕捉复杂场景中潜在的物理交互动态和人体姿态。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够基于物理空间进行动作识别的模型架构，使具身智能体能够更有效地理解和与周围世界交互。&lt;h4&gt;方法&lt;/h4&gt;提出了一种融合两种互补表示的模型架构：V-JEPA2的上下文预测世界动态和CoMotion的明确抗遮挡人体姿态数据，将动作识别建立在物理空间基础上。&lt;h4&gt;主要发现&lt;/h4&gt;模型在InHARD和UCF-19-Y-OCC基准测试上分别验证了在通用动作识别和高遮挡动作识别任务上的性能，优于其他三种基线方法，特别是在复杂、有遮挡的场景中表现更好。&lt;h4&gt;结论&lt;/h4&gt;动作识别需要空间理解的支持，而不仅仅是统计模式识别。&lt;h4&gt;翻译&lt;/h4&gt;为了使具身智能体能够有效地理解和与周围世界互动，他们需要对基于物理空间的人类动作有细致的理解。当前的动作识别模型通常依赖于RGB视频，学习的是模式与动作标签之间的表面关联，因此难以捕捉复杂场景中潜在的物理交互动态和人体姿态。我们提出了一种模型架构，通过融合两种强大且互补的表示将动作识别建立在物理空间基础上：V-JEPA2的上下文预测世界动态和CoMotion的明确抗遮挡人体姿态数据。我们的模型分别在InHARD和UCF-19-Y-OCC基准测试上进行了验证，分别用于通用动作识别和高遮挡动作识别。我们的模型优于其他三种基线方法，特别是在复杂、有遮挡的场景中。我们的研究结果强调，动作识别需要空间理解的支持，而不仅仅是统计模式识别。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For embodied agents to effectively understand and interact within the worldaround them, they require a nuanced comprehension of human actions grounded inphysical space. Current action recognition models, often relying on RGB video,learn superficial correlations between patterns and action labels, so theystruggle to capture underlying physical interaction dynamics and human poses incomplex scenes. We propose a model architecture that grounds action recognitionin physical space by fusing two powerful, complementary representations: V-JEPA2's contextual, predictive world dynamics and CoMotion's explicit,occlusion-tolerant human pose data. Our model is validated on both the InHARDand UCF-19-Y-OCC benchmarks for general action recognition and high-occlusionaction recognition, respectively. Our model outperforms three other baselines,especially within complex, occlusive scenes. Our findings emphasize a need foraction recognition to be supported by spatial understanding instead ofstatistical pattern recognition.</description>
      <author>example@mail.com (Nicholas Babey, Tiffany Gu, Yiheng Li, Cristian Meo, Kevin Zhu)</author>
      <guid isPermaLink="false">2511.05622v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Segmentation of Ischemic Stroke Lesions using Transfer Learning on Multi-sequence MRI</title>
      <link>http://arxiv.org/abs/2511.07281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Ischemic Stroke, Segmentation, Transfer Learning, Magnetic Resonance  Imaging, Deep Learning, Res-UNet&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于Res-Unet架构的新框架，用于在各种MRI序列上快速自动分割缺血性中风病变。通过迁移学习和多数投票分类器的集成，该方法在ISLES 2015数据集上实现了80.5%的Dice分数和74.03%的准确率，展示了其有效性。&lt;h4&gt;背景&lt;/h4&gt;缺血性中风病变的准确理解对中风患者的有效治疗和预后至关重要。磁共振成像（MRI）是诊断中风的常用方法，但专家手动分割病变繁琐、耗时且容易存在观察者不一致性。先前基于手工设计特征的方法无法充分捕捉病变的不规则和生理复杂形状。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速自动分割各种MRI序列上缺血性中风病变的新框架，以克服手动分割的挑战，并提高分割的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出一个新框架用于在T1加权、T2加权、DWI和FLAIR等MRI序列上分割缺血性中风病变。在ISLES 2015脑中风序列数据集上使用Res-Unet架构训练模型，分两次进行：使用预训练权重和不使用预训练权重，以探索迁移学习的好处。计算Dice分数和敏感性等评估指标，并集成多数投票分类器来合并每个轴的结果。&lt;h4&gt;主要发现&lt;/h4&gt;使用迁移学习（预训练权重）的模型表现更好。最终方法实现了80.5%的Dice分数和74.03%的准确率，表明该方法能有效分割缺血性中风病变。&lt;h4&gt;结论&lt;/h4&gt;所提出的分割方法有效，能够准确分割缺血性中风病变，为中风患者的诊断和治疗提供了有价值的工具。&lt;h4&gt;翻译&lt;/h4&gt;缺血性中风病变的准确理解对于中风患者的有效治疗和预后至关重要。磁共振成像（MRI）对急性缺血性中风敏感，是中风诊断的常用方法。然而，专家进行的手动病变分割繁琐、耗时，并且容易存在观察者不一致性。已经提出了自动医学图像分析方法来克服这一挑战。然而，先前的方法依赖于手工设计的特征，这些特征可能无法捕捉缺血性中风病变的不规则和生理复杂形状。在本研究中，我们提出了一种新框架，用于在各种MRI序列上快速自动分割缺血性中风病变，包括T1加权、T2加权、DWI和FLAIR。所提出的方法在ISLES 2015脑中风序列数据集上得到验证，我们使用Res-Unet架构两次训练我们的模型：首先使用预训练权重，然后不使用，以探索迁移学习的好处。计算了包括Dice分数和敏感性在内的评估指标，跨3D体积进行计算。最后，集成了多数投票分类器来合并每个轴的结果，形成了一种全面的分割方法。我们的努力最终实现了80.5%的Dice分数和74.03%的准确率，展示了我们分割方法的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何自动、准确地从多序列MRI图像中分割出缺血性脑卒中病灶的问题。这个问题很重要，因为缺血性脑卒中占所有脑卒中病例的80-85%，而准确理解病灶位置和范围对于有效治疗和预后评估至关重要。治疗时间窗口非常窄（通常只有4.5小时），快速准确的诊断直接影响治疗效果。目前依赖专家手动分割的方法繁琐、耗时且容易受观察者主观因素影响，而自动化方法可以克服传统方法无法捕捉病灶不规则和生理复杂形状的局限。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先指出传统手动分割方法的局限性，然后指出早期自动方法的不足在于依赖手工特征，无法捕捉病灶的复杂形状。接着提出利用深度学习特别是卷积神经网络进行自动分割的潜力，考虑到医学数据的特殊性，提出使用迁移学习。作者选择了Res-UNet架构，结合了U-Net的分割能力和ResNet的特征提取能力。为了克服3D计算成本高的问题，采用多平面2D切片方法，并使用多数投票分类器融合三个平面的预测结果。该方法借鉴了U-Net架构、ResNet特征提取器、迁移学习思想和多数投票分类器等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合迁移学习和多平面信息融合来提高缺血性脑卒中病灶分割的准确性，使用Res-UNet架构，并通过三个不同平面分别训练模型后融合结果。整体流程包括：1)数据准备，将3D MRI图像沿三个平面切分为2D切片；2)模型训练，每个平面训练一个Res-UNet模型，使用混合损失函数解决类别不平衡问题；3)预测阶段，每个模型对输入图像进行预测，生成三个平面的分割掩码；4)融合阶段，使用多数投票分类器结合三个模型的预测，生成最终的3D分割掩码；5)评估，使用Dice系数、准确性等指标评估模型性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多平面信息融合策略，利用三个平面的信息提高分割准确性；2)迁移学习应用，探索了使用预训练权重的优势；3)混合损失函数，解决类别不平衡问题；4)计算效率优化，采用多平面2D切片方法。相比之前的工作，该方法不同于依赖手工特征的传统方法，不同于仅使用单一平面的方法，不同于大多数使用3D卷积的方法，并且明确评估了迁移学习在脑卒中病灶分割任务中的效果。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于迁移学习和多平面信息融合的Res-UNet框架，实现了从多序列MRI图像中自动、准确地分割缺血性脑卒中病灶，达到了80.5%的Dice分数和74.03%的准确率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The accurate understanding of ischemic stroke lesions is critical forefficient therapy and prognosis of stroke patients. Magnetic resonance imaging(MRI) is sensitive to acute ischemic stroke and is a common diagnostic methodfor stroke. However, manual lesion segmentation performed by experts istedious, time-consuming, and prone to observer inconsistency. Automatic medicalimage analysis methods have been proposed to overcome this challenge. However,previous approaches have relied on hand-crafted features that may not capturethe irregular and physiologically complex shapes of ischemic stroke lesions. Inthis study, we present a novel framework for quickly and automaticallysegmenting ischemic stroke lesions on various MRI sequences, includingT1-weighted, T2-weighted, DWI, and FLAIR. The proposed methodology is validatedon the ISLES 2015 Brain Stroke sequence dataset, where we trained our modelusing the Res-Unet architecture twice: first, with pre-existing weights, andthen without, to explore the benefits of transfer learning. Evaluation metrics,including the Dice score and sensitivity, were computed across 3D volumes.Finally, a Majority Voting Classifier was integrated to amalgamate the outcomesfrom each axis, resulting in a comprehensive segmentation method. Our effortsculminated in achieving a Dice score of 80.5\% and an accuracy of 74.03\%,showcasing the efficacy of our segmentation approach.</description>
      <author>example@mail.com (R. P. Chowdhury, T. Rahman)</author>
      <guid isPermaLink="false">2511.07281v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>A Hybrid Autoencoder-Transformer Model for Robust Day-Ahead Electricity Price Forecasting under Extreme Conditions</title>
      <link>http://arxiv.org/abs/2511.06898v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in 2025 IEEE 1st International Symposium on the Application  of Artificial Intelligence in Electrical Engineering (AAIEE)  https://ieeexplore.ieee.org/document/11100637&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型混合深度学习框架，结合蒸馏注意力变换器和自编码器自回归模型，用于解决极端条件和市场异常对日前电力价格预测的挑战。&lt;h4&gt;背景&lt;/h4&gt;准确的日前电力价格预测对电力系统高效运行至关重要，但极端条件和市场异常对现有预测方法构成显著挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型混合深度学习框架，提高电力价格预测的准确性、鲁棒性和计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出结合蒸馏注意力变换器(DAT)和自编码器自回归模型(ASM)的混合框架。DAT利用自注意力机制动态分配权重给历史数据关键部分，捕捉长期趋势和短期波动；ASM采用无监督学习检测和隔离由极端条件引起的异常模式。&lt;h4&gt;主要发现&lt;/h4&gt;在加利福尼亚和山东省数据集上的实验表明，该框架在预测准确性、鲁棒性和计算效率方面显著优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;该框架有望增强电网韧性并优化未来电力系统的市场运营。&lt;h4&gt;翻译&lt;/h4&gt;准确的日前电力价格预测(DAEPF)对电力系统的高效运行至关重要，但极端条件和市场异常对现有预测方法构成重大挑战。为克服这些挑战，本文提出了一种新型混合深度学习框架，整合了蒸馏注意力变换器(DAT)模型和自编码器自回归模型(ASM)。DAT利用自注意力机制动态分配更高权重给历史数据的关键部分，有效捕捉长期趋势和短期波动。同时，ASM采用无监督学习检测和隔离由极端条件(如暴雨、热浪或人类节日)引起的异常模式。在加利福尼亚和山东省数据集上的实验表明，我们的框架在预测准确性、鲁棒性和计算效率方面显著优于最先进的方法。因此，该框架有望增强电网韧性并优化未来电力系统的市场运营。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate day-ahead electricity price forecasting (DAEPF) is critical for theefficient operation of power systems, but extreme condition and marketanomalies pose significant challenges to existing forecasting methods. Toovercome these challenges, this paper proposes a novel hybrid deep learningframework that integrates a Distilled Attention Transformer (DAT) model and anAutoencoder Self-regression Model (ASM). The DAT leverages a self-attentionmechanism to dynamically assign higher weights to critical segments ofhistorical data, effectively capturing both long-term trends and short-termfluctuations. Concurrently, the ASM employs unsupervised learning to detect andisolate anomalous patterns induced by extreme conditions, such as heavy rain,heat waves, or human festivals. Experiments on datasets sampled from Californiaand Shandong Province demonstrate that our framework significantly outperformsstate-of-the-art methods in prediction accuracy, robustness, and computationalefficiency. Our framework thus holds promise for enhancing grid resilience andoptimizing market operations in future power systems.</description>
      <author>example@mail.com (Boyan Tang, Xuanhao Ren, Peng Xiao, Shunbo Lei, Xiaorong Sun, Jianghua Wu)</author>
      <guid isPermaLink="false">2511.06898v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Neyman-Pearson Classification under Both Null and Alternative Distributions Shift</title>
      <link>http://arxiv.org/abs/2511.06641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了Neyman-Pearson分类中的迁移学习问题，提出了一种自适应程序，能够在控制两种类型错误的同时处理分布偏移问题。&lt;h4&gt;背景&lt;/h4&gt;传统分类中的迁移学习已被广泛研究，但在Neyman-Pearson分类等不平衡分类场景下的迁移学习研究较少。现有工作仅考虑了μ₁分布中的偏移，而实际场景中偏移可能同时发生在μ₀和μ₁中。&lt;h4&gt;目的&lt;/h4&gt;开发一种迁移学习方法，针对分布μ₁最小化误差，同时确保针对分布μ₀的误差保持在规定阈值以下，并适应源数据是否有信息量的情况。&lt;h4&gt;方法&lt;/h4&gt;推导了一种自适应程序，当源数据具有信息量时保证改进的Type-I和Type-II错误，同时自动适应源数据无信息量的情况以避免负迁移。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够在源数据具有信息量时提供改进的错误率，同时避免在源数据无信息量时发生负迁移。&lt;h4&gt;结论&lt;/h4&gt;该方法不仅提供了统计保证，还在计算上表现出高效性，解决了Neyman-Pearson分类中迁移学习的独特挑战。&lt;h4&gt;翻译&lt;/h4&gt;我们考虑Neyman-Pearson分类中的迁移学习问题，其目标是在针对分布μ₀的误差保持在规定阈值以下的约束下，最小化针对分布μ₁的误差。虽然传统分类中的迁移学习已被广泛研究，但像Neyman-Pearson分类这样的不平衡分类中的迁移学习受到的关注较少。这种设置带来了独特挑战，因为必须同时控制两种类型的错误。现有工作仅处理了μ₁中的分布偏移情况，而实际场景中偏移可能同时发生在μ₀和μ₁中。我们推导了一种自适应程序，不仅能在源数据具有信息量时保证改进的Type-I和Type-II错误，还能自动适应源数据无信息量的情况，从而避免负迁移。除了这些统计保证外，该程序在计算上也表现出高效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider the problem of transfer learning in Neyman-Pearsonclassification, where the objective is to minimize the error w.r.t. adistribution $\mu_1$, subject to the constraint that the error w.r.t. adistribution $\mu_0$ remains below a prescribed threshold. While transferlearning has been extensively studied in traditional classification, transferlearning in imbalanced classification such as Neyman-Pearson classification hasreceived much less attention. This setting poses unique challenges, as bothtypes of errors must be simultaneously controlled. Existing works address onlythe case of distribution shift in $\mu_1$, whereas in many practical scenariosshifts may occur in both $\mu_0$ and $\mu_1$. We derive an adaptive procedurethat not only guarantees improved Type-I and Type-II errors when the source isinformative, but also automatically adapt to situations where the source isuninformative, thereby avoiding negative transfer. In addition to suchstatistical guarantees, the procedures is efficient, as shown via complementarycomputational guarantees.</description>
      <author>example@mail.com (Mohammadreza M. Kalan, Yuyang Deng, Eitan J. Neugut, Samory Kpotufe)</author>
      <guid isPermaLink="false">2511.06641v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>TriShGAN: Enhancing Sparsity and Robustness in Multivariate Time Series Counterfactuals Explanation</title>
      <link>http://arxiv.org/abs/2511.06529v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了TriShGAN方法，用于为多元时间序列数据生成更稳健的反事实解释。该方法结合了三元组损失和形状提取器，通过距离度量学习平衡最小成本和稳健性，提高了解释的稀疏性和训练效率。&lt;h4&gt;背景&lt;/h4&gt;在决策过程中，利益相关者通常依赖反事实解释，这提供了关于应更改查询实例中的哪些内容以改变AI系统结果的建议。然而，为多元时间序列生成这些解释具有挑战性，因为它们具有复杂的多维性质。传统方法要么直接替换子序列（不现实），要么主要关注最小化成本而忽略将反事实解释远离决策边界的重要性。&lt;h4&gt;目的&lt;/h4&gt;生成更稳健的反事实解释，使其不仅保持接近查询的时间序列，还捕获具有期望结果的实例的特征分布，从而在最小成本和稳健性之间实现更好的平衡。&lt;h4&gt;方法&lt;/h4&gt;作者引入了TriShGAN，这是在CounteRGAN框架基础上通过结合三元组损失而增强的方法。这是一种无监督学习方法，使用距离度量学习来鼓励反事实解释保持接近查询的时间序列同时捕获具有期望结果的实例的特征分布。此外，整合了一个形状提取器，战略性地选择高维查询时间序列中最具判别性的部分。&lt;h4&gt;主要发现&lt;/h4&gt;通过结合三元组损失和形状提取器，TriShGAN方法能够在最小成本和稳健性之间实现更好的平衡，生成更高质量的反事实解释，同时提高了训练效率和解释的稀疏性。&lt;h4&gt;结论&lt;/h4&gt;TriShGAN方法通过距离度量学习和形状提取器的整合，有效地解决了多元时间序列反事实解释生成中的挑战，提供了更稳健和稀疏的解释，同时提高了训练效率。&lt;h4&gt;翻译&lt;/h4&gt;在决策过程中，利益相关者通常依赖反事实解释，这提供了关于应更改查询实例中的哪些内容以改变AI系统结果的建议。然而，为多元时间序列生成这些解释具有挑战性，因为它们具有复杂的多维性质。传统的基于最近异邻域的方法通常用来自NUN的有影响力子序列替换查询时间序列中的子序列，由于严格的直接替换，这在现实场景中并不总是现实的。基于残差生成对抗网络的反事实方法旨在通过学习观测数据的分布来生成合成反事实解释来解决这一问题。然而，这些方法主要关注最小化从查询时间序列到反事实解释的成本，而常常忽略将反事实解释远离决策边界的重要性。这种疏忽可能导致在模型内发生微小变化时，解释不再符合反事实条件。为了生成更稳健的反事实解释，我们引入了TriShGAN，这是在CounteRGAN框架基础上通过结合三元组损失而增强的方法。这种无监督学习方法使用距离度量学习来鼓励反事实解释不仅保持接近查询的时间序列，还捕获具有期望结果的实例的特征分布，从而在最小成本和稳健性之间实现更好的平衡。此外，我们整合了一个形状提取器，它战略性地选择高维查询时间序列中最具判别性的部分，以提高反事实解释的稀疏性和训练过程的效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In decision-making processes, stakeholders often rely on counterfactualexplanations, which provide suggestions about what should be changed in thequeried instance to alter the outcome of an AI system. However, generatingthese explanations for multivariate time series presents challenges due totheir complex, multi-dimensional nature. Traditional Nearest UnlikeNeighbor-based methods typically substitute subsequences in a queried timeseries with influential subsequences from an NUN, which is not always realisticin real-world scenarios due to the rigid direct substitution. Counterfactualwith Residual Generative Adversarial Networks-based methods aim to address thisby learning from the distribution of observed data to generate syntheticcounterfactual explanations. However, these methods primarily focus onminimizing the cost from the queried time series to the counterfactualexplanations and often neglect the importance of distancing the counterfactualexplanation from the decision boundary. This oversight can result inexplanations that no longer qualify as counterfactual if minor changes occurwithin the model. To generate a more robust counterfactual explanation, weintroduce TriShGAN, under the CounteRGAN framework enhanced by theincorporation of triplet loss. This unsupervised learning approach usesdistance metric learning to encourage the counterfactual explanations not onlyto remain close to the queried time series but also to capture the featuredistribution of the instance with the desired outcome, thereby achieving abetter balance between minimal cost and robustness. Additionally, we integratea Shapelet Extractor that strategically selects the most discriminative partsof the high-dimensional queried time series to enhance the sparsity ofcounterfactual explanation and efficiency of the training process.</description>
      <author>example@mail.com (Hongnan Ma, Yiwei Shi, Guanxiong Sun, Mengyue Yang, Weiru Liu)</author>
      <guid isPermaLink="false">2511.06529v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Label-Efficient 3D Forest Mapping: Self-Supervised and Transfer Learning for Individual, Structural, and Species Analysis</title>
      <link>http://arxiv.org/abs/2511.06331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索使用自监督学习和迁移学习减少对大型标注数据集的依赖，改进从激光扫描点云中提取个体树木信息的方法，开发统一框架提高效率并减少碳排放。&lt;h4&gt;背景&lt;/h4&gt;个体树木级别的详细结构和物种信息对精准林业、生物多样性保护和碳制图至关重要。激光扫描点云是获取此类信息的主要数据源，但深度学习模型需要大量标注数据，而3D点云标注在复杂森林环境中劳动密集且难以规模化。&lt;h4&gt;目的&lt;/h4&gt;探索减少对大型标注数据集依赖的策略，使用自监督学习和迁移学习架构，通过真实和可操作的训练集改进实例分割、语义分割和树木分类任务。&lt;h4&gt;方法&lt;/h4&gt;采用自监督学习与领域适应结合的方法进行实例分割，使用自监督学习进行语义分割，采用分层迁移学习进行树木分类，并将所有任务整合到一个统一框架中。&lt;h4&gt;主要发现&lt;/h4&gt;结合自监督学习和领域适应使实例分割性能提升16.98%（AP50）；自监督学习使语义分割提升1.79%（mIoU）；分层迁移学习使未见物种分类准确率提升6.07%（Jaccard）；预训练模型减少约21%的能源消耗和碳排放。&lt;h4&gt;结论&lt;/h4&gt;这项开源贡献加速了从激光扫描点云中提取个体树木信息的操作性过程，支持林业、生物多样性和碳制图，同时提高了效率和可持续性。&lt;h4&gt;翻译&lt;/h4&gt;个体树木级别的详细结构和物种信息正变得越来越重要，以支持精准林业、生物多样性保护，并为生物量和碳制图提供参考数据。目前，来自机载和地面激光扫描的点云是最适合大规模快速获取此类信息的数据源。深度学习的最新进展改进了对个体树木的分割和分类以及识别语义树木组件的能力。然而，深度学习模型通常需要大量标注的训练数据，这限制了进一步的改进。为三维点云生成密集、高质量的标注，特别是在复杂的森林环境中，是劳动密集型的且难以规模化。我们探索了使用自监督学习和迁移学习架构来减少对大型标注数据集依赖的策略。我们的目标是使用真实和可操作的训练集，改进三个任务：实例分割、语义分割和树木分类的性能。我们的研究结果表明，与从头开始训练相比，结合自监督学习和领域适应显著增强了实例分割（AP50 +16.98%），自监督学习足以用于语义分割（mIoU +1.79%），分层迁移学习能够准确分类未见物种（Jaccard +6.07%）。为了简化使用并鼓励采用，我们将这些任务整合到一个统一框架中，简化了从原始点云到树木描绘、结构分析和物种分类的过程。预训练模型减少了约21%的能源消耗和碳排放。这项开源贡献旨在加速从激光扫描点云中操作性地提取个体树木信息，以支持林业、生物多样性和碳制图。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决深度学习模型在3D森林分析中需要大量标注数据的问题。在复杂森林环境中，对3D点云进行高质量标注是劳动密集型且难以规模化的。这个问题很重要，因为详细的树木结构和物种信息对精准林业、生物多样性保护和碳制图至关重要，而气候变化增加了森林干扰频率，使传统森林调查方法时效性不足。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考到深度学习需要大量标注数据，而森林环境中的点云标注特别困难。他们借鉴了自监督学习（特别是对比学习）来利用未标记数据，领域适应技术解决不同森林区域间的差异，以及分层迁移学习改进细粒度分类。设计上，他们使用对比自监督预训练学习通用特征，然后通过领域适应和分层迁移学习将这些特征应用到三个具体任务：实例分割、语义分割和树木分类。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自监督学习从大量未标记数据中学习通用特征，然后利用领域适应和分层迁移学习将这些特征应用到具体任务。整体流程：1)用掩码场景对比在大型未标记点云上预训练Sparse UNet编码器；2)对实例分割任务，添加偏移头预测点到实例中心，再用聚类算法分组；3)对语义分割，添加分类头对每个点分类；4)对树木分类，先在宽泛类别预训练，再在特定物种微调；5)通过领域适应解决不同森林区域间的差异。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的3D深度学习框架整合预训练、领域适应和分层迁移学习；2)在多样化森林类型的大规模数据上预训练；3)在极少量标注数据(如每棵树仅4-5个点)下实现有效性能；4)跨多个站点的大量验证；5)减少21%的能源消耗和碳排放。相比之前工作，不同之处在于将多种技术整合到一个框架中同时处理三个任务，在极度稀疏标注条件下实现有效性能，并提供从原始点云到最终分析的可扩展工作流程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合自监督学习和迁移学习的统一框架，显著减少了对标注数据的依赖，实现了在有限监督下从3D点云中进行高效个体树木识别、结构分析和物种分类。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detailed structural and species information on individual tree level isincreasingly important to support precision forestry, biodiversityconservation, and provide reference data for biomass and carbon mapping. Pointclouds from airborne and ground-based laser scanning are currently the mostsuitable data source to rapidly derive such information at scale. Recentadvancements in deep learning improved segmenting and classifying individualtrees and identifying semantic tree components. However, deep learning modelstypically require large amounts of annotated training data which limits furtherimprovement. Producing dense, high-quality annotations for 3D point clouds,especially in complex forests, is labor-intensive and challenging to scale. Weexplore strategies to reduce dependence on large annotated datasets usingself-supervised and transfer learning architectures. Our objective is toimprove performance across three tasks: instance segmentation, semanticsegmentation, and tree classification using realistic and operational trainingsets. Our findings indicate that combining self-supervised learning with domainadaptation significantly enhances instance segmentation compared to trainingfrom scratch (AP50 +16.98%), self-supervised learning suffices for semanticsegmentation (mIoU +1.79%), and hierarchical transfer learning enables accurateclassification of unseen species (Jaccard +6.07%). To simplify use andencourage uptake, we integrated the tasks into a unified framework,streamlining the process from raw point clouds to tree delineation, structuralanalysis, and species classification. Pretrained models reduce energyconsumption and carbon emissions by ~21%. This open-source contribution aims toaccelerate operational extraction of individual tree information from laserscanning point clouds to support forestry, biodiversity, and carbon mapping.</description>
      <author>example@mail.com (Aldino Rizaldy, Fabian Ewald Fassnacht, Ahmed Jamal Afifi, Hua Jiang, Richard Gloaguen, Pedram Ghamisi)</author>
      <guid isPermaLink="false">2511.06331v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Modal Fine-Tuning of 3D Convolutional Foundation Models for ADHD Classification with Low-Rank Adaptation</title>
      <link>http://arxiv.org/abs/2511.06163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于3D低秩适应(LoRA)的参数高效迁移学习方法，用于ADHD的神经影像学诊断，显著减少了可训练参数同时实现了高准确率。&lt;h4&gt;背景&lt;/h4&gt;早期诊断儿童注意缺陷多动障碍(ADHD)对改善教育和心理健康结果至关重要。然而，使用神经影像数据诊断ADHD具有挑战性，因为其表现异质且与其他疾病症状重叠。&lt;h4&gt;目的&lt;/h4&gt;提出一种参数高效的迁移学习方法，将大规模3D卷积基础模型适配到基于MRI的ADHD分类任务。&lt;h4&gt;方法&lt;/h4&gt;引入3D低秩适应(LoRA)，将3D卷积核分解为2D低秩更新，减少可训练参数。在公共扩散MRI数据库上进行五折交叉验证评估。&lt;h4&gt;主要发现&lt;/h4&gt;3D LoRA微调策略达到最先进结果，一个模型变体准确率达71.9%，另一个变体AUC为0.716，两者仅使用164万可训练参数（比完全微调的基础模型少113倍以上）。&lt;h4&gt;结论&lt;/h4&gt;该研究代表了神经影像学中基础模型跨模态（CT到MRI）适配的首次成功之一，为ADHD分类建立了新基准并大幅提高了效率。&lt;h4&gt;翻译&lt;/h4&gt;儿童注意缺陷多动障碍(ADHD)的早期诊断在改善教育和心理健康结果方面起着至关重要的作用。然而，使用神经影像数据诊断ADHD仍然具有挑战性，因为表现异质且与其他疾病症状重叠。为解决这一问题，我们提出了一种新颖的参数高效的迁移学习方法，将大规模3D卷积基础模型（在CT图像上预训练）适配到基于MRI的ADHD分类任务。我们的方法通过将3D卷积核分解为2D低秩更新引入了3D低秩适应(LoRA)，显著减少了可训练参数，同时实现了卓越性能。在公共扩散MRI数据库上的五折交叉验证评估中，我们的3D LoRA微调策略取得了最先进的结果，一个模型变体达到71.9%的准确率，另一个变体达到0.716的AUC。两个变体仅使用164万可训练参数（比完全微调的基础模型少113倍以上）。我们的结果代表了神经影像学中基础模型跨模态（CT到MRI）适配的首次成功之一，为ADHD分类建立了新基准，同时大大提高了效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early diagnosis of attention-deficit/hyperactivity disorder (ADHD) inchildren plays a crucial role in improving outcomes in education and mentalhealth. Diagnosing ADHD using neuroimaging data, however, remains challengingdue to heterogeneous presentations and overlapping symptoms with otherconditions. To address this, we propose a novel parameter-efficient transferlearning approach that adapts a large-scale 3D convolutional foundation model,pre-trained on CT images, to an MRI-based ADHD classification task. Our methodintroduces Low-Rank Adaptation (LoRA) in 3D by factorizing 3D convolutionalkernels into 2D low-rank updates, dramatically reducing trainable parameterswhile achieving superior performance. In a five-fold cross-validated evaluationon a public diffusion MRI database, our 3D LoRA fine-tuning strategy achievedstate-of-the-art results, with one model variant reaching 71.9% accuracy andanother attaining an AUC of 0.716. Both variants use only 1.64 milliontrainable parameters (over 113x fewer than a fully fine-tuned foundationmodel). Our results represent one of the first successful cross-modal(CT-to-MRI) adaptations of a foundation model in neuroimaging, establishing anew benchmark for ADHD classification while greatly improving efficiency.</description>
      <author>example@mail.com (Jyun-Ping Kao, Shinyeong Rho, Shahar Lazarev, Hyun-Hae Cho, Fangxu Xing, Taehoon Shin, C. -C. Jay Kuo, Jonghye Woo)</author>
      <guid isPermaLink="false">2511.06163v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>LLM Attention Transplant for Transfer Learning of Tabular Data Across Disparate Domains</title>
      <link>http://arxiv.org/abs/2511.06161v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LATTLE的新型迁移学习方法，通过将大型语言模型的选择性注意力权重移植到专门设计的表格数据模型中，解决了表格数据迁移学习中的异质性问题&lt;h4&gt;背景&lt;/h4&gt;表格数据的迁移学习具有挑战性，因为不同领域间的特征空间存在异质性；传统深度学习在表格知识迁移方面表现有限；虽然大型语言模型可以提升迁移学习效果，但由于文本提示和上下文学习的限制，其在处理混合数据类型的表格时效果往往停滞不前&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级迁移学习框架，消除对共享特征、LLM提示工程和大规模预训练模型的需求，有效解决表格数据间的迁移学习问题&lt;h4&gt;方法&lt;/h4&gt;使用源表格数据微调LLM，将其选择性key和value投影权重移植到为表格数据设计的门控特征标记变换器(gFTT)中，然后使用目标表格数据微调具有跨域注意力的gFTT模型进行迁移学习&lt;h4&gt;主要发现&lt;/h4&gt;通过十对源-目标数据集和12个基线的实验证明，LATTLE方法优于传统机器学习模型、最先进的深度表格架构以及从数千到数十亿表格样本训练的迁移学习模型；注意力迁移为在低资源学习环境中使用LLM学习表格间关系提供了有效解决方案&lt;h4&gt;结论&lt;/h4&gt;所提出的注意力迁移是一种有效方法，可以在低资源环境下利用LLM学习表格数据间的关系；该方法的源代码已公开可用&lt;h4&gt;翻译&lt;/h4&gt;表格数据的迁移学习具有挑战性，因为不同领域间的特征空间存在异质性。传统深度学习在表格知识迁移方面的有限成功可以通过利用大型语言模型来提升。然而，由于文本提示和上下文学习的限制，LLMs在处理表格中混合数据类型时的效果往往停滞不前。我们提出了一种轻量级迁移学习框架，使用源表格数据微调LLM，并将LLM的选择性key和value投影权重移植到为表格数据构建的门控特征标记变换器(gFTT)中。使用目标表格数据微调具有跨域注意力的gFTT模型进行迁移学习，消除对共享特征、LLM提示工程和大规模预训练模型的需求。我们使用十对源-目标数据集和12个基线进行的实验证明了所提出的LLM-注意力迁移(LATTLE)方法优于传统机器学习模型、最先进的深度表格架构以及从数千到数十亿表格样本训练的迁移学习模型。所提出的注意力迁移为在低资源学习环境中使用LLM学习表格间关系提供了一种有效解决方案。所提出方法的源代码已公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning of tabular data is non-trivial due to heterogeneity in thefeature space across disparate domains. The limited success of traditional deeplearning in tabular knowledge transfer can be advanced by leveraging largelanguage models (LLMs). However, the efficacy of LLMs often stagnates for mixeddata types structured in tables due to the limitations of text prompts andin-context learning. We propose a lightweight transfer learning framework thatfine-tunes an LLM using source tabular data and transplants the LLM's selective$key$ and $value$ projection weights into a gated feature tokenized transformer(gFTT) built for tabular data. The gFTT model with cross-domain attention isfine-tuned using target tabular data for transfer learning, eliminating theneed for shared features, LLM prompt engineering, and large-scale pretrainedmodels. Our experiments using ten pairs of source-target data sets and 12baselines demonstrate the superiority of the proposed LLM-attention transplantfor transfer learning (LATTLE) method over traditional ML models,state-of-the-art deep tabular architectures, and transfer learning modelstrained on thousands to billions of tabular samples. The proposed attentiontransfer demonstrates an effective solution to learning relationships betweendata tables using an LLM in a low-resource learning environment. The sourcecode for the proposed method is publicly available.</description>
      <author>example@mail.com (Ibna Kowsar, Kazi F. Akhter, Manar D. Samad)</author>
      <guid isPermaLink="false">2511.06161v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>FusionLog: Cross-System Log-based Anomaly Detection via Fusion of General and Proprietary Knowledge</title>
      <link>http://arxiv.org/abs/2511.05878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 4 figures, and 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FusionLog是一种创新的零标记跨系统日志异常检测方法，通过融合通用知识和专有知识，实现了无需标记目标日志的跨系统异常检测，在三个不同系统的公共日志数据集上取得了超过90%的F1分数，显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;基于日志的异常检测对确保Web系统稳定性和可靠性至关重要，但缺乏足够标记日志限制了新系统的快速部署。现有方法利用成熟系统的大规模标记日志和新系统的小量标记日志，通过迁移学习提取通用知识。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法只关注通用知识迁移而忽视与目标系统专有知识间差异的问题，提出一种无需标记目标日志即可实现跨系统泛化的异常检测方法。&lt;h4&gt;方法&lt;/h4&gt;设计基于语义相似性的无需训练路由器，动态划分未标记目标日志为'通用日志'和'专有日志'。通用日志采用系统无关表示元学习的小型模型处理，继承共享异常模式；专有日志通过迭代生成伪标签并利用大型语言模型和小型模型的多轮协作知识蒸馏与融合来微调模型。&lt;h4&gt;主要发现&lt;/h4&gt;在三个不同系统的公共日志数据集上，FusionLog在完全零标记设置下实现超过90%的F1分数，性能显著优于现有最先进的跨系统日志异常检测方法。&lt;h4&gt;结论&lt;/h4&gt;FusionLog有效解决了跨系统日志异常检测中标记数据不足的问题，通过通用知识和专有知识的融合，实现了高性能的零标记异常检测，具有良好的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;基于日志的异常检测对于确保Web系统的稳定性和可靠性至关重要。这项任务中的一个关键问题是缺乏足够的标记日志，这限制了在新系统中的快速部署。现有工作通常利用来自成熟Web系统的大规模标记日志和新系统的小量标记日志，使用迁移学习来提取和泛化两个领域间的通用知识。然而，这些方法只关注通用知识的迁移，忽视了这种知识与目标系统专有知识之间的差异和潜在不匹配，从而限制了性能。为解决这一局限，我们提出了FusionLog，一种创新的零标记跨系统日志异常检测方法，有效实现了通用知识和专有知识的融合，无需任何标记的目标日志即可实现跨系统泛化。具体而言，我们首先设计了一个基于语义相似性的无需训练的路由器，动态将未标记的目标日志划分为'通用日志'和'专有日志'。对于通用日志，FusionLog使用基于系统无关表示元学习的小型模型进行直接训练和推理，继承源系统和目标系统之间共享的通用异常模式。对于专有日志，我们通过迭代生成伪标签并使用基于大型语言模型和小型模型的多轮协作知识蒸馏和融合来微调小型模型，增强其识别目标系统特定异常模式的能力。在三个来自不同系统的公共日志数据集上的实验结果表明，FusionLog在完全零标记设置下实现了超过90%的F1分数，显著优于最先进的跨系统日志异常检测方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Log-based anomaly detection is critical for ensuring the stability andreliability of web systems. One of the key problems in this task is the lack ofsufficient labeled logs, which limits the rapid deployment in new systems.Existing works usually leverage large-scale labeled logs from a mature websystem and a small amount of labeled logs from a new system, using transferlearning to extract and generalize general knowledge across both domains.However, these methods focus solely on the transfer of general knowledge andneglect the disparity and potential mismatch between such knowledge and theproprietary knowledge of target system, thus constraining performance. Toaddress this limitation, we propose FusionLog, a novel zero-label cross-systemlog-based anomaly detection method that effectively achieves the fusion ofgeneral and proprietary knowledge, enabling cross-system generalization withoutany labeled target logs. Specifically, we first design a training-free routerbased on semantic similarity that dynamically partitions unlabeled target logsinto 'general logs' and 'proprietary logs.' For general logs, FusionLog employsa small model based on system-agnostic representation meta-learning for directtraining and inference, inheriting the general anomaly patterns shared betweenthe source and target systems. For proprietary logs, we iteratively generatepseudo-labels and fine-tune the small model using multi-round collaborativeknowledge distillation and fusion based on large language model (LLM) and smallmodel (SM) to enhance its capability to recognize anomaly patterns specific tothe target system. Experimental results on three public log datasets fromdifferent systems show that FusionLog achieves over 90% F1-score under a fullyzero-label setting, significantly outperforming state-of-the-art cross-systemlog-based anomaly detection methods.</description>
      <author>example@mail.com (Xinlong Zhao, Tong Jia, Minghua He, Xixuan Yang, Ying Li)</author>
      <guid isPermaLink="false">2511.05878v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>ZeroLog: Zero-Label Generalizable Cross-System Log-based Anomaly Detection</title>
      <link>http://arxiv.org/abs/2511.05862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 17 figures, and 3 tables; accepted by ISSRE 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了ZeroLog，一种系统无关的表示元学习方法，实现了零标记条件下的跨系统日志异常检测，无需目标系统的任何标记日志即可达到超过80%的F1分数。&lt;h4&gt;背景&lt;/h4&gt;基于日志的异常检测是确保软件系统稳定性和可靠性的重要任务，但缺乏标记日志是一个关键问题。现有方法通常利用成熟系统的大规模标记日志通过迁移学习训练目标系统模型，但仍需目标系统的一些标记日志。&lt;h4&gt;目的&lt;/h4&gt;研究零标记跨系统日志异常检测这一有价值但未被充分探索的设置，即目标系统中没有可用的标记日志。&lt;h4&gt;方法&lt;/h4&gt;提出ZeroLog，利用无监督域适应在源域和目标域之间进行对抗训练，学习系统无关的通用特征表示，并通过元学习将学习到的表示推广到目标系统，无需任何目标标记。&lt;h4&gt;主要发现&lt;/h4&gt;在三个不同系统的公共日志数据集上，ZeroLog在无标记情况下达到超过80%的F1分数，可与使用标记日志训练的最先进跨系统方法相媲美，并且在零标记条件下优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;ZeroLog成功实现了零标记条件下的跨系统日志异常检测，解决了目标系统缺乏标记日志的问题，为实际应用提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于日志的异常检测是确保软件系统稳定性和可靠性的重要任务。此任务中的一个关键问题是缺乏标记日志。现有工作通常利用成熟系统的大规模标记日志，基于迁移学习的思想来训练目标系统的异常检测模型。然而，这些工作仍然需要目标系统一定数量的标记日志。在本文中，我们进一步研究了一个有价值但未被充分探索的设置：零标记跨系统日志异常检测，即目标系统中没有可用的标记日志。具体来说，我们提出了ZeroLog，一种系统无关的表示元学习方法，使零标记条件下的跨系统日志异常检测成为可能。为此，我们利用无监督域适应在源域和目标域之间进行对抗训练，旨在学习系统无关的通用特征表示。通过采用元学习，学习到的表示可以进一步推广到目标系统，而无需任何目标标记。在来自不同系统的三个公共日志数据集上的实验结果表明，ZeroLog在无标记情况下达到超过80%的F1分数，可与使用标记日志训练的最先进跨系统方法相媲美，并且在零标记条件下优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Log-based anomaly detection is an important task in ensuring the stabilityand reliability of software systems. One of the key problems in this task isthe lack of labeled logs. Existing works usually leverage large-scale labeledlogs from mature systems to train an anomaly detection model of a target systembased on the idea of transfer learning. However, these works still require acertain number of labeled logs from the target system. In this paper, we take astep forward and study a valuable yet underexplored setting: zero-labelcross-system log-based anomaly detection, that is, no labeled logs areavailable in the target system. Specifically, we propose ZeroLog, asystem-agnostic representation meta-learning method that enables cross-systemlog-based anomaly detection under zero-label conditions. To achieve this, weleverage unsupervised domain adaptation to perform adversarial training betweenthe source and target domains, aiming to learn system-agnostic general featurerepresentations. By employing meta-learning, the learned representations arefurther generalized to the target system without any target labels.Experimental results on three public log datasets from different systems showthat ZeroLog reaches over 80% F1-score without labels, comparable tostate-of-the-art cross-system methods trained with labeled logs, andoutperforms existing methods under zero-label conditions.</description>
      <author>example@mail.com (Xinlong Zhao, Tong Jia, Minghua He, Ying Li, Gang Huang)</author>
      <guid isPermaLink="false">2511.05862v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Google-MedGemma Based Abnormality Detection in Musculoskeletal radiographs</title>
      <link>http://arxiv.org/abs/2511.05600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of ICICT 2026, London, Springer (Forthcoming, February  2026; Accepted for Publication)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于MedGemma框架的肌肉骨骼放射照片异常自动检测方法，利用现代医学基础模型提高分类性能和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统的异常检测方法主要依赖自编码器和神经网络管道，在肌肉骨骼放射照片分析中存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于MedGemma的高性能异常检测系统，用于肌肉骨骼放射照片的自动分类，提高检测准确性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;利用MedGemma基础模型和SigLIP衍生的视觉编码器，将预处理的X光图像编码为高维嵌入，通过轻量级多层感知器进行二元分类，并采用选择性编码器块解冻等模块化训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;MedGemma驱动的分类器表现出强大的性能，超过了传统卷积和基于自编码器的指标；模型利用迁移学习能力提高了泛化能力和特征工程优化。&lt;h4&gt;结论&lt;/h4&gt;MedGemma驱动的分类系统可以通过提供可扩展和准确的异常检测来推进临床放射照片分类，并在自动医学图像分析中有更广泛的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于MedGemma的框架，用于肌肉骨骼放射照片的异常自动检测。与传统自编码器和神经网络管道不同，该方法利用了MedGemma基础模型，并集成了在多种医学成像模态上预训练的SigLIP衍生的视觉编码器。预处理的X光图像使用MedGemma视觉主干编码为高维嵌入，然后通过轻量级多层感知器进行二元分类。实验评估显示，MedGemma驱动的分类器表现出强大的性能，超过了传统卷积和基于自编码器的指标。此外，该模型利用了MedGemma的迁移学习能力，提高了泛化能力和特征工程优化。集成现代医学基础模型不仅增强了表示学习，还促进了模块化训练策略，如选择性编码器块解冻，以实现高效的领域适应。研究结果表明，MedGemma驱动的分类系统可以通过提供可扩展和准确的异常检测来推进临床放射照片分类，并在自动医学图像分析中有更广泛的应用潜力。关键词：Google MedGemma, MURA, 医学图像, 分类。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a MedGemma-based framework for automatic abnormalitydetection in musculoskeletal radiographs. Departing from conventionalautoencoder and neural network pipelines, the proposed method leverages theMedGemma foundation model, incorporating a SigLIP-derived vision encoderpretrained on diverse medical imaging modalities. Preprocessed X-ray images areencoded into high-dimensional embeddings using the MedGemma vision backbone,which are subsequently passed through a lightweight multilayer perceptron forbinary classification. Experimental assessment reveals that the MedGemma-drivenclassifier exhibits strong performance, exceeding conventional convolutionaland autoencoder-based metrics. Additionally, the model leverages MedGemma'stransfer learning capabilities, enhancing generalization and optimizing featureengineering. The integration of a modern medical foundation model not onlyenhances representation learning but also facilitates modular trainingstrategies such as selective encoder block unfreezing for efficient domainadaptation. The findings suggest that MedGemma-powered classification systemscan advance clinical radiograph triage by providing scalable and accurateabnormality detection, with potential for broader applications in automatedmedical image analysis.  Keywords: Google MedGemma, MURA, Medical Image, Classification.</description>
      <author>example@mail.com (Soumyajit Maity, Pranjal Kamboj, Sneha Maity, Rajat Singh, Sankhadeep Chatterjee)</author>
      <guid isPermaLink="false">2511.05600v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>LoReTTA: A Low Resource Framework To Poison Continuous Time Dynamic Graphs</title>
      <link>http://arxiv.org/abs/2511.07379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LoReTTA是一种针对连续时间动态图的新型对抗攻击框架，通过两阶段方法显著降低时间图神经网络(TGNNs)的性能，同时保持不可察觉性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;时间图神经网络(TGNNs)越来越多地应用于金融预测、推荐系统和欺诈检测等高风险领域，但这些模型容易受到中毒攻击，构成严重安全风险。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效降低TGNN性能的对抗攻击方法，同时保持攻击的不可察觉性和对抗防御的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;LoReTTA采用两阶段方法：首先使用16种时间重要性指标之一移除高影响力边稀疏化图；然后通过新颖的保持度数的负采样算法用对抗性负样本替换被移除的边。其插件式设计无需昂贵代理模型且符合真实的不可察觉性约束。&lt;h4&gt;主要发现&lt;/h4&gt;LoReTTA在4个基准数据集和4个SotA模型上平均降低TGNN性能29.47%，具体为MOOC降低42.0%、Wikipedia降低31.5%、UCI降低28.8%、Enron降低15.6%。该方法优于11个攻击基线，对4种异常检测系统保持不可检测，并对4种SotA防御训练方法具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;LoReTTA在有效性、不可察觉性和鲁棒性方面均表现优异，是针对时间图神经网络的强大对抗攻击框架。&lt;h4&gt;翻译&lt;/h4&gt;时间图神经网络(TGNNs)越来越多地用于高风险领域，如金融预测、推荐系统和欺诈检测。然而，它们对中毒攻击的敏感性构成了严重的安全风险。我们引入LoReTTA（低资源两阶段时间攻击），一种针对连续时间动态图的新型对抗框架，该框架在4个广泛基准数据集和4个最先进模型上平均使TGNN性能降低29.47%。LoReTTA通过两阶段方法运行：(1)使用任何16种测试的时间重要性指标移除高影响力边来稀疏化图，(2)通过LoReTTA新颖的保持度数的负采样算法用对抗性负样本战略性地替换被移除的边。我们的插件式设计消除了对昂贵代理模型的需求，同时遵循真实的不可察觉性约束。LoReTTA在MOOC上降低性能42.0%，在Wikipedia上降低31.5%，在UCI上降低28.8%，在Enron上降低15.6%。LoReTTA优于11个攻击基线，对4种领先的异常检测系统保持不可检测，并对4种SotA对抗防御训练方法具有鲁棒性，确立了其有效性、不可察觉性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal Graph Neural Networks (TGNNs) are increasingly used in high-stakesdomains, such as financial forecasting, recommendation systems, and frauddetection. However, their susceptibility to poisoning attacks poses a criticalsecurity risk. We introduce LoReTTA (Low Resource Two-phase Temporal Attack), anovel adversarial framework on Continuous-Time Dynamic Graphs, which degradesTGNN performance by an average of 29.47% across 4 widely benchmark datasets and4 State-of-the-Art (SotA) models. LoReTTA operates through a two-stageapproach: (1) sparsify the graph by removing high-impact edges using any of the16 tested temporal importance metrics, (2) strategically replace removed edgeswith adversarial negatives via LoReTTA's novel degree-preserving negativesampling algorithm. Our plug-and-play design eliminates the need for expensivesurrogate models while adhering to realistic unnoticeability constraints.LoReTTA degrades performance by upto 42.0% on MOOC, 31.5% on Wikipedia, 28.8%on UCI, and 15.6% on Enron. LoReTTA outperforms 11 attack baselines, remainsundetectable to 4 leading anomaly detection systems, and is robust to 4 SotAadversarial defense training methods, establishing its effectiveness,unnoticeability, and robustness.</description>
      <author>example@mail.com (Himanshu Pal, Venkata Sai Pranav Bachina, Ankit Gangwal, Charu Sharma)</author>
      <guid isPermaLink="false">2511.07379v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>MG-HGNN: A Heterogeneous GNN Framework for Indoor Wi-Fi Fingerprint-Based Localization</title>
      <link>http://arxiv.org/abs/2511.07282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 11 figures, 11 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为MG-HGNN的多图异质图神经网络框架，用于提高基于接收信号强度指示器(RSSI)的室内定位准确性。该框架通过两个图构建分支进行节点和边嵌入，然后使用异质图神经网络进行图表示学习，从而增强空间感知能力和定位性能。&lt;h4&gt;背景&lt;/h4&gt;接收信号强度指示器(RSSI)是Wi-Fi指纹的主要表现形式，并在室内定位中扮演着关键角色。然而，现有的基于RSSI的定位方法常常由于环境复杂性以及多源信息处理方面的挑战而导致准确性降低。&lt;h4&gt;目的&lt;/h4&gt;解决现有RSSI定位方法在复杂环境下准确性的降低问题，以及多源信息处理的挑战，提高室内定位的精度和性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种多图异质GNN框架(MG-HGNN)，该框架包含两个图构建分支分别进行节点和边嵌入，以生成信息丰富的图。随后，使用异质图神经网络进行图表示学习，实现准确定位。主要创新包括：1)多类型任务导向的图构建，结合标签估计和特征编码以获取更丰富的图信息；2)异质GNN结构，增强传统GNN模型的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在UJIIndoorLoc和UTSIndoorLoc公共数据集上的评估表明，MG-HGNN不仅比几种最先进的方法实现了更优的性能，而且为增强基于GNN的定位方法提供了新的视角。消融研究进一步证实了所提出框架的合理性和有效性。&lt;h4&gt;结论&lt;/h4&gt;MG-HGNN框架通过创新的图构建方法和异质GNN结构，有效解决了复杂环境下基于RSSI的室内定位准确性降低的问题，为室内定位技术提供了新的解决方案和研究方向。&lt;h4&gt;翻译&lt;/h4&gt;接收信号强度指示器(RSSI)是Wi-Fi指纹的主要表现形式，并在室内定位中扮演着关键角色。然而，现有的基于RSSI的定位方法常常由于环境复杂性以及多源信息处理方面的挑战而导致准确性降低。为解决这些问题，我们提出了一种新颖的多图异质GNN框架(MG-HGNN)，以增强空间感知能力和提高定位性能。在该框架中，两个图构建分支分别进行节点和边嵌入，以生成信息丰富的图。随后，采用异质图神经网络进行图表示学习，实现准确定位。MG-HGNN框架引入了以下关键创新：1)多类型任务导向的图构建，结合标签估计和特征编码以获取更丰富的图信息；2)异质GNN结构，增强传统GNN模型的性能。在UJIIndoorLoc和UTSIndoorLoc公共数据集上的评估表明，MG-HGNN不仅比几种最先进的方法实现了更优的性能，而且为增强基于GNN的定位方法提供了新的视角。消融研究进一步证实了所提出框架的合理性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Received signal strength indicator (RSSI) is the primary representation ofWi-Fi fingerprints and serves as a crucial tool for indoor localization.However, existing RSSI-based positioning methods often suffer from reducedaccuracy due to environmental complexity and challenges in processingmulti-source information. To address these issues, we propose a novelmulti-graph heterogeneous GNN framework (MG-HGNN) to enhance spatial awarenessand improve positioning performance. In this framework, two graph constructionbranches perform node and edge embedding, respectively, to generate informativegraphs. Subsequently, a heterogeneous graph neural network is employed forgraph representation learning, enabling accurate positioning. The MG-HGNNframework introduces the following key innovations: 1) multi-type task-directedgraph construction that combines label estimation and feature encoding forricher graph information; 2) a heterogeneous GNN structure that enhances theperformance of conventional GNN models. Evaluations on the UJIIndoorLoc andUTSIndoorLoc public datasets demonstrate that MG-HGNN not only achievessuperior performance compared to several state-of-the-art methods, but alsoprovides a novel perspective for enhancing GNN-based localization methods.Ablation studies further confirm the rationality and effectiveness of theproposed framework.</description>
      <author>example@mail.com (Yibu Wang, Zhaoxin Zhang, Ning Li, Xinlong Zhao, Dong Zhao, Tianzi Zhao)</author>
      <guid isPermaLink="false">2511.07282v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>On Stealing Graph Neural Network Models</title>
      <link>http://arxiv.org/abs/2511.07170v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在查询数量极其有限的情况下提取图神经网络(GNN)模型的方法，即使存在防御措施也能有效实施。&lt;h4&gt;背景&lt;/h4&gt;当前GNN模型窃取方法严重依赖对目标模型的查询，假设没有严格的查询限制，但现实中查询数量可能受到严格限制。&lt;h4&gt;目的&lt;/h4&gt;展示如何通过非常有限的模型交互来提取GNN模型。&lt;h4&gt;方法&lt;/h4&gt;首先使攻击者能够无需直接查询目标模型就获取模型骨干，然后战略性地利用固定的查询限制来提取最信息丰富的数据。&lt;h4&gt;主要发现&lt;/h4&gt;在八个真实世界数据集上的实验证明了该方法的有效性，即使在非常有限的查询限制下，并且在已有防御模型提取措施的情况下也是如此。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了需要对GNN模型提取威胁采取稳健防御的必要性。&lt;h4&gt;翻译&lt;/h4&gt;当前的图神经网络(GNN)模型窃取方法严重依赖对目标模型的查询，假设没有严格的查询限制。然而，实际上允许的查询数量可能会受到严格限制。在本文中，我们展示了攻击者如何通过与模型非常有限的交互来提取GNN。我们的方法首先使攻击者能够在不直接查询目标模型的情况下获取模型骨干，然后战略性地利用固定的查询限制来提取最信息丰富的数据。在八个真实世界数据集上的实验证明了攻击的有效性，即使在非常有限的查询限制下，并且在已有防御模型提取措施的情况下也是如此。我们的研究结果强调了对GNN模型提取威胁需要采取稳健防御的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current graph neural network (GNN) model-stealing methods rely heavily onqueries to the victim model, assuming no hard query limits. However, inreality, the number of allowed queries can be severely limited. In this paper,we demonstrate how an adversary can extract the GNN with very limitedinteractions with the model. Our approach first enables the adversary to obtainthe model backbone without making direct queries to the victim model and thento strategically utilize a fixed query limit to extract the most informativedata. The experiments on eight real-world datasets demonstrate theeffectiveness of the attack, even under a very restricted query limit and underdefense against model extraction in place. Our findings underscore the need forrobust defenses against GNN model extraction threats.</description>
      <author>example@mail.com (Marcin Podhajski, Jan Dubiński, Franziska Boenisch, Adam Dziedzic, Agnieszka Pręgowska, Tomasz P. Michalak)</author>
      <guid isPermaLink="false">2511.07170v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Direct Molecular Polarizability Prediction with SO(3) Equivariant Local Frame GNNs</title>
      <link>http://arxiv.org/abs/2511.07087v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新型的等变图神经网络架构，用于预测分子的张量响应性质，通过局部坐标系保持SO(3)-等变性，并在分子极化率预测中展示了优于传统方法的性能。&lt;h4&gt;背景&lt;/h4&gt;传统图神经网络框架主要专注于回归标量量并从导数中推导张量性质，缺乏对分子几何结构的有效捕捉能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接预测分子张量响应性质的神经网络架构，保持旋转等变性并有效捕获几何信息。&lt;h4&gt;方法&lt;/h4&gt;设计了一种新型的等变图神经网络，通过使用局部坐标系保持SO(3)-等变性，并在局部消息传递框架内整合标量、向量和张量通道来捕获几何信息。&lt;h4&gt;主要发现&lt;/h4&gt;在QM7-X数据集上的实验表明，张量消息传递模型在预测分子极化率方面优于标量消息传递模型。&lt;h4&gt;结论&lt;/h4&gt;这项工作代表了向开发用于分子性质预测的结构化、几何感知神经模型的重要进展，为分子性质预测提供了新的方法。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种新型的等变图神经网络架构，旨在预测分子的张量响应性质。与专注于回归标量量并从导数中推导张量性质的传统框架不同，我们的方法通过使用局部坐标系来保持SO(3)-等变性。我们的GNN通过在局部消息传递框架内整合标量、向量和张量通道，有效地捕获了几何信息。为了评估模型的准确性，我们将其应用于预测QM7-X数据集中分子的极化率，并表明张量消息传递优于标量消息传递模型。这项工作朝着开发用于分子性质预测的结构化、几何感知神经模型迈出了一步。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何直接预测分子的极化率张量问题。极化率描述分子电子云对外部电场的响应能力，决定了分子间相互作用和介电行为，对理解材料光学性质和介电特性至关重要。传统方法通常通过预测标量量再求导获得张量，不够准确；而密度泛函理论虽然准确但计算成本高，尤其对大分子系统，因此需要高效的机器学习替代方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有的等变架构，如张量场网络、Cormorant等，这些工作使用球谐函数和Clebsch-Gordan张量积编码旋转对称性。但作者注意到这些方法通常通过标量导数获得张量，而非直接回归。作者还受局部参考帧概念启发，特别是Lippmann等人的工作，但改进之处在于保持独立的标量、向量和张量通道，并允许它们之间的显式控制，通过设计专门的张量消息传递机制来捕获方向性相互作用。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用局部参考帧保持SO(3)等变性，同时结合标量、向量和张量通道捕获分子几何信息。每个原子都有自己的局部坐标系，用于表示和交换张量特征。实现流程：1)为每个原子构建电荷加权PCA局部参考帧；2)初始化原子特征(标量、向量、张量)；3)通过相对旋转在节点间传递特征；4)使用边缘MLP混合张量消息；5)进行邻域聚合；6)预测每个节点的局部3×3贡献；7)旋转到全局坐标并池化，得到最终分子极化率张量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)直接预测极化率张量而非通过标量导数获得；2)使用局部参考帧保持SO(3)等变性，同时处理多种类型特征；3)设计专门的张量消息传递机制；4)在消息传递过程中保持特征形状。相比之前工作的不同：不同于传统框架回归标量再导出张量，本文直接回归秩-2张量；与Lippmann等人不同，本文保持独立通道并允许显式交互；本文通过张量消息传递捕获方向性相互作用，超越了仅使用标量消息的方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于SO(3)等变局部参考帧图神经网络的新架构，通过结合标量、向量和张量通道的消息传递机制，实现了比传统方法更准确地直接预测分子极化率张量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel equivariant graph neural network (GNN) architecturedesigned to predict the tensorial response properties of molecules. Unliketraditional frameworks that focus on regressing scalar quantities and derivetensorial properties from their derivatives, our approach maintains$SO(3)$-equivariance through the use of local coordinate frames. Our GNNeffectively captures geometric information by integrating scalar, vector, andtensor channels within a local message-passing framework. To assess theaccuracy of our model, we apply it to predict the polarizabilities of moleculesin the QM7-X dataset and show that tensorial message passing outperforms scalarmessage passing models. This work marks an advancement towards developingstructured, geometry-aware neural models for molecular property prediction.</description>
      <author>example@mail.com (Jean Philip Filling, Felix Post, Michael Wand, Denis Andrienko)</author>
      <guid isPermaLink="false">2511.07087v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>CGLE: Class-label Graph Link Estimator for Link Prediction</title>
      <link>http://arxiv.org/abs/2511.06982v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper accepted at the IEEE International Conference on Data Mining  (ICDM 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CGLE(Class-label Graph Link Estimator)框架，通过整合类级别语义信息来增强图神经网络在链接预测任务中的表现，在多个基准数据集上实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;链接预测是图挖掘中的关键任务，在社交网络、推荐系统和知识图谱补全等领域有广泛应用。然而，许多领先的图神经网络模型往往忽略了在类级别聚合的有价值语义信息。&lt;h4&gt;目的&lt;/h4&gt;解决现有GNN模型忽略类级别语义信息的局限性，提出一种新颖的框架来增强基于GNN的链接预测模型。&lt;h4&gt;方法&lt;/h4&gt;CGLE通过构建类条件链接概率矩阵，该矩阵可从真实标签或聚类获得的伪标签中推导。然后将基于类的先验与骨干GNN的结构链接嵌入连接，使用多层感知器处理组合表示进行最终预测。该方法封装在高效预处理阶段，不影响底层GNN模型的计算复杂度。&lt;h4&gt;主要发现&lt;/h4&gt;在广泛的基准数据集上验证了该方法，包括同配性和稀疏异配性图。与NCN和NCNC等强基线相比，CGLE在PubMed和DBLP等同配性数据集上HR@100提高了10多个百分点，在Chameleon稀疏异配性数据集上MRR提高了4%以上。&lt;h4&gt;结论&lt;/h4&gt;集成全局、数据驱动的语义先验是有效的，为追求越来越复杂的模型架构提供了有吸引力的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;链接预测是图挖掘中的一个关键任务，在社交网络、推荐系统和知识图谱补全中有广泛应用。然而，许多领先的图神经网络模型通常忽略了在类级别聚合的有价值语义信息。为解决这一局限，本文引入了CGLE(类标签图链接估计器)，一个旨在增强基于GNN的链接预测模型的新颖框架。CGLE通过构建类条件链接概率矩阵来运作，其中每个条目代表两个节点类别之间形成链接的概率。该矩阵从可用的真实标签或通过聚类获得的伪标签中推导得出。然后，将基于类的先验与来自骨干GNN的结构链接嵌入连接，由多层感知器处理组合表示进行最终预测。关键的是，CGLE的逻辑封装在高效的预处理阶段，不影响底层GNN模型的计算复杂度。我们在广泛的基准数据集套件上验证了我们的方法，涵盖了同配性和稀疏异配性图。结果表明，CGLE与NCN和NCNC等强基线相比取得了显著的性能提升，在PubMed和DBLP等同配性数据集上HR@100提高了10多个百分点。在稀疏异配性图上，CGLE在Chameleon数据集上实现了MRR超过4%的提升。我们的工作强调了集成全局、数据驱动的语义先验的有效性，为追求越来越复杂的模型架构提供了有吸引力的替代方案。重现我们研究成果的代码可在https://github.com/data-iitd/cgle-icdm2025获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction is a pivotal task in graph mining with wide-rangingapplications in social networks, recommendation systems, and knowledge graphcompletion. However, many leading Graph Neural Network (GNN) models oftenneglect the valuable semantic information aggregated at the class level. Toaddress this limitation, this paper introduces CGLE (Class-label Graph LinkEstimator), a novel framework designed to augment GNN-based link predictionmodels. CGLE operates by constructing a class-conditioned link probabilitymatrix, where each entry represents the probability of a link forming betweentwo node classes. This matrix is derived from either available ground-truthlabels or from pseudo-labels obtained through clustering. The resultingclass-based prior is then concatenated with the structural link embedding froma backbone GNN, and the combined representation is processed by a Multi-LayerPerceptron (MLP) for the final prediction. Crucially, CGLE's logic isencapsulated in an efficient preprocessing stage, leaving the computationalcomplexity of the underlying GNN model unaffected. We validate our approachthrough extensive experiments on a broad suite of benchmark datasets, coveringboth homophilous and sparse heterophilous graphs. The results show that CGLEyields substantial performance gains over strong baselines such as NCN andNCNC, with improvements in HR@100 of over 10 percentage points on homophilousdatasets like Pubmed and DBLP. On sparse heterophilous graphs, CGLE delivers anMRR improvement of over 4% on the Chameleon dataset. Our work underscores theefficacy of integrating global, data-driven semantic priors, presenting acompelling alternative to the pursuit of increasingly complex modelarchitectures. Code to reproduce our findings is available at:https://github.com/data-iitd/cgle-icdm2025.</description>
      <author>example@mail.com (Ankit Mazumder, Srikanta Bedathur)</author>
      <guid isPermaLink="false">2511.06982v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Dual Mamba for Node-Specific Representation Learning: Tackling Over-Smoothing with Selective State Space Modeling</title>
      <link>http://arxiv.org/abs/2511.06756v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种双Mamba增强图卷积网络(DMbaGCN)，通过整合Mamba到GNNs中，从局部和全局两个角度解决深度图神经网络中的过度平滑问题。&lt;h4&gt;背景&lt;/h4&gt;深度图神经网络中存在过度平滑的基本挑战，重复的消息传递导致节点表示变得不可区分。现有解决方案如残差连接和跳跃层虽然在一定程度上缓解了这个问题，但存在局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法无法明确建模节点表示在层间的节点特定和渐进式演化，以及不考虑全局信息的问题。&lt;h4&gt;方法&lt;/h4&gt;提出DMbaGCN框架，包含两个模块：局部状态演化Mamba(LSEMba)用于局部邻域聚合和捕获节点特定表示动态；全局上下文感知Mamba(GCAMba)利用Mamba的全局注意力能力为每个节点整合全局上下文。&lt;h4&gt;主要发现&lt;/h4&gt;DMbaGCN通过结合这些组件，增强了深度GNNs中节点的区分性，从而缓解了过度平滑问题。&lt;h4&gt;结论&lt;/h4&gt;在多个基准测试上的大量实验证明了DMbaGCN方法的有效性和效率。&lt;h4&gt;翻译&lt;/h4&gt;过度平滑仍然是深度图神经网络(GNNs)中的一个基本挑战，其中重复的消息传递导致节点表示变得不可区分。虽然现有的解决方案，如残差连接和跳跃层，在一定程度上缓解了这个问题，但它们未能明确地以节点特定和渐进的方式建模节点表示在层之间的演化。此外，这些方法没有考虑全局信息，这对于缓解过度平滑问题也至关重要。为了解决上述问题，在本文中，我们提出了一个双Mamba增强图卷积网络(DMbaGCN)，这是一个将Mamba整合到GNNs中的新框架，从局部和全局两个角度解决过度平滑问题。DMbaGCN包含两个模块：局部状态演化Mamba(LSEMba)用于局部邻域聚合，并利用Mamba的选择性状态空间建模来捕获跨层的节点特定表示动态；全局上下文感知Mamba(GCAMba)利用Mamba的全局注意力能力为每个节点整合全局上下文。通过结合这些组件，DMbaGCN增强了深度GNNs中节点的区分性，从而缓解了过度平滑问题。在多个基准测试上的大量实验证明了我们方法的有效性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over-smoothing remains a fundamental challenge in deep Graph Neural Networks(GNNs), where repeated message passing causes node representations to becomeindistinguishable. While existing solutions, such as residual connections andskip layers, alleviate this issue to some extent, they fail to explicitly modelhow node representations evolve in a node-specific and progressive manneracross layers. Moreover, these methods do not take global information intoaccount, which is also crucial for mitigating the over-smoothing problem. Toaddress the aforementioned issues, in this work, we propose a DualMamba-enhanced Graph Convolutional Network (DMbaGCN), which is a novelframework that integrates Mamba into GNNs to address over-smoothing from bothlocal and global perspectives. DMbaGCN consists of two modules: the LocalState-Evolution Mamba (LSEMba) for local neighborhood aggregation and utilizingMamba's selective state space modeling to capture node-specific representationdynamics across layers, and the Global Context-Aware Mamba (GCAMba) thatleverages Mamba's global attention capabilities to incorporate global contextfor each node. By combining these components, DMbaGCN enhances nodediscriminability in deep GNNs, thereby mitigating over-smoothing. Extensiveexperiments on multiple benchmarks demonstrate the effectiveness and efficiencyof our method.</description>
      <author>example@mail.com (Xin He, Yili Wang, Yiwei Dai, Xin Wang)</author>
      <guid isPermaLink="false">2511.06756v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>S-DAG: A Subject-Based Directed Acyclic Graph for Multi-Agent Heterogeneous Reasoning</title>
      <link>http://arxiv.org/abs/2511.06727v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新型框架，通过在学科层面进行细粒度分析，并配备专门的多智能体协作策略来解决异构问题推理。研究构建了基于学科的有向无环图(S-DAG)，并根据模型的专业评分选择最佳匹配模型，实现图结构的多智能体协作。实验表明该方法在准确性和效率上均优于现有基线。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在复杂推理问题上已取得显著成效，但其有效性高度依赖于任务的具体性质，特别是所需的领域知识。现有方法如专家混合模型通常在任务级别操作，对于涉及多个学科的异构问题来说过于粗糙。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效解决涉及多学科的复杂推理问题的框架，通过在学科层面进行细粒度分析和多智能体协作，提高模型在复杂多学科问题上的推理能力。&lt;h4&gt;方法&lt;/h4&gt;1. 使用图神经网络识别输入查询的相关学科并推断相互依赖关系，生成基于学科的有向无环图(S-DAG)；2. 为每个模型分配学科特定的专业评分，选择最佳模型匹配S-DAG中的相应学科；3. 实现基于图结构的多智能体协作，信息从起始模型通过S-DAG流向结束模型。&lt;h4&gt;主要发现&lt;/h4&gt;1. 整理并发布了标准基准测试(MMLU-Pro, GPQA, MedMCQA)的多学科子集，更好地反映复杂真实的推理任务；2. 大量实验表明，该方法在准确性和效率上都显著优于现有的任务级模型选择和多智能体协作基线。&lt;h4&gt;结论&lt;/h4&gt;这些结果强调了在解决复杂和多学科问题时，学科感知推理和结构化协作的有效性。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在复杂推理问题上已取得了令人印象深刻的性能。它们的有效性高度依赖于任务的具体性质，特别是所需的领域知识。现有方法，如专家混合模型，通常在任务级别操作；对于涉及多个学科的异构问题来说，它们过于粗糙。本文提出了一种新型框架，在学科层面进行细粒度分析，并配备专门的多智能体协作策略来解决异构问题推理。具体来说，给定一个输入查询，我们首先使用图神经网络识别相关学科并推断它们之间的相互依赖关系，生成一个基于学科的有向无环图(S-DAG)，其中节点代表学科，边编码信息流。然后，我们通过为每个模型分配学科特定的专业评分来分析大型语言模型，并为S-DAG的相应学科选择表现最佳的模型。这种学科-模型匹配实现了图结构的多智能体协作，信息从起始模型通过S-DAG流向结束模型。我们整理并发布了标准基准测试(MMLU-Pro, GPQA, MedMCQA)的多学科子集，以更好地反映复杂、真实的推理任务。大量实验表明，我们的方法在准确性和效率上都显著优于现有的任务级模型选择和多智能体协作基线。这些结果强调了在解决复杂和多学科问题时，学科感知推理和结构化协作的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have achieved impressive performance in complexreasoning problems. Their effectiveness highly depends on the specific natureof the task, especially the required domain knowledge. Existing approaches,such as mixture-of-experts, typically operate at the task level; they are toocoarse to effectively solve the heterogeneous problems involving multiplesubjects. This work proposes a novel framework that performs fine-grainedanalysis at subject level equipped with a designated multi-agent collaborationstrategy for addressing heterogeneous problem reasoning. Specifically, given aninput query, we first employ a Graph Neural Network to identify the relevantsubjects and infer their interdependencies to generate an \textit{Subject-basedDirected Acyclic Graph} (S-DAG), where nodes represent subjects and edgesencode information flow. Then we profile the LLM models by assigning each modela subject-specific expertise score, and select the top-performing one formatching corresponding subject of the S-DAG. Such subject-model matchingenables graph-structured multi-agent collaboration where information flows fromthe starting model to the ending model over S-DAG. We curate and releasemulti-subject subsets of standard benchmarks (MMLU-Pro, GPQA, MedMCQA) tobetter reflect complex, real-world reasoning tasks. Extensive experiments showthat our approach significantly outperforms existing task-level model selectionand multi-agent collaboration baselines in accuracy and efficiency. Theseresults highlight the effectiveness of subject-aware reasoning and structuredcollaboration in addressing complex and multi-subject problems.</description>
      <author>example@mail.com (Jiangwen Dong, Zehui Lin, Wanyu Lin, Mingjin Zhang)</author>
      <guid isPermaLink="false">2511.06727v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Magnitude-Modulated Equivariant Adapter for Parameter-Efficient Fine-Tuning of Equivariant Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2511.06696v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的等变微调方法MMEA，通过轻量级标量门控按阶和多重性调制特征幅度，在保持严格等变性的同时，提高了能量和力预测性能，且训练参数更少。&lt;h4&gt;背景&lt;/h4&gt;基于球谐函数的预训练等变图神经网络为计算密集型从头计算方法提供了高效准确的替代方案，但适应新任务需要微调。传统PEFT技术如Adapters和LoRA会破坏对称性，与等变架构不兼容。ELoRA作为首个等变PEFT方法，保留了较高的张量阶自由度，可能扰动预训练特征分布。&lt;h4&gt;目的&lt;/h4&gt;解决ELoRA保留高自由度导致的问题，提出一种不破坏对称性的等变微调方法。&lt;h4&gt;方法&lt;/h4&gt;提出幅度调制等变适配器（MMEA），采用轻量级标量门控来按阶和多重性调制特征幅度，保持严格的等变性。&lt;h4&gt;主要发现&lt;/h4&gt;MMEA在多个基准测试中持续提高了能量和力预测性能，达到最先进水平，同时训练的参数比竞争方法更少。&lt;h4&gt;结论&lt;/h4&gt;在许多实际场景中，调制通道幅度足以使等变模型适应新的化学环境而不会破坏对称性，为等变PEFT设计指明了新范式。&lt;h4&gt;翻译&lt;/h4&gt;基于球谐函数的预训练等变图神经网络为计算密集型的从头计算方法提供了高效准确的替代方案，但适应它们到新任务和化学环境仍需要微调。传统的参数高效微调技术如Adapters和LoRA通常会破坏对称性，使其与那些等变架构不兼容。最近提出的ELoRA是首个等变PEFT方法，它在许多基准测试中实现了改进的参数效率和性能。然而，它在每个张量阶中保留的相对较高的自由度仍然会扰动预训练的特征分布并最终降低性能。为解决这一问题，我们提出了幅度调制等变适配器（MMEA），一种新的等变微调方法，它采用轻量级标量门控来按阶和多重性调制特征幅度。我们证明了MMEA保持了严格的等变性，并且在多个基准测试中，持续改进了能量和力预测到最先进水平，同时训练的参数比竞争方法更少。这些结果表明，在许多实际场景中，调制通道幅度足以使等变模型适应新的化学环境而不会破坏对称性，为等变PEFT设计指明了新的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pretrained equivariant graph neural networks based on spherical harmonicsoffer efficient and accurate alternatives to computationally expensiveab-initio methods, yet adapting them to new tasks and chemical environmentsstill requires fine-tuning. Conventional parameter-efficient fine-tuning (PEFT)techniques, such as Adapters and LoRA, typically break symmetry, making themincompatible with those equivariant architectures. ELoRA, recently proposed, isthe first equivariant PEFT method. It achieves improved parameter efficiencyand performance on many benchmarks. However, the relatively high degrees offreedom it retains within each tensor order can still perturb pretrainedfeature distributions and ultimately degrade performance. To address this, wepresent Magnitude-Modulated Equivariant Adapter (MMEA), a novel equivariantfine-tuning method which employs lightweight scalar gating to modulate featuremagnitudes on a per-order and per-multiplicity basis. We demonstrate that MMEApreserves strict equivariance and, across multiple benchmarks, consistentlyimproves energy and force predictions to state-of-the-art levels while trainingfewer parameters than competing approaches. These results suggest that, in manypractical scenarios, modulating channel magnitudes is sufficient to adaptequivariant models to new chemical environments without breaking symmetry,pointing toward a new paradigm for equivariant PEFT design.</description>
      <author>example@mail.com (Dian Jin, Yancheng Yuan, Xiaoming Tao)</author>
      <guid isPermaLink="false">2511.06696v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>GNN-Enabled Robust Hybrid Beamforming with Score-Based CSI Generation and Denoising</title>
      <link>http://arxiv.org/abs/2511.06663v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出利用图神经网络和基于分数的生成模型解决不完美信道状态信息(CSI)条件下的混合波束成形(HBF)问题，开发了HMGAT、NCSN和DeBERT三种模型，在DeepMIMO数据集上验证了其优越性能。&lt;h4&gt;背景&lt;/h4&gt;准确的信道状态信息(CSI)对于混合波束成形(HBF)任务至关重要，但在实际无线通信系统中获取高分辨率的CSI仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;解决在不完美CSI条件下实现鲁棒HBF的问题，提高无线通信系统的性能和可靠性。&lt;h4&gt;方法&lt;/h4&gt;1. 开发混合消息图注意力网络(HMGAT)，通过节点级和边级消息传递更新节点和边特征；2. 设计基于BERT的噪声条件分数网络(NCSN)，学习高分辨率CSI分布并促进数据增强；3. 提出去噪分数网络(DSN)框架及其实现DeBERT，可在任意信道错误级别下对不完美CSI去噪。&lt;h4&gt;主要发现&lt;/h4&gt;在DeepMIMO城市数据集上的实验表明，所提模型在各种HBF任务中具有优越的泛化能力、可扩展性和鲁棒性，适用于完美和不完美CSI条件。&lt;h4&gt;结论&lt;/h4&gt;结合图神经网络和基于分数的生成模型可以有效解决不完美CSI条件下的混合波束成形问题，提高系统鲁棒性和性能。&lt;h4&gt;翻译&lt;/h4&gt;准确的信道状态信息(CSI)对于混合波束成形(HBF)任务至关重要。然而，在实际无线通信系统中，获取高分辨率的CSI仍然具有挑战性。为了解决这个问题，我们提议利用图神经网络(GNNs)和基于分数的生成模型，在不完美CSI条件下实现鲁棒的HBF。首先，我们开发了混合消息图注意力网络(HMGAT)，通过节点级和边级消息传递来更新节点和边特征。其次，我们设计了一个基于Transformer的双向编码器表示(BERT)的噪声条件分数网络(NCSN)，用于学习高分辨率CSI的分布，促进CSI生成和数据增强，从而进一步提高HMGAT的性能。最后，我们提出了一个去噪分数网络(DSN)框架及其实现形式，称为DeBERT，它可以在任意信道错误级别下对不完美的CSI去噪，从而促进鲁棒的HBF。在DeepMIMO城市数据集上的实验证明了所提出的模型在各种HBF任务中具有优越的泛化能力、可扩展性和鲁棒性，无论是在完美还是不完美的CSI条件下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate Channel State Information (CSI) is critical for Hybrid Beamforming(HBF) tasks. However, obtaining high-resolution CSI remains challenging inpractical wireless communication systems. To address this issue, we propose toutilize Graph Neural Networks (GNNs) and score-based generative models toenable robust HBF under imperfect CSI conditions. Firstly, we develop theHybrid Message Graph Attention Network (HMGAT) which updates both node and edgefeatures through node-level and edge-level message passing. Secondly, we designa Bidirectional Encoder Representations from Transformers (BERT)-based NoiseConditional Score Network (NCSN) to learn the distribution of high-resolutionCSI, facilitating CSI generation and data augmentation to further improveHMGAT's performance. Finally, we present a Denoising Score Network (DSN)framework and its instantiation, termed DeBERT, which can denoise imperfect CSIunder arbitrary channel error levels, thereby facilitating robust HBF.Experiments on DeepMIMO urban datasets demonstrate the proposed models'superior generalization, scalability, and robustness across various HBF taskswith perfect and imperfect CSI.</description>
      <author>example@mail.com (Yuhang Li, Yang Lu, Bo Ai, Zhiguo Ding, Dusit Niyato, Arumugam Nallanathan)</author>
      <guid isPermaLink="false">2511.06663v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Dual-branch Spatial-Temporal Self-supervised Representation for Enhanced Road Network Learning</title>
      <link>http://arxiv.org/abs/2511.06633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DST的双分支时空自监督表示框架，用于解决道路网络表示学习中的空间异质性和时间动态性问题。&lt;h4&gt;背景&lt;/h4&gt;道路网络表示学习随着各种时空任务的涌现而受到越来越多的关注，现有方法主要利用图神经网络和对比学习来表征路段的空间结构。&lt;h4&gt;目的&lt;/h4&gt;解决道路网络的空间异质性和时间动态性对自监督GNN邻域平滑机制带来的挑战，提高道路网络表示的效果。&lt;h4&gt;方法&lt;/h4&gt;DST框架包含空间和时间两个分支：空间分支设计混合跳转过渡矩阵用于图卷积，并通过对比普通道路网络与超图的表示来捕捉长程关系；时间分支使用因果Transformer在交通动态序列上进行下一个令牌预测，并通过区分工作日和周末的交通模式进行正则化。&lt;h4&gt;主要发现&lt;/h4&gt;与最先进方法的广泛实验验证了DST框架的优越性，全面的时空建模使DST在零学习场景中表现出色。&lt;h4&gt;结论&lt;/h4&gt;DST框架通过双分支时空自监督表示有效解决了道路网络表示学习中的挑战，在各种应用场景中具有优越性能。&lt;h4&gt;翻译&lt;/h4&gt;道路网络表示学习随着各种时空任务的涌现而受到研究人员和从业者的越来越多的关注。最近的高级方法利用图神经网络和对比学习来表征路段的空间结构，采用自监督范式。然而，道路网络的空间异质性和时间动态性对自监督GNN的邻域平滑机制提出了严重挑战。为解决这些问题，我们提出了一种双分支时空自监督表示框架DST，用于增强道路表示。一方面，DST设计了一个混合跳转过渡矩阵用于图卷积，从轨迹中整合道路的动态关系。此外，DST通过空间自监督方式对比普通道路网络与超图的表示，超图基于三种类型的超边构建以捕捉长程关系。另一方面，DST在基于因果Transformer的交通动态序列上执行下一个令牌预测作为时间自监督任务，并通过区分工作日和周末的交通模式进行进一步正则化。与最先进方法的广泛实验验证了我们提出框架的优越性。此外，全面的时空建模使DST在零学习场景中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Road network representation learning (RNRL) has attracted increasingattention from both researchers and practitioners as various spatiotemporaltasks are emerging. Recent advanced methods leverage Graph Neural Networks(GNNs) and contrastive learning to characterize the spatial structure of roadsegments in a self-supervised paradigm. However, spatial heterogeneity andtemporal dynamics of road networks raise severe challenges to the neighborhoodsmoothing mechanism of self-supervised GNNs. To address these issues, wepropose a $\textbf{D}$ual-branch $\textbf{S}$patial-$\textbf{T}$emporalself-supervised representation framework for enhanced road representations,termed as DST. On one hand, DST designs a mix-hop transition matrix for graphconvolution to incorporate dynamic relations of roads from trajectories.Besides, DST contrasts road representations of the vanilla road network againstthat of the hypergraph in a spatial self-supervised way. The hypergraph isnewly built based on three types of hyperedges to capture long-range relations.On the other hand, DST performs next token prediction as the temporalself-supervised task on the sequences of traffic dynamics based on a causalTransformer, which is further regularized by differentiating traffic modes ofweekdays from those of weekends. Extensive experiments against state-of-the-artmethods verify the superiority of our proposed framework. Moreover, thecomprehensive spatiotemporal modeling facilitates DST to excel in zero-shotlearning scenarios.</description>
      <author>example@mail.com (Qinghong Guo, Yu Wang, Ji Cao, Tongya Zheng, Junshu Dai, Bingde Hu, Shunyu Liu, Canghong Jin)</author>
      <guid isPermaLink="false">2511.06633v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Fixed Depth: Adaptive Graph Neural Networks for Node Classification Under Varying Homophily</title>
      <link>http://arxiv.org/abs/2511.06608v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自适应深度的图神经网络架构，能够根据节点的局部同质性水平和邻域结构动态选择聚合深度，从而在同类和异类图中都能有效提升节点分类性能。&lt;h4&gt;背景&lt;/h4&gt;传统图神经网络在节点分类任务中取得了显著成功，但在异类图（连接节点通常属于不同标签或属性）上效果会下降。现有方法存在局限性：大多数模型对所有节点应用固定聚合深度，且多数方法仅针对同类或异类场景设计，缺乏通用性。&lt;h4&gt;目的&lt;/h4&gt;解决传统GNN在异类图上性能下降的问题，开发一种能够自适应处理同类和异类图结构的GNN架构，通过动态选择节点特定的聚合深度来提升节点分类性能。&lt;h4&gt;方法&lt;/h4&gt;开发了一个理论框架，将局部结构和标签特征与节点级别的信息传播动力学联系起来；基于此分析提出一种新型自适应深度GNN架构，使用基于理论指标的度量动态选择节点特定的聚合深度，使模型能够同时适应同类和异类模式。&lt;h4&gt;主要发现&lt;/h4&gt;最优聚合深度因节点而异，且对于保留类别判别信息至关重要；通过动态选择节点特定的聚合深度，模型能够在不同同质性的图中保持良好性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的自适应深度GNN架构在不同基准测试中一致地提升了标准GNN骨干网络的性能，为处理具有不同同质性的图结构提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNNs）在解决节点分类任务方面已取得显著成功。然而，传统GNNs在异类图上的效果会下降，在这些图中，连接的节点通常属于不同的标签或属性。尽管近期工作已引入了提高GNN在异质性条件下性能的机制，但仍存在某些关键局限性。大多数现有模型对所有节点应用固定的聚合深度，忽略了节点可能需要根据其局部同质性水平和邻域结构采用不同的传播深度。此外，许多方法专门针对同类或异类场景设计，缺乏在两种模式下的通用性。为解决这些挑战，我们开发了一个理论框架，将局部结构和标签特征与节点级别的信息传播动力学联系起来。我们的分析表明，最优聚合深度因节点而异，对于保留类别判别信息至关重要。受此见解指导，我们提出了一种新型自适应深度GNN架构，使用基于理论的度量动态选择节点特定的聚合深度。我们的方法在统一模型中能够无缝适应同类和异类模式。大量实验证明，我们的方法在不同基准测试中一致地提升了标准GNN骨干网络的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved significant success in addressingnode classification tasks. However, the effectiveness of traditional GNNsdegrades on heterophilic graphs, where connected nodes often belong todifferent labels or properties. While recent work has introduced mechanisms toimprove GNN performance under heterophily, certain key limitations still exist.Most existing models apply a fixed aggregation depth across all nodes,overlooking the fact that nodes may require different propagation depths basedon their local homophily levels and neighborhood structures. Moreover, manymethods are tailored to either homophilic or heterophilic settings, lacking theflexibility to generalize across both regimes. To address these challenges, wedevelop a theoretical framework that links local structural and labelcharacteristics to information propagation dynamics at the node level. Ouranalysis shows that optimal aggregation depth varies across nodes and iscritical for preserving class-discriminative information. Guided by thisinsight, we propose a novel adaptive-depth GNN architecture that dynamicallyselects node-specific aggregation depths using theoretically grounded metrics.Our method seamlessly adapts to both homophilic and heterophilic patternswithin a unified model. Extensive experiments demonstrate that our approachconsistently enhances the performance of standard GNN backbones across diversebenchmarks.</description>
      <author>example@mail.com (Asela Hevapathige, Asiri Wijesinghe, Ahad N. Zehmakan)</author>
      <guid isPermaLink="false">2511.06608v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Initial Residual Connections for GNNs with Theoretical Guarantees</title>
      <link>http://arxiv.org/abs/2511.06598v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the full version of the paper accepted to the 40th Annual  AAAI Conference on Artificial Intelligence (AAAI-2026)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了图神经网络中的自适应残差连接方案，证明了这种方法可以防止过度平滑并保持嵌入表达能力，实验验证了该方法在异质图上的优越性，并提出了时间复杂度更低的启发式变体。&lt;h4&gt;背景&lt;/h4&gt;消息传递是图神经网络的核心操作，每个节点通过聚合邻居信息来更新其嵌入表示。然而，在深度架构中，这个过程常常导致表达能力减弱。&lt;h4&gt;目的&lt;/h4&gt;研究一种自适应残差方案，其中不同节点具有不同的残差强度，证明这种方法可以防止过度平滑，确保嵌入的Dirichlet能量保持远离零的状态。&lt;h4&gt;方法&lt;/h4&gt;使用自适应残差连接方案，不同节点具有不同的残差强度。引入一种变体，其中残差强度不是学习得到的，而是启发式设置的，以提高时间复杂度。&lt;h4&gt;主要发现&lt;/h4&gt;这种方法可以防止过度平滑，嵌入的Dirichlet能量保持远离零；这是自适应设置以及带激活函数的静态残差连接的第一个理论保证；广泛的实验表明，这种方法优于标准和最先进的消息传递机制，特别是在异质图上；启发式设置的残差强度与可学习版本表现相当。&lt;h4&gt;结论&lt;/h4&gt;自适应残差方案在防止过度平滑方面有效，在异质图上表现尤其出色，启发式设置的残差强度与可学习版本表现相当。&lt;h4&gt;翻译&lt;/h4&gt;消息传递是图神经网络的核心操作，其中每个节点通过聚合来自邻居的信息来更新其嵌入表示。然而，在深度架构中，这个过程常常导致表达能力减弱。一个流行的解决方案是使用残差连接，即将当前（或初始）层的输入添加到聚合的邻居信息中，以在层之间保留嵌入表示。遵循最近的研究路线，我们研究了一种自适应残差方案，其中不同节点具有不同的残差强度。我们证明这种方法可以防止过度平滑；特别是，我们展示了嵌入的Dirichlet能量保持远离零。这不仅是对自适应设置的理论保证，也是对带激活函数的静态残差连接（残差强度在节点间共享）的第一个理论保证。此外，大量实验表明，这种方法优于标准和最先进的消息传递机制，特别是在异质图上。为了提高我们方法的时间复杂度，我们引入了一种变体，其中残差强度不是学习得到的，而是启发式设置的，这种选择与可学习版本表现相当。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Message passing is the core operation in graph neural networks, where eachnode updates its embeddings by aggregating information from its neighbors.However, in deep architectures, this process often leads to diminishedexpressiveness. A popular solution is to use residual connections, where theinput from the current (or initial) layer is added to aggregated neighborinformation to preserve embeddings across layers. Following a recent line ofresearch, we investigate an adaptive residual scheme in which different nodeshave varying residual strengths. We prove that this approach preventsoversmoothing; particularly, we show that the Dirichlet energy of theembeddings remains bounded away from zero. This is the first theoreticalguarantee not only for the adaptive setting, but also for static residualconnections (where residual strengths are shared across nodes) with activationfunctions. Furthermore, extensive experiments show that this adaptive approachoutperforms standard and state-of-the-art message passing mechanisms,especially on heterophilic graphs. To improve the time complexity of ourapproach, we introduce a variant in which residual strengths are not learnedbut instead set heuristically, a choice that performs as well as the learnableversion.</description>
      <author>example@mail.com (Mohammad Shirzadi, Ali Safarpoor Dehkordi, Ahad N. Zehmakan)</author>
      <guid isPermaLink="false">2511.06598v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>How Wide and How Deep? Mitigating Over-Squashing of GNNs via Channel Capacity Constrained Estimation</title>
      <link>http://arxiv.org/abs/2511.06443v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 11 figures. Author manuscript accepted for the 40th Annual  AAAI Conference on Artificial Intelligence (AAAI-26), January 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为信道容量约束估计(C3E)的新框架，用于解决图神经网络中的过度压缩问题，通过信息论方法优化隐藏维度和传播深度的选择。&lt;h4&gt;背景&lt;/h4&gt;现有的图神经网络通常依赖于对隐藏维度和传播深度的启发式选择，这往往导致传播过程中的严重信息丢失，称为过度压缩。&lt;h4&gt;目的&lt;/h4&gt;解决图神经网络中的过度压缩问题，提出一种更科学的方法来选择隐藏维度和传播深度。&lt;h4&gt;方法&lt;/h4&gt;提出信道容量约束估计(C3E)框架，将隐藏维度和深度的选择构建为基于信息论的非线性规划问题，通过将谱图神经网络建模为通信信道，将信道容量与隐藏维度、传播深度、传播机制和图结构直接联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;1) C3E估计的隐藏维度和深度可减轻过度压缩并改进表示学习；2) 过度压缩是由于表示矩阵中信息的累积压缩造成的；3) 增加隐藏维度可减轻信息压缩，而传播深度的作用更为微妙；4) 存在信息压缩与表示复杂性之间的基本平衡。&lt;h4&gt;结论&lt;/h4&gt;C3E框架能够有效解决图神经网络中的过度压缩问题，通过基于信息论的方法优化网络参数，提高表示学习效果。&lt;h4&gt;翻译&lt;/h4&gt;现有的图神经网络通常依赖于对隐藏维度和传播深度的启发式选择，这通常会导致传播过程中的严重信息丢失，称为过度压缩。为解决这一问题，我们提出了信道容量约束估计(C3E)，一个新颖的框架，将隐藏维度和深度的选择构建为一个基于信息论的非线性规划问题。通过将谱图神经网络建模为通信信道，我们的方法直接将信道容量与隐藏维度、传播深度、传播机制和图结构联系起来。在九个公共数据集上的大量实验表明，通过C3E估计的隐藏维度和深度可以减轻过度压缩并持续改进表示学习。实验结果显示，过度压缩是由于表示矩阵中信息的累积压缩造成的。此外，我们的研究表明增加隐藏维度确实可以减轻信息压缩，而传播深度的作用更为微妙，揭示了信息压缩与表示复杂性之间的基本平衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing graph neural networks typically rely on heuristic choices for hiddendimensions and propagation depths, which often lead to severe information lossduring propagation, known as over-squashing. To address this issue, we proposeChannel Capacity Constrained Estimation (C3E), a novel framework thatformulates the selection of hidden dimensions and depth as a nonlinearprogramming problem grounded in information theory. Through modeling spectralgraph neural networks as communication channels, our approach directly connectschannel capacity to hidden dimensions, propagation depth, propagationmechanism, and graph structure. Extensive experiments on nine public datasetsdemonstrate that hidden dimensions and depths estimated by C3E can mitigateover-squashing and consistently improve representation learning. Experimentalresults show that over-squashing occurs due to the cumulative compression ofinformation in representation matrices. Furthermore, our findings show thatincreasing hidden dimensions indeed mitigate information compression, while therole of propagation depth is more nuanced, uncovering a fundamental balancebetween information compression and representation complexity.</description>
      <author>example@mail.com (Zinuo You, Jin Zheng, John Cartlidge)</author>
      <guid isPermaLink="false">2511.06443v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Privacy-Preserving Federated Learning for Fair and Efficient Urban Traffic Optimization</title>
      <link>http://arxiv.org/abs/2511.06363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review at IEEE journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FedFair-Traffic是一种隐私保护的联邦学习框架，能够同时优化出行效率、交通公平性和差分隐私保护。通过整合图神经网络与差分隐私机制及公平约束，该框架在实验中显著减少了平均旅行时间，提高了交通公平性，提供了高隐私保护，并大幅降低了通信开销。&lt;h4&gt;背景&lt;/h4&gt;城市交通优化面临在交通效率和隐私保护间平衡的挑战，同时需要考虑社会经济多样性的交通公平分配。现有集中式交通管理方案侵犯用户位置隐私并加剧交通不平等，而现有联邦学习框架未考虑多目标交通设置中的公平约束。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为FedFair-Traffic的隐私保护联邦学习框架，联合并同时优化出行效率、交通公平性和差分隐私保护。&lt;h4&gt;方法&lt;/h4&gt;首次整合三个冲突目标改善城市交通系统。通过集成图神经网络与差分隐私机制和使用基尼系数的公平约束，实现相关车辆间的协作学习与数据本地化。采用梯度裁剪和噪声注入的联邦聚合方法提供差分隐私，并优化效率-公平权衡的帕累托有效解决方案。&lt;h4&gt;主要发现&lt;/h4&gt;在METR-LA交通数据集上的实验显示：FedFair-Traffic比集中式基线减少平均旅行时间7%（14.2分钟），提高交通公平性73%（基尼系数0.78），提供高隐私保护（隐私分数0.8），并减少89%的通信开销。&lt;h4&gt;结论&lt;/h4&gt;FedFair-Traffic是可扩展的隐私感知智能城市基础设施，在都市交通流量控制和联邦交通网络中具有应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;城市交通优化面临着在交通效率和隐私保护之间取得平衡的复杂性威胁，以及基于社会经济多样化社区的交通公平分配问题。当前集中式交通管理方案侵犯了用户位置隐私，并通过提供劣势路线建议进一步加剧了交通不平等，而当前联邦学习框架在多目标交通设置中未考虑公平约束。本研究提出了一种隐私保护的联邦学习框架，称为FedFair-Traffic，它联合并同时优化出行效率、交通公平性和差分隐私保护。这是首次尝试整合三个冲突目标以改善城市交通系统。所提出的方法通过将图神经网络与差分隐私机制和使用基尼系数的公平约束相结合，实现了相关车辆之间的协作学习和数据本地化，采用多目标优化。该框架使用梯度裁剪和噪声注入的联邦聚合方法提供差分隐私，并优化效率-公平权衡的帕累托有效解决方案。在METR-LA交通数据集上的真实世界综合实验表明，与集中式基线相比，FedFair-Traffic可以减少7%（14.2分钟）的平均旅行时间，提高73%的交通公平性（基尼系数，0.78），并提供高隐私保护（隐私分数，0.8），同时减少89%的通信开销。这些结果表明，FedFair-Traffic是一种可扩展的隐私感知智能城市基础设施，在都市交通流量控制和联邦交通网络中可能有应用案例。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The optimization of urban traffic is threatened by the complexity ofachieving a balance between transport efficiency and the maintenance ofprivacy, as well as the equitable distribution of traffic based onsocioeconomically diverse neighborhoods. Current centralized traffic managementschemes invade user location privacy and further entrench traffic disparity byoffering disadvantaged route suggestions, whereas current federated learningframeworks do not consider fairness constraints in multi-objective trafficsettings. This study presents a privacy-preserving federated learningframework, termed FedFair-Traffic, that jointly and simultaneously optimizestravel efficiency, traffic fairness, and differential privacy protection. Thisis the first attempt to integrate three conflicting objectives to improve urbantransportation systems. The proposed methodology enables collaborative learningbetween related vehicles with data locality by integrating Graph NeuralNetworks with differential privacy mechanisms ($\epsilon$-privacy guarantees)and Gini coefficient-based fair constraints using multi-objective optimization.The framework uses federated aggregation methods of gradient clipping and noiseinjection to provide differential privacy and optimize Pareto-efficientsolutions for the efficiency-fairness tradeoff. Real-world comprehensiveexperiments on the METR-LA traffic dataset showed that FedFair-Traffic canreduce the average travel time by 7\% (14.2 minutes) compared with theircentralized baselines, promote traffic fairness by 73\% (Gini coefficient,0.78), and offer high privacy protection (privacy score, 0.8) with an 89\%reduction in communication overhead. These outcomes demonstrate thatFedFair-Traffic is a scalable privacy-aware smart city infrastructure withpossible use-cases in metropolitan traffic flow control and federatedtransportation networks.</description>
      <author>example@mail.com (Rathin Chandra Shit, Sharmila Subudhi)</author>
      <guid isPermaLink="false">2511.06363v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Multimodal Misinformation Detection by Replaying the Whole Story from Image Modality Perspective</title>
      <link>http://arxiv.org/abs/2511.06284v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2026. 13 pages, 6 figures. Code:  https://github.com/wangbing1416/RETSIMD&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RETSIMD的新方法用于多模态虚假信息检测，该方法基于文本比图像提供更多信息的观察，通过分割文本并生成增强图像来提高检测效果。&lt;h4&gt;背景&lt;/h4&gt;多模态虚假信息检测(MMD)是检测社交媒体中包含虚假信息的任务，这些帖子通常包含文本和图像两种模态。&lt;h4&gt;目的&lt;/h4&gt;提高多模态虚假信息检测的准确性，通过利用文本信息生成增强图像来弥补图像模态信息不足的问题。&lt;h4&gt;方法&lt;/h4&gt;将文本分割成多个片段，每个片段描述一个可由图像呈现的场景；将这些片段输入预训练的文本到图像生成器生成增强图像；整合文本-图像和图像-标签互信息的辅助目标；在辅助数据集上后训练生成器；定义图像间的三种启发式关系构建图结构，使用图神经网络生成融合特征。&lt;h4&gt;主要发现&lt;/h4&gt;文本模态在虚假信息检测中比图像模态提供更多信息；图像模态对MMD任务的贡献较小；通过文本分割和图像增强可以提高虚假信息检测的准确性。&lt;h4&gt;结论&lt;/h4&gt;RETSIMD方法通过利用文本信息生成增强图像，并结合图神经网络进行特征融合，能够有效提高多模态虚假信息检测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;多模态虚假信息检测(MMD)是指检测涉及虚假信息的社交媒体帖子，这些帖子通常包含文本和图像模态。然而，通过观察MMD帖子，我们认为文本模态可能比图像模态提供更多信息，因为文本通常描述了当前帖子的整体事件/故事，而图像往往只呈现部分场景。我们初步的经验结果表明图像模态确实对MMD贡献较少。基于这一想法，我们提出了一种名为RETSIMD的新MMD方法。具体来说，我们假设每段文本可以分为几个片段，每个文本片段描述了一个可以通过图像呈现的部分场景。因此，我们将文本分割成一系列片段，并将这些片段输入预训练的文本到图像生成器，以增强一系列图像。我们还整合了两个关于文本-图像和图像-标签互信息的辅助目标，并在辅助的文本到图像生成基准数据集上对生成器进行后训练。此外，我们通过定义图像之间的三种启发式关系提出了一种图结构，并使用图神经网络生成融合特征。大量的经验结果验证了RETSIMD的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Misinformation Detection (MMD) refers to the task of detectingsocial media posts involving misinformation, where the post often contains textand image modalities. However, by observing the MMD posts, we hold that thetext modality may be much more informative than the image modality because thetext generally describes the whole event/story of the current post but theimage often presents partial scenes only. Our preliminary empirical resultsindicate that the image modality exactly contributes less to MMD. Upon thisidea, we propose a new MMD method named RETSIMD. Specifically, we suppose thateach text can be divided into several segments, and each text segment describesa partial scene that can be presented by an image. Accordingly, we split thetext into a sequence of segments, and feed these segments into a pre-trainedtext-to-image generator to augment a sequence of images. We further incorporatetwo auxiliary objectives concerning text-image and image-label mutualinformation, and further post-train the generator over an auxiliarytext-to-image generation benchmark dataset. Additionally, we propose a graphstructure by defining three heuristic relationships between images, and use agraph neural network to generate the fused features. Extensive empiricalresults validate the effectiveness of RETSIMD.</description>
      <author>example@mail.com (Bing Wang, Ximing Li, Yanjun Wang, Changchun Li, Lin Yuanbo Wu, Buyu Wang, Shengsheng Wang)</author>
      <guid isPermaLink="false">2511.06284v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Resilience Inference for Supply Chains with Hypergraph Neural Network</title>
      <link>http://arxiv.org/abs/2511.06208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为SC-RIHN的新型超图网络模型，用于推断供应链韧性，解决了现有方法无法有效捕捉供应链网络中高阶多实体依赖关系的问题。实验表明，该方法在合成基准测试中显著优于传统方法，为复杂供应链系统的早期风险评估提供了实用工具。&lt;h4&gt;背景&lt;/h4&gt;供应链对全球经济稳定至关重要，但干扰会迅速通过网络传播造成重大经济影响。准确及时地推断供应链韧性（即在干扰期间维持核心功能的能力）对主动风险缓解和稳健网络设计至关重要。&lt;h4&gt;目的&lt;/h4&gt;定义并解决供应链韧性推断（SCRI）问题，即使用超图拓扑和观察到的库存轨迹预测供应链韧性，而不使用显式动态方程。&lt;h4&gt;方法&lt;/h4&gt;提出供应链韧性推断超图网络（SC-RIHN），这是一种基于超图的新型模型，利用基于集合的编码和超图消息传递来捕获多方企业-产品交互关系。&lt;h4&gt;主要发现&lt;/h4&gt;全面的实验表明，SC-RIHN在合成基准测试中显著优于传统的MLP、代表性的图神经网络变体和ResInf基线方法，凸显了其在复杂供应链系统中进行实际、早期风险评估的潜力。&lt;h4&gt;结论&lt;/h4&gt;SC-RIHN模型为供应链韧性推断提供了有效解决方案，能够捕捉供应链网络中的复杂交互关系，无需明确的系统动力学方程，具有重要的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;供应链是全球经济稳定的重要组成部分，然而干扰可以通过相互关联的网络迅速传播，造成重大经济影响。准确及时地推断供应链韧性——即在干扰期间维持核心功能的能力——对于主动风险缓解和稳健网络设计至关重要。然而，现有方法缺乏在没有明确系统动力学的情况下有效推断供应链韧性的机制，难以表示供应链网络中固有的高阶、多实体依赖关系。这些局限性促使定义了一个新问题并开发针对性的建模解决方案。为解决这些挑战，我们正式定义了一个新问题：供应链韧性推断（SCRI），即使用超图拓扑和观察到的库存轨迹来预测供应链韧性，而不使用显式动态方程。为解决此问题，我们提出了供应链韧性推断超图网络（SC-RIHN），这是一种基于超图的新型模型，利用基于集合的编码和超图消息传递来捕获多方企业-产品交互。全面的实验表明，SC-RIHN在合成基准测试中显著优于传统的MLP、代表性的图神经网络变体和ResInf基线方法，凸显了其在复杂供应链系统中进行实际、早期风险评估的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Supply chains are integral to global economic stability, yet disruptions canswiftly propagate through interconnected networks, resulting in substantialeconomic impacts. Accurate and timely inference of supply chain resilience thecapability to maintain core functions during disruptions is crucial forproactive risk mitigation and robust network design. However, existingapproaches lack effective mechanisms to infer supply chain resilience withoutexplicit system dynamics and struggle to represent the higher-order,multi-entity dependencies inherent in supply chain networks. These limitationsmotivate the definition of a novel problem and the development of targetedmodeling solutions. To address these challenges, we formalize a novel problem:Supply Chain Resilience Inference (SCRI), defined as predicting supply chainresilience using hypergraph topology and observed inventory trajectorieswithout explicit dynamic equations. To solve this problem, we propose theSupply Chain Resilience Inference Hypergraph Network (SC-RIHN), a novelhypergraph-based model leveraging set-based encoding and hypergraph messagepassing to capture multi-party firm-product interactions. Comprehensiveexperiments demonstrate that SC-RIHN significantly outperforms traditional MLP,representative graph neural network variants, and ResInf baselines acrosssynthetic benchmarks, underscoring its potential for practical, early-warningrisk assessment in complex supply chain systems.</description>
      <author>example@mail.com (Zetian Shen, Hongjun Wang, Jiyuan Chen, Xuan Song)</author>
      <guid isPermaLink="false">2511.06208v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Robustness of Graph Neural Networks through p-Laplacian</title>
      <link>http://arxiv.org/abs/2511.06143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at 5th Workshop on Graphs and more Complex Structures For  Learning and Reasoning (GCLR), The 40th AAAI Conference on Artificial  Intelligence (AAAI-26)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为pLAPGNN的计算高效框架，基于加权p-Laplacian，用于增强图神经网络对抗对抗攻击的鲁棒性，并在真实数据集上验证了其有效性和效率。&lt;h4&gt;背景&lt;/h4&gt;随着数据量增加，企业和利益相关者需要分析数据以做出更好预测。传统关系数据分析已不足以满足需求，图数据分析因其能更真实灵活地建模复杂关系而成为重要工具。图神经网络在社交网络分析、推荐系统、药物发现等领域显示出巨大潜力，但容易受到对抗攻击影响。&lt;h4&gt;目的&lt;/h4&gt;提高图神经网络对抗对抗攻击的鲁棒性，同时保持计算效率，解决现有鲁棒性方法计算量大且在高强度攻击下表现不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种名为pLAPGNN的计算高效框架，基于加权p-Laplacian来增强GNNs的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;通过在真实数据集上的实证评估，证明了所提出方法在对抗攻击下表现良好，且计算成本较低，有效性和效率均得到验证。&lt;h4&gt;结论&lt;/h4&gt;pLAPGNN框架能够有效提高GNNs对抗对抗攻击的鲁棒性，同时保持计算效率，解决了现有方法在高强度攻击下表现不佳的问题。&lt;h4&gt;翻译&lt;/h4&gt;随着日常生活中数据的增加，企业和不同的利益相关者需要分析数据以做出更好的预测。传统上，关系数据一直是各种洞察的来源，但随着计算能力的提高和对实体间更深层次关系理解的需求，设计新技术的需求已经出现。因此，图数据分析已成为理解数据的非凡工具，它揭示了复杂关系更真实和灵活的建模。最近，图神经网络在各种应用中显示出巨大的潜力，如社交网络分析、推荐系统、药物发现等。然而，许多对抗性攻击可能会发生在数据上，无论是在训练期间还是测试期间，都可能对GNN模型期望的结果产生不利影响。因此，使GNNs对这类攻击具有鲁棒性至关重要。现有的鲁棒性方法计算量大，且在攻击强度增加时表现不佳。本文提出了一种计算高效的框架，即基于加权p-Laplacian的pLAPGNN，用于提高GNNs的鲁棒性。在真实数据集上的实证评估确立了所提出方法的有效性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the increase of data in day-to-day life, businesses and differentstakeholders need to analyze the data for better pre- dictions. Traditionally,relational data has been a source of various insights, but with the increase incomputational power and the need to understand deeper relationships between en-tities, the need to design new techniques has arisen. For this graph dataanalysis has become an extraordinary tool for un- derstanding the data, whichreveals more realistic and flexible modelling of complex relationships.Recently, Graph Neural Networks (GNNs) have shown great promise in various ap-plications, such as social network analysis, recommendation systems, drugdiscovery, and more. However, many adversar- ial attacks can happen over thedata, whether during training (poisoning attack) or during testing (evasionattack), which can adversely manipulate the desired outcome from the GNN model.Therefore, it is crucial to make the GNNs robust to such attacks. The existingrobustness methods are computa- tionally demanding and perform poorly when theintensity of attack increases. This paper presents a computationally ef-ficient framework, namely, pLAPGNN, based on weighted p-Laplacian for makingGNNs robust. Empirical evaluation on real datasets establishes the efficacy andefficiency of the proposed method.</description>
      <author>example@mail.com (Anuj Kumar Sirohi, Subhanu Halder, Kabir Kumar, Sandeep Kumar)</author>
      <guid isPermaLink="false">2511.06143v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Reperio-rPPG: Relational Temporal Graph Neural Networks for Periodicity Learning in Remote Physiological Measurement</title>
      <link>http://arxiv.org/abs/2511.05946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Reperio-rPPG是一种新型框架，通过整合关系卷积网络与图Transformer来有效捕捉生理信号的内在周期性结构，并引入CutMix增强方法提高模型泛化能力，在多种基准数据集上取得了最先进性能且表现出显著鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;远程光容积脉搏波描记法(rPPG)是一种新兴的无接触式生理传感技术，利用面部视频中的细微颜色变化来估算心率、呼吸率等生命体征，因其可扩展性和便利性在远程医疗、情感计算、驾驶员疲劳检测和健康监测等领域受到关注。&lt;h4&gt;目的&lt;/h4&gt;解决以往rPPG方法中对生理信号内在周期性特征探索不足或建模不充分的问题，提出能够捕捉细粒度时间动态的新框架，并提高模型在现实条件下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;战略性地整合关系卷积网络与图Transformer来捕捉生理信号的周期性结构，引入定制的CutMix数据增强方法，并在PURE、UBFC-rPPG和MMPD三个基准数据集上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;Reperio-rPPG在三个基准数据集上实现了最先进的性能，并且在各种运动条件(静止、旋转、说话、行走)和光照条件(自然光、低LED、高LED)下表现出显著的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;Reperio-rPPG有效解决了以往方法中对生理信号内在周期性建模不足的问题，为远程生理信号测量提供了更强大、更鲁棒的解决方案，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;远程光容积脉搏波描记法(rPPG)是一种新兴的无接触式生理传感技术，它利用面部视频中的细微颜色变化来估算心率、呼吸率等生命体征。这种非侵入式方法因其可扩展性和便利性，在远程医疗、情感计算、驾驶员疲劳检测和健康监测等多个领域受到关注。尽管在远程生理信号测量方面取得了显著进展，但一个关键特征——内在周期性——在以往的方法中往往被探索不足或建模不充分，限制了它们在现实条件下捕捉细粒度时间动态的能力。为填补这一空白，我们提出了Reperio-rPPG，一个新型框架，通过战略性地整合关系卷积网络与图Transformer来有效捕捉生理信号中固有的周期性结构。此外，认识到现有rPPG数据集的多样性有限，我们进一步引入了定制的CutMix增强方法来提高模型的泛化能力。在三个广泛使用的基准数据集(PURE、UBFC-rPPG和MMPD)上进行的大量实验表明，Reperio-rPPG不仅取得了最先进的性能，而且在各种运动(如静止、旋转、说话、行走)和光照条件(如自然光、低LED、高LED)下表现出显著的鲁棒性。代码已在https://github.com/deconasser/Reperio-rPPG公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote photoplethysmography (rPPG) is an emerging contactless physiologicalsensing technique that leverages subtle color variations in facial videos toestimate vital signs such as heart rate and respiratory rate. This non-invasivemethod has gained traction across diverse domains, including telemedicine,affective computing, driver fatigue detection, and health monitoring, owing toits scalability and convenience. Despite significant progress in remotephysiological signal measurement, a crucial characteristic - the intrinsicperiodicity - has often been underexplored or insufficiently modeled inprevious approaches, limiting their ability to capture fine-grained temporaldynamics under real-world conditions. To bridge this gap, we proposeReperio-rPPG, a novel framework that strategically integrates RelationalConvolutional Networks with a Graph Transformer to effectively capture theperiodic structure inherent in physiological signals. Additionally, recognizingthe limited diversity of existing rPPG datasets, we further introduce atailored CutMix augmentation to enhance the model's generalizability. Extensiveexperiments conducted on three widely used benchmark datasets - PURE,UBFC-rPPG, and MMPD - demonstrate that Reperio-rPPG not only achievesstate-of-the-art performance but also exhibits remarkable robustness undervarious motion (e.g., stationary, rotation, talking, walking) and illuminationconditions (e.g., nature, low LED, high LED). The code is publicly available athttps://github.com/deconasser/Reperio-rPPG.</description>
      <author>example@mail.com (Ba-Thinh Nguyen, Thach-Ha Ngoc Pham, Hoang-Long Duc Nguyen, Thi-Duyen Ngo, Thanh-Ha Le)</author>
      <guid isPermaLink="false">2511.05946v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering</title>
      <link>http://arxiv.org/abs/2511.05876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AAAI'2026 oral paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MoEGCL的新型多视图聚类方法，通过自我图混合对比表示学习解决了现有方法中的粗粒度图融合问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)的进步近年来显著推动了多视图聚类(MVC)的发展，但现有方法存在粗粒度图融合的问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有多视图聚类方法中在视图级别进行图结构加权融合的粗糙策略，实现更细粒度的图融合。&lt;h4&gt;方法&lt;/h4&gt;提出MoEGCL方法，包含两个主要模块：1)自我图混合融合(MoEGF)模块，构建自我图并使用专家混合网络在样本级别实现细粒度融合；2)自我图对比学习(EGCL)模块，将融合表示与视图特定表示对齐，增强同一聚类样本的表示相似性。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验验证，MoEGCL在深度多视图聚类任务中达到了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;MoEGCL通过在样本级别而非视图级别进行细粒度图融合，以及增强同一聚类样本的表示相似性，有效提升了多视图聚类的性能。&lt;h4&gt;翻译&lt;/h4&gt;近年来，图神经网络(GNNs)的进步显著推动了多视图聚类(MVC)的发展。然而，现有方法面临粗粒度图融合的问题。具体而言，当前方法通常为每个视图生成单独的图结构，然后在视图级别进行图结构的加权融合，这是一种相对粗糙的策略。为解决这一局限，我们提出了一种新颖的自我图混合对比表示学习(MoEGCL)方法。它主要由两个模块组成。特别是，我们提出了创新的自我图混合融合(MoEGF)方法，该方法构建自我图并利用专家混合网络在样本级别实现自我图的细粒度融合，而非传统的视图级别融合。此外，我们提出了自我图对比学习(EGCL)模块，将融合后的表示与视图特定表示对齐。EGCL模块增强了来自同一聚类而不仅仅是同一样本的样本表示相似性，进一步提升了细粒度图表示。大量实验证明，MoEGCL在深度多视图聚类任务中取得了最先进的结果。源代码已在https://github.com/HackerHyper/MoEGCL公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the advancement of Graph Neural Networks (GNNs) hassignificantly propelled progress in Multi-View Clustering (MVC). However,existing methods face the problem of coarse-grained graph fusion. Specifically,current approaches typically generate a separate graph structure for each viewand then perform weighted fusion of graph structures at the view level, whichis a relatively rough strategy. To address this limitation, we present a novelMixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainlyconsists of two modules. In particular, we propose an innovative Mixture ofEgo-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes aMixture-of-Experts network to implement fine-grained fusion of ego graphs atthe sample level, rather than the conventional view-level fusion. Additionally,we present the Ego Graph Contrastive Learning (EGCL) module to align the fusedrepresentation with the view-specific representation. The EGCL module enhancesthe representation similarity of samples from the same cluster, not merely fromthe same sample, further boosting fine-grained graph representation. Extensiveexperiments demonstrate that MoEGCL achieves state-of-the-art results in deepmulti-view clustering tasks. The source code is publicly available athttps://github.com/HackerHyper/MoEGCL.</description>
      <author>example@mail.com (Jian Zhu, Xin Zou, Jun Sun, Cheng Luo, Lei Liu, Lingfang Zeng, Ning Zhang, Bian Wu, Chang Tang, Lirong Dai)</author>
      <guid isPermaLink="false">2511.05876v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Scale Feature Fusion and Graph Neural Network Integration for Text Classification with Large Language Models</title>
      <link>http://arxiv.org/abs/2511.05752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种用于文本分类的混合方法，整合了大语言模型的深度特征提取、特征金字塔的多尺度融合以及图神经网络的结构化建模，以提升复杂语义上下文中的分类性能。&lt;h4&gt;背景&lt;/h4&gt;在复杂语义语境下，需要更有效的文本分类方法来处理文本中的深层语义关系和复杂交互。&lt;h4&gt;目的&lt;/h4&gt;通过整合多种技术方法，构建一个能够平衡全局与局部信息、语义与结构的文本分类框架，提高分类性能。&lt;h4&gt;方法&lt;/h4&gt;首先使用大语言模型捕获文本的上下文依赖和深层语义表示；然后通过特征金字塔机制融合不同尺度的语义特征，平衡全局信息和局部细节；接着将融合特征转换为图表示，利用图神经网络捕获潜在语义关系和逻辑依赖；最后通过读取和分类模块生成最终类别预测。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在鲁棒性对齐实验中表现出显著优势，在ACC、F1-Score、AUC和Precision等指标上均优于现有模型，验证了框架的有效性和稳定性。&lt;h4&gt;结论&lt;/h4&gt;该方法构建了一个平衡全局和局部信息、语义和结构的集成框架，为文本分类任务中的多尺度特征融合和结构化语义建模提供了新视角。&lt;h4&gt;翻译&lt;/h4&gt;本研究调查了一种用于文本分类的混合方法，该方法整合了大语言模型的深度特征提取、特征金字塔的多尺度融合以及图神经网络的结构化建模，以增强复杂语义上下文中的性能。首先，大语言模型捕获输入文本的上下文依赖和深层语义表示，为后续建模提供丰富的特征基础。然后，基于多级特征表示，特征金字塔机制有效融合不同尺度的语义特征，平衡全局信息和局部细节，构建层次化语义表达。此外，融合的特征被转换为图表示，并采用图神经网络捕获文本中的潜在语义关系和逻辑依赖，实现对语义单元间复杂交互的全面建模。在此基础上，读取和分类模块生成最终的类别预测。该方法在鲁棒性对齐实验中表现出显著优势，在ACC、F1-Score、AUC和Precision等指标上优于现有模型，验证了该框架的有效性和稳定性。本研究不仅构建了一个平衡全局和局部信息以及语义和结构的集成框架，还为文本分类任务中的多尺度特征融合和结构化语义建模提供了新视角。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates a hybrid method for text classification thatintegrates deep feature extraction from large language models, multi-scalefusion through feature pyramids, and structured modeling with graph neuralnetworks to enhance performance in complex semantic contexts. First, the largelanguage model captures contextual dependencies and deep semanticrepresentations of the input text, providing a rich feature foundation forsubsequent modeling. Then, based on multi-level feature representations, thefeature pyramid mechanism effectively integrates semantic features of differentscales, balancing global information and local details to constructhierarchical semantic expressions. Furthermore, the fused features aretransformed into graph representations, and graph neural networks are employedto capture latent semantic relations and logical dependencies in the text,enabling comprehensive modeling of complex interactions among semantic units.On this basis, the readout and classification modules generate the finalcategory predictions. The proposed method demonstrates significant advantagesin robustness alignment experiments, outperforming existing models on ACC,F1-Score, AUC, and Precision, which verifies the effectiveness and stability ofthe framework. This study not only constructs an integrated framework thatbalances global and local information as well as semantics and structure, butalso provides a new perspective for multi-scale feature fusion and structuredsemantic modeling in text classification tasks.</description>
      <author>example@mail.com (Xiangchen Song, Yulin Huang, Jinxu Guo, Yuchen Liu, Yaxuan Luan)</author>
      <guid isPermaLink="false">2511.05752v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Personalized Image Editing in Text-to-Image Diffusion Models via Collaborative Direct Preference Optimization</title>
      <link>http://arxiv.org/abs/2511.05616v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at NeurIPS'25 Main Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了首个个性化图像编辑框架，通过协作直接偏好优化(C-DPO)方法使扩散模型能够适应用户特定的美学偏好，同时利用相似品味用户之间的协作信号。&lt;h4&gt;背景&lt;/h4&gt;文本到图像(T2I)扩散模型在根据文本生成和编辑高保真图像方面取得了显著进展，但这些模型本质上仍然是通用的，无法适应用户细微的美学偏好差异。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适应用户特定审美偏好的个性化图像编辑方法，同时利用具有相似视觉品味的用户之间的协作信息来提高编辑质量。&lt;h4&gt;方法&lt;/h4&gt;提出协作直接偏好优化(C-DPO)框架，将每个用户编码为动态偏好图中的一个节点，通过轻量级图神经网络学习嵌入表示，实现具有重叠视觉品味用户之间的信息共享，并将这些个性化嵌入整合到新的DPO目标中，同时优化个体对齐和邻域一致性。&lt;h4&gt;主要发现&lt;/h4&gt;通过用户研究和定量基准测试的综合实验表明，该方法在生成符合用户偏好的编辑方面始终优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;C-DPO框架成功解决了扩散模型无法适应用户特定审美偏好的问题，通过用户协作信号显著提高了个性化图像编辑的质量和用户满意度。&lt;h4&gt;翻译&lt;/h4&gt;文本到图像(T2I)扩散模型在根据文本生成和编辑高保真图像方面取得了显著进展。然而，这些模型本质上仍然是通用的，无法适应用户细微的美学偏好。在本工作中，我们提出了扩散模型中首个个性化图像编辑框架，引入协作直接偏好优化(C-DPO)，一种新方法，它使图像编辑与用户特定偏好保持一致，同时利用具有相似品味个体的协作信号。我们的方法将每个用户编码为动态偏好图中的一个节点，并通过轻量级图神经网络学习嵌入表示，使具有重叠视觉品味的用户之间能够共享信息。我们将这些个性化嵌入整合到新的DPO目标中，增强扩散模型的编辑能力，同时优化个体对齐和邻域一致性。包括用户研究和定量基准测试的综合实验表明，我们的方法在生成符合用户偏好的编辑方面始终优于基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-to-image (T2I) diffusion models have made remarkable strides ingenerating and editing high-fidelity images from text. Yet, these models remainfundamentally generic, failing to adapt to the nuanced aesthetic preferences ofindividual users. In this work, we present the first framework for personalizedimage editing in diffusion models, introducing Collaborative Direct PreferenceOptimization (C-DPO), a novel method that aligns image edits with user-specificpreferences while leveraging collaborative signals from like-mindedindividuals. Our approach encodes each user as a node in a dynamic preferencegraph and learns embeddings via a lightweight graph neural network, enablinginformation sharing across users with overlapping visual tastes. We enhance adiffusion model's editing capabilities by integrating these personalizedembeddings into a novel DPO objective, which jointly optimizes for individualalignment and neighborhood coherence. Comprehensive experiments, including userstudies and quantitative benchmarks, demonstrate that our method consistentlyoutperforms baselines in generating edits that are aligned with userpreferences.</description>
      <author>example@mail.com (Connor Dunlop, Matthew Zheng, Kavana Venkatesh, Pinar Yanardag)</author>
      <guid isPermaLink="false">2511.05616v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects</title>
      <link>http://arxiv.org/abs/2511.06378v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ArtReg是一种创新的视觉-触觉跟踪方法，能够在没有先验知识的情况下跟踪未知和关节式物体，并通过实验验证了其在各种条件下的鲁棒性和精确性。&lt;h4&gt;背景&lt;/h4&gt;机器人在真实环境中经常遇到具有复杂结构和关节组件的未知物体，如门、抽屉、柜子、工具等。在没有预先了解物体几何或运动学属性的情况下，感知、跟踪和操作这些物体仍然是机器人技术中的一个基本挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的方法，用于机器人在交互过程中对未知物体（单个、多个或关节式物体）进行视觉-触觉跟踪，无需预先假设物体的形状或动力学知识。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了一种名为ArtReg（关节式配准）的姿态跟踪方法；2. ArtReg在SE(3)李群中使用无迹卡尔曼滤波公式整合视觉-触觉点云进行点云配准；3. 使用两个机器人团队进行有目的的操作动作（如推或握-拉）来检测物体可能的关节；4. 利用ArtReg开发了一个闭环控制器，用于关节物体的目标驱动操作。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在各种类型的未知物体上通过真实机器人实验广泛评估了该方法；2. 通过评估具有不同质心、低光条件和具有挑战性视觉背景的物体，证明了该方法的鲁棒性；3. 在标准关节物体数据集上进行了基准测试，与最先进方法相比，姿态精度有所提高；4. 实验表明，利用视觉-触觉信息进行鲁棒精确的姿态跟踪，使机器人能够感知和交互未见的复杂关节物体。&lt;h4&gt;结论&lt;/h4&gt;基于视觉-触觉信息的鲁棒精确姿态跟踪使机器人能够感知和交互未见的复杂关节物体，这是机器人技术的重要进展。&lt;h4&gt;翻译&lt;/h4&gt;在真实环境中运行的机器人经常遇到具有复杂结构和关节组件的未知物体，如门、抽屉、柜子、工具等。在没有预先了解物体几何或运动学属性的情况下，感知、跟踪和操作这些物体仍然是机器人技术中的一个基本挑战。在这项工作中，我们提出了一种新颖的方法，用于机器人在交互过程中对未知物体（单个、多个或关节式物体）进行视觉-触觉跟踪，无需预先假设物体的形状或动力学知识。我们称之为ArtReg（关节式配准）的新颖姿态跟踪方法，在SE(3)李群中使用无迹卡尔曼滤波公式整合视觉-触觉点云进行点云配准。ArtReg使用两个机器人团队进行有目的的操作动作（如推或握-拉）来检测物体可能的关节。此外，我们利用ArtReg开发了一个闭环控制器，用于关节物体的目标驱动操作，将物体移动到期望的姿态配置。我们通过各种类型的未知物体进行了广泛的真实机器人实验评估。我们还通过评估具有不同质心、低光条件和具有挑战性视觉背景的物体，证明了我们方法的鲁棒性。此外，我们在标准关节物体数据集上对我们的方法进行了基准测试，并证明了与最先进方法相比在姿态精度方面的改进。我们的实验表明，利用视觉-触觉信息进行鲁棒精确的姿态跟踪，使机器人能够感知和交互未见的复杂关节物体（具有旋转或平移关节）。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人如何在没有预先了解物体几何形状或运动特性的情况下，感知、跟踪和操作具有复杂结构和铰接组件的未知物体。这个问题在现实中非常重要，因为机器人经常需要与日常物体如门、抽屉、柜子、工具等交互，这些物体具有多个自由度和非线性动力学特性，使得准确跟踪和操作变得非常困难。当前大多数方法依赖于预先知道的物体模型或仅使用视觉信息，无法处理完全未知的复杂铰接物体。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：大多数铰接物体跟踪方法依赖于几何特征跟踪或基于标记的方法，或者假设预先知道物体模型。作者借鉴了多模态感知的思想，结合视觉和触觉信息，因为触觉信息可以提供关于物体特性的互补信息，并且对遮挡、环境光照和物体透明度具有不变性。作者设计了ArtReg方法，这是一种基于SE(3)李群上的流形无迹卡尔曼滤波器的视觉-触觉点云配准方法，并使用双机器人系统（一个配备RGB-D视觉传感器，另一个配备触觉传感器阵列）来实现交互式感知和操作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合视觉和触觉信息，在没有预先了解物体形状或动力学的情况下，实现对未知单物体、多物体或铰接物体的姿态跟踪和操作。整体实现流程包括：1) 使用ArtReg方法进行视觉-触觉姿态跟踪，基于SE(3)李群上的流形无迹卡尔曼滤波器整合视觉和触觉点云数据；2) 使用交互式感知检测铰接关节，通过有目的的操作动作（如推、握-拉）来检测可能的铰接关节；3) 使用视觉-触觉闭环控制器进行目标驱动的物体操作，将物体移动到期望的姿态配置。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了ArtReg方法，一种基于SE(3)李群上的流形无迹卡尔曼滤波器的视觉-触觉点云配准方法；2) 基于ArtReg开发了检测物体中运动链（旋转或平移关节）的新方法；3) 基于ArtReg开发了视觉-触觉闭环控制算法；4) 在各种条件下进行了广泛的实验验证。相比之前的工作，这篇论文的主要不同之处在于：不需要预先了解物体的形状或动力学特性；结合了视觉和触觉信息，而不仅仅是视觉信息；可以处理单物体、多物体和铰接物体；提供了从检测、跟踪到操作的完整框架；在各种条件下展示了方法的鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种名为ArtReg的视觉-触觉融合方法，使机器人能够在没有预先了解物体模型的情况下，准确跟踪和操作未知的单物体、多物体和铰接物体。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robots operating in real-world environments frequently encounter unknownobjects with complex structures and articulated components, such as doors,drawers, cabinets, and tools. The ability to perceive, track, and manipulatethese objects without prior knowledge of their geometry or kinematic propertiesremains a fundamental challenge in robotics. In this work, we present a novelmethod for visuo-tactile-based tracking of unseen objects (single, multiple, orarticulated) during robotic interaction without assuming any prior knowledgeregarding object shape or dynamics. Our novel pose tracking approach termedArtReg (stands for Articulated Registration) integrates visuo-tactile pointclouds in an unscented Kalman Filter formulation in the SE(3) Lie Group forpoint cloud registration. ArtReg is used to detect possible articulated jointsin objects using purposeful manipulation maneuvers such as pushing orhold-pulling with a two-robot team. Furthermore, we leverage ArtReg to developa closed-loop controller for goal-driven manipulation of articulated objects tomove the object into the desired pose configuration. We have extensivelyevaluated our approach on various types of unknown objects through real robotexperiments. We also demonstrate the robustness of our method by evaluatingobjects with varying center of mass, low-light conditions, and with challengingvisual backgrounds. Furthermore, we benchmarked our approach on a standarddataset of articulated objects and demonstrated improved performance in termsof pose accuracy compared to state-of-the-art methods. Our experiments indicatethat robust and accurate pose tracking leveraging visuo-tactile informationenables robots to perceive and interact with unseen complex articulated objects(with revolute or prismatic joints).</description>
      <author>example@mail.com (Prajval Kumar Murali, Mohsen Kaboli)</author>
      <guid isPermaLink="false">2511.06378v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Agent Selection and Interaction Network for Image-to-point cloud Registration</title>
      <link>http://arxiv.org/abs/2511.05965v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的跨模态配准框架，包含迭代代理选择(IAS)和可靠代理交互(RAI)两个模块，解决了无检测图像到点云配准在噪声环境下表现不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;典型的无检测图像到点云配准方法利用基于Transformer的架构聚合跨模态特征并建立对应关系，但在噪声干扰下容易产生错误对应，且难以有效选择跨模态中的相关信息表示。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在挑战性条件下表现不佳的问题，提高配准的鲁棒性和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一个由两个关键模块组成的跨模态配准框架：1) 迭代代理选择(IAS)模块，使用相位图增强结构特征感知并采用强化学习选择可靠代理；2) 可靠代理交互(RAI)模块，利用选定的代理引导跨模态交互，减少不匹配。&lt;h4&gt;主要发现&lt;/h4&gt;在RGB-D Scenes v2和7-Scenes基准测试上进行了大量实验，结果表明该方法始终达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过IAS和RAI模块的组合，有效解决了噪声环境下的配准挑战，提高了跨模态配准的准确性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;典型的无检测图像到点云配准方法利用基于Transformer的架构来聚合跨模态特征并建立对应关系。然而，它们在具有挑战性的条件下往往表现不佳，其中噪声会干扰相似性计算并导致错误的对应关系。此外，如果没有专门的设计，仍然难以有效选择跨模态中信息丰富且相关的表示，从而限制了配准的鲁棒性和准确性。为解决这些挑战，我们提出了一种新型的跨模态配准框架，由两个关键模块组成：迭代代理选择(IAS)模块和可靠代理交互(RAI)模块。IAS通过相位图增强结构特征感知能力，并采用强化学习原则来高效选择可靠的代理。然后，RAI利用这些选定的代理来引导跨模态交互，有效减少不匹配并提高整体鲁棒性。在RGB-D Scenes v2和7-Scenes基准测试上的大量实验表明，我们的方法始终达到了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决图像到点云配准（Image-to-point cloud Registration, I2P）问题，特别是在噪声干扰、重复结构和非重叠区域等挑战性条件下的配准鲁棒性和准确性问题。这个问题在现实中非常重要，因为I2P是3D重建、SLAM（同步定位与地图构建）和视觉定位等关键视觉任务的基础步骤，对于机器人导航、自动驾驶和增强现实等领域至关重要。准确配准能够弥合2D图像和3D点云之间的模态差距，提高空间感知能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统transformer-based方法在噪声环境下容易产生错误对应关系的问题，以及难以有效选择跨模态信息丰富表示的挑战。他们借鉴了相位图提取技术（来自NeRF等工作的启发）来增强图像结构特征感知，并引入强化学习原理设计代理选择策略。整体设计思路是通过增强结构特征感知能力，并采用智能代理选择来提高配准质量。该方法确实借鉴了现有工作，包括transformer架构、强化学习、粗到细匹配策略等，但进行了创新性整合和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过相位图增强图像的结构特征感知能力，并采用强化学习原则设计三阶段代理优化策略，从冗余查询中识别可靠代理，然后利用这些代理指导跨模态特征交互，减少噪声影响。整体流程包括：1)使用ResNet和KPFCNN分别提取图像和点云特征；2)应用傅里叶变换提取图像相位信息增强结构感知；3)三阶段代理优化（预热训练、奖励引导训练、最优代理选择）；4)可靠代理交互模块进行跨模态特征融合；5)通过PnP-RANSAC估计最终刚性变换。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出自适应代理选择与交互网络（A2SI）；2)设计迭代代理选择（IAS）模块，利用相位图增强结构特征感知和三阶段代理优化策略；3)设计可靠代理交互（RAI）模块，用代理引导交互替代传统transformer融合。相比之前的工作，A2SI采用无检测设计避免了关键点检测的模态依赖问题；通过专门设计的代理选择机制选择信息丰富且相关的表示；使用强化学习优化代理选择而非简单top-k选择；减少噪声特征影响，提高匹配鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种自适应代理选择与交互网络，通过相位图增强和强化学习引导的代理选择，有效解决了图像到点云配准中跨模态特征匹配的挑战，显著提高了配准的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Typical detection-free methods for image-to-point cloud registration leveragetransformer-based architectures to aggregate cross-modal features and establishcorrespondences. However, they often struggle under challenging conditions,where noise disrupts similarity computation and leads to incorrectcorrespondences. Moreover, without dedicated designs, it remains difficult toeffectively select informative and correlated representations acrossmodalities, thereby limiting the robustness and accuracy of registration. Toaddress these challenges, we propose a novel cross-modal registration frameworkcomposed of two key modules: the Iterative Agents Selection (IAS) module andthe Reliable Agents Interaction (RAI) module. IAS enhances structural featureawareness with phase maps and employs reinforcement learning principles toefficiently select reliable agents. RAI then leverages these selected agents toguide cross-modal interactions, effectively reducing mismatches and improvingoverall robustness. Extensive experiments on the RGB-D Scenes v2 and 7-Scenesbenchmarks demonstrate that our method consistently achieves state-of-the-artperformance.</description>
      <author>example@mail.com (Zhixin Cheng, Xiaotian Yin, Jiacheng Deng, Bohao Liao, Yujia Chen, Xu Zhou, Baoqun Yin, Tianzhu Zhang)</author>
      <guid isPermaLink="false">2511.05965v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Do Street View Imagery and Public Participation GIS align: Comparative Analysis of Urban Attractiveness</title>
      <link>http://arxiv.org/abs/2511.05570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了街景图像(SVI)和公众参与地理信息系统(PPGIS)两种方法在捕捉城市环境感知方面的一致性，发现两者只有部分匹配，SVI无法完全捕捉PPGIS记录的丰富体验。&lt;h4&gt;背景&lt;/h4&gt;数字工具越来越多地影响空间规划实践，了解不同数据源如何反映城市环境中的人类体验至关重要。SVI和PPGIS是两种突出的方法，用于收集基于地点的认知以支持城市规划决策，但它们的可比性尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究基于SVI的感知吸引力和通过城市范围的PPGIS调查收集的居民报告经验之间的一致性，研究地点为芬兰赫尔辛基。&lt;h4&gt;方法&lt;/h4&gt;使用参与者评分的SVI数据和语义图像分割，训练机器学习模型基于视觉特征预测感知吸引力，并将这些预测与PPGIS确定的有吸引力或没有吸引力的位置进行比较，使用两套严格和适中的标准计算一致性。&lt;h4&gt;主要发现&lt;/h4&gt;两个数据集之间只有部分一致性：使用中等标准时，对于有吸引力和没有吸引力的地方，一致性分别达到67%和77%；使用严格标准时，一致性分别下降到27%和29%。非视觉因素（如噪音、交通、人口存在和土地利用）显著导致了不匹配，模型无法解释图像中不可见的体验维度。&lt;h4&gt;结论&lt;/h4&gt;虽然SVI提供了可扩展的城市感知视觉代理，但它不能完全替代PPGIS捕捉的体验丰富性。两种方法都有价值但服务于不同目的，需要更综合的方法来全面捕捉人们如何感知城市环境。&lt;h4&gt;翻译&lt;/h4&gt;随着数字工具日益塑造空间规划实践，了解不同数据源如何反映城市环境中的人类体验至关重要。街景图像(SVI)和公众参与地理信息系统(PPGIS)代表了两种突出的捕捉基于地点认知的方法，可以支持城市规划决策，但它们的可比性尚未得到充分探索。本研究调查了基于SVI的感知吸引力和通过芬兰赫尔辛基全市范围的PPGIS调查收集的居民报告经验之间的一致性。使用参与者评分的SVI数据和语义图像分割，我们训练了一个机器学习模型来基于视觉特征预测感知吸引力。我们将这些预测与PPGIS确定的有吸引力或没有吸引力的位置进行比较，使用两套严格和适中的标准计算一致性。我们的发现显示两个数据集之间只有部分一致性。虽然使用中等标准时，对于有吸引力和没有吸引力的地方，一致性分别达到67%和77%，但使用严格标准时，一致性分别下降到27%和29%。通过分析各种背景变量，包括噪音、交通、人口存在和土地利用，我们发现非视觉因素显著导致了不匹配。模型无法解释塑造感知但图像中不可见的体验维度，如活动水平和环境压力因素。这些结果表明，虽然SVI提供了可扩展的城市感知视觉代理，但它不能完全替代PPGIS捕捉的体验丰富性。我们认为两种方法都有价值但服务于不同目的；因此，需要更综合的方法来全面捕捉人们如何感知城市环境。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As digital tools increasingly shape spatial planning practices, understandinghow different data sources reflect human experiences of urban environments isessential. Street View Imagery (SVI) and Public Participation GIS (PPGIS)represent two prominent approaches for capturing place-based perceptions thatcan support urban planning decisions, yet their comparability remainsunderexplored. This study investigates the alignment between SVI-basedperceived attractiveness and residents' reported experiences gathered via acity-wide PPGIS survey in Helsinki, Finland. Using participant-rated SVI dataand semantic image segmentation, we trained a machine learning model to predictperceived attractiveness based on visual features. We compared thesepredictions to PPGIS-identified locations marked as attractive or unattractive,calculating agreement using two sets of strict and moderate criteria. Ourfindings reveal only partial alignment between the two datasets. Whileagreement (with a moderate threshold) reached 67% for attractive and 77% forunattractive places, agreement (with a strict threshold) dropped to 27% and29%, respectively. By analysing a range of contextual variables, includingnoise, traffic, population presence, and land use, we found that non-visualcues significantly contributed to mismatches. The model failed to account forexperiential dimensions such as activity levels and environmental stressorsthat shape perceptions but are not visible in images. These results suggestthat while SVI offers a scalable and visual proxy for urban perception, itcannot fully substitute the experiential richness captured through PPGIS. Weargue that both methods are valuable but serve different purposes; therefore, amore integrated approach is needed to holistically capture how people perceiveurban environments.</description>
      <author>example@mail.com (Milad Malekzadeh, Elias Willberg, Jussi Torkko, Silviya Korpilo, Kamyar Hasanzadeh, Olle Järv, Tuuli Toivonen)</author>
      <guid isPermaLink="false">2511.05570v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs</title>
      <link>http://arxiv.org/abs/2511.07250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MVU-Eval，这是首个专门用于评估多模态大语言模型在多视频理解能力的基准测试。通过大量多样化的视频和问题对，评估了模型在基础感知和高阶推理方面的能力，研究发现当前MLLMs在多视频理解方面存在明显不足。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型的出现已将AI能力扩展到视觉模态，但现有评估基准仍局限于单视频理解，忽略了现实场景(如体育分析和自动驾驶)中多视频理解的关键需求。&lt;h4&gt;目的&lt;/h4&gt;引入MVU-Eval，首个用于评估MLLMs多视频理解的全面基准，以解决现有评估基准的显著差距。&lt;h4&gt;方法&lt;/h4&gt;MVU-Eval通过1,824个精心策划的问题-答案对评估八项核心能力，涵盖4,959个来自不同领域的视频，包括基础感知任务和高阶推理任务，并与现实世界应用严格对齐。&lt;h4&gt;主要发现&lt;/h4&gt;通过对最先进的开源和闭源模型进行广泛评估，揭示了当前MLLMs在多视频理解能力方面存在显著的性能差异和局限性。&lt;h4&gt;结论&lt;/h4&gt;该基准将公开发布，以促进未来多视频理解领域的研究发展。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型的出现已将AI能力扩展到视觉模态，然而现有的评估基准仍然局限于单视频理解，忽略了现实场景(如体育分析和自动驾驶)中多视频理解的关键需求。为解决这一重要差距，我们引入了MVU-Eval，这是首个用于评估MLLMs多视频理解的全面基准。具体而言，我们的MVU-Eval主要通过1,824个精心策划的问题-答案对来评估八项核心能力，这些答案对涵盖来自不同领域的4,959个视频，既包括基础感知任务，也包括高阶推理任务。这些能力与现实世界应用(如自动驾驶系统中的多传感器合成和跨角度体育分析)严格对齐。通过对最先进的开源和闭源模型进行广泛评估，我们揭示了当前MLLMs在多视频理解能力方面存在显著的性能差异和局限性。该基准将公开发布以促进未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of Multimodal Large Language Models (MLLMs) has expanded AIcapabilities to visual modalities, yet existing evaluation benchmarks remainlimited to single-video understanding, overlooking the critical need formulti-video understanding in real-world scenarios (e.g., sports analytics andautonomous driving). To address this significant gap, we introduce MVU-Eval,the first comprehensive benchmark for evaluating Multi-Video Understanding forMLLMs. Specifically, our MVU-Eval mainly assesses eight core competenciesthrough 1,824 meticulously curated question-answer pairs spanning 4,959 videosfrom diverse domains, addressing both fundamental perception tasks andhigh-order reasoning tasks. These capabilities are rigorously aligned withreal-world applications such as multi-sensor synthesis in autonomous systemsand cross-angle sports analytics. Through extensive evaluation ofstate-of-the-art open-source and closed-source models, we reveal significantperformance discrepancies and limitations in current MLLMs' ability to performunderstanding across multiple videos. The benchmark will be made publiclyavailable to foster future research.</description>
      <author>example@mail.com (Tianhao Peng, Haochen Wang, Yuanxing Zhang, Zekun Wang, Zili Wang, Ge Zhang, Jian Yang, Shihao Li, Yanghai Wang, Xintao Wang, Houyi Li, Wei Ji, Pengfei Wan, Wenhao Huang, Zhaoxiang Zhang, Jiaheng Liu)</author>
      <guid isPermaLink="false">2511.07250v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>4DSTR: Advancing Generative 4D Gaussians with Spatial-Temporal Rectification for High-Quality and Consistent 4D Generation</title>
      <link>http://arxiv.org/abs/2511.07241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2026.The first two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一个名为4DSTR的新型4D生成网络，通过时空校正调制生成式4D高斯飞溅，解决了4D生成中的时空一致性和快速时间变化适应性问题。&lt;h4&gt;背景&lt;/h4&gt;最近2D图像和3D形状生成的显著进步引起了人们对动态4D内容生成的关注，但现有方法在时空一致性和快速时间变化适应性方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有4D生成方法难以保持时空一致性、对快速时间变化适应性差的问题，通过有效的时空建模提高4D生成质量。&lt;h4&gt;方法&lt;/h4&gt;提出4DSTR网络，通过时空校正调制生成式4D高斯飞溅，设计时间相关性校正可变形尺度和旋转保证时间一致性，并提出自适应空间密集化和修剪策略处理显著时间变化。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明4DSTR在视频到4D生成方面达到最先进性能，在重建质量、时空一致性和快速时间运动适应性方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;4DSTR通过时空校正和自适应空间密集化/修剪策略有效解决了4D生成中的时空一致性和快速时间变化适应性问题，实现了高质量的4D内容生成。&lt;h4&gt;翻译&lt;/h4&gt;最近2D图像和3D形状生成的显著进步引起了人们对动态4D内容生成的显著关注。然而，由于缺乏有效的时空建模，先前的4D生成方法通常难以保持时空一致性，并且对快速的时间变化适应不良。为了解决这些问题，我们提出了一种名为4DSTR的新型4D生成网络，它通过时空校正来调制生成式4D高斯飞溅。具体来说，通过设计生成4D序列之间的时间相关性来校正可变形的尺度和旋转，并保证时间一致性。此外，还提出了一种自适应空间密集化和修剪策略，通过动态添加或删除高斯点并考虑它们前一帧的运动，来解决显著的时间变化问题。大量实验表明，4DSTR在视频到4D生成方面达到了最先进的性能，在重建质量、时空一致性和对快速时间运动的适应性方面表现出色。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决4D生成方法中的时空一致性和快速时变变化适应性问题。在现实中，高质量的4D内容对自动驾驶仿真、虚拟现实和数字角色动画等领域至关重要，而现有方法在动态区域经常出现不一致，难以捕捉快速变化的内容，限制了这些应用的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有4D表示方法的局限性，包括独立处理每帧导致缺乏时序相关性，以及固定数量的高斯点难以适应快速变化。他们借鉴了预训练扩散模型、可变形4D高斯溅射表示、Mamba架构和STAG4D的时空锚点设计，并在此基础上创新性地设计了时序相关性和自适应高斯点管理策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过时空校正增强4D生成内容的时空一致性，并动态调整高斯点数量以适应快速变化。整体流程包括：1)使用Zero123++生成多视角帧并初始化3D高斯；2)通过轻量级解码器映射体素特征到4D高斯参数；3)使用Mamba时序编码层关联序列并回归尺度和旋转残差；4)根据累积梯度自适应增密和剪枝高斯点；5)通过多视图SDS损失优化确保时空一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于Mamba的时序相关性模块，建立跨帧关联并回归校正参数；2)自适应高斯增密和剪枝策略，根据纹理变化动态调整点数量。相比STAG4D等方法，4DSTR显式建模时序相关性，考虑同一区域跨帧的快速纹理差异，并动态适应高斯点数量需求，实验显示在FID-VID和FVD指标上分别有15.1%和19.9%的提升。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 4DSTR通过时空校正和自适应高斯点管理，显著提高了4D生成内容的时空一致性和对快速变化的适应能力，实现了高质量的动态四维内容生成。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remarkable advances in recent 2D image and 3D shape generation have induced asignificant focus on dynamic 4D content generation. However, previous 4Dgeneration methods commonly struggle to maintain spatial-temporal consistencyand adapt poorly to rapid temporal variations, due to the lack of effectivespatial-temporal modeling. To address these problems, we propose a novel 4Dgeneration network called 4DSTR, which modulates generative 4D GaussianSplatting with spatial-temporal rectification. Specifically, temporalcorrelation across generated 4D sequences is designed to rectify deformablescales and rotations and guarantee temporal consistency. Furthermore, anadaptive spatial densification and pruning strategy is proposed to addresssignificant temporal variations by dynamically adding or deleting Gaussianpoints with the awareness of their pre-frame movements. Extensive experimentsdemonstrate that our 4DSTR achieves state-of-the-art performance in video-to-4Dgeneration, excelling in reconstruction quality, spatial-temporal consistency,and adaptation to rapid temporal movements.</description>
      <author>example@mail.com (Mengmeng Liu, Jiuming Liu, Yunpeng Zhang, Jiangtao Li, Michael Ying Yang, Francesco Nex, Hao Cheng)</author>
      <guid isPermaLink="false">2511.07241v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Pandar128 dataset for lane line detection</title>
      <link>http://arxiv.org/abs/2511.07084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Pandar128是基于128线激光雷达的最大公开车道线检测数据集，包含52,000多张相机帧和34,000多个激光雷达扫描，在德国各种真实世界条件下采集。&lt;h4&gt;背景&lt;/h4&gt;车道线检测领域需要高质量数据集和相关方法的研究支持。&lt;h4&gt;目的&lt;/h4&gt;创建大规模公开数据集，开发简单有效的基线方法，并提出标准化的评估指标。&lt;h4&gt;方法&lt;/h4&gt;构建Pandar128数据集包含完整传感器校准和同步里程计；开发SimpleLidarLane方法结合BEV分割、聚类和多项式拟合；提出基于多项式的IAM-F1评估指标，采用BEV空间中的插值感知横向匹配。&lt;h4&gt;主要发现&lt;/h4&gt;SimpleLidarLane方法虽简单，但在各种挑战性条件下（如雨天、稀疏返回）表现优异，表明模块化管道与高质量数据和原则性评估可媲美复杂方法。&lt;h4&gt;结论&lt;/h4&gt;所有数据和代码已公开发布，以支持基于激光雷达的车道检测研究的可复现性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Pandar128，这是基于128线激光雷达的最大的公开车道线检测数据集。它包含超过52,000张相机帧和34,000个激光雷达扫描，在德国各种真实世界条件下采集。该数据集包含完整的传感器校准（内参、外参）和同步的里程计，支持投影、融合和时间建模等任务。为补充该数据集，我们还引入了SimpleLidarLane，这是一种轻量级的车道线重建基线方法，结合了BEV分割、聚类和多项式拟合。尽管方法简单，但我们的方法在各种挑战性条件下（如雨天、稀疏返回）表现出色，表明模块化管道与高质量数据和原则性评估可以与更复杂的方法竞争。此外，为解决缺乏标准化评估的问题，我们提出了一种新的基于多项式的指标 - 插值感知匹配F1（IAM-F1），该指标在BEV空间中采用插值感知的横向匹配。所有数据和代码均已公开发布，以支持基于激光雷达的车道检测的可复现性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Pandar128, the largest public dataset for lane line detectionusing a 128-beam LiDAR. It contains over 52,000 camera frames and 34,000 LiDARscans, captured in diverse real-world conditions in Germany. The datasetincludes full sensor calibration (intrinsics, extrinsics) and synchronizedodometry, supporting tasks such as projection, fusion, and temporal modeling.  To complement the dataset, we also introduce SimpleLidarLane, a light-weightbaseline method for lane line reconstruction that combines BEV segmentation,clustering, and polyline fitting. Despite its simplicity, our method achievesstrong performance under challenging various conditions (e.g., rain, sparsereturns), showing that modular pipelines paired with high-quality data andprincipled evaluation can compete with more complex approaches.  Furthermore, to address the lack of standardized evaluation, we propose anovel polyline-based metric - Interpolation-Aware Matching F1 (IAM-F1) - thatemploys interpolation-aware lateral matching in BEV space.  All data and code are publicly released to support reproducibility inLiDAR-based lane detection.</description>
      <author>example@mail.com (Filip Beránek, Václav Diviš, Ivan Gruber)</author>
      <guid isPermaLink="false">2511.07084v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>DTTNet: Improving Video Shadow Detection via Dark-Aware Guidance and Tokenized Temporal Modeling</title>
      <link>http://arxiv.org/abs/2511.06925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的视频阴影检测方法，通过视觉语言匹配模块和暗感知语义块解决阴影-背景模糊问题，并使用标记化时间块处理动态阴影变形，实现了高精度和实时效率。&lt;h4&gt;背景&lt;/h4&gt;视频阴影检测面临两个相互关联的困难：从复杂背景中区分阴影，以及在不同光照条件下建模动态阴影变形。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确区分阴影与背景并有效建模动态阴影变形的视频阴影检测方法，同时保持高效率和实时性能。&lt;h4&gt;方法&lt;/h4&gt;提出视觉语言匹配模块(VMM)和暗感知语义块(DSB)利用语言先验区分阴影与暗物体；引入自适应掩码重加权弱化半影区域；在解码器阶段应用边缘掩码增强监督；提出标记化时间块(TTB)解耦时空学习，将跨帧阴影语义总结为可学习的时间标记实现高效序列编码。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的综合实验表明，该方法达到了最先进的准确性，同时保持了实时推理效率。&lt;h4&gt;结论&lt;/h4&gt;该方法通过结合视觉语言先验和创新的时空建模技术，有效解决了视频阴影检测中的关键挑战，为实际应用提供了高效准确的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;视频阴影检测面临两个相互关联的困难：从复杂背景中区分阴影以及在各种光照条件下建模动态阴影变形。为了解决阴影-背景模糊问题，我们通过提出的视觉语言匹配模块(VMM)和暗感知语义块(DSB)利用语言先验，提取文本引导的特征以明确区分阴影和暗物体。此外，我们引入自适应掩码重加权以在训练期间弱化半影区域，并在最终解码器阶段应用边缘掩码以获得更好的监督。对于可变阴影形状的时间建模，我们提出了标记化时间块(TTB)，该块解耦了时空学习。TTB将跨帧阴影语义总结为可学习的时间标记，实现了具有最小计算开销的高效序列编码。在多个基准数据集上的综合实验证明了最先进的准确性和实时推理效率。代码可在https://github.com/city-cheng/DTTNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video shadow detection confronts two entwined difficulties: distinguishingshadows from complex backgrounds and modeling dynamic shadow deformations undervarying illumination. To address shadow-background ambiguity, we leveragelinguistic priors through the proposed Vision-language Match Module (VMM) and aDark-aware Semantic Block (DSB), extracting text-guided features to explicitlydifferentiate shadows from dark objects. Furthermore, we introduce adaptivemask reweighting to downweight penumbra regions during training and apply edgemasks at the final decoder stage for better supervision. For temporal modelingof variable shadow shapes, we propose a Tokenized Temporal Block (TTB) thatdecouples spatiotemporal learning. TTB summarizes cross-frame shadow semanticsinto learnable temporal tokens, enabling efficient sequence encoding withminimal computation overhead. Comprehensive Experiments on multiple benchmarkdatasets demonstrate state-of-the-art accuracy and real-time inferenceefficiency. Codes are available at https://github.com/city-cheng/DTTNet.</description>
      <author>example@mail.com (Zhicheng Li, Kunyang Sun, Rui Yao, Hancheng Zhu, Fuyuan Hu, Jiaqi Zhao, Zhiwen Shao, Yong Zhou)</author>
      <guid isPermaLink="false">2511.06925v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>TiS-TSL: Image-Label Supervised Surgical Video Stereo Matching via Time-Switchable Teacher-Student Learning</title>
      <link>http://arxiv.org/abs/2511.06817v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, accepted by BiBM2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TiS-TSL的时间可切换教师-学生学习框架，用于微创手术中的视频立体匹配，通过时空一致性建模解决了现有方法中存在的视差预测不稳定和严重闪烁问题。&lt;h4&gt;背景&lt;/h4&gt;微创手术中的立体匹配对新一代导航和增强现实至关重要，但由于解剖结构限制，几乎无法提供密集视差监督，通常只能获取少量图像级标签。&lt;h4&gt;目的&lt;/h4&gt;克服现有教师-学生学习方法仅限于图像级监督、缺乏时间一致性估计的问题，解决视差预测不稳定和视频帧间严重闪烁的问题。&lt;h4&gt;方法&lt;/h4&gt;提出TiS-TSL框架，核心是统一模型，可在图像预测(IP)、前向视频预测(FVP)和后向视频预测(BVP)三种模式下运行。采用两阶段学习策略：图像到视频(I2V)阶段初始化时域建模；视频到视频(V2V)阶段通过比较前向和后向预测计算双向时空一致性，识别不可靠区域，过滤噪声伪标签，并强制时间相干性。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共数据集上的实验表明，TiS-TSL超越了其他基于图像的最先进方法，TEPE和EPE指标分别提高了至少2.11%和4.54%。&lt;h4&gt;结论&lt;/h4&gt;TiS-TSL通过时空一致性建模有效解决了微创手术中立体匹配的挑战，提高了视差预测的稳定性和准确性。&lt;h4&gt;翻译&lt;/h4&gt;微创手术中的立体匹配对新一代导航和增强现实至关重要。然而，由于解剖结构的限制，几乎不可能提供密集的视差监督，这通常限制了注释只能在获取一些图像级标签后进行。教师-学生学习通过利用在稀疏标签上训练的教师模型从丰富的未标记手术视频中生成伪标签和相关置信度图，提供了一个有前途的解决方案。然而，现有的TSL方法仅限于图像级监督，仅提供空间置信度，缺乏时间一致性估计。这种时空可靠性的缺失导致视差预测不稳定和视频帧间的严重闪烁伪影。为了克服这些挑战，我们提出了TiS-TSL，一种用于最小监督下视频立体匹配的新型时间可切换教师-学生学习框架。其核心是一个统一模型，可在三种不同模式下运行：图像预测、前向视频预测和后向视频预测，在单一架构中实现灵活的时域建模。通过这一统一模型，TiS-TSL采用两阶段学习策略。图像到视频阶段将稀疏图像级知识转移到初始化时域建模。随后的视频到视频阶段通过比较前向和后向预测来计算双向时空一致性，从而改进时域视差预测。这种一致性可以识别跨帧的不可靠区域，过滤嘈杂的视频级伪标签，并强制执行时间相干性。两个公共数据集上的实验结果表明，TiS-TSL通过将TEPE和EPE分别提高至少2.11%和4.54%，超过了其他基于图像的最先进方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决微创手术中的视频立体匹配问题，特别是在只有少量图像级标签的情况下如何实现高质量且时间一致的立体匹配。这个问题在现实中非常重要，因为微创手术需要精确的3D导航和重建，但受限于解剖结构，难以获得密集的深度标注数据。现有方法在处理视频序列时缺乏时间一致性，导致深度预测在不同帧之间闪烁，这在医疗环境中可能影响手术安全和精度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到现有教师-学生学习方法在静态图像上表现良好，但直接应用于视频时会导致严重的深度闪烁伪影。他们发现这是因为图像级教师产生时间独立的伪标签，而视频级学生需要学习时间连贯的深度表示。作者借鉴了现有的迭代优化立体匹配方法(如RAFT-Stereo和IGEV-Stereo)的门控循环单元(GRU)架构，以及教师-学生学习框架和双向预测机制，然后设计了时间可切换的三种模式(IP、FVP、BVP)来统一处理图像和视频输入。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计一个时间可切换的教师-学生学习框架(TiS-TSL)，通过统一模型在三种模式下运行(图像预测、前向视频预测、后向视频预测)，并采用两阶段学习策略：图像到视频(I2V)阶段初始化时间建模，视频到视频(V2V)阶段通过双向预测一致性过滤噪声伪标签。整体流程是：1)在I2V阶段，教师模型用IP模式为未标记视频生成伪标签，学生模型在FVP模式下学习；2)在V2V阶段，教师模型在FVP和BVP模式下进行双向预测，计算时空一致性置信度图，过滤噪声并强制时间连贯性；3)两个阶段都只更新学生参数，教师参数通过指数移动平均更新。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)时间可切换视频立体匹配模型，统一处理图像和视频输入；2)时间可切换教师-学生学习框架(TiS-TSL)，包含I2V和V2V两个阶段；3)时空置信度过滤机制，通过双向预测评估时间一致性。相比之前工作，不同之处在于：现有图像级TSL方法缺乏时间一致性估计，现有视频方法需要密集标注，而TiS-TSL只需少量图像级标签就能实现高质量时间一致的视频立体匹配；现有半监督方法只考虑空间维度，而TiS-TSL同时处理空间和时间维度的一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TiS-TSL通过时间可切换的教师-学生学习框架和时空置信度过滤机制，实现了在仅有少量图像级标签监督下的高质量、时间一致性的微创手术视频立体匹配，显著提高了预测精度并消除了深度闪烁伪影。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stereo matching in minimally invasive surgery (MIS) is essential fornext-generation navigation and augmented reality. Yet, dense disparitysupervision is nearly impossible due to anatomical constraints, typicallylimiting annotations to only a few image-level labels acquired before theendoscope enters deep body cavities. Teacher-Student Learning (TSL) offers apromising solution by leveraging a teacher trained on sparse labels to generatepseudo labels and associated confidence maps from abundant unlabeled surgicalvideos. However, existing TSL methods are confined to image-level supervision,providing only spatial confidence and lacking temporal consistency estimation.This absence of spatio-temporal reliability results in unstable disparitypredictions and severe flickering artifacts across video frames. To overcomethese challenges, we propose TiS-TSL, a novel time-switchable teacher-studentlearning framework for video stereo matching under minimal supervision. At itscore is a unified model that operates in three distinct modes: Image-Prediction(IP), Forward Video-Prediction (FVP), and Backward Video-Prediction (BVP),enabling flexible temporal modeling within a single architecture. Enabled bythis unified model, TiS-TSL adopts a two-stage learning strategy. TheImage-to-Video (I2V) stage transfers sparse image-level knowledge to initializetemporal modeling. The subsequent Video-to-Video (V2V) stage refines temporaldisparity predictions by comparing forward and backward predictions tocalculate bidirectional spatio-temporal consistency. This consistencyidentifies unreliable regions across frames, filters noisy video-level pseudolabels, and enforces temporal coherence. Experimental results on two publicdatasets demonstrate that TiS-TSL exceeds other image-based state-of-the-artsby improving TEPE and EPE by at least 2.11% and 4.54%, respectively..</description>
      <author>example@mail.com (Rui Wang, Ying Zhou, Hao Wang, Wenwei Zhang, Qiang Li, Zhiwei Wang)</author>
      <guid isPermaLink="false">2511.06817v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Otter: Mitigating Background Distractions of Wide-Angle Few-Shot Action Recognition with Enhanced RWKV</title>
      <link>http://arxiv.org/abs/2511.06741v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2026 Oral&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Otter的模型，用于解决宽视角视频少样本动作识别中的背景干扰和时间关系重建问题。该模型结合了复合分割模块和时间重建模块，能够有效突出主体并重建时间关系，在多个基准数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;宽视角视频在少样本动作识别中能有效表达特定场景中的动作，但由于背景干扰，缺乏对主体和背景的全局理解使得识别具有挑战性。RWKV虽然适合全局建模，但直接应用于宽视角FSAR时无法突出主体，且相似背景帧会降低时间关系。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够有效分割和强调关键区域、重建时间关系的模型，提高宽视角视频少样本动作识别的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了CompOund SegmenTation and Temporal REconstructing RWKV (Otter)，包含复合分割模块(CSM)用于分割和强调每帧中的关键区域，时间重建模块(TRM)用于实现双向扫描重建时间关系，并将常规原型与时间增强原型结合。&lt;h4&gt;主要发现&lt;/h4&gt;复合分割模块能有效突出主体信息，时间重建模块能更好地重建时间关系，两者的结合显著提高了宽视角少样本动作识别的性能。&lt;h4&gt;结论&lt;/h4&gt;Otter模型在SSv2、Kinetics、UCF101和HMDB51等基准测试上取得了最先进的结果，并在VideoBadminton数据集上的额外评估进一步验证了其优越性。&lt;h4&gt;翻译&lt;/h4&gt;宽视角视频在少样本动作识别中能有效表达特定场景中的动作。然而，缺乏对主体和背景的全局理解使得识别此类样本中的动作具有挑战性，因为背景干扰。学习不同维度之间交互的RWKV在全局建模方面显示出潜力。然而，将RWKV直接应用于宽视角FSAR可能因过多的背景信息而无法突出主体。此外，相似背景帧降低的时间关系难以重建，进一步影响性能。因此，我们设计了复合分割和时间重建RWKV(Otter)。具体而言，设计了复合分割模块来分割和强调每帧中的关键区域，有效突出主体信息。将时间重建模块整合到时间增强的原型构建中，实现双向扫描，更好地重建时间关系。此外，将常规原型与时间增强原型结合，同时增强主体强调和时间建模，提高宽视角FSAR性能。在SSv2、Kinetics、UCF101和HMDB51等基准上的大量实验表明，Otter达到了最先进的性能。在VideoBadminton数据集上的额外评估进一步验证了Otter在宽视角FSAR中的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wide-angle videos in few-shot action recognition (FSAR) effectively expressactions within specific scenarios. However, without a global understanding ofboth subjects and background, recognizing actions in such samples remainschallenging because of the background distractions. Receptance Weighted KeyValue (RWKV), which learns interaction between various dimensions, showspromise for global modeling. While directly applying RWKV to wide-angle FSARmay fail to highlight subjects due to excessive background information.Additionally, temporal relation degraded by frames with similar backgrounds isdifficult to reconstruct, further impacting performance. Therefore, we designthe CompOund SegmenTation and Temporal REconstructing RWKV (Otter).Specifically, the Compound Segmentation Module~(CSM) is devised to segment andemphasize key patches in each frame, effectively highlighting subjects againstbackground information. The Temporal Reconstruction Module (TRM) isincorporated into the temporal-enhanced prototype construction to enablebidirectional scanning, allowing better reconstruct temporal relation.Furthermore, a regular prototype is combined with the temporal-enhancedprototype to simultaneously enhance subject emphasis and temporal modeling,improving wide-angle FSAR performance. Extensive experiments on benchmarks suchas SSv2, Kinetics, UCF101, and HMDB51 demonstrate that Otter achievesstate-of-the-art performance. Extra evaluation on the VideoBadminton datasetfurther validates the superiority of Otter in wide-angle FSAR.</description>
      <author>example@mail.com (Wenbo Huang, Jinghui Zhang, Zhenghao Chen, Guang Li, Lei Zhang, Yang Cao, Fang Dong, Takahiro Ogawa, Miki Haseyama)</author>
      <guid isPermaLink="false">2511.06741v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>VideoSSR: Video Self-Supervised Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2511.06281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VideoSSR的视频自监督强化学习框架，用于提升多模态大语言模型(MLLMs)的视频理解能力。通过引入三种自监督预训练任务并构建相关数据集，实验表明该方法在17个跨四大视频领域的基准测试上平均提升性能超过5%。&lt;h4&gt;背景&lt;/h4&gt;可验证奖励的强化学习(RLVR)已显著提升MLLMs的视频理解能力，但MLLMs的快速发展已超出现有视频数据集的复杂性，且手动标注新的高质量数据成本过高。&lt;h4&gt;目的&lt;/h4&gt;研究是否能够利用视频内部丰富的内在信息来自生成高质量、可验证的训练数据，以解决数据标注成本高的问题。&lt;h4&gt;方法&lt;/h4&gt;引入三种自监督预训练任务(异常定位、目标计数和时间拼图)，构建视频内在理解基准(VIUBench)验证任务难度，开发VideoSSR-30K数据集，并提出VideoSSR框架用于视频自监督强化学习。&lt;h4&gt;主要发现&lt;/h4&gt;当前最先进的MLLMs在提出的预训练任务上表现不佳；VideoSSR在17个跨四大视频领域(通用视频QA、长视频QA、时间定位和复杂推理)的基准测试上一致提升模型性能，平均改进超过5%。&lt;h4&gt;结论&lt;/h4&gt;VideoSSR被确立为开发更先进MLLMs视频理解能力的强大基础框架，有效利用视频内在信息生成高质量训练数据。&lt;h4&gt;翻译&lt;/h4&gt;可验证奖励的强化学习(RLVR)已经显著提升了多模态大语言模型(MLLMs)的视频理解能力。然而，MLLMs的快速发展超出了现有视频数据集的复杂性，而手动标注新的高质量数据仍然成本过高。这项研究探讨了一个关键问题：能否利用视频内部丰富的内在信息来自生成高质量、可验证的训练数据？为了研究这一点，我们引入了三种自监督的预训练任务：异常定位、目标计数和时间拼图。我们构建了视频内在理解基准(VIUBench)来验证这些任务的难度，揭示当前最先进的MLLMs在这些任务上表现显著不佳。基于这些预训练任务，我们开发了VideoSSR-30K数据集，并提出了VideoSSR，一种用于RLVR的新型视频自监督强化学习框架。在跨越四大视频领域(通用视频QA、长视频QA、时间定位和复杂推理)的17个基准测试上的广泛实验表明，VideoSSR一致提升了模型性能，平均改进超过5%。这些结果确立了VideoSSR作为开发MLLMs更先进视频理解能力的强大基础框架。代码可在https://github.com/lcqysl/VideoSSR获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning with Verifiable Rewards (RLVR) has substantiallyadvanced the video understanding capabilities of Multimodal Large LanguageModels (MLLMs). However, the rapid progress of MLLMs is outpacing thecomplexity of existing video datasets, while the manual annotation of new,high-quality data remains prohibitively expensive. This work investigates apivotal question: Can the rich, intrinsic information within videos beharnessed to self-generate high-quality, verifiable training data? Toinvestigate this, we introduce three self-supervised pretext tasks: AnomalyGrounding, Object Counting, and Temporal Jigsaw. We construct the VideoIntrinsic Understanding Benchmark (VIUBench) to validate their difficulty,revealing that current state-of-the-art MLLMs struggle significantly on thesetasks. Building upon these pretext tasks, we develop the VideoSSR-30K datasetand propose VideoSSR, a novel video self-supervised reinforcement learningframework for RLVR. Extensive experiments across 17 benchmarks, spanning fourmajor video domains (General Video QA, Long Video QA, Temporal Grounding, andComplex Reasoning), demonstrate that VideoSSR consistently enhances modelperformance, yielding an average improvement of over 5\%. These resultsestablish VideoSSR as a potent foundational framework for developing moreadvanced video understanding in MLLMs. The code is available athttps://github.com/lcqysl/VideoSSR.</description>
      <author>example@mail.com (Zefeng He, Xiaoye Qu, Yafu Li, Siyuan Huang, Daizong Liu, Yu Cheng)</author>
      <guid isPermaLink="false">2511.06281v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Temporal-Guided Visual Foundation Models for Event-Based Vision</title>
      <link>http://arxiv.org/abs/2511.06238v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为时间引导的视觉基础模型（TGVFM）的新框架，成功地将图像预训练的视觉基础模型应用于事件相机视觉任务，通过引入时间上下文融合块解决了异步事件流处理的挑战，在多个视觉任务上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;事件相机在具有挑战性的环境视觉任务中具有独特优势，但处理异步事件流仍然是一个开放的挑战。现有方法依赖于专门的架构或资源密集型训练，而利用在图像数据上预训练的现代视觉基础模型在基于事件的视觉方面的潜力尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为时间引导的视觉基础模型（TGVFM）的新框架，将VFMs与时间上下文融合块无缝集成，以弥合图像预训练模型与事件相机视觉任务之间的差距。&lt;h4&gt;方法&lt;/h4&gt;TGVFM框架包含三个关键的时间块组件：1) 长程时间注意力用于建模全局时间依赖关系；2) 双时空注意力用于多尺度帧相关性；3) 深度特征引导机制用于融合语义-时间特征。通过在真实数据上重新训练事件到视频模型，并利用基于transformer的VFMs，保留时空动态性同时利用预训练表示。&lt;h4&gt;主要发现&lt;/h4&gt;在语义分割、深度估计和目标检测等任务上，TGVFM分别比现有方法提高了16%、21%和16%，展示了最先进的性能。这证明了基于图像的VFMs在具有时间推理能力的基于事件的视觉中的跨模态潜力。&lt;h4&gt;结论&lt;/h4&gt;这项工作成功释放了基于图像的视觉基础模型在事件相机视觉任务中的跨模态潜力，通过时间引导的框架有效解决了异步事件流处理的挑战，为未来研究提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;事件相机在具有挑战性环境中的视觉任务方面提供独特优势，但处理异步事件流仍然是一个开放的挑战。虽然现有方法依赖于专门的架构或资源密集型训练，但在基于事件的视觉中利用在图像数据上预训练的现代视觉基础模型的潜力仍未被充分探索。为此，我们提出了时间引导的视觉基础模型（TGVFM），这是一个新颖的框架，将VFMs与我们的时间上下文融合块无缝集成以弥合这一差距。我们的时间块引入了三个关键组件：（1）长程时间注意力用于建模全局时间依赖关系，（2）双时空注意力用于多尺度帧相关性，（3）深度特征引导机制用于融合语义-时间特征。通过在真实数据上重新训练事件到视频模型并利用基于transformer的VFMs，TGVFM保留了时空动态性同时利用了预训练表示。实验在语义分割、深度估计和目标检测等任务上展示了最先进的性能，分别比现有方法提高了16%、21%和16%。总体而言，这项工作释放了基于图像的VFMs在具有时间推理能力的基于事件的视觉中的跨模态潜力。代码可在https://github.com/XiaRho/TGVFM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras offer unique advantages for vision tasks in challengingenvironments, yet processing asynchronous event streams remains an openchallenge. While existing methods rely on specialized architectures orresource-intensive training, the potential of leveraging modern VisualFoundation Models (VFMs) pretrained on image data remains under-explored forevent-based vision. To address this, we propose Temporal-Guided VFM (TGVFM), anovel framework that integrates VFMs with our temporal context fusion blockseamlessly to bridge this gap. Our temporal block introduces three keycomponents: (1) Long-Range Temporal Attention to model global temporaldependencies, (2) Dual Spatiotemporal Attention for multi-scale framecorrelation, and (3) Deep Feature Guidance Mechanism to fuse semantic-temporalfeatures. By retraining event-to-video models on real-world data and leveragingtransformer-based VFMs, TGVFM preserves spatiotemporal dynamics whileharnessing pretrained representations. Experiments demonstrate SoTA performanceacross semantic segmentation, depth estimation, and object detection, withimprovements of 16%, 21%, and 16% over existing methods, respectively. Overall,this work unlocks the cross-modality potential of image-based VFMs forevent-based vision with temporal reasoning. Code is available athttps://github.com/XiaRho/TGVFM.</description>
      <author>example@mail.com (Ruihao Xia, Junhong Cai, Luziwei Leng, Liuyi Wang, Chengju Liu, Ran Cheng, Yang Tang, Pan Zhou)</author>
      <guid isPermaLink="false">2511.06238v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>TYrPPG: Uncomplicated and Enhanced Learning Capability rPPG for Remote Heart Rate Estimation</title>
      <link>http://arxiv.org/abs/2511.05833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The 6th International Workshop on AI for Social Good in the Connected  World (AI4SG)@ IEEE WI-IAT 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为TYrPPG的新型rPPG算法，通过基于Mambaout结构的门控视频理解块(GVB)和综合监督损失函数(CSL)，实现了在远程心率估计方面的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;rPPG技术可以从RGB视频中远程提取生理信号，在心率检测方面具有低成本和非侵入性的优势。现有基于transformer的rPPG模型计算效率较低，而Mamba模型在自然语言处理中表现出高效性能，但研究表明其核心SSM模块对视觉任务并非必需。&lt;h4&gt;目的&lt;/h4&gt;证明使用基于Mambaout的模块远程学习心率的可行性，并开发一种高效的rPPG算法。&lt;h4&gt;方法&lt;/h4&gt;提出TYrPPG算法，包含创新的门控视频理解块(GVB)，基于Mambaout结构整合2D-CNN和3D-CNN增强视频理解，同时提出综合监督损失函数(CSL)及其弱监督变体来提高模型学习能力。&lt;h4&gt;主要发现&lt;/h4&gt;TYrPPG在常用数据集上实现了最先进的性能，证明了其在远程心率估计方面的前景和优越性。&lt;h4&gt;结论&lt;/h4&gt;TYrPPG算法在远程心率估计方面表现出色，具有实际应用价值，源代码已公开可供使用。&lt;h4&gt;翻译&lt;/h4&gt;远程光电容积描记(rPPG)可以从RGB视频中远程提取生理信号，在心率检测方面具有许多优势，如低成本和非侵入性。现有的rPPG模型通常基于transformer模块，计算效率较低。最近，Mamba模型在自然语言处理任务中因其高效的性能而获得越来越多的关注，显示出作为基于transformer算法替代品的潜力。然而，Mambaout模型及其变体证明，Mamba模型的核心组件SSM模块对于视觉任务不是必需的。因此，我们希望证明使用基于Mambaout的模块远程学习心率的可行性。具体来说，我们提出了一种名为TYrPPG(简单且增强学习能力的rPPG)的新型rPPG算法。本文介绍了一个创新的门控视频理解块(GVB)，专为高效分析RGB视频而设计。基于Mambaout结构，该块整合了2D-CNN和3D-CNN以增强视频理解用于分析。此外，我们提出了一个综合监督损失函数(CSL)来提高模型的学习能力，以及其弱监督变体。实验表明，我们的TYrPPG在常用数据集上可以实现最先进的性能，表明其在远程心率估计方面具有前景和优越性。源代码可在https://github.com/Taixi-CHEN/TYrPPG获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remote photoplethysmography (rPPG) can remotely extract physiological signalsfrom RGB video, which has many advantages in detecting heart rate, such as lowcost and no invasion to patients. The existing rPPG model is usually based onthe transformer module, which has low computation efficiency. Recently, theMamba model has garnered increasing attention due to its efficient performancein natural language processing tasks, demonstrating potential as a substitutefor transformer-based algorithms. However, the Mambaout model and its variantsprove that the SSM module, which is the core component of the Mamba model, isunnecessary for the vision task. Therefore, we hope to prove the feasibility ofusing the Mambaout-based module to remotely learn the heart rate. Specifically,we propose a novel rPPG algorithm called uncomplicated and enhanced learningcapability rPPG (TYrPPG). This paper introduces an innovative gated videounderstanding block (GVB) designed for efficient analysis of RGB videos. Basedon the Mambaout structure, this block integrates 2D-CNN and 3D-CNN to enhancevideo understanding for analysis. In addition, we propose a comprehensivesupervised loss function (CSL) to improve the model's learning capability,along with its weakly supervised variants. The experiments show that our TYrPPGcan achieve state-of-the-art performance in commonly used datasets, indicatingits prospects and superiority in remote heart rate estimation. The source codeis available at https://github.com/Taixi-CHEN/TYrPPG.</description>
      <author>example@mail.com (Taixi Chen, Yiu-ming Cheung)</author>
      <guid isPermaLink="false">2511.05833v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>MARAuder's Map: Motion-Aware Real-time Activity Recognition with Layout-Based Trajectories</title>
      <link>http://arxiv.org/abs/2511.05773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MARAuder's Map的新框架，用于从原始、未分割的传感器流中进行实时人类活动识别，通过将传感器激活投影到物理平面图上，生成轨迹感知的类图像序列，并结合混合深度学习模型处理空间结构和时间依赖性。&lt;h4&gt;背景&lt;/h4&gt;基于环境传感器的人类活动识别在智能家居中面临挑战，需要实时推理、空间定位推理和上下文感知的时间建模。现有方法通常依赖预分割的活动内数据，忽略环境物理布局，限制了在连续真实世界部署中的鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从原始、未分割传感器流中进行实时活动识别的框架，克服现有方法的局限性，提高在智能家居环境中的活动识别准确性。&lt;h4&gt;方法&lt;/h4&gt;将传感器激活投影到物理平面图生成轨迹感知的类图像序列；使用混合深度学习模型同时捕获空间结构和时间依赖性；引入可学习的时间嵌入模块编码上下文线索；采用基于注意力的编码器选择性关注信息段，处理跨活动转换和时间模糊情况。&lt;h4&gt;主要发现&lt;/h4&gt;在多个真实世界智能家居数据集上的实验表明，该方法优于强基线方法，为环境传感器环境中的实时HAR提供了实用的解决方案。&lt;h4&gt;结论&lt;/h4&gt;MARAuder's Map框架通过考虑空间布局和时间上下文，有效提高了智能家居中实时活动识别的准确性和鲁棒性，为实际应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于环境传感器的人类活动识别在智能家居中仍然具有挑战性，因为需要实时推理、空间定位推理和上下文感知的时间建模。现有方法通常依赖于预分割的活动内数据，并忽略环境的物理布局，限制了它们在连续、真实世界部署中的鲁棒性。在本文中，我们提出了MARAuder's Map，一种用于从原始、未分割的传感器流中进行实时活动识别的新框架。我们的方法将传感器激活投影到物理平面图上，生成轨迹感知的类图像序列，捕获人类运动的空间流动。这些表示由混合深度学习模型处理，该模型同时捕获空间结构和时间依赖性。为了增强时间感知，我们引入了一个可学习的时间嵌入模块，编码上下文线索，如一天中的小时和一周中的天。此外，基于注意力的编码器选择性地关注每个观察窗口中的信息段，即使在跨活动转换和时间模糊的情况下也能实现准确的识别。在多个真实世界智能家居数据集上的广泛实验表明，我们的方法优于强基线方法，为环境传感器环境中的实时HAR提供了实用的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ambient sensor-based human activity recognition (HAR) in smart homes remainschallenging due to the need for real-time inference, spatially groundedreasoning, and context-aware temporal modeling. Existing approaches often relyon pre-segmented, within-activity data and overlook the physical layout of theenvironment, limiting their robustness in continuous, real-world deployments.In this paper, we propose MARAuder's Map, a novel framework for real-timeactivity recognition from raw, unsegmented sensor streams. Our method projectssensor activations onto the physical floorplan to generate trajectory-aware,image-like sequences that capture the spatial flow of human movement. Theserepresentations are processed by a hybrid deep learning model that jointlycaptures spatial structure and temporal dependencies. To enhance temporalawareness, we introduce a learnable time embedding module that encodescontextual cues such as hour-of-day and day-of-week. Additionally, anattention-based encoder selectively focuses on informative segments within eachobservation window, enabling accurate recognition even under cross-activitytransitions and temporal ambiguity. Extensive experiments on multiplereal-world smart home datasets demonstrate that our method outperforms strongbaselines, offering a practical solution for real-time HAR in ambient sensorenvironments.</description>
      <author>example@mail.com (Zishuai Liu, Weihang You, Jin Lu, Fei Dou)</author>
      <guid isPermaLink="false">2511.05773v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Sign language recognition from skeletal data using graph and recurrent neural networks</title>
      <link>http://arxiv.org/abs/2511.05772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种基于骨架姿态数据的手语手势识别方法，使用图门控循环单元网络建模时空依赖关系，在土耳其手语数据集上实现了高准确率的识别。&lt;h4&gt;背景&lt;/h4&gt;手语识别是帮助听障人士与人交流的重要技术，传统方法可能存在局限性，需要更有效的识别方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确识别孤立手语手势的方法，利用骨架姿态数据并有效建模时空关系。&lt;h4&gt;方法&lt;/h4&gt;使用从视频中提取的骨架姿态数据，提出图门控循环单元（Graph-GRU）时序网络来建模帧间的空间和时间依赖关系，并在AUTSL数据集上进行训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，将基于图的空间表示与时序建模相结合是有效的，在手语识别任务中取得了高准确率。&lt;h4&gt;结论&lt;/h4&gt;姿态驱动的方法在手语理解方面具有巨大潜力，所提出的框架为手语识别提供了可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;这项工作提出了一种使用从视频序列中提取的基于骨架的姿态数据来识别孤立手语手势的方法。提出了一种图门控循环单元（Graph-GRU）时序网络，用于建模帧之间的空间和时间依赖关系，实现准确分类。该模型在AUTSL（安卡拉大学土耳其手语）数据集上进行训练和评估，取得了高准确率。实验结果表明，将基于图的空间表示与时序建模相结合是有效的，为手语识别提供了可扩展的框架。这种方法的结果强调了姿态驱动方法在手语理解方面的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决孤立手语识别（ISLR）问题，即识别单个、无上下文的手语手势。这个问题在现实中非常重要，因为它可以为听障人士提供无障碍交流方式，支持手语语言表达的理解。在研究中，它是连续手语理解的基础，且传统方法在处理不同表演者、光照变化和背景差异时泛化能力有限，需要更鲁棒的解决方案。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了手语识别面临的挑战，包括类内变化和表演者差异。他们评估了现有方法（如基于RGB视频、3D-CNN和transformer架构）的局限性，发现基于姿势的方法更有潜力。作者借鉴了图神经网络处理图结构数据的能力，将人体关节表示为节点；借鉴了循环神经网络（特别是GRU）建模时间序列的能力；还借鉴了ResNet的残差连接和注意力机制。通过这些创新性组合，设计了Graph-GRU时间网络，同时捕获空间和时间依赖关系。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将每个手语手势建模为图序列，其中每个图表示特定帧的人体姿势，使用图神经网络捕获空间结构，使用GRU建模时间依赖，并通过残差连接和注意力机制增强特征保存。整体流程包括：1)从视频中提取2D骨骼关键点；2)将输入表示为T个时间步的图序列；3)通过K个由GNN和GRU组成的处理块，每块间有残差连接；4)应用时间注意力机制强调重要帧；5)通过多层分类器将特征映射到手语类别。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)混合Graph-GRU架构同时建模空间和时间依赖；2)在时空处理块间应用残差连接提高训练稳定性；3)时间注意力机制自动识别并强调手势序列中信息量最大的帧；4)高效的骨骼表示减少计算需求。相比之前的工作，该方法不同于传统RGB方法（计算量大且易受环境影响），区别于其他骨骼方法（要么缺乏时间建模，要么未充分利用关节间空间关系），性能上达到90.04%的准确率，训练时间仅约1小时，推理时间0.95秒，显著优于基于视频的方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合图神经网络和门控循环单元的混合架构，通过骨骼数据高效准确地识别孤立手语手势，为听障人士提供了更便捷的无障碍交流方式。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents an approach for recognizing isolated sign languagegestures using skeleton-based pose data extracted from video sequences. AGraph-GRU temporal network is proposed to model both spatial and temporaldependencies between frames, enabling accurate classification. The model istrained and evaluated on the AUTSL (Ankara university Turkish sign language)dataset, achieving high accuracy. Experimental results demonstrate theeffectiveness of integrating graph-based spatial representations with temporalmodeling, providing a scalable framework for sign language recognition. Theresults of this approach highlight the potential of pose-driven methods forsign language understanding.</description>
      <author>example@mail.com (B. Mederos, J. Mejía, A. Medina-Reyes, Y. Espinosa-Almeyda, J. D. Díaz-Roman, I. Rodríguez-Mederos, M. Mejía-Carreon, F. Gonzalez-Lopez)</author>
      <guid isPermaLink="false">2511.05772v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2511.05489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 17 figures. Official code:  https://github.com/Time-Search/TimeSearch-R&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出TimeSearch-R方法，通过将时间搜索重新表述为交错文本-视频思考过程，使用强化学习将视频片段搜索整合到推理过程中，并引入GRPO-CSV方法解决无监督中间搜索决策问题，显著提升了时间搜索和长视频理解能力。&lt;h4&gt;背景&lt;/h4&gt;时间搜索旨在根据给定查询从数万个帧中识别最小相关帧集，作为长视频理解的基础。现有工作尝试逐步缩小搜索空间，但这些方法通常依赖手工设计的搜索过程，缺乏端到端优化来学习最优搜索策略。&lt;h4&gt;目的&lt;/h4&gt;解决现有时间搜索方法中缺乏端到端优化的问题，提出一种能够学习最优搜索策略的方法，并通过强化学习将视频片段搜索整合到推理过程中，同时解决无监督中间搜索决策带来的探索不充分和推理不一致问题。&lt;h4&gt;方法&lt;/h4&gt;1. 提出TimeSearch-R，将时间搜索重新表述为交错文本-视频思考过程；2. 使用强化学习将视频片段搜索无缝整合到推理过程中；3. 引入GRPO-CSV方法，通过收集交错推理过程中搜索的视频帧，并使用相同的策略模型验证搜索帧的充分性；4. 构建专门设计的数据集，用于GRPO-CSV的SFT冷启动和RL训练，过滤掉时间依赖性弱的样本。&lt;h4&gt;主要发现&lt;/h4&gt;1. TimeSearch-R在Haystack-LVBench和Haystack-Ego4D等时间搜索基准测试上取得了显著改进；2. 在VideoMME和MLVU等长视频理解基准测试上也表现出色；3. 在LongVideoBench上创造了新的最先进水平，比基础模型Qwen2.5-VL提高了4.1%，比先进视频推理模型Video-R1提高了2.0%。&lt;h4&gt;结论&lt;/h4&gt;TimeSearch-R通过将时间搜索与交错文本-视频思考相结合，并引入GRPO-CSV方法，有效解决了现有方法中的局限性，显著提高了时间搜索和长视频理解的能力。&lt;h4&gt;翻译&lt;/h4&gt;时间搜索旨在根据给定查询从数万个帧中识别最小相关帧集，作为准确长视频理解的基础。现有工作尝试逐步缩小搜索空间。然而，这些方法通常依赖手工设计的搜索过程，缺乏端到端优化来学习最优搜索策略。在本文中，我们提出TimeSearch-R，它将时间搜索重新表述为交错文本-视频思考，通过强化学习将视频片段搜索无缝整合到推理过程中。然而，将RL训练方法应用于视频推理可能导致无监督的中间搜索决策，这导致视频内容探索不充分和逻辑推理不一致。为解决这些问题，我们引入了带完整性自验证的GRPO，它从交错推理过程中收集搜索的视频帧，并使用相同的策略模型验证搜索帧的充分性，从而提高视频推理的完整性。此外，我们构建了专门为GRPO-CSV的SFT冷启动和RL训练设计的数据集，过滤掉时间依赖性弱的样本，以增强任务难度并提高时间搜索能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal search aims to identify a minimal set of relevant frames from tensof thousands based on a given query, serving as a foundation for accuratelong-form video understanding. Existing works attempt to progressively narrowthe search space. However, these approaches typically rely on a hand-craftedsearch process, lacking end-to-end optimization for learning optimal searchstrategies. In this paper, we propose TimeSearch-R, which reformulates temporalsearch as interleaved text-video thinking, seamlessly integrating searchingvideo clips into the reasoning process through reinforcement learning (RL).However, applying RL training methods, such as Group Relative PolicyOptimization (GRPO), to video reasoning can result in unsupervised intermediatesearch decisions. This leads to insufficient exploration of the video contentand inconsistent logical reasoning. To address these issues, we introduce GRPOwith Completeness Self-Verification (GRPO-CSV), which gathers searched videoframes from the interleaved reasoning process and utilizes the same policymodel to verify the adequacy of searched frames, thereby improving thecompleteness of video reasoning. Additionally, we construct datasetsspecifically designed for the SFT cold-start and RL training of GRPO-CSV,filtering out samples with weak temporal dependencies to enhance taskdifficulty and improve temporal search capabilities. Extensive experimentsdemonstrate that TimeSearch-R achieves significant improvements on temporalsearch benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well aslong-form video understanding benchmarks like VideoMME and MLVU. Notably,TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%improvement over the base model Qwen2.5-VL and 2.0% over the advanced videoreasoning model Video-R1. Our code is available athttps://github.com/Time-Search/TimeSearch-R.</description>
      <author>example@mail.com (Junwen Pan, Qizhe Zhang, Rui Zhang, Ming Lu, Xin Wan, Yuan Zhang, Chang Liu, Qi She)</author>
      <guid isPermaLink="false">2511.05489v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Canonical Space Representation for 4D Panoptic Segmentation of Articulated Objects</title>
      <link>http://arxiv.org/abs/2511.05356v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 6 figures, 4 tables, submitted to Expert Systems With  Applications&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的4D全景分割框架CanonSeg4D，以及配套的Artic4D数据集，用于解决关节物体感知中的时间动态性问题。&lt;h4&gt;背景&lt;/h4&gt;关节物体感知在计算机视觉中具有重大挑战，因为大多数现有方法忽略了时间动态性，尽管这类物体本质上是动态的。4D时间数据在关节物体感知中尚未得到充分探索，在全景分割领域也未被研究，且缺乏基准数据集。&lt;h4&gt;目的&lt;/h4&gt;为了解决关节物体感知中忽略时间动态性的问题，作者引入Artic4D数据集并提出CanonSeg4D框架，用于4D全景分割。&lt;h4&gt;方法&lt;/h4&gt;Artic4D数据集源自PartNet Mobility，并增加了合成传感器数据，具有4D全景标注和关节参数。CanonSeg4D框架通过估计每帧偏移量，将观察到的物体部分映射到学习的规范空间，实现部分级分割增强，并使用规范表示实现跨连续帧的物体部分一致对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在Artic4D上的全面实验表明，所提出的CanonSeg4D在更复杂场景下的全景分割准确性优于最先进的方法，验证了时间建模和规范对齐在动态物体理解中的有效性。&lt;h4&gt;结论&lt;/h4&gt;时间建模和规范对齐对动态物体理解至关重要，该研究为4D关节物体感知的未来进展铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;关节物体感知在计算机视觉中提出了重大挑战，特别是因为大多数现有方法忽略了时间动态性，尽管这类物体本质上是动态的。4D时间数据在关节物体感知中尚未得到充分探索，在全景分割领域也尚未被研究。缺乏基准数据集进一步阻碍了该领域的发展。为此，我们引入了Artic4D作为从PartNet Mobility派生的新数据集，并增加了合成传感器数据，具有4D全景标注和关节参数。基于此数据集，我们提出了CanonSeg4D，一种新颖的4D全景分割框架。该方法明确估计每帧偏移量，将观察到的物体部分映射到学习的规范空间，从而增强部分级分割。该框架使用这种规范表示来实现跨连续帧的物体部分的一致对齐。在Artic4D上的全面实验表明，所提出的CanonSeg4D在更复杂场景下的全景分割准确性优于最先进的方法。这些发现强调了时间建模和规范对齐在动态物体理解中的有效性，并为4D关节物体感知的未来进展铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决铰接物体的4D全景分割问题。铰接物体（如柜子、抽屉、洗衣机等由多个部分通过关节连接而成）在日常生活中非常常见，理解它们的动态特性对机器人应用（如服务机器人、自动驾驶、医疗机器人）至关重要。现有方法主要针对刚性物体，忽略了铰接物体的时间动态特性，且缺乏专门的基准数据集，导致研究结果难以比较。解决这一问题对于机器人抓取、操作等任务非常重要，能够提升机器人与环境中常见铰接物体的交互能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：大多数4D分割方法是为自动驾驶场景设计的，假设物体是刚性的，不适用于铰接物体；现有变换方法基于实例质心，但铰接物体的质心会随铰接状态移动，导致表示不一致。作者借鉴了PST-Transformer用于特征提取，规范空间表示的概念（在铰接物体姿态估计中已有应用），以及Lovász-Softmax损失函数处理类别不平衡问题。核心创新是设计了规范模块，将点从4D点云序列变换到学习的规范空间，实现与铰接状态无关的一致表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用规范空间表示，学习一个与铰接状态无关的规范表示，使得不同铰接状态下的同一物体部分在规范空间中具有一致的表示，从而实现一致的实例分割。整体实现流程：1) 输入4D点云序列；2) 使用PST-Transformer backbone提取时空特征；3) 通过语义头预测每个点的语义标签；4) 通过规范模块将点变换到规范空间；5) 在规范空间中使用聚类算法将点分组为实例；6) 输出每个点在每一帧的语义和实例标签。训练时结合语义损失和规范变换损失进行优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) Artic4D数据集：首个专门用于铰接物体4D全景分割的基准数据集；2) CanonSeg4D方法：首个专门为铰接物体设计的4D全景分割框架；3) 规范空间表示：使用与铰接状态无关的规范表示实现一致的实例分割；4) 时空特征建模：充分利用时间信息建模铰接物体的动态特性；5) 专门的损失函数：使用Lovász-Softmax损失处理类别不平衡问题。相比之前的工作，CanonSeg4D专门针对铰接物体设计，规范空间提供了一致的参考框架，而非常规的质心方法；充分利用时间信息而非仅使用静态数据或两个时间戳；具有更好的泛化能力，不需要多个类别特定模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了首个专门针对铰接物体的4D全景分割方法CanonSeg4D，通过规范空间表示和时空特征建模，显著提升了铰接物体在复杂场景下的分割性能，并发布了新的基准数据集Artic4D推动该领域研究。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Articulated object perception presents significant challenges in computervision, particularly because most existing methods ignore temporal dynamicsdespite the inherently dynamic nature of such objects. The use of 4D temporaldata has not been thoroughly explored in articulated object perception andremains unexamined for panoptic segmentation. The lack of a benchmark datasetfurther hurt this field. To this end, we introduce Artic4D as a new datasetderived from PartNet Mobility and augmented with synthetic sensor data,featuring 4D panoptic annotations and articulation parameters. Building on thisdataset, we propose CanonSeg4D, a novel 4D panoptic segmentation framework.This approach explicitly estimates per-frame offsets mapping observed objectparts to a learned canonical space, thereby enhancing part-level segmentation.The framework employs this canonical representation to achieve consistentalignment of object parts across sequential frames. Comprehensive experimentson Artic4D demonstrate that the proposed CanonSeg4D outperforms state of theart approaches in panoptic segmentation accuracy in more complex scenarios.These findings highlight the effectiveness of temporal modeling and canonicalalignment in dynamic object understanding, and pave the way for future advancesin 4D articulated object perception.</description>
      <author>example@mail.com (Manuel Gomes, Bogdan Raducanu, Miguel Oliveira)</author>
      <guid isPermaLink="false">2511.05356v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>LiveStar: Live Streaming Assistant for Real-World Online Video Understanding</title>
      <link>http://arxiv.org/abs/2511.05299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 Accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LiveStar是一个创新的直播助手，通过自适应流式解码实现持续主动响应，解决了现有在线视频大语言模型在同时处理连续帧输入和确定最佳响应时间方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;现有的在线视频大语言模型(Video-LLMs)通常难以同时处理连续帧输入和确定最佳响应时间，这往往损害了实时响应能力和叙事连贯性。&lt;h4&gt;目的&lt;/h4&gt;引入LiveStar，一个通过自适应流式解码实现持续主动响应的直播助手，以解决现有在线Video-LLMs的局限性。&lt;h4&gt;方法&lt;/h4&gt;LiveStar包含三个主要创新：(1)一种训练策略，实现可变长度视频流的增量视频-语言对齐，保持动态演化帧序列的时间一致性；(2)一种响应-静默解码框架，通过单次前向验证确定最佳主动响应时间；(3)一种通过峰值结束内存压缩实现的内存感知加速，结合流式键值缓存，实现10+分钟视频的在线推理，推理速度提升1.53倍。同时构建了OmniStar数据集，包含15个多样化的真实场景和5个评估任务。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准测试中，LiveStar展示了最先进的性能，与现有在线Video-LLMs相比，语义正确性平均提高19.5%，时间差异减少18.1%，所有五个OmniStar任务的FPS提高12.0%。&lt;h4&gt;结论&lt;/h4&gt;LiveStar成功解决了在线视频理解中的实时响应和叙事连贯性问题，同时提高了推理速度，为在线视频理解领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;尽管离线视频理解中的视频大语言模型(Video-LLMs)取得了显著进展，但现有的在线Video-LLMs通常难以同时处理连续的逐帧输入并确定最佳响应时间，常常牺牲实时响应能力和叙事连贯性。为解决这些局限性，我们引入了LiveStar，这是一个开创性的直播助手，通过自适应流式解码实现持续主动响应。具体而言，LiveStar包含：(1)一种训练策略，实现可变长度视频流的增量视频-语言对齐，在动态演化的帧序列中保持时间一致性；(2)一种响应-静默解码框架，通过单次前向验证确定最佳主动响应时间；(3)通过峰值结束内存压缩实现的内存感知加速，用于10+分钟视频的在线推理，结合流式键值缓存实现1.53倍的更快推理速度。我们还构建了OmniStar数据集，这是一个全面的训练和基准测试数据集，包含15个多样化的真实场景和5个用于在线视频理解的评估任务。在三个基准测试中的大量实验证明了LiveStar的最先进性能，与现有在线Video-LLMs相比，语义正确性平均提高19.5%，时间差异减少18.1%，同时在所有五个OmniStar任务中FPS提高12.0%。我们的模型和数据集可在https://github.com/yzy-bupt/LiveStar获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant progress in Video Large Language Models (Video-LLMs) foroffline video understanding, existing online Video-LLMs typically struggle tosimultaneously process continuous frame-by-frame inputs and determine optimalresponse timing, often compromising real-time responsiveness and narrativecoherence. To address these limitations, we introduce LiveStar, a pioneeringlive streaming assistant that achieves always-on proactive responses throughadaptive streaming decoding. Specifically, LiveStar incorporates: (1) atraining strategy enabling incremental video-language alignment forvariable-length video streams, preserving temporal consistency acrossdynamically evolving frame sequences; (2) a response-silence decoding frameworkthat determines optimal proactive response timing via a single forward passverification; (3) memory-aware acceleration via peak-end memory compression foronline inference on 10+ minute videos, combined with streaming key-value cacheto achieve 1.53x faster inference. We also construct an OmniStar dataset, acomprehensive dataset for training and benchmarking that encompasses 15 diversereal-world scenarios and 5 evaluation tasks for online video understanding.Extensive experiments across three benchmarks demonstrate LiveStar'sstate-of-the-art performance, achieving an average 19.5% improvement insemantic correctness with 18.1% reduced timing difference compared to existingonline Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.</description>
      <author>example@mail.com (Zhenyu Yang, Kairui Zhang, Yuhang Hu, Bing Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Weiming Dong, Changsheng Xu)</author>
      <guid isPermaLink="false">2511.05299v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>M2S2L: Mamba-based Multi-Scale Spatial-temporal Learning for Video Anomaly Detection</title>
      <link>http://arxiv.org/abs/2511.05564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE VCIP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于Mamba的多尺度空间-时间学习框架（M2S2L），用于解决视频异常检测中检测精度与计算效率的平衡问题。&lt;h4&gt;背景&lt;/h4&gt;视频异常检测是图像处理领域的重要任务，在视频监控中有广泛应用前景。随着视频内容日益复杂，包含多样的行为模式和情境场景，传统VAD方法难以提供稳健评估，且现有方法要么缺乏全面的空间-时间建模，要么需要过多计算资源用于实时应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种既能保持高检测精度又能满足实时应用计算效率的视频异常检测方法。&lt;h4&gt;方法&lt;/h4&gt;提出Mamba-based多尺度空间-时间学习（M2S2L）框架，包含分层空间编码器在多个粒度上操作，多时间编码器捕获不同时间尺度的运动动态，以及特征分解机制实现外观和运动重建的任务特定优化。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准数据集上取得优异性能：UCSD Ped2上帧级AUC达98.5%，CUHK Avenue上达92.1%，ShanghaiTech上达77.9%，同时保持20.1 GFLOPs计算量和45 FPS推理速度的高效率。&lt;h4&gt;结论&lt;/h4&gt;M2S2L框架在保持高效率的同时实现了高精度，适合实际监控部署。&lt;h4&gt;翻译&lt;/h4&gt;视频异常检测（VAD）是图像处理社区中的一项重要任务，在视频监控领域具有应用前景，但在平衡检测精度与计算效率方面面临基本挑战。随着视频内容因多样的行为模式和情境场景而变得越来越复杂，传统的VAD方法难以对现代监控系统提供稳健评估。现有方法要么缺乏全面的空间-时间建模，要么需要过多的计算资源用于实时应用。在这方面，本文提出了一个基于Mamba的多尺度空间-时间学习（M2S2L）框架。所提出的方法采用在多个粒度上操作的分层空间编码器和捕获不同时间尺度运动动态的多时间编码器。我们还引入了一种特征分解机制，使外观和运动重建能够进行任务特定优化，促进更细致的行为建模和质量感知的异常评估。在三个基准数据集上的实验表明，M2S2L框架在UCSD Ped2、CUHK Avenue和ShanghaiTech上分别实现了98.5%、92.1%和77.9%的帧级AUC，同时保持20.1 GFLOPs的计算效率和45 FPS的推理速度，使其适用于实际监控部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video anomaly detection (VAD) is an essential task in the image processingcommunity with prospects in video surveillance, which faces fundamentalchallenges in balancing detection accuracy with computational efficiency. Asvideo content becomes increasingly complex with diverse behavioral patterns andcontextual scenarios, traditional VAD approaches struggle to provide robustassessment for modern surveillance systems. Existing methods either lackcomprehensive spatial-temporal modeling or require excessive computationalresources for real-time applications. In this regard, we present a Mamba-basedmulti-scale spatial-temporal learning (M2S2L) framework in this paper. Theproposed method employs hierarchical spatial encoders operating at multiplegranularities and multi-temporal encoders capturing motion dynamics acrossdifferent time scales. We also introduce a feature decomposition mechanism toenable task-specific optimization for appearance and motion reconstruction,facilitating more nuanced behavioral modeling and quality-aware anomalyassessment. Experiments on three benchmark datasets demonstrate that M2S2Lframework achieves 98.5%, 92.1%, and 77.9% frame-level AUCs on UCSD Ped2, CUHKAvenue, and ShanghaiTech respectively, while maintaining efficiency with 20.1GFLOPs and 45 FPS inference speed, making it suitable for practical surveillancedeployment.</description>
      <author>example@mail.com (Yang Liu, Boan Chen, Xiaoguang Zhu, Jing Liu, Peng Sun, Wei Zhou)</author>
      <guid isPermaLink="false">2511.05564v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection</title>
      <link>http://arxiv.org/abs/2511.07301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AAAI 2026. Extended version with full Appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的源域免费目标检测(SFOD)框架，利用视觉基础模型(VFMs)作为外部知识源来增强特征对齐和标签质量，通过三个专门设计的模块解决了现有SFOD方法在泛化和伪标签质量方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;现有SFOD方法主要依赖源模型的内部知识，限制了跨域泛化能力，并常常导致有偏差的伪标签，影响可转移性和判别性。视觉基础模型(VFMs)虽具有强大的感知能力和广泛的泛化性，但在SFOD场景中的潜力尚未被充分利用。&lt;h4&gt;目的&lt;/h4&gt;提出一个新的SFOD框架，利用VFMs作为外部知识源来联合增强特征对齐和标签质量，提高目标检测器在目标域上的性能。&lt;h4&gt;方法&lt;/h4&gt;设计了三个基于VFMs的模块：(1)Patch-weighted Global Feature Alignment (PGFA)使用基于patch相似性的权重从VFMs中提取全局特征；(2)Prototype-based Instance Feature Alignment (PIFA)通过动量更新的VFM原型进行实例级对比学习；(3)Dual-source Enhanced Pseudo-label Fusion (DEPF)通过熵感知策略融合检测VFMs和教师模型的预测。&lt;h4&gt;主要发现&lt;/h4&gt;在六个基准测试上的广泛实验表明，该方法实现了最先进的SFOD性能，验证了集成VFMs可以同时提高可转移性和判别性的有效性。&lt;h4&gt;结论&lt;/h4&gt;利用VFMs作为外部知识源可以有效提升SFOD的性能，通过三个专门设计的模块解决了现有方法在泛化和伪标签质量方面的局限性。&lt;h4&gt;翻译&lt;/h4&gt;无源域目标检测(SFOD)旨在将源预训练的目标检测器适应到目标域而无需访问源数据。然而，现有的SFOD方法主要依赖源模型的内部知识，这限制了它们跨域泛化的能力，并常常导致有偏差的伪标签，从而阻碍了可转移性和判别性。相比之下，在大量多样化数据上预训练的视觉基础模型(VFMs)表现出强大的感知能力和广泛的泛化性，但在SFOD设置中它们的潜力仍未被充分利用。在本文中，我们提出了一种新颖的SFOD框架，利用VFMs作为外部知识源来联合增强特征对齐和标签质量。具体来说，我们设计了三个基于VFMs的模块：(1)基于补丁权重的全局特征对齐(PGFA)使用基于补丁相似性的权重从VFMs中提取全局特征，以增强全局特征的可转移性；(2)基于原型的实例特征对齐(PIFA)由动量更新的VFM原型引导，执行实例级对比学习；(3)双源增强的伪标签融合(DEPF)通过熵感知策略融合检测VFMs和教师模型的预测，以获得更可靠的监督。在六个基准测试上的广泛实验证明，我们的方法实现了最先进的SFOD性能，验证了集成VFMs可以同时提高可转移性和判别性的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Source-Free Object Detection (SFOD) aims to adapt a source-pretrained objectdetector to a target domain without access to source data. However, existingSFOD methods predominantly rely on internal knowledge from the source model,which limits their capacity to generalize across domains and often results inbiased pseudo-labels, thereby hindering both transferability anddiscriminability. In contrast, Vision Foundation Models (VFMs), pretrained onmassive and diverse data, exhibit strong perception capabilities and broadgeneralization, yet their potential remains largely untapped in the SFODsetting. In this paper, we propose a novel SFOD framework that leverages VFMsas external knowledge sources to jointly enhance feature alignment and labelquality. Specifically, we design three VFM-based modules: (1) Patch-weightedGlobal Feature Alignment (PGFA) distills global features from VFMs usingpatch-similarity-based weighting to enhance global feature transferability; (2)Prototype-based Instance Feature Alignment (PIFA) performs instance-levelcontrastive learning guided by momentum-updated VFM prototypes; and (3)Dual-source Enhanced Pseudo-label Fusion (DEPF) fuses predictions fromdetection VFMs and teacher models via an entropy-aware strategy to yield morereliable supervision. Extensive experiments on six benchmarks demonstrate thatour method achieves state-of-the-art SFOD performance, validating theeffectiveness of integrating VFMs to simultaneously improve transferability anddiscriminability.</description>
      <author>example@mail.com (Huizai Yao, Sicheng Zhao, Pengteng Li, Yi Cui, Shuo Lu, Weiyu Guo, Yunfan Lu, Yijie Xu, Hui Xiong)</author>
      <guid isPermaLink="false">2511.07301v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Hard vs. Noise: Resolving Hard-Noisy Sample Confusion in Recommender Systems via Large Language Models</title>
      <link>http://arxiv.org/abs/2511.07295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LLMHNI的框架，利用大型语言模型生成的辅助信号来区分推荐系统中的困难样本和噪声样本，解决了传统方法中的困难-噪声混淆问题，显著提高了去噪和推荐性能。&lt;h4&gt;背景&lt;/h4&gt;推荐系统训练中使用的隐式反馈不可避免地面临噪声问题，如误点击和位置偏差等。先前研究试图通过数据模式识别噪声样本并通过样本丢弃或重新加权来减轻其影响。&lt;h4&gt;目的&lt;/h4&gt;解决噪声样本和困难样本之间的混淆问题，避免因错误去除困难样本而影响用户偏好建模，从而提高推荐系统的去噪和推荐性能。&lt;h4&gt;方法&lt;/h4&gt;提出LLMHNI框架，利用大型语言模型生成两种辅助信号：1)用户-项目语义相关性，用于负采样中选择困难负样本并过滤噪声负样本；2)用户-项目交互中的逻辑相关性，用于识别困难样本和噪声样本。还提出目标对齐策略将LLM嵌入投影到优化空间，以及图对比学习策略抑制不可靠边。&lt;h4&gt;主要发现&lt;/h4&gt;LLMHNI框架能有效区分困难样本和噪声样本，显著提高去噪效果和推荐系统性能。&lt;h4&gt;结论&lt;/h4&gt;通过利用大型语言模型的能力，可以有效解决推荐系统中的困难-噪声混淆问题，提升推荐质量。&lt;h4&gt;翻译&lt;/h4&gt;在训练推荐系统时使用的隐式反馈不可避免地因误点击和位置偏差等因素而面临噪声问题。先前研究试图通过数据模式（如更高的损失值）识别噪声样本，并通过样本丢弃或重新加权来减轻其影响。然而，我们观察到噪声样本和困难样本显示相似模式，导致困难-噪声混淆问题。这种混淆是有问题的，因为困难样本对建模用户偏好至关重要。为解决这个问题，我们提出了LLMHNI框架，利用大型语言模型生成的两种辅助用户-项目相关性信号来区分困难样本和噪声样本。LLMHNI从LLM编码的嵌入中获取用户-项目语义相关性，用于负采样中选择困难负样本，同时过滤掉噪声负样本。提出了一种目标对齐策略，将原本用于通用语言任务的LLM编码嵌入投影到针对用户-项目相关性建模优化的表示空间。LLMHNI还利用LLM推断的用户-项目交互中的逻辑相关性来识别困难样本和噪声样本。这些LLM推断的交互被整合到交互图中，并通过跨图对比对齐指导去噪。为消除LLM幻觉引起的不可靠交互的影响，我们提出了一种图对比学习策略，通过对随机边丢弃视图中的表示进行对齐来抑制不可靠边。实验结果表明，LLMHNI显著提高了去噪和推荐性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Implicit feedback, employed in training recommender systems, unavoidablyconfronts noise due to factors such as misclicks and position bias. Previousstudies have attempted to identify noisy samples through their diverged datapatterns, such as higher loss values, and mitigate their influence throughsample dropping or reweighting. However, we observed that noisy samples andhard samples display similar patterns, leading to hard-noisy confusion issue.Such confusion is problematic as hard samples are vital for modeling userpreferences. To solve this problem, we propose LLMHNI framework, leveraging twoauxiliary user-item relevance signals generated by Large Language Models (LLMs)to differentiate hard and noisy samples. LLMHNI obtains user-item semanticrelevance from LLM-encoded embeddings, which is used in negative sampling toselect hard negatives while filtering out noisy false negatives. An objectivealignment strategy is proposed to project LLM-encoded embeddings, originallyfor general language tasks, into a representation space optimized for user-itemrelevance modeling. LLMHNI also exploits LLM-inferred logical relevance withinuser-item interactions to identify hard and noisy samples. These LLM-inferredinteractions are integrated into the interaction graph and guide denoising withcross-graph contrastive alignment. To eliminate the impact of unreliableinteractions induced by LLM hallucination, we propose a graph contrastivelearning strategy that aligns representations from randomly edge-dropped viewsto suppress unreliable edges. Empirical results demonstrate that LLMHNIsignificantly improves denoising and recommendation performance.</description>
      <author>example@mail.com (Tianrui Song, Wen-Shuo Chao, Hao Liu)</author>
      <guid isPermaLink="false">2511.07295v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>From Pretrain to Pain: Adversarial Vulnerability of Video Foundation Models Without Task Knowledge</title>
      <link>http://arxiv.org/abs/2511.07049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AAAI 2026 (Oral presentation)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为可迁移视频攻击(TVA)的新型对抗攻击方法，利用视频基础模型的时间表示动力学创建有效扰动，无需访问目标任务或训练代理模型，实验证明该方法对下游模型和多模态大语言模型有效，揭示了视频模型部署中的安全漏洞。&lt;h4&gt;背景&lt;/h4&gt;大规模视频基础模型(VFMs)已显著推进各种视频相关任务的发展，但其开源性也带来了严重的安全风险，攻击者可利用对VFMs的完整知识发起强大攻击。&lt;h4&gt;目的&lt;/h4&gt;研究一种新颖且实用的对抗威胁场景：攻击基于开源VFMs微调的下游模型或MLLMs，无需访问目标任务、训练数据、模型查询和架构信息。&lt;h4&gt;方法&lt;/h4&gt;提出可迁移视频攻击(TVA)，一种时间感知的对抗攻击方法，整合双向对比学习机制最大化干净和对抗特征差异，并引入时间一致性损失利用运动线索增强扰动顺序影响。&lt;h4&gt;主要发现&lt;/h4&gt;TVA避免了训练昂贵的代理模型或访问特定领域数据的需求，提供更实用高效的攻击策略，在24个视频相关任务上证明了对下游模型和MLLMs的有效性。&lt;h4&gt;结论&lt;/h4&gt;研究揭示了视频模型部署中一个先前未被充分探索的安全漏洞，强调了在开发和部署视频基础模型时需要考虑安全问题。&lt;h4&gt;翻译&lt;/h4&gt;大规模视频基础模型(VFMs)已显著推进了各种视频相关任务的发展，无论是通过特定任务模型还是多模态大语言模型(MLLMs)。然而，VFMs的开源性也引入了严重的安全风险，因为攻击者可以利用对VFMs的完整知识来发起强大攻击。本文研究了一种新颖且实用的对抗威胁场景：攻击基于开源VFMs微调的下游模型或MLLMs，无需访问目标任务、训练数据、模型查询和架构信息。与依赖任务对齐代理模型的传统基于迁移的攻击不同，我们证明对抗性漏洞可以直接从VFMs中利用。为此，我们提出了可迁移视频攻击(TVA)，一种时间感知的对抗攻击方法，利用VFMs的时间表示动力学来创建有效扰动。TVA整合了双向对比学习机制，以最大化干净和对抗特征之间的差异，并引入了时间一致性损失，利用运动线索来增强扰动的顺序影响。TVA避免了训练昂贵的代理模型或访问特定领域数据的需求，从而提供了一种更实用和高效的攻击策略。在24个视频相关任务上的广泛实验证明了TVA对下游模型和MLLMs的有效性，揭示了视频模型部署中一个先前未被充分探索的安全漏洞。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale Video Foundation Models (VFMs) has significantly advanced variousvideo-related tasks, either through task-specific models or Multi-modal LargeLanguage Models (MLLMs). However, the open accessibility of VFMs alsointroduces critical security risks, as adversaries can exploit full knowledgeof the VFMs to launch potent attacks. This paper investigates a novel andpractical adversarial threat scenario: attacking downstream models or MLLMsfine-tuned from open-source VFMs, without requiring access to the victim task,training data, model query, and architecture. In contrast to conventionaltransfer-based attacks that rely on task-aligned surrogate models, wedemonstrate that adversarial vulnerabilities can be exploited directly from theVFMs. To this end, we propose the Transferable Video Attack (TVA), atemporal-aware adversarial attack method that leverages the temporalrepresentation dynamics of VFMs to craft effective perturbations. TVAintegrates a bidirectional contrastive learning mechanism to maximize thediscrepancy between the clean and adversarial features, and introduces atemporal consistency loss that exploits motion cues to enhance the sequentialimpact of perturbations. TVA avoids the need to train expensive surrogatemodels or access to domain-specific data, thereby offering a more practical andefficient attack strategy. Extensive experiments across 24 video-related tasksdemonstrate the efficacy of TVA against downstream models and MLLMs, revealinga previously underexplored security vulnerability in the deployment of videomodels.</description>
      <author>example@mail.com (Hui Lu, Yi Yu, Song Xia, Yiming Yang, Deepu Rajan, Boon Poh Ng, Alex Kot, Xudong Jiang)</author>
      <guid isPermaLink="false">2511.07049v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Design Principles of Zero-Shot Self-Supervised Unknown Emitter Detectors</title>
      <link>http://arxiv.org/abs/2511.07026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对无线设备激增背景下的未知发射器检测问题，提出了全面的系统评估方法，并引入了2D-星座数据模态、KANs网络和SVD初始化程序，显著提升了检测性能。&lt;h4&gt;背景&lt;/h4&gt;无线设备的快速增长使得关键任务如频谱管理和网络安全需要更强大可靠的发射器检测与识别技术。&lt;h4&gt;目的&lt;/h4&gt;对未知发射器检测系统进行全面评估，重点关注数据模态、学习方法和特征学习模块等设计空间的关键方面。&lt;h4&gt;方法&lt;/h4&gt;提出2D-星座数据模态处理不同消息场景；引入可解释的Kolmogorov-Arnold Networks增强模型透明度；提出基于奇异值分解的特征学习模块初始化程序处理稀疏2D-星座数据；评估深度聚类、自编码器和对比学习三种学习方法。&lt;h4&gt;主要发现&lt;/h4&gt;先前方法通常使用相同传输消息的数据集；2D-星座数据模态相比传统I/Q数据在ROC-AUC、NMI和F1指标上提升高达40%；SVD初始化的特征学习模块使深度聚类方法性能提升高达40%。&lt;h4&gt;结论&lt;/h4&gt;通过系统评估和创新方法，显著提升了未知发射器检测系统的性能和可靠性，为频谱管理和网络安全提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;无线设备的激增对关键任务（如频谱管理和网络安全）中的发射器检测和识别提出了更强大可靠的需求。然而，探索未知发射器识别方法的现有研究通常受到标记数据或专有数据集的限制，存在不切实际的假设（如所有样本具有相同的传输消息），或缺乏对不同架构和设计维度的系统评估。在这项工作中，我们对未知发射器检测系统在设计空间的关键方面进行了全面评估，重点关注数据模态、学习方法和特征学习模块。我们证明了先前自监督、零样本发射器检测方法通常使用具有相同传输消息的数据集。为解决这一限制，我们提出了用于不同消息场景的2D-星座数据模态，相比传统原始I/Q数据，在ROC-AUC、NMI和F1指标上实现了高达40%的性能提升。此外，我们引入了可解释的Kolmogorov-Arnold Networks以增强模型透明度，以及基于奇异值分解的特征学习模块初始化程序，用于处理稀疏的2D-星座数据，使深度聚类方法的性能相比无SVD初始化的模块提升了高达40%。我们在三种学习方法（深度聚类、自编码器和对比学习）下评估了所有数据模态和学习模块。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of wireless devices necessitates more robust and reliableemitter detection and identification for critical tasks such as spectrummanagement and network security. Existing studies exploring methods for unknownemitters identification, however, are typically hindered by their dependence onlabeled or proprietary datasets, unrealistic assumptions (e.g. all samples withidentical transmitted messages), or deficiency of systematic evaluations acrossdifferent architectures and design dimensions. In this work, we present acomprehensive evaluation of unknown emitter detection systems across keyaspects of the design space, focusing on data modality, learning approaches,and feature learn- ing modules. We demonstrate that prior self-supervised,zero-shot emitter detection approaches commonly use datasets with identicaltransmitted messages. To address this limitation, we propose a 2D-Constellation data modality for scenarios with varying messages, achieving upto a 40\% performance improvement in ROC-AUC, NMI, and F1 metrics compared toconventional raw I/Q data. Furthermore, we introduce interpretableKolmogorov--Arnold Net- works (KANs) to enhance model transparency, and aSingular Value Decomposition (SVD)-based initialization procedure for featurelearning modules operating on sparse 2D-Constellation data, which improves theperformance of Deep Clustering approaches by up to 40\% across the same metricscomparing to the modules without SVD initialization. We evaluate all datamodalities and learning modules across three learning approaches: DeepClustering, Auto Encoder and Contrastive Learning.</description>
      <author>example@mail.com (Mikhail Krasnov, Ljupcho Milosheski, Mihael Mohorčič, Carolina Fortuna)</author>
      <guid isPermaLink="false">2511.07026v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>S$^2$Drug: Bridging Protein Sequence and 3D Structure in Contrastive Representation Learning for Virtual Screening</title>
      <link>http://arxiv.org/abs/2511.07006v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2026 Main Technical Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了S²Drug两阶段框架，整合蛋白质序列信息和3D结构上下文，通过对比表示学习提高虚拟筛选性能，结合预训练和微调两个阶段，并引入辅助结合位点预测任务增强蛋白质-配体匹配。&lt;h4&gt;背景&lt;/h4&gt;虚拟筛选是药物发现中的关键任务，专注于识别能与特定蛋白质口袋结合的小分子配体。现有深度学习方法主要依赖结构数据，忽略了更易获取且能提高泛化能力的蛋白质序列信息。然而，直接整合蛋白质序列面临大规模蛋白质-配体数据集中的冗余和噪声挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有虚拟筛选方法忽视蛋白质序列信息的问题，处理大规模数据集中的冗余和噪声，开发能有效整合蛋白质序列和结构信息的框架，提高虚拟筛选性能。&lt;h4&gt;方法&lt;/h4&gt;S²Drug采用两阶段框架：第一阶段使用基于ESM2的主干在ChemBL上进行蛋白质序列预训练，结合定制数据采样策略减少冗余和噪声；第二阶段通过残基级门控模块融合序列和结构信息，在PDBBind上微调，并引入辅助结合位点预测任务，指导模型定位结合残基并捕获其3D空间排列。&lt;h4&gt;主要发现&lt;/h4&gt;S²Drug在多个基准测试中一致提高了虚拟筛选性能，并在结合位点预测上取得良好结果，证明了在对比学习中桥接序列和结构的价值。&lt;h4&gt;结论&lt;/h4&gt;通过整合蛋白质序列信息和3D结构上下文，S²Drug有效解决了现有虚拟筛选方法的局限性，提高了虚拟筛选性能，展示了结合序列和结构信息在药物发现中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;虚拟筛选是药物发现中的基本任务，专注于识别能与特定蛋白质口袋结合的小分子配体。从早期的回归模型到最近的对比学习方法，现有的深度学习方法主要依赖结构数据，而忽略了更易获取且能提高泛化能力的蛋白质序列。然而，由于大规模蛋白质-配体数据集中的冗余和噪声，直接整合蛋白质序列存在挑战。为解决这些限制，我们提出了S²Drug，这是一个两阶段框架，明确地将蛋白质序列信息和3D结构上下文整合到蛋白质-配体对比表示学习中。在第一阶段，我们使用基于ESM2的主干在ChemBL上进行蛋白质序列预训练，结合定制的数据采样策略，减少蛋白质和配体两方面的冗余和噪声。在第二阶段，我们通过残基级门控模块融合序列和结构信息，并在PDBBind上进行微调，同时引入辅助结合位点预测任务。这个辅助任务指导模型准确地在蛋白质序列中定位结合残基并捕获其3D空间排列，从而优化蛋白质-配体匹配。在多个基准测试中，S²Drug一致提高了虚拟筛选性能，并在结合位点预测上取得了显著结果，证明了在对比学习中桥接序列和结构的价值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有虚拟筛选方法过度依赖蛋白质3D结构而忽略蛋白质序列信息的问题。这个问题很重要，因为蛋白质序列比3D结构更容易获取，能增强模型泛化能力，且蛋白质序列包含蛋白质折叠和功能的基本信息，影响蛋白质与配体的相互作用。此外，确定3D结构技术复杂且昂贵，限制了大规模训练数据集的扩展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于'序列决定结构，结构决定功能'的基本原理，认识到现有方法仅利用预训练模型编码的序列表示而没有显式学习序列-配体相互作用。他们发现大规模蛋白质序列-配体数据集(如ChemBL)的潜力尚未被充分利用，但直接整合这些数据集面临冗余和噪声挑战。设计了两阶段框架：第一阶段在ChemBL上进行序列预训练，结合数据采样策略减少冗余和噪声；第二阶段在PDBBind上微调，通过门控模块融合序列和结构信息，并引入辅助结合位点预测任务。借鉴了ESM2模型、Uni-Mol编码器和对比学习等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将蛋白质序列和3D结构信息结合起来进行虚拟筛选，通过两阶段训练框架和辅助任务增强模型对蛋白质空间结构的理解。整体流程分为两阶段：第一阶段是序列模型预训练，包括双边数据采样(减少蛋白质侧冗余和配体侧噪声)、使用ESM2和Uni-Mol编码器进行表示学习、以及对比训练对齐蛋白质和配体表示；第二阶段是序列-结构融合微调，包括通过残基级门控模块自适应融合序列和结构信息、引入辅助结合位点预测任务、以及结合对比损失和结合位点预测损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：两阶段对比表示学习框架结合序列和结构信息、双边数据采样策略减少数据集冗余和噪声、序列-结构融合模块通过门控机制自适应融合信息、辅助结合位点预测任务增强空间结构理解。相比之前工作，S2Drug是首个将蛋白质序列和3D结构信息显式结合用于虚拟筛选的对比学习框架，解决了之前方法过度依赖3D结构或仅简单利用序列表示的问题，通过创新的数据采样策略和融合机制显著提高了虚拟筛选的准确性和泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; S2Drug通过创新的两阶段对比学习框架有效整合蛋白质序列和3D结构信息，显著提高了虚拟筛选的准确性和泛化能力，同时为结合位点预测提供了新思路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Virtual screening (VS) is an essential task in drug discovery, focusing onthe identification of small-molecule ligands that bind to specific proteinpockets. Existing deep learning methods, from early regression models to recentcontrastive learning approaches, primarily rely on structural data whileoverlooking protein sequences, which are more accessible and can enhancegeneralizability. However, directly integrating protein sequences poseschallenges due to the redundancy and noise in large-scale protein-liganddatasets. To address these limitations, we propose \textbf{S$^2$Drug}, atwo-stage framework that explicitly incorporates protein \textbf{S}equenceinformation and 3D \textbf{S}tructure context in protein-ligand contrastiverepresentation learning. In the first stage, we perform protein sequencepretraining on ChemBL using an ESM2-based backbone, combined with a tailoreddata sampling strategy to reduce redundancy and noise on both protein andligand sides. In the second stage, we fine-tune on PDBBind by fusing sequenceand structure information through a residue-level gating module, whileintroducing an auxiliary binding site prediction task. This auxiliary taskguides the model to accurately localize binding residues within the proteinsequence and capture their 3D spatial arrangement, thereby refiningprotein-ligand matching. Across multiple benchmarks, S$^2$Drug consistentlyimproves virtual screening performance and achieves strong results on bindingsite prediction, demonstrating the value of bridging sequence and structure incontrastive learning.</description>
      <author>example@mail.com (Bowei He, Bowen Gao, Yankai Chen, Yanyan Lan, Chen Ma, Philip S. Yu, Ya-Qin Zhang, Wei-Ying Ma)</author>
      <guid isPermaLink="false">2511.07006v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning</title>
      <link>http://arxiv.org/abs/2511.06854v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;iTimER是一种自监督预训练框架，通过利用重建误差作为学习信号，为不规则采样时间序列表示学习提供了一种简单有效的方法。&lt;h4&gt;背景&lt;/h4&gt;不规则采样时间序列在现实应用中普遍存在，特点是时间间隔不均匀且自然缺失。现有方法主要依赖观测值来推断未观测值或潜在动态，但忽略了训练过程中产生的重建误差这一重要学习信号源。&lt;h4&gt;目的&lt;/h4&gt;提出iTimER框架，利用重建误差作为学习信号，提高不规则采样时间序列的表示学习效果。&lt;h4&gt;方法&lt;/h4&gt;iTimER建模重建误差在观测值上的分布，通过混合策略为未观测时间戳生成伪观测值，使未观测时间戳成为噪声感知的训练目标。使用Wasserstein度量对齐不同区域间的重建误差分布，并通过对比学习增强表示的判别性。&lt;h4&gt;主要发现&lt;/h4&gt;重建误差能反映模型捕获底层数据结构的程度，可作为未观测值的信息代理。通过将未观测时间戳转化为噪声感知的训练目标，可以实现有意义的重建信号。&lt;h4&gt;结论&lt;/h4&gt;在分类、插值和预测任务上的实验表明，iTimER在ISTS设置下持续优于最先进的方法，证明了其有效性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;不规则采样时间序列具有非均匀时间间隔和自然缺失的特点，在现实世界中很常见。现有的ISTS建模方法主要依赖观测值来推断未观测值或潜在动态。然而，这些方法忽略了一个关键的学习信号来源：模型训练过程中自然产生的重建误差。这种误差隐式地反映了模型捕获底层数据结构的程度，并可作为未观测值的信息代理。为利用这一见解，我们提出了iTimER，一个简单而有效的自监督预训练框架，用于ISTS表示学习。iTimER建模重建误差在观测值上的分布，并通过采样误差和最后可用观测值之间的混合策略为未观测时间戳生成伪观测值。这使未观测时间戳成为噪声感知的训练目标，实现有意义的重建信号。Wasserstein度量对齐观测区域和伪观测区域之间的重建误差分布，同时对比学习目标增强了学习表示的判别性。在分类、插值和预测任务上的广泛实验表明，iTimER在ISTS设置下持续优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Irregularly sampled time series (ISTS), characterized by non-uniform timeintervals with natural missingness, are prevalent in real-world applications.Existing approaches for ISTS modeling primarily rely on observed values toimpute unobserved ones or infer latent dynamics. However, these methodsoverlook a critical source of learning signal: the reconstruction errorinherently produced during model training. Such error implicitly reflects howwell a model captures the underlying data structure and can serve as aninformative proxy for unobserved values. To exploit this insight, we proposeiTimER, a simple yet effective self-supervised pre-training framework for ISTSrepresentation learning. iTimER models the distribution of reconstructionerrors over observed values and generates pseudo-observations for unobservedtimestamps through a mixup strategy between sampled errors and the lastavailable observations. This transforms unobserved timestamps into noise-awaretraining targets, enabling meaningful reconstruction signals. A Wassersteinmetric aligns reconstruction error distributions between observed andpseudo-observed regions, while a contrastive learning objective enhances thediscriminability of learned representations. Extensive experiments onclassification, interpolation, and forecasting tasks demonstrate that iTimERconsistently outperforms state-of-the-art methods under the ISTS setting.</description>
      <author>example@mail.com (Jiexi Liu, Meng Cao, Songcan Chen)</author>
      <guid isPermaLink="false">2511.06854v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Breaking the Modality Barrier: Generative Modeling for Accurate Molecule Retrieval from Mass Spectra</title>
      <link>http://arxiv.org/abs/2511.06259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GLMR是一种基于生成语言模型的检索框架，通过两阶段处理过程解决了现有方法中的质谱库覆盖不足和模态不匹配问题，大幅提高了检索准确性。&lt;h4&gt;背景&lt;/h4&gt;从串联质谱中检索分子结构是快速化合物识别的关键步骤。现有的检索方法存在局限性：传统的质谱库匹配方法受限于质谱库覆盖范围，而最近的跨模态表示学习框架常常遇到模态不匹配问题，导致次优的检索精度和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提高分子结构检索的准确性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了GLMR框架，通过两阶段过程减轻跨模态不匹配：1) 预检索阶段：基于对比学习的模型识别候选分子作为输入质谱的上下文先验；2) 生成检索阶段：将候选分子与输入质谱整合，引导生成模型产生精细化的分子结构，然后基于分子相似度重新排序候选分子。&lt;h4&gt;主要发现&lt;/h4&gt;在MassSpecGym和MassRET-20k数据集上的实验表明，GLMR显著优于现有方法，在top-1准确性上实现了超过40%的改进，并表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;GLMR框架有效解决了跨模态不匹配问题，显著提高了分子结构检索的准确性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;从串联质谱中检索分子结构是快速化合物识别的关键步骤。现有的检索方法，如传统的质谱库匹配，受限于质谱库覆盖范围，而最近的跨模态表示学习框架常常遇到模态不匹配问题，导致次优的检索精度和泛化能力。为解决这些限制，我们提出了GLMR，一种基于生成语言模型的检索框架，通过两阶段过程减轻跨模态不匹配。在预检索阶段，基于对比学习的模型将候选分子识别为输入质谱的上下文先验。在生成检索阶段，这些候选分子与输入质谱整合，引导生成模型产生精细化的分子结构，然后基于分子相似度重新排序候选分子。在MassSpecGym和我们提出的MassRET-20k数据集上的实验表明，GLMR显著优于现有方法，在top-1准确性上实现了超过40%的改进，并表现出强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieving molecular structures from tandem mass spectra is a crucial step inrapid compound identification. Existing retrieval methods, such as traditionalmass spectral library matching, suffer from limited spectral library coverage,while recent cross-modal representation learning frameworks often encountermodality misalignment, resulting in suboptimal retrieval accuracy andgeneralization. To address these limitations, we propose GLMR, a GenerativeLanguage Model-based Retrieval framework that mitigates the cross-modalmisalignment through a two-stage process. In the pre-retrieval stage, acontrastive learning-based model identifies top candidate molecules ascontextual priors for the input mass spectrum. In the generative retrievalstage, these candidate molecules are integrated with the input mass spectrum toguide a generative model in producing refined molecular structures, which arethen used to re-rank the candidates based on molecular similarity. Experimentson both MassSpecGym and the proposed MassRET-20k dataset demonstrate that GLMRsignificantly outperforms existing methods, achieving over 40% improvement intop-1 accuracy and exhibiting strong generalizability.</description>
      <author>example@mail.com (Yiwen Zhang, Keyan Ding, Yihang Wu, Xiang Zhuang, Yi Yang, Qiang Zhang, Huajun Chen)</author>
      <guid isPermaLink="false">2511.06259v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Multi-view Graph Contrastive Learning via Fractional-order Neural Diffusion Networks</title>
      <link>http://arxiv.org/abs/2511.06216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to TPAMI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于分数阶连续动力学的无需增强的多视图图对比学习框架，通过可学习的分数导数阶数自动生成多样化的视图表示，实验证明该方法能产生更鲁棒和更具表达力的嵌入，优于现有最先进的图对比学习方法。&lt;h4&gt;背景&lt;/h4&gt;现有的图对比学习方法通常依赖于固定的、手工制作的视图（通常是局部和全局视角），这限制了它们捕捉多尺度结构模式的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需增强的、多视图的图对比学习框架，能够自动发现信息量大的视图，提高表示学习的鲁棒性和表达力。&lt;h4&gt;方法&lt;/h4&gt;基于分数阶连续动力学，通过改变分数导数阶数使编码器产生连续的视图谱：小阶数产生局部特征，大阶数诱导更广泛的全球聚合。将阶数视为可学习参数，使模型能够适应数据的扩散尺度并自动发现信息量大的视图。&lt;h4&gt;主要发现&lt;/h4&gt;通过将分数导数阶数作为可学习参数，模型能够自动适应数据的扩散尺度并发现信息量大的视图，无需手动增强即可生成多样化、互补的表示。&lt;h4&gt;结论&lt;/h4&gt;在标准基准上的大量实验表明，该方法产生更鲁棒和更具表达力的嵌入，并且优于最先进的图对比学习基线。&lt;h4&gt;翻译&lt;/h4&gt;图对比学习通过对同一图的多个视图进行对比来学习节点和图表示。现有方法通常依赖于固定的、手工制作的视图——通常是局部和全局视角，这限制了它们捕捉多尺度结构模式的能力。我们提出了一种无需增强的、多视图的图对比学习框架，该框架基于分数阶连续动力学。通过改变分数导数阶数，我们的编码器产生连续的视图谱：小阶数产生局部特征，而大阶数诱导更广泛的全局聚合。我们将阶数视为可学习参数，使模型能够适应数据的扩散尺度并自动发现信息量大的视图。这种原则性方法无需手动增强即可生成多样化、互补的表示。在标准基准上的大量实验证明，我们的方法产生更鲁棒和更具表达力的嵌入，并优于最先进的图对比学习基线。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决图对比学习(GCL)中多视图生成的局限性问题。现有方法通常依赖固定的、手工设计的视图(如局部和全局视角)，限制了捕获多尺度结构模式的能力。这个问题很重要，因为真实世界图数据往往包含复杂的多尺度结构信息，自适应生成多样化视图对于提升图表示学习性能至关重要，特别是在异质图(heterophilic graphs)中，简单方法往往效果不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有图对比学习方法受限于固定视图，无法充分捕获多尺度结构。他们借鉴分数阶微分方程(FDEs)理论，特别是其在图扩散建模中的应用。作者的关键洞察是分数阶参数α可以控制扩散尺度，通过改变α产生连续视图谱(小α强调局部，大α强调全局)。基于此，他们设计了FD-MVGCL框架，将α作为可学习参数，并解决了维度坍塌和视图坍塌两个核心挑战。该方法借鉴了图神经网络、分数阶微分方程在GNN中的应用以及对比学习理论，但创新性地将这些思想整合用于自适应多视图生成。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用分数阶微分方程(FDEs)的连续特性，通过分数阶导数阶数α∈(0,1]控制图扩散的尺度，将α作为可学习参数使模型自适应地发现信息丰富的视图。小α值产生局部特征视图，大α值产生全局特征视图，从而生成多样互补的表示。整体流程：1)输入图数据；2)每个编码器进行特征投影、分数阶扩散和输出；3)对连续视图对应用正则化对比损失；4)使用自适应视图学习算法动态选择编码器数量和α值；5)下游任务中计算多个视图的加权平均作为最终表示。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次利用分数阶动力学的多尺度行为进行对比学习，通过α参数生成连续视图谱；2)提供了严格的理论分析，证明不同α值产生的特征表示的可区分性；3)通过小分数阶编码器缓解维度坍塌，通过正则化对比目标防止视图坍塌；4)将α作为可学习参数，提出自适应视图学习算法；5)提供了稳定性分析，量化了扰动影响。相比之前工作，FD-MVGCL无需手工设计视图、无需负样本、无需手动调参，在多种图数据上实现了更优性能和更强鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了基于分数阶神经扩散网络的自适应多视图图对比学习框架，通过可学习的分数阶参数α生成连续的多尺度视图，无需手动增强或负样本，在多种图数据上实现了最先进的性能并表现出更强的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph contrastive learning (GCL) learns node and graph representations bycontrasting multiple views of the same graph. Existing methods typically relyon fixed, handcrafted views-usually a local and a global perspective, whichlimits their ability to capture multi-scale structural patterns. We present anaugmentation-free, multi-view GCL framework grounded in fractional-ordercontinuous dynamics. By varying the fractional derivative order $\alpha \in(0,1]$, our encoders produce a continuous spectrum of views: small $\alpha$yields localized features, while large $\alpha$ induces broader, globalaggregation. We treat $\alpha$ as a learnable parameter so the model can adaptdiffusion scales to the data and automatically discover informative views. Thisprincipled approach generates diverse, complementary representations withoutmanual augmentations. Extensive experiments on standard benchmarks demonstratethat our method produces more robust and expressive embeddings and outperformsstate-of-the-art GCL baselines.</description>
      <author>example@mail.com (Yanan Zhao, Feng Ji, Jingyang Dai, Jiaze Ma, Keyue Jiang, Kai Zhao, Wee Peng Tay)</author>
      <guid isPermaLink="false">2511.06216v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>EMOD: A Unified EEG Emotion Representation Framework Leveraging V-A Guided Contrastive Learning</title>
      <link>http://arxiv.org/abs/2511.05863v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了EMOD框架，一种利用效价-唤醒(V-A)引导的对比学习的统一EEG情感表示框架，解决了现有深度学习方法在跨数据集泛化上的局限性，通过统一表示和对比学习提高了模型对不同EEG格式的适应能力。&lt;h4&gt;背景&lt;/h4&gt;情感识别在情感计算中至关重要，深度学习已被广泛应用于EEG信号的情感识别。现有深度学习方法在单一EEG情感数据集上表现良好，但在跨数据集泛化能力上有限，这种局限性源于标注方案和数据格式的异质性。现有模型通常需要针对特定数据集的架构，缺乏跨不同情感标签的语义对齐。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型在跨数据集泛化上的局限性，弥合语义和结构差距，创建一个统一的EEG情感表示框架。&lt;h4&gt;方法&lt;/h4&gt;提出了EMOD框架：利用效价-唤醒(V-A)引导的对比学习的统一EEG情感表示框架。通过将离散和连续情感标签投影到统一的V-A空间，学习可迁移和情感感知的表示。设计了软监督对比损失函数，鼓励情感相似的样本在潜在空间中聚类。采用灵活的主干网络，包括三域编码器和时空Transformer，以适应可变的EEG格式，能够稳健地提取和整合时间、频谱和空间特征。&lt;h4&gt;主要发现&lt;/h4&gt;在八个公共EEG数据集上预训练EMOD，在三个基准数据集上评估其性能。EMOD实现了最先进的性能，展示了在多样化EEG情感识别场景中的强适应性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;EMOD框架有效解决了EEG情感识别中的跨数据集泛化问题，通过统一表示和对比学习，提高了模型对不同EEG格式的适应能力。&lt;h4&gt;翻译&lt;/h4&gt;从脑电信号中进行情感识别对于情感计算至关重要，并已使用深度学习得到广泛探索。虽然最近的深度学习方法在单一脑电情感数据集上取得了强大性能，但由于标注方案和数据格式的异质性，它们在跨数据集泛化方面仍然有限。现有模型通常需要针对输入结构定制特定数据集的架构，并且缺乏跨多样化情感标签的语义对齐。为了解决这些挑战，我们提出了EMOD：一种利用效价-唤醒(V-A)引导的对比学习的统一脑电情感表示框架。EMOD通过弥合语义和结构差距，从异构数据集中学习可迁移和情感感知的表示。具体而言，我们将离散和连续情感标签投影到统一的V-A空间，并制定了一个软监督对比损失，鼓励情感相似的样本在潜在空间中聚类。为了适应可变的脑电格式，EMOD采用了一个灵活的主干网络，包括三域编码器和时空Transformer，能够稳健地提取和整合时间、频谱和空间特征。我们在八个公共脑电数据集上预训练EMOD，并在三个基准数据集上评估其性能。实验结果表明，EMOD实现了最先进的性能，展示了在多样化基于脑电的情感识别场景中的强适应性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Emotion recognition from EEG signals is essential for affective computing andhas been widely explored using deep learning. While recent deep learningapproaches have achieved strong performance on single EEG emotion datasets,their generalization across datasets remains limited due to the heterogeneityin annotation schemes and data formats. Existing models typically requiredataset-specific architectures tailored to input structure and lack semanticalignment across diverse emotion labels. To address these challenges, wepropose EMOD: A Unified EEG Emotion Representation Framework LeveragingValence-Arousal (V-A) Guided Contrastive Learning. EMOD learns transferable andemotion-aware representations from heterogeneous datasets by bridging bothsemantic and structural gaps. Specifically, we project discrete and continuousemotion labels into a unified V-A space and formulate a soft-weightedsupervised contrastive loss that encourages emotionally similar samples tocluster in the latent space. To accommodate variable EEG formats, EMOD employsa flexible backbone comprising a Triple-Domain Encoder followed by aSpatial-Temporal Transformer, enabling robust extraction and integration oftemporal, spectral, and spatial features. We pretrain EMOD on eight public EEGdatasets and evaluate its performance on three benchmark datasets. Experimentalresults show that EMOD achieves state-of-the-art performance, demonstratingstrong adaptability and generalization across diverse EEG-based emotionrecognition scenarios.</description>
      <author>example@mail.com (Yuning Chen, Sha Zhao, Shijian Li, Gang Pan)</author>
      <guid isPermaLink="false">2511.05863v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Cross-domain EEG-based Emotion Recognition with Contrastive Learning</title>
      <link>http://arxiv.org/abs/2511.05293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了EmotionCLIP模型，将EEG情感识别重新表述为CLIP框架内的EEG-文本匹配任务，使用SST-LegoViT骨干网络捕获多维度特征，实验结果显著优于现有模型。&lt;h4&gt;背景&lt;/h4&gt;基于脑电图(EEG)的情感识别对于情感计算至关重要，但面临特征利用和跨领域泛化的挑战。&lt;h4&gt;目的&lt;/h4&gt;引入EmotionCLIP模型，将识别任务重新表述为CLIP框架内的EEG-文本匹配任务，提高EEG情感识别的准确性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;设计了专门的骨干网络SST-LegoViT，使用多尺度卷积和Transformer模块捕获空间、频谱和时间特征，采用多模态对比学习方法进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;在SEED和SEED-IV数据集上，跨主体准确率分别达到88.69%和73.50%，跨时间准确率分别达到88.46%和77.54%，显著优于现有模型。&lt;h4&gt;结论&lt;/h4&gt;多模态对比学习对稳健的EEG情感识别有效，EmotionCLIP模型在跨主体和跨时间识别任务中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;基于脑电图(EEG)的情感识别对情感计算至关重要，但在特征利用和跨领域泛化方面面临挑战。本研究引入了EmotionCLIP，它将识别任务重新表述为CLIP框架内的EEG-文本匹配任务。一个专门的骨干网络SST-LegoViT使用多尺度卷积和Transformer模块捕获空间、频谱和时间特征。在SEED和SEED-IV数据集上的实验显示出了卓越的跨主体准确率88.69%和73.50%，以及跨时间准确率88.46%和77.54%，优于现有模型。结果表明多模态对比学习对稳健的EEG情感识别有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalogram (EEG)-based emotion recognition is vital for affectivecomputing but faces challenges in feature utilization and cross-domaingeneralization. This work introduces EmotionCLIP, which reformulatesrecognition as an EEG-text matching task within the CLIP framework. A tailoredbackbone, SST-LegoViT, captures spatial, spectral, and temporal features usingmulti-scale convolution and Transformer modules. Experiments on SEED andSEED-IV datasets show superior cross-subject accuracies of 88.69% and 73.50%,and cross-time accuracies of 88.46% and 77.54%, outperforming existing models.Results demonstrate the effectiveness of multimodal contrastive learning forrobust EEG emotion recognition.</description>
      <author>example@mail.com (Rui Yan, Yibo Li, Han Ding, Fei Wang)</author>
      <guid isPermaLink="false">2511.05293v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification</title>
      <link>http://arxiv.org/abs/2511.05092v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种双阶段提示驱动隐私保护范式(DPPP)，用于解决虚拟数据集在行人再识别模型训练中面临的构建复杂和域泛化能力差的问题。通过生成丰富提示和提示驱动解缠机制，构建了大规模虚拟数据集GenePerson，并实现了最先进的泛化性能。&lt;h4&gt;背景&lt;/h4&gt;随着对数据隐私的关注增加，研究人员开始使用虚拟数据替代敏感的真实世界图像来训练行人再识别(Re-ID)模型。然而，现有的游戏引擎生成的虚拟数据集面临构建复杂和域泛化能力差等挑战，难以在实际场景中应用。&lt;h4&gt;目的&lt;/h4&gt;解决现有虚拟数据集在行人再识别模型训练中面临的构建复杂和域泛化能力差的问题，提高模型在实际场景中的应用能力。&lt;h4&gt;方法&lt;/h4&gt;提出双阶段提示驱动隐私保护范式(DPPP)：第一阶段生成包含行人外观、光照和视点等多维属性的丰富提示，驱动扩散模型端到端合成多样化数据，构建GenePerson虚拟数据集；第二阶段提出提示驱动解缠机制(PDM)，通过对比学习和文本反转网络将图像映射为风格和内容的伪词，构建风格解缠的内容提示，引导模型学习域不变内容特征。&lt;h4&gt;主要发现&lt;/h4&gt;在GenePerson数据集上使用PDM训练的模型达到了最先进的泛化性能，超过了在流行真实和虚拟Re-ID数据集上的表现。&lt;h4&gt;结论&lt;/h4&gt;通过DPPP范式和PDM机制，有效解决了虚拟数据集在行人再识别模型训练中的域泛化问题，提高了模型在实际场景中的应用能力。&lt;h4&gt;翻译&lt;/h4&gt;随着对数据隐私问题的日益关注，研究人员开始使用虚拟数据作为替代敏感真实世界图像的方案，用于训练行人再识别(Re-ID)模型。然而，现有由游戏引擎生成的虚拟数据集仍面临构建复杂和域泛化能力差等挑战，难以在实际场景中应用。为解决这些挑战，我们提出了一种双阶段提示驱动隐私保护范式(DPPP)。在第一阶段，我们生成包含行人外观、光照和视点等多维属性的丰富提示，驱动扩散模型端到端合成多样化数据，构建了一个包含130,519张图像和6,641个身份的大规模虚拟数据集GenePerson。在第二阶段，我们提出了一种提示驱动解缠机制(PDM)，学习域不变泛化特征。借助对比学习，我们使用两个文本反转网络将图像分别映射为表示风格和内容的伪词，从而构建风格解缠的内容提示，引导模型在图像层面学习域不变内容特征。实验证明，在GenePerson上使用PDM训练的模型实现了最先进的泛化性能，超过了在流行真实和虚拟Re-ID数据集上的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With growing concerns over data privacy, researchers have started usingvirtual data as an alternative to sensitive real-world images for trainingperson re-identification (Re-ID) models. However, existing virtual datasetsproduced by game engines still face challenges such as complex construction andpoor domain generalization, making them difficult to apply in real scenarios.To address these challenges, we propose a Dual-stage Prompt-drivenPrivacy-preserving Paradigm (DPPP). In the first stage, we generate richprompts incorporating multi-dimensional attributes such as pedestrianappearance, illumination, and viewpoint that drive the diffusion model tosynthesize diverse data end-to-end, building a large-scale virtual datasetnamed GenePerson with 130,519 images of 6,641 identities. In the second stage,we propose a Prompt-driven Disentanglement Mechanism (PDM) to learndomain-invariant generalization features. With the aid of contrastive learning,we employ two textual inversion networks to map images into pseudo-wordsrepresenting style and content, respectively, thereby constructingstyle-disentangled content prompts to guide the model in learningdomain-invariant content features at the image level. Experiments demonstratethat models trained on GenePerson with PDM achieve state-of-the-artgeneralization performance, surpassing those on popular real and virtual Re-IDdatasets.</description>
      <author>example@mail.com (Ruolin Li, Min Liu, Yuan Bian, Zhaoyang Li, Yuzhen Li, Xueping Wang, Yaonan Wang)</author>
      <guid isPermaLink="false">2511.05092v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Medical Referring Image Segmentation via Next-Token Mask Prediction</title>
      <link>http://arxiv.org/abs/2511.05044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE Transactions on Medical  Imaging for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NTP-MRISeg的新型医学图像分割框架，将医学图像分割任务重新表述为自回归下一个令牌预测任务，并通过三种创新策略解决了该表述下的挑战，在多个数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;医学图像分割(MRIS)涉及基于自然语言描述分割医学图像中的目标区域。虽然现有方法取得了有前景的结果，但通常涉及复杂的多模态融合或多阶段解码器设计。&lt;h4&gt;目的&lt;/h4&gt;开发一种简化的医学图像分割框架，消除对模态特定融合和外部分割模型的需求，支持统一的端到端训练架构，并提高模型的泛化能力和适应性。&lt;h4&gt;方法&lt;/h4&gt;将MRIS重新表述为对标记化的图像、文本和掩码表示的统一多模态序列的自回归下一个令牌预测任务。提出三种创新策略：(1)Next-k Token Prediction (NkTP)方案减少累积预测误差；(2)Token-level Contrastive Learning (TCL)增强边界敏感性并缓解长尾分布效应；(3)基于内存的Hard Error Token (HET)优化策略在训练中强调困难令牌。&lt;h4&gt;主要发现&lt;/h4&gt;在QaTa-COV19和MosMedData+数据集上的大量实验表明，NTP-MRISeg实现了新的最先进性能，为传统MRIS流程提供了一种简化且有效的替代方案。&lt;h4&gt;结论&lt;/h4&gt;NTP-MRISeg框架通过统一的多模态序列自回归预测方法，简化了医学图像分割任务的设计，同时提高了性能，是一种有前景的替代传统MRIS流程的方法。&lt;h4&gt;翻译&lt;/h4&gt;医学图像分割涉及基于自然语言描述分割医学图像中的目标区域。虽然取得了有前景的结果，但最近的方法通常涉及复杂的多模态融合或多阶段解码器设计。在这项工作中，我们提出了NTP-MRISeg，一种新型框架，将MRIS重新表述为对标记化的图像、文本和掩码表示的统一多模态序列的自回归下一个令牌预测任务。这种表述通过消除对模态特定融合和外部分割模型的需求，简化了模型设计，支持统一的端到端训练架构。它还允许使用新兴的大规模多模态模型的预训练令牌化器，提高泛化能力和适应性。更重要的是，为了解决这种表述下的挑战，如曝光偏差、长尾令牌分布和细粒度病变边缘，我们提出了三种创新策略：(1)Next-k Token Prediction (NkTP)方案减少累积预测误差；(2)Token-level Contrastive Learning (TCL)增强边界敏感性并缓解长尾分布效应；(3)基于内存的Hard Error Token (HET)优化策略在训练中强调困难令牌。在QaTa-COV19和MosMedData+数据集上的大量实验表明，NTP-MRISeg实现了新的最先进性能，为传统MRIS流程提供了一种简化且有效的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical Referring Image Segmentation (MRIS) involves segmenting targetregions in medical images based on natural language descriptions. Whileachieving promising results, recent approaches usually involve complex designof multimodal fusion or multi-stage decoders. In this work, we proposeNTP-MRISeg, a novel framework that reformulates MRIS as an autoregressivenext-token prediction task over a unified multimodal sequence of tokenizedimage, text, and mask representations. This formulation streamlines modeldesign by eliminating the need for modality-specific fusion and externalsegmentation models, supports a unified architecture for end-to-end training.It also enables the use of pretrained tokenizers from emerging large-scalemultimodal models, enhancing generalization and adaptability. More importantly,to address challenges under this formulation-such as exposure bias, long-tailtoken distributions, and fine-grained lesion edges-we propose three novelstrategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulativeprediction errors, (2) Token-level Contrastive Learning (TCL) to enhanceboundary sensitivity and mitigate long-tail distribution effects, and (3) amemory-based Hard Error Token (HET) optimization strategy that emphasizesdifficult tokens during training. Extensive experiments on the QaTa-COV19 andMosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-artperformance, offering a streamlined and effective alternative to traditionalMRIS pipelines.</description>
      <author>example@mail.com (Xinyu Chen, Yiran Wang, Gaoyang Pang, Jiafu Hao, Chentao Yue, Luping Zhou, Yonghui Li)</author>
      <guid isPermaLink="false">2511.05044v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation</title>
      <link>http://arxiv.org/abs/2511.05034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8pages, 3figures, published to ACM Digital Library&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种动态残差编码与幻灯片级别对比学习(DRE-SLCL)方法，用于解决全幻灯片图像(WSI)端到端表示训练中的挑战，并在多种癌症相关任务上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;全幻灯片图像(WSI)表示对于癌症亚型分型、癌症识别和突变预测至关重要。然而，训练端到端的WSI表示模型存在重大挑战，因为一个标准的千兆像素幻灯片可包含数万个图像块，受当前GPU限制，难以在单个小批量中计算所有图像块的梯度。&lt;h4&gt;目的&lt;/h4&gt;提出一种动态残差编码与幻灯片级别对比学习(DRE-SLCL)方法，用于端到端的WSI表示，以解决GPU计算限制问题。&lt;h4&gt;方法&lt;/h4&gt;使用内存库存储所有WSI的图像块特征；训练时，对于小批量中的每个WSI，随机采样部分图像块并计算其特征，同时从内存库中检索同一WSI的额外图像块特征；采用残差编码技术结合这些特征生成个体WSI表示；最后基于WSI表示和组织病理学报告计算幻灯片级别的对比损失。&lt;h4&gt;主要发现&lt;/h4&gt;在癌症亚型分型、癌症识别和突变预测任务上进行的实验证明了所提出的DRE-SLCL方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;DRE-SLCL方法是一种有效的端到端WSI表示方法，能够克服GPU计算限制，在多种癌症相关任务上表现良好。&lt;h4&gt;翻译&lt;/h4&gt;全幻灯片图像(WSI)表示对于癌症亚型分型、癌症识别和突变预测至关重要。训练端到端的WSI表示模型存在重大挑战，因为一个标准的千兆像素幻灯片可以包含数万个图像块，受当前GPU限制，难以在单个小批量中计算所有图像块的梯度。为应对这一挑战，我们提出了一种动态残差编码与幻灯片级别对比学习(DRE-SLCL)方法用于端到端的WSI表示。我们的方法使用内存库存储数据集中所有WSI的图像块特征。训练时，一个小批量通常包含多个WSI。对于批量中的每个WSI，随机采样一个子集的图像块，并使用图像块编码器计算其特征。然后从内存库中选择来自同一WSI的额外图像块特征。使用残差编码技术生成每个个体WSI的表示，该技术结合了采样特征和从内存库中检索的特征。最后，基于小批量内WSI的表示和组织病理学报告计算幻灯片级别的对比损失。在癌症亚型分型、癌症识别和突变预测任务上进行的实验证明了所提出的DRE-SLCL方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3755469&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Whole Slide Image (WSI) representation is critical for cancer subtyping,cancer recognition and mutation prediction.Training an end-to-end WSIrepresentation model poses significant challenges, as a standard gigapixelslide can contain tens of thousands of image tiles, making it difficult tocompute gradients of all tiles in a single mini-batch due to current GPUlimitations. To address this challenge, we propose a method of dynamic residualencoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSIrepresentation. Our approach utilizes a memory bank to store the features oftiles across all WSIs in the dataset. During training, a mini-batch usuallycontains multiple WSIs. For each WSI in the batch, a subset of tiles israndomly sampled and their features are computed using a tile encoder. Then,additional tile features from the same WSI are selected from the memory bank.The representation of each individual WSI is generated using a residualencoding technique that incorporates both the sampled features and thoseretrieved from the memory bank. Finally, the slide-level contrastive loss iscomputed based on the representations and histopathology reports ofthe WSIswithin the mini-batch. Experiments conducted over cancer subtyping, cancerrecognition, and mutation prediction tasks proved the effectiveness of theproposed DRE-SLCL method.</description>
      <author>example@mail.com (Jing Jin, Xu Liu, Te Gao, Zhihong Shi, Yixiong Liang, Ruiqing Zheng, Hulin Kuang, Min Zeng, Shichao Kan)</author>
      <guid isPermaLink="false">2511.05034v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>RCMCL: A Unified Contrastive Learning Framework for Robust Multi-Modal (RGB-D, Skeleton, Point Cloud) Action Understanding</title>
      <link>http://arxiv.org/abs/2511.04351v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为鲁棒跨模态对比学习(RCMCL)的自监督框架，用于解决多模态人类动作识别在传感器故障或噪声情况下的性能下降问题。该框架通过联合优化跨模态对比目标、模态内自蒸馏目标和退化模拟目标，学习模态不变表示，并引入自适应模态门控网络实现鲁棒融合。&lt;h4&gt;背景&lt;/h4&gt;多模态输入（RGB-D、骨骼、点云）的人类动作识别虽然可以实现高精度，但通常依赖于大型标记数据集，并且在传感器故障或噪声情况下性能会急剧下降。&lt;h4&gt;目的&lt;/h4&gt;开发一个自监督框架，学习模态不变表示，并在模态丢失和损坏情况下保持可靠性，以解决多模态HAR在实际应用中的脆弱性问题。&lt;h4&gt;方法&lt;/h4&gt;RCMCL框架联合优化三个目标：(1)跨模态对比目标对齐异构流，(2)模态内自蒸馏目标提高视图不变性并减少冗余，(3)退化模拟目标训练模型从掩码或损坏输入中恢复。同时，引入自适应模态门控网络为每个模态分配数据驱动的可靠性权重。&lt;h4&gt;主要发现&lt;/h4&gt;在NTU RGB+D 120 (CS/CV)和UWA3D-II数据集上，RCMCL在标准设置中达到最先进准确率，并且在严重双模态丢失情况下仅显示11.5%的性能下降，显著优于监督融合基线。&lt;h4&gt;结论&lt;/h4&gt;自监督跨模态对齐，结合明确的退化建模和自适应融合，是开发可靠可部署的多模态HAR系统的关键。&lt;h4&gt;翻译&lt;/h4&gt;多模态输入（RGB-D、骨骼、点云）的人类动作识别(HAR)可以实现高精度，但通常依赖于大型标记数据集，并且在传感器故障或噪声情况下性能会急剧下降。我们提出了鲁棒跨模态对比学习(RCMCL)，这是一个自监督框架，学习模态不变表示，并在模态丢失和损坏情况下保持可靠性。RCMCL联合优化了(i)跨模态对比目标，对齐异构流；(ii)模态内自蒸馏目标，提高视图不变性并减少冗余；以及(iii)退化模拟目标，明确训练模型从掩码或损坏输入中恢复。在推理时，自适应模态门控(AMG)网络为每个模态分配数据驱动的可靠性权重以实现鲁棒融合。在NTU RGB+D 120 (CS/CV)和UWA3D-II上，RCMCL在标准设置中达到最先进准确率，并表现出明显更好的鲁棒性：在严重的双模态丢失情况下仅显示11.5%的性能下降，显著优于强大的监督融合基线。这些结果表明，自监督跨模态对齐，结合明确的退化建模和自适应融合，是可部署的多模态HAR的关键。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态人体动作识别系统的两个关键挑战：一是严重依赖大量标注数据，二是实际部署中因传感器故障或噪声导致的性能严重下降。这个问题在现实中非常重要，因为人体动作识别系统需要在各种条件下可靠工作，包括传感器可能失效或数据被噪声污染的情况；同时，数据标注成本高昂，且多模态系统在部分模态丢失时性能会显著下降，限制了其在实际应用中的部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到多模态动作识别的优势（互补信息）和挑战（异构数据融合、标注需求、鲁棒性问题）。然后借鉴了对比学习在自监督表征学习中的成功经验，特别是跨模态对比学习。作者设计了三个核心组件：跨模态一致性损失（LCM）用于特征对齐，模态内自蒸馏损失（LIM）提高内部表征质量，退化模拟损失（Ldeg）和自适应模态门控（AMG）提高鲁棒性。这些设计借鉴了现有工作中的对比学习、跨模态一致性学习、自蒸馏、退化模拟和自适应门控机制等思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习一个统一的、模态不变的特征空间，使表征对噪声和模态缺失具有内在鲁棒性；通过跨模态对比学习强制不同模态的特征对齐；显式训练模型以应对和补偿模态丢失和损坏；使用自适应门控机制动态调整不同模态的权重。整体实现流程分为四个阶段：1）模态特定特征编码（使用三种不同编码器处理RGB-D、骨骼和点云数据）；2）自监督跨模态预训练（包括LCM、LIM和Ldeg三种损失函数）；3）模态自适应鲁棒融合（通过AMG网络动态加权融合特征）；4）训练流程（预训练阶段优化所有组件，微调阶段训练分类器）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）统一的三模态自监督对比学习框架，同时处理RGB-D、骨骼和点云三种异构模态；2）创新的对比损失设计，结合跨模态一致性和模态内自蒸馏；3）退化模拟与自适应门控机制，处理模态丢失和噪声问题；4）联合优化策略，统一训练所有组件。相比之前工作，RCMCL首次统一处理三种不同的3D数据类型，显式解决实际部署中的模态丢失和噪声问题，结合多种自监督技术在一个框架中，并引入自适应门控机制根据输入质量动态调整模态权重。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RCMCL通过统一的自监督跨模态对比学习框架，结合创新的退化模拟和自适应门控机制，显著提高了多模态人体动作识别系统在数据标注有限和传感器故障条件下的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human action recognition (HAR) with multi-modal inputs (RGB-D, skeleton,point cloud) can achieve high accuracy but typically relies on large labeleddatasets and degrades sharply when sensors fail or are noisy. We present RobustCross-Modal Contrastive Learning (RCMCL), a self-supervised framework thatlearns modality-invariant representations and remains reliable under modalitydropout and corruption. RCMCL jointly optimizes (i) a cross-modal contrastiveobjective that aligns heterogeneous streams, (ii) an intra-modalself-distillation objective that improves view-invariance and reducesredundancy, and (iii) a degradation simulation objective that explicitly trainsmodels to recover from masked or corrupted inputs. At inference, an AdaptiveModality Gating (AMG) network assigns data-driven reliability weights to eachmodality for robust fusion. On NTU RGB+D 120 (CS/CV) and UWA3D-II, RCMCLattains state-of-the-art accuracy in standard settings and exhibits markedlybetter robustness: under severe dual-modality dropout it shows only an 11.5%degradation, significantly outperforming strong supervised fusion baselines.These results indicate that self-supervised cross-modal alignment, coupled withexplicit degradation modeling and adaptive fusion, is key to deployablemulti-modal HAR.</description>
      <author>example@mail.com (Hasan Akgul, Mari Eplik, Javier Rojas, Akira Yamamoto, Rajesh Kumar, Maya Singh)</author>
      <guid isPermaLink="false">2511.04351v2</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>C3-Diff: Super-resolving Spatial Transcriptomics via Cross-modal Cross-content Contrastive Diffusion Modelling</title>
      <link>http://arxiv.org/abs/2511.05571v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个名为C3-Diff的跨模态跨内容对比扩散框架，用于空间转录组学增强，通过整合组织学图像提高基因表达分辨率&lt;h4&gt;背景&lt;/h4&gt;空间转录组学的发展使在原始组织中测量基因表达成为可能，但当前ST平台常面临分辨率低的问题，限制了空间基因表达的深入理解&lt;h4&gt;目的&lt;/h4&gt;提高ST地图分辨率，通过建模组织学图像与基因表达之间的相互作用，实现有效的ST增强&lt;h4&gt;方法&lt;/h4&gt;提出C3-Diff框架，改进传统对比学习范式提取模态不变和内容不变特征，在特征单元超球面上进行噪声信息增强，并采用动态跨模态插值训练策略缓解数据稀缺问题&lt;h4&gt;主要发现&lt;/h4&gt;在四个公共数据集上测试，C3-Diff性能显著优于竞争方法，并在细胞类型定位、基因表达相关性和单细胞水平基因表达预测等下游任务上表现良好&lt;h4&gt;结论&lt;/h4&gt;C3-Diff促进了人工智能增强的生物技术在生物医学研究和临床应用中的应用&lt;h4&gt;翻译&lt;/h4&gt;空间转录组学（ST）的快速发展，即空间基因表达，使得在原始组织中测量基因表达成为可能，使我们能够发现分子机制。然而，当前的ST平台经常面临分辨率低的问题，限制了空间基因表达的深入理解。超分辨率方法通过整合组织学图像和已分析组织斑点的基因表达，有望增强ST地图。然而，建模组织学图像和基因表达之间的相互作用以实现有效的ST增强仍然是一个挑战。本研究提出了一个名为C3-Diff的跨模态跨内容对比扩散框架，用于以组织学图像为指导的ST增强。在C3-Diff中，我们首先分析了传统对比学习范式的不足，然后加以改进，以提取ST地图和组织学图像的模态不变和内容不变特征。此外，为了克服ST地图中测序灵敏度低的问题，我们在特征单元超球面上进行了基于噪声的信息增强。最后，我们提出了一种动态跨模态插值训练策略，以缓解ST数据稀缺问题。我们在四个公共数据集上测试了C3-Diff，其性能显著优于竞争方法。此外，我们在细胞类型定位、基因表达相关性和单细胞水平基因表达预测等下游任务上评估了C3-Diff，促进了人工智能增强的生物技术在生物医学研究和临床应用中的应用。代码可在https://github.com/XiaofeiWang2018/C3-Diff获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement of spatial transcriptomics (ST), i.e., spatial geneexpressions, has made it possible to measure gene expression within originaltissue, enabling us to discover molecular mechanisms. However, current STplatforms frequently suffer from low resolution, limiting the in-depthunderstanding of spatial gene expression. Super-resolution approaches promiseto enhance ST maps by integrating histology images with gene expressions ofprofiled tissue spots. However, it remains a challenge to model theinteractions between histology images and gene expressions for effective STenhancement. This study presents a cross-modal cross-content contrastivediffusion framework, called C3-Diff, for ST enhancement with histology imagesas guidance. In C3-Diff, we firstly analyze the deficiency of traditionalcontrastive learning paradigm, which is then refined to extract bothmodal-invariant and content-invariant features of ST maps and histology images.Further, to overcome the problem of low sequencing sensitivity in ST maps, weperform nosing-based information augmentation on the surface of feature unithypersphere. Finally, we propose a dynamic cross-modal imputation-basedtraining strategy to mitigate ST data scarcity. We tested C3-Diff bybenchmarking its performance on four public datasets, where it achievessignificant improvements over competing methods. Moreover, we evaluate C3-Diffon downstream tasks of cell type localization, gene expression correlation andsingle-cell-level gene expression prediction, promoting AI-enhancedbiotechnology for biomedical research and clinical applications. Codes areavailable at https://github.com/XiaofeiWang2018/C3-Diff.</description>
      <author>example@mail.com (Xiaofei Wang, Stephen Price, Chao Li)</author>
      <guid isPermaLink="false">2511.05571v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Point Cloud Segmentation of Integrated Circuits Package Substrates Surface Defects Using Causal Inference: Dataset Construction and Methodology</title>
      <link>http://arxiv.org/abs/2511.05853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究构建了用于陶瓷封装基板(CPS)表面缺陷3D分割的高质量点云数据集CPS3D-Seg，并提出了一种基于因果推断的新型3D分割方法CINet，实验表明该方法在性能上显著优于现有算法。&lt;h4&gt;背景&lt;/h4&gt;3D数据的有效分割对工业应用至关重要，特别是在集成电路(IC)领域检测微小缺陷。陶瓷封装基板(CPS)作为重要电子材料，因其优异的物理化学性质在IC封装中必不可少。然而，CPS的复杂结构和微小缺陷，加上缺乏公开可用的数据集，严重阻碍了CPS表面缺陷检测的发展。&lt;h4&gt;目的&lt;/h4&gt;构建一个高质量的CPS表面缺陷3D分割点云数据集，并开发一种有效的3D分割方法来提高检测性能。&lt;h4&gt;方法&lt;/h4&gt;1. 构建了名为CPS3D-Seg的高质量点云数据集，包含20个产品类别下的1300个点云样本，每个样本提供精确的点级标注；2. 基于最先进的点云分割算法进行了全面的基准测试；3. 提出了一种基于因果推断的新型3D分割方法CINet，通过结构化精炼(SR)和质量评估(QA)模块量化点云中的潜在混杂因素。&lt;h4&gt;主要发现&lt;/h4&gt;1. CPS3D-Seg数据集在点分辨率和精度上优于现有的3D工业数据集；2. CINet在mIoU和准确率方面显著优于现有算法。&lt;h4&gt;结论&lt;/h4&gt;通过构建高质量数据集和提出创新的分割方法，有效解决了CPS表面缺陷检测的挑战，为工业应用提供了可靠的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;3D数据的有效分割对广泛的工业应用至关重要，特别是在集成电路(IC)领域检测微小缺陷。陶瓷封装基板(CPS)作为一种重要的电子材料，由于其优异的物理和化学性质，在IC封装中必不可少。然而，CPS的复杂结构和微小缺陷，加上缺乏公开可用的数据集，严重阻碍了CPS表面缺陷检测的发展。在本研究中，我们构建了一个用于CPS表面缺陷3D分割的高质量点云数据集，即CPS3D-Seg，与现有的3D工业数据集相比，它具有最佳的点分辨率和精度。CPS3D-Seg包含20个产品类别下的1300个点云样本，每个样本都提供精确的点级标注。同时，我们基于最先进的点云分割算法进行了全面的基准测试，以验证CPS3D-Seg的有效性。此外，我们提出了一种基于因果推断的新型3D分割方法(CINet)，通过结构化精炼(SR)和质量评估(QA)模块量化点云中的潜在混杂因素。大量实验证明，CINet在mIoU和准确率方面显著优于现有算法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决集成电路封装基板表面缺陷的高精度3D点云分割问题。这个问题很重要，因为封装基板是电子设备的关键组成部分，其表面微小缺陷可能导致设备故障；同时，现有2D方法缺乏深度信息，无法准确检测深度方向的缺陷，而高精度3D数据集的缺乏也限制了相关算法的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有3D工业数据集与陶瓷封装基板之间存在显著差异，后者具有复杂的三维结构和表面电路。作者发现高密度点云在预处理过程中可能导致表示效果差异，这些差异可概括为点云的复杂性。受此启发，作者将因果推断引入点云分割。作者借鉴了现有的点云采集技术、数据集构建方法（如MVTec 3D-AD、Real3D-AD）、多种分割方法（CNN、图神经网络、Transformer、Mamba）以及计算机视觉中的因果推断应用，但将其专门应用于集成电路领域的高精度缺陷检测。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建高质量的3D点云数据集并基于因果推断进行分割。具体流程：1) 使用四个高精度线激光扫描仪构建数据采集平台，从实际生产线收集20种产品样本并进行点级标注；2) 提出CINet方法，包含三个模块：质量评估模块(QA)使用GMM捕获点云特征，结构精炼模块(SR)通过FPS和K-NN处理点云结构，映射注意力检测模块(MAD)整合特征；3) 构建结构因果模型(SCM)处理点云中的潜在混杂因素，通过后门调整方法控制混杂因素的影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 构建CPS3D-Seg数据集，包含1300个样本，点云分辨率0.0025毫米，精度0.0003毫米，比现有数据集高一个数量级；2) 提出基于因果推断的CINet分割方法，通过结构因果模型解决点云中的潜在混杂因素；3) 构建全面的基准测试，涵盖最新点云分割算法。相比之前工作，CPS3D-Seg全部来自实际生产线而非合成数据，具有最高分辨率和精度；CINet专门针对陶瓷封装基板复杂数据设计，引入因果推断而非仅关注统计相关性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文构建了目前最高精度的集成电路封装基板表面缺陷3D点云数据集，并提出了一种基于因果推断的创新分割方法，显著提高了微小缺陷检测的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The effective segmentation of 3D data is crucial for a wide range ofindustrial applications, especially for detecting subtle defects in the fieldof integrated circuits (IC). Ceramic package substrates (CPS), as an importantelectronic material, are essential in IC packaging owing to their superiorphysical and chemical properties. However, the complex structure and minordefects of CPS, along with the absence of a publically available dataset,significantly hinder the development of CPS surface defect detection. In thisstudy, we construct a high-quality point cloud dataset for 3D segmentation ofsurface defects in CPS, i.e., CPS3D-Seg, which has the best point resolutionand precision compared to existing 3D industrial datasets. CPS3D-Seg consistsof 1300 point cloud samples under 20 product categories, and each sampleprovides accurate point-level annotations. Meanwhile, we conduct acomprehensive benchmark based on SOTA point cloud segmentation algorithms tovalidate the effectiveness of CPS3D-Seg. Additionally, we propose a novel 3Dsegmentation method based on causal inference (CINet), which quantifiespotential confounders in point clouds through Structural Refine (SR) andQuality Assessment (QA) Modules. Extensive experiments demonstrate that CINetsignificantly outperforms existing algorithms in both mIoU and accuracy.</description>
      <author>example@mail.com (Bingyang Guo, Qiang Zuo, Ruiyun Yu)</author>
      <guid isPermaLink="false">2511.05853v1</guid>
      <pubDate>Tue, 11 Nov 2025 16:30:28 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Land Anywhere: Transferable Generative Models for Aircraft Trajectories</title>
      <link>http://arxiv.org/abs/2511.04155v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探索了使用迁移学习将数据丰富机场的生成模型适应到数据稀缺机场的方法，特别关注基于扩散和流匹配的架构。实验表明，扩散模型仅需少量目标机场数据(5-20%)就能达到与完整数据训练相当的性能，显著优于从头训练的模型，为解决航空交通管理中的数据稀缺问题提供了有效途径。&lt;h4&gt;背景&lt;/h4&gt;航空交通管理(ATM)解决方案的开发和验证需要获取轨迹数据，但许多次要和区域机场面临严重的数据稀缺问题。这限制了机器学习方法的应用以及进行大规模模拟或'假设分析'的能力。&lt;h4&gt;目的&lt;/h4&gt;研究是否可以使用迁移学习将在数据丰富的机场上训练的生成模型有效地适应数据稀缺的机场。&lt;h4&gt;方法&lt;/h4&gt;将最先进的基于扩散和流匹配的架构适应到航空领域，并评估它们在苏黎世(源)和都柏林(目标)着陆轨迹数据集之间的可迁移性。模型在苏黎世上进行预训练，然后在都柏林上进行微调，使用不同比例的本地数据，从0%到100%不等。&lt;h4&gt;主要发现&lt;/h4&gt;基于扩散的模型仅使用5%的都柏林数据就能获得有竞争力的性能，在20%左右达到基线水平性能；在各种指标和视觉检查中，这些模型始终优于从头开始训练的模型；潜在流匹配和潜在扩散模型也从预训练中受益，尽管收益更多变；流匹配模型显示出较弱的泛化能力；尽管在捕获罕见轨迹模式方面存在挑战。&lt;h4&gt;结论&lt;/h4&gt;这些发现证明了迁移学习在大幅减少ATM中轨迹生成的数据需求方面的潜力，使得即使在历史记录有限的环境中也能实现真实的合成数据生成。&lt;h4&gt;翻译&lt;/h4&gt;获取轨迹数据是开发和验证空中交通管理(ATM)解决方案的关键要求，然而许多次要和区域机场面临严重的数据稀缺问题。这限制了机器学习方法的应用能力以及执行大规模模拟或'假设分析'的能力。在本文中，我们研究了是否可以使用迁移学习将在数据丰富的机场上训练的生成模型有效地适应数据稀缺的机场。我们将最先进的基于扩散和流匹配的架构适应到航空领域，并评估它们在苏黎世(源)和都柏林(目标)着陆轨迹数据集之间的可迁移性。模型在苏黎世上预训练，然后在都柏林上微调，使用不同比例的本地数据，范围从0%到100%。结果表明，基于扩散的模型仅使用5%的都柏林数据就能获得有竞争力的性能，并在20%左右达到基线水平性能，在各种指标和视觉检查中始终优于从头开始训练的模型。潜在流匹配和潜在扩散模型也从预训练中受益，尽管收益更多变，而流匹配模型显示出较弱的泛化能力。尽管在捕获罕见轨迹模式方面存在挑战，但这些发现证明了迁移学习在大幅减少ATM中轨迹生成数据需求方面的潜力，使得即使在历史记录有限的环境中也能实现真实的合成数据生成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Access to trajectory data is a key requirement for developing and validatingAir Traffic Management (ATM) solutions, yet many secondary and regionalairports face severe data scarcity. This limits the applicability of machinelearning methods and the ability to perform large-scale simulations or"what-if" analyses. In this paper, we investigate whether generative modelstrained on data-rich airports can be efficiently adapted to data-scarceairports using transfer learning. We adapt state-of-the-art diffusion- andflow-matching-based architectures to the aviation domain and evaluate theirtransferability between Zurich (source) and Dublin (target) landing trajectorydatasets. Models are pretrained on Zurich and fine-tuned on Dublin with varyingamounts of local data, ranging from 0% to 100%. Results show thatdiffusion-based models achieve competitive performance with as little as 5% ofthe Dublin data and reach baseline-level performance around 20%, consistentlyoutperforming models trained from scratch across metrics and visualinspections. Latent flow matching and latent diffusion models also benefit frompretraining, though with more variable gains, while flow matching models showweaker generalization. Despite challenges in capturing rare trajectorypatterns, these findings demonstrate the potential of transfer learning tosubstantially reduce data requirements for trajectory generation in ATM,enabling realistic synthetic data generation even in environments with limitedhistorical records.</description>
      <author>example@mail.com (Olav Finne Praesteng Larsen, Massimiliano Ruocco, Michail Spitieris, Abdulmajid Murad, Martina Ragosta)</author>
      <guid isPermaLink="false">2511.04155v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
  <item>
      <title>Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation</title>
      <link>http://arxiv.org/abs/2511.05308v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted at International Conference on 3D Vision  (3DV) 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了3D点云生成模型的评估指标和生成方法，提出了改进的评估指标和新的生成模型架构，在ShapeNet数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;3D点云已成为现代技术的基石，对复杂生成模型和可靠评估指标的需求呈指数级增长。&lt;h4&gt;目的&lt;/h4&gt;暴露常用评估指标的局限性，提出更可靠的评估方法，并开发新的生成模型架构。&lt;h4&gt;方法&lt;/h4&gt;指出基于Chamfer Distance的指标缺乏鲁棒性；提出在对齐样本后再计算距离；引入Density-Aware Chamfer Distance；提出新的Surface Normal Concordance指标；开发Diffusion Point Transformer模型架构。&lt;h4&gt;主要发现&lt;/h4&gt;传统CD指标对缺陷缺乏鲁棒性；样本对齐和DCD能提高评估的一致性和鲁棒性；SNC指标通过比较点法线近似表面相似性；结合传统指标提供更全面的评估。&lt;h4&gt;结论&lt;/h4&gt;提出的模型在ShapeNet数据集上优于之前的解决方案；在生成点云质量方面取得了新的最先进水平；代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;随着3D点云成为现代技术的基石，对复杂生成模型和可靠评估指标的需求呈指数级增长。在这项工作中，我们首先指出一些常用的评估生成点云的指标，特别是基于Chamfer Distance的指标，对缺陷缺乏鲁棒性，并且作为质量指标时无法捕捉几何保真度和局部形状一致性。我们进一步表明，在距离计算前引入样本对齐先验，并用Density-Aware Chamfer Distance替代CD，是确保点云生成模型评估指标一致性和鲁棒性的简单而必要的步骤。虽然现有指标主要关注直接比较3D欧几里得坐标，但我们提出了一个名为Surface Normal Concordance的新指标，通过比较估计的点法线来近似表面相似性。这个新指标与传统指标结合，能对生成样本的质量提供更全面的评估。最后，利用基于Transformer的点云分析模型的最新进展，如序列化块注意力，我们提出了一种用于生成高保真3D结构的新架构——Diffusion Point Transformer。我们在ShapeNet数据集上进行了广泛的实验和比较，表明我们的模型优于之前的解决方案，特别是在生成点云质量方面，取得了新的最先进水平。代码可在https://github.com/matteo-bastico/DiffusionPointTransformer获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决两个问题：一是3D点云生成模型的评估指标缺乏鲁棒性问题，二是现有生成模型架构因依赖体素化或下采样而导致局部结构细节损失的问题。这些问题在现实中非常重要，因为3D点云是自动驾驶、机器人和医疗等现代技术的核心，可靠的评估方法和高质量的生成模型对于推动这些领域的技术进步至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有评估指标的局限性，如图1所示传统指标对噪声和移位的敏感性，然后借鉴了密度感知Chamfer Distance(DCD)概念并结合质心对齐来改进评估。对于生成模型，作者受到DiT-3D的扩散结构和Point Transformer的点云处理架构启发，采用空间填充曲线序列化点云，并引入序列化块注意力和增强条件位置编码等创新技术。论文确实大量借鉴了现有工作，包括扩散模型、Transformer架构和点云处理技术，但进行了创新性整合和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想包括：1)评估指标方面，通过质心对齐确保评估稳定性，使用密度感知Chamfer Distance考虑点密度，引入表面一致性指标(SNC)通过比较点法线评估表面质量；2)生成模型方面，直接在原始点云上进行点级扩散避免细节损失，使用Transformer架构捕捉点间关系。整体流程为：点云序列化→随机排序→嵌入编码→DiPT块处理(条件位置编码→序列化块注意力→线性变换→特征调制→缩放和残差连接)→重复N次→输出处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)评估指标：质心对齐方法、密度感知Chamfer Distance(DCD)、表面一致性指标(SNC)；2)生成模型：Diffusion Point Transformer(DiPT)直接处理原始点云、序列化块注意力机制、增强条件位置编码(xCPE)。相比之前工作的不同：传统评估指标对噪声敏感而改进后指标更鲁棒；SNC关注表面法线而非仅欧氏距离；DiPT直接处理原始点云而非依赖体素化或下采样，使用序列化块注意力提高效率，保留原始分辨率避免细节损失。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过改进3D点云生成模型的评估指标（质心对齐、密度感知Chamfer Distance和表面一致性指标）并提出一种直接在原始点云上进行点级扩散的Transformer架构（Diffusion Point Transformer），显著提高了生成点云的质量和评估的可靠性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As 3D point clouds become a cornerstone of modern technology, the need forsophisticated generative models and reliable evaluation metrics has grownexponentially. In this work, we first expose that some commonly used metricsfor evaluating generated point clouds, particularly those based on ChamferDistance (CD), lack robustness against defects and fail to capture geometricfidelity and local shape consistency when used as quality indicators. Wefurther show that introducing samples alignment prior to distance calculationand replacing CD with Density-Aware Chamfer Distance (DCD) are simple yetessential steps to ensure the consistency and robustness of point cloudgenerative model evaluation metrics. While existing metrics primarily focus ondirectly comparing 3D Euclidean coordinates, we present a novel metric, namedSurface Normal Concordance (SNC), which approximates surface similarity bycomparing estimated point normals. This new metric, when combined withtraditional ones, provides a more comprehensive evaluation of the quality ofgenerated samples. Finally, leveraging recent advancements in transformer-basedmodels for point cloud analysis, such as serialized patch attention , wepropose a new architecture for generating high-fidelity 3D structures, theDiffusion Point Transformer. We perform extensive experiments and comparisonson the ShapeNet dataset, showing that our model outperforms previous solutions,particularly in terms of quality of generated point clouds, achieving newstate-of-the-art. Code available athttps://github.com/matteo-bastico/DiffusionPointTransformer.</description>
      <author>example@mail.com (Matteo Bastico, David Ryckelynck, Laurent Corté, Yannick Tillier, Etienne Decencière)</author>
      <guid isPermaLink="false">2511.05308v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>Implicit reconstruction from point cloud: an adaptive level-set-based semi-Lagrangian method</title>
      <link>http://arxiv.org/abs/2511.05145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于水平集的半拉格朗日方法，用于从点云数据重建表面，使用分级自适应笛卡尔网格和曲率约束，生成高质量的隐式表面表示。&lt;h4&gt;背景&lt;/h4&gt;从点云数据重建表面是一个重要问题，需要获得能够作为偏微分方程模型计算域的高质量隐式表示。&lt;h4&gt;目的&lt;/h4&gt;获取对真实形状的隐式、高质量表示，作为偏微分方程模型的计算域。&lt;h4&gt;方法&lt;/h4&gt;使用变分数学公式结合曲率约束，将问题重新表述为平流-扩散方程，采用半拉格朗日方案和局部高阶插值器求解，并利用四叉树和八叉树数据结构在表面附近生成精细网格。&lt;h4&gt;主要发现&lt;/h4&gt;通过二维和三维的大量数值测试验证了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效地从点云数据重建高质量表面，适用于复杂和演化拓扑的情况。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于水平集的半拉格朗日方法，用于在分级自适应笛卡尔网格上解决从点云数据重建表面的问题。目标是获得对真实形状的隐式、高质量表示，随后可以作为偏微分方程模型的计算域。数学公式是变分的，结合了曲率约束，在最小化表面积的同时，考虑重建表面与输入点云的距离的权重。在水平集框架内，这个问题被重新表述为平流-扩散方程，我们使用半拉格朗日方案结合局部高阶插值器来求解。基于水平集和半拉格朗日方法的特性，我们使用四叉树和八叉树数据结构来表示网格，并在零水平集（即重建表面界面）附近生成具有最精细分辨率的网格。描述了完整的表面重建工作流程，包括定位和重新初始化技术，以及处理复杂和演化拓扑的策略。展示了二维和三维的大量数值测试，以评估该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a level-set-based semi-Lagrangian method on graded adaptiveCartesian grids to address the problem of surface reconstruction from pointclouds. The goal is to obtain an implicit, high-quality representation of realshapes that can subsequently serve as computational domain for partialdifferential equation models. The mathematical formulation is variational,incorporating a curvature constraint that minimizes the surface area whilebeing weighted by the distance of the reconstructed surface from the inputpoint cloud. Within the level set framework, this problem is reformulated as anadvection-diffusion equation, which we solve using a semi-Lagrangian schemecoupled with a local high-order interpolator. Building on the features of thelevel set and semi-Lagrangian method, we use quadtree and octree datastructures to represent the grid and generate a mesh with the finest resolutionnear the zero level set, i.e., the reconstructed surface interface. Thecomplete surface reconstruction workflow is described, including localizationand reinitialization techniques, as well as strategies to handle complex andevolving topologies. A broad set of numerical tests in two and three dimensionsis presented to assess the effectiveness of the method.</description>
      <author>example@mail.com (Silvia Preda, Matteo Semplice)</author>
      <guid isPermaLink="false">2511.05145v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>Efficient representation of 3D spatial data for defense-related applications</title>
      <link>http://arxiv.org/abs/2511.05109v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文比较了地理空间传感器数据的传统表示方法和现代神经表示技术，提出了一种混合系统架构，结合传统方法的几何准确性和现代方法的视觉保真度。&lt;h4&gt;背景&lt;/h4&gt;地理空间传感器数据对现代国防和安全至关重要，提供态势感知所需的3D信息，这些数据来自激光雷达传感器和光学相机等来源。&lt;h4&gt;目的&lt;/h4&gt;对传统表示方法（点云、体素网格、三角形网格）与现代神经和隐式技术（NeRFs、3DGS）进行比较分析，评估它们在地理空间数据处理中的优缺点。&lt;h4&gt;方法&lt;/h4&gt;通过比较传统表示方法和现代神经表示技术在几何准确性和视觉保真度方面的表现，评估它们在不同应用场景中的适用性。&lt;h4&gt;主要发现&lt;/h4&gt;传统模型提供强大的几何准确性，适合功能性任务如视线分析和物理模拟；现代方法在产生高保真、照片级真实感的视觉效果方面表现出色，但往往缺乏几何可靠性。&lt;h4&gt;结论&lt;/h4&gt;混合方法是最有前途的发展路径，结合传统网格支架以保证几何完整性，以及神经表示以提供视觉细节，在分层场景结构中进行管理以确保可扩展性和性能。&lt;h4&gt;翻译&lt;/h4&gt;地理空间传感器数据对现代国防和安全至关重要，为态势感知提供了不可或缺的3D信息。这些数据来自激光雷达传感器和光学相机等来源，能够为操作环境创建详细模型。在本文中，我们对传统表示方法（如点云、体素网格和三角形网格）与现代神经和隐式技术（如神经辐射场NeRFs和3D高斯溅射3DGS）进行了比较分析。我们的评估揭示了一个基本权衡：传统模型提供强大的几何准确性，适合视线分析和物理模拟等功能性任务，而现代方法在产生高保真、照片级真实的视觉效果方面表现出色，但往往缺乏几何可靠性。基于这些发现，我们得出结论，混合方法是最有前途的发展路径。我们提出了一种系统架构，结合传统网格支架以保证几何完整性，以及像3DGS这样的神经表示以提供视觉细节，在分层场景结构中进行管理，以确保可扩展性和性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何高效表示3D空间数据，特别是针对国防应用的问题。在现实中，这非常重要，因为现代国防需要快速准确地感知环境信息，3D表示比2D能提供更丰富的信息（如路线优化、视线评估），而实时整合处理来自不同传感器（激光雷达、相机等）的海量异构数据集是一个重大技术挑战，原始传感器数据通常过于庞大或无结构，无法直接用于战术决策。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析传统表示方法（点云、体素网格、网格）和现代神经表示方法（NeRF、3DGS）的优缺点，发现传统模型提供强大几何准确性但视觉保真度不足，而现代方法视觉效果好但几何可靠性差。基于这一权衡分析，作者设计了一个混合方法，借鉴了现有工作中的多种技术：神经辐射场(NeRF)、3D高斯溅射(3DGS)、可微分渲染技术（如软光栅化器）以及表面对齐的高斯溅射(SuGaR)等方法，将它们整合到一个统一的框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用混合方法结合传统3D表示的几何准确性和现代神经表示的视觉保真度。整体流程分为四个阶段：1)数据收集阶段，从各种传感器获取原始数据；2)数据融合阶段，将异构传感器数据统一为时空格式，进行对齐和后处理；3)聚合阶段，构建混合模型，使用实例分割创建初始实例，训练每个实例的几何网格支架和3DGS表示，通过组合损失函数同时优化几何准确性和视觉保真度；4)使用阶段，支持视线分析、可视化、路线规划等多种应用。整个系统使用层次加速结构(BVH)管理大型环境，确保可扩展性和性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)混合表示架构，结合传统网格支架和现代神经表示；2)双目标优化方法，同时优化几何准确性和视觉保真度；3)基于实例的处理流程，每个实例独立训练其表示；4)传感器数据融合策略，有效整合激光雷达和相机数据。相比之前的工作，这篇论文专注于国防应用领域，提供系统级解决方案而非单一算法，强调实用性和可扩展性，并提供了针对国防相关应用场景的全面评估框架。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的混合3D数据表示方法，通过结合传统网格支架的几何准确性和现代神经表示的视觉保真度，为国防应用提供了既可靠又高效的战场空间可视化解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1117/12.3069693&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geospatial sensor data is essential for modern defense and security, offeringindispensable 3D information for situational awareness. This data, gatheredfrom sources like lidar sensors and optical cameras, allows for the creation ofdetailed models of operational environments. In this paper, we provide acomparative analysis of traditional representation methods, such as pointclouds, voxel grids, and triangle meshes, alongside modern neural and implicittechniques like Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting(3DGS). Our evaluation reveals a fundamental trade-off: traditional modelsoffer robust geometric accuracy ideal for functional tasks like line-of-sightanalysis and physics simulations, while modern methods excel at producinghigh-fidelity, photorealistic visuals but often lack geometric reliability.Based on these findings, we conclude that a hybrid approach is the mostpromising path forward. We propose a system architecture that combines atraditional mesh scaffold for geometric integrity with a neural representationlike 3DGS for visual detail, managed within a hierarchical scene structure toensure scalability and performance.</description>
      <author>example@mail.com (Benjamin Kahl, Marcus Hebel, Michael Arens)</author>
      <guid isPermaLink="false">2511.05109v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>J-SGFT: Joint Spatial and Graph Fourier Domain Learning for Point Cloud Attribute Deblocking</title>
      <link>http://arxiv.org/abs/2511.05047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICIP 2025 Workshop on Generative AI for World Simulations  and Communications &amp; Celebrating 40 Years of Excellence in Education:  Honoring Professor Aggelos Katsaggelos, Sept. 2025, Alaska&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种多尺度后处理框架，有效去除重建点云中的块状伪影，显著提高视觉质量且开销最小&lt;h4&gt;背景&lt;/h4&gt;点云对于AR/VR和自动驾驶至关重要，但其体积大、不规则采样和稀疏性给压缩方案带来挑战。MPEG的GPCC方法虽成功降低比特率，但会在重建点云中引入明显的块状伪影&lt;h4&gt;目的&lt;/h4&gt;开发一种新的多尺度后处理框架，有效去除重建点云中的块状伪影&lt;h4&gt;方法&lt;/h4&gt;融合图傅里叶潜在属性表示与稀疏卷积和通道注意力的多尺度后处理框架，用于高效去除重建点云中的块状伪影&lt;h4&gt;主要发现&lt;/h4&gt;与GPCC TMC13v14基准相比，在8iVFBv2数据集上实现Y通道18.81%和联合YUV 18.14%的BD-rate降低，显著提高视觉保真度&lt;h4&gt;结论&lt;/h4&gt;所提出方法能有效去除重建点云中的块状伪影，在保持最小开销的同时显著提高视觉质量&lt;h4&gt;翻译&lt;/h4&gt;点云对于AR/VR和自动驾驶至关重要，但其体积大、不规则采样和稀疏性给压缩方案带来挑战。MPEG的基于几何的点云压缩方法成功降低了比特率；然而，它们在重建的点云中引入了明显的块状伪影。我们引入了一种新颖的多尺度后处理框架，该框架融合了图傅里叶潜在属性表示与稀疏卷积和通道注意力，以高效去除重建点云中的块状伪影。与GPCC TMC13v14基准相比，我们的方法在8iVFBv2数据集上实现了Y通道18.81%的BD-rate降低和联合YUV上18.14%的BD-rate降低，以最小的开销提供了显著改进的视觉保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds (PC) are essential for AR/VR and autonomous driving butchallenge compression schemes with their size, irregular sampling, andsparsity. MPEG's Geometry-based Point Cloud Compression (GPCC) methodssuccessfully reduce bitrate; however, they introduce significant blockyartifacts in the reconstructed point cloud. We introduce a novel multi-scalepostprocessing framework that fuses graph-Fourier latent attributerepresentations with sparse convolutions and channel-wise attention toefficiently deblock reconstructed point clouds. Against the GPCC TMC13v14baseline, our approach achieves BD-rate reduction of 18.81\% in the Y channeland 18.14\% in the joint YUV on the 8iVFBv2 dataset, delivering markedlyimproved visual fidelity with minimal overhead.</description>
      <author>example@mail.com (Muhammad Talha, Qi Yang, Zhu Li, Anique Akhtar, Geert Van Der Auwera)</author>
      <guid isPermaLink="false">2511.05047v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>4D Imaging in ISAC Systems: A Framework Based on 5G NR Downlink Signals</title>
      <link>http://arxiv.org/abs/2511.04913v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  TVT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种完全符合5G新空口(NR)协议的4D成像框架，通过端到端处理链路和Zoom-OMP算法实现高精度环境重建，为6G网络中的实际ISAC应用奠定基础。&lt;h4&gt;背景&lt;/h4&gt;集成感知与通信(ISAC)已成为第六代(6G)无线网络的关键使能技术，支持频谱共享和硬件集成。除了增强通信外，ISAC还能实现高精度的环境重建和成像，这对自动驾驶和数字孪生等应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种与蜂窝系统兼容的4D成像框架，实现高精度的环境重建和成像，为6G网络中的实际ISAC应用奠定基础。&lt;h4&gt;方法&lt;/h4&gt;提出完全符合5G新空口(NR)协议的4D成像框架；开发覆盖波形生成、回波处理和多基站点云融合的端到端处理链路；引入Zoom-OMP算法，一种用于高分辨率角度估计的从粗到细的稀疏恢复算法，以在降低计算成本的同时实现高精度。&lt;h4&gt;主要发现&lt;/h4&gt;提出的框架相比传统基准方法具有更优的空间精度和重建质量，实现了稳健的4D成像性能。&lt;h4&gt;结论&lt;/h4&gt;该研究为6G网络中的实际ISAC环境重建铺平了道路，展示了ISAC技术在6G网络中的实际应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;集成感知与通信(ISAC)已成为第六代(6G)无线网络的关键使能技术，支持频谱共享和硬件集成。除了增强通信外，ISAC还能实现高精度的环境重建和成像，这对自动驾驶和数字孪生等应用至关重要。本文提出了一种完全符合5G新空口(NR)协议的4D成像框架，确保与蜂窝系统的兼容性。具体而言，我们开发了一个覆盖波形生成、回波处理和多基站点云融合的端到端处理链路。此外，我们引入了Zoom-OMP，一种用于高分辨率角度估计的从粗到细的稀疏恢复算法，以在降低计算成本的同时实现高精度。仿真结果表明，与传统的基准方法相比，提出的框架具有更优的空间精度和重建质量，实现了稳健的4D成像性能，为6G网络中的实际ISAC环境重建铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何利用5G NR下行信号实现高精度4D成像的问题。现有研究大多依赖理想化的OFDM信号模型，忽略了5G NR标准的特定物理层结构，阻碍了实际部署；同时，成像分辨率与计算复杂度之间的权衡是实现实时应用的主要瓶颈。这个问题很重要，因为ISAC被认为是6G网络的关键技术，能将通信网络转变为无处不在的感知平台，实现高精度的环境重建，对自动驾驶、数字孪生等应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有研究的局限性，包括对理想化OFDM信号模型的依赖和成像分辨率与计算复杂度之间的权衡问题。作者借鉴了早期工作通过分析反射毫米波信号中子载波相位偏移估计ToF的方法，以及后续研究结合传统2D-FFT和超分辨率算法的混合框架。在此基础上，作者创新性地设计了一个完全符合5G NR标准的4D成像框架，开发了端到端处理链，并提出了名为Zoom-OMP的粗到细稀疏恢复算法，在减少计算成本的同时实现高精度角度估计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用标准5G NR下行信号作为感知源，通过多基站协作感知架构利用空间多样性增强检测覆盖范围，并采用粗到细的稀疏恢复策略实现高精度角度估计。整体实现流程包括：1) 联合距离-速度估计：从回波信号提取散射体参数，通过时频域转换获得距离-多普勒图，检测峰值并计算距离和速度；2) 高分辨率角度估计：使用Zoom-OMP算法，先进行粗搜索再进行精细搜索，精确估计方位角和仰角；3) 多基站结果融合：定义全局坐标系，通过坐标变换将所有局部点云注册到全局坐标系中，聚合获得最终全球点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 完全符合5G NR标准的4D成像框架，确保与蜂窝系统兼容；2) 提出Zoom-OMP算法，采用粗到细稀疏恢复策略，显著降低计算复杂度；3) 多基站协作感知架构，解决单视点感知的遮挡问题。相比之前工作，本文框架更注重实际应用，完全符合5G NR标准而非理想化模型；Zoom-OMP算法在保持高成像质量的同时，计算复杂度远低于传统OMP和2D-MUSIC算法；系统级设计不仅关注算法，还包括完整的端到端处理链和多基站协作机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于5G NR下行信号的完全兼容的ISAC 4D成像框架，通过创新的Zoom-OMP算法和多基站协作感知，实现了高精度、高效率的环境重建，为6G网络中的实际应用铺平了道路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrated sensing and communication (ISAC) has emerged as a key enabler forsixth-generation (6G) wireless networks, supporting spectrum sharing andhardware integration. Beyond communication enhancement, ISAC also enableshigh-accuracy environment reconstruction and imaging, which are crucial forapplications such as autonomous driving and digital twins. This paper proposesa 4D imaging framework fully compliant with the 5G New Radio (NR) protocol,ensuring compatibility with cellular systems. Specifically, we develop anend-to-end processing chain that covers waveform generation, echo processing,and multi-BS point cloud fusion. Furthermore, we introduce Zoom-OMP, acoarse-to-fine sparse recovery algorithm for high-resolution angle estimationthat achieves high accuracy with reduced computational cost. The simulationresults demonstrate that the proposed framework achieves robust 4D imagingperformance with superior spatial accuracy and reconstruction quality comparedto conventional benchmarks, paving the way for practical ISAC-enabledenvironment reconstruction in 6G networks.</description>
      <author>example@mail.com (Haoyang Weng, Haisu Wu, Hong Ren, Cunhua Pan, Jiangzhou Wang)</author>
      <guid isPermaLink="false">2511.04913v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction</title>
      <link>http://arxiv.org/abs/2511.04864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at 3DV 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种从不规则点云恢复高质量表面的方法，通过隐式自先验技术直接从输入数据中学习形状特定的先验，并将其与隐式神经表示结合。该方法仅使用自监督损失进行训练，不需要外部数据，并通过隐式移动最小二乘法整合学习到的先验，能够在保留精细几何细节的同时正则化稀疏区域。&lt;h4&gt;背景&lt;/h4&gt;从不规则点云恢复高质量表面是一个不适定问题，除非有强大的几何先验可用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从点云中恢复高质量表面的方法，无需依赖外部训练数据或强几何先验，同时保留精细几何细节并对数据退化具有鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;引入隐式自先验方法，联合训练可学习嵌入字典和隐式距离场，通过交叉注意力机制捕获形状的重复结构和长程相关性；使用自监督点云重建损失进行训练；通过自动微分采样提取密集点和法线；最后整合到稳健的隐式移动最小二乘公式中。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够有效捕获和重用形状中的重复结构和长程相关性；仅使用自监督损失即可训练出高质量模型；能够保留输入数据中的精细几何细节，同时利用学习到的先验正则化稀疏区域；对常见数据退化具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在生成高保真表面方面优于经典方法和基于学习的方法，具有卓越的细节保持能力和对数据退化的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;从不规则点云恢复高质量表面是一个不适定问题，除非有强大的几何先验可用。我们引入了一种隐式自先验方法，直接从输入点云中提炼形状特定的先验，并将其嵌入到隐式神经表示中。这是通过联合训练一个小的可学习嵌入字典和一个隐式距离场来实现的；在每一个查询位置，该字段通过交叉注意力机制关注字典，使网络能够捕获和重用形状中固有的重复结构和长程相关性。仅使用自监督的点云重建损失进行优化，我们的方法不需要外部训练数据。为了在保持输入保真度的同时有效整合这种学习到的先验，通过自动微分对训练好的场进行采样，以提取密集分布的点和分析法线。我们将得到的密集点云和相应的法线集成到稳健的隐式移动最小二乘公式中。我们表明这种混合策略保留了输入数据中的精细几何细节，同时利用学习到的先验来正则化稀疏区域。实验表明，我们的方法在生成具有卓越细节保持能力和对常见数据退化具有鲁棒性的高保真表面方面，优于经典方法和基于学习的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从离散、不规则的点云中恢复高质量表面的问题。这个问题在现实中非常重要，因为3D扫描设备经常产生不完整、有噪声或稀疏的点云数据，而高质量表面重建对于文化遗产数字化、工业检测、自动驾驶、医学成像和增强现实等应用至关重要。由于点云数据的不完整性，这个问题在数学上是不适定的，需要引入几何先验知识来约束重建过程。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统基于全局平滑先验的方法难以保留尖锐特征，而基于变形的方法如Point2Mesh虽能学习自相似性但受限于固定拓扑结构。作者从信号处理中获取灵感，将复杂信号表示为共享原子的稀疏组合，并结合隐式神经表示和注意力机制，设计出通过交叉注意力机制利用可学习字典捕获点云中自相似模式的方法。这种方法借鉴了Point2Mesh的自先验概念、神经隐式表示的灵活性以及注意力机制的非局部建模能力，但创新地将它们结合成一种新的自监督方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过交叉注意力机制利用可学习字典捕获点云中的自相似模式和重复结构，从而学习特定形状的几何先验，无需外部训练数据。整体流程分为两个阶段：第一阶段，训练一个MLP近似神经SDF场，使用可学习字典编码自先验，通过交叉注意力机制为每个查询点生成特征表示并预测SDF值；第二阶段，离散化学习到的几何场，使用鲁棒隐式移动最小二乘法(RIMLS)定义最终形状，结合MLP的表达能力和RIMLS的特征保持属性，产生全局一致且细节丰富的重建。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)自监督隐式框架，通过交叉注意力机制从输入点云学习特定形状的几何先验；2)将学习到的场与鲁棒隐式MLS相结合，产生准确、灵活的表面重建；3)在自相似形状上实现了最先进的性能。相比之前的工作，这篇论文不依赖外部训练数据，完全自监督；结合了隐式神经表示的灵活性和注意力机制的非局部建模能力；能捕获和重用重复结构和长程相关性；通过两阶段流程结合了学习先验和传统重建方法的优势；相比Point2Mesh等方法，不依赖固定拓扑结构，能处理任意形状。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种自监督的隐式注意力先验方法，通过可学习字典和交叉注意力机制从点云本身学习特定形状的几何模式，实现了高质量、细节丰富的表面重建，无需外部训练数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recovering high-quality surfaces from irregular point cloud is ill-posedunless strong geometric priors are available. We introduce an implicitself-prior approach that distills a shape-specific prior directly from theinput point cloud itself and embeds it within an implicit neuralrepresentation. This is achieved by jointly training a small dictionary oflearnable embeddings with an implicit distance field; at every query location,the field attends to the dictionary via cross-attention, enabling the networkto capture and reuse repeating structures and long-range correlations inherentto the shape. Optimized solely with self-supervised point cloud reconstructionlosses, our approach requires no external training data. To effectivelyintegrate this learned prior while preserving input fidelity, the trained fieldis then sampled to extract densely distributed points and analytic normals viaautomatic differentiation. We integrate the resulting dense point cloud andcorresponding normals into a robust implicit moving least squares (RIMLS)formulation. We show this hybrid strategy preserves fine geometric details inthe input data, while leveraging the learned prior to regularize sparseregions. Experiments show that our method outperforms both classical andlearning-based approaches in generating high-fidelity surfaces with superiordetail preservation and robustness to common data degradations.</description>
      <author>example@mail.com (Kyle Fogarty, Chenyue Cai, Jing Yang, Zhilin Guo, Cengiz Öztireli)</author>
      <guid isPermaLink="false">2511.04864v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>iFlyBot-VLM Technical Report</title>
      <link>http://arxiv.org/abs/2511.04976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;iFlyBot-VLM是一个通用的视觉语言模型，用于提升具身智能领域。它旨在弥合高维环境感知与低级机器人运动控制之间的跨模态语义鸿沟。&lt;h4&gt;背景&lt;/h4&gt;具身智能领域需要处理复杂的视觉和空间信息，并将其转化为机器人可理解的语言。&lt;h4&gt;目的&lt;/h4&gt;创建一个可扩展和可推广的基础模型，用于具身AI，促进从专门的面向任务的系统向通用的、具有认知能力的智能体发展。&lt;h4&gt;方法&lt;/h4&gt;该模型通过将复杂的视觉和空间信息抽象为与身体无关且可转移的操作语言，实现跨不同机器人平台的感知-动作闭环协调。其架构被系统设计以实现四个关键功能能力：空间理解和度量推理；交互式目标定位；动作抽象和控制参数生成；任务规划和技能排序。&lt;h4&gt;主要发现&lt;/h4&gt;在10个当前主流的具身智能相关VLM基准数据集（如Blink和Where2Place）上进行了评估，取得了最佳性能，同时保留了模型的通用能力。&lt;h4&gt;结论&lt;/h4&gt;iFlyBot-VLM代表了具身智能领域的重要进展，研究团队将公开发布训练数据和模型权重，以促进该领域的进一步研究和开发。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了iFlyBot-VLM，这是一个通用的视觉语言模型，用于提升具身智能领域。iFlyBot-VLM的核心目标是弥合高维环境感知与低级机器人运动控制之间的跨模态语义鸿沟。为此，该模型将复杂的视觉和空间信息抽象为与身体无关且可转移的操作语言，从而能够在多样化的机器人平台上实现无缝的感知-动作闭环协调。iFlyBot-VLM的架构经过系统设计，以实现具身智能所需的四个关键功能能力：1）空间理解和度量推理；2）交互式目标定位；3）动作抽象和控制参数生成；4）任务规划和技能排序。我们将iFlyBot-VLM视为具身AI的一个可扩展和可推广的基础模型，促进从专门的面向任务的系统向通用的、具有认知能力的智能体发展。我们在10个当前主流的具身智能相关VLM基准数据集（如Blink和Where2Place）上进行了评估，在保持模型通用能力的同时取得了最佳性能。我们将公开发布训练数据和模型权重，以促进具身智能领域的进一步研究和开发。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决高维环境感知与低级机器人运动控制之间的跨模态语义鸿沟问题，以及现有视觉语言模型在具身智能领域面临的'动作记忆'现象和泛化能力弱的问题。这个问题很重要，因为它使机器人能够真正理解物理世界并执行复杂任务，而不是仅限于数字世界中的感知任务，是实现通用机器人的关键挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有VLM在具身智能应用的局限性，认识到需要抽象环境信息为机器人可理解的'操作语言'。模型架构借鉴了主流的'ViT-Projector-LLM'三阶段范式，特别是InternVL3的设计，同时在位置编码层进行了创新。数据构建整合了多个现有数据集，如ScanNet、AgiBotWorld等，并进行了针对性的处理和扩充。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将复杂视觉和空间信息抽象为独立于机器人本体且可转移的'操作语言'，实现感知-动作闭环协调。整体流程包括：采用三阶段'ViT-Projector-LLM'架构；创新性地使用维度扩展位置嵌入(DEPE)增强空间感知；构建约380万样本的多源数据集；通过监督微调和链式思维引导进行训练；在多个基准数据集上进行评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出'操作语言'概念；创新维度扩展位置嵌入(DEPE)方法；构建全面的多源数据集；设计四个核心能力模块；引入链式思维引导推理。相比之前工作，iFlyBot-VLM更专注于具身智能领域，解决了跨模态语义鸿沟，在多个基准测试中表现更优，如Where2Place上得分70.23，Refspatial-bench上得分51.5，显著优于现有模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; iFlyBot-VLM通过创新的'操作语言'概念和全面的数据集构建，有效解决了高维环境感知与低级机器人运动控制之间的跨模态语义鸿沟，显著提升了机器人在具身智能任务中的空间理解、目标定位、动作抽象和任务规划能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) usedto improve the domain of Embodied Intelligence. The central objective ofiFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensionalenvironmental perception and low-level robotic motion control. To this end, themodel abstracts complex visual and spatial information into a body-agnostic andtransferable Operational Language, thereby enabling seamless perception-actionclosed-loop coordination across diverse robotic platforms. The architecture ofiFlyBot-VLM is systematically designed to realize four key functionalcapabilities essential for embodied intelligence: 1) Spatial Understanding andMetric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction andControl Parameter Generation; 4) Task Planning and Skill Sequencing. Weenvision iFlyBot-VLM as a scalable and generalizable foundation model forembodied AI, facilitating the progression from specialized task-orientedsystems toward generalist, cognitively capable agents. We conducted evaluationson 10 current mainstream embodied intelligence-related VLM benchmark datasets,such as Blink and Where2Place, and achieved optimal performance whilepreserving the model's general capabilities. We will publicly release both thetraining data and model weights to foster further research and development inthe field of Embodied Intelligence.</description>
      <author>example@mail.com (Xin Nie, Zhiyuan Cheng, Yuan Zhang, Chao Ji, Jiajia Wu, Yuhan Zhang, Jia Pan)</author>
      <guid isPermaLink="false">2511.04976v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2511.05462v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究建立了无监督聚类方法与统计学混合模型之间的联系，开发了名为SiamMM的新模型，该方法在自监督学习基准测试中表现优异，学习到的聚类与真实标签高度相似，同时揭示了数据集中可能存在的错误标记问题。&lt;h4&gt;背景&lt;/h4&gt;近期研究表明基于聚类的方法在自监督和无监督学习中是有效的，然而聚类的应用通常是启发式的，最佳方法尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;建立无监督聚类方法与统计学中的经典混合模型之间的联系，改进现有聚类方法。&lt;h4&gt;方法&lt;/h4&gt;通过建立聚类方法与统计学混合模型的框架，显著改进现有聚类方法，开发名为SiamMM的新模型。&lt;h4&gt;主要发现&lt;/h4&gt;SiamMM方法在各种自监督学习基准测试中达到了最先进的性能，学习到的聚类与未见过的真实标签高度相似。&lt;h4&gt;结论&lt;/h4&gt;对学习到的聚类进行检查发现它们与真实标签高度相似，同时揭示了数据集中可能存在的错误标记实例。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究已经证明了基于聚类的方法在自监督和无监督学习中的有效性。然而，聚类的应用通常是启发式的，最佳方法尚不清楚。在这项工作中，我们建立了这些无监督聚类方法与统计学中的经典混合模型之间的联系。通过这个框架，我们展示了这些聚类方法的显著改进，开发了一个名为SiamMM的新模型。我们的方法在各种自监督学习基准测试中取得了最先进的性能。对学习到的聚类进行检查发现，它们与未见过的真实标签高度相似，揭示了可能存在的错误标记实例。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies have demonstrated the effectiveness of clustering-basedapproaches for self-supervised and unsupervised learning. However, theapplication of clustering is often heuristic, and the optimal methodologyremains unclear. In this work, we establish connections between theseunsupervised clustering methods and classical mixture models from statistics.Through this framework, we demonstrate significant enhancements to theseclustering methods, leading to the development of a novel model named SiamMM.Our method attains state-of-the-art performance across various self-supervisedlearning benchmarks. Inspection of the learned clusters reveals a strongresemblance to unseen ground truth labels, uncovering potential instances ofmislabeling.</description>
      <author>example@mail.com (Xiaodong Wang, Jing Huang, Kevin J Liang)</author>
      <guid isPermaLink="false">2511.05462v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>Deep Progressive Training: scaling up depth capacity of zero/one-layer models</title>
      <link>http://arxiv.org/abs/2511.04981v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了深度学习中模型深度的扩展问题，提出了一种零/一层渐进训练方法，以在计算效率和模型性能之间实现最佳权衡。&lt;h4&gt;背景&lt;/h4&gt;深度学习中模型深度是一把双刃剑：更深层的模型能获得更高的准确性，但需要更高的计算成本。&lt;h4&gt;目的&lt;/h4&gt;为了有效地大规模训练模型，研究通过渐进式训练来扩展大模型的深度，探索优化理论和特征学习视角下的模型扩展。&lt;h4&gt;方法&lt;/h4&gt;提出了一种零/一层渐进训练方法，用于在计算和损失之间实现最佳权衡。研究内容包括新层的初始化、超参数传递、学习率调度和模型扩展时机的选择。&lt;h4&gt;主要发现&lt;/h4&gt;在GPT2上应用零/一层渐进训练，与完全训练的60层、70亿参数模型相比，可以节省约80%的计算量，或等效地加速约5倍，同时实现几乎相同的损失。&lt;h4&gt;结论&lt;/h4&gt;渐进式训练是一种有效的方法，可以在训练过程中扩展模型容量，显著减少计算量，同时几乎没有性能下降。&lt;h4&gt;翻译&lt;/h4&gt;在深度学习中，模型深度是一把双刃剑：更深层的模型能获得更高的准确性，但需要更高的计算成本。为了有效地大规模训练模型，渐进式训练是一种有效的策略，它在训练过程中扩展模型容量，从而显著减少计算量，同时几乎没有性能下降。在本工作中，我们从优化理论和特征学习的角度研究了大型模型的深度扩展，提供了关于新层初始化、超参数传递、学习率调度和模型扩展时机的见解。具体而言，我们提出了零/一层渐进训练，以在计算和损失之间实现最佳权衡。例如，与完全训练的60层、70亿参数模型相比，在GPT2上应用零/一层渐进训练可以节省约80%的计算量，或等效地加速约5倍，同时实现几乎相同的损失。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model depth is a double-edged sword in deep learning: deeper models achievehigher accuracy but require higher computational cost. To efficiently trainmodels at scale, an effective strategy is the progressive training, whichscales up model capacity during training, hence significantly reducingcomputation with little to none performance degradation. In this work, we studythe depth expansion of large models through the lens of optimization theory andfeature learning, offering insights on the initialization of new layers,hyperparameter transfer, learning rate schedule, and timing of model expansion.Specifically, we propose zero/one-layer progressive training for the optimaltradeoff between computation and loss. For example, zero/one-layer progressivetraining on GPT2 can save $\approx 80\%$ compute, or equivalently accelerate$\approx 5\times$ while achieving almost the same loss, compared to to a fullytrained 60-layer model with 7B parameters.</description>
      <author>example@mail.com (Zhiqi Bu)</author>
      <guid isPermaLink="false">2511.04981v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining</title>
      <link>http://arxiv.org/abs/2511.05245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种专门针对异常检测任务的预训练表征学习框架，解决了当前基于ImageNet预训练的特征网络在异常检测任务中的局限性。&lt;h4&gt;背景&lt;/h4&gt;当前主流和最先进的异常检测方法主要依赖于ImageNet预训练产生的特征网络，但ImageNet预训练过程与异常检测目标不匹配，且自然图像与工业图像数据存在分布偏移，导致ImageNet预训练特征对AD任务次优。&lt;h4&gt;目的&lt;/h4&gt;为了进一步促进AD领域发展，提出专门针对AD任务的预训练表征学习框架，学习鲁棒性和判别性更强的预训练表征用于工业异常检测。&lt;h4&gt;方法&lt;/h4&gt;提出角度和方向导向的对比损失，同时最大化正常和异常特征之间的角度大小和范数差异；在大型AD数据集RealIAD上进行预训练以避免分布偏移；基于类可泛化的残差特征学习预训练AD表征以缓解预训练数据和下游AD数据集之间的潜在偏移。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基于嵌入的AD方法和五个AD数据集、五个骨干网络上的广泛实验一致表明，作者提出的预训练特征具有优越性。&lt;h4&gt;结论&lt;/h4&gt;专门为异常检测任务设计的预训练表征能有效提升异常检测性能，代码已公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;当前主流和最先进的异常检测(AD)方法主要建立在ImageNet预训练产生的预训练特征网络基础上。然而，无论监督还是自监督预训练，ImageNet上的预训练过程都与异常检测的目标不匹配（即自然图像预训练不旨在区分正常和异常）。此外，自然图像和AD场景中的工业图像数据通常存在分布偏移。这两个问题可能导致ImageNet预训练特征对AD任务次优。为了进一步促进AD领域的发展，专门针对AD任务的预训练表征是迫切且有价值的。为此，我们提出了一种新颖的AD表征学习框架，专门设计用于学习工业异常检测的鲁棒性和判别性预训练表征。具体来说，紧密围绕异常检测的目标（即关注正常和异常之间的差异），我们提出了角度和方向导向的对比损失，同时最大化正常和异常特征之间的角度大小和范数差异。为了避免从自然图像到AD图像的分布偏移，我们的预训练在大型AD数据集RealIAD上进行。为了进一步缓解预训练数据和下游AD数据集之间的潜在偏移，我们基于类可泛化的表征、残差特征来学习预训练的AD表征。对于评估，基于五个基于嵌入的AD方法，我们简单地将它们的原始特征替换为我们的预训练表征。在五个AD数据集和五个骨干网络上的广泛实验一致表明我们预训练特征的优越性。代码可在https://github.com/xcyao00/ADPretrain获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The current mainstream and state-of-the-art anomaly detection (AD) methodsare substantially established on pretrained feature networks yielded byImageNet pretraining. However, regardless of supervised or self-supervisedpretraining, the pretraining process on ImageNet does not match the goal ofanomaly detection (i.e., pretraining in natural images doesn't aim todistinguish between normal and abnormal). Moreover, natural images andindustrial image data in AD scenarios typically have the distribution shift.The two issues can cause ImageNet-pretrained features to be suboptimal for ADtasks. To further promote the development of the AD field, pretrainedrepresentations specially for AD tasks are eager and very valuable. To thisend, we propose a novel AD representation learning framework specially designedfor learning robust and discriminative pretrained representations forindustrial anomaly detection. Specifically, closely surrounding the goal ofanomaly detection (i.e., focus on discrepancies between normals and anomalies),we propose angle- and norm-oriented contrastive losses to maximize the anglesize and norm difference between normal and abnormal features simultaneously.To avoid the distribution shift from natural images to AD images, ourpretraining is performed on a large-scale AD dataset, RealIAD. To furtheralleviate the potential shift between pretraining data and downstream ADdatasets, we learn the pretrained AD representations based on theclass-generalizable representation, residual features. For evaluation, based onfive embedding-based AD methods, we simply replace their original features withour pretrained representations. Extensive experiments on five AD datasets andfive backbones consistently show the superiority of our pretrained features.The code is available at https://github.com/xcyao00/ADPretrain.</description>
      <author>example@mail.com (Xincheng Yao, Yan Luo, Zefeng Qian, Chongyang Zhang)</author>
      <guid isPermaLink="false">2511.05245v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>Causal Structure and Representation Learning with Biomedical Applications</title>
      <link>http://arxiv.org/abs/2511.04790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This article has successfully completed peer review and will appear  in the Proceedings of the International Congress of Mathematicians 2026. Both  authors contributed equally to this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了表示学习与因果推理的结合，特别是在多模态数据背景下，提出了一种统计和计算框架来解决因果结构和表示学习问题。&lt;h4&gt;背景&lt;/h4&gt;大规模数据收集有望更好地理解复杂现象并做出更好的决策。表示学习已成为深度学习应用的关键驱动力，因为它允许学习捕获数据重要属性的潜在空间，而无需监督注释。&lt;h4&gt;目的&lt;/h4&gt;开发一种统计和计算框架，用于因果结构和表示学习，解决基本的生物医学问题，包括如何有效利用观察性和扰动性数据进行因果发现、如何利用多模态视图学习因果变量，以及如何设计最优扰动。&lt;h4&gt;方法&lt;/h4&gt;提出了一种结合表示学习和因果推理的统计和计算框架，利用日益增长的多模态数据（观察性和扰动性、基于成像和基于测序的、在不同生物水平上的数据）。&lt;h4&gt;主要发现&lt;/h4&gt;表示学习在预测任务中非常成功，但在因果任务中可能失败，这表明需要将表示学习与因果推理相结合。多模态数据的可用性为这种结合提供了机会。&lt;h4&gt;结论&lt;/h4&gt;表示学习与因果推理的结合，特别是在多模态数据背景下，为理解和预测复杂现象提供了新的机会，特别是在生物医学领域。&lt;h4&gt;翻译&lt;/h4&gt;大规模数据收集有望更好地理解复杂现象，并最终做出更好的决策。表示学习已成为深度学习应用的关键驱动力，因为它允许学习捕获数据重要属性的潜在空间，而无需任何监督注释。尽管表示学习在预测任务中取得了巨大成功，但在因果任务（包括预测扰动/干预的效果）中可能会彻底失败。这需要将表示学习与因果推理相结合。在这方面，一个令人兴奋的机会来自于多模态数据（观察性和扰动性、基于成像和基于测序的、在单细胞水平、组织水平和生物体水平）的日益增长的可获得性。我们概述了一个统计和计算框架，用于因果结构和表示学习，其动机是基本的生物医学问题：如何有效地利用观察性和扰动性数据对观察到的因果变量进行因果发现；如何利用系统的多模态视图学习因果变量；以及如何设计最优扰动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Massive data collection holds the promise of a better understanding ofcomplex phenomena and, ultimately, better decisions. Representation learninghas become a key driver of deep learning applications, as it allows learninglatent spaces that capture important properties of the data without requiringany supervised annotations. Although representation learning has been hugelysuccessful in predictive tasks, it can fail miserably in causal tasks includingpredicting the effect of a perturbation/intervention. This calls for a marriagebetween representation learning and causal inference. An exciting opportunityin this regard stems from the growing availability of multi-modal data(observational and perturbational, imaging-based and sequencing-based, at thesingle-cell level, tissue-level, and organism-level). We outline a statisticaland computational framework for causal structure and representation learningmotivated by fundamental biomedical questions: how to effectively useobservational and perturbational data to perform causal discovery on observedcausal variables; how to use multi-modal views of the system to learn causalvariables; and how to design optimal perturbations.</description>
      <author>example@mail.com (Caroline Uhler, Jiaqi Zhang)</author>
      <guid isPermaLink="false">2511.04790v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction</title>
      <link>http://arxiv.org/abs/2511.05483v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了DGTN(Diffused Graph-Transformer Network)架构，通过扩散机制共同学习图神经网络的结构先验和transformer注意力，有效捕捉了蛋白质局部结构几何与全局序列模式之间的复杂耦合关系，在酶热力学稳定性预测任务上取得了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;预测氨基酸突变对酶热力学稳定性(DDG)是蛋白质工程和药物设计的基础。虽然最近的深度学习方法显示出潜力，但它们通常独立处理序列和结构信息，无法捕捉局部结构几何与全局序列模式之间的复杂耦合关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效整合蛋白质序列和结构信息的方法，以更准确地预测氨基酸突变对酶热力学稳定性的影响。&lt;h4&gt;方法&lt;/h4&gt;提出DGTN架构，通过扩散机制共同学习图神经网络(GNN)的结构先验和transformer注意力权重。关键创新是双向扩散过程：(1)GNN衍生的结构嵌入通过可学习的扩散核引导transformer注意力；(2)transformer表示通过注意力调节的图更新来改进GNN消息传递。提供了严格的数学分析证明该共同学习方案的优越性。&lt;h4&gt;主要发现&lt;/h4&gt;在ProTherm和SKEMPI基准测试上，DGTN取得了最先进性能(Pearson Rho = 0.87, RMSE = 1.21 kcal/mol)，比最佳基线提高了6.2%。消融研究表明扩散机制对相关性的贡献为4.8个百分点。理论分析证明扩散的注意力收敛到最优结构-序列耦合，收敛速率为O(1/sqrt(T))，其中T是扩散步数。&lt;h4&gt;结论&lt;/h4&gt;该工作建立了一个通过可学习扩散整合异构蛋白质表示的原则性框架，为蛋白质工程和药物设计提供了新的工具。&lt;h4&gt;翻译&lt;/h4&gt;预测氨基酸突变对酶热力学稳定性(DDG)的影响是蛋白质工程和药物设计的基础。虽然最近的深度学习方法显示出前景，但它们通常独立处理序列和结构信息，无法捕捉局部结构几何与全局序列模式之间的复杂耦合关系。我们提出了DGTN(Diffused Graph-Transformer Network)，一种新颖的架构，通过扩散机制共同学习图神经网络(GNN)的结构先验和transformer注意力权重。我们的关键创新是双向扩散过程：(1)GNN衍生的结构嵌入通过可学习的扩散核引导transformer注意力；(2)transformer表示通过注意力调节的图更新来改进GNN消息传递。我们提供了严格的数学分析，证明这种共同学习方案比独立处理具有更好的近似界限。在ProTherm和SKEMPI基准测试上，DGTN取得了最先进性能(Pearson Rho = 0.87, RMSE = 1.21 kcal/mol)，比最佳基线提高了6.2%。消融研究表明扩散机制对相关性的贡献为4.8个百分点。我们的理论分析证明扩散的注意力收敛到最优结构-序列耦合，收敛速率为O(1/sqrt(T))，其中T是扩散步数。这项工作建立了一个通过可学习扩散整合异构蛋白质表示的原则性框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting the effect of amino acid mutations on enzyme thermodynamicstability (DDG) is fundamental to protein engineering and drug design. Whilerecent deep learning approaches have shown promise, they often process sequenceand structure information independently, failing to capture the intricatecoupling between local structural geometry and global sequential patterns. Wepresent DGTN (Diffused Graph-Transformer Network), a novel architecture thatco-learns graph neural network (GNN) weights for structural priors andtransformer attention through a diffusion mechanism. Our key innovation is abidirectional diffusion process where: (1) GNN-derived structural embeddingsguide transformer attention via learnable diffusion kernels, and (2)transformer representations refine GNN message passing throughattention-modulated graph updates. We provide rigorous mathematical analysisshowing this co-learning scheme achieves provably better approximation boundsthan independent processing. On ProTherm and SKEMPI benchmarks, DGTN achievesstate-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with6.2% improvement over best baselines. Ablation studies confirm the diffusionmechanism contributes 4.8 points to correlation. Our theoretical analysisproves the diffused attention converges to optimal structure-sequence coupling,with convergence rate O(1/sqrt(T) ) where T is diffusion steps. This workestablishes a principled framework for integrating heterogeneous proteinrepresentations through learnable diffusion.</description>
      <author>example@mail.com (Abigail Lin)</author>
      <guid isPermaLink="false">2511.05483v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>Linking Warm Dark Matter to Merger Tree Histories via Deep Learning Networks</title>
      <link>http://arxiv.org/abs/2511.05367v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 9 figures, submitted to ApJ&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用深度学习方法从暗物质晕的合并树结构中推断温暗物质粒子质量和反馈参数，证明合并树本身包含宇宙学参数信息。&lt;h4&gt;背景&lt;/h4&gt;暗物质晕在宇宙中通过一系列合并事件形成层次结构，宇宙模拟可将这些合并表示为图状'树'结构。已知这些合并树对宇宙学模拟参数敏感，但作为暗物质结构，它们对暗物质模型的敏感性仍不清楚。&lt;h4&gt;目的&lt;/h4&gt;研究使用深度学习方法训练合并树以推断温暗物质(WDM)粒子质量的可行性，并探索将相同方法应用于超新星和活动星系核反馈参数的推断。&lt;h4&gt;方法&lt;/h4&gt;将1,024个放大模拟中的合并树组织成图结构，节点表示星系合并历史，边表示遗传链接。变化节点特征复杂性，训练图神经网络(GNN)使用合并树图表示作为输入来预测WDM质量和反馈参数。&lt;h4&gt;主要发现&lt;/h4&gt;GNN能成功预测WDM粒子质量(R²从0.07到0.95)，预测效果取决于图的复杂性和节点特征。同样方法成功应用于推断超新星和活动星系核反馈参数。即使没有任何节点特征，GNN也能从合并树结构推断WDM质量，表明合并树结构本身继承了形成它们的模拟的宇宙学参数信息。&lt;h4&gt;结论&lt;/h4&gt;合并树的结构包含关于宇宙学参数的信息，即使不包含节点特征。图神经网络可以有效地从合并树中推断暗物质模型参数和反馈参数。&lt;h4&gt;翻译&lt;/h4&gt;暗物质晕在宇宙中通过一系列合并事件形成层次结构。宇宙模拟可以将这一系列合并表示为类似图的'树'结构。先前的工作表明这些合并树对宇宙学模拟参数敏感，但作为暗物质结构，它们对暗物质模型的敏感性仍然未知。在这项工作中，我们研究了使用深度学习方法训练合并树以从DREAMS模拟套件中推断温暗物质(WDM)粒子质量的可行性。我们将1,024个放大模拟中的合并树组织成图，节点代表星系的合并历史，边表示遗传链接。我们变化图中包含的节点特征的复杂性，从单个节点特征到多个星系属性(如晕质量、恒星形成率等)。我们训练图神经网络(GNN)使用合并树的图表示作为输入来预测WDM质量。我们发现GNN可以预测WDM粒子的质量(R²从0.07到0.95)，成功与否取决于图的复杂性和节点特征。我们将相同的方法扩展到超新星和活动星系核反馈参数A_SN1、A_SN2和A_AGN，成功推断出超新星参数。GNN甚至可以在没有任何节点特征的情况下从合并树历史推断WDM质量，表明合并树的结构本身就继承了它们形成的模拟的宇宙学参数信息。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dark matter (DM) halos form hierarchically in the Universe through a seriesof merger events. Cosmological simulations can represent this series of mergersas a graph-like ``tree'' structure. Previous work has shown these merger treesare sensitive to cosmology simulation parameters, but as DM structures, theoutstanding question of their sensitivity to DM models remains unanswered. Inthis work, we investigate the feasibility of deep learning methods trained onmerger trees to infer Warm Dark Matter (WDM) particles masses from the DREAMSsimulation suite. We organize the merger trees from 1,024 zoom-in simulationsinto graphs with nodes representing the merger history of galaxies and edgesdenoting hereditary links. We vary the complexity of the node features includedin the graphs ranging from a single node feature up through an array of severalgalactic properties (e.g., halo mass, star formation rate, etc.). We train aGraph Neural Network (GNN) to predict the WDM mass using the graphrepresentation of the merger tree as input. We find that the GNN can predictthe mass of the WDM particle ($R^2$ from 0.07 to 0.95), with success dependingon the graph complexity and node features. We extend the same methods tosupernovae and active galactic nuclei feedback parameters $A_\text{SN1}$,$A_\text{SN2}$, and $A_\text{AGN}$, successfully inferring the supernovaeparameters. The GNN can even infer the WDM mass from merger tree historieswithout any node features, indicating that the structure of merger trees aloneinherits information about the cosmological parameters of the simulations fromwhich they form.</description>
      <author>example@mail.com (Ilem Leisher, Paul Torrey, Alex M. Garcia, Jonah C. Rose, Francisco Villaescusa-Navarro, Zachary Lubberts, Arya Farahi, Stephanie O'Neil, Xuejian Shen, Olivia Mostow, Nitya Kallivayalil, Dhruv Zimmerman, Desika Narayanan, Mark Vogelsberger)</author>
      <guid isPermaLink="false">2511.05367v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs with Graph Neural Networks and Foundation Models</title>
      <link>http://arxiv.org/abs/2511.05179v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统分析了不同传感器部署密度和采样间隔条件下各类预测模型的性能表现，发现时空图神经网络(STGNNs)在稀疏部署和中等采样率下表现优异，而时序基础模型(TSFMs)在高频采样下更具竞争力，多变量TSF模型Moirai通过学习跨传感器依赖关系整体性能最佳。&lt;h4&gt;背景&lt;/h4&gt;现代物联网环境感知系统产生大量时空数据用于预测任务，现有边缘数据优化技术忽视了采样频率和空间覆盖变化对模型性能的影响。采样频率、空间覆盖与预测模型架构间的相互作用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究不同传感器节点密度和采样间隔条件下各类预测模型的性能表现，为构建时空系统中的高效预测流水线提供实用见解。&lt;h4&gt;方法&lt;/h4&gt;使用无线传感器网络中的真实温度数据，测试多种预测模型包括经典模型(VAR)、神经网络(GRU, Transformer)、时空图神经网络(STGNNs)和时序基础模型(TSFMs: Chronos, Moirai, TimesFM)，通过变化传感器节点密度和采样间隔评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;STGNNs在传感器部署稀疏且采样率适中时有效，利用空间相关性补偿有限覆盖；TSFMs在高频采样下表现好但空间覆盖减少时性能下降；多变量TSF模型Moirai通过原生学习跨传感器依赖关系，性能超越所有其他模型。&lt;h4&gt;结论&lt;/h4&gt;不同预测模型在不同传感器部署和采样条件下各有优势，合理选择模型和采样策略可构建高效的时空预测系统。所有代码已开源以确保研究结果可复现。&lt;h4&gt;翻译&lt;/h4&gt;现代物联网环境感知部署产生大量时空数据，以支持预测等下游任务，通常由机器学习模型驱动。虽然现有的过滤和战略部署技术优化了边缘收集的数据量，但它们忽略了采样频率和空间覆盖的变化如何影响下游模型性能。在许多预测模型中，通过提供更广泛的空间上下文，来自额外传感器的数据可以通过去噪预测。这种采样频率、空间覆盖与不同预测模型架构之间的相互作用仍未得到充分探索。这项工作提出了一项对预测模型的系统研究 - 经典模型(VAR)、神经网络(GRU, Transformer)、时空图神经网络(STGNNs)以及在不同空间传感器节点密度和采样间隔下的时序基础模型(TSFMs: Chronos, Moirai, TimesFM)，使用无线传感器网络中的真实温度数据。我们的结果表明，当传感器部署稀疏且采样率适中时，STGNNs有效，它们通过编码的图结构利用空间相关性来补偿有限的覆盖范围。相比之下，TSFMs在高频率下表现具有竞争力，但当来自相邻传感器的空间覆盖减少时性能下降。关键的是，多变量TSF模型Moirai通过原生学习跨传感器依赖关系，性能优于所有模型。这些发现为构建时空系统中的高效预测流水线提供了实用见解。所有模型配置、训练、数据集和日志代码均已开源以确保可复现性：https://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern IoT deployments for environmental sensing produce high volumespatiotemporal data to support downstream tasks such as forecasting, typicallypowered by machine learning models. While existing filtering and strategicdeployment techniques optimize collected data volume at the edge, they overlookhow variations in sampling frequencies and spatial coverage affect downstreammodel performance. In many forecasting models, incorporating data fromadditional sensors denoise predictions by providing broader spatial contexts.This interplay between sampling frequency, spatial coverage and differentforecasting model architectures remain underexplored. This work presents asystematic study of forecasting models - classical models (VAR), neuralnetworks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs),and time series foundation models (TSFMs: Chronos Moirai, TimesFM) undervarying spatial sensor nodes density and sampling intervals using real-worldtemperature data in a wireless sensor network. Our results show that STGNNs areeffective when sensor deployments are sparse and sampling rate is moderate,leveraging spatial correlations via encoded graph structure to compensate forlimited coverage. In contrast, TSFMs perform competitively at high frequenciesbut degrade when spatial coverage from neighboring sensors is reduced.Crucially, the multivariate TSFM Moirai outperforms all models by nativelylearning cross-sensor dependencies. These findings offer actionable insightsfor building efficient forecasting pipelines in spatio-temporal systems. Allcode for model configurations, training, dataset, and logs are open-sourced forreproducibility:https://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models</description>
      <author>example@mail.com (Ragini Gupta, Naman Raina, Bo Chen, Li Chen, Claudiu Danilov, Josh Eckhardt, Keyshla Bernard, Klara Nahrstedt)</author>
      <guid isPermaLink="false">2511.05179v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding</title>
      <link>http://arxiv.org/abs/2511.04984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Abstract 1 page, main text 9 pages, references 2 pages, 4 figures.  Submitted to RECOMB 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Peptide2Mol，一种E(3)-等变图神经网络扩散模型，通过参考原始肽配体及其周围蛋白质口袋环境来生成小分子，解决了传统AI方法忽略内源性蛋白与肽相互作用的问题。&lt;h4&gt;背景&lt;/h4&gt;基于结构的药物设计在整合人工智能方面取得了显著进展，特别是在生成命中化合物和先导化合物方面。然而，大多数AI方法忽略了内源性蛋白与肽相互作用的重要性，可能导致次优的分子设计。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够考虑原始肽配体及其周围蛋白质口袋环境的模型，用于生成和优化生物活性小分子。&lt;h4&gt;方法&lt;/h4&gt;Peptide2Mol是一种E(3)-等变图神经网络扩散模型，在大数据集上训练，并利用复杂的建模技术。&lt;h4&gt;主要发现&lt;/h4&gt;1. Peptide2Mol在非自回归生成任务中取得了最先进的性能；2. 产生的分子与原始肽配体具有相似性；3. 该模型允许通过部分扩散过程进行分子优化和肽模拟物设计。&lt;h4&gt;结论&lt;/h4&gt;Peptide2Mol是一种有效的深度生成模型，可用于从蛋白质结合口袋生成和优化生物活性小分子。&lt;h4&gt;翻译&lt;/h4&gt;基于结构的药物设计随着人工智能的整合取得了显著进展，特别是在生成命中化合物和先导化合物方面。然而，大多数AI驱动的方法忽略了内源性蛋白与肽相互作用的重要性，这可能导致次优的分子设计。在这项工作中，我们提出了Peptide2Mol，一种E(3)-等变图神经网络扩散模型，它通过参考原始肽配体及其周围的蛋白质口袋环境来生成小分子。在大数据集上训练并利用复杂的建模技术，Peptide2Mol不仅在非自回归生成任务中取得了最先进的性能，还产生了与原始肽配体相似的分子。此外，该模型允许通过部分扩散过程进行分子优化和肽模拟物设计。我们的研究结果表明，Peptide2Mol是一种有效的深度生成模型，可用于从蛋白质结合口袋生成和优化生物活性小分子。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Structure-based drug design has seen significant advancements with theintegration of artificial intelligence (AI), particularly in the generation ofhit and lead compounds. However, most AI-driven approaches neglect theimportance of endogenous protein interactions with peptides, which may resultin suboptimal molecule designs. In this work, we present Peptide2Mol, anE(3)-equivariant graph neural network diffusion model that generates smallmolecules by referencing both the original peptide binders and theirsurrounding protein pocket environments. Trained on large datasets andleveraging sophisticated modeling techniques, Peptide2Mol not only achievesstate-of-the-art performance in non-autoregressive generative tasks, but alsoproduces molecules with similarity to the original peptide binder.Additionally, the model allows for molecule optimization and peptidomimeticdesign through a partial diffusion process. Our results highlight Peptide2Molas an effective deep generative model for generating and optimizing bioactivesmall molecules from protein binding pockets.</description>
      <author>example@mail.com (Xinheng He, Yijia Zhang, Haowei Lin, Xingang Peng, Xiangzhe Kong, Mingyu Li, Jianzhu Ma)</author>
      <guid isPermaLink="false">2511.04984v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression</title>
      <link>http://arxiv.org/abs/2511.04838v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SPECTRA是一种光谱目标感知的图增强框架，用于在分子属性预测中生成真实的分子图，解决了标准GNN在处理稀有但重要分子时的不足，避免了现有过采样方法对分子拓扑的扭曲。&lt;h4&gt;背景&lt;/h4&gt;在分子属性预测中，有价值的化合物（如高活性）通常占据目标空间的稀疏区域。标准图神经网络（GNNs）通常优化平均误差，在这些罕见但关键的情况下表现不佳，而现有的过采样方法通常会扭曲分子拓扑结构。&lt;h4&gt;目的&lt;/h4&gt;引入SPECTRA框架，在光谱域生成真实的分子图，改善标准GNN在处理稀有分子时的性能，同时避免对分子拓扑结构的扭曲。&lt;h4&gt;方法&lt;/h4&gt;SPECTRA框架包括：从SMILES重建多属性分子图；通过(Fused)Gromov-Wasserstein耦合对齐分子对；在稳定共享基中插值拉普拉斯特征值、特征向量和节点特征；重建边以合成物理合理的中间体。结合基于标签核密度估计的稀有感知预算方案和边缘感知切比雪夫卷积的光谱GNN。&lt;h4&gt;主要发现&lt;/h4&gt;SPECTRA能够在保持全球平均绝对误差竞争力的同时，持续改善相关目标范围内的误差，并产生可解释的合成分子，其结构反映了底层的光谱几何。&lt;h4&gt;结论&lt;/h4&gt;光谱、几何感知的增强是不平衡分子属性回归的有效且高效的策略。&lt;h4&gt;翻译&lt;/h4&gt;在分子属性预测中，最有价值的化合物（例如高活性）通常占据目标空间的稀疏区域。标准图神经网络（GNNs）通常优化平均误差，在这些罕见但关键的情况下表现不佳，而现有的过采样方法往往会扭曲分子拓扑结构。在本文中，我们引入了SPECTRA，一种光谱目标感知的图增强框架，用于在光谱域生成真实的分子图。SPECTRA（i）从SMILES重建多属性分子图；（ii）通过(Fused)Gromov-Wasserstein耦合对齐分子对以获得节点对应关系；（iii）在稳定共享基中插值拉普拉斯特征值、特征向量和节点特征；（iv）重建边以合成具有插值目标的物理合理的中间体。一种从标签核密度估计导出的稀有感知预算方案，在数据稀缺的地方集中增强。结合使用边缘感知切比雪夫卷积的光谱GNN，SPECTRA在保持全球准确性的同时，增加了代表性不足的区域。在基准测试中，SPECTRA持续改善相关目标范围内的误差，同时保持竞争性的整体MAE，并产生可解释的合成分子，其结构反映了底层的光谱几何。我们的结果表明，光谱、几何感知的增强是不平衡分子属性回归的有效且高效的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In molecular property prediction, the most valuable compounds (e.g., highpotency) often occupy sparse regions of the target space. Standard Graph NeuralNetworks (GNNs) commonly optimize for the average error, underperforming onthese uncommon but critical cases, with existing oversampling methods oftendistorting molecular topology. In this paper, we introduce SPECTRA, a SpectralTarget-Aware graph augmentation framework that generates realistic moleculargraphs in the spectral domain. SPECTRA (i) reconstructs multi-attributemolecular graphs from SMILES; (ii) aligns molecule pairs via (Fused)Gromov-Wasserstein couplings to obtain node correspondences; (iii) interpolatesLaplacian eigenvalues, eigenvectors and node features in a stable share-basis;and (iv) reconstructs edges to synthesize physically plausible intermediateswith interpolated targets. A rarity-aware budgeting scheme, derived from akernel density estimation of labels, concentrates augmentation where data arescarce. Coupled with a spectral GNN using edge-aware Chebyshev convolutions,SPECTRA densifies underrepresented regions without degrading global accuracy.On benchmarks, SPECTRA consistently improves error in relevant target rangeswhile maintaining competitive overall MAE, and yields interpretable syntheticmolecules whose structure reflects the underlying spectral geometry. Ourresults demonstrate that spectral, geometry-aware augmentation is an effectiveand efficient strategy for imbalanced molecular property regression.</description>
      <author>example@mail.com (Brenda Nogueira, Meng Jiang, Nitesh V. Chawla, Nuno Moniz)</author>
      <guid isPermaLink="false">2511.04838v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>Hardware-Accelerated GNN-based Hit Filtering for the Belle II Level-1 Trigger</title>
      <link>http://arxiv.org/abs/2511.04731v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于FPGA的硬件加速击中过滤系统，使用图神经网络(GNN)处理Belle II Level-1触发器中的数据。该系统能够实时处理数据，实现探测器级别的背景抑制，具有低延迟和高吞吐量的特点。&lt;h4&gt;背景&lt;/h4&gt;Belle II实验需要实时处理高亮度对撞机条件下的海量数据，需要低延迟和高吞吐量的解决方案。传统的数据处理方法可能无法满足这些严格要求。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于GNN的硬件加速击中过滤系统，用于Belle II实验的Level-1触发器，实现实时数据处理和背景抑制，同时满足严格的延迟和吞吐量要求。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络处理传感线击中数据；通过量化、剪枝和静态图构建优化GNN以实现高吞吐量硬件操作；采用扇区空间并行化扩展到全探测器覆盖；在AMD Ultrascale XVCU190 FPGA上实现原型系统。&lt;h4&gt;主要发现&lt;/h4&gt;系统在31.804 MHz的持续吞吐量下实时处理传感线数据；实现探测器级别的背景抑制，测量延迟为632.4 ns；使用35.65%的查找表和29.75%的触发器，零数字信号处理使用；离线验证达到83%的背景击中拒绝率，同时保持95%的信号击中效率。&lt;h4&gt;结论&lt;/h4&gt;该工作确立了在FPGA上基于GNN的击中级别过滤作为高亮度对撞机条件下实时数据缩减的可扩展低延迟解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种硬件加速的击中过滤系统，在Belle II Level-1触发器的现场可编程门阵列上采用图神经网络。该GNN利用传感线击中之间的空间和时间关系，并通过量化、剪枝和静态图构建优化为高吞吐量硬件操作。扇区空间并行化允许扩展到全探测器覆盖，满足严格的延迟和吞吐量要求。在31.804 MHz的持续吞吐量下，系统实时处理传感线数据，并实现探测器级别的背景抑制，测量延迟为632.4 ns，同时使用35.65%的查找表和29.75%的触发器，零数字信号处理使用，如在对单个扇区的AMD Ultrascale XVCU190上的原型实现所示。使用Belle II数据的离线验证产生83%的背景击中拒绝率，同时保持95%的信号击中效率。这项工作确立了在FPGA上基于GNN的击中级别过滤作为高亮度对撞机条件下实时数据缩减的可扩展低延迟解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a hardware-accelerated hit filtering system employing Graph NeuralNetworks (GNNs) on Field-Programmable Gate Arrays (FPGAs) for the Belle IILevel-1 Trigger. The GNN exploits spatial and temporal relationships amongsense wire hits and is optimized for high-throughput hardware operation viaquantization, pruning, and static graph-building. Sector-wise spatialparallelization permits scaling to full-detector coverage, satisfying stringentlatency and throughput requirements. At a sustained throughput of 31.804 MHz,the system processes sense wire data in real-time and achieves detector-levelbackground suppression with a measured latency of 632.4 ns while utilizing35.65% of Look-Up Tables (LUTs), and 29.75% of Flip-Flops, with zero DigitalSignal Processing (DSP) usage, as demonstrated in a prototype implementationfor a single sector on an AMD Ultrascale XVCU190. Offline validation usingBelle II data yields a background hit rejection of 83% while maintaining 95%signal hit efficiency. This work establishes hit-level GNN-based filtering onFPGAs as a scalable low-latency solution for real-time data reduction inhigh-luminosity collider conditions.</description>
      <author>example@mail.com (Greta Heine, Fabio Mayer, Marc Neu, Jürgen Becker, Torben Ferber)</author>
      <guid isPermaLink="false">2511.04731v1</guid>
      <pubDate>Mon, 10 Nov 2025 14:36:03 +0800</pubDate>
    </item>
    <item>
      <title>RCMCL: A Unified Contrastive Learning Framework for Robust Multi-Modal (RGB-D, Skeleton, Point Cloud) Action Understanding</title>
      <link>http://arxiv.org/abs/2511.04351v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为鲁棒跨模态对比学习(RCMCL)的自监督框架，用于解决多模态人类动作识别中传感器故障或噪声导致性能下降的问题。&lt;h4&gt;背景&lt;/h4&gt;人类动作识别(HAR)使用多模态输入(RGB-D、骨骼、点云)可实现高精度，但通常依赖大型标记数据集，并在传感器故障或噪声情况下性能急剧下降。&lt;h4&gt;目的&lt;/h4&gt;开发一种能学习模态不变表示且在模态丢失和损坏情况下保持可靠性的自监督框架。&lt;h4&gt;方法&lt;/h4&gt;RCMCL联合优化三个目标：(1)跨模态对比目标对齐异构流，(2)模态内自蒸馏目标提高视图不变性并减少冗余，(3)退化模拟目标训练模型从掩码或损坏输入中恢复；推理时使用自适应模态门控(AMG)网络为各模态分配数据驱动的可靠性权重。&lt;h4&gt;主要发现&lt;/h4&gt;在NTU RGB+D 120和UWA3D-II数据集上，RCMCL在标准设置下达到最先进准确率，在严重双模态丢失情况下仅显示11.5%性能下降，显著优于监督融合基线。&lt;h4&gt;结论&lt;/h4&gt;自监督跨模态对齐，结合明确的退化建模和自适应融合，是实现可部署多模态HAR的关键。&lt;h4&gt;翻译&lt;/h4&gt;人类动作识别(HAR)使用多模态输入(RGB-D、骨骼、点云)可以实现高精度，但通常依赖大型标记数据集，并且在传感器故障或噪声情况下性能会急剧下降。我们提出了鲁棒跨模态对比学习(RCMCL)，一种自监督框架，学习模态不变表示，并在模态丢失和损坏情况下保持可靠性。RCMCL联合优化了：(i)对齐异构流的跨模态对比目标，(ii)提高视图不变性并减少冗余的模态内自蒸馏目标，(iii)明确训练模型从掩码或损坏输入中恢复的退化模拟目标。在推理时，自适应模态门控(AMG)网络为每个模态分配数据驱动的可靠性权重以实现鲁棒融合。在NTU RGB+D 120(CS/CV)和UWA3D-II上，RCMCL在标准设置下达到最先进的准确率，并且表现出明显更好的鲁棒性：在严重的双模态丢失情况下，仅显示11.5%的性能下降，显著优于强大的监督融合基线。这些结果表明，自监督跨模态对齐，结合明确的退化建模和自适应融合，是可部署的多模态HAR的关键。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human action recognition (HAR) with multi-modal inputs (RGB-D, skeleton,point cloud) can achieve high accuracy but typically relies on large labeleddatasets and degrades sharply when sensors fail or are noisy. We present RobustCross-Modal Contrastive Learning (RCMCL), a self-supervised framework thatlearns modality-invariant representations and remains reliable under modalitydropout and corruption. RCMCL jointly optimizes (i) a cross-modal contrastiveobjective that aligns heterogeneous streams, (ii) an intra-modalself-distillation objective that improves view-invariance and reducesredundancy, and (iii) a degradation simulation objective that explicitly trainsmodels to recover from masked or corrupted inputs. At inference, an AdaptiveModality Gating (AMG) network assigns data-driven reliability weights to eachmodality for robust fusion. On NTU RGB+D 120 (CS/CV) and UWA3D-II, RCMCLattains state-of-the-art accuracy in standard settings and exhibits markedlybetter robustness: under severe dual-modality dropout it shows only an 11.5%degradation, significantly outperforming strong supervised fusion baselines.These results indicate that self-supervised cross-modal alignment, coupled withexplicit degradation modeling and adaptive fusion, is key to deployablemulti-modal HAR.</description>
      <author>example@mail.com (Hasan Akgul, Mari Eplik, Javier Rojas, Akira Yamamoto, Rajesh Kumar, Maya Singh)</author>
      <guid isPermaLink="false">2511.04351v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
  <item>
      <title>Active Domain Adaptation for mmWave-based HAR via Renyi Entropy-based Uncertainty Estimation</title>
      <link>http://arxiv.org/abs/2511.04219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为mmADA的主动域适应框架，用于解决毫米波雷达人类活动识别中的域偏移问题，通过Renyi熵不确定性估计、对比学习和伪标记技术，以最少的标记数据实现高准确率，在各种跨域场景中表现出色。&lt;h4&gt;背景&lt;/h4&gt;人类活动识别(HAR)使用毫米波雷达是传统基于传感器方法的非侵入式替代方案，但存在域偏移问题，即模型在新用户、新位置或新环境中性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为mmADA的主动域适应(ADA)框架，以最少的标记数据高效适应基于毫米波的HAR模型。&lt;h4&gt;方法&lt;/h4&gt;mmADA通过引入基于Renyi熵的不确定性估计来识别和标记最具信息量的目标样本，从而增强适应性。此外，它利用对比学习和伪标记来使用未标记数据改进特征对齐。&lt;h4&gt;主要发现&lt;/h4&gt;使用TI IWR1443BOOST雷达在多个用户、位置和环境中的评估显示，mmADA在各种跨域设置中实现了超过90%的准确率。与五个基线的比较确认了其优越的适应性能，而在未见过的用户、环境以及另外两个开源数据集上的进一步测试验证了其鲁棒性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;mmADA框架能够有效解决毫米波雷达HAR中的域偏移问题，以最少的标记数据实现高准确率，并在各种跨域场景中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;人类活动识别(HAR)使用毫米波雷达为传统基于传感器的方法提供了非侵入式替代方案，但在新用户、新位置或新环境中存在域偏移问题，导致模型性能下降。为解决这一问题，我们提出了mmADA，这是一种主动域适应(ADA)框架，能够以最少的标记数据高效适应基于毫米波的HAR模型。mmADA通过引入基于Renyi熵的不确定性估计来识别和标记最具信息量的目标样本，从而增强适应性。此外，它还利用对比学习和伪标记来使用未标记数据改进特征对齐。在多个用户、位置和环境中使用TI IWR1443BOOST雷达进行的评估表明，mmADA在各种跨域设置中实现了超过90%的准确率。与五个基线的比较确认了其优越的适应性能，而在未见过的用户、环境以及另外两个开源数据集上的进一步测试验证了其鲁棒性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human Activity Recognition (HAR) using mmWave radar provides a non-invasivealternative to traditional sensor-based methods but suffers from domain shift,where model performance declines in new users, positions, or environments. Toaddress this, we propose mmADA, an Active Domain Adaptation (ADA) frameworkthat efficiently adapts mmWave-based HAR models with minimal labeled data.mmADA enhances adaptation by introducing Renyi Entropy-based uncertaintyestimation to identify and label the most informative target samples.Additionally, it leverages contrastive learning and pseudo-labeling to refinefeature alignment using unlabeled data. Evaluations with a TI IWR1443BOOSTradar across multiple users, positions, and environments show that mmADAachieves over 90% accuracy in various cross-domain settings. Comparisons withfive baselines confirm its superior adaptation performance, while further testson unseen users, environments, and two additional open-source datasets validateits robustness and generalization.</description>
      <author>example@mail.com (Mingzhi Lin, Teng Huang, Han Ding, Cui Zhao, Fei Wang, Ge Wang, Wei Xi)</author>
      <guid isPermaLink="false">2511.04219v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>DeNoise: Learning Robust Graph Representations for Unsupervised Graph-Level Anomaly Detection</title>
      <link>http://arxiv.org/abs/2511.04086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了DeNoise框架，解决了无监督图级别异常检测(UGAD)中训练数据被异常图污染的问题，通过对抗性训练、编码器锚点对齐和对比学习技术学习抗噪声的图表示，在多个数据集上显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;随着关键领域中图结构数据的快速增长，无监督图级别异常检测(UGAD)已成为重要任务。然而，大多数图神经网络方法假设训练集只包含正常图，这在实践中很少成立，即使少量异常图污染也会扭曲学习表示并降低性能。&lt;h4&gt;目的&lt;/h4&gt;设计一个健壮的UGAD框架，专门处理被污染的训练数据，学习对噪声不敏感的图表示。&lt;h4&gt;方法&lt;/h4&gt;DeNoise通过对抗性目标联合优化图级别编码器、属性解码器和结构解码器；引入编码器锚点对齐去噪机制，将正常图中高信息节点嵌入融合到所有图嵌入中；使用对比学习组件在潜在空间中压缩正常图嵌入并排斥异常嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;在八个真实数据集上的实验表明，DeNoise能够在不同噪声强度下学习可靠的图级别表示，显著优于最先进的UGAD基线方法。&lt;h4&gt;结论&lt;/h4&gt;DeNoise是一个有效的框架，能够处理被污染的训练数据，通过创新的去噪机制提高了UGAD的性能，在实际应用中具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;随着关键领域中图结构数据的快速增长，无监督图级别异常检测(UGAD)已成为一项关键任务。UGAD旨在识别偏离正常行为模式的整个图。然而，大多数图神经网络方法隐含假设训练集是干净的，只包含正常图，这在实践中很少成立。即使有少量异常图的污染也会扭曲学习到的表示并显著降低性能。为应对这一挑战，我们提出了DeNoise，一个专为被污染训练数据设计的健壮UGAD框架。它通过对抗性目标联合优化图级别编码器、属性解码器和结构解码器，学习抗噪声的嵌入。此外，DeNoise引入了编码器锚点对齐去噪机制，将正常图中高信息节点嵌入融合到所有图嵌入中，提高表示质量同时抑制异常干扰。然后，对比学习组件在潜在空间中压缩正常图嵌入并排斥异常嵌入。在八个真实数据集上的广泛实验表明，DeNoise能够在不同噪声强度下一致学习可靠的图级别表示，并显著优于最先进的UGAD基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid growth of graph-structured data in critical domains,unsupervised graph-level anomaly detection (UGAD) has become a pivotal task.UGAD seeks to identify entire graphs that deviate from normal behavioralpatterns. However, most Graph Neural Network (GNN) approaches implicitly assumethat the training set is clean, containing only normal graphs, which is rarelytrue in practice. Even modest contamination by anomalous graphs can distortlearned representations and sharply degrade performance. To address thischallenge, we propose DeNoise, a robust UGAD framework explicitly designed forcontaminated training data. It jointly optimizes a graph-level encoder, anattribute decoder, and a structure decoder via an adversarial objective tolearn noise-resistant embeddings. Further, DeNoise introduces an encoderanchor-alignment denoising mechanism that fuses high-information nodeembeddings from normal graphs into all graph embeddings, improvingrepresentation quality while suppressing anomaly interference. A contrastivelearning component then compacts normal graph embeddings and repels anomalousones in the latent space. Extensive experiments on eight real-world datasetsdemonstrate that DeNoise consistently learns reliable graph-levelrepresentations under varying noise intensities and significantly outperformsstate-of-the-art UGAD baselines.</description>
      <author>example@mail.com (Qingfeng Chen, Haojin Zeng, Jingyi Jie, Shichao Zhang, Debo Cheng)</author>
      <guid isPermaLink="false">2511.04086v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>KAN-Enhanced Contrastive Learning Accelerating Crystal Structure Identification from XRD Patterns</title>
      <link>http://arxiv.org/abs/2511.04055v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;XCCP是一种物理引导的对比学习框架，用于粉末X射线衍射分析，通过将衍射图与晶体结构在共享嵌入空间中对齐，实现高效结构检索和对称性识别，在结构检索和空间群识别任务中表现出色，并支持零样本迁移。&lt;h4&gt;背景&lt;/h4&gt;准确确定晶体结构对材料科学至关重要，粉末X射线衍射是一种关键技术，但当前分析流程仍严重依赖专家知识和缓慢的迭代拟合，限制了在高通量和自主环境中的可扩展性。&lt;h4&gt;目的&lt;/h4&gt;引入一种称为XCCP的物理引导对比学习框架，使粉末衍射图与候选晶体结构在共享嵌入空间中对齐，实现高效的结构检索和对称性识别。&lt;h4&gt;方法&lt;/h4&gt;XRD编码器采用双专家设计，带有Kolmogorov-Arnold Network投影头，一个分支强调低角度反射反映长程有序性，另一个分支捕获由对称性形成的密集高角度峰，结合晶体图编码器进行对比预训练。&lt;h4&gt;主要发现&lt;/h4&gt;XCCP在结构检索任务中达到0.89准确率，空间群识别达到0.93准确率，可推广到成分相似的多主元合金，并展示了对实验模式的零样本迁移能力。&lt;h4&gt;结论&lt;/h4&gt;XCCP是一种稳健、可解释和可扩展的方法，为X射线衍射分析提供了新范式，促进高通量筛选、快速结构验证并集成到自主实验室中。&lt;h4&gt;翻译&lt;/h4&gt;准确确定晶体结构对材料科学至关重要，支撑着成分-结构-性能关系的理解和新材料的发现。粉末X射线衍射是这一追求中的关键技术，因其多功能性和可靠性。然而，当前分析流程仍然严重依赖专家知识和缓慢的迭代拟合，限制了它们在高通量和自主环境中的可扩展性。在此，我们介绍了一种称为XCCP的物理引导对比学习框架。它使粉末衍射图与候选晶体结构在共享嵌入空间中对齐，以实现高效的结构检索和对称性识别。XRD编码器采用双专家设计，带有Kolmogorov-Arnold Network投影头，一个分支强调反映长程有序性的低角度反射，而另一个分支捕获由对称性塑造的密集高角度峰。与晶体图编码器相结合，对比预训练产生物理基础的表示。XCCP在各项任务中表现出色，结构检索达到0.89，空间群识别达到0.93的准确率。该框架进一步推广到成分相似的多主元合金，并展示了对实验模式的零样本迁移能力。这些结果确立了XCCP是一种稳健、可解释和可扩展的方法，为X射线衍射分析提供了新范式。XCCP促进高通量筛选、快速结构验证并集成到自主实验室中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate determination of crystal structures is central to materials science,underpinning the understanding of composition-structure-property relationshipsand the discovery of new materials. Powder X-ray diffraction is a key techniquein this pursuit due to its versatility and reliability. However, currentanalysis pipelines still rely heavily on expert knowledge and slow iterativefitting, limiting their scalability in high-throughput and autonomous settings.Here, we introduce a physics-guided contrastive learning framework termed asXCCP. It aligns powder diffraction patterns with candidate crystal structuresin a shared embedding space to enable efficient structure retrieval andsymmetry recognition. The XRD encoder employs a dual-expert design with aKolmogorov-Arnold Network projection head, one branch emphasizes low anglereflections reflecting long-range order, while the other captures dense highangle peaks shaped by symmetry. Coupled with a crystal graph encoder,contrastive pretraining yields physically grounded representations. XCCPdemonstrates strong performance across tasks, with structure retrieval reaching0.89 and space group identification attains 0.93 accuracy. The frameworkfurther generalizes to compositionally similar multi principal element alloysand demonstrates zero-shot transfer to experimental patterns. These resultsestablish XCCP as a robust, interpretable, and scalable approach that offers anew paradigm for X-ray diffraction analysis. XCCP facilitates high-throughputscreening, rapid structural validation and integration into autonomouslaboratories.</description>
      <author>example@mail.com (Chenlei Xu, Tianhao Su, Jie Xiong, Yue Wu, Shuya Dong, Tian Jiang, Mengwei He, Shuai Chen, Tong-Yi Zhang)</author>
      <guid isPermaLink="false">2511.04055v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Climbing the label tree: Hierarchy-preserving contrastive learning for medical imaging</title>
      <link>http://arxiv.org/abs/2511.03771v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种层次保持对比框架，使医学图像标签的层次结构成为训练信号和评估目标，通过HWC和LAM两个插件目标改进自监督学习在医学图像表示学习中的性能。&lt;h4&gt;背景&lt;/h4&gt;医学图像标签通常按分类法组织（如器官-组织-亚型），但标准自监督学习忽略了这种层次结构。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够尊重标签树结构的医学图像表示学习方法，提高性能和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出层次加权对比（HWC）和层级感知边界（LAM）两个插件目标，HWC通过共享祖先缩放正/负对强度，LAM在不同层级上分离祖先组，适用于欧几里得和双曲嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试中，包括乳腺组织病理学，所提出的方法比强SSL基线提高了表示质量，同时更好地尊重了分类法；HWC和LAM即使在没有曲率的情况下也有效，结合两者产生最符合分类法的表示。&lt;h4&gt;结论&lt;/h4&gt;这种方法为学习尊重标签树的医学图像表示提供了一种简单、通用的方法，在层次丰富的领域中同时提高了性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;医学图像标签通常按分类法（例如器官-组织-亚型）组织，但标准自监督学习忽略了这种结构。我们提出了一种层次保持对比框架，使标签树成为一级训练信号和评估目标。我们的方法引入了两个插件目标：层次加权对比（HWC），通过共享祖先缩放正/负对强度以促进父级内部一致性，以及层级感知边界（LAM），一种在不同层级上分离祖先组的原型边界。该公式与几何无关，适用于欧几里得和双曲嵌入，无需架构更改。在包括乳腺组织病理学在内的多个基准测试中，所提出的目标始终比强大的SSL基线提高表示质量，同时更好地尊重分类法。我们使用适合层次忠实度的指标进行评估：HF1（层次F1）、H-Acc（树距离加权准确率）和父距离违规率。我们还报告了top-1准确率以保持完整性。消融实验表明，即使没有曲率，HWC和LAM也是有效的，而结合两者会产生最符合分类法的表示。总之，这些结果为学习尊重标签树的医学图像表示提供了一种简单、通用的方法，并在层次丰富的领域中同时提高了性能和可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical image labels are often organized by taxonomies (e.g., organ - tissue- subtype), yet standard self-supervised learning (SSL) ignores this structure.We present a hierarchy-preserving contrastive framework that makes the labeltree a first-class training signal and an evaluation target. Our approachintroduces two plug-in objectives: Hierarchy-Weighted Contrastive (HWC), whichscales positive/negative pair strengths by shared ancestors to promotewithin-parent coherence, and Level-Aware Margin (LAM), a prototype margin thatseparates ancestor groups across levels. The formulation is geometry-agnosticand applies to Euclidean and hyperbolic embeddings without architecturalchanges. Across several benchmarks, including breast histopathology, theproposed objectives consistently improve representation quality over strong SSLbaselines while better respecting the taxonomy. We evaluate with metricstailored to hierarchy faithfulness: HF1 (hierarchical F1), H-Acc(tree-distance-weighted accuracy), and parent-distance violation rate. We alsoreport top-1 accuracy for completeness. Ablations show that HWC and LAM areeffective even without curvature, and combining them yields the mosttaxonomy-aligned representations. Taken together, these results provide asimple, general recipe for learning medical image representations that respectthe label tree and advance both performance and interpretability inhierarchy-rich domains.</description>
      <author>example@mail.com (Alif Elham Khan)</author>
      <guid isPermaLink="false">2511.03771v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>MacroNav: Multi-Task Context Representation Learning Enables Efficient Navigation in Unknown Environments</title>
      <link>http://arxiv.org/abs/2511.04320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MacroNav的基于学习的导航框架，解决了未知环境中自主导航的挑战，能够在保持计算效率的同时提高导航性能。&lt;h4&gt;背景&lt;/h4&gt;未知环境中的自主导航需要在部分可观测性下进行紧凑而富有表现力的空间理解，以支持高层决策制定。现有方法难以在丰富的上下文表示和导航效率和效率之间取得平衡。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够平衡丰富上下文表示与导航效率的导航框架，提高未知环境中的自主导航性能。&lt;h4&gt;方法&lt;/h4&gt;MacroNav框架包含两个关键组件：(1)通过多任务自监督学习训练的轻量级上下文编码器，用于捕获多尺度、以导航为中心的空间表示；(2)一种强化学习策略，将这些表示与基于图的推理无缝集成，用于高效的动作选择。&lt;h4&gt;主要发现&lt;/h4&gt;上下文编码器能够高效且稳健地理解环境；在实际部署中，MacroNav在成功率（SR）和路径长度加权成功率（SPL）方面显著优于最先进的导航方法，同时保持较低的计算成本。&lt;h4&gt;结论&lt;/h4&gt;MacroNav是一种有效的导航框架，能够在保持计算效率的同时提高导航性能，解决了现有方法在上下文表示与导航效率之间的平衡问题。&lt;h4&gt;翻译&lt;/h4&gt;未知环境中的自主导航需要在部分可观测性下进行紧凑而富有表现力的空间理解，以支持高层决策制定。现有方法难以在丰富的上下文表示与导航效率之间取得平衡。我们提出了MacroNav，一种基于学习的导航框架，具有两个关键组件：(1)通过多任务自监督学习训练的轻量级上下文编码器，用于捕获多尺度、以导航为中心的空间表示；(2)一种强化学习策略，将这些表示与基于图的推理无缝集成，用于高效的动作选择。大量实验证明了上下文编码器的高效和稳健的环境理解能力。实际部署进一步验证了MacroNav的有效性，在成功率（SR）和路径长度加权成功率（SPL）方面比最先进的导航方法有显著提升，同时保持较低的计算成本。代码将在接受后发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在未知环境中自主导航时如何平衡丰富的环境表示与导航效率之间的矛盾。这个问题非常重要，因为自主导航是机器人的基本挑战，需要在部分可观察条件下实时决策并找到到达目标的路径，而现有方法要么计算成本高，要么在复杂环境中表现不佳，限制了机器人在救援、自动驾驶和家庭服务等实际应用中的效能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括传统方法在动态场景中的不足和基于学习的方法的泛化问题。他们借鉴了视觉变换器架构用于空间表示学习、自监督学习方法（但针对导航任务专门设计）、强化学习中的软演员-批评家算法以及图推理方法。作者的创新在于设计了三个互补的自监督任务（随机路径掩码、视野预测和掩码自编码）来同时捕捉全局结构、局部几何和遮挡鲁棒性，并通过分层交叉注意力将这些表示与导航策略融合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多任务自监督学习学习针对导航优化的上下文表示，然后与强化学习策略结合实现高效导航。整体流程包括：1) 将占用地图分割成块序列，通过三个自监督任务训练上下文编码器；2) 在机器人当前位置周围采样候选航点构建局部拓扑图；3) 使用分层交叉注意力融合上下文表示和节点特征；4) 通过指针注意力模块生成航点选择概率；5) 使用软演员-批评家算法训练导航策略，平衡任务完成、轨迹效率和目标导向行为。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 多任务自监督学习框架，包含三个互补任务（SPM、FOV和MAE）分别针对全局结构、局部几何和遮挡鲁棒性；2) 上下文感知的RL导航策略，通过分层交叉注意力融合多尺度上下文表示与拓扑图推理；3) 轻量级但高效的环境理解能力。相比之前工作，不同之处在于：不依赖手工规则（与传统方法相比）、显式建模环境上下文（与端到端RL方法相比）、保留细粒度几何信息（与图结构方法相比）、以及专门针对导航任务设计（与通用视觉模型相比）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MacroNav通过多任务自监督学习学习针对导航优化的多尺度上下文表示，并结合强化学习实现了在未知环境中高效且鲁棒的自主导航，显著提升了成功率和路径效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous navigation in unknown environments requires compact yet expressivespatial understanding under partial observability to support high-leveldecision making. Existing approaches struggle to balance rich contextualrepresentation with navigation efficiency. We present MacroNav, alearning-based navigation framework featuring two key components: (1) alightweight context encoder trained via multi-task self-supervised learning tocapture multi-scale, navigation-centric spatial representations; and (2) areinforcement learning policy that seamlessly integrates these representationswith graph-based reasoning for efficient action selection. Extensiveexperiments demonstrate the context encoder's efficient and robustenvironmental understanding. Real-world deployments further validate MacroNav'seffectiveness, yielding significant gains over state-of-the-art navigationmethods in both Success Rate (SR) and Success weighted by Path Length (SPL),while maintaining low computational cost. Code will be released uponacceptance.</description>
      <author>example@mail.com (Kuankuan Sima, Longbin Tang, Haozhe Ma, Lin Zhao)</author>
      <guid isPermaLink="false">2511.04320v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation</title>
      <link>http://arxiv.org/abs/2511.03992v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了CaRF框架，解决了3D高斯分割中的跨视图一致性问题，通过直接在3D高斯空间中操作并引入相机感知的高斯场编码和训练中配对视图监督，实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;现有的3D高斯分割方法虽然实现了语言和3D几何之间的跨模态对齐，但由于依赖2D渲染的伪监督和特定于视图的特征学习，仍然面临跨视图一致性的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个完全可分的框架，直接在3D高斯空间中操作，实现多视图一致性，提高3D场景理解的可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出CaRF框架，包括高斯场相机编码（GFCE）将相机几何纳入高斯文本交互，以及训练中配对视图监督（ITPVS）对齐校准视图的每个高斯logits。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准测试上，CaRF在mIoU上分别比最先进方法提高了16.8%（Ref LERF）、4.3%（LERF OVS）和2.0%（3D OVS）。&lt;h4&gt;结论&lt;/h4&gt;CaRF框架促进了更可靠和视图一致的3D场景理解，对具身AI、AR/VR交互和自主感知有潜在益处。&lt;h4&gt;翻译&lt;/h4&gt;Referring 3D Gaussian Splatting Segmentation (R3DGS)指的是3D高斯喷洒分割，Camera Aware Referring Field (CaRF)是相机感知的参考场，Gaussian Field Camera Encoding (GFCE)是高斯场相机编码，In Training Paired View Supervision (ITPVS)是训练中配对视图监督。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决Referring 3D Gaussian Splatting Segmentation (R3DGS)中的多视角一致性问题。现有方法在不同视角下会产生不一致的分割结果，这影响了3D场景理解的准确性和可靠性。这个问题在现实中很重要，因为一致的3D场景理解对于具身AI、AR/VR交互和自主感知等应用至关重要，而不一致的分割会导致空间推理错误和用户体验下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有R3DGS方法的局限性，特别是它们依赖单视角伪监督和视图特定特征学习的问题。他们借鉴了3D高斯溅射(3DGS)的基本框架和R3DGS的基本概念，但认识到需要在3D高斯空间中直接操作。作者的创新思路是引入相机几何信息和多视角监督机制，设计了Gaussian Field Camera Encoding (GFCE)将相机参数整合到高斯-文本交互中，以及In-Training Paired-View Supervision (ITPVS)在训练过程中对齐校准视角下的高斯特征，从而实现多视角一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; CaRF的核心思想是通过直接在3D高斯空间中操作并引入相机几何信息来实现多视角一致性。具体包括：1)相机感知的referring场，将相机参数整合到高斯特征空间中以捕获视角依赖线索；2)训练过程中的配对视角监督，将选定高斯投影到两个校准视角中。整体流程包括：预处理阶段训练3DGS模型并生成伪真实掩码；训练阶段采样视角对，执行跨模态交互，生成相机特征，计算视角感知特征，渲染referring掩码，计算双视角损失和对比损失，更新参数；推理阶段使用训练好的模型渲染任意视角的referring掩码。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Gaussian Field Camera Encoding (GFCE)，将相机参数嵌入高斯特征空间明确建模视角依赖变化；2)In-Training Paired-View Supervision (ITPVS)，在训练过程中对齐校准视角下的高斯logit；3)完全可微框架直接在3D高斯空间操作。相比之前的工作，CaRF与ReferSplat不同在于引入多视角监督和显式几何条件；与基于2D特征/掩码的方法不同在于直接利用3D几何信息；与传统多视图一致性方法不同在于避免了非可微分组件并更好地利用了3D几何。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CaRF通过引入相机感知的高斯场编码和训练过程中的配对视角监督，显著提升了3D高斯溅射分割中的多视角一致性，实现了更可靠和几何一致的3D场景理解。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpretfree-form language expressions and localize the corresponding 3D regions inGaussian fields. While recent advances have introduced cross-modal alignmentbetween language and 3D geometry, existing pipelines still struggle withcross-view consistency due to their reliance on 2D rendered pseudo supervisionand view specific feature learning. In this work, we present Camera AwareReferring Field (CaRF), a fully differentiable framework that operates directlyin the 3D Gaussian space and achieves multi view consistency. Specifically,CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporatescamera geometry into Gaussian text interactions to explicitly model viewdependent variations and enhance geometric reasoning. Building on this, InTraining Paired View Supervision (ITPVS) is proposed to align per Gaussianlogits across calibrated views during training, effectively mitigating singleview overfitting and exposing inter view discrepancies for optimization.Extensive experiments on three representative benchmarks demonstrate that CaRFachieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state ofthe art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.Moreover, this work promotes more reliable and view consistent 3D sceneunderstanding, with potential benefits for embodied AI, AR/VR interaction, andautonomous perception.</description>
      <author>example@mail.com (Yuwen Tao, Kanglei Zhou, Xin Tan, Yuan Xie)</author>
      <guid isPermaLink="false">2511.03992v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Simple 3D Pose Features Support Human and Machine Social Scene Understanding</title>
      <link>http://arxiv.org/abs/2511.03988v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究通过比较人类和AI系统在社交互动识别上的能力，发现人类依赖3D视觉空间姿态信息，而大多数AI模型缺乏这种明确的表示。研究推导出简化的3D社交姿态特征，不仅能匹配人类判断，还能提升现有AI模型性能。&lt;h4&gt;背景&lt;/h4&gt;人类能够快速轻松地从视觉输入中提取关于他人社交互动的各种信息，从基本的视觉空间线索到更高级的信息。然而，支持这些能力的计算机制仍不清楚，社交互动识别甚至对最先进的AI视觉系统来说仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;验证人类是否依赖3D视觉空间姿态信息来进行社交互动判断，而这些信息在大多数AI视觉模型中是缺失的。&lt;h4&gt;方法&lt;/h4&gt;结合最先进的姿态和深度估计算法，从描绘日常人类行为的短视频中提取人的3D关节位置，并将其预测人类社交互动判断的能力与当前AI视觉模型进行比较。此外，推导出一组紧凑的3D社交姿态特征，仅描述视频中面部的3D位置和方向。&lt;h4&gt;主要发现&lt;/h4&gt;1) 3D关节位置的表现优于大多数当前AI视觉模型；2) 关键社交信息在明确的身体位置中可用，而不是在大多数视觉模型的特征中；3) 最小化的3D社交姿态特征与完整3D关节集的预测强度相匹配；4) 当与现有AI视觉模型的嵌入结合时，这些特征显著提高了模型性能；5) 每个现成AI视觉模型中3D社交姿态特征的表示程度预测了模型匹配人类社交判断的能力。&lt;h4&gt;结论&lt;/h4&gt;研究结果提供了强有力的证据，表明人类社交场景理解依赖于明确的3D姿态表示，并且可以通过简单的、结构化的视觉空间原语来支持。&lt;h4&gt;翻译&lt;/h4&gt;人类能够快速轻松地从视觉输入中提取关于他人社交互动的各种信息，从基本的视觉空间线索（如两个人是否面对面）到更高级的信息。然而，支持这些能力的计算机制仍不清楚，社交互动识别甚至对最先进的AI视觉系统来说仍然是一个挑战。在这里，我们假设人类依赖3D视觉空间姿态信息来进行社交互动判断，而这些信息在大多数AI视觉模型中是缺失的。为了验证这一点，我们结合了最先进的姿态和深度估计算法，从描绘日常人类行为的短视频中提取人的3D关节位置，并将其预测人类社交互动判断的能力与当前AI视觉模型进行比较。令人惊讶的是，3D关节位置的表现优于大多数当前AI视觉模型，这表明关键社交信息在明确的身体位置中可用，而不是在大多数视觉模型的特征中，包括甚至用于提取关节位置的姿态模型的逐层嵌入。为了揭示人类用于社交判断的关键姿态特征，我们推导出一组紧凑的3D社交姿态特征，仅描述视频中面部的3D位置和方向。我们发现这些最小描述符与完整3D关节集的预测强度相匹配，并且当与现成AI视觉模型的嵌入结合时，显著提高了这些模型的性能。此外，每个现成AI视觉模型中3D社交姿态特征的表示程度预测了模型匹配人类社交判断的能力。总之，我们的研究结果提供了强有力的证据，表明人类社交场景理解依赖于明确的3D姿态表示，并且可以通过简单的、结构化的视觉空间原语来支持。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决人类社交场景理解与AI视觉模型之间的差距问题。人类能快速从视觉输入中提取社交互动信息，而最先进的AI系统在这方面表现不佳。这个问题重要是因为社交理解是人类核心能力，也是AI系统在社交机器人、人机交互等领域应用的关键瓶颈，理解人类如何进行社交判断有助于改进AI系统。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于认知理论和现有AI模型的局限性思考，假设人类依赖3D视觉空间姿态信息进行社交判断，而大多数AI模型缺乏这种显式表示。他们借鉴了认知理论中关于简单视觉空间线索（如距离和朝向）是社交互动基础的观点，以及现有计算机视觉中的姿态估计和深度估计算法。设计上结合最先进技术提取3D关节位置，并创造性地简化为仅包含头部位置和朝向的紧凑特征集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是人类社交理解依赖于显式的3D视觉空间姿态信息，特别是简单的几何基元（如头部位置和朝向），而当前AI模型缺乏这种表示。整体流程包括：1)使用250个短视频数据集和人类评分；2)提取AI视觉模型特征和3D关节位置；3)从3D关节推导出紧凑的3D社交姿态特征；4)通过岭回归将特征映射到五个社交维度；5)比较不同特征集预测人类评分的能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)发现显式3D关节位置预测社交判断优于大多数AI模型；2)提出仅含头部位置和朝向的紧凑3D社交姿态特征；3)证明结合3D姿态特征可显著提升AI模型性能；4)发现能更好编码3D社交姿态的模型更符合人类判断。相比之前工作，本研究使用显式、可解释的3D表示而非从数据中学习的特征，采用简单几何基元而非复杂网络，首次证明3D信息对社交理解的关键作用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文证明了人类社交场景理解依赖于简单的3D视觉空间姿态信息，并展示了显式的几何表示可以显著提升AI系统对社交场景的理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans can quickly and effortlessly extract a variety of information aboutothers' social interactions from visual input, ranging from visuospatial cueslike whether two people are facing each other to higher-level information. Yet,the computations supporting these abilities remain poorly understood, andsocial interaction recognition continues to challenge even the most advanced AIvision systems. Here, we hypothesized that humans rely on 3D visuospatial poseinformation to make social interaction judgments, which is absent in most AIvision models. To test this, we combined state-of-the-art pose and depthestimation algorithms to extract 3D joint positions of people in short videoclips depicting everyday human actions and compared their ability to predicthuman social interaction judgments with current AI vision models. Strikingly,3D joint positions outperformed most current AI vision models, revealing thatkey social information is available in explicit body position but not in thelearned features of most vision models, including even the layer-wiseembeddings of the pose models used to extract joint positions. To uncover thecritical pose features humans use to make social judgments, we derived acompact set of 3D social pose features describing only the 3D position anddirection of faces in the videos. We found that these minimal descriptorsmatched the predictive strength of the full set of 3D joints and significantlyimproved the performance of off-the-shelf AI vision models when combined withtheir embeddings. Moreover, the degree to which 3D social pose features wererepresented in each off-the-shelf AI vision model predicted the model's abilityto match human social judgments. Together, our findings provide strong evidencethat human social scene understanding relies on explicit representations of 3Dpose and can be supported by simple, structured visuospatial primitives.</description>
      <author>example@mail.com (Wenshuo Qin, Leyla Isik)</author>
      <guid isPermaLink="false">2511.03988v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding</title>
      <link>http://arxiv.org/abs/2511.03325v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了SurgViVQA，一个专门用于手术领域的视频问答模型，能够处理时间连贯的事件而非孤立图像，从而增强手术过程中的理解能力。研究团队还创建了REAL-Colon-VQA数据集来评估模型性能。&lt;h4&gt;背景&lt;/h4&gt;当前手术视频问答方法局限于静态图像特征，且可用数据集通常缺乏时间标注，忽略了准确解读手术程序所需的关键动态信息。&lt;h4&gt;目的&lt;/h4&gt;开发能够从静态图像视觉推理扩展到动态手术场景的视频问答模型，捕捉时间线索如运动和工具-组织交互，从而更有效地解释动态手术程序背景。&lt;h4&gt;方法&lt;/h4&gt;1. 提出SurgViVQA模型，使用掩码视频-文本编码器融合视频和问题特征；2. 捕捉运动和工具-组织交互等时间线索；3. 使用微调的大语言模型解码这些特征为连贯答案；4. 创建REAL-Colon-VQA结肠镜视频数据集，包含运动相关问题、诊断属性和模板外问题；5. 在REAL-Colon-VQA和公开的EndoVis18-VQA数据集上验证性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. SurgViVQA在关键词准确性上优于现有基于图像的VQA基准模型；2. 在REAL-Colon-VQA上比PitVQA提高11%；3. 在EndoVis18-VQA上比PitVQA提高9%；4. 问题扰动研究证实了模型对问题表述变化的泛化能力和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;SurgViVQA和REAL-Colon-VQA数据集为手术视频问答中的时间感知理解提供了框架，使AI模型能够更有效地解释动态程序背景。&lt;h4&gt;翻译&lt;/h4&gt;手术领域的视频问答旨在通过使AI模型能够对时间连贯的事件而非孤立帧进行推理，从而增强手术中的理解能力。当前方法仅限于静态图像特征，且可用数据集通常缺乏时间标注，忽略了准确解读手术程序所需的关键动态信息。我们提出了SurgViVQA，一种手术视频问答模型，将视觉推理从静态图像扩展到动态手术场景。它使用掩码视频-文本编码器融合视频和问题特征，捕捉运动和工具-组织交互等时间线索，然后由微调的大语言模型解码为连贯答案。为评估其性能，我们整理了REAL-Colon-VQA，一个包含运动相关问题、诊断属性以及重新表述或语义改变的问题表述的结肠镜视频数据集，以评估模型的鲁棒性。在REAL-Colon-VQA和公开的EndoVis18-VQA数据集上的实验验证表明，SurgViVQA在关键词准确性上优于现有的基于图像的VQA基准模型，在REAL-Colon-VQA上比PitVQA提高11%，在EndoVis18-VQA上提高9%。对问题的扰动研究进一步证实了模型对问题表述变化的泛化能力和鲁棒性。SurgViVQA和REAL-Colon-VQA数据集为手术视频问答中的时间感知理解提供了框架，使AI模型能够更有效地解释动态程序背景。代码和数据集可在https://github.com/madratak/SurgViVQA获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Question Answering (VideoQA) in the surgical domain aims to enhanceintraoperative understanding by enabling AI models to reason over temporallycoherent events rather than isolated frames. Current approaches are limited tostatic image features, and available datasets often lack temporal annotations,ignoring the dynamics critical for accurate procedural interpretation. Wepropose SurgViVQA, a surgical VideoQA model that extends visual reasoning fromstatic images to dynamic surgical scenes. It uses a Masked Video--Text Encoderto fuse video and question features, capturing temporal cues such as motion andtool--tissue interactions, which a fine-tuned large language model (LLM) thendecodes into coherent answers. To evaluate its performance, we curatedREAL-Colon-VQA, a colonoscopic video dataset that includes motion-relatedquestions and diagnostic attributes, as well as out-of-template questions withrephrased or semantically altered formulations to assess model robustness.Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA datasetshows that SurgViVQA outperforms existing image-based VQA benchmark models,particularly in keyword accuracy, improving over PitVQA by +11\% onREAL-Colon-VQA and +9\% on EndoVis18-VQA. A perturbation study on the questionsfurther confirms improved generalizability and robustness to variations inquestion phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a frameworkfor temporally-aware understanding in surgical VideoQA, enabling AI models tointerpret dynamic procedural contexts more effectively. Code and datasetavailable at https://github.com/madratak/SurgViVQA.</description>
      <author>example@mail.com (Mauro Orazio Drago, Luca Carlini, Pelinsu Celebi Balyemez, Dennis Pierantozzi, Chiara Lena, Cesare Hassan, Danail Stoyanov, Elena De Momi, Sophia Bano, Mobarak I. Hoque)</author>
      <guid isPermaLink="false">2511.03325v2</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction</title>
      <link>http://arxiv.org/abs/2511.04595v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了UniSplat，一种通用的前馈3D重建框架，通过统一的潜在时空融合学习鲁棒的动态场景重建，解决自动驾驶中稀疏、非重叠摄像头视图和复杂场景动态性的挑战。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶中的前馈3D重建技术发展迅速，但现有方法难以同时处理稀疏、非重叠的摄像头视图和复杂场景动态性这两个联合挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个通用的前馈框架UniSplat，通过统一的潜在时空融合学习鲁棒的动态场景重建，解决现有方法面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;1. 构建3D潜在支架，利用预训练基础模型捕获几何和语义场景上下文；2. 引入高效融合机制，直接在3D支架内操作实现时空对齐；3. 设计双分支解码器，结合点锚定细化与基于体素的生成；4. 保持静态高斯函数的持久记忆实现流式场景完成。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上，UniSplat在新视角合成方面达到最先进性能，即使对于原始摄像头覆盖范围之外的视点，也能提供鲁棒且高质量的渲染。&lt;h4&gt;结论&lt;/h4&gt;UniSpat是一个有效的通用前馈框架，能够通过统一的潜在时空融合学习鲁棒的动态场景重建，在稀疏、非重叠的摄像头视图和复杂场景动态性条件下提供高质量的3D重建结果。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶的前馈3D重建已经迅速发展，然而现有方法难以处理稀疏、非重叠摄像头视图和复杂场景动态性的联合挑战。我们提出了UniSplat，一个通用的前馈框架，通过统一的潜在时空融合学习鲁棒的动态场景重建。UniSplat构建一个3D潜在支架，这是一种结构化表示，通过利用预训练的基础模型捕获几何和语义场景上下文。为了有效跨空间视图和时间帧整合信息，我们引入了一个高效的融合机制，直接在3D支架内操作，实现一致的时空对齐。为确保完整和详细的重建，我们设计了一个双分支解码器，通过结合点锚定细化和基于体素的生成，从融合支架生成动态感知的高斯函数，并保持静态高斯函数的持久记忆，以实现当前摄像头覆盖范围之外的流式场景完成。在真实世界数据集上的大量实验表明，UniSplat在新视角合成方面取得了最先进的性能，同时即使对于原始摄像头覆盖范围之外的视点，也能提供鲁棒且高质量的渲染。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶场景中3D重建面临的挑战，特别是处理稀疏、非重叠的摄像头视图和复杂动态场景的问题。这个问题在现实中非常重要，因为高质量的3D场景重建是自动驾驶系统的核心能力，支持仿真、场景理解和长期规划，对于自动驾驶系统的感知、决策和控制至关重要。现有方法通常假设输入图像之间存在大量视点重叠，且需要针对每个场景进行优化，这限制了它们在实时驾驶场景中的适用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性，包括传统方法在2D空间中融合空间信息时受限于有限视图重叠，以及现有方法通常处理原始历史图像而非潜在表示。作者借鉴了几何基础模型来推断多视图图像的连贯3D结构，视觉基础模型提取语义信息，以及3D高斯溅射作为场景表示基础。作者的创新设计包括构建统一的3D潜在支架来融合多视图空间信息和多帧时间信息，在3D支架空间中直接执行空间融合，在支架表示内直接整合时间线索，以及设计双分支解码器生成动态感知的高斯原语。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个统一的3D潜在支架，融合多视图空间信息和多帧时间信息，在统一的以自我为中心的空间中编码显式3D几何，支持直接和高效的多视图和多帧时空融合，并使用双分支解码器生成动态感知的高斯原语。整体流程包括：1)3D支架构建：使用几何基础模型预测3D点图，通过尺度对齐解决模糊问题，组织成稀疏体素网格融合几何和语义特征；2)统一的时空支架融合：使用稀疏3D U-Net进行空间融合，通过变形前一帧支架到当前坐标系进行时间融合；3)动态感知的高斯生成：通过点解码器和体素解码器双分支生成高斯原语，并维护静态高斯存储器进行长期场景补全。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的3D潜在支架表示，融合多视图空间和多帧时间信息；2)基于支架的融合机制，支持统一的时空对齐和渐进式场景记忆集成；3)双分支高斯生成机制，结合点锚定细化和基于体素的生成；4)动态感知处理，通过动态分数识别静态内容并维护跨帧记忆。相比之前工作，不同之处在于UniSplat在统一的3D潜在支架中同时处理时空信息，而非分别处理；直接在3D空间进行融合而非2D图像空间；在支架表示内直接整合时间线索而非处理原始图像；能够更好地分离动态与静态场景内容。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UniSplat通过统一的3D潜在支架表示，实现了自动驾驶场景中稀疏、非重叠摄像头视图下高效、高质量的动态场景重建与新颖视图合成，同时支持长期场景记忆和补全。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,yet existing methods struggle with the joint challenges of sparse,non-overlapping camera views and complex scene dynamics. We present UniSplat, ageneral feed-forward framework that learns robust dynamic scene reconstructionthrough unified latent spatio-temporal fusion. UniSplat constructs a 3D latentscaffold, a structured representation that captures geometric and semanticscene context by leveraging pretrained foundation models. To effectivelyintegrate information across spatial views and temporal frames, we introduce anefficient fusion mechanism that operates directly within the 3D scaffold,enabling consistent spatio-temporal alignment. To ensure complete and detailedreconstructions, we design a dual-branch decoder that generates dynamic-awareGaussians from the fused scaffold by combining point-anchored refinement withvoxel-based generation, and maintain a persistent memory of static Gaussians toenable streaming scene completion beyond current camera coverage. Extensiveexperiments on real-world datasets demonstrate that UniSplat achievesstate-of-the-art performance in novel view synthesis, while providing robustand high-quality renderings even for viewpoints outside the original cameracoverage.</description>
      <author>example@mail.com (Chen Shi, Shaoshuai Shi, Xiaoyang Lyu, Chunyang Liu, Kehua Sheng, Bo Zhang, Li Jiang)</author>
      <guid isPermaLink="false">2511.04595v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability</title>
      <link>http://arxiv.org/abs/2511.04474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种三轴分析框架，用于适应地理空间基础模型(GeoFMs)进行滑坡测绘，展示了其在不同条件下的优越性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;滑坡对生命、基础设施和环境造成严重损害，准确及时的测绘对灾害准备和响应至关重要。然而，传统深度学习模型在不同传感器、区域或训练数据有限的情况下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一个三轴分析框架（传感器、标签和领域）来适应地理空间基础模型(GeoFMs)，专注于Prithvi-EO-2.0模型用于滑坡测绘。&lt;h4&gt;方法&lt;/h4&gt;基于全球预训练、自监督和可适应微调构建模型，并通过一系列实验比较不同模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在性能上超越了特定任务的CNNs（U-Net，U-Net++）、视觉变换器（Segformer，SwinV2-B）和其他GeoFMs（TerraMind，SatMAE）；对光谱变化具有鲁棒性；在标签稀缺的情况下保持准确性；在不同数据集和地理环境中更可靠地泛化。&lt;h4&gt;结论&lt;/h4&gt;GeoFMs为减少滑坡风险和环境监测提供了更强大和可扩展的方法。研究也指出了剩余挑战，如计算成本和滑坡研究中可重用的AI就绪训练数据的有限可用性。&lt;h4&gt;翻译&lt;/h4&gt;滑坡对生命、基础设施和环境造成严重损害，使得准确及时的测绘对灾害准备和响应至关重要。然而，传统的深度学习模型通常在不同传感器、区域或训练数据有限的情况下表现不佳。为应对这些挑战，我们提出了一个三轴分析框架（传感器、标签和领域）来适应地理空间基础模型(GeoFMs)，专注于Prithvi-EO-2.0用于滑坡测绘。通过一系列实验，我们表明它始终超越特定任务的CNNs（U-Net，U-Net++）、视觉变换器（Segformer，SwinV2-B）和其他GeoFMs（TerraMind，SatMAE）。该模型基于全球预训练、自监督和可适应微调，证明了对光谱变化的鲁棒性，在标签稀缺的情况下保持准确性，并在不同数据集和地理环境中更可靠地泛化。除了这些优势，我们还强调了剩余的挑战，如计算成本和滑坡研究中可重用的AI就绪训练数据的有限可用性。总体而言，我们的研究将GeoFMs定位为减少滑坡风险和环境监测的更强大和可扩展的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Landslides cause severe damage to lives, infrastructure, and the environment,making accurate and timely mapping essential for disaster preparedness andresponse. However, conventional deep learning models often struggle whenapplied across different sensors, regions, or under conditions of limitedtraining data. To address these challenges, we present a three-axis analyticalframework of sensor, label, and domain for adapting geospatial foundationmodels (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through aseries of experiments, we show that it consistently outperforms task-specificCNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and otherGeoFMs (TerraMind, SatMAE). The model, built on global pretraining,self-supervision, and adaptable fine-tuning, proved resilient to spectralvariation, maintained accuracy under label scarcity, and generalized morereliably across diverse datasets and geographic settings. Alongside thesestrengths, we also highlight remaining challenges such as computational costand the limited availability of reusable AI-ready training data for landslideresearch. Overall, our study positions GeoFMs as a step toward more robust andscalable approaches for landslide risk reduction and environmental monitoring.</description>
      <author>example@mail.com (Wenwen Li, Sizhe Wang, Hyunho Lee, Chenyan Lu, Sujit Roy, Rahul Ramachandran, Chia-Yu Hsu)</author>
      <guid isPermaLink="false">2511.04474v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment</title>
      <link>http://arxiv.org/abs/2511.04288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种针对除草剂试验的领域特定视觉模型，通过自监督学习在农业数据集上训练，显著提高了物种识别和损伤分类的准确性，特别是在未见条件下表现更佳，同时大幅减少了标注需求。&lt;h4&gt;背景&lt;/h4&gt;除草剂田间试验需要准确识别植物物种并评估除草剂引起的损伤。通用视觉基础模型在复杂视觉领域表现良好，但在农业领域表现有限，因为农业中需要精细区分物种和损伤类型。&lt;h4&gt;目的&lt;/h4&gt;将通用视觉基础模型适应于除草剂试验特征描述，提高物种识别和损伤分类的准确性。&lt;h4&gt;方法&lt;/h4&gt;使用自监督学习方法在大型精选农业数据集上训练模型，学习针对除草剂试验图像优化的丰富且可迁移的表示。&lt;h4&gt;主要发现&lt;/h4&gt;领域特定模型在物种识别(F1分数从0.91提高到0.94)和损伤分类(从0.26提高到0.33)方面显著优于通用模型；在未见条件下获得更大提升；在无人机图像等领域转换场景中保持强性能；领域特定预训练提高了分割准确性；在未见条件下，领域特定模型比通用模型实现5.4%更高的F1分数，同时使用80%更少的标记样本。&lt;h4&gt;结论&lt;/h4&gt;领域特定基础模型具有强大的泛化能力，可以显著减少手动标注工作量，为除草剂试验分析提供可扩展和自动化的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;除草剂田间试验需要准确识别植物物种并评估除草剂引起的损伤，跨越不同环境。虽然通用视觉基础模型在复杂视觉领域显示出有希望的结果，但它们在农业领域的表现可能有限，因为物种和损伤类型之间的精细区分至关重要。在这项工作中，我们将通用视觉基础模型适应于除草剂试验特征描述。使用自监督学习方法在大型精选农业数据集上训练，该模型学习了针对除草剂试验图像优化的丰富且可迁移的表示。我们的领域特定模型在物种识别(F1分数从0.91提高到0.94)和损伤分类(从0.26提高到0.33)方面显著优于最佳通用基础模型。在未见条件下(新地点和其他时间)，它获得更大提升(物种识别从0.56提高到0.66；损伤分类从0.17提高到0.27)。在领域转换场景中，如无人机图像，它保持强性能(物种分类从0.49提高到0.60)。此外，我们表明领域特定预训练提高了分割准确性，特别是在低标注情况下。标注效率分析显示，在未见条件下，领域特定模型比通用模型实现5.4%更高的F1分数，同时使用80%更少的标记样本。这些结果证明了领域特定基础模型的泛化能力及其显著减少手动标注工作的潜力，为除草剂试验分析提供了可扩展和自动化的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Herbicide field trials require accurate identification of plant species andassessment of herbicide-induced damage across diverse environments. Whilegeneral-purpose vision foundation models have shown promising results incomplex visual domains, their performance can be limited in agriculture, wherefine-grained distinctions between species and damage types are critical.  In this work, we adapt a general-purpose vision foundation model to herbicidetrial characterization. Trained using a self-supervised learning approach on alarge, curated agricultural dataset, the model learns rich and transferablerepresentations optimized for herbicide trials images.  Our domain-specific model significantly outperforms the best general-purposefoundation model in both species identification (F1 score improvement from 0.91to 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions(new locations and other time), it achieves even greater gains (speciesidentification from 0.56 to 0.66; damage classification from 0.17 to 0.27). Indomain-shift scenarios, such as drone imagery, it maintains strong performance(species classification from 0.49 to 0.60).  Additionally, we show that domain-specific pretraining enhances segmentationaccuracy, particularly in low-annotation regimes. An annotation-efficiencyanalysis reveals that, under unseen conditions, the domain-specific modelachieves 5.4% higher F1 score than the general-purpose model, while using 80%fewer labeled samples.  These results demonstrate the generalization capabilities of domain-specificfoundation models and their potential to significantly reduce manual annotationefforts, offering a scalable and automated solution for herbicide trialanalysis.</description>
      <author>example@mail.com (Leire Benito-Del-Valle, Artzai Picón, Daniel Mugica, Manuel Ramos, Eva Portillo, Javier Romero, Carlos Javier Jimenez, Ramón Navarra-Mestre)</author>
      <guid isPermaLink="false">2511.04288v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>A Parallel Region-Adaptive Differential Privacy Framework for Image Pixelization</title>
      <link>http://arxiv.org/abs/2511.04261v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的并行、区域自适应像素化框架，结合差分隐私的理论严谨性和实际效率，解决了高分辨率视觉系统中视频应用的隐私风险问题。&lt;h4&gt;背景&lt;/h4&gt;高分辨率视觉传感系统的广泛部署和基础模型的兴起增加了基于视频的应用中的隐私风险。差分私有像素化虽然提供数学保证的保护，但在保持任务保真度、实现可扩展性和高效实时部署方面仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种并行、区域自适应像素化框架，结合差分隐私的理论严谨性和实际效率，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出自适应调整基于区域复杂性的网格大小和噪声规模的方法，利用GPU并行性实现运行时加速，引入轻量级存储方案减少空间开销，并在拉普拉斯机制和并行组合定理下提供正式隐私分析。&lt;h4&gt;主要发现&lt;/h4&gt;在PETS、Venice-2和PPM-100数据集上的实验展示了有利的隐私-效用权衡和显著的运行时/存储减少。CelebA上的面部重新识别攻击实验证实了该方法在防止身份推断方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法适合实时关键隐私应用，如老年人护理、智能家居监控、驾驶员行为分析和人群行为监控。&lt;h4&gt;翻译&lt;/h4&gt;高分辨率视觉传感系统的广泛部署，结合基础模型的兴起，增加了基于视频的应用中的隐私风险。差分私有像素化通过基于网格的噪声添加为视觉数据提供数学保证的保护，但在保持任务相关的保真度、实现可扩展性和实现高效实时部署方面仍存在挑战。为此，我们提出了一种新颖的并行、区域自适应像素化框架，结合差分隐私的理论严谨性和实际效率。我们的方法基于区域复杂性自适应调整网格大小和噪声规模，利用GPU并行性实现比经典基线显著的运行时加速。通过仅保留必要的噪声统计信息，引入了轻量级存储方案，显著减少了空间开销。在拉普拉斯机制和并行组合定理下提供了正式的隐私分析。在PETS、Venice-2和PPM-100数据集上的大量实验展示了有利的隐私-效用权衡和显著的运行时/存储减少。在CelebA上的面部重新识别攻击实验进一步证实了该方法在防止身份推断方面的有效性。这验证了其适合于实时关键隐私应用，如老年人护理、智能家居监控、驾驶员行为分析和人群行为监控。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The widespread deployment of high-resolution visual sensing systems, coupledwith the rise of foundation models, has amplified privacy risks in video-basedapplications. Differentially private pixelization offers mathematicallyguaranteed protection for visual data through grid-based noise addition, butchallenges remain in preserving task-relevant fidelity, achieving scalability,and enabling efficient real-time deployment. To address this, we propose anovel parallel, region-adaptive pixelization framework that combines thetheoretical rigor of differential privacy with practical efficiency. Our methodadaptively adjusts grid sizes and noise scales based on regional complexity,leveraging GPU parallelism to achieve significant runtime acceleration comparedto the classical baseline. A lightweight storage scheme is introduced byretaining only essential noisy statistics, significantly reducing spaceoverhead. Formal privacy analysis is provided under the Laplace mechanism andparallel composition theorem. Extensive experiments on the PETS, Venice-2, andPPM-100 datasets demonstrate favorable privacy-utility trade-offs andsignificant runtime/storage reductions. A face re-identification attackexperiment on CelebA further confirms the method's effectiveness in preventingidentity inference. This validates its suitability for real-timeprivacy-critical applications such as elderly care, smart home monitoring,driver behavior analysis, and crowd behavior monitoring.</description>
      <author>example@mail.com (Ming Liu)</author>
      <guid isPermaLink="false">2511.04261v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection</title>
      <link>http://arxiv.org/abs/2511.04255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文重新审视了以人为中心的基础模型在医学影像解剖标志检测中的应用，提出了MedSapiens模型，在多个数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;解剖标志检测传统上依赖特定领域模型，而大规模预训练视觉模型的出现提供了新机会。以人为中心的基础模型具有空间姿态定位的优化潜力，但这一潜力尚未被充分利用。&lt;h4&gt;目的&lt;/h4&gt;研究将Sapiens（一种为姿态估计设计的以人为中心的基础模型）通过多数据集预训练适应到医学影像中，建立新的最先进性能。&lt;h4&gt;方法&lt;/h4&gt;通过多数据集预训练将Sapiens模型适应到医学影像领域，提出MedSapiens模型，并在多个数据集和有限数据设置下评估其性能。&lt;h4&gt;主要发现&lt;/h4&gt;MedSapiens在多个数据集上建立了新的最先进性能；相比通用模型，平均成功检测率提高5.26%；相比专业模型提高21.81%；在有限数据设置下，相比少样本最先进方法提高2.69%。&lt;h4&gt;结论&lt;/h4&gt;以人为中心的基础模型因其内在的空间姿态定位优化，为解剖标志检测提供了强大的先验知识，这一潜力可以通过适当的方法得到充分利用。&lt;h4&gt;翻译&lt;/h4&gt;本文并未引入新颖的架构；相反，它重新审视了一个基本但被忽视的基线：将以人为中心的基础模型适应用于医学影像中的解剖标志检测。虽然标志检测传统上依赖于特定领域的模型，但大规模预训练视觉模型的出现提供了新的机会。在本研究中，我们通过多数据集预训练研究将Sapiens（一种为姿态估计设计的以人为中心的基础模型）适应到医学影像中，在多个数据集上建立了新的最先进性能。我们提出的MedSapiens模型证明，以人为中心的基础模型，因其内在的空间姿态定位优化，为解剖标志检测提供了强大的先验知识，但这一潜力在很大程度上尚未被利用。我们将MedSapiens与现有最先进模型进行基准测试，在平均成功检测率上相比通用模型提高5.26%，相比专业模型提高21.81%。为了进一步评估MedSapiens在标注数据有限的情况下对新下游任务的适应能力，我们在有限数据设置下评估了其性能，在SDR上相比少样本最先进方法提高2.69%。代码和模型权重可在https://github.com/xmed-lab/MedSapiens获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper does not introduce a novel architecture; instead, it revisits afundamental yet overlooked baseline: adapting human-centric foundation modelsfor anatomical landmark detection in medical imaging. While landmark detectionhas traditionally relied on domain-specific models, the emergence oflarge-scale pre-trained vision models presents new opportunities. In thisstudy, we investigate the adaptation of Sapiens, a human-centric foundationmodel designed for pose estimation, to medical imaging through multi-datasetpretraining, establishing a new state of the art across multiple datasets. Ourproposed model, MedSapiens, demonstrates that human-centric foundation models,inherently optimized for spatial pose localization, provide strong priors foranatomical landmark detection, yet this potential has remained largelyuntapped. We benchmark MedSapiens against existing state-of-the-art models,achieving up to 5.26% improvement over generalist models and up to 21.81%improvement over specialist models in the average success detection rate (SDR).To further assess MedSapiens adaptability to novel downstream tasks with fewannotations, we evaluate its performance in limited-data settings, achieving2.69% improvement over the few-shot state of the art in SDR. Code and modelweights are available at https://github.com/xmed-lab/MedSapiens .</description>
      <author>example@mail.com (Marawan Elbatel, Anbang Wang, Keyuan Liu, Kaouther Mouheb, Enrique Almar-Munoz, Lizhuo Lin, Yanqi Yang, Karim Lekadir, Xiaomeng Li)</author>
      <guid isPermaLink="false">2511.04255v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2511.04131v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了BFM-Zero框架，一种用于类人机器人的行为基础模型，通过学习共享潜在表示将运动、目标和奖励嵌入公共空间，实现单一策略支持多种下游任务，无需重新训练。&lt;h4&gt;背景&lt;/h4&gt;现有方法要么只在模拟环境中部署，要么专门针对特定任务如跟踪，缺乏统一的多任务控制能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在真实世界中实现多样化且稳健整体技能的行为基础模型框架，支持零样本运动跟踪、目标到达和奖励优化等多种推理方法。&lt;h4&gt;方法&lt;/h4&gt;基于无监督强化学习和前向-后向(FB)模型构建，结合奖励塑造、领域随机化和历史依赖的非对称学习来弥合模拟到现实的差距。&lt;h4&gt;主要发现&lt;/h4&gt;BFM-Zero在Unitree G1类人机器人上实现了多样化且稳健的整体技能，通过结构良好的潜在空间支持多种推理方法，且关键设计选择在模拟中得到了定量验证。&lt;h4&gt;结论&lt;/h4&gt;BFM-Zero作为首创模型，为可扩展、可提示的行为基础模型用于整体类人控制奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;为类人机器人构建行为基础模型(BFMs)有潜力将多样化的控制任务统一在单个可提示的通用策略下。然而，现有方法要么仅在模拟的类人角色上部署，要么专门针对特定任务如跟踪。我们提出了BFM-Zero框架，它学习了一个有效的共享潜在表示，将运动、目标和奖励嵌入公共空间，使单一策略能够被提示执行多个下游任务而无需重新训练。BFM-Zero中的这种结构良好的潜在空间使得在真实世界的Unitree G1类人机器人上的多样化且稳健的整体技能成为可能，通过多样化的推理方法，包括零样本运动跟踪、目标到达和奖励优化，以及少样本基于优化的自适应。与先前的在线强化学习(RL)框架不同，BFM-Zero建立在无监督RL和前向-后向(FB)模型的最新进展之上，这些模型为中心目标、可解释和流畅的整体运动潜在表示提供了支持。我们进一步通过关键的奖励塑造、领域随机化和历史依赖的非对称学习扩展了BFM-Zero，以弥合模拟到现实的差距。这些关键设计选择在模拟中进行了定量消融实验。作为首创模型，BFM-Zero为可扩展、可提示的行为基础模型用于整体类人控制迈出了第一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building Behavioral Foundation Models (BFMs) for humanoid robots has thepotential to unify diverse control tasks under a single, promptable generalistpolicy. However, existing approaches are either exclusively deployed onsimulated humanoid characters, or specialized to specific tasks such astracking. We propose BFM-Zero, a framework that learns an effective sharedlatent representation that embeds motions, goals, and rewards into a commonspace, enabling a single policy to be prompted for multiple downstream taskswithout retraining. This well-structured latent space in BFM-Zero enablesversatile and robust whole-body skills on a Unitree G1 humanoid in the realworld, via diverse inference methods, including zero-shot motion tracking, goalreaching, and reward optimization, and few-shot optimization-based adaptation.Unlike prior on-policy reinforcement learning (RL) frameworks, BFM-Zero buildsupon recent advancements in unsupervised RL and Forward-Backward (FB) models,which offer an objective-centric, explainable, and smooth latent representationof whole-body motions. We further extend BFM-Zero with critical reward shaping,domain randomization, and history-dependent asymmetric learning to bridge thesim-to-real gap. Those key design choices are quantitatively ablated insimulation. A first-of-its-kind model, BFM-Zero establishes a step towardscalable, promptable behavioral foundation models for whole-body humanoidcontrol.</description>
      <author>example@mail.com (Yitang Li, Zhengyi Luo, Tonghe Zhang, Cunxi Dai, Anssi Kanervisto, Andrea Tirinzoni, Haoyang Weng, Kris Kitani, Mateusz Guzek, Ahmed Touati, Alessandro Lazaric, Matteo Pirotta, Guanya Shi)</author>
      <guid isPermaLink="false">2511.04131v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks</title>
      <link>http://arxiv.org/abs/2511.04115v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 tables, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了英语语言水平对大型语言模型生成代码质量和正确性的影响，发现自然语言能力是控制代码生成的关键因素。&lt;h4&gt;背景&lt;/h4&gt;随着基础模型驱动的工具在软件工程中的广泛采用，自然语言提示已成为开发人员与大型语言模型之间的重要接口，但自然语言能力对代码生成质量的影响研究不足。&lt;h4&gt;目的&lt;/h4&gt;调查英语语言能力本身（独立于提示技术）是否会影响大型语言模型生成代码的能力和正确性。&lt;h4&gt;方法&lt;/h4&gt;使用HumanEval数据集，对164个编程任务的提示英语水平从基础到高级进行系统变化，并测量生成的代码水平和正确性。&lt;h4&gt;主要发现&lt;/h4&gt;大型语言模型默认使用中级（B2）自然语言水平；对生成代码水平的影响因模型而异；在所有模型中，更高水平的提示始终产生更正确的代码。&lt;h4&gt;结论&lt;/h4&gt;自然语言能力是控制代码生成的关键杠杆，可以帮助开发人员定制AI输出并提高解决方案的可靠性。&lt;h4&gt;翻译&lt;/h4&gt;随着基础模型驱动的工具在软件工程中的广泛采用，自然语言提示已成为开发人员与大型语言模型之间的重要接口。虽然许多研究关注提示结构，但自然语言能力这一影响生成代码质量的因素却很少被探索。本文研究了英语语言能力本身（独立于提示技术）是否会影响大型语言模型生成代码的能力和正确性。使用HumanEval数据集，我们对164个编程任务的提示英语水平从基础到高级进行了系统变化，并测量了生成的代码水平和正确性。我们的发现表明，大型语言模型默认使用中级（B2）自然语言水平。虽然对生成代码水平的影响因模型而异，但我们发现所有模型中更高水平的提示始终产生更正确的代码。这些结果表明，自然语言能力是控制代码生成的关键杠杆，可以帮助开发人员定制AI输出并提高解决方案的可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/MS.2025.3622690&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the widespread adoption of Foundation Model (FM)-powered tools insoftware engineering, the natural language prompt has become a criticalinterface between developers and Large Language Models (LLMs). While muchresearch has focused on prompt structure, the natural language proficiency isan underexplored factor that can influence the quality of generated code. Thispaper investigates whether the English language proficiency itself independentof the prompting technique affects the proficiency and correctness of codegenerated by LLMs. Using the HumanEval dataset, we systematically varied theEnglish proficiency of prompts from basic to advanced for 164 programming tasksand measured the resulting code proficiency and correctness. Our findings showthat LLMs default to an intermediate (B2) natural language level. While theeffect on the resulting code proficiency was model-dependent, we found thathigher-proficiency prompts consistently yielded more correct code across allmodels. These results demonstrate that natural language proficiency is a keylever for controlling code generation, helping developers tailor AI output andimprove the reliability of solutions.</description>
      <author>example@mail.com (Ruksit Rojpaisarnkit, Youmei Fan, Kenichi Matsumoto, Raula Gaikovina Kula)</author>
      <guid isPermaLink="false">2511.04115v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Tiny-WiFo: A Lightweight Wireless Foundation Model for Channel Prediction via Multi-Component Adaptive Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2511.04015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多组件自适应知识蒸馏框架，通过选择性知识提取和学习策略平衡，实现了无线基础模型在边缘设备上的高效部署。&lt;h4&gt;背景&lt;/h4&gt;无线基础模型规模巨大，阻碍了它们在边缘设备上的实时部署。&lt;h4&gt;目的&lt;/h4&gt;超越标准知识蒸馏方法，提出新的知识蒸馏框架以实现模型的高效压缩与边缘部署。&lt;h4&gt;方法&lt;/h4&gt;引入基于交叉注意力的知识选择模块从教师模型中选择性识别关键特征，以及自主学习-被动学习策略平衡知识转移与独立学习，在可管理计算成本下实现高训练效率。&lt;h4&gt;主要发现&lt;/h4&gt;应用于WiFo FM时，蒸馏出的Tiny-WiFo模型仅有550万个参数，在边缘硬件上实现1.6毫秒推理时间，同时保留了WiFo超过98%的性能及其关键的零样本泛化能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法使得无线基础模型在边缘设备上的实时部署成为可能。&lt;h4&gt;翻译&lt;/h4&gt;无线基础模型的巨大规模阻碍了它们在边缘设备上的实时部署。本文通过引入一种新的多组件自适应知识蒸馏框架，超越了标准知识蒸馏。关键创新包括一个基于交叉注意力的知识选择模块，它从教师模型中选择性地识别关键特征，以及一个自主学习-被动学习策略，该策略平衡知识转移与独立学习，以在可管理的计算成本下实现高训练效率。当应用于WiFo FM时，蒸馏出的Tiny-WiFo模型仅有550万个参数，在边缘硬件上实现1.6毫秒的推理时间，同时保留了WiFo超过98%的性能及其关键的零样本泛化能力，使得实时FM部署成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The massive scale of Wireless Foundation Models (FMs) hinders their real-timedeployment on edge devices. This letter moves beyond standard knowledgedistillation by introducing a novel Multi-Component Adaptive KnowledgeDistillation (MCAKD) framework. Key innovations include a Cross-Attention-BasedKnowledge Selection (CA-KS) module that selectively identifies criticalfeatures from the teacher model, and an Autonomous Learning-Passive Learning(AL-PL) strategy that balances knowledge transfer with independent learning toachieve high training efficiency at a manageable computational cost. Whenapplied to the WiFo FM, the distilled Tiny-WiFo model, with only 5.5Mparameters, achieves a 1.6 ms inference time on edge hardware while retainingover 98% of WiFo's performance and its crucial zero-shot generalizationcapability, making real-time FM deployment viable.</description>
      <author>example@mail.com (Haotian Zhang, Shijian Gao, Xiang Cheng)</author>
      <guid isPermaLink="false">2511.04015v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>SynQuE: Estimating Synthetic Dataset Quality Without Annotations</title>
      <link>http://arxiv.org/abs/2511.03928v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;作者引入并形式化了合成数据集质量评估(SynQuE)问题，建立了首个全面基准，提出了代理指标LENS，并通过实验证明SynQuE代理能有效选择合成数据提高任务性能。&lt;h4&gt;背景&lt;/h4&gt;由于收集成本或隐私限制导致数据稀缺，这是一个关键且开放的挑战。&lt;h4&gt;目的&lt;/h4&gt;建立SynQuE问题的首个全面基准，通过引入和评估代理指标来选择合成数据以最大化在真实数据上的任务性能。&lt;h4&gt;方法&lt;/h4&gt;引入首个SynQuE代理指标，通过嵌入模型调整基于分布和多样性的距离度量；提出LENS，一个利用大语言模型推理的新代理，以解决这些指标在复杂规划任务上的不足。&lt;h4&gt;主要发现&lt;/h4&gt;SynQuE代理与多种任务（情感分析、Text2SQL、网页导航和图像分类）的真实任务性能相关；LENS在复杂任务上始终优于其他代理；在文本到SQL解析任务中，通过SynQuE代理选择的前3个合成数据集训练，平均准确率提高了8.1%。&lt;h4&gt;结论&lt;/h4&gt;这项工作建立了SynQuE作为真实数据稀缺下合成数据选择的实用框架，并激励了基于基础模型的数据表征和细粒度数据选择的未来研究。&lt;h4&gt;翻译&lt;/h4&gt;我们引入并形式化了合成数据集质量评估(SynQuE)问题：仅使用有限的无标注真实数据，按预期真实世界任务性能对合成数据集进行排序。这解决了一个关键且开放的挑战，由于收集成本或隐私限制导致数据稀缺。我们通过引入和评估选择合成数据用于训练以最大化在真实数据上任务性能的代理指标，为这个问题建立了首个全面基准。我们通过嵌入模型调整基于分布和多样性的距离度量，引入了首个SynQuE代理指标。为了解决这些指标在复杂规划任务上的不足，我们提出了LENS，一个利用大语言模型推理的新颖代理。我们的结果显示，SynQuE代理与多种任务（包括情感分析、Text2SQL、网页导航和图像分类）的真实任务性能相关，LENS通过捕捉细微特征在复杂任务上始终优于其他代理。例如，在文本到SQL解析任务中，通过SynQuE代理选择的前3个合成数据集训练，平均准确率从不加选择地选择数据的30.4%提高到38.4%（+8.1%）。这项工作建立了SynQuE作为真实数据稀缺下合成数据选择的实用框架，并激励了基于基础模型的数据表征和细粒度数据选择的未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce and formalize the Synthetic Dataset Quality Estimation (SynQuE)problem: ranking synthetic datasets by their expected real-world taskperformance using only limited unannotated real data. This addresses a criticaland open challenge where data is scarce due to collection costs or privacyconstraints. We establish the first comprehensive benchmarks for this problemby introducing and evaluating proxy metrics that choose synthetic data fortraining to maximize task performance on real data. We introduce the firstproxy metrics for SynQuE by adapting distribution and diversity-based distancemeasures to our context via embedding models. To address the shortcomings ofthese metrics on complex planning tasks, we propose LENS, a novel proxy thatleverages large language model reasoning. Our results show that SynQuE proxiescorrelate with real task performance across diverse tasks, including sentimentanalysis, Text2SQL, web navigation, and image classification, with LENSconsistently outperforming others on complex tasks by capturing nuancedcharacteristics. For instance, on text-to-SQL parsing, training on the top-3synthetic datasets selected via SynQuE proxies can raise accuracy from 30.4% to38.4 (+8.1)% on average compared to selecting data indiscriminately. This workestablishes SynQuE as a practical framework for synthetic data selection underreal-data scarcity and motivates future research on foundation model-based datacharacterization and fine-grained data selection.</description>
      <author>example@mail.com (Arthur Chen, Victor Zhong)</author>
      <guid isPermaLink="false">2511.03928v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>PLLuM: A Family of Polish Large Language Models</title>
      <link>http://arxiv.org/abs/2511.03823v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  83 pages, 19 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PLLuM是最大的开源波兰语基础模型家族，由波兰主要研究机构联盟开发，旨在提供高质量、透明且文化相关的波兰语语言模型，以应对英语主导的商业语言模型格局。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在人工智能中扮演核心角色，但其发展主要集中于英语，导致对其他语言的支持有限。&lt;h4&gt;目的&lt;/h4&gt;开发专门针对波兰语言的高质量、透明和文化相关的基础模型，满足非英语语言需求，并促进开放研究和加强波兰的主权AI技术。&lt;h4&gt;方法&lt;/h4&gt;由波兰主要研究机构联盟开发；构建了1400亿个波兰语文本语料库进行预训练；创建了77k的自定义指令数据集和100k的偏好优化数据集；采用负责任的AI框架，包括严格的数据治理和用于输出校正与安全过滤的混合模块；详细说明了模型的架构、训练过程和对齐技术。&lt;h4&gt;主要发现&lt;/h4&gt;PLLuM模型在公共管理领域的下游任务中展示了其实用性，证明了专门针对特定语言开发的大型语言模型的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过公开发布PLLuM模型，旨在促进开放研究并加强波兰的主权AI技术发展。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在现代人工智能中扮演核心角色，但其开发主要集中于英语，导致对其他语言的支持有限。我们提出了PLLuM（波兰大型语言模型），这是专门为波兰语言定制的最大开源基础模型家族。由波兰主要研究机构联盟开发，PLLuM满足了在英语主导的商业格局之外，对高质量、透明且与文化相关的语言模型的需求。我们描述了开发过程，包括为预训练构建新的1400亿个波兰语文本语料库、77k的自定义指令数据集和100k的偏好优化数据集。关键组成部分是一个负责任的AI框架，它采用严格的数据治理和用于输出校正与安全过滤的混合模块。我们详细说明了基础模型和指令调整变体的架构、训练过程和对齐技术，并在公共管理领域的下游任务中展示了它们的实用性。通过公开发布这些模型，PLLuM旨在促进开放研究并加强波兰的主权AI技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) play a central role in modern artificialintelligence, yet their development has been primarily focused on English,resulting in limited support for other languages. We present PLLuM (PolishLarge Language Model), the largest open-source family of foundation modelstailored specifically for the Polish language. Developed by a consortium ofmajor Polish research institutions, PLLuM addresses the need for high-quality,transparent, and culturally relevant language models beyond the English-centriccommercial landscape. We describe the development process, including theconstruction of a new 140-billion-token Polish text corpus for pre-training, a77k custom instructions dataset, and a 100k preference optimization dataset. Akey component is a Responsible AI framework that incorporates strict datagovernance and a hybrid module for output correction and safety filtering. Wedetail the models' architecture, training procedures, and alignment techniquesfor both base and instruction-tuned variants, and demonstrate their utility ina downstream task within public administration. By releasing these modelspublicly, PLLuM aims to foster open research and strengthen sovereign AItechnologies in Poland.</description>
      <author>example@mail.com (Jan Kocoń, Maciej Piasecki, Arkadiusz Janz, Teddy Ferdinan, Łukasz Radliński, Bartłomiej Koptyra, Marcin Oleksy, Stanisław Woźniak, Paweł Walkowiak, Konrad Wojtasik, Julia Moska, Tomasz Naskręt, Bartosz Walkowiak, Mateusz Gniewkowski, Kamil Szyc, Dawid Motyka, Dawid Banach, Jonatan Dalasiński, Ewa Rudnicka, Bartłomiej Alberski, Tomasz Walkowiak, Aleksander Szczęsny, Maciej Markiewicz, Tomasz Bernaś, Hubert Mazur, Kamil Żyta, Mateusz Tykierko, Grzegorz Chodak, Tomasz Kajdanowicz, Przemysław Kazienko, Agnieszka Karlińska, Karolina Seweryn, Anna Kołos, Maciej Chrabąszcz, Katarzyna Lorenc, Aleksandra Krasnodębska, Artur Wilczek, Katarzyna Dziewulska, Paula Betscher, Zofia Cieślińska, Katarzyna Kowol, Daria Mikoś, Maciej Trzciński, Dawid Krutul, Marek Kozłowski, Sławomir Dadas, Rafał Poświata, Michał Perełkiewicz, Małgorzata Grębowiec, Maciej Kazuła, Marcin Białas, Roman Roszko, Danuta Roszko, Jurgita Vaičenonienė, Andrius Utka, Paweł Levchuk, Paweł Kowalski, Irena Prawdzic-Jankowska, Maciej Ogrodniczuk, Monika Borys, Anna Bulińska, Wiktoria Gumienna, Witold Kieraś, Dorota Komosińska, Katarzyna Krasnowska-Kieraś, Łukasz Kobyliński, Martyna Lewandowska, Marek Łaziński, Mikołaj Łątkowski, Dawid Mastalerz, Beata Milewicz, Agnieszka Anna Mykowiecka, Angelika Peljak-Łapińska, Sandra Penno, Zuzanna Przybysz, Michał Rudolf, Piotr Rybak, Karolina Saputa, Aleksandra Tomaszewska, Aleksander Wawer, Marcin Woliński, Joanna Wołoszyn, Alina Wróblewska, Bartosz Żuk, Filip Żarnecki, Konrad Kaczyński, Anna Cichosz, Zuzanna Deckert, Monika Garnys, Izabela Grabarczyk, Wojciech Janowski, Sylwia Karasińska, Aleksandra Kujawiak, Piotr Misztela, Maria Szymańska, Karolina Walkusz, Igor Siek, Jakub Kwiatkowski, Piotr Pęzik)</author>
      <guid isPermaLink="false">2511.03823v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features</title>
      <link>http://arxiv.org/abs/2511.03806v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FusionDP是一个两步框架，用于在特征级别差分隐私下增强模型效用，通过利用大型基础模型估算敏感特征并修改DP-SGD算法，在保持隐私的同时提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;在隐私保护机器学习中，确保敏感训练数据的隐私至关重要，但实际场景中可能只需要对部分特征进行隐私保护，例如ICU数据中的人口统计属性比原始实验室结果更敏感。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，能够在特征级别差分隐私下增强模型效用，避免传统DP-SGD方法对所有特征强制执行隐私保护导致的过度噪声注入和效用下降问题。&lt;h4&gt;方法&lt;/h4&gt;FusionDP采用两步框架：首先利用大型基础模型根据非敏感特征估算敏感特征，作为外部先验提供高质量估计；其次引入修改的DP-SGD算法，在原始和估算特征上同时训练模型，同时保留原始敏感特征的隐私。&lt;h4&gt;主要发现&lt;/h4&gt;在PhysioNet的败血症预测任务和MIMIC-III的临床笔记分类任务上评估显示，FusionDP与隐私保护基线相比，在保持严格特征级别隐私的同时显著提高了模型性能。&lt;h4&gt;结论&lt;/h4&gt;基础模型驱动的估算能够增强各种模态的隐私-效用权衡，为隐私保护机器学习提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;确保敏感训练数据的隐私在隐私保护机器学习中至关重要。然而，在实际场景中，可能只需要对部分特征进行隐私保护。例如，在ICU数据中，年龄和性别等人口统计属性由于重新识别的潜在风险而具有更高的隐私风险，而原始实验室结果通常敏感性较低。传统的DP-SGD对样本中的所有特征强制执行隐私保护，导致过度噪声注入和显著的效用下降。我们提出了FusionDP，一个两步框架，用于在特征级别差分隐私下增强模型效用。首先，FusionDP利用大型基础模型根据非敏感特征估算敏感特征，将它们作为外部先验，在模型训练期间不访问真实值的情况下提供高质量敏感属性估计。其次，我们引入了修改的DP-SGD算法，在原始和估算特征上同时训练模型，同时正式保留原始敏感特征的隐私。我们在两种模态上评估了FusionDP：来自PhysioNet的表格数据上的败血症预测任务和来自MIMIC-III的临床笔记分类任务。通过与隐私保护基线进行比较，我们的结果显示FusionDP在保持严格特征级别隐私的同时显著提高了模型性能，展示了基础模型驱动的估算在增强各种模态的隐私-效用权衡方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring the privacy of sensitive training data is crucial inprivacy-preserving machine learning. However, in practical scenarios, privacyprotection may be required for only a subset of features. For instance, in ICUdata, demographic attributes like age and gender pose higher privacy risks dueto their re-identification potential, whereas raw lab results are generallyless sensitive. Traditional DP-SGD enforces privacy protection on all featuresin one sample, leading to excessive noise injection and significant utilitydegradation. We propose FusionDP, a two-step framework that enhances modelutility under feature-level differential privacy. First, FusionDP leverageslarge foundation models to impute sensitive features given non-sensitivefeatures, treating them as external priors that provide high-quality estimatesof sensitive attributes without accessing the true values during modeltraining. Second, we introduce a modified DP-SGD algorithm that trains modelson both original and imputed features while formally preserving the privacy ofthe original sensitive features. We evaluate FusionDP on two modalities: asepsis prediction task on tabular data from PhysioNet and a clinical noteclassification task from MIMIC-III. By comparing against privacy-preservingbaselines, our results show that FusionDP significantly improves modelperformance while maintaining rigorous feature-level privacy, demonstrating thepotential of foundation model-driven imputation to enhance the privacy-utilitytrade-off for various modalities.</description>
      <author>example@mail.com (Linghui Zeng, Ruixuan Liu, Atiquer Rahman Sarkar, Xiaoqian Jiang, Joyce C. Ho, Li Xiong)</author>
      <guid isPermaLink="false">2511.03806v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Transformer-Based Modeling of Nonlinear Pulse Evolution in Er-Doped Fiber Amplifiers</title>
      <link>http://arxiv.org/abs/2511.04057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种基于Transformer架构的神经网络模型，用于预测掺铒光纤放大器中光脉冲的非线性演化。通过两阶段训练策略（先在合成数据上预训练，再用少量实验数据微调），成功解决了数据稀缺问题，能够准确重现实验中观察到的光脉冲精细光谱结构。&lt;h4&gt;背景&lt;/h4&gt;在实验数据有限的情况下预测掺铒光纤放大器中光脉冲的非线性演化是一个挑战。传统的数值模拟方法可能难以准确捕捉实验中观察到的精细光谱结构，而纯粹的机器学习方法又面临数据稀缺的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测掺铒光纤放大器中光脉冲非线性演化的神经网络模型，特别是在实验数据有限的情况下，能够准确重现实验中观察到的光脉冲精细光谱结构。&lt;h4&gt;方法&lt;/h4&gt;采用基于Transformer架构的神经网络模型，并实施两阶段训练策略：第一阶段在通过数值模拟生成的合成数据集上预训练模型；第二阶段使用少量实验测量数据对模型进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够准确重现实验中观察到的光脉冲的精细光谱结构，在各种非线性演化情况下表现良好，包括调制不稳定性的发展和高阶孤子的传播。&lt;h4&gt;结论&lt;/h4&gt;结合物理模型（通过数值模拟生成合成数据）和机器学习（神经网络模型）的两阶段训练策略，可以有效解决数据稀缺问题，实现对光脉冲非线性演化的准确预测。&lt;h4&gt;翻译&lt;/h4&gt;基于Transformer架构的神经网络模型已经开发出来，用于在实验数据有限的情况下预测掺铒光纤放大器中光脉冲的非线性演化。为了解决数据稀缺问题，采用了两阶段训练策略。在第一阶段，模型在通过放大器非线性动力学的数值模拟生成的合成数据集上进行预训练。在第二阶段，使用少量实验测量数据对模型进行微调。这种方法能够准确重现实验中观察到的各种非线性演化情况下的光脉冲精细光谱结构，包括调制不稳定性的发展和高阶孤子的传播。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A neural network model based on the Transformer architecture has beendeveloped to predict the nonlinear evolution of optical pulses in Er-dopedfiber amplifier under conditions of limited experimental data. To address datascarcity, a two-stage training strategy is employed. In the first stage, themodel is pretrained on a synthetic dataset generated through numericalsimulations of the amplifier's nonlinear dynamics. In the second stage, themodel is fine-tuned using a small set of experimental measurements. Thisapproach enables accurate reproduction of the fine spectral structure ofoptical pulses observed in experiments across various nonlinear evolutionregimes, including the development of modulational instability and thepropagation of high-order solitons.</description>
      <author>example@mail.com (Anastasia Bednyakova, Artem Gemuzov, Mikhail Mishevsky, Karina Saraeva, Alexey Redyuk, Aram Mkrtchyan, Albert Nasibulin, Yuriy Gladush)</author>
      <guid isPermaLink="false">2511.04057v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Shared Spatial Memory Through Predictive Coding</title>
      <link>http://arxiv.org/abs/2511.04235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  We have prepared the open-source code and video demonstration pages:  1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种多智能体预测编码框架，将协调视为智能体间相互不确定性的最小化。该框架通过信息瓶颈目标促使智能体学习何时以及与谁通信什么内容，并采用类似网格细胞的度量作为内部空间编码。基于此，智能体发展出带宽高效的通信机制和专门编码伙伴位置的神经群体，通过分层强化学习策略主动探索以减少联合不确定性。&lt;h4&gt;背景&lt;/h4&gt;在多智能体系统中，共享和重建一致的空间记忆是一个关键挑战。部分可观测性和有限带宽常常导致协调中的灾难性故障。&lt;h4&gt;目的&lt;/h4&gt;开发一种多智能体预测编码框架，将协调表述为智能体间相互不确定性的最小化，并解决带宽限制下的协调问题。&lt;h4&gt;方法&lt;/h4&gt;1. 引入多智能体预测编码框架，将协调表述为最小化智能体间的不确定性；2. 使用信息瓶颈目标促使智能体学习何时以及与谁通信什么内容；3. 采用类似网格细胞的度量作为内部空间编码用于自我定位；4. 通过自我监督的运动预测形成内部空间编码；5. 基于内部空间编码发展带宽高效的通信机制；6. 开发专门编码伙伴位置的神经群体；7. 使用分层强化学习策略主动探索以减少联合不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;在Memory-Maze基准测试中，该方法对带宽限制表现出极强的鲁棒性：当带宽从128位/步缩减到4位/步时，成功率从73.5%逐渐下降到64.4%；而全广播基线方法则从67.6%急剧下降到28.6%。&lt;h4&gt;结论&lt;/h4&gt;研究结果为复杂社交表征如何从统一的预测驱动中涌现提供了理论上合理且生物学 plausible 的基础，从而形成社交集体智能。&lt;h4&gt;翻译&lt;/h4&gt;共享和重建一致的空间记忆是多智能体系统中的一个关键挑战，其中部分可观测性和有限带宽常常导致协调中的灾难性故障。我们引入了一种多智能体预测编码框架，将协调表述为智能体间相互不确定性的最小化。作为信息瓶颈目标的实例化，它促使智能体学习不仅与谁通信什么内容，还有何时通信。该框架的基础是一种类似网格细胞的度量，作为自我定位的内部空间编码，通过自我监督的运动预测自发形成。基于这种内部空间编码，智能体逐渐发展出带宽高效的通信机制和专门编码伙伴位置的神经群体：海马体社交位置细胞的一种人工模拟。这些社交表示通过分层强化学习策略进一步实现，该策略主动探索以减少联合不确定性。在Memory-Maze基准测试中，我们的方法对带宽限制表现出极强的鲁棒性：当带宽从128位/步缩减到4位/步时，成功率从73.5%逐渐下降到64.4%，而全广播基线方法则从67.6%急剧下降到28.6%。我们的研究结果为复杂社交表征如何从统一的预测驱动中涌现提供了理论上合理且生物学 plausible 的基础，从而形成社交集体智能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多智能体系统中的共享空间记忆问题，特别是在通信带宽受限的情况下如何实现有效的协调。这个问题在现实中非常重要，因为多智能体系统在探索、搜索和救援等领域具有广泛应用，但通信带宽往往是有限的资源。在生物学上，理解多智能体如何共享空间记忆有助于理解生物群体的协调机制。在技术层面，解决'通信瓶颈'问题对于构建高效的多智能体系统至关重要，避免因通信限制导致的协调失败。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过整合生物学启发、预测编码理论和信息论原理设计该方法。首先借鉴了生物学中的海马体-内嗅系统和社会位置细胞的发现，然后结合预测编码理论（大脑通过预测感官输入并最小化误差处理信息）和信息瓶颈理论（在压缩率和预测效用间权衡）。作者设计了三层框架：个体感知（网格细胞网络）、社会通信（信息瓶颈目标）和战略探索（分层强化学习）。该方法借鉴了多智能体强化学习的MAPPO算法，并受到大型语言模型'下一个token预测'目标的启发，将预测学习作为构建共享世界模型的通用机制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过最小化智能体之间的相互预测不确定性，使多智能体系统能自发形成高效的空间记忆共享机制。整体实现流程分为三步：1) 个体层面，每个智能体使用网格细胞网络（路径积分）和视觉预测编码（BEV地图生成）构建内部空间模型；2) 社会层面，通过信息瓶颈目标学习传输压缩的离散符号，在关键时刻减少伙伴的不确定性；3) 战略层面，使用分层强化学习框架（HRL-ICM），基于预测不确定性做出决策，协调探索以找到隐藏目标。这种方法使智能体在极低带宽条件下（4-128位/步）保持高效协调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一预测框架，通过三个层次实现共享空间记忆；2) 自发形成的网格细胞样表征，用于自我定位；3) 基于信息瓶颈的高效社会通信机制，形成有意义的符号词汇表；4) 涌现的社会位置细胞，编码伙伴位置；5) 分层强化学习框架，基于预测不确定性做决策。相比之前工作，本文方法在带宽受限条件下表现出色（带宽从128降到4位/步时，成功率仅从73.5%降到64.4%，而全广播基线从67.6%降到28.6%），且表征是自发涌现而非预设的，为集体智能提供了生物学合理的解释。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过基于预测编码的多智能体框架，展示了智能体如何在有限带宽条件下自发形成高效的空间记忆共享机制，包括网格细胞样的空间度量、带宽优化的社会通信和社会位置细胞表征，为理解生物集体智能和设计高效的人工多智能体系统提供了理论基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sharing and reconstructing a consistent spatial memory is a criticalchallenge in multi-agent systems, where partial observability and limitedbandwidth often lead to catastrophic failures in coordination. We introduce amulti-agent predictive coding framework that formulate coordination as theminimization of mutual uncertainty among agents. Instantiated as an informationbottleneck objective, it prompts agents to learn not only who and what tocommunicate but also when. At the foundation of this framework lies agrid-cell-like metric as internal spatial coding for self-localization,emerging spontaneously from self-supervised motion prediction. Building uponthis internal spatial code, agents gradually develop a bandwidth-efficientcommunication mechanism and specialized neural populations that encodepartners' locations: an artificial analogue of hippocampal social place cells(SPCs). These social representations are further enacted by a hierarchicalreinforcement learning policy that actively explores to reduce jointuncertainty. On the Memory-Maze benchmark, our approach shows exceptionalresilience to bandwidth constraints: success degrades gracefully from 73.5% to64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcastbaseline collapses from 67.6% to 28.6%. Our findings establish a theoreticallyprincipled and biologically plausible basis for how complex socialrepresentations emerge from a unified predictive drive, leading to socialcollective intelligence.</description>
      <author>example@mail.com (Zhengru Fang, Yu Guo, Jingjing Wang, Yuang Zhang, Haonan An, Yinhai Wang, Yuguang Fang)</author>
      <guid isPermaLink="false">2511.04235v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</title>
      <link>http://arxiv.org/abs/2511.04668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://ellisbrown.github.io/sims-v&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了多模态语言模型在空间推理方面的挑战，通过创新的模拟数据生成框架SIMS-V，有效提高了模型在真实世界空间任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态语言模型在高级视频理解方面表现出色，但在跨时间和空间的空间推理方面存在困难。当前的空间训练方法依赖于真实世界的视频数据，但获取具有精确空间标注的多样化素材仍然是一个瓶颈。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一瓶颈，作者提出了SIMS-V框架，这是一个系统性的数据生成框架，利用3D模拟器的特权信息来创建空间丰富的视频训练数据，用于多模态语言模型。&lt;h4&gt;方法&lt;/h4&gt;作者使用这个框架，通过系统性地消融问题类型、混合和规模，研究了哪些模拟数据的特性能够有效促进真实世界的迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;识别出三个最有效的问题类别（度量测量、视角依赖推理和时间跟踪），它们对于开发可迁移的空间智能最为有效；尽管使用的问类型较少，但这三个类别比全面覆盖更有效；仅使用25K个模拟示例进行微调的70亿参数视频LLM，性能超过了更大的720亿基线模型；在严格的真实世界空间推理基准测试中，与专有模型实现了竞争性性能。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了强大的泛化能力，在保持一般视频理解性能的同时，在具身和真实世界空间任务上显示出显著改进。&lt;h4&gt;翻译&lt;/h4&gt;尽管在高级视频理解方面表现出色，多模态语言模型在跨时间和空间的空间推理方面仍然存在困难。虽然当前的空间训练方法依赖于真实世界的视频数据，但获取具有精确空间标注的多样化素材仍然是一个瓶颈。为了缓解这一瓶颈，我们提出了SIMS-V——一个系统性的数据生成框架，它利用3D模拟器的特权信息来创建空间丰富的视频训练数据，用于多模态语言模型。使用这个框架，我们通过系统性地消融问题类型、混合和规模，研究了哪些模拟数据的特性能够有效促进真实世界的迁移学习。我们确定了一个最小的三个问题类别集合（度量测量、视角依赖推理和时间跟踪），它们被证明对于开发可迁移的空间智能最为有效，尽管使用的问类型较少，但比全面覆盖更有效。这些见解实现了高效的训练：仅使用25K个模拟示例进行微调的70亿参数视频LLM，性能超过了更大的720亿基线模型，并在严格的真实世界空间推理基准测试中与专有模型实现了竞争性性能。我们的方法展示了强大的泛化能力，在保持一般视频理解性能的同时，在具身和真实世界空间任务上显示出显著改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态语言模型在空间推理方面的局限性，特别是在跨越时间和空间维度的空间理解能力。这个问题很重要，因为空间推理是人类智能的核心能力，对于理解和交互物理世界至关重要，但获取真实世界视频数据并添加精确的空间标注非常困难和昂贵，成为数据瓶颈，限制了模型在这方面的能力发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到多模态语言模型在空间推理方面的弱点，然后考虑使用3D模拟器作为解决方案，因为模拟器可以提供完美的空间信息和可扩展的数据生成。他们设计了SIMS-V框架，系统化地研究哪些模拟数据属性能有效转移到真实世界。作者借鉴了现有的3D模拟器工作（如AI2-THOR、ProcTHOR）、多模态语言模型架构（如LLaVA）以及空间推理基准测试（如VSI-Bench），并在这些基础上进行了创新和扩展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用3D模拟器的特权信息（完美的空间标注和可控制的环境）生成高质量的、空间丰富的视频训练数据，用于训练多模态语言模型的空间推理能力。整体流程包括：1) 使用AI2-THOR等模拟器生成多样化室内环境和捕获代理导航轨迹；2) 提取两种互补的元数据（观察级数据和全局空间数据）；3) 基于这些元数据程序化生成精确的空间推理问题及答案；4) 使用生成的数据训练模型并评估其在真实世界空间推理任务上的表现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 系统化的模拟数据生成框架SIMS-V；2) 通过系统消融实验识别出三种最有效的问题类别（度量测量、视角相关推理和时间跟踪）；3) 展示了高效的数据利用，仅用数千个示例就能获得强大性能；4) 实现了强大的模拟到真实世界的迁移能力。相比之前工作，SIMS-V更专注于视频中的时空推理而非静态图像，系统研究了哪些模拟数据属性能驱动真实世界迁移，训练效率更高，且在保持一般视频理解能力的同时展示了在具身推理和真实世界场景中的强泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SIMS-V通过系统化的3D模拟数据生成框架和识别的最小有效问题集，实现了高效的空间推理能力训练，使小型模型在真实世界空间推理任务上达到与大型专有模型相当的性能，同时保持强大的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite impressive high-level video comprehension, multimodal language modelsstruggle with spatial reasoning across time and space. While current spatialtraining approaches rely on real-world video data, obtaining diverse footagewith precise spatial annotations remains a bottleneck. To alleviate thisbottleneck, we present SIMS-V -- a systematic data-generation framework thatleverages the privileged information of 3D simulators to create spatially-richvideo training data for multimodal language models. Using this framework, weinvestigate which properties of simulated data drive effective real-worldtransfer through systematic ablations of question types, mixes, and scales. Weidentify a minimal set of three question categories (metric measurement,perspective-dependent reasoning, and temporal tracking) that prove mosteffective for developing transferable spatial intelligence, outperformingcomprehensive coverage despite using fewer question types. These insightsenable highly efficient training: our 7B-parameter video LLM fine-tuned on just25K simulated examples outperforms the larger 72B baseline and achievescompetitive performance with proprietary models on rigorous real-world spatialreasoning benchmarks. Our approach demonstrates robust generalization,maintaining performance on general video understanding while showingsubstantial improvements on embodied and real-world spatial tasks.</description>
      <author>example@mail.com (Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie)</author>
      <guid isPermaLink="false">2511.04668v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment</title>
      <link>http://arxiv.org/abs/2511.04628v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可扩展的、基于流的无参考且无意见感知的视频质量评估模型，通过时间感知的卷积架构直接从退化视频中预测全参考指标。&lt;h4&gt;背景&lt;/h4&gt;视频质量评估对计算机视觉任务至关重要，但现有方法存在局限性：全参考指标需要干净的参考视频，大多数无参考模型依赖昂贵的人类意见标签，且大多数无意见感知方法基于图像忽略了时间上下文。&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展的、基于流的无参考且无意见感知的视频质量评估模型，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;利用DAVIS数据集的合成退化，训练时间感知的卷积架构，直接从退化视频中预测全参考指标（LPIPS、PSNR、SSIM），推理时不需要参考视频。&lt;h4&gt;主要发现&lt;/h4&gt;流式方法优于基于图像的基线模型，能够推广到不同退化类型；时间建模对现实世界视觉系统中可扩展VQA有价值；与广泛使用的BRISQUE基线相比，模型与全参考指标的相关性更高。&lt;h4&gt;结论&lt;/h4&gt;时间感知的无意见感知方法在视频质量评估中有效，为解决现有方法的局限性提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;视频质量评估（VQA）对计算机视觉任务至关重要，但现有方法面临主要局限性：全参考（FR）指标需要干净的参考视频，大多数无参考（NR）模型依赖于昂贵的人类意见标签进行训练。此外，大多数无意见感知的NR方法是基于图像的，忽略了视频对象检测中关键的时间上下文。在这项工作中，我们提出了一种可扩展的、基于流的无参考且无意见感知的VQA模型。我们的模型利用DAVIS数据集的合成退化，训练一个时间感知的卷积架构，直接从退化视频中预测FR指标（LPIPS、PSNR、SSIM），推理时不需要参考。我们表明，我们的流式方法通过推广到各种退化类型，优于我们自己的基于图像的基线，强调了时间建模对现实世界视觉系统中可扩展VQA的价值。此外，我们证明与广泛使用的有意见感知的图像质量评估基线BRISQUE相比，我们的模型与全参考指标的相关性更高，验证了我们时间感知、无意见感知方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video quality assessment (VQA) is vital for computer vision tasks, butexisting approaches face major limitations: full-reference (FR) metrics requireclean reference videos, and most no-reference (NR) models depend on training oncostly human opinion labels. Moreover, most opinion-unaware NR methods areimage-based, ignoring temporal context critical for video object detection. Inthis work, we present a scalable, streaming-based VQA model that is bothno-reference and opinion-unaware. Our model leverages synthetic degradations ofthe DAVIS dataset, training a temporal-aware convolutional architecture topredict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, withoutreferences at inference. We show that our streaming approach outperforms ourown image-based baseline by generalizing across diverse degradations,underscoring the value of temporal modeling for scalable VQA in real-worldvision systems. Additionally, we demonstrate that our model achieves highercorrelation with full-reference metrics compared to BRISQUE, a widely-usedopinion-aware image quality assessment baseline, validating the effectivenessof our temporal, opinion-unaware approach.</description>
      <author>example@mail.com (Kylie Cancilla, Alexander Moore, Amar Saini, Carmen Carrano)</author>
      <guid isPermaLink="false">2511.04628v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic Textual Time Series Depression Detection</title>
      <link>http://arxiv.org/abs/2511.04476v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PTTSD的概率性文本时间序列抑郁检测框架，能够从语句级别的临床访谈中预测抑郁严重程度并提供不确定性估计，在多个数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;准确且可解释的抑郁严重程度预测对临床决策支持至关重要，然而现有模型通常缺乏不确定性估计和时间建模能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从语句级别的临床访谈中预测PHQ-8分数的框架，同时对时间维度上的不确定性进行建模。&lt;h4&gt;方法&lt;/h4&gt;提出PTTSD框架，包含序列到序列和序列到一两种变体，结合双向LSTM、自注意力和残差连接，使用高斯或Student-t输出头，通过负对数似然进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;在E-DAIC和DAIC-WOZ数据集上评估，PTTSD在纯文本系统中达到最先进性能（E-DAIC上MAE为3.85，DAIC上为3.55），产生校准良好的预测区间；消融研究证实了注意力和概率建模的价值；与MentalBERT的比较建立了通用性；三部分校准分析和案例研究强调了不确定性感知预测的可解释性和临床相关性。&lt;h4&gt;结论&lt;/h4&gt;PTTSD框架能有效预测抑郁严重程度并提供不确定性估计，这种不确定性感知的预测具有临床相关性和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;准确且可解释的抑郁严重程度预测对临床决策支持至关重要，但现有模型通常缺乏不确定性估计和时间建模能力。我们提出了PTTSD，一种概率性文本时间序列抑郁检测框架，能够从语句级别的临床访谈中预测PHQ-8分数，同时对时间维度上的不确定性进行建模。PTTSD包含序列到序列和序列到一两种变体，两者都结合了双向LSTM、自注意力和残差连接，并通过负对数似然训练的高斯或Student-t输出头。在E-DAIC和DAIC-WOZ上的评估显示，PTTSD在纯文本系统中达到最先进的性能（例如，E-DAIC上MAE为3.85，DAIC上为3.55），并产生校准良好的预测区间。消融研究证实了注意力和概率建模的价值，与MentalBERT的比较建立了通用性。三部分校准分析和定性案例研究进一步强调了不确定性感知预测的可解释性和临床相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and interpretable predictions of depression severity are essentialfor clinical decision support, yet existing models often lack uncertaintyestimates and temporal modeling. We propose PTTSD, a Probabilistic Textual TimeSeries Depression Detection framework that predicts PHQ-8 scores fromutterance-level clinical interviews while modeling uncertainty over time. PTTSDincludes sequence-to-sequence and sequence-to-one variants, both combiningbidirectional LSTMs, self-attention, and residual connections with Gaussian orStudent-t output heads trained via negative log-likelihood. Evaluated on E-DAICand DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-onlysystems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibratedprediction intervals. Ablations confirm the value of attention andprobabilistic modeling, while comparisons with MentalBERT establish generality.A three-part calibration analysis and qualitative case studies furtherhighlight the interpretability and clinical relevance of uncertainty-awareforecasting.</description>
      <author>example@mail.com (Fabian Schmidt, Seyedehmoniba Ravan, Vladimir Vlassov)</author>
      <guid isPermaLink="false">2511.04476v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs</title>
      <link>http://arxiv.org/abs/2511.04432v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 1 figure, 3 tables, submitted to aconference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了大型语言模型进行时间推理的能力，通过使用1940年的挪威书籍中的问题测试了多种LLMs在不同语言和模型规模下的表现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在处理时间相关任务的能力尚未充分研究，特别是对于历史语境的理解。&lt;h4&gt;目的&lt;/h4&gt;评估LLMs在不同语言和模型规模下进行时间推理的能力，特别关注历史语境的理解。&lt;h4&gt;方法&lt;/h4&gt;使用1940年的挪威书籍中的琐事问题，让LLMs以1940年的身份回答，同时使用英语和挪威语提示。答案以句子形式呈现，评分采用LLM作为评判者，并由母语人士抽样检查。&lt;h4&gt;主要发现&lt;/h4&gt;1) 英语提示比挪威语提示效果更好，这一结果出乎意料；2) 使用更大的LLMs可以提高回答质量；3) 测试了多种模型家族，包括DeepSeek-R1、Gemma3、Qwen3和Llama3.1，以及专门为挪威语设计的最大LLM。&lt;h4&gt;结论&lt;/h4&gt;LLMs在时间推理任务中表现出色，但语言选择和模型规模对结果有显著影响，英语提示比挪威语提示效果更好，而更大的模型通常能提供更准确的回答。&lt;h4&gt;翻译&lt;/h4&gt;在本研究中，我们尝试了大型语言模型进行时间推理的能力。我们使用了一本1940年的挪威书籍，其中包含琐事问题，我们提示LLMs以1940年的身份回答这些问题。我们还用英语和挪威语提出了问题。正确答案通常以句子形式呈现，评分方式是使用LLM作为评判者，并由母语人士进行抽样检查。使用英语提示比挪威语提示效果更好，这是一个意外的结果。相比之下，使用更大的LLMs提高了结果。我们测试了DeepSeek-R1、Gemma3、Qwen3和Llama3.1模型系列，以及专门为挪威语设计的最大可用LLM。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we experiment with the ability of LLMs to do temporalreasoning. Using a Norwegian book from 1940 containing trivia questions, weprompt the LLMs to answer the questions as if it were 1940. We also pose thequestions in both English and Norwegian. Correct answers are often presented assentences, and grading is done by means of LLM-as-judge, with sampled checks bya native speaker. Prompting in English consistently gave better results than inNorwegian, an unexpected result. In contrast, using larger LLMs improvedresults. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,and also the largest available LLM especially crafted for Norwegian.</description>
      <author>example@mail.com (Lars Bungum, Charles Yijia Huang, Abeer Kashar)</author>
      <guid isPermaLink="false">2511.04432v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>LUME-DBN: Full Bayesian Learning of DBNs from Incomplete data in Intensive Care</title>
      <link>http://arxiv.org/abs/2511.04333v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 8 figures, 3 tables, presented at HC@AIxIA + HYDRA 2025  Workshop located at ECAI 2025 Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于Gibbs采样的新方法，用于从不完整数据中学习动态贝叶斯网络，解决了医疗数据中处理缺失数据的问题，特别是在重症监护等需要理解时间动态性的场景中。&lt;h4&gt;背景&lt;/h4&gt;动态贝叶斯网络在医疗保健中日益被使用，因为它们能够对病人数据中的复杂时间关系进行建模，同时保持可解释性。然而，处理纵向临床数据中缺失数据的方法大多来自静态贝叶斯网络文献，未能充分考虑数据的时间性质，这限制了随时间量化不确定性的能力，特别是在重症监护等环境中。&lt;h4&gt;目的&lt;/h4&gt;开发一个完整的贝叶斯框架，整合缺失数据处理，以便在动态贝叶斯网络中进行更可靠的推断和更准确的缺失值插补。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于Gibbs采样的新方法，用于从不完整数据中学习动态贝叶斯网络。该方法将每个缺失值视为遵循高斯分布的未知参数。在每次迭代中，从未观测值的完全条件分布中采样，允许进行合理的插补和不确定性估计。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟数据集和来自重症监护病人的真实世界数据上评估了该方法。与标准的模型无关技术(如MICE)相比，这种贝叶斯方法展示了更好的重建准确性和收敛特性。这些结果强调了在时间模型中整合完整贝叶斯推断的临床相关性，提供了更可靠的插补和对模型行为的更深入见解。&lt;h4&gt;结论&lt;/h4&gt;该方法支持更安全和更明智的临床决策，特别是在缺失数据频繁且可能产生重大影响的场景中。&lt;h4&gt;翻译&lt;/h4&gt;动态贝叶斯网络(DBNs)在医疗保健中的应用日益增多，因为它们能够对病人数据中的复杂时间关系进行建模，同时保持可解释性，这是临床决策的必要特征。然而，处理纵向临床数据中缺失数据的现有方法大多来自静态贝叶斯网络文献，未能充分考虑数据的时间性质。这一局限限制了随时间量化不确定性的能力，在重症监护等环境中尤为重要，因为理解时间动态性对模型的可信度和在不同患者群体中的适用性至关重要。尽管动态贝叶斯网络具有潜力，但整合缺失数据处理的完整贝叶斯框架仍然发展不完善。在这项工作中，我们提出了一种基于Gibbs采样的新方法，用于从不完整数据中学习动态贝叶斯网络。我们的方法将每个缺失值视为遵循高斯分布的未知参数。在每次迭代中，从未观测值的完全条件分布中采样，允许进行合理的插补和不确定性估计。我们在模拟数据集和来自重症监护病人的真实世界数据上评估了我们的方法。与标准的模型无关技术(如MICE)相比，我们的贝叶斯方法展示了更好的重建准确性和收敛特性。这些结果强调了在时间模型中整合完整贝叶斯推断的临床相关性，提供了更可靠的插补和对模型行为的更深入见解。我们的方法支持更安全和更明智的临床决策，特别是在缺失数据频繁且可能产生重大影响的场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic Bayesian networks (DBNs) are increasingly used in healthcare due totheir ability to model complex temporal relationships in patient data whilemaintaining interpretability, an essential feature for clinicaldecision-making. However, existing approaches to handling missing data inlongitudinal clinical datasets are largely derived from static Bayesiannetworks literature, failing to properly account for the temporal nature of thedata. This gap limits the ability to quantify uncertainty over time, which isparticularly critical in settings such as intensive care, where understandingthe temporal dynamics is fundamental for model trustworthiness andapplicability across diverse patient groups. Despite the potential of DBNs, afull Bayesian framework that integrates missing data handling remainsunderdeveloped. In this work, we propose a novel Gibbs sampling-based methodfor learning DBNs from incomplete data. Our method treats each missing value asan unknown parameter following a Gaussian distribution. At each iteration, theunobserved values are sampled from their full conditional distributions,allowing for principled imputation and uncertainty estimation. We evaluate ourmethod on both simulated datasets and real-world intensive care data fromcritically ill patients. Compared to standard model-agnostic techniques such asMICE, our Bayesian approach demonstrates superior reconstruction accuracy andconvergence properties. These results highlight the clinical relevance ofincorporating full Bayesian inference in temporal models, providing morereliable imputations and offering deeper insight into model behavior. Ourapproach supports safer and more informed clinical decision-making,particularly in settings where missing data are frequent and potentiallyimpactful.</description>
      <author>example@mail.com (Federico Pirola, Fabio Stella, Marco Grzegorczyk)</author>
      <guid isPermaLink="false">2511.04333v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering</title>
      <link>http://arxiv.org/abs/2511.04072v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PoK的Plan of Knowledge框架，结合结构化规划与时序知识检索，解决了大语言模型在时间推理方面的局限性，显著提高了时序知识图谱问答的检索精度和推理准确性。&lt;h4&gt;背景&lt;/h4&gt;先前研究使用预训练的时序知识图谱嵌入或图神经网络注入时间知识，但未能充分理解时间约束的复杂语义信息。大语言模型虽有强大的语义理解和推理泛化能力，但时间推理能力有限，且常出现幻觉和知识缺乏问题。&lt;h4&gt;目的&lt;/h4&gt;解决大语言模型在时间推理方面的局限性，提高时序知识图谱问答的检索精度和推理准确性，增强时间推理的可解释性和事实一致性。&lt;h4&gt;方法&lt;/h4&gt;提出Plan of Knowledge框架(PoK)，包含对比时间检索器。Plan of Knowledge模块将复杂时间问题分解为从预定义工具中提取的子目标序列；构建带有对比检索框架的时序知识库(TKS)，使模型能选择性地从时序知识图谱中检索语义和时间对齐的事实。&lt;h4&gt;主要发现&lt;/h4&gt;PoK有效提高了时间推理的可解释性和事实一致性。在四个基准时序知识图谱问答数据集上的实验表明，PoK显著提高了大语言模型的检索精度和推理准确性，最多比最先进的时序知识图谱问答方法高出56.0%的性能。&lt;h4&gt;结论&lt;/h4&gt;通过结合结构化规划与时序知识检索，PoK框架成功解决了大语言模型在时间推理方面的局限性，显著提升了时序知识图谱问答的性能，为未来研究提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;时序知识图谱问答(TKGQA)旨在通过利用时序知识图谱(TKGs)中的事实信息来回答时间敏感问题。虽然先前的研究采用预训练的时序知识图谱嵌入或图神经网络来注入时间知识，但它们未能完全理解时间约束的复杂语义信息。最近，大语言模型(LLMs)已显示出显著进展，受益于其强大的语义理解和推理泛化能力。然而，它们的时间推理能力仍然有限。LLMs经常出现幻觉和知识缺乏问题。为解决这些局限性，我们提出了带有对比时间检索器的Plan of Knowledge框架，命名为PoK。具体而言，所提出的Plan of Knowledge模块将复杂的时间问题分解为从预定义工具中提取的一系列子目标，作为推理探索的中间指导。同时，我们构建了一个带有对比检索框架的时序知识库(TKS)，使模型能够从时序知识图谱中选择性地检索语义和时间对齐的事实。通过结合结构化规划与时序知识检索，PoK有效提高了时间推理的可解释性和事实一致性。在四个基准时序知识图谱问答数据集上的大量实验表明，PoK显著提高了大语言模型的检索精度和推理准确性，最多比最先进的时序知识图谱问答方法高出56.0%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal Knowledge Graph Question Answering (TKGQA) aims to answertime-sensitive questions by leveraging factual information from TemporalKnowledge Graphs (TKGs). While previous studies have employed pre-trained TKGembeddings or graph neural networks to inject temporal knowledge, they fail tofully understand the complex semantic information of time constraints.Recently, Large Language Models (LLMs) have shown remarkable progress,benefiting from their strong semantic understanding and reasoninggeneralization capabilities. However, their temporal reasoning ability remainslimited. LLMs frequently suffer from hallucination and a lack of knowledge. Toaddress these limitations, we propose the Plan of Knowledge framework with acontrastive temporal retriever, which is named PoK. Specifically, the proposedPlan of Knowledge module decomposes a complex temporal question into a sequenceof sub-objectives from the pre-defined tools, serving as intermediate guidancefor reasoning exploration. In parallel, we construct a Temporal Knowledge Store(TKS) with a contrastive retrieval framework, enabling the model to selectivelyretrieve semantically and temporally aligned facts from TKGs. By combiningstructured planning with temporal knowledge retrieval, PoK effectively enhancesthe interpretability and factual consistency of temporal reasoning. Extensiveexperiments on four benchmark TKGQA datasets demonstrate that PoK significantlyimproves the retrieval precision and reasoning accuracy of LLMs, surpassing theperformance of the state-of-the-art TKGQA methods by 56.0% at most.</description>
      <author>example@mail.com (Xinying Qian, Ying Zhang, Yu Zhao, Baohang Zhou, Xuhui Sui, Xiaojie Yuan)</author>
      <guid isPermaLink="false">2511.04072v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Machine learning-driven elasticity prediction in advanced inorganic materials via convolutional neural networks</title>
      <link>http://arxiv.org/abs/2511.04468v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 7 figures,All the data presented in this paper are openly  available at https://doi.org/10.57760/sciencedb.j00213.00104.Published in  Acta Physica Sinica&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究使用晶体图卷积神经网络(CGCNNs)成功预测了大量无机晶体材料的弹性性质，为材料设计提供了重要数据支持。&lt;h4&gt;背景&lt;/h4&gt;无机晶体材料因优异的物理化学性质具有广阔应用前景，其弹性性质(剪切模量、体积模量)对预测材料的电导率、热导率和机械性能至关重要。传统实验测量成本高且效率低，而基于图神经网络的机器学习方法已成为有效替代方案。&lt;h4&gt;目的&lt;/h4&gt;预测无机晶体材料的弹性性质(剪切模量和体积模量)，建立大规模材料弹性性质数据库。&lt;h4&gt;方法&lt;/h4&gt;使用Matbench v0.1数据集中10987种材料的剪切模量和体积模量数据训练两个CGCNN模型；筛选材料保留带隙在0.1-3.0 eV之间的材料，排除含放射性元素的化合物；预测数据集包括Materials Project数据库中的54359个晶体结构和Merchant等人发现的26305个晶体结构。&lt;h4&gt;主要发现&lt;/h4&gt;训练的CGCNN模型具有高精度(平均绝对误差小于13，决定系数R平方接近1)和良好的泛化能力；完成了80664种无机晶体剪切模量和体积模量的预测。&lt;h4&gt;结论&lt;/h4&gt;这项工作丰富了现有的材料弹性数据资源，为材料设计提供了强有力的支持，所有数据已在指定平台公开可用。&lt;h4&gt;翻译&lt;/h4&gt;无机晶体材料因优异的物理化学性质具有广阔的应用潜力，其弹性性质(剪切模量、体积模量)对预测材料的电导率、热导率和机械性能至关重要。传统实验测量成本高且效率低，而理论模拟和基于图神经网络的机器学习方法——特别是晶体图卷积神经网络(CGCNNs)——已成为有效替代方法，在预测材料弹性性质方面取得了显著成果。本研究使用Matbench v0.1数据集中10987种材料的剪切模量和体积模量数据训练了两个CGCNN模型，这些模型表现出高精度(平均绝对误差&lt;13，决定系数R²接近1)和良好的泛化能力。对材料进行筛选，保留带隙在0.1-3.0 eV之间的材料，排除含放射性元素的化合物。最终的预测数据集包括两部分：Materials Project数据库中的54359个晶体结构和Merchant等人(2023 Nature 624 80)发现的26305个晶体结构。最终，本研究完成了80664种无机晶体剪切模量和体积模量的预测。这项工作丰富了现有的材料弹性数据资源，为材料设计提供了强有力的支持，所有数据已在https://doi.org/10.57760/sciencedb.j00213.00104公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.7498/aps.74.20250127&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inorganic crystal materials have broad application potential due to excellentphysical and chemical properties, with elastic properties (shear modulus, bulkmodulus) crucial for predicting materials' electrical conductivity, thermalconductivity and mechanical properties. Traditional experimental measurementsuffers from high cost and low efficiency, while theoretical simulation andgraph neural network-based machine learning methods--especially crystal graphconvolutional neural networks (CGCNNs)--have become effective alternatives,achieving remarkable results in predicting material elastic properties. Thisstudy trained two CGCNN models using shear modulus and bulk modulus data of10987 materials from the Matbench v0.1 dataset, which exhibit high accuracy(mean absolute error &lt;13, coefficient of determination R-squared close to 1)and good generalization ability. Materials were screened to retain those withband gaps between 0.1-3.0 eV and exclude radioactive element-containingcompounds. The final predicted dataset comprises two parts: 54359 crystalstructures from the Materials Project database and 26305 crystal structuresdiscovered by Merchant et al. (2023 Nature 624 80). Ultimately, this studycompleted the prediction of shear modulus and bulk modulus for 80664 inorganiccrystals. This work enriches existing material elastic data resources andprovides robust support for material design, with all data openly available athttps://doi.org/10.57760/sciencedb.j00213.00104.</description>
      <author>example@mail.com (Yujie Liu, Zhenyu Wang, Hang Lei, Guoyu Zhang, Jiawei Xian, Zhibin Gao, Jun Sun, Haifeng Song, Xiangdong Ding)</author>
      <guid isPermaLink="false">2511.04468v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Denoised Recommendation Model with Collaborative Signal Decoupling</title>
      <link>http://arxiv.org/abs/2511.04237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DRCSD的新型GNN-based CF模型，用于去噪用户-物品交互矩阵中的不稳定交互，通过协同信号解耦和阶段式去噪模块提高了推荐系统的鲁棒性和准确性。&lt;h4&gt;背景&lt;/h4&gt;协同过滤(CF)算法在推荐系统中取得了显著性能，但用户-物品交互矩阵中的噪声导致推荐性能次优。现有的去噪研究大多在单个图上进行，可能导致协同信号衰减。&lt;h4&gt;目的&lt;/h4&gt;解决现有去噪方法在单一图上去噪导致的协同信号衰减问题，提高推荐系统对不稳定交互的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出DRCSD模型，包含两个核心模块：1)协同信号解耦模块，根据结构特征将信号分解为不同阶；2)阶段式去噪模块，对每个阶进行有针对性的去噪。同时修改传统GNN-based CF模型的信息聚合机制，避免跨阶信号干扰。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公共真实世界数据集上的实验表明，DRCSD对不稳定交互具有更强的鲁棒性，与最先进的基线模型相比，在推荐准确性指标上实现了统计学上显著的性能改进。&lt;h4&gt;结论&lt;/h4&gt;DRCSD模型通过协同信号解耦和阶段式去噪有效解决了协同过滤中的噪声问题，显著提高了推荐系统的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;尽管协同过滤(CF)算法在推荐系统中取得了显著性能，但由于用户-物品交互矩阵中的噪声，其推荐性能次优。许多去噪研究改进了推荐模型，但大多数现有方法在单个图上进行去噪。这可能导致协同信号衰减：移除两个节点之间的边会中断其他节点之间的路径，弱化路径依赖的协同信息。为解决这些局限性，本研究提出了一种名为DRCSD的新型基于GNN的CF模型，用于去噪不稳定交互。DRCSD包含两个核心模块：协同信号解耦模块（根据结构特征将信号分解为不同阶）和阶段式去噪模块（对每个阶进行有针对性的去噪）。此外，修改了传统基于GNN的CF模型的信息聚合机制，避免跨阶信号干扰，直到最终池化操作。在三个公共真实世界数据集上的大量实验表明，DRCSD对不稳定交互具有更强的鲁棒性，与最先进的基线模型相比，在推荐准确性指标上实现了统计学上显著的性能改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although the collaborative filtering (CF) algorithm has achieved remarkableperformance in recommendation systems, it suffers from suboptimalrecommendation performance due to noise in the user-item interaction matrix.Numerous noise-removal studies have improved recommendation models, but mostexisting approaches conduct denoising on a single graph. This may causeattenuation of collaborative signals: removing edges between two nodes caninterrupt paths between other nodes, weakening path-dependent collaborativeinformation. To address these limitations, this study proposes a novelGNN-based CF model called DRCSD for denoising unstable interactions. DRCSDincludes two core modules: a collaborative signal decoupling module (decomposessignals into distinct orders by structural characteristics) and an order-wisedenoising module (performs targeted denoising on each order). Additionally, theinformation aggregation mechanism of traditional GNN-based CF models ismodified to avoid cross-order signal interference until the final poolingoperation. Extensive experiments on three public real-world datasets show thatDRCSD has superior robustness against unstable interactions and achievesstatistically significant performance improvements in recommendation accuracymetrics compared to state-of-the-art baseline models.</description>
      <author>example@mail.com (Zefeng Li, Ning Yang)</author>
      <guid isPermaLink="false">2511.04237v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Networks for User Satisfaction Classification in Human-Computer Interaction</title>
      <link>http://arxiv.org/abs/2511.04166v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络的用户满意度分类框架，能够有效处理复杂交互关系和多维特征，通过图结构建模、图卷积和注意力机制提高了分类性能。&lt;h4&gt;背景&lt;/h4&gt;传统方法在处理用户满意度分类问题时，难以有效处理复杂的交互关系和多维特征，存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于图神经网络的框架，解决传统方法在处理复杂交互关系和多维特征方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;将用户行为、界面元素及其潜在连接抽象为图结构，使用节点和边的联合建模捕获交互过程中的语义和依赖关系；引入图卷积和注意力机制融合局部特征和全局上下文；应用全局池化和分类层实现自动满意度分类；从结构化数据中提取深度模式，提高多源异构和动态环境下的适应性和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在Kaggle公开用户满意度调查数据集上的实验表明，该方法在准确性、F1分数、AUC和精确度等指标上优于多个基线模型，验证了基于图建模在满意度预测任务中的优势。&lt;h4&gt;结论&lt;/h4&gt;该研究不仅丰富了用户建模的理论框架，还在优化人机交互体验方面展示了实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;本研究关注用户满意度分类问题，并提出了一种基于图神经网络的框架，以解决传统方法在处理复杂交互关系和多维特征方面的局限性。用户行为、界面元素及其潜在连接被抽象为图结构，并通过节点和边的联合建模来捕获交互过程中的语义和依赖关系。引入图卷积和注意力机制来融合局部特征和全局上下文，并应用全局池化和分类层实现自动满意度分类。该方法从结构化数据中提取深度模式，提高了多源异构和动态环境下的适应性和鲁棒性。为了验证有效性，使用了Kaggle上的公开用户满意度调查数据集，并与多个基线模型在多个性能指标上进行了比较。实验表明，该方法在准确性、F1分数、AUC和精确度方面优于现有方法，展示了基于图建模在满意度预测任务中的优势。该研究不仅丰富了用户建模的理论框架，还突显了其在优化人机交互体验方面的实际价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study focuses on the problem of user satisfaction classification andproposes a framework based on graph neural networks to address the limitationsof traditional methods in handling complex interaction relationships andmultidimensional features. User behaviors, interface elements, and theirpotential connections are abstracted into a graph structure, and joint modelingof nodes and edges is used to capture semantics and dependencies in theinteraction process. Graph convolution and attention mechanisms are introducedto fuse local features and global context, and global pooling with aclassification layer is applied to achieve automated satisfactionclassification. The method extracts deep patterns from structured data andimproves adaptability and robustness in multi-source heterogeneous and dynamicenvironments. To verify effectiveness, a public user satisfaction surveydataset from Kaggle is used, and results are compared with multiple baselinemodels across several performance metrics. Experiments show that the methodoutperforms existing approaches in accuracy, F1-Score, AUC, and Precision,demonstrating the advantage of graph-based modeling in satisfaction predictiontasks. The study not only enriches the theoretical framework of user modelingbut also highlights its practical value in optimizing human-computerinteraction experience.</description>
      <author>example@mail.com (Rui Liu, Runsheng Zhang, Shixiao Wang)</author>
      <guid isPermaLink="false">2511.04166v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>ScaleDL: Towards Scalable and Efficient Runtime Prediction for Distributed Deep Learning Workloads</title>
      <link>http://arxiv.org/abs/2511.04162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ScaleDL，一种结合非线性逐层建模和基于图神经网络的跨层交互机制的新型运行时预测框架，实现了深度神经网络运行时预测的高准确性和跨架构泛化能力，同时采用D-optimal方法降低数据收集成本。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络是现代AI服务的基础，支持自动驾驶、聊天机器人和推荐系统等应用。随着模型规模和复杂度增加，DNN工作负载对计算资源需求剧增，准确的运行时预测对优化开发和资源分配至关重要。传统加性计算单元模型准确性和泛化能力有限，而图增强建模虽提高性能但显著增加数据收集成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种在准确性、泛化能力和数据收集成本之间取得平衡的深度神经网络运行时预测方法。&lt;h4&gt;方法&lt;/h4&gt;提出ScaleDL框架，结合非线性逐层建模和基于图神经网络的跨层交互机制实现准确预测和跨架构泛化，同时采用D-optimal方法减少数据收集成本。&lt;h4&gt;主要发现&lt;/h4&gt;在五种流行DNN模型工作负载上的实验证明，ScaleDL相比基线模型实现了6倍更低的平均相对误差和5倍更低的均方根误差，显著提高了运行时预测的准确性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;ScaleDL框架通过创新的建模方法解决了深度神经网络运行时预测中的准确性、泛化能力和数据收集成本之间的平衡问题，为DNN开发和资源优化提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络(DNN)构成了现代AI服务的基础，支持包括自动驾驶、聊天机器人和推荐系统在内的广泛应用。随着模型规模和复杂度的增加，DNN工作负载如训练和推理任务对分布式计算资源提出了前所未有的需求，这使得准确的运行时预测对于优化开发和资源分配至关重要。传统方法依赖于加性计算单元模型，限制了其准确性和泛化能力。相比之下，基于图增强的建模提高了性能，但显著增加了数据收集成本。因此，亟需一种能够在准确性、泛化能力和数据收集成本之间取得平衡的方法。为解决这些挑战，我们提出了ScaleDL，一种新颖的运行时预测框架，它结合了非线性逐层建模和基于图神经网络(GNN)的跨层交互机制，实现了准确的DNN运行时预测和跨不同网络架构的层次泛化能力。此外，我们采用D-optimal方法来减少数据收集成本。在五种流行DNN模型工作负载上的实验证明，ScaleDL提高了运行时预测的准确性和泛化能力，相比基线模型实现了6倍更低的平均相对误差和5倍更低的均方根误差。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks (DNNs) form the cornerstone of modern AI services,supporting a wide range of applications, including autonomous driving,chatbots, and recommendation systems. As models increase in size andcomplexity, DNN workloads like training and inference tasks imposeunprecedented demands on distributed computing resources, making the accurateprediction of runtime essential for optimizing development and resourceallocation. Traditional methods rely on additive computational unit models,limiting their accuracy and generalizability. In contrast, graph-enhancedmodeling improves performance but significantly increases data collectioncosts. Therefore, there is a critical need for a method that strikes a balancebetween accuracy, generalizability, and the costs of data collection. Toaddress these challenges, we propose ScaleDL, a novel runtime predictionframework that combines nonlinear layer-wise modeling with graph neural network(GNN)-based cross-layer interaction mechanism, enabling accurate DNN runtimeprediction and hierarchical generalizability across different networkarchitectures. Additionally, we employ the D-optimal method to reduce datacollection costs. Experiments on the workloads of five popular DNN models provethat ScaleDL enhances runtime prediction accuracy and generalizability,achieving 6$\times$ lower MRE and 5$\times$ lower RMSE compared to baselinemodels.</description>
      <author>example@mail.com (Xiaokai Wang, Shaoyuan Huang, Yuting Li, Xiaofei Wang)</author>
      <guid isPermaLink="false">2511.04162v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>KGFR: A Foundation Retriever for Generalized Knowledge Graph Question Answering</title>
      <link>http://arxiv.org/abs/2511.04093v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出LLM-KGFR协作框架，结合大型语言模型与知识图谱基础检索器，解决知识密集型问题处理中的限制，提高在大型知识图谱上的可扩展性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在推理方面表现出色，但由于上下文和参数化知识的限制，在处理知识密集型问题时存在困难。现有方法依赖于微调的LLMs或GNN检索器，但受限于数据集特定的调优以及在大型或未见过的图上的可扩展性。&lt;h4&gt;目的&lt;/h4&gt;提出一个协作框架，解决知识密集型问题处理中的限制，提高在大型知识图谱上的可扩展性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出LLM-KGFR协作框架，其中LLM与结构化检索器KGFR协同工作。KGFR使用LLM生成的描述对关系进行编码，并根据实体在问题中的作用初始化实体，实现零样本泛化。采用非对称渐进传播处理大型图，通过节点级、边级和路径级接口形成可控的推理循环。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明LLM-KGFR在保持可扩展性和泛化能力的同时实现了强大的性能。&lt;h4&gt;结论&lt;/h4&gt;LLM-KGFR为知识图谱增强推理提供了实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在推理方面表现出色，但由于上下文和参数化知识的限制，在处理知识密集型问题时存在困难。然而，现有的依赖于微调LLMs或GNN检索器的方法受限于数据集特定的调优以及在大型或未见过的图上的可扩展性。我们提出了LLM-KGFR协作框架，其中LLM与结构化检索器知识图谱基础检索器协同工作。KGFR使用LLM生成的描述对关系进行编码，并根据实体在问题中的作用初始化实体，实现对未见过的知识图谱的零样本泛化。为有效处理大型图，它采用非对称渐进传播——一种逐步扩展方法，在选择性地限制高阶节点的同时保留信息路径。通过节点级、边级和路径级接口，LLM迭代地请求候选答案、支持事实和推理路径，形成可控的推理循环。实验证明LLM-KGFR在保持可扩展性和泛化能力的同时实现了强大的性能，为知识图谱增强推理提供了实用的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) excel at reasoning but struggle withknowledge-intensive questions due to limited context and parametric knowledge.However, existing methods that rely on finetuned LLMs or GNN retrievers arelimited by dataset-specific tuning and scalability on large or unseen graphs.We propose the LLM-KGFR collaborative framework, where an LLM works with astructured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFRencodes relations using LLM-generated descriptions and initializes entitiesbased on their roles in the question, enabling zero-shot generalization tounseen KGs. To handle large graphs efficiently, it employs AsymmetricProgressive Propagation (APP)- a stepwise expansion that selectively limitshigh-degree nodes while retaining informative paths. Through node-, edge-, andpath-level interfaces, the LLM iteratively requests candidate answers,supporting facts, and reasoning paths, forming a controllable reasoning loop.Experiments demonstrate that LLM-KGFR achieves strong performance whilemaintaining scalability and generalization, providing a practical solution forKG-augmented reasoning.</description>
      <author>example@mail.com (Yuanning Cui, Zequn Sun, Wei Hu, Zhangjie Fu)</author>
      <guid isPermaLink="false">2511.04093v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization</title>
      <link>http://arxiv.org/abs/2511.04008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GNN-MoE的新方法，通过结合图神经网络和专家混合框架，实现了在领域泛化任务中高效且鲁棒的Vision Transformer微调。&lt;h4&gt;背景&lt;/h4&gt;领域泛化(DG)旨在使Vision Transformer在未见过的领域上保持鲁棒性能，但高效地预训练ViT用于DG具有挑战性，标准的微调方法成本高昂且可能损害泛化能力。&lt;h4&gt;目的&lt;/h4&gt;增强参数高效微调(PEFT)在领域泛化任务上的应用性能，实现更高效、更鲁棒的模型适应。&lt;h4&gt;方法&lt;/h4&gt;提出GNN-MoE方法，使用专家混合(MoE)框架结合高效的Kronecker适配器，采用基于图神经网络的路由器(GCN, GAT, SAGE)在补丁间图上操作，动态分配补丁给专门专家，利用补丁间关系更好地适应域偏移。&lt;h4&gt;主要发现&lt;/h4&gt;GNN-MoE在领域泛化基准测试中实现了最先进或具有竞争力的性能，同时保持高参数效率。&lt;h4&gt;结论&lt;/h4&gt;基于图的上下文路由对于实现鲁棒、轻量级的领域泛化具有实用价值。&lt;h4&gt;翻译&lt;/h4&gt;领域泛化(DG)寻求在未见过的领域上实现鲁棒的Vision Transformer(ViT)性能。高效地预训练ViT用于DG具有挑战性；标准的微调成本高昂且可能损害泛化能力。我们提出了GNN-MoE，使用高效的Kronecker适配器，通过专家混合(MoE)框架增强了参数高效微调(PEFT)在DG上的应用。与基于token的路由不同，一种新颖的图神经网络(GNN)路由器(GCN, GAT, SAGE)在补丁间图上操作，动态地将补丁分配给专门的专家。这种上下文感知的GNN路由利用补丁间关系，更好地适应域偏移。GNN-MoE以高参数效率实现了最先进或具有竞争力的DG基准性能，突显了基于图的上下文路由对于鲁棒、轻量级DG的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain generalization (DG) seeks robust Vision Transformer (ViT) performanceon unseen domains. Efficiently adapting pretrained ViTs for DG is challenging;standard fine-tuning is costly and can impair generalization. We proposeGNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with aMixture-of-Experts (MoE) framework using efficient Kronecker adapters. Insteadof token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT,SAGE) operates on inter-patch graphs to dynamically assign patches tospecialized experts. This context-aware GNN routing leverages inter-patchrelationships for better adaptation to domain shifts. GNN-MoE achievesstate-of-the-art or competitive DG benchmark performance with high parameterefficiency, highlighting the utility of graph-based contextual routing forrobust, lightweight DG.</description>
      <author>example@mail.com (Mahmoud Soliman, Omar Abdelaziz, Ahmed Radwan, Anand, Mohamed Shehata)</author>
      <guid isPermaLink="false">2511.04008v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Sketch-Augmented Features Improve Learning Long-Range Dependencies in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2511.03824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过注入随机化的全局节点特征嵌入（称为'草图随机特征'）来增强图神经网络性能的方法，有效解决了GNN面临的三个关键挑战。&lt;h4&gt;背景&lt;/h4&gt;图神经网络通过迭代聚合局部邻域信息在图结构数据上进行学习。这种局部消息传递范式虽然提供了强大的归纳偏置并利用了图稀疏性，但也带来了三个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;解决图神经网络面临的三个关键挑战：(i) 长程信息的过度压缩，(ii) 节点表示的过度平滑，(iii) 有限的表达能力。&lt;h4&gt;方法&lt;/h4&gt;将随机化的节点特征全局嵌入（称为'草图随机特征'）注入到标准GNN中，使它们能够有效地捕获长程依赖关系。这些嵌入是唯一的、距离敏感的且与拓扑无关的。&lt;h4&gt;主要发现&lt;/h4&gt;通过分析和实验证明，当将这些嵌入注入到GNN中时，可以减轻上述提到的局限性。在真实世界的图学习任务上的实验结果表明，这种策略比基线GNN能持续提高性能。&lt;h4&gt;结论&lt;/h4&gt;该策略既可以作为独立解决方案，也可以作为现有技术（如图位置编码）的补充增强。源代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络通过迭代聚合图结构数据上的局部邻域信息进行学习。虽然这种局部消息传递范式提供了强大的归纳偏置并利用了图稀疏性，但也带来了三个关键挑战：(i) 长程信息的过度压缩，(ii) 节点表示的过度平滑，(iii) 有限的表达能力。在这项工作中，我们将节点特征的随机化全局嵌入（我们称之为'草图随机特征'）注入到标准GNN中，使它们能够有效地捕获长程依赖关系。这些嵌入是唯一的、距离敏感的且与拓扑无关的——通过分析和实验，我们证明了当这些嵌入注入到GNN中时，可以减轻上述提到的局限性。在真实世界图学习任务上的实验结果证实，这种策略比基线GNN能持续提高性能，既可作为独立解决方案，也可作为现有技术（如图位置编码）的补充增强。我们的源代码可在https://github.com/ryienh/sketched-random-features获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks learn on graph-structured data by iterativelyaggregating local neighborhood information. While this local message passingparadigm imparts a powerful inductive bias and exploits graph sparsity, it alsoyields three key challenges: (i) oversquashing of long-range information, (ii)oversmoothing of node representations, and (iii) limited expressive power. Inthis work we inject randomized global embeddings of node features, which weterm \textit{Sketched Random Features}, into standard GNNs, enabling them toefficiently capture long-range dependencies. The embeddings are unique,distance-sensitive, and topology-agnostic -- properties which we analyticallyand empirically show alleviate the aforementioned limitations when injectedinto GNNs. Experimental results on real-world graph learning tasks confirm thatthis strategy consistently improves performance over baseline GNNs, offeringboth a standalone solution and a complementary enhancement to existingtechniques such as graph positional encodings. Our source code is available at\href{https://github.com/ryienh/sketched-random-features}{https://github.com/ryienh/sketched-random-features}.</description>
      <author>example@mail.com (Ryien Hosseini, Filippo Simini, Venkatram Vishwanath, Rebecca Willett, Henry Hoffmann)</author>
      <guid isPermaLink="false">2511.03824v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Causal Graph Neural Networks for Healthcare</title>
      <link>http://arxiv.org/abs/2511.02531v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;医疗人工智能系统在跨机构部署时经常失败，表现为性能下降和历史数据中歧视性模式的持续存在。这种脆弱性部分源于系统学习的是统计关联而非因果机制。因果图神经网络通过结合生物医学数据的图表示和因果推理原则，学习不变机制而非虚假相关性，以解决分布偏移、歧视性和不可解释性三重危机。&lt;h4&gt;背景&lt;/h4&gt;医疗人工智能系统在跨机构部署时经常失败，有记录显示性能下降且延续了历史数据中的歧视性模式。这种脆弱性部分源于系统学习的是统计关联而非因果机制。&lt;h4&gt;目的&lt;/h4&gt;通过因果图神经网络解决医疗AI面临的分布偏移、歧视性和不可解释性三重危机，学习不变机制而非虚假相关性。&lt;h4&gt;方法&lt;/h4&gt;结合生物医学数据的图表示和因果推理原则，采用结构因果模型、解纠缠因果表征学习，以及图上的干预预测和反事实推理技术。&lt;h4&gt;主要发现&lt;/h4&gt;因果图神经网络在多个医疗领域展示了临床价值：精神疾病诊断通过脑网络分析、癌症亚型分析通过多组学因果整合、连续生理监测与机制解释、纠正处方偏差的药物推荐。&lt;h4&gt;结论&lt;/h4&gt;这些进展为患者特异性因果数字孪生奠定了基础，可实现计算机内临床实验，并整合大语言模型进行假设生成和因果图神经网络进行机制验证。仍存在重大障碍，包括计算需求限制实时部署、验证挑战需要多模态证据三角测量，以及因果清洗风险。建议提出区分因果启发架构和因果验证发现的分层框架，并确定关键研究重点，提出因果而非纯关联性主张。&lt;h4&gt;翻译&lt;/h4&gt;医疗人工智能系统在跨机构部署时经常失败，有记录显示性能下降且延续了历史数据中的歧视性模式。这种脆弱性部分源于学习的是统计关联而非因果机制。因果图神经网络通过结合生物医学数据的图表示和因果推理原则，学习不变机制而非虚假相关性，以解决分布偏移、歧视性和不可解释性三重危机。本综述审视了结构因果模型、解纠缠因果表征学习，以及图上的干预预测和反事实推理技术等方法论基础。我们分析了在精神疾病诊断通过脑网络分析、癌症亚型分析通过多组学因果整合、连续生理监测与机制解释、纠正处方偏差的药物推荐等领域展示临床价值的应用。这些进展为患者特异性因果数字孪生奠定了基础，可实现计算机内临床实验，并整合大语言模型进行假设生成和因果图神经网络进行机制验证。仍存在重大障碍，包括计算需求限制实时部署、验证挑战需要多模态证据三角测量，以及因果清洗风险（方法使用因果术语但缺乏严格证据支持）。我们提出区分因果启发架构和因果验证发现的分层框架，并确定提出因果而非纯关联性主张的关键研究重点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Healthcare artificial intelligence systems routinely fail when deployedacross institutions, with documented performance drops and perpetuation ofdiscriminatory patterns embedded in historical data. This brittleness stems, inpart, from learning statistical associations rather than causal mechanisms.Causal graph neural networks address this triple crisis of distribution shift,discrimination, and inscrutability by combining graph-based representations ofbiomedical data with causal inference principles to learn invariant mechanismsrather than spurious correlations. This Review examines methodologicalfoundations spanning structural causal models, disentangled causalrepresentation learning, and techniques for interventional prediction andcounterfactual reasoning on graphs. We analyse applications demonstratingclinical value across psychiatric diagnosis through brain network analysis,cancer subtyping via multi-omics causal integration, continuous physiologicalmonitoring with mechanistic interpretation, and drug recommendation correctingprescription bias. These advances establish foundations for patient-specificCausal Digital Twins, enabling in silico clinical experimentation, withintegration of large language models for hypothesis generation and causal graphneural networks for mechanistic validation. Substantial barriers remain,including computational requirements precluding real-time deployment,validation challenges demanding multi-modal evidence triangulation beyondcross-validation, and risks of causal-washing where methods employ causalterminology without rigorous evidentiary support. We propose tiered frameworksdistinguishing causally-inspired architectures from causally-validateddiscoveries and identify critical research priorities making causal rather thanpurely associational claims.</description>
      <author>example@mail.com (Munib Mesinovic, Max Buhlan, Tingting Zhu)</author>
      <guid isPermaLink="false">2511.02531v2</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2511.04347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了传感器遮挡对基于鸟瞰图(BEV)的3D物体检测系统性能的影响，发现摄像头和激光雷达在不同遮挡条件下的表现差异显著，且融合系统更依赖激光雷达数据。&lt;h4&gt;背景&lt;/h4&gt;准确的3D物体检测对自动驾驶车辆在复杂环境中安全导航至关重要。鸟瞰图(BEV)表示方法通过将多传感器数据投影到俯视空间格式，已成为强大的鲁棒感知方法。然而，由环境条件引起的传感器遮挡对3D检测精度的影响尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究遮挡对摄像头和激光雷达(LiDAR)输出在3D检测任务中的影响，评估不同传感器在不利环境条件下的性能表现。&lt;h4&gt;方法&lt;/h4&gt;使用BEVFusion架构进行研究，在nuScenes数据集上评估，并采用平均精度均值(mAP)和nuScenes检测分数(NDS)作为性能测量指标。&lt;h4&gt;主要发现&lt;/h4&gt;中等程度的摄像头遮挡导致仅基于摄像头的检测mAP下降41.3%；激光雷达仅在严重遮挡下性能急剧下降，mAP下降47.3%，且对远距离检测影响严重；在融合系统中，遮挡摄像头导致轻微4.1%的性能下降，而遮挡激光雷达则导致26.8%的较大下降，表明模型更依赖激光雷达进行3D物体检测。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了未来研究需要关注感知遮挡的评估方法，并开发能在部分传感器失效或降级时保持检测精度的改进传感器融合技术。&lt;h4&gt;翻译&lt;/h4&gt;准确的3D物体检测对自动驾驶车辆在复杂真实世界环境中安全导航至关重要。鸟瞰图(BEV)表示方法通过将多传感器数据投影到俯视空间格式，已成为一种强大的鲁棒感知方法。尽管基于BEV的融合架构通过多模态集成展示了强大的性能，但由雾、霾或物理障碍等环境条件引起的传感器遮挡对3D检测精度的影响仍未得到充分探索。在这项工作中，我们使用BEVFusion架构研究了遮挡对摄像头和激光雷达(LiDAR)输出的影响，并在nuScenes数据集上进行了评估。检测性能使用平均精度均值(mAP)和nuScenes检测分数(NDS)进行测量。我们的结果表明，当仅基于摄像头检测时，中等程度的摄像头遮挡导致mAP下降41.3%(从35.6%降至20.9%)。另一方面，激光雷达仅在严重遮挡下性能急剧下降，mAP下降47.3%(从64.7%降至34.1%)，对远距离检测有严重影响。在融合设置中，效果取决于哪个传感器被遮挡：遮挡摄像头导致轻微4.1%的下降(从68.5%降至65.7%)，而遮挡激光雷达导致较大26.8%的下降(降至50.1%)，揭示了模型在3D物体检测任务中更依赖激光雷达。我们的研究结果强调了未来研究需要关注感知遮挡的评估方法，以及改进传感器融合技术，以便在部分传感器失效或因不利环境条件而降级时保持检测精度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文研究天气引起的传感器遮挡（如雾、霾或物理障碍物）对BEVFusion在3D物体检测中的影响。这个问题很重要，因为自动驾驶车辆需要在复杂环境中安全导航，准确的3D物体检测至关重要。虽然基于鸟瞰图(BEV)的传感器融合方法表现强大，但大多数研究都在理想条件下评估，而实际环境中传感器经常因天气因素退化，可能导致检测性能大幅下降，带来安全风险。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到先前研究只部分探索了遮挡问题：Xie等人研究了相机遮挡但未考虑激光雷达退化，Brophy等人分析了雨对相机检测的影响但未检查多模态融合。作者决定系统性地研究BEVFusion架构中两种传感器的遮挡影响。他们借鉴了Woodscape数据集的污染掩码来模拟相机遮挡，并采用随机点云丢弃来模拟激光雷达退化，这种方法参考了Chan等人的工作。作者选择BEVFusion是因为它处理不同传感器模态独立，适合隔离遮挡效应。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是评估传感器遮挡对BEVFusion架构在3D物体检测中的影响，模拟真实世界中的传感器退化情况。实现流程包括：1)使用nuScenes数据集；2)模拟相机遮挡（应用Woodscape污染掩码和高斯滤波）；3)模拟激光雷达遮挡（随机丢弃点云）；4)在未修改的BEVFusion上评估性能；5)使用mAP和NDS指标测量检测性能；6)分析不同遮挡程度对单独传感器和融合设置的影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次系统研究BEV融合模型中传感器特定遮挡的影响；2)结合相机和激光雷达的遮挡模拟方法；3)定量分析不同遮挡程度对检测性能的影响；4)揭示模型对激光雷达的较强依赖性。相比之前工作，本文不仅研究相机或激光雷达单独的遮挡影响，还研究融合情况下的表现，提供了更全面的遮挡影响分析，并展示了融合模型如何在不同传感器退化条件下保持性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统评估传感器遮挡对BEVFusion 3D物体检测的影响，揭示了模型对激光雷达的较强依赖性，并为开发能在恶劣环境条件下保持检测准确性的鲁棒传感器融合技术提供了重要指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D object detection is essential for automated vehicles to navigatesafely in complex real-world environments. Bird's Eye View (BEV)representations, which project multi-sensor data into a top-down spatialformat, have emerged as a powerful approach for robust perception. AlthoughBEV-based fusion architectures have demonstrated strong performance throughmultimodal integration, the effects of sensor occlusions, caused byenvironmental conditions such as fog, haze, or physical obstructions, on 3Ddetection accuracy remain underexplored. In this work, we investigate theimpact of occlusions on both camera and Light Detection and Ranging (LiDAR)outputs using the BEVFusion architecture, evaluated on the nuScenes dataset.Detection performance is measured using mean Average Precision (mAP) and thenuScenes Detection Score (NDS). Our results show that moderate cameraocclusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection isbased only on the camera. On the other hand, LiDAR sharply drops in performanceonly under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),with a severe impact on long-range detection. In fused settings, the effectdepends on which sensor is occluded: occluding the camera leads to a minor 4.1%drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the taskof 3D object detection. Our results highlight the need for future research intoocclusion-aware evaluation methods and improved sensor fusion techniques thatcan maintain detection accuracy in the presence of partial sensor failure ordegradation due to adverse environmental conditions.</description>
      <author>example@mail.com (Sanjay Kumar, Tim Brophy, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eising)</author>
      <guid isPermaLink="false">2511.04347v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale</title>
      <link>http://arxiv.org/abs/2511.04394v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  code: https://github.com/wuji3/DORAEMON&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DORAEMON是一个开源的PyTorch库，统一了不同尺度下的视觉对象建模和表示学习。&lt;h4&gt;背景&lt;/h4&gt;视觉识别和表示学习领域需要整合多种技术、模型和数据集，以加速研究进展并应用于实际场景。&lt;h4&gt;目的&lt;/h4&gt;提供一个可扩展的基础平台，用于快速实验视觉识别和表示学习，促进研究成果向实际应用的高效转化。&lt;h4&gt;方法&lt;/h4&gt;创建单一的YAML驱动工作流程涵盖分类、检索和度量学习；提供1000多个预训练骨干网络通过timm兼容接口访问；包含模块化损失函数、数据增强和分布式训练工具；支持一键导出到ONNX或HuggingFace格式。&lt;h4&gt;主要发现&lt;/h4&gt;可复现的配方在ImageNet-1K、MS-Celeb-1M和Stanford在线产品上匹配或超过了参考结果；通过整合数据集、模型和训练技术，提供了可扩展的基础平台。&lt;h4&gt;结论&lt;/h4&gt;DORAEMON通过将数据集、模型和训练技术整合到一个平台，为视觉识别和表示学习的快速实验提供了可扩展的基础，实现了研究成果向实际应用的高效转化。&lt;h4&gt;翻译&lt;/h4&gt;DORAEMON是一个开源的PyTorch库，统一了不同尺度下的视觉对象建模和表示学习。单一的YAML驱动工作流程涵盖分类、检索和度量学习；通过timm兼容接口提供了1000多个预训练骨干网络，以及模块化损失函数、数据增强和分布式训练工具。可复现的配方在ImageNet-1K、MS-Celeb-1M和Stanford在线产品上匹配或超过了参考结果，而一键导出到ONNX或HuggingFace则连接了研究和部署。通过将数据集、模型和训练技术整合到一个平台，DORAEMON为视觉识别和表示学习的快速实验提供了可扩展的基础，实现了研究成果向实际应用的高效转化。代码仓库可在https://github.com/wuji3/DORAEMON获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; DORAEMON is an open-source PyTorch library that unifies visual objectmodeling and representation learning across diverse scales. A singleYAML-driven workflow covers classification, retrieval and metric learning; morethan 1000 pretrained backbones are exposed through a timm-compatible interface,together with modular losses, augmentations and distributed-training utilities.Reproducible recipes match or exceed reference results on ImageNet-1K,MS-Celeb-1M and Stanford online products, while one-command export to ONNX orHuggingFace bridges research and deployment. By consolidating datasets, models,and training techniques into one platform, DORAEMON offers a scalablefoundation for rapid experimentation in visual recognition and representationlearning, enabling efficient transfer of research advances to real-worldapplications. The repository is available at https://github.com/wuji3/DORAEMON.</description>
      <author>example@mail.com (Ke Du, Yimin Peng, Chao Gao, Fan Zhou, Siqiao Xue)</author>
      <guid isPermaLink="false">2511.04394v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification</title>
      <link>http://arxiv.org/abs/2511.04281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于DINOv2的步态表示学习框架(DinoGRL)，用于解决可见光-红外视频行人重识别问题，通过结合步态特征和外观特征，实现了跨模态视频匹配的显著改进。&lt;h4&gt;背景&lt;/h4&gt;现有视频可见光-红外行人重识别方法主要利用模态不变视觉特征，但忽略了富含时间动态信息的步态特征，这限制了它们对跨模态视频匹配中时空一致性的建模能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用步态特征的跨模态视频行人重识别方法，提高可见光和红外模态下同一行人的检索准确率。&lt;h4&gt;方法&lt;/h4&gt;提出DINOv2-Driven Gait Representation Learning (DinoGRL)框架，包含语义感知的轮廓和步态学习(SASGL)模型和渐进式双向多粒度增强(PBMGE)模块，利用DINOv2的视觉先验学习步态特征，并通过多粒度双向交互优化特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;通过在HITSZ-VCM和BUPT数据集上的实验证明，该方法显著优于现有的最先进方法，有效结合了步态特征和外观特征的互补优势。&lt;h4&gt;结论&lt;/h4&gt;通过整合步态特征与外观特征，并利用DINOv2的丰富视觉先验和多粒度增强策略，该方法成功提高了跨模态视频行人重识别的性能。&lt;h4&gt;翻译&lt;/h4&gt;基于视频的可见光-红外行人重识别(VVI-ReID)旨在从视频序列中跨可见光和红外模态检索同一行人。现有方法倾向于利用模态不变的视觉特征，但 largely 忽略了步态特征，步态特征不仅是模态不变的，还富含时间动态信息，从而限制了它们对跨模态视频匹配中必不可少的时空一致性建模能力。为解决这些挑战，我们提出了DINOv2驱动的步态表示学习(DinoGRL)框架，利用DINOv2的丰富视觉先验学习与外观线索互补的步态特征，促进跨模态检索的鲁棒序列级表示。具体而言，我们引入了语义感知的轮廓和步态学习(SASGL)模型，利用DINOv2的通用语义先验生成和增强轮廓表示，并与ReID目标联合优化，实现语义丰富且任务自适应的步态特征学习。此外，我们开发了渐进式双向多粒度增强(PBMGE)模块，通过在步态和外观流之间实现多空间粒度的双向交互，逐步细化特征表示，充分利用它们的互补性，用丰富的局部细节增强全局表示，产生高度判别性的特征。在HITSZ-VCM和BUPT数据集上的广泛实验证明了我们方法的优越性，显著优于现有的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video-based Visible-Infrared person re-identification (VVI-ReID) aims toretrieve the same pedestrian across visible and infrared modalities from videosequences. Existing methods tend to exploit modality-invariant visual featuresbut largely overlook gait features, which are not only modality-invariant butalso rich in temporal dynamics, thus limiting their ability to model thespatiotemporal consistency essential for cross-modal video matching. To addressthese challenges, we propose a DINOv2-Driven Gait Representation Learning(DinoGRL) framework that leverages the rich visual priors of DINOv2 to learngait features complementary to appearance cues, facilitating robustsequence-level representations for cross-modal retrieval. Specifically, weintroduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, whichgenerates and enhances silhouette representations with general-purpose semanticpriors from DINOv2 and jointly optimizes them with the ReID objective toachieve semantically enriched and task-adaptive gait feature learning.Furthermore, we develop a Progressive Bidirectional Multi-GranularityEnhancement (PBMGE) module, which progressively refines feature representationsby enabling bidirectional interactions between gait and appearance streamsacross multiple spatial granularities, fully leveraging their complementarityto enhance global representations with rich local details and produce highlydiscriminative features. Extensive experiments on HITSZ-VCM and BUPT datasetsdemonstrate the superiority of our approach, significantly outperformingexisting state-of-the-art methods.</description>
      <author>example@mail.com (Yujie Yang, Shuang Li, Jun Ye, Neng Dong, Fan Li, Huafeng Li)</author>
      <guid isPermaLink="false">2511.04281v1</guid>
      <pubDate>Fri, 07 Nov 2025 15:09:04 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Compact Satellite Embeddings and Graph Neural Networks for Large-Scale Poverty Mapping</title>
      <link>http://arxiv.org/abs/2511.01408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于图的方法，利用卫星嵌入来预测撒哈拉以南非洲的集群级别财富指数，改善了贫困地图的绘制质量。&lt;h4&gt;背景&lt;/h4&gt;全球南方国家缺乏精确、细粒度的贫困地图。人口与健康调查(DHS)虽然提供高质量的社会经济数据，但空间覆盖有限，且报告的坐标被随机位移以保护隐私，进一步降低了数据质量。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于图的方法，利用低维AlphaEarth卫星嵌入来预测撒哈拉以南非洲的集群级别财富指数。&lt;h4&gt;方法&lt;/h4&gt;通过建模已调查位置和未标记位置之间的空间关系，并引入概率'模糊标签'损失函数来解释坐标位移，改进了财富预测的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在37个DHS数据集(2017-2023)上的实验表明，与'仅图像'基线相比，结合图结构略微提高了准确性，展示了紧凑的地球观测嵌入在大规模社会经济绘图中的潜力。&lt;h4&gt;结论&lt;/h4&gt;基于图的方法结合卫星嵌入可以改善贫困地图的绘制，特别是在处理数据稀疏和坐标位移问题方面。&lt;h4&gt;翻译&lt;/h4&gt;精确的、细粒度的贫困地图在大多数全球南方国家仍然稀缺。虽然人口与健康调查(DHS)提供高质量的社会经济数据，但它们的空间覆盖有限，且报告的坐标被随机位移以保护隐私，进一步降低了数据质量。我们提出了一种基于图的方法，利用低维AlphaEarth卫星嵌入来预测撒哈拉以南非洲的集群级别财富指数。通过建模已调查和未标记位置之间的空间关系，并通过引入概率'模糊标签'损失函数来解释坐标位移，我们改进了财富预测在现有调查之外的泛化能力。我们在37个DHS数据集(2017-2023)上的实验表明，与'仅图像'基线相比，结合图结构略微提高了准确性，展示了紧凑的地球观测嵌入在大规模社会经济绘图中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate, fine-grained poverty maps remain scarce across much of the GlobalSouth. While Demographic and Health Surveys (DHS) provide high-qualitysocioeconomic data, their spatial coverage is limited and reported coordinatesare randomly displaced for privacy, further reducing their quality. We proposea graph-based approach leveraging low-dimensional AlphaEarth satelliteembeddings to predict cluster-level wealth indices across Sub-Saharan Africa.By modeling spatial relations between surveyed and unlabeled locations, and byintroducing a probabilistic "fuzzy label" loss to account for coordinatedisplacement, we improve the generalization of wealth predictions beyondexisting surveys. Our experiments on 37 DHS datasets (2017-2023) show thatincorporating graph structure slightly improves accuracy compared to"image-only" baselines, demonstrating the potential of compact EO embeddingsfor large-scale socioeconomic mapping.</description>
      <author>example@mail.com (Markus B. Pettersson, Adel Daoud)</author>
      <guid isPermaLink="false">2511.01408v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
  <item>
      <title>Machine Learning for RNA Secondary Structure Prediction: a review of current methods and challenges</title>
      <link>http://arxiv.org/abs/2511.02622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述讨论了RNA二级结构预测领域从热力学方法到数据驱动方法的演变，包括单序列模型、基于进化的模型和混合模型，以及该领域面临的'泛化危机'和RNA基础模型的兴起。同时，文章展望了未来需要解决的主要挑战，包括复杂基序的准确预测、扩展到千碱基长度转录本、整合修饰核苷酸的化学多样性，以及从静态结构转向动态集合。&lt;h4&gt;背景&lt;/h4&gt;RNA二级结构预测是计算生物学的核心挑战，对于理解分子功能和设计新型治疗药物至关重要。该领域已经从基础但准确度有限的热力学方法演变为由机器学习和深度学习主导的新数据驱动范式。&lt;h4&gt;目的&lt;/h4&gt;这篇综述旨在概述现代RNA二级结构预测方法，评估当前进展，并展望未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;现代方法包括单序列模型、基于进化的模型以及将机器学习与生物物理学相结合的混合模型。此外，为应对数据稀缺问题，RNA基础模型应运而生，它们从大量未标记的序列语料库中学习以提高泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;领域面临'泛化危机'，强大的模型在新的RNA家族上表现不佳，这促使整个社区转向更严格、同源感知的基准测试。RNA基础模型通过从未标记的大规模序列数据中学习，提高了泛化能力。&lt;h4&gt;结论&lt;/h4&gt;未来需要解决的主要挑战包括准确预测假结等复杂基序、扩展到千碱基长度转录本、整合修饰核苷酸的化学多样性，以及将预测目标从静态结构转向更好地捕捉生物功能的动态集合。同时，需要标准化的前瞻性基准测试系统，以确保无偏验证并加速进展。&lt;h4&gt;翻译&lt;/h4&gt;预测RNA的二级结构是计算生物学的核心挑战，对于理解分子功能和设计新型治疗药物至关重要。该领域已经从基础但准确度有限的热力学方法演变为由机器学习和深度学习主导的新数据驱动范式。这些模型直接从数据中学习折叠模式，带来了显著的性能提升。这篇综述概述了这些现代方法的现状，涵盖单序列模型、基于进化的模型以及将机器学习与生物物理学相结合的混合模型。一个中心主题是该领域的'泛化危机'，强大的模型被发现在新RNA家族上表现不佳，促使整个社区转向更严格、同源感知的基准测试。为应对潜在的数据稀缺挑战，RNA基础模型已经出现，它们从大量未标记的序列语料库中学习以提高泛化能力。最后，我们展望下一组主要障碍——包括准确预测假结等复杂基序、扩展到千碱基长度转录本、整合修饰核苷酸的化学多样性，以及将预测目标从静态结构转向更好地捕捉生物功能的动态集合。我们还强调了需要标准化的前瞻性基准测试系统，以确保无偏验证并加速进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting the secondary structure of RNA is a core challenge incomputational biology, essential for understanding molecular function anddesigning novel therapeutics. The field has evolved from foundational butaccuracy-limited thermodynamic approaches to a new data-driven paradigmdominated by machine learning and deep learning. These models learn foldingpatterns directly from data, leading to significant performance gains. Thisreview surveys the modern landscape of these methods, covering single-sequence,evolutionary-based, and hybrid models that blend machine learning withbiophysics. A central theme is the field's "generalization crisis," wherepowerful models were found to fail on new RNA families, prompting acommunity-wide shift to stricter, homology-aware benchmarking. In response tothe underlying challenge of data scarcity, RNA foundation models have emerged,learning from massive, unlabeled sequence corpora to improve generalization.Finally, we look ahead to the next set of major hurdles-including the accurateprediction of complex motifs like pseudoknots, scaling to kilobase-lengthtranscripts, incorporating the chemical diversity of modified nucleotides, andshifting the prediction target from static structures to the dynamic ensemblesthat better capture biological function. We also highlight the need for astandardized, prospective benchmarking system to ensure unbiased validation andaccelerate progress.</description>
      <author>example@mail.com (Giuseppe Sacco, Giovanni Bussi, Guido Sanguinetti)</author>
      <guid isPermaLink="false">2511.02622v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes</title>
      <link>http://arxiv.org/abs/2511.02503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了PtychoBench基准，用于比较高级显微镜工作流程自动化中两种模型专业化策略（监督微调SFT和上下文学习ICL）的效果，发现最优策略取决于任务类型，为科学AI系统开发提供了框架。&lt;h4&gt;背景&lt;/h4&gt;高级显微镜工作流程自动化是关键目标，基础模型如LLMs和VLMs显示巨大潜力，但将这些通用模型适应专业科学任务的最佳策略尚不明确。&lt;h4&gt;目的&lt;/h4&gt;通过引入PtychoBench基准，系统比较SFT和ICL两种专业化策略在显微镜分析任务上的效果，确定最优专业化路径。&lt;h4&gt;方法&lt;/h4&gt;创建多模态、多任务PtychoBench基准，使用VLMs进行视觉伪影检测任务，使用LLMs进行文本参数推荐任务，评估SFT和ICL策略在数据稀缺环境下的表现。&lt;h4&gt;主要发现&lt;/h4&gt;最优专业化路径取决于任务类型；视觉任务中SFT和ICL互补效果最佳；文本任务中在大基础模型上使用ICL更优；上下文感知提示具有优越性；微调模型中存在上下文干扰现象。&lt;h4&gt;结论&lt;/h4&gt;科学AI系统的最优专业化路径取决于任务模态，为开发更有效的科学智能系统提供了明确框架。&lt;h4&gt;翻译&lt;/h4&gt;高级显微镜工作流程的自动化是一个关键目标，其中像语言模型（LLMs）和视觉-语言模型（VLMs）这样的基础模型显示出巨大潜力。然而，将这些通用模型适应专业科学任务是至关重要的，最佳领域适应策略通常不明确。为此，我们引入了PtychoBench，一个用于衍射分析的新多模态、多任务基准。使用此基准，我们系统性地比较了两种专业化策略：监督微调（SFT）和上下文学习（ICL）。我们在数据稀缺环境下，使用VLMs评估视觉伪影检测任务，使用LLMs评估文本参数推荐任务。我们的研究结果表明，最优的专业化路径取决于任务类型。对于视觉任务，SFT和ICL高度互补，由上下文感知示例引导的微调模型实现了最高的平均性能（Micro-F1为0.728）。相反，对于文本任务，在大基础模型上使用ICL是更优策略，达到峰值Micro-F1为0.847，优于强大的'超级专家'SFT模型（0-shot Micro-F1为0.839）。我们还确认了上下文感知提示的优越性，并在微调模型中发现了持续的上下文干扰现象。这些结果与包括GPT-4o和基于DINOv3的分类器在内的强大基线进行了基准测试，为科学AI提供了关键观察：在我们的基准中，最优专业化路径取决于任务模态，为开发更有效的科学智能系统提供了明确的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The automation of workflows in advanced microscopy is a key goal wherefoundation models like Language Models (LLMs) and Vision-Language Models (VLMs)show great potential. However, adapting these general-purpose models forspecialized scientific tasks is critical, and the optimal domain adaptationstrategy is often unclear. To address this, we introduce PtychoBench, a newmulti-modal, multi-task benchmark for ptychographic analysis. Using thisbenchmark, we systematically compare two specialization strategies: SupervisedFine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategieson a visual artifact detection task with VLMs and a textual parameterrecommendation task with LLMs in a data-scarce regime. Our findings reveal thatthe optimal specialization pathway is task-dependent. For the visual task, SFTand ICL are highly complementary, with a fine-tuned model guided bycontext-aware examples achieving the highest mean performance (Micro-F1 of0.728). Conversely, for the textual task, ICL on a large base model is thesuperior strategy, reaching a peak Micro-F1 of 0.847 and outperforming apowerful "super-expert" SFT model (0-shot Micro-F1 of 0.839). We also confirmthe superiority of context-aware prompting and identify a consistent contextualinterference phenomenon in fine-tuned models. These results, benchmarkedagainst strong baselines including GPT-4o and a DINOv3-based classifier, offerkey observations for AI in science: the optimal specialization path in ourbenchmark is dependent on the task modality, offering a clear framework fordeveloping more effective science-based agentic systems.</description>
      <author>example@mail.com (Robinson Umeike, Neil Getty, Yin Xiangyu, Yi Jiang)</author>
      <guid isPermaLink="false">2511.02503v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Model order reduction via Lie groups</title>
      <link>http://arxiv.org/abs/2511.03520v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 21 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MORLie的新型基于李群的模型降阶框架，该框架能够将流形上的高维动力系统近似为李群上的低维动力系统，特别适用于处理实际应用中常见的非等变动力学问题。&lt;h4&gt;背景&lt;/h4&gt;李群及其作用在物理系统的描述中无处不在，模型降阶(MOR)是处理高维系统的重要方法。&lt;h4&gt;目的&lt;/h4&gt;探索李群在模型降阶中的影响，并提出一种新的基于李群的模型降阶框架。&lt;h4&gt;方法&lt;/h4&gt;提出MORLie框架，在流形上高维动力系统与李群上低维动力系统之间建立近似关系，提供基于几何公式的新非侵入式MOR方法，能够处理非等变动力学。&lt;h4&gt;主要发现&lt;/h4&gt;MORLie的误差边界低于Kolmogorov N-宽度，限制了线性子空间方法；在三个应用案例中表现优异：1)变形体建模中优于POD方法；2)肝脏运动重建接近最先进水平且训练时间大幅减少；3)解析例证显示方法的通用性。&lt;h4&gt;结论&lt;/h4&gt;MORLi是一种有效的模型降阶方法，能够处理非等变动力学，在多个实际应用中展现出色性能，包括变形体建模和医学图像处理等。&lt;h4&gt;翻译&lt;/h4&gt;李群及其作用在物理系统的描述中无处不在，我们探索了在模型降阶(MOR)设置中的影响。我们提出了一个名为MORLie的基于李群的新型模型降阶框架，其中流形上的高维动力系统被李群上的低维动力系统近似。与其他李群方法相比，我们能够处理实际应用中常见的非等变动力学，并基于提出的几何公式提供了新的非侵入式MOR方法。我们还通过数值计算强调，MORLie的误差边界低于限制线性子空间方法的Kolmogorov N-宽度。该方法应用于各种示例：1. 对遵循剪切运动的噪声点云数据建模的变形体简化模型，其中MORLie在准确性和降维方面优于简单的POD方法；2. 通过超声扫描边缘检测数据重建呼吸期间的肝脏运动，MORLi的性能接近最先进水平，同时将训练时间从计算集群上的几小时减少到移动工作站上的几分钟；3. 一个解析例子，显示冻结方法作为特例被解析恢复，表明了几何框架的通用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lie groups and their actions are ubiquitous in the description of physicalsystems, and we explore implications in the setting of model order reduction(MOR). We present a novel framework of MOR via Lie groups, called MORLie, inwhich high-dimensional dynamical systems on manifolds are approximated bylow-dimensional dynamical systems on Lie groups. In comparison to other Liegroup methods we are able to attack non-equivariant dynamics, which arefrequent in practical applications, and we provide new non-intrusive MORmethods based on the presented geometric formulation. We also highlightnumerically that MORLie has a lower error bound than the Kolmogorov $N$-width,which limits linear-subspace methods. The method is applied to variousexamples: 1. MOR of a simplified deforming body modeled by a noisy point clouddata following a sheering motion, where MORLie outperforms a naive POD approachin terms of accuracy and dimensionality reduction. 2. Reconstructing livermotion during respiration with data from edge detection in ultrasound scans,where MORLie reaches performance approaching the state of the art, whilereducing the training time from hours on a computing cluster to minutes on amobile workstation. 3. An analytic example showing that the method of freezingis analytically recovered as a special case, showing the generality of thegeometric framework.</description>
      <author>example@mail.com (Yannik P. Wotte, Patrick Buchfink, Silke Glas, Federico Califano, Stefano Stramigioli)</author>
      <guid isPermaLink="false">2511.03520v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>IEC3D-AD: A 3D Dataset of Industrial Equipment Components for Unsupervised Point Cloud Anomaly Detection</title>
      <link>http://arxiv.org/abs/2511.03267v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一个针对真实工业场景的点云异常检测数据集(IEC3D-AD)和新的3D异常检测范式(GMANet)，解决了现有数据集无法捕捉真实工业环境复杂性的问题。&lt;h4&gt;背景&lt;/h4&gt;3D异常检测在工业制造中至关重要，特别是对核心设备组件的可靠性和安全。现有数据集如Real3D-AD和MVTec3D-AD无法捕捉真实工业环境中的复杂性和细微缺陷，限制了工业设备组件(如轴承、环和螺栓)的精确异常检测研究。&lt;h4&gt;目的&lt;/h4&gt;开发一个针对真实工业场景的点云异常检测数据集，直接从实际生产线收集，确保高保真度和相关性，以支持更严格的异常检测任务。&lt;h4&gt;方法&lt;/h4&gt;创建IEC3D-AD数据集，具有改进的点云分辨率和缺陷注释粒度；引入GMANet范式，基于几何形态分析生成合成点云样本，通过空间差异优化减少正常和异常点级特征之间的边界并增加重叠。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明，所提出的方法在IEC3D-AD和其他数据集上均表现出有效性。&lt;h4&gt;结论&lt;/h4&gt;新开发的数据集和方法有效解决了工业3D异常检测中的挑战，提高了异常检测的精确度。&lt;h4&gt;翻译&lt;/h4&gt;三维异常检测在工业制造中发挥着关键作用，特别是在确保核心设备组件的可靠性和安全性方面。尽管现有的三维数据集如Real3D-AD和MVTec3D-AD提供了广泛的应用支持，但它们无法捕捉真实工业环境中存在的复杂性和细微缺陷。这一限制阻碍了精确异常检测研究，特别是对于轴承、环和螺栓等工业设备组件。为了应对这一挑战，我们开发了一个针对真实工业场景的点云异常检测数据集(IEC3D-AD)。该数据集直接从实际生产线收集，确保了高保真度和相关性。与现有数据集相比，IEC3D-AD具有显著改进的点云分辨率和缺陷注释粒度，支持更严格的异常检测任务。此外，受生成式二维异常检测方法的启发，我们在IEC3D-AD上引入了一种新的三维异常检测范式(GMANet)。该范式基于几何形态分析生成合成点云样本，然后通过空间差异优化减少正常和异常点级特征之间的边界并增加重叠。大量实验证明了我们的方法在IEC3D-AD和其他数据集上的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有3D异常检测数据集缺乏真实工业场景样本的问题，以及无法同时平衡空间覆盖率和数据密度的局限性。这个问题在现实中非常重要，因为工业设备组件是工业基础和产业链现代化的重要连接点，确保其可靠性和安全对工业生产至关重要；在研究中，现有数据集多来自模具和玩具而非真实工业环境，导致算法在实际应用中性能下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到现有数据集缺乏真实工业场景数据，因此构建了专门针对真实工业场景的数据集IEC3D-AD。方法设计上，作者受生成式2D异常检测方法的启发，引入了基于几何形态分析的合成点云生成(SPCG)模块，借鉴了教师-学生网络的思想使用专家域和学徒域两个编码器，并参考了焦点损失来设计权重优化特征分布差异。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过几何形态分析生成合成异常样本，然后利用空间差异优化减少正常和异常特征边界，增加特征重叠。整体流程分为训练和测试两个阶段：训练阶段使用正常样本，通过SPCG生成合成异常样本，双编码器提取特征，计算差异并优化；测试阶段输入真实样本，使用训练好的编码器提取特征并计算异常分数识别异常点。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建了IEC3D-AD数据集，直接从工业生产线收集，实现360度全覆盖和高点云密度；2)提出了GMANet方法，包含SPCG和SDO两个创新模块；3)提供了全面的基准测试。相比之前工作，不同之处在于：数据来源更真实(工业生产线vs模具玩具)，同时实现了高覆盖率和密度，缺陷比例更低(0.78%-2.28%)，且包含功能拓扑结构和微观几何畸变等真实工业特征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一个专门针对工业设备组件的高质量3D点云异常检测数据集IEC3D-AD，以及一种基于几何形态分析和空间差异优化的无监督异常检测方法GMANet，显著提升了在真实工业场景中检测微小缺陷的能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D anomaly detection (3D-AD) plays a critical role in industrialmanufacturing, particularly in ensuring the reliability and safety of coreequipment components. Although existing 3D datasets like Real3D-AD and MVTec3D-AD offer broad application support, they fall short in capturing thecomplexities and subtle defects found in real industrial environments. Thislimitation hampers precise anomaly detection research, especially forindustrial equipment components (IEC) such as bearings, rings, and bolts. Toaddress this challenge, we have developed a point cloud anomaly detectiondataset (IEC3D-AD) specific to real industrial scenarios. This dataset isdirectly collected from actual production lines, ensuring high fidelity andrelevance. Compared to existing datasets, IEC3D-AD features significantlyimproved point cloud resolution and defect annotation granularity, facilitatingmore demanding anomaly detection tasks. Furthermore, inspired by generative2D-AD methods, we introduce a novel 3D-AD paradigm (GMANet) on IEC3D-AD. Thisparadigm generates synthetic point cloud samples based on geometricmorphological analysis, then reduces the margin and increases the overlapbetween normal and abnormal point-level features through spatial discrepancyoptimization. Extensive experiments demonstrate the effectiveness of our methodon both IEC3D-AD and other datasets.</description>
      <author>example@mail.com (Bingyang Guo, Hongjie Li, Ruiyun Yu, Hanzhe Liang, Jinbao Wang)</author>
      <guid isPermaLink="false">2511.03267v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models</title>
      <link>http://arxiv.org/abs/2511.03147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Lecture Notes in Computer Science (LNCS), 20th International  Symposium on Visual Computing 2025, 12 pages, 4 figures, preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进神经符号距离函数(SDFs)在CAD表面重建中的方法，通过引入时变调度策略优化非对角魏恩加滕(ODW)损失，显著提升了重建质量。&lt;h4&gt;背景&lt;/h4&gt;神经符号距离函数已成为从点云进行几何重建的强大表示方法，但通常需要基于梯度和曲率的正则化来抑制伪影并保持结构保真度。FlatCAD虽然引入了高效的ODW损失作为二阶先验，但使用固定权重存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发ODW损失的调度策略，在高初始权重稳定优化的同时，逐渐衰减权重以允许后期细尺度细节恢复，从而提升CAD重建质量。&lt;h4&gt;方法&lt;/h4&gt;研究并实现了多种调度策略，包括常数、线性、五次和步长插值调度，以及增加的预热变体，并在ABC CAD数据集上进行了实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;时变调度策略显著优于固定权重方法，与FlatCAD基线相比，在Chamfer距离上实现了高达35%的性能提升。&lt;h4&gt;结论&lt;/h4&gt;将调度作为曲率正则化的简单而有效的扩展，能够显著提升神经符号距离函数在CAD重建中的鲁棒性和质量。&lt;h4&gt;翻译&lt;/h4&gt;神经符号距离函数(SDFs)已成为从点云进行几何重建的强大表示方法，但它们通常需要基于梯度和曲率的正则化来抑制伪影并保持结构保真度。FlatCAD引入了非对角魏恩加滕(ODW)损失作为CAD表面的高效二阶先验，以大约一半的计算成本近似完整Hessian正则化。然而，FlatCAD在整个训练过程中应用固定的ODW权重，这是次优的：强正则化稳定了早期优化，但在后期阶段抑制了细节恢复。我们提出了ODW损失的调度策略，分配高初始权重以稳定优化，并逐渐衰减它以允许细尺度细化。我们研究了常数、线性、五次和步长插值调度，以及一个增加的预热变体。在ABC CAD数据集上的实验表明，时变调度始终优于固定权重。我们的方法在Chamfer距离上比FlatCAD基线实现了高达35%的改进，确立了调度作为曲率正则化的简单而有效的扩展，用于鲁棒的CAD重建。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的问题是神经符号距离函数在重建CAD模型时使用固定的Off-Diagonal Weingarten损失权重导致的问题。固定权重在训练早期能稳定优化但会抑制后期细节恢复。这个问题重要是因为CAD模型通常由简单几何形状组成，需要平衡结构稳定性和细节精度，而固定权重无法满足训练不同阶段的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到固定权重无法平衡训练早期的稳定性和后期的细节恢复，因此借鉴了课程学习和多任务学习中的动态权重调整思想。他们设计了多种调度策略，包括常量、线性、五次多项式和阶跃插值，并进行了系统比较。他们还受到Neuralangelo的粗到细策略启发，但将其应用于曲率正则化领域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用'强开始-衰减'策略：训练初期使用高权重ODW损失稳定优化并抑制伪影，随着训练进行逐渐降低权重，允许网络恢复几何细节。实现流程是：定义一组关键点指定训练进度和对应权重；根据当前训练进度确定所在区间；在区间内根据选择的调度策略计算当前权重；使用动态权重更新总损失函数进行网络优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次系统研究ODW损失调度策略；提出多种调度方法；引入'强开始-衰减'训练范式；对比衰减和预热策略。相比之前工作，本文解决了固定权重无法适应训练不同阶段需求的问题，首次系统研究了ODW权重调度，证明了动态权重比固定权重效果更好，最多提升35%的Chamfer距离。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种针对神经符号距离函数中Off-Diagonal Weingarten损失的动态权重调度策略，通过初期高权重稳定优化、后期降低权重允许细节恢复，显著提高了CAD模型的重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural signed distance functions (SDFs) have become a powerful representationfor geometric reconstruction from point clouds, yet they often require bothgradient- and curvature-based regularization to suppress spurious warp andpreserve structural fidelity. FlatCAD introduced the Off-Diagonal Weingarten(ODW) loss as an efficient second-order prior for CAD surfaces, approximatingfull-Hessian regularization at roughly half the computational cost. However,FlatCAD applies a fixed ODW weight throughout training, which is suboptimal:strong regularization stabilizes early optimization but suppresses detailrecovery in later stages. We present scheduling strategies for the ODW lossthat assign a high initial weight to stabilize optimization and progressivelydecay it to permit fine-scale refinement. We investigate constant, linear,quintic, and step interpolation schedules, as well as an increasing warm-upvariant. Experiments on the ABC CAD dataset demonstrate that time-varyingschedules consistently outperform fixed weights. Our method achieves up to a35% improvement in Chamfer Distance over the FlatCAD baseline, establishingscheduling as a simple yet effective extension of curvature regularization forrobust CAD reconstruction.</description>
      <author>example@mail.com (Haotian Yin, Przemyslaw Musialski)</author>
      <guid isPermaLink="false">2511.03147v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs</title>
      <link>http://arxiv.org/abs/2511.03099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DentalSplat的框架，用于从稀疏正畸图像进行3D重建，解决了传统3D高斯溅射技术在正畸远程医疗应用中的局限性。&lt;h4&gt;背景&lt;/h4&gt;在正畸治疗特别是远程医疗背景下，从多角度观察患者咬合情况有助于及时临床决策。传统3D高斯溅射技术需要密集多视角输入和精确相机姿态，但正畸病例通常只有三张稀疏图像（正面和双侧颊视图），使重建极具挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个有效框架，能够从稀疏正畸图像进行高质量3D重建，支持远程正畸诊断。&lt;h4&gt;方法&lt;/h4&gt;DentalSplat框架利用先验引导的密集立体重建模型初始化点云，采用尺度自适应剪枝策略提高3DGS训练效率和重建质量，在极度稀疏视角情况下结合光流作为几何约束和梯度正则化来提高渲染保真度。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能有效处理稀疏输入场景，在咬合可视化方面实现卓越的新视角合成质量，优于现有最先进技术。&lt;h4&gt;结论&lt;/h4&gt;DentalSplat成功解决了正畸治疗中从稀疏图像进行3D重建的挑战，为远程正畸诊断提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;在正畸治疗中，特别是在远程医疗背景下，从多角度观察患者的咬合情况有助于及时的临床决策。最近的3D高斯溅射（3DGS）技术在3D重建和新视角合成方面显示出强大潜力。然而，传统的3DGS流程通常依赖于密集捕获的多视角输入和精确初始化的相机姿态，限制了其实用性。正畸病例通常只有三张稀疏图像，即正面视图和双侧颊视图，使重建任务特别具有挑战性。输入视图的极度稀疏会严重降低重建质量，而相机姿态信息的缺失进一步复杂化了这一过程。为了克服这些限制，我们提出了DentalSplat，一个从稀疏正畸图像进行3D重建的有效框架。我们的方法利用先验引导的密集立体重建模型初始化点云，随后采用尺度自适应剪枝策略提高3DGS的训练效率和重建质量。在极度稀疏视角的情况下，我们进一步结合光流作为几何约束，并结合梯度正则化来提高渲染保真度。我们在一个包含950个临床病例的大型数据集上验证了我们的方法，以及一个额外的基于视频的测试集，包含195个病例，用于模拟现实世界远程正畸成像条件。实验结果表明，我们的方法能有效处理稀疏输入场景，并在咬合可视化方面实现卓越的新视角合成质量，优于最先进的技术。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从稀疏口腔内照片中合成新视角以观察牙齿咬合的问题。在正畸治疗中，特别是远程医疗场景下，这有助于及时临床决策。传统方法如CBCT和IOS需要专业设备，限制远程使用；而现有AI系统主要依赖单视图图像，无法全面评估咬合关系。此问题对提高远程正畸治疗效果、降低专业设备依赖具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统3DGS需要密集多视角输入和精确相机姿态；其他方法如MVSplat、Nope-NeRF假设有重叠视图，不适合真正稀疏场景；DUSt3R虽解决稀疏输入问题但在正畸应用中面临设备差异、牙齿反射、运动模糊等挑战。作者借鉴了3DGS和DUSt3R技术，并针对正畸场景特点设计了专门改进：尺度自适应修剪策略处理密集点云，集成光学流约束和梯度正则化提高渲染质量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合DUSt3R和3DGS优势，解决正畸场景中稀疏输入和未知相机姿态挑战。流程包括：1)用DUSt3R生成初始点云和相机姿态；2)应用尺度自适应修剪策略处理点云；3)在3DGS优化中结合光学流约束确保几何一致性；4)使用梯度约束增强密集化过程；5)通过联合优化相机姿态和3D高斯原语实现高质量渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)尺度自适应修剪(SAP)策略，减少点云大小同时保持质量；2)增强的差分高斯光栅化模块，集成光学流约束和梯度权重计算；3)专门针对正畸场景的优化，处理反射、模糊等问题。不同之处：专为三张稀疏图像设计，而其他方法假设更多输入；解决DUSt3R密集点云导致的计算效率问题；结合多种约束提高复杂牙齿结构渲染质量；无需相机姿态信息也能工作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DentalSplat首次实现了从稀疏、无姿态的口腔内图像中快速（一分钟内）生成高质量牙齿咬合3D重建和新视角合成的方法，显著优于现有技术，为远程正畸治疗提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In orthodontic treatment, particularly within telemedicine contexts,observing patients' dental occlusion from multiple viewpoints facilitatestimely clinical decision-making. Recent advances in 3D Gaussian Splatting(3DGS) have shown strong potential in 3D reconstruction and novel viewsynthesis. However, conventional 3DGS pipelines typically rely on denselycaptured multi-view inputs and precisely initialized camera poses, limitingtheir practicality. Orthodontic cases, in contrast, often comprise only threesparse images, specifically, the anterior view and bilateral buccal views,rendering the reconstruction task especially challenging. The extreme sparsityof input views severely degrades reconstruction quality, while the absence ofcamera pose information further complicates the process. To overcome theselimitations, we propose DentalSplat, an effective framework for 3Dreconstruction from sparse orthodontic imagery. Our method leverages aprior-guided dense stereo reconstruction model to initialize the point cloud,followed by a scale-adaptive pruning strategy to improve the trainingefficiency and reconstruction quality of 3DGS. In scenarios with extremelysparse viewpoints, we further incorporate optical flow as a geometricconstraint, coupled with gradient regularization, to enhance renderingfidelity. We validate our approach on a large-scale dataset comprising 950clinical cases and an additional video-based test set of 195 cases designed tosimulate real-world remote orthodontic imaging conditions. Experimental resultsdemonstrate that our method effectively handles sparse input scenarios andachieves superior novel view synthesis quality for dental occlusionvisualization, outperforming state-of-the-art techniques.</description>
      <author>example@mail.com (Yiyi Miao, Taoyu Wu, Tong Chen, Sihao Li, Ji Jiang, Youpeng Yang, Angelos Stefanidis, Limin Yu, Jionglong Su)</author>
      <guid isPermaLink="false">2511.03099v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth</title>
      <link>http://arxiv.org/abs/2511.03053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于学习的MLS点云不确定性评估框架，结合最优邻域估计和几何特征提取，实验证明其可行且高效。&lt;h4&gt;背景&lt;/h4&gt;移动激光扫描点云在许多高精度应用（如扫描到建筑信息模型、变形分析和三维建模）中的可靠使用依赖于不确定性评估。然而，在许多实际应用中，获取用于评估的地面真实值通常成本高昂且不可行。&lt;h4&gt;目的&lt;/h4&gt;减少不确定性评估研究中对地面真实值的长期依赖，提出一种基于学习的框架用于MLS点云的不确定性评估。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于学习的框架，结合最优邻域估计和几何特征提取，使用XGBoost模型进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;提出的框架可行；XGBoost模型与随机森林相比具有完全相当的准确性，同时效率更高（快约3倍）；几何特征可用于预测由点到点距离量化的点级不确定性。&lt;h4&gt;结论&lt;/h4&gt;MLS点云的不确定性是可以学习的，为不确定性评估研究提供了新的基于学习的视角。&lt;h4&gt;翻译&lt;/h4&gt;评估不确定性对于移动激光扫描点云在许多高精度应用（如扫描到建筑信息模型、变形分析和三维建模）中的可靠使用至关重要。然而，在许多实际应用中，获取用于评估的地面真实值通常成本高昂且不可行。为了减少不确定性评估研究中对地面真实值的长期依赖，本研究提出了一个MLS点云的基于学习框架，结合了最优邻域估计和几何特征提取。在真实世界数据集上的实验表明，所提出的框架是可行的，XGBoost模型与随机森林相比具有完全相当的准确性，同时实现了更高的效率（快约3倍），初步证明了几何特征可用于预测由点到点距离量化的点级不确定性。总之，这项研究表明MLS点云的不确定性是可以学习的，为不确定性评估研究提供了新的基于学习的视角。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决MLS点云不确定性评估中过度依赖地面真实值(GT)的问题。这个问题很重要，因为在高精度应用如Scan-to-BIM、变形分析和3D建模中，不仅需要准确几何信息，还需要可靠的不确定性估计；而传统方法要么难以全面建模所有误差源，要么获取GT成本过高，限制了MLS技术的广泛应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统前向和后向建模方法的局限性，特别是后向建模对GT的依赖问题。然后转向学习-based方法，将不确定性评估转化为监督学习问题，学习点的误差与局部几何特征间的关系。方法设计借鉴了TLS测量中使用集成方法的研究成果，采用了点云分类中表现良好的几何特征提取方法，以及最优邻域估计策略，但将其首次应用于真实世界的MLS数据集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是点级不确定性可以通过局部几何特征学习和预测，无需为每个新场景都获取GT。流程包括：1)数据准备(使用C2C距离量化不确定性，筛选C2C&lt;80mm的点)；2)特征工程(估计最优邻域，提取26种几何特征)；3)模型训练(Random Forest和XGBoost两种集成学习模型)；4)模型测试(使用多种评估指标和可视化)；5)结果分析(比较模型性能，进行特征重要性分析)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个针对MLS点云的学习-based不确定性评估框架；2)集成最优邻域估计与几何特征提取；3)证明XGBoost在保持与RF相当精度的同时效率更高；4)通过SHAP和排列重要性分析揭示关键几何特征。相比之前工作，本文从TLS扩展到复杂MLS场景，从实验室条件扩展到真实世界，使用C2C距离提供更敏感的局部误差表征，并减少了GT需求，提高了实用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文首次提出了一种基于机器学习的MLS点云点级不确定性评估框架，通过几何特征预测不确定性，减少了对地面真实值的依赖，同时证明了XGBoost模型在保持与随机森林相当精度的同时具有更高的计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning(MLS) point clouds in many high-precision applications such as Scan-to-BIM,deformation analysis, and 3D modeling. However, obtaining the ground truth (GT)for evaluation is often costly and infeasible in many real-world applications.To reduce this long-standing reliance on GT in uncertainty evaluation research,this study presents a learning-based framework for MLS point clouds thatintegrates optimal neighborhood estimation with geometric feature extraction.Experiments on a real-world dataset show that the proposed framework isfeasible and the XGBoost model delivers fully comparable accuracy to RandomForest while achieving substantially higher efficiency (about 3 times faster),providing initial evidence that geometric features can be used to predictpoint-level uncertainty quantified by the C2C distance. In summary, this studyshows that MLS point clouds' uncertainty is learnable, offering a novellearning-based viewpoint towards uncertainty evaluation research.</description>
      <author>example@mail.com (Ziyang Xu, Olaf Wysocki, Christoph Holst)</author>
      <guid isPermaLink="false">2511.03053v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Curvature of high-dimensional data</title>
      <link>http://arxiv.org/abs/2511.02873v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了从带噪声的样本数据中估计曲率的问题，探讨了高维情况下曲率估计的偏差问题，并提出了一种改进的概率框架来构建更准确的曲率估计器。&lt;h4&gt;背景&lt;/h4&gt;对于维度大于一的流形，存在多种局部曲率的定义，每种定义对应不同的估计过程。最近的研究证明了'局部点云曲率'估计会随着点云密度趋近于无限而收敛到相关的局部曲率光滑概念。&lt;h4&gt;目的&lt;/h4&gt;研究收敛定理的实际局限性，分析曲率估计中偏差的显著影响，特别是在高维情况下，并提出构建更准确曲率估计器的方法。&lt;h4&gt;方法&lt;/h4&gt;提出一个概率框架，为任意噪声模型构建更准确的曲率估计器，并在高达十二维的球体上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;偏差在高维情况下会急剧增加，在高维情况下，朴素曲率估计落在真实曲率附近小区间内的概率可能接近于零；提出的概率框架能够构建更准确的曲率估计器。&lt;h4&gt;结论&lt;/h4&gt;在高维流形中，曲率估计面临显著的偏差挑战，但通过提出的概率框架可以有效提高估计的准确性。&lt;h4&gt;翻译&lt;/h4&gt;我们考虑估计曲率的问题，其中数据可以被视为来自基础流形的噪声样本。对于维度大于一的流形，存在多种局部曲率的定义，每种定义对给定数据集提出了不同的估计过程。最近，在证明'局部点云曲率'估计随着点云密度趋近于无限而收敛到相关的局部曲率光滑概念方面取得了进展。在此，我们研究了这些收敛定理的实际局限性，并讨论了最近文献中报道的此类估计中偏差的显著影响。我们提供了理论论证，证明偏差在高维情况下急剧增加，以至于在高维情况下，朴素曲率估计落在真实曲率附近小区间内的概率可能接近于零。我们提出了一个概率框架，能够为任意噪声模型构建更准确的曲率估计器。我们在高达十二维的球体上的实验支持了我们技术的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider the problem of estimating curvature where the data can be viewedas a noisy sample from an underlying manifold. For manifolds of dimensiongreater than one there are multiple definitions of local curvature, eachsuggesting a different estimation process for a given data set. Recently, therehas been progress in proving that estimates of ``local point cloud curvature"converge to the related smooth notion of local curvature as the density of thepoint cloud approaches infinity. Herein we investigate practical limitations ofsuch convergence theorems and discuss the significant impact of bias in suchestimates as reported in recent literature. We provide theoretical argumentsfor the fact that bias increases drastically in higher dimensions, so much sothat in high dimensions, the probability that a naive curvature estimate liesin a small interval near the true curvature could be near zero. We present aprobabilistic framework that enables the construction of more accurateestimators of curvature for arbitrary noise models. The efficacy of ourtechnique is supported with experiments on spheres of dimension as large astwelve.</description>
      <author>example@mail.com (Jiayi Chen, Mohammad Javad Latifi Jebelli, Daniel N. Rockmore)</author>
      <guid isPermaLink="false">2511.02873v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>POEMS: Product of Experts for Interpretable Multi-omic Integration using Sparse Decoding</title>
      <link>http://arxiv.org/abs/2511.03464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;POEMS是一种新的无监督概率框架，用于整合多组学数据，通过稀疏解码和专家乘积模型，在保持预测性能的同时提供可解释性，实现了生物标志物发现和跨组学关联。&lt;h4&gt;背景&lt;/h4&gt;整合不同分子层（即多组学数据）对于理解疾病复杂性至关重要；然而，大多数深度生成模型要么优先考虑预测性能而牺牲可解释性，要么通过线性化解码器来强制可解释性，从而削弱了网络的非线性表达能力。&lt;h4&gt;目的&lt;/h4&gt;克服预测性能和可解释性之间的权衡，开发一种能够同时保持预测性能并提供可解释性的无监督概率框架。&lt;h4&gt;方法&lt;/h4&gt;引入POEMS框架，通过以下方式提供可解释性而不需要线性化网络的任何部分：1)使用稀疏连接将特征映射到潜在因子，直接转化为生物标志物发现；2)使用专家乘积模型通过共享潜在空间实现跨组学关联；3)通过门控网络报告每个组学的贡献，该网络自适应地计算它们在表示学习中的影响；此外，还提出了一种高效的稀疏解码器。&lt;h4&gt;主要发现&lt;/h4&gt;在癌症亚型分型案例研究中，POEMS实现了具有竞争力的聚类和分类性能，同时提供了一套新的解释方法，证明基于生物标志物的洞察力和预测准确性可以在多组学表示学习中共存。&lt;h4&gt;结论&lt;/h4&gt;POEMS框架成功整合了预测性能和可解释性，证明在多组学表示学习中，生物标志物洞察力和预测准确性可以共存。&lt;h4&gt;翻译&lt;/h4&gt;整合不同的分子层，即多组学数据，对于揭示疾病复杂性至关重要；然而，大多数深度生成模型要么优先考虑预测性能而牺牲可解释性，要么通过线性化解码器来强制可解释性，从而削弱了网络的非线性表达能力。为了克服这种权衡，我们引入了POEMS：使用稀疏解码的可解释多组学集成的专家乘积模型，这是一个无监督概率框架，在提供可解释性的同时保持预测性能。POEMS通过以下方式在不线性化网络任何部分的情况下提供可解释性：1)使用稀疏连接将特征映射到潜在因子，直接转化为生物标志物发现；2)通过专家乘积模型使用共享潜在空间实现跨组学关联；3)通过门控网络报告每个组学的贡献，该网络自适应地计算它们在表示学习中的影响。此外，我们还提出了一种高效的稀疏解码器。在癌症亚型分型案例研究中，POEMS实现了具有竞争力的聚类和分类性能，同时提供了一套新的解释方法，证明了基于生物标志物的洞察力和预测准确性可以在多组学表示学习中共存。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating different molecular layers, i.e., multiomics data, is crucial forunraveling the complexity of diseases; yet, most deep generative models eitherprioritize predictive performance at the expense of interpretability or enforceinterpretability by linearizing the decoder, thereby weakening the network'snonlinear expressiveness. To overcome this tradeoff, we introduce POEMS:Product Of Experts for Interpretable Multiomics Integration using SparseDecoding, an unsupervised probabilistic framework that preserves predictiveperformance while providing interpretability. POEMS provides interpretabilitywithout linearizing any part of the network by 1) mapping features to latentfactors using sparse connections, which directly translates to biomarkerdiscovery, 2) allowing for cross-omic associations through a shared latentspace using product of experts model, and 3) reporting contributions of eachomic by a gating network that adaptively computes their influence in therepresentation learning. Additionally, we present an efficient sparse decoder.In a cancer subtyping case study, POEMS achieves competitive clustering andclassification performance while offering our novel set of interpretations,demonstrating that biomarker based insight and predictive accuracy can coexistin multiomics representation learning.</description>
      <author>example@mail.com (Mihriban Kocak Balik, Pekka Marttinen, Negar Safinianaini)</author>
      <guid isPermaLink="false">2511.03464v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology</title>
      <link>http://arxiv.org/abs/2511.02946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究介绍了一个名为ProM3E的概率掩码多模态嵌入模型，用于生态学中的多模态表示生成。该模型基于嵌入空间中的掩码模态重建，支持模态反转，能够分析模态融合的可行性，并提出了新的跨模态检索方法，展示了优越的表示学习能力。&lt;h4&gt;背景&lt;/h4&gt;生态学研究需要处理多种模态的数据，但现有方法可能无法有效处理任意模态间的转换和融合。需要一种能够处理多模态数据并支持任意模态间转换的模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够支持生态学多模态表示任意生成的模型，学习推断缺失的模态，分析模态融合的可行性，并提高跨模态检索性能。&lt;h4&gt;方法&lt;/h4&gt;提出ProM3E模型，基于嵌入空间中的掩码模态重建；支持模态反转功能；利用概率性质分析模态融合的可行性；提出结合模态间和模态内相似性的跨模态检索方法；使用隐藏表示进行线性探测任务。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的跨模态检索方法在所有检索任务中实现了优越的性能；模型展示了卓越的表示学习能力；能够有效分析不同模态融合的可行性。&lt;h4&gt;结论&lt;/h4&gt;ProM3E模型为生态学多模态数据的处理提供了有效解决方案，支持任意模态间的转换和融合，并在跨模态检索和表示学习任务中表现出色。研究团队将公开代码、数据集和模型以促进进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了ProM3E，一个用于生态学多模态表示的任意生成概率掩码多模态嵌入模型。ProM3E基于嵌入空间中的掩码模态重建，学习在给定少量上下文模态的情况下推断缺失的模态。根据设计，我们的模型支持嵌入空间中的模态反转。我们模型的概率性质使我们能够分析融合各种模态以用于给定下游任务的可行性，本质上学习融合什么。利用我们模型的这些特性，我们提出了一种新的跨模态检索方法，该方法结合了模态间和模态内相似性，以在所有检索任务中实现卓越的性能。我们进一步利用我们模型的隐藏表示来执行线性探测任务，并展示了我们模型卓越的表示学习能力。我们所有的代码、数据集和模型将在https://vishu26.github.io/prom3e上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce ProM3E, a probabilistic masked multimodal embedding model forany-to-any generation of multimodal representations for ecology. ProM3E isbased on masked modality reconstruction in the embedding space, learning toinfer missing modalities given a few context modalities. By design, our modelsupports modality inversion in the embedding space. The probabilistic nature ofour model allows us to analyse the feasibility of fusing various modalities forgiven downstream tasks, essentially learning what to fuse. Using these featuresof our model, we propose a novel cross-modal retrieval approach that mixesinter-modal and intra-modal similarities to achieve superior performance acrossall retrieval tasks. We further leverage the hidden representation from ourmodel to perform linear probing tasks and demonstrate the superiorrepresentation learning capability of our model. All our code, datasets andmodel will be released at https://vishu26.github.io/prom3e.</description>
      <author>example@mail.com (Srikumar Sastry, Subash Khanal, Aayush Dhakal, Jiayu Lin, Dan Cher, Phoenix Jarosz, Nathan Jacobs)</author>
      <guid isPermaLink="false">2511.02946v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations</title>
      <link>http://arxiv.org/abs/2511.02776v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了XR-1框架，通过统一视觉运动编码(UVMC)和三阶段训练范式解决了视觉语言动作模型面临的两个基本挑战：从高维观测中产生精确低级动作和弥合跨异构数据源的领域差距。&lt;h4&gt;背景&lt;/h4&gt;大规模机器人数据集和视觉语言模型的发展推动了VLA模型研究，但现有模型面临两个挑战：从高维观测产生精确低级动作，以及弥合不同机器人形态和人类演示数据之间的领域差距。现有方法未能充分利用大规模异构数据集中的互补多模态知识。&lt;h4&gt;目的&lt;/h4&gt;开发一个多功能可扩展的VLA学习框架(XR-1)，能够在多样化机器人、任务和环境上有效工作。&lt;h4&gt;方法&lt;/h4&gt;引入统一视觉运动编码(UVMC)，一种通过双分支VQ-VAE学习的离散潜在表示，同时编码视觉动力学和机器人运动。采用三阶段训练范式：自监督的UVMC学习、UVMC引导的大规模跨形态机器人数据集预训练、以及任务特定的后训练。&lt;h4&gt;主要发现&lt;/h4&gt;在六种不同机器人形态上进行了超过14,000次滚动的真实世界实验，涵盖120多种操作任务。XR-1持续优于π₀.₅、π₀、RDT、UniVLA和GR00T-N1.5等基线方法，并对新物体、背景变化、干扰物和光照变化展现出强大泛化能力。&lt;h4&gt;结论&lt;/h4&gt;XR-1通过UVMC和三阶段训练范式成功解决了VLA模型的关键挑战，在多样化场景中实现了优越的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;最近大规模机器人数据集和视觉语言模型(VLMs)的进展推动了视觉语言动作(VLA)模型的研究。然而，现有VLA模型仍面临两个基本挑战：(i)从高维观测中产生精确的低级动作，(ii)弥合跨异构数据源的领域差距，包括多样化的机器人形态和人类演示。现有方法通常从视觉动力学或机器人动作中编码潜在变量来指导策略学习，但它们未能充分利用大规模、异构数据集中存在的互补多模态知识。在这项工作中，我们提出了X机器人模型1(XR-1)，一个适用于多样化机器人、任务和环境的多功能可扩展VLA学习框架。XR-1引入了统一视觉运动编码(UVMC)，一种通过双分支VQ-VAE学习的离散潜在表示，可同时编码视觉动力学和机器人运动。UVMC通过(i)作为观测和动作之间的中间表示，和(ii)对齐来自异构数据源的多模态动态信息以捕获互补知识来解决这些挑战。为了有效利用UVMC，我们提出了三阶段训练范式：(i)自监督的UVMC学习，(ii)在大型跨形态机器人数据集上进行UVMC引导的预训练，和(iii)任务特定的后训练。我们通过在六种不同机器人形态上进行超过14,000次滚动的广泛真实世界实验验证了XR-1，涵盖120多种不同的操作任务。XR-1在性能上持续优于最先进的基线方法，如π₀.₅、π₀、RDT、UniVLA和GR00T-N1.5，同时展现出对新物体、背景变化、干扰物和光照变化的强大泛化能力。我们的项目网址是https://xr-1-vla.github.io/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决两个问题：1) 从高维观测生成精确的低级行动困难；2) 跨形态数据集利用受到形态异质性阻碍。这些问题在现实中很重要，因为精确的低级行动对机器人执行实际任务至关重要，特别是在需要精确操作的场景；而有效利用跨形态数据集可以提高机器人学习的数据效率和泛化能力，推动机器人向更通用、适应性强的方向发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有方法要么只编码视觉动态，要么只编码机器人动作，没有充分利用多模态知识。受人类认知启发——人类自然将异构感官输入整合成跨模态代码，作者设计了双分支VQ-VAE架构，将视觉动态和机器人运动编码到共享的离散潜在空间中，并添加了对齐损失强制视觉代码与运动代码保持一致。该方法借鉴了VQ-VAE架构、大规模预训练数据集和视觉-语言模型，但创新性地将它们整合用于统一的视觉-运动表示学习。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学习统一的视觉-运动表示(UVMC)实现跨模态对齐，作为观测和动作之间的中间表示。整体实现流程分为三阶段：1) 第一阶段学习UVMC，使用双分支VQ-VAE分别编码视觉动态和机器人运动到共享潜在空间，并添加跨模态对齐损失；2) 第二阶段UVMC指导预训练，将UVMC作为监督信号注入VLM；3) 第三阶段任务特定后训练，微调VLA策略提高特定任务性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一的视觉-运动表示(UVMC)同时编码视觉和运动信息；2) 三阶段训练范式增加UVMC学习阶段；3) 跨模态对齐损失强制视觉与运动代码一致；4) 模型无关设计可灵活应用于不同VLA架构。相比之前工作，XR-1不仅利用视觉信息还整合机器人运动信息，不需要大量标记的机器人动作数据，能从人类演示视频中学习，并通过增加UVMC学习阶段更有效利用异构数据源。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; XR-1通过学习统一的视觉-运动表示，实现了跨数据利用、跨模态对齐和跨形态控制的通用视觉-语言-动作模型，显著提高了机器人在多样化任务和环境中的性能和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in large-scale robotic datasets and vision-language models(VLMs) has advanced research on vision-language-action (VLA) models. However,existing VLA models still face two fundamental challenges: (i) producingprecise low-level actions from high-dimensional observations, (ii) bridgingdomain gaps across heterogeneous data sources, including diverse robotembodiments and human demonstrations. Existing methods often encode latentvariables from either visual dynamics or robotic actions to guide policylearning, but they fail to fully exploit the complementary multi-modalknowledge present in large-scale, heterogeneous datasets. In this work, wepresent X Robotic Model 1 (XR-1), a novel framework for versatile and scalableVLA learning across diverse robots, tasks, and environments. XR-1 introducesthe \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representationlearned via a dual-branch VQ-VAE that jointly encodes visual dynamics androbotic motion. UVMC addresses these challenges by (i) serving as anintermediate representation between the observations and actions, and (ii)aligning multimodal dynamic information from heterogeneous data sources tocapture complementary knowledge. To effectively exploit UVMC, we propose athree-stage training paradigm: (i) self-supervised UVMC learning, (ii)UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and(iii) task-specific post-training. We validate XR-1 through extensivereal-world experiments with more than 14,000 rollouts on six different robotembodiments, spanning over 120 diverse manipulation tasks. XR-1 consistentlyoutperforms state-of-the-art baselines such as $\pi_{0.5}$, $\pi_0$, RDT,UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novelobjects, background variations, distractors, and illumination changes. Ourproject is at https://xr-1-vla.github.io/.</description>
      <author>example@mail.com (Shichao Fan, Kun Wu, Zhengping Che, Xinhua Wang, Di Wu, Fei Liao, Ning Liu, Yixue Zhang, Zhen Zhao, Zhiyuan Xu, Meng Li, Qingjie Liu, Shanghang Zhang, Min Wan, Jian Tang)</author>
      <guid isPermaLink="false">2511.02776v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>DANIEL: A Distributed and Scalable Approach for Global Representation Learning with EHR Applications</title>
      <link>http://arxiv.org/abs/2511.02754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究重新审视了Ising模型，开发了一个分布式框架，使具有内在低秩结构的大规模二元数据的可扩展和隐私保护表示学习成为可能。通过双因子梯度下降优化非凸替代损失函数，在多机构电子健康记录数据集上验证了算法的优越性，突显了在高维环境中进行统计推断的潜力。&lt;h4&gt;背景&lt;/h4&gt;传统的概率图模型在现代数据环境中面临基本挑战，这些数据环境具有高维度、源异构性和严格的数据共享限制。&lt;h4&gt;目的&lt;/h4&gt;重新审视Ising模型，并开发一个分布式框架，使具有内在低秩结构的大规模二元数据的可扩展和隐私保护表示学习成为可能。&lt;h4&gt;方法&lt;/h4&gt;通过双因子梯度下降优化非凸替代损失函数，与传统的凸方法相比，提供了显著的计算和通信优势。&lt;h4&gt;主要发现&lt;/h4&gt;在匹兹堡大学医学中心和马萨诸塞州总医院的58,248名患者的多机构电子健康记录数据集上评估了算法，在全局表示学习和下游临床任务（包括关系检测、患者表型和患者聚类）中表现出优越性能。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了在联邦、高维环境中进行统计推断的更广泛潜力，同时解决了数据复杂性和多机构整合的实际挑战。&lt;h4&gt;翻译&lt;/h4&gt;传统的概率图模型在现代数据环境中面临基本挑战，这些数据环境具有高维度、源异构性和严格的数据共享限制。在本工作中，我们重新审视了Ising模型，它是马尔可夫随机场家族中一个成熟的成员，并开发了一个分布式框架，使具有内在低秩结构的大规模二元数据的可扩展和隐私保护表示学习成为可能。我们的方法通过双因子梯度下降优化非凸替代损失函数，与传统的凸方法相比，提供了显著的计算和通信优势。我们在匹兹堡大学医学中心和马萨诸塞州总医院的58,248名患者的多机构电子健康记录数据集上评估了我们的算法，在全局表示学习和下游临床任务（包括关系检测、患者表型和患者聚类）中表现出优越性能。这些结果突显了在联邦、高维环境中进行统计推断的更广泛潜力，同时解决了数据复杂性和多机构整合的实际挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classical probabilistic graphical models face fundamental challenges inmodern data environments, which are characterized by high dimensionality,source heterogeneity, and stringent data-sharing constraints. In this work, werevisit the Ising model, a well-established member of the Markov Random Field(MRF) family, and develop a distributed framework that enables scalable andprivacy-preserving representation learning from large-scale binary data withinherent low-rank structure. Our approach optimizes a non-convex surrogate lossfunction via bi-factored gradient descent, offering substantial computationaland communication advantages over conventional convex approaches. We evaluateour algorithm on multi-institutional electronic health record (EHR) datasetsfrom 58,248 patients across the University of Pittsburgh Medical Center (UPMC)and Mass General Brigham (MGB), demonstrating superior performance in globalrepresentation learning and downstream clinical tasks, including relationshipdetection, patient phenotyping, and patient clustering. These results highlighta broader potential for statistical inference in federated, high-dimensionalsettings while addressing the practical challenges of data complexity andmulti-institutional integration.</description>
      <author>example@mail.com (Zebin Wang, Ziming Gan, Weijing Tang, Zongqi Xia, Tianrun Cai, Tianxi Cai, Junwei Lu)</author>
      <guid isPermaLink="false">2511.02754v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Modality-Transition Representation Learning for Visible-Infrared Person Re-Identification</title>
      <link>http://arxiv.org/abs/2511.02685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为模态转换表示学习（MTRL）的新型VI-ReID框架，通过中间生成的图像作为模态转换的桥梁，有效对齐可见光和红外模态的特征，无需额外参数即可提升性能。&lt;h4&gt;背景&lt;/h4&gt;可见光-红外行人重识别（VI-ReID）技术能够在背景光照变化场景中关联不同模态的行人图像，但可见光和红外模态间存在本质差距。现有方法主要依赖中间表示来对齐跨模态特征，但这些方法要么通过生成中间图像（数据增强），要么融合中间特征（参数多，可解释性差），且未能充分利用中间特征。&lt;h4&gt;目的&lt;/h4&gt;解决可见光和红外模态间的差距问题，改进现有的VI-ReID方法，使其能够更有效地对齐跨模态特征，同时保持推理速度不增加额外参数。&lt;h4&gt;方法&lt;/h4&gt;提出模态转换表示学习（MTRL）框架，使用中间生成的图像作为从可见光到红外模态的传输器，这些图像与原始可见光图像完全对齐且与红外模态相似。采用模态转换对比损失和模态查询正则化损失进行训练，实现更有效的跨模态特征对齐。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的MTRL框架不需要额外参数，保持与骨干网络相同的推理速度，同时在VI-ReID任务上性能得到提升。在三个典型VI-ReID数据集上的实验结果表明，该方法显著且一致地优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;通过模态转换表示学习框架，可以有效解决可见光和红外模态间的差距问题，提升VI-ReID性能，且不增加计算负担，为跨模态行人重识别提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;可见光-红外行人重识别（VI-ReID）技术能够在实际场景中关联可见光和红外模态的行人图像，特别是在背景光照变化的情况下。然而，这两种模态之间本质上存在显著差距。此外，现有方法主要依靠中间表示来对齐同一人的跨模态特征。这些中间特征表示通常是通过生成中间图像（一种数据增强）或融合中间特征（参数更多，可解释性差）来创建的，并且它们没有很好地利用中间特征。因此，我们提出了一种通过模态转换表示学习（MTRL）的新型VI-ReID框架，使用中间生成的图像作为从可见光到红外模态的传输器，这些图像与原始可见光图像完全对齐，并且与红外模态相似。之后，使用模态转换对比损失和模态查询正则化损失进行训练，可以更有效地对齐跨模态特征。值得注意的是，我们提出的框架不需要任何额外参数，在保持与骨干网络相同推理速度的同时，提高了其在VI-ReID任务上的性能。大量实验结果表明，在三个典型的VI-ReID数据集上，我们的模型显著且一致地优于现有的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visible-infrared person re-identification (VI-ReID) technique could associatethe pedestrian images across visible and infrared modalities in the practicalscenarios of background illumination changes. However, a substantial gapinherently exists between these two modalities. Besides, existing methodsprimarily rely on intermediate representations to align cross-modal features ofthe same person. The intermediate feature representations are usually create bygenerating intermediate images (kind of data enhancement), or fusingintermediate features (more parameters, lack of interpretability), and they donot make good use of the intermediate features. Thus, we propose a novelVI-ReID framework via Modality-Transition Representation Learning (MTRL) with amiddle generated image as a transmitter from visible to infrared modals, whichare fully aligned with the original visible images and similar to the infraredmodality. After that, using a modality-transition contrastive loss and amodality-query regularization loss for training, which could align thecross-modal features more effectively. Notably, our proposed framework does notneed any additional parameters, which achieves the same inference speed to thebackbone while improving its performance on VI-ReID task. Extensiveexperimental results illustrate that our model significantly and consistentlyoutperforms existing SOTAs on three typical VI-ReID datasets.</description>
      <author>example@mail.com (Chao Yuan, Zanwu Liu, Guiwei Zhang, Haoxuan Xu, Yujian Zhao, Guanglin Niu, Bo Li)</author>
      <guid isPermaLink="false">2511.02685v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Using Deep Learning for Robust Classification of Fast Radio Bursts</title>
      <link>http://arxiv.org/abs/2511.02634v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 4 figures, 9 tables. Comments are welcome&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究使用深度学习方法对快速射电暴进行分类并分析其潜在结构模式，通过监督变分自编码器模型实现了高分类准确率，并揭示了重复发射器与非重复发射器之间的特征差异。&lt;h4&gt;背景&lt;/h4&gt;快速射电暴的本质仍然未知，但群体层面的分析可以阐明这些信号中的潜在结构。研究使用了第一个CHIME目录中的数据。&lt;h4&gt;目的&lt;/h4&gt;使用深度学习方法对FRBs进行分类，并分析从CHIME目录中学习的潜在空间中的结构模式。&lt;h4&gt;方法&lt;/h4&gt;采用监督变分自编码器(sVAE)架构，结合变分自编码器的表示学习能力和监督分类任务，构建学习到的潜在空间并进行进一步降维以寻找数据中的潜在结构。&lt;h4&gt;主要发现&lt;/h4&gt;sVAE模型对FRB重复发射器实现了高分类准确率，揭示了重复发射器和非重复发射器群体之间的分离。色散测量过剩、光谱指数和光谱运行是区分重复发射器和非重复发射器的主导特征。研究还确定了四个非重复FRBs作为重复发射器候选者，其中两个在先前研究中已被独立标记。&lt;h4&gt;结论&lt;/h4&gt;深度学习方法可以有效地对FRBs进行分类并揭示其潜在结构，重复发射器和非重复发射器之间存在可区分的特征差异。&lt;h4&gt;翻译&lt;/h4&gt;尽管快速射电暴的性质仍然未知，但群体层面的分析可以阐明这些信号中的潜在结构。在本研究中，我们采用深度学习方法来对FRBs进行分类，并分析从第一个CHIME目录中学习的潜在空间中的结构模式。我们采用监督变分自编码器架构，该架构结合了变分自编码器的表示学习能力和监督分类任务，从而提高了分类性能和潜在空间的可解释性。我们在构建的潜在空间中执行进一步的降维，以寻找数据中的潜在结构。我们的结果表明，sVAE模型对FRB重复发射器实现了高分类准确率，并揭示了重复发射器与非重复发射器群体之间的分离。通过对潜在空间的进一步分析，我们观察到色散测量过剩、光谱指数和光谱运行是区分重复发射器与非重复发射器的主导特征。我们还确定了四个非重复FRBs作为重复发射器候选者，其中两个在先前研究中已被独立标记。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While the nature of fast radio bursts (FRBs) remains unknown,population-level analyses can elucidate underlying structure in these signals.In this study, we employ deep learning methods to both classify FRBs andanalyze structural patterns in the latent space learned from the first CHIMEcatalog. We adopt a Supervised Variational Autoencoder (sVAE) architecturewhich combines the representational learning capabilities of VariationalAutoencoders (VAEs) with a supervised classification task, thereby improvingboth classification performance and the interpretability of the latent space.We construct a learned latent space in which we perform further dimensionalityreduction to find underlying structure in the data. Our results demonstratethat the sVAE model achieves high classification accuracy for FRB repeaters andreveals separation between repeater and non-repeater populations. Upon furtheranalysis of the latent space, we observe that dispersion measure excess,spectral index, and spectral running are the dominant features distinguishingrepeaters from non-repeaters. We also identify four non-repeating FRBs asrepeater candidates, two of which have been independently flagged in previousstudies.</description>
      <author>example@mail.com (Rohan Arni, Carlos Blanco, Anirudh Prabhu)</author>
      <guid isPermaLink="false">2511.02634v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>SKGE: Spherical Knowledge Graph Embedding with Geometric Regularization</title>
      <link>http://arxiv.org/abs/2511.02460v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了球面知识图谱嵌入(SKGE)模型，通过将实体表示限制在超球面上，克服了传统欧几里得空间KGE模型的局限性。实验证明SKGE在多个基准测试上优于TransE模型，特别是在大规模数据集上表现出色。研究表明几何约束作为正则化器可以提高性能，且球面几何创造了更好的负采样环境。&lt;h4&gt;背景&lt;/h4&gt;知识图谱嵌入(KGE)已成为多关系数据表示学习的基本技术。许多经典模型(如TransE)在无界欧几里得空间中操作，这种方法在建模复杂关系时存在固有局限性，可能导致训练效率低下。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的模型来挑战现有范式，通过将实体表示限制在紧凑流形(超球面)上，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了球面知识图谱嵌入(SKGE)模型，使用可学习的非线性球化层将实体映射到球面上，并将关系解释为混合平移-投影变换。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准数据集(FB15k-237, CoDEx-S, CoDEx-M)上的广泛实验表明，SKGE一致且显著优于TransE模型，特别是在大型基准测试上表现更佳。球面几何先验的有效性得到验证，几何约束作为一种强大的正则化器，导致所有关系类型的性能全面提升。球面几何创造了'内在困难负采样'环境，自然消除平凡负样本，迫使模型学习更强大和语义一致的表示。&lt;h4&gt;结论&lt;/h4&gt;流形的选择不仅仅是实现细节，而是基本设计原则。倡议将几何先验作为设计下一代强大且稳定的KGE模型的基础。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱嵌入(KGE)已成为多关系数据表示学习的基本技术。许多开创性模型，如TransE，在无界欧几里得空间中运行，这在建模复杂关系时存在固有局限性，并可能导致训练效率低下。在本文中，我们提出了球面知识图谱嵌入(SKGE)模型，通过将实体表示限制在紧凑流形(超球面)上，挑战了这一范式。SKGE使用可学习的非线性球化层将实体映射到球面上，并将关系解释为混合平移-投影变换。通过对三个基准数据集FB15k-237、CoDEx-S和CoDEx-M的广泛实验，我们证明SKGE一致且显著地优于其强大的欧几里得对应模型TransE，特别是在FB15k-237和CoDEx-M等大规模基准测试上，证明了球面几何先验的有效性。我们提供了深入分析以揭示这种优势的来源，表明这种几何约束作为一种强大的正则化器，导致所有关系类型的全面性能提升。更根本的是，我们证明了球面几何创造了'内在困难负采样'环境，自然消除平凡负样本，迫使模型学习更强大和语义一致的表示。我们的发现有力地证明了流形的选择不仅仅是实现细节，而是基本设计原则，倡导将几何先验作为设计下一代强大且稳定的KGE模型的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge graph embedding (KGE) has become a fundamental technique forrepresentation learning on multi-relational data. Many seminal models, such asTransE, operate in an unbounded Euclidean space, which presents inherentlimitations in modeling complex relations and can lead to inefficient training.In this paper, we propose Spherical Knowledge Graph Embedding (SKGE), a modelthat challenges this paradigm by constraining entity representations to acompact manifold: a hypersphere. SKGE employs a learnable, non-linearSpherization Layer to map entities onto the sphere and interprets relations asa hybrid translate-then-project transformation. Through extensive experimentson three benchmark datasets, FB15k-237, CoDEx-S, and CoDEx-M, we demonstratethat SKGE consistently and significantly outperforms its strong Euclideancounterpart, TransE, particularly on large-scale benchmarks such as FB15k-237and CoDEx-M, demonstrating the efficacy of the spherical geometric prior. Weprovide an in-depth analysis to reveal the sources of this advantage, showingthat this geometric constraint acts as a powerful regularizer, leading tocomprehensive performance gains across all relation types. More fundamentally,we prove that the spherical geometry creates an "inherently hard negativesampling" environment, naturally eliminating trivial negatives and forcing themodel to learn more robust and semantically coherent representations. Ourfindings compellingly demonstrate that the choice of manifold is not merely animplementation detail but a fundamental design principle, advocating forgeometric priors as a cornerstone for designing the next generation of powerfuland stable KGE models.</description>
      <author>example@mail.com (Xuan-Truong Quan, Xuan-Son Quan, Duc Do Minh, Vinh Nguyen Van)</author>
      <guid isPermaLink="false">2511.02460v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Measuring the Intrinsic Dimension of Earth Representations</title>
      <link>http://arxiv.org/abs/2511.02101v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Pre-print. 27 pages, 11 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究首次探讨了地理隐式神经表示(INRs)的内在维度特性，发现其内在维度在2到10之间，且与下游任务性能相关，为无监督评估模型提供了新方法。&lt;h4&gt;背景&lt;/h4&gt;地理隐式神经表示(INRs)在地球观测学习中将低维位置输入(经度、纬度)嵌入到高维表示中，基于地理参考的卫星、图像或文本数据训练，但缺乏对其信息含量和分布的理解。&lt;h4&gt;目的&lt;/h4&gt;探究地理INRs的内在维度，了解这些表示中包含多少信息以及信息集中在哪里，为模型评估提供新方法。&lt;h4&gt;方法&lt;/h4&gt;分析内在维度在256到512之间的INRs，研究内在维度如何捕获数据局部变化，评估内在维度与下游任务性能的关系。&lt;h4&gt;主要发现&lt;/h4&gt;地理INRs的内在维度大致在2到10之间；对INR预训练过程中变化的空间分辨率和输入模态敏感；与下游任务性能相关；能够捕获空间伪影，促进模型评估和诊断。&lt;h4&gt;结论&lt;/h4&gt;提供了一种与架构无关、无标签的信息内容度量方法，可以在INRs之间实现无监督评估、模型选择和预训练设计。&lt;h4&gt;翻译&lt;/h4&gt;在地球观测表征学习的背景下，地理隐式神经表示(INRs)将低维位置输入(经度、纬度)嵌入到高维嵌入中，这些模型基于地理参考的卫星、图像或文本数据训练。尽管地理INRs的共同目标是将地球数据提炼成紧凑、易于学习的表示，但我们缺乏对这些地球表示中包含多少信息以及这些信息集中在哪里理解。数据集的内在维度衡量了捕获其局部变化所需的自由度数量，无论它嵌入的高维空间如何。这项工作提供了对地理INRs内在维度的首次研究。分析ambent维度在256到512之间的INRs，我们发现它们的内在维度大致在2到10之间，并且对INR预训练过程中变化的空间分辨率和输入模态敏感。此外，我们表明地理INRs的内在维度与下游任务性能相关，并且可以捕获空间伪影，促进模型评估和诊断。更广泛地说，我们的工作提供了一种与架构无关、无标签的信息内容度量方法，可以在INRs之间实现无监督评估、模型选择和预训练设计。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何量化地理隐式神经表示(Geographic INRs)的信息含量及信息集中位置的问题。这个问题很重要，因为目前地球表示模型的质量主要通过特定下游任务的有监督性能评估，缺乏对模型基本表示能力的理解，而理解地球表示的信息含量对于评估和改进地理表示学习模型至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从地理INRs将低维位置输入映射到高维嵌入但实际信息含量可能远低于环境维度这一观察出发，借鉴了内在维度(ID)测量这一已有概念，设计了全局ID和局部ID两种测量方法。作者借鉴了基于距离的估计器(如MLE、TwoNN)和基于角度的估计器(如FisherS)等现有ID估计方法，以及SatCLIP、GeoCLIP等地理INRs的现有工作，将其应用于地球表示这一特定领域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过测量地理INRs嵌入的内在维度(ID)来量化地球表示的信息丰富度和信息集中位置。整体流程包括：1)使用预训练位置编码器生成地理嵌入；2)通过角度估计器计算全局ID，通过基于距离的估计器计算局部ID生成ID地图；3)在下游任务激活空间中测量ID评估任务对齐；4)分析ID与下游性能的关系及分辨率和输入模态的影响；5)解释ID如何反映模型表示能力和任务对齐。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次研究地理INRs的内在维度；提出ID作为无监督评估指标；区分表示性和任务对齐；揭示空间伪影；建立ID与下游性能的关系。相比之前工作，不同之处在于：评估方法从有监督转向无监督；评估深度从'学习友好性'深入到信息含量和表示能力；应用范围从特定任务扩展到一般INRs；增加了诊断模型空间异质性和伪影的能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次系统地测量了地理隐式神经表示的内在维度，发现其远低于环境维度但与下游任务性能相关，为地球表示学习提供了一种新的无监督评估方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Within the context of representation learning for Earth observation,geographic Implicit Neural Representations (INRs) embed low-dimensionallocation inputs (longitude, latitude) into high-dimensional embeddings, throughmodels trained on geo-referenced satellite, image or text data. Despite thecommon aim of geographic INRs to distill Earth's data into compact,learning-friendly representations, we lack an understanding of how muchinformation is contained in these Earth representations, and where thatinformation is concentrated. The intrinsic dimension of a dataset measures thenumber of degrees of freedom required to capture its local variability,regardless of the ambient high-dimensional space in which it is embedded. Thiswork provides the first study of the intrinsic dimensionality of geographicINRs. Analyzing INRs with ambient dimension between 256 and 512, we find thattheir intrinsic dimensions fall roughly between 2 and 10 and are sensitive tochanging spatial resolution and input modalities during INR pre-training.Furthermore, we show that the intrinsic dimension of a geographic INRcorrelates with downstream task performance and can capture spatial artifacts,facilitating model evaluation and diagnostics. More broadly, our work offers anarchitecture-agnostic, label-free metric of information content that can enableunsupervised evaluation, model selection, and pre-training design across INRs.</description>
      <author>example@mail.com (Arjun Rao, Marc Rußwurm, Konstantin Klemmer, Esther Rolf)</author>
      <guid isPermaLink="false">2511.02101v2</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure</title>
      <link>http://arxiv.org/abs/2511.01847v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究终身学习中的表示学习问题，提出了一种基于多任务经验风险最小化的算法，并基于任务回避维度建立了样本复杂度界限。&lt;h4&gt;背景&lt;/h4&gt;终身学习中，学习者面临一系列具有共享结构的任务，需要识别并利用这些结构来加速学习。与多任务学习不同，终身学习要求学习者利用现有知识，同时在线方式持续收集部分信息。&lt;h4&gt;目的&lt;/h4&gt;研究一个广义的终身表示学习框架，开发一种简单算法来处理在线方式下的持续学习，并建立样本复杂度界限。&lt;h4&gt;方法&lt;/h4&gt;提出一种使用多任务经验风险最小化作为子程序的算法，并基于引入的任务回避维度概念建立样本复杂度界限。&lt;h4&gt;主要发现&lt;/h4&gt;基于任务回避维度建立了样本复杂度界限，该结果适用于涉及一般函数类的广泛学习问题，并在噪声下的分类和回归任务中得到了具体应用。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法和理论框架为终身学习提供了有效的表示学习方法，适用于多种学习场景。&lt;h4&gt;翻译&lt;/h4&gt;在终身学习中，学习者面临一系列具有共享结构的任务，旨在识别并利用这些结构来加速学习。我们研究了一种结构通过数据的共同表示来捕捉的场景。与多任务学习或学习-to-learn不同（在这些方法中任务一开始就可用以学习表示），终身学习要求学习者利用现有知识，同时在线方式持续收集部分信息。在本文中，我们考虑了一个广义的终身表示学习框架。我们提出了一种使用多任务经验风险最小化作为子程序的简单算法，并基于我们引入的新概念——任务回避维度，建立了样本复杂度界限。我们的结果适用于涉及一般函数类的广泛学习问题。作为具体例子，我们在噪声下的分类和回归任务中实例化了我们的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In lifelong learning, a learner faces a sequence of tasks with sharedstructure and aims to identify and leverage it to accelerate learning. We studythe setting where such structure is captured by a common representation ofdata. Unlike multi-task learning or learning-to-learn, where tasks areavailable upfront to learn the representation, lifelong learning requires thelearner to make use of its existing knowledge while continually gatheringpartial information in an online fashion. In this paper, we consider ageneralized framework of lifelong representation learning. We propose a simplealgorithm that uses multi-task empirical risk minimization as a subroutine andestablish a sample complexity bound based on a new notion we introduce--thetask-eluder dimension. Our result applies to a wide range of learning problemsinvolving general function classes. As concrete examples, we instantiate ourresult on classification and regression tasks under noise.</description>
      <author>example@mail.com (Zhi Wang, Chicheng Zhang, Ramya Korlakai Vinayak)</author>
      <guid isPermaLink="false">2511.01847v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>From Pixels to Cooperation Multi Agent Reinforcement Learning based on Multimodal World Models</title>
      <link>http://arxiv.org/abs/2511.01310v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于共享生成多模态世界模型（MWM）的新框架，用于解决从高维多模态感官输入学习合作多智能体策略的样本效率问题。&lt;h4&gt;背景&lt;/h4&gt;从高维、多模态感官输入（如像素和音频）直接学习合作多智能体策略存在样本效率低的问题。无模型多智能体强化学习算法面临表示学习、部分可观察性和信用分配的联合挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于共享生成多模态世界模型（MWM）的新框架，提高合作多智能体强化学习的样本效率，解决表示学习、部分可观察性和信用分配的联合挑战。&lt;h4&gt;方法&lt;/h4&gt;提出多模态世界模型（MWM），使用基于注意力的机制融合所有智能体的分布式多模态观测，学习环境动力学的压缩潜在表示；利用MWM作为快速'想象'模拟器，在潜在空间内训练合作MARL策略（如MAPPO），将表示学习与策略学习解耦；引入基于3D物理模拟器的多模态、多智能体基准测试集。&lt;h4&gt;主要发现&lt;/h4&gt;MWM-MARL框架与最先进的无模型MARL基线相比实现了数量级更高的样本效率；在感觉不对称环境中，所提出的多模态融合对任务成功至关重要；架构对传感器脱落具有更好的鲁棒性，这对实际部署至关重要。&lt;h4&gt;结论&lt;/h4&gt;基于共享生成多模态世界模型的框架能够有效解决从高维多模态输入学习合作多智能体策略的样本效率问题，并在感觉不对称环境和传感器脱落情况下表现出优越的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;从高维、多模态感官输入（如像素和音频）直接学习合作多智能体策略存在众所周知的样本效率低下问题。无模型多智能体强化学习算法难以应对表示学习、部分可观察性和信用分配的联合挑战。为此，我们提出了一种基于共享生成式多模态世界模型（MWM）的新框架。我们的MWM通过使用可扩展的基于注意力的机制融合所有智能体的分布式多模态观测，学习环境动力学的压缩潜在表示。随后，我们利用这个学习的MWM作为快速的'想象'模拟器，在其潜在空间内完全训练合作MARL策略（如MAPPO），将表示学习与策略学习解耦。我们引入了一组基于3D物理模拟器的新挑战性多模态、多智能体基准测试。我们的实验证明，与最先进的无模型MARL基线相比，我们的MWM-MARL框架实现了数量级更高的样本效率。我们进一步表明，在感觉不对称的环境中，我们提出的多模态融合对任务成功至关重要，并且我们的架构对传感器脱落提供了更强的鲁棒性，这是实际部署的关键特性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从高维多模态感官输入(如像素和音频)直接学习合作多智能体策略时样本效率低下的问题。这个问题在现实中很重要，因为它关系到开发能在物理世界协作的智能体系统(如机器人团队、自动驾驶车辆等)；在研究中也很重要，因为现有的模型无关MARL算法难以同时处理高维感官输入、部分可观察环境和复杂社会推理的联合挑战，限制了智能体系统在复杂环境中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到模型无关MARL算法在高维多模态输入上的样本效率低下问题，然后借鉴了单智能体'World Models'范式的成功经验，意识到需要将其扩展到多智能体多模态环境。作者还考虑了计算效率和可扩展性挑战。该方法借鉴了多个领域的工作：单智能体世界模型(如Dreamer系列)、MARL的价值分解方法和CTDE范式、多模态学习(如CLIP、Vision Transformer)以及高效大规模系统(如MoE架构、联邦学习原则)。作者设计了一个基于共享多模态世界模型(MWM)的框架，使用注意力机制融合多智能体信息，并在潜在空间中训练策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1)学习一个共享的多模态世界模型(MWM)来统一表示环境；2)将表示学习与策略学习解耦，先学习环境模型再在潜在空间中训练策略；3)使用分层注意力机制动态融合多智能体多模态信息；4)利用MWM作为'想象'模拟器在潜在空间中训练策略。整体流程分为两个阶段：第一阶段学习MWM，包括多模态观察编码、分层融合、循环潜在动力学建模和训练；第二阶段在潜在空间中训练合作策略，包括使用MWM生成'梦想'轨迹、计算优势并更新策略、最终实现分散执行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将单智能体世界模型扩展到多智能体多模态环境；2)提出基于注意力的分层多模态融合机制，实现动态信息路由；3)在潜在空间中完全训练合作MARL策略，解耦表示学习与策略学习；4)基于RSSM的概率性质提高对传感器故障的鲁棒性；5)引入新的多智能体多模态基准测试。相比之前工作，本文不仅扩展了世界模型到多智能体环境，还引入了多模态融合机制；与传统MARL不同，不是直接从原始数据学习而是先学习环境模型；相比多模态学习，不仅学习表示还学习环境动力学；相比联邦学习，将高效通信原则应用于多智能体协作场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于多模态世界模型的多智能体强化学习框架，通过解耦表示学习与策略学习，实现了从高维多模态感官输入直接学习合作策略的样本效率大幅提升，并显著提高了对传感器故障的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning cooperative multi-agent policies directly from high-dimensional,multimodal sensory inputs like pixels and audio (from pixels) is notoriouslysample-inefficient. Model-free Multi-Agent Reinforcement Learning (MARL)algorithms struggle with the joint challenge of representation learning,partial observability, and credit assignment. To address this, we propose anovel framework based on a shared, generative Multimodal World Model (MWM). OurMWM is trained to learn a compressed latent representation of the environment'sdynamics by fusing distributed, multimodal observations from all agents using ascalable attention-based mechanism. Subsequently, we leverage this learned MWMas a fast, "imagined" simulator to train cooperative MARL policies (e.g.,MAPPO) entirely within its latent space, decoupling representation learningfrom policy learning. We introduce a new set of challenging multimodal,multi-agent benchmarks built on a 3D physics simulator. Our experimentsdemonstrate that our MWM-MARL framework achieves orders-of-magnitude greatersample efficiency compared to state-of-the-art model-free MARL baselines. Wefurther show that our proposed multimodal fusion is essential for task successin environments with sensory asymmetry and that our architecture providessuperior robustness to sensor-dropout, a critical feature for real-worlddeployment.</description>
      <author>example@mail.com (Sureyya Akin, Kavita Srivastava, Prateek B. Kapoor, Pradeep G. Sethi, Sunita Q. Patel, Rahu Srivastava)</author>
      <guid isPermaLink="false">2511.01310v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Influence-aware Causal Autoencoder Network for Node Importance Ranking in Complex Networks</title>
      <link>http://arxiv.org/abs/2511.01228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ICAN（影响感知因果自编码网络）的新框架，通过因果表示学习获取稳健、不变的节点嵌入，用于跨网络排序任务。ICAN在合成网络上训练，但能有效应用于真实网络，解决了现有方法依赖目标网络拓扑结构导致的隐私问题和泛化能力差的问题。&lt;h4&gt;背景&lt;/h4&gt;节点重要性排序是图数据分析中的基础问题。现有方法通常依赖于传统中心性度量或先进图表示学习方法，这些方法直接依赖目标网络的拓扑结构，引发隐私问题，且在不同网络间泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;设计一个仅在合成网络上训练，但能有效应用于真实网络的节点重要性排序模型，消除对目标网络拓扑的依赖，提高实用性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出影响感知因果自编码网络（ICAN），在自编码器架构中引入影响感知因果表示学习模块，提取与节点重要性因果相关的节点嵌入；引入因果排序损失，设计统一优化框架，联合优化重建和排序目标，实现节点表示学习和排序优化的相互强化。&lt;h4&gt;主要发现&lt;/h4&gt;ICAN在合成网络上训练，能有效泛化到多样化的真实图；在多个基准数据集上的实验表明，ICAN在排序准确性和泛化能力方面持续优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;ICAN成功解决了在合成网络上训练并应用于真实网络的问题，通过因果表示学习提高了模型的泛化能力和实用性。&lt;h4&gt;翻译&lt;/h4&gt;节点重要性排序是图数据分析中的一个基础问题。现有方法通常依赖于从传统中心性度量或先进的图表示学习方法中推导出的节点特征，这些方法直接依赖于目标网络的拓扑结构。然而，这种对结构信息的依赖引发了隐私问题，并且通常在不同网络间的泛化能力较差。在这项工作中，我们解决了一个关键问题：我们能否设计一个仅在合成网络上训练的节点重要性排序模型，并有效应用于真实网络，消除对目标网络拓扑的依赖，同时提高实用性和泛化能力？我们通过提出影响感知因果自编码网络（ICAN）对此问题给予了肯定的回答，这是一个利用因果表示学习获取稳健、不变的节点嵌入的新框架，用于跨网络排序任务。首先，ICAN在自编码器架构中引入了影响感知因果表示学习模块，提取与节点重要性因果相关的节点嵌入。此外，我们引入了因果排序损失，并设计了一个统一优化框架，联合优化重建和排序目标，使节点表示学习和排序优化能够相互强化。这种设计使得ICAN在合成网络上训练后，能够有效泛化到多样化的真实图。在多个基准数据集上的广泛实验表明，ICAN在排序准确性和泛化能力方面持续优于最先进的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Node importance ranking is a fundamental problem in graph data analysis.Existing approaches typically rely on node features derived from eithertraditional centrality measures or advanced graph representation learningmethods, which depend directly on the target network's topology. However, thisreliance on structural information raises privacy concerns and often leads topoor generalization across different networks. In this work, we address a keyquestion: Can we design a node importance ranking model trained exclusively onsynthetic networks that is effectively appliable to real-world networks,eliminating the need to rely on the topology of target networks and improvingboth practicality and generalizability? We answer this question affirmativelyby proposing the Influence-aware Causal Autoencoder Network (ICAN), a novelframework that leverages causal representation learning to get robust,invariant node embeddings for cross-network ranking tasks. Firstly, ICANintroduces an influence-aware causal representation learning module within anautoencoder architecture to extract node embeddings that are causally relatedto node importance. Moreover, we introduce a causal ranking loss and design aunified optimization framework that jointly optimizes the reconstruction andranking objectives, enabling mutual reinforcement between node representationlearning and ranking optimization. This design allows ICAN, trained onsynthetic networks, to generalize effectively across diverse real-world graphs.Extensive experiments on multiple benchmark datasets demonstrate that ICANconsistently outperforms state-of-the-art baselines in terms of both rankingaccuracy and generalization capability.</description>
      <author>example@mail.com (Jiahui Gao, Kuang Zhou, Yuchen Zhu)</author>
      <guid isPermaLink="false">2511.01228v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Anatomically Constrained Transformers for Echocardiogram Analysis</title>
      <link>http://arxiv.org/abs/2511.01109v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Video Anatomically Constrained Transformer (ViACT)，一种将解剖先验直接整合到变换器架构中的新框架，用于超声心动图分析。&lt;h4&gt;背景&lt;/h4&gt;视频变换器在超声心动图分析中显示出强大潜力，但与其他视频模型一样，它们容易从非诊断区域（如图像背景）学习到虚假相关性。&lt;h4&gt;目的&lt;/h4&gt;克服现有视频变换器模型的局限性，通过整合解剖先验来提高模型在医学图像分析中的表现和可解释性。&lt;h4&gt;方法&lt;/h4&gt;ViACT将变形的解剖结构表示为点集，将其空间几何和相应的图像块编码为变换器token；在预训练过程中采用掩码自编码策略，仅掩码和重建解剖区域；预训练模型可针对该区域的任务进行微调；专注于心肌应用，并在左心室射血分数回归和心脏淀粉样变性检测等任务上展示框架。&lt;h4&gt;主要发现&lt;/h4&gt;解剖约束将变换器的注意力集中在心肌区域，产生与已知病理区域对齐的可解释注意力图；ViACT能够推广到心肌点跟踪，无需专门跟踪网络中的特定任务组件。&lt;h4&gt;结论&lt;/h4&gt;ViACT通过整合解剖先验，解决了视频变换器在医学图像分析中学习非相关区域的问题，提高了模型在超声心动图分析任务中的性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;视频变换器最近在超声心动图分析中显示出强大的潜力，利用自监督预训练和灵活适应不同任务的能力。然而，与其他视频模型一样，它们容易从非诊断区域（如图像背景）学习到虚假相关性。为克服这一局限，我们提出了视频解剖约束变换器（ViACT），一种将解剖先验直接整合到变换器架构的新框架。ViACT将变形的解剖结构表示为点集，并将其空间几何和相应的图像块编码为变换器token。在预训练过程中，ViACT遵循掩码自编码策略，仅掩码和重建解剖区域，强制表示学习集中在解剖区域。预训练模型随后可以针对该区域的任务进行微调。本文中我们专注于心肌，在左心室射血分数回归和心脏淀粉样变性检测等超声分析任务上展示了该框架。解剖约束将变换器的注意力集中在心肌区域，产生与已知CA病理区域对齐的可解释注意力图。此外，ViACT能够推广到心肌点跟踪，而无需专门跟踪网络中使用的特定任务组件，如相关体积。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video transformers have recently demonstrated strong potential forechocardiogram (echo) analysis, leveraging self-supervised pre-training andflexible adaptation across diverse tasks. However, like other models operatingon videos, they are prone to learning spurious correlations from non-diagnosticregions such as image backgrounds. To overcome this limitation, we propose theVideo Anatomically Constrained Transformer (ViACT), a novel framework thatintegrates anatomical priors directly into the transformer architecture. ViACTrepresents a deforming anatomical structure as a point set and encodes both itsspatial geometry and corresponding image patches into transformer tokens.During pre-training, ViACT follows a masked autoencoding strategy that masksand reconstructs only anatomical patches, enforcing that representationlearning is focused on the anatomical region. The pre-trained model can then befine-tuned for tasks localized to this region. In this work we focus on themyocardium, demonstrating the framework on echo analysis tasks such as leftventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)detection. The anatomical constraint focuses transformer attention within themyocardium, yielding interpretable attention maps aligned with regions of knownCA pathology. Moreover, ViACT generalizes to myocardium point tracking withoutrequiring task-specific components such as correlation volumes used inspecialized tracking networks.</description>
      <author>example@mail.com (Alexander Thorley, Agis Chartsias, Jordan Strom, Jeremy Slivnick, Dipak Kotecha, Alberto Gomez, Jinming Duan)</author>
      <guid isPermaLink="false">2511.01109v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>The Geometry of Grokking: Norm Minimization on the Zero-Loss Manifold</title>
      <link>http://arxiv.org/abs/2511.01938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了神经网络中的'Grokking'现象，即神经网络在完全记忆训练数据后，经过显著延迟才出现完全泛化的现象。论文提出通过约束优化视角理解记忆后的学习过程，证明梯度下降实际上是在零损失流形上最小化权重范数。&lt;h4&gt;背景&lt;/h4&gt;先前研究将延迟泛化归因于权重衰减驱动的表示学习，但精确的潜在动态机制仍然不清楚。&lt;h4&gt;目的&lt;/h4&gt;通过约束优化的视角理解神经网络记忆后的学习过程。&lt;h4&gt;方法&lt;/h4&gt;论文在无限小学习率和权重衰减系数的极限下形式化证明梯度下降在零损失流形上最小化权重范数；引入近似方法解耦参数学习动态；推导两层网络第一层记忆后动力学的闭式表达式。&lt;h4&gt;主要发现&lt;/h4&gt;梯度下降在零损失流形上最小化权重范数；通过近似方法可解耦参数学习动态；推导出两层网络第一层记忆后动力学的闭式表达式。&lt;h4&gt;结论&lt;/h4&gt;实验证实使用预测梯度模拟训练过程能重现 grokking 的延迟泛化和表示学习特征。&lt;h4&gt;翻译&lt;/h4&gt;Grokking 是神经网络中的一种令人费解的现象，其中完全泛化仅在完全记忆训练数据后经过显著延迟才发生。先前的研究将这种延迟泛化与权重衰减驱动的表示学习联系起来，但潜在的精确动态机制仍然不清楚。在本文中，我们认为记忆后的学习可以通过约束优化的视角来理解：梯度下降有效地在零损失流形上最小化权重范数。我们在学习率和权重衰减系数无限小的极限情况下形式化证明了这一点。为了进一步剖析这一机制，我们引入了一个近似方法，将网络中一部分参数的学习动态与其他参数解耦。应用这一框架，我们推导出两层网络第一层记忆后动力学的闭式表达式。实验证实，使用我们预测的梯度模拟训练过程能够重现 grokking 的延迟泛化和表示学习特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grokking is a puzzling phenomenon in neural networks where fullgeneralization occurs only after a substantial delay following the completememorization of the training data. Previous research has linked this delayedgeneralization to representation learning driven by weight decay, but theprecise underlying dynamics remain elusive. In this paper, we argue thatpost-memorization learning can be understood through the lens of constrainedoptimization: gradient descent effectively minimizes the weight norm on thezero-loss manifold. We formally prove this in the limit of infinitesimallysmall learning rates and weight decay coefficients. To further dissect thisregime, we introduce an approximation that decouples the learning dynamics of asubset of parameters from the rest of the network. Applying this framework, wederive a closed-form expression for the post-memorization dynamics of the firstlayer in a two-layer network. Experiments confirm that simulating the trainingprocess using our predicted gradients reproduces both the delayedgeneralization and representation learning characteristic of grokking.</description>
      <author>example@mail.com (Tiberiu Musat)</author>
      <guid isPermaLink="false">2511.01938v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>TRISKELION-1: Unified Descriptive-Predictive-Generative AI</title>
      <link>http://arxiv.org/abs/2511.00711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 18 figures, submitted to arXiv (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TRISKELION-1是一种统一架构，在一个编码器-解码器框架中集成了描述性、预测性和生成性功能，能够同时实现描述性重建、预测分类和生成采样。&lt;h4&gt;背景&lt;/h4&gt;当前人工智能领域需要能够同时处理描述性、预测性和生成性任务的统一架构。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够联合优化描述性表示学习、预测推理和生成合成的统一模型框架。&lt;h4&gt;方法&lt;/h4&gt;TRISKELION-1架构在一个编码器-解码器框架中集成了统计、机制和生成推理，使用变分目标进行联合优化。&lt;h4&gt;主要发现&lt;/h4&gt;在MNIST数据集上的实验表明，描述性重建、预测分类和生成采样可以在一个模型中稳定共存。&lt;h4&gt;结论&lt;/h4&gt;该框架为连接可解释性、准确性和创造力的通用智能架构提供了蓝图。&lt;h4&gt;翻译&lt;/h4&gt;TRISKELION-1是一种统一的描述性-预测性-生成性架构，它在单个编码器-解码器框架中集成了统计、机制和生成推理。该模型展示了如何使用变分目标联合优化描述性表示学习、预测推理和生成合成。在MNIST上的实验验证了描述性重建、预测分类和生成采样可以在一个模型中稳定共存。该框架为连接可解释性、准确性和创造力的通用智能架构提供了蓝图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; TRISKELION-1 is a unified descriptive-predictive-generative architecture thatintegrates statistical, mechanistic, and generative reasoning within a singleencoder-decoder framework. The model demonstrates how descriptiverepresentation learning, predictive inference, and generative synthesis can bejointly optimized using variational objectives. Experiments on MNIST validatethat descriptive reconstruction, predictive classification, and generativesampling can coexist stably within one model. The framework provides ablueprint toward universal intelligence architectures that connectinterpretability, accuracy, and creativity.</description>
      <author>example@mail.com (Nardeep Kumar, Arun Kanwar)</author>
      <guid isPermaLink="false">2511.00711v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy</title>
      <link>http://arxiv.org/abs/2511.00555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE T-RO&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DeepKoopman增强的双分支扩散策略(D3P)的算法，用于解决机器人操作模仿学习中现有扩散模型难以捕捉多步骤间强时间依赖性的问题，特别是在整合本体感受输入时。D3P通过双分支架构解耦不同感官模态的作用，并利用Deep Koopman Operator模块增强视觉表示学习，显著提高了任务执行效果。&lt;h4&gt;背景&lt;/h4&gt;将生成模型与动作块结合在机器人操作模仿学习中显示出巨大潜力，但现有的基于扩散的方法往往难以捕捉多步骤间的强时间依赖性，特别是在整合本体感受输入时，这会导致任务失败，策略过度适应本体感受线索而忽略视觉特征。&lt;h4&gt;目的&lt;/h4&gt;克服现有扩散模型在捕捉多步骤间强时间依赖性方面的局限性，特别是当整合本体感受输入时，防止策略过度适应本体感受线索而忽略视觉特征，从而提高机器人操作任务的执行效果。&lt;h4&gt;方法&lt;/h4&gt;提出D3P算法，采用双分支架构解耦不同感官模态组合的作用：视觉分支编码视觉观察以指示任务进展，融合分支整合视觉和本体感受输入实现精确操作。当机器人无法完成中间目标时，策略可动态切换到视觉分支生成的动作块。同时，集成Deep Koopman Operator模块捕捉视觉输入中的结构化时间动态，并利用测试时损失作为置信信号引导预测动作块的聚合。&lt;h4&gt;主要发现&lt;/h4&gt;在六个RLBench桌面任务的模拟实验中，D3P比最先进的扩散策略平均高出14.6%。在三个真实世界机器人操作任务中，实现了15.0%的改进。代码已公开在GitHub上。&lt;h4&gt;结论&lt;/h4&gt;D3P算法通过双分支架构和Deep Koopman Operator模块有效解决了现有扩散模型在捕捉时间依赖性和整合多感官输入方面的局限性，显著提高了机器人操作任务的执行效果和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;将生成模型与动作块整合在机器人操作模仿学习中显示出巨大潜力。然而，现有的基于扩散的范式往往难以捕捉多步骤间的强时间依赖性，特别是在整合本体感受输入时。这种局限性会导致任务失败，策略过度适应本体感受线索而牺牲对任务中视觉派生特征的捕捉。为克服这一挑战，我们提出了DeepKoopman增强的双分支扩散策略(D3P)算法。D3P引入双分支架构来解耦不同感官模态组合的作用。视觉分支编码视觉观察以指示任务进展，而融合分支整合视觉和本体感受输入以实现精确操作。在此架构中，当机器人无法完成中间目标（如抓取抽屉把手）时，策略可动态切换到执行由视觉分支生成的动作块，允许恢复到先前观察的状态并促进任务重试。为进一步增强视觉表示学习，我们集成了Deep Koopman Operator模块，从视觉输入中捕捉结构化时间动态。在推理过程中，我们使用生成模型的测试时损失作为置信信号来指导时间重叠预测动作块的聚合，从而提高策略执行的可靠性。在六个RLBench桌面任务的模拟实验中，D3P比最先进的扩散策略平均高出14.6%。在三个真实世界机器人操作任务中，实现了15.0%的改进。代码：https://github.com/dianyeHuang/D3P。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating generative models with action chunking has shown significantpromise in imitation learning for robotic manipulation. However, the existingdiffusion-based paradigm often struggles to capture strong temporaldependencies across multiple steps, particularly when incorporatingproprioceptive input. This limitation can lead to task failures, where thepolicy overfits to proprioceptive cues at the expense of capturing the visuallyderived features of the task. To overcome this challenge, we propose the DeepKoopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces adual-branch architecture to decouple the roles of different sensory modalitycombinations. The visual branch encodes the visual observations to indicatetask progression, while the fused branch integrates both visual andproprioceptive inputs for precise manipulation. Within this architecture, whenthe robot fails to accomplish intermediate goals, such as grasping a drawerhandle, the policy can dynamically switch to execute action chunks generated bythe visual branch, allowing recovery to previously observed states andfacilitating retrial of the task. To further enhance visual representationlearning, we incorporate a Deep Koopman Operator module that capturesstructured temporal dynamics from visual inputs. During inference, we use thetest-time loss of the generative model as a confidence signal to guide theaggregation of the temporally overlapping predicted action chunks, therebyenhancing the reliability of policy execution. In simulation experiments acrosssix RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusionpolicy by an average of 14.6\%. On three real-world robotic manipulation tasks,it achieves a 15.0\% improvement. Code: https://github.com/dianyeHuang/D3P.</description>
      <author>example@mail.com (Dianye Huang, Nassir Navab, Zhongliang Jiang)</author>
      <guid isPermaLink="false">2511.00555v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts</title>
      <link>http://arxiv.org/abs/2511.00480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FedMGP是一种用于视觉语言模型中个性化联邦提示学习的新范式，通过多组配对的文本和视觉提示捕捉多样化语义，采用动态提示聚合策略平衡全局与本地特征，实现高效且高性能的联邦学习。&lt;h4&gt;背景&lt;/h4&gt;在联邦学习环境中，视觉语言模型需要同时考虑全局知识的共享和客户端个性化特征的保留，而传统的联邦提示学习方法在平衡这两方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的联邦提示学习范式，能够有效捕捉多样化、细粒度的语义和实例级线索，平衡通用知识保留与客户端特定特征，同时保持参数效率。&lt;h4&gt;方法&lt;/h4&gt;FedMGP为每个客户端配备多组配对的文本和视觉提示，引入多样性损失促使各组提示专注于不同语义方面，采用基于相似度引导概率采样的动态提示聚合策略进行通信，通过softmax加权分布选择最相关的提示组。&lt;h4&gt;主要发现&lt;/h4&gt;动态聚合策略通过强化共享语义同时抑制客户端特定噪声，促进鲁棒的全局表征学习；FedMGP在所有联邦提示学习方法中实现了最低的通信参数，同时达到最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;FedMGP在多样化的联邦视觉语言基准测试中，在个性化和领域泛化方面均始终优于先前的方法，为联邦视觉语言模型提供了一种高效且有效的个性化学习框架。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了FedMGP，一种用于视觉语言模型中个性化联邦提示学习的新范式。FedMGP为每个客户端配备多组配对的文本和视觉提示，使模型能够捕捉多样化、细粒度的语义和实例级线索。引入了多样性损失促使每组提示专注于不同且互补的语义方面，确保各组共同覆盖更广泛的本地特征。在通信过程中，FedMGP采用基于相似度引导概率采样的动态提示聚合策略：每个客户端计算其提示组与上一轮全局提示之间的余弦相似度，然后通过softmax加权分布采样s个组。这种软选择机制优先聚合语义对齐的知识，同时有效探索代表性不足的模式，平衡了通用知识的保留与客户端特定特征。值得注意的是，FedMGP通过在多个组之间重新分配固定的提示容量，在所有联邦提示学习方法中实现了最低的通信参数，同时达到最先进的性能。理论分析表明，我们的动态聚合策略通过强化共享语义同时抑制客户端特定噪声，促进鲁棒的全局表征学习。大量实验证明，在多样化的联邦视觉语言基准测试中，FedMGP在个性化和领域泛化方面均始终优于先前的方法。代码将在https://github.com/weihao-bo/FedMGP.git上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce FedMGP, a new paradigm for personalized federatedprompt learning in vision-language models. FedMGP equips each client withmultiple groups of paired textual and visual prompts, enabling the model tocapture diverse, fine-grained semantic and instance-level cues. A diversityloss is introduced to drive each prompt group to specialize in distinct andcomplementary semantic aspects, ensuring that the groups collectively cover abroader range of local characteristics. During communication, FedMGP employs adynamic prompt aggregation strategy based on similarity-guided probabilisticsampling: each client computes the cosine similarity between its prompt groupsand the global prompts from the previous round, then samples s groups via asoftmax-weighted distribution. This soft selection mechanism preferentiallyaggregates semantically aligned knowledge while still enabling exploration ofunderrepresented patterns effectively balancing the preservation of commonknowledge with client-specific features. Notably, FedMGP maintains parameterefficiency by redistributing a fixed prompt capacity across multiple groups,achieving state-of-the-art performance with the lowest communication parametersamong all federated prompt learning methods. Theoretical analysis shows thatour dynamic aggregation strategy promotes robust global representation learningby reinforcing shared semantics while suppressing client-specific noise.Extensive experiments demonstrate that FedMGP consistently outperforms priorapproaches in both personalization and domain generalization across diversefederated vision-language benchmarks. The code will be released onhttps://github.com/weihao-bo/FedMGP.git.</description>
      <author>example@mail.com (Weihao Bo, Yanpeng Sun, Yu Wang, Xinyu Zhang, Zechao Li)</author>
      <guid isPermaLink="false">2511.00480v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>SHIELD: Securing Healthcare IoT with Efficient Machine Learning Techniques for Anomaly Detection</title>
      <link>http://arxiv.org/abs/2511.03661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种机器学习驱动的框架，用于检测物联网医疗环境中的恶意网络攻击和设备异常，通过评估多种机器学习模型，确定了最优解决方案并提升了医疗设备安全性。&lt;h4&gt;背景&lt;/h4&gt;物联网设备在医疗领域的整合引入了显著的安全性和可靠性挑战，增加了系统对网络威胁和操作异常的易感性。&lt;h4&gt;目的&lt;/h4&gt;开发一种机器学习框架，用于检测恶意网络攻击和识别故障设备异常，从而提高物联网医疗环境的安全性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;利用包含20万条记录的数据集，评估了八种机器学习模型，包括监督学习(XGBoost、K-NN)、半监督学习(GAN、VAE)和无监督学习(One-Class SVM、Isolation Forest、GNN、LSTM自编码器)方法，通过F1分数、精确率、召回率、准确率、ROC-AUC和计算效率等指标进行综合评估。&lt;h4&gt;主要发现&lt;/h4&gt;异常检测方面，XGBoost表现最优(99%准确率，0.04秒计算时间)，隔离森林有效平衡了精确率和召回率，LSTM自编码器表现较差；攻击检测方面，KNN实现接近完美的指标且计算成本最低(0.05秒)，VAE准确率达97%，GAN计算成本最高但准确率和ROC-AUC最低。&lt;h4&gt;结论&lt;/h4&gt;该框架通过有效的异常检测策略增强了物联网医疗安全性，能够早期检测网络威胁和设备故障，防止数据泄露，最小化系统停机时间，确保医疗设备持续安全运行，最终保护患者健康和对物联网医疗解决方案的信任。&lt;h4&gt;翻译&lt;/h4&gt;物联网设备在医疗领域的整合引入了显著的安全性和可靠性挑战，增加了对网络威胁和操作异常的易感性。本研究提出了一种机器学习驱动的框架，用于(1)检测恶意网络攻击和(2)识别故障设备异常，利用包含20万条记录的数据集。评估了八种机器学习模型，涵盖三种学习方法：监督学习(XGBoost、K-近邻)、半监督学习(生成对抗网络、变分自编码器)和无监督学习(一类支持向量机、隔离森林、图神经网络和长短期记忆自编码器)。通过F1分数、精确率、召回率、准确率、ROC-AUC和计算效率等多个指标进行综合评估。XGBoost以99%的准确率和最小的计算开销(0.04秒)实现了异常检测，隔离森林有效地平衡了精确率和召回率。LSTM自编码器表现较差，准确率较低且延迟较高。在攻击检测方面，KNN以最低的计算成本(0.05秒)实现了接近完美的精确率、召回率和F1分数，其次是VAE，准确率为97%。GAN显示出最高的计算成本，但准确率和ROC-AUC最低。这些发现通过有效的异常检测策略增强了物联网医疗安全性。通过提高网络威胁和设备故障的早期检测，该框架有可能防止数据泄露，最小化系统停机时间，并确保医疗设备的持续安全运行，最终保护患者健康和对物联网驱动医疗解决方案的信任。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/AIIoT65859.2025.11105287&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of IoT devices in healthcare introduces significant securityand reliability challenges, increasing susceptibility to cyber threats andoperational anomalies. This study proposes a machine learning-driven frameworkfor (1) detecting malicious cyberattacks and (2) identifying faulty deviceanomalies, leveraging a dataset of 200,000 records. Eight machine learningmodels are evaluated across three learning approaches: supervised learning(XGBoost, K-Nearest Neighbors (K- NN)), semi-supervised learning (GenerativeAdversarial Networks (GAN), Variational Autoencoders (VAE)), and unsupervisedlearning (One-Class Support Vector Machine (SVM), Isolation Forest, GraphNeural Networks (GNN), and Long Short-Term Memory (LSTM) Autoencoders). Thecomprehensive evaluation was conducted across multiple metrics like F1-score,precision, recall, accuracy, ROC-AUC, computational efficiency. XGBoostachieved 99\% accuracy with minimal computational overhead (0.04s) for anomalydetection, while Isolation Forest balanced precision and recall effectively.LSTM Autoencoders underperformed with lower accuracy and higher latency. Forattack detection, KNN achieved near-perfect precision, recall, and F1-scorewith the lowest computational cost (0.05s), followed by VAE at 97% accuracy.GAN showed the highest computational cost with lowest accuracy and ROC-AUC.These findings enhance IoT-enabled healthcare security through effectiveanomaly detection strategies. By improving early detection of cyber threats anddevice failures, this framework has the potential to prevent data breaches,minimize system downtime, and ensure the continuous and safe operation ofmedical devices, ultimately safeguarding patient health and trust in IoT-drivenhealthcare solutions.</description>
      <author>example@mail.com (Mahek Desai, Apoorva Rumale, Marjan Asadinia)</author>
      <guid isPermaLink="false">2511.03661v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>NABench: Large-Scale Benchmarks of Nucleotide Foundation Models for Fitness Prediction</title>
      <link>http://arxiv.org/abs/2511.02888v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NABench是一个用于核酸适应性预测的大规模、系统化基准测试平台，汇集了162个高通量测定和260万个突变序列，涵盖多种DNA和RNA家族。研究团队评估了29个代表性基础模型在不同场景下的表现，建立了强大且可重现的基准线，并公开了代码以促进核酸建模和相关应用。&lt;h4&gt;背景&lt;/h4&gt;核苷酸序列变异可能导致功能适应性的显著变化。最近的核苷酸基础模型可以直接从序列预测这些适应性效应，但数据集的不一致性和预处理的不统一使得难以公平地比较不同DNA和RNA家族的方法。&lt;h4&gt;目的&lt;/h4&gt;介绍NABench，一个用于核酸适应性预测的大规模、系统化基准测试平台，以解决现有方法比较困难的问题。&lt;h4&gt;方法&lt;/h4&gt;NABench汇集了162个高通量测定，整理了260万个突变序列，涵盖了多种DNA和RNA家族，并具有标准化的分割和丰富的元数据。研究者在统一的评估套件下，严格评估了29个代表性基础模型，包括零样本预测、少样本预测、迁移学习和监督设置等场景。&lt;h4&gt;主要发现&lt;/h4&gt;NABench在规模、多样性和数据质量上超越了先前的核苷酸适应性基准测试；评估结果量化了不同任务和核酸类型之间的性能异质性；展示了不同建模选择的明显优势和失败模式；建立了强大且可重现的基准线。&lt;h4&gt;结论&lt;/h4&gt;研究团队发布了NABench以促进核酸建模，支持RNA/DNA设计、合成生物学和生物化学等下游应用。代码已在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;核苷酸序列变异可能引起功能适应性的显著变化。最近的核苷酸基础模型有望直接从序列预测这些适应性效应，然而异构数据集和不一致的预处理使得难以在DNA和RNA家族间公平地比较方法。在此，我们引入NABench，一个用于核酸适应性预测的大规模、系统化基准。NABench汇集了162个高通量测定，整理了260万个突变序列，涵盖多样化的DNA和RNA家族，具有标准化的分割和丰富的元数据。我们证明NABench在规模、多样性和数据质量上超越了先前的核苷酸适应性基准测试。在统一的评估套件下，我们严格评估了29个代表性基础模型，包括零样本预测、少样本预测、迁移学习和监督设置。结果量化了不同任务和核酸类型之间的性能异质性，展示了不同建模选择的明显优势和失败模式，并建立了强大、可重现的基准线。我们发布NABench以推进核酸建模，支持RNA/DNA设计、合成生物学和生物化学的下游应用。我们的代码可在https://github.com/mrzzmrzz/NABench获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nucleotide sequence variation can induce significant shifts in functionalfitness. Recent nucleotide foundation models promise to predict such fitnesseffects directly from sequence, yet heterogeneous datasets and inconsistentpreprocessing make it difficult to compare methods fairly across DNA and RNAfamilies. Here we introduce NABench, a large-scale, systematic benchmark fornucleic acid fitness prediction. NABench aggregates 162 high-throughput assaysand curates 2.6 million mutated sequences spanning diverse DNA and RNAfamilies, with standardized splits and rich metadata. We show that NABenchsurpasses prior nucleotide fitness benchmarks in scale, diversity, and dataquality. Under a unified evaluation suite, we rigorously assess 29representative foundation models across zero-shot, few-shot prediction,transfer learning, and supervised settings. The results quantify performanceheterogeneity across tasks and nucleic-acid types, demonstrating clearstrengths and failure modes for different modeling choices and establishingstrong, reproducible baselines. We release NABench to advance nucleic acidmodeling, supporting downstream applications in RNA/DNA design, syntheticbiology, and biochemistry. Our code is available athttps://github.com/mrzzmrzz/NABench.</description>
      <author>example@mail.com (Zhongmin Li, Runze Ma, Jiahao Tan, Chengzi Tan, Shuangjia Zheng)</author>
      <guid isPermaLink="false">2511.02888v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural AI with Temporal Dynamics for Comprehensive Anomaly Detection in Microservices</title>
      <link>http://arxiv.org/abs/2511.03285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合图神经网络与时间建模的统一框架，用于微服务架构中的异常检测和根因追踪。该框架将微服务调用链抽象为有向图，通过图卷积聚合特征并建模依赖关系，同时使用门控循环单元建模时间演化，最终实现从局部异常检测到全局调用链追踪的统一建模。&lt;h4&gt;背景&lt;/h4&gt;微服务架构中的异常检测和根因追踪问题&lt;h4&gt;目的&lt;/h4&gt;提出一个统一框架来解决微服务架构中的异常检测和根因追踪问题&lt;h4&gt;方法&lt;/h4&gt;结合图神经网络与时间建模的统一框架：将微服务调用链抽象为有向图，使用节点和边的多维特征构建服务拓扑表示，应用图卷积聚合节点特征并建模依赖关系，引入门控循环单元(GRU)建模调用链的时间演化，使用多层堆叠和连接操作联合获取结构和时间表示，定义节点和路径级别的异常评分函数&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架在AUC、ACC、Recall和F1-Score等关键指标上优于基线方法，在动态拓扑和复杂环境下保持高准确性和稳定性&lt;h4&gt;结论&lt;/h4&gt;该研究不仅为微服务异常检测提供了新的技术路径，也为分布式系统的智能运维奠定了方法论基础&lt;h4&gt;翻译&lt;/h4&gt;本研究解决了微服务架构中的异常检测和根因追踪问题，并提出了一种结合图神经网络与时间建模的统一框架。微服务调用链被抽象为有向图，其中节点和边的多维特征用于构建服务拓扑表示，并应用图卷积来聚合节点特征并建模依赖关系，捕捉服务间的复杂结构关系。在此基础上，引入门控循环单元来建模调用链的时间演化，并使用多层堆叠和连接操作联合获取结构和时间表示，提高识别异常模式的能力。此外，定义了节点和路径级别的异常评分函数，实现从局部异常检测到全局调用链追踪的统一建模，从而能够识别异常服务节点并重建潜在的异常传播路径。随后从超参数、环境干扰和数据分布等多个维度设计了敏感性实验来评估该框架，结果表明其在AUC、ACC、Recall和F1-Score等关键指标上优于基线方法，在动态拓扑和复杂环境下保持高准确性和稳定性。这项研究不仅为微服务异常检测提供了新的技术路径，也为分布式系统的智能运维奠定了方法论基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study addresses the problem of anomaly detection and root cause tracingin microservice architectures and proposes a unified framework that combinesgraph neural networks with temporal modeling. The microservice call chain isabstracted as a directed graph, where multidimensional features of nodes andedges are used to construct a service topology representation, and graphconvolution is applied to aggregate features across nodes and modeldependencies, capturing complex structural relationships among services. Onthis basis, gated recurrent units are introduced to model the temporalevolution of call chains, and multi-layer stacking and concatenation operationsare used to jointly obtain structural and temporal representations, improvingthe ability to identify anomaly patterns. Furthermore, anomaly scoringfunctions at both the node and path levels are defined to achieve unifiedmodeling from local anomaly detection to global call chain tracing, whichenables the identification of abnormal service nodes and the reconstruction ofpotential anomaly propagation paths. Sensitivity experiments are then designedfrom multiple dimensions, including hyperparameters, environmentaldisturbances, and data distribution, to evaluate the framework, and resultsshow that it outperforms baseline methods in key metrics such as AUC, ACC,Recall, and F1-Score, maintaining high accuracy and stability under dynamictopologies and complex environments. This research not only provides a newtechnical path for anomaly detection in microservices but also lays amethodological foundation for intelligent operations in distributed systems.</description>
      <author>example@mail.com (Qingyuan Zhang, Ning Lyu, Le Liu, Yuxi Wang, Ziyu Cheng, Cancan Hua)</author>
      <guid isPermaLink="false">2511.03285v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention</title>
      <link>http://arxiv.org/abs/2511.03178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了PitVQA-Anticipation数据集和SurgAnt-ViVQA模型，用于解决经鼻蝶垂体手术中的前瞻性手术推理问题。通过结合时间感知编码器和细粒度门控交叉注意力，系统能够从回顾性描述转向主动预测，为手术实时辅助提供支持。&lt;h4&gt;背景&lt;/h4&gt;在经鼻蝶垂体手术中，视野有限且工作流程变化迅速，预测即将发生的手术事件对实时辅助至关重要。现有视觉问答系统基于孤立帧进行静态推理，对预测下一步或器械需求支持有限，且现有数据集关注当前场景而非近期未来。&lt;h4&gt;目的&lt;/h4&gt;创建第一个用于前瞻性手术推理的VQA数据集，并提出一种能够预测手术未来阶段、步骤、器械需求和剩余时间的视频语言模型。&lt;h4&gt;方法&lt;/h4&gt;构建了包含33.5小时手术视频和734,769个问答对的PitVQA-Anticipation数据集，涵盖四个预测任务。提出SurgAnt-ViVQA模型，使用双向GRU编码帧动态，通过自适应门将视觉上下文注入语言流，并采用参数高效微调定制语言主干。&lt;h4&gt;主要发现&lt;/h4&gt;SurgAnt-ViVQA在PitVQA-Anticipation和EndoVis数据集上超越了现有基线。消融研究表明时间循环和门控 fusion带来主要性能提升。帧预算研究显示8帧最大化流畅性，32帧略微降低BLEU但改进数值时间估计。&lt;h4&gt;结论&lt;/h4&gt;SurgAnt-ViVQA将手术VQA从回顾性描述推进到主动预测，PitVQA-Anticipation为此提供了全面基准，强调了针对时间建模对可靠、未来感知型手术辅助的重要性。&lt;h4&gt;翻译&lt;/h4&gt;预测即将发生的手术事件对于经鼻蝶垂体手术中的实时辅助至关重要，在这些手术中视野有限且工作流程变化迅速。大多数视觉问答系统基于孤立帧进行推理，使用静态视觉语言对齐，对预测下一步或器械需求提供很少支持。现有的手术VQA数据集同样关注当前场景而非近期未来。我们引入了PitVQA-Anticipation，这是第一个用于前瞻性手术推理的VQA数据集。它包含33.5小时手术视频和734,769个问答对，这些问答对基于时间分组剪辑和专家注释构建，涵盖四个任务：预测未来阶段、下一步、即将使用的器械和剩余时间。我们进一步提出了SurgAnt-ViVQA，这是一个视频语言模型，使用GRU门控时间交叉注意力模块来适应大语言模型。双向GRU编码帧到帧的动态，同时自适应门将视觉上下文注入到令牌级别的语言流中。参数高效微调将语言主干定制到手术领域。SurgAnt-ViVQA在PitVQA-Anticipation和EndoVis数据集上测试，超越了强大的基于图像和视频的基线。消融研究表明，时间循环和门控融合带来了大部分性能提升。帧预算研究表明存在权衡：8帧最大化流畅性，而32帧略微降低BLEU但改进了数值时间估计。通过将时间感知编码器与细粒度门控交叉注意力相结合，SurgAnt-ViVQA推动手术VQA从回顾性描述发展到主动预测。PitVQA-Anticipation为此设置提供了全面的基准，并强调了针对时间建模对可靠、未来感知型手术辅助的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anticipating forthcoming surgical events is vital for real-time assistance inendonasal transsphenoidal pituitary surgery, where visibility is limited andworkflow changes rapidly. Most visual question answering (VQA) systems reasonon isolated frames with static vision language alignment, providing littlesupport for forecasting next steps or instrument needs. Existing surgical VQAdatasets likewise center on the current scene rather than the near future. Weintroduce PitVQA-Anticipation, the first VQA dataset designed for forwardlooking surgical reasoning. It comprises 33.5 hours of operative video and734,769 question answer pairs built from temporally grouped clips and expertannotations across four tasks: predicting the future phase, next step, upcominginstrument, and remaining duration. We further propose SurgAnt-ViVQA, a videolanguage model that adapts a large language model using a GRU Gated TemporalCross-Attention module. A bidirectional GRU encodes frame to frame dynamics,while an adaptive gate injects visual context into the language stream at thetoken level. Parameter efficient fine tuning customizes the language backboneto the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation andEndoVis datasets, surpassing strong image and video based baselines. Ablationsshow that temporal recurrence and gated fusion drive most of the gains. A framebudget study indicates a trade-off: 8 frames maximize fluency, whereas 32frames slightly reduce BLEU but improve numeric time estimation. By pairing atemporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQAadvances surgical VQA from retrospective description to proactive anticipation.PitVQA-Anticipation offers a comprehensive benchmark for this setting andhighlights the importance of targeted temporal modeling for reliable, futureaware surgical assistance.</description>
      <author>example@mail.com (Shreyas C. Dhake, Jiayuan Huang, Runlong He, Danyal Z. Khan, Evangelos B. Mazomenos, Sophia Bano, Hani J. Marcus, Danail Stoyanov, Matthew J. Clarkson, Mobarak I. Hoque)</author>
      <guid isPermaLink="false">2511.03178v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.26241v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究评估了视觉-语言模型(VLMs)对视频中时间信息的理解能力，发现当前模型在判断时间方向方面表现接近随机水平，远低于人类准确度。&lt;h4&gt;背景&lt;/h4&gt;现代视觉-语言模型在许多多模态任务中表现出色，但它们对视频中时间信息的掌握仍然薄弱，且这一点尚未得到充分评估。&lt;h4&gt;目的&lt;/h4&gt;通过判断时间方向(AoT)的挑战来探测VLMs在时间理解方面的差距，即判断短片是正向播放还是反向播放。&lt;h4&gt;方法&lt;/h4&gt;引入AoT-PsyPhyBENCH基准测试，这是一个经过心理物理学验证的评估工具，使用与人类相同的刺激和行为基线来测试VLMs推断自然视频中时间方向的能力。&lt;h4&gt;主要发现&lt;/h4&gt;对各类VLMs的全面评估显示，大多数模型表现接近随机水平，即使在物理不可逆过程和因果手动操作上，表现最好的模型也远低于人类准确度，而人类能几乎立即识别这些过程。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了当前多模态系统的基本差距：虽然模型能捕捉丰富的视觉-语义相关性，但缺乏时间连续性和因果理解所需的归纳偏差。&lt;h4&gt;翻译&lt;/h4&gt;现代视觉-语言模型在许多多模态任务中表现出色，但它们对视频中时间信息的掌握仍然薄弱，并且这一点尚未得到充分评估。我们通过一个看似简单但具有揭示性的挑战来探测这一差距：判断时间方向(AoT)——即判断短片是正向播放还是反向播放。我们引入了AoT-PsyPhyBENCH，这是一个经过心理物理学验证的基准测试，用于测试VLMs是否能推断自然视频中的时间方向，使用与人类相同的刺激和行为基线。对开源和专有、推理和非推理VLMs的全面评估表明，大多数模型的表现接近随机水平，即使在物理不可逆过程(如自由落体、扩散/爆炸)和因果手动操作(分割/添加)上，表现最好的模型也远低于人类的准确度，而这些过程人类几乎可以立即识别。这些结果突显了当前多模态系统中的一个基本差距：虽然它们捕捉了丰富的视觉-语义相关性，但缺乏时间连续性和因果理解所需的归纳偏差。我们发布了AoT-PsyPhyBENCH的代码和数据，以鼓励VLMs在物理和时间推理能力方面的进一步进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern vision-language models (VLMs) excel at many multimodal tasks, yettheir grasp of temporal information in video remains weak and, crucially,under-evaluated. We probe this gap with a deceptively simple but revealingchallenge: judging the arrow of time (AoT)-whether a short clip is playedforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validatedbenchmark that tests whether VLMs can infer temporal direction in naturalvideos using the same stimuli and behavioral baselines established for humans.Our comprehensive evaluation of open-weight and proprietary, reasoning andnon-reasoning VLMs reveals that most models perform near chance, and even thebest lag far behind human accuracy on physically irreversible processes (e.g.,free fall, diffusion/explosion) and causal manual actions (division/addition)that humans recognize almost instantly. These results highlight a fundamentalgap in current multimodal systems: while they capture rich visual-semanticcorrelations, they lack the inductive biases required for temporal continuityand causal understanding. We release the code and data for AoT-PsyPhyBENCH toencourage further progress in the physical and temporal reasoning capabilitiesof VLMs.</description>
      <author>example@mail.com (Shiho Matta, Lis Kanashiro Pereira, Peitao Han, Fei Cheng, Shigeru Kitazawa)</author>
      <guid isPermaLink="false">2510.26241v2</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>SVG Decomposition for Enhancing Large Multimodal Models Visualization Comprehension: A Study with Floor Plans</title>
      <link>http://arxiv.org/abs/2511.03478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了使用可缩放矢量图形(SVG)作为分解策略，以提高大型多模态模型(LMMs)对平面图理解能力的效果。研究分析了三种LMM模型在75个平面图上的表现，发现SVG与光栅输入结合可提高空间理解任务表现，但可能阻碍空间推理能力。&lt;h4&gt;背景&lt;/h4&gt;大型多模态模型(LMMs)虽然越来越能够解释可视化，但在空间推理方面仍然存在困难。平面图是一个有价值的测试平台，因为它们结合了几何、拓扑和语义，且其可靠理解对盲人和低视力人士的无障碍服务有实际应用价值。&lt;h4&gt;目的&lt;/h4&gt;研究可缩放矢量图形(SVG)作为一种分解策略，以提高LMMs对平面图理解的效果。&lt;h4&gt;方法&lt;/h4&gt;进行了一项探索性研究，使用了三个LMMs（GPT-4o、Claude 3.7 Sonnet和Llama 3.2 11B Vision Instruct），分析了75个平面图的表现。&lt;h4&gt;主要发现&lt;/h4&gt;将SVG与光栅输入结合(SVG+PNG)可以提高空间理解任务的表现，但往往阻碍空间推理，特别是在路径查找方面。&lt;h4&gt;结论&lt;/h4&gt;这些发现突显了分解策略在推进空间可视化理解方面的潜力和局限性。&lt;h4&gt;翻译&lt;/h4&gt;大型多模态模型(LMMs)越来越能够解释可视化，但在空间推理方面仍然存在困难。一种提出的策略是分解，将复杂的可视化分解为结构化组件。在这项工作中，我们研究了可缩放矢量图形(SVG)作为一种分解策略的有效性，以提高LMMs对平面图理解的能力。平面图是一个有价值的测试平台，因为它们结合了几何、拓扑和语义，且其可靠理解有实际应用价值，例如为盲人和低视力人士提供无障碍服务。我们对三种LMMs（GPT-4o、Claude 3.7 Sonnet和Llama 3.2 11B Vision Instruct）进行了探索性研究，分析了75个平面图。结果表明，将SVG与光栅输入结合(SVG+PNG)可以提高空间理解任务的表现，但往往阻碍空间推理，特别是在路径查找方面。这些发现突显了分解策略在推进空间可视化理解方面的潜力和局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large multimodal models (LMMs) are increasingly capable of interpretingvisualizations, yet they continue to struggle with spatial reasoning. Oneproposed strategy is decomposition, which breaks down complex visualizationsinto structured components. In this work, we examine the efficacy of scalablevector graphics (SVGs) as a decomposition strategy for improving LMMs'performance on floor plans comprehension. Floor plans serve as a valuabletestbed because they combine geometry, topology, and semantics, and theirreliable comprehension has real-world applications, such as accessibility forblind and low-vision individuals. We conducted an exploratory study with threeLMMs (GPT-4o, Claude 3.7 Sonnet, and Llama 3.2 11B Vision Instruct) across 75floor plans. Results show that combining SVG with raster input (SVG+PNG)improves performance on spatial understanding tasks but often hinders spatialreasoning, particularly in pathfinding. These findings highlight both thepromise and limitations of decomposition as a strategy for advancing spatialvisualization comprehension.</description>
      <author>example@mail.com (Jeongah Lee, Ali Sarvghad)</author>
      <guid isPermaLink="false">2511.03478v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding</title>
      <link>http://arxiv.org/abs/2511.03325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了SurgViVQA，一个专门用于手术领域的视频问答模型，能够捕捉时间连贯的事件而非孤立图像。该模型使用掩码视频-文本编码器融合视频和问题特征，捕捉运动和工具-组织交互等时间线索，由微调的大语言模型解码为连贯答案。作者还创建了REAL-Colon-VQA数据集进行评估，实验表明该模型在关键词准确性上显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;当前手术领域的视频问答方法局限于静态图像特征，可用的数据集通常缺乏时间标注，忽略了准确解读手术程序所需的关键动态信息。手术视频问答旨在通过AI模型对时间连贯事件进行推理来增强手术中的理解。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从静态图像扩展到动态手术场景的视频问答模型，捕捉运动和工具-组织交互等时间线索，提高AI模型对动态手术程序的解读能力。&lt;h4&gt;方法&lt;/h4&gt;提出SurgViVQA模型，使用掩码视频-文本编码器融合视频和问题特征，捕捉时间线索；创建REAL-Colon-VQA数据集，包括与运动相关的问题和诊断属性，以及重新表述或语义改变的问题形式来评估模型鲁棒性；在REAL-Colon-VQA和EndoVis18-VQA数据集上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SurgViVQA在关键词准确性上显著优于现有基于图像的VQA基准模型，在REAL-Colon-VQA上比PitVQA提高11%，在EndoVis18-VQA上提高9%。对问题的扰动研究证实了模型对问题表述变化的泛化能力和鲁棒性得到改善。&lt;h4&gt;结论&lt;/h4&gt;SurgViVQA和REAL-Colon-VQA数据集为手术视频问答中的时间感知理解提供了框架，使AI模型能够更有效地解读动态手术程序上下文。&lt;h4&gt;翻译&lt;/h4&gt;手术领域的视频问答旨在通过使AI模型能够对时间上连贯的事件进行推理，而不是孤立的帧，来增强手术中的理解。当前的方法局限于静态图像特征，可用的数据集通常缺乏时间标注，忽略了准确解读手术程序所需的关键动态信息。我们提出了SurgViVQA，一个手术视频问答模型，它将视觉推理从静态图像扩展到动态手术场景。它使用掩码视频-文本编码器来融合视频和问题特征，捕捉运动和工具-组织交互等时间线索，然后由微调的大语言模型解码为连贯的答案。为了评估其性能，我们创建了REAL-Colon-VQA，一个结肠镜视频数据集，包括与运动相关的问题和诊断属性，以及重新表述或语义改变的问题形式来评估模型的鲁棒性。在REAL-Colon-VQA和公共的EndoVis18-VQA数据集上的实验验证表明，SurgViVQA优于现有的基于图像的VQA基准模型，特别是在关键词准确性方面，在REAL-Colon-VQA上比PitVQA提高了11%，在EndoVis18-VQA上提高了9%。对问题的扰动研究进一步证实了模型对问题表述变化的泛化能力和鲁棒性得到了改善。SurgViVQA和REAL-Colon-VQA数据集为手术视频问答中的时间感知理解提供了框架，使AI模型能够更有效地解读动态手术程序上下文。代码和数据集可在https://github.com/madratak/SurgViVQA获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Question Answering (VideoQA) in the surgical domain aims to enhanceintraoperative understanding by enabling AI models to reason over temporallycoherent events rather than isolated frames. Current approaches are limited tostatic image features, and available datasets often lack temporal annotations,ignoring the dynamics critical for accurate procedural interpretation. Wepropose SurgViVQA, a surgical VideoQA model that extends visual reasoning fromstatic images to dynamic surgical scenes. It uses a Masked Video--Text Encoderto fuse video and question features, capturing temporal cues such as motion andtool--tissue interactions, which a fine-tuned large language model (LLM) thendecodes into coherent answers. To evaluate its performance, we curatedREAL-Colon-VQA, a colonoscopic video dataset that includes motion-relatedquestions and diagnostic attributes, as well as out-of-template questions withrephrased or semantically altered formulations to assess model robustness.Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA datasetshows that SurgViVQA outperforms existing image-based VQA benchmark models,particularly in keyword accuracy, improving over PitVQA by +11\% onREAL-Colon-VQA and +9\% on EndoVis18-VQA. A perturbation study on the questionsfurther confirms improved generalizability and robustness to variations inquestion phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a frameworkfor temporally-aware understanding in surgical VideoQA, enabling AI models tointerpret dynamic procedural contexts more effectively. Code and datasetavailable at https://github.com/madratak/SurgViVQA.</description>
      <author>example@mail.com (Mauro Orazio Drago, Luca Carlini, Pelinsu Celebi Balyemez, Dennis Pierantozzi, Chiara Lena, Cesare Hassan, Danail Stoyanov, Elena De Momi, Sophia Bano, Mobarak I. Hoque)</author>
      <guid isPermaLink="false">2511.03325v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity</title>
      <link>http://arxiv.org/abs/2511.03146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MME-CC基准测试，用于评估多模态大语言模型在视觉信息处理方面的认知能力，通过16个模型的实验发现闭源模型整体领先，但空间和几何推理能力普遍较弱，并确定了常见错误模式。&lt;h4&gt;背景&lt;/h4&gt;随着推理模型的迅速扩展，多模态在人类认知中的重要作用日益凸显，但现有多模态基准测试要么过度强调文本推理，要么未能系统地捕捉以视觉为中心的认知行为，导致对MLLMs的认知能力评估不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态基准测试的局限性，引入MME-CC基准测试，系统评估MLLMs在视觉信息处理方面的认知能力。&lt;h4&gt;方法&lt;/h4&gt;创建MME-CC基准测试，将11个代表性推理任务组织为空间、几何和基于知识的三个基本类别，并对16个代表性MLLMs进行广泛实验。&lt;h4&gt;主要发现&lt;/h4&gt;闭源模型目前整体领先（如Gemini-2.5-Pro得分为42.66，GLM-4.5V为30.45）；空间和几何推理能力普遍较弱（≤30%）；常见错误包括方向错误、脆弱的跨视图身份持久性和对反事实指令的遵循不良；思维链通常遵循提取→推理→验证的三阶段过程，且严重依赖视觉提取。&lt;h4&gt;结论&lt;/h4&gt;希望这项工作能促使将MLLMs的认知能力作为评估和模型设计的中心。&lt;h4&gt;翻译&lt;/h4&gt;随着推理模型的迅速扩展，多模态在人类认知中的重要作用日益凸显，促使人们需要探索以视觉为中心的认知行为。然而，现有的多模态基准测试要么过度强调文本推理，要么未能系统地捕捉以视觉为中心的认知行为，导致对MLLMs的认知能力评估不足。为了解决这一局限性，我们引入了MME-CC（认知能力多模态评估基准），这是一个以视觉为基础的基准测试，将11个代表性的推理任务组织为三个基本类别：空间、几何和基于知识的推理，并提供了MLLMs在这些维度上认知能力的细粒度分析。基于MME-CC，我们对16个代表性的MLLMs进行了广泛的实验。我们的研究表明，闭源模型目前整体领先（例如，Gemini-2.5-Pro得分为42.66，而GLM-4.5V得分为30.45），而空间和几何推理能力普遍较弱（小于或等于30%）。我们进一步确定了常见的错误模式，包括方向错误、脆弱的跨视图身份持久性以及未能很好地遵循反事实指令，并观察到思维链通常遵循三阶段过程（提取→推理→验证），且严重依赖视觉提取。我们希望这项工作能够促使将MLLMs的认知能力作为评估和模型设计的中心。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As reasoning models scale rapidly, the essential role of multimodality inhuman cognition has come into sharp relief, driving a growing need to probevision-centric cognitive behaviors. Yet, existing multimodal benchmarks eitheroveremphasize textual reasoning or fall short of systematically capturingvision-centric cognitive behaviors, leaving the cognitive capacity of MLLMsinsufficiently assessed. To address this limitation, we introduce MME-CC(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-groundedbenchmark that organizes 11 representative reasoning tasks into threefundamental categories of visual information: spatial, geometric, andknowledge-based reasoning, and provides fine-grained analyses of MLLMs'cognitive capacity across these dimensions. Based on MME-CC, we conductextensive experiments over 16 representative MLLMs. Our study reveals thatclosed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak(less than or equal to 30%). We further identify common error patterns,including orientation mistakes, fragile cross-view identity persistence, andpoor adherence to counterfactual instructions, and observe thatChain-of-Thought typically follows a three-stage process (extract -&gt; reason -&gt;verify) with heavy reliance on visual extraction. We hope this work catalyzes ashift toward treating the cognitive capacity of MLLMs as central to bothevaluation and model design.</description>
      <author>example@mail.com (Kaiyuan Zhang, Chenghao Yang, Zhoufutu Wen, Sihang Yuan, Qiuyue Wang, Chaoyi Huang, Guosheng Zhu, He Wang, Huawenyu Lu, Jianing Wen, Jianpeng Jiao, Lishu Luo, Longxiang Liu, Sijin Wu, Xiaolei Zhu, Xuanliang Zhang, Ge Zhang, Yi Lin, Guang Shi, Chaoyou Fu, Wenhao Huang)</author>
      <guid isPermaLink="false">2511.03146v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN</title>
      <link>http://arxiv.org/abs/2511.03634v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;nanoTabPFN是一个简化的TabPFN v2架构实现，使表格基础模型对学生和研究人员更易访问，在小数据场景下性能与传统机器学习基线相当，且预训练速度快160,000倍。&lt;h4&gt;背景&lt;/h4&gt;表格基础模型如TabPFN已革新表格数据的预测机器学习，但现有开源实现复杂（超过10,000行代码），缺乏架构文档和代码质量，难以理解和适应新实验。&lt;h4&gt;目的&lt;/h4&gt;解决现有表格基础模型实现复杂、难以理解、对初学者不友好的问题，使表格基础模型更易于教育和研究使用。&lt;h4&gt;方法&lt;/h4&gt;引入nanoTabPFN，作为TabPFN v2架构的简化轻量级实现，并使用预生成的训练数据实现相应的训练循环。&lt;h4&gt;主要发现&lt;/h4&gt;在小数据场景下，nanoTabPFN在单个GPU上进行一分钟预训练后性能与传统机器学习基线相当，预训练速度比TabPFN v2快160,000倍，消除了对大型计算资源的需求。&lt;h4&gt;结论&lt;/h4&gt;nanoTabPFN使表格基础模型更易于访问，其代码已在GitHub上提供（https://github.com/automl/nanoTabPFN）。&lt;h4&gt;翻译&lt;/h4&gt;表格基础模型如TabPFN已经革新了表格数据的预测机器学习。同时，这一革命的驱动因素难以理解。现有的开源表格基础模型在复杂的管道中实现，拥有超过10,000行代码，缺乏架构文档或代码质量。简而言之，这些实现难以理解，对初学者不友好，且难以适应新实验。我们引入了nanoTabPFN，这是TabPFN v2架构的简化和轻量级实现，以及使用预生成训练数据的相应训练循环。nanoTabPFN使表格基础模型对学生和研究人员都更加易于访问。例如，限制在小数据场景下，它在单个GPU上进行一分钟预训练后，实现了与传统机器学习基线相当的性能（比TabPFN v2预训练快160,000倍）。这种对大型计算资源需求的消除使表格基础模型的预训练可用于教育目的。我们的代码可在https://github.com/automl/nanoTabPFN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular foundation models such as TabPFN have revolutionized predictivemachine learning for tabular data. At the same time, the driving factors ofthis revolution are hard to understand. Existing open-source tabular foundationmodels are implemented in complicated pipelines boasting over 10,000 lines ofcode, lack architecture documentation or code quality. In short, theimplementations are hard to understand, not beginner-friendly, and complicatedto adapt for new experiments. We introduce nanoTabPFN, a simplified andlightweight implementation of the TabPFN v2 architecture and a correspondingtraining loop that uses pre-generated training data. nanoTabPFN makes tabularfoundation models more accessible to students and researchers alike. Forexample, restricted to a small data setting it achieves a performancecomparable to traditional machine learning baselines within one minute ofpre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). Thiseliminated requirement of large computational resources makes pre-trainingtabular foundation models accessible for educational purposes. Our code isavailable at https://github.com/automl/nanoTabPFN.</description>
      <author>example@mail.com (Alexander Pfefferle, Johannes Hog, Lennart Purucker, Frank Hutter)</author>
      <guid isPermaLink="false">2511.03634v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained Robot Policy Enhancement</title>
      <link>http://arxiv.org/abs/2511.03400v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, Accepted by IEEE IROS 2025 Workshop WIR-M&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GUIDES是一个轻量级框架，通过注入基础模型的语义指导来增强预训练的机器人策略，无需重新设计架构。该方法结合了微调的视觉语言模型生成上下文指令，并通过辅助模块编码为引导嵌入，注入到策略的潜在空间中。此外，基于大语言模型的Reflector模块在推理时监控指导模型的置信度，并在置信度低时启动推理循环以增强鲁棒性。实验表明GUIDES能显著提高任务成功率并增强运动精度。&lt;h4&gt;背景&lt;/h4&gt;预训练的机器人策略是许多已验证的机器人系统的基础，包含大量具身知识。然而，它们通常缺乏基础模型特有的语义感知能力，而完全替换这些策略在许多情况下是不切实际的，因为成本高昂且会损失积累的知识。&lt;h4&gt;目的&lt;/h4&gt;解决预训练机器人策略缺乏语义感知能力的问题，同时避免完全替换这些策略带来的高成本和知识损失，提供一种实用且资源高效的升级途径。&lt;h4&gt;方法&lt;/h4&gt;GUIDES框架通过以下步骤实现：1)使用微调的视觉语言模型(Instructor)生成上下文指令；2)通过辅助模块将指令编码为引导嵌入；3)将嵌入注入到策略的潜在空间中；4)通过短暂微调使传统模型适应新语义输入；5)使用基于大语言模型的Reflector模块监控置信度；6)当置信度低时启动推理循环分析历史并优化后续行动。&lt;h4&gt;主要发现&lt;/h4&gt;在RoboCasa仿真环境中对多种策略架构进行的广泛验证显示，GUIDES在任务成功率方面有一致且显著的提高。在UR5机器人上的实际部署进一步表明，GUIDES增强了抓取等关键子任务的运动精度。&lt;h4&gt;结论&lt;/h4&gt;GUIDES提供了一种实用且资源高效的途径来升级而非替换已验证的机器人策略，使传统策略能够获得基础模型的语义感知能力，同时保留其积累的知识和经验。&lt;h4&gt;翻译&lt;/h4&gt;预训练的机器人策略是许多已验证的机器人系统的基础，它们封装了大量具身知识。然而，它们通常缺乏基础模型特有的语义感知能力，并且在许多情况下完全替换它们是不切实际的，因为成本高昂且会损失积累的知识。为了解决这一差距，我们引入了GUIDES，这是一个轻量级框架，通过基础模型的语义指导增强预训练策略，而无需重新设计架构。GUIDES使用微调的视觉语言模型(Instructor)生成上下文指令，这些指令由辅助模块编码为引导嵌入。这些嵌入被注入到策略的潜在空间中，使传统模型能够通过短暂、有针对性的微调来适应这种新的语义输入。为了提高推理时的鲁棒性，基于大语言模型的Reflector模块监控Instructor的置信度，当置信度低时，启动一个推理循环，分析执行历史，检索相关示例，并增强VLM的上下文以细化后续行动。在RoboCasa仿真环境中对多种策略架构进行的广泛验证显示，任务成功率有一致且显著的提高。在UR5机器人上的实际部署进一步表明，GUIDES增强了抓取等关键子任务的运动精度。总体而言，GUIDES提供了一种实用且资源高效的途径来升级而非替换已验证的机器人策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained robot policies serve as the foundation of many validated roboticsystems, which encapsulate extensive embodied knowledge. However, they oftenlack the semantic awareness characteristic of foundation models, and replacingthem entirely is impractical in many situations due to high costs and the lossof accumulated knowledge. To address this gap, we introduce GUIDES, alightweight framework that augments pre-trained policies with semantic guidancefrom foundation models without requiring architectural redesign. GUIDES employsa fine-tuned vision-language model (Instructor) to generate contextualinstructions, which are encoded by an auxiliary module into guidanceembeddings. These embeddings are injected into the policy's latent space,allowing the legacy model to adapt to this new semantic input through brief,targeted fine-tuning. For inference-time robustness, a large languagemodel-based Reflector monitors the Instructor's confidence and, when confidenceis low, initiates a reasoning loop that analyzes execution history, retrievesrelevant examples, and augments the VLM's context to refine subsequent actions.Extensive validation in the RoboCasa simulation environment across diversepolicy architectures shows consistent and substantial improvements in tasksuccess rates. Real-world deployment on a UR5 robot further demonstrates thatGUIDES enhances motion precision for critical sub-tasks such as grasping.Overall, GUIDES offers a practical and resource-efficient pathway to upgrade,rather than replace, validated robot policies.</description>
      <author>example@mail.com (Minquan Gao, Xinyi Li, Qing Yan, Xiaojian Sun, Xiaopan Zhang, Chien-Ming Huang, Jiachen Li)</author>
      <guid isPermaLink="false">2511.03400v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2511.03251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出GMoPE框架，结合专家混合架构与基于提示的图学习，提高图神经网络跨领域泛化能力并降低适应成本&lt;h4&gt;背景&lt;/h4&gt;图神经网络在特定任务上表现优异，但跨领域和任务泛化能力有限，现有方法存在负迁移、可扩展性问题和高适应成本&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提高图神经网络的泛化能力和效率&lt;h4&gt;方法&lt;/h4&gt;提出GMoPE框架，利用专家特定提示向量和结构感知的MoE路由，引入提示向量间软正交约束促进专家多样性，采用仅提示微调策略降低时空复杂度&lt;h4&gt;主要发现&lt;/h4&gt;实验表明GMoPE优于最先进基线，性能接近完整参数微调，但适应开销显著降低&lt;h4&gt;结论&lt;/h4&gt;GMoPE为推进可泛化和高效的图基础模型提供了有原则且可扩展的框架&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在特定任务基准上表现出色，但它们跨不同领域和任务的泛化能力仍然有限。现有方法通常难以处理负迁移、可扩展性问题和高适应成本。为应对这些挑战，我们提出了GMoPE（图专家混合提示），一种将专家混合架构与基于提示的图学习无缝集成的新框架。GMoPE利用专家特定的提示向量和结构感知的MoE路由，使每个专家能够专注于不同的子域，并动态贡献预测。为了促进多样性和防止专家坍塌，我们在提示向量之间引入了软正交约束，鼓励专家专业化并促进更平衡的专家利用。此外，我们采用仅提示微调策略，显著减少了迁移过程中的时空复杂度。我们通过各种预训练策略和多个下游任务的广泛实验验证了GMoPE。结果表明，GMoPE始终优于最先进的基线方法，并且实现了与完整参数微调相当的性能，同时只需要一小部分的适应开销。我们的工作为推进可泛化和高效的图基础模型提供了一个有原则且可扩展的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated impressive performance ontask-specific benchmarks, yet their ability to generalize across diversedomains and tasks remains limited. Existing approaches often struggle withnegative transfer, scalability issues, and high adaptation costs. To addressthese challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novelframework that seamlessly integrates the Mixture-of-Experts (MoE) architecturewith prompt-based learning for graphs. GMoPE leverages expert-specific promptvectors and structure-aware MoE routing to enable each expert to specialize indistinct subdomains and dynamically contribute to predictions. To promotediversity and prevent expert collapse, we introduce a soft orthogonalityconstraint across prompt vectors, encouraging expert specialization andfacilitating a more balanced expert utilization. Additionally, we adopt aprompt-only fine-tuning strategy that significantly reduces spatiotemporalcomplexity during transfer. We validate GMoPE through extensive experimentsunder various pretraining strategies and multiple downstream tasks. Resultsshow that GMoPE consistently outperforms state-of-the-art baselines andachieves performance comparable to full parameter fine-tuning-while requiringonly a fraction of the adaptation overhead. Our work provides a principled andscalable framework for advancing generalizable and efficient graph foundationmodels.</description>
      <author>example@mail.com (Zhibin Wang, Zhixing Zhang, Shuqi Wang, Xuanting Xie, Zhao Kang)</author>
      <guid isPermaLink="false">2511.03251v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>SENT Map - Semantically Enhanced Topological Maps with Foundation Models</title>
      <link>http://arxiv.org/abs/2511.03165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICRA 2025 Workshop on Foundation Models and  Neuro-Symbolic AI for Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SENT-Map是一种语义增强的拓扑地图，用于表示室内环境，通过基础模型支持机器人的自主导航和操作。&lt;h4&gt;背景&lt;/h4&gt;室内环境的表示对机器人自主导航和操作至关重要，基础模型的发展为环境表示和规划提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够结合语义信息的基础模型友好的室内环境表示方法，使机器人能够在规划过程中避免不可行状态。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段方法：首先使用视觉基础模型与操作员一起映射环境；然后使用SENT-Map表示和自然语言查询在基础模型中进行规划。SENT-Map以JSON文本格式表示环境，使人类和基础模型都能理解并编辑语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;语义增强使即使是小型本地部署的基础模型也能够成功规划室内环境。&lt;h4&gt;结论&lt;/h4&gt;SENT-Map通过JSON文本格式表示环境，支持语义信息的添加和编辑，同时帮助机器人在规划过程中避免不可行状态。&lt;h4&gt;翻译&lt;/h4&gt;我们引入SENT-Map，一种用于表示室内环境的语义增强拓扑地图，旨在通过利用基础模型的进步来支持自主导航和操作。通过以JSON文本格式表示环境，我们能够以人类和基础模型都能理解的方式添加和编辑语义信息，同时在规划过程中将机器人与现有节点绑定，以避免部署过程中的不可行状态。我们提出的框架采用两阶段方法，首先使用视觉基础模型与操作员一起映射环境，然后使用SENT-Map表示和自然语言查询在基础模型中进行规划。我们的实验结果表明，语义增强使即使是小型本地部署的基础模型也能够成功规划室内环境。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何让机器人在复杂人类环境中进行自主导航和操作的问题，同时利用基础模型的能力但避免其幻觉和虚假置信度等风险。这个问题很重要，因为基础模型虽提供了强大的语义理解和规划能力，但在实际应用中存在可靠性问题，而将基础模型与现实世界位置相结合并通过拓扑地图表示，可以让机器人更安全、可靠地执行任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过认识到基础模型的潜力与风险，设计了一个两阶段框架：映射阶段和规划/执行阶段。在映射阶段，人类引导机器人构建环境地图并标记语义节点；在规划阶段，利用基础模型生成任务计划。作者借鉴了现有工作如SLAM、视觉语言模型和对象中心映射方法，但解决了这些方法的高计算需求、缺乏操作推理、3D重建困难以及地图不可编辑等问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一种语义增强的拓扑地图(SENT-Map)，结合基础模型能力与人类可编辑的JSON表示，使机器人能理解环境语义并可靠执行任务。实现流程分两阶段：1)映射阶段：人类引导机器人探索环境，机器人拍摄关键位置图像，基础模型生成JSON格式的语义节点，人类可编辑完善；2)规划阶段：基础模型根据SENT-Map、技能描述和自然语言命令生成可执行计划，机器人执行这些计划完成导航和操作任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)SENT-Map：一种人类可编辑的JSON格式语义增强拓扑地图；2)使用基础模型构建和规划SENT-Map的框架；3)实验证明SENT-Map能提高基础模型规划成功率。相比之前工作，SENT-Map解决了高计算需求、缺乏操作保证、3D重建困难、地图不可验证等问题，通过JSON格式使地图可由人类验证和编辑，同时支持开放集语义增强和自然语言任务规范。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SENT-Map通过结合人类引导的基础模型和可编辑的JSON表示，使机器人能够在复杂环境中更可靠地理解和执行自然语言指令的导航和操作任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce SENT-Map, a semantically enhanced topological map forrepresenting indoor environments, designed to support autonomous navigation andmanipulation by leveraging advancements in foundational models (FMs). Throughrepresenting the environment in a JSON text format, we enable semanticinformation to be added and edited in a format that both humans and FMsunderstand, while grounding the robot to existing nodes during planning toavoid infeasible states during deployment. Our proposed framework employs a twostage approach, first mapping the environment alongside an operator with aVision-FM, then using the SENT-Map representation alongside a natural-languagequery within an FM for planning. Our experimental results show thatsemantic-enhancement enables even small locally-deployable FMs to successfullyplan over indoor environments.</description>
      <author>example@mail.com (Raj Surya Rajendran Kathirvel, Zach A Chavis, Stephen J. Guy, Karthik Desingh)</author>
      <guid isPermaLink="false">2511.03165v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation</title>
      <link>http://arxiv.org/abs/2511.03163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种深度引导的肝脏地标分割框架，通过视觉基础编码器整合语义和几何线索，并引入SRFT-GaLore方法高效适应大型视觉模型，在腹腔镜肝脏手术中实现了精确的解剖结构检测和分割。&lt;h4&gt;背景&lt;/h4&gt;医学影像中解剖结构的精确检测和描绘对计算机辅助手术至关重要，尤其在腹腔镜肝脏手术中，2D视频流限制了深度感知，使地标定位复杂化。现有研究利用单目深度线索增强地标检测，但在融合RGB和深度特征以及高效调整大规模视觉模型方面仍有挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种深度引导的肝脏地标分割框架，整合语义和几何线索，高效适应大型视觉模型，并评估其跨数据集泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用Segment Anything Model V2 (SAM2)编码器提取RGB特征，Depth Anything V2 (DA2)编码器提取深度感知特征；引入SRFT-GaLore低秩梯度投影方法替代计算昂贵的SVD；实现交叉注意力融合模块整合RGB和深度线索；构建新的腹腔镜肝脏手术数据集(LLSD)作为外部验证基准。&lt;h4&gt;主要发现&lt;/h4&gt;在公共L3D数据集上，Dice相似系数提高4.85%，平均对称表面距离减少11.78个百分点；在LLSD数据集上，模型保持竞争性性能，显著优于基于SAM的基线，展示了强大的跨数据集鲁棒性和对未见手术环境的适应性。&lt;h4&gt;结论&lt;/h4&gt;SRFT-GaLore增强的双编码器框架能够在实时、深度受限的手术设置中实现可扩展和精确的分割。&lt;h4&gt;翻译&lt;/h4&gt;医学影像中解剖结构的精确检测和描绘对计算机辅助手术至关重要，特别是在腹腔镜肝脏手术中，2D视频流限制了深度感知并使地标定位复杂化。虽然最近的工作已利用单目深度线索增强地标检测，但在融合RGB和深度特征以及高效调整大规模视觉模型以适应手术领域方面仍存在挑战。我们提出了一种深度引导的肝脏地标分割框架，通过视觉基础编码器整合语义和几何线索。我们采用Segment Anything Model V2 (SAM2)编码器提取RGB特征，Depth Anything V2 (DA2)编码器提取深度感知特征。为高效适应SAM2，我们引入了SRFT-GaLore，一种新颖的低秩梯度投影方法，用子采样随机傅里叶变换(SRFT)替代计算昂贵的SVD。这能够在不牺牲表征能力的情况下高效微调高维注意力层。交叉注意力融合模块进一步整合RGB和深度线索。为评估跨数据集泛化能力，我们还构建了一个新的腹腔镜肝脏手术数据集(LLSD)作为外部验证基准。在公共L3D数据集上，我们的方法比D2GPLand提高了4.85%的Dice相似系数，并将平均对称表面距离降低了11.78个百分点。为进一步评估泛化能力，我们在LLSD数据集上评估了我们的模型。我们的模型保持竞争性性能，并显著优于基于SAM的基线，展示了强大的跨数据集鲁棒性和对未见手术环境的适应性。这些结果表明，我们的SRFT-GaLore增强的双编码器框架能够在实时、深度受限的手术设置中实现可扩展和精确的分割。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate detection and delineation of anatomical structures in medicalimaging are critical for computer-assisted interventions, particularly inlaparoscopic liver surgery where 2D video streams limit depth perception andcomplicate landmark localization. While recent works have leveraged monoculardepth cues for enhanced landmark detection, challenges remain in fusing RGB anddepth features and in efficiently adapting large-scale vision models tosurgical domains. We propose a depth-guided liver landmark segmentationframework integrating semantic and geometric cues via vision foundationencoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGBfeatures and Depth Anything V2 (DA2) encoder to extract depth-aware features.To efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradientprojection method that replaces the computationally expensive SVD with aSubsampled Randomized Fourier Transform (SRFT). This enables efficientfine-tuning of high-dimensional attention layers without sacrificingrepresentational power. A cross-attention fusion module further integrates RGBand depth cues. To assess cross-dataset generalization, we also construct a newLaparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark.On the public L3D dataset, our method achieves a 4.85% improvement in DiceSimilarity Coefficient and a 11.78-point reduction in Average Symmetric SurfaceDistance compared to the D2GPLand. To further assess generalization capability,we evaluate our model on LLSD dataset. Our model maintains competitiveperformance and significantly outperforms SAM-based baselines, demonstratingstrong cross-dataset robustness and adaptability to unseen surgicalenvironments. These results demonstrate that our SRFT-GaLore-enhanceddual-encoder framework enables scalable and precise segmentation underreal-time, depth-constrained surgical settings.</description>
      <author>example@mail.com (Yun-Chen Lin, Jiayuan Huang, Hanyuan Zhang, Sergi Kavtaradze, Matthew J. Clarkson, Mobarak I. Hoque)</author>
      <guid isPermaLink="false">2511.03163v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction</title>
      <link>http://arxiv.org/abs/2511.03149v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为Forecast2Anomaly (F2A)的新框架，使时间序列基础模型(TSFMs)具备异常预测能力，通过联合预测-异常损失和检索增强生成(RAG)模块实现，能够在不更新模型的情况下跟踪演变的异常。&lt;h4&gt;背景&lt;/h4&gt;在不同现实世界、动态和复杂系统中预测多变量时间序列中的异常对于预防关键故障至关重要。现有方法仅适用于特定系统，无法随时间演变泛化到异常模式。尽管预训练的时间序列基础模型展示了强大的泛化和零样本预测能力，但其异常预测潜力尚未被开发。&lt;h4&gt;目的&lt;/h4&gt;开发一个新框架，使预训练的时间序列基础模型(TSFMs)具备异常预测能力，弥合TSFM零样本预测和零样本异常预测之间的差距。&lt;h4&gt;方法&lt;/h4&gt;F2A框架包含两个关键创新：1) 提出联合预测-异常损失，微调TSFMs以准确预测异常时间点的未来信号；2) 引入检索增强生成(RAG)模块，检索历史上相关的范围并基于它们进行条件预测，使模型在推理时能动态适应分布变化。&lt;h4&gt;主要发现&lt;/h4&gt;在16个不同数据集和多个TSFM骨干网络上的广泛实验表明，F2A持续优于最先进的方法，提供了可扩展的零样本异常预测解决方案。&lt;h4&gt;结论&lt;/h4&gt;通过结合目标微调和动态检索，F2A成功将TSFM的零样本预测能力扩展到零样本异常预测，为实际应用提供了一种有效的异常预测解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要翻译：来自不同现实世界、动态和复杂系统的多变量时间序列中的异常预测（异常预测）对于预防关键故障至关重要，能够显著降低运营成本和人工劳动。然而，现有方法仅适用于特定系统，无法随时间演变泛化到异常模式。相比之下，预训练的时间序列基础模型(TSFMs)最近展示了强大的泛化和零样本预测能力。然而，它们在异常预测方面的潜力尚未被开发，因为异常预测与预测正常行为的任务根本不同。因此，我们提出了Forecast2Anomaly (F2A)，一个新颖的框架，通过两个关键创新使TSFMs具备异常预测能力。首先，我们提出了一种联合预测-异常损失，微调TSFMs以准确预测异常时间点的未来信号。其次，我们引入了检索增强生成(RAG)模块，检索历史上相关的范围并基于它们进行条件预测。该组件在推理时动态适应分布变化，使F2A能够在不更新模型的情况下跟踪演变的异常。通过结合目标微调和动态检索，F2A弥合了健壮的TSFM零样本预测和零样本异常预测之间的差距。在16个不同数据集和多个TSFM骨干网络上的广泛实验表明，F2A持续优于最先进的方法，为实际应用提供了可扩展的零样本异常预测解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forecasting anomalies (anomaly prediction) in multivariate time series fromdifferent real-world, dynamic, and complex systems is vital for preemptingcritical failures, leading to a substantial minimization in operational costsand human labor. Yet, existing methods are limited to specific systems whilefailing to generalize to evolving anomaly patterns over time. In contrast,pretrained Time Series Foundation Models (TSFMs) have recently demonstratedstrong generalization and zero-shot forecasting capabilities. However, theirpotential remains untapped for anomaly prediction, a task fundamentallydifferent from forecasting normal behavior. Thus, we present Forecast2Anomaly(F2A), a novel framework that empowers TSFMs with anomaly prediction abilitiesthrough two key innovations. First, we propose a joint forecast-anomaly lossthat fine-tunes TSFMs to accurately forecast future signals even at anomaloustime points. Second, we introduce a Retrieval-Augmented Generation (RAG) modulethat retrieves historically relevant horizons and conditions predictions onthem. This component dynamically adapts to distributional shifts at inferencetime, enabling F2A to track evolving anomalies without requiring model updates.By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gapbetween robust TSFM zero-shot forecasting and zero-shot anomaly prediction.Extensive experiments across 16 diverse datasets and multiple TSFM backbonesshow that F2A consistently outperforms state-of-the-art methods, offering ascalable, zero-shot anomaly prediction solution for real-world applications.</description>
      <author>example@mail.com (Atif Hassan, Tarun Kumar, Ashish Mishra, Sergey Serebryakov, Satish Kumar Mopur, Phanidhar Koganti, Murthy Chelankuri, Ramanagopal Vogety, Suparna Bhattacharya, Martin Foltin)</author>
      <guid isPermaLink="false">2511.03149v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>A Foundation Model for Brain MRI with Dynamic Modality Integration</title>
      <link>http://arxiv.org/abs/2511.03014v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preliminary work; results ongoing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种脑部MRI基础模型，能够处理不同成像序列的组合，使用单一编码器架构，通过可学习模态嵌入和条件层归一化技术，实现了对缺失模态的自适应处理，无需为每种模态单独建模。&lt;h4&gt;背景&lt;/h4&gt;传统脑部MRI分析方法通常需要为不同成像序列分别训练模型，当某些模态缺失或未见时，模型性能会显著下降，限制了临床应用的实际价值。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够灵活处理不同成像序列组合的脑部MRI基础模型，使其能够在模态缺失或未见的情况下保持良好性能，提高模型的实用性和适应性。&lt;h4&gt;方法&lt;/h4&gt;使用单个编码器架构，配备可学习模态嵌入和条件层归一化技术；采用考虑缺失模态的掩码自编码目标函数；应用方差-协方差正则化器稳定特征学习；在约60,000多中心MRI数据上通过自监督重建和模态插值进行训练；利用可学习模态嵌入引导特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;模型能够有效处理不同模态组合；在序列缺失或未见时能够自适应调整；方差-协方差正则化器有助于稳定特征学习并提高表示多样性；初步结果显示该方法可行。&lt;h4&gt;结论&lt;/h4&gt;所提出的脑部MRI基础模型能够灵活处理不同成像序列组合，无需为每种模态单独建模，在模态缺失情况下仍能保持良好性能，代码和预训练模型已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种脑部MRI基础模型，它可以处理不同成像序列的组合。该模型使用一个带有可学习模态嵌入的编码器、条件层归一化，以及一个考虑缺失模态的掩码自编码目标函数。应用了方差-协方差正则化器来稳定特征学习并提高表示多样性。这种设计消除了为每种模态单独建模的需要，并允许网络在某些序列缺失或未见时进行自适应调整。模型在约60,000多中心MRI上通过自监督重建和模态插值进行训练，以学习灵活的表示。可学习的模态嵌入引导特征提取，使编码器能够适应不同的输入。我们描述了计划在脑肿瘤和多发性硬化症分割以及病变分类方面的评估，在各种模态设置下进行。初步结果显示该方法可行，并计划进行更多实验以更详细地研究其性能。所有代码和预训练模型可在https://github.com/BrainFM/brainfm获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决脑部MRI分析中不同成像序列组合不一致的问题。现实中，不同医院使用的MRI协议不同，导致可用的成像序列（如T1、T2、FLAIR等）存在差异，现有方法通常需要为每种模态组合训练单独的模型，效率低下且难以处理缺失或未见过的新模态组合。这个问题限制了医学影像分析的泛化能力和实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了多个现有工作：自然语言处理和计算机视觉中的掩码自编码器、医学影像的自监督学习方法（如Models Genesis）以及多模态MRI处理技术（如AMAES、M4oE、MoME和mmFormer）。作者首先分析了现有方法的局限性，然后设计了一个单一编码器结合可学习模态嵌入的架构，通过条件层归一化和掩码自编码目标使模型能够适应不同模态组合，并应用方差-协方差正则化来稳定特征学习。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用单一编码器处理不同模态组合的MRI数据，通过可学习的模态嵌入和条件层归一化使模型能够自适应不同输入。整体流程包括：1)数据准备与预处理，将MRI调整为统一大小并应用数据增强；2)模型架构，将MRI分割为3D块并添加模态和位置嵌入；3)使用条件层归一化的Transformer编码器处理可见块，轻量级解码器重建被掩盖块；4)训练目标结合掩码重建和方差-协方差正则化；5)下游任务适配时丢弃解码器，保留编码器。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)动态模态集成，可处理任意MRI序列组合；2)单一编码器架构，无需为每种模态组合创建单独模型；3)可学习模态嵌入，能泛化到未见过序列；4)条件层归一化，使编码器自适应不同模态；5)模态感知掩码重建，提高对缺失数据的鲁棒性；6)方差-协方差正则化，稳定特征学习。相比之前工作，它超越了AMAES的单模态限制，避免了MoME和M4oE需要专家网络的复杂性，比mmFormer更专注于广泛下游应用，且能处理未见过的模态组合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了BrainFM-MRI，一个能够动态适应不同MRI模态组合的基础模型，通过单一编码器和可学习模态嵌入解决了医疗成像中模态不一致和缺失的问题，为脑部MRI分析提供了更加灵活和鲁棒的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a foundation model for brain MRI that can work with differentcombinations of imaging sequences. The model uses one encoder with learnablemodality embeddings, conditional layer normalization, and a masked autoencodingobjective that accounts for missing modalities. A variance-covarianceregularizer is applied to stabilize feature learning and improve representationdiversity. This design removes the need for separate models for each modalityand allows the network to adapt when some sequences are missing or unseen. Itis trained on about 60,000 multi-center MRIs using self-supervisedreconstruction and modality imputation to learn flexible representations. Alearnable modality embedding guides feature extraction so the encoder canadjust to different inputs. We describe our planned evaluation on brain tumorand multiple sclerosis segmentation, as well as lesion classification, undervarious modality settings. Preliminary results show that the method worksfeasibly, and further experiments are planned to study its performance in moredetail. All code and pretrained models are available athttps://github.com/BrainFM/brainfm</description>
      <author>example@mail.com (Minh Sao Khue Luu, Bair N. Tuchinov)</author>
      <guid isPermaLink="false">2511.03014v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Zero-shot data citation function classification using transformer-based large language models (LLMs)</title>
      <link>http://arxiv.org/abs/2511.02936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究应用开源大型语言模型Llama 3.1-405B为引用特定基因组数据集的出版物生成结构化的数据使用案例标签，并引入新的评估框架验证方法有效性。结果显示模型在零样本分类任务上达到0.674的F1分数，但面临数据可用性、提示过拟合等挑战。&lt;h4&gt;背景&lt;/h4&gt;近年来，确定特定数据集与包含这些数据集的科学文献之间关联的努力有所增加。当已知某出版物引用了某数据集后，探索该数据如何或为何被使用成为下一步逻辑。基于预训练转换器的大型语言模型进步为扩展文献中数据使用案例描述提供了新途径，避免了传统机器学习系统需要昂贵的手动标注和训练数据集开发。&lt;h4&gt;目的&lt;/h4&gt;应用开源LLM Llama 3.1-405B为已知包含特定基因组数据集的出版物生成结构化数据使用案例标签，并引入新的评估框架确定方法有效性。&lt;h4&gt;方法&lt;/h4&gt;使用开源大型语言模型Llama 3.1-405B生成结构化数据使用案例标签，针对引用特定基因组数据集的出版物。同时引入创新评估框架验证方法效果。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在没有预先定义类别的零样本数据引用分类任务上达到0.674的F1分数，表明使用大型语言模型描述数据使用案例具有良好前景。&lt;h4&gt;结论&lt;/h4&gt;尽管结果很有希望，但研究受到数据可用性、提示过拟合、计算基础设施和负责任性能评估成本等因素的限制，这些挑战需要在实际应用中加以解决。&lt;h4&gt;翻译&lt;/h4&gt;近年来，确定特定数据集与包含这些数据集的科学文献之间关联的努力有所增加。已知某出版物引用了某数据集后，下一步逻辑就是探索该数据是如何或为何被使用的。近年来基于预训练、转换器的大型语言模型（LLMs）的进步，为扩展已发表文献中数据使用案例的描述提供了潜在手段。这避免了传统机器学习系统需要昂贵的手动标记和训练数据集开发。在本研究中，我们应用开源LLM Llama 3.1-405B，为已知包含特定基因组数据集的出版物生成结构化的数据使用案例标签。我们还引入了一种新的评估框架来确定我们方法的有效性。我们的结果表明，基础模型在没有预先定义类别的零样本数据引用分类任务上可以达到0.674的F1分数。虽然结果很有前景，但我们的结果受到与数据可用性、提示过拟合、计算基础设施和进行负责任的性能评估所需成本相关的限制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efforts have increased in recent years to identify associations betweenspecific datasets and the scientific literature that incorporates them. Knowingthat a given publication cites a given dataset, the next logical step is toexplore how or why that data was used. Advances in recent years withpretrained, transformer-based large language models (LLMs) offer potentialmeans for scaling the description of data use cases in the publishedliterature. This avoids expensive manual labeling and the development oftraining datasets for classical machine-learning (ML) systems. In this work weapply an open-source LLM, Llama 3.1-405B, to generate structured data use caselabels for publications known to incorporate specific genomic datasets. We alsointroduce a novel evaluation framework for determining the efficacy of ourmethods. Our results demonstrate that the stock model can achieve an F1 scoreof .674 on a zero-shot data citation classification task with no previouslydefined categories. While promising, our results are qualified by barriersrelated to data availability, prompt overfitting, computational infrastructure,and the expense required to conduct responsible performance evaluation.</description>
      <author>example@mail.com (Neil Byers, Ali Zaidi, Valerie Skye, Chris Beecroft, Kjiersten Fagnan)</author>
      <guid isPermaLink="false">2511.02936v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything</title>
      <link>http://arxiv.org/abs/2511.02834v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 7 figures, 14 tables. Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Agent-Omni框架通过主代理系统协调现有基础模型，实现灵活的多模态推理而无需重新训练，在多种模态和跨模态推理任务上取得最先进性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)目前只能处理固定模态对，需要大量对齐数据进行昂贵的微调，构建完全全能的模型(能整合文本、图像、音频和视频)仍然不切实际且缺乏强大的推理支持。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够灵活处理多种模态推理的框架，无需重新训练模型，并能整合现有基础模型的能力。&lt;h4&gt;方法&lt;/h4&gt;提出Agent-Omni框架，通过主代理系统协调现有基础模型，主代理解释用户意图，将子任务委托给特定模态的代理，并将它们的输出整合成连贯的响应。&lt;h4&gt;主要发现&lt;/h4&gt;Agent-Omni在文本、图像、音频、视频和全能基准测试中持续取得最先进的性能，特别是在需要复杂跨模态推理的任务上表现出色；基于代理的设计实现了专业基础模型的无缝集成，确保对不同输入的适应性，同时保持透明性和可解释性。&lt;h4&gt;结论&lt;/h4&gt;Agent-Omni框架是模块化和可扩展的，允许随着更强大模型的可用性进行未来改进，为多模态推理提供了灵活且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)已展现出强大的能力，但仍局限于固定的模态对，并且需要使用大型对齐数据集进行昂贵的微调。构建能够整合文本、图像、音频和视频的完全全能模型仍然不切实际，且缺乏强大的推理支持。在本文中，我们提出了一个Agent-Omni框架，通过主代理系统协调现有基础模型，实现灵活的多模态推理而无需重新训练。主代理解释用户意图，将子任务委托给特定模态的代理，并将它们的输出整合成连贯的响应。在文本、图像、音频、视频和全能基准测试中的大量实验表明，Agent-Omni持续取得最先进的性能，特别是在需要复杂跨模态推理的任务上。其基于代理的设计实现了专业基础模型的无缝集成，确保对不同输入的适应性，同时保持透明性和可解释性。此外，该框架是模块化和可扩展的，允许随着更强模型的可用性进行未来改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have shown strong capabilities butremain limited to fixed modality pairs and require costly fine-tuning withlarge aligned datasets. Building fully omni-capable models that can integratetext, images, audio, and video remains impractical and lacks robust reasoningsupport. In this paper, we propose an Agent-Omni framework that coordinatesexisting foundation models through a master-agent system, enabling flexiblemultimodal reasoning without retraining. The master agent interprets userintent, delegates subtasks to modality-specific agents, and integrates theiroutputs into coherent responses. Extensive experiments across text, image,audio, video, and omni benchmarks show that Agent-Omni consistently achievesstate-of-the-art performance, particularly on tasks requiring complexcross-modal reasoning. Its agent-based design enables seamless integration ofspecialized foundation models, ensuring adaptability to diverse inputs whilemaintaining transparency and interpretability. In addition, the framework ismodular and easily extensible, allowing future improvements as stronger modelsbecome available.</description>
      <author>example@mail.com (Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh)</author>
      <guid isPermaLink="false">2511.02834v2</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>PLUTO-4: Frontier Pathology Foundation Models</title>
      <link>http://arxiv.org/abs/2511.02826v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PLUTO-4是病理学基础模型的下一代版本，包含PLUTO-4S和PLUTO-4G两种互补架构，在多种病理学任务上实现了最先进的性能，具有实际应用潜力。&lt;h4&gt;背景&lt;/h4&gt;基于大规模病理图像语料库训练的基础模型已显示出在多样组织病理学任务中的强大迁移能力，PLUTO是病理学通用转换器模型。&lt;h4&gt;目的&lt;/h4&gt;介绍PLUTO-4，扩展PLUTO到前沿规模，并提供两种互补的视觉转换器架构。&lt;h4&gt;方法&lt;/h4&gt;PLUTO-4S使用FlexiViT设置进行多尺度部署，采用2D-RoPE嵌入；PLUTO-4G使用单一补丁大小训练；两者均使用基于DINOv2的自监督目标在包含551,164个WSI的多机构语料库上预训练；在公共和内部基准上评估性能。&lt;h4&gt;主要发现&lt;/h4&gt;PLUTO-4在补丁级分类、分割和幻灯片级诊断等任务上实现最先进性能；PLUTO-4S提供高吞吐量和稳健性能；PLUTO-4G在多个病理学基准上建立新性能前沿，皮肤病理学诊断提高11%。&lt;h4&gt;结论&lt;/h4&gt;PLUTO-4的多样化改进强调了其作为转化研究和诊断用例主干，改变现实世界应用的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在大型病理图像语料库上训练的基础模型已在多种组织病理学任务中展现出强大的迁移能力。在此基础上，我们介绍了PLUTO-4，这是我们的病理学基础模型下一代版本，将病理学通用转换器(PLUTO)扩展到前沿规模。我们在PLUTO-4家族中分享了两种互补的视觉转换器架构：紧凑高效的PLUTO-4S模型，使用FlexiViT设置进行多尺度部署，采用2D-RoPE嵌入；以及前沿规模的PLUTO-4G模型，使用单一补丁大小训练以最大化表示能力和稳定性。两种模型都使用从DINOv2衍生的自监督目标进行预训练，在包含来自137,144名患者的551,164个WSI的多机构语料库上训练，涵盖50多个机构、60多种疾病类型和100多种染色方法。在公共和内部基准上的全面评估表明，PLUTO-4在需要不同空间和生物学背景的任务上实现了最先进的性能，包括补丁级分类、分割和幻灯片级诊断。紧凑的PLUTO-4S为实际部署提供高吞吐量和稳健性能，而PLUTO-4G在多个病理学基准上建立了新的性能前沿，包括在皮肤病理学诊断上提高11%。这些多样化的改进强调了PLUTO-4作为转化研究和诊断用例主干，改变现实世界应用的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models trained on large-scale pathology image corpora havedemonstrated strong transfer capabilities across diverse histopathology tasks.Building on this progress, we introduce PLUTO-4, our next generation ofpathology foundation models that extend the Pathology-Universal Transformer(PLUTO) to frontier scale. We share two complementary Vision Transformerarchitectures in the PLUTO-4 family: a compact and efficient PLUTO-4S modeloptimized for multi-scale deployment using a FlexiViT setup with 2D-RoPEembeddings, and a frontier-scale PLUTO-4G model trained with a single patchsize to maximize representation capacity and stability. Both models arepretrained using a self-supervised objective derived from DINOv2 on a largemulti-institutional corpus containing 551,164 WSIs from 137,144 patients acrossover 50 institutions, spanning over 60 disease types and over 100 stains.Comprehensive evaluation across public and internal benchmarks demonstratesthat PLUTO-4 achieves state-of-the-art performance on tasks requiring varyingspatial and biological context, including patch-level classification,segmentation, and slide-level diagnosis. The compact PLUTO-4S provideshigh-throughput and robust performance for practical deployment, while PLUTO-4Gestablishes new performance frontiers across multiple pathology benchmarks,including an 11% improvement in dermatopathology diagnosis. These diverseimprovements underscore PLUTO-4's potential to transform real-worldapplications as a backbone for translational research and diagnostic use cases.</description>
      <author>example@mail.com (Harshith Padigela, Shima Nofallah, Atchuth Naveen Chilaparasetti, Ryun Han, Andrew Walker, Judy Shen, Chintan Shah, Blake Martin, Aashish Sood, Elliot Miller, Ben Glass, Andy Beck, Harsha Pokkalla, Syed Ashar Javed)</author>
      <guid isPermaLink="false">2511.02826v2</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models</title>
      <link>http://arxiv.org/abs/2511.02802v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The library is open source and available at  https://github.com/Lexsi-Labs/TabTune&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TabTune是一个统一库，旨在标准化表格基础模型的完整工作流程，通过单一接口提供一致的服务，解决了表格基础模型采用中的主要障碍，包括异构预处理管道、碎片化API、不一致微调程序和缺乏标准化评估等问题。&lt;h4&gt;背景&lt;/h4&gt;表格基础模型是结构化数据学习中的一个新兴范式，将大规模预训练的好处扩展到表格领域，但其采用仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的库来标准化表格基础模型的完整工作流程，通过单一接口提供一致的服务。&lt;h4&gt;方法&lt;/h4&gt;提出TabTune库，提供对七种最先进模型的一致访问，支持零样本推理、元学习、监督微调和参数高效微调等多种适应策略；框架自动化模型感知预处理，内部管理架构异构性，并集成性能、校准和公平性的评估模块。&lt;h4&gt;主要发现&lt;/h4&gt;表格基础模型的采用受到异构预处理管道、碎片化API、不一致微调程序和缺乏针对校准和公平性等部署导向指标的标准化评估的限制。&lt;h4&gt;结论&lt;/h4&gt;TabTune通过提供可扩展且可重现的框架，能够对表格基础模型的适应策略进行一致的基准测试，解决了表格基础模型采用中的主要障碍。&lt;h4&gt;翻译&lt;/h4&gt;表格基础模型代表了结构化数据学习中的一个不断增长的范式，将大规模预训练的好处扩展到表格领域。然而，由于异构预处理管道、碎片化API、不一致的微调程序以及缺乏针对校准和公平性等部署导向指标的标准化评估，它们的采用仍然有限。我们提出了TabTune，这是一个通过单一界面标准化表格基础模型完整工作流程的统一库。TabTune提供对七种最先进模型的一致访问，支持多种适应策略，包括零样本推理、元学习、监督微调(SFT)和参数高效微调(PEFT)。该框架自动化模型感知预处理，内部管理架构异构性，并集成性能、校准和公平性的评估模块。TabTune设计为可扩展和可重现，能够对表格基础模型的适应策略进行一致的基准测试。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular foundation models represent a growing paradigm in structured datalearning, extending the benefits of large-scale pretraining to tabular domains.However, their adoption remains limited due to heterogeneous preprocessingpipelines, fragmented APIs, inconsistent fine-tuning procedures, and theabsence of standardized evaluation for deployment-oriented metrics such ascalibration and fairness. We present TabTune, a unified library thatstandardizes the complete workflow for tabular foundation models through asingle interface. TabTune provides consistent access to seven state-of-the-artmodels supporting multiple adaptation strategies, including zero-shotinference, meta-learning, supervised fine-tuning (SFT), and parameter-efficientfine-tuning (PEFT). The framework automates model-aware preprocessing, managesarchitectural heterogeneity internally, and integrates evaluation modules forperformance, calibration, and fairness. Designed for extensibility andreproducibility, TabTune enables consistent benchmarking of adaptationstrategies of tabular foundation models.</description>
      <author>example@mail.com (Aditya Tanna, Pratinav Seth, Mohamed Bouadi, Utsav Avaiya, Vinay Kumar Sankarapu)</author>
      <guid isPermaLink="false">2511.02802v2</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning</title>
      <link>http://arxiv.org/abs/2511.03330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了OMRC-MR框架，一种结合问答式OMRC摘要、多级对比学习和结构感知重排序的层次化方法，用于科学论文推荐，显著提高了推荐的精确度和召回率。&lt;h4&gt;背景&lt;/h4&gt;开放获取出版物的快速增长使识别相关科学论文更具挑战性。由于隐私限制和用户交互数据有限访问，研究转向基于内容的推荐方法，但这些方法通常将论文视为非结构化文本，忽略了话语组织结构，限制了语义完整性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发能够捕捉论文结构化信息的内容推荐方法，提高科学论文推荐的精确度和召回率，同时增强推荐结果的可解释性。&lt;h4&gt;方法&lt;/h4&gt;OMRC-MR框架包含三个主要模块：问答式OMRC摘要模块将原始论文转换为结构化表示；多级对比学习目标在元数据、章节和文档级别对齐语义表示；结构感知重排序阶段通过上下文相似度校准优化检索精确度。&lt;h4&gt;主要发现&lt;/h4&gt;在DBLP、S2ORC和Sci-OMRC数据集上的实验表明，OMRC-MR在Precision@10和Recall@10上分别实现了高达7.2%和3.8%的改进。问答式摘要产生了更连贯和事实完整的表示。&lt;h4&gt;结论&lt;/h4&gt;OMRC-MR为科学论文推荐提供了一个统一且可解释的内容范式，推进了可信和隐私感知的学术信息检索。&lt;h4&gt;翻译&lt;/h4&gt;开放获取出版物的快速增长加剧了识别相关科学论文的挑战。由于隐私限制和用户交互数据的有限访问，近期努力转向了基于内容的推荐，这完全依赖于文本信息。然而，现有模型通常将论文视为非结构化文本，忽略了它们的话语组织，从而限制了语义完整性和可解释性。为了解决这些限制，我们提出了OMRC-MR，这是一个层次化框架，集成了问答式OMRC摘要、多级对比学习和结构感知重排序用于学术推荐。问答式摘要模块将原始论文转换为结构化和话语一致的表示，而多级对比目标在元数据、章节和文档级别对齐语义表示。最终的重排序阶段通过上下文相似度校准进一步优化检索精确度。在DBLP、S2ORC和新构建的Sci-OMRC数据集上的实验表明，OMRC-MR始终超越最先进的基线，在Precision@10和Recall@10上分别实现了高达7.2%和3.8%的改进。额外的评估确认问答式摘要产生了更连贯和事实完整的表示。总体而言，OMRC-MR为科学论文推荐提供了一个统一且可解释的内容范式，推进了可信和隐私感知的学术信息检索。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of open-access (OA) publications has intensified thechallenge of identifying relevant scientific papers. Due to privacy constraintsand limited access to user interaction data, recent efforts have shifted towardcontent-based recommendation, which relies solely on textual information.However, existing models typically treat papers as unstructured text,neglecting their discourse organization and thereby limiting semanticcompleteness and interpretability. To address these limitations, we proposeOMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective,Method, Result, Conclusion) summarization, multi-level contrastive learning,and structure-aware re-ranking for scholarly recommendation. The QA-stylesummarization module converts raw papers into structured anddiscourse-consistent representations, while multi-level contrastive objectivesalign semantic representations across metadata, section, and document levels.The final re-ranking stage further refines retrieval precision throughcontextual similarity calibration. Experiments on DBLP, S2ORC, and the newlyconstructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpassesstate-of-the-art baselines, achieving up to 7.2% and 3.8% improvements inPrecision@10 and Recall@10, respectively. Additional evaluations confirm thatQA-style summarization produces more coherent and factually completerepresentations. Overall, OMRC-MR provides a unified and interpretablecontent-based paradigm for scientific paper recommendation, advancingtrustworthy and privacy-aware scholarly information retrieval.</description>
      <author>example@mail.com (Shenghua Wang, Zhen Yin)</author>
      <guid isPermaLink="false">2511.03330v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>An Augmentation Overlap Theory of Contrastive Learning</title>
      <link>http://arxiv.org/abs/2511.03114v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了自监督对比学习的底层工作机制，提出了基于增强重叠的理论框架，并开发了与下游性能高度一致的无监督评估指标。&lt;h4&gt;背景&lt;/h4&gt;自监督对比学习在各种任务上取得了巨大成功，但其底层工作机制尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;探索对比学习的底层工作机制，特别是放松条件独立假设，研究更实际的增强重叠假设对下游性能的影响。&lt;h4&gt;方法&lt;/h4&gt;基于条件独立假设提供最紧的界限；将条件独立假设放松到增强重叠假设，推导下游性能的渐近闭合界限；提出增强重叠理论；开发用于对比学习表示评估的无监督指标。&lt;h4&gt;主要发现&lt;/h4&gt;不同类内样本的支撑集在激进数据增强下会变得更加重叠，简单对齐正样本可使对比学习将类内样本聚集在一起。&lt;h4&gt;结论&lt;/h4&gt;所提出的增强重叠理论解释了对比学习的机制，开发的无监督评估指标与下游性能高度一致，几乎不需要额外模块。&lt;h4&gt;翻译&lt;/h4&gt;最近，自监督对比学习在各种任务上取得了巨大成功。然而，其底层工作机制尚不清楚。在本文中，我们首先基于广泛采用的条件独立假设提供最紧的界限。进一步，我们将条件独立假设放松到更实际的增强重叠假设，并推导出下游性能的渐近闭合界限。我们提出的增强重叠理论基于这样的洞察：不同类内样本的支撑集在激进的数据增强下会变得更加重叠，因此简单地对齐正样本（同一样本的增强视图）可以使对比学习将类内样本聚集在一起。此外，从新推导的增强重叠角度，我们开发了一种用于对比学习表示评估的无监督指标，它与下游性能几乎完美一致，且几乎不需要依赖额外模块。代码可在https://github.com/PKU-ML/GARC获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, self-supervised contrastive learning has achieved great success onvarious tasks. However, its underlying working mechanism is yet unclear. Inthis paper, we first provide the tightest bounds based on the widely adoptedassumption of conditional independence. Further, we relax the conditionalindependence assumption to a more practical assumption of augmentation overlapand derive the asymptotically closed bounds for the downstream performance. Ourproposed augmentation overlap theory hinges on the insight that the support ofdifferent intra-class samples will become more overlapped under aggressive dataaugmentations, thus simply aligning the positive samples (augmented views ofthe same sample) could make contrastive learning cluster intra-class samplestogether. Moreover, from the newly derived augmentation overlap perspective, wedevelop an unsupervised metric for the representation evaluation of contrastivelearning, which aligns well with the downstream performance almost withoutrelying on additional modules. Code is available athttps://github.com/PKU-ML/GARC.</description>
      <author>example@mail.com (Qi Zhang, Yifei Wang, Yisen Wang)</author>
      <guid isPermaLink="false">2511.03114v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Stochastic Deep Graph Clustering for Practical Group Formation</title>
      <link>http://arxiv.org/abs/2511.02879v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DeepForm框架，解决群组推荐系统中的动态群组形成问题，满足高阶用户信息整合、实时群组形成和动态群组数量调整三个关键需求。&lt;h4&gt;背景&lt;/h4&gt;现有群组推荐系统研究主要关注提高推荐准确性，但多数方法假设群组静态或预定义，不适合动态、真实场景。&lt;h4&gt;目的&lt;/h4&gt;重新将群组形成视为群组推荐系统的核心挑战，提出满足三个关键操作要求的框架：整合高阶用户信息、实时群组形成和动态调整群组数量。&lt;h4&gt;方法&lt;/h4&gt;DeepForm采用轻量级GCN架构捕获高阶结构信号，通过随机聚类学习实现无需重新训练的自适应群组重新配置，利用对比学习在动态条件下优化群组。&lt;h4&gt;主要发现&lt;/h4&gt;多个数据集实验表明，DeepForm在群组形成质量、效率和推荐准确性方面均优于各种基线方法。&lt;h4&gt;结论&lt;/h4&gt;DeepForm为动态场景下的群组形成提供了有效解决方案，同时保持了高质量的推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;虽然先前关于群组推荐系统(GRSs)的研究主要集中在提高推荐准确性上，但大多数方法假设群组是静态或预定义的，使它们不适合动态、真实的场景。我们将群组形成重新定义为GRSs中的核心挑战，并提出了DeepForm（用于实际群组形成的随机深度图聚类），这是一个旨在满足三个关键操作要求的框架：(1)整合高阶用户信息，(2)实时群组形成，(3)动态调整群组数量。DeepForm采用轻量级GCN架构，有效捕获高阶结构信号。随机聚类学习使群组能够自适应重新配置而无需重新训练，同时对比学习在动态条件下优化群组。在多个数据集上的实验表明，与各种基线相比，DeepForm在群组形成质量、效率和推荐准确性方面都表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While prior work on group recommender systems (GRSs) has primarily focused onimproving recommendation accuracy, most approaches assume static or predefinedgroups, making them unsuitable for dynamic, real-world scenarios. We reframegroup formation as a core challenge in GRSs and propose DeepForm (StochasticDeep Graph Clustering for Practical Group Formation), a framework designed tomeet three key operational requirements: (1) the incorporation of high-orderuser information, (2) real-time group formation, and (3) dynamic adjustment ofthe number of groups. DeepForm employs a lightweight GCN architecture thateffectively captures high-order structural signals. Stochastic cluster learningenables adaptive group reconfiguration without retraining, while contrastivelearning refines groups under dynamic conditions. Experiments on multipledatasets demonstrate that DeepForm achieves superior group formation quality,efficiency, and recommendation accuracy compared with various baselines.</description>
      <author>example@mail.com (Junhyung Park, Hyungjin Kim, Seokho Ahn, Young-Duk Seo)</author>
      <guid isPermaLink="false">2511.02879v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic Graph Cuts</title>
      <link>http://arxiv.org/abs/2511.02272v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;作者提出了一种统一的概率框架，作为谱聚类的可微分替代方法，涵盖广泛的图切类型，包括Normalized Cut。该框架提供了紧密的解析上界，具有闭合形式的前向和反向传播，为可扩展的、可微分的图分割提供了严格且数值稳定的基础。&lt;h4&gt;背景&lt;/h4&gt;概率松弛的图切作为谱聚类的可微分替代方法，可以在不进行特征分解的情况下实现端到端和在线学习。然而，先前的工作主要集中在RatioCut上，缺乏通用保证和有原则的梯度。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的概率框架，涵盖广泛的图切类型，包括Normalized Cut，并提供紧密的解析上界和具有闭合形式的前向和反向传播。&lt;h4&gt;方法&lt;/h4&gt;构建统一的概率框架，通过积分表示和具有闭合形式前向和反向传播的高斯超几何函数，为期望离散切提供紧密的解析上界。&lt;h4&gt;主要发现&lt;/h4&gt;该框架提供了紧密的解析上界，具有闭合形式的前向和反向传播，使得算法在数值上更加稳定。&lt;h4&gt;结论&lt;/h4&gt;这些结果为可扩展的、可微分的图分割提供了严格且数值稳定的基础，涵盖了广泛的聚类和对比学习目标。&lt;h4&gt;翻译&lt;/h4&gt;概率松弛的图切为谱聚类提供了可微分的替代方案，无需特征分解即可实现端到端和在线学习，但先前的工作主要集中在RatioCut上，缺乏通用保证和有原则的梯度。我们提出了一个统一的概率框架，涵盖了广泛的图切类型，包括Normalized Cut。我们的框架通过积分表示和具有闭合形式前向和反向传播的高斯超几何函数，为期望离散切提供了紧密的解析上界。这些结果共同为可扩展的、可微分的图分割提供了严格且数值稳定的基础，涵盖了广泛的聚类和对比学习目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Probabilistic relaxations of graph cuts offer a differentiable alternative tospectral clustering, enabling end-to-end and online learning withouteigendecompositions, yet prior work centered on RatioCut and lacked generalguarantees and principled gradients. We present a unified probabilisticframework that covers a wide class of cuts, including Normalized Cut. Ourframework provides tight analytic upper bounds on expected discrete cuts viaintegral representations and Gauss hypergeometric functions with closed-formforward and backward. Together, these results deliver a rigorous, numericallystable foundation for scalable, differentiable graph partitioning covering awide range of clustering and contrastive learning objectives.</description>
      <author>example@mail.com (Ayoub Ghriss)</author>
      <guid isPermaLink="false">2511.02272v2</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing composition-based materials property prediction by cross-modal knowledge transfer</title>
      <link>http://arxiv.org/abs/2511.03371v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 2 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;晶体图神经网络在建模实验合成的化合物和具有未知合成可能性的假设材料方面具有广泛应用。相比之下，结构不可知预测算法可以探索化学空间中以前无法访问的领域。本研究提出了一种通过跨模态知识转移来增强基于成分的材料属性预测的通用方法。&lt;h4&gt;背景&lt;/h4&gt;晶体图神经网络广泛用于建模实验合成的化合物和未知合成可能性的假设材料。结构不可知预测算法则允许探索化学空间中以前无法访问的领域。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用方法，通过跨模态知识转移来增强基于成分的材料属性预测。&lt;h4&gt;方法&lt;/h4&gt;提出了两种公式：隐式转移涉及在多模态嵌入上预训练化学语言模型，而显式转移建议生成晶体结构并实现结构感知预测器。这些方法在LLM4Mat-Bench和MatBench任务上进行了基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法在32个案例中的25个案例中达到了最先进的性能。此外，展示了化学语言模型的另一个建模方面——可解释性——如何通过应用博弈论方法受益，该方法能够纳入高阶特征交互。&lt;h4&gt;结论&lt;/h4&gt;通过跨模态知识转移的方法可以显著提高基于成分的材料属性预测性能，化学语言模型的可解释性可以通过博弈论方法得到增强。&lt;h4&gt;翻译&lt;/h4&gt;晶体图神经网络在建模实验合成的化合物和具有未知合成可能性的假设材料方面具有广泛应用。相比之下，结构不可知预测算法可以探索化学空间中以前无法访问的领域。在此，我们提出了一种通过跨模态知识转移来增强基于成分的材料属性预测的通用方法。提出了两种公式：隐式转移涉及在多模态嵌入上预训练化学语言模型，而显式转移建议生成晶体结构并实现结构感知预测器。所提出的方法在LLM4Mat-Bench和MatBench任务上进行了基准测试，在32个案例中的25个案例中取得了最先进的性能。此外，我们展示了化学语言模型的另一个建模方面——可解释性——如何通过应用博弈论方法受益，该方法能够纳入高阶特征交互。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crystal graph neural networks are widely applicable in modelingexperimentally synthesized compounds and hypothetical materials with unknownsynthesizability. In contrast, structure-agnostic predictive algorithms allowexploring previously inaccessible domains of chemical space. Here we present auniversal approach for enhancing composition-based materials propertyprediction by means of cross-modal knowledge transfer. Two formulations areproposed: implicit transfer involves pretraining chemical language models onmultimodal embeddings, whereas explicit transfer suggests generating crystalstructures and implementing structure-aware predictors. The proposed approacheswere benchmarked on LLM4Mat-Bench and MatBench tasks, achievingstate-of-the-art performance in 25 out of 32 cases. In addition, wedemonstrated how another modeling aspect of chemical language models -interpretability - benefits from applying a game-theoretic approach, which isable to incorporate high-order feature interactions.</description>
      <author>example@mail.com (Ivan Rubtsov, Ivan Dudakov, Yuri Kuratov, Vadim Korolev)</author>
      <guid isPermaLink="false">2511.03371v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>GraphCliff: Short-Long Range Gating for Subtle Differences but Critical Changes</title>
      <link>http://arxiv.org/abs/2511.03170v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GraphCliff的新模型，通过门控机制整合短程和长程信息，解决了分子图嵌入在区分结构相似但功能不同分子时表现不佳的问题，从而提高了在活性悬崖和非悬崖化合物上的预测性能。&lt;h4&gt;背景&lt;/h4&gt;定量构效关系(QSAR)假设分子结构与生物活性之间存在平滑关系，但活性悬崖(结构相似但效力差异大的化合物对)会破坏这种连续性。最近的基准测试表明，具有扩展连接性指纹的经典机器学习模型在处理活性悬崖时优于图神经网络。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够保留分子图结构表示并有效区分结构相似但功能不同分子的模型，以提高在活性悬崖和非悬崖化合物上的预测性能。&lt;h4&gt;方法&lt;/h4&gt;提出名为GraphCliff的新模型，通过门控机制整合短程和长程信息，保留分子作为图的结构表示。&lt;h4&gt;主要发现&lt;/h4&gt;GraphCliff在非悬崖和悬崖化合物上都一致提高了性能；分层节点嵌入分析显示与强基线图模型相比，减少了过平滑并增强了判别能力。&lt;h4&gt;结论&lt;/h4&gt;GraphCliff成功解决了分子图嵌入在区分结构相似但功能不同分子时表现不佳的问题，同时保留了分子图结构的表达力，为处理活性悬崖问题提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;定量构效关系假设分子结构和生物活性之间存在平滑关系。然而，活性悬崖被定义为结构相似的化合物对，其效力差异很大，这破坏了这种连续性。最近针对活性悬崖的基准测试表明，具有扩展连接性指纹的经典机器学习模型优于图神经网络。我们的分析表明，图嵌入无法在嵌入空间中充分分离结构相似的分子，这使得难以区分结构相似但功能不同的分子。尽管存在这一限制，分子图结构本质上具有表达力且吸引人，因为它们保留了分子拓扑结构。为了保留分子作为图的结构表示，我们提出了一个新模型GraphCliff，它通过门控机制整合短程和长程信息。实验结果表明，GraphCliff在非悬崖和悬崖化合物上都一致提高了性能。此外，分层节点嵌入分析显示与强基线图模型相比，减少了过平滑并增强了判别能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantitative structure-activity relationship assumes a smooth relationshipbetween molecular structure and biological activity. However, activity cliffsdefined as pairs of structurally similar compounds with large potencydifferences break this continuity. Recent benchmarks targeting activity cliffshave revealed that classical machine learning models with extended connectivityfingerprints outperform graph neural networks. Our analysis shows that graphembeddings fail to adequately separate structurally similar molecules in theembedding space, making it difficult to distinguish between structurallysimilar but functionally different molecules. Despite this limitation,molecular graph structures are inherently expressive and attractive, as theypreserve molecular topology. To preserve the structural representation ofmolecules as graphs, we propose a new model, GraphCliff, which integratesshort- and long-range information through a gating mechanism. Experimentalresults demonstrate that GraphCliff consistently improves performance on bothnon-cliff and cliff compounds. Furthermore, layer-wise node embedding analysesreveal reduced over-smoothing and enhanced discriminative power relative tostrong baseline graph models.</description>
      <author>example@mail.com (Hajung Kim, Jueon Park, Junseok Choe, Sheunheun Baek, Hyeon Hwang, Jaewoo Kang)</author>
      <guid isPermaLink="false">2511.03170v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Homomorphism distortion: A metric to distinguish them all and in the latent space bind them</title>
      <link>http://arxiv.org/abs/2511.03068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图相似性度量方法——图同态失真(graph homomorphism distortion)，它能够完全表征图，是一种完整的图嵌入。通过采样方法可以有效计算这一度量，并从中获得一个度量标准。实证表明，该方法能区分传统方法无法区分的图，并在特定数据集上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;长期以来，图神经网络的表达能力仅通过组合性质来衡量，缺乏对图相似性的原则性测量方法。&lt;h4&gt;目的&lt;/h4&gt;提供一种测量顶点属性图相似性的原则性方法，并开发一种新的图相似性度量。&lt;h4&gt;方法&lt;/h4&gt;引入图同态失真(graph homomorphism distortion)作为相似性度量，证明它可以完全表征图，因此也是一种完整的图嵌入。为解决图规范化问题，通过采样方法有效计算这一度量，并从中获得一个度量标准。&lt;h4&gt;主要发现&lt;/h4&gt;图同态失真能够：(1)完全区分BREC数据集中的图，包括那些通过4-WL无法区分的图；(2)在ZINC-12k数据集上，优于之前受同态启发的方法。&lt;h4&gt;结论&lt;/h4&gt;这些理论和实证结果为未来图的表征铺平了道路，将图论传统扩展到新的前沿领域。&lt;h4&gt;翻译&lt;/h4&gt;长期以来，图神经网络的表达能力仅通过组合性质来衡量。本文打破了这一传统，提供了一种测量顶点属性图相似性的原则性方法，我们将其称为图同态失真(graph homomorphism distortion)。我们证明它可以完全表征图，因此也是一种完整的图嵌入。然而，在研究过程中，我们遇到了图规范化问题。为克服这一障碍，我们设计了通过采样来有效计算这一度量的方法，期望上保证了完整性。此外，我们还发现可以从这一度量中获得一个度量标准。我们通过实证验证了我们的主张，发现图同态失真：(1)能完全区分BREC数据集中的图，包括那些通过4-WL无法区分的图；(2)在ZINC-12k数据集上，优于之前受同态启发的方法。这些理论结果（及其实证验证）为未来图的表征铺平了道路，将图论传统扩展到新的前沿领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For far too long, expressivity of graph neural networks has been measured\emph{only} in terms of combinatorial properties. In this work we stray awayfrom this tradition and provide a principled way to measure similarity betweenvertex attributed graphs. We denote this measure as the \emph{graphhomomorphism distortion}. We show it can \emph{completely characterize} graphsand thus is also a \emph{complete graph embedding}. However, somewhere alongthe road, we run into the graph canonization problem. To circumvent thisobstacle, we devise to efficiently compute this measure via sampling, which inexpectation ensures \emph{completeness}. Additionally, we also discovered thatwe can obtain a metric from this measure. We validate our claims empiricallyand find that the \emph{graph homomorphism distortion}: (1.) fullydistinguishes the \texttt{BREC} dataset with up to $4$-WL non-distinguishablegraphs, and (2.) \emph{outperforms} previous methods inspired in homomorphismsunder the \texttt{ZINC-12k} dataset.  These theoretical results, (and their empirical validation), pave the way forfuture characterization of graphs, extending the graph theoretic tradition tonew frontiers.</description>
      <author>example@mail.com (Martin Carrasco, Olga Zaghen, Erik Bekkers, Bastian Rieck)</author>
      <guid isPermaLink="false">2511.03068v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2511.02957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合数字孪生和图神经网络的创新方法，用于路面基础设施的智能监测和维护，解决了传统路面管理系统被动响应的问题，实现了主动维护和预测性规划。&lt;h4&gt;背景&lt;/h4&gt;路面基础设施监测面临复杂的空间依赖性、变化的环境条件和道路网络上的非线性退化等挑战。传统的路面管理系统(PMS)主要是被动的，缺乏故障预防和最优维护规划的实时智能。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的数字孪生(DT)和图神经网络(GNN)框架，用于可扩展、数据驱动的路面健康监测和预测性维护。&lt;h4&gt;方法&lt;/h4&gt;将路段和空间关系建模为图的节点和边，实时无人机、传感器和LiDAR数据流入数字孪生系统。归纳式GNN从图结构输入中学习退化模式以预测损坏。开发了交互式仪表板和强化学习模块用于模拟、可视化和自适应维护规划。&lt;h4&gt;主要发现&lt;/h4&gt;模型实现了0.3798的R²值，优于基线回归器，并有效捕获了非线性退化模式。&lt;h4&gt;结论&lt;/h4&gt;DT-GNN集成提高了预测精度并建立了持续改进的闭环反馈系统，为主动、智能和可持续的路面管理奠定了基础，未来将向实际部署、多智能体协调和智慧城市集成扩展。&lt;h4&gt;翻译&lt;/h4&gt;路面基础设施监测面临复杂的空间依赖性、变化的环境条件和道路网络上的非线性退化等挑战。传统的路面管理系统(PMS)主要是被动的，缺乏故障预防和最优维护规划的实时智能。为解决这一问题，我们提出了一个统一的数字孪生(DT)和图神经网络(GNN)框架，用于可扩展、数据驱动的路面健康监测和预测性维护。路段和空间关系被建模为图的节点和边，而实时无人机、传感器和LiDAR数据流入数字孪生系统。归纳式GNN从图结构输入中学习退化模式，以预测损坏并实现主动干预。在具有路段属性和动态连接的真实世界启发的数据集上训练，我们的模型实现了0.3798的R²值，优于基线回归器，并有效捕获了非线性退化。我们还开发了交互式仪表板和强化学习模块，用于模拟、可视化和自适应维护规划。这种DT-GNN集成提高了预测精度，并建立了持续改进的闭环反馈系统，将该方法定位为主动、智能和可持续路面管理的基础，未来将向实际部署、多智能体协调和智慧城市集成扩展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决传统路面管理系统(PMS)的被动维护问题，这些系统只在路面出现故障时才进行干预，缺乏实时智能来预防故障和优化维护计划。这个问题很重要，因为路面是现代交通系统的支柱，恶化路面会导致旅行时间延长、燃料消耗增加、车辆运营成本提高和交通事故风险增加。传统固定时间表的维护方法可能与实际退化轨迹不匹配，造成过早的结构性故障或过高的维护成本。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统路面管理系统的局限性，然后注意到数字孪生(DT)技术可以提供物理资产的详细实时虚拟表示，图神经网络(GNN)能够处理复杂的时空数据。作者借鉴了DT技术在多个领域的应用经验，以及GNN在分析相互连接系统方面的能力，将两者结合创建了一个统一框架。他们整合了现有的数据采集技术，如实时UAV、传感器和LiDAR数据，并利用了已有的路面监测和预测方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将数字孪生(DT)和图神经网络(GNN)集成到一个统一框架中，使用图结构表示路面段和空间关系，通过实时数据更新DT，并利用GNN学习退化模式。整体实现流程包括：1)数据合成与集成层，收集和预处理多源数据；2)图构建模块，将数据转换为动态图表示；3)模拟和分析引擎，使用有限元建模、无人机和LiDAR评估以及GNN预测分析；4)交互式维护和可视化系统，提供决策支持和可视化界面。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)集成数字孪生框架实现实时动态监测；2)开发基于GNN的预测模型捕获路面段间的时空依赖关系；3)DT-GNN集成支持交互式模拟和假设场景分析；4)全面的比较评估证明系统性能优于传统方法。相比之前工作，不同之处在于：之前研究主要关注孤立的DT应用或独立的GNN模型，而本文将两者统一集成，实现了从被动到主动维护的范式转变，建立了闭环反馈系统，提高了预测精度并促进了连续监测。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过集成数字孪生和图神经网络技术，建立了一个实时、数据驱动的路面健康监测和维护优化框架，实现了从被动到主动维护的范式转变，提高了预测准确性并延长了路面使用寿命。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pavement infrastructure monitoring is challenged by complex spatialdependencies, changing environmental conditions, and non-linear deteriorationacross road networks. Traditional Pavement Management Systems (PMS) remainlargely reactive, lacking real-time intelligence for failure prevention andoptimal maintenance planning. To address this, we propose a unified DigitalTwin (DT) and Graph Neural Network (GNN) framework for scalable, data-drivenpavement health monitoring and predictive maintenance. Pavement segments andspatial relations are modeled as graph nodes and edges, while real-time UAV,sensor, and LiDAR data stream into the DT. The inductive GNN learnsdeterioration patterns from graph-structured inputs to forecast distress andenable proactive interventions. Trained on a real-world-inspired dataset withsegment attributes and dynamic connectivity, our model achieves an R2 of0.3798, outperforming baseline regressors and effectively capturing non-lineardegradation. We also develop an interactive dashboard and reinforcementlearning module for simulation, visualization, and adaptive maintenanceplanning. This DT-GNN integration enhances forecasting precision andestablishes a closed feedback loop for continuous improvement, positioning theapproach as a foundation for proactive, intelligent, and sustainable pavementmanagement, with future extensions toward real-world deployment, multi-agentcoordination, and smart-city integration.</description>
      <author>example@mail.com (Mohsin Mahmud Topu, Mahfuz Ahmed Anik, Azmine Toushik Wasi, Md Manjurul Ahsan)</author>
      <guid isPermaLink="false">2511.02957v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models</title>
      <link>http://arxiv.org/abs/2510.27629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 Pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个名为eval的框架，用于评估旨在减少生物基础模型双重使用能力的程序的鲁棒性。研究发现当前的数据过滤方法可能不太有效，被排除的知识可以通过微调恢复，且双重使用信号可能已经存在于预训练表示中。&lt;h4&gt;背景&lt;/h4&gt;开放权重生物基础模型呈现双重使用困境。它们虽然有望加速科学研究和药物开发，但也可能被恶意行为者用于开发更致命的生物武器。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为eval的框架，用于评估旨在减少生物基础模型双重使用能力的程序的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;eval通过三种视角评估模型对病毒的理解：序列建模、突变效应预测和毒力预测。&lt;h4&gt;主要发现&lt;/h4&gt;当前过滤实践可能不太有效；在某些情况下，被排除的知识可以通过微调快速恢复；在序列建模中表现出更广泛的泛化能力；双重使用信号可能已经存在于预训练表示中，可以通过简单的线性探测来提取。&lt;h4&gt;结论&lt;/h4&gt;这些发现强调了数据过滤作为独立程序的挑战，突显了对开放权重生物基础模型进行稳健安全和安全策略进一步研究的必要性。&lt;h4&gt;翻译&lt;/h4&gt;开放权重生物基础模型呈现双重使用困境。虽然它们在加速科学研究和药物开发方面具有巨大潜力，但也可能被恶意行为者用于开发更致命的生物武器。为了减轻这些模型带来的风险，当前的方法集中在预训练期间过滤生物危害数据。然而，这种方法的有效性仍然不清楚，特别是针对那些可能对这些模型进行微调以进行恶意使用的坚定行为者。为了解决这一差距，我们提出了eval，一个用于评估旨在减少生物基础模型双重使用能力的程序的鲁棒性的框架。eval通过三种视角评估模型对病毒的理解，包括序列建模、突变效应预测和毒力预测。我们的结果表明，当前的过滤实践可能不是特别有效的：在某些情况下，被排除的知识可以通过微调快速恢复，并且在序列建模中表现出更广泛的泛化能力。此外，双重使用信号可能已经存在于预训练表示中，可以通过简单的线性探测来提取。这些发现强调了数据过滤作为独立程序的挑战，突显了对开放权重生物基础模型进行稳健安全和安全策略进一步研究的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-weight bio-foundation models present a dual-use dilemma. While holdinggreat promise for accelerating scientific research and drug development, theycould also enable bad actors to develop more deadly bioweapons. To mitigate therisk posed by these models, current approaches focus on filtering biohazardousdata during pre-training. However, the effectiveness of such an approachremains unclear, particularly against determined actors who might fine-tunethese models for malicious use. To address this gap, we propose \eval, aframework to evaluate the robustness of procedures that are intended to reducethe dual-use capabilities of bio-foundation models. \eval assesses models'virus understanding through three lenses, including sequence modeling,mutational effects prediction, and virulence prediction. Our results show thatcurrent filtering practices may not be particularly effective: Excludedknowledge can be rapidly recovered in some cases via fine-tuning, and exhibitsbroader generalizability in sequence modeling. Furthermore, dual-use signalsmay already reside in the pretrained representations, and can be elicited viasimple linear probing. These findings highlight the challenges of datafiltering as a standalone procedure, underscoring the need for further researchinto robust safety and security strategies for open-weight bio-foundationmodels.</description>
      <author>example@mail.com (Boyi Wei, Zora Che, Nathaniel Li, Udari Madhushani Sehwag, Jasper Götting, Samira Nedungadi, Julian Michael, Summer Yue, Dan Hendrycks, Peter Henderson, Zifan Wang, Seth Donoughe, Mantas Mazeika)</author>
      <guid isPermaLink="false">2510.27629v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
  <item>
      <title>Dynamic Reflections: Probing Video Representations with Text Alignment</title>
      <link>http://arxiv.org/abs/2511.02767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究进行了首次全面的视频-文本表征对齐研究，探索现代视频和语言编码器的能力，并提出了参数化的测试时缩放定律。&lt;h4&gt;背景&lt;/h4&gt;多模态表征对齐已被证明可以提供不同编码器在跨数据类型结构相似性和下游能力方面的见解。虽然图像与文本的对齐已取得显著进展，但视频数据的时序特性在这一背景下尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;进行首次全面的视频-文本表征对齐研究，探究现代视频和语言编码器的性能和能力。&lt;h4&gt;方法&lt;/h4&gt;研究视频-文本表征对齐，探究不同视觉和文本数据丰富度对跨模态对齐的影响，分析语义对齐与下游任务性能的关联，以及时序推理与跨模态对齐的关系。&lt;h4&gt;主要发现&lt;/h4&gt;1) 跨模态对齐高度依赖于测试时提供的视觉和文本数据的丰富程度；2) 提出的参数化测试时缩放定律具有显著的预测能力；3) 与文本编码器的强对齐可能与通用视频表征和理解相关；4) 时序推理与跨模态对齐的关联为视觉和语言模型提供了挑战性测试平台。&lt;h4&gt;结论&lt;/h4&gt;将视频-文本对齐引入为一种信息丰富的零样本方法，用于探测不同编码器在时空数据上的表征能力。&lt;h4&gt;翻译&lt;/h4&gt;不同模态表征的对齐最近已被证明能够提供关于不同编码器在跨数据类型结构相似性和下游能力方面的见解。虽然在对齐图像与文本方面已取得重大进展，但视频数据的时序特性在此背景下仍 largely 未经探索。在本工作中，我们进行了首次全面的视频-文本表征对齐研究，探究现代视频和语言编码器的能力。我们的发现揭示了几个关键见解。首先，我们证明跨模态对齐高度依赖于测试时提供的视觉（静态图像与多帧视频）和文本（单一标题与集合）数据的丰富程度，特别是使用最先进的视频编码器时。我们提出了捕捉这种行为的参数化测试时缩放定律，并显示出与经验观察相比显著的预测能力。其次，我们研究了语义对齐与语义和非语义下游任务性能之间的相关性，提供了初步证据，表明与文本编码器的强对齐可能与通用视频表征和理解相关。最后，我们将时序推理与跨模态对齐相关联，为视觉和语言模型提供了具有挑战性的测试平台。总体而言，我们的工作将视频-文本对齐引入为一种信息丰富的零样本方法，用于探测不同编码器在时空数据上的表征能力。项目页面可在 https://video-prh.github.io/ 找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The alignment of representations from different modalities has recently beenshown to provide insights on the structural similarities and downstreamcapabilities of different encoders across diverse data types. While significantprogress has been made in aligning images with text, the temporal nature ofvideo data remains largely unexplored in this context. In this work, we conductthe first comprehensive study of video-text representation alignment, probingthe capabilities of modern video and language encoders. Our findings revealseveral key insights. First, we demonstrate that cross-modal alignment highlydepends on the richness of both visual (static images vs. multi-frame videos)and text (single caption vs. a collection) data provided at test time,especially when using state-of-the-art video encoders. We propose parametrictest-time scaling laws that capture this behavior and show remarkablepredictive power against empirical observations. Secondly, we investigate thecorrelation between semantic alignment and performance on both semantic andnon-semantic downstream tasks, providing initial evidence that strong alignmentagainst text encoders may be linked to general-purpose video representation andunderstanding. Finally, we correlate temporal reasoning with cross-modalalignment providing a challenging test-bed for vision and language models.Overall, our work introduces video-text alignment as an informative zero-shotway to probe the representation power of different encoders for spatio-temporaldata. Project page can be found at https://video-prh.github.io/</description>
      <author>example@mail.com (Tyler Zhu, Tengda Han, Leonidas Guibas, Viorica Pătrăucean, Maks Ovsjanikov)</author>
      <guid isPermaLink="false">2511.02767v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics</title>
      <link>http://arxiv.org/abs/2511.02427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 6 figures, 1 table; accepted for AI-2025 Forty-fifth SGAI  International Conference on Artificial Intelligence CAMBRIDGE, ENGLAND 16-18  DECEMBER 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究调查了先进的视觉语言模型(VLMs)在场景解释和动作识别任务上的能力，特别关注适合在移动机器人背景下部署到边缘设备的小型VLMs。&lt;h4&gt;背景&lt;/h4&gt;视频理解、场景解释和常识推理是使智能体能够解释视觉信息、感知环境、互动并做出决策的关键任务。大型语言模型和视觉语言模型在这些领域取得了显著进展，支持特定领域应用和零样本开放词汇任务，但计算复杂性限制了它们在边缘设备和移动机器人中的应用。&lt;h4&gt;目的&lt;/h4&gt;调查最先进的VLMs在场景解释和动作识别任务上的能力，特别关注能在移动机器人背景下部署到边缘设备的小型VLMs。&lt;h4&gt;方法&lt;/h4&gt;提出一个评估管道，并在多样化数据集上进行测试，包括各种真实世界城市景观、校园和室内场景。&lt;h4&gt;主要发现&lt;/h4&gt;讨论了小型模型在边缘设备上的潜力，特别强调了挑战、弱点、固有模型偏差以及获取信息的应用。&lt;h4&gt;结论&lt;/h4&gt;小型VLMs在边缘设备上部署具有潜力，但仍面临计算效率、准确性和推理时间之间的权衡挑战。&lt;h4&gt;翻译&lt;/h4&gt;视频理解、场景解释和常识推理是非常具有挑战性的任务，它们使智能体能够解释视觉信息，允许智能体在其环境中感知、互动并做出理性决策。大型语言模型和视觉语言模型近年来在这些领域显示出显著的进步，使特定领域的应用以及零样本开放词汇任务成为可能，并能够结合多个领域。然而，所需的计算复杂性对它们在边缘设备和移动机器人环境中的应用提出了挑战，特别是在考虑准确性和推理时间之间的权衡时。在本文中，我们调查了最先进的VLMs在场景解释和动作识别任务上的能力，特别关注能够在移动机器人背景下部署到边缘设备的小型VLMs。所提出的管道在各种包含不同真实世界城市景观、校园和室内场景的多样化数据集上进行了评估。实验评估讨论了这些小型模型在边缘设备上的潜力，特别强调了挑战、弱点、固有模型偏差以及获取信息的应用。补充材料可通过以下存储库获取：https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何在边缘设备上部署小型视觉语言模型(VLMs)进行零样本场景解释的问题，特别是在移动机器人应用中。这个问题很重要，因为移动机器人需要在动态环境中自主运行，而视觉常识推理对机器人理解环境和做出决策至关重要。边缘设备上的本地解决方案对于无法保证外部服务可用性的场景特别重要，同时零样本能力允许开放域使用，不受限于预定义的动作集合，更接近真实世界场景的复杂性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了视觉常识推理的不同方法，特别是利用大型语言模型(LLMs)和视觉语言模型(VLMs)的优势。他们研究了各种模型，特别关注适用于边缘设备的小型模型(sVLMs)。作者借鉴了现有工作如ViCor(结合LLMs和VLMs的优势)、VLMaps(结合视觉语言特征与3D重建)等，设计了一个混合架构，结合本地边缘设备处理和云支持的优势，以平衡计算能力、隐私保护和实时性需求。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在边缘设备上使用小型VLM进行场景解释，同时保持隐私并利用零样本能力处理开放词汇场景。整体实现流程是：1)边缘设备上的小型VLM生成最近时间间隔内图像序列的文本描述；2)生成的描述被分解为名词，用于提示零样本分割和跟踪；3)使用Grounded DINO和SAM进行零样本目标检测和分割；4)生成的描述可用于各种下游任务，如与本地或云端LLMs进行进一步推理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)专注于在边缘设备上部署小型VLMs进行零样本场景解释；2)提出结合本地边缘设备和云支持的混合架构；3)在多样化的真实世界数据集上评估小型VLMs的能力；4)研究不同场景域(校园室内、校园室外和城市)的性能差异；5)提出语义引导的分割方法，专注于描述中重要的元素。相比之前的工作，这种方法不依赖外部服务器进行所有处理，保护隐私；使用小型模型而非大型模型，更适合边缘计算环境；关注真实世界场景而非受控环境；提供了在移动机器人背景下应用VLMs的全面评估。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文评估并展示了小型视觉语言模型在边缘设备上进行零样本场景解释的可行性，为移动机器人在真实世界环境中提供了一种隐私保护的本地解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Understanding, Scene Interpretation and Commonsense Reasoning arehighly challenging tasks enabling the interpretation of visual information,allowing agents to perceive, interact with and make rational decisions in itsenvironment. Large Language Models (LLMs) and Visual Language Models (VLMs)have shown remarkable advancements in these areas in recent years, enablingdomain-specific applications as well as zero-shot open vocabulary tasks,combining multiple domains. However, the required computational complexityposes challenges for their application on edge devices and in the context ofMobile Robotics, especially considering the trade-off between accuracy andinference time. In this paper, we investigate the capabilities ofstate-of-the-art VLMs for the task of Scene Interpretation and ActionRecognition, with special regard to small VLMs capable of being deployed toedge devices in the context of Mobile Robotics. The proposed pipeline isevaluated on a diverse dataset consisting of various real-world cityscape,on-campus and indoor scenarios. The experimental evaluation discusses thepotential of these small models on edge devices, with particular emphasis onchallenges, weaknesses, inherent model biases and the application of the gainedinformation. Supplementary material is provided via the following repository:https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/</description>
      <author>example@mail.com (Nicolas Schuler, Lea Dewald, Nick Baldig, Jürgen Graf)</author>
      <guid isPermaLink="false">2511.02427v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>M3PD Dataset: Dual-view Photoplethysmography (PPG) Using Front-and-rear Cameras of Smartphones in Lab and Clinical Settings</title>
      <link>http://arxiv.org/abs/2511.02349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于智能手机的双视图光电容积描记术方法，通过融合面部和指尖视频数据，提高了心率监测的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;便携式生理监测对心血管疾病的早期检测和管理至关重要，但当前方法通常需要专业设备限制了可及性，或者要求患者保持不切实际的姿势。基于智能手机的视频光电容积描记术虽提供了便捷的无创替代方案，但仍面临运动伪影、光照变化和单视图限制等可靠性挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，引入首个公开可用的双视图移动光电容积描记术数据集，并提出一种融合双视图数据的新方法，提高心血管患者心率监测的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;构建M3PD数据集，包含60名参与者（47名心血管患者）通过前置和后置智能手机摄像头同时采集的面部和指尖同步视频。基于此双视图设置，提出F3Mamba模型，通过基于Mamba的时间建模融合面部和指尖视图。&lt;h4&gt;主要发现&lt;/h4&gt;F3Mamba模型将心率误差比现有单视图基线降低了21.9%至30.2%，同时在具有挑战性的现实场景中提高了监测的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;双视图移动光电容积描记术结合先进的融合算法可有效提高心血管患者心率监测的准确性和可靠性，为便携式心血管健康监测提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;便携式生理监测对心血管疾病的早期检测和管理至关重要，但当前方法通常需要专业设备限制了可及性，或者要求患者保持不切实际的姿势。基于智能手机的视频光电容积描记术提供了便捷的无创替代方案，但仍面临由运动伪影、光照变化和单视图限制引起的可靠性挑战。很少有研究证明其在心血管患者中的可靠应用，且缺乏广泛使用的开放数据集用于跨设备准确性评估。为解决这些限制，我们引入了M3PD数据集，这是首个公开可用的双视图移动光电容积描记术数据集，包含通过前置和后置智能手机摄像头同时采集的60名参与者（包括47名心血管患者）的面部和指尖同步视频。基于这种双视图设置，我们进一步提出F3Mamba，通过基于Mamba的时间建模融合面部和指尖视图。该模型将心率误差比现有单视图基线降低了21.9%至30.2%，同时在具有挑战性的现实场景中提高了鲁棒性。数据和代码：https://github.com/Health-HCI-Group/F3Mamba。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Portable physiological monitoring is essential for early detection andmanagement of cardiovascular disease, but current methods often requirespecialized equipment that limits accessibility or impose impractical posturesthat patients cannot maintain. Video-based photoplethysmography on smartphonesoffers a convenient noninvasive alternative, yet it still faces reliabilitychallenges caused by motion artifacts, lighting variations, and single-viewconstraints. Few studies have demonstrated reliable application tocardiovascular patients, and no widely used open datasets exist forcross-device accuracy. To address these limitations, we introduce the M3PDdataset, the first publicly available dual-view mobile photoplethysmographydataset, comprising synchronized facial and fingertip videos capturedsimultaneously via front and rear smartphone cameras from 60 participants(including 47 cardiovascular patients). Building on this dual-view setting, wefurther propose F3Mamba, which fuses the facial and fingertip views throughMamba-based temporal modeling. The model reduces heart-rate error by 21.9 to30.2 percent over existing single-view baselines while improving robustness inchallenging real-world scenarios. Data and code:https://github.com/Health-HCI-Group/F3Mamba.</description>
      <author>example@mail.com (Jiankai Tang, Tao Zhang, Jia Li, Yiru Zhang, Mingyu Zhang, Kegang Wang, Yuming Hao, Bolin Wang, Haiyang Li, Xingyao Wang, Yuanchun Shi, Yuntao Wang, Sichong Qian)</author>
      <guid isPermaLink="false">2511.02349v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>LiCoMemory: Lightweight and Cognitive Agentic Memory for Efficient Long-Term Reasoning</title>
      <link>http://arxiv.org/abs/2511.01448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LiCoMemory是一种端到端代理记忆框架，通过引入CogniGraph轻量级分层图解决大型语言模型记忆限制问题，在长期对话任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLM)代理具有出色的对话和推理能力，但受限于上下文窗口小和缺乏持久性记忆。&lt;h4&gt;目的&lt;/h4&gt;解决现有外部记忆架构中扁平、纠缠结构导致的冗余表示、非结构化检索以及效率和准确性下降问题。&lt;h4&gt;方法&lt;/h4&gt;提出LiCoMemory框架，引入CogniGraph轻量级分层图，利用实体和关系作为语义索引层，采用时间和层次感知搜索与集成重排序进行自适应知识检索。&lt;h4&gt;主要发现&lt;/h4&gt;在LoCoMo和LongMemEval基准测试上，LiCoMemory在时间推理、多会话一致性和检索效率方面优于基线模型，并显著降低了更新延迟。&lt;h4&gt;结论&lt;/h4&gt;LiCoMemory有效解决了大型语言模型的记忆限制问题，提高了长期对话任务中的性能和效率。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLM)代理展现出卓越的对话和推理能力，但仍然受限于有限的上下文窗口和持久性记忆的缺乏。最近的工作通过外部记忆架构解决这些限制，通常采用基于图的表示，但大多数采用扁平、纠缠的结构，将语义与拓扑交织在一起，导致冗余表示、非结构化检索以及效率和准确性的下降。为解决这些问题，我们提出了LiCoMemory，一个用于实时更新和检索的端到端代理记忆框架，它引入了CogniGraph，一种利用实体和关系作为语义索引层的轻量级分层图，并采用时间和层次感知搜索与集成重排序进行自适应和连贯的知识检索。在长期对话基准LoCoMo和LongMemEval上的实验表明，LiCoMemory不仅在时间推理、多会话一致性和检索效率方面优于既定的基线，而且显著降低了更新延迟。我们的官方代码和数据可在https://github.com/EverM0re/LiCoMemory获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Model (LLM) agents exhibit remarkable conversational andreasoning capabilities but remain constrained by limited context windows andthe lack of persistent memory. Recent efforts address these limitations viaexternal memory architectures, often employing graph-based representations, yetmost adopt flat, entangled structures that intertwine semantics with topology,leading to redundant representations, unstructured retrieval, and degradedefficiency and accuracy. To resolve these issues, we propose LiCoMemory, anend-to-end agentic memory framework for real-time updating and retrieval, whichintroduces CogniGraph, a lightweight hierarchical graph that utilizes entitiesand relations as semantic indexing layers, and employs temporal andhierarchy-aware search with integrated reranking for adaptive and coherentknowledge retrieval. Experiments on long-term dialogue benchmarks, LoCoMo andLongMemEval, show that LiCoMemory not only outperforms established baselines intemporal reasoning, multi-session consistency, and retrieval efficiency, butalso notably reduces update latency. Our official code and data are availableat https://github.com/EverM0re/LiCoMemory.</description>
      <author>example@mail.com (Zhengjun Huang, Zhoujin Tian, Qintian Guo, Fangyuan Zhang, Yingli Zhou, Di Jiang, Xiaofang Zhou)</author>
      <guid isPermaLink="false">2511.01448v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>DeepSpecs: Expert-Level Questions Answering in 5G</title>
      <link>http://arxiv.org/abs/2511.01305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeepSpecs是一个通过结构化和时间推理增强的RAG系统，通过三个元数据库（SpecDB、ChangeDB和TDocDB）解决5G标准文档中的交叉引用和演变问题，显著提高了回答5G规范专业级问题的能力。&lt;h4&gt;背景&lt;/h4&gt;5G技术为数十亿用户提供移动互联网接入，回答关于5G规范的专业级问题需要浏览数千页交叉引用的标准文档。现有的检索增强生成(RAG)框架依赖语义相似性，无法可靠地解决交叉引用或对规范演变进行推理。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理5G标准文档复杂性的系统，解决现有RAG框架在处理交叉引用和规范演变方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出DeepSpecs系统，使用三个元数据库：SpecDB（条款对齐的规范文本）、ChangeDB（行级版本差异）和TDocDB（标准化会议文档）。通过元数据查找递归检索引用条款解决交叉引用，通过挖掘变化并链接到变更请求来跟踪规范演变。整理了两个5G问答数据集：573条专家注释的真实世界问题和350条演变问题。&lt;h4&gt;主要发现&lt;/h4&gt;DeepSpecs在多个LLM后端上优于基础模型和最先进的电信RAG系统。消融研究证实明确的交叉引用解决和演变感知检索显著提高了答案质量，强调了建模5G标准的结构和时间特性的价值。&lt;h4&gt;结论&lt;/h4&gt;DeepSpecs通过结构化和时间推理有效解决了5G标准文档的复杂性，显著提高了回答关于5G规范的专业级问题的能力。&lt;h4&gt;翻译&lt;/h4&gt;5G技术为数十亿用户提供了移动互联网接入。回答关于5G规范的专业级问题需要浏览数千页交叉引用的标准文档，这些标准在不同版本中不断演变。现有的检索增强生成(RAG)框架，包括电信特定方法，依赖语义相似性，无法可靠地解决交叉引用或对规范演变进行推理。我们提出了DeepSpecs，一个通过结构化和时间推理增强的RAG系统，通过三个丰富的元数据库：SpecDB（条款对齐的规范文本）、ChangeDB（行级版本差异）和TDocDB（标准化会议文档）。DeepSpecs通过元数据查找递归检索引用的条款，明确解决交叉引用，并通过挖掘变化并将它们链接到记录设计原理的变更请求来跟踪规范演变。我们整理了两个5G问答数据集：573条来自从业者论坛和教育资源的专家注释的真实世界问题，以及350条从已批准变更请求中演变而来的问题。在多个LLM后端上，DeepSpecs优于基础模型和最先进的电信RAG系统；消融研究证实明确的交叉引用解决和演变感知检索显著提高了答案质量，强调了建模5G标准的结构和时间特性的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 5G technology enables mobile Internet access for billions of users. Answeringexpert-level questions about 5G specifications requires navigating thousands ofpages of cross-referenced standards that evolve across releases. Existingretrieval-augmented generation (RAG) frameworks, including telecom-specificapproaches, rely on semantic similarity and cannot reliably resolvecross-references or reason about specification evolution. We present DeepSpecs,a RAG system enhanced by structural and temporal reasoning via threemetadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB(line-level version diffs), and TDocDB (standardization meeting documents).DeepSpecs explicitly resolves cross-references by recursively retrievingreferenced clauses through metadata lookup, and traces specification evolutionby mining changes and linking them to Change Requests that document designrationale. We curate two 5G QA datasets: 573 expert-annotated real-worldquestions from practitioner forums and educational resources, and 350evolution-focused questions derived from approved Change Requests. Acrossmultiple LLM backends, DeepSpecs outperforms base models and state-of-the-arttelecom RAG systems; ablations confirm that explicit cross-reference resolutionand evolution-aware retrieval substantially improve answer quality,underscoring the value of modeling the structural and temporal properties of 5Gstandards.</description>
      <author>example@mail.com (Aman Ganapathy Manvattira, Yifei Xu, Ziyue Dang, Songwu Lu)</author>
      <guid isPermaLink="false">2511.01305v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>KAT-GNN: A Knowledge-Augmented Temporal Graph Neural Network for Risk Prediction in Electronic Health Records</title>
      <link>http://arxiv.org/abs/2511.01249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了KAT-GNN（知识增强型时序图神经网络）框架，用于基于电子健康记录的临床风险预测，通过整合临床知识和时序动态，在冠状动脉疾病预测和住院死亡率预测任务中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;使用电子健康记录进行临床风险预测对于及时干预和临床决策支持至关重要。然而，建模异构和不规则的时序EHR数据存在重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效整合临床知识和时序动态的图神经网络框架，用于提高临床风险预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;KAT-GNN首先从EHR中构建特定模态的患者图，然后使用两种知识来源增强这些图：（1）来自SNOMED CT的本体驱动边；（2）从EHR中提取的共现先验。随后，采用时间感知transformer来捕捉图编码的患者表示中的纵向动态。&lt;h4&gt;主要发现&lt;/h4&gt;KAT-GNN在冠状动脉疾病预测中达到最先进性能（AUROC: 0.9269 ± 0.0029），在MIMIC-III（AUROC: 0.9230 ± 0.0070）和MIMIC-IV（AUROC: 0.8849 ± 0.0089）的死亡率预测中也表现出色，持续优于GRASP和RETAIN等基线模型。消融研究证实，基于知识的增强和时序建模组件都是性能提升的重要贡献者。&lt;h4&gt;结论&lt;/h4&gt;将临床知识整合到图表示中，结合时间感知注意力机制，为跨不同临床任务和数据集的风险预测提供了一种有效且可推广的方法。&lt;h4&gt;翻译&lt;/h4&gt;使用电子健康记录进行临床风险预测对于促进及时干预和临床决策支持至关重要。然而，建模异构和不规则的时序EHR数据存在重大挑战。我们提出了KAT-GNN（知识增强型时序图神经网络），一种基于图的框架，整合临床知识和时序动态用于风险预测。KAT-GNN首先从EHR中构建特定模态的患者图，然后使用两种知识来源增强这些图：（1）来自SNOMED CT的本体驱动边；（2）从EHR中提取的共现先验。随后，采用时间感知transformer来捕捉图编码的患者表示中的纵向动态。KAT-GNN在三个不同的数据集和任务上进行了评估：使用长庚研究数据库进行冠状动脉疾病预测，以及使用MIMIC-III和MIMIC-IV数据集进行住院死亡率预测。KAT-GNN在CAD预测中达到最先进性能，在MIMIC-III和MIMIC-IV的死亡率预测中表现出色，持续优于GRASP和RETAIN等基线模型。消融研究证实，基于知识的增强和时序建模组件都是性能提升的重要贡献者。这些发现表明，将临床知识整合到图表示中，结合时间感知注意力机制，为跨不同临床任务和数据集的风险预测提供了一种有效且可推广的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clinical risk prediction using electronic health records (EHRs) is vital tofacilitate timely interventions and clinical decision support. However,modeling heterogeneous and irregular temporal EHR data presents significantchallenges. We propose \textbf{KAT-GNN} (Knowledge-Augmented Temporal GraphNeural Network), a graph-based framework that integrates clinical knowledge andtemporal dynamics for risk prediction. KAT-GNN first constructsmodality-specific patient graphs from EHRs. These graphs are then augmentedusing two knowledge sources: (1) ontology-driven edges derived from SNOMED CTand (2) co-occurrence priors extracted from EHRs. Subsequently, a time-awaretransformer is employed to capture longitudinal dynamics from the graph-encodedpatient representations. KAT-GNN is evaluated on three distinct datasets andtasks: coronary artery disease (CAD) prediction using the Chang Gung ResearchDatabase (CGRD) and in-hospital mortality prediction using the MIMIC-III andMIMIC-IV datasets. KAT-GNN achieves state-of-the-art performance in CADprediction (AUROC: 0.9269 $\pm$ 0.0029) and demonstrated strong results inmortality prediction in MIMIC-III (AUROC: 0.9230 $\pm$ 0.0070) and MIMIC-IV(AUROC: 0.8849 $\pm$ 0.0089), consistently outperforming established baselinessuch as GRASP and RETAIN. Ablation studies confirm that both knowledge-basedaugmentation and the temporal modeling component are significant contributorsto performance gains. These findings demonstrate that the integration ofclinical knowledge into graph representations, coupled with a time-awareattention mechanism, provides an effective and generalizable approach for riskprediction across diverse clinical tasks and datasets.</description>
      <author>example@mail.com (Kun-Wei Lin, Yu-Chen Kuo, Hsin-Yao Wang, Yi-Ju Tseng)</author>
      <guid isPermaLink="false">2511.01249v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs</title>
      <link>http://arxiv.org/abs/2511.00916v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Fleming-VL是一个统一的端到端框架，用于跨异构模态的综合医学视觉理解。通过三种关键策略解决了医学数据异质性和格式不一致的挑战，并在多个基准测试上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在通用领域表现出色，研究人员正致力于赋予其医学对话能力。然而，医学数据具有异质性，包含2D图像、3D体积扫描和时序视频序列等多种模态，这些模态间的领域差距和数据格式不一致阻碍了统一医学MLLMs的发展。&lt;h4&gt;目的&lt;/h4&gt;解决医学数据异质性和格式不一致的挑战，开发一个统一的端到端框架，用于跨异构模态的综合医学视觉理解。&lt;h4&gt;方法&lt;/h4&gt;从数据角度出发通过三种策略：(1)整合自然域和医学特定领域的长上下文数据扩大预训练；(2)补充稀有医学数据（包括整体视频分析和代表性不足的2D模态）；(3)扩展评估框架，纳入3D体积和视频理解基准。通过监督微调(SFT)和组相对策略优化(GRPO)开发了多种模型规模的Fleming-VL。&lt;h4&gt;主要发现&lt;/h4&gt;Fleming-VL在多个基准测试上取得了最先进的性能，包括医学VQA、视频问答和3D医学图像理解。&lt;h4&gt;结论&lt;/h4&gt;Fleming-VL成功解决了医学数据异质性和格式不一致的挑战，为跨模态医学视觉理解提供了统一框架。作者公开发布了Fleming-VL以促进医学AI的透明、可复现和可审计的进展。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)已在各种通用领域场景中展现出显著的有效性，如视觉问答和图像描述。最近，研究人员越来越专注于赋予MLLMs医学对话能力，这对临床应用具有重要前景。然而，医学数据由于其异质性而呈现独特挑战——包含多种模态，包括2D图像、3D体积扫描和时序视频序列。这些模态之间的显著领域差距和数据格式不一致阻碍了统一医学MLLMs的发展。为应对这些挑战，我们提出了Fleming-VL，一个用于跨异构模态综合医学视觉理解的统一端到端框架。Fleming-VL从数据角度通过三种关键策略解决这个问题：(1)通过整合自然域和医学特定领域的长上下文数据扩大预训练；(2)通过补充稀有医学数据（包括整体视频分析和代表性不足的2D模态，如超声和皮肤镜图像）来完善微调；(3)扩展现有评估框架，纳入3D体积和视频理解基准。通过监督微调(SFT)和组相对策略优化(GRPO)，我们开发了多种模型规模的Fleming-VL。大量实验表明，Fleming-VL在多个基准测试上取得了最先进的性能，包括医学VQA、视频问答和3D医学图像理解。我们公开发布Fleming-VL，以促进医学AI的透明、可复现和可审计的进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have demonstrated remarkableeffectiveness in various general-domain scenarios, such as visual questionanswering and image captioning. Recently, researchers have increasingly focusedon empowering MLLMs with medical conversational abilities, which holdsignificant promise for clinical applications. However, medical data presentsunique challenges due to its heterogeneous nature -- encompassing diversemodalities including 2D images, 3D volumetric scans, and temporal videosequences. The substantial domain gap and data format inconsistencies acrossthese modalities have hindered the development of unified medical MLLMs. Toaddress these challenges, we propose Fleming-VL, a unified end-to-end frameworkfor comprehensive medical visual understanding across heterogeneous modalities.Fleming-VL tackles this problem from a data-centric perspective through threekey strategies: (1) scaling up pretraining by integrating long-context datafrom both natural and medical-specific domains; (2) complementing fine-tuningwith rare medical data, including holistic video analysis and underrepresented2D modalities such as ultrasound and dermoscopy images; (3) extending existingevaluation frameworks to incorporate 3D volumetric and video understandingbenchmarks. Through supervised fine-tuning (SFT) and group relative policyoptimization (GRPO), we develop Fleming-VL in multiple model scales. Extensiveexperiments demonstrate that Fleming-VL achieves state-of-the-art performanceacross multiple benchmarks, including medical VQA, video QA, and 3D medicalimage understanding. We publicly release Fleming-VL to promote transparent,reproducible, and auditable progress in medical AI.</description>
      <author>example@mail.com (Yan Shu, Chi Liu, Robin Chen, Derek Li, Bryan Dai)</author>
      <guid isPermaLink="false">2511.00916v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>A Systematic Review of Spatio-Temporal Statistical Models: Theory, Structure, and Applications</title>
      <link>http://arxiv.org/abs/2511.00422v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一项关于时空数据统计模型的系统文献综述，提出了时空模型结构的分类方案，并分析了不同领域中的应用情况和模型特点。&lt;h4&gt;背景&lt;/h4&gt;具有时空属性的数据在许多研究领域普遍存在，分析时空关系的统计模型被广泛应用。现有的综述要么专注于特定领域，要么专注于特定模型类型，缺乏全面的、跨学科的综合概述。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有综述的局限性，作者旨在提出时空模型结构的分类方案，并突出它们在常见领域的应用。&lt;h4&gt;方法&lt;/h4&gt;作者遵循PRISMA指南进行了系统文献综述，搜索了两个数据库，时间跨度为2021-2025年，确定了83篇符合标准的出版物。&lt;h4&gt;主要发现&lt;/h4&gt;层次模型是最常使用的模型；大多数模型包含加性成分以考虑时空依赖性；不同应用领域的首选模型结构不同；研究工作主要集中在少数特定学科，尽管时空数据具有更广泛的相关性；可重复性仍然有限。&lt;h4&gt;结论&lt;/h4&gt;作者的综述不仅为跨学科比较模型结构提供了灵感，还强调了提高透明度、可访问性和跨领域知识转移的机会。&lt;h4&gt;翻译&lt;/h4&gt;具有时空属性的数据在许多研究领域普遍存在，分析时空关系的统计模型被广泛应用。现有的综述要么专注于特定领域，要么专注于特定模型类型，造成缺乏全面的、跨学科综合概述的空白。为解决这一问题，我们遵循PRISMA指南进行了系统文献综述，搜索了两个数据库中2021-2025年的文献，确定了83篇符合我们标准的出版物。我们提出了时空模型结构的分类方案，并突出了它们在常见领域中的应用：流行病学、生态学、公共卫生、经济学和犯罪学。尽管不同领域的任务有所不同，但许多模型具有相似之处。我们发现层次模型是最常使用的，大多数模型包含加性成分以考虑时空依赖性。应用领域的首选模型结构各不相同。我们还注意到，尽管时空数据具有更广泛的相关性，但研究工作主要集中在少数特定学科。此外，我们发现可重复性仍然有限。因此，我们的综述不仅为跨学科比较模型结构提供了灵感，还强调了提高透明度、可访问性和跨领域知识转移的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data with spatial-temporal attributes are prevalent across many researchfields, and statistical models for analyzing spatio-temporal relationships arewidely used. Existing reviews focus either on specific domains or model types,creating a gap in comprehensive, cross-disciplinary overviews. To address this,we conducted a systematic literature review following the PRISMA guidelines,searched two databases for the years 2021-2025, and identified 83 publicationsthat met our criteria. We propose a classification scheme for spatio-temporalmodel structures and highlight their application in the most common fields:epidemiology, ecology, public health, economics, and criminology. Althoughtasks vary by domain, many models share similarities. We found thathierarchical models are the most frequently used, and most models incorporateadditive components to account for spatial-temporal dependencies. The preferredmodel structures differ among fields of application. We also observe thatresearch efforts are concentrated in only a few specific disciplines, despitethe broader relevance of spatio-temporal data. Furthermore, we notice thatreproducibility remains limited. Our review, therefore, not only offersinspiration for comparing model structures in an interdisciplinary manner butalso highlights opportunities for greater transparency, accessibility, andcross-domain knowledge transfer.</description>
      <author>example@mail.com (Isabella Habereder, Thomas Kneib, Isao Echizen, Timo Spinde)</author>
      <guid isPermaLink="false">2511.00422v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>LongCat-Flash-Omni Technical Report</title>
      <link>http://arxiv.org/abs/2511.00279v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LongCat-Flash-Omni是一个最先进的开源多模态模型，具有5600亿参数，擅长实时音频视觉交互。它采用课程启发的渐进式训练策略，从简单到复杂的模态序列建模任务过渡，在保持强大单模态能力的同时获得全面的多模态能力。&lt;h4&gt;背景&lt;/h4&gt;基于LongCat-Flash模型，该模型采用高性能的捷径连接专家混合架构，具有零计算专家。LongCat-Flash-Omni集成了高效的多模态感知和语音重建模块。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实现低延迟实时音频视觉交互的大型多模态模型，同时保持高性能和效率。&lt;h4&gt;方法&lt;/h4&gt;采用课程启发的渐进式训练策略；基于LongCat-Flash的捷径连接MoE架构；集成高效多模态感知和语音重建模块；开发模态解耦并行化方案来管理大规模多模态训练中的数据和模型异质性。&lt;h4&gt;主要发现&lt;/h4&gt;尽管模型庞大（5600亿参数，其中270亿被激活），仍能实现低延迟实时音频视觉交互；模态解耦并行化方案效率高，能维持文本-only训练超过90%的吞吐量；在多模态基准测试中取得开源模型的最先进性能；在文本、图像、视频理解以及音频理解和生成等多种模态特定任务上具有高度竞争力。&lt;h4&gt;结论&lt;/h4&gt;LongCat-Flash-Omni是一个高效的大型多模态模型，通过创新的训练策略和架构设计，实现了实时音频视觉交互，并在各种任务上取得了优异性能。研究团队开源了该模型，以促进未来的研究和社区发展。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了LongCat-Flash-Omni，这是一个最先进的开源多模态模型，具有5600亿参数，擅长实时音频视觉交互。通过采用课程启发的渐进式训练策略，从简单到复杂的模态序列建模任务过渡，LongCat-Flash-Omni在保持强大单模态能力的同时获得了全面的多模态能力。基于采用高性能捷径连接专家混合架构且具有零计算专家的LongCat-Flash，LongCat-Flash-Omni集成了高效的多模态感知和语音重建模块。尽管其庞大的5600亿参数（其中270亿被激活），LongCat-Flash-Omni仍实现了低延迟的实时音频视觉交互。在训练基础设施方面，我们开发了一种模态解耦并行化方案，专门用于管理大规模多模态训练中固有的数据和模型异质性。这种创新方法通过维持文本-only训练超过90%的吞吐量，展示了卓越的效率。广泛的评估表明，LongCat-Flash-Omni在开源模型的多模态基准测试中取得了最先进的性能。此外，它在广泛的模态特定任务上提供了高度竞争性的结果，包括文本、图像和视频理解，以及音频理解和生成。我们全面概述了模型架构设计、训练流程和数据策略，并开源了该模型，以促进社区未来的研究和开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modalmodel with 560 billion parameters, excelling at real-time audio-visualinteraction. By adopting a curriculum-inspired progressive training strategythat transitions from simpler to increasingly complex modality sequencemodeling tasks, LongCat-Flash-Omni attains comprehensive multimodalcapabilities while maintaining strong unimodal capability. Building uponLongCat-Flash, which adopts a high-performance Shortcut-connectedMixture-of-Experts (MoE) architecture with zero-computation experts,LongCat-Flash-Omni integrates efficient multimodal perception and speechreconstruction modules. Despite its immense size of 560B parameters (with 27Bactivated), LongCat-Flash-Omni achieves low-latency real-time audio-visualinteraction. For training infrastructure, we developed a modality-decoupledparallelism scheme specifically designed to manage the data and modelheterogeneity inherent in large-scale multimodal training. This innovativeapproach demonstrates exceptional efficiency by sustaining over 90% of thethroughput achieved by text-only training. Extensive evaluations show thatLongCat-Flash-Omni achieves state-of-the-art performance on omni-modalbenchmarks among open-source models. Furthermore, it delivers highlycompetitive results across a wide range of modality-specific tasks, includingtext, image, and video understanding, as well as audio understanding andgeneration. We provide a comprehensive overview of the model architecturedesign, training procedures, and data strategies, and open-source the model tofoster future research and development in the community.</description>
      <author>example@mail.com (Meituan LongCat Team, Bairui Wang, Bayan, Bin Xiao, Bo Zhang, Bolin Rong, Borun Chen, Chang Wan, Chao Zhang, Chen Huang, Chen Chen, Chen Chen, Chengxu Yang, Chengzuo Yang, Cong Han, Dandan Peng, Delian Ruan, Detai Xin, Disong Wang, Dongchao Yang, Fanfan Liu, Fengjiao Chen, Fengyu Yang, Gan Dong, Gang Huang, Gang Xu, Guanglu Wan, Guoqiang Tan, Guoqiao Yu, Haibo Qiu, Hao Lu, Hongbo Liu, Hongyu Xiang, Jiaheng Wu, Jian Yang, Jiaxing Liu, Jing Huang, Jingang Wang, Jinrui Ding, Juchao Jiang, Jun Kuang, Jun Wang, Junhui Mei, Ke Ding, Kefeng Zhang, Lei Chen, Liang Shi, Limeng Qiao, Liming Zheng, Lin Ma, Liuyang Guo, Liya Ma, Luying Sun, Man Gao, Mengshen Zhu, Miao Cao, Minliang Lin, Nuo Xu, Peng Shi, Qi Zhang, Qian Fang, Qian Wang, Qian Yang, Quanxiu Wang, Rongxiang Weng, Rongxin Guo, Ruoxuan Liang, Senbin Yang, Shanbo Xu, Shanglin Lei, Shengze Ye, Shimin Chen, Shuaiqi Chen, Shujie Hu, Shuo Li, Siqi Yang, Siyu Xu, Siyu Ren, Song Li, Songxiang Liu, Tianhao Bai, Tianye Dai, Wei Hong, Wei Wang, Weixiao Zhao, Wengang Cao, Wenlong Zhu, Wenlong He, Xi Su, Xi Nan, Xiaohan Zhao, Xiaohao Wang, Xiaoyu Zhao, Xiaoyu Wang, Xiaoyu Li, Xin Pan, Xin Chen, Xiusong Sun, Xu Xiang, Xudong Xing, Xuezhi Cao, Xunliang Cai, Yang Yang, Yanli Tan, Yao Yao, Yerui Sun, Yi Chen, Yifan Lu, Yin Gong, Yining Zhang, Yitian Chen, Yiyang Gan, Yuchen Tang, Yuchen Xie, Yueqian Wang, Yuewen Zheng, Yufei Zhang, Yufeng Zhong, Yulei Qian, Yuqi Peng, Yuwei Jiang, Zeyang Hu, Zheng Zhang, Zhengkun Tian, Zhiqing Hong, Zhixiong Zeng, Zhuqi Mi, Ziran Li, Ziwen Wang, Ziyi Zhao, Ziyuan Zhuang, Zizhe Zhao)</author>
      <guid isPermaLink="false">2511.00279v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2511.00141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FLoC框架，一种基于设施定位函数的高效视觉标记压缩方法，用于解决长视频理解中视觉标记过多导致的扩展性问题，在保证接近最优性能的同时显著减少视觉标记数量。&lt;h4&gt;背景&lt;/h4&gt;最近的长视频理解研究利用了大型多模态模型(LMMs)先进的视觉-语言推理能力，推动了专门处理长视频序列的视频-LMMs的发展。然而，这些模型的扩展性受到长视频序列产生的大量视觉标记的严重限制。&lt;h4&gt;目的&lt;/h4&gt;解决长视频理解中视觉标记过多导致的模型扩展性问题，开发一种高效的方法来压缩视觉标记，同时保持模型的性能。&lt;h4&gt;方法&lt;/h4&gt;提出FLoC框架，基于设施定位函数的视觉标记压缩方法，通过懒贪婪算法快速选择紧凑且具有代表性和多样性的视觉标记子集，在预定义的视觉标记数量预算内工作。该方法无需训练，与模型和查询无关。&lt;h4&gt;主要发现&lt;/h4&gt;在Video-MME、MLVU和LongVideoBench等大规模基准测试上的广泛评估表明，该框架始终优于最近的压缩技术，展示了其在解决长视频理解关键挑战方面的有效性和稳健性，以及处理速度方面的效率。&lt;h4&gt;结论&lt;/h4&gt;FLoC框架为长视频理解提供了一个通用的解决方案，能够无缝集成到各种视频-LLMs和现有工作流程中，显著减少视觉标记数量而不牺牲性能。&lt;h4&gt;翻译&lt;/h4&gt;最近的长视频理解研究利用了大型多模态模型(LMMs)先进的视觉-语言推理能力，推动了专门处理长视频序列的视频-LMMs的发展。然而，这些模型的扩展性受到长视频序列产生的大量视觉标记的严重限制。为应对这一挑战，本文提出了FLoC，一种基于设施定位函数的高效视觉标记压缩框架，这是一种在视觉标记数量预定义预算内快速选择紧凑但高度代表性和多样性视觉标记子集的原则性方法。通过集成懒贪婪算法，我们的方法通过快速选择紧凑的标记子集实现了显著的效率提升，在保证接近最优性能的同时大幅减少了视觉标记数量。值得注意的是，我们的方法无需训练，与模型和查询无关，提供了一个通用的解决方案，可以无缝集成到各种视频-LLMs和现有工作流程中。在Video-MME、MLVU和LongVideoBench等大规模基准上的广泛评估表明，我们的框架始终优于最近的压缩技术，这不仅突显了其在解决长视频理解关键挑战方面的有效性和稳健性，也展示了其在处理速度方面的效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies in long video understanding have harnessed the advancedvisual-language reasoning capabilities of Large Multimodal Models (LMMs),driving the evolution of video-LMMs specialized for processing extended videosequences. However, the scalability of these models is severely limited by theoverwhelming volume of visual tokens generated from extended video sequences.To address this challenge, this paper proposes FLoC, an efficient visual tokencompression framework based on the facility location function, a principledapproach that swiftly selects a compact yet highly representative and diversesubset of visual tokens within a predefined budget on the number of visualtokens. By integrating the lazy greedy algorithm, our method achievesremarkable efficiency gains by swiftly selecting a compact subset of tokens,drastically reducing the number of visual tokens while guaranteeingnear-optimal performance. Notably, our approach is training-free,model-agnostic, and query-agnostic, providing a versatile solution thatseamlessly integrates with diverse video-LLMs and existing workflows. Extensiveevaluations on large-scale benchmarks, such as Video-MME, MLVU, andLongVideoBench, demonstrate that our framework consistently surpasses recentcompression techniques, highlighting not only its effectiveness and robustnessin addressing the critical challenges of long video understanding, but also itsefficiency in processing speed.</description>
      <author>example@mail.com (Janghoon Cho, Jungsoo Lee, Munawar Hayat, Kyuwoong Hwang, Fatih Porikli, Sungha Choi)</author>
      <guid isPermaLink="false">2511.00141v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency</title>
      <link>http://arxiv.org/abs/2511.00107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了MOVAI框架，解决了文本到视频生成中的时间一致性、组合理解和精细控制问题，通过创新的场景解析、注意力机制和视频细化模块实现了高质量视频生成。&lt;h4&gt;背景&lt;/h4&gt;文本到视频生成是生成式人工智能的关键前沿，但现有方法在保持时间一致性、组合理解和精细控制视觉叙事方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一个名为MOVAI的新型分层框架，用于高保真文本到视频合成，结合组合场景理解和时间感知扩散模型。&lt;h4&gt;方法&lt;/h4&gt;MOVAI框架包含三个关键创新：(1)组合场景解析器(CSP)将文本描述分解为带有时间注释的分层场景图；(2)时间-空间注意力机制(TSAM)确保帧间连贯运动动态同时保持空间细节；(3)渐进式视频细化(PVR)模块通过多尺度时间推理迭代提高视频质量。&lt;h4&gt;主要发现&lt;/h4&gt;在标准基准上的实验表明，MOVAI实现了最先进的性能，相比现有方法，LPIPS指标提高15.3%，FVD指标提高12.7%，用户偏好研究提高18.9%。&lt;h4&gt;结论&lt;/h4&gt;MOVAI框架在生成具有真实时间动态和精细语义控制的多对象复杂场景方面表现出特别优势。&lt;h4&gt;翻译&lt;/h4&gt;文本到视频生成已成为生成式人工智能的关键前沿，然而现有方法在保持时间一致性、组合理解和精细控制视觉叙事方面存在困难。我们提出了MOVAI（多模态原始视频AI），这是一种创新的分层框架，将组合场景理解与时间感知扩散模型相结合，用于高保真文本到视频合成。我们的方法引入了三个关键创新：(1)组合场景解析器(CSP)，将文本描述分解为带有时间注释的分层场景图；(2)时间-空间注意力机制(TSAM)，确保帧间连贯的运动动态同时保持空间细节；(3)渐进式视频细化(PVR)模块，通过多尺度时间推理迭代提高视频质量。在标准基准上的广泛实验表明，MOVAI实现了最先进的性能，与现有方法相比，在LPIPS指标上提高15.3%，在FVD指标上提高12.7%，在用户偏好研究中提高18.9%。我们的框架在生成具有真实时间动态和精细语义控制的多对象复杂场景方面表现出特别优势。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决文本到视频生成中的时间一致性、组合理解和精细控制问题。这些问题很重要，因为视频不仅是图像集合，而是复杂的时间叙事，需要确保物体在帧间保持一致、动作流畅自然，同时能准确理解文本描述中的复杂场景关系。现有方法常出现闪烁、物体变形、控制不足等问题，限制了视频生成技术的实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到视频生成比图像生成更具挑战性，需要从根本上设计时间建模，而非简单地在图像生成基础上添加时间维度。他们借鉴了文本到图像生成中的扩散模型技术（如Stable Diffusion），视频生成中的3D卷积和Transformer架构（如CogVideo），以及组合理解中的场景图生成技术。基于这些现有工作，作者设计了MOVAI框架，从基础层面构建专门用于时间视觉叙事的系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将文本描述分解为层次化场景图，同时建模帧内空间关系和跨帧时间依赖，通过多尺度时间推理渐进式提高视频质量。整体流程分为四个阶段：1)文本输入处理：使用BERT编码器将文本转换为密集嵌入；2)场景理解：CSP模块通过图神经网络生成包含对象、关系和时间约束的场景图；3)注意力处理：TSAM模块通过空间、时间和跨模态三种注意力机制确保生成一致性；4)视频生成：PVR模块通过三个分辨率级别迭代细化视频质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三个：1)组合场景解析器(CSP)：将文本分解为层次化场景图，提供精细控制；2)时间-空间注意力机制(TSAM)：统一建模空间和时间关系，保持物体一致性并确保流畅运动；3)渐进式视频细化(PVR)：多阶段细化过程提高生成稳定性。相比之前工作，MOVAI不是简单地将图像生成方法扩展到视频，而是专为时间视觉叙事设计的系统，解决了现有方法在复杂场景中表现不佳的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MOVAI通过创新的组合场景解析、时间-空间注意力和渐进式视频细化框架，显著提高了文本到视频生成的时间一致性和视觉质量，为复杂场景中的高质量视频生成提供了新方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text to video generation has emerged as a critical frontier in generativeartificial intelligence, yet existing approaches struggle with maintainingtemporal consistency, compositional understanding, and fine grained controlover visual narratives. We present MOVAI (Multimodal Original Video AI), anovel hierarchical framework that integrates compositional scene understandingwith temporal aware diffusion models for high fidelity text to video synthesis.Our approach introduces three key innovations: (1) a Compositional Scene Parser(CSP) that decomposes textual descriptions into hierarchical scene graphs withtemporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) thatensures coherent motion dynamics across frames while preserving spatialdetails, and (3) a Progressive Video Refinement (PVR) module that iterativelyenhances video quality through multi-scale temporal reasoning. Extensiveexperiments on standard benchmarks demonstrate that MOVAI achievesstate-of-the-art performance, improving video quality metrics by 15.3% inLPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existingmethods. Our framework shows particular strength in generating complexmulti-object scenes with realistic temporal dynamics and fine-grained semanticcontrol.</description>
      <author>example@mail.com (Piyushkumar Patel)</author>
      <guid isPermaLink="false">2511.00107v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning</title>
      <link>http://arxiv.org/abs/2511.00085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MaGNet，一种基于Mamba双超图网络的股票趋势预测方法，通过三个关键创新解决了现有方法在捕捉时间依赖性和动态股票间互动方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;股票趋势预测对盈利交易策略和投资组合管理至关重要，但由于市场波动性、复杂时间动态和股票间多维关系的存在，这一任务极具挑战性。现有方法难以有效捕捉时间依赖性和动态股票间互动，常忽略市场横截面影响，依赖静态相关性，对节点和边采用统一处理，并混淆多样化关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型股票预测模型，有效捕捉时间依赖性和动态股票间互动，提高预测准确性和投资回报。&lt;h4&gt;方法&lt;/h4&gt;MaGNet包含三个关键创新：(1) MAGE块：利用双向Mamba和自适应门控机制进行上下文时间建模，集成稀疏专家混合层动态适应不同市场条件，使用多头注意力捕获全局依赖；(2) 特征级和股票级二维时空注意力模块：实现多元特征精确融合和跨股票依赖关系，桥接时间建模与关系推理；(3) 双超图框架：包含时间约束超图(TCH)捕获细粒度因果依赖，以及全局概率超图(GPH)建模市场范围模式，实现多尺度关系学习。&lt;h4&gt;主要发现&lt;/h4&gt;在六个主要股票指数上的广泛实验表明，MaGNet在预测性能和投资回报方面均优于最先进方法，且具有出色的风险管理能力。&lt;h4&gt;结论&lt;/h4&gt;MaGNet通过创新的架构设计有效解决了股票趋势预测中的关键挑战，能够处理市场波动性、复杂时间动态和股票间多维关系，为交易策略和投资组合管理提供了更可靠的预测工具。&lt;h4&gt;翻译&lt;/h4&gt;股票趋势预测对盈利交易策略和投资组合管理至关重要，但由于市场波动性、复杂的时间动态和股票间的多维关系，这一任务仍然具有挑战性。现有方法难以有效捕捉时间依赖性和动态的股票间互动，常常忽略市场横截面影响，依赖静态相关性，对节点和边采用统一处理，并混淆多样化关系。这项工作引入了MaGNet，一种用于股票预测的新型Mamba双超图网络，整合了三个关键创新：(1) MAGE块，利用双向Mamba和自适应门控机制进行上下文时间建模，并集成稀疏专家混合层以动态适应不同市场条件，同时使用多头注意力捕获全局依赖；(2) 特征级和股票级二维时空注意力模块实现多元特征的精确融合和跨股票依赖关系，有效增强信息量同时保留内在数据结构，桥接时间建模与关系推理；(3) 双超图框架包括时间约束超图(TCH)，通过时间约束捕获细粒度因果依赖，以及全局概率超图(GPH)，通过软超边分配和Jensen-Shannon散度加权机制建模市场范围模式，共同分离局部时间影响与瞬时全局结构，实现多尺度关系学习。在六个主要股票指数上的广泛实验表明，MaGNet在预测性能和投资回报方面均优于最先进方法，并具有出色的风险管理能力。代码可在https://github.com/PeilinTime/MaGNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stock trend prediction is crucial for profitable trading strategies andportfolio management yet remains challenging due to market volatility, complextemporal dynamics and multifaceted inter-stock relationships. Existing methodsstruggle to effectively capture temporal dependencies and dynamic inter-stockinteractions, often neglecting cross-sectional market influences, relying onstatic correlations, employing uniform treatments of nodes and edges, andconflating diverse relationships. This work introduces MaGNet, a novel Mambadual-hyperGraph Network for stock prediction, integrating three keyinnovations: (1) a MAGE block, which leverages bidirectional Mamba withadaptive gating mechanisms for contextual temporal modeling and integrates asparse Mixture-of-Experts layer to enable dynamic adaptation to diverse marketconditions, alongside multi-head attention for capturing global dependencies;(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enableprecise fusion of multivariate features and cross-stock dependencies,effectively enhancing informativeness while preserving intrinsic datastructures, bridging temporal modeling with relational reasoning; and (3) adual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)that captures fine-grained causal dependencies with temporal constraints, andGlobal Probabilistic Hypergraph (GPH) that models market-wide patterns throughsoft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,jointly disentangling localized temporal influences from instantaneous globalstructures for multi-scale relational learning. Extensive experiments on sixmajor stock indices demonstrate MaGNet outperforms state-of-the-art methods inboth superior predictive performance and exceptional investment returns withrobust risk management capabilities. Codes available at:https://github.com/PeilinTime/MaGNet.</description>
      <author>example@mail.com (Peilin Tan, Chuanqi Shi, Dian Tu, Liang Xie)</author>
      <guid isPermaLink="false">2511.00085v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs</title>
      <link>http://arxiv.org/abs/2511.01768v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为UniLION的统一自动驾驶模型，能够高效处理大规模LiDAR点云、高分辨率多视角图像和事件时间序列数据，基于线性群组RNN算子。该模型作为单一通用架构，可无缝支持多种专业变体，并在多项核心任务上取得具有竞争力的甚至最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;Transformer模型在各个领域表现出色，但其二次注意力机制在处理长序列数据时引入了显著的计算开销。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一的多模态自动驾驶模型，能够处理多种数据类型，并在3D感知、预测和规划等核心任务上保持高性能，同时简化多模态和多任务自动驾驶系统的设计。&lt;h4&gt;方法&lt;/h4&gt;提出了UniLION模型，基于线性群组RNN算子，能够高效处理大规模LiDAR点云、高分辨率多视角图像和事件时间序列。该模型作为单一架构支持多种专业变体（仅LiDAR、时间LiDAR、多模态、多模态时间融合配置），无需显式的时间或多模态融合模块。&lt;h4&gt;主要发现&lt;/h4&gt;UniLION在3D感知（3D目标检测、3D目标跟踪、3D占用预测、BEV地图分割）、预测（运动预测）和规划（端到端规划）等广泛核心任务中持续提供具有竞争力和最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;这种统一范式自然地简化了多模态和多任务自动驾驶系统的设计，同时保持卓越性能，为自动驾驶中3D基础模型的发展提供了新视角。&lt;h4&gt;翻译&lt;/h4&gt;尽管Transformer已在各个领域展现出卓越的能力，但其二次注意力机制在处理长序列数据时引入了显著的计算开销。在本文中，我们提出了一个统一的自动驾驶模型UniLION，它基于线性群组RNN算子（即对分组特征执行线性RNN），能够高效处理大规模LiDAR点云、高分辨率多视角图像和事件时间序列。值得注意的是，UniLION作为一个单一的多功能架构，可以无缝支持多种专业变体（即仅LiDAR、时间LiDAR、多模态和多模态时间融合配置），而无需显式的时间或多模态融合模块。此外，UniLION在广泛的核心任务中持续提供具有竞争力和最先进的性能，包括3D感知（例如3D目标检测、3D目标跟踪、3D占用预测、BEV地图分割）、预测（例如运动预测）和规划（例如端到端规划）。这种统一范式自然地简化了多模态和多任务自动驾驶系统的设计，同时保持卓越性能。最终，我们希望UniLION能为自动驾驶中3D基础模型的发展提供新的视角。代码可在https://github.com/happinesslz/UniLION获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统中多模态数据（激光雷达点云和摄像头图像）和时间序列信息的统一处理问题。现实中，自动驾驶需要同时处理来自不同传感器的异构数据和时间维度的信息，而现有方法通常需要复杂的融合模块和顺序依赖的架构，导致系统复杂、计算效率低。解决这个问题可以简化自动驾驶系统设计，提高计算效率，增强系统鲁棒性和适应性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到Transformer的二次方复杂度不适合处理长序列数据，而线性RNN具有线性计算复杂度的优势。他们借鉴了之前的工作LION（基于线性RNN的3D目标检测），并扩展到更广泛的自动驾驶任务。作者还参考了BEV表示方法和多模态融合技术，但通过线性组RNN的创新应用，消除了对显式融合模块的需求，实现了更简洁高效的统一架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用线性组RNN作为统一架构的基础，通过直接标记级连接将多模态（激光雷达和图像）和时间信息统一处理，消除显式的融合模块，生成紧凑的BEV特征表示。整体流程包括：1) 编码阶段处理激光雷达点云和多视角图像；2) 使用3D稀疏窗口分区将输入体素分组；3) 通过UniLION块进行特征交互；4) 使用自回归体素生成策略增强特征；5) 通过BEV主干网络生成统一表示；6) 并行执行各种下游任务（检测、跟踪等）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一异构输入，通过直接标记连接集成多模态和时间信息；2) 统一模型架构，实现不同输入格式的参数共享；3) 统一输出表示，将多模态时间信息压缩为紧凑BEV特征；4) 在多种任务上实现竞争性或最先进性能。相比之前工作，UniLION消除了显式融合模块，使用线性RNN降低计算复杂度，支持单一模型处理多种配置，实现并行多任务学习而非顺序依赖，提高了系统灵活性和故障容错能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UniLION提出了一种基于线性组RNN的统一自动驾驶模型框架，能够高效处理多模态和时间序列数据，在保持高性能的同时显著简化了系统架构并提高了计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although transformers have demonstrated remarkable capabilities acrossvarious domains, their quadratic attention mechanisms introduce significantcomputational overhead when processing long-sequence data. In this paper, wepresent a unified autonomous driving model, UniLION, which efficiently handleslarge-scale LiDAR point clouds, high-resolution multi-view images, and eventemporal sequences based on the linear group RNN operator (i.e., performslinear RNN for grouped features). Remarkably, UniLION serves as a singleversatile architecture that can seamlessly support multiple specializedvariants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modaltemporal fusion configurations) without requiring explicit temporal ormulti-modal fusion modules. Moreover, UniLION consistently delivers competitiveand even state-of-the-art performance across a wide range of core tasks,including 3D perception (e.g., 3D object detection, 3D object tracking, 3Doccupancy prediction, BEV map segmentation), prediction (e.g., motionprediction), and planning (e.g., end-to-end planning). This unified paradigmnaturally simplifies the design of multi-modal and multi-task autonomousdriving systems while maintaining superior performance. Ultimately, we hopeUniLION offers a fresh perspective on the development of 3D foundation modelsin autonomous driving. Code is available athttps://github.com/happinesslz/UniLION</description>
      <author>example@mail.com (Zhe Liu, Jinghua Hou, Xiaoqing Ye, Jingdong Wang, Hengshuang Zhao, Xiang Bai)</author>
      <guid isPermaLink="false">2511.01768v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System</title>
      <link>http://arxiv.org/abs/2511.00096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to The 3rd ACM SIGSPATIAL International Workshop on Advances  in Urban AI (UrbanAI'25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Urban-MAS是一个基于大语言模型的多智能体系统框架，通过三种专门设计的智能体提高城市预测的准确性和可靠性，在零样本设置下进行以人为中心的城市预测任务。&lt;h4&gt;背景&lt;/h4&gt;Urban AI在以人为中心的城市任务方面取得进展，大语言模型虽能整合多模态输入处理城市异构数据，但在特定领域任务上表现欠佳。&lt;h4&gt;目的&lt;/h4&gt;介绍Urban-MAS框架，用于在零样本设置下进行以人为中心的城市预测，解决单一大语言模型在城市特定任务上的局限性。&lt;h4&gt;方法&lt;/h4&gt;Urban-MAS包含三种智能体：预测因素引导智能体（优先考虑关键预测因素指导知识提取）、可靠城市信息提取智能体（通过比较输出、验证一致性提高鲁棒性）、多城市信息推理智能体（跨维度整合多源信息进行预测）。&lt;h4&gt;主要发现&lt;/h4&gt;在东京、米兰和西雅图的实验中，Urban-MAS相比单-LLM基线显著减少预测误差；消融研究表明预测因素引导智能体对提高预测性能最为关键。&lt;h4&gt;结论&lt;/h4&gt;Urban-MAS被定位为可扩展的以人为中心的城市AI预测范式，代码已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;城市人工智能（Urban AI）已推进了以人为中心的城市任务，如感知预测和人类动态。大语言模型可以整合多模态输入以处理复杂城市系统中的异构数据，但在特定领域任务上往往表现不佳。Urban-MAS是一个基于大语言模型的多智能体系统（MAS）框架，用于在零样本设置下进行以人为中心的城市预测。它包括三种智能体类型：预测因素引导智能体，优先考虑关键预测因素以指导知识提取，增强压缩城市知识在大语言模型中的有效性；可靠城市信息提取智能体，通过比较多个输出、验证一致性并在发生冲突时重新提取来提高鲁棒性；多城市信息推理智能体，跨维度整合提取的多源信息进行预测。在东京、米兰和西雅图的运行量预测和城市感知实验表明，Urban-MAS与单-LLM基线相比显著减少了误差。消融研究表明预测因素引导智能体对提高预测性能最为关键，使Urban-MAS成为以人为中心的城市AI预测的可扩展范式。代码可在项目网站获取：https://github.com/THETUREHOOHA/UrbanMAS&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban Artificial Intelligence (Urban AI) has advanced human-centered urbantasks such as perception prediction and human dynamics. Large Language Models(LLMs) can integrate multimodal inputs to address heterogeneous data in complexurban systems but often underperform on domain-specific tasks. Urban-MAS, anLLM-based Multi-Agent System (MAS) framework, is introduced for human-centeredurban prediction under zero-shot settings. It includes three agent types:Predictive Factor Guidance Agents, which prioritize key predictive factors toguide knowledge extraction and enhance the effectiveness of compressed urbanknowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improverobustness by comparing multiple outputs, validating consistency, andre-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, whichintegrate extracted multi-source information across dimensions for prediction.Experiments on running-amount prediction and urban perception across Tokyo,Milan, and Seattle demonstrate that Urban-MAS substantially reduces errorscompared to single-LLM baselines. Ablation studies indicate that PredictiveFactor Guidance Agents are most critical for enhancing predictive performance,positioning Urban-MAS as a scalable paradigm for human-centered urban AIprediction. Code is available on the projectwebsite:https://github.com/THETUREHOOHA/UrbanMAS</description>
      <author>example@mail.com (Shangyu Lou)</author>
      <guid isPermaLink="false">2511.00096v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning</title>
      <link>http://arxiv.org/abs/2511.02205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 12 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OmniField是一种连续感知框架，能够处理真实世界实验数据的多模态时空学习挑战，通过学习基于可用模态条件化的连续神经场并迭代融合跨模态上下文，实现统一的重建、插值、预测和跨模态预测功能。&lt;h4&gt;背景&lt;/h4&gt;真实世界实验数据的多模态时空学习面临两个主要挑战：单模态测量数据稀疏、不规则且带有噪声，但跨模态之间存在相关性；可用模态集合随空间和时间变化，导致可用记录减少。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够适应任意模态子集并处理稀疏、不规则、带噪声数据的框架，用于多模态时空学习。&lt;h4&gt;方法&lt;/h4&gt;提出OmniField框架，该框架学习基于可用模态条件化的连续神经场，并通过多模态串扰块架构与迭代跨模态细化相结合，在解码器前对齐信号，无需网格化或代理预处理即可实现统一功能。&lt;h4&gt;主要发现&lt;/h4&gt;OmniField在评估中始终优于八个强大的多模态时空基线；即使在严重的模拟传感器噪声下，性能仍接近清洁输入水平，显示出对损坏测量的强鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;OmniField是一种有效解决多模态时空学习中数据稀疏、不规则、带噪声以及模态集合变化等挑战的框架。&lt;h4&gt;翻译&lt;/h4&gt;真实世界实验数据的多模态时空学习受两个挑战限制：单模态测量稀疏、不规则且带有噪声(QA/QC伪影)，但跨模态相关；可用模态集合随空间和时间变化，除非模型能够在训练和测试时适应任意子集，否则会缩小可用记录。我们提出了OmniField，一种连续感知框架，学习基于可用模态条件化的连续神经场，并迭代融合跨模态上下文。多模态串扰块架构与迭代跨模态细化相结合，在解码器前对齐信号，无需网格化或代理预处理即可实现统一的重建、插值、预测和跨模态预测。广泛评估显示，OmniField始终优于八个强大的多模态时空基线。在严重的模拟传感器噪声下，性能仍接近清洁输入水平，突显了对损坏测量的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态时空学习中的两个关键挑战：1) 同一模态的测量数据稀疏、不规则且含噪声，但不同模态间相互关联；2) 可用模态集在空间和时间上变化，缩小可用记录规模。这些问题在气候科学、空气污染研究、材料科学等多个领域都至关重要，因为现有方法要么依赖数据预处理引入系统副作用，要么使用模型方法但假设固定观测算子，难以处理真实世界中的传感器数据不完整性和噪声问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：数据预处理引入平滑偏差和不确定性崩溃，而模型方法假设固定观测算子在实际情况中不成立。基于此，作者设计了一个连续感知的框架OmniField，它基于条件神经场(CNFs)原理，扩展了SCENT的工作。借鉴了多模态融合(MIA)、神经场表示(NeRF)和算子学习(FNO)等现有方法，但针对科学数据的特殊性进行了改进，特别是处理稀疏、不规则、噪声数据的能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习一个连续感知的多模态条件神经场，能够处理稀疏、不规则、噪声数据，并适应不同模态的可用性。整体流程采用编码器-处理器-解码器架构：1) 编码器将不规则观测转换为固定长度表示；2) 处理器融合坐标编码与上下文摘要，形成条件神经场；3) 解码器生成各模态预测。关键组件包括高斯傅里叶特征(GFF)和正弦初始化解决低频偏差，多模态串扰(MCT)块实现跨模态信息交换，迭代跨模态精炼(ICMR)渐进对齐信号，以及灵活模态 fusion处理模态缺失情况。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 连续感知的多模态条件神经场框架；2) 高斯傅里叶特征和正弦初始化解决低频偏差；3) 多模态串扰(MCT)块实现跨模态信息交换；4) 迭代跨模态精炼(ICMR)渐进信号对齐；5) 灵活模态融合处理模态缺失。相比之前工作，OmniField扩展了SCENT的多模态能力，不同于PROSE-FD的PDE算子学习，区别于MIA的双层优化方法，且无需网格化预处理，能更好处理科学数据中的稀疏性和噪声问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OmniField通过引入连续感知的多模态条件神经场、多模态串扰块和迭代跨模态精炼机制，有效解决了科学实验中多模态时空学习的稀疏性、不规则性和噪声挑战，实现了在无需网格化或代理预处理的情况下，对多种任务的鲁棒统一处理。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal spatiotemporal learning on real-world experimental data isconstrained by two challenges: within-modality measurements are sparse,irregular, and noisy (QA/QC artifacts) but cross-modally correlated; the set ofavailable modalities varies across space and time, shrinking the usable recordunless models can adapt to arbitrary subsets at train and test time. We proposeOmniField, a continuity-aware framework that learns a continuous neural fieldconditioned on available modalities and iteratively fuses cross-modal context.A multimodal crosstalk block architecture paired with iterative cross-modalrefinement aligns signals prior to the decoder, enabling unifiedreconstruction, interpolation, forecasting, and cross-modal prediction withoutgridding or surrogate preprocessing. Extensive evaluations show that OmniFieldconsistently outperforms eight strong multimodal spatiotemporal baselines.Under heavy simulated sensor noise, performance remains close to clean-inputlevels, highlighting robustness to corrupted measurements.</description>
      <author>example@mail.com (Kevin Valencia, Thilina Balasooriya, Xihaier Luo, Shinjae Yoo, David Keetae Park)</author>
      <guid isPermaLink="false">2511.02205v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Bayesian full waveform inversion with learned prior using deep convolutional autoencoder</title>
      <link>http://arxiv.org/abs/2511.02737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 19 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合深度卷积自编码器和贝叶斯全波形反演的方法，通过降低模型维度和优化计算过程，实现了更高效的速度模型重建和不确定性评估。&lt;h4&gt;背景&lt;/h4&gt;全波形反演(FWI)可以用贝叶斯框架表达，其中相关的不确定性由后验概率分布(PPD)捕获。然而，使用基于采样的方法如马尔可夫链蒙特卡洛(MCMC)解决贝叶斯FWI在计算上非常困难，因为模型空间的维度极高。&lt;h4&gt;目的&lt;/h4&gt;为了缓解计算困难，作者开发了一种深度卷积自编码器(CAE)作为反演的学习先验，以提高计算效率。&lt;h4&gt;方法&lt;/h4&gt;1) 使用CAE将详细的地下速度模型压缩为低维潜在表示；2) 采用自适应梯度MCMC算法，通过基于自动微分的FWI在潜在空间中高效计算梯度；3) 实现迁移学习策略，通过反演过程中的在线微调，使框架能够适应原始训练集中未表示的速度结构。&lt;h4&gt;主要发现&lt;/h4&gt;使用合成数据的数值实验表明，与传统MCMC方法相比，该方法能以更高的效率重建速度模型并评估不确定性。&lt;h4&gt;结论&lt;/h4&gt;结合深度学习和贝叶斯方法的创新框架有效解决了高维模型空间中的计算挑战，实现了更高效的全波形反演。&lt;h4&gt;翻译&lt;/h4&gt;全波形反演(FWI)可以用贝叶斯框架表达，其中相关的不确定性由后验概率分布(PPD)捕获。实际上，使用基于采样的方法如马尔可夫链蒙特卡洛(MCMC)解决贝叶斯FWI在计算上非常困难，因为模型空间的维度极高。为了缓解这一困难，我们开发了一种深度卷积自编码器(CAE)，作为反演的学习先验。CAE将详细的地下速度模型压缩为低维潜在表示，实现了比传统降维方法更有效且地质一致性的模型简化。反演过程采用自适应梯度MCMC算法，通过基于自动微分的FWI在潜在空间中高效计算梯度。此外，我们通过反演过程中的在线微调实现了迁移学习策略，使框架能够适应原始训练集中未表示的速度结构。使用合成数据的数值实验表明，与传统MCMC方法相比，该方法能以更高的效率重建速度模型并评估不确定性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决贝叶斯全波形反演(Bayesian FWI)的计算效率问题。传统基于采样的贝叶斯方法(如MCMC)因模型空间维度极高而计算需求巨大，难以实际应用。这个问题很重要，因为贝叶斯框架能自然包含不确定性，提供更全面的地下结构成像结果，而传统方法只给出单一最优解，无法评估不确定性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到贝叶斯FWI面临维度灾难问题后，借鉴了深度学习中的自编码器技术，特别是卷积自编码器(CAE)的降维能力。他们结合了贝叶斯推理框架、MCMC采样方法、自动微分技术和迁移学习策略，提出在低维潜在空间中进行采样以提高效率。该方法确实借鉴了大量现有工作，包括贝叶斯理论基础、自编码器应用、MCMC方法和自动微分技术等。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用深度卷积自编码器将高维地下速度模型压缩到低维潜在空间，在这个低维空间中进行贝叶斯MCMC采样，解决维度灾难问题。流程包括：1)训练阶段：收集地下速度模型数据，训练CAE；2)反演阶段：将初始模型映射到潜在空间，执行自适应梯度MCMC采样，使用自动微分计算梯度；3)对于分布外情况，先进行解码器在线微调，再进行采样；4)结果分析：收集后验样本，计算统计量和评估不确定性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用深度卷积自编码器作为学习先验，优于传统参数化方法；2)在潜在空间中进行自适应梯度MCMC采样，大幅降低计算复杂度；3)提出在线微调的迁移学习策略，解决分布外反演问题；4)使用正弦激活函数提高性能。相比之前工作，本文更紧密地结合自编码器与贝叶斯框架，提出完整流程，证明了CAE在保持地质特征方面的优势，并解决了分布外反演挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合深度卷积自编码器与贝叶斯全波形反演的创新方法，通过在低维潜在空间中进行高效采样，显著提高了地下结构成像的计算效率，同时能够量化反演结果的不确定性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Full waveform inversion (FWI) can be expressed in a Bayesian framework, wherethe associated uncertainties are captured by the posterior probabilitydistribution (PPD). In practice, solving Bayesian FWI with sampling-basedmethods such as Markov chain Monte Carlo (MCMC) is computationally demandingbecause of the extremely high dimensionality of the model space. To alleviatethis difficulty, we develop a deep convolutional autoencoder (CAE) that servesas a learned prior for the inversion. The CAE compresses detailed subsurfacevelocity models into a low-dimensional latent representation, achieving moreeffective and geologically consistent model reduction than conventionaldimension reduction approaches. The inversion procedure employs an adaptivegradient-based MCMC algorithm enhanced by automatic differentiation-based FWIto compute gradients efficiently in the latent space. In addition, we implementa transfer learning strategy through online fine-tuning during inversion,enabling the framework to adapt to velocity structures not represented in theoriginal training set. Numerical experiments with synthetic data show that themethod can reconstruct velocity models and assess uncertainty with improvedefficiency compared to traditional MCMC methods.</description>
      <author>example@mail.com (Shuhua Hu, Mrinal K Sen, Zeyu Zhao, Abdelrahman Elmeliegy, Shuo Zhang)</author>
      <guid isPermaLink="false">2511.02737v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data</title>
      <link>http://arxiv.org/abs/2511.02541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 6 figures, 1 table; accepted for AI-2025 Forty-fifth SGAI  International Conference on Artificial Intelligence CAMBRIDGE, ENGLAND 16-18  DECEMBER 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了无监督学习方法在剪切散斑图像自动异常检测中的应用，比较了三种深度学习模型，发现师生特征匹配方法在分类鲁棒性和缺陷定位方面表现最佳，为工业环境中的高效无损检测提供了新思路。&lt;h4&gt;背景&lt;/h4&gt;剪切散斑测量是一种高灵敏度、全场检测能力的无损检测方法，可用于检测表面下缺陷。然而，由于其需要专家解读，在工业应用中的推广受到限制。&lt;h4&gt;目的&lt;/h4&gt;减少对标记数据和人工评估的依赖，探索无监督学习方法用于剪切散斑图像中的自动异常检测，提高工业应用中的检测效率和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;评估了三种架构：全连接自编码器、卷积自编码器和师生特征匹配模型。所有模型仅使用无缺陷数据进行训练。开发了一个使用具有可重复缺陷模式的自定义试样的受控数据集，定义了两个训练子集：一个只包含无畸变、无缺陷样本，另一个额外包含全局变形但无缺陷的数据。评估包括二元分类和空间缺陷定位。&lt;h4&gt;主要发现&lt;/h4&gt;师生特征匹配方法实现了卓越的分类鲁棒性和精确的定位能力。与自编码器模型相比，它表现出更好的特征表示可分性，通过t-SNE嵌入可视化。使用YOLOv8作为参考基准验证了定位质量。&lt;h4&gt;结论&lt;/h4&gt;无监督深度学习在工业环境中具有可扩展性和标签效率，为剪切散斑检测提供了有前景的解决方案，特别是在减少对专家解读的依赖方面。&lt;h4&gt;翻译&lt;/h4&gt;剪切散斑测量是一种用于检测表面下缺陷的无损检测方法，具有高灵敏性和全场检测能力。然而，由于其需要专家解读，在工业应用中的推广仍然有限。为了减少对标记数据和人工评估的依赖，本研究探索了无监督学习方法用于剪切散斑图像中的自动异常检测。评估了三种架构：全连接自编码器、卷积自编码器和师生特征匹配模型。所有模型仅使用无缺陷数据进行训练。开发了一个使用具有可重复缺陷模式的自定义试样的受控数据集，实现了在理想和现实变形条件下系统获取剪切散斑测量。定义了两个训练子集：一个只包含无畸变、无缺陷样本，另一个额外包含全局变形但无缺陷的数据。后者通过包含可能掩盖局部异常的变形引起的条纹图案来模拟实际检测条件。从二元分类和师生模型的缺陷定位方面评估了模型。结果表明，师生方法实现了卓越的分类鲁棒性和精确的定位能力。与基于自编码器的模型相比，它表现出更好的特征表示可分性，通过t-SNE嵌入可视化。此外，在标记缺陷数据上训练的YOLOv8模型作为定位质量的参考基准。这项研究强调了无监督深度学习在工业环境中可扩展、标签高效的剪切散斑检测的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Shearography is a non-destructive testing method for detecting subsurfacedefects, offering high sensitivity and full-field inspection capabilities.However, its industrial adoption remains limited due to the need for expertinterpretation. To reduce reliance on labeled data and manual evaluation, thisstudy explores unsupervised learning methods for automated anomaly detection inshearographic images. Three architectures are evaluated: a fully connectedautoencoder, a convolutional autoencoder, and a student-teacher featurematching model. All models are trained solely on defect-free data. A controlleddataset was developed using a custom specimen with reproducible defectpatterns, enabling systematic acquisition of shearographic measurements underboth ideal and realistic deformation conditions. Two training subsets weredefined: one containing only undistorted, defect-free samples, and oneadditionally including globally deformed, yet defect-free, data. The lattersimulates practical inspection conditions by incorporating deformation-inducedfringe patterns that may obscure localized anomalies. The models are evaluatedin terms of binary classification and, for the student-teacher model, spatialdefect localization. Results show that the student-teacher approach achievessuperior classification robustness and enables precise localization. Comparedto the autoencoder-based models, it demonstrates improved separability offeature representations, as visualized through t-SNE embeddings. Additionally,a YOLOv8 model trained on labeled defect data serves as a reference tobenchmark localization quality. This study underscores the potential ofunsupervised deep learning for scalable, label-efficient shearographicinspection in industrial environments.</description>
      <author>example@mail.com (Jessica Plassmann, Nicolas Schuler, Georg von Freymann, Michael Schuth)</author>
      <guid isPermaLink="false">2511.02541v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Phenotype Discovery in Electronic Health Records through Prior Knowledge-Guided Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2511.02102v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to JAMIA; preprint is the author's original version. Github  repo: https://github.com/mm4963/prior-guided-EHR-phenotyping/tree/main&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种结合领域特定知识的贝叶斯潜在类别框架，用于改进基于电子健康记录的无监督学习表型发现方法。通过信息先验引导聚类向临床相关亚组发展，在哮喘患者数据中成功识别出与2型炎症特征相关的'T2高炎症'亚型。&lt;h4&gt;背景&lt;/h4&gt;传统的基于电子健康记录的无监督学习方法在表型发现方面显示出前景，但这些方法通常忽略了现有的临床信息，限制了结果的可解释性。在缺乏明确表型定义的异质性疾病研究中，需要能够整合临床知识的方法。&lt;h4&gt;目的&lt;/h4&gt;旨在通过整合领域特定知识来提高EHR衍生表型的临床意义，并展示该方法在识别与2型炎症特征相关的哮喘亚型方面的效用。&lt;h4&gt;方法&lt;/h4&gt;开发了一个框架，通过信息先验将临床知识整合到贝叶斯潜在类别模型中，引导无监督聚类向临床相关的亚组方向发展。该方法能够建模缺失值，考虑潜在的随机缺失模式，并提供患者级别的表型分配概率及其不确定性。研究者在包含44,642名成年哮喘患者的大型哮喘EHR队列中应用了该模型，为2型炎症相关特征指定了信息先验，为其他临床变量指定了弱信息先验。&lt;h4&gt;主要发现&lt;/h4&gt;使用2017年1月至2024年2月的就诊数据，研究者发现了表型分配的双峰后验分布，表明存在明显的类别分离。T2炎症信息类(38.7%)的特征是嗜酸性粒细胞水平和过敏标志物升高，以及高医疗利用率和药物使用，尽管后者的变量只有弱信息先验。这些模式表明存在一个'未控制的T2高'亚型。&lt;h4&gt;结论&lt;/h4&gt;贝叶斯潜在类别建模方法支持在缺乏明确表型定义的异质性疾病研究中进行假设生成和队列识别，展示了如何通过整合临床知识提高EHR数据分析的临床相关性。&lt;h4&gt;翻译&lt;/h4&gt;目标：利用电子健康记录数据进行无监督学习在表型发现方面显示出前景，但通常方法忽略了现有的临床信息，限制了可解释性。我们将贝叶斯潜在类别框架具体化用于表型分析，整合领域特定知识以提高EHR衍生表型的临床意义，并通过识别与2型炎症特征相关的哮喘亚型来说明其效用。材料与方法：我们通过信息先验将临床知识整合到贝叶斯潜在类别模型中，引导无监督聚类向临床相关的亚组方向发展。该方法对缺失值进行建模，考虑潜在的随机缺失模式，并提供患者级别的表型分配概率及其不确定性。使用可重用且灵活的代码，我们将该模型应用于大型哮喘EHR队列，为T2炎症相关特征指定信息先验，为其他临床变量指定弱信息先验，让数据 informing 后验分布。结果与结论：使用2017年1月至2024年2月间44,642名成年哮喘患者的就诊数据，我们发现表型分配的后验分布呈双峰，表明存在明显的类别分离。T2炎症信息类(38.7%)的特征是嗜酸性粒细胞水平和过敏标志物升高，以及高医疗利用率和药物使用，尽管后者的变量只有弱信息先验。这些模式表明存在一个'未控制的T2高'亚型。这展示了我们的贝叶斯潜在类别建模方法如何支持在缺乏明确表型定义的异质性疾病EHR研究中进行假设生成和队列识别。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Objectives: Unsupervised learning with electronic health record (EHR) datahas shown promise for phenotype discovery, but approaches typically disregardexisting clinical information, limiting interpretability. We operationalize aBayesian latent class framework for phenotyping that incorporatesdomain-specific knowledge to improve clinical meaningfulness of EHR-derivedphenotypes and illustrate its utility by identifying an asthma sub-phenotypeinformed by features of Type 2 (T2) inflammation.  Materials and methods: We illustrate a framework for incorporating clinicalknowledge into a Bayesian latent class model via informative priors to guideunsupervised clustering toward clinically relevant subgroups. This approachmodels missingness, accounting for potential missing-not-at-random patterns,and provides patient-level probabilities for phenotype assignment withuncertainty. Using reusable and flexible code, we applied the model to a largeasthma EHR cohort, specifying informative priors for T2 inflammation-relatedfeatures and weakly informative priors for other clinical variables, allowingthe data to inform posterior distributions.  Results and Conclusion: Using encounter data from January 2017 to February2024 for 44,642 adult asthma patients, we found a bimodal posteriordistribution of phenotype assignment, indicating clear class separation. The T2inflammation-informed class (38.7%) was characterized by elevated eosinophillevels and allergy markers, plus high healthcare utilization and medicationuse, despite weakly informative priors on the latter variables. These patternssuggest an "uncontrolled T2-high" sub-phenotype. This demonstrates how ourBayesian latent class modeling approach supports hypothesis generation andcohort identification in EHR-based studies of heterogeneous diseases withoutwell-established phenotype definitions.</description>
      <author>example@mail.com (Melanie Mayer, Kimberly Lactaoen, Gary E. Weissman, Blanca E. Himes, Rebecca A. Hubbard)</author>
      <guid isPermaLink="false">2511.02102v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Machine and Deep Learning for Indoor UWB Jammer Localization</title>
      <link>http://arxiv.org/abs/2511.01819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 20th International Conference on Risks and Security  of Internet and Systems (CRiSIS 2025, Gatineau-Canada,  https://crisis2025.uqo.ca/). The paper will soon be published as  post-proceedings in Springer's LNCS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了超宽带(UWB)定位系统易受干扰攻击的问题，提出了一种域对抗ConvNeXt自编码器(A-CNT)方法，能够在室内布局变化的情况下实现鲁棒的干扰器定位。&lt;h4&gt;背景&lt;/h4&gt;超宽带(UWB)定位能提供厘米级精度，但容易受到干扰攻击，对智能建筑中的资产跟踪和入侵检测构成安全风险。尽管机器学习和深度学习方法已改进标签定位，但在单个房间内定位恶意干扰器以及应对变化的室内布局方面研究不足。&lt;h4&gt;目的&lt;/h4&gt;研究如何在室内布局变化的情况下实现鲁棒的干扰器定位，解决域偏移问题。&lt;h4&gt;方法&lt;/h4&gt;引入两个新的UWB数据集（原始和修改后的房间配置），建立全面的ML/DL基线，使用多种分类和回归指标评估性能。提出域对抗ConvNeXt自编码器(A-CNT)，利用梯度反转层对齐跨域的CIR衍生特征。&lt;h4&gt;主要发现&lt;/h4&gt;在源数据集上，随机森林达到最高的F1-macro分数0.95，XGBoost达到最低的平均欧几里得误差20.16厘米。但在修改后的房间布局中，XGBoost的平均误差增加十倍至207.99厘米。A-CNT框架将平均误差降低到34.67厘米，比非对抗性迁移学习提高77%，比最佳基线提高83%，使30厘米内的样本比例恢复到0.56。&lt;h4&gt;结论&lt;/h4&gt;对抗特征对齐使得尽管环境变化，室内干扰器定位能够保持鲁棒性和可转移性。&lt;h4&gt;翻译&lt;/h4&gt;超宽带(UWB)定位提供厘米级精度但容易受到干扰攻击，对智能建筑中的资产跟踪和入侵检测构成安全风险。虽然机器学习(ML)和深度学习(DL)方法已改进标签定位，但在单个房间内定位恶意干扰器以及应对变化的室内布局方面 largely unexplored。研究引入两个新的UWB数据集，分别在原始和修改后的房间配置下收集，建立全面的ML/DL基线。使用多种分类和回归指标严格评估性能。在源数据集上，随机森林达到最高的F1-macro分数0.95，XGBoost达到最低的平均欧几里得误差20.16厘米。但在修改后的房间布局中部署源训练的模型导致性能严重下降，XGBoost的平均误差增加十倍至207.99厘米，显示出显著的域偏移。为缓解这种退化，提出域对抗ConvNeXt自编码器(A-CNT)，利用梯度反转层对齐跨域的CIR衍生特征。A-CNT框架通过将平均误差降低到34.67厘米恢复定位性能，比非对抗性迁移学习提高77%，比最佳基线提高83%，使30厘米内的样本比例恢复到0.56。总体结果表明，尽管环境变化，对抗特征对齐 enables robust and transferable indoor jammer localization。代码和数据集可在 https://github.com/afbf4c8996f/Jammer-Loc 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ultra-wideband (UWB) localization delivers centimeter-scale accuracy but isvulnerable to jamming attacks, creating security risks for asset tracking andintrusion detection in smart buildings. Although machine learning (ML) and deeplearning (DL) methods have improved tag localization, localizing maliciousjammers within a single room and across changing indoor layouts remains largelyunexplored. Two novel UWB datasets, collected under original and modified roomconfigurations, are introduced to establish comprehensive ML/DL baselines.Performance is rigorously evaluated using a variety of classification andregression metrics. On the source dataset with the collected UWB features,Random Forest achieves the highest F1-macro score of 0.95 and XGBoost achievesthe lowest mean Euclidean error of 20.16 cm. However, deploying thesesource-trained models in the modified room layout led to severe performancedegradation, with XGBoost's mean Euclidean error increasing tenfold to 207.99cm, demonstrating significant domain shift. To mitigate this degradation, adomain-adversarial ConvNeXt autoencoder (A-CNT) is proposed that leverages agradient-reversal layer to align CIR-derived features across domains. The A-CNTframework restores localization performance by reducing the mean Euclideanerror to 34.67 cm. This represents a 77 percent improvement overnon-adversarial transfer learning and an 83 percent improvement over the bestbaseline, restoring the fraction of samples within 30 cm to 0.56. Overall, theresults demonstrate that adversarial feature alignment enables robust andtransferable indoor jammer localization despite environmental changes. Code anddataset available at https://github.com/afbf4c8996f/Jammer-Loc</description>
      <author>example@mail.com (Hamed Fard, Mahsa Kholghi, Benedikt Groß, Gerhard Wunder)</author>
      <guid isPermaLink="false">2511.01819v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications</title>
      <link>http://arxiv.org/abs/2511.01745v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OSBAD是一个开源基准测试平台，用于电池应用中的异常检测框架，通过测试15种不同算法实现异常检测方法的系统性比较，提出的特征转换工作流程和贝叶斯优化管道提高了异常检测性能，具有跨化学泛化能力，为电池安全分析提供了重要工具和方法。&lt;h4&gt;背景&lt;/h4&gt;电池安全在从消费电子产品到电动汽车和飞机的各种应用中至关重要，未被发现的异常可能引发安全隐患或导致昂贵的停机时间。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为OSBAD的开源基准测试平台，用于电池应用中的异常检测框架，实现不同数据集上异常检测方法的系统性比较。&lt;h4&gt;方法&lt;/h4&gt;对15种不同的算法进行基准测试，包括统计方法、基于距离的方法和无监督机器学习方法；展示基于物理和统计信息的特征转换工作流程，通过将集体异常分解为点异常提高异常可分离性；提出贝叶斯优化管道，基于迁移学习和回归代理实现自动超参数调优。&lt;h4&gt;主要发现&lt;/h4&gt;通过涵盖液态和固态化学成分的数据集验证，证明了OSBAD的跨化学泛化能力，能够识别不同电化学系统中的异常情况；物理和统计信息驱动的特征工程以及概率超参数调优的模型选择对推进关键能源系统的可信数据驱动诊断具有重要意义。&lt;h4&gt;结论&lt;/h4&gt;通过向社区提供开源可复现的异常检测工作流程的基准测试数据库，OSBAD为开发安全、可扩展和可转移的电池分析异常检测工具建立了统一基础；强调了物理和统计信息驱动的特征工程以及概率超参数调优的模型选择在推进关键能源系统可信数据驱动诊断中的重要性。&lt;h4&gt;翻译&lt;/h4&gt;电池安全在从消费电子产品到电动汽车和飞机的各种应用中至关重要，未被发现的异常可能引发安全隐患或导致昂贵的停机时间。在本研究中，我们提出了OSBAD作为电池应用中异常检测框架的开源基准。通过对涵盖统计、基于距离和无监督机器学习方法在内的15种不同算法进行基准测试，OSBAD实现了跨异构数据集异常检测方法的系统性比较。此外，我们展示了如何通过基于物理和统计信息的特征转换工作流程，通过将集体异常分解为点异常来提高异常可分离性。为解决无监督异常检测中因标签不完整导致的主要瓶颈，我们提出了一种基于迁移学习和回归代理的贝叶斯优化管道，实现自动超参数调优。通过在涵盖液态和固态化学成分的数据集上进行验证，我们进一步证明了OSBAD的跨化学泛化能力，能够识别不同电化学系统中的不规则性。通过向社区提供包含开源可复现异常检测工作流程的基准测试数据库，OSBAD为开发电池分析中安全、可扩展和可转移的异常检测工具建立了统一基础。这项研究强调了物理和统计信息驱动的特征工程以及概率超参数调优的模型选择在推进关键能源系统可信数据驱动诊断中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Battery safety is critical in applications ranging from consumer electronicsto electric vehicles and aircraft, where undetected anomalies could triggersafety hazards or costly downtime. In this study, we present OSBAD as anopen-source benchmark for anomaly detection frameworks in battery applications.By benchmarking 15 diverse algorithms encompassing statistical, distance-based,and unsupervised machine-learning methods, OSBAD enables a systematiccomparison of anomaly detection methods across heterogeneous datasets. Inaddition, we demonstrate how a physics- and statistics-informed featuretransformation workflow enhances anomaly separability by decomposing collectiveanomalies into point anomalies. To address a major bottleneck in unsupervisedanomaly detection due to incomplete labels, we propose a Bayesian optimizationpipeline that facilitates automated hyperparameter tuning based ontransfer-learning and regression proxies. Through validation on datasetscovering both liquid and solid-state chemistries, we further demonstrate thecross-chemistry generalization capability of OSBAD to identify irregularitiesacross different electrochemical systems. By making benchmarking database withopen-source reproducible anomaly detection workflows available to thecommunity, OSBAD establishes a unified foundation for developing safe,scalable, and transferable anomaly detection tools in battery analytics. Thisresearch underscores the significance of physics- and statistics-informedfeature engineering as well as model selection with probabilistichyperparameter tuning, in advancing trustworthy, data-driven diagnostics forsafety-critical energy systems.</description>
      <author>example@mail.com (Mei-Chin Pang, Suraj Adhikari, Takuma Kasahara, Nagihiro Haba, Saneyuki Ohno)</author>
      <guid isPermaLink="false">2511.01745v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning</title>
      <link>http://arxiv.org/abs/2511.01502v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DiMoDE的深度和自我运动联合学习框架，通过区分性处理运动组件，利用各自刚性流的几何规律来改善深度和自我运动估计，在多个数据集上取得了最先进的表现。&lt;h4&gt;背景&lt;/h4&gt;无监督学习深度和自我运动这两个基础3D感知任务近年来取得了显著进展。然而，大多数方法将自我运动视为辅助任务，要么混合所有运动类型，要么在监督中排除与深度无关的旋转运动，限制了强几何约束的融入，降低了在多样化条件下的可靠性和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;引入对运动组件的区分性处理，利用各自刚性流的几何规律来改善深度和自我运动估计，提高系统在多样化条件下的可靠性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;给定连续视频帧，网络首先对齐源相机和目标相机的光轴和成像平面。帧之间的光流通过这些对齐进行变换，并量化偏差以对每个自我运动组件单独施加几何约束。这些对齐进一步将联合学习过程重新表述为共轴和共面形式，其中深度和每个平移分量可以通过闭合形式的几何关系相互推导。&lt;h4&gt;主要发现&lt;/h4&gt;通过区分性处理运动组件并利用各自刚性流的几何规律，DiMoDE框架在多个公共数据集和新收集的多样化真实世界数据集上实现了最先进的表现，特别是在具有挑战性的条件下表现优异。&lt;h4&gt;结论&lt;/h4&gt;DiMoDE是一种通用的深度和自我运动联合学习框架，通过区分性处理运动组件和利用几何约束，显著提高了深度和自我运动估计的准确性和鲁棒性，特别是在具有挑战性的条件下。&lt;h4&gt;翻译&lt;/h4&gt;无监督学习深度和自我运动这两个基础3D感知任务近年来取得了显著进展。然而，大多数方法将自我运动视为辅助任务，要么混合所有运动类型，要么在监督中排除与深度无关的旋转运动。这种设计限制了强几何约束的融入，降低了在多样化条件下的可靠性和鲁棒性。本研究引入了对运动组件的区分性处理，利用各自刚性流的几何规律来改善深度和自我运动估计。给定连续视频帧，网络首先对齐源相机和目标相机的光轴和成像平面。帧之间的光流通过这些对齐进行变换，并量化偏差以对每个自我运动组件单独施加几何约束，实现更精细的优化。这些对齐进一步将联合学习过程重新表述为共轴和共面形式，其中深度和每个平移分量可以通过闭合形式的几何关系相互推导，引入互补约束以提高深度鲁棒性。DiMoDE是一种融合了这些设计的通用深度和自我运动联合学习框架，在多个公共数据集和新收集的多样化真实世界数据集上取得了最先进的表现，特别是在具有挑战性的条件下。我们的源代码将在发表后于mias.group/DiMoDE公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决无监督学习深度和自运动估计中运动组件处理不当的问题。现有方法要么不加区分地混合所有运动类型，要么排除旋转运动，阻碍了强几何约束的融入，限制了模型在复杂环境下的可靠性。这个问题很重要，因为深度和自运动估计是3D感知的基础，在自动驾驶、机器人导航等领域有广泛应用，而更准确鲁棒的估计能提高系统在各种实际环境中的性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析不同运动类型（旋转、切向平移、径向平移）产生的刚性流动差异，发现旋转产生不规则深度无关流动，而平移产生规则但不同的深度相关流动。现有方法混合这些流动导致监督信号质量下降。作者借鉴了现有无监督学习方法（如SfMLearner、Monodepth2）使用光流作为监督信号的思想，以及相机校准和几何变换原理，但创新性地应用于区分不同运动组件的处理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是区分性处理运动组件，利用每种组件固有的几何规律性：旋转产生不规则流动，切向平移产生平行流动，径向平移产生朝向/远离主点的流动。实现流程包括：1)将自运动分解为旋转、切向平移和径向平移；2)用旋转对齐光轴消除不规则流动；3)用平移对齐成像平面生成规则流动；4)施加双重几何约束（优化PoseNet和DepthNet）；5)整合所有约束进行联合训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次明确区分三种运动组件并分别处理；2)通过光轴和成像平面对齐将学习重新表述为同轴和共面形式；3)引入双重几何约束分别优化PoseNet和DepthNet；4)提出兼容多种架构的DiMoDE统一框架。相比之前工作，本文不混合处理所有运动类型或完全排除旋转，而是区分切向和径向平移，引入更高层次的几何约束而非仅依赖像素级光度一致性，显著提高了模型在复杂环境下的鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过区分性处理运动组件并引入双重几何约束，提出了一种改进的无监督深度和自运动联合学习框架DiMoDE，显著提高了模型在不同环境条件下的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised learning of depth and ego-motion, two fundamental 3D perceptiontasks, has made significant strides in recent years. However, most methodstreat ego-motion as an auxiliary task, either mixing all motion types orexcluding depth-independent rotational motions in supervision. Such designslimit the incorporation of strong geometric constraints, reducing reliabilityand robustness under diverse conditions. This study introduces a discriminativetreatment of motion components, leveraging the geometric regularities of theirrespective rigid flows to benefit both depth and ego-motion estimation. Givenconsecutive video frames, network outputs first align the optical axes andimaging planes of the source and target cameras. Optical flows between framesare transformed through these alignments, and deviations are quantified toimpose geometric constraints individually on each ego-motion component,enabling more targeted refinement. These alignments further reformulate thejoint learning process into coaxial and coplanar forms, where depth and eachtranslation component can be mutually derived through closed-form geometricrelationships, introducing complementary constraints that improve depthrobustness. DiMoDE, a general depth and ego-motion joint learning frameworkincorporating these designs, achieves state-of-the-art performance on multiplepublic datasets and a newly collected diverse real-world dataset, particularlyunder challenging conditions. Our source code will be publicly available atmias.group/DiMoDE upon publication.</description>
      <author>example@mail.com (Mengtan Zhang, Zizhan Guo, Hongbo Zhao, Yi Feng, Zuyi Xiong, Yue Wang, Shaoyi Du, Hanli Wang, Rui Fan)</author>
      <guid isPermaLink="false">2511.01502v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>A Soft-partitioned Semi-supervised Collaborative Transfer Learning Approach for Multi-Domain Recommendation</title>
      <link>http://arxiv.org/abs/2511.01404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对多领域推荐系统中的数据不平衡问题提出了解决方案，通过软分区半监督协同迁移学习(SSCTL)方法显著提升了推荐效果。&lt;h4&gt;背景&lt;/h4&gt;多领域推荐(MDR)在工业实践中至关重要，共享-特定架构被广泛用于捕获共享和独特属性，但不同领域间数据不平衡导致模型性能问题。&lt;h4&gt;目的&lt;/h4&gt;解决多领域推荐中因数据不平衡导致的两个关键问题：主导领域数据过多导致模型偏向，以及非主导领域数据稀疏导致过拟合。&lt;h4&gt;方法&lt;/h4&gt;提出软分区半监督协同迁移学习(SSCTL)方法，通过生成动态参数解决主导问题，利用主导领域实例的带权伪标签增强非主导领域数据以对抗过拟合。&lt;h4&gt;主要发现&lt;/h4&gt;在线实验表明，该方法在各个领域均取得显著改进，GMV增长0.54%至2.90%，CTR提升0.22%至1.69%。&lt;h4&gt;结论&lt;/h4&gt;SSCTL方法有效解决了多领域推荐中的数据不平衡问题，提升了非主导领域的推荐性能，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;在工业实践中，多领域推荐(MDR)起着关键作用。共享-特定架构在工业解决方案中被广泛使用，通过共享和特定参数来捕获共享和独特属性。然而，由于不同领域间数据不平衡，这些模型面临两个关键问题：(1)主导问题：主导领域的数据使模型性能偏向，忽略了非主导领域。(2)过拟合问题：非主导领域的数据稀疏导致特定参数过拟合。为应对这些挑战，我们提出了用于多领域推荐的软分区半监督协同迁移学习(SSCTL)。SSCTL生成动态参数来解决主导问题，从而将重点转向非主导领域的样本。为了对抗过拟合，它利用主导领域实例的带权伪标签来增强非主导领域数据。我们进行了全面的在线和离线实验来验证所提出方法的有效性。在线测试在各种领域都取得了显著改进，GMV增长了0.54%至2.90%，CTR提升了0.22%至1.69%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In industrial practice, Multi-domain Recommendation (MDR) plays a crucialrole. Shared-specific architectures are widely used in industrial solutions tocapture shared and unique attributes via shared and specific parameters.However, with imbalanced data across different domains, these models face twokey issues: (1) Overwhelming: Dominant domain data skews model performance,neglecting non-dominant domains. (2) Overfitting: Sparse data in non-dominantdomains leads to overfitting in specific parameters. To tackle thesechallenges, we propose Soft-partitioned Semi-supervised Collaborative TransferLearning (SSCTL) for multi-domain recommendation. SSCTL generates dynamicparameters to address the overwhelming issue, thus shifting focus towardssamples from non-dominant domains. To combat overfitting, it leveragespseudo-labels with weights from dominant domain instances to enhancenon-dominant domain data. We conduct comprehensive experiments, both online andoffline, to validate the efficacy of our proposed method. Online tests yieldedsignificant improvements across various domains, with increases in GMV rangingfrom 0.54% to 2.90% and enhancements in CTR ranging from 0.22% to 1.69%.</description>
      <author>example@mail.com (Xiaoyu Liu, Yiqing Wu, Ruidong Han, Fuzhen Zhuang, Xiang Li, Wei Lin)</author>
      <guid isPermaLink="false">2511.01404v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Embodiment Transfer Learning for Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2511.01224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为ET-VLA的新型框架，通过合成继续预训练(SCP)和具身思维图技术，解决了VLA模型在多机器人协作方面的挑战，显著提升了模型在多embodiment环境中的性能。&lt;h4&gt;背景&lt;/h4&gt;Vision-language-action (VLA)模型已在机器人学习领域取得显著进展，能够在大规模、跨embodiment数据上训练并针对特定机器人进行微调。然而，最先进的自回归VLA模型在多机器人协作方面表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效有效的框架，将预训练的VLA模型转移到多机器人系统，解决多机器人协作问题。&lt;h4&gt;方法&lt;/h4&gt;ET-VLA框架的核心是合成继续预训练(SCP)，使用合成生成的数据使模型适应新的embodiment，避免真实人类演示，降低数据收集成本。SCP使模型学习正确动作和精确动作令牌数量，随后在目标embodiment数据上微调。此外，提出具身思维图技术，将子任务表述为节点，使VLA模型在任务执行中区分各embodiment的功能和角色。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟基准测试和三种不同双臂机器人的真实机器人上验证了方法有效性。ET-VLA在六个真实世界任务上的表现比OpenVLA高出53.2%。&lt;h4&gt;结论&lt;/h4&gt;将开源所有代码，支持社区推进用于机器人学习的VLA模型发展。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-动作(VLA)模型显著推进了机器人学习，使能够在大规模、跨embodiment数据上训练并为特定机器人进行微调。然而，最先进的自回归VLA模型在多机器人协作方面存在困难。我们引入了embodiment迁移学习，称为ET-VLA，这是一种新型框架，用于高效有效地将预训练的VLA模型转移到多机器人系统。ET-VLA的核心是合成继续预训练(SCP)，它使用合成的生成数据来使模型适应新的embodiment，绕过对真实人类演示的需求，降低数据收集成本。SCP使模型能够学习正确的动作和精确的动作令牌数量。在SCP之后，模型在目标embodiment数据上进行微调。为了进一步提高模型在多embodiment上的性能，我们提出了具身思维图技术，一种新颖的方法，将每个子任务表述为一个节点，使VLA模型能够在任务执行过程中区分每个embodiment的功能和角色。我们的研究考虑了双臂机器人，作为多机器人的一个简单版本来验证我们的方法。我们在模拟基准测试和覆盖三种不同双臂embodiment的真实机器人上验证了我们方法的有效性。特别是，我们提出的ET-VLA在六个真实世界任务上可以比OpenVLA高出53.2%。我们将开源所有代码，支持社区推进用于机器人学习的VLA模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language-action (VLA) models have significantly advanced roboticlearning, enabling training on large-scale, cross-embodiment data andfine-tuning for specific robots. However, state-of-the-art autoregressive VLAsstruggle with multi-robot collaboration. We introduce embodiment transferlearning, denoted as ET-VLA, a novel framework for efficient and effectivetransfer of pre-trained VLAs to multi-robot. ET-VLA's core is SyntheticContinued Pretraining (SCP), which uses synthetically generated data to warm upthe model for the new embodiment, bypassing the need for real humandemonstrations and reducing data collection costs. SCP enables the model tolearn correct actions and precise action token numbers. Following SCP, themodel is fine-tuned on target embodiment data. To further enhance the modelperformance on multi-embodiment, we present the Embodied Graph-of-Thoughttechnique, a novel approach that formulates each sub-task as a node, thatallows the VLA model to distinguish the functionalities and roles of eachembodiment during task execution. Our work considers bimanual robots, a simpleversion of multi-robot to verify our approaches. We validate the effectivenessof our method on both simulation benchmarks and real robots covering threedifferent bimanual embodiments. In particular, our proposed ET-VLA \space canoutperform OpenVLA on six real-world tasks over 53.2%. We will open-source allcodes to support the community in advancing VLA models for robot learning.</description>
      <author>example@mail.com (Chengmeng Li, Yaxin Peng)</author>
      <guid isPermaLink="false">2511.01224v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>STELLAR-koff: A Transfer Learning Model for Protein-Ligand Dissociation Rate Constant Prediction Based on Interaction Landscape</title>
      <link>http://arxiv.org/abs/2511.01171v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种名为STELLAR-koff的图神经网络模型，用于预测蛋白质-配体解离速率常数，通过迁移学习将多个配体构象转化为相互作用景观，扩展了数据集，并在测试中表现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;成功的药物设计关键在于正确理解蛋白质-配体相互作用，目前已有许多预测热力学性质的深度学习模型，但缺乏成熟的预测动力学性质的模型，主要原因是缺乏动力学数据。&lt;h4&gt;目的&lt;/h4&gt;开发一个预测蛋白质-配体解离速率常数的模型，解决缺乏动力学数据的问题。&lt;h4&gt;方法&lt;/h4&gt;开发名为STELLAR-koff的图神经网络模型，使用迁移学习将蛋白质内多个配体构象转化为蛋白质-配体相互作用景观，扩展PDBbind koff数据集从680到1197个条目，通过五折交叉验证和外部集测试模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;STELLAR-koff在五折交叉验证中达到0.729的皮尔逊相关系数，性能超越或与大多数已发表的预测方法相当；在外部集上对未见蛋白质的预测表现出强大性能，特别是在粘着斑激酶上达到0.838的皮尔逊相关系数；在周期依赖性激酶上的实验验证证明了其在真实药物发现场景中的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究为预测蛋白质-配体解离速率常数提供了有效工具，为该领域的未来发展提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;成功药物设计的关键在于正确理解蛋白质-配体相互作用。在当前知识框架下，这些相互作用可以从热力学和动力学两个角度进行描述。近年来，许多深度学习模型 emerged 用于预测蛋白质-配体相互作用的热力学性质。然而，目前缺乏成熟的预测动力学性质的模型，主要原因是缺乏动力学数据。为解决这个问题，我们开发了一个名为STELLAR-koff（基于结构的迁移学习用于配体活性回归）的图神经网络模型来预测蛋白质-配体解离速率常数。与传统的蛋白质-配体性质预测模型不同，后者通常使用单一复合物构象作为输入，STELLAR-koff采用迁移学习将蛋白质内多个配体构象转化为蛋白质-配体相互作用景观，并将这种景观作为模型的主要输入。此外，我们将PDBbind koff数据集从680个扩展到1197个条目，并使用增强的数据集进行模型训练和测试。通过五折交叉验证测试时，STELLAR-koff达到0.729的皮尔逊相关系数，性能超越或与大多数已发表的预测方法相当。在外部集测试中，STELLAR-koff在对未见蛋白质的预测中表现出强大性能，特别是在粘着斑激酶上达到了0.838的皮尔逊相关系数。在周期依赖性激酶上的实验验证也证明了STELLAR-koff在真实药物发现场景中的有效性。我们相信这项研究为预测蛋白质-配体解离速率常数提供了有效工具，并为该领域的未来发展提供了新的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The key to successful drug design lies in the correct comprehension ofprotein-ligand interactions. Within the current knowledge paragm, theseinteractions can be described from both thermodynamic and kinetic perspectives.In recent years, many deep learning models have emerged for predicting thethermodynamic properties of protein-ligand interactions. However, there iscurrently no mature model for predicting kinetic properties, primarily due tolack of kinetic data. To tackle this problem, we have developed a graph neuralnetwork model called STELLAR-koff (Structure-based TransfEr Learning for LigandActivity Regression) to predict protein-ligand dissociation rate constant.Unlike traditional protein-ligand property prediction models, which typicallyuse a single complex conformation as input, STELLAR-koff employs transferlearning to transform multiple ligand conformations within the protein into aprotein ligand interaction landscape, and uses this landscape as the primaryinput for the model. In addition, we expanded the PDBbind koff dataset from 680to 1,197 entries and employed the augmented dataset for model training andtesting. When tested through five-fold cross-validation, STELLAR-koff achievedPearson correlation coefficient of 0.729 surpassing or being on pair with mostof the published prediction methods. Tested on external set, STELLAR-koffdemonstrated strong predictive performance on unseen protein, achieving aPearson of 0.838 on the focal adhesion kinase in particular. Experimentalvalidation on cyclin-dependent kinase also demonstrated the effectiveness ofSTELLAR-koff in real drug discovering scenarios. We believe this study providesan effective tool for predicting protein-ligand dissociation rate constant andoffers new insight for the future development of this field.</description>
      <author>example@mail.com (Jingyuan Li)</author>
      <guid isPermaLink="false">2511.01171v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Few-Shot Multimodal Medical Imaging: A Theoretical Framework</title>
      <link>http://arxiv.org/abs/2511.01140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 Pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的理论框架，用于描述低资源医学影像条件下的学习和推理，旨在解决医学影像领域数据稀缺问题。&lt;h4&gt;背景&lt;/h4&gt;医学影像依赖于大型标记数据集，但在临床环境中这些数据集不易获取，从业者面临数据有限、数据系统碎片化和数据集不平衡等结构障碍。&lt;h4&gt;目的&lt;/h4&gt;为数据稀缺情况下医学影像学习方法提供坚实的理论基础，解释为什么某些方法成功或失败。&lt;h4&gt;方法&lt;/h4&gt;在少样本条件下形式化学习目标并计算样本复杂度约束；基于PAC学习和PAC-Bayesian理论解释多模态集成如何促进泛化和量化不确定性；提出解释稳定性形式化度量。&lt;h4&gt;主要发现&lt;/h4&gt;多模态集成可以在数据稀缺条件下促进泛化和量化不确定性；解释稳定性度量可以为低数据条件下的可解释性提供保证。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架通过统一描述样本效率、不确定量化和可解释性，为构建可靠、数据高效的医学影像诊断系统奠定了理论基础。&lt;h4&gt;翻译&lt;/h4&gt;医学成像在很大程度上依赖于大型标记数据集。但不幸的是，在临床环境中它们并不总是容易获取。此外，许多从业者经常面临各种结构障碍，如数据可用性有限、数据系统碎片化和数据集不平衡。这些障碍通常导致诊断不确定性增加、某些疾病表现不足、模型鲁棒性降低和诊断决策有偏见。为应对这些挑战，转移学习、元学习和多模态融合等方法已取得长足进展。然而，在数据稀缺的情况下，它们成功或失败的原因仍缺乏坚实的理论依据。为解决这一差距，我们提出了一个统一的理论框架，用于描述低资源医学影像条件下的学习和推理。我们首先在少样本条件下形式化学习目标，并计算样本复杂度约束，以估计实现临床可靠精度所需的最小数据量。然后基于PAC学习和PAC-Bayesian理论的思想，我们解释多模态集成如何促进泛化，以及在稀疏监督下量化不确定性。我们进一步提出了解释稳定性的形式化度量，为低数据条件下的可解释性提供保证。总之，所提出的框架通过在统一理论设定中共同描述样本效率、不确定量化和可解释性，为构建可靠、数据高效的诊断系统奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical imaging relies heavily on large, labeled datasets. But,unfortunately, they are not always easily accessible in clinical settings.Additionally, many practitioners often face various structural obstacles likelimited data availability, fragmented data systems, and unbalanced datasets.These barriers often lead to the increased diagnostic uncertainty,underrepresentation of certain conditions, reduced model robustness, and biaseddiagnostic decisions. In response to these challenges, approaches such astransfer learning, meta-learning, and multimodal fusion have made greatstrides. However, they still need a solid theoretical justification for whythey succeed or fail in situations where data is scarce. To address this gap,we propose a unified theoretical framework that characterizes learning andinference under low-resource medical imaging conditions. We first formalize thelearning objective under few-shot conditions and compute sample complexityconstraints to estimate the smallest quantity of data needed to achieveclinically reliable accuracy. Then based on ideas from PAC-learning andPAC-Bayesian theory, we explain how multimodal integration encouragesgeneralization and quantifies uncertainty under sparse supervision. We furtherpropose a formal metric for explanation stability, offering interpretabilityguarantees under low-data conditions. Taken together, the proposed frameworkestablishes a principled foundation for constructing dependable, data-efficientdiagnostic systems by jointly characterizing sample efficiency, uncertaintyquantification, and interpretability in a unified theoretical setting.</description>
      <author>example@mail.com (Md Talha Mohsin, Ismail Abdulrashid)</author>
      <guid isPermaLink="false">2511.01140v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance</title>
      <link>http://arxiv.org/abs/2511.00543v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Lo-Hp的解耦两阶段权重生成框架，解决了生成式模型在神经网络权重生成中的过耦合和长时程问题，提高了灵活性和效率。&lt;h4&gt;背景&lt;/h4&gt;生成式模型的最新进展使得神经网络能够在不依赖基于梯度优化的情况下生成权重，但当前方法受限于过耦合和长时程问题。过耦合将权重生成与特定任务目标紧密绑定，限制了学习优化器的灵活性；长时程问题因缺乏局部约束导致推理效率低下和准确性不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有权重生成方法的过耦合和长时程问题，提高框架的灵活性和推理效率，特别是在需要频繁权重更新的任务中。&lt;h4&gt;方法&lt;/h4&gt;提出Lo-Hp框架，一个解耦的两阶段权重生成方法，通过学习各种优化策略增强灵活性。采用混合策略子轨迹平衡目标，结合在线学习和离线学习来捕获局部优化策略。&lt;h4&gt;主要发现&lt;/h4&gt;理论上证明了仅学习局部优化策略可以解决长时程问题，同时增强全局最优权重的生成。在需要频繁权重更新的任务中验证了Lo-Hp的优越准确性和推理效率。&lt;h4&gt;结论&lt;/h4&gt;Lo-Hp在迁移学习、小样本学习、领域泛化和大型语言模型适应等需要频繁权重更新的任务中表现出色，具有更高的准确性和推理效率。&lt;h4&gt;翻译&lt;/h4&gt;生成式建模的最新进展使神经网络能够在不依赖基于梯度优化的情况下生成权重。然而，当前方法受限于过耦合和长时程问题。前者将权重生成与特定任务目标紧密绑定，从而限制了学习优化器的灵活性。后者因缺乏局部约束导致推理效率低下和准确性不足。在本文中，我们提出了Lo-Hp，一个解耦的两阶段权重生成框架，通过学习各种优化策略来提高灵活性。它采用混合策略子轨迹平衡目标，结合在线学习和离线学习来捕获局部优化策略。理论上，我们证明了仅学习局部优化策略可以解决长时程问题，同时增强全局最优权重的生成。此外，我们在需要频繁权重更新的任务（如迁移学习、小样本学习、领域泛化和大型语言模型适应）中验证了Lo-Hp的优越准确性和推理效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in generative modeling enable neural networks to generateweights without relying on gradient-based optimization. However, currentmethods are limited by issues of over-coupling and long-horizon. The formertightly binds weight generation with task-specific objectives, thereby limitingthe flexibility of the learned optimizer. The latter leads to inefficiency andlow accuracy during inference, caused by the lack of local constraints. In thispaper, we propose Lo-Hp, a decoupled two-stage weight generation framework thatenhances flexibility through learning various optimization policies. It adoptsa hybrid-policy sub-trajectory balance objective, which integrates on-policyand off-policy learning to capture local optimization policies. Theoretically,we demonstrate that learning solely local optimization policies can address thelong-horizon issue while enhancing the generation of global optimal weights. Inaddition, we validate Lo-Hp's superior accuracy and inference efficiency intasks that require frequent weight updates, such as transfer learning, few-shotlearning, domain generalization, and large language model adaptation.</description>
      <author>example@mail.com (Yunchuan Guan, Yu Liu, Ke Zhou, Hui Li, Sen Jia, Zhiqi Shen, Ziyang Wang, Xinglin Zhang, Tao Chen, Jenq-Neng Hwang, Lei Li)</author>
      <guid isPermaLink="false">2511.00543v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation</title>
      <link>http://arxiv.org/abs/2511.00357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work was presented at the TerraBytes Workshop at the 42nd  International Conference on Machine Learning. This version is not part of the  official ICML proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了CubeSat任务中热红外云分割的挑战，通过迁移学习和轻量级架构实现了准确、高效的在轨云分割，支持实时决策。&lt;h4&gt;背景&lt;/h4&gt;CubeSat任务在热红外地球观测中的云分割是一个关键但研究不足的任务。这些卫星受限于硬件，通常只能依赖单一热红外波段，且缺乏足够的标记数据，使得传统云掩膜技术不可行。&lt;h4&gt;目的&lt;/h4&gt;解决CubeSat任务中热红外云分割的挑战，通过迁移学习方法实现准确、高效的热红外云分割，支持实时决策。&lt;h4&gt;方法&lt;/h4&gt;将迁移学习应用于FOREST-2 CubeSat的热红外云分割任务，使用带有轻量级MobileNet编码器的UNet模型。在公共的Landsat-7云覆盖评估数据集上预训练模型，然后在联合训练设置中使用少量任务特定样本进行微调。最后将模型转换为TensorRT引擎，在NVIDIA Jetson Nano上实现全图像推理。&lt;h4&gt;主要发现&lt;/h4&gt;通过迁移学习方法，宏F1分数从仅使用FOREST-2基线的0.850提高到0.877。利用公共数据集和轻量级架构可以在轨上实现准确、高效的热红外云分割，全图像推理时间不到5秒。&lt;h4&gt;结论&lt;/h4&gt;利用公共数据集和轻量级架构可以在轨上实现准确、高效的热红外云分割，支持数据受限地球观测任务中的实时决策。&lt;h4&gt;翻译&lt;/h4&gt;机载云分割是热红外地球观测中的一个关键但研究不足的任务，特别是对于受限于有限硬件和光谱信息的CubeSat任务。CubeSat通常依赖单一热红外波段，且缺乏足够的标记数据，这使得传统的云掩膜技术不可行。这项工作通过将迁移学习应用于FOREST-2 CubeSat的热红外云分割来解决这些挑战，使用带有轻量级MobileNet编码器的UNet模型。我们在公共的Landsat-7云覆盖评估数据集上预训练模型，然后在联合训练设置中使用少量任务特定样本进行微调，使宏F1分数从仅使用FOREST-2基线的0.850提高到0.877。我们将模型转换为TensorRT引擎，并在NVIDIA Jetson Nano上演示了全图像推理，时间不到5秒。这些结果表明，利用公共数据集和轻量级架构可以在轨上实现准确、高效的热红外云分割，支持数据受限地球观测任务中的实时决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Onboard cloud segmentation is a critical yet underexplored task in thermalEarth observation (EO), particularly for CubeSat missions constrained bylimited hardware and spectral information. CubeSats often rely on a singlethermal band and lack sufficient labeled data, making conventional cloudmasking techniques infeasible. This work addresses these challenges by applyingtransfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, usinga UNet with a lightweight MobileNet encoder. We pretrain the model on thepublic Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a smallset of mission-specific samples in a joint-training setup, improving the macroF1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to aTensorRT engine and demonstrate full-image inference in under 5 seconds on anNVIDIA Jetson Nano. These results show that leveraging public datasets andlightweight architectures can enable accurate, efficient thermal-only cloudmasking on-orbit, supporting real-time decision-making in data-limited EOmissions.</description>
      <author>example@mail.com (Niklas Wölki, Lukas Kondmann, Christian Mollière, Martin Langer, Julia Gottfriedsen, Martin Werner)</author>
      <guid isPermaLink="false">2511.00357v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture</title>
      <link>http://arxiv.org/abs/2511.00269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合冻结的CLIP视觉变换器和轻量级变换器分类器的联邦学习框架，用于解决智能农业中的准确分类问题，同时处理隐私保护和非IID数据分布的挑战。&lt;h4&gt;背景&lt;/h4&gt;准确分类在智能农业中扮演关键角色，但传统集中式训练引发隐私问题，标准联邦学习难以处理非IID数据且通信成本高。&lt;h4&gt;目的&lt;/h4&gt;开发一种联邦学习框架，解决数据隐私和非IID数据分布问题，同时降低通信成本。&lt;h4&gt;方法&lt;/h4&gt;利用预训练的CLIP ViT特征提取能力，避免从头训练大规模模型；将联邦更新限制在紧凑分类器上减少传输开销；共享1%的CLIP特征表示对齐不同参与者的类别表示，同时保护隐私。&lt;h4&gt;主要发现&lt;/h4&gt;在农业分类任务上达到86.6%的准确率，比基线联邦学习方法高出4倍以上。&lt;h4&gt;结论&lt;/h4&gt;结合视觉-语言模型特征与联邦学习能有效实现隐私保护和可扩展的农业智能。&lt;h4&gt;翻译&lt;/h4&gt;准确的分类在智能农业中起着关键作用，支持作物监测、果实识别和病虫害检测等应用。然而，传统的集中式训练通常需要大规模数据收集，这引发了隐私问题，而标准的联邦学习难以处理非独立同分布数据并产生高通信成本。为解决这些挑战，我们提出了一种结合冻结的对比语言-图像预训练视觉变换器和轻量级变换器分类器的联邦学习框架。通过利用预训练的CLIP视觉变换器的强大特征提取能力，该框架避免了从头开始训练大规模模型，并将联邦更新限制在紧凑的分类器上，从而显著减少了传输开销。此外，为了减轻由非IID数据分布导致的性能下降，从所有类别中共享一小部分(1%)的CLIP提取的特征表示。这些共享的特征无法逆向还原为原始图像，确保了隐私保护，同时使不同参与者的类别表示保持一致。在农业分类任务上的实验结果表明，所提出的方法达到了86.6%的准确率，比基线联邦学习方法高出4倍以上。这证明了将视觉-语言模型特征与联邦学习相结合的有效性和效率，为隐私保护和可扩展的农业智能提供了新方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate classification plays a pivotal role in smart agriculture, enablingapplications such as crop monitoring, fruit recognition, and pest detection.However, conventional centralized training often requires large-scale datacollection, which raises privacy concerns, while standard federated learningstruggles with non-independent and identically distributed (non-IID) data andincurs high communication costs. To address these challenges, we propose afederated learning framework that integrates a frozen ContrastiveLanguage-Image Pre-training (CLIP) vision transformer (ViT) with a lightweighttransformer classifier. By leveraging the strong feature extraction capabilityof the pre-trained CLIP ViT, the framework avoids training large-scale modelsfrom scratch and restricts federated updates to a compact classifier, therebyreducing transmission overhead significantly. Furthermore, to mitigateperformance degradation caused by non-IID data distribution, a small subset(1%) of CLIP-extracted feature representations from all classes is sharedacross clients. These shared features are non-reversible to raw images,ensuring privacy preservation while aligning class representation acrossparticipants. Experimental results on agricultural classification tasks showthat the proposed method achieve 86.6% accuracy, which is more than 4 timeshigher compared to baseline federated learning approaches. This demonstratesthe effectiveness and efficiency of combining vision-language model featureswith federated learning for privacy-preserving and scalable agriculturalintelligence.</description>
      <author>example@mail.com (Long Li, Jiajia Li, Dong Chen, Lina Pu, Haibo Yao, Yanbo Huang)</author>
      <guid isPermaLink="false">2511.00269v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Melanoma Classification Through Deep Ensemble Learning and Explainable AI</title>
      <link>http://arxiv.org/abs/2511.00246v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Publisher-formatted version provided under CC BY-NC-ND 4.0 license.  Original source produced by SciTePress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合可解释人工智能技术的机器学习模型，用于黑色素瘤的早期检测，以提高诊断的可靠性和信任度。&lt;h4&gt;背景&lt;/h4&gt;黑色素瘤是最具侵袭性和致命性的皮肤癌之一，如果在早期不被发现和治疗会导致死亡。人工智能技术，特别是深度学习已被开发用于帮助皮肤科医生早期检测黑色素瘤，并取得了高准确率。然而，深度学习模型的黑盒操作导致缺乏可靠性和信任度。&lt;h4&gt;目的&lt;/h4&gt;开发一种可靠的机器学习模型，通过集成学习和可解释人工智能技术来提高黑色素瘤早期检测的准确性和可信度。&lt;h4&gt;方法&lt;/h4&gt;提出了一种机器学习模型，使用三种最先进的深度迁移学习网络的集成学习，并结合可解释人工智能技术来解释预测的基础。&lt;h4&gt;主要发现&lt;/h4&gt;通过集成三种最先进的深度迁移学习网络和可解释人工智能技术，可以提高黑色素瘤检测的准确性和可靠性。&lt;h4&gt;结论&lt;/h4&gt;可解释人工智能技术可以解决深度学习模型黑盒操作导致的可靠性和信任度问题，为医疗诊断领域提供更可靠的AI辅助诊断工具。&lt;h4&gt;翻译&lt;/h4&gt;黑色素瘤是最具侵袭性和致命性的皮肤癌之一，如果在早期不被发现和治疗会导致死亡。人工智能技术最近已被开发用于帮助皮肤科医生早期检测黑色素瘤，基于深度学习的系统能够以高准确率检测这些病变。然而，整个社区必须克服可解释性的限制，才能在医疗诊断领域从深度学习中获得最大收益。由于深度学习模型决策中的黑盒操作缺陷，结果缺乏可靠性和信任度。然而，可解释人工智能(XAI)可以通过解释AI系统的预测来解决这个问题。本文提出了一种机器学习模型，使用三种最先进的深度迁移学习网络的集成学习，并利用XAI技术解释预测的基础，确保预测的可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.5220/0012575400003657&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Melanoma is one of the most aggressive and deadliest skin cancers, leading tomortality if not detected and treated in the early stages. Artificialintelligence techniques have recently been developed to help dermatologists inthe early detection of melanoma, and systems based on deep learning (DL) havebeen able to detect these lesions with high accuracy. However, the entirecommunity must overcome the explainability limit to get the maximum benefitfrom DL for diagnostics in the healthcare domain. Because of the black boxoperation's shortcomings in DL models' decisions, there is a lack ofreliability and trust in the outcomes. However, Explainable ArtificialIntelligence (XAI) can solve this problem by interpreting the predictions of AIsystems. This paper proposes a machine learning model using ensemble learningof three state-of-the-art deep transfer Learning networks, along with anapproach to ensure the reliability of the predictions by utilizing XAItechniques to explain the basis of the predictions.</description>
      <author>example@mail.com (Wadduwage Shanika Perera, ABM Islam, Van Vung Pham, Min Kyung An)</author>
      <guid isPermaLink="false">2511.00246v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals</title>
      <link>http://arxiv.org/abs/2511.00211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种高效的迁移学习方法，用于检测地面终端组件上的天气相关条件，如积雪和潮湿等，以提高低地球轨道卫星互联网系统在恶劣天气条件下的性能和可靠性。该方法在检测多种天气条件下表现出色，且具有很好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;随着低地球轨道卫星星座在卫星互联网中的广泛应用，为农村和偏远地区提供了无处不在的连接能力。然而，天气事件对卫星互联网的性能和可靠性有显著影响。雪、雨等不良天气会严重干扰卫星天线等关键地面终端组件的性能，破坏LEO卫星与地面站之间的空间-地面链路条件。&lt;h4&gt;目的&lt;/h4&gt;研究需要基于地区的天气预报以及对地面终端组件上精细化天气条件的检测能力，以支持卫星互联网的故障诊断和缓解，确保系统的可靠性。然而，目前缺乏有效的解决方案，且在实际部署中需要考虑解决方案的有效性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;论文讨论了一种高效的迁移学习(TL)方法，使地面组件能够本地检测代表性的天气相关条件。该方法可以检测由不良和典型天气事件引起的积雪、潮湿等条件。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的迁移学习方法在检测多种天气条件下表现出色，与典型的深度学习方法如YOLOv7、YOLOv9、Faster R-CNN和R-YOLO相比具有优越性能。此外，该方法还显示出能够泛化到各种场景的优势。&lt;h4&gt;结论&lt;/h4&gt;该迁移学习方法为解决天气对卫星互联网地面终端组件的影响提供了有效解决方案，能够提高卫星互联网在恶劣天气条件下的可靠性和性能，且具有良好的泛化能力，适用于各种实际部署场景。&lt;h4&gt;翻译&lt;/h4&gt;随着低地球轨道(LEO)卫星星座在卫星互联网中的日益普及，为农村和偏远地区提供了无处不在的连接能力。然而，天气事件对卫星互联网的性能和可靠性有重大影响。雪、雨等不良天气事件会显著干扰卫星互联网关键地面终端组件（如卫星天线）的性能和运行，严重破坏LEO卫星与地面站之间的空间-地面链路条件。这一挑战不仅需要基于地区的天气预报，还需要对地面终端组件上的精细化天气条件进行精细检测。这种能力可以帮助卫星互联网进行故障诊断和缓解，确保可靠性，但相应的解决方案仍然缺乏，更不用说在实际部署中必不可少的有效性和泛化能力。本文讨论了一种高效的迁移学习(TL)方法，使地面组件能够本地检测代表性的天气相关条件。所提出的方法可以检测由不良和典型天气事件引起的积雪、潮湿等条件，与典型的深度学习方法（如YOLOv7、YOLOv9、Faster R-CNN和R-YOLO）相比表现出优越性能。我们的迁移学习方法还显示出能够泛化到各种场景的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TAES.2024.3496857&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing adoption of satellite Internet with low-Earth-orbit (LEO)satellites in mega-constellations allows ubiquitous connectivity to rural andremote areas. However, weather events have a significant impact on theperformance and reliability of satellite Internet. Adverse weather events suchas snow and rain can disturb the performance and operations of satelliteInternet's essential ground terminal components, such as satellite antennas,significantly disrupting the space-ground link conditions between LEOsatellites and ground stations. This challenge calls for not only region-basedweather forecasts but also fine-grained detection capability on ground terminalcomponents of fine-grained weather conditions. Such a capability can assist infault diagnostics and mitigation for reliable satellite Internet, but itssolutions are lacking, not to mention the effectiveness and generalization thatare essential in real-world deployments. This paper discusses an efficienttransfer learning (TL) method that can enable a ground component to locallydetect representative weather-related conditions. The proposed method candetect snow, wet, and other conditions resulting from adverse and typicalweather events and shows superior performance compared to the typical deeplearning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TLmethod also shows the advantage of being generalizable to various scenarios.</description>
      <author>example@mail.com (Wenxuan Zhang, Peng Hu)</author>
      <guid isPermaLink="false">2511.00211v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Transfer learning discovery of molecular modulators for perovskite solar cells</title>
      <link>http://arxiv.org/abs/2511.00204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种基于预训练深度神经网络的化学信息迁移学习框架，用于预测分子调节剂对钙钛矿太阳能电池功率转换效率的影响，并通过实验验证了该方法的有效性，实现了26.91%的高效率。&lt;h4&gt;背景&lt;/h4&gt;钙钛矿太阳能电池的有效分子调节剂发现对推进其发展至关重要，但化学空间的庞大以及耗时昂贵的实验筛选阻碍了研究进程。同时，机器学习在加速材料发现方面有潜力，但由于数据稀缺和传统定量结构-性质关系模型的限制，将其应用于钙钛矿太阳能电池仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效预测分子调节剂对钙钛矿太阳能电池性能影响的机器学习方法，以加速新型分子调节剂的发现过程。&lt;h4&gt;方法&lt;/h4&gt;应用基于预训练深度神经网络的化学信息迁移学习框架，通过系统性地对多种分子表示进行基准测试，建立预测模型，并利用可解释性技术可视化学习到的化学表示，最后对筛选出的候选分子进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够低成本、高通量地筛选79,043种商业可用分子，并准确预测分子调节剂对钙钛矿太阳能电池功率转换效率的影响。实验验证表明，该框架确定的前分子调节剂显著提高了电池性能，实现了26.91%的冠军功率转换效率。&lt;h4&gt;结论&lt;/h4&gt;基于预训练深度神经网络的化学信息迁移学习框架为钙钛矿太阳能电池分子调节剂的发现提供了有效工具，克服了传统方法中的数据稀缺和模型限制问题，显著加速了材料发现进程。&lt;h4&gt;翻译&lt;/h4&gt;有效分子调节剂的发现对推进钙钛矿太阳能电池的发展至关重要，但研究过程受到化学空间庞大以及耗时昂贵的实验筛选的阻碍。同时，机器学习在加速材料发现方面具有巨大潜力。然而，由于数据稀缺和传统定量结构-性质关系模型的局限性，将机器学习应用于钙钛矿太阳能电池仍是一个重大挑战。在此，我们应用了一种基于预训练深度神经网络的化学信息迁移学习框架，能够高精度地预测分子调节剂对钙钛矿太阳能电池功率转换效率的影响。通过对多种分子表示进行系统性基准测试建立了该框架，能够以低成本和高通量方式对79,043种商业可用分子进行虚拟筛选。此外，我们利用可解释性技术可视化学习到的化学表示，并对得到的调节剂-钙钛矿相互作用进行实验表征。该框架确定的前分子调节剂随后通过实验验证，在钙钛矿太阳能电池中实现了显著提高的冠军功率转换效率26.91%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The discovery of effective molecular modulators is essential for advancingperovskite solar cells (PSCs), but the research process is hindered by thevastness of chemical space and the time-consuming and expensive trial-and-errorexperimental screening. Concurrently, machine learning (ML) offers significantpotential for accelerating materials discovery. However, applying ML to PSCsremains a major challenge due to data scarcity and limitations of traditionalquantitative structure-property relationship (QSPR) models. Here, we apply achemical informed transfer learning framework based on pre-trained deep neuralnetworks, which achieves high accuracy in predicting the molecular modulator'seffect on the power conversion efficiency (PCE) of PSCs. This framework isestablished through systematical benchmarking of diverse molecularrepresentations, enabling lowcost and high-throughput virtual screening over79,043 commercially available molecules. Furthermore, we leverageinterpretability techniques to visualize the learned chemical representationand experimentally characterize the resulting modulator-perovskiteinteractions. The top molecular modulators identified by the framework aresubsequently validated experimentally, delivering a remarkably improvedchampion PCE of 26.91% in PSCs.</description>
      <author>example@mail.com (Haoming Yan, Xinyu Chen, Yanran Wang, Zhengchao Luo, Weizheng Huang, Hongshuai Wang, Peng Chen, Yuzhi Zhang, Weijie Sun, Jinzhuo Wang, Qihuang Gong, Rui Zhu, Lichen Zhao)</author>
      <guid isPermaLink="false">2511.00204v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Retrieval-Augmented Multimodal Depression Detection</title>
      <link>http://arxiv.org/abs/2511.01892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE EMBC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新颖的检索增强生成（RAG）框架，用于抑郁症检测，通过情感提示作为辅助模态提高情感表示和可解释性。&lt;h4&gt;背景&lt;/h4&gt;多模态深度学习通过整合文本、音频和视频信号在抑郁症检测中显示出潜力。然而，现有利用情感分析增强情感理解的方法存在计算成本高、领域不匹配和静态知识限制的问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有抑郁症检测方法中计算成本高、领域不匹配和静态知识限制的问题。&lt;h4&gt;方法&lt;/h4&gt;提出检索增强生成（RAG）框架，给定抑郁症相关文本，从情感数据集中检索语义相关的情感内容，使用大型语言模型生成情感提示作为辅助模态，以丰富情感表示并提高可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;在AVEC 2019数据集上的实验表明，该方法实现了最先进的性能，CCC达到0.593，MAE达到3.95，超过了之前的迁移学习和多任务学习基线。&lt;h4&gt;结论&lt;/h4&gt;检索增强生成框架能有效解决现有方法在抑郁症检测中的问题，提高性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;多模态深度学习通过整合文本、音频和视频信号在抑郁症检测中显示出潜力。最近的研究利用情感分析来增强情感理解，但存在计算成本高、领域不匹配和静态知识限制的问题。为解决这些问题，我们提出了一种新颖的检索增强生成（RAG）框架。给定一个与抑郁症相关的文本，我们的方法从情感数据集中检索语义上相关的情感内容，并使用大型语言模型生成情感提示作为辅助模态。这个提示丰富了情感表示并提高了可解释性。在AVEC 2019数据集上的实验表明，我们的方法实现了最先进的性能，CCC达到0.593，MAE达到3.95，超过了之前的迁移学习和多任务学习基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal deep learning has shown promise in depression detection byintegrating text, audio, and video signals. Recent work leverages sentimentanalysis to enhance emotional understanding, yet suffers from highcomputational cost, domain mismatch, and static knowledge limitations. Toaddress these issues, we propose a novel Retrieval-Augmented Generation (RAG)framework. Given a depression-related text, our method retrieves semanticallyrelevant emotional content from a sentiment dataset and uses a Large LanguageModel (LLM) to generate an Emotion Prompt as an auxiliary modality. This promptenriches emotional representation and improves interpretability. Experiments onthe AVEC 2019 dataset show our approach achieves state-of-the-art performancewith CCC of 0.593 and MAE of 3.95, surpassing previous transfer learning andmulti-task learning baselines.</description>
      <author>example@mail.com (Ruibo Hou, Shiyu Teng, Jiaqing Liu, Shurong Chai, Yinhao Li, Lanfen Lin, Yen-Wei Chen)</author>
      <guid isPermaLink="false">2511.01892v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2511.00635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Multi-Mapcher的新型多会话同步定位与地图构建框架，通过大规模地图到地图配准实现会话间初始对齐，克服了传统方法对回环检测的依赖，实验证明该方法在各种LiDAR传感器条件下表现优异且速度更快。&lt;h4&gt;背景&lt;/h4&gt;随着各种3D光探测和测距（LiDAR）传感器被引入市场，使用异构LiDAR传感器进行多会话同步定位与地图构建的研究已经积极展开。现有的MSS方法大多依赖于回环检测来实现会话间的对齐；然而，由于不同会话中使用的传感器在密度和视场（FoV）上存在差异，回环检测的性能可能会降低。&lt;h4&gt;目的&lt;/h4&gt;挑战现有方法严重依赖检测模块的范式，提出一种使用大规模地图到地图配准实现会话间初始对齐的新型MSS框架，这通常被认为不可行。&lt;h4&gt;方法&lt;/h4&gt;利用抗异常值的3D点云配准技术进行会话间初始对齐，然后在假设初始对齐足够精确的情况下，通过基于半径搜索发现会话间的回环，最后采用基于锚节点的鲁棒姿态图优化来构建一致的全局地图。&lt;h4&gt;主要发现&lt;/h4&gt;使用各种LiDAR传感器捕获会话时，新方法表现出显著更好的MSS性能，并且比最先进的方法更快。&lt;h4&gt;结论&lt;/h4&gt;Multi-Mapcher框架通过创新地使用大规模地图到地图配准，有效解决了异构LiDAR传感器在多会话同步定位与地图构建中的挑战，代码已公开在https://github.com/url-kaist/multi-mapcher。&lt;h4&gt;翻译&lt;/h4&gt;随着各种3D光探测和测距（LiDAR）传感器被引入市场，使用异构LiDAR传感器进行多会话同步定位与地图构建（MSS）的研究已经积极展开。现有的MSS方法大多依赖于回环检测来实现会话间的对齐；然而，由于不同会话中使用的传感器在密度和视场（FoV）上存在差异，回环检测的性能可能会降低。本研究挑战了现有方法严重依赖检测模块的范式，提出了一种名为Multi-Mapcher的新型MSS框架，该框架采用大规模地图到地图配准来实现会话间初始对齐，这通常被认为不可行，方法是利用抗异常值的3D点云配准。在假设会话间初始对齐足够精确的情况下，通过基于半径搜索来发现会话间的回环，然后采用基于锚节点的鲁棒姿态图优化来构建一致的全局地图。如我们的实验所示，我们的方法在使用各种LiDAR传感器捕获会话时表现出显著更好的MSS性能，并且比最先进的方法更快。我们的代码可在https://github.com/url-kaist/multi-mapcher获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多会话SLAM中，当使用不同类型激光雷达传感器时，现有方法依赖的回环检测模块性能下降导致会话间对齐失败的问题。这个问题很重要，因为随着市场上各种激光雷达传感器的出现，自动驾驶车辆和机器人可能配备不同类型的传感器，而现有方法在处理异构传感器数据时表现不佳，导致无法构建一致的全局地图，限制了长期地图管理和多机器人协作的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有MSS方法在异构激光雷达传感器上表现不佳，挑战了依赖回环检测的范式。他们借鉴了异常值鲁棒的点云注册方法（如Quatro），使用快速点特征直方图（FPFH）建立对应关系，并引入锚节点概念解决多会话轨迹参考帧不固定的问题。作者还借鉴了基于因子图的姿态图优化方法，但进行了改进以适应多会话场景，实现了地图到地图和扫描到扫描两个级别的鲁棒注册。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是不依赖传统回环检测模块，而是使用异常值鲁棒的点云注册实现会话间对齐。整体流程包括：1) 会话内SLAM处理；2) 地图到地图级别的会话间初始对齐，使用FPFH特征和鲁棒注册估计变换矩阵；3) 扫描到扫描级别的会话间回环闭合，通过半径搜索和截断均方误差过滤错误候选；4) 基于锚节点的姿态图优化；5) 构建一致的全局地图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 不依赖回环检测的LCD-free方法；2) 实现大规模地图到地图注册；3) 在地图和扫描两个级别使用鲁棒注册；4) 提出截断均方误差处理视场差异；5) 支持异构激光雷达传感器。相比之前工作，Multi-Mapcher在处理异构传感器数据时表现更好，不会因LCD性能下降导致失败；将2D地图合并扩展到3D；对部分重叠和环境变化更鲁棒；速度比现有方法快5-9倍。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Multi-Mapcher通过不依赖回环检测的鲁棒注册方法，实现了对异构激光雷达传感器捕获的多会话SLAM的精确对齐，构建了一致的全局地图，同时比现有方法更快且更鲁棒。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As various 3D light detection and ranging (LiDAR) sensors have beenintroduced to the market, research on multi-session simultaneous localizationand mapping (MSS) using heterogeneous LiDAR sensors has been activelyconducted. Existing MSS methods mostly rely on loop closure detection forinter-session alignment; however, the performance of loop closure detection canbe potentially degraded owing to the differences in the density and field ofview (FoV) of the sensors used in different sessions. In this study, wechallenge the existing paradigm that relies heavily on loop detection modulesand propose a novel MSS framework, called Multi-Mapcher, that employslarge-scale map-to-map registration to perform inter-session initial alignment,which is commonly assumed to be infeasible, by leveraging outlier-robust 3Dpoint cloud registration. Next, after finding inter-session loops by radiussearch based on the assumption that the inter-session initial alignment issufficiently precise, anchor node-based robust pose graph optimization isemployed to build a consistent global map. As demonstrated in our experiments,our approach shows substantially better MSS performance for various LiDARsensors used to capture the sessions and is faster than state-of-the-artapproaches. Our code is available athttps://github.com/url-kaist/multi-mapcher.</description>
      <author>example@mail.com (Hyungtae Lim, Daebeom Kim, Hyun Myung)</author>
      <guid isPermaLink="false">2511.00635v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba</title>
      <link>http://arxiv.org/abs/2511.00260v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures, 3 tables, IPCAI conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MambaNetLK的新型3D点云配准方法，以及一个名为C3VD-Raycasting-10k的大规模临床数据集，用于解决结肠镜引导中生物组织特征退化和术前术后域差异导致的配准稳定性问题。&lt;h4&gt;背景&lt;/h4&gt;准确的3D点云配准是可靠的图像引导结肠镜检查的基础，直接影响病变定位、边缘评估和导航安全性。然而，生物组织具有重复纹理和局部均匀几何特征，导致特征退化，同时术前解剖和术中观察之间的显著域差异进一步降低了配准稳定性。&lt;h4&gt;目的&lt;/h4&gt;解决临床关键挑战，引入一种针对内镜导航的新型3D配准方法，并创建高质量、临床基础的数据集以支持严格且可复现的基准测试。&lt;h4&gt;方法&lt;/h4&gt;引入C3VD-Raycasting-10k数据集，包含10,014对从临床CT数据派生的几何对齐点云；提出MambaNetLK框架，通过将Mamba状态空间模型作为跨模态特征提取器集成到PointNetLK架构中增强其能力，有效捕获长程依赖关系，并使用Lucas-Kanade算法迭代实现对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在临床数据集C3VD-Raycasting-10k上，MambaNetLK实现了最先进的性能，与第二好的方法相比，中值旋转误差降低了56.04%，RMSE平移误差降低了26.19%；该模型在ModelNet40上表现出强大的泛化能力，并对初始姿态扰动具有优异的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;MambaNetLK为外科导航中的3D配准提供了强大基础，基于SSM的全局表达特征提取器与大规模临床数据集的结合，使微创手术如结肠镜检查中的引导系统更加准确和可靠。&lt;h4&gt;翻译&lt;/h4&gt;准确的3D点云配准是可靠的图像引导结肠镜检查的基础，直接影响病变定位、边缘评估和导航安全性。然而，生物组织具有重复纹理和局部均匀几何特征，导致特征退化，同时术前解剖和术中观察之间的显著域差异进一步降低了配准稳定性。为解决这些临床关键挑战，我们引入了一种针对内镜导航的新型3D配准方法和高质量、临床基础的数据集以支持严格且可复现的基准测试。我们引入了C3VD-Raycasting-10k，这是一个包含10,014对从临床CT数据派生的几何对齐点云的大规模基准数据集。我们提出了MambaNetLK，一种新的无对应点配准框架，通过将Mamba状态空间模型作为跨模态特征提取器集成到PointNetLK架构中来增强它。因此，所提出的框架以线性时间复杂度有效捕获长程依赖关系。对齐是通过使用Lucas-Kanade算法迭代实现的。在临床数据集C3VD-Raycasting-10k上，MambaNetLK与最先进的方法相比实现了最佳性能，中值旋转误差比第二好的方法降低了56.04%，RMSE平移误差降低了26.19%。该模型在ModelNet40上也表现出强大的泛化能力，并对初始姿态扰动具有优异的鲁棒性。MambaNetLK为外科导航中的3D配准提供了强大的基础。基于SSM的全局表达特征提取器与大规模临床数据集的结合，使结肠镜等微创手术中的引导系统更加准确和可靠。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决结肠镜检查中的3D点云配准问题，即如何准确地将手术中的实时内窥镜数据与术前的CT扫描模型进行对齐。这个问题非常重要，因为准确的配准直接关系到病变定位、边界评估和导航安全，而生物组织的重复纹理和局部均匀几何特性会导致特征退化，同时术前与术中数据间的域偏移会进一步降低配准稳定性，影响临床诊断和治疗效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于对现有方法局限性的分析进行设计：基于对应关系的方法在平滑、无纹理的器官表面存在特征退化；PointNetLK等无对应关系方法依赖MLP特征提取器，难以捕获长距离几何依赖；Transformer虽有长距离建模能力但在手术应用中受限。作者借鉴了PointNetLK的框架，但用Mamba状态空间模型替代了MLP特征提取器，并创建了新的临床数据集C3VD-Raycasting-10k来解决基准数据缺乏的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将Mamba状态空间模型整合到Lucas-Kanade配准流程中，通过将点云视为序列来有效捕获全局几何结构，避免显式点对应关系。整体流程：1)输入点云并进行序列化和位置编码；2)使用Mamba块处理位置感知特征序列，选择性传播或遗忘信息；3)通过MLP层和最大池化生成全局特征描述符；4)使用Lucas-Kanade算法迭代最小化源点和目标点云特征差异直至收敛；5)利用相机姿态生成匹配的源点和目标点云对。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)MambaNetLK框架，结合Mamba SSM和IC-LK实现长距离依赖建模；2)C3VD-Raycasting-10k临床数据集，提供10,014个视点匹配的点云对；3)高效捕获全局几何结构，克服MLP特征提取器局限；4)线性时间复杂度的长距离依赖建模。相比之前工作：避免了基于对应关系方法中的特征退化问题；比PointNetLK更好地捕获长距离几何依赖；比Transformer更高效且更适合手术应用；解决了缺乏3D配准基准数据集的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MambaNetLK通过整合Mamba状态空间模型和创建C3VD-Raycasting-10k临床数据集，显著提高了结肠镜检查中3D点云配准的准确性和鲁棒性，为微创手术中的实时临床导航系统奠定了坚实基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D point cloud registration underpins reliable image-guidedcolonoscopy, directly affecting lesion localization, margin assessment, andnavigation safety. However, biological tissue exhibits repetitive textures andlocally homogeneous geometry that cause feature degeneracy, while substantialdomain shifts between pre-operative anatomy and intra-operative observationsfurther degrade alignment stability. To address these clinically criticalchallenges, we introduce a novel 3D registration method tailored for endoscopicnavigation and a high-quality, clinically grounded dataset to support rigorousand reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scalebenchmark dataset with 10,014 geometrically aligned point cloud pairs derivedfrom clinical CT data. We propose MambaNetLK, a novel correspondence-freeregistration framework, which enhances the PointNetLK architecture byintegrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.As a result, the proposed framework efficiently captures long-rangedependencies with linear-time complexity. The alignment is achieved iterativelyusing the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,MambaNetLK achieves the best performance compared with the state-of-the-artmethods, reducing median rotation error by 56.04% and RMSE translation error by26.19% over the second-best method. The model also demonstrates stronggeneralization on ModelNet40 and superior robustness to initial poseperturbations. MambaNetLK provides a robust foundation for 3D registration insurgical navigation. The combination of a globally expressive SSM-based featureextractor and a large-scale clinical dataset enables more accurate and reliableguidance systems in minimally invasive procedures like colonoscopy.</description>
      <author>example@mail.com (Linzhe Jiang, Jiayuan Huang, Sophia Bano, Matthew J. Clarkson, Zhehua Mao, Mobarak I. Hoque)</author>
      <guid isPermaLink="false">2511.00260v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Grouping-Based Hybrid Color Correction Algorithm for Color Point Clouds</title>
      <link>http://arxiv.org/abs/2511.02397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于分组的混合颜色校正算法用于彩色点云，根据点云重叠率自适应分组，并针对不同组别采用不同的颜色校正方法，通过大量测试验证了算法的有效性。&lt;h4&gt;背景&lt;/h4&gt;彩色点云的颜色一致性校正是3D渲染和压缩应用中的基础且重要任务，而以往的颜色校正方法主要针对彩色图像，而非点云数据。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于分组的混合颜色校正算法，专门用于彩色点云的颜色一致性校正。&lt;h4&gt;方法&lt;/h4&gt;1) 估计对齐后的源点云和目标点云之间的重叠率；2) 根据重叠率高低，将目标点分为两组(Gcl和Gmod)或三组(Gcl、Gmod和Gdist)；3) 对Gcl组使用K近邻双边插值方法；4) 对Gmod组使用结合KBI和直方图均衡化的方法；5) 对Gdist组使用直方图均衡化方法；6) 讨论算法的分组效应特性和消融研究。&lt;h4&gt;主要发现&lt;/h4&gt;算法的颜色一致性校正效果已通过1086对测试彩色点云与最先进方法进行了验证，证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的基于分组的混合颜色校正算法能够有效实现彩色点云的颜色一致性校正，相关C++源代码已在GitHub平台公开。&lt;h4&gt;翻译&lt;/h4&gt;彩色点云的颜色一致性校正是3D渲染和压缩应用中的一个基础而重要的任务。过去，大多数先前的颜色校正方法旨在校正彩色图像的颜色。本文的目的是提出一种基于分组的混合颜色校正算法用于彩色点云。我们的算法首先估计对齐后的源点云和目标点云之间的重叠率，然后根据估计的重叠率高低，自适应地将目标点分为两组，即近距离组Gcl和中距离组Gmod，或三组，即Gcl、Gmod和远距离组Gdist。为了校正Gcl中目标点的颜色，提出了一种基于K近邻的双边插值(KBI)方法。为了校正Gmod中目标点的颜色，提出了一种结合KBI和直方图均衡化(JKHE)的方法。对于Gdist中的目标点，提出了一种直方图均衡化(HE)方法进行颜色校正。最后，我们讨论了算法中无分组效应特性和消融研究。我们的算法在1086对测试彩色点云上与最先进方法进行了比较，证明了期望的颜色一致性校正效果。本算法的C++源代码可以从网站https://github.com/ivpml84079/Point-cloud-color-correction获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决彩色点云对齐后的颜色一致性问题。当两个彩色点云对齐时，由于采集设备参数、光照条件等因素差异，常会出现颜色不一致现象，导致视觉不协调。这个问题在3D视觉、自动驾驶、虚拟现实等领域非常重要，因为颜色不一致会影响点云压缩、人体姿态估计和点云渲染等应用的质量，最终影响合成3D场景的真实感和美观度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有颜色校正方法（如最近邻法、K近邻法、直方图匹配法等）的优缺点，发现这些方法主要针对彩色图像而非点云。作者借鉴了这些方法的基本思想，但针对点云特性进行了改进。作者观察到点云中点与周围点的关系不同于图像像素，因此考虑了点之间的空间距离关系，提出了基于重叠率的分组策略，并针对不同距离的组设计了专门的校正方法，从而形成了这个混合算法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是根据源点云和目标点云之间的重叠率，自适应地将目标点分为不同组，然后对不同组的点采用最适合的颜色校正方法。整体流程为：1)计算点云对齐的重叠率；2)根据重叠率决定将目标点分为两组或三组；3)对近距离组使用K近邻双边插值法校正颜色；4)对中等距离组使用联合K近邻双边插值法和直方图均衡化法校正颜色；5)对远距离组使用直方图均衡化法校正颜色；6)输出颜色校正后的结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出基于重叠率的自适应分组策略；2)针对不同距离组设计专门的校正方法；3)证明算法具有分组效应自由性质，确保组间边界颜色平滑；4)在大量点云对上验证了算法性能。相比之前工作，本文专注于点云而非图像，采用分组策略而非单一方法处理所有点，并在中等距离组中引入了动态权重调整机制，显著提高了颜色校正效果和视觉质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于分组混合的颜色校正算法，通过自适应分组和针对性校正方法，有效解决了彩色点云对齐后的颜色一致性问题，显著提升了合成3D场景的视觉质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Color consistency correction for color point clouds is a fundamental yetimportant task in 3D rendering and compression applications. In the past, mostprevious color correction methods aimed at correcting color for color images.The purpose of this paper is to propose a grouping-based hybrid colorcorrection algorithm for color point clouds. Our algorithm begins by estimatingthe overlapping rate between the aligned source and target point clouds, andthen adaptively partitions the target points into two groups, namely the closeproximity group Gcl and the moderate proximity group Gmod, or three groups,namely Gcl, Gmod, and the distant proximity group Gdist, when the estimatedoverlapping rate is low or high, respectively. To correct color for targetpoints in Gcl, a K-nearest neighbors based bilateral interpolation (KBI) methodis proposed. To correct color for target points in Gmod, a joint KBI and thehistogram equalization (JKHE) method is proposed. For target points in Gdist, ahistogram equalization (HE) method is proposed for color correction. Finally,we discuss the grouping-effect free property and the ablation study in ouralgorithm. The desired color consistency correction benefit of our algorithmhas been justified through 1086 testing color point cloud pairs against thestate-of-the-art methods. The C++ source code of our algorithm can be accessedfrom the website: https://github.com/ivpml84079/Point-cloud-color-correction.</description>
      <author>example@mail.com (Kuo-Liang Chung, Ting-Chung Tang)</author>
      <guid isPermaLink="false">2511.02397v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds</title>
      <link>http://arxiv.org/abs/2511.02395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at IEEE International Conference on  Intelligent Transportation Systems (ITSC 2025), 8 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种自监督学习方法，用于稀疏和有噪声的雷达点云的运动目标分割。该方法采用两步式方法：首先使用基于聚类的对比自监督表示学习进行预训练，然后使用有限的标注数据进行监督微调。作者提出了一种新颖的基于聚类的对比损失函数，通过动态点移除进行聚类优化，使网络能够生成雷达数据的运动感知表示。通过自监督预训练，该方法在微调后提高了标签效率，并有效提升了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;运动目标分割对自动驾驶等自主移动系统的安全和可靠性至关重要，可以提高后续任务（如SLAM或路径规划）的可靠性和鲁棒性。虽然相机或LiDAR数据的分割已被广泛研究并取得良好成果，但通常需要累积时间序列来获得必要的时间上下文，从而增加了延迟。雷达传感器通过提供点的多普勒速度的直接测量值解决了单扫描运动目标分割的问题。然而，雷达点云通常稀疏且有噪声，使得在监督学习中使用的数据标注非常繁琐、耗时且成本高昂。&lt;h4&gt;目的&lt;/h4&gt;解决雷达点云数据标注困难的问题，提出一种自监督方法，用于稀疏和有噪声的雷达点云的运动目标分割，减少对大量标注数据的依赖，提高分割效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;采用两步方法：1) 对比自监督表示学习：提出一种新颖的基于聚类的对比损失函数，通过动态点移除进行聚类优化，预训练网络以生成雷达数据的运动感知表示；2) 监督微调：使用有限的标注数据进行监督微调。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在微调后提高了标签效率，通过自监督预训练有效提升了最先进性能。&lt;h4&gt;结论&lt;/h4&gt;自监督学习方法可以有效解决雷达点云运动目标分割中标注数据不足的问题，通过预训练和微调的两步策略，能够在有限标注数据的情况下实现高性能的分割结果。&lt;h4&gt;翻译&lt;/h4&gt;运动目标分割对于自动驾驶等自主移动系统的安全和可靠性至关重要，可以提高后续任务（如SLAM或路径规划）的可靠性和鲁棒性。虽然相机或LiDAR数据的分割已被广泛研究并取得良好成果，但通常需要累积时间序列来获得必要的时间上下文，从而增加了延迟。雷达传感器通过提供点的多普勒速度的直接测量值解决了这个问题，可用于单扫描运动目标分割。然而，雷达点云通常稀疏且有噪声，使得在监督学习中使用的数据标注非常繁琐、耗时且成本高昂。为解决这个问题，我们解决了稀疏和有噪声的雷达点云的自监督运动目标分割任务。我们采用对比自监督表示学习与后续使用有限标注数据的监督微调的两步方法。我们提出了一种新颖的基于聚类的对比损失函数，通过动态点移除进行聚类优化，预训练网络以生成雷达数据的运动感知表示。我们的方法在微调后提高了标签效率，通过自监督预训练有效提升了最先进性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决稀疏和有噪声的雷达点云上的移动物体分割问题。这个问题在现实中非常重要，因为移动物体分割对自动驾驶汽车等自主移动系统的安全和可靠性至关重要，能够提高后续任务如SLAM或路径规划的可靠性和鲁棒性。虽然相机和LiDAR数据的分割研究广泛，但它们需要积累时间序列数据，增加了系统延迟。雷达传感器可以通过多普勒速度直接测量实现单次扫描的移动物体分割，但雷达点云的稀疏性和噪声性使得数据标注非常困难、耗时且成本高昂。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的问题：相机和LiDAR需要时间序列增加延迟，雷达虽能单次扫描但数据质量差导致标注困难。作者借鉴了自监督学习思想，采用两步方法：先进行对比自监督表征学习预训练，再用有限标注数据监督微调。设计上采用学生-教师框架（自监督学习中常用），并基于雷达实例变换器（RIT）架构进行修改。核心创新是设计了新的对比损失函数，结合聚类和动态点移除（DPR）算法来生成伪标签，使网络学习运动感知的表征。作者还借鉴了LiDAR领域的自监督表征学习方法，但针对雷达数据的特性进行了调整。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用自监督学习减少对标注数据的依赖，设计专门的对比损失函数使网络学习能区分运动和静止物体的表征，结合空间信息和运动信息提高分割性能。整体流程包括：1)数据准备：使用包含坐标、多普勒速度和雷达横截面积的雷达点云；2)自监督预训练：构建学生-教师框架，使用HDBSCAN聚类，通过DPR算法细化聚类分离运动和静止点，计算对比损失拉近同类聚类、推远异类聚类；3)监督微调：添加MLP生成分割掩码，在少量标注数据上微调；4)评估：在View-of-Delft和RadarScenes数据集上用IoU指标评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新的聚类对比损失函数，专门针对雷达数据设计，关注运动信息而非语义特征；2)自监督预训练框架首次应用于雷达点云的移动物体分割；3)两步训练方法提高标签效率。相比之前工作的不同：与相机/LiDAR方法不需时间序列减少延迟；与RaFlow等自监督方法相比在少量标注数据上表现更好；与现有监督方法在相同标注量下性能更高；与现有对比损失函数不同，专注于运动信息而非语义特征；结合聚类和算法方法生成伪标签而非依赖数据标注。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种新的自监督学习方法，通过设计专门的聚类对比损失函数，在少量标注数据的情况下显著提高了稀疏和噪声雷达点云上的移动物体分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Moving object segmentation is a crucial task for safe and reliable autonomousmobile systems like self-driving cars, improving the reliability and robustnessof subsequent tasks like SLAM or path planning. While the segmentation ofcamera or LiDAR data is widely researched and achieves great results, it oftenintroduces an increased latency by requiring the accumulation of temporalsequences to gain the necessary temporal context. Radar sensors overcome thisproblem with their ability to provide a direct measurement of a point's Dopplervelocity, which can be exploited for single-scan moving object segmentation.However, radar point clouds are often sparse and noisy, making data annotationfor use in supervised learning very tedious, time-consuming, andcost-intensive. To overcome this problem, we address the task ofself-supervised moving object segmentation of sparse and noisy radar pointclouds. We follow a two-step approach of contrastive self-supervisedrepresentation learning with subsequent supervised fine-tuning using limitedamounts of annotated data. We propose a novel clustering-based contrastive lossfunction with cluster refinement based on dynamic points removal to pretrainthe network to produce motion-aware representations of the radar data. Ourmethod improves label efficiency after fine-tuning, effectively boostingstate-of-the-art performance by self-supervised pretraining.</description>
      <author>example@mail.com (Leon Schwarzer, Matthias Zeller, Daniel Casado Herraez, Simon Dierl, Michael Heidingsfeld, Cyrill Stachniss)</author>
      <guid isPermaLink="false">2511.02395v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>3D Point Cloud Object Detection on Edge Devices for Split Computing</title>
      <link>http://arxiv.org/abs/2511.02293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages. This version includes minor lstlisting configuration  adjustments for successful compilation. No changes to content or layout.  Originally published at ACM/IEEE RAGE 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用分割计算技术解决了自动驾驶领域中深度学习模型在边缘设备上计算负担重、处理时间长和功耗高的问题，实验结果表明该方法能有效减少推理时间和边缘设备执行时间。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶技术领域正在快速发展，深度学习是其中的关键组成部分。特别是在感知领域，利用LiDAR收集的3D点云数据来运行深度神经网络模型进行3D物体检测。&lt;h4&gt;目的&lt;/h4&gt;解决最先进的深度学习模型在边缘设备上处理时间长和功耗高的问题，通过分割计算减轻边缘设备的计算负担。&lt;h4&gt;方法&lt;/h4&gt;采用分割计算，一种分布式机器学习推理方法，将计算任务分割处理，只传输深度神经网络模型的中间数据，从而减少边缘设备的计算负担和数据泄露风险。&lt;h4&gt;主要发现&lt;/h4&gt;在体素化后分割，推理时间减少70.8%，边缘设备执行时间减少90.0%；在网络内分割，推理时间减少高达57.1%，边缘设备执行时间减少高达69.5%。&lt;h4&gt;结论&lt;/h4&gt;分割计算能有效解决自动驾驶技术中边缘设备计算负担重的问题，显著减少处理时间和功耗，同时提高数据安全性。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶技术领域正在快速发展，深度学习是其关键组成部分。特别是在感知领域，利用LiDAR收集的3D点云数据来运行深度神经网络模型进行3D物体检测。然而，这些最先进的模型复杂度高，导致边缘设备处理时间长和功耗增加。本研究旨在通过利用分割计算（一种分布式机器学习推理方法）来解决这些问题。分割计算旨在减轻边缘设备的计算负担，从而减少处理时间和功耗。此外，它仅传输深度神经网络模型的中间数据，从而最小化数据泄露风险。实验结果表明，在体素化后分割可将推理时间减少70.8%，边缘设备执行时间减少90.0%。在网络内分割时，推理时间可减少高达57.1%，边缘设备执行时间可减少高达69.5%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决边缘设备进行3D点云目标检测时计算负担过重的问题。在自动驾驶领域，LiDAR收集的3D点云数据需要通过复杂深度神经网络处理，但边缘设备计算能力有限，导致处理时间长、功耗高，影响实时性能和安全。这一问题重要是因为它关系到自动驾驶系统的可靠性和实用性，计算效率不足可能导致安全隐患，而轻量级模型又会降低检测准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了边缘设备处理复杂模型的局限性，考虑了轻量级模型和直接传输数据到服务器两种方案，但发现它们分别存在准确性和隐私问题。因此，作者借鉴了Split Computing这一分布式机器学习方法，将其应用于3D点云目标检测场景。作者使用了OpenPCDet作为检测工具箱，并参考了BottleFit等现有SC方法，同时选择了Voxel R-CNN作为基础模型，通过实验确定最佳分割点。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将深度神经网络模型在中间分割成两部分(head model和tail model)，head模型在边缘设备运行，tail模型在边缘服务器运行，只传输中间处理结果而非原始数据。实现流程为：1)边缘设备接收点云数据并预处理；2)模型在预设分割点处分割；3)边缘设备运行head模型；4)将head模型输出传输到边缘服务器；5)边缘服务器运行tail模型生成预测结果；6)将结果返回给边缘设备。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将Split Computing应用于3D点云目标检测；2)系统研究了不同分割点对性能的影响；3)提出选择分割点的两个关键标准(早期分割和最小输出数据大小)；4)通过实验验证了方法的有效性。相比之前工作，这种方法在保持高检测准确性的同时显著降低了边缘设备计算负担和执行时间，相比直接传输原始数据保护了隐私，相比轻量级模型维持了更好的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于Split Computing的3D点云目标检测方法，通过将深度学习模型分割并在边缘设备和边缘服务器间协同计算，显著降低了边缘设备的计算负担和执行时间，同时保护了数据隐私。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/RAGE62451.2024.00009&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The field of autonomous driving technology is rapidly advancing, with deeplearning being a key component. Particularly in the field of sensing, 3D pointcloud data collected by LiDAR is utilized to run deep neural network models for3D object detection. However, these state-of-the-art models are complex,leading to longer processing times and increased power consumption on edgedevices. The objective of this study is to address these issues by leveragingSplit Computing, a distributed machine learning inference method. SplitComputing aims to lessen the computational burden on edge devices, therebyreducing processing time and power consumption. Furthermore, it minimizes therisk of data breaches by only transmitting intermediate data from the deepneural network model. Experimental results show that splitting aftervoxelization reduces the inference time by 70.8% and the edge device executiontime by 90.0%. When splitting within the network, the inference time is reducedby up to 57.1%, and the edge device execution time is reduced by up to 69.5%.</description>
      <author>example@mail.com (Taisuke Noguchi, Takuya Azumi)</author>
      <guid isPermaLink="false">2511.02293v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping</title>
      <link>http://arxiv.org/abs/2511.01186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LiDAR-VGGT的新型框架，通过两阶段融合流程将激光雷达惯性里程计与VGGT模型紧密结合，实现了大规模彩色点云的有效重建，解决了现有方法在可扩展性和度量尺度方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;大规模彩色点云重建是机器人学中的重要任务，支持感知、导航和场景理解。然而，现有的激光雷达惯性视觉里程计(LIVO)对外部校准高度敏感，而3D视觉基础模型如VGGT在大规模环境中可扩展性有限且缺乏度量尺度。&lt;h4&gt;目的&lt;/h4&gt;克服现有技术的局限性，提出一种能够生成密集、全局一致的彩色点云的新型框架。&lt;h4&gt;方法&lt;/h4&gt;提出LiDAR-VGGT框架，通过两阶段由粗到细的融合流程紧密耦合激光雷达惯性里程计与VGGT模型。第一阶段采用预融合模块进行鲁棒的初始化细化，有效估计VGGT姿态和具有粗略度量尺度的点云；第二阶段通过后融合模块增强跨模态3D相似性变换，使用基于边界框的正则化减少传感器间视场不一致导致的尺度失真。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的实验表明，LiDAR-VGGT实现了密集、全局一致的彩色点云，性能优于基于VGGT的方法和LIVO基线。&lt;h4&gt;结论&lt;/h4&gt;提出的LiDAR-VGGT框架有效解决了现有技术在点云重建中的局限性，并将新型彩色点云评估工具包实现为开源软件。&lt;h4&gt;翻译&lt;/h4&gt;重建大规模彩色点云是机器人学中的重要任务，支持感知、导航和场景理解。尽管激光雷达惯性视觉里程计(LIVO)有所进步，但其性能对外部校准高度敏感。同时，3D视觉基础模型如VGGT在大规模环境中可扩展性有限且缺乏度量尺度。为克服这些限制，我们提出了LiDAR-VGGT，一种通过两阶段由粗到细的融合流程紧密耦合激光雷达惯性里程计与最先进的VGGT模型的新型框架：首先，具有鲁棒初始化细化的预融合模块有效估计了每个会话内具有粗略度量尺度的VGGT姿态和点云；然后，后融合模块增强跨模态3D相似性变换，使用基于边界框的正则化减少激光雷达和相机传感器之间视场不一致导致的尺度失真。在多个数据集上的大量实验表明，LiDAR-VGGT实现了密集、全局一致的彩色点云，性能优于基于VGGT的方法和LIVO基线。我们提出的新型彩色点云评估工具包实现将作为开源软件发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决两个问题：1) 传统LiDAR惯性视觉里程计(LIVO)方法对传感器外参校准敏感且点云稀疏；2) 3D视觉基础模型(如VGGT)在大环境中缺乏全局一致性和度量尺度。这个问题在机器人领域非常重要，因为准确的彩色点云重建是机器人感知、导航和场景理解的基础，对自主导航、多机器人协作和自动驾驶等应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的优缺点：LIVO提供精确定位但对校准敏感且点云稀疏，VGGT能生成密集彩色点云但缺乏全局一致性和度量尺度。作者借鉴了LIVO的LiDAR-IMU融合获取真实尺度参考，借鉴VGGT的视觉几何变换生成密集重建，借鉴SLAM中的位图优化确保全局一致性。在此基础上，作者设计了新的两阶段粗到细融合框架：预融合模块使用LIVO初始化和校准VGGT，后融合模块通过增强的跨模态配准进一步优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过将LiDAR-IMU视觉里程计与VGGT模型紧密耦合，利用LiDAR提供真实世界尺度信息解决VGGT缺乏度量尺度的问题，同时利用VGGT生成密集彩色点云的能力克服LiDAR点云稀疏的局限性。整体流程分为：1) 预融合模块：将长图像序列分成多个会话，独立使用VGGT处理，通过线性验证和尺度RANSAC精炼VGGT位姿，转换到世界坐标系；2) 后融合模块：使用基于边界框正则化的增强跨模态Sim(3)配准，将VGGT点云与LiDAR点云对齐，应用全局位图优化确保全局一致性；3) 彩色地图评估：提出四种评估指标评估重建质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 提出LiDAR-VGGT框架，首次将LiDAR与VGGT融合；2) 设计预融合模块，通过线性验证和尺度RANSAC精炼VGGT位姿；3) 引入基于边界框正则化的跨模态Sim(3)配准，解决视场不一致导致的尺度失真；4) 提出新的彩色点云评估工具。相比之前工作：1) 比纯LIVO方法生成更密集点云且对校准误差不那么敏感；2) 比纯VGGT方法提供真实度量尺度和更好全局一致性；3) 比其他融合方法直接处理RGB图像，效率更高；4) 专门评估彩色点云质量，不仅关注几何质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LiDAR-VGGT通过创新的粗到细跨模态融合方法，成功将LiDAR的真实世界度量信息与VGGT的密集彩色重建能力相结合，实现了大规模、全局一致且具有准确度量尺度的彩色点云地图重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing large-scale colored point clouds is an important task inrobotics, supporting perception, navigation, and scene understanding. Despiteadvances in LiDAR inertial visual odometry (LIVO), its performance remainshighly sensitive to extrinsic calibration. Meanwhile, 3D vision foundationmodels, such as VGGT, suffer from limited scalability in large environments andinherently lack metric scale. To overcome these limitations, we proposeLiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry withthe state-of-the-art VGGT model through a two-stage coarse- to-fine fusionpipeline: First, a pre-fusion module with robust initialization refinementefficiently estimates VGGT poses and point clouds with coarse metric scalewithin each session. Then, a post-fusion module enhances cross-modal 3Dsimilarity transformation, using bounding-box-based regularization to reducescale distortions caused by inconsistent FOVs between LiDAR and camera sensors.Extensive experiments across multiple datasets demonstrate that LiDAR-VGGTachieves dense, globally consistent colored point clouds and outperforms bothVGGT-based methods and LIVO baselines. The implementation of our proposed novelcolor point cloud evaluation toolkit will be released as open source.</description>
      <author>example@mail.com (Lijie Wang, Lianjie Guo, Ziyi Xu, Qianhao Wang, Fei Gao, Xieyuanli Chen)</author>
      <guid isPermaLink="false">2511.01186v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies</title>
      <link>http://arxiv.org/abs/2511.00998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025. Project page:  https://ziyeeee.github.io/gaudp.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出GauDP，一种新的高斯图像协同表示方法，用于在多智能体协作系统中实现可扩展的、感知感知的模仿学习。该方法通过从分散的RGB观测构建全局一致的3D高斯场，并动态将3D高斯属性重新分配给各智能体的局部视角，使智能体能够保持各自视角的同时从共享场景表示中查询任务关键特征。&lt;h4&gt;背景&lt;/h4&gt;在具身多智能体系统中实现有效协调仍然是一个基本挑战，特别是在智能体必须平衡个体视角与全局环境感知的场景中。现有方法往往难以平衡细粒度的局部控制和全面的场景理解，导致可扩展性有限且协作质量受损。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的表示方法，使多智能体系统能够同时实现细粒度控制和全局连贯的行为，而无需额外的感知模式（如3D点云）。&lt;h4&gt;方法&lt;/h4&gt;GauDP方法包括：1) 从分散的RGB观测构建全局一致的3D高斯场；2) 动态将3D高斯属性重新分配给每个智能体的局部视角；3) 使所有智能体能够从共享场景表示中自适应查询任务关键特征，同时保持各自的视角。&lt;h4&gt;主要发现&lt;/h4&gt;在RoboFactory基准测试（包括多种多臂操作任务）上评估，GauDP方法比现有基于图像的方法表现出优越的性能，接近点云驱动方法的有效性，同时随着智能体数量的增加保持强大的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;GauDP提供了一种新的表示方法，能够在多智能体协作系统中实现细粒度控制和全局连贯的行为，无需额外的感知模式，且具有良好的可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;最近，在具身多智能体系统中实现有效协调仍然是一个基本挑战，特别是在智能体必须平衡个体视角与全局环境感知的场景中。现有方法往往难以平衡细粒度的局部控制和全面的场景理解，导致可扩展性有限且协作质量受损。在本文中，我们提出了GauDP，一种新的高斯图像协同表示方法，用于在多智能体协作系统中实现可扩展的、感知感知的模仿学习。具体来说，GauDP从分散的RGB观测构建全局一致的3D高斯场，然后动态将3D高斯属性重新分配给每个智能体的局部视角。这使得所有智能体能够在保持各自视角的同时从共享场景表示中自适应查询任务关键特征。这种设计实现了细粒度控制和全局连贯的行为，而无需额外的感知模式（如3D点云）。我们在RoboFactory基准测试上评估了GauDP，该测试包括多种多臂操作任务。我们的方法比现有基于图像的方法表现出优越的性能，接近点云驱动方法的有效性，同时随着智能体数量的增加保持强大的可扩展性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决具身多智能体系统中的有效协调问题，特别是在智能体需要平衡个体视角与全局环境感知的场景中。这个问题在现实中非常重要，因为许多实际应用（如工业装配、手术机器人和辅助家务任务）需要多个智能体协调工作。如果智能体之间不能有效协调，可能会导致碰撞或任务中断等灾难性失败。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了多智能体控制的两种主要范式：聚合所有智能体的局部观察或使用全局环境观察。发现第一种方法无法捕捉联合协作状态，第二种方法缺乏高分辨率细节。作者借鉴了3D高斯溅射技术用于3D场景重建，以及扩散策略框架用于动作生成。他们设计了一个统一的图像-高斯表示框架，通过3D高斯场构建全局一致表示，然后动态分配给各智能体。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过3D高斯表示融合局部和全局观察，使智能体在保持个体视角的同时，能从共享场景中查询任务关键特征。流程包括：1) 从各智能体的RGB图像构建全局3D高斯场；2) 动态将高斯属性重新分配给各智能体；3) 智能体从共享高斯表示中提取任务特征；4) 使用扩散策略基于融合特征预测动作。具体实现使用Noposplat网络重建3D高斯，通过交叉视图ViT解码器融合信息，并引入深度监督提高保真度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一的图像-高斯表示框架；2) 动态表示选择机制；3) 选择性全局上下文分发；4) 像素级协同策略。相比之前工作，GauDP的不同之处在于：仅使用RGB输入就能达到接近点云方法的性能；能自然扩展到更多智能体；同时提供精细控制和全局一致行为；在RoboFactory基准测试中显著优于现有图像方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GauDP通过3D高斯-图像协同表示，使多智能体系统在仅使用RGB输入的情况下就能实现接近点云方法的协作性能，同时保持良好的可扩展性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, effective coordination in embodied multi-agent systems has remaineda fundamental challenge, particularly in scenarios where agents must balanceindividual perspectives with global environmental awareness. Existingapproaches often struggle to balance fine-grained local control withcomprehensive scene understanding, resulting in limited scalability andcompromised collaboration quality. In this paper, we present GauDP, a novelGaussian-image synergistic representation that facilitates scalable,perception-aware imitation learning in multi-agent collaborative systems.Specifically, GauDP constructs a globally consistent 3D Gaussian field fromdecentralized RGB observations, then dynamically redistributes 3D Gaussianattributes to each agent's local perspective. This enables all agents toadaptively query task-critical features from the shared scene representationwhile maintaining their individual viewpoints. This design facilitates bothfine-grained control and globally coherent behavior without requiringadditional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on theRoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Ourmethod achieves superior performance over existing image-based methods andapproaches the effectiveness of point-cloud-driven methods, while maintainingstrong scalability as the number of agents increases.</description>
      <author>example@mail.com (Ziye Wang, Li Kang, Yiran Qin, Jiahua Ma, Zhanglin Peng, Lei Bai, Ruimao Zhang)</author>
      <guid isPermaLink="false">2511.00998v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow</title>
      <link>http://arxiv.org/abs/2511.00977v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 15 figures, to appear in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究介绍了一种名为NicheFlow的基于流的生成模型，用于推断细胞微环境在时空数据中的演化轨迹。&lt;h4&gt;背景&lt;/h4&gt;理解时空数据中细胞微环境的演化对于解析组织发育和疾病进展至关重要。当前模拟细胞进化的方法在单细胞水平上操作，忽略了组织中细胞状态的协调发育。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够推断细胞微环境在连续空间切片上的时间轨迹的方法，克服现有单细胞水平方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;NicheFlow是一种基于流的生成模型，通过将局部细胞邻域表示为点云，使用最优传输和变分流匹配联合建模细胞状态和空间坐标的演化。&lt;h4&gt;主要发现&lt;/h4&gt;NicheFlow成功从多样化的时空数据集中恢复了全局空间架构和局部微环境组成，从胚胎发育到大脑发育。&lt;h4&gt;结论&lt;/h4&gt;NicheFlow能够有效地建模细胞微环境的时空演化，为理解组织发育和疾病进展提供了新的工具。&lt;h4&gt;翻译&lt;/h4&gt;理解时空数据中细胞微环境的演化对于解析组织发育和疾病进展至关重要。虽然像空间转录组学这样的实验技术现在能够在时空上实现组织组织的高分辨率映射，但当前模拟细胞进化的方法在单细胞水平上操作，忽略了组织中细胞状态的协调发育。我们介绍了NicheFlow，一种基于流的生成模型，用于推断细胞微环境在连续空间切片上的时间轨迹。通过将局部细胞邻域表示为点云，NicheFlow使用最优传输和变分流匹配联合建模细胞状态和空间坐标的演化。我们的方法成功从多样化的时空数据集中恢复了全局空间架构和局部微环境组成，从胚胎发育到大脑发育。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the evolution of cellular microenvironments in spatiotemporaldata is essential for deciphering tissue development and disease progression.While experimental techniques like spatial transcriptomics now enablehigh-resolution mapping of tissue organization across space and time, currentmethods that model cellular evolution operate at the single-cell level,overlooking the coordinated development of cellular states in a tissue. Weintroduce NicheFlow, a flow-based generative model that infers the temporaltrajectory of cellular microenvironments across sequential spatial slides. Byrepresenting local cell neighborhoods as point clouds, NicheFlow jointly modelsthe evolution of cell states and spatial coordinates using optimal transportand Variational Flow Matching. Our approach successfully recovers both globalspatial architecture and local microenvironment composition across diversespatiotemporal datasets, from embryonic to brain development.</description>
      <author>example@mail.com (Kristiyan Sakalyan, Alessandro Palma, Filippo Guerranti, Fabian J. Theis, Stephan Günnemann)</author>
      <guid isPermaLink="false">2511.00977v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model</title>
      <link>http://arxiv.org/abs/2511.00940v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the 39th Conference on Neural Information Processing  Systems (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为URDF-Anything的端到端自动重建框架，基于3D多模态大语言模型，用于构建关节物体的精确数字孪生，显著提高了几何分割、运动学参数预测和物理执行能力。&lt;h4&gt;背景&lt;/h4&gt;构建关节物体的精确数字孪生对于机器人模拟训练和具身AI世界模型构建至关重要，但传统方法需要繁琐的手动建模或多阶段流程。&lt;h4&gt;目的&lt;/h4&gt;开发一种端到端的自动重建框架，简化关节物体数字孪生的构建过程，提高其准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出URDF-Anything框架，基于3D多模态大语言模型，利用点云和文本多模态输入的自回归预测框架联合优化几何分割和运动学参数预测，实现专门的[SEG]令牌机制与点云特征交互。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该方法在几何分割(mIoU提高17%)、运动学参数预测(平均误差减少29%)和物理执行能力(比基线提高50%)方面显著优于现有方法，且表现出优秀的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;URDF-Anything为构建机器人模拟的数字孪生提供了高效解决方案，显著提高了模拟到现实的迁移能力。&lt;h4&gt;翻译&lt;/h4&gt;构建关节物体的精确数字孪生对于机器人模拟训练和具身AI世界模型构建至关重要，但传统上需要繁琐的手动建模或多阶段流程。在这项工作中，我们提出了URDF-Anything，一种基于3D多模态大语言模型的端到端自动重建框架。URDF-Anything利用基于点云和文本多模态输入的自回归预测框架，联合优化几何分割和运动学参数预测。它实现了一个专门的[SEG]令牌机制，直接与点云特征交互，实现细粒度的部件级分割，同时保持与运动学参数预测的一致性。在模拟和真实世界数据集上的实验表明，我们的方法在几何分割(mIoU提高17%)、运动学参数预测(平均误差减少29%)和物理执行能力(比基线提高50%)方面显著优于现有方法。值得注意的是，我们的方法表现出优秀的泛化能力，即使在训练集外的物体上也能良好表现。这项工作为构建机器人模拟的数字孪生提供了高效解决方案，显著提高了模拟到现实的迁移能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决铰接物体（如门、抽屉、剪刀等具有内部连接结构的物体）的数字孪生自动重建问题。传统方法需要繁琐的手动建模或多阶段处理流程，而本文提出的方法可以直接从视觉输入（单视图或多视图图像）自动生成功能性的URDF模型。这个问题在机器人仿真训练、具身AI世界模型构建、自动驾驶和交互式虚拟/增强现实环境中至关重要，因为这些应用需要精确的物体表示来进行准确的物理模拟和交互。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：之前的铰接物体重建方法要么依赖给定的网格资产库，要么涉及单独的部件分割阶段，无法实现端到端的处理。作者设计了一个基于3D多模态大语言模型（MLLM）的框架，利用其处理多模态输入的能力、大规模预训练获取的3D形状先验以及直接理解空间关系的能力。该方法借鉴了ShapeLLM作为骨干网络，并创新性地应用了LISA中的[SEG]标记机制来实现符号铰接结构与几何分割的同步预测。在点云生成方面，作者使用了DUSt3R（多视图）和LGM（单视图）等现有方法，但在整体框架上进行了创新设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用3D多模态大语言模型的能力，通过特殊的[SEG]标记机制实现符号铰接结构与几何分割的联合预测，确保预测的运动学与重建几何之间的一致性。整体流程包括：1)输入表示：将视觉输入转换为3D点云；2)多模态铰接解析：3D MLLM联合预测部件分割和运动学参数；3)几何分割：通过[SEG]标记机制对点云进行精细分割；4)网格转换和URDF生成：将分割结果和运动学参数整合为标准URDF文件。这种方法实现了从原始视觉输入到可直接用于物理模拟的完整URDF模型的端到端处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个用于铰接物体重建的端到端3D MLLM框架；2)[SEG]标记机制实现几何分割和运动学参数的深度耦合与联合预测；3)端到端训练确保几何与运动学之间的一致性；4)强大的泛化能力，在训练集外物体上表现优异。相比之前的工作，本文直接使用原始3D点云作为输入而非简化表示（如OBB），采用端到端处理而非多阶段流水线，同时预测几何和运动学参数而非分别处理，并通过创新机制确保两者之间的一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了URDF-Anything，一种基于3D多模态大语言模型的端到端框架，能够从视觉输入自动重建铰接物体的功能URDF数字孪生，实现了几何分割和运动学参数的高精度联合预测，显著提升了模拟到现实转换的能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Constructing accurate digital twins of articulated objects is essential forrobotic simulation training and embodied AI world model building, yethistorically requires painstaking manual modeling or multi-stage pipelines. Inthis work, we propose \textbf{URDF-Anything}, an end-to-end automaticreconstruction framework based on a 3D multimodal large language model (MLLM).URDF-Anything utilizes an autoregressive prediction framework based onpoint-cloud and text multimodal input to jointly optimize geometricsegmentation and kinematic parameter prediction. It implements a specialized$[SEG]$ token mechanism that interacts directly with point cloud features,enabling fine-grained part-level segmentation while maintaining consistencywith the kinematic parameter predictions. Experiments on both simulated andreal-world datasets demonstrate that our method significantly outperformsexisting approaches regarding geometric segmentation (mIoU 17\% improvement),kinematic parameter prediction (average error reduction of 29\%), and physicalexecutability (surpassing baselines by 50\%). Notably, our method exhibitsexcellent generalization ability, performing well even on objects outside thetraining set. This work provides an efficient solution for constructing digitaltwins for robotic simulation, significantly enhancing the sim-to-real transfercapability.</description>
      <author>example@mail.com (Zhe Li, Xiang Bai, Jieyu Zhang, Zhuangzhe Wu, Che Xu, Ying Li, Chengkai Hou, Shanghang Zhang)</author>
      <guid isPermaLink="false">2511.00940v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Persistence-Based Statistics for Detecting Structural Changes in High-Dimensional Point Clouds</title>
      <link>http://arxiv.org/abs/2511.00938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 3 figures, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了持久性统计在分布变化下的概率行为，并提出了一种用于检测高维随机点云结构变化的新非参数框架。&lt;h4&gt;背景&lt;/h4&gt;持久性统计在拓扑数据分析中用于研究点云数据的结构特性，但在分布变化下的行为尚需深入研究。&lt;h4&gt;目的&lt;/h4&gt;建立持久性统计在一般分布下的理论性质，并开发一种能够检测高维点云结构变化的统计方法。&lt;h4&gt;方法&lt;/h4&gt;建立经典持久性统计量（总持久性和最大持久性）的矩界和紧性结果，引入基于持久性景观与Jensen-Shannon散度的标准化统计量，并证明其Hölder连续性。&lt;h4&gt;主要发现&lt;/h4&gt;持久性统计在高斯混合模型中具有明确的尺度行为，所提出的统计量具有稳定性、尺度和位移不变性，能够通过置换测试进行非参数推断。&lt;h4&gt;结论&lt;/h4&gt;该方法能够捕获制度转变和演化的几何复杂性，为随机持久性的理论理解和复杂高维系统中拓扑变化的检测提供了严格的统计基础。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了持久性统计在分布变化下的概率行为，并提出了一种用于检测高维随机点云结构变化的新非参数框架。我们首先在一般分布下建立了经典持久性统计量（总持久性和最大持久性）的矩界和紧性结果，并为高斯混合模型推导了明确的尺度行为。基于这些理论基础，我们引入了一种结合持久性景观和Jensen-Shannon散度的标准化统计量，并证明了它相对于输入点云扰动的Hölder连续性。所得的测度是稳定的、尺度和位移不变的，并通过置换测试适合非参数推断。使用去中心化治理数据的动态属性向量进行的数值说明展示了所提出的方法如何能够捕获制度转变和演化的几何复杂性。我们的结果为随机持久性的理论理解做出了贡献，并为复杂高维系统中拓扑变化的检测提供了严格的统计基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the probabilistic behavior of persistence statistics underdistributional variability and propose a novel nonparametric framework fordetecting structural changes in high-dimensional random point clouds. We firstestablish moment bounds and tightness results for classical persistencestatistics - total and maximum persistence - under general distributions, withexplicit scaling behavior derived for Gaussian mixture models. Building onthese theoretical foundations, we introduce a normalized statistic based onpersistence landscapes combined with the Jensen-Shannon divergence, and weprove its Holder continuity with respect to perturbations of input pointclouds. The resulting measure is stable, scale- and shift-invariant, andsuitable for nonparametric inference via permutation testing. A numericalillustration using dynamic attribute vectors from decentralized governance datademonstrates how the proposed method can capture regime shifts and evolvinggeometric complexity. Our results contribute to the theoretical understandingof random persistence and provide a rigorous statistical foundation fortopological change-point detection in complex, high-dimensional systems.</description>
      <author>example@mail.com (Toshiyuki Nakayama)</author>
      <guid isPermaLink="false">2511.00938v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Neural Green's Functions</title>
      <link>http://arxiv.org/abs/2511.01924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为'Neural Green's Function'的神经网络解决方案算子，用于求解具有特征分解的线性偏微分方程。&lt;h4&gt;背景&lt;/h4&gt;Green函数是线性偏微分方程的解算子，它们仅依赖于域的几何形状。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够模仿Green函数行为的神经网络，实现在不同不规则几何形状和源函数及边界条件下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;Neural Green's Function从表示问题域的体积点云中提取逐点特征，并使用它们来预测解算子的分解，然后通过数值积分评估解。&lt;h4&gt;主要发现&lt;/h4&gt;该框架对训练中使用的特定函数不敏感，能够实现稳健高效的泛化。在MCB数据集上对机械零件几何形状的稳态热分析中，Neural Green's Function优于最先进的神经算子，在五个形状类别上平均误差减少了13.9%，比需要计算密集网格化的数值求解器快350倍。&lt;h4&gt;结论&lt;/h4&gt;Neural Green's Function是一种有效的神经网络解决方案算子，能够处理线性偏微分方程，并在不同几何形状和条件下实现良好的泛化性能。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了神经格林函数，一种用于线性偏微分方程的神经网络解算子，其微分算子允许特征分解。受格林函数的启发，线性偏微分方程的解算子仅依赖于域的几何形状，我们设计了神经格林函数来模仿它们的行为，实现在各种不规则几何形状、源函数和边界函数上的优越泛化能力。具体而言，神经格林函数从表示问题域的体积点云中提取逐点特征，并使用它们来预测解算子的分解，随后通过数值积分应用来评估解。与最近基于学习的解算子不同，这些解算子通常难以泛化到未见过的源函数或边界函数，我们的框架在设计上对训练中使用的特定函数不敏感，能够实现稳健和高效的泛化。在MCB数据集中机械零件几何形状的稳态热分析中，神经格林函数优于最先进的神经算子，在五个形状类别上平均误差减少了13.9%，而比需要计算密集网格化的数值求解器快350倍。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决线性偏微分方程(PDEs)求解的问题，特别是那些微分算子可以进行特征分解的线性PDEs（如泊松方程和双调和方程）。这个问题在现实和研究中非常重要，因为PDE在科学和工程领域有广泛应用，包括热分析、静电学、流体动力学和弹性力学等。传统数值求解方法（如有限元法）依赖于计算密集型的网格生成过程，这限制了在工程设计早期阶段的快速迭代评估。现有学习求解器虽然不需要网格，但往往难以推广到未见过的源函数和边界函数，当问题域、源函数和边界函数同时变化时表现不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到Green函数的启发，Green函数是线性PDE的解算子，仅依赖于问题域的几何形状而不依赖于特定的源函数或边界函数。作者基于线性PDE解的数学表达式，利用微分算子的特征分解性质，将解算子表示为特征向量和特征值的乘积。设计神经网络仅从域几何中提取特征，而不依赖于特定的源函数或边界函数。作者借鉴了Green函数理论、神经算子（如GNO、FNO）以及Transolver的网络架构，同时扩展了先前学习Green函数的工作（如Boullé和Teng等人的工作），使其能够处理更复杂的几何形状。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计一个仅从问题域几何中提取特征的神经网络，使用这些特征近似Green函数的特征分解，预测必要的微分量（如质量矩阵）以进行数值积分，从而实现解的计算。整体实现流程如下：1) 输入表示问题域几何的查询点；2) 使用神经网络（基于Transolver架构）从查询点坐标提取特征；3) 使用提取的特征构造神经Green函数，作为真实Green函数的近似；4) 预测每个顶点的质量值和算子的子矩阵；5) 通过基于Green函数的数值积分公式计算解；6) 通过最小化预测解与真实解之间的误差以及质量预测的正则化项来优化网络参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 仅从域几何提取特征，不将源函数和边界函数作为输入，使模型能够推广到未见过的函数；2) 利用Green函数的特征分解性质设计更有效的表示；3) 预测必要的微分量（如质量矩阵），使方法能够处理复杂几何形状；4) 专注于可以特征分解的线性PDE，如泊松方程和双调和方程。相比之前的工作，该方法比传统数值求解器快350倍（不需要网格生成），比PINNs不需要为每个问题实例重新训练，比神经算子（如GNO、FNO）能够更好地推广到未见过的源函数和边界函数，比先前学习Green函数的工作能够处理更复杂的几何形状且不需要为每个域重新训练。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种神经Green函数方法，通过仅从问题域几何中提取特征并近似Green函数的特征分解，实现了对线性偏微分方程的高效求解，能够推广到各种不规则几何形状和未见过的源函数、边界函数，且比传统数值求解器快350倍。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Neural Green's Function, a neural solution operator for linearpartial differential equations (PDEs) whose differential operators admiteigendecompositions. Inspired by Green's functions, the solution operators oflinear PDEs that depend exclusively on the domain geometry, we design NeuralGreen's Function to imitate their behavior, achieving superior generalizationacross diverse irregular geometries and source and boundary functions.Specifically, Neural Green's Function extracts per-point features from avolumetric point cloud representing the problem domain and uses them to predicta decomposition of the solution operator, which is subsequently applied toevaluate solutions via numerical integration. Unlike recent learning-basedsolution operators, which often struggle to generalize to unseen source orboundary functions, our framework is, by design, agnostic to the specificfunctions used during training, enabling robust and efficient generalization.In the steady-state thermal analysis of mechanical part geometries from the MCBdataset, Neural Green's Function outperforms state-of-the-art neural operators,achieving an average error reduction of 13.9\% across five shape categories,while being up to 350 times faster than a numerical solver that requirescomputationally expensive meshing.</description>
      <author>example@mail.com (Seungwoo Yoo, Kyeongmin Yeo, Jisung Hwang, Minhyuk Sung)</author>
      <guid isPermaLink="false">2511.01924v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset</title>
      <link>http://arxiv.org/abs/2511.00653v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了FGI-EMIT，首个用于单木分割的大规模多谱段机载激光扫描基准数据集，并比较了传统无监督算法与深度学习方法在树木分割任务上的性能表现。&lt;h4&gt;背景&lt;/h4&gt;单木分割(LiDAR点云)是森林资源清查、碳监测和生物多样性评估的基础应用。传统方法采用无监督几何算法，近期转向监督深度学习。过去因缺乏大规模基准数据集限制了方法发展，尽管多光谱反射率能提高分割准确性，但多光谱LiDAR数据格式仍然有限。&lt;h4&gt;目的&lt;/h4&gt;创建首个用于单木分割的大规模多谱段机载激光扫描基准数据集，并全面评估不同算法的性能。&lt;h4&gt;方法&lt;/h4&gt;FGI-EMIT数据集在532、905和1,550 nm波长处捕获，包含1,561个手动标注的树木，特别关注小林下树木。研究评估了四种传统无监督算法和四种监督深度学习方法，其中无监督方法使用贝叶斯优化超参数，深度学习模型从头训练。&lt;h4&gt;主要发现&lt;/h4&gt;无监督方法中Treeiso表现最佳，F1分数为52.7%；深度学习方法整体显著更好，最佳模型ForestFormer3D达到73.3%的F1分数。林下树木性能差异最显著，ForestFormer3D比Treeiso高出25.9个百分点。当前深度学习方法未能有效利用多谱段反射率信息，但单通道反射率可略微提高准确性，特别是对林下树木。即使点密度低至10点/m²，深度学习方法仍优于无监督算法。&lt;h4&gt;结论&lt;/h4&gt;深度学习方法在树木分割任务上显著优于传统无监督方法，多光谱数据有提高分割准确性的潜力，但当前深度学习模型未能充分利用这些信息。即使在低点密度条件下，深度学习方法也保持优势。&lt;h4&gt;翻译&lt;/h4&gt;从激光雷达点云中进行单木分割(ITS)对于森林资源清查、碳监测和生物多样性评估等应用至关重要。传统上，ITS通过无监督的几何算法实现，而最近的进展已转向监督深度学习(DL)。过去，由于缺乏大规模基准数据集，方法开发进展受限，尽管有证据表明多光谱(MS)反射率可以提高ITS的准确性，但新型数据格式(特别是多光谱激光雷达)的可用性至今仍然有限。本研究引入了FGI-EMIT，这是首个用于ITS的大规模多谱段机载激光扫描基准数据集。该数据集在532、905和1,550 nm波长处捕获，包含1,561个手动标注的树木，特别关注小林下树木。利用FGI-EMIT，我们全面评估了四种传统无监督算法和四种监督深度学习方法。无监督方法的超参数使用贝叶斯方法优化，而深度学习模型从头开始训练。在无监督方法中，Treeiso实现了最高的测试集F1分数，为52.7%。深度学习方法整体表现显著更好，最佳模型ForestFormer3D达到了73.3%的F1分数。林下树木观察到最显著的差异，ForestFormer3D比Treeiso高出25.9个百分点。消融研究表明，当多谱段反射率作为额外输入特征提供时，当前的基于深度学习的方法通常无法有效利用这些信息，尽管单通道反射率可以略微提高准确性，特别是对于林下树木。在不同点密度下的性能分析进一步表明，即使点密度低至10点/m²，深度学习方法仍然始终优于无监督算法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决个体树木分割(ITS)方法的评估和比较问题，特别是缺乏大规模多光谱激光扫描基准数据集的挑战。这个问题在现实中很重要，因为准确的树木分割是林业调查、碳监测和生物多样性评估的基础应用，而缺乏标准化的评估框架使得研究人员和从业者难以选择最适合特定应用的方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有数据集的局限性，如缺乏多光谱数据和高质量3D标注，然后设计了FGI-EMIT数据集。他们借鉴了FOR-Instance等数据集的经验，但增加了多光谱数据和更详细的标注；在评估方法上借鉴了计算机视觉领域的3D IoU指标；在超参数优化上使用了贝叶斯优化；在数据分割上采用了分层随机采样确保代表性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模、高质量的多光谱激光扫描数据集，用于系统性地评估传统无监督算法和深度学习方法。整体流程包括：1)使用多波长激光扫描仪采集数据；2)预处理和合并数据；3)手动标注树木实例和语义信息；4)计算树木位置和高度；5)将树木分为四种类别；6)分割数据集为训练集和测试集；7)使用3D IoU指标评估多种方法的性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建了第一个大规模多光谱激光扫描个体树木分割基准数据集FGI-EMIT；2)进行了全面的性能比较，评估了四种传统方法和四种深度学习方法；3)首次研究了多光谱信息在深度学习方法中的利用。相比之前的工作，FGI-EMIT是第一个包含城市环境人工结构的多光谱数据集，并且提供了系统性的超参数优化和多种点云密度下的性能评估。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建首个大规模多光谱激光扫描个体树木分割基准数据集并进行全面的性能比较，证明了深度学习方法显著优于传统无监督算法，同时揭示了当前深度学习方法未能充分利用多光谱信息的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Individual tree segmentation (ITS) from LiDAR point clouds is fundamental forapplications such as forest inventory, carbon monitoring and biodiversityassessment. Traditionally, ITS has been achieved with unsupervisedgeometry-based algorithms, while more recent advances have shifted towardsupervised deep learning (DL). In the past, progress in method development washindered by the lack of large-scale benchmark datasets, and the availability ofnovel data formats, particularly multispectral (MS) LiDAR, remains limited tothis day, despite evidence that MS reflectance can improve the accuracy of ITS.This study introduces FGI-EMIT, the first large-scale MS airborne laserscanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550nm, the dataset consists of 1,561 manually annotated trees, with a particularfocus on small understory trees. Using FGI-EMIT, we comprehensively benchmarkedfour conventional unsupervised algorithms and four supervised DL approaches.Hyperparameters of unsupervised methods were optimized using a Bayesianapproach, while DL models were trained from scratch. Among the unsupervisedmethods, Treeiso achieved the highest test set F1-score of 52.7%. The DLapproaches performed significantly better overall, with the best model,ForestFormer3D, attaining an F1-score of 73.3%. The most significant differencewas observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9percentage points. An ablation study demonstrated that current DL-basedapproaches generally fail to leverage MS reflectance information when it isprovided as additional input features, although single channel reflectance canimprove accuracy marginally, especially for understory trees. A performanceanalysis across point densities further showed that DL methods consistentlyremain superior to unsupervised algorithms, even at densities as low as 10points/m$^2$.</description>
      <author>example@mail.com (Lassi Ruoppa, Tarmo Hietala, Verneri Seppänen, Josef Taher, Teemu Hakala, Xiaowei Yu, Antero Kukko, Harri Kaartinen, Juha Hyyppä)</author>
      <guid isPermaLink="false">2511.00653v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Been There, Scanned That: Nostalgia-Driven LiDAR Compression for Self-Driving Cars</title>
      <link>http://arxiv.org/abs/2511.00652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DejaView的3D点云数据压缩方法，通过利用自动驾驶车辆在大时间尺度上的数据冗余，实现了高效的数据压缩。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆每天可产生数TB传感器数据，其中大量是由LiDAR等深度传感器生成的3D点云数据。这些数据需传输到云端用于机器学习模型训练或事故分析，但网络和存储成本高昂。&lt;h4&gt;目的&lt;/h4&gt;减少自动驾驶车辆3D点云数据的网络传输和存储成本，提高数据压缩效率。&lt;h4&gt;方法&lt;/h4&gt;DejaView利用自动驾驶车辆活动区域有限且主要行驶固定路线的特点，在更大的时间尺度（天和月）上寻找数据冗余，而非传统的帧间冗余。其核心是一个diff操作，将点云紧凑地表示为相对于过去3D数据的增量。&lt;h4&gt;主要发现&lt;/h4&gt;使用两个月LiDAR数据的测试表明，DejaView的端到端实现可将点云压缩210倍，同时保持仅15厘米的重构误差。&lt;h4&gt;结论&lt;/h4&gt;DejaView是一种有效的3D点云数据压缩方法，特别适用于自动驾驶车辆场景，能够显著降低数据存储和传输成本。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶车辆每天可以产生数TB的传感器数据。其中很大一部分是由深度传感器（如LiDAR）产生的3D点云数据。这些数据必须传输到云存储，用于训练机器学习模型或进行分析，例如在发生事故时进行取证调查。为了减少网络和存储成本，本文介绍了DejaView。尽管先前的工作使用帧间冗余来压缩数据，但DejaView在更大的时间尺度（天和月）上搜索并利用冗余，以实现更有效的压缩。我们基于自动驾驶车辆的活动区域有限且主要每天行驶相同路线的洞察设计了DejaView。因此，车辆每天收集的3D数据可能与过去捕获的数据相似。为此，DejaView的核心是一个diff操作，将点云紧凑地表示为相对于过去3D数据的增量。使用两个月的LiDAR数据，DejaView的端到端实现可以在仅15厘米的重构误差下将点云压缩210倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; An autonomous vehicle can generate several terabytes of sensor data per day.A significant portion of this data consists of 3D point clouds produced bydepth sensors such as LiDARs. This data must be transferred to cloud storage,where it is utilized for training machine learning models or conductinganalyses, such as forensic investigations in the event of an accident. Toreduce network and storage costs, this paper introduces DejaView. Althoughprior work uses interframe redundancies to compress data, DejaView searches forand uses redundancies on larger temporal scales (days and months) for moreeffective compression. We designed DejaView with the insight that the operatingarea of autonomous vehicles is limited and that vehicles mostly traverse thesame routes daily. Consequently, the 3D data they collect daily is likelysimilar to the data they have captured in the past. To capture this, the coreof DejaView is a diff operation that compactly represents point clouds as deltaw.r.t. 3D data from the past. Using two months of LiDAR data, an end-to-endimplementation of DejaView can compress point clouds by a factor of 210 at areconstruction error of only 15 cm.</description>
      <author>example@mail.com (Ali Khalid, Jaiaid Mobin, Sumanth Rao Appala, Avinash Maurya, Stephany Berrio Perez, M. Mustafa Rafique, Fawad Ahmad)</author>
      <guid isPermaLink="false">2511.00652v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Three-dimensional narrow volume reconstruction method with unconditional stability based on a phase-field Lagrange multiplier approach</title>
      <link>http://arxiv.org/abs/2511.00508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint, 30+ pages; multiple figures and tables; code and data:  https://github.com/cfdyang521/C-3PO/tree/main; intended for submission to a  computational mathematics journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于Allen-Cahn模型的有效点云重建算法，采用拉格朗日乘子法，通过增强的边缘检测函数重建窄壳结构。&lt;h4&gt;背景&lt;/h4&gt;从点云重建物体在假肢、医学成像、计算机视觉等领域非常重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的基于Allen-Cahn模型的重建算法。&lt;h4&gt;方法&lt;/h4&gt;采用拉格朗日乘子法，利用物体的散乱数据点，通过求解增强有边缘检测函数的控制方程来重建窄壳；边缘检测函数基于无符号距离函数设计；使用Crank-Nicolson时间离散化和有限差分法近似空间运算。&lt;h4&gt;主要发现&lt;/h4&gt;算法可以稳定和解耦地更新解；全离散方案被证明是无条件稳定的；复杂3D体积重建实验验证了算法的准确性、稳定性和有效性。&lt;h4&gt;结论&lt;/h4&gt;分析了特定参数选择如何影响重建体积的细节和精细度；分享了计算代码和数据以便其他研究者理解和使用该算法。&lt;h4&gt;翻译&lt;/h4&gt;从点云重建物体在假肢、医学成像、计算机视觉等领域至关重要。我们提出了一种基于Allen-Cahn模型的有效重建算法，采用拉格朗日乘子法。利用物体的散乱数据点，我们通过求解增强有从无符号距离函数导出的边缘检测函数的控制方程来重建窄壳。特别设计的边缘检测函数确保了能量稳定性。通过拉格朗日乘子技术重新表述控制方程并实施Crank-Nicolson时间离散化，我们可以以稳定和解耦的方式更新解。空间运算使用有限差分法近似，我们通过分析证明了全离散方案的无条件稳定性。包括重建《星球大战》中的字符等复杂3D体积在内的全面数值实验，验证了算法的准确性、稳定性和有效性。此外，我们分析了特定参数选择如何影响重建体积的细节和精细度。为了便于感兴趣的读者理解我们的算法，我们在https://github.com/cfdyang521/C-3PO/tree/main分享了计算代码和数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstruction of an object from points cloud is essential in prosthetics,medical imaging, computer vision, etc. We present an effective algorithm for anAllen--Cahn-type model of reconstruction, employing the Lagrange multiplierapproach. Utilizing scattered data points from an object, we reconstruct anarrow shell by solving the governing equation enhanced with an edge detectionfunction derived from the unsigned distance function. The specifically designededge detection function ensures the energy stability. By reformulating thegoverning equation through the Lagrange multiplier technique and implementing aCrank--Nicolson time discretization, we can update the solutions in a stableand decoupled manner. The spatial operations are approximated using the finitedifference method, and we analytically demonstrate the unconditional stabilityof the fully discrete scheme. Comprehensive numerical experiments, includingreconstructions of complex 3D volumes such as characters from \textit{StarWars}, validate the algorithm's accuracy, stability, and effectiveness.Additionally, we analyze how specific parameter selections influence the levelof detail and refinement in the reconstructed volumes. To facilitate theinterested readers to understand our algorithm, we share the computationalcodes and data in https://github.com/cfdyang521/C-3PO/tree/main.</description>
      <author>example@mail.com (Renjun Gao, Xiangjie Kong, Dongting Cai, Boyi Fu, Junxiang Yang)</author>
      <guid isPermaLink="false">2511.00508v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Dataset for Indoor Radio Mapping with 3D Point Clouds and RSSI</title>
      <link>http://arxiv.org/abs/2511.00494v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures, 3 tables, under review to Nature Scientific Data&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个多模态数据集，结合高分辨率3D激光雷达扫描与Wi-Fi接收信号强度测量，用于研究室内无线信号传播特性，特别是在不同AP配置和人员存在情况下的动态环境影响。&lt;h4&gt;背景&lt;/h4&gt;随着支持带宽密集型和延迟敏感型应用的智能设备增多，室内环境需要可靠的无线连接。准确的无线电环境图(REMs)估计对自适应网络规划和接入点优化至关重要。&lt;h4&gt;目的&lt;/h4&gt;克服室内空间复杂性导致的真实REMs生成挑战，为数据驱动的无线建模研究提供资源，特别是在IEEE 802.11be(Wi-Fi 7)等新兴高频标准的背景下，促进高容量室内通信系统发展。&lt;h4&gt;方法&lt;/h4&gt;创建并展示了一个多模态数据集，整合了高分辨率3D激光雷达扫描与Wi-Fi RSSI测量数据，在多房间室内环境中，20种不同AP配置下收集，包含无人和有人两种场景的测量数据。&lt;h4&gt;主要发现&lt;/h4&gt;该数据集支持研究动态环境（如人员存在）对无线信号传播的影响，为室内无线通信系统优化提供了基础数据支持。&lt;h4&gt;结论&lt;/h4&gt;所提出的数据集作为研究资源，有助于推动数据驱动的室内无线建模，特别是在高频标准下的通信系统优化，为开发稳健、高容量的室内通信系统奠定基础。&lt;h4&gt;翻译&lt;/h4&gt;随着支持带宽密集型和延迟敏感型应用（如实时视频分析、智能感知和扩展现实XR）的智能设备数量不断增加，室内环境需要可靠的无线连接。在此，准确的无线电环境图(REMs)估计能够支持自适应无线网络规划和接入点(AP)部署优化。然而，由于室内空间的复杂性，生成真实的REMs仍然具有挑战性。为克服这一挑战，本文引入了一个多模态数据集，该数据集集成了高分辨率3D激光雷达扫描与在多房间室内环境中20种不同AP配置下收集的Wi-Fi接收信号强度指示器(RSSI)测量。该数据集捕获了两种测量场景：第一种是环境中无人存在的情况，第二种是有人存在的情况。因此，所提供的数据集支持研究动态环境对无线信号传播的影响。该资源旨在促进数据驱动的无线建模研究，特别是在IEEE 802.11be(Wi-Fi 7)等新兴高频标准的背景下，旨在推动稳健、高容量室内通信系统的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing number of smart devices supporting bandwidth-intensive andlatency-sensitive applications, such as real-time video analytics, smartsensing, and Extended Reality (XR), necessitates reliable wireless connectivityin indoor environments. Therein, accurate estimation of Radio Environment Maps(REMs) enables adaptive wireless network planning and optimization of AccessPoint (AP) placement. However, generating realistic REMs remains challengingdue to the complexity of indoor spaces. To overcome this challenge, this paperintroduces a multimodal dataset that integrates high-resolution 3D LiDAR scanswith Wi-Fi Received Signal Strength Indicator (RSSI) measurements collectedunder 20 distinct AP configurations in a multi-room indoor environment. Thedataset captures two measurement scenarios: the first without human presence inthe environment, and the second with human presence. Thus, the presenteddataset supports the study of dynamic environmental effects on wireless signalpropagation. This resource is designed to facilitate research in data-drivenwireless modeling, particularly in the context of emerging high-frequencystandards such as IEEE 802.11be (Wi-Fi 7), and aims to advance the developmentof robust, high-capacity indoor communication systems.</description>
      <author>example@mail.com (Ljupcho Milosheski, Kuon Akiyama, Blaž Bertalanič, Jernej Hribar, Ryoichi Shinkuma)</author>
      <guid isPermaLink="false">2511.00494v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images</title>
      <link>http://arxiv.org/abs/2511.00120v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted to IEIE( The Institute Of Electronics  and Information Engineering, South Korea) Fall,2025 Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VLM6D是一种新颖的双流架构，利用RGB-D输入中的视觉和几何数据实现鲁棒且精确的6D物体姿态估计，在具有挑战性的Occluded-LineMOD数据集上取得了新的SOTA性能。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉中精确计算6D物体姿态的主要挑战在于当前方法在从合成数据到真实环境的泛化方面存在困难，特别是在光照变化、无纹理物体和严重遮挡的情况下表现脆弱。&lt;h4&gt;目的&lt;/h4&gt;提出VLM6D，一种新颖的双流架构，利用RGB-D输入中的视觉和几何数据的优势，实现鲁棒且精确的姿态估计。&lt;h4&gt;方法&lt;/h4&gt;VLM6D框架集成了两个专门的编码器：一个强大的自监督视觉Transformer（DINOv2）处理RGB模态，利用其丰富的预训练视觉理解能力；一个PointNet++编码器处理深度数据衍生的3D点云，实现鲁棒的几何推理；这两个互补的特征流被有效融合，用于多任务预测头。&lt;h4&gt;主要发现&lt;/h4&gt;通过全面实验，VLM6D在具有挑战性的Occluded-LineMOD数据集上取得了新的SOTA性能，验证了其优越的鲁棒性和准确性。&lt;h4&gt;结论&lt;/h4&gt;VLM6D通过结合视觉和几何信息，成功解决了计算机视觉中6D物体姿态估计的挑战，特别是在处理复杂环境（如光照变化、无纹理物体和严重遮挡）时表现出色。&lt;h4&gt;翻译&lt;/h4&gt;计算机视觉中的主要挑战是精确计算6D物体的姿态，然而许多当前方法仍然脆弱且难以从合成数据泛化到具有变化光照、无纹理物体和严重遮挡的真实世界情况。为解决这些限制，VLM6D是一种新颖的双流架构，它利用RGB-D输入中视觉和几何数据的独特优势进行鲁棒且精确的姿态估计。我们的框架独特地集成了两个专门的编码器：一个强大的自监督视觉Transformer（DINOv2）处理RGB模态，利用其丰富、预训练的视觉语法理解能力，实现对纹理和光照变化的显著抵抗力。同时，一个PointNet++编码器处理从深度数据衍生的3D点云，实现鲁棒的几何推理，即使在严重遮挡情况下典型的稀疏、碎片化数据中也能表现出色。这些互补的特征流被有效融合，为多任务预测头提供信息。我们通过全面实验证明，VLM6D在具有挑战性的Occluded-LineMOD上获得了新的SOTA性能，验证了其优越的鲁棒性和准确性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决计算机视觉中6D物体姿态估计的挑战，特别是处理真实世界中常见的问题如光照变化、无纹理物体和严重遮挡。这个问题非常重要，因为准确的6D姿态估计是机器人抓取、自动驾驶、增强现实和自动化装配等应用的基础能力，使机器能够感知、理解和与周围环境互动。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性（在真实世界复杂情况下表现脆弱）设计了VLM6D。他们借鉴了双流架构思想，分别处理RGB和深度数据，但进行了创新：使用DINOv2处理RGB数据（利用其强大的视觉理解和泛化能力），使用PointNet++处理深度数据（处理几何信息）。作者还借鉴了密集对应点的思想，但通过双流架构和特征融合策略改进了计算效率和准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用双流架构，分别从RGB图像和深度数据中提取互补的特征信息（RGB提供视觉信息，深度提供几何信息），然后融合这些特征进行6D姿态估计。实现流程包括：1)输入处理（RGB图像调整归一化，深度图像转为点云）；2)双流特征提取（DINOv2处理RGB，PointNet++处理点云）；3)特征融合（连接特征向量并通过MLP处理）；4)多任务预测（旋转、平移、置信度和物体分类）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双流架构设计（结合DINOv2和PointNet++）；2)特征融合策略（晚期融合）；3)多任务预测头。相比之前工作（如RDPN6D），VLM6D具有更强的泛化能力（使用自监督学习）、更高效的计算（不需要预测密集坐标图）、更好的鲁棒性（在反射表面、无纹理物体和极端遮挡条件下表现更好），并且能够处理高达80%的物体遮挡。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VLM6D通过创新性地结合自监督视觉Transformer和点云处理网络，实现了在复杂场景下（严重遮挡、光照变化、无纹理物体）更加鲁棒和准确的6D物体姿态估计。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The primary challenge in computer vision is precisely calculating the pose of6D objects, however many current approaches are still fragile and have troublegeneralizing from synthetic data to real-world situations with fluctuatinglighting, textureless objects, and significant occlusions. To address theselimitations, VLM6D, a novel dual-stream architecture that leverages thedistinct strengths of visual and geometric data from RGB-D input for robust andprecise pose estimation. Our framework uniquely integrates two specializedencoders: a powerful, self-supervised Vision Transformer (DINOv2) processes theRGB modality, harnessing its rich, pre-trained understanding of visual grammarto achieve remarkable resilience against texture and lighting variations.Concurrently, a PointNet++ encoder processes the 3D point cloud derived fromdepth data, enabling robust geometric reasoning that excels even with thesparse, fragmented data typical of severe occlusion. These complementaryfeature streams are effectively fused to inform a multi task prediction head.We demonstrate through comprehensive experiments that VLM6D obtained new SOTAperformance on the challenging Occluded-LineMOD, validating its superiorrobustness and accuracy.</description>
      <author>example@mail.com (Md Selim Sarowar, Sungho Kim)</author>
      <guid isPermaLink="false">2511.00120v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>D$^2$GS: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction</title>
      <link>http://arxiv.org/abs/2510.25173v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出D²GS，一个无LiDAR的城市场景重建框架，能够产生比LiDAR更密集、更准确的几何先验，实验表明其性能优于现有方法，甚至超过使用真实LiDAR数据的方法。&lt;h4&gt;背景&lt;/h4&gt;高斯散射在自动驾驶城市场景重建中潜力巨大，但当前方法依赖LiDAR和图像等多模态传感器。LiDAR提供的几何先验虽可减轻重建不适定性，但实践中获取准确LiDAR数据面临挑战：需要精确时空校准且不同传感器位置会导致重投影误差。&lt;h4&gt;目的&lt;/h4&gt;避免获取准确LiDAR深度的困难，开发一种无LiDAR的城市场景重建框架，获得与LiDAR一样有效但更密集、更准确的几何先验。&lt;h4&gt;方法&lt;/h4&gt;D²GS框架包含三个主要部分：首先，通过反向投影多视图度量深度预测初始化密集点云，并用渐进修剪策略优化以提高全局一致性；其次，通过深度增强器联合优化高斯几何和预测深度，利用深度基础模型的扩散先验增强高斯渲染的深度图；最后，约束道路区域内高斯的形状和法线属性以提高地面几何准确性。&lt;h4&gt;主要发现&lt;/h4&gt;Waymo数据集上的大量实验表明，该方法持续优于最先进方法，产生更准确的几何，即使与使用真实LiDAR数据的方法相比也是如此。&lt;h4&gt;结论&lt;/h4&gt;D²GS成功实现了无LiDAR的城市场景重建，能够产生比传统LiDAR方法更密集、更准确的几何先验，性能超越现有最先进方法。&lt;h4&gt;翻译&lt;/h4&gt;最近，高斯散射在自动驾驶领域的城市场景重建中显示出巨大潜力。然而，当前的城市场景重建方法通常依赖于多模态传感器作为输入，即LiDAR和图像。尽管LiDAR点云提供的几何先验可以大大减轻重建中的不适定性，但在实践中获取准确的LiDAR数据仍然具有挑战性：i)需要LiDAR与其他传感器之间的精确时空校准，因为它们可能无法同时捕获数据；ii)当LiDAR和相机安装在不同位置时，空间错位会导致重投影误差。为了避免获取准确LiDAR深度的困难，我们提出了D²GS，一个无LiDAR的城市场景重建框架。在这项工作中，我们获得了与LiDAR一样有效但更密集、更准确的几何先验。首先，我们通过反向投影多视图度量深度预测来初始化密集点云。然后通过渐进修剪策略优化该点云以提高全局一致性。其次，我们通过深度增强器联合优化高斯几何和预测的密集度量深度。具体来说，我们利用来自深度基础模型的扩散先验来增强由高斯渲染的深度图。反过来，增强的深度在高斯训练期间提供更强的几何约束。最后，我们通过约束道路区域内高斯的形状和法线属性来提高地面几何的准确性。在Waymo数据集上的大量实验表明，我们的方法持续优于最先进的方法，产生更准确的几何，即使与使用真实LiDAR数据的方法相比也是如此。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶领域城市场景重建中对LiDAR传感器的依赖问题。这个问题很重要，因为获取LiDAR数据需要昂贵设备、专业车辆，且传感器间精确校准困难，同时LiDAR和相机安装在不同位置会导致重投影误差，这些都限制了实际应用的可扩展性和成本效益。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了LiDAR依赖带来的校准困难和数据获取成本问题，然后意识到需要替代LiDAR的密集几何先验。方法设计上，他们使用多视图深度预测初始化点云，通过渐进式修剪策略优化，借鉴了3DGS的高效特性、扩散模型的深度生成先验以及场景图表示方法来组织不同类型的Gaussian，并结合道路区域的强几何先验知识。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过仅使用相机输入创建不依赖LiDAR的城市街道场景重建框架，利用图像衍生的几何先验替代LiDAR点云。实现流程包括：1)使用多视图深度估计和渐进式修剪初始化紧凑Gaussian表示；2)使用基于扩散的深度增强器进行深度和Gaussian的联合优化；3)在场景图中引入专门的道路节点利用强几何先验显式建模地面平面；4)迭代优化Gaussian参数和深度表示。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)完全LiDAR-free的重建框架；2)渐进式修剪策略从密集点云获得紧凑表示；3)基于扩散的深度增强器实现深度和Gaussian的联合优化；4)专门的道路节点利用强几何先验。相比之前工作，D2GS消除了对LiDAR的依赖和校准误差，避免了单目深度估计的尺度模糊和多视图深度估计不适合动态场景的问题，并通过迭代优化提供了更密集、准确的深度监督。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; D2GS提出了一种不依赖LiDAR的城市街道场景重建框架，通过渐进式修剪、深度增强和道路节点优化，实现了比使用LiDAR数据更准确的几何重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Gaussian Splatting (GS) has shown great potential for urban scenereconstruction in the field of autonomous driving. However, current urban scenereconstruction methods often depend on multimodal sensors as inputs,\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDARpoint clouds can largely mitigate ill-posedness in reconstruction, acquiringsuch accurate LiDAR data is still challenging in practice: i) precisespatiotemporal calibration between LiDAR and other sensors is required, as theymay not capture data simultaneously; ii) reprojection errors arise from spatialmisalignment when LiDAR and cameras are mounted at different locations. Toavoid the difficulty of acquiring accurate LiDAR depth, we propose D$^2$GS, aLiDAR-free urban scene reconstruction framework. In this work, we obtaingeometry priors that are as effective as LiDAR while being denser and moreaccurate. $\textbf{First}$, we initialize a dense point cloud byback-projecting multi-view metric depth predictions. This point cloud is thenoptimized by a Progressive Pruning strategy to improve the global consistency.$\textbf{Second}$, we jointly refine Gaussian geometry and predicted densemetric depth via a Depth Enhancer. Specifically, we leverage diffusion priorsfrom a depth foundation model to enhance the depth maps rendered by Gaussians.In turn, the enhanced depths provide stronger geometric constraints duringGaussian training. $\textbf{Finally}$, we improve the accuracy of groundgeometry by constraining the shape and normal attributes of Gaussians withinroad regions. Extensive experiments on the Waymo dataset demonstrate that ourmethod consistently outperforms state-of-the-art methods, producing moreaccurate geometry even when compared with those using ground-truth LiDAR data.</description>
      <author>example@mail.com (Kejing Xia, Jidong Jia, Ke Jin, Yucai Bai, Li Sun, Dacheng Tao, Youjian Zhang)</author>
      <guid isPermaLink="false">2510.25173v2</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?</title>
      <link>http://arxiv.org/abs/2511.00060v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了不同LiDAR扫描模式对路边感知性能的影响，创建了'InfraLiDARs' Benchmark'数据集，比较了重复式和非重复式扫描LiDAR的性能，发现两者检测性能相当，非重复式LiDAR虽然感知范围有限但成本效益高&lt;h4&gt;背景&lt;/h4&gt;基于LiDAR的路边感知是智能交通系统的基石，现有研究多关注LiDAR的最佳放置位置，而不同扫描模式对感知性能的影响研究不足&lt;h4&gt;目的&lt;/h4&gt;系统研究基础设施背景下不同LiDAR扫描模式的差异，评估这些模式对3D目标检测算法性能的影响&lt;h4&gt;方法&lt;/h4&gt;在CARLA仿真环境中创建'InfraLiDARs' Benchmark'数据集，使用同时运行的重复式和非重复式扫描LiDAR进行数据收集，进行统计分析并评估多种3D目标检测算法的性能&lt;h4&gt;主要发现&lt;/h4&gt;非重复扫描LiDAR和128线重复扫描LiDAR在各种场景中检测性能相当；尽管非重复LiDAR感知范围有限，但因其价格低廉而具有成本效益；不同扫描模式产生不同点云分布，影响目标检测和环境理解效果&lt;h4&gt;结论&lt;/h4&gt;为设置具有最佳LiDAR扫描模式和兼容算法的路边感知系统提供见解，适应不同路边应用需求，并公开数据集促进进一步研究&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的路边感知是先进智能交通系统(ITS)的基石。虽然已有大量研究解决了基础设施LiDAR的最佳放置问题，但不同LiDAR扫描模式对感知性能的深远影响尚未得到充分研究。各种扫描模式的固有特性——如传统重复式（机械/固态）与新兴的非重复式（如基于棱镜的系统）——导致在不同距离下产生不同的点云分布，这直接影响目标检测和整体环境理解的效果。为了系统性地研究基础设施背景下的这些差异，我们引入了'InfraLiDARs' Benchmark'，这是一个在CARLA仿真环境中精心收集的新数据集，使用了同时运行的基础设施LiDAR，展示了两种扫描模式。利用这个基准，我们对各种LiDAR扫描能力进行了全面的统计分析，并评估了这些不同模式对各种领先3D目标检测算法性能的影响。我们的研究揭示，非重复扫描LiDAR和128线重复扫描LiDAR在各种场景中表现出相当的检测性能。尽管非重复LiDAR的感知范围有限，但考虑到其低廉的价格，它是一种经济有效的选择。最终，这项研究为设置具有最佳LiDAR扫描模式和兼容算法的路边感知系统提供了见解，以适应不同的路边应用需求，并公开发布了'InfraLiDARs' Benchmark'数据集，以促进进一步的研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要想解决的问题是：对于路侧感知系统，重复扫描模式（repetitive）和非重复扫描模式（non-repetitive）的激光雷达（LiDAR）哪种性能更好？这个问题在现实中非常重要，因为LiDAR路侧感知是智能交通系统的基础，直接影响交通安全性、流量管理和自动驾驶能力；在研究中也很重要，因为虽然已有大量研究关注LiDAR部署位置，但不同扫描模式对感知性能的影响研究相对不足，而不同的扫描模式会导致不同距离下的点云分布差异，直接影响物体检测和环境理解的效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了研究空白：虽然已有车载LiDAR研究，但路侧部署的不同扫描模式比较不足。然后设计了综合评估框架，包括统计基准和性能基准。他们在CARLA仿真环境中创建了专用数据集'InfraLiDARs' Benchmark'，收集了不同LiDAR扫描模式的数据。作者借鉴了现有的3D物体检测算法（如PointRCNN、PointPillars、PV-RCNN和DSVT），并参考了已有的LiDAR分类框架，但基于扫描模式而非传统分类方法。他们还参考了路侧感知研究，但专注于扫描模式而非传感器放置策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过系统比较重复扫描和非重复扫描两种LiDAR模式在路侧感知中的性能，为实际部署提供数据驱动的建议，并考虑成本效益因素。整体实现流程包括：1）在CARLA仿真环境中创建三种场景（高速公路、十字路口、弯道），使用四种LiDAR在相同位置和方向部署；2）进行统计基准测试，分析点云质量和不同距离下的检测能力；3）使用多种3D物体检测算法进行性能基准测试，采用整体AP分析、距离分段AP分析和高质量检测区域分析；4）综合比较结果，提出针对不同应用场景的最优配置建议。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）创建了'InfraLiDARs' Benchmark'数据集，专为路侧感知设计；2）首次系统比较了路侧部署中重复与非重复扫描LiDAR的性能；3）提出了全面的评估框架，结合统计基准和性能基准；4）发现了非重复扫描LiDAR在远距离检测中的优势。相比之前的工作，本文的研究焦点不同（关注扫描模式而非车载LiDAR或传感器放置），使用的数据集不同（专为路侧感知设计），评估维度更全面（考虑距离分布和高质量检测区域），并引入了成本效益分析。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建专用数据集和综合评估框架，系统比较了重复扫描与非重复扫描LiDAR在路侧感知中的性能，发现非重复扫描LiDAR与128线重复扫描LiDAR具有相当的检测性能，且在远距离检测中表现更佳，为路侧感知系统的优化部署提供了数据驱动的建议。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based roadside perception is a cornerstone of advanced IntelligentTransportation Systems (ITS). While considerable research has addressed optimalLiDAR placement for infrastructure, the profound impact of differing LiDARscanning patterns on perceptual performance remains comparativelyunder-investigated. The inherent nature of various scanning modes - such astraditional repetitive (mechanical/solid-state) versus emerging non-repetitive(e.g. prism-based) systems - leads to distinct point cloud distributions atvarying distances, critically dictating the efficacy of object detection andoverall environmental understanding. To systematically investigate thesedifferences in infrastructure-based contexts, we introduce the "InfraLiDARs'Benchmark," a novel dataset meticulously collected in the CARLA simulationenvironment using concurrently operating infrastructure-based LiDARs exhibitingboth scanning paradigms. Leveraging this benchmark, we conduct a comprehensivestatistical analysis of the respective LiDAR scanning abilities and evaluatethe impact of these distinct patterns on the performance of various leading 3Dobject detection algorithms. Our findings reveal that non-repetitive scanningLiDAR and the 128-line repetitive LiDAR were found to exhibit comparabledetection performance across various scenarios. Despite non-repetitive LiDAR'slimited perception range, it's a cost-effective option considering its lowprice. Ultimately, this study provides insights for setting up roadsideperception system with optimal LiDAR scanning patterns and compatiblealgorithms for diverse roadside applications, and publicly releases the"InfraLiDARs' Benchmark" dataset to foster further research.</description>
      <author>example@mail.com (Zhiqi Qi, Runxin Zhao, Hanyang Zhuang, Chunxiang Wang, Ming Yang)</author>
      <guid isPermaLink="false">2511.00060v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>UniField: Joint Multi-Domain Training for Universal Surface Pressure Modeling</title>
      <link>http://arxiv.org/abs/2510.24106v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为UniField的通用流场表示模型，通过整合多个子领域的空气动力学数据进行联合训练，解决了神经网络在空气动力学模拟中面临的数据稀缺问题。&lt;h4&gt;背景&lt;/h4&gt;表面压力场的空气动力学模拟对许多工程问题至关重要。近年来，深度神经网络已成为传统计算流体动力学(CFD)模拟的高效替代方案，但数据稀缺性仍然是一个基本挑战，限制了神经网络的应用。&lt;h4&gt;目的&lt;/h4&gt;为了解决数据稀缺的限制，作者提出整合多个子领域的空气动力学数据进行联合训练，以学习更通用的场域表示。&lt;h4&gt;方法&lt;/h4&gt;作者整合了五个不同的数据集，涵盖汽车、火车、飞机和一般形状等多个领域。面对不同领域之间的显著数据差异，他们提出了UniField，它采用领域无关的Transformer模块提取通用的点云特征，并定制领域特定的流条件适配器来适应不同子领域的流信息。&lt;h4&gt;主要发现&lt;/h4&gt;尽管不同子领域的空气动力学数据通常受不同方程支配，但作者发现，在所有数据上联合训练的模型通常比在单独数据集上分别训练的模型表现更好。这表明这些数据相互补充，帮助模型学习更好的流场表示。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了UniField作为通用流场表示模型的潜力，并为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;物体表面压力场的空气动力学模拟对许多工程问题至关重要。近年来，深度神经网络已成为传统计算上昂贵的CFD模拟的高效替代方案，用于建模表面压力场。然而，数据稀缺仍然是一个基本挑战，限制了神经网络的应用。为了解决这一限制，我们提出整合多个子领域的空气动力学数据并进行联合训练，以学习更通用的场域表示。我们整合了五个不同的数据集，涵盖各种领域，包括汽车、火车、飞机和一般形状。面对不同领域之间的显著数据差异，我们提出了UniField，它采用领域无关的Transformer模块提取通用的点云特征，并定制领域特定的流条件适配器来适应不同子领域的流信息。尽管不同子领域的空气动力学数据通常受不同方程支配，但我们比较了在所有数据上联合训练的模型与在单独数据集上分别训练的模型，发现联合训练的模型通常表现出更好的性能。这表明这些数据相互补充，帮助模型学习更好的流场表示。这些结果突显了UniField作为通用流场表示模型的潜力，并为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aerodynamic simulation of the surface pressure field around objects iscrucial for many engineering problems. In recent years, deep neural networkshave emerged as an efficient alternative to traditional, computationallyexpensive CFD simulations for modeling surface pressure fields. However, datascarcity remains a fundamental challenge, limiting the application of neuralnetworks. To address this limitation, we propose to integrate aerodynamic datafrom multiple subfields and conduct joint training to learn more general fieldrepresentations. We consolidate five different datasets covering variousfields, including automobiles, trains, aircraft, and general shapes. Facingsignificant data differences across different domains, we propose UniField,which employs a domain-agnostic Transformer module to extract general pointcloud features and customizes domain-specific flow-conditioned adapters toadapt to the flow information in different subfields. Despite the fact thataerodynamic data from different subfields are typically governed by differentequations, we compare models trained jointly on all data with those trainedseparately on individual datasets and find that the jointly-trained modelcommonly demonstrates better performance. This indicates that these datacomplement each other to help the model learn better flow fieldrepresentations. These results highlight the potential of UniField as auniversal flow field representation model and lay the foundation for broaderapplications of neural networks in aerodynamic analysis.</description>
      <author>example@mail.com (Junhong Zou, Zhenxu Sun, Wei Qiu, Zhaoxiang Zhang, Zhen Lei, Xiangyu Zhu)</author>
      <guid isPermaLink="false">2510.24106v3</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding</title>
      <link>http://arxiv.org/abs/2511.02565v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages main text with 6 figures (excluding references),  supplementary material included&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为视觉皮层流架构（VCFlow）的新型层次解码框架，能够无需针对特定受试者训练的情况下，从fMRI数据重建连续视觉体验，解决了跨受试者泛化困难和大脑信号复杂性的挑战。&lt;h4&gt;背景&lt;/h4&gt;主题无关的脑解码技术在临床应用方面有很大潜力，但由于跨受试者泛化困难和大脑信号的复杂性，这一方向仍处于探索阶段。传统方法需要每个受试者超过12小时的数据和大量计算资源。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的解码框架，能够高效地重建视觉体验，减少对受试者特定数据的依赖，并提高计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出视觉皮层流架构（VCFlow），这是一种层次解码框架，明确模拟人类视觉系统的腹侧-背侧架构。通过解离和利用早期视觉皮层、腹侧和背侧流中的特征，捕获视觉重建所需的多样化和互补认知信息。同时引入特征级对比学习策略，增强提取受试者不变的语义表征，提高对未见受试者的适用性。&lt;h4&gt;主要发现&lt;/h4&gt;VCFlow相比传统方法仅损失7%的准确率，但无需重新训练，每10秒即可生成重建的视频，提供了一种快速且临床可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;VCFlow为视觉脑解码领域提供了一个高效、实用的解决方案，有望在临床应用中发挥作用。&lt;h4&gt;翻译&lt;/h4&gt;主题无关的脑解码旨在无需针对特定受试者训练的情况下，从fMRI数据重建连续视觉体验，在临床应用方面有很大潜力。然而，由于跨受试者泛化困难和大脑信号的复杂性，这一方向仍处于探索阶段。在本工作中，我们提出了视觉皮层流架构（VCFlow），一种新的层次解码框架，明确模拟人类视觉系统的腹侧-背侧架构，以学习多维表征。通过解离和利用来自早期视觉皮层、腹侧和背侧流的特征，VCFlow捕获了视觉重建所需的多样化和互补认知信息。此外，我们引入了特征级对比学习策略，以增强提取受试者不变的语义表征，从而增强对未见受试者的主题无关适用性。与需要每个受试者超过12小时数据和大量计算的传统方法不同，VCFlow平均仅损失7%的准确率，无需重新训练，每10秒即可生成每个重建视频，提供了快速且临床可扩展的解决方案。论文接受后将发布源代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Subject-agnostic brain decoding, which aims to reconstruct continuous visualexperiences from fMRI without subject-specific training, holds great potentialfor clinical applications. However, this direction remains underexplored due tochallenges in cross-subject generalization and the complex nature of brainsignals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), anovel hierarchical decoding framework that explicitly models the ventral-dorsalarchitecture of the human visual system to learn multi-dimensionalrepresentations. By disentangling and leveraging features from early visualcortex, ventral, and dorsal streams, VCFlow captures diverse and complementarycognitive information essential for visual reconstruction. Furthermore, weintroduce a feature-level contrastive learning strategy to enhance theextraction of subject-invariant semantic representations, thereby enhancingsubject-agnostic applicability to previously unseen subjects. Unlikeconventional pipelines that need more than 12 hours of per-subject data andheavy computation, VCFlow sacrifices only 7\% accuracy on average yet generateseach reconstructed video in 10 seconds without any retraining, offering a fastand clinically scalable solution. The source code will be released uponacceptance of the paper.</description>
      <author>example@mail.com (Jingyu Lu, Haonan Wang, Qixiang Zhang, Xiaomeng Li)</author>
      <guid isPermaLink="false">2511.02565v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification</title>
      <link>http://arxiv.org/abs/2511.02564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了MTF-CVReID，一个针对视频跨视角行人重识别的参数高效框架，通过七个专门设计的模块解决了视角变化大、尺度差异和时间不一致性带来的挑战，在保持实时效率的同时实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;视频跨视角行人重识别(如空中-地面监控)由于极端视角变化、尺度差异和时间不一致性仍然是一个开放问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个参数高效的框架，解决跨视角视频行人重识别中的挑战，同时保持实时计算效率。&lt;h4&gt;方法&lt;/h4&gt;在ViT-B/16骨干网络上引入七个互补模块：CSFN(校正相机和视角偏差)、MRFH(实现跨高度的尺度稳定)、IAMM(强化持久的身份特征)、TDM(用于感知运动的短期时间编码)、IVFA(实现视角不变的表征对齐)、HTPL(捕获多尺度时间规律)和MVICL(使用对比学习范式强制跨视角身份一致性)。&lt;h4&gt;主要发现&lt;/h4&gt;尽管只增加约200万个参数和0.7 GFLOPs，MTF-CVReID保持了实时效率(189 FPS)，在AG-VPReID基准测试的所有高度级别上实现了最先进性能，并在G2A-VReID和MARS数据集上表现出强大的跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;精心设计的基于适配器的模块可以在不牺牲计算效率的情况下显著增强跨视角鲁棒性和时间一致性。&lt;h4&gt;翻译&lt;/h4&gt;基于视频的跨域视角行人重识别(例如，空中-地面监控)由于极端视角变化、尺度差异和时间不一致性仍然是一个开放问题。为了解决这些挑战，我们提出了MTF-CVReID，一个参数高效的框架，在ViT-B/16骨干网络上引入了七个互补模块。具体来说，我们包括：(1)跨流特征归一化(CSFN)来校正相机和视角偏差；(2)多分辨率特征调和(MRFH)用于跨高度的尺度稳定；(3)身份感知记忆模块(IAMM)来强化持久的身份特征；(4)时间动态建模(TDM)用于感知运动的短期时间编码；(5)跨视角特征对齐(IVFA)实现视角不变的表征对齐；(6)分层时间模式学习(HTPL)捕获多尺度时间规律；以及(7)多视角身份一致性学习(MVICL)，使用对比学习范式强制跨视角身份一致性。尽管比基线模型只增加了约200万个参数和0.7 GFLOPs，MTF-CVReID保持了实时效率(189 FPS)，并在AG-VPReID基准测试的所有高度级别上实现了最先进性能，同时在G2A-VReID和MARS数据集上具有强大的跨数据集泛化能力。这些结果表明，精心设计的基于适配器的模块可以在不牺牲计算效率的情况下显著增强跨视角鲁棒性和时间一致性。源代码可在https://github.com/MdRashidunnabi/MTF-CVReID获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video-based person re-identification (ReID) in cross-view domains (forexample, aerial-ground surveillance) remains an open problem because of extremeviewpoint shifts, scale disparities, and temporal inconsistencies. To addressthese challenges, we propose MTF-CVReID, a parameter-efficient framework thatintroduces seven complementary modules over a ViT-B/16 backbone. Specifically,we include: (1) Cross-Stream Feature Normalization (CSFN) to correct camera andview biases; (2) Multi-Resolution Feature Harmonization (MRFH) for scalestabilization across altitudes; (3) Identity-Aware Memory Module (IAMM) toreinforce persistent identity traits; (4) Temporal Dynamics Modeling (TDM) formotion-aware short-term temporal encoding; (5) Inter-View Feature Alignment(IVFA) for perspective-invariant representation alignment; (6) HierarchicalTemporal Pattern Learning (HTPL) to capture multi-scale temporal regularities;and (7) Multi-View Identity Consistency Learning (MVICL) that enforcescross-view identity coherence using a contrastive learning paradigm. Despiteadding only about 2 million parameters and 0.7 GFLOPs over the baseline,MTF-CVReID maintains real-time efficiency (189 FPS) and achievesstate-of-the-art performance on the AG-VPReID benchmark across all altitudelevels, with strong cross-dataset generalization to G2A-VReID and MARSdatasets. These results show that carefully designed adapter-based modules cansubstantially enhance cross-view robustness and temporal consistency withoutcompromising computational efficiency. The source code is available athttps://github.com/MdRashidunnabi/MTF-CVReID</description>
      <author>example@mail.com (Md Rashidunnabi, Kailash A. Hambarde, Vasco Lopes, Joao C. Neves, Hugo Proenca)</author>
      <guid isPermaLink="false">2511.02564v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning</title>
      <link>http://arxiv.org/abs/2511.02360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoCoVa是一种新的视觉-语言模型框架，通过连续跨模态推理解决传统模型在处理高维视觉感知方面的局限性，在多个基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;人类认知中存在难以用言语表达的隐性思维过程，使人类能以多种方式理解世界；而当代视觉-语言模型局限于离散的语言标记空间推理，限制了视觉感知的丰富高维特性。&lt;h4&gt;目的&lt;/h4&gt;弥合人类隐性思维与当前视觉-语言模型之间的差距，提出利用连续跨模态推理处理多样化视觉-语言任务的新框架。&lt;h4&gt;方法&lt;/h4&gt;CoCoVa的核心是迭代推理循环，使用潜在Q-Former作为动态推理引擎，通过跨模态融合优化潜在思维向量链；实现令牌选择机制识别显著视觉区域；结合对比学习和基于扩散的重构进行多任务训练，确保潜在表示与视觉和文本模态对齐。&lt;h4&gt;主要发现&lt;/h4&gt;CoCoVa在准确率和令牌效率上优于强基线模型；1.5B主干模型在几乎所有基准测试中与7B-9B模型竞争或超越；扩展到7B LLM主干模型时仍保持竞争力；学习到的潜在空间捕捉了可解释和结构化的推理模式。&lt;h4&gt;结论&lt;/h4&gt;CoCoVa成功弥合了离散语言处理与视觉理解连续性之间的表征差距，展示了桥接这一差距的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在人类认知中，存在许多难以言表且超越言语表达的思维过程，使我们能够以多种方式理解和与世界互动。然而，当代视觉-语言模型仍然局限于在离散且刚性的语言标记空间中进行推理，从而限制了视觉感知的丰富高维特性。为了弥合这一差距，我们提出了CoCoVa（连续视觉-语言思维链），一种利用连续跨模态推理处理多样化视觉-语言任务的新框架。CoCoVa的核心是一个迭代推理循环，其中一种新型的潜在Q-Former作为动态推理引擎，通过跨模态融合迭代优化潜在思维向量链。为了聚焦这一过程，令牌选择机制动态识别显著的视觉区域，模拟注意力焦点。为确保这些潜在思维保持基础，我们使用结合对比学习和基于扩散的重构的多任务目标训练模型，强制潜在表示与视觉和文本模态保持对齐。评估显示，CoCoVa在准确率和令牌效率方面优于强基线模型。使用1.5B主干模型时，它在几乎所有基准测试中与更大的7B-9B模型竞争或超越。当扩展到7B LLM主干模型时，它仍能与最先进模型保持竞争力。定性分析验证了学习到的潜在空间捕捉了可解释和结构化的推理模式，突显了CoCoVa弥合离散语言处理与视觉理解连续性之间表征差距的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In human cognition, there exist numerous thought processes that are tacit andbeyond verbal expression, enabling us to understand and interact with the worldin multiple ways. However, contemporary Vision-Language Models (VLMs) remainconstrained to reasoning within the discrete and rigid space of linguistictokens, thereby bottlenecking the rich, high-dimensional nature of visualperception. To bridge this gap, we propose CoCoVa (Chain of ContinuousVision-Language Thought), a novel framework for vision-language model thatleverages continuous cross-modal reasoning for diverse vision-language tasks.The core of CoCoVa is an iterative reasoning cycle, where a novel LatentQ-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining achain of latent thought vectors through cross-modal fusion. To focus thisprocess, a token selection mechanism dynamically identifies salient visualregions, mimicking attentional focus. To ensure these latent thoughts remaingrounded, we train the model with a multi-task objective that combinescontrastive learning and diffusion-based reconstruction, enforcing alignmentbetween latent representations and both visual and textual modalities.Evaluations show CoCoVa improves accuracy and token efficiency over strongbaselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9Bmodels on almost all benchmarks. When scaled to 7B LLM backbones, it remainscompetitive with state-of-the-art models. Qualitative analysis validates thatlearned latent space captures interpretable and structured reasoning patterns,highlighting the potential of CoCoVa to bridge the representational gap betweendiscrete language processing and the continuous nature of visual understanding.</description>
      <author>example@mail.com (Jizheng Ma, Xiaofei Zhou, Yanlong Song, Han Yan)</author>
      <guid isPermaLink="false">2511.02360v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic Graph Cuts</title>
      <link>http://arxiv.org/abs/2511.02272v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的概率框架，用于图割的概率松弛，作为谱聚类的可微分替代方案，无需特征分解即可实现端到端和在线学习，为可扩展、可微分的图划分提供了严谨的数值稳定基础。&lt;h4&gt;背景&lt;/h4&gt;现有的图割概率松弛方法主要集中在RatioCut上，缺乏通用保证和原则性梯度，限制了其在各种聚类和对比学习任务中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个覆盖广泛割类型（包括Normalized Cut）的统一概率框架，提供期望离散割的紧密解析上界，并建立可扩展、可微分图划分的严谨数值稳定基础。&lt;h4&gt;方法&lt;/h4&gt;通过积分表示和高斯超几何函数构建统一的概率框架，提供具有闭式前向和反向传播的解析上界，实现可微分图划分。&lt;h4&gt;主要发现&lt;/h4&gt;提出的统一框架覆盖了广泛的割类型，通过积分表示和高斯超几何函数提供了期望离散割的紧密解析上界，并具有闭式前向和反向传播。&lt;h4&gt;结论&lt;/h4&gt;该研究为可扩展、可微分的图划分提供了严谨、数值稳定的基础，能够支持广泛的聚类和对比学习目标，克服了现有方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;图割的概率松弛为谱聚类提供了不同的可微分替代方案，能够在不进行特征分解的情况下实现端到端和在线学习，但先前的工作主要集中在RatioCut上，缺乏通用保证和原则性梯度。我们提出了一个统一的概率框架，涵盖了广泛的割类型，包括Normalized Cut。我们的框架通过积分表示和具有闭式前向和反向传播的高斯超几何函数，为期望离散割提供了紧密的解析上界。这些结果共同为可扩展、可微分的图划分提供了一个严谨的、数值稳定的基础，涵盖了广泛的聚类和对比学习目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Probabilistic relaxations of graph cuts offer a differentiable alternative tospectral clustering, enabling end-to-end and online learning withouteigendecompositions, yet prior work centered on RatioCut and lacked generalguarantees and principled gradients. We present a unified probabilisticframework that covers a wide class of cuts, including Normalized Cut. Ourframework provides tight analytic upper bounds on expected discrete cuts viaintegral representations and Gauss hypergeometric functions with closed-formforward and backward. Together, these results deliver a rigorous, numericallystable foundation for scalable, differentiable graph partitioning covering awide range of clustering and contrastive learning objectives.</description>
      <author>example@mail.com (Ayoub Ghriss)</author>
      <guid isPermaLink="false">2511.02272v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation</title>
      <link>http://arxiv.org/abs/2511.01517v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NSYNC的新型对比学习框架，通过生成负合成数据集与正真实图像结合进行对比训练，以提高大型文本到图像扩散模型的风格化能力。&lt;h4&gt;背景&lt;/h4&gt;当前文本条件图像生成方法能生成逼真图像但无法捕捉特定风格，即使在目标风格数据集上微调也难以掌握风格特征。&lt;h4&gt;目的&lt;/h4&gt;提高大型文本到图像扩散模型的风格化能力，使其能够更好地捕捉特定风格特征。&lt;h4&gt;方法&lt;/h4&gt;NSYNC框架专注于生成负合成数据集，与正真实图像一起用于对比训练。同时处理负数据和正数据获得对应梯度，通过从正梯度中减去其在负梯度上的投影得到正交分量，基于此更新参数，消除正负数据中都存在的平凡属性，引导模型捕捉独特风格。&lt;h4&gt;主要发现&lt;/h4&gt;在各种画家和插画师风格的实验中，NSYNC在定量和定性评估上都优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;NSYNC框架能有效提升文本到图像扩散模型的风格化能力，通过对比学习方法和负合成数据集的生成，使模型能够更好地捕捉特定风格特征。&lt;h4&gt;翻译&lt;/h4&gt;当前文本条件图像生成方法输出看起来真实的图像，但它们无法捕捉特定风格。简单地在目标风格数据集上微调仍然难以掌握风格特征。在这项工作中，我们提出了一种新颖的对比学习框架来提高大型文本到图像扩散模型的风格化能力。受图像生成模型惊人进展的启发，我们在方法中利用了合成图像生成。通常，生成的合成数据依赖于任务，大多数情况下用于扩大可用的真实训练数据集。有了NSYNC，我们专注于生成负合成数据集，与真实正图像一起用于新颖的对比训练方案。在我们提出的训练设置中，我们将负数据与正数据一起前向传播，分别获得负梯度和正梯度。然后我们通过从正梯度中减去其在负梯度上的投影来获得正交分量，基于此更新参数。这个正交分量消除了正负数据中都存在的平凡属性，并引导模型捕捉更独特的风格。在各种画家和插画师风格上的实验表明，我们的方法在定量和定性上都优于基线方法。我们的代码可在https://github.com/giddyyupp/NSYNC获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current text conditioned image generation methods output realistic lookingimages, but they fail to capture specific styles. Simply finetuning them on thetarget style datasets still struggles to grasp the style features. In thiswork, we present a novel contrastive learning framework to improve thestylization capability of large text-to-image diffusion models. Motivated bythe astonishing advance in image generation models that makes synthetic data anintrinsic part of model training in various computer vision tasks, we exploitsynthetic image generation in our approach. Usually, the generated syntheticdata is dependent on the task, and most of the time it is used to enlarge theavailable real training dataset. With NSYNC, alternatively, we focus ongenerating negative synthetic sets to be used in a novel contrastive trainingscheme along with real positive images. In our proposed training setup, weforward negative data along with positive data and obtain negative and positivegradients, respectively. We then refine the positive gradient by subtractingits projection onto the negative gradient to get the orthogonal component,based on which the parameters are updated. This orthogonal component eliminatesthe trivial attributes that are present in both positive and negative data anddirects the model towards capturing a more unique style. Experiments on variousstyles of painters and illustrators show that our approach improves theperformance over the baseline methods both quantitatively and qualitatively.Our code is available at https://github.com/giddyyupp/NSYNC.</description>
      <author>example@mail.com (Serkan Ozturk, Samet Hicsonmez, Pinar Duygulu)</author>
      <guid isPermaLink="false">2511.01517v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Embodied Cognition Augmented End2End Autonomous Driving</title>
      <link>http://arxiv.org/abs/2511.01334v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages,4 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为E³AD的新范式，通过在视觉特征提取网络和通用脑电图大模型之间进行对比学习，学习潜在的人类驾驶认知以增强端到端自动驾驶规划性能。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的端到端自动驾驶已成为新范式，但现有方法通常依赖于标签监督下训练的视觉特征提取网络，这种有限监督框架限制了驾驶模型的通用性和适用性。&lt;h4&gt;目的&lt;/h4&gt;提出E³AD范式，通过对比学习整合人类驾驶认知，以增强端到端自动驾驶的规划性能。&lt;h4&gt;方法&lt;/h4&gt;收集认知数据集用于对比学习；研究使用人类驾驶认知增强端到端规划的方法和机制；在公开数据集上使用流行驾驶模型作为基线；进行开环和闭环测试全面评估规划性能。&lt;h4&gt;主要发现&lt;/h4&gt;E³AD范式显著增强了基线模型的端到端规划性能；消融研究验证了驾驶认知的贡献和对比学习过程的有效性。&lt;h4&gt;结论&lt;/h4&gt;这是首个将人类驾驶认知整合到端到端自动驾驶规划中的工作；是具身认知数据融入端到端自动驾驶的初步尝试；为脑启发自动驾驶系统提供了有价值的见解；代码将在Github上提供。&lt;h4&gt;翻译&lt;/h4&gt;近年来，基于视觉的端到端自动驾驶已成为一种新范式。然而，流行的端到端方法通常依赖于在标签监督下训练的视觉特征提取网络。这种有限监督框架限制了驾驶模型的通用性和适用性。在本文中，我们提出了一种称为E³AD的新范式，主张在视觉特征提取网络和通用脑电图大模型之间进行对比学习，以学习潜在的人类驾驶认知，从而增强端到端规划。在这项工作中，我们收集了用于上述对比学习过程的数据集。随后，我们研究了使用人类驾驶认知增强端到端规划的方法和潜在机制，在公开可用的自动驾驶数据集上使用流行的驾驶模型作为基线。进行了开环和闭环测试，以全面评估规划性能。实验结果表明，E³AD范式显著增强了基线模型的端到端规划性能。消融研究进一步验证了驾驶认知的贡献和对比学习过程的有效性。据我们所知，这是第一个将人类驾驶认知整合用于改进端到端自动驾驶规划的工作。它代表了将具身认知数据融入端到端自动驾驶的初步尝试，为未来的脑启发自动驾驶系统提供了有价值的见解。我们的代码将在Github上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, vision-based end-to-end autonomous driving has emerged as anew paradigm. However, popular end-to-end approaches typically rely on visualfeature extraction networks trained under label supervision. This limitedsupervision framework restricts the generality and applicability of drivingmodels. In this paper, we propose a novel paradigm termed $E^{3}AD$, whichadvocates for comparative learning between visual feature extraction networksand the general EEG large model, in order to learn latent human drivingcognition for enhancing end-to-end planning. In this work, we collected acognitive dataset for the mentioned contrastive learning process. Subsequently,we investigated the methods and potential mechanisms for enhancing end-to-endplanning with human driving cognition, using popular driving models asbaselines on publicly available autonomous driving datasets. Both open-loop andclosed-loop tests are conducted for a comprehensive evaluation of planningperformance. Experimental results demonstrate that the $E^{3}AD$ paradigmsignificantly enhances the end-to-end planning performance of baseline models.Ablation studies further validate the contribution of driving cognition and theeffectiveness of comparative learning process. To the best of our knowledge,this is the first work to integrate human driving cognition for improvingend-to-end autonomous driving planning. It represents an initial attempt toincorporate embodied cognitive data into end-to-end autonomous driving,providing valuable insights for future brain-inspired autonomous drivingsystems. Our code will be made available at Github</description>
      <author>example@mail.com (Ling Niu, Xiaoji Zheng, Han Wang, Chen Zheng, Ziyuan Yang, Bokui Chen, Jiangtao Gong)</author>
      <guid isPermaLink="false">2511.01334v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval</title>
      <link>http://arxiv.org/abs/2511.00903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ColMate，一个专门针对多模态文档检索的模型，通过OCR预训练、自监督掩码对比学习和后期交互评分机制，改进了现有的文档检索方法，在基准测试中取得了更好的性能。&lt;h4&gt;背景&lt;/h4&gt;检索增强生成在模型需要专业知识或最新数据访问时已被证明具有实用性。然而，现有的多模态文档检索方法通常复制仅为文本检索开发的技术，无论是在文档编码方式、训练目标定义还是相似度分数计算方面。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态文档检索方法的局限性，弥合多模态表示学习与文档检索之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出ColMate模型，它利用三种关键技术：基于OCR的预训练目标、自监督掩码对比学习目标，以及与多模态文档结构和视觉特征更相关的后期交互评分机制。&lt;h4&gt;主要发现&lt;/h4&gt;ColMate在ViDoRe V2基准测试上比现有检索模型提高了3.61%，并且显示出对域外基准测试的更强泛化能力。&lt;h4&gt;结论&lt;/h4&gt;ColMate是一个有效的文档检索模型，它专门针对多模态文档的特点进行了优化，能够提供比现有方法更好的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;检索增强生成已被证明在模型需要专业知识或访问最新数据时具有实用性。然而，现有的多模态文档检索方法通常复制仅为文本检索开发的技术，无论是在文档编码方式、训练目标定义还是相似度分数计算方面。为解决这些局限性，我们提出了ColMate，一个弥合多模态表示学习与文档检索之间差距的文档检索模型。ColMate利用了基于OCR的预训练目标、自监督掩码对比学习目标，以及一种与多模态文档结构和视觉特征更相关的后期交互评分机制。ColMate在ViDoRe V2基准测试上比现有检索模型提高了3.61%，显示出对域外基准测试的更强泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieval-augmented generation has proven practical when models requirespecialized knowledge or access to the latest data. However, existing methodsfor multimodal document retrieval often replicate techniques developed fortext-only retrieval, whether in how they encode documents, define trainingobjectives, or compute similarity scores. To address these limitations, wepresent ColMate, a document retrieval model that bridges the gap betweenmultimodal representation learning and document retrieval. ColMate utilizes anovel OCR-based pretraining objective, a self-supervised masked contrastivelearning objective, and a late interaction scoring mechanism more relevant tomultimodal document structures and visual characteristics. ColMate obtains3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,demonstrating stronger generalization to out-of-domain benchmarks.</description>
      <author>example@mail.com (Ahmed Masry, Megh Thakkar, Patrice Bechard, Sathwik Tejaswi Madhusudhan, Rabiul Awal, Shambhavi Mishra, Akshay Kalkunte Suresh, Srivatsava Daruru, Enamul Hoque, Spandana Gella, Torsten Scholak, Sai Rajeswar)</author>
      <guid isPermaLink="false">2511.00903v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models</title>
      <link>http://arxiv.org/abs/2511.00854v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TriCon-Fair的对比学习框架，用于解决大型语言模型中的社会偏见传播问题。该方法通过解耦损失函数，结合三元组和语言建模项，消除正负耦合，减少歧视性输出，同时保持强大的下游性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的广泛应用引发了关于社会偏见传播的严重担忧，这可能导致有害和不公平的结果。现有的去偏见方法独立处理有偏见和无偏见的样本，忽略了它们之间的相互关系。&lt;h4&gt;目的&lt;/h4&gt;解决现有去偏见方法中存在的隐藏负-正耦合问题，即对一组的改进无意中损害另一组，导致残余社会偏见持续存在。&lt;h4&gt;方法&lt;/h4&gt;提出TriCon-Fair，一种对比学习框架，采用解耦损失函数，结合三元组和语言建模项，消除正负耦合。该方法为每个锚点分配明确的有偏见负样本和无偏见正样本，解耦推-拉动态，避免正负耦合，并联合优化语言建模目标以保持通用能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，TriCon-Fair能够超越现有的去偏见基线，减少歧视性输出，同时保持强大的下游性能。&lt;h4&gt;结论&lt;/h4&gt;TriCon-Fair为敏感的自然语言处理应用提供了一种实用且合乎伦理的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型日益增多的应用引发了关于社会偏见传播的严重担忧，这可能导致有害和不公平的结果。然而，现有的去偏见方法独立处理有偏见和无偏见的样本，从而忽略了它们之间的相互关系。这种疏忽导致了一种隐藏的负-正耦合，即对一组的改进无意中损害另一组，使残余社会偏见得以持续。在本文中，我们介绍了TriCon-Fair，一种对比学习框架，采用结合三元组和语言建模项的解耦损失函数来消除负-正耦合。我们的TriCon-Fair为每个锚点分配明确的有偏见负样本和无偏见正样本，解耦推-拉动态，避免负-正耦合，并联合优化语言建模(LM)目标以保持通用能力。实验结果表明，TriCon-Fair超越了现有的去偏见基线，减少了歧视性输出，同时保持强大的下游性能。这表明我们提出的TriCon-Fair为敏感的自然语言处理应用提供了一种实用且合乎伦理的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing utilization of large language models raises significantconcerns about the propagation of social biases, which may result in harmfuland unfair outcomes. However, existing debiasing methods treat the biased andunbiased samples independently, thus ignoring their mutual relationship. Thisoversight enables a hidden negative-positive coupling, where improvements forone group inadvertently compromise the other, allowing residual social bias topersist. In this paper, we introduce TriCon-Fair, a contrastive learningframework that employs a decoupled loss that combines triplet and languagemodeling terms to eliminate positive-negative coupling. Our TriCon-Fair assignseach anchor an explicitly biased negative and an unbiased positive, decouplingthe push-pull dynamics and avoiding positive-negative coupling, and jointlyoptimizes a language modeling (LM) objective to preserve general capability.Experimental results demonstrate that TriCon-Fair reduces discriminatory outputbeyond existing debiasing baselines while maintaining strong downstreamperformance. This suggests that our proposed TriCon-Fair offers a practical andethical solution for sensitive NLP applications.</description>
      <author>example@mail.com (Chong Lyu, Lin Li, Shiqing Wu, Jingling Yuan)</author>
      <guid isPermaLink="false">2511.00854v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Towards classification-based representation learning for place recognition on LiDAR scans</title>
      <link>http://arxiv.org/abs/2511.00738v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将地点识别作为多类分类问题的替代方法，通过为LiDAR扫描分配离散位置标签并训练编码器-解码器模型来直接分类位置，在NuScenes数据集上验证了其与对比学习方法相当的竞争性能，同时具有更高的训练效率和稳定性。&lt;h4&gt;背景&lt;/h4&gt;地点识别是自动驾驶中的关键任务，允许车辆使用传感器数据确定自身位置。现有方法大多依赖于对比学习。&lt;h4&gt;目的&lt;/h4&gt;探索一种替代对比学习的方法，将地点识别作为多类分类问题处理。&lt;h4&gt;方法&lt;/h4&gt;为LiDAR扫描分配离散位置标签，训练编码器-解码器模型直接分类每个扫描的位置。&lt;h4&gt;主要发现&lt;/h4&gt;在NuScenes数据集上评估显示，该方法与基于对比学习的方法具有竞争性能，同时在训练效率和稳定性方面具有优势。&lt;h4&gt;结论&lt;/h4&gt;将地点识别作为多类分类问题是一种有效的替代方法，具有与对比学习方法相当的性能，并且在训练效率和稳定性方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;地点识别是自动驾驶中的一个关键任务，它允许车辆使用传感器数据来确定自己的位置。虽然大多数现有方法依赖于对比学习，但我们通过将地点识别构建为一个多类分类问题来探索一种替代方法。我们的方法为LiDAR扫描分配离散的位置标签，并训练一个编码器-解码器模型来直接分类每个扫描的位置。我们在NuScenes数据集上评估了这种方法，并表明它与基于对比学习的方法相比具有竞争力的性能，同时在训练效率和稳定性方面提供优势。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于激光雷达(LiDAR)扫描的地点识别问题，即根据传感器数据确定车辆位置。这个问题在现实中非常重要，因为在GPS信号不可靠的环境（如城市峡谷、隧道或恶劣天气）中，准确的定位对自动驾驶车辆的导航、地图绘制和安全决策至关重要。在研究中，这个问题也很重要，因为现有方法大多基于对比学习，需要复杂的负样本挖掘策略且训练效率低、稳定性差。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到人脸识别领域分类方法的启发，思考是否可以将地点识别问题重新定义为分类任务。作者注意到地点不像标准分类任务中的对象那样有明确边界，因此通过将连续位置离散化为网格单元来解决这一问题。他们借鉴了PointNet++作为骨干网络处理点云，采用了两塔(two-tower)架构分离索引构建和查询服务，并参考了LCPR的数据集划分策略。作者设计了掩码交叉熵损失函数，避免惩罚预测到正确位置相邻网格的预测，解决了空间相关位置之间的梯度冲突问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将地点识别问题从传统的对比学习框架重新定义为多类别分类问题，通过将连续位置离散化为网格单元，训练模型直接预测激光扫描对应的离散位置类别。整体流程包括：1)数据准备和划分；2)位置离散化，将连续坐标转换为网格坐标并分配唯一类别标签；3)构建编码器-解码器模型，使用PointNet++处理点云并添加分类头；4)使用掩码交叉熵损失进行训练，避免惩罚相邻位置的预测；5)评估时使用KNN搜索在预构建的嵌入数据库中查找最相似的样本，计算召回率指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将地点识别重新定义为多类别分类问题；2)提出新颖的位置离散化方法，使用3元组表示离散位置；3)设计掩码交叉熵损失解决空间相关位置的梯度冲突；4)避免对比学习中的复杂负样本挖掘，提高训练效率；5)通过整合多地图数据展示大规模训练可行性。相比之前的工作，不同之处在于训练目标（分类vs度量学习）、负样本处理（无需复杂挖掘）、模型架构（添加分类头）和训练稳定性（分类方法更稳定）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于分类的新型地点识别方法，通过位置离散化和掩码损失函数，为激光雷达扫描的地点识别提供了更高效、更稳定的训练框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Place recognition is a crucial task in autonomous driving, allowing vehiclesto determine their position using sensor data. While most existing methods relyon contrastive learning, we explore an alternative approach by framing placerecognition as a multi-class classification problem. Our method assignsdiscrete location labels to LiDAR scans and trains an encoder-decoder model toclassify each scan's position directly. We evaluate this approach on theNuScenes dataset and show that it achieves competitive performance compared tocontrastive learning-based methods while offering advantages in trainingefficiency and stability.</description>
      <author>example@mail.com (Maksim Konoplia, Dmitrii Khizbullin)</author>
      <guid isPermaLink="false">2511.00738v2</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Reasoning Planning for Language Models</title>
      <link>http://arxiv.org/abs/2511.00521v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EPIC的集成规划与对比学习框架，用于解决语言模型生成中选择合适推理方法的问题，通过理论分析和创新框架实现了准确性和计算效率的平衡。&lt;h4&gt;背景&lt;/h4&gt;在语言模型生成中，为给定查询选择合适的推理方法仍然是一个关键挑战。现有方法通常生成多个候选响应并使用聚合策略选择输出答案，且往往假设更多的候选答案会带来更高的准确性。&lt;h4&gt;目的&lt;/h4&gt;重新审视'更多候选答案意味着更高准确性'这一假设，通过严谨的理论分析，推导固定生成分布和候选大小下标准聚合方法的准确性界限。&lt;h4&gt;方法&lt;/h4&gt;提出EPIC（Ensemble Planning with Contrastive learning）框架，学习一个共享的表示空间来捕捉模型推理能力和查询方法兼容性，并将概率界限作为正则化项纳入效用驱动的优化中，平衡准确性和计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析对现有假设有了更深入的理解；EPIC能够在各种数学推理任务中一致地选择最优推理方法；在提高准确性的同时减少了计算开销。&lt;h4&gt;结论&lt;/h4&gt;EPIC框架有效地解决了语言模型生成中选择合适推理方法的挑战，通过理论分析和创新框架实现了准确性和计算效率的平衡。&lt;h4&gt;翻译&lt;/h4&gt;为给定查询选择合适的推理方法在语言模型生成中仍然是一个关键挑战。现有方法通常生成多个候选响应并使用聚合策略来选择输出答案，常常假设更多的候选答案会带来更高的准确性。我们通过严谨的理论分析重新审视这一假设，推导了固定生成分布和候选大小下标准聚合方法的准确性界限。基于这些见解，我们引入了EPIC，一种集成规划与对比学习框架，用于学习一个共享的表示空间，该空间能够捕捉模型推理能力和查询方法兼容性。EPIC将我们的概率界限作为正则化项纳入到效用驱动的优化中，平衡准确性和计算成本。在各种数学推理任务上的实验表明，EPIC能够一致地选择最优推理方法，在提高准确性的同时减少计算开销。我们的代码可以在https://github.com/nguyenngocbaocmt02/EPIC找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Selecting an appropriate reasoning method for a given query remains a keychallenge in language model generation. Existing approaches typically generatemultiple candidate responses and use an aggregation strategy to select theoutput answer, often assuming that more candidate answers yield higheraccuracy. We revisit this assumption through a rigorous theoretical analysis,deriving accuracy bounds for standard aggregation methods under fixedgeneration distributions and candidate sizes. Building on these insights, weintroduce EPIC, an Ensemble Planning with Contrastive learning framework tolearn a shared representation space that captures both model reasoningabilities and query-method compatibility. EPIC incorporates our probabilitybounds as a regularizer in a utility-driven optimization that balances accuracyand computational cost. Experiments on diverse mathematical reasoning tasksshow that EPIC consistently selects optimal reasoning methods, improvingaccuracy while reducing computational overhead. Our code can be found athttps://github.com/nguyenngocbaocmt02/EPIC.</description>
      <author>example@mail.com (Bao Nguyen, Hieu Trung Nguyen, Ruifeng She, Xiaojin Fu, Viet Anh Nguyen)</author>
      <guid isPermaLink="false">2511.00521v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training</title>
      <link>http://arxiv.org/abs/2511.00446v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为ToxicTextCLIP的框架，用于生成高质量的对抗性文本来攻击CLIP模型在预训练阶段。&lt;h4&gt;背景&lt;/h4&gt;CLIP模型通过自监督对比学习对齐大规模网络数据中的图像-文本对推动了视觉-语言建模，但它依赖未筛选的互联网数据，面临数据投毒和后门风险。现有研究主要关注基于图像的攻击，而同样对CLIP训练至关重要的文本模态尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对CLIP预训练阶段的高质量对抗文本生成框架，解决背景不一致导致的语义错位和背景一致文本稀缺性问题。&lt;h4&gt;方法&lt;/h4&gt;ToxicTextCLIP框架迭代应用两种技术：1) 背景感知选择器，优先选择与目标类别背景内容对齐的文本；2) 背景驱动增强器，生成语义连贯且多样化的投毒样本。&lt;h4&gt;主要发现&lt;/h4&gt;在分类和检索任务上的实验表明，ToxicTextCLIP实现了高达95.83%的投毒成功率和98.68%的后门Hit@1，同时成功绕过了RoCLIP、CleanCLIP和SafeCLIP防御。&lt;h4&gt;结论&lt;/h4&gt;ToxicTextCLIP是一种有效的针对CLIP预训练阶段的文本投毒框架，能够高效生成高质量的对抗文本。&lt;h4&gt;翻译&lt;/h4&gt;对比语言图像预训练（CLIP）模型通过自监督对比学习对齐大规模网络数据中的图像-文本对，显著推动了视觉-语言建模的发展。然而，它对未筛选的互联网来源数据的依赖使其面临数据投毒和后门风险。虽然现有研究主要调查基于图像的攻击，但对CLIP训练同样至关重要的文本模态仍未被充分探索。在这项工作中，我们引入了ToxicTextCLIP，一个用于在预训练阶段针对CLIP生成高质量对抗文本的框架。该框架解决了两个关键挑战：由背景与目标类别不一致导致的语义错位，以及背景一致文本的稀缺性。为此，ToxicTextCLIP迭代应用：1) 一个背景感知选择器，优先选择与目标类别背景内容对齐的文本；2) 一个背景驱动增强器，生成语义连贯且多样化的投毒样本。在分类和检索任务上的广泛实验表明，ToxicTextCLIP实现了高达95.83%的投毒成功率和98.68%的后门Hit@1，同时绕过了RoCLIP、CleanCLIP和SafeCLIP防御。源代码可通过https://github.com/xinyaocse/ToxicTextCLIP/访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Contrastive Language-Image Pretraining (CLIP) model has significantlyadvanced vision-language modeling by aligning image-text pairs from large-scaleweb data through self-supervised contrastive learning. Yet, its reliance onuncurated Internet-sourced data exposes it to data poisoning and backdoorrisks. While existing studies primarily investigate image-based attacks, thetext modality, which is equally central to CLIP's training, remainsunderexplored. In this work, we introduce ToxicTextCLIP, a framework forgenerating high-quality adversarial texts that target CLIP during thepre-training phase. The framework addresses two key challenges: semanticmisalignment caused by background inconsistency with the target class, and thescarcity of background-consistent texts. To this end, ToxicTextCLIP iterativelyapplies: 1) a background-aware selector that prioritizes texts with backgroundcontent aligned to the target class, and 2) a background-driven augmenter thatgenerates semantically coherent and diverse poisoned samples. Extensiveexperiments on classification and retrieval tasks show that ToxicTextCLIPachieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, whilebypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can beaccessed via https://github.com/xinyaocse/ToxicTextCLIP/.</description>
      <author>example@mail.com (Xin Yao, Haiyang Zhao, Yimin Chen, Jiawei Guo, Kecheng Huang, Ming Zhao)</author>
      <guid isPermaLink="false">2511.00446v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Simple and Behavior-Driven Augmentation for Recommendation with Rich Collaborative Signals</title>
      <link>http://arxiv.org/abs/2511.00436v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages. This paper is accepted at IEEE BigData 2025 (Short)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SCAR(Simple Collaborative Augmentation for Recommendation)的简单协作增强方法，用于改进图协同过滤中的对比学习效果。该方法通过生成伪交互而非删除信息来增强数据视图，在四个基准数据集上表现出色，尤其在稀疏数据场景中效果显著。&lt;h4&gt;背景&lt;/h4&gt;对比学习(CI)已被广泛用于增强图协同过滤(GCF)的性能以实现个性化推荐。数据增强在对比学习的成功中起着关键作用，先前的工作设计了去除用户和项目之间噪声交互的增强方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单而直观的增强方法SCAR，旨在最大化对比学习对图协同过滤的有效性，同时避免复杂增强模块的缺点。&lt;h4&gt;方法&lt;/h4&gt;SCAR不删除信息，而是利用从用户-项目交互中提取的协作信号生成伪交互，然后将这些伪交互添加到现有交互中或用来替换现有交互，从而生成更鲁棒的数据表示。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准数据集上的实验表明，SCAR在关键评估指标上优于之前的基于对比学习的图协同过滤方法以及其他最先进的自监督学习方法。SCAR在不同的超参数设置下表现出强大的鲁棒性，并且在稀疏数据场景中特别有效。&lt;h4&gt;结论&lt;/h4&gt;通过避免定义噪声的模糊性问题，SCAR能够保留核心信息并生成更可靠的数据视图，同时避免了过于复杂的增强模块，是一种更有效且直观的数据增强方法。&lt;h4&gt;翻译&lt;/h4&gt;对比学习(CI)已被广泛用于增强图协同过滤(GCF)的性能以实现个性化推荐。由于数据增强在对比学习的成功中起着关键作用，先前的工作设计了去除用户和项目之间噪声交互的增强方法，以生成有效的增强视图。然而，定义'噪声'的模糊性持续存在丢失核心信息和生成不可靠数据视图的风险，同时增加了增强的整体复杂性。在本文中，我们提出了用于推荐的简单协作增强(SCAR)，这是一种新颖而直观的增强方法，旨在最大化对比学习对图协同过滤的有效性。SCAR不删除信息，而是利用从用户-项目交互中提取的协作信号生成伪交互，然后将这些伪交互添加到现有交互中或用来替换现有交互。这产生了更鲁棒的表示，同时避免了过于复杂的增强模块的缺陷。我们在四个基准数据集上进行了实验，表明SCAR在关键评估指标上优于之前的基于对比学习的图协同过滤方法以及其他最先进的自监督学习方法。SCAR在不同的超参数设置下表现出强大的鲁棒性，并且在稀疏数据场景中特别有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning (CL) has been widely used for enhancing the performanceof graph collaborative filtering (GCF) for personalized recommendation. Sincedata augmentation plays a crucial role in the success of CL, previous workshave designed augmentation methods to remove noisy interactions between usersand items in order to generate effective augmented views. However, theambiguity in defining ''noisiness'' presents a persistent risk of losing coreinformation and generating unreliable data views, while increasing the overallcomplexity of augmentation. In this paper, we propose Simple CollaborativeAugmentation for Recommendation (SCAR), a novel and intuitive augmentationmethod designed to maximize the effectiveness of CL for GCF. Instead ofremoving information, SCAR leverages collaborative signals extracted fromuser-item interactions to generate pseudo-interactions, which are then eitheradded to or used to replace existing interactions. This results in more robustrepresentations while avoiding the pitfalls of overly complex augmentationmodules. We conduct experiments on four benchmark datasets and show that SCARoutperforms previous CL-based GCF methods as well as other state-of-the-artself-supervised learning approaches across key evaluation metrics. SCARexhibits strong robustness across different hyperparameter settings and isparticularly effective in sparse data scenarios.</description>
      <author>example@mail.com (Doyun Choi, Cheonwoo Lee, Jaemin Yoo)</author>
      <guid isPermaLink="false">2511.00436v2</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Mutual Information guided Visual Contrastive Learning</title>
      <link>http://arxiv.org/abs/2511.00028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tech Report - Undergraduate Thesis - 2023&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于互信息的数据选择方法，以提高表征学习在开放环境中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;使用InfoNCE损失的表征学习方法能够有效减少人工标注，但数据选择和增强仍依赖人工假设或工程方法，可能不是最优的。例如，对比学习中的数据增强主要关注颜色抖动，旨在模拟真实世界的光照变化。&lt;h4&gt;目的&lt;/h4&gt;研究基于从实际分布计算互信息来选择训练数据的潜力，使学习到的特征在开放环境中具有更好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;考虑在自然扰动（如颜色变化和运动）下表现出高互信息的场景补丁作为对比损失学习的正样本，提出了一种基于互信息的数据增强方法。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上评估了所提出的互信息感知数据增强方法，证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;基于互信息的数据选择方法是一个有前途的未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;使用InfoNCE损失的表征学习方法已证明通过训练不变性神经网络特征提取器能够显著减少人工标注工作量。尽管不同变体的训练目标遵循数据与学习特征之间的信息最大化原则，但数据选择和增强仍然依赖人类假设或工程方法，这可能不是最优的。例如，对比学习中的数据增强主要关注颜色抖动，旨在模拟真实世界的光照变化。在本工作中，我们研究了基于从实际分布计算的互信息来选择训练数据的潜力，原则上，这应该使学习到的特征在开放环境中应用时具有更好的泛化能力。具体而言，我们将具有自然扰动（如颜色变化和运动）下表现出高互信息的场景补丁视为使用对比损失学习的正样本。我们在多个最先进的表征学习框架的几个基准上评估了所提出的互信息感知数据增强方法，证明了其有效性，并确立了其作为未来研究的有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning methods utilizing the InfoNCE loss have demonstratedconsiderable capacity in reducing human annotation effort by training invariantneural feature extractors. Although different variants of the trainingobjective adhere to the information maximization principle between the data andlearned features, data selection and augmentation still rely on humanhypotheses or engineering, which may be suboptimal. For instance, dataaugmentation in contrastive learning primarily focuses on color jittering,aiming to emulate real-world illumination changes. In this work, we investigatethe potential of selecting training data based on their mutual informationcomputed from real-world distributions, which, in principle, should endow thelearned features with better generalization when applied in open environments.Specifically, we consider patches attached to scenes that exhibit high mutualinformation under natural perturbations, such as color changes and motion, aspositive samples for learning with contrastive loss. We evaluate the proposedmutual-information-informed data augmentation method on several benchmarksacross multiple state-of-the-art representation learning frameworks,demonstrating its effectiveness and establishing it as a promising directionfor future research.</description>
      <author>example@mail.com (Hanyang Chen, Yanchao Yang)</author>
      <guid isPermaLink="false">2511.00028v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Causal Graph Neural Networks for Healthcare</title>
      <link>http://arxiv.org/abs/2511.02531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;医疗人工智能系统在不同机构部署时经常失败，表现为性能下降和历史数据中歧视模式的延续。这种脆弱性部分源于学习统计关联而非因果机制。因果图神经网络通过结合生物医学数据的图表示和因果推理原理，解决了分布转移、歧视和不可解释性的三重危机，学习不变的机制而非虚假相关性。&lt;h4&gt;背景&lt;/h4&gt;医疗人工智能系统在不同机构部署时经常失败，有记录显示性能下降和延续了历史数据中的歧视模式。这种脆弱性部分源于学习统计关联而非因果机制。&lt;h4&gt;目的&lt;/h4&gt;解决医疗人工智能系统的分布转移、歧视和不可解释性三重危机，通过学习不变的机制而非虚假相关性来提高系统性能和公平性。&lt;h4&gt;方法&lt;/h4&gt;因果图神经网络，结合结构因果模型、解纠缠的因果表征学习，以及图上的干预预测和反事实推理技术。&lt;h4&gt;主要发现&lt;/h4&gt;因果图神经网络已应用于精神疾病诊断（脑网络分析）、癌症亚型分类（多组学因果整合）、连续生理监测（机械解释）和药物推荐（纠正处方偏见）。这些进展为患者特异性因果数字孪生奠定基础，结合大型语言模型进行假设生成和因果图神经网络进行机械验证。&lt;h4&gt;结论&lt;/h4&gt;仍存在重大障碍，包括计算需求阻碍实时部署、验证挑战需要多模态证据三角测量、以及因果清洗风险。提出分层框架区分受因果启发的架构和因果验证的发现，并确定关键研究优先事项，以做出因果而非纯粹关联的声明。&lt;h4&gt;翻译&lt;/h4&gt;医疗人工智能系统在不同机构部署时经常失败，有记录显示性能下降和延续了历史数据中的歧视模式。这种脆弱性部分源于学习统计关联而非因果机制。因果图神经网络通过结合生物医学数据的图表示和因果推理原理，解决了分布转移、歧视和不可解释性的三重危机，学习不变的机制而非虚假相关性。本综述审视了方法论基础，包括结构因果模型、解纠缠的因果表征学习，以及图上的干预预测和反事实推理技术。我们分析了具有临床价值的应用，包括通过脑网络分析的精神疾病诊断、通过多组学因果整合的癌症亚型分类、具有机械解释的连续生理监测，以及纠正处方偏见的药物推荐。这些进展为患者特异性因果数字孪生奠定基础，使计算机内临床实验成为可能，并整合大型语言模型进行假设生成和因果图神经网络进行机械验证。仍然存在重大障碍，包括计算需求阻碍实时部署、验证挑战需要超越交叉验证的多模态证据三角测量，以及因果清洗的风险（方法使用因果术语但缺乏严格的证据支持）。我们提出分层框架区分受因果启发的架构和因果验证的发现，并确定关键研究优先事项，以做出因果而非纯粹关联的声明。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Healthcare artificial intelligence systems routinely fail when deployedacross institutions, with documented performance drops and perpetuation ofdiscriminatory patterns embedded in historical data. This brittleness stems, inpart, from learning statistical associations rather than causal mechanisms.Causal graph neural networks address this triple crisis of distribution shift,discrimination, and inscrutability by combining graph-based representations ofbiomedical data with causal inference principles to learn invariant mechanismsrather than spurious correlations. This Review examines methodologicalfoundations spanning structural causal models, disentangled causalrepresentation learning, and techniques for interventional prediction andcounterfactual reasoning on graphs. We analyse applications demonstratingclinical value across psychiatric diagnosis through brain network analysis,cancer subtyping via multi-omics causal integration, continuous physiologicalmonitoring with mechanistic interpretation, and drug recommendation correctingprescription bias. These advances establish foundations for patient-specificCausal Digital Twins, enabling in silico clinical experimentation, withintegration of large language models for hypothesis generation and causal graphneural networks for mechanistic validation. Substantial barriers remain,including computational requirements precluding real-time deployment,validation challenges demanding multi-modal evidence triangulation beyondcross-validation, and risks of causal-washing where methods employ causalterminology without rigorous evidentiary support. We propose tiered frameworksdistinguishing causally-inspired architectures from causally-validateddiscoveries and identify critical research priorities making causal rather thanpurely associational claims.</description>
      <author>example@mail.com (Munib Mesinovic, Max Buhlan, Tingting Zhu)</author>
      <guid isPermaLink="false">2511.02531v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization</title>
      <link>http://arxiv.org/abs/2511.02489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, Submitted to IEEE TIM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于目标检测的跨视图无人机定位框架，通过图神经网络处理图像间和图像内节点关系，有效解决了GNSS受限区域的定位问题，并在异构航空图像匹配中表现出色。&lt;h4&gt;背景&lt;/h4&gt;随着低空经济的快速发展，无人机在巡逻系统中的测量和跟踪变得至关重要。然而，在GNSS受限区域，基于卫星的定位方法容易失效，需要新的定位解决方案。&lt;h4&gt;目的&lt;/h4&gt;开发一个跨视图无人机定位框架，通过目标检测进行地图匹配，有效解决跨时间、跨视图、异构航空图像匹配问题，提高无人机在无卫星信号环境下的定位能力。&lt;h4&gt;方法&lt;/h4&gt;利用现代目标检测从无人机和卫星图像中提取显著实例，集成图神经网络推理图像间和图像内节点关系，采用细粒度的基于图的节点相似度度量方法实现检索和定位。与传统图像检索方法和分类任务方法相比，避免了极坐标重投影、透视变换或生成对抗网络可能带来的错位、内容损失和真实性有限问题。&lt;h4&gt;主要发现&lt;/h4&gt;在公共和真实世界数据集上的广泛实验表明，该方法能有效处理异构外观差异，具有良好泛化能力，适用于具有更大模态差距的场景，如红外-可见光图像匹配。&lt;h4&gt;结论&lt;/h4&gt;该框架为GNSS受限区域的无人机定位提供了有效解决方案，相关数据集将在https://github.com/liutao23/ODGNNLoc.git公开，为后续研究提供支持。&lt;h4&gt;翻译&lt;/h4&gt;随着低空经济的快速增长，无人机已成为巡逻系统中测量和跟踪的关键工具。然而，在GNSS受限区域，基于卫星的定位方法容易失效。本文提出了一种跨视图无人机定位框架，通过目标检测执行地图匹配，旨在有效解决跨时间、跨视图、异构航空图像匹配问题。在典型流程中，无人机视觉定位被表述为图像检索问题：提取特征构建定位地图，并通过将查询图像与具有已知姿态的参考数据库匹配来估计其姿态。由于公开的无人机定位数据集有限，许多方法将定位重新表述为分类任务，并依赖这些数据集中的场景标签来确保准确性。其他方法使用极坐标重投影、透视变换或生成对抗网络来减少跨域差异；然而，它们可能存在错位、内容损失和真实性有限的问题。相比之下，我们利用现代目标检测从无人机和卫星图像中准确提取显著实例，并集成图神经网络来推理图像间和图像内节点关系。使用细粒度的、基于图的节点相似度度量方法，我们的方法实现了强大的检索和定位性能。在公共和真实世界数据集上的广泛实验表明，我们的方法能有效处理异构外观差异，并具有良好的泛化能力，使其适用于具有更大模态差距的场景，如红外-可见光图像匹配。我们的数据集将在以下网址公开：https://github.com/liutao23/ODGNNLoc.git。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid growth of the low-altitude economy, UAVs have become crucialfor measurement and tracking in patrol systems. However, in GNSS-denied areas,satellite-based localization methods are prone to failure. This paper presentsa cross-view UAV localization framework that performs map matching via objectdetection, aimed at effectively addressing cross-temporal, cross-view,heterogeneous aerial image matching. In typical pipelines, UAV visuallocalization is formulated as an image-retrieval problem: features areextracted to build a localization map, and the pose of a query image isestimated by matching it to a reference database with known poses. Becausepublicly available UAV localization datasets are limited, many approachesrecast localization as a classification task and rely on scene labels in thesedatasets to ensure accuracy. Other methods seek to reduce cross-domaindifferences using polar-coordinate reprojection, perspective transformations,or generative adversarial networks; however, they can suffer from misalignment,content loss, and limited realism. In contrast, we leverage modern objectdetection to accurately extract salient instances from UAV and satelliteimages, and integrate a graph neural network to reason about inter-image andintra-image node relationships. Using a fine-grained, graph-basednode-similarity metric, our method achieves strong retrieval and localizationperformance. Extensive experiments on public and real-world datasets show thatour approach handles heterogeneous appearance differences effectively andgeneralizes well, making it applicable to scenarios with larger modality gaps,such as infrared-visible image matching. Our dataset will be publicly availableat the following URL: https://github.com/liutao23/ODGNNLoc.git.</description>
      <author>example@mail.com (Tao Liu, Kan Ren, Qian Chen)</author>
      <guid isPermaLink="false">2511.02489v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Using ensemble learning with hybrid graph neural networks and transformers to predict traffic in cities</title>
      <link>http://arxiv.org/abs/2511.02484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HybridST的混合架构，用于提高城市交通预测的准确性，特别是在复杂的多模式交通环境中。&lt;h4&gt;背景&lt;/h4&gt;智能交通系统(ITS)在城市交通预测方面仍然存在困难，特别是在具有复杂时空动态的大规模多模式交通环境中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确捕捉空间依赖性、长期时间模式和外部信号的新型交通预测模型。&lt;h4&gt;方法&lt;/h4&gt;提出HybridST混合架构，整合图神经网络(GNNs)、多头时间序列Transformer和监督集成学习方法(XGBoost或RandomForest)，以综合捕捉空间依赖性、长期时间模式和包括天气、日历或控制状态在内的外部信号。&lt;h4&gt;主要发现&lt;/h4&gt;在METR-LA、PEMS-BAY和Seattle Loop tree三个公开基准数据集上的实验表明，HybridST在MAE和RMSE等重要指标上始终优于经典基线模型(LSTM, GCN, DCRNN, PDFormer)，同时保持良好的可扩展性和易于理解的特点。&lt;h4&gt;结论&lt;/h4&gt;HybridST框架为实时城市交通规划、能源优化和缓解拥堵策略提供了有前景的途径，特别适用于智慧城市和2030年世界杯等重大活动的交通管理。&lt;h4&gt;翻译&lt;/h4&gt;智能交通系统(ITS)在城市交通预测方面仍然面临挑战，特别是在具有复杂时空动态的大规模多模式交通环境中。本文提出了HybridST，一种混合架构，整合了图神经网络(GNNs)、多头时间序列Transformer和监督集成学习方法(XGBoost或RandomForest)，以共同捕捉空间依赖性、长期时间模式和外部信号，包括天气、日历或控制状态。我们在METR-LA、PEMS-BAY和Seattle Loop tree三个公开基准数据集上测试了我们的模型。这些数据集涵盖了从高速公路传感器网络到车路协同感知的各种场景。实验结果表明，HybridST在MAE和RMSE等重要指标上始终优于经典基线模型(LSTM, GCN, DCRNN, PDFormer)，同时仍然保持高度可扩展性和易于理解的特点。所提出的框架为实时城市交通规划、能源优化和缓解拥堵策略提供了有前景的途径，特别是在智慧城市和2030年世界杯等重大活动的框架内。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.5281/zenodo.17521951&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intelligent transportation systems (ITS) still have a hard time accuratelypredicting traffic in cities, especially in big, multimodal settings withcomplicated spatiotemporal dynamics. This paper presents HybridST, a hybridarchitecture that integrates Graph Neural Networks (GNNs), multi-head temporalTransformers, and supervised ensemble learning methods (XGBoost or RandomForest) to collectively capture spatial dependencies, long-range temporalpatterns, and exogenous signals, including weather, calendar, or controlstates. We test our model on the METR-LA, PEMS-BAY, and Seattle Loop treepublic benchmark datasets. These datasets include situations ranging fromfreeway sensor networks to vehicle-infrastructure cooperative perception.Experimental results show that HybridST consistently beats classical baselines(LSTM, GCN, DCRNN, PDFormer) on important metrics like MAE and RMSE, whilestill being very scalable and easy to understand. The proposed frameworkpresents a promising avenue for real-time urban mobility planning, energyoptimization, and congestion alleviation strategies, especially within theframework of smart cities and significant events such as the 2030 FIFA WorldCup.</description>
      <author>example@mail.com (Ismail Zrigui, Samira Khoulji, Mohamed Larbi Kerkeb)</author>
      <guid isPermaLink="false">2511.02484v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Evolving Graph Learning for Out-of-Distribution Generalization in Non-stationary Environments</title>
      <link>http://arxiv.org/abs/2511.02354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EvoOOD的新型演化图学习框架，通过环境感知不变模式识别来解决动态图在分布偏移下的泛化能力问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在动态图的空间和时间模式利用方面表现出色，但现有GNN在分布偏移下泛化能力差，这在动态场景中是不可避免的。&lt;h4&gt;目的&lt;/h4&gt;探索动态图生成在潜在非平稳环境中对分布外(OOD)泛化的影响，并提出一种用于OOD泛化的新型演化图学习框架。&lt;h4&gt;方法&lt;/h4&gt;设计环境序列变分自编码器建模环境演化并推断环境分布；引入环境感知不变模式识别机制解决环境多样化问题；使用混合实例化环境样本对单个节点进行细粒度因果干预。&lt;h4&gt;主要发现&lt;/h4&gt;该方法有助于区分用于OOD预测的时空不变模式，特别是在非平稳环境中；实验证明了EvoOOD在真实世界和合成动态数据集分布偏移下的优越性。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是首次从环境演化角度研究动态图OOD泛化问题。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在利用动态图上的空间和时间模式方面表现出色。然而，现有的GNN在分布偏移下表现出较差的泛化能力，这在动态场景中是不可避免的。随着动态图生成在潜在非平稳环境中不断推进，探索它们对分布外(OOD)泛化的影响至关重要。本文通过环境感知不变模式识别，提出了一种用于OOD泛化的新型演化图学习框架(EvoOOD)。具体来说，我们首先设计了一个环境序列变分自编码器来建模环境演化并推断潜在环境分布。然后，我们引入了一种环境感知不变模式识别机制，通过推断的分布来解决环境多样化问题。最后，我们使用混合实例化环境样本对单个节点进行细粒度因果干预。这种方法有助于区分用于OOD预测的时空不变模式，特别是在非平稳环境中。实验结果证明了EvoOOD在真实世界和合成动态数据集分布偏移下的优越性。据我们所知，这是首次从环境演化角度研究动态图OOD泛化问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have shown remarkable success in exploiting the spatialand temporal patterns on dynamic graphs. However, existing GNNs exhibit poorgeneralization ability under distribution shifts, which is inevitable indynamic scenarios. As dynamic graph generation progresses amid evolving latentnon-stationary environments, it is imperative to explore their effects onout-of-distribution (OOD) generalization. This paper proposes a novel EvolvingGraph Learning framework for OOD generalization (EvoOOD) by environment-awareinvariant pattern recognition. Specifically, we first design an environmentsequential variational auto-encoder to model environment evolution and inferthe underlying environment distribution. Then, we introduce a mechanism forenvironment-aware invariant pattern recognition, tailored to addressenvironmental diversification through inferred distributions. Finally, weconduct fine-grained causal interventions on individual nodes using a mixtureof instantiated environment samples. This approach helps to distinguishspatio-temporal invariant patterns for OOD prediction, especially innon-stationary environments. Experimental results demonstrate the superiorityof EvoGOOD on both real-world and synthetic dynamic datasets under distributionshifts. To the best of our knowledge, it is the first attempt to study thedynamic graph OOD generalization problem from the environment evolutionperspective.</description>
      <author>example@mail.com (Qingyun Sun, Jiayi Luo, Haonan Yuan, Xingcheng Fu, Hao Peng, Jianxin Li, Philip S. Yu)</author>
      <guid isPermaLink="false">2511.02354v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions</title>
      <link>http://arxiv.org/abs/2511.02288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted for ICDAR2025-WML&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络的手写数学表达式识别方法，通过将数学表达式建模为图结构，结合深度BLSTM网络和图神经网络进行识别和结构优化。&lt;h4&gt;背景&lt;/h4&gt;手写数学表达式识别是一个具有挑战性的任务，需要同时识别符号并理解它们之间的空间关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确识别手写数学表达式并正确理解其结构的方法。&lt;h4&gt;方法&lt;/h4&gt;将手写数学表达式建模为图，其中节点代表符号，边代表空间依赖关系；使用深度BLSTM网络进行符号分割、识别和空间关系分类，形成初始原始图；使用2D-CFG解析器生成所有可能的空间关系；应用基于GNN的链接预测模型优化结构，移除不必要的连接，形成最终的符号标记图。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在手写数学表达式结构识别方面具有良好的性能。&lt;h4&gt;结论&lt;/h4&gt;基于图神经网络的方法在处理手写数学表达式识别任务中是有效的，能够准确识别符号并正确理解它们之间的空间关系。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于图神经网络的手写数学表达式识别方法，通过将手写数学表达式建模为图来表示，其中节点代表符号，边捕获空间依赖关系。使用深度BLSTM网络进行符号分割、识别和空间关系分类，形成初始原始图。然后，2D-CFG解析器生成所有可能的空间关系，而基于GNN的链接预测模型通过移除不必要的连接来优化结构，最终形成符号标记图。实验结果证明了我们方法的有效性，在手写数学表达式结构识别方面显示出良好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a Graph Neural Network (GNN)-based approach for HandwrittenMathematical Expression (HME) recognition by modeling HMEs as graphs, wherenodes represent symbols and edges capture spatial dependencies. A deep BLSTMnetwork is used for symbol segmentation, recognition, and spatial relationclassification, forming an initial primitive graph. A 2D-CFG parser thengenerates all possible spatial relations, while the GNN-based link predictionmodel refines the structure by removing unnecessary connections, ultimatelyforming the Symbol Label Graph. Experimental results demonstrate theeffectiveness of our approach, showing promising performance in HME structurerecognition.</description>
      <author>example@mail.com (Cuong Tuan Nguyen, Ngoc Tuan Nguyen, Triet Hoang Minh Dao, Huy Minh Nhat, Huy Truong Dinh)</author>
      <guid isPermaLink="false">2511.02288v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>PrivGNN: High-Performance Secure Inference for Cryptographic Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2511.02185v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to FC'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究设计并实现了一种名为SysName的轻量级密码学方案，用于在云环境中进行安全图神经网络推断，通过混合加法和函数秘密共享技术，显著提升了计算效率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是分析和学习图结构数据的强大工具，可在多种服务中应用。但在隐私关键型云环境中部署这些服务需要开发安全推断协议来保护敏感的图结构数据。现有解决方案主要关注图像和文本数据的卷积模型，而保护图神经网络和图结构数据的安全挑战相对未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级密码学方案，用于在云环境中安全地进行图神经网络推断，保护敏感图结构数据的同时保持高效性和准确性。&lt;h4&gt;方法&lt;/h4&gt;设计并实现SysName方案，在安全两方计算框架下混合使用加法秘密共享和函数秘密共享，基于一系列新颖的交互协议，优化了线性层和非线性层的计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;SysName与最先进解决方案相比，线性层速度提升1.5倍至1.7倍，非线性层速度提升2倍至15倍。在四个数据集上的实验表明，安全预测速度提升1.3倍至4.7倍，同时保持与明文图属性推断相当的准确性。&lt;h4&gt;结论&lt;/h4&gt;SysName是一种高效、安全的图神经网络推断方案，能够在保护数据隐私的同时提供与明文计算相当的准确性和显著更快的计算速度。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络是分析和学习图结构数据的强大工具，促进广泛的服务应用。在隐私关键的云环境中部署此类服务需要开发安全推断协议以保护敏感的图结构数据。然而，现有的安全推断解决方案主要关注图像和文本数据的卷积模型，而保护图神经网络和图结构数据的挑战相对未被充分探索。在本工作中，我们设计、实现并评估了SysName，这是一种用于云中以图为中心推断的轻量级密码学方案。通过在安全两方计算中混合加法秘密共享和函数秘密共享，SysName基于一系列新颖的交互协议精心设计，与最先进解决方案相比，线性层速度提升1.5倍至1.7倍，非线性层速度提升2倍至15倍。提供了彻底的理论分析以证明SysName的正确性、安全性和轻量级特性。在四个数据集上的广泛实验表明，SysName具有卓越的效率，安全预测速度提升1.3倍至4.7倍，同时保持与明文图属性推断相当的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are powerful tools for analyzing and learningfrom graph-structured (GS) data, facilitating a wide range of services.Deploying such services in privacy-critical cloud environments necessitates thedevelopment of secure inference (SI) protocols that safeguard sensitive GSdata. However, existing SI solutions largely focus on convolutional models forimage and text data, leaving the challenge of securing GNNs and GS datarelatively underexplored. In this work, we design, implement, and evaluate$\sysname$, a lightweight cryptographic scheme for graph-centric inference inthe cloud. By hybridizing additive and function secret sharings within securetwo-party computation (2PC), $\sysname$ is carefully designed based on a seriesof novel 2PC interactive protocols that achieve $1.5\times \sim 1.7\times$speedups for linear layers and $2\times \sim 15\times$ for non-linear layersover state-of-the-art (SotA) solutions. A thorough theoretical analysis isprovided to prove $\sysname$'s correctness, security, and lightweight nature.Extensive experiments across four datasets demonstrate $\sysname$'s superiorefficiency with $1.3\times \sim 4.7\times$ faster secure predictions whilemaintaining accuracy comparable to plaintext graph property inference.</description>
      <author>example@mail.com (Fuyi Wang, Zekai Chen, Mingyuan Fan, Jianying Zhou, Lei Pan, Leo Yu Zhang)</author>
      <guid isPermaLink="false">2511.02185v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking LLM Human Simulation: When a Graph is What You Need</title>
      <link>http://arxiv.org/abs/2511.02135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code: https://github.com/schang-lab/gems&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为GEMS的基于图的轻量级模型，用于人类模拟任务，在保持与大型语言模型相当或更好准确性的同时，显著提高了效率、可解释性和透明度。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型越来越多地被用来模拟人类，应用范围从调查预测到决策制定。然而，这些模型通常体积庞大，计算资源需求高。&lt;h4&gt;目的&lt;/h4&gt;探究在人类模拟任务中，是否可以使用更小、更专业的模型替代大型语言模型，特别是在个体在离散选项中做出选择的场景下。&lt;h4&gt;方法&lt;/h4&gt;提出Graph-basEd Models for human Simulation (GEMS)框架，将离散选择模拟任务转化为图上的链接预测问题，利用关系知识并仅在需要时融入语言表示。使用图神经网络作为基础架构。&lt;h4&gt;主要发现&lt;/h4&gt;在三个模拟数据集上的三种关键设置中评估显示，尽管图神经网络模型比大型语言模型小三个数量级，但它能够匹配或超越强大的大型语言模型基线模型的性能。GEMS在准确性、效率、可解释性和透明度方面均表现出色。&lt;h4&gt;结论&lt;/h4&gt;基于图的建模作为大型语言模型用于人类模拟的轻量级替代方案具有显著前景，特别是在需要高效、可解释和透明的模拟系统的场景中。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型正越来越多地被用来模拟人类，应用范围从调查预测到决策制定。然而，大型语言模型是否严格必要，还是更小、领域专用的模型就足够了？我们确定了一类模拟问题，即个体在离散选项中做出选择的问题，在这类问题中，图神经网络可以匹配甚至超越强大的大型语言模型基线模型，尽管小三个数量级。我们提出了基于图的人类模拟模型，它将离散选择模拟任务作为图上的链接预测问题，利用关系知识，仅在需要时才融入语言表示。在三个模拟数据集上的三种关键设置中的评估表明，GEMS实现了与大型语言模型相当或更好的准确性，同时具有更高的效率、可解释性和透明度，突显了基于图的建模作为大型语言模型用于人类模拟的轻量级替代方案的潜力。我们的代码可在https://github.com/schang-lab/gems获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) are increasingly used to simulate humans, withapplications ranging from survey prediction to decision-making. However, areLLMs strictly necessary, or can smaller, domain-grounded models suffice? Weidentify a large class of simulation problems in which individuals make choicesamong discrete options, where a graph neural network (GNN) can match or surpassstrong LLM baselines despite being three orders of magnitude smaller. Weintroduce Graph-basEd Models for human Simulation (GEMS), which casts discretechoice simulation tasks as a link prediction problem on graphs, leveragingrelational knowledge while incorporating language representations only whenneeded. Evaluations across three key settings on three simulation datasets showthat GEMS achieves comparable or better accuracy than LLMs, with far greaterefficiency, interpretability, and transparency, highlighting the promise ofgraph-based modeling as a lightweight alternative to LLMs for human simulation.Our code is available at https://github.com/schang-lab/gems.</description>
      <author>example@mail.com (Joseph Suh, Suhong Moon, Serina Chang)</author>
      <guid isPermaLink="false">2511.02135v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Microbial Interactions Using Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2511.02038v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures, NeurIPS 2025 Workshop New Perspectives in Graph  Machine Learning&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用图神经网络(GNNs)预测微生物间的相互作用类型，包括二元相互作用(正/负)和复杂相互作用类型(如互利共生、竞争和寄生)，取得了80.44%的F1分数，显著优于传统XGBoost模型的72.76%。&lt;h4&gt;背景&lt;/h4&gt;预测物种间相互作用是微生物生态学中的一个关键挑战，因为这些相互作用对微生物群落的结构和活性起着决定性作用。&lt;h4&gt;目的&lt;/h4&gt;利用单培养生长能力、与其他物种的相互作用以及系统发育数据来预测微生物相互作用的负向或正向效应。&lt;h4&gt;方法&lt;/h4&gt;使用包含超过7,500个相互作用的数据集(涉及两个分类群的20个物种在40种不同碳条件下的共培养)训练模型；构建微生物相互作用的边图，利用图神经网络(GNNs)预测相互作用模式。&lt;h4&gt;主要发现&lt;/h4&gt;模型不仅能预测二元相互作用，还能分类更复杂的相互作用类型；初始结果F1分数达80.44%，显著优于文献中可比的传统XGBoost模型(72.76%)。&lt;h4&gt;结论&lt;/h4&gt;图神经网络是预测微生物相互作用的强大工具，为理解微生物群落结构和功能提供了有效方法。&lt;h4&gt;翻译&lt;/h4&gt;预测物种间相互作用是微生物生态学中的一个关键挑战，因为这些相互作用对微生物群落的结构和活性至关重要。在本工作中，我们使用单培养生长能力、与其他物种的相互作用以及系统发育数据来预测相互作用的负向或正向效应。更准确地说，我们使用了最大的可用成对相互作用数据集之一来训练我们的模型，包含在40种不同碳条件下共培养的两个分类群的20个物种之间的7,500多个相互作用，主要关注Nestor等人[28]的工作。在本工作中，我们提出图神经网络(GNNs)作为预测效应方向的有力分类器。我们构建成对微生物相互作用的边图，以利用单个共培养实验中的共享信息，并使用GNNs预测相互作用模式。我们的模型不仅可以预测二元相互作用(正/负)，还可以分类更复杂的相互作用类型，如互利共生、竞争和寄生。我们的初步结果令人鼓舞，F1分数达到80.44%。这显著优于文献中可比的方法，包括传统的极端梯度提升(XGBoost)模型，后者报告的F1分数为72.76%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting interspecies interactions is a key challenge in microbial ecology,as these interactions are critical to determining the structure and activity ofmicrobial communities. In this work, we used data on monoculture growthcapabilities, interactions with other species, and phylogeny to predict anegative or positive effect of interactions. More precisely, we used one of thelargest available pairwise interaction datasets to train our models, comprisingover 7,500 interactions be- tween 20 species from two taxonomic groupsco-cultured under 40 distinct carbon conditions, with a primary focus on thework of Nestor et al.[28 ]. In this work, we propose Graph Neural Networks(GNNs) as a powerful classifier to predict the direction of the effect. Weconstruct edge-graphs of pairwise microbial interactions in order to leverageshared information across individual co-culture experiments, and use GNNs topredict modes of interaction. Our model can not only predict binaryinteractions (positive/negative) but also classify more complex interactiontypes such as mutualism, competition, and parasitism. Our initial results wereencouraging, achieving an F1-score of 80.44%. This significantly outperformscomparable methods in the literature, including conventional Extreme GradientBoosting (XGBoost) models, which reported an F1-score of 72.76%.</description>
      <author>example@mail.com (Elham Gholamzadeh, Kajal Singla, Nico Scherf)</author>
      <guid isPermaLink="false">2511.02038v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>HyperNQ: A Hypergraph Neural Network Decoder for Quantum LDPC Codes</title>
      <link>http://arxiv.org/abs/2511.01741v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 4 figures, Submitted to the IEEE International Conference on  Communications (ICC 2026). Preprint version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HyperNQ是一种基于超图神经网络的量子低密度奇偶校验码解码器，通过利用超边捕获高阶稳定器约束，显著提高了量子错误纠正性能。&lt;h4&gt;背景&lt;/h4&gt;量子计算需要有效的错误纠正策略来缓解噪声和退相干问题。量子低密度奇偶校验码通过支持恒定速率编码和稀疏奇偶校验结构，已成为可扩展量子错误纠正应用的有前途的解决方案。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕获高阶稳定器约束的QLDPC解码器，以解决传统解码方法如置信传播在短循环存在时收敛性差的问题，以及图神经网络仅限于成对交互而无法捕获高阶相关性的限制。&lt;h4&gt;方法&lt;/h4&gt;提出HyperNQ，第一个基于超图神经网络的QLDPC解码器，利用超边捕获高阶稳定器约束，实现高度表达性和紧凑的解码。采用两阶段消息传递方案，并在伪阈值区域评估解码器性能。&lt;h4&gt;主要发现&lt;/h4&gt;在伪阈值标记以下，HyperNQ将逻辑错误率比置信传播提高了最多84%，比基于图神经网络的策略提高了50%，展示了优于现有最先进解码器的性能。&lt;h4&gt;结论&lt;/h4&gt;HyperNQ通过超图神经网络有效解决了传统QLDPC解码方法的局限性，为量子错误纠正提供了一种创新且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;量子计算需要有效的错误纠正策略来缓解噪声和退相干。量子低密度奇偶校验码通过支持恒定速率编码和稀疏奇偶校验结构，已成为可扩展量子错误纠正应用的有前途的解决方案。然而，通过置信传播等传统方法解码QLDPC码在存在短循环时收敛性差。图神经网络等机器学习技术利用节点特征进行学习消息传递，但它们仅限于Tanner图上的成对交互，限制了捕获高阶相关性的能力。在这项工作中，我们提出了HyperNQ，这是第一个基于超图神经网络的QLDPC解码器，通过利用超边捕获高阶稳定器约束，从而实现高度表达性和紧凑的解码。我们使用两阶段消息传递方案，并在伪阈值区域评估解码器。在伪阈值标记以下，HyperNQ将逻辑错误率比BP提高了最多84%，比基于GNN的策略提高了50%，展示了优于现有最先进解码器的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum computing requires effective error correction strategies to mitigatenoise and decoherence. Quantum Low-Density Parity-Check (QLDPC) codes haveemerged as a promising solution for scalable Quantum Error Correction (QEC)applications by supporting constant-rate encoding and a sparse parity-checkstructure. However, decoding QLDPC codes via traditional approaches such asBelief Propagation (BP) suffers from poor convergence in the presence of shortcycles. Machine learning techniques like Graph Neural Networks (GNNs) utilizelearned message passing over their node features; however, they are restrictedto pairwise interactions on Tanner graphs, which limits their ability tocapture higher-order correlations. In this work, we propose HyperNQ, the firstHypergraph Neural Network (HGNN)- based QLDPC decoder that captureshigher-order stabilizer constraints by utilizing hyperedges-thus enablinghighly expressive and compact decoding. We use a two-stage message passingscheme and evaluate the decoder over the pseudo-threshold region. Below thepseudo-threshold mark, HyperNQ improves the Logical Error Rate (LER) up to 84%over BP and 50% over GNN-based strategies, demonstrating enhanced performanceover the existing state-of-the-art decoders.</description>
      <author>example@mail.com (Ameya S. Bhave, Navnil Choudhury, Kanad Basu)</author>
      <guid isPermaLink="false">2511.01741v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Panther: A Cost-Effective Privacy-Preserving Framework for GNN Training and Inference Services in Cloud Environments</title>
      <link>http://arxiv.org/abs/2511.01654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in IEEE Transactions on Services Computing  (TSC)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Panther，一个在云环境中用于图神经网络训练和推理的经济有效的隐私保护框架，能够在保护隐私的同时显著降低计算和通信成本。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在多个领域有重大影响，随着用户向云计算迁移，在云环境中保护GNN隐私成为关键问题。现有隐私保护技术虽然存在，但计算和通信开销较高，导致财务成本高，限制了其广泛采用。&lt;h4&gt;目的&lt;/h4&gt;保护GNN隐私同时降低额外经济成本，开发一个适用于云环境的经济有效的隐私保护框架。&lt;h4&gt;方法&lt;/h4&gt;Panther利用四方计算异步执行安全数组访问协议，并随机填充GNN节点的邻居信息来保护隐私。&lt;h4&gt;主要发现&lt;/h4&gt;与最先进方法相比，Panther将训练和推理时间分别平均减少75.28%和82.80%，通信开销分别减少52.61%和50.26%，在Google Cloud平台上估计为GNN训练和推理分别节省55.05%和59.00%的财务成本。&lt;h4&gt;结论&lt;/h4&gt;Panther是一个有效的隐私保护框架，能够在保护GNN隐私的同时显著降低计算和通信成本，有望促进隐私保护GNN技术在云环境中的广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在交通状态预测、社交推荐、知识感知问答等领域已产生重大影响。随着越来越多用户转向云计算，在云环境中释放GNN能力的同时保护隐私已成为关键问题。具体而言，GNN的训练数据和推理数据需要防止被外部对手窃取。同时，云计算的经济成本是用户的另一个主要关注点。因此，尽管现有研究已提出云环境中GNN的隐私保护技术，但其额外的计算和通信开销仍然相对较高，导致高额财务成本，限制了用户中的广泛采用。为了在保护GNN隐私的同时降低额外财务成本，我们引入了Panther，这是一个在云环境中用于GNN训练和推理服务的经济有效的隐私保护框架。技术上，Panther利用四方计算异步执行安全数组访问协议，并随机填充GNN节点的邻居信息。我们证明了Panther可以保护GNN模型训练和推理的隐私。我们的评估显示，与最先进的方法相比，Panther分别将训练和推理时间平均减少75.28%和82.80%，通信开销平均减少52.61%和50.26%，这估计在Google Cloud平台上为GNN训练和推理过程分别节省55.05%和59.00%的财务成本（基于按需定价模型）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have marked significant impact in traffic stateprediction, social recommendation, knowledge-aware question answering and soon. As more and more users move towards cloud computing, it has become acritical issue to unleash the power of GNNs while protecting the privacy incloud environments. Specifically, the training data and inference data for GNNsneed to be protected from being stolen by external adversaries. Meanwhile, thefinancial cost of cloud computing is another primary concern for users.Therefore, although existing studies have proposed privacy-preservingtechniques for GNNs in cloud environments, their additional computational andcommunication overhead remain relatively high, causing high financial coststhat limit their widespread adoption among users.  To protect GNN privacy while lowering the additional financial costs, weintroduce Panther, a cost-effective privacy-preserving framework for GNNtraining and inference services in cloud environments. Technically, Pantherleverages four-party computation to asynchronously executing the secure arrayaccess protocol, and randomly pads the neighbor information of GNN nodes. Weprove that Panther can protect privacy for both training and inference of GNNmodels. Our evaluation shows that Panther reduces the training and inferencetime by an average of 75.28% and 82.80%, respectively, and communicationoverhead by an average of 52.61% and 50.26% compared with the state-of-the-art,which is estimated to save an average of 55.05% and 59.00% in financial costs(based on on-demand pricing model) for the GNN training and inference processon Google Cloud Platform.</description>
      <author>example@mail.com (Congcong Chen, Xinyu Liu, Kaifeng Huang, Lifei Wei, Yang Shi)</author>
      <guid isPermaLink="false">2511.01654v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization</title>
      <link>http://arxiv.org/abs/2511.01639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26pages,6figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究开发了一种创新的动态图神经网络模型IVGAE-TAMA-BO，用于预测全球粮食贸易网络中的未来链接。该模型通过捕捉时间演变和结构依赖关系，显著提高了预测准确性，结合贝叶斯优化的模型在各种贸易场景下表现优异。&lt;h4&gt;背景&lt;/h4&gt;全球粮食贸易在确保粮食安全和维持供应链稳定方面发挥着关键作用。然而，粮食贸易网络结构在政治、经济和环境因素影响下动态演变，使得建模和预测未来的贸易链接具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;有效捕捉粮食贸易网络中的时间模式，提高链接预测的准确性和稳健性。&lt;h4&gt;方法&lt;/h4&gt;提出了IVGAE-TAMA-BO，一种新型的动态图神经网络。基于原始IVGAE框架，引入了Trade-Aware Momentum Aggregator (TAMA)来捕捉贸易网络的时间演变，联合建模短期波动和长期结构依赖关系。使用基于动量的结构记忆机制提高预测的稳定性和性能，并使用贝叶斯优化自动调整关键超参数。&lt;h4&gt;主要发现&lt;/h4&gt;这是首次将动态图神经网络应用于粮食贸易网络领域，显著提高了预测性能。在五种作物特定数据集上的实验表明，IVGAE-TAMA明显优于静态IVGAE和其他动态基线，通过有效建模时间依赖性，贝叶斯优化进一步提升了IVGAE-TAMA-BO的性能。&lt;h4&gt;结论&lt;/h4&gt;提出的框架是全球贸易网络结构预测的稳健且可扩展的解决方案，在粮食安全监测和政策决策支持方面具有强大的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;全球粮食贸易在确保粮食安全和维持供应链稳定方面发挥着关键作用。然而，其网络结构在政治、经济和环境因素的影响下动态演变，使得建模和预测未来的贸易链接具有挑战性。因此，有效捕捉粮食贸易网络中的时间模式对于提高链接预测的准确性和稳健性至关重要。本研究引入了IVGAE-TAMA-BO，一种专为模拟不断演变的贸易结构和预测全球粮食贸易网络中未来链接而设计的新型动态图神经网络。据我们所知，这是首次将动态图神经网络应用于该领域，显著提高了预测性能。基于原始IVGAE框架，所提出的模型纳入了一个贸易感知动量聚合器(TAMA)来捕捉贸易网络的时间演变，联合建模短期波动和长期结构依赖关系。基于动量的结构记忆机制进一步提高了预测的稳定性和性能。此外，使用贝叶斯优化来自动调整关键超参数，增强在不同贸易场景下的泛化能力。在五种作物特定数据集上的广泛实验表明，IVGAE-TAMA通过有效建模时间依赖性，明显优于静态IVGAE和其他动态基线，而贝叶斯优化进一步提升了IVGAE-TAMA-BO的性能。这些结果表明，所提出的框架是全球贸易网络结构预测的稳健且可扩展的解决方案，在粮食安全监测和政策决策支持方面具有强大的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Global food trade plays a crucial role in ensuring food security andmaintaining supply chain stability. However, its network structure evolvesdynamically under the influence of geopolitical, economic, and environmentalfactors, making it challenging to model and predict future trade links.Effectively capturing temporal patterns in food trade networks is thereforeessential for improving the accuracy and robustness of link prediction. Thisstudy introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designedto model evolving trade structures and predict future links in global foodtrade networks. To the best of our knowledge, this is the first work to applydynamic graph neural networks to this domain, significantly enhancingpredictive performance. Building upon the original IVGAE framework, theproposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capturethe temporal evolution of trade networks, jointly modeling short-termfluctuations and long-term structural dependencies. A momentum-based structuralmemory mechanism further improves predictive stability and performance. Inaddition, Bayesian optimization is used to automatically tune keyhyperparameters, enhancing generalization across diverse trade scenarios.Extensive experiments on five crop-specific datasets demonstrate thatIVGAE-TAMA substantially outperforms the static IVGAE and other dynamicbaselines by effectively modeling temporal dependencies, while Bayesianoptimization further boosts performance in IVGAE-TAMA-BO. These resultshighlight the proposed framework as a robust and scalable solution forstructural prediction in global trade networks, with strong potential forapplications in food security monitoring and policy decision support.</description>
      <author>example@mail.com (Sicheng Wang, Shuhao Chen, Jingran Zhou, Chengyi Tu)</author>
      <guid isPermaLink="false">2511.01639v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Gated Fusion Enhanced Multi-Scale Hierarchical Graph Convolutional Network for Stock Movement Prediction</title>
      <link>http://arxiv.org/abs/2511.01570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MS-HGFN(多尺度分层图融合网络)的新型模型，用于解决股票市场预测中的挑战，通过分层GNN模块和自上而下的门控方法，有效捕捉股票间的复杂关系和多尺度特征，实验表明该模型在预测准确性和稳定性方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;准确预测股票市场走势由于股票固有的波动性和股票间复杂的相互依赖关系仍然是一个巨大的挑战。&lt;h4&gt;目的&lt;/h4&gt;克服现有多尺度图神经网络在股票预测中忽视的两个关键点：每个股票内部的细微属性模式以及多尺度采样中对粗粒度和细粒度特征的偏见注意力。&lt;h4&gt;方法&lt;/h4&gt;提出MS-HGFN模型，包含一个分层GNN模块，通过在不同时间尺度上学习内部属性模式和外部属性特征形成动态图，并采用自上而下的门控方法促进多尺度时空特征的融合。&lt;h4&gt;主要发现&lt;/h4&gt;使用美国和中国股票市场的真实数据集进行实验，MS-HGFN优于传统和先进模型，预测准确性提高了高达1.4%，在回报模拟中增强了稳定性。&lt;h4&gt;结论&lt;/h4&gt;MS-HGFN通过有效捕捉股票间的复杂关系和多尺度特征，显著提升了股票市场预测的准确性和稳定性。&lt;h4&gt;翻译&lt;/h4&gt;准确预测股票市场走势由于股票固有的波动性和股票间复杂的相互依赖关系仍然是一个巨大的挑战。虽然多尺度图神经网络在建模这些关系方面具有潜力，但它们经常忽视两个关键点：每个股票内部影响股票间相关性的细微的内部属性模式，以及在多尺度采样过程中对粗粒度和细粒度特征的偏见注意力。为了克服这些挑战，我们引入了MS-HGFN(多尺度分层图融合网络)。该模型具有一个分层GNN模块，通过在不同时间尺度上学习内部属性模式和外部属性特征来形成动态图，从而全面捕捉时空依赖关系。此外，自上而下的门控方法促进多尺度时空特征的整合，保留了关键的粗粒度和细粒度特征而不过多干扰。利用美国和中国股票市场的真实数据集进行的实验表明，MS-HGFN优于传统和先进模型，预测准确性提高了高达1.4%，并在回报模拟中增强了稳定性。代码可在https://anonymous.4open.science/r/MS-HGFN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting stock market movements remains a formidable challengedue to the inherent volatility and complex interdependencies among stocks.Although multi-scale Graph Neural Networks (GNNs) hold potential for modelingthese relationships, they frequently neglect two key points: the subtleintra-attribute patterns within each stock affecting inter-stock correlation,and the biased attention to coarse- and fine-grained features duringmulti-scale sampling. To overcome these challenges, we introduce MS-HGFN(Multi-Scale Hierarchical Graph Fusion Network). The model features ahierarchical GNN module that forms dynamic graphs by learning patterns fromintra-attributes and features from inter-attributes over different time scales,thus comprehensively capturing spatio-temporal dependencies. Additionally, atop-down gating approach facilitates the integration of multi-scalespatio-temporal features, preserving critical coarse- and fine-grained featureswithout too much interference. Experiments utilizing real-world datasets fromU.S. and Chinese stock markets demonstrate that MS-HGFN outperforms bothtraditional and advanced models, yielding up to a 1.4% improvement inprediction accuracy and enhanced stability in return simulations. The code isavailable at https://anonymous.4open.science/r/MS-HGFN.</description>
      <author>example@mail.com (Xiaosha Xue, Peibo Duan, Zhipeng Liu, Qi Chu, Changsheng Zhang, Bin zhang)</author>
      <guid isPermaLink="false">2511.01570v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Curvature-aware Graph Network</title>
      <link>http://arxiv.org/abs/2511.01443v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为有效电阻曲率的新型图曲率度量方法，解决了现有Ollivier-Ricci曲率计算复杂度高的问题，在保持相当几何表达能力的同时显著提高了计算效率。&lt;h4&gt;背景&lt;/h4&gt;图曲率能为图神经网络提供几何先验，增强其建模复杂图结构的能力，特别是在结构感知、鲁棒性和理论可解释性方面。现有Ollivier-Ricci曲率虽具有强几何可解释性，但计算复杂度极高，限制了其在大型图数据集上的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种计算效率更高且保持相当几何表达能力的图曲率度量方法，以替代计算复杂度高的Ollivier-Ricci曲率。&lt;h4&gt;方法&lt;/h4&gt;提出有效电阻曲率，使用节点对之间的有效电阻而非最优传输距离来量化沿图边的消息传递难易度。&lt;h4&gt;主要发现&lt;/h4&gt;有效电阻曲率显著优于Ollivier-Ricci曲率在计算效率方面，同时保持了相当的几何表达能力；理论证明了其低计算复杂度和对Ollivier-Ricci曲率的可替代性；实验表明其在多种GNN任务上实现了竞争性性能，同时大幅降低计算开销。&lt;h4&gt;结论&lt;/h4&gt;有效电阻曲率为图神经网络提供了一种高效且具有竞争力的几何先验方法，解决了Ollivier-Ricci曲率在大规模图数据集上的应用限制。&lt;h4&gt;翻译&lt;/h4&gt;图曲率为图神经网络(GNNs)提供几何先验，增强其建模复杂图结构的能力，特别是在结构感知、鲁棒性和理论可解释性方面。在现有方法中，Ollivier-Ricci曲率因其强几何可解释性而被广泛研究，能有效表征节点间的局部几何分布。然而，其极高的计算复杂度限制了其在大型图数据集上的适用性。为应对这一挑战，我们提出了一种新的图曲率度量方法——有效电阻曲率，它使用节点对之间的有效电阻而非最优传输距离来量化沿图边的消息传递难易度。该方法在计算效率上显著优于Ollivier-Ricci曲率，同时保持了相当的几何表达能力。理论上，我们证明了有效电阻曲率的低计算复杂度，并建立了其对Ollivier-Ricci曲率的可替代性。此外，在多种GNN任务上的广泛实验表明，我们的方法在大幅降低计算开销的同时，实现了与Ollivier-Ricci曲率相当的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph curvature provides geometric priors for Graph Neural Networks (GNNs),enhancing their ability to model complex graph structures, particularly interms of structural awareness, robustness, and theoretical interpretability.Among existing methods, Ollivier-Ricci curvature has been extensively studieddue to its strong geometric interpretability, effectively characterizing thelocal geometric distribution between nodes. However, its prohibitively highcomputational complexity limits its applicability to large-scale graphdatasets. To address this challenge, we propose a novel graph curvaturemeasure--Effective Resistance Curvature--which quantifies the ease of messagepassing along graph edges using the effective resistance between node pairs,instead of the optimal transport distance. This method significantlyoutperforms Ollivier-Ricci curvature in computational efficiency whilepreserving comparable geometric expressiveness. Theoretically, we prove the lowcomputational complexity of effective resistance curvature and establish itssubstitutability for Ollivier-Ricci curvature. Furthermore, extensiveexperiments on diverse GNN tasks demonstrate that our method achievescompetitive performance with Ollivier-Ricci curvature while drasticallyreducing computational overhead.</description>
      <author>example@mail.com (Chaoqun Fei, Tinglve Zhou, Tianyong Hao, Yangyang Li)</author>
      <guid isPermaLink="false">2511.01443v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Based Solver for CNF Placement on the Cloud-Continuum</title>
      <link>http://arxiv.org/abs/2511.01343v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures. Presented at PE-WASUN'25 (IEEE International  Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, and  Ubiquitous Networks)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于去噪扩散概率模型(DDPM)的新理论框架，用于解决云原生网络功能(CNFs)在云连续体中的部署问题，实现了比传统方法更快且更有效的解决方案。&lt;h4&gt;背景&lt;/h4&gt;云原生网络功能(CNFs)在云连续体(Cloud-Continuum)中的部署是当前5G和未来6G网络编排的核心挑战。这个过程涉及将互依赖的计算任务(结构化为服务功能链)放置在分布式云基础设施上，同时满足严格的资源、带宽和延迟约束。&lt;h4&gt;目的&lt;/h4&gt;解决传统方法(包括混合整数非线性规划、启发式和强化学习)在可扩展性、约束处理和泛化能力方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出基于去噪扩散概率模型(DDPM)的新理论框架，将部署问题重新概念化为生成图到分配任务，将部署问题编码为异构图，训练图神经网络去噪器迭代细化有噪的CNF到云分配矩阵，并将特定约束的损失直接整合到损失函数中。&lt;h4&gt;主要发现&lt;/h4&gt;在各种拓扑结构上进行了广泛评估，证实该模型始终产生可行解决方案，推理速度比MINLP求解器快几个数量级。&lt;h4&gt;结论&lt;/h4&gt;基于扩散的生成模型在受约束的网络嵌入问题中显示出潜力，对分布式云原生网络功能的实际、可扩展编排产生影响。&lt;h4&gt;翻译&lt;/h4&gt;将云原生网络功能(CNFs)跨云连续体(Cloud-Continuum)的部署代表当前5G和未来6G网络编排中的一个核心挑战。该过程涉及将互依赖的计算任务(结构化为服务功能链)放置在分布式云基础设施上，同时满足严格的资源、带宽和延迟约束。公认的是，包括混合整数非线性规划、启发式和强化学习在内的传统方法在可扩展性、约束处理和泛化能力方面存在局限性。在本研究中，提出了一种基于去噪扩散概率模型(DDPM)的新理论框架用于CNF部署。当前方法将重新概念化部署为生成图到分配任务，其中部署问题被编码为异构图，并且训练图神经网络去噪器来迭代细化有噪的CNF到云分配矩阵。该模型将特定约束的损失直接整合到损失函数中，从而使其能够学习可行的解空间。通过严谨和系统的方法实现了DDPM公式与结构组合约束的集成。已在各种拓扑结构上进行了广泛评估，证实该模型始终产生可行解决方案，推理速度比MINLP求解器快几个数量级。获得的结果证明了基于扩散的生成模型在受约束的网络嵌入问题中的潜力，对分布式云原生网络功能的实际、可扩展编排产生影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The placement of Cloud-Native Network Functions (CNFs) across theCloud-Continuum represents a core challenge in the orchestration of current 5Gand future 6G networks. The process involves the placement of interdependentcomputing tasks, structured as Service Function Chains, over distributed cloudinfrastructures. This is achieved while satisfying strict resource, bandwidthand latency constraints. It is acknowledged that classical approaches,including mixed-integer nonlinear programming, heuristics and reinforcementlearning are limited in terms of scalability, constraint handling andgeneralisation capacity. In the present study, a novel theoretical framework isproposed, which is based on Denoising Diffusion Probabilistic Models (DDPM) forCNF placement. The present approach proposes a reconceptualisation of placementas a generative graph to assignment task, where the placement problem isencoded as a heterogeneous graph, and a Graph Neural Network denoiser istrained to iteratively refine noisy CNF-to-cloud assignment matrices. The modelincorporates constraint-specific losses directly into the loss function,thereby allowing it to learn feasible solution spaces. The integration of theDDPM formulation with structured combinatorial constraints is achieved througha rigorous and systematic approach. Extensive evaluations across diversetopologies have been conducted, which have confirmed that the modelconsistently produces feasible solutions with orders of magnitude fasterinference than MINLP solvers. The results obtained demonstrate the potential ofdiffusion-based generative modelling for constrained network embeddingproblems, making an impact towards the practical, scalable orchestration ofdistributed Cloud-Native Network Functions.</description>
      <author>example@mail.com (Álvaro Vázquez Rodríguez, Manuel Fernández-Veiga, Carlos Giraldo-Rodríguez)</author>
      <guid isPermaLink="false">2511.01343v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems</title>
      <link>http://arxiv.org/abs/2511.01258v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种半监督开集故障诊断(SOFD)框架，用于解决海洋机械系统中未知故障类型的检测问题，增强了深度学习模型在实际工业环境中的适用性。&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的海洋机械系统故障诊断方法在航运业受到关注，但现有研究假设训练和测试数据集的故障类别一致且已知，在受控环境下表现良好。然而实际应用中可能出现训练期间未见的故障类型，导致现有方法失效。&lt;h4&gt;目的&lt;/h4&gt;解决未知故障类型导致现有故障诊断方法失效的问题，增强和扩展深度学习模型在开集故障诊断场景中的适用性。&lt;h4&gt;方法&lt;/h4&gt;提出半监督开集故障诊断(SOFD)框架，包含可靠性子集构建过程，使用监督特征学习模型提取的多层融合特征表示选择未标记测试子集，将标记训练集和伪标记测试子集输入半监督诊断模型，学习判别性特征以实现已知故障分类和未知样本检测。&lt;h4&gt;主要发现&lt;/h4&gt;在公共海事基准数据集上的实验结果证明所提出的SOFD框架具有有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;SOFD框架能够有效处理海洋机械系统中的开集故障诊断问题，在工业应用中具有潜力，可应对实际环境中出现的未知故障类型。&lt;h4&gt;翻译&lt;/h4&gt;最近，基于深度学习模型的海洋机械系统故障诊断方法在航运业引起了广泛关注。大多数现有研究假设训练和测试数据集中的故障类别是一致且已知的，这些方法在受控环境下表现良好。然而，在实践中，可能会出现先前未见或未知的故障类型（即训练期间不存在的分布外或开集观测），导致这些方法失效，并对它们在工业中的广泛部署构成重大挑战。为应对这一挑战，本文提出了一种半监督开集故障诊断(SOFD)框架，增强了深度学习模型在开集故障诊断场景中的适用性。该框架包括一个可靠性子集构建过程，使用监督特征学习模型提取的多层融合特征表示来选择未标记的测试子集。然后，将标记的训练集和伪标记的测试子集输入半监督诊断模型，学习每个类的判别性特征，实现对已知故障的准确分类和未知样本的有效检测。在公共海事基准数据集上的实验结果证明了所提出的SOFD框架的有效性和优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, fault diagnosis methods for marine machinery systems based on deeplearning models have attracted considerable attention in the shipping industry.Most existing studies assume fault classes are consistent and known between thetraining and test datasets, and these methods perform well under controlledenvironment. In practice, however, previously unseen or unknown fault types(i.e., out-of-distribution or open-set observations not present duringtraining) can occur, causing such methods to fail and posing a significantchallenge to their widespread industrial deployment. To address this challenge,this paper proposes a semi-supervised open-set fault diagnosis (SOFD) frameworkthat enhances and extends the applicability of deep learning models in open-setfault diagnosis scenarios. The framework includes a reliability subsetconstruction process, which uses a multi-layer fusion feature representationextracted by a supervised feature learning model to select an unlabeled testsubset. The labeled training set and pseudo-labeled test subset are then fedinto a semi-supervised diagnosis model to learn discriminative features foreach class, enabling accurate classification of known faults and effectivedetection of unknown samples. Experimental results on a public maritimebenchmark dataset demonstrate the effectiveness and superiority of the proposedSOFD framework.</description>
      <author>example@mail.com (Chuyue Lou, M. Amine Atoui)</author>
      <guid isPermaLink="false">2511.01258v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>WindMiL: Equivariant Graph Learning for Wind Loading Prediction</title>
      <link>http://arxiv.org/abs/2511.01226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WindMiL是一种新的机器学习框架，通过结合系统数据集生成与对称感知的图神经网络，实现了建筑物风荷载的高效、可扩展和准确预测，解决了传统方法成本高的问题。&lt;h4&gt;背景&lt;/h4&gt;准确预测建筑物风荷载对结构安全和可持续设计至关重要，但传统方法如风洞测试和大涡模拟(LES)成本过高，每个LES案例需要至少24小时计算时间，使得全面参数研究不可行。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效、准确预测建筑物风荷载的方法，降低计算成本，使大规模参数研究成为可能。&lt;h4&gt;方法&lt;/h4&gt;构建大规模风荷载数据集，通过符号距离函数插值处理屋面几何形状，模拟462个不同形状和风向的案例；开发反射等变性图神经网络，确保在镜像几何形状下的物理一致性预测。&lt;h4&gt;主要发现&lt;/h4&gt;WindMiL在插值和外推评估中取得高精度，表面压力系数的平均值和标准差的误差小于等于0.02；在反射测试评估中保持96%以上的命中率，比非等变性基线模型提高10%以上。&lt;h4&gt;结论&lt;/h4&gt;WindMiL通过将系统数据集与等变性代理模型配对，实现了建筑物风荷载的高效、可扩展和准确预测，为结构安全和可持续设计提供了实用工具。&lt;h4&gt;翻译&lt;/h4&gt;准确预测建筑物上的风荷载对于结构安全和可持续设计至关重要，然而传统方法如风洞测试和大涡模拟(LES)对于大规模探索来说成本过高。每个LES案例通常需要至少24小时的计算时间，这使得全面的参数研究不可行。我们提出了WindMiL，这是一种新的机器学习框架，结合了系统数据集生成与对称感知的图神经网络(GNNs)。首先，我们通过对屋面几何形状应用符号距离函数插值，并在不同形状和风向条件下模拟462个案例，构建了一个关于低层建筑物风荷载的大规模数据集。其次，我们开发了一种反射等变性GNN，确保在镜像几何形状下物理预测的一致性。在插值和外推评估中，WindMiL在表面压力系数的平均值和标准差方面都取得了高精度，并且在反射测试评估中保持准确，命中率保持在96%以上，而非等变性基线模型的命中率下降了10%以上。通过将系统数据集与等变性代理模型配对，WindMiL能够实现建筑物风荷载的高效、可扩展和准确的预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of wind loading on buildings is crucial for structuralsafety and sustainable design, yet conventional approaches such as wind tunneltesting and large-eddy simulation (LES) are prohibitively expensive forlarge-scale exploration. Each LES case typically requires at least 24 hours ofcomputation, making comprehensive parametric studies infeasible. We introduceWindMiL, a new machine learning framework that combines systematic datasetgeneration with symmetry-aware graph neural networks (GNNs). First, weintroduce a large-scale dataset of wind loads on low-rise buildings by applyingsigned distance function interpolation to roof geometries and simulating 462cases with LES across varying shapes and wind directions. Second, we develop areflection-equivariant GNN that guarantees physically consistent predictionsunder mirrored geometries. Across interpolation and extrapolation evaluations,WindMiL achieves high accuracy for both the mean and the standard deviation ofsurface pressure coefficients (e.g., RMSE $\leq 0.02$ for mean $C_p$) andremains accurate under reflected-test evaluation, maintaining hit rates above$96\%$ where the non-equivariant baseline model drops by more than $10\%$. Bypairing a systematic dataset with an equivariant surrogate, WindMiL enablesefficient, scalable, and accurate predictions of wind loads on buildings.</description>
      <author>example@mail.com (Themistoklis Vargiemezis, Charilaos Kanatsoulis, Catherine Gorlé)</author>
      <guid isPermaLink="false">2511.01226v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>An Interdisciplinary and Cross-Task Review on Missing Data Imputation</title>
      <link>http://arxiv.org/abs/2511.01196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于缺失数据插补方法的综合性综述，连接了统计基础与现代机器学习进展，涵盖了从传统到现代的各种插补方法，特别关注复杂数据类型和与下游任务的集成，并提出了未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;缺失数据是数据科学中的基本挑战，在医疗保健、生物信息学、社会科学、电子商务和工业监控等多个领域显著阻碍了分析和决策。尽管有数十年的研究和多种插补方法，但文献仍然分散在不同领域，缺乏将统计基础与现代机器学习进展联系起来的全面综合。&lt;h4&gt;目的&lt;/h4&gt;系统性地回顾缺失数据的核心概念，包括缺失机制、单次与多次插补以及不同的插补目标；检查各领域的问题特征；提供插补方法的全面分类；研究插补与下游任务的集成；评估理论保证、基准资源和评估指标；确定关键挑战和未来方向。&lt;h4&gt;方法&lt;/h4&gt;从经典技术（如回归、EM算法）到现代方法（如低秩和高秩矩阵补全、深度学习模型、大型语言模型）进行全面分类；特别关注复杂数据类型（张量、时间序列、流数据、图结构数据、分类数据和多模态数据）的插补方法；研究插补与下游任务（分类、聚类、异常检测）的集成方式，包括顺序管道和联合优化框架；评估理论保证、基准资源和评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;提供了从传统到现代的插补方法全面分类；特别关注了复杂数据类型的插补方法；探讨了插补与下游任务的集成方式；强调了模型选择和超参数优化的重要性；指出了隐私保护插补的日益重要性；提出了追求可泛化模型的方向。&lt;h4&gt;结论&lt;/h4&gt;确定了关键挑战和未来方向，包括模型选择和超参数优化的重要性；通过联邦学习进行隐私保护插补的日益重要性；追求能够跨领域和数据类型适应的可泛化模型；为未来研究勾勒出路线图。&lt;h4&gt;翻译&lt;/h4&gt;缺失数据是数据科学中的一个基本挑战，在医疗保健、生物信息学、社会科学、电子商务和工业监控等广泛领域显著阻碍了分析和决策。尽管有数十年的研究和众多的插补方法，但文献仍然分散在不同领域，迫切需要将统计基础与现代机器学习进展联系起来的全面综合。本工作系统性地回顾了核心概念，包括缺失机制、单次与多次插补以及不同的插补目标，并检查了各领域的问题特征。它提供了插补方法的全面分类，涵盖从经典技术（如回归、EM算法）到现代方法，如低秩和高秩矩阵补全、深度学习模型（自编码器、GAN、扩散模型、图神经网络）和大型语言模型。特别关注了复杂数据类型的方法，如张量、时间序列、流数据、图结构数据、分类数据和多模态数据。除了方法论，我们还研究了插补与下游任务（如分类、聚类和异常检测）的关键集成，检查了顺序管道和联合优化框架。该综述还评估了理论保证、基准资源和评估指标。最后，我们确定了关键挑战和未来方向，强调模型选择和超参数优化的重要性、通过联邦学习进行隐私保护插补的日益重要性，以及追求能够跨领域和数据类型适应的可泛化模型，从而为未来研究勾勒出路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Missing data is a fundamental challenge in data science, significantlyhindering analysis and decision-making across a wide range of disciplines,including healthcare, bioinformatics, social science, e-commerce, andindustrial monitoring. Despite decades of research and numerous imputationmethods, the literature remains fragmented across fields, creating a criticalneed for a comprehensive synthesis that connects statistical foundations withmodern machine learning advances. This work systematically reviews coreconcepts-including missingness mechanisms, single versus multiple imputation,and different imputation goals-and examines problem characteristics acrossvarious domains. It provides a thorough categorization of imputation methods,spanning classical techniques (e.g., regression, the EM algorithm) to modernapproaches like low-rank and high-rank matrix completion, deep learning models(autoencoders, GANs, diffusion models, graph neural networks), and largelanguage models. Special attention is given to methods for complex data types,such as tensors, time series, streaming data, graph-structured data,categorical data, and multimodal data. Beyond methodology, we investigate thecrucial integration of imputation with downstream tasks like classification,clustering, and anomaly detection, examining both sequential pipelines andjoint optimization frameworks. The review also assesses theoretical guarantees,benchmarking resources, and evaluation metrics. Finally, we identify criticalchallenges and future directions, emphasizing model selection andhyperparameter optimization, the growing importance of privacy-preservingimputation via federated learning, and the pursuit of generalizable models thatcan adapt across domains and data types, thereby outlining a roadmap for futureresearch.</description>
      <author>example@mail.com (Jicong Fan)</author>
      <guid isPermaLink="false">2511.01196v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2511.00908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出GraphGeo，一种基于异构图神经网络的多代理辩论框架，用于视觉地理定位任务，通过结构化辩论将代理间的认知冲突转化为增强的定位精度。&lt;h4&gt;背景&lt;/h4&gt;视觉地理定位需要广泛地理知识和复杂推理来确定无GPS元数据的图像位置；传统检索方法受限于数据库覆盖和质量；大型视觉-语言模型虽能直接从图像内容推理位置，但单个模型难以处理多样化地理区域和复杂场景；现有多代理系统统一处理所有代理交互，缺乏有效处理冲突预测的机制。&lt;h4&gt;目的&lt;/h4&gt;提出GraphGeo框架，利用异构图神经网络和结构化辩论机制提高视觉地理定位的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用类型化边建模多样化辩论关系，区分支持性协作、竞争性论证和知识转移；引入双重辩论机制，结合节点级细化和边级论证建模；采用跨层拓扑细化策略，实现图结构与代理表示的共同演化。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试中，GraphGeo显著优于最先进的方法，有效将代理间的认知冲突转化为增强的地理定位精度。&lt;h4&gt;结论&lt;/h4&gt;通过结构化辩论机制，GraphGeo能够有效处理代理间的认知冲突，显著提升视觉地理定位的性能，为多代理系统在复杂推理任务中的应用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;视觉地理定位需要广泛的地理知识和复杂的推理来确定没有GPS元数据的图像位置。传统检索方法受限于数据库覆盖范围和质量。最近的大型视觉-语言模型(LVLMs)能够直接从图像内容进行位置推理，但单个模型难以处理多样化的地理区域和复杂场景。现有的多代理系统通过模型协作提高性能，但统一处理所有代理交互，缺乏有效处理冲突预测的机制。我们提出GraphGeo，一个使用异构图神经网络进行视觉地理定位的多代理辩论框架。我们的方法通过类型化边建模多样化的辩论关系，区分支持性协作、竞争性论证和知识转移。我们引入了双重辩论机制，结合节点级细化和边级论证建模。跨层拓扑细化策略实现了图结构与代理表示的共同演化。在多个基准测试中的实验表明，GraphGeo显著优于最先进的方法。我们的框架通过结构化辩论将代理之间的认知冲突转化为增强的地理定位精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual geo-localization requires extensive geographic knowledge andsophisticated reasoning to determine image locations without GPS metadata.Traditional retrieval methods are constrained by database coverage and quality.Recent Large Vision-Language Models (LVLMs) enable direct location reasoningfrom image content, yet individual models struggle with diverse geographicregions and complex scenes. Existing multi-agent systems improve performancethrough model collaboration but treat all agent interactions uniformly. Theylack mechanisms to handle conflicting predictions effectively. We propose\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graphneural networks for visual geo-localization. Our approach models diverse debaterelationships through typed edges, distinguishing supportive collaboration,competitive argumentation, and knowledge transfer. We introduce a dual-leveldebate mechanism combining node-level refinement and edge-level argumentationmodeling. A cross-level topology refinement strategy enables co-evolutionbetween graph structure and agent representations. Experiments on multiplebenchmarks demonstrate GraphGeo significantly outperforms state-of-the-artmethods. Our framework transforms cognitive conflicts between agents intoenhanced geo-localization accuracy through structured debate.</description>
      <author>example@mail.com (Heng Zheng, Yuling Shi, Xiaodong Gu, Haochen You, Zijian Zhang, Lubin Gan, Hao Zhang, Wenjun Huang, Jin Huang)</author>
      <guid isPermaLink="false">2511.00908v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval</title>
      <link>http://arxiv.org/abs/2511.00268v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025 (Main)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了IL-PCR语料库，用于法规检索和先例检索两项任务的共同开发，并利用LLM重新排序方法取得了最佳性能。&lt;h4&gt;背景&lt;/h4&gt;法律从业者经常需要识别和检索相关法规和先例案例，但研究人员至今独立处理这两项任务，忽略了它们之间的内在联系。&lt;h4&gt;目的&lt;/h4&gt;解决法规检索和先例检索任务之间的研究缺口，开发可以利用两项任务依赖关系的模型。&lt;h4&gt;方法&lt;/h4&gt;提出IL-PCR语料库作为共同测试平台，使用词汇模型、语义模型和基于GNN的集成模型进行实验，并开发基于LLM的重新排序方法。&lt;h4&gt;主要发现&lt;/h4&gt;基于LLM的重新排序方法能够有效利用法规检索和先例检索任务之间的依赖关系，取得了最佳性能。&lt;h4&gt;结论&lt;/h4&gt;通过IL-PCR语料库和基于LLM的重新排序方法，可以更好地同时处理法规检索和先例检索任务，提高法律检索的效率和准确性。&lt;h4&gt;翻译&lt;/h4&gt;识别/检索给定法律情况下的相关法规和先例/判例是法律从业者常见的任务。迄今为止，研究人员独立处理这两项任务，为每个任务开发了完全不同的数据集和模型；然而，这两项检索任务本质上是相关的，例如，相似的案例往往会引用相似的法规（由于相似的事实情况）。在本文中，我们解决了这一研究缺口。我们提出了IL-PCR（印度法律先例和法规检索语料库），这是一个独特的语料库，为开发法规检索和先例检索两项任务的模型提供了共同测试平台，可以利用两项任务之间的依赖关系。我们使用多种基线模型对这两项任务进行了广泛实验，包括词汇模型、语义模型和基于GNN的集成模型。此外，为了利用两项任务之间的依赖关系，我们开发了一种基于LLM的重新排序方法，取得了最佳性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying/retrieving relevant statutes and prior cases/precedents for agiven legal situation are common tasks exercised by law practitioners.Researchers to date have addressed the two tasks independently, thus developingcompletely different datasets and models for each task; however, both retrievaltasks are inherently related, e.g., similar cases tend to cite similar statutes(due to similar factual situation). In this paper, we address this gap. Wepropose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),which is a unique corpus that provides a common testbed for developing modelsfor both the tasks (Statute Retrieval and Precedent Retrieval) that can exploitthe dependence between the two. We experiment extensively with several baselinemodels on the tasks, including lexical models, semantic models and ensemblebased on GNNs. Further, to exploit the dependence between the two tasks, wedevelop an LLM-based re-ranking approach that gives the best performance.</description>
      <author>example@mail.com (Shounak Paul, Dhananjay Ghumare, Pawan Goyal, Saptarshi Ghosh, Ashutosh Modi)</author>
      <guid isPermaLink="false">2511.00268v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials</title>
      <link>http://arxiv.org/abs/2511.00113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MeixnerNet是一种新的谱图神经网络架构，使用离散正交多项式而非连续正交多项式，解决了连续滤波器应用于离散图结构的理论不匹配问题。&lt;h4&gt;背景&lt;/h4&gt;Spectral GNNs通过在谱域定义图卷积实现了最先进的结果，而ChebyNet使用基于连续正交多项式的滤波器是一种常见方法。&lt;h4&gt;目的&lt;/h4&gt;解决连续域滤波器应用于离散图结构造成的不匹配问题，提高模型性能并增强对超参数设置的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;引入MeixnerNet，使用离散Meixner多项式作为滤波器基础，使多项式参数可学习，并通过结合Laplacian缩放和LayerNorm的技术解决数值不稳定性问题。&lt;h4&gt;主要发现&lt;/h4&gt;在最佳设置下，MeixnerNet在三个基准测试中胜过ChebyNet两个；更重要的是，MeixnerNet对多项式次数的变化异常稳健，而ChebyNet对此超参数非常脆弱。&lt;h4&gt;结论&lt;/h4&gt;使用离散正交多项式而非连续正交多项式可以解决理论不匹配问题，提高模型性能和对超参数的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;谱图神经网络通过在谱域定义图卷积取得了最先进的结果。ChebyNet推广的一种常见方法是使用基于连续正交多项式（如Chebyshev）的多项式滤波器。这造成了理论上的脱节，因为这些连续域滤波器被应用于本质上离散的图结构。我们假设这种不匹配可能导致次优性能和对超参数设置的脆弱性。在本文中，我们介绍了MeixnerNet，一种新的谱GNN架构，它采用离散正交多项式——特别是Meixner多项式。我们的模型使多项式的两个关键形状参数可学习，允许滤波器根据给定图的特定谱属性调整其多项式基。我们通过引入一种结合Laplacian缩放和每个基的LayerNorm的新稳定技术，克服了这些多项式显著的数值不稳定性。实验证明，在最佳设置下，MeixnerNet与强大的ChebyNet基线相比具有竞争性到优越的性能（在3个基准测试中赢得2个）。更重要的是，我们表明MeixnerNet对多项式次数的变化异常稳健，而ChebyNet对此超参数证明是非常脆弱的，在性能崩溃的地方MeixnerNet保持稳定。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art resultsby defining graph convolutions in the spectral domain. A common approach,popularized by ChebyNet, is to use polynomial filters based on continuousorthogonal polynomials (e.g., Chebyshev). This creates a theoreticaldisconnect, as these continuous-domain filters are applied to inherentlydiscrete graph structures. We hypothesize this mismatch can lead to suboptimalperformance and fragility to hyperparameter settings.  In this paper, we introduce MeixnerNet, a novel spectral GNN architecturethat employs discrete orthogonal polynomials -- specifically, the Meixnerpolynomials $M_k(x; \beta, c)$. Our model makes the two key shape parameters ofthe polynomial, beta and c, learnable, allowing the filter to adapt itspolynomial basis to the specific spectral properties of a given graph. Weovercome the significant numerical instability of these polynomials byintroducing a novel stabilization technique that combines Laplacian scalingwith per-basis LayerNorm.  We demonstrate experimentally that MeixnerNet achievescompetitive-to-superior performance against the strong ChebyNet baseline at theoptimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, weshow that MeixnerNet is exceptionally robust to variations in the polynomialdegree K, a hyperparameter to which ChebyNet proves to be highly fragile,collapsing in performance where MeixnerNet remains stable.</description>
      <author>example@mail.com (Huseyin Goksu)</author>
      <guid isPermaLink="false">2511.00113v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything</title>
      <link>http://arxiv.org/abs/2511.02834v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 7 figures, 14 tables. Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Agent-Omni框架通过主代理系统协调现有基础模型，实现了无需重新训练的灵活多模态推理，在多种模态和全能任务上取得了最先进性能，特别是复杂跨模态推理任务，且具有模块化、可扩展和透明的特点。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型（MLLMs）已展现出强大的能力，但仍局限于固定的模态对，并且需要使用大型对齐数据集进行昂贵的微调。构建能够完全整合文本、图像、音频和视频的全能模型仍然不切实际，且缺乏强大的推理支持。&lt;h4&gt;目的&lt;/h4&gt;提出一个Agent-Omni框架，通过主代理系统协调现有基础模型，实现灵活的多模态推理，无需重新训练。&lt;h4&gt;方法&lt;/h4&gt;主代理解释用户意图，将子任务委托给特定模态的代理，并将它们的输出整合成连贯的响应。&lt;h4&gt;主要发现&lt;/h4&gt;在文本、图像、音频、视频和全能基准上的广泛实验表明，Agent-Omni始终取得了最先进的性能，特别是在需要复杂跨模态推理的任务上。其基于代理的设计实现了专业基础模型的无缝集成，确保了对多样化输入的适应性，同时保持透明性和可解释性。&lt;h4&gt;结论&lt;/h4&gt;该框架是模块化的且易于扩展，允许随着更强大模型的可用而进行未来改进。作者发布了开源实现，以支持对可扩展和可靠的全模态推理的持续研究。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型（MLLMs）已展现出强大的能力，但仍局限于固定的模态对，并且需要使用大型对齐数据集进行昂贵的微调。构建能够完全整合文本、图像、音频和视频的全能模型仍然不切实际，且缺乏强大的推理支持。在本文中，我们提出了一个Agent-Omni框架，通过主代理系统协调现有基础模型，实现了无需重新训练的灵活多模态推理。主代理解释用户意图，将子任务委托给特定模态的代理，并将它们的输出整合成连贯的响应。在文本、图像、音频、视频和全能基准上的广泛实验表明，Agent-Omni始终取得了最先进的性能，特别是在需要复杂跨模态推理的任务上。其基于代理的设计实现了专业基础模型的无缝集成，确保了对多样化输入的适应性，同时保持透明性和可解释性。此外，该框架是模块化的且易于扩展，允许随着更强大模型的可用而进行未来改进。我们发布了开源实现，以支持对可扩展和可靠的全模态推理的持续研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have shown strong capabilities butremain limited to fixed modality pairs and require costly fine-tuning withlarge aligned datasets. Building fully omni-capable models that can integratetext, images, audio, and video remains impractical and lacks robust reasoningsupport. In this paper, we propose an Agent-Omni framework that coordinatesexisting foundation models through a master-agent system, enabling flexiblemultimodal reasoning without retraining. The master agent interprets userintent, delegates subtasks to modality-specific agents, and integrates theiroutputs into coherent responses. Extensive experiments across text, image,audio, video, and omni benchmarks show that Agent-Omni consistently achievesstate-of-the-art performance, particularly on tasks requiring complexcross-modal reasoning. Its agent-based design enables seamless integration ofspecialized foundation models, ensuring adaptability to diverse inputs whilemaintaining transparency and interpretability. In addition, the framework ismodular and easily extensible, allowing future improvements as stronger modelsbecome available. %We release an open-source implementation to supportcontinued research on scalable and reliable omni-modal reasoning.</description>
      <author>example@mail.com (Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh)</author>
      <guid isPermaLink="false">2511.02834v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>GeoCrossBench: Cross-Band Generalization for Remote Sensing</title>
      <link>http://arxiv.org/abs/2511.02831v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了GeoCrossBench基准测试，用于评估遥感基础模型对新卫星的泛化能力，并开发了ChiViT模型来改善跨卫星性能。研究发现现有模型在分布内表现不佳，对无波段重叠卫星的泛化能力显著下降，但对额外波段有一定适应能力。仅微调最后一层线性层即可获得一致性能，表明该基准远未饱和。&lt;h4&gt;背景&lt;/h4&gt;遥感卫星的数量和多样性随时间增长，而大多数标记数据来自较老的卫星。随着地球观测基础模型的扩展，支持新卫星的训练成本增加，模型对新卫星的泛化能力变得尤为重要。&lt;h4&gt;目的&lt;/h4&gt;介绍GeoCrossBench基准测试，扩展流行的GeoBench基准，并开发新的评估协议来测试模型对无波段重叠卫星和具有额外波段卫星的泛化能力。同时开发ChiViT模型以改善跨卫星性能。&lt;h4&gt;方法&lt;/h4&gt;创建GeoCrossBench基准测试，开发ChannelViT的自监督扩展ChiViT，并进行多项实验：评估分布内性能、评估对无波段重叠卫星的泛化能力、评估对具有额外波段卫星的泛化能力，以及测试仅微调最后一层线性层的性能。&lt;h4&gt;主要发现&lt;/h4&gt;1) 即使最好的遥感基础模型在分布内设置下也不如通用模型如DINOv3；2) 当泛化到无波段重叠的新卫星时，所有模型性能下降2-4倍，ChiViT显著优于第二名DINOv3；3) 当测试时提供额外波段，所有测试模型性能平均下降5-25%；4) 仅使用所有波段标签微调最后一层线性层，可在所有卫星上获得相对一致的性能。&lt;h4&gt;结论&lt;/h4&gt;该基准测试远未达到饱和。作者公开发布了代码和数据集，以鼓励开发具有更强跨卫星泛化能力的更面向未来的遥感模型。&lt;h4&gt;翻译&lt;/h4&gt;随着遥感卫星的数量和多样性随时间增长，而绝大多数标记数据来自较老的卫星。随着地球观测基础模型的扩展，支持新卫星的(重新)训练成本也在增加，因此模型对新卫星的泛化能力变得越来越重要。在这项工作中，我们介绍了GeoCrossBench，这是流行的GeoBench基准的扩展，具有新的评估协议：它测试分布内性能；对无波段重叠卫星的泛化能力；以及对具有训练集之外额外波段卫星的泛化能力。我们还开发了ChannelViT的自监督扩展ChiViT，以改善其跨卫星性能。首先，我们表明即使最好的遥感基础模型(DOFA, TerraFM)在分布内设置下也不如通用模型如DINOv3。其次，当泛化到无波段重叠的新卫星时，所有模型性能下降2-4倍，而ChiViT显著优于第二名DINOv3。第三，当测试时提供额外波段，所有测试模型的性能平均下降5-25%。最后，我们表明仅使用所有波段标签微调这些模型的最后一层线性层，可以在所有卫星上获得相对一致的性能，这突显出该基准远未饱和。我们公开发布了代码和数据集，以鼓励开发具有更强跨卫星泛化能力的更面向未来的遥感模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The number and diversity of remote sensing satellites grows over time, whilethe vast majority of labeled data comes from older satellites. As thefoundation models for Earth observation scale up, the cost of (re-)training tosupport new satellites grows too, so the generalization capabilities of themodels towards new satellites become increasingly important. In this work weintroduce GeoCrossBench, an extension of the popular GeoBench benchmark with anew evaluation protocol: it tests the in-distribution performance;generalization to satellites with no band overlap; and generalization tosatellites with additional bands with respect to the training set. We alsodevelop a self-supervised extension of ChannelViT, ChiViT, to improve itscross-satellite performance. First, we show that even the best foundationmodels for remote sensing (DOFA, TerraFM) do not outperform general purposemodels like DINOv3 in the in-distribution setting. Second, when generalizing tonew satellites with no band overlap, all models suffer 2-4x drop inperformance, and ChiViT significantly outperforms the runner-up DINOv3. Third,the performance of all tested models drops on average by 5-25\% when givenadditional bands during test time. Finally, we show that fine-tuning just thelast linear layer of these models using oracle labels from all bands can getrelatively consistent performance across all satellites, highlighting that thebenchmark is far from being saturated. We publicly release the code and thedatasets to encourage the development of more future-proof remote sensingmodels with stronger cross-satellite generalization.</description>
      <author>example@mail.com (Hakob Tamazyan, Ani Vanyan, Alvard Barseghyan, Anna Khosrovyan, Evan Shelhamer, Hrant Khachatrian)</author>
      <guid isPermaLink="false">2511.02831v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>PLUTO-4: Frontier Pathology Foundation Models</title>
      <link>http://arxiv.org/abs/2511.02826v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究人员介绍了PLUTO-4，这是新一代病理基础模型，扩展了Pathology-Universal Transformer到前沿规模。该模型家族包含两种互补的Vision Transformer架构：PLUTO-4S（紧凑高效，适合多规模部署）和PLUTO-4G（前沿规模，最大化表示能力）。模型在大型多机构数据集上预训练，并在多种病理学任务上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型在大型病理图像语料库上的训练显示出在多种组织病理学任务中的强大迁移能力。&lt;h4&gt;目的&lt;/h4&gt;开发下一代病理基础模型PLUTO-4，扩展Pathology-Universal Transformer到前沿规模，提升病理图像分析能力。&lt;h4&gt;方法&lt;/h4&gt;提供两种互补的Vision Transformer架构：PLUTO-4S（使用FlexiViT设置和2D-RoPE嵌入）和PLUTO-4G（使用单一补丁大小训练）。模型使用基于DINOv2的自监督目标进行预训练，训练数据包含来自137,144名患者的551,164例WSI，跨越50个机构，覆盖60多种疾病类型和100多种染色方法。&lt;h4&gt;主要发现&lt;/h4&gt;PLUTO-4在需要不同空间和生物学背景的任务上达到最先进性能，包括补丁级分类、分割和幻灯片级诊断。PLUTO-4S提供高吞吐量和稳健性能，适合实际部署；PLUTO-4G在多个病理学基准上建立新性能前沿，在皮肤病理学诊断中提高11%性能。&lt;h4&gt;结论&lt;/h4&gt;PLUTO-4的多样化改进凸显了其作为转化研究和诊断用例骨干的潜力，能够改变现实世界应用。&lt;h4&gt;翻译&lt;/h4&gt;在大型病理图像语料库上训练的基础模型已显示出在多种组织病理学任务中的强大迁移能力。基于这一进展，我们介绍了PLUTO-4，我们的下一代病理基础模型，将病理学通用转换器(PLUTO)扩展到前沿规模。我们在PLUTO-4家族中分享了两种互补的Vision Transformer架构：一个紧凑高效的PLUTO-4S模型，使用带有2D-RoPE嵌入的FlexiViT设置进行优化，适用于多规模部署；以及一个前沿规模的PLUTO-4G模型，使用单一补丁大小训练以最大化表示能力和稳定性。两个模型都使用从DINOv2衍生的自监督目标进行预训练，在包含来自137,144名患者的551,164例WSI的大型多机构语料库上训练，跨越50个机构，涵盖60多种疾病类型和100多种染色方法。在公共和内部基准上的全面评估表明，PLUTO-4在需要不同空间和生物学背景的任务上实现了最先进的性能，包括补丁级分类、分割和幻灯片级诊断。紧凑的PLUTO-4S为实际部署提供高吞吐量和稳健性能，而PLUTO-4G在多个病理学基准上建立了新的性能前沿，包括在皮肤病理学诊断中提高11%的性能。这些多样化的改进强调了PLUTO-4作为转化研究和诊断用例骨干的潜力，可以改变现实世界应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models trained on large-scale pathology image corpora havedemonstrated strong transfer capabilities across diverse histopathology tasks.Building on this progress, we introduce PLUTO-4, our next generation ofpathology foundation models that extend the Pathology-Universal Transformer(PLUTO) to frontier scale. We share two complementary Vision Transformerarchitectures in the PLUTO-4 family: a compact and efficient PLUTO-4S modeloptimized for multi-scale deployment using a FlexiViT setup with 2D-RoPEembeddings, and a frontier-scale PLUTO-4G model trained with a single patchsize to maximize representation capacity and stability. Both models arepretrained using a self-supervised objective derived from DINOv2 on a largemulti-institutional corpus containing 551,164 WSIs from 137,144 patients acrossover 50 institutions, spanning over 60 disease types and over 100 stains.Comprehensive evaluation across public and internal benchmarks demonstratesthat PLUTO-4 achieves state-of-the-art performance on tasks requiring varyingspatial and biological context, including patch-level classification,segmentation, and slide-level diagnosis. The compact PLUTO-4S provideshigh-throughput and robust performance for practical deployment, while PLUTO-4Gestablishes new performance frontiers across multiple pathology benchmarks,including an 11% improvement in dermatopathology diagnosis. These diverseimprovements underscore PLUTO-4's potential to transform real-worldapplications as a backbone for translational research and diagnostic use cases.</description>
      <author>example@mail.com (Harshith Padigela, Shima Nofallah, Atchuth Naveen Chilaparasetti, Ryun Han, Andrew Walker, Judy Shen, Chintan Shah, Blake Martin, Aashish Sood, Elliot Miller, Ben Glass, Andy Beck, Harsha Pokkalla, Syed Ashar Javed)</author>
      <guid isPermaLink="false">2511.02826v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models</title>
      <link>http://arxiv.org/abs/2511.02802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TabTune是一个统一的开源库，通过单一接口标准化了表格基础模型的完整工作流程，解决了当前表格基础模型采用受限的问题。&lt;h4&gt;背景&lt;/h4&gt;表格基础模型在结构化数据学习中是一个不断发展的范式，但由于异构预处理管道、碎片化API、不一致的微调程序以及缺乏针对部署导向指标的标准化评估，其采用仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提出TabTune库，标准化表格基础模型的完整工作流程，提供一致访问和评估能力。&lt;h4&gt;方法&lt;/h4&gt;TabTune提供对七种最先进模型的一致访问，支持零样本推理、元学习、监督微调(SFT)和参数高效微调(PEFT)等多种适应策略，自动化模型感知预处理，管理架构异构性，并集成性能、校准和公平性评估模块。&lt;h4&gt;主要发现&lt;/h4&gt;TabTune框架支持一致的基准测试，表格基础模型可以通过多种适应策略进行有效应用。&lt;h4&gt;结论&lt;/h4&gt;TabTune库为表格基础模型的可扩展性和可重复性提供了解决方案，使研究人员和实践者能够一致地评估和部署这些模型。&lt;h4&gt;翻译&lt;/h4&gt;表格基础模型代表了结构化数据学习中不断增长的范式，将大规模预训练的好处扩展到表格领域。然而，由于异构预处理管道、碎片化的API、不一致的微调程序以及缺乏针对部署导向指标(如校准和公平性)的标准化评估，其采用仍然有限。我们提出了TabTune，一个通过单一接口标准化表格基础模型完整工作流程的统一库。TabTune提供对七种最先进模型的一致访问，支持多种适应策略，包括零样本推理、元学习、监督微调(SFT)和参数高效微调(PEFT)。该框架自动化模型感知的预处理，在内部管理架构异构性，并集成了用于性能、校准和公平性的评估模块。TabTune是为可扩展性和可重复性而设计的，它能够对表格基础模型的适应策略进行一致的基准测试。该库是开源的，可在https://github.com/Lexsi-Labs/TabTune获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular foundation models represent a growing paradigm in structured datalearning, extending the benefits of large-scale pretraining to tabular domains.However, their adoption remains limited due to heterogeneous preprocessingpipelines, fragmented APIs, inconsistent fine-tuning procedures, and theabsence of standardized evaluation for deployment-oriented metrics such ascalibration and fairness. We present TabTune, a unified library thatstandardizes the complete workflow for tabular foundation models through asingle interface. TabTune provides consistent access to seven state-of-the-artmodels supporting multiple adaptation strategies, including zero-shotinference, meta-learning, supervised fine-tuning (SFT), and parameter-efficientfine-tuning (PEFT). The framework automates model-aware preprocessing, managesarchitectural heterogeneity internally, and integrates evaluation modules forperformance, calibration, and fairness. Designed for extensibility andreproducibility, TabTune enables consistent benchmarking of adaptationstrategies of tabular foundation models. The library is open source andavailable at https://github.com/Lexsi-Labs/TabTune .</description>
      <author>example@mail.com (Aditya Tanna, Pratinav Seth, Mohamed Bouadi, Utsav Avaiya, Vinay Kumar Sankarapu)</author>
      <guid isPermaLink="false">2511.02802v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning</title>
      <link>http://arxiv.org/abs/2511.02794v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Multimodal Algorithmic Reasoning (MAR) Workshop,  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种诊断多模态大语言模型推理过程的新方法，通过'模态破坏'概念分析模态间的交互关系，并开发了一种轻量级评估框架来识别贡献模态和破坏模态。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)虽快速发展，但其推理过程仍不透明：不清楚哪个模态驱动预测，冲突如何解决，或何时一个信息流占主导地位。&lt;h4&gt;目的&lt;/h4&gt;分析模态间动态交互关系，特别是高置信度单模态错误如何覆盖其他证据并误导融合结果，为多模态推理提供诊断支架。&lt;h4&gt;方法&lt;/h4&gt;提出轻量级、模型无关的评估层，将每个模态视为独立代理产生候选标签和自我评估，通过简单融合机制聚合输出，区分贡献者(支持正确结果的模态)和破坏者(误导的模态)。&lt;h4&gt;主要发现&lt;/h4&gt;在多模态情感识别基准案例研究中应用该方法，揭示了基础模型的系统性可靠性特征，帮助区分失败源于数据集伪影还是模型内在限制。&lt;h4&gt;结论&lt;/h4&gt;该框架为多模态推理提供诊断支架，支持对融合动力学的原则性审计，为可能的模型改进干预措施提供指导。&lt;h4&gt;翻译&lt;/h4&gt;尽管多模态大语言模型(MLLMs)迅速发展，它们的推理过程仍然不透明：通常不清楚哪个模态驱动预测，冲突如何解决，或者何时一个信息流占主导地位。在本文中，我们引入了模态破坏，这是一种诊断性失效模式，其中高置信度的单模态错误会覆盖其他证据并误导融合结果。为了分析此类动态过程，我们提出了一种轻量级、模型无关的评估层，将每个模态视为一个代理，产生候选标签和用于审计的简短自我评估。一个简单的融合机制聚合这些输出，暴露贡献者(支持正确结果的模态)和破坏者(误导的模态)。在基础模型的多模态情感识别基准案例研究中应用我们的诊断层，揭示了系统性的可靠性特征，提供了关于失败可能源于数据集伪影还是模型限制的见解。更广泛地说，我们的框架为多模态推理提供了诊断支架，支持对融合动力学的原则性审计，并为可能的干预措施提供信息。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite rapid growth in multimodal large language models (MLLMs), theirreasoning traces remain opaque: it is often unclear which modality drives aprediction, how conflicts are resolved, or when one stream dominates. In thispaper, we introduce modality sabotage, a diagnostic failure mode in which ahigh-confidence unimodal error overrides other evidence and misleads the fusedresult. To analyze such dynamics, we propose a lightweight, model-agnosticevaluation layer that treats each modality as an agent, producing candidatelabels and a brief self-assessment used for auditing. A simple fusion mechanismaggregates these outputs, exposing contributors (modalities supporting correctoutcomes) and saboteurs (modalities that mislead). Applying our diagnosticlayer in a case study on multimodal emotion recognition benchmarks withfoundation models revealed systematic reliability profiles, providing insightinto whether failures may arise from dataset artifacts or model limitations.More broadly, our framework offers a diagnostic scaffold for multimodalreasoning, supporting principled auditing of fusion dynamics and informingpossible interventions.</description>
      <author>example@mail.com (Chenyu Zhang, Minsol Kim, Shohreh Ghorbani, Jingyao Wu, Rosalind Picard, Patricia Maes, Paul Pu Liang)</author>
      <guid isPermaLink="false">2511.02794v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models</title>
      <link>http://arxiv.org/abs/2511.02712v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  41 pages, 26 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的情感线索引导推理框架和视频情感基础模型(VidEmo)，解决了视频中情感理解和预测的挑战，通过建立Emo-CFG数据集和两阶段调优方法，在15个面部感知任务中取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;理解和预测视频中的情绪在近期研究中引起了广泛关注，得益于视频大型语言模型(VideoLLMs)的进步。尽管先进方法在视频情绪分析方面取得了进展，但情绪的本质特性（动态和线索依赖）仍带来重大挑战，使得难以以合理的理由理解复杂且不断变化的情绪状态。&lt;h4&gt;目的&lt;/h4&gt;提出一种情感线索引导推理框架，统一基本属性感知、表达分析和高级情感理解；设计专门用于情感推理和指令跟随的视频情感基础模型(VidEmo)；建立情感理解任务的基础数据基础设施。&lt;h4&gt;方法&lt;/h4&gt;提出情感线索引导推理框架，分阶段统一基本属性感知、表达分析和高级情感理解；开发视频情感基础模型(VidEmo)家族；采用两阶段调优过程：课程情感学习注入情感知识，情感树强化学习进行情感推理；建立包含210万多样化指令样本的情感细粒度数据集(Emo-CFG)，包括可解释的情感问答、细粒度字幕和相关理由。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在15个面部感知任务中取得了具有竞争力的性能，树立了新的里程碑。Emo-CFG数据集为情感理解任务提供了必要资源，包括可解释的情感问答、细粒度字幕和相关理由。&lt;h4&gt;结论&lt;/h4&gt;通过统一的框架和专门设计的模型，有效解决了视频中情感理解和预测的挑战。建立的数据基础设施和数据集为情感理解任务提供了重要资源，推动了该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;理解和预测视频中的情绪在近期研究中引起了广泛关注，这得益于视频大型语言模型(VideoLLMs)的进步。尽管先进方法在视频情绪分析方面取得了进展，但情绪的本质特性仍带来重大挑战。情绪具有动态和线索依赖的特性，使得难以以合理的理由理解复杂且不断变化的情绪状态。为应对这些挑战，我们提出了一种新颖的情感线索引导推理框架，以分阶段的方式统一基本属性感知、表达分析和高级情感理解。我们方法的核心是一系列专为情感推理和指令跟随而设计的视频情感基础模型(VidEmo)。这些模型经历两阶段调优过程：首先进行课程情感学习以注入情感知识，然后进行情感树强化学习以进行情感推理。此外，我们建立了基础数据基础设施，并引入了一个以情感为中心的细粒度数据集(Emo-CFG)，包含210万多样化的基于指令的样本。Emo-CFG包括可解释的情感问答、细粒度字幕和相关理由，为推进情感理解任务提供了必要资源。实验结果表明，我们的方法取得了具有竞争力的性能，在15个面部感知任务中树立了新的里程碑。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and predicting emotion from videos has gathered significantattention in recent studies, driven by advancements in video large languagemodels (VideoLLMs). While advanced methods have made progress in video emotionanalysis, the intrinsic nature of emotions poses significant challenges.Emotions are characterized by dynamic and cues-dependent properties, making itdifficult to understand complex and evolving emotional states with reasonablerationale. To tackle these challenges, we propose a novel affective cues-guidedreasoning framework that unifies fundamental attribute perception, expressionanalysis, and high-level emotional understanding in a stage-wise manner. At thecore of our approach is a family of video emotion foundation models (VidEmo),specifically designed for emotion reasoning and instruction-following. Thesemodels undergo a two-stage tuning process: first, curriculum emotion learningfor injecting emotion knowledge, followed by affective-tree reinforcementlearning for emotion reasoning. Moreover, we establish a foundational datainfrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG)consisting of 2.1M diverse instruction-based samples. Emo-CFG includesexplainable emotional question-answering, fine-grained captions, and associatedrationales, providing essential resources for advancing emotion understandingtasks. Experimental results demonstrate that our approach achieves competitiveperformance, setting a new milestone across 15 face perception tasks.</description>
      <author>example@mail.com (Zhicheng Zhang, Weicheng Wang, Yongjie Zhu, Wenyu Qin, Pengfei Wan, Di Zhang, Jufeng Yang)</author>
      <guid isPermaLink="false">2511.02712v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Apriel-H1: Towards Efficient Enterprise Reasoning Models</title>
      <link>http://arxiv.org/abs/2511.02651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合transformer注意力和状态空间模型(SSM)的混合架构，实现了比传统transformer模型更高的推理效率，同时保持了良好的推理性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型通过transformer架构和注意力机制实现了显著的推理能力，但transformers在注意力模块中具有二次时间和内存复杂度，且需要缓存键值状态，这严重限制了吞吐量和可扩展性。高推理吞吐量对智能体任务、长上下文推理等应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合LLM架构，结合transformer注意力和SSM序列混合器，实现高效的推理能力，同时保持较高的推理吞吐量。&lt;h4&gt;方法&lt;/h4&gt;提出Apriel-H1系列混合LLMs，通过增量蒸馏从预训练推理transformer(Apriel-Nemotron-15B-Thinker)获得，逐步用线性Mamba块替换不关键的注意力层。发布了多种后蒸馏变体，分析了不同SSM与MHA比例对推理性能的影响，并在vLLM环境中测试了30/50混合变体的性能。&lt;h4&gt;主要发现&lt;/h4&gt;蒸馏后的混合SSM-Transformer架构在生产环境中实现了超过2倍的更高推理吞吐量，同时推理性能仅最小程度下降。随着更多Mamba层替换MHA，推理性能会逐渐下降，但效率显著提升。&lt;h4&gt;结论&lt;/h4&gt;混合SSM-Transformer架构能够在不显著损害推理质量的情况下，比预训练的transformer等效模型提供实质性的效率提升，为大型语言模型提供了一种有前途的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)通过具有注意力机制的transformer架构实现了显著的推理能力。然而，transformers在注意力模块(MHA)中具有二次时间和内存复杂度，并且在推理过程中需要缓存键值状态，这严重限制了吞吐量和可扩展性。高推理吞吐量对于智能体任务、长上下文推理、高请求负载下的高效部署以及更高效的测试时计算缩放至关重要。状态空间模型(SSMs)如Mamba通过具有固定大小隐藏状态的循环计算提供了具有线性推理复杂度和常量内存占用的有前途的替代方案。在本技术报告中，我们引入了Apriel-H1系列混合LLMs，结合了transformer注意力和SSM序列混合器，在150亿模型规模下实现高效推理。这些模型通过从预训练推理transformer(Apriel-Nemotron-15B-Thinker)进行增量蒸馏获得，逐步用线性Mamba块替换不太关键的注意力层。我们发布了多种后蒸馏变体，具有不同的SSM与MHA比例，并分析了当更多Mamba层替换MHA时推理性能如何下降。此外，我们发布了30/50混合变体，在推理轨迹的监督数据集上进一步微调，在生产就绪的vLLM环境中实现了超过2倍的更高推理吞吐量，且推理性能最小程度下降。这表明，蒸馏后的混合SSM-Transformer架构可以在不显著损害推理质量的情况下，比预训练的transformer等效模型提供实质性的效率提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) achieve remarkable reasoning capabilitiesthrough transformer architectures with attention mechanisms. However,transformers suffer from quadratic time and memory complexity in the attentionmodule (MHA) and require caching key-value states during inference, whichseverely limits throughput and scalability. High inference throughput iscritical for agentic tasks, long-context reasoning, efficient deployment underhigh request loads, and more efficient test-time compute scaling.  State Space Models (SSMs) such as Mamba offer a promising alternative withlinear inference complexity and a constant memory footprint via recurrentcomputation with fixed-size hidden states. In this technical report weintroduce the Apriel-H1 family of hybrid LLMs that combine transformerattention and SSM sequence mixers for efficient reasoning at 15B model size.These models are obtained through incremental distillation from a pretrainedreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacingless critical attention layers with linear Mamba blocks.  We release multiple post-distillation variants of Apriel-H1-15B-Thinker withdifferent SSM-to-MHA ratios and analyse how reasoning performance degrades asmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variantof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,achieving over 2x higher inference throughput when deployed in theproduction-ready vLLM environment, with minimal degradation in reasoningperformance. This shows that distilled hybrid SSM-Transformer architectures candeliver substantial efficiency gains over the pretrained transformer equivalentwithout substantially compromising the reasoning quality.</description>
      <author>example@mail.com (Oleksiy Ostapenko, Luke Kumar, Raymond Li, Denis Kocetkov, Joel Lamy-Poirier, Shruthan Radhakrishna, Soham Parikh, Shambhavi Mishra, Sebastien Paquet, Srinivas Sunkara, Valérie Bécaert, Sathwik Tejaswi Madhusudhan, Torsten Scholak)</author>
      <guid isPermaLink="false">2511.02651v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Zero-Shot Multi-Animal Tracking in the Wild</title>
      <link>http://arxiv.org/abs/2511.02591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于视觉基础模型的多动物跟踪框架，无需重新训练即可应用于新数据集，在多种物种和环境中表现出强大且一致的性能。&lt;h4&gt;背景&lt;/h4&gt;多动物跟踪对于理解动物生态和行为至关重要，但由于栖息地、运动模式和物种外观的差异，这仍然是一个具有挑战性的任务。传统方法通常需要对每个应用场景进行大量的模型微调和启发式设计。&lt;h4&gt;目的&lt;/h4&gt;探索最近的视觉基础模型在零样本多动物跟踪中的潜力，开发一个无需重新训练或超参数调整的通用跟踪框架。&lt;h4&gt;方法&lt;/h4&gt;结合Grounding Dino目标检测器和Segment Anything Model 2 (SAM 2)跟踪器，以及精心设计的启发式方法，构建了一个可应用于新数据集的跟踪框架。&lt;h4&gt;主要发现&lt;/h4&gt;在ChimpAct、Bird Flock Tracking、AnimalTrack和GMOT-40子集上的评估表明，该框架在多样物种和环境中表现出强大且一致的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于视觉基础模型的跟踪框架能够在不进行重新训练的情况下适应新的数据集和应用场景，为多动物跟踪提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多动物跟踪对于理解动物生态和行为至关重要。然而，由于栖息地、运动模式和物种外观的差异，这仍然是一项具有挑战性的任务。传统方法通常需要对每个应用场景进行大量的模型微调和启发式设计。在这项工作中，我们探索了最近的视觉基础模型在零样本多动物跟踪中的潜力。通过结合Grounding Dino目标检测器和Segment Anything Model 2 (SAM 2)跟踪器以及精心设计的启发式方法，我们开发了一个跟踪框架，可以应用于新数据集而无需重新训练或超参数调整。在ChimpAct、Bird Flock Tracking、AnimalTrack和GMOT-40子集上的评估表明，该框架在多样物种和环境中表现出强大且一致的性能。代码可在https://github.com/ecker-lab/SAM2-Animal-Tracking获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-animal tracking is crucial for understanding animal ecology andbehavior. However, it remains a challenging task due to variations in habitat,motion patterns, and species appearance. Traditional approaches typicallyrequire extensive model fine-tuning and heuristic design for each applicationscenario. In this work, we explore the potential of recent vision foundationmodels for zero-shot multi-animal tracking. By combining a Grounding Dinoobject detector with the Segment Anything Model 2 (SAM 2) tracker and carefullydesigned heuristics, we develop a tracking framework that can be applied to newdatasets without any retraining or hyperparameter adaptation. Evaluations onChimpAct, Bird Flock Tracking, AnimalTrack, and a subset of GMOT-40 demonstratestrong and consistent performance across diverse species and environments. Thecode is available at https://github.com/ecker-lab/SAM2-Animal-Tracking.</description>
      <author>example@mail.com (Jan Frederik Meier, Timo Lüddecke)</author>
      <guid isPermaLink="false">2511.02591v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Resource-efficient Automatic Refinement of Segmentations via Weak Supervision from Light Feedback</title>
      <link>http://arxiv.org/abs/2511.02576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SCORE（基于区域评估的分割校正）框架，一种弱监督方法，用于改进医学图像分割结果。SCORE仅需轻量级反馈即可学习改进分割预测，无需密集的训练图像标注，在肱骨CT扫描上显著提升了初始分割性能。&lt;h4&gt;背景&lt;/h4&gt;手动分割医学图像解剖区域虽然准确但劳动强度大且易变，推动了自动化方法的发展。虽然基础模型可实现多种解剖结构和成像模态的自动分割，但可能不总能达到临床准确性标准。现有分割改进方法要么需要大量用户交互，要么需要完全监督的分割进行训练。&lt;h4&gt;目的&lt;/h4&gt;开发一种弱监督框架，能够仅使用轻量级反馈来改进分割预测，减少对密集训练标注的依赖。&lt;h4&gt;方法&lt;/h4&gt;SCORE框架引入了一种新的损失函数，利用区域质量分数和过分割/欠分割错误标签，而不是依赖密集的训练图像标注。该方法在肱骨CT扫描上进行了验证。&lt;h4&gt;主要发现&lt;/h4&gt;SCORE在肱骨CT扫描上显著改进了TotalSegmentator的初始预测，实现了与现有改进方法相当的性能，同时大大减少了监督需求和标注时间。&lt;h4&gt;结论&lt;/h4&gt;SCORE提供了一种有效的弱监督分割改进方法，能够在不牺牲性能的情况下，显著减少对大量标注数据的依赖。&lt;h4&gt;翻译&lt;/h4&gt;在医学图像分析中，解剖区域的划分是一项关键任务。手动分割虽然能实现高精度，但劳动强度大且容易产生变化，这促使了自动化方法的发展。最近，大量基础模型使得在多种解剖结构和成像模态上实现自动分割成为可能，但这些模型并不总能达到临床准确性标准。虽然分割改进策略可以提高性能，但当前方法依赖于大量用户交互或需要完全监督的分割进行训练。在此，我们提出了SCORE（基于区域评估的分割校正），一种弱监督框架，它仅使用训练过程中的轻量级反馈来学习改进掩膜预测。具体而言，SCORE不依赖密集的训练图像标注，而是引入了一种新的损失函数，利用区域质量分数和过分割/欠分割错误标签。我们在肱骨CT扫描上验证了SCORE，它显著改进了TotalSegmentator的初始预测，并实现了与现有改进方法相当的性能，同时大大减少了它们的监督需求和标注时间。我们的代码可在以下网址获取：https://gitlab.inria.fr/adelangl/SCORE。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何高效改进医学图像分割模型的预测结果问题。医学图像分割是临床诊断的关键任务，手动分割准确但耗时费力，而自动化基础模型虽提高了效率，但其预测结果往往达不到临床应用标准。现有的改进方法要么需要大量人工交互，要么需要完全标注的训练数据，限制了它们在临床环境中的广泛应用。因此，开发一种既能提高分割准确性又能减少标注负担的方法对推动医学图像分析的临床应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有分割改进方法的局限性：半自动方法需要大量人工交互，全自动方法则需要完全标注的地面真实数据。基于这一分析，作者提出使用'轻量反馈'（区域质量评分和错误类型标签）作为弱监督信号来训练改进网络。他们借鉴了3D U-Net网络架构、TotalSegmentator基础模型和Otsu阈值计算等方法，但创新性地设计了一个形态学启发的三部分损失函数，将区域级别的反馈转换为体素级别的校正指导。同时，作者还开发了专门的数据增强策略，提高模型对不同强度分割误差的鲁棒性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用'轻量反馈'（区域质量评分和错误类型标签）作为弱监督信号来训练分割改进网络，而不需要完全标注的地面真实数据。整体流程包括：1)输入3D医学图像、基础模型生成的初始分割和结构轮廓概率图；2)使用3D U-Net作为改进网络；3)通过三部分损失函数进行训练：稳定性损失保持正确区域内部不变，扩张损失针对欠分割区域，收缩损失针对过分割区域；4)训练好的网络接收输入图像、初始分割和边界概率图，输出改进后的分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)弱监督训练策略，仅使用轻量反馈而非完全标注数据；2)形态学启发的三部分损失函数，将区域级反馈转换为体素级校正；3)边界先验整合，指导网络在解剖学合理位置进行校正；4)专门的数据增强策略，提高模型鲁棒性。相比之前的工作，SCORE完全自动化且不需要专家在推理过程中交互，训练标注时间减少了约95%，同时在多个测试集上达到了与完全监督方法相当的性能，为医学图像分割的临床应用提供了更实用的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCORE提出了一种创新的弱监督框架，仅通过轻量反馈就能高效训练分割改进模型，显著减少了标注需求同时达到了与完全监督方法相当的性能，为医学图像分割的临床应用提供了实用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Delineating anatomical regions is a key task in medical image analysis.Manual segmentation achieves high accuracy but is labor-intensive and prone tovariability, thus prompting the development of automated approaches. Recently,a breadth of foundation models has enabled automated segmentations acrossdiverse anatomies and imaging modalities, but these may not always meet theclinical accuracy standards. While segmentation refinement strategies canimprove performance, current methods depend on heavy user interactions orrequire fully supervised segmentations for training. Here, we present SCORE(Segmentation COrrection from Regional Evaluations), a weakly supervisedframework that learns to refine mask predictions only using light feedbackduring training. Specifically, instead of relying on dense training imageannotations, SCORE introduces a novel loss that leverages region-wise qualityscores and over/under-segmentation error labels. We demonstrate SCORE onhumerus CT scans, where it considerably improves initial predictions fromTotalSegmentator, and achieves performance on par with existing refinementmethods, while greatly reducing their supervision requirements and annotationtime. Our code is available at: https://gitlab.inria.fr/adelangl/SCORE.</description>
      <author>example@mail.com (Alix de Langlais, Benjamin Billot, Théo Aguilar Vidal, Marc-Olivier Gauci, Hervé Delingette)</author>
      <guid isPermaLink="false">2511.02576v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Can Foundation Models Revolutionize Mobile AR Sparse Sensing?</title>
      <link>http://arxiv.org/abs/2511.02215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基础模型如何改变移动稀疏感知的格局，通过真实移动AR数据评估，发现基础模型在几何感知图像变换方面提供显著改进，证明了基于基础模型的稀疏感知的可扩展性及其在3D场景重建中的领先性能。&lt;h4&gt;背景&lt;/h4&gt;移动感知系统长期以来在感知质量和效率之间面临基本权衡，这种权衡源于计算能力、功率和其他限制。稀疏感知作为一种关键策略，旨在获取和处理仅一部分传感器数据，以在这些限制下维持性能。&lt;h4&gt;目的&lt;/h4&gt;探究基础模型是否能改变移动稀疏感知的格局。&lt;h4&gt;方法&lt;/h4&gt;使用真实的移动AR数据进行评估，研究基础模型在几何感知图像变换方面的表现，这是实现准确重用跨帧信息的关键技术。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在几何感知图像变换方面提供了显著改进；基于基础模型的稀疏感知具有可扩展性；在3D场景重建方面表现领先。&lt;h4&gt;结论&lt;/h4&gt;研究揭示了将基础模型集成到移动稀疏感知系统中的前景和开放挑战的关键方面。&lt;h4&gt;翻译&lt;/h4&gt;移动感知系统长期以来由于计算、功率和其他限制，在感知质量和效率之间面临基本权衡。稀疏感知作为一种旨在获取和处理仅一部分传感器数据的关键策略，一直是在这些限制下维持性能的重要方法。然而，现有的稀疏感知方法通常面临准确性降低的问题，因为缺失的空间和时间信息给许多感知系统带来了不确定性。在本研究中，我们探究基础模型是否能改变移动稀疏感知的格局。使用真实的移动AR数据进行评估，我们的研究表明基础模型在几何感知图像变换方面提供了显著改进，这是实现准确重用跨帧信息的关键技术。此外，我们的研究证明了基于基础模型的稀疏感知的可扩展性，并显示了其在3D场景重建中的领先性能。总体而言，我们的研究揭示了将基础模型集成到移动稀疏感知系统中的前景和开放挑战的关键方面。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决移动增强现实系统中的感知质量和效率之间的权衡问题。由于移动设备在计算能力、功耗等方面的限制，传统稀疏感知方法虽然减少了数据采集和处理量，但往往导致准确性下降。这个问题在现实中很重要，因为连续感知会消耗大量移动设备能源，影响设备续航和用户体验，而随着AR应用普及，如何在资源受限的移动设备上实现高效准确的感知变得至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到移动AR系统面临感知质量和效率的权衡问题，发现传统稀疏感知方法存在准确性下降的缺陷。他们注意到基础模型具有大规模预训练和强大泛化能力，可能从稀疏输入中提取有意义信息，从而解决传统稀疏感知问题。作者借鉴了现有稀疏感知技术、基础模型(如DINOv3和Metric3DV2)、几何感知图像变形技术和3D重建方法(如Poisson表面重建和ICP)，但将它们创新性地结合应用于移动AR稀疏感知场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用基础模型增强移动AR中的稀疏感知能力，通过基础模型从稀疏传感器数据中提取有意义信息，在减少感知频率的同时保持或提高感知质量。整体流程包括：1)使用基础模型从RGB图像估计深度图替代传统LiDAR；2)利用估计的深度图进行几何感知图像变形，实现跨帧信息重用；3)使用基础模型估计的深度图进行3D环境重建；4)分析不同稀疏感知策略下的信息重叠，优化控制策略。实验使用ScanNet++数据集，通过比较基础模型与LiDAR在图像变形和3D重建中的表现来验证方法效果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将基础模型应用于移动AR稀疏感知；2)利用基础模型估计深度图显著提高几何感知图像变形准确性；3)展示基础模型在低帧率条件下也能实现高质量3D重建；4)分析时空稀疏感知策略下的信息重叠，提出混合策略思路。相比之前工作，本文方法减少了对硬件传感器的依赖，通过更精确的深度估计提高了跨帧信息重用效果，并提出了考虑时间和空间两个维度的混合稀疏感知策略，而非传统基于时间间隔或单一维度的控制策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文展示了基础模型如何通过更精确的深度估计和跨帧信息重用，显著提升移动AR系统中的稀疏感知能力，实现了在减少感知频率的同时甚至提高3D重建质量的效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobile sensing systems have long faced a fundamental trade-off betweensensing quality and efficiency due to constraints in computation, power, andother limitations. Sparse sensing, which aims to acquire and process only asubset of sensor data, has been a key strategy for maintaining performanceunder such constraints. However, existing sparse sensing methods often sufferfrom reduced accuracy, as missing information across space and time introducesuncertainty into many sensing systems. In this work, we investigate whetherfoundation models can change the landscape of mobile sparse sensing. Usingreal-world mobile AR data, our evaluations demonstrate that foundation modelsoffer significant improvements in geometry-aware image warping, a centraltechnique for enabling accurate reuse of cross-frame information. Furthermore,our study demonstrates the scalability of foundation model-based sparse sensingand shows its leading performance in 3D scene reconstruction. Collectively, ourstudy reveals critical aspects of the promises and the open challenges ofintegrating foundation models into mobile sparse sensing systems.</description>
      <author>example@mail.com (Yiqin Zhao, Tian Guo)</author>
      <guid isPermaLink="false">2511.02215v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Automated Reward Design for Gran Turismo</title>
      <link>http://arxiv.org/abs/2511.02094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了如何利用基础模型在奖励函数空间中进行搜索，仅基于文本指令生成期望的强化学习代理，特别是在Gran Turismo 7赛车游戏中的应用。&lt;h4&gt;背景&lt;/h4&gt;在强化学习代理设计中，设计师通过定义奖励函数传达期望行为，但将期望行为映射到奖励函数是一个困难的过程，特别是在自动驾驶赛车等复杂环境中。&lt;h4&gt;目的&lt;/h4&gt;展示如何利用当前基础模型有效地搜索奖励函数空间，仅基于文本指令生成期望的强化学习代理。&lt;h4&gt;方法&lt;/h4&gt;结合基于大型语言模型(LLM)的奖励生成、基于视觉语言模型(VLM)的偏好评估和人类反馈的系统。&lt;h4&gt;主要发现&lt;/h4&gt;该系统能够生成与GT Sophy(冠军级强化学习赛车代理)具有竞争力的赛车代理，并能生成新颖的行为。&lt;h4&gt;结论&lt;/h4&gt;为实际应用中的自动化奖励设计铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;在设计强化学习(RL)代理时，设计师通过定义奖励函数来传达期望的代理行为——作为对代理行动的奖励或惩罚而给予的数值反馈。然而，将期望行为映射到奖励函数可能是一个困难的过程，特别是在自动驾驶赛车等复杂环境中。在本文中，我们展示了当前基础模型如何能够有效地搜索奖励函数空间，仅基于文本指令为Gran Turismo 7赛车游戏生成期望的强化学习代理。通过结合基于大型语言模型的奖励生成、基于视觉语言模型的偏好评估和人类反馈，我们展示了如何使用我们的系统生成与GT Sophy(冠军级强化学习赛车代理)具有竞争力的赛车代理，以及生成新颖的行为，为实际应用中的自动化奖励设计铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When designing reinforcement learning (RL) agents, a designer communicatesthe desired agent behavior through the definition of reward functions -numerical feedback given to the agent as reward or punishment for its actions.However, mapping desired behaviors to reward functions can be a difficultprocess, especially in complex environments such as autonomous racing. In thispaper, we demonstrate how current foundation models can effectively search overa space of reward functions to produce desirable RL agents for the Gran Turismo7 racing game, given only text-based instructions. Through a combination ofLLM-based reward generation, VLM preference-based evaluation, and humanfeedback we demonstrate how our system can be used to produce racing agentscompetitive with GT Sophy, a champion-level RL racing agent, as well asgenerate novel behaviors, paving the way for practical automated reward designin real world applications.</description>
      <author>example@mail.com (Michel Ma, Takuma Seno, Kaushik Subramanian, Peter R. Wurman, Peter Stone, Craig Sherstan)</author>
      <guid isPermaLink="false">2511.02094v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis</title>
      <link>http://arxiv.org/abs/2511.02046v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种自动化方法，用于基于场景文本创建大规模视觉问答数据集，整合多种模型和技术避免了繁琐的人工标注过程。&lt;h4&gt;背景&lt;/h4&gt;创建大规模视觉问答(text-VQA)数据库需要熟练的人工标注，这既繁琐又具有挑战性。随着处理视觉和语言模态的基础模型的出现以及OCR系统的成熟，为解决这个问题提供了新的可能。&lt;h4&gt;目的&lt;/h4&gt;建立一个端到端的管道，能够根据给定图像中的场景文本自动合成问答(QA)对，实现text-VQA数据集的自动化合成。&lt;h4&gt;方法&lt;/h4&gt;提出一个自动化合成text-VQA数据集的管道，整合多种模型和算法，包括OCR检测和识别(文本定位)、感兴趣区域(ROI)检测、标题生成和问题生成，将这些组件整合成一个连贯的管道以自动合成和验证QA对。&lt;h4&gt;主要发现&lt;/h4&gt;该管道能够生成可靠的QA对，并能根据场景文本数据的可用性进行扩展，成功创建了包含约72K个QA对、基于约44K张图像的大规模text-VQA数据集。&lt;h4&gt;结论&lt;/h4&gt;据我们所知，这是第一个提出的管道，可以自动合成和验证大规模text-VQA数据集，为视觉问答领域提供了新的数据集构建方法。&lt;h4&gt;翻译&lt;/h4&gt;为视觉问答任务创建大规模数据库，特别是针对场景中的文本数据(text-VQA)，需要熟练的人工标注，这既繁琐又具有挑战性。随着处理视觉和语言模态的基础模型的出现以及OCR系统的成熟，现在有必要建立一个端到端的管道，能够根据给定图像中的场景文本合成问答(QA)对。我们提出了一个用于text-VQA数据集自动合成的管道，可以生成可靠的QA对，并能根据场景文本数据的可用性进行扩展。我们提出的方法利用了多种模型和算法的能力，包括OCR检测和识别(文本定位)、感兴趣区域(ROI)检测、标题生成和问题生成。这些组件被整合成一个连贯的管道，以自动合成和验证QA对。据我们所知，这是第一个提出的管道，可以自动合成和验证包含约72K个QA对、基于约44K张图像的大规模text-VQA数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Creation of large-scale databases for Visual Question Answering taskspertaining to the text data in a scene (text-VQA) involves skilful humanannotation, which is tedious and challenging. With the advent of foundationmodels that handle vision and language modalities, and with the maturity of OCRsystems, it is the need of the hour to establish an end-to-end pipeline thatcan synthesize Question-Answer (QA) pairs based on scene-text from a givenimage. We propose a pipeline for automated synthesis for text-VQA dataset thatcan produce faithful QA pairs, and which scales up with the availability ofscene text data. Our proposed method harnesses the capabilities of multiplemodels and algorithms involving OCR detection and recognition (text spotting),region of interest (ROI) detection, caption generation, and questiongeneration. These components are streamlined into a cohesive pipeline toautomate the synthesis and validation of QA pairs. To the best of ourknowledge, this is the first pipeline proposed to automatically synthesize andvalidate a large-scale text-VQA dataset comprising around 72K QA pairs based onaround 44K images.</description>
      <author>example@mail.com (Soham Joshi, Shwet Kamal Mishra, Viswanath Gopalakrishnan)</author>
      <guid isPermaLink="false">2511.02046v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Assessing the value of Geo-Foundational Models for Flood Inundation Mapping: Benchmarking models for Sentinel-1, Sentinel-2, and Planetscope for end-users</title>
      <link>http://arxiv.org/abs/2511.01990v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究对多种地理基础模型(GFMs)与传统模型在洪水淹没制图方面进行了系统比较，发现GFMs特别是Clay模型在性能和效率方面均优于传统模型，即使在数据有限的情况下也能保持良好表现。&lt;h4&gt;背景&lt;/h4&gt;地理基础模型(GFMs)能够快速可靠地从卫星影像中提取时空信息，通过利用位置和时间嵌入改进洪水淹没制图。然而，尚不清楚GFMs是否优于传统模型如U-Net，且缺乏对不同传感器和数据可用性场景的系统比较。&lt;h4&gt;目的&lt;/h4&gt;评估三种GFMs(Prithvi 2.0、Clay V1.5、DOFA和UViT)与TransNorm、U-Net和Attention U-Net在PlanetScope、Sentinel-1和Sentinel-2上的表现，为用户提供模型选择指导。&lt;h4&gt;方法&lt;/h4&gt;使用多种传感器数据进行模型比较，进行区域外交叉验证(跨五个区域)，进行少样本实验评估少量训练数据下的表现，并比较不同模型的计算时间和模型大小。&lt;h4&gt;主要发现&lt;/h4&gt;1) 所有GFMs表现相当，不同传感器上最佳和最差模型间仅2-5%差异；2) Clay在PlanetScope和Sentinel-2上表现最佳，Prithvi在Sentinel-1上领先；3) 在交叉验证中，Clay在所有传感器上表现略优于其他模型；4) Clay在保留细节方面具有优势；5) 仅用五张训练图像，Clay表现优于其他模型；6) Clay计算效率更高，模型较小(2600万参数)，比Prithvi快约3倍，比DOFA快2倍。&lt;h4&gt;结论&lt;/h4&gt;与先前发现相反，研究结果表明GFMs在洪水制图准确性方面比传统U-Net有小到中等程度的提升，同时计算成本和标注工作量更低。&lt;h4&gt;翻译&lt;/h4&gt;地理基础模型(GFMs)能够快速可靠地从卫星影像中提取时空信息，通过利用位置和时间嵌入改进洪水淹没制图。尽管它们有潜力，但尚不清楚GFMs是否优于传统模型如U-Net。对不同传感器和数据可用性场景的系统比较仍然缺乏，这是指导用户选择模型的重要步骤。为此，我们评估了三种GFMs(Prithvi 2.0、Clay V1.5、DOFA和Prithvi的变体UViT)与TransNorm、U-Net和Attention U-Net在PlanetScope、Sentinel-1和Sentinel-2上的表现。我们观察到所有GFMs都具有竞争力，不同传感器上最佳和最差模型间仅2-5%的差异。Clay在PlanetScope(0.79 mIoU)和Sentinel-2(0.70)上表现最佳，而Prithvi在Sentinel-1上领先(0.57)。在跨五个区域的外交叉验证中，Clay在所有传感器上表现略好(PlanetScope: 0.72(0.04), Sentinel-2: 0.66(0.07), Sentinel-1: 0.51(0.08))，优于Prithvi和DOFA。在所有19个站点的外交叉验证中，Clay比U-Net提高了4%的准确率。视觉检查显示Clay在保留细节方面具有优势。少样本实验显示，仅用五张训练图像，Clay在PlanetScope上达到0.64 mIoU，优于Prithvi(0.24)和DOFA(0.35)。在计算时间方面，由于模型较小(2600万参数)，Clay是更好的选择，比Prithvi(6.5亿参数)快约3倍，比DOFA(4.1亿参数)快2倍。与先前发现相反，我们的研究结果表明，与传统U-Net相比，GFMs在洪水制图准确性方面有小到中等程度的提升，同时计算成本和标注工作量更低。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geo-Foundational Models (GFMs) enable fast and reliable extraction ofspatiotemporal information from satellite imagery, improving flood inundationmapping by leveraging location and time embeddings. Despite their potential, itremains unclear whether GFMs outperform traditional models like U-Net. Asystematic comparison across sensors and data availability scenarios is stilllacking, which is an essential step to guide end-users in model selection. Toaddress this, we evaluate three GFMs, Prithvi 2.0, Clay V1.5, DOFA, and UViT (aPrithvi variant), against TransNorm, U-Net, and Attention U-Net usingPlanetScope, Sentinel-1, and Sentinel-2. We observe competitive performanceamong all GFMs, with only 2-5% variation between the best and worst modelsacross sensors. Clay outperforms others on PlanetScope (0.79 mIoU) andSentinel-2 (0.70), while Prithvi leads on Sentinel-1 (0.57). Inleave-one-region-out cross-validation across five regions, Clay shows slightlybetter performance across all sensors (mIoU: 0.72(0.04), 0.66(0.07),0.51(0.08)) compared to Prithvi (0.70(0.05), 0.64(0.09), 0.49(0.13)) and DOFA(0.67(0.07), 0.64(0.04), 0.49(0.09)) for PlanetScope, Sentinel-2, andSentinel-1, respectively. Across all 19 sites, leave-one-region-outcross-validation reveals a 4% improvement by Clay compared to U-Net. Visualinspection highlights Clay's superior ability to retain fine details. Few-shotexperiments show Clay achieves 0.64 mIoU on PlanetScope with just five trainingimages, outperforming Prithvi (0.24) and DOFA (0.35). In terms of computationaltime, Clay is a better choice due to its smaller model size (26M parameters),making it ~3x faster than Prithvi (650M) and 2x faster than DOFA (410M).Contrary to previous findings, our results suggest GFMs offer small to moderateimprovements in flood mapping accuracy at lower computational cost and labelingeffort compared to traditional U-Net.</description>
      <author>example@mail.com (Saurabh Kaushik, Lalit Maurya, Elizabeth Tellman, ZhiJie Zhang)</author>
      <guid isPermaLink="false">2511.01990v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Towards Robust Mathematical Reasoning</title>
      <link>http://arxiv.org/abs/2511.01846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025 (main conference),  https://aclanthology.org/2025.emnlp-main.1794/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了IMO-Bench，一个针对国际数学奥林匹克竞赛(IMO)级别的高级推理评估基准套件，包含IMO-AnswerBench和IMO-Proof Bench两部分，用于评估基础模型的数学推理能力。该基准在Gemini Deep Think模型上取得了显著成果，并在IMO 2025上获得金牌表现。&lt;h4&gt;背景&lt;/h4&gt;现有基础模型的数学推理能力评估存在局限性，要么过于简单，要么只关注获取简短正确答案，缺乏对高级数学推理能力的有效评估。&lt;h4&gt;目的&lt;/h4&gt;开发一个针对国际数学奥林匹克竞赛(IMO)级别的高级推理评估基准，以更准确地评估基础模型的数学推理能力。&lt;h4&gt;方法&lt;/h4&gt;构建了IMO-Bench评估套件，包括IMO-AnswerBench(测试400个多样化奥林匹克问题)和IMO-Proof Bench(评估证明写作能力，包含基础和高级IMO级别问题及详细评分指南)。此外，还构建了IMO-GradingBench，包含1000个人类评分的证明。&lt;h4&gt;主要发现&lt;/h4&gt;Gemini Deep Think模型在IMO-AnswerBench上达到80.0%的准确率，在高级IMO-Proof Bench上达到65.7%，分别领先其他最佳非Gemini模型6.9%和42.4%。基于Gemini推理能力的自动评分器与人工评估有很好的相关性。&lt;h4&gt;结论&lt;/h4&gt;IMO-Bench为评估高级数学推理能力提供了有效工具，IMO-GradingBench促进了长答案自动评估的发展，这些工具将帮助社区推进强大的数学推理能力。&lt;h4&gt;翻译&lt;/h4&gt;找到正确的北极星指标对于推进基础模型的数学推理能力至关重要，特别是考虑到现有评估要么太简单，要么只关注获取正确的简短答案。为解决这些问题，我们提出了IMO-Bench，一个由顶级专家评审的高级推理基准套件，专门针对国际数学奥林匹克竞赛(IMO)的水平，这是年轻数学家最负盛名的平台。IMO-AnswerBench首先测试模型在400个多样化奥林匹克问题上的表现，这些问题有可验证的简短答案。IMO-Proof Bench是下一级别的证明写作能力评估，包含基础和高级IMO级别问题以及详细的评分指南，以促进自动评分。这些基准在我们的Gemini Deep Think在IMO 2025上取得历史性金牌表现(Luong和Lockhart, 2025)中发挥了关键作用。我们的模型在IMO-AnswerBench上达到80.0%，在高级IMO-Proof Bench上达到65.7%，分别大幅领先最佳非Gemini模型6.9%和42.4%。我们还表明，使用Gemini推理能力构建的自动评分器与人工评估有很好的相关性，并构建了IMO-GradingBench，包含1000个证明的人类评分，以促进长答案自动评估的进一步发展。我们希望IMO-Bench将帮助社区推进强大的数学推理能力，并在https://imobench.github.io/上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Finding the right north-star metrics is highly critical for advancing themathematical reasoning capabilities of foundation models, especially given thatexisting evaluations are either too easy or only focus on getting correct shortanswers. To address these issues, we present IMO-Bench, a suite of advancedreasoning benchmarks, vetted by a panel of top specialists and thatspecifically targets the level of the International Mathematical Olympiad(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBenchfirst tests models on 400 diverse Olympiad problems with verifiable shortanswers. IMO-Proof Bench is the next-level evaluation for proof-writingcapabilities, which includes both basic and advanced IMO level problems as wellas detailed grading guidelines to facilitate automatic grading. Thesebenchmarks played a crucial role in our historic achievement of the gold-levelperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Ourmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-ProofBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%respectively. We also showed that autograders built with Gemini reasoningcorrelate well with human evaluations and construct IMO-GradingBench, with 1000human gradings on proofs, to enable further progress in automatic evaluation oflong-form answers. We hope that IMO-Bench will help the community towardsadvancing robust mathematical reasoning and release it athttps://imobench.github.io/.</description>
      <author>example@mail.com (Thang Luong, Dawsen Hwang, Hoang H. Nguyen, Golnaz Ghiasi, Yuri Chervonyi, Insuk Seo, Junsu Kim, Garrett Bingham, Jonathan Lee, Swaroop Mishra, Alex Zhai, Clara Huiyi Hu, Henryk Michalewski, Jimin Kim, Jeonghyun Ahn, Junhwi Bae, Xingyou Song, Trieu H. Trinh, Quoc V. Le, Junehyuk Jung)</author>
      <guid isPermaLink="false">2511.01846v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment</title>
      <link>http://arxiv.org/abs/2511.01775v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了SurgVeo基准测试和手术合理性金字塔(SPP)框架，用于评估手术视频生成模型。研究发现先进模型在视觉层面表现良好，但在理解手术操作、环境反馈和手术意图等深层知识方面存在明显不足。&lt;h4&gt;背景&lt;/h4&gt;基础模型在视频生成领域展现出作为物理世界模拟模型的潜力，但在手术等高风险领域需要深度、专业的因果知识，而非通用物理规则，这一领域仍存在研究空白。&lt;h4&gt;目的&lt;/h4&gt;系统解决手术视频生成模型评估的挑战，提出首个专家策划的手术视频生成模型评估基准SurgVeo，以及专门用于评估模型输出的四层框架SPP。&lt;h4&gt;方法&lt;/h4&gt;基于SurgVeo基准，让先进Veo-3模型对腹腔镜和神经外科手术片段进行零样本预测，由四位认证外科医生根据SPP框架评估生成视频。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了明显的'合理性差距'：Veo-3在视觉感知合理性方面表现出色，但在器械操作合理性、环境反馈合理性和手术意图合理性等更高层次评估中严重失败。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了手术AI中视觉模仿与因果理解之间鸿沟的首次定量证据，为开发能够处理专业医疗领域复杂性的未来模型奠定了基础和路线图。&lt;h4&gt;翻译&lt;/h4&gt;视频生成领域的基础模型作为模拟物理世界的潜在世界模型展现出卓越能力。然而，在手术等高风险领域的应用仍是一个关键未探索的空白，这些领域需要深度、专业的因果知识而非通用物理规则。为系统解决这一挑战，我们提出了SurgVeo，这是首个用于手术视频生成模型评估的专家策划基准，以及手术合理性金字塔(SPP)，这是一个新颖的四层框架，专门用于从基本外观到复杂手术策略评估模型输出。基于SurgVeo基准，我们让先进的Veo-3模型对来自腹腔镜和神经外科手术片段进行零样本预测任务。由四位认证外科医生组成的评估小组根据SPP评估生成的视频。我们的研究结果揭示了一个明显的'合理性差距'：虽然Veo-3在视觉感知合理性方面取得了卓越成就，但在SPP的更高层次上却严重失败，包括器械操作合理性、环境反馈合理性和手术意图合理性。这项工作提供了手术AI中视觉上令人信服的模仿与因果理解之间鸿沟的首次定量证据。我们从SurgVeo和SPP中获得的研究结果为开发能够处理专业、现实医疗领域复杂性的未来模型奠定了重要基础和路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models in video generation are demonstrating remarkablecapabilities as potential world models for simulating the physical world.However, their application in high-stakes domains like surgery, which demanddeep, specialized causal knowledge rather than general physical rules, remainsa critical unexplored gap. To systematically address this challenge, we presentSurgVeo, the first expert-curated benchmark for video generation modelevaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,four-tiered framework tailored to assess model outputs from basic appearance tocomplex surgical strategy. On the basis of the SurgVeo benchmark, we task theadvanced Veo-3 model with a zero-shot prediction task on surgical clips fromlaparoscopic and neurosurgical procedures. A panel of four board-certifiedsurgeons evaluates the generated videos according to the SPP. Our resultsreveal a distinct "plausibility gap": while Veo-3 achieves exceptional VisualPerceptual Plausibility, it fails critically at higher levels of the SPP,including Instrument Operation Plausibility, Environment Feedback Plausibility,and Surgical Intent Plausibility. This work provides the first quantitativeevidence of the chasm between visually convincing mimicry and causalunderstanding in surgical AI. Our findings from SurgVeo and the SPP establish acrucial foundation and roadmap for developing future models capable ofnavigating the complexities of specialized, real-world healthcare domains.</description>
      <author>example@mail.com (Zhen Chen, Qing Xu, Jinlin Wu, Biao Yang, Yuhao Zhai, Geng Guo, Jing Zhang, Yinlu Ding, Nassir Navab, Jiebo Luo)</author>
      <guid isPermaLink="false">2511.01775v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>AnyPPG: An ECG-Guided PPG Foundation Model Trained on Over 100,000 Hours of Recordings for Holistic Health Profiling</title>
      <link>http://arxiv.org/abs/2511.01747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了AnyPPG，一个在大型多源同步PPG-ECG数据上预训练的PPG基础模型，通过与ECG表示对齐学习有生理意义的特征，在多项生理分析和多器官疾病诊断任务中取得了最先进的性能，展示了PPG作为全面健康评估模式的潜力。&lt;h4&gt;背景&lt;/h4&gt;PPG是一种非侵入式且易于获取的健康监测方式，可在临床环境外使用。然而，现有研究受限于标记数据的规模和多样性，这限制了模型的准确性、泛化能力以及更广泛应用的探索。&lt;h4&gt;目的&lt;/h4&gt;研究通过整合基础模型技术，探索PPG进行全面健康档案构建的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出了AnyPPG，一个在大型、多源同步PPG-ECG数据上预训练的PPG基础模型。通过在共享空间中对齐PPG和ECG表示，AnyPPG从未标记的信号中学习有生理意义的特征。在多样化的下游任务中评估了其能力，包括传统的生理分析和全面的多器官疾病诊断。&lt;h4&gt;主要发现&lt;/h4&gt;在跨越六个独立数据集的十一个生理分析任务中，AnyPPG取得了最先进的性能，与次优模型相比，回归任务平均提高了12.8%，分类任务平均提高了9.1%。在多器官疾病诊断中，AnyPPG展示了广泛的跨系统诊断潜力，在1,014个ICD-10三位数疾病类别中，13个达到0.8以上的AUC，137个超过0.7。除了在心血管疾病方面表现强劲外，AnyPPG在帕金森病和慢性肾病等非心血管状况中也显示出实质性的诊断价值。&lt;h4&gt;结论&lt;/h4&gt;AnyPPG证明，通过与ECG进行生理对齐训练的PPG基础模型可以产生准确且稳健的信号表示。基于此能力，它强调了PPG作为评估全身和多器官健康模式的潜力。&lt;h4&gt;翻译&lt;/h4&gt;背景：光电容积脉搏波描记术提供了一种非侵入式且易于获取的健康监测方式，可用于临床环境之外的健康监测。然而，现有研究受限于标记数据的规模和多样性，这限制了模型的准确性、泛化能力以及更广泛应用的探索。本研究通过整合基础模型技术，调查了PPG进行全面健康档案构建的潜力。方法：我们提出了AnyPPG，一个在大型、多源同步PPG-ECG数据上预训练的PPG基础模型。通过在共享空间中对齐PPG和ECG表示，AnyPPG从未标记的信号中学习有生理意义的特征。其能力在多样化的下游任务中得到了进一步评估，涵盖传统的生理分析和全面的多器官疾病诊断。结果：在跨越六个独立数据集的十一个生理分析任务中，AnyPPG取得了最先进的性能，与次优模型相比，回归任务平均提高了12.8%，分类任务平均提高了9.1%。在多器官疾病诊断中，AnyPPG展示了广泛的跨系统诊断潜力。在1,014个ICD-10三位数疾病类别中，13个达到0.8以上的AUC，137个超过0.7。除了在心力衰竭、瓣膜疾病和高血压等心血管疾病方面表现强劲外，AnyPPG在帕金森病和慢性肾病等非心血管状况中也显示出实质性的诊断价值。结论：AnyPPG证明，通过与ECG进行生理对齐训练的PPG基础模型可以产生准确且稳健的信号表示。基于此能力，它强调了PPG作为评估全身和多器官健康模式的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Background: Photoplethysmography (PPG) offers a noninvasive and accessiblemodality for health monitoring beyond clinical settings. However, existingstudies are limited by the scale and diversity of labeled data, constrainingmodel accuracy, generalizability, and the exploration of broader applications.This study investigates the potential of PPG for holistic health profilingthrough the integration of foundation model techniques.  Methods: We present AnyPPG, a PPG foundation model pretrained on large-scale,multi-source synchronized PPG-ECG data. By aligning PPG and ECG representationswithin a shared space, AnyPPG learns physiologically meaningful features fromunlabeled signals. Its capability was further evaluated across a diverse set ofdownstream tasks, encompassing both conventional physiological analysis andcomprehensive multi-organ disease diagnosis.  Results: Across eleven physiological analysis tasks spanning six independentdatasets, AnyPPG achieved state-of-the-art performance, with averageimprovements of 12.8% in regression and 9.1% in classification tasks over thenext-best model. In multi-organ disease diagnosis, AnyPPG demonstrated broadcross-system diagnostic potential. Among 1,014 ICD-10 three-digit diseasecategories, 13 achieved an AUC above 0.8 and 137 exceeded 0.7. Beyond strongperformance in cardiovascular diseases such as heart failure, valvulardisorders, and hypertension, AnyPPG also showed substantial diagnostic valuefor non-cardiovascular conditions, exemplified by Parkinson's disease (AUC =0.78) and chronic kidney disease (AUC = 0.74).  Conclusions: AnyPPG demonstrates that a PPG foundation model trained throughphysiological alignment with ECG can produce accurate and robust signalrepresentations. Building on this capability, it underscores the potential ofPPG as a modality for comprehensive assessment of systemic and multi-organhealth.</description>
      <author>example@mail.com (Guangkun Nie, Gongzheng Tang, Yujie Xiao, Jun Li, Shun Huang, Deyun Zhang, Qinghao Zhao, Shenda Hong)</author>
      <guid isPermaLink="false">2511.01747v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>DINO-MX: A Modular &amp; Flexible Framework for Self-Supervised Learning</title>
      <link>http://arxiv.org/abs/2511.01610v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DINO-MX是一个模块化、可扩展的视觉基础模型训练框架，结合了DINO、DINOv2和DINOv3的核心原则，支持多种Transformer架构和Hugging Face生态系统，通过多种训练策略和分布式训练方法实现了高效的自监督视觉模型训练。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉基础模型训练流程通常不够灵活、局限于特定领域或计算成本高，限制了它们在不同领域和资源环境中的可用性。&lt;h4&gt;目的&lt;/h4&gt;开发一个模块化、可扩展的训练框架，结合DINO系列模型的核心原则，支持各种基于Transformer的架构，并与Hugging Face生态系统完全兼容，提高视觉基础模型的可用性和适应性。&lt;h4&gt;方法&lt;/h4&gt;DINO-MX是一个统一配置驱动的训练框架，支持低秩自适应(LoRA)、层冻结、知识蒸馏等多种训练策略，通过分布式数据并行(DDP)和全分片数据并行(FSDP)支持分布式训练，适用于自然和专门的数据类型，包括单通道和多通道图像。&lt;h4&gt;主要发现&lt;/h4&gt;在不同数据集上的实验结果表明，DINO-MX在显著降低计算成本的同时实现了具有竞争力的性能。此外，它提供了可解释性工具和标签引导的数据增强方法，可以改进基于注意力的定位，无需额外的检测或分割头。&lt;h4&gt;结论&lt;/h4&gt;DINO-MX为开发、适应和基准测试自监督视觉模型提供了一个可重现且可扩展的基础，适用于各种研究和实际应用。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型(VFMs)通过自监督方法推动了表征学习的进步。然而，现有的训练流程通常不够灵活、局限于特定领域或计算成本高，这限制了它们在不同领域和资源环境中的可用性。DINO-MX是一个模块化且可扩展的训练框架，在统一的配置驱动系统中结合了DINO、DINOv2和DINOv3的核心原则。它支持各种基于Transformer的架构，并与Hugging Face生态系统完全兼容。该框架包括多种训练策略，如低秩自适应(LoRA)、层冻结和知识蒸馏，同时通过分布式数据并行(DDP)和全分片数据并行(FSDP)支持分布式训练。DINO-MX设计用于处理自然和专门的数据类型，包括单通道和多通道图像。在不同数据集上的实验结果表明，DINO-MX在显著降低计算成本的同时实现了具有竞争力的性能。此外，它提供了解释性工具和一种标签引导的数据增强方法，可以改进基于注意力的定位，而无需额外的检测或分割头。DINO-MX为开发、适应和基准测试自监督视觉模型提供了一个可重现且可扩展的基础，适用于各种研究和实际应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Foundation Models (VFMs) have advanced representation learning throughself-supervised methods. However, existing training pipelines are ofteninflexible, domain-specific, or computationally expensive, which limits theirusability across different domains and resource settings. DINO-MX is a modularand extensible training framework that combines the core principles of DINO,DINOv2 and DINOv3 within a unified configuration-driven system. It supports avariety of transformer-based architectures and is fully compatible with theHugging Face ecosystem. The framework includes multiple training strategiessuch as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,along with support for distributed training through both Distributed DataParallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed towork with both natural and specialized data types, including single- andmulti-channel images. Experimental results on diverse datasets show thatDINO-MX achieves competitive performance while significantly reducingcomputational costs. Additionally, it offers interpretability tools and alabel-guided data augmentation method that improves attention-basedlocalization without the need for extra detection or segmentation heads.DINO-MX provides a reproducible and scalable foundation for developing,adapting, and benchmarking self-supervised vision models across a range ofresearch and real-world applications.</description>
      <author>example@mail.com (Mahmut Selman Gokmen, Cody Bumgardner)</author>
      <guid isPermaLink="false">2511.01610v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Analyzing Sustainability Messaging in Large-Scale Corporate Social Media</title>
      <link>http://arxiv.org/abs/2511.01550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个多模态分析流程，利用视觉和语言领域的大型基础模型来分析企业社交媒体内容，特别是与可持续发展相关的传播。&lt;h4&gt;背景&lt;/h4&gt;企业在X平台(前Twitter)上的信息在不断变化、多模态且往往模糊不清，这给分析带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;分析企业社交媒体内容，揭示不同行业在可持续发展目标参与度上的差异、时间趋势以及企业信息、环境、社会、治理风险和消费者参与之间的关联。&lt;h4&gt;方法&lt;/h4&gt;使用大型语言模型集合标注企业推文与可持续发展目标的一致性，并利用视觉语言模型在语义簇框架内分析视觉可持续性传播模式。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了不同行业在可持续发展目标参与度上的差异、时间趋势以及企业信息、环境、社会、治理风险和消费者参与之间的关联。&lt;h4&gt;结论&lt;/h4&gt;自动标签生成和语义视觉聚类方法可广泛适用于其他领域，为大规模社交媒体分析提供了灵活框架。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们介绍了一个多模态分析流程，它利用视觉和语言领域的大型基础模型来分析企业社交媒体内容，重点关注与可持续发展相关的传播。针对企业在X平台(前Twitter)等平台上不断变化、多模态且往往模糊不清的企业信息传播所面临的挑战，我们采用大型语言模型集合来标注大量企业推文，使其与17个可持续发展目标的主题保持一致。这种方法避免了昂贵的、特定任务的标注需求，并探索了此类模型作为社交媒体数据的临时标注者的潜力，能够以可扩展的方式高效捕捉对可持续性主题的显性和隐性引用。作为文本分析的补充，我们在一个使用语义簇的视觉理解框架内利用视觉语言模型，揭示视觉可持续性传播中的模式。这种综合方法揭示了不同行业在可持续发展目标参与度上的差异、时间趋势以及企业信息、环境、社会、治理风险和消费者参与之间的关联。我们的方法——自动标签生成和语义视觉聚类——可广泛适用于其他领域，并为大规模社交媒体分析提供了一个灵活的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we introduce a multimodal analysis pipeline that leverageslarge foundation models in vision and language to analyze corporate socialmedia content, with a focus on sustainability-related communication. Addressingthe challenges of evolving, multimodal, and often ambiguous corporate messagingon platforms such as X (formerly Twitter), we employ an ensemble of largelanguage models (LLMs) to annotate a large corpus of corporate tweets on theirtopical alignment with the 17 Sustainable Development Goals (SDGs). Thisapproach avoids the need for costly, task-specific annotations and explores thepotential of such models as ad-hoc annotators for social media data that canefficiently capture both explicit and implicit references to sustainabilitythemes in a scalable manner. Complementing this textual analysis, we utilizevision-language models (VLMs), within a visual understanding framework thatuses semantic clusters to uncover patterns in visual sustainabilitycommunication. This integrated approach reveals sectoral differences in SDGengagement, temporal trends, and associations between corporate messaging,environmental, social, governance (ESG) risks, and consumer engagement. Ourmethods-automatic label generation and semantic visual clustering-are broadlyapplicable to other domains and offer a flexible framework for large-scalesocial media analysis.</description>
      <author>example@mail.com (Ujjwal Sharma, Stevan Rudinac, Ana Mićković, Willemijn van Dolen, Marcel Worring)</author>
      <guid isPermaLink="false">2511.01550v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Driving scenario generation and evaluation using a structured layer representation and foundational models</title>
      <link>http://arxiv.org/abs/2511.01541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结构化的五层模型，用于改进罕见驾驶场景的评估和生成，并结合大型基础模型使用数据增强策略生成新的驾驶场景。该模型为场景中的每个代理引入子类和特征，使用特定于层模型的嵌入进行比较，并评估了合成数据集的相关性。&lt;h4&gt;背景&lt;/h4&gt;罕见且具有挑战性的驾驶场景对自动驾驶车辆的发展至关重要。由于这些场景难以遇到，使用生成模型模拟或生成它们是一种流行的方法。之前的研究已经尝试在层模型中结构化驾驶场景表示。&lt;h4&gt;目的&lt;/h4&gt;提出一种结构化的五层模型，以提高罕见场景的评估和生成能力。研究并调整两个指标来评估合成数据集在结构化表示背景下的相关性。&lt;h4&gt;方法&lt;/h4&gt;1. 提出结构化的五层模型改进驾驶场景表示；2. 结合大型基础模型采用数据增强策略生成新场景；3. 为每个代理引入子类和特征；4. 使用特定于层模型的嵌入进行比较；5. 研究多样性分数和原创性分数两个评估指标；6. 在不同生成设置下展示指标并进行合成视频的定性评估。&lt;h4&gt;主要发现&lt;/h4&gt;论文展示了在不同生成设置下多样性和原创性分数的应用，以及从结构化场景描述生成的合成视频的定性评估结果。代码和扩展结果可在提供的GitHub链接获取。&lt;h4&gt;结论&lt;/h4&gt;提出的结构化五层模型能够有效改进罕见驾驶场景的评估和生成，结合大型基础模型和数据增强策略可以生成高质量的合成驾驶场景。&lt;h4&gt;翻译&lt;/h4&gt;罕见且具有挑战性的驾驶场景对自动驾驶车辆的发展至关重要。由于它们难以遇到，使用生成模型模拟或生成它们是一种流行方法。在之前尝试在层模型中结构化驾驶场景表示的基础上，我们提出了一种结构化的五层模型来改进罕见场景的评估和生成。我们使用该模型与大型基础模型结合，采用数据增强策略生成新的驾驶场景。与之前的表示方法不同，我们的结构为场景中的每个代理引入了子类和特征，使我们能够使用特定于我们层模型的嵌入来比较它们。我们研究并调整了两个指标来评估合成数据集在结构化表示背景下的相关性：多样性分数估计数据集中场景之间的差异程度，而原创性分数计算合成数据集与真实参考集的相似程度。本文展示了在不同生成设置下的这两个指标，以及对从结构化场景描述生成的合成视频的定性评估。代码和扩展结果可在https://github.com/Valgiz/5LMSG找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成和评估罕见或具有挑战性的驾驶场景（Edge Cases）的问题。这个问题在现实中非常重要，因为这类场景对自动驾驶系统的开发至关重要，但它们在真实驾驶数据中非常稀缺，难以遇到。通过模拟或生成这些罕见场景，研究人员可以测试和改进自动驾驶系统应对极端情况的能力，从而提高系统的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到罕见驾驶场景对自动驾驶开发的重要性，但同时也意识到这些场景难以获取。他们借鉴了Scholtes等人提出的6层模型(6LM)，但将其简化为5层模型(5LM)，去掉了与数字信息相关的第6层。作者利用大型语言模型(LLM)和基础模型来生成新的驾驶场景，采用数据增强策略。他们还研究并调整了两个指标来评估合成数据集的相关性：多样性分数和原创性分数。整个设计过程是基于对现有工作的分析和对自动驾驶场景生成需求的深入理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用结构化的五层模型(5LM)来表示驾驶场景，以便更好地生成和评估罕见场景，并利用大型语言模型和基础模型来增强现有真实场景数据集。整体实现流程包括：1)将驾驶场景分解为5个层次（道路结构、周围建筑、临时变化、动态对象、环境条件）；2)使用多模态大语言模型分析真实驾驶数据并根据5层模型格式化；3)通过两种策略生成新场景（非结构化编辑和结构化层编辑）；4)使用专门的评估指标（原创性分数和多样性分数）来评估生成场景的质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)结构化的五层模型(5LM)提供标准化的驾驶场景表示，引入子类和特征使每个代理都能被详细描述；2)新的合成场景生成策略，通过编辑现有真实场景的特定组件创建边缘案例；3)文本评估方法，提出原创性分数和多样性分数来评估生成场景质量；4)结合视觉和文本生成，先生成结构化场景描述再转化为视觉表示。相比之前的工作，这种方法提供了更精细的场景控制、更全面的评估指标，并确保了生成场景的语义合理性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于结构化五层模型和基础模型的驾驶场景生成与评估方法，通过数据增强策略创建多样化且原创的罕见驾驶场景，为自动驾驶系统开发和测试提供了新的工具和评估框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rare and challenging driving scenarios are critical for autonomous vehicledevelopment. Since they are difficult to encounter, simulating or generatingthem using generative models is a popular approach. Following previous effortsto structure driving scenario representations in a layer model, we propose astructured five-layer model to improve the evaluation and generation of rarescenarios. We use this model alongside large foundational models to generatenew driving scenarios using a data augmentation strategy. Unlike previousrepresentations, our structure introduces subclasses and characteristics forevery agent of the scenario, allowing us to compare them using an embeddingspecific to our layer-model. We study and adapt two metrics to evaluate therelevance of a synthetic dataset in the context of a structured representation:the diversity score estimates how different the scenarios of a dataset are fromone another, while the originality score calculates how similar a syntheticdataset is from a real reference set. This paper showcases both metrics indifferent generation setup, as well as a qualitative evaluation of syntheticvideos generated from structured scenario descriptions. The code and extendedresults can be found at https://github.com/Valgiz/5LMSG.</description>
      <author>example@mail.com (Arthur Hubert, Gamal Elghazaly, Raphaël Frank)</author>
      <guid isPermaLink="false">2511.01541v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA</title>
      <link>http://arxiv.org/abs/2511.01463v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5figures. The Thirty-Ninth Annual Conference on Neural  Information Processing Systems&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了HMVLM模型，基于MoE LoRA策略的统一框架，用于解决人类运动与语言模型整合中的模态差异和灾难性遗忘问题，同时改进姿态表示方法。&lt;h4&gt;背景&lt;/h4&gt;指令调优数据的扩展使基础语言模型能够表现出改进的指令遵循能力和在多样化下游任务上的卓越性能。语义丰富的3D人体运动正逐渐与这些基础模型集成，以增强多模态理解和跨模态生成能力。然而，人体运动和文本之间的模态差异引发了关于这种整合过程中灾难性遗忘的未解决问题。此外，开发能够在异构下游任务中保持泛化能力的自回归兼容姿态表示仍然是一个关键的技术障碍。&lt;h4&gt;目的&lt;/h4&gt;解决人体运动与文本之间的模态差异导致的灾难性遗忘问题，以及开发能够在多样化下游任务中保持泛化能力的自回归兼容姿态表示。&lt;h4&gt;方法&lt;/h4&gt;提出HMVLM（人类运动-视觉-语言模型），这是一个基于专家混合低秩适应（MoE LoRA）策略的统一框架。该框架利用门控网络根据输入提示动态分配LoRA专家权重，实现多任务的同步微调。为减轻指令调优过程中的灾难性遗忘，引入了一种新型零专家，用于保留预训练参数以处理一般语言任务。对于姿态表示，通过将人体划分为不同的关节组，实现了身体部位特定的标记化，提高了表示的空间分辨率。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法有效减轻了指令调优过程中的知识遗忘，并在多样化的人体运动下游任务上取得了卓越的性能。&lt;h4&gt;结论&lt;/h4&gt;HMVLM模型通过MoE LoRA策略和零专家机制，成功解决了人体运动与语言模型整合中的灾难性遗忘问题，同时通过身体部位特定的标记化改进了姿态表示，为多模态理解和生成提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;指令调优数据的扩展使基础语言模型能够在多样化的下游任务上表现出改进的指令遵循能力和卓越性能。语义丰富的3D人体运动正逐渐与这些基础模型集成，以增强多模态理解和跨模态生成能力。然而，人体运动和文本之间的模态差异引发了关于这种整合过程中灾难性遗忘的未解决问题。此外，开发能够在异构下游任务中保持泛化能力的自回归兼容姿态表示仍然是一个关键的技术障碍。为解决这些问题，我们提出了HMVLM（人类运动-视觉-语言模型），这是一个基于专家混合低秩适应（MoE LoRA）策略的统一框架。该框架利用门控网络根据输入提示动态分配LoRA专家权重，实现多任务的同步微调。为减轻指令调优过程中的灾难性遗忘，我们引入了一种新型零专家，用于保留预训练参数以处理一般语言任务。对于姿态表示，我们通过将人体划分为不同的关节组，实现了身体部位特定的标记化，提高了表示的空间分辨率。实验表明，我们的方法有效减轻了指令调优过程中的知识遗忘，并在多样化的人体运动下游任务上取得了卓越的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决两个关键问题：1) 当基础语言模型集成人类运动模态时出现的灾难性遗忘问题，即模型在训练过程中遗忘原有的世界知识和语言能力；2) 如何开发能保留跨任务泛化能力的自回归兼容姿态表示问题。这些问题在现实中很重要，因为随着大型基础模型在多模态理解中的应用日益广泛，将人类运动（包含丰富语义和情感）集成到这些模型中变得至关重要，而遗忘原有知识会导致模型变成对话能力有限的任务特定系统，姿态表示的局限性则会限制虚拟现实、具身智能等多个领域的应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到监督指令微调会过度关注新标记导致遗忘原有知识，因此借鉴了MoE和LoRA技术设计MoE LoRA框架，并引入零专家来保留预训练参数。针对姿态表示问题，作者受图像处理中基于块标记化的启发，将人体划分为不同肢体部分分别编码。作者借鉴了多项现有工作：MoE架构和LoRA技术用于多任务微调，基于块的图像编码用于空间建模，VQ-VAE架构用于离散化运动序列，以及Transformer架构用于自回归生成。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想包括：1) 使用MoE LoRA框架通过门控网络动态分配专家权重实现多任务微调；2) 引入零专家保留预训练参数减轻灾难性遗忘；3) 采用基于身体部位的标记化提高表示空间分辨率。整体流程：首先处理输入指令和提示，通过门控网络分配专家权重；然后对模态特定输入进行投影对齐；接着根据权重动态组合LoRA专家；同时使用身体部位标记器将姿态和运动离散化为标记；最后基础模型和加权专家共同生成输出。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) MoE LoRA框架引入零专家机制保留基础模型知识；2) 基于身体部位的标记化方法提高姿态表示的空间分辨率；3) 统一框架支持多种人体相关下游任务。相比之前工作不同：与MotionGPT等模型相比，HMVLM通过MoE LoRA和零专家更好保留了对话能力；与传统运动标记化相比，同时考虑空间信息提高表示精细度；与单任务模型相比，作为统一多任务框架在多任务场景下仍保持良好性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了HMVLM框架，通过MoE LoRA和基于身体部位的标记化方法，有效解决了人类运动与语言模型集成中的灾难性遗忘和表示问题，同时支持多种人体相关任务的高效执行。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The expansion of instruction-tuning data has enabled foundation languagemodels to exhibit improved instruction adherence and superior performanceacross diverse downstream tasks. Semantically-rich 3D human motion is beingprogressively integrated with these foundation models to enhance multimodalunderstanding and cross-modal generation capabilities. However, the modalitygap between human motion and text raises unresolved concerns about catastrophicforgetting during this integration. In addition, developingautoregressive-compatible pose representations that preserve generalizabilityacross heterogeneous downstream tasks remains a critical technical barrier. Toaddress these issues, we propose the Human Motion-Vision-Language Model(HMVLM), a unified framework based on the Mixture of Expert Low-RankAdaption(MoE LoRA) strategy. The framework leverages the gating network todynamically allocate LoRA expert weights based on the input prompt, enablingsynchronized fine-tuning of multiple tasks. To mitigate catastrophic forgettingduring instruction-tuning, we introduce a novel zero expert that preserves thepre-trained parameters for general linguistic tasks. For pose representation,we implement body-part-specific tokenization by partitioning the human bodyinto different joint groups, enhancing the spatial resolution of therepresentation. Experiments show that our method effectively alleviatesknowledge forgetting during instruction-tuning and achieves remarkableperformance across diverse human motion downstream tasks.</description>
      <author>example@mail.com (Lei Hu, Yongjing Ye, Shihong Xia)</author>
      <guid isPermaLink="false">2511.01463v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation</title>
      <link>http://arxiv.org/abs/2511.01445v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14pages, 7 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种分层多智能体框架，将被动医疗AI系统转变为主动询问智能体，通过自主任务编排优化预咨询流程，在提高效率的同时保证了临床质量。&lt;h4&gt;背景&lt;/h4&gt;全球医疗系统面临患者数量增加和咨询时间有限的挑战，许多国家初级保健咨询平均不足5分钟。现有预咨询流程受限于被动交互模式和AI系统的上下文管理挑战。&lt;h4&gt;目的&lt;/h4&gt;引入分层多智能体框架，将被动医疗AI系统转变为主动询问智能体，通过自主任务编排提升预咨询效率和质量。&lt;h4&gt;方法&lt;/h4&gt;开发八智能体架构，具有集中控制机制，将预咨询分解为四个主要任务（分诊、现病史采集、既往史采集、主诉生成）及13个子任务。在1,372个中国医疗平台电子健康记录上评估，使用GPT-OSS 20B、Qwen3-8B、Phi4-14B等基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;框架在初级科室分诊准确率达87.0%，二级科室分类达80.5%；智能体驱动调度任务完成率98.2%，高于顺序处理的93.1%；临床质量评分平均为：主诉4.56分、现病史4.48分、既往史4.69分（5分制）；T2在12.7轮内完成，T3在16.9轮内完成。&lt;h4&gt;结论&lt;/h4&gt;模型无关架构在不同基础模型上保持高性能，通过本地部署保护数据隐私，展示了自主AI系统增强临床预咨询效率和质量的可能性。&lt;h4&gt;翻译&lt;/h4&gt;全球医疗系统正面临患者数量增加和咨询时间有限的严峻挑战，在许多国家，初级保健咨询的平均时间不足5分钟。虽然涵盖分诊和结构化病史采集的预咨询流程提供了潜在解决方案，但它们仍受限于现有AI系统中的被动交互模式和上下文管理挑战。本研究引入了一种分层多智能体框架，通过自主任务编排将被动的医疗AI系统转变为主动的询问智能体。我们开发了一个具有集中控制机制的八智能体架构，将预咨询分解为四个主要任务：分诊、现病史采集、既往史采集和主诉生成，其中T1-T3进一步细分为13个特定领域的子任务。在中国医疗平台的1,372个经过验证的电子健康记录上，使用多个基础模型（GPT-OSS 20B、Qwen3-8B、Phi4-14B）评估，该框架在初级科室分诊上达到87.0%的准确率，在二级科室分类上达到80.5%的准确率，使用智能体驱动的调度任务完成率达到98.2%，而顺序处理为93.1%。18名医师的临床质量评分平均为：主诉4.56分、现病史4.48分、既往史4.69分（5分制），T2在12.7轮内完成，T3在16.9轮内完成。与模型无关的架构在不同基础模型上保持了高性能，同时通过本地部署保护数据隐私，展示了自主AI系统增强临床环境中预咨询效率和质量的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Global healthcare systems face critical challenges from increasing patientvolumes and limited consultation times, with primary care visits averagingunder 5 minutes in many countries. While pre-consultation processesencompassing triage and structured history-taking offer potential solutions,they remain limited by passive interaction paradigms and context managementchallenges in existing AI systems. This study introduces a hierarchicalmulti-agent framework that transforms passive medical AI systems into proactiveinquiry agents through autonomous task orchestration. We developed aneight-agent architecture with centralized control mechanisms that decomposespre-consultation into four primary tasks: Triage ($T_1$), History of PresentIllness collection ($T_2$), Past History collection ($T_3$), and ChiefComplaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13domain-specific subtasks. Evaluated on 1,372 validated electronic healthrecords from a Chinese medical platform across multiple foundation models(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy forprimary department triage and 80.5% for secondary department classification,with task completion rates reaching 98.2% using agent-driven scheduling versus93.1% with sequential processing. Clinical quality scores from 18 physiciansaveraged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and4.69 for Past History on a 5-point scale, with consultations completed within12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnosticarchitecture maintained high performance across different foundation modelswhile preserving data privacy through local deployment, demonstrating thepotential for autonomous AI systems to enhance pre-consultation efficiency andquality in clinical settings.</description>
      <author>example@mail.com (ChengZhang Yu, YingRu He, Hongyan Cheng, nuo Cheng, Zhixing Liu, Dongxu Mu, Zhangrui Shen, Zhanpeng Jin)</author>
      <guid isPermaLink="false">2511.01445v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Towards General Auditory Intelligence: Large Multimodal Models for Machine Listening and Speaking</title>
      <link>http://arxiv.org/abs/2511.01299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是一篇综述，探讨了在大语言模型和通用人工智能时代，计算机听觉如何超越传统范式，充分利用基础模型的能力，朝着更全面的理解、更自然的生成和更类人的交互方向发展。&lt;h4&gt;背景&lt;/h4&gt;在大语言模型和通用人工智能时代，计算机听觉需要发展以适应新的技术环境。音频作为一种包含丰富语义、情感和上下文线索的模式，在实现自然化和具身机器智能方面发挥着关键作用。&lt;h4&gt;目的&lt;/h4&gt;这篇综述旨在全面回顾近期将音频整合到LLMs中的进展，分析LLMs如何重塑音频感知和推理能力，探索音频和视觉模式的融合如何增强情境感知和跨模态推理，并确定构建音频原生AGI系统的关键挑战和未来方向。&lt;h4&gt;方法&lt;/h4&gt;论文采用综述方法，分析近期在音频与LLMs整合方面的研究进展，特别关注四个关键领域：音频理解、音频生成、基于语音的交互和音频-视觉理解。论文分析了LLMs如何改变音频感知和推理，探索了多模态智能的边界。&lt;h4&gt;主要发现&lt;/h4&gt;LLMs使系统能够在更深层次的语义水平上理解声音，生成富有表现力的音频输出，进行类人的口语交互，并且音频和视觉模式的融合增强了情境感知和跨模态推理，推动了多模态智能的边界。&lt;h4&gt;结论&lt;/h4&gt;这篇综述不仅综合了现有研究，还确定了构建能够像人类一样通过声音感知、理解和交互的音频原生AGI系统的关键挑战和未来方向。&lt;h4&gt;翻译&lt;/h4&gt;在大语言模型和通用人工智能时代，计算机听觉必须超越传统范式，充分利用基础模型的能力，朝着更全面的理解、更自然的生成和更类人的交互方向发展。音频作为一种富含语义、情感和上下文线索的模式，在实现自然化和具身机器智能方面发挥着至关重要的作用。这篇综述全面回顾了近期将音频整合到LLMs中的进展，重点关注四个关键领域：音频理解、音频生成、基于语音的交互和音频-视觉理解。我们分析了LLMs如何重塑音频感知和推理，使系统能够在更深层次的语义水平上理解声音，生成富有表现力的音频输出，并进行类人的口语交互。此外，我们还探索了音频和视觉模式的融合如何增强情境感知和跨模态推理，推动多模态智能的边界。这篇综述不仅综合了现有研究，还确定了构建能够像人类一样自然地通过声音感知、理解和交互的音频原生AGI系统的关键挑战和未来方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of large language models (LLMs) and artificial generalintelligence (AGI), computer audition must evolve beyond traditional paradigmsto fully leverage the capabilities of foundation models, towards morecomprehensive understanding, more natural generation and more human-likeinteraction. Audio, as a modality rich in semantic, emotional, and contextualcues, plays a vital role in achieving naturalistic and embodied machineintelligence. This survey provides a comprehensive review of recent progress inintegrating audio into LLMs, with a focus on four key areas: audiocomprehension, audio generation, speech-based interaction, and audio-visualunderstanding. We analyze how LLMs are reshaping audio perception andreasoning, enabling systems to understand sound at a deeper semantic level,generate expressive audio outputs, and engage in human-like spoken interaction.Furthermore, we explore how the fusion of audio and visual modalities enhancessituational awareness and cross-modal reasoning, pushing the boundaries ofmultimodal intelligence. This survey not only synthesizes existing research butalso identifies critical challenges and future directions for buildingaudio-native AGI systems capable of perceiving, understanding, and interactingthrough sound as naturally as humans do.</description>
      <author>example@mail.com (Siyin Wang, Zengrui Jin, Changli Tang, Qiujia Li, Bo Li, Chen Chen, Yuchen Hu, Wenyi Yu, Yixuan Li, Jimin Zhuang, Yudong Yang, Mingqiu Wang, Michael Han, Yifan Ding, Junwen Bai, Tom Ouyang, Shuo-yiin Chang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Lu Lu, Guangzhi Sun, Zhehuai Chen, Ji Wu, Bowen Zhou, Yuxuan Wang, Tara Sainath, Yonghui Wu, Chao Zhang)</author>
      <guid isPermaLink="false">2511.01299v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Optimal Attention Temperature Enhances In-Context Learning under Distribution Shift</title>
      <link>http://arxiv.org/abs/2511.01292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了在分布偏移条件下注意力温度对预训练Transformer模型上下文学习性能的影响，首次提供了理论和实证研究，证明了最优注意力温度可以最小化分布偏移导致的误差，提高了ICL的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;预训练Transformer模型在上下文学习方面表现出色，但当预训练和测试数据间存在分布偏移时，ICL性能会急剧下降，这种情况在实际部署中越来越常见。虽然调整注意力温度可以提高Transformer性能，但在分布偏移条件下注意力温度对ICL的影响尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;提供关于在分布偏移条件下ICL注意力温度的首个理论和实证研究，探索最优注意力温度对提高ICL鲁棒性的作用。&lt;h4&gt;方法&lt;/h4&gt;使用'线性化softmax'框架推导闭式泛化误差表达式，证明输入协方差变化和标签噪声对ICL的影响，并通过线性回归任务的模拟和GPT-2、LLaMA2-7B在问答基准上的大规模实验验证理论预测。&lt;h4&gt;主要发现&lt;/h4&gt;输入协方差的变化或标签噪声会显著损害ICL性能；存在最优注意力温度可以最小化分布偏移条件下的误差；注意力温度是提高预训练Transformer中ICL鲁棒性的有效机制。&lt;h4&gt;结论&lt;/h4&gt;注意力温度是提高预训练Transformer中ICL鲁棒性的原则性和强大机制，研究推进了理论理解，并为实践中选择注意力温度提供了可行的指导。&lt;h4&gt;翻译&lt;/h4&gt;预训练Transformer在上下文学习方面表现出色，仅从少量例子中就能推断新任务。然而，当预训练和测试数据之间存在分布偏移时，它们的ICL性能可能会急剧下降，这种情况在实际部署中越来越常见。虽然最近的实证研究表明调整softmax中的注意力温度可以提高Transformer性能，但在分布偏移条件下，注意力温度在ICL中的作用仍未被探索。本文首次提供了关于在分布偏移条件下ICL注意力温度的理论和实证研究。使用简化但富有表现力的'线性化softmax'框架，我们推导出闭式泛化误差表达式，并证明输入协方差的变化或标签噪声会显著损害ICL，但存在最优注意力温度可以最小化这种误差。然后，我们通过线性回归任务的广泛模拟和GPT-2及LLaMA2-7B在问答基准上的大规模实验验证了我们的预测。我们的研究结果表明，注意力温度是提高预训练Transformer中ICL鲁棒性的原则性和强大机制，推进了理论理解，并为实践中选择注意力温度提供了可行的指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pretrained Transformers excel at in-context learning (ICL), inferring newtasks from only a handful of examples. Yet, their ICL performance can degradesharply under distribution shift between pretraining and test data, a regimeincreasingly common in real-world deployments. While recent empirical workhints that adjusting the attention temperature in the softmax can enhanceTransformer performance, the attention temperature's role in ICL underdistribution shift remains unexplored. This paper provides the firsttheoretical and empirical study of attention temperature for ICL underdistribution shift. Using a simplified but expressive "linearized softmax"framework, we derive closed-form generalization error expressions and provethat shifts in input covariance or label noise substantially impair ICL, butthat an optimal attention temperature exists which minimizes this error. Wethen validate our predictions through extensive simulations on linearregression tasks and large-scale experiments with GPT-2 and LLaMA2-7B onquestion-answering benchmarks. Our results establish attention temperature as aprincipled and powerful mechanism for improving the robustness of ICL inpretrained Transformers, advancing theoretical understanding and providingactionable guidance for selecting attention temperature in practice.</description>
      <author>example@mail.com (Samet Demir, Zafer Dogan)</author>
      <guid isPermaLink="false">2511.01292v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions</title>
      <link>http://arxiv.org/abs/2511.01284v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于Foundation models在医学图像分析中应用的综述文章，评估了将FMs适应医学成像特定需求的多种策略，包括监督微调、领域特定预训练等方法，并指出了新兴研究方向如持续学习、隐私保护方法等，为开发适应性、可信且临床整合的FMs提供了路线图。&lt;h4&gt;背景&lt;/h4&gt;Foundation models已成为医学图像分析中的一种变革性范式，能够为广泛的临床任务和成像模式提供可泛化、任务无关的解决方案。它们从大规模数据中学习可迁移表示的能力，有望解决传统特定任务模型的局限性。然而，将FMs适应真实临床实践仍面临关键挑战，包括域偏移、高质量标注数据有限、计算需求大以及严格的隐私要求。&lt;h4&gt;目的&lt;/h4&gt;本文对将FMs适应医学成像特定需求的策略进行了全面评估。&lt;h4&gt;方法&lt;/h4&gt;检查了监督微调、领域特定预训练、参数高效微调、自监督学习、混合方法以及多模态或跨模态框架等适应FMs的方法，并对每种方法评估了报告的性能提升、临床适用性和局限性，同时确定了先前综述经常忽视的权衡和未解决的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;新兴研究方向包括：实现动态部署的持续学习；保护敏感数据的联邦和隐私保护方法；提高数据效率的混合自监督学习；结合合成生成与人工验证循环的数据中心管道；以及评估在真实临床变异性下的鲁棒泛化的系统基准测试。&lt;h4&gt;结论&lt;/h4&gt;通过概述这些策略和相关研究差距，本综述为开发适应性、可信且临床整合的FMs提供了路线图，使其能够满足真实医学成像的需求。&lt;h4&gt;翻译&lt;/h4&gt;基础模型已成为医学图像分析中的一种变革性范式，能够为广泛的临床任务和成像模式提供可泛化、任务无关的解决方案。它们从大规模数据中学习可迁移表示的能力，有望解决传统特定任务模型的局限性。然而，将FMs适应真实临床实践仍面临关键挑战，包括域偏移、高质量标注数据有限、计算需求大以及严格的隐私要求。本文对将FMs适应医学成像特定需求的策略进行了全面评估。我们检查了监督微调、领域特定预训练、参数高效微调、自监督学习、混合方法以及多模态或跨模态框架等方法。对于每种方法，我们评估了报告的性能提升、临床适用性和局限性，同时确定了先前综述经常忽视的权衡和未解决的挑战。除了这些已建立的技术外，我们还强调了旨在解决当前差距的新兴方向，包括实现动态部署的持续学习、保护敏感数据的联邦和隐私保护方法、提高数据效率的混合自监督学习、结合合成生成与人工验证循环的数据中心管道，以及评估在真实临床变异性下的鲁棒泛化的系统基准测试。通过概述这些策略和相关研究差距，本综述为开发适应性、可信且临床整合的FMs提供了路线图，使其能够满足真实医学成像的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) have emerged as a transformative paradigm in medicalimage analysis, offering the potential to provide generalizable, task-agnosticsolutions across a wide range of clinical tasks and imaging modalities. Theircapacity to learn transferable representations from large-scale data has thepotential to address the limitations of conventional task-specific models.However, adaptation of FMs to real-world clinical practice remains constrainedby key challenges, including domain shifts, limited availability ofhigh-quality annotated data, substantial computational demands, and strictprivacy requirements. This review presents a comprehensive assessment ofstrategies for adapting FMs to the specific demands of medical imaging. Weexamine approaches such as supervised fine-tuning, domain-specific pretraining,parameter-efficient fine-tuning, self-supervised learning, hybrid methods, andmultimodal or cross-modal frameworks. For each, we evaluate reportedperformance gains, clinical applicability, and limitations, while identifyingtrade-offs and unresolved challenges that prior reviews have often overlooked.Beyond these established techniques, we also highlight emerging directionsaimed at addressing current gaps. These include continual learning to enabledynamic deployment, federated and privacy-preserving approaches to safeguardsensitive data, hybrid self-supervised learning to enhance data efficiency,data-centric pipelines that combine synthetic generation with human-in-the-loopvalidation, and systematic benchmarking to assess robust generalization underreal-world clinical variability. By outlining these strategies and associatedresearch gaps, this review provides a roadmap for developing adaptive,trustworthy, and clinically integrated FMs capable of meeting the demands ofreal-world medical imaging.</description>
      <author>example@mail.com (Karma Phuntsho, Abdullah, Kyungmi Lee, Ickjai Lee, Euijoon Ahn)</author>
      <guid isPermaLink="false">2511.01284v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play</title>
      <link>http://arxiv.org/abs/2511.01261v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  67 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Speech-DRAME是一个统一的框架，用于评估语音角色扮演系统，包含评估基准、微调评估模型和语音角色扮演基准三个层面贡献，通过原型评估和真实性评估两种互补策略提供全面的评估基础。&lt;h4&gt;背景&lt;/h4&gt;角色扮演已成为生成模型的关键测试平台，从纯文本对话扩展到多模态交互。将角色扮演扩展到语音可以捕捉韵律、情感和表达方式，但也带来了新的评估挑战。&lt;h4&gt;目的&lt;/h4&gt;提出Speech-DRAME框架，解决当前语音角色扮演评估中存在的问题，为语音角色扮演提供全面、可复制的评估基础。&lt;h4&gt;方法&lt;/h4&gt;Speech-DRAME框架在三个层面做出贡献：(i)Speech-DRAME-EvalBench评估基准，包含双语人工注释数据和用于训练测试语音评估模型的协议，(ii)DRAME-Eval微调评估模型，显著优于零样本和少样本音频大语言模型，(iii)Speech-DRAME-RoleBench语音角色扮演基准，利用DRAME-Eval作为自动评估者。同时区分了原型评估和真实性评估两种互补的评估策略。&lt;h4&gt;主要发现&lt;/h4&gt;与零样本音频大语言模型评估者相比，DRAME-Eval与人类评分的一致性更强，原型评估中的相关系数从0.480提高到0.629，真实性评估中从0.390提高到0.625。&lt;h4&gt;结论&lt;/h4&gt;通过整合透明的基准资源、建模方法和系统级评估，Speech-DRAME为评估语音角色扮演提供了首个全面、可复制的基础。&lt;h4&gt;翻译&lt;/h4&gt;角色扮演已成为生成模型的关键测试平台，从纯文本对话扩展到多模态交互。将角色扮演扩展到语音可以捕捉韵律、情感和表达方式，但也带来了新的评估挑战。当前的评估流程通常使用音频大语言模型作为零样本评估者，这些模型会忽略副语言线索，将多个方面合并为粗略的分数，并依赖无法反映现实世界角色的合成语音参考。我们提出了Speech-DRAME，一个统一框架，在三个层面做出贡献：(i)Speech-DRAME-EvalBench，一个包含双语人工注释数据和用于训练测试语音评估模型的协议的评估基准，(ii)DRAME-Eval，一个微调的评估模型，显著优于零样本和少样本音频大语言模型，(iii)Speech-DRAME-RoleBench，一个利用DRAME-Eval作为自动评估者来比较语音基础模型的语音角色扮演基准。Speech-DRAME区分了两种互补的评估策略：原型评估，一种自上而下的方法，衡量对广泛角色原型的遵循程度；真实性评估，一种基于真实人类语音的自下而上方法，强调细微的角色质量。与零样本音频大语言模型评估者相比，DRAME-Eval与人类评分的一致性更强（原型评估中的相关系数从0.480提高到0.629，真实性评估中从0.390提高到0.625）。通过整合透明的基准资源、建模方法和系统级评估，Speech-DRAME为评估语音角色扮演提供了首个全面、可复制的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Role-play has become a key testbed for generative models, expanding fromtext-only dialogue to multimodal interaction. Extending role-play to speechcaptures prosody, emotion, and delivery, but also poses new evaluationchallenges. Current pipelines often use audio large language models (ALLMs) aszero-shot judges, which miss paralinguistic cues, collapse multiple aspectsinto coarse scores, and rely on synthetic speech references that fail toreflect real-world roles. We present Speech-DRAME, a unified framework thatcontributes at three levels: (i) Speech-DRAME-EvalBench, an evaluationbenchmark with bilingual human-annotated data and protocols for training andtesting speech evaluation models (SEMs), (ii) DRAME-Eval, a fine-tunedevaluation model, which substantially outperforms zero-shot and few-shot ALLMs,and (iii) Speech-DRAME-RoleBench, a speech role-play benchmark that leveragesDRAME-Eval as an automatic judge to compare speech foundation models (SFMs).Speech-DRAME distinguishes between two complementary evaluation strategies:Archetype Evaluation, a top-down approach measuring adherence to broad rolearchetypes, and Realism Evaluation, a bottom-up approach grounded in real humanspeech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges,DRAME-Eval achieves stronger agreement with human ratings (Pearson correlationfrom 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). Byintegrating transparent benchmark resources, modeling approaches, andsystem-level evaluation, Speech-DRAME provides the first comprehensive,reproducible foundation for assessing spoken role-play.</description>
      <author>example@mail.com (Jiatong Shi, Jionghao Han, Yichen Lu, Santiago Pascual, Pengfei Wu, Chenye Cui, Shinji Watanabe, Chao Weng, Cong Zhou)</author>
      <guid isPermaLink="false">2511.01261v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel</title>
      <link>http://arxiv.org/abs/2511.00981v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VesSAM是一个专门针对2D血管分割的强大高效框架，通过结合卷积适配器、多提示编码器和轻量级解码器，解决了血管分割中的挑战，在各种成像模态上表现优异，且在分布外数据上具有良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;准确的血管分割对临床应用（如疾病诊断和手术规划）至关重要，但由于血管具有细小、分支结构和低纹理对比度，血管分割仍然具有挑战性。基础模型如Segment Anything Model (SAM)在通用分割方面显示出前景，但在血管结构上表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对2D血管分割的强大而高效的框架，以克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;VesSAM框架包含三个关键组件：(1)卷积适配器增强局部纹理特征，(2)多提示编码器通过分层交叉注意力融合解剖学提示（包括骨架、分叉点和线段中点），(3)轻量级掩码解码器减少锯齿伪影。此外，还引入了自动化流程生成结构化多提示标注，并整理了包含5种成像模态8个数据集的多样化基准数据集。&lt;h4&gt;主要发现&lt;/h4&gt;VesSAM在Dice和IoU指标上比最先进的基于PEFT的SAM变体高出10%和13%，与完全微调的方法相比实现了具有竞争力的性能，且参数显著减少。VesSAM在分布外（OoD）设置中表现良好，在平均OoD Dice和IoU上优于所有基线。&lt;h4&gt;结论&lt;/h4&gt;VesSAM是一个专门为血管分割设计的有效框架，通过结合创新架构和提示机制，显著提升了血管分割的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;准确的血管分割对临床应用（如疾病诊断和手术规划）至关重要，但由于血管细小、分支结构和低纹理对比度，仍然具有挑战性。虽然像Segment Anything Model (SAM)这样的基础模型在通用分割方面显示出前景，但在血管结构上表现不佳。在这项工作中，我们提出了VesSAM，一个专门针对2D血管分割的强大而高效的框架。VesSAM集成了(1)卷积适配器增强局部纹理特征，(2)多提示编码器通过分层交叉注意力融合解剖学提示，包括骨架、分叉点和线段中点，以及(3)轻量级掩码解码器减少锯齿伪影。我们还引入了自动化流程生成结构化多提示标注，并整理了一个包含5种成像模态8个数据集的多样化基准数据集。实验结果表明，VesSAM在Dice和IoU指标上持续比最先进的基于PEFT的SAM变体高出10%和13%，与完全微调的方法相比实现了具有竞争力的性能，且参数显著减少。VesSAM在分布外（OoD）设置中泛化良好，在平均OoD Dice和IoU上优于所有基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate vessel segmentation is critical for clinical applications such asdisease diagnosis and surgical planning, yet remains challenging due to thin,branching structures and low texture contrast. While foundation models like theSegment Anything Model (SAM) have shown promise in generic segmentation, theyperform sub-optimally on vascular structures. In this work, we present VesSAM,a powerful and efficient framework tailored for 2D vessel segmentation. VesSAMintegrates (1) a convolutional adapter to enhance local texture features, (2) amulti-prompt encoder that fuses anatomical prompts, including skeletons,bifurcation points, and segment midpoints, via hierarchical cross-attention,and (3) a lightweight mask decoder to reduce jagged artifacts. We alsointroduce an automated pipeline to generate structured multi-promptannotations, and curate a diverse benchmark dataset spanning 8 datasets across5 imaging modalities. Experimental results demonstrate that VesSAM consistentlyoutperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%IoU, and achieves competitive performance compared to fully fine-tuned methods,with significantly fewer parameters. VesSAM also generalizes well toout-of-distribution (OoD) settings, outperforming all baselines in average OoDDice and IoU.</description>
      <author>example@mail.com (Suzhong Fu, Rui Sun, Xuan Ding, Jingqi Dong, Yiming Yang, Yao Zhu, Min Chang Jordan Ren, Delin Deng, Angelica Aviles-Rivero, Shuguang Cui, Zhen Li)</author>
      <guid isPermaLink="false">2511.00981v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2511.01618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过引入视角学习任务和Viewpoint-100K数据集，采用两阶段微调策略和混合冷启动初始化方法，有效提升了多模态大语言模型在3D推理任务中的空间推理能力，为未来机器人、自主系统和3D场景理解等领域的发展提供了支持。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在2D视觉理解方面取得了显著进展，引起了人们对将其应用于复杂3D推理任务的兴趣。然而，尚不清楚这些模型是否能有效捕捉稳健现实世界性能所需的详细空间信息，特别是跨视图一致性，这是准确3D推理的关键要求。&lt;h4&gt;目的&lt;/h4&gt;引入视角学习任务，旨在评估和改进多模态大语言模型的空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出Viewpoint-100K数据集，包含10万个以对象为中心的图像对，具有多样化的视角和相应的问题-答案对；采用两阶段微调策略：首先通过监督微调将基础知识注入基础模型，然后通过强化学习增强泛化能力；引入混合冷启动初始化方法，同时学习视角表示并保持连贯的推理思维。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法显著激活了多模态大语言模型的空间推理能力，提高了在领域内和领域外推理任务上的性能。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了在多模态大语言模型中开发基础空间技能的价值，支持机器人、自主系统和3D场景理解方面的未来进步。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型的最新进展显著提高了二维视觉理解能力，促使人们对其在复杂三维推理任务中的应用产生兴趣。然而，这些模型是否能有效捕捉稳健现实世界性能所需的详细空间信息，特别是跨视图一致性，这一准确三维推理的关键要求，仍不清楚。考虑到这一问题，我们引入了视角学习，这是一个旨在评估和改进多模态大语言模型空间推理能力的任务。我们提出了Viewpoint-100K数据集，包含10万个以对象为中心的图像对，具有多样化的视角和相应的问题-答案对。我们的方法采用两阶段微调策略：首先，通过在Viewpoint-100K上进行监督微调将基础知识注入基础多模态大语言模型，在多个任务上取得显著改进；其次，通过在更广泛的问题集上使用组相对策略优化算法的强化学习来增强泛化能力。此外，我们引入了一种混合冷启动初始化方法，旨在同时学习视角表示并保持连贯的推理思维。实验结果表明，我们的方法显著激活了多模态大语言模型的空间推理能力，提高了在领域内和领域外推理任务上的性能。我们的研究结果强调了在多模态大语言模型中开发基础空间技能的价值，支持机器人、自主系统和三维场景理解方面的未来发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态大语言模型(MLLMs)无法有效捕捉3D空间推理所需的详细空间信息，特别是跨视图一致性的问题。这个问题很重要，因为虽然MLLMs在2D视觉理解方面进步显著，但在需要准确3D推理的现实应用中表现不佳，限制了机器人在复杂环境中的导航、自主系统的空间感知以及3D场景理解等关键应用的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到当前MLLMs主要在强调2D连续性的视频数据上训练，导致它们难以理解3D空间一致性。他们从计算机视觉领域的相机校准和立体匹配方法中获得启发，设计了简化的视角学习任务，将复杂的3D问题分解为简单的多选题。作者借鉴了参考帧(FoR)概念、Group Relative Policy Optimization算法、冷启动初始化和思维链(CoT)等现有技术，但将其创新性地应用于激活MLLMs的空间推理能力上。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过专门的视角学习任务激活MLLMs的空间推理能力，采用两阶段微调策略：先注入基础知识，再增强泛化能力。整体流程包括：1)创建Viewpoint-100K数据集，包含10万个对象中心的图像对和问答对；2)第一阶段使用监督微调(SFT)和混合冷启动初始化(90%真实数据+10%伪思维链)注入基础知识；3)第二阶段使用强化学习GRPO算法在SAT数据集上增强泛化能力，鼓励模型生成自己的推理链并应用已学空间知识。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出视角学习任务评估和改进MLLMs空间推理能力；2)创建Viewpoint-100K数据集；3)采用两阶段微调策略；4)设计混合冷启动初始化方法；5)将复杂3D问题简化为多选题。相比之前工作，本文更关注空间推理的基础任务(如视角估计)而非高级推理；不依赖额外3D信息而是激活模型内在能力；不仅提高特定任务性能，还增强领域外泛化能力；强调基础的3D感知能力而非仅关注高层次的推理。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过视角学习任务和两阶段微调策略，成功激活了多模态大语言模型的空间推理能力，显著提升了模型在视觉和空间推理任务中的性能，特别是在领域外任务中的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Multimodal Large Language Models (MLLMs) havesignificantly improved 2D visual understanding, prompting interest in theirapplication to complex 3D reasoning tasks. However, it remains unclear whetherthese models can effectively capture the detailed spatial information requiredfor robust real-world performance, especially cross-view consistency, a keyrequirement for accurate 3D reasoning. Considering this issue, we introduceViewpoint Learning, a task designed to evaluate and improve the spatialreasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,consisting of 100K object-centric image pairs with diverse viewpoints andcorresponding question-answer pairs. Our approach employs a two-stagefine-tuning strategy: first, foundational knowledge is injected to the baselineMLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting insignificant improvements across multiple tasks; second, generalization isenhanced through Reinforcement Learning using the Group Relative PolicyOptimization (GRPO) algorithm on a broader set of questions. Additionally, weintroduce a hybrid cold-start initialization method designed to simultaneouslylearn viewpoint representations and maintain coherent reasoning thinking.Experimental results show that our approach significantly activates the spatialreasoning ability of MLLM, improving performance on both in-domain andout-of-domain reasoning tasks. Our findings highlight the value of developingfoundational spatial skills in MLLMs, supporting future progress in robotics,autonomous systems, and 3D scene understanding.</description>
      <author>example@mail.com (Xiaoyu Zhan, Wenxuan Huang, Hao Sun, Xinyu Fu, Changfeng Ma, Shaosheng Cao, Bohan Jia, Shaohui Lin, Zhenfei Yin, Lei Bai, Wanli Ouyang, Yuanqi Li, Jie Guo, Yanwen Guo)</author>
      <guid isPermaLink="false">2511.01618v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model</title>
      <link>http://arxiv.org/abs/2511.01571v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17pages,7 figures, 5 tabels&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PixelVLA是一种新型Vision-Language-Action模型，支持像素级推理和多模态提示，通过两阶段自动注释流程生成Pixel-160K数据集，实验显示其比OpenVLA提高操作成功率10.1%-17.8%，同时仅需1.5%的预训练成本。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language-Action models (VLAs)是学习通用视觉运动控制策略的有力工具，但当前VLAs主要在大规模图像-文本-动作数据上训练，存在两个关键限制：难以进行像素级场景理解，以及严重依赖文本提示，降低了在现实世界环境中的灵活性。&lt;h4&gt;目的&lt;/h4&gt;解决当前VLAs在像素级场景理解和现实世界应用灵活性方面的限制，引入支持像素级推理和多模态提示（文本和视觉输入）的VLA模型。&lt;h4&gt;方法&lt;/h4&gt;基于新的视觉运动指令调整框架，集成多尺度像素感知编码器和视觉提示编码器，提出两阶段自动注释流程生成Pixel-160K数据集，该数据集具有从现有机器人数据派生的像素级注释。&lt;h4&gt;主要发现&lt;/h4&gt;在三个标准VLA基准测试和两个VLA模型变体上的实验表明，PixelVLA比OpenVLA提高操作成功率10.1%-17.8%，同时仅需OpenVLA 1.5%的预训练成本。&lt;h4&gt;结论&lt;/h4&gt;PixelVLA可以集成到现有VLAs中，在复杂环境中实现更准确、高效和多功能的机器人控制，数据集和代码将作为开源发布。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-动作模型（VLAs）正成为学习通用视觉运动控制策略的有力工具。然而，当前的VLAs主要在大规模的图像-文本-动作数据上训练，并在两个方面存在局限：（i）它们难以进行像素级场景理解，（ii）它们严重依赖文本提示，这降低了它们在现实世界环境中的灵活性。为应对这些挑战，我们引入了PixelVLA，这是第一个支持像素级推理和多模态提示（文本和视觉输入）的VLA模型。我们的方法建立在一种新的视觉运动指令调整框架上，该框架集成了多尺度像素感知编码器和视觉提示编码器。为了有效训练PixelVLA，我们进一步提出了一种两阶段自动注释流程，生成Pixel-160K，这是一个从现有机器人数据派生的大型像素级注释数据集。在三个标准VLA基准测试和两个VLA模型变体上的实验表明，PixelVLA比OpenVLA提高操作成功率10.1%-17.8%，同时仅需其1.5%的预训练成本。这些结果表明，PixelVLA可以集成到现有VLAs中，在复杂环境中实现更准确、高效和多功能的机器人控制。数据集和代码将作为开源发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有视觉-语言-动作模型(VLAs)的两个关键限制：1)缺乏像素级场景理解能力，2)过度依赖文本提示而缺乏对视觉提示的灵活处理。这些问题在现实中很重要，因为像素级理解对于机器人在复杂环境中进行精确操作至关重要，而多模态提示能力则能增强人机交互的灵活性和适应性，使机器人能更好地应对现实世界中的多样化任务和场景变化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有VLAs的局限性，然后借鉴了视觉指令调优在视觉语言模型中的成功经验。具体设计上，作者参考了OpenVLA和π0等VLA模型的基本架构，利用SAM模型进行图像分割，并采用LoRA技术进行模型微调。通过引入多尺度像素感知编码器、视觉提示编码器和连续动作解码器这三个核心组件，以及设计两阶段自动注释管道来生成高质量数据集，作者构建了PixelVLA模型，实现了像素级理解和多模态提示能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过像素级理解和多模态提示能力增强VLAs在复杂环境中的空间感知和操作精度。整体实现流程分为三部分：1)架构设计，包含视觉编码器、视觉提示编码器、多尺度像素感知编码器、LLM骨干和连续动作解码器；2)数据生成，通过两阶段自动注释管道(夹爪感知区域提案阶段和多模态对象分割阶段)创建Pixel-160K数据集；3)训练流程，包括连续动作训练阶段和像素级理解增强阶段，后者使用LoRA适配进行微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多尺度像素感知编码器，实现像素级场景理解；2)视觉提示编码器，支持点、线、区域和掩码等多种视觉提示；3)两阶段自动注释管道，生成高质量像素级标注数据集Pixel-160K；4)连续动作解码器，直接预测连续动作表示。相比之前工作，PixelVLA突破了传统VLAs仅处理图像级别信息和依赖文本提示的限制，实现了更精细的空间理解和更灵活的人机交互方式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PixelVLA通过引入像素级理解和多模态提示能力，显著提升了机器人在复杂环境中的操作精度和泛化能力，同时仅需1.5%的预训练成本就能实现10.1%~28.7%的操作成功率提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action models (VLAs) are emerging as powerful tools forlearning generalizable visuomotor control policies. However, current VLAs aremostly trained on large-scale image-text-action data and remain limited in twokey ways: (i) they struggle with pixel-level scene understanding, and (ii) theyrely heavily on textual prompts, which reduces their flexibility in real-worldsettings. To address these challenges, we introduce PixelVLA, the first VLAmodel designed to support both pixel-level reasoning and multimodal promptingwith text and visual inputs. Our approach is built on a new visuomotorinstruction tuning framework that integrates a multiscale pixel-aware encoderwith a visual prompting encoder. To train PixelVLA effectively, we furtherpropose a two-stage automated annotation pipeline that generates Pixel-160K, alarge-scale dataset with pixel-level annotations derived from existing robotdata. Experiments on three standard VLA benchmarks and two VLA model variantsshow that PixelVLA improves manipulation success rates by 10.1%-17.8% overOpenVLA, while requiring only 1.5% of its pretraining cost. These resultsdemonstrate that PixelVLA can be integrated into existing VLAs to enable moreaccurate, efficient, and versatile robot control in complex environments. Thedataset and code will be released as open source.</description>
      <author>example@mail.com (Wenqi Liang, Gan Sun, Yao He, Jiahua Dong, Suyan Dai, Ivan Laptev, Salman Khan, Yang Cong)</author>
      <guid isPermaLink="false">2511.01571v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach</title>
      <link>http://arxiv.org/abs/2511.00643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的手术动作三元组空间定位方法，通过器械实例分割来实现空间定位的&lt;器械、动词、目标&gt;输出，并构建了相应的大规模数据集和评估基准。&lt;h4&gt;背景&lt;/h4&gt;现有的手术动作三元组识别方法仅限于帧级分类学习，无法可靠地将动作与特定器械实例关联。之前的空间定位方法主要依赖类激活图，缺乏精确性和鲁棒性，无法满足详细的器械-组织交互分析需求。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在手术动作三元组空间定位上的局限性，提出一种能够将动作与特定器械实例空间关联的统一框架。&lt;h4&gt;方法&lt;/h4&gt;提出了'三元组分割'任务，构建了CholecTriplet-Seg大规模数据集（包含30,000+标注帧），并设计了TargetFusionNet架构，通过目标感知融合机制扩展Mask2Former，融合弱解剖先验与器械实例查询以提高解剖目标预测准确性。&lt;h4&gt;主要发现&lt;/h4&gt;TargetFusionNet在识别、检测和三元组分割指标上均优于现有基线，证明强实例监督与弱目标先验相结合能显著提高手术动作理解的准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;三元组分割为手术动作三元组的空间定位建立了统一框架，所提出的基准和架构为更可解释的手术场景理解铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;理解手术器械-组织交互不仅需要识别哪个器械在哪个解剖目标上执行什么动作，还需要将这些交互在手术场景中空间定位。现有的手术动作三元组识别方法仅限于从帧级分类学习，无法可靠地将动作与特定的器械实例关联起来。之前的空间定位尝试主要依赖于类激活图，但这些方法缺乏详细的器械-组织交互分析所需的精确性和鲁棒性。为解决这一差距，我们提出了通过器械实例分割来定位手术动作三元组，简称为三元组分割，这是一个新的统一任务，可以产生空间定位的&lt;器械、动词、目标&gt;输出。我们首先介绍了CholecTriplet-Seg数据集，这是一个包含超过30,000个标注帧的大规模数据集，将器械实例掩码与动作动词和解剖目标标注相关联，并建立了首个用于强监督、实例级三元组定位和评估的基准。为了学习三元组分割，我们提出了TargetFusionNet，这是一种新颖的架构，它通过目标感知融合机制扩展了Mask2Former，以解决通过将弱解剖先验与器械实例查询融合来准确预测解剖目标的挑战。在识别、检测和三元组分割指标上的评估表明，TargetFusionNet始终优于现有基线，证明强实例监督与弱目标先验相结合显著提高了手术动作理解的准确性和鲁棒性。三元组分割为空间定位手术动作三元组建立了统一框架。所提出的基准和架构为更可解释的手术场景理解铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding surgical instrument-tissue interactions requires not onlyidentifying which instrument performs which action on which anatomical target,but also grounding these interactions spatially within the surgical scene.Existing surgical action triplet recognition methods are limited to learningfrom frame-level classification, failing to reliably link actions to specificinstrument instances.Previous attempts at spatial grounding have primarilyrelied on class activation maps, which lack the precision and robustnessrequired for detailed instrument-tissue interaction analysis.To address thisgap, we propose grounding surgical action triplets with instrument instancesegmentation, or triplet segmentation for short, a new unified task whichproduces spatially grounded &lt;instrument, verb, target&gt; outputs.We start bypresenting CholecTriplet-Seg, a large-scale dataset containing over 30,000annotated frames, linking instrument instance masks with action verb andanatomical target annotations, and establishing the first benchmark forstrongly supervised, instance-level triplet grounding and evaluation.To learntriplet segmentation, we propose TargetFusionNet, a novel architecture thatextends Mask2Former with a target-aware fusion mechanism to address thechallenge of accurate anatomical target prediction by fusing weak anatomypriors with instrument instance queries.Evaluated across recognition,detection, and triplet segmentation metrics, TargetFusionNet consistentlyimproves performance over existing baselines, demonstrating that stronginstance supervision combined with weak target priors significantly enhancesthe accuracy and robustness of surgical action understanding.Tripletsegmentation establishes a unified framework for spatially grounding surgicalaction triplets. The proposed benchmark and architecture pave the way for moreinterpretable, surgical scene understanding.</description>
      <author>example@mail.com (Oluwatosin Alabi, Meng Wei, Charlie Budd, Tom Vercauteren, Miaojing Shi)</author>
      <guid isPermaLink="false">2511.00643v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid-Task Meta-Learning: A GNN Approach for Scalable and Transferable Bandwidth Allocation</title>
      <link>http://arxiv.org/abs/2401.10253v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于深度学习的带宽分配策略，具有可扩展性和可转移性特点。通过使用图神经网络和混合任务元学习算法，实现了在不同通信场景下的高效带宽分配。&lt;h4&gt;背景&lt;/h4&gt;随着用户数量和通信场景的多样化，传统的带宽分配方法面临可扩展性和泛化能力不足的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种既可随用户数量扩展，又能适应不同通信场景（如非平稳无线信道、不同服务质量要求和动态可用资源）的带宽分配策略。&lt;h4&gt;方法&lt;/h4&gt;1. 使用图神经网络(GNN)表示带宽分配策略，确保参数数量不随用户数量变化；2. 开发混合任务元学习(HML)算法，在元训练阶段使用不同通信场景训练GNN初始参数；3. 在元测试阶段使用少量样本对未见过的通信场景进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;1. HML方法比现有基准提高初始性能8.79%，样本效率提高73%；2. 微调后的GNN策略以更低推理复杂度获得接近最优策略的奖励；3. HML比最优迭代算法减少约200到2000倍的计算时间。&lt;h4&gt;结论&lt;/h4&gt;基于GNN和HML的带宽分配策略在性能、效率和计算复杂度方面均优于现有方法，为实际通信系统中的资源分配提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们开发了一种基于深度学习的带宽分配策略，该策略：1)随用户数量可扩展；2)可转移到不同的通信场景，如非平稳无线信道、不同的服务质量要求和动态可用资源。为了支持可扩展性，带宽分配策略由图神经网络(GNN)表示，其训练参数数量不随用户数量变化。为了实现GNN的泛化能力，我们开发了一种混合任务元学习(HML)算法，在元训练期间使用不同的通信场景训练GNN的初始参数。接下来，在元测试期间，使用少量样本对未见过的通信场景微调GNN。仿真结果表明，与现有基准相比，我们的HML方法可以提高初始性能8.79%，样本效率提高73%。微调后，我们的次优GNN策略与使用迭代优化获得的最优策略相比，可以以低得多的推理复杂度获得几乎相同的奖励。数值结果验证，与最优迭代算法相比，我们的HML可以减少约200到2000倍的计算时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we develop a deep learning-based bandwidth allocation policythat is: 1) scalable with the number of users and 2) transferable to differentcommunication scenarios, such as non-stationary wireless channels, differentquality-of-service (QoS) requirements, and dynamically available resources. Tosupport scalability, the bandwidth allocation policy is represented by a graphneural network (GNN), with which the number of training parameters does notchange with the number of users. To enable the generalization of the GNN, wedevelop a hybrid-task meta-learning (HML) algorithm that trains the initialparameters of the GNN with different communication scenarios duringmeta-training. Next, during meta-testing, a few samples are used to fine-tunethe GNN with unseen communication scenarios. Simulation results demonstratethat our HML approach can improve the initial performance by 8.79%, and sampleefficiency by 73%, compared with existing benchmarks. After fine-tuning, ournear-optimal GNN-based policy can achieve close to the same reward with muchlower inference complexity compared to the optimal policy obtained usingiterative optimization. Numerical results validate that our HML can reduce thecomputation time by approximately 200 to 2000 times than the optimal iterativealgorithm.</description>
      <author>example@mail.com (Xin Hao, Changyang She, Phee Lep Yeoh, Yuhong Liu, Branka Vucetic, Yonghui Li)</author>
      <guid isPermaLink="false">2401.10253v3</guid>
      <pubDate>Tue, 04 Nov 2025 14:08:49 +0800</pubDate>
    </item>
  <item>
      <title>Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</title>
      <link>http://arxiv.org/abs/2510.25760v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文对大型模型在多模态空间推理任务方面进行了全面综述，分类了多模态大型语言模型的最新进展，并介绍了开放基准进行评估。&lt;h4&gt;背景&lt;/h4&gt;人类具有通过视觉和声音等多模态观察理解空间的空间推理能力。大型多模态推理模型通过学习感知和推理，在多样化的空间任务中展现出有前景的性能，但系统综述和公开可用的基准仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提供对大型模型多模态空间推理任务的全面回顾，分类多模态大型语言模型(MLLMs)的最新进展，并介绍用于评估的开放基准。&lt;h4&gt;方法&lt;/h4&gt;从概述一般空间推理开始，重点关注训练后技术、可解释性和架构。研究空间关系推理、场景和布局理解、3D空间中的视觉问答和定位。回顾具身AI进展，包括视觉语言导航和动作模型。考虑音频和第一人称视频等新兴模态。&lt;h4&gt;主要发现&lt;/h4&gt;多模态大型模型在空间推理任务中展现出有前景的性能，但仍需要更多系统研究和公开基准来评估这些模型的能力。&lt;h4&gt;结论&lt;/h4&gt;这篇综述为多模态空间推理这一不断发展的领域奠定了坚实基础并提供了见解。相关更新信息、代码和开放基准的实现可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;人类具有空间推理能力，使他们能够通过视觉和声音等多模态观察理解空间。大型多模态推理模型通过学习感知和推理扩展了这些能力，在多样化的空间任务中展现出有前景的性能。然而，这些模型的系统综述和公开可用的基准仍然有限。在这篇综述中，我们提供了对大型模型多模态空间推理任务的全面回顾，分类了多模态大型语言模型(MLLMs)的最新进展，并介绍了用于评估的开放基准。我们首先概述一般空间推理，重点关注训练后技术、可解释性和架构。除了传统的2D任务外，我们还研究了空间关系推理、场景和布局理解，以及3D空间中的视觉问答和定位。我们还回顾了具身AI的进展，包括视觉语言导航和动作模型。此外，我们还考虑了音频和第一人称视频等新兴模态，这些模态通过新型传感器促进新的空间理解。我们相信这篇综述为多模态空间推理这一不断发展的领域奠定了坚实基础并提供了见解。关于这篇综述的更新信息、代码和开放基准的实现可在https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning上找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态空间推理领域缺乏系统性回顾和公开可用基准的问题。这个问题很重要，因为人类具有通过视觉、声音等多模态感知理解空间的能力，而大型多模态模型虽已展现出色性能，但缺乏系统评估和比较标准，阻碍了该领域的快速发展。空间推理对导航、物体关系理解和复杂场景交互等实际应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过构建分类框架（如图2所示）组织多模态空间推理研究，从一般多模态空间推理到3D空间推理，再到具身AI和新兴模态。作者借鉴了多模态模型、空间推理和具身AI等领域的现有工作，同时发现前人研究存在空白，如Wang等人专注于单模态任务，Ke等人未深入多模态空间推理，Bi等人未提供系统评估框架。作者通过系统性文献回顾和基准构建填补了这些空白。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过系统回顾和基准测试促进多模态空间推理研究发展。流程包括：1)明确定义多模态空间推理任务和评估协议；2)构建分类框架涵盖2D到3D、静态到动态、传统到新兴模态；3)全面回顾文献，包括后训练技术、可解释性、架构设计等；4)开发开放基准评估模型性能；5)提供代码和实现资源促进进一步研究。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提供多模态空间推理的系统性全面回顾；2)构建详细分类框架（图2）覆盖广泛任务；3)引入开放基准标准化评估；4)整合空间推理与具身AI；5)提供资源促进研究。相比前人工作，本文更全面系统，不仅涵盖2D到3D任务，还整合新兴模态和具身AI，并提供实用评估基准，而非仅关注单一领域或理论实现。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性的文献回顾、分类框架构建和开放基准引入，为多模态空间推理领域提供了坚实基础和评估标准，促进了该领域的研究发展和实际应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans possess spatial reasoning abilities that enable them to understandspaces through multimodal observations, such as vision and sound. Largemultimodal reasoning models extend these abilities by learning to perceive andreason, showing promising performance across diverse spatial tasks. However,systematic reviews and publicly available benchmarks for these models remainlimited. In this survey, we provide a comprehensive review of multimodalspatial reasoning tasks with large models, categorizing recent progress inmultimodal large language models (MLLMs) and introducing open benchmarks forevaluation. We begin by outlining general spatial reasoning, focusing onpost-training techniques, explainability, and architecture. Beyond classical 2Dtasks, we examine spatial relationship reasoning, scene and layoutunderstanding, as well as visual question answering and grounding in 3D space.We also review advances in embodied AI, including vision-language navigationand action models. Additionally, we consider emerging modalities such as audioand egocentric video, which contribute to novel spatial understanding throughnew sensors. We believe this survey establishes a solid foundation and offersinsights into the growing field of multimodal spatial reasoning. Updatedinformation about this survey, codes and implementation of the open benchmarkscan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.</description>
      <author>example@mail.com (Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu)</author>
      <guid isPermaLink="false">2510.25760v2</guid>
      <pubDate>Tue, 04 Nov 2025 14:08:49 +0800</pubDate>
    </item>
    <item>
      <title>Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models</title>
      <link>http://arxiv.org/abs/2510.27629v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 Pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为eval的框架，用于评估旨在减少生物基础模型双重使用能力的程序的鲁棒性。研究通过序列建模、突变效应预测和毒力预测三个角度评估模型对病毒的理解能力，发现当前的数据过滤方法可能不够有效，被排除的知识可以通过微调恢复，且双重使用信号可能已存在于预训练表示中。&lt;h4&gt;背景&lt;/h4&gt;开放权重生物基础模型呈现双重使用困境。这些模型有加速科学研究和药物开发的巨大潜力，但也可能被恶意行为者用于开发更致命的生物武器。当前的方法主要关注在预训练过程中过滤生物危害数据，但其有效性尚不明确。&lt;h4&gt;目的&lt;/h4&gt;解决当前过滤生物危害数据方法的有效性不明确问题，特别是针对可能微调这些模型进行恶意使用的有决心的行为者。提出一个框架来评估旨在减少生物基础模型双重使用能力的程序的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出了名为eval的框架，通过三个角度评估模型的病毒理解能力：序列建模、突变效应预测和毒力预测。&lt;h4&gt;主要发现&lt;/h4&gt;当前过滤实践可能不是特别有效；在某些情况下，被排除的知识可以通过微调快速恢复，并在序列建模中表现出更广泛的泛化能力；双重使用信号可能已经存在于预训练表示中，可以通过简单的线性探测来引出。&lt;h4&gt;结论&lt;/h4&gt;数据过滤作为独立程序面临挑战，强调需要对开放权重生物基础模型进行更深入的安全和安保策略研究。&lt;h4&gt;翻译&lt;/h4&gt;开放权重生物基础模型呈现双重使用困境。虽然这些模型在加速科学研究和药物开发方面展现出巨大潜力，但也可能被恶意行为者用于开发更致命的生物武器。为了减轻这些模型带来的风险，当前的方法主要关注在预训练过程中过滤生物危害数据。然而，这种方法的有效性仍不明确，特别是针对可能微调这些模型进行恶意使用的有决心的行为者。为了解决这一空白，我们提出了eval框架，用于评估旨在减少生物基础模型双重使用能力的程序的鲁棒性。eval通过序列建模、突变效应预测和毒力预测三个角度评估模型对病毒的理解能力。我们的结果表明，当前的过滤实践可能不是特别有效：在某些情况下，被排除的知识可以通过微调快速恢复，并在序列建模中表现出更广泛的泛化能力。此外，双重使用信号可能已经存在于预训练表示中，可以通过简单的线性探测来引出。这些发现强调了数据过滤作为独立程序所面临的挑战，突显了对开放权重生物基础模型进行更深入的安全和安保策略研究的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-weight bio-foundation models present a dual-use dilemma. While holdinggreat promise for accelerating scientific research and drug development, theycould also enable bad actors to develop more deadly bioweapons. To mitigate therisk posed by these models, current approaches focus on filtering biohazardousdata during pre-training. However, the effectiveness of such an approachremains unclear, particularly against determined actors who might fine-tunethese models for malicious use. To address this gap, we propose \eval, aframework to evaluate the robustness of procedures that are intended to reducethe dual-use capabilities of bio-foundation models. \eval assesses models'virus understanding through three lenses, including sequence modeling,mutational effects prediction, and virulence prediction. Our results show thatcurrent filtering practices may not be particularly effective: Excludedknowledge can be rapidly recovered in some cases via fine-tuning, and exhibitsbroader generalizability in sequence modeling. Furthermore, dual-use signalsmay already reside in the pretrained representations, and can be elicited viasimple linear probing. These findings highlight the challenges of datafiltering as a standalone procedure, underscoring the need for further researchinto robust safety and security strategies for open-weight bio-foundationmodels.</description>
      <author>example@mail.com (Boyi Wei, Zora Che, Nathaniel Li, Udari Madhushani Sehwag, Jasper Götting, Samira Nedungadi, Julian Michael, Summer Yue, Dan Hendrycks, Peter Henderson, Zifan Wang, Seth Donoughe, Mantas Mazeika)</author>
      <guid isPermaLink="false">2510.27629v2</guid>
      <pubDate>Tue, 04 Nov 2025 14:08:49 +0800</pubDate>
    </item>
    <item>
      <title>Image Hashing via Cross-View Code Alignment in the Age of Foundation Models</title>
      <link>http://arxiv.org/abs/2510.27584v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CroVCA（Cross-View Code Alignment），一种简单统一的原则，用于学习在不同语义对齐视图中保持一致的二进制码。通过HashCoder实现，该方法仅需5个训练周期就能达到最先进的结果，在16位时表现尤为出色，具有高效率、适应性和广泛适用性。&lt;h4&gt;背景&lt;/h4&gt;大规模检索需要紧凑且具有判别性的表示。基础模型提供强大的视觉和多模态嵌入，但这些高维空间中的最近邻搜索计算成本高。哈希提供了一种高效的替代方案，但现有方法通常依赖复杂流程、多目标项、针对单一学习范式设计以及长时间训练。&lt;h4&gt;目的&lt;/h4&gt;引入CroVCA，一种简单统一的原则，用于学习在不同语义对齐视图中保持一致的二进制码，实现高效、快速、准确的哈希方法。&lt;h4&gt;方法&lt;/h4&gt;使用单个二元交叉熵损失强制对齐，编码率最大化作为反崩溃正则化器促进平衡和多样化的码。设计了HashCoder，一个轻量级的MLP哈希网络，带有最终的批归一化层以强制平衡的码。HashCoder可用作冻结嵌入上的探测头，或通过LoRA微调有效地适配编码器。&lt;h4&gt;主要发现&lt;/h4&gt;CroVCA在基准测试中取得了最先进的结果，只需5个训练周期。在16位时表现特别好，例如在COCO上的无监督哈希在不到2分钟内完成，在ImageNet100上的监督哈希在单个GPU上约3分钟内完成。&lt;h4&gt;结论&lt;/h4&gt;CroVCA展示了其效率、适应性和广泛的适用性。&lt;h4&gt;翻译&lt;/h4&gt;高效的大规模检索需要既紧凑又具有判别性的表示。基础模型提供了强大的视觉和多模态嵌入，但这些高维空间中的最近邻搜索计算成本高昂。哈希通过使用二进制码实现快速汉明距离搜索，提供了一种高效的替代方案，然而现有方法通常依赖复杂流程、多目标项、针对单一学习范式的设计以及长时间的训练。我们引入了CroVCA（Cross-View Code Alignment），这是一种学习在不同语义对齐视图中保持一致的二进制码的简单统一原则。单个二元交叉熵损失强制对齐，而编码率最大化作为反崩溃正则化器促进平衡和多样化的码。为此，我们设计了HashCoder，一个带有最终批归一化层以强制平衡码的轻量级MLP哈希网络。HashCoder可以用作冻结嵌入上的探测头，或通过LoRA微调有效地适配编码器。在基准测试中，CroVCA仅需5个训练周期就取得了最先进的结果。在16位时，它表现特别好——例如，在单个GPU上，COCO上的无监督哈希在不到2分钟内完成，ImageNet100上的监督哈希在约3分钟内完成。这些结果凸显了CroVCA的效率、适应性和广泛的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient large-scale retrieval requires representations that are bothcompact and discriminative. Foundation models provide powerful visual andmultimodal embeddings, but nearest neighbor search in these high-dimensionalspaces is computationally expensive. Hashing offers an efficient alternative byenabling fast Hamming distance search with binary codes, yet existingapproaches often rely on complex pipelines, multi-term objectives, designsspecialized for a single learning paradigm, and long training times. Weintroduce CroVCA (Cross-View Code Alignment), a simple and unified principlefor learning binary codes that remain consistent across semantically alignedviews. A single binary cross-entropy loss enforces alignment, while coding-ratemaximization serves as an anti-collapse regularizer to promote balanced anddiverse codes. To implement this, we design HashCoder, a lightweight MLPhashing network with a final batch normalization layer to enforce balancedcodes. HashCoder can be used as a probing head on frozen embeddings or to adaptencoders efficiently via LoRA fine-tuning. Across benchmarks, CroVCA achievesstate-of-the-art results in just 5 training epochs. At 16 bits, it particularlywell-for instance, unsupervised hashing on COCO completes in under 2 minutesand supervised hashing on ImageNet100 in about 3 minutes on a single GPU. Theseresults highlight CroVCA's efficiency, adaptability, and broad applicability.</description>
      <author>example@mail.com (Ilyass Moummad, Kawtar Zaher, Hervé Goëau, Alexis Joly)</author>
      <guid isPermaLink="false">2510.27584v2</guid>
      <pubDate>Tue, 04 Nov 2025 14:08:49 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Genomics into Multimodal EHR Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23639v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种创新的电子健康记录（EHR）基础模型，整合多基因风险评分（PRS）作为基础数据模态，超越了传统EHR-only方法，构建更全面的健康档案。该多模态框架利用'全民研究计划'的多样化数据，学习临床数据和遗传易感性间的复杂关系，增强预测能力和可解释性。&lt;h4&gt;背景&lt;/h4&gt;传统电子健康记录方法仅使用临床数据，未充分利用遗传信息，需要整合多种数据类型构建更全面的健康档案。&lt;h4&gt;目的&lt;/h4&gt;开发创新的EHR基础模型，整合PRS作为基础数据模态，构建更全面的健康档案，学习临床数据和遗传易感性间的复杂关系，增强预测能力和可解释性。&lt;h4&gt;方法&lt;/h4&gt;利用'全民研究计划'的广泛多样化数据，开发多模态框架整合PRS和EHR数据，将生成式人工智能进展应用于EHR基础模型领域，使用迁移学习进行定制分类任务，并在AoU数据上评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;模型对多种疾病（特别是2型糖尿病）的发作具有预测价值；展示了PRS与EHR数据间的相互作用；架构具有多样性和效率，适用于迁移学习；能解锁疾病预测、主动健康管理、风险分层和个性化治疗策略的新见解。&lt;h4&gt;结论&lt;/h4&gt;这种整合PRS的EHR基础模型方法超越了传统方法，为医疗保健中更个性化、公平和可行的真实世界证据生成奠定基础，对疾病预测和个性化治疗具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了一种创新的电子健康记录（EHR）基础模型，该模型将多基因风险评分（PRS）整合为基础数据模态，超越了传统的仅基于EHR的方法，以构建更全面的健康档案。利用'全民研究计划'（AoU）的广泛多样化数据，这种多模态框架旨在学习临床数据和遗传易感性之间的复杂关系。该方法将生成式人工智能的进展扩展到EHR基础模型领域，增强了预测能力和可解释性。在AoU数据上的评估展示了该模型对多种疾病（特别是2型糖尿病）发作的预测价值，并说明了PRS与EHR数据之间的相互作用。该研究还探讨了针对定制分类任务的迁移学习，展示了架构的多样性和效率。这种方法对于解锁疾病预测、主动健康管理、风险分层和个性化治疗策略的新见解至关重要，为医疗保健中更个性化、公平和可行的真实世界证据生成奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces an innovative Electronic Health Record (EHR) foundationmodel that integrates Polygenic Risk Scores (PRS) as a foundational datamodality, moving beyond traditional EHR-only approaches to build more holistichealth profiles. Leveraging the extensive and diverse data from the All of Us(AoU) Research Program, this multimodal framework aims to learn complexrelationships between clinical data and genetic predispositions. Themethodology extends advancements in generative AI to the EHR foundation modelspace, enhancing predictive capabilities and interpretability. Evaluation onAoU data demonstrates the model's predictive value for the onset of variousconditions, particularly Type 2 Diabetes (T2D), and illustrates the interplaybetween PRS and EHR data. The work also explores transfer learning for customclassification tasks, showcasing the architecture's versatility and efficiency.This approach is pivotal for unlocking new insights into disease prediction,proactive health management, risk stratification, and personalized treatmentstrategies, laying the groundwork for more personalized, equitable, andactionable real-world evidence generation in healthcare.</description>
      <author>example@mail.com (Jonathan Amar, Edward Liu, Alessandra Breschi, Liangliang Zhang, Pouya Kheradpour, Sylvia Li, Lisa Soleymani Lehmann, Alessandro Giulianelli, Matt Edwards, Yugang Jia, David Nola, Raghav Mani, Pankaj Vats, Jesse Tetreault, T. J. Chen, Cory Y. McLean)</author>
      <guid isPermaLink="false">2510.23639v2</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
  <item>
      <title>Validity Is What You Need</title>
      <link>http://arxiv.org/abs/2510.27628v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文讨论了Agentic AI的定义、特性和验证方法，提出Agentic AI是一种软件交付机制，类似于SaaS，使应用程序能够在复杂企业环境中自主工作。研究指出Agentic AI系统主要是应用程序而非基础模型，其成功依赖于用户验证，并强调在良好验证措施下，可用更简单模型替代复杂基础模型。&lt;h4&gt;背景&lt;/h4&gt;AI代理在计算机科学领域早已被讨论和研究，但当前的Agentic AI系统是新的发展。大型语言模型(LLMs)等基础模型的进步推动了Agentic AI的发展。&lt;h4&gt;目的&lt;/h4&gt;考虑其他Agentic AI的定义并提出一个新的现实主义定义。&lt;h4&gt;方法&lt;/h4&gt;将Agentic AI定义为软件交付机制，类似于软件即服务(SaaS)，使应用程序能够在复杂的企业环境中自主工作。&lt;h4&gt;主要发现&lt;/h4&gt;Agentic AI系统主要是应用程序而非基础模型，其成功依赖于最终用户和主要利益相关者的验证；验证应用程序的工具与技术评估基础模型的技术不同；在有良好验证措施的情况下，基础模型通常可以用更简单、更快、更可解释的模型替代。&lt;h4&gt;结论&lt;/h4&gt;对于Agentic AI，有效性是关键需求，LLMs是可能实现这一目标的一种选择。&lt;h4&gt;翻译&lt;/h4&gt;虽然AI代理在计算机科学领域早已被讨论和研究，但今天的Agentic AI系统是全新的。我们考虑了其他Agentic AI的定义，并提出了一个新的现实主义定义。Agentic AI是一种软件交付机制，类似于软件即服务(SaaS)，它使应用程序能够在复杂的企业环境中自主工作。大型语言模型(LLMs)等基础模型的最新进展推动了Agentic AI的发展。然而，我们注意到Agentic AI系统主要是应用程序，而非基础模型，因此其成功依赖于最终用户和主要利益相关者的验证。主要用户验证其应用程序所需的工具与技术用于评估基础模型的技术有很大不同。讽刺的是，在有良好验证措施的情况下，在许多情况下基础模型可以用更简单、更快、更可解释的模型来替代，这些模型处理核心逻辑。当涉及到Agentic AI时，有效性是您所需要的。LLMs是实现这一目标的一种选择。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While AI agents have long been discussed and studied in computer science,today's Agentic AI systems are something new. We consider other definitions ofAgentic AI and propose a new realist definition. Agentic AI is a softwaredelivery mechanism, comparable to software as a service (SaaS), which puts anapplication to work autonomously in a complex enterprise setting. Recentadvances in large language models (LLMs) as foundation models have drivenexcitement in Agentic AI. We note, however, that Agentic AI systems areprimarily applications, not foundations, and so their success depends onvalidation by end users and principal stakeholders. The tools and techniquesneeded by the principal users to validate their applications are quitedifferent from the tools and techniques used to evaluate foundation models.Ironically, with good validation measures in place, in many cases thefoundation models can be replaced with much simpler, faster, and moreinterpretable models that handle core logic. When it comes to Agentic AI,validity is what you need. LLMs are one option that might achieve it.</description>
      <author>example@mail.com (Sebastian Benthall, Andrew Clark)</author>
      <guid isPermaLink="false">2510.27628v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Image Hashing via Cross-View Code Alignment in the Age of Foundation Models</title>
      <link>http://arxiv.org/abs/2510.27584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了CroVCA（Cross-View Code Alignment）方法，一种简单统一的原则，用于学习在语义对齐视图中保持一致的二进制码，用于高效的大规模检索。&lt;h4&gt;背景&lt;/h4&gt;高效的大规模检索需要既紧凑又有区分度的表示。基础模型提供强大的视觉和多模态嵌入，但这些高维空间中的最近邻搜索计算成本高。现有哈希方法通常依赖复杂的流程、多目标函数、专为单一学习范式设计且训练时间长。&lt;h4&gt;目的&lt;/h4&gt;开发一种简单、高效且适应性强的方法来学习二进制码，用于快速的大规模检索，同时减少训练时间和计算资源需求。&lt;h4&gt;方法&lt;/h4&gt;引入CroVCA原则，使用单个二元交叉熵损失强制语义对齐视图之间的码对齐，并采用编码率最大化作为防坍塌正则化子。设计了HashCoder，一种轻量级的MLP哈希网络，带有最终的批量归一化层以强制平衡的码。HashCoder可作为冻结嵌入上的探测头或通过LoRA微调适应编码器。&lt;h4&gt;主要发现&lt;/h4&gt;CroVCA在基准测试中仅需5个训练周期就取得了最先进的结果。在16位时表现特别优异，例如在COCO上的无监督哈希在单GPU上不到2分钟完成，在ImageNet100上的监督哈希约3分钟完成。&lt;h4&gt;结论&lt;/h4&gt;CroVCA提供了一种高效、适应性强的哈希方法，具有广泛的适用性，显著减少了训练时间和计算资源需求，同时保持了高质量的检索性能。&lt;h4&gt;翻译&lt;/h4&gt;高效的大规模检索需要既紧凑又有区分度的表示。基础模型提供强大的视觉和多模态嵌入，但这些高维空间中的最近邻搜索计算成本高。哈希通过使用二进制码实现快速的汉明距离搜索，提供了一种高效的替代方案。然而，现有方法通常依赖复杂的流程、多目标函数、专为单一学习范式设计且训练时间长。我们介绍了CroVCA（Cross-View Code Alignment），一种简单统一的原则，用于学习在语义对齐视图中保持一致的二进制码。单个二元交叉熵损失强制对齐，而编码率最大化作为防坍塌正则化子促进平衡和多样化的码。为此，我们设计了HashCoder，一种轻量级的MLP哈希网络，带有最终的批量归一化层以强制平衡的码。HashCoder可用作冻结嵌入上的探测头，或通过LoRA微调高效适应编码器。在基准测试中，CroVCA仅需5个训练周期就取得了最先进的结果。在16位时，性能特别好，例如在COCO上的无监督哈希在单GPU上不到2分钟完成，在ImageNet100上的监督哈希约3分钟完成。这些结果突显了CroVCA的高效性、适应性和广泛适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient large-scale retrieval requires representations that are bothcompact and discriminative. Foundation models provide powerful visual andmultimodal embeddings, but nearest neighbor search in these high-dimensionalspaces is computationally expensive. Hashing offers an efficient alternative byenabling fast Hamming distance search with binary codes, yet existingapproaches often rely on complex pipelines, multi-term objectives, designsspecialized for a single learning paradigm, and long training times. Weintroduce CroVCA (Cross-View Code Alignment), a simple and unified principlefor learning binary codes that remain consistent across semantically alignedviews. A single binary cross-entropy loss enforces alignment, while coding-ratemaximization serves as an anti-collapse regularizer to promote balanced anddiverse codes. To implement this, we design HashCoder, a lightweight MLPhashing network with a final batch normalization layer to enforce balancedcodes. HashCoder can be used as a probing head on frozen embeddings or to adaptencoders efficiently via LoRA fine-tuning. Across benchmarks, CroVCA achievesstate-of-the-art results in just 5 training epochs. At 16 bits, it particularlywell-for instance, unsupervised hashing on COCO completes in under 2 minutesand supervised hashing on ImageNet100 in about 3 minutes on a single GPU. Theseresults highlight CroVCA's efficiency, adaptability, and broad applicability.</description>
      <author>example@mail.com (Ilyass Moummad, Kawtar Zaher, Hervé Goëau, Alexis Joly)</author>
      <guid isPermaLink="false">2510.27584v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs</title>
      <link>http://arxiv.org/abs/2510.27558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种无需领域特定训练的机器人操作框架，该框架利用预训练基础模型进行机器人操作，通过集成现成模型、多模态感知和通用推理模型实现稳健任务排序，并通过动态场景图提供空间感知和环境一致性推理能力。&lt;h4&gt;背景&lt;/h4&gt;机器人操作领域通常需要领域特定的训练，而预训练基础模型在多个领域已显示出强大能力，但如何有效利用这些模型进行机器人操作尚不明确。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需领域特定训练的机器人操作框架，有效利用预训练基础模型进行机器人操作任务。&lt;h4&gt;方法&lt;/h4&gt;集成现成的预训练基础模型，结合多模态感知和通用推理模型，动态维护场景图提供空间感知，通过场景图实现环境一致性推理，并在桌面机器人操作实验中评估框架性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过一系列桌面机器人操作实验评估，该框架展示了在现成基础模型之上直接构建机器人操作系统的潜力。&lt;h4&gt;结论&lt;/h4&gt;该框架证明了利用预训练基础模型进行机器人操作的可行性，无需领域特定训练，为构建机器人操作系统提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一个框架，该框架利用预训练的基础模型进行机器人操作，无需领域特定的训练。该框架集成现成的模型，结合来自基础模型的多模态感知和能够进行稳健任务排序的通用推理模型。在框架内动态维护的场景图提供了空间感知能力，并使对环境的一致推理成为可能。通过一系列桌面机器人操作实验对该框架进行了评估，结果凸显了在现成基础模型之上直接构建机器人操作系统的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a framework that leverages pre-trained foundation modelsfor robotic manipulation without domain-specific training. The frameworkintegrates off-the-shelf models, combining multimodal perception fromfoundation models with a general-purpose reasoning model capable of robust tasksequencing. Scene graphs, dynamically maintained within the framework, providespatial awareness and enable consistent reasoning about the environment. Theframework is evaluated through a series of tabletop robotic manipulationexperiments, and the results highlight its potential for building roboticmanipulation systems directly on top of off-the-shelf foundation models.</description>
      <author>example@mail.com (Sushil Samuel Dinesh, Shinkyu Park)</author>
      <guid isPermaLink="false">2510.27558v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>MapSAM2: Adapting SAM2 for Automatic Segmentation of Historical Map Images and Time Series</title>
      <link>http://arxiv.org/abs/2510.27547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MapSAM2是一个基于视觉基础模型的统一框架，用于自动分割历史地图图像和时间序列，通过将图像和时间序列视为视频处理，有效解决了历史地图分析中的风格多样性和数据稀缺性问题。&lt;h4&gt;背景&lt;/h4&gt;历史地图是记录不同时间段地理特征的有价值档案，但由于其风格多样性和标注训练数据稀缺，自动分析面临挑战；从历史地图时间序列构建链接时空数据集更为耗时耗力，需要综合多张地图信息。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一框架自动分割历史地图图像和时间序列，支持建筑物年代测定、道路网络和聚落发展分析、环境变化研究等多种应用。&lt;h4&gt;方法&lt;/h4&gt;MapSAM2基于视觉基础模型，使用少样本微调适应不同分割任务；关键创新是将历史地图图像和时间序列都视为视频处理，对于图像将图块组作为视频处理使内存注意力机制整合上下文线索，对于时间序列引入Siegfried建筑时间序列数据集并提出通过模拟时间变换从单年地图生成伪时间序列以减少标注成本。&lt;h4&gt;主要发现&lt;/h4&gt;MapSAM2能有效学习时间关联，在有限监督或使用伪视频的情况下可准确分割和时间序列中的建筑物，提高了几何准确性，特别是对于区域特征。&lt;h4&gt;结论&lt;/h4&gt;MapSAM2框架成功解决了历史地图图像和时间序列的自动分割问题，作者将发布数据集和代码以支持未来研究。&lt;h4&gt;翻译&lt;/h4&gt;历史地图是记录不同时间段地理特征独特且有价值的档案。然而，由于其风格多样性和标注训练数据的稀缺性，历史地图图像的自动分析仍然是一个重大挑战。从历史地图时间序列构建链接时空数据集更加耗时耗力，因为它需要综合多张地图的信息。这类数据集对于建筑物年代测定、道路网络和聚落发展分析、环境变化研究等应用至关重要。我们提出了MapSAM2，一个用于自动分割历史地图图像和时间序列的统一框架。MapSAM2基于视觉基础模型，使用少样本微调适应不同的分割任务。我们的关键创新是将历史地图图像和时间序列都视为视频处理。对于图像，我们将一组图块作为视频处理，使内存注意力机制能够整合来自相似图块的上下文线索，从而提高了几何准确性，特别是对于区域特征。对于时间序列，我们引入了标注的Siegfried建筑时间序列数据集，并为了减少标注成本，提出了通过模拟常见的时间变换从单年地图生成伪时间序列。实验结果表明，MapSAM2能有效学习时间关联，并在有限监督或使用伪视频的情况下准确分割和时间序列中的建筑物。我们将发布我们的数据集和代码以支持未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Historical maps are unique and valuable archives that document geographicfeatures across different time periods. However, automated analysis ofhistorical map images remains a significant challenge due to their widestylistic variability and the scarcity of annotated training data. Constructinglinked spatio-temporal datasets from historical map time series is even moretime-consuming and labor-intensive, as it requires synthesizing informationfrom multiple maps. Such datasets are essential for applications such as datingbuildings, analyzing the development of road networks and settlements, studyingenvironmental changes etc. We present MapSAM2, a unified framework forautomatically segmenting both historical map images and time series. Built on avisual foundation model, MapSAM2 adapts to diverse segmentation tasks withfew-shot fine-tuning. Our key innovation is to treat both historical map imagesand time series as videos. For images, we process a set of tiles as a video,enabling the memory attention mechanism to incorporate contextual cues fromsimilar tiles, leading to improved geometric accuracy, particularly for arealfeatures. For time series, we introduce the annotated Siegfried Building TimeSeries Dataset and, to reduce annotation costs, propose generating pseudo timeseries from single-year maps by simulating common temporal transformations.Experimental results show that MapSAM2 learns temporal associations effectivelyand can accurately segment and link buildings in time series under limitedsupervision or using pseudo videos. We will release both our dataset and codeto support future research.</description>
      <author>example@mail.com (Xue Xia, Randall Balestriero, Tao Zhang, Yixin Zhou, Andrew Ding, Dev Saini, Lorenz Hurni)</author>
      <guid isPermaLink="false">2510.27547v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Generic Time Series Foundation Models for EEG Classification</title>
      <link>http://arxiv.org/abs/2510.27522v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作探索了时间序列基础模型在脑电图(EEG)任务中的应用潜力，发现即使是在非神经数据或合成信号上预训练的通用模型也能有效地迁移到EEG任务，并且性能超过特定于EEG的模型。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型正在成为强大的通用主干网络，但这些模型在特定领域生物医学信号（如脑电图EEG）方面的应用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究最近提出的时间序列分类基础模型在不同EEG任务上的适用性，包括运动想象分类和睡眠阶段预测。&lt;h4&gt;方法&lt;/h4&gt;测试两种预训练方案：(a)在来自多个领域的异构真实世界时间序列上进行预训练，(b)在纯合成数据上进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;两种变体都表现出强大的性能，一致优于广泛使用的卷积基线EEGNet和最新的特定于EEG的基础模型CBraMod，表明通用时间序列基础模型能有效迁移到EEG任务。&lt;h4&gt;结论&lt;/h4&gt;利用跨领域预训练模型进行脑信号分析具有前景，EEG可能会从更广泛的时间序列文献的进步中受益。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型正在作为强大的通用主干网络出现，然而它们在特定领域生物医学信号（如脑电图EEG）方面的潜力仍然相当未被探索。在这项工作中，我们研究了一种最近提出的时间序列分类基础模型在不同EEG任务（如运动想象分类和睡眠阶段预测）上的适用性。我们测试了两种预训练方案：(a)在来自多个领域的异构真实世界时间序列上预训练，(b)在纯合成数据上预训练。我们发现两种变体都表现出强大的性能，一致优于广泛使用的卷积基线EEGNet和最新的特定于EEG的基础模型CBraMod。这些结果表明，通用时间序列基础模型，即使是在非神经起源数据或合成信号上预训练的，也能有效地迁移到EEG。我们的发现强调了利用跨领域预训练模型进行脑信号分析的前景，表明EEG可能会从更广泛的时间序列文献的进步中受益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for time series are emerging as powerful general-purposebackbones, yet their potential for domain-specific biomedical signals such aselectroencephalography (EEG) remains rather unexplored. In this work, weinvestigate the applicability a recently proposed time series classificationfoundation model, to a different EEG tasks such as motor imagery classificationand sleep stage prediction. We test two pretraining regimes: (a) pretraining onheterogeneous real-world time series from multiple domains, and (b) pretrainingon purely synthetic data. We find that both variants yield strong performance,consistently outperforming EEGNet, a widely used convolutional baseline, andCBraMod, the most recent EEG-specific foundation model. These results suggestthat generalist time series foundation models, even when pretrained on data ofnon-neural origin or on synthetic signals, can transfer effectively to EEG. Ourfindings highlight the promise of leveraging cross-domain pretrained models forbrain signal analysis, suggesting that EEG may benefit from advances in thebroader time series literature.</description>
      <author>example@mail.com (Théo Gnassounou, Yessin Moakher, Shifeng Xie, Vasilii Feofanov, Ievgen Redko)</author>
      <guid isPermaLink="false">2510.27522v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Semantic Collapse in Partially Relevant Video Retrieval</title>
      <link>http://arxiv.org/abs/2510.27432v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accpeted to NeurIPS 2025. Code is available at  https://github.com/admins97/MSC_PRVR&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种解决部分相关视频检索中语义崩塌问题的框架，通过文本相关性保持学习和跨分支视频对齐方法，显著提高了检索准确性。&lt;h4&gt;背景&lt;/h4&gt;部分相关视频检索旨在检索与文本查询部分内容匹配的视频。现有方法将每个标注的文本-视频对视为正例，其他视为负例，忽略了单个视频内部和不同视频之间的丰富语义变化，导致同一视频中不同事件的嵌入空间压缩在一起，而不同视频中语义相似的查询和片段被分开。&lt;h4&gt;目的&lt;/h4&gt;解决文本和视频嵌入空间中的语义崩塌问题，提高部分相关视频检索的准确性，特别是在处理包含多个不同事件的视频时。&lt;h4&gt;方法&lt;/h4&gt;1) 文本相关性保持学习，保留基础模型编码的文本查询之间的语义关系；2) 跨分支视频对齐，一种对比对齐方法，解耦时间尺度上的分层视频表示；3) 保留顺序的令牌合并和自适应CBVA，生成内部连贯且相互区别的视频片段以增强对齐效果。&lt;h4&gt;主要发现&lt;/h4&gt;提出的框架有效防止了语义崩塌，并在PRVR基准测试上显著提高了检索准确性。&lt;h4&gt;结论&lt;/h4&gt;通过解决语义崩塌问题，该研究改进了部分相关视频检索的性能，特别是在处理包含多个不同事件的视频时。&lt;h4&gt;翻译&lt;/h4&gt;部分相关视频检索(PRVPR)旨在检索与文本查询部分内容匹配的视频。现有方法将每个标注的文本-视频对视为正例，其他视为负例，忽略了单个视频内部和不同视频之间的丰富语义变化。因此，同一视频中不同事件的查询及其对应的视频片段段的嵌入空间压缩在一起，而不同视频中语义相似的查询和片段的嵌入空间被分开。这限制了视频包含多个、多样事件时的检索性能。本文解决了上述问题，称为文本和视频嵌入空间中的语义崩塌。我们首先引入文本相关性保持学习，保留基础模型编码的文本查询之间的语义关系。为解决视频嵌入中的崩塌问题，我们提出了跨分支视频对齐(CBVA)，一种对比对齐方法，解耦时间尺度上的分层视频表示。随后，我们引入保留顺序的令牌合并和自适应CBVA，通过生成内部连贯且相互区别的视频片段来增强对齐效果。在PRVR基准测试上的大量实验表明，我们的框架有效防止了语义崩塌并显著提高了检索准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Partially Relevant Video Retrieval (PRVR) seeks videos where only part of thecontent matches a text query. Existing methods treat every annotated text-videopair as a positive and all others as negatives, ignoring the rich semanticvariation both within a single video and across different videos. Consequently,embeddings of both queries and their corresponding video-clip segments fordistinct events within the same video collapse together, while embeddings ofsemantically similar queries and segments from different videos are drivenapart. This limits retrieval performance when videos contain multiple, diverseevents. This paper addresses the aforementioned problems, termed as semanticcollapse, in both the text and video embedding spaces. We first introduce TextCorrelation Preservation Learning, which preserves the semantic relationshipsencoded by the foundation model across text queries. To address collapse invideo embeddings, we propose Cross-Branch Video Alignment (CBVA), a contrastivealignment method that disentangles hierarchical video representations acrosstemporal scales. Subsequently, we introduce order-preserving token merging andadaptive CBVA to enhance alignment by producing video segments that areinternally coherent yet mutually distinctive. Extensive experiments on PRVRbenchmarks demonstrate that our framework effectively prevents semanticcollapse and substantially improves retrieval accuracy.</description>
      <author>example@mail.com (WonJun Moon, MinSeok Jung, Gilhan Park, Tae-Young Kim, Cheol-Ho Cho, Woojin Jun, Jae-Pil Heo)</author>
      <guid isPermaLink="false">2510.27432v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>A Sensing Whole Brain Zebrafish Foundation Model for Neuron Dynamics and Behavior</title>
      <link>http://arxiv.org/abs/2510.27366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了一种名为SBM的稀疏注意力全脑基础模型，用于斑马鱼幼虫，能够基于感觉刺激预测神经元放电概率，并将大脑状态与行为联系起来。该模型支持对复杂神经现象的快速、基于行为的探索。&lt;h4&gt;背景&lt;/h4&gt;神经动力学支撑着从记忆到睡眠的各种行为，但识别高阶现象（如社交互动）的机制在实验上具有挑战性。现有的全脑模型往往无法扩展到单神经元分辨率，忽略了行为读出，或依赖于PCA/卷积管道，这些方法会错过长程、非线性相互作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够预测神经元放电概率并将大脑状态与行为联系起来的全脑模型，同时保持全脑规模和可解释性。&lt;h4&gt;方法&lt;/h4&gt;研究团队引入了一种稀疏注意力全脑基础模型（SBM），该模型在神经元和时间上分解注意力，从而实现全脑规模和可解释性。模型与一个排列不变的行为头相结合，能够基于梯度合成引发目标行为的神经模式。&lt;h4&gt;主要发现&lt;/h4&gt;在保留主体上，该模型实现了平均绝对误差小于0.02的校准预测和稳定的自回归滚动。通过排列不变的行为头，SBM能够基于梯度合成引发目标行为的神经模式。&lt;h4&gt;结论&lt;/h4&gt;该框架支持对复杂神经现象的快速、基于行为的探索，为研究高阶神经现象提供了新的工具。&lt;h4&gt;翻译&lt;/h4&gt;神经动力学支撑着从记忆到睡眠的各种行为，但识别高阶现象（如社交互动）的机制在实验上具有挑战性。现有的全脑模型往往无法扩展到单神经元分辨率，忽略行为读出，或依赖于PCA/卷积管道，这些方法会错过长程、非线性相互作用。我们引入了一种用于斑马鱼幼虫的稀疏注意力全脑基础模型（SBM），该模型基于感觉刺激预测神经元放电概率，并将大脑状态与行为联系起来。SBM在神经元和时间上分解注意力，从而实现全脑规模和可解释性。在保留主体上，它实现了平均绝对误差小于0.02的校准预测和稳定的自回归滚动。与一个排列不变的行为头相结合，SBM能够基于梯度合成引发目标行为的神经模式。该框架支持对复杂神经现象的快速、基于行为的探索。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural dynamics underlie behaviors from memory to sleep, yet identifyingmechanisms for higher-order phenomena (e.g., social interaction) isexperimentally challenging. Existing whole-brain models often fail to scale tosingle-neuron resolution, omit behavioral readouts, or rely on PCA/convpipelines that miss long-range, non-linear interactions. We introduce asparse-attention whole-brain foundation model (SBM) for larval zebrafish thatforecasts neuron spike probabilities conditioned on sensory stimuli and linksbrain state to behavior. SBM factorizes attention across neurons and alongtime, enabling whole-brain scale and interpretability. On a held-out subject,it achieves mean absolute error &lt;0.02 with calibrated predictions and stableautoregressive rollouts. Coupled to a permutation-invariant behavior head, SBMenables gradient-based synthesis of neural patterns that elicit targetbehaviors. This framework supports rapid, behavior-grounded exploration ofcomplex neural phenomena.</description>
      <author>example@mail.com (Sam Fatehmanesh Vegas, Matt Thomson, James Gornet, David Prober)</author>
      <guid isPermaLink="false">2510.27366v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Band Channel Impulse Response Prediction: Leveraging 3.5 GHz Channels for Upper Mid-Band</title>
      <link>http://arxiv.org/abs/2510.27349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures, 4 tables, this work has been submitted to IEEE  International Conference on Communications (ICC) 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CIR-UNext的深度学习框架，用于6G网络中的跨频带信道预测，解决了中高频段信道预测的计算复杂性和数据采集成本高的问题。&lt;h4&gt;背景&lt;/h4&gt;6G网络中，特别是在中高频段（FR3，7-24GHz），穿透损耗和阻塞问题严重。射线追踪方法虽然能提供高保真建模，但计算量大且高频数据采集成本高。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且可扩展的跨频带信道预测方法，利用低频段数据预测高频段信道特性。&lt;h4&gt;方法&lt;/h4&gt;提出CIR-UNext深度学习框架，利用丰富的3.5GHz信道脉冲响应（CIR）来预测7GHz的CIR。该框架结合基于射线追踪的数据集流程和注意力U-Net（AU-Net）变体进行增益和相位预测。&lt;h4&gt;主要发现&lt;/h4&gt;AU-Net-Aux模型在未见过的复杂环境中实现了0.58dB的中值增益误差和0.27rad的相位预测误差。扩展的Channel2ComMap基础模型在MIMO-OFDM系统中的吞吐量预测表现优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;CIR-UNext为跨频带预测提供了高效且可扩展的解决方案，可应用于6G网络中的定位、波束管理、数字孪生和智能资源分配等领域。&lt;h4&gt;翻译&lt;/h4&gt;精确的跨频带信道预测对6G网络至关重要，特别是在中高频段（FR3，7-24GHz），该频段的穿透损耗和阻塞问题严重。虽然射线追踪（RT）能提供高保真建模，但计算量大，且高频数据采集成本高。为应对这些挑战，我们提出了CIR-UNext，这是一个深度学习框架，旨在利用丰富的3.5GHz信道脉冲响应（CIR）来预测7GHz的CIR。该框架将基于RT的数据集流程与用于增益和相位预测的注意力U-Net（AU-Net）变体相结合。提出的AU-Net-Aux模型在未见过的复杂环境中实现了0.58dB的中值增益误差和0.27rad的相位预测误差。此外，我们将CIR-UNext扩展为一个基础模型Channel2ComMap，用于MIMO-OFDM系统中的吞吐量预测，显示出与现有方法相比的优越性能。总体而言，CIR-UNext为跨频带预测提供了高效且可扩展的解决方案，使定位、波束管理、数字孪生和智能资源分配等应用在6G网络中成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate cross-band channel prediction is essential for 6G networks,particularly in the upper mid-band (FR3, 7--24 GHz), where penetration loss andblockage are severe. Although ray tracing (RT) provides high-fidelity modeling,it remains computationally intensive, and high-frequency data acquisition iscostly. To address these challenges, we propose CIR-UNext, a deep learningframework designed to predict 7 GHz channel impulse responses (CIRs) byleveraging abundant 3.5 GHz CIRs. The framework integrates an RT-based datasetpipeline with attention U-Net (AU-Net) variants for gain and phase prediction.The proposed AU-Net-Aux model achieves a median gain error of 0.58 dB and aphase prediction error of 0.27 rad on unseen complex environments. Furthermore,we extend CIR-UNext into a foundation model, Channel2ComMap, for throughputprediction in MIMO-OFDM systems, demonstrating superior performance comparedwith existing approaches. Overall, CIR-UNext provides an efficient and scalablesolution for cross-band prediction, enabling applications such as localization,beam management, digital twins, and intelligent resource allocation in 6Gnetworks.</description>
      <author>example@mail.com (Fan-Hao Lin, Chi-Jui Sung, Chu-Hsiang Huang, Hui Chen, Chao-Kai Wen, Henk Wymeersch)</author>
      <guid isPermaLink="false">2510.27349v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing</title>
      <link>http://arxiv.org/abs/2510.27335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CIELR的新方法，通过将复杂用户指令转换为简单明确的编辑动作，避免了联合微调大型语言模型和扩散模型的高计算成本，实现了高效的复杂图像编辑。&lt;h4&gt;背景&lt;/h4&gt;现有图像编辑方法在处理简单编辑指令时表现良好，但处理复杂指令时通常需要联合微调大型语言模型(LLMs)和扩散模型(DMs)，这涉及极高的计算复杂性和训练成本。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在处理复杂图像编辑指令时的高计算成本问题，提出一种无需联合微调LLMs和DMs的新方法。&lt;h4&gt;方法&lt;/h4&gt;CIELR方法首先利用基础模型构建输入图像的结构化语义表示，然后引入迭代更新机制逐步细化这一表示，获得图像场景的细粒度视觉表示，从而能够执行复杂且灵活的图像编辑任务。&lt;h4&gt;主要发现&lt;/h4&gt;在SmartEdit Reasoning Scenario Set上的实验显示，该方法在PSNR指标上比之前的最先进方法高出9.955 dB，表明其在保持应保持一致区域方面的优越性。此外，作者还构建了包含86个图像样本的CIEBench基准，CIELR在该基准上也优于之前的方法。&lt;h4&gt;结论&lt;/h4&gt;CIELR方法成功避免了联合微调LLMs和DMs的需要，通过将复杂指令分解为简单动作，实现了高效且高质量的复杂图像编辑，代码和数据集已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;现有的图像编辑方法可以很好地处理简单的编辑指令。为了处理复杂的编辑指令，它们通常需要联合微调大型语言模型(LLMs)和扩散模型(DMs)，这涉及非常高的计算复杂性和训练成本。为了解决这个问题，我们提出了一种新方法，称为CIELR(Complex Image Editing via LLM Reasoning)，它将复杂的用户指令转换为一组简单明确的编辑动作，消除了联合微调大型语言模型和扩散模型的需要。具体来说，我们首先使用基础模型构建输入图像的结构化语义表示。然后，我们引入一个迭代更新机制，可以逐步细化这一表示，获得图像场景的细粒度视觉表示。这使我们能够执行复杂且灵活的图像编辑任务。在SmartEdit Reasoning Scenario Set上的大量实验表明，我们的方法在PSNR指标上比之前的最先进方法高出9.955 dB，表明其在保持应保持一致区域方面的优越性。由于公共数据集中复杂推理图像编辑的样本有限，我们构建了一个名为CIEBench的基准，包含86个图像样本，以及一个专门用于基于推理的图像编辑的指标。CIELR在这个基准上也优于之前的方法。代码和数据集可在https://github.com/Jia-shao/Reasoning-Editing获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing image editing methods can handle simple editing instructions verywell. To deal with complex editing instructions, they often need to jointlyfine-tune the large language models (LLMs) and diffusion models (DMs), whichinvolves very high computational complexity and training cost. To address thisissue, we propose a new method, called \textbf{C}omplex \textbf{I}mage\textbf{E}diting via \textbf{L}LM \textbf{R}easoning (CIELR), which converts acomplex user instruction into a set of simple and explicit editing actions,eliminating the need for jointly fine-tuning the large language models anddiffusion models. Specifically, we first construct a structured semanticrepresentation of the input image using foundation models. Then, we introducean iterative update mechanism that can progressively refine thisrepresentation, obtaining a fine-grained visual representation of the imagescene. This allows us to perform complex and flexible image editing tasks.Extensive experiments on the SmartEdit Reasoning Scenario Set show that ourmethod surpasses the previous state-of-the-art by 9.955 dB in PSNR, indicatingits superior preservation of regions that should remain consistent. Due to thelimited number of samples of public datasets of complex image editing withreasoning, we construct a benchmark named CIEBench, containing 86 imagesamples, together with a metric specifically for reasoning-based image editing.CIELR also outperforms previous methods on this benchmark. The code and datasetare available at\href{https://github.com/Jia-shao/Reasoning-Editing}{https://github.com/Jia-shao/Reasoning-Editing}.</description>
      <author>example@mail.com (Yijia Wang, Yiqing Shen, Weiming Chen, Zhihai He)</author>
      <guid isPermaLink="false">2510.27335v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Fusion of Heterogeneous Pathology Foundation Models for Whole Slide Image Analysis</title>
      <link>http://arxiv.org/abs/2510.27237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为FuseCPath的新型框架，用于融合异质性病理学基础模型，提升模型整体性能。该框架通过多视图聚类过滤代表性补丁、集群级重新嵌入策略融合补丁级模型，以及协作蒸馏策略融合幻灯片级模型，在多种癌症数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;全幻灯片图像分析已成为计算病理学中日益重要的技术。最近的病理学基础模型在从WSI中提取有意义的补丁级或幻灯片级特征方面显示出显著优势。然而，当前病理学基础模型由于多样化的私有训练数据集和不同网络架构表现出显著异质性，导致使用不同基础模型提取的特征进行下游任务时性能不稳定。&lt;h4&gt;目的&lt;/h4&gt;为了充分利用多种基础模型的优势，本研究旨在提出一种新型框架来融合异质性病理学基础模型，从而获得具有优越集成性能的模型。&lt;h4&gt;方法&lt;/h4&gt;FuseCPath框架包含三个主要贡献：(i) 基于多视图聚类的方法，通过多个基础模型的嵌入来过滤出具有代表性的训练补丁；(ii) 集群级重新嵌入策略，在线捕获补丁级局部特征，以有效融合异质性补丁级基础模型；(iii) 协作蒸馏策略，探索基础模型之间的连接，以有效融合异质性幻灯片级基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;在癌症基因组图谱的肺癌、膀胱癌和结直肠癌数据集上进行的大量实验表明，所提出的FuseCPath在这些公共数据集的多个任务上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;FuseCPath框架通过有效融合异质性病理学基础模型，解决了当前病理学基础模型存在的异质性问题，为全幻灯片图像分析提供了更强大的工具，在多种癌症数据集上取得了优异的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;全幻灯片图像分析已成为计算病理学中日益重要的技术。最近的病理学基础模型进展在从全幻灯片图像中提取有意义的补丁级或幻灯片级特征表示方面显示出显著优势。然而，当前的病理学基础模型由于多样化的私有训练数据集和不同的网络架构表现出显著的异质性。这种异质性在使用不同基础模型提取的特征进行下游任务时会导致性能变化。为了充分利用多种基础模型的优势，在本工作中，我们提出了一种用于融合异质性病理学基础模型的新型框架，称为FuseCPath，产生了一个具有优越集成性能的模型。我们框架的主要贡献可以总结如下：(i) 为了保证训练补丁的代表性，我们提出了一种基于多视图聚类的方法，通过多个基础模型的嵌入来过滤出判别性补丁；(ii) 为了有效融合异质性补丁级基础模型，我们设计了一种集群级重新嵌入策略，在线捕获补丁级局部特征；(iii) 为了有效融合异质性幻灯片级基础模型，我们设计了一种协作蒸馏策略，探索幻灯片级基础模型之间的连接。在癌症基因组图谱的肺癌、膀胱癌和结直肠癌数据集上进行的大量实验表明，所提出的FuseCPath在这些公共数据集的多个任务上实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Whole slide image (WSI) analysis has emerged as an increasingly essentialtechnique in computational pathology. Recent advances in the pathologicalfoundation models (FMs) have demonstrated significant advantages in derivingmeaningful patch-level or slide-level feature representations from WSIs.However, current pathological FMs have exhibited substantial heterogeneitycaused by diverse private training datasets and different networkarchitectures. This heterogeneity introduces performance variability when weutilize the extracted features from different FMs in the downstream tasks. Tofully explore the advantage of multiple FMs effectively, in this work, wepropose a novel framework for the fusion of heterogeneous pathological FMs,called FuseCPath, yielding a model with a superior ensemble performance. Themain contributions of our framework can be summarized as follows: (i) Toguarantee the representativeness of the training patches, we propose amulti-view clustering-based method to filter out the discriminative patches viamultiple FMs' embeddings. (ii) To effectively fuse the heterogeneouspatch-level FMs, we devise a cluster-level re-embedding strategy to onlinecapture patch-level local features. (iii) To effectively fuse the heterogeneousslide-level FMs, we devise a collaborative distillation strategy to explore theconnections between slide-level FMs. Extensive experiments conducted on lungcancer, bladder cancer, and colorectal cancer datasets from The Cancer GenomeAtlas (TCGA) have demonstrated that the proposed FuseCPath achievesstate-of-the-art performance across multiple tasks on these public datasets.</description>
      <author>example@mail.com (Zhidong Yang, Xiuhui Shi, Wei Ba, Zhigang Song, Haijing Luan, Taiyuan Hu, Senlin Lin, Jiguang Wang, Shaohua Kevin Zhou, Rui Yan)</author>
      <guid isPermaLink="false">2510.27237v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>MoRE: 3D Visual Geometry Reconstruction Meets Mixture-of-Experts</title>
      <link>http://arxiv.org/abs/2510.27234v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://g-1nonly.github.io/MoRE_Website/, Code:  https://github.com/alibaba/Taobao3D&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MoRE，一种基于专家混合(MoE)架构的密集3D视觉基础模型，通过动态路由特征到任务特定专家，解决了3D模型扩展的挑战，实现了在各种几何任务中的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;语言和视觉领域的最新进展表明增加模型容量可提高任务表现；3D视觉几何重建中大规模训练也被证明有效，但由于几何监督复杂性和3D数据多样性，进一步扩展3D模型面临挑战。&lt;h4&gt;目的&lt;/h4&gt;克服3D模型扩展的限制，提出一种提高可扩展性和适应性的密集3D视觉基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出MoRE模型，采用专家混合架构动态路由特征到任务特定专家；包含基于置信度的深度细化模块以稳定几何估计；集成密集语义特征与全局对齐的3D主干表示进行表面法线预测；使用定制的损失函数确保鲁棒学习。&lt;h4&gt;主要发现&lt;/h4&gt;MoRE在多个基准测试中实现最先进性能，支持有效的下游应用且无需额外计算。&lt;h4&gt;结论&lt;/h4&gt;MoRE通过专家混合架构解决了3D模型扩展挑战，提高了模型的可扩展性和适应性，并在真实世界条件下增强了鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;近期语言和视觉领域的进展表明，扩展模型容量可以持续提高各种任务的表现。在3D视觉几何重建中，大规模训练同样被证明对学习通用表示有效。然而，由于几何监督的复杂性和3D数据的多样性，进一步扩展3D模型具有挑战性。为克服这些限制，我们提出了MoRE，一种基于专家混合(MoE)架构的密集3D视觉基础模型，它动态地将特征路由到任务特定的专家，使它们能够专业化于互补的数据方面，提高可扩展性和适应性。为提高真实世界条件下的鲁棒性，MoRE包含一个基于置信度的深度细化模块，用于稳定和细化几何估计。此外，它将密集语义特征与全局对齐的3D主干表示集成，用于高保真表面法线预测。MoRE还使用定制的损失函数进行优化，确保对各种输入和多种几何任务的鲁棒学习。大量实验表明，MoRE在多个基准测试中实现了最先进的性能，并支持有效的下游应用，无需额外计算。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D视觉几何重建领域的可扩展性问题。传统方法依赖场景特定优化，缺乏灵活性，难以在AR/VR、游戏内容创建、机器人和自动驾驶等需要强大几何先验和跨场景一致性的现实应用中发挥作用。随着模型和数据规模的扩大，3D重建面临几何监督复杂性和数据多样性的挑战，限制了性能提升。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3D重建需要像语言和视觉领域那样的可扩展基础模型，但直接扩展3D模型面临特殊挑战。他们受混合专家模型(MoE)框架启发，该框架在大型语言模型中已证明能有效扩展神经网络。作者借鉴了VGGT作为基础架构，参考了DINOv2的特征提取技术，并受MoGe等单目几何估计模型的启发。他们设计了一个结合MoE的3D重建框架，通过动态路由特征到专家，实现不同场景的自适应表示学习。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将混合专家(MoE)框架引入3D视觉几何重建，创建一个密集的3D视觉基础模型，使模型能够动态路由特征到任务特定专家，学习不同场景的自适应和互补表示。整体流程包括：1)使用密集视觉Transformer骨干网络提取特征；2)实现MoE层，动态路由特征到专家；3)设计基于置信度的深度细化模块过滤噪声；4)融合全局3D特征与局部语义特征增强法线预测；5)使用多任务训练目标和自适应损失策略稳定训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将MoE框架系统应用于3D视觉几何重建；2)基于置信度的深度细化模块提高深度估计准确性；3)密集语义特征融合增强表面法线预测；4)多任务训练策略确保稳定学习。相比之前工作，MoRE通过MoE架构实现了更好的可扩展性和适应性，能处理各种3D场景，而传统方法通常针对特定场景优化；与其他前馈3D重建方法相比，MoRE能保持跨场景的高性能，并通过融合语义特征提高了细节表现。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MoRE通过将混合专家框架引入3D视觉几何重建，创建了一个可扩展且自适应的基础模型，能够通过动态路由特征到任务特定专家，实现高质量、鲁棒的3D几何预测，并融合语义特征提高表面法线估计的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in language and vision have demonstrated that scaling upmodel capacity consistently improves performance across diverse tasks. In 3Dvisual geometry reconstruction, large-scale training has likewise proveneffective for learning versatile representations. However, further scaling of3D models is challenging due to the complexity of geometric supervision and thediversity of 3D data. To overcome these limitations, we propose MoRE, a dense3D visual foundation model based on a Mixture-of-Experts (MoE) architecturethat dynamically routes features to task-specific experts, allowing them tospecialize in complementary data aspects and enhance both scalability andadaptability. Aiming to improve robustness under real-world conditions, MoREincorporates a confidence-based depth refinement module that stabilizes andrefines geometric estimation. In addition, it integrates dense semanticfeatures with globally aligned 3D backbone representations for high-fidelitysurface normal prediction. MoRE is further optimized with tailored lossfunctions to ensure robust learning across diverse inputs and multiplegeometric tasks. Extensive experiments demonstrate that MoRE achievesstate-of-the-art performance across multiple benchmarks and supports effectivedownstream applications without extra computation.</description>
      <author>example@mail.com (Jingnan Gao, Zhe Wang, Xianze Fang, Xingyu Ren, Zhuo Chen, Shengqi Liu, Yuhao Cheng, Jiangjing Lyu, Xiaokang Yang, Yichao Yan)</author>
      <guid isPermaLink="false">2510.27234v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>SpecAware: A Spectral-Content Aware Foundation Model for Unifying Multi-Sensor Learning in Hyperspectral Remote Sensing Mapping</title>
      <link>http://arxiv.org/abs/2510.27219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SpecAware，一种新型的高光谱光谱内容感知基础模型，用于统一多传感器学习以进行高光谱成像映射。作者还构建了Hyper-400K数据集，包含超过40万个图像块。SpecAware采用两步超网络驱动编码过程，能够感知和解释不同场景和传感器中的空间-光谱特征，实验证明其在多个任务上表现优异。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像(HSI)是精细土地利用和土地覆盖(LULC)制图的重要工具，但其数据的内在异质性一直是开发通用模型的主要障碍。现有HSI基础模型忽略了传感器元属性的关键指导作用，且难以进行多传感器训练，限制了可迁移性。&lt;h4&gt;目的&lt;/h4&gt;解决HSI基础模型中传感器元属性被忽略以及多传感器训练困难的问题，开发一个能够统一多传感器学习的高光谱成像映射基础模型。&lt;h4&gt;方法&lt;/h4&gt;1. 提出SpecAware模型；2. 构建Hyper-400K数据集；3. 设计两步超网络驱动编码过程：元内容感知模块融合传感器元属性和图像内容生成条件输入，超嵌入模块通过样本条件超网络动态生成矩阵因子对进行通道编码。&lt;h4&gt;主要发现&lt;/h4&gt;SpecAware能够感知和解释不同场景和传感器中的空间-光谱特征，自适应处理可变数量的光谱通道，建立统一的联合预训练框架。在六个数据集上的实验表明，SpecAware在土地覆盖语义分割分类、变化检测和场景分类方面表现优异。&lt;h4&gt;结论&lt;/h4&gt;SpecAware通过考虑传感器元属性和采用两步超网络驱动编码过程，成功解决了HSI基础模型在多传感器训练和可迁移性方面的挑战，为高光谱成像映射提供了统一且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像(HSI)是精细土地利用和土地覆盖(LULC)制图的重要工具。然而，HSI数据的内在异质性长期以来一直是通过联合训练开发通用模型的主要障碍。尽管HSI基础模型在不同下游任务中显示出潜力，但现有方法通常忽略了传感器元属性的关键指导作用，并且难以进行多传感器训练，限制了它们的可迁移性。为了解决这些挑战，我们提出了SpecAware，这是一种新型的高光谱光谱内容感知基础模型，用于统一多传感器学习以进行HSI映射。我们还构建了Hyper-400K数据集来促进这项研究，这是一个新的包含超过40万个来自不同机载AVIRIS传感器图像块的大规模高质量基准数据集。SpecAware的核心是一个两步超网络驱动的HSI数据编码过程。首先，我们设计了一个元内容感知模块，通过融合传感器元属性和其自身的图像内容，为每个HSI块生成一个独特的条件输入，针对每个样本的每个光谱波段定制。其次，我们设计了超嵌入模块，其中样本条件超网络动态生成一对用于通道编码的矩阵因子，包括自适应空间模式提取和潜在语义特征重投影。因此，SpecAware获得了感知和解释不同场景和传感器中空间-光谱特征的能力。这反过来又使SpecAware能够自适应处理可变数量的光谱通道，建立统一的联合预训练框架。在六个数据集上的广泛实验表明，SpecAware能够学习优越的特征表示，在土地覆盖语义分割分类、变化检测和场景分类方面表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) is a vital tool for fine-grained land-use andland-cover (LULC) mapping. However, the inherent heterogeneity of HSI data haslong posed a major barrier to developing generalized models via joint training.Although HSI foundation models have shown promise for different downstreamtasks, the existing approaches typically overlook the critical guiding role ofsensor meta-attributes, and struggle with multi-sensor training, limiting theirtransferability. To address these challenges, we propose SpecAware, which is anovel hyperspectral spectral-content aware foundation model for unifyingmulti-sensor learning for HSI mapping. We also constructed the Hyper-400Kdataset to facilitate this research, which is a new large-scale, high-qualitybenchmark dataset with over 400k image patches from diverse airborne AVIRISsensors. The core of SpecAware is a two-step hypernetwork-driven encodingprocess for HSI data. Firstly, we designed a meta-content aware module togenerate a unique conditional input for each HSI patch, tailored to eachspectral band of every sample by fusing the sensor meta-attributes and its ownimage content. Secondly, we designed the HyperEmbedding module, where asample-conditioned hypernetwork dynamically generates a pair of matrix factorsfor channel-wise encoding, consisting of adaptive spatial pattern extractionand latent semantic feature re-projection. Thus, SpecAware gains the ability toperceive and interpret spatial-spectral features across diverse scenes andsensors. This, in turn, allows SpecAware to adaptively process a variablenumber of spectral channels, establishing a unified framework for jointpre-training. Extensive experiments on six datasets demonstrate that SpecAwarecan learn superior feature representations, excelling in land-cover semanticsegmentation classification, change detection, and scene classification.</description>
      <author>example@mail.com (Renjie Ji, Xue Wang, Chao Niu, Wen Zhang, Yong Mei, Kun Tan)</author>
      <guid isPermaLink="false">2510.27219v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>FMint-SDE: A Multimodal Foundation Model for Accelerating Numerical Simulation of SDEs via Error Correction</title>
      <link>http://arxiv.org/abs/2510.27173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FMint-SDE的多模态基础模型，用于快速准确地模拟动态系统，解决了传统数值积分器在准确性和效率之间的权衡问题，以及现有神经网络方法需要为每个案例单独训练模型的限制。&lt;h4&gt;背景&lt;/h4&gt;快速准确地模拟动态系统是科学和工程领域的基本挑战。传统数值积分器通常在准确性和计算效率之间面临权衡，而现有的基于神经网络的方法通常需要为每种情况单独训练一个模型。&lt;h4&gt;目的&lt;/h4&gt;克服传统方法和现有神经网络方法的局限性，引入一种新的多模态基础模型用于大规模微分方程模拟。&lt;h4&gt;方法&lt;/h4&gt;提出了FMint-SDE（基于初始化的随机微分方程基础模型），它基于仅解码器的transformer架构，具有上下文学习能力。该模型利用数值和文本模态学习通用误差校正方案，使用传统求解器生成的粗解序列进行提示训练，从而实现对不同系统的广泛泛化。&lt;h4&gt;主要发现&lt;/h4&gt;在涵盖分子动力学、机械系统、金融和生物学应用的一系列具有挑战性的SDE基准测试上，实验结果表明FMint-SDE相比经典求解器实现了更优的准确性-效率权衡。&lt;h4&gt;结论&lt;/h4&gt;FMint-SDE作为动态系统通用仿真工具具有巨大潜力，能够实现快速准确的模拟。&lt;h4&gt;翻译&lt;/h4&gt;快速准确地模拟动态系统是科学和工程领域的基本挑战。传统数值积分器通常在准确性和计算效率之间面临权衡，而现有的基于神经网络的方法通常需要为每种情况单独训练一个模型。为了克服这些限制，我们引入了一种新颖的多模态基础模型FMint-SDE（基于初始化的随机微分方程基础模型），用于大规模微分方程模拟。基于具有上下文学习能力的仅解码器transformer，FMint-SDE利用数值和文本模态学习通用误差校正方案。它使用传统求解器生成的粗解序列进行提示训练，从而实现对不同系统的广泛泛化。我们在一系列具有挑战性的SDE基准测试上评估了我们的模型，这些测试涵盖了分子动力学、机械系统、金融和生物学应用。实验结果表明，我们的方法相比经典求解器实现了更优的准确性-效率权衡，凸显了FMint-SDE作为动态系统通用仿真工具的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fast and accurate simulation of dynamical systems is a fundamental challengeacross scientific and engineering domains. Traditional numerical integratorsoften face a trade-off between accuracy and computational efficiency, whileexisting neural network-based approaches typically require training a separatemodel for each case. To overcome these limitations, we introduce a novelmulti-modal foundation model for large-scale simulations of differentialequations: FMint-SDE (Foundation Model based on Initialization for stochasticdifferential equations). Based on a decoder-only transformer with in-contextlearning, FMint-SDE leverages numerical and textual modalities to learn auniversal error-correction scheme. It is trained using prompted sequences ofcoarse solutions generated by conventional solvers, enabling broadgeneralization across diverse systems. We evaluate our models on a suite ofchallenging SDE benchmarks spanning applications in molecular dynamics,mechanical systems, finance, and biology. Experimental results show that ourapproach achieves a superior accuracy-efficiency tradeoff compared to classicalsolvers, underscoring the potential of FMint-SDE as a general-purposesimulation tool for dynamical systems.</description>
      <author>example@mail.com (Jiaxin Yuan, Haizhao Yang, Maria Cameron)</author>
      <guid isPermaLink="false">2510.27173v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>AD-SAM: Fine-Tuning the Segment Anything Vision Foundation Model for Autonomous Driving Perception</title>
      <link>http://arxiv.org/abs/2510.27047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE Transactions on Intelligent Transportation Systems  (IEEE T-ITS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AD-SAM模型，这是一个针对自动驾驶领域语义分割任务优化的视觉基础模型。该模型通过双编码器和可变形解码器扩展了SAM模型，在道路场景的复杂空间和几何特性上表现出色。实验证明AD-SAM在多个基准测试中超越了现有模型，具有更好的分割精度、跨域泛化能力和数据效率。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶需要精确的语义分割来理解复杂的道路场景。现有的基础模型如SAM在自动驾驶领域的应用可能无法充分满足对空间和几何复杂性的需求。此外，自动驾驶领域需要模型能够高效学习并减少标注成本。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对自动驾驶场景优化的语义分割模型，提高分割精度、边界精确度，同时实现更好的泛化能力和数据效率，减少对大量标注数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;1. 双编码器结构：结合SAM预训练的Vision Transformer (ViT-H)的全局语义上下文和可训练的卷积深度学习主干网络（ResNet-50）的局部空间细节，生成多尺度融合表示。2. 可变形融合模块：对齐不同尺度和对象几何形状的异构特征。3. 可变形注意力解码器：执行渐进式多阶段细化。4. 混合损失函数：结合Focal、Dice、Lovasz-Softmax和Surface损失，提高语义类别平衡性、边界精确度和优化稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在Cityscapes和BDD100K基准测试中，AD-SAM的分割精度分别达到68.1 mIoU和59.5 mIoU，显著优于SAM、G-SAM和DeepLabV3。2. 在结构化和多样化的道路场景中分别领先最多22.9和19.2 mIoU。3. AD-SAM表现出强大的跨域泛化能力，保留得分为0.87（而SAM为0.76）。4. 学习速度更快且更稳定，在30-40个周期内收敛，是基准模型学习速度的两倍。5. 仅使用1000个样本，AD-SAM仍能保持0.607 mIoU，显示出高数据效率。&lt;h4&gt;结论&lt;/h4&gt;针对基础模型进行有针对性的架构和优化增强，能够实现可靠且可扩展的自动驾驶感知。AD-SAM通过结合全局语义和局部空间信息，以及使用可变形注意力机制，显著提高了自动驾驶场景中的语义分割性能，同时减少了训练数据需求。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了自动驾驶分割一切模型（AD-SAM），这是一个针对自动驾驶（AD）中语义分割任务微调的视觉基础模型。AD-SAM通过双编码器和可变形解码器扩展了分割一切模型（SAM），以适应道路场景的空间和几何复杂性。双编码器通过结合SAM预训练的Vision Transformer (ViT-H)的全局语义上下文和可训练的卷积深度学习主干网络（即ResNet-50）的局部空间细节，产生多尺度融合表示。可变形融合模块对齐了不同尺度和对象几何形状之间的异构特征。解码器使用可变形注意力执行渐进式多阶段细化。训练由混合损失指导，该损失整合了Focal、Dice、Lovasz-Softmax和Surface损失，提高了语义类别平衡性、边界精确度和优化稳定性。在Cityscapes和伯克利深度驾驶100K（BDD100K）基准测试中的实验表明，AD-SAM在分割精度上超越了SAM、广义SAM（G-SAM）和深度学习基线（DeepLabV3）。它在Cityscapes上达到68.1平均交并比（mIoU），在BDD100K上达到59.5 mIoU，在结构化和多样化的道路场景中分别领先SAM、G-SAM和DeepLabV3最多22.9和19.2 mIoU。AD-SAM表现出强大的跨域泛化能力，保留得分为0.87（SAM为0.76），并且学习速度更快、更稳定，在30-40个周期内收敛，是基准模型学习速度的两倍。仅使用1000个样本，AD-SAM仍能保持0.607 mIoU，表明数据效率对于降低标注成本至关重要。这些结果证实，对基础模型进行有针对性的架构和优化增强能够实现可靠且可扩展的AD感知。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents the Autonomous Driving Segment Anything Model (AD-SAM), afine-tuned vision foundation model for semantic segmentation in autonomousdriving (AD). AD-SAM extends the Segment Anything Model (SAM) with adual-encoder and deformable decoder tailored to spatial and geometriccomplexity of road scenes. The dual-encoder produces multi-scale fusedrepresentations by combining global semantic context from SAM's pretrainedVision Transformer (ViT-H) with local spatial detail from a trainableconvolutional deep learning backbone (i.e., ResNet-50). A deformable fusionmodule aligns heterogeneous features across scales and object geometries. Thedecoder performs progressive multi-stage refinement using deformable attention.Training is guided by a hybrid loss that integrates Focal, Dice,Lovasz-Softmax, and Surface losses, improving semantic class balance, boundaryprecision, and optimization stability. Experiments on the Cityscapes andBerkeley DeepDrive 100K (BDD100K) benchmarks show that AD-SAM surpasses SAM,Generalized SAM (G-SAM), and a deep learning baseline (DeepLabV3) insegmentation accuracy. It achieves 68.1 mean Intersection over Union (mIoU) onCityscapes and 59.5 mIoU on BDD100K, outperforming SAM, G-SAM, and DeepLabV3 bymargins of up to +22.9 and +19.2 mIoU in structured and diverse road scenes,respectively. AD-SAM demonstrates strong cross-domain generalization with a0.87 retention score (vs. 0.76 for SAM), and faster, more stable learningdynamics, converging within 30-40 epochs, enjoying double the learning speed ofbenchmark models. It maintains 0.607 mIoU with only 1000 samples, suggestingdata efficiency critical for reducing annotation costs. These results confirmthat targeted architectural and optimization enhancements to foundation modelsenable reliable and scalable AD perception.</description>
      <author>example@mail.com (Mario Camarena, Het Patel, Fatemeh Nazari, Evangelos Papalexakis, Mohamadhossein Noruzoliaee, Jia Chen)</author>
      <guid isPermaLink="false">2510.27047v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>GeoPep: A geometry-aware masked language model for protein-peptide binding site prediction</title>
      <link>http://arxiv.org/abs/2510.27040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GeoPep是一种创新的肽结合位点预测框架，通过迁移学习从ESM3蛋白质基础模型获取知识，有效解决了蛋白质-肽相互作用预测中的挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态方法整合蛋白质结构和序列在蛋白质-蛋白质界面预测中已取得显著成功，但由于肽的内在构象灵活性和结构数据有限，这些方法难以扩展到蛋白质-肽相互作用领域。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服肽构象灵活性和结构数据有限性问题的蛋白质-肽结合位点预测方法。&lt;h4&gt;方法&lt;/h4&gt;GeoPep框架利用ESM3多模态蛋白质基础模型的迁移学习，微调其预学习的蛋白质-蛋白质结合表示，并与参数高效的神经网络架构集成，使用基于距离的损失函数训练模型，利用3D结构信息增强预测能力。&lt;h4&gt;主要发现&lt;/h4&gt;全面评估表明GeoPep能够有效捕获稀疏和异质的结合模式，在蛋白质-肽结合位点预测方面显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;GeoPep为蛋白质-肽相互作用预测提供了一种有效的解决方案，成功克服了该领域的关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;整合蛋白质结构和序列的多模态方法在蛋白质-蛋白质界面预测中取得了显著成功。然而，由于肽的内在构象灵活性和结构数据有限，阻碍了结构感知模型的直接训练，将这些方法扩展到蛋白质-肽相互作用仍然具有挑战性。为解决这些局限性，我们引入了GeoPep，这是一种新颖的肽结合位点预测框架，它利用从ESM3（多模态蛋白质基础模型）的迁移学习。GeoPep微调ESM3从蛋白质-蛋白质结合中预学习的丰富表示，以解决蛋白质-肽结合数据有限的问题。微调的模型进一步与能够从稀疏数据中学习复杂模式的参数高效神经网络架构集成。此外，模型使用基于距离的损失函数进行训练，利用3D结构信息增强结合位点预测。全面评估表明，GeoPep通过有效捕获稀疏和异质的结合模式，在蛋白质-肽结合位点预测方面显著优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal approaches that integrate protein structure and sequence haveachieved remarkable success in protein-protein interface prediction. However,extending these methods to protein-peptide interactions remains challenging dueto the inherent conformational flexibility of peptides and the limitedavailability of structural data that hinder direct training of structure-awaremodels. To address these limitations, we introduce GeoPep, a novel frameworkfor peptide binding site prediction that leverages transfer learning from ESM3,a multimodal protein foundation model. GeoPep fine-tunes ESM3's richpre-learned representations from protein-protein binding to address the limitedavailability of protein-peptide binding data. The fine-tuned model is furtherintegrated with a parameter-efficient neural network architecture capable oflearning complex patterns from sparse data. Furthermore, the model is trainedusing distance-based loss functions that exploit 3D structural information toenhance binding site prediction. Comprehensive evaluations demonstrate thatGeoPep significantly outperforms existing methods in protein-peptide bindingsite prediction by effectively capturing sparse and heterogeneous bindingpatterns.</description>
      <author>example@mail.com (Dian Chen, Yunkai Chen, Tong Lin, Sijie Chen, Xiaolin Cheng)</author>
      <guid isPermaLink="false">2510.27040v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>MoME: Mixture of Visual Language Medical Experts for Medical Imaging Segmentation</title>
      <link>http://arxiv.org/abs/2510.26996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MoME是一种用于医学图像分割的视觉语言专家混合模型，结合了多尺度视觉特征和文本嵌入，通过动态专家选择实现高性能的医学图像分割。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型中广泛使用的专家混合范式在医学视觉语言任务中的应用尚不充分，医学图像分析需要更有效的处理方法。&lt;h4&gt;目的&lt;/h4&gt;将成功的MoE范式应用于医学图像分割任务，探索视觉语言模型在医学领域的新型集成方法，实现稳健的医学图像分析。&lt;h4&gt;方法&lt;/h4&gt;提出MoME架构，有效利用针对医学图像复杂性定制的多尺度视觉特征，并结合文本嵌入实现动态专家选择。使用包含3410个CT扫描的10个数据集进行训练和测试。&lt;h4&gt;主要发现&lt;/h4&gt;MoME在全面的医学图像分割基准测试中表现出强大的性能，在多个数据集上展示了具有竞争力的精确度。通过整合文本信息，模型性能得到显著提升。&lt;h4&gt;结论&lt;/h4&gt;MoME探索了一种用于实现稳健医学图像分析结果的新型架构，证明了将基础模型和MoE范式应用于医学图像分割的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在这项研究中，我们提出了MoME，一种用于医学图像分割的视觉语言医学专家混合模型。MoME借鉴了大型语言模型中广泛使用的专家混合范式，应用于医学视觉语言任务。该架构通过有效利用针对医学图像复杂性定制的多尺度视觉特征，并结合文本嵌入，实现了动态专家选择。这项工作探索了视觉语言模型在该领域的新型集成方法。利用包含3410个CT扫描的10个数据集，MoME在全面的医学图像分割基准测试中表现出强大的性能。我们的方法探索了基础模型在医学图像中的应用，受益于MoE通过整合文本信息提高模型性能的已证实有效性。MoME在多个数据集上展示了具有竞争力的精确度，探索了一种用于实现稳健医学图像分析结果的新型架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we propose MoME, a Mixture of Visual Language Medical Experts,for Medical Image Segmentation. MoME adapts the successful Mixture of Experts(MoE) paradigm, widely used in Large Language Models (LLMs), for medicalvision-language tasks. The architecture enables dynamic expert selection byeffectively utilizing multi-scale visual features tailored to the intricaciesof medical imagery, enriched with textual embeddings. This work explores anovel integration of vision-language models for this domain. Utilizing anassembly of 10 datasets, encompassing 3,410 CT scans, MoME demonstrates strongperformance on a comprehensive medical imaging segmentation benchmark. Ourapproach explores the integration of foundation models for medical imaging,benefiting from the established efficacy of MoE in boosting model performanceby incorporating textual information. Demonstrating competitive precisionacross multiple datasets, MoME explores a novel architecture for achievingrobust results in medical image analysis.</description>
      <author>example@mail.com (Arghavan Rezvani, Xiangyi Yan, Anthony T. Wu, Kun Han, Pooya Khosravi, Xiaohui Xie)</author>
      <guid isPermaLink="false">2510.26996v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>NaviTrace: Evaluating Embodied Navigation of Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.26909v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures, under review at IEEE conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了NaviTrace，一个高质量的视觉问答基准测试，用于评估视觉语言模型在机器人导航任务中的表现。该基准测试包含1000多个场景和3000多条专家轨迹，使用语义感知轨迹分数评估了八个最先进的VLM，发现模型在空间定位和目标定位方面与人类性能存在差距。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在多种任务和场景中展现出前所未有的性能和泛化能力，将其集成到机器人导航系统中有助于构建通用机器人。然而，评估这些模型导航能力受到现实世界试验成本高、模拟过于简单以及基准有限等限制。&lt;h4&gt;目的&lt;/h4&gt;开发一个高质量的基准测试来评估视觉语言模型在机器人导航任务中的能力，解决当前评估方法存在的问题。&lt;h4&gt;方法&lt;/h4&gt;提出NaviTrace基准测试，模型接收指令和实体类型（人类、腿式机器人、轮式机器人、自行车），在图像空间输出二维导航轨迹。使用语义感知轨迹分数（结合动态时间规整距离、目标端点误差和基于逐像素语义的实体条件惩罚）进行评估，并与人类偏好进行对比。&lt;h4&gt;主要发现&lt;/h4&gt;八个最先进的VLM在机器人导航任务中与人类性能存在持续差距，主要问题在于空间定位和目标定位能力不足。NaviTrace为现实世界机器人导航提供了可扩展且可重现的基准测试。&lt;h4&gt;结论&lt;/h4&gt;NaviTrace基准测试填补了视觉语言模型在机器人导航评估方面的空白，为未来研究提供了可靠的评估工具，有助于推动通用机器人的发展。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在各种任务和场景中展现出前所未有的性能和泛化能力。将这些基础模型集成到机器人导航系统中，为构建通用机器人开辟了途径。然而，评估这些模型的导航能力仍然受到现实世界试验成本高、模拟过于简单以及基准有限等限制。我们引入了NaviTrace，这是一个高质量的视觉问答基准测试，模型接收指令和实体类型（人类、腿式机器人、轮式机器人、自行车），必须在图像空间输出二维导航轨迹。在1000多个场景和3000多条专家轨迹中，我们使用新引入的语义感知轨迹分数系统地评估了八个最先进的VLM。该指标结合了动态时间规整距离、目标端点误差以及基于逐像素语义的实体条件惩罚，并与人类偏好相关。我们的评估显示，由于空间定位和目标定位能力差，模型与人类性能之间存在持续差距。NaviTrace为现实世界机器人导航建立了可扩展且可重现的基准测试。基准测试和排行榜可以在https://leggedrobotics.github.io/navitrace_webpage/找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models demonstrate unprecedented performance andgeneralization across a wide range of tasks and scenarios. Integrating thesefoundation models into robotic navigation systems opens pathways towardbuilding general-purpose robots. Yet, evaluating these models' navigationcapabilities remains constrained by costly real-world trials, overly simplifiedsimulations, and limited benchmarks. We introduce NaviTrace, a high-qualityVisual Question Answering benchmark where a model receives an instruction andembodiment type (human, legged robot, wheeled robot, bicycle) and must output a2D navigation trace in image space. Across 1000 scenarios and more than 3000expert traces, we systematically evaluate eight state-of-the-art VLMs using anewly introduced semantic-aware trace score. This metric combines Dynamic TimeWarping distance, goal endpoint error, and embodiment-conditioned penaltiesderived from per-pixel semantics and correlates with human preferences. Ourevaluation reveals consistent gap to human performance caused by poor spatialgrounding and goal localization. NaviTrace establishes a scalable andreproducible benchmark for real-world robotic navigation. The benchmark andleaderboard can be found athttps://leggedrobotics.github.io/navitrace_webpage/.</description>
      <author>example@mail.com (Tim Windecker, Manthan Patel, Moritz Reuss, Richard Schwarzkopf, Cesar Cadena, Rudolf Lioutikov, Marco Hutter, Jonas Frey)</author>
      <guid isPermaLink="false">2510.26909v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Cognition Envelopes for Bounded AI Reasoning in Autonomous UAS Operations</title>
      <link>http://arxiv.org/abs/2510.26905v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10.5 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了认知包络(Cognition Envelopes)的概念，用于约束AI模型在网络物理系统中的决策，以应对模型引入的新类型错误。&lt;h4&gt;背景&lt;/h4&gt;网络物理系统越来越多地依赖基础模型如大型语言模型(LLMs)和视觉语言模型(VLMs)，通过增强感知、推理和规划来提高自主性。&lt;h4&gt;目的&lt;/h4&gt;解决基础模型引入的新类型错误(如幻觉、过度泛化和上下文不匹配)导致的错误决策问题。&lt;h4&gt;方法&lt;/h4&gt;引入认知包络概念，建立推理边界来约束AI生成的决策，同时补充元认知和传统安全包络的使用，并为其制定实际指南和系统化流程进行定义、验证和保证。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体研究发现。&lt;h4&gt;结论&lt;/h4&gt;认知包络是管理AI模型决策风险的一种有效方法，需要系统化的流程来确保其有效性。&lt;h4&gt;翻译&lt;/h4&gt;网络物理系统越来越多地依赖基础模型，如大型语言模型(LLMs)和视觉语言模型(VLMs)，通过增强感知、推理和规划来提高自主性。然而，这些模型也引入了新的错误类型，如幻觉、过度泛化和上下文不匹配，导致错误和有缺陷的决策。为了解决这个问题，我们引入了认知包络的概念，旨在建立推理边界，以约束AI生成的决策，同时补充元认知和传统安全包络的使用。与安全包络一样，认知包络需要其实际指南和系统化的流程来进行定义、验证和保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cyber-physical systems increasingly rely on Foundational Models such as LargeLanguage Models (LLMs) and Vision-Language Models (VLMs) to increase autonomythrough enhanced perception, inference, and planning. However, these modelsalso introduce new types of errors, such as hallucinations,overgeneralizations, and context misalignments, resulting in incorrect andflawed decisions. To address this, we introduce the concept of CognitionEnvelopes, designed to establish reasoning boundaries that constrainAI-generated decisions while complementing the use of meta-cognition andtraditional safety envelopes. As with safety envelopes, Cognition Envelopesrequire practical guidelines and systematic processes for their definition,validation, and assurance.</description>
      <author>example@mail.com (Pedro Antonio Alarcón Granadeno, Arturo Miguel Bernal Russell, Sofia Nelson, Demetrius Hernandez, Maureen Petterson, Michael Murphy, Walter J. Scheirer, Jane Cleland-Huang)</author>
      <guid isPermaLink="false">2510.26905v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for Time Series Classification</title>
      <link>http://arxiv.org/abs/2510.26777v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 Workshop on Recent Advances in Time Series Foundation  Models (BERT2S)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了预训练的时间序列预测模型在分类任务上的表示有效性，挑战了任务特定预训练的必要性假设。&lt;h4&gt;背景&lt;/h4&gt;最近关于时间序列基础模型的研究主要集中在预测任务上，不清楚这些模型学习到的表示有多大的通用性。&lt;h4&gt;目的&lt;/h4&gt;研究冻结预训练的预测模型是否能为分类任务提供有效的表示，以及预测性能与分类性能之间的关系。&lt;h4&gt;方法&lt;/h4&gt;比较不同的表示提取策略，并引入两种与模型无关的嵌入增强方法，通过实验评估预测模型在分类任务上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;最好的预测模型在分类任务上实现的准确率与或甚至超过了专门为分类预训练的最先进模型，且预测性能和分类性能之间存在正相关。&lt;h4&gt;结论&lt;/h4&gt;学习预测可能是构建通用时间序列基础模型的有力途径，挑战了任务特定预训练的必要性假设。&lt;h4&gt;翻译&lt;/h4&gt;最近关于时间序列基础模型的研究主要集中在预测上，这使得它们学习到的表示的通用性尚不清楚。在本研究中，我们考察冻结预训练的预测模型是否能为分类提供有效表示。为此，我们比较了不同的表示提取策略，并引入了两种与模型无关的嵌入增强方法。我们的实验表明，最好的预测模型实现的分类准确率与甚至超过了专门为分类预训练的最先进模型。此外，我们观察到预测性能和分类性能之间存在正相关。这些发现挑战了任务特定预训练的必要性假设，并表明学习预测可能是构建通用时间序列基础模型的有力途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research on time series foundation models has primarily focused onforecasting, leaving it unclear how generalizable their learned representationsare. In this study, we examine whether frozen pre-trained forecasting modelscan provide effective representations for classification. To this end, wecompare different representation extraction strategies and introduce twomodel-agnostic embedding augmentations. Our experiments show that the bestforecasting models achieve classification accuracy that matches or evensurpasses that of state-of-the-art models pre-trained specifically forclassification. Moreover, we observe a positive correlation between forecastingand classification performance. These findings challenge the assumption thattask-specific pre-training is necessary, and suggest that learning to forecastmay provide a powerful route toward constructing general-purpose time seriesfoundation models.</description>
      <author>example@mail.com (Andreas Auer, Daniel Klotz, Sebastinan Böck, Sepp Hochreiter)</author>
      <guid isPermaLink="false">2510.26777v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models</title>
      <link>http://arxiv.org/abs/2510.26732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对当代基础模型的推理能力进行了全面的跨平台评估，建立了不依赖特定基础设施的基准测试，涵盖高性能计算超级计算机、云平台和大学集群三种计算范式。&lt;h4&gt;背景&lt;/h4&gt;当前基础模型在不同平台上的推理能力表现尚不明确，需要建立一个统一的评估框架。&lt;h4&gt;目的&lt;/h4&gt;建立基础设施无关的基准测试，评估15个基础模型在8个学术领域79个问题上的推理能力，并探索不同计算环境下的性能差异。&lt;h4&gt;方法&lt;/h4&gt;通过三个实验阶段进行评估：(1)基线建立：在MareNostrum 5上使用6个模型评估19个问题；(2)基础设施验证：在大学集群和Nebius AI Studio上重复基准测试；(3)扩展评估：在两个平台上对完整的79个问题进行全面评估。&lt;h4&gt;主要发现&lt;/h4&gt;挑战了传统的扩展假设，确立了训练数据质量比模型大小更重要，为不同环境中的模型选择提供了可操作的指导原则。&lt;h4&gt;结论&lt;/h4&gt;三种基础设施的方法和79个问题的基准测试使能够随着基础模型的演变进行推理能力的纵向跟踪。&lt;h4&gt;翻译&lt;/h4&gt;本文对当代基础模型的推理能力进行了全面的跨平台评估，建立了一个不依赖特定基础设施的基准测试，涵盖高性能计算超级计算机、云平台和大学集群三种计算范式。我们评估了15个基础模型，涵盖物理、数学、化学、经济学、生物学、统计学、微积分和优化八个学术领域的79个问题。通过三个实验阶段：(1)基线建立：在MareNostrum 5上使用6个模型评估19个问题，建立方法和参考性能；(2)基础设施验证：在大学集群和Nebius AI Studio上重复19个问题的基准测试，确认与基础设施无关的可重复性；(3)扩展评估：在两个平台上对完整的79个问题进行全面评估，探索架构多样性方面的规模化泛化能力。研究挑战了传统的扩展假设，确立了训练数据质量比模型大小更重要，为教育、生产和研究环境中的模型选择提供了可操作的指导原则。三种基础设施的方法和79个问题的基准测试使能够随着基础模型的演变进行推理能力的纵向跟踪。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a comprehensive cross-platform evaluation of reasoningcapabilities in contemporary foundation models, establishing aninfrastructure-agnostic benchmark across three computational paradigms: HPCsupercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), anduniversity clusters (a node with eight H200 GPUs).  We evaluate 15 foundation models across 79 problems spanning eight academicdomains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,Calculus, and Optimization) through three experimental phases: (1) Baselineestablishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishingmethodology and reference performance; (2) Infrastructure validation: The19-problem benchmark repeated on university cluster (seven models includingFalcon-Mamba state-space architecture) and Nebius AI Studio (ninestate-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen330B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnosticreproducibility; (3) Extended evaluation: Full 79-problem assessment on bothuniversity cluster and Nebius platforms, probing generalization at scale acrossarchitectural diversity.  The findings challenge conventional scaling assumptions, establish trainingdata quality as more critical than model size, and provide actionableguidelines for model selection across educational, production, and researchcontexts. The tri-infrastructure methodology and 79-problem benchmark enablelongitudinal tracking of reasoning capabilities as foundation models evolve.</description>
      <author>example@mail.com (J. de Curtò, I. de Zarzà, Pablo García, Jordi Cabot)</author>
      <guid isPermaLink="false">2510.26732v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>LSM-MS2: A Foundation Model Bridging Spectral Identification and Biological Interpretation</title>
      <link>http://arxiv.org/abs/2510.26715v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LSM-MS2是一个大规模深度学习基础模型，通过学习语义化学空间，显著提高了光谱识别的性能，特别是在异构化合物识别、复杂生物样本分析和低浓度条件下，并能直接进行生物学解释和临床预测。&lt;h4&gt;背景&lt;/h4&gt;大多数质谱数据仍未被表征，导致大量生物和化学信息未被利用。机器学习的最新进展开始解决这个问题，特别是在串联质谱数据的光谱识别任务中。&lt;h4&gt;目的&lt;/h4&gt;介绍LSM-MS2，一个大规模深度学习基础模型，旨在学习语义化学空间并解决质谱数据表征不足的问题。&lt;h4&gt;方法&lt;/h4&gt;LSM-MS2是在数百万张光谱上训练的大规模深度学习基础模型，它学习语义化学空间，能够生成丰富的光谱嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;LSM-MS2在光谱识别方面取得了最先进的性能：在识别具有挑战性的异构化合物方面比现有方法提高30%的准确率；在复杂生物样本中产生42%的正确识别；在低浓度条件下保持稳健性；产生的光谱嵌入可直接从最少的下游数据进行生物学解释；成功区分不同的疾病状态并预测各种转化应用中的临床结果。&lt;h4&gt;结论&lt;/h4&gt;LSM-MS2模型有效地解决了质谱数据表征不足的问题，为生物和化学信息的利用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;绝大多数质谱数据仍未被表征，导致其大量生物和化学信息未被利用。机器学习的最新进展开始解决这个问题，特别是在串联质谱数据的光谱识别任务方面。在此，我们介绍了LSM-MS2的最新一代，这是一个在数百万张光谱上训练的大规模深度学习基础模型，用于学习语义化学空间。LSM-MS2在光谱识别方面取得了最先进的性能，在识别具有挑战性的异构化合物方面比现有方法提高30%的准确率，在复杂生物样本中产生42%的正确识别，并在低浓度条件下保持稳健性。此外，LSM-MS2产生丰富的光谱嵌入，能够直接从最少的下游数据进行生物学解释，成功区分不同的疾病状态并预测各种转化应用中的临床结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A vast majority of mass spectrometry data remains uncharacterized, leavingmuch of its biological and chemical information untapped. Recent advances inmachine learning have begun to address this gap, particularly for tasks such asspectral identification in tandem mass spectrometry data. Here, we present thelatest generation of LSM-MS2, a large-scale deep learning foundation modeltrained on millions of spectra to learn a semantic chemical space. LSM-MS2achieves state-of-the-art performance in spectral identification, improving onexisting methods by 30% in accuracy of identifying challenging isomericcompounds, yielding 42% more correct identifications in complex biologicalsamples, and maintaining robustness under low-concentration conditions.Furthermore, LSM-MS2 produces rich spectral embeddings that enable directbiological interpretation from minimal downstream data, successfullydifferentiating disease states and predicting clinical outcomes across diversetranslational applications.</description>
      <author>example@mail.com (Gabriel Asher, Devesh Shah, Amy A. Caudy, Luke Ferro, Lea Amar, Ana S. H. Costa, Thomas Patton, Niall O'Connor, Jennifer M. Campbell, Jack Geremia)</author>
      <guid isPermaLink="false">2510.26715v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>ProstNFound+: A Prospective Study using Medical Foundation Models for Prostate Cancer Detection</title>
      <link>http://arxiv.org/abs/2510.26703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了ProstNFound+，一种基于医学基础模型的前列腺癌检测系统，并通过前瞻性验证了其在临床微超声图像中的有效性和实用性。&lt;h4&gt;背景&lt;/h4&gt;医学基础模型为构建高性能诊断系统提供了新途径，但在前列腺癌微超声检测领域的临床应用尚未得到验证。&lt;h4&gt;目的&lt;/h4&gt;开发并验证ProstNFound+系统，将医学基础模型应用于前列腺癌的微超声检测，并进行临床前瞻性验证。&lt;h4&gt;方法&lt;/h4&gt;ProstNFound+整合了医学基础模型、适配器调优和嵌入前列腺癌特异性临床生物标志物的自定义提示编码器，生成癌症热图和风险评分。模型在多中心回顾性数据上训练，并在五年后新临床站点获取的数据上进行前瞻性评估，与标准临床评分方案(PRI-MUS和PI-RADS)进行对比。&lt;h4&gt;主要发现&lt;/h4&gt;ProstNFound+在前瞻性数据上表现出强大的泛化能力，性能未出现下降；与临床评分高度一致；生成的热图具有可解释性，与活检证实的病变一致。&lt;h4&gt;结论&lt;/h4&gt;ProstNFound+具有临床部署潜力，为专家驱动协议提供了可扩展且可解释的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;目的：医学基础模型为构建高性能诊断系统提供了一种途径。然而，这些模型在前列腺癌微超声检测中的临床应用尚未得到验证。我们提出了ProstNFound+，这是一个针对前列腺癌微超声检测的基础模型适应版本，并进行了首次前瞻性验证。方法：ProstNFound+整合了医学基础模型、适配器调优和嵌入前列腺癌特异性临床生物标志物的自定义提示编码器。该模型生成癌症热图和临床显著前列腺癌的风险评分。在多中心回顾性数据上训练后，模型在五年后从新临床站点获取的数据上进行前瞻性评估。模型预测与标准临床评分方案(PRI-MUS和PI-RADS)进行基准测试。结果：ProstNFound+在前瞻性数据上显示出强大的泛化能力，与回顾性评估相比没有性能下降。与临床评分高度一致，生成与活检证实的病变一致的可解释热图。结论：结果突显了ProstNFound+在临床部署方面的潜力，提供了可扩展且可解释的替代方案，替代专家驱动的协议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Medical foundation models (FMs) offer a path to buildhigh-performance diagnostic systems. However, their application to prostatecancer (PCa) detection from micro-ultrasound ({\mu}US) remains untested inclinical settings. We present ProstNFound+, an adaptation of FMs for PCadetection from {\mu}US, along with its first prospective validation. Methods:ProstNFound+ incorporates a medical FM, adapter tuning, and a custom promptencoder that embeds PCa-specific clinical biomarkers. The model generates acancer heatmap and a risk score for clinically significant PCa. Followingtraining on multi-center retrospective data, the model is prospectivelyevaluated on data acquired five years later from a new clinical site. Modelpredictions are benchmarked against standard clinical scoring protocols(PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to theprospective data, with no performance degradation compared to retrospectiveevaluation. It aligns closely with clinical scores and produces interpretableheatmaps consistent with biopsy-confirmed lesions. Conclusion: The resultshighlight its potential for clinical deployment, offering a scalable andinterpretable alternative to expert-driven protocols.</description>
      <author>example@mail.com (Paul F. R. Wilson, Mohamed Harmanani, Minh Nguyen Nhat To, Amoon Jamzad, Tarek Elghareb, Zhuoxin Guo, Adam Kinnaird, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi)</author>
      <guid isPermaLink="false">2510.26703v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Aeolus: A Multi-structural Flight Delay Dataset</title>
      <link>http://arxiv.org/abs/2510.26616v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍Aeolus，一个大规模多模态航班延误数据集，用于推进航班延误预测研究和支持表格数据基础模型的开发。&lt;h4&gt;背景&lt;/h4&gt;现有领域数据集通常限于扁平表格结构，无法捕捉航班延误传播中固有的时空动态特性。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集的局限性，提供能够捕捉航班延误传播特性的多模态数据集。&lt;h4&gt;方法&lt;/h4&gt;提供三种对齐模态：(i) 包含丰富运营、气象和机场级别特征的表格数据集，涵盖超过500万次航班；(ii) 航班链模块，模拟沿连续航班航段的延误传播，捕捉上下游依赖关系；(iii) 航班网络图，编码共享的飞机、机组人员和机场资源连接，实现跨航班关系推理。&lt;h4&gt;主要发现&lt;/h4&gt;数据集通过时间分割、全面特征和严格的防泄漏措施精心构建，支持真实且可复现的机器学习评估。&lt;h4&gt;结论&lt;/h4&gt;Aeolus支持多种任务，包括回归、分类、时间结构建模和图学习，可作为表格、序列和图模态的统一基准，填补了领域特定建模和通用结构数据研究的关键空白。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍Aeolus，这是一个大规模多模态航班延误数据集，旨在推进航班延误预测研究并支持表格数据基础模型的开发。该领域现有数据集通常限于扁平表格结构，无法捕捉延误传播中固有的时空动态特性。Aeolus通过提供三种对齐模态解决了这一局限：(i) 包含丰富运营、气象和机场级别特征的表格数据集，涵盖超过500万次航班；(ii) 航班链模块，用于模拟沿连续航班航段的延误传播，捕捉上游和下游依赖关系；(iii) 航班网络图，编码共享的飞机、机组人员和机场资源连接，实现跨航班关系推理。该数据集通过时间分割、全面特征和严格的防泄漏措施精心构建，支持真实且可复现的机器学习评估。Aeolus支持多种任务，包括回归、分类、时间结构建模和图学习，可作为表格、序列和图模态的统一基准。我们发布了基线实验和预处理工具以促进采用。Aeolus填补了领域特定建模和通用结构数据研究的关键空白。我们的源代码和数据可在https://github.com/Flnny/Delay-data获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designedto advance research on flight delay prediction and support the development offoundation models for tabular data. Existing datasets in this domain aretypically limited to flat tabular structures and fail to capture thespatiotemporal dynamics inherent in delay propagation. Aeolus addresses thislimitation by providing three aligned modalities: (i) a tabular dataset withrich operational, meteorological, and airportlevel features for over 50 millionflights; (ii) a flight chain module that models delay propagation alongsequential flight legs, capturing upstream and downstream dependencies; and(iii) a flight network graph that encodes shared aircraft, crew, and airportresource connections, enabling cross-flight relational reasoning. The datasetis carefully constructed with temporal splits, comprehensive features, andstrict leakage prevention to support realistic and reproducible machinelearning evaluation. Aeolus supports a broad range of tasks, includingregression, classification, temporal structure modeling, and graph learning,serving as a unified benchmark across tabular, sequential, and graphmodalities. We release baseline experiments and preprocessing tools tofacilitate adoption. Aeolus fills a key gap for both domain-specific modelingand general-purpose structured data research.Our source code and data can beaccessed at https://github.com/Flnny/Delay-data</description>
      <author>example@mail.com (Lin Xu, Xinyun Yuan, Yuxuan Liang, Suwan Yin, Yuankai Wu)</author>
      <guid isPermaLink="false">2510.26616v2</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Foundation Models for Enhancing Robot Perception and Action</title>
      <link>http://arxiv.org/abs/2510.26855v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Doctoral thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了如何系统性地利用基础模型来增强机器人能力，特别是在非结构化环境中实现更有效的定位、交互和操作。研究围绕四个核心问题展开，共同构建了一个语义感知的机器人智能框架。&lt;h4&gt;背景&lt;/h4&gt;基础模型在机器人领域的应用是一个新兴的研究方向，特别是在处理非结构化环境中的挑战时。&lt;h4&gt;目的&lt;/h4&gt;探索基础模型如何被系统性地应用于机器人领域，以增强机器人在非结构化环境中的定位、交互和操作能力。&lt;h4&gt;方法&lt;/h4&gt;研究围绕四个核心问题展开，每个问题解决机器人领域的一个基本挑战，共同构建一个语义感知的机器人智能框架。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型可以被系统性地利用来增强机器人在非结构化环境中的能力，包括更有效的定位、交互和操作。&lt;h4&gt;结论&lt;/h4&gt;基础模型为增强机器人能力提供了新的途径，特别是在处理非结构化环境中的复杂任务时。&lt;h4&gt;翻译&lt;/h4&gt;本论文研究了如何系统性地利用基础模型来增强机器人能力，使机器人在非结构化环境中能够实现更有效的定位、交互和操作。这项工作围绕四个核心问题展开，每个问题都解决了机器人领域的一个基本挑战，同时共同构建了一个语义感知的机器人智能框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何系统性地将基础模型（Foundation Models）集成到机器人系统中，以增强机器人在非结构化环境中的感知和动作能力。这个问题很重要，因为传统机器人系统依赖于特定任务模型，难以在真实世界的复杂、动态环境中泛化和适应。基础模型具有强大的语义理解和泛化能力，可以帮助机器人更好地解释复杂场景、适应新任务，并在变化的环境中灵活响应，最终实现更接近人类的灵活推理和行为。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统机器人系统在真实环境中的局限性，然后发现了基础模型（如GPT-3、CLIP等）的潜力。作者设计方法时借鉴了现有工作：在定位方面借鉴了语义信息增强方法，但使用了更强大的基础模型；在抓取方面借鉴了语言引导方法，但结合了大型语言模型和视觉语言模型；在分类方面借鉴了知识蒸馏技术；在视觉鲁棒性方面借鉴了视觉抽象方法。作者基于四个核心研究问题（定位、抓取、分类和操作）设计了四个方法，每个方法都利用基础模型的特定能力，并通过不同技术（如零样本推理、模型蒸馏）解决机器人面临的挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这篇论文提出了四个主要方法，每个针对不同机器人任务：1) FM-Loc使用基础模型生成语义图像描述符，通过CLIP检测对象、GPT-3生成房间标签、再次使用CLIP选择最可能的房间标签，最后计算查询与参考图像的语义相似度；2) Lan-grasp利用大型语言模型确定可抓取部分，视觉语言模型定位这些部分，并引入反馈机制动态调整抓取策略；3) VLM-Vac使用知识蒸馏将视觉语言模型能力转移到轻量级模型，并通过语言引导的经验回放实现持续学习；4) ARRO使用开放词汇分割和对象检测隔离任务相关组件，投影到虚拟背景上，减少视觉域变化的影响。整体流程都是先利用基础模型进行语义理解，然后根据具体任务设计相应的推理或决策机制。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) FM-Loc使用基础模型构建语义描述符，无需额外数据收集或微调；2) Lan-grasp结合语言模型和视觉模型进行语义抓取，引入反馈机制；3) VLM-Vac使用知识蒸馏和经验回放实现持续学习；4) ARRO提出视觉抽象方法提高视觉运动策略鲁棒性。相比之前工作，这些创新更系统地利用基础模型能力，注重语义理解和上下文感知，采用零样本推理等技术解决机器人挑战，并在真实环境中广泛评估，展示了实用性和有效性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性地将基础模型集成到机器人核心任务中，利用零样本推理、模型蒸馏和视觉抽象等技术，显著提高了机器人在非结构化环境中的语义理解、泛化能力和鲁棒性，为开发能够与人类在动态环境中可靠协作的自主机器人提供了新框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This thesis investigates how foundation models can be systematicallyleveraged to enhance robotic capabilities, enabling more effectivelocalization, interaction, and manipulation in unstructured environments. Thework is structured around four core lines of inquiry, each addressing afundamental challenge in robotics while collectively contributing to a cohesiveframework for semantics-aware robotic intelligence.</description>
      <author>example@mail.com (Reihaneh Mirjalili)</author>
      <guid isPermaLink="false">2510.26855v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing</title>
      <link>http://arxiv.org/abs/2510.26609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了CYPRESS，一种基于深度学习的油菜籽产量预测模型，通过利用预训练的地理空间基础模型，将多时相卫星图像转换为高分辨率的像素级产量图，为精准农业提供了更实用的工具。&lt;h4&gt;背景&lt;/h4&gt;准确及时的作物产量预测对全球粮食安全和现代农业管理至关重要，但传统方法缺乏精准农业所需的可扩展性和粒度。&lt;h4&gt;目的&lt;/h4&gt;开发一种名为CYPRESS的深度学习模型，用于高分辨率、田间级别的油菜籽产量预测。&lt;h4&gt;方法&lt;/h4&gt;CYPRESS利用预训练的大规模地理空间基础模型(Prithvi-EO-2.0-600M)，将其适配为连续回归任务，将多时相卫星图像转换为密集的像素级产量图。&lt;h4&gt;主要发现&lt;/h4&gt;在加拿大草原综合数据集上评估，CYPRESS表现优于现有的基于深度学习的产量预测模型，证明了微调基础模型用于专业农业应用的有效性。&lt;h4&gt;结论&lt;/h4&gt;CYPRESS提供连续、高分辨率的输出，比传统方法更实用；该工作弥合了大规模地球观测和农场决策之间的差距，为详细的农业监测提供了可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确及时的作物产量预测对全球粮食安全和现代农业管理至关重要。传统方法往往缺乏精准农业所需的可扩展性和粒度。本文介绍了CYPRESS（通过卫星传感的Prithvi编码器回归进行作物产量预测），这是一种专为高分辨率、田间级油菜籽产量预测设计的深度学习模型。CYPRESS利用预训练的大规模地理空间基础模型（Prithvi-EO-2.0-600M），并对其进行适配以用于连续回归任务，将多时相卫星图像转换为密集的像素级产量图。在加拿大草原综合数据集上的评估表明，CYPRESS优于现有的基于深度学习的产量预测模型，突显了微调基础模型用于专业农业应用的有效性。通过提供连续、高分辨率的输出，CYPRESS比传统的分类或县级聚合方法为精准农业提供了更实用的工具。这项工作验证了一种新颖的方法，弥合了大规模地球观测和农场决策之间的差距，为详细的农业监测提供了可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and timely crop yield prediction is crucial for global food securityand modern agricultural management. Traditional methods often lack thescalability and granularity required for precision farming. This paperintroduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoderfor Satellite Sensing), a deep learning model designed for high-resolution,intra-field canola yield prediction. CYPRESS leverages a pre-trained,large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it fora continuous regression task, transforming multi-temporal satellite imageryinto dense, pixel-level yield maps. Evaluated on a comprehensive dataset fromthe Canadian Prairies, CYPRESS demonstrates superior performance over existingdeep learning-based yield prediction models, highlighting the effectiveness offine-tuning foundation models for specialized agricultural applications. Byproviding a continuous, high-resolution output, CYPRESS offers a moreactionable tool for precision agriculture than conventional classification orcounty-level aggregation methods. This work validates a novel approach thatbridges the gap between large-scale Earth observation and on-farmdecision-making, offering a scalable solution for detailed agriculturalmonitoring.</description>
      <author>example@mail.com (Shayan Nejadshamsi, Yuanyuan Zhang, Shadi Zaki, Brock Porth, Lysa Porth, Vahab Khoshdel)</author>
      <guid isPermaLink="false">2510.26609v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems</title>
      <link>http://arxiv.org/abs/2510.26585v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SupervisorAgent是一个轻量级和模块化的框架，用于运行时自适应监督，无需更改基础智能体的架构。它通过无LLM的自适应过滤器触发，在关键时刻进行干预，主动纠正错误，指导低效行为，并净化观测信息，从而显著减少多智能体系统中的令牌消耗，同时保持成功率。&lt;h4&gt;背景&lt;/h4&gt;多智能体系统在处理复杂任务时表现出色，但随着操作复杂性的增加，它们的自主性增强会导致关键效率问题，如过度消耗令牌和因错误信息导致的失败。现有方法主要关注事后故障归因，缺乏主动的、实时干预来增强鲁棒性和效率。&lt;h4&gt;目的&lt;/h4&gt;引入SupervisorAgent，一个轻量级和模块化的框架，用于运行时自适应监督，无需更改基础智能体的架构，以解决多智能体系统中的效率问题。&lt;h4&gt;方法&lt;/h4&gt;SupervisorAgent由一个无LLM的自适应过滤器触发，在关键时刻进行干预，主动纠正错误，指导低效行为，并净化观测信息。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的GAIA基准测试中，SupervisorAgent将Smolagent框架的令牌消耗平均减少了29.45%，同时不损害其成功率。在五个额外的基准测试（数学推理、代码生成和问答）和各种最先进的基础模型上的广泛实验验证了我们方法的广泛适用性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;SupervisorAgent是一个有效的解决方案，可以解决多智能体系统中的效率问题，同时保持其性能，代码已开源。&lt;h4&gt;翻译&lt;/h4&gt;虽然多智能体系统在复杂任务中表现出色，但随着操作复杂性的增加，它们的自主性增强常常导致关键效率问题，如过度消耗令牌和因错误信息导致的失败。现有方法主要关注事后故障归因，缺乏主动的、实时干预来增强鲁棒性和效率。为此，我们引入了SupervisorAgent，一个轻量级和模块化的框架，用于运行时自适应监督，无需更改基础智能体的架构。由无LLM的自适应过滤器触发，SupervisorAgent在关键时刻进行干预，主动纠正错误，指导低效行为，并净化观测信息。在具有挑战性的GAIA基准测试中，SupervisorAgent将Smolagent框架的令牌消耗平均减少了29.45%，同时不损害其成功率。在五个额外的基准测试（数学推理、代码生成和问答）和各种最先进的基础模型上的广泛实验验证了我们方法的广泛适用性和鲁棒性。代码可在https://github.com/LINs-lab/SupervisorAgent获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Multi-Agent Systems (MAS) excel at complex tasks, their growingautonomy with operational complexity often leads to critical inefficiencies,such as excessive token consumption and failures arising from misinformation.Existing methods primarily focus on post-hoc failure attribution, lackingproactive, real-time interventions to enhance robustness and efficiency. Tothis end, we introduce SupervisorAgent, a lightweight and modular framework forruntime, adaptive supervision that operates without altering the base agent'sarchitecture. Triggered by an LLM-free adaptive filter, SupervisorAgentintervenes at critical junctures to proactively correct errors, guideinefficient behaviors, and purify observations. On the challenging GAIAbenchmark, SupervisorAgent reduces the token consumption of the Smolagentframework by an average of 29.45% without compromising its success rate.Extensive experiments across five additional benchmarks (math reasoning, codegeneration, and question answering) and various SoTA foundation models validatethe broad applicability and robustness of our approach. The code is availableat https://github.com/LINs-lab/SupervisorAgent.</description>
      <author>example@mail.com (Fulin Lin, Shaowen Chen, Ruishan Fang, Hongwei Wang, Tao Lin)</author>
      <guid isPermaLink="false">2510.26585v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders</title>
      <link>http://arxiv.org/abs/2510.26411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究通过应用医学稀疏自编码器(MedSAEs)提高医疗视觉中的机制可解释性，在保持高性能的同时增加了AI模型的透明度。&lt;h4&gt;背景&lt;/h4&gt;人工智能在医疗保健领域需要既准确又可解释的模型，特别是在医疗视觉分析方面。&lt;h4&gt;目的&lt;/h4&gt;通过将MedSAEs应用于MedCLIP的潜在空间来提高医疗视觉中的机制可解释性，MedCLIP是一种在胸部X光片和报告上训练的视觉-语言模型。&lt;h4&gt;方法&lt;/h4&gt;提出一个结合相关性指标、熵分析和通过MedGEMMA基础模型进行自动神经元命名的评估框架，并在CheXpert数据集上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;MedSAE神经元比原始MedCLIP特征实现了更高的单语义性和可解释性。&lt;h4&gt;结论&lt;/h4&gt;研究结果连接了高性能医疗AI和透明度，为发展临床可靠表示提供了可扩展的步骤。&lt;h4&gt;翻译&lt;/h4&gt;医疗保健中的人工智能需要准确且可解释的模型。我们通过将医学稀疏自编码器(MedSAEs)应用于MedCLIP的潜在空间，推进了医疗视觉中的机制可解释性，MedCLIP是一种在胸部X光片和报告上训练的视觉-语言模型。为了量化可解释性，我们提出一个结合相关性指标、熵分析和通过MedGEMMA基础模型进行自动神经元命名的评估框架。在CheXpert数据集上的实验表明，MedSAE神经元比原始MedCLIP特征具有更高的单语义性和可解释性。我们的研究结果连接了高性能医疗AI和透明度，为向临床可靠表示发展提供了可扩展的步骤。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence in healthcare requires models that are accurate andinterpretable. We advance mechanistic interpretability in medical vision byapplying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,a vision-language model trained on chest radiographs and reports. To quantifyinterpretability, we propose an evaluation framework that combines correlationmetrics, entropy analyzes, and automated neuron naming via the MedGEMMAfoundation model. Experiments on the CheXpert dataset show that MedSAE neuronsachieve higher monosemanticity and interpretability than raw MedCLIP features.Our findings bridge high-performing medical AI and transparency, offering ascalable step toward clinically reliable representations.</description>
      <author>example@mail.com (Riccardo Renzulli, Colas Lepoutre, Enrico Cassano, Marco Grangetto)</author>
      <guid isPermaLink="false">2510.26411v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Towards Explainable and Reliable AI in Finance</title>
      <link>http://arxiv.org/abs/2510.26353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了金融领域可解释和可靠AI的三种方法，包括Time-LLM模型使用提示避免错误预测、结合基础模型与可靠性估计器过滤不可靠预测，以及使用符号推理编码领域规则提供透明解释。&lt;h4&gt;背景&lt;/h4&gt;金融预测越来越多地使用大型神经网络模型，但这些模型的透明度低，对信任和监管合规性提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出几种金融领域可解释和可靠AI的方法，以解决模型的透明度和可靠性问题。&lt;h4&gt;方法&lt;/h4&gt;1) 描述Time-LLM（时间序列基础模型）如何使用提示来避免错误的方向性预测；2) 展示将时间序列预测的基础模型与可靠性估计器结合可以过滤不可靠的预测；3) 主张使用符号推理来编码领域规则，以提供透明的解释。&lt;h4&gt;主要发现&lt;/h4&gt;这些方法强调只执行既可靠又可解释的预测；在股票和加密货币数据上的实验表明，该架构减少了误报并支持选择性执行。&lt;h4&gt;结论&lt;/h4&gt;通过整合预测性能、可靠性估计和基于规则的推理，该框架推进了透明和可审计的金融AI系统。&lt;h4&gt;翻译&lt;/h4&gt;金融预测越来越多地使用大型神经网络模型，但其不透明性对信任和监管合规性提出了挑战。我们提出了几种金融领域可解释和可靠AI的方法。首先，我们描述了Time-LLM（时间序列基础模型）如何使用提示来避免错误的方向性预测。其次，我们展示了将时间序列预测的基础模型与可靠性估计器结合可以过滤不可靠的预测。第三，我们主张使用符号推理来编码领域规则以提供透明的解释。这些方法强调只执行既可靠又可解释的预测。在股票和加密货币数据上的实验表明，该架构减少了误报并支持选择性执行。通过整合预测性能、可靠性估计和基于规则的推理，我们的框架推进了透明和可审计的金融AI系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial forecasting increasingly uses large neural network models, buttheir opacity raises challenges for trust and regulatory compliance. We presentseveral approaches to explainable and reliable AI in finance. \emph{First}, wedescribe how Time-LLM, a time series foundation model, uses a prompt to avoid awrong directional forecast. \emph{Second}, we show that combining foundationmodels for time series forecasting with a reliability estimator can filter ourunreliable predictions. \emph{Third}, we argue for symbolic reasoning encodingdomain rules for transparent justification. These approaches shift emphasizeexecuting only forecasts that are both reliable and explainable. Experiments onequity and cryptocurrency data show that the architecture reduces falsepositives and supports selective execution. By integrating predictiveperformance with reliability estimation and rule-based reasoning, our frameworkadvances transparent and auditable financial AI systems.</description>
      <author>example@mail.com (Albi Isufaj, Pablo Mollá, Helmut Prendinger)</author>
      <guid isPermaLink="false">2510.26353v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts</title>
      <link>http://arxiv.org/abs/2510.26186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the Thirty-Ninth Conference on Neural Information  Processing Systems (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ConceptScope框架，用于通过发现和量化人类可解释的概念来分析视觉数据集中的偏差。&lt;h4&gt;背景&lt;/h4&gt;数据集偏差在机器学习数据集中普遍存在，但系统性识别这些偏差具有挑战性，因为需要昂贵的、细粒度的属性标注。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展且自动化的框架来分析视觉数据集，通过发现和量化人类可解释的概念来识别和量化数据集偏差。&lt;h4&gt;方法&lt;/h4&gt;ConceptScope使用在视觉基础模型表示上训练的稀疏自编码器来发现和量化人类可解释的概念。根据概念与类标签的语义相关性和统计相关性，将概念分为目标、上下文和偏差类型，从而实现类级别的数据集特征描述、偏差识别和鲁棒性评估。&lt;h4&gt;主要发现&lt;/h4&gt;ConceptScope能够捕获广泛的视觉概念，包括物体、纹理、背景、面部属性、情绪和动作。概念激活产生的空间归因与语义上有意义的图像区域一致。该框架能够可靠地检测已知偏差（如Waterbirds中的背景偏差）并发现先前未标注的偏差（如ImageNet中共现的物体）。&lt;h4&gt;结论&lt;/h4&gt;ConceptScope为数据集审计和模型诊断提供了实用的工具。&lt;h4&gt;翻译&lt;/h4&gt;数据集偏差在机器学习数据集无处不在，其中数据点偏向于某些概念。然而，在没有昂贵的细粒度属性标注的情况下，系统性地识别这些偏差具有挑战性。我们提出了ConceptScope，这是一个可扩展且自动化的框架，用于通过使用在视觉基础模型表示上训练的稀疏自编码器发现和量化人类可解释的概念来分析视觉数据集。ConceptScope根据概念与类标签的语义相关性和统计相关性将概念分为目标、上下文和偏差类型，从而通过基于概念的子分组实现类级别的数据集特征描述、偏差识别和鲁棒性评估。通过与标注数据集的比较，我们验证了ConceptScope能够捕获广泛的视觉概念，包括物体、纹理、背景、面部属性、情绪和动作。此外，我们表明概念激活产生的空间归因与语义上有意义的图像区域一致。ConceptScope可靠地检测了已知偏差（例如Waterbirds中的背景偏差）并发现了先前未标注的偏差（例如ImageNet中共现的物体），为数据集审计和模型诊断提供了实用的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dataset bias, where data points are skewed to certain concepts, is ubiquitousin machine learning datasets. Yet, systematically identifying these biases ischallenging without costly, fine-grained attribute annotations. We presentConceptScope, a scalable and automated framework for analyzing visual datasetsby discovering and quantifying human-interpretable concepts using SparseAutoencoders trained on representations from vision foundation models.ConceptScope categorizes concepts into target, context, and bias types based ontheir semantic relevance and statistical correlation to class labels, enablingclass-level dataset characterization, bias identification, and robustnessevaluation through concept-based subgrouping. We validate that ConceptScopecaptures a wide range of visual concepts, including objects, textures,backgrounds, facial attributes, emotions, and actions, through comparisons withannotated datasets. Furthermore, we show that concept activations producespatial attributions that align with semantically meaningful image regions.ConceptScope reliably detects known biases (e.g., background bias inWaterbirds) and uncovers previously unannotated ones (e.g, co-occurring objectsin ImageNet), offering a practical tool for dataset auditing and modeldiagnostics.</description>
      <author>example@mail.com (Jinho Choi, Hyesu Lim, Steffen Schneider, Jaegul Choo)</author>
      <guid isPermaLink="false">2510.26186v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Deep Neural Watermarking for Robust Copyright Protection in 3D Point Clouds</title>
      <link>http://arxiv.org/abs/2510.27533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于3D点云版权保护和所有权验证的鲁棒深度神经水印框架，通过奇异值分解将二进制水印嵌入到3D点云块中，并利用PointNet++神经网络架构实现水印的可靠提取。&lt;h4&gt;背景&lt;/h4&gt;随着数字媒体中三维内容的快速增长，知识产权保护变得至关重要。与传统的图像或视频不同，3D点云在版权执行方面面临独特挑战，因为它们特别容易受到各种几何和非几何攻击的影响，这些攻击可以轻易降低或移除传统水印信号。&lt;h4&gt;目的&lt;/h4&gt;解决3D点云在版权保护中的挑战，提出一种能够抵抗各种攻击的鲁棒深度神经水印框架，用于3D点云的版权保护和所有权验证。&lt;h4&gt;方法&lt;/h4&gt;使用奇异值分解(SVD)将二进制水印嵌入到3D点云块的奇异值中，并利用PointNet++神经网络架构的深度学习提取能力。训练网络以在数据经过旋转、缩放、噪声、裁剪和信号失真等各种攻击后仍能可靠提取水印。&lt;h4&gt;主要发现&lt;/h4&gt;在ModelNet40数据集上的验证表明，深度学习提取方法在具有挑战性的条件下显著优于传统的SVD技术。对于实验中最严重的几何失真——裁剪(70%)攻击，深度学习方法实现了0.83的位准确度和0.80的交并比(IoU)，而传统SVD方法仅实现了0.58的位准确度和0.26的IoU。&lt;h4&gt;结论&lt;/h4&gt;该方法即使在严重失真条件下也能实现卓越的水印恢复并保持高保真度，证明了其在实际应用中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;随着数字媒体中三维内容的快速增长，知识产权保护变得至关重要。与传统的图像或视频不同，3D点云在版权执行方面面临独特挑战，因为它们特别容易受到各种几何和非几何攻击的影响，这些攻击可以轻易降低或移除传统水印信号。在本文中，我们通过提出一种用于3D点云版权保护和所有权验证的鲁棒深度神经水印框架来解决这些挑战。我们的方法使用奇异值分解(SVD)将二进制水印嵌入到3D点云块的奇异值中，并利用PointNet++神经网络架构的深度学习提取能力。网络经过训练，即使在数据经过旋转、缩放、噪声、裁剪和信号失真等各种攻击后也能可靠提取水印。我们使用公开的ModelNet40数据集验证了我们的方法，证明在具有挑战性的条件下，基于深度学习的提取显著优于传统的SVD技术。我们的实验评估表明，基于深度学习的提取方法显著优于现有的SVD方法，深度学习在裁剪(70%)攻击(实验中最严重的几何失真)下实现了0.83的位准确度和0.80的交并比(IoU)，而SVD仅实现了0.58的位准确度和0.26的IoU。这证明了我们的方法即使在严重失真条件下也能实现卓越的水印恢复并保持高保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.33166/AETiC.2025.05.002&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The protection of intellectual property has become critical due to the rapidgrowth of three-dimensional content in digital media. Unlike traditional imagesor videos, 3D point clouds present unique challenges for copyright enforcement,as they are especially vulnerable to a range of geometric and non-geometricattacks that can easily degrade or remove conventional watermark signals. Inthis paper, we address these challenges by proposing a robust deep neuralwatermarking framework for 3D point cloud copyright protection and ownershipverification. Our approach embeds binary watermarks into the singular values of3D point cloud blocks using spectral decomposition, i.e. Singular ValueDecomposition (SVD), and leverages the extraction capabilities of Deep Learningusing PointNet++ neural network architecture. The network is trained toreliably extract watermarks even after the data undergoes various attacks suchas rotation, scaling, noise, cropping and signal distortions. We validated ourmethod using the publicly available ModelNet40 dataset, demonstrating that deeplearning-based extraction significantly outperforms traditional SVD-basedtechniques under challenging conditions. Our experimental evaluationdemonstrates that the deep learning-based extraction approach significantlyoutperforms existing SVD-based methods with deep learning achieving bitwiseaccuracy up to 0.83 and Intersection over Union (IoU) of 0.80, compared to SVDachieving a bitwise accuracy of 0.58 and IoU of 0.26 for the Crop (70%) attack,which is the most severe geometric distortion in our experiment. Thisdemonstrates our method's ability to achieve superior watermark recovery andmaintain high fidelity even under severe distortions.</description>
      <author>example@mail.com (Khandoker Ashik Uz Zaman, Mohammad Zahangir Alam, Mohammed N. M. Ali, Mahdi H. Miraz)</author>
      <guid isPermaLink="false">2510.27533v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics</title>
      <link>http://arxiv.org/abs/2510.27033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种整合全景图像和3D点云信息的神经符号框架，结合神经感知与符号推理，用于解决视觉推理中的细粒度空间推理问题，在拥挤的人造环境中表现出优越性能和可靠性，同时保持轻量级设计。&lt;h4&gt;背景&lt;/h4&gt;视觉推理，特别是空间推理，是一个具有挑战性的认知任务，需要理解物体关系及其在复杂环境中的交互。现有的视觉语言模型在感知任务上表现出色，但在细粒度空间推理方面存在困难，因为它们采用隐式、基于相关性的推理且仅依赖图像。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的神经符号框架，整合全景图像和3D点云信息，结合神经感知与符号推理，以明确建模空间和逻辑关系。&lt;h4&gt;方法&lt;/h4&gt;框架由感知模块(用于检测实体和提取属性)和推理模块(构建结构化的场景图以支持精确、可解释的查询)组成。&lt;h4&gt;主要发现&lt;/h4&gt;在JRDB-Reasoning数据集上评估，该方法在拥挤的人造环境中表现出优越的性能和可靠性，同时保持轻量级设计。&lt;h4&gt;结论&lt;/h4&gt;该框架适用于机器人和具身AI应用。&lt;h4&gt;翻译&lt;/h4&gt;视觉推理，特别是空间推理，是一项具有挑战性的认知任务，需要理解物体关系及其在复杂环境中的交互，尤其是在机器人领域。现有的视觉语言模型在感知任务上表现出色，但由于其隐式、基于相关性的推理和仅依赖图像的能力，在细粒度空间推理方面存在困难。我们提出了一种新的神经符号框架，整合全景图像和3D点云信息，结合神经感知与符号推理，以明确建模空间和逻辑关系。我们的框架由感知模块(用于检测实体和提取属性)和推理模块(构建结构化的场景图以支持精确、可解释的查询)组成。在JRDB-Reasoning数据集上的评估表明，该方法在拥挤的人造环境中表现出优越的性能和可靠性，同时保持轻量级设计，适用于机器人和具身AI应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人在复杂环境中进行细粒度空间推理的问题，特别是在理解物体和人类之间精确空间关系方面的挑战。这个问题很重要，因为空间推理是机器人导航和交互的基础能力，在人类建造的拥挤环境中，机器人需要准确理解多个实体之间的空间关系来完成各种任务，而现有模型在这方面存在明显不足，导致在机器人、导航和具身AI应用中可靠性不高。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有视觉-语言模型(VLMs)在空间推理方面的局限性，包括其隐式推理方式、仅依赖2D图像而忽略3D信息、以及在相对人类定位等任务中的表现不佳。基于这些分析，作者设计了一个结合神经感知与符号推理的框架，借鉴了基础视觉-语言主干网络用于特征提取，以及场景图概念来表示实体关系。通过整合全景图像和3D点云信息，作者构建了一个能够进行显式几何和逻辑结构推理的系统，从而克服了现有模型的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合神经感知与符号推理，利用全景图像和3D点云信息进行更准确、可解释的空间推理，通过构建场景图作为中间推理层来支持精确查询。整体流程分为两个主要部分：感知部分和推理部分。感知部分包括特征提取模块(检测实体并提取属性)和投影模块(整合语义特征与几何关系)；推理部分是图搜索模块，包含句子解析(将查询转换为结构化表示)和搜索算法(在场景图上查找答案)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 多模态轻量级框架，整合全景图像和3D点云信息，参数少(1.3B)且适合机器人应用；2) 新颖的显式符号推理，通过构建场景图减少推理错误；3) 在拥挤环境中表现出优越的性能和可靠性。相比之前的工作，不同之处在于：结合了2D图像和3D点云信息而非仅依赖2D图像；使用显式符号推理结构而非隐式统计相关性；轻量级设计同时实现高级推理能力；特别擅长处理细粒度空间关系和复杂查询。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种轻量级多模态神经符号框架，通过整合全景图像和3D点云信息，结合神经感知与符号推理，显著提高了机器人在复杂环境中进行细粒度空间推理的能力，同时保持了模型的轻量化和可解释性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual reasoning, particularly spatial reasoning, is a challenging cognitivetask that requires understanding object relationships and their interactionswithin complex environments, especially in robotics domain. Existingvision_language models (VLMs) excel at perception tasks but struggle withfine-grained spatial reasoning due to their implicit, correlation-drivenreasoning and reliance solely on images. We propose a novel neuro_symbolicframework that integrates both panoramic-image and 3D point cloud information,combining neural perception with symbolic reasoning to explicitly model spatialand logical relationships. Our framework consists of a perception module fordetecting entities and extracting attributes, and a reasoning module thatconstructs a structured scene graph to support precise, interpretable queries.Evaluated on the JRDB-Reasoning dataset, our approach demonstrates superiorperformance and reliability in crowded, human_built environments whilemaintaining a lightweight design suitable for robotics and embodied AIapplications.</description>
      <author>example@mail.com (Simindokht Jahangard, Mehrzad Mohammadi, Abhinav Dhall, Hamid Rezatofighi)</author>
      <guid isPermaLink="false">2510.27033v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Active transfer learning for structural health monitoring</title>
      <link>http://arxiv.org/abs/2510.27525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种贝叶斯框架用于基于群体的结构健康监测中的域适应，结合主动采样策略提高数据效率，减少标记数据需求，降低结构运营成本。&lt;h4&gt;背景&lt;/h4&gt;用于训练结构健康监测系统的数据通常昂贵且难以获取，特别是标记数据。基于群体的结构健康监测试图通过利用多个结构的数据解决这一问题，但不同结构的数据分布差异可能导致传统机器学习方法产生较大泛化误差。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够利用有限标记目标数据改进无监督域适应映射的贝叶斯框架，并将其与主动采样策略集成，指导检查选择最有信息量的观察结果进行标记。&lt;h4&gt;方法&lt;/h4&gt;使用域适应技术对齐数据分布，提出贝叶斯框架用于群体结构健康监测中的域适应，集成主动采样策略指导检查，并在实验桥梁群体上评估该方法的有效性，包括多种损伤状态和环境条件的数据。&lt;h4&gt;主要发现&lt;/h4&gt;结合迁移学习和主动学习可以在标记稀缺场景中提高学习分类模型的数据效率，对数据驱动的结构运营和维护有重要影响。&lt;h4&gt;结论&lt;/h4&gt;通过采用所提出的方法，可以减少结构运营寿命内的检查次数，从而降低运营成本，同时保持有效的结构健康监测。&lt;h4&gt;翻译&lt;/h4&gt;用于训练结构健康监测系统的数据通常昂贵且/或不切实际，特别是对于标记数据。基于群体的结构健康监测旨在通过利用来自多个结构的数据来解决这一限制。然而，来自不同结构的数据将遵循不同的分布，可能导致通过传统机器学习方法学习的模型产生较大的泛化误差。为了解决这个问题，可以采用迁移学习--以域适应的形式--来对齐数据分布。大多数先前的方法只考虑了无监督域适应，其中没有可用的标记目标数据；它们没有考虑如何将这些技术整合到在线框架中--随着在整个监测过程中获得标签而进行更新。本文提出了用于群体结构健康监测中域适应的贝叶斯框架，可以使用有限数量的标记目标数据改进无监督域适应映射。此外，该模型被集成到主动采样策略中，指导检查选择最有信息量的观察结果进行标记--从而进一步减少学习目标分类器所需的标记数据。该方法的有效性在一组实验桥梁群体上进行了评估。具体而言，该群体包括对应于多种损伤状态的数据，以及一套全面的环境条件。研究发现，结合迁移学习和主动学习可以在标记稀缺场景中提高学习分类模型时的数据效率。这一结果对数据驱动的结构运营和维护有影响，表明可以在结构的运营寿命内减少检查次数--从而降低运营成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.ymssp.2025.113260&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data for training structural health monitoring (SHM) systems are oftenexpensive and/or impractical to obtain, particularly for labelled data.Population-based SHM (PBSHM) aims to address this limitation by leveraging datafrom multiple structures. However, data from different structures will followdistinct distributions, potentially leading to large generalisation errors formodels learnt via conventional machine learning methods. To address this issue,transfer learning -- in the form of domain adaptation (DA) -- can be used toalign the data distributions. Most previous approaches have only considered\emph{unsupervised} DA, where no labelled target data are available; they donot consider how to incorporate these technologies in an online framework --updating as labels are obtained throughout the monitoring campaign. This paperproposes a Bayesian framework for DA in PBSHM, that can improve unsupervised DAmappings using a limited quantity of labelled target data. In addition, thismodel is integrated into an active sampling strategy to guide inspections toselect the most informative observations to label -- leading to furtherreductions in the required labelled data to learn a target classifier. Theeffectiveness of this methodology is evaluated on a population of experimentalbridges. Specifically, this population includes data corresponding to severaldamage states, as well as, a comprehensive set of environmental conditions. Itis found that combining transfer learning and active learning can improve dataefficiency when learning classification models in label-scarce scenarios. Thisresult has implications for data-informed operation and maintenance ofstructures, suggesting a reduction in inspections over the operational lifetimeof a structure -- and therefore a reduction in operational costs -- can beachieved.</description>
      <author>example@mail.com (J. Poole, N. Dervilis, K. Worden, P. Gardner, V. Giglioni, R. S. Mills, A. J. Hughes)</author>
      <guid isPermaLink="false">2510.27525v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>pDANSE: Particle-based Data-driven Nonlinear State Estimation from Nonlinear Measurements</title>
      <link>http://arxiv.org/abs/2510.27503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 10 figures, under review at IEEE Transactions on Signal  Processing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种粒子数据驱动非线性状态估计方法(pDANSE)，用于处理状态转换模型未知的模型自由过程，通过循环神经网络和基于重参数化技巧的粒子采样方法处理非线性测量系统，实现了高效的状态估计。&lt;h4&gt;背景&lt;/h4&gt;传统数据驱动非线性状态估计方法(DANSE)在处理线性测量系统时可获得状态后验的闭式解，但面对非线性测量系统时这种方法不再适用，需要开发新的解决方案。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理非线性测量系统的状态估计方法，避免使用计算密集的顺序蒙特卡洛(SMC)和祖先采样，并支持在有标签和无标签数据下分别进行半监督和无监督学习。&lt;h4&gt;方法&lt;/h4&gt;使用循环神经网络(RNN)提供高斯先验参数描述模型自由过程状态，采用基于重参数化技巧的粒子采样方法处理非线性测量，估计状态后验的二阶统计量，并在随机Lorenz-63系统上验证性能。&lt;h4&gt;主要发现&lt;/h4&gt;pDANSE能有效利用顺序测量避免计算密集方法，在立方非线性、相机模型非线性(无监督学习)以及半波整流非线性、笛卡尔到球面非线性(半监督学习)四种测量系统上均表现良好，性能与具有完整状态转换模型知识的粒子滤波器相当。&lt;h4&gt;结论&lt;/h4&gt;pDANSE方法成功解决了模型自由过程的非线性测量状态估计问题，通过创新方法实现了高效准确的状态估计，为未知模型系统的状态估计提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们考虑设计一种数据驱动的非线性状态估计方法，该方法使用（带噪声的）非线性测量值来处理底层状态转换模型未知的过程。这样的过程被称为无模型过程。循环神经网络提供高斯先验的参数，该参数使用给定时间点的所有先前测量值来描述无模型过程的状态。在DANSE的情况下，测量系统是线性的，从而得到状态后验的闭式解。然而，非线性测量系统的存在使得闭式解不可行。相反，使用在时间点观察到的非线性测量值来计算状态后验的二阶统计量。我们使用基于重参数化技巧的粒子采样方法处理非线性测量，并估计状态后验的二阶统计量。所提出的方法被称为基于粒子的DANSE(pDANSE)。pDANSE的RNN有效利用顺序测量，避免了使用计算密集的顺序蒙特卡洛和/或祖先采样。我们描述了pDANSE的半监督学习方法，在没有标签数据的情况下过渡到无监督学习。使用随机Lorenz-63系统作为基准过程，我们实验性地展示了四种非线性测量系统的状态估计性能。我们探索了立方非线性和相机模型非线性，这里使用无监督学习；然后我们探索了半波整流非线性和笛卡尔到球面非线性，这里使用半监督学习。状态估计的性能被证明与具有完整Lorenz-63系统状态转换模型知识的粒子滤波器相比具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider the problem of designing a data-driven nonlinear state estimation(DANSE) method that uses (noisy) nonlinear measurements of a process whoseunderlying state transition model (STM) is unknown. Such a process is referredto as a model-free process. A recurrent neural network (RNN) providesparameters of a Gaussian prior that characterize the state of the model-freeprocess, using all previous measurements at a given time point. In the case ofDANSE, the measurement system was linear, leading to a closed-form solution forthe state posterior. However, the presence of a nonlinear measurement systemrenders a closed-form solution infeasible. Instead, the second-order statisticsof the state posterior are computed using the nonlinear measurements observedat the time point. We address the nonlinear measurements using areparameterization trick-based particle sampling approach, and estimate thesecond-order statistics of the state posterior. The proposed method is referredto as particle-based DANSE (pDANSE). The RNN of pDANSE uses sequentialmeasurements efficiently and avoids the use of computationally intensivesequential Monte-Carlo (SMC) and/or ancestral sampling. We describe thesemi-supervised learning method for pDANSE, which transitions to unsupervisedlearning in the absence of labeled data. Using a stochastic Lorenz-$63$ systemas a benchmark process, we experimentally demonstrate the state estimationperformance for four nonlinear measurement systems. We explore cubicnonlinearity and a camera-model nonlinearity where unsupervised learning isused; then we explore half-wave rectification nonlinearity andCartesian-to-spherical nonlinearity where semi-supervised learning is used. Theperformance of state estimation is shown to be competitive vis-\`a-vis particlefilters that have complete knowledge of the STM of the Lorenz-$63$ system.</description>
      <author>example@mail.com (Anubhab Ghosh, Yonina C. Eldar, Saikat Chatterjee)</author>
      <guid isPermaLink="false">2510.27503v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>UNILocPro: Unified Localization Integrating Model-Based Geometry and Channel Charting</title>
      <link>http://arxiv.org/abs/2510.27394v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UNILocPro的统一定位框架，结合基于模型的定位和信道映射方法，用于处理混合视距/非视距场景。该框架通过自适应激活两种方法并使用多种损失函数进行无监督学习，显著提高了定位精度。同时提出了低复杂度的UNILoc实现，大幅降低训练复杂度而性能仅有轻微下降。&lt;h4&gt;背景&lt;/h4&gt;在混合视距(LoS)/非视距(NLoS)场景中，单一的定位方法难以满足高精度需求。基于模型的定位方法和信道映射(Channel Charting, CC)方法各有优势，但单独使用时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一框架，有效结合基于模型的定位和信道映射方法，提高混合LoS/NLoS场景下的定位精度，同时降低训练复杂度。&lt;h4&gt;方法&lt;/h4&gt;1. 提出UNILocPro框架，根据LoS/NLoS识别自适应激活基于模型和基于CC的方法；2. 利用基于模型方法的信息训练CC模型；3. 使用多种损失函数：成对距离损失、三元组损失(如果有时间戳)、基于LoS的损失和基于最优传输(OT)的损失；4. 提出低复杂度实现UNILoc，使用自生成标签训练CC模型，避免迭代Sinkhorn更新。&lt;h4&gt;主要发现&lt;/h4&gt;1. 统一框架比单独的基于模型和基于CC的方法显著提高了定位精度；2. 带有时间戳的UNILocPro性能与完全监督的指纹识别相当，无需标记训练数据；3. UNILoc大幅降低训练复杂度，性能仅有轻微下降。&lt;h4&gt;结论&lt;/h4&gt;UNILocPro框架有效结合了两种定位方法的优势，在混合LoS/NLoS场景中实现了高精度定位。UNLoc作为低复杂度实现，在实际应用中具有更好的实用性。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种统一的定位框架（称为UNILocPro），该框架集成了基于模型的定位和信道映射（CC）方法，用于处理混合视距（LoS）/非视距（NLoS）场景。具体而言，基于LoS/NLoS识别，在基于模型和基于CC的方法之间进行自适应激活。针对无监督学习，利用基于模型方法获得的信息来训练CC模型，联合使用成对距离损失（涉及新的不相似度度量设计）、三元组损失（如果有时间戳）、基于LoS的损失和基于最优传输（OT）的损失，以保持全局几何结构。为了减少UNILocPro的训练复杂度，我们提出了一种低复杂度实现（称为UNILoc），其中CC模型使用通过单个预训练OT转换生成的自生成标签进行训练，避免了OT损失计算中涉及的迭代Sinkhorn更新。大量的数值实验表明，所提出的统一框架比基于模型和基于CC的方法显著提高了定位精度。值得注意的是，带有时间戳的UNILocPro性能与完全监督的指纹识别相当，尽管它不使用标记的训练数据。研究还表明，低复杂度的UNLoc可以显著减少训练复杂度，而性能仅有轻微下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a unified localization framework (called UNILocPro)that integrates model-based localization and channel charting (CC) for mixedline-of-sight (LoS)/non-line-of-sight (NLoS) scenarios. Specifically, based onLoS/NLoS identification, an adaptive activation between the model-based andCC-based methods is conducted. Aiming for unsupervised learning, informationobtained from the model-based method is utilized to train the CC model, where apairwise distance loss (involving a new dissimilarity metric design), a tripletloss (if timestamps are available), a LoS-based loss, and an optimal transport(OT)-based loss are jointly employed such that the global geometry can be wellpreserved. To reduce the training complexity of UNILocPro, we propose alow-complexity implementation (called UNILoc), where the CC model is trainedwith self-generated labels produced by a single pre-training OT transformation,which avoids iterative Sinkhorn updates involved in the OT-based losscomputation. Extensive numerical experiments demonstrate that the proposedunified frameworks achieve significantly improved positioning accuracy comparedto both model-based and CC-based methods. Notably, UNILocPro with timestampsattains performance on par with fully-supervised fingerprinting despiteoperating without labelled training data. It is also shown that thelow-complexity UNILoc can substantially reduce training complexity with onlymarginal performance degradation.</description>
      <author>example@mail.com (Yuhao Zhang, Guangjin Pan, Musa Furkan Keskin, Ossi Kaltiokallio, Mikko Valkama, Henk Wymeersch)</author>
      <guid isPermaLink="false">2510.27394v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Soft Task-Aware Routing of Experts for Equivariant Representation Learning</title>
      <link>http://arxiv.org/abs/2510.27222v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了软任务感知路由（STAR）策略，用于解决等变表示学习和不变表示学习联合训练中的冗余特征学习问题。STAR通过将投影头建模为专家，促使它们专门捕捉共享信息或任务特定信息，从而减少冗余并提高模型效率。&lt;h4&gt;背景&lt;/h4&gt;等变表示学习旨在捕捉输入变换在表示空间中引起的变异，而不变表示学习通过忽略这些变换来编码语义信息。最近研究表明，联合学习这两种表示对下游任务有益，通常通过使用单独的投影头实现。&lt;h4&gt;目的&lt;/h4&gt;解决不变学习和等变学习联合训练中信息共享被忽略的问题，减少冗余特征学习，提高模型容量的利用效率。&lt;h4&gt;方法&lt;/h4&gt;引入软任务感知路由（STAR）策略，将投影头建模为专家，促使它们专门捕捉共享信息或任务特定信息。&lt;h4&gt;主要发现&lt;/h4&gt;STAR策略使不变嵌入和等变嵌入之间的标准相关性降低，减少了冗余特征学习。实验结果表明，STAR在各种迁移学习任务中实现了持续改进。&lt;h4&gt;结论&lt;/h4&gt;STAR通过有效的任务感知路由策略，解决了等变和不变表示学习中的冗余问题，提高了模型效率，在各种迁移学习任务中展现了优越性能。&lt;h4&gt;翻译&lt;/h4&gt;等变表示学习旨在捕捉输入变换在表示空间中引起的变异，而不变表示学习通过忽略这些变换来编码语义信息。最近研究表明，联合学习这两种表示通常对下游任务有益，通常通过使用单独的投影头实现。然而，这种设计忽略了不变学习和等变学习之间共享的信息，导致冗余的特征学习和模型容量的低效利用。为此，我们引入了软任务感知路由（STAR），这是一种针对投影头的路由策略，将它们建模为专家。STAR促使专家专门捕捉共享信息或任务特定信息，从而减少冗余的特征学习。我们通过观察不变嵌入和等变嵌入之间较低的标准相关性来验证这一效果。实验结果表明在各种迁移学习任务中都有持续改进。代码可在https://github.com/YonseiML/star获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决不变表示学习和等变表示学习之间的冗余特征学习问题。当使用两个独立投影头分别处理这两个任务时，它们会冗余地捕获共享信息，导致模型容量使用效率低下。这个问题很重要，因为不变和等变学习实际上是相互依赖的，而非完全独立，传统方法忽略这种依赖关系会导致模型性能受限，同时保留语义内容(不变性)和变换相关信息(等变性)对许多视觉任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过'陨石坑错觉'例子识别了不变和等变学习间的相互依赖关系，意识到传统双分支方法会导致冗余特征学习。为此，他们设计了两种形式的Soft Task-Aware Routing (STAR)：单一共享投影头(添加一个共享投影头提供共同嵌入)和MMoE投影(采用多门控混合专家架构动态分配专家)。作者借鉴了现有的混合专家框架，特别是MMoE架构，但创新性地将其仅限制在预训练阶段的投影头中，解决了传统MoE模型在跨任务转移中的可转移性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过专家路由显式协调共享信息和任务特定信息，减少冗余特征学习。整体流程为：1)为输入图像生成两个增强视图；2)用共享编码器提取潜在表示；3)通过STAR投影模块生成不变和等变嵌入——SS版本使用三个专家(不变、等变和共享)输出相加，MMoE版本使用共享专家和任务特定路由器动态分配权重；4)等变学习通过预测器预测变换后的嵌入；5)使用对比损失训练模型；预训练后仅保留编码器用于下游任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)揭示不变和等变学习间的内在依赖性；2)提出STAR策略协调共享与任务特定信息；3)设计STAR的两种实现形式(单一共享投影和MMoE投影)；4)创新性地将MMoE限制在预训练阶段。相比EquiMod等传统方法，STAR显式建模任务间共享信息，减少冗余特征学习，能根据输入动态分配专家而非使用固定投影头，并在多种下游任务上取得更好性能，同时降低了不变和等变嵌入间的典型相关性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了Soft Task-Aware Routing策略，通过专家路由显式协调不变和等变表示学习中的共享与任务特定信息，有效减少了冗余特征学习，并在多种下游任务上提升了表示学习的性能和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Equivariant representation learning aims to capture variations induced byinput transformations in the representation space, whereas invariantrepresentation learning encodes semantic information by disregarding suchtransformations. Recent studies have shown that jointly learning both types ofrepresentations is often beneficial for downstream tasks, typically byemploying separate projection heads. However, this design overlooks informationshared between invariant and equivariant learning, which leads to redundantfeature learning and inefficient use of model capacity. To address this, weintroduce Soft Task-Aware Routing (STAR), a routing strategy for projectionheads that models them as experts. STAR induces the experts to specialize incapturing either shared or task-specific information, thereby reducingredundant feature learning. We validate this effect by observing lowercanonical correlations between invariant and equivariant embeddings.Experimental results show consistent improvements across diverse transferlearning tasks. The code is available at https://github.com/YonseiML/star.</description>
      <author>example@mail.com (Jaebyeong Jeon, Hyeonseo Jang, Jy-yong Sohn, Kibok Lee)</author>
      <guid isPermaLink="false">2510.27222v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Functional Analysis of Loss-development Patterns in P&amp;C Insurance</title>
      <link>http://arxiv.org/abs/2510.27204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages. Keywords: loss development; loss reserving; incremental  loss ratios; unsupervised learning; functional data; functional depth;  outlier detection; IBNR&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文使用函数数据分析方法分析NAIC P计划损失三角形的损失发展，提出了一种基于偏最小二乘回归和函数自举的概率预测框架，能够提供更准确的函数预测区间。&lt;h4&gt;背景&lt;/h4&gt;NAIC P计划损失三角形是保险业常用的损失发展分析工具，传统方法如链梯法在处理不确定性方面存在局限。&lt;h4&gt;目的&lt;/h4&gt;研究增量损失比率的发展模式，识别异常曲线，并开发一种更准确的概率预测方法来估计未来损失发展。&lt;h4&gt;方法&lt;/h4&gt;采用函数数据分析方法，将数据视为3300多条增量损失比率曲线；使用函数数据深度研究发展模式；提出基于偏最小二乘回归的函数模型来完成部分发展的曲线；结合函数自举量化不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;基于公司特定协变量可以识别发展模式的相似性和差异；能够识别异常的增量损失比率曲线；所提出的方法相比链梯法具有更好的概率评分。&lt;h4&gt;结论&lt;/h4&gt;函数数据分析方法为损失发展分析提供了新的视角，所提出的概率预测框架能够提供更准确的函数预测区间，有助于保险公司更好地评估未来损失发展。&lt;h4&gt;翻译&lt;/h4&gt;我们使用函数数据分析方法分析NAIC P计划损失三角形的损失发展。采用函数观点，我们的数据集包含24个事故年份中工人赔偿险种的3300多条增量损失比率曲线。依赖函数数据深度，我们首先基于公司特定协变量研究发展模式的相似性和差异，并识别异常的增量损失比率曲线。探索性发现激励了论文后半部分发展的概率预测框架。我们提出了一种函数模型，基于主成分分析得分的偏最小二乘回归来完成部分发展的增量损失比率曲线。结合上述方法与函数自举，使我们能够量化所有未来滞后的未来增量损失比率的不确定性。我们证明，与链梯法相比，我们的方法具有更好的概率评分，特别是能够提供准确的函数预测区间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We analyze loss development in NAIC Schedule P loss triangles usingfunctional data analysis methods. Adopting the functional viewpoint, ourdataset comprises 3300+ curves of incremental loss ratios (ILR) of workers'compensation lines over 24 accident years. Relying on functional data depth, wefirst study similarities and differences in development patterns based oncompany-specific covariates, as well as identify anomalous ILR curves.  The exploratory findings motivate the probabilistic forecasting frameworkdeveloped in the second half of the paper. We propose a functional model tocomplete partially developed ILR curves based on partial least squaresregression of PCA scores. Coupling the above with functional bootstrappingallows us to quantify future ILR uncertainty jointly across all future lags. Wedemonstrate that our method has much better probabilistic scores relative toChain Ladder and in particular can provide accurate functional predictiveintervals.</description>
      <author>example@mail.com (Arthur Charpentier, Qiheng Guo, Mike Ludkovski)</author>
      <guid isPermaLink="false">2510.27204v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry</title>
      <link>http://arxiv.org/abs/2510.26008v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures, submitted to nsdi 26&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Reveal系统，一种仅依赖硬件信号进行机器学习工作负载优化的方法，成功识别系统问题并加速模型性能。&lt;h4&gt;背景&lt;/h4&gt;现代机器学习已发展为紧密结合的全栈生态系统，用户依赖云提供商获取弹性资源，但这些平台即服务使用虚拟化，导致运营商对用户工作负载了解有限。&lt;h4&gt;目的&lt;/h4&gt;论证工作负载知识对于系统级优化不是必需的，提出一种仅依赖硬件信号的优化方法。&lt;h4&gt;方法&lt;/h4&gt;提出Reveal系统，采用以硬件为中心的方法，使用从系统收集的低级信号，通过无监督学习流程检测异常。该流程基于30多种流行ML模型在各种硬件平台上的分析开发，确保对新兴工作负载的适应性。&lt;h4&gt;主要发现&lt;/h4&gt;使用Reveal成功识别了网络和系统配置问题，加速了DeepSeek模型5.97%的性能。&lt;h4&gt;结论&lt;/h4&gt;工作负载知识对于系统级优化不是必需的，通过仅依赖硬件信号，运营商可以实现有效的系统优化。&lt;h4&gt;翻译&lt;/h4&gt;现代机器学习(ML)已发展为紧密结合的全栈生态系统，结合了硬件、软件、网络和应用。许多用户依赖云提供商提供弹性、隔离和成本高效的资源。不幸的是，这些平台即服务使用虚拟化，这意味着运营商对用户的工作负载了解有限。这阻碍了运营商进行资源优化，而资源优化对确保成本效率和最小化执行时间至关重要。在本文中，我们认为工作负载知识对于系统级优化不是必需的。我们提出了Reveal，它采用以硬件为中心的方法，仅依赖硬件信号-运营商完全可以访问。使用从系统收集的低级信号，Reveal通过无监督学习流程检测异常。该流程是通过分析各种硬件平台上超过30种流行ML模型开发的，确保对新兴工作负载和未知部署模式的适应性。使用Reveal，我们成功识别了网络和系统配置问题，将DeepSeek模型加速了5.97%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern machine learning (ML) has grown into a tightly coupled, full-stackecosystem that combines hardware, software, network, and applications. Manyusers rely on cloud providers for elastic, isolated, and cost-efficientresources. Unfortunately, these platforms as a service use virtualization,which means operators have little insight into the users' workloads. Thishinders resource optimizations by the operator, which is essential to ensurecost efficiency and minimize execution time. In this paper, we argue thatworkload knowledge is unnecessary for system-level optimization. We proposeReveal, which takes a hardware-centric approach, relying only on hardwaresignals - fully accessible by operators. Using low-level signals collected fromthe system, Reveal detects anomalies through an unsupervised learning pipeline.The pipeline is developed by analyzing over 30 popular ML models on varioushardware platforms, ensuring adaptability to emerging workloads and unknowndeployment patterns. Using Reveal, we successfully identified both network andsystem configuration issues, accelerating the DeepSeek model by 5.97%.</description>
      <author>example@mail.com (Ziji Chen, Steven W. D. Chien, Peng Qian, Noa Zilberman)</author>
      <guid isPermaLink="false">2510.26008v2</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>ANCHOR: Integrating Adversarial Training with Hard-mined Supervised Contrastive Learning for Robust Representation Learning</title>
      <link>http://arxiv.org/abs/2510.27599v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ANCHOR的框架，通过结合监督对比学习和硬正样本挖掘，提高神经网络模型对抗对抗性攻击的鲁棒性，同时保持较高的准确率。&lt;h4&gt;背景&lt;/h4&gt;神经网络通过遵循梯度学习，逐步调整参数识别数据中的模式，但这种学习机制也使模型容易受到对抗性攻击，即通过微小、不可察觉的输入变化导致模型做出错误判断。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够学习更稳定、更有意义的模式表示的方法，使模型对对抗性攻击更加鲁棒，同时保持高准确率。&lt;h4&gt;方法&lt;/h4&gt;提出ANCHOR框架，利用监督对比学习和显式硬正样本挖掘，使图像、其增强版本和扰动版本在嵌入空间中与同类图像聚类，同时与其他类别图像分离，从而专注于稳定模式而非脆弱梯度线索。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10数据集上，ANCHOR在干净和PGD-20攻击下的鲁棒准确性均优于标准对抗训练方法，表明结合对抗性指导与硬挖掘的对比监督有助于模型学习更结构化和鲁棒性的表示。&lt;h4&gt;结论&lt;/h4&gt;结合对抗性指导和硬挖掘的对比监督可以有效缩小模型准确性和鲁棒性之间的差距，使神经网络能够更好地抵抗对抗性攻击。&lt;h4&gt;翻译&lt;/h4&gt;神经网络改变了机器解释世界的方式。从根本上说，它们通过遵循梯度学习，逐步调整参数，直到识别出数据中最具判别性的模式。这一过程赋予了它们力量，但也打开了一个隐藏缺陷的大门。正是这些帮助模型学习的梯度，也可以用来产生微小、不可察觉的调整，导致模型完全改变其决策。这种调整被称为对抗性攻击。这些攻击通过向图像添加微小、不可察觉的变化来利用这一漏洞，这些变化虽然对人类眼睛来说是相同的，但会导致模型做出错误预测。在这项工作中，我们提出了对抗训练的对比性硬挖掘用于优化鲁棒性（ANCHOR）框架，该框架利用监督对比学习的力量，结合显式的硬正样本挖掘，使模型能够学习图像的表示，使图像、其增强版本和扰动版本在嵌入空间中与同一类别的其他图像聚类在一起，同时与其他类别的图像分离。这种对齐帮助模型专注于稳定、有意义的模式，而不是脆弱的梯度线索。在CIFAR-10上，我们的方法在干净和鲁棒准确性方面都取得了令人印象深刻的结果，在PGD-20（epsilon = 0.031）攻击下优于标准的对抗训练方法。我们的结果表明，将对抗性指导与硬挖掘的对比监督相结合，有助于模型学习更有结构和鲁棒性的表示，缩小了准确性和鲁棒性之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural networks have changed the way machines interpret the world. At theircore, they learn by following gradients, adjusting their parameters step bystep until they identify the most discriminant patterns in the data. Thisprocess gives them their strength, yet it also opens the door to a hidden flaw.The very gradients that help a model learn can also be used to produce small,imperceptible tweaks that cause the model to completely alter its decision.Such tweaks are called adversarial attacks. These attacks exploit thisvulnerability by adding tiny, imperceptible changes to images that, whileleaving them identical to the human eye, cause the model to make wrongpredictions. In this work, we propose Adversarially-trained ContrastiveHard-mining for Optimized Robustness (ANCHOR), a framework that leverages thepower of supervised contrastive learning with explicit hard positive mining toenable the model to learn representations for images such that the embeddingsfor the images, their augmentations, and their perturbed versions clustertogether in the embedding space along with those for other images of the sameclass while being separated from images of other classes. This alignment helpsthe model focus on stable, meaningful patterns rather than fragile gradientcues. On CIFAR-10, our approach achieves impressive results for both clean androbust accuracy under PGD-20 (epsilon = 0.031), outperforming standardadversarial training methods. Our results indicate that combining adversarialguidance with hard-mined contrastive supervision helps models learn morestructured and robust representations, narrowing the gap between accuracy androbustness.</description>
      <author>example@mail.com (Samarup Bhattacharya, Anubhab Bhattacharya, Abir Chakraborty)</author>
      <guid isPermaLink="false">2510.27599v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>C-LEAD: Contrastive Learning for Enhanced Adversarial Defense</title>
      <link>http://arxiv.org/abs/2510.27249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用对比学习进行对抗防御的新方法，通过同时优化模型参数和扰动，使网络学习鲁棒表示，实验结果表明该方法显著提高了模型对抗各种对抗性扰动的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络在计算机视觉任务中取得了显著成功，但它们容易受到对抗性攻击，这种攻击只需对输入图像进行微小扰动就能导致错误预测。&lt;h4&gt;目的&lt;/h4&gt;解决对抗性攻击问题，以便部署稳健的深度学习系统。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的方法，利用对比学习进行对抗防御。该方法利用对比损失函数，通过同时使用干净和对抗性扰动的图像来增强分类模型的鲁棒性。通过同时优化模型参数和扰动，使网络学习对对抗攻击不太敏感的鲁棒表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，模型对各种类型的对抗性扰动有显著改进。这表明对比损失有助于提取更具信息性和弹性的特征，有助于深度学习的对抗鲁棒性领域。&lt;h4&gt;结论&lt;/h4&gt;对比学习在对抗防御方面是一个有前景的方向，能够提高模型对对抗攻击的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络在图像分类、分割和目标检测等计算机视觉任务中取得了显著成功。然而，它们容易受到对抗性攻击，这种攻击只需对输入图像进行微小扰动就能导致错误预测。解决这个问题对于部署稳健的深度学习系统至关重要。本文提出了一种新颖的方法，利用对比学习进行对抗防御，这是一个先前未被探索的领域。我们的方法利用对比损失函数，通过同时使用干净和对抗性扰动的图像来增强分类模型的鲁棒性。通过同时优化模型参数和扰动，我们的方法使网络学习对对抗攻击不太敏感的鲁棒表示。实验结果表明，模型对各种类型的对抗性扰动的鲁棒性有显著提高。这表明对比损失有助于提取更具信息性和弹性的特征，为深度学习的对抗鲁棒性领域做出了贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks (DNNs) have achieved remarkable success in computervision tasks such as image classification, segmentation, and object detection.However, they are vulnerable to adversarial attacks, which can cause incorrectpredictions with small perturbations in input images. Addressing this issue iscrucial for deploying robust deep-learning systems. This paper presents a novelapproach that utilizes contrastive learning for adversarial defense, apreviously unexplored area. Our method leverages the contrastive loss functionto enhance the robustness of classification models by training them with bothclean and adversarially perturbed images. By optimizing the model's parametersalongside the perturbations, our approach enables the network to learn robustrepresentations that are less susceptible to adversarial attacks. Experimentalresults show significant improvements in the model's robustness against varioustypes of adversarial perturbations. This suggests that contrastive loss helpsextract more informative and resilient features, contributing to the field ofadversarial robustness in deep learning.</description>
      <author>example@mail.com (Suklav Ghosh, Sonal Kumar, Arijit Sur)</author>
      <guid isPermaLink="false">2510.27249v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2510.22706v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/lifuguan/IGGT_official&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出InstanceGrounded Geometry Transformer (IGGT)，一个端到端的大型统一transformer，用于统一空间重建和实例级上下文理解的知识。&lt;h4&gt;背景&lt;/h4&gt;人类自然地将3D世界的几何结构和语义内容视为交织的维度，能够连贯准确地理解复杂场景。然而，大多数先前方法优先训练大型几何模型用于低级3D重建，并将高级空间理解孤立处理，忽视了这两个方面的关键互动，导致泛化能力有限和下游任务表现不佳。最近的尝试通过简单对齐3D模型与特定语言模型来缓解此问题，但限制了感知能力和下游任务适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够统一空间重建和实例级上下文理解知识的端到端大型统一transformer模型。&lt;h4&gt;方法&lt;/h4&gt;提出Instance Grounded Geometry Transformer (IGGT)，设计3D一致的对比学习策略，指导模型通过仅2D视觉输入编码具有几何结构和实例聚类的统一表示，支持将2D输入提升为具有明确不同对象实例的连贯3D场景。同时构建InsScene-15K数据集，包含高质量的RGB图像、姿态、深度图和3D一致的实例级掩码注释。&lt;h4&gt;主要发现&lt;/h4&gt;通过统一几何结构和语义理解，可以改善3D场景分析的性能；仅通过2D视觉输入就能实现有效的3D重建和实例理解。&lt;h4&gt;结论&lt;/h4&gt;IGGT模型能够有效统一几何结构和语义理解，通过3D一致的对比学习策略，仅从2D视觉输入就能生成具有明确对象实例的连贯3D场景。&lt;h4&gt;翻译&lt;/h4&gt;人类自然地将3D世界的几何结构和语义内容视为交织的维度，能够连贯准确地理解复杂场景。然而，大多数先前方法优先训练大型几何模型用于低级3D重建，并将高级空间理解孤立处理，忽视了这两个3D场景分析基本方面之间的关键互动，从而限制了泛化能力，导致在下游3D理解任务中表现不佳。最近的尝试通过简单地将3D模型与特定语言模型对齐来缓解此问题，从而限制了感知能力并限制了下游任务的适应性。在本文中，我们提出了InstanceGrounded Geometry Transformer (IGGT)，一个端到端的大型统一transformer，用于统一空间重建和实例级上下文理解的知识。具体来说，我们设计了一种3D一致的对比学习策略，指导IGGT通过仅2D视觉输入编码具有几何结构和实例聚类的统一表示。这种表示支持将2D视觉输入一致地提升到具有明确不同对象实例的连贯3D场景。为促进此任务，我们进一步构建了InsScene-15K，一个通过新颖数据整理流程构建的大规模数据集，包含高质量的RGB图像、姿态、深度图和3D一致的实例级掩码注释。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景重建中几何结构和语义理解的分离问题。人类自然将几何结构和语义内容交织理解，而现有方法将这两者孤立处理，导致模型泛化能力差，在下游3D理解任务中表现不佳。这个问题在机器人操作、AR/VR和空间规划等应用中至关重要，这些应用需要同时理解场景的精确几何结构和丰富语义内容。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：分离处理几何重建和语义理解，或简单将3D模型与特定语言模型对齐，限制了模型适应性和性能。作者认为应通过联合训练将几何结构和实例级语义耦合，让模型自主学习两者关系。设计上借鉴了VGGT的统一Transformer架构，使用DINOv2提取特征，采用DPT-like架构进行密集预测，并利用SAM2进行数据标注。核心创新在于设计了3D一致的对比学习策略，确保跨视图的实例一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合训练将几何结构和实例级语义耦合，实现相互提升，并使用实例掩码作为桥接连接统一表示与各种视觉语言模型。整体流程：1) 构建InsScene-15K数据集；2) 使用大型统一变换器编码多视图图像为统一场景表示；3) 通过几何头部和实例头部分别预测几何结构和实例特征；4) 应用跨模态融合块增强实例特征的细粒度空间感知；5) 使用3D一致的对比学习策略训练模型；6) 通过实例掩码桥接策略支持下游任务如实例跟踪、开放词汇分割和QA场景定位。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 实例级几何-语义统一表示，通过联合训练实现相互提升；2) 3D一致的对比学习策略，确保跨视图实例一致性；3) 实例掩码桥接策略，实现与各种VLMs和LMMs的即插即用集成；4) 构建InsScene-15K大规模高质量数据集。相比之前工作：不同于分离处理几何和语义的方法，IGGT实现两者统一；不同于简单对齐特定语言模型的方法，IGT通过联合训练自主学习关系；不同于仅支持类别级特征的方法，IGT能区分同一类别中的不同实例；不绑定特定VLM，而是通过实例掩码灵活集成各种模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; IGGT通过统一几何重建和实例级语义理解，并引入实例掩码桥接策略，实现了高质量的语义3D重建和灵活的下游任务支持，显著提升了3D场景理解的性能和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans naturally perceive the geometric structure and semantic content of a3D world as intertwined dimensions, enabling coherent and accurateunderstanding of complex scenes. However, most prior approaches prioritizetraining large geometry models for low-level 3D reconstruction and treathigh-level spatial understanding in isolation, overlooking the crucialinterplay between these two fundamental aspects of 3D-scene analysis, therebylimiting generalization and leading to poor performance in downstream 3Dunderstanding tasks. Recent attempts have mitigated this issue by simplyaligning 3D models with specific language models, thus restricting perceptionto the aligned model's capacity and limiting adaptability to downstream tasks.In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), anend-to-end large unified transformer to unify the knowledge for both spatialreconstruction and instance-level contextual understanding. Specifically, wedesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encodea unified representation with geometric structures and instance-groundedclustering through only 2D visual inputs. This representation supportsconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitlydistinct object instances. To facilitate this task, we further constructInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depthmaps, and 3D-consistent instance-level mask annotations with a novel datacuration pipeline.</description>
      <author>example@mail.com (Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, Manyuan Zhang, Gang Yu, Dingwen Zhang, Ziwei Liu)</author>
      <guid isPermaLink="false">2510.22706v3</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.27606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Spatial-SSRL是一种自监督强化学习范式，通过从普通RGB或RGB-D图像中提取可验证信号，设计了五个捕捉2D和3D空间结构的预训练任务，显著提升了大型视觉语言模型的空间理解能力，无需昂贵的监督或专业工具。&lt;h4&gt;背景&lt;/h4&gt;大型视觉语言模型(LVLMs)在空间理解方面存在弱点，现有的监督微调(SFT)和可验证奖励强化学习(RLVR)方法依赖昂贵的监督、专业工具或受限环境，限制了规模扩展。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需昂贵监督和专业工具的自强化学习范式，提升LVLMs的空间理解能力，同时保持通用视觉能力。&lt;h4&gt;方法&lt;/h4&gt;提出Spatial-SSRL，一种自监督RL范式，自动设计五个预训练任务：打乱块重排序、翻转块识别、裁剪块修复、区域深度排序和相对3D位置预测，这些任务提供易于验证的真实答案，无需人工或LVLM注释。&lt;h4&gt;主要发现&lt;/h4&gt;在七个空间理解基准测试中，Spatial-SSRL相比Qwen2.5-VL基线实现了显著提升：3B模型平均准确率提高4.63%，7B模型提高3.89%，同时保留了通用视觉能力。&lt;h4&gt;结论&lt;/h4&gt;简单、内在的监督使大规模RLVR成为可能，为LVLMs提供更强的空间智能的实用途径，无需依赖昂贵的监督或专业工具。&lt;h4&gt;翻译&lt;/h4&gt;空间理解仍然是大型视觉语言模型(LVLMs)的弱点。现有的监督微调(SFT)和最近的可验证奖励强化学习(RLVR)流程依赖于昂贵的监督、专业工具或受限环境，限制了规模扩展。我们引入了Spatial-SSRL，一种自监督RL范式，直接从普通RGB或RGB-D图像中派生可验证信号。Spatial-SSRL自动设计了五个捕捉2D和3D空间结构的预训练任务：打乱块重排序、翻转块识别、裁剪块修复、区域深度排序和相对3D位置预测。这些任务提供易于验证的真实答案，不需要人工或LVLM注释。在我们的任务上训练显著提高了空间推理能力，同时保留了通用视觉能力。在图像和视频设置下的七个空间理解基准测试中，Spatial-SSRL相比Qwen2.5-VL基线实现了平均准确率提升：3B模型提升4.63%，7B模型提升3.89%。我们的结果表明，简单、内在的监督使大规模RLVR成为可能，并为LVLMs提供更强的空间智能的实用途径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决大型视觉语言模型（LVLMs）在空间理解方面的不足问题。空间理解对LVLMs分析复杂真实世界场景至关重要，能够推理深度、距离、方位和相对物体位置，实现3D环境重建，并支持自动驾驶、机器人操作和具身导航等应用。尽管LVLMs在其他任务上表现优异，但其空间理解能力仍远低于人类水平，限制了它们在现实世界中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从现有方法的局限性出发，认识到监督微调（SFT）和可验证奖励强化学习（RLVR）依赖昂贵监督和专业工具的问题。他们借鉴了视觉自监督学习（SSL）的理念，认为普通RGB或RGB-D图像中固有的内在一致性信号可以自然地监督空间理解。作者设计了自监督任务作为可验证奖励函数，并使用组相对策略优化（GRPO）进行强化学习训练。这种方法结合了SSL的无监督特性和RLVR的优化优势，但创新性地将其应用于空间理解任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用图像内在结构作为自监督信号，生成可验证的奖励，通过强化学习优化LVLM的空间理解能力，无需人工标注或专业工具。整体流程包括：1) 设计五类自监督任务（三类无深度任务：打乱块重排、翻转块识别、裁剪块修复；两类基于深度任务：区域深度排序、相对3D位置预测）；2) 从COCO等数据集收集原始图像，自动构建Spatial-SSRL-81k数据集；3) 采用SFT冷启动后，使用GRPO进行强化学习训练，结合准确度奖励和格式奖励优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出Spatial-SSRL自监督强化学习新范式；2) 设计覆盖2D和3D空间结构的五类互补pretext任务；3) 构建完全自动生成的Spatial-SSRL-81k数据集；4) 结合SFT冷启动和GRPO优化。相比之前工作，这种方法不依赖昂贵标注或专业工具，避免了SFT的过拟合问题和RLVR的环境限制，同时将SSL从表示学习转移到行为优化，实现了更好的泛化能力和可扩展性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Spatial-SSRL通过图像内在结构自监督和强化学习，显著提升了LVLM的空间理解能力，同时保持通用视觉能力，且成本更低、可扩展性更强。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial understanding remains a weakness of Large Vision-Language Models(LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcementlearning with verifiable rewards (RLVR) pipelines depend on costly supervision,specialized tools, or constrained environments that limit scale. We introduceSpatial-SSRL, a self-supervised RL paradigm that derives verifiable signalsdirectly from ordinary RGB or RGB-D images. Spatial-SSRL automaticallyformulates five pretext tasks that capture 2D and 3D spatial structure:shuffled patch reordering, flipped patch recognition, cropped patch inpainting,regional depth ordering, and relative 3D position prediction. These tasksprovide ground-truth answers that are easy to verify and require no human orLVLM annotation. Training on our tasks substantially improves spatial reasoningwhile preserving general visual capabilities. On seven spatial understandingbenchmarks in both image and video settings, Spatial-SSRL delivers averageaccuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Ourresults show that simple, intrinsic supervision enables RLVR at scale andprovides a practical route to stronger spatial intelligence in LVLMs.</description>
      <author>example@mail.com (Yuhong Liu, Beichen Zhang, Yuhang Zang, Yuhang Cao, Long Xing, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang)</author>
      <guid isPermaLink="false">2510.27606v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>NAUTILUS: A Large Multimodal Model for Underwater Scene Understanding</title>
      <link>http://arxiv.org/abs/2510.27481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025. Data and models are available at  https://github.com/H-EmbodVis/NAUTILUS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究构建了NautData数据集并提出视觉特征增强模块，开发了名为NAUTILUS的水下大语言模型，有效提高了水下场景理解能力。&lt;h4&gt;背景&lt;/h4&gt;水下探索对了解地球和资源勘探、国家安全等应用至关重要，但水下场景理解需要多任务感知能力，而目前缺乏大规模水下多任务指令调整数据集。&lt;h4&gt;目的&lt;/h4&gt;构建支持八种水下场景理解任务的大规模数据集，并开发能够提高水下场景理解鲁棒性的方法，解决水下图像退化挑战。&lt;h4&gt;方法&lt;/h4&gt;构建包含145万张图像-文本对的NautData数据集；引入基于水下成像模型的物理先验，提出即插即用的视觉特征增强模块；将模块集成到LLaVA-1.5和Qwen2.5-VL基线模型中，构建NAUTILUS模型。&lt;h4&gt;主要发现&lt;/h4&gt;VFE模块在NautData和公共水下数据集上实验证明有效，能够持续提高基线模型在大多数支持任务上的性能，确保了NAUTILUS在水下场景理解领域的优越性。&lt;h4&gt;结论&lt;/h4&gt;通过大规模数据集构建和视觉特征增强模块提出，成功提高了水下场景理解性能，NAUTILUS模型在水下场景理解任务中表现优越。&lt;h4&gt;翻译&lt;/h4&gt;水下探索为我们提供了了解地球的关键见解，并在资源勘探、国家安全等方面吸引越来越多的关注。我们研究水下场景理解方法，旨在实现水下探索的自动化。水下场景理解任务需要从多个粒度进行多任务感知。然而，缺乏大规模水下多任务指令调整数据集阻碍了这项研究的进展。为了弥补这一差距，我们构建了NautData，这是一个包含145万张图像-文本对的数据集，支持八种水下场景理解任务。它使水下场景理解模型的发展和全面评估成为可能。水下图像退化是一个广泛认可的问题，它干扰水下任务。为了提高水下场景理解的鲁棒性，我们引入了基于水下成像模型的物理先验，并提出了一种即插即用的视觉特征增强模块，该模块明确恢复了清晰的水下信息。我们将此模块集成到著名的基线模型LLaVA-1.5和Qwen2.5-VL中，构建了我们的水下大语言模型NAUTILUS。在NautData和公共水下数据集上进行的实验证明了VFE模块的有效性，持续提高了基线模型在大多数支持任务上的性能，从而确保了NAUTILUS在水下场景理解领域的优越性。数据和模型可在https://github.com/H-EmbodVis/NAUTILUS获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater exploration offers critical insights into our planet and attractsincreasing attention for its broader applications in resource exploration,national security, etc. We study the underwater scene understanding methods,which aim to achieve automated underwater exploration. The underwater sceneunderstanding task demands multi-task perceptions from multiple granularities.However, the absence of large-scale underwater multi-task instruction-tuningdatasets hinders the progress of this research. To bridge this gap, weconstruct NautData, a dataset containing 1.45 M image-text pairs supportingeight underwater scene understanding tasks. It enables the development andthorough evaluation of the underwater scene understanding models. Underwaterimage degradation is a widely recognized challenge that interferes withunderwater tasks. To improve the robustness of underwater scene understanding,we introduce physical priors derived from underwater imaging models and proposea plug-and-play vision feature enhancement (VFE) module, which explicitlyrestores clear underwater information. We integrate this module into renownedbaselines LLaVA-1.5 and Qwen2.5-VL and build our underwater LMM, NAUTILUS.Experiments conducted on the NautData and public underwater datasetsdemonstrate the effectiveness of the VFE module, consistently improving theperformance of both baselines on the majority of supported tasks, thus ensuringthe superiority of NAUTILUS in the underwater scene understanding area. Dataand models are available at https://github.com/H-EmbodVis/NAUTILUS.</description>
      <author>example@mail.com (Wei Xu, Cheng Wang, Dingkang Liang, Zongchuang Zhao, Xingyu Jiang, Peng Zhang, Xiang Bai)</author>
      <guid isPermaLink="false">2510.27481v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>GeoFM: Enhancing Geometric Reasoning of MLLMs via Synthetic Data Generation through Formal Language</title>
      <link>http://arxiv.org/abs/2510.27448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GeoFM的新方法，用于合成高质量的几何数据，解决了多模态大语言模型在几何推理中的挑战。通过形式化语言探索度量空间内条件的组合，GeoFM能够生成多样化且正确的几何问题，实验证明其显著提升了模型在几何任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在学术界和工业界因其处理多模态任务的能力而受到广泛关注。然而，由于高质量几何数据的稀缺，这些模型在数学几何推理方面面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型在数学几何推理中的挑战，通过开发一种新的合成几何数据方法来提升模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出GeoFM方法，使用形式化语言探索度量空间内条件的组合，生成高保真度的几何问题，并通过符号引擎确保正确性。&lt;h4&gt;主要发现&lt;/h4&gt;使用GeoFM合成数据训练的模型在MathVista几何问题解决任务上超越专有GPT-4o模型18.7%，在GeoQA上超越16.5%；在MathVista上超越领先开源模型5.7%，在GeoQA上超越2.7%。&lt;h4&gt;结论&lt;/h4&gt;GeoFM方法能够生成高质量、多样化的几何数据，有效提升了模型在几何推理任务上的性能，显著优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在学术界和工业界因其处理多模态任务的能力而受到广泛关注。然而，由于高质量几何数据的稀缺，这些模型在数学几何推理方面面临挑战。为了解决这个问题，合成几何数据已成为一种必要策略。当前生成合成几何数据的方法包括重新表述或扩展现有问题，以及使用预定义规则和模板创建几何图像和问题。然而，这些方法往往产生的数据多样性不足或容易引入噪声。此外，现有方法合成的几何图像变化有限，与真实几何图差异显著。为了克服这些限制，我们提出了GeoFM，一种新的合成几何数据方法。GeoFM使用形式化语言探索度量空间内条件的组合，生成与原始问题不同但保持高保真度的几何问题，并通过符号引擎确保正确性。实验结果表明，我们的合成数据显著优于现有方法。使用我们数据训练的模型在MathVista的几何问题解决任务上超越专有GPT-4o模型18.7%，在GeoQA上超越16.5%。此外，它在MathVista上超越领先开源模型5.7%，在GeoQA上超越2.7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal Large Language Models (MLLMs) have gained significant attentionin both academia and industry for their capabilities in handling multi-modaltasks. However, these models face challenges in mathematical geometricreasoning due to the scarcity of high-quality geometric data. To address thisissue, synthetic geometric data has become an essential strategy. Currentmethods for generating synthetic geometric data involve rephrasing or expandingexisting problems and utilizing predefined rules and templates to creategeometric images and problems. However, these approaches often produce datathat lacks diversity or is prone to noise. Additionally, the geometric imagessynthesized by existing methods tend to exhibit limited variation and deviatesignificantly from authentic geometric diagrams. To overcome these limitations,we propose GeoFM, a novel method for synthesizing geometric data. GeoFM usesformal languages to explore combinations of conditions within metric space,generating high-fidelity geometric problems that differ from the originalswhile ensuring correctness through a symbolic engine. Experimental results showthat our synthetic data significantly outperforms existing methods. The modeltrained with our data surpass the proprietary GPT-4o model by 18.7\% ongeometry problem-solving tasks in MathVista and by 16.5\% on GeoQA.Additionally, it exceeds the performance of a leading open-source model by5.7\% on MathVista and by 2.7\% on GeoQA.</description>
      <author>example@mail.com (Yuhao Zhang, Dingxin Hu, Tinghao Yu, Hao Liu, Yiting Liu)</author>
      <guid isPermaLink="false">2510.27448v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Learning Sparse Approximate Inverse Preconditioners for Conjugate Gradient Solvers on GPUs</title>
      <link>http://arxiv.org/abs/2510.27517v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025, poster&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络(GNN)的稀疏近似逆(SPAI)预处理器方法，用于解决共轭梯度求解器在GPU上的并行化和长距离依赖问题，显著提高了求解速度和性能。&lt;h4&gt;背景&lt;/h4&gt;共轭梯度(CG)求解器是求解对称正定线性系统Ax=b的常用方法，有效预处理器对快速收敛至关重要。传统预处理器依赖预设算法提供理论保证但限制数据优化能力，现有基于学习的方法利用GNN提高性能，但依赖不完全分解导致GPU并行化困难和长距离依赖建模挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种GPU友好的学习型预处理器，特别是使用GNN构建稀疏近似逆(SPAI)预处理器，避免三角求解并减少每个CG步骤的计算量。&lt;h4&gt;方法&lt;/h4&gt;使用GNN构建稀疏近似逆(SPAI)预处理器，每个CG步骤只需两次矩阵-向量乘积；利用矩阵-向量乘积的局部性与GNN局部传播机制的兼容性；引入基于统计的尺度不变损失函数，匹配CG收敛率取决于条件数而非绝对尺度的特性。&lt;h4&gt;主要发现&lt;/h4&gt;在三个PDE导出数据集和一个合成数据集上评估，该方法在GPU上优于标准预处理器(对角线、IC和传统SPAI)及之前的基于学习预处理器；减少40%-53%求解时间(快68%-113%)，具有更好条件数和泛化性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的GNN构建的SPAI预处理器成功解决了现有方法在GPU并行化和长距离依赖建模方面的挑战，在GPU上实现了更快的求解时间和更好的性能，适用于广泛的应用场景。&lt;h4&gt;翻译&lt;/h4&gt;共轭梯度求解器(CG)是求解对称正定线性系统Ax=b的常用方法，其中有效的预处理器对快速收敛至关重要。传统预处理器依赖于预设算法来提供严格的理论保证，同时限制了利用数据优化的能力。现有的基于学习的方法通常利用图神经网络(GNN)来提高性能和加速构建过程。然而，它们对不完全分解的依赖导致了重大挑战：相关的三角求解在实践中阻碍了GPU并行化，并引入了难以被GNN建模的长距离依赖关系。为解决这些问题，我们提出了一种基于学习的方法来生成GPU友好的预处理器，特别是使用GNN构建稀疏近似逆(SPAI)预处理器，避免了三角求解，每个CG步骤只需要两次矩阵-向量乘积。矩阵-向量乘积的局部性与GNN的局部传播机制兼容。GNN的灵活性也使我们的方法可以应用于广泛场景。此外，我们引入了一种基于统计的尺度不变损失函数，其设计匹配了CG的性质——收敛率取决于条件数，而不是A的绝对尺度，从而提高了学习到的预处理器的性能。在三个从PDE导出的数据集和一个合成数据集上的评估表明，我们的方法在GPU上优于标准预处理器(对角线、IC和传统SPAI)以及之前的基于学习的预处理器。我们在GPU上减少了40%-53%的求解时间(快68%-113%)，同时具有更好的条件数和优异的泛化性能。源代码可在https://github.com/Adversarr/LearningSparsePreconditioner4GPU获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The conjugate gradient solver (CG) is a prevalent method for solvingsymmetric and positive definite linear systems Ax=b, where effectivepreconditioners are crucial for fast convergence. Traditional preconditionersrely on prescribed algorithms to offer rigorous theoretical guarantees, whilelimiting their ability to exploit optimization from data. Existinglearning-based methods often utilize Graph Neural Networks (GNNs) to improvethe performance and speed up the construction. However, their reliance onincomplete factorization leads to significant challenges: the associatedtriangular solve hinders GPU parallelization in practice, and introduceslong-range dependencies which are difficult for GNNs to model. To address theseissues, we propose a learning-based method to generate GPU-friendlypreconditioners, particularly using GNNs to construct Sparse ApproximateInverse (SPAI) preconditioners, which avoids triangular solves and requiresonly two matrix-vector products at each CG step. The locality of matrix-vectorproduct is compatible with the local propagation mechanism of GNNs. Theflexibility of GNNs also allows our approach to be applied in a wide range ofscenarios. Furthermore, we introduce a statistics-based scale-invariant lossfunction. Its design matches CG's property that the convergence rate depends onthe condition number, rather than the absolute scale of A, leading to improvedperformance of the learned preconditioner. Evaluations on three PDE-deriveddatasets and one synthetic dataset demonstrate that our method outperformsstandard preconditioners (Diagonal, IC, and traditional SPAI) and previouslearning-based preconditioners on GPUs. We reduce solution time on GPUs by40%-53% (68%-113% faster), along with better condition numbers and superiorgeneralization performance. Source code available athttps://github.com/Adversarr/LearningSparsePreconditioner4GPU</description>
      <author>example@mail.com (Zherui Yang, Zhehao Li, Kangbo Lyu, Yixuan Li, Tao Du, Ligang Liu)</author>
      <guid isPermaLink="false">2510.27517v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Spectral Neural Graph Sparsification</title>
      <link>http://arxiv.org/abs/2510.27474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的图表示学习框架——谱保持网络，通过生成简化的图作为原始图的忠实代理，使下游任务能够在降低计算成本的情况下进行。&lt;h4&gt;背景&lt;/h4&gt;图是建模复杂系统的核心工具，应用于社交网络、分子化学和神经科学等领域。图神经网络已成为图学习的标准工具，但仍受限于固定结构的依赖和过平滑问题。&lt;h4&gt;目的&lt;/h4&gt;提出谱保持网络框架，生成简化的图作为原始图的忠实代理，使社区检测、影响传播和信息扩散等下游任务能够在降低计算成本的情况下进行。&lt;h4&gt;方法&lt;/h4&gt;谱保持网络引入两个关键组件：联合图进化层，同时变换图拓扑和节点特征矩阵，使结构和属性在层间自适应演化；谱一致性损失，通过强制保持图的谱特性和节点特征向量的一致性来正则化这些变换。&lt;h4&gt;主要发现&lt;/h4&gt;通过节点级稀化分析评估了谱保持网络的有效性，使用成熟指标并与最先进方法进行基准测试，实验结果表明该方法具有优越的性能和明显优势。&lt;h4&gt;结论&lt;/h4&gt;谱保持网络是一种有效的图表示学习方法，能够生成简化的图同时保持原始图的关键特性，在降低计算成本的同时，在下游任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;图是建模复杂系统的核心工具，应用于社交网络、分子化学和神经科学等领域。虽然图神经网络，特别是图卷积网络已成为图学习的标准工具，但它们仍然受限于对固定结构的依赖和过平滑问题。我们提出了谱保持网络，这是一种用于图表示学习的新框架，它生成简化的图作为原始图的忠实代理，使社区检测、影响传播和信息扩散等下游任务能够以降低的计算成本进行。谱保持网络引入了两个关键组件：联合图进化层和谱一致性损失。前者同时变换图拓扑和节点特征矩阵，使结构和属性在层间自适应演化，克服了静态邻域聚合的刚性。后者通过强制保持图的谱特性和节点特征向量的一致性来正则化这些变换。我们通过分析成熟的指标和与最先进方法进行基准测试，评估了谱保持网络在节点级稀化方面的有效性。实验结果表明我们的方法具有优越的性能和明显的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphs are central to modeling complex systems in domains such as socialnetworks, molecular chemistry, and neuroscience. While Graph Neural Networks,particularly Graph Convolutional Networks, have become standard tools for graphlearning, they remain constrained by reliance on fixed structures andsusceptibility to over-smoothing. We propose the Spectral Preservation Network,a new framework for graph representation learning that generates reduced graphsserving as faithful proxies of the original, enabling downstream tasks such ascommunity detection, influence propagation, and information diffusion at areduced computational cost. The Spectral Preservation Network introduces twokey components: the Joint Graph Evolution layer and the Spectral Concordanceloss. The former jointly transforms both the graph topology and the nodefeature matrix, allowing the structure and attributes to evolve adaptivelyacross layers and overcoming the rigidity of static neighborhood aggregation.The latter regularizes these transformations by enforcing consistency in boththe spectral properties of the graph and the feature vectors of the nodes. Weevaluate the effectiveness of Spectral Preservation Network on node-levelsparsification by analyzing well-established metrics and benchmarking againststate-of-the-art methods. The experimental results demonstrate the superiorperformance and clear advantages of our approach.</description>
      <author>example@mail.com (Angelica Liguori, Ettore Ritacco, Pietro Sabatino, Annalisa Socievole)</author>
      <guid isPermaLink="false">2510.27474v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Feature Fusion for Spatial Morphology Analysis of Traditional Villages via Hierarchical Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.27208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种分层图神经网络模型，整合多源数据对村庄空间形态进行分析，解决了现有研究中的局限性，在多模态融合和分类任务上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;村庄区域在人地关系研究中具有重要性，但随着城市化发展，村庄空间特征逐渐消失，景观同质化问题突出。现有研究主要采用单一学科视角，过度依赖定性分析方法，且受限于数字基础设施不足和数据缺乏。&lt;h4&gt;目的&lt;/h4&gt;为解决当前研究局限性，提出一种分层图神经网络模型，整合多源数据对村庄空间形态进行深入分析。&lt;h4&gt;方法&lt;/h4&gt;提出包含两种节点（输入节点和通信节点）和两种边（静态输入边和动态通信边）的分层图神经网络模型。通过结合图卷积网络和图注意力网络，在两阶段特征更新机制下融合多模态特征，并引入关系池化机制，对17个子类型实施联合训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在多模态融合和分类任务上比现有方法取得显著性能提升。所有子类型的联合优化使平均准确率/F1值从独立模型的0.71/0.83提升到0.82/0.90，其中地块任务带来了6%的提升。&lt;h4&gt;结论&lt;/h4&gt;该方法为探索村庄空间格局和生成逻辑提供了科学依据。&lt;h4&gt;翻译&lt;/h4&gt;村庄区域在研究人地关系中具有重要性。然而，随着城市化的推进，空间特征的逐渐消失和景观的同质化已成为突出问题。现有研究主要采用单一学科视角分析村庄空间形态及其影响因素，过度依赖定性分析方法。这些研究常受限于数字基础设施不足和数据缺乏。为解决当前研究局限性，本文提出了一种整合多源数据的分层图神经网络模型，对村庄空间形态进行深入分析。该框架包含两种节点类型（输入节点和通信节点）和两种边类型（静态输入边和动态通信边）。通过结合图卷积网络和图注意力网络，所提出的模型在两阶段特征更新机制下高效融合多模态特征。此外，基于现有的村庄空间形态分类原则，本文引入了关系池化机制，并对17个子类型实施了联合训练策略。实验结果表明，该方法在多模态融合和分类任务上比现有方法取得了显著的性能提升。此外，所有子类型的联合优化使平均准确率/F1值从独立模型的0.71/0.83提升到0.82/0.90，其中地块任务带来了6%的提升。我们的方法为探索村庄空间格局和生成逻辑提供了科学证据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Villages areas hold significant importance in the study of human-landrelationships. However, with the advancement of urbanization, the gradualdisappearance of spatial characteristics and the homogenization of landscapeshave emerged as prominent issues. Existing studies primarily adopt asingle-disciplinary perspective to analyze villages spatial morphology and itsinfluencing factors, relying heavily on qualitative analysis methods. Theseefforts are often constrained by the lack of digital infrastructure andinsufficient data. To address the current research limitations, this paperproposes a Hierarchical Graph Neural Network (HGNN) model that integratesmulti-source data to conduct an in-depth analysis of villages spatialmorphology. The framework includes two types of nodes-input nodes andcommunication nodes-and two types of edges-static input edges and dynamiccommunication edges. By combining Graph Convolutional Networks (GCN) and GraphAttention Networks (GAT), the proposed model efficiently integrates multimodalfeatures under a two-stage feature update mechanism. Additionally, based onexisting principles for classifying villages spatial morphology, the paperintroduces a relational pooling mechanism and implements a joint trainingstrategy across 17 subtypes. Experimental results demonstrate that this methodachieves significant performance improvements over existing approaches inmultimodal fusion and classification tasks. Additionally, the proposed jointoptimization of all sub-types lifts mean accuracy/F1 from 0.71/0.83(independent models) to 0.82/0.90, driven by a 6% gain for parcel tasks. Ourmethod provides scientific evidence for exploring villages spatial patterns andgenerative logic.</description>
      <author>example@mail.com (Jiaxin Zhang, Zehong Zhu, Junye Deng, Yunqin Li, and Bowen Wang)</author>
      <guid isPermaLink="false">2510.27208v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>MDAS-GNN: Multi-Dimensional Spatiotemporal GNN with Spatial Diffusion for Urban Traffic Risk Forecasting</title>
      <link>http://arxiv.org/abs/2510.27197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于多维注意力的空间扩散图神经网络(MDAS-GNN)，用于交通事故预测，整合了交通安全、基础设施和环境三个核心风险维度，在多个城市区域的数据集上表现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;交通事故是全球严重的公共健康挑战，每年导致超过135万人死亡。传统事故预测模型将道路段独立处理，无法捕捉城市交通网络中的复杂空间关系和时间依赖性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉城市交通网络中复杂空间关系和时间依赖性的交通事故预测模型，为交通基础设施设计和城市规划提供数据支持。&lt;h4&gt;方法&lt;/h4&gt;构建了MDAS-GNN模型，整合三个核心风险维度：交通安全、基础设施和环境风险。采用特定特征的空间扩散机制和多头时间注意力来捕捉不同时间跨度的依赖关系，并在英国交通部提供的事故数据上进行了验证。&lt;h4&gt;主要发现&lt;/h4&gt;MDAS-GNN相比既定的基线方法表现出优越性能，在短期、中期和长期预测中都保持了较低的预测误差，特别是在长期预测方面表现出色。消融研究表明，集成的多维特征比单特征方法更有效，可将预测误差降低高达40%。&lt;h4&gt;结论&lt;/h4&gt;该框架为土木工程师和城市规划者提供了先进的预测能力，支持交通基础设施设计，使决策者能够基于数据进行路网优化、基础设施资源改进和城市发展项目中的战略安全干预。&lt;h4&gt;翻译&lt;/h4&gt;交通事故代表着一项关键的公共卫生挑战，每年在全球范围内造成超过135万人死亡。传统的事故预测模型将道路段独立处理，无法捕捉城市交通网络中复杂的空间关系和时间依赖性。本研究开发了MDAS-GNN，一种基于多维注意力的空间扩散图神经网络，整合了三个核心风险维度：交通安全、基础设施和环境风险。该框架采用特定特征的空间扩散机制和多头时间注意力来捕捉不同时间跨度的依赖关系。在英国交通部提供的伦敦中部、曼彻斯特南部和伯明翰东南部的事故数据评估中，MDAS-GNN相比既定的基线方法取得了优越的性能。该模型在短期、中期和长期预测中都保持了一致的低预测误差，特别是在长期预测方面具有特别优势。消融研究证实，集成的多维特征优于单特征方法，可将预测误差降低高达40%。该框架为土木工程师和城市规划者提供了先进的预测能力，用于交通基础设施设计，使他们能够为路网优化、基础设施资源改进和城市发展项目中的战略安全干预提供数据驱动的决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic accidents represent a critical public health challenge, claiming over1.35 million lives annually worldwide. Traditional accident prediction modelstreat road segments independently, failing to capture complex spatialrelationships and temporal dependencies in urban transportation networks. Thisstudy develops MDAS-GNN, a Multi-Dimensional Attention-based Spatial-diffusionGraph Neural Network integrating three core risk dimensions: traffic safety,infrastructure, and environmental risk. The framework employs feature-specificspatial diffusion mechanisms and multi-head temporal attention to capturedependencies across different time horizons. Evaluated on UK Department forTransport accident data across Central London, South Manchester, and SEBirmingham, MDASGNN achieves superior performance compared to establishedbaseline methods. The model maintains consistently low prediction errors acrossshort, medium, and long-term periods, with particular strength in long-termforecasting. Ablation studies confirm that integrated multi-dimensionalfeatures outperform singlefeature approaches, reducing prediction errors by upto 40%. This framework provides civil engineers and urban planners withadvanced predictive capabilities for transportation infrastructure design,enabling data-driven decisions for road network optimization, infrastructureresource improvements, and strategic safety interventions in urban developmentprojects.</description>
      <author>example@mail.com (Ziyuan Gao)</author>
      <guid isPermaLink="false">2510.27197v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Relation-Aware Bayesian Optimization of DBMS Configurations Guided by Affinity Scores</title>
      <link>http://arxiv.org/abs/2510.27145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了RelTune框架，通过关系图表示参数依赖关系，并引入混合评分引导的贝叶斯优化方法，解决了现有DBMS参数自动调优方法的局限性，在多个DBMS和工作负载上实现了更快的收敛和更高的优化效率。&lt;h4&gt;背景&lt;/h4&gt;数据库管理系统(DBMSs)对于管理大规模异构数据至关重要，其性能受配置参数影响。有效的参数调优对于适应不同工作负载、最大化吞吐量同时最小化延迟至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有自动配置优化方法的关键局限性，包括忽略参数间依赖关系、仅选择部分参数进行优化、贝叶斯优化依赖代理模型导致的预测不稳定和探索效率低下等问题。&lt;h4&gt;方法&lt;/h4&gt;提出RelTune框架，将参数依赖关系表示为关系图，学习基于图神经网络(GNN)的潜在嵌入编码性能相关语义；引入混合评分引导的贝叶斯优化(HBO)，结合代理预测与亲和性评分，测量与先前高性能配置的接近度。&lt;h4&gt;主要发现&lt;/h4&gt;在多个DBMS和工作负载上的实验结果表明，RelTune比传统基于贝叶斯优化的方法实现更快的收敛和更高的优化效率，在所有评估场景中取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;RelTune框架有效解决了现有DBMS参数自动调优方法的关键局限性，通过考虑参数依赖关系和更全面的参数优化，实现了更优的性能。&lt;h4&gt;翻译&lt;/h4&gt;数据库管理系统(DBMSs)是管理大规模异构数据的基础，其性能受配置参数的影响。有效调整这些参数对于适应不同的工作负载、最大化吞吐量同时最小化延迟至关重要。最近的研究集中在使用机器学习进行自动配置优化；然而，现有方法仍存在几个关键局限性。大多数调优框架忽略了参数之间的依赖关系，假设每个参数独立运作。这种简化限制了优化器利用参数间的关联效应，限制了其捕捉性能敏感交互的能力。此外，为降低高维搜索空间的复杂性，先前工作通常只选择前几个参数进行优化，忽略了其他对性能有重要贡献的参数。作为自动调优最常用的方法，贝叶斯优化(BO)也受限于其对代理模型的依赖，这可能导致预测不稳定和探索效率低下。为克服这些局限性，我们提出了RelTune，一种新框架，它将参数依赖关系表示为关系图，并学习基于图神经网络的潜在嵌入，编码性能相关的语义。RelTune进一步引入了混合评分引导的贝叶斯优化(HBO)，结合代理预测与亲和性评分，测量与先前高性能配置的接近度。在多个DBMS和工作负载上的实验结果表明，RelTune比传统的基于BO的方法实现更快的收敛和更高的优化效率，在所有评估场景中取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Database Management Systems (DBMSs) are fundamental for managing large-scaleand heterogeneous data, and their performance is critically influenced byconfiguration parameters. Effective tuning of these parameters is essential foradapting to diverse workloads and maximizing throughput while minimizinglatency. Recent research has focused on automated configuration optimizationusing machine learning; however, existing approaches still exhibit several keylimitations. Most tuning frameworks disregard the dependencies amongparameters, assuming that each operates independently. This simplificationprevents optimizers from leveraging relational effects across parameters,limiting their capacity to capture performancesensitive interactions. Moreover,to reduce the complexity of the high-dimensional search space, prior work oftenselects only the top few parameters for optimization, overlooking others thatcontribute meaningfully to performance. Bayesian Optimization (BO), the mostcommon method for automatic tuning, is also constrained by its reliance onsurrogate models, which can lead to unstable predictions and inefficientexploration. To overcome these limitations, we propose RelTune, a novelframework that represents parameter dependencies as a Relational Graph andlearns GNN-based latent embeddings that encode performancerelevant semantics.RelTune further introduces Hybrid-Score-Guided Bayesian Optimization (HBO),which combines surrogate predictions with an Affinity Score measuring proximityto previously high-performing configurations. Experimental results on multipleDBMSs and workloads demonstrate that RelTune achieves faster convergence andhigher optimization efficiency than conventional BO-based methods, achievingstate-of-the-art performance across all evaluated scenarios.</description>
      <author>example@mail.com (Sein Kwon, Seulgi Baek, Hyunseo Yang, Youngwan Jo, Sanghyun Park)</author>
      <guid isPermaLink="false">2510.27145v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration</title>
      <link>http://arxiv.org/abs/2510.27039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于云的混合模型，结合时空图神经网络和Transformer架构，用于交通流量预测，该模型能有效捕捉复杂时空依赖关系并整合外部特征，在云平台上部署实现了可扩展性和实时适应性。&lt;h4&gt;背景&lt;/h4&gt;准确的交通流量预测对智能交通系统的发展至关重要，支持交通信号优化、拥堵管理和路线规划等任务。传统模型往往无法有效捕捉大规模路网中的复杂时空依赖关系，特别是在天气、节假日和交通事故等外部因素的影响下。&lt;h4&gt;目的&lt;/h4&gt;解决传统模型在捕捉大规模路网复杂时空依赖关系方面的不足，特别是在外部因素影响下的预测问题，开发一种能够有效整合空间和时间信息的混合模型，提高交通流量预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于云的混合模型，结合时空图神经网络和Transformer架构。该模型利用GNN在建模路网空间相关性方面的优势以及Transformer捕捉长期时间依赖关系的能力。通过特征融合整合外部上下文特征以提高预测准确性。模型部署在云计算平台上以实现可扩展性和实时适应性。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估显示，该模型在数据集上的表现优于基线方法，RMSE仅为17.92，MAE仅为10.53。这些发现表明混合GNN-Transformer方法为基于云的ITS应用提供了有效且可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;混合GNN-Transformer方法为交通流量预测提供了方法论进步，并为拥堵缓解提供了实际应用价值，是一种有效且可扩展的基于云的ITS应用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的交通流量预测对智能交通系统的发展至关重要，支持交通信号优化、拥堵管理和路线规划等任务。传统模型往往无法有效捕捉大规模路网中的复杂时空依赖关系，特别是在天气、节假日和交通事故等外部因素的影响下。为应对这一挑战，本文提出了一种基于云的混合模型，结合时空图神经网络和Transformer架构进行交通流量预测。该模型利用了GNN在建模路网空间相关性方面的优势以及Transformer捕捉长期时间依赖关系的能力。通过特征融合整合外部上下文特征以提高预测准确性。所提出的模型部署在云计算平台上以实现可扩展性和实时适应性。对数据集的实验评估显示，我们的模型优于基线方法，RMSE仅为17.92，MAE仅为10.53。这些发现表明，混合GNN-Transformer方法为基于云的ITS应用提供了有效且可扩展的解决方案，为交通流量预测提供了方法论进步，并为拥堵缓解提供了实际应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate traffic flow forecasting is essential for the development ofintelligent transportation systems (ITS), supporting tasks such as trafficsignal optimization, congestion management, and route planning. Traditionalmodels often fail to effectively capture complex spatial-temporal dependenciesin large-scale road networks, especially under the influence of externalfactors such as weather, holidays, and traffic accidents. To address thischallenge, this paper proposes a cloud-based hybrid model that integratesSpatio-Temporal Graph Neural Networks (ST-GNN) with a Transformer architecturefor traffic flow prediction. The model leverages the strengths of GNNs inmodeling spatial correlations across road networks and the Transformers'ability to capture long-term temporal dependencies. External contextualfeatures are incorporated via feature fusion to enhance predictive accuracy.The proposed model is deployed on a cloud computing platform to achievescalability and real-time adaptability. Experimental evaluation of the datasetshows that our model outperforms baseline methods (LSTM, TCN, GCN, pureTransformer) with an RMSE of only 17.92 and a MAE of only 10.53. These findingssuggest that the hybrid GNN-Transformer approach provides an effective andscalable solution for cloud-based ITS applications, offering methodologicaladvancements for traffic flow forecasting and practical implications forcongestion mitigation.</description>
      <author>example@mail.com (Zhuo Zheng, Lingran Meng, Ziyu Lin)</author>
      <guid isPermaLink="false">2510.27039v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Oral Tradition-Encoded NanyinHGNN: Integrating Nanyin Music Preservation and Generation through a Pipa-Centric Dataset</title>
      <link>http://arxiv.org/abs/2510.26817v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了NanyinHGNN，一个用于生成南音乐器音乐的异构图网络模型。该模型通过构建以琵琶为中心的MIDI数据集和专门的标记化方法，将装饰音生成为异构图中的节点，结合图神经网络和规则系统生成真实的南音乐曲。&lt;h4&gt;背景&lt;/h4&gt;南音是联合国教科文组织认可的无形文化遗产，它是一种以琵琶为中心的异声传统，核心旋律以传统记谱法记谱，而装饰音则通过口头传承，这对保存和当代创新都提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;解决南音音乐保存和创新中的挑战，特别是处理传统记谱与口头传承装饰音之间的差异。&lt;h4&gt;方法&lt;/h4&gt;构建以琵琶为中心的MIDI数据集，开发NanyinTok作为专门的标记化方法，使用图转换器将符号序列转换为图结构，将装饰音生成重新定义为异构图中装饰音节点的创建，结合图神经网络生成旋律轮廓，并使用基于南音表演实践的规则系统完善装饰音。&lt;h4&gt;主要发现&lt;/h4&gt;该模型成功生成了包含四种传统乐器的真实异声合奏，证明了将领域特定知识整合到模型架构中可以有效缓解民族音乐计算学中的数据稀缺挑战。&lt;h4&gt;结论&lt;/h4&gt;整合领域特定知识到模型架构中可以有效解决民族音乐计算学中的数据稀缺问题，为南音等传统音乐的保存和创新提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了NanyinHGNN，一个用于生成南音乐器音乐的异构图网络模型。作为联合国教科文组织认可的无形文化遗产，南音遵循一种以琵琶为中心的异声传统，核心旋律以传统记谱法记谱，而装饰音则通过口头传承，这对保存和当代创新都提出了挑战。为解决这一问题，我们构建了一个以琵琶为中心的MIDI数据集，开发了NanyinTok作为专门的标记化方法，并使用图转换器将符号序列转换为图结构，以确保保留关键音乐特征。我们的主要创新是将装饰音生成为异构图中装饰音节点的创建。首先，图神经网络生成针对装饰音优化的旋律轮廓；然后，一个由南音表演实践指导的规则系统将这些轮廓完善为完整的装饰音，而在训练期间不需要明确的装饰音注释。实验结果表明，我们的模型成功生成了包含四种传统乐器的真实异声合奏。这些发现证明将领域特定知识整合到模型架构中可以有效缓解民族音乐计算学中的数据稀缺挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose NanyinHGNN, a heterogeneous graph network model for generatingNanyin instrumental music. As a UNESCO-recognized intangible cultural heritage,Nanyin follows a heterophonic tradition centered around the pipa, where coremelodies are notated in traditional notation while ornamentations are passeddown orally, presenting challenges for both preservation and contemporaryinnovation. To address this, we construct a Pipa-Centric MIDI dataset, developNanyinTok as a specialized tokenization method, and convert symbolic sequencesinto graph structures using a Graph Converter to ensure that key musicalfeatures are preserved. Our key innovation reformulates ornamentationgeneration as the creation of ornamentation nodes within a heterogeneous graph.First, a graph neural network generates melodic outlines optimized forornamentations. Then, a rule-guided system informed by Nanyin performancepractices refines these outlines into complete ornamentations without requiringexplicit ornamentation annotations during training. Experimental resultsdemonstrate that our model successfully generates authentic heterophonicensembles featuring four traditional instruments. These findings validate thatintegrating domain-specific knowledge into model architecture can effectivelymitigate data scarcity challenges in computational ethnomusicology.</description>
      <author>example@mail.com (Jianbing Xiahou, Weixi Zhai, Xu Cui)</author>
      <guid isPermaLink="false">2510.26817v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Effect of Domain Generalization Techniques in Low Resource Systems</title>
      <link>http://arxiv.org/abs/2510.27512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了资源有限情况下自然语言任务中的两种因果领域泛化技术，评估了它们在处理分布偏移问题上的有效性。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型通常假设训练和测试数据遵循相同分布，但现实中的分布偏移常导致这一假设失效。在资源有限的环境中，数据稀缺和领域多样性不足进一步阻碍了模型的稳健泛化能力。&lt;h4&gt;目的&lt;/h4&gt;研究两种不同的因果领域泛化技术，提高资源有限自然语言任务中模型对分布偏移的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1) 因果数据增强(CDA)方法：自动生成反事实例子应用于NaijaSenti Twitter语料库的情感分类，通过添加语义等效的释义模拟受控分布偏移；2) 不变因果表征学习(ICRL)方法：使用DINER框架，将其适配到多语言情感分析任务中。&lt;h4&gt;主要发现&lt;/h4&gt;两种方法都提高了对未见领域的鲁棒性：反事实数据增强在情感分类中带来了一致的跨域准确率提升；而使用DINER的因果表征学习在多语言情感分析中改善了分布外性能，尽管不同语言间的提升程度有所不同。&lt;h4&gt;结论&lt;/h4&gt;因果领域泛化技术能有效提高资源有限情况下自然语言处理任务的跨域泛化能力，但不同方法在不同任务和语言中的效果存在差异。&lt;h4&gt;翻译&lt;/h4&gt;机器学习模型通常假设训练和测试数据遵循相同分布，这一假设在现实场景中常因分布偏移而失效。这一问题在资源有限的环境中尤为突出，因为数据稀缺和有限的领域多样性阻碍了稳健的泛化。领域泛化(DG)方法通过学习跨领域保持不变的特征来解决这一挑战，通常使用因果机制提高模型的鲁棒性。在本研究中，我们考察了资源有限的自然语言任务中的两种不同因果DG技术。首先，我们研究了一种因果数据增强(CDA)方法，自动生成反事实例子以提高对虚假相关性的鲁棒性。我们将此方法应用于NaijaSenti Twitter语料库上的情感分类，通过添加语义等效的释义来模拟受控的分布偏移。其次，我们探索了使用DINER框架的不变因果表征学习(ICRL)方法，该方法最初用于消除基于方面的情感分析的偏见。我们将DINER适配到多语言环境中。我们的研究结果表明，两种方法都提高了对未见领域的鲁棒性：反事实数据增强在情感分类中带来了一致的跨域准确率提升，而使用DINER的因果表征学习在多语言情感分析中改善了分布外性能，尽管不同语言间的提升程度有所不同。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning models typically assume that training and test data followthe same distribution, an assumption that often fails in real-world scenariosdue to distribution shifts. This issue is especially pronounced in low-resourcesettings, where data scarcity and limited domain diversity hinder robustgeneralization. Domain generalization (DG) approaches address this challenge bylearning features that remain invariant across domains, often using causalmechanisms to improve model robustness. In this study, we examine two distinctcausal DG techniques in low-resource natural language tasks. First, weinvestigate a causal data augmentation (CDA) approach that automaticallygenerates counterfactual examples to improve robustness to spuriouscorrelations. We apply this method to sentiment classification on theNaijaSenti Twitter corpus, expanding the training data with semanticallyequivalent paraphrases to simulate controlled distribution shifts. Second, weexplore an invariant causal representation learning (ICRL) approach using theDINER framework, originally proposed for debiasing aspect-based sentimentanalysis. We adapt DINER to a multilingual setting. Our findings demonstratethat both approaches enhance robustness to unseen domains: counterfactual dataaugmentation yields consistent cross-domain accuracy gains in sentimentclassification, while causal representation learning with DINER improvesout-of-distribution performance in multilingual sentiment analysis, albeit withvarying gains across languages.</description>
      <author>example@mail.com (Mahi Aminu, Chisom Chibuike, Fatimo Adebanjo, Omokolade Awosanya, Samuel Oyeneye)</author>
      <guid isPermaLink="false">2510.27512v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Model-Aware Counterfactual Explanations for Random Forest</title>
      <link>http://arxiv.org/abs/2510.27397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at XAI-FIN-2025: International Joint Workshop on  Explainable AI in Finance: Achieving Trustworthy Financial Decision-Making;  November 15, 2025; Singapore&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于随机森林的反事实解释方法，用于解决机器学习模型在受监管行业中解释性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型尽管具有强大的预测能力，但在受监管行业（如金融）中应用受限，因为它们提供解释的能力有限。现有的模型无关框架（如Shapley值）通常与所寻求的因果解释不一致。&lt;h4&gt;目的&lt;/h4&gt;解决反事实案例搜索和解释的问题，生成更直观、可行且稀疏有用的解释。&lt;h4&gt;方法&lt;/h4&gt;将反事实搜索和解释问题表述为相似性学习问题，利用随机森林预测模型本身学习的表示。找到反事实后，计算解释的特征重要性作为从原始实例到达反事实所穿越的随机森林分区的函数。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在MNIST手写数字数据集和德国信用数据集上生成的解释比Shapley值更稀疏和有用。&lt;h4&gt;结论&lt;/h4&gt;基于随机森林的反事实解释方法能够提供更符合监管行业需求的解释性结果。&lt;h4&gt;翻译&lt;/h4&gt;尽管机器学习模型具有巨大的预测能力，但由于它们提供解释的能力有限，通常不适合在金融等受监管行业应用。虽然像Shapley值这样的模型无关框架已被证明是方便且流行的，但它们很少符合通常所寻求的因果解释类型。反事实案例解释，即告知个人哪些情况需要不同才能导致结果变化，可能更直观和可行。然而，寻找合适的反事实案例是一个开放的挑战，同样解释哪些特征对结果变化最为关键也是如此。在这里，我们将反事实搜索和解释问题表述为相似性学习，利用随机森林预测模型本身学习的表示。一旦找到反事实，解释的特征重要性被计算为从原始实例到达它所穿越的随机森林分区的函数。我们在MNIST手写数字数据集和德国信用数据集上展示了这种方法，发现它生成的解释比Shapley值更稀疏和有用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite their enormous predictive power, machine learning models are oftenunsuitable for applications in regulated industries such as finance, due totheir limited capacity to provide explanations. While model-agnostic frameworkssuch as Shapley values have proved to be convenient and popular, they rarelyalign with the kinds of causal explanations that are typically sought after.Counterfactual case-based explanations, where an individual is informed ofwhich circumstances would need to be different to cause a change in outcome,may be more intuitive and actionable. However, finding appropriatecounterfactual cases is an open challenge, as is interpreting which featuresare most critical for the change in outcome. Here, we pose the question ofcounterfactual search and interpretation in terms of similarity learning,exploiting the representation learned by the random forest predictive modelitself. Once a counterfactual is found, the feature importance of theexplanation is computed as a function of which random forest partitions arecrossed in order to reach it from the original instance. We demonstrate thismethod on both the MNIST hand-drawn digit dataset and the German creditdataset, finding that it generates explanations that are sparser and moreuseful than Shapley values.</description>
      <author>example@mail.com (Joshua S. Harvey, Guanchao Feng, Sai Anusha Meesala, Tina Zhao, Dhagash Mehta)</author>
      <guid isPermaLink="false">2510.27397v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Functional embeddings enable Aggregation of multi-area SEEG recordings over subjects and sessions</title>
      <link>http://arxiv.org/abs/2510.27090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICLR 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种可扩展的表示学习框架，用于解决跨受试者颅内记录数据聚合的挑战。该框架通过学习电极的功能身份和建模区域间关系，实现了受试者无关的神经数据处理，为大规模跨受试者数据分析和预训练提供了新途径。&lt;h4&gt;背景&lt;/h4&gt;跨受试者颅内记录数据聚合面临电极数量、位置和覆盖区域差异大的挑战。传统空间归一化方法（如MNI坐标）虽然提供了解剖参考，但往往无法捕捉真正的功能相似性，尤其在定位不精确时，即使匹配解剖坐标，不同个体的目标脑区域和神经动力学也可能存在显著差异。&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展的表示学习框架，学习受试者无关的电极功能身份，并建模区域间关系，以实现跨受试者颅内神经数据的有效聚合和分析，特别是在缺乏严格任务结构和均匀传感器布局的情况下。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种双阶段框架：(1) 使用具有对比目标的孪生编码器，从多区域局部场电位中学习每个电极的受试者无关功能身份，诱导对区域特定神经签名具有局部敏感性的嵌入几何结构；(2) 将这些嵌入标记化，用于变换器模型，使用可变数量的通道建模区域间关系。研究在包含20名受试者的基底节-丘脑区域数据集上评估了该方法，这些数据集来自灵活的休息/运动记录会话，具有异构的电极布局。&lt;h4&gt;主要发现&lt;/h4&gt;学习到的功能空间支持准确的受试者内辨别，形成清晰、区域一致的聚类，并能零样本迁移到未见过的通道。变换器在功能标记上操作，无需受试者特定的头部或监督，能够捕获跨区域依赖关系并重建被屏蔽的通道，为下游解码提供了受试者无关的主干。&lt;h4&gt;结论&lt;/h4&gt;该研究为颅内神经数据在缺乏严格任务结构和均匀传感器布局情况下的跨受试者聚合和预训练提供了一条新途径，有望促进大规模神经数据分析的发展。&lt;h4&gt;翻译&lt;/h4&gt;跨受试者颅内记录数据聚合具有挑战性，因为电极数量、放置位置和覆盖区域差异很大。像MNI坐标这样的空间归一化方法提供了共享的解剖参考，但往往无法捕捉真正的功能相似性，尤其是在定位不精确时；即使在匹配的解剖坐标上，不同个体之间的目标脑区域和潜在的神经动力学也可能存在显著差异。我们提出了一种可扩展的表示学习框架，该框架(i)使用具有对比目标的孪生编码器，从多区域局部场电位中学习每个电极的受试者无关功能身份，诱导一种对区域特定神经签名具有局部敏感性的嵌入几何结构，以及(ii)将这些嵌入标记化，用于一个变换器，该变换器使用可变数量的通道建模区域间关系。我们在一个包含20名受试者的数据集上评估了该框架，这些数据集涵盖了基底节-丘脑区域，是在灵活的休息/运动记录会话期间收集的，具有异构的电极布局。学习到的功能空间支持准确的受试者内辨别，并形成清晰、区域一致的聚类；它可以零样本迁移到未见过的通道。变换器在功能标记上操作，无需受试者特定的头部或监督，能够捕获跨区域依赖关系，并重建被屏蔽的通道，为下游解码提供了受试者无关的主干。这些结果表明，在严格的任务结构和均匀传感器布局不可用的情况下，为颅内神经数据实现大规模、跨受试者聚合和预训练提供了一条途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aggregating intracranial recordings across subjects is challenging sinceelectrode count, placement, and covered regions vary widely. Spatialnormalization methods like MNI coordinates offer a shared anatomical reference,but often fail to capture true functional similarity, particularly whenlocalization is imprecise; even at matched anatomical coordinates, the targetedbrain region and underlying neural dynamics can differ substantially betweenindividuals. We propose a scalable representation-learning framework that (i)learns a subject-agnostic functional identity for each electrode frommulti-region local field potentials using a Siamese encoder with contrastiveobjectives, inducing an embedding geometry that is locality-sensitive toregion-specific neural signatures, and (ii) tokenizes these embeddings for atransformer that models inter-regional relationships with a variable number ofchannels. We evaluate this framework on a 20-subject dataset spanning basalganglia-thalamic regions collected during flexible rest/movement recordingsessions with heterogeneous electrode layouts. The learned functional spacesupports accurate within-subject discrimination and forms clear,region-consistent clusters; it transfers zero-shot to unseen channels. Thetransformer, operating on functional tokens without subject-specific heads orsupervision, captures cross-region dependencies and enables reconstruction ofmasked channels, providing a subject-agnostic backbone for downstream decoding.Together, these results indicate a path toward large-scale, cross-subjectaggregation and pretraining for intracranial neural data where strict taskstructure and uniform sensor placement are unavailable.</description>
      <author>example@mail.com (Sina Javadzadeh, Rahil Soroushmojdehi, S. Alireza Seyyed Mousavi, Mehrnaz Asadi, Sumiko Abe, Terence D. Sanger)</author>
      <guid isPermaLink="false">2510.27090v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Incremental Human-Object Interaction Detection with Invariant Relation Representation Learning</title>
      <link>http://arxiv.org/abs/2510.27020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一个名为'无样本增量关系蒸馏'(IRD)的框架，用于解决开放环境中人类-物体交互(HOI)的增量检测问题，以应对动态环境中的交互漂移和零样本HOI组合检测挑战。&lt;h4&gt;背景&lt;/h4&gt;在开放世界环境中，人类-物体交互(HOI)不断演变，这挑战了传统的封闭世界HOI检测模型。人类具有渐进式获取知识的能力，而现有的增量学习模型面临灾难性遗忘问题，以及交互漂移和零样本HOI组合检测的特殊挑战。&lt;h4&gt;目的&lt;/h4&gt;开发能够识别动态环境中人类-物体关系的智能体，即增量HOI检测(IHOID)，以解决增量学习中的灾难性遗忘问题，以及交互漂移和零样本HOI组合检测的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一个无样本增量关系蒸馏(IRD)框架。IRD将物体和关系的学习解耦，并引入了两种独特的蒸馏损失，用于学习在不同HOI组合中共享相同关系的不变关系特征。&lt;h4&gt;主要发现&lt;/h4&gt;在HICO-DET和V-COCO数据集上的大量实验表明，该方法在减轻遗忘、增强对交互漂移的鲁棒性以及零样本HOI的泛化能力方面优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;IRD框架有效地解决了开放世界环境中HOI检测的增量学习挑战，特别是在处理动态交互和零样本场景方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;在开放世界环境中，人类-物体交互(HOI)不断演变，这对传统的封闭世界HOI检测模型提出了挑战。受人类渐进式获取知识能力的启发，我们探索了增量HOI检测(IHOID)，以开发能够识别此类动态环境中人类-物体关系的智能体。这种设置不仅面临增量学习中常见的灾难性遗忘问题，还面临由交互漂移和检测具有连续到达数据的零样本HOI组合带来的特殊挑战。因此，我们提出了一个新颖的无样本增量关系蒸馏(IRD)框架。IRD将物体和关系的学习解耦，并引入了两种独特的蒸馏损失，用于学习在不同HOI组合中共享相同关系的不变关系特征。在HICO-DET和V-COCO数据集上的大量实验证明了我们的方法在减轻遗忘、增强对交互漂移的鲁棒性以及零样本HOI的泛化能力方面优于最先进的基线方法。代码可在以下网址获取：https://github.com/weiyana/ContinualHOI&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In open-world environments, human-object interactions (HOIs) evolvecontinuously, challenging conventional closed-world HOI detection models.Inspired by humans' ability to progressively acquire knowledge, we exploreincremental HOI detection (IHOID) to develop agents capable of discerninghuman-object relations in such dynamic environments. This setup confronts notonly the common issue of catastrophic forgetting in incremental learning butalso distinct challenges posed by interaction drift and detecting zero-shot HOIcombinations with sequentially arriving data. Therefore, we propose a novelexemplar-free incremental relation distillation (IRD) framework. IRD decouplesthe learning of objects and relations, and introduces two unique distillationlosses for learning invariant relation features across different HOIcombinations that share the same relation. Extensive experiments on HICO-DETand V-COCO datasets demonstrate the superiority of our method overstate-of-the-art baselines in mitigating forgetting, strengthening robustnessagainst interaction drift, and generalization on zero-shot HOIs. Code isavailable at \href{https://github.com/weiyana/ContinualHOI}{this HTTP URL}</description>
      <author>example@mail.com (Yana Wei, Zeen Chi, Chongyu Wang, Yu Wu, Shipeng Yan, Yongfei Liu, Xuming He)</author>
      <guid isPermaLink="false">2510.27020v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>FOCUS: Efficient Keyframe Selection for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2510.27280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FOCUS的关键帧选择方法，用于解决多模态大语言模型处理长视频时的标记预算问题，在处理不到2%视频帧的情况下实现了显著的准确性提升。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型将图像和视频帧表示为视觉标记，但扩展到长视频时标记预算会远超实际限制。现有方法要么均匀采样，要么使用小模型进行关键帧选择，但这些方法依赖预过滤且可能错过重要信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需训练、与模型无关的关键帧选择模块，在严格标记预算下选择与查询相关的帧，避免错过重要信息。&lt;h4&gt;方法&lt;/h4&gt;FOCUS将关键帧选择表述为多臂老虎机中的组合纯探索问题，将短时间片段视为臂，使用经验均值和伯恩斯坦置信半径识别信息区域，同时保留对不确定区域的探索。采用两阶段探索-利用程序，先识别高价值时间区域，再在每个区域内选择最高分帧。&lt;h4&gt;主要发现&lt;/h4&gt;在两个长视频问答基准测试中，FOCUS处理不到2%的视频帧实现了显著的准确性提升。对于超过20分钟的视频，在LongVideoBench上实现了11.9%的准确率提升。&lt;h4&gt;结论&lt;/h4&gt;FOCUS作为关键帧选择方法的有效性得到验证，为MLLMs可扩展的长视频理解提供了一种简单通用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)将图像和视频帧表示为视觉标记。然而，从单图像扩展到数小时长的视频会使标记预算远超实际限制。因此，常用方法要么均匀下采样，要么使用较小的视觉语言模型进行基于检索评分的关键帧选择。然而，这些关键帧选择方法仍然依赖选择前的预过滤来降低推理成本，并可能错过最具信息量的时刻。我们提出FOCUS，即Frame-Optimistic Confidence Upper-bound Selection，这是一种无需训练、与模型无关的关键帧选择模块，在严格的标记预算下选择与查询相关的帧。FOCUS将关键帧选择表述为多臂老虎机中的组合纯探索问题：它将短时间片段视为臂，并使用经验均值和伯恩斯坦置信半径来识别信息丰富的区域，同时保留对不确定区域的探索。 resulting两阶段探索-利用程序从具有理论保证的顺序策略简化而来，首先识别高价值时间区域，然后在每个区域内选择得分最高的帧。在两个长视频问答基准测试中，FOCUS在处理不到2%的视频帧的同时提供了显著的准确性提升。对于超过20分钟的视频，它在LongVideoBench上实现了11.9%的准确率提升，证明了其作为关键帧选择方法的有效性，并为MLLMs可扩展的长视频理解提供了一种简单通用的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) represent images and video frames asvisual tokens. Scaling from single images to hour-long videos, however,inflates the token budget far beyond practical limits. Popular pipelinestherefore either uniformly subsample or apply keyframe selection withretrieval-style scoring using smaller vision-language models. However, thesekeyframe selection methods still rely on pre-filtering before selection toreduce the inference cost and can miss the most informative moments.  We propose FOCUS, Frame-Optimistic Confidence Upper-bound Selection, atraining-free, model-agnostic keyframe selection module that selectsquery-relevant frames under a strict token budget. FOCUS formulates keyframeselection as a combinatorial pure-exploration (CPE) problem in multi-armedbandits: it treats short temporal clips as arms, and uses empirical means andBernstein confidence radius to identify informative regions while preservingexploration of uncertain areas. The resulting two-stageexploration-exploitation procedure reduces from a sequential policy withtheoretical guarantees, first identifying high-value temporal regions, thenselecting top-scoring frames within each region On two long-videoquestion-answering benchmarks, FOCUS delivers substantial accuracy improvementswhile processing less than 2% of video frames. For videos longer than 20minutes, it achieves an 11.9% gain in accuracy on LongVideoBench, demonstratingits effectiveness as a keyframe selection method and providing a simple andgeneral solution for scalable long-video understanding with MLLMs.</description>
      <author>example@mail.com (Zirui Zhu, Hailun Xu, Yang Luo, Yong Liu, Kanchan Sarkar, Zhenheng Yang, Yang You)</author>
      <guid isPermaLink="false">2510.27280v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Adapting Large Language Models to Emerging Cybersecurity using Retrieval Augmented Generation</title>
      <link>http://arxiv.org/abs/2510.27080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一个基于检索增强生成(RAG)的框架，用于增强大型语言模型在网络安全任务中的适应性和可靠性，特别是在知识保留和时间推理方面。&lt;h4&gt;背景&lt;/h4&gt;安全应用越来越多地依赖大型语言模型进行网络威胁检测，但其不透明的推理限制了信任，特别是在需要特定领域知识的决策中。安全威胁迅速演变，要求模型不仅回忆历史事件，还要适应新漏洞和攻击模式。RAG在一般LLM应用中已显示出有效性，但在网络安全领域的潜力尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于RAG的框架，使网络安全数据情境化，并增强大型语言模型在知识保留和时间推理方面的准确性。&lt;h4&gt;方法&lt;/h4&gt;使用外部数据集和Llama-3-8B-Instruct模型，评估基准RAG和优化的混合检索方法，并在多个性能指标上进行比较分析。&lt;h4&gt;主要发现&lt;/h4&gt;混合检索方法在增强大型语言模型对网络安全任务的适应性和可靠性方面显示出显著前景。&lt;h4&gt;结论&lt;/h4&gt;基于RAG的框架可以有效提升大型语言模型在网络安全领域的应用能力，特别是在处理不断演变的安全威胁时。&lt;h4&gt;翻译&lt;/h4&gt;安全应用越来越多地依赖大型语言模型(LLMs)进行网络威胁检测；然而，它们的不透明推理常常限制了信任，特别是在需要特定领域网络安全知识的决策中。由于安全威胁迅速演变，LLM不仅要回忆历史事件，还要适应新兴的漏洞和攻击模式。检索增强生成(RAG)已在一般LLM应用中显示出有效性，但其在网络安全领域的潜力仍未得到充分探索。在这项工作中，我们引入了一个基于RAG的框架，用于使网络安全数据情境化，并增强LLM在知识保留和时间推理方面的准确性。使用外部数据集和Llama-3-8B-Instruct模型，我们评估了基准RAG和优化的混合检索方法，并在多个性能指标上进行了比较分析。我们的研究结果强调了混合检索在增强LLM对网络安全任务的适应性和可靠性方面的前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Security applications are increasingly relying on large language models(LLMs) for cyber threat detection; however, their opaque reasoning often limitstrust, particularly in decisions that require domain-specific cybersecurityknowledge. Because security threats evolve rapidly, LLMs must not only recallhistorical incidents but also adapt to emerging vulnerabilities and attackpatterns. Retrieval-Augmented Generation (RAG) has demonstrated effectivenessin general LLM applications, but its potential for cybersecurity remainsunderexplored. In this work, we introduce a RAG-based framework designed tocontextualize cybersecurity data and enhance LLM accuracy in knowledgeretention and temporal reasoning. Using external datasets and theLlama-3-8B-Instruct model, we evaluate baseline RAG, an optimized hybridretrieval approach, and conduct a comparative analysis across multipleperformance metrics. Our findings highlight the promise of hybrid retrieval instrengthening the adaptability and reliability of LLMs for cybersecurity tasks.</description>
      <author>example@mail.com (Arnabh Borah, Md Tanvirul Alam, Nidhi Rastogi)</author>
      <guid isPermaLink="false">2510.27080v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>M^3Detection: Multi-Frame Multi-Level Feature Fusion for Multi-Modal 3D Object Detection with Camera and 4D Imaging Radar</title>
      <link>http://arxiv.org/abs/2510.27166v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;M^3Detection是一个统一的多帧3D物体检测框架，通过融合相机和4D成像雷达数据，实现多级特征融合，解决了单帧融合信息不完整的问题，在多帧检测中取得了最先进的效果。&lt;h4&gt;背景&lt;/h4&gt;4D成像雷达在恶劣天气条件下提供稳健感知，相机传感器提供密集语义信息，两者互补融合对3D感知具有潜力。然而现有方法多限于单帧输入，无法捕捉完整场景，且图像退化和雷达稀疏性影响检测性能。&lt;h4&gt;目的&lt;/h4&gt;解决多帧融合面临的两个挑战：实现跨帧和跨模态的稳健有效物体特征融合，以及减少冗余特征提取的计算成本。&lt;h4&gt;方法&lt;/h4&gt;提出M^3Detection框架，在多模态数据上进行多级特征融合；利用基线检测器的中间特征和跟踪器生成参考轨迹；设计雷达信息引导的全局级间对象特征聚合模块对候选提案进行全局特征对齐；设计局部级间网格特征聚合模块扩展局部特征；使用轨迹级多帧时空推理模块编码跨帧交互并增强时间表示。&lt;h4&gt;主要发现&lt;/h4&gt;在VoD和TJ4DRadSet数据集上的实验表明，M^3Detection实现了最先进的3D检测性能，验证了其在多帧检测与相机-4D成像雷达融合方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;M^3Detection通过多级特征融合和时空推理，有效解决了相机-雷达融合中的信息不完整问题，提高了检测性能，为多模态多帧3D感知提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近的4D成像雷达进展使得在恶劣天气条件下能够实现稳健的感知，而相机传感器则提供密集的语义信息。融合这些互补的模态在成本效益高的3D感知方面具有巨大潜力。然而，大多数现有的相机-雷达融合方法仅限于单帧输入，只能捕捉场景的部分视图。不完整的场景信息，加上图像退化和4D雷达的稀疏性，阻碍了整体检测性能。相比之下，多帧融合提供了更丰富的时空信息，但面临两个挑战：实现跨帧和跨模态的稳健有效物体特征融合，以及减少冗余特征提取的计算成本。因此，我们提出了M^3Detection，一个统一的多帧3D物体检测框架，在来自相机和4D成像雷达的多模态数据上进行多级特征融合。我们的框架利用基线检测器的中间特征并使用跟踪器生成参考轨迹，提高计算效率并为第二阶段提供更丰富的信息。在第二阶段，我们设计了一个雷达信息引导的全局级间对象特征聚合模块，对候选提案进行全局特征对齐，以及一个局部级间网格特征聚合模块，沿着参考轨迹扩展局部特征以增强细粒度物体表示。然后，聚合的特征通过轨迹级多帧时空推理模块进行处理，以编码跨帧交互并增强时间表示。在VoD和TJ4DRadSet数据集上的大量实验表明，M^3Detection实现了最先进的3D检测性能，验证了其在多帧检测与相机-4D成像雷达融合方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决单帧相机-雷达融合3D物体检测的局限性，即单帧输入只能捕捉场景部分视图，导致不完整场景信息、图像退化和4D雷达稀疏性影响检测性能。这个问题在自动驾驶领域至关重要，因为3D物体检测是自动驾驶的基础技术，而多帧信息能提供更丰富的时空上下文，提高检测准确性和鲁棒性，同时解决计算效率问题对实时系统至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了单帧检测的局限性及多帧融合的潜力与挑战，然后设计了两阶段框架：第一阶段利用基线检测器提取特征并生成初始检测结果和参考轨迹；第二阶段进行多帧多级特征融合。作者借鉴了现有BEV特征融合检测器（如BEVFusion）、两阶段检测框架（如MPPNet）、注意力机制和跟踪算法（如Immortal），但创新性地设计了多级特征融合策略来解决多模态多帧检测问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多帧多级特征融合提升相机和4D雷达多模态3D物体检测性能，利用丰富时空信息同时避免冗余特征提取。流程分两阶段：第一阶段使用基线检测器提取特征并生成轨迹；第二阶段包括三个模块：全局级间物体特征聚合(GOA)减轻跟踪不确定性，局部级间网格特征聚合(LGA)增强细粒度表示，轨迹级多帧时空推理(MSTR)建模时间特征交互，最终融合全局和局部特征进行检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的多帧3D物体检测框架M3Detection；2)多帧多级特征融合策略(GOA、LGA和MSTR模块)。相比之前工作，不同之处在于：避免了传统多帧方法的冗余特征提取；采用多级特征融合同时关注全局上下文和局部细节；专门针对相机和4D雷达多模态融合优化；通过多假设策略减轻跟踪不确定性影响；不仅进行场景级时间聚合，还进行物体级精细建模。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; M3Detection通过创新的多帧多级特征融合框架，有效整合了相机和4D成像雷达的互补信息，在保持计算效率的同时显著提高了3D物体检测的准确性和鲁棒性，为自动驾驶感知系统提供了更可靠的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in 4D imaging radar have enabled robust perception in adverseweather, while camera sensors provide dense semantic information. Fusing thethese complementary modalities has great potential for cost-effective 3Dperception. However, most existing camera-radar fusion methods are limited tosingle-frame inputs, capturing only a partial view of the scene. The incompletescene information, compounded by image degradation and 4D radar sparsity,hinders overall detection performance. In contrast, multi-frame fusion offersricher spatiotemporal information but faces two challenges: achieving robustand effective object feature fusion across frames and modalities, andmitigating the computational cost of redundant feature extraction.Consequently, we propose M^3Detection, a unified multi-frame 3D objectdetection framework that performs multi-level feature fusion on multi-modaldata from camera and 4D imaging radar. Our framework leverages intermediatefeatures from the baseline detector and employs the tracker to producereference trajectories, improving computational efficiency and providing richerinformation for second-stage. In the second stage, we design a global-levelinter-object feature aggregation module guided by radar information to alignglobal features across candidate proposals and a local-level inter-grid featureaggregation module that expands local features along the reference trajectoriesto enhance fine-grained object representation. The aggregated features are thenprocessed by a trajectory-level multi-frame spatiotemporal reasoning module toencode cross-frame interactions and enhance temporal representation. Extensiveexperiments on the VoD and TJ4DRadSet datasets demonstrate that M^3Detectionachieves state-of-the-art 3D detection performance, validating itseffectiveness in multi-frame detection with camera-4D imaging radar fusion.</description>
      <author>example@mail.com (Xiaozhi Li, Huijun Di, Jian Li, Feng Liu, Wei Liang)</author>
      <guid isPermaLink="false">2510.27166v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>MLPerf Automotive</title>
      <link>http://arxiv.org/abs/2510.27065v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 5 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MLPerf Automotive是首个用于评估汽车系统中AI加速部署的机器学习系统的标准化公共基准测试。&lt;h4&gt;背景&lt;/h4&gt;现有的基准测试套件无法用于汽车系统，因为汽车工作负载具有独特的约束，包括安全性和实时处理能力，这些特性使它们区别于之前引入基准测试所针对的领域。&lt;h4&gt;目的&lt;/h4&gt;解决汽车机器学习系统需要标准化性能评估方法的问题。&lt;h4&gt;方法&lt;/h4&gt;通过MLCommons和自动驾驶计算联盟之间的合作开发，提供延迟和准确性指标以及评估协议，使不同硬件平台和软件实现之间能够进行一致且可重复的性能比较。&lt;h4&gt;主要发现&lt;/h4&gt;第一版基准测试包括2D目标检测、2D语义分割和3D目标检测等汽车感知任务。描述了基准设计的方法论，包括任务选择、参考模型和提交规则。讨论了第一轮基准提交以及获取数据集和开发参考实现所涉及的挑战。&lt;h4&gt;结论&lt;/h4&gt;MLPerf Automotive基准测试为汽车AI系统提供了标准化的评估方法，使不同平台和实现之间的性能比较成为可能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了MLPerf Automotive，这是首个用于评估部署在汽车系统中用于AI加速的机器学习系统的标准化公共基准测试。该基准测试由MLCommons和自动驾驶计算联盟通过合作开发，解决了汽车机器学习系统需要标准化性能评估方法的需求。现有的基准测试套件无法用于这些系统，因为汽车工作负载具有独特的约束，包括安全性和实时处理能力，这些特性使它们区别于之前引入基准测试所针对的领域。我们的基准测试框架提供了延迟和准确性指标以及评估协议，使不同硬件平台和软件实现之间能够进行一致且可重复的性能比较。第一版基准测试包括2D目标检测、2D语义分割和3D目标检测等汽车感知任务。我们描述了基准设计背后的方法论，包括任务选择、参考模型和提交规则。我们还讨论了第一轮基准提交以及获取数据集和开发参考实现所涉及的挑战。我们的基准测试代码可在https://github.com/mlcommons/mlperf_automotive获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present MLPerf Automotive, the first standardized public benchmark forevaluating Machine Learning systems that are deployed for AI acceleration inautomotive systems. Developed through a collaborative partnership betweenMLCommons and the Autonomous Vehicle Computing Consortium, this benchmarkaddresses the need for standardized performance evaluation methodologies inautomotive machine learning systems. Existing benchmark suites cannot beutilized for these systems since automotive workloads have unique constraintsincluding safety and real-time processing that distinguish them from thedomains that previously introduced benchmarks target. Our benchmarkingframework provides latency and accuracy metrics along with evaluation protocolsthat enable consistent and reproducible performance comparisons acrossdifferent hardware platforms and software implementations. The first iterationof the benchmark consists of automotive perception tasks in 2D objectdetection, 2D semantic segmentation, and 3D object detection. We describe themethodology behind the benchmark design including the task selection, referencemodels, and submission rules. We also discuss the first round of benchmarksubmissions and the challenges involved in acquiring the datasets and theengineering efforts to develop the reference implementations. Our benchmarkcode is available at https://github.com/mlcommons/mlperf_automotive.</description>
      <author>example@mail.com (Radoyeh Shojaei, Predrag Djurdjevic, Mostafa El-Khamy, James Goel, Kasper Mecklenburg, John Owens, Pınar Muyan-Özçelik, Tom St. John, Jinho Suh, Arjun Suresh)</author>
      <guid isPermaLink="false">2510.27065v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>UniField: Joint Multi-Domain Training for Universal Surface Pressure Modeling</title>
      <link>http://arxiv.org/abs/2510.24106v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UniField的方法，通过整合多个子领域的空气动力学数据进行联合训练，解决了数据稀缺性问题，实现了更通用的流场表示。&lt;h4&gt;背景&lt;/h4&gt;表面压力场的空气动力学模拟对许多工程问题至关重要。深度神经网络已成为传统计算流体力学(CFD)模拟的高效替代方案，但数据稀缺性限制了神经网络的应用。&lt;h4&gt;目的&lt;/h4&gt;整合多个子领域的空气动力学数据进行联合训练，以学习更通用的流场表示，解决数据稀缺性问题。&lt;h4&gt;方法&lt;/h4&gt;整合五个涵盖汽车、火车、飞机和一般形状等不同领域的数据集。提出UniField方法，采用领域无关的Transformer模块提取通用点云特征，并定制领域特定的流条件适配器以适应不同子领域的流信息。&lt;h4&gt;主要发现&lt;/h4&gt;尽管不同子领域的空气动力学数据遵循不同方程，但联合训练的模型比单独训练的模型表现更好，表明这些数据相互补充，帮助模型学习更好的流场表示。&lt;h4&gt;结论&lt;/h4&gt;UniField作为通用流场表示模型具有潜力，为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;物体表面压力场的空气动力学模拟对许多工程问题至关重要。近年来，深度神经网络已成为传统计算成本高昂的CFD模拟的高效替代方案，用于建模表面压力场。然而，数据稀缺性仍然是一个基本挑战，限制了神经网络的应用。为了解决这一限制，我们提出整合多个子领域的空气动力学数据进行联合训练，以学习更通用的流场表示。我们整合了五个涵盖不同领域的数据集，包括汽车、火车、飞机和一般形状。面对不同领域间的显著数据差异，我们提出了UniField，它采用领域无关的Transformer模块提取通用点云特征，并定制领域特定的流条件适配器以适应不同子领域的流信息。尽管不同子领域的空气动力学数据通常遵循不同的方程，但我们比较了在所有数据上联合训练的模型与在单个数据集上分别训练的模型，发现联合训练的模型通常表现更好。这表明这些数据相互补充，帮助模型学习更好的流场表示。这些结果突显了UniField作为通用流场表示模型的潜力，为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aerodynamic simulation of the surface pressure field around objects iscrucial for many engineering problems. In recent years, deep neural networkshave emerged as an efficient alternative to traditional, computationallyexpensive CFD simulations for modeling surface pressure fields. However, datascarcity remains a fundamental challenge, limiting the application of neuralnetworks. To address this limitation, we propose to integrate aerodynamic datafrom multiple subfields and conduct joint training to learn more general fieldrepresentations. We consolidate five different datasets covering variousfields, including automobiles, trains, aircraft, and general shapes. Facingsignificant data differences across different domains, we propose UniField,which employs a domain-agnostic Transformer module to extract general pointcloud features and customizes domain-specific flow-conditioned adapters toadapt to the flow information in different subfields. Despite the fact thataerodynamic data from different subfields are typically governed by differentequations, we compare models trained jointly on all data with those trainedseparately on individual datasets and find that the jointly-trained modelcommonly demonstrates better performance. This indicates that these datacomplement each other to help the model learn better flow fieldrepresentations. These results highlight the potential of UniField as auniversal flow field representation model and lay the foundation for broaderapplications of neural networks in aerodynamic analysis.</description>
      <author>example@mail.com (Junhong Zou, Zhenxu Sun, Yueqing Wang, Wei Qiu, Zhaoxiang Zhang, Zhen Lei, Xiangyu Zhu)</author>
      <guid isPermaLink="false">2510.24106v2</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
  <item>
      <title>Linearized Optimal Transport for Analysis of High-Dimensional Point-Cloud and Single-Cell Data</title>
      <link>http://arxiv.org/abs/2510.22033v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种线性最优传输(LOT)框架，用于处理单细胞技术生成的高维点云数据，实现了预测准确性、可解释性和生成建模的统一。&lt;h4&gt;背景&lt;/h4&gt;单细胞技术生成高维点云数据，能详细表征复杂患者状态和治疗反应。每个患者由不规则点云而非简单向量表示，难以直接量化和比较个体间生物学差异。现有非线性方法虽准确但如同黑箱，生物学可解释性差。&lt;h4&gt;目的&lt;/h4&gt;开发一种既能保持预测准确性又具有生物学可解释性的方法，解决单细胞点云数据分析中的挑战。&lt;h4&gt;方法&lt;/h4&gt;适配线性最优传输(LOT)框架，将不规则点云嵌入固定维度欧几里得空间，同时保留分布结构。这种嵌入提供有原则的线性表示，形成患者间的配准，支持直接比较细胞分布。&lt;h4&gt;主要发现&lt;/h4&gt;LOT实现了：(i) COVID-19患者状态的准确且可解释的分类，分类器权重映射回特定标志物和空间区域；(ii) 患者来源类器官的合成数据生成。LOT形心产生平均细胞谱，支持药物相互作用测试。&lt;h4&gt;结论&lt;/h4&gt;LOT作为统一框架连接了预测性能、可解释性和生成建模，通过将异质点云转换为结构化嵌入，为理解高维生物系统中的免疫变异和治疗效应开辟新机会。&lt;h4&gt;翻译&lt;/h4&gt;单细胞技术生成细胞的高维点云，能够详细表征复杂的患者状态和治疗反应。然而每个患者由不规则点云而非简单向量表示，使得难以直接量化和比较个体间的生物学差异。诸如核方法和神经网络等非线性方法能达到预测准确性，但如同黑箱，提供很少的生物学可解释性。为解决这些局限性，我们将线性最优传输(LOT)框架适配到这一场景，将不规则点云嵌入到固定维度的欧几里得空间，同时保留分布结构。这种嵌入提供了有原则的线性表示，保留了最优传输几何，同时支持下游分析。它还形成了任意两个患者之间的配准，使其能够直接比较细胞分布。在此空间中，LOT实现了：(i) COVID-19患者状态的准确且可解释的分类，其中分类器权重映射回驱动预测的特定标志物和空间区域；(ii) 患者来源类器官的合成数据生成，利用LOT嵌入的线性性。LOT形心产生平均细胞谱，代表组合条件或样本，支持药物相互作用测试。这些结果共同确立了LOT作为连接预测性能、可解释性和生成建模的统一框架。通过将异质点云转换为可直接追溯到原始数据的结构化嵌入，LOT为理解高维生物系统中的免疫变异和治疗效应开辟了新机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell technologies generate high-dimensional point clouds of cells,enabling detailed characterization of complex patient states and treatmentresponses. Yet each patient is represented by an irregular point cloud ratherthan a simple vector, making it difficult to directly quantify and comparebiological differences between individuals. Nonlinear methods such as kernelsand neural networks achieve predictive accuracy but act as black boxes,offering little biological interpretability.  To address these limitations, we adapt the Linear Optimal Transport (LOT)framework to this setting, embedding irregular point clouds into afixed-dimensional Euclidean space while preserving distributional structure.This embedding provides a principled linear representation that preservesoptimal transport geometry while enabling downstream analysis. It also forms aregistration between any two patients, enabling direct comparison of theircellular distributions. Within this space, LOT enables: (i) \textbf{accurateand interpretable classification} of COVID-19 patient states, where classifierweights map back to specific markers and spatial regions driving predictions;and (ii) \textbf{synthetic data generation} for patient-derived organoids,exploiting the linearity of the LOT embedding. LOT barycenters yield averagedcellular profiles representing combined conditions or samples, supporting druginteraction testing.  Together, these results establish LOT as a unified framework that bridgespredictive performance, interpretability, and generative modeling. Bytransforming heterogeneous point clouds into structured embeddings directlytraceable to the original data, LOT opens new opportunities for understandingimmune variation and treatment effects in high-dimensional biological systems.</description>
      <author>example@mail.com (Tianxiang Wang, Yingtong Ke, Dhananjay Bhaskar, Smita Krishnaswamy, Alexander Cloninger)</author>
      <guid isPermaLink="false">2510.22033v2</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Clone Deterministic 3D Worlds with Geometrically-Regularized World Models</title>
      <link>http://arxiv.org/abs/2510.26782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种几何正则化世界模型(GRWM)，通过改进表示学习来提高世界模型的性能，特别是在长期预测任务中表现出更好的保真度和稳定性。&lt;h4&gt;背景&lt;/h4&gt;世界模型是一种内部模型，用于模拟世界的发展，基于过去的观察和行动预测智能体及其环境的未来。准确的世界模型对智能体在复杂环境中有效思考、规划和推理至关重要。然而，当前世界模型在长期范围内表现脆弱且会退化。&lt;h4&gt;目的&lt;/h4&gt;研究仅通过改进表示学习是否能显著提高世界模型的性能，并构建一个能够完全克隆并拟合确定性3D世界的模型。&lt;h4&gt;方法&lt;/h4&gt;提出几何正则化世界模型(GRWM)，强制自然感觉轨迹上的连续点在潜在表示空间中保持接近，从而学习与环境真实拓扑紧密对齐的潜在表示。&lt;h4&gt;主要发现&lt;/h4&gt;GRWM显著提高了确定性3D环境和长期预测任务中的滚动保真度和稳定性，其优势源于学习具有优越几何结构的潜在流形。GRWM是即插即用的，只需最小架构修改，可随轨迹长度扩展，且兼容各种潜在生成主干网络。&lt;h4&gt;结论&lt;/h4&gt;改进表示学习是构建健壮世界模型的直接且有用的途径，无需扩大动态模块即可提供可靠的长期预测。&lt;h4&gt;翻译&lt;/h4&gt;世界模型是一种内部模型，用于模拟世界的发展。基于过去的观察和行动，它预测智能体及其环境的未来。准确的世界模型对于智能体在复杂动态环境中有效思考、规划和推理至关重要。尽管进展迅速，但当前世界模型仍然脆弱，且在长期范围内会退化。我们认为，一个主要原因是表示质量：外部输入（如图像）是高维度的，且有损或纠缠的潜在表示使动态学习变得不必要地困难。因此，我们研究仅通过改进表示学习是否能显著提高世界模型的性能。在本文中，我们通过解决一个基本但尚未解决的问题，朝着构建真正准确的世界模型迈出了一步：构建一个能够完全克隆并拟合确定性3D世界的模型。我们提出了几何正则化世界模型(GRWM)，强制自然感觉轨迹上的连续点在潜在表示空间中保持接近。这种方法产生了显著改进的潜在表示，与环境真实拓扑紧密对齐。GRWM是即插即用的，只需要最小的架构修改，可随轨迹长度扩展，并且兼容各种潜在生成主干网络。在确定性3D环境和长期预测任务中，GRWM显著提高了滚动保真度和稳定性。分析表明，其优势源于学习具有优越几何结构的潜在流形。这些发现支持一个明确的结论：改进表示学习是构建健壮世界模型的直接且有用的途径，无需扩大动态模块即可提供可靠的长期预测。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是如何构建能够准确克隆确定性3D世界的世界模型，特别是解决长期预测中误差累积导致轨迹偏离现实的问题。这个问题在现实中非常重要，因为世界模型是强化学习、机器人规划和游戏内容生成等应用的核心工具，而当前世界模型在长期预测方面表现脆弱，无法满足需要精确模拟环境的实际应用需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过实验发现，当世界模型直接使用真实底层状态时预测效果极佳，但使用标准VAE的潜在空间时性能急剧下降，这使他们认识到表示质量是主要瓶颈。他们借鉴了对比学习(特别是时序对比学习)和几何正则化在3D物体表示学习中的应用，将其扩展到3D环境建模领域。具体设计上，他们提出了结合时序上下文架构和时序对比正则化的GRWM方法，确保潜在空间结构与环境的真实状态流形一致。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过改进表示学习来提升世界模型的性能，使潜在空间结构与环境的真实状态流形保持一致。整体实现流程包括：1)使用因果编码器处理连续观察序列生成潜在表示；2)结合重构损失、KL散度项、时序缓慢损失(确保连续状态在潜在空间中接近)和潜在均匀性损失(防止特征坍塌)进行训练；3)将训练好的潜在表示作为输入提供给各种动力学模型进行状态转换学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)重新定义问题，从开放世界生成转向确定性环境的精确复制；2)提出'表示质量优先'的观点，与当前侧重改进动力学模型的主流思路不同；3)引入几何正则化方法，确保潜在空间结构与环境的真实拓扑一致；4)设计插件式组件，可无缝集成到现有模型中；5)显著提高长期轨迹预测的稳定性和准确性。相比之前工作，GRWM更注重表示质量而非复杂动力学模型，结合了时序上下文和几何正则化，且完全无监督，不需要真实状态标签。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过几何正则化改进潜在表示学习，GRWM显著提高了确定性3D世界模型的长期预测准确性和稳定性，证明了表示质量对构建可靠世界模型的关键作用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A world model is an internal model that simulates how the world evolves.Given past observations and actions, it predicts the future of both theembodied agent and its environment. Accurate world models are essential forenabling agents to think, plan, and reason effectively in complex, dynamicsettings. Despite rapid progress, current world models remain brittle anddegrade over long horizons. We argue that a central cause is representationquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy orentangled latents make dynamics learning unnecessarily hard. We therefore askwhether improving representation learning alone can substantially improveworld-model performance. In this work, we take a step toward building a trulyaccurate world model by addressing a fundamental yet open problem: constructinga model that can fully clone and overfit to a deterministic 3D world. Wepropose Geometrically-Regularized World Models (GRWM), which enforces thatconsecutive points along a natural sensory trajectory remain close in latentrepresentation space. This approach yields significantly improved latentrepresentations that align closely with the true topology of the environment.GRWM is plug-and-play, requires only minimal architectural modification, scaleswith trajectory length, and is compatible with diverse latent generativebackbones. Across deterministic 3D settings and long-horizon prediction tasks,GRWM significantly increases rollout fidelity and stability. Analyses show thatits benefits stem from learning a latent manifold with superior geometricstructure. These findings support a clear takeaway: improving representationlearning is a direct and useful path to robust world models, deliveringreliable long-horizon predictions without enlarging the dynamics module.</description>
      <author>example@mail.com (Zaishuo Xia, Yukuan Lu, Xinyi Li, Yifan Xu, Yubei Chen)</author>
      <guid isPermaLink="false">2510.26782v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens</title>
      <link>http://arxiv.org/abs/2510.26372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UniTok-Audio框架，解决了音频生成模型在质量和泛化能力方面的挑战，实现了统一的音频生成任务处理。&lt;h4&gt;背景&lt;/h4&gt;生成式建模在文本、图像和音频领域取得显著成功，但音频生成模型仍面临音频质量和跨任务泛化能力的挑战，导致开发冗余、性能不一致和扩展性有限。&lt;h4&gt;目的&lt;/h4&gt;提出UniTok-Audio，一个可扩展且可扩展的统一音频生成任务框架，解决现有音频生成模型的碎片化问题。&lt;h4&gt;方法&lt;/h4&gt;1) 提取条件的连续特征，以自回归方式生成目标音频的离散令牌；2) 使用特殊任务标识符令牌统一不同任务的学习模式；3) 开发包含声学和语义分支的双流音频编解码器实现高保真波形重构。&lt;h4&gt;主要发现&lt;/h4&gt;UniTok-Audio在五个时间对齐任务（语音恢复、目标说话人提取、语音分离、语音转换和语言查询音频源分离）上与最先进的特定任务或多任务系统相比具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;UniTok-Audio提供了一个统一的音频生成框架，解决了现有模型的碎片化问题，作者将开源代码库以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;生成式建模最近在文本、图像和音频领域取得了显著成功，展示了统一表征学习的强大能力。然而，音频生成模型在音频质量和跨任务泛化能力方面仍面临挑战。这种碎片化导致了冗余的开发工作、不一致的性能和有限的扩展性。为了解决这些问题，我们提出了UniTok-Audio，一个可扩展且可扩展的统一音频生成任务框架。具体而言，1) UniTok-Audio以自回归方式提取条件的连续特征，生成目标音频的离散令牌；2) 特殊的任务标识符令牌在单一框架中统一了多种任务的学习模式；3) 开发了包含声学和语义分支的双流音频编解码器，用于高保真波形重构。实验结果表明，UniTok-Audio在五个时间对齐任务（语音恢复、目标说话人提取、语音分离、语音转换和语言查询音频源分离）上与最先进的特定任务或多任务系统相比具有竞争力的性能。为了促进未来的研究，我们将开源我们的代码库。我们工作的演示页面可以在以下网址找到：https://alibaba.github.io/unified-audio。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling has recently achieved remarkable success across text,image, and audio domains, demonstrating powerful capabilities for unifiedrepresentation learning. However, audio generation models still face challengesin terms of audio quality and generalization ability across tasks. Thisfragmentation results in redundant development efforts, inconsistentperformance, and limited extensibility. To address these issues, we propose\textbf{UniTok-Audio}, a scalable and extensible framework for unified audiogeneration tasks. Specifically, 1) UniTok-Audio extracts continuous feature ofconditions to generates discrete tokens of target audio in an autoregressivemanner; 2) a special task identifier token unifies different learning patternsof multiple tasks in a single framework; 3) a dual-stream audio codec involvingacoustic and semantic branch is developed for high-fidelity waveformreconstruction. Experimental results demonstrate that UniTok-Audio achievescompetitive performance in comparation with state-of-the-art task-specific ormulti-task systems across five time-aligned tasks: speech restoration, targetspeaker extraction, speech separation, voice conversion, and language-queriedaudio source separation. To foster future research, we will open-source ourcodebase. The demo page of our work can be found here:https://alibaba.github.io/unified-audio.</description>
      <author>example@mail.com (Chengwei Liu, Haoyin Yan, Shaofei Xue, Xiaotao Liang, Yinghao Liu, Zheng Xue, Gang Song, Boyang Zhou)</author>
      <guid isPermaLink="false">2510.26372v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens</title>
      <link>http://arxiv.org/abs/2510.26302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对CLIP模型在组合推理方面的局限性提出了一种基于标记的因果表示学习框架，揭示了CLIP在处理对象、属性和关系组合时的脆弱性根源，并为改进模型提供了理论指导。&lt;h4&gt;背景&lt;/h4&gt;CLIP模型通过在共享嵌入空间中对齐图像和文本实现了强大的跨模态泛化能力，但在处理对象、属性和关系的组合推理方面持续失败，其行为类似于词袋匹配器。先前的因果解释通常将文本建模为单个向量，掩盖了标记级别的结构，无法解释提示敏感性和对困难样本失败等核心现象。&lt;h4&gt;目的&lt;/h4&gt;解决现有CLIP模型在组合推理方面的局限性，提供对标记级别结构的更好理解，并解释模型在提示敏感性和困难样本处理上的失败原因。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于标记的因果表示学习（CRL）框架，该框架基于顺序的语言标记结构因果模型（SCM）。将块可识别性理论扩展到标记化文本，证明CLIP的对比目标可以在句子级和标记级SCM下恢复模态不变潜在变量。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了CLIP的组合脆弱性源于'组合不可识别性'现象。存在伪最优文本编码器，它们可以实现完美的模态不变对齐，但对SWAP、REPLACE和ADD操作在原子概念上明显不敏感，因此无法区分正确的标题和困难样本。分析还表明语言侧的不可识别性与视觉侧的失败通过模态差距相联系，迭代组合运算会增加问题难度。&lt;h4&gt;结论&lt;/h4&gt;标记级别的因果表示学习框架能够解释CLIP在组合推理方面的失败，这些发现为改进负样本挖掘策略和提高模型组合推理能力提供了理论依据。&lt;h4&gt;翻译&lt;/h4&gt;对比语言-图像预训练（CLIP）通过在共享嵌入空间中对齐图像和文本，提供了强大的跨模态泛化能力，但在处理对象、属性和关系的组合推理方面持续失败，其行为常常类似于词袋匹配器。先前的因果解释通常将文本建模为单个向量，掩盖了标记级别的结构，无法解释提示敏感性和对困难样本失败等核心现象。我们通过基于标记的因果表示学习（CRL）框架解决了这一差距，该框架基于顺序的语言标记结构因果模型（SCM）。我们的理论将块可识别性扩展到标记化文本，证明CLIP的对比目标可以在句子级和标记级SCM下恢复模态不变的潜在变量。关键是，标记粒度为CLIP的组合脆弱性提供了第一个原则性解释：组合不可识别性。我们展示了伪最优文本编码器的存在，这些编码器可以实现完美的模态不变对齐，但对SWAP、REPLACE和ADD操作在原子概念上明显不敏感，因此尽管与真正最优编码器优化相同的训练目标，却无法区分正确的标题和困难样本。分析进一步将语言侧的不可识别性与视觉侧的失败通过模态差距联系起来，并展示了迭代组合运算如何增加难度，从而改进了负样本挖掘策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Language-Image Pre-training (CLIP) delivers strong cross modalgeneralization by aligning images and texts in a shared embedding space, yet itpersistently fails at compositional reasoning over objects, attributes, andrelations often behaving like a bag-of-words matcher. Prior causal accountstypically model text as a single vector, obscuring token-level structure andleaving core phenomena-such as prompt sensitivity and failures on hardnegatives unexplained. We address this gap with a token-aware causalrepresentation learning (CRL) framework grounded in a sequential,language-token SCM. Our theory extends block identifiability to tokenized text,proving that CLIP's contrastive objective can recover the modal-invariantlatent variable under both sentence-level and token-level SCMs. Crucially,token granularity yields the first principled explanation of CLIP'scompositional brittleness: composition nonidentifiability. We show theexistence of pseudo-optimal text encoders that achieve perfect modal-invariantalignment yet are provably insensitive to SWAP, REPLACE, and ADD operationsover atomic concepts, thereby failing to distinguish correct captions from hardnegatives despite optimizing the same training objective as true-optimalencoders. The analysis further links language-side nonidentifiability tovisual-side failures via the modality gap and shows how iterated compositionoperators compound hardness, motivating improved negative mining strategies.</description>
      <author>example@mail.com (Ziliang Chen, Tianang Xiao, Jusheng Zhang, Yongsen Zheng, Xipeng Chen)</author>
      <guid isPermaLink="false">2510.26302v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations with LLMs</title>
      <link>http://arxiv.org/abs/2510.26178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了ReaKase-8B框架，通过整合法律事实、法律问题、法律关系三元组和法律推理，显著提高了法律案例检索的性能。&lt;h4&gt;背景&lt;/h4&gt;法律案例检索(LCR)是现实世界法律决策的基石，现有方法主要依赖传统的词汇模型和预训练语言模型编码法律文本，但忽略了法律实体间的关系以及法律事实和法律问题如何导致司法决策的推理过程。&lt;h4&gt;目的&lt;/h4&gt;将法律关系信息和关键推理过程整合到精确的案例嵌入中，以提高案例检索的准确性，并提出ReaKase-8B框架有效利用这些信息进行法律案例检索。&lt;h4&gt;方法&lt;/h4&gt;ReaKase-8B设计了上下文法律案例表示学习范式，使用微调的大语言模型，整合提取的法律事实、法律问题、法律关系三元组和法律推理信息。&lt;h4&gt;主要发现&lt;/h4&gt;在COLIEE 2022和COLIEE 2023两个基准数据集上的实验表明，知识和推理增强的嵌入显著提高了检索性能，超越了基线模型。&lt;h4&gt;结论&lt;/h4&gt;集成法律推理到法律案例检索系统中具有巨大潜力，ReaKase-8B框架展示了这种整合的有效性，代码已在GitHub发布。&lt;h4&gt;翻译&lt;/h4&gt;该摘要已按要求翻译为中文，提取了论文的核心要素，包括背景、目的、方法、发现和结论，并以JSON格式组织。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Legal case retrieval (LCR) is a cornerstone of real-world legal decisionmaking, as it enables practitioners to identify precedents for a given querycase. Existing approaches mainly rely on traditional lexical models andpretrained language models to encode the texts of legal cases. Yet there arerich information in the relations among different legal entities as well as thecrucial reasoning process that uncovers how legal facts and legal issues canlead to judicial decisions. Such relational reasoning process reflects thedistinctive characteristics of each case that can distinguish one from another,mirroring the real-world judicial process. Naturally, incorporating suchinformation into the precise case embedding could further enhance the accuracyof case retrieval. In this paper, a novel ReaKase-8B framework is proposed toleverage extracted legal facts, legal issues, legal relation triplets and legalreasoning for effective legal case retrieval. ReaKase-8B designs an in-contextlegal case representation learning paradigm with a fine-tuned large languagemodel. Extensive experiments on two benchmark datasets from COLIEE 2022 andCOLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddingssubstantially improve retrieval performance over baseline models, highlightingthe potential of integrating legal reasoning into legal case retrieval systems.The code has been released on https://github.com/yanran-tang/ReaKase-8B.</description>
      <author>example@mail.com (Yanran Tang, Ruihong Qiu, Xue Li, Zi Huang)</author>
      <guid isPermaLink="false">2510.26178v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment</title>
      <link>http://arxiv.org/abs/2510.26157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025 (main)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MolBridge，一种基于亚结构感知对齐的新型分子-文本学习框架，通过增强分子亚结构和化学短语之间的细粒度对齐，有效提升了分子表示学习的性能。&lt;h4&gt;背景&lt;/h4&gt;分子和文本表示学习越来越受到关注，因为它有潜力增强对化学信息的理解。然而，现有模型往往难以捕捉分子及其描述之间的细微差异。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型缺乏学习分子亚结构和化学短语之间细粒度对齐能力的问题，提高分子表示学习的准确性。&lt;h4&gt;方法&lt;/h4&gt;通过从分子亚结构和化学短语中衍生的额外对齐信号来增强原始分子-描述对，采用亚结构感知对比学习，并结合自我完善机制来过滤有噪声的对齐信号。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MolBridge能够有效捕获细粒度对应关系，并在广泛的分子基准测试中优于最先进的基线模型。&lt;h4&gt;结论&lt;/h4&gt;亚结构感知对齐在分子-文本学习中具有重要意义，MolBridge框架为分子表示学习提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;分子和文本表示学习因其增强化学信息理解的潜力而日益受到关注。然而，现有模型往往难以捕捉分子及其描述之间的细微差异，因为它们缺乏学习分子亚结构和化学短语之间细粒度对齐的能力。为解决这一局限性，我们引入了MolBridge，一种基于亚结构感知对齐的新型分子-文本学习框架。具体而言，我们通过从分子亚结构和化学短语中衍生的额外对齐信号来增强原始分子-描述对。为了有效学习这些丰富的对齐信息，MolBridge采用亚结构感知对比学习，并结合一种自我完善机制来过滤有噪声的对齐信号。实验结果表明，MolBridge能够有效捕获细粒度对应关系，并在广泛的分子基准测试中优于最先进的基线模型，突显了亚结构感知对齐在分子-文本学习中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecule and text representation learning has gained increasing interest dueto its potential for enhancing the understanding of chemical information.However, existing models often struggle to capture subtle differences betweenmolecules and their descriptions, as they lack the ability to learnfine-grained alignments between molecular substructures and chemical phrases.To address this limitation, we introduce MolBridge, a novel molecule-textlearning framework based on substructure-aware alignments. Specifically, weaugment the original molecule-description pairs with additional alignmentsignals derived from molecular substructures and chemical phrases. Toeffectively learn from these enriched alignments, MolBridge employssubstructure-aware contrastive learning, coupled with a self-refinementmechanism that filters out noisy alignment signals. Experimental results showthat MolBridge effectively captures fine-grained correspondences andoutperforms state-of-the-art baselines on a wide range of molecular benchmarks,highlighting the significance of substructure-aware alignment in molecule-textlearning.</description>
      <author>example@mail.com (Hyuntae Park, Yeachan Kim, SangKeun Lee)</author>
      <guid isPermaLink="false">2510.26157v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization</title>
      <link>http://arxiv.org/abs/2510.26068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新颖的机器学习范式，超越了传统的参数优化方法。它将模型本身视为可变形的几何实体，通过优化流形上的度量张量场来动态塑造模型空间的几何结构。&lt;h4&gt;背景&lt;/h4&gt;传统的机器学习方法在固定的几何空间内搜索最优参数，而本文提出了一种不同的思路。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的机器学习范式，通过优化度量张量场来动态调整模型的几何结构，从而提高模型的表示能力和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;构建了一个变分框架，其损失函数平衡了数据保真度和流形的内在几何复杂度。为解决这个无限维优化问题的计算挑战，引入了一种基于离散微分几何的实用方法，将连续流形离散化为三角形网格，并通过边长参数化度量张量。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析揭示了该框架与广义相对论中的爱因斯坦-希尔伯特作用之间的深刻类比，为'数据驱动几何'概念提供了优雅的物理解释。即使拓扑结构固定，度量优化也比固定几何的模型具有更强的表达能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作为构建能够自主进化其几何和拓扑结构的完全动态'元学习器'奠定了坚实基础，并在科学模型发现和鲁棒表示学习等领域具有广阔的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种超越传统参数优化的机器学习新范式。与在固定几何空间内搜索最优参数的传统方法不同，我们的核心思想是将模型本身视为可变形的几何实体。具体而言，我们在具有预定义拓扑的流形上优化度量张量场，从而动态塑造模型空间的几何结构。为此，我们构建了一个变分框架，其损失函数仔细平衡了数据保真度与流形的内在几何复杂性。前者确保模型能有效解释观测数据，而后者则作为正则化项，对过度弯曲或不规则的几何结构进行惩罚，以鼓励更简单的模型并防止过拟合。为解决这个无限维优化问题的计算挑战，我们引入了一种基于离散微分几何的实用方法：将连续流形离散化为三角形网格，并通过边长参数化度量张量，从而能够使用自动微分工具进行高效优化。理论分析揭示了我们的框架与广义相对论中爱因斯坦-希尔伯特作用之间的深刻类比，为'数据驱动几何'概念提供了优雅的物理解释。我们进一步认为，即使拓扑结构固定，度量优化也比具有固定几何的模型具有更强的表达能力。这项工作为构建能够自主进化其几何和拓扑结构的完全动态'元学习器'奠定了坚实基础，并指向了科学模型发现和鲁棒表示学习等领域的广泛应用前景。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决传统机器学习方法的局限性：模型在固定的几何空间中寻找最优参数，而非优化模型空间本身的几何结构。当数据的本质几何结构是非欧几里得、弯曲或具有复杂拓扑时，传统方法可能导致效率低下、泛化能力弱或难以解释。这个问题的重要性在于，它限制了模型对数据内在结构的捕捉能力，阻碍了机器学习在处理复杂数据结构时的表现，同时也限制了我们对学习过程本质的理解。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者的思考始于对传统机器学习局限性的反思，提出不仅应优化参数点，还应优化塑造空间的几何结构本身。他借鉴了信息几何的启发，将参数化的概率分布视为微分流形，但超越了传统信息几何中静态度量的观念。作者设计了在固定拓扑流形上优化度量张量场的理论框架，构建了平衡数据保真度和几何复杂性的变分框架，并引入基于离散微分几何的实用计算方法。这项工作借鉴了信息几何、几何深度学习、生成模型和离散微分几何等领域的研究成果，但提出了全新的整合视角。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将机器学习模型视为可塑的几何实体，其学习过程表现为流形自身度量结构的自我优化，通过优化流形上的度量张量场来动态塑造模型空间的几何结构。实现流程包括：1)问题形式化，给定固定拓扑流形和观测数据集；2)设计变分框架，包含数据保真度项和几何复杂性项；3)将连续流形离散化为三角形网格，通过边长参数化度量张量；4)使用基于自动微分的优化算法进行迭代优化，通过投影梯度更新确保满足三角形不等式约束。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)从参数学习到结构学习的范式转变；2)将流形上的度量张量场本身作为优化目标；3)构建平衡数据保真度和几何复杂性的变分框架；4)基于离散微分几何的实用计算方法；5)揭示框架与广义相对论中爱因斯坦-希尔伯特作用的深刻联系。相比之前的工作，本文超越了传统信息几何中静态度量的观念，不同于几何深度学习中的固定几何假设，区别于传统生成模型中预设的简单潜在空间，也不同于传统非线性降维中固定的描述性流形，使几何结构成为学习过程的核心部分。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种机器学习新范式，通过优化流形上的度量张量场使模型能够动态塑造自身的几何结构，为构建完全自适应的'元学习器'奠定了理论基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a novel paradigm for machine learning that moves beyondtraditional parameter optimization. Unlike conventional approaches that searchfor optimal parameters within a fixed geometric space, our core idea is totreat the model itself as a malleable geometric entity. Specifically, weoptimize the metric tensor field on a manifold with a predefined topology,thereby dynamically shaping the geometric structure of the model space. Toachieve this, we construct a variational framework whose loss functioncarefully balances data fidelity against the intrinsic geometric complexity ofthe manifold. The former ensures the model effectively explains observed data,while the latter acts as a regularizer, penalizing overly curved or irregulargeometries to encourage simpler models and prevent overfitting. To address thecomputational challenges of this infinite-dimensional optimization problem, weintroduce a practical method based on discrete differential geometry: thecontinuous manifold is discretized into a triangular mesh, and the metrictensor is parameterized by edge lengths, enabling efficient optimization usingautomatic differentiation tools. Theoretical analysis reveals a profoundanalogy between our framework and the Einstein-Hilbert action in generalrelativity, providing an elegant physical interpretation for the concept of"data-driven geometry". We further argue that even with fixed topology, metricoptimization offers significantly greater expressive power than models withfixed geometry. This work lays a solid foundation for constructing fullydynamic "meta-learners" capable of autonomously evolving their geometry andtopology, and it points to broad application prospects in areas such asscientific model discovery and robust representation learning.</description>
      <author>example@mail.com (Di Zhang)</author>
      <guid isPermaLink="false">2510.26068v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis</title>
      <link>http://arxiv.org/abs/2510.26014v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025 workshop Learning from Time Series for  Health (TS4H)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于离散时间生存分析的双重专家混合框架，有效解决了患者异质性建模和风险预测适应性的挑战，在乳腺癌数据集上表现出色。&lt;h4&gt;背景&lt;/h4&gt;生存分析是建模直到感兴趣事件发生的时间的任务，广泛应用于临床和生物医学研究。主要挑战是如何建模患者异质性，同时使风险预测适应个体特征和时间动态性。&lt;h4&gt;目的&lt;/h4&gt;开发一个双重专家混合框架，用于离散时间生存分析，以有效处理患者异质性和时间动态性。&lt;h4&gt;方法&lt;/h4&gt;提出双重专家混合框架，结合特征编码器专家混合用于亚组感知的表示学习，以及风险专家混合利用患者特征和时间嵌入来捕获时间动态性。该设计可灵活集成到现有的基于深度学习的生存分析管道中。&lt;h4&gt;主要发现&lt;/h4&gt;在METABRIC和GBSG乳腺癌数据集上，该方法持续提高了性能，将时间依赖的C-index提升了最多0.04（在测试集上），当整合到Consurv框架中时获得了进一步的提升。&lt;h4&gt;结论&lt;/h4&gt;双重MoE框架能够有效处理患者异质性并适应时间动态性，在乳腺癌数据集上表现优异，且可以与现有框架结合使用。&lt;h4&gt;翻译&lt;/h4&gt;生存分析是一项建模直到感兴趣事件发生的时间的任务，广泛应用于临床和生物医学研究。一个关键挑战是建模患者异质性，同时使风险预测适应个体特征和时间动态性。我们提出了一个用于离散时间生存分析的双重专家混合框架。我们的方法结合了一个特征编码器专家混合，用于亚组感知的表示学习，以及一个风险专家混合，利用患者特征和时间嵌入来捕获时间动态性。这种双重MoE设计可以灵活地与现有的基于深度学习的生存分析管道集成。在METABRIC和GBSG乳腺癌数据集上，我们的方法持续提高了性能，将时间依赖的C-index提升了最多0.04（在测试集上），当整合到Consurv框架中时获得了进一步的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Survival analysis is a task to model the time until an event of interestoccurs, widely used in clinical and biomedical research. A key challenge is tomodel patient heterogeneity while also adapting risk predictions to bothindividual characteristics and temporal dynamics. We propose a dualmixture-of-experts (MoE) framework for discrete-time survival analysis. Ourapproach combines a feature-encoder MoE for subgroup-aware representationlearning with a hazard MoE that leverages patient features and time embeddingsto capture temporal dynamics. This dual-MoE design flexibly integrates withexisting deep learning based survival pipelines. On METABRIC and GBSG breastcancer datasets, our method consistently improves performance, boosting thetime-dependent C-index up to 0.04 on the test sets, and yields further gainswhen incorporated into the Consurv framework.</description>
      <author>example@mail.com (Hyeonjun Lee, Hyungseob Shin, Gunhee Nam, Hyeonsoo Lee)</author>
      <guid isPermaLink="false">2510.26014v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Predictive Coding Done Right for Mutual Information Estimation</title>
      <link>http://arxiv.org/abs/2510.25983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文指出InfoNCE作为互信息估计器的局限性，并提出改进方法InfoNCE-anchor，通过引入辅助锚点类实现更准确的互信息估计。研究还揭示了对比表示学习的真正价值在于学习结构化密度比，而非准确估计互信息。&lt;h4&gt;背景&lt;/h4&gt;InfoNCE目标最初用于对比表示学习，已成为互信息(MI)估计的流行选择，尽管它与MI的联系是间接的。&lt;h4&gt;目的&lt;/h4&gt;证明为什么InfoNCE不应被视为有效的MI估计器，并引入一个称为InfoNCE-anchor的简单修改，用于准确的MI估计。&lt;h4&gt;方法&lt;/h4&gt;通过引入一个辅助锚点类来修改InfoNCE，实现了一致的密度比估计，并产生了一个偏差显著减少的即插即用MI估计器。此外，使用适当的评分规则将框架推广，当使用对数评分时，InfoNCE-anchor作为特例被恢复。这个公式将一系列对比目标（包括NCE、InfoNCE和f-散度变体）统一在一个单一的原则性框架下。&lt;h4&gt;主要发现&lt;/h4&gt;使用对数评分的InfoNCE-anchor能实现最准确的MI估计；然而，在自监督表示学习实验中，锚点并未提高下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;对比表示学习受益的不是准确的MI估计本身，而是结构化密度比的学习。&lt;h4&gt;翻译&lt;/h4&gt;InfoNCE目标最初引入用于对比表示学习，尽管它与互信息(MI)的联系是间接的，但已成为MI估计的流行选择。在本文中，我们证明了为什么不应将InfoNCE视为有效的MI估计器，并介绍了一个简单的修改，我们称之为InfoNCE-anchor，用于准确的MI估计。我们的修改引入了一个辅助锚点类，实现了一致的密度比估计，并产生了一个偏差显著减少的即插即用MI估计器。除此之外，我们使用适当的评分规则推广了我们的框架，当使用对数评分时，InfoNCE-anchor作为特例被恢复。这个公式在单一原则性框架下统一了广泛的对比目标，包括NCE、InfoNCE和f-散度变体。从经验上看，我们发现使用对数评分的InfoNCE-anchor实现了最准确的MI估计；然而，在自监督表示学习实验中，我们发现锚点并未提高下游任务性能。这些发现证实了对比表示学习受益的不是准确的MI估计本身，而是结构化密度比的学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The InfoNCE objective, originally introduced for contrastive representationlearning, has become a popular choice for mutual information (MI) estimation,despite its indirect connection to MI. In this paper, we demonstrate whyInfoNCE should not be regarded as a valid MI estimator, and we introduce asimple modification, which we refer to as InfoNCE-anchor, for accurate MIestimation. Our modification introduces an auxiliary anchor class, enablingconsistent density ratio estimation and yielding a plug-in MI estimator withsignificantly reduced bias. Beyond this, we generalize our framework usingproper scoring rules, which recover InfoNCE-anchor as a special case when thelog score is employed. This formulation unifies a broad spectrum of contrastiveobjectives, including NCE, InfoNCE, and $f$-divergence variants, under a singleprincipled framework. Empirically, we find that InfoNCE-anchor with the logscore achieves the most accurate MI estimates; however, in self-supervisedrepresentation learning experiments, we find that the anchor does not improvethe downstream task performance. These findings corroborate that contrastiverepresentation learning benefits not from accurate MI estimation per se, butfrom the learning of structured density ratios.</description>
      <author>example@mail.com (J. Jon Ryu, Pavan Yeddanapudi, Xiangxiang Xu, Gregory W. Wornell)</author>
      <guid isPermaLink="false">2510.25983v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series</title>
      <link>http://arxiv.org/abs/2510.25785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为HiMAE的自监督学习方法，用于分析可穿戴传感器产生的生理时间序列数据，探索时间分辨率对预测效用的影响。&lt;h4&gt;背景&lt;/h4&gt;可穿戴传感器提供了丰富的生理时间序列数据，但支配这些数据预测效用的基本原理尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;测试时间分辨率作为表征学习基本轴的假设，即不同的临床和行为结果依赖于不同尺度的结构。&lt;h4&gt;方法&lt;/h4&gt;引入HiMAE（分层掩码自编码器），一种结合掩码自编码和分层卷积编码器-解码器的自监督框架。&lt;h4&gt;主要发现&lt;/h4&gt;HiMAE能产生多分辨率嵌入，系统评估哪些时间尺度携带预测信号；在分类、回归和生成基准测试中优于最先进模型；体积小几个数量级；可在智能手表上实现亚毫秒级推理，支持真正的边缘计算。&lt;h4&gt;结论&lt;/h4&gt;HiMAE既是高效的自监督学习方法，也是发现可穿戴健康数据中尺度敏感结构的有效工具。&lt;h4&gt;翻译&lt;/h4&gt;可穿戴传感器提供了丰富的生理时间序列，但支配其预测效用的原理仍然不清楚。我们假设时间分辨率是表征学习的基本轴，不同的临床和行为结果依赖于不同尺度的结构。为了测试这一分辨率假设，我们引入了HiMAE（分层掩码自编码器），这是一种结合掩码自编码和分层卷积编码器-解码器的自监督框架。HiMAE产生多分辨率嵌入，能够系统评估哪些时间尺度携带预测信号，将分辨率从超参数转变为可解释性的探针。在分类、回归和生成基准测试中，HiMAE始终优于将尺度压缩的最先进基础模型，同时体积小几个数量级。HiMAE是一种高效的表征学习器，足够紧凑，可以在手表上完全运行，在智能手表类CPU上实现亚毫秒级推理，实现真正的边缘推理。总之，这些贡献使HiMAE既成为高效的自监督学习方法，也成为发现可穿戴健康中尺度敏感结构的发现工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wearable sensors provide abundant physiological time series, yet theprinciples governing their predictive utility remain unclear. We hypothesizethat temporal resolution is a fundamental axis of representation learning, withdifferent clinical and behavioral outcomes relying on structure at distinctscales. To test this resolution hypothesis, we introduce HiMAE (HierarchicalMasked Autoencoder), a self supervised framework that combines maskedautoencoding with a hierarchical convolutional encoder decoder. HiMAE producesmulti resolution embeddings that enable systematic evaluation of which temporalscales carry predictive signal, transforming resolution from a hyperparameterinto a probe for interpretability. Across classification, regression, andgenerative benchmarks, HiMAE consistently outperforms state of the artfoundation models that collapse scale, while being orders of magnitude smaller.HiMAE is an efficient representation learner compact enough to run entirely onwatch, achieving sub millisecond inference on smartwatch class CPUs for trueedge inference. Together, these contributions position HiMAE as both anefficient self supervised learning method and a discovery tool for scalesensitive structure in wearable health.</description>
      <author>example@mail.com (Simon A. Lee, Cyrus Tanade, Hao Zhou, Juhyeon Lee, Megha Thukral, Minji Han, Rachel Choi, Md Sazzad Hissain Khan, Baiying Lu, Migyeong Gwak, Mehrab Bin Morshed, Viswam Nathan, Md Mahbubur Rahman, Li Zhu, Subramaniam Venkatraman, Sharanya Arcot Desai)</author>
      <guid isPermaLink="false">2510.25785v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education</title>
      <link>http://arxiv.org/abs/2510.26402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Autograder+是一个创新的自动评分系统，通过AI驱动的反馈、语义聚类和交互式可视化，将自动评分从纯总结性过程转变为形成性学习体验，减轻教师工作量的同时支持有针对性的教学并促进更强的学习成果。&lt;h4&gt;背景&lt;/h4&gt;编程教育的快速增长已经超过了传统评估工具的发展，使教师难以提供有意义、可扩展的反馈。传统的自动评分器虽然高效，但作为黑盒系统仅返回通过/失败结果，很少提供关于学生思维或学习需求的见解。&lt;h4&gt;目的&lt;/h4&gt;将自动评分从纯总结性过程转变为形成性学习体验，为教师提供更有效的评估工具，同时为学生提供更有价值的反馈。&lt;h4&gt;方法&lt;/h4&gt;引入两个关键功能：1) 使用微调的大语言模型自动生成反馈；2) 可视化学生代码提交以发现学习模式。模型经过精心筛选的学生代码和专家反馈进行微调，确保教育对齐和上下文感知的指导。系统支持提示池，允许教师通过选择的提示模板指导反馈风格。&lt;h4&gt;主要发现&lt;/h4&gt;在来自多个编程任务的600多个学生提交的评估中，系统生成的反馈与教师评论具有很强的语义一致性。基于1000个带注释的提交训练的对比学习代码嵌入能够基于功能和方法的相似性将解决方案分组为有意义的集群。&lt;h4&gt;结论&lt;/h4&gt;通过整合AI驱动的反馈、语义聚类和交互式可视化，Autograder+减轻了教师的工作量，同时支持有针对性的教学并促进更强的学习成果。&lt;h4&gt;翻译&lt;/h4&gt;编程教育的快速增长已经超过了传统评估工具的发展，使教师难以提供有意义、可扩展的反馈。传统的自动评分器虽然高效，但作为黑盒系统仅返回通过/失败结果，很少提供关于学生思维或学习需求的见解。Autograder+旨在将自动评分从纯总结性过程转变为形成性学习体验。它引入了两个关键功能：使用微调的大语言模型自动生成反馈，以及可视化学生代码提交以发现学习模式。该模型经过精心筛选的学生代码和专家反馈进行微调，确保教育对齐、上下文感知的指导。在来自多个编程任务的600多个学生提交的评估中，系统生成的反馈与教师评论具有很强的语义一致性。对于可视化功能，基于1000个带注释的提交训练的对比学习代码嵌入能够基于功能和方法的相似性将解决方案分组为有意义的集群。该系统还支持提示池，允许教师通过选择的提示模板指导反馈风格。通过整合AI驱动的反馈、语义聚类和交互式可视化，Autograder+减轻了教师的工作量，同时支持有针对性的教学并促进更强的学习成果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of programming education has outpaced traditional assessmenttools, leaving faculty with limited means to provide meaningful, scalablefeedback. Conventional autograders, while efficient, act as black-box systemsthat simply return pass/fail results, offering little insight into studentthinking or learning needs.  Autograder+ is designed to shift autograding from a purely summative processto a formative learning experience. It introduces two key capabilities:automated feedback generation using a fine-tuned Large Language Model, andvisualization of student code submissions to uncover learning patterns. Themodel is fine-tuned on curated student code and expert feedback to ensurepedagogically aligned, context-aware guidance.  In evaluation across 600 student submissions from multiple programming tasks,the system produced feedback with strong semantic alignment to instructorcomments. For visualization, contrastively learned code embeddings trained on1,000 annotated submissions enable grouping solutions into meaningful clustersbased on functionality and approach. The system also supports prompt-pooling,allowing instructors to guide feedback style through selected prompt templates.  By integrating AI-driven feedback, semantic clustering, and interactivevisualization, Autograder+ reduces instructor workload while supportingtargeted instruction and promoting stronger learning outcomes.</description>
      <author>example@mail.com (Vikrant Sahu, Gagan Raj Gupta, Raghav Borikar, Nitin Mane)</author>
      <guid isPermaLink="false">2510.26402v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.26241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究揭示了当前视觉语言模型在视频时间信息理解上的显著缺陷，提出了一个简单但有效的评估方法——时间方向判断(AoT)。&lt;h4&gt;背景&lt;/h4&gt;现代视觉语言模型在许多多模态任务中表现出色，但它们对视频中时间信息的理解仍然薄弱，并且这一点尚未得到充分评估。&lt;h4&gt;目的&lt;/h4&gt;探究视觉语言模型在理解视频时间信息方面的差距，通过判断视频播放方向(正向或反向)的挑战来评估其时间推理能力。&lt;h4&gt;方法&lt;/h4&gt;引入AoT-PsyPhyBENCH基准测试，使用经过心理物理学验证的刺激和行为基线，测试VLMs能否推断自然视频中的时间方向，并对开源和专有模型进行全面评估。&lt;h4&gt;主要发现&lt;/h4&gt;大多数模型表现接近随机水平，即使在物理不可逆过程和因果手动操作上表现最好的模型也远低于人类准确度，而人类几乎能立即识别这些内容。&lt;h4&gt;结论&lt;/h4&gt;当前多模态系统存在基本差距：虽然捕捉了丰富的视觉语义相关性，但缺乏时间连续性和因果理解所需的归纳偏置。&lt;h4&gt;翻译&lt;/h4&gt;现代视觉语言模型在许多多模态任务中表现出色，但它们对视频中时间信息的理解仍然薄弱，且这一点尚未得到充分评估。我们通过一个看似简单但具有揭示性的挑战来探究这一差距：判断时间方向(AoT)，即判断短视频片段是正向播放还是反向播放。我们引入了AoT-PsyPhyBENCH，这是一个经过心理物理学验证的基准测试，用于测试VLMs能否使用与人类相同的刺激和行为基线来推断自然视频中的时间方向。我们对开源和专有的、推理和非推理的VLMs进行了全面评估，发现大多数模型表现接近随机水平，即使在物理不可逆过程(如自由落体、扩散/爆炸)和因果手动操作(分割/添加)上表现最好的模型也远低于人类的准确度，而人类几乎能立即识别这些内容。这些结果突显了当前多模态系统的一个基本差距：虽然它们捕捉了丰富的视觉语义相关性，但缺乏时间连续性和因果理解所需的归纳偏置。我们发布了AoT-PsyPhyBENCH的代码和数据，以鼓励VLMs在物理和时间推理能力方面的进一步发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern vision-language models (VLMs) excel at many multimodal tasks, yettheir grasp of temporal information in video remains weak and, crucially,under-evaluated. We probe this gap with a deceptively simple but revealingchallenge: judging the arrow of time (AoT)-whether a short clip is playedforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validatedbenchmark that tests whether VLMs can infer temporal direction in naturalvideos using the same stimuli and behavioral baselines established for humans.Our comprehensive evaluation of open-weight and proprietary, reasoning andnon-reasoning VLMs reveals that most models perform near chance, and even thebest lag far behind human accuracy on physically irreversible processes (e.g.,free fall, diffusion/explosion) and causal manual actions (division/addition)that humans recognize almost instantly. These results highlight a fundamentalgap in current multimodal systems: while they capture rich visual-semanticcorrelations, they lack the inductive biases required for temporal continuityand causal understanding. We release the code and data for AoT-PsyPhyBENCH toencourage further progress in the physical and temporal reasoning capabilitiesof VLMs.</description>
      <author>example@mail.com (Shiho Matta, Lis Kanashiro Pereira, Peitao Han, Fei Cheng, Shigeru Kitazawa)</author>
      <guid isPermaLink="false">2510.26241v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing Environments</title>
      <link>http://arxiv.org/abs/2510.26148v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为STAR的边缘AI优化框架，用于在低功耗嵌入式设备上实现实时、节能的人类活动识别(HAR)系统。该系统通过简化的神经网络、自适应信号处理和硬件感知优化，在保持高准确率的同时显著提高了计算效率。&lt;h4&gt;背景&lt;/h4&gt;人类活动识别(HAR)通过Wi-Fi信道状态信息(CSI)提供了一种保护隐私、无需接触的感知方法，适用于智能家居、健康监测和移动物联网系统。然而，现有方法存在计算效率低下、高延迟和资源受限环境中可行性有限的问题。&lt;h4&gt;目的&lt;/h4&gt;开发STAR(Sensing Technology for Activity Recognition)框架，在低功耗嵌入式设备上实现实时、节能的HAR，解决现有方法在资源受限环境中的局限性。&lt;h4&gt;方法&lt;/h4&gt;集成轻量级神经网络架构、自适应信号处理和硬件感知联合优化；使用简化的基于门控循环单元(GRU)的循环神经网络，比传统LSTM减少33%参数；采用多阶段预处理管道(中值滤波、8阶巴特沃斯低通滤波和经验模态分解)；在配备NPU的Rockchip RV1126处理器上实现，并与ESP32-S3 CSI采集模块接口。&lt;h4&gt;主要发现&lt;/h4&gt;在七类活动上的平均识别准确率达93.52%，人体存在检测准确率达99.11%；使用仅97.6k参数的紧凑模型；INT8量化推理以33 MHz速度运行，仅占用8% CPU利用率，比CPU执行快六倍；系统具有亚秒级响应延迟和低功耗。&lt;h4&gt;结论&lt;/h4&gt;STAR系统确保了实时、隐私保护的HAR，为移动和普适计算环境提供了实用、可扩展的解决方案，有效解决了传统方法在资源受限嵌入式环境中的局限性。&lt;h4&gt;翻译&lt;/h4&gt;人类活动识别(HAR)通过Wi-Fi信道状态信息(CSI)提供了一种保护隐私、无需接触的感知方法，适用于智能家居、健康监测和移动物联网系统。然而，现有方法常遇到计算效率低下、高延迟以及在资源受限的嵌入式移动边缘环境中可行性有限的问题。本文提出了STAR(Sensing Technology for Activity Recognition)，这是一个边缘AI优化的框架，集成了轻量级神经网络架构、自适应信号处理和硬件感知的联合优化，以在低功耗嵌入式设备上实现实时、节能的HAR。STAR采用简化的基于门控循环单元(GRU)的循环神经网络，比传统LSTM模型减少33%的模型参数，同时保持有效的时间建模能力。采用结合中值滤波、8阶巴特沃斯低通滤波和经验模态分解(EMD)的多阶段预处理管道，用于去噪CSI振幅数据并提取时空特征。对于设备上部署，STAR在配备嵌入式神经处理单元(NPU)的Rockchip RV1126处理器上实现，与基于ESP32-S3的CSI采集模块接口。实验结果显示，在七类活动上的平均识别准确率为93.52%，人体存在检测为99.11%，使用紧凑的97.6k参数模型。INT8量化推理以33 MHz的处理速度运行，仅占用8%的CPU利用率，比基于CPU的执行速度快六倍。凭借亚秒级响应延迟和低功耗，该系统确保了实时、隐私保护的HAR，为移动和普适计算环境提供了实用、可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)presents a privacy-preserving, contactless sensing approach suitable for smarthomes, healthcare monitoring, and mobile IoT systems. However, existing methodsoften encounter computational inefficiency, high latency, and limitedfeasibility within resource-constrained, embedded mobile edge environments.This paper proposes STAR (Sensing Technology for Activity Recognition), anedge-AI-optimized framework that integrates a lightweight neural architecture,adaptive signal processing, and hardware-aware co-optimization to enablereal-time, energy-efficient HAR on low-power embedded devices. STARincorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neuralnetwork, reducing model parameters by 33% compared to conventional LSTM modelswhile maintaining effective temporal modeling capability. A multi-stagepre-processing pipeline combining median filtering, 8th-order Butterworthlow-pass filtering, and Empirical Mode Decomposition (EMD) is employed todenoise CSI amplitude data and extract spatial-temporal features. For on-devicedeployment, STAR is implemented on a Rockchip RV1126 processor equipped with anembedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSIacquisition module. Experimental results demonstrate a mean recognitionaccuracy of 93.52% across seven activity classes and 99.11% for human presencedetection, utilizing a compact 97.6k-parameter model. INT8 quantized inferenceachieves a processing speed of 33 MHz with just 8% CPU utilization, deliveringsixfold speed improvements over CPU-based execution. With sub-second responselatency and low power consumption, the system ensures real-time,privacy-preserving HAR, offering a practical, scalable solution for mobile andpervasive computing environments.</description>
      <author>example@mail.com (Kexing Liu)</author>
      <guid isPermaLink="false">2510.26148v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>EgoExo-Con: Exploring View-Invariant Video Temporal Understanding</title>
      <link>http://arxiv.org/abs/2510.26113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  project page:  \url{https://minjoong507.github.io/projects/EgoExo-Con/}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视频大语言模型在不同视角下捕捉同一事件时的时间理解一致性问题，提出了EgoExo-Con基准测试和View-GRPO强化学习框架来解决现有模型的局限性。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型在不同视角下捕捉同一事件时可能存在时间理解不一致的问题。&lt;h4&gt;目的&lt;/h4&gt;研究视频大语言模型在不同视角下捕捉同一事件时是否能保持一致的时间理解能力，并提出改进方法。&lt;h4&gt;方法&lt;/h4&gt;引入EgoExo-Con基准测试，包含全面同步的第一人称和第三人称视角视频对及人类优化的自然语言查询，强调时间验证和时间定位两个任务，并提出View-GRPO强化学习框架来增强特定视角的时间推理并促进跨视角一致性理解。&lt;h4&gt;主要发现&lt;/h4&gt;现有Video-LLMs存在两个关键限制：(1)模型通常无法保持一致性，结果远差于单视角表现；(2)当使用双视角同步视频微调时，模型显示出一致性改进，但表现往往不如单视角训练的模型。&lt;h4&gt;结论&lt;/h4&gt;View-GRPO方法在提高跨视角一致性方面优于简单的SFT和GRPO，所有研究资源将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;当视频从不同视角捕捉同一事件时，视频大语言模型能否实现一致的时间理解？为研究此问题，我们引入了EgoExo-Con(一致性)基准，该基准包含全面同步的第一人称和第三人称视频对以及人类优化的自然语言查询。EgoExo-Con强调两个时间理解任务：时间验证和时间定位。它不仅评估正确性，还评估跨视角的一致性。我们的分析揭示了现有Video-LLMs的两个关键局限：(1)模型通常无法保持一致性，结果远差于其单视角表现。(2)当使用双视角同步视频进行微调时，模型显示出一致性改进，但往往表现不如单视角训练的模型。为改进，我们提出了View-GRPO，一种新的强化学习框架，能有效增强特定视角的时间推理，同时鼓励跨视角的一致性理解。我们的方法在提高跨视角一致性方面优于简单的SFT和GRPO。所有资源将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Can Video-LLMs achieve consistent temporal understanding when videos capturethe same event from different viewpoints? To study this, we introduceEgoExo-Con (Consistency), a benchmark of comprehensively synchronizedegocentric and exocentric video pairs with human-refined queries in naturallanguage. EgoExo-Con emphasizes two temporal understanding tasks: TemporalVerification and Temporal Grounding. It evaluates not only correctness butconsistency across viewpoints. Our analysis reveals two critical limitations ofexisting Video-LLMs: (1) models often fail to maintain consistency, withresults far worse than their single-view performances. (2) When naivelyfinetuned with synchronized videos of both viewpoints, the models show improvedconsistency but often underperform those trained on a single view. Forimprovements, we propose View-GRPO, a novel reinforcement learning frameworkthat effectively strengthens view-specific temporal reasoning while encouragingconsistent comprehension across viewpoints. Our method demonstrates itssuperiority over naive SFT and GRPO, especially for improving cross-viewconsistency. All resources will be made publicly available.</description>
      <author>example@mail.com (Minjoon Jung, Junbin Xiao, Junghyun Kim, Byoung-Tak Zhang, Angela Yao)</author>
      <guid isPermaLink="false">2510.26113v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders</title>
      <link>http://arxiv.org/abs/2510.26027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的视频大语言模型架构，通过在视觉编码器中引入堆叠时序注意力模块，解决了当前模型在理解视频时序动态方面的局限性，显著提升了时序推理能力和动作识别性能。&lt;h4&gt;背景&lt;/h4&gt;尽管多模态大语言模型(MLLMs)已取得显著进展，但理解视频中复杂的时序动态仍然是一个主要挑战。当前视频大语言模型(Video-LLM)架构在时序理解方面存在关键局限性，难以处理需要详细理解动作序列和时间进展的任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的Video-LLM架构，增强模型对视频时序动态的理解能力，特别是在动作序列和时间进展方面的理解。&lt;h4&gt;方法&lt;/h4&gt;在视觉编码器中直接引入堆叠的时序注意力模块，在视觉编码器中融入时序注意力，使模型能够更好地捕捉动作进展和帧之间的关系，在将视觉令牌传递给LLM之前增强时序理解。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著提高了时序推理能力，在视频问答任务中优于现有模型，特别是在动作识别方面。在VITATECS、MVBench和Video-MME等基准测试中，性能提升了高达5.5%。&lt;h4&gt;结论&lt;/h4&gt;通过增强视觉编码器的时序结构，解决了Video-LLMs在视频理解方面的关键差距，为视频时序理解提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;尽管多模态大语言模型(MLLMs)取得了显著进展，但理解视频中复杂的时序动态仍然是一个主要挑战。我们的实验表明，当前视频大语言模型(Video-LLM)架构在时序理解方面存在关键局限性，难以处理需要详细理解动作序列和时间进展的任务。在这项工作中，我们提出了一种Video-LLM架构，在视觉编码器中直接引入堆叠的时序注意力模块。这种设计在视觉编码器中融入了时序注意力，使模型能够在将视觉令牌传递给LLM之前更好地捕捉动作进展和帧之间的关系。我们的结果表明，这种方法显著提高了时序推理能力，并在视频问答任务中优于现有模型，特别是在动作识别方面。我们在VITATECS、MVBench和Video-MME等基准测试中提高了高达5.5%。通过增强视觉编码器的时序结构，我们解决了Video-LLMs在视频理解方面的关键差距。项目页面和代码可在以下网址获取：https://alirasekh.github.io/STAVEQ2/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant advances in Multimodal Large Language Models (MLLMs),understanding complex temporal dynamics in videos remains a major challenge.Our experiments show that current Video Large Language Model (Video-LLM)architectures have critical limitations in temporal understanding, strugglingwith tasks that require detailed comprehension of action sequences and temporalprogression. In this work, we propose a Video-LLM architecture that introducesstacked temporal attention modules directly within the vision encoder. Thisdesign incorporates a temporal attention in vision encoder, enabling the modelto better capture the progression of actions and the relationships betweenframes before passing visual tokens to the LLM. Our results show that thisapproach significantly improves temporal reasoning and outperforms existingmodels in video question answering tasks, specifically in action recognition.We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to+5.5%. By enhancing the vision encoder with temporal structure, we address acritical gap in video understanding for Video-LLMs. Project page and code areavailable at: https://alirasekh.github.io/STAVEQ2/.</description>
      <author>example@mail.com (Ali Rasekh, Erfan Bagheri Soula, Omid Daliran, Simon Gottschalk, Mohsen Fayyaz)</author>
      <guid isPermaLink="false">2510.26027v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL</title>
      <link>http://arxiv.org/abs/2510.25997v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, GeoGenAgent'25 - ACM SIGSPATIAL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于代理的管道系统，用于处理复杂的空间和时间自然语言查询，显著提高了查询准确性和用户友好性。&lt;h4&gt;背景&lt;/h4&gt;现有的自然语言到SQL系统在处理真实空间和时间查询时存在困难，需要将模糊的用户表述与特定模式类别匹配、处理时间推理并选择适当输出。&lt;h4&gt;目的&lt;/h4&gt;开发能够处理复杂空间和时间查询的系统，支持缺乏SQL专业知识、详细模式知识或提示技能的用户。&lt;h4&gt;方法&lt;/h4&gt;构建了一个基于代理的管道，通过基于Mistral的ReAct代理对基础文本到SQL模型(llama-3-sqlcoder-8b)进行编排，使代理能够通过模式检查、SQL生成、执行和可视化工具来规划、分解和调整查询。&lt;h4&gt;主要发现&lt;/h4&gt;在纽约和东京签到数据集的35个自然语言查询评估中，代理系统准确率达到91.4%，而基础模型仅为28.6%，并通过地图、图表和结构化的自然语言摘要显著增强了可用性。&lt;h4&gt;结论&lt;/h4&gt;代理编排而非更强的SQL生成器本身是构建交互式地理空间助手的有前途的基础。&lt;h4&gt;翻译&lt;/h4&gt;自然语言到SQL系统有望使结构化数据访问民主化，使用户无需学习SQL即可查询数据库。然而，现有系统在处理现实空间时间查询方面存在困难，成功需要将模糊的用户表述与特定模式类别对齐、处理时间推理并选择适当输出。我们提出了一种基于代理的管道，通过基于Mistral的ReAct代理的编排，扩展了一个基础文本到SQL模型(llama-3-sqlcoder-8b)。该代理可以通过模式检查、SQL生成、执行和可视化工具来规划、分解和调整查询。我们在纽约和东京签到数据集上的35个自然语言查询进行了评估，涵盖了空间、时间和多数据集推理。代理的准确率显著高于基础模型，达到91.4%对28.6%，并通过地图、图表和结构化的自然语言摘要增强了可用性。关键的是，我们的设计支持了更自然的人机数据库交互，支持缺乏SQL专业知识、详细模式知识或提示技能的用户。我们得出结论，代理编排而非更强的SQL生成器本身，是交互式地理空间助手的有前途的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3764915.3770724&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizingaccess to structured data, allowing users to query databases without learningSQL. Yet existing systems struggle with realistic spatio-temporal queries,where success requires aligning vague user phrasing with schema-specificcategories, handling temporal reasoning, and choosing appropriate outputs. Wepresent an agentic pipeline that extends a naive text-to-SQL baseline(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. Theagent can plan, decompose, and adapt queries through schema inspection, SQLgeneration, execution, and visualization tools. We evaluate on 35natural-language queries over the NYC and Tokyo check-in dataset, coveringspatial, temporal, and multi-dataset reasoning. The agent achievessubstantially higher accuracy than the naive baseline 91.4% vs. 28.6% andenhances usability through maps, plots, and structured natural-languagesummaries. Crucially, our design enables more natural human-databaseinteraction, supporting users who lack SQL expertise, detailed schemaknowledge, or prompting skill. We conclude that agentic orchestration, ratherthan stronger SQL generators alone, is a promising foundation for interactivegeospatial assistants.</description>
      <author>example@mail.com (Manu Redd, Tao Zhe, Dongjie Wang)</author>
      <guid isPermaLink="false">2510.25997v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks</title>
      <link>http://arxiv.org/abs/2510.25797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了时空建模和空间注意力机制在水下物体检测中的有效性，比较了标准YOLOv5、T-YOLOv5及其与CBAM结合的变体性能。&lt;h4&gt;背景&lt;/h4&gt;水下物体检测在动态海洋环境中面临挑战，如突然运动、部分遮挡和逐渐运动等。&lt;h4&gt;目的&lt;/h4&gt;研究时空建模和空间注意力机制如何提高水下物体检测的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;分两个阶段进行，第一阶段评估T-YOLOv5与标准YOLOv5的性能比较；第二阶段开发添加了卷积块注意力模块(CBAM)的T-YOLOv5增强版本。&lt;h4&gt;主要发现&lt;/h4&gt;T-YOLOv5和T-YOLOv5与CBAM的变体在mAP@50-95指标上分别达到0.813和0.811，显著优于标准YOLOv5的0.563。&lt;h4&gt;结论&lt;/h4&gt;T-YOLOv5相比标准模型显著提高了检测可靠性，而T-YOLOv5与CBAM在具有挑战性的场景中进一步提高了性能，但在简单场景中会损失一些准确性。&lt;h4&gt;翻译&lt;/h4&gt;该研究检验了时空建模和空间注意力机制在深度学习模型中用于水下物体检测的有效性。具体而言，在第一阶段，评估了增强时序的YOLOv5变体T-YOLOv5与标准YOLOv5的性能比较。在第二阶段，通过添加卷积块注意力模块(CBAM)开发了T-YOLOv5的增强版本。研究表明，CBAM如何通过时序建模提高了在动态海洋环境中的检测准确性，特别是在突然运动、部分遮挡和逐渐运动的条件下。测试结果显示，YOLOv5达到了0.563的mAP@50-95，而T-YOLOv5和带有CBAM的T-YOLOv5分别以0.813和0.811的mAP@50-95分数表现更优，突显了它们在检测复杂物体方面的卓越准确性和泛化能力。研究结果表明，与标准模型相比，T-YOLOv5显著提高了检测可靠性，而带有CBAM的T-YOLOv5在具有挑战性的场景中进一步提高了性能，尽管在简单场景中会损失一些准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study examines the effectiveness of spatio-temporal modeling and theintegration of spatial attention mechanisms in deep learning models forunderwater object detection. Specifically, in the first phase, the performanceof temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison withthe standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 isdeveloped, through the addition of a Convolutional Block Attention Module(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 andT-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, theresearch highlights how temporal modeling improves detection accuracy indynamic marine environments, particularly under conditions of sudden movements,partial occlusions, and gradual motion. The testing results showed that YOLOv5achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAMoutperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,highlighting their superior accuracy and generalization in detecting complexobjects. The findings demonstrate that T-YOLOv5 significantly enhancesdetection reliability compared to the standard model, while T-YOLOv5 with CBAMfurther improves performance in challenging scenarios, although there is a lossof accuracy when it comes to simpler scenarios.</description>
      <author>example@mail.com (Sai Likhith Karri, Ansh Saxena)</author>
      <guid isPermaLink="false">2510.25797v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios</title>
      <link>http://arxiv.org/abs/2510.26580v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint under review at IEEE Transactions on Pattern Analysis and  Machine Intelligence (TPAMI), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种动态上下文感知场景推理框架，利用视觉语言对齐解决零样本现实世界场景问题，通过结合视觉Transformer和大语言模型显著提高了复杂环境中的场景理解准确性。&lt;h4&gt;背景&lt;/h4&gt;在现实世界环境中，AI系统经常面临没有标记数据的陌生场景，这给传统场景理解模型带来重大挑战。无法在未见过的上下文中进行泛化限制了基于视觉的应用程序在动态、非结构化环境中的部署。&lt;h4&gt;目的&lt;/h4&gt;使智能系统能够在没有特定任务先验训练的情况下推断并适应新环境。&lt;h4&gt;方法&lt;/h4&gt;提出的方法集成了预训练的视觉Transformer和大语言模型，将视觉语义与自然语言描述对齐增强上下文理解能力。动态推理模块通过结合全局场景线索和由语言先验引导的对象级交互来优化预测。&lt;h4&gt;主要发现&lt;/h4&gt;在COCO、Visual Genome和Open Images等零样本基准上的实验表明，在复杂且未见过的环境中，场景理解准确性比基线模型提高了高达18%。由于视觉和语言的协同融合，系统在模糊或杂乱的场景中表现出强大的性能。&lt;h4&gt;结论&lt;/h4&gt;该框架为上下文感知推理提供了一种可扩展和可解释的方法，推动了动态现实世界环境中的零样本泛化。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界环境中，AI系统经常面临没有标记数据的陌生场景，这给传统的场景理解模型带来了重大挑战。无法在未见过的上下文中进行泛化限制了基于视觉的应用程序在动态、非结构化环境中的部署。这项工作引入了一种动态上下文感知场景推理框架，利用视觉语言对齐来解决零样本现实世界场景问题。目标是使智能系统能够在没有特定任务先验训练的情况下推断并适应新环境。提出的方法集成了预训练的视觉Transformer和大语言模型，将视觉语义与自然语言描述对齐，增强上下文理解能力。动态推理模块通过结合全局场景线索和由语言先验引导的对象级交互来优化预测。在COCO、Visual Genome和Open Images等零样本基准上的广泛实验表明，在复杂且未见过的环境中，场景理解准确性比基线模型提高了高达18%。结果还显示，由于视觉和语言的协同融合，在模糊或杂乱的场景中表现出强大的性能。该框架为上下文感知推理提供了一种可扩展和可解释的方法，推动了动态现实世界环境中的零样本泛化。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决AI系统在零样本(real-world)场景下进行动态上下文感知场景推理的问题。传统场景理解模型在面对没有标记数据的新环境时无法有效泛化，这限制了视觉应用在自动驾驶、机器人导航、监控等动态、非结构化环境中的部署。这个问题在现实中非常重要，因为真实世界环境往往是复杂、多变且缺乏标记数据的，AI系统需要能够理解和适应从未见过的新场景而不需要针对每个特定任务重新训练。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过整合预训练的视觉转换器(visual transformers)和大语言模型(large language models)来设计这个方法。他们借鉴了多项现有工作，包括Context VLM用于自动驾驶安全、结合开放世界检测器和大视觉语言模型的零样本目标识别、基于图的视觉语言模型调整方法、利用CLIP模型的动态场景恢复框架等。作者在这些工作的基础上，提出了一个动态推理模块，该模块利用全局场景线索和对象级交互，由语言先验指导，从而实现更有效的场景理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过视觉-语言对齐来实现动态上下文感知的场景推理，使AI系统能够在没有任务特定监督的情况下理解和适应新环境。整体实现流程包括：1)使用视觉编码器(如CLIP-ViT)提取丰富的视觉特征；2)使用语言编码器(如GPT或BERT变体)建模语义先验；3)采用基于注意力的跨模态融合机制对齐视觉和语言；4)引入上下文精炼单元，建模对象级交互和全局场景语义；5)在零样本场景下评估模型性能，通过计算视觉和文本嵌入的余弦相似度来进行场景理解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出动态上下文感知场景推理框架，整合预训练视觉转换器和大语言模型；2)引入动态推理模块，利用全局场景线索和对象级交互；3)在多个零样本基准数据集上证明模型泛化和适应性；4)在模糊或杂乱场景中表现出强大性能；5)提供可扩展和可解释的上下文感知推理方法。相比之前的工作，这个方法的主要不同在于：结合了动态推理能力和视觉-语言对齐；不依赖任务特定监督；能处理复杂、模糊场景；具有更好的泛化能力和可解释性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的动态上下文感知场景推理框架，通过视觉-语言对齐实现了在没有任务特定监督的情况下理解和适应新环境的能力，显著提升了AI系统在复杂、模糊和杂乱场景中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world environments, AI systems often face unfamiliar scenarioswithout labeled data, creating a major challenge for conventional sceneunderstanding models. The inability to generalize across unseen contexts limitsthe deployment of vision-based applications in dynamic, unstructured settings.This work introduces a Dynamic Context-Aware Scene Reasoning framework thatleverages Vision-Language Alignment to address zero-shot real-world scenarios.The goal is to enable intelligent systems to infer and adapt to newenvironments without prior task-specific training. The proposed approachintegrates pre-trained vision transformers and large language models to alignvisual semantics with natural language descriptions, enhancing contextualcomprehension. A dynamic reasoning module refines predictions by combiningglobal scene cues and object-level interactions guided by linguistic priors.Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, andOpen Images demonstrate up to 18% improvement in scene understanding accuracyover baseline models in complex and unseen environments. Results also showrobust performance in ambiguous or cluttered scenes due to the synergisticfusion of vision and language. This framework offers a scalable andinterpretable approach for context-aware reasoning, advancing zero-shotgeneralization in dynamic real-world settings.</description>
      <author>example@mail.com (Manjunath Prasad Holenarasipura Rajiv, B. M. Vidyavathi)</author>
      <guid isPermaLink="false">2510.26580v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM</title>
      <link>http://arxiv.org/abs/2510.26358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AgriGS-SLAM是一种结合视觉和激光雷达的SLAM框架，利用多相机3D高斯溅射技术实现果园环境的实时3D场景理解，克服了重复几何结构、季节变化和风吹 foliage 运动的挑战。&lt;h4&gt;背景&lt;/h4&gt;果园中的自主机器人需要实时3D场景理解，尽管存在重复的行列几何结构、季节性外观变化和风吹 foliage 运动。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理果园环境特殊挑战的实时3D场景理解系统。&lt;h4&gt;方法&lt;/h4&gt;结合直接激光雷达里程计和闭环检测与多相机3D高斯溅射渲染；通过互补视角的批量栅格化恢复遮挡下的果园结构；在关键帧之间执行统一的梯度驱动地图生命周期；基于概率激光雷达深度一致性项进行姿态优化并加强几何-外观耦合。&lt;h4&gt;主要发现&lt;/h4&gt;在苹果和梨果园的休眠期、开花期和收获期测试；跨季节和站点提供比先进3DGS-SLAM基线更清晰、更稳定的重建和轨迹；在拖拉机上保持实时性能。&lt;h4&gt;结论&lt;/h4&gt;虽然演示于果园监测，但该方法可应用于需要鲁棒多模态感知的其他户外领域。&lt;h4&gt;翻译&lt;/h4&gt;果园中的自主机器人需要实时3D场景理解，尽管存在重复的行列几何结构、季节性外观变化和风吹 foliage 运动。我们提出了AgriGS-SLAM，这是一种结合直接激光雷达里程计和闭环检测与多相机3D高斯溅射渲染的视觉-激光雷达SLAM框架。通过互补视角的批量栅格化恢复遮挡下的果园结构，同时在关键帧之间执行统一的梯度驱动地图生命周期以保留精细细节并限制内存使用。姿态优化由基于概率激光雷达的深度一致性项引导，通过相机投影反向传播以加强几何-外观耦合。我们在苹果和梨果园的休眠期、开花期和收获期使用标准化轨迹协议部署了该系统，评估训练视图和新颖视图合成以减少3DGS评估中的过拟合。跨季节和站点，AgriGS-SLAM比最近的先进3DGS-SLAM基线提供更清晰、更稳定的重建和更稳定的轨迹，同时在拖拉机上保持实时性能。虽然演示于果园监测，但该方法可应用于需要鲁棒多模态感知的其他户外领域。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决果园环境中自主机器人的实时3D场景理解问题，特别是在面对季节性外观变化、重复的行几何结构和风吹引起的叶片运动等挑战时的SLAM系统适应性。这个问题很重要，因为全球人口增长和劳动力短缺增加了对自主农业技术的需求，果园机器人需要准确的3D重建能力来执行导航、收获、喷洒和修剪等任务，同时农民需要即时反馈来调整田间操作，农业数字孪子也需要物理世界与数字表示之间的持续同步。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有SLAM系统在农业环境中的局限性：视觉-only方法在重复作物模式和植被运动下会失败，而激光雷达-only系统在几何稀疏性方面受限。他们借鉴了神经渲染的最新进展，特别是3D高斯泼溅(3DGS)的显式点表示和高效光栅化特性，适合大型场景SLAM的增量特性。方法设计上结合了直接激光雷达里程计(DLO)作为前端，因子图作为后端，并扩展了3DGS到多视图设置以处理果园遮挡，同时参考了现有的多视图优化和闭环检测技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合视觉和激光雷达两种传感器的优势，使用多摄像头系统提供互补视角解决遮挡问题，利用3D高斯泼溅进行实时场景表示，设计梯度驱动的地图生命周期管理，以及使用多模态损失函数联合优化场景表示和机器人定位。整体流程包括：SLAM前端使用直接激光雷达里程计估计运动；SLAM后端维护关键帧姿势的因子图；多视图3D高斯泼溅部分进行增量映射、内存管理和泼溅生命周期操作；优化循环调度各种操作；最后通过多模态损失函数结合光度一致性和几何一致性进行优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 实时视觉-激光雷达3DGS-SLAM系统；2) 跨季节适用性基准评估方法；3) 统一的梯度驱动的3DGS-SLAM地图和定位优化；4) 支持多摄像头设置的户外3DGS-SLAM框架。相比之前工作，该方法专门针对果园环境设计，使用多摄像头系统而非单摄像头，结合激光雷达里程计和闭环检测而非仅依赖视觉或激光雷达，设计了特定的地图生命周期管理策略，使用多模态损失函数结合光度和几何一致性，并在真实果园的不同生长阶段进行了广泛测试。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AgriGS-SLAM通过结合多视图3D高斯泼溅与激光雷达里程计和闭环检测，实现了果园环境中的实时、跨季节精确地图构建和机器人定位，解决了季节性变化、重复结构和遮挡带来的挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous robots in orchards require real-time 3D scene understandingdespite repetitive row geometry, seasonal appearance changes, and wind-drivenfoliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework thatcouples direct LiDAR odometry and loop closures with multi-camera 3D GaussianSplatting (3DGS) rendering. Batch rasterization across complementary viewpointsrecovers orchard structure under occlusions, while a unified gradient-drivenmap lifecycle executed between keyframes preserves fine details and boundsmemory. Pose refinement is guided by a probabilistic LiDAR-based depthconsistency term, back-propagated through the camera projection to tightengeometry-appearance coupling. We deploy the system on a field platform in appleand pear orchards across dormancy, flowering, and harvesting, using astandardized trajectory protocol that evaluates both training-view andnovel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasonsand sites, AgriGS-SLAM delivers sharper, more stable reconstructions andsteadier trajectories than recent state-of-the-art 3DGS-SLAM baselines whilemaintaining real-time performance on-tractor. While demonstrated in orchardmonitoring, the approach can be applied to other outdoor domains requiringrobust multimodal perception.</description>
      <author>example@mail.com (Mirko Usuelli, David Rapado-Rincon, Gert Kootstra, Matteo Matteucci)</author>
      <guid isPermaLink="false">2510.26358v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>A Three-Stage Bayesian Transfer Learning Framework to Improve Predictions in Data-Scarce Domains</title>
      <link>http://arxiv.org/abs/2510.26541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Engineering Applications of Artificial Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种全监督的三阶段框架（staged B-DANN），结合参数迁移和共享潜在空间适应，用于解决数据稀缺领域的机器学习问题。该方法通过确定性特征提取、对抗性优化和贝叶斯微调三个阶段，提高了预测准确性和泛化能力，同时提供不确定性估计。&lt;h4&gt;背景&lt;/h4&gt;机器学习在工程领域的应用持续增长，深度神经网络因其性能和可访问性被广泛采用，但需要大量高质量数据集。实验数据通常稀疏、嘈杂或不足以构建稳健的数据驱动模型。迁移学习利用数据丰富的源领域来辅助数据稀缺目标领域的学习，但传统参数迁移在领域差异较大时性能下降，领域对抗神经网络(DANNs)在半监督设置下可以处理更大的领域偏移，但训练不稳定且缺乏不确定性量化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理领域差异、提供不确定性估计并提高数据稀缺领域预测准确性和泛化能力的机器学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种全监督的三阶段框架：阶段性贝叶斯领域对抗神经网络(staged B-DANN)。第一阶段在源领域训练确定性特征提取器；第二阶段使用DANN对抗性地优化特征提取器；第三阶段在适应的特征提取器上构建贝叶斯神经网络，用于在目标领域微调，处理条件偏移并提供校准的不确定性估计。&lt;h4&gt;主要发现&lt;/h4&gt;在合成基准测试中，该方法显著优于标准迁移技术；应用于预测矩形通道中的临界热通量时，利用管状实验数据作为源领域，结果显示staged B-DANN方法可以提高预测准确性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;staged B-DANN方法可以改进预测准确性和泛化能力，并提供不确定性估计，可能有助于核工程等其他领域的应用。&lt;h4&gt;翻译&lt;/h4&gt;机器学习在工程领域的应用持续增长，以支持广泛的应用。在这些方法中，深度神经网络因其性能和可访问性被广泛采用，但它们需要大量高质量的数据集。实验数据通常稀疏、嘈杂或不足以构建稳健的数据驱动模型。迁移学习利用数据丰富的相关源领域来辅助数据稀缺目标领域的学习，已显示出有效性。参数迁移（重用预训练权重）很常见，但在大的领域偏移下性能会下降。领域对抗神经网络(DANNs)通过学习领域不变的表示来解决这个问题，从而在半监督设置下提高较大领域偏移的迁移效果。然而，DANNs在训练过程中可能不稳定，且缺乏原生的不确定性量化手段。本研究引入了一种全监督的三阶段框架——阶段性贝叶斯领域对抗神经网络(staged B-DANN)，它结合了参数迁移和共享潜在空间适应。在第一阶段，在源领域训练确定性特征提取器。然后在第二阶段使用DANN对抗性地优化该特征提取器。在第三阶段，在适应的特征提取器上构建贝叶斯神经网络，用于在目标领域进行微调，以处理条件偏移并产生校准的不确定性估计。首先在合成基准测试中验证了这种阶段性B-DANN方法，结果表明它显著优于标准迁移技术。然后将其应用于预测矩形通道中的临界热通量任务，利用管状实验数据作为源领域。本研究结果表明，阶段性B-DANN方法可以提高预测准确性和泛化能力，可能有助于核工程的其他领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of ML in engineering has grown steadily to support a wide array ofapplications. Among these methods, deep neural networks have been widelyadopted due to their performance and accessibility, but they require large,high-quality datasets. Experimental data are often sparse, noisy, orinsufficient to build resilient data-driven models. Transfer learning, whichleverages relevant data-abundant source domains to assist learning indata-scarce target domains, has shown efficacy. Parameter transfer, wherepretrained weights are reused, is common but degrades under large domainshifts. Domain-adversarial neural networks (DANNs) help address this issue bylearning domain-invariant representations, thereby improving transfer undergreater domain shifts in a semi-supervised setting. However, DANNs can beunstable during training and lack a native means for uncertaintyquantification. This study introduces a fully-supervised three-stage framework,the staged Bayesian domain-adversarial neural network (staged B-DANN), thatcombines parameter transfer and shared latent space adaptation. In Stage 1, adeterministic feature extractor is trained on the source domain. This featureextractor is then adversarially refined using a DANN in Stage 2. In Stage 3, aBayesian neural network is built on the adapted feature extractor forfine-tuning on the target domain to handle conditional shifts and yieldcalibrated uncertainty estimates. This staged B-DANN approach was firstvalidated on a synthetic benchmark, where it was shown to significantlyoutperform standard transfer techniques. It was then applied to the task ofpredicting critical heat flux in rectangular channels, leveraging data fromtube experiments as the source domain. The results of this study show that thestaged B-DANN method can improve predictive accuracy and generalization,potentially assisting other domains in nuclear engineering.</description>
      <author>example@mail.com (Aidan Furlong, Robert Salko, Xingang Zhao, Xu Wu)</author>
      <guid isPermaLink="false">2510.26541v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm</title>
      <link>http://arxiv.org/abs/2510.26509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于二维元胞自动机的可适应边缘检测器，通过元启发式方法和迁移学习技术进行优化。研究分析了优化阶段搜索空间扩展的影响以及检测器在不同图像集上的适应性。&lt;h4&gt;背景&lt;/h4&gt;边缘检测是图像处理中提取相关信息的重要任务，但现有检测器存在难以检测松散边缘和缺乏上下文信息等问题。&lt;h4&gt;目的&lt;/h4&gt;分析扩大优化阶段搜索空间的影响，以及检测器在识别自然图像集及其专门子集边缘时的适应性稳健性。&lt;h4&gt;方法&lt;/h4&gt;开发了一种由二维元胞自动机描述并通过元启发式方法结合迁移学习技术优化的可适应检测器。&lt;h4&gt;主要发现&lt;/h4&gt;扩大优化阶段的搜索空间对所选图像集并不有效；模型能够适应不同输入，但迁移学习技术未带来显著改进。&lt;h4&gt;结论&lt;/h4&gt;所提出的检测器具有良好的适应性，但扩大搜索空间和迁移学习技术未能显著提高性能。&lt;h4&gt;翻译&lt;/h4&gt;边缘检测任务在图像处理中至关重要，旨在从图像中提取相关信息。此任务中存在一个反复出现的问题，即某些检测器的弱点，例如难以检测松散边缘以及缺乏从特定问题中提取相关信息的上下文。为解决这些弱点并使检测器适应图像特性，研究人员开发了一种由二维元胞自动机描述并通过元启发式方法结合迁移学习技术优化的可适应检测器。本研究旨在分析扩大优化阶段搜索空间的影响，以及检测器在识别自然图像集及其从同一图像集中提取的专门子集边缘时的适应性稳健性。获得的结果证明，对于所选图像集，扩大优化阶段的搜索空间并不有效。研究还通过一系列实验和验证技术分析了模型的适应性，发现无论采用何种验证方法，模型都能适应输入，并且应用于模型的迁移学习技术未显示出显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The edge detection task is essential in image processing aiming to extractrelevant information from an image. One recurring problem in this task is theweaknesses found in some detectors, such as the difficulty in detecting looseedges and the lack of context to extract relevant information from specificproblems. To address these weaknesses and adapt the detector to the propertiesof an image, an adaptable detector described by two-dimensional cellularautomaton and optimized by meta-heuristic combined with transfer learningtechniques was developed. This study aims to analyze the impact of expandingthe search space of the optimization phase and the robustness of theadaptability of the detector in identifying edges of a set of natural imagesand specialized subsets extracted from the same image set. The results obtainedprove that expanding the search space of the optimization phase was noteffective for the chosen image set. The study also analyzed the adaptability ofthe model through a series of experiments and validation techniques and foundthat, regardless of the validation, the model was able to adapt to the inputand the transfer learning techniques applied to the model showed no significantimprovements.</description>
      <author>example@mail.com (Vinícius Ferraria, Eurico Ruivo)</author>
      <guid isPermaLink="false">2510.26509v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Applications of Machine Learning in Polymer Materials: Property Prediction, Material Design, and Systematic Processes</title>
      <link>http://arxiv.org/abs/2510.26100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  55 pages, 6 tables, 9 figures, a systematic review on the research  progress and application prospects of machine learning in polymer materials&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文系统综述了机器学习技术在聚合物材料领域的研究进展和应用前景，介绍了基本技术、应用方法、当前挑战和未来趋势。&lt;h4&gt;背景&lt;/h4&gt;机器学习技术在聚合物材料领域发展迅速，显著加速了材料预测和设计，但其复杂性也给传统领域研究者带来理解和应用困难。聚合物材料研发面临结构复杂性和传统试错方法局限性等挑战。&lt;h4&gt;目的&lt;/h4&gt;应对聚合物材料研发中的挑战，解决机器学习方法的复杂性给传统领域研究者带来的理解和应用困难，促进机器学习技术在聚合物材料领域的有效应用。&lt;h4&gt;方法&lt;/h4&gt;分析聚合物材料研发中的固有挑战；介绍分子描述符、特征表示、数据标准化和清洗等关键技术；记录高质量聚合物数据库；构建高可靠性机器学习模型；实施实验验证、模型评估和优化方法。&lt;h4&gt;主要发现&lt;/h4&gt;机器学习在聚合物性能预测和材料设计中发挥关键作用；传统机器学习、深度学习和迁移学习等算法有具体应用；数据驱动设计策略包括反向设计、高通量虚拟筛选和多目标优化。&lt;h4&gt;结论&lt;/h4&gt;当前研究面临数据质量和模型泛化能力等技术挑战；未来发展趋势包括多尺度建模、物理信息机器学习、标准化数据共享和可解释机器学习。&lt;h4&gt;翻译&lt;/h4&gt;本文系统综述了机器学习技术在聚合物材料领域的研究进展和应用前景。目前，机器学习方法在聚合物材料研究中发展迅速；尽管它们显著加速了材料预测和设计，但其复杂性也给传统领域研究者的理解和应用带来了困难。针对上述问题，本文首先分析了聚合物材料研发中的固有挑战，包括结构复杂性和传统试错方法的局限性。为解决这些问题，它重点介绍了分子描述符和特征表示、数据标准化和清洗等关键基础技术，并记录了多个高质量的聚合物数据库。随后，它详细阐述了机器学习在聚合物性能预测和材料设计中的关键作用，涵盖了传统机器学习、深度学习和迁移学习等算法的具体应用；进一步深入阐述了数据驱动设计策略，如反向设计、高通量虚拟筛选和多目标优化。本文还系统介绍了构建高可靠性机器学习模型的完整过程，总结了有效的实验验证、模型评估和优化方法。最后，它总结了当前研究中的技术挑战，如数据质量和模型泛化能力，并展望了包括多尺度建模、物理信息机器学习、标准化数据共享和可解释机器学习在内的未来发展趋势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper systematically reviews the research progress and applicationprospects of machine learning technologies in the field of polymer materials.Currently, machine learning methods are developing rapidly in polymer materialresearch; although they have significantly accelerated material prediction anddesign, their complexity has also caused difficulties in understanding andapplication for researchers in traditional fields. In response to the aboveissues, this paper first analyzes the inherent challenges in the research anddevelopment of polymer materials, including structural complexity and thelimitations of traditional trial-and-error methods. To address these problems,it focuses on introducing key basic technologies such as molecular descriptorsand feature representation, data standardization and cleaning, and records anumber of high-quality polymer databases. Subsequently, it elaborates on thekey role of machine learning in polymer property prediction and materialdesign, covering the specific applications of algorithms such as traditionalmachine learning, deep learning, and transfer learning; further, it deeplyexpounds on data-driven design strategies, such as reverse design,high-throughput virtual screening, and multi-objective optimization. The paperalso systematically introduces the complete process of constructinghigh-reliability machine learning models and summarizes effective experimentalverification, model evaluation, and optimization methods. Finally, itsummarizes the current technical challenges in research, such as data qualityand model generalization ability, and looks forward to future developmenttrends including multi-scale modeling, physics-informed machine learning,standardized data sharing, and interpretable machine learning.</description>
      <author>example@mail.com (Hongtao Guo Shuai Li Shu Li)</author>
      <guid isPermaLink="false">2510.26100v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry</title>
      <link>http://arxiv.org/abs/2510.26008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures, submitted to nsdi 26&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为System-X的系统级优化方法，它采用以硬件为中心的思路，仅依赖硬件信号而非工作负载知识进行优化，成功识别了网络和系统配置问题，加速了DeepSeek模型5.97%。&lt;h4&gt;背景&lt;/h4&gt;现代机器学习已成为一个紧密结合的全栈生态系统，许多用户依赖云提供商提供弹性、隔离和成本高效的资源。然而，这些平台即服务使用虚拟化，导致运营商对用户的工作负载了解有限，阻碍了资源优化。&lt;h4&gt;目的&lt;/h4&gt;论证工作负载知识对系统级优化不是必需的，并提出一种仅依赖硬件信号的优化方法。&lt;h4&gt;方法&lt;/h4&gt;提出System-X系统，采用以硬件为中心的方法，仅依赖运营商完全可访问的硬件信号。通过从系统收集低级信号，使用无监督学习管道检测异常。该管道通过分析各种硬件平台上30多种流行的ML模型开发，确保能够适应新兴工作负载和未知部署模式。&lt;h4&gt;主要发现&lt;/h4&gt;使用System-X成功识别了网络和系统配置问题，加速了DeepSeek模型5.97%。&lt;h4&gt;结论&lt;/h4&gt;系统级优化可以通过仅依赖硬件信号来实现，无需了解具体的工作负载细节。&lt;h4&gt;翻译&lt;/h4&gt;现代机器学习已发展成为一个紧密结合的全栈生态系统，结合了硬件、软件、网络和应用。许多用户依赖云提供商提供弹性、隔离和成本高效的资源。不幸的是，这些平台即服务使用虚拟化，这意味着运营商对用户的工作负载了解有限。这阻碍了运营商的资源优化，而这对确保成本效率和最小化执行时间至关重要。在本文中，我们认为工作负载知识对系统级优化不是必需的。我们提出了System-X，它采用以硬件为中心的方法，仅依赖硬件信号——这些信号完全可被运营商访问。使用从系统收集的低级信号，System-X通过无监督学习管道检测异常。该管道是通过分析各种硬件平台上30多种流行的ML模型开发的，确保了对新兴工作负载和未知部署模式的适应性。使用System-X，我们成功识别了网络和系统配置问题，将DeepSeek模型加速了5.97%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern machine learning (ML) has grown into a tightly coupled, full-stackecosystem that combines hardware, software, network, and applications. Manyusers rely on cloud providers for elastic, isolated, and cost-efficientresources. Unfortunately, these platforms as a service use virtualization,which means operators have little insight into the users' workloads. Thishinders resource optimizations by the operator, which is essential to ensurecost efficiency and minimize execution time. In this paper, we argue thatworkload knowledge is unnecessary for system-level optimization. We proposeSystem-X, which takes a \emph{hardware-centric} approach, relying only onhardware signals -- fully accessible by operators. Using low-level signalscollected from the system, System-X detects anomalies through an unsupervisedlearning pipeline. The pipeline is developed by analyzing over 30 popular MLmodels on various hardware platforms, ensuring adaptability to emergingworkloads and unknown deployment patterns. Using System-X, we successfullyidentified both network and system configuration issues, accelerating theDeepSeek model by 5.97%.</description>
      <author>example@mail.com (Ziji Chen, Steven Chien, Peng Qian, Noa Zilberman)</author>
      <guid isPermaLink="false">2510.26008v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised local learning based on voltage-dependent synaptic plasticity for resistive and ferroelectric synapses</title>
      <link>http://arxiv.org/abs/2510.25787v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于忆阻器件的电压依赖性突触可塑性(VDSP)学习方法，用于解决边缘计算设备上AI部署的能耗问题。该方法实现了低功耗的无监督学习，在MNIST模式识别任务上取得了超过83%的准确率，并针对不同类型的忆阻器件进行了适应性调整和鲁棒性优化。&lt;h4&gt;背景&lt;/h4&gt;边缘计算设备上部署人工智能面临显著的能耗和功能性挑战。这些设备需要低功耗且能够实时适应的学习机制。基于纳米尺度电阻存储器的内存计算技术可能在这些边缘设备上执行AI工作负载方面发挥关键作用。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一种高效的无监督和局部学习方法，基于赫布原理在忆阻突触中实现，使AI能够在边缘设备上低功耗运行，同时保持高性能。&lt;h4&gt;方法&lt;/h4&gt;研究引入了电压依赖性突触可塑性(VDSP)作为基于赫布原理的忆阻突触无监督和局部学习方法。这种方法无需复杂脉冲整形电路即可实现在线学习，展示了如何将VDSP适应到三种具有不同开关特性的忆阻器件：TiO₂、基于HfO₂的金属氧化物丝状突触和基于HfZrO₄的铁电隧道结(FTJ)。通过系统级模拟验证了这些器件在脉冲神经网络中的无监督学习能力。&lt;h4&gt;主要发现&lt;/h4&gt;1. 所有测试的忆阻器件在使用200个神经元的情况下，在基于MNIST的模式识别任务上实现了超过83%的准确率，达到最先进性能。2. VDSP方法成功适应了三种不同类型的忆阻器件，证明了其通用性。3. 研究评估了器件变异性(如开关阈值和高低电阻状态水平比率)对性能的影响，并提出了增强系统鲁棒性的缓解策略。&lt;h4&gt;结论&lt;/h4&gt;VDSP方法为边缘计算设备上的AI部署提供了一种高效、低功耗的学习解决方案，无需复杂电路即可实现无监督学习。该方法在不同类型的忆阻器件上表现出色，并通过针对器件变异性的缓解策略增强了系统鲁棒性，为边缘AI应用提供了实用可行的技术路径。&lt;h4&gt;翻译&lt;/h4&gt;在边缘计算设备上部署人工智能面临与能耗和功能相关的重大挑战。这些设备可以从大脑启发的学习机制中极大受益，允许在低功耗条件下进行实时适应。使用纳米尺度电阻存储器的内存计算可能在使这些边缘设备上执行AI工作负载方面发挥关键作用。在本研究中，我们引入了电压依赖性突触可塑性(VDSP)作为一种基于赫布原理的忆阻突触中无监督和局部学习的高效方法。这种方法无需脉冲时间依赖可塑性(STDP)通常需要的复杂脉冲整形电路即可实现在线学习。我们展示了如何将VDSP advantageous地适应到三种具有不同开关特性的忆阻器件：TiO₂、基于HfO₂的金属氧化物丝状突触和基于HfZrO₄的铁电隧道结(FTJ)。进行了包含这些器件的脉冲神经网络系统级模拟，以验证基于MNIST的模式识别任务上的无监督学习，达到了最先进的性能。结果表明所有设备在使用200个神经元的情况下都实现了超过83%的准确率。此外，我们评估了器件变异性的影响，如开关阈值和高低电阻状态水平比率，并提出了增强鲁棒性的缓解策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of AI on edge computing devices faces significant challengesrelated to energy consumption and functionality. These devices could greatlybenefit from brain-inspired learning mechanisms, allowing for real-timeadaptation while using low-power. In-memory computing with nanoscale resistivememories may play a crucial role in enabling the execution of AI workloads onthese edge devices. In this study, we introduce voltage-dependent synapticplasticity (VDSP) as an efficient approach for unsupervised and local learningin memristive synapses based on Hebbian principles. This method enables onlinelearning without requiring complex pulse-shaping circuits typically necessaryfor spike-timing-dependent plasticity (STDP). We show how VDSP can beadvantageously adapted to three types of memristive devices (TiO$_2$,HfO$_2$-based metal-oxide filamentary synapses, and HfZrO$_4$-basedferroelectric tunnel junctions (FTJ)) with disctinctive switchingcharacteristics. System-level simulations of spiking neural networksincorporating these devices were conducted to validate unsupervised learning onMNIST-based pattern recognition tasks, achieving state-of-the-art performance.The results demonstrated over 83% accuracy across all devices using 200neurons. Additionally, we assessed the impact of device variability, such asswitching thresholds and ratios between high and low resistance state levels,and proposed mitigation strategies to enhance robustness.</description>
      <author>example@mail.com (Nikhil Garg, Ismael Balafrej, Joao Henrique Quintino Palhares, Laura Bégon-Lours, Davide Florini, Donato Francesco Falcone, Tommaso Stecconi, Valeria Bragaglia, Bert Jan Offrein, Jean-Michel Portal, Damien Querlioz, Yann Beilliard, Dominique Drouin, Fabien Alibart)</author>
      <guid isPermaLink="false">2510.25787v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>HEIR: Learning Graph-Based Motion Hierarchies</title>
      <link>http://arxiv.org/abs/2510.26786v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code link: https://github.com/princeton-computational-imaging/HEIR&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通用的层次运动建模方法，通过图神经网络直接从数据中学习结构化的、可解释的运动关系，克服了传统方法依赖手动定义层次结构和固定运动基元的局限性，在多种运动类型上展现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;运动的层次结构存在于计算机视觉、图形学和机器人学等多个研究领域，复杂动力学通常源于简单运动组件之间的协调相互作用。现有方法通常依赖于手动定义的或启发式的层次结构，具有固定的运动基元，限制了它们在不同任务间的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种通用的层次运动建模方法，直接从数据中学习结构化的、可解释的运动关系，适用于广泛的以运动为中心的任务。&lt;h4&gt;方法&lt;/h4&gt;使用基于图的层次结构表示观察到的运动，将全局绝对运动分解为父继承模式和局部运动残差；将层次推断制定为可微的图学习问题，其中顶点表示基本运动，有向边通过图神经网络捕获学习的父子依赖关系；在一维平移运动、二维旋转运动和动态三维场景变形三个示例上评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在一维和二维情况下成功重建了内在的运动层次结构；与基线相比，在动态3D高斯飞溅场景中产生了更真实、可解释的变形效果。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一种适应性强、数据驱动的层次建模范式，适用于广泛的以运动为中心的任务，通过学习而非预设层次结构实现了更好的泛化能力和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;运动层次结构存在于包括计算机视觉、图形学和机器人学在内的研究领域，其中复杂动力学通常源于简单运动组件之间的协调相互作用。现有方法对这类动力学进行建模通常依赖于手动定义的或启发式的层次结构，具有固定的运动基元，限制了它们在不同任务间的泛化能力。在本工作中，我们提出了一种通用的层次运动建模方法，直接从数据中学习结构化的、可解释的运动关系。我们的方法使用基于图的层次结构来表示观察到的运动，明确地将全局绝对运动分解为父继承模式和局部运动残差。我们将层次推断制定为可微的图学习问题，其中顶点表示基本运动，有向边通过图神经网络捕获学习的父子依赖关系。我们在三个示例上评估了我们的层次重建方法：一维平移运动、二维旋转运动和通过高斯飞溅的动态三维场景变形。实验结果表明，我们的方法在一维和二维情况下重建了内在的运动层次结构，与基线相比，在动态3D高斯飞溅场景中产生了更真实、可解释的变形。通过提供一种适应性强、数据驱动的层次建模范式，我们的方法适用于广泛的以运动为中心的任务。项目页面：https://light.princeton.edu/HEIR/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决运动层次结构的自动建模问题，现有方法通常依赖手动定义的层次结构或固定的运动基元，限制了跨任务泛化能力。这个问题在计算机视觉、图形学和机器人学等多个领域都很重要，因为复杂运动往往源于简单运动组件间的协调，层次结构能帮助理解、生成、预测和控制运动，解决多尺度依赖关系和组合爆炸问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法（手动定义模板或非可解释神经模块）的局限性，注意到不同研究领域面临共同挑战，需要自适应选择合适抽象层次的方法。他们借鉴了图神经网络思想用于学习边权重和父子关系，使用Gumbel-Softmax技巧处理离散层次结构的可微分采样，还参考了层次运动表示（如骨骼定义关系）和3D场景变形（如NeMF、MovingParts）等领域的现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于图的层次结构表示运动，将全局绝对运动分解为父继承模式和局部运动残差，将层次推断转化为可微分图学习问题。流程包括：1)构建邻近有向图，顶点表示运动元素；2)通过图注意力层计算边权重；3)使用Gumbel-Softmax采样层次结构；4)沿层次结构累积相对速度重建绝对运动；5)通过重建损失和正则化项训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：直接从数据学习可解释运动关系；使用图结构显式分解运动为父继承和局部残差；将层次推断转化为可微分图学习问题；适用于多种运动类型。相比之前工作，不依赖手动定义层次或固定基元，提供可解释结构，不假设特定领域或维度，在3D场景变形中产生更真实结果，具有更好泛化能力和可解释性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于图的层次运动学习方法HEIR，能够直接从数据中学习可解释的运动层次结构，有效分解复杂运动为父继承模式和局部残差，并在多种运动建模任务中展现出优越的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hierarchical structures of motion exist across research fields, includingcomputer vision, graphics, and robotics, where complex dynamics typically arisefrom coordinated interactions among simpler motion components. Existing methodsto model such dynamics typically rely on manually-defined or heuristichierarchies with fixed motion primitives, limiting their generalizabilityacross different tasks. In this work, we propose a general hierarchical motionmodeling method that learns structured, interpretable motion relationshipsdirectly from data. Our method represents observed motions using graph-basedhierarchies, explicitly decomposing global absolute motions intoparent-inherited patterns and local motion residuals. We formulate hierarchyinference as a differentiable graph learning problem, where vertices representelemental motions and directed edges capture learned parent-child dependenciesthrough graph neural networks. We evaluate our hierarchical reconstructionapproach on three examples: 1D translational motion, 2D rotational motion, anddynamic 3D scene deformation via Gaussian splatting. Experimental results showthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,and produces more realistic and interpretable deformations compared to thebaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,data-driven hierarchical modeling paradigm, our method offers a formulationapplicable to a broad range of motion-centric tasks. Project Page:https://light.princeton.edu/HEIR/</description>
      <author>example@mail.com (Cheng Zheng, William Koch, Baiang Li, Felix Heide)</author>
      <guid isPermaLink="false">2510.26786v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Graph Guided Modulo Recovery of EEG Signals</title>
      <link>http://arxiv.org/abs/2510.26756v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure, and 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络的GraphUnwrapNet方法，用于解决脑电图(EEG)信号模数采样恢复问题。通过将EEG信号表示为有组织的图结构，并引入预估计引导的特征注入模块，有效提升了在信号折叠边界处的恢复稳定性。实验结果表明，该方法优于传统优化技术，并与当前深度学习模型具有竞争性。&lt;h4&gt;背景&lt;/h4&gt;脑电图(EEG)在不同人之间常表现出显著变异性，这种波动会干扰可靠信号采集并可能导致信号失真或削波。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的方法从模数采样的折叠观测中恢复原始EEG信号，解决这一高度不适定问题。&lt;h4&gt;方法&lt;/h4&gt;提出GraphUnwrapNet，一种基于图神经网络的方法，将EEG信号表示为有组织的图结构，并引入预估计引导的特征注入模块，提供粗略的折叠指示器以增强恢复稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;在STEW数据集上的实验表明，与传统优化技术相比有持续提升，与当前深度学习模型相比具有竞争性的准确性。&lt;h4&gt;结论&lt;/h4&gt;基于图的方法在鲁棒模数EEG恢复方面具有显著潜力。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)通常表现出显著的个体间变异性。这种波动会干扰可靠的信号采集并可能导致失真或削波。模数采样现在是解决这个问题的有前途的方法，通过折叠信号而不是使它们饱和。从折叠观测中恢复原始波形是一个高度不适定的问题。在本工作中，我们提出了一种基于图神经网络的方法，称为GraphUnwrapNet，用于EEG信号的模数恢复。我们的核心思想是将EEG信号表示为一个有组织的图，其通道和时间连接建立了潜在的相互依赖关系。我们的一个关键贡献是引入了一个预估计引导的特征注入模块，提供粗略的折叠指示器，增强在折叠边界处的恢复稳定性。这种设计将结构信息与折叠先验集成到一个统一的框架中。我们在同时任务脑电图工作负荷(STEW)数据集上进行了全面的实验。结果表明与传统优化技术相比有持续的提升，与当前深度学习模型相比具有竞争性的准确性。我们的发现强调了基于图的方法在鲁棒模数EEG恢复方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) often shows significant variability amongpeople. This fluctuation disrupts reliable acquisition and may result indistortion or clipping. Modulo sampling is now a promising solution to thisproblem, by folding signals instead of saturating them. Recovery of theoriginal waveform from folded observations is a highly ill-posed problem. Inthis work, we propose a method based on a graph neural network, referred to asGraphUnwrapNet, for the modulo recovery of EEG signals. Our core idea is torepresent an EEG signal as an organized graph whose channels and temporalconnections establish underlying interdependence. One of our key contributionsis in introducing a pre-estimation guided feature injection module to providecoarse folding indicators that enhance stability during recovery at wrapboundaries. This design integrates structural information with folding priorsinto an integrated framework. We performed comprehensive experiments on theSimultaneous Task EEG Workload (STEW) dataset. The results demonstrateconsistent enhancements over traditional optimization techniques andcompetitive accuracy relative to current deep learning models. Our findingsemphasize the potential of graph-based methodology for robust modulo EEGrecovery.</description>
      <author>example@mail.com (Soujanya Hazra, Sanjay Ghosh)</author>
      <guid isPermaLink="false">2510.26756v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras</title>
      <link>http://arxiv.org/abs/2510.26614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对事件相机的tokenization方法，称为Spiking Patches，能够保留事件流的异步性和空间稀疏性特性，同时保持高准确性，并且推理速度比传统方法更快。&lt;h4&gt;背景&lt;/h4&gt;现有的事件表示方法（如帧或体素）虽然是同步的且降低了空间稀疏性，但能产生高准确性。&lt;h4&gt;目的&lt;/h4&gt;发现一种能够保留事件相机独特属性的事件表示方法。&lt;h4&gt;方法&lt;/h4&gt;提出Spiking Patches tokenizer，专门为事件相机设计，能够保留事件流的异步性和空间稀疏性。&lt;h4&gt;主要发现&lt;/h4&gt;使用GNN、PCN和Transformer在手势识别和物体检测任务上评估，Spiking Patches的token比基于体素的token推理速度快3.4倍，比基于帧的token推理速度快10.4倍，在保持相同准确性的同时，在某些情况下甚至超越它们，手势识别绝对改进最高达3.8，物体检测绝对改进最高达1.4。&lt;h4&gt;结论&lt;/h4&gt;tokenization是事件视觉领域的新方向，标志着保留事件相机属性方法的发展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出事件的tokenization并展示了一个tokenizer，Spiking Patches，专门为事件相机设计。给定异步和空间稀疏的事件流，我们的目标是发现保留这些属性的事件表示。先前的工作将事件表示为帧或体素。然而，虽然这些表示能产生高准确性，但帧和体素都是同步的，降低了空间稀疏性。Spiking Patches提供了保留事件相机独特属性的方法，我们在实验中证明这不会牺牲准确性。我们使用GNN、PCN和Transformer在手势识别和物体检测任务上评估我们的tokenizer。来自Spiking Patches的token比基于体素的token推理速度快3.4倍，比基于帧的token推理速度快10.4倍。我们在保持相同准确性的同时实现了这一点，在某些情况下甚至超越它们，手势识别绝对改进最高达3.8，物体检测绝对改进最高达1.4。因此，tokenization构成了事件视觉领域的一个新方向，标志着保留事件相机属性方法的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose tokenization of events and present a tokenizer, Spiking Patches,specifically designed for event cameras. Given a stream of asynchronous andspatially sparse events, our goal is to discover an event representation thatpreserves these properties. Prior works have represented events as frames or asvoxels. However, while these representations yield high accuracy, both framesand voxels are synchronous and decrease the spatial sparsity. Spiking Patchesgives the means to preserve the unique properties of event cameras and we showin our experiments that this comes without sacrificing accuracy. We evaluateour tokenizer using a GNN, PCN, and a Transformer on gesture recognition andobject detection. Tokens from Spiking Patches yield inference times that are upto 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. Weachieve this while matching their accuracy and even surpassing in some caseswith absolute improvements up to 3.8 for gesture recognition and up to 1.4 forobject detection. Thus, tokenization constitutes a novel direction inevent-based vision and marks a step towards methods that preserve theproperties of event cameras.</description>
      <author>example@mail.com (Christoffer Koo Øhrstrøm, Ronja Güldenring, Lazaros Nalpantidis)</author>
      <guid isPermaLink="false">2510.26614v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation</title>
      <link>http://arxiv.org/abs/2510.26350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出UnifiedFL，一种动态联邦学习框架，用于处理具有不同神经网络架构和非相同分布数据的客户端之间的协作训练，通过图神经网络优化异构本地网络，实验证明在多个基准测试中表现优越。&lt;h4&gt;背景&lt;/h4&gt;联邦学习作为关键范式，允许多个客户端在不共享原始数据的情况下协作训练模型，支持隐私保护应用。然而，关于具有不同神经网络架构和非相同分布数据集的客户端之间的协作训练研究仍然很少。&lt;h4&gt;目的&lt;/h4&gt;解决现有联邦学习框架在支持根本不同架构客户端、处理数据统计异质性和领域断裂问题上的局限性，提高模型在不同测试域上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出UnifiedFL框架，将异构本地网络表示为有向模型图中的节点和边，通过共享图神经网络优化。引入通用GNN参数化所有架构、基于客户端参数之间欧几里得距离的距离驱动聚类，以及平衡收敛性和多样性的两层聚合策略。&lt;h4&gt;主要发现&lt;/h4&gt;现有联邦学习方法只能支持单一模型家族内的变体，假设共享全局架构，无法适应不同网络类型；现有方法通常只处理统计异质性，忽略领域断裂问题；当客户端使用不同架构、具有非相同分布数据并遇到不同测试域时，当前方法表现不佳。&lt;h4&gt;结论&lt;/h4&gt;UnifiedFL在MedMNIST分类和海马体分割基准测试中表现出优越性能，代码和数据可在https://github.com/basiralab/UnifiedFL获取。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习（FL）已成为一种关键范式，使多个客户端能够在不共享原始数据的情况下协作训练模型，在放射学和病理学等领域支持隐私保护应用。然而，关于具有根本不同神经网络架构和非相同分布数据集的客户端之间的协作训练研究仍然很少。现有的联邦学习框架面临几个局限性。尽管声称支持架构异构性，但大多数联邦学习方法只容忍单一模型家族内的变体（例如，更浅、更深或更宽的CNN），仍然假设共享全局架构，无法适应客户端部署不同网络类型（例如，CNN、GNN、MLP）的联邦。此外，现有方法通常只处理统计异质性，而忽略了领域断裂问题，即每个客户端的数据分布与测试时面临的数据分布明显不同，从而削弱了模型的泛化能力。当客户端使用不同架构、具有非相同分布数据并遇到不同的测试域时，当前方法表现不佳。为解决这些挑战，我们提出UnifiedFL，一种动态联邦学习框架，将异构本地网络表示为有向模型图中的节点和边，并通过共享的图神经网络（GNN）进行优化。UnifiedFL引入了（i）通用GNN参数化所有架构，（ii）基于客户端参数之间欧几里得距离的距离驱动聚类，以及（iii）平衡收敛性和多样性的两层聚合策略。在MedMNIST分类和海马体分割基准测试中进行的实验证明了UnifiedFL的优越性能。代码和数据：https://github.com/basiralab/UnifiedFL&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated learning (FL) has emerged as a key paradigm for collaborative modeltraining across multiple clients without sharing raw data, enablingprivacy-preserving applications in areas such as radiology and pathology.However, works on collaborative training across clients with fundamentallydifferent neural architectures and non-identically distributed datasets remainscarce. Existing FL frameworks face several limitations. Despite claiming tosupport architectural heterogeneity, most recent FL methods only toleratevariants within a single model family (e.g., shallower, deeper, or wider CNNs),still presuming a shared global architecture and failing to accommodatefederations where clients deploy fundamentally different network types (e.g.,CNNs, GNNs, MLPs). Moreover, existing approaches often address only statisticalheterogeneity while overlooking the domain-fracture problem, where eachclient's data distribution differs markedly from that faced at testing time,undermining model generalizability. When clients use different architectures,have non-identically distributed data, and encounter distinct test domains,current methods perform poorly. To address these challenges, we proposeUnifiedFL, a dynamic federated learning framework that represents heterogeneouslocal networks as nodes and edges in a directed model graph optimized by ashared graph neural network (GNN). UnifiedFL introduces (i) a common GNN toparameterize all architectures, (ii) distance-driven clustering via Euclideandistances between clients' parameters, and (iii) a two-tier aggregation policybalancing convergence and diversity. Experiments on MedMNIST classification andhippocampus segmentation benchmarks demonstrate UnifiedFL's superiorperformance. Code and data: https://github.com/basiralab/UnifiedFL</description>
      <author>example@mail.com (Furkan Pala, Islem Rekik)</author>
      <guid isPermaLink="false">2510.26350v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>From Embedding to Control: Representations for Stochastic Multi-Object Systems</title>
      <link>http://arxiv.org/abs/2510.26344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出图可控嵌入（GCE）框架，用于学习线性控制下的随机多对象动力学系统。该框架基于希尔伯特空间嵌入，将受控随机动力学的概率分布嵌入到再生核希尔伯特空间中，保留非线性表达能力的同时支持线性操作。GCE采用平均场近似技术捕获对象间依赖关系，并通过图神经网络构建适应动态交互模式的核特征，能够推广到未见过的拓扑结构。&lt;h4&gt;背景&lt;/h4&gt;具有多个相互作用的随机非线性动力学系统的精确建模和控制是一个具有挑战性的问题。非均匀交互和随机拓扑结构使得这一任务更加困难，需要开发新的方法来处理这些复杂情况。&lt;h4&gt;目的&lt;/h4&gt;研究如何实现具有多个相互作用的随机非线性动力学系统的精确建模和有效控制，特别是应对非均匀交互和随机拓扑带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了图可控嵌入（GCE）框架，这是一种基于希尔伯特空间嵌入的通用方法。GCE将受控随机动力学的概率分布直接嵌入到再生核希尔伯特空间（RKHS）中，允许在保留非线性表达能力的同时进行线性操作。该方法采用平均场近似技术来捕获对象间依赖关系，并通过整合图神经网络构建数据相关的核特征，使其能够适应动态交互模式并推广到未见过的拓扑结构。&lt;h4&gt;主要发现&lt;/h4&gt;1. GCE提供了关于存在性、收敛性和适用性的理论保证；2. 平均场近似技术能够有效捕获对象间依赖关系，实现低样本复杂度；3. 构建的核特征能够适应动态交互模式，仅用有限训练实例即可推广到未见拓扑；4. GCE可无缝扩展到不同大小和拓扑的多对象系统；5. 希尔伯特空间的线性支持简单而有效的控制算法来合成最优序列。&lt;h4&gt;结论&lt;/h4&gt;图可控嵌入（GCE）框架为随机多对象动力学系统的建模和控制提供了一种有效方法。通过结合希尔伯特空间嵌入、平均场近似和图神经网络，GCE能够处理非均匀交互和随机拓扑带来的挑战，并在物理系统、机器人和电力系统实验中展现出优越性能，特别是在分布内和少样本测试中优于其他嵌入方法。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了如何在具有多个相互作用的随机非线性动力学系统中实现精确建模和有效控制。然而，非均匀交互和随机拓扑使这一任务具有挑战性。我们通过提出图可控嵌入（GCE）来解决这些挑战，这是一个用于学习线性控制下随机多对象动力学的通用框架。具体来说，GCE建立在希尔伯特空间嵌入的基础上，允许将受控随机动力学的概率分布直接嵌入到再生核希尔伯特空间（RKHS）中，这使其RKHS中的线性操作能够保留非线性表达能力。我们提供了关于GCE的存在性、收敛性和适用性的理论保证。值得注意的是，采用平均场近似技术来有效捕获对象间依赖关系，并实现可证明的低样本复杂度。通过整合图神经网络，我们构建了能够适应动态交互模式的数据相关核特征，并且仅用有限的训练实例就能推广到未见过的拓扑结构。GCE可以无缝扩展到不同大小和拓扑的多对象系统。利用希尔伯特空间的线性，GCE还支持简单而有效的控制算法来合成最优序列。在物理系统、机器人和电力系统上的实验验证了GCE的有效性，并在分布内和少样本测试中，相比各种有竞争力的嵌入方法都展现出一致的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper studies how to achieve accurate modeling and effective control instochastic nonlinear dynamics with multiple interacting objects. However,non-uniform interactions and random topologies make this task challenging. Weaddress these challenges by proposing \textit{Graph Controllable Embeddings}(GCE), a general framework to learn stochastic multi-object dynamics for linearcontrol. Specifically, GCE is built on Hilbert space embeddings, allowingdirect embedding of probability distributions of controlled stochastic dynamicsinto a reproducing kernel Hilbert space (RKHS), which enables linear operationsin its RKHS while retaining nonlinear expressiveness. We provide theoreticalguarantees on the existence, convergence, and applicability of GCE. Notably, amean field approximation technique is adopted to efficiently captureinter-object dependencies and achieve provably low sample complexity. Byintegrating graph neural networks, we construct data-dependent kernel featuresthat are capable of adapting to dynamic interaction patterns and generalizingto even unseen topologies with only limited training instances. GCE scalesseamlessly to multi-object systems of varying sizes and topologies. Leveragingthe linearity of Hilbert spaces, GCE also supports simple yet effective controlalgorithms for synthesizing optimal sequences. Experiments on physical systems,robotics, and power grids validate GCE and demonstrate consistent performanceimprovement over various competitive embedding methods in both in-distributionand few-shot tests</description>
      <author>example@mail.com (Xiaoyuan Cheng, Yiming Yang, Wei Jiang, Chenyang Yuan, Zhuo Sun, Yukun Hu)</author>
      <guid isPermaLink="false">2510.26344v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Heterogeneous Graph Neural Networks for Cybersecurity Anomaly Detection</title>
      <link>http://arxiv.org/abs/2510.26307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 4 figures, 86 references. Submitted to Journal of Computer  Security (under review)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提供了对网络安全中基于异构图神经网络(HGNN)的异常检测方法的全面综述，建立了分类法，分析了代表性模型，回顾了基准数据集和评估指标，并确定了未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;异常检测在网络安全中是关键任务，需要识别内部威胁、访问违规和协调攻击。基于图的方法在建模实体交互方面变得越来越重要，但大多数依赖于同质和静态结构，这限制了它们捕捉现实环境中异构性和时间演化的能力。&lt;h4&gt;目的&lt;/h4&gt;解决基于HGNN的异常检测研究分散、缺乏比较评估和标准化基准的问题，为该领域建立结构化的基础。&lt;h4&gt;方法&lt;/h4&gt;引入按异常类型和图动力学对方法进行分类的分类法，分析代表性模型并将其映射到关键网络安全应用，回顾常用基准数据集和评估指标，强调其优缺点。&lt;h4&gt;主要发现&lt;/h4&gt;确定了与建模、数据和部署相关的主要开放挑战，概述了未来研究的有希望的方向。&lt;h4&gt;结论&lt;/h4&gt;该综述旨在为推进基于HGNN的异常检测建立结构化的基础，使其可扩展、可解释且实际可部署。&lt;h4&gt;翻译&lt;/h4&gt;异常检测是网络安全中的关键任务，其中识别内部威胁、访问违规和协调攻击对于确保系统弹性至关重要。基于图的方法在建模实体交互方面变得越来越重要，但大多数依赖于同质和静态结构，这限制了它们捕捉现实环境中异构性和时间演化的能力。异构图神经网络已成为一种有前景的异常检测范式，通过整合类型感知变换和关系敏感聚合，能够对复杂的网络数据进行更具表现力的建模。然而，当前关于基于HGNN的异常检测研究仍然分散，建模策略多样，比较评估有限，且缺乏标准化基准。为了解决这一差距，我们对网络安全中基于HGNN的异常检测方法进行了全面综述。我们引入了一个按异常类型和图动力学对方法进行分类的分类法，分析了代表性模型，并将它们映射到关键的网络安全应用。我们还回顾了常用的基准数据集和评估指标，强调了它们的优缺点。最后，我们确定了与建模、数据和部署相关的主要开放挑战，并概述了未来研究的有希望的方向。本综述旨在为推进基于HGNN的异常检测建立结构化的基础，朝着可扩展、可解释和实际可部署的解决方案发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection is a critical task in cybersecurity, where identifyinginsider threats, access violations, and coordinated attacks is essential forensuring system resilience. Graph-based approaches have become increasinglyimportant for modeling entity interactions, yet most rely on homogeneous andstatic structures, which limits their ability to capture the heterogeneity andtemporal evolution of real-world environments. Heterogeneous Graph NeuralNetworks (HGNNs) have emerged as a promising paradigm for anomaly detection byincorporating type-aware transformations and relation-sensitive aggregation,enabling more expressive modeling of complex cyber data. However, currentresearch on HGNN-based anomaly detection remains fragmented, with diversemodeling strategies, limited comparative evaluation, and an absence ofstandardized benchmarks. To address this gap, we provide a comprehensive surveyof HGNN-based anomaly detection methods in cybersecurity. We introduce ataxonomy that classifies approaches by anomaly type and graph dynamics, analyzerepresentative models, and map them to key cybersecurity applications. We alsoreview commonly used benchmark datasets and evaluation metrics, highlightingtheir strengths and limitations. Finally, we identify key open challengesrelated to modeling, data, and deployment, and outline promising directions forfuture research. This survey aims to establish a structured foundation foradvancing HGNN-based anomaly detection toward scalable, interpretable, andpractically deployable solutions.</description>
      <author>example@mail.com (Laura Jiang, Reza Ryan, Qian Li, Nasim Ferdosian)</author>
      <guid isPermaLink="false">2510.26307v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Morphology-Aware Graph Reinforcement Learning for Tensegrity Robot Locomotion</title>
      <link>http://arxiv.org/abs/2510.26067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种形态感知的强化学习框架，通过将图神经网络集成到Soft Actor-Critic算法中，解决了张力完整性机器人的运动控制问题。&lt;h4&gt;背景&lt;/h4&gt;张力完整性机器人结合刚性杆和弹性缆索，具有高弹性和可部署性，但因其欠驱动和高度耦合的动力学特性，在运动控制方面面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种强化学习方法，利用机器人的结构先验知识，提高张力完整性机器人的运动控制性能。&lt;h4&gt;方法&lt;/h4&gt;将机器人的物理拓扑表示为图，使用基于图神经网络(GNN)的策略捕捉组件间的耦合关系，并将其集成到Soft Actor-Critic (SAC)算法中。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在物理三杆张力完整性机器人上得到验证，在直线跟踪和双向转向等运动任务中表现出优异的样本效率、对噪声和刚度变化的鲁棒性以及改进的轨迹精度；学习到的策略可以直接从模拟转移到硬件，无需微调。&lt;h4&gt;结论&lt;/h4&gt;将结构先验知识整合到强化学习中对于张力完整性机器人控制具有显著优势，能够实现更高效、更稳定的控制策略。&lt;h4&gt;翻译&lt;/h4&gt;张力完整性机器人结合刚性杆和弹性缆索，提供高弹性和可部署性，但由于其欠驱动和高度耦合的动力学特性，给运动控制带来了重大挑战。本文引入了一种形态感知的强化学习框架，将图神经网络(GNN)集成到Soft Actor-Critic (SAC)算法中。通过将机器人的物理拓扑表示为图，所提出的基于GNN的策略捕捉了组件之间的耦合关系，实现了比传统多层感知器(MLP)策略更快且更稳定的学习。该方法在物理三杆张力完整性机器人上得到了验证，包括直线跟踪和双向转向在内的三种运动原语。它显示出优异的样本效率、对噪声和刚度变化的鲁棒性，以及改进的轨迹精度。值得注意的是，学习到的策略可以直接从模拟转移到硬件而无需微调，实现了稳定的真实世界运动。这些结果表明，将结构先验知识整合到强化学习中对于张力完整性机器人控制具有优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tensegrity robots combine rigid rods and elastic cables, offering highresilience and deployability but posing major challenges for locomotion controldue to their underactuated and highly coupled dynamics. This paper introduces amorphology-aware reinforcement learning framework that integrates a graphneural network (GNN) into the Soft Actor-Critic (SAC) algorithm. Byrepresenting the robot's physical topology as a graph, the proposed GNN-basedpolicy captures coupling among components, enabling faster and more stablelearning than conventional multilayer perceptron (MLP) policies. The method isvalidated on a physical 3-bar tensegrity robot across three locomotionprimitives, including straight-line tracking and bidirectional turning. Itshows superior sample efficiency, robustness to noise and stiffness variations,and improved trajectory accuracy. Notably, the learned policies transferdirectly from simulation to hardware without fine-tuning, achieving stablereal-world locomotion. These results demonstrate the advantages ofincorporating structural priors into reinforcement learning for tensegrityrobot control.</description>
      <author>example@mail.com (Chi Zhang, Mingrui Li, Wenzhe Tong, Xiaonan Huang)</author>
      <guid isPermaLink="false">2510.26067v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Data-driven Projection Generation for Efficiently Solving Heterogeneous Quadratic Programming Problems</title>
      <link>http://arxiv.org/abs/2510.26061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出一种数据驱动的框架，通过针对特定实例的投影减少高维二次规划问题的变量数量，使用图神经网络生成定制化投影，高效解决二次规划问题。&lt;h4&gt;背景&lt;/h4&gt;二次规划问题在高维情况下求解复杂度高，传统方法面临计算挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效解决高维二次规划问题的方法，通过减少变量数量降低计算复杂度，同时保证解的质量。&lt;h4&gt;方法&lt;/h4&gt;设计基于图神经网络的模型生成定制化投影；使用双层优化训练模型，内层优化在给定投影下解决QP问题，外层优化更新模型参数；开发高效算法计算参数梯度，无需通过求解器反向传播；提供神经网络生成投影矩阵解决QP问题的泛化能力理论分析。&lt;h4&gt;主要发现&lt;/h4&gt;方法能产生高质量可行解并减少计算时间；即使对未见过的QP问题也能生成高质量解决方案；实验结果优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的数据驱动框架通过针对特定实例的投影和图神经网络模型，能高效解决高维二次规划问题，在保证解质量的同时显著减少计算时间。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种数据驱动的框架，通过使用针对特定实例的投影来减少高维二次规划问题中的变量数量，从而高效解决二次规划问题。我们设计了一个基于图神经网络的模型，为每个二次规划实例生成定制化投影，使我们能够即使对于未见过的也能产生高质量解。该模型在异构QP上进行训练，以最小化在投影解上评估的期望目标值。这被表述为一个双层优化问题；内层优化在给定投影下使用QP求解器解决QP问题，而外层优化更新模型参数。我们开发了一种高效算法来解决这个双层优化问题，计算参数梯度时无需通过求解器进行反向传播。我们提供了使用神经网络生成的投影矩阵解决QP问题的泛化能力理论分析。实验结果表明，我们的方法产生了高质量可行解并减少了计算时间，优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a data-driven framework for efficiently solving quadraticprogramming (QP) problems by reducing the number of variables inhigh-dimensional QPs using instance-specific projection. A graph neuralnetwork-based model is designed to generate projections tailored to each QPinstance, enabling us to produce high-quality solutions even for previouslyunseen problems. The model is trained on heterogeneous QPs to minimize theexpected objective value evaluated on the projected solutions. This isformulated as a bilevel optimization problem; the inner optimization solves theQP under a given projection using a QP solver, while the outer optimizationupdates the model parameters. We develop an efficient algorithm to solve thisbilevel optimization problem, which computes parameter gradients withoutbackpropagating through the solver. We provide a theoretical analysis of thegeneralization ability of solving QPs with projection matrices generated byneural networks. Experimental results demonstrate that our method produceshigh-quality feasible solutions with reduced computation time, outperformingexisting methods.</description>
      <author>example@mail.com (Tomoharu Iwata, Futoshi Futami)</author>
      <guid isPermaLink="false">2510.26061v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Robust GNN Watermarking via Implicit Perception of Topological Invariants</title>
      <link>http://arxiv.org/abs/2510.25934v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为InvGNN-WM的图神经网络水印技术，它不依赖后门触发器，而是将所有权与模型对图不变性的隐式感知联系起来，实现了黑盒验证且对任务影响微小的水印方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是有价值的知识产权，但现有水印技术大多依赖后门触发器，这些触发器在常见模型编辑下会被破坏，导致所有权模糊。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需触发器、支持黑盒验证且对任务影响微小的图神经网络水印技术，以解决现有水印技术的局限性。&lt;h4&gt;方法&lt;/h4&gt;使用轻量级头在所有者私有的载体集上预测归一化代数连通性；使用敏感解码器输出比特；使用校准阈值控制误报率。&lt;h4&gt;主要发现&lt;/h4&gt;在多种节点和图分类数据集和主干网络上，InvGNN-WM保持了清洁准确率，同时比基线方法产生更高的水印准确率；该方法在非结构化剪枝、微调和后训练量化条件下保持强健性；纯知识蒸馏会削弱水印，而带有水印损失的知识蒸馏可以恢复水印。&lt;h4&gt;结论&lt;/h4&gt;InvGNN-WM提供了不可感知性和鲁棒性的保证，精确移除该水印被证明是NP完全问题。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)是有价值的知识产权，但许多水印依赖于后门触发器，这些触发器在常见的模型编辑下会被破坏并导致所有权模糊。我们提出了InvGNN-WM，它将所有权与模型对图不变性的隐式感知联系起来，实现了无需触发器、黑盒验证且对任务影响微小的水印。轻量级头在所有者私有的载体集上预测归一化代数连通性；敏感解码器输出比特，校准阈值控制误报率。在多样化的节点和图分类数据集及主干网络上，InvGNN-WM匹配清洁准确率，同时比基于触发器和压缩的基线产生更高的水印准确率。它在非结构化剪枝、微调和后训练量化下保持强健性；普通知识蒸馏(KD)会削弱水印，而带有水印损失的KD(KD+WM)可恢复它。我们提供了不可感知性和鲁棒性的保证，并证明精确移除是NP完全的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are valuable intellectual property, yet manywatermarks rely on backdoor triggers that break under common model edits andcreate ownership ambiguity. We present InvGNN-WM, which ties ownership to amodel's implicit perception of a graph invariant, enabling trigger-free,black-box verification with negligible task impact. A lightweight head predictsnormalized algebraic connectivity on an owner-private carrier set; asign-sensitive decoder outputs bits, and a calibrated threshold controls thefalse-positive rate. Across diverse node and graph classification datasets andbackbones, InvGNN-WM matches clean accuracy while yielding higher watermarkaccuracy than trigger- and compression-based baselines. It remains strong underunstructured pruning, fine-tuning, and post-training quantization; plainknowledge distillation (KD) weakens the mark, while KD with a watermark loss(KD+WM) restores it. We provide guarantees for imperceptibility and robustness,and we prove that exact removal is NP-complete.</description>
      <author>example@mail.com (Jipeng Li, Yannning Shen)</author>
      <guid isPermaLink="false">2510.25934v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Attention Augmented GNN RNN-Attention Models for Advanced Cybersecurity Intrusion Detection</title>
      <link>http://arxiv.org/abs/2510.25802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型混合深度学习架构，结合图神经网络、循环神经网络和多头注意力机制，显著提升了网络安全入侵检测能力。&lt;h4&gt;背景&lt;/h4&gt;现代网络安全环境需要实时入侵检测系统，且需要将计算资源集中在高影响安全事件上。UNSW-NB15数据集包含多样的网络流量模式，为研究提供了基础。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕获网络流量中的空间依赖性和时间动态性的入侵检测系统，提高检测复杂攻击模式的能力。&lt;h4&gt;方法&lt;/h4&gt;提出混合深度学习架构，结合图神经网络(GNNs)捕获空间依赖性、循环神经网络(RNNs)进行序列分析，以及多头注意力机制提高模型可解释性和特征选择。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，与传统机器学习方法和独立深度学习模型相比，该混合模型在准确率、精确率、召回率和F1分数等多个评估指标上表现更优。特别是在检测高级持续性威胁(APTs)、分布式拒绝服务(DDoS)攻击和零日漏洞等复杂攻击模式方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;该混合模型是复杂网络环境中下一代网络安全应用的有前景解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种新型混合深度学习架构，协同结合图神经网络(GNNs)、循环神经网络(RNNs)和多头注意力机制，显著提升了网络安全入侵检测能力。通过利用包含多样化网络流量模式的UNSW-NB15综合数据集，我们的方法有效地通过图结构关系捕获空间依赖性，并通过网络事件的序列分析捕获时间动态性。集成的注意力机制提供了提高模型可解释性和增强特征选择的双重好处，使网络安全分析师能够将计算资源集中在高影响安全事件上——这是现代实时入侵检测系统的关键要求。我们广泛的实验评估表明，与传统机器学习方法和独立的深度学习模型相比，所提出的混合模型在多个评估指标上实现了优越的性能，包括准确率、精确率、召回率和F1分数。该模型在检测高级持续性威胁(APTs)、分布式拒绝服务(DDoS)攻击和零日漏洞等复杂攻击模式方面表现出特别强的性能，使其成为复杂网络环境中下一代网络安全应用的有前景解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a novel hybrid deep learning architecture thatsynergistically combines Graph Neural Networks (GNNs), Recurrent NeuralNetworks (RNNs), and multi-head attention mechanisms to significantly enhancecybersecurity intrusion detection capabilities. By leveraging the comprehensiveUNSW-NB15 dataset containing diverse network traffic patterns, our approacheffectively captures both spatial dependencies through graph structuralrelationships and temporal dynamics through sequential analysis of networkevents. The integrated attention mechanism provides dual benefits of improvedmodel interpretability and enhanced feature selection, enabling cybersecurityanalysts to focus computational resources on high-impact security events -- acritical requirement in modern real-time intrusion detection systems. Ourextensive experimental evaluation demonstrates that the proposed hybrid modelachieves superior performance compared to traditional machine learningapproaches and standalone deep learning models across multiple evaluationmetrics, including accuracy, precision, recall, and F1-score. The modelachieves particularly strong performance in detecting sophisticated attackpatterns such as Advanced Persistent Threats (APTs), Distributed Denial ofService (DDoS) attacks, and zero-day exploits, making it a promising solutionfor next-generation cybersecurity applications in complex network environments.</description>
      <author>example@mail.com (Jayant Biradar, Smit Shah, Tanmay Naik)</author>
      <guid isPermaLink="false">2510.25802v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>SHA-256 Infused Embedding-Driven Generative Modeling of High-Energy Molecules in Low-Data Regimes</title>
      <link>http://arxiv.org/abs/2510.25788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合LSTM网络和注意力GNN的新方法用于高能分子生成和属性预测，通过创新的嵌入空间构建策略实现了67.5%的有效性和37.5%的新颖性，成功识别出37种新型超爆炸物。&lt;h4&gt;背景&lt;/h4&gt;高能材料在推进和防御领域至关重要，但其发现受限于实验数据和测试设施的有限获取。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法来发现高能分子，特别是高能爆炸材料。&lt;h4&gt;方法&lt;/h4&gt;结合长短期记忆网络(LSTM)进行分子生成，使用注意力图神经网络(GNN)进行属性预测，提出了一种创新的嵌入空间构建策略，整合固定的SHA-256嵌入和部分可训练表示，在学习开始前重塑分子输入空间，不依赖预训练。&lt;h4&gt;主要发现&lt;/h4&gt;生成器达到67.5%的有效性和37.5%的新颖性；生成的库相对于训练集的平均Tanimoto系数为0.214，表明框架能够生成多样化的化学空间；识别出37种新型超爆炸物，预测爆速超过9公里/秒。&lt;h4&gt;结论&lt;/h4&gt;这种新方法能够有效发现新型高能材料，特别是超爆炸物，为高能材料的研究提供了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;高能材料(HEMs)对于推进和防御领域至关重要，但其发现受限于实验数据和测试设施的有限获取。这项工作通过结合用于分子生成的长短期记忆网络(LSTM)和用于属性预测的注意力图神经网络(GNN)，提出了一种针对高能分子的新方法。我们提出了一种变革性的嵌入空间构建策略，整合固定的SHA-256嵌入和部分可训练表示。与传统的正则化技术不同，这改变了表示基础本身，在学习开始前重塑了分子输入空间。无需依赖预训练，生成器实现了67.5%的有效性和37.5%的新颖性。生成的库相对于训练集的平均Tanimoto系数为0.214，表明该框架能够生成多样化的化学空间。我们识别出37种新型超爆炸物，预测爆速超过9公里/秒。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-energy materials (HEMs) are critical for propulsion and defense domains,yet their discovery remains constrained by experimental data and restrictedaccess to testing facilities. This work presents a novel approach towardhigh-energy molecules by combining Long Short-Term Memory (LSTM) networks formolecular generation and Attentive Graph Neural Networks (GNN) for propertypredictions. We propose a transformative embedding space construction strategythat integrates fixed SHA-256 embeddings with partially trainablerepresentations. Unlike conventional regularization techniques, this changesthe representational basis itself, reshaping the molecular input space beforelearning begins. Without recourse to pretraining, the generator achieves 67.5%validity and 37.5% novelty. The generated library exhibits a mean Tanimotocoefficient of 0.214 relative to training set signifying the ability offramework to generate a diverse chemical space. We identified 37 new superexplosives higher than 9 km/s predicted detonation velocity.</description>
      <author>example@mail.com (Siddharth Verma, Alankar Alankar)</author>
      <guid isPermaLink="false">2510.25788v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>4-Doodle: Text to 3D Sketches that Move!</title>
      <link>http://arxiv.org/abs/2510.25319v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了4-Doodle框架，一个从文本描述生成动态3D草图动画的无需训练方法，通过双空间蒸馏方案解决文本到3D草图动画任务中的关键挑战。&lt;h4&gt;背景&lt;/h4&gt;现有3D内容生成方法主要关注逼真内容，忽视了稀疏、风格化的3D矢量草图这一轻量级媒介。该任务面临三大挑战：缺乏配对数据集、结构抽象难以建模、动画需要时间一致性和多视角一致性。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需训练的框架，从文本生成动态、时间一致且多视角一致的3D矢量草图动画，实现结构稳定的动画效果，包括翻转、旋转和关节运动等。&lt;h4&gt;方法&lt;/h4&gt;4-Doodle采用双空间蒸馏方案：一个空间使用可微贝塞尔曲线捕获多视角一致的几何形状；另一个通过时间感知先验编码运动动态。采用多视图优化确保结构对齐，并引入结构感知的运动模块将保持形状的轨迹与变形感知的变化分开。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，4-Doodle能生成时间逼真且结构稳定的3D草图动画，在保真度和可控性方面优于现有基线。多视图优化确保结构对齐，结构感知运动模块实现富有表现力的动画效果。&lt;h4&gt;结论&lt;/h4&gt;4-Doodle为文本到动态3D草图动画提供了有效解决方案，使4D内容创作更加直观和易于访问，填补了相关研究空白，为视觉交流和原型设计提供新可能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一个新任务：文本到3D草图动画，旨在让自由形式的草图在动态3D空间中'活起来'。与专注于生成逼真内容的前期工作不同，我们目标是稀疏的、风格化的和视角一致的3D矢量草图，这是一种轻量级且可解释的媒介，非常适合视觉交流和原型设计。然而，这项任务非常具有挑战性：(i) 没有文本和3D（或4D）草图的配对数据集；(ii) 草图需要结构抽象，难以用传统的3D表示建模；(iii) 为这样的草图添加动画需要时间一致性和多视角一致性，而当前的处理流程无法解决这个问题。因此，我们提出了4-Doodle，这是第一个从文本生成动态3D草图的无需训练的框架。它通过双空间蒸馏方案利用预训练的图像和视频扩散模型：一个空间使用可微的贝塞尔曲线捕获多视角一致的几何形状，而另一个通过时间感知先验编码运动动态。与之前的工作不同，后者每步从单一视图进行优化，而我们的多视图优化确保了结构对齐并避免了视图模糊性，这对稀疏草图至关重要。此外，我们引入了一个结构感知的运动模块，该模块将保持形状的轨迹与变形感知的变化分开，实现翻转、旋转和关节运动等富有表现力的动作。大量实验表明，我们的方法能够生成时间上逼真且结构稳定的3D草图动画，在保真度和可控性方面都优于现有的基线。我们希望这项工作能推动更加直观和易于访问的4D内容创作发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从文本描述生成动态3D矢量草图的问题。这个问题很重要，因为随着空间计算平台（如Apple Vision Pro和Meta Quest）的兴起，创建和动画3D草图成为沉浸式内容创建的基础。传统的3D内容生成方法主要关注照片真实感内容，而3D草图作为一种轻量级、可解释的媒介，非常适合设计原型、视觉叙事和空间用户界面等应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将问题分解为两个互补阶段：构建一致的3D草图结构和添加动画。他们借鉴了现有工作，如DreamFusion中的Score Distillation Sampling技术、3Doodle中的贝塞尔曲线表示、LiveSketch中的运动先验蒸馏，以及MVDream等多视角扩散模型。作者的核心创新在于设计了一个双空间知识蒸馏框架，利用预训练的图像和视频扩散模型，通过多视角优化和基于贝塞尔曲线的表示来生成动态3D草图，避免了需要成对的文本-4D草图训练数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是双空间知识蒸馏框架，利用预训练的图像和视频扩散模型将3D结构和运动动力学的知识转移过来，无需特定训练数据。整体流程分为两个阶段：第一阶段是多视角3D草图生成，通过随机初始化贝塞尔曲线，从多个视角（前、后、左、右）渲染并使用Score Distillation Sampling优化曲线参数；第二阶段是运动场学习，将3D场景投影到前视图和侧视图，利用视频扩散模型预测位移序列，然后重建3D位移向量，最后添加时间平滑确保动画流畅。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）首个文本到3D草图动画框架；2）基于可微分贝塞尔曲线的双空间架构；3）多视角优化策略减少歧义并确保结构对齐；4）结构感知的运动生成模块；5）投影-重建策略使视频扩散模型能在3D空间中合成运动。相比之前工作，4-Doodle不仅处理静态3D草图（如SketchDream、Sketch2NeRF），还能生成动态内容；不需要手动绘制运动轨迹（如Sketch2Anim）；专注于结构抽象和草图感知（如Animate3D、3DTopia）；支持动态草图抽象和跨视图时间一致性（如CLAY）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 4-Doodle首次实现了从文本描述直接生成动态、空间一致且富有表现力的3D矢量草图动画，通过双空间知识蒸馏框架和基于贝塞尔曲线的可微表示，解决了草图动画中的结构一致性和时间连贯性挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel task: text-to-3D sketch animation, which aims to bringfreeform sketches to life in dynamic 3D space. Unlike prior works focused onphotorealistic content generation, we target sparse, stylized, andview-consistent 3D vector sketches, a lightweight and interpretable mediumwell-suited for visual communication and prototyping. However, this task isvery challenging: (i) no paired dataset exists for text and 3D (or 4D)sketches; (ii) sketches require structural abstraction that is difficult tomodel with conventional 3D representations like NeRFs or point clouds; and(iii) animating such sketches demands temporal coherence and multi-viewconsistency, which current pipelines do not address. Therefore, we propose4-Doodle, the first training-free framework for generating dynamic 3D sketchesfrom text. It leverages pretrained image and video diffusion models through adual-space distillation scheme: one space captures multi-view-consistentgeometry using differentiable B\'ezier curves, while the other encodes motiondynamics via temporally-aware priors. Unlike prior work (e.g., DreamFusion),which optimizes from a single view per step, our multi-view optimizationensures structural alignment and avoids view ambiguity, critical for sparsesketches. Furthermore, we introduce a structure-aware motion module thatseparates shape-preserving trajectories from deformation-aware changes,enabling expressive motion such as flipping, rotation, and articulatedmovement. Extensive experiments show that our method produces temporallyrealistic and structurally stable 3D sketch animations, outperforming existingbaselines in both fidelity and controllability. We hope this work serves as astep toward more intuitive and accessible 4D content creation.</description>
      <author>example@mail.com (Hao Chen, Jiaqi Wang, Yonggang Qi, Ke Li, Kaiyue Pang, Yi-Zhe Song)</author>
      <guid isPermaLink="false">2510.25319v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
  <item>
      <title>SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation</title>
      <link>http://arxiv.org/abs/2510.25268v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SynHLMA框架，用于生成关节物体的手部语言操作序列，实现了HAOI生成、预测和插值三种任务，在HAOI-lang数据集上展示了优越性能，并可用于机器人抓取应用。&lt;h4&gt;背景&lt;/h4&gt;生成手部抓取动作是具身AI和VR/AR应用中的广泛研究课题。当涉及关节物体交互时，手部抓取合成需要同时考虑物体功能性和物体变形过程中的长期操作序列。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的HAOI序列生成框架SynHLMA，用于合成关节物体的手部语言操作。&lt;h4&gt;方法&lt;/h4&gt;给定关节物体的完整点云，使用离散的HAOI表示建模每个手部物体交互帧；结合自然语言嵌入，通过HAOI操作语言模型训练这些表示，在共享表示空间中对齐抓取过程与语言描述；采用关节感知损失确保手部抓取遵循关节物体的动态变化。&lt;h4&gt;主要发现&lt;/h4&gt;SynHLMA实现了关节物体的三种典型手部操作任务：HAOI生成、HAOI预测和HAOI插值；在HAOI-lang数据集上的评估结果显示，与最先进方法相比具有优越的手部抓取序列生成性能；通过使用SynHLMA提供的操作序列，机器人可以实现灵巧抓取的模仿学习。&lt;h4&gt;结论&lt;/h4&gt;SynHLMA框架在关节物体的手部抓取序列生成方面表现优越，代码和数据集将公开可用，为具身AI和VR/AR应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;通过语言指令生成手部抓取是一个广泛研究的课题，受益于具身AI和VR/AR应用。当转化为关节物体交互时，手部抓取合成不仅需要物体功能性，还需要考虑物体变形过程中的长期操作序列。本文提出了一个新的HAOI序列生成框架SynHLMA，用于合成关节物体的手部语言操作。给定关节物体的完整点云，我们使用离散的HAOI表示来建模每个手部物体交互帧。结合自然语言嵌入，通过HAOI操作语言模型训练这些表示，在共享表示空间中对齐抓取过程与语言描述。采用关节感知损失来确保手部抓取遵循关节物体的动态变化。通过这种方式，我们的SynHLMA实现了关节物体的三种典型手部操作任务：HAOI生成、HAOI预测和HAOI插值。我们在构建的HAOI-lang数据集上评估SynHLMA，实验结果展示了与最先进方法相比的优越手部抓取序列生成性能。我们还展示了机器抓取应用，通过使用SynHLMA提供的操作序列，使模仿学习能够执行灵巧抓取。我们的代码和数据集将公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating hand grasps with language instructions is a widely studied topicthat benefits from embodied AI and VR/AR applications. While transferring intohand articulatied object interaction (HAOI), the hand grasps synthesis requiresnot only object functionality but also long-term manipulation sequence alongthe object deformation. This paper proposes a novel HAOI sequence generationframework SynHLMA, to synthesize hand language manipulation for articulatedobjects. Given a complete point cloud of an articulated object, we utilize adiscrete HAOI representation to model each hand object interaction frame. Alongwith the natural language embeddings, the representations are trained by anHAOI manipulation language model to align the grasping process with itslanguage description in a shared representation space. A joint-aware loss isemployed to ensure hand grasps follow the dynamic variations of articulatedobject joints. In this way, our SynHLMA achieves three typical handmanipulation tasks for articulated objects of HAOI generation, HAOI predictionand HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset andexperimental results demonstrate the superior hand grasp sequence generationperformance comparing with state-of-the-art. We also show a robotics graspapplication that enables dexterous grasps execution from imitation learningusing the manipulation sequence provided by our SynHLMA. Our codes and datasetswill be made publicly available.</description>
      <author>example@mail.com (Wang zhi, Yuyan Liu, Liu Liu, Li Zhang, Ruixuan Lu, Dan Guo)</author>
      <guid isPermaLink="false">2510.25268v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>U-CAN: Unsupervised Point Cloud Denoising with Consistency-Aware Noise2Noise Matching</title>
      <link>http://arxiv.org/abs/2510.25210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025. Project page:  https://gloriasze.github.io/U-CAN/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为U-CAN的无监督点云去噪框架，采用一致性感知的Noise2Noise匹配方法，通过神经网络推断多步去噪路径，并引入几何一致性约束，无需大量人工标注数据即可达到与监督方法相当的去噪效果。&lt;h4&gt;背景&lt;/h4&gt;扫描传感器捕获的点云数据通常受到噪声干扰，这对下游任务（如表面重建和形状理解）有负面影响。先前的工作大多使用含噪-清洁点云对训练神经网络来学习去噪先验，这需要大量的人工努力。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需人工标注数据的无监督点云去噪方法，以减少对大量含噪-清洁点云对的依赖。&lt;h4&gt;方法&lt;/h4&gt;U-CAN框架利用神经网络推断每个点或场景的多步去噪路径，通过噪声到噪声匹配方案实现。通过一种新的损失函数，使模型能够对多个含噪点云观测进行统计推理。引入一种去噪后几何一致性约束，以学习一致性感知的去噪模式。该约束不仅限于3D领域，还可以贡献于2D图像去噪领域。&lt;h4&gt;主要发现&lt;/h4&gt;在点云去噪、上采样和图像去噪的广泛基准测试中，U-CAN比最先进的无监督方法有显著改进，并且产生的结果与监督方法相当。&lt;h4&gt;结论&lt;/h4&gt;U-CAN是一种有效的无监督点云去噪方法，不需要大量人工标注的数据，同时能够达到与监督方法相当的性能，为点云去噪领域提供了一种新的无监督解决方案。&lt;h4&gt;翻译&lt;/h4&gt;扫描传感器捕获的点云数据常常受到噪声干扰，这对下游任务（例如表面重建和形状理解）有严重的负面影响。先前的工作主要集中在使用含噪-清洁点云对训练神经网络来学习去噪先验，这需要大量的人工努力。在本工作中，我们引入了U-CAN，一种基于一致性感知的Noise2Noise匹配的无监督点云去噪框架。具体来说，我们利用神经网络推断形状或场景中每个点的多步去噪路径，采用噪声到噪声匹配方案。我们通过一种新的损失函数实现这一点，该损失函数能够在多个含噪点云观测上进行统计推理。我们进一步引入了一种对去噪后几何一致性的新约束，以学习一致性感知的去噪模式。我们证明所提出的约束是一个通用术语，不仅限于3D领域，还可以贡献于2D图像去噪领域。在点云去噪、上采样和图像去噪的广泛基准测试中，我们的评估显示比最先进的无监督方法有显著改进，其中U-CAN也产生了与监督方法相当的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无监督点云去噪问题。点云数据（如激光雷达扫描获取的三维点数据）通常包含噪声，影响下游任务如表面重建和形状理解。现有方法需要成对的'带噪-干净'点云数据训练，需要大量人工标注，成本高。在现实中，自动驾驶汽车、手机等设备每天都在产生大量带噪点云，而干净数据获取困难。解决这个问题能减少对人工标注的依赖，使去噪技术更容易应用于实际场景，提升自动驾驶、增强现实和机器人等领域的性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到尽管干净点云有限，但带噪点云数据每天都在快速增长。借鉴了2D图像的Noise2Noise方法，但发现点云无序不规则，没有像素间的一对一对应关系，不能直接应用。现有无监督方法如TotalDenoising只使用全局约束，难以保持局部几何结构。因此，作者设计多步去噪框架，通过神经网络为每个点推断去噪路径；提出点对点噪声到噪声匹配，使用地球移动距离建立点间对应关系；引入一致性感知约束确保不同噪声观测的去噪预测保持一致。借鉴了PointNet、PointNet++等点云处理架构和TotalDenoising的全局约束思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是噪声到噪声匹配和一致性感知约束。通过学习从一个带噪点云到另一个带噪点云的映射，利用统计推理从多个带噪观测中揭示干净结构；同时确保不同噪声观测的去噪预测间保持几何一致性，解决缺乏真实表面位置信息导致的收敛不稳定问题。整体流程：输入带噪点云→多步去噪框架（每步包含特征提取和路径预测）→噪声到噪声匹配（使用地球移动距离建立点间对应）→一致性感知约束（最小化不同去噪预测间的几何差异）→输出去噪后点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) U-CAN无监督框架，利用噪声到噪声匹配和一致性感知约束；2) 点对点噪声匹配方案，使用地球移动距离建立点云对应关系；3) 去噪几何一致性约束，确保不同噪声观测的去噪预测一致；4) 证明该约束不仅限于3D领域，也可用于2D图像去噪；5) 可用于无监督点云上采样任务。不同之处：相比监督方法，无需干净数据；相比TotalDenoising等，不仅用全局约束，还引入局部约束；相比直接应用Noise2Noise，解决了点云对应关系缺失问题；相比其他无监督方法，引入一致性约束解决收敛不稳定问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; U-CAN通过创新的噪声到噪声匹配和一致性感知约束，实现了仅使用带噪点云数据就能达到与监督方法相当的去噪效果，无需人工标注的干净数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds captured by scanning sensors are often perturbed by noise, whichhave a highly negative impact on downstream tasks (e.g. surface reconstructionand shape understanding). Previous works mostly focus on training neuralnetworks with noisy-clean point cloud pairs for learning denoising priors,which requires extensively manual efforts. In this work, we introduce U-CAN, anUnsupervised framework for point cloud denoising with Consistency-AwareNoise2Noise matching. Specifically, we leverage a neural network to infer amulti-step denoising path for each point of a shape or scene with a noise tonoise matching scheme. We achieve this by a novel loss which enablesstatistical reasoning on multiple noisy point cloud observations. We furtherintroduce a novel constraint on the denoised geometry consistency for learningconsistency-aware denoising patterns. We justify that the proposed constraintis a general term which is not limited to 3D domain and can also contribute tothe area of 2D image denoising. Our evaluations under the widely usedbenchmarks in point cloud denoising, upsampling and image denoising showsignificant improvement over the state-of-the-art unsupervised methods, whereU-CAN also produces comparable results with the supervised methods.</description>
      <author>example@mail.com (Junsheng Zhou, Xingyu Shi, Haichuan Song, Yi Fang, Yu-Shen Liu, Zhizhong Han)</author>
      <guid isPermaLink="false">2510.25210v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>$D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction</title>
      <link>http://arxiv.org/abs/2510.25173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;D²GS是一种无LiDAR的城市场景重建框架，通过多视图深度预测和优化技术，实现了比使用LiDAR的方法更准确的几何重建。&lt;h4&gt;背景&lt;/h4&gt;高斯溅射(GS)在自动驾驶城市场景重建中显示出潜力，但现有方法依赖多模态传感器(如LiDAR和图像)，而LiDAR数据获取存在时空校准困难和空间不对齐问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需LiDAR数据的城市场景重建方法，避免获取准确LiDAR深度的困难，同时保持或提高重建质量。&lt;h4&gt;方法&lt;/h4&gt;1) 通过反向投影多视图度量深度预测初始化密集点云，并用渐进修剪策略优化；2) 利用深度基础模型的扩散先验增强高斯渲染的深度图，提供更强几何约束；3) 约束道路区域内高斯的形状和法线属性提高地面几何准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在Waymo数据集上的实验表明，D²GS方法始终优于最先进方法，即使与使用真实LiDAR数据的方法相比，也能产生更准确的几何。&lt;h4&gt;结论&lt;/h4&gt;D²GS框架成功实现了无LiDAR的城市场景重建，获得了比LiDAR方法更密集、更准确的几何先验，证明了深度先验和优化策略的有效性。&lt;h4&gt;翻译&lt;/h4&gt;最近，高斯溅射(GS)在自动驾驶领域的城市场景重建中显示出巨大潜力。然而，当前的城市场景重建方法通常依赖于多模态传感器作为输入，即LiDAR和图像。虽然LiDAR点云提供的几何先验可以大大减轻重建中的不适定性，但在实践中获取准确的LiDAR数据仍然具有挑战性：i)需要LiDAR和其他传感器之间精确的时空校准，因为它们可能不会同时捕获数据；ii)当LiDAR和相机安装在不同位置时，空间不对齐会导致重投影误差。为了避免获取准确LiDAR深度的困难，我们提出了D²GS，一个无LiDAR的城市场景重建框架。在这项工作中，我们获得了与LiDAR一样有效但更密集、更准确的几何先验。首先，我们通过反向投影多视图度量深度预测来初始化密集点云。然后通过渐进修剪策略优化该点云以提高全局一致性。其次，我们通过深度增强器联合优化高斯几何和预测的密集度量深度。具体来说，我们利用来自深度基础模型的扩散先验来增强由高斯渲染的深度图。反过来，增强的深度在高斯训练期间提供更强的几何约束。最后，我们通过约束道路区域内高斯的形状和法线属性来提高地面几何的准确性。在Waymo数据集上的大量实验表明，我们的方法始终优于最先进的方法，即使与使用真实LiDAR数据的方法相比，也能产生更准确的几何。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶领域城市场景重建中对LiDAR（激光雷达）数据的依赖问题。这个问题很重要，因为获取准确的LiDAR数据在实际应用中面临诸多挑战：需要专业设备和车辆进行数据收集、传感器间需要精确的时空校准、LiDAR与相机安装在不同位置会导致重投影误差，此外LiDAR数据成本高昂且难以扩展，限制了大规模应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法对LiDAR依赖的问题，探索了替代方案。他们借鉴了多视图深度估计网络、3D Gaussian Splatting框架、扩散先验模型（如Marigold）、场景图表示方法以及道路几何先验知识。在此基础上，设计了三个关键组件：利用渐进式剪枝策略管理密集点云、通过深度增强模块迭代优化深度和高斯表示、在场景图中引入专门的道路节点约束。这些设计既吸收了现有工作的优点，又针对LiDAR-free场景进行了创新改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过有效利用从图像中推导的几何先验，消除对LiDAR数据的依赖，创建一个仅使用相机输入的动态城市街道场景重建框架。整体流程分为：1)初始化阶段：使用多视图深度估计预测深度图，反投影得到点云，通过渐进式剪枝获得代表性点集；2)优化阶段：创建道路节点约束，实施联合优化策略，使用深度增强模块利用扩散先验细化深度；3)训练阶段：迭代更新高斯参数和深度估计，利用置信度图指导深度增强；4)评估阶段：在Waymo数据集上评估性能，与使用LiDAR的方法进行比较。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)LiDAR-free框架，消除对LiDAR数据的需求和校准误差；2)渐进式剪枝策略，有效管理密集点云；3)基于扩散的深度增强联合优化策略，提供密集度量深度监督；4)道路节点约束，利用地面平面先验提高道路重建精度。相比之前工作，不同之处在于：不需要LiDAR数据避免校准问题；不依赖单目深度估计避免尺度模糊；能处理动态场景而多视图深度估计不能；将生成深度先验直接集成到3DGS优化循环中；提供比LiDAR更密集、更准确的几何先验。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; D²GS提出了一种无需LiDAR的城市场景重建框架，通过渐进式剪枝、深度增强和道路节点约束，仅使用相机输入就能实现比使用LiDAR数据更准确的动态城市街道场景重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Gaussian Splatting (GS) has shown great potential for urban scenereconstruction in the field of autonomous driving. However, current urban scenereconstruction methods often depend on multimodal sensors as inputs,\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDARpoint clouds can largely mitigate ill-posedness in reconstruction, acquiringsuch accurate LiDAR data is still challenging in practice: i) precisespatiotemporal calibration between LiDAR and other sensors is required, as theymay not capture data simultaneously; ii) reprojection errors arise from spatialmisalignment when LiDAR and cameras are mounted at different locations. Toavoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, aLiDAR-free urban scene reconstruction framework. In this work, we obtaingeometry priors that are as effective as LiDAR while being denser and moreaccurate. $\textbf{First}$, we initialize a dense point cloud byback-projecting multi-view metric depth predictions. This point cloud is thenoptimized by a Progressive Pruning strategy to improve the global consistency.$\textbf{Second}$, we jointly refine Gaussian geometry and predicted densemetric depth via a Depth Enhancer. Specifically, we leverage diffusion priorsfrom a depth foundation model to enhance the depth maps rendered by Gaussians.In turn, the enhanced depths provide stronger geometric constraints duringGaussian training. $\textbf{Finally}$, we improve the accuracy of groundgeometry by constraining the shape and normal attributes of Gaussians withinroad regions. Extensive experiments on the Waymo dataset demonstrate that ourmethod consistently outperforms state-of-the-art methods, producing moreaccurate geometry even when compared with those using ground-truth LiDAR data.</description>
      <author>example@mail.com (Kejing Xia, Jidong Jia, Ke Jin, Yucai Bai, Li Sun, Dacheng Tao, Youjian Zhang)</author>
      <guid isPermaLink="false">2510.25173v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Point-level Uncertainty Evaluation of Mobile Laser Scanning Point Clouds</title>
      <link>http://arxiv.org/abs/2510.24773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于机器学习的框架，用于评估移动激光扫描点云的点级别不确定性，无需依赖高精度参考数据。&lt;h4&gt;背景&lt;/h4&gt;移动激光扫描点云中的不确定性可靠量化对3D制图、建模和变化分析等下游应用的准确性和可信度至关重要，而传统方法高度依赖难以获取的高精度参考数据。&lt;h4&gt;目的&lt;/h4&gt;解决传统不确定性建模方法依赖高精度参考数据的问题，开发一种不依赖此类数据的点级别不确定性评估方法。&lt;h4&gt;方法&lt;/h4&gt;提出基于机器学习的框架，学习局部几何特征与点级别误差之间的关系，使用随机森林和XGBoost两种集成学习模型，并在空间分区化的真实世界数据集上训练验证以避免数据泄露。&lt;h4&gt;主要发现&lt;/h4&gt;两种模型能有效捕捉几何特征与不确定性间的非线性关系，平均ROC-AUC值超过0.87；描述高程变化、点密度和局部结构复杂性的几何特征在预测不确定性中起主导作用。&lt;h4&gt;结论&lt;/h4&gt;该框架为不确定性评估提供了数据驱动的方法，为大规模点云的质量控制和误差分析提供了可扩展且适应性强的基础。&lt;h4&gt;翻译&lt;/h4&gt;移动激光扫描点云中不确定性的可靠量化对于确保3D制图、建模和变化分析等下游应用的准确性和可信度至关重要。传统的不确定性建模方法高度依赖于高精度参考数据，而这些数据在大规模情况下通常成本高昂或难以获取。为解决这一问题，本研究提出了一种基于机器学习的点级别不确定性评估框架，学习局部几何特征与点级别误差之间的关系。该框架使用随机森林和XGBoost两种集成学习模型实现，在空间分区化的真实世界数据集上进行训练和验证以避免数据泄露。实验结果表明，两种模型都能有效捕捉几何特征与不确定性之间的非线性关系，平均ROC-AUC值超过0.87。分析进一步表明，描述高程变化、点密度和局部结构复杂性的几何特征在预测不确定性中起主导作用。所提出的框架为不确定性评估提供了数据驱动的方法，为未来大规模点云的质量控制和误差分析提供了可扩展且适应性强的基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决移动激光扫描点云的点级不确定性评估问题，特别是减少对高精度参考数据的依赖。这个问题很重要，因为可靠的不确定性量化对3D建模、变化分析等下游应用的准确性和可信度至关重要，而不充分评估点云质量会影响高精度应用如导航和变化分析，不仅降低可靠性，还会浪费时间和资源。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了MLS系统中的不确定性来源，评估了现有方法（前向建模和后向建模）的局限性，特别是后向建模对参考数据的依赖和高成本问题。然后提出用机器学习替代方案，建立点云特征与不确定性关系。该方法借鉴了现有工作，如使用C2C距离作为不确定性度量、基于KNN的邻域定义策略，以及采用随机森林和XGBoost等集成学习方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过机器学习学习局部几何特征与点级误差之间的关系，将不确定性评估转化为二分类问题。整体流程包括：1)使用C2C距离定义不确定性度量；2)基于KNN提取每个点的局部几何特征；3)采用随机森林和XGBoost模型进行二分类训练；4)使用多种指标和空间分区5折交叉验证评估模型性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出基于机器学习的框架减少对参考数据依赖；将不确定性评估转化为二分类问题；使用局部几何特征预测不确定性；采用互补的集成学习方法验证；通过特征重要性分析提供误差源新见解。相比传统方法，本研究采用数据驱动方式，训练后不再需要参考数据，提供了更可扩展和适应性强的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本研究提出了一种基于机器学习的框架，能够通过学习点云的局部几何特征与点级误差之间的关系，实现对移动激光扫描点云的点级不确定性预测，减少了对高精度参考数据的依赖，为大规模点云质量评估提供了新的数据驱动方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable quantification of uncertainty in Mobile Laser Scanning (MLS) pointclouds is essential for ensuring the accuracy and credibility of downstreamapplications such as 3D mapping, modeling, and change analysis. Traditionalbackward uncertainty modeling heavily rely on high-precision reference data,which are often costly or infeasible to obtain at large scales. To address thisissue, this study proposes a machine learning-based framework for point-leveluncertainty evaluation that learns the relationship between local geometricfeatures and point-level errors. The framework is implemented using twoensemble learning models, Random Forest (RF) and XGBoost, which are trained andvalidated on a spatially partitioned real-world dataset to avoid data leakage.Experimental results demonstrate that both models can effectively capture thenonlinear relationships between geometric characteristics and uncertainty,achieving mean ROC-AUC values above 0.87. The analysis further reveals thatgeometric features describing elevation variation, point density, and localstructural complexity play a dominant role in predicting uncertainty. Theproposed framework offers a data-driven perspective of uncertainty evaluation,providing a scalable and adaptable foundation for future quality control anderror analysis of large-scale point clouds.</description>
      <author>example@mail.com (Ziyang Xu, Olaf Wysocki, Christoph Holst)</author>
      <guid isPermaLink="false">2510.24773v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Controlling Contrastive Self-Supervised Learning with Knowledge-Driven Multiple Hypothesis: Application to Beat Tracking</title>
      <link>http://arxiv.org/abs/2510.25560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种对比自监督预训练方法，利用多种可能的正样本假设来解决数据模糊性问题，在音乐节拍跟踪任务上取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;数据中的模糊性和问题约束的多样性会导致机器学习任务产生多种同样合理的不同结果。例如在节拍和强拍跟踪中，不同听众可能采用各种节奏解释，这些解释都不一定是错误的。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理数据模糊性的方法，通过利用多种可能的正样本假设来提高机器学习模型的性能，特别是在音乐表示学习领域。&lt;h4&gt;方法&lt;/h4&gt;提出一种对比自监督预训练方法，模型被训练为学习与不同假设兼容的表示，这些假设通过基于知识的评分函数选择，以保留最合理的假设。&lt;h4&gt;主要发现&lt;/h4&gt;在有标签数据上进行微调时，该方法在标准基准测试上优于现有方法，证明了将领域知识与多假设选择相结合的有效性。&lt;h4&gt;结论&lt;/h4&gt;将领域知识与多假设选择相结合在音乐表示学习中具有显著优势，能够有效处理数据中的模糊性问题并提高模型性能。&lt;h4&gt;翻译&lt;/h4&gt;数据中的模糊性和问题约束可能导致机器学习任务产生多种同样合理的不同结果。例如在节拍和强拍跟踪中，不同听众可能采用各种节奏解释，这些解释都不一定是错误的。为此，我们提出了一种对比自监督预训练方法，利用数据中可能的正样本的多种假设。我们的模型被训练为学习与不同假设兼容的表示，这些假设通过基于知识的评分函数选择，以保留最合理的假设。在有标签数据上进行微调时，我们的模型在标准基准测试上优于现有方法，展示了将领域知识与多假设选择相结合在音乐表示学习中的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ambiguities in data and problem constraints can lead to diverse, equallyplausible outcomes for a machine learning task. In beat and downbeat tracking,for instance, different listeners may adopt various rhythmic interpretations,none of which would necessarily be incorrect. To address this, we propose acontrastive self-supervised pre-training approach that leverages multiplehypotheses about possible positive samples in the data. Our model is trained tolearn representations compatible with different such hypotheses, which areselected with a knowledge-based scoring function to retain the most plausibleones. When fine-tuned on labeled data, our model outperforms existing methodson standard benchmarks, showcasing the advantages of integrating domainknowledge with multi-hypothesis selection in music representation learning inparticular.</description>
      <author>example@mail.com (Antonin Gagnere, Slim Essid, Geoffroy Peeters)</author>
      <guid isPermaLink="false">2510.25560v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning</title>
      <link>http://arxiv.org/abs/2510.25262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于信息瓶颈原理的新归一化方法IBNorm，通过有界压缩操作鼓励嵌入保留预测信息同时抑制无用变异性，在大规模语言模型和视觉模型上均优于传统归一化方法。&lt;h4&gt;背景&lt;/h4&gt;归一化是深度学习的基础，但现有方法如BatchNorm、LayerNorm和RMSNorm都是方差中心的，通过强制零均值和单位方差稳定训练，但没有控制表示如何捕获任务相关信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的归一化方法，能够鼓励表示保留预测信息同时抑制无用变异性，从而产生更具信息量的表示。&lt;h4&gt;方法&lt;/h4&gt;提出IB-Inspired Normalization (IBNorm)，一种基于信息瓶颈原理的简单而强大的方法系列，引入有界压缩操作来优化信息表示。&lt;h4&gt;主要发现&lt;/h4&gt;理论上证明IBNorm比方差中心方法获得更高的IB值和更紧的泛化边界；实验上在大型语言模型和视觉模型上一致优于传统归一化方法，互信息分析证实了其优越的信息瓶颈行为。&lt;h4&gt;结论&lt;/h4&gt;IBNorm能够产生更具信息量的表示，同时保持标准归一化的稳定性和兼容性，是一种优于传统归一化方法的新方法。&lt;h4&gt;翻译&lt;/h4&gt;归一化是深度学习的基础，但现有的方法如BatchNorm、LayerNorm和RMSNorm都是方差中心的，通过强制零均值和单位方差来稳定训练，而没有控制表示如何捕获任务相关信息。我们提出了受信息瓶颈原理启发的归一化方法（IBNorm），这是一种简单而强大的方法系列。IBNorm引入了有界压缩操作，鼓励嵌入保留预测信息同时抑制无用变异性，从而产生更具信息量的表示，同时保持标准归一化的稳定性和兼容性。理论上，我们证明IBNorm比方差中心方法获得更高的IB值和更紧的泛化边界。实验上，IBNorm在大型语言模型（LLaMA、GPT-2）和视觉模型（ResNet、ViT）上一致优于BatchNorm、LayerNorm和RMSNorm，互信息分析证实了其优越的信息瓶颈行为。代码将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Normalization is fundamental to deep learning, but existing approaches suchas BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zeromean and unit variance, stabilizing training without controlling howrepresentations capture task-relevant information. We propose IB-InspiredNormalization (IBNorm), a simple yet powerful family of methods grounded in theInformation Bottleneck principle. IBNorm introduces bounded compressionoperations that encourage embeddings to preserve predictive information whilesuppressing nuisance variability, yielding more informative representationswhile retaining the stability and compatibility of standard normalization.Theoretically, we prove that IBNorm achieves a higher IB value and tightergeneralization bounds than variance-centric methods. Empirically, IBNormconsistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scalelanguage models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutualinformation analysis confirming superior information bottleneck behavior. Codewill be released publicly.</description>
      <author>example@mail.com (Xiandong Zou, Pan Zhou)</author>
      <guid isPermaLink="false">2510.25262v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Improving time series estimation and prediction via transfer learning</title>
      <link>http://arxiv.org/abs/2510.25236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于表示的迁移学习框架，用于解决高维度但样本量有限的时间序列数据集的估计和预测问题，通过利用相关源数据集的丰富观测信息提高估计效率。&lt;h4&gt;背景&lt;/h4&gt;许多时间序列数据集（如宏观经济变量）具有高维度但样本量有限，仅使用这些数据集本身几乎无法获得有效的估计和准确的预测。&lt;h4&gt;目的&lt;/h4&gt;引入一种基于表示的迁移学习框架用于向量自回归模型，利用相关源数据集的丰富观测信息，通过表示学习提高估计效率。&lt;h4&gt;方法&lt;/h4&gt;提出一种具有良好非渐近性质的两阶段正则化估计程序，并建议使用交替更新算法来寻找估计值。该框架能够处理具有不同样本量和异步开始/结束时间点的时间序列，灵活整合来自不同数据集的信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟实验评估了所提出方法的有限样本性能，并通过对日本和其他20个宏观经济变量的实证分析证明了该方法的有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;该迁移学习框架解决了高维度但样本量有限的时间序列分析问题，通过利用相关源数据集的信息提高了估计效率和预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;现有文献中存在许多高维度但样本量有限的时间序列，如宏观经济变量，仅使用相应的数据集本身几乎不可能获得有效的估计和准确的预测。本文通过引入一种基于表示的迁移学习框架来填补这一空白，该框架用于向量自回归模型，可以通过表示学习利用来自相关源数据集的丰富观测信息来提高估计效率。提出了一种具有良好建立的非渐近性质的两阶段正则化估计程序，并建议使用交替更新算法来寻找估计值。我们的迁移学习框架可以处理具有不同样本量和异步开始/结束时间点的时间序列，从而在整合来自不同数据集的信息方面提供了显著的灵活性。进行了模拟实验来评估所提出方法的有限样本性能，并通过分析日本和其他20个宏观经济变量的实证分析证明了其有用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There are many time series in the literature with high dimension yet limitedsample sizes, such as macroeconomic variables, and it is almost impossible toobtain efficient estimation and accurate prediction by using the correspondingdatasets themselves. This paper fills the gap by introducing a novelrepresentation-based transfer learning framework for vector autoregressivemodels, and information from related source datasets with rich observations canbe leveraged to enhance estimation efficiency through representation learning.A two-stage regularized estimation procedure is proposed with well establishednon-asymptotic properties, and algorithms with alternating updates aresuggested to search for the estimates. Our transfer learning framework canhandle time series with varying sample sizes and asynchronous starting and/orending time points, thereby offering remarkable flexibility in integratinginformation from diverse datasets. Simulation experiments are conducted toevaluate the finite-sample performance of the proposed methodology, and itsusefulness is demonstrated by an empirical analysis on 20 macroeconomicvariables from Japan and another nine countries.</description>
      <author>example@mail.com (Yuchang Lin, Qianqian Zhu, Guodong Li)</author>
      <guid isPermaLink="false">2510.25236v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Learning Fair Graph Representations with Multi-view Information Bottleneck</title>
      <link>http://arxiv.org/abs/2510.25096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FairMIB是一种多视图信息瓶颈框架，通过分解图为特征、结构和扩散视图，结合对比学习和逆概率加权邻域校正，有效减轻图神经网络中的偏见传播，在保持高任务效用的同时提高公平性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在处理关系数据时表现优秀，但会放大训练数据中的偏见，将歧视性属性和结构不平衡传播到不公平的结果中。现有公平性方法将偏见视为单一来源，忽略了不同的属性和结构效应，导致公平性和实用性之间的次优权衡。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时考虑属性和结构效应的框架，以减轻图神经网络中的偏见传播，实现更好的公平性和实用性权衡。&lt;h4&gt;方法&lt;/h4&gt;FairMIB是一种多视图信息瓶颈框架，将图分解为特征、结构和扩散三个视图。它使用对比学习最大化跨视图互信息实现无偏见表示学习，整合多视角条件信息瓶颈目标平衡任务效用和公平性，并在扩散视图中引入逆概率加权邻域校正减少偏见传播。&lt;h4&gt;主要发现&lt;/h4&gt;FairMIB能够有效分解并处理不同类型的偏见，通过多视图方法实现了比现有方法更好的公平性和实用性权衡。实验表明它在五个真实世界基准数据集上达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;FairMIB通过多视图信息瓶颈框架和创新的偏见缓解技术，成功解决了图神经网络中的偏见问题，在不牺牲任务效用的前提下显著提高了公平性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络通过在节点特征和结构上传递消息，在关系数据上表现出色，但它们会放大训练数据中的偏见，将歧视性属性和结构不平衡传播到不公平的结果中。许多公平性方法将偏见视为单一来源，忽略了不同的属性和结构效应，导致公平性和实用性之间的次优权衡。为了克服这一挑战，我们提出了FairMIB，一种多视图信息瓶颈框架，旨在将图分解为特征、结构和扩散视图，以减轻图神经网络中的复杂度偏见。特别是，所提出的FairMIB采用对比学习来最大化跨视图互信息，实现无偏见的表示学习。它进一步整合多视角条件信息瓶颈目标，通过最小化与敏感属性的互信息来平衡任务效用和公平性。此外，FairMIB在扩散视图中引入了逆概率加权邻域校正，减少了消息传递过程中偏见的传播。在五个真实世界基准数据集上的实验表明，FairMIB在效用和公平性指标上都达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) excel on relational data by passing messagesover node features and structure, but they can amplify training data biases,propagating discriminatory attributes and structural imbalances into unfairoutcomes. Many fairness methods treat bias as a single source, ignoringdistinct attribute and structure effects and leading to suboptimal fairness andutility trade-offs. To overcome this challenge, we propose FairMIB, amulti-view information bottleneck framework designed to decompose graphs intofeature, structural, and diffusion views for mitigating complexity biases inGNNs. Especially, the proposed FairMIB employs contrastive learning to maximizecross-view mutual information for bias-free representation learning. It furtherintegrates multi-perspective conditional information bottleneck objectives tobalance task utility and fairness by minimizing mutual information withsensitive attributes. Additionally, FairMIB introduces an inverseprobability-weighted (IPW) adjacency correction in the diffusion view, whichreduces the spread of bias propagation during message passing. Experiments onfive real-world benchmark datasets demonstrate that FairMIB achievesstate-of-the-art performance across both utility and fairness metrics.</description>
      <author>example@mail.com (Chuxun Liu, Debo Cheng, Qingfeng Chen, Jiangzhang Gan, Jiuyong Li, Lin Liu)</author>
      <guid isPermaLink="false">2510.25096v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Topic Analysis with Side Information: A Neural-Augmented LDA Approach</title>
      <link>http://arxiv.org/abs/2510.24918v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;nnLDA是一种创新的神经增强概率主题模型，通过神经先验机制动态整合辅助信息，解决了传统主题模型难以融入元数据、用户属性或文档标签等辅助信息的局限性，在多个基准数据集上表现出色。&lt;h4&gt;背景&lt;/h4&gt;传统主题模型如LDA被广泛用于揭示文本语料库中的潜在结构，但这些模型往往难以整合辅助信息如元数据、用户属性或文档标签，限制了它们的表现力、个性化和可解释性。&lt;h4&gt;目的&lt;/h4&gt;提出nnLDA，一种神经增强的概率主题模型，通过神经先验机制动态整合辅助信息，以克服传统主题模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;nnLDA将每个文档建模为潜在主题的混合，其中主题比例的先验由基于辅助特征的神经网络生成。这种设计使模型能够捕获辅助信息和主题分布之间复杂的非线性交互，并开发了随机变分期望最大化算法来联合优化神经和概率组件。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上，nnLDA在主题一致性、困惑度和下游分类方面持续优于LDA和Dirichlet-Multinomial Regression。&lt;h4&gt;结论&lt;/h4&gt;当辅助信息可用时，结合神经表示学习和概率主题建模能够带来显著优势，nnLDA证明了这种混合方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;传统的主题模型如潜在狄利克雷分配（LDA）已被广泛用于揭示文本语料库中的潜在结构，但它们往往难以整合辅助信息，如元数据、用户属性或文档标签。这些局限性限制了它们的表现力、个性化和可解释性。为此，我们提出了nnLDA，一种神经增强的概率主题模型，通过神经先验机制动态整合辅助信息。nnLDA将每个文档建模为潜在主题的混合，其中主题比例的先验由基于辅助特征的神经网络生成。这种设计使模型能够捕获辅助信息和主题分布之间复杂的非线性交互，这是静态狄利克雷先验无法表示的。我们开发了一种随机变分期望最大化算法来联合优化神经和概率组件。在多个基准数据集上，nnLDA在主题一致性、困惑度和下游分类方面持续优于LDA和狄利克雷-多项式回归。这些结果强调了在辅助信息可用的情况下，结合神经表示学习和概率主题建模的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional topic models such as Latent Dirichlet Allocation (LDA) have beenwidely used to uncover latent structures in text corpora, but they oftenstruggle to integrate auxiliary information such as metadata, user attributes,or document labels. These limitations restrict their expressiveness,personalization, and interpretability. To address this, we propose nnLDA, aneural-augmented probabilistic topic model that dynamically incorporates sideinformation through a neural prior mechanism. nnLDA models each document as amixture of latent topics, where the prior over topic proportions is generatedby a neural network conditioned on auxiliary features. This design allows themodel to capture complex nonlinear interactions between side information andtopic distributions that static Dirichlet priors cannot represent. We develop astochastic variational Expectation-Maximization algorithm to jointly optimizethe neural and probabilistic components. Across multiple benchmark datasets,nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression intopic coherence, perplexity, and downstream classification. These resultshighlight the benefits of combining neural representation learning withprobabilistic topic modeling in settings where side information is available.</description>
      <author>example@mail.com (Biyi Fang, Kripa Rajshekhar, Truong Vo, Diego Klabjan)</author>
      <guid isPermaLink="false">2510.24918v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Transformers from Compressed Representations</title>
      <link>http://arxiv.org/abs/2510.23665v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TEMPEST是一种利用压缩文件字节流结构进行表示学习的方法，通过紧凑编码实现高效语义表示，同时保持与最先进方法相当的准确性。&lt;h4&gt;背景&lt;/h4&gt;压缩文件格式是高效数据存储和传输的基石，但其在表示学习方面的潜力尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;引入一种能够利用压缩文件固有字节流结构进行有效标记化和编码策略的方法，直接从压缩数据流中学习语义表示。&lt;h4&gt;方法&lt;/h4&gt;TEMPEST（TransformErs froM comPressed rEpreSenTations）利用压缩文件的固有字节流结构设计标记化和编码策略，使标准transformer可以直接从压缩数据流中学习语义表示，绕过原始字节级处理或完整媒体解码的需要。&lt;h4&gt;主要发现&lt;/h4&gt;TEMPEST显著减少了语义分类所需的标记数量，降低了计算复杂性和内存使用；在多个数据集、编码方案和模态的实验中，实现了与最先进方法相当的准确性，同时在内存和计算方面提高了效率。&lt;h4&gt;结论&lt;/h4&gt;TEMPEST是一种有效的方法，可以从压缩数据中学习语义表示，在保持准确性的同时提高了效率。&lt;h4&gt;翻译&lt;/h4&gt;压缩文件格式是高效数据存储和传输的基石，但其在表示学习方面的潜力仍未被充分探索。我们引入了TEMPEST（一种基于压缩表示的transformer方法），它利用压缩文件的固有字节流结构来设计有效的标记化和编码策略。通过利用这种紧凑编码，标准的transformer可以直接从压缩数据流中学习语义表示，绕过了原始字节级处理或完整媒体解码的需要。我们的提议显著减少了语义分类所需的标记数量，从而降低了计算复杂性和内存使用。通过在不同数据集、编码方案和模态上的广泛实验，我们表明TEMPEST实现了与最先进方法相当的准确性，同时在内存和计算方面带来了效率提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compressed file formats are the corner stone of efficient data storage andtransmission, yet their potential for representation learning remains largelyunderexplored. We introduce TEMPEST (TransformErs froM comPressedrEpreSenTations), a method that exploits the inherent byte-stream structure ofcompressed files to design an effective tokenization and encoding strategy. Byleveraging this compact encoding, a standard transformer can directly learnsemantic representations from compressed data streams, bypassing the need forraw byte-level processing or full media decoding. Our proposal substantiallyreduces the number of tokens required for semantic classification, therebylowering both computational complexity and memory usage. Through extensiveexperiments across diverse datasets, coding schemes, and modalities, we showthat TEMPEST achieves accuracy competitive wit the state-of-the-art whiledelivering efficiency gains in memory and compute.</description>
      <author>example@mail.com (Juan C. Leon Alcazar, Mattia Soldan, Mohammad Saatialsoruji, Alejandro Pardo, Hani Itani, Juan Camilo Perez, Bernard Ghanem)</author>
      <guid isPermaLink="false">2510.23665v2</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis</title>
      <link>http://arxiv.org/abs/2510.24777v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 8 figures, and 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种多模态交叉增强融合框架，通过整合眼动追踪和面部特征进行阿尔茨海默病诊断，并在包含25名AD患者和25名健康对照者的数据集上实现了95.11%的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;准确诊断阿尔茨海默病对及时干预和减缓疾病进展至关重要。多模态诊断方法通过整合行为和感知领域的互补信息显示出巨大潜力，而眼动追踪和面部特征是认知功能的重要指标，但很少有研究探索它们的联合集成用于辅助AD诊断。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够协同利用眼动追踪和面部特征进行AD检测的多模态交叉增强融合框架，提高诊断性能的鲁棒性和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含两个关键模块的多模态框架：(a)交叉增强融合注意力模块(CEFAM)，通过交叉注意力和全局增强建模模态间交互；(b)方向感知卷积模块(DACM)，通过水平-垂直感受野捕获细粒度方向性面部特征。同时构建了一个同步多模态数据集，包括AD患者和健康对照者在视觉记忆搜索范式中的面部视频和眼动追踪序列。&lt;h4&gt;主要发现&lt;/h4&gt;在构建的数据集上，该框架优于传统的后期融合和特征连接方法，在区分AD和健康对照者方面实现了95.11%的分类准确率，通过明确建模模态间依赖性和模态特定贡献，显示出卓越的鲁棒性和诊断性能。&lt;h4&gt;结论&lt;/h4&gt;多模态交叉增强融合框架通过协同整合眼动追踪和面部特征，能够有效提高阿尔茨海默病的诊断准确性和鲁棒性，为AD的辅助诊断提供了新的方法。&lt;h4&gt;翻译&lt;/h4&gt;阿尔茨海默病(AD)的准确诊断对于实现及时干预和减缓疾病进展至关重要。多模态诊断方法通过整合行为和感知领域的互补信息显示出巨大潜力。特别是，眼动追踪和面部特征是认知功能的重要指标，反映了注意力分布和神经认知状态。然而，很少有研究探索它们的联合集成用于辅助AD诊断。在本研究中，我们提出了一种多模态交叉增强融合框架，通过协同利用眼动追踪和面部特征进行AD检测。该框架包含两个关键模块：(a)交叉增强融合注意力模块(CEFAM)，通过交叉注意力和全局增强建模模态间交互；(b)方向感知卷积模块(DACM)，通过水平-垂直感受野捕获细粒度方向性面部特征。这些模块共同实现了自适应和判别性多模态表示学习。为支持这项工作，我们构建了一个同步多模态数据集，包括25名AD患者和25名健康对照者(HC)，通过在视觉记忆搜索范式期间记录对齐的面部视频和眼动追踪序列，为评估集成策略提供了生态有效资源。在该数据集上的大量实验表明，我们的框架优于传统的后期融合和特征连接方法，在区分AD和HC方面实现了95.11%的分类准确率，通过明确建模模态间依赖性和模态特定贡献，突显了卓越的鲁棒性和诊断性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate diagnosis of Alzheimer's disease (AD) is essential for enablingtimely intervention and slowing disease progression. Multimodal diagnosticapproaches offer considerable promise by integrating complementary informationacross behavioral and perceptual domains. Eye-tracking and facial features, inparticular, are important indicators of cognitive function, reflectingattentional distribution and neurocognitive state. However, few studies haveexplored their joint integration for auxiliary AD diagnosis. In this study, wepropose a multimodal cross-enhanced fusion framework that synergisticallyleverages eye-tracking and facial features for AD detection. The frameworkincorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module(CEFAM), which models inter-modal interactions through cross-attention andglobal enhancement, and (b) a Direction-Aware Convolution Module (DACM), whichcaptures fine-grained directional facial features via horizontal-verticalreceptive fields. Together, these modules enable adaptive and discriminativemultimodal representation learning. To support this work, we constructed asynchronized multimodal dataset, including 25 patients with AD and 25 healthycontrols (HC), by recording aligned facial video and eye-tracking sequencesduring a visual memory-search paradigm, providing an ecologically validresource for evaluating integration strategies. Extensive experiments on thisdataset demonstrate that our framework outperforms traditional late fusion andfeature concatenation methods, achieving a classification accuracy of 95.11% indistinguishing AD from HC, highlighting superior robustness and diagnosticperformance by explicitly modeling inter-modal dependencies andmodality-specific contributions.</description>
      <author>example@mail.com (Yujie Nie, Jianzhang Ni, Yonglong Ye, Yuan-Ting Zhang, Yun Kwok Wing, Xiangqing Xu, Xin Ma, Lizhou Fan)</author>
      <guid isPermaLink="false">2510.24777v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning</title>
      <link>http://arxiv.org/abs/2510.24927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, submitted to the 12th International Conference on Soft  Computing and Machine Intelligence (ISCMI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了WBT-BGRL框架，一种用于二分图链接预测的加权非对比学习方法，通过三元损失中的加权机制增强自举学习，在真实数据集上展示了有竞争力的性能。&lt;h4&gt;背景&lt;/h4&gt;二分图链接预测对推荐系统和故障检测等应用至关重要，但研究较少；对比方法在负采样上效率低且偏差大，非对比方法仅依赖正样本；现有模型在直推式设置中表现良好，但在归纳式、加权和二分场景中效果未验证。&lt;h4&gt;目的&lt;/h4&gt;解决现有二分图链接预测方法的局限性，特别是在归纳、加权和二分场景中的有效性问题。&lt;h4&gt;方法&lt;/h4&gt;提出加权二分图三元自举图潜在表示(WBT-BGRL)，采用非对比框架，通过三元损失中的新加权机制增强自举学习；使用具有双GCN编码器的二分架构；与适配的最先进模型(T-BGRL, BGRL, GBT, CCA-SSG)进行比较评估。&lt;h4&gt;主要发现&lt;/h4&gt;在工业和电子商务真实数据集上，WBT-BGRL展现出有竞争力的性能，特别是在预训练过程中应用加权时效果更佳，突显了加权的非对比学习在二分图归纳链接预测中的价值。&lt;h4&gt;结论&lt;/h4&gt;加权的非对比学习对于二分图中的归纳链接预测具有重要价值，WBT-BGRL框架为此提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;二分图中的链接预测对于推荐系统和故障检测等应用至关重要，但相比单分图的研究较少。对比方法在负采样方面效率低下且存在偏差，而非对比方法仅依赖正样本。现有模型在直推式设置中表现良好，但在归纳式、加权和二分场景中的有效性尚未得到验证。为解决这一问题，我们提出了加权二分图三元自举图潜在表示(WBT-BGRL)，这是一种非对比框架，通过三元损失中的新加权机制增强自举学习。使用具有双GCN编码器的二分架构，将WBT-BGRL与适配的最先进模型(T-BGRL, BGRL, GBT, CCA-SSG)进行比较评估。在工业和电子商务真实世界数据集上的结果显示了具有竞争力的性能，特别是在预训练过程中应用加权时，突显了加权的非对比学习在二分图归纳链接预测中的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction in bipartite graphs is crucial for applications likerecommendation systems and failure detection, yet it is less studied than inmonopartite graphs. Contrastive methods struggle with inefficient and biasednegative sampling, while non-contrastive approaches rely solely on positivesamples. Existing models perform well in transductive settings, but theireffectiveness in inductive, weighted, and bipartite scenarios remains untested.To address this, we propose Weighted Bipartite Triplet-Bootstrapped GraphLatents (WBT-BGRL), a non-contrastive framework that enhances bootstrappedlearning with a novel weighting mechanism in the triplet loss. Using abipartite architecture with dual GCN encoders, WBT-BGRL is evaluated againstadapted state-of-the-art models (T-BGRL, BGRL, GBT, CCA-SSG). Results onreal-world datasets (Industry and E-commerce) show competitive performance,especially when weighting is applied during pretraining-highlighting the valueof weighted, non-contrastive learning for inductive link prediction inbipartite graphs.</description>
      <author>example@mail.com (Joel Frank Huarayo Quispe, Lilian Berton, Didier Vega-Oliveros)</author>
      <guid isPermaLink="false">2510.24927v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Improving Temporal Consistency and Fidelity at Inference-time in Perceptual Video Restoration by Zero-shot Image-based Diffusion Models</title>
      <link>http://arxiv.org/abs/2510.25420v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出两种无需训练的推理时策略，以改善基于零样本图像扩散模型的时间一致性视频修复，通过感知直线引导(PSG)和多路径集成采样(MPES)技术，实现时间稳定的高保真感知视频修复。&lt;h4&gt;背景&lt;/h4&gt;扩散模型已成为单图像修复的强大先验，但由于采样的随机性和整合显式时间建模的复杂性，将其应用于零样本视频修复时存在时间一致性问题。&lt;h4&gt;目的&lt;/h4&gt;在不重新训练或修改预训练扩散模型架构的情况下，提高视频修复中的时间一致性，实现时间稳定的高保真感知视频修复。&lt;h4&gt;方法&lt;/h4&gt;提出两种互补的推理时策略：(1)感知直线引导(PSG)：基于神经科学启发的感知直线假设，通过在感知空间中引入曲率惩罚，引导扩散去噪过程向更平滑的时间演化发展；(2)多路径集成采样(MPES)：通过集成多个扩散轨迹来减少随机变化，提高保真度分数而不牺牲清晰度。&lt;h4&gt;主要发现&lt;/h4&gt;PSG增强了时间自然性，特别是在时间模糊的情况下；MPES在所有任务中一致提高了保真度和时空感知-失真权衡。这两种策略无需重新训练或修改模型架构即可实现显著改进。&lt;h4&gt;结论&lt;/h4&gt;这些无需训练的技术为使用大型预训练扩散模型实现时间稳定的高保真感知视频修复提供了实用路径，通过结合PSG和MPES可以同时改善时间自然性和保真度。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型已成为单图像修复的强大先验，但将其应用于零样本视频修复时，由于采样的随机性和整合显式时间建模的复杂性，存在时间一致性问题。在本工作中，我们解决了在不重新训练或修改其架构的情况下，使用零样本基于图像的扩散模型提高视频修复时间一致性的挑战。我们提出了两种互补的推理时策略：(1)基于神经科学启发的感知直线假设的感知直线引导(PSG)，通过在感知空间中引入曲率惩罚来引导扩散去噪过程向更平滑的时间演化发展，以改善时间感知分数，如Fréchet视频距离(FVD)和感知直线度；(2)多路径集成采样(MPES)，旨在通过集成多个扩散轨迹来减少随机变化，提高保真度(失真)分数，如PSNR和SSIM，而不牺牲清晰度。这些无需训练的技术共同为使用大型预训练扩散模型实现时间稳定的高保真感知视频修复提供了实用路径。我们在多个数据集和退化类型上进行了广泛实验，系统评估了每种策略以了解其优势和局限性。我们的结果表明，虽然PSG增强了时间自然性，特别是在时间模糊的情况下，但MPES在所有任务中一致提高了保真度和时空感知-失真权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models have emerged as powerful priors for single-imagerestoration, but their application to zero-shot video restoration suffers fromtemporal inconsistencies due to the stochastic nature of sampling andcomplexity of incorporating explicit temporal modeling. In this work, weaddress the challenge of improving temporal coherence in video restorationusing zero-shot image-based diffusion models without retraining or modifyingtheir architecture. We propose two complementary inference-time strategies: (1)Perceptual Straightening Guidance (PSG) based on the neuroscience-inspiredperceptual straightening hypothesis, which steers the diffusion denoisingprocess towards smoother temporal evolution by incorporating a curvaturepenalty in a perceptual space to improve temporal perceptual scores, such asFr\'echet Video Distance (FVD) and perceptual straightness; and (2) Multi-PathEnsemble Sampling (MPES), which aims at reducing stochastic variation byensembling multiple diffusion trajectories to improve fidelity (distortion)scores, such as PSNR and SSIM, without sacrificing sharpness. Together, thesetraining-free techniques provide a practical path toward temporally stablehigh-fidelity perceptual video restoration using large pretrained diffusionmodels. We performed extensive experiments over multiple datasets anddegradation types, systematically evaluating each strategy to understand theirstrengths and limitations. Our results show that while PSG enhances temporalnaturalness, particularly in case of temporal blur, MPES consistently improvesfidelity and spatio-temporal perception--distortion trade-off across all tasks.</description>
      <author>example@mail.com (Nasrin Rahimi, A. Murat Tekalp)</author>
      <guid isPermaLink="false">2510.25420v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA</title>
      <link>http://arxiv.org/abs/2510.25332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StreamingCoT是首个专为流媒体视频问答中的时间演化和多模态思维链任务设计的数据集，解决了现有VideoQA数据集无法捕捉时间动态和缺少明确推理过程标注的问题。&lt;h4&gt;背景&lt;/h4&gt;流媒体视频应用的快速增长需要具有增强时间动态理解和复杂推理能力多模态模型，但当前VideoQA数据集存在静态标注机制无法捕捉视频流中答案的演变性质，以及缺少明确推理过程标注两大关键限制。&lt;h4&gt;目的&lt;/h4&gt;解决现有VideoQA数据集的两个关键限制：1)静态标注机制无法捕捉时间视频流中答案的演变性质；2)缺少明确的推理过程标注，限制了模型的可解释性和逻辑推理能力。&lt;h4&gt;方法&lt;/h4&gt;建立动态分层标注架构，生成每秒密集描述并通过相似性融合构建时间依赖的语义段，加入受时间演化模式约束的问题-答案集；提出明确推理链生成范式，通过关键帧语义提取时空对象，使用大语言模型基于对象状态转换推导推理路径，并通过人工验证确保逻辑一致性。&lt;h4&gt;主要发现&lt;/h4&gt;通过StreamingCoT数据集的构建，为推进流媒体视频理解、复杂时间推理和多模态推理研究奠定了基础。&lt;h4&gt;结论&lt;/h4&gt;StreamingCoT数据集及其构建工具包为解决流媒体视频理解中的时间动态和复杂推理问题提供了有效支持，相关资源已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;流媒体视频应用的快速增长需要具有增强时间动态理解和复杂推理能力多模态模型。然而，当前视频问答(VideoQA)数据集存在两个关键限制：1)静态标注机制无法捕捉时间视频流中答案的演变性质；2)缺少明确的推理过程标注，限制了模型的可解释性和逻辑推理能力。为解决这些挑战，我们引入了StreamingCoT，这是首个专为流媒体视频问答中的时间演化推理和多模态思维链(CoT)任务设计的数据集。我们的框架首先建立了动态分层标注架构，生成每秒密集描述并通过相似性融合构建时间依赖的语义段，同时加入受时间演化模式约束的问题-答案集。我们进一步提出了明确的推理链生成范式，通过关键帧语义对齐提取时空对象，使用大语言模型基于对象状态转换推导推理路径，并通过人工验证确保逻辑一致性。该数据集为推进流媒体视频理解、复杂时间推理和多模态推理研究奠定了基础。我们的StreamingCoT及其构建工具包可在https://github.com/Fleeting-hyh/StreamingCoT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758311&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of streaming video applications demands multimodal modelswith enhanced capabilities for temporal dynamics understanding and complexreasoning. However, current Video Question Answering (VideoQA) datasets sufferfrom two critical limitations: 1) Static annotation mechanisms fail to capturethe evolving nature of answers in temporal video streams, and 2) The absence ofexplicit reasoning process annotations restricts model interpretability andlogical deduction capabilities. To address these challenges, We introduceStreamingCoT, the first dataset explicitly designed for temporally evolvingreasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Ourframework first establishes a dynamic hierarchical annotation architecture thatgenerates per-second dense descriptions and constructs temporally-dependentsemantic segments through similarity fusion, paired with question-answer setsconstrained by temporal evolution patterns. We further propose an explicitreasoning chain generation paradigm that extracts spatiotemporal objects viakeyframe semantic alignment, derives object state transition-based reasoningpaths using large language models, and ensures logical coherence throughhuman-verified validation. This dataset establishes a foundation for advancingresearch in streaming video understanding, complex temporal reasoning, andmultimodal inference. Our StreamingCoT and its construction toolkit can beaccessed at https://github.com/Fleeting-hyh/StreamingCoT.</description>
      <author>example@mail.com (Yuhang Hu, Zhenyu Yang, Shihan Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Changsheng Xu)</author>
      <guid isPermaLink="false">2510.25332v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</title>
      <link>http://arxiv.org/abs/2510.25760v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述文章全面回顾了大型多模态空间推理模型在各类任务中的进展，分类了多模态大语言模型的最新研究，并引入了开放基准进行评估。&lt;h4&gt;背景&lt;/h4&gt;人类具有空间推理能力，能够通过视觉和声音等多模态观察来理解空间。大型多模态推理模型扩展了这些能力，在各种空间任务中展现出有希望的性能，但系统性综述和公开可用的基准仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提供对大型多模态空间推理任务的全面回顾，分类多模态大语言模型的进展，并引入用于评估的开放基准。&lt;h4&gt;方法&lt;/h4&gt;文章首先概述通用空间推理，重点关注训练后技术、可解释性和架构。研究内容包括空间关系推理、场景和布局理解、3D空间中的视觉问答和定位，以及具身AI（如视觉语言导航和动作模型）。此外还探讨了音频和第一人称视频等新兴模态对空间理解的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;多模态空间推理模型在2D和3D空间理解、视觉问答、具身AI等任务中展现出有希望的性能。音频和第一人称视频等新兴模态通过新传感器为空间理解提供了新的视角。&lt;h4&gt;结论&lt;/h4&gt;这篇综述为多模态空间推理这一不断发展的领域奠定了坚实的基础，并提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;人类拥有空间推理能力，使他们能够通过多模态观察（如视觉和声音）来理解空间。大型多模态推理模型通过学习感知和推理扩展了这些能力，在各种空间任务中展现出有希望的性能。然而，对这些模型的系统性综述和公开可用的基准仍然有限。在本综述中，我们对大型多模态空间推理任务进行了全面回顾，分类了多模态大语言模型的最新进展，并引入了用于评估的开放基准。我们首先概述了通用的空间推理，重点关注训练后技术、可解释性和架构。除了传统的2D任务外，我们还研究了空间关系推理、场景和布局理解，以及3D空间中的视觉问答和定位。我们还回顾了具身AI的进展，包括视觉语言导航和动作模型。此外，我们还考虑了音频和第一人称视频等新兴模态，这些模态通过新传感器为空间理解做出贡献。我们相信本综述为多模态空间推理这一不断发展的领域奠定了坚实的基础，并提供了有价值的见解。关于本综述的更新信息、开放基准的代码和实现可以在https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态空间推理在大型模型时代缺乏系统综述和公开基准测试的问题。这个问题很重要，因为人类通过视觉、声音等多模态输入理解空间的能力是基础性的，而大型语言模型虽然文本处理能力强，但空间推理能力有限。整合多模态信息增强空间推理对机器人导航、增强现实、自动驾驶等现实应用至关重要，同时缺乏系统评估阻碍了该领域的标准化发展和比较研究。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性回顾和分析现有文献构建了这篇综述。他们首先定义多模态空间推理，然后分类各类空间任务（从2D到3D，从静态到动态），分析技术进展（测试时扩展、后训练方法、架构修改等），最后引入评估基准。作者确实借鉴了大量现有工作，如Wang等人的小型推理模型研究、Ke等人的推理扩展分析、Zha等人的3D能力研究等，但指出这些工作要么未深入多模态空间推理，要么缺乏系统评估框架，因此他们的综述填补了这一空白。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提供首个多模态空间推理在大型模型时代的全面综述，建立系统分类框架，并引入开放基准。整体流程：1)定义多模态空间推理并概述评估维度；2)分析一般多模态空间推理技术（测试时扩展、后训练、架构修改等）；3)探讨3D空间中的核心任务（视觉定位、场景推理、3D生成）；4)讨论具身AI中的空间推理；5)考虑音频和第一人称视频等新兴模态；6)提供开放基准和评估框架。作者还通过GitHub仓库提供代码和最新信息，方便研究实践。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)构建首个专门针对多模态空间推理的系统综述框架；2)建立详细任务分类体系，涵盖从2D到3D、静态到动态、视觉到其他模态的广泛任务；3)引入开放基准标准化评估；4)整合音频和第一人称视频等新兴模态；5)提供跨领域视角连接传统2D理解与3D推理、具身AI等。相比之前工作，本文专注多模态空间推理而非一般推理，提供系统性评估框架而非单一任务分析，引入开放基准而非仅文献回顾，并通过GitHub提供实用资源，更全面且实用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文为多模态空间推理在大型模型时代提供了首个全面的综述框架，系统性地分类了各类空间任务，引入了开放评估基准，并通过整合新兴模态为该领域的研究和实践奠定了坚实基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans possess spatial reasoning abilities that enable them to understandspaces through multimodal observations, such as vision and sound. Largemultimodal reasoning models extend these abilities by learning to perceive andreason, showing promising performance across diverse spatial tasks. However,systematic reviews and publicly available benchmarks for these models remainlimited. In this survey, we provide a comprehensive review of multimodalspatial reasoning tasks with large models, categorizing recent progress inmultimodal large language models (MLLMs) and introducing open benchmarks forevaluation. We begin by outlining general spatial reasoning, focusing onpost-training techniques, explainability, and architecture. Beyond classical 2Dtasks, we examine spatial relationship reasoning, scene and layoutunderstanding, as well as visual question answering and grounding in 3D space.We also review advances in embodied AI, including vision-language navigationand action models. Additionally, we consider emerging modalities such as audioand egocentric video, which contribute to novel spatial understanding throughnew sensors. We believe this survey establishes a solid foundation and offersinsights into the growing field of multimodal spatial reasoning. Updatedinformation about this survey, codes and implementation of the open benchmarkscan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.</description>
      <author>example@mail.com (Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu)</author>
      <guid isPermaLink="false">2510.25760v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>EA3D: Online Open-World 3D Object Extraction from Streaming Videos</title>
      <link>http://arxiv.org/abs/2510.25146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The Thirty-Ninth Annual Conference on Neural Information Processing  Systems(NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ExtractAnything3D (EA3D)，一个统一的在线开放世界3D物体提取框架，能够同时实现几何重建和整体场景理解。&lt;h4&gt;背景&lt;/h4&gt;当前3D场景理解方法受限于离线收集的多视图数据或预构建的3D几何形状。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的在线框架，用于开放世界的3D物体提取，同时实现几何重建和整体场景理解。&lt;h4&gt;方法&lt;/h4&gt;EA3D通过视觉语言和2D视觉基础编码器动态解释每个视频帧提取物体级知识，使用前馈在线更新策略将知识集成到高斯特征图中，从历史帧迭代估计视觉里程计并增量更新高斯特征，通过循环联合优化模块引导模型关注感兴趣区域。&lt;h4&gt;主要发现&lt;/h4&gt;EA3D在多样化基准测试和任务中表现出色，包括照片级真实感渲染、语义和实例分割、3D边界框和语义占用估计以及3D网格生成。&lt;h4&gt;结论&lt;/h4&gt;EA3D建立了统一的、高效的框架，用于联合在线3D重建和整体场景理解，能够支持广泛的下游任务。&lt;h4&gt;翻译&lt;/h4&gt;当前的3D场景理解方法受限于离线收集的多视图数据或预构建的3D几何形状。在本文中，我们提出了ExtractAnything3D (EA3D)，一个统一的在线开放世界3D物体提取框架，能够同时实现几何重建和整体场景理解。给定流式视频，EA3D使用视觉语言和2D视觉基础编码器动态解释每个帧，提取物体级知识。这些知识通过前馈在线更新策略被集成并嵌入到高斯特征图中。然后我们从历史帧迭代估计视觉里程计，并用新观察增量更新在线高斯特征。循环联合优化模块引导模型关注感兴趣区域，同时增强几何重建和语义理解。在多样化的基准测试和任务中的大量实验，包括照片级真实感渲染、语义和实例分割、3D边界框和语义占用估计以及3D网格生成，证明了EA3D的有效性。我们的方法为联合在线3D重建和整体场景理解建立了统一且高效的框架，能够支持广泛的下游任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从流式视频中实时提取和理解开放世界中的3D物体问题。这个问题在现实中非常重要，因为自主智能体（如机器人）需要在陌生环境中实时理解和重建周围环境，而现实世界中的场景是开放的，物体种类和数量未知，需要同时处理流式视频输入并理解物体的几何结构和语义信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有的视觉语言模型在2D开放世界理解上表现出色，但在3D领域存在视角不一致和几何错位问题。他们发现直接将2D模型提升到3D的方法需要预构建的几何和标注数据，而现有的可微分渲染框架又需要完整的多视图图像。受人类感知启发，作者设计了EA3D，使其能像人类一样进入环境时立即开始处理视觉输入。该方法借鉴了视觉语言模型进行开放世界解释，利用视觉基础模型提取特征，基于高斯泼溅构建在线表示，并参考了在线视觉里程计和高斯更新的方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是建立一个统一的在线开放世界3D物体提取框架，能同时进行几何重建和场景理解，无需几何或姿态先验。整体流程包括：1)知识提取与集成：使用VLMs识别物体，维护语义缓存，利用VFMs提取特征并嵌入高斯表示；2)在线3D物体提取：通过在线视觉里程计估计相机姿态，利用在线高斯更新重建几何和理解语义；3)循环联合优化：设计语义感知正则化，联合优化高斯特征和相机姿态，结合多种损失函数提升重建和理解质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的在线开放世界3D物体提取框架，能同时进行重建和理解；2)在线特征高斯表示，结合在线视觉里程计和高斯更新；3)循环联合优化策略，动态引导模型注意力；4)支持多种下游任务。相比之前的工作，EA3D的不同之处在于：它能在线处理流式视频而非依赖完整多视图；能处理开放世界中的未知物体类别；无需预构建几何或姿态先验；提供支持多种任务的统一框架而非专注于单一任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; EA3D提出了一种统一的在线框架，能够从流式视频中实时提取开放世界中的3D物体，同时进行几何重建和语义理解，无需任何几何或姿态先验，支持多种下游3D感知任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current 3D scene understanding methods are limited by offline-collectedmulti-view data or pre-constructed 3D geometry. In this paper, we presentExtractAnything3D (EA3D), a unified online framework for open-world 3D objectextraction that enables simultaneous geometric reconstruction and holisticscene understanding. Given a streaming video, EA3D dynamically interprets eachframe using vision-language and 2D vision foundation encoders to extractobject-level knowledge. This knowledge is integrated and embedded into aGaussian feature map via a feed-forward online update strategy. We theniteratively estimate visual odometry from historical frames and incrementallyupdate online Gaussian features with new observations. A recurrent jointoptimization module directs the model's attention to regions of interest,simultaneously enhancing both geometric reconstruction and semanticunderstanding. Extensive experiments across diverse benchmarks and tasks,including photo-realistic rendering, semantic and instance segmentation, 3Dbounding box and semantic occupancy estimation, and 3D mesh generation,demonstrate the effectiveness of EA3D. Our method establishes a unified andefficient framework for joint online 3D reconstruction and holistic sceneunderstanding, enabling a broad range of downstream tasks.</description>
      <author>example@mail.com (Xiaoyu Zhou, Jingqi Wang, Yuang Jia, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang)</author>
      <guid isPermaLink="false">2510.25146v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Vision-Language Integration for Zero-Shot Scene Understanding in Real-World Environments</title>
      <link>http://arxiv.org/abs/2510.25070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint under review at IEEE Transactions on Pattern Analysis and  Machine Intelligence (TPAMI), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种视觉-语言集成框架，通过统一预训练视觉编码器和大语言模型，实现零样本场景理解，在多个数据集上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;真实世界环境中的零样本场景理解面临重大挑战，由于自然场景的复杂性和可变性，模型必须在没有先前标记示例的情况下识别新对象、动作和上下文。&lt;h4&gt;目的&lt;/h4&gt;实现稳健的零样本场景理解，利用自然语言作为桥梁，推广到未见过的类别和上下文。&lt;h4&gt;方法&lt;/h4&gt;提出视觉-语言集成框架，统一预训练视觉编码器（如CLIP、ViT）和大语言模型（如基于GPT的架构），开发将视觉输入和文本提示嵌入共享空间的统一模型，并使用多模态融合和推理层进行上下文解释。&lt;h4&gt;主要发现&lt;/h4&gt;在Visual Genome、COCO、ADE20K和自定义真实世界数据集上的实验表明，与最先进的零样本模型相比，在对象识别、活动检测和场景字幕生成方面有显著提升，top-1准确率提高高达18%，语义一致性指标也有显著提升。&lt;h4&gt;结论&lt;/h4&gt;跨模态对齐和语言锚定在增强真实世界场景理解的泛化能力方面非常有效。&lt;h4&gt;翻译&lt;/h4&gt;真实世界环境中的零样本场景理解由于自然场景的复杂性和可变性而面临重大挑战，模型必须在没有先前标记示例的情况下识别新对象、动作和上下文。这项工作提出了一种视觉-语言集成框架，统一了预训练的视觉编码器（如CLIP、ViT）和大语言模型（如基于GPT的架构），以实现视觉和文本模态之间的语义对齐。目标是利用自然语言作为桥梁，推广到未见过的类别和上下文，实现稳健的零样本场景理解。我们的方法开发了一个统一模型，将视觉输入和文本提示嵌入到共享空间，然后使用多模态融合和推理层进行上下文解释。在Visual Genome、COCO、ADE20K和自定义真实世界数据集上的实验表明，与最先进的零样本模型相比，在对象识别、活动检测和场景字幕生成方面有显著提升。提出的系统在top-1准确率上提高了高达18%，在语义一致性指标方面也有显著提升，突显了跨模态对齐和语言锚定在增强真实世界场景理解泛化能力方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-shot scene understanding in real-world settings presents majorchallenges due to the complexity and variability of natural scenes, wheremodels must recognize new objects, actions, and contexts without prior labeledexamples. This work proposes a vision-language integration framework thatunifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models(e.g., GPT-based architectures) to achieve semantic alignment between visualand textual modalities. The goal is to enable robust zero-shot comprehension ofscenes by leveraging natural language as a bridge to generalize over unseencategories and contexts. Our approach develops a unified model that embedsvisual inputs and textual prompts into a shared space, followed by multimodalfusion and reasoning layers for contextual interpretation. Experiments onVisual Genome, COCO, ADE20K, and custom real-world datasets demonstratesignificant gains over state-of-the-art zero-shot models in object recognition,activity detection, and scene captioning. The proposed system achieves up to18% improvement in top-1 accuracy and notable gains in semantic coherencemetrics, highlighting the effectiveness of cross-modal alignment and languagegrounding in enhancing generalization for real-world scene understanding.</description>
      <author>example@mail.com (Manjunath Prasad Holenarasipura Rajiv, B. M. Vidyavathi)</author>
      <guid isPermaLink="false">2510.25070v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.24792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 11 tables and figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PISA-Bench是一个基于PISA测试的多语言多模态推理基准，包含六种语言的平行数据集，用于评估视觉语言模型在不同语言和推理任务上的表现。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在多模态推理方面取得显著进展，但现有基准测试在高质量、人工验证的例子方面有限，且大多数数据集仅限于英语，翻译样本的质量保证既耗时又昂贵。&lt;h4&gt;目的&lt;/h4&gt;填补高质量多语言多模态推理基准的空白，提供一个包含多种语言的人工验证数据集。&lt;h4&gt;方法&lt;/h4&gt;从专家创建的PISA测试英语例子中衍生出PISA-Bench，包含人工提取的指令、问题、答案选项和图像，并增加问题类型分类；将这些内容从英语翻译成西班牙语、德语、中文、法语和意大利语，形成完全平行的六语言语料库。&lt;h4&gt;主要发现&lt;/h4&gt;小型视觉语言模型(&lt;20B参数)在PISA-Bench上无法获得高分；模型在非英语部分的性能显著下降；当处理空间和几何推理任务时，模型错误率高。&lt;h4&gt;结论&lt;/h4&gt;通过发布PISA-Bench数据集和评估框架，为推进多模态多语言推理研究提供了重要资源。&lt;h4&gt;翻译&lt;/h4&gt;原文摘要为英文，上述内容已将其核心信息翻译并结构化为中文。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) have demonstrated remarkable progress inmultimodal reasoning. However, existing benchmarks remain limited in terms ofhigh-quality, human-verified examples. Many current datasets rely onsynthetically generated content by large language models (LLMs). Furthermore,most datasets are limited to English, as manual quality assurance of translatedsamples is time-consuming and costly. To fill this gap, we introducePISA-Bench, a multilingual benchmark derived from English examples of theexpert-created PISA tests, a unified framework for the assessment of studentcompetencies in over eighty countries. Each example consists of human-extractedinstructions, questions, answer options, and images, enriched with questiontype categories, and has been translated from English into five additionallanguages (Spanish, German, Chinese, French, and Italian), resulting in a fullyparallel corpus covering six languages. We evaluate state-of-the-artvision-language models on PISA-Bench and find that especially small models(&lt;20B parameters) fail to achieve high test scores. We further find substantialperformance degradation on non-English splits as well as high error-rates whenmodels are tasked with spatial and geometric reasoning. By releasing thedataset and evaluation framework, we provide a resource for advancing researchon multilingual multimodal reasoning.</description>
      <author>example@mail.com (Patrick Haller, Fabio Barth, Jonas Golde, Georg Rehm, Alan Akbik)</author>
      <guid isPermaLink="false">2510.24792v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs</title>
      <link>http://arxiv.org/abs/2510.25753v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025, 24 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了预训练Transformer模型中的上下文学习（ICL）能力，特别是在具有非线性MLP头部的模型上，从多个异构数据源学习的非线性任务。作者通过理论分析和实验验证，证明了非线性MLP相比线性基线能显著提升ICL性能，特别是在非线性任务上，并确定了高质量数据源的关键特性和特征学习的条件。&lt;h4&gt;背景&lt;/h4&gt;预训练Transformer模型展现出显著的上下文学习能力，使其能够在无需参数更新的情况下从演示中适应新任务。然而，理论研究通常依赖于简化的架构（如省略MLP）、数据模型（如具有各向同性输入的线性回归）和单源训练，这限制了它们与现实设置的相关性。&lt;h4&gt;目的&lt;/h4&gt;研究具有非线性MLP头部的预训练Transformer在从多个具有异构输入、任务和噪声分布的数据源获取的非线性任务上的ICL能力，分析数据混合效应，并提供关于架构和数据在ICL中作用的可操作见解。&lt;h4&gt;方法&lt;/h4&gt;分析一个包含两层的MLP模型，其中第一层通过单次梯度步骤训练，第二层完全优化。在高维渐近条件下，利用高斯普适性和正交多项式理论，证明这类模型的ICL误差等价于结构化多项式预测器。在各种激活函数、模型大小和数据分布上进行经验验证，并在多语言情感分析的真实场景中进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;非线性MLP能显著提升ICL性能，特别是在非线性任务上；高质量数据源具有低噪声和结构化协方差的关键特性；只有当任务协方差具有足够结构时，特征学习才会出现；这些发现在各种激活函数、模型大小和数据分布上得到了经验验证；多语言情感分析实验表明这些发现可以扩展到真实世界案例。&lt;h4&gt;结论&lt;/h4&gt;这项工作推进了Transformer中ICL的理论基础，并提供了关于架构和数据在ICL中作用的可操作见解，特别是在非线性任务和异构数据源设置下。&lt;h4&gt;翻译&lt;/h4&gt;预训练的Transformer模型展现出显著的上下文学习（ICL）能力，使它们能够在无需参数更新的情况下从演示中适应新任务。然而，理论研究通常依赖于简化的架构（例如，省略MLP）、数据模型（例如，具有各向同性输入的线性回归）和单源训练，限制了它们与现实设置的相关性。在这项工作中，我们研究了具有非线性MLP头部的预训练Transformer的ICL能力，这些模型在从多个具有异构输入、任务和噪声分布的数据源中获取的非线性任务上表现。我们分析了一个模型，其中MLP包含两层，第一层通过单次梯度步骤训练，第二层完全优化。在高维渐近条件下，我们证明这类模型的ICL误差等价于结构化多项式预测器，利用了高斯普适性和正交多项式理论的结果。这种等价性表明非线性MLP相比线性基线能显著提升ICL性能，特别是在非线性任务上。它还使数据分析混合效应的精确分析成为可能：我们确定了高质量数据源的关键特性（低噪声、结构化协方差），并表明只有当任务协方差具有足够结构时，特征学习才会出现。这些发现在各种激活函数、模型大小和数据分布上得到了经验验证。最后，我们在一个涉及多语言情感分析的真实场景中进行了实验，每种语言被视为不同的数据源。这个案例的实验结果说明了我们的发现如何扩展到真实世界案例。总体而言，我们的工作推进了Transformer中ICL的理论基础，并提供了关于架构和数据在ICL中作用的可操作见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pretrained Transformers demonstrate remarkable in-context learning (ICL)capabilities, enabling them to adapt to new tasks from demonstrations withoutparameter updates. However, theoretical studies often rely on simplifiedarchitectures (e.g., omitting MLPs), data models (e.g., linear regression withisotropic inputs), and single-source training, limiting their relevance torealistic settings. In this work, we study ICL in pretrained Transformers withnonlinear MLP heads on nonlinear tasks drawn from multiple data sources withheterogeneous input, task, and noise distributions. We analyze a model wherethe MLP comprises two layers, with the first layer trained via a singlegradient step and the second layer fully optimized. Under high-dimensionalasymptotics, we prove that such models are equivalent in ICL error tostructured polynomial predictors, leveraging results from the theory ofGaussian universality and orthogonal polynomials. This equivalence reveals thatnonlinear MLPs meaningfully enhance ICL performance, particularly on nonlineartasks, compared to linear baselines. It also enables a precise analysis of datamixing effects: we identify key properties of high-quality data sources (lownoise, structured covariances) and show that feature learning emerges only whenthe task covariance exhibits sufficient structure. These results are validatedempirically across various activation functions, model sizes, and datadistributions. Finally, we experiment with a real-world scenario involvingmultilingual sentiment analysis where each language is treated as a differentsource. Our experimental results for this case exemplify how our findingsextend to real-world cases. Overall, our work advances the theoreticalfoundations of ICL in Transformers and provides actionable insight into therole of architecture and data in ICL.</description>
      <author>example@mail.com (Samet Demir, Zafer Dogan)</author>
      <guid isPermaLink="false">2510.25753v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models</title>
      <link>http://arxiv.org/abs/2510.25577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures, 4 tables, submitted to LREC 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了语音基础模型(SFMs)如何处理语音中的副语言变化，特别是音质(如嘶哑和气声)对模型行为的影响。作者通过开放式生成任务和语音情感识别评估模型对不同音质输入的一致性反应，并引入了新的平行数据集来评估SFMs对音质的敏感性。&lt;h4&gt;背景&lt;/h4&gt;语音基础模型(SFMs)可直接处理原始音频中的口语，绕过文本表示，因此能接触到语音信号中的副语言变化。音质是副语言变化中未被充分探索的维度，包括嘶哑和气声等发声类型，这些类型影响听众对情感状态、立场和社会意义的推断。现有语音理解基准主要依赖多项选择题格式，容易失败，难以捕捉副语言特征对模型行为的微妙影响。&lt;h4&gt;目的&lt;/h4&gt;通过开放式生成任务和语音情感识别探测SFMs，评估模型行为在不同音质输入下是否一致；引入包含音质合成修改的平行数据集，评估SFMs对嘶哑和气声的反应；提供对SFMs对这些特定语音感知非词汇方面敏感性的首次检验。&lt;h4&gt;方法&lt;/h4&gt;使用开放式生成任务探测SFMs；通过语音情感识别评估模型反应；引入包含音质合成修改的平行数据集；评估SFMs对嘶哑和气声的反应。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体实验结果，主要介绍了研究方法和数据集的构建。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及具体结论，主要介绍了研究的创新点和贡献，即首次检验了SFMs对语音中特定非词汇方面的敏感性。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型(SFMs)的最新进展使得可以直接处理原始音频中的口语，绕过中间的文本表示。这种能力使SFMs能够接触到输入语音信号中嵌入的丰富副语言变化，并可能对这些变化做出响应。副语言变化的一个未被充分探索的维度是音质，包括嘶哑和气声等发声类型。这些发声类型已知会影响听众如何推断语音中的情感状态、立场和社会意义。现有的语音理解基准测试主要依赖多项选择题问答(MCQA)格式，这些格式容易失败，因此在捕捉副语言特征如何微妙影响模型行为方面并不可靠。在本文中，我们通过开放式生成任务和语音情感识别来探测SFMs，评估模型行为在不同音质输入下是否一致。我们引入了一个新的平行数据集，其中包含对音质的合成修改，旨在评估SFMs对嘶哑和气声的反应。我们的工作首次检验了SFMs对这些特定语音感知非词汇方面的敏感性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in speech foundation models (SFMs) have enabled the directprocessing of spoken language from raw audio, bypassing intermediate textualrepresentations. This capability allows SFMs to be exposed to, and potentiallyrespond to, rich paralinguistic variations embedded in the input speech signal.One under-explored dimension of paralinguistic variation is voice quality,encompassing phonation types such as creaky and breathy voice. These phonationtypes are known to influence how listeners infer affective state, stance andsocial meaning in speech. Existing benchmarks for speech understanding largelyrely on multiple-choice question answering (MCQA) formats, which are prone tofailure and therefore unreliable in capturing the nuanced ways paralinguisticfeatures influence model behaviour. In this paper, we probe SFMs throughopen-ended generation tasks and speech emotion recognition, evaluating whethermodel behaviours are consistent across different phonation inputs. We introducea new parallel dataset featuring synthesized modifications to voice quality,designed to evaluate SFM responses to creaky and breathy voice. Our workprovides the first examination of SFM sensitivity to these particularnon-lexical aspects of speech perception.</description>
      <author>example@mail.com (Harm Lameris, Shree Harsha Bokkahalli Satish, Joakim Gustafson, Éva Székely)</author>
      <guid isPermaLink="false">2510.25577v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting</title>
      <link>http://arxiv.org/abs/2510.25563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究将大气预报深度学习模型Aurora适应化用于海洋预测，通过微调实现了高精度的海表温度预测，同时降低了计算成本，为数据驱动的海洋预报提供了新方法。&lt;h4&gt;背景&lt;/h4&gt;准确的海洋变量预测对理解气候变化、管理海洋资源和优化海洋活动至关重要。传统海洋预报依赖数值模型，但面临计算成本高和可扩展性有限的问题。&lt;h4&gt;目的&lt;/h4&gt;将Aurora深度学习模型（原为大气预报设计）适应化，用于预测加那利上升流系统的海表温度(SST)，探索深度学习在海洋预报中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;使用高分辨率海洋再分析数据对模型进行分阶段微调，结合纬度加权误差指标，优化超参数以实现高效学习，减少计算需求。&lt;h4&gt;主要发现&lt;/h4&gt;模型实现了0.119K的低均方根误差，异常相关系数高达0.997，成功重现大尺度SST结构，但在捕捉沿海地区精细细节方面存在挑战。&lt;h4&gt;结论&lt;/h4&gt;研究证明使用在不同领域预训练的深度学习模型进行海洋应用具有可行性，未来改进方向包括整合更多海洋变量、提高空间分辨率和探索物理信息神经网络。&lt;h4&gt;翻译&lt;/h4&gt;准确的海洋变量预测对于理解气候变化、管理海洋资源和优化海洋活动至关重要。传统的海洋预报依赖于数值模型；然而，这些方法在计算成本和可扩展性方面存在局限性。在本研究中，我们将Aurora（一种最初为大气预报设计的基础深度学习模型）适应化，用于预测加那利上升流系统的海表温度(SST)。通过使用高分辨率的海洋再分析数据对模型进行微调，我们展示了其捕捉复杂时空模式的能力，同时减少了计算需求。我们的方法包括分阶段微调过程，结合纬度加权误差指标，并优化超参数以实现高效学习。实验结果显示，模型实现了0.119K的低均方根误差，并保持高的异常相关系数(ACC≈0.997)。模型成功重现了大尺度SST结构，但在捕捉沿海地区更精细的细节方面面临挑战。这项工作通过证明使用在不同领域预训练的深度学习模型进行海洋应用的可行性，为数据驱动的海洋预报领域做出了贡献。未来改进包括整合额外的海洋变量、提高空间分辨率，以及探索物理信息神经网络以增强可解释性和理解。这些进步可以改善气候建模和海洋预测精度，支持环境和经济部门的决策制定。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The accurate prediction of oceanographic variables is crucial forunderstanding climate change, managing marine resources, and optimizingmaritime activities. Traditional ocean forecasting relies on numerical models;however, these approaches face limitations in terms of computational cost andscalability. In this study, we adapt Aurora, a foundational deep learning modeloriginally designed for atmospheric forecasting, to predict sea surfacetemperature (SST) in the Canary Upwelling System. By fine-tuning this modelwith high-resolution oceanographic reanalysis data, we demonstrate its abilityto capture complex spatiotemporal patterns while reducing computationaldemands. Our methodology involves a staged fine-tuning process, incorporatinglatitude-weighted error metrics and optimizing hyperparameters for efficientlearning. The experimental results show that the model achieves a low RMSE of0.119K, maintaining high anomaly correlation coefficients (ACC $\approx0.997$). The model successfully reproduces large-scale SST structures but faceschallenges in capturing finer details in coastal regions. This work contributesto the field of data-driven ocean forecasting by demonstrating the feasibilityof using deep learning models pre-trained in different domains for oceanicapplications. Future improvements include integrating additional oceanographicvariables, increasing spatial resolution, and exploring physics-informed neuralnetworks to enhance interpretability and understanding. These advancements canimprove climate modeling and ocean prediction accuracy, supportingdecision-making in environmental and economic sectors.</description>
      <author>example@mail.com (Víctor Medina, Giovanny A. Cuervo-Londoño, Javier Sánchez)</author>
      <guid isPermaLink="false">2510.25563v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>FaCT: Faithful Concept Traces for Explaining Neural Network Decisions</title>
      <link>http://arxiv.org/abs/2510.25512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025; Code is available at  https://github.com/m-parchami/FaCT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种具有模型内在机制概念解释的新模型，强调概念化解释的忠实性，并引入了概念一致性度量C²-Score来评估概念化方法。&lt;h4&gt;背景&lt;/h4&gt;深度网络在各种任务上表现出色，但要全面理解其工作机制仍然是一个挑战。现有的后验概念化方法在解释模型时并不总是忠实于模型本身，且对模型学习的概念做出了严格的假设。&lt;h4&gt;目的&lt;/h4&gt;强调概念化解释的忠实性，提出一种具有模型内在机制概念解释的新模型，并开发一种新的概念一致性度量标准来评估概念化方法。&lt;h4&gt;方法&lt;/h4&gt;提出的新模型的概念跨类别共享，可以从任何层追踪其对logit的贡献和输入可视化。利用基础模型提出了一种新的概念一致性度量标准C²-Score，用于评估概念化方法。&lt;h4&gt;主要发现&lt;/h4&gt;与先前的工作相比，提出的模型在定量上更加一致，用户发现其概念更具可解释性，同时保持了有竞争力的ImageNet性能。&lt;h4&gt;结论&lt;/h4&gt;通过强调概念化解释的忠实性和提出新的度量标准，该研究为理解深度网络的工作机制提供了更有效的方法。&lt;h4&gt;翻译&lt;/h4&gt;深度网络在广泛任务中表现出色，但要全面理解其工作机制仍然是一个关键挑战。许多后验概念化方法被引入以解释其工作原理，但它们并不总是忠实于模型。此外，它们对模型学习的概念做出了严格的假设，如类别特异性、小的空间范围或符合人类预期。在本工作中，我们强调此类概念化解释的忠实性，并提出了一种具有模型内在机制概念解释的新模型。我们的概念跨类别共享，并且可以从任何层追踪其对logit的贡献及其输入可视化。我们还利用基础模型提出了一个新的概念一致性度量标准C²-Score，可用于评估概念化方法。我们表明，与先前的工作相比，我们的概念在定量上更加一致，用户发现我们的概念更具可解释性，同时保持了有竞争力的ImageNet性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep networks have shown remarkable performance across a wide range of tasks,yet getting a global concept-level understanding of how they function remains akey challenge. Many post-hoc concept-based approaches have been introduced tounderstand their workings, yet they are not always faithful to the model.Further, they make restrictive assumptions on the concepts a model learns, suchas class-specificity, small spatial extent, or alignment to human expectations.In this work, we put emphasis on the faithfulness of such concept-basedexplanations and propose a new model with model-inherent mechanisticconcept-explanations. Our concepts are shared across classes and, from anylayer, their contribution to the logit and their input-visualization can befaithfully traced. We also leverage foundation models to propose a newconcept-consistency metric, C$^2$-Score, that can be used to evaluateconcept-based methods. We show that, compared to prior work, our concepts arequantitatively more consistent and users find our concepts to be moreinterpretable, all while retaining competitive ImageNet performance.</description>
      <author>example@mail.com (Amin Parchami-Araghi, Sukrut Rao, Jonas Fischer, Bernt Schiele)</author>
      <guid isPermaLink="false">2510.25512v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.25502v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 18 figures, 13 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TempoPFN，一种基于线性循环神经网络的单变量时间序列基础模型，仅在合成数据上预训练，解决了零样本时间序列预测中的长期预测效率和可重现性问题。&lt;h4&gt;背景&lt;/h4&gt;零样本时间序列预测的基础模型面临长期预测效率低和可重现性差的挑战，现有的仅使用合成数据的方法在具有挑战性的基准测试中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且可重现的时间序列基础模型，用于零样本预测，超越现有仅使用合成数据的方法的性能。&lt;h4&gt;方法&lt;/h4&gt;TempoPFN采用GatedDeltaProduct架构和状态编织技术，实现跨序列长度的完全并行化训练，消除对窗口化或摘要技术的需求。综合合成数据管道统一了随机微分方程、高斯过程和音频合成等多种生成器，并引入新颖的数据增强技术。&lt;h4&gt;主要发现&lt;/h4&gt;在Gift-Eval基准测试中，TempoPFN达到顶尖竞争性能，超越所有现有的仅使用合成数据的方法，并超过绝大多数在真实数据上训练的模型。同时，通过完全并行化的训练和推理，比现有基线更高效。&lt;h4&gt;结论&lt;/h4&gt;开源完整的数据生成管道和训练代码，为未来研究提供可重现的基础，推动零样本时间序列预测领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;零样本时间序列预测的基础模型面临长期预测效率低和可重现性差的挑战，现有的仅使用合成数据的方法在具有挑战性的基准测试中表现不佳。本文提出了TempoPFN，一种基于线性循环神经网络的单变量时间序列基础模型，该模型仅在合成数据上进行预训练。该模型采用GatedDeltaProduct架构和状态编织技术，实现跨序列长度的完全并行化训练，消除对窗口化或摘要技术的需求，同时保持强大的时间状态跟踪能力。我们的综合合成数据管道统一了多种生成器，包括随机微分方程、高斯过程和音频合成，并引入了新颖的数据增强技术。在Gift-Eval基准的零样本评估中，TempoPFN达到了顶尖的竞争性能，超越了所有现有的仅使用合成数据的方法，并超过了绝大多数在真实数据上训练的模型，同时通过利用完全并行化的训练和推理，比现有基线更高效。我们开源了完整的数据生成管道和训练代码，为未来研究提供可重现的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for zero-shot time series forecasting face challenges inefficient long-horizon prediction and reproducibility, with existingsynthetic-only approaches underperforming on challenging benchmarks. This paperpresents TempoPFN, a univariate time series foundation model based on linearRecurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. Themodel uses a GatedDeltaProduct architecture with state-weaving for fullyparallelizable training across sequence lengths, eliminating the need forwindowing or summarization techniques while maintaining robust temporalstate-tracking. Our comprehensive synthetic data pipeline unifies diversegenerators, including stochastic differential equations, Gaussian processes,and audio synthesis, with novel augmentations. In zero-shot evaluations on theGift-Eval benchmark, TempoPFN achieves top-tier competitive performance,outperforming all existing synthetic-only approaches and surpassing the vastmajority of models trained on real-world data, while being more efficient thanexisting baselines by leveraging fully parallelizable training and inference.We open-source our complete data generation pipeline and training code,providing a reproducible foundation for future research.</description>
      <author>example@mail.com (Vladyslav Moroshan, Julien Siems, Arber Zela, Timur Carstensen, Frank Hutter)</author>
      <guid isPermaLink="false">2510.25502v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Position: Biology is the Challenge Physics-Informed ML Needs to Evolve</title>
      <link>http://arxiv.org/abs/2510.25368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出将物理信息机器学习(PIML)扩展到生物学领域，称为生物学信息机器学习(BIML)，以应对生物建模的独特挑战。&lt;h4&gt;背景&lt;/h4&gt;物理信息机器学习已成功将机理理解整合到机器学习中，特别是在受已知物理定律支配的领域，这一成功促使人们尝试将其应用于生物学领域。&lt;h4&gt;目的&lt;/h4&gt;将PIML的原则性方法扩展到生物学，创建BIML框架，使其能够适应生物学的实际现实，而非视为障碍。&lt;h4&gt;方法&lt;/h4&gt;重新调整PIML的方法，使其能够在更软性、概率形式的先验知识下运行，提出四大基础支柱：不确定性量化、上下文化、受限潜在结构推断和可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;生物建模面临的挑战（多方面且不确定的先验知识、异构且嘈杂的数据、部分可观察性以及复杂的高维网络）不应被视为PIML的障碍，而应被视为其进化的催化剂。&lt;h4&gt;结论&lt;/h4&gt;基础模型和大语言模型将成为关键推动因素，将人类专业知识与计算建模结合，构建BIML生态系统，并将PIML启发的创新引向具有高度科学和社会相关性的挑战。&lt;h4&gt;翻译&lt;/h4&gt;物理信息机器学习(PIML)已成功将机理理解整合到机器学习中，特别是在受已知物理定律支配的领域。这一成功促使人们尝试将PIML应用于生物学，这是一个充满动态系统但受不同约束塑造的领域。然而，生物建模面临独特挑战：多方面且不确定的先验知识、异构且嘈杂的数据、部分可观察性以及复杂的高维网络。在这篇立场论文中，我们认为这些挑战不应被视为PIML的障碍，而应是其进化的催化剂。我们提出了生物学信息机器学习(BIML)：PIML的原则性扩展，保留了其结构基础，同时适应生物学的实际现实。BIML不是取代PIML，而是重新调整其方法，使其能够在更软性、概率形式的先验知识下运行。我们概述了四个基础支柱作为这一转变的路线图：不确定性量化、上下文化、受限潜在结构推断和可扩展性。基础模型和大语言模型将成为关键推动因素，将人类专业知识与计算建模结合起来。最后，我们提出具体建议，以构建BIML生态系统，并将PIML启发的创新引向具有高度科学和社会相关性的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physics-Informed Machine Learning (PIML) has successfully integratedmechanistic understanding into machine learning, particularly in domainsgoverned by well-known physical laws. This success has motivated efforts toapply PIML to biology, a field rich in dynamical systems but shaped bydifferent constraints. Biological modeling, however, presents uniquechallenges: multi-faceted and uncertain prior knowledge, heterogeneous andnoisy data, partial observability, and complex, high-dimensional networks. Inthis position paper, we argue that these challenges should not be seen asobstacles to PIML, but as catalysts for its evolution. We proposeBiology-Informed Machine Learning (BIML): a principled extension of PIML thatretains its structural grounding while adapting to the practical realities ofbiology. Rather than replacing PIML, BIML retools its methods to operate undersofter, probabilistic forms of prior knowledge. We outline four foundationalpillars as a roadmap for this transition: uncertainty quantification,contextualization, constrained latent structure inference, and scalability.Foundation Models and Large Language Models will be key enablers, bridginghuman expertise with computational modeling. We conclude with concreterecommendations to build the BIML ecosystem and channel PIML-inspiredinnovation toward challenges of high scientific and societal relevance.</description>
      <author>example@mail.com (Julien Martinelli)</author>
      <guid isPermaLink="false">2510.25368v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine Learning Framework</title>
      <link>http://arxiv.org/abs/2510.25347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 Figures, MICCAI AMAI 2025 workshop, to be published in  Volume 16206 of the Lecture Notes in Computer Science series&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于放射组学的流程，利用伪标记生成训练标签，解决了非对比冠状动脉计算机断层血管造影(CCTA)扫描中冠状动脉钙化(CAC)评分标记数据有限的问题。&lt;h4&gt;背景&lt;/h4&gt;冠状动脉钙化(CAC)评分在冠状动脉疾病(CAD)的早期检测和风险分层中起着关键作用。非对比冠状动脉计算机断层血管造影(CCTA)扫描在临床中常用于早期钙化检测。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于放射组学的流程，利用伪标记生成训练标签，避免需要专家定义的分割，并探索预训练基础模型在特征提取中的应用。&lt;h4&gt;方法&lt;/h4&gt;提出基于放射组学的流程，利用伪标记生成训练标签；探索使用预训练基础模型(CT-FM和RadImageNet)提取图像特征并与传统分类器结合；比较深度学习特征与放射组学特征性能；在包含182名患者的临床CCTA数据集上评估，将患者分为零钙化评分组和非零钙化评分组；研究在非对比数据集与对比+非对比数据集上训练的影响。&lt;h4&gt;主要发现&lt;/h4&gt;基于放射组学的模型显著优于来自基础模型的CNN嵌入，达到84%的准确率(p&lt;0.05)，尽管没有专家标注可用。&lt;h4&gt;结论&lt;/h4&gt;基于放射组学的方法在冠状动脉钙化检测中表现出色，即使在没有专家标注的情况下也能达到高准确率。&lt;h4&gt;翻译&lt;/h4&gt;冠状动脉钙化(CAC)评分在冠状动脉疾病(CAD)的早期检测和风险分层中起着关键作用。在本研究中，我们关注非对比冠状动脉计算机断层血管造影(CCTA)扫描，这些扫描在临床中常用于早期钙化检测。为解决标记数据有限这一挑战，我们提出了一种基于放射组学的流程，利用伪标记生成训练标签，从而消除对专家定义分割的需求。此外，我们探索了使用预训练基础模型(特别是CT-FM和RadImageNet)提取图像特征，然后与传统分类器一起使用。我们将这些深度学习特征与放射组学特征的性能进行了比较。评估在包含182名患者的临床CCTA数据集上进行，个体被分为两组：零钙化评分组与非零钙化评分组。我们进一步研究了在非对比数据集与对比+非对比数据集上训练的影响，测试仅在非对比扫描上进行。结果表明，尽管没有专家标注可用，但基于放射组学的模型显著优于来自基础模型的CNN嵌入(达到84%的准确率和p&lt;0.05)。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Coronary artery calcium (CAC) scoring plays a crucial role in the earlydetection and risk stratification of coronary artery disease (CAD). In thisstudy, we focus on non-contrast coronary computed tomography angiography (CCTA)scans, which are commonly used for early calcification detection in clinicalsettings. To address the challenge of limited annotated data, we propose aradiomics-based pipeline that leverages pseudo-labeling to generate traininglabels, thereby eliminating the need for expert-defined segmentations.Additionally, we explore the use of pretrained foundation models, specificallyCT-FM and RadImageNet, to extract image features, which are then used withtraditional classifiers. We compare the performance of these deep learningfeatures with that of radiomics features. Evaluation is conducted on a clinicalCCTA dataset comprising 182 patients, where individuals are classified into twogroups: zero versus non-zero calcium scores. We further investigate the impactof training on non-contrast datasets versus combined contrast and non-contrastdatasets, with testing performed only on non contrast scans. Results show thatradiomics-based models significantly outperform CNN-derived embeddings fromfoundation models (achieving 84% accuracy and p&lt;0.05), despite theunavailability of expert annotations.</description>
      <author>example@mail.com (Ayman Abaid, Gianpiero Guidone, Sara Alsubai, Foziyah Alquahtani, Talha Iqbal, Ruth Sharif, Hesham Elzomor, Emiliano Bianchini, Naeif Almagal, Michael G. Madden, Faisal Sharif, Ihsan Ullah)</author>
      <guid isPermaLink="false">2510.25347v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.25320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了基于图的代理规划(GAP)框架，通过建模任务间依赖关系实现工具的并行和顺序执行，解决了现有顺序推理范式的效率问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型驱动的自主代理在工具操作方面显示出强大能力，但现有范式(如ReAct)依赖顺序推理和执行，无法利用独立子任务之间的内在并行性。&lt;h4&gt;目的&lt;/h4&gt;解决顺序推理瓶颈导致的工具利用效率低下和多步推理场景中表现不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;提出基于图的代理规划(GAP)框架，训练基础模型将复杂任务分解为依赖感知的子任务图，自主确定工具的并行或顺序执行方式；采用两阶段训练策略：监督微调(SFT)和强化学习(RL)。&lt;h4&gt;主要发现&lt;/h4&gt;GAP在MHQA数据集上显著优于传统ReAct基线，特别是在多步检索任务上，同时通过智能并行化实现了工具调用效率的显著提升。&lt;h4&gt;结论&lt;/h4&gt;依赖感知的任务编排在执行效率和任务准确性方面都取得了实质性改进。&lt;h4&gt;翻译&lt;/h4&gt;由大型语言模型(LLM)驱动的自主代理在工具操作方面展现出解决复杂任务的强大能力。然而，现有的ReAct等范式依赖顺序推理和执行，无法利用独立子任务之间的内在并行性。这种顺序瓶颈导致工具利用效率低下，以及在多步推理场景中表现不佳。我们引入了基于图的代理规划(GAP)，这是一个新框架，通过基于图的规划明确建模任务间依赖关系，实现自适应并行和顺序工具执行。我们的方法训练基础模型将复杂任务分解为依赖感知的子任务图，自主确定哪些工具可以并行执行，哪些必须遵循顺序依赖。这种依赖感知的编排在执行效率和任务准确性方面都取得了实质性改进。为了训练GAP，我们从多跳问答(MHQA)基准构建了高质量的基于图的规划轨迹数据集。我们采用两阶段训练策略：首先在整理的数据集上进行监督微调(SFT)，然后在基于正确性奖励函数的强化学习(RL)阶段，在战略采样的查询上进行训练。在MHQA数据集上的实验结果表明，GAP显著优于传统的ReAct基线，特别是在多步检索任务上，同时通过智能并行化实现了工具调用效率的显著提升。项目页面可在以下网址访问：https://github.com/WJQ7777/Graph-Agent-Planning。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous agents powered by large language models (LLMs) have shownimpressive capabilities in tool manipulation for complex task-solving. However,existing paradigms such as ReAct rely on sequential reasoning and execution,failing to exploit the inherent parallelism among independent sub-tasks. Thissequential bottleneck leads to inefficient tool utilization and suboptimalperformance in multi-step reasoning scenarios. We introduce Graph-based AgentPlanning (GAP), a novel framework that explicitly models inter-taskdependencies through graph-based planning to enable adaptive parallel andserial tool execution. Our approach trains agent foundation models to decomposecomplex tasks into dependency-aware sub-task graphs, autonomously determiningwhich tools can be executed in parallel and which must follow sequentialdependencies. This dependency-aware orchestration achieves substantialimprovements in both execution efficiency and task accuracy. To train GAP, weconstruct a high-quality dataset of graph-based planning traces derived fromthe Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stagetraining strategy: supervised fine-tuning (SFT) on the curated dataset,followed by reinforcement learning (RL) with a correctness-based rewardfunction on strategically sampled queries where tool-based reasoning providesmaximum value. Experimental results on MHQA datasets demonstrate that GAPsignificantly outperforms traditional ReAct baselines, particularly onmulti-step retrieval tasks, while achieving dramatic improvements in toolinvocation efficiency through intelligent parallelization. The project page isavailable at: https://github.com/WJQ7777/Graph-Agent-Planning.</description>
      <author>example@mail.com (Jiaqi Wu, Qinlao Zhao, Zefeng Chen, Kai Qin, Yifei Zhao, Xueqian Wang, Yuhang Yao)</author>
      <guid isPermaLink="false">2510.25320v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2510.25257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种成本效益高且适应性强的蒸馏框架，利用视觉基础模型(VFMs)增强轻量级目标检测器，解决了高速度推理与特征表示能力之间的矛盾。&lt;h4&gt;背景&lt;/h4&gt;实时目标检测通过精心设计的架构和优化策略取得了显著进展，但轻量级网络设计追求高速推理往往导致特征表示能力下降，阻碍了性能提升和实际设备部署。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用视觉基础模型(VFMs)能力增强轻量级目标检测器的成本效益高且适应性强的蒸馏框架，解决VFM与资源受限检测器之间架构和学习目标差异导致的语义传输挑战。&lt;h4&gt;方法&lt;/h4&gt;引入深度语义注入器(DSI)模块促进VFM高级表示与检测器深层集成；设计基于梯度的自适应调制(GAM)策略，根据梯度范数比率动态调整语义传输强度。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在不增加部署和推理开销的情况下，为各种基于DETR的模型带来显著且一致的性能提升；新模型RT-DETRv4在COCO上取得最先进结果，在273/169/124/78 FPS速度下分别达到49.7/53.5/55.4/57.0的AP分数。&lt;h4&gt;结论&lt;/h4&gt;该方法强调了其在实时检测中的实际效用，为实时目标检测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;实时目标检测通过精心设计的架构和优化策略取得了实质性进展。然而，通过轻量级网络设计追求高速推理通常会导致特征表示能力下降，这阻碍了性能的进一步改进和实际设备部署。在本文中，我们提出了一种具有成本效益且高度适应性的蒸馏框架，利用视觉基础模型(VFMs)的快速发展能力来增强轻量级目标检测器。鉴于VFM与资源受限检测器之间存在显著的架构和学习目标差异，实现稳定且任务对齐的语义传输具有挑战性。为此，一方面，我们引入了深度语义注入器(DSI)模块，促进VFM的高级表示与检测器深层层的集成；另一方面，我们设计了基于梯度的自适应调制(GAM)策略，根据梯度范数比率动态调整语义传输强度。在不增加部署和推理开销的情况下，我们的方法在各种基于DETR的模型上轻松实现了显著且一致的性能提升，凸显了其在实时检测中的实际效用。我们的新模型系列RT-DETRv4在COCO上取得了最先进的结果，在相应速度为273/169/124/78 FPS时分别达到49.7/53.5/55.4/57.0的AP分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time object detection has achieved substantial progress throughmeticulously designed architectures and optimization strategies. However, thepursuit of high-speed inference via lightweight network designs often leads todegraded feature representation, which hinders further performance improvementsand practical on-device deployment. In this paper, we propose a cost-effectiveand highly adaptable distillation framework that harnesses the rapidly evolvingcapabilities of Vision Foundation Models (VFMs) to enhance lightweight objectdetectors. Given the significant architectural and learning objectivedisparities between VFMs and resource-constrained detectors, achieving stableand task-aligned semantic transfer is challenging. To address this, on onehand, we introduce a Deep Semantic Injector (DSI) module that facilitates theintegration of high-level representations from VFMs into the deep layers of thedetector. On the other hand, we devise a Gradient-guided Adaptive Modulation(GAM) strategy, which dynamically adjusts the intensity of semantic transferbased on gradient norm ratios. Without increasing deployment and inferenceoverhead, our approach painlessly delivers striking and consistent performancegains across diverse DETR-based models, underscoring its practical utility forreal-time detection. Our new model family, RT-DETRv4, achieves state-of-the-artresults on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at correspondingspeeds of 273/169/124/78 FPS.</description>
      <author>example@mail.com (Zijun Liao, Yian Zhao, Xin Shan, Yu Yan, Chang Liu, Lei Lu, Xiangyang Ji, Jie Chen)</author>
      <guid isPermaLink="false">2510.25257v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Test-Time Adaptive Object Detection with Foundation Model</title>
      <link>http://arxiv.org/abs/2510.25175v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于基础模型的测试时自适应目标检测方法，消除了对源数据的完全需求，克服了传统封闭集限制，实现了高效的跨域和跨类别适应。&lt;h4&gt;背景&lt;/h4&gt;测试时自适应目标检测近年来受到越来越多的关注，因为它在在线领域适应方面具有独特优势，更接近实际应用场景。然而，现有方法严重依赖于源域统计特征，并假设源域和目标域共享相同的类别空间。&lt;h4&gt;目的&lt;/h4&gt;提出第一个基于基础模型的测试时自适应目标检测方法，消除对源数据的完全需求，克服传统封闭集限制，实现任意跨域和跨类别的目标数据适应。&lt;h4&gt;方法&lt;/h4&gt;设计了一个多模态提示的Mean-Teacher框架，结合文本和视觉提示调整，以参数高效的方式适应语言和视觉表示空间；提出了针对视觉提示的测试时预热策略；维护实例动态内存模块存储高质量伪标签；并提出了内存增强和内存幻觉两种新策略。&lt;h4&gt;主要发现&lt;/h4&gt;在跨损坏和跨数据集基准上的广泛实验表明，该方法持续优于之前的最先进方法，能够适应任意跨域和跨类别的目标数据。&lt;h4&gt;结论&lt;/h4&gt;基于基础模型的方法在测试时自适应目标检测方面表现出色，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;近年来，测试时自适应目标检测因其在线领域适应中的独特优势而受到越来越多的关注，这更接近实际应用场景。然而，现有方法严重依赖于源域统计特征，同时做出源域和目标域共享相同类别空间的强假设。本文提出了第一个基于基础模型的测试时自适应目标检测方法，完全消除了对源数据的需求并克服了传统封闭集限制。具体而言，我们设计了一个多模态提示的Mean-Teacher框架，用于视觉-语言检测器驱动的测试时适应，结合文本和视觉提示调整，以参数高效的方式在测试数据上适应语言和视觉表示空间。相应地，我们提出了针对视觉提示定制的测试时预热策略，以有效保持视觉分支的表示能力。此外，为保证每个测试批次的高质量伪标签，我们维护了一个存储来自先前测试样本的高质量伪标签的实例动态内存模块，并提出了两种新策略——内存增强和内存幻觉，分别利用IDM的高质量实例来增强原始预测和对没有可用伪标签的图像进行幻觉处理。在跨损坏和跨数据集基准上的广泛实验表明，我们的方法持续优于之前的最先进方法，并能适应任意跨域和跨类别的目标数据。代码可在https://github.com/gaoyingjay/ttaod_foundation获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, test-time adaptive object detection has attracted increasingattention due to its unique advantages in online domain adaptation, whichaligns more closely with real-world application scenarios. However, existingapproaches heavily rely on source-derived statistical characteristics whilemaking the strong assumption that the source and target domains share anidentical category space. In this paper, we propose the first foundationmodel-powered test-time adaptive object detection method that eliminates theneed for source data entirely and overcomes traditional closed-set limitations.Specifically, we design a Multi-modal Prompt-based Mean-Teacher framework forvision-language detector-driven test-time adaptation, which incorporates textand visual prompt tuning to adapt both language and vision representationspaces on the test data in a parameter-efficient manner. Correspondingly, wepropose a Test-time Warm-start strategy tailored for the visual prompts toeffectively preserve the representation capability of the vision branch.Furthermore, to guarantee high-quality pseudo-labels in every test batch, wemaintain an Instance Dynamic Memory (IDM) module that stores high-qualitypseudo-labels from previous test samples, and propose two novelstrategies-Memory Enhancement and Memory Hallucination-to leverage IDM'shigh-quality instances for enhancing original predictions and hallucinatingimages without available pseudo-labels, respectively. Extensive experiments oncross-corruption and cross-dataset benchmarks demonstrate that our methodconsistently outperforms previous state-of-the-art methods, and can adapt toarbitrary cross-domain and cross-category target data. Code is available athttps://github.com/gaoyingjay/ttaod_foundation.</description>
      <author>example@mail.com (Yingjie Gao, Yanan Zhang, Zhi Cai, Di Huang)</author>
      <guid isPermaLink="false">2510.25175v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>TabMGP: Martingale Posterior with TabPFN</title>
      <link>http://arxiv.org/abs/2510.25154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages (+3 reference, +22 appendix). Extra plots in  https://drive.google.com/drive/folders/1ct_effOoTEGpiWUf0_1xI3VqLWHtJY16&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于基础转换器的鞅后验方法（TabMGP），用于解决贝叶斯推断中的不确定性量化问题。&lt;h4&gt;背景&lt;/h4&gt;贝叶斯推断在不确定性量化方面具有优势，但面临先验设定、似然误设和计算负担等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的预测规则，用于构建高质量的鞅后验方法，提高不确定性量化的准确性。&lt;h4&gt;方法&lt;/h4&gt;利用基础转换器（特别是TabPFN，一种表格数据领域的最先进模型）构建鞅后验（TabMGP），通过自回归生成模拟前向数据生成过程。&lt;h4&gt;主要发现&lt;/h4&gt;TabMGP产生的可信集具有接近名义覆盖范围的性能，并且通常优于现有的MGP结构和标准贝叶斯方法。&lt;h4&gt;结论&lt;/h4&gt;基于基础转换器的鞅后验方法（TabMGP）在表格数据的不确定性量化方面表现优异，为贝叶斯推断提供了有效替代方案。&lt;h4&gt;翻译&lt;/h4&gt;贝叶斯推断提供了有原则的不确定性量化，但常常受到先验设定、似然误设和计算负担的限制。鞅后验（MGP，Fong等人，2023年）提供了一种替代方案，用预测规则（即一步前向预测分布序列）替代先验-似然设定，用于前向数据生成。MGP的有效性取决于预测规则的选择，但文献中很少有令人信服的例子。基础转换器在这里非常适合，因为它们的自回归生成模拟了这种前向模拟，并且它们的通用设计能够实现丰富的预测建模。我们介绍了TabMGP，这是一种基于TabPFN构建的MGP，TabPFN是表格数据当前最先进的基础模型。TabMGP产生的可信集具有接近名义覆盖范围的性能，并且通常优于现有的MGP结构和标准贝叶斯方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bayesian inference provides principled uncertainty quantification but isoften limited by challenges of prior elicitation, likelihood misspecification,and computational burden. The martingale posterior (MGP, Fong et al., 2023)offers an alternative, replacing prior-likelihood elicitation with a predictiverule - namely, a sequence of one-step-ahead predictive distributions - forforward data generation. The utility of MGPs depends on the choice ofpredictive rule, yet the literature has offered few compelling examples.Foundation transformers are well-suited here, as their autoregressivegeneration mirrors this forward simulation and their general-purpose designenables rich predictive modeling. We introduce TabMGP, an MGP built on TabPFN,a transformer foundation model that is currently state-of-the-art for tabulardata. TabMGP produces credible sets with near-nominal coverage and oftenoutperforms both existing MGP constructions and standard Bayes.</description>
      <author>example@mail.com (Kenyon Ng, Edwin Fong, David T. Frazier, Jeremias Knoblauch, Susan Wei)</author>
      <guid isPermaLink="false">2510.25154v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>POWSM: A Phonetic Open Whisper-Style Speech Foundation Model</title>
      <link>http://arxiv.org/abs/2510.24992v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;POWSM是一个统一的语音处理框架，能够同时执行多个音位相关任务，性能与专业模型相当，且支持多种转换功能。&lt;h4&gt;背景&lt;/h4&gt;语音处理领域在自动语音识别(ASR)、音位识别(PR)、字形到音位转换(G2P)和音位到字形转换(P2G)等音位任务上取得了显著进展，但这些任务大多是孤立研究的，每个任务都依赖于特定的架构和数据集。&lt;h4&gt;目的&lt;/h4&gt;引入POWSM（Phonetic Open Whisper-style Speech Model），创建第一个能够同时执行多个与音位相关任务的统一框架。&lt;h4&gt;方法&lt;/h4&gt;POWSM模型实现了音频、文本（字形）和音位之间的无缝转换，为通用和低资源语音处理开辟了新的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;POWSM在性能上与类似大小的专业PR模型（Wav2Vec2Phoneme和ZIPA）相当或更优，同时支持G2P、P2G和ASR。&lt;h4&gt;结论&lt;/h4&gt;训练数据、代码和模型已公开发布，以促进开放科学。&lt;h4&gt;翻译&lt;/h4&gt;最近语音处理方面的进展已在语音识别、音位识别、字形到音位转换和音位到字形转换等音位任务中取得实质性进展。尽管这些任务在概念上相似，但它们大多被孤立研究，每个任务都依赖于特定的架构和数据集。在本文中，我们引入了POWSM（Phonetic Open Whisper-style Speech Model），这是第一个能够同时执行多个音位相关任务的统一框架。POWSM实现了音频、文本（字形）和音位之间的无缝转换，为通用和低资源语音处理开辟了新的可能性。我们的模型在性能上与类似大小的专业PR模型（Wav2Vec2Phoneme和ZIPA）相当或更优，同时支持G2P、P2G和ASR。我们的训练数据、代码和模型已公开发布，以促进开放科学。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in spoken language processing have led to substantialprogress in phonetic tasks such as automatic speech recognition (ASR), phonerecognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-graphemeconversion (P2G). Despite their conceptual similarity, these tasks have largelybeen studied in isolation, each relying on task-specific architectures anddatasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style SpeechModel), the first unified framework capable of jointly performing multiplephone-related tasks. POWSM enables seamless conversion between audio, text(graphemes), and phones, opening up new possibilities for universal andlow-resource speech processing. Our model outperforms or matches specialized PRmodels of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P,P2G, and ASR. Our training data, code and models are released to foster openscience.</description>
      <author>example@mail.com (Chin-Jou Li, Kalvin Chang, Shikhar Bharadwaj, Eunjung Yeo, Kwanghee Choi, Jian Zhu, David Mortensen, Shinji Watanabe)</author>
      <guid isPermaLink="false">2510.24992v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Pearl: A Foundation Model for Placing Every Atom in the Right Location</title>
      <link>http://arxiv.org/abs/2510.24670v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Pearl是一个用于蛋白质-配体共同折叠的基础模型，通过创新的训练方法、架构设计和推理控制，显著提高了蛋白质-配体复合物结构预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确预测蛋白质-配体复合物的三维结构是计算药物发现中的基本挑战，限制了治疗设计的速度和成功。深度学习方法虽有潜力，但受限于实验数据稀缺、架构效率低下、物理无效构象以及辅助信息利用能力有限等因素。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够克服当前蛋白质-配体结构预测局限性的基础模型，提高预测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;Pearl通过三个关键创新解决挑战：(1)包含大规模合成数据的训练配方，克服数据稀缺；(2)融合SO(3)-等变扩散模块的架构，尊重3D旋转对称性，提高泛化能力；(3)可控推理系统，支持蛋白质和非聚合物组分以及无条件/条件模式。&lt;h4&gt;主要发现&lt;/h4&gt;Pearl在蛋白质-配体共同折叠方面建立了新性能标准，在公共基准测试中超越AlphaFold3和其他开源模型14%以上；在口袋条件共同折叠模式下，对真实世界药物目标实现了3.6倍的改进；模型性能与训练中使用的合成数据集大小直接相关。&lt;h4&gt;结论&lt;/h4&gt;Pearl通过创新的训练方法、架构设计和推理控制，显著提高了蛋白质-配体复合物结构预测的准确性，为药物设计提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;准确预测蛋白质-配体复合物的三维结构仍然是计算药物发现中的一个基本挑战，它限制了治疗设计的速度和成功率。深度学习方法最近作为结构预测工具显示出强大的潜力，在各种生物分子系统中取得了有希望的准确性。然而，它们的性能和实用性受到实验数据稀缺、架构效率低下、物理无效构象以及在推理过程中利用辅助信息能力有限等因素的制约。为解决这些问题，我们引入了Pearl（Placing Every Atom in the Right Location），一个用于大规模蛋白质-配体共同折叠的基础模型。Pearl通过三个关键创新来解决这些挑战：(1)包含大规模合成数据的训练配方，以克服数据稀缺；(2)融合SO(3)-等变扩散模块的架构， inherently尊重3D旋转对称性，提高泛化能力和样本效率；(3)可控推理，包括支持蛋白质和非聚合物组分以及无条件/条件模式的双链通用模板系统。Pearl在蛋白质-配体共同折叠方面建立了新的最先进性能。在生成准确（RMSD &lt; 2Å）和物理有效构象的关键指标上，Pearl在公共Runs N' Poses和PoseBusters基准测试中超越了AlphaFold3和其他开源基线，比次优模型分别提高了14.5%和14.2%。在口袋条件共同折叠模式下，Pearl在一个具有挑战性的真实世界药物目标专有集上，在更严格的RMSD &lt; 1Å阈值下实现了3.6倍的改进。最后，我们证明了模型性能与训练中使用的合成数据集大小直接相关。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting the three-dimensional structures of protein-ligandcomplexes remains a fundamental challenge in computational drug discovery thatlimits the pace and success of therapeutic design. Deep learning methods haverecently shown strong potential as structural prediction tools, achievingpromising accuracy across diverse biomolecular systems. However, theirperformance and utility are constrained by scarce experimental data,inefficient architectures, physically invalid poses, and the limited ability toexploit auxiliary information available at inference. To address these issues,we introduce Pearl (Placing Every Atom in the Right Location), a foundationmodel for protein-ligand cofolding at scale. Pearl addresses these challengeswith three key innovations: (1) training recipes that include large-scalesynthetic data to overcome data scarcity; (2) architectures that incorporate anSO(3)-equivariant diffusion module to inherently respect 3D rotationalsymmetries, improving generalization and sample efficiency, and (3)controllable inference, including a generalized multi-chain templating systemsupporting both protein and non-polymeric components as well as dualunconditional/conditional modes. Pearl establishes a new state-of-the-artperformance in protein-ligand cofolding. On the key metric of generatingaccurate (RMSD &lt; 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold3 and other open source baselines on the public Runs N' Poses and PoseBustersbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over thenext best model. In the pocket-conditional cofolding regime, Pearl delivers$3.6\times$ improvement on a proprietary set of challenging, real-world drugtargets at the more rigorous RMSD &lt; 1 \r{A} threshold. Finally, we demonstratethat model performance correlates directly with synthetic dataset size used intraining.</description>
      <author>example@mail.com (Genesis Research Team, Alejandro Dobles, Nina Jovic, Kenneth Leidal, Pranav Murugan, David C. Williams, Drausin Wulsin, Nate Gruver, Christina X. Ji, Korrawat Pruegsanusak, Gianluca Scarpellini, Ansh Sharma, Wojciech Swiderski, Andrea Bootsma, Richard Strong Bowen, Charlotte Chen, Jamin Chen, Marc André Dämgen, Benjamin DiFrancesco, J. D. Fishman, Alla Ivanova, Zach Kagin, David Li-Bland, Zuli Liu, Igor Morozov, Jeffrey Ouyang-Zhang, Frank C. Pickard IV, Kushal S. Shah, Ben Shor, Gabriel Monteiro da Silva, Roy Tal, Maxx Tessmer, Carl Tilbury, Cyr Vetcher, Daniel Zeng, Maruan Al-Shedivat, Aleksandra Faust, Evan N. Feinberg, Michael V. LeVine, Matteus Pan)</author>
      <guid isPermaLink="false">2510.24670v2</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering</title>
      <link>http://arxiv.org/abs/2510.24799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 5 figures, submitted to ACM Transactions on Software  Engineering and Methodology&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Compiler.next，一种基于搜索的新型编译器，旨在解决AI辅助软件工程中的认知过载、工具集成效率低和AI副驾驶功能有限等问题，实现AI原生软件系统的无缝演进。&lt;h4&gt;背景&lt;/h4&gt;AI辅助软件工程快速发展，但现有工具和范式受认知过载、工具集成效率低下和AI副驾驶功能有限等因素的限制。&lt;h4&gt;目的&lt;/h4&gt;提出Compiler.next作为软件工程3.0时代的一部分，实现AI原生软件系统的无缝演进，降低非专家技术门槛，实现可扩展、适应性强和可靠的AI驱动软件。&lt;h4&gt;方法&lt;/h4&gt;Compiler.next接受人类编写的意图，通过搜索最优解决方案自动生成工作软件，涉及认知架构及其组成部分的动态优化，在准确性、成本和延迟等多个目标间找到最佳平衡。&lt;h4&gt;主要发现&lt;/h4&gt;Compiler.next的架构设计使其能够作为降低非专家技术门槛、实现可扩展、适应性强和可靠的AI驱动软件的基石。&lt;h4&gt;结论&lt;/h4&gt;Compiler.next为完全自动化、搜索驱动的软件开发奠定了基础，促进了更快创新和更高效的AI驱动系统，解决了意图编译的核心挑战。&lt;h4&gt;翻译&lt;/h4&gt;AI辅助软件工程的快速发展为软件工程领域带来了变革潜力，但现有工具和范式仍然受到认知过载、工具集成效率低下以及AI副驾驶功能有限等因素的限制。为此，我们提出了Compiler.next，一种基于搜索的新型编译器，作为新兴软件工程3.0时代的一部分，旨在实现AI原生软件系统的无缝演进。与传统的静态编译器不同，Compiler.next接受人类编写的意图，并通过搜索最优解决方案来自动生成工作软件。这一过程涉及认知架构及其组成部分（如提示、基础模型配置和系统参数）的动态优化，同时找到准确性、成本和延迟等多个目标之间的最佳权衡。本文概述了Compiler.next的架构，并将其定位为降低非专家技术门槛、实现可扩展、适应性强和可靠的AI驱动软件的基石。我们提出了一个路线图来解决意图编译中的核心挑战，包括开发高质量编程构造、有效搜索启发式方法、可复现性以及编译器之间的互操作性。我们的愿景为完全自动化、搜索驱动的软件开发奠定了基础，促进了更快创新和更高效的AI驱动系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement of AI-assisted software engineering has broughttransformative potential to the field of software engineering, but existingtools and paradigms remain limited by cognitive overload, inefficient toolintegration, and the narrow capabilities of AI copilots. In response, wepropose Compiler.next, a novel search-based compiler designed to enable theseamless evolution of AI-native software systems as part of the emergingSoftware Engineering 3.0 era. Unlike traditional static compilers,Compiler.next takes human-written intents and automatically generates workingsoftware by searching for an optimal solution. This process involves dynamicoptimization of cognitive architectures and their constituents (e.g., prompts,foundation model configurations, and system parameters) while finding theoptimal trade-off between several objectives, such as accuracy, cost, andlatency. This paper outlines the architecture of Compiler.next and positions itas a cornerstone in democratizing software development by lowering thetechnical barrier for non-experts, enabling scalable, adaptable, and reliableAI-powered software. We present a roadmap to address the core challenges inintent compilation, including developing quality programming constructs,effective search heuristics, reproducibility, and interoperability betweencompilers. Our vision lays the groundwork for fully automated, search-drivensoftware development, fostering faster innovation and more efficient AI-drivensystems.</description>
      <author>example@mail.com (Filipe R. Cogo, Gustavo A. Oliva, Ahmed E. Hassan)</author>
      <guid isPermaLink="false">2510.24799v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Why Foundation Models in Pathology Are Failing</title>
      <link>http://arxiv.org/abs/2510.23807v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基础模型在计算病理学应用中的不足，指出其存在诊断准确率低、鲁棒性差等问题，并分析了这些问题背后的七个相互关联原因，认为当前病理学基础模型在概念上与组织形态学本质不匹配，需要范式重构。&lt;h4&gt;背景&lt;/h4&gt;在非医疗领域，基础模型通过大规模自监督和多模态学习彻底改变了计算机视觉和语言处理，人们预期其在计算病理学领域也能取得类似突破。&lt;h4&gt;目的&lt;/h4&gt;检查基础模型在计算病理学应用中的不足，分析这些缺点背后的根本原因。&lt;h4&gt;方法&lt;/h4&gt;通过系统评估方法检查基础模型的缺点，并分析这些缺点背后的原因。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型存在诊断准确率低、鲁棒性差、几何不稳定性、计算需求量大以及安全漏洞等问题；这些问题源于七个相互关联的原因：生物复杂性、无效的自监督、过度泛化、过度的架构复杂性、缺乏领域特定创新、数据不足以及与组织块大小相关的基本设计缺陷。&lt;h4&gt;结论&lt;/h4&gt;当前病理学基础模型在概念上与组织形态学本质不匹配，需要对范式本身进行根本性的重新思考。&lt;h4&gt;翻译&lt;/h4&gt;在非医疗领域，基础模型通过大规模自监督和多模态学习彻底改变了计算机视觉和语言处理。因此，计算病理学领域对这类模型的快速应用预期能在癌症诊断、预后判断和多模态检索方面带来类似的突破。然而，最近的系统评估揭示了根本性弱点：低诊断准确率、差鲁棒性、几何不稳定性、高计算需求以及令人担忧的安全漏洞。这篇短文检查了这些不足，并认为它们源于主流人工智能中通用基础建模的基本假设与人体组织内在复杂性之间的更深层次概念不匹配。确定了七个相互关联的原因：生物复杂性、无效的自监督、过度泛化、过度的架构复杂性、缺乏领域特定创新、数据不足以及与组织块大小相关的基本设计缺陷。这些发现表明，当前的病理学基础模型在概念上仍与组织形态学的本质不匹配，需要对范式本身进行根本性的重新思考。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In non-medical domains, foundation models (FMs) have revolutionized computervision and language processing through large-scale self-supervised andmultimodal learning. Consequently, their rapid adoption in computationalpathology was expected to deliver comparable breakthroughs in cancer diagnosis,prognostication, and multimodal retrieval. However, recent systematicevaluations reveal fundamental weaknesses: low diagnostic accuracy, poorrobustness, geometric instability, heavy computational demands, and concerningsafety vulnerabilities. This short paper examines these shortcomings and arguesthat they stem from deeper conceptual mismatches between the assumptionsunderlying generic foundation modeling in mainstream AI and the intrinsiccomplexity of human tissue. Seven interrelated causes are identified:biological complexity, ineffective self-supervision, overgeneralization,excessive architectural complexity, lack of domain-specific innovation,insufficient data, and a fundamental design flaw related to tissue patch size.These findings suggest that current pathology foundation models remainconceptually misaligned with the nature of tissue morphology and call for afundamental rethinking of the paradigm itself.</description>
      <author>example@mail.com (Hamid R. Tizhoosh)</author>
      <guid isPermaLink="false">2510.23807v2</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans</title>
      <link>http://arxiv.org/abs/2510.14205v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出动态人格完善框架(DPRF)，用于提高大型语言模型角色扮演代理的行为与目标个体的一致性，通过迭代识别认知分歧并优化个人资料实现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型角色扮演代理旨在模拟个体人类行为，但手动创建的个人资料(如精心挑选的信息和个性特征)未经验证与目标个体的对齐度，导致人格保真度受损。&lt;h4&gt;目的&lt;/h4&gt;优化LLM RPAs的行为与目标个体行为的一致性，提高角色扮演的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;DPRF通过迭代识别生成行为与人类真实认知之间的分歧(无论是自由形式还是基于理论的结构化分析)，并完善个人资料以减轻这些分歧。&lt;h4&gt;主要发现&lt;/h4&gt;在五个大型语言模型和四种多样的行为预测场景(正式辩论、涉及心理健康问题的社交媒体帖子、公开采访和电影评论)中，DPRF能够显著提高行为一致性，并且跨模型和场景具有通用性。&lt;h4&gt;结论&lt;/h4&gt;该工作为创建高保真度个人资料和增强下游应用(如用户模拟、社会研究和个性化AI)的有效性提供了稳健的方法论。&lt;h4&gt;翻译&lt;/h4&gt;新兴的大型语言模型角色扮演代理旨在模拟个体人类行为，但人格保真度常因手动创建的个人资料(如精心挑选的信息和个性特征)未经验证与目标个体的对齐度而受损。为解决这一局限，我们的工作引入了动态人格完善框架(DPRF)。DPRF旨在通过迭代识别生成行为与人类真实认知之间的分歧(无论是自由形式还是基于理论的结构化分析)，并完善个人资料以减轻这些分歧，从而优化LLM RPAs的行为与目标个体行为的一致性。我们在五个大型语言模型和四种多样的行为预测场景(正式辩论、涉及心理健康问题的社交媒体帖子、公开采访和电影评论)中评估了DPRF。DPRF能够显著提高行为一致性，并且跨模型和场景具有通用性。我们的工作为创建高保真度个人资料和增强下游应用(如用户模拟、社会研究和个性化AI)的有效性提供了稳健的方法论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emerging large language model role-playing agents (LLM RPAs) aim tosimulate individual human behaviors, but the persona fidelity is oftenundermined by manually-created profiles (e.g., cherry-picked information andpersonality characteristics) without validating the alignment with the targetindividuals. To address this limitation, our work introduces the DynamicPersona Refinement Framework (DPRF). DPRF aims to optimize the alignment of LLMRPAs' behaviors with those of target individuals by iteratively identifying thecognitive divergence, either through free-form or theory-grounded, structuredanalysis, between generated behaviors and human ground truth, and refining thepersona profile to mitigate these divergences. We evaluate DPRF with five LLMson four diverse behavior-prediction scenarios: formal debates, social mediaposts with mental health issues, public interviews, and movie reviews. DPRF canconsistently improve behavioral alignment considerably over baseline personasand generalizes across models and scenarios. Our work provides a robustmethodology for creating high-fidelity persona profiles and enhancing thevalidity of downstream applications, such as user simulation, social studies,and personalized AI.</description>
      <author>example@mail.com (Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang)</author>
      <guid isPermaLink="false">2510.14205v3</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics</title>
      <link>http://arxiv.org/abs/2510.25683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为GNSS的图神经网络框架，用于动态结构问题的代理建模，通过三个关键特征解决了现有方法的局限性，在案例研究中表现出色，实现了比传统方法更快的推理速度。&lt;h4&gt;背景&lt;/h4&gt;图神经网络作为数值模拟的代理模型已在计算流体动力学领域有所研究，但在结构问题特别是动态情况中的应用相对较少，存在研究空白。&lt;h4&gt;目的&lt;/h4&gt;为了填补动态结构问题中图神经网络应用的空白，作者开发了GNSS框架，专门用于动态结构问题的代理建模。&lt;h4&gt;方法&lt;/h4&gt;GNSS遵循编码-处理-解码范式，具有三个关键特征：在节点固定的局部框架中表达节点运动学；采用符号感知回归损失减少相位误差；使用波长感知的连接半径优化图结构构建。&lt;h4&gt;主要发现&lt;/h4&gt;GNSS在50kHz汉宁调制脉冲激励梁的案例研究中，能够准确复现物理特性并泛化到未见过的加载条件，而现有GNN方法无法收敛。与有限元方法相比，GNSS实现了显著的推理加速同时保持空间和时间保真度。&lt;h4&gt;结论&lt;/h4&gt;具有物理一致性更新规则且保持局部性的图神经网络是动态、波主导结构模拟的有竞争力的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络最近被探索作为数值模拟的代理模型。虽然它们在计算流体动力学中的应用已被研究，但很少被应用于结构问题，特别是动态情况。为了解决这一研究空白，我们引入了基于图网络的结构模拟器，这是一个用于动态结构问题代理建模的图神经网络框架。GNSS遵循基于GNN的机器学习模型的典型编码-处理-解码范式，其设计使其特别适合动态模拟，这得益于三个关键特征：在节点固定的局部框架中表达节点运动学，避免有限差分速度中的灾难性取消；采用符号感知回归损失，减少长期rollout中的相位误差；使用波长感知的连接半径，优化图结构构建。我们在一个涉及由50kHz汉宁调制脉冲激励的梁的案例研究中评估了GNSS。结果表明GNSS能够在数百个时间步长内准确复现问题的物理特性，并能泛化到未见过的加载条件，而现有的GNN方法无法收敛或提供有意义的预测。与显式有限元基线方法相比，GNSS在保持空间和时间保真度的同时实现了显著的推理加速。这些发现表明，具有物理一致性更新规则且保持局部性的GNN是动态、波主导结构模拟的有竞争力的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have recently been explored as surrogate modelsfor numerical simulations. While their applications in computational fluiddynamics have been investigated, little attention has been given to structuralproblems, especially for dynamic cases. To address this gap, we introduce theGraph Network-based Structural Simulator (GNSS), a GNN framework for surrogatemodeling of dynamic structural problems.  GNSS follows the encode-process-decode paradigm typical of GNN-based machinelearning models, and its design makes it particularly suited for dynamicsimulations thanks to three key features: (i) expressing node kinematics innode-fixed local frames, which avoids catastrophic cancellation infinite-difference velocities; (ii) employing a sign-aware regression loss,which reduces phase errors in long rollouts; and (iii) using awavelength-informed connectivity radius, which optimizes graph construction.  We evaluate GNSS on a case study involving a beam excited by a 50kHzHanning-modulated pulse. The results show that GNSS accurately reproduces thephysics of the problem over hundreds of timesteps and generalizes to unseenloading conditions, where existing GNNs fail to converge or deliver meaningfulpredictions.  Compared with explicit finite element baselines, GNSS achieves substantialinference speedups while preserving spatial and temporal fidelity. Thesefindings demonstrate that locality-preserving GNNs with physics-consistentupdate rules are a competitive alternative for dynamic, wave-dominatedstructural simulations.</description>
      <author>example@mail.com (Alessandro Lucchetti, Francesco Cadini, Marco Giglio, Luca Lomazzi)</author>
      <guid isPermaLink="false">2510.25683v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Bridging the Divide: End-to-End Sequence-Graph Learning</title>
      <link>http://arxiv.org/abs/2510.25126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BRIDGE是一种统一的端到端架构，能够联合学习序列和图信息，在友谊预测和欺诈检测任务上表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;现实世界的数据集通常是序列性和关系性的，每个节点携带事件序列，边编码交互。现有方法往往只考虑一种模态而忽略另一种。&lt;h4&gt;目的&lt;/h4&gt;作者认为序列和图是同一数据集的互补方面，应该联合学习，而不是作为独立问题处理。&lt;h4&gt;方法&lt;/h4&gt;BRIDGE将序列编码器与图神经网络(GNN)耦合在单一目标下，允许梯度在两个模块间流动。添加了TOKENXATTN标记级交叉注意力层，实现邻居间细粒度的标记级消息传递。&lt;h4&gt;主要发现&lt;/h4&gt;在友谊预测(Brightkite)和欺诈检测(Amazon)两种场景下，BRIDGE在排序和分类指标上始终优于静态GNN、时图方法和仅基于序列的基线。&lt;h4&gt;结论&lt;/h4&gt;BRIDGE通过联合学习序列和图信息，能够学习任务对齐的表示，在各种任务上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;许多现实世界的数据集既是序列性的又是关系性的：每个节点携带事件序列，而边则编码交互。现有的序列建模和图建模方法通常忽略了一种或另一种模态。我们认为序列和图不是独立的问题，而是同一数据集的互补方面，应该联合学习。我们引入了BRIDGE，一个统一的端到端架构，将序列编码器与GNN在单一目标下耦合，允许梯度在两个模块间流动，并学习任务对齐的表示。为了实现邻居间细粒度的标记级消息传递，我们添加了TOKENXATTN，一个标记级交叉注意力层，用于在相邻序列的事件之间传递消息。在友谊预测(Brightkite)和欺诈检测(Amazon)两种设置下，BRIDGE在排序和分类指标上始终优于静态GNN、时图方法和仅基于序列的基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many real-world datasets are both sequential and relational: each nodecarries an event sequence while edges encode interactions. Existing methods insequence modeling and graph modeling often neglect one modality or the other.We argue that sequences and graphs are not separate problems but complementaryfacets of the same dataset, and should be learned jointly. We introduce BRIDGE,a unified end-to-end architecture that couples a sequence encoder with a GNNunder a single objective, allowing gradients to flow across both modules andlearning task-aligned representations. To enable fine-grained token-levelmessage passing among neighbors, we add TOKENXATTN, a token-levelcross-attention layer that passes messages between events in neighboringsequences. Across two settings, friendship prediction (Brightkite) and frauddetection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graphmethods, and sequence-only baselines on ranking and classification metrics.</description>
      <author>example@mail.com (Yuen Chen, Yulun Wu, Samuel Sharpe, Igor Melnyk, Nam H. Nguyen, Furong Huang, C. Bayan Bruss, Rizal Fathony)</author>
      <guid isPermaLink="false">2510.25126v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>The Underappreciated Power of Vision Models for Graph Structural Understanding</title>
      <link>http://arxiv.org/abs/2510.24788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了图神经网络与人类视觉感知的差异，发现视觉模型在图理解任务中具有与GNNs相当的性能但展现出不同的学习模式。作者提出了新的基准测试GraphAbstract，评估模型对全局图属性的理解能力，结果表明视觉模型在需要整体结构理解的任务上优于GNNs。&lt;h4&gt;背景&lt;/h4&gt;图神经网络通过自下而上的消息传递机制工作，这与人类视觉感知先捕捉全局结构的直觉方式有根本不同。现有基准测试将领域特征与拓扑理解混为一谈，无法有效评估模型对图全局结构的理解能力。&lt;h4&gt;目的&lt;/h4&gt;研究视觉模型在图理解方面的潜力，评估它们与GNNs在性能和学习模式上的差异，并开发新的基准测试来评估模型对全局图属性的理解能力。&lt;h4&gt;方法&lt;/h4&gt;提出了名为GraphAbstract的新基准测试，评估模型识别组织原型、检测对称性、感知连接强度和识别关键元素的能力，这些能力与人类对图的全局理解方式相似。&lt;h4&gt;主要发现&lt;/h4&gt;视觉模型在需要整体结构理解的任务上显著优于GNNs；视觉模型在不同图规模上保持泛化能力；GNNs在全局模式抽象方面存在困难，且随着图规模增大性能下降；视觉模型具有显著的但未被充分利用的图结构理解能力。&lt;h4&gt;结论&lt;/h4&gt;视觉模型在需要全局拓扑意识和尺度不变推理的问题上具有显著的能力，这些发现为开发更有效的图基础模型开辟了新途径，特别是对于那些由整体模式识别主导的任务。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络通过自下而上的消息传递运行，这与人类视觉感知有根本不同，人类视觉直觉上首先捕捉全局结构。我们研究了视觉模型在图理解方面的未被充分认识到的潜力，发现它们在既定基准上实现了与GNNs相当的性能，同时表现出明显不同的学习模式。这些不同的行为，加上现有基准将领域特征与拓扑理解混为一谈的限制，促使我们引入GraphAbstract。这个基准评估模型像人类一样感知全局图属性的能力：识别组织原型、检测对称性、感知连接强度和识别关键元素。我们的结果显示，在需要整体结构理解的任务上，视觉模型显著优于GNNs，并且在不同图规模上保持泛化能力，而GNNs在全局模式抽象方面存在困难，且随着图规模增大性能下降。这项工作表明，视觉模型具有显著的但未被充分利用的图结构理解能力，特别是对于需要全局拓扑意识和尺度不变推理的问题。这些发现为利用这种未被充分认识到的潜力开发更有效的图基础模型开辟了新途径，这些任务主要由整体模式识别主导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks operate through bottom-up message-passing,fundamentally differing from human visual perception, which intuitivelycaptures global structures first. We investigate the underappreciated potentialof vision models for graph understanding, finding they achieve performancecomparable to GNNs on established benchmarks while exhibiting distinctlydifferent learning patterns. These divergent behaviors, combined withlimitations of existing benchmarks that conflate domain features withtopological understanding, motivate our introduction of GraphAbstract. Thisbenchmark evaluates models' ability to perceive global graph properties ashumans do: recognizing organizational archetypes, detecting symmetry, sensingconnectivity strength, and identifying critical elements. Our results revealthat vision models significantly outperform GNNs on tasks requiring holisticstructural understanding and maintain generalizability across varying graphscales, while GNNs struggle with global pattern abstraction and degrade withincreasing graph size. This work demonstrates that vision models possessremarkable yet underutilized capabilities for graph structural understanding,particularly for problems requiring global topological awareness andscale-invariant reasoning. These findings open new avenues to leverage thisunderappreciated potential for developing more effective graph foundationmodels for tasks dominated by holistic pattern recognition.</description>
      <author>example@mail.com (Xinjian Zhao, Wei Pang, Zhongkai Xue, Xiangru Jian, Lei Zhang, Yaoyao Xu, Xiaozhuang Song, Shu Wu, Tianshu Yu)</author>
      <guid isPermaLink="false">2510.24788v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>FastJAM: a Fast Joint Alignment Model for Images</title>
      <link>http://arxiv.org/abs/2510.22842v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025. Pages 1-10 are the Main Paper. Pages 23-31  are Supplemental Material. FastJAM website -  https://bgu-cs-vil.github.io/FastJAM/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为FastJAM的快速图像联合对齐方法，显著降低计算复杂度，实现高质量对齐效果。&lt;h4&gt;背景&lt;/h4&gt;现有图像联合对齐方法通常需要长时间训练、大容量模型和大量超参数调整，计算效率低下。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、高效的图像联合对齐方法，减少计算时间并提高对齐质量。&lt;h4&gt;方法&lt;/h4&gt;FastJAM基于图方法，利用现成图像匹配器计算pairwise匹配，结合快速非参数聚类构建表示图像内和图像间关键点关系的图。通过图神经网络传播和聚合对应关系，利用图像级池化预测单应性参数。采用逆组合损失消除正则化项需求，避免超参数调整。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，FastJAM在多个基准测试上实现了优于现有现代JA方法的对齐质量，同时将计算时间从小时或分钟级减少到几秒钟。&lt;h4&gt;结论&lt;/h4&gt;FastJAM通过创新的图神经网络和逆组合损失方法，实现了快速、高效的图像联合对齐，为图像处理领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图像联合对齐（JA）旨在将一组图像对齐到统一的坐标系中，使语义相似的特征出现在对应的空间位置。大多数现有方法通常需要长时间训练、大容量模型和大量超参数调整。我们引入了FastJAM，一种快速的基于图的方法，显著降低了联合对齐任务的计算复杂度。FastJAM利用现成的图像匹配器计算的pairwise匹配，结合快速非参数聚类，构建表示图像内和图像间关键点关系的图。图神经网络传播和聚合这些对应关系，通过图像级池化有效预测每个图像的单应性参数。利用逆组合损失，消除了对预测变换的正则化项的需求（因此也避免了与这些项相关的超参数调整），FastJAM能够快速有效地执行图像JA。在几个基准测试上的实验结果表明，FastJAM在对齐质量方面优于现有的现代JA方法，同时将计算时间从小时或分钟减少到几秒钟。我们的代码可在项目网页获取：https://bgu-cs-vil.github.io/FastJAM/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Joint Alignment (JA) of images aims to align a collection of images into aunified coordinate frame, such that semantically-similar features appear atcorresponding spatial locations. Most existing approaches often require longtraining times, large-capacity models, and extensive hyperparameter tuning. Weintroduce FastJAM, a rapid, graph-based method that drastically reduces thecomputational complexity of joint alignment tasks. FastJAM leverages pairwisematches computed by an off-the-shelf image matcher, together with a rapidnonparametric clustering, to construct a graph representing intra- andinter-image keypoint relations. A graph neural network propagates andaggregates these correspondences, efficiently predicting per-image homographyparameters via image-level pooling. Utilizing an inverse-compositional loss,that eliminates the need for a regularization term over the predictedtransformations (and thus also obviates the hyperparameter tuning associatedwith such terms), FastJAM performs image JA quickly and effectively.Experimental results on several benchmarks demonstrate that FastJAM achievesresults better than existing modern JA methods in terms of alignment quality,while reducing computation time from hours or minutes to mere seconds. Our codeis available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/</description>
      <author>example@mail.com (Omri Hirsch, Ron Shapira Weber, Shira Ifergane, Oren Freifeld)</author>
      <guid isPermaLink="false">2510.22842v2</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>JiuTian Chuanliu: A Large Spatiotemporal Model for General-purpose Dynamic Urban Sensing</title>
      <link>http://arxiv.org/abs/2510.23662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为通用动态人类移动性嵌入(GDHME)的框架，用于处理大规模人类移动性数据，发现移动行为背后的潜在语义，并支持各种城市感知任务。&lt;h4&gt;背景&lt;/h4&gt;人类移动性作为城市感知的窗口，包含丰富的时空信息，反映了居民行为偏好和城市区域功能。现有方法通常从特定角度处理特定任务，导致对人类移动性建模不足，所学知识在下游应用中适用性有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，通过时空模型处理大量人类移动性数据，发现潜在语义，支持多种城市感知任务。&lt;h4&gt;方法&lt;/h4&gt;GDHME框架遵循自监督学习思想，包含两个阶段：第一阶段将人和区域视为动态图中的节点，统一为人-区域-时间交互，使用连续时间编码器计算演化节点表示，并设计自回归自监督任务引导学习；第二阶段利用这些表示支持各种任务。通过基站系统收集大规模移动数据，并构建多任务城市感知基准进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;离线实验证明GDHME能从大量数据中自动学习有价值的节点特征。该框架已成功部署九天川流大模型，该系统在2023年中国移动全球合作伙伴大会上展示。&lt;h4&gt;结论&lt;/h4&gt;GDHME框架能有效处理人类移动性数据，提取有价值特征，支持多种城市感知任务，具有广泛的适用性和实用价值。&lt;h4&gt;翻译&lt;/h4&gt;作为城市感知的窗口，人类移动性包含丰富的时空信息，反映了居民的行为偏好和城市区域的功能。人类移动性分析吸引了众多研究者的关注。然而，现有方法通常从特定角度处理特定任务，导致对人类移动性建模不足，所学知识在各种下游应用中适用性有限。为解决这些挑战，本文提出将大量人类移动性数据输入时空模型，发现移动行为背后的潜在语义，并支持各种城市感知任务。具体来说，通过无处不在的基站系统收集大规模、广泛覆盖的人类移动性数据，并引入了一个名为通用动态人类移动性嵌入(GDHME)的城市感知框架。该框架遵循自监督学习思想，包含两个主要阶段。第一阶段，GDHME将人和区域视为动态图中的节点，将人类移动性数据统一为人-区域-时间交互。在连续时间运行的编码器动态计算演化的节点表示，捕捉人和区域的动态状态。此外，专门设计了自回归自监督任务来引导通用节点嵌入的学习。第二阶段，利用这些表示来支持各种任务。为评估GDHME框架的有效性，作者构建了一个多任务城市感知基准。离线实验证明了GDHME能够从大量数据中自动学习有价值的节点特征。此外，该框架被用于部署九天川流大模型，该系统已在2023年中国移动全球合作伙伴大会上展示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a window for urban sensing, human mobility contains rich spatiotemporalinformation that reflects both residents' behavior preferences and thefunctions of urban areas. The analysis of human mobility has attracted theattention of many researchers. However, existing methods often address specifictasks from a particular perspective, leading to insufficient modeling of humanmobility and limited applicability of the learned knowledge in variousdownstream applications. To address these challenges, this paper proposes topush massive amounts of human mobility data into a spatiotemporal model,discover latent semantics behind mobility behavior and support various urbansensing tasks. Specifically, a large-scale and widely covering human mobilitydata is collected through the ubiquitous base station system and a frameworknamed General-purpose and Dynamic Human Mobility Embedding (GDHME) for urbansensing is introduced. The framework follows the self-supervised learning ideaand contains two major stages. In stage 1, GDHME treats people and regions asnodes within a dynamic graph, unifying human mobility data aspeople-region-time interactions. An encoder operating in continuous-timedynamically computes evolving node representations, capturing dynamic statesfor both people and regions. Moreover, an autoregressive self-supervised taskis specially designed to guide the learning of the general-purpose nodeembeddings. In stage 2, these representations are utilized to support varioustasks. To evaluate the effectiveness of our GDHME framework, we furtherconstruct a multi-task urban sensing benchmark. Offline experiments demonstrateGDHME's ability to automatically learn valuable node features from vast amountsof data. Furthermore, our framework is used to deploy the JiuTian ChuanLiu BigModel, a system that has been presented at the 2023 China Mobile WorldwidePartner Conference.</description>
      <author>example@mail.com (Liangzhe Han, Leilei Sun, Tongyu Zhu, Tao Tao, Jibin Wang, Weifeng Lv)</author>
      <guid isPermaLink="false">2510.23662v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
  <item>
      <title>Pearl: A Foundation Model for Placing Every Atom in the Right Location</title>
      <link>http://arxiv.org/abs/2510.24670v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了Pearl，一种用于蛋白质-配体协同折叠的基础模型，通过三个关键创新解决了深度学习方法在结构预测中的局限性，实现了最先进的性能表现。&lt;h4&gt;背景&lt;/h4&gt;准确预测蛋白质-配体复合物的三维结构是计算药物发现中的基本挑战，限制了治疗设计的速度和成功率。虽然深度学习方法显示出潜力，但其性能受限于实验数据稀少、架构效率低下、物理无效构象以及无法充分利用可用辅助信息等因素。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服数据稀缺、提高架构效率、确保物理有效性并充分利用辅助信息的蛋白质-配体结构预测模型。&lt;h4&gt;方法&lt;/h4&gt;作者提出了Pearl（Placing Every Atom in the Right Location）模型，包含三个关键创新：(1) 使用大规模合成数据的训练方法以克服数据稀缺；(2) 融入SO(3)-等变扩散模块的架构，尊重3D旋转对称性；(3) 支持蛋白质和非聚合物组分的通用多链模板系统，以及无条件/条件双模式的可控推理。&lt;h4&gt;主要发现&lt;/h4&gt;Pearl在蛋白质-配体协同折叠方面建立了新的性能标准，在公共基准测试中超越了AlphaFold3和其他开源基线模型，准确构象生成比次优模型分别提高了14.5%和14.2%。在口袋条件协同折叠模式下，Pearl在严格标准下实现了3.6倍的改进，且模型性能与训练中使用的合成数据集大小直接相关。&lt;h4&gt;结论&lt;/h4&gt;Pearl通过创新的训练方法、架构设计和推理机制，显著提高了蛋白质-配体复合物结构预测的准确性，合成数据的使用对模型性能有直接积极影响。&lt;h4&gt;翻译&lt;/h4&gt;准确预测蛋白质-配体复合物的三维结构仍然是计算药物发现中的一个基本挑战，它限制了治疗设计的速度和成功率。深度学习方法最近显示出作为结构预测工具的强大潜力，在多样化的生物分子系统中取得了有希望的准确性。然而，它们的性能和效用受到实验数据稀少、架构效率低下、物理无效构象以及在推理阶段利用可用辅助信息的能力有限等因素的制约。为了解决这些问题，我们引入了Pearl（Placing Every Atom in the Right Location），一种用于大规模蛋白质-配体协同折叠的基础模型。Pearl通过三个关键创新解决了这些挑战：(1) 包括大规模合成数据的训练方法，以克服数据稀缺；(2) 融入SO(3)-等变扩散模块的架构，本质上尊重3D旋转对称性，提高泛化能力和样本效率；(3) 可控推理，包括支持蛋白质和非聚合物组分的通用多链模板系统，以及无条件/条件双模式。Pearl在蛋白质-配体协同折叠方面建立了新的最先进性能。在生成准确和物理有效构象的关键指标上，Pearl在公共基准测试中超越了AlphaFold3和其他开源基线模型，比次优模型分别提高了14.5%和14.2%。在口袋条件协同折叠模式下，Pearl在更严格的标准下实现了3.6倍的改进。最后，我们研究表明模型性能与训练中使用的合成数据集大小直接相关。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting the three-dimensional structures of protein-ligandcomplexes remains a fundamental challenge in computational drug discovery thatlimits the pace and success of therapeutic design. Deep learning methods haverecently shown strong potential as structural prediction tools, achievingpromising accuracy across diverse biomolecular systems. However, theirperformance and utility are constrained by scarce experimental data,inefficient architectures, physically invalid poses, and the limited ability toexploit auxiliary information available at inference. To address these issues,we introduce Pearl (Placing Every Atom in the Right Location), a foundationmodel for protein-ligand cofolding at scale. Pearl addresses these challengeswith three key innovations: (1) training recipes that include large-scalesynthetic data to overcome data scarcity; (2) architectures that incorporate anSO(3)-equivariant diffusion module to inherently respect 3D rotationalsymmetries, improving generalization and sample efficiency, and (3)controllable inference, including a generalized multi-chain templating systemsupporting both protein and non-polymeric components as well as dualunconditional/conditional modes. Pearl establishes a new state-of-the-artperformance in protein-ligand cofolding. On the key metric of generatingaccurate (RMSD &lt; 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold3 and other open source baselines on the public Runs N' Poses and PoseBustersbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over thenext best model. In the pocket-conditional cofolding regime, Pearl delivers$3.6\times$ improvement on a proprietary set of challenging, real-world drugtargets at the more rigorous RMSD &lt; 1 \r{A} threshold. Finally, we demonstratethat model performance correlates directly with synthetic dataset size used intraining.</description>
      <author>example@mail.com (Genesis Research Team, Alejandro Dobles, Nina Jovic, Kenneth Leidal, Pranav Murugan, David C. Williams, Drausin Wulsin, Nate Gruver, Christina X. Ji, Korrawat Pruegsanusak, Gianluca Scarpellini, Ansh Sharma, Wojciech Swiderski, Andrea Bootsma, Richard Strong Bowen, Charlotte Chen, Jamin Chen, Marc André Dämgen, Roy Tal Dew, Benjamin DiFrancesco, J. D. Fishman, Alla Ivanova, Zach Kagin, David Li-Bland, Zuli Liu, Igor Morozov, Jeffrey Ouyang-Zhang, Frank C. Pickard IV, Kushal S. Shah, Ben Shor, Gabriel Monteiro da Silva, Maxx Tessmer, Carl Tilbury, Cyr Vetcher, Daniel Zeng, Maruan Al-Shedivat, Aleksandra Faust, Evan N. Feinberg, Michael V. LeVine, Matteus Pan)</author>
      <guid isPermaLink="false">2510.24670v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning</title>
      <link>http://arxiv.org/abs/2510.24650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 8 figures, and 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该综述探讨了基础模型(FMs)在农业特定地点疾病管理(SSDM)中的应用，重点关注大型语言模型(LLMs)和视觉语言模型(VLMs)的发展及其在自适应学习、强化学习和数字孪生框架中的作用。&lt;h4&gt;背景&lt;/h4&gt;农业特定地点疾病管理通过机器学习和深度学习在实时计算机视觉方面取得快速进展，研究从手工特征提取发展到大规模自动化特征学习，基础模型为作物疾病数据处理带来全新方式。&lt;h4&gt;目的&lt;/h4&gt;筛选约40篇关于基础模型在SSDM中应用的论文，讨论其在自适应学习、强化学习和数字孪生框架中的作用，并分析当前发展状况和挑战。&lt;h4&gt;方法&lt;/h4&gt;分析基础模型如何整合视觉和文本数据，解释症状文本，推理症状-管理关系，支持交互式问答，以及机器人和自适应学习如何支持基于现场的疾病管理。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在2023-24年文献激增；视觉语言模型发表数量比大型语言模型多5-10倍；强化学习和自适应学习在智能喷洒方面仍处于起步阶段；数字孪生可虚拟模拟目标喷洒；解决模拟到现实的差距对实际部署至关重要；人机协作仍有限；具有实时反馈的多模态基础模型将推动下一代SSDM。&lt;h4&gt;结论&lt;/h4&gt;基础模型特别是视觉语言模型在农业特定地点疾病管理中展现出巨大潜力，但仍需解决模拟到现实的差距和人机协作的局限性等挑战。&lt;h4&gt;翻译&lt;/h4&gt;作物特定地点疾病管理通过机器学习和深度学习在实时计算机视觉方面取得了快速进展。研究从手工特征提取发展到大规模自动化特征学习。随着基础模型的出现，作物疾病数据现在以全新的方式被处理。与传统神经网络不同，基础模型整合视觉和文本数据，解释文本中的症状，推理症状-管理关系，并为种植者和教育者支持交互式问答。机器人和自适应学习进一步支持基于现场的疾病管理。本综述筛选了约40篇关于基础模型在特定地点疾病管理中应用的论文，重点关注大型语言模型和视觉语言模型，并讨论它们在自适应学习、强化学习和用于目标喷洒的数字孪生框架中的作用。主要发现：基础模型在2023-24年文献激增中越来越受欢迎；视觉语言模型的发表数量比大型语言模型多5-10倍；强化学习和自适应学习在智能喷洒方面仍处于起步阶段；带有强化学习的数字孪生可以虚拟模拟目标喷洒；解决模拟到现实的差距对实际部署至关重要；人机协作仍然有限，特别是在人机交互方法中，机器人检测早期症状，人类验证不确定情况；具有实时反馈的多模态基础模型将推动下一代特定地点疾病管理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Site-specific disease management (SSDM) in crops has advanced rapidly throughmachine and deep learning (ML and DL) for real-time computer vision. Researchevolved from handcrafted feature extraction to large-scale automated featurelearning. With foundation models (FMs), crop disease datasets are now processedin fundamentally new ways. Unlike traditional neural networks, FMs integratevisual and textual data, interpret symptoms in text, reason aboutsymptom-management relationships, and support interactive QA for growers andeducators. Adaptive and imitation learning in robotics further enablesfield-based disease management. This review screened approx. 40 articles on FMapplications for SSDM, focusing on large-language models (LLMs) andvision-language models (VLMs), and discussing their role in adaptive learning(AL), reinforcement learning (RL), and digital twin frameworks for targetedspraying. Key findings: (a) FMs are gaining traction with surging literature in2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RLand AL are still nascent for smart spraying; (d) digital twins with RL cansimulate targeted spraying virtually; (e) addressing the sim-to-real gap iscritical for real-world deployment; (f) human-robot collaboration remainslimited, especially in human-in-the-loop approaches where robots detect earlysymptoms and humans validate uncertain cases; (g) multi-modal FMs withreal-time feedback will drive next-gen SSDM. For updates, resources, andcontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, tosubmit papers, code, or datasets.</description>
      <author>example@mail.com (Nitin Rai, Daeun, Choi, Nathan S. Boyd, Arnold W. Schumann)</author>
      <guid isPermaLink="false">2510.24650v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives</title>
      <link>http://arxiv.org/abs/2510.24551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种以数据为中心的医疗保健生成式人工智能系统设计范式，通过构建医疗数据生态系统作为基础支撑，实现高质量、有效的医疗保健服务。&lt;h4&gt;背景&lt;/h4&gt;生成式人工智能(GenAI)正在全球范围内迅速发展，为医疗保健领域带来变革性机会。从大型语言模型用于临床笔记合成和对话式辅助，到整合医学影像、电子健康记录和基因组数据的多模态系统用于决策支持，GenAI正在改变医学实践和医疗保健提供方式。&lt;h4&gt;目的&lt;/h4&gt;提出一种数据中心的范式，用于医疗保健领域生成式人工智能系统的设计和部署，解决GenAI在医疗保健应用中的挑战。&lt;h4&gt;方法&lt;/h4&gt;重新定位数据生命周期，将医疗数据生态系统作为生成式医疗保健系统的基础支撑。该生态系统支持多样化医疗数据和知识的集成、表示和检索，通过语义向量搜索和上下文查询等数据处理管道，为上游模型组件和下游临床应用提供支持。&lt;h4&gt;主要发现&lt;/h4&gt;生成式人工智能在医疗保健领域部署需要深入了解医疗保健任务以及可以实现和不能实现的目标，医疗数据生态系统是解决这一挑战的关键。&lt;h4&gt;结论&lt;/h4&gt;通过构建可持续的医疗数据生态系统，不仅能为基础模型提供高质量、多模态数据用于大规模预训练和领域特定微调，还能作为知识检索后端支持特定任务推理，使GenAI能够高质量、有效地部署医疗保健服务。&lt;h4&gt;翻译&lt;/h4&gt;生成式人工智能(GenAI)正在席卷全球。它承诺为推进和颠覆现有实践（包括医疗保健）带来变革性机会。从用于临床笔记合成和对话式辅助的大型语言模型(LLMs)，到整合医学影像、电子健康记录和基因组数据用于决策支持的多模态系统，GenAI正在改变医学实践和医疗保健的提供方式，如诊断和个性化治疗，有潜力减轻临床医生的认知负担，从而改善整体医疗保健服务。然而，GenAI在医疗保健领域的部署需要对医疗保健任务以及可实现和不可实现的目标有深入理解。在本文中，我们提出了一个以数据为中心的范式，用于医疗保健领域生成式人工智能系统的设计和部署。具体而言，我们通过将医疗数据生态系统作为生成式医疗保健系统的基础支撑物，重新定位了数据生命周期。该生态系统旨在可持续地支持多样化医疗数据和知识的集成、表示和检索。通过有效和高效的数据处理管道，如语义向量搜索和上下文查询，它使生成式人工智能能够为上游模型组件和下游临床应用提供支持。最终，它不仅为基础模型提供高质量、多模态数据用于大规模预训练和领域特定微调，还充当知识检索后端，通过智能层支持特定任务的推理。该生态系统使生成式人工智能能够高质量、有效地部署医疗保健服务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative Artificial Intelligence (GenAI) is taking the world by storm. Itpromises transformative opportunities for advancing and disrupting existingpractices, including healthcare. From large language models (LLMs) for clinicalnote synthesis and conversational assistance to multimodal systems thatintegrate medical imaging, electronic health records, and genomic data fordecision support, GenAI is transforming the practice of medicine and thedelivery of healthcare, such as diagnosis and personalized treatments, withgreat potential in reducing the cognitive burden on clinicians, therebyimproving overall healthcare delivery. However, GenAI deployment in healthcarerequires an in-depth understanding of healthcare tasks and what can and cannotbe achieved. In this paper, we propose a data-centric paradigm in the designand deployment of GenAI systems for healthcare. Specifically, we reposition thedata life cycle by making the medical data ecosystem as the foundationalsubstrate for generative healthcare systems. This ecosystem is designed tosustainably support the integration, representation, and retrieval of diversemedical data and knowledge. With effective and efficient data processingpipelines, such as semantic vector search and contextual querying, it enablesGenAI-powered operations for upstream model components and downstream clinicalapplications. Ultimately, it not only supplies foundation models withhigh-quality, multimodal data for large-scale pretraining and domain-specificfine-tuning, but also serves as a knowledge retrieval backend to supporttask-specific inference via the agentic layer. The ecosystem enables thedeployment of GenAI for high-quality and effective healthcare delivery.</description>
      <author>example@mail.com (Gang Chen, Changshuo Liu, Gene Anne Ooi, Marcus Tan, Zhongle Xie, Jianwei Yin, James Wei Luen Yip, Wenqiao Zhang, Jiaqi Zhu, Beng Chin Ooi)</author>
      <guid isPermaLink="false">2510.24551v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Affordance Representation and Recognition for Autonomous Agents</title>
      <link>http://arxiv.org/abs/2510.24459v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种从结构化数据构建世界模型的新方法，解决了软件代理在适应不断变化的Web环境时面临的两个关键挑战。&lt;h4&gt;背景&lt;/h4&gt;软件代理的自主性依赖于其从结构化数据（如网页DOM和Web服务语义描述）构建内部世界模型的能力。然而，原始HTML的冗长性和硬编码API集成的静态性构成了重大障碍。&lt;h4&gt;目的&lt;/h4&gt;开发一种模式语言，使软件代理能够高效构建和维护准确的世界模型，从而实现跨Web及其扩展资源的可扩展、自适应和互操作性自动化。&lt;h4&gt;方法&lt;/h4&gt;提出两种互补的架构模式：1) DOM转换模式，将冗长的原始DOM提炼为紧凑的、任务相关的表示；2) 超媒体功能识别模式，通过解析语义描述动态发现和集成未知Web服务的能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过结合这两种模式，软件代理能够克服数据冗余和服务动态变化的挑战，构建和维护准确的世界模型。&lt;h4&gt;结论&lt;/h4&gt;所提出的模式语言为构建能够高效适应不断演化的数字环境的智能代理提供了强大框架，增强了自动化系统的可扩展性、适应性和互操作性。&lt;h4&gt;翻译&lt;/h4&gt;软件代理的自主性根本上取决于其从定义其数字环境的结构化数据（如网页的文档对象模型和Web服务的语义描述）构建可行的内部世界模型的能力。然而，从原始结构化数据构建此世界模型存在两个关键挑战：原始HTML的冗长性使其在计算上难以被基础模型直接使用，而硬编码API集成的静态性质阻止了代理适应不断发展的服务。本文介绍了一种从结构化数据进行世界建模的模式语言，提出了两种互补的架构模式。DOM转换模式通过将冗长的原始DOM提炼为紧凑的、任务相关的表示或为代理推理核心优化的世界模型，解决了网页复杂性的挑战。同时，超媒体功能识别模式使代理能够通过解析标准化的语义描述来动态丰富其世界模型，从而在运行时发现和集成未知Web服务的能力。这些模式共同提供了一个强大的框架，用于构建能够高效构建和维护准确世界模型的代理，从而实现Web及其扩展资源的可扩展、自适应和互操作性自动化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The autonomy of software agents is fundamentally dependent on their abilityto construct an actionable internal world model from the structured data thatdefines their digital environment, such as the Document Object Model (DOM) ofweb pages and the semantic descriptions of web services. However, constructingthis world model from raw structured data presents two critical challenges: theverbosity of raw HTML makes it computationally intractable for direct use byfoundation models, while the static nature of hardcoded API integrationsprevents agents from adapting to evolving services.  This paper introduces a pattern language for world modeling from structureddata, presenting two complementary architectural patterns. The DOM TransductionPattern addresses the challenge of web page complexity by distilling} averbose, raw DOM into a compact, task-relevant representation or world modeloptimized for an agent's reasoning core. Concurrently, the HypermediaAffordances Recognition Pattern enables the agent to dynamically enrich itsworld model by parsing standardized semantic descriptions to discover andintegrate the capabilities of unknown web services at runtime. Together, thesepatterns provide a robust framework for engineering agents that can efficientlyconstruct and maintain an accurate world model, enabling scalable, adaptive,and interoperable automation across the web and its extended resources.</description>
      <author>example@mail.com (Habtom Kahsay Gidey, Niklas Huber, Alexander Lenz, Alois Knoll)</author>
      <guid isPermaLink="false">2510.24459v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Visual Intelligence: Insights from Video Pretraining</title>
      <link>http://arxiv.org/abs/2510.24448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on  visual intelligence. This work can be considered as v2&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究表明视频扩散模型在视觉任务上比语言模型更高效，视频预训练提供的归纳偏置有助于构建视觉基础模型。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在大规模预训练后能够在语言领域快速适应新问题，但这种成功在视觉领域并未同样有效，包括LLMs在内的模型在组合理解、样本效率和通用问题解决方面仍然存在困难。&lt;h4&gt;目的&lt;/h4&gt;研究视频扩散模型作为弥合语言模型与视觉模型差距的有前途方向，测试视频预训练是否能提供支持广泛任务适应性的归纳偏置。&lt;h4&gt;方法&lt;/h4&gt;在时空数据上预训练视频扩散模型，使其具有结构和动态性的强归纳偏置；设计对照评估，让预训练的LLM和VDM都配备轻量级适配器，并以自然方式呈现任务；在多个基准测试中评估，包括ARC-AGI、ConceptARC、视觉游戏、路线规划和细胞自动机。&lt;h4&gt;主要发现&lt;/h4&gt;VDMs比语言对应模型具有更高的数据效率；视频预训练提供的归纳偏置支持视觉基础模型的进展。&lt;h4&gt;结论&lt;/h4&gt;视频预训练提供了归纳偏置，支持向视觉基础模型进展。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型已经证明，大规模预训练使系统能够在语言领域以少量监督快速适应新问题。然而，这种成功并没有同样有效地转化为视觉领域，包括LLMs在内的模型在组合理解、样本效率和通用问题解决方面仍然存在困难。我们研究视频扩散模型作为弥合这一差距的有前途方向。在时空数据上的预训练赋予这些模型结构和动态性的强归纳偏置，我们假设这可以支持广泛的任务适应性。为了验证这一点，我们设计了一个对照评估，让预训练的LLM和预训练的VDM都配备轻量级适配器，并以它们自然的方式呈现任务。在包括ARC-AGI、ConceptARC、视觉游戏、路线规划和细胞自动机在内的基准测试中，VDMs比其语言对应模型表现出更高的数据效率。综上所述，我们的结果表明视频预训练提供了归纳偏置，支持向视觉基础模型的进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated that large-scale pretrainingenables systems to adapt rapidly to new problems with little supervision in thelanguage domain. This success, however, has not translated as effectively tothe visual domain, where models, including LLMs, continue to struggle withcompositional understanding, sample efficiency, and general-purposeproblem-solving. We investigate Video Diffusion Models (VDMs) as a promisingdirection for bridging this gap. Pretraining on spatiotemporal data endowsthese models with strong inductive biases for structure and dynamics, which wehypothesize can support broad task adaptability. To test this, we design acontrolled evaluation in which both a pretrained LLM and a pretrained VDM areequipped with lightweight adapters and presented with tasks in their naturalmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,route planning, and cellular automata, VDMs demonstrate higher data efficiencythan their language counterparts. Taken together, our results indicate thatvideo pretraining offers inductive biases that support progress toward visualfoundation models.</description>
      <author>example@mail.com (Pablo Acuaviva, Aram Davtyan, Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Alexandre Alahi, Paolo Favaro)</author>
      <guid isPermaLink="false">2510.24448v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>A Unified Geometric Space Bridging AI Models and the Human Brain</title>
      <link>http://arxiv.org/abs/2510.24342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了'类脑空间'概念，通过映射AI模型的内在空间注意力拓扑结构到人类功能性脑网络，实现了跨模态AI模型的统一比较框架，揭示了机器与大脑之间的深层组织原则。&lt;h4&gt;背景&lt;/h4&gt;几十年来，神经科学家和计算机科学家一直致力于理解并构建智能。现代人工神经网络在语言、感知和推理方面已可与人类媲美，但这些系统是否像大脑一样组织信息仍然在很大程度上未知。现有的大脑-AI对齐研究虽展示了两个系统间的对应关系，但比较局限于特定输入和任务，缺乏跨模态AI模型内在组织的共同比较基础。&lt;h4&gt;目的&lt;/h4&gt;引入'类脑空间'概念，这是一个统一的几何空间，无论输入模态、任务或感觉域如何，都能通过将AI模型内在的空间注意力拓扑结构映射到典型人类功能性脑网络上，实现对AI模型的精确定位和比较。&lt;h4&gt;方法&lt;/h4&gt;对151个基于Transformer的模型进行广泛分析，涵盖最先进的大型视觉模型、大型语言模型和大型多模态模型。&lt;h4&gt;主要发现&lt;/h4&gt;在类脑空间中存在连续的弧形几何结构，反映类脑性的逐渐增加；不同模型表现出不同的分布模式，与不同程度的类脑性相关，这种模式不仅受模态影响，还受预训练范式是否强调全局语义抽象以及位置编码方案是否促进不同模态间深度融合的影响；此外，模型的类脑程度和其下游任务表现并非完全相同。&lt;h4&gt;结论&lt;/h4&gt;类脑空间提供了第一个跨领域的定位、量化和比较智能的统一框架，揭示了连接机器和大脑的深层组织原则。&lt;h4&gt;翻译&lt;/h4&gt;几十年来，神经科学家和计算机科学家一直怀有共同的志向：理解智能并构建它。现代人工神经网络在语言、感知和推理方面现在可以与人类相媲美，但这些人工系统是否像大脑一样组织信息仍然在很大程度上未知。现有的大脑-AI对齐研究已经展示了两个系统之间的惊人对应关系，但这样的比较仍然局限于特定的输入和任务，没有提供共同的基础来比较不同模态（视觉、语言或多模态）的AI模型是如何内在组织的。在这里，我们引入了一个突破性的概念：类脑空间：一个统一的几何空间，通过将AI模型内在的空间注意力拓扑组织映射到典型的人类功能性脑网络上，无论输入模态、任务或感觉域如何，每个AI模型都可以在这个空间中被精确定位和比较。我们对151个基于Transformer的模型进行了广泛分析，这些模型涵盖了最先进的大型视觉模型、大型语言模型和大型多模态模型，在这个空间中发现了一个连续的弧形几何结构，反映了类脑性的逐渐增加；不同的模型在这个几何结构中表现出不同的分布模式，与不同程度的类脑性相关，这种模式不仅受模态影响，还受预训练范式是否强调全局语义抽象以及位置编码方案是否促进不同模态间的深度融合的影响。此外，模型的类脑程度和其下游任务表现并非'完全相同'。类脑空间提供了第一个跨领域的定位、量化和比较智能的统一框架，揭示了连接机器和大脑的深层组织原则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For decades, neuroscientists and computer scientists have pursued a sharedambition: to understand intelligence and build it. Modern artificial neuralnetworks now rival humans in language, perception, and reasoning, yet it isstill largely unknown whether these artificial systems organize information asthe brain does. Existing brain-AI alignment studies have shown the strikingcorrespondence between the two systems, but such comparisons remain bound tospecific inputs and tasks, offering no common ground for comparing how AImodels with different kinds of modalities-vision, language, or multimodal-areintrinsically organized. Here we introduce a groundbreaking concept ofBrain-like Space: a unified geometric space in which every AI model can beprecisely situated and compared by mapping its intrinsic spatial attentiontopological organization onto canonical human functional brain networks,regardless of input modality, task, or sensory domain. Our extensive analysisof 151 Transformer-based models spanning state-of-the-art large vision models,large language models, and large multimodal models uncovers a continuousarc-shaped geometry within this space, reflecting a gradual increase ofbrain-likeness; different models exhibit distinct distribution patterns withinthis geometry associated with different degrees of brain-likeness, shaped notmerely by their modality but by whether the pretraining paradigm emphasizesglobal semantic abstraction and whether the positional encoding schemefacilitates deep fusion across different modalities. Moreover, the degree ofbrain-likeness for a model and its downstream task performance are not"identical twins". The Brain-like Space provides the first unified frameworkfor situating, quantifying, and comparing intelligence across domains,revealing the deep organizational principles that bridge machines and thebrain.</description>
      <author>example@mail.com (Silin Chen, Yuzhong Chen, Zifan Wang, Junhao Wang, Zifeng Jia, Keith M Kendrick, Tuo Zhang, Lin Zhao, Dezhong Yao, Tianming Liu, Xi Jiang)</author>
      <guid isPermaLink="false">2510.24342v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2</title>
      <link>http://arxiv.org/abs/2510.24195v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UAP-SAM2，第一个针对SAM2的跨提示通用对抗攻击，通过双语义偏差驱动，有效解决了SAM2架构差异带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;图像分割基础模型SAM对对抗样本存在脆弱性，其后续模型SAM2在视频分割方面表现出强大的泛化能力，但其鲁棒性尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;分析现有攻击在SAM和SAM2之间的性能差距，并提出一种有效的对抗攻击方法针对SAM2。&lt;h4&gt;方法&lt;/h4&gt;提出UAP-SAM2方法，包括：1) 设计目标扫描策略，将每帧划分为k个区域，每个区域随机分配提示，减少优化过程中的提示依赖性；2) 设计双语义偏差框架，通过扭曲当前帧内语义和破坏连续帧间语义一致性来优化UAP。&lt;h4&gt;主要发现&lt;/h4&gt;现有攻击在SAM和SAM2之间存在性能差距，主要由于SAM2架构差异带来的两个关键挑战：来自提示的方向性指导和连续帧之间的语义纠缠。&lt;h4&gt;结论&lt;/h4&gt;UAP-SAM2在两个分割任务上的六个数据集实验中表现出有效性，以较大优势显著优于最先进的攻击方法。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究揭示了图像分割基础模型SAM对对抗样本的脆弱性。其后续模型SAM2因其强大的视频分割泛化能力而受到广泛关注。然而，其鲁棒性尚未被探索，目前还不清楚现有的针对SAM的攻击是否可以直接转移到SAM2上。在本文中，我们首先分析了现有攻击在SAM和SAM2之间的性能差距，并指出了它们架构差异带来的两个关键挑战：来自提示的方向性指导和连续帧之间的语义纠缠。为解决这些问题，我们提出了UAP-SAM2，这是第一个由双语义偏差驱动的针对SAM2的跨提示通用对抗攻击。为实现跨提示可转移性，我们首先设计了一个目标扫描策略，将每帧划分为k个区域，每个区域随机分配一个提示，以减少优化过程中的提示依赖性。为提高有效性，我们设计了一个双语义偏差框架，通过扭曲当前帧内的语义和破坏连续帧之间的语义一致性来优化UAP。在两个分割任务上的六个数据集进行的广泛实验证明了所提方法对SAM2的有效性。比较结果显示，UAP-SAM2以较大优势显著优于最先进的(SOTA)攻击。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies reveal the vulnerability of the image segmentation foundationmodel SAM to adversarial examples. Its successor, SAM2, has attractedsignificant attention due to its strong generalization capability in videosegmentation. However, its robustness remains unexplored, and it is unclearwhether existing attacks on SAM can be directly transferred to SAM2. In thispaper, we first analyze the performance gap of existing attacks between SAM andSAM2 and highlight two key challenges arising from their architecturaldifferences: directional guidance from the prompt and semantic entanglementacross consecutive frames. To address these issues, we propose UAP-SAM2, thefirst cross-prompt universal adversarial attack against SAM2 driven by dualsemantic deviation. For cross-prompt transferability, we begin by designing atarget-scanning strategy that divides each frame into k regions, each randomlyassigned a prompt, to reduce prompt dependency during optimization. Foreffectiveness, we design a dual semantic deviation framework that optimizes aUAP by distorting the semantics within the current frame and disrupting thesemantic consistency across consecutive frames. Extensive experiments on sixdatasets across two segmentation tasks demonstrate the effectiveness of theproposed method for SAM2. The comparative results show that UAP-SAM2significantly outperforms state-of-the-art (SOTA) attacks by a large margin.</description>
      <author>example@mail.com (Ziqi Zhou, Yifan Hu, Yufei Song, Zijing Li, Shengshan Hu, Leo Yu Zhang, Dezhong Yao, Long Zheng, Hai Jin)</author>
      <guid isPermaLink="false">2510.24195v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames</title>
      <link>http://arxiv.org/abs/2510.24194v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;行为克隆是一种从演示中学习序列决策的简单而有效技术，本文提出让演示者在部分信息不足的情况下进行演示，发现这种'蒙眼'专家的克隆方法在泛化到未见任务时表现更好。&lt;h4&gt;背景&lt;/h4&gt;行为克隆已成为物理世界基础模型的核心，实现泛化需要无数任务的演示。通常，具有完整任务信息的人类专家会演示最优行为。&lt;h4&gt;目的&lt;/h4&gt;研究向演示者隐藏部分任务信息是否能提高行为克隆的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出'蒙眼专家'方法，即向任务演示者隐藏部分信息，迫使其使用非平凡的探索策略来解决任务。在真实世界机器人插销任务和Procgen基准视频游戏上进行实验，并进行理论分析。&lt;h4&gt;主要发现&lt;/h4&gt;克隆'蒙眼专家'的行为比克隆完全信息专家的行为在未见任务上泛化能力更好。理论分析表明泛化误差与演示者可用的任务信息量和演示任务数量有关，使用更少演示任务时克隆蒙眼专家能实现更好泛化。&lt;h4&gt;结论&lt;/h4&gt;理论和实践均表明，使用更少的演示任务，克隆蒙眼专家能够实现更好的泛化效果。&lt;h4&gt;翻译&lt;/h4&gt;行为克隆是一种简单而有效的技术，用于从演示中学习序列决策。最近，它已成为物理世界基础模型的核心，其中实现泛化需要无数任务的演示。通常，具有任务完整信息的人类专家会演示（几乎）最优的行为。在本文中，我们提出向演示者隐藏任务的部分信息。这种'蒙眼'专家被迫采用非平凡的探索来解决任务。我们证明，克隆蒙眼专家比完全信息的专家在未见任务上泛化能力更好。我们在有限人类演示下的真实世界机器人插销任务以及Procgen基准的视频游戏上进行了实验。此外，我们通过理论分析支持了这一发现，理论和实践都表明，用更少的演示任务克隆蒙眼专家能更好地泛化。项目页面包含视频和代码：https://sites.google.com/view/blindfoldedexperts/home&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Behavioral cloning is a simple yet effective technique for learningsequential decision-making from demonstrations. Recently, it has gainedprominence as the core of foundation models for the physical world, whereachieving generalization requires countless demonstrations of a multitude oftasks. Typically, a human expert with full information on the task demonstratesa (nearly) optimal behavior. In this paper, we propose to hide some of thetask's information from the demonstrator. This ``blindfolded'' expert iscompelled to employ non-trivial exploration to solve the task. We show thatcloning the blindfolded expert generalizes better to unseen tasks than itsfully-informed counterpart. We conduct experiments of real-world robot peginsertion tasks with (limited) human demonstrations, alongside videogames fromthe Procgen benchmark. Additionally, we support our findings with theoreticalanalysis, which confirms that the generalization error scales with$\sqrt{I/m}$, where $I$ measures the amount of task information available tothe demonstrator, and $m$ is the number of demonstrated tasks. Both theory andpractice indicate that cloning blindfolded experts generalizes better withfewer demonstrated tasks. Project page with videos and code:https://sites.google.com/view/blindfoldedexperts/home</description>
      <author>example@mail.com (Ev Zisselman, Mirco Mutti, Shelly Francis-Meretzki, Elisei Shafer, Aviv Tamar)</author>
      <guid isPermaLink="false">2510.24194v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning</title>
      <link>http://arxiv.org/abs/2510.24161v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Boundless Large Model (BLM₁)，一个多模态空间基础模型，能够在数字和物理空间无缝操作，实现跨具身和任务泛化。通过两阶段训练范式，BLM₁整合了跨空间迁移、跨任务学习和跨具身泛化能力，在数字和物理基准测试中超越了多种模型家族。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在视觉-语言推理方面取得进展并应用于具身智能体，但仍存在显著局限性：MLLMs在数字-物理空间和具身形式间泛化能力差；视觉-语言-动作模型(VLAs)产生低级行动但缺乏稳健的高层次具身推理；大多数具身大语言模型(ELLMs)局限于数字空间，对物理世界泛化能力差。缺乏能够在数字和物理空间无缝操作、跨具身和任务泛化的统一模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在数字和物理空间无缝操作、跨具身和任务泛化的统一模型。&lt;h4&gt;方法&lt;/h4&gt;提出Boundless Large Model (BLM₁)，一个多模态空间基础模型，保留指令跟随和推理能力，整合具身知识，支持稳健的跨具身控制。通过两阶段训练范式整合三种关键能力：跨空间迁移、跨任务学习和跨具身泛化。第一阶段通过精心挑选的数字语料库将具身知识注入MLLM，同时保持语言能力；第二阶段通过意图桥接接口训练策略模块，从MLLM中提取高层语义来指导控制，无需微调MLLM主干。使用自收集的跨具身演示套件，涵盖四种机器人具身和六种渐进式挑战任务。&lt;h4&gt;主要发现&lt;/h4&gt;在数字和物理基准测试中评估显示，单个BLM₁实例优于四种模型家族（MLLMs、ELLMs、VLAs和GMLMs），在数字任务中实现约6%的提升，在物理任务中实现约3%的提升。&lt;h4&gt;结论&lt;/h4&gt;BLM₁是一个有效的多模态空间基础模型，能够整合具身知识并实现跨空间、跨任务和跨具身的泛化能力，为具身智能体提供了新的发展方向。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)已推进视觉-语言推理，并越来越多地部署在具身智能体中。然而，仍存在显著局限性：MLLMs在数字-物理空间和具身形式间泛化能力差；视觉-语言-动作模型(VLAs)产生低级行动但缺乏稳健的高层次具身推理；大多数具身大语言模型(ELLMs)局限于数字空间，对物理世界泛化能力差。因此，能够在数字和物理空间无缝操作、跨具身和任务泛化的统一模型仍然缺失。我们引入了Boundless Large Model (BLM₁)，一个多模态空间基础模型，保留了指令跟随和推理能力，整合了具身知识，并支持稳健的跨具身控制。BLM₁通过两阶段训练范式整合了三种关键能力——跨空间迁移、跨任务学习和跨具身泛化。第一阶段通过精心挑选的数字语料库将具身知识注入MLLM，同时保持语言能力。第二阶段通过意图桥接接口训练策略模块，从MLLM中提取高层语义来指导控制，无需微调MLLM主干。这一过程得到了一个自收集的跨具身演示套件的支持，涵盖四种机器人具身和六种渐进式挑战任务。在数字和物理基准测试中的评估显示，单个BLM₁实例优于四种模型家族——MLLMs、ELLMs、VLAs和GMLMs，在数字任务中实现约6%的提升，在物理任务中实现约3%的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have advanced vision-languagereasoning and are increasingly deployed in embodied agents. However,significant limitations remain: MLLMs generalize poorly across digital-physicalspaces and embodiments; vision-language-action models (VLAs) produce low-levelactions yet lack robust high-level embodied reasoning; and most embodied largelanguage models (ELLMs) are constrained to digital-space with poorgeneralization to the physical world. Thus, unified models that operateseamlessly across digital and physical spaces while generalizing acrossembodiments and tasks remain absent. We introduce the \textbf{Boundless LargeModel (BLM$_1$)}, a multimodal spatial foundation model that preservesinstruction following and reasoning, incorporates embodied knowledge, andsupports robust cross-embodiment control. BLM$_1$ integrates three keycapabilities -- \textit{cross-space transfer, cross-task learning, andcross-embodiment generalization} -- via a two-stage training paradigm. Stage Iinjects embodied knowledge into the MLLM through curated digital corpora whilemaintaining language competence. Stage II trains a policy module through anintent-bridging interface that extracts high-level semantics from the MLLM toguide control, without fine-tuning the MLLM backbone. This process is supportedby a self-collected cross-embodiment demonstration suite spanning four robotembodiments and six progressively challenging tasks. Evaluations across digitaland physical benchmarks show that a single BLM$_1$ instance outperforms fourmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving$\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physicaltasks.</description>
      <author>example@mail.com (Wentao Tan, Bowen Wang, Heng Zhi, Chenyu Liu, Zhe Li, Jian Liu, Zengrong Lin, Yukun Dai, Yipeng Chen, Wenjie Yang, Enci Xie, Hao Xue, Baixu Ji, Chen Xu, Zhibin Wang, Tianshi Wang, Lei Zhu, Heng Tao Shen)</author>
      <guid isPermaLink="false">2510.24161v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Global Chlorophyll-\textit{a} Retrieval algorithm from Sentinel 2 Using Residual Deep Learning and Novel Machine Learning Water Classification</title>
      <link>http://arxiv.org/abs/2510.24124v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为全球水分类器(GWC)的监督式机器学习分类器，用于全球范围内水体识别和叶绿素-a浓度反演，并通过残差CNN校正提高了反演精度。&lt;h4&gt;背景&lt;/h4&gt;传统的水体识别和叶绿素-a浓度反演面临多种干扰因素，如云、太阳耀斑、雪、冰、水生植被、陆地和沉积物等，需要一种能够全球应用且稳健的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够全球范围内准确识别水体并反演叶绿素-a浓度的方法，克服地理和气候条件的限制。&lt;h4&gt;方法&lt;/h4&gt;1. 使用Sen2Cor校正的Sentinel-2地表反射率数据训练全球水分类器(GWC)2. 基于近100个全球分布的内陆水体样本进行训练3. 使用XGBoost回归器进行叶绿素-a浓度反演4. 添加残差CNN(RCNN)校正阶段提高反演精度5. 在867个水体上进行测试验证&lt;h4&gt;主要发现&lt;/h4&gt;1. GWC能够有效区分不同叶绿素-a水平的水体与非水体光谱2. GWC表现出地理位置稳定的性能3. GWC正标记的场景产生的叶绿素-a反演值更准确4. 残差CNN校正阶段显著提高了反演精度5. 最终算法在测试中表现出稳健性、可扩展性和全球可转移性&lt;h4&gt;结论&lt;/h4&gt;全球水分类器结合残差CNN校正的方法能够准确、稳健地进行全球水体识别和叶绿素-a浓度反演，无需针对不同地区进行额外调优，具有很高的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了全球水分类器(GWC)，一种监督式、地理范围广泛的机器学习分类器，基于Sen2Cor校正的Sentinel-2地表反射率数据训练。使用近100个全球分布的内陆水体，GWC能够区分不同叶绿素-a水平的水体光谱与非水体光谱(云、太阳耀斑、雪、冰、水生植被、陆地和沉积物)，并表现出地理位置稳定的性能。在此基础模型上，我们使用匹配的Sentinel-2反射率数据与美国地质调查局(USGS)AquaMatch现场数据集进行叶绿素-a反演，覆盖了多样的地理和水文条件。我们在13626个匹配点上训练了一个XGBoost回归器。GWC正标记的场景持续优于负标记场景，并产生更准确的叶绿素-a反演值，这证实了分类器在减少各种干扰方面的优势。接下来，回归预测的残差分析揭示了结构化误差，促使我们添加了残差CNN(RCNN)校正阶段。我们添加了一个基于归一化残差训练的CNN残差阶段，取得了显著改进。我们的算法在867个水体上进行了测试，超过2000个预测，叶绿素-a值高达1000毫克每立方米，实现了R² = 0.79，平均绝对误差 = 13.52毫克每立方米，斜率 = 0.91，证明了其稳健、可扩展且全球可转移的性能，无需额外调优。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the Global Water Classifier (GWC), a supervised, geospatiallyextensive Machine Learning (ML) classifier trained on Sen2Cor correctedSentinel-2 surface reflectance data. Using nearly 100 globally distributedinland water bodies, GWC distinguishes water across Chlorophyll-a (Chla) levelsfrom non-water spectra (clouds, sun glint, snow, ice, aquatic vegetation, landand sediments) and shows geographically stable performance.  Building on this foundation model, we perform Chla retrieval based on amatchup Sentinel-2 reflectance data with the United States Geological Survey(USGS) AquaMatch in-situ dataset, covering diverse geographical andhydrological conditions.  We train an XGBoost regressor on 13626 matchup points. The positive labeledscenes by the GWC consistently outperform the negatives and produce moreaccurate Chla retrieval values, which confirms the classifiers advantage inreducing various interferences.  Next, residual analysis of the regression predictions revealed structurederrors, motivating a residual CNN (RCNN) correction stage. We add a CNNresidual stage trained on normalized residuals, which yield substantialimprovement. Our algorithm was tested on 867 water bodies with over 2,000predictions and Chla values up to 1000~mg$/m^{3}$, achieving $R^2$ = 0.79, MAE= 13.52~mg$/m^{3}$, and slope = 0.91, demonstrating robust, scalable, andglobally transferable performance without additional tuning.</description>
      <author>example@mail.com (Yotam Sherf, Bar Efrati, Gabriel Rozman, Moshe Harel)</author>
      <guid isPermaLink="false">2510.24124v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>OmniLearned: A Foundation Model Framework for All Tasks Involving Jet Physics</title>
      <link>http://arxiv.org/abs/2510.24066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了OmniLearn基础模型的重大升级，形成了OmniLearned框架。该框架包含模型架构和训练的更新、使用超过十亿个喷注进行训练，以及提供完善的软件访问工具。通过三个代表性任务展示了该框架的性能，结果表明它在所有任务中都是最先进的，显著增强了粒子物理实验的发现潜力。&lt;h4&gt;背景&lt;/h4&gt;基础模型利用大型数据集构建有效的数据表示，可应用于多样化的下游任务。之前开发的OmniLearn基础模型针对粒子物理喷注，利用了粒子物理的独特性质，能够显著增强对撞机实验的发现潜力。&lt;h4&gt;目的&lt;/h4&gt;对现有的OmniLearn基础模型进行重大升级，创建OmniLearned框架，进一步提升其在粒子物理实验中的性能和可用性，扩展过去、当前和未来对撞机实验的发现潜力。&lt;h4&gt;方法&lt;/h4&gt;开发OmniLearned框架，包含三个新元素：更新模型架构和训练方法、使用超过十亿个喷注进行训练、提供完善的软件用于访问所有数据集和模型。通过三个代表性任务进行验证：top夸克喷注标记、b标记和异常检测。&lt;h4&gt;主要发现&lt;/h4&gt;在三个代表性任务（top夸克喷注标记、b标记和异常检测）中，OmniLearned均达到了最先进的性能水平。该框架能够显著增强对撞机实验的发现潜力，包括过去、当前和未来的实验。&lt;h4&gt;结论&lt;/h4&gt;OmniLearned框架代表了基础模型在粒子物理领域的重要进展，通过架构更新、大规模训练和完善的软件工具，显著提升了模型性能，为粒子物理研究提供了更强大的分析工具。&lt;h4&gt;翻译&lt;/h4&gt;基础模型使用大型数据集构建有效的数据表示，可部署在多样化的下游任务中。先前的研究开发了用于粒子物理喷注的OmniLearn基础模型，利用了粒子物理的独特性质，并表明它可以显著增强对撞机实验的发现潜力。本文介绍了一个重大升级，结果是OmniLearned框架。该框架有三个新元素：(1)对模型架构和训练的更新，(2)使用超过十亿个喷注进行训练，(3)提供完善的软件用于访问所有数据集和模型。我们通过三个代表性任务展示了OmniLearned：使用社区Delphes基准数据集进行top夸克喷注标记，使用ATLAS全模拟进行b标记，以及使用CMS实验数据进行异常检测。在每种情况下，OmniLearned都是最先进的，进一步扩展了过去、当前和未来对撞机实验的发现潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models use large datasets to build an effective representation ofdata that can be deployed on diverse downstream tasks. Previous researchdeveloped the OmniLearn foundation model for jet physics, using uniqueproperties of particle physics, and showed that it could significantly advancediscovery potential across collider experiments. This paper introduces a majorupgrade, resulting in the OmniLearned framework. This framework has three newelements: (1) updates to the model architecture and training, (2) using overone billion jets used for training, and (3) providing well-documented softwarefor accessing all datasets and models. We demonstrate OmniLearned with threerepresentative tasks: top-quark jet tagging with the community Delphes-basedbenchmark dataset, b-tagging with ATLAS full simulation, and anomaly detectionwith CMS experimental data. In each case, OmniLearned is the state of the art,further expanding the discovery potential of past, current, and future colliderexperiments.</description>
      <author>example@mail.com (Wahid Bhimji, Chris Harris, Vinicius Mikuni, Benjamin Nachman)</author>
      <guid isPermaLink="false">2510.24066v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Human Machine Social Hybrid Intelligence:A Collaborative Decision Making Framework for Large Model Agent Groups and Human Experts</title>
      <link>http://arxiv.org/abs/2510.24030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新型的人机社会混合智能(HMS-HI)框架，通过共享认知空间、动态角色任务分配和跨物种信任校准三个核心组件，实现了人类专家和AI代理之间的深度协作决策，在应急响应模拟中显著降低了伤亡和认知负荷。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型和多智能体系统快速发展提供了前所未有的能力，但当前人机协同(HiTL)范式未能充分整合人类专业知识，在复杂、高风险环境中常导致认知过载和决策瓶颈。&lt;h4&gt;目的&lt;/h4&gt;设计一种新型架构，用于人类专家群体和基于大语言模型的AI代理之间的深度协作决策，解决现有人机协同方法的不足。&lt;h4&gt;方法&lt;/h4&gt;HMS-HI建立在三个核心支柱上：(1)共享认知空间(SCS)用于统一的多模态态势感知和结构化世界建模；(2)动态角色和任务分配(DRTA)模块根据能力和工作负载将任务自适应分配给最适合的代理；(3)跨物种信任校准(CSTC)协议通过可解释声明和结构化反馈促进透明度、责任和相互适应。&lt;h4&gt;主要发现&lt;/h4&gt;在高保真城市应急响应模拟中，HMS-HI相比传统HiTL方法将平民伤亡减少72%，认知负荷减少70%，证明了卓越的决策质量、效率和人类-AI信任。消融研究确认了每个模块的关键贡献，表明工程化的信任和共享背景是可扩展的人机协作基础。&lt;h4&gt;结论&lt;/h4&gt;HMS-HI框架通过三个核心组件的整合，在复杂、高风险环境中实现了更有效的人机协作决策，显著提高了决策质量和效率，同时减轻了人类认知负担。&lt;h4&gt;翻译&lt;/h4&gt;大型基础模型和多智能体系统的快速发展提供了前所未有的能力，但当前人机协同(HiTL)范式未能充分整合人类专业知识，在复杂、高风险环境中常常导致认知过载和决策瓶颈。我们提出了'人机社会混合智能'(HMS-HI)框架，这是一种专为人类专家群体和由大语言模型驱动的AI代理之间的深度协作决策而设计的新型架构。HMS-HI建立在三个核心支柱上：(1)共享认知空间(SCS)用于统一、多模态的态势感知和结构化世界建模；(2)动态角色和任务分配(DRTA)模块，根据能力和工作负载将任务自适应地分配给最适合的代理(人类或AI)；(3)跨物种信任校准(CSTC)协议，通过可解释声明和结构化反馈促进透明度、责任和相互适应。在高保真的城市应急响应模拟中验证，HMS-HI与传统HiTL方法相比，平民伤亡减少了72%，认知负荷减少了70%，证明了卓越的决策质量、效率和人类-AI信任。消融研究确认了每个模块的关键贡献，强调工程化的信任和共享背景是可扩展的、协同的人机协作的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancements in large foundation models and multi-agent systemsoffer unprecedented capabilities, yet current Human-in-the-Loop (HiTL)paradigms inadequately integrate human expertise, often leading to cognitiveoverload and decision-making bottlenecks in complex, high-stakes environments.We propose the "Human-Machine Social Hybrid Intelligence" (HMS-HI) framework, anovel architecture designed for deep, collaborative decision-making betweengroups of human experts and LLM-powered AI agents. HMS-HI is built upon threecore pillars: (1) a \textbf{Shared Cognitive Space (SCS)} for unified,multi-modal situational awareness and structured world modeling; (2) a\textbf{Dynamic Role and Task Allocation (DRTA)} module that adaptively assignstasks to the most suitable agent (human or AI) based on capabilities andworkload; and (3) a \textbf{Cross-Species Trust Calibration (CSTC)} protocolthat fosters transparency, accountability, and mutual adaptation throughexplainable declarations and structured feedback. Validated in a high-fidelityurban emergency response simulation, HMS-HI significantly reduced civiliancasualties by 72\% and cognitive load by 70\% compared to traditional HiTLapproaches, demonstrating superior decision quality, efficiency, and human-AItrust. An ablation study confirms the critical contribution of each module,highlighting that engineered trust and shared context are foundational forscalable, synergistic human-AI collaboration.</description>
      <author>example@mail.com (Ahmet Akkaya Melih, Yamuna Singh, Kunal L. Agarwal, Priya Mukherjee, Kiran Pattnaik, Hanuman Bhatia)</author>
      <guid isPermaLink="false">2510.24030v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks</title>
      <link>http://arxiv.org/abs/2510.24010v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Mars-Bench，这是第一个用于系统评估火星相关任务的基准，包含20个数据集，涵盖分类、分割和目标检测，专注于关键地质特征。研究结果表明火星特定的基础模型可能比通用领域模型具有优势，为火星科学领域的机器学习模型开发提供了标准化基础。&lt;h4&gt;背景&lt;/h4&gt;基础模型通过大规模无标签数据预训练在许多专业领域取得了快速进展，表现出强大的泛化能力。然而，这类模型在火星科学领域的应用仍然有限，主要原因是火星科学缺乏标准化基准和评估框架，限制了火星任务基础模型的发展。&lt;h4&gt;目的&lt;/h4&gt;引入Mars-Bench，第一个基准，旨在系统评估使用轨道和表面图像的广泛火星相关任务模型，为火星科学领域的机器学习模型开发提供标准化基础。&lt;h4&gt;方法&lt;/h4&gt;Mars-Bench包含20个数据集，涵盖分类、分割和目标检测，专注于陨石坑、锥体、巨石和霜等关键地质特征。提供标准化、即用型数据集和基线评估，使用在自然图像、地球卫星数据和最先进的视觉语言模型上预训练的模型进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;所有分析结果表明，火星特定的基础模型可能比通用领域模型具有优势，这激励了对领域自适应预训练的进一步探索。&lt;h4&gt;结论&lt;/h4&gt;Mars-Bench旨在为开发和比较火星科学的机器学习模型建立标准化基础，其数据、模型和代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;基础模型通过大规模无标签数据预训练在许多专业领域取得了快速进展，显示出对各种下游任务的强大泛化能力。尽管这类模型在地球观测等领域受到广泛关注，但在火星科学领域的应用仍然有限。其他领域取得进展的一个关键因素是标准化基准的可用性，这些基准支持系统评估。相比之下，火星科学缺乏此类基准和标准化评估框架，这限制了火星任务基础模型的发展。为解决这一差距，我们引入了Mars-Bench，这是第一个基准，旨在使用轨道和表面图像系统评估广泛火星相关任务的模型。Mars-Bench包含20个数据集，涵盖分类、分割和目标检测，专注于陨石坑、锥体、巨石和霜等关键地质特征。我们提供了标准化、即用型数据集和基线评估，使用在自然图像、地球卫星数据和最先进的视觉语言模型上预训练的模型。所有分析的结果表明，火星特定的基础模型可能比通用领域对应模型具有优势，这激励了对领域自适应预训练的进一步探索。Mars-Bench旨在为开发和比较火星科学的机器学习模型建立标准化基础。我们的数据、模型和代码可在 https://mars-bench.github.io/ 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have enabled rapid progress across many specialized domainsby leveraging large-scale pre-training on unlabeled data, demonstrating stronggeneralization to a variety of downstream tasks. While such models have gainedsignificant attention in fields like Earth Observation, their application toMars science remains limited. A key enabler of progress in other domains hasbeen the availability of standardized benchmarks that support systematicevaluation. In contrast, Mars science lacks such benchmarks and standardizedevaluation frameworks, which have limited progress toward developing foundationmodels for Martian tasks. To address this gap, we introduce Mars-Bench, thefirst benchmark designed to systematically evaluate models across a broad rangeof Mars-related tasks using both orbital and surface imagery. Mars-Benchcomprises 20 datasets spanning classification, segmentation, and objectdetection, focused on key geologic features such as craters, cones, boulders,and frost. We provide standardized, ready-to-use datasets and baselineevaluations using models pre-trained on natural images, Earth satellite data,and state-of-the-art vision-language models. Results from all analyses suggestthat Mars-specific foundation models may offer advantages over general-domaincounterparts, motivating further exploration of domain-adapted pre-training.Mars-Bench aims to establish a standardized foundation for developing andcomparing machine learning models for Mars science. Our data, models, and codeare available at: https://mars-bench.github.io/.</description>
      <author>example@mail.com (Mirali Purohit, Bimal Gajera, Vatsal Malaviya, Irish Mehta, Kunal Kasodekar, Jacob Adler, Steven Lu, Umaa Rebbapragada, Hannah Kerner)</author>
      <guid isPermaLink="false">2510.24010v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Why Foundation Models in Pathology Are Failing</title>
      <link>http://arxiv.org/abs/2510.23807v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基础模型在非医学领域取得成功，但在计算病理学应用中存在根本性概念不匹配，需要重新思考建模范式&lt;h4&gt;背景&lt;/h4&gt;在非医学领域，基础模型通过大规模自监督和多模态学习彻底改变了计算机视觉和语言处理，预期计算病理学中也会迅速采用这些模型&lt;h4&gt;目的&lt;/h4&gt;检查基础模型在计算病理学中的缺点，论证这些缺点源于通用基础建模假设与人体组织复杂性之间的概念性不匹配&lt;h4&gt;方法&lt;/h4&gt;进行系统评估，识别导致基础模型在计算病理学中失效的七个相互关联原因&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在计算病理学中存在低诊断准确性、鲁棒性差、几何不稳定性、计算需求量大以及安全漏洞等基本弱点&lt;h4&gt;结论&lt;/h4&gt;当前病理学基础模型在概念上与组织形态学性质不一致，需要对范式本身进行根本性重新思考&lt;h4&gt;翻译&lt;/h4&gt;在非医学领域，基础模型(FMs)通过大规模自监督和多模态学习彻底改变了计算机视觉和语言处理。因此，预期计算病理学中会迅速采用这些模型，并在癌症诊断、预后和多模态检索方面取得类似突破。然而，最近的系统评估揭示了基本弱点：低诊断准确性、鲁棒性差、几何不稳定性、计算需求量大，以及令人担忧的安全漏洞。这篇简短论文检查了这些缺点，并论证它们源于主流人工智能中通用基础建模的假设与人体组织内在复杂性之间的更深层次的概念性不匹配。确定了七个相互关联的原因：生物复杂性、无效的自监督、过度概括、过度的架构复杂性、缺乏领域特定创新、数据不足，以及与组织块大小相关的基本设计缺陷。这些发现表明，当前病理学基础模型在概念上仍然与组织形态学的性质不一致，需要对这一范式本身进行根本性的重新思考。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In non-medical domains, foundation models (FMs) have revolutionized computervision and language processing through large-scale self-supervised andmultimodal learning. Consequently, their rapid adoption in computationalpathology was expected to deliver comparable breakthroughs in cancer diagnosis,prognostication, and multimodal retrieval. However, recent systematicevaluations reveal fundamental weaknesses: low diagnostic accuracy, poorrobustness, geometric instability, heavy computational demands, and concerningsafety vulnerabilities. This short paper examines these shortcomings and arguesthat they stem from deeper conceptual mismatches between the assumptionsunderlying generic foundation modeling in mainstream AI and the intrinsiccomplexity of human tissue. Seven interrelated causes are identified:biological complexity, ineffective self-supervision, overgeneralization,excessive architectural complexity, lack of domain-specific innovation,insufficient data, and a fundamental design flaw related to tissue patch size.These findings suggest that current pathology foundation models remainconceptually misaligned with the nature of tissue morphology and call for afundamental rethinking of the paradigm itself.</description>
      <author>example@mail.com (Hamid R. Tizhoosh)</author>
      <guid isPermaLink="false">2510.23807v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting</title>
      <link>http://arxiv.org/abs/2510.23785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了CountFormer，一个基于transformer的框架，用于学习识别重复和结构一致性，实现类别无关的物体计数。该模型使用DINOv2作为视觉编码器，并融入位置嵌入融合，在FSC-147数据集上实现了与当前最先进方法相当的性能，同时在结构复杂或密集场景中表现更优。&lt;h4&gt;背景&lt;/h4&gt;人类能够通过感知视觉重复和结构关系而非依赖类别身份来计数多样化物体，但大多数现有计数模型在物体具有复杂形状、内部对称性或重叠组件时经常计数错误。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够像人类一样通过感知视觉重复和结构关系来进行计数的模型，实现类别无关的物体计数。&lt;h4&gt;方法&lt;/h4&gt;基于CounTR架构，用自监督基础模型DINOv2替换视觉编码器以获得更丰富的特征表示，融入位置嵌入融合保留几何关系，并通过轻量级卷积解码器将特征解码为密度图。&lt;h4&gt;主要发现&lt;/h4&gt;在FSC-147数据集上，CountFormer实现了与当前最先进方法相当的性能，同时在结构复杂或密集堆积的场景中表现出更高的准确性。&lt;h4&gt;结论&lt;/h4&gt;集成基础模型如DINOv2可以使计数系统接近人类的结构感知能力，朝着真正通用和无样本范例的计数范式迈进。&lt;h4&gt;翻译&lt;/h4&gt;人类可以通过感知视觉重复和结构关系而不是依赖类别身份来轻松计数多样化的物体。然而，大多数现有的计数模型无法复制这种能力；当物体表现出复杂形状、内部对称性或重叠组件时，它们经常计数错误。在这项工作中，我们引入了CountFormer，一个基于transformer的框架，用于学习识别重复和结构一致性，实现类别无关的物体计数。基于CounTR架构，我们的模型用自监督基础模型DINOv2替换了其视觉编码器，DINOv2产生更丰富和空间一致的特征表示。我们进一步融合位置嵌入，在通过轻量级卷积解码器将这些特征解码为密度图之前保留几何关系。在FSC-147数据集上评估，我们的模型实现了与当前最先进方法相当的性能，同时在结构复杂或密集堆积的场景中表现出更高的准确性。我们的研究结果表明，集成基础模型如DINOv2可以使计数系统接近人类的结构感知能力，朝着真正通用和无样本范例的计数范式迈进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans can effortlessly count diverse objects by perceiving visual repetitionand structural relationships rather than relying on class identity. However,most existing counting models fail to replicate this ability; they oftenmiscount when objects exhibit complex shapes, internal symmetry, or overlappingcomponents. In this work, we introduce CountFormer, a transformer-basedframework that learns to recognize repetition and structural coherence forclass-agnostic object counting. Built upon the CounTR architecture, our modelreplaces its visual encoder with the self-supervised foundation model DINOv2,which produces richer and spatially consistent feature representations. Wefurther incorporate positional embedding fusion to preserve geometricrelationships before decoding these features into density maps through alightweight convolutional decoder. Evaluated on the FSC-147 dataset, our modelachieves performance comparable to current state-of-the-art methods whiledemonstrating superior accuracy on structurally intricate or densely packedscenes. Our findings indicate that integrating foundation models such as DINOv2enables counting systems to approach human-like structural perception,advancing toward a truly general and exemplar-free counting paradigm.</description>
      <author>example@mail.com (Md Tanvir Hossain, Akif Islam, Mohd Ruhul Ameen)</author>
      <guid isPermaLink="false">2510.23785v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Long-Term Memory for Long-Context Question Answering</title>
      <link>http://arxiv.org/abs/2510.23730v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages including appendix, 3 figures. Submitted to October ARR and  to Metacognition in Generative AI EurIPS workshop (under review for both)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究系统评估了不同类型的记忆增强方法，发现记忆架构复杂度应与模型能力相匹配，不同类型模型适合不同记忆方法，情景记忆可帮助大语言模型识别自身知识局限性。&lt;h4&gt;背景&lt;/h4&gt;大语言模型需要记忆功能实现真正的对话连续性和经验学习，但研究虽已聚焦复杂记忆系统开发，仍不清楚哪种记忆类型对长上下文对话任务最有效。&lt;h4&gt;目的&lt;/h4&gt;使用LoCoMo基准（一个需要多种推理策略的问答任务合成长上下文对话基准）系统评估增强记忆的方法。&lt;h4&gt;方法&lt;/h4&gt;分析四种记忆增强方法：全上下文提示、通过检索增强生成和智能体记忆实现的语义记忆、通过上下文学习实现的情景记忆、通过提示优化的程序记忆。&lt;h4&gt;主要发现&lt;/h4&gt;增强记忆方法可减少90%以上的token使用量同时保持有竞争力准确性；小型基础模型从RAG中获益最多；强大指令微调推理模型通过反思获得情景学习好处并受益于更复杂智能体语义记忆。&lt;h4&gt;结论&lt;/h4&gt;记忆架构复杂度应与模型能力相匹配，情景记忆可以帮助大语言模型识别自身知识的局限性。&lt;h4&gt;翻译&lt;/h4&gt;为了让大语言模型实现真正的对话连续性和从经验学习中受益，它们需要记忆功能。虽然研究已集中在复杂记忆系统的开发上，但目前尚不清楚哪种类型的记忆对长上下文对话任务最有效。我们使用LoCoMo（一个为需要多种推理策略的问答任务标注的合成长上下文对话基准）对增强记忆的方法进行了系统评估。我们分析了全上下文提示、通过检索增强生成和智能体记忆实现的语义记忆、通过上下文学习实现的情景记忆，以及通过提示优化的程序记忆。我们的研究结果表明，增强记忆的方法在保持有竞争力的准确性的同时，可减少90%以上的token使用量。记忆架构的复杂度应与模型能力相匹配，小型基础模型从RAG中获益最多，而强大的指令微调推理模型通过反思获得情景学习的好处，并受益于更复杂的智能体语义记忆。特别是，情景记忆可以帮助大语言模型识别自身知识的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In order for large language models to achieve true conversational continuityand benefit from experiential learning, they need memory. While research hasfocused on the development of complex memory systems, it remains unclear whichtypes of memory are most effective for long-context conversational tasks. Wepresent a systematic evaluation of memory-augmented methods using LoCoMo, abenchmark of synthetic long-context dialogues annotated for question-answeringtasks that require diverse reasoning strategies. We analyse full-contextprompting, semantic memory through retrieval-augmented generation and agenticmemory, episodic memory through in-context learning, and procedural memorythrough prompt optimization. Our findings show that memory-augmented approachesreduce token usage by over 90% while maintaining competitive accuracy. Memoryarchitecture complexity should scale with model capability, with smallfoundation models benefitting most from RAG, and strong instruction-tunedreasoning model gaining from episodic learning through reflections and morecomplex agentic semantic memory. In particular, episodic memory can help LLMsrecognise the limits of their own knowledge.</description>
      <author>example@mail.com (Alessandra Terranova, Björn Ross, Alexandra Birch)</author>
      <guid isPermaLink="false">2510.23730v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents</title>
      <link>http://arxiv.org/abs/2510.23691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Game-TARS是一种基于人类对齐的原生键盘鼠标输入的通用游戏代理，使用统一的可扩展动作空间进行训练，在多种游戏任务上表现出色&lt;h4&gt;背景&lt;/h4&gt;与API或GUI方法不同，需要能够在异构领域(如OS、网络和模拟游戏)进行大规模持续预训练的游戏代理&lt;h4&gt;目的&lt;/h4&gt;开发一种通用游戏代理，通过简单的可扩展动作表示与大规模预训练相结合，实现广泛的计算机使用能力&lt;h4&gt;方法&lt;/h4&gt;Game-TARS在超过500B tokens的多样化轨迹和多模态数据上进行预训练，采用衰减持续损失减少因果混淆，以及高效的稀疏思考策略平衡推理深度和推理成本&lt;h4&gt;主要发现&lt;/h4&gt;在开放世界Minecraft任务上成功率比前SOTA模型高约2倍；在未见过的网络3D游戏中通用性接近新鲜人类；在FPS基准测试中优于GPT-5、Gemini-2.5-Pro和Claude-4-Sonnet；统一动作空间在扩展到跨游戏和多模态数据时能持续改进&lt;h4&gt;结论&lt;/h4&gt;简单的可扩展动作表示与大规模预训练相结合，为具有广泛计算机使用能力的通用代理提供了有前途的发展路径&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Game-TARS，一种通用游戏代理，通过统一的、可扩展的动作空间进行训练，该动作空间锚定在人类对齐的原生键盘鼠标输入上。与基于API或GUI的方法不同，这种范式能够在包括操作系统、网络和模拟游戏在内的异构领域进行大规模持续预训练。Game-TARS在超过500B tokens的多样化轨迹和多模态数据上进行预训练。关键技术包括减少因果混淆的衰减持续损失，以及平衡推理深度和推理成本的高效稀疏思考策略。实验表明，Game-TARS在开放世界Minecraft任务上的成功率比之前的SOTA模型高出约2倍，在未见过的网络3D游戏中接近新鲜人类的通用性，并在FPS基准测试中优于GPT-5、Gemini-2.5-Pro和Claude-4-Sonnet。训练时间和测试时间的扩展结果证实，统一动作空间在扩展到跨游戏和多模态数据时能够持续改进。我们的结果表明，简单的可扩展动作表示与大规模预训练相结合，为具有广泛计算机使用能力的通用代理提供了一条有前途的道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何创建真正可扩展且具有广泛泛化能力的游戏智能体问题。这个问题很重要，因为构建能够与复杂动态数字环境无缝交互的通用人工智能体是实现AGI的关键路径，而视频游戏因其多样化的任务目标和丰富的视觉信息，成为训练和评估此类智能体的理想平台。现有方法在创建具有真正泛化能力的智能体方面仍面临重大挑战，限制了AI系统在开放世界环境中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统智能体的局限性，认识到动作空间与特定环境紧密耦合限制了泛化能力。他们提出将动作空间抽象到更低层次，直接锚定到人类交互的基本输入设备——键盘和鼠标。设计过程借鉴了ReAct范式将推理和动作统一输出，采用Deitke等人的在线思考协议(think-aloud protocol)收集高质量轨迹，使用视觉锚点方法解决多模态数据对齐问题，并在后训练阶段借鉴拒绝采样优化推理-动作链。这些方法基于对现有工作的理解，但进行了创新性整合和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用统一锚定于键盘-鼠标输入的动作空间，结合稀疏思考策略和大规模持续预训练，实现跨领域的泛化能力。整体流程分为两个主要阶段：1)持续预训练阶段：使用统一动作空间收集游戏轨迹，通过在线思考协议收集稀疏ReAct轨迹，使用视觉锚点对齐多模态数据，采用衰减损失函数处理动作分布不平衡，在500B+ token上预训练；2)后训练阶段：通过指令遵循、多模态提示、稀疏思考优化、双层记忆架构和跨域数据整合，提升智能体的核心能力。最终在Minecraft、FPS游戏等未见环境中进行评估验证。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一动作空间：锚定到底层键盘-鼠标输入而非高层API，实现跨环境泛化；2)稀疏思考策略：只在关键决策点推理，通过在线思考协议收集高质量轨迹；3)衰减持续损失：解决动作分布不平衡导致的因果混淆；4)双层记忆架构：结合短期上下文和长期摘要记忆；5)跨域数据整合：将游戏数据与其他领域智能体轨迹结合。相比之前工作，传统API/GUI方法使用定制化动作集与环境紧密耦合，而Game-TARS的统一动作空间具有更好的泛化性；现有游戏智能体通常专注于特定环境，而Game-TARS通过大规模预训练实现了真正的泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Game-TARS通过统一锚定于键盘-鼠标输入的动作空间和大规模持续预训练，实现了在多样化游戏和环境中表现卓越的通用游戏智能体，相比之前的方法展现出显著的泛化能力和性能提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Game-TARS, a generalist game agent trained with a unified,scalable action space anchored to human-aligned native keyboard-mouse inputs.Unlike API- or GUI-based approaches, this paradigm enables large-scalecontinual pre-training across heterogeneous domains, including OS, web, andsimulation games. Game-TARS is pre-trained on over 500B tokens with diversetrajectories and multimodal data. Key techniques include a decaying continualloss to reduce causal confusion and an efficient Sparse-Thinking strategy thatbalances reasoning depth and inference cost. Experiments show that Game-TARSachieves about 2 times the success rate over the previous sota model onopen-world Minecraft tasks, is close to the generality of fresh humans inunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnetin FPS benchmarks. Scaling results on training-time and test-time confirm thatthe unified action space sustains improvements when scaled to cross-game andmultimodal data. Our results demonstrate that simple, scalable actionrepresentations combined with large-scale pre-training provide a promising pathtoward generalist agents with broad computer-use abilities.</description>
      <author>example@mail.com (Zihao Wang, Xujing Li, Yining Ye, Junjie Fang, Haoming Wang, Longxiang Liu, Shihao Liang, Junting Lu, Zhiyong Wu, Jiazhan Feng, Wanjun Zhong, Zili Li, Yu Wang, Yu Miao, Bo Zhou, Yuanfan Li, Hao Wang, Zhongkai Zhao, Faming Wu, Zhengxuan Jiang, Weihao Tan, Heyuan Yao, Shi Yan, Xiangyang Li, Yitao Liang, Yujia Qin, Guang Shi)</author>
      <guid isPermaLink="false">2510.23691v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Explicit Memory through Online 3D Gaussian Splatting Improves Class-Agnostic Video Segmentation</title>
      <link>http://arxiv.org/abs/2510.23521v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE Robotics and Automation Letters September 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用显式3D记忆增强视频分割模型的方法，通过在线3D高斯溅射技术存储预测的对象级片段，显著提高了预测的准确性和一致性。&lt;h4&gt;背景&lt;/h4&gt;现有视频分割算法通常不使用对象级记忆（如FastSAM）或仅使用循环神经网络特征的隐式记忆（如SAM2），而记住过去预测的对象片段位置对提高分割质量至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种显式3D记忆方法来增强现有分割模型，使增强后的模型具有更准确和一致的预测能力。&lt;h4&gt;方法&lt;/h4&gt;开发在线3D高斯溅射（3DGS）技术存储视频过程中生成的预测对象级片段，并基于此开发融合技术FastSAM-Splat和SAM2-Splat，利用显式3DGS记忆改进各自基础模型预测。&lt;h4&gt;主要发现&lt;/h4&gt;消融实验验证了所提技术和超参数设置的有效性；真实世界和模拟基准实验结果表明，使用显式3D记忆的模型比无记忆或仅使用隐式神经网络记忆的模型产生更准确和一致的预测。&lt;h4&gt;结论&lt;/h4&gt;显式3D记忆技术可以显著改善视频分割模型的性能，提高预测的准确性和一致性。&lt;h4&gt;翻译&lt;/h4&gt;记住过去预测的对象片段位置对提高无类别视频分割算法的准确性和一致性是有用的。现有的视频分割算法通常使用不使用对象级记忆（例如FastSAM）或使用循环神经网络特征的隐式记忆（例如SAM2）。在本文中，我们使用显式3D记忆增强这两种分割模型，并证明 resulting 模型具有更准确和一致的预测。为此，我们开发了一种在线3D高斯溅射（3DGS）技术来存储在整个视频持续时间内生成的预测对象级片段。基于这种3DGS表示，开发了一系列融合技术，分别命名为FastSAM-Splat和SAM2-Splat，它们使用显式3DGS记忆来改进各自基础模型的预测。使用消融实验来验证所提技术的设计和超参数设置。来自真实世界和模拟基准实验的结果表明，使用显式3D记忆的模型比不使用记忆或仅使用隐式神经网络记忆的模型产生更准确和一致的预测。项目页面：https://topipari.com/projects/FastSAM-Splat/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决类无关视频分割(class-agnostic video segmentation)的准确性和一致性问题。这个问题在机器人应用中非常重要，因为机器人需要在人类家庭环境中构建有用的语义地图，必须能够检测和分割任何类别的物体（包括部署前未知的物体）。现实环境中的遮挡、低光照、重复和动态物体等因素使得这一挑战更加复杂，而现有的视频分割算法要么不使用物体级记忆，要么使用隐式记忆，导致分割结果在时间维度上不一致，影响机器人对环境的理解和交互。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到记住物体分割在过去的预测位置对提高视频分割一致性很有用，并假设模型如果能访问过去预测的密集3D记忆将会受益。他们借鉴了3D高斯溅射(3DGS)技术，这是一种用于密集3D场景重建的强大表示方法。作者还受到在线3DGS技术的启发，这些技术可以从视频输入中实时构建环境3D地图。他们扩展了3DGS表示，将每个高斯与一个段ID特征向量关联，用于存储语义记忆，并基于对比学习优化段ID码本，确保不同物体段的ID向量之间有足够距离。这种方法结合了现有的视频分割模型(如FastSAM和SAM2)和3D重建技术，创造性地将显式3D记忆引入视频分割任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用显式的3D记忆来增强视频分割模型，以提高分割的一致性和准确性。通过在线3D高斯溅射(3DGS)技术存储物体分割的历史信息，形成一个3D记忆系统，利用这个记忆来指导当前帧的分割预测。整体实现流程包括：1)构建3DGS表示，每个高斯参数包括位置、方向、缩放、不透明度、颜色和段ID特征；2)创建段ID码本，使用对比损失优化确保不同段ID间有足够距离；3)对于FastSAM-Splat，将渲染的3DGS段与FastSAM预测的图像段匹配并融合；4)对于SAM2-Splat，使用SAM2预测的跟踪ID与3DGS段关联，识别不一致并通过重新提示SAM2来纠正错误；5)更新3DGS记忆以对齐当前帧的预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)FastSAM-Splat模型，扩展了原本没有时间记忆的FastSAM，通过集成3DGS记忆提高分割一致性；2)SAM2-Splat模型，开发了基于3DGS的重新提示策略，通过整合显式3D记忆减少SAM2的不一致预测；3)通过实验验证显式3D记忆的优势。相比之前的工作，这种方法使用显式的3D记忆而非无物体级记忆或隐式记忆(如循环神经网络特征)；使用3D高斯溅射存储和表示物体分割历史，这是一种更密集和结构化的表示；专注于机器人应用场景，利用深度和相机姿态信息构建3D记忆；针对不同类型的分割模型(FastSAM和SAM2)设计了不同的融合策略；实验表明在处理遮挡和物体重新出现等挑战性场景时性能更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过引入基于在线3D高斯溅射的显式记忆机制，显著提升了类无关视频分割的准确性和时间一致性，为机器人感知任务提供了更可靠的分割解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3619783&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remembering where object segments were predicted in the past is useful forimproving the accuracy and consistency of class-agnostic video segmentationalgorithms. Existing video segmentation algorithms typically use either noobject-level memory (e.g. FastSAM) or they use implicit memories in the form ofrecurrent neural network features (e.g. SAM2). In this paper, we augment bothtypes of segmentation models using an explicit 3D memory and show that theresulting models have more accurate and consistent predictions. For this, wedevelop an online 3D Gaussian Splatting (3DGS) technique to store predictedobject-level segments generated throughout the duration of a video. Based onthis 3DGS representation, a set of fusion techniques are developed, namedFastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improvetheir respective foundation models' predictions. Ablation experiments are usedto validate the proposed techniques' design and hyperparameter settings.Results from both real-world and simulated benchmarking experiments show thatmodels which use explicit 3D memories result in more accurate and consistentpredictions than those which use no memory or only implicit neural networkmemories. Project Page: https://topipari.com/projects/FastSAM-Splat/</description>
      <author>example@mail.com (Anthony Opipari, Aravindhan K Krishnan, Shreekant Gayaka, Min Sun, Cheng-Hao Kuo, Arnie Sen, Odest Chadwicke Jenkins)</author>
      <guid isPermaLink="false">2510.23521v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Dexbotic: Open-Source Vision-Language-Action Toolbox</title>
      <link>http://arxiv.org/abs/2510.23511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Authors are listed in alphabetical order. The official website is  located at https://dexbotic.com/. Code is available at  https://github.com/Dexmal/dexbotic&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Dexbotic，一个基于PyTorch的开源视觉-语言-行动模型工具箱，为具身智能研究提供一站式服务。&lt;h4&gt;背景&lt;/h4&gt;具身智能领域需要有效的工具来支持视觉-语言-行动模型的研究和开发，现有工具可能缺乏统一性和易用性。&lt;h4&gt;目的&lt;/h4&gt;提供一个开源的、统一的VLA模型工具箱，使研究人员能够轻松复现各种VLA方法，快速开发新实验，并利用更强大的预训练模型提升性能。&lt;h4&gt;方法&lt;/h4&gt;开发了一个基于PyTorch的Dexbotic工具箱，支持多种主流VLA策略，提供实验为中心的开发环境，并开发更强大的预训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;该工具箱能够支持多种VLA策略的统一实现，通过简单的环境设置即可复现各种方法；通过修改Exp脚本可以快速开发新实验；使用更强大的预训练模型可以显著提升最先进VLA策略的性能。&lt;h4&gt;结论&lt;/h4&gt;Dexbotic作为一个开源工具箱，有效简化了VLA模型的研究流程，提高了研究效率，并将持续更新以包含最新的预训练模型和前沿VLA模型。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了Dexbotic，一个基于PyTorch的开源视觉-语言-行动模型工具箱。它旨在为具身智能领域的专业人士提供一站式VLA研究服务。它提供了一个代码库，同时支持多种主流VLA策略，使用户只需通过单一环境设置即可重现各种VLA方法。该工具箱以实验为中心，用户只需修改Exp脚本即可快速开发新的VLA实验。此外，我们提供了更强大的预训练模型，以实现最先进的VLA策略的性能提升。Dexbotic将不断更新，以包含更多最新的预训练基础模型和行业前沿的VLA模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present Dexbotic, an open-source Vision-Language-Action(VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLAresearch service for professionals in the field of embodied intelligence. Itoffers a codebase that supports multiple mainstream VLA policiessimultaneously, allowing users to reproduce various VLA methods with just asingle environment setup. The toolbox is experiment-centric, where the userscan quickly develop new VLA experiments by simply modifying the Exp script.Moreover, we provide much stronger pretrained models to achieve greatperformance improvements for state-of-the-art VLA policies. Dexbotic willcontinuously update to include more of the latest pre-trained foundation modelsand cutting-edge VLA models in the industry.</description>
      <author>example@mail.com (Bin Xie, Erjin Zhou, Fan Jia, Hao Shi, Haoqiang Fan, Haowei Zhang, Hebei Li, Jianjian Sun, Jie Bin, Junwen Huang, Kai Liu, Kaixin Liu, Kefan Gu, Lin Sun, Meng Zhang, Peilong Han, Ruitao Hao, Ruitao Zhang, Saike Huang, Songhan Xie, Tiancai Wang, Tianle Liu, Wenbin Tang, Wenqi Zhu, Yang Chen, Yingfei Liu, Yizhuang Zhou, Yu Liu, Yucheng Zhao, Yunchao Ma, Yunfei Wei, Yuxiang Chen, Ze Chen, Zeming Li, Zhao Wu, Ziheng Zhang, Ziming Liu, Ziwei Yan, Ziyu Zhang)</author>
      <guid isPermaLink="false">2510.23511v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Vulnerability in AI Industry</title>
      <link>http://arxiv.org/abs/2510.23421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preliminary Draft&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种合成AI脆弱性指数(AIVI)，评估Foundation Models(FMs)生产上游价值链的脆弱性，重点关注计算、数据、人才、资本和能源五个关键输入因素。&lt;h4&gt;背景&lt;/h4&gt;Foundation Models(FMs)因Transformer架构的快速发展而推动当前AI生态系统。这些大型模型通过大规模训练和下游适应性获得广泛采用，形成由平台经济学和激烈投资塑造的动荡市场。&lt;h4&gt;目的&lt;/h4&gt;由于数据限制，评估快速发展的AI行业脆弱性具有挑战性。本研究旨在提出一种专注于FM生产上游价值链的合成AI脆弱性指数(AIVI)，优先考虑公开可用数据。&lt;h4&gt;方法&lt;/h4&gt;将FM输出建模为五个输入的函数：计算(Compute)、数据(Data)、人才(Talent)、资本(Capital)和能源(Energy)，假设任何输入的供应脆弱性都会威胁整个行业。使用加权几何平均数聚合子指数，并使用理论或经验基准进行归一化。&lt;h4&gt;主要发现&lt;/h4&gt;关键脆弱性包括：计算集中、数据稀缺和法律风险、人才瓶颈、资本密集度和战略依赖性，以及不断增长的能源需求。&lt;h4&gt;结论&lt;/h4&gt;尽管存在局限性和改进空间，但这一初步指数旨在量化AI核心生产引擎中的系统性风险，并间接揭示下游价值链的风险。&lt;h4&gt;翻译&lt;/h4&gt;Foundation Models(FMs)的快速发展，得益于Transformer架构，推动了当前的AI生态系统。这些大型模型以大规模训练和下游适应性为特征（如GPT系列），已获得广泛采用，促成了由平台经济学和激烈投资塑造的动荡市场。由于数据限制，评估这个快速发展的行业的脆弱性至关重要且具有挑战性。本文提出了一种专注于FM生产上游价值链的合成AI脆弱性指数(AIVI)，优先考虑公开可用数据。我们将FM输出建模为五个输入的函数：计算、数据、人才、资本和能源，并假设任何输入的供应脆弱性都会威胁整个行业。主要脆弱性包括计算集中、数据稀缺和法律风险、人才瓶颈、资本密集度和战略依赖性，以及不断增长的能源需求。考虑到输入的不完全可替代性，我们提出使用加权几何平均数来聚合子指数，并使用理论或经验基准进行归一化。尽管存在局限性和改进空间，但这一初步指数旨在量化AI核心生产引擎中的系统性风险，并间接揭示了下游价值链的风险。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid ascent of Foundation Models (FMs), enabled by the Transformerarchitecture, drives the current AI ecosystem. Characterized by large-scaletraining and downstream adaptability, FMs (as GPT family) have achieved massivepublic adoption, fueling a turbulent market shaped by platform economics andintense investment. Assessing the vulnerability of this fast-evolving industryis critical yet challenging due to data limitations. This paper proposes asynthetic AI Vulnerability Index (AIVI) focusing on the upstream value chainfor FM production, prioritizing publicly available data. We model FM output asa function of five inputs: Compute, Data, Talent, Capital, and Energy,hypothesizing that supply vulnerability in any input threatens the industry.Key vulnerabilities include compute concentration, data scarcity and legalrisks, talent bottlenecks, capital intensity and strategic dependencies, aswell as escalating energy demands. Acknowledging imperfect inputsubstitutability, we propose a weighted geometrical average of aggregatesubindexes, normalized using theoretical or empirical benchmarks. Despitelimitations and room for improvement, this preliminary index aims to quantifysystemic risks in AI's core production engine, and implicitly shed a light onthe risks for downstream value chain.</description>
      <author>example@mail.com (Claudio Pirrone, Stefano Fricano, Gioacchino Fazio)</author>
      <guid isPermaLink="false">2510.23421v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Towards Generalisable Foundation Models for 3D Brain MRI</title>
      <link>http://arxiv.org/abs/2510.23415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BrainFound是一种自监督基础模型，通过扩展DINO-v2视觉转换器构建，专为脑部MRI设计，能够处理3D脑部解剖信息，支持单模态和多模态输入，在标签稀缺和多对比度环境下表现优异，提高了诊断准确性并减少了对专家标注的依赖。&lt;h4&gt;背景&lt;/h4&gt;人工智能基础模型通过大规模无标签数据集实现通用特征学习，正在改变医学影像领域。&lt;h4&gt;目的&lt;/h4&gt;开发一个针对脑部MRI的自监督基础模型，能够处理3D脑部解剖信息，支持多种下游任务。&lt;h4&gt;方法&lt;/h4&gt;通过扩展DINO-v2视觉转换器构建BrainFound，整合连续MRI切片的体积信息来建模完整3D脑部解剖结构，支持单模态和多模态输入，适用于多种MRI模态（如T1、T2、FLAIR）。&lt;h4&gt;主要发现&lt;/h4&gt;BrainFound在性能上始终优于现有的自监督预训练策略和监督基线，特别是在标签稀缺和多对比度设置下；通过整合多种3D MRI模态信息，提高了诊断准确性，减少了对大量专家标注的依赖。&lt;h4&gt;结论&lt;/h4&gt;BrainFound的灵活性使其成为3D神经影像流程的可扩展且实用的解决方案，具有在临床部署和研究创新方面的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;人工智能（AI）中的基础模型正在通过大规模无标签数据集实现通用特征学习，从而改变医学影像。在这项工作中，我们介绍了BrainFound，这是一个用于脑部MRI的自监督基础模型，通过扩展DINO-v2（一种最初为2D自然图像设计的视觉转换器）构建。BrainFound通过整合连续MRI切片的体积信息来适应DINO-v2，以建模完整的3D脑部解剖结构，超越了传统的单切片范式。它支持单模态和多模态输入，能够实现广泛的下游任务，包括疾病检测和图像分割，同时能够在不同的成像协议和临床场景中泛化。我们证明BrainFound在性能上始终优于现有的自监督预训练策略和监督基线，特别是在标签稀缺和多对比度设置下。通过整合多种3D MRI模态（如T1、T2、FLAIR）的信息，它提高了诊断准确性，减少了对大量专家标注的依赖。这种灵活性使BrainFound成为3D神经影像流程的可扩展且实用的解决方案，在临床部署和研究创新方面具有巨大潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何构建通用的基础模型用于3D脑部MRI分析的问题。这个问题在现实中非常重要，因为放射科医生面临巨大工作压力（平均每3-4秒需解读一张图像），导致诊断延迟和错误；深度学习在放射学领域虽潜力巨大，但成功依赖于大量昂贵耗时的标记数据；现有监督模型难以跨领域泛化；而大多数基础模型是为2D自然图像设计，无法有效处理3D医学影像数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了DINO-v2框架（一种为2D自然图像设计的视觉变换器）和自监督学习方法，特别是对比学习和知识蒸馏。设计思路是将DINO-v2从2D扩展到3D，通过处理3D扫描作为2D切片序列；设计支持单模态和多模态MRI输入的架构；将T1、T2和FLAIR扫描堆叠为通道输入（类似RGB图像）；采用多尺度裁剪策略捕获全局和局部脑结构；使用双域预训练（先在自然图像上预训练，再在脑部MRI上微调）。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D脑部MRI作为2D切片序列处理，利用自监督学习在大规模未标记数据上学习通用特征表示，结合自然图像和脑部MRI的双域预训练，并支持多模态输入整合不同MRI对比度的互补信息。整体流程包括：1)收集10,000个体积脑部MRI图像并进行标准化预处理；2)基于DINO-v2构建支持多模态输入的Vision Transformer架构；3)使用多尺度裁剪和自监督知识蒸馏方法进行预训练；4)在下游任务（疾病检测和图像分割）上进行微调应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将DINO-v2从2D扩展到3D脑部MRI处理；2)支持可变体积深度，提高对不同MRI任务的适应性；3)设计能处理单模态和多模态MRI输入的架构，将不同MRI模态堆叠为通道输入；4)采用双域预训练策略，结合自然图像和脑部MRI的优势；5)统一处理疾病检测和图像分割任务。相比之前的工作，BrainFound在多种任务上表现优于仅使用自然图像预训的模型、仅在脑部图像上从头训练的自监督模型以及其他自监督方法，其多模态设计也提供了比单模态模型更强的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BrainFound通过将DINO-v2框架扩展到3D脑部MRI并采用双域预训练策略，创建了一个强大的自监督基础模型，能够有效处理多模态输入并在疾病检测和图像分割任务上实现卓越的泛化性能，减少了对大量标记数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models in artificial intelligence (AI) are transforming medicalimaging by enabling general-purpose feature learning from large-scale,unlabeled datasets. In this work, we introduce BrainFound, a self-supervisedfoundation model for brain MRI, built by extending DINO-v2, a visiontransformer originally designed for 2D natural images. BrainFound adaptsDINO-v2 to model full 3D brain anatomy by incorporating volumetric informationfrom sequential MRI slices, moving beyond conventional single-slice paradigms.It supports both single- and multimodal inputs, enabling a broad range ofdownstream tasks, including disease detection and image segmentation, whilegeneralising across varied imaging protocols and clinical scenarios. We showthat BrainFound consistently outperforms existing self-supervised pretrainingstrategies and supervised baselines, particularly in label-scarce andmulti-contrast settings. By integrating information from diverse 3D MRImodalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reducesdependency on extensive expert annotations. This flexibility makes BrainFound ascalable and practical solution for 3D neuroimaging pipelines, with significantpotential for clinical deployment and research innovation.</description>
      <author>example@mail.com (Moona Mazher, Geoff J. M. Parker, Daniel C. Alexander)</author>
      <guid isPermaLink="false">2510.23415v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Bid2X: Revealing Dynamics of Bidding Environment in Online Advertising from A Foundation Model Lens</title>
      <link>http://arxiv.org/abs/2510.23410v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Bid2X出价基础模型，通过统一的函数估计特定出价下的效果，解决了传统出价模型在不同场景间泛化能力有限的问题。该模型结合了序列嵌入、双注意力机制和零膨胀投影模块，在淘宝广告平台部署后显著提升了广告效果。&lt;h4&gt;背景&lt;/h4&gt;自动出价对在线广告至关重要，但现有出价模型通常针对特定场景设计，在不同环境间的泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;探索场景无关的出价原则，提出一个统一的函数来估计特定出价下的效果，如预算消耗、商品交易总额(GMV)和页面浏览量等。&lt;h4&gt;方法&lt;/h4&gt;提出Bid2X出价基础模型，构建在统一序列嵌入之上，通过定制嵌入方法编码异构数据；提出两种注意力机制分别处理不同变量和不同时间的嵌入；使用变量感知融合模块进行自适应出价结果预测；设计零膨胀投影模块将估计的非零概率纳入值预测，形成包含分类和回归的联合优化目标。&lt;h4&gt;主要发现&lt;/h4&gt;模型已在淘宝广告平台部署；在八个数据集上的离线评估显示Bid2X优于各种基线模型且在不同场景中具有通用性；在线A/B测试中GMV增加4.65%，ROI增加2.44%。&lt;h4&gt;结论&lt;/h4&gt;Bid2X为计算广告中的出价基础模型铺平了道路，展示了基础模型在广告出价领域的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;自动出价对于通过为广告商自动提供出价来促进在线广告至关重要。虽然之前的工作在建模出价环境以获得更好的广告效果方面做出了巨大努力，但这些模型通常针对特定的出价场景定制，在不同环境间的泛化能力存在局限性。为此，我们通过一个统一的函数来探索场景无关的原则，该函数估计特定出价下的效果，如预算消耗、商品交易总额(GMV)、页面浏览量等。然后，我们提出了Bid2X出价基础模型，从各种场景的数据中学习这个基本函数。我们的Bid2X构建在统一序列嵌入之上，通过定制的嵌入方法编码异构数据。为了捕捉出价数据中复杂的变量间动态和时间依赖关系，我们提出了两种注意力机制，分别将不同变量和不同时间的嵌入作为注意力令牌进行表示学习。在学习到的变量和时间表示之上，使用变量感知融合模块进行自适应出价结果预测。为了建模独特的出价数据分布，我们设计了一个零膨胀投影模块，将估计的非零概率纳入其值预测，这构成了一个包含分类和回归的联合优化目标。该目标被证明可以收敛到零膨胀分布。我们的模型已部署在淘宝广告平台上，这是世界上最大的电子商务平台之一。在八个数据集上的离线评估显示，与各种基线相比，Bid2X具有优越性，并且在不同场景中具有通用性。Bid2X在线A/B测试中使GMV增加了4.65%，ROI增加了2.44%，为计算广告中的出价基础模型铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Auto-bidding is crucial in facilitating online advertising by automaticallyproviding bids for advertisers. While previous work has made great efforts tomodel bidding environments for better ad performance, it has limitations ingeneralizability across environments since these models are typically tailoredfor specific bidding scenarios. To this end, we approach thescenario-independent principles through a unified function that estimates theachieved effect under specific bids, such as budget consumption, grossmerchandise volume (GMV), page views, etc. Then, we propose a biddingfoundation model Bid2X to learn this fundamental function from data in variousscenarios. Our Bid2X is built over uniform series embeddings that encodeheterogeneous data through tailored embedding methods. To capture complexinter-variable and dynamic temporal dependencies in bidding data, we proposetwo attention mechanisms separately treating embeddings of different variablesand embeddings at different times as attention tokens for representationlearning. On top of the learned variable and temporal representations, avariable-aware fusion module is used to perform adaptive bidding outcomeprediction. To model the unique bidding data distribution, we devise azero-inflated projection module to incorporate the estimated non-zeroprobability into its value prediction, which makes up a joint optimizationobjective containing classification and regression. The objective is proven toconverge to the zero-inflated distribution. Our model has been deployed on thead platform in Taobao, one of the world's largest e-commerce platforms. Offlineevaluation on eight datasets exhibits Bid2X's superiority compared to variousbaselines and its generality across different scenarios. Bid2X increased GMV by4.65% and ROI by 2.44% in online A/B tests, paving the way for biddingfoundation model in computational advertising.</description>
      <author>example@mail.com (Jiahao Ji, Tianyu Wang, Yeshu Li, Yushen Huo, Zhilin Zhang, Chuan Yu, Jian Xu, Bo Zheng)</author>
      <guid isPermaLink="false">2510.23410v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Solar flare forecasting with foundational transformer models across image, video, and time-series modalities</title>
      <link>http://arxiv.org/abs/2510.23400v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了基于Transformer的架构在利用异构数据模态（包括图像、视频序列和时间序列观测）进行太阳耀斑预测方面的性能&lt;h4&gt;背景&lt;/h4&gt;太阳活动预测对于空间天气预警至关重要，但需要处理多种类型的数据&lt;h4&gt;目的&lt;/h4&gt;评估Transformer主干架构在太阳活动的空间和时间表示方面的优势和局限性&lt;h4&gt;方法&lt;/h4&gt;使用三种预训练模型（SigLIP2用于图像编码，VideoMAE用于时空视频表示，Moirai2用于多元时间序列预测）处理来自SDO/HMI任务的太阳磁图和GOES卫星的软X射线通量数据，并采用多种损失函数和训练平衡策略来处理类别不平衡问题&lt;h4&gt;主要发现&lt;/h4&gt;虽然SigLIP2和VideoMAE在图像和视频数据上达到典型性能（真实技能统计约0.60-0.65），但仅基于辐照度时间演化的Moirai2时间序列模型达到了优越的预测技能（真实技能统计约0.74）&lt;h4&gt;结论&lt;/h4&gt;预训练Transformer架构和跨模态学习对推进业务空间天气预报具有潜力，为整合视觉和时间信息的统一多模态模型铺平了道路&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一项基于Transformer架构的太阳耀斑预测的比较研究，该研究使用异构数据模态，包括图像、视频序列和时间序列观测。我们的分析评估了三个最近的基础模型 - 用于图像编码的SigLIP2，用于时空视频表示的VideoMAE，以及用于多元时间序列预测的Moirai2 - 应用于来自SDO/HMI任务的太阳磁图公共数据集和GOES卫星获取的软X射线通量。所有模型在一致的数据分割和评估标准下进行训练和验证，旨在评估Transformer主干架构在太阳活动的空间和时间表示方面的优势和局限性。我们研究了多种损失公式（加权BCE、focal和分数导向的）和训练平衡策略，以减轻耀斑数据集中典型的类别不平衡。结果表明，虽然SigLIP2和VideoMAE在图像和视频数据上达到典型性能（真实技能统计约0.60-0.65），但时间序列模型Moirai2仅基于辐照度时间演化就达到了优越的预测技能（真实技能统计约0.74）。这些发现突显了预训练Transformer架构和跨模态学习在推进业务空间天气预报方面的潜力，为整合视觉和时间信息的统一多模态模型铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a comparative study of transformer-based architectures for solarflare forecasting using heterogeneous data modalities, including images, videosequences, and time-series observations. Our analysis evaluates three recentfoundational models - SigLIP2 for image encoding, VideoMAE for spatio-temporalvideo representation, and Moirai2 for multivariate time-series forecasting -applied to publicly available datasets of solar magnetograms from the SDO/HMImission and soft X-ray fluxes acquired by GOES satellites. All models aretrained and validated under consistent data splits and evaluation criteria,with the goal of assessing the strengths and limitations of transformerbackbones across spatial and temporal representations of solar activity. Weinvestigate multiple loss formulations (weighted BCE, focal, andscore-oriented) and training balance strategies to mitigate class imbalancetypical of flare datasets. Results show that while both SigLIP2 and VideoMAEachieve typical performance on image and video data (True Skill StatisticTSS~0.60-0.65), the time-series model Moirai2 reaches superior forecastingskill (TSS~0.74) using irradiance-based temporal evolution alone. Thesefindings highlight the potential of pretrained transformer architectures andcross-modal learning for advancing operational space weather forecasting,paving the way toward unified multimodal models that integrate visual andtemporal information.</description>
      <author>example@mail.com (S. Riggi, P. Romano, A. Pilzer, U. Becciani)</author>
      <guid isPermaLink="false">2510.23400v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility Mapping</title>
      <link>http://arxiv.org/abs/2510.23364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint submitted to EUSAR 2026 (under review)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ZeroFlood是一种地理空间基础模型框架，通过思维模态推理实现数据高效的洪水易感性映射，能够在数据稀缺地区从基本地球观测数据进行洪水预测。&lt;h4&gt;背景&lt;/h4&gt;洪水易感性映射对于灾害预防至关重要，但在数据稀缺地区面临挑战，因为传统水动力模型需要密集的地球物理输入数据。&lt;h4&gt;目的&lt;/h4&gt;开发ZeroFlood框架，解决数据稀缺地区洪水易感性映射的问题，提供一种数据高效的解决方案。&lt;h4&gt;方法&lt;/h4&gt;使用思维模态(TiM)推理微调地理空间基础模型(GFMs)，从Sentinel-1或Sentinel-2等基本地球观测数据进行洪水预测；利用数据丰富地区的成对地球观测和模拟洪水地图，通过跨模态表示学习弥合数据差距；使用TerraMind和Prithvi GFMs进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;TiM推理增强了模型的鲁棒性，TerraMind-Large配置实现了67.21的F1分数。&lt;h4&gt;结论&lt;/h4&gt;基于基础模型的FSM是一种可扩展且数据高效的洪水风险管理解决方案，适用于数据稀缺地区。&lt;h4&gt;翻译&lt;/h4&gt;洪水易感性映射(FSM)对灾害预防至关重要，但在需要密集地球物理输入的水动力模型难以应用的稀缺数据地区仍然具有挑战性。本文介绍了ZeroFlood，一种用于数据高效FSM的地理空间基础模型框架。该方法通过思维模态(TiM)推理微调地理空间基础模型(GFMs)，能够从Sentinel-1或Sentinel-2等基本地球观测数据进行洪水预测。利用数据丰富地区的成对地球观测和模拟洪水地图，ZeroFlood通过跨模态表示学习弥合了数据可用性差距。使用TerraMind和Prithvi GFMs的实验表明，TiM增强了模型鲁棒性，其中TerraMind-Large配置实现了67.21的F1分数。结果证明了基于基础模型的FSM作为可扩展和数据高效的洪水风险管理解决方案的可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Flood susceptibility mapping (FSM) is vital for disaster prevention butremains challenging in data-scarce regions where hydrodynamic models requiredense geophysical inputs. This work introduces ZeroFlood, a geospatialfoundation model framework for data-efficient FSM. The approach fine-tunesGeospatial Foundation Models (GFMs) with Thinking-in-Modality (TiM) reasoning,enabling flood prediction from basic Earth observation data such as Sentinel-1or Sentinel-2 imagery. Using paired EO and simulated flood maps from data-richregions, ZeroFlood bridges data availability gaps through cross-modalrepresentation learning. Experiments with TerraMind and Prithvi GFMs show thatTiM enhances model robustness, with the TerraMind-Large configuration achievingan F1 score of 67.21. The results demonstrate the feasibility offoundation-model-based FSM as a scalable and data-efficient solution for floodrisk management.</description>
      <author>example@mail.com (Hyeongkyun Kim, Orestis Oikonomou)</author>
      <guid isPermaLink="false">2510.23364v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Provable test-time adaptivity and distributional robustness of in-context learning</title>
      <link>http://arxiv.org/abs/2510.23254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  44 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了预训练Transformer在不同难度任务上的性能表现，证明其能够达到与任务难度相对应的最优收敛速率，并且对分布偏移具有鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;研究上下文学习问题，其中Transformer在从混合分布中抽取的任务上进行预训练，该混合分布由不同难度级别的任务分布组成。&lt;h4&gt;目的&lt;/h4&gt;理解预训练Transformer在不同于预训练分布的测试分布上的性能，特别是当测试分布与预训练分布中对应难度级别的分布存在卡方散度限制的偏移时。&lt;h4&gt;方法&lt;/h4&gt;考虑非参数回归问题和多指标模型，分析大型预训练Transformer在这些模型上的收敛性能。&lt;h4&gt;主要发现&lt;/h4&gt;预训练Transformer能够达到与任务难度级别相对应的最优收敛速率，并且在卡方散度球内的测试分布上是一致的；Transformer在较容易任务上收敛更快，且对分布偏移具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;预训练Transformer即使面对分布偏移也能保持最优性能，其性能优于理论上能够访问测试分布的估计器，提供了比最小最大下界更合适的最优性保证。&lt;h4&gt;翻译&lt;/h4&gt;我们研究上下文学习问题，其中Transformer在从混合分布中抽取的任务上进行预训练，称为预训练先验，其中每个混合分量是针对特定难度级别的任务分布。我们的目标是理解预训练Transformer在不同于测试分布上的性能表现，该测试分布由固定难度的任务组成，并且相对于对应难度级别的分布可能存在分布偏移，但卡方散度至多为某个常数。特别是，我们考虑具有随机光滑性的非参数回归问题，以及具有随机光滑性和随机有效维度的多指标模型。我们证明，在足够数据上预训练的大型Transformer能够达到与难度级别相对应的最优收敛速率，并且在卡方散度球内的测试分布上是一致的。因此，预训练的Transformer能够在较容易的任务上实现更快的收敛速率，并且对测试时的分布偏移具有鲁棒性。最后，我们证明即使估计器能够访问测试分布，其在测试分布上的期望风险的收敛速率也不会比预训练的Transformer更快，从而提供了比最小最大下界更合适的最优性保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study in-context learning problems where a Transformer is pretrained ontasks drawn from a mixture distribution $\pi=\sum_{\alpha\in\mathcal{A}}\lambda_{\alpha} \pi_{\alpha}$, called the pretraining prior, in which eachmixture component $\pi_{\alpha}$ is a distribution on tasks of a specificdifficulty level indexed by $\alpha$. Our goal is to understand the performanceof the pretrained Transformer when evaluated on a different test distribution$\mu$, consisting of tasks of fixed difficulty $\beta\in\mathcal{A}$, and withpotential distribution shift relative to $\pi_\beta$, subject to thechi-squared divergence $\chi^2(\mu,\pi_{\beta})$ being at most $\kappa$. Inparticular, we consider nonparametric regression problems with randomsmoothness, and multi-index models with random smoothness as well as randomeffective dimension. We prove that a large Transformer pretrained on sufficientdata achieves the optimal rate of convergence corresponding to the difficultylevel $\beta$, uniformly over test distributions $\mu$ in the chi-squareddivergence ball. Thus, the pretrained Transformer is able to achieve fasterrates of convergence on easier tasks and is robust to distribution shift attest time. Finally, we prove that even if an estimator had access to the testdistribution $\mu$, the convergence rate of its expected risk over $\mu$ couldnot be faster than that of our pretrained Transformers, thereby providing amore appropriate optimality guarantee than minimax lower bounds.</description>
      <author>example@mail.com (Tianyi Ma, Tengyao Wang, Richard J. Samworth)</author>
      <guid isPermaLink="false">2510.23254v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?</title>
      <link>http://arxiv.org/abs/2510.23252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This manuscript contains 11 pages, 5 tables and 16 figures This was  accepted at International Joint Conference on Natural Language Processing &amp;  Asia-Pacific Chapter of the Association for Computational Linguistics  (IJCNLP-AACL) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了名为Ben-10的孟加拉语音转文本语料库，研究了方言变化对自动语音识别(ASR)的影响，发现语音基础模型在区域方言ASR中表现不佳，但方言特定模型训练可缓解此问题。&lt;h4&gt;背景&lt;/h4&gt;传统语音识别研究大多使用标准形式处理低资源语言，而区域方言的自动语音识别(ASR)被视为微调任务。&lt;h4&gt;目的&lt;/h4&gt;研究方言变化对自动语音识别(ASR)的影响。&lt;h4&gt;方法&lt;/h4&gt;开发了一个78小时标注的孟加拉语音转文本(STT)语料库，命名为Ben-10，并从语言学和数据驱动角度进行研究。&lt;h4&gt;主要发现&lt;/h4&gt;语音基础模型在区域方言ASR中表现不佳，无论是零样本还是微调设置；所有深度学习方法都难以在方言变化条件下建模语音数据，但方言特定的模型训练可以缓解这一问题。&lt;h4&gt;结论&lt;/h4&gt;该数据集可作为ASR算法在资源受限条件下建模的分布外(OOD)资源。&lt;h4&gt;翻译&lt;/h4&gt;传统语音识别建模研究大多依赖标准形式处理大多数低资源语言，而区域方言的自动语音识别(ASR)被视为微调任务。为研究对方言变化对ASR的影响，我们开发了一个名为Ben-10的78小时标注的孟加拉语音转文本(STT)语料库。从语言学和数据驱动角度的研究表明，语音基础模型在区域方言ASR中表现严重不佳，无论是在零样本还是微调设置下。我们观察到所有深度学习方法都难以在方言变化条件下建模语音数据，但方言特定的模型训练可以缓解这一问题。我们的数据集也可作为ASR算法在资源受限条件下建模的分布外(OOD)资源。该项目开发的数据集和代码已公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional research on speech recognition modeling relies on the canonicalform for most low-resource languages while automatic speech recognition (ASR)for regional dialects is treated as a fine-tuning task. To investigate theeffects of dialectal variations on ASR we develop a 78-hour annotated BengaliSpeech-to-Text (STT) corpus named Ben-10. Investigation from linguistic anddata-driven perspectives shows that speech foundation models struggle heavilyin regional dialect ASR, both in zero-shot and fine-tuned settings. We observethat all deep learning methods struggle to model speech data under dialectalvariations but dialect specific model training alleviates the issue. Ourdataset also serves as a out of-distribution (OOD) resource for ASR modelingunder constrained resources in ASR algorithms. The dataset and code developedfor this project are publicly available</description>
      <author>example@mail.com (Tawsif Tashwar Dipto, Azmol Hossain, Rubayet Sabbir Faruque, Md. Rezuwan Hassan, Kanij Fatema, Tanmoy Shome, Ruwad Naswan, Md. Foriduzzaman Zihad, Mohaymen Ul Anam, Nazia Tasnim, Hasan Mahmud, Md Kamrul Hasan, Md. Mehedi Hasan Shawon, Farig Sadeque, Tahsin Reasat)</author>
      <guid isPermaLink="false">2510.23252v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Finding 3D Scene Analogies with Multimodal Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to FM4RoboPlan workshop at RSS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出使用多模态基础模型在零样本、开放词汇设置下寻找3D场景类比，通过混合神经表示实现复杂场景间的准确对应关系，支持轨迹和航路点转移。&lt;h4&gt;背景&lt;/h4&gt;将当前观察与先验经验连接有助于机器人在新3D环境中适应和规划。3D场景类比可作为平滑映射对齐具有共同空间关系的场景区域，支持轨迹或航路点转移，可用于模仿学习示范转移或跨场景任务规划。&lt;h4&gt;目的&lt;/h4&gt;提出使用多模态基础模型在零样本、开放词汇设置下寻找3D场景类比，避免现有方法需要的额外训练和固定物体词汇表限制。&lt;h4&gt;方法&lt;/h4&gt;采用混合神经表示场景，包括基于视觉语言模型特征的稀疏图和来自3D形状基础模型的特征场。通过粗到细方式寻找3D场景类比，首先对齐图，然后用特征场细化对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够建立复杂场景之间的准确对应关系，并成功应用于轨迹和航路点转移。&lt;h4&gt;结论&lt;/h4&gt;使用多模态基础模型可以在无需额外训练和固定词汇表的情况下实现3D场景类比，为机器人在新环境中的适应和规划提供了有效方法。&lt;h4&gt;翻译&lt;/h4&gt;将当前观察与先验经验连接起来有助于机器人在新的、未见过的3D环境中进行适应和规划。最近，3D场景类比被提出用于连接两个3D场景，这些是平滑的映射，能够对齐具有共同空间关系的场景区域。这些映射可以支持轨迹或航路点的详细转移，可能支持模仿学习的示范转移或跨场景的任务规划转移。然而，现有方法需要额外的训练和固定的物体词汇表。在本文中，我们提出使用多模态基础模型在零样本、开放词汇设置下寻找3D场景类比。我们方法的核心是场景的混合神经表示，包括基于视觉语言模型特征的稀疏图和来自3D形状基础模型的特征场。3D场景类比通过粗到细的方式找到，首先对齐图，然后使用特征场细化对应关系。我们的方法能够建立复杂场景之间的准确对应关系，我们展示了在轨迹和航路点转移中的应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的是如何在没有额外训练和固定词汇表限制的情况下，找到3D场景之间的类比关系。这个问题很重要，因为它能帮助机器人将新环境与已知经验联系起来，从而在未知环境中更好地进行规划和行动。3D场景类比可以创建场景间的平滑映射，支持轨迹或路径点的转移，可用于模仿学习或跨场景的任务规划。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到现有方法要么需要特定领域训练（神经描述符场），要么依赖语义标签（场景图匹配），限制了泛化能力。因此，作者转向利用已在大量多模态数据上训练的基础模型。方法借鉴了视觉语言模型（CLIP）提取对象特征、3D形状模型（PartField）构建特征场、图匹配技术、DBSCAN聚类和薄板样条拟合等现有技术，但将它们创新地组合成一个新的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用多模态基础模型在零样本、开放词汇表设置下寻找3D场景类比，通过混合神经表示（稀疏图+密集特征场）实现从粗到细的场景类比估计。流程包括：1)构建场景图（对象节点+CLIP特征）和特征场（PartField特征）；2)图匹配获得粗粒度对象关联；3)DBSCAN聚类并拟合仿射映射；4)基于特征场优化局部位移；5)用薄板样条拟合得到最终映射。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用多模态基础模型实现零样本3D场景类比；2)提出混合神经表示方法结合稀疏图和密集场；3)采用从粗到细的估计策略；4)支持开放词汇表场景。相比之前工作，不同之处在于：无需特定领域训练（优于神经场景图方法）、不需要预知语义标签（优于场景图匹配方法）、能处理复杂场景且映射准确性更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于多模态基础模型的3D场景类比方法，通过结合视觉语言和3D形状特征的混合表示，实现了零样本、开放词汇表场景下的高精度场景映射，为机器人规划和模仿学习提供了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Connecting current observations with prior experiences helps robots adapt andplan in new, unseen 3D environments. Recently, 3D scene analogies have beenproposed to connect two 3D scenes, which are smooth maps that align sceneregions with common spatial relationships. These maps enable detailed transferof trajectories or waypoints, potentially supporting demonstration transfer forimitation learning or task plan transfer across scenes. However, existingmethods for the task require additional training and fixed object vocabularies.In this work, we propose to use multimodal foundation models for finding 3Dscene analogies in a zero-shot, open-vocabulary setting. Central to ourapproach is a hybrid neural representation of scenes that consists of a sparsegraph based on vision-language model features and a feature field derived from3D shape foundation models. 3D scene analogies are then found in acoarse-to-fine manner, by first aligning the graph and refining thecorrespondence with feature fields. Our method can establish accuratecorrespondences between complex scenes, and we showcase applications intrajectory and waypoint transfer.</description>
      <author>example@mail.com (Junho Kim, Young Min Kim)</author>
      <guid isPermaLink="false">2510.23184v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Guiding Skill Discovery with Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23167v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Foundation model Guided (FoG)的技能发现方法，通过基础模型将人类意图融入技能发现过程，解决了现有方法只关注技能多样性而忽略人类偏好导致不理想行为的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的技能发现方法仅专注于最大化技能多样性，不考虑人类偏好，这会导致不理想甚至危险的行为。例如，使用先前方法训练的猎豹机器人学会向各个方向翻滚以最大化技能多样性，而非我们期望的奔跑行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将人类意图融入技能发现过程的方法，从而学习符合人类偏好的多样化技能，避免不理想和危险行为。&lt;h4&gt;方法&lt;/h4&gt;FoG方法从基础模型中提取评分函数，根据人类意图评估状态，为理想状态赋予更高值，不理想状态赋予更低值。然后使用这些分数重新加权技能发现算法的奖励，通过优化重新加权的奖励来指导技能发现过程。&lt;h4&gt;主要发现&lt;/h4&gt;FoG成功消除了不理想行为，如翻转或翻滚，并在基于状态和基于像素的任务中有效避免了危险区域。此外，该方法能够发现涉及难以定义的行为的技能。&lt;h4&gt;结论&lt;/h4&gt;FoG方法通过将人类意图融入技能发现过程，解决了现有方法只关注多样性而忽略人类偏好的问题，使强化学习能够学习更符合人类期望的技能，从而加速下游任务的强化学习过程。&lt;h4&gt;翻译&lt;/h4&gt;无需手工设计的奖励函数即可学习多样化技能，可以加速下游任务中的强化学习。然而，现有的技能发现方法只专注于最大化技能多样性，而没有考虑人类偏好，这会导致不理想的行为甚至危险技能。例如，使用先前方法训练的猎豹机器人学会向各个方向翻滚以最大化技能多样性，而我们更希望它能够奔跑而不翻转或进入危险区域。在这项工作中，我们提出了一种基础模型引导(FoG)的技能发现方法，通过基础模型将人类意图融入技能发现。具体来说，FoG从基础模型中提取评分函数，根据人类意图评估状态，为理想状态赋予更高值，为不理想状态赋予更低值。然后使用这些分数重新加权技能发现算法的奖励。通过优化重新加权的技能发现奖励，FoG成功消除了不理想行为，如翻转或翻滚，并在基于状态和基于像素的任务中避免了危险区域。有趣的是，我们表明FoG可以发现涉及难以定义的行为的技能。交互式可视化可通过https://sites.google.com/view/submission-fog获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning diverse skills without hand-crafted reward functions couldaccelerate reinforcement learning in downstream tasks. However, existing skilldiscovery methods focus solely on maximizing the diversity of skills withoutconsidering human preferences, which leads to undesirable behaviors andpossibly dangerous skills. For instance, a cheetah robot trained using previousmethods learns to roll in all directions to maximize skill diversity, whereaswe would prefer it to run without flipping or entering hazardous areas. In thiswork, we propose a Foundation model Guided (FoG) skill discovery method, whichincorporates human intentions into skill discovery through foundation models.Specifically, FoG extracts a score function from foundation models to evaluatestates based on human intentions, assigning higher values to desirable statesand lower to undesirable ones. These scores are then used to re-weight therewards of skill discovery algorithms. By optimizing the re-weighted skilldiscovery rewards, FoG successfully learns to eliminate undesirable behaviors,such as flipping or rolling, and to avoid hazardous areas in both state-basedand pixel-based tasks. Interestingly, we show that FoG can discover skillsinvolving behaviors that are difficult to define. Interactive visualisationsare available from https://sites.google.com/view/submission-fog.</description>
      <author>example@mail.com (Zhao Yang, Thomas M. Moerland, Mike Preuss, Aske Plaat, Vincent François-Lavet, Edward S. Hu)</author>
      <guid isPermaLink="false">2510.23167v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Implicit Modeling for Transferability Estimation of Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为隐式可迁移性建模(ITM)的新框架，用于评估预训练模型对下游任务的适用性，无需进行完整的微调过程。&lt;h4&gt;背景&lt;/h4&gt;可迁移性估计能够识别最适合下游任务的预训练模型，避免完整微调的高计算成本，促进模型部署和预训练-微调范式发展。然而，现有方法在评估具有多样化架构、训练策略和任务对齐的新兴预训练模型时，准确性有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确评估各种类型预训练模型可迁移性的新方法，使其能够在更广泛的模型和下游任务上实现泛化。&lt;h4&gt;方法&lt;/h4&gt;提出隐式可迁移性建模(ITM)框架，隐式建模每个模型的内在可迁移性，并结合分治变分近似(DVA)策略来有效近似嵌入空间的演化过程。&lt;h4&gt;主要发现&lt;/h4&gt;在涵盖多种训练策略和模型类型的综合基准测试中，ITM在稳定性、有效性和效率方面持续优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;ITM框架为评估新兴预训练模型的可迁移性提供了更准确、更高效的解决方案，有助于推动预训练和微调范式的发展。&lt;h4&gt;翻译&lt;/h4&gt;可迁移性估计能够识别最适合下游任务的预训练模型，而无需承担完整微调的高计算成本。这种能力促进了部署并推动了预训练和微调范式的发展。然而，现有方法在评估具有多样化架构、训练策略和任务对齐的新兴预训练模型的可迁移性时，往往难以准确评估。在这项工作中，我们提出了隐式可迁移性建模(ITM)，这是一个新颖的框架，它隐式地建模每个模型的内在可迁移性，并结合分治变分近似(DVA)策略来有效近似嵌入空间的演化。这种设计使模型能够在更广泛的模型和下游任务上实现泛化。在涵盖广泛训练策略和更多样化模型类型的综合基准上的大量实验表明，ITM在稳定性、有效性和效率方面持续优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transferability estimation identifies the best pre-trained models fordownstream tasks without incurring the high computational cost of fullfine-tuning. This capability facilitates deployment and advances thepre-training and fine-tuning paradigm. However, existing methods often struggleto accurately assess transferability for emerging pre-trained models withdiverse architectures, training strategies, and task alignments. In this work,we propose Implicit Transferability Modeling (ITM), a novel framework thatimplicitly models each model's intrinsic transferability, coupled with aDivide-and-Conquer Variational Approximation (DVA) strategy to efficientlyapproximate embedding space evolution. This design enables generalizationacross a broader range of models and downstream tasks. Extensive experiments ona comprehensive benchmark--spanning extensive training regimes and a widervariety of model types--demonstrate that ITM consistently outperforms existingmethods in terms of stability, effectiveness, and efficiency.</description>
      <author>example@mail.com (Yaoyan Zheng, Huiqun Wang, Nan Zhou, Di Huang)</author>
      <guid isPermaLink="false">2510.23145v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and Force Feedback</title>
      <link>http://arxiv.org/abs/2510.23119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://isee-laboratory.github.io/OmniDexGrasp/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了OmniDexGrasp框架，结合基础模型与转移控制策略，实现了机器人根据人类命令灵巧抓取和操作物体的通用能力。&lt;h4&gt;背景&lt;/h4&gt;让机器人根据人类命令灵巧地抓取和操作物体是机器人学的一个有前景的方向，但现有方法由于语义灵巧抓取数据集规模有限，难以在不同物体或任务上泛化。&lt;h4&gt;目的&lt;/h4&gt;解决基础模型与物理机器人执行之间的差距问题，开发一个能在用户提示、灵巧抓取和抓取任务方面实现全能力的通用框架。&lt;h4&gt;方法&lt;/h4&gt;OmniDexGrasp集成了三个关键模块：使用基础模型生成人类抓取图像增强泛化能力；人类图像到机器人行动的转移策略实现全灵巧抓取；力感知自适应抓取策略确保稳健稳定的抓取执行。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实机器人上的实验验证了OmniDexGrasp在不同用户提示、抓取任务和灵巧手方面的有效性，且可扩展到灵巧操作任务。&lt;h4&gt;结论&lt;/h4&gt;OmniDexGrasp通过结合基础模型与转移控制策略，显著提升了机器人灵巧抓取和操作能力，具有广泛的适用性和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;使机器人能够根据人类命令灵巧地抓取和操作物体是机器人学中的一个有前景的方向。然而，由于语义灵巧抓取数据集的规模有限，现有方法难以在多样化的物体或任务上泛化。基础模型提供了一种增强泛化的新方法，但由于抽象模型知识与物理机器人执行之间的差距，直接利用它们生成可行的机器人行动仍然具有挑战性。为了解决这些挑战，我们提出了OmniDexGrasp，一个通用框架，通过结合基础模型与转移和控制策略，在用户提示、灵巧抓取和抓取任务方面实现全能力。OmniDexGrasp集成了三个关键模块：(i) 使用基础模型生成人类抓取图像，增强泛化能力，支持用户提示和任务的全能力；(ii) 人类图像到机器人行动的转移策略将人类演示转化为可执行的机器人行动，实现全灵巧抓取；(iii) 力感知自适应抓取策略确保稳健和稳定的抓取执行。在模拟和真实机器人上的实验验证了OmniDexGrasp在不同用户提示、抓取任务和灵巧手方面的有效性，进一步的结果显示其可扩展到灵巧操作任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enabling robots to dexterously grasp and manipulate objects based on humancommands is a promising direction in robotics. However, existing approaches arechallenging to generalize across diverse objects or tasks due to the limitedscale of semantic dexterous grasp datasets. Foundation models offer a new wayto enhance generalization, yet directly leveraging them to generate feasiblerobotic actions remains challenging due to the gap between abstract modelknowledge and physical robot execution. To address these challenges, we proposeOmniDexGrasp, a generalizable framework that achieves omni-capabilities in userprompting, dexterous embodiment, and grasping tasks by combining foundationmodels with the transfer and control strategies. OmniDexGrasp integrates threekey modules: (i) foundation models are used to enhance generalization bygenerating human grasp images supporting omni-capability of user prompt andtask; (ii) a human-image-to-robot-action transfer strategy converts humandemonstrations into executable robot actions, enabling omni dexterousembodiment; (iii) force-aware adaptive grasp strategy ensures robust and stablegrasp execution. Experiments in simulation and on real robots validate theeffectiveness of OmniDexGrasp on diverse user prompts, grasp task and dexteroushands, and further results show its extensibility to dexterous manipulationtasks.</description>
      <author>example@mail.com (Yi-Lin Wei, Zhexi Luo, Yuhao Lin, Mu Lin, Zhizhao Liang, Shuoyu Chen, Wei-Shi Zheng)</author>
      <guid isPermaLink="false">2510.23119v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Eigenfunction Extraction for Ordered Representation Learning</title>
      <link>http://arxiv.org/abs/2510.24672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种通用框架，用于提取有序且可识别的特征函数，解决了现有表示学习方法只能恢复核的前几个特征函数线性张成空间的问题。&lt;h4&gt;背景&lt;/h4&gt;表示学习的最新进展表明，广泛使用的目标（如对比和非对比方法）隐式地对由输入与其上下文之间的关系诱导的上下文核执行谱分解。&lt;h4&gt;目的&lt;/h4&gt;开发一个能提取有序且可识别特征函数的通用框架，满足与上下文核兼容和可扩展到现代设置的需求。&lt;h4&gt;方法&lt;/h4&gt;展示低秩近似和瑞利商优化两种主要方法论范式如何与特征函数提取框架对齐，基于模块化构建块设计。&lt;h4&gt;主要发现&lt;/h4&gt;恢复的特征值可作为特征选择的有效重要性分数，通过自适应维度表示实现原则性的效率-准确性权衡。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在合成核和真实图像数据集上均得到验证，能够提供更精确的特征提取和理解，有助于特征选择和模型效率与准确性的平衡。&lt;h4&gt;翻译&lt;/h4&gt;表示学习的最新进展表明，广泛使用的目标（如对比和非对比方法）隐式地对由输入与其上下文之间的关系诱导的上下文核执行谱分解。然而，这些方法只能恢复核的前几个特征函数的线性张成空间，而精确的谱分解对于理解特征排序和重要性至关重要。在本研究中，我们提出一个通用框架来提取有序且可识别的特征函数，基于满足关键需求的模块化构建块设计，包括与上下文核的兼容性和可扩展到现代设置的能力。然后，我们展示了两种主要方法论范式（低秩近似和瑞利商优化）如何与这一特征函数提取框架对齐。最后，我们在合成核上验证了我们的方法，并在真实图像数据集上证明恢复的特征值可作为特征选择的有效重要性分数，通过自适应维度表示实现原则性的效率-准确性权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in representation learning reveal that widely usedobjectives, such as contrastive and non-contrastive, implicitly performspectral decomposition of a contextual kernel, induced by the relationshipbetween inputs and their contexts. Yet, these methods recover only the linearspan of top eigenfunctions of the kernel, whereas exact spectral decompositionis essential for understanding feature ordering and importance. In this work,we propose a general framework to extract ordered and identifiableeigenfunctions, based on modular building blocks designed to satisfy keydesiderata, including compatibility with the contextual kernel and scalabilityto modern settings. We then show how two main methodological paradigms,low-rank approximation and Rayleigh quotient optimization, align with thisframework for eigenfunction extraction. Finally, we validate our approach onsynthetic kernels and demonstrate on real-world image datasets that therecovered eigenvalues act as effective importance scores for feature selection,enabling principled efficiency-accuracy tradeoffs via adaptive-dimensionalrepresentations.</description>
      <author>example@mail.com (Burak Varıcı, Che-Ping Tsai, Ritabrata Ray, Nicholas M. Boffi, Pradeep Ravikumar)</author>
      <guid isPermaLink="false">2510.24672v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning</title>
      <link>http://arxiv.org/abs/2510.24356v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出感知学习(PeL)范式，使用任务无关信号优化智能体的感官接口，与下游决策学习解耦。PeL直接针对无标签的感知属性，通过表示不变的指标评估，形式化了感知与决策的分离，并证明PeL更新与贝叶斯任务风险梯度正交。&lt;h4&gt;背景&lt;/h4&gt;传统学习中感知和决策通常紧密耦合，限制了智能体发展通用感知能力，因为学习过于关注特定任务表现而忽略基本感知属性。&lt;h4&gt;目的&lt;/h4&gt;提出PeL范式解耦感知与决策；优化智能体感官接口；定义独立于任务目标的感知属性；提供评估感知质量的指标。&lt;h4&gt;方法&lt;/h4&gt;使用任务无关信号优化感官接口；将感知学习与下游决策学习解耦；针对稳定性、信息量、几何结构等感知属性优化；使用表示不变的指标评估；形式化感知与决策分离；证明PeL更新与贝叶斯任务风险梯度正交；提供任务无关评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;感知与决策可成功分离；可定义独立于目标或重新参数化的感知属性；保持不变量的PeL更新与贝叶斯任务风险梯度正交；提供有效评估指标认证感知质量。&lt;h4&gt;结论&lt;/h4&gt;PeL范式为智能体提供新感知学习框架，通过解耦感知与决策，发展更通用、鲁棒的感知能力，提高任务表现并增强环境理解和适应能力。&lt;h4&gt;翻译&lt;/h4&gt;我们引入感知学习(PeL)，一种使用任务无关信号优化智能体感官接口的范式，与下游决策学习解耦。PeL直接针对无标签的感知属性，如对扰动的稳定性、信息量而不崩溃、受控几何结构等，通过表示不变的指标进行评估。我们形式化了感知与决策的分离，定义了独立于目标或重新参数化的感知属性，并证明了保持足够不变量的PeL更新与贝叶斯任务风险梯度正交。此外，我们还提供了一套任务无关的评估指标来认证感知质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Perception Learning (PeL), a paradigm that optimizes an agent'ssensory interface $f_\phi:\mathcal{X}\to\mathcal{Z}$ using task-agnosticsignals, decoupled from downstream decision learning$g_\theta:\mathcal{Z}\to\mathcal{Y}$. PeL directly targets label-freeperceptual properties, such as stability to nuisances, informativeness withoutcollapse, and controlled geometry, assessed via objectiverepresentation-invariant metrics. We formalize the separation of perception anddecision, define perceptual properties independent of objectives orreparameterizations, and prove that PeL updates preserving sufficientinvariants are orthogonal to Bayes task-risk gradients. Additionally, weprovide a suite of task-agnostic evaluation metrics to certify perceptualquality.</description>
      <author>example@mail.com (Suman Sanyal)</author>
      <guid isPermaLink="false">2510.24356v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.24261v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出DynaRend框架，通过可微分体积渲染学习3D感知和动态信息的三平面特征，统一捕获空间几何、未来动态和任务语义，有效提升机器人操作任务的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;由于缺乏多样化的真实世界训练数据，学习可泛化的机器人操作策略仍面临挑战。现有方法要么依赖2D视觉预训练范式关注静态语义或几何，要么利用视频预测模型强调2D动态，无法同时学习操作所需的几何、语义和动态信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够联合学习几何、语义和动态的表示学习框架，解决机器人操作任务中数据稀缺和泛化能力不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出DynaRend表示学习框架，通过掩码重建和未来预测学习三平面特征，在多视图RGB-D视频数据上进行预训练，并通过动作价值图预测将学习到的表示转移到下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;在RLBench和Colosseum基准测试及真实世界实验中，DynaRend在策略成功率、环境扰动泛化能力和多样化任务实际适用性方面均有显著提升。&lt;h4&gt;结论&lt;/h4&gt;DynaRend成功解决了现有方法的局限性，能够有效联合学习几何、语义和动态信息，大幅提高机器人操作策略的泛化能力和实际应用效果。&lt;h4&gt;翻译&lt;/h4&gt;由于缺乏多样化的真实世界训练数据，学习可泛化的机器人操作策略仍然是一个关键挑战。尽管最近的方法尝试通过自监督表示学习来缓解这一问题，但大多数方法要么依赖于2D视觉预训练范式如掩码图像建模，主要关注静态语义或场景几何，要么利用大规模视频预测模型强调2D动态，因此无法有效操作所需的几何、语义和动态的联合学习。在本文中，我们提出DynaRend，一个表示学习框架，通过可微分体积渲染进行掩码重建和未来预测，学习3D感知和动态信息的三平面特征。通过在多视图RGB-D视频数据上预训练，DynaRend能够在统一的三平面表示中同时捕获空间几何、未来动态和任务语义。学习到的表示可以通过动作价值图预测有效地转移到下游机器人操作任务中。我们在两个具有挑战性的基准测试RLBench和Colosseum以及真实世界机器人实验中评估DynaRend，证明了其在策略成功率、对环境扰动的泛化能力以及在不同操作任务中的实际适用性方面都有显著改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人操作策略难以泛化的问题，原因是缺乏多样化的真实世界训练数据。现有方法要么依赖2D视觉预训练（关注静态语义或几何），要么使用视频预测模型（强调2D动态），无法同时学习有效操作所需的几何、语义和动态信息。这个问题很重要，因为机器人操作需要理解3D环境变化，真实世界数据收集成本高，且有效的操作需要综合理解场景结构、物体语义和动态变化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：2D预训练缺乏3D感知，3D方法结构复杂难以扩展，渲染方法需要密集相机设置不实用。作者设计了一个统一框架，通过掩码重建和未来预测学习3D感知和动态信息的三平面特征。借鉴了掩码图像建模、神经渲染、视频预测和三平面表示等技术，但创新性地结合了这些方法，并引入目标视图增强技术提高真实世界适用性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'掩码未来渲染'学习3D感知和动态信息的三平面特征，同时捕获场景几何、语义信息和未来动态变化。流程包括：1)从多视图RGB-D重建点云并投影为三平面特征；2)随机掩码部分特征，通过重建网络恢复当前场景，通过预测网络生成未来场景；3)对重建和预测结果进行体积渲染，用RGB、语义和深度损失进行监督；4)利用预训练模型合成新视图增强监督；5)预训练后添加动作解码器，在下游任务上微调预测动作值图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的3D表示学习框架，联合学习几何、动态和语义；2)掩码未来渲染技术，结合重建和预测两个互补目标；3)目标视图增强技术，减少对密集相机设置的依赖；4)多任务渲染损失，同时优化RGB、语义和深度。相比之前工作，DynaRend具有明确的3D空间感知能力，能捕获3D动态而非2D，使用更高效简洁的三平面表示，且通过目标视图增强提高了真实世界适用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DynaRend通过掩码未来渲染技术，首次在统一的三平面表示中联合学习空间几何、未来动态和任务语义，显著提高了机器人操作策略的泛化能力和环境适应性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning generalizable robotic manipulation policies remains a key challengedue to the scarcity of diverse real-world training data. While recentapproaches have attempted to mitigate this through self-supervisedrepresentation learning, most either rely on 2D vision pretraining paradigmssuch as masked image modeling, which primarily focus on static semantics orscene geometry, or utilize large-scale video prediction models that emphasize2D dynamics, thus failing to jointly learn the geometry, semantics, anddynamics required for effective manipulation. In this paper, we presentDynaRend, a representation learning framework that learns 3D-aware anddynamics-informed triplane features via masked reconstruction and futureprediction using differentiable volumetric rendering. By pretraining onmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, futuredynamics, and task semantics in a unified triplane representation. The learnedrepresentations can be effectively transferred to downstream roboticmanipulation tasks via action value map prediction. We evaluate DynaRend on twochallenging benchmarks, RLBench and Colosseum, as well as in real-world roboticexperiments, demonstrating substantial improvements in policy success rate,generalization to environmental perturbations, and real-world applicabilityacross diverse manipulation tasks.</description>
      <author>example@mail.com (Jingyi Tian, Le Wang, Sanping Zhou, Sen Wang, Jiayi Li, Gang Hua)</author>
      <guid isPermaLink="false">2510.24261v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Debiasing Reward Models by Representation Learning with Guarantees</title>
      <link>http://arxiv.org/abs/2510.23751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种减轻大型语言模型对齐过程中奖励模型偏见的新框架，通过识别和利用非虚假潜在变量来提高模型的稳健性。&lt;h4&gt;背景&lt;/h4&gt;近期基于人类反馈的强化学习等对齐技术被广泛采用，但这些模型常常利用虚假相关性，如响应长度、歧视性、奉承和概念偏见等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个有原则的框架，减轻奖励模型中的偏见，同时保留反映预期潜在偏好的因素。&lt;h4&gt;方法&lt;/h4&gt;提供数据生成过程的公式化，假设观测数据由虚假和非虚假潜在变量生成；使用变分推断来恢复这些变量并利用它们训练奖励模型。&lt;h4&gt;主要发现&lt;/h4&gt;非虚假潜在变量理论上可以从数据中识别出来，无论虚假潜在变量的代理变量是否可用。&lt;h4&gt;结论&lt;/h4&gt;在合成和真实世界数据集上的实验表明，该方法有效减轻了虚假相关性问题，并产生了更稳健的奖励模型。&lt;h4&gt;翻译&lt;/h4&gt;近期的对齐技术，如基于人类反馈的强化学习，已被广泛采用，通过学习和利用奖励模型使大型语言模型与人类偏好保持一致。在实践中，这些模型常常利用虚假相关性，例如响应长度、歧视性、奉承和概念偏见，这个问题已受到越来越多的关注。在这项工作中，我们提出了一个有原则的框架，在减轻奖励模型中这些偏见的同时，保留反映预期潜在偏好的因素。我们首先提供了数据生成过程的公式化，假设观测数据是由虚假和非虚假的潜在变量生成的。我们有趣地表明，这些非虚假的潜在变量理论上可以从数据中识别出来，无论虚假潜在变量的代理变量是否可用。这进一步启发了一种使用变分推断来恢复这些变量并利用它们训练奖励模型的实用方法。在合成和真实世界数据集上的实验表明，我们的方法有效减轻了虚假相关性问题，并产生了更稳健的奖励模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent alignment techniques, such as reinforcement learning from humanfeedback, have been widely adopted to align large language models with humanpreferences by learning and leveraging reward models. In practice, these modelsoften exploit spurious correlations, involving, e.g., response length,discrimination, sycophancy, and conceptual bias, which is a problem that hasreceived increasing attention. In this work, we propose a principled frameworkthat mitigates these biases in reward models while preserving the underlyingfactors that reflect intended preferences. We first provide a formulation ofthe data-generating process, assuming that the observed data (e.g., text) isgenerated from both spurious and non-spurious latent variables. We show that,interestingly, these non-spurious latent variables can be theoreticallyidentified from data, regardless of whether a surrogate for the spurious latentvariables is available. This further inspires a practical method that usesvariational inference to recover these variables and leverages them to trainreward models. Experiments on synthetic and real-world datasets demonstratethat our method effectively mitigates spurious correlation issues and yieldsmore robust reward models.</description>
      <author>example@mail.com (Ignavier Ng, Patrick Blöbaum, Siddharth Bhandari, Kun Zhang, Shiva Kasiviswanathan)</author>
      <guid isPermaLink="false">2510.23751v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Manifold Approximation leads to Robust Kernel Alignment</title>
      <link>http://arxiv.org/abs/2510.22953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures + supplementary&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的流形近似核对齐(MKA)方法，用于改进现有的中心化核对齐(CKA)度量方法，使其能够考虑底层流形几何，从而提供更稳健的表征测量基础。&lt;h4&gt;背景&lt;/h4&gt;中心化核对齐(CKA)是一种流行的度量方法，广泛应用于比较表征、确定网络等价性和神经科学研究。然而，CKA存在局限性，它没有考虑底层流形，并且依赖于许多启发式方法，导致在不同数据尺度下表现不一致。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够考虑流形几何的核对齐方法，以克服CKA的局限性，提供更稳健的表征测量基础。&lt;h4&gt;方法&lt;/h4&gt;提出流形近似核对齐(MKA)方法，将流形几何整合到对齐任务中，并推导了MKA的理论框架。在合成数据集和真实世界示例上进行经验评估，以表征和比较MKA及其当代方法。&lt;h4&gt;主要发现&lt;/h4&gt;考虑流形的核对齐为测量表征提供了更稳健的基础，在表征学习中有潜在应用价值。&lt;h4&gt;结论&lt;/h4&gt;MKA方法通过考虑底层流形，克服了CKA的局限性，为表征比较提供了更可靠的理论和实践基础。&lt;h4&gt;翻译&lt;/h4&gt;中心化核对齐(CKA)是一种流行的度量方法，用于比较表征、确定网络等价性和神经科学研究。然而，CKA没有考虑底层流形，并且依赖于许多启发式方法，导致它在不同数据尺度下表现不同。在这项工作中，我们提出了流形近似核对齐(MKA)，将流形几何整合到对齐任务中。我们推导了MKA的理论框架。我们在合成数据集和真实世界示例上进行经验评估，以表征和比较MKA及其当代方法。我们的研究结果表明，考虑流形的核对齐为测量表征提供了更稳健的基础，在表征学习中有潜在应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Centered kernel alignment (CKA) is a popular metric for comparingrepresentations, determining equivalence of networks, and neuroscienceresearch. However, CKA does not account for the underlying manifold and relieson numerous heuristics that cause it to behave differently at different scalesof data. In this work, we propose Manifold approximated Kernel Alignment (MKA),which incorporates manifold geometry into the alignment task. We derive atheoretical framework for MKA. We perform empirical evaluations on syntheticdatasets and real-world examples to characterize and compare MKA to itscontemporaries. Our findings suggest that manifold-aware kernel alignmentprovides a more robust foundation for measuring representations, with potentialapplications in representation learning.</description>
      <author>example@mail.com (Mohammad Tariqul Islam, Du Liu, Deblina Sarkar)</author>
      <guid isPermaLink="false">2510.22953v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Switchable Token-Specific Codebook Quantization For Face Image Compression</title>
      <link>http://arxiv.org/abs/2510.22943v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种可切换的特定token码本量化方法，用于面部图像压缩，通过为不同类别图像学习不同码本组并为每个token分配独立码本，提高了低比特率下的重建性能。&lt;h4&gt;背景&lt;/h4&gt;随着视觉数据量不断增加，高效无损的传输及其解释理解成为现代信息系统的关键瓶颈。现有的基于码本的解决方案使用全局共享码本，忽视了面部图像内部的类别相关性和token间的语义差异，导致低比特率下性能不佳。&lt;h4&gt;目的&lt;/h4&gt;解决全局码本策略在面部图像压缩中的局限性，特别是在低比特率(bpp)情况下的性能问题，提高重建图像的质量和识别准确率。&lt;h4&gt;方法&lt;/h4&gt;提出可切换的特定token码本量化方法，为不同图像类别学习不同的码本组，为每个token分配独立码本，并用少量比特记录每个token所属的码本组，以减少码本组减小时造成的损失。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用少量比特记录token所属的码本组，可以在较低总bpp下拥有更多码本总数，增强表达能力并提高重建性能。该方法在面部识别数据集上表现出色，0.05 bpp下重建图像的平均准确率达到93.51%。&lt;h4&gt;结论&lt;/h4&gt;所提出的可切换特定token码本量化方法有效解决了全局码本策略在面部图像压缩中的局限性，特别是在低比特率情况下，且具有良好的通用性，可集成到现有基于码本的表示学习方法中。&lt;h4&gt;翻译&lt;/h4&gt;随着视觉数据量的不断增加，高效无损的传输及其随后的解释理解已成为现代信息系统的关键瓶颈。新兴的基于码本的解决方案利用全局共享码本对每个token进行量化和反量化，通过调整token数量或码本大小来控制bpp。然而，对于富含属性的面部图像，这种全局码本策略忽视了图像内部的类别特定相关性以及token之间的语义差异，导致性能不佳，特别是在低bpp情况下。受这些观察的启发，我们提出了一种用于面部图像压缩的可切换特定token码本量化方法，该方法为不同图像类别学习不同的码本组，并为每个token分配独立的码本。通过用少量比特记录每个token所属的码本组，我们的方法可以减少减小每个码本组大小时造成的损失。这使得在较低的总bpp下可以拥有更多的码本总数，从而增强表达能力并提高重建性能。由于其通用设计，我们的方法可以集成到任何现有的基于码本的表示学习方法中，并在面部识别数据集上证明了其有效性，在0.05 bpp下重建图像的平均准确率达到93.51%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the ever-increasing volume of visual data, the efficient and losslesstransmission, along with its subsequent interpretation and understanding, hasbecome a critical bottleneck in modern information systems. The emergedcodebook-based solution utilize a globally shared codebook to quantize anddequantize each token, controlling the bpp by adjusting the number of tokens orthe codebook size. However, for facial images, which are rich in attributes,such global codebook strategies overlook both the category-specificcorrelations within images and the semantic differences among tokens, resultingin suboptimal performance, especially at low bpp. Motivated by theseobservations, we propose a Switchable Token-Specific Codebook Quantization forface image compression, which learns distinct codebook groups for differentimage categories and assigns an independent codebook to each token. Byrecording the codebook group to which each token belongs with a small number ofbits, our method can reduce the loss incurred when decreasing the size of eachcodebook group. This enables a larger total number of codebooks under a loweroverall bpp, thereby enhancing the expressive capability and improvingreconstruction performance. Owing to its generalizable design, our method canbe integrated into any existing codebook-based representation learning approachand has demonstrated its effectiveness on face recognition datasets, achievingan average accuracy of 93.51% for reconstructed images at 0.05 bpp.</description>
      <author>example@mail.com (Yongbo Wang, Haonan Wang, Guodong Mu, Ruixin Zhang, Jiaqi Chen, Jingyun Zhang, Jun Wang, Yuan Xie, Zhizhong Zhang, Shouhong Ding)</author>
      <guid isPermaLink="false">2510.22943v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Transformers from Compressed Representations</title>
      <link>http://arxiv.org/abs/2510.23665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了TEMPEST方法，它利用压缩文件的字节流结构设计了一种有效的标记化和编码策略，使标准变换器可以直接从压缩数据流中学习语义表示，绕过原始字节级处理或完全媒体解码的需求。&lt;h4&gt;背景&lt;/h4&gt;压缩文件格式是高效数据存储和传输的基石，但它们在表示学习方面的潜力在很大程度上尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，能够直接从压缩数据中学习语义表示，同时减少计算复杂性和内存使用。&lt;h4&gt;方法&lt;/h4&gt;TEMPEST（TransformErs froM comPressed rEpreSenTations）利用压缩文件固有的字节流结构设计有效的标记化和编码策略，使标准变换器可以直接从压缩数据流中学习语义表示。&lt;h4&gt;主要发现&lt;/h4&gt;TEMPEST显著减少了语义分类所需的标记数量，从而降低了计算复杂性和内存使用。通过在不同数据集、编码方案和模态上的大量实验，TEMPEST实现了与最先进技术相当的准确性，同时在内存和计算方面提供了效率提升。&lt;h4&gt;结论&lt;/h4&gt;TEMPEST是一种有效的方法，可以直接从压缩数据中学习语义表示，同时保持高准确性和提高效率。&lt;h4&gt;翻译&lt;/h4&gt;压缩文件格式是高效数据存储和传输的基石，但它们在表示学习方面的潜力在很大程度上尚未被探索。我们介绍了TEMPEST（来自压缩表示的变换器），这是一种利用压缩文件固有字节流结构设计有效标记化和编码策略的方法。通过利用这种紧凑编码，标准变换器可以直接从压缩数据流中学习语义表示，绕过原始字节级处理或完全媒体解码的需求。我们的提议显著减少了语义分类所需的标记数量，从而降低了计算复杂性和内存使用。通过在不同数据集、编码方案和模态上的大量实验，我们表明TEMPEST实现了与最先进技术相当的准确性，同时在内存和计算方面提供了效率提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compressed file formats are the corner stone of efficient data storage andtransmission, yet their potential for representation learning remains largelyunderexplored. We introduce TEMPEST (TransformErs froM comPressedrEpreSenTations), a method that exploits the inherent byte-stream structure ofcompressed files to design an effective tokenization and encoding strategy. Byleveraging this compact encoding, a standard transformer can directly learnsemantic representations from compressed data streams, bypassing the need forraw byte-level processing or full media decoding. Our proposal substantiallyreduces the number of tokens required for semantic classification, therebylowering both computational complexity and memory usage. Through extensiveexperiments across diverse datasets, coding schemes, and modalities, we showthat TEMPEST achieves accuracy competitive wit the state-of-the-art whiledelivering efficiency gains in memory and compute.</description>
      <author>example@mail.com (Juan C. Leon Alcazar, Mattia Soldan, Mohammad Saatialsoruji, Alejandro Pardo, Hani Itani, Juan Camilo Perez, Bernard Ghanem)</author>
      <guid isPermaLink="false">2510.23665v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Learning Without Augmenting: Unsupervised Time Series Representation Learning via Frame Projections</title>
      <link>http://arxiv.org/abs/2510.22655v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at the Conference on Neural Information Processing Systems  (NeurIPS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用正交基和过完备帧替代传统数据增强的自监督学习方法，通过联合利用不同流形的互补几何特性，在多个时间序列任务上实现了高达15-20%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;自监督学习(SSL)是一种无需标记数据即可学习表征的强大范式，但大多数SSL方法依赖于强大、成熟、手工制作的数据增强技术，这需要领域特定知识并可能限制模型的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种无监督表征学习方法，用正交基和过完备帧生成视图来替代传统的数据增强技术，避免其带来的限制。&lt;h4&gt;方法&lt;/h4&gt;使用正交基和过完备帧生成视图替代传统数据增强，联合利用从正交和过完备空间学习到的不同流形上的互补几何特性。&lt;h4&gt;主要发现&lt;/h4&gt;从正交和过完备空间学习到的嵌入位于由不同空间中表示样本所引入的几何偏差形成的不同流形上，通过联合利用这些不同流形的互补几何特性，可以在不通过强增强人为增加数据多样性的情况下实现优越性能。&lt;h4&gt;结论&lt;/h4&gt;在五个时间序列任务上的九个数据集上证明了该方法的有效性，在这些信号特性使得数据增强具有挑战性的任务上，不依赖于增强引起的多样性，实现了高达15-20%的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习(SSL)已成为一种无需标记数据即可学习表征的强大范式。大多数SSL方法依赖于强大、成熟、手工制作的数据增强来为表征学习生成多样化视图。然而，设计此类增强需要领域特定知识，并隐式地对模型施加表征不变性，这可能限制泛化能力。在这项工作中，我们提出了一种无监督表征学习方法，使用正交基和过完备帧生成视图来替代数据增强。我们表明，从正交和过完备空间学习到的嵌入位于不同的流形上，这些流形由在不同空间中表示样本所引入的几何偏差形成。通过联合利用这些不同流形的互补几何，我们的方法在不通过强增强人为增加数据多样性的情况下实现了优越性能。我们在五个时间序列任务上的九个数据集上证明了该方法的有效性，在这些任务中，信号特定特性使得数据增强特别具有挑战性。在不依赖于增强引起的多样性的情况下，我们的方法相比现有自监督方法实现了高达15-20%的性能提升。源代码：https://github.com/eth-siplab/Learning-with-FrameProjections&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has emerged as a powerful paradigm forlearning representations without labeled data. Most SSL approaches rely onstrong, well-established, handcrafted data augmentations to generate diverseviews for representation learning. However, designing such augmentationsrequires domain-specific knowledge and implicitly imposes representationalinvariances on the model, which can limit generalization. In this work, wepropose an unsupervised representation learning method that replacesaugmentations by generating views using orthonormal bases and overcompleteframes. We show that embeddings learned from orthonormal and overcompletespaces reside on distinct manifolds, shaped by the geometric biases introducedby representing samples in different spaces. By jointly leveraging thecomplementary geometry of these distinct manifolds, our approach achievessuperior performance without artificially increasing data diversity throughstrong augmentations. We demonstrate the effectiveness of our method on ninedatasets across five temporal sequence tasks, where signal-specificcharacteristics make data augmentations particularly challenging. Withoutrelying on augmentation-induced diversity, our method achieves performancegains of up to 15--20\% over existing self-supervised approaches. Source code:https://github.com/eth-siplab/Learning-with-FrameProjections</description>
      <author>example@mail.com (Berken Utku Demirel, Christian Holz)</author>
      <guid isPermaLink="false">2510.22655v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DynaCausal: Dynamic Causality-Aware Root Cause Analysis for Distributed Microservices</title>
      <link>http://arxiv.org/abs/2510.22613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DynaCausal是一种动态因果感知框架，用于分布式微服务系统的根本原因分析，通过统一多模态动态信号和动态对比机制，有效解决了级联故障传播建模不足、噪声干扰和概念漂移以及过度依赖服务偏离强度等挑战。&lt;h4&gt;背景&lt;/h4&gt;云原生微服务支持快速迭代和可扩展部署，但创建了复杂且快速演化的依赖关系，对可靠诊断构成挑战。现有的根本原因分析方法在捕捉动态行为和变化的服务关系方面有限。&lt;h4&gt;目的&lt;/h4&gt;解决三个关键挑战：级联故障传播建模不足、噪声干扰和概念漂移影响、以及过度依赖服务偏离强度掩盖真正根本原因的问题。&lt;h4&gt;方法&lt;/h4&gt;提出DynaCausal框架，统一多模态动态信号通过交互感知表征学习捕捉时空依赖关系，引入动态对比机制分离故障指标与噪声，采用因果优先的成对排序目标优化因果归因。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准上的评估显示，DynaCausal持续超越最先进方法，平均AC@1达到0.63，绝对增益从0.25到0.46，在高度动态的微服务环境中提供准确且可解释的诊断。&lt;h4&gt;结论&lt;/h4&gt;DynaCausal通过动态因果感知的方法有效解决了微服务系统故障诊断中的关键挑战，实现了更准确和可解释的根本原因分析。&lt;h4&gt;翻译&lt;/h4&gt;云原生微服务支持快速迭代和可扩展部署，但也创建了复杂且快速演化的依赖关系，对可靠诊断构成挑战。现有的根本原因分析方法，即使融合了日志、追踪和指标等多模态数据，在捕捉动态行为和变化的服务关系方面仍然有限。三个关键挑战仍然存在：(i)级联故障传播建模不足，(ii)容易受到正常服务行为中噪声干扰和概念漂移的影响，(iii)过度依赖服务偏离强度掩盖了真正的根本原因。为解决这些挑战，我们提出了DynaCausal，一种用于分布式微服务系统RCA的动态因果感知框架。DynaCausal统一多模态动态信号，通过交互感知的表征学习捕捉时空依赖关系。它进一步引入了动态对比机制，将真正的故障指标与上下文噪声分离，并采用因果优先的成对排序目标，明确优化因果归因。在公共基准上的全面评估表明，DynaCausal持续超越最先进的方法，平均AC@1达到0.63，绝对增益从0.25到0.46，在高度动态的微服务环境中提供准确且可解释的诊断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cloud-native microservices enable rapid iteration and scalable deployment butalso create complex, fast-evolving dependencies that challenge reliablediagnosis. Existing root cause analysis (RCA) approaches, even with multi-modalfusion of logs, traces, and metrics, remain limited in capturing dynamicbehaviors and shifting service relationships. Three critical challengespersist: (i) inadequate modeling of cascading fault propagation, (ii)vulnerability to noise interference and concept drift in normal servicebehavior, and (iii) over-reliance on service deviation intensity that obscurestrue root causes. To address these challenges, we propose DynaCausal, a dynamiccausality-aware framework for RCA in distributed microservice systems.DynaCausal unifies multi-modal dynamic signals to capture time-varyingspatio-temporal dependencies through interaction-aware representation learning.It further introduces a dynamic contrastive mechanism to disentangle true faultindicators from contextual noise and adopts a causal-prioritized pairwiseranking objective to explicitly optimize causal attribution. Comprehensiveevaluations on public benchmarks demonstrate that DynaCausal consistentlysurpasses state-of-the-art methods, attaining an average AC@1 of 0.63 withabsolute gains from 0.25 to 0.46, and delivering both accurate andinterpretable diagnoses in highly dynamic microservice environments.</description>
      <author>example@mail.com (Songhan Zhang, Aoyang Fang, Yifan Yang, Ruiyi Cheng, Xiaoying Tang, Pinjia He)</author>
      <guid isPermaLink="false">2510.22613v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Random Search Neural Networks for Efficient and Expressive Graph Learning</title>
      <link>http://arxiv.org/abs/2510.22520v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NEURIPS 2025; version with full appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的随机搜索神经网络(RSNNs)方法，用于解决随机游走神经网络(RWNNs)在图表示学习中的局限性。RSNNs通过保证完全节点覆盖的随机搜索，显著降低了采样复杂度，并在理论和实验上都表现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;随机游走神经网络(RWNNs)已成为图表示学习的一种有前景的方法，利用序列模型处理随机游走。然而，在现实采样约束下，RWNNs往往无法捕捉全局结构，即使在小型图中也是如此，这是因为节点和边覆盖不完整，限制了其表达能力。&lt;h4&gt;目的&lt;/h4&gt;解决RWNNs在捕捉全局结构方面的局限性，提高图表示学习的效率和表达能力，特别是在稀疏图上的表现。&lt;h4&gt;方法&lt;/h4&gt;提出随机搜索神经网络(RSNNs)，在保证完全节点覆盖的随机搜索上运行。理论上证明在稀疏图中，只需O(log |V|)次搜索即可实现完全边覆盖，显著低于RWNNs所需的O(|V|)次游走。RSNNs与通用序列模型配对时是通用近似器，且对图同构具有概率不变性。&lt;h4&gt;主要发现&lt;/h4&gt;在稀疏图中，RSNNs只需O(log |V|)次搜索就能实现完全边覆盖，比RWNNs的O(|V|)次游走大幅降低采样复杂度。RSNNs是通用近似器，且对图同构具有概率不变性。实验表明，RSNNs在分子和蛋白质基准测试中持续优于RWNNs，使用最多减少16倍的采样序列就能达到相当或更好的性能。&lt;h4&gt;结论&lt;/h4&gt;RSNNs弥合了基于随机游走方法在理论和实践上的差距，为稀疏图上的学习提供了一种高效且具有表达力的框架，在保持高性能的同时显著降低了计算复杂度。&lt;h4&gt;翻译&lt;/h4&gt;随机游走神经网络(RWNNs)已成为图表示学习的一种有前景的方法，利用序列模型的最新进展来处理随机游走。然而，在现实采样约束下，RWNNs往往无法捕捉全局结构，即使在小型图中也是如此，这是由于节点和边覆盖不完整，限制了其表达能力。为解决这一问题，我们提出了随机搜索神经网络(RSNNs)，它在随机搜索上运行，每个搜索都能保证完全的节点覆盖。理论上，我们证明了在稀疏图中，只需O(log |V|)次搜索就能实现完全边覆盖，与RWNNs所需的O(|V|)次游走相比，显著降低了采样复杂度（假设游走长度随图大小缩放）。此外，当与通用序列模型配对时，RSNNs是通用近似器。最后，我们证明了RSNNs对图同构具有概率不变性，确保其期望是同构不变的图函数。实验上，RSNNs在分子和蛋白质基准测试中持续优于RWNNs，使用最多减少16倍的采样序列就能实现相当或更好的性能。我们的工作弥合了基于随机游走方法在理论和实践上的进展，为在稀疏图上的学习提供了一种高效且具有表达力的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Random walk neural networks (RWNNs) have emerged as a promising approach forgraph representation learning, leveraging recent advances in sequence models toprocess random walks. However, under realistic sampling constraints, RWNNsoften fail to capture global structure even in small graphs due to incompletenode and edge coverage, limiting their expressivity. To address this, wepropose \textit{random search neural networks} (RSNNs), which operate on randomsearches, each of which guarantees full node coverage. Theoretically, wedemonstrate that in sparse graphs, only $O(\log |V|)$ searches are needed toachieve full edge coverage, substantially reducing sampling complexity comparedto the $O(|V|)$ walks required by RWNNs (assuming walk lengths scale with graphsize). Furthermore, when paired with universal sequence models, RSNNs areuniversal approximators. We lastly show RSNNs are probabilistically invariantto graph isomorphisms, ensuring their expectation is an isomorphism-invariantgraph function. Empirically, RSNNs consistently outperform RWNNs on molecularand protein benchmarks, achieving comparable or superior performance with up to16$\times$ fewer sampled sequences. Our work bridges theoretical and practicaladvances in random walk based approaches, offering an efficient and expressiveframework for learning on sparse graphs.</description>
      <author>example@mail.com (Michael Ito, Danai Koutra, Jenna Wiens)</author>
      <guid isPermaLink="false">2510.22520v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MAGIC-Flow: Multiscale Adaptive Conditional Flows for Generation and Interpretable Classification</title>
      <link>http://arxiv.org/abs/2510.22070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MAGIC-Flow，一种条件多尺度归一化流架构，在单一模块化框架内同时执行生成和分类任务，解决了生成式建模在医学影像等困难领域应用时缺乏任务对齐的问题。&lt;h4&gt;背景&lt;/h4&gt;生成式建模已成为表示学习的强大范式，但其在医学影像等困难领域的直接应用仍然有限，仅进行生成而不考虑任务对齐，无法为临床应用提供稳健的基础。&lt;h4&gt;目的&lt;/h4&gt;提出MAGIC-Flow，一种条件多尺度归一化流架构，在单一模块化框架内执行生成和分类，以解决生成式建模在临床应用中的局限性。&lt;h4&gt;方法&lt;/h4&gt;MAGIC-Flow构建为可逆和可微双射的层次结构，其中雅可比行列式在子变换中因子化，确保了似然的精确计算和稳定的优化。通过基于类标签的条件化，支持可控样本合成和原则性类概率估计，同时可逆性使得样本似然的显式可视化成为可能，为模型推理提供了可解释的视角。&lt;h4&gt;主要发现&lt;/h4&gt;MAGIC-Flow在相似性、保真度和多样性指标上与顶级基线相当，在多个数据集上解决了扫描噪声下的生成和分类问题，以及模态特定的合成和识别。结果显示MAGIC-Flow创建了真实、多样的样本并改进了分类性能。&lt;h4&gt;结论&lt;/h4&gt;MAGIC-Flow是数据有限领域中生成和分类的有效策略，对隐私保护增强、鲁棒泛化和可信医疗AI有直接益处。&lt;h4&gt;翻译&lt;/h4&gt;生成式建模已成为表示学习的强大范式，但其在医学影像等困难领域的直接应用仍然有限：仅进行生成而不考虑任务对齐，无法为临床应用提供稳健的基础。我们提出MAGIC-Flow，一种条件多尺度归一化流架构，在单一模块化框架内执行生成和分类。该模型构建为可逆和可微双射的层次结构，其中雅可比行列式在子变换中因子化。我们展示了这如何确保似然的精确计算和稳定的优化，同时可逆性使得样本似然的显式可视化成为可能，为模型推理提供了可解释的视角。通过基于类标签的条件化，MAGIC-Flow支持可控样本合成和原则性类概率估计，有效地辅助生成性和判别性目标。我们使用相似性、保真度和多样性指标将MAGIC-Flow与顶级基线进行比较。在多个数据集上，它解决了扫描噪声下的生成和分类，以及模态特定的合成和识别。结果表明MAGIC-Flow创建了真实、多样的样本并改进了分类。MAGIC-Flow是数据有限领域中生成和分类的有效策略，对隐私保护增强、鲁棒泛化和可信医疗AI有直接益处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling has emerged as a powerful paradigm for representationlearning, but its direct applicability to challenging fields like medicalimaging remains limited: mere generation, without task alignment, fails toprovide a robust foundation for clinical use. We propose MAGIC-Flow, aconditional multiscale normalizing flow architecture that performs generationand classification within a single modular framework. The model is built as ahierarchy of invertible and differentiable bijections, where the Jacobiandeterminant factorizes across sub-transformations. We show how this ensuresexact likelihood computation and stable optimization, while invertibilityenables explicit visualization of sample likelihoods, providing aninterpretable lens into the model's reasoning. By conditioning on class labels,MAGIC-Flow supports controllable sample synthesis and principledclass-probability estimation, effectively aiding both generative anddiscriminative objectives. We evaluate MAGIC-Flow against top baselines usingmetrics for similarity, fidelity, and diversity. Across multiple datasets, itaddresses generation and classification under scanner noise, andmodality-specific synthesis and identification. Results show MAGIC-Flow createsrealistic, diverse samples and improves classification. MAGIC-Flow is aneffective strategy for generation and classification in data-limited domains,with direct benefits for privacy-preserving augmentation, robustgeneralization, and trustworthy medical AI.</description>
      <author>example@mail.com (Luca Caldera, Giacomo Bottacini, Lara Cavinato)</author>
      <guid isPermaLink="false">2510.22070v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability</title>
      <link>http://arxiv.org/abs/2510.22039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to Annual Conference on Neural Information Processing  Systems (NeurIPS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了在部分可观测环境中，通过整合预测编码模块到元强化学习中，以学习更紧凑、可解释的贝叶斯最优表示，从而提高代理的适应性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;在部分可观测环境中，学习历史信息的紧凑表示对规划和泛化至关重要。现有元强化学习代理虽能接近贝叶斯最优策略，但往往无法学习到紧凑且可解释的贝叶斯最优信念状态，这种表示效率低下限制了代理的适应性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;研究将自监督预测编码模块整合到元强化学习中，是否能够促进贝叶斯最优表示的学习，从而提高代理在部分可观测环境中的表现。&lt;h4&gt;方法&lt;/h4&gt;受神经科学中预测编码和深度强化学习中辅助预测目标的启发，作者将预测编码模块整合到元强化学习框架中，并通过状态机模拟进行了实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;带有预测模块的元强化学习能够生成更可解释的表示，更好地近似贝叶斯最优信念状态；在需要主动信息获取的挑战性任务中，只有带有预测模块的元强化学习成功学习了最优表示和策略；更好的表示学习能够提高泛化能力。&lt;h4&gt;结论&lt;/h4&gt;预测学习作为代理在部分可观测环境中有效表示学习的指导原则具有重要价值，能够显著提升代理的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;学习历史信息的紧凑表示对于部分可观测环境中的规划和泛化至关重要。虽然元强化学习代理能够获得接近贝叶斯最优的策略，但它们往往无法学习到紧凑、可解释的贝叶斯最优信念状态。这种表示效率低下可能限制了代理的适应性和泛化能力。受神经科学中预测编码的启发——它表明大脑预测感觉输入是贝叶斯推断的神经实现——以及深度强化学习中的辅助预测目标，我们研究了将自监督预测编码模块整合到元强化学习中是否有助于学习贝叶斯最优表示。通过状态机模拟，我们表明带有预测模块的元强化学习能够生成更可解释的表示，更好地近似贝叶斯最优信念状态，与传统的元强化学习相比，在多种任务中表现一致，即使两者都实现了最优策略。在需要主动信息获取的挑战性任务中，只有带有预测模块的元强化学习成功学习了最优表示和策略，而传统元强化学习在表示学习方面表现不足。最后，我们证明了更好的表示学习能够提高泛化能力。我们的结果强烈表明，预测学习作为代理在部分可观测环境中有效表示学习的指导原则具有重要价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning a compact representation of history is critical for planning andgeneralization in partially observable environments. While meta-reinforcementlearning (RL) agents can attain near Bayes-optimal policies, they often fail tolearn the compact, interpretable Bayes-optimal belief states. Thisrepresentational inefficiency potentially limits the agent's adaptability andgeneralization capacity. Inspired by predictive coding in neuroscience--whichsuggests that the brain predicts sensory inputs as a neural implementation ofBayesian inference--and by auxiliary predictive objectives in deep RL, weinvestigate whether integrating self-supervised predictive coding modules intometa-RL can facilitate learning of Bayes-optimal representations. Through statemachine simulation, we show that meta-RL with predictive modules consistentlygenerates more interpretable representations that better approximateBayes-optimal belief states compared to conventional meta-RL across a widevariety of tasks, even when both achieve optimal policies. In challenging tasksrequiring active information seeking, only meta-RL with predictive modulessuccessfully learns optimal representations and policies, whereas conventionalmeta-RL struggles with inadequate representation learning. Finally, wedemonstrate that better representation learning leads to improvedgeneralization. Our results strongly suggest the role of predictive learning asa guiding principle for effective representation learning in agents navigatingpartial observability.</description>
      <author>example@mail.com (Po-Chen Kuo, Han Hou, Will Dabney, Edgar Y. Walker)</author>
      <guid isPermaLink="false">2510.22039v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Orbital Minimization Method for Neural Operator Decomposition</title>
      <link>http://arxiv.org/abs/2510.21952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 8 figures. To appear at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究重新审视了轨道最小化方法(OMM)，这是一种来自计算物理学的经典优化框架，用于训练神经网络分解线性算子，展示了其在现代机器学习中的实用性和优势。&lt;h4&gt;背景&lt;/h4&gt;谱分解线性算子在机器学习和科学计算中扮演核心角色。近期工作探索了训练神经网络来近似这些算子的特征函数，为表示学习、动力系统和偏微分方程提供了可扩展的方法。&lt;h4&gt;目的&lt;/h4&gt;证明轨道最小化方法(OMM)在现代学习流程中的更广泛应用性，并将其调整为训练神经网络分解正半定算子的框架。&lt;h4&gt;方法&lt;/h4&gt;重新审视轨道最小化方法(OMM)，提供OMM目标一致性的简单线性代数证明，揭示此方法与其他领域独立出现的思想之间的联系，并调整该框架用于训练神经网络。&lt;h4&gt;主要发现&lt;/h4&gt;轨道最小化方法在一系列基准任务中展示了实际优势，通过现代理论和计算的角度重新审视经典数值方法，可以为数值模拟中部署神经网络提供原则性方法，同时为机器学习提供有效且可扩展的工具。&lt;h4&gt;结论&lt;/h4&gt;重新审视经典数值方法可以为机器学习和科学计算提供新的视角和实用工具，扩展了神经网络在数值模拟和机器学习中的应用范围。&lt;h4&gt;翻译&lt;/h4&gt;线性算子的谱分解在机器学习和科学计算的许多领域中扮演着核心角色。近期工作探索了训练神经网络来近似这些算子的特征函数，使表示学习、动力系统和偏微分方程(PDEs)的方法具有可扩展性。在本文中，我们重新审视了来自计算物理学文献的一个经典优化框架，称为轨道最小化方法(OMM)，最初在1990年代提出用于计算化学中的特征值问题。我们提供了OMM目标一致性的简单线性代数证明，并揭示了此方法与不同领域独立出现的几种思想之间的联系。我们的主要目标是证明它在现代学习流程中的更广泛应用性。我们将这一框架调整为训练神经网络来分解正半定算子，并在一系列基准任务中展示了其实际优势。我们的结果强调了如何通过现代理论和计算的角度重新审视经典数值方法，不仅可以为在数值模拟中部署神经网络提供原则性方法，还可以为机器学习提供有效且可扩展的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spectral decomposition of linear operators plays a central role in many areasof machine learning and scientific computing. Recent work has explored trainingneural networks to approximate eigenfunctions of such operators, enablingscalable approaches to representation learning, dynamical systems, and partialdifferential equations (PDEs). In this paper, we revisit a classicaloptimization framework from the computational physics literature known as the\emph{orbital minimization method} (OMM), originally proposed in the 1990s forsolving eigenvalue problems in computational chemistry. We provide a simplelinear-algebraic proof of the consistency of the OMM objective, and revealconnections between this method and several ideas that have appearedindependently across different domains. Our primary goal is to justify itsbroader applicability in modern learning pipelines. We adapt this framework totrain neural networks to decompose positive semidefinite operators, anddemonstrate its practical advantages across a range of benchmark tasks. Ourresults highlight how revisiting classical numerical methods through the lensof modern theory and computation can provide not only a principled approach fordeploying neural networks in numerical simulation, but also effective andscalable tools for machine learning.</description>
      <author>example@mail.com (J. Jon Ryu, Samuel Zhou, Gregory W. Wornell)</author>
      <guid isPermaLink="false">2510.21952v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning</title>
      <link>http://arxiv.org/abs/2510.23640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了MuMo，一种结构化多模态融合框架，解决了分子表示中的3D构象不稳定性和模态崩溃问题，提高了模型的鲁棒性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;多模态分子模型通常受到3D构象不可靠性和模态崩溃的限制，这影响了它们的鲁棒性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;设计MuMo框架，解决分子表示中的3D构象不稳定性和模态崩溃问题，提高分子表示的质量。&lt;h4&gt;方法&lt;/h4&gt;1) 设计结构化融合管道（SFP），将2D拓扑和3D几何结合成统一且稳定的结构先验；2) 引入渐进注入（PI）机制，非对称地将先验整合到序列流中，保留模态特定建模同时实现跨模态增强；3) 基于状态空间主干构建，支持长程依赖建模和鲁棒信息传播。&lt;h4&gt;主要发现&lt;/h4&gt;在TDC和MoleculeNet的29个基准任务上，MuMo平均比最佳基线提高2.7%，在22个任务中排名第一，包括在LD50任务上提高27%，验证了模型对3D构象噪声的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;MuMo框架通过结构化多模态融合有效解决了分子表示中的3D构象不稳定性和模态崩溃问题，在各种任务上表现出色。&lt;h4&gt;翻译&lt;/h4&gt;多模态分子模型通常受到3D构象不可靠性和模态崩溃的限制，限制了它们的鲁棒性和泛化能力。我们提出了MuMo，一种结构化多模态融合框架，通过两个关键策略解决了分子表示中的这些挑战。为了减少构象依赖融合的不稳定性，我们设计了一个结构化融合管道（SFP），将2D拓扑和3D几何结合成统一且稳定的结构先验。为了缓解朴素融合引起的模态崩溃，我们引入了渐进注入（PI）机制，非对称地将此先验整合到序列流中，同时保留模态特定建模并实现跨模态增强。基于状态空间主干构建，MuMo支持长程依赖建模和鲁棒信息传播。在来自Therapeutics Data Commons（TDC）和MoleculeNet的29个基准任务上，MuMo在每个任务上平均比最佳基线提高2.7%，在22个任务中排名第一，包括在LD50任务上提高27%。这些结果验证了它对3D构象噪声的鲁棒性以及多模态融合在分子表示中的有效性。代码可在github.com/selmiss/MuMo获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal molecular models often suffer from 3D conformer unreliability andmodality collapse, limiting their robustness and generalization. We proposeMuMo, a structured multimodal fusion framework that addresses these challengesin molecular representation through two key strategies. To reduce theinstability of conformer-dependent fusion, we design a Structured FusionPipeline (SFP) that combines 2D topology and 3D geometry into a unified andstable structural prior. To mitigate modality collapse caused by naive fusion,we introduce a Progressive Injection (PI) mechanism that asymmetricallyintegrates this prior into the sequence stream, preserving modality-specificmodeling while enabling cross-modal enrichment. Built on a state spacebackbone, MuMo supports long-range dependency modeling and robust informationpropagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) andMoleculeNet, MuMo achieves an average improvement of 2.7% over thebest-performing baseline on each task, ranking first on 22 of them, including a27% improvement on the LD50 task. These results validate its robustness to 3Dconformer noise and the effectiveness of multimodal fusion in molecularrepresentation. The code is available at: github.com/selmiss/MuMo.</description>
      <author>example@mail.com (Zihao Jing, Yan Sun, Yan Yi Li, Sugitha Janarthanan, Alana Deng, Pingzhao Hu)</author>
      <guid isPermaLink="false">2510.23640v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models in Dermatopathology: Skin Tissue Classification</title>
      <link>http://arxiv.org/abs/2510.21664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究比较了两种基础模型(UNI和Virchow2)在皮肤病理学全切片图像分类中的表现，发现Virchow2特征提取器表现略优，使用逻辑回归分类器可达到90%的准确率。研究还探索了数据增强和图像归一化技术，并证实平均聚合方法能有效生成切片级特征表示。&lt;h4&gt;背景&lt;/h4&gt;皮肤病理学中全切片图像(WSI)的快速生成需要自动化方法进行高效处理和准确分类。&lt;h4&gt;目的&lt;/h4&gt;评估两种基础模型(UNI和Virchow2)作为特征提取器，用于将WSI分类为三种诊断类别：黑素细胞性、基底样性和鳞状病变。&lt;h4&gt;方法&lt;/h4&gt;使用平均聚合策略将块级嵌入聚合成切片级特征；训练多种机器学习分类器，包括逻辑回归、梯度提升树和随机森林模型；使用精确度、召回率、真正例率、假正例率和AUROC评估性能；探索数据增强技术和图像归一化；使用WandB.ai跟踪和可视化实验结果。&lt;h4&gt;主要发现&lt;/h4&gt;使用Virchow2提取的块级特征在大多数切片级分类器上优于通过UNI提取的特征；使用Virchow2的逻辑回归模型达到了最高的准确率(90%)，但差异不具有统计学意义；平均聚合方法提供了可靠的切片级特征表示。&lt;h4&gt;结论&lt;/h4&gt;这项研究强调了基础模型在自动化WSI分类中的潜力；为皮肤病理学诊断提供了一种可扩展且有效的方法；为切片级表示学习的未来进展铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;皮肤病理学中全切片图像(WSI)的快速生成需要自动化方法进行高效处理和准确分类。本研究评估了两种基础模型UNI和Virchow2作为特征提取器的性能，用于将WSI分类为三种诊断类别：黑素细胞性、基底样性和鳞状病变。使用平均聚合策略将块级嵌入聚合成切片级特征，并随后用于训练多种机器学习分类器，包括逻辑回归、梯度提升树和随机森林模型。使用精确度、召回率、真正例率、假正例率和接收者操作特征曲线下面积(AUROC)在测试集上评估性能。结果表明，使用Virchow2提取的块级特征在大多数切片级分类器上优于通过UNI提取的特征，其中使用Virchow2的逻辑回归达到了最高的准确率(90%)，尽管差异不具有统计学意义。该研究还探索了数据增强技术和图像归一化，以提高模型的鲁棒性和泛化能力。平均聚合方法提供了可靠的切片级特征表示。所有实验结果和指标都使用WandB.ai进行跟踪和可视化，促进了可重复性和可解释性。这项研究强调了基础模型在自动化WSI分类中的潜力，为皮肤病理学诊断提供了一种可扩展且有效的方法，同时为切片级表示学习的未来进展铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid generation of whole-slide images (WSIs) in dermatopathologynecessitates automated methods for efficient processing and accurateclassification. This study evaluates the performance of two foundation models,UNI and Virchow2, as feature extractors for classifying WSIs into threediagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-levelembeddings were aggregated into slide-level features using a mean-aggregationstrategy and subsequently used to train multiple machine learning classifiers,including logistic regression, gradient-boosted trees, and random forestmodels. Performance was assessed using precision, recall, true positive rate,false positive rate, and the area under the receiver operating characteristiccurve (AUROC) on the test set. Results demonstrate that patch-level featuresextracted using Virchow2 outperformed those extracted via UNI across mostslide-level classifiers, with logistic regression achieving the highestaccuracy (90%) for Virchow2, though the difference was not statisticallysignificant. The study also explored data augmentation techniques and imagenormalization to enhance model robustness and generalizability. Themean-aggregation approach provided reliable slide-level featurerepresentations. All experimental results and metrics were tracked andvisualized using WandB.ai, facilitating reproducibility and interpretability.This research highlights the potential of foundation models for automated WSIclassification, providing a scalable and effective approach fordermatopathological diagnosis while paving the way for future advancements inslide-level representation learning.</description>
      <author>example@mail.com (Riya Gupta, Yiwei Zong, Dennis H. Murphree)</author>
      <guid isPermaLink="false">2510.21664v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Causality Meets Locality: Provably Generalizable and Scalable Policy Learning for Networked Systems</title>
      <link>http://arxiv.org/abs/2510.21427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 (Spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出GSAC框架，结合因果表示学习和元actor-critic学习，解决了大规模网络系统（如交通、电力和无线网格）中强化学习的规模和环境变化挑战。&lt;h4&gt;背景&lt;/h4&gt;大规模网络系统（如交通、电力和无线网格）对强化学习代理提出了规模和环境变化的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够同时实现可扩展性和领域泛化的框架，解决大规模网络系统中的强化学习挑战。&lt;h4&gt;方法&lt;/h4&gt;提出GSAC（Generalizable and Scalable Actor-Critic）框架，每个代理学习稀疏局部因果掩码识别关键变量，生成状态和领域因素的紧凑表示（ACRs），元actor-critic训练跨多个源领域的共享策略，测试时通过少量轨迹估计新领域因素并部署自适应策略。&lt;h4&gt;主要发现&lt;/h4&gt;建立了因果恢复、actor-critic收敛和自适应差距的有限样本保证，GSAC能够快速适应并显著优于传统方法。&lt;h4&gt;结论&lt;/h4&gt;GSAC框架有效解决了大规模网络系统中的强化学习挑战，实现了可扩展性和领域泛化。&lt;h4&gt;翻译&lt;/h4&gt;大规模网络系统，如交通、电力和无线网格，对强化学习代理提出了规模和环境变化的挑战。为应对这些挑战，我们提出了GSAC（Generalizable and Scalable Actor-Critic）框架，该框架结合因果表示学习和元actor-critic学习，以实现可扩展性和领域泛化。每个代理首先学习一个稀疏的局部因果掩码，可识别影响其动态的最小邻域变量，生成状态和领域因素的指数紧致近似紧凑表示（ACRs）。这些ACRs限制了将值函数截断到κ-跳邻域的误差，实现在图上的高效学习。元actor-critic则在多个源领域上训练共享策略，同时基于紧凑的领域因素进行条件化；在测试时，只需少量轨迹即可估计新的领域因素并部署自适应策略。我们建立了因果恢复、actor-critic收敛和自适应差距的有限样本保证，并表明GSAC能够快速适应且显著优于从头学习和传统自适应基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale networked systems, such as traffic, power, and wireless grids,challenge reinforcement-learning agents with both scale and environment shifts.To address these challenges, we propose GSAC (Generalizable and ScalableActor-Critic), a framework that couples causal representation learning withmeta actor-critic learning to achieve both scalability and domaingeneralization. Each agent first learns a sparse local causal mask thatprovably identifies the minimal neighborhood variables influencing itsdynamics, yielding exponentially tight approximately compact representations(ACRs) of state and domain factors. These ACRs bound the error of truncatingvalue functions to $\kappa$-hop neighborhoods, enabling efficient learning ongraphs. A meta actor-critic then trains a shared policy across multiple sourcedomains while conditioning on the compact domain factors; at test time, a fewtrajectories suffice to estimate the new domain factor and deploy the adaptedpolicy. We establish finite-sample guarantees on causal recovery, actor-criticconvergence, and adaptation gap, and show that GSAC adapts rapidly andsignificantly outperforms learning-from-scratch and conventional adaptationbaselines.</description>
      <author>example@mail.com (Hao Liang, Shuqing Shi, Yudi Zhang, Biwei Huang, Yali Du)</author>
      <guid isPermaLink="false">2510.21427v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Disentangled Representation Learning via Modular Compositional Bias</title>
      <link>http://arxiv.org/abs/2510.21402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种组合偏置方法，用于解决解耦表示学习中的归纳偏置问题。通过随机混合潜在变量并根据特定规则重组，该方法能够在不修改架构或目标的情况下实现属性、对象甚至两者的解耦。&lt;h4&gt;背景&lt;/h4&gt;当前的解耦表示学习方法主要依赖于特定策略的归纳偏置，包括为属性学习特定目标或为对象设计特定架构。这种依赖性在新的变化因素与先验假设不匹配或多个因素共存时会导致显著开销。&lt;h4&gt;目的&lt;/h4&gt;提出一种组合偏置，一种与目标和架构都解耦的模块化归纳偏置，解决当多个因素共存时需要重新设计架构或目标的问题。&lt;h4&gt;方法&lt;/h4&gt;根据不同因素在数据分布中的不同重组规则（全局属性互斥，对象共享公共支持），随机混合潜在变量。通过两个互补目标强制编码器发现混合策略反映的因素结构：(i)先验损失确保每个混合解码为真实图像；(ii)组合一致性损失将复合图像与其对应的复合潜在变量对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在通用框架下，只需调整混合策略即可实现属性、对象甚至两者的解耦，无需修改目标或架构。实验表明该方法在属性和对象解耦方面具有竞争性性能，且唯一实现了全局风格和对象的联合解耦。&lt;h4&gt;结论&lt;/h4&gt;提出的组合偏置框架提供了一种灵活的方法来处理不同类型的解耦表示学习任务，只需调整混合策略而无需修改架构或目标。&lt;h4&gt;翻译&lt;/h4&gt;最近的解耦表示学习方法严重依赖于特定因素策略-无论是为属性学习目标还是为对象设计架构-以嵌入归纳偏置。这种不同的方法在新的变化因素与先验假设（如统计独立性或空间排他性）不匹配或多个因素共存时会导致显著开销，因为从业者必须重新设计架构或目标。为此，我们提出了一种组合偏置，一种与目标和架构都解耦的模块化归纳偏置。我们的关键见解是，不同因素在数据分布中遵循不同的重组规则：全局属性是互斥的，例如一张脸只有一个鼻子，而对象共享公共支持（任何对象子集都可以共存）。因此，我们根据特定规则随机混合潜在变量，即混合策略，并通过两个互补目标强制编码器发现混合策略反映的任何因素结构：(i)确保每个混合解码为真实图像的先验损失；(ii)Wiedemer等人(arXiv:2310.05327)引入的组合一致性损失，它将每个复合图像与其对应的复合潜在变量对齐。在这一通用框架下，只需调整混合策略即可实现属性、对象甚至两者的解耦，而无需修改目标或架构。大量实验表明，我们的方法在属性和对象解耦方面都表现出竞争性性能，并且唯一实现了全局风格和对象的联合解耦。代码可在https://github.com/whieya/Compositional-DRL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent disentangled representation learning (DRL) methods heavily rely onfactor specific strategies-either learning objectives for attributes or modelarchitectures for objects-to embed inductive biases. Such divergent approachesresult in significant overhead when novel factors of variation do not alignwith prior assumptions, such as statistical independence or spatialexclusivity, or when multiple factors coexist, as practitioners must redesignarchitectures or objectives. To address this, we propose a compositional bias,a modular inductive bias decoupled from both objectives and architectures. Ourkey insight is that different factors obey distinct recombination rules in thedata distribution: global attributes are mutually exclusive, e.g., a face hasone nose, while objects share a common support (any subset of objects canco-exist). We therefore randomly remix latents according to factor-specificrules, i.e., a mixing strategy, and force the encoder to discover whicheverfactor structure the mixing strategy reflects through two complementaryobjectives: (i) a prior loss that ensures every remix decodes into a realisticimage, and (ii) the compositional consistency loss introduced by Wiedemer etal. (arXiv:2310.05327), which aligns each composite image with itscorresponding composite latent. Under this general framework, simply adjustingthe mixing strategy enables disentanglement of attributes, objects, and evenboth, without modifying the objectives or architectures. Extensive experimentsdemonstrate that our method shows competitive performance in both attribute andobject disentanglement, and uniquely achieves joint disentanglement of globalstyle and objects. Code is available athttps://github.com/whieya/Compositional-DRL.</description>
      <author>example@mail.com (Whie Jung, Dong Hoon Lee, Seunghoon Hong)</author>
      <guid isPermaLink="false">2510.21402v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Models Meet Text-Attributed Graphs: A Survey of Integration Frameworks and Applications</title>
      <link>http://arxiv.org/abs/2510.21131v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Surveys and overviews; Natural language processing; Knowledge  representation and reasoning; Graph algorithms&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述从编排的角度系统地回顾了大型语言模型和文本属性图的集成研究，展示了两者结合带来的互补优势。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在自然语言处理方面取得了显著成功，但它们的黑盒性质限制了结构化和多跳推理能力；文本属性图提供了明确的文本关系结构，但往往缺乏语义深度。&lt;h4&gt;目的&lt;/h4&gt;首次从编排的角度系统地回顾LLM和TAG的集成，总结现有方法，并指出未来在语言和图学习交叉领域的研究方向。&lt;h4&gt;方法&lt;/h4&gt;引入新的分类法涵盖两个基本方向：LLM用于TAG（丰富基于图的任务）和TAG用于LLM（改进LLM推理）；将编排策略分为顺序、并行和多模块框架；讨论TAG特定的预训练、提示和参数高效微调的进展。&lt;h4&gt;主要发现&lt;/h4&gt;结合LLMs和TAGs可以产生互补的好处：增强TAG表示学习和提高LLMs的推理能力和可解释性。&lt;h4&gt;结论&lt;/h4&gt;总结了经验性见解，整理了可用数据集，强调了在推荐系统、生物医学分析和知识密集型问答等领域的应用，并指出了开放的挑战和有希望的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型通过强大的语义理解和生成在自然语言处理方面取得了显著成功。然而，它们的黑盒性质限制了结构化和多跳推理能力。相比之下，文本属性图提供了丰富的文本关系结构，但往往缺乏语义深度。最近的研究表明，结合LLMs和TAGs可以带来互补的好处：增强TAG表示学习并提高LLMs的推理能力和可解释性。这篇综述从编排的角度首次系统地回顾了LLM和TAG的集成。我们介绍了一种新的分类法，涵盖两个基本方向：LLM用于TAG，即LLMs丰富基于图的任务；以及TAG用于LLM，即结构化图改进LLM推理。我们将编排策略分为顺序、并行和多模块框架，并讨论了TAG特定的预训练、提示和参数高效微调的进展。除了方法论，我们还总结了经验性见解，整理了可用数据集，并强调了在推荐系统、生物医学分析和知识密集型问答等领域的多样化应用。最后，我们指出了开放的挑战和有希望的研究方向，旨在指导未来在语言和图学习交叉领域的工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have achieved remarkable success in naturallanguage processing through strong semantic understanding and generation.However, their black-box nature limits structured and multi-hop reasoning. Incontrast, Text-Attributed Graphs (TAGs) provide explicit relational structuresenriched with textual context, yet often lack semantic depth. Recent researchshows that combining LLMs and TAGs yields complementary benefits: enhancing TAGrepresentation learning and improving the reasoning and interpretability ofLLMs. This survey provides the first systematic review of LLM--TAG integrationfrom an orchestration perspective. We introduce a novel taxonomy covering twofundamental directions: LLM for TAG, where LLMs enrich graph-based tasks, andTAG for LLM, where structured graphs improve LLM reasoning. We categorizeorchestration strategies into sequential, parallel, and multi-moduleframeworks, and discuss advances in TAG-specific pretraining, prompting, andparameter-efficient fine-tuning. Beyond methodology, we summarize empiricalinsights, curate available datasets, and highlight diverse applications acrossrecommendation systems, biomedical analysis, and knowledge-intensive questionanswering. Finally, we outline open challenges and promising researchdirections, aiming to guide future work at the intersection of language andgraph learning.</description>
      <author>example@mail.com (Guangxin Su, Hanchen Wang, Jianwei Wang, Wenjie Zhang, Ying Zhang, Jian Pei)</author>
      <guid isPermaLink="false">2510.21131v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging semantic similarity for experimentation with AI-generated treatments</title>
      <link>http://arxiv.org/abs/2510.21119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种双核表示学习方法，用于处理大型语言模型生成的高维数字实验处理，并通过学习低维表示捕捉处理的基本结构，应用于指导生成模型产生有意义的处理变体和促进在线实验的自适应分配。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）使数字实验能够以新的形式进行，其中处理方式结合了人类和模型生成的内容，且方式日益复杂。在这种环境下，主要的方法论挑战是如何表示这些高维处理而不丢失其语义含义或使分析变得不可行。&lt;h4&gt;目的&lt;/h4&gt;解决高维处理的表示问题，学习能够捕捉此类处理基本结构的低维表示，并应用于下游任务如指导生成模型和促进在线实验的自适应分配。&lt;h4&gt;方法&lt;/h4&gt;提出双核表示学习方法，通过处理和用户协变量的基于核的表示的内积来建模因果效应。开发了一种交替最小化算法，能够从数据中高效学习这些表示，并在低秩因子模型下提供收敛保证。引入了一种在线实验的自适应设计策略作为该框架的应用。&lt;h4&gt;主要发现&lt;/h4&gt;双核表示学习方法能够有效地从数据中学习处理的高维表示，并在低秩因子模型下保证收敛。该方法在在线实验的自适应设计中表现出有效性。&lt;h4&gt;结论&lt;/h4&gt;通过学习低维表示来捕捉高维处理的基本结构，可以有效地解决大型语言模型数字实验中的表示挑战，并应用于多种下游任务。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）使数字实验能够以新形式进行，其中处理方式结合了人类和模型生成的内容，且方式日益复杂。这种环境下的主要方法论挑战是如何表示这些高维处理而不丢失其语义含义或使分析变得不可行。在此，我们通过专注于学习能够捕捉此类处理基本结构的低维表示来解决这一问题。这些表示使下游应用成为可能，例如指导生成模型产生有意义的处理变体和促进在线实验中的自适应分配。我们提出了双核表示学习方法，通过处理和用户协变量的基于核的表示的内积来建模因果效应。我们开发了一种交替最小化算法，能够从数据中高效学习这些表示，并在低秩因子模型下提供收敛保证。作为该框架的应用，我们引入了一种在线实验的自适应设计策略，并通过数值实验证明了该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) enable a new form of digital experimentationwhere treatments combine human and model-generated content in increasinglysophisticated ways. The main methodological challenge in this setting isrepresenting these high-dimensional treatments without losing their semanticmeaning or rendering analysis intractable. Here, we address this problem byfocusing on learning low-dimensional representations that capture theunderlying structure of such treatments. These representations enabledownstream applications such as guiding generative models to produce meaningfultreatment variants and facilitating adaptive assignment in online experiments.We propose double kernel representation learning, which models the causaleffect through the inner product of kernel-based representations of treatmentsand user covariates. We develop an alternating-minimization algorithm thatlearns these representations efficiently from data and provides convergenceguarantees under a low-rank factor model. As an application of this framework,we introduce an adaptive design strategy for online experimentation anddemonstrate the method's effectiveness through numerical experiments.</description>
      <author>example@mail.com (Lei Shi, David Arbour, Raghavendra Addanki, Ritwik Sinha, Avi Feller)</author>
      <guid isPermaLink="false">2510.21119v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Fair Representation Learning with Controllable High Confidence Guarantees via Adversarial Inference</title>
      <link>http://arxiv.org/abs/2510.21017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了FRG框架，用于学习具有高置信度公平性保证的表示，确保在下游任务中人口统计差异被限制在用户定义的阈值内。&lt;h4&gt;背景&lt;/h4&gt;表示学习被广泛应用于生成能在多个下游任务中泛化的表示，但确保其公平性对于防止对特定人口统计群体产生不公平至关重要。&lt;h4&gt;目的&lt;/h4&gt;正式引入学习高置信度公平性表示的任务，保证每个下游预测中的人口统计差异被限制在用户定义的错误阈值内，并以可控的高概率实现。&lt;h4&gt;方法&lt;/h4&gt;提出FRG(Fair Representation learning with high-confidence Guarantees)框架，通过利用优化的对抗模型提供高置信度公平性保证。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界数据集上的实证评估表明，FRG相比六种最先进的公平表示学习方法，能够在一系列下游模型和任务中持续限制不公平性。&lt;h4&gt;结论&lt;/h4&gt;FRG框架能有效提供高置信度的公平性保证，确保在多种下游任务中公平性得到控制。&lt;h4&gt;翻译&lt;/h4&gt;表示学习越来越多地被应用于生成能够在多个下游任务中泛化的表示。确保表示学习中的公平性保证对于防止在下游任务中对特定人口统计群体产生不公平至关重要。在这项工作中，我们正式引入了学习能够实现高置信度公平性表示的任务。我们旨在保证每个下游预测中的人口统计差异被限制在用户定义的错误阈值内，并以可控的高概率实现。为此，我们提出了FRG框架，该框架通过利用优化的对抗模型提供这些高置信度公平性保证。我们在三个真实世界数据集上对FRG进行了实证评估，将其性能与六种最先进的公平表示学习方法进行比较。我们的结果表明FRG能够在一系列下游模型和任务中持续限制不公平性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning is increasingly applied to generate representationsthat generalize well across multiple downstream tasks. Ensuring fairnessguarantees in representation learning is crucial to prevent unfairness towardspecific demographic groups in downstream tasks. In this work, we formallyintroduce the task of learning representations that achieve high-confidencefairness. We aim to guarantee that demographic disparity in every downstreamprediction remains bounded by a *user-defined* error threshold $\epsilon$, with*controllable* high probability. To this end, we propose the ***F**air**R**epresentation learning with high-confidence **G**uarantees (FRG)*framework, which provides these high-confidence fairness guarantees byleveraging an optimized adversarial model. We empirically evaluate FRG on threereal-world datasets, comparing its performance to six state-of-the-art fairrepresentation learning methods. Our results demonstrate that FRG consistentlybounds unfairness across a range of downstream models and tasks.</description>
      <author>example@mail.com (Yuhong Luo, Austin Hoag, Xintong Wang, Philip S. Thomas, Przemyslaw A. Grabowicz)</author>
      <guid isPermaLink="false">2510.21017v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks</title>
      <link>http://arxiv.org/abs/2510.20976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了L2M3OF，首个专门用于金属有机框架设计的多模态大语言模型，通过整合晶体表示学习与语言理解能力，在材料设计领域取得了突破性进展。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在自然语言任务中展现出强大推理能力，但在科学发现方面的突破有限。理解复杂物理现象需要超越语言的多方面表示，MOFs设计面临巨大三维原子排列空间和严格网状规则的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理MOFs设计的多模态大语言模型，克服仅基于文本表示的局限性，减少对人类专家经验的依赖。&lt;h4&gt;方法&lt;/h4&gt;L2M3OF整合晶体表示学习与语言理解，联合处理结构、文本和知识模态；使用预训练的晶体编码器和轻量级投影层压缩结构信息；构建晶体材料的结构-性质-知识数据库；与GPT-5、Gemini-2.5-Pro和DeepSeek-R1等闭源LLM进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;L2M3OF在性质预测和知识生成任务上优于领先的基于文本的闭源LLM，尽管使用的参数少得多。&lt;h4&gt;结论&lt;/h4&gt;多模态方法对于多孔材料理解的重要性，L2M3OF成为材料发现领域下一代AI系统的基础。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型已在各种自然语言任务中展现出卓越的推理能力。然而，在科学发现方面的可比突破更为有限，因为理解复杂的物理现象需要超越语言本身的多方面表示。一个有力的例子是功能材料如金属有机框架的设计，这些材料对于碳捕获和氢储存等一系列重要应用至关重要。由于其可能存在的三维原子排列数量众多以及配位几何和拓扑的严格网状规则，在基于语言的、可被大语言模型解释的表示中导航其巨大而复杂的设计空间具有挑战性。尽管在更简单的材料系统中，大语言模型辅助的发现已显示出有希望的结果，但MOF设计仍然严重依赖于很少仅以文本信息编码的隐性人类专业知识。为了克服这一障碍，我们引入了L2M3OF，这是首个用于MOFs的多模态大语言模型。L2M3OF整合了晶体表示学习与语言理解，以联合处理结构、文本和知识模态。L2M3OF采用预训练的晶体编码器和轻量级投影层将结构信息压缩到标记空间，从而实现与语言指令的有效对齐。为了促进训练和评估，我们整理了一个包含晶体材料的结构-性质-知识数据库，并将L2M3OF与最先进的闭源大语言模型（如GPT-5、Gemini-2.5-Pro和DeepSeek-R1）进行基准测试。实验表明，尽管使用的参数少得多，L2M3OF在性质预测和知识生成任务上优于领先的基于文本的闭源大语言模型。这些结果突显了多模态方法对于多孔材料理解的重要性，并将L2M3OF确立为材料发现领域下一代AI系统的基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用大型语言模型有效理解和设计金属有机框架（MOFs）材料的问题。这个问题很重要，因为MOFs是一类多孔晶体材料，在碳捕获、氢储存、水收集和药物输送等领域有广泛应用潜力。然而，MOFs的三维复杂结构难以用纯文本表示，现有方法无法有效捕捉其三维对称性、周期性和长程结构相关性，限制了材料设计的效率和准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有LLMs在材料科学中的局限性，特别是它们难以处理MOFs的三维结构信息。他们借鉴了晶体表示学习（如PMTransformer）和指令微调范式，设计了L2M3OF模型。该方法结合了预训练的晶体编码器与轻量级投影层，将结构信息压缩到标记空间，实现与语言指令的高效对齐。同时，作者还采用了分组训练策略增强上下文多样性。这些设计基于对现有工作的深入分析，并针对MOFs的特殊性进行了创新改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态学习，将MOFs的三维结构信息与文本知识相结合，使模型能够同时理解和处理材料的结构、属性和应用知识。整体流程包括：1) 构建MOF-SPK数据库，包含超过10万种MOFs的结构、属性和知识；2) 使用PMTransformer作为晶体结构编码器，将三维结构转换为潜在表示；3) 通过多模态投影桥将结构信息压缩并投影到语言模型空间；4) 使用指令微调范式训练模型，采用分组训练策略增强上下文多样性；5) 在属性预测、结构提取、描述生成和问答等任务上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个专门为MOFs设计的多模态大型语言模型；2) 构建了首个包含MOFs结构、属性和领域知识的综合数据库MOF-SPK；3) 设计了高效的多模态对齐方法，使用轻量级投影层实现结构信息与语言指令的融合；4) 提出了分组训练策略增强模型性能。相比之前的工作，L2M3OF能够处理复杂的三维MOF结构，超越了纯文本表示的局限性；同时能够同时处理多种任务（属性预测、结构提取、描述生成和问答），在性能上超越了参数更多的商业LLMs，尽管使用了更少的参数。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; L2M3OF通过整合三维晶体结构表示与领域知识，开创了多模态大型语言模型在金属有机框架材料设计中的应用，实现了超越纯文本模型的性能，为材料科学发现提供了新的AI辅助工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models have demonstrated remarkable reasoning capabilitiesacross diverse natural language tasks. However, comparable breakthroughs inscientific discovery are more limited, because understanding complex physicalphenomena demands multifaceted representations far beyond language alone. Acompelling example is the design of functional materials such as MOFs-criticalfor a range of impactful applications like carbon capture and hydrogen storage.Navigating their vast and intricate design space in language-basedrepresentations interpretable by LLMs is challenging due to the numerouspossible three-dimensional atomic arrangements and strict reticular rules ofcoordination geometry and topology. Despite promising early results inLLM-assisted discovery for simpler materials systems, MOF design remainsheavily reliant on tacit human expertise rarely codified in textual informationalone. To overcome this barrier, we introduce L2M3OF, the first multimodal LLMfor MOFs. L2M3OF integrates crystal representation learning with languageunderstanding to process structural, textual, and knowledge modalities jointly.L2M3OF employs a pre-trained crystal encoder with a lightweight projectionlayer to compress structural information into a token space, enabling efficientalignment with language instructions. To facilitate training and evaluation, wecurate a structure-property-knowledge database of crystalline materials andbenchmark L2M3OF against state-of-the-art closed-source LLMs such as GPT-5,Gemini-2.5-Pro and DeepSeek-R1. Experiments show that L2M3OF outperformsleading text-based closed-source LLMs in property prediction and knowledgegeneration tasks, despite using far fewer parameters. These results highlightthe importance of multimodal approaches for porous material understanding andestablish L2M3OF as a foundation for next-generation AI systems in materialsdiscovery.</description>
      <author>example@mail.com (Jiyu Cui, Fang Wu, Haokai Zhao, Minggao Feng, Xenophon Evangelopoulos, Andrew I. Cooper, Yejin Choi)</author>
      <guid isPermaLink="false">2510.20976v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Irish-BLiMP: A Linguistic Benchmark for Evaluating Human and Language Model Performance in a Low-Resource Setting</title>
      <link>http://arxiv.org/abs/2510.20957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Irish-BLiMP是首个专门为评估爱尔兰语语言能力而设计的数据集和框架，研究对比了人类和大型语言模型在爱尔兰语语法知识上的表现。&lt;h4&gt;背景&lt;/h4&gt;爱尔兰语是一种濒危语言，缺乏专门的语言能力评估工具和框架。&lt;h4&gt;目的&lt;/h4&gt;评估现有大型语言模型和流利人类参与者在爱尔兰语语法知识方面的表现，并分析两者之间的差异。&lt;h4&gt;方法&lt;/h4&gt;基于多种语言学文献和参考资料，由流利的爱尔兰语使用者团队手动构建和审查了1020个最小对比对，涵盖11个语言学特征的分类法。&lt;h4&gt;主要发现&lt;/h4&gt;人类在所有语言学特征上的表现都优于所有模型，平均准确率高出16.6%；开源和闭源大型语言模型之间存在18.1%的性能差距；最强模型(gpt-5)准确率为73.5%，人类为90.1%；人类和模型在不同语法方面存在不同困难。&lt;h4&gt;结论&lt;/h4&gt;Irish-BLiMP为评估LLMs在爱尔兰语中的语法能力提供了首个系统性框架，为低资源语言理解研究提供了有价值的基准。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Irish-BLiMP(爱尔兰语语言能力最小对比数据集)，这是第一个专门为评估爱尔兰语(一种濒危语言)语言能力而设计和构建的数据集和框架。借鉴多种语言学文献和语法参考资料，我们由流利的爱尔兰语使用者团队手动构建并审查了涵盖11个语言学特征分类法的1020个最小对比对。我们评估了现有大型语言模型和流利人类参与者在爱尔兰语语法知识方面的表现。我们的发现显示，人类在所有语言学特征上的表现都优于所有模型，平均准确率高出16.6%。此外，开源和闭源大型语言模型之间存在18.1%的显著性能差距，即使是最强的模型也仅达到73.5%的准确率，而人类达到90.1%。有趣的是，人类参与者和模型在爱尔兰语语法的不同方面存在困难，这突显了模型学习表征的差异。总体而言，Irish-BLiMP为评估大型语言模型在爱尔兰语中的语法能力提供了首个系统性框架，并为推进低资源语言理解研究提供了有价值的基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Irish-BLiMP (Irish Benchmark of Linguistic Minimal Pairs), thefirst dataset and framework designed for fine-grained evaluation of linguisticcompetence in the Irish language, an endangered language. Drawing on a varietyof linguistic literature and grammar reference works, we manually constructedand reviewed 1020 minimal pairs across a taxonomy of 11 linguistic features,through a team of fluent Irish speakers. We evaluate both existing LargeLanguage Models (LLMs) and fluent human participants on their syntacticknowledge of Irish. Our findings show that humans outperform all models acrossall linguistic features, achieving 16.6% higher accuracy on average. Moreover,a substantial performance gap of 18.1% persists between open- and closed-sourceLLMs, with even the strongest model (gpt-5) reaching only 73.5% accuracycompared to 90.1% by human. Interestingly, human participants and modelsstruggle on different aspects of Irish grammar, thus highlighting a differencein representation learned by the models. Overall, Irish-BLiMP provides thefirst systematic framework for evaluating the grammatical competence of LLMs inIrish and offers a valuable benchmark for advancing research on linguisticunderstanding in low-resource languages.</description>
      <author>example@mail.com (Josh McGiff, Khanh-Tung Tran, William Mulcahy, Dáibhidh Ó Luinín, Jake Dalzell, Róisín Ní Bhroin, Adam Burke, Barry O'Sullivan, Hoang D. Nguyen, Nikola S. Nikolov)</author>
      <guid isPermaLink="false">2510.20957v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>ROPES: Robotic Pose Estimation via Score-Based Causal Representation Learning</title>
      <link>http://arxiv.org/abs/2510.20884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A preliminary version of this paper appeared at NeurIPS 2025 Workshop  on Embodied World Models for Decision Making&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出ROPES方法，将因果表征学习应用于机器人姿态估计，这是一个无监督框架，能够解离潜在生成因素并识别可通过致动直接控制的变量。&lt;h4&gt;背景&lt;/h4&gt;因果表征学习(CRL)是一种强大的无监督框架，能够解离高维数据下的潜在生成因素，并学习解离变量之间的因果关系。尽管在可识别性和实际应用方面取得了进展，但理论与实践之间仍存在显著差距。&lt;h4&gt;目的&lt;/h4&gt;将CRL引入机器人领域，以缩小理论与实践之间的差距。具体解决机器人姿态估计问题，即从原始图像中恢复位置和方向。&lt;h4&gt;方法&lt;/h4&gt;提出ROPES（基于分数的CRL的机器人姿态估计）框架。这是一个无监督框架，通过识别被致动的生成因素来体现干预性CRL的本质。图像由内在和外在潜在因素生成（如关节角度、手臂/肢体几何形状、光照、背景和相机配置），目标是解离和恢复可控制的潜在变量。&lt;h4&gt;主要发现&lt;/h4&gt;在半合成机械臂实验中的经验评估表明，ROPES能够高保真度地解离潜在生成因素。这是仅通过利用分布变化实现的，而没有使用任何标记数据。论文还包括了与基于半监督框架的基线方法的比较。&lt;h4&gt;结论&lt;/h4&gt;将机器人姿态定位为CRL的近实用测试平台。&lt;h4&gt;翻译&lt;/h4&gt;因果表征学习（CRL）已成为一种强大的无监督框架，它（i）解离高维数据下的潜在生成因素，以及（ii）学习解离变量之间的因果关系。尽管最近在可识别性方面取得了广泛进展并有一些实践进展，但理论与实践之间仍然存在巨大差距。本文通过将CRL引入机器人领域（这一领域推动了CRL的发展）来缩小这一差距。具体而言，本文通过引入基于分数的CRL的机器人姿态估计（ROPES）来解决明确的机器人姿态估计问题——从原始图像中恢复位置和方向。作为一个无监督框架，ROPES通过识别被致动的生成因素体现了干预性CRL的本质：图像由内在和外在潜在因素（例如，关节角度、手臂/肢体几何形状、光照、背景和相机配置）生成，目标是解离和恢复可控制的潜在变量，即那些可以通过致动直接操作（干预）的变量。干预性CRL理论表明，通过干预经历变化的变量可以被识别。在机器人领域，这种干预通过命令各种关节的致动器并在不同控制下记录图像自然产生。在半合成机械臂实验中的经验评估表明，ROPES能够高保真度地解离潜在生成因素。关键的是，这是仅通过利用分布变化实现的，而没有使用任何标记数据。本文还包括了与最近提出的半监督框架的基线方法的比较。本文最后将机器人姿态定位为CRL的近实用测试平台。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal representation learning (CRL) has emerged as a powerful unsupervisedframework that (i) disentangles the latent generative factors underlyinghigh-dimensional data, and (ii) learns the cause-and-effect interactions amongthe disentangled variables. Despite extensive recent advances inidentifiability and some practical progress, a substantial gap remains betweentheory and real-world practice. This paper takes a step toward closing that gapby bringing CRL to robotics, a domain that has motivated CRL. Specifically,this paper addresses the well-defined robot pose estimation -- the recovery ofposition and orientation from raw images -- by introducing Robotic PoseEstimation via Score-Based CRL (ROPES). Being an unsupervised framework, ROPESembodies the essence of interventional CRL by identifying those generativefactors that are actuated: images are generated by intrinsic and extrinsiclatent factors (e.g., joint angles, arm/limb geometry, lighting, background,and camera configuration) and the objective is to disentangle and recover thecontrollable latent variables, i.e., those that can be directly manipulated(intervened upon) through actuation. Interventional CRL theory shows thatvariables that undergo variations via interventions can be identified. Inrobotics, such interventions arise naturally by commanding actuators of variousjoints and recording images under varied controls. Empirical evaluations insemi-synthetic manipulator experiments demonstrate that ROPES successfullydisentangles latent generative factors with high fidelity with respect to theground truth. Crucially, this is achieved by leveraging only distributionalchanges, without using any labeled data. The paper also includes a comparisonwith a baseline based on a recently proposed semi-supervised framework. Thispaper concludes by positioning robot pose estimation as a near-practicaltestbed for CRL.</description>
      <author>example@mail.com (Pranamya Kulkarni, Puranjay Datta, Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Ali Tajer)</author>
      <guid isPermaLink="false">2510.20884v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Machine-Learning-Assisted Comparison of Regression Functions</title>
      <link>http://arxiv.org/abs/2510.24714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文重新审视了比较回归函数的经典问题，提出了一种基于核的条件均值依赖性的广义概念，并开发了两种新的检验方法，利用现代机器学习方法进行灵活估计。&lt;h4&gt;背景&lt;/h4&gt;比较回归函数是统计推断中的一个基本问题，与数据集成、迁移学习和因果推断等现代应用密切相关。现有方法通常依赖于平滑技术，因此受到维度诅咒的限制。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来比较回归函数，克服现有方法在维度诅咒下的局限性，并减少对分布假设的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出基于核的条件均值依赖性的广义概念，为回归函数相等的零假设提供新表征，并基于此重新表述开发两种新的检验方法，利用现代机器学习方法进行灵活估计。&lt;h4&gt;主要发现&lt;/h4&gt;建立了检验统计量的渐近性质，这些性质在固定维度和高维度情况下都成立；与需要严格分布假设的现有方法不同，该框架仅施加温和的矩条件。&lt;h4&gt;结论&lt;/h4&gt;所提出的检验方法在广泛的数值研究中证明了其有效性，为比较回归函数提供了更灵活和实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们重新审视了比较回归函数的经典问题，这是统计推断中的一个基本问题，与数据集成、迁移学习和因果推断等现代应用密切相关。现有方法通常依赖于平滑技术，因此受到维度诅咒的限制。我们提出了一种基于核的条件均值依赖性的广义概念，为回归函数相等的零假设提供了新的表征。基于这一重新表述，我们开发了两种新的检验方法，利用现代机器学习方法进行灵活估计。我们建立了检验统计量的渐近性质，这些性质在固定维度和高维度情况下都成立。与通常需要严格分布假设的现有方法不同，我们的框架仅施加温和的矩条件。所提出检验方法的有效性通过大量的数值研究得到了证明。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We revisit the classical problem of comparing regression functions, afundamental question in statistical inference with broad relevance to modernapplications such as data integration, transfer learning, and causal inference.Existing approaches typically rely on smoothing techniques and are thushindered by the curse of dimensionality. We propose a generalized notion ofkernel-based conditional mean dependence that provides a new characterizationof the null hypothesis of equal regression functions. Building on thisreformulation, we develop two novel tests that leverage modern machine learningmethods for flexible estimation. We establish the asymptotic properties of thetest statistics, which hold under both fixed- and high-dimensional regimes.Unlike existing methods that often require restrictive distributionalassumptions, our framework only imposes mild moment conditions. The efficacy ofthe proposed tests is demonstrated through extensive numerical studies.</description>
      <author>example@mail.com (Jian Yan, Zhuoxi Li, Yang Ning, Yong Chen)</author>
      <guid isPermaLink="false">2510.24714v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Cluster Dose Prediction in Carbon Ion Therapy: Using Transfer Learning from a Pretrained Dose Prediction U-Net</title>
      <link>http://arxiv.org/abs/2510.24703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究使用神经网络预测簇剂量分布，以替代计算密集型模拟，并通过迁移学习技术优化了U-Net架构，实现了快速且准确的簇剂量估计。&lt;h4&gt;背景&lt;/h4&gt;簇剂量概念为基于放射生物学效应(RBE)的模型提供了替代方案，用于描述辐射诱导的生物效应。&lt;h4&gt;目的&lt;/h4&gt;研究应用神经网络预测簇剂量分布，以替代当前需要的计算密集型模拟。&lt;h4&gt;方法&lt;/h4&gt;使用U-Net架构预测簇剂量分布，该网络首先在常规剂量分布上预训练，然后通过迁移学习技术对解码器路径进行适应；训练和预训练数据集包括来自多个患者头颈区域的不同能量和位置的碳离子束；使用蒙特卡洛模拟生成真实簇剂量分布作为基准。&lt;h4&gt;主要发现&lt;/h4&gt;U-Net能够在使用图形处理单元(GPU)的情况下，在几毫秒内完成单笔束的簇剂量估计；预测的簇剂量分布与真实值的偏差小于0.35%。&lt;h4&gt;结论&lt;/h4&gt;该原理验证研究证明了使用机器学习在临床可接受的计算时间内准确估计簇剂量的可行性；通过利用预训练神经网络和应用迁移学习技术，显著减少了对大规模、计算成本高昂的训练数据的需求。&lt;h4&gt;翻译&lt;/h4&gt;簇剂量概念为基于放射生物学效应(RBE)的模型提供了替代方案，用于描述辐射诱导的生物效应。本研究探讨了应用神经网络预测簇剂量分布的可能性，旨在替代当前需要的计算密集型模拟。使用最初在常规剂量分布上预训练的U-Net来预测簇剂量分布，并通过迁移学习技术对解码器路径进行适应以用于簇剂量估计。训练和预训练数据集包括来自多个患者头颈区域的不同能量和位置的碳离子束。使用蒙特卡洛(MC)模拟生成真实簇剂量分布。U-Net能够在使用图形处理单元(GPU)的情况下，在几毫秒内完成单笔束的簇剂量估计。预测的簇剂量分布与真实值的偏差小于0.35%。这项原理验证研究证明了使用机器学习(ML)在临床可接受的计算时间内准确估计簇剂量的可行性。通过利用预训练神经网络和应用迁移学习技术，该方法显著减少了对大规模、计算成本高昂的训练数据的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The cluster dose concept offers an alternative to the radiobiologicaleffectiveness (RBE)-based model for describing radiation-induced biologicaleffects. This study examines the application of a neural network to predictcluster dose distributions, with the goal of replacing the computationallyintensive simulations currently required. Cluster dose distributions arepredicted using a U-Net that was initially pretrained on conventional dosedistributions. Using transfer learning techniques, the decoder path is adaptedfor cluster dose estimation. Both the training and pretraining datasets includehead and neck regions from multiple patients and carbon ion beams of varyingenergies and positions. Monte Carlo (MC) simulations were used to generate theground truth cluster dose distributions. The U-Net enables cluster doseestimation for a single pencil beam within milliseconds using a graphicsprocessing unit (GPU). The predicted cluster dose distributions deviate fromthe ground truth by less than 0.35%. This proof-of-principle study demonstratesthe feasibility of accurately estimating cluster doses within clinicallyacceptable computation times using machine learning (ML). By leveraging apretrained neural network and applying transfer learning techniques, theapproach significantly reduces the need for large-scale, computationallyexpensive training data.</description>
      <author>example@mail.com (Miriam Schwarze, Hui Khee Looe, Björn Poppe, Leo Thomas, Hans Rabus)</author>
      <guid isPermaLink="false">2510.24703v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Semi-supervised and unsupervised learning for health indicator extraction from guided waves in aerospace composite structures</title>
      <link>http://arxiv.org/abs/2510.24614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种综合数据驱动框架，通过结合两种学习方法与多域信号处理来提取航空航天复合材料结构的健康指标，解决了健康指标提取中的挑战。&lt;h4&gt;背景&lt;/h4&gt;健康指标对于诊断和预测航空航天复合材料结构的状况至关重要，有助于高效维护和操作安全。然而，由于材料特性的变异性、损伤演变的随机性和多样化的损伤模式，提取可靠的健康指标具有挑战性。制造缺陷和服役期间的事故进一步增加了复杂性。&lt;h4&gt;目的&lt;/h4&gt;开发一种综合数据驱动框架，通过两种学习方法与多域信号处理相结合来学习健康指标，由于缺乏真实健康指标，提出半监督和无监督方法来解决这一问题。&lt;h4&gt;方法&lt;/h4&gt;提出两种学习方法：多样性深度半监督异常检测(Diversity-DeepSAD)方法，使用连续辅助标签作为假设损伤代理；退化趋势约束变分自编码器(DTC-VAE)，通过显式趋势约束嵌入单调性准则。使用多种激励频率的导波监测单加筋复合材料结构，探索时间、频率和时间频域表示，并通过无监督集成融合各频率健康指标。&lt;h4&gt;主要发现&lt;/h4&gt;使用快速傅里叶变换特征，增强的Diversity-DeepSAD模型达到81.6%的性能，DTC-VAE提供最一致的健康指标，达到92.3%的性能，优于现有基线方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的数据驱动框架，特别是DTC-VAE方法，能够有效提取航空航天复合材料结构的健康指标，为结构健康监测提供了可靠解决方案。&lt;h4&gt;翻译&lt;/h4&gt;健康指标对于诊断和预测航空航天复合材料结构的状况至关重要，能够实现高效维护和操作安全。然而，由于材料特性的变异性、损伤演变的随机性和多样化的损伤模式，提取可靠的健康指标仍然具有挑战性。制造缺陷（如脱粘）和服役期间的事故（如鸟撞）进一步使这一过程复杂化。本研究提出了一种综合数据驱动框架，通过结合两种学习方法与多域信号处理来学习健康指标。由于缺乏真实健康指标，提出了半监督和无监督方法：(i)多样性深度半监督异常检测方法，使用连续辅助标签作为假设损伤代理，克服了仅区分健康和故障状态的二元标签的局限性；(ii)退化趋势约束变分自编码器，其中单调性准则通过显式趋势约束嵌入。使用多种激励频率的导波来监测在疲劳载荷下的单加筋复合材料结构，并通过无监督集成融合各频率的健康指标，以减少频率依赖性和方差。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Health indicators (HIs) are central to diagnosing and prognosing thecondition of aerospace composite structures, enabling efficient maintenance andoperational safety. However, extracting reliable HIs remains challenging due tovariability in material properties, stochastic damage evolution, and diversedamage modes. Manufacturing defects (e.g., disbonds) and in-service incidents(e.g., bird strikes) further complicate this process. This study presents acomprehensive data-driven framework that learns HIs via two learning approachesintegrated with multi-domain signal processing. Because ground-truth HIs areunavailable, a semi-supervised and an unsupervised approach are proposed: (i) adiversity deep semi-supervised anomaly detection (Diversity-DeepSAD) approachaugmented with continuous auxiliary labels used as hypothetical damage proxies,which overcomes the limitation of prior binary labels that only distinguishhealthy and failed states while neglecting intermediate degradation, and (ii) adegradation-trend-constrained variational autoencoder (DTC-VAE), in which themonotonicity criterion is embedded via an explicit trend constraint. Guidedwaves with multiple excitation frequencies are used to monitor single-stiffenercomposite structures under fatigue loading. Time, frequency, and time-frequencyrepresentations are explored, and per-frequency HIs are fused via unsupervisedensemble learning to mitigate frequency dependence and reduce variance. Usingfast Fourier transform features, the augmented Diversity-DeepSAD model achieved81.6% performance, while DTC-VAE delivered the most consistent HIs with 92.3%performance, outperforming existing baselines.</description>
      <author>example@mail.com (James Josep Perry, Pablo Garcia-Conde Ortiz, George Konstantinou, Cornelie Vergouwen, Edlyn Santha Kumaran, Morteza Moradi)</author>
      <guid isPermaLink="false">2510.24614v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised learning for variability detection with Gaia DR3 photometry. The main sequence-white dwarf valley</title>
      <link>http://arxiv.org/abs/2510.23776v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in Astronomy &amp; Astrophysics (A&amp;A); 10 pages,  9 figures, 1 appendix (7 additional figures, 2 tables)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用无监督学习方法从Gaia DR3数据中识别变星和特殊系统，成功发现了包括热亚矮星、激变变星、食双星等多种天体类型，并证实该方法在大规模恒星群体分析中的有效性。&lt;h4&gt;背景&lt;/h4&gt;来自空间和地面望远镜的空前数量和质量的数据为机器学习提供了机会，使其能够识别传统方法可能忽略的新变星类别和特殊系统。之前已有相关方法学工作。&lt;h4&gt;目的&lt;/h4&gt;研究无监督学习方法在大恒星群体（包括拥挤场中的天体）上的扩展潜力，无需预先选择的目录，专注于从Gaia DR3中选出的13405个源。&lt;h4&gt;方法&lt;/h4&gt;使用基于从Gaia DR3时代测光中提取的统计特征的无监督聚类技术，采用t-SNE算法识别变星类别、子类型和仪器效应引起的虚假变异性。&lt;h4&gt;主要发现&lt;/h4&gt;聚类结果显示了不同组别，包括热亚矮星、激变变星、食双星和仙女座场中的拥挤场天体；发现了潜在的恒星子类型；被标记为RR Lyrae的天体出现在CMD的意外区域，可能由于不可靠的天体测量或替代演化途径。&lt;h4&gt;结论&lt;/h4&gt;所提出方法在寻找Gaia CMD大区域中可变天体（包括可变热亚矮星和激变变星）具有稳健性，展示了检测扩展恒星群体中变异性的效率，该无监督学习框架可扩展到大型数据集并在识别恒星子类方面有前景。&lt;h4&gt;翻译&lt;/h4&gt;来自空间和地面望远镜的空前数量和质量的数据为机器学习提供了识别新类别变星和可能被传统方法忽视的特殊系统的机会。在先前方法学研究的基础上，本研究探讨了无监督学习方法在大恒星群体（包括拥挤场中的天体）上的扩展潜力，无需预先选择的目录，特别专注于从Gaia DR3中选出的13405个源，位于选定CMD区域。我们的方法主要基于从Gaia DR3时代测光中提取的统计特征，采用无监督聚类技术。我们使用t-SNE算法来识别变星类别、其子类型以及由仪器效应引起的虚假变异性。聚类结果显示了不同的组别，包括热亚矮星、激变变星、食双星以及拥挤场中的天体，如仙女座(M31)场中的天体。在这些集群中还出现了几种潜在的恒星子类型。值得注意的是，先前被标记为RR Lyrae的天体在CMD的意外区域被发现，可能是由于不可靠的天体测量（如双星性）或替代的演化途径。本研究强调了所提出方法在寻找Gaia CMD大区域中可变天体的稳健性，包括可变热亚矮星和激变变星，同时展示了其在检测扩展恒星群体中变异性的效率。所提出的无监督学习框架可扩展到大型数据集，并在识别恒星子类方面有前景的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The unprecedented volume and quality of data from space- and ground-basedtelescopes present an opportunity for machine learning to identify new classesof variable stars and peculiar systems that may have been overlooked bytraditional methods. Extending prior methodological work, this studyinvestigates the potential of an unsupervised learning approach to scaleeffectively to larger stellar populations, including objects in crowded fields,and without the need for pre-selected catalogues, specifically focusing on 13405 sources selected from Gaia DR3 and lying in the selected region of the CMD.Our methodology incorporates unsupervised clustering techniques based primarilyon statistical features extracted from Gaia DR3 epoch photometry. We used thet-distributed stochastic neighbour embedding (t-SNE) algorithm to identifyvariability classes, their subtypes, and spurious variability induced byinstrumental effects. The clustering results revealed distinct groups,including hot subdwarfs, cataclysmic variables (CVs), eclipsing binaries, andobjects in crowded fields, such as those in the Andromeda (M31) field. Severalpotential stellar subtypes also emerged within these clusters. Notably, objectspreviously labelled as RR Lyrae were found in an unexpected region of the CMD,potentially due to either unreliable astrometric measurements (e.g., due tobinarity) or alternative evolutionary pathways. This study emphasises therobustness of the proposed method in finding variable objects in a large regionof the Gaia CMD, including variable hot subdwarfs and CVs, while demonstratingits efficiency in detecting variability in extended stellar populations. Theproposed unsupervised learning framework demonstrates scalability to largedatasets and yields promising results in identifying stellar subclasses.</description>
      <author>example@mail.com (P. Ranaivomanana, C. Johnston, G. Iorio, P. J. Groot, M. Uzundag, T. Kupfer, C. Aerts)</author>
      <guid isPermaLink="false">2510.23776v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Genomics into Multimodal EHR Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种创新的电子健康记录(EHR)基础模型，整合多基因风险评分(PRS)作为基础数据模态，超越传统EHR-only方法，构建更全面的健康档案。&lt;h4&gt;背景&lt;/h4&gt;传统EHR模型仅使用临床数据，忽略了遗传因素对健康的影响。All of Us (AoU)研究项目提供了广泛而多样的数据资源。&lt;h4&gt;目的&lt;/h4&gt;开发一个多模态框架，学习临床数据和遗传倾向之间的复杂关系，增强预测能力和可解释性。&lt;h4&gt;方法&lt;/h4&gt;将生成式AI的进步扩展到EHR基础模型空间，利用AoU研究项目的数据进行训练，并探索迁移学习用于定制分类任务。&lt;h4&gt;主要发现&lt;/h4&gt;在AoU数据上的评估表明，该模型对多种疾病发作(特别是2型糖尿病)具有预测价值，并展示了PRS和EHR数据之间的相互作用。&lt;h4&gt;结论&lt;/h4&gt;这种方法对于解锁疾病预测、主动健康管理、风险分层和个性化治疗策略的新见解至关重要，为医疗保健中更个性化、公平和可行的真实世界证据生成奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种创新的电子健康记录(EHR)基础模型，将多基因风险评分(PRS)作为基础数据模态整合其中，超越了传统的仅使用EHR的方法，以构建更全面的健康档案。利用All of Us (AoU)研究项目的广泛而多样的数据，这个多模态框架旨在学习临床数据和遗传倾向之间的复杂关系。该方法将生成式AI的进步扩展到EHR基础模型空间，增强了预测能力和可解释性。在AoU数据上的评估证明了该模型对多种疾病发作(特别是2型糖尿病)的预测价值，并说明了PRS和EHR数据之间的相互作用。该研究还探索了迁移学习用于定制分类任务，展示了架构的多功能性和效率。这种方法对于解锁疾病预测、主动健康管理、风险分层和个性化治疗策略的新见解至关重要，为医疗保健中更个性化、公平和可行的真实世界证据生成奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces an innovative Electronic Health Record (EHR) foundationmodel that integrates Polygenic Risk Scores (PRS) as a foundational datamodality, moving beyond traditional EHR-only approaches to build more holistichealth profiles. Leveraging the extensive and diverse data from the All of Us(AoU) Research Program, this multimodal framework aims to learn complexrelationships between clinical data and genetic predispositions. Themethodology extends advancements in generative AI to the EHR foundation modelspace, enhancing predictive capabilities and interpretability. Evaluation onAoU data demonstrates the model's predictive value for the onset of variousconditions, particularly Type 2 Diabetes (T2D), and illustrates the interplaybetween PRS and EHR data. The work also explores transfer learning for customclassification tasks, showcasing the architecture's versatility and efficiency.This approach is pivotal for unlocking new insights into disease prediction,proactive health management, risk stratification, and personalized treatmentstrategies, laying the groundwork for more personalized, equitable, andactionable real-world evidence generation in healthcare.</description>
      <author>example@mail.com (Jonathan Amar, Edward Liu, Alessandra Breschi, Liangliang Zhang, Pouya Kheradpour, Sylvia Li, Lisa Soleymani Lehmann, Alessandro Giulianelli, Matt Edwards, Yugang Jia, David Nola, Raghav Mani, Pankaj Vats, Jesse Tetreault, T. J. Chen, Cory Y. McLean)</author>
      <guid isPermaLink="false">2510.23639v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>An unsupervised tour through the hidden pathways of deep neural networks</title>
      <link>http://arxiv.org/abs/2510.21582v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文旨在提高对深度人工神经网络创建有意义表示并能泛化的内部机制的理解，专注于使用无监督学习工具表征隐藏表示的语义内容。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络创建有意义表示和泛化的内部机制尚不完全清楚，需要工具来表征隐藏表示的语义内容并利用数据的低维结构。&lt;h4&gt;目的&lt;/h4&gt;改进对深度神经网络如何创建有意义表示并能泛化的内部机制的理解，开发无监督学习工具来表征隐藏表示的语义内容。&lt;h4&gt;方法&lt;/h4&gt;开发无监督学习工具利用数据的低维结构；介绍Gride方法估计数据内在维度作为尺度的显式函数；研究深度神经网络隐藏层概率密度的演变；分析深度神经网络中的泛化问题。&lt;h4&gt;主要发现&lt;/h4&gt;初始层产生单模态概率密度，消除与分类无关的结构；后续层中密度峰以分层方式出现，反映概念语义层次；输出层概率密度的峰地形可重建类别语义关系；宽神经网络学习冗余表示而非对虚假相关性过拟合；冗余神经元只在网络被正则化且训练误差为零时出现。&lt;h4&gt;结论&lt;/h4&gt;深度神经网络通过分层方式构建语义层次结构；增加参数到插值训练数据的网络会改善泛化性能，与经典偏差-方差权衡相悖；宽神经网络学习冗余表示而非过拟合。&lt;h4&gt;翻译&lt;/h4&gt;本论文的目标是提高我们对深度人工神经网络创建有意义表示并能泛化的内部机制的理解。我们专注于使用无监督学习工具表征隐藏表示的语义内容，这些工具由我们部分开发并在本论文中描述，它们能够利用数据的低维结构。第二章介绍了Gride，一种方法，允许将数据的内在维度估计为尺度的显式函数，而无需对数据集进行任何降采样。我们的方法基于严格的分布结果，能够量化估计的不确定性。此外，我们的方法简单且计算高效，因为它仅依赖于最近数据点之间的距离。在第三章中，我们研究了最先进深度神经网络中隐藏层概率密度的演变。我们发现初始层产生单模态概率密度，消除任何与分类无关的结构。在后续层中，密度峰以分层方式出现，反映了概念的语义层次结构。这个过程在输出层的概率密度中留下了痕迹，其中峰的地形允许重建类别的语义关系。在第四章中，我们研究了深度神经网络中的泛化问题：向插值训练数据的网络添加参数通常会改善其泛化性能，这与经典的偏差-方差权衡相悖。我们证明宽神经网络学习冗余表示而不是对虚假相关性过拟合，并且只有当网络被正则化且训练误差为零时，冗余神经元才会出现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of this thesis is to improve our understanding of the internalmechanisms by which deep artificial neural networks create meaningfulrepresentations and are able to generalize. We focus on the challenge ofcharacterizing the semantic content of the hidden representations withunsupervised learning tools, partially developed by us and described in thisthesis, which allow harnessing the low-dimensional structure of the data.Chapter 2. introduces Gride, a method that allows estimating the intrinsicdimension of the data as an explicit function of the scale without performingany decimation of the data set. Our approach is based on rigorousdistributional results that enable the quantification of uncertainty of theestimates. Moreover, our method is simple and computationally efficient sinceit relies only on the distances among nearest data points. In Chapter 3, westudy the evolution of the probability density across the hidden layers in somestate-of-the-art deep neural networks. We find that the initial layers generatea unimodal probability density getting rid of any structure irrelevant toclassification. In subsequent layers, density peaks arise in a hierarchicalfashion that mirrors the semantic hierarchy of the concepts. This processleaves a footprint in the probability density of the output layer, where thetopography of the peaks allows reconstructing the semantic relationships of thecategories. In Chapter 4, we study the problem of generalization in deep neuralnetworks: adding parameters to a network that interpolates its training datawill typically improve its generalization performance, at odds with theclassical bias-variance trade-off. We show that wide neural networks learnredundant representations instead of overfitting to spurious correlation andthat redundant neurons appear only if the network is regularized and thetraining error is zero.</description>
      <author>example@mail.com (Diego Doimo)</author>
      <guid isPermaLink="false">2510.21582v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MFiSP: A Multimodal Fire Spread Prediction Framework</title>
      <link>http://arxiv.org/abs/2510.23934v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个多模态火灾蔓延预测框架(MFiSP)，整合社交媒体数据和遥感观测以提高预测准确性，评估结果显示该方法优于传统火灾预测方法。&lt;h4&gt;背景&lt;/h4&gt;2019-2020年澳大利亚黑色夏季山火摧毁了1900万公顷土地，3000栋房屋，持续七个月，显示了野火威胁的规模和紧迫性，需要更好的预测来有效应对。&lt;h4&gt;目的&lt;/h4&gt;开发一种更准确的火灾蔓延预测方法，以应对日益严重的野火威胁，提高应急响应效率。&lt;h4&gt;方法&lt;/h4&gt;提出多模态火灾蔓延预测框架(MFiSP)，整合社交媒体数据和遥感观测，通过调整燃料图操纵策略动态调整火灾行为预测，使其与观察到的蔓延速率保持一致。&lt;h4&gt;主要发现&lt;/h4&gt;整合多模态数据的MFiSP可以提高火灾蔓延预测的准确性，超越依赖消防行为分析师专业知识和静态输入的传统方法。&lt;h4&gt;结论&lt;/h4&gt;新兴数据源如NASA的FIRMS卫星图像和自愿地理信息，结合多模态数据整合方法，能够显著改善火灾蔓延预测，为应对日益严重的野火威胁提供有效工具。&lt;h4&gt;翻译&lt;/h4&gt;2019-2020年澳大利亚黑色夏季山火摧毁了1900万公顷土地，3000栋房屋，持续七个月，显示了野火威胁规模和紧迫性的升级，需要更好的预测以实现有效应对。传统火灾建模依赖于消防行为分析师(FBAns)的手动解读和静态环境数据，常常导致不准确和操作限制。新兴数据源，如NASA的FIRMS卫星图像和自愿地理信息，通过实现动态火灾蔓延预测，提供了改进的可能性。本研究提出了一个多模态火灾蔓延预测框架(MFiSP)，整合社交媒体数据和遥感观测以提高预测准确性。通过在同化周期之间调整燃料图操纵策略，该框架动态调整火灾行为预测，以与观察到的蔓延速率保持一致。我们使用在不同场景中合成的火灾事件多边形评估MFiSP的有效性，分析个体和组合对预测边界的影响。结果表明，整合多模态数据的MFiSP可以提高火灾蔓延预测，超越依赖FBAn专业知识和静态输入的传统方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 2019-2020 Black Summer bushfires in Australia devastated 19 millionhectares, destroyed 3,000 homes, and lasted seven months, demonstrating theescalating scale and urgency of wildfire threats requiring better forecastingfor effective response. Traditional fire modeling relies on manualinterpretation by Fire Behaviour Analysts (FBAns) and static environmentaldata, often leading to inaccuracies and operational limitations. Emerging datasources, such as NASA's FIRMS satellite imagery and Volunteered GeographicInformation, offer potential improvements by enabling dynamic fire spreadprediction. This study proposes a Multimodal Fire Spread Prediction Framework(MFiSP) that integrates social media data and remote sensing observations toenhance forecast accuracy. By adapting fuel map manipulation strategies betweenassimilation cycles, the framework dynamically adjusts fire behaviorpredictions to align with the observed rate of spread. We evaluate the efficacyof MFiSP using synthetically generated fire event polygons across multiplescenarios, analyzing individual and combined impacts on forecast perimeters.Results suggest that our MFiSP integrating multimodal data can improve firespread prediction beyond conventional methods reliant on FBAn expertise andstatic inputs.</description>
      <author>example@mail.com (Alec Sathiyamoorthy, Wenhao Zhou, Xiangmin Zhou, Xiaodong Li, Iqbal Gondal)</author>
      <guid isPermaLink="false">2510.23934v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans</title>
      <link>http://arxiv.org/abs/2510.14205v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;动态人格完善框架（DPRF）通过迭代识别和解决认知差异，提高了LLM RPAs与目标个体行为的一致性&lt;h4&gt;背景&lt;/h4&gt;大语言模型角色扮演代理（LLM RPAs）旨在模拟个人人类行为，但人格保真度常因手动创建的档案（如精心挑选的信息和人格特征）而受损，这些档案未经与目标个体一致性的验证&lt;h4&gt;目的&lt;/h4&gt;解决上述限制，引入动态人格完善框架（DPRF），优化LLM RPAs行为与目标个体行为的一致性&lt;h4&gt;方法&lt;/h4&gt;DPRF通过迭代识别认知差异来优化LLM RPAs与目标个体行为的对齐，这些认知差异可以通过自由形式或基于理论的、结构化的分析来识别生成行为与人类真实情况之间的差异，并完善人格档案以减轻这些差异&lt;h4&gt;主要发现&lt;/h4&gt;在五个大语言模型和四种多样的行为预测场景上评估了DPRF，这些场景包括正式辩论、涉及心理健康问题的社交媒体帖子、公开访谈和电影评论，DPRF能够持续显著提高行为对齐度，优于基线人格，且能够跨模型和场景泛化&lt;h4&gt;结论&lt;/h4&gt;提供了一种创建高保真人格档案的稳健方法，提高了下游应用的有效性，如用户模拟、社会研究和个性化AI&lt;h4&gt;翻译&lt;/h4&gt;新兴的大语言模型角色扮演代理（LLM RPAs）旨在模拟个人人类行为，但人格保真度常因手动创建的档案（如精心挑选的信息和人格特征）而受损，这些档案未经与目标个体一致性的验证。为解决这一限制，我们的工作引入了动态人格完善框架（DPRF）。DPRF旨在通过迭代识别生成行为与人类真实情况之间的认知差异（无论是通过自由形式还是基于理论的、结构化的分析）来优化LLM RPAs行为与目标个体行为的一致性，并完善人格档案以减轻这些差异。我们在四个多样的行为预测场景中用五个大语言模型评估了DPRF：正式辩论、涉及心理健康问题的社交媒体帖子、公开访谈和电影评论。DPRF能够持续显著提高行为对齐度，优于基线人格，并且能够跨模型和场景泛化。我们的工作为创建高保真人格档案提供了一种稳健方法，并增强了下游应用的有效性，如用户模拟、社会研究和个性化AI。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emerging large language model role-playing agents (LLM RPAs) aim tosimulate individual human behaviors, but the persona fidelity is oftenundermined by manually-created profiles (e.g., cherry-picked information andpersonality characteristics) without validating the alignment with the targetindividuals. To address this limitation, our work introduces the DynamicPersona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLMRPAs' behaviors with those of target individuals by iteratively identifying thecognitive divergence, either through free-form or theory-grounded, structuredanalysis, between generated behaviors and human ground truth, and refining thepersona profile to mitigate these divergences.We evaluate DPRF with five LLMson four diverse behavior-prediction scenarios: formal debates, social mediaposts with mental health issues, public interviews, and movie reviews.DPRF canconsistently improve behavioral alignment considerably over baseline personasand generalizes across models and scenarios.Our work provides a robustmethodology for creating high-fidelity persona profiles and enhancing thevalidity of downstream applications, such as user simulation, social studies,and personalized AI.</description>
      <author>example@mail.com (Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang)</author>
      <guid isPermaLink="false">2510.14205v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?</title>
      <link>http://arxiv.org/abs/2510.24706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文引入了ComboBench基准测试，评估大型语言模型将语义动作转换为VR设备操作序列的能力，发现即使是顶级模型在程序推理和空间理解方面仍与人类存在差距。&lt;h4&gt;背景&lt;/h4&gt;虚拟现实游戏需要玩家将高级语义动作转换为使用控制器和头戴显示器的精确设备操作，而人类基于常识和具身理解直观地执行这种转换，但大型语言模型是否能有效复制这种能力尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;引入一个名为ComboBench的基准测试，评估大型语言模型将语义动作转换为VR设备操作序列的能力。&lt;h4&gt;方法&lt;/h4&gt;从四个流行的VR游戏(《半衰期：爱莉克斯》、《Into the Radius》、《Moss：Book II》和《Vivecraft》)的262个场景中评估七个大型语言模型(GPT-3.5、GPT-4、GPT-4o、Gemini-1.5-Pro、LLaMA-3-8B、Mixtral-8x7B和GLM-4-Flash)，并与标注的真实基线和人类表现进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;表现最佳的模型(如Gemini-1.5-Pro)展示了强大的任务分解能力，但在程序推理和空间理解方面仍与人类存在差距；不同游戏之间的性能差异显著，表明对交互复杂性的敏感性；少样本示例显著提高了性能，表明有潜力针对性地增强大型语言模型的VR操作能力。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型在VR设备操作序列生成方面仍有改进空间，特别是在程序推理和空间理解方面。&lt;h4&gt;翻译&lt;/h4&gt;虚拟现实游戏要求玩家使用控制器和头戴显示器将高级语义动作转换为精确的设备操作。虽然人类基于常识和具身理解直观地执行这种转换，但大型语言模型是否能有效复制这种能力尚未得到充分探索。本文引入了ComboBench基准测试，评估大型语言模型将语义动作转换为VR设备操作序列的能力，涵盖来自四个流行VR游戏(半衰期：爱莉克斯、Into the Radius、Moss：Book II和Vivecraft)的262个场景。我们评估了七个大型语言模型，包括GPT-3.5、GPT-4、GPT-4o、Gemini-1.5-Pro、LLaMA-3-8B、Mixtral-8x7B和GLM-4-Flash，并与标注的真实基线和人类表现进行比较。结果表明，尽管表现最佳的模型(如Gemini-1.5-Pro)展示了强大的任务分解能力，但在程序推理和空间理解方面仍与人类存在差距。不同游戏之间的性能差异显著，表明对交互复杂性的敏感性。少样本示例显著提高了性能，表明有潜力针对性地增强大型语言模型的VR操作能力。我们在https://sites.google.com/view/combobench发布了所有材料。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文研究大型语言模型（LLMs）是否能够有效地将高级语义动作（如'投降'、'驯服马匹'）转化为精确的VR设备操作序列（如'按X键'、'将头显朝向苦力怕'）。这个问题重要是因为VR游戏需要玩家将抽象意图转化为具体物理操作，这种能力是人类智能的关键组成部分，但目前尚不清楚LLMs是否具备这种具身认知能力，这对开发更智能的虚拟代理和游戏AI具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过认知科学专家访谈确定了VR交互所需的六种核心认知能力（任务分解、程序推理等），然后系统性地选择了四款代表性VR游戏，提取了262个语义动作场景，并由经验VR玩家进行精细的设备操作序列标注。他们借鉴了机器人系统（如SayCan）、虚拟环境智能体（如Voyager）和多步骤规划策略（如Chain-of-Thought）的工作，但专注于VR设备操作的精确映射，而非代码生成或机器人控制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建全面基准测试ComboBench评估LLMs将语义动作转化为VR设备操作的能力，并采用多维度认知框架分析其表现。整体流程包括：1)构建基准：确定认知能力、选择游戏、提取场景、标注操作序列和认知能力；2)模型评估：在多个模型和少样本设置下测试；3)性能分析：比较LLMs与人类表现、分析认知能力差异、研究少样本影响和游戏复杂度关系。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门评估LLMs将语义动作转化为VR设备操作能力的基准测试；2)建立六种核心认知能力框架并实现步骤级别标注；3)设计多维度评估指标全面分析性能；4)收集四款不同类型VR游戏的262个场景提供多样化测试环境；5)通过与人类对比揭示LLMs在具身认知方面的优势和不足。相比之前工作，ComboBench专注于VR环境中的物理设备操作而非代码生成、机器人控制或平面界面操作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了ComboBench基准测试，首次系统评估了大型语言模型将高级语义动作转化为VR设备操作的能力，揭示了当前LLMs在具身认知方面的优势与局限，为开发更智能的VR交互AI提供了重要指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Virtual Reality (VR) games require players to translate high-level semanticactions into precise device manipulations using controllers and head-mounteddisplays (HMDs). While humans intuitively perform this translation based oncommon sense and embodied understanding, whether Large Language Models (LLMs)can effectively replicate this ability remains underexplored. This paperintroduces a benchmark, ComboBench, evaluating LLMs' capability to translatesemantic actions into VR device manipulation sequences across 262 scenariosfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared againstannotated ground truth and human performance. Our results reveal that whiletop-performing models like Gemini-1.5-Pro demonstrate strong task decompositioncapabilities, they still struggle with procedural reasoning and spatialunderstanding compared to humans. Performance varies significantly acrossgames, suggesting sensitivity to interaction complexity. Few-shot examplessubstantially improve performance, indicating potential for targetedenhancement of LLMs' VR manipulation capabilities. We release all materials athttps://sites.google.com/view/combobench.</description>
      <author>example@mail.com (Shuqing Li, Jiayi Yan, Chenyu Niu, Jen-tse Huang, Yun Peng, Wenxuan Wang, Yepang Liu, Michael R. Lyu)</author>
      <guid isPermaLink="false">2510.24706v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Sound Source Localization for Spatial Mapping of Surgical Actions in Dynamic Scenes</title>
      <link>http://arxiv.org/abs/2510.24332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种创新方法，通过整合3D声学信息和视觉数据，增强了手术场景的理解。该方法能够将声学事件在3D空间中定位并与视觉元素关联，实验证明在真实手术室环境中表现良好。&lt;h4&gt;背景&lt;/h4&gt;手术场景理解对于推进计算机辅助和智能手术系统至关重要。当前方法主要依赖视觉数据或端到端学习，这限制了细粒度上下文建模。&lt;h4&gt;目的&lt;/h4&gt;通过整合3D声学信息来增强手术场景表示，实现对手术环境在时间和空间上的多模态理解。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的框架，用于生成手术场景的4D视听表示。通过将相控麦克风阵列的声学定位信息投影到RGB-D相机的动态点云上，并使用基于Transformer的声学事件检测模块识别包含工具-组织相互作用的相关时间片段。在专家执行的模拟手术程序期间，在真实的手术室设置中进行了实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法成功地将手术声学事件在3D空间中定位，并与视觉场景元素关联。实验评估证明了精确的空间声音定位和多模态数据的稳健融合，提供了手术活动的全面、动态表示。&lt;h4&gt;结论&lt;/h4&gt;这项工作首次引入了动态手术场景中的空间声音定位方法，向多模态手术场景表示迈出了重要一步。通过整合声学和视觉数据，所提出的框架实现了更丰富的上下文理解，为未来的智能和自主手术系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;目的：手术场景理解对于推进计算机辅助和智能手术系统至关重要。当前方法主要依赖视觉数据或端到端学习，这限制了细粒度上下文建模。这项工作旨在通过整合3D声学信息来增强手术场景表示，实现对手术环境在时间和空间上的多模态理解。方法：我们提出了一种新颖的框架，通过将相控麦克风阵列的声学定位信息投影到RGB-D相机的动态点云上，生成手术场景的4D视听表示。基于Transformer的声学事件检测模块识别包含工具-组织相互作用的相关时间片段，这些片段在视听场景表示中进行空间定位。系统在专家执行的模拟手术程序期间，在真实的手术室设置中进行了实验评估。结果：所提出的方法成功地将手术声学事件在3D空间中定位，并将它们与视觉场景元素关联。实验评估证明了精确的空间声音定位和多模态数据的稳健融合，提供了手术活动的全面、动态表示。结论：这项工作首次引入了动态手术场景中的空间声音定位方法，向多模态手术场景表示迈出了重要一步。通过整合声学和视觉数据，所提出的框架实现了更丰富的上下文理解，为未来的智能和自主手术系统奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决手术场景中的声学事件定位问题，目的是通过整合声学信息来增强手术场景的数字表示。这个问题很重要，因为当前手术场景理解主要依赖视觉数据，无法捕捉工具-组织相互作用的细粒度信息，而声学信息可以提供视觉无法获取的关键细节，对于开发智能手术系统和提高手术安全性与效率至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有手术场景理解方法的局限性，然后提出多模态融合的思路，认为结合声学和视觉信息可以提供更全面的手术场景理解。他们借鉴了AudioSpectrogramTransformer模型进行声学事件检测，利用现有的声学波束形成技术生成2D声学热图，并参考了点云处理和3D定位技术。整个系统设计围绕如何有效融合声学和视觉信息，创建时空一致的4D手术场景表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过融合声学和视觉信息创建更全面的4D（3D空间+时间）手术场景表示，利用声学事件补充视觉信息，提供工具-组织相互作用的上下文。整体流程包括：1)使用相控麦克风阵列和RGB-D相机采集多模态数据；2)通过波束形成生成2D声学热图并投影到3D点云上；3)使用transformer模型检测手术声学事件；4)通过聚类算法定位声源并生成3D边界框；5)评估系统性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次在动态手术场景中进行空间声音定位；2)提出4D音频-视觉手术场景表示的新概念；3)基于transformer的声学事件检测方法；4)有效的多模态融合方法。相比之前工作，本文不仅整合了声学和视觉两种模态信息，还创建了时空一致的4D表示，专注于细粒度的声学事件检测和空间定位，而非高层概念预测，提供了更好的可解释性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次通过融合声学和视觉信息创建了4D动态手术场景表示，实现了手术声学事件的空间定位，为开发更智能、更全面的手术理解系统奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Surgical scene understanding is key to advancing computer-aided andintelligent surgical systems. Current approaches predominantly rely on visualdata or end-to-end learning, which limits fine-grained contextual modeling.This work aims to enhance surgical scene representations by integrating 3Dacoustic information, enabling temporally and spatially aware multimodalunderstanding of surgical environments.  Methods: We propose a novel framework for generating 4D audio-visualrepresentations of surgical scenes by projecting acoustic localizationinformation from a phased microphone array onto dynamic point clouds from anRGB-D camera. A transformer-based acoustic event detection module identifiesrelevant temporal segments containing tool-tissue interactions which arespatially localized in the audio-visual scene representation. The system wasexperimentally evaluated in a realistic operating room setup during simulatedsurgical procedures performed by experts.  Results: The proposed method successfully localizes surgical acoustic eventsin 3D space and associates them with visual scene elements. Experimentalevaluation demonstrates accurate spatial sound localization and robust fusionof multimodal data, providing a comprehensive, dynamic representation ofsurgical activity.  Conclusion: This work introduces the first approach for spatial soundlocalization in dynamic surgical scenes, marking a significant advancementtoward multimodal surgical scene representations. By integrating acoustic andvisual data, the proposed framework enables richer contextual understanding andprovides a foundation for future intelligent and autonomous surgical systems.</description>
      <author>example@mail.com (Jonas Hein, Lazaros Vlachopoulos, Maurits Geert Laurent Olthof, Bastian Sigrist, Philipp Fürnstahl, Matthias Seibold)</author>
      <guid isPermaLink="false">2510.24332v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning</title>
      <link>http://arxiv.org/abs/2510.24152v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  RoboSense Challenge with IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个系统性框架，用于提高视觉语言模型在自动驾驶场景理解任务中的性能，通过四个核心组件实现问题分类、任务特定提示设计、视觉信息组装和模型参数优化。&lt;h4&gt;背景&lt;/h4&gt;IROS 2025 RoboSense Challenge评估视觉语言模型在自动驾驶场景理解方面的能力，涵盖感知、预测、规划和损坏检测四个任务领域。&lt;h4&gt;目的&lt;/h4&gt;开发一个有效的框架，提升视觉语言模型在安全关键型自动驾驶任务中的表现，特别是在处理干净数据和损坏数据时的准确率。&lt;h4&gt;方法&lt;/h4&gt;构建了一个四组件框架：1)混合提示路由器分类并分派问题；2)特定任务提示嵌入坐标系、空间推理规则等；3)视觉组装模块组合多视图图像；4)按任务配置模型推理参数。&lt;h4&gt;主要发现&lt;/h4&gt;在Qwen2.5-VL-72B模型上实现，该方法在第一阶段(干净数据)达到70.87%平均准确率，在第二阶段(损坏数据)达到72.85%准确率。&lt;h4&gt;结论&lt;/h4&gt;结构化提示和空间接地能显著提升视觉语言模型在安全关键型自动驾驶任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;本技术报告介绍了我们在IROS 2025 RoboSense Challenge上的解决方案，该方案评估视觉语言模型在自动驾驶场景理解方面的能力，涵盖感知、预测、规划和损坏检测任务。我们提出了一个基于四个核心组件构建的系统性框架。首先，混合提示路由器对问题进行分类并将其分派给特定任务的专家提示，消除了不同问题类型之间的干扰。其次，特定任务提示嵌入明确的坐标系、空间推理规则、角色扮演、思维链/思维树推理以及为每个任务定制的小样本示例。第三，视觉组装模块根据问题要求组合多视图图像、对象裁剪、洋红色标记和自适应历史帧。第四，我们按任务配置模型推理参数(温度、top-p、消息角色)以优化输出质量。在Qwen2.5-VL-72B上实现，我们的方法在第一阶段(干净数据)上平均准确率达到70.87%，在第二阶段(损坏数据)上达到72.85%，证明结构化提示和空间接地显著提高了VLM在安全关键型自动驾驶任务上的性能。代码和提示可在https://github.com/wuaodi/UCAS-CSU-phase2获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉-语言模型(VLMs)在自动驾驶场景理解中面临的三个关键挑战：多视角场景中的空间推理困难(如混淆左右方向、错误判断物体位置)、不同任务类型之间的提示干扰(单一通用提示难以同时优化感知、预测、规划等不同任务)、以及时间上下文集成不当(添加历史帧可能引入噪声而非有用信息)。这些问题在现实中非常重要，因为自动驾驶系统需要准确的场景理解才能做出安全决策，解决这些问题能显著提高VLMs在安全关键自动驾驶任务中的可靠性和性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过三个关键设计原则来解决问题：消除提示干扰(使用专家提示而非通用提示)、增强空间推理(明确定义坐标系统和约束)、以及自适应时间上下文(根据问题类型选择适当历史帧)。作者借鉴了多项现有工作，包括Mixture-of-Prompts(使用多个专家提示)、Role-playing(为模型分配特定角色)、Chain-of-Thought/Tree-of-Thought推理(逐步推理和探索多种可能性)、In-context learning(通过示例引导模型)以及Visual prompting(视觉注意力引导)。作者将这些现有技术组合并调整，以适应自动驾驶场景的特殊需求，特别是空间推理和时间上下文处理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过系统化的提示工程增强VLM在自动驾驶场景理解中的性能，特别关注空间推理和任务特定提示设计。整体实现流程包含四个核心组件：1)路由器：分类测试查询并分配到适当的任务专家提示；2)任务特定提示：包含坐标系统、空间规则、角色扮演、链式/树式思维推理和少样本示例；3)视觉组装模块：根据问题需求组合多视角图像、物体裁剪、标记和自适应历史帧；4)模型选择和推理参数：使用Qwen2.5-VL-72B并根据任务类型调整推理参数。流程是：路由器分类问题→选择任务特定提示→组合视觉输入→使用特定推理参数调用VLM生成答案。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Mixture-of-Prompts路由器，消除不同任务类型间的提示干扰；2)明确的坐标系统和空间规则，增强多视角空间定位能力；3)自适应视觉组装，根据问题类型组合视觉输入；4)任务特定的推理参数，优化不同任务类型的输出质量；5)结合多种推理技术，为不同任务定制推理策略。相比之前工作，本文的主要不同在于不是通过微调模型来增强性能，而是通过提示级别的设计(明确空间定位、自适应时间证据和任务特定路由)来提高可靠性，这种方法在保持基础模型不变的情况下显著提升了性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于任务特定提示和空间推理的系统化框架，显著提升了视觉-语言模型在自动驾驶场景理解任务中的性能，特别是在多视角空间定位和不同任务类型处理方面。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This technical report presents our solution for the RoboSense Challenge atIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous drivingscene understanding across perception, prediction, planning, and corruptiondetection tasks. We propose a systematic framework built on four corecomponents. First, a Mixture-of-Prompts router classifies questions anddispatches them to task-specific expert prompts, eliminating interferenceacross diverse question types. Second, task-specific prompts embed explicitcoordinate systems, spatial reasoning rules, role-playing,Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored toeach task. Third, a visual assembly module composes multi-view images withobject crops, magenta markers, and adaptive historical frames based on questionrequirements. Fourth, we configure model inference parameters (temperature,top-p, message roles) per task to optimize output quality. Implemented onQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (cleandata) and 72.85% on Phase-2 (corrupted data), demonstrating that structuredprompting and spatial grounding substantially enhance VLM performance onsafety-critical autonomous driving tasks. Code and prompt are available athttps://github.com/wuaodi/UCAS-CSU-phase2.</description>
      <author>example@mail.com (Aodi Wu, Xubo Luo)</author>
      <guid isPermaLink="false">2510.24152v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations</title>
      <link>http://arxiv.org/abs/2510.23607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025, produced by Pointcept, project page:  https://pointcept.github.io/Concerto&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Concerto是一个模拟人类概念学习的模型，通过结合3D模态内自蒸馏和2D-3D跨模态联合嵌入，学习空间认知中的抽象概念，表现出优越的性能。&lt;h4&gt;背景&lt;/h4&gt;人类通过多感官协同学习抽象概念，一旦形成，可以从单一感官回忆这些表示。&lt;h4&gt;目的&lt;/h4&gt;受人类学习原理启发，开发一个用于空间认知的概念学习模型。&lt;h4&gt;方法&lt;/h4&gt;Concerto结合了3D模态内自蒸馏和2D-3D跨模态联合嵌入的方法。&lt;h4&gt;主要发现&lt;/h4&gt;Concerto学习到更连贯和信息丰富的空间特征；在零样本可视化中表现出色；在线性探测中分别比最先进的2D和3D自监督模型高出14.2%和4.8%；完全微调后在多个场景理解基准测试中设置了新的最先进结果；提出了针对视频提升点云空间理解的变体；开发了将表示投影到CLIP语言空间的翻译器，实现开放世界感知。&lt;h4&gt;结论&lt;/h4&gt;Concerto产生了具有优越细粒度几何和语义一致性的空间表示。&lt;h4&gt;翻译&lt;/h4&gt;人类通过多感官协同学习抽象概念，一旦形成，这样的表示通常可以从单一感官回忆。受这一原理启发，我们引入了Concerto，这是一个用于空间认知的人类概念学习的极简模拟，结合了3D模态内自蒸馏和2D-3D跨模态联合嵌入。尽管简单，但Concerto学习到更连贯和信息丰富的空间特征，如零样本可视化所示。在3D场景感知的线性探测中，它分别比独立的最新2D和3D自监督模型高出14.2%和4.8%，也优于它们的特征连接。通过完全微调，Concerto在多个场景理解基准测试中设置了新的最新结果（例如在ScanNet上达到80.7% mIoU）。我们进一步提出了一个针对视频提升点云空间理解的Concerto变体，以及一个将Concerto表示线性投影到CLIP语言空间的翻译器，实现开放世界感知。这些结果表明，Concerto产生了具有优越细粒度几何和语义一致性的空间表示。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何通过联合2D图像和3D点云的自监督学习，学习更丰富、更一致的空间表示问题。这个问题在现实中很重要，因为空间认知是自动驾驶、混合现实和机器人等应用的基础，而多模态学习可以提供更全面的空间理解，减少对标注数据的依赖，使模型能够从大量无标签数据中学习更强大的表示。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受人类多感官协同学习抽象过程的启发，认识到人类可以通过不同感官（如视觉和触觉）形成统一概念，并能从单一感官唤起完整体验。他们首先进行了初步研究，验证了简单拼接2D和3D特征优于单一模态，进而设计了更复杂的框架。该方法借鉴了Sonata框架用于3D点云表示学习的单模态自蒸馏技术，以及基于LeCun的联合嵌入预测架构(JEPA)的跨模态对齐方法，将两者结合形成Concerto框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过模拟人类多感官协同学习的方式，结合2D图像和3D点云的自监督学习，学习更丰富、更一致的空间表示，使得模型能够从单一模态中唤出完整的空间概念。整体实现流程包括：1) 单模态自蒸馏：使用Point Transformer V3作为点云编码器，通过教师-学生范式训练，使用在线聚类目标函数增强一致性；2) 跨模态联合嵌入预测：使用预训练图像编码器提取特征，建立点云点和图像像素对应关系，预测点云特征以匹配图像特征，使用余弦相似度作为损失；3) 协同训练：结合两个目标函数，适当平衡权重，训练出能从单一模态唤出丰富空间表示的模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 多模态协同学习框架，通过跨模态预测而非简单特征融合学习统一表示；2) 自监督点云Transformer，结合单模态自蒸馏和跨模态联合嵌入；3) 视频感知变体，利用前馈重建从视频中生成点云数据；4) 语言桥接，将表示映射到CLIP语言空间实现开放词汇感知。相比之前的工作，Concerto不仅整合了2D图像和3D点云信息，还通过联合学习产生了比单一模态或简单特征拼接更丰富、更一致的表示空间，在多个场景理解任务上取得了最先进的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Concerto通过模拟人类多感官协同学习的方式，联合2D图像和3D点云的自监督学习，学习到了比单一模态更丰富、更一致的空间表示，并在多个场景理解任务上取得了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans learn abstract concepts through multisensory synergy, and once formed,such representations can often be recalled from a single modality. Inspired bythis principle, we introduce Concerto, a minimalist simulation of human conceptlearning for spatial cognition, combining 3D intra-modal self-distillation with2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns morecoherent and informative spatial features, as demonstrated by zero-shotvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervisedmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,in linear probing for 3D scene perception. With full fine-tuning, Concerto setsnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%mIoU on ScanNet). We further present a variant of Concerto tailored forvideo-lifted point cloud spatial understanding, and a translator that linearlyprojects Concerto representations into CLIP's language space, enablingopen-world perception. These results highlight that Concerto emerges spatialrepresentations with superior fine-grained geometric and semantic consistency.</description>
      <author>example@mail.com (Yujia Zhang, Xiaoyang Wu, Yixing Lao, Chengyao Wang, Zhuotao Tian, Naiyan Wang, Hengshuang Zhao)</author>
      <guid isPermaLink="false">2510.23607v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Localising under the drape: proprioception in the era of distributed surgical robotic system</title>
      <link>http://arxiv.org/abs/2510.23512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种无需标记的手术机器人本体感觉方法，通过轻量级立体RGB摄像头和基于Transformer的深度学习模型，实现了在无菌遮挡情况下的精确定位，提高了手术场景的可见性和追踪能力。&lt;h4&gt;背景&lt;/h4&gt;手术机器人虽然机械精密，但对周围环境缺乏感知能力，导致碰撞、系统恢复和工作流程中断等问题。现有的追踪系统依赖笨重的红外摄像头和反射标记，只能提供有限视角并增加手术室硬件负担。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需标记的本体感觉方法，使手术机器人在无菌遮挡情况下能够精确定位，提高手术场景的可见性和追踪能力，减少硬件负担，提高手术安全性。&lt;h4&gt;方法&lt;/h4&gt;使用轻量级立体RGB摄像头和基于Transformer的新型深度学习模型。基于最大的多中心空间机器人手术数据集（140万张来自人体尸体和临床前体内研究的自注释图像），通过跟踪整个机器人和手术场景而非单个标记来实现定位。&lt;h4&gt;主要发现&lt;/h4&gt;该方法提供对遮挡具有鲁棒性的整体视图，支持手术场景理解和上下文感知控制。在体内呼吸补偿中展示了临床应用潜力，可获取组织动力学；在多机器人系统中实现精确定位。与现有系统相比，消除了标记并将追踪可见性提高了25%。&lt;h4&gt;结论&lt;/h4&gt;这是首次展示完全覆盖的手术机器人的无标记本体感觉，降低了设置复杂性，提高了安全性，并为模块化和自主机器人手术铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;尽管手术机器人具有机械精密性，但它们仍然无法感知周围环境。这种空间意识的缺乏导致碰撞、系统恢复和工作流程中断等问题，随着具有独立交互臂的分布式机器人的引入，这些问题将加剧。现有的追踪系统依赖笨重的红外摄像头和反射标记，仅提供手术场景的有限视角，并在拥挤的手术室中增加硬件负担。我们提出了一种无需标记的本体感觉方法，使手术机器人在无菌遮挡的情况下能够精确定位，尽管视觉线索被遮挡。我们的方法仅依靠轻量级立体RGB摄像头和基于Transformer的新型深度学习模型。它基于迄今为止最大的多中心空间机器人手术数据集（来自人体尸体和临床前体内研究的140万张自注释图像）。通过跟踪整个机器人和手术场景，而不是单个标记，我们的方法提供了对遮挡具有鲁棒性的整体视图，支持手术场景理解和上下文感知控制。我们展示了体内呼吸补偿的潜在临床应用示例，可以获取最先进追踪技术无法观察到的组织动力学，并在多机器人系统中精确定位以支持未来的智能交互。此外，与现有系统相比，我们的方法消除了标记并将追踪可见性提高了25%。据我们所知，这是首次展示完全覆盖的手术机器人的无标记本体感觉，降低了设置复杂性，提高了安全性，并为模块化和自主机器人手术铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决手术机器人在被无菌布覆盖后无法精确定位的问题。这个问题很重要，因为当前手术机器人缺乏环境感知能力，会导致碰撞、系统恢复和工作流程中断，随着分布式多臂机器人系统的普及，这些问题会更加严重。现有的红外跟踪系统笨重、容易被遮挡、需要严格校准，且难以扩展到多机器人环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有红外跟踪系统的局限性，然后设计了基于轻量级立体RGB相机和Transformer深度学习模型的解决方案。他们借鉴了工业机器人中的无标记定位方法，但进行了修改以适应外科环境中的无菌布遮挡问题。方法核心是立体可微分渲染技术，结合了粒子群优化算法进行初始估计，并通过'上下文先验'方法迭代改进分割结果。作者还构建了140万张图像的大型多中心数据集来训练模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用轻量级立体RGB相机和深度学习模型，通过立体可微分渲染技术实现对被覆盖手术机器人的精确定位，无需传统反射标记。整体流程包括：1)收集和预处理多中心数据集；2)训练能够处理遮挡的分割模型；3)使用相机群优化进行初始估计；4)应用立体可微分渲染优化姿态估计；5)使用'上下文先验'方法迭代改进结果；6)在临床前和临床环境中验证方法。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次实现完全覆盖的无标记手术机器人定位；2)开发立体可微分渲染技术处理遮挡；3)构建最大规模的多中心手术机器人数据集；4)开发遮挡不变的分割方法；5)提出'上下文先验'方法改进分割；6)支持多机器人设置。相比之前工作，本文方法无需标记、硬件更轻量(轻13倍、体积小3倍)、提供更完整的场景理解、能捕获传统系统不可见的组织动态，并在真实临床条件下验证了亚毫米级定位精度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于轻量级立体RGB相机和深度学习的无标记定位方法，实现了对被无菌布覆盖的手术机器人的精确定位，提高了手术场景可见性并揭示了传统系统无法观察到的组织动态，为模块化和自主机器人手术铺平了道路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite their mechanical sophistication, surgical robots remain blind totheir surroundings. This lack of spatial awareness causes collisions, systemrecoveries, and workflow disruptions, issues that will intensify with theintroduction of distributed robots with independent interacting arms. Existingtracking systems rely on bulky infrared cameras and reflective markers,providing only limited views of the surgical scene and adding hardware burdenin crowded operating rooms. We present a marker-free proprioception method thatenables precise localisation of surgical robots under their sterile drapingdespite associated obstruction of visual cues. Our method solely relies onlightweight stereo-RGB cameras and novel transformer-based deep learningmodels. It builds on the largest multi-centre spatial robotic surgery datasetto date (1.4M self-annotated images from human cadaveric and preclinical invivo studies). By tracking the entire robot and surgical scene, rather thanindividual markers, our approach provides a holistic view robust to occlusions,supporting surgical scene understanding and context-aware control. Wedemonstrate an example of potential clinical benefits during in vivo breathingcompensation with access to tissue dynamics, unobservable under state of theart tracking, and accurately locate in multi-robot systems for futureintelligent interaction. In addition, and compared with existing systems, ourmethod eliminates markers and improves tracking visibility by 25%. To ourknowledge, this is the first demonstration of marker-free proprioception forfully draped surgical robots, reducing setup complexity, enhancing safety, andpaving the way toward modular and autonomous robotic surgery.</description>
      <author>example@mail.com (Martin Huber, Nicola A. Cavalcanti, Ayoob Davoodi, Ruixuan Li, Christopher E. Mower, Fabio Carrillo, Christoph J. Laux, Francois Teyssere, Thibault Chandanson, Antoine Harlé, Elie Saghbiny, Mazda Farshad, Guillaume Morel, Emmanuel Vander Poorten, Philipp Fürnstahl, Sébastien Ourselin, Christos Bergeles, Tom Vercauteren)</author>
      <guid isPermaLink="false">2510.23512v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception</title>
      <link>http://arxiv.org/abs/2510.23478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025. Including supplemental material. For code  and dataset, see https://github.com/thi-ad/UrbanIng-V2X&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UrbanIng-V2X是首个大规模、多模态数据集，支持德国Ingolstadt三个城市交叉路口的车辆和基础设施传感器之间的合作感知，包含34个20秒的传感器序列，提供多种传感器数据，并以10Hz频率标注3D边界框。&lt;h4&gt;背景&lt;/h4&gt;现有合作感知数据集在智能移动应用中起关键作用，但真实世界数据集通常仅限于单个交叉路口或单辆车，缺乏多连接车辆和基础设施传感器跨越多个交叉路口的全面感知数据集，限制了算法在多样化交通环境中的基准测试。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集的局限性，引入首个大规模、多模态合作感知数据集UrbanIng-V2X，支持车辆和基础设施传感器之间的合作感知。&lt;h4&gt;方法&lt;/h4&gt;在德国Ingolstadt的三个城市交叉路口部署传感器，收集34个时间对齐和空间校准的传感器序列(每个20秒)，涉及两辆车和最多三个基础设施传感器杆，使用12个车载RGB摄像头、2个车载LiDAR、17个基础设施热成像摄像头和12个基础设施LiDAR，以10Hz频率标注13个类别的3D边界框。&lt;h4&gt;主要发现&lt;/h4&gt;提供使用最先进的合作感知方法的全面评估，验证了数据集的有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;公开提供代码库、数据集、高清地图和完整数据收集环境的数字孪生，促进合作感知领域的研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;近期的合作感知数据集通过促进智能体之间的信息交换，在推进智能移动应用方面发挥了关键作用，帮助克服遮挡等挑战，并提高整体场景理解能力。虽然一些现有的真实世界数据集同时包含车辆对车辆和车辆对基础设施的交互，但它们通常仅限于单个交叉路口或单辆车。一个包含多个连接车辆和基础设施传感器跨越多个交叉路口的全面感知数据集仍然不可用，限制了算法在多样化交通环境中的基准测试。因此，可能会发生过拟合，模型可能由于相似的交叉路口布局和交通参与者行为而表现出误导性的高性能。为解决这一差距，我们引入了UrbanIng-V2X，这是首个大规模、多模态数据集，支持在德国Ingolstadt三个城市交叉路口部署的车辆和基础设施传感器之间的合作感知。UrbanIng-V2X包含34个时间对齐和空间校准的传感器序列，每个持续20秒。所有序列包含三个交叉路口中一个的记录，涉及两辆车和最多三个基础设施安装的传感器杆，在协调场景中运行。总的来说，UrbanIng-V2X提供来自12个车载RGB摄像头、2个车载LiDAR、17个基础设施热成像摄像头和12个基础设施LiDAR的数据。所有序列以10Hz的频率标注3D边界框，涵盖13个对象类别，导致整个数据集大约有712k个标注实例。我们使用最先进的合作感知方法提供了全面评估，并公开提供了代码库、数据集、高清地图和完整数据收集环境的数字孪生。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是缺乏一个大规模、多车辆、多基础设施、多交叉路口的合作感知数据集。这个问题在现实中很重要，因为城市交叉路口是自动驾驶中最复杂的场景之一，单一智能体的感知系统常因视野受限和遮挡而无法检测关键物体，而合作感知可以克服这些限制。缺乏多样性的数据集会导致算法过拟合，模型可能因相似场景而表现出误导性的高性能，限制了真实世界应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有数据集的不足（如只包含单个交叉路口或车辆、缺乏多种传感器类型）来设计方法。他们精心设计了传感器部署（两辆车各配备6个RGB摄像头和1个激光雷达，三个交叉路口配备热成像摄像头和激光雷达系统），实现了精确的传感器同步和校准，并从8小时数据中挑选34个代表性场景进行标注。作者借鉴了现有工作如V2V4Real、DAIR-V2X-C等数据集的经验，同时采用了类似nuScenes的标注方法和OpenCOOD的格式转换工具。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模、多模态、多交叉路口的合作感知数据集，使研究人员能够开发和评估在复杂城市环境中能有效协作的感知算法。整体流程包括：1)传感器部署（车载和基础设施）；2)数据采集（三个交叉路口34个20秒序列）；3)传感器同步与校准（UTC时钟同步、精确校准）；4)场景选择与标注（多样化光照条件、10Hz频率标注13个物体类别）；5)数据发布与工具提供（代码库、高清地图、数字孪生）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个多车辆、多基础设施、多交叉路口的合作感知数据集；2)引入最多的合作传感器和热成像相机等多模态传感器；3)支持多种合作感知基准任务；4)提供综合基准评估；5)提供开发者工具包和数字孪生。相比之前工作，UrbanIng-V2X同时支持多车辆和多基础设施，覆盖多个交叉路口，提供更丰富的传感器组合和更全面的标注（13个类别、712k实例），还提供了数字孪生环境和新的数据集分割策略以评估泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UrbanIng-V2X数据集通过提供首个大规模、多车辆、多基础设施、多交叉路口的合作感知数据集，克服了现有数据集的局限性，为开发和评估在复杂城市环境中能有效协作的感知算法提供了坚实基础，同时揭示了模型在未见环境中的泛化挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent cooperative perception datasets have played a crucial role inadvancing smart mobility applications by enabling information exchange betweenintelligent agents, helping to overcome challenges such as occlusions andimproving overall scene understanding. While some existing real-world datasetsincorporate both vehicle-to-vehicle and vehicle-to-infrastructure interactions,they are typically limited to a single intersection or a single vehicle. Acomprehensive perception dataset featuring multiple connected vehicles andinfrastructure sensors across several intersections remains unavailable,limiting the benchmarking of algorithms in diverse traffic environments.Consequently, overfitting can occur, and models may demonstrate misleadinglyhigh performance due to similar intersection layouts and traffic participantbehavior. To address this gap, we introduce UrbanIng-V2X, the firstlarge-scale, multi-modal dataset supporting cooperative perception involvingvehicles and infrastructure sensors deployed across three urban intersectionsin Ingolstadt, Germany. UrbanIng-V2X consists of 34 temporally aligned andspatially calibrated sensor sequences, each lasting 20 seconds. All sequencescontain recordings from one of three intersections, involving two vehicles andup to three infrastructure-mounted sensor poles operating in coordinatedscenarios. In total, UrbanIng-V2X provides data from 12 vehicle-mounted RGBcameras, 2 vehicle LiDARs, 17 infrastructure thermal cameras, and 12infrastructure LiDARs. All sequences are annotated at a frequency of 10 Hz with3D bounding boxes spanning 13 object classes, resulting in approximately 712kannotated instances across the dataset. We provide comprehensive evaluationsusing state-of-the-art cooperative perception methods and publicly release thecodebase, dataset, HD map, and a digital twin of the complete data collectionenvironment.</description>
      <author>example@mail.com (Karthikeyan Chandra Sekaran, Markus Geisler, Dominik Rößle, Adithya Mohan, Daniel Cremers, Wolfgang Utschick, Michael Botsch, Werner Huber, Torsten Schön)</author>
      <guid isPermaLink="false">2510.23478v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions</title>
      <link>http://arxiv.org/abs/2510.22798v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025. Project Website: https://vehme.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍VEHME，一种用于评估手写数学表达式的视觉语言模型，能够以高准确性和可解释的推理痕迹评估开放形式的手写数学答案。&lt;h4&gt;背景&lt;/h4&gt;自动评估手写数学解题是教育技术中的重要问题，具有实际应用，但由于学生作业的多样格式、非结构化布局和符号复杂性，这仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发VEHME模型，以高准确性和可解释的推理痕迹评估开放形式的手写数学答案。&lt;h4&gt;方法&lt;/h4&gt;VEHME采用两阶段训练管道：使用结构化推理数据进行监督微调；通过强化学习使模型输出与多维度评分目标（正确性、推理深度和错误定位）保持一致；提出表达式感知的视觉提示模块，在合成的多行数学表达式数据集上训练，以在视觉异构输入中稳健地引导注意力。&lt;h4&gt;主要发现&lt;/h4&gt;在AIHub和FERMAT数据集上评估，VEHME在开源模型中取得了最先进的性能，并接近专有系统的准确性。&lt;h4&gt;结论&lt;/h4&gt;VEHME展示了其作为可扩展且可访问的自动数学评估工具的潜力，训练和实验代码已在GitHub公开存储库中可用。&lt;h4&gt;翻译&lt;/h4&gt;自动评估手写数学解题是教育技术中的一个重要问题，具有实际应用，但由于学生作业的多样格式、非结构化布局和符号复杂性，这仍然是一个重大挑战。为应对这一挑战，我们介绍了VEHME——一种用于评估手写数学表达式的视觉语言模型——旨在以高准确性和可解释的推理痕迹评估开放形式的手写数学答案。VEHME集成了一个两阶段训练管道：(i) 使用结构化推理数据进行监督微调，(ii) 强化学习使模型输出与多维度评分目标（包括正确性、推理深度和错误定位）保持一致。为增强空间理解，我们提出了一个表达式感知的视觉提示模块，在我们合成的多行数学表达式数据集上训练，以在视觉异构输入中稳健地引导注意力。在AIHub和FERMAT数据集上评估，VEHME在开源模型中取得了最先进的性能，并接近专有系统的准确性，展示了其作为可扩展且可访问的自动数学评估工具的潜力。我们的训练和实验代码已在GitHub公开存储库中提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatically assessing handwritten mathematical solutions is an importantproblem in educational technology with practical applications, but it remains asignificant challenge due to the diverse formats, unstructured layouts, andsymbolic complexity of student work. To address this challenge, we introduceVEHME-a Vision-Language Model for Evaluating Handwritten MathematicsExpressions-designed to assess open-form handwritten math responses with highaccuracy and interpretable reasoning traces. VEHME integrates a two-phasetraining pipeline: (i) supervised fine-tuning using structured reasoning data,and (ii) reinforcement learning that aligns model outputs withmulti-dimensional grading objectives, including correctness, reasoning depth,and error localization. To enhance spatial understanding, we propose anExpression-Aware Visual Prompting Module, trained on our synthesized multi-linemath expressions dataset to robustly guide attention in visually heterogeneousinputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-artperformance among open-source models and approaches the accuracy of proprietarysystems, demonstrating its potential as a scalable and accessible tool forautomated math assessment. Our training and experiment code is publiclyavailable at our GitHub repository.</description>
      <author>example@mail.com (Thu Phuong Nguyen, Duc M. Nguyen, Hyotaek Jeon, Hyunwook Lee, Hyunmin Song, Sungahn Ko, Taehwan Kim)</author>
      <guid isPermaLink="false">2510.22798v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2510.22706v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/lifuguan/IGGT_official&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了InstanceGrounded Geometry Transformer (IGGT)，一种端到端的大型统一transformer，用于统一3D场景的空间重建和实例级上下文理解。&lt;h4&gt;背景&lt;/h4&gt;人类自然将3D世界的几何结构和语义内容作为相互交织的维度感知，但先前方法优先训练几何模型进行低级3D重建，将高级空间理解孤立处理，忽视了两者间的相互作用，限制了泛化能力和下游任务表现。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一框架，同时处理3D场景的几何结构和语义理解，提高3D场景分析的准确性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出IGGT模型，设计3D一致性对比学习策略，通过仅2D视觉输入编码具有几何结构和实例聚类的统一表示，并构建InsScene-15K数据集，包含高质量RGB图像、姿态、深度图和3D一致的实例级掩码注释。&lt;h4&gt;主要发现&lt;/h4&gt;通过统一几何结构和语义理解的方法，可以改善3D场景的理解和重建效果，实现从2D输入到连贯3D场景的有效转换。&lt;h4&gt;结论&lt;/h4&gt;所提出的IGGT方法和3D一致性对比学习策略能够有效地将2D视觉输入转换为连贯的3D场景，并明确区分对象实例，为3D场景分析提供了新的统一框架。&lt;h4&gt;翻译&lt;/h4&gt;人类自然地将3D世界的几何结构和语义内容作为相互交织的维度来感知，从而能够连贯准确地理解复杂场景。然而，先前的方法优先训练大型几何模型进行低级3D重建，将高级空间理解孤立处理，忽视了这两个基本方面之间的相互作用，从而限制了泛化能力并在下游3D理解任务中表现不佳。最近的尝试通过简单地将3D模型与特定语言模型对齐来缓解这一问题，从而限制了感知能力，并限制了下游任务的适应性。在本文中，我们提出了InstanceGrounded Geometry Transformer (IGGT)，一个端到端的大型统一transformer，用于统一空间重建和实例级上下文理解的知识。具体来说，我们设计了一种3D一致性对比学习策略，指导IGGT仅通过2D视觉输入来编码具有几何结构和实例聚类的统一表示。该表示支持将2D视觉输入一致提升为具有明确不同对象实例的连贯3D场景。为了促进这一任务，我们进一步构建了InsScene-15K，这是一个大规模数据集，包含高质量的RGB图像、姿态、深度图和3D一致的实例级掩码注释，采用新颖的数据整理流程。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D场景几何重建与高层次语义理解相分离的问题。人类自然地将3D世界的几何结构和语义内容视为交织在一起的维度，而现有方法通常将这两个方面视为独立任务，导致它们无法相互增强，限制了模型在下游任务中的泛化能力和性能。这个问题很重要，因为统一的几何-语义表示对于机器人操作、增强现实/虚拟现实和空间规划等应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法的局限性：几何重建和语义理解被分离处理，或简单地将3D模型与特定语言模型对齐，导致感知能力受限和适应性差。他们设计了一个端到端的统一框架，通过联合训练耦合几何和语义特征，让模型自主学习3D实例级语义与几何结构的关系。作者借鉴了VGGT的结构，使用DINOv2进行特征提取，采用DPT架构进行密集预测，并利用SAM2进行数据标注。他们还创新性地设计了3D一致的对比学习策略来增强模型性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合训练将几何和实例级语义特征耦合，实现上下文理解和几何重建的相互改进。整体流程包括：1)接收多视图图像输入；2)使用大型统一变换器提取统一表示；3)通过几何头和实例头分别预测几何结构和实例特征；4)应用跨模态融合块增强实例特征的细粒度空间感知；5)使用3D一致的对比监督确保多视图一致性；6)通过聚类生成实例掩码，用于下游任务如空间跟踪、开放词汇分割和场景理解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的3D重建与理解框架IGGT；2)3D一致的对比学习策略；3)InsScene-15K大规模数据集；4)实例级场景理解范式。相比之前的工作，不同之处在于：不是简单对齐几何与语言特征，而是通过联合训练实现相互增强；不与特定视觉语言模型紧密耦合，可以集成更强大的基础模型；能够区分同一语义类别内的不同对象，支持更复杂的下游应用；实现了即插即用的方式与各种视觉语言模型和大型多模态模型集成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; IGGT通过提出统一的实例级几何变换器框架和3D一致的对比学习策略，实现了几何重建与语义理解的深度融合，显著提升了3D场景重建与理解的质量和一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans naturally perceive the geometric structure and semantic content of a3D world as intertwined dimensions, enabling coherent and accurateunderstanding of complex scenes. However, most prior approaches prioritizetraining large geometry models for low-level 3D reconstruction and treathigh-level spatial understanding in isolation, overlooking the crucialinterplay between these two fundamental aspects of 3D-scene analysis, therebylimiting generalization and leading to poor performance in downstream 3Dunderstanding tasks. Recent attempts have mitigated this issue by simplyaligning 3D models with specific language models, thus restricting perceptionto the aligned model's capacity and limiting adaptability to downstream tasks.In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), anend-to-end large unified transformer to unify the knowledge for both spatialreconstruction and instance-level contextual understanding. Specifically, wedesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encodea unified representation with geometric structures and instance-groundedclustering through only 2D visual inputs. This representation supportsconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitlydistinct object instances. To facilitate this task, we further constructInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depthmaps, and 3D-consistent instance-level mask annotations with a novel datacuration pipeline.</description>
      <author>example@mail.com (Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, Manyuan Zhang, Gang Yu, Dingwen Zhang, Ziwei Liu)</author>
      <guid isPermaLink="false">2510.22706v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework for Lane Keeping in Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2510.22370v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/Amin-A96/BLIP-FusePPO-A-Vision-Language-Deep-Reinforcement-Learning-Framework-for-Lane-Keeping-in-Autonomous.git&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了BLIP-FusePPO，一种用于自动驾驶车道保持的新型多模态强化学习框架，将视觉语言模型生成的语义嵌入与几何状态、LiDAR观测和PID控制反馈相融合。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶中的车道保持需要结合高级语义理解和低级控制信号，而现有方法可能仅使用语义模型来塑造奖励或未充分利用多模态信息。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够学习具有环境意识且易于理解的驾驶规则的多模态强化学习框架，结合视觉语言模型的高级场景理解与低级控制和空间信号。&lt;h4&gt;方法&lt;/h4&gt;提出BLIP-FusePPO框架，将视觉语言模型生成的语义嵌入直接融合到代理观测空间中的几何状态、LiDAR观测和PID控制反馈，结合语义、几何和控制感知表示，并使用包含语义对齐、车道保持准确性、障碍物避让和速度调节的混合奖励函数。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在车道保持的稳定性和适应性方面优于最佳视觉和多模态强化学习基线，在各种困难的驾驶情况下表现良好，且直接嵌入语义特征减少了昂贵的运行时推理，确保语义指导始终可用。&lt;h4&gt;结论&lt;/h4&gt;BLIP-FusePPO是一个有效的多模态强化学习框架，能够提高自动驾驶车道保持任务的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了基于引导语言-图像预训练的融合状态表示近端策略优化（BLIP-FusePPO），这是一种用于自动驾驶车道保持的新型多模态强化学习框架，其中视觉语言模型生成的语义嵌入直接与几何状态、LiDAR观测和基于比例-积分-微分的控制反馈在代理观测空间内融合。所提出的方法通过结合视觉语言模型的高级场景理解与低级控制和空间信号，使代理能够学习具有环境意识且易于理解的驾驶规则。我们的架构将语义、几何和控制感知表示结合在一起，使策略学习更加稳健。包含语义对齐、车道保持准确性、障碍物避让和速度调节的混合奖励函数有助于学习更加高效和可泛化。我们的方法不同于仅使用语义模型来塑造奖励的方法，而是直接将语义特征嵌入到状态表示中。这减少了昂贵的运行时推理，并确保语义指导始终可用。仿真结果表明，在广泛的困难驾驶情况下，所提出的模型在车道保持的稳定性和适应性方面优于最佳视觉和多模态强化学习基线。我们公开提供代码。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶车辆车道保持任务中语义感知与控制感知融合不足的问题。这一问题在现实中很重要，因为现有系统在复杂环境（如车道标记磨损、不同光照条件或被遮挡车道）中表现有限，而缺乏对场景语义理解的系统难以适应多变路况，影响自动驾驶的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统视觉方法缺乏语义理解、多模态RL方法仅用VLM塑造奖励而非融入状态、传统控制器缺乏可解释性。作者借鉴了BLIP视觉语言模型用于语义提取、PPO算法用于稳定策略学习、PID控制器提供控制反馈等现有工作。创新点在于将语义特征与几何状态、LiDAR观测和PID控制反馈直接融合到状态表示中，而非仅用于奖励塑造。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将语义感知（通过BLIP提取）与控制感知（通过PID获取）直接融合到强化学习智能体的状态表示中，使智能体同时理解场景语义上下文和执行精确控制。整体流程包括：1)混合状态表示（RGB视觉、LiDAR数据、PID反馈、语义嵌入）；2)预处理和特征提取；3)数据增强（水平镜像）；4)连续动作空间设计（转向和速度控制）；5)混合奖励函数（车道保持、障碍物避免、速度匹配等）；6)使用PPO算法训练策略网络。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)新颖架构将BLIP语义嵌入和PID信号直接注入状态表示；2)基于PID控制的状态增强技术提高学习稳定性；3)新型混合奖励函数整合语义对齐和几何遵循；4)直接语义特征嵌入而非仅用于奖励塑造。相比之前工作，不同之处在于：传统方法缺乏语义理解、现有多模态RL方法仅用VLM塑造奖励、传统控制器缺乏可解释性、基于VLM的RL系统计算开销大。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了BLIP-FusePPO框架，通过融合语义感知与控制感知到状态表示中，显著提高了自动驾驶车道保持的稳定性和适应性，同时降低了计算开销，为更安全鲁棒的自动驾驶系统提供了新思路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose Bootstrapped Language-Image Pretraining-drivenFused State Representation in Proximal Policy Optimization (BLIP-FusePPO), anovel multimodal reinforcement learning (RL) framework for autonomouslane-keeping (LK), in which semantic embeddings generated by a vision-languagemodel (VLM) are directly fused with geometric states, LiDAR observations, andProportional-Integral-Derivative-based (PID) control feedback within the agentobservation space. The proposed method lets the agent learn driving rules thatare aware of their surroundings and easy to understand by combining high-levelscene understanding from the VLM with low-level control and spatial signals.Our architecture brings together semantic, geometric, and control-awarerepresentations to make policy learning more robust. A hybrid reward functionthat includes semantic alignment, LK accuracy, obstacle avoidance, and speedregulation helps learning to be more efficient and generalizable. Our method isdifferent from the approaches that only use semantic models to shape rewards.Instead, it directly embeds semantic features into the state representation.This cuts down on expensive runtime inference and makes sure that semanticguidance is always available. The simulation results show that the proposedmodel is better at LK stability and adaptability than the best vision-based andmultimodal RL baselines in a wide range of difficult driving situations. Wemake our code publicly available.</description>
      <author>example@mail.com (Seyed Ahmad Hosseini Miangoleh, Amin Jalal Aghdasian, Farzaneh Abdollahi)</author>
      <guid isPermaLink="false">2510.22370v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MOGRAS: Human Motion with Grasping in 3D Scenes</title>
      <link>http://arxiv.org/abs/2510.22199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  British Machine Vision Conference Workshop - From Scene Understanding  to Human Modeling&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了MOGRAS数据集和一种简单有效的方法，用于解决在3D场景中生成物理合理的全身抓取运动的挑战，通过定量和定性实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;生成与物体交互的真实全身运动对机器人技术、虚拟现实和人机交互应用至关重要，但现有方法要么缺乏精细任务的保真度，要么忽略了周围3D场景。&lt;h4&gt;目的&lt;/h4&gt;弥合现有方法在生成全身抓取运动方面的局限性，提供能够在3D场景中生成物理合理全身抓取运动的解决方案。&lt;h4&gt;方法&lt;/h4&gt;引入MOGRAS（3D场景中的人体抓取运动）数据集，提供预抓取全身行走运动和最终抓取姿态；利用该数据集基准测试现有方法；提出一种简单有效的方法使现有方法能在3D场景中无缝工作。&lt;h4&gt;主要发现&lt;/h4&gt;现有全身抓握方法在场景感知生成方面存在局限性；所提出的方法在全身抓取运动生成方面取得了显著改进；通过大量定量和定性实验验证了数据集的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究为更真实的人体-场景交互铺平了道路，展示了在3D场景中生成物理合理全身抓取运动的可行性。&lt;h4&gt;翻译&lt;/h4&gt;生成与物体交互的真实全身运动对机器人技术、虚拟现实和人机交互应用至关重要。虽然现有方法可以生成3D场景中的全身运动，但它们通常缺乏精细任务（如物体抓取）的保真度。相反，生成精确抓取运动的方法通常忽略了周围的3D场景。在3D场景中生成物理上合理的全身抓取运动仍然是一个重大挑战。为解决这一问题，我们引入了MOGRAS（3D场景中的人体抓取运动），这是一个弥合这一差距的大规模数据集。MOGRAS在丰富的3D室内场景标注中提供了预抓取的全身行走运动和最终抓取姿态。我们利用MOGRAS对现有的全身抓取方法进行基准测试，展示了它们在场景感知生成方面的局限性。此外，我们提出了一种简单而有效的方法，使现有方法能够在3D场景中无缝工作。通过大量的定量和定性实验，我们验证了数据集的有效性，并突显了我们提出方法所取得的显著改进，为更真实的人体-场景交互铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在3D场景中生成物理合理的全身抓取运动的问题。现有方法要么能生成全身运动但缺乏精细抓保真度，要么能生成精确抓取但忽略周围3D场景。这个问题对机器人、虚拟现实和人机交互等领域至关重要，因为准确建模人-物体交互能支持行为分析、智能机器人系统开发和逼真虚拟环境创建。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别现有研究的差距：全身运动方法缺乏精细抓取能力，抓取方法忽略场景上下文。考虑到手动捕获此类数据成本高昂，他们设计了自动化数据生成框架。作者借鉴了HUMANISE的运动对齐方法、AMASS的行走序列、ScanNetv2的3D环境、BABEL的运动标签、GRAB的抓取物体，并改进了FLEX的抓取姿势生成和PriorMDM的运动填充技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建MOGRAS数据集，提供3D室内场景中的全身抓取运动序列，包括预抓取行走和最终抓取姿势。实现流程分五步：1)行走运动对齐和物体放置；2)改进ScanNet场景的地板对齐；3)使用改进的FLEX生成抓取姿势；4)用PriorMDM生成从行走到抓取的平滑过渡；5)确保数据集规模和质量，通过自动过滤和人类评估保证物理合理性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)MOGRAS数据集：首个结合全身运动、精细抓取和3D场景的大规模合成数据集；2)GNet++方法：通过场景编码和穿透损失实现场景感知抓取；3)场景处理改进：解决地板不对齐问题。相比之前工作，MOGRAS是首个同时包含三种元素(3D场景、精细抓取、全身运动)的数据集，而GNet++显式考虑环境约束，而之前方法如GOAL和SAGA忽略了场景上下文。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过引入首个大规模场景感知全身抓取数据集MOGRAS和相应生成方法GNet++，论文弥合了3D场景中全身运动生成与精细物体抓取之间的差距，为更真实的人-场景交互研究奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating realistic full-body motion interacting with objects is criticalfor applications in robotics, virtual reality, and human-computer interaction.While existing methods can generate full-body motion within 3D scenes, theyoften lack the fidelity for fine-grained tasks like object grasping.Conversely, methods that generate precise grasping motions typically ignore thesurrounding 3D scene. This gap, generating full-body grasping motions that arephysically plausible within a 3D scene, remains a significant challenge. Toaddress this, we introduce MOGRAS (Human MOtion with GRAsping in 3D Scenes), alarge-scale dataset that bridges this gap. MOGRAS provides pre-graspingfull-body walking motions and final grasping poses within richly annotated 3Dindoor scenes. We leverage MOGRAS to benchmark existing full-body graspingmethods and demonstrate their limitations in scene-aware generation.Furthermore, we propose a simple yet effective method to adapt existingapproaches to work seamlessly within 3D scenes. Through extensive quantitativeand qualitative experiments, we validate the effectiveness of our dataset andhighlight the significant improvements our proposed method achieves, paving theway for more realistic human-scene interactions.</description>
      <author>example@mail.com (Kunal Bhosikar, Siddharth Katageri, Vivek Madhavaram, Kai Han, Charu Sharma)</author>
      <guid isPermaLink="false">2510.22199v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>CogStereo: Neural Stereo Matching with Implicit Spatial Cognition Embedding</title>
      <link>http://arxiv.org/abs/2510.22119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为CogStereo的新型立体匹配框架，通过引入空间认知机制来改进立体匹配性能，特别是在处理遮挡或弱纹理等挑战性区域时表现出色，同时提高了跨域泛化能力。&lt;h4&gt;背景&lt;/h4&gt;深度立体匹配通过微调在基准数据集上取得了显著进展，但在零样本泛化方面不如其他视觉任务中的基础模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖数据集特定先验的框架，解决立体匹配中的挑战性问题，提高跨域泛化能力，并将立体视觉转向认知驱动的方法。&lt;h4&gt;方法&lt;/h4&gt;CogStereo通过使用单目深度特征作为先验，将隐式空间认知嵌入到细化过程中，捕获超越局部对应的全场景理解。该方法采用双条件细化机制，结合逐像素不确定性和认知引导特征，实现对不匹配的一致性全局校正。&lt;h4&gt;主要发现&lt;/h4&gt;CogStereo在Scene Flow、KITTI、Middlebury、ETH3D、EuRoc和真实世界等多个数据集上实现了最先进的结果，并在跨域泛化方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;CogStereo成功解决了传统立体匹配方法在处理挑战性区域时的局限性，通过引入空间认知机制提高了立体视觉系统的泛化能力，推动了立体视觉向认知驱动方向发展。&lt;h4&gt;翻译&lt;/h4&gt;深度立体匹配通过微调在基准数据集上取得了显著进展，但在零样本泛化方面不如其他视觉任务中的基础模型。我们引入了CogStereo，一种新颖的框架，它解决了遮挡或弱纹理等挑战性区域，而不依赖于数据集特定的先验。CogStereo通过使用单目深度特征作为先验，将隐式空间认知嵌入到细化过程中，捕获超越局部对应的全局场景理解。这种方法确保了结构一致性的视差估计，即使在仅靠几何不足的区域。CogStereo采用双条件细化机制，结合逐像素不确定性和认知引导特征，实现对不匹配的一致性全局校正。在Scene Flow、KITTI、Middlebury、ETH3D、EuRoc和真实世界上的大量实验表明，CogStereo不仅取得了最先进的结果，还在跨域泛化方面表现出色，将立体视觉转向认知驱动的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决深度立体匹配方法在零样本泛化能力上的不足问题。当前方法虽然在基准数据集上表现良好，但在遮挡区域、弱纹理等困难区域表现不佳，且缺乏强大的零样本泛化能力。这个问题在自动驾驶、机器人等应用中至关重要，因为这些应用需要在各种不同环境下保持一致的深度估计性能，而现有方法过度依赖局部几何对应，缺乏全局场景理解能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察立体匹配与其他视觉任务基础模型的差距，引入了'空间认知'概念，借鉴单目深度估计的成功经验，特别是Depth Anything v2捕获的物体级几何和全局场景理解能力。作者还借鉴了条件控制思想，设计了双条件修正机制。创新之处在于将不确定性估计提前到成本体积阶段，而非视差回归之后，并设计了不确定性引导的空间认知注意力机制、低不确定性区域的KNN对齐策略和突然深度差异感知梯度损失等组件。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将隐式空间认知嵌入到立体匹配过程中，利用单目深度特征作为先验，超越局部对应关系，捕获整体场景理解。整体流程分为两阶段：第一阶段是成本体积不确定性估计预训练，使用标准立体匹配骨干网络提取特征，构建三维成本体积，并预测每个像素的不确定性；第二阶段是通过空间认知的双条件修正，整合不确定性先验与空间认知特征，使用注意力机制进行视差修正，并通过KNN对齐防止度量漂移，最后应用特殊损失函数确保视差图的平滑性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将隐式空间认知概念引入立体匹配；2)设计双条件修正机制结合不确定性和认知特征；3)在成本体积阶段而非视差回归后进行不确定性估计；4)引入低不确定性区域的KNN对齐策略防止度量漂移；5)设计突然深度差异感知梯度损失。相比之前工作，CogStereo超越了纯几何匹配，实现了强大的零样本泛化，改变了不确定性处理方式，创新性地利用深度基础模型的中间特征而非原始深度预测，并针对遮挡、弱纹理等困难区域提供了更鲁棒的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CogStereo通过引入隐式空间认知嵌入到立体匹配过程中，结合像素级不确定性与认知引导特征，显著提升了在遮挡、弱纹理等困难区域的零样本泛化能力和视差估计准确性，实现了从纯几何匹配向认知驱动立体视觉的转变。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep stereo matching has advanced significantly on benchmark datasets throughfine-tuning but falls short of the zero-shot generalization seen in foundationmodels in other vision tasks. We introduce CogStereo, a novel framework thataddresses challenging regions, such as occlusions or weak textures, withoutrelying on dataset-specific priors. CogStereo embeds implicit spatial cognitioninto the refinement process by using monocular depth features as priors,capturing holistic scene understanding beyond local correspondences. Thisapproach ensures structurally coherent disparity estimation, even in areaswhere geometry alone is inadequate. CogStereo employs a dual-conditionalrefinement mechanism that combines pixel-wise uncertainty with cognition-guidedfeatures for consistent global correction of mismatches. Extensive experimentson Scene Flow, KITTI, Middlebury, ETH3D, EuRoc, and real-world demonstrate thatCogStereo not only achieves state-of-the-art results but also excels incross-domain generalization, shifting stereo vision towards a cognition-drivenapproach.</description>
      <author>example@mail.com (Lihuang Fang, Xiao Hu, Yuchen Zou, Hong Zhang)</author>
      <guid isPermaLink="false">2510.22119v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.21881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对大型语言模型在几何视觉推理任务中的性能下降问题，开发了一个名为GeoThoughts的几何推理数据集和一个名为GeoThought-MLLM的多模态数学推理模型，通过链式思维训练显著提升了模型在几何问题上的表现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在文本数学问题解决中表现出强大的推理能力，但在视觉推理任务，特别是几何问题解决中，其性能显著下降。这是因为几何问题具有独特挑战：一是几何本身的复杂性需要详细的图像理解和多步推理；二是现有数据集规模不足、多样性有限且缺乏明确的推理痕迹，阻碍了有效模型训练。&lt;h4&gt;目的&lt;/h4&gt;开发一个全面的几何推理数据集和一个多模态数学推理模型，以解决大型语言模型在几何问题解决中的性能下降问题。&lt;h4&gt;方法&lt;/h4&gt;1. 创建了GeoThoughts数据集，包含两个子集：Geo-Thought-6K（6,243个样本）和Geo-Thought-Augmented-10K（10,834个样本）。2. 每个数据条目包括视觉描述、分步解决方案、明确的推理链、反思步骤和最终答案。3. 基于此数据集开发了GeoThought-MLLM，一个在问题解决过程中生成详细思考过程的多模态数学推理模型。&lt;h4&gt;主要发现&lt;/h4&gt;1. 使用链式思维数据集训练的GeoThought-MLLM在几何任务上优于现有基准。2. 训练显著提升了模型在领域内和领域外几何推理能力。3. 错误主要源于数学概念错误解释或空间判断失误。4. 通过调用链式思维纠正这些错误，模型能够产生正确答案。&lt;h4&gt;结论&lt;/h4&gt;GeoThoughts数据集和GeoThought-MLLM模型有效解决了大型语言模型在几何问题解决中的性能下降问题，为几何视觉推理任务提供了新的解决方案和见解。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在基于文本的数学问题解决中表现出强大的推理能力；然而，当适应到视觉推理任务，特别是几何问题解决时，它们的性能大幅下降，因为几何问题带来了独特的挑战。具体来说，这些挑战源于两个关键因素：首先，几何本身的复杂性需要详细的图像理解和多步推理；其次，现有数据集的规模、多样性和明确的推理痕迹不足，从而阻碍了有效的模型训练。为应对这些挑战，我们开发了GeoThoughts数据集，这是一个全面的几何推理语料库，包含两个子集：包含6,243个样本的Geo-Thought-6K及其增强版本Geo-Thought-Augmented-10K，包含10,834个样本。每个条目包括视觉描述、分步解决方案、明确的推理链、反思步骤和最终答案。使用此数据集，我们开发了GeoThought-MLLM，一个在问题解决过程中生成详细思考过程的数学推理多模态模型。我们的模型在几何任务上优于现有基准，证明使用我们的链式思维数据集进行训练可以提升领域内和领域外设置的几何推理能力。最后，我们分析了失败案例，发现错误主要源于数学概念错误解释或空间判断失误。通过调用链式思维纠正这些错误，模型产生了正确答案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated strong reasoning capabilitiesin text-based mathematical problem solving; however, when adapted to visualreasoning tasks, particularly geometric problem solving, their performancesubstantially declines because geometric problems present unique challenges.Specifically, these challenges stem from two key factors: first, the intrinsiccomplexity of geometry requiring detailed image comprehension and multi-stepreasoning, and second, the limitations of existing datasets which lacksufficient scale, diversity, and explicit reasoning traces, consequentlyhindering effective model training. To address these challenges, we developedthe GeoThoughts dataset, a comprehensive geometric reasoning corpus with twosubsets: Geo-Thought-6K with 6,243 samples and its augmented versionGeo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visualdescriptions, step-by-step solutions, explicit reasoning chains, reflectionsteps, and final answers. Using this dataset, we developed GeoThought-MLLM, amathematical reasoning multimodal model that generates detailed thinkingprocesses during problem-solving. Our model outperforms existing benchmarks ingeometric tasks, demonstrating that training with our Chain-of-Thought datasetimproves geometric reasoning capabilities across both in-domain andout-of-domain settings. Finally, we analyze failure cases and observe thaterrors primarily arise from incorrect interpretation of mathematical conceptsor spatial misjudgment. By invoking CoT to correct these mistakes, the modelproduces correct answers.</description>
      <author>example@mail.com (Nannan Shi, Chuanyu Qin, Shipeng Song, Man Luo)</author>
      <guid isPermaLink="false">2510.21881v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>GRAPHIA: Harnessing Social Graph Data to Enhance LLM-Based Social Simulation</title>
      <link>http://arxiv.org/abs/2510.24251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Graphia是一种基于大型语言模型的社会图模拟框架，利用图数据作为监督信号通过强化学习对LLM进行后训练，能够预测互动对象和互动方式，在微观和宏观层面都显示出显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在模拟类人社会行为方面展现出潜力，但社会图作为包含局部交互和全局网络结构的高质量监督信号，在LLM训练中仍未得到充分利用。&lt;h4&gt;目的&lt;/h4&gt;提出Graphia框架，利用图数据作为监督信号，通过强化学习对LLM进行后训练，以缩小代理行为与基于LLM的模拟中网络动力学之间的差距。&lt;h4&gt;方法&lt;/h4&gt;Graphia训练专门的代理来预测与谁互动（目标选择）和如何互动（边生成），使用基于GNN的结构性奖励，并设计了图生成流程。在归因动态图生成（TDGG）和归纳动态图生成（IDGG）两种设置下进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界网络上，Graphia相比最强基线模型，微观对齐方面：综合目标选择分数提高6.1%，边分类准确率提高12%，边内容BERTScore提高27.9%；宏观对齐方面：结构相似性提高41.11%，社会现象（如幂律和回音室）复制能力提高32.98%。Graphia还支持反事实模拟，能在平台激励下生成合理的行为转变。&lt;h4&gt;结论&lt;/h4&gt;社会图可以作为LLM后训练的高质量监督信号，有效缩小基于LLM的模拟中代理行为与网络动力学之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在模拟类人社会行为方面展现出潜力。社会图提供了高质量监督信号，编码了局部交互和全局网络结构，但这些信号在LLM训练中仍未得到充分利用。为解决这一差距，我们提出了Graphia，这是第一个基于LLM的社会图模拟通用框架，它利用图数据作为监督信号，通过强化学习对LLM进行后训练。基于GNN的结构性奖励，Graphia训练专门的代理来预测与谁互动（目标选择）和如何互动（边生成），然后使用设计的图生成流程。我们在两种设置下评估Graphia：归因动态图生成（TDGG），这是使用我们提出的节点级交互对齐指标的微观任务；以及归纳动态图生成（IDGG），这是使用我们提出的指标对齐涌现网络属性的宏观任务。在三个真实世界网络上，Graphia相比最强基线模型，在微观对齐方面提高了6.1%的综合目标选择分数，12%的边分类准确率和27.9%的边内容BERTScore。对于宏观对齐，它实现了41.11%更高的结构相似性和32.98%更好的社会现象（如幂律和回音室）复制能力。Graphia还支持反事实模拟，在平台激励下生成合理的行为转变。我们的结果表明，社会图可以作为LLM后训练的高质量监督信号，缩小基于LLM的模拟中代理行为与网络动力学之间的差距。代码可在https://github.com/Ji-Cather/Graphia.git获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have shown promise in simulating human-likesocial behaviors. Social graphs provide high-quality supervision signals thatencode both local interactions and global network structure, yet they remainunderutilized for LLM training. To address this gap, we propose Graphia, thefirst general LLM-based social graph simulation framework that leverages graphdata as supervision for LLM post-training via reinforcement learning. WithGNN-based structural rewards, Graphia trains specialized agents to predict whomto interact with (destination selection) and how to interact (edge generation),followed by designed graph generation pipelines. We evaluate Graphia under twosettings: Transductive Dynamic Graph Generation (TDGG), a micro-level task withour proposed node-wise interaction alignment metrics; and Inductive DynamicGraph Generation (IDGG), a macro-level task with our proposed metrics foraligning emergent network properties. On three real-world networks, Graphiaimproves micro-level alignment by 6.1% in the composite destination selectionscore, 12% in edge classification accuracy, and 27.9% in edge content BERTScoreover the strongest baseline. For macro-level alignment, it achieves 41.11%higher structural similarity and 32.98% better replication of social phenomenasuch as power laws and echo chambers. Graphia also supports counterfactualsimulation, generating plausible behavioral shifts under platform incentives.Our results show that social graphs can serve as high-quality supervisionsignals for LLM post-training, closing the gap between agent behaviors andnetwork dynamics for LLM-based simulation. Code is available athttps://github.com/Ji-Cather/Graphia.git.</description>
      <author>example@mail.com (Jiarui Ji, Zehua Zhang, Zhewei Wei, Bin Tong, Guan Wang, Bo Zheng)</author>
      <guid isPermaLink="false">2510.24251v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MAGNET: A Multi-Graph Attentional Network for Code Clone Detection</title>
      <link>http://arxiv.org/abs/2510.24241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MAGNET是一种多图注意力框架，通过联合利用AST、CFG和DFG表示来捕获源代码的语法和语义特征，实现了代码克隆检测的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;代码克隆检测是软件工程中的基础任务，支持重构、调试、剽窃检测和漏洞分析。现有方法通常依赖单一表示（如AST、CFG、DFG），只能捕捉代码语义的部分方面，而混合方法的融合策略通常是手工设计的且效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出MAGNET框架，一种多图注意力框架，联合利用AST、CFG和DFG表示来捕获源代码的语法和语义特征。&lt;h4&gt;方法&lt;/h4&gt;MAGNET结合残差图神经网络与节点级自注意力学习局部和长距离依赖关系，引入门控交叉注意力机制用于细粒度的图间交互，采用Set2Set池化将多图嵌入融合为统一的程序级表示。&lt;h4&gt;主要发现&lt;/h4&gt;在BigCloneBench和Google Code Jam上的实验表明，MAGNET分别达到96.5%和99.2%的总体F1分数，实现了最先进的性能。消融研究证实了多图融合和每个注意力组件的关键贡献。&lt;h4&gt;结论&lt;/h4&gt;MAGNET通过多图表示和注意力机制实现了高效的代码克隆检测，代码已开源于https://github.com/ZixianReid/Multigraph_match。&lt;h4&gt;翻译&lt;/h4&gt;代码克隆检测是软件工程中的一个基础任务，它支持重构、调试、剽窃检测和漏洞分析。现有方法通常依赖于单一表示，如抽象语法树（AST）、控制流图（CFG）和数据流图（DFG），这些表示只能捕捉代码语义的部分方面。混合方法已经出现，但它们的融合策略通常是手工设计的且效果不佳。在本研究中，我们提出了MAGNET，一种多图注意力框架，它联合利用AST、CFG和DFG表示来捕获源代码的语法和语义特征。MAGNET将残差图神经网络与节点级自注意力相结合，学习局部和长距离依赖关系，引入门控交叉注意力机制用于细粒度的图间交互，并采用Set2Set池化将多图嵌入融合为统一的程序级表示。在BigCloneBench和Google Code Jam上的大量实验表明，MAGNET在两个数据集上分别实现了96.5%和99.2%的总体F1分数，达到了最先进的性能。消融研究证实了多图融合和每个注意力组件的关键贡献。我们的代码可在https://github.com/ZixianReid/Multigraph_match获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Code clone detection is a fundamental task in software engineering thatunderpins refactoring, debugging, plagiarism detection, and vulnerabilityanalysis. Existing methods often rely on singular representations such asabstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs(DFGs), which capture only partial aspects of code semantics. Hybrid approacheshave emerged, but their fusion strategies are typically handcrafted andineffective. In this study, we propose MAGNET, a multi-graph attentionalframework that jointly leverages AST, CFG, and DFG representations to capturesyntactic and semantic features of source code. MAGNET integrates residualgraph neural networks with node-level self-attention to learn both local andlong-range dependencies, introduces a gated cross-attention mechanism forfine-grained inter-graph interactions, and employs Set2Set pooling to fusemulti-graph embeddings into unified program-level representations. Extensiveexperiments on BigCloneBench and Google Code Jam demonstrate that MAGNETachieves state-of-the-art performance with an overall F1 score of 96.5\% and99.2\% on the two datasets, respectively. Ablation studies confirm the criticalcontributions of multi-graph fusion and each attentional component. Our code isavailable at https://github.com/ZixianReid/Multigraph_match</description>
      <author>example@mail.com (Zixian Zhang, Takfarinas Saber)</author>
      <guid isPermaLink="false">2510.24241v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>HyperGraphX: Graph Transductive Learning with Hyperdimensional Computing and Message Passing</title>
      <link>http://arxiv.org/abs/2510.23980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为hdgc的新型算法，该算法结合了图卷积与高维计算中的绑定和捆绑操作，用于归纳图学习。&lt;h4&gt;背景&lt;/h4&gt;在图学习领域，图神经网络和高维计算是两种重要的方法，各有优势和局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时利用图神经网络和高维计算优势的新算法，提高图学习的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;hdgc算法将图卷积操作与高维计算中的绑定和捆绑操作相结合，主要在二进制向量上进行学习操作。&lt;h4&gt;主要发现&lt;/h4&gt;hdgc在预测准确性上优于主流图神经网络实现和最先进的高维计算实现，适用于同质图和异质图；在相同GPU平台上，hdgc比gcnii图神经网络实现平均快9561倍，比hdgl高维计算实现平均快144.5倍。&lt;h4&gt;结论&lt;/h4&gt;hdgc算法在多种图类型上表现出色，由于主要操作在二进制向量上进行，预期在神经形态和新兴存储器处理设备上具有出色的能源性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为hdgc的新颖算法，该算法将图卷积与高维计算中的绑定和捆绑操作相结合，用于归纳图学习。在预测准确性方面，hdgc优于主要的和流行的图神经网络实现以及最先进的高维计算实现，适用于一系列同质图和异质图。与我们测试的最准确的学习方法相比，在相同的目标GPU平台上，hdgc比图神经网络实现gcnii平均快9561.0倍，比高维计算实现hdgl平均快144.5倍。由于大部分学习操作在二进制向量上进行，我们期望hdgc在神经形态和新兴存储器处理设备上具有出色的能源性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel algorithm, \hdgc, that marries graph convolution withbinding and bundling operations in hyperdimensional computing for transductivegraph learning. For prediction accuracy \hdgc outperforms major and populargraph neural network implementations as well as state-of-the-arthyperdimensional computing implementations for a collection of homophilicgraphs and heterophilic graphs. Compared with the most accurate learningmethodologies we have tested, on the same target GPU platform, \hdgc is onaverage 9561.0 and 144.5 times faster than \gcnii, a graph neural networkimplementation and HDGL, a hyperdimensional computing implementation,respectively. As the majority of the learning operates on binary vectors, weexpect outstanding energy performance of \hdgc on neuromorphic and emergingprocess-in-memory devices.</description>
      <author>example@mail.com (Guojing Cong, Tom Potok, Hamed Poursiami, Maryam Parsa)</author>
      <guid isPermaLink="false">2510.23980v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Exploring an image-based $b$-jet tagging method using convolution neural networks</title>
      <link>http://arxiv.org/abs/2510.23962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于图像的喷流味道识别方法，利用主顶点周围的图像和喷流锥内的带电粒子，通过卷积神经网络进行分析，在b-喷流识别中实现了80-90%的效率，有望提高高能核物理实验的准确性。&lt;h4&gt;背景&lt;/h4&gt;喷流味道识别（识别起源于c夸克、b夸克和其他夸克（轻夸克和胶子）的喷流）在高能重离子物理中至关重要，因为它能够研究重离子碰撞产生的热密核介质中的味道依赖响应。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的喷流味道识别方法，提高在高能核物理实验中的准确性。&lt;h4&gt;方法&lt;/h4&gt;基于主顶点周围的图像，利用喷流锥内的带电粒子（可通过硅跟踪系统测量），使用卷积神经网络进行分析。研究假设跟踪系统具有理想性能。&lt;h4&gt;主要发现&lt;/h4&gt;基于图像的味道识别方法在横向动量范围从20到100 GeV/c的喷流中，实现了80-90%的b-喷流识别效率。&lt;h4&gt;结论&lt;/h4&gt;这种基于图像的喷流味道识别方法有潜力显著提高高能核物理实验中喷流味道识别的准确性。&lt;h4&gt;翻译&lt;/h4&gt;喷流味道识别，即识别起源于c夸克、b夸克和其他夸克（轻夸克和胶子）的喷流，是高能重离子物理中的关键任务，因为它能够研究重离子碰撞产生的热密核介质中的味道依赖响应。最近，基于深度学习技术（如深度神经网络和图神经网络）的几种方法已被开发。这些基于深度学习的方法相比依赖轨迹影响参数和次级顶点的传统方法表现出显著改进的性能。在识别算法中，使用了喷流和组成带电粒子的各种属性作为输入参数。我们探索了一种基于主顶点周围图像的新方法，利用喷流锥内的带电粒子，这些粒子可以通过硅跟踪系统测量。对于这项初步实验研究，我们假设跟踪系统具有理想性能。为了分析这些图像，我们采用了卷积神经网络。基于图像的味道识别方法在横向动量范围从20到100 GeV/c的喷流中显示了80-90%的b-喷流识别效率。这种方法有潜力显著提高高能核物理实验中喷流味道识别的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/s40042-025-01506-3&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Jet flavor tagging, the identification of jets originating from $c$-quarks,$b$-quarks, and other quarks (light quarks and gluons), is a crucial task inhigh-energy heavy-ion physics, as it enables the investigation offlavor-dependent responses within the hot and dense nuclear medium produced inheavy-ion collisions. Recently, several methods based on deep learningtechniques, such as deep neural networks and graph neural networks, have beendeveloped. These deep-learning-based methods demonstrate significantly improvedperformance compared to traditional methods that rely on track impactparameters and secondary vertices. In the tagging algorithms, variousproperties of jets and constituent charged particles are used as inputparameters. We explore a new method based on images surrounding the primaryvertex, utilizing charged particles within the jet cone, which can be measuredusing a silicon tracking system. For this initial experimental study, we assumethe ideal performance of the tracking system. To analyze these images, weemployed convolutional neural networks. The image-based flavor tagging methodshows an 80-90% $b$-jet tagging efficiency for jets in the transverse momentumrange from 20 to 100 GeV/$c$. This approach has the potential to significantlyimprove the accuracy of jet flavor tagging in high-energy nuclear physicsexperiments.</description>
      <author>example@mail.com (Hangil Jang, Sanghoon Lim)</author>
      <guid isPermaLink="false">2510.23962v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network Assisted Genetic Algorithm for Structural Dynamic Response and Parameter Optimization</title>
      <link>http://arxiv.org/abs/2510.22839v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器，用于结构参数优化，以克服传统数值方法计算成本高的问题。&lt;h4&gt;背景&lt;/h4&gt;优化结构参数(如质量m、刚度k和阻尼系数c)对于设计高效、有韧性和稳定的结构至关重要。传统的数值方法，如有限元法(FEM)和计算流体动力学(CFD)模拟，虽然能提供高精度结果，但在迭代优化任务中计算成本高昂，因为每次评估都需要为每个参数组合求解控制方程。&lt;h4&gt;目的&lt;/h4&gt;开发一种计算效率更高的方法来优化结构参数，避免传统数值方法的计算负担，实现自动化和智能化的结构设计。&lt;h4&gt;方法&lt;/h4&gt;提出混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器；使用GNN学习结构参数与动态位移响应之间的非线性映射；使用Newmark Beta方法生成单自由度(SDOF)系统响应数据集；GA通过最小化预测位移和提高动态稳定性来搜索全局最优参数集。&lt;h4&gt;主要发现&lt;/h4&gt;GNN和GA框架实现了强收敛性和鲁棒泛化能力；与传统模拟相比，显著降低了计算成本；结合机器学习代理和进化优化的方法对于自动化和智能结构设计是有效的。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法克服了传统数值优化方法的计算效率问题，为结构参数优化提供了一种有效途径，展示了机器学习代理与进化优化结合在自动化和智能结构设计中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;结构参数(如质量m、刚度k和阻尼系数c)的优化对于设计高效、有韧性和稳定的结构至关重要。传统的数值方法，包括有限元法(FEM)和计算流体动力学(CFD)模拟，虽然能提供高精度结果，但在迭代优化任务中计算成本高昂，因为每次评估都需要为每个参数组合求解控制方程。本研究提出了一种混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器来克服这些挑战。GNN被训练以准确学习结构参数与动态位移响应之间的非线性映射，实现无需重复求解系统方程的快速预测。使用Newmark Beta方法生成单自由度(SDOF)系统响应数据集，涵盖多种质量、刚度和阻尼配置。然后，GA通过最小化预测位移和提高动态稳定性来搜索全局最优参数集。结果表明，与传统模拟相比，GNN和GA框架实现了强收敛性、鲁棒泛化能力和显著降低的计算成本。这种方法突显了将机器学习代理与进化优化相结合用于自动化和智能结构设计的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The optimization of structural parameters, such as mass(m), stiffness(k), anddamping coefficient(c), is critical for designing efficient, resilient, andstable structures. Conventional numerical approaches, including Finite ElementMethod (FEM) and Computational Fluid Dynamics (CFD) simulations, providehigh-fidelity results but are computationally expensive for iterativeoptimization tasks, as each evaluation requires solving the governing equationsfor every parameter combination. This study proposes a hybrid data-drivenframework that integrates a Graph Neural Network (GNN) surrogate model with aGenetic Algorithm (GA) optimizer to overcome these challenges. The GNN istrained to accurately learn the nonlinear mapping between structural parametersand dynamic displacement responses, enabling rapid predictions withoutrepeatedly solving the system equations. A dataset of single-degree-of-freedom(SDOF) system responses is generated using the Newmark Beta method acrossdiverse mass, stiffness, and damping configurations. The GA then searches forglobally optimal parameter sets by minimizing predicted displacements andenhancing dynamic stability. Results demonstrate that the GNN and GA frameworkachieves strong convergence, robust generalization, and significantly reducedcomputational cost compared to conventional simulations. This approachhighlights the effectiveness of combining machine learning surrogates withevolutionary optimization for automated and intelligent structural design.</description>
      <author>example@mail.com (Sagnik Mukherjee)</author>
      <guid isPermaLink="false">2510.22839v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2510.24688v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;基于基础设施的感知在智能交通系统中起着关键作用，提供全局态势感知并支持协作自动驾驶。然而，现有基于摄像头的检测模型在多视图基础设施设置、多样化摄像头配置、降级视觉输入和各种道路布局等场景下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出MIC-BEV，一种基于Transformer的鸟瞰图(BEV)感知框架，用于基于基础设施的多摄像头3D物体检测。&lt;h4&gt;方法&lt;/h4&gt;MIC-BEV灵活支持具有异构内参和外参的变量摄像头数量，在传感器降级情况下表现出强大的鲁棒性。提出的图增强融合模块通过利用摄像头和BEV单元之间的几何关系以及潜在视觉线索，将多视图图像特征集成到BEV空间。同时引入M2I数据集，用于基于基础设施的物体检测，具有多样化的摄像头配置、道路布局和环境条件。&lt;h4&gt;主要发现&lt;/h4&gt;在M2I和真实世界数据集RoScenes上的大量实验表明，MIC-BEV在3D物体检测方面实现了最先进的性能，在极端天气和传感器降级等具有挑战性的条件下保持鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;MIC-BEV的结果突显了其在现实世界部署的潜力，数据集和源代码可在GitHub链接获取。&lt;h4&gt;翻译&lt;/h4&gt;基于基础设施的感知在智能交通系统中起着关键作用，提供全局态势感知并支持协作自动驾驶。然而，现有基于摄像头的检测模型在多视图基础设施设置、多样化摄像头配置、降级视觉输入和各种道路布局等场景下表现不佳。我们提出MIC-BEV，一种基于Transformer的鸟瞰图(BEV)感知框架，用于基于基础设施的多摄像头3D物体检测。MIC-BEV灵活支持具有异构内参和外参的变量摄像头数量，在传感器降级情况下表现出强大的鲁棒性。MIC-BEV中提出的图增强融合模块通过利用摄像头和BEV单元之间的几何关系以及潜在视觉线索，将多视图图像特征集成到BEV空间。为支持训练和评估，我们引入M2I数据集，用于基于基础设施的物体检测，具有多样化的摄像头配置、道路布局和环境条件。在M2I和真实世界数据集RoScenes上的大量实验表明，MIC-BEV在3D物体检测方面实现了最先进的性能，在极端天气和传感器降级等具有挑战性的条件下保持鲁棒性。这些结果突显了MIC-BEV在现实世界部署的潜力。数据集和源代码可在以下链接获取：https://github.com/HandsomeYun/MIC-BEV。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于基础设施的多摄像头3D目标检测问题，特别是在多视图基础设施设置、多样化摄像头配置、退化视觉输入和复杂道路布局等挑战场景下的性能提升。这个问题在现实中很重要，因为基于基础设施的感知是智能交通系统的关键组成部分，能提供全局态势感知和协同自主能力；同时，相比昂贵的激光雷达方案，摄像头更实惠、可扩展且提供丰富的语义信息，但现有摄像头检测模型在这些复杂场景中表现不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了基础设施感知与车载感知的区别，指出基础设施摄像头空间分布广泛，具有异构参数，这给现有BEV方法带来挑战。他们设计MIC-BEV框架时借鉴了现有BEV感知方法（如BEVFormer）的Transformer架构，但针对基础设施场景进行了创新。具体来说，他们引入了图增强融合模块，利用摄像头和BEV单元间的几何关系进行特征融合；借鉴了可变形注意力机制进行特征提取；采用DETR风格解码器进行目标检测；并使用图注意力网络(GAT)来学习融合权重，使模型能够自适应地处理不同摄像头配置。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过图神经网络建模摄像头和BEV单元之间的几何关系，实现关系感知的多视图特征融合，使模型能够根据每个摄像头与BEV单元的几何相关性动态分配权重。整体流程包括：1)处理可变数量的摄像头输入；2)使用ResNet-101和FPN提取多尺度特征；3)初始化可学习的BEV查询；4)通过Transformer编码进行特征处理，包括时间自注意力和关系增强空间交叉注意力(ReSCA)；5)使用任务特定解码器进行3D目标检测和BEV语义分割。其中ReSCA模块是关键，它为每个BEV查询生成3D参考点，投影到摄像头视图，使用可变形注意力提取特征，并通过GAT学习融合权重。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)MIC-BEV框架，专门设计用于处理基础设施多摄像头系统的异构配置；2)关系增强的空间交叉注意力(ReSCA)，利用图神经网络建模几何关系；3)M2I数据集，提供多样化的摄像头配置、道路布局和环境条件；4)摄像头掩码策略，提高对传感器降级的鲁棒性。相比之前工作，MIC-BEV能适应不同数量、方向、高度和视场的摄像头；使用显式的关系建模而非隐式融合；考虑摄像头和BEV单元间的几何关系；通过多任务学习（3D检测和BEV分割）增强空间理解；并在训练中模拟传感器故障以提高鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MIC-BEV通过引入关系感知的图增强融合机制和M2I多样化数据集，显著提升了基础设施多摄像头系统在复杂场景下的3D目标检测性能和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infrastructure-based perception plays a crucial role in intelligenttransportation systems, offering global situational awareness and enablingcooperative autonomy. However, existing camera-based detection models oftenunderperform in such scenarios due to challenges such as multi-viewinfrastructure setup, diverse camera configurations, degraded visual inputs,and various road layouts. We introduce MIC-BEV, a Transformer-basedbird's-eye-view (BEV) perception framework for infrastructure-basedmulti-camera 3D object detection. MIC-BEV flexibly supports a variable numberof cameras with heterogeneous intrinsic and extrinsic parameters anddemonstrates strong robustness under sensor degradation. The proposedgraph-enhanced fusion module in MIC-BEV integrates multi-view image featuresinto the BEV space by exploiting geometric relationships between cameras andBEV cells alongside latent visual cues. To support training and evaluation, weintroduce M2I, a synthetic dataset for infrastructure-based object detection,featuring diverse camera configurations, road layouts, and environmentalconditions. Extensive experiments on both M2I and the real-world datasetRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3Dobject detection. It also remains robust under challenging conditions,including extreme weather and sensor degradation. These results highlight thepotential of MIC-BEV for real-world deployment. The dataset and source code areavailable at: https://github.com/HandsomeYun/MIC-BEV.</description>
      <author>example@mail.com (Yun Zhang, Zhaoliang Zheng, Johnson Liu, Zhiyu Huang, Zewei Zhou, Zonglin Meng, Tianhui Cai, Jiaqi Ma)</author>
      <guid isPermaLink="false">2510.24688v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Optimizing Retrieval for RAG via Reinforced Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.24652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;R3是一种通过试验和反馈强化对比学习为RAG优化的检索框架，能够在RAG环境中动态探索和优化相关性，实验表明其性能优于原始检索器和最先进检索器，且高效实用。&lt;h4&gt;背景&lt;/h4&gt;随着检索增强生成(RAG)的普及，信息检索(IR)的角色正从为人类用户检索信息转变为为AI系统检索上下文知识，这使得相关性难以预先定义或标注。&lt;h4&gt;目的&lt;/h4&gt;提出一种解决方案，使检索器能够在RAG环境中动态探索和优化相关性，无需依赖预先标注的数据。&lt;h4&gt;方法&lt;/h4&gt;R3框架通过检索结果与环境交互产生对比信号，自动引导检索器的自我改进，不同于依赖标注或合成数据进行监督微调的先前方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明R3比原始检索器提高RAG性能5.2%，超越最先进检索器4.9%，同时与LLM增强检索和基于后训练或指令调整LLM的RAG系统性能相当。&lt;h4&gt;结论&lt;/h4&gt;R3是一种高效实用的解决方案，解决了在RAG环境中定义和优化相关性的挑战，只需4个GPU一天内完成训练。&lt;h4&gt;翻译&lt;/h4&gt;随着检索增强生成(RAG)变得越来越普遍，信息检索(IR)的角色正从为人类用户检索信息转变为为人工智能(AI)系统检索上下文知识，这使得相关性变得难以预先定义或标注。为应对这一挑战，我们提出了R3，一种通过试验和反馈强化对比学习为RAG优化的检索框架。与依赖标注或合成数据进行监督微调的先前方法不同，R3使检索器能够在RAG环境中动态探索和优化相关性。在训练过程中，检索结果与环境交互以产生对比信号，自动引导检索器的自我改进。在各种不同任务上的大量实验表明，R3比原始检索器提高RAG性能5.2%，超越最先进检索器4.9%，同时与LLM增强检索和基于后训练或指令调整LLM构建的RAG系统相当。R3既高效又实用，只需4个GPU，并在一天内完成训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As retrieval-augmented generation (RAG) becomes increasingly widespread, therole of information retrieval (IR) is shifting from retrieving information forhuman users to retrieving contextual knowledge for artificial intelligence (AI)systems, where relevance becomes difficult to define or annotate beforehand. Toaddress this challenge, we propose R3, a Retrieval framework optimized for RAGthrough trialand-feedback Reinforced contrastive learning. Unlike priorapproaches that rely on annotated or synthetic data for supervised fine-tuning,R3 enables the retriever to dynamically explore and optimize relevance withinthe RAG environment. During training, the retrieved results interact with theenvironment to produce contrastive signals that automatically guide theretriever's self-improvement. Extensive experiments across diverse tasksdemonstrate that R3 improves RAG performance by 5.2% over the originalretriever and surpasses state-of-the-art retrievers by 4.9%, while achievingcomparable results to LLM-augmented retrieval and RAG systems built onpost-trained or instruction-tuned LLMs. It is both efficient and practical,requiring only 4 GPUs and completing training within a single day.</description>
      <author>example@mail.com (Jiawei Zhou, Lei Chen)</author>
      <guid isPermaLink="false">2510.24652v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DeshadowMamba: Deshadowing as 1D Sequential Similarity</title>
      <link>http://arxiv.org/abs/2510.24260v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于Mamba模型的图像阴影去除方法，通过引入CrossGate方向调制机制和ColorShift正则化技术，有效解决了现有方法中阴影结构扭曲和颜色不一致的问题，实现了高质量的阴影去除效果。&lt;h4&gt;背景&lt;/h4&gt;当前基于深度学习的图像阴影去除方法通常依赖于基于注意力的架构来捕获长距离依赖关系，但这些固定的注意模式往往会混合来自不相关区域的照明线索，导致结构扭曲和颜色不一致。&lt;h4&gt;目的&lt;/h4&gt;重新审视阴影去除问题，从序列建模的角度探索更有效的解决方案，解决现有方法中阴影结构扭曲和颜色不一致的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 从序列建模角度探索使用Mamba（选择性状态空间模型）来传播全局上下文；2. 提出CrossGate方向调制机制，将阴影感知相似性注入Mamba的输入门；3. 引入ColorShift正则化，通过合成结构化的信息负样本引导模型抑制颜色污染并实现稳健的颜色恢复。&lt;h4&gt;主要发现&lt;/h4&gt;1. Mamba模型能通过方向状态转换实现有效的全局感受野同时保持位置连续性；2. 直接将Mamba应用于图像数据缺乏阴影-非阴影语义意识且易受颜色干扰；3. 所提出的CrossGate和ColorShift正则化技术能有效解决这些问题；4. DeshadowMamba在公共基准测试上实现了最先进的视觉质量和强大的定量性能。&lt;h4&gt;结论&lt;/h4&gt;通过将序列建模适应于阴影去除所需的完整结构和色度一致性，所提出的方法DeshadowMamba在图像阴影去除任务中取得了显著效果，为解决阴影去除中的结构完整性和色度一致性问题提供了有效途径。&lt;h4&gt;翻译&lt;/h4&gt;最近的图像阴影去除深度模型通常依赖于基于注意力的架构来捕获长距离依赖关系。然而，它们的固定注意模式往往会混合来自不相关区域的照明线索，导致结构扭曲和颜色不一致。在这项工作中，我们从序列建模的角度重新审视阴影去除问题，并探索使用Mamba（一种选择性状态空间模型）来通过方向状态转换传播全局上下文。这些转换产生有效的全局感受野，同时保持位置连续性。尽管有潜力，但直接将Mamba应用于图像数据并非最佳选择，因为它缺乏阴影-非阴影语义意识，并且仍然容易受到附近区域颜色干扰的干扰。为了解决这些局限性，我们提出了CrossGate，一种方向调制机制，将阴影感知相似性注入Mamba的输入门，允许沿过渡轴选择性地集成相关上下文。为了进一步确保外观保真度，我们引入了ColorShift正则化，这是一种由全局颜色统计驱动的对比学习目标。通过合成结构化的信息负样本，它引导模型抑制颜色污染并实现稳健的颜色恢复。这些组件共同将序列建模适应于阴影去除所需的完整结构和色度一致性。在公共基准测试上的大量实验表明，DeshadowMamba实现了最先进的视觉质量和强大的定量性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent deep models for image shadow removal often rely on attention-basedarchitectures to capture long-range dependencies. However, their fixedattention patterns tend to mix illumination cues from irrelevant regions,leading to distorted structures and inconsistent colors. In this work, werevisit shadow removal from a sequence modeling perspective and explore the useof Mamba, a selective state space model that propagates global context throughdirectional state transitions. These transitions yield an efficient globalreceptive field while preserving positional continuity. Despite its potential,directly applying Mamba to image data is suboptimal, since it lacks awarenessof shadow-non-shadow semantics and remains susceptible to color interferencefrom nearby regions. To address these limitations, we propose CrossGate, adirectional modulation mechanism that injects shadow-aware similarity intoMamba's input gate, allowing selective integration of relevant context alongtransition axes. To further ensure appearance fidelity, we introduce ColorShiftregularization, a contrastive learning objective driven by global colorstatistics. By synthesizing structured informative negatives, it guides themodel to suppress color contamination and achieve robust color restoration.Together, these components adapt sequence modeling to the structural integrityand chromatic consistency required for shadow removal. Extensive experiments onpublic benchmarks demonstrate that DeshadowMamba achieves state-of-the-artvisual quality and strong quantitative performance.</description>
      <author>example@mail.com (Zhaotong Yang, Yi Chen, Yanying Li, Shengfeng He, Yangyang Xu, Junyu Dong, Jian Yang, Yong Du)</author>
      <guid isPermaLink="false">2510.24260v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MATCH: Task-Driven Code Evaluation through Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.23169v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MATCH，一种新颖的无参考代码评估指标，用于解决AI生成代码与开发者意图匹配度评估的挑战。&lt;h4&gt;背景&lt;/h4&gt;AI代码生成越来越普遍，GitHub Copilot估计生成了GitHub上46%的代码。准确评估生成代码与开发者意图的匹配程度仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决无参考代码评估的空白问题，除了ICE-Score等少数替代方案外，引入MATCH作为一种新颖的无参考代码指标。&lt;h4&gt;方法&lt;/h4&gt;MATCH使用对比学习为代码和自然语言任务描述生成有意义的嵌入，实现反映生成代码执行任务程度的相似性评分。&lt;h4&gt;主要发现&lt;/h4&gt;MATCH在多种编程语言上，与功能正确性和人类偏好相比，比现有指标实现了更强的相关性。&lt;h4&gt;结论&lt;/h4&gt;MATCH是一种有效的无参考代码评估指标，能够更好地评估生成代码与开发者意图的匹配程度。&lt;h4&gt;翻译&lt;/h4&gt;基于AI的代码生成日益普及，GitHub Copilot估计生成了GitHub上46%的代码。准确评估生成代码与开发者意图的匹配程度仍然是一个重大挑战。传统评估方法，如单元测试，通常难以扩展且成本高昂。语法相似性指标（如BLEU、ROUGE）无法捕捉代码功能，而像CodeBERTScore这样的指标需要参考代码，但参考代码并不总是可用的。为了解决无参考代码评估的空白问题，除了ICE-Score等少数替代方案外，本文引入了MATCH，一种新颖的无参考代码指标。MATCH使用对比学习为代码和自然语言任务描述生成有意义的嵌入，实现反映生成代码执行任务程度的相似性评分。我们表明，MATCH在多种编程语言上，与功能正确性和人类偏好相比，比现有指标实现了更强的相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-based code generation is increasingly prevalent, with GitHub Copilotestimated to generate 46% of the code on GitHub. Accurately evaluating how wellgenerated code aligns with developer intent remains a critical challenge.Traditional evaluation methods, such as unit tests, are often unscalable andcostly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture codefunctionality, and metrics like CodeBERTScore require reference code, which isnot always available. To address the gap in reference-free evaluation, with fewalternatives such as ICE-Score, this paper introduces MATCH, a novelreference-free metric. MATCH uses Contrastive Learning to generate meaningfulembeddings for code and natural language task descriptions, enabling similarityscoring that reflects how well generated code implements the task. We show thatMATCH achieves stronger correlations with functional correctness and humanpreference than existing metrics across multiple programming languages.</description>
      <author>example@mail.com (Marah Ghoummaid, Vladimir Tchuiev, Ofek Glick, Michal Moshkovitz, Dotan Di Castro)</author>
      <guid isPermaLink="false">2510.23169v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>UniField: Joint Multi-Domain Training for Universal Surface Pressure Modeling</title>
      <link>http://arxiv.org/abs/2510.24106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为UniField的方法，通过整合多个子领域的空气动力学数据进行联合训练，解决了数据稀缺问题，实现了更好的流场表示。&lt;h4&gt;背景&lt;/h4&gt;物体表面压力场的空气动力学模拟对许多工程问题至关重要。深度神经网络已成为传统计算流体力学(CFD)模拟的高效替代方案，但数据稀缺限制了其应用。&lt;h4&gt;目的&lt;/h4&gt;解决空气动力学数据稀缺问题，通过整合多个子领域的数据进行联合训练，学习更通用的流场表示。&lt;h4&gt;方法&lt;/h4&gt;提出UniField方法，整合五个不同数据集（涵盖汽车、火车、飞机和一般形状）。该方法采用领域无关的Transformer模块提取通用点云特征，并定制领域特定的流动条件适配器来适应不同子领域的流动信息。&lt;h4&gt;主要发现&lt;/h4&gt;尽管不同子领域的空气动力学数据通常遵循不同方程，但联合训练的模型通常比单独训练的模型表现更好，表明这些数据相互补充，帮助模型学习更好的流场表示。&lt;h4&gt;结论&lt;/h4&gt;UniField作为通用流场表示模型具有潜力，为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;物体表面压力场的空气动力学模拟对许多工程问题至关重要。近年来，深度神经网络已成为传统计算成本高昂的CFD模拟的高效替代方案，用于建模表面压力场。然而，数据稀缺仍然是一个基本挑战，限制了神经网络的应用。为了解决这一限制，我们提出整合多个子领域的空气动力学数据进行联合训练，以学习更通用的流场表示。我们整合了涵盖不同领域的五个不同数据集，包括汽车、火车、飞机和一般形状。面对不同领域间的显著数据差异，我们提出了UniField，它采用领域无关的Transformer模块提取通用点云特征，并定制领域特定的流动条件适配器来适应不同子领域的流动信息。尽管不同子领域的空气动力学数据通常遵循不同的方程，但我们比较了在所有数据上联合训练的模型与在单个数据集上单独训练的模型，发现联合训练的模型通常表现出更好的性能。这表明这些数据相互补充，帮助模型学习更好的流场表示。这些结果突显了UniField作为通用流场表示模型的潜力，为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aerodynamic simulation of the surface pressure field around objects iscrucial for many engineering problems. In recent years, deep neural networkshave emerged as an efficient alternative to traditional, computationallyexpensive CFD simulations for modeling surface pressure fields. However, datascarcity remains a fundamental challenge, limiting the application of neuralnetworks. To address this limitation, we propose to integrate aerodynamic datafrom multiple subfields and conduct joint training to learn more general fieldrepresentations. We consolidate five different datasets covering variousfields, including automobiles, trains, aircraft, and general shapes. Facingsignificant data differences across different domains, we propose UniField,which employs a domain-agnostic Transformer module to extract general pointcloud features and customizes domain-specific flow-conditioned adapters toadapt to the flow information in different subfields. Despite the fact thataerodynamic data from different subfields are typically governed by differentequations, we compare models trained jointly on all data with those trainedseparately on individual datasets and find that the jointly-trained modelcommonly demonstrates better performance. This indicates that these datacomplement each other to help the model learn better flow fieldrepresentations. These results highlight the potential of UniField as auniversal flow field representation model and lay the foundation for broaderapplications of neural networks in aerodynamic analysis.</description>
      <author>example@mail.com (Junhong Zou, Zhenxu Sun, Yueqing Wang, Wei Qiu, Zhaoxiang Zhang, Zhen Lei, Xiangyu Zhu)</author>
      <guid isPermaLink="false">2510.24106v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised Domain Adaptation in 3D LiDAR Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2510.23525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for publication at the 2025 IEEE/RSJ  International Conference on Intelligent Robots and Systems (IROS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种动态伪标签过滤(DPLF)方案和先导引导的数据增强流水线(PG-DAP)，用于增强点云无监督域适应语义分割中真实数据的利用，提高合成到真实点云语义分割的性能。&lt;h4&gt;背景&lt;/h4&gt;为智能自主系统标注真实世界的LiDAR点云成本很高，现有基于自训练的无监督域适应方法在利用合成点云数据时未能有效利用未标记数据。&lt;h4&gt;目的&lt;/h4&gt;提高点云语义分割性能，更有效地利用未标记数据，克服现有方法依赖预定义或固定置信度阈值导致的性能限制。&lt;h4&gt;方法&lt;/h4&gt;提出动态伪标签过滤(DPLF)方案增强真实数据利用，设计先导引导的数据增强流水线(PG-DAP)减轻域偏移，并使用数据混合一致性损失推动模型学习上下文无关表示。&lt;h4&gt;主要发现&lt;/h4&gt;在两个具有挑战性的合成到真实点云语义分割任务上，该方法取得了优越的性能，消融研究证实了DPLF和PG-DAP模块的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效解决现有点云UDA语义分割中未标记数据利用不足的问题，显著提高了合成到真实点云语义分割的性能。&lt;h4&gt;翻译&lt;/h4&gt;为智能自主系统使用而标注真实世界的LiDAR点云成本很高。为克服这一限制，基于自训练的无监督域适应(UDA)已被广泛用于利用合成点云数据提高点云语义分割。然而，我们认为现有方法没有有效利用未标记数据，因为它们要么依赖于预定义或固定的置信度阈值，导致性能不佳。在本文中，我们提出了一种动态伪标签过滤(DPLF)方案，以增强点云UDA语义分割中真实数据的利用。此外，我们设计了一个简单高效的先导引导的数据增强流水线(PG-DAP)，以减轻合成和真实世界点云之间的域偏移。最后，我们使用数据混合一致性损失来推动模型学习上下文无关的表示。我们通过最先进方法的广泛比较实施并彻底评估了我们的方法。在两个具有挑战性的合成到真实点云语义分割任务上的实验表明，我们的方法取得了优越的性能。消融研究证实了DPLF和PG-DAP模块的有效性。我们在本文中发布了我们方法的代码。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D激光雷达点云语义分割中的无监督域适应问题，具体是如何有效利用带有自动标签的合成数据来改善对真实世界点云的语义分割，而无需对真实世界数据进行昂贵的标注。这个问题很重要，因为对真实世界的激光雷达点云进行密集标注成本高昂且耗时，而智能自主系统（如自动驾驶汽车）需要在动态真实环境中工作。虽然合成数据可以自动生成标签，但合成环境和真实世界之间存在域分布差异，导致在合成数据上训练的模型在真实数据上性能下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，特别是发现固定置信度阈值导致真实数据利用效率低下和类别不平衡问题。然后设计了解决方案：1) 动态伪标签过滤(DPLF)方案，自适应调整置信度阈值；2) 先验引导的数据增强管道(PG-DAP)，缓解域差异；3) 数据混合一致性损失，学习上下文无关表示。作者借鉴了Mean Teacher模型架构、LaserMix数据混合框架、局部和全局仿射变换以及指数移动平均(EMA)更新机制，但进行了创新性改进以解决特定问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过动态调整伪标签过滤策略和利用先验知识进行高效数据增强，解决合成到真实数据的域适应问题。整体流程：1) 使用Mean Teacher架构，预训练教师网络；2) 教师网络生成目标域伪标签；3) 通过DPLF进行伪标签过滤(距离加权、分层过滤、动态阈值更新)；4) 使用PG-DAP进行数据增强(DAS、DAJ、HAJ)；5) 应用LaserMix进行数据混合；6) 计算分割损失和数据混合一致性损失；7) 更新学生网络参数，使用EMA更新教师网络参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 动态伪标签过滤(DPLF)：自适应调整全局和类别特定置信度阈值，使用距离权重和EMA动态更新，解决固定阈值导致的类别不平衡；2) 先验引导数据增强(PG-DAP)：包含DAS(密度感知采样)、DAJ(距离感知抖动)和HAJ(高度感知抖动)，基于先验知识无需额外学习；3) 数据混合一致性损失：推动模型学习上下文无关表示。相比之前工作，不同之处在于：不使用固定置信度阈值，避免类别不平衡；不依赖计算资源昂贵的GAN进行域转换；结合了动态伪标签过滤和高效数据增强，通过一致性损失进一步改善特征表示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DPGLA通过动态伪标签过滤和先验引导数据增强，有效解决了3D激光雷达点云语义分割中合成到真实数据的无监督域适应问题，显著提升了模型在真实场景中的分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Annotating real-world LiDAR point clouds for use in intelligent autonomoussystems is costly. To overcome this limitation, self-training-basedUnsupervised Domain Adaptation (UDA) has been widely used to improve pointcloud semantic segmentation by leveraging synthetic point cloud data. However,we argue that existing methods do not effectively utilize unlabeled data, asthey either rely on predefined or fixed confidence thresholds, resulting insuboptimal performance. In this paper, we propose a Dynamic Pseudo-LabelFiltering (DPLF) scheme to enhance real data utilization in point cloud UDAsemantic segmentation. Additionally, we design a simple and efficientPrior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shiftbetween synthetic and real-world point clouds. Finally, we utilize data mixingconsistency loss to push the model to learn context-free representations. Weimplement and thoroughly evaluate our approach through extensive comparisonswith state-of-the-art methods. Experiments on two challenging synthetic-to-realpoint cloud semantic segmentation tasks demonstrate that our approach achievessuperior performance. Ablation studies confirm the effectiveness of the DPLFand PG-DAP modules. We release the code of our method in this paper.</description>
      <author>example@mail.com (Wanmeng Li, Simone Mosco, Daniel Fusaro, Alberto Pretto)</author>
      <guid isPermaLink="false">2510.23525v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Quality-controlled registration of urban MLS point clouds reducing drift effects by adaptive fragmentation</title>
      <link>http://arxiv.org/abs/2510.23416v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures. This manuscript is currently under review at the  International Journal of Applied Earth Observation and Geoinformation  (Elsevier). A preprint version will also be available on SSRN (Elsevier  Preprints) with a DOI once processed. This is the original preprint version  submitted for peer review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型工作流程，用于高效准确地在大规模城市街道场景中将移动激光扫描(MLS)点云配准到目标模型点云。&lt;h4&gt;背景&lt;/h4&gt;城市环境中的点云配准面临复杂挑战，包括点云密度差异、噪声特性和遮挡场景，这些在城市中心尤为常见。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够应对城市环境复杂性的工作流程，实现大规模MLS点云与目标模型点云的高效准确配准。&lt;h4&gt;方法&lt;/h4&gt;提出两种方法创新：1) 半球检查(SSC)预处理技术，通过识别相互正交的平面表面分割MLS轨迹数据，减少MLS漂移影响；2) 平面体素广义最近点迭代算法(PV-GICP)，在体素分区中选择性使用平面表面进行精细配准。&lt;h4&gt;主要发现&lt;/h4&gt;慕尼黑市中心真实数据集实验表明，该工作流程实现了平均亚0.01米的配准精度，同时比传统点对平面ICP方法减少50%以上的计算时间。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够推动自动化三维城市建模和更新，在城市规划、基础设施管理和动态城市监测中有直接应用。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种新型工作流程，旨在高效准确地在大规模移动激光扫描(MLS)点云与城市街道场景中的目标模型点云之间进行配准。该工作流程专门针对城市环境中固有的复杂性，巧妙地解决了点云密度、噪声特性和遮挡场景差异带来的挑战，这些挑战在城市中心中普遍存在。研究引入了两种方法创新。首先，提出的半球检查(SSC)预处理技术通过识别相互正交的平面表面，最优地分割MLS轨迹数据。这一步骤减少了MLS漂移对整个点云配准精度的影响，同时确保每个片段内有足够的几何特征以避免局部最小值。其次，我们提出了平面体素广义最近点迭代算法(PV-GICP)，一种在体素分区中选择性使用平面表面的精细配准方法。这种预处理策略不仅提高了配准精度，而且比传统的点对平面ICP方法减少了50%以上的计算时间。在慕尼黑市中心的真实数据集实验中，我们的工作流程实现了亚0.01米的平均配准精度，同时显著缩短了处理时间。研究结果强调了所提出方法在推进自动化三维城市建模和更新方面的潜力，可直接应用于城市规划、基础设施管理和动态城市监测。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决城市环境中大规模移动激光扫描(MLS)点云与目标模型点云的高效准确配准问题，特别是处理城市环境中的复杂性和挑战，包括不同密度、噪声特性和遮挡场景的点云集成，以及MLS长时间扫描过程中产生的漂移效应。这个问题在现实中非常重要，因为城市环境变化迅速，需要频繁更新3D城市模型用于城市规划、基础设施管理和城市发展监测等应用，而传统更新方法劳动密集且容易出错。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有方法的局限性，包括传统点云配准技术在处理大规模城市数据时效率低下，单个变换矩阵可能不足以实现精确对齐，以及固定分割方法在某些缺乏特征的片段中可能失败。在此基础上，作者借鉴了现有的点云分割技术(如等时间间隔分割)、特征匹配方法(如RANSAC)和ICP算法，但进行了创新改进，提出了自适应分割策略确保每个片段包含足够几何特征，以及专门针对城市环境的配准流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自适应分割将大型MLS点云分成包含足够几何特征的小片段减少漂移影响，只在稳定的平面区域进行精细配提高效率和准确性，并利用变换参数分析MLS漂移效应。整体流程包括：1)数据预处理(重采样、去噪、语义分类)；2)数据分割(初始分割后通过半球检查验证)；3)粗配准(特征检测、匹配和异常值去除)；4)精细配准(识别平面区域并执行GICP)；5)漂移分析(评估变换参数)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)半球检查(SSC)自适应分割技术，确保每个片段包含足够的相互正交平面表面；2)基于平面体素的广义ICP(PV-GICP)，选择性使用平面区域提高配准精度并减少50%以上的计算时间；3)漂移分析策略，通过分割过程识别和减少漂移误差。相比之前的工作，传统分割方法使用固定时间间隔可能导致某些片段缺乏特征，传统ICP方法在整个点云上运行计算量大且对非刚性区域敏感，而本文方法通过自适应分割和选择性平面配准解决了这些问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一种基于自适应分割和选择性平面配准的城市MLS点云高质量配准方法，显著提高了配准精度并减少了计算时间，同时有效量化了MLS漂移效应，为自动化3D城市模型更新提供了实用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents a novel workflow designed to efficiently and accuratelyregister large-scale mobile laser scanning (MLS) point clouds to a target modelpoint cloud in urban street scenarios. This workflow specifically targets thecomplexities inherent in urban environments and adeptly addresses thechallenges of integrating point clouds that vary in density, noisecharacteristics, and occlusion scenarios, which are common in bustling citycenters. Two methodological advancements are introduced. First, the proposedSemi-sphere Check (SSC) preprocessing technique optimally fragments MLStrajectory data by identifying mutually orthogonal planar surfaces. This stepreduces the impact of MLS drift on the accuracy of the entire point cloudregistration, while ensuring sufficient geometric features within each fragmentto avoid local minima. Second, we propose Planar Voxel-based GeneralizedIterative Closest Point (PV-GICP), a fine registration method that selectivelyutilizes planar surfaces within voxel partitions. This pre-process strategy notonly improves registration accuracy but also reduces computation time by morethan 50% compared to conventional point-to-plane ICP methods. Experiments onreal-world datasets from Munich's inner city demonstrate that our workflowachieves sub-0.01 m average registration accuracy while significantlyshortening processing times. The results underscore the potential of theproposed methods to advance automated 3D urban modeling and updating, withdirect applications in urban planning, infrastructure management, and dynamiccity monitoring.</description>
      <author>example@mail.com (Marco Antonio Ortiz Rincon, Yihui Yang, Christoph Holst)</author>
      <guid isPermaLink="false">2510.23416v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Symmetria: A Synthetic Dataset for Learning in Point Clouds</title>
      <link>http://arxiv.org/abs/2510.23414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  40 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Symmetria是一种公式驱动的点云数据集，通过利用对称性概念克服了点云学习中数据稀缺的问题，能够按任意规模生成，提供精确的地面真值，促进数据高效实验，并支持广泛的泛化和扩展。&lt;h4&gt;背景&lt;/h4&gt;与图像或文本领域受益于大量大型数据集不同，点云学习技术经常由于缺乏大规模数据集而遇到限制，这成为研究中的一个主要挑战。&lt;h4&gt;目的&lt;/h4&gt;克服点云数据集稀缺的限制，提出一种可按任意规模生成的公式驱动数据集，为点云学习提供充足且高质量的数据支持。&lt;h4&gt;方法&lt;/h4&gt;利用对称性概念创建具有已知结构和高度可变性的形状，确保精确地面真值的绝对可用性，设计数据集以促进数据高效的实验，实现跨不同几何设置的广泛泛化，并为新任务和模态提供易于扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;该数据集对点云自监督预训练非常有效，训练的模型在分类和分割等下游任务中表现出强大的性能，同时也显示出良好的少样本学习能力；该数据集还可用于真实世界物体的分类，展示了方法的实用价值；作者还引入了一个具有挑战性的对称检测任务，并为基线比较提供了基准。&lt;h4&gt;结论&lt;/h4&gt;Symmetria数据集和相关代码的公开可用性，以及能够生成非常大的数据集集合的能力，为点云学习领域的进一步研究和创新提供了重要基础。&lt;h4&gt;翻译&lt;/h4&gt;与图像或文本领域受益于大量大型数据集不同，点云学习技术经常由于缺乏大规模数据集而遇到限制。为了克服这一限制，我们提出了Symmetria，一种可按任意规模生成的公式驱动数据集。通过构造，它确保精确地面真值的绝对可用性，通过需要更少的样本促进数据高效的实验，实现跨不同几何设置的广泛泛化，并为新任务和模态提供易于扩展性。利用对称性的概念，我们创建了具有已知结构和高度可变性的形状，使神经网络能够有效地学习点云特征。我们的结果表明，该数据集对于点云自监督预训练非常有效，产生的模型在分类和分割等下游任务中表现出强大的性能，同时也显示出良好的少样本学习能力。此外，我们的数据集可以支持将模型微调以分类真实世界物体，突显了我们方法的实用性和应用价值。我们还引入了一个具有挑战性的对称检测任务，并为基线比较提供了基准。我们方法的一个显著优势是数据集、配套代码的公开可用性，以及生成非常大集合的能力，促进了点云学习的进一步研究和创新。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云学习领域中的数据稀缺问题。与图像或文本领域有大量大规模数据集不同，点云学习技术经常因缺乏大规模数据集而受到限制。这个问题很重要，因为点云是3D视觉的重要表示形式，广泛应用于机器人、自动驾驶等领域；缺乏大规模数据集限制了点云深度学习模型的发展；现有的3D数据集存在版权、隐私和标注成本等问题；真实世界的3D数据获取成本高、耗时长，难以满足机器学习所需的规模。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到图像领域有公式驱动的数据集表现出了有趣性能，甚至在某些任务上超过了ImageNet，这启发了他们在3D点云领域采用类似方法。他们借鉴了SHREC23数据集的工作，但进行了扩展和改进。作者基于对称性概念设计数据集，因为几乎现实世界中的每个物体都表现出某种对称性。他们设计数据集时遵循了几个原则：大规模探索、数据效率、可用的真实标签、隐私保护和通用可扩展性。从平面参数曲线开始生成3D形状，这些曲线具有已知的几何特性，然后通过挤压或旋转操作将它们转换为3D表面。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用对称性作为生成合成点云数据集的基础，通过参数化平面曲线生成具有已知对称结构的形状，然后通过几何变换转换为3D表面，并添加各种扰动增加数据集的多样性和挑战性。整体实现流程包括：1)从具有对称性的参数化平面曲线开始；2)通过挤压（大多数曲线）或旋转（贝塞尔曲线）将曲线转换为3D表面；3)确保3D形状保持原始平面曲线的对称性；4)应用各种变换增加数据集多样性；5)以一定概率应用随机平移和/或旋转；6)将生成的点云组织成不同复杂度的子数据集；7)为每个点云提供其对称性的真实标签信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于对称性的合成数据集；2)公式驱动的生成方法；3)精确的真实标签；4)数据效率；5)多样化的几何变换；6)可扩展性；7)多任务支持。相比之前的工作（如ShapeNet、ModelNet、SHREC23等）的不同之处：1)数据来源不同，Symmetria是完全合成的，避免了版权和隐私问题；2)生成方式不同，使用了更丰富的参数化平面曲线库；3)数据规模可按需生成任意规模；4)提供了更全面的真实标签注释；5)不仅验证了在传统任务上的有效性，还专门验证了对称性检测这一特定任务上的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了Symmetria，一个基于对称性的大规模合成点云数据集，通过程序化生成方法解决了3D点云学习中的数据稀缺问题，同时提供了精确的真实标签和多样化的几何变换，有效支持了自监督预训练和对称性检测等任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unlike image or text domains that benefit from an abundance of large-scaledatasets, point cloud learning techniques frequently encounter limitations dueto the scarcity of extensive datasets. To overcome this limitation, we presentSymmetria, a formula-driven dataset that can be generated at any arbitraryscale. By construction, it ensures the absolute availability of precise groundtruth, promotes data-efficient experimentation by requiring fewer samples,enables broad generalization across diverse geometric settings, and offers easyextensibility to new tasks and modalities. Using the concept of symmetry, wecreate shapes with known structure and high variability, enabling neuralnetworks to learn point cloud features effectively. Our results demonstratethat this dataset is highly effective for point cloud self-supervisedpre-training, yielding models with strong performance in downstream tasks suchas classification and segmentation, which also show good few-shot learningcapabilities. Additionally, our dataset can support fine-tuning models toclassify real-world objects, highlighting our approach's practical utility andapplication. We also introduce a challenging task for symmetry detection andprovide a benchmark for baseline comparisons. A significant advantage of ourapproach is the public availability of the dataset, the accompanying code, andthe ability to generate very large collections, promoting further research andinnovation in point cloud learning.</description>
      <author>example@mail.com (Ivan Sipiran, Gustavo Santelices, Lucas Oyarzún, Andrea Ranieri, Chiara Romanengo, Silvia Biasotti, Bianca Falcidieno)</author>
      <guid isPermaLink="false">2510.23414v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Workspace Registration and Collision Detection for Industrial Robotics Applications</title>
      <link>http://arxiv.org/abs/2510.23227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文主要研究机器人运动规划中的环境建模和碰撞检测方法&lt;h4&gt;背景&lt;/h4&gt;机器人运动规划依赖于对环境的精确知识，需要定义受限区域并考虑碰撞物体&lt;h4&gt;目的&lt;/h4&gt;比较不同传感器，说明从检测到完成碰撞环境的过程，以及检测机器人与环境的碰撞&lt;h4&gt;方法&lt;/h4&gt;使用各种传感器获取环境点云，通过区域增长分割和VCCS算法识别碰撞物体，并对点簇进行近似处理&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体发现&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及结论&lt;h4&gt;翻译&lt;/h4&gt;机器人运动规划依赖于对环境的精确知识，以便能够定义受限区域并考虑碰撞物体。为了捕获工作空间，使用各种传感器获取环境的点云。碰撞物体通过区域增长分割和VCCS算法进行识别。随后对点簇进行近似处理。本文的目的是比较不同的传感器，说明从检测到完成碰撞环境的过程，并检测机器人与该环境之间的碰撞。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决工业机器人在复杂生产环境中进行精确环境感知和碰撞检测的问题。这个问题在现实中非常重要，因为它关系到机器人能否安全高效地工作，避免与周围环境发生碰撞，同时最大化利用可用工作空间，确保生产过程的顺利进行和人员设备的安全。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先考虑了机器人操作环境需要精确建模的需求，然后设计了一套从环境感知到碰撞检测的完整流程。他们借鉴了多项现有工作：使用了Point Cloud Library (PCL)中的方法和算法；应用了区域增长分割算法来识别物体；采用了Voxel Cloud Connectivity Segmentation (VCCS)算法进行更精细的分割；并使用了分离轴定理和基于Minkowski差的碰撞检测方法。作者的主要贡献在于将这些技术整合成一个完整的工业机器人应用系统，并进行了系统性的比较和优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过传感器获取环境点云数据，经过处理和分割后将环境近似为简单的几何形状，然后应用碰撞检测算法确保机器人安全运行。整体流程包括：1)环境描述：将机器人操作环境表示为点云数据；2)检测和特征提取：使用3D传感器获取环境数据，进行预处理和去噪，通过分割算法识别和聚类物体；3)边界框生成：将聚类近似为立方体边界框，并与物体协方差矩阵主轴对齐以减少空间损失；4)碰撞检测：使用分离轴定理或基于Minkowski差的方法检测机器人与环境的碰撞。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)系统比较了不同传感器(TOF、主动立体视觉)在工业机器人环境感知中的性能差异；2)提出了一套从环境检测到完成碰撞环境的完整流程；3)研究了不同的物体近似方法及其对可用空间的影响；4)比较了不同碰撞检测方法在可用空间和约束数量方面的表现。相比之前的工作，这篇论文不仅关注单一算法，而是关注整个系统流程；不仅评估算法性能，还考虑了实际可用工作空间；通过物体对齐方法和基于Minkowski差的碰撞检测，显著减少了约束数量，提高了路径规划效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一套完整的工业机器人环境感知与碰撞检测方法，通过系统比较不同传感器和碰撞检测算法，优化了机器人工作空间利用率和路径规划效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion planning for robotic manipulators relies on precise knowledge of theenvironment in order to be able to define restricted areas and to takecollision objects into account. To capture the workspace, point clouds of theenvironment are acquired using various sensors. The collision objects areidentified by region growing segmentation and VCCS algorithm. Subsequently thepoint clusters are approximated. The aim of the present paper is to comparedifferent sensors, to illustrate the process from detection to the finishedcollision environment and to detect collisions between the robot and thisenvironment.</description>
      <author>example@mail.com (Klaus Zauner, Josef El Dib, Hubert Gattringer, Andreas Mueller)</author>
      <guid isPermaLink="false">2510.23227v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>UGAE: Unified Geometry and Attribute Enhancement for G-PCC Compressed Point Clouds</title>
      <link>http://arxiv.org/abs/2510.23009v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的几何和属性增强(UGAE)框架，通过三个核心组件(PoGE、PAE和PoAE)有效解决了点云有损压缩导致的几何结构和属性信息失真问题，在多个基准数据集上显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;点云的有损压缩可以减少存储和传输成本，但不可避免地导致几何结构和属性信息中的不可逆失真。&lt;h4&gt;目的&lt;/h4&gt;解决有损压缩导致的几何结构和属性信息失真问题，提高压缩后的点云质量。&lt;h4&gt;方法&lt;/h4&gt;提出统一的几何和属性增强(UGAE)框架，包含三个核心组件：1)后几何增强(PoGE)使用基于Transformer的稀疏卷积U-Net重建几何结构；2)预属性增强(PAE)引入增强几何引导的重新着色策略，使用DA-KNN方法保留高频细节；3)后属性增强(PoAE)使用带W-MSE损失的属性残差预测网络增强高频区域质量。&lt;h4&gt;主要发现&lt;/h4&gt;UGAE在8iVFB、Owlii和MVUB三个基准数据集上显著优于现有方法；与G-PCC测试模型相比，几何部分平均BD-PSNR增益9.98 dB，BD-比特率节省90.98%；属性部分BD-PSNR提高3.67 dB，BD-比特率节省56.88%；显著改善了感知质量。&lt;h4&gt;结论&lt;/h4&gt;UGAE框架能有效解决点云有损压缩导致的失真问题，在多个指标上表现优异，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;有损压缩点云减少了存储和传输成本；然而，它不可避免地导致几何结构和属性信息中的不可逆失真。为解决这些问题，我们提出了统一的几何和属性增强(UGAE)框架，包含三个核心组件：后几何增强(PoGE)、预属性增强(PAE)和后属性增强(PoAE)。在PoGE中，使用基于Transformer的稀疏卷积U-Net通过预测体素占用概率高精度重建几何结构。基于改进的几何结构，PAE引入创新的增强几何引导重新着色策略，使用细节感知的K-近邻(DA-KNN)方法实现精确重新着色，并在属性压缩前有效保留高频细节。最后，在解码器端，PoAE使用带加权均方误差(W-MSE)损失的属性残差预测网络，增强高频区域质量，同时保持低频区域的保真度。UGAE在三个基准数据集上显著优于现有方法：8iVFB、Owlii和MVUB。与最新的G-PCC测试模型(TMC13v29)相比，UGAE在D1指标下几何部分平均BD-PSNR增益9.98 dB，BD-比特率节省90.98%，属性部分在Y分量上BD-PSNR提高3.67 dB，BD-比特率节省56.88%。此外，它显著改善了感知质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云压缩过程中几何结构和属性信息不可避免产生的不可逆失真问题。这个问题在现实中非常重要，因为点云数据广泛应用于自动驾驶、文化遗产保护和虚拟现实等领域，高精度点云数据量大，存储和传输成本高，而现有压缩方法在减少数据量的同时会引入失真，影响3D应用的效率和用户体验。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云压缩方法的局限性，发现它们主要分别优化几何或属性，忽略了两者间的耦合关系。现有联合增强方法仅在解码器端进行增强，无法充分利用几何增强对属性压缩的好处。作者借鉴了点云上采样方法（如PU-Net、PUFA-GAN）、稀疏卷积方法（如PU-Dense、GRNet）以及网络架构（Transformer和U-Net）和损失函数（BCE和MSE），设计了包含PoGE、PAE和PoAE三个核心组件的UGAE框架，在整个压缩过程中协同优化几何和属性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是联合优化几何和属性，通过编码器-解码器协同增强来保留高频细节，分阶段解决压缩失真问题。整体流程是：编码器端，PoGE接收有损几何并生成增强几何结构；PAE使用增强几何和原始属性通过DA-KNN重新着色生成中间属性。传输有损几何比特流和重新着色后的属性比特流。解码器端，PoGE重建相同的增强几何；PoAE使用W-MSE损失函数专注于重建属性残差，特别是在高频区域。最终输出联合增强的点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一的几何和属性增强框架(UGAE)；2) 后几何增强(PoGE)结合Transformer和U-Net架构，使用密集连接并解决GPU随机性问题；3) 前属性增强(PAE)引入DA-KNN算法保留高频细节；4) 后属性增强(PoAE)使用W-MSE损失函数专注于高频区域。相比之前工作，UGAE同时处理几何和属性失真，考虑两者耦合关系，在编码器和解码器端都进行增强，而G-PCC++等现有方法仅在解码器端增强，且基于有损几何进行重新着色。UGAE在三个基准数据集上性能显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了UGAE框架，通过在编码器和解码器端协同优化几何和属性增强，显著提高了点云压缩质量，特别是在保留高频细节方面表现优异。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lossy compression of point clouds reduces storage and transmission costs;however, it inevitably leads to irreversible distortion in geometry structureand attribute information. To address these issues, we propose a unifiedgeometry and attribute enhancement (UGAE) framework, which consists of threecore components: post-geometry enhancement (PoGE), pre-attribute enhancement(PAE), and post-attribute enhancement (PoAE). In PoGE, a Transformer-basedsparse convolutional U-Net is used to reconstruct the geometry structure withhigh precision by predicting voxel occupancy probabilities. Building on therefined geometry structure, PAE introduces an innovative enhancedgeometry-guided recoloring strategy, which uses a detail-aware K-NearestNeighbors (DA-KNN) method to achieve accurate recoloring and effectivelypreserve high-frequency details before attribute compression. Finally, at thedecoder side, PoAE uses an attribute residual prediction network with aweighted mean squared error (W-MSE) loss to enhance the quality ofhigh-frequency regions while maintaining the fidelity of low-frequency regions.UGAE significantly outperformed existing methods on three benchmark datasets:8iVFB, Owlii, and MVUB. Compared to the latest G-PCC test model (TMC13v29),UGAE achieved an average BD-PSNR gain of 9.98 dB and 90.98% BD-bitrate savingsfor geometry under the D1 metric, as well as a 3.67 dB BD-PSNR improvement with56.88% BD-bitrate savings for attributes on the Y component. Additionally, itimproved perceptual quality significantly.</description>
      <author>example@mail.com (Pan Zhao, Hui Yuan, Chongzhen Tian, Tian Guo, Raouf Hamzaoui, Zhigeng Pan)</author>
      <guid isPermaLink="false">2510.23009v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method</title>
      <link>http://arxiv.org/abs/2510.22973v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的占据中心驾驶场景生成方法，通过创建大规模语义占据数据集Nuplan-Occ和开发统一框架，解决了占据中心方法对标注数据依赖的问题，实现了高质量语义占据、多视图视频和LiDAR点云的联合生成。&lt;h4&gt;背景&lt;/h4&gt;场景生成是自动驾驶的关键领域，占据中心方法最近取得了最先进的结果，但这些方法严重依赖于标注的占据数据，而这类数据仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;克服占据中心方法对标注数据的依赖限制，创建大规模语义占据数据集，并开发统一框架实现多模态场景生成。&lt;h4&gt;方法&lt;/h4&gt;创建Nuplan-Occ数据集，开发统一框架联合生成语义占据、多视图视频和LiDAR点云，采用时空解耦架构支持4D动态占据的扩展和预测，提出基于高斯飞溅的稀疏点图渲染策略和传感器感知嵌入策略。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在生成保真度和可扩展性方面优于现有方法，在下游任务中验证了其实用价值。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的统一占据中心驾驶场景生成方法在自动驾驶场景生成方面表现出色，有助于感知和规划评估等下游应用。&lt;h4&gt;翻译&lt;/h4&gt;场景生成是自动驾驶的关键领域，使包括感知和规划评估在内的下游应用成为可能。占据中心方法最近通过提供跨帧和模态的一致条件取得了最先进的结果；然而，它们的性能严重依赖于标注的占据数据，而这种数据仍然稀缺。为了克服这一限制，我们整理了Nuplan-Occ，这是迄今为止最大的语义占据数据集，由广泛使用的Nuplan基准构建而成。其规模和多样性不仅促进了大规模生成建模，也促进了自动驾驶的下游应用。基于此数据集，我们开发了一个统一框架，联合合成高质量语义占据、多视图视频和LiDAR点云。我们的方法采用时空解耦架构，支持4D动态占据的高保真空间扩展和时间预测。为了弥合模态差距，我们进一步提出了两种新颖技术：一种基于高斯飞溅的稀疏点图渲染策略，增强多视图视频生成；一种传感器感知嵌入策略，明确建模LiDAR传感器特性，以实现真实的多LiDAR模拟。大量实验表明，与现有方法相比，我们的方法在生成保真度和可扩展性方面取得了优越的性能，并在下游任务中验证了其实际价值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶场景生成中的规模限制问题。现有占用中心方法虽先进，但受限于标注数据稀缺，无法实现大规模训练；同时，多模态生成（语义占用、视频、LiDAR）存在模态差距，导致生成质量受限。这一问题对自动驾驶领域至关重要，因为高质量场景生成能支持感知和规划评估，降低开发成本，提高系统鲁棒性和安全性，同时支持算法在多样化环境中训练，提升泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法受限于数据稀缺性，无法实现大规模训练；然后意识到需要构建更大规模数据集；观察到多模态生成存在模态差距，需要新技术弥合；提出时空解耦架构分解4D占用生成；为解决视频生成中的传感器校准问题，引入高斯飞溅稀疏点图渲染；为实现真实LiDAR模拟，提出传感器感知嵌入策略。该方法借鉴了UniScene的前期工作，利用了扩散模型、3D高斯表示、CogVideoX的3D因果VAE和体积渲染技术等现有成果。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是以语义占用为中心构建统一框架，通过时空解耦架构将4D动态占用生成分解为空间扩展和时间预测，利用大规模数据集支持训练，并通过专门技术弥合模态差距。整体流程：1)构建Nuplan-Occ数据集，使用前景-背景分离聚合策略；2)4D占用生成，使用VAE和DiT编码解码，时空解耦处理；3)视频生成，将占用转为3D高斯基元渲染成稀疏点图，用视频扩散Transformer生成；4)LiDAR生成，使用传感器感知嵌入和稀疏UNet，应用射线平滑正则化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)可扩展的统一4D动态场景生成框架，联合扩展模型架构和数据；2)4D占用的时空解耦建模，分离空间扩展和时间预测；3)多传感器真实性的模态桥接策略，包括稀疏点图渲染和传感器感知嵌入；4)构建Nuplan-Occ最大语义占用数据集。相比之前工作，本文数据规模更大（比Nuscenes-Occupancy大19倍），采用时空解耦架构而非混合处理，使用分层生成策略和稀疏渲染技术，在多个任务上取得了最先进性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了UniScenev2，一个基于大规模Nuplan-Occ数据集的统一占用中心框架，通过时空解耦架构和模态桥接技术，实现了高质量语义占用、多视角视频和LiDAR点云的联合生成，显著提升了自动驾驶场景生成的规模和质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driving scene generation is a critical domain for autonomous driving,enabling downstream applications, including perception and planning evaluation.Occupancy-centric methods have recently achieved state-of-the-art results byoffering consistent conditioning across frames and modalities; however, theirperformance heavily depends on annotated occupancy data, which still remainsscarce. To overcome this limitation, we curate Nuplan-Occ, the largest semanticoccupancy dataset to date, constructed from the widely used Nuplan benchmark.Its scale and diversity facilitate not only large-scale generative modeling butalso autonomous driving downstream applications. Based on this dataset, wedevelop a unified framework that jointly synthesizes high-quality semanticoccupancy, multi-view videos, and LiDAR point clouds. Our approach incorporatesa spatio-temporal disentangled architecture to support high-fidelity spatialexpansion and temporal forecasting of 4D dynamic occupancy. To bridge modalgaps, we further propose two novel techniques: a Gaussian splatting-basedsparse point map rendering strategy that enhances multi-view video generation,and a sensor-aware embedding strategy that explicitly models LiDAR sensorproperties for realistic multi-LiDAR simulation. Extensive experimentsdemonstrate that our method achieves superior generation fidelity andscalability compared to existing approaches, and validates its practical valuein downstream tasks. Repo:https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2</description>
      <author>example@mail.com (Bohan Li, Xin Jin, Hu Zhu, Hongsi Liu, Ruikai Li, Jiazhe Guo, Kaiwen Cai, Chao Ma, Yueming Jin, Hao Zhao, Xiaokang Yang, Wenjun Zeng)</author>
      <guid isPermaLink="false">2510.22973v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments</title>
      <link>http://arxiv.org/abs/2510.22754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the IEEE/RSJ International Conference on Intelligent  Robots and Systems (IROS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了TWC-SLAM，一种多智能体协作SLAM框架，通过整合文本语义和WiFi信号特征来增强位置识别和回环检测，以提高在具有重复结构的相似室内环境中的协作SLAM性能。&lt;h4&gt;背景&lt;/h4&gt;多智能体协作SLAM在具有重复结构的相似室内环境中（如走廊和房间）常面临挑战。当使用基于点云的技术时，这些挑战会导致共享位置识别出现显著不准确。&lt;h4&gt;目的&lt;/h4&gt;减轻多智能体协作SLAM在相似室内环境中的挑战，提高共享位置识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;TWC-SLAM框架包括基于FAST-LIO2的单智能体前端里程计模块、利用文本语义和WiFi特征的位置识别和回环检测模块以及全局映射模块。智能体配备了能够捕获文本信息和检测WiFi信号的传感器。通过关联这些数据源，TWC-SLAM建立共同位置，促进不同智能体地图之间的点云对齐，并采用回环检测和优化模块实现全局优化和一致性映射。&lt;h4&gt;主要发现&lt;/h4&gt;使用具有相似走廊、房间和文本标志的室内数据集评估的结果表明，TWC-SLAM显著提高了在具有重复建筑特征的复杂环境中协作SLAM系统的性能。&lt;h4&gt;结论&lt;/h4&gt;整合文本语义和WiFi信号特征可以有效提高多智能体协作SLAM在具有重复结构的相似室内环境中的性能，特别是在位置识别和回环检测方面。&lt;h4&gt;翻译&lt;/h4&gt;多智能体协作SLAM常在具有重复结构的相似室内环境（如走廊和房间）中遇到挑战。当采用基于点云的技术时，这些挑战可能导致共享位置识别出现显著不准确。为缓解这些问题，我们引入了TWC-SLAM，一种多智能体协作SLAM框架，它整合文本语义和WiFi信号特征以增强位置识别和回环检测。TWC-SLAM包括基于FAST-LIO2的单智能体前端里程计模块、利用文本语义和WiFi特征的位置识别和回环检测模块以及全局映射模块。智能体配备了能够捕获文本信息和检测WiFi信号的传感器。通过关联这些数据源，TWC-SLAM建立共同位置，促进不同智能体地图之间的点云对齐。此外，系统采用回环检测和优化模块来实现全局优化和一致性映射。我们使用具有相似走廊、房间和文本标志的室内数据集评估了我们的方法。结果表明，TWC-SLAM显著提高了在具有重复建筑特征的复杂环境中协作SLAM系统的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多智能体协同SLAM在具有重复结构的相似室内环境（如走廊和相似房间）中面临的位置识别不准确问题。这个问题在现实中很重要，因为许多室内环境（如办公楼、医院、学校）都有相似结构，传统基于点云的技术容易在这些环境中产生错误匹配，导致地图不一致和定位错误，影响多智能体系统在检查、救援、物流等领域的实际应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法在相似环境中的局限性，然后借鉴了现有工作如FAST-LIO2作为前端里程计模块，参考了TextSLAM等文本识别方法和SpotFi等WiFi定位技术。作者设计了多模态融合思路，结合文本语义（提供明确标识但可能重复）和WiFi特征（提供环境特定信号但区分度有限）两种互补信息，设计了四个主要组件：多智能体前端里程计、文本语义匹配、WiFi特征匹配和全局映射模块，通过算法1实现多模态位置识别，确保位置识别的准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过整合文本语义和WiFi特征这两种互补的模态信息，解决相似室内环境中多智能体协同SLAM的位置识别挑战。整体流程包括：1)多智能体使用FAST-LIO2计算里程和生成点云地图；2)通过OCR提取文本语义，使用Levenshtein距离计算文本相似度进行匹配；3)收集WiFi数据，计算MAC地址相似度和RSS值相似度进行验证；4)基于匹配结果识别相同位置，使用点云匹配方法计算坐标变换，执行回环检测和全局优化，生成一致的全局地图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首创性地整合文本语义与WiFi特征进行多智能体协同SLAM；2)提出新颖的回环检测和位置识别方法，协同利用两种模态信息；3)构建专门的多智能体数据集。相比之前工作，TWC-SLAM比传统点云方法精度高88%，比纯文本方法高82%，比纯WiFi方法高92%。它解决了单一模态方法在相似环境中的局限性，通过多模态融合提高了位置识别的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TWC-SLAM通过创新性地整合文本语义与WiFi特征，显著提高了多智能体协同SLAM系统在具有重复结构的相似室内环境中的定位精度和地图一致性，解决了传统方法在复杂环境中易出现错误匹配的关键挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent cooperative SLAM often encounters challenges in similar indoorenvironments characterized by repetitive structures, such as corridors androoms. These challenges can lead to significant inaccuracies in shared locationidentification when employing point cloud-based techniques. To mitigate theseissues, we introduce TWC-SLAM, a multi-agent cooperative SLAM framework thatintegrates text semantics and WiFi signal features to enhance locationidentification and loop closure detection. TWC-SLAM comprises a single-agentfront-end odometry module based on FAST-LIO2, a location identification andloop closure detection module that leverages text semantics and WiFi features,and a global mapping module. The agents are equipped with sensors capable ofcapturing textual information and detecting WiFi signals. By correlating thesedata sources, TWC-SLAM establishes a common location, facilitating point cloudalignment across different agents' maps. Furthermore, the system employs loopclosure detection and optimization modules to achieve global optimization andcohesive mapping. We evaluated our approach using an indoor dataset featuringsimilar corridors, rooms, and text signs. The results demonstrate that TWC-SLAMsignificantly improves the performance of cooperative SLAM systems in complexenvironments with repetitive architectural features.</description>
      <author>example@mail.com (Chunyu Li, Shoubin Chen, Dong Li, Weixing Xue, Qingquan Li)</author>
      <guid isPermaLink="false">2510.22754v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Estimating Continuum Robot Shape under External Loading using Spatiotemporal Neural Networks</title>
      <link>http://arxiv.org/abs/2510.22339v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2025 IEEE/RSJ International Conference on Intelligent Robots and  Systems (IROS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于学习的方法，用于准确估计受外部负载的柔性连续体机器人的3D形状。该方法通过时空神经网络架构融合多模态输入，生成点云表示机器人变形配置，并通过拟合贝塞尔曲线实现连续3D形状重建。实验验证显示该方法具有高精度，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;柔性连续体机器人在外部负载下的3D形状估计是一个挑战性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于学习的方法来准确估计柔性连续体机器人在外部负载下的3D形状。&lt;h4&gt;方法&lt;/h4&gt;提出时空神经网络架构，融合多模态输入（当前和历史肌腱位移数据以及RGB图像），生成点云表示机器人变形配置。网络集成了循环神经模块进行时间特征提取，编码模块进行空间特征提取，以及多模态融合模块结合视觉数据的空间特征和历史执行器输入的时间依赖性。通过将贝塞尔曲线拟合到预测点云上实现连续3D形状重建。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证显示该方法具有高精度，无负载时平均形状估计误差为0.08毫米，负载时为0.22毫米，优于最先进的TDCRs形状传感方法。&lt;h4&gt;结论&lt;/h4&gt;基于深度学习的时空数据融合在负载条件下能有效实现精确的形状估计。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于学习的方法，用于准确估计受外部负载的柔性连续体机器人的3D形状。提出的方法引入了一种时空神经网络架构，融合多模态输入，包括当前和历史肌腱位移数据以及RGB图像，生成代表机器人变形配置的点云。网络集成了循环神经模块进行时间特征提取，编码模块进行空间特征提取，以及多模态融合模块来结合从视觉数据中提取的空间特征和来自历史执行器输入的时间依赖性。通过将贝塞尔曲线拟合到预测的点云上实现连续3D形状重建。实验验证表明，我们的方法实现了高精度，无负载时平均形状估计误差为0.08毫米，负载时为0.22毫米，优于TDCRs形状传感的最先进方法。结果证明了基于深度学习的时空数据融合在负载条件下精确形状估计的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何准确估计在外部负载作用下的柔性连续机器人的3D形状问题。这个问题很重要，因为连续机器人在医疗手术、工业检测等领域有广泛应用，而外部负载会改变它们的变形行为，使得准确预测形状变得复杂。精确的形状估计对机器人控制至关重要，能帮助它们动态调整配置以优化任务执行和环境交互。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括传感器方法的精度限制和易损坏问题，模型方法难以处理材料非线性和未知外部负载，以及数据驱动方法在负载下鲁棒性有限。他们设计了一种时空神经网络架构，融合多模态输入数据。该方法借鉴了U-Net架构用于视觉特征提取，LSTM网络捕捉时间依赖性，以及空间注意力机制进行特征融合。同时创新性地将这些技术组合，形成了一个能够处理外部负载条件下的形状估计系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用时空神经网络融合视觉数据（RGB图像）和肌腱位移数据，建立这些多模态输入与连续机器人3D形状之间的非线性映射关系。整体流程包括：1)输入当前和历史肌腱位移数据及RGB图像；2)通过空间特征提取器从图像中提取空间特征；3)利用时间特征提取器处理肌腱位移序列；4)通过注意力机制融合时空特征；5)预测代表机器人形状的3D点云；6)使用贝塞尔曲线拟合点云生成连续3D形状。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多模态数据融合，同时利用视觉和肌腱位移数据；2)时空神经网络架构，结合卷积和循环神经网络；3)在外部负载下实现高精度形状估计；4)端到端学习框架。相比之前工作，该方法结合了视觉和本体感觉数据，提高了精度和鲁棒性；使用更先进的神经网络架构；在各种负载条件下表现出更好的一致性；无需显式机械建模，能处理材料非线性和未知外部负载。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于时空神经网络的创新方法，通过融合视觉和肌腱位移数据，实现了在外部负载条件下高精度估计连续机器人3D形状的目标，相比现有方法在精度和鲁棒性上均有显著提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a learning-based approach for accurately estimating the3D shape of flexible continuum robots subjected to external loads. The proposedmethod introduces a spatiotemporal neural network architecture that fusesmulti-modal inputs, including current and historical tendon displacement dataand RGB images, to generate point clouds representing the robot's deformedconfiguration. The network integrates a recurrent neural module for temporalfeature extraction, an encoding module for spatial feature extraction, and amulti-modal fusion module to combine spatial features extracted from visualdata with temporal dependencies from historical actuator inputs. Continuous 3Dshape reconstruction is achieved by fitting B\'ezier curves to the predictedpoint clouds. Experimental validation demonstrates that our approach achieveshigh precision, with mean shape estimation errors of 0.08 mm (unloaded) and0.22 mm (loaded), outperforming state-of-the-art methods in shape sensing forTDCRs. The results validate the efficacy of deep learning-based spatiotemporaldata fusion for precise shape estimation under loading conditions.</description>
      <author>example@mail.com (Enyi Wang, Zhen Deng, Chuanchuan Pan, Bingwei He, Jianwei Zhang)</author>
      <guid isPermaLink="false">2510.22339v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Breaking the Static Assumption: A Dynamic-Aware LIO Framework Via Spatio-Temporal Normal Analysis</title>
      <link>http://arxiv.org/abs/2510.22313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, Accepted to IEEE Robotics and Automation Letters  (RA-L)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种解决动态环境中激光雷达-惯性里程计(LIO)挑战的新方法，通过将动态感知直接集成到点云配准过程中，打破了静态特征识别和姿态估计之间的循环依赖关系。&lt;h4&gt;背景&lt;/h4&gt;传统LIO算法基于静态世界假设，在动态环境中表现不佳，特别是在动态物体主导场景和几何稀疏环境中。当前动态LIO方法面临根本性挑战：准确的定位需要可靠识别静态特征，而区分动态物体又需要精确的姿态估计。&lt;h4&gt;目的&lt;/h4&gt;解决动态环境中的LIO挑战，打破静态特征识别和姿态估计之间的循环依赖关系。&lt;h4&gt;方法&lt;/h4&gt;引入了一种新颖的动态感知迭代最近点算法，利用时空法线分析，并配以高效的空间一致性验证方法来增强静态地图构建。&lt;h4&gt;主要发现&lt;/h4&gt;在具有有限几何结构的挑战性动态环境中，与最先进的LIO系统相比，性能有显著提升。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效解决了动态环境中的LIO问题，代码和数据集已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了动态环境中激光雷达-惯性里程计(LIO)的挑战，传统方法由于其静态世界假设经常失败。当动态物体主导场景，特别是在几何稀疏环境中时，传统LIO算法表现不佳。当前动态LIO方法面临一个基本挑战：准确的定位需要可靠识别静态特征，而区分动态物体又需要精确的姿态估计。我们的解决方案通过将动态感知直接集成到点云配准过程中，打破了这种循环依赖。我们引入了一种新颖的动态感知迭代最近点算法，利用时空法线分析，并辅以高效的空间一致性验证方法来增强静态地图构建。实验评估表明，在具有有限几何结构的挑战性动态环境中，与最先进的LIO系统相比，性能有显著提升。代码和数据集可在https://github.com/thisparticle/btsa获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决激光雷达-惯性里程计（LIO）在动态环境中的定位和地图构建问题。传统LIO系统假设环境是静态的，当场景中存在大量移动物体（如行人、车辆）时会导致严重的定位误差。这个问题在现实中非常重要，因为真实世界环境通常是动态的，特别是在几何特征稀疏的环境中，动态物体可能主导场景，使传统系统完全失效。此外，现有方法存在循环依赖问题：准确的定位需要可靠的静态特征识别，而有效的动态物体检测又需要精确的位姿估计。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统方法将动态物体检测作为预处理步骤而非解决配准算法的根本问题；学习技术只能检测预定义物体类别且需要大量训练数据；几何方法依赖于特定假设且在复杂场景中表现不佳。作者借鉴了时空法线分析的概念，但创新性地将其直接集成到点云配准过程中，而不是作为后处理。同时，作者采用了双地图架构（时间滑动窗口地图用于时空法线计算，长期体素地图提供全局一致性），并扩展了点对点ICP算法的异常值剔除步骤。通过这种方式，作者打破了状态估计和动态物体检测之间的循环依赖。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过时空法线分析直接将动态感知集成到点云配准过程中，同时解决状态估计和动态点分类问题，打破两者之间的循环依赖。整体流程包括：1）输入数据预处理（IMU预积分和点云畸变校正）；2）点云下采样；3）动态感知点云配准（计算时空法线、分类稳定/不稳定点、迭代优化位姿）；4）静态地图构建（不稳定点上采样、DBSCAN聚类、空间一致性检查）。这一过程在每个迭代中同时评估点的动态性和优化位姿估计，而不是分步处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）动态感知ICP算法，通过时空法线分析直接集成动态感知到配准过程；2）双地图架构平衡时空计算需求；3）高效的空间一致性验证方法改进静态地图构建；4）在真正具有挑战性的动态环境中进行验证。相比之前工作的不同：传统方法将动态检测作为预处理步骤，而本文直接集成到配准中；现有方法要么依赖学习技术（需大量训练数据且泛化有限），要么依赖几何假设（在复杂场景中表现不佳）；大多数方法假设已知精确位姿，而本文同时优化位姿和动态点分类；评估不仅限于几何丰富的城市环境，还包括几何退化和动态物体主导的环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通过时空法线分析直接集成动态感知到激光雷达-惯性里程计配准过程中的新框架，打破了状态估计和动态物体检测之间的循环依赖，在具有挑战性的动态环境中实现了更准确和鲁棒的定位与地图构建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenge of Lidar-Inertial Odometry (LIO) indynamic environments, where conventional methods often fail due to theirstatic-world assumptions. Traditional LIO algorithms perform poorly whendynamic objects dominate the scenes, particularly in geometrically sparseenvironments. Current approaches to dynamic LIO face a fundamental challenge:accurate localization requires a reliable identification of static features,yet distinguishing dynamic objects necessitates precise pose estimation. Oursolution breaks this circular dependency by integrating dynamic awarenessdirectly into the point cloud registration process. We introduce a noveldynamic-aware iterative closest point algorithm that leverages spatio-temporalnormal analysis, complemented by an efficient spatial consistency verificationmethod to enhance static map construction. Experimental evaluations demonstratesignificant performance improvements over state-of-the-art LIO systems inchallenging dynamic environments with limited geometric structure. The code anddataset are available at https://github.com/thisparticle/btsa.</description>
      <author>example@mail.com (Chen Zhiqiang, Le Gentil Cedric, Lin Fuling, Lu Minghao, Qiao Qiyuan, Xu Bowen, Qi Yuhua, Lu Peng)</author>
      <guid isPermaLink="false">2510.22313v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Linearized Optimal Transport for Analysis of High-Dimensional Point-Cloud and Single-Cell Data</title>
      <link>http://arxiv.org/abs/2510.22033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于线性最优传输(LOT)的框架，用于处理单细胞技术生成的高维点云数据，解决了不规则点云难以直接量化和比较的问题，同时兼顾了预测准确性和生物学可解释性。&lt;h4&gt;背景&lt;/h4&gt;单细胞技术生成高维点云数据，能够详细表征复杂的患者状态和治疗反应，但每个患者由不规则点云表示，难以直接量化和比较个体间的生物学差异。非线性方法虽能达到预测准确性，但作为黑盒模型，生物学可解释性差。&lt;h4&gt;目的&lt;/h4&gt;将不规则点云嵌入到固定维度的欧几里得空间中，同时保留分布结构，提供一种有原则性的线性表示，保留最优传输几何结构，同时支持下游分析。&lt;h4&gt;方法&lt;/h4&gt;适应线性最优传输(LOT)框架到单细胞数据分析场景，将不规则点云嵌入到固定维度的欧几里得空间中，保留分布结构，形成任意两个患者之间的配准，使其能够直接比较细胞分布。&lt;h4&gt;主要发现&lt;/h4&gt;LOT实现了COVID-19患者状态的准确且可解释的分类，分类器权重映射回驱动预测的特定标记物和空间区域；同时实现了患者来源类器官的合成数据生成，利用LOT嵌入的线性特性；LOT重心产生表示组合条件或样本的平均细胞谱，支持药物相互作用测试。&lt;h4&gt;结论&lt;/h4&gt;LOT作为一个统一框架，连接了预测性能、可解释性和生成建模，通过将异质点云转换为可直接追踪到原始数据的结构化嵌入，为理解高维生物系统中的免疫变异和治疗效应开辟了新机会。&lt;h4&gt;翻译&lt;/h4&gt;单细胞技术生成细胞的高维点云，能够详细表征复杂的患者状态和治疗反应。然而每个患者由不规则点云而非简单向量表示，使得难以直接量化和比较个体间的生物学差异。非线性方法如核方法和神经网络能实现预测准确性，但作为黑箱模型，提供的生物学可解释性有限。为解决这些限制，我们将线性最优传输(LOT)框架适应到这一场景，将不规则点云嵌入到固定维度的欧几里得空间中，同时保留分布结构。这种嵌入提供了有原则性的线性表示，保留最优传输几何结构，同时支持下游分析。它还形成了任意两个患者之间的配准，使其能够直接比较细胞分布。在此空间中，LOT实现了：(i) COVID-19患者状态的准确且可解释的分类，其中分类器权重映射回驱动预测的特定标记物和空间区域；(ii) 患者来源类器官的合成数据生成，利用LOT嵌入的线性特性。LOT重心产生表示组合条件或样本的平均细胞谱，支持药物相互作用测试。这些结果共同确立了LOT作为连接预测性能、可解释性和生成建模的统一框架。通过将异质点云转换为可直接追踪到原始数据的结构化嵌入，LOT为理解高维生物系统中的免疫变异和治疗效应开辟了新机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell technologies generate high-dimensional point clouds of cells,enabling detailed characterization of complex patient states and treatmentresponses. Yet each patient is represented by an irregular point cloud ratherthan a simple vector, making it difficult to directly quantify and comparebiological differences between individuals. Nonlinear methods such as kernelsand neural networks achieve predictive accuracy but act as black boxes,offering little biological interpretability.  To address these limitations, we adapt the Linear Optimal Transport (LOT)framework to this setting, embedding irregular point clouds into afixed-dimensional Euclidean space while preserving distributional structure.This embedding provides a principled linear representation that preservesoptimal transport geometry while enabling downstream analysis. It also forms aregistration between any two patients, enabling direct comparison of theircellular distributions. Within this space, LOT enables: (i) \textbf{accurateand interpretable classification} of COVID-19 patient states, where classifierweights map back to specific markers and spatial regions driving predictions;and (ii) \textbf{synthetic data generation} for patient-derived organoids,exploiting the linearity of the LOT embedding. LOT barycenters yield averagedcellular profiles representing combined conditions or samples, supporting druginteraction testing.  Together, these results establish LOT as a unified framework that bridgespredictive performance, interpretability, and generative modeling. Bytransforming heterogeneous point clouds into structured embeddings directlytraceable to the original data, LOT opens new opportunities for understandingimmune variation and treatment effects in high-dimensional biological systems.</description>
      <author>example@mail.com (Tianxiang Wang, Yingtong Ke, Dhananjay Bhaskar, Smita Krishnaswamy, Alexander Cloninger)</author>
      <guid isPermaLink="false">2510.22033v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging</title>
      <link>http://arxiv.org/abs/2510.23641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为空间感知线性变换器(SAL-T)的新型架构，解决了在高能物理高数据吞吐量环境中部署Transformer模型时的资源消耗和延迟问题。&lt;h4&gt;背景&lt;/h4&gt;Transformers在捕获高能粒子碰撞中的全局和局部相关性方面非常有效，但在高数据吞吐量环境(如CERN LHC)中部署面临挑战，其二次复杂度需要大量资源并增加推理延迟。&lt;h4&gt;目的&lt;/h4&gt;开发一种资源高效且低延迟的Transformer变体，能够在保持高性能的同时适应高能物理等高数据吞吐量环境。&lt;h4&gt;方法&lt;/h4&gt;SAL-T基于linformer架构，结合了空间感知粒子分区(基于运动学特征)和卷积层(捕获局部相关性)，实现了物理意义显著的区域间注意力计算。&lt;h4&gt;主要发现&lt;/h4&gt;SAL-T在喷流分类任务中优于标准linformer，实现了与全注意力Transformer相当的结果，同时显著减少了资源使用和推理延迟；在ModelNet10点云分类数据集上验证了这一优势。&lt;h4&gt;结论&lt;/h4&gt;SAL-T是一种高效解决方案，能够在保持高性能的同时显著降低计算需求和延迟，特别适合高能物理等资源受限环境。&lt;h4&gt;翻译&lt;/h4&gt;Transformers在捕获高能粒子碰撞中的全局和局部相关性方面非常有效，但在高数据吞吐量环境中部署时面临挑战，例如CERN LHC。Transformer模型的二次复杂度需要大量资源并增加推理时的延迟。为了解决这些问题，我们引入了空间感知线性变换器(SAL-T)，这是对linformer架构的物理启发式增强，保持了线性注意力。我们的方法基于运动学特征对粒子进行空间感知分区，从而计算物理意义显著的区域之间的注意力。此外，我们使用卷积层来捕获局部相关性，这些见解来自喷流物理学。除了在喷流分类任务中优于标准linformer外，SAL-T还实现了与全注意力Transformer相当的分类结果，同时在推理过程中使用更少的资源和更低的延迟。在通用点云分类数据集(ModelNet10)上的实验进一步证实了这一趋势。我们的代码可在https://github.com/aaronw5/SAL-T4HEP获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决传统Transformer模型在粒子物理应用中的计算复杂度问题。传统Transformer具有二次方复杂度(O(n²))，导致在处理高能物理数据时资源需求大、推理延迟高。这个问题在现实中非常重要，因为CERN大型强子对撞机每秒产生4000万次碰撞事件，需要实时过滤系统(触发系统)来筛选数据，而传统Transformer无法满足这种低延迟、高吞吐量的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有高效Transformer变体(如Longformer、Linformer)在粒子物理中等长度输入(约100个token)上的局限性，然后结合粒子物理专业知识设计了SAL-T。方法借鉴了Linformer的线性注意力机制作为基础，并融入了粒子物理中的kT排序概念(用于粒子聚类算法)和卷积层设计来捕获局部相关性。作者通过三种主要修改来增强Linformer：基于kT指标的空间感知排序、分区注意力机制和卷积增强注意力，创造出一种既高效又能捕捉物理相关空间信息的新方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; SAL-T的核心思想是通过空间感知的分区和卷积增强来改进Linformer架构，使其能够更有效地处理粒子喷射分类任务，同时保持线性计算复杂度。具体实现流程包括：1)使用kT=pTΔR指标对粒子进行排序，确保物理相关的邻近粒子在序列中彼此接近；2)将排序后的键和值向量分区，使每个投影头只关注自己的粒子子集；3)在每个头的原始注意力分数上应用小型深度2D卷积，以融入局部邻居交互；4)通过线性分区粒子多头注意力(LPP-MHA)计算注意力并聚合值表示；5)对注意力输出进行最大聚合并传递到分类层。整个流程保持了线性计算复杂度(O(np))，同时捕获了局部喷射子结构。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)空间感知分区：基于kT指标对粒子排序并分区，使每个投影头关注物理相关的粒子子集；2)卷积增强注意力：在注意力分数上应用2D卷积，捕获局部邻居交互；3)物理启发的特征排序：使用kT而非传统pT排序，更好地保留空间结构；4)在保持线性复杂度的同时实现与全注意力Transformer相当的性能。相比之前工作，SAL-T克服了标准Linformer不编码空间信息的局限，实现了与全注意力Transformer相当的性能但计算复杂度显著降低，同时比特定于粒子物理的Transformer变体更适合资源受限的触发系统。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAL-T通过空间感知的分区和卷积增强，在保持线性计算复杂度的同时，实现了与全注意力Transformer相当的粒子喷射分类性能，使其成为资源受限的粒子物理触发系统的可行选择。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers are very effective in capturing both global and localcorrelations within high-energy particle collisions, but they presentdeployment challenges in high-data-throughput environments, such as the CERNLHC. The quadratic complexity of transformer models demands substantialresources and increases latency during inference. In order to address theseissues, we introduce the Spatially Aware Linear Transformer (SAL-T), aphysics-inspired enhancement of the linformer architecture that maintainslinear attention. Our method incorporates spatially aware partitioning ofparticles based on kinematic features, thereby computing attention betweenregions of physical significance. Additionally, we employ convolutional layersto capture local correlations, informed by insights from jet physics. Inaddition to outperforming the standard linformer in jet classification tasks,SAL-T also achieves classification results comparable to full-attentiontransformers, while using considerably fewer resources with lower latencyduring inference. Experiments on a generic point cloud classification dataset(ModelNet10) further confirm this trend. Our code is available athttps://github.com/aaronw5/SAL-T4HEP.</description>
      <author>example@mail.com (Aaron Wang, Zihan Zhao, Subash Katel, Vivekanand Gyanchand Sahu, Elham E Khoda, Abhijith Gandrakota, Jennifer Ngadiuba, Richard Cavanaugh, Javier Duarte)</author>
      <guid isPermaLink="false">2510.23641v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization</title>
      <link>http://arxiv.org/abs/2510.20974v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PCA点云(PPC)的标准化框架，用于解决点云强化学习中对相机姿态不匹配的敏感性问题，提高了算法在现实环境中的鲁棒性和可靠性。&lt;h4&gt;背景&lt;/h4&gt;强化学习从原始视觉输入中取得了显著成功，但对分布外变化(如光照、颜色和视点变化)仍然很脆弱。点云强化学习减轻了基于外观的脆弱性，但仍然受相机姿态不匹配的影响。&lt;h4&gt;目的&lt;/h4&gt;解决点云强化学习对相机姿态不匹配的敏感性，提高在现实环境中的可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出PCA点云(PPC)框架，将任意刚体变换下的点云映射到唯一的规范姿态，使观测与一致框架对齐，减少视点引起的不一致性。&lt;h4&gt;主要发现&lt;/h4&gt;PPC提高了对具有挑战性的机器人任务中未见过的相机姿态的鲁棒性，为领域随机化提供了有原则的替代方案。&lt;h4&gt;结论&lt;/h4&gt;PPC框架有效解决了点云强化学习中的视点不一致性问题，提高了算法在现实环境中的鲁棒性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;从原始视觉输入的强化学习(RL)近年来取得了显著的成功，但它对分布外变化(如光照、颜色和视点变化)仍然很脆弱。点云强化学习(PC-RL)通过减轻基于外观的脆弱性提供了一种有前途的替代方案，但它对相机姿态不匹配的敏感性继续削弱了在现实环境中的可靠性。为了解决这一挑战，我们提出了PCA点云(PPC)，这是一种专门为下游机器人控制设计的标准化框架。PPC将任意刚体变换下的点云映射到唯一的规范姿态，使观测与一致框架对齐，从而显著减少视点引起的不一致性。在我们的实验中，我们表明PPC提高了对具有挑战性的机器人任务中未见过的相机姿态的鲁棒性，为领域随机化提供了有原则的替代方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云强化学习（PC-RL）中的视角敏感性问题。点云通常在相机局部坐标系中表示，即使微小的视角变化也会导致场景表示显著改变，影响算法在实际环境中的可靠性。这个问题很重要，因为视角变化是现实世界中的常见现象，而现有方法如域随机化往往导致样本效率低下且理论保证较弱。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出PC-RL虽然减轻了外观相关的脆弱性，但仍存在视角敏感性问题。然后分析了域随机化方法的局限性，借鉴了旋转不变点云分析领域的进展，特别是基于PCA的标准化方法。作者识别出PCA方法存在符号歧义问题，设计了新颖的几何驱动消歧步骤。该方法借鉴了PointNet、PointNet++等点云处理方法，以及PointPatch RL作为基础强化学习框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过基于PCA的标准化方法，将点云映射到唯一的规范姿态，实现对刚体变换（平移和旋转）的不变性。具体流程包括：1)点云下采样（体素下采样+最远点采样）；2)中心化处理（减去质心）；3)PCA对齐（与特征向量对齐）；4)符号消歧（使用几何驱动分数函数解决PCA符号歧义）；5)生成规范表示（在消歧后的坐标系中重新表达点云）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出PCA点云（PPC）标准化框架；2)设计几何驱动的消歧方法解决PCA符号歧义；3)提供理论保证证明对刚体变换的不变性；4)模块化设计可集成到任何PC-RL算法中。相比之前工作，PPC比域随机化更高效且有理论保证；比现有旋转不变分析方法解决符号歧义问题；比传统PCA方法确保确定性输出；比其他点云方法更适合强化学习任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于PCA的点云标准化方法（PPC），通过解决PCA的符号歧义问题，实现了点云表示对相机视角变化的鲁棒性，显著提高了点云强化学习在未见视角下的零样本泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) from raw visual input has achieved impressivesuccesses in recent years, yet it remains fragile to out-of-distributionvariations such as changes in lighting, color, and viewpoint. Point CloudReinforcement Learning (PC-RL) offers a promising alternative by mitigatingappearance-based brittleness, but its sensitivity to camera pose mismatchescontinues to undermine reliability in realistic settings. To address thischallenge, we propose PCA Point Cloud (PPC), a canonicalization frameworkspecifically tailored for downstream robotic control. PPC maps point cloudsunder arbitrary rigid-body transformations to a unique canonical pose, aligningobservations to a consistent frame, thereby substantially decreasingviewpoint-induced inconsistencies. In our experiments, we show that PPCimproves robustness to unseen camera poses across challenging robotic tasks,providing a principled alternative to domain randomization.</description>
      <author>example@mail.com (Michael Bezick, Vittorio Giammarino, Ahmed H. Qureshi)</author>
      <guid isPermaLink="false">2510.20974v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence</title>
      <link>http://arxiv.org/abs/2510.24693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Homepage: https://internlm.github.io/StarBench/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了STAR-Bench，一个新的音频基准测试，用于评估模型对声音在时间和三维空间中的动态推理能力。与现有基准测试不同，STAR-Bench专注于文本描述难以捕捉的精细感知推理。&lt;h4&gt;背景&lt;/h4&gt;尽管多模态大语言模型和大型音频语言模型发展迅速，但现有的音频基准测试主要测试可以从文本标题中恢复的语义，掩盖了在精细感知推理方面的不足。&lt;h4&gt;目的&lt;/h4&gt;正式定义音频4D智能（即对时间和三维空间中声音动态的推理），并引入STAR-Bench基准测试来衡量这种能力，揭示当前模型在理解物理世界方面的不足。&lt;h4&gt;方法&lt;/h4&gt;STAR-Bench结合基础声学感知设置（六个属性）和整体时空推理设置（包括段重排序和空间任务）。数据收集使用程序合成和物理模拟的音频，以及包括人工注释的四阶段流程确保高质量样本。&lt;h4&gt;主要发现&lt;/h4&gt;对19个模型的评估显示与人类存在显著差距，闭源模型受限于精细感知，而开源模型在感知、知识和推理方面都落后。STAR-Bench导致的准确性下降远大于先前基准测试（时间维度-31.5%，空间维度-35.2%）。&lt;h4&gt;结论&lt;/h4&gt;STAR-Bench为开发具有更强大物理世界理解能力的未来模型提供了关键见解和明确的发展路径。&lt;h4&gt;翻译&lt;/h4&gt;尽管多模态大语言模型和大型音频语言模型取得了快速进展，但现有的音频基准测试主要测试可以从文本标题中恢复的语义，掩盖了在精细感知推理方面的不足。我们将音频4D智能正式定义为对时间和三维空间中声音动态的推理，并引入STAR-Bench来衡量它。STAR-Bench结合了基础声学感知设置（绝对和相对条件下的六个属性）和整体时空推理设置，包括连续和离散过程的段重排序以及跨越静态定位、多源关系和动态轨迹的空间任务。我们的数据收集流程使用两种方法确保高质量样本。对于基础任务，我们使用程序合成和物理模拟的音频。对于整体数据，我们遵循包括人工注释和基于人类表现的最终选择在内的四阶段流程。与先前仅通过标题回答略微降低准确性的基准测试不同，STAR-Bench导致更大的下降（时间维度-31.5%，空间维度-35.2%），证明了其对语言难以描述线索的关注。对19个模型的评估显示与人类存在显著差距，并揭示了能力层次结构：闭源模型受限于精细感知，而开源模型在感知、知识和推理方面都落后。我们的STAR-Bench为开发具有更强大物理世界理解能力的未来模型提供了关键见解和明确的发展路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite rapid progress in Multi-modal Large Language Models and LargeAudio-Language Models, existing audio benchmarks largely test semantics thatcan be recovered from text captions, masking deficits in fine-grainedperceptual reasoning. We formalize audio 4D intelligence that is defined asreasoning over sound dynamics in time and 3D space, and introduce STAR-Bench tomeasure it. STAR-Bench combines a Foundational Acoustic Perception setting (sixattributes under absolute and relative regimes) with a Holistic Spatio-TemporalReasoning setting that includes segment reordering for continuous and discreteprocesses and spatial tasks spanning static localization, multi-sourcerelations, and dynamic trajectories. Our data curation pipeline uses twomethods to ensure high-quality samples. For foundational tasks, we useprocedurally synthesized and physics-simulated audio. For holistic data, wefollow a four-stage process that includes human annotation and final selectionbased on human performance. Unlike prior benchmarks where caption-onlyanswering reduces accuracy slightly, STAR-Bench induces far larger drops(-31.5\% temporal, -35.2\% spatial), evidencing its focus on linguisticallyhard-to-describe cues. Evaluating 19 models reveals substantial gaps comparedwith humans and a capability hierarchy: closed-source models are bottleneckedby fine-grained perception, while open-source models lag across perception,knowledge, and reasoning. Our STAR-Bench provides critical insights and a clearpath forward for developing future models with a more robust understanding ofthe physical world.</description>
      <author>example@mail.com (Zihan Liu, Zhikang Niu, Qiuyang Xiao, Zhisheng Zheng, Ruoqi Yuan, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Jianze Liang, Xie Chen, Leilei Sun, Dahua Lin, Jiaqi Wang)</author>
      <guid isPermaLink="false">2510.24693v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning</title>
      <link>http://arxiv.org/abs/2510.23907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为DynaStride的管道方法，用于生成教学视频中场景级别的连贯字幕，无需手动场景分割。该方法通过自适应帧采样和多模态窗口捕获关键转换，采用多模态思维链过程生成动作-对象对，并使用动态步长窗口选择算法进行融合，最终生成整合视觉语义和时间推理的场景级字幕。&lt;h4&gt;背景&lt;/h4&gt;教学视频中的场景级字幕通过理解视觉线索和时间结构来增强学习。将视觉线索与文本指导相结合支持程序学习和多模态推理，为技能获取提供丰富上下文。然而，未能捕捉这种结构的字幕可能缺乏连贯性和质量，造成混淆并破坏视频的教育意图。&lt;h4&gt;目的&lt;/h4&gt;解决现有字幕生成方法无法有效捕捉教学视频中时间结构和视觉语义的问题，开发一种能够生成连贯、高质量场景级字幕的方法，而无需手动场景分割。&lt;h4&gt;方法&lt;/h4&gt;作者提出了DynaStride管道，使用YouCookII数据集的场景注释，执行自适应帧采样和多模态窗口化来捕获每个场景内的关键转换。然后采用多模态思维链过程产生多个动作-对象对，并使用动态步长窗口选择算法进行精炼和融合，该算法自适应地平衡时间上下文和冗余。最终的场景级字幕将视觉语义和时间推理整合在一个教学字幕中。&lt;h4&gt;主要发现&lt;/h4&gt;与包括VLLaMA3和GPT-4o在内的强大基线相比，DynaStride在基于N-gram的指标(BLEU, METEOR)和语义相似性度量(BERTScore, CLIPScore)上均表现出一致的性能提升。定性分析进一步表明，DynaStride生成的字幕在时间连贯性和信息性方面更优。&lt;h4&gt;结论&lt;/h4&gt;DynaStride为改进AI驱动的教学内容生成提供了有希望的方向，能够生成更连贯、信息更丰富的场景级字幕，有助于提高教学视频的学习效果。&lt;h4&gt;翻译&lt;/h4&gt;教学视频中的场景级字幕可以通过要求理解视觉线索和时间结构来增强学习。通过将视觉线索与文本指导相一致，这种理解支持程序学习和多模态推理，为技能获取提供更丰富的上下文。然而，未能捕捉这种结构的字幕可能缺乏连贯性和质量，这可能造成混淆并破坏视频的教育意图。为了解决这一差距，我们引入了DynaStride，一个无需手动场景分割即可生成连贯场景级字幕的管道。使用YouCookII数据集的场景注释，DynaStride执行自适应帧采样和多模态窗口化来捕获每个场景内的关键转换。然后，它采用多模态思维链过程产生多个动作-对象对，这些对使用动态步长窗口选择算法进行精炼和融合，该算法自适应地平衡时间上下文和冗余。最终的场景级字幕将视觉语义和时间推理整合在一个教学字幕中。与包括VLLaMA3和GPT-4o在内的强大基线的经验评估表明，在基于N-gram的指标(BLEU, METEOR)和语义相似性度量(BERTScore, CLIPScore)上均表现出一致的提升。定性分析进一步表明，DynaStride生成的字幕在时间连贯性和信息性方面更优，这表明改进AI驱动的教学内容生成是一个有希望的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene-level captioning in instructional videos can enhance learning byrequiring an understanding of both visual cues and temporal structure. Byaligning visual cues with textual guidance, this understanding supportsprocedural learning and multimodal reasoning, providing a richer context forskill acquisition. However, captions that fail to capture this structure maylack coherence and quality, which can create confusion and undermine thevideo's educational intent. To address this gap, we introduce DynaStride, apipeline to generate coherent, scene-level captions without requiring manualscene segmentation. Using the YouCookII dataset's scene annotations, DynaStrideperforms adaptive frame sampling and multimodal windowing to capture keytransitions within each scene. It then employs a multimodal chain-of-thoughtprocess to produce multiple action-object pairs, which are refined and fusedusing a dynamic stride window selection algorithm that adaptively balancestemporal context and redundancy. The final scene-level caption integratesvisual semantics and temporal reasoning in a single instructional caption.Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) andsemantic similarity measures (BERTScore, CLIPScore). Qualitative analysesfurther show that DynaStride produces captions that are more temporallycoherent and informative, suggesting a promising direction for improvingAI-powered instructional content generation.</description>
      <author>example@mail.com (Eddison Pham, Prisha Priyadarshini, Adrian Maliackel, Kanishk Bandi, Cristian Meo, Kevin Zhu)</author>
      <guid isPermaLink="false">2510.23907v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations</title>
      <link>http://arxiv.org/abs/2510.23397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VideoTG-R1是一种新型课程强化学习框架，通过反射边界标注解决视频时间定位中的训练样本质量和难度问题，实现了数据高效训练。&lt;h4&gt;背景&lt;/h4&gt;视频时间定位(VTG)是根据语言查询在视频中定位精确片段的基础挑战。多模态大型语言模型(MLLMs)通过强化学习(RL)在VTG方面显示出潜力，但忽视了训练样本质量和难度带来的挑战。&lt;h4&gt;目的&lt;/h4&gt;解决VTG训练中部分标注样本和难以定位样本带来的问题，提高训练效率。&lt;h4&gt;方法&lt;/h4&gt;提出VideoTG-R1框架，包含边界反射代理(识别并过滤部分标注样本)和难度估计代理(评估样本难度并设计课程RL策略)，实现数据高效训练。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用10%的训练样本和21%的计算预算，VideoTG-R1在组相对策略优化(GRPO)和监督微调(SFT)下都优于全数据对应方法。&lt;h4&gt;结论&lt;/h4&gt;VideoTG-R1通过解决训练样本质量和难度问题，实现了在VTG和基于视频的问答任务上的有效性能提升。&lt;h4&gt;翻译&lt;/h4&gt;视频时间定位(VTG)旨在根据语言查询在视频中定位精确片段，这是视频理解中的一个基础挑战。虽然最近的多模态大型语言模型(MLLMs)通过强化学习(RL)在解决VTG方面显示出潜力，但它们忽视了训练样本质量和难度带来的挑战。(1)部分标注样本：许多样本包含超出标注区间的相关片段，引入了模糊监督。(2)难以定位的样本：零样本性能差的样本在RL训练中产生持续低且不可区分的奖励，在多个输出中没有明显偏好，从而阻碍学习效率。为解决这些挑战，我们提出VideoTG-R1，一个具有反射边界标注的新型课程RL框架，实现数据高效训练。具体来说，我们提出边界反射代理，利用MLLMs预测标注区间外的查询相关时间戳，使我们能够识别并过滤部分标注样本，从而减少模糊性。此外，我们引入难度估计代理来评估每个样本的训练难度，并设计课程RL策略，根据训练步骤动态掩盖难以定位样本的视频，降低训练难度并提供更清晰的偏好。在VTG和基于视频的问答任务上的实验证明了我们方法的有效性。值得注意的是，仅使用10%的训练样本和21%的计算预算，VideoTG-R1在组相对策略优化(GRPO)和监督微调(SFT)下都优于全数据对应方法。代码可在https://github.com/ldong1111/VideoTG-R1获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video temporal grounding (VTG) aims to locate precise segments in videosbased on language queries, which is a fundamental challenge in videounderstanding. While recent Multimodal Large Language Models (MLLMs) have shownpromise in tackling VTG through reinforcement learning (RL), they overlook thechallenges arising from both the quality and difficulty of training samples.(1) Partially annotated samples. Many samples contain relevant segments beyondthe annotated interval, introducing ambiguous supervision. (2) Hard-to-groundsamples. Samples with poor zero-shot performance produce consistently low andindistinguishable rewards during RL training, exhibiting no clear preferenceamong multiple outputs and thus hindering learning efficiency. To address thesechallenges, we propose VideoTG-R1, a novel curriculum RL framework withreflected boundary annotations, enabling data-efficient training. Specifically,we propose a Boundary Reflection Agent that utilizes MLLMs to predictquery-relevant timestamps outside the annotated intervals, allowing us toidentify and filter out partially annotated samples, thereby reducingambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assessthe training difficulty of each sample and design a curriculum RL strategy thatdynamically masks the videos of hard-to-ground samples according to thetraining steps, easing the training difficulty and providing clearerpreference. Experiments on the VTG and grounded VideoQA tasks demonstrate theeffectiveness of our method. Remarkably, with only 10% of the training samplesand 21% of the computational budget, VideoTG-R1 outperforms full-datacounterparts under both group relative policy optimization (GRPO) andsupervised fine-tuning (SFT). The code is available athttps://github.com/ldong1111/VideoTG-R1.</description>
      <author>example@mail.com (Lu Dong, Haiyu Zhang, Han Lin, Ziang Yan, Xiangyu Zeng, Hongjie Zhang, Yifei Huang, Yi Wang, Zhen-Hua Ling, Limin Wang, Yali Wang)</author>
      <guid isPermaLink="false">2510.23397v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Evaluation of Vision-LLMs in Surveillance Video</title>
      <link>http://arxiv.org/abs/2510.23190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as poster in the NeurIPS 2025 Workshop on Space in Vision,  Language, and Embodied AI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视觉语言模型在异常动作识别中的空间推理能力，将其作为零样本、语言基础的任务，解决了从稀疏2D视频中解释动态3D场景的具身感知挑战。&lt;h4&gt;背景&lt;/h4&gt;社会中摄像机的广泛应用产生了大量视频数据，远远超出人工监控能力，这对公共安全构成重大挑战，因为及时检测异常或犯罪事件对有效预防和响应至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究视觉语言模型（VLMs）的空间推理能力，探索小型预训练视觉-语言模型作为空间基础的零样本异常检测器的可行性，并评估其在提示和隐私保护条件下的表现。&lt;h4&gt;方法&lt;/h4&gt;将视频转换为文本描述并通过文本蕴含对标签进行评分，在UCF-Crime和RWF-2000数据集上评估四个开放模型，研究少样本示例和隐私过滤器对模型性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;少样本示例可以提高某些模型的准确性但可能增加误报；隐私过滤器（尤其是全身GAN变换）会引入不一致性降低准确性；当前视觉-语言模型在简单、空间显著事件上表现良好，但在处理嘈杂空间线索和身份模糊时表现不佳。&lt;h4&gt;结论&lt;/h4&gt;提出了加强空间基础的具体路径，包括结构感知提示、跨片段轻量级空间记忆、描述过程中的场景图或3D姿态先验，以及保留动作相关几何形状的隐私方法，使零样本、语言基础的管道成为具身、真实世界视频理解的适应性构建块。&lt;h4&gt;翻译&lt;/h4&gt;我们社会中摄像机的广泛应用产生了大量视频数据，远远超出了人工监控的能力。这对公共安全和安全构成了关键挑战，因为及时检测异常或犯罪事件对于有效预防和应对至关重要。具身代理识别意外事件的能力根本上与其空间推理能力相关。本文通过将异常动作识别构架为零样本、语言基础的任务，研究了视觉语言模型（VLMs）的空间推理，解决了从稀疏2D视频中解释动态3D场景的具身感知挑战。具体来说，我们调查了小型预训练视觉-语言模型是否可以通过将视频转换为文本描述并通过文本蕴含对标签进行评分，作为空间基础的零样本异常检测器。我们在提示和隐私保护条件下，在UCF-Crime和RWF-2000数据集上评估了四个开放模型。少样本示例可以提高某些模型的准确性，但可能增加误报，而隐私过滤器——尤其是全身GAN变换——会引入不一致性，降低准确性。这些结果展示了当前视觉-语言模型在哪些方面成功（简单、空间显著事件）和哪些方面失败（嘈杂的空间线索、身份模糊）。展望未来，我们概述了加强空间基础的具体路径，无需任务特定训练：结构感知提示、跨片段轻量级空间记忆、描述过程中的场景图或3D姿态先验，以及保留动作相关几何形状的隐私方法。这使零样本、语言基础的管道成为具身、真实世界视频理解的适应性构建块。我们用于评估VLMs的实现已在以下公开可用：https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何评估视觉-语言模型在监控视频中进行零样本异常行为识别的问题。这个问题很重要，因为社会上有大量摄像头产生的视频数据远远超过人类监控能力，及时检测异常或犯罪事件对公共安全和有效预防至关重要。此外，现有的公共异常行为识别数据集有限，仅在这些数据集上训练的模型可能无法很好地泛化到新的异常类型。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考过程是认识到传统视频异常检测依赖监督学习，需要大量带注释的数据集，成本高且难以识别新异常。因此他们设计了一个零样本框架，将异常分类重新构建为语言基础推理任务而非像素到标签映射。该方法借鉴了现有VLM的语义推理和世界知识，但不同于之前的AnomalyCLIP、LAVAD等方法，它不需要任务特定微调，而是专注于纯零样本异常检测。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将异常分类转化为语言基础推理任务，利用大型预训练视觉-语言模型的语义推理能力。整体流程分为两步：1)文本描述生成：视觉-LLM处理视频输入，基于视觉和提示生成描述性文本；2)零样本分类：使用预训练的NLI分类器评估文本与候选异常类别的逻辑蕴含程度，选择最高分数的类别作为结果。整个过程无需对模型进行梯度更新，实现了真正的零样本学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：系统评估小型视觉-LLMs在零样本异常识别中的能力；设计多种提示策略实验；研究隐私保护变换对模型性能的影响；提出改进空间推理的具体方法。与之前工作不同，该方法不需要大量标注数据，专注于纯零样本检测，首次系统评估了隐私保护变换对模型的影响，并揭示了提示技术与隐私过滤器之间的关键权衡。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统评估视觉-LLMs在零样本异常检测中的能力，揭示了提示技术和隐私过滤器之间的关键权衡，为设计更安全、更有效的视频理解系统提供了实用建议。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The widespread use of cameras in our society has created an overwhelmingamount of video data, far exceeding the capacity for human monitoring. Thispresents a critical challenge for public safety and security, as the timelydetection of anomalous or criminal events is crucial for effective response andprevention. The ability for an embodied agent to recognize unexpected events isfundamentally tied to its capacity for spatial reasoning. This paperinvestigates the spatial reasoning of vision-language models (VLMs) by framinganomalous action recognition as a zero-shot, language-grounded task, addressingthe embodied perception challenge of interpreting dynamic 3D scenes from sparse2D video. Specifically, we investigate whether small, pre-trained vision--LLMscan act as spatially-grounded, zero-shot anomaly detectors by converting videointo text descriptions and scoring labels via textual entailment. We evaluatefour open models on UCF-Crime and RWF-2000 under prompting andprivacy-preserving conditions. Few-shot exemplars can improve accuracy for somemodels, but may increase false positives, and privacy filters -- especiallyfull-body GAN transforms -- introduce inconsistencies that degrade accuracy.These results chart where current vision--LLMs succeed (simple, spatiallysalient events) and where they falter (noisy spatial cues, identityobfuscation). Looking forward, we outline concrete paths to strengthen spatialgrounding without task-specific training: structure-aware prompts, lightweightspatial memory across clips, scene-graph or 3D-pose priors during description,and privacy methods that preserve action-relevant geometry. This positionszero-shot, language-grounded pipelines as adaptable building blocks forembodied, real-world video understanding. Our implementation for evaluatingVLMs is publicly available at:https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition</description>
      <author>example@mail.com (Pascal Benschop, Cristian Meo, Justin Dauwels, Jelte P. Mense)</author>
      <guid isPermaLink="false">2510.23190v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Large-Model AI for Near Field Beam Prediction: A CNN-GPT2 Framework for 6G XL-MIMO</title>
      <link>http://arxiv.org/abs/2510.22557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于CNN-GPT2的新型近场波束预测框架，用于解决毫米波通信中极大规模天线阵列在高移动性场景下的近场波束预测挑战。&lt;h4&gt;背景&lt;/h4&gt;毫米波通信中极大规模天线阵列在高移动性场景下的应用凸显了近场波束预测的重要性。与传统远场假设不同，近场波束预测需要在角度和距离域联合采样码本，导致导频开销大幅增加。此外，最优近场波束指数表现出突发的非线性动态特性，对时间建模构成挑战。&lt;h4&gt;目的&lt;/h4&gt;解决近场波束预测中的挑战，设计一个有效的近场波束预测框架，以应对导频开销增加和波束指数非线性动态特性问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于CNN-GPT2的新型近场波束预测框架。具体包括：设计上行链路导频传输策略，通过宽波束模拟预编码和频率变化数字预编码实现高效信道探测；接收的导频信号经过预处理后，通过基于CNN的特征提取器；然后通过GPT-2模型捕获多个帧之间的时间依赖性，以端到端方式直接预测近场波束指数。&lt;h4&gt;主要发现&lt;/h4&gt;CNN-GPT2框架能够有效处理近场波束预测的挑战，所提出的导频传输策略实现了高效信道探测，该方法能够捕获时间依赖性并直接预测近场波束指数。&lt;h4&gt;结论&lt;/h4&gt;基于CNN-GPT2的近场波束预测框架为解决毫米波通信中极大规模天线阵列在高移动性场景下的近场波束预测问题提供了有效方案。&lt;h4&gt;翻译&lt;/h4&gt;毫米波通信中极大规模天线阵列的出现，尤其是在高移动性场景下，凸显了近场波束预测的重要性。与传统远场假设不同，近场波束预测需要同时在角度和距离域采样的码本，这导致导频开销大幅增加。此外，与远场情况下最优波束演化的时间平滑性不同，最优近场波束指数由于其同时依赖于用户角度和距离而表现出突发和非线性动态特性，给时间建模带来了重大挑战。为应对这些挑战，我们提出了一种新颖的基于卷积神经网络-生成预训练Transformer 2（CNN-GPT2）的近场波束预测框架。具体而言，设计了一种上行链路导频传输策略，通过宽波束模拟预编码和频率变化数字预编码实现高效信道探测。接收的导频信号经过预处理后，通过基于CNN的特征提取器，然后由GPT-2模型捕获多个帧之间的时间依赖性，并以端到端方式直接预测近场波束指数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of extremely large-scale antenna arrays (ELAA) inmillimeter-wave (mmWave) communications, particularly in high-mobilityscenarios, highlights the importance of near-field beam prediction. Unlike theconventional far-field assumption, near-field beam prediction requirescodebooks that jointly sample the angular and distance domains, which leads toa dramatic increase in pilot overhead. Moreover, unlike the far-field casewhere the optimal beam evolution is temporally smooth, the optimal near-fieldbeam index exhibits abrupt and nonlinear dynamics due to its joint dependenceon user angle and distance, posing significant challenges for temporalmodeling. To address these challenges, we propose a novel Convolutional NeuralNetwork-Generative Pre-trained Transformer 2 (CNN-GPT2) based near-field beamprediction framework. Specifically, an uplink pilot transmission strategy isdesigned to enable efficient channel probing through widebeam analog precodingand frequency-varying digital precoding. The received pilot signals arepreprocessed and passed through a CNN-based feature extractor, followed by aGPT-2 model that captures temporal dependencies across multiple frames anddirectly predicts the near-field beam index in an end-to-end manner.</description>
      <author>example@mail.com (Wang Liu, Cunhua Pan, Hong Ren, Wei Zhang, Cheng-Xiang Wang, Jiangzhou Wang)</author>
      <guid isPermaLink="false">2510.22557v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>TERRA: A Transformer-Enabled Recursive R-learner for Longitudinal Heterogeneous Treatment Effect Estimation</title>
      <link>http://arxiv.org/abs/2510.22407v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为TERRA的新方法，用于解决纵向数据中异质性处理效应估计的挑战，特别是在时变干预情况下。&lt;h4&gt;背景&lt;/h4&gt;在纵向数据中准确估计异质性处理效应对医疗保健、公共政策、教育和数字营销等领域的个性化决策至关重要。然而，时变干预带来了延续效应、时变异质性和处理后偏差等独特挑战，这些挑战无法通过标准HTE方法解决。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理时变干预带来的独特挑战（延续效应、时变异质性和处理后偏差）的方法，以准确估计纵向数据中的异质性处理效应。&lt;h4&gt;方法&lt;/h4&gt;引入TERRA（Transformer-Enabled Recursive R-learner），包含两个主要组件：1）使用Transformer架构编码完整的处理特征历史，捕捉长期时间依赖性和延续效应；2）开发递归残差学习公式，将经典结构嵌套均值模型推广到参数规范之外，解决处理后偏差问题。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和数据应用中，TERRA在HTE估计的准确性和稳定性方面始终优于强大的基线方法。&lt;h4&gt;结论&lt;/h4&gt;将有原则的因果结构与高容量的序列模型相结合，对于纵向异质性处理效应估计具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;在纵向环境中准确估计异质性处理效应(HTE)对于医疗保健、公共政策、教育和数字营销等领域的个性化决策至关重要。然而，时变干预带来了许多独特挑战，如延续效应、时变异质性和处理后偏差，这些问题标准HTE方法无法解决。为应对这些挑战，我们引入了TERRA（Transformer-Enabled Recursive R-learner），它促进具有灵活时间建模和学习的纵向HTE估计。TERRA有两个组件。首先，我们使用Transformer架构编码完整的处理特征历史，使表示长期时间依赖性和延续效应成为可能，从而更全面地捕捉个体和特定时间的处理效应变化。其次，我们开发了一种递归残差学习公式，将经典结构嵌套均值模型(SNMMs)推广到参数规范之外，解决了处理后偏差问题，同时减少了对功能假设的依赖。在模拟和数据应用中，TERRA在HTE估计的准确性和稳定性方面始终优于强大的基线方法，突显了将原则性因果结构与高容量序列模型相结合对纵向HTE的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately estimating heterogeneous treatment effects (HTE) in longitudinalsettings is essential for personalized decision-making across healthcare,public policy, education, and digital marketing. However, time-varyinginterventions introduce many unique challenges, such as carryover effects,time-varying heterogeneity, and post-treatment bias, which are not addressed bystandard HTE methods. To address these challenges, we introduce TERRA(Transformer-Enabled Recursive R-learner), which facilitates longitudinal HTEestimation with flexible temporal modeling and learning. TERRA has twocomponents. First, we use a Transformer architecture to encode fulltreatment-feature histories, enabling the representation of long-range temporaldependencies and carryover effects, hence capturing individual- andtime-specific treatment effect variation more comprehensively. Second, wedevelop a recursive residual-learning formulation that generalizes theclassical structural nested mean models (SNMMs) beyond parametricspecifications, addressing post-treatment bias while reducing reliance onfunctional assumptions. In simulations and data applications, TERRAconsistently outperforms strong baselines in HTE estimation in both accuracyand stability, highlighting the value of combining principled causal structurewith high-capacity sequence models for longitudinal HTE.</description>
      <author>example@mail.com (Lei Shi, Sizhu Lu, Qiuran Lyu, Peng Ding, Nikos Vlassis)</author>
      <guid isPermaLink="false">2510.22407v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World and Spatio-Temporal Deep Learning</title>
      <link>http://arxiv.org/abs/2510.22056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种稳健的深度学习框架，通过结合以人为中心的预处理和时空建模来解决监控视频异常检测中的挑战，在UCF-Crime数据集上实现了92.41%的平均测试准确率。&lt;h4&gt;背景&lt;/h4&gt;监控视频中的异常检测面临异常事件多样性、类别不平衡和场景依赖的视觉混乱等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个稳健的深度学习框架，整合以人为中心的预处理与时空建模，用于多类异常分类。&lt;h4&gt;方法&lt;/h4&gt;使用YOLO-World识别人体实例，ByteTrack进行身份感知跟踪，通过高斯模糊抑制背景区域，使用InceptionV3进行空间特征提取，并用双向LSTM捕获时间动态进行序列级分类。&lt;h4&gt;主要发现&lt;/h4&gt;在UCF-Crime五类子集上评估，三次独立试验中平均测试准确率达92.41%，每类F1分数均超过0.85，展示了对类别不平衡的强鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;前景聚焦的预处理显著增强了现实监控场景中的异常辨别能力。&lt;h4&gt;翻译&lt;/h4&gt;监控视频中的异常检测由于异常事件的多样性、类别不平衡和场景依赖的视觉混乱而仍然是一项具有挑战性的任务。为了解决这些问题，我们提出了一个稳健的深度学习框架，该框架整合了以人为中心的预处理与时空建模，用于多类异常分类。我们的流程首先应用YOLO-World（一种开放词汇的视觉语言检测器）来识别原始视频片段中的人体实例，然后使用ByteTrack进行一致的身份感知跟踪。通过高斯模糊抑制检测边界框外的背景区域，有效减少场景特定的干扰，使模型专注于行为相关的前景内容。然后，经过精炼的帧由在ImageNet上预训练的InceptionV3网络处理进行空间特征提取，并使用双向LSTM（BiLSTM）捕获时间动态，进行序列级分类。在UCF-Crime数据集的五类子集（正常、入室盗窃、打架、纵火、爆炸）上评估，我们的方法在三次独立试验中平均测试准确率达到92.41%，每个类别的F1分数均超过0.85。全面的评估指标，包括混淆矩阵、ROC曲线和宏/加权平均值，展示了强大的泛化能力和对类别不平衡的鲁棒性。结果证实，前景聚焦的预处理显著增强了现实监控场景中的异常辨别能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection in surveillance videos remains a challenging task due tothe diversity of abnormal events, class imbalance, and scene-dependent visualclutter. To address these issues, we propose a robust deep learning frameworkthat integrates human-centric preprocessing with spatio-temporal modeling formulti-class anomaly classification. Our pipeline begins by applying YOLO-World- an open-vocabulary vision-language detector - to identify human instances inraw video clips, followed by ByteTrack for consistent identity-aware tracking.Background regions outside detected bounding boxes are suppressed via Gaussianblurring, effectively reducing scene-specific distractions and focusing themodel on behaviorally relevant foreground content. The refined frames are thenprocessed by an ImageNet-pretrained InceptionV3 network for spatial featureextraction, and temporal dynamics are captured using a bidirectional LSTM(BiLSTM) for sequence-level classification. Evaluated on a five-class subset ofthe UCF-Crime dataset (Normal, Burglary, Fighting, Arson, Explosion), ourmethod achieves a mean test accuracy of 92.41% across three independent trials,with per-class F1-scores consistently exceeding 0.85. Comprehensive evaluationmetrics - including confusion matrices, ROC curves, and macro/weighted averages- demonstrate strong generalization and resilience to class imbalance. Theresults confirm that foreground-focused preprocessing significantly enhancesanomaly discrimination in real-world surveillance scenarios.</description>
      <author>example@mail.com (Mohammad Ali Etemadi Naeen, Hoda Mohammadzade, Saeed Bagheri Shouraki)</author>
      <guid isPermaLink="false">2510.22056v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues</title>
      <link>http://arxiv.org/abs/2510.18016v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为ViBED-Net的新型深度学习框架，用于从视频数据中检测学生在在线学习环境中的参与度。该模型通过结合面部表情和全场景上下文信息，利用双流架构和EfficientNetV2进行特征提取，并使用LSTM或Transformer进行时间建模。&lt;h4&gt;背景&lt;/h4&gt;在线学习环境中的参与度检测对于提高学生成果和个性化教学至关重要。存在一个名为DAiSEE的大规模基准数据集，用于电子学习中情感状态识别。&lt;h4&gt;目的&lt;/h4&gt;提出ViBED-Net（基于视频的参与度检测网络），一种新的深度学习框架，设计用于从视频数据评估学生参与度。&lt;h4&gt;方法&lt;/h4&gt;采用双流架构，使用EfficientNetV2进行空间特征提取，处理面部裁剪和完整视频帧来捕捉面部表情和全场景上下文。使用两种时间建模策略分析特征：长短期记忆（LSTM）网络和Transformer编码器。在DAiSEE数据集上评估模型，并应用有针对性的数据增强技术提高在代表性不足的参与度类别上的性能。&lt;h4&gt;主要发现&lt;/h4&gt;ViBED-Net与LSTM的组合达到73.43%的准确率，优于现有的最先进方法。结合面部感知和场景感知的时空线索显著提高了参与度检测的准确性。&lt;h4&gt;结论&lt;/h4&gt;ViBED-Net的模块化设计使其具有灵活性，可应用于教育、用户体验研究和内容个性化。该工作通过为现实世界的参与度分析提供可扩展的高性能解决方案，推动了基于视频的情感计算发展。项目源代码可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;在线学习环境中的参与度检测对于提高学生成果和个性化教学至关重要。我们提出了ViBED-Net（基于视频的参与度检测网络），一种新的深度学习框架，旨在通过双流架构从视频数据评估学生参与度。ViBED-Net通过EfficientNetV2处理面部裁剪和完整视频帧来捕捉面部表情和全场景上下文，用于空间特征提取。然后使用两种时间建模策略分析这些特征：长短期记忆（LSTM）网络和Transformer编码器。我们的模型在DAiSEE数据集上进行了评估，这是一个电子学习中情感状态识别的大规模基准。为了提高在代表性不足的参与度类别上的性能，我们应用了有针对性的数据增强技术。在测试的变体中，ViBED-Net与LSTM结合实现了73.43%的准确率，优于现有的最先进方法。ViBED-Net证明，结合面部感知和场景感知的时空线索显著提高了参与度检测的准确性。其模块化设计使其具有灵活性，可应用于教育、用户体验研究和内容个性化。这项工作通过为现实世界的参与度分析提供可扩展的高性能解决方案，推动了基于视频的情感计算发展。该项目的源代码可在https://github.com/prateek-gothwal/ViBED-Net获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Engagement detection in online learning environments is vital for improvingstudent outcomes and personalizing instruction. We present ViBED-Net(Video-Based Engagement Detection Network), a novel deep learning frameworkdesigned to assess student engagement from video data using a dual-streamarchitecture. ViBED-Net captures both facial expressions and full-scene contextby processing facial crops and entire video frames through EfficientNetV2 forspatial feature extraction. These features are then analyzed over time usingtwo temporal modeling strategies: Long Short-Term Memory (LSTM) networks andTransformer encoders. Our model is evaluated on the DAiSEE dataset, alarge-scale benchmark for affective state recognition in e-learning. To enhanceperformance on underrepresented engagement classes, we apply targeted dataaugmentation techniques. Among the tested variants, ViBED-Net with LSTMachieves 73.43\% accuracy, outperforming existing state-of-the-art approaches.ViBED-Net demonstrates that combining face-aware and scene-aware spatiotemporalcues significantly improves engagement detection accuracy. Its modular designallows flexibility for application across education, user experience research,and content personalization. This work advances video-based affective computingby offering a scalable, high-performing solution for real-world engagementanalysis. The source code for this project is available onhttps://github.com/prateek-gothwal/ViBED-Net .</description>
      <author>example@mail.com (Prateek Gothwal, Deeptimaan Banerjee, Ashis Kumer Biswas)</author>
      <guid isPermaLink="false">2510.18016v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment</title>
      <link>http://arxiv.org/abs/2510.23224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PathSearch是一个结合细粒度注意力马赛克表示与全局幻灯片嵌入的检索框架，通过视觉语言对比学习对齐，在数字病理学中实现了准确和灵活的幻灯片检索，提高了诊断准确性和观察者一致性。&lt;h4&gt;背景&lt;/h4&gt;病理切片的快速数字化为临床和研究工作流程中的计算工具开辟了新可能。基于内容的幻灯片检索使病理学家能够识别形态学和语义上相似的病例，支持精确诊断、增强观察者间一致性并协助基于案例的教育。然而，全幻灯片图像的有效检索仍具挑战性，因其具有千兆像素规模且在大量无关内容中捕捉细微语义差异困难。&lt;h4&gt;目的&lt;/h4&gt;开发一个检索框架来克服全幻灯片图像检索的挑战，实现准确、高效的幻灯片检索功能。&lt;h4&gt;方法&lt;/h4&gt;提出PathSearch检索框架，统一细粒度注意力马赛克表示与全局幻灯片嵌入，通过视觉语言对比学习对齐。在6,926个幻灯片-报告对语料库上训练，支持两种关键功能：(1)基于马赛克的图像到图像检索；(2)多模态检索，文本查询可直接检索相关幻灯片。&lt;h4&gt;主要发现&lt;/h4&gt;在四个公共病理数据集和三个内部队列上进行了严格评估，涵盖解剖部位检索、肿瘤亚型分类、肿瘤与非肿瘤鉴别及跨多个器官的分级任务。外部结果显示PathSearch优于传统图像到图像检索框架。多中心读者研究证明在真实临床场景中提高了诊断准确性，增强了信心，并提高了观察者间一致性。&lt;h4&gt;结论&lt;/h4&gt;PathSearch被确立为数字病理学中可扩展和通用的检索解决方案。&lt;h4&gt;翻译&lt;/h4&gt;病理切片的快速数字化为临床和研究工作流程中的计算工具开辟了新的可能性。在这些工具中，基于内容的幻灯片检索脱颖而出，使病理学家能够识别形态学和语义上相似的病例，从而支持精确诊断，增强观察者间的一致性，并协助基于案例的教育。然而，由于全幻灯片图像的千兆像素规模以及在大量无关内容中捕捉细微语义差异的困难，全幻灯片图像的有效检索仍然具有挑战性。为了克服这些挑战，我们提出了PathSearch，这是一个检索框架，统一了细粒度注意力马赛克表示与全局幻灯片嵌入，通过视觉语言对比学习对齐。在包含6,926个幻灯片-报告对的语料库上训练，PathSearch捕捉了细粒度形态学线索和高级语义模式，以实现准确和灵活的检索。该框架支持两个关键功能：(1)基于马赛克的图像到图像检索，确保准确和高效的幻灯片研究；(2)多模态检索，文本查询可以直接检索相关幻灯片。PathSearch在四个公共病理数据集和三个内部队列上进行了严格评估，涵盖了解剖部位检索、肿瘤亚型分类、肿瘤与非肿瘤鉴别以及跨乳腺、肺、肾、肝和胃等多种器官的分级任务。外部结果显示，PathSearch优于传统的图像到图像检索框架。多中心读者研究进一步证明，在真实临床场景中，PathSearch提高了病理学家的诊断准确性，增强了信心，并提高了观察者间的一致性。这些结果确立了PathSearch作为数字病理学中可扩展和通用的检索解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid digitization of histopathology slides has opened up newpossibilities for computational tools in clinical and research workflows. Amongthese, content-based slide retrieval stands out, enabling pathologists toidentify morphologically and semantically similar cases, thereby supportingprecise diagnoses, enhancing consistency across observers, and assistingexample-based education. However, effective retrieval of whole slide images(WSIs) remains challenging due to their gigapixel scale and the difficulty ofcapturing subtle semantic differences amid abundant irrelevant content. Toovercome these challenges, we present PathSearch, a retrieval framework thatunifies fine-grained attentive mosaic representations with global-wise slideembeddings aligned through vision-language contrastive learning. Trained on acorpus of 6,926 slide-report pairs, PathSearch captures both fine-grainedmorphological cues and high-level semantic patterns to enable accurate andflexible retrieval. The framework supports two key functionalities: (1)mosaic-based image-to-image retrieval, ensuring accurate and efficient slideresearch; and (2) multi-modal retrieval, where text queries can directlyretrieve relevant slides. PathSearch was rigorously evaluated on four publicpathology datasets and three in-house cohorts, covering tasks includinganatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination,and grading across diverse organs such as breast, lung, kidney, liver, andstomach. External results show that PathSearch outperforms traditionalimage-to-image retrieval frameworks. A multi-center reader study furtherdemonstrates that PathSearch improves diagnostic accuracy, boosts confidence,and enhances inter-observer agreement among pathologists in real clinicalscenarios. These results establish PathSearch as a scalable and generalizableretrieval solution for digital pathology.</description>
      <author>example@mail.com (Hongyi Wang, Zhengjie Zhu, Jiabo Ma, Fang Wang, Yue Shi, Bo Luo, Jili Wang, Qiuyu Cai, Xiuming Zhang, Yen-Wei Chen, Lanfen Lin, Hao Chen)</author>
      <guid isPermaLink="false">2510.23224v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
  <item>
      <title>MATCH: Task-Driven Code Evaluation through Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.23169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MATCH的新型无参考代码评估指标，使用对比学习为代码和自然语言任务描述生成有意义的嵌入，实现相似性评分。&lt;h4&gt;背景&lt;/h4&gt;AI代码生成日益普及，GitHub Copilot估计生成GitHub上46%的代码。准确评估生成代码与开发者意图的匹配度是重大挑战，传统方法如单元测试难以扩展且成本高，语法相似性指标无法捕捉代码功能，而需要参考代码的指标如CodeBERTScore并不总是可用。&lt;h4&gt;目的&lt;/h4&gt;解决无参考代码评估的空白，提供一种不依赖参考代码的代码生成质量评估方法。&lt;h4&gt;方法&lt;/h4&gt;引入MATCH指标，使用对比学习技术为代码和自然语言任务描述生成有意义的嵌入，实现反映生成代码实现任务程度的相似性评分。&lt;h4&gt;主要发现&lt;/h4&gt;MATCH在多种编程语言上实现了比现有指标与功能正确性和人类偏好更强的相关性。&lt;h4&gt;结论&lt;/h4&gt;MATCH是一种有效的无参考代码评估指标，能够更好地评估生成代码与任务意图的匹配度。&lt;h4&gt;翻译&lt;/h4&gt;基于AI的代码生成日益普及，GitHub Copilot估计生成了GitHub上46%的代码。准确评估生成代码与开发者意图的匹配度仍然是一个重大挑战。传统的评估方法，如单元测试，通常难以扩展且成本高昂。语法相似性指标（如BLEU、ROUGE）无法捕捉代码功能，而像CodeBERTScore这样的指标需要参考代码，但参考代码并不总是可用的。为了解决无参考评估的空白，除了ICE-Score等少数替代方案外，本文引入了MATCH，一种新型的无参考指标。MATCH使用对比学习为代码和自然语言任务描述生成有意义的嵌入，实现反映生成代码实现任务程度的相似性评分。我们证明，在多种编程语言上，MATCH比现有指标实现了与功能正确性和人类偏好更强的相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-based code generation is increasingly prevalent, with GitHub Copilotestimated to generate 46% of the code on GitHub. Accurately evaluating how wellgenerated code aligns with developer intent remains a critical challenge.Traditional evaluation methods, such as unit tests, are often unscalable andcostly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture codefunctionality, and metrics like CodeBERTScore require reference code, which isnot always available. To address the gap in reference-free evaluation, with fewalternatives such as ICE-Score, this paper introduces MATCH, a novelreference-free metric. MATCH uses Contrastive Learning to generate meaningfulembeddings for code and natural language task descriptions, enabling similarityscoring that reflects how well generated code implements the task. We show thatMATCH achieves stronger correlations with functional correctness and humanpreference than existing metrics across multiple programming languages.</description>
      <author>example@mail.com (Marah Ghoummaid, Vladimir Tchuiev, Ofek Glick, Michal Moschkovitz, Dotan Di Castro)</author>
      <guid isPermaLink="false">2510.23169v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>VALA: Learning Latent Anchors for Training-Free and Temporally Consistent</title>
      <link>http://arxiv.org/abs/2510.22970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VALA（Variational Alignment for Latent Anchors）变分对齐模块，用于解决现有无需训练的视频编辑方法中帧选择和时间一致性的问题。&lt;h4&gt;背景&lt;/h4&gt;无需训练的视频编辑技术最近取得了进展，利用预训练的文本到图像扩散模型实现了轻量级和精确的跨帧生成。然而，现有方法通常依赖启发式帧选择来维持DDIM反演过程中的时间一致性，这引入了人工偏差并降低了端到端推理的可扩展性。&lt;h4&gt;目的&lt;/h4&gt;开发一种自适应选择关键帧并将它们的潜在特征压缩为语义锚点的方法，以实现一致的视频编辑。&lt;h4&gt;方法&lt;/h4&gt;VALA采用具有对比学习目标的变分框架，将跨帧潜在表示转换为保留内容和时间一致性的压缩潜在锚点。该方法可以完全集成到无需训练的基于文本到图像的视频编辑模型中。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界视频编辑基准上的大量实验表明，VALA在反演保真度、编辑质量和时间一致性方面达到了最先进的性能，同时相比之前的方法提供了更高的效率。&lt;h4&gt;结论&lt;/h4&gt;VALA是一种有效的变分对齐模块，能够解决现有视频编辑方法中帧选择和时间一致性的挑战，提高了视频编辑的质量和效率。&lt;h4&gt;翻译&lt;/h4&gt;最近无需训练的视频编辑技术的进步使得利用预训练的文本到图像扩散模型实现了轻量级和精确的跨帧生成。然而，现有方法通常依赖启发式帧选择来维持DDIM反演过程中的时间一致性，这引入了人工偏差并降低了端到端推理的可扩展性。在本文中，我们提出了VALA（变分锚点对齐），这是一种变分对齐模块，可以自适应选择关键帧并将它们的潜在特征压缩为语义锚点，以实现一致的视频编辑。为了学习有意义的分配，VALA提出了一个具有对比学习目标的变分框架。因此，它可以将跨帧潜在表示转换为保留内容和时间一致性的压缩潜在锚点。我们的方法可以完全集成到无需训练的基于文本到图像的视频编辑模型中。在真实世界视频编辑基准上的大量实验表明，VALA在反演保真度、编辑质量和时间一致性方面达到了最先进的性能，同时相比之前的方法提供了更高的效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in training-free video editing have enabled lightweight andprecise cross-frame generation by leveraging pre-trained text-to-imagediffusion models. However, existing methods often rely on heuristic frameselection to maintain temporal consistency during DDIM inversion, whichintroduces manual bias and reduces the scalability of end-to-end inference. Inthis paper, we propose~\textbf{VALA} (\textbf{V}ariational \textbf{A}lignmentfor \textbf{L}atent \textbf{A}nchors), a variational alignment module thatadaptively selects key frames and compresses their latent features intosemantic anchors for consistent video editing. To learn meaningful assignments,VALA propose a variational framework with a contrastive learning objective.Therefore, it can transform cross-frame latent representations into compressedlatent anchors that preserve both content and temporal coherence. Our methodcan be fully integrated into training-free text-to-image based video editingmodels. Extensive experiments on real-world video editing benchmarks show thatVALA achieves state-of-the-art performance in inversion fidelity, editingquality, and temporal consistency, while offering improved efficiency overprior methods.</description>
      <author>example@mail.com (Zhangkai Wu, Xuhui Fan, Zhongyuan Xie, Kaize Shi, Longbing Cao)</author>
      <guid isPermaLink="false">2510.22970v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Lingual Sponsored Search via Dual-Encoder and Graph Neural Networks for Context-Aware Query Translation in Advertising Platforms</title>
      <link>http://arxiv.org/abs/2510.22957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AdGraphTrans是一种结合图神经网络的双编码器框架，用于广告中的上下文感知查询翻译，显著提高了跨语言搜索广告的效果。&lt;h4&gt;背景&lt;/h4&gt;跨语言搜索广告对全球广告平台至关重要，但传统机器翻译方法无法捕捉查询特定的上下文线索，导致语义歧义，影响点击率和转化率。&lt;h4&gt;目的&lt;/h4&gt;解决传统翻译方法在广告搜索中的局限性，提高跨语言搜索广告的效果。&lt;h4&gt;方法&lt;/h4&gt;提出AdGraphTrans框架，使用多语言Transformer编码器独立编码用户查询和广告内容，将上下文关系建模为异构图，应用图注意力网络改进嵌入，并通过对比学习对齐嵌入以减少翻译歧义。&lt;h4&gt;主要发现&lt;/h4&gt;AdGraphTrans在EN-ZH、EN-ES、EN-FR语言对上实现BLEU得分38.9和语义相似度0.83，优于mBERT和M2M-100基线方法；在下游广告检索任务中提高4.67%点击率和1.72%转化率。&lt;h4&gt;结论&lt;/h4&gt;将基于图的上下文信号与双编码器翻译相结合，为增强广告平台中的跨语言搜索广告提供了强大的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;跨语言搜索广告对全球广告平台至关重要，用户来自不同语言背景并与多语言广告互动。传统机器翻译方法往往无法捕捉查询特定的上下文线索，导致语义歧义，对点击率和转化率产生负面影响。为应对这一挑战，我们提出了AdGraphTrans，一种结合图神经网络的双编码器新框架，用于广告中的上下文感知查询翻译。具体而言，使用多语言Transformer编码器独立编码用户查询和广告内容，并将上下文关系（如共同点击的广告、用户搜索会话和查询-广告共现）建模为异构图。然后应用图注意力网络利用语义和行为上下文改进嵌入。这些嵌入通过对比学习对齐，以减少翻译歧义。在从Google Ads和Amazon Ads收集的跨语言搜索广告数据集（EN-ZH、EN-ES、EN-FR对）上进行的实验表明，AdGraphTrans显著提高了查询翻译质量，实现BLEU得分38.9和语义相似度0.83，优于mBERT和M2M-100等强基线方法。此外，在下游广告检索任务中，AdGraphTrans比基线方法提高了4.67%的点击率和1.72%的转化率。这些结果证实，将基于图的上下文信号与双编码器翻译相结合，为增强广告平台中的跨语言搜索广告提供了强大的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-lingual sponsored search is crucial for global advertising platforms,where users from different language backgrounds interact with multilingual ads.Traditional machine translation methods often fail to capture query-specificcontextual cues, leading to semantic ambiguities that negatively impactclick-through rates (CTR) and conversion rates (CVR). To address thischallenge, we propose AdGraphTrans, a novel dual-encoder framework enhancedwith graph neural networks (GNNs) for context-aware query translation inadvertising. Specifically, user queries and ad contents are independentlyencoded using multilingual Transformer-based encoders (mBERT/XLM-R), andcontextual relations-such as co-clicked ads, user search sessions, and query-adco-occurrence-are modeled as a heterogeneous graph. A graph attention network(GAT) is then applied to refine embeddings by leveraging semantic andbehavioral context. These embeddings are aligned via contrastive learning toreduce translation ambiguity. Experiments conducted on a cross-lingualsponsored search dataset collected from Google Ads and Amazon Ads (EN-ZH,EN-ES, EN-FR pairs) demonstrate that AdGraphTrans significantly improves querytranslation quality, achieving a BLEU score of 38.9 and semantic similarity(cosine score) of 0.83, outperforming strong baselines such as mBERT andM2M-100. Moreover, in downstream ad retrieval tasks, AdGraphTrans yields +4.67%CTR and +1.72% CVR improvements over baseline methods. These results confirmthat incorporating graph-based contextual signals with dual-encoder translationprovides a robust solution for enhancing cross-lingual sponsored search inadvertising platforms.</description>
      <author>example@mail.com (Ziyang Gao, Yuanliang Qu, Yi Han)</author>
      <guid isPermaLink="false">2510.22957v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics</title>
      <link>http://arxiv.org/abs/2510.22937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究挑战了生物特征统计学上不相关的传统假设，证明同一个体的生物特征（特别是虹膜）实际上存在相关性，并使用双编码器网络和深度学习模型验证了这一发现。&lt;h4&gt;背景&lt;/h4&gt;历史上一直假设个体的生物特征在统计学上是不相关的，这一假设需要被检验。&lt;h4&gt;目的&lt;/h4&gt;测试个体生物特征统计学上不相关的假设，通过训练双编码器网络进行三种生物特征验证任务。&lt;h4&gt;方法&lt;/h4&gt;在274名受试者上训练双编码器网络（约10万张指纹图像和7千张虹膜图像），进行三种匹配任务：指纹到指纹匹配、虹膜到虹膜匹配、跨模态指纹到虹膜匹配。使用ResNet-50和Vision Transformer骨干网络构建双编码器架构，最小化来自同一个体的图像之间的对比损失。&lt;h4&gt;主要发现&lt;/h4&gt;虹膜ResNet架构在虹膜到虹膜匹配中达到91的ROC AUC分数，证明个体的左右虹膜是相关的；指纹模型重现了先前工作提出的正样本内相关性；这是首次尝试使用Vision Transformer进行此类匹配；跨模态匹配仅略高于随机水平。&lt;h4&gt;结论&lt;/h4&gt;这些发现继续挑战生物特征的独立性假设，研究计划将这项工作扩展到其他生物特征。&lt;h4&gt;翻译&lt;/h4&gt;历史上一直假设个体的生物特征在统计学上是不相关的。我们通过在三种验证任务上训练双编码器网络来测试这一假设，包括指纹到指纹匹配、虹膜到虹膜匹配，以及使用274名受试者（约10万张指纹和7千张虹膜图像）进行的跨模态指纹到虹膜匹配。我们在双编码器架构中训练了ResNet-50和Vision Transformer骨干网络，以最小化来自同一个体的图像之间的对比损失。虹膜ResNet架构在虹膜到虹膜匹配中达到91的ROC AUC分数，提供了个体左右虹膜相关的明确证据。指纹模型重现了该领域先前工作所提出的正样本内相关性。这是首次尝试使用Vision Transformer进行此类匹配。跨模态匹配仅略高于随机水平，这表明需要更多数据和更复杂的管道才能获得令人信服的结果。这些发现继续挑战生物特征的独立性假设，我们计划将这项工作扩展到其他生物特征。代码可用：https://github.com/MatthewSo/bio_fingerprints_iris。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There has been a historic assumption that the biometrics of an individual arestatistically uncorrelated. We test this assumption by training Bi-Encodernetworks on three verification tasks, including fingerprint-to-fingerprintmatching, iris-to-iris matching, and cross-modal fingerprint-to-iris matchingusing 274 subjects with $\sim$100k fingerprints and 7k iris images. We trainedResNet-50 and Vision Transformer backbones in Bi-Encoder architectures suchthat the contrastive loss between images sampled from the same individual isminimized. The iris ResNet architecture reaches 91 ROC AUC score foriris-to-iris matching, providing clear evidence that the left and right irisesof an individual are correlated. Fingerprint models reproduce the positiveintra-subject suggested by prior work in this space. This is the first workattempting to use Vision Transformers for this matching. Cross-modal matchingrises only slightly above chance, which suggests that more data and a moresophisticated pipeline is needed to obtain compelling results. These findingscontinue challenge independence assumptions of biometrics and we plan to extendthis work to other biometrics in the future. Code available:https://github.com/MatthewSo/bio_fingerprints_iris.</description>
      <author>example@mail.com (Matthew So, Judah Goldfeder, Mark Lis, Hod Lipson)</author>
      <guid isPermaLink="false">2510.22937v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-Preserving Cross-Style Visual Reasoning for Robust Multi-Modal Understanding in Large Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.22838v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SP-CSVR的新型框架，用于解决大型视觉-语言模型中的'风格陷阱'问题，实现稳定语义理解和跨风格视觉推理。&lt;h4&gt;背景&lt;/h4&gt;'风格陷阱'阻碍了大型视觉-语言模型在不同视觉风格下的稳健语义理解，特别是在上下文学习中。现有方法难以有效解耦风格与内容，限制了模型的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实现稳定语义理解和自适应跨风格视觉推理的框架，以克服风格陷阱问题。&lt;h4&gt;方法&lt;/h4&gt;提出语义保持跨风格视觉推理器(SP-CSVR)，包含三个核心组件：跨风格特征编码器(CSFE)用于风格-内容解耦；语义对齐的上下文解码器(SAICD)用于高效的少样本风格适应；自适应语义一致性模块(ASCM)采用多任务对比学习强制跨风格语义不变性。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的多风格数据集上，SP-CSVR在视觉描述、视觉问答和上下文风格适应方面达到了最先进的性能。消融研究和泛化分析证实了该方法在增强稳健性、泛化能力和效率方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;SP-CSVR成功解决了大型视觉-语言模型中的风格陷阱问题，实现了跨风格的稳定语义理解和高效推理。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为'语义保持跨风格视觉推理器'(SP-CSVR)的新型框架，旨在解决大型视觉-语言模型(LVLMs)中的'风格陷阱'问题，从而实现稳定的语义理解和自适应的跨风格视觉推理。SP-CSVR集成了跨风格特征编码器(CSFE)用于风格-内容解耦，语义对齐的上下文解码器(SAICD)用于高效的少样本风格适应，以及采用多任务对比学习的自适应语义一致性模块(ASCM)来强制跨风格语义不变性。在具有挑战性的多风格数据集上的广泛实验表明，SP-CSVR在视觉描述、视觉问答和上下文风格适应方面达到了最先进的性能。包括消融研究和泛化分析在内的全面评估证实了SP-CSVR在增强稳健性、泛化能力和效率方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The "style trap" poses a significant challenge for Large Vision-LanguageModels (LVLMs), hindering robust semantic understanding across diverse visualstyles, especially in in-context learning (ICL). Existing methods often fail toeffectively decouple style from content, hindering generalization. To addressthis, we propose the Semantic-Preserving Cross-Style Visual Reasoner (SP-CSVR),a novel framework for stable semantic understanding and adaptive cross-stylevisual reasoning. SP-CSVR integrates a Cross-Style Feature Encoder (CSFE) forstyle-content disentanglement, a Semantic-Aligned In-Context Decoder (SAICD)for efficient few-shot style adaptation, and an Adaptive Semantic ConsistencyModule (ASCM) employing multi-task contrastive learning to enforce cross-stylesemantic invariance. Extensive experiments on a challenging multi-style datasetdemonstrate SP-CSVR's state-of-the-art performance across visual captioning,visual question answering, and in-context style adaptation. Comprehensiveevaluations, including ablation studies and generalization analysis, confirmSP-CSVR's efficacy in enhancing robustness, generalization, and efficiencyacross diverse visual styles.</description>
      <author>example@mail.com (Aya Nakayama, Brian Wong, Yuji Nishimura, Kaito Tanaka)</author>
      <guid isPermaLink="false">2510.22838v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2510.22706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/lifuguan/IGGT_official&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了InstanceGrounded Geometry Transformer (IGGT)，一个端到端的大型统一transformer，用于统一空间重建和实例级上下文理解的知识，并通过3D一致的对比学习策略仅使用2D视觉输入编码具有几何结构和基于实例聚类的统一表示。&lt;h4&gt;背景&lt;/h4&gt;人类自然地将3D世界的几何结构和语义内容视为相互交织的维度，但大多数先前方法优先训练大型几何模型进行低级3D重建，将高级空间理解孤立处理，忽视了这两个基本方面间的关键相互作用，导致下游3D理解任务表现不佳。最近的尝试通过简单对齐3D模型与特定语言模型来缓解问题，但限制了感知能力和下游任务适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够统一几何结构和语义理解的模型，改善3D场景的理解和重建能力，提高在下游任务中的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;设计了IGGT模型和3D一致的对比学习策略，指导模型仅通过2D视觉输入编码具有几何结构和基于实例聚类的统一表示。同时构建了InsScene-15K数据集，包含高质量的RGB图像、姿态、深度图和3D一致的实例级掩码注释。&lt;h4&gt;主要发现&lt;/h4&gt;通过统一几何结构和语义理解，可以有效地将2D视觉输入一致提升到具有明显不同对象实例的连贯3D场景，改善3D场景的理解和重建。&lt;h4&gt;结论&lt;/h4&gt;IGGT模型和InsScene-15K数据集能够有效解决3D场景分析中几何结构和语义理解分离的问题，提高下游3D理解任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;人类自然地将3D世界的几何结构和语义内容视为交织的维度，使能够连贯准确地理解复杂场景。然而，大多数先前方法优先训练大型几何模型进行低级3D重建，并将高级空间理解孤立处理，忽视了这两个3D场景分析基本方面之间的关键相互作用，从而限制了泛化能力，导致在下游3D理解任务中表现不佳。最近的尝试通过简单地将3D模型与特定语言模型对齐来缓解这一问题，但这限制了感知能力，并限制了下游任务的适应性。在本文中，我们提出了InstanceGrounded Geometry Transformer (IGGT)，一个端到端的大型统一transformer，用于统一空间重建和实例级上下文理解的知识。具体来说，我们设计了一种3D一致的对比学习策略，指导IGGT仅通过2D视觉输入编码具有几何结构和基于实例聚类的统一表示。这种表示支持将2D视觉输入一致提升到具有明显不同对象实例的连贯3D场景。为了促进这一任务，我们进一步构建了InsScene-15K，一个包含高质量RGB图像、姿态、深度图和3D一致的实例级掩码注释的大规模数据集，采用了新颖的数据整理流程。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D几何重建和高级语义理解被分离处理的问题。人类自然将几何结构和语义内容作为交织维度理解3D世界，但当前方法要么优先处理低级几何重建而忽视高级语义理解，要么简单将几何与特定语言模型对齐。这种分离限制了模型泛化能力，导致在下游3D理解任务中表现不佳。解决这个问题对实现接近人类的空间智能理解至关重要，对机器人操作、AR/VR和空间规划等应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到人类自然地将几何结构和语义内容作为交织维度理解3D世界，然后指出当前方法的局限性：将几何重建和语义理解分离，或者简单地将几何与语言模型对齐。他们设计了一种新思路：通过联合训练将几何和实例级语义特征耦合，让模型自主学习3D实例级语义与其几何结构之间的关系。该方法借鉴了VGGT的Transformer架构、DINOv2的图像特征提取、DPT的密集预测架构以及SAM2的视频对象分割技术，并在实例空间跟踪中受到SAMPart3D的启发。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合训练将几何结构和实例级语义特征耦合，使模型能够自主学习3D实例级语义与其几何结构之间的关系，并使用实例掩码作为桥梁连接统一表示与各种视觉语言模型。整体流程为：1)输入多张RGB图像；2)使用大型统一Transformer编码为统一标记表示；3)通过几何头部和实例头部解码生成几何点图和实例聚类场；4)使用跨模态融合块增强实例特征的空间感知；5)应用3D一致的对比学习策略确保跨视图一致性；6)使用无监督聚类生成实例掩码；7)这些掩码引导视觉语言模型执行下游任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出IGGT框架，统一几何重建和语义理解；2)设计3D一致的对比学习策略；3)构建InsScene-15K数据集；4)提出实例级场景理解范式。相比之前工作，不同之处在于：不再将几何和语义分离处理，而是通过联合训练实现相互增强；不再绑定特定语言模型，而是通过实例掩码灵活集成各种视觉语言模型；能区分同一类别内的不同对象实例；实现更好的多视图一致性，特别是在大视角变化和复杂场景中。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; IGGT通过统一几何重建和语义理解，并引入实例级场景理解范式，实现了高质量、一致的语义3D重建，并能灵活支持多种下游应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans naturally perceive the geometric structure and semantic content of a3D world as intertwined dimensions, enabling coherent and accurateunderstanding of complex scenes. However, most prior approaches prioritizetraining large geometry models for low-level 3D reconstruction and treathigh-level spatial understanding in isolation, overlooking the crucialinterplay between these two fundamental aspects of 3D-scene analysis, therebylimiting generalization and leading to poor performance in downstream 3Dunderstanding tasks. Recent attempts have mitigated this issue by simplyaligning 3D models with specific language models, thus restricting perceptionto the aligned model's capacity and limiting adaptability to downstream tasks.In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), anend-to-end large unified transformer to unify the knowledge for both spatialreconstruction and instance-level contextual understanding. Specifically, wedesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encodea unified representation with geometric structures and instance-groundedclustering through only 2D visual inputs. This representation supportsconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitlydistinct object instances. To facilitate this task, we further constructInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depthmaps, and 3D-consistent instance-level mask annotations with a novel datacuration pipeline.</description>
      <author>example@mail.com (Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, Manyuan Zhang, Gang Yu, Dingwen Zhang, Ziwei Liu)</author>
      <guid isPermaLink="false">2510.22706v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>CLEANet: Robust and Efficient Anomaly Detection in Contaminated Multivariate Time Series</title>
      <link>http://arxiv.org/abs/2510.22619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为CLEANet的稳健高效的多变量时间序列异常检测框架，用于解决训练数据污染和模型推理效率低下的问题。CLEANet通过抗污染训练框架和轻量级共轭MLP设计，显著提高了检测性能并降低了计算成本。&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列异常检测对于维护工业系统可靠性至关重要，但现实部署面临两大挑战：训练数据污染（噪声和隐藏异常）和低效的模型推理。现有无监督方法假设训练数据干净，但污染会扭曲学习模式并降低检测准确性；同时，复杂深度模型容易过度拟合到污染数据上且延迟高，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种稳健且高效的多变量时间序列异常检测框架，能够有效处理训练数据污染问题，避免模型过度拟合，并提高计算效率，从而提升异常检测的准确性和实用性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了CLEANet框架，包含两个核心组件：1) 抗污染训练框架(CRTF)，通过自适应重建权重策略结合聚类引导的对比学习减轻污染样本影响；2) 轻量级共轭MLP，用于分离时间跨特征依赖关系，避免过度拟合并提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;在五个公共数据集上，CLEANet比十个最先进的基线方法实现了高达73.04%的F1提升和81.28%的运行时间减少。此外，将CRTF集成到三个先进模型中平均获得5.35%的F1提升，证明了其良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CLEANet框架有效解决了多变量时间序列异常检测中的训练数据污染和模型推理效率问题，通过抗污染训练策略和轻量级模型设计显著提升了检测性能和计算效率，具有良好的实用价值和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;多变量时间序列异常检测对于维护工业系统可靠性至关重要，但现实部署受到两个关键挑战的阻碍：训练数据污染（噪声和隐藏异常）和低效的模型推理。现有无监督方法假设训练数据干净，但污染会扭曲学习模式并降低检测准确性。同时，复杂深度模型往往过度拟合到污染数据上并遭受高延迟，限制了实际应用。为解决这些挑战，我们提出了CLEANet，一个在受污染多变量时间序列中稳健且高效的异常检测框架。CLEANet引入了抗污染训练框架(CRTF)，通过自适应重建权重策略结合聚类引导的对比学习减轻污染样本的影响，从而增强稳健性。为进一步避免在污染数据上过度拟合并提高计算效率，我们设计了一个轻量级共轭MLP，用于分离时间跨特征依赖关系。在五个公共数据集上，CLEANet比十个最先进的基线方法实现了高达73.04%的F1提升和81.28%的运行时间减少。此外，将CRTF集成到三个先进模型中平均获得5.35%的F1提升，证实了其强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series (MTS) anomaly detection is essential for maintainingthe reliability of industrial systems, yet real-world deployment is hindered bytwo critical challenges: training data contamination (noises and hiddenanomalies) and inefficient model inference. Existing unsupervised methodsassume clean training data, but contamination distorts learned patterns anddegrades detection accuracy. Meanwhile, complex deep models often overfit tocontamination and suffer from high latency, limiting practical use. To addressthese challenges, we propose CLEANet, a robust and efficient anomaly detectionframework in contaminated multivariate time series. CLEANet introduces aContamination-Resilient Training Framework (CRTF) that mitigates the impact ofcorrupted samples through an adaptive reconstruction weighting strategycombined with clustering-guided contrastive learning, thereby enhancingrobustness. To further avoid overfitting on contaminated data and improvecomputational efficiency, we design a lightweight conjugate MLP thatdisentangles temporal and cross-feature dependencies. Across five publicdatasets, CLEANet achieves up to 73.04% higher F1 and 81.28% lower runtimecompared with ten state-of-the-art baselines. Furthermore, integrating CRTFinto three advanced models yields an average 5.35% F1 gain, confirming itsstrong generalizability.</description>
      <author>example@mail.com (Songhan Zhang, Yuanhao Lai, Pengfei Zheng, Boxi Yu, Xiaoying Tang, Qiuai Fu, Pinjia He)</author>
      <guid isPermaLink="false">2510.22619v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers</title>
      <link>http://arxiv.org/abs/2510.22555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CP-GBA(Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers)，一种新的可转移图后门攻击方法，通过图提示学习训练通用子图触发器，实现了跨学习范式的有效攻击。&lt;h4&gt;背景&lt;/h4&gt;图神经网络易受后门攻击，现有触发器生成器结构简单，过度依赖特定特征，局限于单一图学习范式（如图监督学习、图对比学习或图提示学习），导致跨范式转移性差，无法充分利用图数据的复杂结构信息和节点多样性。&lt;h4&gt;目的&lt;/h4&gt;解决现有触发器生成器的局限性，提高攻击成功率，开发一种能在多种学习范式中有效工作的可转移图后门攻击方法。&lt;h4&gt;方法&lt;/h4&gt;提出CP-GBA方法，首先从目标图中提炼出紧凑且具有表达力的触发器集合，通过联合强制类感知性、特征丰富度和结构保真度来实现；其次探索了GPL在基于提示的目标下训练这些触发器的理论可转移性，使其能泛化到多样化和未见过的测试时范式。&lt;h4&gt;主要发现&lt;/h4&gt;CP-GBA在多个真实数据集和防御场景中实现了最先进的攻击成功率，证明了其在不同学习范式间的有效泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CP-GBA通过利用图提示学习训练通用子图触发器，解决了现有触发器生成器的局限性，提高了攻击成功率，为图神经网络的后门攻击研究提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)容易受到后门攻击，攻击者植入恶意触发器来操纵模型预测。现有的触发器生成器通常结构简单，过度依赖特定特征，将其限制在单一图学习范式中。这种专门化设计导致触发器在应用于其他学习范式时转移性差。此外，这些简单生成器通常无法利用图数据中的复杂结构信息或节点多样性，限制了攻击成功率。因此，我们提出了CP-GBA，采用图提示学习训练一组通用子图触发器，通过提炼紧凑且具有表达力的触发器集合和探索理论可转移性，实现了在多种学习范式中的有效攻击，并在多个数据集和防御场景中取得了最先进的攻击成功率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks(GNNs) are vulnerable to backdoor attacks, whereadversaries implant malicious triggers to manipulate model predictions.  Existing trigger generators are often simplistic in structure and overlyreliant on specific features, confining them to a single graph learningparadigm, such as graph supervised learning, graph contrastive learning, orgraph prompt learning.  This specialized design, which aligns the trigger with one learningobjective, results in poor transferability when applied to other learningparadigms.  For instance, triggers generated for the graph supervised learning paradigmperform poorly when tested within graph contrastive learning or graph promptlearning environments.  Furthermore, these simple generators often fail to utilize complex structuralinformation or node diversity within the graph data.  These constraints limit the attack success rates of such methods in generaltesting scenarios.  Therefore, to address these limitations, we propose Cross-Paradigm GraphBackdoor Attacks with Promptable Subgraph Triggers(CP-GBA), a new transferablegraph backdoor attack that employs graph prompt learning(GPL) to train a set ofuniversal subgraph triggers.  First, we distill a compact yet expressive trigger set from target graphs,which is structured as a queryable repository, by jointly enforcingclass-awareness, feature richness, and structural fidelity.  Second, we conduct the first exploration of the theoretical transferabilityof GPL to train these triggers under prompt-based objectives, enablingeffective generalization to diverse and unseen test-time paradigms.  Extensive experiments across multiple real-world datasets and defensescenarios show that CP-GBA achieves state-of-the-art attack success rates.</description>
      <author>example@mail.com (Dongyi Liu, Jiangtong Li, Dawei Cheng, Changjun Jiang)</author>
      <guid isPermaLink="false">2510.22555v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding</title>
      <link>http://arxiv.org/abs/2510.22264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了PatenTEB，一个全面的专利文本嵌入基准测试，包含15个任务，跨越检索、分类、释义和聚类领域，共206万个示例。同时开发了patembed模型家族，通过多任务训练，参数量从67M到344M不等，上下文长度最高可达4096个token。外部验证显示patembed-base在MTEB BigPatentClustering.v2上达到最先进水平（0.494 V-measure vs. 之前的0.445最佳），而patembed-large在DAPFAM上达到0.377 NDCG@100。&lt;h4&gt;背景&lt;/h4&gt;专利文本嵌入能够实现现有技术搜索、技术景观分析和专利分析，但现有的基准测试无法充分捕捉专利特有的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够更好地捕捉专利特定挑战的基准测试和模型，以提高专利文本嵌入的性能和适用性。&lt;h4&gt;方法&lt;/h4&gt;1. 创建PatenTEB基准测试，包含15个任务，跨越检索、分类、释义和聚类领域；2. 使用领域分层分割、领域特定硬负挖掘和系统覆盖不对称片段到文档匹配场景；3. 开发patembed模型家族，通过多任务训练，参数量从67M到344M；4. 使用领域预训练初始化。&lt;h4&gt;主要发现&lt;/h4&gt;1. 多任务训练提高了外部泛化能力，尽管对基准测试有轻微影响；2. 领域预训练初始化在任务家族中提供了持续的优势；3. patembed-base在MTEB BigPatentClustering.v2上达到最先进水平（0.494 V-measure）；4. patembed-large在DAPFAM上达到0.377 NDCG@100。&lt;h4&gt;结论&lt;/h4&gt;PatenTEB基准测试和patembed模型家族能够有效解决专利文本嵌入中的特定挑战，并通过多任务训练和领域预训练初始化实现了更好的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;专利检索、句子嵌入、多任务学习、不对称检索、基准测试评估、对比学习&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patent text embeddings enable prior art search, technology landscaping, andpatent analysis, yet existing benchmarks inadequately capture patent-specificchallenges. We introduce PatenTEB, a comprehensive benchmark comprising 15tasks across retrieval, classification, paraphrase, and clustering, with 2.06million examples. PatenTEB employs domain-stratified splits, domain specifichard negative mining, and systematic coverage of asymmetricfragment-to-document matching scenarios absent from general embeddingbenchmarks. We develop the patembed model family through multi-task training,spanning 67M to 344M parameters with context lengths up to 4096 tokens.External validation shows strong generalization: patembed-base achievesstate-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.Systematic ablations reveal that multi-task training improves externalgeneralization despite minor benchmark costs, and that domain-pretrainedinitialization provides consistent advantages across task families. Allresources will be made available at https://github.com/iliass-y/patenteb.Keywords: patent retrieval, sentence embeddings, multi-task learning,asymmetric retrieval, benchmark evaluation, contrastive learning.</description>
      <author>example@mail.com (Iliass Ayaou, Denis Cavallucci)</author>
      <guid isPermaLink="false">2510.22264v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Attention Residual Fusion Network with Contrast for Source-free Domain Adaptation</title>
      <link>http://arxiv.org/abs/2510.22142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于对比学习的注意力残差融合网络(ARFNet)框架，用于解决源域无适应(SFDA)中的负迁移和域偏移问题。&lt;h4&gt;背景&lt;/h4&gt;源域无适应(SFDA)是在源域训练模型后应用于相关目标域，但适应过程中无法访问源数据和标签的任务。场景信息复杂和缺乏源域数据使SFDA成为一项困难任务。现有研究虽取得一定成果，但许多方法只关注域偏移而忽略负迁移的影响。&lt;h4&gt;目的&lt;/h4&gt;解决SFDA中的负迁移和域偏移问题，提高模型在适应过程中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出ARFNet框架，利用三种技术：1)注意力机制捕获目标对象的判别区域；2)将注意力特征分解为空间和通道注意力，实现跨层注意力残差融合和自蒸馏；3)对比全局和局部表示，提高类别感知能力；4)动态质心评估策略评估可信质心和标签，用于自监督自蒸馏，减轻域偏移。&lt;h4&gt;主要发现&lt;/h4&gt;在五个不同规模的基准测试上进行的综合实验表明，该方法优于其他技术，在SFDA基准测试中取得了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;ARFNet框架有效解决了SFDA中的负迁移和域偏移问题，在多个基准测试上证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;源域无适应(SFDA)涉及在源域训练模型，然后将其应用于相关目标域，但在适应过程中无法访问源数据和标签。场景信息复杂和缺乏源域数据使SFDA成为一项困难任务。最近研究显示出有希望的结果，但许多域适应方法集中在域偏移上，而忽略了负迁移的影响，这可能阻碍模型在适应过程中的性能提升。在本文中，针对这一问题，我们提出了一个基于对比学习的注意力残差融合网络(ARFNet)框架，用于SFDA，以减轻适应过程中的负迁移和域偏移，其中利用了注意力残差融合、全局-局部注意力对比和动态质心评估。具体来说，首先利用注意力机制捕获目标对象的判别区域。然后，在每个块中，注意力特征被分解为空间注意力和通道注意力，以逐步实现跨层注意力残差融合和自蒸馏。在适应过程中，我们对比全局和局部表示，以提高不同类别的感知能力，使模型能够区分类内和类间变化。最后，利用动态质心评估策略评估可信质心和标签，用于自监督自蒸馏，旨在准确近似源域中心和伪标签，以减轻域偏移。为了验证有效性，我们在五个不同规模的基准上进行了综合实验。实验结果表明，我们的方法优于其他技术，在SFDA基准测试中取得了优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TCSVT.2025.3626247&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Source-free domain adaptation (SFDA) involves training a model on sourcedomain and then applying it to a related target domain without access to thesource data and labels during adaptation. The complexity of scene informationand lack of the source domain make SFDA a difficult task. Recent studies haveshown promising results, but many approaches to domain adaptation concentrateon domain shift and neglect the effects of negative transfer, which may impedeenhancements of model performance during adaptation. n this paper, addressingthis issue, we propose a novel framework of Attention Residual Fusion Network(ARFNet) based on contrast learning for SFDA to alleviate negative transfer anddomain shift during the progress of adaptation, in which attention residualfusion, global-local attention contrast, and dynamic centroid evaluation areexploited. Concretely, the attention mechanism is first exploited to capturethe discriminative region of the target object. Then, in each block, attentionfeatures are decomposed into spatial-wise and channel-wise attentions toachieve the cross-layer attention residual fusion progressively andself-distillation. During adaptation progress, we contrast global and localrepresentations to improve the perceptual capabilities of different categories,which enables the model to discriminate variations between inner-class andintra-class. Finally, a dynamic centroid evaluation strategy is exploited toevaluate the trustworthy centroids and labels for self-supervisedself-distillation, which aims to accurately approximate the center of thesource domain and pseudo-labels to mitigate domain shift. To validate theefficacy, we execute comprehensive experiments on five benchmarks of varyingscales. Experimental outcomes indicate that our method surpasses othertechniques, attaining superior performance across SFDA benchmarks.</description>
      <author>example@mail.com (Renrong Shao, Wei Zhang, Jun Wang)</author>
      <guid isPermaLink="false">2510.22142v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction</title>
      <link>http://arxiv.org/abs/2510.22141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了LOC框架，一种用于3D场景理解的视觉语言模型方法，通过密集对比学习增强开放集识别能力。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在开放集挑战中取得了显著进展，但3D数据集的有限可用性限制了它们在3D场景理解中的有效应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的语言引导框架，适应各种占据网络，支持监督和自监督学习范式，以改善VLMs在3D场景理解中的应用。&lt;h4&gt;方法&lt;/h4&gt;提出LOC框架，融合多帧LiDAR点，使用泊松重建填补空洞，通过KNN分配体素语义，引入DCL缓解特征过度同质化，预测嵌入CLIP特征空间的密集体素特征。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验表明，该方法对已知类别实现了高精度预测，能够区分未知类别而无需额外训练数据。&lt;h4&gt;结论&lt;/h4&gt;LOC框架有效解决了VLMs在3D场景理解中的应用限制，通过密集对比学习增强了开放集识别能力，同时支持监督和自监督学习。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在开放集挑战中已显示出显著进展。然而，3D数据集的有限可用性阻碍了它们在3D场景理解中的有效应用。我们提出了LOC，一个通用的语言引导框架，可适应各种占据网络，支持监督和自监督学习范式。对于自监督任务，我们采用了一种融合多帧LiDAR点以处理动态/静态场景的策略，使用泊松重建填补空洞，并通过K近邻为体素分配语义，以获得全面的体素表示。为了缓解直接高维特征蒸馏导致的特征过度同质化问题，我们引入了密集对比学习。DCL利用密集体素语义信息和预定义的文本提示，有效增强了开放集识别能力，无需密集像素级监督，我们的框架还可以利用现有真实数据进一步改善性能。我们的模型预测嵌入在CLIP特征空间中的密集体素特征，整合文本和图像像素信息，并基于文本和语义相似性进行分类。在nuScenes数据集上的实验证明了该方法的优越性能，对已知类别实现了高精度预测，并能区分未知类别而无需额外训练数据。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决开放集3D占用预测问题，即让模型能够识别训练数据中未包含的新物体类别。这个问题在自动驾驶等领域非常重要，因为现实世界中物体种类繁多，训练数据无法覆盖所有可能的物体类别，系统需要能够识别未知物体以确保安全。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D占用预测方法的局限性，即它们只能识别已知类别。然后借鉴了视觉语言模型(如CLIP)的知识，利用它们在大量图像-文本对上训练的优势。同时采用了多帧LiDAR点云融合、Poisson重建和KNN等技术来处理3D数据的稀疏性问题。最终设计了LOC框架，结合了监督学习和自监督学习，并通过密集对比学习解决了特征过度同质化的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练的视觉语言模型的丰富语义知识，通过密集对比学习将2D空间的知识转移到3D空间，增强模型对未知类别的识别能力。整体流程包括：1)将2D图像特征投影到3D体素空间；2)使用占用头预测体素状态；3)通过语言头将体素特征映射到文本嵌入空间；4)应用鲁棒密集化策略生成密集3D表示；5)使用密集对比学习对齐体素特征与文本提示；6)结合两个头的输出实现开放集预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)LOC框架，首个用于开放集3D占用预测的语言引导框架；2)密集对比学习(DCL)，有效增强开放集识别能力并避免特征过度同质化；3)鲁棒密集化策略，生成高质量密集3D占用表示。相比之前的工作，LOC能够同时处理已知和未知类别，而传统方法只能识别已知类别；LOC避免了直接高维特征蒸馏的问题；LOC支持监督和自监督学习，能更好地利用有限标注数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LOC是一个通用的语言引导框架，通过密集对比学习将2D视觉语言模型的知识有效转移到3D空间，实现了对已知类别的高精度预测和对未知类别的有效区分，无需额外训练数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have shown significant progress in open-setchallenges. However, the limited availability of 3D datasets hinders theireffective application in 3D scene understanding. We propose LOC, a generallanguage-guided framework adaptable to various occupancy networks, supportingboth supervised and self-supervised learning paradigms. For self-supervisedtasks, we employ a strategy that fuses multi-frame LiDAR points fordynamic/static scenes, using Poisson reconstruction to fill voids, andassigning semantics to voxels via K-Nearest Neighbor (KNN) to obtaincomprehensive voxel representations. To mitigate feature over-homogenizationcaused by direct high-dimensional feature distillation, we introduce DenselyContrastive Learning (DCL). DCL leverages dense voxel semantic information andpredefined textual prompts. This efficiently enhances open-set recognitionwithout dense pixel-level supervision, and our framework can also leverageexisting ground truth to further improve performance. Our model predicts densevoxel features embedded in the CLIP feature space, integrating textual andimage pixel information, and classifies based on text and semantic similarity.Experiments on the nuScenes dataset demonstrate the method's superiorperformance, achieving high-precision predictions for known classes anddistinguishing unknown classes without additional training data.</description>
      <author>example@mail.com (Yuhang Gao, Xiang Xiang, Sheng Zhong, Guoyou Wang)</author>
      <guid isPermaLink="false">2510.22141v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.21957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper was accepted in the 2025 IEEE International Conference on  Computer Design (ICCD)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合自监督对比学习和神经架构搜索的框架，用于解决勒索软件检测中的三大局限性，实现了更高的检测准确性和更快的响应时间。&lt;h4&gt;背景&lt;/h4&gt;勒索软件已成为网络安全的严重威胁，因其快速演变、需要早期检测且多样性增加，对传统检测方法构成重大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有AI勒索软件检测方法的三大局限性：特征依赖性、响应延迟和对未知变种的适应性有限。&lt;h4&gt;方法&lt;/h4&gt;提出一个结合自监督对比学习和神经架构搜索的框架，具体包括：(1)设计结合硬件性能计数器的对比学习框架分析勒索软件运行时行为；(2)引入自定义损失函数实现早期检测减少延迟；(3)部署神经架构搜索框架自动构建自适应模型架构。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与现有方法相比，提出的方法在检测准确性上提升高达16.1%，响应时间改善高达6倍，并在规避攻击下保持鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在检测准确性、响应时间和鲁棒性方面均优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;勒索软件由于其快速演变、早期检测的必要性和日益增长的多样性，已成为网络安全的关键威胁，对传统检测方法构成了重大挑战。虽然先前的研究提出了基于人工智能的方法来辅助勒索软件检测，但现有方法存在三个主要局限性：特定的特征依赖性、响应延迟以及对未见变种的适应性有限。在本文中，我们提出了一种结合自监督对比学习和神经架构搜索(NAS)的框架来解决这些挑战。具体来说，本文提供了三个重要贡献：(1)我们设计了一个结合硬件性能计数器(HPC)的对比学习框架，用于分析目标勒索软件的运行时行为。(2)我们引入了一个自定义的损失函数，鼓励对恶意活动的早期检测，并显著减少了检测延迟。(3)我们部署了一个神经架构搜索(NAS)框架，自动构建自适应的模型架构，使检测器能够灵活地与未见的勒索软件变种保持一致。实验结果表明，与现有方法相比，我们提出的方法在检测准确性(高达16.1%)和响应时间(高达6倍)方面都有显著提高，同时在规避攻击下保持鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ransomware has become a critical threat to cybersecurity due to its rapidevolution, the necessity for early detection, and growing diversity, posingsignificant challenges to traditional detection methods. While AI-basedapproaches had been proposed by prior works to assist ransomware detection,existing methods suffer from three major limitations, ad-hoc featuredependencies, delayed response, and limited adaptability to unseen variants. Inthis paper, we propose a framework that integrates self-supervised contrastivelearning with neural architecture search (NAS) to address these challenges.Specifically, this paper offers three important contributions. (1) We design acontrastive learning framework that incorporates hardware performance counters(HPC) to analyze the runtime behavior of target ransomware. (2) We introduce acustomized loss function that encourages early-stage detection of maliciousactivity, and significantly reduces the detection latency. (3) We deploy aneural architecture search (NAS) framework to automatically construct adaptivemodel architectures, allowing the detector to flexibly align with unseenransomware variants. Experimental results show that our proposed methodachieves significant improvements in both detection accuracy (up to 16.1%) andresponse time (up to 6x) compared to existing approaches while maintainingrobustness under evasive attacks.</description>
      <author>example@mail.com (Zhixin Pan, Ziyu Shu, Amberbir Alemayoh)</author>
      <guid isPermaLink="false">2510.21957v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Learning Neural Observer-Predictor Models for Limb-level Sampling-based Locomotion Planning</title>
      <link>http://arxiv.org/abs/2510.22789v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于学习的观察器-预测器框架，用于准确预测足式机器人的全身运动，解决了简化运动学模型无法捕捉复杂闭环动力学的问题。该系统通过神经观察器提供可靠状态估计，并使用高效预测器评估潜在轨迹，在四足机器人上成功实现了肢体感知的运动规划。&lt;h4&gt;背景&lt;/h4&gt;准确的全身运动预测对足式机器人的安全自主导航至关重要，特别是在复杂环境中进行肢体级碰撞检查。简化的运动学模型往往无法捕捉机器人和其底层控制器的复杂闭环动力学，导致预测仅限于简单的平面运动。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测足式机器人复杂全身运动的框架，克服传统简化模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出一个基于学习的观察器-预测器框架，包含：1)具有可证明UUB保证的神经观察器，从本体感觉测量历史中提供可靠的潜在状态估计；2)计算高效的预测器，能够快速并行评估数千条潜在轨迹；3)将系统集成到基于MPPI的规划器中，并在Vision 60四足机器人上进行了硬件实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;硬件实验成功展示了系统在具有挑战性的狭窄通道和小物体上的有效肢体感知运动规划能力，证明该系统为动态机器人平台上的高性能、碰撞感知规划提供了稳健基础。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于学习的观察器-预测器框架能够准确预测足式机器人的全身运动，为复杂环境中的安全自主导航和碰撞感知规划提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的全身运动预测对足式机器人的安全自主导航至关重要，能够实现如杂乱环境中肢体级碰撞检查等关键功能。简化的运动学模型往往无法捕捉机器人和其底层控制器的复杂闭环动力学，限制了它们仅能预测简单的平面运动。为此，我们提出了一种基于学习的观察器-预测器框架，能够准确预测这种运动。我们的方法特点是一个具有可证明UUB保证的神经观察器，它从本体感觉测量历史中提供可靠的潜在状态估计。这个稳定的估计初始化了一个计算高效的预测器，专为现代采样规划器所需的大量潜在轨迹的快速并行评估而设计。我们通过将神经预测器集成到Vision 60四足机器人的基于MPPI的规划器中验证了该系统。硬件实验成功展示了在具有挑战性的狭窄通道和小物体上的有效肢体感知运动规划，突显了我们系统为动态机器人平台上的高性能、碰撞感知规划提供稳健基础的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate full-body motion prediction is essential for the safe, autonomousnavigation of legged robots, enabling critical capabilities like limb-levelcollision checking in cluttered environments. Simplified kinematic models oftenfail to capture the complex, closed-loop dynamics of the robot and itslow-level controller, limiting their predictions to simple planar motion. Toaddress this, we present a learning-based observer-predictor framework thataccurately predicts this motion. Our method features a neural observer withprovable UUB guarantees that provides a reliable latent state estimate from ahistory of proprioceptive measurements. This stable estimate initializes acomputationally efficient predictor, designed for the rapid, parallelevaluation of thousands of potential trajectories required by modernsampling-based planners. We validated the system by integrating our neuralpredictor into an MPPI-based planner on a Vision 60 quadruped. Hardwareexperiments successfully demonstrated effective, limb-aware motion planning ina challenging, narrow passage and over small objects, highlighting our system'sability to provide a robust foundation for high-performance, collision-awareplanning on dynamic robotic platforms.</description>
      <author>example@mail.com (Abhijeet M. Kulkarni, Ioannis Poulakakis, Guoquan Huang)</author>
      <guid isPermaLink="false">2510.22789v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing</title>
      <link>http://arxiv.org/abs/2510.19661v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AgentSense是一种混合的、无需训练的框架，将大型语言模型集成到参与式城市感知中，通过多智能体进化系统适应动态城市条件，提供自然语言解释以提高透明度。&lt;h4&gt;背景&lt;/h4&gt;基于网络的参与式城市感知已成为现代城市管理的重要方法，利用移动个体作为分布式传感器。然而，现有系统在跨不同城市场景的泛化能力有限，且在决策过程中可解释性差。&lt;h4&gt;目的&lt;/h4&gt;解决现有城市感知系统的局限性，提高系统的泛化能力和可解释性。&lt;h4&gt;方法&lt;/h4&gt;AgentSense框架将大型语言模型集成到参与式城市感知中，通过多智能体进化系统实现。它首先使用经典规划器生成基线解决方案，然后迭代改进这些解决方案以适应动态城市条件和异构工人偏好，同时生成自然语言解释以提高透明度和信任度。&lt;h4&gt;主要发现&lt;/h4&gt;在两个大规模移动数据集和七种动态干扰的广泛实验中，AgentSense在适应性和可解释性方面明显优于传统方法。与单一智能体LLM基线相比，该方法在性能和鲁棒性方面表现更好，并提供更合理和透明的解释。&lt;h4&gt;结论&lt;/h4&gt;AgentSense是部署自适应和可解释的网络城市感知系统的重要进展。&lt;h4&gt;翻译&lt;/h4&gt;基于网络的参与式城市感知已通过利用移动个体作为分布式传感器，成为现代城市管理的重要方法。然而，现有的城市感知系统在跨不同城市场景的泛化能力有限，且在决策过程中的可解释性较差。在这项工作中，我们介绍了AgentSense，这是一种混合的、无需训练的框架，通过多智能体进化系统将大型语言模型（LLMs）集成到参与式城市感知中。AgentSense首先使用经典规划器生成基线解决方案，然后迭代改进这些解决方案，以使感知任务分配适应动态城市条件和异构工人偏好，同时生成自然语言解释以提高透明度和信任度。在两个大规模移动数据集和七种动态干扰的广泛实验中证明，与传统方法相比，AgentSense在适应性和可解释性方面具有明显优势。此外，与单一智能体LLM基线相比，我们的方法在性能和鲁棒性方面都更优，并提供更合理和透明的解释。这些结果表明，AgentSense是向网络部署自适应和可解释的城市感知系统迈进的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Web-based participatory urban sensing has emerged as a vital approach formodern urban management by leveraging mobile individuals as distributedsensors. However, existing urban sensing systems struggle with limitedgeneralization across diverse urban scenarios and poor interpretability indecision-making. In this work, we introduce AgentSense, a hybrid, training-freeframework that integrates large language models (LLMs) into participatory urbansensing through a multi-agent evolution system. AgentSense initially employsclassical planner to generate baseline solutions and then iteratively refinesthem to adapt sensing task assignments to dynamic urban conditions andheterogeneous worker preferences, while producing natural language explanationsthat enhance transparency and trust. Extensive experiments across twolarge-scale mobility datasets and seven types of dynamic disturbancesdemonstrate that AgentSense offers distinct advantages in adaptivity andexplainability over traditional methods. Furthermore, compared to single-agentLLM baselines, our approach outperforms in both performance and robustness,while delivering more reasonable and transparent explanations. These resultsposition AgentSense as a significant advancement towards deploying adaptive andexplainable urban sensing systems on the web.</description>
      <author>example@mail.com (Xusen Guo, Mingxing Peng, Xixuan Hao, Xingchen Zou, Qiongyan Wang, Sijie Ruan, Yuxuan Liang)</author>
      <guid isPermaLink="false">2510.19661v2</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>CURVETE: Curriculum Learning and Progressive Self-supervised Training for Medical Image Classification</title>
      <link>http://arxiv.org/abs/2510.23442v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in the proceedings of ICONIP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CURVETE是一种创新的深度卷积神经网络，通过课程学习和类别分解方法解决了医学图像分析中的样本有限和类别分布不规则的挑战，在各种医学图像数据集上表现出优越的分类性能。&lt;h4&gt;背景&lt;/h4&gt;在医学图像分析中，识别高质量且易于获取的标注样本是一个显著挑战。迁移学习技术利用预训练数据为这一问题提供了灵活的解决方案。然而，当数据集在类别间呈现不规则分布时，微调的效果会减弱。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为课程学习和渐进式自监督训练(CURVETE)的新型深度卷积神经网络，解决与样本有限相关的挑战，增强模型泛化能力，并提高整体分类性能。&lt;h4&gt;方法&lt;/h4&gt;CURVETE采用基于样本分解粒度的课程学习策略，在训练通用未标记样本时使用；在下游任务中整合类别分解方法，解决类别分布不规则的挑战；在脑肿瘤、数字膝盖X光和Mini-DDSM三个医学图像数据集上进行评估，研究了使用通用自监督样本分解方法的分类性能，包括和不包括课程学习组件。&lt;h4&gt;主要发现&lt;/h4&gt;CURVETE模型在测试集上实现了优越的性能，使用基线ResNet-50在脑肿瘤数据集上达到96.60%的准确率，在数字膝盖X光数据集上达到75.60%，在Mini-DDSM数据集上达到93.35%；使用基线DenseNet-121，在三个数据集上分别达到95.77%、80.36%和93.22%的准确率，优于其他训练策略。&lt;h4&gt;结论&lt;/h4&gt;CURVETE模型能够有效解决医学图像分析中的样本有限和类别分布不规则的挑战，通过课程学习和渐进式自监督训练，显著提高了分类性能。&lt;h4&gt;翻译&lt;/h4&gt;在医学图像分析中，识别高质量且易于获取的标注样本是一个显著挑战。迁移学习技术利用预训练数据为这一问题提供了灵活的解决方案。然而，当数据集在类别间呈现不规则分布时，微调的效果会减弱。本文提出了一种名为课程学习和渐进式自监督训练(CURVETE)的新型深度卷积神经网络。CURVETE通过在训练通用未标记样本时采用基于样本分解粒度的课程学习策略，解决了与样本有限相关的挑战，增强了模型泛化能力，并提高了整体分类性能。此外，CURVETE通过在下游任务中整合类别分解方法，解决了类别分布不规则的挑战。该方法在三个不同的医学图像数据集上进行了评估：脑肿瘤、数字膝盖X光和Mini-DDSM数据集。我们研究了使用通用自监督样本分解方法进行分类性能，包括和不包括在训练预任务中使用课程学习组件。实验结果表明，CURVETE模型在测试集上实现了优越的性能，使用基线ResNet-50在脑肿瘤数据集上达到96.60%的准确率，在数字膝盖X光数据集上达到75.60%，在Mini-DDSM数据集上达到93.35%。此外，使用基线DenseNet-121，在脑肿瘤、数字膝盖X光和Mini-DDSM数据集上分别达到95.77%、80.36%和93.22%的准确率，优于其他训练策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying high-quality and easily accessible annotated samples poses anotable challenge in medical image analysis. Transfer learning techniques,leveraging pre-training data, offer a flexible solution to this issue. However,the impact of fine-tuning diminishes when the dataset exhibits an irregulardistribution between classes. This paper introduces a novel deep convolutionalneural network, named Curriculum Learning and Progressive Self-supervisedTraining (CURVETE). CURVETE addresses challenges related to limited samples,enhances model generalisability, and improves overall classificationperformance. It achieves this by employing a curriculum learning strategy basedon the granularity of sample decomposition during the training of genericunlabelled samples. Moreover, CURVETE address the challenge of irregular classdistribution by incorporating a class decomposition approach in the downstreamtask. The proposed method undergoes evaluation on three distinct medical imagedatasets: brain tumour, digital knee x-ray, and Mini-DDSM datasets. Weinvestigate the classification performance using a generic self-supervisedsample decomposition approach with and without the curriculum learningcomponent in training the pretext task. Experimental results demonstrate thatthe CURVETE model achieves superior performance on test sets with an accuracyof 96.60% on the brain tumour dataset, 75.60% on the digital knee x-raydataset, and 93.35% on the Mini-DDSM dataset using the baseline ResNet-50.Furthermore, with the baseline DenseNet-121, it achieved accuracies of 95.77%,80.36%, and 93.22% on the brain tumour, digital knee x-ray, and Mini-DDSMdatasets, respectively, outperforming other training strategies.</description>
      <author>example@mail.com (Asmaa Abbas, Mohamed Gaber, Mohammed M. Abdelsamea)</author>
      <guid isPermaLink="false">2510.23442v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>DREaM: Drug-Drug Relation Extraction via Transfer Learning Method</title>
      <link>http://arxiv.org/abs/2510.23189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为DREAM的方法，用于药物关系抽取，通过结合关系抽取模型和大型语言模型构建药物关系本体并验证结果。&lt;h4&gt;背景&lt;/h4&gt;药物关系抽取对识别药物相互作用和预测副作用至关重要。机器学习方法和大型医学文本数据库的发展降低了关系抽取成本，但目前缺乏专门针对药物关系抽取的数据集。&lt;h4&gt;目的&lt;/h4&gt;由于缺乏专业数据集，需要采用迁移学习方法来应用机器学习技术进行药物关系抽取，并构建药物关系本体。&lt;h4&gt;方法&lt;/h4&gt;DREAM方法首先使用训练好的关系抽取模型发现实体间关系，然后将模型应用于医学文本语料库构建药物关系本体，最后使用大型语言模型验证抽取的关系。&lt;h4&gt;主要发现&lt;/h4&gt;定量结果显示，大型语言模型同意从PubMed摘要子集中提取的71个关系。定性分析表明该方法能揭示医学领域的模糊性，突显了关系抽取的挑战。&lt;h4&gt;结论&lt;/h4&gt;通过迁移学习和大型语言模型验证，DREAM方法能有效提取药物关系并构建药物关系本体，同时揭示了医学领域中关系抽取的固有挑战。&lt;h4&gt;翻译&lt;/h4&gt;药物之间的关系抽取在识别药物-药物相互作用和预测副作用方面起着至关重要的作用。机器学习方法在关系抽取方面的进步，以及大型医学文本数据库的发展，使得与其他通常需要专业知识的方法相比，这种关系的提取成本更低。然而，据我们所知，目前专门用于药物关系抽取的数据集非常有限。因此，采用迁移学习成为在该领域应用机器学习方法的必要手段。在本研究中，我们提出了DREAM方法，该方法首先使用训练好的关系抽取模型发现实体间的关系，然后将该模型应用于医学文本语料库以构建药物关系本体。随后使用大型语言模型验证抽取的关系。定量结果表明，大型语言模型同意从PubMed摘要子集中提取的71个关系。此外，我们的定性分析表明，这种方法可以揭示医学领域中的模糊性，突显了该领域关系抽取的固有挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relation extraction between drugs plays a crucial role in identifying drugdrug interactions and predicting side effects. The advancement of machinelearning methods in relation extraction, along with the development of largemedical text databases, has enabled the low cost extraction of such relationscompared to other approaches that typically require expert knowledge. However,to the best of our knowledge, there are limited datasets specifically designedfor drug drug relation extraction currently available. Therefore, employingtransfer learning becomes necessary to apply machine learning methods in thisdomain. In this study, we propose DREAM, a method that first employs a trainedrelation extraction model to discover relations between entities and thenapplies this model to a corpus of medical texts to construct an ontology ofdrug relationships. The extracted relations are subsequently validated using alarge language model. Quantitative results indicate that the LLM agreed with 71of the relations extracted from a subset of PubMed abstracts. Furthermore, ourqualitative analysis indicates that this approach can uncover ambiguities inthe medical domain, highlighting the challenges inherent in relation extractionin this field.</description>
      <author>example@mail.com (Ali Fata, Hossein Rahmani, Parinaz Soltanzadeh, Amirhossein Derakhshan, Behrouz Minaei Bidgoli)</author>
      <guid isPermaLink="false">2510.23189v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>LightPFP: A Lightweight Route to Ab Initio Accuracy at Scale</title>
      <link>http://arxiv.org/abs/2510.23064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LightPFP是一种数据高效的知识蒸馏框架，利用通用机器学习原子势(u-MLIP)生成针对特定材料的高质量训练数据，结合预训练轻量级MLIP提高效率，实现了比传统DFT方法快三个数量级的模型开发速度，同时保持与第一性原理相当的准确性，且生成的特定任务MLIP(ts-MLIP)在保持高精度的同时实现了1-2个数量级的推理速度提升。&lt;h4&gt;背景&lt;/h4&gt;原子模拟方法已从量子力学发展到密度泛函理论(DFT)，再到机器学习原子势(MLIPs)。通用MLIPs(u-MLIPs)具有良好的可转移性但计算开销大，限制了大规模应用；特定任务MLIPs(ts-MLIPs)效率更高但为每个材料系统生成DFT数据的成本极高。&lt;h4&gt;目的&lt;/h4&gt;提出一种数据高效的知识蒸馏框架LightPFP，解决传统方法中DFT计算成本高的问题，实现快速开发高精度、高效的特定任务MLIPs。&lt;h4&gt;方法&lt;/h4&gt;LightPFP框架利用u-MLIP生成针对特定材料的高质量训练数据，并使用预训练的轻量级MLIP进一步提高数据效率，通过知识蒸馏技术生成ts-MLIP，同时支持高效的精度迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;LightPFP比传统基于DFT的方法快三个数量级的模型开发速度，同时保持与第一性原理预测相当的准确性；蒸馏出的ts-MLIP比u-MLIP快1-2个数量级的推理速度；仅需10个高精度DFT数据点即可校正u-MLIP的系统误差。&lt;h4&gt;结论&lt;/h4&gt;这种u-MLIP驱动的蒸馏方法能够为材料科学应用快速开发高保真度、高效的MLIPs。&lt;h4&gt;翻译&lt;/h4&gt;原子模拟方法已经通过连续的计算层级逐步发展，每个层级都建立在更基础的方法之上：从量子力学到密度泛函理论(DFT)，随后发展到机器学习原子势(MLIPs)。虽然通用MLIPs(u-MLIPs)具有广泛的可转移性，但其计算开销限制了大规模应用。特定任务MLIPs(ts-MLIPs)实现了更高的效率，但为每个材料系统生成DFT数据的成本高得令人望而却步。在本文中，我们提出了LightPFP，一种数据高效的知识蒸馏框架。LightPFP不使用昂贵的DFT计算，而是利用u-MLIP生成针对特定材料定制的高质量训练数据，并使用预训练的轻量级MLIP进一步提高数据效率，从而生成蒸馏的ts-MLIP。在包括固态电解质、高熵合金和反应离子系统在内的广泛材料范围内，LightPFP比传统基于DFT的方法快三个数量级的模型开发速度，同时保持与第一性原理预测相当的准确性。此外，蒸馏出的ts-MLs进一步维持了大规模分子动力学计算所必需的计算效率，比u-MLIPs快1-2个数量级的推理速度。该框架还支持高效的精度迁移学习，其中可以使用少至10个高精度DFT数据点来校正u-MLIP的系统误差，如在MgO熔点预测中所演示的。这种u-MLIP驱动的蒸馏方法能够为材料科学应用快速开发高保真度、高效的MLIPs。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Atomistic simulation methods have evolved through successive computationallevels, each building upon more fundamental approaches: from quantum mechanicsto density functional theory (DFT), and subsequently, to machine learninginteratomic potentials (MLIPs). While universal MLIPs (u-MLIPs) offer broadtransferability, their computational overhead limits large-scale applications.Task-specific MLIPs (ts-MLIPs) achieve superior efficiency but requireprohibitively expensive DFT data generation for each material system. In thispaper, we propose LightPFP, a data-efficient knowledge distillation framework.Instead of using costly DFT calculations, LightPFP generates a distilledts-MLIP by leveraging u-MLIP to generate high-quality training data tailoredfor specific materials and utilizing a pre-trained light-weight MLIP to furtherenhance data efficiency. Across a broad spectrum of materials, includingsolid-state electrolytes, high-entropy alloys, and reactive ionic systems,LightPFP delivers three orders of magnitude faster model development thanconventional DFT-based methods, while maintaining accuracy on par withfirst-principles predictions. Moreover, the distilled ts-MLIPs further sustainthe computational efficiency essential for large-scale molecular dynamics,achieving 1-2 orders of magnitude faster inference than u-MLIPs. The frameworkfurther enables efficient precision transfer learning, where systematic errorsfrom the u-MLIP can be corrected using as few as 10 high-accuracy DFT datapoints, as demonstrated for MgO melting point prediction. This u-MLIP-drivendistillation approach enables rapid development of high-fidelity, efficientMLIPs for materials science applications.</description>
      <author>example@mail.com (Wenwen Li, Nontawat Charoenphakdee, Yong-Bin Zhuang, Ryuhei Okuno, Yuta Tsuboi, So Takamoto, Junichi Ishida, Ju Li)</author>
      <guid isPermaLink="false">2510.23064v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>TLCD: A Deep Transfer Learning Framework for Cross-Disciplinary Cognitive Diagnosis</title>
      <link>http://arxiv.org/abs/2510.23062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种创新的跨学科认知诊断方法(TLCD)，结合深度学习和迁移学习策略，解决了跨学科领域中认知诊断面临的挑战，提高了对学生学习情况评估的准确性。&lt;h4&gt;背景&lt;/h4&gt;在线教育模式已成为教育产业的重要组成部分。认知诊断技术可利用学生学习数据评估其能力水平，但跨学科领域存在特征提取复杂性和学科数据稀缺性问题，传统认知诊断方法面临挑战。&lt;h4&gt;目的&lt;/h4&gt;针对不同学科间知识系统、认知结构和数据特征的差异，研究神经网络认知诊断和知识关联神经网络认知诊断，提出创新的跨学科认知诊断方法。&lt;h4&gt;方法&lt;/h4&gt;提出跨学科认知诊断方法(TLCD)，结合深度学习技术和迁移学习策略，通过利用主学科的共同特征来提高模型在目标学科中的性能。&lt;h4&gt;主要发现&lt;/h4&gt;基于深度学习的跨学科认知诊断模型在跨学科认知诊断任务中表现优于基础模型，能够更准确地评估学生的学习情况。&lt;h4&gt;结论&lt;/h4&gt;跨学科认知诊断方法(TLCD)有效解决了跨学科认知诊断中的挑战，提高了诊断的准确性和性能，对智能教育领域具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;受智能教育和人工智能技术的双重驱动，在线教育模式已迅速成为教育产业的重要组成部分。认知诊断技术可以利用教育评估中学生学习的数据和反馈信息，准确评估他们在知识层面的能力水平。然而，大量信息虽然提供了丰富的数据资源，但也带来了特征提取的复杂性和学科数据的稀缺性。在跨学科领域，传统的认知诊断方法仍面临许多挑战。鉴于不同学科之间知识系统、认知结构和数据特征的差异，本文对神经网络认知诊断和知识关联神经网络认知诊断进行了深入研究，并提出了一种创新的跨学科认知诊断方法(TLCD)。该方法结合了深度学习技术和迁移学习策略，通过利用主学科的共同特征来提高模型在目标学科中的性能。实验结果表明，基于深度学习的跨学科认知诊断模型在跨学科认知诊断任务中表现优于基础模型，能够更准确地评估学生的学习情况。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driven by the dual principles of smart education and artificial intelligencetechnology, the online education model has rapidly emerged as an importantcomponent of the education industry. Cognitive diagnostic technology canutilize students' learning data and feedback information in educationalevaluation to accurately assess their ability level at the knowledge level.However, while massive amounts of information provide abundant data resources,they also bring about complexity in feature extraction and scarcity ofdisciplinary data. In cross-disciplinary fields, traditional cognitivediagnostic methods still face many challenges. Given the differences inknowledge systems, cognitive structures, and data characteristics betweendifferent disciplines, this paper conducts in-depth research on neural networkcognitive diagnosis and knowledge association neural network cognitivediagnosis, and proposes an innovative cross-disciplinary cognitive diagnosismethod (TLCD). This method combines deep learning techniques and transferlearning strategies to enhance the performance of the model in the targetdiscipline by utilizing the common features of the main discipline. Theexperimental results show that the cross-disciplinary cognitive diagnosis modelbased on deep learning performs better than the basic model incross-disciplinary cognitive diagnosis tasks, and can more accurately evaluatestudents' learning situation.</description>
      <author>example@mail.com (Zhifeng Wang, Meixin Su, Yang Yang, Chunyan Zeng, Lizhi Ye)</author>
      <guid isPermaLink="false">2510.23062v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges</title>
      <link>http://arxiv.org/abs/2510.22964v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述从模态驱动视角对多模态地理空间基础模型(GFMs)进行全面回顾，涵盖五种核心视觉和视觉-语言模态，分析其在遥感图像分析中的应用、挑战和未来发展方向。&lt;h4&gt;背景&lt;/h4&gt;基础模型已改变自然语言处理和计算机视觉领域，其影响正在重塑遥感图像分析。基础模型强大的泛化和迁移学习能力与遥感数据的多模态、多分辨率和多时态特性自然契合。&lt;h4&gt;目的&lt;/h4&gt;解决遥感领域的独特挑战，通过多模态地理空间基础模型(GFMs)这一专门研究前沿，提供从模态驱动视角的全面回顾，并分析关键技术、评估模型性能和应用场景。&lt;h4&gt;方法&lt;/h4&gt;涵盖五种核心视觉和视觉-语言模态，检查成像物理和数据表示差异如何塑造交互设计，分析对齐、集成和知识转移的关键技术，系统评估训练范式、架构和适应策略进展，在十个下游任务上评估代表性模型，并通过真实案例研究展示应用潜力。&lt;h4&gt;主要发现&lt;/h4&gt;多模态GFMs在土地覆盖制图、农业监测、灾害响应、气候研究和地理空间情报等领域展现实际应用潜力，不同模型在架构、性能和应用场景上存在差异，需要针对模态异构性、分布偏移和语义差距进行优化。&lt;h4&gt;结论&lt;/h4&gt;领域泛化、可解释性、效率和隐私是GFMs发展面临的紧迫挑战，未来研究需要在这些方面探索有前途的方向，进一步提升模型性能和应用范围。&lt;h4&gt;翻译&lt;/h4&gt;基础模型已经改变了自然语言处理和计算机视觉，它们的影响现在正在重塑遥感图像分析。凭借强大的泛化和迁移学习能力，它们与遥感数据的多模态、多分辨率和多时态特性自然契合。为解决该领域的独特挑战，多模态地理空间基础模型(GFMs)已成为专门的研究前沿。这篇综述从模态驱动视角对多模态GFMs进行全面回顾，涵盖五种核心视觉和视觉-语言模态。我们检查成像物理和数据表示差异如何塑造交互设计，并分析对齐、集成和知识转移的关键技术，以处理模态异构性、分布偏移和语义差距。系统评估了训练范式、架构和任务特定适应策略的进展，以及大量新兴基准。在十个下游任务上评估了代表性的多模态视觉和视觉-语言GFMs，深入了解它们的架构、性能和应用场景。真实案例研究，涵盖土地覆盖制图、农业监测、灾害响应、气候研究和地理空间情报，展示了GFMs的实际潜力。最后，我们概述了领域泛化、可解释性、效率和隐私方面的紧迫挑战，并为未来研究规划了有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have transformed natural language processing and computervision, and their impact is now reshaping remote sensing image analysis. Withpowerful generalization and transfer learning capabilities, they alignnaturally with the multimodal, multi-resolution, and multi-temporalcharacteristics of remote sensing data. To address unique challenges in thefield, multimodal geospatial foundation models (GFMs) have emerged as adedicated research frontier. This survey delivers a comprehensive review ofmultimodal GFMs from a modality-driven perspective, covering five core visualand vision-language modalities. We examine how differences in imaging physicsand data representation shape interaction design, and we analyze key techniquesfor alignment, integration, and knowledge transfer to tackle modalityheterogeneity, distribution shifts, and semantic gaps. Advances in trainingparadigms, architectures, and task-specific adaptation strategies aresystematically assessed alongside a wealth of emerging benchmarks.Representative multimodal visual and vision-language GFMs are evaluated acrossten downstream tasks, with insights into their architectures, performance, andapplication scenarios. Real-world case studies, spanning land cover mapping,agricultural monitoring, disaster response, climate studies, and geospatialintelligence, demonstrate the practical potential of GFMs. Finally, we outlinepressing challenges in domain generalization, interpretability, efficiency, andprivacy, and chart promising avenues for future research.</description>
      <author>example@mail.com (Liling Yang, Ning Chen, Jun Yue, Yidan Liu, Jiayi Ma, Pedram Ghamisi, Antonio Plaza, Leyuan Fang)</author>
      <guid isPermaLink="false">2510.22964v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Inductive Transfer Learning for Graph-Based Recommenders</title>
      <link>http://arxiv.org/abs/2510.22799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the New Perspectives in Graph Machine Learning Workshop  at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了NBF-Rec，一种支持跨不同数据集进行归纳迁移学习的图推荐模型，能够在不重新训练的情况下处理新用户、新项目或新数据集。&lt;h4&gt;背景&lt;/h4&gt;图推荐系统通常在归纳设置下训练，这限制了它们对新用户、新项目或新数据集的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种支持在不同数据集上进行归纳迁移学习的图推荐模型，解决传统方法需要为每个领域重新训练的问题。&lt;h4&gt;方法&lt;/h4&gt;提出NBF-Rec模型，一种基于图的推荐模型，可以在用户和项目集合不相交的数据集之间进行归纳迁移学习。与传统基于嵌入的方法不同，NBF-Rec在推理时动态计算节点嵌入，无需为每个领域重新训练。&lt;h4&gt;主要发现&lt;/h4&gt;NBF-Rec在七个真实世界数据集（涵盖电影、音乐、电子商务和地点签到等领域）上进行了评估，在零样本设置下（不使用目标域数据进行训练）取得了具有竞争力的性能，并通过轻量级微调进一步提高了性能。&lt;h4&gt;结论&lt;/h4&gt;归纳迁移在图推荐中是可行的，交互级别的消息传递支持跨数据集的泛化，而无需对齐用户或项目。&lt;h4&gt;翻译&lt;/h4&gt;基于图的推荐系统通常在归纳设置下进行训练，这限制了它们对新用户、新项目或新数据集的适用性。我们提出了NBF-Rec，一种基于图的推荐模型，支持在不同用户和项目集合不相交的数据集上进行归纳迁移学习。与需要为每个领域重新训练的传统基于嵌入的方法不同，NBF-Rec在推理时动态计算节点嵌入。我们在七个涵盖电影、音乐、电子商务和地点签到的真实世界数据集上评估了该方法。NBF-Rec在零样本设置下（不使用目标域数据进行训练）取得了具有竞争力的性能，并通过轻量级微调展示了进一步的改进。这些结果表明，归纳迁移在图推荐中是可行的，并且交互级别的消息传递支持跨数据集的泛化，而无需对齐用户或项目。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-based recommender systems are commonly trained in transductivesettings, which limits their applicability to new users, items, or datasets. Wepropose NBF-Rec, a graph-based recommendation model that supports inductivetransfer learning across datasets with disjoint user and item sets. Unlikeconventional embedding-based methods that require retraining for each domain,NBF-Rec computes node embeddings dynamically at inference time. We evaluate themethod on seven real-world datasets spanning movies, music, e-commerce, andlocation check-ins. NBF-Rec achieves competitive performance in zero-shotsettings, where no target domain data is used for training, and demonstratesfurther improvements through lightweight fine-tuning. These results show thatinductive transfer is feasible in graph-based recommendation and thatinteraction-level message passing supports generalization across datasetswithout requiring aligned users or items.</description>
      <author>example@mail.com (Florian Grötschla, Elia Trachsel, Luca A. Lanzendörfer, Roger Wattenhofer)</author>
      <guid isPermaLink="false">2510.22799v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Qlustering: Harnessing Network-Based Quantum Transport for Data Clustering</title>
      <link>http://arxiv.org/abs/2510.22727v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Qlustering，一种受量子启发的无监督学习算法，利用基于网络的量子传输进行数据聚类，在多种数据集上表现出与经典方法相当或更优的性能，特别是在处理非凸或高维数据时。&lt;h4&gt;背景&lt;/h4&gt;传统聚类方法主要基于距离度量，而量子计算提供了新的计算范式，可以解决传统方法难以处理的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的量子启发式聚类算法，能够有效处理非凸或高维数据，并具有计算效率和物理可实现性。&lt;h4&gt;方法&lt;/h4&gt;Qlustering将数据编码为紧束缚哈密顿量框架中的输入状态，通过量子粒子在网络中的传播动力学进行计算，聚类分配从终端节点的稳态输出电流中产生，算法通过迭代优化网络哈密顿量和随机更新实现收敛。&lt;h4&gt;主要发现&lt;/h4&gt;在合成数据集、定位问题、QM9分子数据库和Iris数据集上，Qlustering与k-means等经典方法相比具有竞争力或更优的性能，特别是在处理非凸或高维数据时表现出色。&lt;h4&gt;结论&lt;/h4&gt;Qlustering具有内在的鲁棒性、低计算复杂性和与光子实现的兼容性，为构建物理可实现的、量子原生的聚类架构提供了有前途的途径。&lt;h4&gt;翻译&lt;/h4&gt;我们引入Qlustering，一种用于无监督学习的受量子启发的算法，它利用基于网络的量子传输来执行数据聚类。与传统的基于距离的方法不同，Qlustering将量子粒子通过网络传播的稳态动力学视为计算资源。数据被编码为由Lindblad主方程控制的紧束缚哈密顿量框架中的输入状态，聚类分配从终端节点的稳态输出电流中产生。该算法迭代地优化网络的哈密顿量以最小化物理动机的成本函数，通过随机更新实现收敛。我们在合成数据集、定位问题以及真实的化学和生物数据（即QM9分子数据库和Iris数据集的子集）上对Qlustering进行了基准测试。在这些多样化的任务中，Qlustering展示了与k-means等经典方法相比具有竞争力或更优的性能，特别是对于非凸或高维数据。其内在的鲁棒性、低计算复杂性和与光子实现的兼容性表明，其有望实现物理可实现的、量子原生的聚类架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Qlustering, a quantum-inspired algorithm for unsupervisedlearning that leverages network-based quantum transport to perform dataclustering. In contrast to traditional distance-based methods, Qlusteringtreats the steady-state dynamics of quantum particles propagating through anetwork as a computational resource. Data are encoded as input states in atight-binding Hamiltonian framework governed by the Lindblad master equation,and cluster assignments emerge from steady-state output currents at terminalnodes. The algorithm iteratively optimizes the network's Hamiltonian tominimize a physically motivated cost function, achieving convergence throughstochastic updates. We benchmark Qlustering on synthetic datasets, alocalization problem, and real-world chemical and biological data, namelysubsets of the QM9 molecular database and the Iris dataset. Across thesediverse tasks, Qlustering demonstrates competitive or superior performancecompared with classical methods such as k-means, particularly for non-convex orhigh-dimensional data. Its intrinsic robustness, low computational complexity,and compatibility with photonic implementations suggest a promising routetoward physically realizable, quantum-native clustering architectures.</description>
      <author>example@mail.com (Shmuel Lorber, Yonatan Dubi)</author>
      <guid isPermaLink="false">2510.22727v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Species Transfer Learning in Agricultural AI: Evaluating ZebraPose Adaptation for Dairy Cattle Pose Estimation</title>
      <link>http://arxiv.org/abs/2510.22618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 11 figures, 6 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了使用在斑马图像上训练的ZebraPose模型进行奶牛姿态估计的跨物种迁移学习潜力，发现在不同环境间存在显著泛化挑战。&lt;h4&gt;背景&lt;/h4&gt;姿态估计是计算机视觉的核心技术，用于理解动物姿态、行为和福利，但农业应用受限于缺乏大型标注的牲畜数据集，特别是奶牛数据集。&lt;h4&gt;目的&lt;/h4&gt;评估跨物种迁移学习的潜力和局限性，通过将ZebraPose模型适应于谷仓条件下奶牛的27个关键点检测。&lt;h4&gt;方法&lt;/h4&gt;使用三种配置评估模型：自定义农场数据集（375张图像）、APT-36K基准数据集的子集以及它们的组合，系统评估了模型在不同环境中的准确性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;组合模型在分布内数据上表现良好，但在未见过的谷仓和奶牛群体上出现显著泛化失败，表明合成到真实域差距是农业AI部署的主要障碍，物种形态相似性不足以实现跨域迁移。&lt;h4&gt;结论&lt;/h4&gt;研究强调了数据集多样性、环境变化性和计算约束对现实世界部署的影响，呼吁以农业为先的AI设计，优先考虑农场级真实性、跨环境鲁棒性和开放基准数据集。&lt;h4&gt;翻译&lt;/h4&gt;姿态估计作为计算机视觉的基石，用于理解动物姿态、行为和福利。然而，农业应用仍然受限于大型标注牲畜数据集的稀缺，特别是奶牛。本研究通过将ZebraPose（一种基于视觉变换器的模型，在合成斑马图像上训练）适应于谷仓条件下奶牛的27个关键点检测，评估了跨物种迁移学习的潜力和局限性。使用三种配置——自定义农场数据集（375张图像，加拿大新不伦瑞克州苏塞克斯）、APT-36K基准数据集的子集以及它们的组合，我们系统评估了模型在不同环境中的准确性和泛化能力。虽然组合模型在分布内数据上取得了有希望的性能，但当应用于未见过的谷仓和奶牛群体时，出现了显著的泛化失败。这些发现揭示了合成到真实域差距是农业AI部署的主要障碍，并强调物种间的形态相似性不足以进行跨域迁移。研究提供了关于数据集多样性、环境变化性和计算约束影响现实世界部署的实践见解。我们呼吁以农业为先的AI设计，优先考虑农场级真实性、跨环境鲁棒性和开放基准数据集，以推进可信和可扩展的以动物为中心的技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pose estimation serves as a cornerstone of computer vision for understandinganimal posture, behavior, and welfare. Yet, agricultural applications remainconstrained by the scarcity of large, annotated datasets for livestock,especially dairy cattle. This study evaluates the potential and limitations ofcross-species transfer learning by adapting ZebraPose - a visiontransformer-based model trained on synthetic zebra imagery - for 27-keypointdetection in dairy cows under real barn conditions. Using three configurations- a custom on-farm dataset (375 images, Sussex, New Brunswick, Canada), asubset of the APT-36K benchmark dataset, and their combination, wesystematically assessed model accuracy and generalization across environments.While the combined model achieved promising performance (AP = 0.86, AR = 0.87,PCK 0.5 = 0.869) on in-distribution data, substantial generalization failuresoccurred when applied to unseen barns and cow populations. These findingsexpose the synthetic-to-real domain gap as a major obstacle to agricultural AIdeployment and emphasize that morphological similarity between species isinsufficient for cross-domain transfer. The study provides practical insightsinto dataset diversity, environmental variability, and computationalconstraints that influence real-world deployment of livestock monitoringsystems. We conclude with a call for agriculture-first AI design, prioritizingfarm-level realism, cross-environment robustness, and open benchmark datasetsto advance trustworthy and scalable animal-centric technologies.</description>
      <author>example@mail.com (Mackenzie Tapp, Sibi Chakravarthy Parivendan, Kashfia Sailunaz, Suresh Neethirajan)</author>
      <guid isPermaLink="false">2510.22618v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>A roadmap for curvature-based geometric data analysis and learning</title>
      <link>http://arxiv.org/abs/2510.22599v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提供了对离散曲率模型的首次全面综述，涵盖了数学基础、计算公式以及在数据分析和学习中的实际应用。文章从黎曼几何和度量几何角度讨论离散曲率，并提出了一种曲率驱动的数据分析系统流程。&lt;h4&gt;背景&lt;/h4&gt;几何数据分析和学习已成为一个独特且快速发展的研究领域，因其跨领域的有效性而日益受到认可。曲率是该领域的核心概念，它能够捕捉内在几何结构并支持从社区检测到几何深度学习的众多任务。针对图、单纯复形、立方体复形和流形采样点云等多种数据表示，已经提出了广泛的离散曲率模型。&lt;h4&gt;目的&lt;/h4&gt;这篇论文旨在对现有的离散曲率模型进行首次全面综述，涵盖其数学基础、计算公式以及在数据分析和学习中的实际应用。&lt;h4&gt;方法&lt;/h4&gt;作者从黎曼几何和度量几何两个角度讨论离散曲率，并提出了一种曲率驱动的数据分析系统流程。他们还检查了不同数据表示下的相应计算算法，提供了详细的比较和见解。&lt;h4&gt;主要发现&lt;/h4&gt;离散曲率模型不仅为数据几何提供了有效的表征，而且构成了几何学习框架的基本组成部分。这些模型在各种数据表示上都有应用，并在监督和无监督学习中取得了最先进的应用效果。&lt;h4&gt;结论&lt;/h4&gt;这篇综述为研究人员提供了一个概念性和实践性的路线图，帮助他们更好地理解离散曲率作为几何理解和学习的基本工具。&lt;h4&gt;翻译&lt;/h4&gt;几何数据分析和学习已成为一个独特且快速发展的研究领域，其有效性在多样化的应用中日益得到认可。该领域的核心是曲率，一个强大且可解释的概念，它捕捉内在的几何结构并支撑着从社区检测到几何深度学习的众多任务。针对图、单纯复形、立方体复形和从流形采样的点云等多种数据表示，已经提出了广泛的离散曲率模型。这些模型不仅为数据几何提供了有效的表征，而且构成了几何学习框架的基本组成部分。在本文中，我们首次对现有的离散曲率模型进行了全面综述，涵盖了它们的数学基础、计算公式以及数据分析和学习中的实际应用。特别是，我们从黎曼几何和度量几何的角度讨论了离散曲率，并提出了一个曲率驱动的数据分析系统流程。我们进一步检查了不同数据表示下的相应计算算法，提供了详细的比较和见解。最后，我们回顾了曲率在监督和无监督学习中的最先进应用。本综述为研究人员提供了一个概念性和实践性的路线图，使他们能够更好地理解离散曲率作为几何理解和学习的基本工具。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是离散曲率模型的系统综述和整合。目前存在多种离散曲率模型（如Forman-Ricci、Ollivier-Ricci等），它们基于不同数学原理，应用于不同数据表示，但缺乏统一框架和比较。这个问题的重要性在于几何数据分析和学习已成为快速发展的研究领域，曲率是理解数据内在几何结构的关键概念，从社区检测到几何深度学习都有重要应用，而离散曲率模型为数据几何提供了高效表征，并构成几何学习框架的基本组成部分。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性地回顾现有文献来构建他们的方法。首先介绍几何数据分析和学习的背景，强调曲率的重要性；然后回顾黎曼几何和度量几何中曲率的数学基础；接着介绍多种离散曲率模型的定义和计算方法；最后提出一个三步流程用于基于曲率的数据分析。作者借鉴了大量现有工作，包括Forman基于Bochner-Weitzenböck公式的组合曲率方法、Ollivier基于最优输运的粗糙Ricci曲率、Bakry-Émery的Ricci曲率下界、Joharinad和Jost的sectional曲率，以及Menger曲率、Haantjes曲率和电阻曲率等网络分析中的定义。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是曲率作为理解数据内在几何结构的基本工具，可以用于分析和学习各种非欧几里得数据。整体实现流程是三步流程：1）数据表示：根据应用领域提取合适的拓扑表示（如图、单纯复形、立方体复形或超图）；2）离散曲率计算：在提取的拓扑表示上应用适当的曲率定义（如Forman-Ricci、Ollivier-Ricci等）；3）特征提取：从计算出的曲率中提取有意义的几何特征，如边基曲率特征化边或连接，顶点基曲率特征化顶点或数据点。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次全面综述离散曲率模型；系统化分类曲率模型；跨领域整合不同领域的曲率概念；提出实用三步流程；提供各种曲率模型在不同数据表示上的具体计算方法。相比之前的工作，这篇论文的不同之处在于：之前的文献通常专注于单一曲率模型或特定应用，而本文提供了全面视角，比较了不同模型的优缺点；不仅关注理论，还关注实际计算和应用；强调了曲率在不同数据表示上的通用性，展示了其在几何深度学习中的广泛应用潜力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提供了离散曲率模型的全面综述，建立了从理论到实践的系统性框架，使研究者能够理解和应用曲率作为几何数据分析和学习的基本工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometric data analysis and learning has emerged as a distinct and rapidlydeveloping research area, increasingly recognized for its effectiveness acrossdiverse applications. At the heart of this field lies curvature, a powerful andinterpretable concept that captures intrinsic geometric structure and underpinsnumerous tasks, from community detection to geometric deep learning. A widerange of discrete curvature models have been proposed for various datarepresentations, including graphs, simplicial complexes, cubical complexes, andpoint clouds sampled from manifolds. These models not only provide efficientcharacterizations of data geometry but also constitute essential components ingeometric learning frameworks. In this paper, we present the firstcomprehensive review of existing discrete curvature models, covering theirmathematical foundations, computational formulations, and practicalapplications in data analysis and learning. In particular, we discuss discretecurvature from both Riemannian and metric geometry perspectives and propose asystematic pipeline for curvature-driven data analysis. We further examine thecorresponding computational algorithms across different data representations,offering detailed comparisons and insights. Finally, we review state-of-the-artapplications of curvature in both supervised and unsupervised learning. Thissurvey provides a conceptual and practical roadmap for researchers to gain abetter understanding of discrete curvature as a fundamental tool for geometricunderstanding and learning.</description>
      <author>example@mail.com (Yasharth Yadav, Kelin Xia)</author>
      <guid isPermaLink="false">2510.22599v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>AnyECG-Lab: An Exploration Study of Fine-tuning an ECG Foundation Model to Estimate Laboratory Values from Single-Lead ECG Signals</title>
      <link>http://arxiv.org/abs/2510.22301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了利用心电图(ECG)通过深度学习技术估计实验室值的可行性，提供了一种非侵入性、快速的临床决策支持方法。&lt;h4&gt;背景&lt;/h4&gt;当前实验室检测依赖于侵入性静脉采样，存在延迟问题。心电图作为无创且广泛可用的信号，为快速估计实验室值提供了有前景的途径，但现有模型受限于低信噪比、个体间变异性大、数据多样性有限以及泛化能力不足。&lt;h4&gt;目的&lt;/h4&gt;探索使用迁移学习微调大规模预训练的ECG基础模型(ECGFounder)，实现从ECG信号中估计实验室值，并建立实时、无创估计实验室值的可行性范围。&lt;h4&gt;方法&lt;/h4&gt;利用斯坦福大学的多模式临床监测急诊数据集(MC-MED)进行探索性研究，使用迁移学习技术对ECGFounder大型预训练模型进行微调，并生成了超过2000万个标准化的10秒ECG片段以增强对细微生化相关性的敏感性。&lt;h4&gt;主要发现&lt;/h4&gt;在内部验证中，模型对33项实验室指标表现出强的预测性能(曲线下面积高于0.65)，对59项指标表现出中等性能(曲线下面积在0.55到0.65之间)，对16项指标表现有限(曲线下面积低于0.55)。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种高效的人工智能驱动解决方案，并建立了实时、无创估计实验室值的可行性范围。&lt;h4&gt;翻译&lt;/h4&gt;及时获取实验室值对临床决策至关重要，但当前方法依赖于侵入性静脉采样且本质上存在延迟。心电图作为一种无创且广泛可用的信号，为快速估计实验室值提供了有前景的方式。深度学习的最新进展使得从心电图中提取潜在的血液学特征成为可能。然而，现有模型受限于低信噪比、显著的个体间变异性、有限的数据多样性和次优的泛化能力，特别是在适配到导联数较少的可穿戴设备时。在本工作中，我们进行了一项探索性研究，利用迁移学习技术在斯坦福大学的多模式临床监测急诊数据集(MC-MED)上微调ECGFounder——一个大规模预训练的心电基础模型。我们生成了超过2000万个标准化的十秒心电片段，以增强对细微生化相关性的敏感性。在内部验证中，该模型对三十三项实验室指标表现出强的预测性能(曲线下面积高于0.65)，对五十九项指标表现出中等性能(在0.55和0.65之间)，对十六项指标表现有限(低于0.55)。该研究提供了一种高效的人工智能驱动解决方案，并建立了实时、无创估计实验室值的可行性范围。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely access to laboratory values is critical for clinical decision-making,yet current approaches rely on invasive venous sampling and are intrinsicallydelayed. Electrocardiography (ECG), as a non-invasive and widely availablesignal, offers a promising modality for rapid laboratory estimation. Recentprogress in deep learning has enabled the extraction of latent hematologicalsignatures from ECGs. However, existing models are constrained by lowsignal-to-noise ratios, substantial inter-individual variability, limited datadiversity, and suboptimal generalization, especially when adapted to low-leadwearable devices. In this work, we conduct an exploratory study leveragingtransfer learning to fine-tune ECGFounder, a large-scale pre-trained ECGfoundation model, on the Multimodal Clinical Monitoring in the EmergencyDepartment (MC-MED) dataset from Stanford. We generated a corpus of more than20 million standardized ten-second ECG segments to enhance sensitivity tosubtle biochemical correlates. On internal validation, the model demonstratedstrong predictive performance (area under the curve above 0.65) forthirty-three laboratory indicators, moderate performance (between 0.55 and0.65) for fifty-nine indicators, and limited performance (below 0.55) forsixteen indicators. This study provides an efficient artificial-intelligencedriven solution and establishes the feasibility scope for real-time,non-invasive estimation of laboratory values.</description>
      <author>example@mail.com (Yujie Xiao, Gongzhen Tang, Wenhui Liu, Jun Li, Guangkun Nie, Zhuoran Kan, Deyun Zhang, Qinghao Zhao, Shenda Hong)</author>
      <guid isPermaLink="false">2510.22301v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Synthetic-to-Real Transfer Learning for Chromatin-Sensitive PWS Microscopy</title>
      <link>http://arxiv.org/abs/2510.22239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 5 figures and 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出CFU Net，一种分层分割架构，使用三阶段课程在合成多模态数据上训练，实现了近乎完美的细胞核分割性能，应用于超过一万个细胞核的自动分析，提取了区分正常与癌前组织的染色质生物标志物，为专业显微镜中的合成到真实迁移学习提供了通用框架。&lt;h4&gt;背景&lt;/h4&gt;染色质敏感部分波谱(csPWS)显微镜技术能够无标记检测发生在可见细胞转化之前的纳米级染色质包装变化，但手动细胞核分割限制了群体规模分析，且缺乏注释的csPWS成像数据阻碍了标准深度学习方法的使用。&lt;h4&gt;目的&lt;/h4&gt;解决手动细胞核分割限制群体规模分析的问题，克服缺乏注释csPWS成像数据的挑战，开发能够自动分析染色质包装变化的方法，用于早期癌症检测中的生物标志物发现。&lt;h4&gt;方法&lt;/h4&gt;提出CFU Net分层分割架构，使用三阶段课程在合成多模态数据上训练；采用基于物理的渲染，结合染色质包装统计、Mie散射模型和模态特定噪声；整合五种架构元素：ConvNeXt骨干网络、特征金字塔网络、UNet++密集连接、双注意力和深度监督；实现INT8量化以提高效率。&lt;h4&gt;主要发现&lt;/h4&gt;在合成测试数据上实现近乎完美性能（Dice 0.9879，IoU 0.9895）；与基础UNet相比Dice提高8.3%；通过量化实现240倍吞吐量增益；提取的染色质生物标志物区分正常与癌前组织效应量显著（Cohen's d在1.31到2.98之间）；分类准确率达94%。&lt;h4&gt;结论&lt;/h4&gt;该工作为专业显微镜中的合成到真实迁移学习提供了通用框架，并提供了社区在临床标本上进行验证的开放资源，有效应用于早期癌症检测中的生物标志物发现。&lt;h4&gt;翻译&lt;/h4&gt;染色质敏感部分波谱(csPWS)显微镜技术能够无标记检测发生在可见细胞转化之前的纳米级染色质包装变化。然而，手动细胞核分割限制了早期癌症检测中生物标志物发现所需的群体规模分析。缺乏注释的csPWS成像数据阻碍了标准深度学习方法的直接使用。我们提出了CFU Net，一种使用三阶段课程在合成多模态数据上训练的分层分割架构。CFU Net在代表多样化光谱成像条件的保留合成测试数据上实现了近乎完美的性能，无需手动注释（Dice 0.9879，IoU 0.9895）。我们的方法使用基于物理的渲染，结合经验支持的染色质包装统计、Mie散射模型和模态特定噪声，并结合一个从对抗性RGB预训练进展到光谱微调和组织学验证的课程。CFU Net整合了五种架构元素（ConvNeXt骨干网络、特征金字塔网络、UNet++密集连接、双注意力和深度监督），这些元素共同使Dice比基础UNet提高了8.3%。我们展示了可部署的INT8量化，压缩率为74.9%，推理时间为0.15秒，比手动分析提高了240倍的吞吐量。应用于来自合成测试数据的超过一万个自动分割的细胞核，该流程提取了区分正常与癌前组织的染色质生物标志物，具有大的效应量（Cohen's d在1.31到2.98之间），达到94%的分类准确率。这项工作为专业显微镜中的合成到真实迁移学习提供了通用框架，并为社区在临床标本上进行验证提供了开放资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chromatin sensitive partial wave spectroscopic (csPWS) microscopy enableslabel free detection of nanoscale chromatin packing alterations that occurbefore visible cellular transformation. However, manual nuclear segmentationlimits population scale analysis needed for biomarker discovery in early cancerdetection. The lack of annotated csPWS imaging data prevents direct use ofstandard deep learning methods. We present CFU Net, a hierarchical segmentationarchitecture trained with a three stage curriculum on synthetic multimodaldata. CFU Net achieves near perfect performance on held out synthetic test datathat represent diverse spectroscopic imaging conditions without manualannotations (Dice 0.9879, IoU 0.9895). Our approach uses physics basedrendering that incorporates empirically supported chromatin packing statistics,Mie scattering models, and modality specific noise, combined with a curriculumthat progresses from adversarial RGB pretraining to spectroscopic fine tuningand histology validation. CFU Net integrates five architectural elements(ConvNeXt backbone, Feature Pyramid Network, UNet plus plus dense connections,dual attention, and deep supervision) that together improve Dice over abaseline UNet by 8.3 percent. We demonstrate deployment ready INT8 quantizationwith 74.9 percent compression and 0.15 second inference, giving a 240 timesthroughput gain over manual analysis. Applied to more than ten thousandautomatically segmented nuclei from synthetic test data, the pipeline extractschromatin biomarkers that distinguish normal from pre cancerous tissue withlarge effect sizes (Cohens d between 1.31 and 2.98), reaching 94 percentclassification accuracy. This work provides a general framework for syntheticto real transfer learning in specialized microscopy and open resources forcommunity validation on clinical specimens.</description>
      <author>example@mail.com (Jahidul Arafat, Sanjaya Poudel)</author>
      <guid isPermaLink="false">2510.22239v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model</title>
      <link>http://arxiv.org/abs/2510.22057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 12 figures, and 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种自动化系统来检测在线学习期间的学生参与度，同时确保模型不依赖性别等敏感特征进行预测，提高了模型的公平性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;随着在线和虚拟学习的兴起，监控和提升学生参与度已成为有效教育的重要方面，但传统评估方法可能不直接适用于虚拟环境。&lt;h4&gt;目的&lt;/h4&gt;开发一个自动化系统来检测在线学习期间学生的参与度水平，解决传统方法在虚拟环境中不适用的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的训练方法，应用属性正则正交化技术到分割模型分类器中，并使用多种迁移学习策略，以阻止模型利用敏感特征如性别进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法不仅有助于执行道德标准，还能增强模型预测的可解释性；通过该方法，预测敏感群体的分布差异从未缓解模型的皮尔逊相关系数0.897降低到缓解模型的0.999。&lt;h4&gt;结论&lt;/h4&gt;成功开发了一个能够检测在线学习中学生参与度的自动化系统，同时确保了模型的公平性和可解释性，源代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;随着在线和虚拟学习的兴起，监控和提升学生参与度已成为有效教育的重要方面。评估学生参与度的传统方法可能不直接适用于虚拟环境。在本研究中，我们关注这一问题，致力于开发一个自动化系统来检测在线学习期间学生的参与度水平。我们提出了一种新的训练方法，可以阻止模型利用性别等敏感特征进行预测。所提出的方法不仅在执行道德标准方面有益，还能增强模型预测的可解释性。我们将属性正则正交化技术应用于分割模型分类器，该分类器使用多种迁移学习策略，在减少预测敏感群体的分布差异方面取得了有效成果，从未缓解模型的皮尔逊相关系数0.897降低到缓解模型的0.999。该项目的源代码可在https://github.com/ashiskb/elearning-engagement-study获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rise of online and virtual learning, monitoring and enhancingstudent engagement have become an important aspect of effective education.Traditional methods of assessing a student's involvement might not beapplicable directly to virtual environments. In this study, we focused on thisproblem and addressed the need to develop an automated system to detect studentengagement levels during online learning. We proposed a novel training methodwhich can discourage a model from leveraging sensitive features like gender forits predictions. The proposed method offers benefits not only in theenforcement of ethical standards, but also to enhance interpretability of themodel predictions. We applied an attribute-orthogonal regularization techniqueto a split-model classifier, which uses multiple transfer learning strategiesto achieve effective results in reducing disparity in the distribution ofprediction for sensitivity groups from a Pearson correlation coefficient of0.897 for the unmitigated model, to 0.999 for the mitigated model. The sourcecode for this project is available onhttps://github.com/ashiskb/elearning-engagement-study .</description>
      <author>example@mail.com (James Thiering, Tarun Sethupat Radha Krishna, Dylan Zelkin, Ashis Kumer Biswas)</author>
      <guid isPermaLink="false">2510.22057v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>LiteDiff</title>
      <link>http://arxiv.org/abs/2510.22004v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Lite-Diff，一种轻量级扩散模型适应方法，通过将轻量适应层集成到冻结的扩散U-Net中，结合潜在形态自编码器和像素级判别器，显著降低了计算成本并减少了过拟合，即使在数据有限的情况下也能高效工作。&lt;h4&gt;背景&lt;/h4&gt;扩散模型在高保真图像合成方面取得了显著成功，但在特定领域（如医学成像）微调这些模型仍然具有挑战性，原因是领域特定数据有限和完整模型适应的高计算成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效微调方法，使扩散模型能够在特定领域（如医学成像）有效适应，同时降低计算成本并减少过拟合风险。&lt;h4&gt;方法&lt;/h4&gt;Lite-Diff将轻量级适应层集成到冻结的扩散U-Net中，同时使用潜在形态自编码器（用于领域特定潜在一致性）和像素级判别器（用于对抗对齐）来增强训练。通过冻结基础模型权重并仅优化小型残差适配器模块实现轻量化。&lt;h4&gt;主要发现&lt;/h4&gt;选择性在不同U-Net块中集成适应层可以找到效率与性能的最佳平衡。在三个胸部X光数据集（Kaggle Chest X-Ray Pneumonia、NIH Chest X-ray14和VinBigData Chest X_ray）上的实验表明，Lite-Diff相比传统完整微调实现了更好的适应效率。&lt;h4&gt;结论&lt;/h4&gt;Lite-Diff框架为扩散模型的迁移学习提供了有希望的方向，促进了它们在多样化低数据领域中的部署。&lt;h4&gt;翻译&lt;/h4&gt;近年来，扩散模型在高保真图像合成方面表现出色。然而，由于领域特定数据有限和完整模型适应的高计算成本，将这些模型微调到专业领域（如医学成像）仍然具有挑战性。在本文中，我们引入了Lite-Diff（轻量级扩散模型适应），一种新的微调方法，它将轻量级适应层集成到冻结的扩散U-Net中，同时使用潜在形态自编码器（用于领域特定潜在一致性）和像素级判别器（用于对抗对齐）来增强训练。通过冻结基础模型的权重并仅优化小型残差适配器模块，Lite-Diff显著降低了计算开销并减轻了过拟合，即使在数据有限的情况下也是如此。此外，我们进行了消融研究，分析了在不同U-Net块中选择性集成适应层的效果，揭示了效率与性能之间的最佳平衡。在三个胸部X光数据集 - (1) Kaggle胸部X光肺炎、(2) NIH胸部X光14和(3) VinBigData胸部X光上的实验表明，Lite-Diff相比传统完整微调实现了更好的适应效率。我们的框架为扩散模型的迁移学习提供了有希望的方向，促进了它们在多样化低数据领域中的部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, diffusion models have demonstrated remarkable success inhigh-fidelity image synthesis. However, fine-tuning these models forspecialized domains, such as medical imaging, remains challenging due tolimited domain-specific data and the high computational cost of full modeladaptation. In this paper, we introduce Lite-Diff (Lightweight Diffusion ModelAdaptation), a novel finetuning approach that integrates lightweight adaptationlayers into a frozen diffusion U-Net while enhancing training with a latentmorphological autoencoder (for domain-specific latent consistency) and a pixellevel discriminator(for adversarial alignment). By freezing weights of the basemodel and optimizing only small residual adapter modules, LiteDiffsignificantly reduces the computational overhead and mitigates overfitting,even in minimal-data settings. Additionally, we conduct ablation studies toanalyze the effects of selectively integrating adaptation layers in differentU-Net blocks, revealing an optimal balance between efficiency and performance.Experiments on three chest X-ray datasets - (1) Kaggle Chest X-Ray Pneumonia,(2) NIH Chest X-ray14 and (3) VinBigData Chest X_ray demonstrate that LiteDiffachieves superior adaptation efficiency compared to naive full fine-tuning. Ourframework provides a promising direction for transfer learning in diffusionmodels, facilitating their deployment in diverse low data domains.</description>
      <author>example@mail.com (Ruchir Namjoshi, Nagasai Thadishetty, Vignesh Kumar, Hemanth Venkateshwara)</author>
      <guid isPermaLink="false">2510.22004v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Split-MMD Training for Small-Sample Cross-Dataset P300 EEG Classification</title>
      <link>http://arxiv.org/abs/2510.21969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures. Submitted to IEEE BIBM 2025 Workshop on Machine  Learning for EEG Signal Processing (MLESP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种自适应分割最大均值差异训练(AS-MMD)方法，用于解决从脑电图(EEG)中检测单次试验P300时数据量有限的问题，特别是在跨数据集迁移学习中的分布偏移挑战。&lt;h4&gt;背景&lt;/h4&gt;当只有少量标记试验可用时，从脑电图(EEG)中检测单次试验P300是困难的。当尝试通过迁移学习用大型源数据集增强小型目标集时，会出现跨数据集偏移问题。&lt;h4&gt;目的&lt;/h4&gt;研究两个公共视觉oddball ERP数据集之间的迁移学习，解决在小样本设置下(目标:每个受试者10次试验；源:每个受试者80次试验)的跨数据集分布不一致问题。&lt;h4&gt;方法&lt;/h4&gt;提出自适应分割最大均值差异训练(AS-MMD)，结合了三种技术：(1)与源/目标大小比值相关的目标加权损失和预热；(2)具有共享参数和每域统计的分割批量归一化；(3)使用中带带宽启发式的无参数对数级RBF核最大均值差异项。该方法在EEG Conformer上实现，与主干网络无关且保持推理模型不变。&lt;h4&gt;主要发现&lt;/h4&gt;在两种迁移方向上，AS-MMD均优于仅目标训练和联合训练(Active Visual Oddball: 准确率/AUC为0.66/0.74；ERP CORE P3: 0.61/0.65)，与联合训练相比的增益在统计上显著。消融研究表明所有三个组件都对性能提升有贡献。&lt;h4&gt;结论&lt;/h4&gt;AS-MMD方法有效解决了小样本条件下EEG信号P300检测中的跨数据集迁移学习挑战，通过结合三种创新技术显著提高了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;当只有少量标记试验可用时，从脑电图(EEG)中检测单次试验P300是困难的。当尝试通过迁移学习用大型源数据集增强小型目标集时，会出现跨数据集偏移。为应对这一挑战，我们在严格的小样本设置下(目标:每个受试者10次试验；源:每个受试者80次试验)，研究了使用五个共享电极(Fz, Pz, P3, P4, Oz)在两个公共视觉oddball ERP数据集之间的迁移学习。我们引入了自适应分割最大均值差异训练(AS-MMD)，它结合了(i)与源/目标大小比值的平方根相关的目标加权损失和预热，(ii)具有共享仿射参数和每域运行统计的分割批量归一化(Split-BN)，以及(iii)使用中带带宽启发式的无参数对数级径向基函数核最大均值差异(RBF-MMD)项。在EEG Conformer上实现后，AS-MMD与主干网络无关且保持推理时模型不变。在两种迁移方向上，它都优于仅目标训练和联合训练(Active Visual Oddball: 准确率/AUC为0.66/0.74；ERP CORE P3: 0.61/0.65)，与联合训练相比的增益在校正后的配对t检验下显著。消融研究将改进归因于所有三个组件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting single-trial P300 from EEG is difficult when only a few labeledtrials are available. When attempting to boost a small target set with a largesource dataset through transfer learning, cross-dataset shift arises. Toaddress this challenge, we study transfer between two public visual-oddball ERPdatasets using five shared electrodes (Fz, Pz, P3, P4, Oz) under a strictsmall-sample regime (target: 10 trials/subject; source: 80 trials/subject). Weintroduce Adaptive Split Maximum Mean Discrepancy Training (AS-MMD), whichcombines (i) a target-weighted loss with warm-up tied to the square root of thesource/target size ratio, (ii) Split Batch Normalization (Split-BN) with sharedaffine parameters and per-domain running statistics, and (iii) a parameter-freelogit-level Radial Basis Function kernel Maximum Mean Discrepancy (RBF-MMD)term using the median-bandwidth heuristic. Implemented on an EEG Conformer,AS-MMD is backbone-agnostic and leaves the inference-time model unchanged.Across both transfer directions, it outperforms target-only and pooled training(Active Visual Oddball: accuracy/AUC 0.66/0.74; ERP CORE P3: 0.61/0.65), withgains over pooling significant under corrected paired t-tests. Ablationsattribute improvements to all three components.</description>
      <author>example@mail.com (Weiyu Chen, Arnaud Delorme)</author>
      <guid isPermaLink="false">2510.21969v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>An unsupervised tour through the hidden pathways of deep neural networks</title>
      <link>http://arxiv.org/abs/2510.21582v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究旨在深入理解深度人工神经网络创建有意义表示并实现泛化的内部机制。研究重点关注使用无监督学习工具描述隐藏表示的语义内容，并利用数据的低维结构。论文介绍了Gride方法用于估计数据内在维度，研究了深度神经网络中隐藏层概率密度的演变，以及探讨了深度神经网络中的泛化问题。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络虽然取得了显著成功，但其内部工作机制和泛化能力仍不完全清楚。理解神经网络如何创建有意义的表示以及它们如何能够泛化到未见数据是深度学习领域的关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提高对深度人工神经网络创建有意义表示和泛化能力的内部机制的理解。重点在于使用无监督学习工具描述隐藏表示的语义内容，并利用数据的低维结构。&lt;h4&gt;方法&lt;/h4&gt;1. 开发了Gride方法，用于估计数据内在维度作为尺度的显式函数，无需降采样数据集。2. 研究了最先进深度神经网络中隐藏层概率密度的演变。3. 研究了深度神经网络中的泛化问题，特别是添加参数如何提高泛化性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. Gride方法基于严格的分布结果，能够量化估计的不确定性，且计算效率高。2. 深度神经网络的初始层生成单模态概率密度，消除与分类无关的结构；后续层中密度峰以分层方式出现，反映概念的语义层次。3. 宽神经网络学习冗余表示而非对虚假相关性过拟合，冗余神经元仅在网络被正则化且训练误差为零时出现。&lt;h4&gt;结论&lt;/h4&gt;深度神经网络通过分层结构创建有意义的表示，初始层消除无关结构，后续层建立语义层次。网络的泛化能力与冗余表示学习相关，而非传统的偏差-方差权衡。Gride方法为分析数据内在结构提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文的目的是提高我们对深度人工神经网络创建有意义表示并能够泛化的内部机制的理解。我们专注于使用无监督学习工具描述隐藏表示的语义内容的挑战，这些工具部分由我们开发并在本论文中描述，它们允许利用数据的低维结构。第2章介绍了Gride，一种允许将数据的内在维度估计为尺度的显式函数的方法，而无需对数据集进行任何降采样。我们的方法基于严格的分布结果，能够量化估计的不确定性。此外，我们的方法简单且计算高效，因为它仅依赖于最近数据点之间的距离。在第3章中，我们研究了一些最先进的深度神经网络中隐藏层概率密度的演变。我们发现初始层生成单模态概率密度，消除任何与分类无关的结构。在后续层中，密度峰以分层方式出现，反映概念的语义层次结构。这个过程在输出层的概率密度中留下了足迹，其中峰的地形可以重建类别的语义关系。在第4章中，我们研究了深度神经网络中的泛化问题：向插值其训练数据的网络添加参数通常会提高其泛化性能，这与经典的偏差-方差权衡相悖。我们证明宽神经网络学习冗余表示，而不是对虚假相关性过拟合，并且只有当网络被正则化且训练误差为零时，冗余神经元才会出现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of this thesis is to improve our understanding of the internalmechanisms by which deep artificial neural networks create meaningfulrepresentations and are able to generalize. We focus on the challenge ofcharacterizing the semantic content of the hidden representations withunsupervised learning tools, partially developed by us and described in thisthesis, which allow harnessing the low-dimensional structure of the data.Chapter 2. introduces Gride, a method that allows estimating the intrinsicdimension of the data as an explicit function of the scale without performingany decimation of the data set. Our approach is based on rigorousdistributional results that enable the quantification of uncertainty of theestimates. Moreover, our method is simple and computationally efficient sinceit relies only on the distances among nearest data points. In Chapter 3, westudy the evolution of the probability density across the hidden layers in somestate-of-the-art deep neural networks. We find that the initial layers generatea unimodal probability density getting rid of any structure irrelevant toclassification. In subsequent layers, density peaks arise in a hierarchicalfashion that mirrors the semantic hierarchy of the concepts. This processleaves a footprint in the probability density of the output layer, where thetopography of the peaks allows reconstructing the semantic relationships of thecategories. In Chapter 4, we study the problem of generalization in deep neuralnetworks: adding parameters to a network that interpolates its training datawill typically improve its generalization performance, at odds with theclassical bias-variance trade-off. We show that wide neural networks learnredundant representations instead of overfitting to spurious correlation andthat redundant neurons appear only if the network is regularized and thetraining error is zero.</description>
      <author>example@mail.com (Diego Doimo)</author>
      <guid isPermaLink="false">2510.21582v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Cost-Sensitive Freeze-thaw Bayesian Optimization for Efficient Hyperparameter Tuning</title>
      <link>http://arxiv.org/abs/2510.21379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于冻结-解冻贝叶斯优化的成本敏感超参数优化方法，通过引入效用函数、新的获取函数和停止准则，实现了在成本和性能之间的动态权衡，并通过迁移学习提高了样本效率。&lt;h4&gt;背景&lt;/h4&gt;研究基于冻结-解冻贝叶斯优化的成本敏感超参数优化问题，关注用户在预期性能改进相对于额外计算成本不够满意时提前停止HPO过程的场景。&lt;h4&gt;目的&lt;/h4&gt;引入描述成本与性能之间权衡的效用函数，结合新的获取函数和停止准则，动态选择最优配置并自动停止HPO过程，同时通过迁移学习提高样本效率。&lt;h4&gt;方法&lt;/h4&gt;提出成本敏感HPO方法，引入效用函数，设计新的获取函数和停止准则，使用迁移学习开发专门的代理模型，提高冻结-解冻方法的样本效率。&lt;h4&gt;主要发现&lt;/h4&gt;在多保真度HPO基准测试上验证了算法性能，优于所有考虑的冻结-解冻BO和迁移-BO基线方法，实现了成本和性能之间显著更好的权衡。&lt;h4&gt;结论&lt;/h4&gt;所提方法在成本敏感HPO问题上表现出色，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们解决了基于冻结-解冻贝叶斯优化（BO）的成本敏感超参数优化（HPO）问题。具体而言，我们假设一种场景，即当预期性能改进相对于额外计算成本不够令人满意时，用户希望提前停止HPO过程。受此场景启发，我们在冻结-解冻框架中引入了'效用'，这是一个描述成本与性能之间权衡的函数，可以从用户偏好数据中估计。这个效用函数结合我们新的获取函数和停止准则，使我们能够动态继续训练我们预期未来效用最大化的配置，并在效用最大值附近自动停止HPO过程。此外，我们通过迁移学习改进了现有冻结-解冻方法的样本效率，为成本敏感HPO问题开发了专门的代理模型。我们在既定的多保真度HPO基准上验证了我们的算法，并表明它优于我们考虑的所有先前冻结-解冻BO和迁移-BO基线方法，同时实现了成本和性能之间显著更好的权衡。我们的代码已在https://github.com/db-Lee/CFBO公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we address the problem of \emph{cost-sensitive} hyperparameteroptimization (HPO) built upon freeze-thaw Bayesian optimization (BO).Specifically, we assume a scenario where users want to early-stop the HPOprocess when the expected performance improvement is not satisfactory withrespect to the additional computational cost. Motivated by this scenario, weintroduce \emph{utility} in the freeze-thaw framework, a function describingthe trade-off between the cost and performance that can be estimated from theuser's preference data. This utility function, combined with our novelacquisition function and stopping criterion, allows us to dynamically continuetraining the configuration that we expect to maximally improve the utility inthe future, and also automatically stop the HPO process around the maximumutility. Further, we improve the sample efficiency of existing freeze-thawmethods with transfer learning to develop a specialized surrogate model for thecost-sensitive HPO problem. We validate our algorithm on establishedmulti-fidelity HPO benchmarks and show that it outperforms all the previousfreeze-thaw BO and transfer-BO baselines we consider, while achieving asignificantly better trade-off between the cost and performance. Our code ispublicly available at https://github.com/db-Lee/CFBO.</description>
      <author>example@mail.com (Dong Bok Lee, Aoxuan Silvia Zhang, Byungjoo Kim, Junhyeon Park, Steven Adriaensen, Juho Lee, Sung Ju Hwang, Hae Beom Lee)</author>
      <guid isPermaLink="false">2510.21379v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>$α$-LoRA: Effective Fine-Tuning via Base Model Rescaling</title>
      <link>http://arxiv.org/abs/2510.21345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一类新的用于迁移学习的重参数化方法，旨在提高微调模型的泛化能力，并通过理论分析和实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;微调被证明是使预训练模型在少量数据样本上在新任务上表现更好的有效方法，其中重参数化方法是最广泛使用的方法之一。&lt;h4&gt;目的&lt;/h4&gt;设计一类新的重参数化方法，以增强微调模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出一类新的重参数化方法，在高维二分类设置中使用随机矩阵理论工具建立其有效性，并通过微调大型语言模型等实验进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的重参数化方法在高维二分类任务中表现有效，且通过微调LLMs的实验进一步验证了理论发现。&lt;h4&gt;结论&lt;/h4&gt;新提出的重参数化方法能够有效提高微调模型的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;微调已被证明是使预训练模型在少量数据样本上在新任务上表现更好的有效方法。其中最广泛使用的方法是重参数化方法，它们通过添加一个额外的可训练权重矩阵来更新目标模块的冻结权重矩阵。最突出的例子是低秩适应(LoRA)，近年来受到了广泛关注。在本文中，我们介绍了一类用于迁移学习的新型重参数化方法，旨在提高微调模型的泛化能力。我们使用随机矩阵理论工具在高维二分类设置中建立了该方法的有效性，并通过更真实的实验（如微调大型语言模型）进一步验证了我们的理论发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning has proven to be highly effective in adapting pre-trained modelsto perform better on new desired tasks with minimal data samples. Among themost widely used approaches are reparameterization methods, which update atarget module by augmenting its frozen weight matrix with an additionaltrainable weight matrix. The most prominent example is Low Rank Adaption(LoRA), which gained significant attention in recent years. In this paper, weintroduce a new class of reparameterization methods for transfer learning,designed to enhance the generalization ability of fine-tuned models. Weestablish the effectiveness of our approach in a high-dimensional binaryclassification setting using tools from Random Matrix Theory, and furthervalidate our theoretical findings through more realistic experiments, such asfine-tuning LLMs.</description>
      <author>example@mail.com (Aymane El Firdoussi, El Mahdi Chayti, Mohamed El Amine Seddik, Martin Jaggi)</author>
      <guid isPermaLink="false">2510.21345v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Graph Mixture of Residual Experts: Unsupervised Learning on Diverse Graphs with Heterogeneous Specialization</title>
      <link>http://arxiv.org/abs/2510.21207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出ADaMoRE框架，解决了图神经网络在适应多样化图结构方面的挑战，通过无监督训练实现了异构专家的有效组合，在各种任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;图神经网络面临基本适应性挑战：固定的消息传递架构难以应对现实世界图的巨大多样性，最优计算策略因局部结构和任务而异。现有图专家混合方法依赖监督信号且训练异构专家时存在不稳定性。&lt;h4&gt;目的&lt;/h4&gt;引入ADaMoRE框架，实现在图上进行异构专家混合的稳健、完全无监督训练。&lt;h4&gt;方法&lt;/h4&gt;ADaMoRE采用骨干-残差专家架构，基础编码器提供稳定性，残差专家捕获不同计算模式；结构感知门控网络执行细粒度节点路由；通过统一无监督目标进行端到端训练，结合重建任务和信息论多样性正则化器强制专家功能专业化。&lt;h4&gt;主要发现&lt;/h4&gt;在16个基准测试上验证了ADaMoRE在无监督节点分类和少样本学习方面的最先进性能，以及优越的泛化能力、训练效率和更快收敛速度。&lt;h4&gt;结论&lt;/h4&gt;ADaMoRE框架通过无监督训练有效解决了图神经网络适应性问题，在多样化图和任务上展现出卓越性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)面临一个基本的适应性挑战：它们固定的消息传递架构难以应对现实世界图的巨大多样性，而最优的计算策略因局部结构和任务的不同而异。尽管专家混合(MoE)为适应性提供了一条有前景的路径，但现有的图MoE方法仍然依赖于监督信号，并且在训练异构专家时存在不稳定性。我们引入ADaMoRE(Adaptive Mixture of Residual Experts)，这是一个原则性框架，能够在图上实现异构MoE的稳健、完全无监督训练。ADaMoRE采用骨干-残差专家架构，其中基础编码器提供稳定性，而专门的残差专家捕获不同的计算模式。一个结构感知的门控网络执行细粒度的节点路由。整个架构通过统一的无监督目标进行端到端训练，该目标结合了主要的重建任务和信息论多样性正则化器，以明确强制专家之间的功能专业化。理论分析证实了他们的设计提高了数据效率和训练稳定性。在16个基准测试上的广泛评估验证了ADaMoRE在无监督节点分类和少样本学习方面的最先进性能，以及在多样化图和任务上的优越泛化能力、训练效率和更快收敛速度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) face a fundamental adaptability challenge: theirfixed message-passing architectures struggle with the immense diversity ofreal-world graphs, where optimal computational strategies vary by localstructure and task. While Mixture-of-Experts (MoE) offers a promising pathwayto adaptability, existing graph MoE methods remain constrained by theirreliance on supervised signals and instability when training heterogeneousexperts. We introduce ADaMoRE (Adaptive Mixture of Residual Experts), aprincipled framework that enables robust, fully unsupervised training ofheterogeneous MoE on graphs. ADaMoRE employs a backbone-residual expertarchitecture where foundational encoders provide stability while specializedresidual experts capture diverse computational patterns. A structurally-awaregating network performs fine-grained node routing. The entire architecture istrained end-to-end using a unified unsupervised objective, which integrates aprimary reconstruction task with an information-theoretic diversity regularizerto explicitly enforce functional specialization among the experts. Theoreticalanalysis confirms our design improves data efficiency and training stability.Extensive evaluation across 16 benchmarks validates ADaMoRE's state-of-the-artperformance in unsupervised node classification and few-shot learning,alongside superior generalization, training efficiency, and faster convergenceon diverse graphs and tasks.</description>
      <author>example@mail.com (Yunlong Chu, Minglai Shao, Zengyi Wo, Bing Hao, Yuhang Liu, Ruijie Wang, Jianxin Li)</author>
      <guid isPermaLink="false">2510.21207v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>CIPHER: Scalable Time Series Analysis for Physical Sciences with Application to Solar Wind Phenomena</title>
      <link>http://arxiv.org/abs/2510.21022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures, Machine Learning and the Physical Sciences  Workshop @ NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为CIPHER的框架，用于加速物理学中复杂时间序列的大规模标注。该框架结合了可索引符号聚合近似、基于密度的聚类和人类专家验证，解决了物理科学中时间序列标注稀缺、成本高且不一致的问题。&lt;h4&gt;背景&lt;/h4&gt;在物理科学中，时间序列的标注或分类是一个持续的挑战。专家标注稀缺、成本高且往往不一致，但稳健的标注对于启用机器学习模型进行理解、预测和预测至关重要。&lt;h4&gt;目的&lt;/h4&gt;设计一个框架来加速物理学中复杂时间序列的大规模标注，解决专家标注稀缺的问题。&lt;h4&gt;方法&lt;/h4&gt;CIPHER框架集成了以下组件：1. 可索引符号聚合近似用于可解释的压缩和索引；2. 基于密度的聚类来分组重复出现的现象；3. 人类在环中的步骤用于高效的专家验证。领域科学家对代表性样本进行标注，然后将这些标注传播到整个集群中，产生系统化的、可扩展的分类。&lt;h4&gt;主要发现&lt;/h4&gt;作者在OMNI数据中分类太阳风现象的任务上评估了CIPHER，这是空间天气研究中的一个核心挑战。结果表明，该框架能够识别有意义的现象，如日冕物质抛射和流相互作用区域。&lt;h4&gt;结论&lt;/h4&gt;CIPHER展示了一种结合符号表示、无监督学习和专业知识的通用策略，以解决物理科学中时间序列标注稀缺的问题。研究所用的代码和配置文件是公开的，以支持可重复性。&lt;h4&gt;翻译&lt;/h4&gt;时间序列的标注或分类在物理科学中是一个持续的挑战，其中专家标注稀缺、成本高昂且往往不一致。然而，稳健的标注对于启用机器学习模型进行理解、预测和预测至关重要。我们提出了'聚类与索引管道及人类评估用于识别'，这是一个旨在加速物理学中复杂时间序列大规模标注的框架。CIPHER集成了可索引符号聚合近似用于可解释的压缩和索引，基于密度的聚类来分组重复出现的现象，以及一个人机交互的步骤用于高效的专家验证。代表性样本由领域科学家标注，这些标注被传播到整个集群中，产生系统化、可扩展的分类。我们在OMNI数据中分类太阳风现象的任务上评估了CIPHER，这是空间天气研究中的一个核心挑战，结果表明该框架能够识别有意义的现象，如日冕物质抛射和流相互作用区域。除了这个案例研究，CIPHER强调了一种结合符号表示、无监督学习和专业知识的通用策略，以解决物理科学中时间序列的标注稀缺问题。本研究使用的代码和配置文件是公开的，以支持可重复性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Labeling or classifying time series is a persistent challenge in the physicalsciences, where expert annotations are scarce, costly, and often inconsistent.Yet robust labeling is essential to enable machine learning models forunderstanding, prediction, and forecasting. We present the \textit{Clusteringand Indexation Pipeline with Human Evaluation for Recognition} (CIPHER), aframework designed to accelerate large-scale labeling of complex time series inphysics. CIPHER integrates \textit{indexable Symbolic Aggregate approXimation}(iSAX) for interpretable compression and indexing, density-based clustering(HDBSCAN) to group recurring phenomena, and a human-in-the-loop step forefficient expert validation. Representative samples are labeled by domainscientists, and these annotations are propagated across clusters to yieldsystematic, scalable classifications. We evaluate CIPHER on the task ofclassifying solar wind phenomena in OMNI data, a central challenge in spaceweather research, showing that the framework recovers meaningful phenomena suchas coronal mass ejections and stream interaction regions. Beyond this casestudy, CIPHER highlights a general strategy for combining symbolicrepresentations, unsupervised learning, and expert knowledge to address labelscarcity in time series across the physical sciences. The code andconfiguration files used in this study are publicly available to supportreproducibility.</description>
      <author>example@mail.com (Jasmine R. Kobayashi, Daniela Martin, Valmir P Moraes Filho, Connor O'Brien, Jinsu Hong, Sudeshna Boro Saikia, Hala Lamdouar, Nathan D. Miles, Marcella Scoczynski, Mavis Stone, Sairam Sundaresan, Anna Jungbluth, Andrés Muñoz-Jaramillo, Evangelia Samara, Joseph Gallego)</author>
      <guid isPermaLink="false">2510.21022v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Memory Constrained Dynamic Subnetwork Update for Transfer Learning</title>
      <link>http://arxiv.org/abs/2510.20979v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MeDyate的框架，用于解决在设备上神经网络训练面临的内存限制问题，通过动态子网络适应方法实现了在严格内存预算下的有效微调。&lt;h4&gt;背景&lt;/h4&gt;在设备上的神经网络训练面临严重的内存限制，这些限制阻碍了预训练模型对下游任务的适应。&lt;h4&gt;目的&lt;/h4&gt;提出一个有理论依据的框架，用于内存受限的动态子网络适应，实现在严格内存预算下的有效微调。&lt;h4&gt;方法&lt;/h4&gt;MeDyate框架包含两个关键创新：LaRa（Layer Ranking）作为改进的层重要性度量实现有原则的层预选择，以及动态通道采样策略利用微调过程中通道重要性分布的时间稳定性；根据重要性加权概率在周期之间动态重新采样通道，确保在尊重内存预算的同时全面探索参数空间。&lt;h4&gt;主要发现&lt;/h4&gt;在广泛的任务和架构上进行的大量评估表明，MeDyate在极端内存限制下实现了最先进的性能，一致优于现有的静态和动态方法，同时保持高计算效率。&lt;h4&gt;结论&lt;/h4&gt;该方法代表了推动设备上高效学习的重要一步，证明了在内存预算低至几百KB RAM的情况下进行有效微调的可能性。&lt;h4&gt;翻译&lt;/h4&gt;设备上的神经网络训练面临关键的内存限制，这些限制阻碍了预训练模型对下游任务的适应。我们提出了MeDyate，一个有理论依据的框架，用于内存受限的动态子网络适应。我们的方法引入了两个关键创新：LaRa（Layer Ranking），一种改进的层重要性度量，能够实现有原则的层预选择，以及动态通道采样策略，利用微调过程中通道重要性分布的时间稳定性。MeDyate根据重要性加权概率在周期之间动态重新采样通道，确保在尊重严格内存预算的同时全面探索参数空间。在广泛的任务和架构上进行的大量评估表明，MeDyate在极端内存限制下实现了最先进的性能，一致优于现有的静态和动态方法，同时保持高计算效率。我们的方法代表了推动设备上高效学习的重要一步，证明了在内存预算低至几百KB RAM的情况下进行有效微调的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; On-device neural network training faces critical memory constraints thatlimit the adaptation of pre-trained models to downstream tasks. We presentMeDyate, a theoretically-grounded framework for memory-constrained dynamicsubnetwork adaptation. Our approach introduces two key innovations: LaRa (LayerRanking), an improved layer importance metric that enables principled layerpre-selection, and a dynamic channel sampling strategy that exploits thetemporal stability of channel importance distributions during fine-tuning.MeDyate dynamically resamples channels between epochs according toimportance-weighted probabilities, ensuring comprehensive parameter spaceexploration while respecting strict memory budgets. Extensive evaluation acrossa large panel of tasks and architectures demonstrates that MeDyate achievesstate-of-the-art performance under extreme memory constraints, consistentlyoutperforming existing static and dynamic approaches while maintaining highcomputational efficiency. Our method represents a significant step towardsenabling efficient on-device learning by demonstrating effective fine-tuningwith memory budgets as low as a few hundred kB of RAM.</description>
      <author>example@mail.com (Aël Quélennec, Pavlo Mozharovskyi, Van-Tam Nguyen, Enzo Tartaglione)</author>
      <guid isPermaLink="false">2510.20979v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes</title>
      <link>http://arxiv.org/abs/2510.23151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种自适应门控融合方法，通过选择性整合跨模态知识，在复杂场景中实现了更鲁棒的3D目标检测。&lt;h4&gt;背景&lt;/h4&gt;多模态相机-激光雷达融合技术在3D目标检测中应用广泛，但在传感器退化或环境干扰等具有挑战性的场景中，现有方法性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自适应门控融合方法，通过识别可靠的模式来选择性地整合跨模态知识，以实现复杂场景中的鲁棒检测。&lt;h4&gt;方法&lt;/h4&gt;将每个模态的特征投影到统一的BEV空间并使用基于窗口的注意力机制增强特征，然后设计基于跨模态注意力的自适应门控融合模块来整合这些特征，同时构建了一个名为Excavator3D（E3D）的新数据集，专注于具有挑战性的挖掘机操作场景。&lt;h4&gt;主要发现&lt;/h4&gt;在标准的KITTI数据集上达到93.92%的准确率，在具有挑战性的E3D数据集上比基线方法高出24.88%，证明了在复杂工业场景中对不可靠模态信息具有优越的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;提出的AG-Fusion方法在复杂场景中表现优异，特别是在处理不可靠模态信息时具有更强的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;多模态相机-激光雷达融合技术在3D目标检测中已得到广泛应用，并显示出令人鼓舞的性能表现。然而，在传感器退化或环境干扰等具有挑战性的场景中，现有方法表现出显著的性能下降。我们提出了一种新颖的自适应门控融合方法，通过识别可靠的模式来选择性地整合跨模态知识，从而在复杂场景中实现鲁棒检测。具体而言，我们首先将每个模态的特征投影到统一的BEV空间，并使用基于窗口的注意力机制增强这些特征。随后，我们设计了一个基于跨模态注意力的自适应门控融合模块，将这些特征整合为可靠的BEV表示，以应对具有挑战性的环境。此外，我们构建了一个名为Excavator3D（E3D）的新数据集，专注于具有挑战性的挖掘机操作场景，以在复杂条件下评估性能。我们的方法不仅在标准的KITTI数据集上实现了93.92%的准确率，具有竞争力的性能，而且在具有挑战性的E3D数据集上比基线方法高出24.88%，证明了在复杂工业场景中对不可靠模态信息具有优越的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态相机-激光雷达融合技术在复杂场景（如挖掘机操作环境）中性能显著下降的问题。这些问题在现实中很重要，因为灰尘、光照变化导致图像退化，机械部件遮挡和金属表面反射干扰点云数据，这些挑战限制了自动驾驶和工业自动化技术在真实世界中的应用，现有方法在标准数据集上表现良好但在复杂场景中性能大幅下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有BEV融合技术的局限性，指出它们主要依赖卷积操作，无法自适应建模跨模态交互。针对工业场景中的挑战，作者在BEVFusion基础上进行改进，借鉴了Swin Transformer的窗口自注意力机制设计SA-E模块增强特征，并引入双向交叉注意力和自适应门控机制实现更智能的融合。整体设计思路是根据场景特点动态调整不同模态的贡献权重。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自适应选择和融合来自相机和激光雷达的可靠信息，提高在复杂场景中的3D目标检测性能。方法首先使用窗口自注意力增强每个模态的特征；然后通过双向交叉注意力和自适应门控机制融合这些特征，根据场景特点动态调整不同模态的贡献权重；最后将所有特征流集成到统一的BEV表示中进行3D检测。整体流程包括特征提取、增强特征提取、跨模态门控融合和多级特征聚合四个主要步骤。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 自适应门控多模态融合(AG-Fusion)框架，结合双向交叉注意力和空间自适应门控机制；2) 基于窗口的自注意力增强(SA-E)模块，有效降低计算复杂度；3) 构建了专门的挖掘机3D检测数据集(E3D)。相比之前的工作，该方法不再依赖静态或局部约束的特征聚合，能够处理遮挡和传感器噪声，在复杂工业场景中表现显著优于现有方法，在E3D数据集上比基线方法提升24.88%的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种自适应门控多模态融合方法，通过动态整合相机和激光雷达的可靠信息，显著提升了在复杂工业场景中的3D目标检测性能，并构建了专门的挖掘机3D检测数据集验证方法的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal camera-LiDAR fusion technology has found extensive application in3D object detection, demonstrating encouraging performance. However, existingmethods exhibit significant performance degradation in challenging scenarioscharacterized by sensor degradation or environmental disturbances. We propose anovel Adaptive Gated Fusion (AG-Fusion) approach that selectively integratescross-modal knowledge by identifying reliable patterns for robust detection incomplex scenes. Specifically, we first project features from each modality intoa unified BEV space and enhance them using a window-based attention mechanism.Subsequently, an adaptive gated fusion module based on cross-modal attention isdesigned to integrate these features into reliable BEV representations robustto challenging environments. Furthermore, we construct a new dataset namedExcavator3D (E3D) focusing on challenging excavator operation scenarios tobenchmark performance in complex conditions. Our method not only achievescompetitive performance on the standard KITTI dataset with 93.92% accuracy, butalso significantly outperforms the baseline by 24.88% on the challenging E3Ddataset, demonstrating superior robustness to unreliable modal information incomplex industrial scenes.</description>
      <author>example@mail.com (Sixian Liu, Chen Xu, Qiang Wang, Donghai Shi, Yiwen Li)</author>
      <guid isPermaLink="false">2510.23151v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in Traffic Scenarios</title>
      <link>http://arxiv.org/abs/2510.23144v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个名为DQ3D的深度引导查询生成器，用于解决3D目标检测中的参考点采样问题，并通过混合注意力机制处理部分遮挡目标，在nuScenes数据集上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;多视角图像中的3D目标检测在交通场景中近年来受到广泛关注。现有方法依赖于从3D参考点生成的目标查询来定位物体。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中参考点可能远离目标物体导致误检的问题，并处理当前帧中部分遮挡的目标物体。&lt;h4&gt;方法&lt;/h4&gt;提出了深度引导查询生成器(DQ3D)，利用深度信息和2D检测确保参考点从物体表面或内部采样；引入混合注意力机制，将历史检测结果与深度引导查询融合，形成混合查询。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的评估表明，该方法在平均精度(mAP)上比基线提高了6.3%，在NuScenes检测分数(NDS)上提高了4.3%。&lt;h4&gt;结论&lt;/h4&gt;深度引导查询生成器和混合注意力机制能有效提高3D目标检测的性能，特别是在处理参考点采样和遮挡物体方面。&lt;h4&gt;翻译&lt;/h4&gt;近年来，交通场景中基于多视角图像的3D目标检测受到了广泛关注。许多现有方法依赖于从3D参考点生成的目标查询来定位物体。然而，这些方法的一个局限性是，一些参考点通常远离目标物体，这可能导致误检。在本文中，我们提出了一个用于3D目标检测的深度引导查询生成器(DQ3D)，它利用深度信息和2D检测确保参考点从物体表面或内部采样。此外，为了解决当前帧中部分遮挡的物体，我们引入了一种混合注意力机制，将历史检测结果与深度引导查询融合，从而形成混合查询。在nuScenes数据集上的评估表明，我们的方法在平均精度(mAP)上比基线提高了6.3%，在NuScenes检测分数(NDS)上提高了4.3%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection from multi-view images in traffic scenarios has garneredsignificant attention in recent years. Many existing approaches rely on objectqueries that are generated from 3D reference points to localize objects.However, a limitation of these methods is that some reference points are oftenfar from the target object, which can lead to false positive detections. Inthis paper, we propose a depth-guided query generator for 3D object detection(DQ3D) that leverages depth information and 2D detections to ensure thatreference points are sampled from the surface or interior of the object.Furthermore, to address partially occluded objects in current frame, weintroduce a hybrid attention mechanism that fuses historical detection resultswith depth-guided queries, thereby forming hybrid queries. Evaluation on thenuScenes dataset demonstrates that our method outperforms the baseline by 6.3\%in terms of mean Average Precision (mAP) and 4.3\% in the NuScenes DetectionScore (NDS).</description>
      <author>example@mail.com (Ziyu Wang, Wenhao Li, Ji Wu)</author>
      <guid isPermaLink="false">2510.23144v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation</title>
      <link>http://arxiv.org/abs/2509.16500v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了自动驾驶系统中合成视频数据的几何失真问题，提出了强化学习与几何反馈(RLGF)方法，显著提高了合成数据的几何准确性和3D目标检测性能。&lt;h4&gt;背景&lt;/h4&gt;合成数据对推进自动驾驶系统至关重要，但当前最先进的视频生成模型尽管视觉上逼真，却存在微妙的几何失真，限制了其在下游感知任务中的应用。研究显示，使用合成数据与真实数据进行3D目标检测时存在显著性能差距。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来减少合成视频数据中的几何失真，提高其在自动驾驶感知任务中的效用，缩小合成数据与真实数据之间的性能差距。&lt;h4&gt;方法&lt;/h4&gt;研究引入了'带有几何反馈的强化学习'(RLGF)，该方法通过整合来自专用潜在空间自动驾驶感知模型的奖励来优化视频扩散模型。其核心组件包括：1) 潜在空间窗口优化技术，用于在扩散过程中提供针对性反馈；2) 分层几何奖励(HGR)系统，为点线面对齐和场景占用一致性提供多级奖励。研究还提出了GeoScores来量化几何失真。&lt;h4&gt;主要发现&lt;/h4&gt;应用RLGF到DiVE模型上，在nuScenes数据集上显著减少了几何误差（例如：消失点误差降低21%，深度误差降低57%），并大幅提高了3D目标检测mAP达12.7%，缩小了与真实数据性能的差距。&lt;h4&gt;结论&lt;/h4&gt;RLGF为自动驾驶开发提供了一种即插即用的解决方案，能够生成几何准确可靠的合成视频，有助于推进自动驾驶系统的训练和测试。&lt;h4&gt;翻译&lt;/h4&gt;合成数据对推进自动驾驶系统至关重要，但当前最先进的视频生成模型尽管视觉上逼真，却存在微妙的几何失真，限制了其在下游感知任务中的应用。研究确定了并量化了这一关键问题，展示了使用合成数据与真实数据进行3D目标检测时的显著性能差距。为此，研究引入了'带有几何反馈的强化学习'(RLGF)。RLGF通过整合来自专用潜在空间自动驾驶感知模型的奖励，独特地优化了视频扩散模型。其核心组件包括：1) 用于在扩散过程中提供针对性反馈的高效潜在空间窗口优化技术；2) 提供点线面对齐和场景占用一致性多级奖励的分层几何奖励(HGR)系统。为了量化这些失真，研究提出了GeoScores。将RLGF应用于nuScenes上的DiVE等模型，显著减少了几何误差（例如：消失点误差降低21%，深度误差降低57%），并大幅提高了3D目标检测mAP达12.7%，缩小了与真实数据性能的差距。RLGF为生成几何准确可靠的合成视频用于自动驾驶开发提供了一种即插即用的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶视频生成中存在的几何失真问题。虽然当前视频生成模型在视觉上看起来很真实，但它们生成的视频中存在微妙的几何扭曲，这些扭曲限制了它们在下游感知任务中的实用性。这个问题很重要，因为自动驾驶系统需要大量高质量的合成数据进行训练和验证，而这些几何扭曲会导致基于合成数据训练的3D目标检测等下游任务性能显著下降，影响自动驾驶系统的可靠性和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别并量化了当前视频生成模型中的几何失真问题，提出了GeoScores指标来评估这些失真。通过实验发现当前模型虽然保留了2D外观但无法捕捉准确的3D场景结构，主要原因是潜在的几何不一致性。作者借鉴了强化学习从人类反馈的成功经验（如LLMs中的PPO或DPO）、视频扩散模型的研究以及自动驾驶感知模型的设计。但作者指出现有方法主要依赖像素级对齐，无法显式强制遵守复杂的底层几何原理，因此设计了RLGF框架，利用专门的预训练自动驾驶感知模型作为奖励提供者，确保几何保真度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入基于感知模型的几何反馈来增强视频扩散模型的几何完整性，使生成的自动驾驶场景视频不仅在视觉上逼真，而且在几何结构上准确可靠。整体流程包括：1)预训练两个专门的感知模型（潜在几何感知模型Pgeo和潜在占用预测模型Pocc）；2)设计分层几何奖励（HGR）系统，提供点线面几何反馈和场景级占用反馈；3)实现潜在空间窗口化优化，在扩散过程的中间步骤提供反馈；4)使用强化学习微调预训练的视频扩散模型，将感知模型作为奖励提供者，通过LoRA高效更新模型参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次系统量化了自动驾驶视频生成中的几何失真问题，提出GeoScores评估指标；2)引入强化学习与几何反馈（RLGF）框架，将感知模型驱动的几何空间约束直接注入视频生成过程；3)提出潜在空间窗口化优化技术，在扩散过程的中间步骤而非仅最终输出提供反馈；4)设计分层几何奖励（HGR）系统，结合点线面几何反馈和场景级占用反馈。相比之前的工作，RLGF专注于几何完整性而非仅像素级视觉保真度，使用具体的、可解释的几何约束而非人类偏好或高层次奖励信号，并且是即插即用的解决方案，可与现有视频扩散模型集成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了RLGF框架，通过引入基于感知模型的几何反馈强化学习，有效解决了自动驾驶视频生成中的几何失真问题，显著提升了合成数据的3D感知任务性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic data is crucial for advancing autonomous driving (AD) systems, yetcurrent state-of-the-art video generation models, despite their visual realism,suffer from subtle geometric distortions that limit their utility fordownstream perception tasks. We identify and quantify this critical issue,demonstrating a significant performance gap in 3D object detection when usingsynthetic versus real data. To address this, we introduce ReinforcementLearning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusionmodels by incorporating rewards from specialized latent-space AD perceptionmodels. Its core components include an efficient Latent-Space WindowingOptimization technique for targeted feedback during diffusion, and aHierarchical Geometric Reward (HGR) system providing multi-level rewards forpoint-line-plane alignment, and scene occupancy coherence. To quantify thesedistortions, we propose GeoScores. Applied to models like DiVE on nuScenes,RLGF substantially reduces geometric errors (e.g., VP error by 21\%, Deptherror by 57\%) and dramatically improves 3D object detection mAP by 12.7\%,narrowing the gap to real-data performance. RLGF offers a plug-and-playsolution for generating geometrically sound and reliable synthetic videos forAD development.</description>
      <author>example@mail.com (Tianyi Yan, Wencheng Han, Xia Zhou, Xueyang Zhang, Kun Zhan, Cheng-zhong Xu, Jianbing Shen)</author>
      <guid isPermaLink="false">2509.16500v2</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning</title>
      <link>http://arxiv.org/abs/2510.23532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted at NeurIPS 2025 D&amp;B track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了NoRA新基准测试，用于评估系统关系推理模型的泛化能力，突破了现有基于路径组合的简化假设。&lt;h4&gt;背景&lt;/h4&gt;设计能够系统化学习的模型是重要挑战，近年已提出多种解决方案，包括神经符号方法、Transformer变体和图神经网络，但现有基准测试过于简化。&lt;h4&gt;目的&lt;/h4&gt;支持神经网络在系统关系推理领域的进一步发展，引入更全面的评估基准。&lt;h4&gt;方法&lt;/h4&gt;开发NoRA基准测试，增加多个难度级别，要求模型超越简单的路径组合推理。&lt;h4&gt;主要发现&lt;/h4&gt;现有基准测试基于推理可简化为关系路径组合的假设，导致模型在这些基准上表现良好但难以泛化到其他场景。&lt;h4&gt;结论&lt;/h4&gt;需要NoRA这样的新基准来更全面地评估模型的真实系统推理能力，推动领域发展。&lt;h4&gt;翻译&lt;/h4&gt;设计能够以系统化方式学习的模型是一个重要且长期存在的挑战。近年来，针对系统关系推理的特定案例提出了多种解决方案，包括神经符号方法、Transformer架构的变体和专门的图神经网络。然而，现有的系统关系推理基准测试基于过于简化的设置，基于推理可以简化为组合关系路径的假设。事实上，这个假设被嵌入到几个最新模型的架构中，导致这些方法在现有基准上表现良好但难以推广到其他设置。为了支持神经网络在系统关系推理领域的进一步发展，我们引入NoRA，一个新的基准测试，它增加了多个难度级别，要求模型超越基于路径的推理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing models that can learn to reason in a systematic way is an importantand long-standing challenge. In recent years, a wide range of solutions havebeen proposed for the specific case of systematic relational reasoning,including Neuro-Symbolic approaches, variants of the Transformer architecture,and specialised Graph Neural Networks. However, existing benchmarks forsystematic relational reasoning focus on an overly simplified setting, based onthe assumption that reasoning can be reduced to composing relational paths. Infact, this assumption is hard-baked into the architecture of several recentmodels, leading to approaches that can perform well on existing benchmarks butare difficult to generalise to other settings. To support further progress inthe field of systematic relational reasoning with neural networks, we introduceNoRA, a new benchmark which adds several levels of difficulty and requiresmodels to go beyond path-based reasoning.</description>
      <author>example@mail.com (Anirban Das, Irtaza Khalid, Rafael Peñaloza, Steven Schockaert)</author>
      <guid isPermaLink="false">2510.23532v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>iPac: Incorporating Intra-image Patch Context into Graph Neural Networks for Medical Image Classification</title>
      <link>http://arxiv.org/abs/2510.23504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in the proceedings of ICONIP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;iPac是一种创新的图神经网络方法，通过引入图像的新型图表示来改进图像分类性能，特别在医学图像分类中表现出色，比基线方法平均提高5%的准确率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为图像处理的一种有前景的范式，但在图像分类任务中的表现受到限制，因为它们对视觉实体之间的底层结构和关系考虑不足。&lt;h4&gt;目的&lt;/h4&gt;提出iPac方法，引入图像的新型图表示，增强图神经网络在医学图像分类中的性能，通过认识到底层结构和关系在医学图像分类中的重要性。&lt;h4&gt;方法&lt;/h4&gt;iPac集成了多个阶段，包括patch分区、特征提取、聚类、图构建和基于图的学习，将这些阶段整合到一个统一的网络中，通过捕获相关特征并将它们组织成簇，构建有意义的图表示，有效封装图像的语义。&lt;h4&gt;主要发现&lt;/h4&gt;在多种医学图像数据集上的实验评估证明了iPac的有效性，与基线方法相比，平均准确率提高了高达5%。&lt;h4&gt;结论&lt;/h4&gt;该方法为图像分类提供了一种通用且灵活的解决方案，特别是在医学图像领域，通过利用图表示并考虑视觉实体之间的固有结构和关系。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为图像处理的一种有前景的范式，但它们在图像分类任务中的表现受到对视觉实体之间底层结构和关系考虑不足的限制。本文提出了iPac，一种通过引入图像的新型图表示来增强图神经网络图像分类的新方法，通过认识到底层结构和关系在医学图像分类中的重要性。iPac将多个阶段（包括patch分区、特征提取、聚类、图构建和基于图的学习）整合到一个统一的网络中，以推进图神经网络图像分类。通过捕获相关特征并将它们组织成簇，我们构建了一个有意义的图表示，有效地封装了图像的语义。在多种医学图像数据集上的实验评估证明了iPac的有效性，与基线方法相比，平均准确率提高了高达5%。我们的方法通过利用图表示并考虑视觉实体之间的固有结构和关系，为图像分类提供了一种通用且灵活的解决方案，特别是在医学图像领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have emerged as a promising paradigm for imageprocessing, yet their performance in image classification tasks is hindered bya limited consideration of the underlying structure and relationships amongvisual entities. This work presents iPac, a novel approach to introduce a newgraph representation of images to enhance graph neural network imageclassification by recognizing the importance of underlying structure andrelationships in medical image classification. iPac integrates various stages,including patch partitioning, feature extraction, clustering, graphconstruction, and graph-based learning, into a unified network to advance graphneural network image classification. By capturing relevant features andorganising them into clusters, we construct a meaningful graph representationthat effectively encapsulates the semantics of the image. Experimentalevaluation on diverse medical image datasets demonstrates the efficacy of iPac,exhibiting an average accuracy improvement of up to 5% over baseline methods.Our approach offers a versatile and generic solution for image classification,particularly in the realm of medical images, by leveraging the graphrepresentation and accounting for the inherent structure and relationshipsamong visual entities.</description>
      <author>example@mail.com (Usama Zidan, Mohamed Gaber, Mohammed M. Abdelsamea)</author>
      <guid isPermaLink="false">2510.23504v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Dual Prompting: Hierarchical Debiasing for Fairness-aware Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.23469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;近年来，通过在无标签图数据上进行自监督学习来预训练图神经网络已成为图学习中的广泛采用范式。虽然这种范式对预训练强大的GNN模型有效，但预训练和下游任务之间通常存在目标差距。图提示通过额外的可学习提示来调整预训练的GNN模型以适应特定下游任务，同时保持预训练的GNN模型冻结。&lt;h4&gt;目的&lt;/h4&gt;解决现有图提示方法在设计提示时往往忽视公平性的问题。预训练的GNN模型会在不同人口统计子群中产生有区别性的节点表示，因为下游图数据在节点属性和图结构中固有地包含偏见。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自适应双提示(ADPrompt)框架，用于增强预训练GNN模型适应下游任务的公平性。设计了自适应特征校正模块，学习自定义的属性提示，在输入层抑制敏感信息，从源头减少偏见。提出了自适应消息校准模块，在每一层生成结构提示，调整来自邻居节点的消息，实现信息流的动态和软校准。联合优化两个提示模块，以适应预训练的GNN同时增强公平性。&lt;h4&gt;主要发现&lt;/h4&gt;在四个数据集上使用四种预训练策略进行了广泛的实验来评估ADPrompt的性能。结果表明，ADPrompt在节点分类任务上优于七种基线方法。&lt;h4&gt;结论&lt;/h4&gt;ADPrompt框架能够有效提升预训练GNN模型在下游任务中的公平性和性能。&lt;h4&gt;翻译&lt;/h4&gt;近年来，通过在无标签图数据上进行自监督学习来预训练图神经网络已成为图学习中的广泛采用范式。虽然这种范式对预训练强大的GNN模型有效，但预训练和下游任务之间通常存在目标差距。为了弥合这一差距，图提示通过额外的可学习提示来调整预训练的GNN模型以适应特定的下游任务，同时保持预训练的GNN模型冻结。由于最近的图提示方法主要关注增强模型在下游任务上的效用，它们在设计提示进行适应时往往忽视了公平性问题。实际上，预训练的GNN模型会在不同人口统计子群中产生有区别性的节点表示，因为下游图数据在节点属性和图结构中固有地包含偏见。为了解决这个问题，我们提出了一个自适应双提示(ADPrompt)框架，用于增强预训练GNN模型适应下游任务的公平性。为了减轻属性偏见，我们设计了一个自适应特征校正模块，学习自定义的属性提示，在输入层抑制敏感信息，从源头减少偏见。之后，我们提出了一个自适应消息校准模块，在每一层生成结构提示，调整来自邻居节点的消息，实现信息流的动态和软校准。最后，ADPrompt联合优化两个提示模块，以适应预训练的GNN同时增强公平性。我们在四个数据集上使用四种预训练策略进行了广泛的实验来评估ADPrompt的性能。结果表明，我们提出的ADPrompt在节点分类任务上优于七种基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, pre-training Graph Neural Networks (GNNs) throughself-supervised learning on unlabeled graph data has emerged as a widelyadopted paradigm in graph learning. Although the paradigm is effective forpre-training powerful GNN models, the objective gap often exists betweenpre-training and downstream tasks. To bridge this gap, graph prompting adaptspre-trained GNN models to specific downstream tasks with extra learnableprompts while keeping the pre-trained GNN models frozen. As recent graphprompting methods largely focus on enhancing model utility on downstream tasks,they often overlook fairness concerns when designing prompts for adaptation. Infact, pre-trained GNN models will produce discriminative node representationsacross demographic subgroups, as downstream graph data inherently containsbiases in both node attributes and graph structures. To address this issue, wepropose an Adaptive Dual Prompting (ADPrompt) framework that enhances fairnessfor adapting pre-trained GNN models to downstream tasks. To mitigate attributebias, we design an Adaptive Feature Rectification module that learns customizedattribute prompts to suppress sensitive information at the input layer,reducing bias at the source. Afterward, we propose an Adaptive MessageCalibration module that generates structure prompts at each layer, which adjustthe message from neighboring nodes to enable dynamic and soft calibration ofthe information flow. Finally, ADPrompt jointly optimizes the two promptingmodules to adapt the pre-trained GNN while enhancing fairness. We conductextensive experiments on four datasets with four pre-training strategies toevaluate the performance of ADPrompt. The results demonstrate that our proposedADPrompt outperforms seven baseline methods on node classification tasks.</description>
      <author>example@mail.com (Yuhan Yang, Xingbo Fu, Jundong Li)</author>
      <guid isPermaLink="false">2510.23469v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Improving Predictions of Molecular Properties with Graph Featurisation and Heterogeneous Ensemble Models</title>
      <link>http://arxiv.org/abs/2510.23428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合图神经网络学习描述符与传统分子描述符的混合方法，通过MetaModel框架整合多种机器学习模型，以提高分子性质预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;分子性质预测是化学信息学和药物发现中的关键任务，现有的方法通常专注于单一类型的特征或模型架构，可能无法充分利用不同方法的优势。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够结合多种描述符和机器学习模型的'最佳结合'方法，以提高分子性质预测的准确性，特别是在广泛的回归和分类任务中。&lt;h4&gt;方法&lt;/h4&gt;1. 引入MetaModel框架聚合来自多样化领先ML模型的预测；2. 设计特征化方案结合任务特定的GNN衍生特征与传统分子描述符；3. 使用图神经网络(GNN)学习分子描述符；4. 结合通用描述符和混合机器学习模型集成。&lt;h4&gt;主要发现&lt;/h4&gt;在所有测试的回归数据集上优于最先进的ChemProp模型；在9个分类数据集中的6个上优于ChemProp模型；包含从ChemProp衍生的GNN特征可以提升集成模型在多个数据集上的性能；该方法在多种类型的分子性质预测任务中表现优异。&lt;h4&gt;结论&lt;/h4&gt;为了在广泛问题上实现最佳性能，结合通用描述符与任务特定的学习特征，并使用多样化的机器学习模型进行预测至关重要。&lt;h4&gt;翻译&lt;/h4&gt;我们通过结合图神经网络(GNN)学习到的分子描述符与通用描述符以及混合的机器学习模型集成，探索了一种用于建模分子性质的'最佳结合'方法。我们引入了一个MetaModel框架来聚合来自多样化领先ML模型的预测。我们提出了一种特征化方案，用于结合任务特定的GNN衍生特征与传统分子描述符。我们证明，在所有测试的回归数据集上以及在9个分类数据集中的6个上，我们的框架优于最先进的ChemProp模型。我们进一步表明，包含从ChemProp衍生的GNN特征可以提升集成模型在多个数据集上的性能，否则这些数据集上的性能会较差。我们得出结论，为了在广泛问题上实现最佳性能，结合通用描述符与任务特定的学习特征，并使用多样化的ML模型进行预测至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1021/acs.jcim.5c01844&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We explore a "best-of-both" approach to modelling molecular properties bycombining learned molecular descriptors from a graph neural network (GNN) withgeneral-purpose descriptors and a mixed ensemble of machine learning (ML)models. We introduce a MetaModel framework to aggregate predictions from adiverse set of leading ML models. We present a featurisation scheme forcombining task-specific GNN-derived features with conventional moleculardescriptors.  We demonstrate that our framework outperforms the cutting-edge ChemProp modelon all regression datasets tested and 6 of 9 classification datasets. Wefurther show that including the GNN features derived from ChemProp boosts theensemble model's performance on several datasets where it otherwise would haveunderperformed. We conclude that to achieve optimal performance across a wideset of problems, it is vital to combine general-purpose descriptors withtask-specific learned features and use a diverse set of ML models to make thepredictions.</description>
      <author>example@mail.com (Michael L. Parker, Samar Mahmoud, Bailey Montefiore, Mario Öeren, Himani Tandon, Charlotte Wharrick, Matthew D. Segall)</author>
      <guid isPermaLink="false">2510.23428v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Framework for Multi-Modal Protein Representation Learning</title>
      <link>http://arxiv.org/abs/2510.23273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DAMPE是一个统一框架，通过最优传输(OT)表示对齐和条件图生成(CGG)信息融合解决了蛋白质功能预测中的跨模态异质性和嘈杂数据问题，在标准基准上取得了优于或匹配最先进方法的性能。&lt;h4&gt;背景&lt;/h4&gt;准确的蛋白质功能预测需要整合异构的内在信号(如序列和结构)与嘈杂的外部上下文(如蛋白质相互作用和GO术语注释)。然而，两个关键挑战阻碍了有效融合：(i)预训练的内在编码器产生的嵌入之间的跨模态分布不匹配；(ii)外部数据的嘈杂关系图降低了基于GNN的信息聚合效果。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一框架来解决蛋白质功能预测中的跨模态分布不匹配和嘈杂数据关系图问题，提高蛋白质功能预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了DAMPE(Diffused and Aligned Multi-modal Protein Embedding)框架，包含两个核心机制：(1)基于最优传输(OT)的表示对齐，建立不同模态内在嵌入空间之间的对应关系，缓解跨模态异质性；(2)基于条件图生成(CGG)的信息融合方法，条件编码器融合对齐的内在嵌入为图重建提供信息提示。理论分析表明CGG目标驱动条件编码器将图感知知识吸收到蛋白质表示中。&lt;h4&gt;主要发现&lt;/h4&gt;DAMPE在标准GO基准测试上优于或匹配了DPFunc等最先进方法，实现了0.002-0.013 pp的AUPR增益和0.004-0.007 pp的Fmax增益。消融研究表明基于OT的对齐贡献了0.043-0.064 pp的AUPR，基于CGG的融合增加了0.005-0.111 pp的Fmax。&lt;h4&gt;结论&lt;/h4&gt;DAMPE为稳健的多模态蛋白质表示学习提供了一种可扩展且理论上有依据的方法，显著提高了蛋白质功能预测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;准确的蛋白质功能预测需要整合异构的内在信号(如序列和结构)与嘈杂的外部上下文(如蛋白质相互作用和GO术语注释)。然而，两个关键挑战阻碍了有效融合：(i)由预训练的内在编码器产生的嵌入之间的跨模态分布不匹配，以及(ii)外部数据的嘈杂关系图降低了基于GNN的信息聚合效果。我们提出了DAMPE(扩散和对齐的多模态蛋白质嵌入)，一个通过两个核心机制解决这些问题的统一框架。首先，我们提出了基于最优传输(OT)的表示对齐，建立了不同模态内在嵌入空间之间的对应关系，有效缓解了跨模态异质性。其次，我们开发了基于条件图生成(CGG)的信息融合方法，其中条件编码器融合对齐的内在嵌入，为图重建提供信息提示。同时，我们的理论分析表明CGG目标驱动条件编码器将其产生的蛋白质表示吸收图感知知识。经验上，DAMPE在标准GO基准测试上优于或匹配了DPFunc等最先进方法，实现了0.002-0.013 pp的AUPR增益和0.004-0.007 pp的Fmax增益。消融研究进一步表明，基于OT的对齐贡献了0.043-0.064 pp的AUPR，而基于CGG的融合增加了0.005-0.111 pp的Fmax。总体而言，DAMPE为稳健的多模态蛋白质表示学习提供了一种可扩展且理论上有依据的方法，显著提高了蛋白质功能预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate protein function prediction requires integrating heterogeneousintrinsic signals (e.g., sequence and structure) with noisy extrinsic contexts(e.g., protein-protein interactions and GO term annotations). However, two keychallenges hinder effective fusion: (i) cross-modal distributional mismatchamong embeddings produced by pre-trained intrinsic encoders, and (ii) noisyrelational graphs of extrinsic data that degrade GNN-based informationaggregation. We propose Diffused and Aligned Multi-modal Protein Embedding(DAMPE), a unified framework that addresses these through two core mechanisms.First, we propose Optimal Transport (OT)-based representation alignment thatestablishes correspondence between intrinsic embedding spaces of differentmodalities, effectively mitigating cross-modal heterogeneity. Second, wedevelop a Conditional Graph Generation (CGG)-based information fusion method,where a condition encoder fuses the aligned intrinsic embeddings to provideinformative cues for graph reconstruction. Meanwhile, our theoretical analysisimplies that the CGG objective drives this condition encoder to absorbgraph-aware knowledge into its produced protein representations. Empirically,DAMPE outperforms or matches state-of-the-art methods such as DPFunc onstandard GO benchmarks, achieving AUPR gains of 0.002-0.013 pp and Fmax gains0.004-0.007 pp. Ablation studies further show that OT-based alignmentcontributes 0.043-0.064 pp AUPR, while CGG-based fusion adds 0.005-0.111 ppFmax. Overall, DAMPE offers a scalable and theoretically grounded approach forrobust multi-modal protein representation learning, substantially enhancingprotein function prediction.</description>
      <author>example@mail.com (Runjie Zheng, Zhen Wang, Anjie Qiao, Jiancong Xie, Jiahua Rao, Yuedong Yang)</author>
      <guid isPermaLink="false">2510.23273v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation</title>
      <link>http://arxiv.org/abs/2510.22942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures, 4 tables, submitted to ICDE 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GTR-Mamba是一种创新的下一个兴趣点推荐框架，通过结合双曲几何和欧几里得切线空间的优势，解决了现有模型难以同时捕捉空间层次结构和时间动态性的问题。&lt;h4&gt;背景&lt;/h4&gt;下一个兴趣点推荐是现代位置社交网络中的关键任务，现有模型主要基于图神经网络和序列模型，但存在基本局限性。&lt;h4&gt;目的&lt;/h4&gt;克服现有模型的限制，能够同时捕捉空间选择的内在层次结构和用户特定时间上下文的动态变化，提供更精准的个性化推荐。&lt;h4&gt;方法&lt;/h4&gt;提出GTR-Mamba框架，利用双曲几何建模静态树状偏好层次结构，在欧几里得切线空间中通过Mamba层处理动态序列更新，并通过跨流形通道融合时空信息引导状态空间模型。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实数据集上的实验表明，GTR-Mamba在下一个POI推荐任务上持续优于最先进的基线模型。&lt;h4&gt;结论&lt;/h4&gt;GTR-Mamba框架有效解决了现有模型在捕捉空间层次结构和时间动态性方面的局限性，提升了推荐的准确性。&lt;h4&gt;翻译&lt;/h4&gt;下一个兴趣点推荐是现代位置社交网络中的关键任务，旨在对人类移动的复杂决策过程进行建模，为用户的下一个签到位置提供个性化推荐。现有的POI推荐模型主要基于图神经网络和序列模型，已得到广泛研究。然而，这些模型面临一个基本限制：它们难以同时捕捉空间选择的内在层次结构和用户特定时间上下文的动态及不规则变化。为克服这一限制，我们提出了GTR-Mamba，一个用于跨流形条件和路由的新框架。GTR-Mamba利用不同数学空间的不同优势处理不同任务：它在双曲几何中建模静态的树状偏好层次结构，同时将动态序列更新路由到计算稳定且高效的欧几里得切线空间中的新型Mamba层。这一过程由跨流形通道协调，该通道融合时空信息以明确引导状态空间模型，实现对上下文变化的灵活适应。在三个真实数据集上的大量实验表明，GTR-Mamba在下一个POI推荐方面持续优于最先进的基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next Point-of-Interest (POI) recommendation is a critical task in modernLocation-Based Social Networks (LBSNs), aiming to model the complexdecision-making process of human mobility to provide personalizedrecommendations for a user's next check-in location. Existing POIrecommendation models, predominantly based on Graph Neural Networks andsequential models, have been extensively studied. However, these models face afundamental limitation: they struggle to simultaneously capture the inherenthierarchical structure of spatial choices and the dynamics and irregular shiftsof user-specific temporal contexts. To overcome this limitation, we proposeGTR-Mamba, a novel framework for cross-manifold conditioning and routing.GTR-Mamba leverages the distinct advantages of different mathematical spacesfor different tasks: it models the static, tree-like preference hierarchies inhyperbolic geometry, while routing the dynamic sequence updates to a novelMamba layer in the computationally stable and efficient Euclidean tangentspace. This process is coordinated by a cross-manifold channel that fusesspatio-temporal information to explicitly steer the State Space Model (SSM),enabling flexible adaptation to contextual changes. Extensive experiments onthree real-world datasets demonstrate that GTR-Mamba consistently outperformsstate-of-the-art baseline models in next POI recommendation.</description>
      <author>example@mail.com (Zhuoxuan Li, Jieyuan Pei, Tangwei Ye, Zhongyuan Lai, Zihan Liu, Fengyuan Xu, Qi Zhang, Liang Hu)</author>
      <guid isPermaLink="false">2510.22942v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Diffuse to Detect: A Generalizable Framework for Anomaly Detection with Diffusion Models Applications to UAVs and Beyond</title>
      <link>http://arxiv.org/abs/2510.22928v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DTD是一种创新的异常检测框架，通过适应扩散模型并采用单步过程实现快速精确的异常识别，结合图神经网络捕捉时空异常，并通过双分支架构平衡可扩展性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;复杂、高维数据（如无人机传感器读数）中的异常检测对于操作安全至关重要，但现有方法在敏感性、可扩展性和捕捉复杂依赖关系方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种创新的异常检测方法，解决现有方法的局限性，实现快速、精确的异常识别。&lt;h4&gt;方法&lt;/h4&gt;提出Diffuse to Detect (DTD)框架，采用单步扩散过程预测噪声模式；集成图神经网络建模传感器关系为动态图；使用双分支架构（参数化神经网络能量评分和非参数统计方法）平衡可扩展性和可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;DTD能够在不产生重构错误的情况下快速精确识别异常；在无人机传感器数据、多元时间序列和图像上的评估表明DTD优于现有方法；DTD具有跨不同数据模态的通用性。&lt;h4&gt;结论&lt;/h4&gt;DTD因其多功能性和适应性，成为安全关键应用（包括工业监测等）的变革性解决方案。&lt;h4&gt;翻译&lt;/h4&gt;复杂、高维数据（如无人机传感器读数）中的异常检测对于操作安全至关重要，但由于现有方法的敏感性、可扩展性有限且无法捕捉复杂依赖关系，这仍然是一个挑战。我们提出了Diffuse to Detect (DTD)框架，这是一种创新的方法，将扩散模型适应于异常检测，不同于其在具有高推理时间的生成任务中的常规使用。相比之下，DTD采用单步扩散过程来预测噪声模式，能够快速精确地识别异常而不会产生重构错误。这种方法基于稳健的理论基础，将噪声预测与数据分布的得分函数联系起来，确保可靠的偏差检测。通过集成图神经网络将传感器关系建模为动态图，DTD有效捕捉了空间（传感器间）和时间异常。其双分支架构采用基于参数化神经网络的能量评分实现可扩展性，非参数统计方法提供可解释性，在计算效率和透明度之间提供了灵活的权衡。在无人机传感器数据、多元时间序列和图像上的广泛评估证明了DTD优于现有方法的性能，强调了其在不同数据模态上的通用性。这种多功能性及其适应性使DTD成为安全关键应用的变革性解决方案，包括工业监测等。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection in complex, high-dimensional data, such as UAV sensorreadings, is essential for operational safety but challenging for existingmethods due to their limited sensitivity, scalability, and inability to captureintricate dependencies. We propose the Diffuse to Detect (DTD) framework, anovel approach that innovatively adapts diffusion models for anomaly detection,diverging from their conventional use in generative tasks with high inferencetime. By comparison, DTD employs a single-step diffusion process to predictnoise patterns, enabling rapid and precise identification of anomalies withoutreconstruction errors. This approach is grounded in robust theoreticalfoundations that link noise prediction to the data distribution's scorefunction, ensuring reliable deviation detection. By integrating Graph NeuralNetworks to model sensor relationships as dynamic graphs, DTD effectivelycaptures spatial (inter-sensor) and temporal anomalies. Its two-brancharchitecture, with parametric neural network-based energy scoring forscalability and nonparametric statistical methods for interpretability,provides flexible trade-offs between computational efficiency and transparency.Extensive evaluations on UAV sensor data, multivariate time series, and imagesdemonstrate DTD's superior performance over existing methods, underscoring itsgenerality across diverse data modalities. This versatility, combined with itsadaptability, positions DTD as a transformative solution for safety-criticalapplications, including industrial monitoring and beyond.</description>
      <author>example@mail.com (Mingze Gong, Juan Du, Jianbang You)</author>
      <guid isPermaLink="false">2510.22928v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>FastJAM: a Fast Joint Alignment Model for Images</title>
      <link>http://arxiv.org/abs/2510.22842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025. Pages 1-10 are the Main Paper. Pages 23-31  are Supplemental Material. FastJAM website -  https://bgu-cs-vil.github.io/FastJAM/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FastJAM是一种快速的基于图的图像联合对齐方法，能够显著降低计算复杂度，实现从小时或分钟到秒级的速度提升，同时保持或提高对齐质量。&lt;h4&gt;背景&lt;/h4&gt;图像联合对齐(JA)旨在将一组图像对齐到统一坐标系，使语义相似特征出现在对应空间位置。现有方法通常需要长时间训练、大容量模型和大量超参数调整。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、高效的图像联合对齐方法，减少计算时间和资源需求，同时保持或提高对齐质量。&lt;h4&gt;方法&lt;/h4&gt;FastJAM利用现成的图像匹配器计算的对和快速非参数聚类构建图，表示图像内和图像间关键点关系。通过图神经网络传播和聚合这些对应关系，使用图像级池化预测单应性参数。采用逆组合损失消除正则化项需求，避免相关超参数调整。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，FastJAM在对齐质量方面优于现有现代JA方法，同时将计算时间从小时或分钟减少到几秒钟。&lt;h4&gt;结论&lt;/h4&gt;FastJAM是一种高效、快速的图像联合对齐方法，能够在保持高质量对齐的同时显著减少计算时间。&lt;h4&gt;翻译&lt;/h4&gt;图像联合对齐(JA)旨在将一组图像对齐到统一的坐标系中，使语义相似的特征出现在对应的空间位置。大多数现有方法通常需要长时间的训练、大容量模型和大量的超参数调整。我们引入了FastJAM，一种快速的基于图的方法，显著降低了联合对齐任务的计算复杂度。FastJAM利用现成的图像匹配器计算的对，以及快速的非参数聚类，来构建表示图像内和图像间关键点关系的图。图神经网络传播和聚合这些对应关系，通过图像级池化有效地预测每个图像的单应性参数。利用逆组合损失消除了对预测变换的正则化项的需求（因此也避免了与这些项相关的超参数调整），FastJAM能够快速有效地执行图像联合对齐。在几个基准测试上的实验结果表明，FastJAM在对齐质量方面优于现有的现代JA方法，同时将计算时间从小时或分钟减少到几秒钟。我们的代码可在项目网页获取，https://bgu-cs-vil.github.io/FastJAM/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Joint Alignment (JA) of images aims to align a collection of images into aunified coordinate frame, such that semantically-similar features appear atcorresponding spatial locations. Most existing approaches often require longtraining times, large-capacity models, and extensive hyperparameter tuning. Weintroduce FastJAM, a rapid, graph-based method that drastically reduces thecomputational complexity of joint alignment tasks. FastJAM leverages pairwisematches computed by an off-the-shelf image matcher, together with a rapidnonparametric clustering, to construct a graph representing intra- andinter-image keypoint relations. A graph neural network propagates andaggregates these correspondences, efficiently predicting per-image homographyparameters via image-level pooling. Utilizing an inverse-compositional loss,that eliminates the need for a regularization term over the predictedtransformations (and thus also obviates the hyperparameter tuning associatedwith such terms), FastJAM performs image JA quickly and effectively.Experimental results on several benchmarks demonstrate that FastJAM achievesresults better than existing modern JA methods in terms of alignment quality,while reducing computation time from hours or minutes to mere seconds. Our codeis available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/</description>
      <author>example@mail.com (Omri Hirsch, Ron Shapira Weber, Shira Ifergane, Oren Freifeld)</author>
      <guid isPermaLink="false">2510.22842v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network Assisted Genetic Algorithm for Structural Dynamic Response and Parameter Optimization</title>
      <link>http://arxiv.org/abs/2510.22839v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合图神经网络(GNN)代理模型和遗传算法(GA)优化器的混合数据驱动框架，用于优化结构参数（质量、刚度和阻尼系数）。该方法克服了传统数值方法在迭代优化任务中计算成本高的问题，实现了强收敛性、良好泛化能力和显著降低的计算成本，为自动化和智能结构设计提供了有效途径。&lt;h4&gt;背景&lt;/h4&gt;结构参数（质量m、刚度k和阻尼系数c）的优化对设计高效、有韧性和稳定的结构至关重要。传统的数值方法如有限元法(FEM)和计算流体动力学(CFD)模拟虽能提供高精度结果，但在迭代优化任务中计算成本高昂，因为每次评估都需要为每个参数组合求解控制方程。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器，以克服传统数值方法在结构参数优化中的计算成本高问题，实现更高效、准确的参数优化。&lt;h4&gt;方法&lt;/h4&gt;研究采用了一种混合数据驱动框架，包括：1) 使用图神经网络(GNN)作为代理模型，学习结构参数与动态位移响应之间的非线性映射；2) 使用Newmark Beta方法生成具有不同质量、刚度和阻尼配置的单自由度(SDOF)系统响应数据集；3) 应用遗传算法(GA)通过最小化预测位移和提高动态稳定性来搜索全局最优参数集。&lt;h4&gt;主要发现&lt;/h4&gt;GNN和GA框架实现了强收敛性、良好的泛化能力，并显著降低了计算成本，相比传统模拟方法具有明显优势。该方法能够准确学习结构参数与动态响应之间的复杂关系，实现快速预测和优化。&lt;h4&gt;结论&lt;/h4&gt;结合机器学习代理与进化优化的方法在自动化和智能结构设计中具有显著有效性。该框架为结构参数优化提供了一种高效、准确的解决方案，克服了传统数值方法的计算瓶颈。&lt;h4&gt;翻译&lt;/h4&gt;结构参数（如质量m、刚度k和阻尼系数c）的优化对设计高效、有韧性和稳定的结构至关重要。传统的数值方法，包括有限元法(FEM)和计算流体动力学(CFD)模拟，能提供高精度结果，但在迭代优化任务中计算成本高，因为每次评估都需要为每个参数组合求解控制方程。本研究提出了一种混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器来克服这些挑战。GNN被训练来准确学习结构参数和动态位移响应之间的非线性映射，从而能够快速预测而无需重复求解系统方程。使用Newmark Beta方法生成了具有不同质量、刚度和阻尼配置的单自由度(SDOF)系统响应数据集。然后，GA通过最小化预测位移和提高动态稳定性来搜索全局最优参数集。结果表明，与传统模拟相比，GNN和GA框架实现了强收敛性、良好的泛化能力和显著降低的计算成本。这种方法强调了将机器学习代理与进化优化相结合在自动化和智能结构设计中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The optimization of structural parameters, such as mass(m), stiffness(k), anddamping coefficient(c), is critical for designing efficient, resilient, andstable structures. Conventional numerical approaches, including Finite ElementMethod (FEM) and Computational Fluid Dynamics (CFD) simulations, providehigh-fidelity results but are computationally expensive for iterativeoptimization tasks, as each evaluation requires solving the governing equationsfor every parameter combination. This study proposes a hybrid data-drivenframework that integrates a Graph Neural Network (GNN) surrogate model with aGenetic Algorithm (GA) optimizer to overcome these challenges. The GNN istrained to accurately learn the nonlinear mapping between structural parametersand dynamic displacement responses, enabling rapid predictions withoutrepeatedly solving the system equations. A dataset of single-degree-of-freedom(SDOF) system responses is generated using the Newmark Beta method acrossdiverse mass, stiffness, and damping configurations. The GA then searches forglobally optimal parameter sets by minimizing predicted displacements andenhancing dynamic stability. Results demonstrate that the GNN and GA frameworkachieves strong convergence, robust generalization, and significantly reducedcomputational cost compared to conventional simulations. This approachhighlights the effectiveness of combining machine learning surrogates withevolutionary optimization for automated and intelligent structural design.</description>
      <author>example@mail.com (Sagnik Mukherjee)</author>
      <guid isPermaLink="false">2510.22839v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM</title>
      <link>http://arxiv.org/abs/2510.22740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE International Symposium on Multi-Robot &amp; Multi-Agent Systems  (MRS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于多智能体强化学习的分布式姿态图优化框架，解决了传统方法收敛到局部最小值的问题，显著提高了轨迹估计的准确性和计算效率。&lt;h4&gt;背景&lt;/h4&gt;分布式姿态图优化（PGO）是多机器人同时定位与地图构建（SLAM）中精确轨迹估计的基础。传统迭代方法将高度非凸优化目标线性化，需要重复求解正规方程，通常收敛到局部最小值，从而产生次优估计。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展、抗异常值的分布式平面PGO框架，利用多智能体强化学习（MARL）来提高轨迹估计的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;将分布式PGO建模为部分可观察马尔可夫博弈，每个机器人运行带有自适应边缘门控的循环边缘条件图神经网络（GNN）编码器来去噪，通过混合策略利用先验动作记忆和图嵌入优化姿态，最后使用一致性方案解决机器人间的不一致。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实世界数据集上的评估表明，该方法比最先进的分布式PGO框架平均减少37.5%的全局目标，同时将推理效率提高至少6倍；单个学习策略无需重新训练即可扩展到更大的机器人团队。&lt;h4&gt;结论&lt;/h4&gt;基于MARL的分布式PGO框架在提高轨迹估计精度的同时显著增强了计算效率，具有良好的可扩展性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;我们考虑分布式姿态图优化（PGO）问题，这是多机器人同时定位与地图构建（SLAM）中精确轨迹估计的基础。传统迭代方法将高度非凸优化目标线性化，需要重复求解正规方程，通常收敛到局部最小值，从而产生次优估计。我们提出了一种使用多智能体强化学习（MARL）的可扩展、抗异常值的分布式平面PGO框架。我们将分布式PGO构建为定义在局部姿态图上的部分可观察马尔可夫博弈，其中每个动作优化单个边的姿态估计。图分区器分解全局姿态图，每个机器人运行带有自适应边缘门控的循环边缘条件图神经网络（GNN）编码器来去噪噪声边缘。机器人通过利用先验动作记忆和图嵌入的混合策略顺序优化姿态。在局部图校正后，使用一致性方案解决机器人间的不一致，产生全局一致的估计。我们在综合的合成和真实世界数据集上的广泛评估表明，我们学习的基于MARL的智能体比最先进的分布式PGO框架平均减少37.5%的全局目标，同时将推理效率提高至少6倍。我们还证明了智能体复制允许单个学习策略无需重新训练即可轻松扩展到更大的机器人团队。代码可在https://github.com/herolab-uga/policies-over-poses公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider the distributed pose-graph optimization (PGO) problem, which isfundamental in accurate trajectory estimation in multi-robot simultaneouslocalization and mapping (SLAM). Conventional iterative approaches linearize ahighly non-convex optimization objective, requiring repeated solving of normalequations, which often converge to local minima and thus produce suboptimalestimates. We propose a scalable, outlier-robust distributed planar PGOframework using Multi-Agent Reinforcement Learning (MARL). We cast distributedPGO as a partially observable Markov game defined on local pose-graphs, whereeach action refines a single edge's pose estimate. A graph partitionerdecomposes the global pose graph, and each robot runs a recurrentedge-conditioned Graph Neural Network (GNN) encoder with adaptive edge-gatingto denoise noisy edges. Robots sequentially refine poses through a hybridpolicy that utilizes prior action memory and graph embeddings. After localgraph correction, a consensus scheme reconciles inter-robot disagreements toproduce a globally consistent estimate. Our extensive evaluations on acomprehensive suite of synthetic and real-world datasets demonstrate that ourlearned MARL-based actors reduce the global objective by an average of 37.5%more than the state-of-the-art distributed PGO framework, while enhancinginference efficiency by at least 6X. We also demonstrate that actor replicationallows a single learned policy to scale effortlessly to substantially largerrobot teams without any retraining. Code is publicly available athttps://github.com/herolab-uga/policies-over-poses.</description>
      <author>example@mail.com (Sai Krishna Ghanta, Ramviyas Parasuraman)</author>
      <guid isPermaLink="false">2510.22740v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking</title>
      <link>http://arxiv.org/abs/2510.22726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpoofTrackBench是一个可重现、模块化的基准测试，用于评估雷达欺骗下的实时定位和跟踪系统的对抗鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;雷达欺骗攻击对实时定位和跟踪系统构成威胁，需要有效的评估方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个基准测试框架，评估不同跟踪架构在雷达欺骗攻击下的性能和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用Hampton University Skyler Radar Sensor数据集，模拟漂移、幽灵和镜像类型的欺骗攻击，并使用联合概率数据关联(JPDA)和全局最近邻(GNN)架构评估跟踪器性能。框架分离干净和欺骗的检测流，可视化轨迹偏离，并量化分配错误。&lt;h4&gt;主要发现&lt;/h4&gt;通过聚类叠加、注入感知时间线和场景自适应可视化，实现了不同欺骗类型和配置下的结果可解释性。评估图表和日志自动导出，确保可重现性。&lt;h4&gt;结论&lt;/h4&gt;SpoofTrackBench为开放、合乎道德的欺骗感知跟踪管道基准测试设定了新标准，实现了严格的跨架构分析和社区验证。&lt;h4&gt;翻译&lt;/h4&gt;SpoofTrackBench是一个可重现、模块化的基准测试，用于评估雷达欺骗下的实时定位和跟踪(RTLS)系统的对抗鲁棒性。利用Hampton University Skyler雷达传感器数据集，我们模拟了漂移、幽灵和镜像类型的欺骗攻击，并使用联合概率数据关联(JPDA)和全局最近邻(GNN)架构评估跟踪器性能。我们的框架分离干净和欺骗的检测流，可视化欺骗引起的轨迹偏离，并通过直接偏离真相的指标量化分配错误。聚类叠加、注入感知时间线和场景自适应可视化使不同欺骗类型和配置下的结果可解释。评估图表和日志自动导出，用于可重现的比较。SpoofTrackBench为开放、合乎道德的欺骗感知跟踪管道基准测试设定了新标准，实现了严格的跨架构分析和社区验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; SpoofTrackBench is a reproducible, modular benchmark for evaluatingadversarial robustness in real-time localization and tracking (RTLS) systemsunder radar spoofing. Leveraging the Hampton University Skyler Radar Sensordataset, we simulate drift, ghost, and mirror-type spoofing attacks andevaluate tracker performance using both Joint Probabilistic Data Association(JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separatesclean and spoofed detection streams, visualizes spoof-induced trajectorydivergence, and quantifies assignment errors via direct drift-from-truthmetrics. Clustering overlays, injection-aware timelines, and scenario-adaptivevisualizations enable interpretability across spoof types and configurations.Evaluation figures and logs are auto-exported for reproducible comparison.SpoofTrackBench sets a new standard for open, ethical benchmarking ofspoof-aware tracking pipelines, enabling rigorous cross-architecture analysisand community validation.</description>
      <author>example@mail.com (Van Le, Tan Le)</author>
      <guid isPermaLink="false">2510.22726v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>If You Want to Be Robust, Be Wary of Initialization</title>
      <link>http://arxiv.org/abs/2510.22652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络(GNNs)的权重初始化和超参数对对抗性鲁棒性的影响，发现适当的初始化方法可显著提升模型防御能力。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在各种图相关任务中表现出色，但容易受到对抗性扰动的攻击。现有防御策略主要关注预处理技术和自适应消息传递方案，而忽略了权重初始化的影响。&lt;h4&gt;目的&lt;/h4&gt;探索权重初始化和相关超参数(如训练周期)对模型鲁棒性的影响，建立初始化策略与网络抗扰动能力之间的理论联系。&lt;h4&gt;方法&lt;/h4&gt;引入连接初始化策略和网络鲁棒性的理论框架，分析初始权重、训练周期与模型脆弱性的关系，并将框架扩展到深度神经网络。通过多种模型和真实数据集的对抗性攻击实验验证理论发现。&lt;h4&gt;主要发现&lt;/h4&gt;适当的权重初始化不仅能保证模型在干净数据集上的性能，还能显著提升对抗性防御能力，与其他初始化方法相比性能差距可达50%。&lt;h4&gt;结论&lt;/h4&gt;权重初始化是提升图神经网络对抗鲁棒性的重要因素，为防御对抗性攻击提供了超越传统机制的新视角。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在各种图相关任务中表现出色，然而人们对它们容易受到对抗性扰动的担忧依然存在。虽然主流防御策略主要关注预处理技术和自适应消息传递方案，但本研究探讨了一个尚未充分研究的维度：权重初始化及相关超参数(如训练周期)对模型鲁棒性的影响。我们引入了一个理论框架，连接初始化策略与网络对抗扰动的恢复能力。我们的分析揭示了初始权重、训练周期数量与模型脆弱性之间的直接关系，为对抗性鲁棒性提供了超越传统防御机制的新见解。虽然我们的主要关注点是图神经网络，但我们扩展了理论框架，提供了一个适用于深度神经网络的通用上界。跨越多种模型和真实世界数据集的广泛实验( subjected to various adversarial attacks)验证了我们的发现。我们说明，选择适当的初始化不仅能确保在干净数据集上的性能，还能增强模型对抗扰动的鲁棒性，与其他初始化方法相比观察到高达50%的性能差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable performance acrossa spectrum of graph-related tasks, however concerns persist regarding theirvulnerability to adversarial perturbations. While prevailing defense strategiesfocus primarily on pre-processing techniques and adaptive message-passingschemes, this study delves into an under-explored dimension: the impact ofweight initialization and associated hyper-parameters, such as training epochs,on a model's robustness. We introduce a theoretical framework bridging theconnection between initialization strategies and a network's resilience toadversarial perturbations. Our analysis reveals a direct relationship betweeninitial weights, number of training epochs and the model's vulnerability,offering new insights into adversarial robustness beyond conventional defensemechanisms. While our primary focus is on GNNs, we extend our theoreticalframework, providing a general upper-bound applicable to Deep Neural Networks.Extensive experiments, spanning diverse models and real-world datasetssubjected to various adversarial attacks, validate our findings. We illustratethat selecting appropriate initialization not only ensures performance on cleandatasets but also enhances model robustness against adversarial perturbations,with observed gaps of up to 50\% compared to alternative initializationapproaches.</description>
      <author>example@mail.com (Sofiane Ennadir, Johannes F. Lutzeyer, Michalis Vazirgiannis, El Houcine Bergou)</author>
      <guid isPermaLink="false">2510.22652v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Graph Classification Robustness with Singular Pooling</title>
      <link>http://arxiv.org/abs/2510.22643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at Neurips 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络在图分类任务中的对抗鲁棒性问题，特别关注了池化操作在塑造鲁棒性方面的作用，并提出了一种名为鲁棒奇异池化(RS-Pool)的新策略。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在图表示学习任务中表现出色，但其在图分类任务中的对抗鲁棒性研究相对较少，尤其是在与节点分类相比时。大多数现有防御方法集中在消息传递组件上，而忽略了池化操作的作用。&lt;h4&gt;目的&lt;/h4&gt;研究池化操作在图神经网络对抗鲁棒性中的角色，并开发一种能够提高图分类任务鲁棒性的新池化策略。&lt;h4&gt;方法&lt;/h4&gt;对标准扁平池化方法(求和、平均和最大值)进行理论分析，推导对抗风险上限，确定不同攻击场景和图结构下的脆弱性。基于这些见解，提出利用节点嵌入矩阵主奇异向量构建鲁棒图级表示的RS-Pool策略。&lt;h4&gt;主要发现&lt;/h4&gt;RS-Pool在遭受最先进对抗攻击时，比其他池化方法提供更好的鲁棒性，同时保持有竞争力的干净准确率。该策略与模型无关，可通过幂迭代有效实现，理论分析表明其具有良好的鲁棒性特性。&lt;h4&gt;结论&lt;/h4&gt;池化操作在图神经网络的对抗鲁棒性中扮演着重要角色，所提出的RS-Pool方法能够有效提高图分类任务的鲁棒性，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在一系列图表示学习任务中已取得强大性能，然而与节点分类相比，其在图分类中的对抗鲁棒性研究仍然不足。虽然大多数现有防御方法集中在消息传递组件上，但本研究调查了池化操作在塑造鲁棒性中被忽视的作用。我们对标准扁平池化方法(求和、平均和最大值)进行了理论分析，推导了它们对抗风险的上限，并确定了它们在不同攻击场景和图结构下的脆弱性。受这些见解启发，我们提出了鲁棒奇异池化(RS-Pool)，这是一种新颖的池化策略，利用节点嵌入矩阵的主奇异向量构建鲁棒的图级表示。我们从理论上研究了RS-Pool的鲁棒性，并解释了所得界限，从而加深对我们提出的池化算子的理解。虽然我们的分析集中在图卷积网络(GCNs)上，但RS-Pool是与模型无关的，可以通过幂迭代有效实现。真实世界基准测试的实证结果表明，当遭受最先进的对抗攻击时，RS-Pool比考虑的池化方法提供更好的鲁棒性，同时保持有竞争力的干净准确率。我们的代码已在GitHub公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved strong performance across a rangeof graph representation learning tasks, yet their adversarial robustness ingraph classification remains underexplored compared to node classification.While most existing defenses focus on the message-passing component, this workinvestigates the overlooked role of pooling operations in shaping robustness.We present a theoretical analysis of standard flat pooling methods (sum,average and max), deriving upper bounds on their adversarial risk andidentifying their vulnerabilities under different attack scenarios and graphstructures. Motivated by these insights, we propose \textit{Robust SingularPooling (RS-Pool)}, a novel pooling strategy that leverages the dominantsingular vector of the node embedding matrix to construct a robust graph-levelrepresentation. We theoretically investigate the robustness of RS-Pool andinterpret the resulting bound leading to improved understanding of our proposedpooling operator. While our analysis centers on Graph Convolutional Networks(GCNs), RS-Pool is model-agnostic and can be implemented efficiently via poweriteration. Empirical results on real-world benchmarks show that RS-Poolprovides better robustness than the considered pooling methods when subject tostate-of-the-art adversarial attacks while maintaining competitive cleanaccuracy. Our code is publicly availableat:\href{https://github.com/king/rs-pool}{https://github.com/king/rs-pool}.</description>
      <author>example@mail.com (Sofiane Ennadir, Oleg Smirnov, Yassine Abbahaddou, Lele Cao, Johannes F. Lutzeyer)</author>
      <guid isPermaLink="false">2510.22643v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval</title>
      <link>http://arxiv.org/abs/2510.22538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了IsoNet++，一种基于子图同构的改进图检索方法，通过技术创新显著提升了检索性能。&lt;h4&gt;背景&lt;/h4&gt;基于子图同构的图检索在场景图检索、分子指纹检测和电路设计等领域有广泛应用。Roy等人提出的IsoNet是一种后期交互模型，用于子图匹配，它先独立计算每对图的节点和边嵌入，再计算可训练的对齐映射。&lt;h4&gt;目的&lt;/h4&gt;开发IsoNet++，一种早期交互图神经网络，通过技术创新改进现有的子图匹配方法，提高图检索的准确性。&lt;h4&gt;方法&lt;/h4&gt;IsoNet++包含三个技术创新：1)通过在两个输入图内部和之间传递消息计算所有节点嵌入，由节点间的单射对齐引导；2)以惰性方式在多轮中更新对齐，每轮基于当前对齐状态从头运行逐层GNN；3)引入节点对伙伴交互概念，将节点对而非单个节点视为潜在伙伴。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，随着轮次增加，对齐 progressively 得到细化，检索性能显著优于现有方法，且所有三个创新都对提升准确性有贡献。&lt;h4&gt;结论&lt;/h4&gt;IsoNet++通过三个技术创新显著提高了图检索性能，代码和数据集已公开在https://github.com/structlearning/isonetpp。&lt;h4&gt;翻译&lt;/h4&gt;基于子图同构的图检索在场景图检索、分子指纹检测和电路设计等现实世界应用中有多种应用。Roy等人提出了IsoNet，一种用于子图匹配的后期交互模型，它首先独立计算每对图中节点和边的嵌入，然后计算可训练的对齐映射。本文提出了IsoNet++，一种早期交互图神经网络(GNN)，基于几项技术创新。首先，我们通过在两个输入图内部和之间传递消息来计算所有节点的嵌入，这些消息由节点之间的单射对齐引导。其次，我们在多轮中以惰性方式更新对齐。每轮中，我们基于当前对齐状态从头开始运行逐层GNN。一轮GNN完成后，我们使用最后一层嵌入更新对齐，然后进入下一轮。第三，IsoNet++引入了节点对伙伴交互的新概念。传统早期交互计算节点与另一图中潜在伙伴之间的注意力，注意力控制跨图传递的消息。相比之下，我们将节点对（而非单个节点）视为潜在伙伴。一个图中节点间存在边而另一图中不存在，提供了细化对齐的重要信号。我们在多个数据集上的实验表明，对齐随着轮次的推进逐步细化，检索性能显著优于现有方法。我们证明了所有三个创新都对提高准确性做出了贡献。我们的代码和数据集已在https://github.com/structlearning/isonetpp公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph retrieval based on subgraph isomorphism has several real-worldapplications such as scene graph retrieval, molecular fingerprint detection andcircuit design. Roy et al. [35] proposed IsoNet, a late interaction model forsubgraph matching, which first computes the node and edge embeddings of eachgraph independently of paired graph and then computes a trainable alignmentmap. Here, we present IsoNet++, an early interaction graph neural network(GNN), based on several technical innovations. First, we compute embeddings ofall nodes by passing messages within and across the two input graphs, guided byan injective alignment between their nodes. Second, we update this alignment ina lazy fashion over multiple rounds. Within each round, we run a layerwise GNNfrom scratch, based on the current state of the alignment. After the completionof one round of GNN, we use the last-layer embeddings to update the alignments,and proceed to the next round. Third, IsoNet++ incorporates a novel notion ofnode-pair partner interaction. Traditional early interaction computes attentionbetween a node and its potential partners in the other graph, the attentionthen controlling messages passed across graphs. In contrast, we consider nodepairs (not single nodes) as potential partners. Existence of an edge betweenthe nodes in one graph and non-existence in the other provide vital signals forrefining the alignment. Our experiments on several datasets show that thealignments get progressively refined with successive rounds, resulting insignificantly better retrieval performance than existing methods. Wedemonstrate that all three innovations contribute to the enhanced accuracy. Ourcode and datasets are publicly available athttps://github.com/structlearning/isonetpp.</description>
      <author>example@mail.com (Ashwin Ramachandran, Vaibhav Raj, Indrayumna Roy, Soumen Chakrabarti, Abir De)</author>
      <guid isPermaLink="false">2510.22538v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Toward Robust Signed Graph Learning through Joint Input-Target Denoising</title>
      <link>http://arxiv.org/abs/2510.22513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了RIDGE框架，一种通过联合去噪图输入和监督目标来实现的鲁棒符号图学习方法，有效提高了SGNN在噪声环境下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;符号图神经网络（SGNNs）被广泛用于分析包含正负链接的符号图中的复杂模式。鉴于现实世界连接的噪声特性，SGNN的鲁棒性也已成为一个关键研究领域。在经验属性监督下，图结构学习已在符号图表示学习中显示出其鲁棒性，然而，缺乏理论指导的鲁棒SGNN研究仍然较少。&lt;h4&gt;目的&lt;/h4&gt;受图信息瓶颈（GIB）在信息提取中的成功启发，提出一种有理论指导的鲁棒SGNN框架，通过联合去噪图输入和监督目标来提高鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了RIDGE框架，扩展了GIB理论以支持目标空间去噪，因为输入和目标空间都存在噪声。通过重参数化机制和变分近似产生的可处理目标函数，RIDGE有效清理输入数据和监督目标。&lt;h4&gt;主要发现&lt;/h4&gt;在四个常用的符号图数据集上的广泛验证表明，RIDGE在各种噪声水平下显著提高了流行SGNN模型的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;RIDGE框架能够有效提高SGNN在噪声环境下的鲁棒性，为符号图分析提供了新的理论指导和实践方法。&lt;h4&gt;翻译&lt;/h4&gt;符号图神经网络（SGNNs）被广泛用于分析包含正负链接的符号图中的复杂模式。鉴于现实世界连接的噪声特性，SGNN的鲁棒性也已成为一个关键研究领域。在经验属性监督下，图结构学习已在符号图表示学习中显示出其鲁棒性，然而，缺乏理论指导的鲁棒SGNN研究仍然较少。受图信息瓶颈（GIB）在信息提取中的成功启发，我们提出了RIDGE，一种通过联合去噪图输入和监督目标来实现鲁棒符号图学习的新框架。与基本GIB不同，我们扩展了GIB理论，使其能够对目标空间进行去噪，因为输入和目标空间都存在噪声。在实例化中，RIDGE通过重参数化机制和变分近似产生的可处理目标函数有效清理输入数据和监督目标。我们在四个常用的符号图数据集上广泛验证了我们的方法，结果表明，在各种噪声水平下，RIDGE显著提高了流行SGNN模型的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Signed Graph Neural Networks (SGNNs) are widely adopted to analyze complexpatterns in signed graphs with both positive and negative links. Given thenoisy nature of real-world connections, the robustness of SGNN has also emergedas a pivotal research area. Under the supervision of empirical properties,graph structure learning has shown its robustness on signed graphrepresentation learning, however, there remains a paucity of researchinvestigating a robust SGNN with theoretical guidance. Inspired by the successof graph information bottleneck (GIB) in information extraction, we proposeRIDGE, a novel framework for Robust sI gned graph learning through jointDenoising of Graph inputs and supervision targEts. Different from the basicGIB, we extend the GIB theory with the capability of target space denoising asthe co-existence of noise in both input and target spaces. In instantiation,RIDGE effectively cleanses input data and supervision targets via a tractableobjective function produced by reparameterization mechanism and variationalapproximation. We extensively validate our method on four prevalent signedgraph datasets, and the results show that RIDGE clearly improves the robustnessof popular SGNN models under various levels of noise.</description>
      <author>example@mail.com (Junran Wu, Beng Chin Ooi, Ke Xu)</author>
      <guid isPermaLink="false">2510.22513v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>GraphTOP: Graph Topology-Oriented Prompting for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.22451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the 39 Annual Conference on Neural Information Processing  Systems (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了GraphTOP框架，开创性地探索了面向图拓扑的提示方法，通过修改图拓扑而非仅修改节点特征来适配预训练的图神经网络模型。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)通过大规模图数据学习强大的图表示，'预训练、适配'方案是训练强大GNN的常见模式。在适配阶段，图提示是一种有效策略，可修改输入图数据同时保持预训练GNN模型冻结。&lt;h4&gt;目的&lt;/h4&gt;进行图提示在图拓扑方面的开创性研究，提出第一个图拓扑导向提示(GraphTOP)框架，有效适配预训练GNN模型用于下游任务。&lt;h4&gt;方法&lt;/h4&gt;将拓扑导向提示表述为多跳局部子图中的边重连问题，通过重参数化将其松弛到连续概率空间，同时确保紧密松弛并保持图的稀疏性。&lt;h4&gt;主要发现&lt;/h4&gt;在四种预训练策略下的五个图数据集上进行大量实验，GraphTOP在多个节点分类数据集上优于六个基线方法。&lt;h4&gt;结论&lt;/h4&gt;GraphTOP框架在图提示领域，特别是面向拓扑的提示方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)通过大规模图数据学习强大的图表示，革新了图学习领域。作为训练强大GNN的常见模式，'预训练、适配'方案首先在无标签图数据上预训练GNN，然后将其适配到特定下游任务。在适配阶段，图提示是一种有效策略，即可学习提示修改输入图数据，同时保持预训练GNN模型冻结。通常，现有图提示研究主要关注面向特征的方法，将图提示应用于节点特征或隐藏表示。然而，这些研究表现次优，因为它们持续忽视了面向拓扑提示的潜力，后者通过修改图拓扑来适配预训练GNN。在本研究中，我们从图拓扑角度对图提示进行了开创性研究。我们提出了第一个图拓扑导向提示(GraphTOP)框架，有效适配预训练GNN模型用于下游任务。更具体地说，我们将拓扑导向提示表述为多跳局部子图中的边重连问题，并通过重参数化将其松弛到连续概率空间，同时确保紧密松弛并保持图的稀疏性。在四种预训练策略下的五个图数据集上进行的大量实验表明，我们提出的GraphTOP在多个节点分类数据集上优于六个基线方法。我们的代码可在https://github.com/xbfu/GraphTOP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have revolutionized the field of graph learningby learning expressive graph representations from massive graph data. As acommon pattern to train powerful GNNs, the "pre-training, adaptation" schemefirst pre-trains GNNs over unlabeled graph data and subsequently adapts them tospecific downstream tasks. In the adaptation phase, graph prompting is aneffective strategy that modifies input graph data with learnable prompts whilekeeping pre-trained GNN models frozen. Typically, existing graph promptingstudies mainly focus on *feature-oriented* methods that apply graph prompts tonode features or hidden representations. However, these studies often achievesuboptimal performance, as they consistently overlook the potential of*topology-oriented* prompting, which adapts pre-trained GNNs by modifying thegraph topology. In this study, we conduct a pioneering investigation of graphprompting in terms of graph topology. We propose the first **Graph****T**opology-**O**riented **P**rompting (GraphTOP) framework to effectivelyadapt pre-trained GNN models for downstream tasks. More specifically, wereformulate topology-oriented prompting as an edge rewiring problem withinmulti-hop local subgraphs and relax it into the continuous probability spacethrough reparameterization while ensuring tight relaxation and preserving graphsparsity. Extensive experiments on five graph datasets under four pre-trainingstrategies demonstrate that our proposed GraphTOP outshines six baselines onmultiple node classification datasets. Our code is available athttps://github.com/xbfu/GraphTOP.</description>
      <author>example@mail.com (Xingbo Fu, Zhenyu Lei, Zihan Chen, Binchi Zhang, Chuxu Zhang, Jundong Li)</author>
      <guid isPermaLink="false">2510.22451v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Augmentation: Leveraging Inter-Instance Relation in Self-Supervised Representation Learning</title>
      <link>http://arxiv.org/abs/2510.22322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE Signal Processing Letters, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将图论与自监督表示学习相结合的新方法，通过构建k近邻图并利用图神经网络进行表示学习，显著提升了模型性能。&lt;h4&gt;背景&lt;/h4&gt;传统自监督表示学习方法主要关注通过数据增强技术生成的实例内变化，但往往忽略了实例间的重要关系信息。&lt;h4&gt;目的&lt;/h4&gt;在保留实例内属性的同时，有效捕获实例间关系，并通过图神经网络实现更广泛的上下文集成，提升表示学习效果。&lt;h4&gt;方法&lt;/h4&gt;在预训练阶段为教师和学生流构建k近邻(KNN)图，其中节点表示样本及其潜在表示，边编码实例间的相似性；在表示细化阶段，使用图神经网络在多个跃点间传播消息，实现更广泛的上下文整合。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10、ImageNet-100和ImageNet-1K三个数据集上，分别实现了7.3%、3.2%和1.0%的准确率提升，显著优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;基于图的机制在自监督表示学习中具有显著优势，能够有效提升模型性能，代码已公开可获取。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了一种将图论整合到自监督表示学习中的新方法。传统方法专注于应用增强技术生成的实例内变化。然而，它们常常忽略了重要的实例间关系。虽然我们的方法保留了实例内属性，但通过在预训练期间为教师和学生流构建k近邻(KNN)图，进一步捕获了实例间关系。在这些图中，节点表示样本及其潜在表示，边编码实例之间的相似性。预训练后，执行表示细化阶段。在此阶段，图神经网络不仅可以在直接邻居之间传播消息，还可以跨越多个跃点，从而实现更广泛的上下文集成。在CIFAR-10、ImageNet-100和ImageNet-1K上的实验结果分别比最先进的方法提高了7.3%、3.2%和1.0%的准确率。这些结果突显了所提出的基于图机制的有效性。代码可在https://github.com/alijavidani/SSL-GraphNNCLR公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LSP.2025.3610549&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel approach that integrates graph theory intoself-supervised representation learning. Traditional methods focus onintra-instance variations generated by applying augmentations. However, theyoften overlook important inter-instance relationships. While our method retainsthe intra-instance property, it further captures inter-instance relationshipsby constructing k-nearest neighbor (KNN) graphs for both teacher and studentstreams during pretraining. In these graphs, nodes represent samples along withtheir latent representations. Edges encode the similarity between instances.Following pretraining, a representation refinement phase is performed. In thisphase, Graph Neural Networks (GNNs) propagate messages not only among immediateneighbors but also across multiple hops, thereby enabling broader contextualintegration. Experimental results on CIFAR-10, ImageNet-100, and ImageNet-1Kdemonstrate accuracy improvements of 7.3%, 3.2%, and 1.0%, respectively, overstate-of-the-art methods. These results highlight the effectiveness of theproposed graph based mechanism. The code is publicly available athttps://github.com/alijavidani/SSL-GraphNNCLR.</description>
      <author>example@mail.com (Ali Javidani, Babak Nadjar Araabi, Mohammad Amin Sadeghi)</author>
      <guid isPermaLink="false">2510.22322v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Does Homophily Help in Robust Test-time Node Classification?</title>
      <link>http://arxiv.org/abs/2510.22289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为GrapHoST的测试时图结构转换方法，通过调整测试图中的同质性来提高预训练图神经网络在节点分类任务上的鲁棒性和性能，无需重新训练模型。&lt;h4&gt;背景&lt;/h4&gt;同质性是现实世界图的基本属性，但现有方法主要关注训练图的学习。然而，测试图常面临数据质量问题和分布偏移，如社交网络中不同地区用户的领域偏移和引文网络中的时间演化偏移，这些因素会降低预训练模型的鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;提高预训练GNN模型在面对测试时数据质量问题和分布偏移情况下的鲁棒性和性能。&lt;h4&gt;方法&lt;/h4&gt;提出GrapHoST方法，开发同质性预测器来区分测试边，通过预测同质性得分的置信度实现自适应的测试时图结构转换。&lt;h4&gt;主要发现&lt;/h4&gt;通过增加同质性图中的同质性或减少异质性图中的同质性来转换测试图结构，可以显著提高预训练GNN在节点分类任务上的鲁棒性和性能，无需模型训练或更新。&lt;h4&gt;结论&lt;/h4&gt;在九个基准数据集上的实验表明，GrapHoST在各种测试时数据质量问题下始终实现了最先进的性能，最高提升达10.92%，代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;同质性，即同一类别节点倾向于连接的特性，是现实世界图的基本属性，支撑着引文网络和社会网络等领域中的结构和语义模式。现有方法通过设计同质性感知的GNN架构或图结构学习策略来利用同质性，但它们主要关注训练图的GNN学习。然而，在现实场景中，测试图常常面临数据质量问题和分布偏移，如社交网络中不同地区用户之间的领域偏移，以及在不同时间段收集的引文网络图中的时间演化偏移。这些因素显著降低了预训练模型的鲁棒性，导致测试时性能下降。通过实证观察和理论分析，我们揭示出通过转换测试图结构——在同质性图中增加同质性或在异质性图中减少同质性——可以显著提高预训练GNN在节点分类任务上的鲁棒性和性能，无需模型训练或更新。基于这些见解，我们提出了一种基于同质性的新颖测试时图结构转换方法，名为GrapHoST。具体来说，开发了一个同质性预测器来区分测试边，通过预测同质性得分的置信度实现自适应的测试时图结构转换。在九个基准数据集上针对多种测试时数据质量问题的广泛实验表明，GrapHoST始终实现了最先进的性能，最高提升达10.92%。我们的代码已在https://github.com/YanJiangJerry/GrapHoST发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Homophily, the tendency of nodes from the same class to connect, is afundamental property of real-world graphs, underpinning structural and semanticpatterns in domains such as citation networks and social networks. Existingmethods exploit homophily through designing homophily-aware GNN architecturesor graph structure learning strategies, yet they primarily focus on GNNlearning with training graphs. However, in real-world scenarios, test graphsoften suffer from data quality issues and distribution shifts, such as domainshifts across users from different regions in social networks and temporalevolution shifts in citation network graphs collected over varying timeperiods. These factors significantly compromise the pre-trained model'srobustness, resulting in degraded test-time performance. With empiricalobservations and theoretical analysis, we reveal that transforming the testgraph structure by increasing homophily in homophilic graphs or decreasing itin heterophilic graphs can significantly improve the robustness and performanceof pre-trained GNNs on node classifications, without requiring model trainingor update. Motivated by these insights, a novel test-time graph structuraltransformation method grounded in homophily, named GrapHoST, is proposed.Specifically, a homophily predictor is developed to discriminate test edges,facilitating adaptive test-time graph structural transformation by theconfidence of predicted homophily scores. Extensive experiments on ninebenchmark datasets under a range of test-time data quality issues demonstratethat GrapHoST consistently achieves state-of-the-art performance, withimprovements of up to 10.92%. Our code has been released athttps://github.com/YanJiangJerry/GrapHoST.</description>
      <author>example@mail.com (Yan Jiang, Ruihong Qiu, Zi Huang)</author>
      <guid isPermaLink="false">2510.22289v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Graph Neural Network for Data-Driven Physiologically Based Pharmacokinetic Modeling</title>
      <link>http://arxiv.org/abs/2510.22096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了使用深度学习替代传统生理药代动力学建模方法，提出了一种动态图神经网络(Dynamic GNN)来模拟器官间的相互作用，实现了更高的预测性能。&lt;h4&gt;背景&lt;/h4&gt;生理药代动力学(PBPK)建模在药物开发中通过预测器官间药物浓度动态发挥关键作用。传统方法依赖常微分方程和强简化假设，限制了其对非线性生理相互作用的适应性。&lt;h4&gt;目的&lt;/h4&gt;探索使用深度学习进行PBPK预测的数据驱动替代方法，以提高预测准确性和适应性。&lt;h4&gt;方法&lt;/h4&gt;实现两种基线架构：多层感知器(MLP)和长短期记忆(LSTM)网络，分别用于捕捉分子和时间依赖性。提出动态图神经网络(Dynamic GNN)将生理连接建模为器官间的递归消息传递过程。&lt;h4&gt;主要发现&lt;/h4&gt;动态GNN在所有模型中表现最佳，预测性能最高，R平方值为0.9342，均方根误差为0.0159，平均绝对误差为0.0116。相比之下，MLP基线获得R平方值0.8705，LSTM获得0.8059。明确建模器官相互作用的时空依赖性可实现更准确和可推广的药物浓度预测。&lt;h4&gt;结论&lt;/h4&gt;动态GNN为传统PBPK公式提供了可扩展的、无方程的替代方案，在临床前和临床研究中，数据驱动的药代动力学建模展现出巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;生理药代动力学(PBPK)建模通过预测器官间药物浓度动态在药物开发中发挥关键作用。传统方法依赖常微分方程和强简化假设，限制了其对非线性生理相互作用的适应性。本研究探索了使用深度学习进行PBPK预测的数据驱动替代方法。实现了两种基线架构：多层感知器(MLP)和长短期记忆(LSTM)网络，分别用于捕捉分子和时间依赖性。为了整合器官间相互作用，我们提出了一种动态图神经网络(Dynamic GNN)，将生理连接建模为器官间的递归消息传递过程。实验结果表明，所提出的动态GNN在所有模型中实现了最高的预测性能，R平方值为0.9342，均方根误差为0.0159，平均绝对误差为0.0116。相比之下，MLP基线获得R平方值0.8705，LSTM获得0.8059。这些结果表明，明确建模器官相互作用的时空依赖性可实现更准确和可推广的药物浓度预测。动态GNN为传统PBPK公式提供了可扩展的、无方程的替代方案，并在临床前和临床研究中展现出数据驱动药代动力学建模的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physiologically Based Pharmacokinetic (PBPK) modeling plays a critical rolein drug development by predicting drug concentration dynamics across organs.Traditional approaches rely on ordinary differential equations with strongsimplifying assumptions, which limit their adaptability to nonlinearphysiological interactions. In this study, we explore data-driven alternativesfor PBPK prediction using deep learning. Two baseline architectures - amultilayer perceptron (MLP) and a long short-term memory (LSTM) network - areimplemented to capture molecular and temporal dependencies, respectively. Toincorporate inter-organ interactions, we propose a Dynamic Graph Neural Network(Dynamic GNN) that models physiological connections as recurrentmessage-passing processes between organs. Experimental results demonstrate thatthe proposed Dynamic GNN achieves the highest predictive performance among allmodels, with an R^2 of 0.9342, an RMSE of 0.0159, and an MAE of 0.0116. Incomparison, the MLP baseline obtains an R^2 of 0.8705 and the LSTM achieves0.8059. These results highlight that explicitly modeling the spatial andtemporal dependencies of organ interactions enables more accurate andgeneralizable drug concentration prediction. The Dynamic GNN provides ascalable, equation-free alternative to traditional PBPK formulations anddemonstrates strong potential for data-driven pharmacokinetic modeling inpreclinical and clinical research.</description>
      <author>example@mail.com (Su Liu, Xin Hu, Shurong Wen, Jiaqi Liu, Jiexi Xu, Lanruo Wang)</author>
      <guid isPermaLink="false">2510.22096v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Graph Networks for Accurate Weather Forecasting via Lightweight Training</title>
      <link>http://arxiv.org/abs/2510.22094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了HiFlowCast和HiAntFlow两种层级图神经网络模型，通过创新机制提高气候事件预测准确性，同时降低训练成本和环境影响。&lt;h4&gt;背景&lt;/h4&gt;气候事件由复杂的全球尺度驱动因素导致的多元动态过程产生，对食物、能源和基础设施有深远影响。然而，由于物理过程跨越不同的时空尺度，固定分辨率方法无法捕捉这些过程，导致准确天气预测仍然困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉多尺度物理过程的气候预测方法，提高预测准确性，特别是对于极端事件。&lt;h4&gt;方法&lt;/h4&gt;提出HiFlowCast和其集合变体HiAntFlow，这是一种层级图神经网络，将物理学嵌入多尺度预测框架。创新点包括：1)潜在记忆保留机制，在向下遍历过程中保持全局趋势；2)从潜在到物理的分支，整合不同尺度上的偏微分方程解场。&lt;h4&gt;主要发现&lt;/h4&gt;在13天提前期的预测中，模型将误差减少了5%以上；在第一和第九十九百分位极端情况下，误差减少了5-8%，提高了罕见事件的可靠性。利用预训练模型权重，模型在一个周期内收敛，显著降低了训练成本和碳足迹。&lt;h4&gt;结论&lt;/h4&gt;提高预测效率对于应对机器学习规模增长带来的可持续性挑战和研究可及性限制至关重要，代码和模型权重见补充材料。&lt;h4&gt;翻译&lt;/h4&gt;气候事件源于由全球尺度驱动的复杂多变量动态过程，深刻影响食物、能源和基础设施。然而，由于物理过程跨越多样的时空尺度展开，固定分辨率方法无法捕捉，准确的天气预测仍然难以实现。层级图神经网络提供多尺度表示，但非线性向下映射通常会抹去全局趋势，削弱物理学与预测的整合。我们引入HiFlowCast及其集合变体HiAntFlow，这些图神经网络将物理学嵌入多尺度预测框架。两个创新支撑了它们的设计：潜在记忆保留机制在向下遍历过程中保持全局趋势，以及从潜在到物理的分支整合不同尺度上的偏微分方程解场。我们的模型在13天提前期将误差减少5%以上，在第一和第九十九百分位极端情况下减少5-8%，提高了罕见事件的可靠性。利用预训练模型权重，它们在一个周期内收敛，降低了训练成本和碳足迹。这种效率至关重要，因为机器学习规模的不断增长挑战可持续性并限制研究可及性。代码和模型权重见补充材料。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Climate events arise from intricate, multivariate dynamics governed byglobal-scale drivers, profoundly impacting food, energy, and infrastructure.Yet, accurate weather prediction remains elusive due to physical processesunfolding across diverse spatio-temporal scales, which fixed-resolution methodscannot capture. Hierarchical Graph Neural Networks (HGNNs) offer a multiscalerepresentation, but nonlinear downward mappings often erase global trends,weakening the integration of physics into forecasts. We introduce HiFlowCastand its ensemble variant HiAntFlow, HGNNs that embed physics within amultiscale prediction framework. Two innovations underpin their design: aLatent-Memory-Retention mechanism that preserves global trends during downwardtraversal, and a Latent-to-Physics branch that integrates PDE solution fieldsacross diverse scales. Our Flow models cut errors by over 5% at 13-day leadtimes and by 5-8% under 1st and 99th quantile extremes, improving reliabilityfor rare events. Leveraging pretrained model weights, they converge within asingle epoch, reducing training cost and their carbon footprint. Suchefficiency is vital as the growing scale of machine learning challengessustainability and limits research accessibility. Code and model weights are inthe supplementary materials.</description>
      <author>example@mail.com (Thomas Bailie, S. Karthik Mukkavilli, Varvara Vetrova, Yun Sing Koh)</author>
      <guid isPermaLink="false">2510.22094v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Pruning and Quantization Impact on Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.22058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了神经网络压缩方法(剪枝和量化)对图神经网络(GNNs)的影响，发现非结构化细粒度和全局剪枝可显著减小模型大小(50%)同时保持或提高精度，而不同量化方法在不同数据集上有不同影响。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在图结构数据学习上具有高准确性，但面临高计算和资源成本问题。&lt;h4&gt;目的&lt;/h4&gt;研究神经网络压缩方法(剪枝和量化)如何减小GNN模型大小同时保持合理准确性。&lt;h4&gt;方法&lt;/h4&gt;在三个图数据集(Cora, Proteins, BBBBP)上评估三种剪枝方法和三种量化方法对三种GNN任务(图分类、节点分类和链接预测)的影响。&lt;h4&gt;主要发现&lt;/h4&gt;非结构化细粒度和全局剪枝可显著减小模型大小(50%)，同时在微调后保持甚至提高精度；不同量化方法对GNN的准确性、推理时间和模型大小在不同数据集上有不同影响。&lt;h4&gt;结论&lt;/h4&gt;神经网络压缩技术(特别是剪枝)可以有效减少GNN模型大小而不牺牲性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在从图结构数据学习方面以高精度著称，但它们面临高计算和资源成本的问题。神经网络压缩方法用于减小模型大小同时保持合理准确性。两种常见的神经网络压缩技术包括剪枝和量化。在这项研究中，我们实证检验了三种剪枝方法和三种量化方法对不同GNN模型的影响，包括图分类任务、节点分类任务和链接预测。我们在三个图数据集上进行了所有实验，包括Cora、Proteins和BBBP。我们的研究结果表明，非结构化细粒度和全局剪枝可以显著减小模型大小(50%)，同时在微调剪枝后的模型后保持甚至提高精度。对GNN上不同量化方法的评估显示，在不同数据集上，这些方法对准确性、推理时间和模型大小有不同影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are known to operate with high accuracy onlearning from graph-structured data, but they suffer from high computationaland resource costs. Neural network compression methods are used to reduce themodel size while maintaining reasonable accuracy. Two of the common neuralnetwork compression techniques include pruning and quantization. In thisresearch, we empirically examine the effects of three pruning methods and threequantization methods on different GNN models, including graph classificationtasks, node classification tasks, and link prediction. We conducted allexperiments on three graph datasets, including Cora, Proteins, and BBBP. Ourfindings demonstrate that unstructured fine-grained and global pruning cansignificantly reduce the model's size(50\%) while maintaining or even improvingprecision after fine-tuning the pruned model. The evaluation of differentquantization methods on GNN shows diverse impacts on accuracy, inference time,and model size across different datasets.</description>
      <author>example@mail.com (Khatoon Khedri, Reza Rawassizadeh, Qifu Wen, Mehdi Hosseinzadeh)</author>
      <guid isPermaLink="false">2510.22058v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>PF$Δ$: A Benchmark Dataset for Power Flow under Load, Generation, and Topology Variations</title>
      <link>http://arxiv.org/abs/2510.22048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 14 figures. Accepted at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PFΔ，一个用于潮流计算的基准数据集，旨在解决电力系统操作中的计算瓶颈和不确定性挑战。&lt;h4&gt;背景&lt;/h4&gt;潮流计算是实时电网操作的基础，但存在计算瓶颈问题。可再生能源整合和气候引起的极端天气增加了电力系统操作的不确定性，需要能够准确高效模拟各种场景的工具。&lt;h4&gt;目的&lt;/h4&gt;引入一个能够捕捉负荷、发电和拓扑多样变化的潮流计算基准数据集PFΔ，以评估现有方法并确定未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;构建包含859,800个已解决潮流计算实例的数据集，涵盖六种不同总线系统规模，包含三种应急场景（N、N-1和N-2），以及接近稳态电压稳定性极限的案例。&lt;h4&gt;主要发现&lt;/h4&gt;评估了传统求解器和基于GNN的方法，突出了现有方法面临的挑战领域，并确定了未来研究的开放性问题。&lt;h4&gt;结论&lt;/h4&gt;PFΔ数据集和相关代码已公开发布，为电力系统潮流计算研究提供了新资源。&lt;h4&gt;翻译&lt;/h4&gt;潮流计算是实时电网操作的基础，贯穿于工作流程中，如contingency analysis（重复PF评估评估停电情况下的电网安全）和拓扑优化（涉及基于PF的组合式大动作空间搜索）。在操作时间尺度上运行这些计算或在大评估空间中运行仍然是主要的计算瓶颈。此外，可再生能源整合和气候引起的极端天气导致的电力系统操作中不断增加的不确定性，也需要能够准确高效地模拟各种场景和操作条件的工具。机器学习方法相比传统求解器提供了潜在的速度提升，但它们在捕捉现实世界变异性的基准上尚未得到系统性评估。本文引入了PFΔ，一个潮流计算的基准数据集，捕捉了负荷、发电和拓扑的多样变化。PFΔ包含859,800个已解决的潮流计算实例，涵盖六种不同总线系统规模，捕获三种类型的应急场景（N、N-1和N-2），并包括接近稳态电压稳定性极限的接近不可行案例。我们评估了传统求解器和基于GNN的方法，突出了现有方法遇到困难的领域，并确定了未来研究的开放性问题。我们的数据集可在https://huggingface.co/datasets/pfdelta/pfdelta/tree/main获取，我们的代码包含数据生成脚本和模型实现，位于https://github.com/MOSSLab-MIT/pfdelta。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Power flow (PF) calculations are the backbone of real-time grid operations,across workflows such as contingency analysis (where repeated PF evaluationsassess grid security under outages) and topology optimization (which involvesPF-based searches over combinatorially large action spaces). Running thesecalculations at operational timescales or across large evaluation spacesremains a major computational bottleneck. Additionally, growing uncertainty inpower system operations from the integration of renewables and climate-inducedextreme weather also calls for tools that can accurately and efficientlysimulate a wide range of scenarios and operating conditions. Machine learningmethods offer a potential speedup over traditional solvers, but theirperformance has not been systematically assessed on benchmarks that capturereal-world variability. This paper introduces PF$\Delta$, a benchmark datasetfor power flow that captures diverse variations in load, generation, andtopology. PF$\Delta$ contains 859,800 solved power flow instances spanning sixdifferent bus system sizes, capturing three types of contingency scenarios (N ,N -1, and N -2), and including close-to-infeasible cases near steady-statevoltage stability limits. We evaluate traditional solvers and GNN-basedmethods, highlighting key areas where existing approaches struggle, andidentifying open problems for future research. Our dataset is available athttps://huggingface.co/datasets/pfdelta/pfdelta/tree/main and our code withdata generation scripts and model implementations is athttps://github.com/MOSSLab-MIT/pfdelta.</description>
      <author>example@mail.com (Ana K. Rivera, Anvita Bhagavathula, Alvaro Carbonero, Priya Donti)</author>
      <guid isPermaLink="false">2510.22048v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>A Hybrid GNN-LSE Method for Fast, Robust, and Physically-Consistent AC Power Flow</title>
      <link>http://arxiv.org/abs/2510.22020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合物理信息图神经网络(GNN)和线性状态估计(LSE)的两阶段混合方法，用于解决传统交流潮流求解器在大规模电力系统中的计算和收敛挑战。&lt;h4&gt;背景&lt;/h4&gt;传统的交流潮流(ACPF)求解器如牛顿-拉夫森法(NR)在现代大规模电力系统中面临显著的计算和收敛挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速且物理一致的电力系统求解方法，适用于实时操作和分析。&lt;h4&gt;方法&lt;/h4&gt;结合物理信息图神经网络与迭代线性状态估计：使用物理信息损失函数训练GNN预测高质量初始系统状态，然后通过LSE细化步骤解决线性方程以强制执行物理定律，绕过传统求解器的非线性和收敛问题。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在IEEE 33节点、69节点和118节点系统上得到验证；GNN变体比牛顿-拉夫森法快高达8400倍；LSE细化能快速获得物理一致解；重载压力测试和N-1 contingencies证明了方法的可靠性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该框架成功连接了快速数据驱动模型与电力系统物理约束，为实时电力系统操作和分析提供了实用工具。&lt;h4&gt;翻译&lt;/h4&gt;传统的交流潮流(ACPF)求解器如牛顿-拉夫森法(NR)在现代大规模电力系统中面临显著的计算和收敛挑战。本文提出了一种新颖的两阶段混合方法，结合物理信息图神经网络(GNN)和稳健的迭代线性状态估计(LSE)细化步骤，以产生快速且物理一致的解。使用具有高效动态加权方案的物理信息损失函数训练的GNN可快速预测高质量的初始系统状态。然后使用受状态估计技术启发的迭代直接线性求解器进行细化。LSE细化步骤解决一系列线性方程以强制执行物理定律，有效绕过传统求解器的非线性和收敛问题。所提出的GNN-LSE框架在从小的辐射状配电网(IEEE 33节点、69节点)到大型网状输电系统(IEEE 118节点)的各种系统上得到了全面验证。结果表明，我们的GNN变体比NR快高达8400倍。LSE细化提供了一条快速获得物理一致解的途径，而重载压力测试(标称值的120%-150%)和N-1 contingencies展示了该方法的可靠性和泛化能力。这项工作提出了一个强大而灵活的框架，用于连接快速的数据驱动模型与电力系统物理的严格约束，为实时操作和分析提供了实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional AC Power Flow (ACPF) solvers like Newton-Raphson (NR) facesignificant computational and convergence challenges in modern, large-scalepower systems. This paper proposes a novel, two-stage hybrid method thatintegrates a Physics-Informed Graph Neural Network (GNN) with a robust,iterative Linear State Estimation (LSE) refinement step to produce fast andphysically-consistent solutions. The GNN, trained with a physics-informed lossfunction featuring an efficient dynamic weighting scheme, rapidly predicts ahigh-quality initial system state. This prediction is then refined using aniterative, direct linear solver inspired by state estimation techniques. ThisLSE refinement step solves a series of linear equations to enforce physicallaws, effectively bypassing the non-linearities and convergence issues oftraditional solvers. The proposed GNN-LSE framework is comprehensivelyvalidated on systems ranging from small radial distribution networks (IEEE33-bus, 69-bus) to a large, meshed transmission system (IEEE 118-bus). Resultsshow that our GNN variants are up to $8.4 \times 10^3$ times faster than NR.The LSE refinement provides a fast route to a physically-consistent solution,while heavy-loading stress tests (120%-150% of nominal) and N-1 contingenciesdemonstrate the method's reliability and generalization. This work presents apowerful and flexible framework for bridging fast, data-driven models with therigorous constraints of power system physics, offering a practical tool forreal-time operations and analysis.</description>
      <author>example@mail.com (Mohamed Shamseldein)</author>
      <guid isPermaLink="false">2510.22020v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Human Protein Embeddings Database: DeepDrug Protein Embeddings Bank (DPEB)</title>
      <link>http://arxiv.org/abs/2510.22008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DPEB是一个整合了四种蛋白质嵌入类型的数据集，用于提高蛋白质-蛋白质相互作用(PPI)预测的准确性，并在多种蛋白质分类任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;计算预测蛋白质-蛋白质相互作用(PPI)具有挑战性，主要原因是缺乏整合的多模态蛋白质表示方法。&lt;h4&gt;目的&lt;/h4&gt;创建一个整合多种蛋白质嵌入类型的数据集，填补AlphaFold2内部神经网络嵌入不可用的空白，为计算建模提供支持。&lt;h4&gt;方法&lt;/h4&gt;DPEB是一个包含22,043个人类蛋白质的精选集合，整合了四种嵌入类型：结构嵌入(AlphaFold2)、基于transformer的序列嵌入(BioEmbeddings)、上下文氨基酸模式(ESM-2)和基于序列的n-gram统计(ProtVec)。&lt;h4&gt;主要发现&lt;/h4&gt;GraphSAGE与BioEmbedding结合实现了最高的PPI预测性能(87.37% AUROC, 79.16%准确率)；该框架在酶分类任务上达到77.42%的准确率；在蛋白质家族分类任务上达到86.04%的准确率。&lt;h4&gt;结论&lt;/h4&gt;DPEB支持多种图神经网络方法进行PPI预测，可应用于系统生物学、药物靶点识别、通路分析和疾病机制研究。&lt;h4&gt;翻译&lt;/h4&gt;计算预测蛋白质-蛋白质相互作用(PPI)具有挑战性，由于缺乏整合的多模态蛋白质表示。DPEB是一个包含22,043个人类蛋白质的精选集合，整合了四种嵌入类型：结构(AlphaFold2)、基于transformer的序列(BioEmbeddings)、上下文氨基酸模式(ESM-2: Evolutionary Scale Modeling)和基于序列的n-gram统计(ProtVec)。AlphaFold2蛋白质结构可通过公共数据库(如AlphaFold2蛋白质结构数据库)获取，但内部神经网络嵌入不可用。DPEB通过提供AlphaFold2衍生的嵌入用于计算建模来填补这一空白。我们的基准评估显示，GraphSAGE与BioEmbedding结合实现了最高的PPI预测性能(87.37% AUROC, 79.16%准确率)。该框架在酶分类上实现了77.42%的准确率，在蛋白质家族分类上实现了86.04%的准确率。DPEB支持多种图神经网络方法进行PPI预测，能够在系统生物学、药物靶点识别、通路分析和疾病机制研究中应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computationally predicting protein-protein interactions (PPIs) is challengingdue to the lack of integrated, multimodal protein representations. DPEB is acurated collection of 22,043 human proteins that integrates four embeddingtypes: structural (AlphaFold2), transformer-based sequence (BioEmbeddings),contextual amino acid patterns (ESM-2: Evolutionary Scale Modeling), andsequence-based n-gram statistics (ProtVec]). AlphaFold2 protein structures areavailable through public databases (e.g., AlphaFold2 Protein StructureDatabase), but the internal neural network embeddings are not. DPEB addressesthis gap by providing AlphaFold2-derived embeddings for computational modeling.Our benchmark evaluations show GraphSAGE with BioEmbedding achieved the highestPPI prediction performance (87.37% AUROC, 79.16% accuracy). The framework alsoachieved 77.42% accuracy for enzyme classification and 86.04% accuracy forprotein family classification. DPEB supports multiple graph neural networkmethods for PPI prediction, enabling applications in systems biology, drugtarget identification, pathway analysis, and disease mechanism studies.</description>
      <author>example@mail.com (Md Saiful Islam Sajol, Magesh Rajasekaran, Hayden Gemeinhardt, Adam Bess, Chris Alvin, Supratik Mukhopadhyay)</author>
      <guid isPermaLink="false">2510.22008v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning on Real-World Graphs</title>
      <link>http://arxiv.org/abs/2510.21994v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The thesis was submitted for the degree of Doctor of Philosophy in  Computing at Imperial College London (February 2024), under the supervision  of Prof. Michael M. Bronstein. It includes work published at ICML, ICLR,  NeurIPS, and the Learning on Graphs Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一系列图神经网络模型，解决了GNNs在实际应用中的关键挑战，包括可扩展性、时间性、方向性、数据不完整性和结构不确定性等问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为学习图结构数据的核心工具，但在实际系统中的应用受到可扩展性、时间性、方向性、数据不完整性和结构不确定性等关键挑战的限制。&lt;h4&gt;目的&lt;/h4&gt;解决GNNs在实际应用中的限制，使其能够应用于工业规模的图数据。&lt;h4&gt;方法&lt;/h4&gt;作者提出了五个模型：SIGN用于可扩展图学习，TGN用于时间图，Dir-GNN用于有向和异质网络，Feature Propagation用于处理缺失节点特征，NuGget用于博弈论结构推断。&lt;h4&gt;主要发现&lt;/h4&gt;这些模型共同弥合了学术基准和工业规模图之间的差距，使GNNs能够在社交系统和推荐系统等领域使用。&lt;h4&gt;结论&lt;/h4&gt;通过这些创新模型，GNNs的实际应用限制得到了解决，使其能够在真实世界系统中有效应用。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为学习图结构数据的核心工具，但它们在实际系统中的应用受到可扩展性、时间性、方向性、数据不完整性和结构不确定性等关键挑战的限制。本论文引入了一系列解决这些限制的模型：SIGN用于可扩展图学习，TGN用于时间图，Dir-GNN用于有向和异质网络，Feature Propagation (FP)用于学习缺失节点特征，NuGget用于博弈论结构推断。这些贡献共同弥合了学术基准和工业规模图之间的差距，使GNNs能够在社交和推荐系统等领域使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.25560/112863&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become a central tool for learning ongraph-structured data, yet their applicability to real-world systems remainslimited by key challenges such as scalability, temporality, directionality,data incompleteness, and structural uncertainty. This thesis introduces aseries of models addressing these limitations: SIGN for scalable graphlearning, TGN for temporal graphs, Dir-GNN for directed and heterophilicnetworks, Feature Propagation (FP) for learning with missing node features, andNuGget for game-theoretic structural inference. Together, these contributionsbridge the gap between academic benchmarks and industrial-scale graphs,enabling the use of GNNs in domains such as social and recommender systems.</description>
      <author>example@mail.com (Emanuele Rossi)</author>
      <guid isPermaLink="false">2510.21994v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Classical Algorithms for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.21574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了通过在经典算法上预训练图神经网络(GNNs)来提升其在分子属性预测任务上的性能。研究证明将经典算法的先验知识嵌入到GNNs中能够提供有用的归纳偏置，从而提升在复杂真实世界图数据上的表现。&lt;h4&gt;背景&lt;/h4&gt;神经网络在处理非结构化数据方面表现出色，但通常无法分布外泛化；而经典算法虽然保证正确性但缺乏灵活性。&lt;h4&gt;目的&lt;/h4&gt;探索通过在经典算法上预训练图神经网络(GNNs)来改善其在Open Graph Benchmark上的分子属性预测任务中的性能，包括ogbg-molhiv(HIV抑制)和ogbg-molclintox(临床毒性)任务。&lt;h4&gt;方法&lt;/h4&gt;使用从CLRS算法推理基准中的24个经典算法训练的GNNs，来初始化并冻结第二GNN的选定层，用于分子预测任务。&lt;h4&gt;主要发现&lt;/h4&gt;与随机初始化的基线相比，预训练模型取得了一致的胜利或平局。其中，基于Segments Intersect算法的预训练在ogbg-molhiv上取得了6%的绝对增益，基于Dijkstra的预训练在ogbg-molclintox上取得了3%的增益。&lt;h4&gt;结论&lt;/h4&gt;将经典算法的先验知识嵌入到GNNs中可以提供有用的归纳偏置，提高在复杂、真实世界图数据上的性能。&lt;h4&gt;翻译&lt;/h4&gt;神经网络在处理非结构化数据方面表现出色，但通常无法分布外泛化，而经典算法虽然保证正确性但缺乏灵活性。我们探索了通过在经典算法上预训练图神经网络(GNNs)来改善其在Open Graph Benchmark上的分子属性预测任务中的性能，包括ogbg-molhiv(HIV抑制)和ogbg-molclintox(临床毒性)任务。使用从CLRS算法推理基准中的24个经典算法训练的GNNs，来初始化并冻结第二GNN的选定层用于分子预测。与随机初始化的基线相比，预训练模型取得了一致的胜利或平局，其中基于Segments Intersect算法的预训练在ogbg-molhiv上取得了6%的绝对增益，基于Dijkstra的预训练在ogbg-molclintox上取得了3%的增益。这些结果表明将经典算法的先验知识嵌入到GNNs中可以提供有用的归纳偏置，提高在复杂、真实世界图数据上的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural networks excel at processing unstructured data but often fail togeneralise out-of-distribution, whereas classical algorithms guaranteecorrectness but lack flexibility. We explore whether pretraining Graph NeuralNetworks (GNNs) on classical algorithms can improve their performance onmolecular property prediction tasks from the Open Graph Benchmark: ogbg-molhiv(HIV inhibition) and ogbg-molclintox (clinical toxicity). GNNs trained on 24classical algorithms from the CLRS Algorithmic Reasoning Benchmark are used toinitialise and freeze selected layers of a second GNN for molecular prediction.Compared to a randomly initialised baseline, the pretrained models achieveconsistent wins or ties, with the Segments Intersect algorithm pretrainingyielding a 6% absolute gain on ogbg-molhiv and Dijkstra pretraining achieving a3% gain on ogbg-molclintox. These results demonstrate embedding classicalalgorithmic priors into GNNs provides useful inductive biases, boostingperformance on complex, real-world graph data.</description>
      <author>example@mail.com (Jason Wu, Petar Veličković)</author>
      <guid isPermaLink="false">2510.21574v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>HollowFlow: Efficient Sample Likelihood Evaluation using Hollow Message Passing</title>
      <link>http://arxiv.org/abs/2510.21542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为HollowFlow的流模型，利用非回溯图神经网络解决大规模系统中的计算瓶颈问题，实现了高达O(n²)的加速，使基于流的生成模型能够应用于更大规模的科学问题。&lt;h4&gt;背景&lt;/h4&gt;流和扩散模型已成为科学应用的强大工具，特别适用于采样非归一化概率分布，如玻尔兹曼生成器(BGs)。然而，这些模型在实际部署时面临关键挑战：它们依赖于样本似然计算，而这种计算的系统规模n呈指数级增长，使得大规模问题难以处理。&lt;h4&gt;目的&lt;/h4&gt;为了解决流模型在大规模系统中的计算效率问题，作者引入了HollowFlow，旨在显著提高似然评估速度，使BGs能够扩展到更大的系统。&lt;h4&gt;方法&lt;/h4&gt;作者提出了HollowFlow，一种利用新型非回溯图神经网络(NoBGNN)的基于流的生成模型。通过强制块对角雅可比结构，HollowFlow的似然评估可以在n中用常数次反向传播完成。该框架具有普适性，任何等变GNN或基于注意力的架构都可以被适配为NoBGNN。&lt;h4&gt;主要发现&lt;/h4&gt;作者通过在两个不同规模的系统上训练BGs验证了HollowFlow。对于这两个系统，采样和似然评估时间都显著减少，遵循了理论上的缩放规律。对于较大的系统，作者获得了100倍的加速，展示了基于HollowFlow的方法在高维科学问题上的潜力。&lt;h4&gt;结论&lt;/h4&gt;HollowFlow为基于流的生成模型在大规模科学问题中的应用提供了有效解决方案，通过创新的图神经网络架构显著提高了计算效率，使得以前因计算限制而无法处理的高维问题现在变得可行。&lt;h4&gt;翻译&lt;/h4&gt;流和扩散模型已成为科学应用的强大工具，特别适用于采样非归一化概率分布，如玻尔兹曼生成器(BGs)。部署这些模型的一个关键挑战是它们依赖于样本似然计算，而这种计算的系统规模n呈指数级增长，通常使得大规模问题变得不可行。为了解决这个问题，我们引入了HollowFlow，这是一种基于流的生成模型，利用了一种新型的非回溯图神经网络(NoBGNN)。通过强制块对角雅可比结构，HollowFlow的似然评估可以在n中用常数次反向传播完成，实现高达O(n²)的加速：这是将BGs扩展到更大系统的重要一步。重要的是，我们的框架具有普适性：任何等变GNN或基于注意力的架构都可以被适配为NoBGNN。我们通过在两个不同规模的系统上训练BGs来验证HollowFlow。对于这两个系统，采样和似然评估时间都显著减少，遵循了理论上的缩放规律。对于较大的系统，我们获得了100倍的加速，清楚地展示了基于HollowFlow的方法在高维科学问题上的潜力，这些问题以前因计算瓶颈而受到阻碍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Flow and diffusion-based models have emerged as powerful tools for scientificapplications, particularly for sampling non-normalized probabilitydistributions, as exemplified by Boltzmann Generators (BGs). A criticalchallenge in deploying these models is their reliance on sample likelihoodcomputations, which scale prohibitively with system size $n$, often renderingthem infeasible for large-scale problems. To address this, we introduce$\textit{HollowFlow}$, a flow-based generative model leveraging a novelnon-backtracking graph neural network (NoBGNN). By enforcing a block-diagonalJacobian structure, HollowFlow likelihoods are evaluated with a constant numberof backward passes in $n$, yielding speed-ups of up to $\mathcal{O}(n^2)$: asignificant step towards scaling BGs to larger systems. Crucially, ourframework generalizes: $\textbf{any equivariant GNN or attention-basedarchitecture}$ can be adapted into a NoBGNN. We validate HollowFlow by trainingBGs on two different systems of increasing size. For both systems, the samplingand likelihood evaluation time decreases dramatically, following ourtheoretical scaling laws. For the larger system we obtain a $10^2\times$speed-up, clearly illustrating the potential of HollowFlow-based approaches forhigh-dimensional scientific problems previously hindered by computationalbottlenecks.</description>
      <author>example@mail.com (Johann Flemming Gloy, Simon Olsson)</author>
      <guid isPermaLink="false">2510.21542v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Estimating Treatment Effects in Networks using Domain Adversarial Training</title>
      <link>http://arxiv.org/abs/2510.21457v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了HINet方法，通过结合图神经网络和领域对抗训练，解决了网络环境中估计异质治疗效应时面临的干扰、未知暴露映射和网络层面协变量偏移等问题。&lt;h4&gt;背景&lt;/h4&gt;在网络环境中估计异质治疗效应受到干扰困扰，一个实例的结果可能受到他人治疗状态的影响。现有方法通常假设已知的暴露映射，这往往不现实。同质性与治疗分配机制的相互作用可能导致网络层面的协变量偏移，进而导致治疗效应估计不准确，这种现象尚未被明确研究。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够在未知暴露映射下估计治疗效应的方法，同时减轻网络层面协变量偏移的影响。&lt;h4&gt;方法&lt;/h4&gt;提出了HINet，一种新颖的方法，结合了图神经网络和领域对抗训练。这种组合允许在未知暴露映射下估计治疗效应，同时减轻网络层面协变量偏移的影响。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和半合成网络数据集上的广泛实证评估证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;HINet方法成功解决了网络环境中估计异质治疗效应的挑战。&lt;h4&gt;翻译&lt;/h4&gt;在网络环境中估计异质治疗效应因干扰而复杂化，这意味着一个实例的结果可能受到他人治疗状态的影响。现有的因果机器学习方法通常假设已知的暴露映射，该映射总结了给定实例的结果如何受他人治疗的影响，这是一种简化的假设，通常不切实际。此外，同质性——相似实例倾向于连接——与治疗分配机制之间的相互作用可能引发网络层面的协变量偏移，可能导致不准确的治疗效应估计，这种现象尚未被明确研究。为了应对这些挑战，我们提出了HINet，一种将图神经网络与领域对抗训练相结合的新颖方法。这种组合允许在未知暴露映射的情况下估计治疗效应，同时减轻（网络层面）协变量偏移的影响。在合成和半合成网络数据集上的广泛实证评估证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating heterogeneous treatment effects in network settings is complicatedby interference, meaning that the outcome of an instance can be influenced bythe treatment status of others. Existing causal machine learning approachesusually assume a known exposure mapping that summarizes how the outcome of agiven instance is influenced by others' treatment, a simplification that isoften unrealistic. Furthermore, the interaction between homophily -- thetendency of similar instances to connect -- and the treatment assignmentmechanism can induce a network-level covariate shift that may lead toinaccurate treatment effect estimates, a phenomenon that has not yet beenexplicitly studied. To address these challenges, we propose HINet, a novelmethod that integrates graph neural networks with domain adversarial training.This combination allows estimating treatment effects under unknown exposuremappings while mitigating the impact of (network-level) covariate shift. Anextensive empirical evaluation on synthetic and semi-synthetic network datasetsdemonstrates the effectiveness of our approach.</description>
      <author>example@mail.com (Daan Caljon, Jente Van Belle, Wouter Verbeke)</author>
      <guid isPermaLink="false">2510.21457v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    </channel>
</rss>